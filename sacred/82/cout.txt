INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "82"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 04:43:12.133004: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:43:12.133045: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:43:12.133052: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:43:12.133057: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:43:12.133062: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:43:13.940710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-06 04:43:13.940746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 04:43:13.940754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 04:43:13.940762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 04:43:18.141608: step 0, loss = 2.03, batch loss = 1.97 (2.8 examples/sec; 2.852 sec/batch; 263h:23m:03s remains)
2017-12-06 04:43:18.583799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3188076 -4.3186374 -4.3226447 -4.3227344 -4.3119445 -4.2908707 -4.264462 -4.2423506 -4.2365 -4.245111 -4.2614832 -4.2815342 -4.3029742 -4.3193345 -4.3237743][-4.3163376 -4.3164334 -4.3199291 -4.3133631 -4.2888021 -4.2508903 -4.2094026 -4.1821718 -4.1837459 -4.2053523 -4.2326884 -4.2602477 -4.2876668 -4.3089895 -4.3179345][-4.3094087 -4.3105779 -4.3088961 -4.2860541 -4.2397532 -4.1768875 -4.113091 -4.0809231 -4.0954633 -4.139122 -4.1858459 -4.2295914 -4.2686224 -4.2977304 -4.3119211][-4.2965832 -4.3001475 -4.2917509 -4.2503314 -4.1827717 -4.094069 -4.0030041 -3.9640696 -3.9988883 -4.0685906 -4.1375542 -4.2017951 -4.2534037 -4.2901473 -4.3088636][-4.2818604 -4.2879238 -4.2712579 -4.2124767 -4.1224961 -4.0022459 -3.8774507 -3.8324308 -3.8983264 -3.99919 -4.09241 -4.1763 -4.240015 -4.285563 -4.3079906][-4.26544 -4.2704654 -4.2446332 -4.16965 -4.0549154 -3.9016054 -3.7413983 -3.7003415 -3.8137875 -3.9498265 -4.0629377 -4.1595764 -4.2305493 -4.2829466 -4.3079205][-4.2498717 -4.2478204 -4.211482 -4.1250124 -3.9935815 -3.817553 -3.6374829 -3.6263576 -3.7923167 -3.9518955 -4.0699105 -4.1657257 -4.2361207 -4.2882342 -4.3110676][-4.2359872 -4.2234612 -4.1823192 -4.0998831 -3.9806786 -3.8241444 -3.6829858 -3.7197266 -3.8882601 -4.0273557 -4.1188259 -4.1954784 -4.2553306 -4.2997 -4.316741][-4.22153 -4.2020607 -4.1657553 -4.1060162 -4.0231872 -3.9194672 -3.8461702 -3.9005189 -4.0234532 -4.1155577 -4.1705451 -4.2261982 -4.2745175 -4.3109117 -4.3220625][-4.2095852 -4.1882215 -4.1618853 -4.12673 -4.0817938 -4.0252247 -3.9961355 -4.04331 -4.1215906 -4.1740012 -4.1997013 -4.2411375 -4.2842307 -4.3164458 -4.3248463][-4.2042923 -4.1853561 -4.1688986 -4.1507573 -4.1308126 -4.1027465 -4.0963287 -4.1304483 -4.176219 -4.2032137 -4.2158337 -4.2516551 -4.2931528 -4.3218613 -4.3274994][-4.2141509 -4.2035875 -4.1982317 -4.1918445 -4.1852393 -4.1725531 -4.1734562 -4.1922145 -4.2107635 -4.2209449 -4.2280006 -4.2612877 -4.30077 -4.3255596 -4.3288751][-4.2342257 -4.233263 -4.2368474 -4.2356725 -4.2337451 -4.2247505 -4.220912 -4.2237091 -4.2235904 -4.2238617 -4.2297277 -4.2622757 -4.3007426 -4.3244338 -4.3276849][-4.2433543 -4.249321 -4.2581663 -4.2582293 -4.256927 -4.2459774 -4.2323933 -4.2223997 -4.2133636 -4.211514 -4.2198892 -4.2548456 -4.2949171 -4.3198967 -4.3249149][-4.2357793 -4.2433362 -4.254405 -4.2556057 -4.2534933 -4.238975 -4.2155819 -4.1950297 -4.1840563 -4.1878214 -4.2020631 -4.2418008 -4.2851319 -4.3130894 -4.3208418]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 04:43:21.462710: step 10, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:53s remains)
INFO - root - 2017-12-06 04:43:23.580651: step 20, loss = 2.04, batch loss = 1.98 (38.0 examples/sec; 0.211 sec/batch; 19h:26m:30s remains)
INFO - root - 2017-12-06 04:43:25.683433: step 30, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 19h:05m:58s remains)
INFO - root - 2017-12-06 04:43:27.822643: step 40, loss = 2.08, batch loss = 2.02 (38.7 examples/sec; 0.207 sec/batch; 19h:05m:58s remains)
INFO - root - 2017-12-06 04:43:29.961008: step 50, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.212 sec/batch; 19h:33m:40s remains)
INFO - root - 2017-12-06 04:43:32.101367: step 60, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.212 sec/batch; 19h:32m:09s remains)
INFO - root - 2017-12-06 04:43:34.171416: step 70, loss = 2.06, batch loss = 2.01 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:18s remains)
INFO - root - 2017-12-06 04:43:36.338043: step 80, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.211 sec/batch; 19h:31m:07s remains)
INFO - root - 2017-12-06 04:43:38.470458: step 90, loss = 2.09, batch loss = 2.03 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:31s remains)
INFO - root - 2017-12-06 04:43:40.584969: step 100, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:34s remains)
2017-12-06 04:43:40.939767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2658238 -4.2528663 -4.2420559 -4.2271609 -4.2218022 -4.2311149 -4.2494707 -4.26585 -4.2661347 -4.2462759 -4.2054253 -4.1446729 -4.0797105 -4.0252991 -4.0018263][-4.2803564 -4.2570558 -4.2344804 -4.2124562 -4.2076144 -4.2184734 -4.237474 -4.2522187 -4.2460146 -4.2166238 -4.1668477 -4.0989895 -4.0311389 -3.98239 -3.97443][-4.2853222 -4.2605486 -4.2348442 -4.2109461 -4.2033978 -4.2091174 -4.2202597 -4.2265162 -4.2140718 -4.1812954 -4.1341839 -4.0809231 -4.0368133 -4.0156279 -4.0301328][-4.2796211 -4.2632627 -4.2420444 -4.2175689 -4.2031908 -4.1965137 -4.188807 -4.1784639 -4.1604486 -4.1324725 -4.1056209 -4.088408 -4.0873871 -4.1031508 -4.1300583][-4.2748861 -4.2687654 -4.25376 -4.2280145 -4.1996408 -4.1696787 -4.1349678 -4.1060214 -4.0877962 -4.079998 -4.0882978 -4.1117029 -4.1468329 -4.18457 -4.2133088][-4.2704635 -4.2678566 -4.2558761 -4.2253547 -4.17826 -4.1194725 -4.0594363 -4.0167894 -4.0066838 -4.0315237 -4.076292 -4.1281109 -4.1834412 -4.2317147 -4.2612638][-4.2558517 -4.2499309 -4.2354708 -4.2017574 -4.1431293 -4.0619669 -3.9835494 -3.9371071 -3.9466739 -4.0099072 -4.0845904 -4.1496196 -4.21072 -4.25918 -4.2887053][-4.2271509 -4.2141628 -4.2005277 -4.1758213 -4.1235371 -4.0445709 -3.9675503 -3.9290447 -3.9574776 -4.0444856 -4.1302891 -4.1931758 -4.2434568 -4.281764 -4.304069][-4.186482 -4.1648545 -4.156558 -4.1522765 -4.1288509 -4.0861483 -4.0458951 -4.0338626 -4.0634971 -4.1313314 -4.1957541 -4.2390804 -4.2696624 -4.2890153 -4.2956433][-4.1342716 -4.110621 -4.1165495 -4.1434669 -4.1581507 -4.1579056 -4.1540422 -4.1616526 -4.1854 -4.2229218 -4.256896 -4.2753534 -4.282114 -4.2803 -4.2730064][-4.0740871 -4.0607157 -4.0891171 -4.1473074 -4.1933966 -4.2215858 -4.2368069 -4.2511435 -4.2687879 -4.2869878 -4.2959309 -4.2903972 -4.2771149 -4.2608142 -4.2434769][-4.018693 -4.0258551 -4.0753813 -4.1527419 -4.2166209 -4.2558236 -4.2773404 -4.2938375 -4.3087106 -4.31856 -4.3133936 -4.2885218 -4.257967 -4.2290936 -4.2042074][-4.0036087 -4.028924 -4.0868144 -4.1620507 -4.2254529 -4.26154 -4.2814851 -4.2983232 -4.31222 -4.3177209 -4.3033195 -4.2669725 -4.2282233 -4.194766 -4.1677647][-4.0460219 -4.0792966 -4.1291246 -4.1876359 -4.2369142 -4.2632732 -4.2764173 -4.2887115 -4.29666 -4.2938995 -4.2736726 -4.2383709 -4.2053852 -4.1795807 -4.1586218][-4.127326 -4.1580491 -4.1916308 -4.2268324 -4.2569847 -4.2713571 -4.2761021 -4.27917 -4.27864 -4.2703404 -4.2519007 -4.2269626 -4.2087779 -4.1986995 -4.1893306]]...]
INFO - root - 2017-12-06 04:43:43.135766: step 110, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:43s remains)
INFO - root - 2017-12-06 04:43:45.291589: step 120, loss = 2.05, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:32s remains)
INFO - root - 2017-12-06 04:43:47.438802: step 130, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:14s remains)
INFO - root - 2017-12-06 04:43:49.594692: step 140, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:28m:06s remains)
INFO - root - 2017-12-06 04:43:51.707552: step 150, loss = 2.09, batch loss = 2.03 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:36s remains)
INFO - root - 2017-12-06 04:43:53.878182: step 160, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:23s remains)
INFO - root - 2017-12-06 04:43:56.052676: step 170, loss = 2.06, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:33s remains)
INFO - root - 2017-12-06 04:43:58.264090: step 180, loss = 2.05, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:06s remains)
INFO - root - 2017-12-06 04:44:00.462440: step 190, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:55s remains)
INFO - root - 2017-12-06 04:44:02.615929: step 200, loss = 2.06, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:54s remains)
2017-12-06 04:44:03.112912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3072076 -4.3020477 -4.2987008 -4.30009 -4.3050818 -4.3111072 -4.3157725 -4.3184571 -4.3213725 -4.3250737 -4.3304977 -4.3340735 -4.3359294 -4.3357663 -4.3345566][-4.2929645 -4.2834449 -4.2785249 -4.2845826 -4.294456 -4.3000841 -4.3005114 -4.2991753 -4.3005733 -4.3054333 -4.3155184 -4.3251762 -4.3305454 -4.3306227 -4.3295364][-4.2766609 -4.2638197 -4.2623158 -4.2734618 -4.282969 -4.2815871 -4.2726951 -4.2630754 -4.2616644 -4.2710533 -4.2908735 -4.3095789 -4.320785 -4.3222346 -4.3219962][-4.2639632 -4.2509432 -4.2533822 -4.265286 -4.2670817 -4.2527022 -4.2296405 -4.2075763 -4.2030692 -4.2218618 -4.2552991 -4.2836661 -4.3020425 -4.3080854 -4.3112893][-4.2572241 -4.24488 -4.246511 -4.2516675 -4.2398386 -4.2095313 -4.1691527 -4.1362066 -4.1378813 -4.1733651 -4.220746 -4.2560077 -4.2807646 -4.2924323 -4.2997584][-4.2372408 -4.2197881 -4.2131166 -4.2076797 -4.1817961 -4.1340494 -4.0747919 -4.0404248 -4.0685916 -4.1304173 -4.1899548 -4.2310276 -4.2618632 -4.27962 -4.2910175][-4.1972318 -4.166677 -4.143414 -4.120718 -4.0781903 -4.0101829 -3.935806 -3.9175768 -3.993788 -4.0894194 -4.1595659 -4.2074223 -4.2466311 -4.2711954 -4.2856236][-4.1646919 -4.1221828 -4.0799313 -4.0367002 -3.9774456 -3.8974023 -3.8225417 -3.8349142 -3.9562182 -4.0748973 -4.1506763 -4.2027369 -4.2463746 -4.2728624 -4.2854981][-4.1426473 -4.0995626 -4.0539675 -4.0106721 -3.9598053 -3.9030356 -3.865346 -3.9016917 -4.0129013 -4.1137762 -4.1782594 -4.2243166 -4.2626925 -4.283474 -4.2911491][-4.1438055 -4.1089416 -4.0771666 -4.0550261 -4.033628 -4.0107746 -4.005332 -4.0406737 -4.1134467 -4.1777182 -4.2214408 -4.2559338 -4.2837629 -4.2965875 -4.299942][-4.1653333 -4.1457205 -4.1342769 -4.1326828 -4.13518 -4.1303535 -4.1335983 -4.1576777 -4.1966019 -4.2300887 -4.2563143 -4.2796774 -4.2994323 -4.3088059 -4.3108077][-4.1992407 -4.1935425 -4.1969905 -4.2043915 -4.2149558 -4.2168369 -4.2199578 -4.2334681 -4.2521825 -4.2685537 -4.2841907 -4.3010416 -4.3163872 -4.3233647 -4.3240438][-4.2396307 -4.2413864 -4.2493954 -4.2565818 -4.26303 -4.2614641 -4.2600684 -4.2653584 -4.2750106 -4.2867689 -4.2991605 -4.3136716 -4.3276711 -4.334373 -4.3348131][-4.2711763 -4.2755818 -4.2810864 -4.2833042 -4.2828913 -4.2769556 -4.2732296 -4.2750235 -4.2810378 -4.2902689 -4.301302 -4.3161087 -4.329987 -4.3370767 -4.3378868][-4.2949929 -4.296639 -4.2986584 -4.2977638 -4.2943082 -4.2893777 -4.2880168 -4.2901073 -4.2933264 -4.2976356 -4.3059464 -4.3187251 -4.3300948 -4.3354526 -4.3359509]]...]
INFO - root - 2017-12-06 04:44:05.293425: step 210, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:09s remains)
INFO - root - 2017-12-06 04:44:07.449819: step 220, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:47s remains)
INFO - root - 2017-12-06 04:44:09.655856: step 230, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:34s remains)
INFO - root - 2017-12-06 04:44:11.825353: step 240, loss = 2.04, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:48s remains)
INFO - root - 2017-12-06 04:44:14.010769: step 250, loss = 2.07, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:14s remains)
INFO - root - 2017-12-06 04:44:16.165424: step 260, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 20h:47m:45s remains)
INFO - root - 2017-12-06 04:44:18.328639: step 270, loss = 2.08, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:19s remains)
INFO - root - 2017-12-06 04:44:20.467725: step 280, loss = 2.04, batch loss = 1.98 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:40s remains)
INFO - root - 2017-12-06 04:44:22.649107: step 290, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:10s remains)
INFO - root - 2017-12-06 04:44:24.834870: step 300, loss = 2.08, batch loss = 2.03 (36.6 examples/sec; 0.218 sec/batch; 20h:09m:08s remains)
2017-12-06 04:44:26.120240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1940274 -4.1894159 -4.1747289 -4.1658354 -4.1601872 -4.1622276 -4.1654739 -4.1593709 -4.1497941 -4.1584091 -4.180799 -4.1970119 -4.1929178 -4.169579 -4.1502967][-4.1792479 -4.1826172 -4.1853857 -4.1889896 -4.1826191 -4.1751108 -4.168252 -4.1575208 -4.1527891 -4.1647916 -4.1812491 -4.190289 -4.1835027 -4.1655903 -4.1604147][-4.1512756 -4.1627703 -4.1817861 -4.1960015 -4.1921263 -4.1764026 -4.1614709 -4.1456203 -4.13576 -4.1410403 -4.1504393 -4.1582241 -4.1569738 -4.1546369 -4.1662717][-4.132503 -4.1459427 -4.1740851 -4.2002444 -4.2004547 -4.1776123 -4.1501608 -4.1231236 -4.0973978 -4.0841084 -4.0891004 -4.106986 -4.123961 -4.1440396 -4.1712308][-4.144834 -4.1530948 -4.1773691 -4.2072654 -4.2093744 -4.1809039 -4.1363783 -4.0875416 -4.039938 -4.0046649 -4.0033927 -4.0373368 -4.0818229 -4.1266685 -4.1684847][-4.1774683 -4.1771069 -4.1929917 -4.2176447 -4.2175012 -4.1861553 -4.1290889 -4.0620122 -3.9972312 -3.9413698 -3.9229922 -3.9562247 -4.0174651 -4.0887165 -4.149992][-4.2022405 -4.1988211 -4.2120938 -4.232739 -4.2353907 -4.2086015 -4.15137 -4.0799665 -4.0114775 -3.9448125 -3.905931 -3.9141593 -3.964081 -4.0492072 -4.127871][-4.227612 -4.2214465 -4.22985 -4.2438035 -4.24986 -4.2351384 -4.1934395 -4.1343074 -4.07394 -4.01195 -3.9624341 -3.9405832 -3.9590194 -4.031673 -4.1131544][-4.2620029 -4.25576 -4.2589045 -4.2657709 -4.2694902 -4.2631536 -4.2391915 -4.199791 -4.15392 -4.1010885 -4.0484037 -4.010726 -4.0011559 -4.0443168 -4.1081176][-4.2905126 -4.2900314 -4.293788 -4.2984352 -4.3002939 -4.2981577 -4.2866712 -4.2627487 -4.2305727 -4.1861315 -4.1354461 -4.0899649 -4.0607629 -4.0729384 -4.1091604][-4.29725 -4.3052378 -4.3140135 -4.3210149 -4.3246422 -4.3263412 -4.32252 -4.3065538 -4.2829728 -4.24683 -4.2026949 -4.1585159 -4.1224723 -4.1137657 -4.1201415][-4.2806726 -4.2955084 -4.3072386 -4.3170323 -4.3263431 -4.3350573 -4.3380141 -4.3309808 -4.3143015 -4.2851534 -4.2497835 -4.2126632 -4.1797123 -4.1576056 -4.1377687][-4.27353 -4.2915578 -4.3045106 -4.313055 -4.322464 -4.3358665 -4.3437996 -4.3433838 -4.3336692 -4.3125124 -4.2860088 -4.2579308 -4.2266474 -4.1968946 -4.16039][-4.2836132 -4.3017855 -4.3137126 -4.3171597 -4.3197241 -4.3283515 -4.338429 -4.3414516 -4.3358293 -4.3249469 -4.3089309 -4.2907281 -4.265276 -4.2335267 -4.1884742][-4.2922392 -4.308888 -4.3166947 -4.31459 -4.3098392 -4.3117523 -4.32179 -4.32755 -4.325264 -4.32203 -4.3130293 -4.3018169 -4.2853308 -4.2570271 -4.21031]]...]
INFO - root - 2017-12-06 04:44:28.284465: step 310, loss = 2.07, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:10s remains)
INFO - root - 2017-12-06 04:44:30.424995: step 320, loss = 2.04, batch loss = 1.99 (38.0 examples/sec; 0.211 sec/batch; 19h:25m:32s remains)
INFO - root - 2017-12-06 04:44:32.623949: step 330, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:17s remains)
INFO - root - 2017-12-06 04:44:34.813663: step 340, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:12s remains)
INFO - root - 2017-12-06 04:44:36.971802: step 350, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:12s remains)
INFO - root - 2017-12-06 04:44:39.074139: step 360, loss = 2.08, batch loss = 2.02 (38.2 examples/sec; 0.209 sec/batch; 19h:19m:34s remains)
INFO - root - 2017-12-06 04:44:41.192983: step 370, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:21s remains)
INFO - root - 2017-12-06 04:44:43.396788: step 380, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:29s remains)
INFO - root - 2017-12-06 04:44:45.579380: step 390, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:41s remains)
INFO - root - 2017-12-06 04:44:47.784857: step 400, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 20h:31m:30s remains)
2017-12-06 04:44:48.904217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2582808 -4.2478361 -4.2526631 -4.2645407 -4.2899427 -4.3030257 -4.2987461 -4.2910814 -4.2796912 -4.2749877 -4.2672114 -4.2554569 -4.2508073 -4.2509255 -4.2491097][-4.2609096 -4.2492471 -4.2493281 -4.2602153 -4.2858086 -4.2959433 -4.2859149 -4.2761722 -4.2620726 -4.2582545 -4.2512407 -4.2394013 -4.2306542 -4.2231197 -4.2193294][-4.2416282 -4.2205033 -4.2141638 -4.2255363 -4.2522407 -4.2580285 -4.2444968 -4.2376165 -4.2294841 -4.2316241 -4.2308774 -4.2243233 -4.2153206 -4.2017875 -4.1946111][-4.2141957 -4.1781917 -4.1594524 -4.16707 -4.1933055 -4.1971426 -4.1847339 -4.1842895 -4.1853271 -4.1948748 -4.2025661 -4.2054925 -4.2005959 -4.1874404 -4.1780519][-4.2118196 -4.1653976 -4.1301928 -4.1237946 -4.135324 -4.1271729 -4.1129789 -4.1185803 -4.1346927 -4.1563182 -4.1747875 -4.1865687 -4.1880274 -4.18082 -4.1723056][-4.2327261 -4.1916175 -4.1511583 -4.13296 -4.129333 -4.1022091 -4.0690446 -4.0625525 -4.0845833 -4.1183896 -4.1478467 -4.1690645 -4.1778121 -4.1790309 -4.1762419][-4.2463984 -4.2157674 -4.1820164 -4.1694827 -4.1741972 -4.1482549 -4.0987349 -4.066525 -4.0732241 -4.098537 -4.1259565 -4.1504078 -4.1661024 -4.1765628 -4.1827693][-4.230094 -4.2026615 -4.176651 -4.1788678 -4.2063966 -4.2002306 -4.1594257 -4.1205568 -4.1079 -4.1109967 -4.1229587 -4.1412597 -4.1557627 -4.1701183 -4.1846385][-4.1908889 -4.160357 -4.1414924 -4.1601319 -4.2083635 -4.2237725 -4.2014923 -4.1766434 -4.1620216 -4.15119 -4.146966 -4.1521873 -4.1587753 -4.171391 -4.1897626][-4.1598725 -4.1237078 -4.1064544 -4.1357527 -4.1957412 -4.2251987 -4.2180138 -4.2067142 -4.1997442 -4.1924748 -4.187325 -4.1857 -4.1847296 -4.1916175 -4.2057009][-4.1602015 -4.1189733 -4.0961895 -4.12293 -4.181951 -4.2173142 -4.222846 -4.2216287 -4.221837 -4.2213216 -4.2226048 -4.2224531 -4.2201567 -4.2214875 -4.2278342][-4.196075 -4.157908 -4.1303945 -4.142848 -4.1853056 -4.212955 -4.2224584 -4.2269125 -4.2316704 -4.2371407 -4.2437572 -4.2486663 -4.2496 -4.24965 -4.2508025][-4.2405605 -4.2124629 -4.186491 -4.1867242 -4.2093911 -4.2244258 -4.2319393 -4.2382922 -4.246026 -4.2536407 -4.2620497 -4.2697806 -4.2728815 -4.2731729 -4.272953][-4.2731905 -4.254643 -4.2357121 -4.2326374 -4.2450972 -4.2524681 -4.2553182 -4.2595816 -4.2654343 -4.2716055 -4.277657 -4.2830949 -4.2856665 -4.286437 -4.2885756][-4.2926621 -4.2821832 -4.2712212 -4.2680874 -4.2729173 -4.2745032 -4.2754207 -4.2791195 -4.2831354 -4.2878966 -4.2921767 -4.2948031 -4.2966342 -4.2982354 -4.3015938]]...]
INFO - root - 2017-12-06 04:44:51.072748: step 410, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:17s remains)
INFO - root - 2017-12-06 04:44:53.269804: step 420, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:32s remains)
INFO - root - 2017-12-06 04:44:55.442060: step 430, loss = 2.08, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:52s remains)
INFO - root - 2017-12-06 04:44:57.600180: step 440, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:06s remains)
INFO - root - 2017-12-06 04:44:59.768466: step 450, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:01s remains)
INFO - root - 2017-12-06 04:45:01.952938: step 460, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:57s remains)
INFO - root - 2017-12-06 04:45:04.115849: step 470, loss = 2.05, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:17s remains)
INFO - root - 2017-12-06 04:45:06.264921: step 480, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:35s remains)
INFO - root - 2017-12-06 04:45:08.446288: step 490, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:40s remains)
INFO - root - 2017-12-06 04:45:10.689262: step 500, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:28s remains)
2017-12-06 04:45:14.906054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2518406 -4.2406363 -4.2342095 -4.226069 -4.2162085 -4.2170973 -4.2205667 -4.2206879 -4.2188382 -4.207695 -4.191751 -4.1686497 -4.1435089 -4.1191182 -4.0984173][-4.2202663 -4.2072382 -4.197475 -4.1854849 -4.1735311 -4.1729274 -4.1764007 -4.1845245 -4.1990671 -4.1971917 -4.1811752 -4.15202 -4.11801 -4.0918946 -4.0722613][-4.1866469 -4.1704497 -4.1522946 -4.1329865 -4.1166587 -4.1180148 -4.1301136 -4.1552668 -4.1853032 -4.1860733 -4.159564 -4.1224771 -4.085309 -4.061511 -4.0475812][-4.1673441 -4.1454568 -4.1134849 -4.0823493 -4.0605969 -4.0669556 -4.0944271 -4.1370363 -4.1754279 -4.1746945 -4.139184 -4.0972271 -4.062068 -4.0405803 -4.0346212][-4.1446362 -4.1161256 -4.0736074 -4.0353403 -4.0124288 -4.023325 -4.05661 -4.106946 -4.1498728 -4.1460848 -4.1084471 -4.0709815 -4.0428548 -4.0310092 -4.04075][-4.1230769 -4.0926337 -4.0480933 -4.0074944 -3.9782567 -3.9784584 -4.0026674 -4.0475917 -4.0917029 -4.0887327 -4.0538406 -4.0278678 -4.0193763 -4.0277076 -4.0517545][-4.1174417 -4.093358 -4.0528307 -4.0106292 -3.9612665 -3.927793 -3.9170136 -3.9388227 -3.9834566 -3.9959507 -3.9766614 -3.976645 -3.9928164 -4.0184507 -4.0523467][-4.1087103 -4.0953479 -4.0681653 -4.0289359 -3.9640374 -3.8901906 -3.8234866 -3.8107519 -3.8714919 -3.9215953 -3.9382257 -3.9676189 -3.9979811 -4.0244217 -4.0557051][-4.0934615 -4.0799007 -4.0595989 -4.0281744 -3.9685922 -3.8912258 -3.8037984 -3.7720227 -3.8388345 -3.9072657 -3.9482336 -3.9975631 -4.0332761 -4.051023 -4.0721149][-4.0805321 -4.0601387 -4.043664 -4.026917 -3.9917977 -3.9432228 -3.8813934 -3.8609114 -3.9071765 -3.9545772 -3.9936121 -4.0481324 -4.0854144 -4.0945988 -4.1000247][-4.0717611 -4.0600657 -4.0554647 -4.0566106 -4.0464859 -4.0240579 -3.9893074 -3.9791112 -3.999434 -4.01531 -4.0366712 -4.0815597 -4.1155605 -4.1185226 -4.1127071][-4.0908122 -4.0962925 -4.10133 -4.1049342 -4.1011672 -4.0896735 -4.0683489 -4.058836 -4.0634561 -4.0570588 -4.0583849 -4.0877695 -4.11727 -4.121881 -4.1186414][-4.1276183 -4.1384134 -4.1440778 -4.1401472 -4.1308079 -4.1211009 -4.1031628 -4.0902591 -4.0852776 -4.0739412 -4.0696964 -4.0908723 -4.1177306 -4.1300063 -4.1374655][-4.1540208 -4.1619277 -4.1647973 -4.1545887 -4.1411386 -4.1313257 -4.1167688 -4.1056838 -4.10231 -4.0979223 -4.0991926 -4.1127591 -4.1279531 -4.1420383 -4.156455][-4.1624951 -4.1686587 -4.1720276 -4.1616554 -4.1490507 -4.140008 -4.1291995 -4.1268229 -4.1327558 -4.1368251 -4.1396022 -4.1384273 -4.1393719 -4.1492376 -4.162859]]...]
INFO - root - 2017-12-06 04:45:17.200841: step 510, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:07s remains)
INFO - root - 2017-12-06 04:45:19.364239: step 520, loss = 2.08, batch loss = 2.02 (38.2 examples/sec; 0.210 sec/batch; 19h:19m:39s remains)
INFO - root - 2017-12-06 04:45:21.565690: step 530, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:59s remains)
INFO - root - 2017-12-06 04:45:23.736306: step 540, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:06s remains)
INFO - root - 2017-12-06 04:45:25.935535: step 550, loss = 2.06, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:59s remains)
INFO - root - 2017-12-06 04:45:28.123947: step 560, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:35s remains)
INFO - root - 2017-12-06 04:45:30.317703: step 570, loss = 2.06, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:00s remains)
INFO - root - 2017-12-06 04:45:32.470209: step 580, loss = 2.07, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:41s remains)
INFO - root - 2017-12-06 04:45:34.623776: step 590, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:58s remains)
INFO - root - 2017-12-06 04:45:36.829285: step 600, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 21h:30m:20s remains)
2017-12-06 04:45:39.278810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3347034 -4.3180304 -4.2987056 -4.2831359 -4.2733097 -4.272645 -4.2794728 -4.2862539 -4.2927017 -4.2999043 -4.3077841 -4.3158851 -4.3258944 -4.3382092 -4.348402][-4.3357077 -4.316391 -4.292768 -4.2714572 -4.2548766 -4.2502255 -4.2541504 -4.2602892 -4.268054 -4.2784071 -4.2890387 -4.2995825 -4.3115654 -4.3277059 -4.3430161][-4.329073 -4.3068428 -4.2775493 -4.2485271 -4.221714 -4.2071314 -4.2047806 -4.2111673 -4.226368 -4.2441912 -4.2637653 -4.2833686 -4.2996149 -4.3173232 -4.3356][-4.3201294 -4.2925153 -4.2547255 -4.2176991 -4.181242 -4.153645 -4.1376538 -4.1371713 -4.1604595 -4.1881814 -4.2195411 -4.2555537 -4.2837658 -4.3064122 -4.3269491][-4.3123622 -4.2773995 -4.2275333 -4.1797581 -4.1273084 -4.0760365 -4.0368829 -4.0277624 -4.0639505 -4.1103835 -4.1587543 -4.2144809 -4.2596722 -4.29033 -4.3155732][-4.2988105 -4.2510824 -4.1860638 -4.12225 -4.0402517 -3.9502649 -3.8785777 -3.8669465 -3.9267924 -4.0005441 -4.0727358 -4.1524873 -4.2199965 -4.2639871 -4.2980804][-4.2895503 -4.2317514 -4.1495514 -4.0640488 -3.9494526 -3.8154576 -3.7052081 -3.6886613 -3.7756844 -3.881671 -3.9800949 -4.0863147 -4.1794977 -4.2403522 -4.2839122][-4.2968655 -4.2426143 -4.1619205 -4.0696115 -3.9470689 -3.7925916 -3.6535721 -3.6274939 -3.7199731 -3.8382194 -3.9478078 -4.0640621 -4.1696029 -4.2364368 -4.2813897][-4.3124681 -4.2756948 -4.2175241 -4.1472578 -4.048068 -3.9151945 -3.7942998 -3.7679687 -3.8372524 -3.9307175 -4.0187826 -4.1142955 -4.2070465 -4.2634106 -4.2980371][-4.3296523 -4.3168058 -4.2898216 -4.250833 -4.1875925 -4.0936937 -4.0065904 -3.9794133 -4.0148025 -4.072608 -4.1334953 -4.2009363 -4.2662096 -4.3042121 -4.3236957][-4.3446555 -4.3465385 -4.34077 -4.3264227 -4.2925873 -4.2369375 -4.186223 -4.1681733 -4.1823707 -4.2109585 -4.2440557 -4.2836652 -4.3201656 -4.3387971 -4.3442903][-4.3511324 -4.3563957 -4.3591723 -4.356308 -4.340642 -4.3134856 -4.2910028 -4.2825265 -4.2865152 -4.2973728 -4.3118248 -4.3309751 -4.3493481 -4.3570304 -4.3561521][-4.3494906 -4.3527789 -4.3571939 -4.3569546 -4.3501954 -4.3388567 -4.3312325 -4.329514 -4.3314943 -4.3340063 -4.3379087 -4.344852 -4.3531585 -4.3578057 -4.3582296][-4.349761 -4.348928 -4.3506818 -4.3502874 -4.3465204 -4.3416729 -4.3393707 -4.341455 -4.3443208 -4.3441529 -4.3423176 -4.3439307 -4.3483534 -4.3532553 -4.3560176][-4.3534946 -4.351604 -4.3509316 -4.3491473 -4.3459811 -4.3420568 -4.3398728 -4.3407073 -4.34242 -4.3420663 -4.3408012 -4.3427105 -4.347003 -4.352016 -4.3556538]]...]
INFO - root - 2017-12-06 04:45:41.460862: step 610, loss = 2.09, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:55s remains)
INFO - root - 2017-12-06 04:45:43.645784: step 620, loss = 2.05, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:45s remains)
INFO - root - 2017-12-06 04:45:45.796174: step 630, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:12s remains)
INFO - root - 2017-12-06 04:45:47.967796: step 640, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:25s remains)
INFO - root - 2017-12-06 04:45:50.165281: step 650, loss = 2.08, batch loss = 2.03 (32.7 examples/sec; 0.245 sec/batch; 22h:32m:37s remains)
INFO - root - 2017-12-06 04:45:52.294740: step 660, loss = 2.05, batch loss = 2.00 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:54s remains)
INFO - root - 2017-12-06 04:45:54.403613: step 670, loss = 2.09, batch loss = 2.03 (38.7 examples/sec; 0.207 sec/batch; 19h:03m:57s remains)
INFO - root - 2017-12-06 04:45:56.555851: step 680, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:15s remains)
INFO - root - 2017-12-06 04:45:58.745971: step 690, loss = 2.07, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:25s remains)
INFO - root - 2017-12-06 04:46:00.931063: step 700, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:36s remains)
2017-12-06 04:46:01.378249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20682 -4.2005987 -4.1970229 -4.1942482 -4.1923361 -4.1900177 -4.1863656 -4.1820483 -4.1801348 -4.1811986 -4.181654 -4.1765351 -4.1643548 -4.1484337 -4.1416335][-4.2208767 -4.2131166 -4.2076926 -4.2020926 -4.1963291 -4.1896281 -4.1814642 -4.1721807 -4.1662579 -4.1645675 -4.1631703 -4.1565566 -4.1453876 -4.1313858 -4.1268125][-4.2400403 -4.2331262 -4.2282429 -4.2227354 -4.2169871 -4.2097263 -4.1993642 -4.1872225 -4.1788359 -4.1752043 -4.17162 -4.16451 -4.1560173 -4.1452694 -4.1427493][-4.2342925 -4.2282882 -4.2245746 -4.2225008 -4.2208982 -4.2167969 -4.2075963 -4.1975918 -4.192843 -4.1938667 -4.1933303 -4.1883726 -4.1814961 -4.1705503 -4.1661878][-4.2057776 -4.1964526 -4.1891003 -4.189395 -4.1930237 -4.192018 -4.1840615 -4.1778221 -4.1814141 -4.1938844 -4.2044525 -4.2063851 -4.2030659 -4.1908522 -4.1827693][-4.1731844 -4.155582 -4.1426196 -4.1437693 -4.1504469 -4.1487331 -4.1375294 -4.1321721 -4.1438775 -4.1697311 -4.1953049 -4.2096295 -4.2143755 -4.2047477 -4.1957588][-4.154501 -4.1332412 -4.1183357 -4.1193352 -4.1233115 -4.1140809 -4.0930781 -4.0817943 -4.0968413 -4.1324043 -4.1698971 -4.1967793 -4.2110252 -4.2074938 -4.200448][-4.1518393 -4.1362758 -4.1265025 -4.1292472 -4.1300907 -4.1153932 -4.0877366 -4.0681224 -4.0774946 -4.1112671 -4.1499043 -4.1811166 -4.1999025 -4.2012 -4.197031][-4.1598563 -4.1538129 -4.1512675 -4.1564322 -4.1576052 -4.1459484 -4.1249452 -4.1086626 -4.1132274 -4.1364493 -4.1656055 -4.18896 -4.2019577 -4.1996441 -4.1932468][-4.1814823 -4.1800675 -4.1791606 -4.1816034 -4.1797094 -4.1697216 -4.1562557 -4.1481509 -4.1549664 -4.1736 -4.1963749 -4.2122993 -4.2168608 -4.2066383 -4.1943326][-4.2125797 -4.2094507 -4.2043972 -4.2000928 -4.1921473 -4.17942 -4.167357 -4.1647892 -4.1752582 -4.1934562 -4.2135334 -4.22664 -4.2276163 -4.2126918 -4.1970706][-4.2367797 -4.2317209 -4.2248139 -4.2181387 -4.2095942 -4.196981 -4.1847038 -4.181704 -4.190043 -4.2043257 -4.2190366 -4.2282009 -4.227344 -4.2132258 -4.2000718][-4.2445807 -4.2387743 -4.2331414 -4.2292442 -4.2249355 -4.217422 -4.2081385 -4.2044444 -4.2083116 -4.2159872 -4.22435 -4.228579 -4.2245827 -4.2117128 -4.2029][-4.2424426 -4.2365289 -4.2307019 -4.227704 -4.225553 -4.2228231 -4.2194057 -4.2181258 -4.2201567 -4.2233877 -4.2269716 -4.2286468 -4.2236433 -4.2125244 -4.207232][-4.233458 -4.2273412 -4.2210169 -4.2179837 -4.2175345 -4.217845 -4.2183595 -4.219059 -4.2201028 -4.22057 -4.2212567 -4.2216225 -4.21713 -4.2088728 -4.2059894]]...]
INFO - root - 2017-12-06 04:46:03.548258: step 710, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:30s remains)
INFO - root - 2017-12-06 04:46:05.728838: step 720, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 21h:08m:39s remains)
INFO - root - 2017-12-06 04:46:07.936242: step 730, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:53s remains)
INFO - root - 2017-12-06 04:46:10.190392: step 740, loss = 2.07, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:14s remains)
INFO - root - 2017-12-06 04:46:12.362591: step 750, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:04s remains)
INFO - root - 2017-12-06 04:46:14.554852: step 760, loss = 2.09, batch loss = 2.04 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:20s remains)
INFO - root - 2017-12-06 04:46:16.743254: step 770, loss = 2.05, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:32s remains)
INFO - root - 2017-12-06 04:46:18.923965: step 780, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:19s remains)
INFO - root - 2017-12-06 04:46:21.059830: step 790, loss = 2.06, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:59s remains)
INFO - root - 2017-12-06 04:46:23.143705: step 800, loss = 2.09, batch loss = 2.03 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:18s remains)
2017-12-06 04:46:23.738063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2801361 -4.2688117 -4.2617536 -4.2550173 -4.2346931 -4.2033534 -4.1782136 -4.1667337 -4.1678824 -4.1732965 -4.1811495 -4.1843815 -4.1863866 -4.1791873 -4.1653385][-4.28311 -4.271915 -4.2647948 -4.2582922 -4.2385178 -4.2075925 -4.1773663 -4.1573954 -4.1499243 -4.14938 -4.1537309 -4.1574378 -4.1626968 -4.1600976 -4.1512055][-4.2643309 -4.2518916 -4.2440643 -4.2363067 -4.2155914 -4.1862073 -4.1559339 -4.1336355 -4.1234407 -4.1215429 -4.1261463 -4.1324081 -4.1405754 -4.1415191 -4.1375809][-4.2273126 -4.2123303 -4.2020535 -4.1929445 -4.1733871 -4.1488791 -4.1233673 -4.1044378 -4.0943146 -4.0946527 -4.1032004 -4.1122193 -4.1194558 -4.1212983 -4.1218505][-4.1851797 -4.1671858 -4.154973 -4.1459937 -4.1298485 -4.1125355 -4.0939493 -4.0789871 -4.0710225 -4.0746017 -4.0895557 -4.1008306 -4.104383 -4.1060467 -4.1122646][-4.1431603 -4.1231942 -4.1077991 -4.0950885 -4.07867 -4.0634508 -4.0446548 -4.0283532 -4.0234852 -4.0384626 -4.0706105 -4.0908036 -4.0970364 -4.1005063 -4.10926][-4.1201696 -4.101686 -4.0811677 -4.0586925 -4.0342121 -4.0098696 -3.976115 -3.9433513 -3.9388611 -3.9721608 -4.0280719 -4.0620475 -4.0748181 -4.0765257 -4.0778275][-4.12083 -4.110415 -4.0909977 -4.0622764 -4.029706 -3.9940512 -3.9430647 -3.8896625 -3.8768744 -3.9183922 -3.9872806 -4.0278969 -4.0403895 -4.0366864 -4.02995][-4.1408091 -4.13882 -4.1239948 -4.0954041 -4.0634141 -4.0332661 -3.9922011 -3.9478943 -3.9335246 -3.9629896 -4.0182133 -4.0488873 -4.053309 -4.0449805 -4.0328984][-4.1768517 -4.1773505 -4.165801 -4.1408992 -4.1148167 -4.0940571 -4.06666 -4.035912 -4.0243144 -4.0421343 -4.0785818 -4.09781 -4.0961108 -4.0835247 -4.0683289][-4.2278318 -4.2260613 -4.2163053 -4.1956763 -4.17434 -4.1586418 -4.1392722 -4.1153727 -4.1046219 -4.1153383 -4.1376853 -4.1494646 -4.1464109 -4.1304326 -4.1105285][-4.2774439 -4.27473 -4.2671242 -4.2526503 -4.2375288 -4.2238278 -4.2084126 -4.1889315 -4.1794057 -4.1856995 -4.1994381 -4.2077537 -4.204082 -4.1877704 -4.167016][-4.3080564 -4.3072691 -4.3036342 -4.2955337 -4.2861376 -4.2760739 -4.2657375 -4.2531157 -4.247921 -4.2522368 -4.25893 -4.26302 -4.2593927 -4.2474527 -4.2322497][-4.3155007 -4.3161731 -4.3150697 -4.3108444 -4.3051467 -4.2995543 -4.2952905 -4.2899947 -4.2889433 -4.2920284 -4.2938771 -4.2931266 -4.28864 -4.2814651 -4.2724247][-4.3147097 -4.3155332 -4.3151441 -4.312037 -4.3080506 -4.3052716 -4.3045449 -4.3034763 -4.303905 -4.3050027 -4.3035984 -4.2996988 -4.2946525 -4.2902718 -4.284472]]...]
INFO - root - 2017-12-06 04:46:25.995622: step 810, loss = 2.05, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:54s remains)
INFO - root - 2017-12-06 04:46:28.198237: step 820, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.214 sec/batch; 19h:40m:45s remains)
INFO - root - 2017-12-06 04:46:30.363050: step 830, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:00s remains)
INFO - root - 2017-12-06 04:46:32.572019: step 840, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.226 sec/batch; 20h:51m:42s remains)
INFO - root - 2017-12-06 04:46:34.772659: step 850, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:15s remains)
INFO - root - 2017-12-06 04:46:36.969579: step 860, loss = 2.09, batch loss = 2.03 (33.8 examples/sec; 0.237 sec/batch; 21h:48m:43s remains)
INFO - root - 2017-12-06 04:46:39.184950: step 870, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:44s remains)
INFO - root - 2017-12-06 04:46:41.338530: step 880, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.211 sec/batch; 19h:28m:27s remains)
INFO - root - 2017-12-06 04:46:43.496949: step 890, loss = 2.09, batch loss = 2.04 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:01s remains)
INFO - root - 2017-12-06 04:46:45.681874: step 900, loss = 2.04, batch loss = 1.98 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:32s remains)
2017-12-06 04:46:49.893155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1186275 -4.1153035 -4.119029 -4.1074891 -4.0889177 -4.0974822 -4.1151037 -4.1195149 -4.1240654 -4.127481 -4.1170607 -4.121829 -4.1120219 -4.0885949 -4.0792255][-4.0968509 -4.1220441 -4.1406641 -4.1326065 -4.1032677 -4.0909638 -4.0987167 -4.1121149 -4.1237888 -4.130558 -4.1194267 -4.1189137 -4.108613 -4.0850182 -4.0776334][-4.0573778 -4.1220236 -4.164979 -4.1597347 -4.123528 -4.0930314 -4.0848846 -4.0969491 -4.1109014 -4.1267276 -4.1220183 -4.1233106 -4.1176219 -4.0940404 -4.0824442][-4.0477667 -4.1277637 -4.1847863 -4.1814194 -4.1390743 -4.0902395 -4.0600319 -4.0571814 -4.0732102 -4.1038752 -4.1141376 -4.1233397 -4.1295185 -4.1090527 -4.0902529][-4.0972576 -4.1559057 -4.2039566 -4.1950412 -4.1425314 -4.0818176 -4.0299878 -4.0032744 -4.0208492 -4.0719142 -4.1003389 -4.1189365 -4.1374388 -4.1231565 -4.0950613][-4.1533928 -4.1778078 -4.2027903 -4.1861992 -4.1253524 -4.0579023 -3.9923816 -3.9469321 -3.971997 -4.0414128 -4.0936236 -4.1193585 -4.1357279 -4.1229863 -4.0914049][-4.1835155 -4.1809521 -4.182519 -4.15511 -4.0866261 -4.0120287 -3.9315236 -3.8785803 -3.9320648 -4.0249262 -4.0935531 -4.1203303 -4.125505 -4.1032987 -4.0703993][-4.1953931 -4.1776242 -4.1621647 -4.1183548 -4.041316 -3.9627593 -3.8872442 -3.8472257 -3.9287386 -4.0404239 -4.1199274 -4.1428032 -4.13337 -4.0968943 -4.052803][-4.204071 -4.1844783 -4.1619453 -4.1139083 -4.0456705 -3.99113 -3.9506505 -3.9316397 -4.0091133 -4.1046562 -4.1693206 -4.1829958 -4.169426 -4.1245017 -4.074297][-4.2170858 -4.2084384 -4.1926513 -4.1537313 -4.1032948 -4.0720296 -4.0584373 -4.0517359 -4.1018643 -4.1610904 -4.1976986 -4.1987076 -4.1918011 -4.1565905 -4.1146021][-4.2202873 -4.2296929 -4.2250338 -4.2008767 -4.1672654 -4.151072 -4.1483231 -4.1406288 -4.16232 -4.1894903 -4.19783 -4.1863976 -4.1924653 -4.1752396 -4.1490116][-4.2141547 -4.2313519 -4.2376704 -4.2266283 -4.2058344 -4.2012725 -4.2042732 -4.1980019 -4.1987758 -4.2064323 -4.1959386 -4.1755009 -4.1809573 -4.1710844 -4.1467443][-4.2156334 -4.2263141 -4.2339263 -4.2314105 -4.2195191 -4.2174654 -4.22108 -4.2189021 -4.2165422 -4.2177739 -4.2041693 -4.1809554 -4.181005 -4.1702046 -4.1454477][-4.2290397 -4.2267323 -4.2286153 -4.2288547 -4.2250266 -4.2250428 -4.2293634 -4.2319388 -4.2310948 -4.2315311 -4.2262535 -4.2112784 -4.2121625 -4.1981473 -4.1712561][-4.24376 -4.2369728 -4.2378621 -4.2415652 -4.243763 -4.2459888 -4.2515993 -4.2525005 -4.2509656 -4.2461796 -4.2421093 -4.2336812 -4.2363563 -4.2225704 -4.1983471]]...]
INFO - root - 2017-12-06 04:46:52.155576: step 910, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:18s remains)
INFO - root - 2017-12-06 04:46:54.324733: step 920, loss = 2.10, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:02s remains)
INFO - root - 2017-12-06 04:46:56.501054: step 930, loss = 2.10, batch loss = 2.04 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:09s remains)
INFO - root - 2017-12-06 04:46:58.710886: step 940, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:40s remains)
INFO - root - 2017-12-06 04:47:00.889584: step 950, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:52s remains)
INFO - root - 2017-12-06 04:47:03.071968: step 960, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.214 sec/batch; 19h:39m:55s remains)
INFO - root - 2017-12-06 04:47:05.236342: step 970, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:47s remains)
INFO - root - 2017-12-06 04:47:07.437208: step 980, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.230 sec/batch; 21h:12m:35s remains)
INFO - root - 2017-12-06 04:47:09.647676: step 990, loss = 2.04, batch loss = 1.98 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:22s remains)
INFO - root - 2017-12-06 04:47:11.871996: step 1000, loss = 2.04, batch loss = 1.98 (34.3 examples/sec; 0.233 sec/batch; 21h:29m:54s remains)
2017-12-06 04:47:12.205423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3043618 -4.29819 -4.289814 -4.28121 -4.2803569 -4.28685 -4.2983694 -4.3088055 -4.3132887 -4.3115373 -4.3042173 -4.2913494 -4.2799168 -4.2769094 -4.2834949][-4.27319 -4.2634511 -4.252717 -4.23994 -4.2368674 -4.2484312 -4.2653294 -4.2763085 -4.2770767 -4.2686348 -4.2541938 -4.2346749 -4.2217546 -4.2213306 -4.2340403][-4.2466469 -4.2345071 -4.2229013 -4.2089272 -4.2035875 -4.2160554 -4.2325239 -4.2404346 -4.2374277 -4.22664 -4.2088032 -4.1834145 -4.1693296 -4.1725159 -4.1902523][-4.2383895 -4.22702 -4.2137032 -4.1971178 -4.1870465 -4.1931443 -4.2004838 -4.1997995 -4.1954842 -4.1937528 -4.1804395 -4.152586 -4.1379061 -4.1436572 -4.16296][-4.2375126 -4.2258024 -4.2086062 -4.1892867 -4.1766381 -4.1708031 -4.1563334 -4.1381221 -4.1344428 -4.1515751 -4.1537695 -4.1325927 -4.1202664 -4.1280003 -4.143889][-4.2258015 -4.2116528 -4.1878095 -4.166924 -4.1527176 -4.1311612 -4.0886025 -4.0477366 -4.0476665 -4.090404 -4.1172881 -4.1122494 -4.1097951 -4.1263323 -4.1420403][-4.2134514 -4.1968069 -4.1679621 -4.1431828 -4.1215234 -4.0815282 -4.0096607 -3.9397902 -3.9413352 -4.0142369 -4.0694723 -4.0841684 -4.0958834 -4.124167 -4.1467891][-4.2016392 -4.1833639 -4.1556268 -4.1344175 -4.1075096 -4.0538793 -3.9618874 -3.8702774 -3.8691182 -3.9535158 -4.0151567 -4.0348253 -4.0551133 -4.0925484 -4.1267376][-4.1843677 -4.1634674 -4.136641 -4.1153679 -4.0889106 -4.0381765 -3.9642313 -3.8919728 -3.8901079 -3.9456329 -3.9827812 -3.9899344 -4.0073009 -4.0465341 -4.0873127][-4.160563 -4.1383095 -4.1144443 -4.0919452 -4.0702114 -4.0357575 -3.994041 -3.9485977 -3.9455869 -3.9709668 -3.9828928 -3.9736729 -3.9831996 -4.0187745 -4.0587049][-4.1565595 -4.1332464 -4.1138067 -4.0969758 -4.0852342 -4.0681992 -4.055037 -4.0310879 -4.0288029 -4.0362463 -4.0269175 -3.9985771 -3.994688 -4.0220709 -4.05937][-4.1955957 -4.1747766 -4.1575332 -4.1472039 -4.1441803 -4.1397519 -4.141449 -4.1337547 -4.1386433 -4.13984 -4.1172848 -4.0777578 -4.0642009 -4.0842476 -4.115221][-4.2604752 -4.243258 -4.2265067 -4.2164583 -4.2141428 -4.2160244 -4.2250247 -4.2275548 -4.2348719 -4.235033 -4.2117934 -4.17284 -4.1554189 -4.1703887 -4.1946917][-4.3144279 -4.3007035 -4.2871823 -4.2793856 -4.2782817 -4.2836018 -4.2929296 -4.2966323 -4.3014221 -4.3006725 -4.2833858 -4.2524815 -4.2355356 -4.2449412 -4.2611547][-4.3411036 -4.3330965 -4.3232718 -4.3171568 -4.3169589 -4.3216152 -4.3274179 -4.3298435 -4.3324161 -4.3319378 -4.3226452 -4.3038683 -4.2926283 -4.2980747 -4.3077068]]...]
INFO - root - 2017-12-06 04:47:14.444318: step 1010, loss = 2.05, batch loss = 2.00 (35.6 examples/sec; 0.224 sec/batch; 20h:39m:56s remains)
INFO - root - 2017-12-06 04:47:16.636582: step 1020, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:46s remains)
INFO - root - 2017-12-06 04:47:18.830598: step 1030, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:41s remains)
INFO - root - 2017-12-06 04:47:21.066561: step 1040, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 20h:51m:12s remains)
INFO - root - 2017-12-06 04:47:23.232151: step 1050, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:54s remains)
INFO - root - 2017-12-06 04:47:25.370381: step 1060, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:14s remains)
INFO - root - 2017-12-06 04:47:27.516631: step 1070, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:38s remains)
INFO - root - 2017-12-06 04:47:29.702730: step 1080, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 21h:30m:18s remains)
INFO - root - 2017-12-06 04:47:31.881928: step 1090, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.209 sec/batch; 19h:11m:46s remains)
INFO - root - 2017-12-06 04:47:34.099944: step 1100, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:18s remains)
2017-12-06 04:47:34.514033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2032404 -4.2090473 -4.2087169 -4.2077103 -4.2048254 -4.2013741 -4.1979704 -4.1972561 -4.1987324 -4.2013545 -4.2048759 -4.2087822 -4.2134023 -4.2191548 -4.2242355][-4.2122164 -4.217566 -4.217855 -4.2158251 -4.2116375 -4.2073579 -4.2032666 -4.2006288 -4.1999092 -4.2029233 -4.2081513 -4.2137995 -4.2206616 -4.2282414 -4.2345705][-4.2190771 -4.2226372 -4.22244 -4.2188439 -4.2127471 -4.207725 -4.2033534 -4.1992722 -4.1967626 -4.2005758 -4.2073159 -4.2142568 -4.2224574 -4.2310557 -4.2375684][-4.2294178 -4.2290635 -4.2263231 -4.2199373 -4.210474 -4.20274 -4.1965046 -4.1898513 -4.186029 -4.1920524 -4.2002168 -4.2063465 -4.2140703 -4.2230458 -4.2301769][-4.2374783 -4.2296257 -4.2219062 -4.2105346 -4.1963029 -4.1858263 -4.1754112 -4.1634045 -4.1586361 -4.1685848 -4.1786189 -4.1836424 -4.1915402 -4.2013707 -4.2087531][-4.2337422 -4.2156944 -4.199759 -4.1825905 -4.1665487 -4.1562643 -4.139523 -4.1167254 -4.1085482 -4.1235375 -4.1385407 -4.1464214 -4.1587553 -4.1713767 -4.180419][-4.21427 -4.1847391 -4.1591597 -4.1384416 -4.1252189 -4.1186419 -4.0958982 -4.058619 -4.0447254 -4.0663276 -4.0914903 -4.1067305 -4.1266265 -4.1460118 -4.1606092][-4.1849418 -4.1474276 -4.1157341 -4.0963297 -4.091568 -4.0916862 -4.067225 -4.0214148 -4.0047607 -4.0317292 -4.0641966 -4.0845962 -4.1131258 -4.1415038 -4.1610584][-4.162818 -4.1285639 -4.1017475 -4.0876632 -4.089509 -4.0975604 -4.0822115 -4.0437894 -4.0263834 -4.0482426 -4.0744085 -4.0924568 -4.1233339 -4.1535182 -4.1718168][-4.1522431 -4.1297541 -4.1130462 -4.1040826 -4.1098561 -4.12582 -4.1243787 -4.0991187 -4.080533 -4.0915394 -4.1064482 -4.1164546 -4.1400127 -4.1657267 -4.1803293][-4.1515045 -4.1407194 -4.1332607 -4.1305723 -4.1399741 -4.1596603 -4.1673856 -4.1536522 -4.1352081 -4.1361294 -4.141192 -4.1421309 -4.1551437 -4.1734591 -4.184442][-4.1631975 -4.1617947 -4.1610303 -4.1630826 -4.173882 -4.1922054 -4.2036638 -4.1982694 -4.1833968 -4.1791968 -4.1779962 -4.1736054 -4.1765909 -4.1843038 -4.1901312][-4.1810737 -4.1849146 -4.1882811 -4.1929493 -4.2046313 -4.2201509 -4.2320323 -4.2305026 -4.2199283 -4.2125072 -4.2073374 -4.1995034 -4.1937881 -4.1921692 -4.1945176][-4.1949463 -4.1998854 -4.2049351 -4.2120156 -4.2231832 -4.2352514 -4.2435188 -4.2427497 -4.23491 -4.2264409 -4.2197046 -4.2121782 -4.2035594 -4.1972547 -4.1968021][-4.2018032 -4.2055893 -4.209691 -4.2163987 -4.2242904 -4.231884 -4.2360668 -4.2352982 -4.2302246 -4.223424 -4.2171645 -4.2114892 -4.2050023 -4.19961 -4.198102]]...]
INFO - root - 2017-12-06 04:47:36.673878: step 1110, loss = 2.04, batch loss = 1.98 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:17s remains)
INFO - root - 2017-12-06 04:47:38.839324: step 1120, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:20s remains)
INFO - root - 2017-12-06 04:47:41.033765: step 1130, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:48s remains)
INFO - root - 2017-12-06 04:47:43.242940: step 1140, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:33s remains)
INFO - root - 2017-12-06 04:47:45.443865: step 1150, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 21h:44m:08s remains)
INFO - root - 2017-12-06 04:47:47.626966: step 1160, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 20h:40m:40s remains)
INFO - root - 2017-12-06 04:47:49.833899: step 1170, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:35s remains)
INFO - root - 2017-12-06 04:47:52.074255: step 1180, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:38s remains)
INFO - root - 2017-12-06 04:47:54.258985: step 1190, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:47s remains)
INFO - root - 2017-12-06 04:47:56.444827: step 1200, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:30s remains)
2017-12-06 04:47:56.757307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626672 -4.25991 -4.2674003 -4.2746372 -4.2757163 -4.2719345 -4.2714849 -4.2797709 -4.29043 -4.2962656 -4.2931995 -4.2838411 -4.2769585 -4.2714214 -4.2660656][-4.2521567 -4.2421031 -4.2491355 -4.2657614 -4.2784963 -4.2810168 -4.2831397 -4.2950311 -4.3130951 -4.3225765 -4.3146467 -4.294446 -4.2764707 -4.2655306 -4.25729][-4.2067585 -4.194396 -4.2058558 -4.2373147 -4.2636318 -4.2731729 -4.2780762 -4.2934227 -4.3161993 -4.3296809 -4.3238835 -4.2982845 -4.2724681 -4.2545056 -4.240519][-4.1522751 -4.1518221 -4.1698523 -4.213089 -4.243825 -4.2511716 -4.247963 -4.2631822 -4.2917552 -4.3100295 -4.3122497 -4.295321 -4.2724667 -4.2521448 -4.2324204][-4.1352382 -4.1506777 -4.1761374 -4.2170911 -4.2358046 -4.2215767 -4.1961627 -4.2030087 -4.2367144 -4.2617488 -4.2783113 -4.2822318 -4.2750931 -4.2602463 -4.2386041][-4.1383739 -4.1664186 -4.1980853 -4.2239714 -4.2157612 -4.1698976 -4.1061077 -4.0898771 -4.1384463 -4.1886525 -4.2312441 -4.2592154 -4.2723141 -4.269649 -4.2526045][-4.1453404 -4.1754742 -4.1997361 -4.2063403 -4.1719518 -4.0905213 -3.974232 -3.9116743 -3.99045 -4.0947442 -4.1757727 -4.229877 -4.2617126 -4.2710338 -4.261704][-4.1638246 -4.1899257 -4.2042708 -4.1933427 -4.1403508 -4.03825 -3.8895173 -3.7867532 -3.8914316 -4.0400319 -4.1475992 -4.2168088 -4.2578578 -4.2746248 -4.2713971][-4.1756821 -4.1947207 -4.2059531 -4.1920753 -4.1446195 -4.0616817 -3.9526331 -3.8809161 -3.955708 -4.0756636 -4.1674066 -4.229722 -4.2666168 -4.2845917 -4.2861652][-4.197391 -4.2093577 -4.2194238 -4.2093358 -4.1816821 -4.1346474 -4.080399 -4.0496778 -4.0869679 -4.1536117 -4.2096581 -4.2458997 -4.2705121 -4.2870359 -4.295814][-4.2358241 -4.241725 -4.2467031 -4.2417541 -4.22838 -4.2053008 -4.187942 -4.1840615 -4.2036486 -4.2319193 -4.2546272 -4.26417 -4.271358 -4.2799234 -4.2922406][-4.2771959 -4.2762389 -4.2737012 -4.2669659 -4.2602792 -4.252111 -4.2537026 -4.2640796 -4.2743249 -4.2765312 -4.2753882 -4.2707028 -4.2667503 -4.2696381 -4.282527][-4.3071589 -4.3004756 -4.2934351 -4.2869306 -4.2813649 -4.2781029 -4.2869539 -4.2957973 -4.2923183 -4.2773561 -4.2653975 -4.2577167 -4.2555728 -4.2588325 -4.2675252][-4.3107076 -4.3014231 -4.2951384 -4.291997 -4.2857733 -4.2820106 -4.289464 -4.2929554 -4.2803168 -4.2579107 -4.2423878 -4.2352982 -4.2370844 -4.24722 -4.2594147][-4.3005133 -4.2925205 -4.2896914 -4.2887006 -4.2837052 -4.2826519 -4.2862682 -4.2844448 -4.2693157 -4.24826 -4.2357373 -4.2314358 -4.2338362 -4.2491541 -4.2683311]]...]
INFO - root - 2017-12-06 04:47:59.044578: step 1210, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:32s remains)
INFO - root - 2017-12-06 04:48:01.210082: step 1220, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:41s remains)
INFO - root - 2017-12-06 04:48:03.446389: step 1230, loss = 2.06, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:25s remains)
INFO - root - 2017-12-06 04:48:05.629122: step 1240, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.211 sec/batch; 19h:27m:18s remains)
INFO - root - 2017-12-06 04:48:07.816915: step 1250, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.215 sec/batch; 19h:49m:44s remains)
INFO - root - 2017-12-06 04:48:10.156327: step 1260, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:55s remains)
INFO - root - 2017-12-06 04:48:12.371828: step 1270, loss = 2.08, batch loss = 2.03 (35.5 examples/sec; 0.225 sec/batch; 20h:43m:21s remains)
INFO - root - 2017-12-06 04:48:14.580433: step 1280, loss = 2.06, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:11s remains)
INFO - root - 2017-12-06 04:48:16.762298: step 1290, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:46s remains)
INFO - root - 2017-12-06 04:48:18.949255: step 1300, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:32s remains)
2017-12-06 04:48:19.384969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.217495 -4.22799 -4.2182994 -4.2046909 -4.1934357 -4.1645155 -4.1334605 -4.13775 -4.1491871 -4.16638 -4.1911712 -4.1938128 -4.2028732 -4.2058725 -4.1871991][-4.2161937 -4.2331867 -4.2293372 -4.2139869 -4.2040167 -4.1754336 -4.1319222 -4.130662 -4.1493006 -4.1667919 -4.1923513 -4.2047172 -4.2172413 -4.2210851 -4.2097425][-4.2117271 -4.2297635 -4.2270532 -4.206902 -4.1992598 -4.1777859 -4.1312628 -4.1244717 -4.1515512 -4.1741934 -4.1986451 -4.2205858 -4.236012 -4.2392726 -4.232132][-4.2098866 -4.2237496 -4.2213712 -4.1940012 -4.1784124 -4.1572871 -4.1140456 -4.1005177 -4.1293235 -4.162118 -4.1908455 -4.2169189 -4.2351379 -4.2358828 -4.2289958][-4.1842613 -4.183567 -4.1738987 -4.1400185 -4.1177816 -4.0968342 -4.0594192 -4.0386291 -4.0617108 -4.1053667 -4.1408191 -4.1752458 -4.2046471 -4.2068038 -4.200088][-4.12357 -4.0990605 -4.0772548 -4.0421176 -4.0210853 -4.0066223 -3.973983 -3.9395416 -3.9489903 -4.00033 -4.0461059 -4.09082 -4.1349645 -4.1480956 -4.1461558][-4.07607 -4.0323796 -4.0021925 -3.9695177 -3.9490724 -3.9316244 -3.8929856 -3.8346586 -3.8255858 -3.8884616 -3.9530685 -4.0054331 -4.0550952 -4.0779352 -4.0803771][-4.0664363 -4.0199261 -3.9871051 -3.9562809 -3.9326668 -3.9054508 -3.8621101 -3.798759 -3.7764554 -3.8370626 -3.9105532 -3.957588 -3.9935632 -4.0107946 -4.0133772][-4.0800295 -4.0384269 -4.0060697 -3.9758286 -3.9507623 -3.9228575 -3.8881822 -3.8467512 -3.831444 -3.872412 -3.9308627 -3.9668579 -3.9868703 -3.9947591 -3.993346][-4.0981169 -4.064858 -4.0380745 -4.0126739 -3.9903958 -3.9691806 -3.9471195 -3.9258089 -3.9185891 -3.9417996 -3.9795079 -4.0020304 -4.010231 -4.0148425 -4.01512][-4.1240311 -4.1007428 -4.082757 -4.0662942 -4.0516715 -4.040288 -4.0283403 -4.0151482 -4.0093312 -4.01476 -4.0295191 -4.0376592 -4.0358825 -4.0355015 -4.03816][-4.1474519 -4.1298523 -4.1186285 -4.1093168 -4.1030889 -4.1017413 -4.0987363 -4.0893235 -4.07897 -4.0704823 -4.0719056 -4.0736241 -4.0665 -4.063592 -4.0664392][-4.1619339 -4.1439843 -4.1297355 -4.1217318 -4.1221671 -4.1301746 -4.1343894 -4.1289964 -4.1199155 -4.1085095 -4.1063805 -4.1105332 -4.1049838 -4.1032085 -4.1069384][-4.1739855 -4.15396 -4.1355963 -4.1250267 -4.1252251 -4.1359377 -4.1435609 -4.1411381 -4.1344905 -4.1256046 -4.1243548 -4.1297908 -4.1269226 -4.1272936 -4.134336][-4.1974807 -4.176847 -4.1542106 -4.1374269 -4.1340389 -4.1434922 -4.150887 -4.1492767 -4.1442733 -4.1384058 -4.1357012 -4.1377459 -4.135159 -4.13707 -4.1474423]]...]
INFO - root - 2017-12-06 04:48:21.553626: step 1310, loss = 2.08, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:17s remains)
INFO - root - 2017-12-06 04:48:23.726427: step 1320, loss = 2.06, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:29m:40s remains)
INFO - root - 2017-12-06 04:48:25.885400: step 1330, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:02s remains)
INFO - root - 2017-12-06 04:48:28.028911: step 1340, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:09s remains)
INFO - root - 2017-12-06 04:48:30.261190: step 1350, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.228 sec/batch; 20h:59m:47s remains)
INFO - root - 2017-12-06 04:48:32.447010: step 1360, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 21h:02m:07s remains)
INFO - root - 2017-12-06 04:48:34.667246: step 1370, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.218 sec/batch; 20h:05m:10s remains)
INFO - root - 2017-12-06 04:48:36.868411: step 1380, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:42s remains)
INFO - root - 2017-12-06 04:48:39.158391: step 1390, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.228 sec/batch; 20h:55m:33s remains)
INFO - root - 2017-12-06 04:48:41.357539: step 1400, loss = 2.05, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:08s remains)
2017-12-06 04:48:45.083400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1984653 -4.2060957 -4.2085361 -4.2007256 -4.1926322 -4.18617 -4.1840143 -4.1794047 -4.1726065 -4.1686378 -4.1714826 -4.176661 -4.1796317 -4.1826906 -4.1934657][-4.185925 -4.1905565 -4.1930923 -4.1861014 -4.1783023 -4.1747131 -4.1747036 -4.1705484 -4.1663976 -4.16709 -4.1756668 -4.1821427 -4.1851916 -4.1884413 -4.1959014][-4.1762204 -4.1754503 -4.1755447 -4.170177 -4.1630306 -4.1577682 -4.1576581 -4.1531248 -4.1507111 -4.1550326 -4.1699142 -4.1819773 -4.1884866 -4.1918631 -4.1985669][-4.1517105 -4.1434736 -4.1407 -4.1390786 -4.1347761 -4.128222 -4.1274467 -4.1240654 -4.1260448 -4.1341109 -4.15289 -4.1689186 -4.1768107 -4.1790547 -4.1848679][-4.1314745 -4.1121573 -4.1018705 -4.0972033 -4.0887403 -4.0755877 -4.0705981 -4.069006 -4.0817261 -4.1007752 -4.1246605 -4.1475325 -4.160049 -4.1627488 -4.1646957][-4.135499 -4.102715 -4.0790782 -4.0616932 -4.0396724 -4.0127292 -4.0025759 -4.0070872 -4.0348067 -4.0729184 -4.107604 -4.1373372 -4.1547456 -4.160223 -4.16][-4.17165 -4.1275229 -4.0890379 -4.0571513 -4.0176039 -3.9685125 -3.9473307 -3.9536724 -3.9931388 -4.0528154 -4.1055956 -4.1497865 -4.17824 -4.1924138 -4.1940117][-4.208107 -4.1639018 -4.1243711 -4.089437 -4.0462079 -3.9911137 -3.9654753 -3.9692683 -4.0045109 -4.0683942 -4.1243405 -4.173058 -4.2093487 -4.2328291 -4.2403803][-4.2316418 -4.195169 -4.1647735 -4.138382 -4.1069975 -4.0658569 -4.0502782 -4.0552711 -4.0798321 -4.1253676 -4.1623626 -4.1941662 -4.2202129 -4.2402959 -4.2496228][-4.2494626 -4.2195048 -4.1969934 -4.1773481 -4.1587982 -4.1367736 -4.1325164 -4.1387138 -4.1511269 -4.1759038 -4.1939688 -4.2042122 -4.2104082 -4.2164884 -4.2229452][-4.2621646 -4.2378731 -4.2178941 -4.1998096 -4.1859255 -4.1722574 -4.1666827 -4.1667295 -4.16874 -4.1809549 -4.1879983 -4.1874056 -4.1837244 -4.1841264 -4.1912112][-4.2660832 -4.2536607 -4.2405925 -4.2263169 -4.2139125 -4.1975722 -4.1801729 -4.1643143 -4.1536627 -4.155767 -4.1581373 -4.1612725 -4.165421 -4.1742334 -4.1892748][-4.2560244 -4.2594256 -4.2574706 -4.2516308 -4.24368 -4.2268977 -4.2009277 -4.1733885 -4.1550875 -4.1491423 -4.147953 -4.15715 -4.1731429 -4.1944189 -4.218514][-4.2289495 -4.2451596 -4.2521663 -4.2538929 -4.2522216 -4.2436175 -4.2242923 -4.2003112 -4.1810169 -4.1696415 -4.1653724 -4.1760697 -4.1958017 -4.220294 -4.2472968][-4.1886363 -4.2100921 -4.2228384 -4.2328143 -4.2385578 -4.2388573 -4.2309389 -4.2179532 -4.2044721 -4.1952257 -4.1933413 -4.2049012 -4.2209797 -4.2383413 -4.2560883]]...]
INFO - root - 2017-12-06 04:48:47.358933: step 1410, loss = 2.08, batch loss = 2.02 (38.6 examples/sec; 0.207 sec/batch; 19h:03m:20s remains)
INFO - root - 2017-12-06 04:48:49.520552: step 1420, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:39s remains)
INFO - root - 2017-12-06 04:48:51.680059: step 1430, loss = 2.08, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:39s remains)
INFO - root - 2017-12-06 04:48:53.878523: step 1440, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 20h:05m:29s remains)
INFO - root - 2017-12-06 04:48:56.038043: step 1450, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:50s remains)
INFO - root - 2017-12-06 04:48:58.250774: step 1460, loss = 2.10, batch loss = 2.04 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:53s remains)
INFO - root - 2017-12-06 04:49:00.427969: step 1470, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:53m:42s remains)
INFO - root - 2017-12-06 04:49:02.653056: step 1480, loss = 2.07, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 20h:45m:43s remains)
INFO - root - 2017-12-06 04:49:04.863827: step 1490, loss = 2.09, batch loss = 2.03 (34.9 examples/sec; 0.229 sec/batch; 21h:02m:51s remains)
INFO - root - 2017-12-06 04:49:07.048541: step 1500, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:05s remains)
2017-12-06 04:49:07.943513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219887 -4.2281837 -4.241622 -4.2514243 -4.2377672 -4.210052 -4.202848 -4.2084122 -4.2180324 -4.2183871 -4.202394 -4.196506 -4.2184315 -4.2413774 -4.2546287][-4.1502366 -4.1578741 -4.18687 -4.2161489 -4.2091289 -4.1752377 -4.1642346 -4.1760359 -4.187222 -4.18386 -4.1659517 -4.1608281 -4.1894426 -4.2205715 -4.238512][-4.0950484 -4.1014967 -4.143899 -4.1902347 -4.1887016 -4.1470642 -4.1270814 -4.1390829 -4.1541805 -4.152977 -4.1392179 -4.1351 -4.1664596 -4.2025919 -4.2257075][-4.0901518 -4.0907393 -4.1328516 -4.1815567 -4.1777067 -4.1287103 -4.0995345 -4.1064334 -4.1222625 -4.1311412 -4.1292357 -4.1287293 -4.16051 -4.1973357 -4.22425][-4.116879 -4.1103415 -4.1474395 -4.1899452 -4.1784697 -4.1218057 -4.0891862 -4.0965333 -4.1119885 -4.1280503 -4.1347184 -4.1388688 -4.1696453 -4.2019119 -4.228405][-4.1500382 -4.1386018 -4.170238 -4.2010059 -4.1776052 -4.1131206 -4.0768285 -4.0874572 -4.1067033 -4.1264009 -4.1341658 -4.1406369 -4.1726303 -4.2032428 -4.2314219][-4.1679335 -4.1591663 -4.182476 -4.1964693 -4.1636047 -4.0935836 -4.0501871 -4.057754 -4.0832982 -4.109036 -4.1154189 -4.1171427 -4.1480451 -4.1818361 -4.21692][-4.15892 -4.1522593 -4.1667843 -4.1662083 -4.1311722 -4.0631409 -4.0124736 -4.0099964 -4.0406632 -4.071322 -4.0712457 -4.0655293 -4.09736 -4.1435885 -4.1895118][-4.1320968 -4.1278033 -4.137732 -4.1328 -4.1032438 -4.0422854 -3.9820094 -3.9622571 -3.9903824 -4.0237327 -4.0156879 -3.9990022 -4.0270376 -4.0876112 -4.1478715][-4.1160827 -4.1152821 -4.1216755 -4.1168756 -4.0967727 -4.0467558 -3.9832945 -3.9504437 -3.9707136 -4.0030756 -3.9929974 -3.9711921 -3.992254 -4.0532851 -4.11779][-4.1064224 -4.109364 -4.1174536 -4.1153121 -4.1074715 -4.0796504 -4.0295939 -3.9923217 -3.9976449 -4.0195093 -4.0121107 -3.9923084 -4.005847 -4.0568771 -4.1112475][-4.1054845 -4.1146808 -4.1293144 -4.1321516 -4.1343441 -4.12768 -4.0990067 -4.0638766 -4.0525122 -4.0615506 -4.05602 -4.0403452 -4.0491118 -4.0871491 -4.1306939][-4.1307669 -4.1441526 -4.1653171 -4.1717958 -4.1753883 -4.1764889 -4.1632586 -4.1381774 -4.1195316 -4.1178627 -4.1134958 -4.1040926 -4.1100388 -4.1360111 -4.1733704][-4.1766262 -4.1897359 -4.2123866 -4.2216578 -4.2241812 -4.2262177 -4.2214575 -4.2100749 -4.1953869 -4.1885071 -4.1829638 -4.1763477 -4.1786985 -4.1938062 -4.2209725][-4.2302079 -4.23866 -4.2545309 -4.2638855 -4.2677155 -4.2702751 -4.2689338 -4.2660866 -4.2593946 -4.2546606 -4.2504125 -4.2467713 -4.2490907 -4.2571535 -4.2728229]]...]
INFO - root - 2017-12-06 04:49:10.119997: step 1510, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:46s remains)
INFO - root - 2017-12-06 04:49:12.327910: step 1520, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:48s remains)
INFO - root - 2017-12-06 04:49:14.517599: step 1530, loss = 2.06, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:21m:33s remains)
INFO - root - 2017-12-06 04:49:16.652535: step 1540, loss = 2.05, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:22m:24s remains)
INFO - root - 2017-12-06 04:49:18.813311: step 1550, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:09s remains)
INFO - root - 2017-12-06 04:49:20.992908: step 1560, loss = 2.06, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:49m:51s remains)
INFO - root - 2017-12-06 04:49:23.187504: step 1570, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:46m:36s remains)
INFO - root - 2017-12-06 04:49:25.342675: step 1580, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:16s remains)
INFO - root - 2017-12-06 04:49:27.531354: step 1590, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:27s remains)
INFO - root - 2017-12-06 04:49:29.784074: step 1600, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:15s remains)
2017-12-06 04:49:30.207132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3249516 -4.32493 -4.3247662 -4.326951 -4.3231678 -4.3166556 -4.3092494 -4.3072267 -4.3128047 -4.3199139 -4.3282328 -4.3376789 -4.3463058 -4.349462 -4.3476233][-4.3058014 -4.3041463 -4.3064871 -4.3104463 -4.3068476 -4.2995739 -4.2914753 -4.2904816 -4.2988997 -4.3092365 -4.3206429 -4.3312426 -4.3373694 -4.3343253 -4.325027][-4.2848806 -4.2807684 -4.2843843 -4.2874203 -4.2830191 -4.2744441 -4.2665758 -4.2669625 -4.27908 -4.2956414 -4.3125896 -4.3253341 -4.3289208 -4.319653 -4.3032913][-4.2672234 -4.25797 -4.2571778 -4.2548823 -4.2433596 -4.2268896 -4.2130747 -4.2135983 -4.2326522 -4.2640719 -4.2958097 -4.3161922 -4.3197536 -4.3111081 -4.2957063][-4.2543716 -4.2371407 -4.2226329 -4.2081189 -4.1854563 -4.15748 -4.1312814 -4.125628 -4.1550546 -4.2073951 -4.2589846 -4.2917376 -4.3022976 -4.3000402 -4.2898073][-4.2481074 -4.2228184 -4.1933694 -4.1618824 -4.1212621 -4.07274 -4.0200024 -3.9919605 -4.0323772 -4.114747 -4.1929607 -4.2455139 -4.2731853 -4.2844872 -4.283844][-4.252037 -4.2215595 -4.182198 -4.1364918 -4.0760107 -4.00094 -3.9074404 -3.8424006 -3.8919623 -4.0024281 -4.1086712 -4.1886148 -4.2385092 -4.2638564 -4.2724195][-4.2612987 -4.2370076 -4.2070603 -4.1719227 -4.1198745 -4.0487981 -3.9491262 -3.8726149 -3.9052942 -3.9961855 -4.0930891 -4.1755381 -4.2301702 -4.2606983 -4.2722578][-4.2698989 -4.2589731 -4.2478752 -4.2362342 -4.2056079 -4.1518426 -4.0735979 -4.0119328 -4.0241427 -4.073246 -4.1359816 -4.195282 -4.2380371 -4.2632971 -4.2733722][-4.2582064 -4.2512078 -4.2492237 -4.2503633 -4.2304845 -4.1857939 -4.1263251 -4.0847688 -4.0929894 -4.1213174 -4.1604104 -4.2025776 -4.2363071 -4.2586713 -4.2691855][-4.2384281 -4.2293754 -4.2301655 -4.2382331 -4.2272635 -4.1911931 -4.15106 -4.126863 -4.1377621 -4.1618624 -4.1901569 -4.220901 -4.2470112 -4.2639742 -4.2714615][-4.2229753 -4.2083874 -4.2067475 -4.2146864 -4.2071743 -4.180932 -4.1565018 -4.144753 -4.1606021 -4.1856852 -4.2077656 -4.2321754 -4.2552834 -4.2705107 -4.2772212][-4.213964 -4.1911182 -4.1830306 -4.1831479 -4.1751595 -4.1566949 -4.1426849 -4.1380534 -4.1584187 -4.1857219 -4.2086511 -4.2338152 -4.2588344 -4.2753 -4.2805324][-4.2171865 -4.1876569 -4.174181 -4.170054 -4.1635952 -4.1518316 -4.1441474 -4.1431274 -4.16445 -4.1913052 -4.214057 -4.2393827 -4.2650037 -4.2812829 -4.2857985][-4.2389035 -4.2099147 -4.195847 -4.1913261 -4.18715 -4.1809564 -4.1745462 -4.1741362 -4.1908007 -4.2109852 -4.2287436 -4.2493029 -4.2716603 -4.2872248 -4.2926245]]...]
INFO - root - 2017-12-06 04:49:32.387358: step 1610, loss = 2.09, batch loss = 2.03 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:47s remains)
INFO - root - 2017-12-06 04:49:34.536168: step 1620, loss = 2.04, batch loss = 1.98 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:45s remains)
INFO - root - 2017-12-06 04:49:36.739037: step 1630, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 20h:00m:05s remains)
INFO - root - 2017-12-06 04:49:38.991306: step 1640, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 21h:11m:44s remains)
INFO - root - 2017-12-06 04:49:41.158293: step 1650, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:18s remains)
INFO - root - 2017-12-06 04:49:43.415583: step 1660, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 21h:07m:11s remains)
INFO - root - 2017-12-06 04:49:45.584156: step 1670, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:35s remains)
INFO - root - 2017-12-06 04:49:47.781531: step 1680, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:25s remains)
INFO - root - 2017-12-06 04:49:49.996158: step 1690, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:29s remains)
INFO - root - 2017-12-06 04:49:52.196623: step 1700, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:43s remains)
2017-12-06 04:49:52.529508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1955981 -4.1909266 -4.1735053 -4.1505427 -4.1302071 -4.1175456 -4.110476 -4.1132746 -4.1283669 -4.151628 -4.1754589 -4.1864104 -4.1842027 -4.1850543 -4.18456][-4.2076564 -4.2124472 -4.2002382 -4.1757879 -4.1506257 -4.13808 -4.14093 -4.1512933 -4.166903 -4.1878057 -4.2078452 -4.2118936 -4.2024727 -4.2002864 -4.1999369][-4.2470531 -4.2573972 -4.2500892 -4.225215 -4.1939979 -4.1767306 -4.1789103 -4.1924071 -4.2090435 -4.228909 -4.2457485 -4.248456 -4.2379694 -4.2329526 -4.2318187][-4.2763953 -4.2877436 -4.2809792 -4.2569528 -4.2254167 -4.2034903 -4.1948285 -4.20201 -4.2200804 -4.2444735 -4.2648463 -4.2713552 -4.2643147 -4.2555995 -4.2497106][-4.2823753 -4.2934332 -4.2856541 -4.2644267 -4.2372746 -4.2121658 -4.1891513 -4.1816783 -4.1939692 -4.2219558 -4.2496066 -4.2658048 -4.2625594 -4.2486887 -4.2375298][-4.2796936 -4.2886658 -4.2747269 -4.2519078 -4.2264767 -4.1943278 -4.1491928 -4.1169906 -4.1202517 -4.1586919 -4.2048926 -4.236588 -4.2413454 -4.23671 -4.2316885][-4.2815495 -4.2874293 -4.2714138 -4.24195 -4.2049856 -4.1472116 -4.0613513 -3.9851654 -3.9779148 -4.0505028 -4.1357393 -4.1943984 -4.2198033 -4.2336655 -4.2431307][-4.2796392 -4.2813349 -4.2638178 -4.2266541 -4.1760573 -4.0944228 -3.9645612 -3.8260465 -3.7937069 -3.9099047 -4.0463696 -4.1388378 -4.1875467 -4.2163754 -4.2402][-4.2617869 -4.2593222 -4.2432714 -4.2089176 -4.1637135 -4.0907311 -3.9675074 -3.8186002 -3.7648177 -3.8739927 -4.0121512 -4.1131372 -4.1722355 -4.2082391 -4.2343984][-4.2338581 -4.2302547 -4.2252278 -4.2085724 -4.1869292 -4.1443925 -4.0698214 -3.9806252 -3.9424367 -3.9928806 -4.0752134 -4.1460233 -4.1906734 -4.2168403 -4.2329149][-4.21507 -4.2115583 -4.2145147 -4.2162528 -4.2137985 -4.1964707 -4.1599059 -4.117065 -4.1000633 -4.1199708 -4.1563559 -4.1900759 -4.212616 -4.2263908 -4.2310305][-4.21548 -4.2118478 -4.2105665 -4.2151937 -4.220149 -4.212873 -4.1912065 -4.171885 -4.1700807 -4.1813712 -4.1972594 -4.2122655 -4.2232232 -4.2280331 -4.2239304][-4.2267237 -4.2260165 -4.2219338 -4.2191272 -4.2205086 -4.2155123 -4.1993256 -4.1881757 -4.1896582 -4.1972694 -4.2066431 -4.2163196 -4.2240472 -4.2232137 -4.2124395][-4.2280955 -4.2362795 -4.2366042 -4.2325792 -4.23024 -4.2230148 -4.2087569 -4.2019629 -4.2048421 -4.2100043 -4.2144647 -4.2204385 -4.2254634 -4.2221527 -4.2067885][-4.2241597 -4.2359495 -4.2428164 -4.2441497 -4.2452703 -4.2399898 -4.2293243 -4.22475 -4.2270703 -4.227478 -4.2251911 -4.2246084 -4.2285004 -4.230896 -4.2221]]...]
INFO - root - 2017-12-06 04:49:54.758654: step 1710, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:08s remains)
INFO - root - 2017-12-06 04:49:56.955392: step 1720, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:22m:48s remains)
INFO - root - 2017-12-06 04:49:59.108616: step 1730, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.215 sec/batch; 19h:47m:24s remains)
INFO - root - 2017-12-06 04:50:01.283865: step 1740, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:49m:28s remains)
INFO - root - 2017-12-06 04:50:03.499787: step 1750, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:04s remains)
INFO - root - 2017-12-06 04:50:05.717607: step 1760, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:51s remains)
INFO - root - 2017-12-06 04:50:07.896785: step 1770, loss = 2.06, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:00s remains)
INFO - root - 2017-12-06 04:50:10.096688: step 1780, loss = 2.08, batch loss = 2.03 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:29s remains)
INFO - root - 2017-12-06 04:50:12.244270: step 1790, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:11s remains)
INFO - root - 2017-12-06 04:50:14.480539: step 1800, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:16s remains)
2017-12-06 04:50:14.923651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3107743 -4.2992511 -4.2942724 -4.2939324 -4.2920785 -4.2828116 -4.2585716 -4.2295127 -4.2123537 -4.2155681 -4.2337542 -4.2540183 -4.2720437 -4.2842693 -4.2883563][-4.2924891 -4.2770452 -4.27087 -4.2687688 -4.2622485 -4.2434773 -4.2023644 -4.1578455 -4.1358814 -4.1478257 -4.1792312 -4.20898 -4.2322688 -4.2472553 -4.2529926][-4.274097 -4.2534881 -4.2439518 -4.2386913 -4.2266197 -4.1976948 -4.1430397 -4.0862656 -4.063405 -4.0878921 -4.135612 -4.1761417 -4.202919 -4.21615 -4.2218146][-4.2593746 -4.2333064 -4.2206488 -4.2115045 -4.1947303 -4.1584215 -4.0922375 -4.025053 -4.0039296 -4.0448208 -4.1135044 -4.1666903 -4.1948996 -4.201745 -4.2025094][-4.2496033 -4.2178664 -4.20098 -4.1867394 -4.1645069 -4.1199088 -4.0407867 -3.9691451 -3.9558554 -4.0175338 -4.1072421 -4.174696 -4.2060142 -4.2062263 -4.1987977][-4.2428889 -4.2045121 -4.181706 -4.1616693 -4.1312003 -4.0762014 -3.987546 -3.920697 -3.9286678 -4.0117426 -4.1145792 -4.1917434 -4.2291279 -4.2296486 -4.2169771][-4.2400627 -4.1973906 -4.170157 -4.1491008 -4.1169548 -4.05919 -3.97616 -3.9275029 -3.957242 -4.0484123 -4.146101 -4.221509 -4.2603989 -4.2615418 -4.2461581][-4.2444692 -4.2026806 -4.1750107 -4.1566381 -4.1308837 -4.0875516 -4.0311661 -4.0083923 -4.042634 -4.1170392 -4.1918478 -4.2508082 -4.2826018 -4.2842937 -4.2689667][-4.2550492 -4.2190495 -4.1960311 -4.1831012 -4.168272 -4.1455154 -4.121109 -4.1180811 -4.1440392 -4.1906848 -4.2323594 -4.2665491 -4.288589 -4.2917624 -4.2775693][-4.2659235 -4.2359872 -4.2168684 -4.2073216 -4.1977558 -4.1891727 -4.1898947 -4.1984491 -4.216054 -4.2382245 -4.2521524 -4.2627778 -4.2748981 -4.2774086 -4.2633243][-4.2700939 -4.2431488 -4.2237434 -4.2143869 -4.2099085 -4.2141609 -4.2293539 -4.241066 -4.2492609 -4.2519951 -4.2446704 -4.2392488 -4.242136 -4.24414 -4.2364869][-4.2672858 -4.2421522 -4.2222681 -4.2114716 -4.211072 -4.2254925 -4.2453942 -4.254941 -4.255157 -4.2458649 -4.2281289 -4.2159758 -4.2129993 -4.2157068 -4.2157426][-4.2690907 -4.2463946 -4.2275658 -4.2141256 -4.21525 -4.2352543 -4.2554989 -4.2641482 -4.2609973 -4.2442336 -4.2204466 -4.2039862 -4.1961446 -4.1992455 -4.2039566][-4.2764063 -4.255558 -4.2384605 -4.22535 -4.2281003 -4.2504311 -4.2687306 -4.2757688 -4.2708163 -4.2510715 -4.2246037 -4.2035937 -4.1914697 -4.1948924 -4.2032881][-4.2884459 -4.2684803 -4.2503586 -4.2363577 -4.2412934 -4.2628441 -4.2777252 -4.2818708 -4.2758737 -4.2559385 -4.229372 -4.2072234 -4.1945186 -4.2008562 -4.2121038]]...]
INFO - root - 2017-12-06 04:50:17.146153: step 1810, loss = 2.05, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:31m:02s remains)
INFO - root - 2017-12-06 04:50:19.378973: step 1820, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:10s remains)
INFO - root - 2017-12-06 04:50:21.607722: step 1830, loss = 2.04, batch loss = 1.98 (36.6 examples/sec; 0.218 sec/batch; 20h:03m:24s remains)
INFO - root - 2017-12-06 04:50:23.838731: step 1840, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:00s remains)
INFO - root - 2017-12-06 04:50:26.027193: step 1850, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:00s remains)
INFO - root - 2017-12-06 04:50:28.247552: step 1860, loss = 2.02, batch loss = 1.96 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:32s remains)
INFO - root - 2017-12-06 04:50:30.395659: step 1870, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:12m:58s remains)
INFO - root - 2017-12-06 04:50:32.552915: step 1880, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:41m:57s remains)
INFO - root - 2017-12-06 04:50:34.740730: step 1890, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:40s remains)
INFO - root - 2017-12-06 04:50:36.885576: step 1900, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:23s remains)
2017-12-06 04:50:37.567149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2869725 -4.2848163 -4.2849383 -4.2864075 -4.2899241 -4.2896342 -4.2894878 -4.2983913 -4.3124232 -4.3194661 -4.3191166 -4.3120236 -4.3079023 -4.3055396 -4.3067088][-4.27472 -4.266902 -4.2641497 -4.2665672 -4.2700725 -4.2646904 -4.2591181 -4.2645192 -4.2786641 -4.2869582 -4.2906036 -4.2914724 -4.2939391 -4.2933664 -4.29212][-4.2717957 -4.2573647 -4.2469616 -4.244019 -4.2435403 -4.232069 -4.218945 -4.2162442 -4.2290916 -4.2389817 -4.2459712 -4.2540216 -4.2632957 -4.2656045 -4.2615819][-4.2488441 -4.2273688 -4.2086105 -4.2007608 -4.1957159 -4.1852913 -4.1678505 -4.1587048 -4.1700153 -4.1829214 -4.19118 -4.2030435 -4.2208042 -4.2313342 -4.2280993][-4.2126513 -4.1873174 -4.1638246 -4.154758 -4.1493812 -4.1468105 -4.1340036 -4.1256342 -4.1340947 -4.1446934 -4.1509247 -4.1608329 -4.1772437 -4.1894369 -4.1852384][-4.1892428 -4.1687446 -4.1484489 -4.1363811 -4.1247225 -4.128448 -4.1270542 -4.1258802 -4.1316528 -4.1400528 -4.1489267 -4.1531067 -4.1584592 -4.1637964 -4.1577368][-4.1742911 -4.1605558 -4.1467791 -4.1319647 -4.1119204 -4.1129436 -4.1208439 -4.1325774 -4.1435881 -4.15931 -4.1761665 -4.1759653 -4.1667938 -4.1611381 -4.1514063][-4.1506867 -4.1456213 -4.1401091 -4.1237183 -4.09124 -4.0810161 -4.0946841 -4.1173906 -4.1408262 -4.1739206 -4.2054753 -4.2130513 -4.19993 -4.1881976 -4.178463][-4.139225 -4.1433606 -4.1373267 -4.1091309 -4.0618844 -4.044229 -4.060904 -4.0960922 -4.1341376 -4.1816006 -4.22713 -4.2485666 -4.2427654 -4.2306795 -4.2190423][-4.1599774 -4.1622915 -4.1491113 -4.1056304 -4.0461926 -4.022604 -4.040947 -4.084363 -4.1281276 -4.1785479 -4.2283621 -4.2611771 -4.2693267 -4.2687659 -4.2638264][-4.2132483 -4.203516 -4.1757879 -4.1190028 -4.0540519 -4.0208368 -4.0355587 -4.0753403 -4.1183596 -4.1695709 -4.2171383 -4.2524619 -4.2708364 -4.2838817 -4.2901564][-4.2778955 -4.2610803 -4.2221146 -4.1592889 -4.0904837 -4.0426388 -4.0476451 -4.0830221 -4.1249275 -4.1699357 -4.2086887 -4.2383723 -4.2578597 -4.2745695 -4.2891307][-4.3258896 -4.3117433 -4.2757325 -4.2193856 -4.1532865 -4.0995021 -4.093554 -4.1172256 -4.1481977 -4.1777315 -4.2043486 -4.2248955 -4.2398353 -4.2578387 -4.2767191][-4.347939 -4.341783 -4.3180141 -4.2814665 -4.2327781 -4.1882606 -4.1719127 -4.1734347 -4.1842613 -4.1964321 -4.2139277 -4.226778 -4.2345185 -4.2524972 -4.2703519][-4.3412104 -4.346343 -4.3379092 -4.3227806 -4.2985969 -4.27298 -4.2551575 -4.241394 -4.2337623 -4.2331238 -4.2442832 -4.250361 -4.2524757 -4.2662816 -4.2733345]]...]
INFO - root - 2017-12-06 04:50:39.798518: step 1910, loss = 2.04, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:14s remains)
INFO - root - 2017-12-06 04:50:41.964710: step 1920, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:33s remains)
INFO - root - 2017-12-06 04:50:44.148284: step 1930, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:48s remains)
INFO - root - 2017-12-06 04:50:46.260294: step 1940, loss = 2.06, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:38s remains)
INFO - root - 2017-12-06 04:50:48.377492: step 1950, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.210 sec/batch; 19h:18m:42s remains)
INFO - root - 2017-12-06 04:50:50.503780: step 1960, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:41m:10s remains)
INFO - root - 2017-12-06 04:50:52.647066: step 1970, loss = 2.09, batch loss = 2.03 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:46s remains)
INFO - root - 2017-12-06 04:50:54.920139: step 1980, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 20h:55m:35s remains)
INFO - root - 2017-12-06 04:50:57.165094: step 1990, loss = 2.09, batch loss = 2.03 (34.3 examples/sec; 0.233 sec/batch; 21h:24m:39s remains)
INFO - root - 2017-12-06 04:50:59.388828: step 2000, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:45s remains)
2017-12-06 04:51:01.160614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3593831 -4.3588128 -4.3575416 -4.3569717 -4.3564072 -4.358736 -4.3622727 -4.3641267 -4.3647075 -4.3615885 -4.357831 -4.3569694 -4.3565092 -4.3583212 -4.3594732][-4.3513274 -4.3465929 -4.340992 -4.3359733 -4.3314447 -4.3333578 -4.3376236 -4.3364444 -4.331624 -4.3234138 -4.3150287 -4.3136454 -4.3157082 -4.325604 -4.335701][-4.3431191 -4.3325028 -4.3195744 -4.3057723 -4.2936916 -4.2923656 -4.2933335 -4.2865787 -4.2774181 -4.2624421 -4.2486849 -4.2466426 -4.255549 -4.27724 -4.29937][-4.3348436 -4.3166685 -4.2938223 -4.2694864 -4.250001 -4.2425232 -4.236937 -4.2224822 -4.2061591 -4.1813769 -4.1586847 -4.1573544 -4.1773577 -4.2161131 -4.2537265][-4.3174939 -4.2887545 -4.2546477 -4.2219496 -4.1987319 -4.1883307 -4.1776342 -4.1534796 -4.1267991 -4.0916414 -4.0655732 -4.0714555 -4.1043429 -4.1609344 -4.2136674][-4.28054 -4.2409439 -4.1983204 -4.1629057 -4.1397548 -4.1295061 -4.1111736 -4.07947 -4.0474591 -4.0117674 -3.9945693 -4.0142212 -4.0593429 -4.1297131 -4.1929631][-4.2351708 -4.1864843 -4.1327243 -4.09192 -4.0699549 -4.062264 -4.0411873 -4.006691 -3.9857037 -3.9747539 -3.9805493 -4.0139389 -4.0681682 -4.1392374 -4.1999369][-4.1800818 -4.1125851 -4.0347023 -3.9842021 -3.9714022 -3.9783993 -3.9676294 -3.9457657 -3.9541008 -3.9754243 -4.002244 -4.0485663 -4.1069603 -4.1737051 -4.2268472][-4.1284823 -4.039485 -3.9316635 -3.8712735 -3.8789148 -3.9162493 -3.9360909 -3.948118 -3.9817855 -4.021235 -4.0607529 -4.11164 -4.1649556 -4.2191372 -4.2600746][-4.114233 -4.0241165 -3.9176581 -3.8652716 -3.8933043 -3.9538741 -4.0013409 -4.033534 -4.0702977 -4.1097717 -4.1489096 -4.1911268 -4.2307458 -4.2672009 -4.2913957][-4.1662045 -4.1054626 -4.0403671 -4.0130024 -4.0389857 -4.0897083 -4.1338296 -4.1601686 -4.1834445 -4.2075806 -4.2320766 -4.2591624 -4.2836585 -4.3038278 -4.3157849][-4.2449379 -4.215117 -4.1919651 -4.1892762 -4.2106156 -4.2417631 -4.2647491 -4.2713943 -4.2763066 -4.2829571 -4.2928662 -4.30783 -4.3222966 -4.3316479 -4.3351579][-4.303916 -4.2919955 -4.2878728 -4.2959847 -4.3110065 -4.3262043 -4.3360209 -4.3331604 -4.3295383 -4.3267665 -4.329596 -4.3366752 -4.3435612 -4.3473067 -4.3480163][-4.3391037 -4.336719 -4.3373213 -4.3436522 -4.3509107 -4.35712 -4.3594093 -4.3549604 -4.3494234 -4.3459473 -4.346941 -4.3498578 -4.3527265 -4.3542991 -4.3545752][-4.3527646 -4.3530526 -4.3530478 -4.3547373 -4.35754 -4.3609633 -4.3619232 -4.35981 -4.3574953 -4.3559036 -4.3560004 -4.356791 -4.3574409 -4.3575964 -4.3574553]]...]
INFO - root - 2017-12-06 04:51:03.345565: step 2010, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:02s remains)
INFO - root - 2017-12-06 04:51:05.483018: step 2020, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:18s remains)
INFO - root - 2017-12-06 04:51:07.694525: step 2030, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:57m:29s remains)
INFO - root - 2017-12-06 04:51:09.857876: step 2040, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:12s remains)
INFO - root - 2017-12-06 04:51:12.091603: step 2050, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:07s remains)
INFO - root - 2017-12-06 04:51:14.268497: step 2060, loss = 2.05, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:29m:08s remains)
INFO - root - 2017-12-06 04:51:16.408229: step 2070, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:48s remains)
INFO - root - 2017-12-06 04:51:18.586718: step 2080, loss = 2.07, batch loss = 2.01 (38.3 examples/sec; 0.209 sec/batch; 19h:09m:43s remains)
INFO - root - 2017-12-06 04:51:20.735762: step 2090, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:47s remains)
INFO - root - 2017-12-06 04:51:22.954798: step 2100, loss = 2.07, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:37s remains)
2017-12-06 04:51:23.382695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3660173 -4.3633857 -4.3609896 -4.3600988 -4.3606839 -4.3624568 -4.3640571 -4.3632298 -4.3603292 -4.3564777 -4.3529334 -4.3511314 -4.3516564 -4.3540487 -4.3565292][-4.3694153 -4.3649588 -4.3591628 -4.3540497 -4.3505926 -4.3492532 -4.3485785 -4.3476214 -4.3463254 -4.3461738 -4.3474817 -4.3512053 -4.3564138 -4.3616562 -4.3643589][-4.37023 -4.3629065 -4.3513093 -4.3384695 -4.3250651 -4.313179 -4.3032541 -4.2971492 -4.295053 -4.3003597 -4.3114362 -4.3272271 -4.3451533 -4.3608651 -4.3693213][-4.363224 -4.3500118 -4.327796 -4.3022556 -4.2747068 -4.2469349 -4.220706 -4.2042832 -4.2004938 -4.2152505 -4.2433596 -4.2781448 -4.3149753 -4.3457818 -4.3630333][-4.3463154 -4.322134 -4.283587 -4.2405324 -4.1933389 -4.1407661 -4.0884771 -4.0579534 -4.0564413 -4.0906911 -4.1431131 -4.2035832 -4.2654591 -4.3145552 -4.3422418][-4.3261027 -4.2887492 -4.2313862 -4.16929 -4.0967484 -4.0120077 -3.9304745 -3.8846924 -3.8884141 -3.9498341 -4.0301633 -4.1151824 -4.1993704 -4.2660875 -4.3035488][-4.3215203 -4.2780185 -4.2141085 -4.1431532 -4.0546203 -3.9477074 -3.847939 -3.7934103 -3.8006754 -3.8797088 -3.9747672 -4.0670652 -4.15551 -4.2252378 -4.2606587][-4.329071 -4.2917328 -4.2356954 -4.1712961 -4.0913854 -3.9966435 -3.9116488 -3.8648705 -3.8707893 -3.9414744 -4.0223484 -4.0972004 -4.1671863 -4.2199812 -4.237164][-4.3375325 -4.3087049 -4.2660522 -4.2160268 -4.158494 -4.0963035 -4.0436368 -4.0152984 -4.0202632 -4.0711842 -4.1282692 -4.1827717 -4.2323256 -4.2629409 -4.2580051][-4.3515148 -4.3315763 -4.3026981 -4.26886 -4.2332354 -4.1984596 -4.1718049 -4.1588058 -4.1632609 -4.1943293 -4.2295318 -4.2666407 -4.3017378 -4.3203216 -4.3077006][-4.3657069 -4.3551674 -4.3399429 -4.3216424 -4.3021955 -4.2839332 -4.2705979 -4.26354 -4.2638307 -4.2786651 -4.2976246 -4.3201947 -4.3443847 -4.3573632 -4.3477812][-4.37399 -4.3718061 -4.3681045 -4.3623328 -4.3536711 -4.3444018 -4.3374481 -4.3324184 -4.3298688 -4.3344951 -4.3425488 -4.3546424 -4.369873 -4.3789473 -4.3735847][-4.3705115 -4.3720536 -4.374114 -4.3752604 -4.3737025 -4.3717022 -4.3714924 -4.3719282 -4.3705769 -4.3707333 -4.3723025 -4.3768439 -4.384655 -4.3901238 -4.387568][-4.3613868 -4.3617034 -4.363924 -4.3662109 -4.3665237 -4.3681407 -4.3726349 -4.3775182 -4.3799596 -4.3802729 -4.3792143 -4.379108 -4.3824263 -4.38631 -4.3868241][-4.3494754 -4.3437853 -4.3424406 -4.34423 -4.3465543 -4.3508048 -4.3591833 -4.3687925 -4.37585 -4.3782582 -4.3765483 -4.3731661 -4.3730035 -4.3759408 -4.3778262]]...]
INFO - root - 2017-12-06 04:51:25.575778: step 2110, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:21s remains)
INFO - root - 2017-12-06 04:51:27.760762: step 2120, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 20h:14m:51s remains)
INFO - root - 2017-12-06 04:51:29.932050: step 2130, loss = 2.04, batch loss = 1.98 (39.0 examples/sec; 0.205 sec/batch; 18h:48m:52s remains)
INFO - root - 2017-12-06 04:51:32.107916: step 2140, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:32s remains)
INFO - root - 2017-12-06 04:51:34.323451: step 2150, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 20h:31m:32s remains)
INFO - root - 2017-12-06 04:51:36.498910: step 2160, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 18h:59m:29s remains)
INFO - root - 2017-12-06 04:51:38.734126: step 2170, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:16s remains)
INFO - root - 2017-12-06 04:51:40.921014: step 2180, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:19m:11s remains)
INFO - root - 2017-12-06 04:51:43.095025: step 2190, loss = 2.05, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:30m:04s remains)
INFO - root - 2017-12-06 04:51:45.295862: step 2200, loss = 2.05, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 20h:12m:43s remains)
2017-12-06 04:51:46.832586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.282311 -4.2897711 -4.2850637 -4.2599378 -4.2224727 -4.1725979 -4.126646 -4.1233048 -4.1551385 -4.1825991 -4.2108459 -4.2450142 -4.2666678 -4.2725558 -4.2587996][-4.2737455 -4.2892265 -4.2898178 -4.2629609 -4.2200289 -4.1638222 -4.1183796 -4.1206203 -4.1602278 -4.1970649 -4.2307863 -4.26949 -4.2951617 -4.3012762 -4.2841587][-4.2703834 -4.2927661 -4.2946253 -4.2597885 -4.2019081 -4.1317048 -4.0854754 -4.1010785 -4.1537962 -4.2046108 -4.2478943 -4.2905984 -4.3218679 -4.3248687 -4.3014226][-4.2771974 -4.3032479 -4.3005705 -4.2522726 -4.1743827 -4.0851288 -4.0325513 -4.05981 -4.1333356 -4.206212 -4.2640071 -4.3131824 -4.3462586 -4.3411546 -4.3078389][-4.2888165 -4.3133445 -4.3031216 -4.2412224 -4.1417651 -4.0310931 -3.9698162 -4.0080843 -4.1071811 -4.2056193 -4.2788363 -4.3343825 -4.3637967 -4.3468146 -4.3006477][-4.2992954 -4.3191204 -4.3002934 -4.2278395 -4.1066241 -3.9699755 -3.894804 -3.9433317 -4.0711985 -4.1966572 -4.2875085 -4.3477421 -4.3686447 -4.3427315 -4.2858539][-4.3124146 -4.3242335 -4.2978792 -4.2190843 -4.0846 -3.9250011 -3.8311083 -3.8795683 -4.0271173 -4.1727781 -4.2800975 -4.3451242 -4.3615236 -4.3333416 -4.2698884][-4.3286433 -4.3307967 -4.2991953 -4.2190504 -4.0846682 -3.9184794 -3.8098502 -3.8462274 -3.9927661 -4.1438274 -4.2618203 -4.3338737 -4.3503757 -4.3206449 -4.2495604][-4.3477592 -4.3425589 -4.30723 -4.2315378 -4.1094618 -3.9540069 -3.8420491 -3.8596597 -3.987695 -4.1294365 -4.2462192 -4.3193889 -4.3380747 -4.31112 -4.2384863][-4.3620214 -4.3536849 -4.3190207 -4.252398 -4.1507463 -4.0190816 -3.9145083 -3.9144053 -4.0137358 -4.1373343 -4.241787 -4.3072662 -4.3241081 -4.3022494 -4.2395473][-4.3649616 -4.3563738 -4.3272371 -4.2734103 -4.1959662 -4.0950074 -4.00665 -3.9933848 -4.0614738 -4.1602716 -4.2468362 -4.2997031 -4.3118706 -4.293252 -4.2426887][-4.3555713 -4.3479395 -4.3254004 -4.2866187 -4.2323833 -4.1613979 -4.0954108 -4.0796432 -4.1249566 -4.1997681 -4.2662411 -4.3046675 -4.3124595 -4.2944121 -4.2530379][-4.34178 -4.3336906 -4.3165908 -4.2892919 -4.250536 -4.2019706 -4.1580286 -4.1498523 -4.1848707 -4.2415175 -4.2902927 -4.3158307 -4.3200135 -4.3021216 -4.2664838][-4.3281536 -4.3173652 -4.30292 -4.2848825 -4.2575316 -4.2237506 -4.1963429 -4.1934075 -4.2225375 -4.264266 -4.2979512 -4.3158236 -4.3208895 -4.3087964 -4.282959][-4.3192062 -4.3063383 -4.2936926 -4.2816949 -4.2623792 -4.2402978 -4.2232876 -4.2221136 -4.2425575 -4.2705269 -4.2937117 -4.3084636 -4.3155317 -4.3102937 -4.2951107]]...]
INFO - root - 2017-12-06 04:51:49.040468: step 2210, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:10s remains)
INFO - root - 2017-12-06 04:51:51.206660: step 2220, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:14s remains)
INFO - root - 2017-12-06 04:51:53.388690: step 2230, loss = 2.09, batch loss = 2.03 (36.8 examples/sec; 0.217 sec/batch; 19h:55m:22s remains)
INFO - root - 2017-12-06 04:51:55.579649: step 2240, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.215 sec/batch; 19h:45m:32s remains)
INFO - root - 2017-12-06 04:51:57.764147: step 2250, loss = 2.08, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 20h:47m:07s remains)
INFO - root - 2017-12-06 04:51:59.917862: step 2260, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:33s remains)
INFO - root - 2017-12-06 04:52:02.197481: step 2270, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:24s remains)
INFO - root - 2017-12-06 04:52:04.377672: step 2280, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:08s remains)
INFO - root - 2017-12-06 04:52:06.519140: step 2290, loss = 2.06, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:30s remains)
INFO - root - 2017-12-06 04:52:08.673030: step 2300, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:54s remains)
2017-12-06 04:52:09.620976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2033668 -4.1459622 -4.1133661 -4.1116691 -4.1309452 -4.1536822 -4.1807137 -4.189734 -4.19504 -4.1965652 -4.1903381 -4.1746106 -4.1661749 -4.1731482 -4.2029996][-4.1945891 -4.1473703 -4.1278372 -4.132102 -4.1444407 -4.1502142 -4.1728916 -4.1923213 -4.2078705 -4.2157469 -4.2063808 -4.1803756 -4.1663408 -4.1681514 -4.1872439][-4.1957378 -4.1542549 -4.1415482 -4.1470823 -4.1507306 -4.1476188 -4.1674032 -4.1890545 -4.2024646 -4.2073197 -4.1951442 -4.1744614 -4.1691508 -4.167377 -4.1716628][-4.2075381 -4.166553 -4.1540742 -4.1572137 -4.1545067 -4.1492434 -4.1662755 -4.1862535 -4.1921062 -4.1911793 -4.1810937 -4.1733637 -4.178443 -4.1748962 -4.1686974][-4.2215014 -4.1810665 -4.1660151 -4.1624002 -4.1522055 -4.1441946 -4.1589413 -4.1772175 -4.1762996 -4.1698489 -4.1667137 -4.1707034 -4.1803613 -4.1780119 -4.1677155][-4.2355533 -4.1941729 -4.1717319 -4.159184 -4.1403904 -4.1309986 -4.141849 -4.1543045 -4.1487374 -4.1434879 -4.150351 -4.162724 -4.1731544 -4.1753435 -4.1688991][-4.2457528 -4.2036672 -4.1736345 -4.1521087 -4.12882 -4.1147165 -4.1156859 -4.1215129 -4.1165862 -4.1160464 -4.136013 -4.16012 -4.1734543 -4.1758194 -4.1710863][-4.2486773 -4.2098579 -4.1805849 -4.1614594 -4.1375318 -4.1133237 -4.1002398 -4.0968671 -4.0911694 -4.0981112 -4.1313419 -4.1639352 -4.1813059 -4.18242 -4.1726665][-4.2469482 -4.2138138 -4.1943741 -4.1876235 -4.1704512 -4.1399822 -4.1134229 -4.0969453 -4.0878892 -4.1023707 -4.146297 -4.1835623 -4.2015009 -4.1993313 -4.1815968][-4.2453241 -4.2156596 -4.2045388 -4.2103095 -4.2006569 -4.1735921 -4.1439571 -4.1169224 -4.1000671 -4.1122031 -4.1565342 -4.1894259 -4.2032657 -4.198513 -4.1765461][-4.249814 -4.2179971 -4.2063293 -4.215385 -4.2145705 -4.1967845 -4.1767755 -4.1500134 -4.1299133 -4.1288795 -4.1527262 -4.1696405 -4.1753697 -4.1703544 -4.1521068][-4.255579 -4.2209311 -4.2050605 -4.2105331 -4.21521 -4.2104211 -4.2021675 -4.1780415 -4.1547546 -4.1383247 -4.136003 -4.13944 -4.1412544 -4.139051 -4.1286793][-4.2578049 -4.2231369 -4.2035303 -4.2027512 -4.2079396 -4.2089925 -4.20616 -4.1864395 -4.1626334 -4.1379261 -4.1230631 -4.1229777 -4.1265988 -4.1273432 -4.1228156][-4.259357 -4.2305593 -4.2131705 -4.208046 -4.2083859 -4.2041903 -4.201189 -4.189095 -4.170393 -4.1484761 -4.1345339 -4.1375718 -4.140275 -4.137527 -4.1346464][-4.2623177 -4.2452612 -4.2374167 -4.2331276 -4.2253647 -4.2120337 -4.2057281 -4.1986322 -4.1833682 -4.1669636 -4.16088 -4.1667156 -4.1649938 -4.1561322 -4.1521082]]...]
INFO - root - 2017-12-06 04:52:11.791970: step 2310, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:48s remains)
INFO - root - 2017-12-06 04:52:14.030419: step 2320, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 21h:33m:41s remains)
INFO - root - 2017-12-06 04:52:16.230360: step 2330, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:06s remains)
INFO - root - 2017-12-06 04:52:18.403360: step 2340, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:07s remains)
INFO - root - 2017-12-06 04:52:20.600348: step 2350, loss = 2.08, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:15m:20s remains)
INFO - root - 2017-12-06 04:52:22.797403: step 2360, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:55s remains)
INFO - root - 2017-12-06 04:52:24.979172: step 2370, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:36s remains)
INFO - root - 2017-12-06 04:52:27.128956: step 2380, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:00s remains)
INFO - root - 2017-12-06 04:52:29.381416: step 2390, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:38s remains)
INFO - root - 2017-12-06 04:52:31.659152: step 2400, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:50s remains)
2017-12-06 04:52:32.045930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.08665 -4.091506 -4.1257839 -4.1627145 -4.1869431 -4.1980982 -4.199264 -4.1931109 -4.1906133 -4.1945868 -4.2009444 -4.2042441 -4.1985683 -4.1869464 -4.1870308][-4.0649891 -4.0705075 -4.1068711 -4.1511464 -4.1820483 -4.2019033 -4.2107277 -4.2074547 -4.2039242 -4.2043419 -4.20596 -4.206769 -4.2065592 -4.2062316 -4.2129107][-4.0823555 -4.0828004 -4.1139913 -4.1612711 -4.1965919 -4.2187181 -4.2275586 -4.2228017 -4.2176924 -4.2163887 -4.2141485 -4.2116179 -4.2147965 -4.2191019 -4.2234368][-4.10958 -4.1095896 -4.1410179 -4.1857157 -4.2165008 -4.2322536 -4.2329593 -4.2213335 -4.2116132 -4.2100296 -4.2098393 -4.2119722 -4.2197938 -4.2207885 -4.2175374][-4.1305361 -4.1354861 -4.1695733 -4.2055807 -4.2230749 -4.2231922 -4.2050338 -4.1799178 -4.1648688 -4.16789 -4.1816792 -4.2020073 -4.2223334 -4.2235274 -4.2126818][-4.129847 -4.1396437 -4.1756988 -4.201817 -4.2037687 -4.1772141 -4.1284165 -4.0796051 -4.0619974 -4.0855889 -4.1304307 -4.1774077 -4.2162776 -4.22814 -4.2175984][-4.1098709 -4.1206741 -4.1551909 -4.1740141 -4.1586537 -4.0997372 -4.0146766 -3.9395247 -3.9258409 -3.9837742 -4.0699663 -4.1477218 -4.2051525 -4.2319632 -4.2265553][-4.0995183 -4.109684 -4.1396813 -4.151917 -4.1194153 -4.0346136 -3.9244571 -3.8324921 -3.8291352 -3.9140062 -4.0266814 -4.1221285 -4.1921034 -4.230989 -4.2335291][-4.1098914 -4.1196961 -4.144825 -4.1546369 -4.1207089 -4.0400009 -3.9448159 -3.8729727 -3.8772409 -3.9471574 -4.0404644 -4.1251268 -4.1916819 -4.2349787 -4.2441115][-4.141273 -4.1444926 -4.1603117 -4.1692543 -4.1471419 -4.0950356 -4.0414829 -4.0063887 -4.0097823 -4.0434122 -4.0954227 -4.1523132 -4.2025986 -4.241684 -4.2551374][-4.181437 -4.1732035 -4.1740413 -4.1798983 -4.1724768 -4.1521025 -4.1365271 -4.1279116 -4.1276193 -4.1338234 -4.1514888 -4.1819515 -4.2145495 -4.244781 -4.2582941][-4.2122054 -4.19596 -4.1844916 -4.1869617 -4.1937943 -4.1976838 -4.2033725 -4.2043476 -4.1981959 -4.186439 -4.1804261 -4.189959 -4.2085009 -4.2289953 -4.23995][-4.2243075 -4.2091446 -4.1958385 -4.19932 -4.212873 -4.2285733 -4.2393208 -4.2351804 -4.2167091 -4.1912246 -4.1701484 -4.1651044 -4.1724625 -4.1856527 -4.1964188][-4.2154794 -4.2046089 -4.1981215 -4.2077675 -4.2251463 -4.2431135 -4.2511067 -4.2379923 -4.2060995 -4.1702051 -4.1387005 -4.1200595 -4.1166806 -4.1235003 -4.1368542][-4.1836977 -4.1785555 -4.1815157 -4.1963196 -4.2134085 -4.22789 -4.2317772 -4.2148933 -4.1791697 -4.1433964 -4.1119533 -4.087913 -4.0782909 -4.08206 -4.100275]]...]
INFO - root - 2017-12-06 04:52:34.254009: step 2410, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 21h:01m:52s remains)
INFO - root - 2017-12-06 04:52:36.449847: step 2420, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:53m:46s remains)
INFO - root - 2017-12-06 04:52:38.659653: step 2430, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:43s remains)
INFO - root - 2017-12-06 04:52:40.818698: step 2440, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:10s remains)
INFO - root - 2017-12-06 04:52:42.974749: step 2450, loss = 2.03, batch loss = 1.97 (38.0 examples/sec; 0.210 sec/batch; 19h:17m:25s remains)
INFO - root - 2017-12-06 04:52:45.176437: step 2460, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:09s remains)
INFO - root - 2017-12-06 04:52:47.336672: step 2470, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:09s remains)
INFO - root - 2017-12-06 04:52:49.517720: step 2480, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:30m:20s remains)
INFO - root - 2017-12-06 04:52:51.736485: step 2490, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 21h:13m:39s remains)
INFO - root - 2017-12-06 04:52:53.928804: step 2500, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:52s remains)
2017-12-06 04:52:54.306246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3544645 -4.3529854 -4.3505692 -4.352663 -4.3546877 -4.3551364 -4.356442 -4.3579869 -4.3587565 -4.3580923 -4.3580303 -4.3592019 -4.3591132 -4.3589673 -4.359797][-4.3441596 -4.3372965 -4.3311229 -4.3343983 -4.341083 -4.3470693 -4.3538103 -4.3604054 -4.3630843 -4.3616533 -4.3604231 -4.3615317 -4.3610559 -4.3608379 -4.3614883][-4.3260794 -4.3115892 -4.3007159 -4.3045592 -4.3145232 -4.3216949 -4.328711 -4.3392072 -4.3435836 -4.34173 -4.3406563 -4.3458242 -4.351099 -4.3554873 -4.3578625][-4.3106184 -4.29145 -4.2736964 -4.2726865 -4.2802596 -4.2852383 -4.2895784 -4.3037529 -4.3109188 -4.3070827 -4.3026934 -4.31064 -4.324625 -4.3383393 -4.3459249][-4.2944388 -4.2730927 -4.2487674 -4.2383709 -4.2388544 -4.2380829 -4.2376056 -4.2542491 -4.2636595 -4.2564163 -4.2460089 -4.2548614 -4.2801 -4.3077078 -4.3242874][-4.2694855 -4.2459426 -4.2116537 -4.1840839 -4.1675835 -4.1535149 -4.1488576 -4.1701088 -4.1847811 -4.18124 -4.1723666 -4.1902733 -4.229517 -4.2729115 -4.3005404][-4.233171 -4.2017074 -4.1539755 -4.1081553 -4.0704966 -4.0371656 -4.025598 -4.058537 -4.0912852 -4.1003609 -4.1036305 -4.1369042 -4.1891532 -4.2422109 -4.2790408][-4.196465 -4.1547933 -4.0982456 -4.0479188 -4.0050521 -3.9647944 -3.9461355 -3.9856994 -4.0327578 -4.051847 -4.0651326 -4.1059794 -4.1615114 -4.2161474 -4.2584248][-4.164607 -4.1148615 -4.0547509 -4.0152249 -3.999239 -3.9867382 -3.9731083 -3.9999521 -4.0376873 -4.055542 -4.0691991 -4.1047797 -4.1549082 -4.2067471 -4.2497077][-4.1517968 -4.1049485 -4.0510745 -4.0284734 -4.0456033 -4.0668678 -4.0646262 -4.0734553 -4.0908866 -4.1013288 -4.1122832 -4.1373744 -4.1767893 -4.2217894 -4.2600827][-4.165133 -4.1258903 -4.0829248 -4.0758 -4.1125655 -4.1509972 -4.1583023 -4.1563816 -4.1592169 -4.1637306 -4.1715422 -4.1889906 -4.2167311 -4.2504239 -4.2795315][-4.1803503 -4.147366 -4.1140904 -4.1181054 -4.1596241 -4.1995926 -4.2093887 -4.207077 -4.2092166 -4.2147593 -4.2236753 -4.23817 -4.257081 -4.278512 -4.2974391][-4.2038536 -4.1804814 -4.1588097 -4.1675777 -4.2025795 -4.232553 -4.2380676 -4.2359972 -4.2404408 -4.247611 -4.2562122 -4.2684917 -4.2834492 -4.2984209 -4.3118167][-4.2360907 -4.2243223 -4.2132921 -4.2197986 -4.2413206 -4.2588015 -4.2604351 -4.2568364 -4.2586579 -4.26306 -4.2682576 -4.2776284 -4.2914982 -4.3056183 -4.3191104][-4.2637033 -4.2577748 -4.2504759 -4.2518482 -4.2638583 -4.274868 -4.2778292 -4.27737 -4.2787671 -4.2808151 -4.2834511 -4.2905531 -4.3024788 -4.3155775 -4.3285193]]...]
INFO - root - 2017-12-06 04:52:56.516485: step 2510, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:41s remains)
INFO - root - 2017-12-06 04:52:58.749416: step 2520, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:48m:29s remains)
INFO - root - 2017-12-06 04:53:00.965411: step 2530, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:24s remains)
INFO - root - 2017-12-06 04:53:03.176557: step 2540, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:09s remains)
INFO - root - 2017-12-06 04:53:05.367386: step 2550, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:31s remains)
INFO - root - 2017-12-06 04:53:07.615149: step 2560, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:32m:52s remains)
INFO - root - 2017-12-06 04:53:09.782806: step 2570, loss = 2.05, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 19h:55m:52s remains)
INFO - root - 2017-12-06 04:53:11.963646: step 2580, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:28m:43s remains)
INFO - root - 2017-12-06 04:53:14.175926: step 2590, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 20h:00m:54s remains)
INFO - root - 2017-12-06 04:53:16.345423: step 2600, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:15s remains)
2017-12-06 04:53:18.676516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.173564 -4.1698003 -4.1738529 -4.1856303 -4.1956067 -4.2010622 -4.2016697 -4.193459 -4.1707888 -4.1426969 -4.1181793 -4.0948315 -4.092926 -4.1163573 -4.1459594][-4.1601892 -4.1579328 -4.1715708 -4.1910319 -4.2052479 -4.2123108 -4.2135944 -4.2063351 -4.1793542 -4.1401634 -4.1046052 -4.0788355 -4.0749245 -4.0935364 -4.1220188][-4.1602268 -4.1651268 -4.1848583 -4.2083082 -4.2211494 -4.2210984 -4.2144065 -4.2016578 -4.1756568 -4.1371808 -4.1013217 -4.0733404 -4.0590973 -4.0608697 -4.0788321][-4.17042 -4.1810379 -4.2009792 -4.2194843 -4.2224689 -4.2136769 -4.20094 -4.1827488 -4.1632133 -4.13671 -4.1099663 -4.0799069 -4.0519538 -4.033524 -4.036809][-4.1805596 -4.1913028 -4.2076049 -4.2191925 -4.2134919 -4.1965895 -4.1750846 -4.1486106 -4.131804 -4.1209259 -4.1095667 -4.0818624 -4.0484829 -4.0186892 -4.0126176][-4.1784658 -4.18486 -4.1983256 -4.2067146 -4.1971188 -4.173656 -4.1391521 -4.1012974 -4.0829225 -4.08662 -4.0900936 -4.0714059 -4.0455184 -4.0200243 -4.0191188][-4.1637044 -4.1732926 -4.18841 -4.1972904 -4.1848893 -4.1536155 -4.1103926 -4.0667267 -4.0505595 -4.0612397 -4.0760074 -4.0720267 -4.0577669 -4.0383873 -4.038537][-4.1606565 -4.1829858 -4.2000217 -4.2063632 -4.1933184 -4.1575789 -4.1121712 -4.071382 -4.0585856 -4.0678062 -4.0820322 -4.0895772 -4.0853019 -4.0683646 -4.0632138][-4.1600504 -4.1944232 -4.212503 -4.2178121 -4.2079268 -4.1740212 -4.1298718 -4.0934005 -4.0847549 -4.094418 -4.1075811 -4.1228747 -4.1269021 -4.1141005 -4.10671][-4.1649151 -4.205595 -4.2249174 -4.2292881 -4.2204561 -4.1913314 -4.1515212 -4.1232114 -4.1235332 -4.1398163 -4.151278 -4.1594748 -4.1567349 -4.1457467 -4.1411257][-4.1787996 -4.2183771 -4.2395487 -4.2464781 -4.2387977 -4.2113218 -4.1773944 -4.158504 -4.1687021 -4.1887 -4.1947546 -4.1894979 -4.1755266 -4.1646371 -4.1673918][-4.1908989 -4.226778 -4.250495 -4.2628675 -4.2581592 -4.2349114 -4.2076674 -4.1961632 -4.2083144 -4.2266622 -4.2286158 -4.2172031 -4.1992326 -4.1898746 -4.195446][-4.2055039 -4.2332125 -4.25711 -4.2729378 -4.2734447 -4.2594008 -4.2416139 -4.2359238 -4.2458329 -4.2577949 -4.2568741 -4.2441268 -4.2273369 -4.2204032 -4.2252774][-4.2340946 -4.2475266 -4.2647324 -4.2796926 -4.2849541 -4.278914 -4.2695436 -4.2672544 -4.2745471 -4.2818122 -4.2810326 -4.271729 -4.2604537 -4.2577477 -4.2605724][-4.2713661 -4.2750006 -4.2838354 -4.2922816 -4.2964334 -4.2948856 -4.2908888 -4.2905369 -4.295876 -4.3018212 -4.3030982 -4.298974 -4.2922635 -4.2904048 -4.291328]]...]
INFO - root - 2017-12-06 04:53:20.924431: step 2610, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:44m:02s remains)
INFO - root - 2017-12-06 04:53:23.104913: step 2620, loss = 2.04, batch loss = 1.98 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:17s remains)
INFO - root - 2017-12-06 04:53:25.297662: step 2630, loss = 2.05, batch loss = 1.99 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:38s remains)
INFO - root - 2017-12-06 04:53:27.463174: step 2640, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:55s remains)
INFO - root - 2017-12-06 04:53:29.613365: step 2650, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:32m:12s remains)
INFO - root - 2017-12-06 04:53:31.791175: step 2660, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:21s remains)
INFO - root - 2017-12-06 04:53:34.004918: step 2670, loss = 2.10, batch loss = 2.04 (36.8 examples/sec; 0.217 sec/batch; 19h:55m:23s remains)
INFO - root - 2017-12-06 04:53:36.156918: step 2680, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:17s remains)
INFO - root - 2017-12-06 04:53:38.381241: step 2690, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 20h:29m:59s remains)
INFO - root - 2017-12-06 04:53:40.591871: step 2700, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:02m:46s remains)
2017-12-06 04:53:40.959371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1791353 -4.1482582 -4.1481967 -4.1928258 -4.2377868 -4.2660465 -4.28016 -4.2902284 -4.2942696 -4.2851686 -4.2534919 -4.2012038 -4.15491 -4.1434031 -4.1484871][-4.1553082 -4.1160755 -4.1170044 -4.1695366 -4.22062 -4.2495756 -4.2605581 -4.269052 -4.2781024 -4.2743182 -4.245501 -4.185606 -4.1231084 -4.1032023 -4.0999813][-4.1539545 -4.1195259 -4.1271667 -4.179987 -4.2258449 -4.2437005 -4.2383213 -4.2341056 -4.2450094 -4.2554541 -4.2400346 -4.1853914 -4.1240473 -4.1034765 -4.0987148][-4.1922398 -4.1693912 -4.1785426 -4.2162542 -4.238925 -4.2315726 -4.1970358 -4.1699529 -4.1781054 -4.214108 -4.2340379 -4.2124867 -4.1784086 -4.1629076 -4.1561489][-4.2391334 -4.2242928 -4.22603 -4.2399116 -4.2305741 -4.1926036 -4.125648 -4.0641308 -4.0675268 -4.1388226 -4.2049427 -4.2281795 -4.2303596 -4.224792 -4.2182307][-4.2668157 -4.25613 -4.2475438 -4.2354217 -4.1934814 -4.1224656 -4.0136819 -3.9011986 -3.8957703 -4.0111923 -4.12922 -4.1956015 -4.2353692 -4.250288 -4.2521791][-4.2800279 -4.2726803 -4.2550964 -4.2221022 -4.1538258 -4.0479736 -3.8863852 -3.7107928 -3.6930325 -3.8569934 -4.0259972 -4.1336432 -4.2057676 -4.243597 -4.2569628][-4.2785192 -4.2713604 -4.2488565 -4.2099867 -4.1386366 -4.0259933 -3.8540144 -3.672111 -3.65941 -3.8311288 -4.0020523 -4.1122026 -4.1894269 -4.2338185 -4.2510791][-4.2629161 -4.2566514 -4.2435217 -4.2174211 -4.1629758 -4.0770235 -3.9456666 -3.8105667 -3.8014455 -3.9316783 -4.0607815 -4.1486306 -4.2098842 -4.242866 -4.2541866][-4.2443109 -4.2398224 -4.2386241 -4.2310576 -4.1987367 -4.1442194 -4.0608578 -3.9759257 -3.9731479 -4.0584722 -4.140656 -4.199903 -4.2423825 -4.2652841 -4.2681][-4.2249665 -4.2167869 -4.2219372 -4.2341032 -4.2240291 -4.1969404 -4.1549144 -4.1134753 -4.1158676 -4.1648407 -4.211587 -4.2462926 -4.2719841 -4.2848082 -4.28357][-4.2050557 -4.1890335 -4.2005396 -4.2327304 -4.24523 -4.2404857 -4.2266574 -4.2092466 -4.2135081 -4.24193 -4.2663612 -4.2837858 -4.2967949 -4.3016081 -4.2985873][-4.1976881 -4.1743 -4.1861815 -4.2303166 -4.2619004 -4.2759433 -4.2764015 -4.2675271 -4.2697363 -4.287941 -4.3021569 -4.3097663 -4.3159313 -4.3151312 -4.309772][-4.1963854 -4.1700644 -4.1802287 -4.2254791 -4.2698627 -4.2973714 -4.3063941 -4.3018069 -4.302474 -4.3129277 -4.3201151 -4.322279 -4.3230982 -4.3205338 -4.3149514][-4.1814618 -4.1536303 -4.16505 -4.2106848 -4.26206 -4.2978673 -4.3129935 -4.3150077 -4.3176126 -4.3240566 -4.3277769 -4.3272467 -4.3262453 -4.3218422 -4.3155885]]...]
INFO - root - 2017-12-06 04:53:43.172322: step 2710, loss = 2.11, batch loss = 2.05 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:59s remains)
INFO - root - 2017-12-06 04:53:45.371335: step 2720, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:10m:09s remains)
INFO - root - 2017-12-06 04:53:47.554600: step 2730, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:26s remains)
INFO - root - 2017-12-06 04:53:49.729860: step 2740, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:26s remains)
INFO - root - 2017-12-06 04:53:51.946618: step 2750, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:53s remains)
INFO - root - 2017-12-06 04:53:54.154098: step 2760, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.237 sec/batch; 21h:41m:55s remains)
INFO - root - 2017-12-06 04:53:56.374162: step 2770, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:00s remains)
INFO - root - 2017-12-06 04:53:58.538943: step 2780, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:40s remains)
INFO - root - 2017-12-06 04:54:00.697019: step 2790, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:15s remains)
INFO - root - 2017-12-06 04:54:02.901899: step 2800, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:27s remains)
2017-12-06 04:54:03.374522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1407747 -4.1387243 -4.1419177 -4.1458735 -4.1564951 -4.1662297 -4.1420932 -4.0864248 -4.0288277 -4.0229454 -4.0749607 -4.14965 -4.2173409 -4.2693071 -4.3067331][-4.1429124 -4.124166 -4.1150293 -4.1105709 -4.1206985 -4.1340384 -4.1187191 -4.070817 -4.0181961 -4.0128589 -4.0695772 -4.1495724 -4.2161641 -4.2644982 -4.296989][-4.1771388 -4.1494393 -4.1238213 -4.101759 -4.1002688 -4.1140022 -4.1114349 -4.0791454 -4.0348234 -4.0318184 -4.0906949 -4.1662083 -4.2263122 -4.2675862 -4.2971349][-4.2061243 -4.1765709 -4.1426163 -4.10778 -4.0916343 -4.0902662 -4.0889082 -4.0715566 -4.0394363 -4.0438871 -4.1094618 -4.1840091 -4.2375712 -4.2744484 -4.3041472][-4.2253885 -4.1975465 -4.1609387 -4.1172132 -4.0857739 -4.05968 -4.0419688 -4.028605 -4.0062041 -4.0192904 -4.0955462 -4.1780381 -4.2308631 -4.268568 -4.3020792][-4.2324457 -4.2092195 -4.1708989 -4.11102 -4.0500097 -3.9985161 -3.9736443 -3.9610651 -3.939827 -3.9625089 -4.0547624 -4.1533833 -4.2147222 -4.2564235 -4.2957778][-4.2289348 -4.2089849 -4.1674957 -4.0965872 -4.0037279 -3.9255381 -3.9046566 -3.899652 -3.8832874 -3.9162688 -4.0214715 -4.133801 -4.2029819 -4.2493315 -4.2921538][-4.2252011 -4.2068295 -4.1596808 -4.0883265 -3.9853 -3.8923907 -3.8736198 -3.8741181 -3.8669593 -3.9049406 -4.011857 -4.1250396 -4.1976924 -4.2484117 -4.2932448][-4.2231507 -4.2029748 -4.1591849 -4.1068983 -4.0407648 -3.9754248 -3.9561448 -3.940145 -3.920856 -3.9417353 -4.0323739 -4.1364388 -4.2042413 -4.2520795 -4.295269][-4.2087383 -4.1821089 -4.1413865 -4.1122074 -4.09743 -4.0788527 -4.066648 -4.0357227 -4.0006995 -3.9979205 -4.0640869 -4.1538606 -4.211802 -4.2526927 -4.2919688][-4.1790519 -4.1450863 -4.1130414 -4.0993652 -4.1052656 -4.1115527 -4.1071892 -4.0797119 -4.0465193 -4.0402589 -4.0939713 -4.1694942 -4.2194562 -4.2550449 -4.291697][-4.1555743 -4.1197619 -4.097147 -4.0907288 -4.0967727 -4.1025529 -4.0997825 -4.0798459 -4.0628715 -4.0674696 -4.1158781 -4.1809645 -4.224206 -4.2582097 -4.2963347][-4.1597524 -4.1264615 -4.1096325 -4.1082516 -4.1131229 -4.1113553 -4.0987182 -4.080646 -4.0749116 -4.08831 -4.1332059 -4.1897311 -4.2276554 -4.2601628 -4.3003979][-4.1768193 -4.1421957 -4.1267953 -4.133009 -4.1407375 -4.1325006 -4.1138344 -4.0995336 -4.0973043 -4.1112056 -4.153091 -4.2004457 -4.2309961 -4.2624369 -4.30576][-4.1809654 -4.1469979 -4.1244087 -4.1275034 -4.137898 -4.1380291 -4.1262727 -4.117362 -4.1224813 -4.1407309 -4.1794438 -4.2161074 -4.2394781 -4.2672935 -4.3090835]]...]
INFO - root - 2017-12-06 04:54:05.594813: step 2810, loss = 2.06, batch loss = 2.01 (35.6 examples/sec; 0.224 sec/batch; 20h:33m:07s remains)
INFO - root - 2017-12-06 04:54:07.831950: step 2820, loss = 2.04, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:44s remains)
INFO - root - 2017-12-06 04:54:10.000441: step 2830, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 19h:59m:23s remains)
INFO - root - 2017-12-06 04:54:12.189098: step 2840, loss = 2.03, batch loss = 1.98 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:28s remains)
INFO - root - 2017-12-06 04:54:14.363577: step 2850, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:59s remains)
INFO - root - 2017-12-06 04:54:16.525219: step 2860, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:48m:10s remains)
INFO - root - 2017-12-06 04:54:18.673133: step 2870, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:28s remains)
INFO - root - 2017-12-06 04:54:21.178383: step 2880, loss = 2.06, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:41m:16s remains)
INFO - root - 2017-12-06 04:54:23.389139: step 2890, loss = 2.04, batch loss = 1.98 (33.9 examples/sec; 0.236 sec/batch; 21h:37m:08s remains)
INFO - root - 2017-12-06 04:54:25.617680: step 2900, loss = 2.08, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:28s remains)
2017-12-06 04:54:30.616833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2865109 -4.2985091 -4.3080907 -4.3109832 -4.3035073 -4.2813444 -4.2452273 -4.2177639 -4.2083354 -4.2197347 -4.2442727 -4.2667694 -4.273994 -4.2647023 -4.2512932][-4.2971869 -4.3047657 -4.3048925 -4.2993479 -4.2900238 -4.2699957 -4.2340021 -4.2057967 -4.1982694 -4.2166677 -4.25142 -4.2856808 -4.3006325 -4.2932458 -4.2766418][-4.3010144 -4.3001719 -4.2857647 -4.2723207 -4.2674704 -4.2608762 -4.236042 -4.2120209 -4.2037153 -4.2216949 -4.2603273 -4.2996759 -4.3186393 -4.3129673 -4.2947059][-4.3042855 -4.2950397 -4.2704058 -4.2550387 -4.2587218 -4.268383 -4.2587919 -4.2401333 -4.2251821 -4.2335782 -4.2653861 -4.2997108 -4.3163772 -4.3123813 -4.2950139][-4.3124547 -4.2980728 -4.2701836 -4.2557878 -4.2653427 -4.2832689 -4.2844882 -4.2697916 -4.2497706 -4.2457995 -4.2641244 -4.2874713 -4.2972503 -4.2919035 -4.2756724][-4.3127928 -4.3020234 -4.2786837 -4.2675548 -4.2766647 -4.2918954 -4.2968283 -4.2871156 -4.2668037 -4.2537665 -4.2580724 -4.2697988 -4.2709527 -4.2623644 -4.2472329][-4.3093271 -4.3066363 -4.2910986 -4.2840285 -4.289649 -4.2989268 -4.3036265 -4.3001037 -4.2844167 -4.2687464 -4.2640491 -4.2654848 -4.2577252 -4.2428908 -4.2255139][-4.3031392 -4.3081112 -4.3004041 -4.2963972 -4.2970891 -4.2990685 -4.3027139 -4.3031659 -4.2947617 -4.2862725 -4.2843065 -4.2847128 -4.2735138 -4.2533827 -4.2320223][-4.2941685 -4.30701 -4.3057232 -4.300262 -4.2915874 -4.2845097 -4.2866583 -4.2892327 -4.2871771 -4.2874951 -4.2931724 -4.298151 -4.2890453 -4.2702408 -4.2507467][-4.2754221 -4.300818 -4.306303 -4.2955861 -4.2750764 -4.2590137 -4.2596664 -4.2627606 -4.2629046 -4.2676935 -4.2768517 -4.2838221 -4.279007 -4.267159 -4.2575016][-4.2472644 -4.2866945 -4.2987 -4.2827106 -4.2518058 -4.2296247 -4.2315445 -4.2361822 -4.2357306 -4.23848 -4.2446704 -4.2512884 -4.2513285 -4.2479787 -4.2500648][-4.2316747 -4.2804842 -4.2948651 -4.2750974 -4.2389331 -4.2127686 -4.2151728 -4.2225394 -4.21985 -4.2151952 -4.2118444 -4.2118487 -4.2103577 -4.213 -4.2269392][-4.2460666 -4.2912426 -4.300323 -4.2782297 -4.2430468 -4.2178912 -4.2211747 -4.2291732 -4.2238011 -4.2096024 -4.1927586 -4.1786003 -4.1690869 -4.1751304 -4.19743][-4.2760792 -4.3051305 -4.3038216 -4.2787924 -4.249959 -4.2325773 -4.2388105 -4.2481623 -4.2446651 -4.2279406 -4.201407 -4.1745224 -4.1583495 -4.1660089 -4.1876245][-4.2940207 -4.3048387 -4.2953191 -4.2735052 -4.256968 -4.249826 -4.2573247 -4.2661381 -4.2652793 -4.24975 -4.2197127 -4.1864457 -4.1712437 -4.1816812 -4.1953235]]...]
INFO - root - 2017-12-06 04:54:32.903640: step 2910, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:35m:43s remains)
INFO - root - 2017-12-06 04:54:35.069871: step 2920, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:04s remains)
INFO - root - 2017-12-06 04:54:37.277461: step 2930, loss = 2.10, batch loss = 2.04 (34.3 examples/sec; 0.233 sec/batch; 21h:20m:38s remains)
INFO - root - 2017-12-06 04:54:39.516662: step 2940, loss = 2.05, batch loss = 2.00 (37.3 examples/sec; 0.214 sec/batch; 19h:37m:30s remains)
INFO - root - 2017-12-06 04:54:41.687981: step 2950, loss = 2.09, batch loss = 2.03 (37.0 examples/sec; 0.216 sec/batch; 19h:48m:30s remains)
INFO - root - 2017-12-06 04:54:43.876244: step 2960, loss = 2.10, batch loss = 2.04 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:33s remains)
INFO - root - 2017-12-06 04:54:46.034863: step 2970, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:18m:57s remains)
INFO - root - 2017-12-06 04:54:48.211734: step 2980, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:45m:07s remains)
INFO - root - 2017-12-06 04:54:50.403511: step 2990, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:22m:26s remains)
INFO - root - 2017-12-06 04:54:52.600622: step 3000, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:29s remains)
2017-12-06 04:54:55.113847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3429708 -4.3431954 -4.3400717 -4.3322325 -4.3139958 -4.2821474 -4.2347879 -4.174859 -4.1098967 -4.0307341 -3.936933 -3.8484528 -3.8161778 -3.86553 -3.9450724][-4.3466291 -4.3491488 -4.3480229 -4.3418274 -4.3264661 -4.2986469 -4.2553434 -4.20082 -4.1424332 -4.0693703 -3.9812474 -3.9004369 -3.8683982 -3.9001389 -3.9608488][-4.350996 -4.3556952 -4.3572693 -4.3543959 -4.343184 -4.3217411 -4.289443 -4.2524061 -4.2153049 -4.1686835 -4.10757 -4.0476947 -4.0183711 -4.028615 -4.0632377][-4.3543353 -4.3601618 -4.3633451 -4.3614855 -4.3501506 -4.3291631 -4.3032427 -4.2804823 -4.2654824 -4.2524686 -4.2277145 -4.1980529 -4.1796937 -4.1795616 -4.1914992][-4.3539534 -4.3595433 -4.3612514 -4.3560481 -4.337337 -4.3081346 -4.2779751 -4.2569427 -4.2546797 -4.2686234 -4.2811909 -4.2837334 -4.2824278 -4.2845483 -4.2870512][-4.3507552 -4.3537049 -4.3492675 -4.3326044 -4.2976141 -4.2493467 -4.202683 -4.17378 -4.1793513 -4.2187581 -4.2700696 -4.3076229 -4.3271217 -4.3370628 -4.3380642][-4.3472929 -4.3460026 -4.3319936 -4.2989578 -4.2414002 -4.1636796 -4.0857916 -4.0343704 -4.0410995 -4.1066618 -4.1971741 -4.2727723 -4.32001 -4.3453789 -4.351768][-4.3454142 -4.3424497 -4.3219042 -4.2773027 -4.2037644 -4.1002145 -3.9845541 -3.8960025 -3.8923278 -3.9833851 -4.1089954 -4.2170982 -4.28986 -4.330862 -4.3464417][-4.3506622 -4.3482533 -4.3278279 -4.2836223 -4.2118025 -4.1052856 -3.9823282 -3.8821909 -3.870954 -3.9656181 -4.093338 -4.205657 -4.2814059 -4.3228712 -4.3413525][-4.356658 -4.3543086 -4.33854 -4.3070073 -4.2572603 -4.1826162 -4.0974679 -4.032506 -4.0249243 -4.0840864 -4.1692762 -4.24942 -4.3034935 -4.3310637 -4.3438096][-4.3393106 -4.3321609 -4.3195825 -4.3021 -4.2780824 -4.2421045 -4.2001472 -4.1688776 -4.164247 -4.1923466 -4.2383375 -4.2864823 -4.320467 -4.3386154 -4.3467488][-4.2910466 -4.2782335 -4.2691445 -4.2643113 -4.2608147 -4.2537947 -4.2399383 -4.2244534 -4.2177782 -4.2275372 -4.252789 -4.2868342 -4.316977 -4.3382635 -4.3485079][-4.2265148 -4.2097349 -4.2040243 -4.2091808 -4.2212439 -4.2329025 -4.2330227 -4.22055 -4.2077432 -4.2064309 -4.2220149 -4.2524896 -4.2865372 -4.3166728 -4.335722][-4.1620007 -4.1428561 -4.1364241 -4.1444874 -4.1645961 -4.1868739 -4.1935277 -4.1795621 -4.1591454 -4.1485167 -4.1580844 -4.1877584 -4.2270784 -4.2661123 -4.2966428][-4.1269379 -4.11252 -4.1066294 -4.1108236 -4.1253581 -4.1446071 -4.1473532 -4.1309156 -4.1061373 -4.0879784 -4.0917368 -4.1194549 -4.1591287 -4.2002211 -4.2350955]]...]
INFO - root - 2017-12-06 04:54:57.329263: step 3010, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:41s remains)
INFO - root - 2017-12-06 04:54:59.513609: step 3020, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:48m:24s remains)
INFO - root - 2017-12-06 04:55:01.707761: step 3030, loss = 2.06, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:50s remains)
INFO - root - 2017-12-06 04:55:03.896514: step 3040, loss = 2.06, batch loss = 2.01 (36.4 examples/sec; 0.219 sec/batch; 20h:05m:14s remains)
INFO - root - 2017-12-06 04:55:06.124614: step 3050, loss = 2.04, batch loss = 1.99 (37.3 examples/sec; 0.214 sec/batch; 19h:37m:22s remains)
INFO - root - 2017-12-06 04:55:08.258325: step 3060, loss = 2.07, batch loss = 2.02 (37.3 examples/sec; 0.214 sec/batch; 19h:36m:21s remains)
INFO - root - 2017-12-06 04:55:10.410331: step 3070, loss = 2.07, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:09m:46s remains)
INFO - root - 2017-12-06 04:55:12.613631: step 3080, loss = 2.07, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:42s remains)
INFO - root - 2017-12-06 04:55:14.851453: step 3090, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:30s remains)
INFO - root - 2017-12-06 04:55:17.061552: step 3100, loss = 2.06, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:39m:44s remains)
2017-12-06 04:55:20.490367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2293482 -4.2107754 -4.1790938 -4.1612649 -4.1705341 -4.1853371 -4.1751442 -4.1463428 -4.1353955 -4.1570683 -4.186235 -4.1960874 -4.1730742 -4.1383018 -4.1354861][-4.20811 -4.1923056 -4.16572 -4.1544709 -4.1676617 -4.1802268 -4.1677227 -4.1410103 -4.1358171 -4.1651273 -4.202805 -4.2173882 -4.1978064 -4.1647191 -4.1577415][-4.1869059 -4.1839185 -4.1743197 -4.1768622 -4.1939797 -4.204412 -4.1933904 -4.1752067 -4.1782703 -4.2120519 -4.2505207 -4.2651191 -4.2484889 -4.2172117 -4.2001009][-4.197648 -4.207551 -4.21441 -4.2256975 -4.2392154 -4.2415905 -4.2285209 -4.2176304 -4.2262435 -4.2536778 -4.2793264 -4.2865472 -4.2735825 -4.2493935 -4.2289023][-4.215807 -4.2257566 -4.2362828 -4.2467346 -4.2485032 -4.2374 -4.2182851 -4.2130327 -4.22772 -4.2490163 -4.2626486 -4.264173 -4.2566795 -4.2423768 -4.2271357][-4.2183728 -4.2170072 -4.2226076 -4.2270017 -4.214601 -4.1834192 -4.1500635 -4.1471138 -4.1718869 -4.1979885 -4.2098637 -4.211288 -4.2098618 -4.2073064 -4.2042651][-4.1967049 -4.1776862 -4.1759753 -4.174552 -4.1490865 -4.0969815 -4.0467782 -4.0472555 -4.0905476 -4.1341577 -4.1546068 -4.1622486 -4.1698451 -4.1786737 -4.18791][-4.1697941 -4.1391897 -4.1323133 -4.12915 -4.0993929 -4.0384073 -3.9826567 -3.9891763 -4.0481958 -4.1098394 -4.1433868 -4.1599107 -4.1756186 -4.1901975 -4.202126][-4.1688838 -4.1471758 -4.1488776 -4.1538982 -4.1332593 -4.0848618 -4.041337 -4.048171 -4.1005511 -4.1574559 -4.1889353 -4.2031393 -4.212769 -4.2213154 -4.2280445][-4.1906872 -4.187892 -4.2029591 -4.2180963 -4.2124481 -4.1851096 -4.1585212 -4.1618247 -4.1973085 -4.2358236 -4.2530322 -4.2540083 -4.2488914 -4.245358 -4.2454205][-4.2209282 -4.2307544 -4.2513909 -4.2704358 -4.2739367 -4.2605138 -4.2452626 -4.2444372 -4.2639713 -4.2866817 -4.29225 -4.2839112 -4.2676468 -4.2549458 -4.2502003][-4.2539034 -4.2666802 -4.2850156 -4.3002138 -4.3058619 -4.3003597 -4.2904959 -4.2848797 -4.2923594 -4.3026013 -4.3012605 -4.2892728 -4.2711205 -4.2575588 -4.2530994][-4.2797503 -4.2887931 -4.2984142 -4.3052688 -4.3080163 -4.3035631 -4.2924004 -4.2805171 -4.2798204 -4.2859507 -4.2872667 -4.2792969 -4.2658796 -4.2574959 -4.2583761][-4.2825937 -4.2860532 -4.2853 -4.2814379 -4.279985 -4.2752619 -4.2616048 -4.24532 -4.2396007 -4.2471914 -4.257195 -4.2574253 -4.2509608 -4.24839 -4.2531414][-4.2664895 -4.2651796 -4.2549415 -4.2424927 -4.2391429 -4.2383657 -4.2284741 -4.2125273 -4.2054567 -4.2163587 -4.235611 -4.243135 -4.240221 -4.2396116 -4.2443905]]...]
INFO - root - 2017-12-06 04:55:22.781562: step 3110, loss = 2.05, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:51s remains)
INFO - root - 2017-12-06 04:55:24.981938: step 3120, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:44m:16s remains)
INFO - root - 2017-12-06 04:55:27.174454: step 3130, loss = 2.08, batch loss = 2.02 (38.4 examples/sec; 0.208 sec/batch; 19h:03m:56s remains)
INFO - root - 2017-12-06 04:55:29.342574: step 3140, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:56m:14s remains)
INFO - root - 2017-12-06 04:55:31.523272: step 3150, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:07s remains)
INFO - root - 2017-12-06 04:55:33.717710: step 3160, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:38m:13s remains)
INFO - root - 2017-12-06 04:55:35.870804: step 3170, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:43m:52s remains)
INFO - root - 2017-12-06 04:55:38.077989: step 3180, loss = 2.08, batch loss = 2.02 (34.3 examples/sec; 0.233 sec/batch; 21h:18m:58s remains)
INFO - root - 2017-12-06 04:55:40.298808: step 3190, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.221 sec/batch; 20h:10m:49s remains)
INFO - root - 2017-12-06 04:55:42.444680: step 3200, loss = 2.06, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:18s remains)
2017-12-06 04:55:42.826714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3126688 -4.3013735 -4.2907543 -4.2868185 -4.2915993 -4.2964988 -4.2934151 -4.288919 -4.2825427 -4.2797403 -4.2782626 -4.2784243 -4.2831969 -4.291748 -4.3013477][-4.296454 -4.2777452 -4.2609692 -4.2526059 -4.25916 -4.2677679 -4.2646079 -4.2564321 -4.2497554 -4.2488151 -4.2519937 -4.2564535 -4.265769 -4.2781444 -4.2873073][-4.2787066 -4.2513733 -4.2281132 -4.2163119 -4.2226462 -4.2308049 -4.2243805 -4.2140126 -4.2104435 -4.2149367 -4.2260056 -4.2366071 -4.2504945 -4.2665572 -4.2758236][-4.2587924 -4.2240896 -4.1944141 -4.1804543 -4.1833544 -4.1844888 -4.1661973 -4.1512346 -4.1514549 -4.1615381 -4.1803789 -4.1992974 -4.2229857 -4.248208 -4.2632065][-4.2357144 -4.1939745 -4.1558862 -4.1355124 -4.1298404 -4.120214 -4.0886068 -4.0723825 -4.0774035 -4.09428 -4.11731 -4.1434731 -4.1832204 -4.2240629 -4.2490363][-4.2081881 -4.1626253 -4.1177149 -4.0877981 -4.0677996 -4.0423894 -3.9976125 -3.9752116 -3.9797742 -3.9971895 -4.0208178 -4.0554075 -4.1180172 -4.1859679 -4.2311335][-4.1629539 -4.11069 -4.0591531 -4.01996 -3.9899871 -3.9549651 -3.9008961 -3.8648386 -3.8590355 -3.8704939 -3.8945377 -3.9441888 -4.035852 -4.1374569 -4.2065334][-4.1104846 -4.0555038 -4.0041127 -3.963551 -3.9298422 -3.8921833 -3.8317595 -3.7841349 -3.7748103 -3.7900681 -3.8240118 -3.8878999 -3.9998207 -4.1195807 -4.2006626][-4.0987363 -4.0521984 -4.010252 -3.9758308 -3.9437821 -3.907058 -3.8470738 -3.8048992 -3.8088779 -3.8368897 -3.8794377 -3.9428692 -4.0423222 -4.1482348 -4.2211962][-4.130794 -4.1002536 -4.0697823 -4.0440431 -4.0195131 -3.9899666 -3.9444451 -3.9175758 -3.9323702 -3.9641387 -4.0050235 -4.0571671 -4.1283126 -4.2040505 -4.2581129][-4.1797152 -4.1605163 -4.1432061 -4.1307974 -4.1170416 -4.0974922 -4.0702286 -4.0581932 -4.0743876 -4.0995517 -4.1322007 -4.1702123 -4.2162337 -4.2651834 -4.298964][-4.2334657 -4.2249508 -4.2184515 -4.21458 -4.2072473 -4.1961942 -4.1838579 -4.1840148 -4.1975994 -4.2148094 -4.2381325 -4.2627449 -4.2884994 -4.3141379 -4.3298497][-4.2697153 -4.2680731 -4.2686353 -4.2693319 -4.2676649 -4.2637038 -4.258985 -4.2611227 -4.270668 -4.282711 -4.298192 -4.3139949 -4.3272123 -4.3380227 -4.34369][-4.298944 -4.2991405 -4.3012967 -4.3045626 -4.3062267 -4.3060393 -4.3037567 -4.30451 -4.3101888 -4.3178978 -4.3266034 -4.3354592 -4.3416381 -4.3460507 -4.34835][-4.3240523 -4.3235192 -4.32455 -4.3271866 -4.3287816 -4.3290882 -4.327445 -4.3273993 -4.3302274 -4.3342242 -4.3385406 -4.3431358 -4.346664 -4.3499718 -4.3519177]]...]
INFO - root - 2017-12-06 04:55:44.986349: step 3210, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:15s remains)
INFO - root - 2017-12-06 04:55:47.196314: step 3220, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:11s remains)
INFO - root - 2017-12-06 04:55:49.448586: step 3230, loss = 2.05, batch loss = 2.00 (34.0 examples/sec; 0.236 sec/batch; 21h:32m:57s remains)
INFO - root - 2017-12-06 04:55:51.656888: step 3240, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:33s remains)
INFO - root - 2017-12-06 04:55:53.843103: step 3250, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:46s remains)
INFO - root - 2017-12-06 04:55:56.040841: step 3260, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:02s remains)
INFO - root - 2017-12-06 04:55:58.204614: step 3270, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 20h:31m:42s remains)
INFO - root - 2017-12-06 04:56:00.394734: step 3280, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:09m:05s remains)
INFO - root - 2017-12-06 04:56:02.589068: step 3290, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:08m:53s remains)
INFO - root - 2017-12-06 04:56:04.815429: step 3300, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:49m:49s remains)
2017-12-06 04:56:05.252523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2317281 -4.247333 -4.2718863 -4.2962322 -4.3151989 -4.3276725 -4.3372908 -4.3371682 -4.3296957 -4.3204446 -4.3065453 -4.2920547 -4.2845654 -4.2809544 -4.271173][-4.220036 -4.2436118 -4.2704406 -4.2921629 -4.3074 -4.3202329 -4.3315287 -4.3375082 -4.3405385 -4.34306 -4.3429627 -4.3391361 -4.329617 -4.3100429 -4.2780371][-4.22853 -4.2484412 -4.2661881 -4.2764344 -4.2817855 -4.2876053 -4.2937636 -4.3008528 -4.3120174 -4.3262253 -4.3399038 -4.3496051 -4.3488731 -4.3301034 -4.2943726][-4.2553949 -4.2602243 -4.2594323 -4.2500911 -4.23659 -4.22633 -4.2202673 -4.2229133 -4.239357 -4.2655497 -4.2958717 -4.3234959 -4.3408337 -4.337697 -4.3155651][-4.2788882 -4.2675 -4.2465 -4.2138014 -4.1745591 -4.139389 -4.1134419 -4.1076231 -4.1311722 -4.172811 -4.2214928 -4.2703333 -4.3100319 -4.3251548 -4.318203][-4.2936921 -4.2746549 -4.2386103 -4.1840258 -4.1188192 -4.0542183 -4.0004849 -3.9831822 -4.0199919 -4.0840063 -4.1531725 -4.2199087 -4.2744889 -4.299674 -4.2999625][-4.3086448 -4.2929654 -4.2519555 -4.1841679 -4.0993977 -4.0074906 -3.923501 -3.8932068 -3.9427876 -4.0270905 -4.1134243 -4.1915307 -4.2509904 -4.2787118 -4.2807198][-4.3243666 -4.3174562 -4.2818985 -4.2150736 -4.1277084 -4.02843 -3.9328973 -3.8974085 -3.9477203 -4.033391 -4.1195478 -4.1941967 -4.2473178 -4.2715178 -4.2747793][-4.341639 -4.3404717 -4.3125615 -4.2565746 -4.1841512 -4.1036425 -4.0286303 -4.0035477 -4.0421581 -4.1068969 -4.1722236 -4.2289085 -4.2687559 -4.288662 -4.2939949][-4.3532653 -4.3519273 -4.3282051 -4.28489 -4.2342544 -4.1839695 -4.1441236 -4.1368361 -4.161994 -4.1987543 -4.2358375 -4.26963 -4.2961049 -4.314846 -4.3239479][-4.3492818 -4.3436079 -4.3233428 -4.2935247 -4.2655292 -4.2446823 -4.2362018 -4.2437663 -4.258287 -4.2700453 -4.2805233 -4.2924151 -4.3080487 -4.3273907 -4.342072][-4.3327146 -4.3205004 -4.3034539 -4.2886486 -4.2823806 -4.2852902 -4.2979307 -4.3136673 -4.3198314 -4.3126941 -4.2995677 -4.2896175 -4.294395 -4.3130722 -4.3334041][-4.3127604 -4.2959146 -4.2810745 -4.2789063 -4.2890754 -4.3070803 -4.3295703 -4.3474956 -4.3488932 -4.3308969 -4.3007841 -4.273037 -4.2660437 -4.2815943 -4.3063254][-4.2989988 -4.2854156 -4.2744255 -4.27846 -4.2946944 -4.315443 -4.3364034 -4.3515186 -4.3525972 -4.3347087 -4.30074 -4.2653732 -4.249404 -4.258018 -4.2821732][-4.2927456 -4.289228 -4.2862606 -4.2937927 -4.3095789 -4.3256359 -4.3389511 -4.3473654 -4.34785 -4.3348141 -4.3073792 -4.2759514 -4.2579637 -4.2609167 -4.2822495]]...]
INFO - root - 2017-12-06 04:56:07.444665: step 3310, loss = 2.07, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:18m:44s remains)
INFO - root - 2017-12-06 04:56:09.732550: step 3320, loss = 2.09, batch loss = 2.03 (34.4 examples/sec; 0.232 sec/batch; 21h:14m:02s remains)
INFO - root - 2017-12-06 04:56:11.970081: step 3330, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:07m:33s remains)
INFO - root - 2017-12-06 04:56:14.159443: step 3340, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:56m:18s remains)
INFO - root - 2017-12-06 04:56:16.339305: step 3350, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:43s remains)
INFO - root - 2017-12-06 04:56:18.533115: step 3360, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:58m:44s remains)
INFO - root - 2017-12-06 04:56:20.699250: step 3370, loss = 2.04, batch loss = 1.98 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:02s remains)
INFO - root - 2017-12-06 04:56:22.906782: step 3380, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:13s remains)
INFO - root - 2017-12-06 04:56:25.124571: step 3390, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:07s remains)
INFO - root - 2017-12-06 04:56:27.327469: step 3400, loss = 2.04, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:52m:40s remains)
2017-12-06 04:56:27.746898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464809 -4.2630038 -4.29347 -4.30525 -4.3113823 -4.3161683 -4.3068709 -4.2901297 -4.2798767 -4.2760243 -4.2761025 -4.2792716 -4.276875 -4.2604456 -4.2235818][-4.2347903 -4.2466578 -4.2777629 -4.2875061 -4.2926598 -4.2938609 -4.2810674 -4.2592 -4.2455368 -4.2398114 -4.24082 -4.2472286 -4.2449675 -4.2216578 -4.1687403][-4.2021012 -4.2059021 -4.2399683 -4.2536273 -4.2601137 -4.2565775 -4.2370405 -4.208251 -4.1924758 -4.188273 -4.1968794 -4.2132621 -4.2147422 -4.1902628 -4.1333766][-4.1532497 -4.1451907 -4.1858239 -4.2112708 -4.21939 -4.2070951 -4.17583 -4.138948 -4.1236777 -4.1274328 -4.1521993 -4.1859674 -4.1985192 -4.1820593 -4.1343][-4.0949588 -4.0765257 -4.1295543 -4.1722379 -4.1812148 -4.1558695 -4.1056948 -4.0571184 -4.0471182 -4.0692186 -4.1178083 -4.1683822 -4.1922317 -4.1882381 -4.1604338][-4.0557837 -4.0347896 -4.0973568 -4.1427832 -4.1443362 -4.1021194 -4.0267735 -3.9568884 -3.960134 -4.0147638 -4.091435 -4.1530881 -4.1846275 -4.1940475 -4.190495][-4.0711966 -4.0622559 -4.1197395 -4.1452961 -4.1230969 -4.0586243 -3.9535742 -3.8605785 -3.8848796 -3.9751849 -4.0712533 -4.1350565 -4.168066 -4.18601 -4.1992259][-4.1253643 -4.1301212 -4.1691461 -4.1654353 -4.1177425 -4.0345836 -3.9067628 -3.8111537 -3.8580749 -3.9740381 -4.0718307 -4.1309266 -4.1649442 -4.1843014 -4.1962061][-4.1774588 -4.1859579 -4.211113 -4.1932249 -4.135819 -4.0484819 -3.930263 -3.8649321 -3.9219494 -4.0248146 -4.1027966 -4.1513262 -4.18284 -4.1989937 -4.198257][-4.2113967 -4.2174311 -4.2338042 -4.2184877 -4.16779 -4.0967765 -4.018826 -3.9974823 -4.0484982 -4.1148324 -4.16273 -4.195435 -4.2192488 -4.226965 -4.2123823][-4.2516389 -4.2519655 -4.2601285 -4.2497282 -4.2153244 -4.1701961 -4.1296124 -4.1312757 -4.1657815 -4.1989961 -4.2250118 -4.2434921 -4.2547517 -4.2500463 -4.2298875][-4.2905436 -4.285872 -4.2830763 -4.2714891 -4.2502646 -4.2258153 -4.2099857 -4.2172375 -4.2365088 -4.2555003 -4.2759805 -4.286016 -4.282382 -4.2667303 -4.2480907][-4.3156452 -4.30873 -4.2995291 -4.2870893 -4.2730002 -4.2610574 -4.2553697 -4.2636609 -4.2759418 -4.2911491 -4.31077 -4.316411 -4.3051653 -4.2871985 -4.2710567][-4.3169775 -4.3117704 -4.304471 -4.2961917 -4.287591 -4.2816229 -4.2807384 -4.2907162 -4.3033686 -4.3174634 -4.32984 -4.3298855 -4.3169274 -4.3014231 -4.2872548][-4.3060403 -4.3028526 -4.3016176 -4.3009119 -4.2977552 -4.29322 -4.2936397 -4.3041615 -4.3155546 -4.3247533 -4.3281336 -4.3248906 -4.3145847 -4.3029203 -4.2931266]]...]
INFO - root - 2017-12-06 04:56:29.987148: step 3410, loss = 2.04, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:22s remains)
INFO - root - 2017-12-06 04:56:32.162955: step 3420, loss = 2.05, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 20h:38m:16s remains)
INFO - root - 2017-12-06 04:56:34.432445: step 3430, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 20h:30m:49s remains)
INFO - root - 2017-12-06 04:56:36.689285: step 3440, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:38m:11s remains)
INFO - root - 2017-12-06 04:56:38.908195: step 3450, loss = 2.11, batch loss = 2.05 (36.5 examples/sec; 0.219 sec/batch; 20h:02m:10s remains)
INFO - root - 2017-12-06 04:56:41.092465: step 3460, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.212 sec/batch; 19h:20m:08s remains)
INFO - root - 2017-12-06 04:56:43.276041: step 3470, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:20s remains)
INFO - root - 2017-12-06 04:56:45.456067: step 3480, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:25m:58s remains)
INFO - root - 2017-12-06 04:56:47.609295: step 3490, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:27s remains)
INFO - root - 2017-12-06 04:56:49.811684: step 3500, loss = 2.03, batch loss = 1.98 (36.6 examples/sec; 0.219 sec/batch; 19h:58m:50s remains)
2017-12-06 04:56:50.173244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3413949 -4.342144 -4.3426461 -4.3418641 -4.3395419 -4.3377137 -4.3376274 -4.337759 -4.3396373 -4.3434143 -4.3482451 -4.3530288 -4.3547873 -4.3520169 -4.3479919][-4.3363295 -4.3339195 -4.3317914 -4.3300996 -4.3266282 -4.3220353 -4.3203468 -4.3211784 -4.3247037 -4.3282795 -4.3316922 -4.3362269 -4.339324 -4.3375115 -4.3351636][-4.3276758 -4.3226838 -4.31805 -4.3149762 -4.3093567 -4.3009586 -4.29518 -4.29383 -4.2965612 -4.29766 -4.29876 -4.3018231 -4.3053675 -4.3072209 -4.3107615][-4.3227806 -4.3147082 -4.30538 -4.2975736 -4.2869148 -4.2733808 -4.2623491 -4.2570806 -4.2562709 -4.255218 -4.2559848 -4.2599821 -4.2644658 -4.2696123 -4.2797379][-4.3141551 -4.3015285 -4.2865887 -4.2717314 -4.2567658 -4.2399116 -4.2257857 -4.2182765 -4.21419 -4.2120132 -4.2141223 -4.2213306 -4.2280731 -4.23622 -4.2497234][-4.3025541 -4.2874312 -4.2679391 -4.2486587 -4.2293329 -4.2080255 -4.1937351 -4.1912766 -4.1852064 -4.1802359 -4.1833196 -4.1955881 -4.2064533 -4.2158432 -4.2309294][-4.29277 -4.2773876 -4.2574925 -4.2366161 -4.2106333 -4.1786671 -4.1615329 -4.1636782 -4.16353 -4.1632295 -4.1687508 -4.18611 -4.199513 -4.2091 -4.2235923][-4.2885556 -4.2751751 -4.2563777 -4.2317181 -4.1943789 -4.1475496 -4.1264176 -4.1355152 -4.1498728 -4.1641092 -4.1796913 -4.2016163 -4.2139935 -4.219677 -4.2276397][-4.2845969 -4.2783222 -4.2597175 -4.224781 -4.1721926 -4.116559 -4.1007109 -4.1201272 -4.15211 -4.1821084 -4.2087793 -4.2277436 -4.2361107 -4.2339816 -4.2325177][-4.2864604 -4.2887583 -4.2712784 -4.2294755 -4.1691618 -4.1168671 -4.1117439 -4.1398196 -4.1771522 -4.208096 -4.2351561 -4.2515426 -4.257637 -4.2499042 -4.2423115][-4.2997313 -4.3100348 -4.2988105 -4.26372 -4.21128 -4.1675577 -4.1653261 -4.1894593 -4.2181697 -4.2416587 -4.2606883 -4.2711172 -4.2747741 -4.2671723 -4.2565556][-4.3118873 -4.3285413 -4.3278317 -4.3091474 -4.2733674 -4.2417035 -4.2367749 -4.2502766 -4.2660217 -4.281538 -4.29077 -4.291398 -4.2871466 -4.275538 -4.260766][-4.3109622 -4.3294744 -4.3356094 -4.3312721 -4.3123841 -4.2924418 -4.2841182 -4.2881665 -4.2967391 -4.3056717 -4.3100014 -4.3050728 -4.2935538 -4.2774282 -4.2571316][-4.3 -4.314683 -4.3207226 -4.3219137 -4.3123531 -4.2975211 -4.2886286 -4.2882371 -4.2961698 -4.3044639 -4.3084445 -4.30263 -4.287859 -4.269556 -4.249341][-4.2763681 -4.2835402 -4.2854552 -4.2874956 -4.2825336 -4.2717705 -4.2629867 -4.2610364 -4.2699466 -4.2778225 -4.2805891 -4.2769175 -4.2657008 -4.2528906 -4.2384272]]...]
INFO - root - 2017-12-06 04:56:52.388399: step 3510, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:16m:04s remains)
INFO - root - 2017-12-06 04:56:54.661233: step 3520, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:17s remains)
INFO - root - 2017-12-06 04:56:56.812085: step 3530, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:52s remains)
INFO - root - 2017-12-06 04:56:58.977472: step 3540, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:23s remains)
INFO - root - 2017-12-06 04:57:01.129675: step 3550, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:38m:46s remains)
INFO - root - 2017-12-06 04:57:03.349705: step 3560, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:23m:35s remains)
INFO - root - 2017-12-06 04:57:05.500080: step 3570, loss = 2.08, batch loss = 2.02 (38.3 examples/sec; 0.209 sec/batch; 19h:03m:58s remains)
INFO - root - 2017-12-06 04:57:07.706151: step 3580, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:38m:51s remains)
INFO - root - 2017-12-06 04:57:09.926734: step 3590, loss = 2.07, batch loss = 2.01 (33.6 examples/sec; 0.238 sec/batch; 21h:45m:11s remains)
INFO - root - 2017-12-06 04:57:12.130117: step 3600, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:42m:50s remains)
2017-12-06 04:57:12.532572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2682667 -4.2462173 -4.2249632 -4.1942258 -4.163105 -4.1513596 -4.156281 -4.167551 -4.1722975 -4.1825094 -4.2066603 -4.2249069 -4.2145357 -4.1765938 -4.1375465][-4.2529488 -4.2367754 -4.2267809 -4.2056561 -4.169116 -4.1409373 -4.1349726 -4.1496887 -4.1664896 -4.1831355 -4.2113471 -4.2312322 -4.2211113 -4.1811161 -4.1380486][-4.239212 -4.234592 -4.2382936 -4.2319384 -4.1987057 -4.1583452 -4.1382914 -4.1502476 -4.1760821 -4.2001534 -4.23015 -4.2509031 -4.2439876 -4.209661 -4.1727695][-4.2285333 -4.2347836 -4.2466187 -4.2519522 -4.2265277 -4.178966 -4.1491008 -4.1570816 -4.1893067 -4.2206526 -4.2533231 -4.275198 -4.2720079 -4.2469406 -4.21809][-4.2149768 -4.2286315 -4.2423267 -4.2514834 -4.2329836 -4.1849461 -4.1486464 -4.1547227 -4.19034 -4.2275958 -4.2634616 -4.2891827 -4.2919221 -4.2761922 -4.2522025][-4.1941156 -4.21003 -4.2208509 -4.2275529 -4.2123895 -4.1702266 -4.1354818 -4.1443081 -4.1855946 -4.2241712 -4.2592478 -4.2868052 -4.295733 -4.2861481 -4.2654481][-4.167171 -4.1789756 -4.1860423 -4.1906261 -4.1794648 -4.1479292 -4.122786 -4.1383371 -4.181179 -4.2153482 -4.2426867 -4.2657046 -4.2779889 -4.2743149 -4.2592072][-4.1427608 -4.1436319 -4.15027 -4.1594539 -4.1558361 -4.134397 -4.1213732 -4.1416893 -4.1796079 -4.2052612 -4.21773 -4.2295732 -4.2406263 -4.2424564 -4.2377429][-4.1369629 -4.1260972 -4.1341038 -4.1500568 -4.1550989 -4.142055 -4.13827 -4.1599827 -4.1872969 -4.198822 -4.19387 -4.1926908 -4.1999135 -4.2060146 -4.2134295][-4.1610994 -4.14292 -4.1455517 -4.1620913 -4.1731834 -4.1694164 -4.170867 -4.1902828 -4.2075834 -4.2056384 -4.1895471 -4.1790609 -4.1805367 -4.185564 -4.1997342][-4.1926022 -4.172833 -4.1677465 -4.1769371 -4.1880374 -4.18881 -4.1924815 -4.2081261 -4.218317 -4.211102 -4.1923008 -4.1821036 -4.1788816 -4.179985 -4.1951404][-4.214746 -4.1962662 -4.184586 -4.1841149 -4.1903534 -4.1928782 -4.1972265 -4.2077727 -4.211431 -4.2039552 -4.1888695 -4.1865363 -4.1851277 -4.1831927 -4.194829][-4.2267785 -4.2095194 -4.1940994 -4.1892929 -4.193439 -4.1950088 -4.1966391 -4.2006226 -4.1997457 -4.1926913 -4.1838355 -4.1878023 -4.1905303 -4.1896095 -4.1978621][-4.2341204 -4.2206821 -4.2066927 -4.2014942 -4.2055974 -4.2086015 -4.2084732 -4.205955 -4.1998849 -4.1919661 -4.1874833 -4.1924777 -4.1959677 -4.1965623 -4.2034245][-4.2465982 -4.2387972 -4.229866 -4.2271461 -4.2311072 -4.2337337 -4.2331247 -4.2273016 -4.2177963 -4.2087059 -4.2051964 -4.2099905 -4.2144475 -4.2167406 -4.2217293]]...]
INFO - root - 2017-12-06 04:57:14.761922: step 3610, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:34s remains)
INFO - root - 2017-12-06 04:57:16.939680: step 3620, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-06 04:57:19.184396: step 3630, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:54s remains)
INFO - root - 2017-12-06 04:57:21.369273: step 3640, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:46m:11s remains)
INFO - root - 2017-12-06 04:57:23.588377: step 3650, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:13s remains)
INFO - root - 2017-12-06 04:57:25.787892: step 3660, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:28s remains)
INFO - root - 2017-12-06 04:57:27.947146: step 3670, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.214 sec/batch; 19h:30m:12s remains)
INFO - root - 2017-12-06 04:57:30.083489: step 3680, loss = 2.06, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:42m:04s remains)
INFO - root - 2017-12-06 04:57:32.249400: step 3690, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:24m:58s remains)
INFO - root - 2017-12-06 04:57:34.380097: step 3700, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:10s remains)
2017-12-06 04:57:34.769554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2792649 -4.264111 -4.2626829 -4.2742414 -4.2810445 -4.2711663 -4.2564459 -4.2382 -4.2177558 -4.1923 -4.1824746 -4.1963935 -4.2077837 -4.2061167 -4.1995792][-4.268671 -4.2547412 -4.2580514 -4.2770648 -4.2844353 -4.2623973 -4.2300515 -4.2048011 -4.1836619 -4.1624146 -4.1591854 -4.179606 -4.1969414 -4.2004719 -4.197402][-4.2539787 -4.242909 -4.254209 -4.2776175 -4.2789068 -4.2427235 -4.1909075 -4.1553721 -4.1349573 -4.1260467 -4.1373181 -4.1641884 -4.1906714 -4.2016187 -4.2016487][-4.2503672 -4.2412887 -4.2521462 -4.2685966 -4.254601 -4.1954904 -4.112339 -4.0582662 -4.0506911 -4.0727119 -4.1017642 -4.135366 -4.1743765 -4.2036476 -4.2100134][-4.2567434 -4.243433 -4.2423491 -4.2392535 -4.1995625 -4.1006508 -3.9640036 -3.8820882 -3.9159143 -3.9953823 -4.0575223 -4.1061454 -4.1623182 -4.2093639 -4.2151041][-4.2671642 -4.2432423 -4.2204127 -4.1852455 -4.1009235 -3.9433925 -3.7419128 -3.6483746 -3.7708168 -3.9378266 -4.0383692 -4.1033235 -4.170094 -4.217906 -4.20766][-4.2680793 -4.2288609 -4.17722 -4.111073 -3.9913211 -3.7993941 -3.5685239 -3.5013123 -3.7254884 -3.9474745 -4.061192 -4.1292267 -4.1935067 -4.218277 -4.1687918][-4.2558908 -4.2014327 -4.1302295 -4.0621915 -3.9618289 -3.8105335 -3.6429272 -3.6411324 -3.8568044 -4.0418048 -4.1246538 -4.1689625 -4.2042108 -4.1858759 -4.0830374][-4.2305207 -4.1649647 -4.0921249 -4.05783 -4.022397 -3.9385662 -3.8475332 -3.8763475 -4.0248723 -4.1443729 -4.1906052 -4.19968 -4.1924109 -4.1327581 -3.9973466][-4.1906915 -4.1179042 -4.0558438 -4.0627947 -4.0823078 -4.051363 -4.0094776 -4.0413404 -4.1300392 -4.1960497 -4.2144151 -4.1957107 -4.1577606 -4.0777965 -3.9463375][-4.1472816 -4.0701427 -4.0200758 -4.0520039 -4.1060696 -4.1097145 -4.096662 -4.124289 -4.1761289 -4.2090039 -4.2085195 -4.1829805 -4.1401825 -4.0662189 -3.9571748][-4.1371388 -4.0684729 -4.0282702 -4.0647988 -4.1204329 -4.140696 -4.1470771 -4.168438 -4.1975775 -4.2125912 -4.2073507 -4.1859889 -4.1593442 -4.1104236 -4.0333009][-4.1714478 -4.1218319 -4.0923409 -4.1158895 -4.1544509 -4.1760111 -4.1879587 -4.2001915 -4.21771 -4.2286391 -4.2265029 -4.2118 -4.1976323 -4.1693268 -4.1258669][-4.221312 -4.1945229 -4.1794462 -4.1923213 -4.2100768 -4.2240095 -4.234077 -4.2422142 -4.2524204 -4.2566009 -4.2524443 -4.2407408 -4.2308793 -4.2169423 -4.1970229][-4.2593603 -4.2497988 -4.2439942 -4.2490067 -4.2521534 -4.2550573 -4.2617617 -4.2677789 -4.2714624 -4.2691565 -4.2634563 -4.253942 -4.2487 -4.2483788 -4.2462111]]...]
INFO - root - 2017-12-06 04:57:37.015796: step 3710, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:25s remains)
INFO - root - 2017-12-06 04:57:39.234096: step 3720, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:12s remains)
INFO - root - 2017-12-06 04:57:41.397088: step 3730, loss = 2.05, batch loss = 1.99 (36.6 examples/sec; 0.219 sec/batch; 19h:58m:15s remains)
INFO - root - 2017-12-06 04:57:43.568930: step 3740, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:29s remains)
INFO - root - 2017-12-06 04:57:45.734061: step 3750, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:28m:05s remains)
INFO - root - 2017-12-06 04:57:47.895387: step 3760, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 19h:56m:39s remains)
INFO - root - 2017-12-06 04:57:50.046549: step 3770, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:48m:03s remains)
INFO - root - 2017-12-06 04:57:52.192155: step 3780, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:17s remains)
INFO - root - 2017-12-06 04:57:54.369365: step 3790, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:22s remains)
INFO - root - 2017-12-06 04:57:56.509576: step 3800, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:44s remains)
2017-12-06 04:57:57.912459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1150784 -4.0439005 -4.0161595 -4.0635543 -4.1498733 -4.2023644 -4.2087655 -4.1971884 -4.188324 -4.1826715 -4.1906371 -4.2065563 -4.210515 -4.2095618 -4.2250743][-4.1254187 -4.069047 -4.0619912 -4.1202822 -4.1932878 -4.2239933 -4.2201858 -4.205297 -4.192977 -4.1870189 -4.2090836 -4.2426376 -4.2604609 -4.2635794 -4.26118][-4.16746 -4.1325312 -4.1428862 -4.1935744 -4.2405148 -4.2488856 -4.2310696 -4.2053442 -4.1850677 -4.1826324 -4.2144661 -4.2541327 -4.2795148 -4.2881174 -4.2818007][-4.2308617 -4.2101231 -4.2228723 -4.2558475 -4.2754645 -4.2616863 -4.219523 -4.17148 -4.1480646 -4.1679268 -4.2182956 -4.2591872 -4.2814512 -4.2907128 -4.2864547][-4.2860355 -4.2805319 -4.2894053 -4.2995672 -4.2875748 -4.2421093 -4.156899 -4.0734057 -4.0584803 -4.1197834 -4.1963029 -4.2421122 -4.2628136 -4.27588 -4.278564][-4.3120532 -4.3189445 -4.3198524 -4.3059764 -4.264699 -4.1839552 -4.0414481 -3.916193 -3.9337227 -4.0516996 -4.1501722 -4.2044539 -4.2364483 -4.2632308 -4.2761712][-4.3261733 -4.3355656 -4.3228855 -4.283041 -4.2104893 -4.0867524 -3.8859658 -3.7345018 -3.8152037 -3.9896159 -4.1067686 -4.174015 -4.2201042 -4.2559252 -4.279758][-4.33516 -4.3418303 -4.3177962 -4.2544122 -4.148746 -3.9894478 -3.7630849 -3.6388755 -3.7830036 -3.9758406 -4.0948915 -4.1683283 -4.2229972 -4.259491 -4.2853203][-4.33677 -4.3386126 -4.3037615 -4.2242975 -4.1062117 -3.95167 -3.7719002 -3.7286136 -3.8750856 -4.0278082 -4.1198573 -4.1832318 -4.2334657 -4.2674212 -4.2893233][-4.3236237 -4.3199034 -4.2806792 -4.1955285 -4.0893154 -3.9752395 -3.8768704 -3.8989627 -4.0125403 -4.1093259 -4.1690416 -4.2204385 -4.2588496 -4.2817707 -4.2908697][-4.3103571 -4.3034954 -4.26491 -4.1926384 -4.11406 -4.0457296 -4.0082173 -4.0535235 -4.12967 -4.1884875 -4.2306333 -4.2715268 -4.29504 -4.3039 -4.3021364][-4.3115959 -4.3033624 -4.2684059 -4.2184081 -4.1732187 -4.1439285 -4.1387243 -4.1770029 -4.2197351 -4.2545919 -4.2849193 -4.3135128 -4.3242488 -4.3219643 -4.3156905][-4.3183541 -4.3103871 -4.2846875 -4.2608209 -4.2460861 -4.2438545 -4.2517586 -4.2718806 -4.2901349 -4.3078532 -4.3264451 -4.3403158 -4.34012 -4.3310528 -4.323832][-4.3224831 -4.31628 -4.3030233 -4.2998652 -4.3051405 -4.3128967 -4.3181486 -4.3217549 -4.3273149 -4.3352704 -4.3438735 -4.3468866 -4.341073 -4.331284 -4.3258681][-4.3221264 -4.3200636 -4.3181157 -4.3237796 -4.3340034 -4.3412986 -4.3406782 -4.3367348 -4.336585 -4.3377781 -4.3394828 -4.3377872 -4.3313761 -4.3252087 -4.3223696]]...]
INFO - root - 2017-12-06 04:58:00.128128: step 3810, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:54m:23s remains)
INFO - root - 2017-12-06 04:58:02.345215: step 3820, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 20h:07m:25s remains)
INFO - root - 2017-12-06 04:58:04.601404: step 3830, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.219 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-06 04:58:06.763753: step 3840, loss = 2.05, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 19h:56m:52s remains)
INFO - root - 2017-12-06 04:58:08.964261: step 3850, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:53m:40s remains)
INFO - root - 2017-12-06 04:58:11.143031: step 3860, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 20h:24m:57s remains)
INFO - root - 2017-12-06 04:58:13.325489: step 3870, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:15s remains)
INFO - root - 2017-12-06 04:58:15.498845: step 3880, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:34m:29s remains)
INFO - root - 2017-12-06 04:58:17.674342: step 3890, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:37m:51s remains)
INFO - root - 2017-12-06 04:58:19.892563: step 3900, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:12s remains)
2017-12-06 04:58:20.319108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2999878 -4.2949805 -4.293376 -4.29345 -4.2952724 -4.2988539 -4.2981544 -4.2880087 -4.2686706 -4.2455587 -4.2302155 -4.2328429 -4.2565074 -4.2893229 -4.3197975][-4.302381 -4.3003225 -4.3015723 -4.3043 -4.3070211 -4.3102746 -4.3108296 -4.30517 -4.2938075 -4.280704 -4.2722764 -4.273612 -4.2900348 -4.3136616 -4.3348393][-4.3091788 -4.3079438 -4.3081355 -4.3079386 -4.3068905 -4.3054986 -4.3039203 -4.3022785 -4.2997537 -4.2965837 -4.2957897 -4.2982321 -4.3107276 -4.3289514 -4.3453345][-4.3188391 -4.316649 -4.31272 -4.3047352 -4.2930326 -4.2811408 -4.2732282 -4.2711716 -4.2737427 -4.2786012 -4.285099 -4.2902794 -4.3029523 -4.3216696 -4.3401756][-4.328505 -4.3239217 -4.3134847 -4.2934465 -4.266 -4.2397962 -4.2221417 -4.2161474 -4.2196727 -4.2272973 -4.2372336 -4.2461042 -4.2641287 -4.2903247 -4.3180666][-4.3317013 -4.3249197 -4.3066616 -4.2730474 -4.2287483 -4.1876936 -4.1611466 -4.1514397 -4.1528726 -4.1576123 -4.1644425 -4.1755404 -4.2027736 -4.2435369 -4.2867713][-4.3192921 -4.3133068 -4.2895207 -4.2429905 -4.1804996 -4.1246505 -4.08916 -4.0778856 -4.0787468 -4.080977 -4.0853219 -4.1034007 -4.1457772 -4.2051072 -4.2643709][-4.287138 -4.2848983 -4.2581477 -4.1986318 -4.1177969 -4.0471678 -4.0032525 -3.9952528 -4.0068908 -4.0210142 -4.0382957 -4.0746236 -4.135396 -4.2059393 -4.26936][-4.2439761 -4.2493906 -4.2239327 -4.1582932 -4.0648785 -3.981703 -3.9311402 -3.9320805 -3.9679308 -4.0106878 -4.0550213 -4.1113009 -4.1782432 -4.2434568 -4.2958479][-4.2088814 -4.225502 -4.2069879 -4.1466174 -4.058723 -3.979634 -3.9353056 -3.9497125 -4.0063934 -4.0694561 -4.1306553 -4.191431 -4.2473235 -4.2951293 -4.3288121][-4.2079468 -4.2349863 -4.2276487 -4.1835446 -4.1158414 -4.0559983 -4.0288558 -4.0498471 -4.1045866 -4.1629739 -4.21775 -4.2656975 -4.3021269 -4.33055 -4.34824][-4.2406688 -4.2740121 -4.2784624 -4.255055 -4.2114429 -4.1700845 -4.1536822 -4.1677232 -4.2045431 -4.2455478 -4.2840829 -4.3136659 -4.3319483 -4.3444095 -4.3519731][-4.2878718 -4.3180327 -4.3263788 -4.3155246 -4.2905145 -4.2632413 -4.2526054 -4.2592344 -4.2796311 -4.3039994 -4.3248367 -4.3376822 -4.3422475 -4.3437414 -4.345283][-4.3243341 -4.3460288 -4.3543839 -4.351366 -4.338326 -4.3200359 -4.3103013 -4.3091125 -4.3168106 -4.3282647 -4.3377328 -4.3407683 -4.3382087 -4.3348107 -4.3350964][-4.3359156 -4.348042 -4.3541322 -4.3538756 -4.3465552 -4.3338375 -4.3242908 -4.3200941 -4.3229532 -4.3298464 -4.3353329 -4.3348379 -4.3304152 -4.3269906 -4.3281603]]...]
INFO - root - 2017-12-06 04:58:22.444133: step 3910, loss = 2.10, batch loss = 2.05 (38.1 examples/sec; 0.210 sec/batch; 19h:09m:26s remains)
INFO - root - 2017-12-06 04:58:24.612404: step 3920, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:20m:57s remains)
INFO - root - 2017-12-06 04:58:26.802857: step 3930, loss = 2.10, batch loss = 2.05 (37.5 examples/sec; 0.213 sec/batch; 19h:29m:04s remains)
INFO - root - 2017-12-06 04:58:28.981269: step 3940, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:30m:10s remains)
INFO - root - 2017-12-06 04:58:31.191623: step 3950, loss = 2.09, batch loss = 2.03 (33.8 examples/sec; 0.237 sec/batch; 21h:36m:14s remains)
INFO - root - 2017-12-06 04:58:33.345852: step 3960, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:16s remains)
INFO - root - 2017-12-06 04:58:35.479313: step 3970, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:00s remains)
INFO - root - 2017-12-06 04:58:37.664784: step 3980, loss = 2.10, batch loss = 2.04 (37.4 examples/sec; 0.214 sec/batch; 19h:31m:52s remains)
INFO - root - 2017-12-06 04:58:39.818713: step 3990, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.221 sec/batch; 20h:08m:10s remains)
INFO - root - 2017-12-06 04:58:41.985878: step 4000, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:31m:14s remains)
2017-12-06 04:58:44.670162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3339357 -4.3376765 -4.3319421 -4.3215489 -4.3132339 -4.3058424 -4.2952986 -4.2855487 -4.2832637 -4.29185 -4.3002095 -4.3037367 -4.3102889 -4.3199677 -4.3285079][-4.3090935 -4.3195367 -4.3181043 -4.3038936 -4.2839627 -4.26122 -4.2357125 -4.214797 -4.2130084 -4.2291889 -4.2450271 -4.2541394 -4.2627239 -4.2806807 -4.3012471][-4.2798181 -4.2975669 -4.2977867 -4.2790127 -4.2439842 -4.1991034 -4.1536078 -4.1121984 -4.1081753 -4.1381941 -4.1654425 -4.1876097 -4.2048578 -4.2314887 -4.2621822][-4.2496181 -4.2706785 -4.2728596 -4.2464805 -4.1929188 -4.1150064 -4.0377464 -3.9757013 -3.9804044 -4.0450149 -4.0997677 -4.1354589 -4.1665668 -4.2013631 -4.2352862][-4.2076945 -4.2296109 -4.2384229 -4.2128096 -4.1369395 -4.0156217 -3.8829169 -3.7970333 -3.8375266 -3.9626715 -4.0645604 -4.1210771 -4.1618438 -4.2036433 -4.2333446][-4.1360674 -4.1576691 -4.1837025 -4.1704388 -4.0838203 -3.9182675 -3.7068372 -3.5738418 -3.6781032 -3.8912184 -4.0505395 -4.1360264 -4.1847734 -4.2265644 -4.242156][-4.0573106 -4.0771947 -4.1208954 -4.1284742 -4.0534854 -3.8812628 -3.6271176 -3.4424214 -3.5930469 -3.8675017 -4.0635905 -4.1692948 -4.2216573 -4.2557769 -4.25559][-4.01127 -4.0216355 -4.0711508 -4.1051178 -4.06302 -3.9377801 -3.7376857 -3.5849776 -3.6860752 -3.9164739 -4.0915089 -4.1878514 -4.23561 -4.2591267 -4.2456832][-4.0118484 -4.0079303 -4.0571952 -4.1106834 -4.1086135 -4.0449414 -3.9365747 -3.8544652 -3.8947754 -4.0211015 -4.1333179 -4.1977577 -4.2234926 -4.2351146 -4.21776][-4.046217 -4.0359259 -4.0862503 -4.1476445 -4.169909 -4.1477613 -4.096849 -4.0557022 -4.060833 -4.1109076 -4.1636267 -4.1992984 -4.2040725 -4.2032628 -4.1851168][-4.0843139 -4.0739703 -4.1215119 -4.1780829 -4.2006674 -4.1914868 -4.1631913 -4.1394281 -4.1249 -4.1281881 -4.1497879 -4.177053 -4.1870284 -4.1932015 -4.1798463][-4.1081357 -4.0997252 -4.1401739 -4.18247 -4.1963353 -4.1899476 -4.1748619 -4.1568127 -4.1301274 -4.1093373 -4.1173897 -4.1525755 -4.1834464 -4.20865 -4.2056293][-4.1380725 -4.1311731 -4.1605182 -4.188179 -4.1922088 -4.1849852 -4.1766224 -4.1606741 -4.1336989 -4.1163831 -4.1272774 -4.1611018 -4.200573 -4.2401347 -4.2496676][-4.181643 -4.1788173 -4.19635 -4.20896 -4.198225 -4.1854711 -4.1789021 -4.1701546 -4.1502781 -4.14608 -4.1655536 -4.1949391 -4.235837 -4.2795253 -4.2915516][-4.2270284 -4.2284117 -4.2396183 -4.2395611 -4.2168083 -4.1936445 -4.1880059 -4.1884823 -4.1767917 -4.1750088 -4.195456 -4.2245407 -4.2656693 -4.3081651 -4.3165436]]...]
INFO - root - 2017-12-06 04:58:46.975283: step 4010, loss = 2.06, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:09m:08s remains)
INFO - root - 2017-12-06 04:58:49.227228: step 4020, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:48m:37s remains)
INFO - root - 2017-12-06 04:58:51.402601: step 4030, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:30m:41s remains)
INFO - root - 2017-12-06 04:58:53.621542: step 4040, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:47s remains)
INFO - root - 2017-12-06 04:58:55.782136: step 4050, loss = 2.05, batch loss = 1.99 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:01s remains)
INFO - root - 2017-12-06 04:58:58.003737: step 4060, loss = 2.04, batch loss = 1.98 (37.3 examples/sec; 0.214 sec/batch; 19h:33m:02s remains)
INFO - root - 2017-12-06 04:59:00.163954: step 4070, loss = 2.04, batch loss = 1.98 (37.5 examples/sec; 0.213 sec/batch; 19h:27m:24s remains)
INFO - root - 2017-12-06 04:59:02.385413: step 4080, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:02m:54s remains)
INFO - root - 2017-12-06 04:59:04.601580: step 4090, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:40m:05s remains)
INFO - root - 2017-12-06 04:59:06.714784: step 4100, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.213 sec/batch; 19h:24m:00s remains)
2017-12-06 04:59:08.130873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3121195 -4.3131733 -4.3135133 -4.3119073 -4.3106461 -4.3110132 -4.3141394 -4.3172569 -4.3186893 -4.3194284 -4.3197627 -4.3198304 -4.3195906 -4.3196688 -4.3196206][-4.2822933 -4.2847652 -4.285583 -4.2815886 -4.2776594 -4.2774038 -4.2837529 -4.2911162 -4.2951579 -4.2974658 -4.299027 -4.2999372 -4.2995806 -4.2997031 -4.2994685][-4.2420821 -4.2459087 -4.246048 -4.238276 -4.2310781 -4.2319808 -4.2446856 -4.2604609 -4.2712955 -4.2765932 -4.2793961 -4.2804713 -4.2798057 -4.2786741 -4.2764091][-4.2108502 -4.2111678 -4.2057533 -4.1895885 -4.176126 -4.17768 -4.1976643 -4.2270122 -4.2534332 -4.2692642 -4.2769608 -4.2796268 -4.2773266 -4.2722669 -4.2639074][-4.1959624 -4.1876507 -4.1700373 -4.1404996 -4.1140947 -4.1053343 -4.1221752 -4.162488 -4.213335 -4.2520151 -4.2742081 -4.2841048 -4.2835503 -4.2756686 -4.26181][-4.18957 -4.1747241 -4.146584 -4.1057549 -4.0610876 -4.0232086 -4.0102 -4.0465422 -4.123106 -4.1899443 -4.2322555 -4.2564626 -4.26493 -4.2611217 -4.250524][-4.1835732 -4.1640129 -4.1302886 -4.0816927 -4.01873 -3.9428687 -3.8757112 -3.88641 -3.984107 -4.0769506 -4.1381917 -4.1778069 -4.2015562 -4.2140694 -4.2191672][-4.1918082 -4.1755004 -4.1457038 -4.1017232 -4.0338745 -3.9398434 -3.8388081 -3.8131816 -3.8982332 -3.98923 -4.0471635 -4.088625 -4.1238093 -4.1559587 -4.1783686][-4.2167392 -4.206522 -4.1842608 -4.1537519 -4.1033821 -4.0310745 -3.9529665 -3.9257915 -3.9676979 -4.0161481 -4.0452337 -4.06788 -4.0947704 -4.1246657 -4.1480031][-4.249794 -4.246902 -4.2327356 -4.2149734 -4.1860566 -4.1438584 -4.1007175 -4.0851512 -4.0998344 -4.1121922 -4.1112046 -4.1090636 -4.1158581 -4.1289773 -4.1433372][-4.2820392 -4.2875628 -4.2813025 -4.2709022 -4.2548084 -4.233664 -4.212471 -4.2027383 -4.204669 -4.2006116 -4.1831155 -4.1643023 -4.1550307 -4.15465 -4.1605716][-4.3000183 -4.3145924 -4.319356 -4.317832 -4.3110166 -4.2997484 -4.2872453 -4.2779784 -4.2703052 -4.2593 -4.2353063 -4.2083058 -4.1922126 -4.1856956 -4.1847291][-4.296669 -4.317543 -4.3320994 -4.3395171 -4.3393879 -4.331955 -4.3227692 -4.3142323 -4.3056417 -4.291431 -4.2651043 -4.2367926 -4.2188735 -4.2078962 -4.200593][-4.2832255 -4.3061461 -4.3251548 -4.3359489 -4.3378739 -4.3310733 -4.32038 -4.3113317 -4.3028622 -4.2866244 -4.262022 -4.2403445 -4.2258954 -4.2140865 -4.2046509][-4.2689548 -4.2900743 -4.3078246 -4.316071 -4.3155046 -4.3046975 -4.289053 -4.277844 -4.2669148 -4.250237 -4.2329159 -4.2212706 -4.2110548 -4.2016306 -4.1929383]]...]
INFO - root - 2017-12-06 04:59:10.335951: step 4110, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:20s remains)
INFO - root - 2017-12-06 04:59:12.528965: step 4120, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 20h:10m:29s remains)
INFO - root - 2017-12-06 04:59:14.672169: step 4130, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:23m:38s remains)
INFO - root - 2017-12-06 04:59:16.843726: step 4140, loss = 2.03, batch loss = 1.97 (35.5 examples/sec; 0.225 sec/batch; 20h:32m:30s remains)
INFO - root - 2017-12-06 04:59:19.040808: step 4150, loss = 2.04, batch loss = 1.98 (37.5 examples/sec; 0.214 sec/batch; 19h:28m:27s remains)
INFO - root - 2017-12-06 04:59:21.203013: step 4160, loss = 2.08, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:24s remains)
INFO - root - 2017-12-06 04:59:23.402152: step 4170, loss = 2.09, batch loss = 2.03 (35.0 examples/sec; 0.229 sec/batch; 20h:51m:28s remains)
INFO - root - 2017-12-06 04:59:25.580779: step 4180, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:51m:39s remains)
INFO - root - 2017-12-06 04:59:27.758715: step 4190, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:58s remains)
INFO - root - 2017-12-06 04:59:29.959455: step 4200, loss = 2.06, batch loss = 2.01 (36.8 examples/sec; 0.218 sec/batch; 19h:51m:00s remains)
2017-12-06 04:59:30.375847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.175694 -4.1886268 -4.2010903 -4.2076125 -4.2105713 -4.1973586 -4.1674161 -4.1301808 -4.1230593 -4.13507 -4.1282306 -4.1062379 -4.1118379 -4.1434612 -4.1761589][-4.1757884 -4.1941872 -4.2111511 -4.2179861 -4.21597 -4.1989732 -4.1713772 -4.1392851 -4.1325779 -4.1437564 -4.1290264 -4.0913949 -4.0779071 -4.1011643 -4.1436338][-4.1869206 -4.2117796 -4.2351251 -4.242527 -4.2335725 -4.2074108 -4.1764359 -4.149869 -4.1413355 -4.1437564 -4.1238027 -4.0800977 -4.0562358 -4.0662279 -4.1014805][-4.2072415 -4.233192 -4.255209 -4.2612829 -4.2430396 -4.2038941 -4.1615415 -4.1330938 -4.1295767 -4.1347914 -4.116323 -4.0777855 -4.0565181 -4.0627623 -4.0858316][-4.234529 -4.2550263 -4.2709556 -4.2726784 -4.243928 -4.1865277 -4.1218648 -4.0768 -4.0792327 -4.102931 -4.1037374 -4.08043 -4.0706611 -4.0770483 -4.0927682][-4.2633562 -4.2783923 -4.2887163 -4.2812943 -4.2385783 -4.16197 -4.0718322 -4.0055876 -4.0044246 -4.0464334 -4.0746803 -4.0800381 -4.0885744 -4.0976496 -4.1117921][-4.2843337 -4.2944932 -4.3003964 -4.2858686 -4.2356353 -4.1471496 -4.0419145 -3.9590564 -3.9416971 -3.9857166 -4.0355926 -4.0708618 -4.1007428 -4.1189933 -4.1376448][-4.296598 -4.3044415 -4.3057957 -4.2856078 -4.2376604 -4.1544695 -4.0554924 -3.967732 -3.9267612 -3.9525657 -4.0043397 -4.0596538 -4.1064754 -4.1385522 -4.1657667][-4.3027163 -4.3096828 -4.3110857 -4.2900519 -4.24824 -4.1835208 -4.1076789 -4.0281916 -3.9699173 -3.9655559 -4.0040517 -4.0652218 -4.1230617 -4.1677923 -4.2034802][-4.3022113 -4.3076153 -4.3123207 -4.2979817 -4.2667084 -4.2208652 -4.1654625 -4.0950551 -4.0324569 -4.006568 -4.0246992 -4.0803595 -4.14229 -4.1936584 -4.2335396][-4.2948194 -4.296566 -4.3039775 -4.3001294 -4.2837505 -4.2575469 -4.21608 -4.1559658 -4.0973125 -4.0600419 -4.0595245 -4.1027913 -4.1616335 -4.2154031 -4.2559214][-4.2935591 -4.2921081 -4.3014622 -4.3076391 -4.3046303 -4.2928891 -4.266541 -4.2221484 -4.1727591 -4.1329827 -4.1195679 -4.1457367 -4.1938491 -4.2428246 -4.2792354][-4.3056459 -4.3028369 -4.3105192 -4.3199096 -4.3233142 -4.3208337 -4.3070846 -4.2766647 -4.2382212 -4.2030373 -4.1846161 -4.1970477 -4.2333093 -4.2746878 -4.3054271][-4.315299 -4.3130465 -4.3190427 -4.3278928 -4.3321142 -4.334053 -4.32804 -4.3095818 -4.2825618 -4.2533789 -4.2352509 -4.2388082 -4.2627344 -4.2926664 -4.3171749][-4.3225427 -4.3203893 -4.3241925 -4.3316231 -4.335793 -4.3386106 -4.3377051 -4.3277059 -4.3102884 -4.2879372 -4.2727761 -4.2729654 -4.2871637 -4.3054619 -4.3221459]]...]
INFO - root - 2017-12-06 04:59:32.536326: step 4210, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:33m:23s remains)
INFO - root - 2017-12-06 04:59:34.730583: step 4220, loss = 2.10, batch loss = 2.04 (38.5 examples/sec; 0.208 sec/batch; 18h:57m:52s remains)
INFO - root - 2017-12-06 04:59:36.895352: step 4230, loss = 2.06, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 20h:07m:06s remains)
INFO - root - 2017-12-06 04:59:39.125287: step 4240, loss = 2.07, batch loss = 2.01 (33.9 examples/sec; 0.236 sec/batch; 21h:31m:50s remains)
INFO - root - 2017-12-06 04:59:41.293429: step 4250, loss = 2.04, batch loss = 1.98 (38.1 examples/sec; 0.210 sec/batch; 19h:08m:02s remains)
INFO - root - 2017-12-06 04:59:43.440719: step 4260, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:21s remains)
INFO - root - 2017-12-06 04:59:45.609702: step 4270, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:42m:46s remains)
INFO - root - 2017-12-06 04:59:47.757508: step 4280, loss = 2.05, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:27m:11s remains)
INFO - root - 2017-12-06 04:59:49.905368: step 4290, loss = 2.06, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:25s remains)
INFO - root - 2017-12-06 04:59:52.075398: step 4300, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.215 sec/batch; 19h:38m:33s remains)
2017-12-06 04:59:56.187104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1802249 -4.1915808 -4.1972032 -4.1982017 -4.1895757 -4.1696663 -4.145328 -4.1311579 -4.1374269 -4.157227 -4.1773481 -4.1926136 -4.2028379 -4.2024064 -4.1951466][-4.162353 -4.1797585 -4.1887264 -4.1882758 -4.1720481 -4.1398983 -4.103303 -4.0868077 -4.1011219 -4.1304059 -4.1573234 -4.1801567 -4.1978498 -4.1999989 -4.1905966][-4.1529617 -4.1733346 -4.1824927 -4.1767793 -4.1488676 -4.1026349 -4.0553384 -4.0393767 -4.0627031 -4.1020312 -4.13767 -4.1683083 -4.1911459 -4.19394 -4.1811876][-4.1546211 -4.1729646 -4.1789236 -4.1659026 -4.1267271 -4.0684958 -4.0140595 -3.9987409 -4.0310059 -4.0822263 -4.1286922 -4.1661448 -4.1898851 -4.1899061 -4.1706362][-4.1698027 -4.1798329 -4.1768045 -4.155128 -4.1066647 -4.040514 -3.9818573 -3.9670331 -4.0111928 -4.0777388 -4.1334424 -4.1756725 -4.1975389 -4.1913209 -4.1610551][-4.1903162 -4.1895881 -4.1756921 -4.1450348 -4.0892549 -4.0170593 -3.9524214 -3.9357362 -3.9931834 -4.07514 -4.1393294 -4.1852951 -4.2051163 -4.1925678 -4.1501808][-4.2035289 -4.1936607 -4.1722126 -4.1371479 -4.077251 -3.9988575 -3.9237351 -3.9022217 -3.9716997 -4.0681643 -4.1397352 -4.18993 -4.2106366 -4.1959333 -4.1464491][-4.2073298 -4.1883326 -4.1651735 -4.1342573 -4.0812731 -4.0052714 -3.9258556 -3.8997185 -3.9722085 -4.0730181 -4.1455269 -4.1972752 -4.2202306 -4.2094307 -4.1611929][-4.2025414 -4.1751084 -4.15466 -4.13638 -4.1035547 -4.0456128 -3.9793987 -3.95739 -4.0207996 -4.1069937 -4.1671443 -4.211935 -4.2336531 -4.2268949 -4.1867971][-4.1922097 -4.1588845 -4.1425662 -4.1394043 -4.130722 -4.0970922 -4.0489225 -4.0338926 -4.0843897 -4.1492567 -4.1930456 -4.2269983 -4.2432828 -4.238904 -4.2089248][-4.1738615 -4.1413984 -4.1338267 -4.1487212 -4.1642451 -4.1538863 -4.1222091 -4.1109667 -4.1452122 -4.18776 -4.2151041 -4.236999 -4.2472262 -4.244648 -4.22177][-4.150907 -4.1250324 -4.1284614 -4.1587863 -4.191586 -4.1986518 -4.181169 -4.1731992 -4.1935725 -4.2207656 -4.2390618 -4.2522173 -4.2554417 -4.249033 -4.2243853][-4.1290812 -4.1138315 -4.1270385 -4.1647263 -4.2044096 -4.2193403 -4.2116117 -4.20681 -4.2193117 -4.238214 -4.2546973 -4.2656636 -4.2657328 -4.25447 -4.2254453][-4.1036515 -4.1037068 -4.1261859 -4.1633816 -4.1991153 -4.2136116 -4.2101078 -4.2062979 -4.2130847 -4.2277741 -4.2476368 -4.261672 -4.2632957 -4.2509942 -4.2224474][-4.0862265 -4.1004772 -4.1283622 -4.1587639 -4.1828976 -4.189373 -4.1866388 -4.1823015 -4.1813564 -4.1938267 -4.2202706 -4.2387972 -4.243989 -4.2348557 -4.2110872]]...]
INFO - root - 2017-12-06 04:59:58.526389: step 4310, loss = 2.09, batch loss = 2.04 (34.5 examples/sec; 0.232 sec/batch; 21h:06m:32s remains)
INFO - root - 2017-12-06 05:00:00.750786: step 4320, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 21h:18m:20s remains)
INFO - root - 2017-12-06 05:00:02.955183: step 4330, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:34m:13s remains)
INFO - root - 2017-12-06 05:00:05.150356: step 4340, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 19h:55m:53s remains)
INFO - root - 2017-12-06 05:00:07.360722: step 4350, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:50s remains)
INFO - root - 2017-12-06 05:00:09.552278: step 4360, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.214 sec/batch; 19h:28m:10s remains)
INFO - root - 2017-12-06 05:00:11.730800: step 4370, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:52s remains)
INFO - root - 2017-12-06 05:00:13.952073: step 4380, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:24m:12s remains)
INFO - root - 2017-12-06 05:00:16.120325: step 4390, loss = 2.04, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:57s remains)
INFO - root - 2017-12-06 05:00:18.280669: step 4400, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 19h:56m:21s remains)
2017-12-06 05:00:18.667379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1617002 -4.1763387 -4.1808047 -4.1769156 -4.1729555 -4.17569 -4.1829143 -4.1837797 -4.15869 -4.1352024 -4.13386 -4.1480002 -4.1801925 -4.2016306 -4.1893425][-4.1666222 -4.1785245 -4.1768222 -4.1709471 -4.1753125 -4.1859841 -4.1938286 -4.1933527 -4.1723585 -4.1525869 -4.152153 -4.160532 -4.1747823 -4.1774864 -4.1602221][-4.1473484 -4.1539345 -4.1448245 -4.1308141 -4.1344566 -4.1416273 -4.1514387 -4.1609588 -4.1604409 -4.1499429 -4.150517 -4.1480556 -4.14485 -4.1377077 -4.12847][-4.0852904 -4.0912848 -4.0870528 -4.0726976 -4.0704241 -4.0728669 -4.0822759 -4.1032844 -4.1187329 -4.115972 -4.1172962 -4.1119289 -4.0972695 -4.0875025 -4.0806994][-4.008152 -4.0182338 -4.0273118 -4.0218534 -4.0181489 -4.0189686 -4.0277238 -4.0484638 -4.0647626 -4.0650043 -4.0665097 -4.0622187 -4.0437241 -4.0335321 -4.0242634][-3.9691319 -3.9814641 -3.9916379 -3.9873557 -3.9789405 -3.975574 -3.970803 -3.9758329 -3.9950402 -4.0089316 -4.0135264 -4.0093069 -3.9916713 -3.9852636 -3.9805863][-3.9559171 -3.9640081 -3.9641798 -3.9505391 -3.9323716 -3.9065268 -3.8660157 -3.8454807 -3.8812461 -3.9254403 -3.9365878 -3.9373839 -3.9293602 -3.9294109 -3.9405475][-3.9688005 -3.9641075 -3.9481747 -3.9169679 -3.8809066 -3.8221934 -3.7366924 -3.6920373 -3.7567883 -3.8393254 -3.8655734 -3.8831224 -3.8934567 -3.91146 -3.9501114][-4.0301504 -4.0071368 -3.9787188 -3.9407463 -3.8993433 -3.8355181 -3.7502184 -3.7110095 -3.7809749 -3.8632443 -3.8918622 -3.9202151 -3.9498403 -3.9809959 -4.0325079][-4.0936484 -4.0683217 -4.0424905 -4.0153131 -3.9874263 -3.9466014 -3.8922853 -3.868135 -3.9158406 -3.970546 -3.9901042 -4.016458 -4.0479212 -4.07935 -4.1225038][-4.1473751 -4.1315327 -4.1151919 -4.0966721 -4.0773 -4.0542216 -4.0253386 -4.0145216 -4.043014 -4.0756307 -4.0881925 -4.1068225 -4.1298246 -4.1509509 -4.1815667][-4.1926527 -4.1836195 -4.1756029 -4.1633806 -4.1506443 -4.1393909 -4.1250815 -4.118825 -4.131588 -4.1486816 -4.1574693 -4.1682825 -4.1815047 -4.1899924 -4.2037768][-4.230864 -4.2221255 -4.2171273 -4.2109032 -4.2056913 -4.2033868 -4.1960988 -4.1887484 -4.1906028 -4.1971097 -4.2010703 -4.2029037 -4.2027235 -4.195385 -4.1928296][-4.2673731 -4.2590981 -4.2543058 -4.2502127 -4.2481575 -4.2482166 -4.2432704 -4.2363358 -4.2329092 -4.2317042 -4.226861 -4.2179575 -4.2047968 -4.1840544 -4.1694584][-4.2958407 -4.2910018 -4.2865882 -4.2836781 -4.2826509 -4.2815857 -4.27831 -4.2750239 -4.2726932 -4.2704678 -4.2640061 -4.2516165 -4.2317829 -4.1999307 -4.16796]]...]
INFO - root - 2017-12-06 05:00:20.813659: step 4410, loss = 2.09, batch loss = 2.03 (38.3 examples/sec; 0.209 sec/batch; 19h:03m:19s remains)
INFO - root - 2017-12-06 05:00:22.962112: step 4420, loss = 2.06, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:32m:28s remains)
INFO - root - 2017-12-06 05:00:25.143492: step 4430, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:59m:28s remains)
INFO - root - 2017-12-06 05:00:27.312009: step 4440, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:01s remains)
INFO - root - 2017-12-06 05:00:29.462037: step 4450, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:24m:58s remains)
INFO - root - 2017-12-06 05:00:31.618823: step 4460, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:50m:11s remains)
INFO - root - 2017-12-06 05:00:33.805110: step 4470, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:26s remains)
INFO - root - 2017-12-06 05:00:35.993637: step 4480, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.215 sec/batch; 19h:37m:24s remains)
INFO - root - 2017-12-06 05:00:38.192226: step 4490, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 20h:32m:42s remains)
INFO - root - 2017-12-06 05:00:40.359352: step 4500, loss = 2.04, batch loss = 1.98 (36.9 examples/sec; 0.217 sec/batch; 19h:45m:04s remains)
2017-12-06 05:00:40.695658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3282137 -4.3314614 -4.3321052 -4.3292332 -4.3182263 -4.3039207 -4.2918434 -4.2753162 -4.2591391 -4.2577224 -4.2591972 -4.2664685 -4.2825122 -4.3003206 -4.3135219][-4.3274431 -4.3348045 -4.3368669 -4.3325648 -4.3157549 -4.2941775 -4.2733006 -4.2443109 -4.2184157 -4.2177625 -4.2237358 -4.2383533 -4.2652326 -4.2901721 -4.3081946][-4.325994 -4.3376937 -4.3420353 -4.3335447 -4.3089862 -4.2785444 -4.2454286 -4.1988425 -4.1662164 -4.1809468 -4.2013659 -4.2214489 -4.2567878 -4.287991 -4.3113766][-4.3189626 -4.332365 -4.3372536 -4.3248887 -4.2960386 -4.2583108 -4.2066326 -4.1352615 -4.1020565 -4.1419859 -4.1818595 -4.2057815 -4.2423763 -4.28013 -4.3097882][-4.3099532 -4.3197303 -4.3207974 -4.30306 -4.2678595 -4.2211561 -4.1516042 -4.0610309 -4.0333285 -4.0995235 -4.1607218 -4.1924014 -4.2328868 -4.2751966 -4.3080692][-4.2987666 -4.3024731 -4.2994509 -4.2748418 -4.2298403 -4.17054 -4.0807996 -3.9709828 -3.956383 -4.0503407 -4.1324496 -4.1772375 -4.2260051 -4.2726469 -4.3064957][-4.2895331 -4.2915697 -4.2886572 -4.259726 -4.2076192 -4.1346035 -4.012105 -3.8702335 -3.8659005 -3.9984705 -4.1053557 -4.1648626 -4.2225528 -4.2756495 -4.31162][-4.2820063 -4.2907763 -4.2930236 -4.265543 -4.2150674 -4.1377716 -3.9954352 -3.8258376 -3.8262262 -3.9866486 -4.107471 -4.1746116 -4.2331524 -4.2870297 -4.3190937][-4.2728009 -4.2909746 -4.3006253 -4.2801652 -4.2401996 -4.1731658 -4.0491662 -3.9027283 -3.8960714 -4.0313454 -4.1363168 -4.20101 -4.2524171 -4.2964993 -4.3196573][-4.2610183 -4.2828507 -4.2951112 -4.2837138 -4.257278 -4.2081528 -4.1197939 -4.0212502 -4.0177121 -4.1055093 -4.1755056 -4.2252793 -4.2654204 -4.2996206 -4.31483][-4.2471256 -4.267158 -4.2752719 -4.2708883 -4.2569609 -4.2252779 -4.1680093 -4.1059775 -4.1086154 -4.1678643 -4.2155333 -4.2476363 -4.2713261 -4.2920423 -4.3004103][-4.2314491 -4.2482386 -4.2541261 -4.2546444 -4.2485447 -4.2262836 -4.1915741 -4.1562953 -4.1624851 -4.2012029 -4.2356567 -4.2554951 -4.2645597 -4.2725239 -4.2754035][-4.2154183 -4.2294769 -4.2404637 -4.2444715 -4.2391052 -4.2266569 -4.2089434 -4.1925216 -4.1966405 -4.2152576 -4.2361622 -4.2454057 -4.2452354 -4.2467642 -4.2480316][-4.2065706 -4.2202244 -4.2349 -4.24043 -4.2355843 -4.2277689 -4.2221403 -4.2207694 -4.2255487 -4.2293935 -4.235374 -4.2331514 -4.2262292 -4.2222781 -4.2192817][-4.2125793 -4.2293072 -4.24566 -4.25032 -4.2441392 -4.236217 -4.2303653 -4.2302465 -4.2343936 -4.2346239 -4.2318606 -4.2244043 -4.2144666 -4.2074046 -4.2006288]]...]
INFO - root - 2017-12-06 05:00:42.840822: step 4510, loss = 2.04, batch loss = 1.98 (38.0 examples/sec; 0.211 sec/batch; 19h:11m:44s remains)
INFO - root - 2017-12-06 05:00:45.002482: step 4520, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:29m:43s remains)
INFO - root - 2017-12-06 05:00:47.137839: step 4530, loss = 2.06, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:08m:52s remains)
INFO - root - 2017-12-06 05:00:49.322899: step 4540, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.208 sec/batch; 18h:57m:48s remains)
INFO - root - 2017-12-06 05:00:51.446740: step 4550, loss = 2.05, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:06s remains)
INFO - root - 2017-12-06 05:00:53.657829: step 4560, loss = 2.08, batch loss = 2.03 (35.3 examples/sec; 0.226 sec/batch; 20h:37m:29s remains)
INFO - root - 2017-12-06 05:00:55.905863: step 4570, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:38m:24s remains)
INFO - root - 2017-12-06 05:00:58.108225: step 4580, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:03m:45s remains)
INFO - root - 2017-12-06 05:01:00.318702: step 4590, loss = 2.08, batch loss = 2.02 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:03s remains)
INFO - root - 2017-12-06 05:01:02.490430: step 4600, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.215 sec/batch; 19h:37m:17s remains)
2017-12-06 05:01:03.523160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2689981 -4.2671461 -4.2681451 -4.269526 -4.270658 -4.2717509 -4.2725606 -4.27363 -4.2732668 -4.2657375 -4.2579064 -4.2546468 -4.2525544 -4.2473874 -4.2402506][-4.2713752 -4.2689028 -4.2694826 -4.269834 -4.2711229 -4.2729621 -4.2753005 -4.2782407 -4.27879 -4.271081 -4.2625141 -4.25563 -4.2507854 -4.2450829 -4.23822][-4.2564149 -4.2512145 -4.2508554 -4.250309 -4.2517262 -4.2539039 -4.2575326 -4.2634077 -4.2688637 -4.2668791 -4.262918 -4.2586842 -4.2565794 -4.2539349 -4.2507524][-4.2333179 -4.223176 -4.2198138 -4.2163157 -4.2151771 -4.2174692 -4.2234325 -4.2327547 -4.2434468 -4.2480764 -4.2513986 -4.2531867 -4.2565169 -4.2580976 -4.2587852][-4.2096829 -4.1935363 -4.1846309 -4.176559 -4.1707544 -4.1692076 -4.1741953 -4.1858315 -4.20294 -4.2166324 -4.2294307 -4.2399731 -4.2488608 -4.2521424 -4.2524853][-4.1655478 -4.1432471 -4.1262875 -4.1132107 -4.1017709 -4.0914478 -4.0894246 -4.09907 -4.1195149 -4.1429057 -4.1671791 -4.1879516 -4.2054367 -4.2162781 -4.2226553][-4.1193862 -4.0898137 -4.0645504 -4.0446157 -4.0244012 -3.9991693 -3.9805803 -3.9814787 -4.0038166 -4.0368118 -4.0751519 -4.1103821 -4.1405888 -4.1621618 -4.1778259][-4.1120629 -4.0842218 -4.059176 -4.0370426 -4.0104322 -3.9728968 -3.9373422 -3.9217215 -3.9334612 -3.963707 -4.0061631 -4.0532174 -4.0967011 -4.129087 -4.1521125][-4.1556149 -4.1345592 -4.1164937 -4.0992193 -4.0804234 -4.05266 -4.0221291 -3.9991784 -3.99486 -4.0092239 -4.0383496 -4.077343 -4.1164765 -4.147172 -4.1699462][-4.2162223 -4.2050738 -4.1948247 -4.1839185 -4.1758108 -4.1610928 -4.1431131 -4.124784 -4.1161165 -4.1223655 -4.1392841 -4.1643748 -4.1904521 -4.2088594 -4.2209158][-4.2541018 -4.2493515 -4.245368 -4.2410979 -4.2423158 -4.239151 -4.2304254 -4.2194381 -4.2129169 -4.2156186 -4.2248778 -4.2387929 -4.2529993 -4.2602825 -4.2627192][-4.2671738 -4.2661209 -4.26622 -4.2677917 -4.2762122 -4.2815003 -4.278739 -4.2727013 -4.2671466 -4.2652736 -4.265604 -4.2691545 -4.2729034 -4.2708759 -4.2671375][-4.2583675 -4.2598181 -4.2622538 -4.2670565 -4.2784872 -4.2864332 -4.2874341 -4.2859006 -4.2826462 -4.2737856 -4.2641296 -4.2579741 -4.2555194 -4.25067 -4.2467504][-4.2390594 -4.2414789 -4.244967 -4.2496166 -4.2589655 -4.2649159 -4.2664485 -4.2673326 -4.26458 -4.24826 -4.2306371 -4.21926 -4.2151089 -4.2127166 -4.2124629][-4.2197347 -4.2215619 -4.2238455 -4.2256355 -4.2292552 -4.2331467 -4.2343779 -4.23623 -4.2333207 -4.2120361 -4.1902585 -4.17679 -4.1755257 -4.1808352 -4.1883416]]...]
INFO - root - 2017-12-06 05:01:05.726775: step 4610, loss = 2.04, batch loss = 1.98 (37.1 examples/sec; 0.215 sec/batch; 19h:37m:22s remains)
INFO - root - 2017-12-06 05:01:07.916819: step 4620, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:17m:15s remains)
INFO - root - 2017-12-06 05:01:10.107802: step 4630, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:29m:50s remains)
INFO - root - 2017-12-06 05:01:12.269474: step 4640, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:04m:00s remains)
INFO - root - 2017-12-06 05:01:14.567643: step 4650, loss = 2.04, batch loss = 1.98 (37.5 examples/sec; 0.213 sec/batch; 19h:26m:10s remains)
INFO - root - 2017-12-06 05:01:16.678584: step 4660, loss = 2.09, batch loss = 2.03 (38.0 examples/sec; 0.210 sec/batch; 19h:08m:54s remains)
INFO - root - 2017-12-06 05:01:18.839055: step 4670, loss = 2.04, batch loss = 1.98 (36.4 examples/sec; 0.220 sec/batch; 19h:59m:19s remains)
INFO - root - 2017-12-06 05:01:21.009398: step 4680, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 20h:30m:34s remains)
INFO - root - 2017-12-06 05:01:23.203726: step 4690, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:55m:34s remains)
INFO - root - 2017-12-06 05:01:25.377222: step 4700, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.215 sec/batch; 19h:36m:47s remains)
2017-12-06 05:01:25.734737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2507014 -4.2577415 -4.28095 -4.2923489 -4.2814512 -4.25858 -4.2493119 -4.2575092 -4.2644377 -4.2680869 -4.2511888 -4.220572 -4.2061849 -4.2284579 -4.2540803][-4.2452741 -4.2632141 -4.2919927 -4.2995539 -4.2790318 -4.2446833 -4.2312565 -4.2421374 -4.2514763 -4.2586417 -4.2468104 -4.2226996 -4.2158632 -4.2438254 -4.2740116][-4.2140765 -4.245749 -4.2836289 -4.2952833 -4.2706437 -4.2278991 -4.2116442 -4.2289519 -4.2500873 -4.2565093 -4.2381129 -4.2169752 -4.2163935 -4.2482424 -4.2799025][-4.1871395 -4.2248635 -4.2654514 -4.2780709 -4.2450337 -4.1871238 -4.1646094 -4.1959314 -4.2346864 -4.2464123 -4.2228613 -4.2043533 -4.2119946 -4.2469878 -4.2808485][-4.1811867 -4.2172875 -4.2538404 -4.2539968 -4.19342 -4.1062536 -4.0790887 -4.1391788 -4.2027297 -4.2257261 -4.2036581 -4.1844935 -4.1982903 -4.2389841 -4.279305][-4.1905327 -4.2210355 -4.2457337 -4.2200842 -4.1209245 -3.9986386 -3.9788308 -4.07907 -4.1665692 -4.1929893 -4.1696496 -4.1493564 -4.1698508 -4.2199945 -4.271153][-4.2031379 -4.229579 -4.240191 -4.1896844 -4.0651579 -3.9303553 -3.923207 -4.0469408 -4.1509061 -4.1728549 -4.1393948 -4.1126509 -4.1398425 -4.2004905 -4.2605782][-4.2327547 -4.2533455 -4.2544904 -4.1975 -4.0829673 -3.9668689 -3.9611533 -4.0707111 -4.1674833 -4.1792378 -4.1362314 -4.1066031 -4.1379051 -4.2035317 -4.2660789][-4.275363 -4.2900534 -4.2815719 -4.2294331 -4.1385536 -4.0458112 -4.0316849 -4.1143026 -4.194613 -4.1979413 -4.1512456 -4.1217451 -4.1535196 -4.219151 -4.2802916][-4.304975 -4.314733 -4.3012218 -4.2577667 -4.1870065 -4.1138167 -4.0946031 -4.15337 -4.2181435 -4.2185178 -4.1702247 -4.13857 -4.1697183 -4.2368217 -4.2931433][-4.3131309 -4.3186569 -4.3057847 -4.2710032 -4.2187505 -4.1669979 -4.1502323 -4.1889744 -4.2396317 -4.2438841 -4.1981025 -4.160706 -4.1865888 -4.2518153 -4.29984][-4.3072119 -4.3126178 -4.304038 -4.2812133 -4.2471604 -4.2136264 -4.201941 -4.2274003 -4.264492 -4.2704611 -4.2305503 -4.1902976 -4.2093663 -4.2682223 -4.3073626][-4.3052773 -4.3103747 -4.3058691 -4.29289 -4.2737412 -4.2545652 -4.2491994 -4.266902 -4.2887692 -4.2894449 -4.2568531 -4.226985 -4.2450333 -4.2910647 -4.3174076][-4.3139238 -4.3195109 -4.3175583 -4.3123016 -4.3011951 -4.2892094 -4.2866263 -4.2990818 -4.3080416 -4.3020635 -4.277566 -4.2616134 -4.2784996 -4.3091335 -4.3230743][-4.3200068 -4.3256 -4.3277369 -4.327868 -4.3218203 -4.312129 -4.3092 -4.3168583 -4.3196163 -4.3109851 -4.2929773 -4.286325 -4.2994475 -4.3177557 -4.3243265]]...]
INFO - root - 2017-12-06 05:01:27.914731: step 4710, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:53s remains)
INFO - root - 2017-12-06 05:01:30.069172: step 4720, loss = 2.04, batch loss = 1.98 (37.3 examples/sec; 0.214 sec/batch; 19h:31m:34s remains)
INFO - root - 2017-12-06 05:01:32.211385: step 4730, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:26s remains)
INFO - root - 2017-12-06 05:01:34.415839: step 4740, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 20h:22m:43s remains)
INFO - root - 2017-12-06 05:01:36.626092: step 4750, loss = 2.03, batch loss = 1.98 (37.9 examples/sec; 0.211 sec/batch; 19h:13m:55s remains)
INFO - root - 2017-12-06 05:01:38.800886: step 4760, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:44m:57s remains)
INFO - root - 2017-12-06 05:01:40.947912: step 4770, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.214 sec/batch; 19h:26m:17s remains)
INFO - root - 2017-12-06 05:01:43.124897: step 4780, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:29m:43s remains)
INFO - root - 2017-12-06 05:01:45.321847: step 4790, loss = 2.08, batch loss = 2.02 (37.8 examples/sec; 0.211 sec/batch; 19h:15m:00s remains)
INFO - root - 2017-12-06 05:01:47.487674: step 4800, loss = 2.05, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 19h:56m:36s remains)
2017-12-06 05:01:48.863181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2278171 -4.2295866 -4.2327328 -4.2321086 -4.2322879 -4.2362671 -4.2418227 -4.2500868 -4.2653832 -4.2808957 -4.2935534 -4.2953334 -4.2848406 -4.2757158 -4.2787867][-4.1771693 -4.18196 -4.1895328 -4.1916685 -4.1915841 -4.1935077 -4.198307 -4.2065687 -4.2257376 -4.2492752 -4.2720819 -4.2852588 -4.2860451 -4.2825975 -4.2849326][-4.1382909 -4.1519928 -4.1638026 -4.1673059 -4.1643286 -4.1610503 -4.1614237 -4.1666055 -4.1848736 -4.2121868 -4.2410355 -4.2623043 -4.2735991 -4.2782965 -4.2850609][-4.1082463 -4.1319747 -4.1468391 -4.1524854 -4.14832 -4.1392741 -4.134562 -4.134697 -4.1495776 -4.17636 -4.2064366 -4.2320104 -4.2498078 -4.2626743 -4.2750607][-4.1004591 -4.121429 -4.13264 -4.1347346 -4.1252508 -4.1068888 -4.0936737 -4.0891728 -4.1037941 -4.1340275 -4.1677065 -4.1962233 -4.2172532 -4.2340708 -4.2493548][-4.1217804 -4.1306443 -4.1291847 -4.1170588 -4.0902591 -4.0541053 -4.02549 -4.0079317 -4.02415 -4.0715728 -4.123013 -4.160398 -4.1836367 -4.1999936 -4.2134][-4.1623673 -4.1642017 -4.1493931 -4.1179094 -4.0675941 -4.0083079 -3.9539468 -3.9073942 -3.9162905 -3.98909 -4.0684476 -4.1224747 -4.1504889 -4.164444 -4.1746707][-4.1996684 -4.2045708 -4.18524 -4.140667 -4.0724268 -3.9947999 -3.9182792 -3.844348 -3.8375249 -3.9200916 -4.0165339 -4.0828047 -4.116519 -4.13276 -4.1438928][-4.2247138 -4.23658 -4.2206411 -4.175426 -4.1079106 -4.0286884 -3.951134 -3.8732746 -3.8548093 -3.9161403 -3.9961836 -4.0550895 -4.087285 -4.1065788 -4.12227][-4.2367129 -4.254365 -4.2494307 -4.2167897 -4.1639066 -4.0963292 -4.030457 -3.9671369 -3.9446411 -3.9752555 -4.0222812 -4.0606685 -4.0838375 -4.0995636 -4.1154461][-4.2385316 -4.257905 -4.2657728 -4.2511473 -4.2159796 -4.1661339 -4.1177387 -4.0749044 -4.056457 -4.069139 -4.0910153 -4.111362 -4.1237249 -4.12976 -4.1350079][-4.2332373 -4.2506084 -4.2696886 -4.2691154 -4.2485089 -4.2159362 -4.1857796 -4.1617279 -4.152812 -4.1601791 -4.1696343 -4.1785283 -4.183166 -4.1825757 -4.1757774][-4.2232947 -4.2352357 -4.257647 -4.2658529 -4.2562876 -4.2391758 -4.2247133 -4.21798 -4.2192311 -4.2251277 -4.2287107 -4.2295227 -4.2289119 -4.2281351 -4.2217479][-4.2088137 -4.2111282 -4.2296643 -4.2412214 -4.2409191 -4.2351308 -4.2322383 -4.2383547 -4.2470894 -4.2545829 -4.2575932 -4.2555175 -4.25247 -4.256238 -4.2588277][-4.1961985 -4.1901512 -4.2024965 -4.2125535 -4.2175179 -4.2175336 -4.2217617 -4.2345057 -4.2469211 -4.2562566 -4.2609186 -4.2601619 -4.2577972 -4.2648396 -4.2755914]]...]
INFO - root - 2017-12-06 05:01:51.053133: step 4810, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:23s remains)
INFO - root - 2017-12-06 05:01:53.235821: step 4820, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 20h:02m:02s remains)
INFO - root - 2017-12-06 05:01:55.396558: step 4830, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:57s remains)
INFO - root - 2017-12-06 05:01:57.579668: step 4840, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 19h:53m:58s remains)
INFO - root - 2017-12-06 05:01:59.788314: step 4850, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:11m:37s remains)
INFO - root - 2017-12-06 05:02:01.991399: step 4860, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:18s remains)
INFO - root - 2017-12-06 05:02:04.178425: step 4870, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:40m:29s remains)
INFO - root - 2017-12-06 05:02:06.365759: step 4880, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:09s remains)
INFO - root - 2017-12-06 05:02:08.541758: step 4890, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 20h:28m:25s remains)
INFO - root - 2017-12-06 05:02:10.687564: step 4900, loss = 2.05, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:12m:07s remains)
2017-12-06 05:02:11.119491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2771492 -4.2687068 -4.2595987 -4.2500262 -4.2421632 -4.2356257 -4.2298579 -4.2279735 -4.2352624 -4.2508349 -4.2609463 -4.2581344 -4.2498603 -4.2477942 -4.2527642][-4.2408657 -4.2373853 -4.2317328 -4.2246304 -4.2220325 -4.2227435 -4.2224622 -4.2232766 -4.2307596 -4.2440243 -4.2522354 -4.2463732 -4.2349105 -4.2325382 -4.2399993][-4.1920319 -4.199996 -4.2024822 -4.200685 -4.20566 -4.2131848 -4.215652 -4.2151628 -4.2178688 -4.2249575 -4.2282333 -4.2197294 -4.2073617 -4.2060089 -4.2170224][-4.1504812 -4.1687632 -4.1774931 -4.1790972 -4.1864963 -4.1940293 -4.192709 -4.1842208 -4.177434 -4.1778154 -4.1782379 -4.1705751 -4.1623974 -4.1647911 -4.1788177][-4.1349969 -4.1487856 -4.1518068 -4.148602 -4.1487875 -4.1485429 -4.1378293 -4.1196785 -4.1076932 -4.1110849 -4.11801 -4.116498 -4.1141162 -4.1194139 -4.135015][-4.1447206 -4.1357927 -4.1189332 -4.0997658 -4.0859084 -4.0729203 -4.0539532 -4.0337291 -4.0341439 -4.0581779 -4.0850654 -4.0974565 -4.1016736 -4.1048574 -4.1129961][-4.1468062 -4.1098547 -4.0682292 -4.0313282 -4.0072284 -3.9910028 -3.9740145 -3.9653976 -3.9922729 -4.0419731 -4.0898218 -4.1168652 -4.1270342 -4.1254096 -4.1209888][-4.1335645 -4.0809894 -4.0325661 -3.9944196 -3.9754951 -3.9703493 -3.9683187 -3.9773142 -4.0168123 -4.0699897 -4.1167946 -4.1435475 -4.1541858 -4.1508846 -4.1405106][-4.1248951 -4.0813384 -4.0518641 -4.0333266 -4.0276904 -4.0314074 -4.0373363 -4.0496883 -4.0804644 -4.117846 -4.1490397 -4.1665921 -4.1733451 -4.1711664 -4.1624732][-4.1155744 -4.09559 -4.0976005 -4.1057281 -4.11421 -4.1192589 -4.1221695 -4.12674 -4.14048 -4.1590061 -4.1713767 -4.1762114 -4.1784515 -4.1808963 -4.1815162][-4.0956354 -4.1015515 -4.13006 -4.1596766 -4.1806345 -4.1899481 -4.1907353 -4.1884551 -4.1890292 -4.1904588 -4.1852942 -4.1762753 -4.1725073 -4.1758142 -4.1805196][-4.0852547 -4.1047611 -4.143507 -4.1794553 -4.2070546 -4.2222648 -4.2265387 -4.224813 -4.2174125 -4.2061982 -4.1910634 -4.1743779 -4.1638556 -4.1597471 -4.1566477][-4.104641 -4.1186581 -4.1484079 -4.1772475 -4.2031646 -4.2204027 -4.2285156 -4.2308497 -4.2223506 -4.2081614 -4.1938467 -4.1798911 -4.1647491 -4.1475773 -4.1293669][-4.139595 -4.1464686 -4.1650872 -4.1838484 -4.2009673 -4.2127519 -4.21872 -4.2207103 -4.2147756 -4.2073326 -4.2021184 -4.1934228 -4.1738372 -4.1446319 -4.1159143][-4.1754069 -4.1778741 -4.187809 -4.1975212 -4.2037463 -4.2061386 -4.2067442 -4.2056003 -4.2018251 -4.20106 -4.2032857 -4.1982741 -4.1769867 -4.1450057 -4.1174083]]...]
INFO - root - 2017-12-06 05:02:13.342819: step 4910, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:18m:52s remains)
INFO - root - 2017-12-06 05:02:15.536646: step 4920, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 21h:06m:21s remains)
INFO - root - 2017-12-06 05:02:17.691091: step 4930, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.214 sec/batch; 19h:25m:57s remains)
INFO - root - 2017-12-06 05:02:19.874213: step 4940, loss = 2.07, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:29s remains)
INFO - root - 2017-12-06 05:02:22.081164: step 4950, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:02m:57s remains)
INFO - root - 2017-12-06 05:02:24.276053: step 4960, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:27m:49s remains)
INFO - root - 2017-12-06 05:02:26.472792: step 4970, loss = 2.06, batch loss = 2.01 (38.1 examples/sec; 0.210 sec/batch; 19h:07m:37s remains)
INFO - root - 2017-12-06 05:02:28.669980: step 4980, loss = 2.09, batch loss = 2.04 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:34s remains)
INFO - root - 2017-12-06 05:02:30.869348: step 4990, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:39m:36s remains)
INFO - root - 2017-12-06 05:02:33.041057: step 5000, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 20h:33m:39s remains)
2017-12-06 05:02:33.967483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3477616 -4.317255 -4.2707343 -4.2306004 -4.2027087 -4.18299 -4.1686449 -4.1513915 -4.1427374 -4.1629 -4.194891 -4.2095432 -4.2025685 -4.1897306 -4.1813483][-4.3560781 -4.327085 -4.2823129 -4.2454724 -4.2197242 -4.2001867 -4.1756158 -4.1376762 -4.10912 -4.1241446 -4.164834 -4.1910453 -4.1902037 -4.1757188 -4.1612825][-4.3600287 -4.3312087 -4.288538 -4.2529135 -4.2266526 -4.2087722 -4.1803513 -4.1268535 -4.080708 -4.0902829 -4.13497 -4.166472 -4.1648488 -4.1457849 -4.1306129][-4.3573089 -4.3280015 -4.2820086 -4.2385297 -4.2080731 -4.1966171 -4.1740885 -4.1246114 -4.0817976 -4.0904403 -4.133584 -4.1644258 -4.158534 -4.1334834 -4.1194572][-4.3524852 -4.3212581 -4.2645507 -4.2065725 -4.165947 -4.1592469 -4.1535358 -4.1296983 -4.1112571 -4.1233983 -4.1590576 -4.1843109 -4.1779361 -4.1532431 -4.1387296][-4.3472142 -4.3111467 -4.2416978 -4.1667366 -4.1105113 -4.098146 -4.1037016 -4.1105652 -4.1269383 -4.1534352 -4.19014 -4.21627 -4.2118196 -4.1882749 -4.1759858][-4.3416915 -4.3029542 -4.2276 -4.1389923 -4.0606275 -4.0272245 -4.0302711 -4.0615706 -4.1073308 -4.1518674 -4.1997089 -4.2357926 -4.236299 -4.2126808 -4.1991992][-4.3369236 -4.3011694 -4.2360697 -4.1535697 -4.06793 -4.0159459 -4.0041375 -4.0380292 -4.0911293 -4.1409373 -4.1880774 -4.2250581 -4.2326641 -4.2169733 -4.203208][-4.33528 -4.3041406 -4.25457 -4.1934247 -4.1263981 -4.0792093 -4.0625029 -4.0861158 -4.1278076 -4.1584868 -4.1867361 -4.2116027 -4.216157 -4.2036228 -4.1927152][-4.33693 -4.3102741 -4.2731271 -4.2347484 -4.1943307 -4.1610427 -4.151197 -4.1703858 -4.2003732 -4.209394 -4.2125731 -4.2192631 -4.2172403 -4.2062268 -4.1991043][-4.3377638 -4.313199 -4.2842708 -4.2620826 -4.2411904 -4.2224355 -4.2207031 -4.2360926 -4.2563925 -4.2567315 -4.2482872 -4.2424874 -4.2360592 -4.2269945 -4.2231112][-4.3373857 -4.3134308 -4.2856512 -4.2673893 -4.2526445 -4.2405915 -4.240694 -4.2511172 -4.2655115 -4.2681971 -4.2617407 -4.2522693 -4.2411041 -4.233036 -4.235528][-4.3321438 -4.3046927 -4.2724853 -4.2493839 -4.2323875 -4.2201324 -4.220962 -4.2292056 -4.2369537 -4.2400255 -4.2370486 -4.227726 -4.2165031 -4.2129512 -4.2251244][-4.3229928 -4.2902169 -4.2494764 -4.2155981 -4.1885748 -4.171833 -4.1729703 -4.1835446 -4.187284 -4.1882172 -4.1897593 -4.18429 -4.1755838 -4.1790957 -4.1972904][-4.31764 -4.28399 -4.2370505 -4.1946716 -4.1534338 -4.1273117 -4.1254072 -4.1330976 -4.1341085 -4.1345825 -4.1398125 -4.1369576 -4.1329618 -4.1445866 -4.1673231]]...]
INFO - root - 2017-12-06 05:02:36.135622: step 5010, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:58s remains)
INFO - root - 2017-12-06 05:02:38.330183: step 5020, loss = 2.07, batch loss = 2.01 (36.8 examples/sec; 0.217 sec/batch; 19h:45m:15s remains)
INFO - root - 2017-12-06 05:02:40.485267: step 5030, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:30m:00s remains)
INFO - root - 2017-12-06 05:02:42.655790: step 5040, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 20h:01m:41s remains)
INFO - root - 2017-12-06 05:02:44.847249: step 5050, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.209 sec/batch; 18h:58m:04s remains)
INFO - root - 2017-12-06 05:02:47.011014: step 5060, loss = 2.04, batch loss = 1.98 (37.3 examples/sec; 0.215 sec/batch; 19h:30m:40s remains)
INFO - root - 2017-12-06 05:02:49.157852: step 5070, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:38m:01s remains)
INFO - root - 2017-12-06 05:02:51.366778: step 5080, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:40m:48s remains)
INFO - root - 2017-12-06 05:02:53.561599: step 5090, loss = 2.07, batch loss = 2.02 (36.9 examples/sec; 0.217 sec/batch; 19h:44m:10s remains)
INFO - root - 2017-12-06 05:02:55.741828: step 5100, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:50m:50s remains)
2017-12-06 05:02:56.077174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2581544 -4.2630887 -4.2694712 -4.2613935 -4.2429895 -4.2228618 -4.2074237 -4.1834617 -4.1668057 -4.1520753 -4.1465349 -4.1431808 -4.1460719 -4.1698155 -4.2075062][-4.2618771 -4.26993 -4.2760797 -4.2676063 -4.2535496 -4.2395215 -4.22153 -4.18576 -4.15636 -4.1298642 -4.1167727 -4.1186423 -4.1342077 -4.1666656 -4.2064128][-4.2563009 -4.2664342 -4.2671609 -4.2606506 -4.2543373 -4.245677 -4.2206621 -4.1728787 -4.134604 -4.1031752 -4.0892224 -4.1013837 -4.1315832 -4.172823 -4.2126284][-4.2405591 -4.2538724 -4.2561731 -4.2536368 -4.2530508 -4.2451577 -4.2103782 -4.1593442 -4.1220269 -4.0917969 -4.0821161 -4.1017518 -4.143456 -4.1896529 -4.2266583][-4.2290707 -4.2417731 -4.2466321 -4.24759 -4.2485027 -4.2351341 -4.1945357 -4.1534758 -4.1249456 -4.1007743 -4.0961866 -4.1180439 -4.1604524 -4.2027006 -4.234601][-4.2186832 -4.2258067 -4.2342176 -4.2396169 -4.2363048 -4.2101669 -4.1643786 -4.1344781 -4.1216111 -4.1120782 -4.1153994 -4.136529 -4.170681 -4.2038703 -4.2303023][-4.205512 -4.2106075 -4.2181034 -4.2142038 -4.1946182 -4.1520071 -4.1031365 -4.0881844 -4.0991592 -4.1120391 -4.128427 -4.15087 -4.1753869 -4.199954 -4.219521][-4.209425 -4.2134485 -4.2077022 -4.1807003 -4.1321311 -4.06553 -4.0074892 -4.0080457 -4.0480738 -4.0910721 -4.1300383 -4.1588964 -4.1784887 -4.1940117 -4.2048173][-4.223175 -4.2248526 -4.2047062 -4.1513925 -4.0671554 -3.9670227 -3.8975048 -3.925658 -3.9977117 -4.0688982 -4.127049 -4.1603112 -4.1756 -4.1824741 -4.1840663][-4.2338243 -4.2246184 -4.1913018 -4.1215749 -4.0173883 -3.9027348 -3.8484709 -3.9136431 -4.0059295 -4.0810175 -4.1362729 -4.1681705 -4.17938 -4.180017 -4.1734219][-4.2436552 -4.2191973 -4.172637 -4.1046262 -4.013 -3.9279099 -3.9147131 -3.9970908 -4.0767274 -4.1310606 -4.1683016 -4.1911354 -4.1987176 -4.197578 -4.1907063][-4.2585697 -4.216763 -4.1582675 -4.0972147 -4.0347853 -3.9929457 -4.0091853 -4.0905428 -4.156311 -4.1944513 -4.2168403 -4.2307687 -4.2353144 -4.2354593 -4.2324109][-4.2731352 -4.2159247 -4.1459718 -4.0831242 -4.0406542 -4.032043 -4.0697246 -4.1427979 -4.1985831 -4.2296572 -4.2465048 -4.2581639 -4.2648568 -4.2704229 -4.2735677][-4.27125 -4.2035079 -4.1208363 -4.052743 -4.0255342 -4.045361 -4.09847 -4.1638832 -4.210083 -4.2382379 -4.2536287 -4.2665825 -4.2770085 -4.2877674 -4.29648][-4.2481918 -4.1731687 -4.0826187 -4.0137033 -3.9995248 -4.0401478 -4.1066403 -4.1680889 -4.2069063 -4.2318439 -4.2471008 -4.2627969 -4.2780452 -4.2937851 -4.3054676]]...]
INFO - root - 2017-12-06 05:02:58.277120: step 5110, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 20h:04m:58s remains)
INFO - root - 2017-12-06 05:03:00.475588: step 5120, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 20h:01m:51s remains)
INFO - root - 2017-12-06 05:03:02.638419: step 5130, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-06 05:03:04.813940: step 5140, loss = 2.06, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:44m:35s remains)
INFO - root - 2017-12-06 05:03:06.932342: step 5150, loss = 2.05, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:33s remains)
INFO - root - 2017-12-06 05:03:09.133078: step 5160, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.226 sec/batch; 20h:30m:40s remains)
INFO - root - 2017-12-06 05:03:11.304319: step 5170, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:00s remains)
INFO - root - 2017-12-06 05:03:13.493986: step 5180, loss = 2.05, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:25m:27s remains)
INFO - root - 2017-12-06 05:03:15.620222: step 5190, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 19h:45m:47s remains)
INFO - root - 2017-12-06 05:03:17.766145: step 5200, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:24m:08s remains)
2017-12-06 05:03:18.143120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3018661 -4.2825813 -4.2716079 -4.2639303 -4.2561021 -4.23598 -4.2105403 -4.1839075 -4.1735907 -4.1828408 -4.2018313 -4.2254176 -4.2278953 -4.2208819 -4.2014661][-4.3010459 -4.281405 -4.2718534 -4.2659063 -4.2603106 -4.2386765 -4.2068415 -4.1794419 -4.1742716 -4.1858745 -4.2029991 -4.2151604 -4.2058554 -4.1965561 -4.1806054][-4.2982583 -4.2789354 -4.2708397 -4.2662787 -4.2615943 -4.2389631 -4.2018542 -4.1714439 -4.1693172 -4.1829743 -4.1966872 -4.2016768 -4.1859083 -4.1749392 -4.1649475][-4.2964182 -4.2796545 -4.2705603 -4.262116 -4.2546625 -4.2303777 -4.187881 -4.1516252 -4.1468611 -4.1602125 -4.1748204 -4.1771197 -4.1629634 -4.1566057 -4.1546936][-4.2955136 -4.2819285 -4.269835 -4.2529612 -4.2367125 -4.2097178 -4.16221 -4.1198254 -4.1119933 -4.1321392 -4.1562419 -4.1620426 -4.1491613 -4.1458321 -4.1498852][-4.2903147 -4.279953 -4.2663994 -4.242754 -4.2184167 -4.1895194 -4.1438465 -4.0976281 -4.0874882 -4.1162949 -4.1535282 -4.1667037 -4.1538739 -4.1496572 -4.1530671][-4.2811475 -4.272305 -4.2560725 -4.2264881 -4.1956892 -4.1656365 -4.1267114 -4.0797863 -4.0659952 -4.1018834 -4.1456637 -4.1607146 -4.1496077 -4.1453524 -4.1468][-4.2649784 -4.2532611 -4.2294521 -4.1907377 -4.1558857 -4.1322818 -4.1078506 -4.06888 -4.0501208 -4.08508 -4.1333003 -4.14508 -4.1324267 -4.1203141 -4.1156716][-4.2472072 -4.2302294 -4.2010708 -4.157413 -4.1225996 -4.1111488 -4.1082134 -4.0842814 -4.0594435 -4.0816078 -4.126236 -4.1384435 -4.1230297 -4.1035023 -4.0943][-4.2304826 -4.2144079 -4.1914306 -4.1532021 -4.12324 -4.1175451 -4.1291175 -4.123076 -4.1011643 -4.108407 -4.14419 -4.1574855 -4.1503153 -4.1399803 -4.1355114][-4.2162147 -4.209959 -4.205318 -4.18584 -4.1699986 -4.1699767 -4.1861038 -4.1945167 -4.1839876 -4.183826 -4.206953 -4.2169261 -4.2139955 -4.2091737 -4.2022004][-4.2036185 -4.2095027 -4.2239513 -4.2261662 -4.2285042 -4.2374563 -4.254189 -4.2672606 -4.2626429 -4.2581034 -4.2670541 -4.2698421 -4.2664008 -4.2595887 -4.2473578][-4.2090421 -4.2154293 -4.2348261 -4.2473407 -4.2580891 -4.2705455 -4.2861056 -4.3004155 -4.2984157 -4.2935944 -4.29611 -4.2965336 -4.2946177 -4.2890067 -4.2764888][-4.2335649 -4.2307539 -4.2414141 -4.2537637 -4.2697587 -4.2833738 -4.2945175 -4.3076248 -4.3080111 -4.3054609 -4.304781 -4.3017044 -4.2989221 -4.2941127 -4.284059][-4.2645292 -4.2509127 -4.2482758 -4.2552261 -4.2716007 -4.2842259 -4.2891421 -4.2964168 -4.2965627 -4.2956171 -4.2929835 -4.2870259 -4.2830567 -4.2806168 -4.2752724]]...]
INFO - root - 2017-12-06 05:03:20.325181: step 5210, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 20h:19m:09s remains)
INFO - root - 2017-12-06 05:03:22.461855: step 5220, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:25m:45s remains)
INFO - root - 2017-12-06 05:03:24.601586: step 5230, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 20h:07m:43s remains)
INFO - root - 2017-12-06 05:03:26.755591: step 5240, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:20m:44s remains)
INFO - root - 2017-12-06 05:03:28.943121: step 5250, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:16m:07s remains)
INFO - root - 2017-12-06 05:03:31.098056: step 5260, loss = 2.10, batch loss = 2.04 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:42s remains)
INFO - root - 2017-12-06 05:03:33.285569: step 5270, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 19h:52m:44s remains)
INFO - root - 2017-12-06 05:03:35.445100: step 5280, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:41m:12s remains)
INFO - root - 2017-12-06 05:03:37.573222: step 5290, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 18h:47m:21s remains)
INFO - root - 2017-12-06 05:03:39.800207: step 5300, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:37m:59s remains)
2017-12-06 05:03:40.245438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3210855 -4.31973 -4.3167195 -4.3111935 -4.3048782 -4.2982607 -4.2961035 -4.2983465 -4.2995882 -4.3040962 -4.3082728 -4.309896 -4.3110962 -4.3110657 -4.3106937][-4.3174438 -4.3106952 -4.3024712 -4.2942734 -4.2867079 -4.2812948 -4.2825146 -4.2879863 -4.2900987 -4.2959538 -4.30116 -4.3016157 -4.3014088 -4.2990985 -4.2974133][-4.3058386 -4.2916241 -4.2738585 -4.2606997 -4.2513208 -4.250628 -4.2583 -4.266993 -4.2695246 -4.2774081 -4.2853189 -4.28702 -4.28903 -4.2871652 -4.285038][-4.2763481 -4.2572861 -4.2324266 -4.2163291 -4.207077 -4.2089014 -4.2201247 -4.2327476 -4.2387552 -4.2517405 -4.2612362 -4.2653289 -4.2723532 -4.2754164 -4.2749133][-4.21781 -4.1995173 -4.1722307 -4.1532683 -4.1425385 -4.1456423 -4.1541672 -4.1694512 -4.1858606 -4.211772 -4.2267418 -4.2334785 -4.2468042 -4.2574663 -4.2626953][-4.1381631 -4.1263785 -4.0996513 -4.0702963 -4.0463223 -4.0437751 -4.0384517 -4.0540152 -4.0872879 -4.1348629 -4.1675229 -4.1869183 -4.2120867 -4.2316661 -4.2438679][-4.0660419 -4.0563312 -4.0277977 -3.9826975 -3.9382114 -3.9193137 -3.8928401 -3.9060094 -3.9567449 -4.0255532 -4.0830832 -4.1261873 -4.1689248 -4.1992078 -4.2188315][-4.0356865 -4.0240259 -3.9923124 -3.9382265 -3.8876419 -3.8545768 -3.803668 -3.8028722 -3.8557549 -3.938803 -4.0174608 -4.0830269 -4.1403918 -4.1776767 -4.2046728][-4.0777068 -4.0658917 -4.0398712 -3.9938862 -3.960398 -3.9294567 -3.8702946 -3.8483593 -3.8752637 -3.9461229 -4.0203915 -4.0863833 -4.1445847 -4.1817961 -4.2095666][-4.1463451 -4.1371307 -4.1189895 -4.0904245 -4.0719023 -4.0500269 -4.0032187 -3.9798324 -3.987638 -4.0329256 -4.0869346 -4.1351752 -4.1788287 -4.2059741 -4.2285852][-4.1986475 -4.1913714 -4.1832047 -4.17115 -4.1621203 -4.146461 -4.11385 -4.1000223 -4.1021595 -4.1281843 -4.1628847 -4.1956196 -4.2248144 -4.238462 -4.2516589][-4.2358322 -4.2310987 -4.2268729 -4.2220554 -4.2191143 -4.2102714 -4.1923075 -4.1864552 -4.1872506 -4.2043447 -4.2274323 -4.250669 -4.2683811 -4.2738352 -4.2774153][-4.2592888 -4.2528477 -4.2471495 -4.24565 -4.2455087 -4.2421169 -4.2350059 -4.2338424 -4.2329478 -4.2457013 -4.2625289 -4.278656 -4.2904205 -4.2929025 -4.2933969][-4.2702217 -4.2621093 -4.2533154 -4.2513576 -4.2522516 -4.2508507 -4.2480021 -4.2487035 -4.2484856 -4.2580695 -4.2713466 -4.2834215 -4.2937627 -4.2966232 -4.2988582][-4.2731934 -4.2645721 -4.255425 -4.2524447 -4.2531137 -4.2527032 -4.2510056 -4.2517872 -4.2526083 -4.2595234 -4.2693958 -4.2787585 -4.2881856 -4.2942057 -4.2995291]]...]
INFO - root - 2017-12-06 05:03:42.429494: step 5310, loss = 2.11, batch loss = 2.06 (35.8 examples/sec; 0.224 sec/batch; 20h:19m:20s remains)
INFO - root - 2017-12-06 05:03:44.628075: step 5320, loss = 2.04, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 20h:05m:03s remains)
INFO - root - 2017-12-06 05:03:46.780988: step 5330, loss = 2.07, batch loss = 2.02 (37.1 examples/sec; 0.215 sec/batch; 19h:34m:40s remains)
INFO - root - 2017-12-06 05:03:48.970863: step 5340, loss = 2.04, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:10m:49s remains)
INFO - root - 2017-12-06 05:03:51.120797: step 5350, loss = 2.06, batch loss = 2.01 (38.6 examples/sec; 0.207 sec/batch; 18h:51m:06s remains)
INFO - root - 2017-12-06 05:03:53.300043: step 5360, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:21m:48s remains)
INFO - root - 2017-12-06 05:03:55.384195: step 5370, loss = 2.07, batch loss = 2.01 (38.9 examples/sec; 0.206 sec/batch; 18h:42m:30s remains)
INFO - root - 2017-12-06 05:03:57.487144: step 5380, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:27m:31s remains)
INFO - root - 2017-12-06 05:03:59.646917: step 5390, loss = 2.09, batch loss = 2.03 (35.7 examples/sec; 0.224 sec/batch; 20h:20m:26s remains)
INFO - root - 2017-12-06 05:04:01.822072: step 5400, loss = 2.05, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:11m:44s remains)
2017-12-06 05:04:02.209647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2238069 -4.1905737 -4.1520452 -4.1269531 -4.12621 -4.1397762 -4.16279 -4.1939478 -4.2161407 -4.2138886 -4.1943746 -4.1871481 -4.2070804 -4.2458682 -4.2832303][-4.196414 -4.1607313 -4.1187239 -4.0954251 -4.095428 -4.1062708 -4.1267934 -4.1600695 -4.1898432 -4.1975636 -4.179266 -4.17044 -4.1870937 -4.223959 -4.2631445][-4.1731725 -4.1453466 -4.1036925 -4.0808015 -4.0776296 -4.0806231 -4.0901651 -4.1190286 -4.1573958 -4.1794429 -4.1727242 -4.1675363 -4.1783247 -4.2070985 -4.2417612][-4.144248 -4.1255007 -4.0934711 -4.0717692 -4.0605907 -4.0494666 -4.0426083 -4.0646162 -4.1132855 -4.1523738 -4.16316 -4.1682477 -4.1798944 -4.2007828 -4.2278695][-4.1194935 -4.1042848 -4.0793495 -4.0544767 -4.0280252 -3.9894376 -3.954391 -3.9665525 -4.0366774 -4.10261 -4.1353474 -4.1558118 -4.1737204 -4.1938572 -4.2169957][-4.1066403 -4.08634 -4.0618172 -4.0294476 -3.9811883 -3.9011772 -3.8174818 -3.8097196 -3.9086604 -4.0083437 -4.0695739 -4.1101017 -4.1424785 -4.1702871 -4.1972022][-4.112206 -4.0867667 -4.0616679 -4.0196943 -3.948216 -3.8337247 -3.70821 -3.6803455 -3.7917557 -3.9040179 -3.9837177 -4.0414567 -4.0909572 -4.1308942 -4.1659307][-4.1340032 -4.1143231 -4.0931888 -4.0556436 -3.9868524 -3.8800709 -3.7644131 -3.728404 -3.7987826 -3.8777208 -3.9405258 -3.9933257 -4.0475354 -4.094553 -4.13809][-4.1679978 -4.1618581 -4.150435 -4.1255579 -4.0762053 -3.9959278 -3.911303 -3.8805065 -3.9112885 -3.9495428 -3.9812515 -4.0141711 -4.0575109 -4.0977693 -4.137569][-4.2080016 -4.2139359 -4.214901 -4.2038264 -4.1775823 -4.1315107 -4.0804739 -4.05776 -4.0711741 -4.0845771 -4.090755 -4.1024079 -4.1285577 -4.1538053 -4.1791425][-4.25708 -4.2648144 -4.2697597 -4.263938 -4.2527556 -4.2355642 -4.2146487 -4.2036862 -4.2092018 -4.2113113 -4.2038803 -4.2014737 -4.2138414 -4.2274294 -4.2384429][-4.2942991 -4.3009257 -4.3040318 -4.300293 -4.2952514 -4.2911196 -4.2873731 -4.2853241 -4.2878981 -4.2861695 -4.2764816 -4.2690248 -4.2724519 -4.2783384 -4.2821035][-4.3158669 -4.3205781 -4.3224678 -4.3201652 -4.3176436 -4.3164158 -4.3165979 -4.3167348 -4.3162789 -4.3125105 -4.3056049 -4.3005986 -4.3007374 -4.30272 -4.3041658][-4.3209181 -4.3232965 -4.3253341 -4.3251519 -4.3239727 -4.3226233 -4.3213062 -4.3193426 -4.3172793 -4.3143864 -4.3114462 -4.3098593 -4.3102756 -4.3117042 -4.3131275][-4.3203568 -4.3205886 -4.3219662 -4.323308 -4.3234558 -4.3227057 -4.3209505 -4.318296 -4.3166342 -4.3157158 -4.3151774 -4.3148174 -4.3148923 -4.31572 -4.3174047]]...]
INFO - root - 2017-12-06 05:04:04.391160: step 5410, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:28m:47s remains)
INFO - root - 2017-12-06 05:04:06.541456: step 5420, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:07s remains)
INFO - root - 2017-12-06 05:04:08.724930: step 5430, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:47s remains)
INFO - root - 2017-12-06 05:04:10.895155: step 5440, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:37m:32s remains)
INFO - root - 2017-12-06 05:04:13.104004: step 5450, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:40m:37s remains)
INFO - root - 2017-12-06 05:04:15.273545: step 5460, loss = 2.09, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 20h:16m:04s remains)
INFO - root - 2017-12-06 05:04:17.458989: step 5470, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:16m:42s remains)
INFO - root - 2017-12-06 05:04:19.601691: step 5480, loss = 2.08, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:58s remains)
INFO - root - 2017-12-06 05:04:21.768169: step 5490, loss = 2.09, batch loss = 2.03 (35.9 examples/sec; 0.223 sec/batch; 20h:14m:05s remains)
INFO - root - 2017-12-06 05:04:23.975368: step 5500, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 21h:16m:21s remains)
2017-12-06 05:04:24.385241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1943736 -4.1842651 -4.182878 -4.1841788 -4.1789 -4.173986 -4.1753287 -4.1851764 -4.1990147 -4.2094445 -4.2208409 -4.240571 -4.248702 -4.2414842 -4.2323866][-4.1800413 -4.1745205 -4.1804261 -4.1922345 -4.190906 -4.1821566 -4.1775236 -4.1811376 -4.1916661 -4.1990647 -4.2054482 -4.2274771 -4.2415347 -4.2395816 -4.2321687][-4.1654739 -4.167223 -4.177084 -4.190279 -4.1928511 -4.1865811 -4.1865134 -4.1906829 -4.1959796 -4.1980391 -4.1947937 -4.2122784 -4.2328811 -4.2388105 -4.2364516][-4.1495771 -4.1525078 -4.1639748 -4.1735773 -4.1797328 -4.1804118 -4.1846395 -4.1867385 -4.1895466 -4.1888928 -4.1756582 -4.1827035 -4.20914 -4.22694 -4.2355385][-4.128161 -4.13101 -4.1374636 -4.1412959 -4.1489739 -4.1545944 -4.1657758 -4.1761847 -4.1820965 -4.1797895 -4.1552958 -4.1456428 -4.1674957 -4.189362 -4.2088747][-4.1066594 -4.1031909 -4.1038361 -4.09331 -4.0758929 -4.0684481 -4.0973163 -4.1363726 -4.1620579 -4.1680741 -4.1467166 -4.1251879 -4.1329527 -4.1443996 -4.1623449][-4.0720372 -4.058671 -4.0491338 -4.0152564 -3.9406471 -3.8837194 -3.9418631 -4.0398636 -4.1047149 -4.134 -4.1263146 -4.1110439 -4.1161418 -4.1230855 -4.1359229][-4.0418859 -4.0180817 -4.0052223 -3.9602275 -3.8374546 -3.7011766 -3.770503 -3.9355919 -4.0443559 -4.0962324 -4.1019006 -4.1045008 -4.1236229 -4.1349998 -4.1452913][-4.0647326 -4.0482378 -4.045599 -4.0188384 -3.9239788 -3.7939322 -3.8141274 -3.9549034 -4.0577917 -4.1039147 -4.1172929 -4.1314 -4.1571779 -4.172791 -4.1709914][-4.102932 -4.1011677 -4.1092553 -4.1071768 -4.0602241 -3.9914169 -3.9851565 -4.057085 -4.1188936 -4.1430483 -4.1600366 -4.176867 -4.199892 -4.2138495 -4.2071338][-4.1303453 -4.1372466 -4.1514573 -4.1679077 -4.1603713 -4.1359072 -4.1242785 -4.1458039 -4.1707964 -4.1789036 -4.1899281 -4.2056479 -4.2283959 -4.2390094 -4.2340922][-4.15924 -4.1741567 -4.1878996 -4.2037416 -4.21254 -4.2122211 -4.2061715 -4.2052307 -4.2105184 -4.2073226 -4.2012067 -4.2100987 -4.232214 -4.24423 -4.2451549][-4.18388 -4.203413 -4.2103343 -4.2215204 -4.2289562 -4.2358565 -4.2333174 -4.2213216 -4.2136731 -4.1996341 -4.1727352 -4.1709695 -4.1973906 -4.2196846 -4.2308283][-4.1941514 -4.2079096 -4.2091203 -4.2151375 -4.2206039 -4.2298741 -4.2275434 -4.2164025 -4.2059464 -4.1885977 -4.1553617 -4.1512074 -4.1863594 -4.21827 -4.2322712][-4.1944733 -4.2011919 -4.2043142 -4.212513 -4.2195091 -4.2274103 -4.2257152 -4.220705 -4.216373 -4.2080073 -4.190702 -4.1936417 -4.2271142 -4.2572041 -4.2663331]]...]
INFO - root - 2017-12-06 05:04:26.560127: step 5510, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 19h:57m:05s remains)
INFO - root - 2017-12-06 05:04:28.764372: step 5520, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:39m:41s remains)
INFO - root - 2017-12-06 05:04:30.913164: step 5530, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:29m:17s remains)
INFO - root - 2017-12-06 05:04:33.082188: step 5540, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:20m:43s remains)
INFO - root - 2017-12-06 05:04:35.288159: step 5550, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:17m:19s remains)
INFO - root - 2017-12-06 05:04:37.476875: step 5560, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:34s remains)
INFO - root - 2017-12-06 05:04:39.612738: step 5570, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:41m:20s remains)
INFO - root - 2017-12-06 05:04:41.781389: step 5580, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:07m:22s remains)
INFO - root - 2017-12-06 05:04:43.925413: step 5590, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.219 sec/batch; 19h:51m:12s remains)
INFO - root - 2017-12-06 05:04:46.058820: step 5600, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:31m:58s remains)
2017-12-06 05:04:46.423050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2772522 -4.2669725 -4.2559314 -4.2455878 -4.235805 -4.2327461 -4.231997 -4.2349234 -4.2446423 -4.2592392 -4.2738228 -4.28158 -4.2803731 -4.2789378 -4.2842069][-4.2589593 -4.24359 -4.22937 -4.2157741 -4.2001748 -4.190836 -4.181293 -4.178412 -4.1899567 -4.2147756 -4.2411451 -4.2564354 -4.25794 -4.2563906 -4.26089][-4.2411342 -4.2224541 -4.2085285 -4.1937766 -4.1685629 -4.1420503 -4.1140027 -4.1045165 -4.1206408 -4.1610661 -4.2058678 -4.2327313 -4.2402821 -4.2372551 -4.23727][-4.2273498 -4.2089176 -4.1993575 -4.1855984 -4.1460428 -4.0941339 -4.0405726 -4.02053 -4.0417876 -4.1013055 -4.1676841 -4.2081203 -4.2245789 -4.2232757 -4.2204843][-4.2179437 -4.2033582 -4.2018504 -4.1895404 -4.1373329 -4.0593848 -3.9774742 -3.9393497 -3.9614403 -4.0400577 -4.1295056 -4.1842337 -4.2106738 -4.2138915 -4.2119427][-4.2026725 -4.1930966 -4.2001615 -4.1921473 -4.1334682 -4.0375414 -3.9290979 -3.859921 -3.8675852 -3.9651966 -4.0832472 -4.1560655 -4.1935382 -4.2050781 -4.2086649][-4.17278 -4.1660247 -4.183394 -4.1885686 -4.1391606 -4.0426517 -3.9144719 -3.8014629 -3.7739735 -3.8872557 -4.0337915 -4.1213207 -4.1665111 -4.18494 -4.1962857][-4.1420302 -4.1325493 -4.1562467 -4.1754465 -4.1436081 -4.0595703 -3.9237647 -3.7796521 -3.7274733 -3.8552456 -4.0138693 -4.1008797 -4.1416669 -4.1599741 -4.1754522][-4.1371088 -4.1236467 -4.1511164 -4.1828232 -4.1711907 -4.1125693 -3.9942098 -3.857223 -3.8042653 -3.9149704 -4.0472465 -4.1100826 -4.1349339 -4.1443834 -4.1571836][-4.1622524 -4.1424403 -4.164731 -4.1993213 -4.203743 -4.1708426 -4.0814643 -3.9655974 -3.9107528 -3.9850037 -4.0870423 -4.1308842 -4.1409178 -4.1425762 -4.1532903][-4.217104 -4.1944256 -4.205204 -4.2316251 -4.24287 -4.2317257 -4.1725073 -4.080277 -4.0230255 -4.0614409 -4.137701 -4.173573 -4.1750937 -4.1736217 -4.1827593][-4.2731571 -4.2535629 -4.254602 -4.2690763 -4.2789884 -4.279048 -4.2425108 -4.1729236 -4.1220226 -4.1359715 -4.187264 -4.2163215 -4.2187972 -4.2189689 -4.2267146][-4.3040876 -4.2903934 -4.2901835 -4.2985587 -4.3051438 -4.3079143 -4.2855034 -4.2353473 -4.1961102 -4.1975708 -4.2285161 -4.2507482 -4.258276 -4.2613463 -4.2682576][-4.3156433 -4.3086796 -4.3099566 -4.3153558 -4.31815 -4.3196025 -4.3052282 -4.2738013 -4.2465734 -4.2422843 -4.2587543 -4.2746081 -4.286509 -4.2926097 -4.2994728][-4.3201475 -4.317728 -4.3198061 -4.3219619 -4.3188171 -4.3151317 -4.3067141 -4.2915134 -4.2756109 -4.2697182 -4.2771721 -4.2883086 -4.3020711 -4.312839 -4.3190336]]...]
INFO - root - 2017-12-06 05:04:48.631421: step 5610, loss = 2.08, batch loss = 2.02 (32.9 examples/sec; 0.243 sec/batch; 22h:03m:13s remains)
INFO - root - 2017-12-06 05:04:50.865540: step 5620, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:39m:36s remains)
INFO - root - 2017-12-06 05:04:53.023946: step 5630, loss = 2.10, batch loss = 2.04 (35.7 examples/sec; 0.224 sec/batch; 20h:20m:14s remains)
INFO - root - 2017-12-06 05:04:55.215637: step 5640, loss = 2.07, batch loss = 2.02 (37.0 examples/sec; 0.216 sec/batch; 19h:37m:23s remains)
INFO - root - 2017-12-06 05:04:57.371427: step 5650, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:24m:43s remains)
INFO - root - 2017-12-06 05:04:59.553122: step 5660, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:53m:29s remains)
INFO - root - 2017-12-06 05:05:01.737317: step 5670, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:14m:52s remains)
INFO - root - 2017-12-06 05:05:03.926654: step 5680, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:40m:09s remains)
INFO - root - 2017-12-06 05:05:06.137829: step 5690, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 20h:54m:37s remains)
INFO - root - 2017-12-06 05:05:08.266183: step 5700, loss = 2.05, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 20h:27m:23s remains)
2017-12-06 05:05:11.551159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2376175 -4.2581453 -4.2593784 -4.2345352 -4.2124009 -4.2154784 -4.233768 -4.2455239 -4.2494845 -4.2564564 -4.250073 -4.2328577 -4.19929 -4.1451788 -4.1106443][-4.2345324 -4.2481613 -4.2523661 -4.2288327 -4.19597 -4.1996951 -4.2210255 -4.2267547 -4.2183304 -4.2217555 -4.2191424 -4.2158732 -4.1985331 -4.1527939 -4.1167922][-4.2327943 -4.238852 -4.2445025 -4.2215466 -4.1898632 -4.1957297 -4.2156758 -4.2133555 -4.197403 -4.2031918 -4.2004619 -4.1960516 -4.184392 -4.1528654 -4.1281052][-4.2301888 -4.2332211 -4.2364268 -4.2151308 -4.1818337 -4.1838603 -4.2012973 -4.2021365 -4.1888413 -4.1982837 -4.1943965 -4.1808152 -4.1654887 -4.1427259 -4.1276345][-4.2299213 -4.2350421 -4.2366457 -4.2159352 -4.1729574 -4.1567392 -4.1608639 -4.1640539 -4.1666441 -4.1892066 -4.1891541 -4.172174 -4.1514726 -4.1256866 -4.1155643][-4.2223492 -4.2343397 -4.2388358 -4.2193389 -4.1639981 -4.1135468 -4.0867758 -4.0864987 -4.116951 -4.1578727 -4.1709008 -4.1562219 -4.1316104 -4.1064858 -4.0955262][-4.210248 -4.23175 -4.2419643 -4.2234116 -4.1582508 -4.0768313 -4.01754 -4.0063558 -4.0530529 -4.1141958 -4.1447158 -4.1369839 -4.1117144 -4.0837669 -4.0642414][-4.2019777 -4.2243214 -4.2371049 -4.22289 -4.1658287 -4.0834951 -4.0084186 -3.9802606 -4.0175028 -4.0811372 -4.1262579 -4.1253972 -4.1005712 -4.0736723 -4.0469694][-4.2076573 -4.2231431 -4.235517 -4.2286181 -4.1890388 -4.1301928 -4.0677414 -4.0224166 -4.0317645 -4.0792708 -4.1305833 -4.1369381 -4.1122088 -4.0829906 -4.05083][-4.22602 -4.2371016 -4.2464705 -4.2415309 -4.2103505 -4.1690655 -4.1252327 -4.0781307 -4.0684013 -4.0980225 -4.1455288 -4.1550574 -4.1302023 -4.1026869 -4.0739107][-4.2454047 -4.255712 -4.2581768 -4.2486134 -4.2214823 -4.190187 -4.1654143 -4.1307592 -4.11413 -4.1308289 -4.166357 -4.1756353 -4.1616111 -4.1429462 -4.119926][-4.2542396 -4.2661881 -4.2694707 -4.2594323 -4.23461 -4.21217 -4.1980643 -4.1735053 -4.160717 -4.171452 -4.1962781 -4.2055721 -4.2012529 -4.1923213 -4.1740036][-4.250855 -4.2630997 -4.2728252 -4.2685237 -4.2484722 -4.2331214 -4.2258391 -4.2105746 -4.2030859 -4.2122326 -4.2299595 -4.235158 -4.2310119 -4.2246051 -4.2084894][-4.2420716 -4.253119 -4.2657228 -4.2682009 -4.2547989 -4.2492981 -4.2510543 -4.2408361 -4.2340775 -4.2417483 -4.2526464 -4.2495294 -4.2410083 -4.2328043 -4.2184005][-4.2302079 -4.2419314 -4.2568817 -4.2640357 -4.2559128 -4.2593746 -4.2691097 -4.2601194 -4.2482204 -4.2475762 -4.2457795 -4.2320657 -4.2169504 -4.2090635 -4.2028308]]...]
INFO - root - 2017-12-06 05:05:13.827332: step 5710, loss = 2.05, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 19h:46m:45s remains)
INFO - root - 2017-12-06 05:05:16.019280: step 5720, loss = 2.03, batch loss = 1.98 (35.5 examples/sec; 0.225 sec/batch; 20h:26m:42s remains)
INFO - root - 2017-12-06 05:05:18.255456: step 5730, loss = 2.05, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 20h:09m:55s remains)
INFO - root - 2017-12-06 05:05:20.502178: step 5740, loss = 2.07, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:36m:31s remains)
INFO - root - 2017-12-06 05:05:22.692451: step 5750, loss = 2.07, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:31m:50s remains)
INFO - root - 2017-12-06 05:05:24.868170: step 5760, loss = 2.09, batch loss = 2.04 (37.0 examples/sec; 0.216 sec/batch; 19h:38m:57s remains)
INFO - root - 2017-12-06 05:05:27.097597: step 5770, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 20h:11m:18s remains)
INFO - root - 2017-12-06 05:05:29.288406: step 5780, loss = 2.09, batch loss = 2.03 (36.5 examples/sec; 0.219 sec/batch; 19h:52m:18s remains)
INFO - root - 2017-12-06 05:05:31.499222: step 5790, loss = 2.07, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:48s remains)
INFO - root - 2017-12-06 05:05:33.685464: step 5800, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:55m:18s remains)
2017-12-06 05:05:34.219551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2703094 -4.2677975 -4.26477 -4.261436 -4.2598505 -4.2628541 -4.2677255 -4.267663 -4.26546 -4.2642884 -4.2618756 -4.2534814 -4.24463 -4.2412992 -4.2442479][-4.2511659 -4.248651 -4.2447791 -4.2407413 -4.2383323 -4.2422519 -4.250133 -4.2511172 -4.2481165 -4.2437606 -4.2348051 -4.2203617 -4.2073393 -4.2019043 -4.2052164][-4.2342324 -4.2320533 -4.2263432 -4.2197552 -4.2139063 -4.2162666 -4.2246761 -4.2242041 -4.2180657 -4.2104969 -4.1952024 -4.1731014 -4.1544614 -4.1467533 -4.1512704][-4.2185817 -4.2181525 -4.2118688 -4.2025614 -4.1921883 -4.1905575 -4.1953444 -4.1898255 -4.1781335 -4.1695862 -4.1546245 -4.1285882 -4.1059022 -4.0967736 -4.1011548][-4.2001133 -4.2026296 -4.1975093 -4.186533 -4.1726456 -4.1669459 -4.1655631 -4.1523104 -4.136375 -4.131711 -4.1220117 -4.0964704 -4.0739884 -4.06869 -4.0766253][-4.1831527 -4.1892471 -4.1872058 -4.1751637 -4.1564379 -4.1438489 -4.1331878 -4.1133404 -4.0966043 -4.1003275 -4.1021433 -4.083406 -4.0633984 -4.0637012 -4.0773396][-4.1741066 -4.179698 -4.1783924 -4.1639395 -4.13915 -4.1181569 -4.0990834 -4.0788755 -4.0702891 -4.0866461 -4.1020036 -4.094377 -4.0822506 -4.08928 -4.1092486][-4.1744723 -4.1766515 -4.1732264 -4.1546493 -4.1225071 -4.0907159 -4.0638781 -4.0486875 -4.05527 -4.087245 -4.1166687 -4.1256375 -4.1254959 -4.1371536 -4.1564131][-4.1715903 -4.1708741 -4.1632161 -4.1393762 -4.097918 -4.0534644 -4.0220985 -4.0186677 -4.0454698 -4.0928941 -4.1346741 -4.1603751 -4.1734738 -4.1865821 -4.1987543][-4.1584263 -4.1583357 -4.1504521 -4.1244693 -4.0756526 -4.0222373 -3.9938204 -4.0076966 -4.0534334 -4.106791 -4.1511655 -4.1846685 -4.2035608 -4.2150865 -4.2199197][-4.145463 -4.146894 -4.1432004 -4.1221342 -4.0773659 -4.0270562 -4.0061297 -4.0312195 -4.082582 -4.131566 -4.1683912 -4.1963172 -4.2093191 -4.213892 -4.2097087][-4.1493969 -4.1489434 -4.147325 -4.1343226 -4.1032877 -4.0678153 -4.0569267 -4.0801 -4.1228828 -4.1624594 -4.1905403 -4.2105083 -4.2164927 -4.2095647 -4.1895547][-4.1584673 -4.1548338 -4.1531558 -4.1482906 -4.1335588 -4.1152959 -4.1127977 -4.1309032 -4.164609 -4.1957593 -4.2177324 -4.2335086 -4.2350039 -4.2198405 -4.1837878][-4.1558924 -4.1514487 -4.1526227 -4.1587477 -4.1596179 -4.1539025 -4.1568232 -4.1718817 -4.1993723 -4.2238379 -4.2394123 -4.2511678 -4.251123 -4.233 -4.18933][-4.1482124 -4.1473026 -4.1542077 -4.170341 -4.1814957 -4.1816778 -4.1853256 -4.1953063 -4.2132831 -4.2289214 -4.2387505 -4.2469578 -4.2486849 -4.2328877 -4.1935163]]...]
INFO - root - 2017-12-06 05:05:36.417331: step 5810, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.218 sec/batch; 19h:45m:00s remains)
INFO - root - 2017-12-06 05:05:38.602766: step 5820, loss = 2.10, batch loss = 2.04 (36.5 examples/sec; 0.219 sec/batch; 19h:54m:04s remains)
INFO - root - 2017-12-06 05:05:40.804023: step 5830, loss = 2.06, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:29s remains)
INFO - root - 2017-12-06 05:05:43.051256: step 5840, loss = 2.09, batch loss = 2.03 (37.1 examples/sec; 0.216 sec/batch; 19h:34m:33s remains)
INFO - root - 2017-12-06 05:05:45.263747: step 5850, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:22m:36s remains)
INFO - root - 2017-12-06 05:05:47.462692: step 5860, loss = 2.08, batch loss = 2.02 (35.0 examples/sec; 0.229 sec/batch; 20h:44m:01s remains)
INFO - root - 2017-12-06 05:05:49.675546: step 5870, loss = 2.07, batch loss = 2.01 (37.3 examples/sec; 0.215 sec/batch; 19h:28m:55s remains)
INFO - root - 2017-12-06 05:05:51.862267: step 5880, loss = 2.06, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:25m:19s remains)
INFO - root - 2017-12-06 05:05:54.275951: step 5890, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:30m:24s remains)
INFO - root - 2017-12-06 05:05:57.925683: step 5900, loss = 2.08, batch loss = 2.02 (28.9 examples/sec; 0.277 sec/batch; 25h:08m:06s remains)
2017-12-06 05:05:59.054018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1836767 -4.1830821 -4.1949782 -4.2074881 -4.2076087 -4.2027974 -4.2034516 -4.2100139 -4.2180595 -4.2264371 -4.2354445 -4.244534 -4.2499747 -4.2512622 -4.2528276][-4.1763129 -4.1731272 -4.1831565 -4.1940918 -4.1916814 -4.1828694 -4.1829586 -4.1947784 -4.209125 -4.223732 -4.2372537 -4.24799 -4.253284 -4.2515287 -4.2480035][-4.1661777 -4.1652222 -4.1787343 -4.1904559 -4.1857481 -4.1719131 -4.1693254 -4.1833687 -4.2010884 -4.2204828 -4.2378283 -4.2500362 -4.2556615 -4.2511768 -4.2417526][-4.1524982 -4.1581488 -4.1813316 -4.1983929 -4.1939526 -4.1753621 -4.1671424 -4.1787286 -4.1969481 -4.2195687 -4.239799 -4.253921 -4.2605915 -4.2548437 -4.2404041][-4.1361065 -4.1473188 -4.1804338 -4.2040567 -4.2014132 -4.1806579 -4.1699476 -4.1794672 -4.1978683 -4.2212481 -4.2407041 -4.2556014 -4.2635522 -4.2581105 -4.2402754][-4.1130252 -4.13007 -4.1717057 -4.1996732 -4.1971316 -4.1757483 -4.1663713 -4.1758475 -4.1936183 -4.215632 -4.2319489 -4.2446928 -4.2523761 -4.2467537 -4.2276182][-4.0942116 -4.120862 -4.169126 -4.1979933 -4.1931257 -4.1712193 -4.163774 -4.1723981 -4.188055 -4.2074308 -4.2226968 -4.2363749 -4.2467775 -4.2432241 -4.224472][-4.0834918 -4.1210938 -4.1727967 -4.1967378 -4.1849208 -4.1592174 -4.15188 -4.1600256 -4.1759243 -4.1971025 -4.214787 -4.2317958 -4.2468772 -4.246377 -4.2289524][-4.0872025 -4.1313372 -4.1828413 -4.2026296 -4.1846366 -4.1558871 -4.1498184 -4.1600308 -4.1786313 -4.1996617 -4.2155719 -4.2321868 -4.2479892 -4.24699 -4.2304335][-4.118114 -4.1603389 -4.2057252 -4.2196655 -4.1968412 -4.1678391 -4.1661658 -4.1813192 -4.20233 -4.2193694 -4.2299366 -4.2418642 -4.2550888 -4.2523847 -4.2364888][-4.151082 -4.187469 -4.2244987 -4.2326393 -4.2067866 -4.1792755 -4.1814113 -4.2013183 -4.2237372 -4.2377348 -4.2446189 -4.2526889 -4.2633934 -4.2609277 -4.2464824][-4.180407 -4.2081103 -4.2359581 -4.2404928 -4.2166548 -4.1947212 -4.1998053 -4.2201052 -4.2414203 -4.2524204 -4.2571583 -4.2636323 -4.2735062 -4.27169 -4.259438][-4.2109008 -4.2292538 -4.2498884 -4.2546816 -4.2377949 -4.2237515 -4.2296262 -4.2461329 -4.2621083 -4.2688761 -4.2716713 -4.2764554 -4.2853832 -4.2851753 -4.2771783][-4.2458344 -4.2555079 -4.2702618 -4.2763314 -4.2681708 -4.2610955 -4.2661815 -4.2777214 -4.2874765 -4.2906876 -4.2915444 -4.2940354 -4.2996421 -4.3003 -4.2969694][-4.279161 -4.2827349 -4.2930083 -4.2998042 -4.2981396 -4.2963853 -4.300292 -4.3068795 -4.3120141 -4.3130612 -4.3129034 -4.3133507 -4.3154173 -4.3151665 -4.3139114]]...]
INFO - root - 2017-12-06 05:06:01.404437: step 5910, loss = 2.04, batch loss = 1.98 (34.3 examples/sec; 0.234 sec/batch; 21h:11m:04s remains)
INFO - root - 2017-12-06 05:06:04.869942: step 5920, loss = 2.06, batch loss = 2.00 (21.2 examples/sec; 0.377 sec/batch; 34h:14m:12s remains)
INFO - root - 2017-12-06 05:06:09.131686: step 5930, loss = 2.09, batch loss = 2.03 (17.9 examples/sec; 0.447 sec/batch; 40h:31m:51s remains)
INFO - root - 2017-12-06 05:06:13.426202: step 5940, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 38h:49m:54s remains)
INFO - root - 2017-12-06 05:06:17.551347: step 5950, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 38h:47m:55s remains)
INFO - root - 2017-12-06 05:06:21.806670: step 5960, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:59m:38s remains)
INFO - root - 2017-12-06 05:06:26.101635: step 5970, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 40h:33m:21s remains)
INFO - root - 2017-12-06 05:06:30.275992: step 5980, loss = 2.08, batch loss = 2.02 (21.2 examples/sec; 0.378 sec/batch; 34h:17m:58s remains)
INFO - root - 2017-12-06 05:06:34.667707: step 5990, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.422 sec/batch; 38h:16m:42s remains)
INFO - root - 2017-12-06 05:06:38.973155: step 6000, loss = 2.06, batch loss = 2.01 (17.6 examples/sec; 0.455 sec/batch; 41h:16m:31s remains)
2017-12-06 05:06:39.495274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1618581 -4.169909 -4.1931639 -4.2188134 -4.2320623 -4.2354918 -4.2430563 -4.2572689 -4.2697468 -4.2711468 -4.2623315 -4.2402182 -4.2065187 -4.1619439 -4.1315346][-4.1202755 -4.1290431 -4.1590943 -4.2024622 -4.2299218 -4.2372565 -4.2453122 -4.2565265 -4.2643552 -4.2619967 -4.2516961 -4.2273707 -4.18783 -4.1407976 -4.1171522][-4.1101117 -4.121985 -4.1481423 -4.189734 -4.2161188 -4.2249322 -4.2327795 -4.241457 -4.2466264 -4.2478447 -4.2400155 -4.2150755 -4.170486 -4.121758 -4.1051555][-4.1230702 -4.1364913 -4.1545777 -4.1854281 -4.2034292 -4.2043529 -4.2132449 -4.2288442 -4.2348156 -4.239109 -4.2327633 -4.2075667 -4.1643138 -4.12269 -4.1123309][-4.1512127 -4.1629066 -4.1693478 -4.18702 -4.1941962 -4.183208 -4.190361 -4.2103758 -4.2175865 -4.2243881 -4.223423 -4.2052369 -4.1673722 -4.1307168 -4.1246581][-4.184895 -4.1960692 -4.1959434 -4.2015467 -4.192131 -4.1645837 -4.1622844 -4.1810026 -4.1864462 -4.1927466 -4.2015796 -4.2028542 -4.1827455 -4.1542907 -4.1447263][-4.2113938 -4.2177634 -4.2111478 -4.2103281 -4.1882887 -4.1470194 -4.1352463 -4.1474285 -4.1422267 -4.1346021 -4.1489606 -4.1762981 -4.1869097 -4.1789541 -4.1747589][-4.205112 -4.2113209 -4.2091308 -4.2128825 -4.1953149 -4.1523623 -4.1308117 -4.1227112 -4.0924449 -4.0593376 -4.0689659 -4.1194243 -4.1654525 -4.1867237 -4.201067][-4.1691451 -4.181859 -4.19478 -4.2186389 -4.2183809 -4.1813984 -4.1491184 -4.1137433 -4.0445609 -3.9762397 -3.9791307 -4.0520921 -4.1304913 -4.1813903 -4.215683][-4.1162648 -4.1481676 -4.1860485 -4.2349572 -4.2521396 -4.2228737 -4.1835442 -4.127408 -4.0251107 -3.9295456 -3.9224236 -4.0038943 -4.0978703 -4.1650438 -4.2118053][-4.0660319 -4.1224508 -4.1879659 -4.2597179 -4.2942657 -4.27335 -4.2272158 -4.1554618 -4.040369 -3.9388838 -3.9262288 -4.0022745 -4.09272 -4.1560922 -4.20188][-4.0221519 -4.10531 -4.1862807 -4.2655058 -4.3113642 -4.2980208 -4.2481527 -4.1752381 -4.0741463 -3.9887166 -3.974715 -4.03451 -4.1103878 -4.1643744 -4.2018366][-3.9809978 -4.0775652 -4.1553574 -4.2278433 -4.28387 -4.2832847 -4.2410121 -4.1804595 -4.1061311 -4.0492735 -4.0381708 -4.077373 -4.1295075 -4.1686382 -4.1966057][-3.9607759 -4.0474033 -4.1091232 -4.1758823 -4.2419214 -4.2541652 -4.2262058 -4.1816077 -4.1337142 -4.1032939 -4.0981083 -4.1157441 -4.1384292 -4.1601124 -4.1796989][-3.9800384 -4.0466728 -4.0903625 -4.1490774 -4.2140951 -4.2302179 -4.212841 -4.1833324 -4.1575265 -4.1460843 -4.145927 -4.1476483 -4.1516085 -4.1623235 -4.1773534]]...]
INFO - root - 2017-12-06 05:06:43.766444: step 6010, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 39h:51m:51s remains)
INFO - root - 2017-12-06 05:06:47.930461: step 6020, loss = 2.04, batch loss = 1.98 (23.1 examples/sec; 0.346 sec/batch; 31h:24m:46s remains)
INFO - root - 2017-12-06 05:06:51.955209: step 6030, loss = 2.05, batch loss = 1.99 (18.4 examples/sec; 0.435 sec/batch; 39h:26m:36s remains)
INFO - root - 2017-12-06 05:06:56.290676: step 6040, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.433 sec/batch; 39h:17m:57s remains)
INFO - root - 2017-12-06 05:07:00.645764: step 6050, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.424 sec/batch; 38h:27m:15s remains)
INFO - root - 2017-12-06 05:07:04.906936: step 6060, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:37m:37s remains)
INFO - root - 2017-12-06 05:07:09.187654: step 6070, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 38h:30m:16s remains)
INFO - root - 2017-12-06 05:07:13.598027: step 6080, loss = 2.04, batch loss = 1.98 (18.0 examples/sec; 0.446 sec/batch; 40h:23m:44s remains)
INFO - root - 2017-12-06 05:07:17.834340: step 6090, loss = 2.05, batch loss = 1.99 (19.2 examples/sec; 0.417 sec/batch; 37h:47m:46s remains)
INFO - root - 2017-12-06 05:07:22.171928: step 6100, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 39h:28m:10s remains)
2017-12-06 05:07:22.689443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3244629 -4.3344417 -4.3436022 -4.3426042 -4.330296 -4.3144355 -4.3030887 -4.3015327 -4.30624 -4.313921 -4.3262544 -4.3359838 -4.3404069 -4.3352847 -4.3262482][-4.3146348 -4.321878 -4.3281608 -4.3225703 -4.3057041 -4.2851853 -4.2696247 -4.2677512 -4.2736282 -4.2840381 -4.3045158 -4.3223619 -4.3293924 -4.3221555 -4.3110542][-4.3102455 -4.314229 -4.31751 -4.3076792 -4.2858567 -4.2606025 -4.2409291 -4.23645 -4.2441449 -4.2552414 -4.2809305 -4.307322 -4.3185511 -4.3092852 -4.2965574][-4.3085327 -4.3094044 -4.3072181 -4.2933493 -4.2691383 -4.2408414 -4.2179275 -4.2093568 -4.2166872 -4.2252645 -4.2529178 -4.2859511 -4.3031983 -4.2934623 -4.2799282][-4.3014879 -4.2969222 -4.287498 -4.2715511 -4.2496085 -4.2231627 -4.1983714 -4.1894894 -4.1994324 -4.2065959 -4.2284422 -4.2584314 -4.2734938 -4.2607279 -4.2450857][-4.2952123 -4.2853875 -4.2706623 -4.25367 -4.2357922 -4.2116842 -4.1864305 -4.1797462 -4.1987114 -4.21111 -4.22793 -4.2471189 -4.2511945 -4.2268305 -4.2031159][-4.2911048 -4.2766075 -4.2580404 -4.2380276 -4.2217069 -4.1984382 -4.1732788 -4.1677885 -4.1917853 -4.2100391 -4.2299356 -4.2461667 -4.2405081 -4.2048697 -4.1720128][-4.2885175 -4.2712584 -4.2489996 -4.2263088 -4.207479 -4.1828675 -4.1543174 -4.1446085 -4.1632338 -4.1840897 -4.2138972 -4.2367949 -4.231216 -4.1969872 -4.1634035][-4.2902703 -4.2744451 -4.2529097 -4.2269177 -4.2014904 -4.173738 -4.1409097 -4.1223779 -4.1300974 -4.1562629 -4.198916 -4.2306471 -4.2310691 -4.2084656 -4.1847868][-4.2953796 -4.2836719 -4.2642684 -4.2376232 -4.2061038 -4.173111 -4.1374512 -4.1174021 -4.1205249 -4.1479926 -4.1935287 -4.2283726 -4.2348104 -4.2245183 -4.2099276][-4.2975917 -4.2898083 -4.2722931 -4.24674 -4.2127509 -4.1749249 -4.1357512 -4.1173649 -4.1248274 -4.1526866 -4.19564 -4.2282963 -4.2384186 -4.234271 -4.2238479][-4.2936668 -4.2854457 -4.2676034 -4.2425265 -4.2115574 -4.1779823 -4.1436176 -4.1312456 -4.1451321 -4.1722689 -4.2065992 -4.2318678 -4.2400589 -4.2391529 -4.2336378][-4.2900944 -4.280386 -4.2642145 -4.2419949 -4.2200556 -4.2005563 -4.177721 -4.1707606 -4.1840186 -4.2044315 -4.2247658 -4.2389145 -4.2431226 -4.2428184 -4.2414403][-4.2885766 -4.2798719 -4.2684007 -4.2534103 -4.2436891 -4.2383919 -4.228158 -4.2257333 -4.2347016 -4.2462773 -4.2547507 -4.2587838 -4.2583952 -4.2574978 -4.2581587][-4.2890573 -4.2809253 -4.2732296 -4.2656646 -4.263546 -4.2661395 -4.2654209 -4.267168 -4.272491 -4.2773342 -4.2786336 -4.2774806 -4.275701 -4.275197 -4.2757673]]...]
INFO - root - 2017-12-06 05:07:26.930412: step 6110, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 38h:07m:16s remains)
INFO - root - 2017-12-06 05:07:31.171500: step 6120, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.429 sec/batch; 38h:54m:32s remains)
INFO - root - 2017-12-06 05:07:34.316729: step 6130, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:14s remains)
INFO - root - 2017-12-06 05:07:37.249491: step 6140, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.449 sec/batch; 40h:41m:54s remains)
INFO - root - 2017-12-06 05:07:41.635297: step 6150, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.445 sec/batch; 40h:18m:30s remains)
INFO - root - 2017-12-06 05:07:45.944840: step 6160, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.443 sec/batch; 40h:11m:45s remains)
INFO - root - 2017-12-06 05:07:50.355924: step 6170, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 40h:05m:12s remains)
INFO - root - 2017-12-06 05:07:54.656519: step 6180, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.434 sec/batch; 39h:22m:13s remains)
INFO - root - 2017-12-06 05:07:58.981715: step 6190, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.422 sec/batch; 38h:15m:41s remains)
INFO - root - 2017-12-06 05:08:03.164751: step 6200, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.418 sec/batch; 37h:52m:13s remains)
2017-12-06 05:08:03.653300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2068248 -4.1986818 -4.2172408 -4.2344074 -4.2317815 -4.2085738 -4.1650558 -4.123899 -4.0980821 -4.1039171 -4.1391454 -4.1727967 -4.1968732 -4.22774 -4.2711205][-4.2169862 -4.2178664 -4.239089 -4.2533402 -4.2417374 -4.2085276 -4.1586552 -4.121232 -4.1026173 -4.1090217 -4.1407933 -4.1811457 -4.2174597 -4.2598276 -4.3044868][-4.2365723 -4.2467718 -4.2701073 -4.2797022 -4.2582736 -4.212574 -4.1559749 -4.1136289 -4.0976405 -4.1037316 -4.138741 -4.1882 -4.2351894 -4.2823372 -4.3201094][-4.257257 -4.275001 -4.2979169 -4.2991509 -4.2651682 -4.2091556 -4.1422105 -4.0887618 -4.0726504 -4.0880833 -4.1339393 -4.1972322 -4.2562919 -4.3027787 -4.3276134][-4.2778296 -4.299283 -4.3154597 -4.302103 -4.2564564 -4.1920142 -4.1109505 -4.0442948 -4.0352879 -4.0738616 -4.1399369 -4.2156587 -4.2824922 -4.3209682 -4.3254991][-4.3015633 -4.3197422 -4.3232479 -4.2947407 -4.2345014 -4.153204 -4.0585794 -3.9891357 -4.0024281 -4.0740881 -4.162137 -4.2440662 -4.3046741 -4.3263569 -4.3082609][-4.3060308 -4.3179517 -4.3106928 -4.2683949 -4.1906691 -4.0900874 -3.9933348 -3.9430444 -3.9907725 -4.0849605 -4.1826439 -4.2624092 -4.3102098 -4.3157721 -4.2803659][-4.2891507 -4.2959838 -4.2808604 -4.2275476 -4.1365857 -4.0323205 -3.9582858 -3.949358 -4.0221729 -4.1139412 -4.2057104 -4.2763138 -4.3083453 -4.2981668 -4.2521548][-4.2658424 -4.2725844 -4.2527375 -4.193131 -4.09948 -4.0062547 -3.9681122 -3.9982674 -4.0726871 -4.152267 -4.2318931 -4.2894635 -4.3029556 -4.277113 -4.2252159][-4.2410541 -4.2510386 -4.229773 -4.1670895 -4.0788851 -4.0057073 -4.0015426 -4.0483637 -4.11493 -4.1849928 -4.2561688 -4.2995973 -4.2973213 -4.2609892 -4.2060342][-4.2168431 -4.2262325 -4.199801 -4.137661 -4.063292 -4.018 -4.0418825 -4.09671 -4.1541348 -4.2140565 -4.2725735 -4.3025923 -4.2922058 -4.2505083 -4.1908545][-4.1958094 -4.197825 -4.1684151 -4.1153493 -4.0636415 -4.0421872 -4.0801444 -4.1380424 -4.1881227 -4.236074 -4.28388 -4.30183 -4.2835684 -4.23523 -4.1705551][-4.1858258 -4.1807785 -4.1532135 -4.1148133 -4.084805 -4.0787826 -4.1159315 -4.1690278 -4.2133312 -4.255475 -4.2933927 -4.3010788 -4.2725549 -4.2139916 -4.1463847][-4.1900773 -4.1800566 -4.1547403 -4.1318741 -4.1205344 -4.1216164 -4.1517653 -4.1949344 -4.2334433 -4.2671309 -4.2907014 -4.2885838 -4.2538161 -4.1930342 -4.1254597][-4.2038879 -4.1996894 -4.1798458 -4.1652646 -4.1611552 -4.1632652 -4.1838994 -4.2139244 -4.2421021 -4.2645211 -4.2784004 -4.2703843 -4.2352386 -4.1791377 -4.1177678]]...]
INFO - root - 2017-12-06 05:08:07.975398: step 6210, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.443 sec/batch; 40h:07m:46s remains)
INFO - root - 2017-12-06 05:08:12.336258: step 6220, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:22m:43s remains)
INFO - root - 2017-12-06 05:08:16.580577: step 6230, loss = 2.07, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:31m:35s remains)
INFO - root - 2017-12-06 05:08:20.899587: step 6240, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.440 sec/batch; 39h:52m:19s remains)
INFO - root - 2017-12-06 05:08:24.583844: step 6250, loss = 2.05, batch loss = 1.99 (18.8 examples/sec; 0.425 sec/batch; 38h:31m:19s remains)
INFO - root - 2017-12-06 05:08:28.892118: step 6260, loss = 2.08, batch loss = 2.03 (17.7 examples/sec; 0.451 sec/batch; 40h:51m:39s remains)
INFO - root - 2017-12-06 05:08:33.129789: step 6270, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:56m:45s remains)
INFO - root - 2017-12-06 05:08:37.416229: step 6280, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 39h:00m:12s remains)
INFO - root - 2017-12-06 05:08:41.741360: step 6290, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.429 sec/batch; 38h:53m:09s remains)
INFO - root - 2017-12-06 05:08:45.957278: step 6300, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 38h:22m:53s remains)
2017-12-06 05:08:46.448055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3273921 -4.2973547 -4.25527 -4.1951957 -4.1235752 -4.0813184 -4.0966096 -4.1306605 -4.1446056 -4.1482811 -4.1559272 -4.1668162 -4.1771092 -4.1909595 -4.205627][-4.3299131 -4.2993469 -4.252717 -4.1845069 -4.1014771 -4.0469518 -4.0610981 -4.1104527 -4.1360717 -4.1419444 -4.1471753 -4.1577892 -4.1717796 -4.1923265 -4.2132349][-4.3335576 -4.3040576 -4.25528 -4.1787815 -4.0822163 -4.0078812 -4.014493 -4.0815654 -4.1228766 -4.1349478 -4.1381154 -4.1473269 -4.1646376 -4.192512 -4.2199764][-4.3375273 -4.3119621 -4.2657714 -4.1844444 -4.0773182 -3.9815338 -3.9706388 -4.048667 -4.1067963 -4.1283193 -4.1296058 -4.1349959 -4.1563344 -4.1920238 -4.2251725][-4.339303 -4.3188272 -4.2782044 -4.1981368 -4.0898314 -3.9778428 -3.9414394 -4.0187593 -4.0929928 -4.12535 -4.1261806 -4.1256146 -4.1476197 -4.1856728 -4.2205095][-4.3377604 -4.3219981 -4.2892551 -4.2177386 -4.1165009 -3.9967015 -3.9358525 -3.9990721 -4.082016 -4.1256771 -4.1318493 -4.1277022 -4.1447906 -4.1776032 -4.2058725][-4.3349986 -4.3222418 -4.2986937 -4.2382441 -4.1473312 -4.0265031 -3.9481027 -3.9879906 -4.068749 -4.1234026 -4.1397743 -4.1360373 -4.1448841 -4.1655068 -4.180552][-4.3324804 -4.3222985 -4.3059807 -4.2576537 -4.1802707 -4.0672994 -3.9828548 -3.9965577 -4.0648422 -4.1275368 -4.1554718 -4.1578331 -4.1589084 -4.1619582 -4.1590443][-4.3318028 -4.3239932 -4.3137231 -4.2779889 -4.2173476 -4.1210647 -4.0423756 -4.03392 -4.0811434 -4.1426196 -4.18029 -4.1928811 -4.1919589 -4.1802173 -4.1603613][-4.3330722 -4.3262672 -4.3195367 -4.2937064 -4.24962 -4.1745892 -4.1081719 -4.0889874 -4.1161118 -4.1675539 -4.2093773 -4.2327666 -4.2338443 -4.2135806 -4.183485][-4.3347244 -4.3280792 -4.3231554 -4.3038216 -4.2749166 -4.2229519 -4.1721025 -4.148541 -4.1592655 -4.19579 -4.237555 -4.2721381 -4.28108 -4.2605152 -4.2255149][-4.3369365 -4.3299828 -4.3254766 -4.3103433 -4.2932534 -4.2640719 -4.2288451 -4.2034011 -4.2004313 -4.2214389 -4.2623243 -4.3055549 -4.3226752 -4.3050976 -4.2710652][-4.3400073 -4.3334966 -4.3291144 -4.317358 -4.3075061 -4.2939286 -4.2730823 -4.2488623 -4.2393942 -4.2504215 -4.2864513 -4.3322382 -4.3548307 -4.34283 -4.3140864][-4.3427424 -4.3363452 -4.3306837 -4.3203878 -4.3139257 -4.3089037 -4.2998872 -4.2815819 -4.2709084 -4.2766919 -4.3044 -4.3435211 -4.365994 -4.3593378 -4.3390484][-4.3424911 -4.3350143 -4.3275557 -4.3200908 -4.3177052 -4.3182726 -4.3179445 -4.3055987 -4.2934165 -4.2916927 -4.3078027 -4.3373985 -4.3588767 -4.3591514 -4.3482728]]...]
INFO - root - 2017-12-06 05:08:50.698441: step 6310, loss = 2.05, batch loss = 2.00 (18.5 examples/sec; 0.432 sec/batch; 39h:06m:20s remains)
INFO - root - 2017-12-06 05:08:55.045307: step 6320, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.444 sec/batch; 40h:12m:52s remains)
INFO - root - 2017-12-06 05:08:59.346224: step 6330, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.432 sec/batch; 39h:09m:03s remains)
INFO - root - 2017-12-06 05:09:03.656146: step 6340, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.430 sec/batch; 38h:57m:01s remains)
INFO - root - 2017-12-06 05:09:07.777841: step 6350, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.425 sec/batch; 38h:28m:50s remains)
INFO - root - 2017-12-06 05:09:12.130243: step 6360, loss = 2.07, batch loss = 2.01 (19.2 examples/sec; 0.416 sec/batch; 37h:43m:04s remains)
INFO - root - 2017-12-06 05:09:16.491640: step 6370, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.431 sec/batch; 39h:03m:28s remains)
INFO - root - 2017-12-06 05:09:20.689589: step 6380, loss = 2.08, batch loss = 2.02 (19.2 examples/sec; 0.416 sec/batch; 37h:39m:12s remains)
INFO - root - 2017-12-06 05:09:25.042452: step 6390, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 38h:46m:31s remains)
INFO - root - 2017-12-06 05:09:29.271423: step 6400, loss = 2.04, batch loss = 1.98 (18.7 examples/sec; 0.428 sec/batch; 38h:47m:42s remains)
2017-12-06 05:09:29.779983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1512003 -4.164032 -4.1672983 -4.1818762 -4.1918893 -4.1917329 -4.189353 -4.1851335 -4.1773729 -4.1511936 -4.1236243 -4.1158447 -4.1212831 -4.124599 -4.13224][-4.1575003 -4.1629167 -4.1669426 -4.1874242 -4.202157 -4.20128 -4.1974244 -4.1909409 -4.18233 -4.1619406 -4.1434259 -4.1393394 -4.140348 -4.1415167 -4.1518979][-4.1436477 -4.1424522 -4.1464844 -4.1713181 -4.1912184 -4.1935649 -4.1943917 -4.1899161 -4.1856418 -4.176384 -4.1677151 -4.1605749 -4.1493793 -4.1419024 -4.1477084][-4.1162672 -4.1066918 -4.1069522 -4.1328688 -4.1530619 -4.1554375 -4.1579976 -4.1653967 -4.1727657 -4.1756763 -4.1717682 -4.1569376 -4.1330128 -4.112812 -4.1135306][-4.0744314 -4.0612636 -4.0520387 -4.0706625 -4.0830469 -4.0771275 -4.0845809 -4.1100712 -4.1302347 -4.139009 -4.1403675 -4.1243753 -4.095787 -4.0696898 -4.0689635][-4.0506663 -4.0372281 -4.0199914 -4.022419 -4.0135069 -3.984133 -3.9969828 -4.0389004 -4.0621409 -4.073379 -4.0843453 -4.0802841 -4.0617943 -4.0447235 -4.0515628][-4.0517335 -4.039669 -4.01994 -4.0026803 -3.9591448 -3.8954515 -3.9036784 -3.9573884 -3.9795604 -3.9983315 -4.0275993 -4.042974 -4.0419741 -4.0408254 -4.0603209][-4.0883322 -4.0833859 -4.0665588 -4.0365186 -3.9692311 -3.8838687 -3.879775 -3.9218779 -3.9451356 -3.9810598 -4.0306621 -4.0580482 -4.0646586 -4.0719042 -4.0958867][-4.1397109 -4.1434779 -4.133378 -4.1071477 -4.04492 -3.973536 -3.9637382 -3.9828632 -4.0008125 -4.0415735 -4.0892186 -4.1134195 -4.1191716 -4.12362 -4.1435075][-4.1903582 -4.2010822 -4.1978316 -4.178493 -4.1331081 -4.0866446 -4.0793104 -4.0868864 -4.0972452 -4.1277308 -4.1559758 -4.1684322 -4.1697335 -4.17215 -4.1873422][-4.2321186 -4.2461472 -4.2462287 -4.2319674 -4.2015581 -4.1702662 -4.1646829 -4.171247 -4.1804729 -4.2037334 -4.217967 -4.2186685 -4.2185478 -4.221755 -4.2334652][-4.2621036 -4.2723536 -4.2704911 -4.26083 -4.2433734 -4.2259011 -4.2221751 -4.2277784 -4.2358155 -4.2505369 -4.255877 -4.25263 -4.255332 -4.2622066 -4.2704215][-4.2810831 -4.2854962 -4.2817454 -4.2749472 -4.26515 -4.2556729 -4.2539005 -4.2586784 -4.2658157 -4.2755761 -4.2783017 -4.27557 -4.2784824 -4.2850108 -4.292202][-4.2962179 -4.2962213 -4.2922444 -4.2875452 -4.2824559 -4.2791481 -4.2793303 -4.2834544 -4.2884521 -4.2935834 -4.2949758 -4.2940083 -4.2962284 -4.3011913 -4.3067975][-4.3091817 -4.3076105 -4.3048539 -4.3027925 -4.3014379 -4.3018374 -4.30359 -4.30665 -4.3089557 -4.3104272 -4.3098741 -4.3089542 -4.3103414 -4.3140697 -4.3187542]]...]
INFO - root - 2017-12-06 05:09:34.106290: step 6410, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:20m:55s remains)
INFO - root - 2017-12-06 05:09:38.377229: step 6420, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.418 sec/batch; 37h:51m:07s remains)
INFO - root - 2017-12-06 05:09:42.726635: step 6430, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.426 sec/batch; 38h:32m:54s remains)
INFO - root - 2017-12-06 05:09:46.989621: step 6440, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.429 sec/batch; 38h:50m:41s remains)
INFO - root - 2017-12-06 05:09:51.394623: step 6450, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:14m:39s remains)
INFO - root - 2017-12-06 05:09:55.435681: step 6460, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 39h:50m:04s remains)
INFO - root - 2017-12-06 05:09:59.873144: step 6470, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.453 sec/batch; 41h:02m:03s remains)
INFO - root - 2017-12-06 05:10:04.110132: step 6480, loss = 2.04, batch loss = 1.98 (18.8 examples/sec; 0.425 sec/batch; 38h:31m:55s remains)
INFO - root - 2017-12-06 05:10:08.461465: step 6490, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.431 sec/batch; 39h:04m:00s remains)
INFO - root - 2017-12-06 05:10:12.890933: step 6500, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.427 sec/batch; 38h:41m:42s remains)
2017-12-06 05:10:13.479756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.132854 -4.1233392 -4.1185541 -4.1328259 -4.1669197 -4.1925459 -4.2090034 -4.2185125 -4.2051606 -4.166635 -4.1230516 -4.0859408 -4.073297 -4.0784464 -4.090034][-4.1571369 -4.1476245 -4.1450877 -4.159534 -4.1917448 -4.2185488 -4.2318807 -4.2336822 -4.2140212 -4.1753454 -4.1353397 -4.1071053 -4.0993447 -4.1072216 -4.1280627][-4.1819892 -4.1782169 -4.1801906 -4.1914053 -4.2081776 -4.21842 -4.2183137 -4.2093081 -4.1910129 -4.162509 -4.1350842 -4.1185021 -4.1190386 -4.1305103 -4.156682][-4.2102838 -4.2117143 -4.2129807 -4.2142992 -4.2071872 -4.1943417 -4.1756926 -4.1570444 -4.1439171 -4.1309614 -4.1247473 -4.1273513 -4.1430535 -4.1596541 -4.1817927][-4.2331557 -4.2351594 -4.229434 -4.2160807 -4.1888542 -4.1555824 -4.1167445 -4.0885663 -4.0856242 -4.09708 -4.1158075 -4.1381149 -4.1645093 -4.181077 -4.1957817][-4.235465 -4.2328968 -4.2229233 -4.2018185 -4.1609793 -4.1082416 -4.04822 -4.006639 -4.016222 -4.052803 -4.0946269 -4.1366653 -4.1735034 -4.1895008 -4.1970119][-4.2118363 -4.201405 -4.1885948 -4.1649523 -4.1178041 -4.0461984 -3.9610279 -3.90465 -3.9275296 -3.9961376 -4.0684834 -4.1339903 -4.1824322 -4.1992249 -4.1976218][-4.1843204 -4.1620669 -4.1390944 -4.1105251 -4.0609026 -3.981704 -3.8883011 -3.8278494 -3.8696442 -3.974889 -4.077878 -4.1604624 -4.207972 -4.2174459 -4.2046204][-4.1576114 -4.1212664 -4.0849643 -4.0497942 -4.0065689 -3.9472797 -3.884336 -3.8527346 -3.9082842 -4.0172186 -4.1193285 -4.1934009 -4.2255354 -4.2236943 -4.2019486][-4.1370645 -4.0937467 -4.0509524 -4.0194516 -3.9968286 -3.9782524 -3.9660654 -3.971478 -4.0209713 -4.0996909 -4.172152 -4.2193608 -4.2321615 -4.2213798 -4.1954913][-4.1395769 -4.1046376 -4.069067 -4.05104 -4.0528221 -4.0638089 -4.07949 -4.0998478 -4.1352472 -4.1809349 -4.2209048 -4.2394834 -4.2344646 -4.2148671 -4.1861453][-4.1543517 -4.1374607 -4.1184559 -4.1140475 -4.128943 -4.1499591 -4.1692367 -4.1870351 -4.2075672 -4.230926 -4.248188 -4.2443805 -4.224112 -4.1967559 -4.1666708][-4.167347 -4.1717238 -4.1704946 -4.1716466 -4.18134 -4.1940131 -4.2053132 -4.218298 -4.2301965 -4.2394032 -4.2413116 -4.2222929 -4.1909041 -4.1597815 -4.1325545][-4.1777759 -4.1962042 -4.2028213 -4.1994004 -4.1955409 -4.1935558 -4.1942625 -4.200007 -4.2045565 -4.2067642 -4.202868 -4.1787014 -4.1418786 -4.111053 -4.0910921][-4.19311 -4.2071071 -4.2046084 -4.1892552 -4.1732035 -4.1629677 -4.1606817 -4.1641865 -4.1652203 -4.163476 -4.1556573 -4.12928 -4.0921307 -4.06733 -4.0583234]]...]
INFO - root - 2017-12-06 05:10:17.747505: step 6510, loss = 2.03, batch loss = 1.97 (18.7 examples/sec; 0.428 sec/batch; 38h:45m:24s remains)
INFO - root - 2017-12-06 05:10:22.037687: step 6520, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.421 sec/batch; 38h:06m:13s remains)
INFO - root - 2017-12-06 05:10:26.274517: step 6530, loss = 2.07, batch loss = 2.02 (19.3 examples/sec; 0.415 sec/batch; 37h:35m:46s remains)
INFO - root - 2017-12-06 05:10:30.548630: step 6540, loss = 2.03, batch loss = 1.97 (18.3 examples/sec; 0.438 sec/batch; 39h:41m:01s remains)
INFO - root - 2017-12-06 05:10:34.879589: step 6550, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.429 sec/batch; 38h:50m:10s remains)
INFO - root - 2017-12-06 05:10:38.888218: step 6560, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 38h:10m:29s remains)
INFO - root - 2017-12-06 05:10:43.210555: step 6570, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.448 sec/batch; 40h:35m:13s remains)
INFO - root - 2017-12-06 05:10:47.613689: step 6580, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.421 sec/batch; 38h:05m:30s remains)
INFO - root - 2017-12-06 05:10:51.913616: step 6590, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 38h:21m:53s remains)
INFO - root - 2017-12-06 05:10:56.314290: step 6600, loss = 2.05, batch loss = 2.00 (18.1 examples/sec; 0.443 sec/batch; 40h:06m:29s remains)
2017-12-06 05:10:56.792416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3295135 -4.31233 -4.27846 -4.238883 -4.2058692 -4.1782894 -4.1523666 -4.1193361 -4.0920343 -4.0809608 -4.0897737 -4.1012774 -4.1239424 -4.1507936 -4.1542749][-4.33478 -4.3192739 -4.2858548 -4.2460232 -4.2100091 -4.1810064 -4.1570358 -4.1262908 -4.0973473 -4.0859585 -4.0923467 -4.1096926 -4.1394429 -4.1730523 -4.1728683][-4.3369341 -4.3215017 -4.2890449 -4.2525969 -4.21857 -4.1880608 -4.1632113 -4.1389046 -4.1170573 -4.1064248 -4.1084709 -4.12701 -4.1580958 -4.1859231 -4.184453][-4.3346686 -4.3168883 -4.2829509 -4.2453737 -4.2094436 -4.1817584 -4.1572266 -4.1406364 -4.1319733 -4.1291022 -4.1279564 -4.1373534 -4.15897 -4.1807914 -4.1836796][-4.3304286 -4.3074989 -4.2665792 -4.2254343 -4.1909256 -4.1677537 -4.1380572 -4.1178818 -4.1142378 -4.1227627 -4.1260757 -4.1262627 -4.1390753 -4.1615939 -4.1744885][-4.3215871 -4.2895913 -4.2377067 -4.1904125 -4.1581955 -4.1313195 -4.0874815 -4.0588374 -4.0638571 -4.0850725 -4.1017103 -4.1065378 -4.1206818 -4.1472182 -4.1674471][-4.3095794 -4.2669544 -4.202476 -4.1453104 -4.1048717 -4.0670257 -4.010715 -3.978178 -3.9960356 -4.0349927 -4.0694141 -4.084465 -4.1018267 -4.1322904 -4.1578908][-4.3007908 -4.2520394 -4.181447 -4.1189518 -4.06884 -4.0231428 -3.9595377 -3.9231615 -3.9449658 -3.9919591 -4.0379767 -4.0622969 -4.0773039 -4.1005983 -4.1243596][-4.2973213 -4.2501931 -4.1895919 -4.1378708 -4.0919862 -4.0490389 -3.9839184 -3.9423456 -3.954797 -3.9966216 -4.0414577 -4.0665975 -4.0713763 -4.0741658 -4.0842705][-4.2969961 -4.2559376 -4.2076125 -4.1681042 -4.1339006 -4.1011462 -4.0448537 -4.0050955 -4.0115256 -4.0472178 -4.0836115 -4.0960579 -4.0823116 -4.0646667 -4.0621853][-4.2944913 -4.2566404 -4.2157297 -4.1845126 -4.159874 -4.1402745 -4.1025014 -4.0762348 -4.0791807 -4.1028895 -4.1242137 -4.1237273 -4.0995207 -4.0740042 -4.0707603][-4.2920537 -4.2575841 -4.2218814 -4.1946239 -4.1777277 -4.1699376 -4.1513066 -4.1383152 -4.1376905 -4.1440873 -4.1460462 -4.1333232 -4.102036 -4.0744157 -4.0767756][-4.2996526 -4.2683072 -4.2337546 -4.2058568 -4.1937671 -4.1930747 -4.18333 -4.1752033 -4.1742206 -4.1724758 -4.1648221 -4.1509118 -4.1251426 -4.0989871 -4.1022692][-4.3147516 -4.2895842 -4.2589083 -4.2324657 -4.2247238 -4.2279015 -4.224 -4.2184949 -4.2153382 -4.2107067 -4.2012081 -4.1900368 -4.1734624 -4.1519279 -4.1482034][-4.3303709 -4.3137054 -4.2920966 -4.2759418 -4.2743206 -4.2808561 -4.2796717 -4.2727075 -4.2665443 -4.2587166 -4.247211 -4.2377815 -4.2276797 -4.2121134 -4.207593]]...]
INFO - root - 2017-12-06 05:11:01.046663: step 6610, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.429 sec/batch; 38h:49m:42s remains)
INFO - root - 2017-12-06 05:11:05.369130: step 6620, loss = 2.05, batch loss = 1.99 (19.1 examples/sec; 0.418 sec/batch; 37h:52m:18s remains)
INFO - root - 2017-12-06 05:11:09.624290: step 6630, loss = 2.07, batch loss = 2.01 (19.6 examples/sec; 0.409 sec/batch; 36h:59m:45s remains)
INFO - root - 2017-12-06 05:11:13.915911: step 6640, loss = 2.06, batch loss = 2.00 (19.3 examples/sec; 0.415 sec/batch; 37h:31m:11s remains)
INFO - root - 2017-12-06 05:11:18.125494: step 6650, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:27m:30s remains)
INFO - root - 2017-12-06 05:11:22.177031: step 6660, loss = 2.06, batch loss = 2.00 (19.2 examples/sec; 0.416 sec/batch; 37h:38m:54s remains)
INFO - root - 2017-12-06 05:11:26.387896: step 6670, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 38h:38m:46s remains)
INFO - root - 2017-12-06 05:11:30.646438: step 6680, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:32m:15s remains)
INFO - root - 2017-12-06 05:11:34.890808: step 6690, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.425 sec/batch; 38h:26m:39s remains)
INFO - root - 2017-12-06 05:11:39.230675: step 6700, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.452 sec/batch; 40h:53m:21s remains)
2017-12-06 05:11:40.009435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1434116 -4.1642 -4.1818972 -4.1714072 -4.1321621 -4.0836396 -4.0371838 -4.0155759 -4.0382795 -4.0933261 -4.1515484 -4.1918459 -4.2215948 -4.23701 -4.2286243][-4.140111 -4.1554403 -4.1788669 -4.1818509 -4.1534853 -4.10518 -4.0433984 -4.0032177 -4.01386 -4.0600729 -4.1112795 -4.1537962 -4.1898441 -4.2144933 -4.2181458][-4.150281 -4.1620512 -4.1840582 -4.1949143 -4.1743569 -4.1292419 -4.0638328 -4.0125675 -4.0060635 -4.0312924 -4.0663781 -4.1058941 -4.1401763 -4.1731434 -4.1904793][-4.1460176 -4.1703229 -4.1995487 -4.2144647 -4.1973672 -4.15867 -4.1016321 -4.0506954 -4.0325532 -4.0363231 -4.05052 -4.0751538 -4.1030374 -4.1382456 -4.1649404][-4.115397 -4.1512394 -4.1935711 -4.217762 -4.2023783 -4.1647277 -4.1136208 -4.0695562 -4.0585532 -4.0629773 -4.0707359 -4.0889659 -4.1083751 -4.1336684 -4.1571612][-4.0753837 -4.1100636 -4.1621771 -4.1938591 -4.1777 -4.1349239 -4.0748529 -4.0289774 -4.0314183 -4.0579429 -4.0840344 -4.1134038 -4.1339674 -4.1486716 -4.1643267][-4.0391183 -4.0633292 -4.1198106 -4.1574488 -4.1436195 -4.0952668 -4.0201364 -3.9616563 -3.967 -4.0182166 -4.0783234 -4.133215 -4.1613617 -4.1721859 -4.183888][-4.0181651 -4.035224 -4.0846539 -4.1194797 -4.1078877 -4.0600505 -3.9870763 -3.9306545 -3.9377444 -3.9944487 -4.065824 -4.1309576 -4.1645765 -4.1752453 -4.184896][-4.0060196 -4.0151877 -4.0538435 -4.0819135 -4.0766697 -4.0366707 -3.9814157 -3.9431179 -3.9501014 -3.995775 -4.0560741 -4.1144075 -4.1446857 -4.1502585 -4.1557894][-4.0089149 -4.0196528 -4.05389 -4.074204 -4.07188 -4.0468187 -4.0107212 -3.9868064 -3.9937453 -4.0293193 -4.0773568 -4.1243982 -4.1437049 -4.1374488 -4.1351733][-4.0169725 -4.0379238 -4.0731959 -4.0896935 -4.0840755 -4.0680618 -4.0476832 -4.0351634 -4.0442052 -4.0722628 -4.1152511 -4.1548724 -4.166604 -4.1510177 -4.1390033][-4.03018 -4.0624928 -4.1003885 -4.1161308 -4.1090026 -4.0980763 -4.0850868 -4.0794897 -4.0951076 -4.1217661 -4.161963 -4.1966758 -4.205771 -4.1863418 -4.166573][-4.089313 -4.118926 -4.1492128 -4.1616025 -4.1573296 -4.1505432 -4.141037 -4.1393752 -4.1544042 -4.1775932 -4.2116928 -4.240335 -4.2487803 -4.233469 -4.2106581][-4.1712012 -4.1889648 -4.2071061 -4.2184825 -4.2189436 -4.2154055 -4.2084188 -4.2057576 -4.2152252 -4.2335567 -4.2591867 -4.2807469 -4.2892122 -4.2800341 -4.2598624][-4.2378097 -4.2475152 -4.259954 -4.2698579 -4.2730389 -4.269609 -4.262476 -4.2589231 -4.264133 -4.277513 -4.2964778 -4.3123355 -4.3197565 -4.3148665 -4.3008771]]...]
INFO - root - 2017-12-06 05:11:44.312413: step 6710, loss = 2.05, batch loss = 1.99 (18.6 examples/sec; 0.431 sec/batch; 39h:00m:52s remains)
INFO - root - 2017-12-06 05:11:48.743041: step 6720, loss = 2.05, batch loss = 1.99 (17.6 examples/sec; 0.454 sec/batch; 41h:02m:28s remains)
INFO - root - 2017-12-06 05:11:52.980230: step 6730, loss = 2.06, batch loss = 2.00 (19.2 examples/sec; 0.417 sec/batch; 37h:44m:52s remains)
INFO - root - 2017-12-06 05:11:57.276373: step 6740, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 40h:48m:38s remains)
INFO - root - 2017-12-06 05:12:01.588714: step 6750, loss = 2.08, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 39h:04m:51s remains)
INFO - root - 2017-12-06 05:12:05.909569: step 6760, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.428 sec/batch; 38h:41m:55s remains)
INFO - root - 2017-12-06 05:12:10.094600: step 6770, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 41h:26m:40s remains)
INFO - root - 2017-12-06 05:12:14.328592: step 6780, loss = 2.06, batch loss = 2.01 (18.5 examples/sec; 0.433 sec/batch; 39h:07m:59s remains)
INFO - root - 2017-12-06 05:12:18.610037: step 6790, loss = 2.09, batch loss = 2.03 (18.9 examples/sec; 0.423 sec/batch; 38h:16m:44s remains)
INFO - root - 2017-12-06 05:12:22.970056: step 6800, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 40h:27m:23s remains)
2017-12-06 05:12:23.454384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2682395 -4.2419157 -4.2169914 -4.202724 -4.2136893 -4.2440372 -4.2551961 -4.2517567 -4.2493181 -4.2445188 -4.241117 -4.232666 -4.2244129 -4.2143006 -4.1937447][-4.2640524 -4.2347169 -4.2039394 -4.1787286 -4.1811571 -4.2103558 -4.2267284 -4.2268953 -4.227654 -4.2324481 -4.2349329 -4.2205243 -4.208601 -4.20309 -4.1815643][-4.2623219 -4.2285681 -4.1881142 -4.1532974 -4.14671 -4.1710668 -4.1873727 -4.1874704 -4.1890974 -4.2037091 -4.2132311 -4.1977129 -4.1794214 -4.1765628 -4.1503291][-4.2592773 -4.2253332 -4.1790619 -4.1372447 -4.1206608 -4.1332693 -4.1435876 -4.1363735 -4.132844 -4.1550236 -4.1728344 -4.1604424 -4.1397414 -4.1352806 -4.1038203][-4.2601147 -4.2250581 -4.177793 -4.130322 -4.1006093 -4.101624 -4.1082826 -4.0972176 -4.0910468 -4.1190939 -4.1422544 -4.1316214 -4.1133184 -4.1066089 -4.0752935][-4.2634039 -4.2280908 -4.1824913 -4.1358423 -4.1026425 -4.0978942 -4.1062331 -4.0984364 -4.0944333 -4.1241827 -4.144846 -4.1409922 -4.1322827 -4.1247149 -4.0983372][-4.2592144 -4.2192326 -4.1704316 -4.1264982 -4.1019378 -4.0979156 -4.1041584 -4.1013446 -4.1050382 -4.1353602 -4.1562643 -4.1625624 -4.1646147 -4.1607847 -4.1412444][-4.2452478 -4.196732 -4.1403971 -4.0997033 -4.0846252 -4.0802975 -4.0792475 -4.0774679 -4.0869412 -4.1171184 -4.1429071 -4.1596575 -4.1683 -4.168303 -4.1592355][-4.2238941 -4.1672955 -4.1044741 -4.0638409 -4.0535388 -4.0466433 -4.0315418 -4.0201907 -4.0238066 -4.0543809 -4.0881686 -4.1170335 -4.1367664 -4.1436644 -4.1469526][-4.1990218 -4.1302924 -4.057426 -4.0095215 -3.9986091 -3.991714 -3.9707587 -3.9511056 -3.947912 -3.9790397 -4.0219874 -4.0626612 -4.0950155 -4.1127553 -4.1283436][-4.1851025 -4.1093073 -4.0318637 -3.9812739 -3.9708698 -3.9699516 -3.9573803 -3.9427257 -3.9394267 -3.9673555 -4.0078006 -4.0504837 -4.0889182 -4.1168962 -4.1430225][-4.204505 -4.1377344 -4.071054 -4.02785 -4.0209703 -4.0290227 -4.0275383 -4.0224681 -4.0210161 -4.042346 -4.074738 -4.1116333 -4.147954 -4.1788254 -4.2058935][-4.2503037 -4.2017565 -4.1578231 -4.1336069 -4.1352062 -4.1476779 -4.1518607 -4.1512294 -4.1498222 -4.1612868 -4.1805387 -4.2026529 -4.2261233 -4.24846 -4.26639][-4.2968769 -4.2659965 -4.2397618 -4.2289 -4.2337732 -4.2445536 -4.2486768 -4.2495079 -4.2465744 -4.2501922 -4.2601614 -4.2715487 -4.2849646 -4.2991529 -4.3094716][-4.3275256 -4.3102365 -4.2931805 -4.2868042 -4.2909369 -4.2985039 -4.3023825 -4.3031812 -4.3001189 -4.2997079 -4.3041329 -4.3097816 -4.3167453 -4.3254395 -4.3320031]]...]
INFO - root - 2017-12-06 05:12:27.727044: step 6810, loss = 2.08, batch loss = 2.03 (17.9 examples/sec; 0.447 sec/batch; 40h:26m:51s remains)
INFO - root - 2017-12-06 05:12:32.091639: step 6820, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.426 sec/batch; 38h:31m:07s remains)
INFO - root - 2017-12-06 05:12:36.345103: step 6830, loss = 2.06, batch loss = 2.00 (19.4 examples/sec; 0.411 sec/batch; 37h:12m:55s remains)
INFO - root - 2017-12-06 05:12:40.626118: step 6840, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.442 sec/batch; 39h:56m:41s remains)
INFO - root - 2017-12-06 05:12:44.933290: step 6850, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.415 sec/batch; 37h:32m:02s remains)
INFO - root - 2017-12-06 05:12:49.251478: step 6860, loss = 2.05, batch loss = 2.00 (18.5 examples/sec; 0.433 sec/batch; 39h:11m:25s remains)
INFO - root - 2017-12-06 05:12:52.849140: step 6870, loss = 2.05, batch loss = 1.99 (18.4 examples/sec; 0.436 sec/batch; 39h:23m:37s remains)
INFO - root - 2017-12-06 05:12:57.201680: step 6880, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.434 sec/batch; 39h:13m:44s remains)
INFO - root - 2017-12-06 05:13:01.517440: step 6890, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.429 sec/batch; 38h:46m:33s remains)
INFO - root - 2017-12-06 05:13:05.805958: step 6900, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:53m:53s remains)
2017-12-06 05:13:06.315069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1570444 -4.1614451 -4.1770926 -4.2015414 -4.2278628 -4.2450304 -4.2447228 -4.23906 -4.2280478 -4.2203751 -4.219049 -4.2135692 -4.2034183 -4.194447 -4.183301][-4.1226716 -4.1239886 -4.1423774 -4.1765866 -4.2127519 -4.2372174 -4.2398691 -4.2347431 -4.2208223 -4.2074676 -4.2000332 -4.1896682 -4.1789904 -4.1732159 -4.1657434][-4.0783658 -4.078166 -4.1036077 -4.1511335 -4.1995974 -4.2296643 -4.2329369 -4.2239251 -4.2082887 -4.1947522 -4.1851449 -4.173142 -4.1660919 -4.16615 -4.1657429][-4.0704594 -4.0737262 -4.1056538 -4.1597791 -4.2110057 -4.2375007 -4.2360687 -4.2198124 -4.2030797 -4.1893511 -4.1822963 -4.1757808 -4.1764722 -4.1807857 -4.1854172][-4.0962691 -4.1093111 -4.1426439 -4.1849794 -4.2150545 -4.2202549 -4.2068105 -4.1881347 -4.1818061 -4.1759067 -4.1729188 -4.1755505 -4.187891 -4.1984677 -4.2056732][-4.1089892 -4.1298265 -4.1573415 -4.1749377 -4.1755223 -4.1511731 -4.1183758 -4.1025734 -4.1176996 -4.1311536 -4.1398535 -4.1558709 -4.1790166 -4.1953 -4.206243][-4.0866 -4.1096907 -4.1290259 -4.1262383 -4.0997338 -4.0416837 -3.9786122 -3.9594681 -3.9997344 -4.0464554 -4.0806637 -4.1209741 -4.1604962 -4.1848845 -4.2005563][-4.073319 -4.0926929 -4.1005931 -4.0848289 -4.0399613 -3.9521825 -3.8566511 -3.8220012 -3.8809996 -3.9578767 -4.0229306 -4.0948744 -4.1590924 -4.194489 -4.2107139][-4.1051383 -4.1151533 -4.1073527 -4.0799932 -4.0240822 -3.9244087 -3.8239367 -3.7872813 -3.8426728 -3.9248862 -4.0058422 -4.0939465 -4.1691976 -4.2118239 -4.229197][-4.155148 -4.157701 -4.1419263 -4.1105366 -4.058301 -3.9710519 -3.8953846 -3.879478 -3.9229004 -3.9818149 -4.0450282 -4.1186042 -4.1807404 -4.217536 -4.2332549][-4.2090058 -4.2111173 -4.1956558 -4.1678929 -4.1254754 -4.0617905 -4.0174875 -4.0217419 -4.0553231 -4.0874987 -4.1217113 -4.1627293 -4.1980267 -4.2205372 -4.2322955][-4.2510519 -4.2562675 -4.2475767 -4.2306576 -4.2040148 -4.1659188 -4.1441712 -4.1539583 -4.1731939 -4.1830034 -4.1935468 -4.2073226 -4.2186551 -4.2275863 -4.2351413][-4.2814126 -4.2887788 -4.2875056 -4.2829695 -4.2722883 -4.2541046 -4.2448111 -4.2531209 -4.2595234 -4.2544332 -4.2523637 -4.2564316 -4.2578082 -4.2591424 -4.2628555][-4.3064404 -4.3125067 -4.3140125 -4.3145075 -4.3117871 -4.3037138 -4.3009057 -4.3064628 -4.3064151 -4.2957792 -4.2892714 -4.2935214 -4.2953148 -4.2959723 -4.29711][-4.3205638 -4.3234959 -4.3227358 -4.322691 -4.3220119 -4.31836 -4.3177533 -4.3205562 -4.3187895 -4.308363 -4.3025508 -4.308053 -4.3122764 -4.314476 -4.3145685]]...]
INFO - root - 2017-12-06 05:13:10.739940: step 6910, loss = 2.07, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:24m:23s remains)
INFO - root - 2017-12-06 05:13:14.982569: step 6920, loss = 2.07, batch loss = 2.01 (19.1 examples/sec; 0.419 sec/batch; 37h:55m:09s remains)
INFO - root - 2017-12-06 05:13:19.289377: step 6930, loss = 2.06, batch loss = 2.01 (18.4 examples/sec; 0.436 sec/batch; 39h:23m:48s remains)
INFO - root - 2017-12-06 05:13:23.514940: step 6940, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.422 sec/batch; 38h:09m:15s remains)
INFO - root - 2017-12-06 05:13:27.840168: step 6950, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 39h:11m:16s remains)
INFO - root - 2017-12-06 05:13:32.110585: step 6960, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.440 sec/batch; 39h:48m:23s remains)
INFO - root - 2017-12-06 05:13:36.385069: step 6970, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.423 sec/batch; 38h:15m:33s remains)
INFO - root - 2017-12-06 05:13:40.279253: step 6980, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.444 sec/batch; 40h:10m:42s remains)
INFO - root - 2017-12-06 05:13:44.518137: step 6990, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.429 sec/batch; 38h:46m:00s remains)
INFO - root - 2017-12-06 05:13:48.814633: step 7000, loss = 2.05, batch loss = 2.00 (18.8 examples/sec; 0.425 sec/batch; 38h:28m:05s remains)
2017-12-06 05:13:50.359056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1332026 -4.131237 -4.1116414 -4.09494 -4.081099 -4.0866385 -4.1068296 -4.1230006 -4.1149659 -4.0938034 -4.0807734 -4.0863686 -4.0979118 -4.1019073 -4.1047344][-4.118577 -4.1106687 -4.0943017 -4.0788283 -4.0661678 -4.0726361 -4.0944471 -4.1157746 -4.1135144 -4.099329 -4.0859365 -4.0862856 -4.0953994 -4.0951548 -4.09105][-4.1114016 -4.10288 -4.0941257 -4.0770025 -4.0603886 -4.0617871 -4.0784459 -4.0955753 -4.0981984 -4.0991082 -4.0942817 -4.0855823 -4.0780983 -4.0728841 -4.0710983][-4.1074095 -4.1123772 -4.1145949 -4.0956349 -4.0726376 -4.0615783 -4.0578532 -4.0583448 -4.0677409 -4.0914884 -4.10053 -4.0850673 -4.0674605 -4.0687637 -4.0732012][-4.1013756 -4.1163073 -4.1220655 -4.1045575 -4.0838194 -4.064877 -4.0332303 -4.011848 -4.035078 -4.0851822 -4.1126428 -4.103282 -4.0926805 -4.10269 -4.1095834][-4.0888453 -4.1086226 -4.1157985 -4.1020956 -4.0851169 -4.0566969 -3.9906411 -3.9460077 -3.993948 -4.076746 -4.1219354 -4.1234708 -4.1227331 -4.1347475 -4.1428952][-4.0474825 -4.0730891 -4.0877686 -4.0813246 -4.0616627 -4.0191059 -3.9274354 -3.8773685 -3.9633832 -4.0731683 -4.1259894 -4.1364322 -4.144742 -4.1553712 -4.1607389][-4.0034261 -4.0361285 -4.0600486 -4.0596523 -4.0404444 -3.9957349 -3.9102755 -3.8866315 -3.9862132 -4.0856476 -4.1246362 -4.1345673 -4.1411924 -4.1480155 -4.1548543][-3.9968989 -4.0344319 -4.0609612 -4.0625548 -4.0508461 -4.0218759 -3.9713249 -3.9779973 -4.0493507 -4.1106706 -4.1256828 -4.1281624 -4.1287985 -4.1282525 -4.1343083][-4.0274248 -4.0636477 -4.081985 -4.0804968 -4.0797167 -4.0667586 -4.0453024 -4.0624676 -4.1103325 -4.1396151 -4.1371 -4.1330533 -4.1290765 -4.1207414 -4.1176891][-4.0672207 -4.1056328 -4.1183319 -4.1123548 -4.113234 -4.102263 -4.0881672 -4.10424 -4.1326332 -4.1448655 -4.1391211 -4.139255 -4.1347265 -4.1170568 -4.1038475][-4.1035309 -4.1405287 -4.150383 -4.1400938 -4.1389127 -4.123023 -4.10204 -4.1110668 -4.1316934 -4.1398978 -4.1386704 -4.1437593 -4.1381369 -4.1178174 -4.1028676][-4.1175032 -4.1450205 -4.1443439 -4.1360092 -4.1416678 -4.1304526 -4.1108484 -4.1194353 -4.143384 -4.15179 -4.1501055 -4.1484189 -4.1355319 -4.1146684 -4.1116757][-4.1142559 -4.1290779 -4.1229048 -4.1220016 -4.1355343 -4.1324964 -4.1165409 -4.1228356 -4.1413774 -4.1469383 -4.1434822 -4.1349854 -4.1177387 -4.1021466 -4.109231][-4.10312 -4.1112342 -4.1107907 -4.1195693 -4.1342874 -4.1355977 -4.1166921 -4.1073594 -4.112668 -4.1185551 -4.123065 -4.122036 -4.106689 -4.0877032 -4.0946288]]...]
INFO - root - 2017-12-06 05:13:54.635304: step 7010, loss = 2.06, batch loss = 2.01 (18.5 examples/sec; 0.431 sec/batch; 38h:59m:45s remains)
INFO - root - 2017-12-06 05:13:58.949765: step 7020, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.418 sec/batch; 37h:48m:14s remains)
INFO - root - 2017-12-06 05:14:03.282147: step 7030, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 41h:48m:41s remains)
INFO - root - 2017-12-06 05:14:07.626654: step 7040, loss = 2.07, batch loss = 2.01 (19.4 examples/sec; 0.411 sec/batch; 37h:12m:03s remains)
INFO - root - 2017-12-06 05:14:11.841182: step 7050, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.420 sec/batch; 37h:55m:53s remains)
INFO - root - 2017-12-06 05:14:16.151569: step 7060, loss = 2.06, batch loss = 2.01 (18.4 examples/sec; 0.434 sec/batch; 39h:14m:37s remains)
INFO - root - 2017-12-06 05:14:20.316963: step 7070, loss = 2.06, batch loss = 2.00 (19.3 examples/sec; 0.415 sec/batch; 37h:29m:55s remains)
INFO - root - 2017-12-06 05:14:24.377239: step 7080, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.468 sec/batch; 42h:19m:59s remains)
INFO - root - 2017-12-06 05:14:28.653062: step 7090, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 37h:58m:47s remains)
INFO - root - 2017-12-06 05:14:32.939928: step 7100, loss = 2.11, batch loss = 2.05 (18.1 examples/sec; 0.442 sec/batch; 39h:57m:08s remains)
2017-12-06 05:14:33.472251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2049527 -4.2005725 -4.1766248 -4.1423225 -4.1215572 -4.11505 -4.1236091 -4.1238828 -4.1130242 -4.1253238 -4.1559691 -4.1736608 -4.1664071 -4.14814 -4.1512856][-4.1957736 -4.1956372 -4.1758542 -4.1432796 -4.121418 -4.1206098 -4.1404295 -4.1504216 -4.1426163 -4.1465883 -4.1639614 -4.1708364 -4.1586289 -4.1394191 -4.1444964][-4.183363 -4.19111 -4.17762 -4.1485767 -4.1267896 -4.1312323 -4.1624441 -4.1830645 -4.18464 -4.1880116 -4.193644 -4.1880403 -4.1675453 -4.1483016 -4.152359][-4.1684718 -4.1858859 -4.1805344 -4.1554346 -4.1312571 -4.1282778 -4.154604 -4.1756124 -4.1903486 -4.2064586 -4.214386 -4.2038045 -4.1784515 -4.1600637 -4.1613927][-4.1577077 -4.1832318 -4.1857443 -4.1685271 -4.13917 -4.1169362 -4.1187615 -4.1254716 -4.1478128 -4.1872821 -4.21242 -4.2050862 -4.1788235 -4.1621304 -4.1608033][-4.1651058 -4.1920362 -4.2012377 -4.1876082 -4.1492786 -4.100709 -4.055593 -4.0245094 -4.0505733 -4.1221504 -4.1750426 -4.182477 -4.1635747 -4.1564941 -4.16419][-4.1961241 -4.2147889 -4.2217875 -4.2039785 -4.1542521 -4.0803957 -3.9843867 -3.901396 -3.9197767 -4.0254855 -4.1126513 -4.1409183 -4.1366968 -4.1474271 -4.1728563][-4.2333012 -4.2430315 -4.2444854 -4.2271056 -4.1760406 -4.0938253 -3.9727876 -3.8558428 -3.8535032 -3.9631388 -4.0665145 -4.1078715 -4.1176925 -4.1437254 -4.1845937][-4.2477312 -4.2560797 -4.25506 -4.2441568 -4.2086735 -4.1442785 -4.0480614 -3.9541039 -3.9356823 -3.99501 -4.0628128 -4.0917587 -4.1040077 -4.1352777 -4.1862421][-4.2231436 -4.234601 -4.23889 -4.2405348 -4.227767 -4.192193 -4.136878 -4.0815144 -4.0603461 -4.0733485 -4.0875025 -4.0857797 -4.0853472 -4.1102986 -4.1621742][-4.1776271 -4.1913853 -4.2044935 -4.2150564 -4.21758 -4.2061095 -4.1842289 -4.164187 -4.154038 -4.148272 -4.1324735 -4.107739 -4.0880218 -4.092864 -4.1311679][-4.1319408 -4.1445079 -4.167479 -4.1851144 -4.1907797 -4.1884117 -4.1889815 -4.196836 -4.2082262 -4.20513 -4.184577 -4.1533709 -4.1204066 -4.1066785 -4.126277][-4.1068649 -4.1233315 -4.1556592 -4.1752577 -4.1756272 -4.1735148 -4.1783004 -4.1948361 -4.2195992 -4.2278633 -4.21436 -4.1865864 -4.156918 -4.139678 -4.1484442][-4.1239104 -4.1437955 -4.17938 -4.196744 -4.1920524 -4.1844053 -4.1801138 -4.1852722 -4.2047429 -4.2176375 -4.2142873 -4.1964602 -4.1772165 -4.1648445 -4.1672812][-4.1545987 -4.1685662 -4.1994877 -4.2171893 -4.21664 -4.2080493 -4.1990304 -4.1899285 -4.1908531 -4.1897941 -4.1875315 -4.1817827 -4.1763911 -4.170332 -4.1686454]]...]
INFO - root - 2017-12-06 05:14:37.839233: step 7110, loss = 2.11, batch loss = 2.05 (18.8 examples/sec; 0.425 sec/batch; 38h:23m:06s remains)
INFO - root - 2017-12-06 05:14:42.148167: step 7120, loss = 2.04, batch loss = 1.98 (18.6 examples/sec; 0.430 sec/batch; 38h:54m:08s remains)
INFO - root - 2017-12-06 05:14:46.324071: step 7130, loss = 2.06, batch loss = 2.01 (19.3 examples/sec; 0.415 sec/batch; 37h:32m:39s remains)
INFO - root - 2017-12-06 05:14:50.546294: step 7140, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 41h:23m:55s remains)
INFO - root - 2017-12-06 05:14:54.819925: step 7150, loss = 2.06, batch loss = 2.00 (19.6 examples/sec; 0.407 sec/batch; 36h:48m:48s remains)
INFO - root - 2017-12-06 05:14:59.109019: step 7160, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 38h:36m:55s remains)
INFO - root - 2017-12-06 05:15:03.418457: step 7170, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.419 sec/batch; 37h:52m:34s remains)
INFO - root - 2017-12-06 05:15:07.500529: step 7180, loss = 2.07, batch loss = 2.01 (20.0 examples/sec; 0.399 sec/batch; 36h:06m:00s remains)
INFO - root - 2017-12-06 05:15:11.702597: step 7190, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 39h:20m:10s remains)
INFO - root - 2017-12-06 05:15:16.052334: step 7200, loss = 2.05, batch loss = 1.99 (19.7 examples/sec; 0.405 sec/batch; 36h:37m:29s remains)
2017-12-06 05:15:16.570275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2229986 -4.2123022 -4.1947594 -4.16626 -4.1456571 -4.1308703 -4.1123819 -4.11786 -4.1506929 -4.1661983 -4.16445 -4.1597195 -4.1483865 -4.1424127 -4.1345797][-4.2042379 -4.2054219 -4.2018304 -4.1834035 -4.1646256 -4.1427183 -4.1135154 -4.1103225 -4.1427035 -4.1652865 -4.1630011 -4.1544819 -4.1379023 -4.1162467 -4.0912881][-4.172132 -4.1872177 -4.1941051 -4.1879554 -4.1770053 -4.16079 -4.1265264 -4.1060319 -4.1249657 -4.1509991 -4.151628 -4.1470203 -4.1281276 -4.090415 -4.0501347][-4.1489668 -4.175076 -4.1869278 -4.1864219 -4.1787586 -4.1630173 -4.1225529 -4.0881777 -4.1014385 -4.1395411 -4.1535826 -4.1534815 -4.1303549 -4.0942669 -4.0521374][-4.15807 -4.1832614 -4.1927409 -4.1869645 -4.172256 -4.1467309 -4.0997224 -4.0643387 -4.0867133 -4.1406631 -4.1783838 -4.1887445 -4.1665354 -4.1354847 -4.0916104][-4.1967306 -4.2156382 -4.2156816 -4.1906037 -4.1601472 -4.1131911 -4.0509343 -4.0198636 -4.0660033 -4.1411495 -4.1996794 -4.2270603 -4.2118163 -4.1828856 -4.1349459][-4.2449121 -4.2558355 -4.24737 -4.2048683 -4.1581411 -4.0863504 -3.9957001 -3.9600372 -4.0253868 -4.1237111 -4.2006855 -4.2428446 -4.2359848 -4.2073703 -4.1553187][-4.2854719 -4.2913017 -4.2770777 -4.232563 -4.1806765 -4.0964718 -3.9906228 -3.9480696 -4.0024462 -4.0999861 -4.1850619 -4.2414794 -4.2516608 -4.22948 -4.177227][-4.3094792 -4.3083916 -4.2956176 -4.2624712 -4.2200761 -4.148335 -4.0599322 -4.020885 -4.0507164 -4.123013 -4.1940808 -4.2519851 -4.2777443 -4.2674589 -4.2230711][-4.3071389 -4.3066826 -4.3066206 -4.28989 -4.2607107 -4.2105665 -4.150753 -4.1139688 -4.1188774 -4.1627707 -4.2180586 -4.2771044 -4.30724 -4.3013163 -4.2655597][-4.2958956 -4.299243 -4.3081121 -4.3001165 -4.2785745 -4.2440825 -4.2077947 -4.1772003 -4.1700697 -4.19539 -4.2377024 -4.2900248 -4.3190041 -4.3142648 -4.2847385][-4.2852278 -4.2894125 -4.3018041 -4.3023939 -4.2899427 -4.267375 -4.24711 -4.2296019 -4.2219663 -4.2351756 -4.264164 -4.3009934 -4.321516 -4.3192773 -4.2969146][-4.2798996 -4.2830248 -4.295186 -4.3031244 -4.2983646 -4.2841411 -4.2747474 -4.2719822 -4.2698441 -4.2727647 -4.286427 -4.3063188 -4.319181 -4.3184638 -4.3026485][-4.2798405 -4.2811227 -4.2905545 -4.2979927 -4.2968178 -4.2884107 -4.2858763 -4.2919197 -4.2941456 -4.2934203 -4.2963343 -4.3048477 -4.3127556 -4.3115907 -4.3030334][-4.2823997 -4.2815847 -4.2863765 -4.2909126 -4.2905645 -4.28738 -4.2898722 -4.2985311 -4.3023357 -4.301445 -4.2977486 -4.2987437 -4.3020945 -4.3004432 -4.2958879]]...]
INFO - root - 2017-12-06 05:15:20.837080: step 7210, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:31m:11s remains)
INFO - root - 2017-12-06 05:15:25.151350: step 7220, loss = 2.08, batch loss = 2.03 (17.8 examples/sec; 0.450 sec/batch; 40h:40m:58s remains)
INFO - root - 2017-12-06 05:15:29.353513: step 7230, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 38h:05m:10s remains)
INFO - root - 2017-12-06 05:15:33.578497: step 7240, loss = 2.03, batch loss = 1.97 (18.7 examples/sec; 0.428 sec/batch; 38h:40m:38s remains)
INFO - root - 2017-12-06 05:15:37.880932: step 7250, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.441 sec/batch; 39h:47m:59s remains)
INFO - root - 2017-12-06 05:15:42.104785: step 7260, loss = 2.06, batch loss = 2.01 (18.9 examples/sec; 0.423 sec/batch; 38h:12m:26s remains)
INFO - root - 2017-12-06 05:15:46.428599: step 7270, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:53m:15s remains)
INFO - root - 2017-12-06 05:15:50.669125: step 7280, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.420 sec/batch; 37h:59m:08s remains)
INFO - root - 2017-12-06 05:15:54.624546: step 7290, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.418 sec/batch; 37h:46m:12s remains)
INFO - root - 2017-12-06 05:15:58.934545: step 7300, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.455 sec/batch; 41h:05m:04s remains)
2017-12-06 05:15:59.435931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3339562 -4.3195548 -4.3016796 -4.2851753 -4.2711058 -4.261652 -4.2583823 -4.2518368 -4.2412667 -4.242064 -4.252562 -4.2594543 -4.2669382 -4.276886 -4.2868776][-4.3317461 -4.3114676 -4.2870879 -4.2656822 -4.2497368 -4.2386913 -4.2322564 -4.2153268 -4.1945696 -4.1958413 -4.2131858 -4.2226062 -4.2333341 -4.2453241 -4.2578759][-4.3349085 -4.3141627 -4.286963 -4.2625837 -4.2423286 -4.2262197 -4.212389 -4.1819696 -4.1486073 -4.1479306 -4.1680079 -4.1777754 -4.1925268 -4.2071915 -4.2213349][-4.3446989 -4.3277359 -4.3024311 -4.2766333 -4.2520876 -4.2299843 -4.2096729 -4.1665392 -4.1196523 -4.1153059 -4.1357236 -4.1425929 -4.15633 -4.1726389 -4.1893234][-4.3521042 -4.3380713 -4.3128152 -4.2835298 -4.2527761 -4.2228441 -4.1913581 -4.1311383 -4.0733447 -4.0726924 -4.1013141 -4.11573 -4.1322489 -4.1509833 -4.16943][-4.351717 -4.3334746 -4.3022537 -4.2655525 -4.2269063 -4.1885862 -4.1400194 -4.0595593 -3.999805 -4.0260997 -4.0797396 -4.1070132 -4.1249332 -4.1411867 -4.1580462][-4.3432174 -4.3146796 -4.2742224 -4.2291903 -4.1837206 -4.1334686 -4.0549521 -3.9370666 -3.8786449 -3.9598355 -4.0548158 -4.1010289 -4.122478 -4.136363 -4.1496754][-4.3290324 -4.2876973 -4.2370758 -4.1866026 -4.1372094 -4.0775657 -3.9669085 -3.8072724 -3.7543967 -3.8952646 -4.0332384 -4.1011391 -4.1299777 -4.143364 -4.1536317][-4.3144026 -4.2651324 -4.211822 -4.1654797 -4.1243038 -4.0709233 -3.961266 -3.8140061 -3.78587 -3.9311762 -4.0673337 -4.1354046 -4.1649437 -4.179265 -4.1896224][-4.3049479 -4.2568855 -4.2116876 -4.1790595 -4.15611 -4.11987 -4.0394559 -3.9413188 -3.9370584 -4.0423365 -4.140954 -4.1905622 -4.2152333 -4.2312551 -4.2440767][-4.3028431 -4.2623906 -4.2303896 -4.2130046 -4.2047043 -4.1830559 -4.1303911 -4.0694423 -4.0752263 -4.1462235 -4.2080507 -4.237668 -4.25609 -4.2723608 -4.286685][-4.3101921 -4.2813559 -4.2637925 -4.2575183 -4.2586102 -4.2499561 -4.2203131 -4.1828547 -4.1908436 -4.2352433 -4.2674804 -4.279633 -4.2884388 -4.3012028 -4.3140783][-4.3251915 -4.3081732 -4.2995591 -4.2984591 -4.3033519 -4.3054967 -4.2943869 -4.2751203 -4.2805595 -4.3032045 -4.3125043 -4.3115673 -4.31273 -4.318995 -4.3282657][-4.3366895 -4.3245268 -4.3182845 -4.3172092 -4.321609 -4.3253407 -4.3216786 -4.3146086 -4.31964 -4.328681 -4.3265076 -4.3207908 -4.3179793 -4.3200393 -4.32644][-4.3428011 -4.3323932 -4.325264 -4.3200254 -4.3178024 -4.3162889 -4.3128376 -4.311358 -4.3160367 -4.3205433 -4.3180032 -4.3148909 -4.3141546 -4.3167086 -4.3233066]]...]
INFO - root - 2017-12-06 05:16:03.705055: step 7310, loss = 2.09, batch loss = 2.03 (18.0 examples/sec; 0.445 sec/batch; 40h:10m:11s remains)
INFO - root - 2017-12-06 05:16:07.944188: step 7320, loss = 2.06, batch loss = 2.00 (19.1 examples/sec; 0.418 sec/batch; 37h:45m:57s remains)
INFO - root - 2017-12-06 05:16:12.235066: step 7330, loss = 2.05, batch loss = 1.99 (18.6 examples/sec; 0.430 sec/batch; 38h:50m:43s remains)
INFO - root - 2017-12-06 05:16:16.506754: step 7340, loss = 2.08, batch loss = 2.03 (18.4 examples/sec; 0.436 sec/batch; 39h:22m:04s remains)
INFO - root - 2017-12-06 05:16:20.764799: step 7350, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.424 sec/batch; 38h:20m:19s remains)
INFO - root - 2017-12-06 05:16:25.050186: step 7360, loss = 2.07, batch loss = 2.02 (19.3 examples/sec; 0.415 sec/batch; 37h:26m:44s remains)
INFO - root - 2017-12-06 05:16:29.320309: step 7370, loss = 2.06, batch loss = 2.00 (18.1 examples/sec; 0.443 sec/batch; 39h:59m:40s remains)
INFO - root - 2017-12-06 05:16:33.723668: step 7380, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.442 sec/batch; 39h:54m:01s remains)
INFO - root - 2017-12-06 05:16:37.770932: step 7390, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.443 sec/batch; 40h:02m:13s remains)
INFO - root - 2017-12-06 05:16:42.075505: step 7400, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.431 sec/batch; 38h:55m:02s remains)
2017-12-06 05:16:42.583864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3587666 -4.3608918 -4.3467703 -4.3238449 -4.2936378 -4.2571521 -4.2211742 -4.1934786 -4.193346 -4.2079406 -4.213376 -4.2171154 -4.2214017 -4.2285514 -4.2377148][-4.3580856 -4.3592443 -4.3438091 -4.3197117 -4.288053 -4.2511687 -4.2138581 -4.1877375 -4.189549 -4.2043447 -4.2075806 -4.2116165 -4.2163949 -4.2229085 -4.2262254][-4.3579154 -4.3590236 -4.3424811 -4.3178868 -4.2861905 -4.2527618 -4.2187648 -4.1939778 -4.1939731 -4.2060671 -4.2066588 -4.2032604 -4.2010427 -4.2036819 -4.200243][-4.3566432 -4.3569117 -4.3381066 -4.3083954 -4.2726531 -4.24284 -4.2158179 -4.1924772 -4.1868534 -4.1921215 -4.1952014 -4.1926718 -4.1897817 -4.1898994 -4.1830096][-4.3561716 -4.3546925 -4.3317161 -4.2924128 -4.2467275 -4.2154207 -4.1926832 -4.1686735 -4.1499195 -4.145803 -4.154552 -4.1646481 -4.1718793 -4.1755023 -4.1706696][-4.3556552 -4.3518515 -4.3233733 -4.2724237 -4.2137227 -4.1765585 -4.1533484 -4.118959 -4.0746627 -4.0572958 -4.0741658 -4.1057 -4.1353254 -4.1529722 -4.1575828][-4.3536587 -4.34949 -4.317832 -4.25633 -4.1863141 -4.1386666 -4.1087151 -4.0664353 -4.003653 -3.9730237 -3.9952304 -4.0488234 -4.1064658 -4.1409903 -4.1525459][-4.348629 -4.3445253 -4.3124962 -4.2473221 -4.1725292 -4.1161761 -4.0845695 -4.04764 -3.995141 -3.976233 -4.0040431 -4.0623589 -4.1261687 -4.1592293 -4.1676979][-4.3412318 -4.3382773 -4.3097472 -4.2481651 -4.1774211 -4.1199536 -4.096509 -4.0764847 -4.0518174 -4.0575294 -4.0878863 -4.1322584 -4.1801462 -4.2054505 -4.2076426][-4.3361025 -4.3374453 -4.3173957 -4.2668409 -4.2099776 -4.1620502 -4.1472754 -4.1371164 -4.1307011 -4.1493006 -4.1734533 -4.2014275 -4.231288 -4.2477341 -4.2465577][-4.3350687 -4.34015 -4.328083 -4.2931757 -4.2541618 -4.2195811 -4.2070475 -4.1987543 -4.2009454 -4.2194228 -4.2317953 -4.2471776 -4.2635474 -4.2746749 -4.2724276][-4.3371506 -4.3439522 -4.3376913 -4.3167963 -4.2919154 -4.2693944 -4.2609358 -4.2548275 -4.257946 -4.2690067 -4.2705522 -4.2736769 -4.2783217 -4.2830639 -4.2792563][-4.3381729 -4.3436208 -4.3410525 -4.3291364 -4.3115048 -4.2938137 -4.285634 -4.2799339 -4.2814651 -4.2879009 -4.2872438 -4.2876105 -4.2866139 -4.2852712 -4.2795815][-4.3370447 -4.3404732 -4.339797 -4.3324013 -4.3158355 -4.2957249 -4.2797194 -4.267076 -4.264535 -4.2717032 -4.276382 -4.2809062 -4.2816424 -4.2813454 -4.2768106][-4.3318539 -4.3324494 -4.3299866 -4.32345 -4.3064694 -4.2809377 -4.2542109 -4.2368274 -4.2345953 -4.2466478 -4.2586646 -4.2697883 -4.2740707 -4.2761593 -4.27427]]...]
INFO - root - 2017-12-06 05:16:46.829802: step 7410, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.419 sec/batch; 37h:48m:35s remains)
INFO - root - 2017-12-06 05:16:51.102386: step 7420, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.432 sec/batch; 39h:01m:21s remains)
INFO - root - 2017-12-06 05:16:55.347610: step 7430, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.427 sec/batch; 38h:31m:29s remains)
INFO - root - 2017-12-06 05:16:59.593717: step 7440, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 38h:31m:32s remains)
INFO - root - 2017-12-06 05:17:03.855146: step 7450, loss = 2.08, batch loss = 2.02 (19.6 examples/sec; 0.407 sec/batch; 36h:46m:52s remains)
INFO - root - 2017-12-06 05:17:08.102783: step 7460, loss = 2.07, batch loss = 2.02 (18.8 examples/sec; 0.427 sec/batch; 38h:31m:05s remains)
INFO - root - 2017-12-06 05:17:12.465480: step 7470, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.436 sec/batch; 39h:19m:15s remains)
INFO - root - 2017-12-06 05:17:16.783686: step 7480, loss = 2.07, batch loss = 2.01 (19.0 examples/sec; 0.420 sec/batch; 37h:57m:00s remains)
INFO - root - 2017-12-06 05:17:21.043791: step 7490, loss = 2.04, batch loss = 1.98 (16.7 examples/sec; 0.480 sec/batch; 43h:20m:41s remains)
INFO - root - 2017-12-06 05:17:25.147502: step 7500, loss = 2.04, batch loss = 1.99 (18.0 examples/sec; 0.445 sec/batch; 40h:10m:28s remains)
2017-12-06 05:17:25.626961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.345345 -4.3442793 -4.3457122 -4.3455234 -4.3391266 -4.3194065 -4.2986274 -4.2915449 -4.3010893 -4.3079581 -4.30926 -4.3082228 -4.3029222 -4.2923403 -4.2760077][-4.3500228 -4.3496261 -4.3492494 -4.3425617 -4.3230019 -4.2885132 -4.2531004 -4.243104 -4.2602749 -4.2787213 -4.2888565 -4.2919059 -4.284637 -4.2691579 -4.2419481][-4.3464794 -4.3484821 -4.3497925 -4.3364305 -4.3021336 -4.24743 -4.1920271 -4.1750245 -4.2087517 -4.2482538 -4.2725477 -4.2813187 -4.275774 -4.2498021 -4.20742][-4.3425121 -4.3482394 -4.3513479 -4.3304892 -4.2814136 -4.2045279 -4.1245141 -4.102222 -4.1602988 -4.22575 -4.2633328 -4.2813578 -4.27597 -4.2376776 -4.182754][-4.3359456 -4.3478851 -4.3508587 -4.3218741 -4.258471 -4.1609149 -4.0541496 -4.0173688 -4.0945396 -4.1881948 -4.2424164 -4.2688789 -4.2624469 -4.2159834 -4.1554885][-4.3302011 -4.3422914 -4.3369379 -4.2990351 -4.2247519 -4.1098208 -3.9784641 -3.9191604 -4.0055184 -4.1292596 -4.2094765 -4.2496667 -4.2479916 -4.2009392 -4.1434622][-4.3227782 -4.3241844 -4.3090234 -4.2672586 -4.19289 -4.0742211 -3.9243965 -3.831964 -3.9192116 -4.0745354 -4.1755962 -4.2291145 -4.2314711 -4.1870685 -4.1409483][-4.3085074 -4.2968249 -4.2675462 -4.221137 -4.1593266 -4.0615783 -3.9144676 -3.7951455 -3.86182 -4.0253296 -4.1341963 -4.1877623 -4.1878514 -4.1456656 -4.1192064][-4.2828503 -4.2611647 -4.2271004 -4.1870561 -4.1510358 -4.0962858 -4.00232 -3.9125452 -3.9419689 -4.04662 -4.1166172 -4.1524272 -4.1460128 -4.1048994 -4.0915775][-4.2532597 -4.2309694 -4.2000918 -4.1717529 -4.1614933 -4.1467724 -4.0977736 -4.042634 -4.0564251 -4.1109085 -4.1466484 -4.1654196 -4.1518316 -4.1109037 -4.0980678][-4.2323918 -4.2111998 -4.1879554 -4.1705236 -4.1733537 -4.1801376 -4.1596208 -4.1261697 -4.1310735 -4.1562643 -4.1743751 -4.1847897 -4.1718135 -4.1335239 -4.1196856][-4.2180886 -4.2001286 -4.1848373 -4.17841 -4.1860094 -4.2055521 -4.2081656 -4.1906309 -4.1890092 -4.201869 -4.2112961 -4.2162771 -4.2067966 -4.179502 -4.1686587][-4.2198977 -4.2044072 -4.1973429 -4.1966848 -4.2061386 -4.2301273 -4.2441344 -4.2390127 -4.2372012 -4.2446566 -4.2507558 -4.2540646 -4.2499533 -4.2343149 -4.22737][-4.2436013 -4.2352786 -4.2347469 -4.2371221 -4.2456961 -4.2659855 -4.2802234 -4.2800779 -4.2807193 -4.2864 -4.2905855 -4.2930164 -4.2941656 -4.2897182 -4.2865009][-4.2793722 -4.2768869 -4.2811804 -4.283699 -4.2883182 -4.3037634 -4.3166571 -4.3212738 -4.3226314 -4.323123 -4.3245244 -4.3272085 -4.3296204 -4.3306837 -4.3281088]]...]
INFO - root - 2017-12-06 05:17:29.857143: step 7510, loss = 2.07, batch loss = 2.02 (18.3 examples/sec; 0.438 sec/batch; 39h:34m:15s remains)
INFO - root - 2017-12-06 05:17:34.100296: step 7520, loss = 2.05, batch loss = 1.99 (18.6 examples/sec; 0.429 sec/batch; 38h:44m:04s remains)
INFO - root - 2017-12-06 05:17:38.506103: step 7530, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.421 sec/batch; 37h:58m:43s remains)
INFO - root - 2017-12-06 05:17:42.856462: step 7540, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.429 sec/batch; 38h:42m:03s remains)
INFO - root - 2017-12-06 05:17:47.190571: step 7550, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.429 sec/batch; 38h:45m:59s remains)
INFO - root - 2017-12-06 05:17:51.455267: step 7560, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.423 sec/batch; 38h:09m:33s remains)
INFO - root - 2017-12-06 05:17:55.643704: step 7570, loss = 2.10, batch loss = 2.04 (20.7 examples/sec; 0.387 sec/batch; 34h:56m:22s remains)
INFO - root - 2017-12-06 05:17:59.881758: step 7580, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.424 sec/batch; 38h:13m:29s remains)
INFO - root - 2017-12-06 05:18:04.232456: step 7590, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 41h:20m:31s remains)
INFO - root - 2017-12-06 05:18:08.347547: step 7600, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.441 sec/batch; 39h:47m:09s remains)
2017-12-06 05:18:08.917547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2024841 -4.2326555 -4.2437663 -4.23365 -4.2347183 -4.2494636 -4.2441607 -4.2165933 -4.1985483 -4.2101545 -4.222331 -4.2369633 -4.2565994 -4.2686448 -4.2643824][-4.2097559 -4.2543583 -4.2726116 -4.2650266 -4.2629781 -4.2721462 -4.2613029 -4.2262549 -4.2054672 -4.2155933 -4.2245245 -4.2368045 -4.2562408 -4.2685533 -4.2629986][-4.2282329 -4.2774358 -4.2962236 -4.28222 -4.2667384 -4.2617149 -4.2422557 -4.2033157 -4.1834922 -4.1997013 -4.2139082 -4.2299809 -4.2509971 -4.2650604 -4.2624879][-4.2494273 -4.2981553 -4.3105469 -4.2846804 -4.2572193 -4.2357445 -4.2039452 -4.1615214 -4.1442227 -4.1689959 -4.1920648 -4.21627 -4.2434788 -4.2627869 -4.2662849][-4.2542844 -4.2940283 -4.3000073 -4.2683196 -4.2303863 -4.19017 -4.1361337 -4.08347 -4.0698776 -4.1056185 -4.1448784 -4.1803322 -4.2181678 -4.2495308 -4.2651763][-4.2420988 -4.2665162 -4.2625036 -4.2229018 -4.1711035 -4.1065607 -4.0218019 -3.9573731 -3.9554062 -4.0134015 -4.0792656 -4.1292138 -4.179266 -4.227416 -4.257411][-4.2121458 -4.2206569 -4.2041097 -4.1578188 -4.0942144 -4.0136404 -3.9154816 -3.8526909 -3.8730235 -3.9516118 -4.0355797 -4.0958071 -4.1532507 -4.2111473 -4.249826][-4.1809473 -4.1756883 -4.1501069 -4.1013074 -4.039752 -3.9716742 -3.9004858 -3.8626812 -3.9023252 -3.9840825 -4.0619984 -4.1119766 -4.1601095 -4.21377 -4.2533531][-4.1715322 -4.157311 -4.1288271 -4.0834694 -4.0369878 -4.0020566 -3.973 -3.9582958 -3.9965584 -4.0635362 -4.1219163 -4.1570964 -4.1926708 -4.2339516 -4.2677026][-4.189065 -4.1734324 -4.1450744 -4.1055951 -4.0744557 -4.065258 -4.0596719 -4.0528412 -4.0775456 -4.1225576 -4.166451 -4.1954532 -4.2247858 -4.256887 -4.2832375][-4.2154241 -4.2014623 -4.1781278 -4.15084 -4.1347022 -4.1370435 -4.1388397 -4.1340775 -4.1464195 -4.1745429 -4.2056575 -4.230453 -4.2544165 -4.2787309 -4.2977896][-4.2523384 -4.2398453 -4.2222333 -4.2072725 -4.2019262 -4.20822 -4.210021 -4.2046375 -4.2068024 -4.222981 -4.2445412 -4.2652903 -4.2850118 -4.3016348 -4.3126545][-4.2975774 -4.2887807 -4.276576 -4.2677097 -4.2658 -4.2699828 -4.270822 -4.2662363 -4.2638087 -4.2724805 -4.2865067 -4.3016896 -4.3148975 -4.3232112 -4.3267097][-4.3252411 -4.3204451 -4.314003 -4.3093214 -4.3076806 -4.3091979 -4.310235 -4.307178 -4.3038855 -4.3087606 -4.317205 -4.3257179 -4.3310981 -4.3330684 -4.3331652][-4.3261747 -4.3252578 -4.3240876 -4.3233752 -4.3232803 -4.32386 -4.3245597 -4.3233337 -4.3215733 -4.3240814 -4.3285465 -4.3329144 -4.335144 -4.3350697 -4.3339105]]...]
INFO - root - 2017-12-06 05:18:13.262413: step 7610, loss = 2.05, batch loss = 2.00 (18.4 examples/sec; 0.435 sec/batch; 39h:13m:39s remains)
INFO - root - 2017-12-06 05:18:17.538779: step 7620, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.434 sec/batch; 39h:11m:19s remains)
INFO - root - 2017-12-06 05:18:21.799501: step 7630, loss = 2.06, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 39h:43m:47s remains)
INFO - root - 2017-12-06 05:18:26.171780: step 7640, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.434 sec/batch; 39h:09m:14s remains)
INFO - root - 2017-12-06 05:18:30.488572: step 7650, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:25m:03s remains)
INFO - root - 2017-12-06 05:18:34.807919: step 7660, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.428 sec/batch; 38h:38m:25s remains)
INFO - root - 2017-12-06 05:18:39.046543: step 7670, loss = 2.05, batch loss = 1.99 (17.9 examples/sec; 0.447 sec/batch; 40h:19m:53s remains)
INFO - root - 2017-12-06 05:18:43.430779: step 7680, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 38h:37m:51s remains)
INFO - root - 2017-12-06 05:18:47.702332: step 7690, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.429 sec/batch; 38h:43m:26s remains)
INFO - root - 2017-12-06 05:18:51.715288: step 7700, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 38h:04m:45s remains)
2017-12-06 05:18:52.237713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1672249 -4.1778026 -4.219049 -4.2572312 -4.2623024 -4.2335067 -4.1754236 -4.1025739 -4.069756 -4.1191897 -4.20065 -4.258678 -4.293942 -4.3147206 -4.338284][-4.1861253 -4.1886086 -4.2158346 -4.2381592 -4.2275877 -4.1893144 -4.1349263 -4.0743647 -4.0562224 -4.1178083 -4.2038827 -4.2654929 -4.2991714 -4.316896 -4.3378496][-4.2157564 -4.2121844 -4.2205348 -4.2168956 -4.1872029 -4.141634 -4.0923357 -4.046504 -4.0438843 -4.1141729 -4.2020082 -4.2694 -4.3061895 -4.3214517 -4.3378224][-4.2744241 -4.2644496 -4.2491961 -4.2147813 -4.1597109 -4.0998044 -4.0436826 -4.0067568 -4.0256968 -4.1112213 -4.2009134 -4.2714334 -4.3124919 -4.3266253 -4.33781][-4.3260641 -4.3142056 -4.2810807 -4.2217736 -4.1466241 -4.068922 -3.9979281 -3.9662955 -4.0090804 -4.1094089 -4.1996946 -4.270967 -4.3167782 -4.330338 -4.3369284][-4.3434725 -4.3320837 -4.2896271 -4.2204275 -4.14324 -4.059526 -3.9789233 -3.9450994 -4.0034661 -4.111887 -4.1991229 -4.269104 -4.3191996 -4.3340015 -4.3362684][-4.3398533 -4.3281045 -4.2804461 -4.2092834 -4.1365509 -4.0536165 -3.9691236 -3.9332178 -3.9990757 -4.1123443 -4.2003064 -4.269218 -4.3212981 -4.3367786 -4.3362169][-4.3322344 -4.3215466 -4.2753305 -4.2053862 -4.1337976 -4.0506725 -3.9748223 -3.9466071 -4.0148158 -4.1236324 -4.2060304 -4.2697949 -4.3211355 -4.337245 -4.3362656][-4.3296719 -4.3208261 -4.2798181 -4.2169476 -4.145339 -4.06013 -3.9939475 -3.9784365 -4.0408425 -4.1358247 -4.2096715 -4.2669463 -4.3146963 -4.3321104 -4.3340492][-4.3295341 -4.3256888 -4.2938409 -4.240346 -4.1744518 -4.0907207 -4.0308867 -4.01942 -4.0688486 -4.1511641 -4.2185802 -4.2690344 -4.3104296 -4.3265376 -4.3329377][-4.3277144 -4.3314404 -4.310101 -4.2642965 -4.2014089 -4.1236229 -4.0714207 -4.0571022 -4.0927176 -4.1646538 -4.2296391 -4.2760458 -4.3112354 -4.3259788 -4.3352432][-4.3220363 -4.3326373 -4.3208008 -4.2836652 -4.2249084 -4.153831 -4.1063213 -4.0881147 -4.110816 -4.175271 -4.2390571 -4.281528 -4.3118658 -4.3261247 -4.3373151][-4.3098865 -4.3301568 -4.3301849 -4.3053551 -4.2532916 -4.18618 -4.1373181 -4.1091466 -4.1185455 -4.1770887 -4.2409277 -4.2817073 -4.3097944 -4.3253832 -4.3388476][-4.29271 -4.32309 -4.336556 -4.3258963 -4.2866225 -4.2261539 -4.1728816 -4.1298609 -4.1266427 -4.1795425 -4.240097 -4.2790451 -4.3078275 -4.3258519 -4.3405504][-4.276154 -4.3133464 -4.3382735 -4.338563 -4.3119931 -4.2613606 -4.2108269 -4.1614184 -4.15268 -4.1960173 -4.2475457 -4.2826309 -4.3117166 -4.33084 -4.3435564]]...]
INFO - root - 2017-12-06 05:18:56.649382: step 7710, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 39h:50m:29s remains)
INFO - root - 2017-12-06 05:19:00.911217: step 7720, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.434 sec/batch; 39h:11m:46s remains)
INFO - root - 2017-12-06 05:19:05.240252: step 7730, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 41h:22m:18s remains)
INFO - root - 2017-12-06 05:19:09.557152: step 7740, loss = 2.09, batch loss = 2.03 (19.0 examples/sec; 0.422 sec/batch; 38h:04m:25s remains)
INFO - root - 2017-12-06 05:19:13.800472: step 7750, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.426 sec/batch; 38h:27m:45s remains)
INFO - root - 2017-12-06 05:19:18.084456: step 7760, loss = 2.09, batch loss = 2.04 (18.4 examples/sec; 0.434 sec/batch; 39h:11m:10s remains)
INFO - root - 2017-12-06 05:19:22.372067: step 7770, loss = 2.06, batch loss = 2.00 (19.6 examples/sec; 0.407 sec/batch; 36h:43m:52s remains)
INFO - root - 2017-12-06 05:19:26.681850: step 7780, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.438 sec/batch; 39h:32m:46s remains)
INFO - root - 2017-12-06 05:19:31.083764: step 7790, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 39h:16m:15s remains)
INFO - root - 2017-12-06 05:19:35.286423: step 7800, loss = 2.07, batch loss = 2.01 (20.4 examples/sec; 0.393 sec/batch; 35h:24m:58s remains)
2017-12-06 05:19:35.730233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2399254 -4.2324777 -4.2308712 -4.2227259 -4.2187128 -4.2233543 -4.2275314 -4.225317 -4.22233 -4.2224522 -4.2276611 -4.2440257 -4.2644224 -4.2733779 -4.2734694][-4.2012897 -4.1999879 -4.2117057 -4.2123837 -4.2141447 -4.2215424 -4.2248278 -4.22138 -4.2169847 -4.21713 -4.2212667 -4.2335386 -4.252419 -4.2596602 -4.2597365][-4.1754274 -4.17745 -4.1988177 -4.2049065 -4.2005668 -4.1986628 -4.1901155 -4.1859493 -4.1920676 -4.2020421 -4.2135797 -4.226347 -4.241673 -4.2463 -4.24779][-4.1779523 -4.17895 -4.1963134 -4.1988482 -4.1822882 -4.1624904 -4.1381807 -4.1408567 -4.1673827 -4.1919074 -4.2119613 -4.2256851 -4.2355375 -4.2390556 -4.2436461][-4.182456 -4.1849375 -4.1975484 -4.1946545 -4.1672397 -4.1276684 -4.0865049 -4.0946965 -4.1478062 -4.1863894 -4.2125244 -4.2300167 -4.2372184 -4.2416759 -4.2483244][-4.17778 -4.1763735 -4.1844196 -4.172895 -4.1280136 -4.062623 -3.9871707 -3.9870124 -4.07649 -4.1460557 -4.1879678 -4.2162967 -4.230154 -4.2413073 -4.2538705][-4.1801276 -4.1750278 -4.1680069 -4.1334271 -4.0592728 -3.9563909 -3.8247266 -3.7996011 -3.9369831 -4.0551915 -4.1261568 -4.1682529 -4.189858 -4.2059822 -4.2249751][-4.2009611 -4.1982937 -4.1812844 -4.1350584 -4.0469723 -3.9274416 -3.7742786 -3.7291229 -3.8793545 -4.017324 -4.09467 -4.1406112 -4.1635895 -4.1782541 -4.1910605][-4.2319 -4.2330885 -4.2196541 -4.1815128 -4.1131105 -4.0316234 -3.930306 -3.89522 -3.9926984 -4.0973444 -4.154623 -4.1823683 -4.198 -4.2022123 -4.2030935][-4.260663 -4.2635589 -4.2534208 -4.2248268 -4.1763363 -4.1260481 -4.0676994 -4.0416589 -4.0968156 -4.1722512 -4.2157207 -4.2344379 -4.2468162 -4.2480087 -4.2474766][-4.2757874 -4.2834411 -4.2777686 -4.256865 -4.2249107 -4.1900744 -4.1517606 -4.131165 -4.158915 -4.2107358 -4.2439365 -4.260046 -4.2713251 -4.2757249 -4.2779055][-4.2905016 -4.3056893 -4.3045473 -4.2876544 -4.26094 -4.232995 -4.2060471 -4.1934924 -4.2093229 -4.2410011 -4.262042 -4.2715206 -4.2782793 -4.2805 -4.2791185][-4.2967281 -4.313272 -4.3152347 -4.3027406 -4.2826958 -4.264358 -4.2536721 -4.2528672 -4.2624588 -4.2787023 -4.2885175 -4.2914248 -4.2925458 -4.2902994 -4.2850718][-4.2935729 -4.3063321 -4.3111987 -4.3026304 -4.2888837 -4.2799 -4.2799821 -4.2859807 -4.2923055 -4.2990823 -4.3038526 -4.3065681 -4.3079438 -4.3071604 -4.3041887][-4.2958908 -4.3061419 -4.3115239 -4.306797 -4.2975779 -4.2946839 -4.2991428 -4.3062716 -4.3094292 -4.3106742 -4.312777 -4.3162346 -4.3193049 -4.3207769 -4.3204269]]...]
INFO - root - 2017-12-06 05:19:39.973345: step 7810, loss = 2.05, batch loss = 1.99 (19.1 examples/sec; 0.418 sec/batch; 37h:41m:09s remains)
INFO - root - 2017-12-06 05:19:44.302923: step 7820, loss = 2.04, batch loss = 1.98 (19.2 examples/sec; 0.417 sec/batch; 37h:38m:33s remains)
INFO - root - 2017-12-06 05:19:48.603393: step 7830, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 40h:35m:44s remains)
INFO - root - 2017-12-06 05:19:52.876432: step 7840, loss = 2.03, batch loss = 1.98 (19.2 examples/sec; 0.418 sec/batch; 37h:40m:18s remains)
INFO - root - 2017-12-06 05:19:57.094344: step 7850, loss = 2.06, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 38h:13m:26s remains)
INFO - root - 2017-12-06 05:20:01.493518: step 7860, loss = 2.04, batch loss = 1.98 (18.0 examples/sec; 0.443 sec/batch; 39h:58m:38s remains)
INFO - root - 2017-12-06 05:20:05.680901: step 7870, loss = 2.06, batch loss = 2.00 (20.0 examples/sec; 0.400 sec/batch; 36h:03m:06s remains)
INFO - root - 2017-12-06 05:20:10.067278: step 7880, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.426 sec/batch; 38h:25m:48s remains)
INFO - root - 2017-12-06 05:20:14.348864: step 7890, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.432 sec/batch; 38h:56m:48s remains)
INFO - root - 2017-12-06 05:20:18.666837: step 7900, loss = 2.09, batch loss = 2.03 (18.5 examples/sec; 0.432 sec/batch; 38h:54m:47s remains)
2017-12-06 05:20:19.159625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2346416 -4.2120934 -4.2080011 -4.2149987 -4.2206569 -4.2263632 -4.2332506 -4.2381067 -4.2405329 -4.248487 -4.2587867 -4.266305 -4.2662764 -4.2597651 -4.258564][-4.1929955 -4.1645179 -4.1666226 -4.1800241 -4.1836748 -4.1859941 -4.1922607 -4.2003589 -4.2054515 -4.2173743 -4.2338715 -4.2455659 -4.245389 -4.2355671 -4.230135][-4.1699719 -4.1424251 -4.1534672 -4.1723595 -4.17271 -4.1688366 -4.1704521 -4.1768866 -4.1837325 -4.1982923 -4.2195387 -4.2345543 -4.2346945 -4.2234511 -4.2154622][-4.1709366 -4.1496429 -4.1635742 -4.1805105 -4.1771474 -4.1672931 -4.1626563 -4.16391 -4.1677566 -4.1816764 -4.2056623 -4.2246222 -4.2270265 -4.2164335 -4.2085481][-4.1766949 -4.1633625 -4.1735973 -4.1833186 -4.1745186 -4.1610031 -4.154551 -4.152184 -4.1508169 -4.1605277 -4.1829362 -4.2033105 -4.2083807 -4.2028723 -4.2022805][-4.1863284 -4.18005 -4.1860695 -4.1883802 -4.1739755 -4.159306 -4.1536279 -4.1487312 -4.142591 -4.1470375 -4.1642923 -4.1823387 -4.1896677 -4.192512 -4.2029386][-4.188859 -4.1837487 -4.1858439 -4.1824751 -4.1673269 -4.1550989 -4.1529703 -4.1485467 -4.1392026 -4.1382322 -4.1504736 -4.1667438 -4.1743412 -4.1828985 -4.1999331][-4.1810856 -4.1737905 -4.1750841 -4.17202 -4.16049 -4.1546755 -4.1585217 -4.1579065 -4.1454549 -4.1396785 -4.1488271 -4.1629214 -4.1657743 -4.1722317 -4.1880012][-4.1649022 -4.1547828 -4.1605587 -4.1649594 -4.1615472 -4.1625733 -4.1728425 -4.1725965 -4.154377 -4.1456447 -4.1520586 -4.1618981 -4.1572623 -4.1566057 -4.1664195][-4.15059 -4.1420312 -4.1558638 -4.1708312 -4.1755252 -4.1792955 -4.1902146 -4.186739 -4.1635895 -4.149219 -4.1498213 -4.1535587 -4.138401 -4.1278787 -4.1335411][-4.1460814 -4.141078 -4.1617761 -4.1840734 -4.1917439 -4.1947656 -4.2024918 -4.195137 -4.1714993 -4.1543541 -4.1512904 -4.149611 -4.1281381 -4.1100888 -4.1133041][-4.1503181 -4.1471968 -4.1699133 -4.1938043 -4.2036948 -4.2060809 -4.2082334 -4.197722 -4.1732874 -4.15577 -4.1503649 -4.1479149 -4.1297669 -4.1137133 -4.1169548][-4.1557884 -4.1544003 -4.1761756 -4.1988044 -4.2129731 -4.2187171 -4.2185688 -4.2063546 -4.183332 -4.1678281 -4.162684 -4.162096 -4.1522036 -4.1424613 -4.1464491][-4.1767077 -4.1744428 -4.1886787 -4.2053075 -4.221221 -4.2290826 -4.2279263 -4.2157459 -4.1971784 -4.1862478 -4.1827006 -4.183444 -4.1820765 -4.1794596 -4.1851621][-4.2079177 -4.2054405 -4.2126737 -4.224093 -4.2377911 -4.245832 -4.2454967 -4.236074 -4.2216048 -4.212676 -4.2118144 -4.212079 -4.2112579 -4.2092166 -4.2161222]]...]
INFO - root - 2017-12-06 05:20:23.222555: step 7910, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.441 sec/batch; 39h:44m:56s remains)
INFO - root - 2017-12-06 05:20:27.511329: step 7920, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.443 sec/batch; 39h:54m:23s remains)
INFO - root - 2017-12-06 05:20:31.884188: step 7930, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.445 sec/batch; 40h:07m:26s remains)
INFO - root - 2017-12-06 05:20:36.276320: step 7940, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:45m:44s remains)
INFO - root - 2017-12-06 05:20:40.513859: step 7950, loss = 2.08, batch loss = 2.02 (19.4 examples/sec; 0.413 sec/batch; 37h:15m:31s remains)
INFO - root - 2017-12-06 05:20:44.862086: step 7960, loss = 2.10, batch loss = 2.04 (18.1 examples/sec; 0.442 sec/batch; 39h:53m:13s remains)
INFO - root - 2017-12-06 05:20:49.095922: step 7970, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.434 sec/batch; 39h:07m:34s remains)
INFO - root - 2017-12-06 05:20:53.359571: step 7980, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.429 sec/batch; 38h:42m:47s remains)
INFO - root - 2017-12-06 05:20:57.668537: step 7990, loss = 2.08, batch loss = 2.02 (17.9 examples/sec; 0.447 sec/batch; 40h:18m:08s remains)
INFO - root - 2017-12-06 05:21:01.999875: step 8000, loss = 2.04, batch loss = 1.98 (18.9 examples/sec; 0.424 sec/batch; 38h:10m:38s remains)
2017-12-06 05:21:02.440668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2079363 -4.2573586 -4.2843537 -4.2974977 -4.3073683 -4.3141212 -4.3098917 -4.2913775 -4.2552214 -4.2118073 -4.1606727 -4.105576 -4.053906 -4.0063767 -3.9864962][-4.168561 -4.2366929 -4.2758441 -4.2955856 -4.31076 -4.3199625 -4.3143177 -4.2912736 -4.2525983 -4.2078595 -4.1586113 -4.1077681 -4.0660019 -4.0315652 -4.0209651][-4.1272268 -4.2070026 -4.2558451 -4.2829051 -4.304091 -4.315733 -4.3078604 -4.2797308 -4.2415323 -4.2045121 -4.1674395 -4.1280832 -4.0986295 -4.081429 -4.0809803][-4.092268 -4.17836 -4.2335696 -4.2625661 -4.2822275 -4.2902145 -4.2746768 -4.2388883 -4.2022762 -4.1810069 -4.1677885 -4.1496964 -4.1352868 -4.1311736 -4.1345854][-4.0654292 -4.150754 -4.210794 -4.2411008 -4.2550058 -4.2547688 -4.2293015 -4.1871653 -4.1538548 -4.1524167 -4.1644583 -4.1658726 -4.1624823 -4.158916 -4.1613808][-4.05594 -4.14062 -4.2031736 -4.2368951 -4.2442279 -4.2338181 -4.19848 -4.14882 -4.1168652 -4.1293764 -4.1603112 -4.1787019 -4.1830125 -4.1771741 -4.1748438][-4.0709305 -4.1512094 -4.2107067 -4.2431736 -4.244699 -4.2303114 -4.1948123 -4.1422162 -4.1104269 -4.1316681 -4.1712031 -4.1968513 -4.20204 -4.1895375 -4.1826019][-4.0967155 -4.1680121 -4.222651 -4.2540092 -4.2550178 -4.24785 -4.222609 -4.1775055 -4.1465912 -4.1649685 -4.1986136 -4.2187085 -4.2215352 -4.2070608 -4.1931558][-4.1316128 -4.1883817 -4.23778 -4.26874 -4.2723446 -4.2731562 -4.2615991 -4.2296114 -4.2037725 -4.2148051 -4.2345071 -4.2431183 -4.240766 -4.2226853 -4.2023678][-4.16745 -4.2065988 -4.2510233 -4.2812457 -4.28731 -4.2924623 -4.289917 -4.2714214 -4.2546868 -4.2619743 -4.2735848 -4.2743359 -4.2662921 -4.2424397 -4.2140951][-4.2116833 -4.2316256 -4.2665968 -4.2948451 -4.3060946 -4.31538 -4.3182869 -4.3055673 -4.2924857 -4.2958031 -4.301476 -4.2990603 -4.2882571 -4.2653904 -4.2382565][-4.2536259 -4.262217 -4.2874141 -4.3112454 -4.3250623 -4.3382835 -4.3446865 -4.3355031 -4.3210135 -4.3168869 -4.3174806 -4.31417 -4.3044405 -4.2889075 -4.2692771][-4.289289 -4.2936754 -4.3101296 -4.3267441 -4.3386149 -4.3496547 -4.3577695 -4.352881 -4.34123 -4.334105 -4.3289089 -4.3245564 -4.3183894 -4.3105764 -4.3008823][-4.3034983 -4.30628 -4.3184996 -4.3301835 -4.3379297 -4.3447433 -4.3514423 -4.3481903 -4.3398395 -4.3345561 -4.3289948 -4.3266349 -4.3244534 -4.3218393 -4.319191][-4.3030586 -4.3046608 -4.3123064 -4.3184052 -4.3216476 -4.3256278 -4.3302922 -4.3285551 -4.3237033 -4.3225965 -4.3233471 -4.3278227 -4.3328772 -4.3331013 -4.3311763]]...]
INFO - root - 2017-12-06 05:21:06.213486: step 8010, loss = 2.09, batch loss = 2.03 (37.7 examples/sec; 0.212 sec/batch; 19h:06m:30s remains)
INFO - root - 2017-12-06 05:21:10.460425: step 8020, loss = 2.04, batch loss = 1.98 (19.0 examples/sec; 0.420 sec/batch; 37h:52m:35s remains)
INFO - root - 2017-12-06 05:21:14.744321: step 8030, loss = 2.05, batch loss = 1.99 (19.4 examples/sec; 0.413 sec/batch; 37h:14m:55s remains)
INFO - root - 2017-12-06 05:21:19.029304: step 8040, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.455 sec/batch; 40h:59m:07s remains)
INFO - root - 2017-12-06 05:21:23.506320: step 8050, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.441 sec/batch; 39h:43m:23s remains)
INFO - root - 2017-12-06 05:21:27.733696: step 8060, loss = 2.05, batch loss = 1.99 (18.8 examples/sec; 0.425 sec/batch; 38h:17m:52s remains)
INFO - root - 2017-12-06 05:21:32.174090: step 8070, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.420 sec/batch; 37h:52m:03s remains)
INFO - root - 2017-12-06 05:21:37.889554: step 8080, loss = 2.10, batch loss = 2.04 (5.6 examples/sec; 1.435 sec/batch; 129h:16m:23s remains)
INFO - root - 2017-12-06 05:21:42.940536: step 8090, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:15m:20s remains)
INFO - root - 2017-12-06 05:21:49.137153: step 8100, loss = 2.08, batch loss = 2.02 (13.6 examples/sec; 0.587 sec/batch; 52h:54m:12s remains)
2017-12-06 05:21:49.737833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2088284 -4.1736121 -4.1523547 -4.1495547 -4.1612077 -4.1716166 -4.1688824 -4.1564264 -4.15816 -4.1609178 -4.1641536 -4.1780696 -4.2036853 -4.2334294 -4.2497482][-4.2022529 -4.1663127 -4.1446133 -4.1363111 -4.141078 -4.1438279 -4.1313787 -4.1137714 -4.121388 -4.1346149 -4.1475472 -4.1649423 -4.1876616 -4.217196 -4.236784][-4.1883731 -4.1562896 -4.1360254 -4.1230469 -4.1218276 -4.1199446 -4.1014032 -4.0839915 -4.1009469 -4.1245947 -4.1455646 -4.1663623 -4.1824188 -4.20725 -4.2283659][-4.1722412 -4.1521316 -4.1376195 -4.1217985 -4.1108656 -4.1015573 -4.0789871 -4.0649009 -4.0927424 -4.1240811 -4.151938 -4.1752124 -4.184402 -4.2024117 -4.2240562][-4.1555367 -4.1502337 -4.1442719 -4.128336 -4.1058273 -4.0815878 -4.0506387 -4.0401897 -4.0802488 -4.120285 -4.1542616 -4.17914 -4.1841555 -4.1978788 -4.2205448][-4.1322656 -4.1374183 -4.1388884 -4.1242013 -4.0909052 -4.0472779 -4.0023332 -3.9945688 -4.0520325 -4.1080108 -4.1500854 -4.1781178 -4.1843567 -4.1982584 -4.2220564][-4.1160855 -4.1232605 -4.1256542 -4.11064 -4.0690255 -4.0060563 -3.9409993 -3.9317212 -4.0084224 -4.0866256 -4.1401367 -4.1734319 -4.1866775 -4.2059345 -4.2316127][-4.129303 -4.1288996 -4.1222057 -4.100389 -4.0531588 -3.9815261 -3.9006853 -3.886045 -3.9751837 -4.0693359 -4.1294088 -4.16586 -4.1878238 -4.2150626 -4.2441297][-4.1660295 -4.1529331 -4.1307635 -4.1013212 -4.0596719 -4.0033455 -3.9347048 -3.9191818 -3.996876 -4.0828185 -4.1365633 -4.1695743 -4.1965551 -4.2289157 -4.2595716][-4.195827 -4.1748047 -4.1432524 -4.1113992 -4.0839119 -4.0579758 -4.023356 -4.0172052 -4.07052 -4.1314969 -4.1711769 -4.1971016 -4.2231116 -4.2538109 -4.2800136][-4.2123427 -4.1884785 -4.1558123 -4.1266832 -4.1135387 -4.1157403 -4.1123667 -4.1162224 -4.1480837 -4.1824307 -4.2053061 -4.2234621 -4.2466922 -4.2720242 -4.2917027][-4.2196465 -4.19617 -4.166378 -4.1400576 -4.1363153 -4.1567192 -4.1723337 -4.1815367 -4.1978087 -4.2131953 -4.2246075 -4.23638 -4.2547722 -4.2755022 -4.2895269][-4.219986 -4.1994863 -4.1772189 -4.1573634 -4.156745 -4.1831236 -4.2060204 -4.2147021 -4.2202911 -4.2253318 -4.2320194 -4.2417455 -4.2572231 -4.2735872 -4.2820468][-4.2265315 -4.210794 -4.1976061 -4.1864543 -4.1854544 -4.2065668 -4.2278376 -4.2339387 -4.2322645 -4.2306061 -4.2343483 -4.2437673 -4.2581224 -4.2700243 -4.2719131][-4.2407389 -4.2330866 -4.2302427 -4.2245932 -4.2195783 -4.2317243 -4.247366 -4.2506084 -4.2439795 -4.2370696 -4.2373047 -4.2470493 -4.25742 -4.2622433 -4.2579136]]...]
INFO - root - 2017-12-06 05:21:55.946862: step 8110, loss = 2.03, batch loss = 1.97 (13.0 examples/sec; 0.616 sec/batch; 55h:30m:46s remains)
INFO - root - 2017-12-06 05:22:00.943422: step 8120, loss = 2.05, batch loss = 2.00 (18.7 examples/sec; 0.427 sec/batch; 38h:28m:49s remains)
INFO - root - 2017-12-06 05:22:06.822655: step 8130, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 57h:56m:34s remains)
INFO - root - 2017-12-06 05:22:13.205848: step 8140, loss = 2.02, batch loss = 1.97 (12.6 examples/sec; 0.633 sec/batch; 57h:01m:48s remains)
INFO - root - 2017-12-06 05:22:19.466902: step 8150, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 56h:57m:47s remains)
INFO - root - 2017-12-06 05:22:25.823453: step 8160, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:50m:42s remains)
INFO - root - 2017-12-06 05:22:32.200847: step 8170, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 60h:36m:58s remains)
INFO - root - 2017-12-06 05:22:38.719891: step 8180, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.614 sec/batch; 55h:18m:04s remains)
INFO - root - 2017-12-06 05:22:45.047519: step 8190, loss = 2.05, batch loss = 1.99 (13.3 examples/sec; 0.601 sec/batch; 54h:08m:39s remains)
INFO - root - 2017-12-06 05:22:51.364511: step 8200, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:39m:35s remains)
2017-12-06 05:22:52.004273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981319 -4.2399874 -4.1601171 -4.0899134 -4.0605788 -4.0808296 -4.1259689 -4.1591525 -4.1729779 -4.176672 -4.1873913 -4.2120914 -4.250988 -4.288703 -4.3158159][-4.2725115 -4.2182174 -4.1509085 -4.0939879 -4.0694265 -4.0860271 -4.1242361 -4.1519504 -4.1594558 -4.157311 -4.1640768 -4.1861072 -4.225101 -4.2682362 -4.3047791][-4.2403831 -4.20094 -4.1595545 -4.1213207 -4.1013937 -4.1089969 -4.1274452 -4.1399884 -4.1394954 -4.1369066 -4.1422782 -4.161376 -4.2011943 -4.2470126 -4.2897358][-4.2106538 -4.1894736 -4.175467 -4.1539297 -4.1347079 -4.1238942 -4.1226077 -4.1157589 -4.1011686 -4.1025991 -4.1120415 -4.1333404 -4.1792521 -4.228755 -4.2751522][-4.1865549 -4.1767225 -4.1799459 -4.1750164 -4.155962 -4.1281061 -4.1033263 -4.0741735 -4.0490355 -4.0517154 -4.0659661 -4.0961723 -4.1545358 -4.2174492 -4.2704062][-4.1461425 -4.1337366 -4.1465397 -4.1595259 -4.1573219 -4.1324759 -4.0955777 -4.0535645 -4.0165119 -4.0100636 -4.0207891 -4.0578504 -4.1318245 -4.2095308 -4.2698622][-4.0952182 -4.0750985 -4.0854869 -4.1128249 -4.1370835 -4.1406331 -4.1139331 -4.0677533 -4.0164146 -3.9905083 -3.9895072 -4.0226703 -4.103137 -4.1889262 -4.2570133][-4.0693803 -4.0445328 -4.0455208 -4.0746555 -4.1190848 -4.14591 -4.1306829 -4.0796719 -4.0125422 -3.9674842 -3.9622622 -3.9922216 -4.0634332 -4.1510344 -4.22875][-4.0757127 -4.0464253 -4.036067 -4.0596967 -4.11201 -4.1502914 -4.1387024 -4.0800052 -3.9958458 -3.936516 -3.9363081 -3.9780509 -4.0429497 -4.12801 -4.2074227][-4.1085534 -4.0779309 -4.0511775 -4.0608659 -4.1121712 -4.1571903 -4.1464396 -4.0837755 -3.9968207 -3.9403918 -3.9449298 -3.9947891 -4.0550227 -4.1306834 -4.2018247][-4.1636262 -4.1433654 -4.1121855 -4.1024365 -4.135942 -4.1741786 -4.1722126 -4.12275 -4.0539079 -4.0103426 -4.0092497 -4.0499678 -4.093401 -4.1510296 -4.2074809][-4.2268362 -4.2189927 -4.1981287 -4.1808848 -4.1886678 -4.2092457 -4.2113461 -4.1887908 -4.1502113 -4.1145816 -4.098711 -4.1195164 -4.1448922 -4.1836748 -4.2253022][-4.275476 -4.27738 -4.2686524 -4.2563014 -4.2525005 -4.2581377 -4.2574348 -4.2521267 -4.2336664 -4.2032027 -4.1787381 -4.1831331 -4.1917839 -4.2179914 -4.2507262][-4.2994027 -4.3062062 -4.3024125 -4.2942252 -4.2896037 -4.2906451 -4.2881837 -4.2896671 -4.2787628 -4.2523818 -4.225142 -4.21993 -4.223865 -4.2445512 -4.2728038][-4.3041306 -4.3105874 -4.3082719 -4.3002911 -4.2981458 -4.3008695 -4.3000183 -4.3022933 -4.294857 -4.2724147 -4.2479672 -4.2423143 -4.249754 -4.2675953 -4.2905207]]...]
INFO - root - 2017-12-06 05:22:58.281396: step 8210, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 56h:02m:52s remains)
INFO - root - 2017-12-06 05:23:04.613467: step 8220, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 56h:33m:09s remains)
INFO - root - 2017-12-06 05:23:10.306230: step 8230, loss = 2.07, batch loss = 2.02 (18.6 examples/sec; 0.431 sec/batch; 38h:47m:55s remains)
INFO - root - 2017-12-06 05:23:15.460943: step 8240, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 57h:59m:03s remains)
INFO - root - 2017-12-06 05:23:21.802809: step 8250, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 57h:42m:28s remains)
INFO - root - 2017-12-06 05:23:28.161337: step 8260, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 57h:00m:29s remains)
INFO - root - 2017-12-06 05:23:34.673121: step 8270, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 62h:01m:34s remains)
INFO - root - 2017-12-06 05:23:41.011174: step 8280, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 56h:52m:21s remains)
INFO - root - 2017-12-06 05:23:47.446179: step 8290, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:37m:02s remains)
INFO - root - 2017-12-06 05:23:52.939780: step 8300, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 56h:42m:12s remains)
2017-12-06 05:23:53.648220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3639851 -4.3615808 -4.358211 -4.3555012 -4.3539324 -4.3530803 -4.3517361 -4.3488445 -4.3471437 -4.3482089 -4.3507333 -4.3531375 -4.3555822 -4.3593183 -4.3644624][-4.3587742 -4.3557158 -4.35194 -4.3480983 -4.345325 -4.3436227 -4.341609 -4.3373785 -4.3353176 -4.3375564 -4.341208 -4.3428879 -4.3433022 -4.34538 -4.3511214][-4.3502703 -4.3450236 -4.3395391 -4.333765 -4.3292565 -4.3264775 -4.320509 -4.3122954 -4.309288 -4.3130183 -4.3174024 -4.3183827 -4.317585 -4.3184886 -4.3253288][-4.3366365 -4.3261046 -4.3159661 -4.3059344 -4.2969856 -4.2882385 -4.2720356 -4.2557855 -4.2503467 -4.2567835 -4.2635326 -4.2653055 -4.2664318 -4.2695618 -4.2818422][-4.310276 -4.2902303 -4.2724309 -4.2558279 -4.2397323 -4.2214746 -4.1920919 -4.1613569 -4.1523647 -4.1680856 -4.1826043 -4.1872797 -4.1937761 -4.203958 -4.2264991][-4.2731905 -4.2410541 -4.2137618 -4.1909561 -4.1665182 -4.1362386 -4.0902061 -4.0382524 -4.0271649 -4.0640416 -4.097043 -4.1087646 -4.1222286 -4.1410165 -4.1742558][-4.234458 -4.1908073 -4.1561794 -4.1305189 -4.0999856 -4.0542803 -3.9860897 -3.9040134 -3.8936574 -3.9675288 -4.0344195 -4.0596418 -4.0766058 -4.1010971 -4.1425934][-4.2104158 -4.1629038 -4.1267838 -4.0974116 -4.0550094 -3.9901652 -3.9026246 -3.8005722 -3.801887 -3.9142828 -4.0113049 -4.0454865 -4.0605464 -4.0848975 -4.1316009][-4.2185574 -4.1788979 -4.1463151 -4.1126122 -4.0621986 -3.9917121 -3.9101727 -3.8238826 -3.8371868 -3.9496279 -4.0469341 -4.07852 -4.0886369 -4.105619 -4.1450725][-4.2438679 -4.2132406 -4.1818037 -4.14395 -4.0977678 -4.0432568 -3.9873931 -3.9317672 -3.9473298 -4.0306811 -4.106853 -4.1330462 -4.1439252 -4.1531463 -4.1798754][-4.2588725 -4.2339921 -4.2072606 -4.1750636 -4.1424117 -4.1075716 -4.0761538 -4.0426989 -4.0503244 -4.0977163 -4.1519518 -4.1781855 -4.1946664 -4.2033157 -4.2196612][-4.2797823 -4.2625422 -4.2440429 -4.2223048 -4.2016754 -4.182332 -4.1667728 -4.1473484 -4.1452913 -4.1651926 -4.1995635 -4.224226 -4.2440357 -4.2524776 -4.2617073][-4.3047452 -4.295188 -4.2853451 -4.2747054 -4.2653561 -4.255322 -4.2492552 -4.2376595 -4.2308927 -4.2336726 -4.2496672 -4.2663412 -4.2842937 -4.2949605 -4.3014894][-4.32795 -4.324645 -4.3223553 -4.3209324 -4.3194237 -4.3174157 -4.3179216 -4.3118634 -4.3021975 -4.2942119 -4.2938881 -4.3011441 -4.3164549 -4.3284535 -4.3341317][-4.3470516 -4.3450627 -4.3437171 -4.3437958 -4.3455033 -4.3484712 -4.3535624 -4.3524671 -4.3449674 -4.33499 -4.3282642 -4.3294482 -4.3395734 -4.3501844 -4.3565717]]...]
INFO - root - 2017-12-06 05:24:00.013458: step 8310, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:58m:14s remains)
INFO - root - 2017-12-06 05:24:06.381157: step 8320, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 58h:19m:12s remains)
INFO - root - 2017-12-06 05:24:12.841544: step 8330, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 57h:53m:07s remains)
INFO - root - 2017-12-06 05:24:19.140426: step 8340, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 0.553 sec/batch; 49h:48m:20s remains)
INFO - root - 2017-12-06 05:24:25.628034: step 8350, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.634 sec/batch; 57h:04m:20s remains)
INFO - root - 2017-12-06 05:24:32.024941: step 8360, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:17m:31s remains)
INFO - root - 2017-12-06 05:24:38.512249: step 8370, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 58h:49m:29s remains)
INFO - root - 2017-12-06 05:24:45.030398: step 8380, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:30m:24s remains)
INFO - root - 2017-12-06 05:24:51.521905: step 8390, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 58h:56m:42s remains)
INFO - root - 2017-12-06 05:24:56.954881: step 8400, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 56h:08m:38s remains)
2017-12-06 05:24:57.592643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3687234 -4.3853192 -4.3905497 -4.3625159 -4.2990389 -4.2157965 -4.1458621 -4.1137605 -4.1268311 -4.1666422 -4.2168555 -4.2574377 -4.2850356 -4.2942672 -4.2925358][-4.3791237 -4.3970356 -4.404789 -4.384202 -4.3330259 -4.258019 -4.181776 -4.1292448 -4.1158962 -4.135035 -4.18059 -4.2295027 -4.2727356 -4.2992306 -4.3089681][-4.3864946 -4.4036951 -4.4107585 -4.3939013 -4.3473978 -4.2747288 -4.1919661 -4.1237822 -4.0896478 -4.0935407 -4.1395884 -4.1982937 -4.2576895 -4.3028989 -4.3279347][-4.3893676 -4.4053221 -4.4118209 -4.3982344 -4.3538265 -4.2818785 -4.195199 -4.1129165 -4.05597 -4.0444565 -4.0929966 -4.1596761 -4.233418 -4.2958646 -4.3367562][-4.390934 -4.4046192 -4.4069695 -4.3909454 -4.3455706 -4.2728391 -4.1833267 -4.0858026 -4.0038986 -3.9769533 -4.0317883 -4.1101117 -4.1962118 -4.2728477 -4.327538][-4.391861 -4.4028325 -4.3981647 -4.3721576 -4.3171153 -4.2372952 -4.1407318 -4.0266151 -3.9205863 -3.8875637 -3.9605026 -4.0557432 -4.1541505 -4.2429724 -4.3098779][-4.3896337 -4.3965397 -4.3830285 -4.3437753 -4.2756295 -4.1893187 -4.0895958 -3.9685025 -3.8502729 -3.8220377 -3.9099948 -4.0150681 -4.11816 -4.2127757 -4.2885213][-4.3846674 -4.3856659 -4.3639021 -4.3127589 -4.2365665 -4.1529365 -4.0642824 -3.9591217 -3.8597479 -3.8454723 -3.9229507 -4.0155869 -4.1094532 -4.1999292 -4.2762651][-4.3778591 -4.373332 -4.3465075 -4.2908568 -4.2149038 -4.14053 -4.0709238 -3.9943223 -3.9287386 -3.9295509 -3.9855049 -4.0543075 -4.1309919 -4.2109227 -4.27912][-4.37143 -4.363533 -4.3367553 -4.284482 -4.2167306 -4.1546993 -4.105494 -4.0601859 -4.0289593 -4.0397019 -4.076426 -4.1233468 -4.1833978 -4.2481556 -4.2998776][-4.3676634 -4.3602524 -4.3390303 -4.297051 -4.2440014 -4.1966872 -4.1647372 -4.1452308 -4.1377249 -4.1501527 -4.1712327 -4.2027545 -4.245892 -4.2920523 -4.3239264][-4.3666091 -4.3631024 -4.3509521 -4.3239603 -4.2880039 -4.2562103 -4.2384171 -4.233808 -4.2371221 -4.2463126 -4.2578683 -4.2772069 -4.3027415 -4.3285942 -4.3401589][-4.36463 -4.3647904 -4.3607869 -4.3479838 -4.3282843 -4.3102064 -4.3018265 -4.3029428 -4.3096294 -4.3169088 -4.3242121 -4.3353581 -4.3467212 -4.354198 -4.3490057][-4.3606954 -4.3624978 -4.3632817 -4.3600569 -4.3529286 -4.3452592 -4.3424459 -4.3452363 -4.3512769 -4.356319 -4.3612332 -4.3658967 -4.365674 -4.3598123 -4.3435407][-4.3566508 -4.3587179 -4.360682 -4.3613868 -4.3605638 -4.3592606 -4.3597455 -4.3622909 -4.3655038 -4.3678088 -4.3695736 -4.3673911 -4.3582358 -4.3436732 -4.321496]]...]
INFO - root - 2017-12-06 05:25:04.068964: step 8410, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 60h:24m:23s remains)
INFO - root - 2017-12-06 05:25:10.510070: step 8420, loss = 2.03, batch loss = 1.97 (12.9 examples/sec; 0.619 sec/batch; 55h:45m:02s remains)
INFO - root - 2017-12-06 05:25:16.952033: step 8430, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 57h:41m:12s remains)
INFO - root - 2017-12-06 05:25:23.338152: step 8440, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:14m:37s remains)
INFO - root - 2017-12-06 05:25:28.455070: step 8450, loss = 2.03, batch loss = 1.98 (19.0 examples/sec; 0.422 sec/batch; 37h:58m:12s remains)
INFO - root - 2017-12-06 05:25:34.613918: step 8460, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 58h:57m:53s remains)
INFO - root - 2017-12-06 05:25:41.123902: step 8470, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 58h:09m:24s remains)
INFO - root - 2017-12-06 05:25:47.407636: step 8480, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 57h:04m:05s remains)
INFO - root - 2017-12-06 05:25:53.757433: step 8490, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 56h:24m:31s remains)
INFO - root - 2017-12-06 05:25:59.993524: step 8500, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 0.506 sec/batch; 45h:34m:00s remains)
2017-12-06 05:26:00.633305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2734261 -4.2880483 -4.2965903 -4.2927613 -4.289844 -4.2937455 -4.296083 -4.2961378 -4.2894964 -4.2799368 -4.2753935 -4.27399 -4.2722306 -4.2689295 -4.2741871][-4.2941651 -4.3075714 -4.3167424 -4.3186464 -4.3179259 -4.315845 -4.3113222 -4.3078537 -4.3012276 -4.2939305 -4.2875957 -4.2809291 -4.2719126 -4.2644792 -4.2679482][-4.3080444 -4.315691 -4.3220558 -4.3256388 -4.3243232 -4.3148317 -4.30379 -4.2955742 -4.2890797 -4.283917 -4.2760825 -4.2657118 -4.2494946 -4.2377267 -4.2395878][-4.3024092 -4.3013387 -4.3034353 -4.3039508 -4.2954531 -4.2742891 -4.2506824 -4.2355413 -4.2318668 -4.2379723 -4.2408404 -4.2343645 -4.2143393 -4.1984019 -4.2003446][-4.259572 -4.2523537 -4.2498951 -4.2409587 -4.2191329 -4.1835895 -4.143805 -4.1163797 -4.1193357 -4.14659 -4.1711178 -4.1804852 -4.1687932 -4.1594973 -4.1695461][-4.182766 -4.1775975 -4.17327 -4.154027 -4.1204023 -4.0731015 -4.0128765 -3.9658177 -3.9791214 -4.0332355 -4.081449 -4.1077013 -4.1124525 -4.1213136 -4.1507182][-4.1107216 -4.1133533 -4.1067123 -4.0813351 -4.041749 -3.9907126 -3.9110627 -3.8468199 -3.8708012 -3.9478586 -4.0161119 -4.0565548 -4.0807819 -4.112443 -4.1606927][-4.0754638 -4.0776458 -4.0673366 -4.0477257 -4.0231857 -3.9868481 -3.9088953 -3.8493454 -3.8744714 -3.947001 -4.0180564 -4.0680547 -4.1049762 -4.1506186 -4.2036552][-4.0945196 -4.0841179 -4.0734487 -4.0719662 -4.0739326 -4.0610209 -4.0100994 -3.9753358 -3.9898186 -4.0292015 -4.0784903 -4.12359 -4.15848 -4.198638 -4.23957][-4.1525116 -4.1324978 -4.1210752 -4.1307483 -4.1435895 -4.1401525 -4.1138463 -4.0955772 -4.1006131 -4.1151276 -4.1460881 -4.1788206 -4.2008529 -4.2244377 -4.245635][-4.2026973 -4.1758146 -4.1591392 -4.1687527 -4.1806426 -4.177042 -4.1614881 -4.1515918 -4.1470804 -4.1472411 -4.1681967 -4.1948562 -4.2082624 -4.2199535 -4.2268677][-4.2425628 -4.213675 -4.1950388 -4.2004428 -4.2051587 -4.1945972 -4.1788988 -4.1689835 -4.161407 -4.1618848 -4.1823878 -4.2073908 -4.2178926 -4.2226744 -4.2197332][-4.273983 -4.2523222 -4.2397776 -4.2425027 -4.2424965 -4.2292333 -4.2146244 -4.2055845 -4.2015839 -4.2084427 -4.2279387 -4.2486043 -4.2580748 -4.2611289 -4.2563257][-4.3056531 -4.2948089 -4.2896032 -4.2915611 -4.2905164 -4.2830496 -4.2754703 -4.2707033 -4.2694378 -4.2754645 -4.2877035 -4.3010144 -4.3092771 -4.3136935 -4.3116632][-4.3315406 -4.3280139 -4.3278418 -4.3300128 -4.3305497 -4.3291717 -4.3271995 -4.3268094 -4.3267264 -4.328774 -4.3327112 -4.3366718 -4.3401761 -4.3431296 -4.3426228]]...]
INFO - root - 2017-12-06 05:26:07.032616: step 8510, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.633 sec/batch; 56h:58m:44s remains)
INFO - root - 2017-12-06 05:26:13.360459: step 8520, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 56h:50m:27s remains)
INFO - root - 2017-12-06 05:26:19.670599: step 8530, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.609 sec/batch; 54h:47m:06s remains)
INFO - root - 2017-12-06 05:26:26.030641: step 8540, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 58h:21m:21s remains)
INFO - root - 2017-12-06 05:26:32.509390: step 8550, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:16m:54s remains)
INFO - root - 2017-12-06 05:26:38.824383: step 8560, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 58h:57m:08s remains)
INFO - root - 2017-12-06 05:26:45.292289: step 8570, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 57h:30m:41s remains)
INFO - root - 2017-12-06 05:26:51.676559: step 8580, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 56h:40m:24s remains)
INFO - root - 2017-12-06 05:26:58.036998: step 8590, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 56h:15m:41s remains)
INFO - root - 2017-12-06 05:27:04.428455: step 8600, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 59h:08m:46s remains)
2017-12-06 05:27:06.653582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2825789 -4.2820082 -4.2808657 -4.2794909 -4.2780061 -4.2747569 -4.2696037 -4.2643342 -4.2607484 -4.2612662 -4.2641463 -4.2673488 -4.2674108 -4.2635837 -4.2555065][-4.2920494 -4.2909732 -4.2906003 -4.2917771 -4.2930837 -4.2911592 -4.2842093 -4.2756758 -4.2696118 -4.2703533 -4.2761145 -4.2824287 -4.2837949 -4.2790418 -4.269259][-4.2910662 -4.289309 -4.2914228 -4.2969241 -4.302114 -4.3009634 -4.29106 -4.2787681 -4.272408 -4.2764621 -4.2850351 -4.2937493 -4.2959523 -4.2909341 -4.2820988][-4.2730131 -4.2665811 -4.2674489 -4.2739744 -4.2794023 -4.2767658 -4.2642632 -4.2494364 -4.2497306 -4.2644334 -4.2785373 -4.2879219 -4.2895708 -4.2841692 -4.27707][-4.2386208 -4.2283683 -4.2281427 -4.2346921 -4.2349224 -4.2207737 -4.1943226 -4.1703963 -4.18045 -4.2136288 -4.2406588 -4.2557807 -4.2614207 -4.2586255 -4.254066][-4.1821704 -4.1710048 -4.17661 -4.1918225 -4.1916003 -4.163712 -4.1133046 -4.0638103 -4.0682912 -4.1214743 -4.1693873 -4.1963158 -4.2096806 -4.2130194 -4.2136827][-4.1413808 -4.1288743 -4.1353951 -4.1547589 -4.1569762 -4.118741 -4.0420971 -3.954509 -3.9404991 -4.0107083 -4.0772495 -4.1176381 -4.1392565 -4.1496167 -4.1572165][-4.1415925 -4.1284461 -4.1315837 -4.1470447 -4.1455398 -4.1045189 -4.0254006 -3.9358997 -3.9143844 -3.9801195 -4.0485497 -4.0943861 -4.1183248 -4.128129 -4.134624][-4.148242 -4.1357193 -4.1385117 -4.1537123 -4.1553855 -4.1284733 -4.077208 -4.020596 -4.0051508 -4.0478554 -4.0957861 -4.1319213 -4.1465859 -4.1466584 -4.1429529][-4.1479988 -4.1386962 -4.14437 -4.1599846 -4.16558 -4.1498652 -4.1234694 -4.095377 -4.0850034 -4.105629 -4.1334376 -4.1607413 -4.1720753 -4.1688356 -4.1584988][-4.1362557 -4.1309948 -4.1393223 -4.1511526 -4.1590967 -4.1523943 -4.1421642 -4.1328297 -4.1253362 -4.1326976 -4.1542945 -4.179038 -4.1913242 -4.1889043 -4.1764112][-4.1365108 -4.1323786 -4.1438417 -4.154716 -4.1631794 -4.1645007 -4.1637788 -4.1616416 -4.1548767 -4.1566725 -4.1779466 -4.2005424 -4.2107482 -4.2090454 -4.2008877][-4.1653061 -4.1610813 -4.1703258 -4.17897 -4.1863446 -4.190311 -4.1904058 -4.1893506 -4.184938 -4.1879091 -4.2060628 -4.2220511 -4.2284212 -4.2288852 -4.2262063][-4.2040582 -4.2007475 -4.2069812 -4.212141 -4.2170248 -4.2191057 -4.2176394 -4.215971 -4.2126017 -4.2159519 -4.2269292 -4.2341638 -4.2376919 -4.2396927 -4.23857][-4.2401752 -4.237937 -4.23967 -4.2397351 -4.2419863 -4.2409072 -4.2375736 -4.2355494 -4.2312741 -4.2308936 -4.2363343 -4.2401724 -4.2420392 -4.243484 -4.2428894]]...]
INFO - root - 2017-12-06 05:27:13.149297: step 8610, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:34m:31s remains)
INFO - root - 2017-12-06 05:27:19.619073: step 8620, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 60h:53m:18s remains)
INFO - root - 2017-12-06 05:27:25.997430: step 8630, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 57h:58m:40s remains)
INFO - root - 2017-12-06 05:27:32.392357: step 8640, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 56h:14m:32s remains)
INFO - root - 2017-12-06 05:27:38.812675: step 8650, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 59h:01m:33s remains)
INFO - root - 2017-12-06 05:27:44.963171: step 8660, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 56h:42m:32s remains)
INFO - root - 2017-12-06 05:27:51.371564: step 8670, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 57h:33m:30s remains)
INFO - root - 2017-12-06 05:27:57.850562: step 8680, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 57h:28m:36s remains)
INFO - root - 2017-12-06 05:28:04.148731: step 8690, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.614 sec/batch; 55h:13m:27s remains)
INFO - root - 2017-12-06 05:28:10.585720: step 8700, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 58h:21m:45s remains)
2017-12-06 05:28:12.046143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3055282 -4.2992783 -4.294559 -4.2944007 -4.3016472 -4.3013411 -4.2957458 -4.2865486 -4.2743683 -4.2661324 -4.2593474 -4.2595644 -4.2616215 -4.2696743 -4.2831783][-4.30407 -4.2940483 -4.285099 -4.2813993 -4.2912927 -4.2914062 -4.2857876 -4.2752185 -4.2662568 -4.265923 -4.2614946 -4.2601681 -4.2609048 -4.2672653 -4.2769489][-4.3004656 -4.2849526 -4.2688427 -4.2581158 -4.2648587 -4.2619362 -4.2528338 -4.2406788 -4.2416654 -4.2562032 -4.2576661 -4.2587924 -4.2612596 -4.2685947 -4.2773042][-4.2898474 -4.268116 -4.2442446 -4.22941 -4.2265463 -4.21341 -4.1891284 -4.1769357 -4.19321 -4.2254496 -4.2386827 -4.2466936 -4.25359 -4.266675 -4.27745][-4.2750578 -4.2482896 -4.2188091 -4.1965971 -4.1726532 -4.134799 -4.0835462 -4.0662632 -4.1032987 -4.1584578 -4.1877503 -4.2103114 -4.2243452 -4.2452836 -4.2587595][-4.2589192 -4.2247729 -4.18436 -4.1459093 -4.0916114 -4.0158033 -3.941576 -3.9368086 -4.0064611 -4.0819497 -4.123322 -4.1647129 -4.1895328 -4.2192626 -4.2366104][-4.2441111 -4.2014956 -4.1476731 -4.0896006 -4.0070939 -3.8861849 -3.7898746 -3.8139937 -3.9231024 -4.0133977 -4.0671062 -4.1254292 -4.1634479 -4.2011242 -4.2256188][-4.2393627 -4.1983452 -4.1413894 -4.0651112 -3.9558086 -3.7928984 -3.6809857 -3.7425244 -3.8891907 -3.9924357 -4.0558071 -4.1162624 -4.1571741 -4.1956687 -4.2257953][-4.2451873 -4.2114596 -4.1556764 -4.0760927 -3.9748061 -3.8204021 -3.7258816 -3.8019059 -3.9480658 -4.0430312 -4.0976691 -4.1437025 -4.1730704 -4.2070374 -4.2361536][-4.2590632 -4.2289152 -4.1757774 -4.1073594 -4.0321121 -3.9223473 -3.8670094 -3.9335587 -4.0496364 -4.1234374 -4.1663723 -4.2001061 -4.2136874 -4.2373228 -4.2594166][-4.2818007 -4.2569456 -4.2113762 -4.1604295 -4.111896 -4.0473576 -4.0259767 -4.0821548 -4.1633387 -4.2119393 -4.2414827 -4.2629809 -4.2657671 -4.2764993 -4.2878261][-4.3019061 -4.2866793 -4.2578487 -4.2281489 -4.2027588 -4.1662431 -4.1633034 -4.20796 -4.2597456 -4.284811 -4.3001108 -4.308526 -4.3081975 -4.3143935 -4.3193517][-4.3172174 -4.3109679 -4.3001137 -4.2911477 -4.2862148 -4.2657571 -4.2655716 -4.2937055 -4.320899 -4.3305268 -4.3339128 -4.333056 -4.3304977 -4.3366694 -4.3399997][-4.3232255 -4.3179388 -4.3170476 -4.3209209 -4.3290758 -4.32278 -4.3209953 -4.3321528 -4.3405423 -4.3411727 -4.3396611 -4.3368678 -4.3340898 -4.3391695 -4.3410835][-4.3141265 -4.3092928 -4.3113127 -4.3201947 -4.333858 -4.3356967 -4.3303089 -4.3314185 -4.3305969 -4.3295894 -4.3276482 -4.3243561 -4.3222237 -4.3264441 -4.3295927]]...]
INFO - root - 2017-12-06 05:28:18.445462: step 8710, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 59h:30m:08s remains)
INFO - root - 2017-12-06 05:28:24.922094: step 8720, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:55m:43s remains)
INFO - root - 2017-12-06 05:28:31.408083: step 8730, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:46m:35s remains)
INFO - root - 2017-12-06 05:28:37.819704: step 8740, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 57h:08m:46s remains)
INFO - root - 2017-12-06 05:28:44.237057: step 8750, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:01m:19s remains)
INFO - root - 2017-12-06 05:28:50.568294: step 8760, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:43m:04s remains)
INFO - root - 2017-12-06 05:28:56.958766: step 8770, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 56h:57m:22s remains)
INFO - root - 2017-12-06 05:29:03.370461: step 8780, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 57h:45m:52s remains)
INFO - root - 2017-12-06 05:29:09.703996: step 8790, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.633 sec/batch; 56h:54m:32s remains)
INFO - root - 2017-12-06 05:29:16.125614: step 8800, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:01m:13s remains)
2017-12-06 05:29:16.709826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2014232 -4.2102818 -4.207665 -4.1930318 -4.1765075 -4.1666937 -4.1682315 -4.1864719 -4.2155228 -4.2454147 -4.2685442 -4.2863955 -4.2999783 -4.3068032 -4.2928038][-4.1857271 -4.2057314 -4.2126851 -4.2048426 -4.194346 -4.1856871 -4.1836867 -4.1959014 -4.218142 -4.242878 -4.263927 -4.2824478 -4.3011942 -4.3094015 -4.2954106][-4.1727419 -4.2067947 -4.2213902 -4.2182927 -4.2111936 -4.2008333 -4.1939774 -4.1969171 -4.2093468 -4.2279119 -4.2469125 -4.2656054 -4.288444 -4.2997737 -4.2858334][-4.1760178 -4.2179646 -4.2356491 -4.2335591 -4.2224922 -4.2038989 -4.1858921 -4.1761584 -4.1799107 -4.1987877 -4.2222037 -4.246027 -4.2731237 -4.2856617 -4.2713976][-4.1975288 -4.24085 -4.2573504 -4.2488966 -4.2226195 -4.1842742 -4.1405282 -4.1109438 -4.1162839 -4.1520824 -4.1934543 -4.2297468 -4.2613239 -4.2716818 -4.2549977][-4.232769 -4.2759585 -4.2838941 -4.262867 -4.2188354 -4.149375 -4.0607128 -3.9983521 -4.0177274 -4.0979552 -4.1730733 -4.2232761 -4.2533078 -4.2595119 -4.2400341][-4.2628818 -4.3020363 -4.3016257 -4.2710271 -4.2120838 -4.1078777 -3.9651029 -3.8641305 -3.9126744 -4.0522461 -4.1644187 -4.2240753 -4.2505183 -4.2501106 -4.227457][-4.2773666 -4.3057022 -4.3009038 -4.2680874 -4.2048683 -4.080657 -3.9028325 -3.7815933 -3.8645351 -4.0434203 -4.1694961 -4.2302771 -4.2503657 -4.2452674 -4.2214751][-4.2795596 -4.2970619 -4.2962375 -4.2718511 -4.2158251 -4.0980983 -3.9342687 -3.8343012 -3.9162672 -4.0773888 -4.1842532 -4.2337904 -4.2469425 -4.24047 -4.2232976][-4.2680912 -4.2811685 -4.2911682 -4.2848277 -4.247467 -4.1540008 -4.0327296 -3.9671812 -4.0197029 -4.1290526 -4.2022347 -4.2370038 -4.2428179 -4.2387528 -4.2327123][-4.2480946 -4.2557983 -4.2772994 -4.2940526 -4.2826943 -4.2218332 -4.1407919 -4.0970006 -4.117044 -4.1766109 -4.2218914 -4.2429786 -4.2417841 -4.2415481 -4.24885][-4.2239318 -4.229517 -4.2630858 -4.3032694 -4.3145981 -4.2818885 -4.2286248 -4.1914988 -4.1866517 -4.2124138 -4.2416363 -4.2529421 -4.2475758 -4.2492585 -4.2604938][-4.2121162 -4.22233 -4.26315 -4.3132944 -4.33269 -4.3164511 -4.27903 -4.2430305 -4.226388 -4.2342777 -4.2527866 -4.2612238 -4.2568 -4.2579589 -4.2665353][-4.2229733 -4.2373352 -4.2760315 -4.3189335 -4.3367214 -4.3256774 -4.2977805 -4.2660789 -4.241766 -4.2399893 -4.2525029 -4.2606392 -4.25587 -4.2544775 -4.2598662][-4.2503939 -4.2619967 -4.291225 -4.3193369 -4.3286 -4.3175588 -4.293992 -4.26279 -4.2336059 -4.2255445 -4.2353587 -4.2464681 -4.2405705 -4.2318015 -4.231966]]...]
INFO - root - 2017-12-06 05:29:23.164093: step 8810, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:33m:32s remains)
INFO - root - 2017-12-06 05:29:29.540491: step 8820, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:27m:29s remains)
INFO - root - 2017-12-06 05:29:36.017634: step 8830, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 56h:47m:43s remains)
INFO - root - 2017-12-06 05:29:42.391509: step 8840, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 56h:20m:51s remains)
INFO - root - 2017-12-06 05:29:48.724833: step 8850, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.623 sec/batch; 56h:03m:05s remains)
INFO - root - 2017-12-06 05:29:54.943628: step 8860, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 57h:59m:12s remains)
INFO - root - 2017-12-06 05:30:01.376933: step 8870, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 56h:44m:29s remains)
INFO - root - 2017-12-06 05:30:07.814659: step 8880, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:31m:49s remains)
INFO - root - 2017-12-06 05:30:14.321238: step 8890, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 57h:08m:05s remains)
INFO - root - 2017-12-06 05:30:20.770228: step 8900, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 58h:59m:28s remains)
2017-12-06 05:30:21.498907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1879945 -4.179409 -4.1773005 -4.1944394 -4.2102361 -4.2073488 -4.2115989 -4.2150488 -4.2190151 -4.2281976 -4.2257032 -4.2075467 -4.183042 -4.1584115 -4.1443229][-4.1271992 -4.1327643 -4.14882 -4.1825709 -4.2145376 -4.2260175 -4.2330904 -4.22881 -4.2245708 -4.231771 -4.2259827 -4.1974788 -4.1526937 -4.1106453 -4.0904403][-4.0489855 -4.0763946 -4.1259136 -4.1820517 -4.2221456 -4.2365818 -4.23503 -4.2191691 -4.2055707 -4.2058992 -4.1967216 -4.160429 -4.1025805 -4.054862 -4.0339508][-3.9798396 -4.03624 -4.1166811 -4.1881971 -4.2254558 -4.2281246 -4.2066693 -4.1757665 -4.1534228 -4.1515245 -4.1480632 -4.1171889 -4.0690308 -4.0372424 -4.0293012][-4.0198445 -4.0915437 -4.1711297 -4.220819 -4.2304916 -4.2041535 -4.151104 -4.096302 -4.0718942 -4.0815744 -4.0956445 -4.0897145 -4.0757861 -4.0726094 -4.0787606][-4.1254745 -4.1840887 -4.2320356 -4.2417789 -4.2141094 -4.1543107 -4.0661106 -3.9903719 -3.984591 -4.0278111 -4.0735593 -4.1010437 -4.11828 -4.13127 -4.138834][-4.2000122 -4.233326 -4.2420721 -4.2097154 -4.1428494 -4.0522056 -3.9302111 -3.8384452 -3.882966 -3.984545 -4.06493 -4.1174703 -4.1529546 -4.1684237 -4.1620355][-4.2290521 -4.2393131 -4.2149076 -4.1485519 -4.0519753 -3.9421103 -3.8113477 -3.7270737 -3.83029 -3.9741366 -4.0659285 -4.1287417 -4.1708016 -4.1781077 -4.1528168][-4.2347846 -4.2271857 -4.1839991 -4.1088457 -4.0180154 -3.9302456 -3.8467605 -3.8087928 -3.9053879 -4.0288653 -4.099545 -4.1493917 -4.1882443 -4.1870971 -4.1542873][-4.2199669 -4.2056966 -4.15961 -4.095325 -4.02939 -3.9777658 -3.9456985 -3.9442551 -4.0107145 -4.093987 -4.1381106 -4.1695075 -4.1979556 -4.1934233 -4.1673756][-4.2139497 -4.1943173 -4.1474171 -4.0949883 -4.051971 -4.03034 -4.0310788 -4.0488539 -4.0942254 -4.1457577 -4.1724648 -4.1905932 -4.2081652 -4.2042122 -4.1872973][-4.2228045 -4.2039413 -4.16889 -4.1382213 -4.1194067 -4.1155534 -4.1262569 -4.1436696 -4.1680913 -4.1944842 -4.2095561 -4.2190342 -4.2275839 -4.2272539 -4.2207403][-4.2552609 -4.2422752 -4.2230229 -4.2114348 -4.2071118 -4.2083964 -4.2186465 -4.2284675 -4.2365522 -4.2464557 -4.2544827 -4.2591906 -4.2645245 -4.268023 -4.2686806][-4.2886343 -4.280272 -4.2708449 -4.2687874 -4.2702913 -4.2747083 -4.284966 -4.2923064 -4.2948751 -4.2967873 -4.3008447 -4.3027024 -4.3058724 -4.3086743 -4.3108339][-4.3082075 -4.3048038 -4.3008776 -4.30211 -4.3044906 -4.3075247 -4.3139086 -4.3175244 -4.3179641 -4.317287 -4.3183155 -4.3180065 -4.3186154 -4.32006 -4.3215666]]...]
INFO - root - 2017-12-06 05:30:27.730859: step 8910, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 56h:42m:40s remains)
INFO - root - 2017-12-06 05:30:34.123838: step 8920, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 58h:43m:59s remains)
INFO - root - 2017-12-06 05:30:40.421742: step 8930, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 55h:51m:19s remains)
INFO - root - 2017-12-06 05:30:46.807439: step 8940, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:24m:07s remains)
INFO - root - 2017-12-06 05:30:53.323017: step 8950, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 58h:17m:46s remains)
INFO - root - 2017-12-06 05:30:59.708498: step 8960, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 58h:15m:27s remains)
INFO - root - 2017-12-06 05:31:05.076491: step 8970, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:16m:51s remains)
INFO - root - 2017-12-06 05:31:11.462987: step 8980, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:35m:53s remains)
INFO - root - 2017-12-06 05:31:17.902141: step 8990, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:40m:48s remains)
INFO - root - 2017-12-06 05:31:24.320666: step 9000, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:24m:08s remains)
2017-12-06 05:31:30.318462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1646118 -4.1523495 -4.1453581 -4.1446338 -4.1422482 -4.1394348 -4.1352372 -4.1274147 -4.1205716 -4.1239181 -4.1344562 -4.1420908 -4.1422014 -4.1289978 -4.1105132][-4.1430845 -4.1260986 -4.1152258 -4.1129737 -4.1090078 -4.1028814 -4.0956855 -4.08811 -4.0866933 -4.0991526 -4.1167126 -4.12898 -4.1308136 -4.11729 -4.0975342][-4.1274037 -4.107348 -4.0939827 -4.0931573 -4.0932364 -4.0878468 -4.08165 -4.0761757 -4.0822983 -4.1040354 -4.12555 -4.1369138 -4.1354847 -4.1200747 -4.0965552][-4.1047235 -4.0886588 -4.0783653 -4.0818143 -4.0865455 -4.083056 -4.0756645 -4.0717406 -4.0862761 -4.11834 -4.14428 -4.1549025 -4.1505241 -4.1354046 -4.1134763][-4.0632277 -4.0574207 -4.051784 -4.0534935 -4.054769 -4.0456619 -4.0310016 -4.0258408 -4.0512862 -4.09922 -4.13663 -4.1548862 -4.1538181 -4.1439929 -4.1318984][-4.0137362 -4.0140867 -4.0095353 -4.0040274 -3.9966772 -3.9778388 -3.9478576 -3.934648 -3.9735966 -4.0412807 -4.0965281 -4.1293139 -4.1396427 -4.1386447 -4.136065][-4.0088387 -4.0034194 -3.9943533 -3.9830291 -3.966876 -3.9380774 -3.8938193 -3.8649626 -3.9037893 -3.981189 -4.049861 -4.0949874 -4.1172919 -4.1218615 -4.1216669][-4.0540862 -4.0456357 -4.0390053 -4.0333209 -4.0227528 -4.0037036 -3.969305 -3.9375765 -3.9548631 -4.0074182 -4.0591516 -4.0936513 -4.1110125 -4.1113038 -4.1035824][-4.1069269 -4.1009288 -4.1002192 -4.1006441 -4.0971613 -4.092329 -4.0808759 -4.0669193 -4.0756989 -4.1029658 -4.130074 -4.1461291 -4.15484 -4.1524649 -4.1378703][-4.1506648 -4.1458483 -4.1482449 -4.1536641 -4.1554976 -4.1574535 -4.1588254 -4.1576567 -4.1657529 -4.1777544 -4.1884665 -4.1940646 -4.1994877 -4.200572 -4.1913404][-4.1789751 -4.174706 -4.176827 -4.1829495 -4.188324 -4.1944361 -4.2021608 -4.2082567 -4.2154703 -4.2193327 -4.2221737 -4.2261972 -4.2300181 -4.2313204 -4.2284102][-4.199832 -4.1951766 -4.1937256 -4.1948376 -4.1970596 -4.2025552 -4.2132974 -4.2239833 -4.2313204 -4.2313061 -4.2324133 -4.2388287 -4.2434015 -4.2441111 -4.2451596][-4.2236757 -4.2172842 -4.2120152 -4.2065821 -4.2009935 -4.2020364 -4.2114086 -4.2206054 -4.2254477 -4.2230525 -4.2234955 -4.2311006 -4.237546 -4.2395663 -4.2425079][-4.2288117 -4.2240081 -4.2208896 -4.2152977 -4.2098083 -4.2112293 -4.2201557 -4.228734 -4.232295 -4.2295914 -4.2272339 -4.229991 -4.2327533 -4.2331848 -4.2356067][-4.2155447 -4.2131872 -4.2142606 -4.2125883 -4.2100949 -4.2131276 -4.2217088 -4.230083 -4.2346888 -4.2354045 -4.2344284 -4.2350941 -4.2375021 -4.2397642 -4.2431178]]...]
INFO - root - 2017-12-06 05:31:36.722513: step 9010, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:00m:37s remains)
INFO - root - 2017-12-06 05:31:43.114782: step 9020, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 57h:09m:49s remains)
INFO - root - 2017-12-06 05:31:49.460038: step 9030, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:06m:22s remains)
INFO - root - 2017-12-06 05:31:55.781251: step 9040, loss = 2.09, batch loss = 2.04 (12.9 examples/sec; 0.620 sec/batch; 55h:44m:01s remains)
INFO - root - 2017-12-06 05:32:02.053056: step 9050, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 58h:22m:54s remains)
INFO - root - 2017-12-06 05:32:08.515952: step 9060, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.617 sec/batch; 55h:25m:53s remains)
INFO - root - 2017-12-06 05:32:14.804741: step 9070, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 56h:49m:38s remains)
INFO - root - 2017-12-06 05:32:21.207104: step 9080, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:32m:58s remains)
INFO - root - 2017-12-06 05:32:27.579925: step 9090, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 57h:36m:54s remains)
INFO - root - 2017-12-06 05:32:34.029658: step 9100, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 56h:34m:52s remains)
2017-12-06 05:32:34.596315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.321506 -4.2936664 -4.246727 -4.1979074 -4.1788335 -4.1823225 -4.1891565 -4.1914353 -4.2016854 -4.2059331 -4.2153606 -4.2316775 -4.2532339 -4.2581344 -4.2386565][-4.3224554 -4.2919574 -4.2435842 -4.1974063 -4.1803327 -4.1846094 -4.1953754 -4.2025342 -4.2118049 -4.2122893 -4.2147226 -4.2277422 -4.250411 -4.2573729 -4.2400627][-4.32652 -4.2943377 -4.2439327 -4.1969128 -4.1791396 -4.1808391 -4.193315 -4.2106695 -4.2242908 -4.2264557 -4.2207279 -4.2210965 -4.2359247 -4.2421679 -4.2316742][-4.3359108 -4.30367 -4.2520137 -4.1979737 -4.1707749 -4.1659203 -4.1770287 -4.2021317 -4.2245121 -4.2300792 -4.2208467 -4.2127481 -4.216372 -4.2194653 -4.2174792][-4.3459487 -4.3143435 -4.2600121 -4.1963596 -4.1530185 -4.1334872 -4.1375475 -4.1646423 -4.2002869 -4.2214308 -4.2189541 -4.208653 -4.2045722 -4.2029681 -4.2070475][-4.3544121 -4.3255606 -4.2700639 -4.1951008 -4.1271219 -4.07957 -4.0632119 -4.0869446 -4.1468167 -4.1996732 -4.2196178 -4.2179203 -4.209002 -4.2011418 -4.2021012][-4.3588996 -4.3345251 -4.2824926 -4.2033052 -4.1138911 -4.0297055 -3.9720676 -3.9794521 -4.0688872 -4.1600885 -4.2129507 -4.2321153 -4.2302785 -4.2203674 -4.2132506][-4.3613148 -4.342566 -4.297389 -4.2232561 -4.1283092 -4.0190272 -3.9148006 -3.8899817 -3.9942737 -4.1112027 -4.1921134 -4.2363758 -4.2513866 -4.2464156 -4.23415][-4.3629622 -4.3469057 -4.3086948 -4.2471089 -4.1650071 -4.0670948 -3.9657307 -3.9295924 -4.00175 -4.0955253 -4.1727314 -4.2283063 -4.2543888 -4.25692 -4.2451282][-4.3627124 -4.348331 -4.316112 -4.2652965 -4.2009296 -4.1330004 -4.0692563 -4.0461397 -4.0755692 -4.1171594 -4.1691375 -4.2210336 -4.2509422 -4.2591515 -4.2522812][-4.3608785 -4.3461156 -4.3145208 -4.265512 -4.2103362 -4.1679287 -4.137012 -4.1321483 -4.1450853 -4.1582503 -4.1883788 -4.2267818 -4.2520256 -4.2573819 -4.2505932][-4.3566871 -4.3400536 -4.3052397 -4.2540994 -4.2039313 -4.1755385 -4.1605868 -4.1649051 -4.1780305 -4.1925759 -4.2150044 -4.2378421 -4.2503548 -4.2466993 -4.2345114][-4.3522983 -4.334013 -4.29458 -4.2403312 -4.1931896 -4.1676702 -4.1558461 -4.1619878 -4.1831245 -4.2099476 -4.2339082 -4.2464223 -4.2474742 -4.2344265 -4.2137866][-4.3506942 -4.3305964 -4.2889571 -4.23504 -4.191123 -4.1674714 -4.158021 -4.1702337 -4.1983118 -4.2256513 -4.2423635 -4.2482691 -4.2429113 -4.2237964 -4.1965141][-4.3498507 -4.3290653 -4.2901087 -4.2453442 -4.2097244 -4.1892653 -4.1805673 -4.1951075 -4.2225361 -4.2439485 -4.2511473 -4.2518744 -4.2429562 -4.223434 -4.196486]]...]
INFO - root - 2017-12-06 05:32:41.065902: step 9110, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 61h:04m:11s remains)
INFO - root - 2017-12-06 05:32:47.413765: step 9120, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 57h:51m:38s remains)
INFO - root - 2017-12-06 05:32:53.805744: step 9130, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:31m:52s remains)
INFO - root - 2017-12-06 05:33:00.151590: step 9140, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 57h:59m:00s remains)
INFO - root - 2017-12-06 05:33:06.427062: step 9150, loss = 2.05, batch loss = 1.99 (13.2 examples/sec; 0.604 sec/batch; 54h:16m:26s remains)
INFO - root - 2017-12-06 05:33:12.667689: step 9160, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 56h:31m:19s remains)
INFO - root - 2017-12-06 05:33:19.098812: step 9170, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 57h:49m:31s remains)
INFO - root - 2017-12-06 05:33:25.445064: step 9180, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.626 sec/batch; 56h:13m:49s remains)
INFO - root - 2017-12-06 05:33:31.858056: step 9190, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 55h:52m:34s remains)
INFO - root - 2017-12-06 05:33:38.319180: step 9200, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 60h:27m:41s remains)
2017-12-06 05:33:40.738902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3157229 -4.3028755 -4.2903924 -4.2738929 -4.2510281 -4.2343431 -4.2378683 -4.2533293 -4.2620392 -4.264082 -4.2710075 -4.2820082 -4.2854309 -4.2852221 -4.2842827][-4.3138723 -4.2996035 -4.2760167 -4.2430468 -4.2051144 -4.18036 -4.1797256 -4.1987357 -4.2178621 -4.228199 -4.2417226 -4.2546029 -4.2576513 -4.2569261 -4.2544351][-4.3144636 -4.2972946 -4.2607689 -4.2100205 -4.1591892 -4.12873 -4.1189704 -4.1359954 -4.1668992 -4.1905851 -4.2125888 -4.2285976 -4.2333951 -4.232924 -4.2283072][-4.3158393 -4.2949338 -4.2489033 -4.1885853 -4.1302629 -4.0888405 -4.0577617 -4.0675559 -4.1076488 -4.1469035 -4.1777182 -4.1958761 -4.1989913 -4.1990094 -4.1940985][-4.3176947 -4.2974515 -4.2462521 -4.1825933 -4.1177816 -4.0538573 -3.9895396 -3.9873533 -4.0450883 -4.1034784 -4.1473813 -4.1671953 -4.170146 -4.1666622 -4.16111][-4.3185382 -4.3005381 -4.2477489 -4.180573 -4.1075554 -4.0185742 -3.9121149 -3.876066 -3.9445481 -4.0327964 -4.1008019 -4.1302786 -4.1359267 -4.1341248 -4.1336565][-4.3130016 -4.2946634 -4.2421947 -4.1695938 -4.08578 -3.9703367 -3.8120177 -3.7202289 -3.8017626 -3.9425359 -4.0456729 -4.0918608 -4.1015754 -4.1022906 -4.1079755][-4.3019047 -4.27971 -4.2237 -4.1436996 -4.0455327 -3.909672 -3.7234511 -3.6016979 -3.7044785 -3.8863544 -4.0122528 -4.0666108 -4.0773888 -4.0823951 -4.0948443][-4.2885313 -4.2636032 -4.2056885 -4.1251674 -4.0319533 -3.9100664 -3.7693796 -3.7052805 -3.8018274 -3.943208 -4.0402927 -4.0775728 -4.0776653 -4.0786023 -4.0913529][-4.2785888 -4.2584295 -4.206737 -4.1405725 -4.0687723 -3.9783363 -3.8908632 -3.8691988 -3.9385686 -4.0184622 -4.0734358 -4.0956841 -4.0944881 -4.0951657 -4.1083612][-4.2742252 -4.2609591 -4.2252736 -4.1806331 -4.1304727 -4.0603476 -3.997654 -3.9904993 -4.0332818 -4.0718803 -4.1003389 -4.11729 -4.1277704 -4.135181 -4.1471024][-4.2792392 -4.2703242 -4.2489595 -4.2187972 -4.1776423 -4.1241455 -4.0804043 -4.0837145 -4.1114464 -4.1249666 -4.1341624 -4.145628 -4.1655679 -4.1784821 -4.1894774][-4.2969079 -4.291935 -4.2773557 -4.253264 -4.2227035 -4.189117 -4.1665649 -4.1744661 -4.184948 -4.1826177 -4.1812296 -4.1893988 -4.2122521 -4.2259064 -4.232389][-4.318306 -4.3160768 -4.3081059 -4.2935619 -4.2739882 -4.2537589 -4.2454376 -4.2515759 -4.25449 -4.2503362 -4.2447643 -4.2470565 -4.2635417 -4.2730517 -4.2741909][-4.3384438 -4.3388796 -4.3342681 -4.3248773 -4.3095837 -4.297667 -4.2971587 -4.3056936 -4.309052 -4.3060055 -4.2995915 -4.297689 -4.3069339 -4.3135324 -4.31096]]...]
INFO - root - 2017-12-06 05:33:47.211158: step 9210, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 59h:01m:00s remains)
INFO - root - 2017-12-06 05:33:53.670158: step 9220, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 58h:00m:32s remains)
INFO - root - 2017-12-06 05:34:00.013582: step 9230, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 56h:17m:16s remains)
INFO - root - 2017-12-06 05:34:06.526639: step 9240, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 59h:23m:23s remains)
INFO - root - 2017-12-06 05:34:12.817254: step 9250, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 58h:04m:20s remains)
INFO - root - 2017-12-06 05:34:19.292158: step 9260, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 55h:59m:11s remains)
INFO - root - 2017-12-06 05:34:25.640753: step 9270, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:08m:46s remains)
INFO - root - 2017-12-06 05:34:32.153441: step 9280, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 58h:51m:20s remains)
INFO - root - 2017-12-06 05:34:38.510448: step 9290, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.616 sec/batch; 55h:16m:39s remains)
INFO - root - 2017-12-06 05:34:45.007304: step 9300, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 56h:18m:02s remains)
2017-12-06 05:34:45.637314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2622867 -4.2424011 -4.2265654 -4.2094369 -4.192409 -4.1807947 -4.1661568 -4.1568809 -4.1490874 -4.1423783 -4.1328483 -4.1321793 -4.155827 -4.1811771 -4.1945558][-4.2558436 -4.24391 -4.232286 -4.2135253 -4.1947322 -4.1828628 -4.1672883 -4.1594515 -4.1603026 -4.1628742 -4.1527572 -4.1474466 -4.16125 -4.1768556 -4.1873932][-4.2302537 -4.2237191 -4.2191105 -4.2068367 -4.19114 -4.1796818 -4.1650033 -4.1612048 -4.1727066 -4.1833048 -4.176723 -4.1671376 -4.1682296 -4.1707416 -4.1744533][-4.2164955 -4.2053347 -4.2005506 -4.1936588 -4.1808333 -4.16524 -4.1513743 -4.1556711 -4.1793036 -4.1988759 -4.1981664 -4.187129 -4.1784258 -4.1690197 -4.1647563][-4.2202353 -4.2005911 -4.1911016 -4.1841159 -4.1706424 -4.1472511 -4.1270914 -4.1308842 -4.1613255 -4.1883149 -4.19722 -4.1906896 -4.1774721 -4.1578426 -4.1454768][-4.2259812 -4.2004533 -4.1841321 -4.1712408 -4.1512833 -4.1184087 -4.0852017 -4.0791931 -4.1140838 -4.1551213 -4.1806111 -4.1851263 -4.1692371 -4.1413488 -4.1213088][-4.2193084 -4.192462 -4.17297 -4.1544743 -4.1258373 -4.0839157 -4.032547 -4.0075884 -4.0437784 -4.1014185 -4.1488781 -4.1702251 -4.1580429 -4.129385 -4.1054716][-4.1903763 -4.1616187 -4.1396985 -4.1166468 -4.0827689 -4.0359221 -3.9675827 -3.9165311 -3.9512391 -4.0258589 -4.091568 -4.1295118 -4.1312518 -4.1108284 -4.0932064][-4.1608295 -4.1307673 -4.1100297 -4.0855732 -4.0542541 -4.0153422 -3.9542682 -3.8976665 -3.9193988 -3.9884067 -4.0529032 -4.0939674 -4.1023736 -4.0900464 -4.0802193][-4.1413755 -4.1109843 -4.0918489 -4.0700841 -4.0471373 -4.0268741 -3.9976752 -3.9669564 -3.9770885 -4.0197635 -4.061038 -4.0846491 -4.0837259 -4.0700083 -4.0614228][-4.1330519 -4.0993881 -4.0758777 -4.0521212 -4.0337338 -4.0276041 -4.0213666 -4.0155687 -4.0254731 -4.0491152 -4.0672555 -4.0709167 -4.056251 -4.0391722 -4.0348973][-4.155539 -4.1255531 -4.10235 -4.0788651 -4.0644255 -4.0657611 -4.0697742 -4.0748687 -4.0858612 -4.1003747 -4.105361 -4.095645 -4.0723934 -4.0537052 -4.0507584][-4.1829085 -4.1582365 -4.1384859 -4.1177125 -4.1060715 -4.1103086 -4.1200819 -4.1281414 -4.1368666 -4.1458759 -4.1461916 -4.1323028 -4.1087246 -4.0925732 -4.0920377][-4.2120438 -4.1905379 -4.1730313 -4.1558123 -4.1472659 -4.1526837 -4.1647878 -4.1744003 -4.1818862 -4.1875458 -4.1875629 -4.1778412 -4.1618905 -4.1508541 -4.1509919][-4.2360139 -4.220758 -4.2085342 -4.1971612 -4.1924706 -4.1974988 -4.2080064 -4.2170482 -4.22277 -4.2259207 -4.225749 -4.2215309 -4.2134609 -4.2072849 -4.208436]]...]
INFO - root - 2017-12-06 05:34:52.048586: step 9310, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:06m:19s remains)
INFO - root - 2017-12-06 05:34:58.613901: step 9320, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 57h:17m:17s remains)
INFO - root - 2017-12-06 05:35:05.025661: step 9330, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:10m:34s remains)
INFO - root - 2017-12-06 05:35:11.629953: step 9340, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.664 sec/batch; 59h:38m:33s remains)
INFO - root - 2017-12-06 05:35:17.971405: step 9350, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 55h:23m:53s remains)
INFO - root - 2017-12-06 05:35:22.701555: step 9360, loss = 2.05, batch loss = 2.00 (18.9 examples/sec; 0.422 sec/batch; 37h:54m:51s remains)
INFO - root - 2017-12-06 05:35:28.618492: step 9370, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 59h:11m:39s remains)
INFO - root - 2017-12-06 05:35:35.051953: step 9380, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 58h:09m:08s remains)
INFO - root - 2017-12-06 05:35:41.567063: step 9390, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 58h:46m:51s remains)
INFO - root - 2017-12-06 05:35:47.928419: step 9400, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 56h:58m:20s remains)
2017-12-06 05:35:51.780321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2819157 -4.2560587 -4.2380953 -4.2326231 -4.233294 -4.2410021 -4.2469406 -4.2445936 -4.2412577 -4.2363091 -4.2353873 -4.2389483 -4.24424 -4.2530141 -4.262928][-4.2635431 -4.2301078 -4.2050352 -4.1981049 -4.1990428 -4.2090425 -4.2140822 -4.2032409 -4.1963959 -4.1934519 -4.1912951 -4.1952786 -4.1962128 -4.2033572 -4.2183247][-4.2414007 -4.2037444 -4.175324 -4.1697173 -4.1749697 -4.1842108 -4.182735 -4.1578722 -4.1452551 -4.1474485 -4.1464796 -4.1482048 -4.1425824 -4.1427445 -4.1625261][-4.2205086 -4.1857257 -4.16085 -4.1608772 -4.1686182 -4.1641197 -4.1434307 -4.0996604 -4.0798168 -4.0911565 -4.100636 -4.10633 -4.0965619 -4.0916934 -4.1156979][-4.2020378 -4.1708417 -4.1528058 -4.1603 -4.1650944 -4.1396236 -4.0925026 -4.0259056 -3.9962869 -4.0227404 -4.055481 -4.0760083 -4.0690479 -4.0655723 -4.0962915][-4.203187 -4.1773553 -4.1651211 -4.1741276 -4.1708035 -4.1224623 -4.0443339 -3.9454746 -3.9027658 -3.955101 -4.01747 -4.0557528 -4.0585265 -4.06526 -4.1028972][-4.2248244 -4.2065763 -4.1971369 -4.2000084 -4.1820426 -4.1114793 -4.0076184 -3.8839278 -3.836921 -3.921108 -4.006659 -4.0553703 -4.0622787 -4.0741272 -4.1161423][-4.2567086 -4.2452936 -4.2351251 -4.2277255 -4.1950927 -4.1166067 -4.0153985 -3.9061143 -3.8788478 -3.9697812 -4.0503016 -4.0894403 -4.0917125 -4.1040535 -4.1462526][-4.2791629 -4.2696738 -4.2553864 -4.238039 -4.200417 -4.13208 -4.0541477 -3.984525 -3.9830649 -4.055182 -4.109488 -4.1289535 -4.1241064 -4.1317425 -4.1674237][-4.2860918 -4.2689238 -4.246335 -4.2239118 -4.1911244 -4.1415687 -4.0914297 -4.0543942 -4.0652757 -4.1152124 -4.1499295 -4.1599517 -4.1530213 -4.1551638 -4.1793][-4.2840371 -4.2586708 -4.2301722 -4.2081428 -4.1863265 -4.1601725 -4.1368475 -4.1170406 -4.1201043 -4.1472654 -4.1732564 -4.1854072 -4.1790271 -4.1724625 -4.1839252][-4.2795086 -4.2499404 -4.2189527 -4.1986108 -4.1895852 -4.1854987 -4.1800475 -4.1639857 -4.1525464 -4.160183 -4.1818428 -4.1998453 -4.1977797 -4.1849432 -4.1849232][-4.2642832 -4.2313619 -4.1994948 -4.1820383 -4.1863012 -4.2001009 -4.2037916 -4.1870914 -4.1718049 -4.175643 -4.1972508 -4.2198749 -4.224215 -4.2111559 -4.2050939][-4.2446585 -4.207943 -4.1749358 -4.1646013 -4.1819715 -4.2055688 -4.2146053 -4.2010994 -4.1896071 -4.1982665 -4.2190361 -4.2420259 -4.2509942 -4.2428141 -4.2399454][-4.2371411 -4.2017317 -4.17257 -4.17019 -4.1948638 -4.2189107 -4.2306747 -4.2262955 -4.2235231 -4.2365642 -4.2535152 -4.2695317 -4.277926 -4.2762012 -4.2797618]]...]
INFO - root - 2017-12-06 05:35:58.243841: step 9410, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 56h:09m:03s remains)
INFO - root - 2017-12-06 05:36:04.739700: step 9420, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 59h:03m:40s remains)
INFO - root - 2017-12-06 05:36:11.218128: step 9430, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 57h:34m:50s remains)
INFO - root - 2017-12-06 05:36:17.612990: step 9440, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 57h:07m:47s remains)
INFO - root - 2017-12-06 05:36:23.990926: step 9450, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:01m:46s remains)
INFO - root - 2017-12-06 05:36:30.223161: step 9460, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 55h:47m:46s remains)
INFO - root - 2017-12-06 05:36:36.628508: step 9470, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:12m:22s remains)
INFO - root - 2017-12-06 05:36:43.192924: step 9480, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 60h:48m:37s remains)
INFO - root - 2017-12-06 05:36:49.583586: step 9490, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 57h:26m:42s remains)
INFO - root - 2017-12-06 05:36:56.116066: step 9500, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:40m:10s remains)
2017-12-06 05:36:57.504760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2266626 -4.2595811 -4.2850518 -4.2514043 -4.1665463 -4.070704 -4.0204539 -4.034584 -4.1274362 -4.2255139 -4.2723203 -4.2931046 -4.3030767 -4.3082223 -4.3046718][-4.2413583 -4.2648473 -4.2815371 -4.245923 -4.1581759 -4.0677724 -4.0211515 -4.0294571 -4.1085148 -4.2006326 -4.2540312 -4.2856288 -4.3065281 -4.3155336 -4.3140054][-4.2516942 -4.2602782 -4.2634764 -4.2273688 -4.1470184 -4.0703311 -4.0304537 -4.0363913 -4.1015787 -4.1773152 -4.2250977 -4.2614064 -4.2932835 -4.3085876 -4.3135538][-4.2399673 -4.2379293 -4.225081 -4.180479 -4.1142812 -4.061235 -4.0311217 -4.0407391 -4.0957427 -4.1526771 -4.1918073 -4.2325878 -4.2719307 -4.292479 -4.3030028][-4.2151842 -4.204042 -4.1739483 -4.1151996 -4.0539346 -4.0208035 -4.0048208 -4.0177922 -4.0677223 -4.116313 -4.1562009 -4.2030315 -4.2419133 -4.2656574 -4.2814732][-4.1890969 -4.1695457 -4.1245542 -4.05715 -4.0012603 -3.9867013 -3.9861708 -3.997669 -4.0322447 -4.0743647 -4.1194243 -4.1734042 -4.2137685 -4.2431169 -4.2645588][-4.1773267 -4.156642 -4.114532 -4.04998 -3.9965191 -3.9927757 -4.0008874 -4.0107937 -4.036469 -4.0706306 -4.1160231 -4.1740565 -4.2150979 -4.2428517 -4.266397][-4.1897697 -4.1747789 -4.1468148 -4.0954652 -4.0433073 -4.0365705 -4.0461054 -4.0549612 -4.07601 -4.106791 -4.1492481 -4.2078514 -4.2454276 -4.264379 -4.2770534][-4.2217407 -4.2157774 -4.2009034 -4.1636219 -4.1189189 -4.112165 -4.1208429 -4.1255355 -4.1387739 -4.1622744 -4.198081 -4.2475343 -4.2798691 -4.2930155 -4.2970576][-4.2536454 -4.2563758 -4.2504134 -4.220715 -4.1848788 -4.1851511 -4.1995482 -4.2016253 -4.2060456 -4.2189813 -4.2423038 -4.2749581 -4.2999034 -4.3108978 -4.3128557][-4.2671361 -4.27286 -4.2693796 -4.2460136 -4.2193923 -4.2239809 -4.2452936 -4.25515 -4.2554564 -4.26049 -4.2703838 -4.2859974 -4.3038321 -4.3125138 -4.3175306][-4.2662344 -4.2714534 -4.2712564 -4.2570448 -4.240314 -4.247375 -4.2696505 -4.2815671 -4.277761 -4.2787242 -4.2854338 -4.2909675 -4.300528 -4.307519 -4.3128018][-4.2717524 -4.2740812 -4.2739768 -4.2628675 -4.2486181 -4.2533913 -4.2687154 -4.27901 -4.2785087 -4.2774858 -4.2838697 -4.2900167 -4.2991409 -4.3068776 -4.3107352][-4.2874041 -4.2870903 -4.2853522 -4.2762203 -4.2637968 -4.2645216 -4.2723994 -4.2797351 -4.2820792 -4.2845564 -4.2896638 -4.2946563 -4.3019686 -4.3093481 -4.3135595][-4.3018045 -4.3007259 -4.2989264 -4.2924633 -4.28297 -4.2795992 -4.2816715 -4.2865286 -4.290906 -4.2953811 -4.3001285 -4.3045049 -4.3088627 -4.3134246 -4.3173251]]...]
INFO - root - 2017-12-06 05:37:04.030034: step 9510, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.675 sec/batch; 60h:36m:06s remains)
INFO - root - 2017-12-06 05:37:10.372116: step 9520, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 55h:42m:24s remains)
INFO - root - 2017-12-06 05:37:16.716511: step 9530, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.612 sec/batch; 54h:52m:35s remains)
INFO - root - 2017-12-06 05:37:23.262157: step 9540, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 57h:12m:45s remains)
INFO - root - 2017-12-06 05:37:29.582301: step 9550, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 57h:29m:20s remains)
INFO - root - 2017-12-06 05:37:35.909163: step 9560, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.648 sec/batch; 58h:07m:18s remains)
INFO - root - 2017-12-06 05:37:42.248011: step 9570, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.662 sec/batch; 59h:22m:33s remains)
INFO - root - 2017-12-06 05:37:48.764926: step 9580, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 58h:45m:45s remains)
INFO - root - 2017-12-06 05:37:55.050384: step 9590, loss = 2.08, batch loss = 2.02 (13.2 examples/sec; 0.607 sec/batch; 54h:24m:46s remains)
INFO - root - 2017-12-06 05:38:01.414628: step 9600, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 58h:08m:05s remains)
2017-12-06 05:38:01.991778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3000665 -4.3012753 -4.2979593 -4.2821674 -4.2581592 -4.2261119 -4.1951237 -4.1801524 -4.1769204 -4.176425 -4.1688581 -4.1583395 -4.1326828 -4.07833 -4.0310626][-4.280798 -4.2840753 -4.2851052 -4.2683606 -4.245542 -4.2123437 -4.1824808 -4.1735029 -4.1768374 -4.1787748 -4.1754375 -4.17204 -4.1556726 -4.1067262 -4.0617566][-4.2562485 -4.2648854 -4.2717376 -4.2569838 -4.2358551 -4.205193 -4.183351 -4.1822405 -4.1940966 -4.1990557 -4.2005253 -4.206748 -4.2048278 -4.1730762 -4.1398363][-4.2130718 -4.2303915 -4.2472239 -4.2377148 -4.2200823 -4.196959 -4.1884437 -4.1947112 -4.2121592 -4.2193584 -4.2245526 -4.2365246 -4.2461715 -4.2335973 -4.2157464][-4.1476207 -4.1746092 -4.2054253 -4.2060819 -4.1941075 -4.1789579 -4.182447 -4.193727 -4.2158332 -4.2272086 -4.2363505 -4.2510262 -4.268003 -4.2727666 -4.2689581][-4.0915656 -4.1258006 -4.165432 -4.1763511 -4.1704049 -4.1603737 -4.1669836 -4.1804843 -4.2074018 -4.225719 -4.2405677 -4.2585626 -4.2803497 -4.2945046 -4.2993178][-4.0880222 -4.113318 -4.1424403 -4.1527877 -4.14876 -4.1371851 -4.1399422 -4.1521654 -4.1825471 -4.2098856 -4.2330132 -4.2597771 -4.2877221 -4.3059916 -4.3139596][-4.1191821 -4.1262536 -4.1328511 -4.1306028 -4.1191778 -4.0973186 -4.0915108 -4.0997643 -4.1334267 -4.1703057 -4.2036991 -4.242806 -4.2821484 -4.3049803 -4.3167276][-4.1555624 -4.1445365 -4.1248245 -4.0988455 -4.068471 -4.0319781 -4.0164628 -4.0206227 -4.0635872 -4.1170139 -4.1630168 -4.2132163 -4.2619333 -4.291 -4.3073659][-4.1764679 -4.1529369 -4.11559 -4.0718489 -4.0274568 -3.9906857 -3.9776845 -3.9809902 -4.0287852 -4.0896878 -4.1403365 -4.1882567 -4.23453 -4.2667332 -4.2867031][-4.1961522 -4.1624117 -4.118886 -4.0791011 -4.0436859 -4.0236306 -4.023221 -4.0282931 -4.0665326 -4.119184 -4.1645985 -4.1989589 -4.2282066 -4.2524972 -4.2701874][-4.1974072 -4.1635666 -4.1267896 -4.1053314 -4.0923438 -4.0899396 -4.0975451 -4.1028757 -4.1276851 -4.1668725 -4.2022986 -4.2223787 -4.236958 -4.252265 -4.26341][-4.1951938 -4.168015 -4.14505 -4.1394486 -4.1429 -4.1518216 -4.1634593 -4.1683469 -4.1839218 -4.2131658 -4.2393503 -4.2504869 -4.2563815 -4.2619557 -4.264873][-4.2015367 -4.1849461 -4.180407 -4.1881237 -4.2010608 -4.2195339 -4.2343693 -4.2381697 -4.2461257 -4.2637577 -4.2786765 -4.2823358 -4.2814689 -4.2788286 -4.2717757][-4.2352438 -4.230093 -4.2367439 -4.2483072 -4.2628098 -4.2817574 -4.2936773 -4.2956471 -4.2991467 -4.3067613 -4.3113103 -4.3099079 -4.3046579 -4.2935743 -4.2762303]]...]
INFO - root - 2017-12-06 05:38:08.291438: step 9610, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 56h:45m:21s remains)
INFO - root - 2017-12-06 05:38:14.774174: step 9620, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 57h:38m:27s remains)
INFO - root - 2017-12-06 05:38:21.185579: step 9630, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 56h:44m:14s remains)
INFO - root - 2017-12-06 05:38:27.529316: step 9640, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 55h:48m:32s remains)
INFO - root - 2017-12-06 05:38:33.905812: step 9650, loss = 2.06, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 55h:57m:49s remains)
INFO - root - 2017-12-06 05:38:40.136193: step 9660, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 56h:51m:16s remains)
INFO - root - 2017-12-06 05:38:46.526773: step 9670, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 59h:23m:10s remains)
INFO - root - 2017-12-06 05:38:52.818960: step 9680, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.632 sec/batch; 56h:42m:43s remains)
INFO - root - 2017-12-06 05:38:59.303600: step 9690, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 58h:30m:58s remains)
INFO - root - 2017-12-06 05:39:05.718833: step 9700, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 58h:23m:24s remains)
2017-12-06 05:39:06.348193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2918153 -4.2918272 -4.2839918 -4.2740722 -4.277833 -4.287982 -4.29019 -4.2862177 -4.2890754 -4.2955294 -4.303679 -4.313643 -4.3219776 -4.3194852 -4.3139234][-4.2724724 -4.2671638 -4.2552347 -4.2404943 -4.2442327 -4.2587295 -4.2549777 -4.2471695 -4.253222 -4.265532 -4.282093 -4.3022971 -4.3164573 -4.3138814 -4.3073921][-4.2571049 -4.2436805 -4.2202134 -4.19926 -4.2028985 -4.2157321 -4.1994586 -4.1874251 -4.2026248 -4.2237978 -4.2511253 -4.2809734 -4.3001728 -4.3005753 -4.2946162][-4.2493773 -4.2256217 -4.1881356 -4.1586003 -4.1593714 -4.1619606 -4.12693 -4.10639 -4.1328077 -4.1721916 -4.2158284 -4.2575326 -4.2816119 -4.2863994 -4.282032][-4.2377543 -4.2075968 -4.1640387 -4.1301703 -4.1225929 -4.1062508 -4.0389824 -4.0054913 -4.050415 -4.1114683 -4.1733618 -4.23046 -4.2616882 -4.2734766 -4.2712245][-4.2365041 -4.2042809 -4.1630831 -4.1287217 -4.1085863 -4.0618167 -3.9495611 -3.9041617 -3.9840045 -4.076107 -4.1524491 -4.2155819 -4.2516894 -4.2684956 -4.2661471][-4.2450323 -4.2194662 -4.1821742 -4.1407986 -4.0944052 -4.0000625 -3.8265238 -3.7776816 -3.9180217 -4.0518956 -4.1395059 -4.206862 -4.2486463 -4.2679262 -4.2664123][-4.2376523 -4.226285 -4.2003031 -4.1542668 -4.0770011 -3.9356916 -3.705426 -3.6678383 -3.8724365 -4.0410151 -4.1377239 -4.2069297 -4.2501936 -4.2714772 -4.2701511][-4.2229657 -4.22651 -4.2152472 -4.1733956 -4.0901632 -3.9535615 -3.7532802 -3.7399423 -3.9320381 -4.0803361 -4.1638775 -4.222209 -4.2584748 -4.278131 -4.2768517][-4.20232 -4.2112703 -4.2134819 -4.1835895 -4.1123114 -4.0043759 -3.8543944 -3.8564103 -4.0105295 -4.126719 -4.1917124 -4.2387567 -4.2697988 -4.2855 -4.2834721][-4.1896052 -4.2057352 -4.2191825 -4.1990404 -4.1392965 -4.0519953 -3.9393408 -3.9514394 -4.077199 -4.1702867 -4.2222643 -4.2588205 -4.2818151 -4.2910008 -4.2880421][-4.1816244 -4.2060089 -4.2279792 -4.2186332 -4.1789231 -4.1178188 -4.0355639 -4.0479436 -4.1435575 -4.213717 -4.2508478 -4.2763624 -4.2924247 -4.2975225 -4.2935114][-4.1897359 -4.2190876 -4.246552 -4.2499862 -4.2331734 -4.1946006 -4.1351533 -4.1419687 -4.2082343 -4.2568703 -4.2828846 -4.2995868 -4.3059058 -4.304215 -4.2985787][-4.2045507 -4.2345948 -4.2616982 -4.2718563 -4.2722292 -4.25109 -4.21032 -4.2104073 -4.2524109 -4.286839 -4.3051553 -4.3130631 -4.309835 -4.30324 -4.2993374][-4.2247787 -4.2537994 -4.2768545 -4.2891817 -4.2966986 -4.2847195 -4.2580471 -4.2550797 -4.279129 -4.3039613 -4.31525 -4.3147044 -4.3052869 -4.297267 -4.2961187]]...]
INFO - root - 2017-12-06 05:39:12.516009: step 9710, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:43m:31s remains)
INFO - root - 2017-12-06 05:39:18.877816: step 9720, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.606 sec/batch; 54h:17m:32s remains)
INFO - root - 2017-12-06 05:39:25.335071: step 9730, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 58h:55m:05s remains)
INFO - root - 2017-12-06 05:39:31.837532: step 9740, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 57h:30m:50s remains)
INFO - root - 2017-12-06 05:39:38.178216: step 9750, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:19m:10s remains)
INFO - root - 2017-12-06 05:39:44.579879: step 9760, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.656 sec/batch; 58h:48m:52s remains)
INFO - root - 2017-12-06 05:39:50.871786: step 9770, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 57h:55m:01s remains)
INFO - root - 2017-12-06 05:39:57.301655: step 9780, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 57h:27m:01s remains)
INFO - root - 2017-12-06 05:40:03.746603: step 9790, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 57h:31m:06s remains)
INFO - root - 2017-12-06 05:40:10.187217: step 9800, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 58h:55m:54s remains)
2017-12-06 05:40:10.838373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2994981 -4.2968025 -4.2942605 -4.2903728 -4.2878056 -4.2882719 -4.2910814 -4.2952695 -4.2994461 -4.3017964 -4.2959046 -4.288425 -4.2871103 -4.2795215 -4.2695518][-4.2784872 -4.2724409 -4.2657557 -4.2574911 -4.2520475 -4.2534213 -4.261045 -4.2715435 -4.2821584 -4.2896633 -4.2849951 -4.2774506 -4.2726579 -4.2585278 -4.2449923][-4.2673793 -4.2587404 -4.24758 -4.23349 -4.2241011 -4.2266 -4.239059 -4.2554855 -4.2722015 -4.28516 -4.2834291 -4.2774343 -4.2697077 -4.2516041 -4.2352824][-4.2675047 -4.2588282 -4.2432723 -4.2219772 -4.2081857 -4.2095351 -4.2242212 -4.2430367 -4.2631774 -4.2814684 -4.2848887 -4.2835703 -4.2777696 -4.2580218 -4.236722][-4.2478151 -4.2369857 -4.2144375 -4.1840019 -4.162828 -4.1601629 -4.17384 -4.1924758 -4.2136559 -4.2377524 -4.251214 -4.2605033 -4.2640204 -4.2496948 -4.2287269][-4.2034082 -4.1910639 -4.1635289 -4.1247406 -4.0920982 -4.0793161 -4.0854511 -4.0974197 -4.1139512 -4.1417456 -4.1690006 -4.1940155 -4.2112985 -4.2097111 -4.1993532][-4.1843472 -4.1760106 -4.1522264 -4.11419 -4.0741243 -4.0487971 -4.0386152 -4.0347013 -4.0408621 -4.0694671 -4.1056719 -4.1385646 -4.1627841 -4.1705852 -4.1730394][-4.227747 -4.222127 -4.2067327 -4.18132 -4.1519556 -4.1289744 -4.1093287 -4.0910482 -4.0838442 -4.099617 -4.1236176 -4.1481576 -4.1661506 -4.1704941 -4.175849][-4.2669177 -4.2621622 -4.2538085 -4.241405 -4.2286062 -4.2174835 -4.2009072 -4.1795983 -4.1638203 -4.1623521 -4.1679516 -4.1804128 -4.191442 -4.1872315 -4.1868653][-4.26873 -4.265913 -4.2620254 -4.256927 -4.2537532 -4.250401 -4.239922 -4.221807 -4.2057385 -4.1959343 -4.1923165 -4.1970086 -4.204051 -4.1958404 -4.1908479][-4.2703114 -4.2675133 -4.2638044 -4.2575965 -4.2537313 -4.2510595 -4.244678 -4.2326722 -4.2213163 -4.2131305 -4.2103543 -4.2119408 -4.2145176 -4.203867 -4.1975789][-4.2862144 -4.2817607 -4.275701 -4.2644129 -4.2528625 -4.2441177 -4.2374477 -4.2306228 -4.2243023 -4.2204084 -4.2209864 -4.2201686 -4.2162042 -4.2023807 -4.1975794][-4.3047323 -4.2996697 -4.2916012 -4.2771606 -4.2588148 -4.2418222 -4.2301121 -4.2246246 -4.2223048 -4.2220507 -4.2230558 -4.2190251 -4.210372 -4.1953945 -4.1927323][-4.32018 -4.3168306 -4.3101149 -4.2975559 -4.2774091 -4.2542076 -4.2368016 -4.2280512 -4.224268 -4.2239547 -4.2239394 -4.2177281 -4.2057176 -4.1900282 -4.1888156][-4.3324695 -4.3327265 -4.3307467 -4.3234715 -4.3070593 -4.2837029 -4.2629833 -4.2482462 -4.2399197 -4.2379842 -4.2370491 -4.2309484 -4.2179518 -4.2031169 -4.2013283]]...]
INFO - root - 2017-12-06 05:40:17.321817: step 9810, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 57h:34m:02s remains)
INFO - root - 2017-12-06 05:40:25.306770: step 9820, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 59h:11m:43s remains)
INFO - root - 2017-12-06 05:40:33.768262: step 9830, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 76h:39m:16s remains)
INFO - root - 2017-12-06 05:40:42.362693: step 9840, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 78h:17m:33s remains)
INFO - root - 2017-12-06 05:40:50.788845: step 9850, loss = 2.04, batch loss = 1.98 (10.1 examples/sec; 0.794 sec/batch; 71h:10m:19s remains)
INFO - root - 2017-12-06 05:40:59.037721: step 9860, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 73h:18m:38s remains)
INFO - root - 2017-12-06 05:41:07.522592: step 9870, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.861 sec/batch; 77h:09m:01s remains)
INFO - root - 2017-12-06 05:41:15.974285: step 9880, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 78h:50m:33s remains)
INFO - root - 2017-12-06 05:41:24.439483: step 9890, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 74h:35m:38s remains)
INFO - root - 2017-12-06 05:41:32.902291: step 9900, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 75h:33m:32s remains)
2017-12-06 05:41:33.620840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2493367 -4.2496376 -4.2590885 -4.2667036 -4.2712317 -4.2740407 -4.2773895 -4.2772822 -4.27393 -4.2718453 -4.2691684 -4.2546482 -4.2364497 -4.2179179 -4.1971583][-4.2461882 -4.2499318 -4.2602453 -4.2626886 -4.2593942 -4.2551074 -4.2531147 -4.2486715 -4.2441239 -4.2422633 -4.2405572 -4.2266436 -4.2076063 -4.1831579 -4.1548343][-4.2417564 -4.2495384 -4.2542257 -4.2462816 -4.236393 -4.2273765 -4.2210293 -4.2107511 -4.2056203 -4.2102442 -4.2141638 -4.2055731 -4.1877146 -4.1575184 -4.1223712][-4.2432003 -4.2514272 -4.2450314 -4.2270126 -4.212028 -4.2034855 -4.1931024 -4.1775217 -4.1706486 -4.1828909 -4.1957417 -4.1963429 -4.184886 -4.1532874 -4.1149712][-4.2470264 -4.2549605 -4.241497 -4.2135043 -4.1922436 -4.1805749 -4.1667566 -4.1475778 -4.13849 -4.1571703 -4.1791577 -4.1910458 -4.1898704 -4.1624193 -4.123291][-4.242383 -4.252306 -4.2328076 -4.1956124 -4.1687083 -4.1500096 -4.1316428 -4.110714 -4.1032758 -4.1281815 -4.1581206 -4.1776628 -4.1865134 -4.1686096 -4.1331911][-4.2259707 -4.2318611 -4.2081938 -4.1700125 -4.1387486 -4.1115527 -4.0890808 -4.0725284 -4.0726514 -4.1016879 -4.1355181 -4.1598954 -4.1713147 -4.1610694 -4.1347876][-4.1969 -4.199203 -4.1764545 -4.1420279 -4.1078248 -4.0753922 -4.0502505 -4.0416789 -4.0538893 -4.0861015 -4.12083 -4.1461263 -4.1531191 -4.1438718 -4.1272492][-4.1777468 -4.170537 -4.1432605 -4.1086869 -4.0766444 -4.0457196 -4.02262 -4.0224314 -4.0402622 -4.074214 -4.1102247 -4.1358428 -4.1408019 -4.1307316 -4.1183906][-4.16859 -4.156527 -4.1231241 -4.0817761 -4.0513391 -4.0293612 -4.0192523 -4.0262914 -4.0452137 -4.0790849 -4.110333 -4.1309171 -4.1358657 -4.1278019 -4.11279][-4.1637931 -4.1495438 -4.1092596 -4.0644741 -4.0393696 -4.0330992 -4.0397391 -4.0558605 -4.0728478 -4.0937133 -4.1134744 -4.1325393 -4.1410933 -4.1348467 -4.1168933][-4.1544318 -4.144187 -4.1075983 -4.0667539 -4.0470271 -4.0507889 -4.0732417 -4.0946059 -4.1041946 -4.1102939 -4.1234045 -4.146121 -4.1607533 -4.1541638 -4.1288519][-4.150744 -4.1417789 -4.1118031 -4.0819154 -4.0724168 -4.08502 -4.113234 -4.1356463 -4.1449032 -4.1481791 -4.158566 -4.178565 -4.1872473 -4.1770096 -4.1503315][-4.1822042 -4.17168 -4.1492867 -4.1305218 -4.1304274 -4.1471577 -4.1709833 -4.1909914 -4.201004 -4.2092595 -4.2185068 -4.2270184 -4.2272463 -4.2170167 -4.1972551][-4.2374024 -4.2301044 -4.2174864 -4.2106471 -4.2158866 -4.2311206 -4.2442484 -4.2549343 -4.262639 -4.2706127 -4.2756491 -4.2755322 -4.2713661 -4.265172 -4.2531662]]...]
INFO - root - 2017-12-06 05:41:41.931458: step 9910, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 72h:49m:54s remains)
INFO - root - 2017-12-06 05:41:50.429220: step 9920, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 77h:00m:34s remains)
INFO - root - 2017-12-06 05:41:58.898180: step 9930, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 75h:15m:04s remains)
INFO - root - 2017-12-06 05:42:07.493628: step 9940, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 79h:35m:21s remains)
INFO - root - 2017-12-06 05:42:16.149992: step 9950, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 75h:30m:32s remains)
INFO - root - 2017-12-06 05:42:24.703375: step 9960, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 79h:01m:52s remains)
INFO - root - 2017-12-06 05:42:33.361261: step 9970, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 76h:12m:48s remains)
INFO - root - 2017-12-06 05:42:41.627799: step 9980, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 78h:30m:53s remains)
INFO - root - 2017-12-06 05:42:50.191405: step 9990, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 75h:57m:59s remains)
INFO - root - 2017-12-06 05:42:58.820100: step 10000, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 76h:16m:22s remains)
2017-12-06 05:42:59.678830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1795373 -4.179389 -4.1875634 -4.2006564 -4.2081776 -4.20698 -4.200263 -4.1827955 -4.1631689 -4.1536055 -4.1463585 -4.1509047 -4.1718388 -4.1990695 -4.2179809][-4.1868987 -4.1925745 -4.20799 -4.225431 -4.235919 -4.2367253 -4.2303324 -4.2135897 -4.1963739 -4.1891212 -4.1843381 -4.188004 -4.1985664 -4.2135739 -4.2267489][-4.200191 -4.211813 -4.2351136 -4.2538 -4.2633462 -4.2612648 -4.2530441 -4.2389803 -4.2286634 -4.2292361 -4.2331648 -4.2389917 -4.2436957 -4.2500658 -4.2572389][-4.2113013 -4.2292557 -4.2533622 -4.2713318 -4.2758021 -4.2643342 -4.2504563 -4.2356396 -4.22875 -4.2358136 -4.2508564 -4.262105 -4.2682276 -4.2751989 -4.2792225][-4.2087278 -4.2261186 -4.2458358 -4.2597632 -4.2551489 -4.2316494 -4.2056785 -4.1799474 -4.1651797 -4.1821704 -4.216291 -4.2437525 -4.2607169 -4.2724586 -4.2785869][-4.1944871 -4.2079124 -4.2206488 -4.2244525 -4.2036366 -4.1600704 -4.1112776 -4.057591 -4.035533 -4.0782595 -4.137897 -4.1897249 -4.2220411 -4.2440143 -4.2591319][-4.1875429 -4.1924887 -4.1852288 -4.1616793 -4.1153989 -4.04808 -3.96729 -3.8698645 -3.8470287 -3.9458604 -4.0522404 -4.1306319 -4.1745882 -4.20656 -4.2369928][-4.1914725 -4.1886129 -4.1598859 -4.1046257 -4.0283442 -3.9323666 -3.8079753 -3.652739 -3.6447716 -3.8156595 -3.958286 -4.0481091 -4.1043062 -4.15756 -4.210834][-4.1998515 -4.2022624 -4.172328 -4.1112609 -4.0285845 -3.9255834 -3.7991462 -3.6698997 -3.68485 -3.8278384 -3.9301782 -3.9924772 -4.0449028 -4.1086607 -4.17779][-4.1827903 -4.1977429 -4.1914086 -4.1622906 -4.11184 -4.0402508 -3.9595604 -3.8920317 -3.9056311 -3.9711277 -4.0038433 -4.0225697 -4.0526533 -4.1074514 -4.168355][-4.1439362 -4.172636 -4.19154 -4.1981349 -4.1843061 -4.149631 -4.1046839 -4.068666 -4.0668435 -4.0806332 -4.0796885 -4.0812387 -4.1039987 -4.1519847 -4.1979303][-4.1005983 -4.1436448 -4.1824193 -4.2071719 -4.2118731 -4.1941791 -4.1645422 -4.1384048 -4.1278319 -4.1197944 -4.1082683 -4.1118884 -4.143167 -4.1969824 -4.2392559][-4.0930328 -4.1468558 -4.1914735 -4.215663 -4.2177305 -4.1972566 -4.16055 -4.1345973 -4.1223197 -4.1104279 -4.1010861 -4.1138945 -4.1548514 -4.2145247 -4.259203][-4.1172414 -4.1676826 -4.2070651 -4.2203074 -4.211544 -4.180275 -4.1369481 -4.1103139 -4.1022534 -4.0933971 -4.0893893 -4.1061745 -4.1453195 -4.2034712 -4.2493439][-4.1342854 -4.1805143 -4.2169352 -4.2238774 -4.2091632 -4.1759439 -4.1324596 -4.1081557 -4.1028132 -4.0953236 -4.0883613 -4.1005239 -4.1376948 -4.1893935 -4.2312646]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-use_nesterov:False/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 05:43:11.894614: step 10010, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 76h:26m:57s remains)
INFO - root - 2017-12-06 05:43:19.649842: step 10020, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 77h:29m:03s remains)
INFO - root - 2017-12-06 05:43:28.195815: step 10030, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 75h:23m:47s remains)
INFO - root - 2017-12-06 05:43:36.617883: step 10040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:02m:52s remains)
INFO - root - 2017-12-06 05:43:45.203192: step 10050, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 0.803 sec/batch; 71h:55m:40s remains)
INFO - root - 2017-12-06 05:43:53.757003: step 10060, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:34m:46s remains)
INFO - root - 2017-12-06 05:44:02.371936: step 10070, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 78h:13m:09s remains)
INFO - root - 2017-12-06 05:44:10.899633: step 10080, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 77h:10m:17s remains)
INFO - root - 2017-12-06 05:44:19.490577: step 10090, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 74h:53m:58s remains)
INFO - root - 2017-12-06 05:44:28.107893: step 10100, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:00m:35s remains)
2017-12-06 05:44:28.880059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2223935 -4.2210555 -4.2410226 -4.2811952 -4.3127222 -4.3323584 -4.3383818 -4.3303642 -4.3174191 -4.299068 -4.2865567 -4.2888794 -4.3021326 -4.313695 -4.3177352][-4.2151947 -4.205987 -4.2185183 -4.2535377 -4.2858996 -4.3097162 -4.3229551 -4.3196683 -4.3071089 -4.2856793 -4.2688408 -4.2687917 -4.2815814 -4.2947392 -4.3020535][-4.21066 -4.190815 -4.1899705 -4.2115326 -4.2398586 -4.2660203 -4.2874732 -4.2912989 -4.2842088 -4.2660942 -4.2499733 -4.2475824 -4.2561278 -4.2683463 -4.2811451][-4.2063928 -4.1773953 -4.1614733 -4.1646366 -4.1801009 -4.1998529 -4.2203426 -4.2312465 -4.2360334 -4.2318139 -4.2249789 -4.2249174 -4.2288871 -4.2392735 -4.2561684][-4.1986318 -4.1600666 -4.1311383 -4.1197853 -4.1165957 -4.116962 -4.1231232 -4.1352725 -4.1559362 -4.1746726 -4.1874208 -4.1990933 -4.2087226 -4.2228851 -4.2414989][-4.196806 -4.147007 -4.1045685 -4.0784125 -4.051796 -4.0227933 -4.0013933 -4.0076818 -4.0486889 -4.100255 -4.1428132 -4.1780558 -4.205461 -4.2256708 -4.2405486][-4.2036424 -4.146728 -4.0936813 -4.0515294 -3.9997082 -3.9337568 -3.8735905 -3.8686991 -3.9348602 -4.0240145 -4.1008763 -4.1647835 -4.213438 -4.2402492 -4.2495732][-4.2126045 -4.1576581 -4.1036305 -4.0530605 -3.9836187 -3.8872848 -3.7902761 -3.7702525 -3.8535743 -3.9684944 -4.0699282 -4.1537561 -4.2161593 -4.2469835 -4.2534413][-4.2099528 -4.1662941 -4.1247449 -4.0830569 -4.0223761 -3.935288 -3.8434079 -3.8132646 -3.8720515 -3.96922 -4.0649824 -4.1489058 -4.2117925 -4.2400966 -4.2442904][-4.1872864 -4.1585665 -4.1389656 -4.121809 -4.089653 -4.0392995 -3.9799724 -3.9511793 -3.9734473 -4.0315962 -4.0980287 -4.1598539 -4.2059402 -4.2210693 -4.2194381][-4.15818 -4.1410289 -4.1424608 -4.15401 -4.1511316 -4.1344767 -4.1054745 -4.0850511 -4.0878391 -4.1156387 -4.147747 -4.1756926 -4.19495 -4.1918964 -4.1794915][-4.1386476 -4.1276 -4.1430712 -4.1769428 -4.1957912 -4.1989179 -4.1895437 -4.1771975 -4.1734781 -4.1817927 -4.1894011 -4.1916404 -4.1877294 -4.1670032 -4.1396456][-4.1471109 -4.1425328 -4.1660938 -4.2106519 -4.2369866 -4.2442961 -4.2380681 -4.2257271 -4.2163782 -4.2125053 -4.2055521 -4.1916676 -4.1754608 -4.1490645 -4.1179032][-4.1900487 -4.1916652 -4.2157683 -4.2571216 -4.2774205 -4.275785 -4.259244 -4.2378902 -4.2179055 -4.1998715 -4.1822853 -4.1629834 -4.1451035 -4.1265354 -4.1094413][-4.2442493 -4.2470479 -4.2675905 -4.2992477 -4.3090148 -4.2980371 -4.2752705 -4.2460608 -4.2126551 -4.1777687 -4.1510277 -4.13091 -4.1168747 -4.11345 -4.1188774]]...]
INFO - root - 2017-12-06 05:44:37.380166: step 10110, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 77h:48m:56s remains)
INFO - root - 2017-12-06 05:44:45.822336: step 10120, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 75h:28m:52s remains)
INFO - root - 2017-12-06 05:44:54.363269: step 10130, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 75h:44m:22s remains)
INFO - root - 2017-12-06 05:45:02.974767: step 10140, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:49m:20s remains)
INFO - root - 2017-12-06 05:45:11.622680: step 10150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 77h:04m:31s remains)
INFO - root - 2017-12-06 05:45:20.334936: step 10160, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 77h:19m:54s remains)
INFO - root - 2017-12-06 05:45:29.003170: step 10170, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 76h:51m:35s remains)
INFO - root - 2017-12-06 05:45:37.544389: step 10180, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 79h:38m:45s remains)
INFO - root - 2017-12-06 05:45:46.306434: step 10190, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 79h:49m:37s remains)
INFO - root - 2017-12-06 05:45:54.779706: step 10200, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 76h:51m:30s remains)
2017-12-06 05:45:55.587513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3004622 -4.3035278 -4.3095345 -4.3166866 -4.3244882 -4.3265877 -4.3241668 -4.321094 -4.3192677 -4.3189425 -4.3219619 -4.328012 -4.3308015 -4.3286247 -4.323][-4.312294 -4.3181205 -4.32377 -4.3296008 -4.3339844 -4.3286619 -4.3161836 -4.3042607 -4.298542 -4.2981806 -4.3024206 -4.3087115 -4.31266 -4.3117251 -4.3057089][-4.3115921 -4.3185706 -4.3239579 -4.3278785 -4.327126 -4.3129072 -4.2895737 -4.2685809 -4.2616711 -4.2671056 -4.2779679 -4.2860155 -4.2914538 -4.2898154 -4.2828817][-4.29947 -4.3101335 -4.3167772 -4.3176036 -4.3091631 -4.2833247 -4.2461777 -4.2145643 -4.2074976 -4.2241597 -4.2470241 -4.2600555 -4.2666183 -4.2627769 -4.2551594][-4.2853947 -4.2988224 -4.3024874 -4.2939138 -4.2717843 -4.2305632 -4.176589 -4.135149 -4.1362476 -4.17326 -4.2154989 -4.2399344 -4.2491674 -4.2426057 -4.2323413][-4.2730265 -4.2845812 -4.280386 -4.2575707 -4.2169209 -4.1568089 -4.0840735 -4.037374 -4.058847 -4.1254692 -4.1920981 -4.2311325 -4.2444038 -4.2347493 -4.2195611][-4.2652578 -4.2715831 -4.257513 -4.2209187 -4.1627603 -4.0882525 -4.0067272 -3.9668529 -4.01575 -4.1067886 -4.1889653 -4.2349586 -4.2468228 -4.2319083 -4.2120743][-4.2646341 -4.2642303 -4.2415104 -4.1982532 -4.1352844 -4.0674262 -4.0033865 -3.985738 -4.0465155 -4.1364822 -4.2122149 -4.2508974 -4.2520862 -4.2321677 -4.2135568][-4.277998 -4.2723265 -4.2467089 -4.2060695 -4.15437 -4.110167 -4.0793953 -4.0820379 -4.1315727 -4.19668 -4.2489843 -4.2715588 -4.2602096 -4.2377472 -4.2233214][-4.2977991 -4.2911158 -4.2714705 -4.2400346 -4.2033157 -4.1796961 -4.1734347 -4.1840086 -4.2154922 -4.2529736 -4.280745 -4.2889571 -4.2727795 -4.2541614 -4.2433348][-4.3087382 -4.3034329 -4.291985 -4.2712574 -4.2496247 -4.2395949 -4.2442594 -4.25537 -4.2728758 -4.2928925 -4.30642 -4.3084273 -4.2967062 -4.2858882 -4.2751827][-4.3087997 -4.3069415 -4.3017635 -4.2894659 -4.277638 -4.2724771 -4.2770958 -4.2861576 -4.2969928 -4.3117895 -4.3235092 -4.3298378 -4.326355 -4.322607 -4.3110328][-4.3046727 -4.3043084 -4.2997108 -4.2884469 -4.27791 -4.2743516 -4.2795472 -4.2888427 -4.2975645 -4.3108211 -4.3250818 -4.3378134 -4.3415418 -4.3426638 -4.3346119][-4.293757 -4.2881093 -4.2759233 -4.2572665 -4.2398205 -4.2346797 -4.2419944 -4.2539344 -4.2655826 -4.2827487 -4.3034139 -4.32344 -4.3321738 -4.3356843 -4.3328309][-4.2741923 -4.2559919 -4.2282958 -4.1975393 -4.1710629 -4.1642485 -4.176599 -4.1976142 -4.2182646 -4.2432103 -4.2699018 -4.2951961 -4.308537 -4.314949 -4.3173504]]...]
INFO - root - 2017-12-06 05:46:03.495842: step 10210, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 75h:42m:28s remains)
INFO - root - 2017-12-06 05:46:11.978684: step 10220, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 79h:11m:04s remains)
INFO - root - 2017-12-06 05:46:20.561557: step 10230, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.858 sec/batch; 76h:50m:47s remains)
INFO - root - 2017-12-06 05:46:28.986671: step 10240, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 77h:59m:43s remains)
INFO - root - 2017-12-06 05:46:37.572163: step 10250, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 78h:35m:20s remains)
INFO - root - 2017-12-06 05:46:46.130616: step 10260, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 77h:12m:33s remains)
INFO - root - 2017-12-06 05:46:54.804950: step 10270, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 78h:43m:47s remains)
INFO - root - 2017-12-06 05:47:03.544162: step 10280, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 76h:32m:59s remains)
INFO - root - 2017-12-06 05:47:12.008360: step 10290, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 77h:09m:28s remains)
INFO - root - 2017-12-06 05:47:20.805874: step 10300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 78h:09m:50s remains)
2017-12-06 05:47:21.666626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.281054 -4.2849936 -4.2894406 -4.2938581 -4.2960062 -4.2974482 -4.2988048 -4.29818 -4.2934427 -4.2815347 -4.26536 -4.2528658 -4.2505589 -4.2621918 -4.2789526][-4.2682343 -4.2703233 -4.2725034 -4.2766938 -4.2809863 -4.2837605 -4.2869086 -4.2878113 -4.2829952 -4.2694311 -4.2537313 -4.2448878 -4.2464495 -4.2598519 -4.2781787][-4.2592549 -4.2599034 -4.2624078 -4.2676044 -4.2718468 -4.2719932 -4.2726154 -4.2738466 -4.2715144 -4.2621751 -4.2519493 -4.2493844 -4.2545271 -4.2681942 -4.2831769][-4.2484312 -4.24689 -4.2506447 -4.2569122 -4.259263 -4.2547846 -4.2500625 -4.2504425 -4.2539678 -4.2533112 -4.250658 -4.2509656 -4.2544179 -4.2660341 -4.2806196][-4.2258673 -4.2223711 -4.2287173 -4.2385588 -4.2407203 -4.2315073 -4.2203212 -4.2203383 -4.2335873 -4.2470932 -4.2531209 -4.2535715 -4.2501478 -4.2548385 -4.2663922][-4.1834664 -4.1821957 -4.19454 -4.2070756 -4.2054567 -4.184618 -4.1594238 -4.1583443 -4.1863017 -4.2214637 -4.2441483 -4.2484984 -4.2356963 -4.2307086 -4.24133][-4.1284738 -4.1408234 -4.1652226 -4.1810331 -4.1696334 -4.1244659 -4.0694346 -4.0558686 -4.104166 -4.1704569 -4.2133889 -4.2239728 -4.2111821 -4.203968 -4.2201276][-4.0765009 -4.1062593 -4.1516757 -4.1796308 -4.1617813 -4.09378 -4.0068789 -3.969805 -4.0270157 -4.1150513 -4.1780434 -4.2009463 -4.1969085 -4.196363 -4.2179561][-4.0528593 -4.093586 -4.15661 -4.2016883 -4.1945338 -4.1311808 -4.0456228 -3.999615 -4.0382013 -4.1109219 -4.172986 -4.2020864 -4.20923 -4.2199988 -4.2459769][-4.0722227 -4.1164093 -4.1799116 -4.2294559 -4.2332144 -4.1893578 -4.1304908 -4.0949645 -4.1129818 -4.1548586 -4.1988306 -4.2244477 -4.2375045 -4.256031 -4.2823138][-4.1168885 -4.15727 -4.2123232 -4.25847 -4.2664709 -4.2358 -4.1971703 -4.1731453 -4.1779866 -4.1930008 -4.2146506 -4.2326818 -4.2488184 -4.2733154 -4.3012905][-4.1486082 -4.1856723 -4.2354693 -4.2790208 -4.28919 -4.2686954 -4.2425952 -4.22501 -4.2191849 -4.2112551 -4.2117591 -4.2227063 -4.2415414 -4.2700005 -4.2992415][-4.1565881 -4.1912508 -4.2386832 -4.2838073 -4.3007417 -4.2912831 -4.2762022 -4.2642503 -4.2544847 -4.2375264 -4.225389 -4.2295275 -4.2452679 -4.2681689 -4.2946343][-4.1559248 -4.1839585 -4.2286086 -4.2772593 -4.3021007 -4.3022623 -4.2973456 -4.2933588 -4.2870412 -4.2729168 -4.2565536 -4.2514482 -4.2580886 -4.2723 -4.294456][-4.1561379 -4.1773386 -4.2195134 -4.2679272 -4.2981076 -4.3066149 -4.3083596 -4.304915 -4.2977977 -4.2845421 -4.2670283 -4.256259 -4.2557807 -4.2666922 -4.2901559]]...]
INFO - root - 2017-12-06 05:47:30.272314: step 10310, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 76h:02m:06s remains)
INFO - root - 2017-12-06 05:47:38.925048: step 10320, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 77h:59m:09s remains)
INFO - root - 2017-12-06 05:47:47.369074: step 10330, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 74h:28m:43s remains)
INFO - root - 2017-12-06 05:47:56.031213: step 10340, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 78h:30m:02s remains)
INFO - root - 2017-12-06 05:48:04.759621: step 10350, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 76h:41m:18s remains)
INFO - root - 2017-12-06 05:48:13.388865: step 10360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 77h:43m:21s remains)
INFO - root - 2017-12-06 05:48:21.925681: step 10370, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 76h:09m:13s remains)
INFO - root - 2017-12-06 05:48:30.447117: step 10380, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 75h:54m:21s remains)
INFO - root - 2017-12-06 05:48:38.983495: step 10390, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 78h:38m:45s remains)
INFO - root - 2017-12-06 05:48:47.636570: step 10400, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 79h:42m:44s remains)
2017-12-06 05:48:50.517433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3410835 -4.3398757 -4.3267369 -4.3016915 -4.2741446 -4.2542968 -4.2511554 -4.2708774 -4.3004689 -4.3228116 -4.335618 -4.34098 -4.3413739 -4.33943 -4.3379693][-4.3430138 -4.3386159 -4.3211389 -4.2920613 -4.261035 -4.2406068 -4.2398405 -4.2634344 -4.2968082 -4.3218675 -4.3370566 -4.3430347 -4.3429627 -4.3407168 -4.339119][-4.3437572 -4.3352575 -4.3130903 -4.279891 -4.2443156 -4.2219815 -4.223649 -4.2517319 -4.289494 -4.31778 -4.3352933 -4.3432493 -4.3436813 -4.3415084 -4.3399577][-4.3406234 -4.3275175 -4.3000708 -4.2599368 -4.2187676 -4.195693 -4.2008791 -4.233561 -4.2767234 -4.3099122 -4.3314471 -4.3421712 -4.3443456 -4.3431773 -4.3419595][-4.32346 -4.3035336 -4.2650194 -4.2135344 -4.1679673 -4.1470127 -4.1572914 -4.194747 -4.2441025 -4.2862496 -4.3167739 -4.3355894 -4.3433175 -4.345171 -4.34452][-4.2860746 -4.2554941 -4.20244 -4.1406379 -4.0952969 -4.0815797 -4.0973539 -4.1366572 -4.1884828 -4.2391877 -4.2822733 -4.3129539 -4.3310204 -4.3402667 -4.3429346][-4.22968 -4.18684 -4.1222143 -4.0589237 -4.0240951 -4.0221415 -4.0419359 -4.0767326 -4.1215787 -4.1775908 -4.2354488 -4.2812929 -4.3118114 -4.3297544 -4.3367343][-4.1761475 -4.1240511 -4.05561 -3.9993248 -3.9787085 -3.9877253 -4.00914 -4.0327625 -4.0618019 -4.1159706 -4.1828485 -4.2427731 -4.2873278 -4.3158092 -4.32913][-4.1510406 -4.0992217 -4.0379515 -3.992806 -3.9820185 -3.9960356 -4.0129342 -4.0229516 -4.0380783 -4.0851278 -4.1512394 -4.2171969 -4.2705975 -4.3062658 -4.3251615][-4.1470218 -4.1021528 -4.0547185 -4.0227227 -4.0186849 -4.0322247 -4.0421619 -4.0406237 -4.045619 -4.0855727 -4.145865 -4.2105136 -4.2669978 -4.3056316 -4.3259258][-4.1607213 -4.126462 -4.0933752 -4.0722737 -4.0710697 -4.0804744 -4.0832834 -4.073791 -4.0731773 -4.1043797 -4.155252 -4.2140145 -4.2693496 -4.3080087 -4.3275366][-4.1934195 -4.170866 -4.15211 -4.1417589 -4.1427956 -4.150403 -4.1507163 -4.1377463 -4.1305609 -4.1495867 -4.1868467 -4.2333 -4.2790384 -4.3122616 -4.3281455][-4.2310014 -4.2203112 -4.2135563 -4.21276 -4.2169251 -4.2226276 -4.2226915 -4.2120519 -4.2013745 -4.2091517 -4.2335339 -4.2658577 -4.2981153 -4.3207994 -4.3289766][-4.2607865 -4.2571397 -4.258111 -4.263895 -4.2709541 -4.2757168 -4.2764378 -4.2696962 -4.2592282 -4.2603488 -4.2754087 -4.2964168 -4.3164377 -4.3289738 -4.3296375][-4.286963 -4.2854886 -4.2884703 -4.2940035 -4.2993565 -4.3028049 -4.3037086 -4.2996616 -4.2927656 -4.2920327 -4.3001485 -4.3127766 -4.3242741 -4.3296003 -4.3264647]]...]
INFO - root - 2017-12-06 05:48:59.015381: step 10410, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 76h:37m:34s remains)
INFO - root - 2017-12-06 05:49:07.508922: step 10420, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 75h:33m:55s remains)
INFO - root - 2017-12-06 05:49:16.204359: step 10430, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 78h:01m:57s remains)
INFO - root - 2017-12-06 05:49:24.805824: step 10440, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 78h:01m:05s remains)
INFO - root - 2017-12-06 05:49:33.402114: step 10450, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 78h:39m:42s remains)
INFO - root - 2017-12-06 05:49:41.933463: step 10460, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 75h:43m:00s remains)
INFO - root - 2017-12-06 05:49:50.497945: step 10470, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 78h:05m:20s remains)
INFO - root - 2017-12-06 05:49:59.024942: step 10480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 76h:05m:07s remains)
INFO - root - 2017-12-06 05:50:07.436546: step 10490, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 78h:42m:39s remains)
INFO - root - 2017-12-06 05:50:16.086080: step 10500, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 75h:38m:00s remains)
2017-12-06 05:50:16.851651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2896757 -4.2714648 -4.2531672 -4.2497034 -4.2521882 -4.2465935 -4.2296429 -4.2059126 -4.1859937 -4.1813736 -4.191596 -4.2119737 -4.2326355 -4.2571483 -4.28542][-4.2626624 -4.2396278 -4.21383 -4.204082 -4.2089634 -4.2156067 -4.2213426 -4.2215176 -4.2187963 -4.2201004 -4.2277822 -4.2372093 -4.245151 -4.2613025 -4.2865591][-4.2327547 -4.2154188 -4.1896739 -4.1766477 -4.1809373 -4.1887827 -4.202776 -4.21739 -4.2290473 -4.2387691 -4.248209 -4.2512593 -4.2492085 -4.2582603 -4.2817159][-4.2179413 -4.2096796 -4.1891723 -4.1774912 -4.1797523 -4.177392 -4.1784134 -4.1873841 -4.2015719 -4.2174621 -4.2335043 -4.2388325 -4.2366247 -4.2466855 -4.2730803][-4.2234578 -4.2165318 -4.1979995 -4.1862335 -4.1840506 -4.1671371 -4.1451964 -4.1402354 -4.1519794 -4.1725965 -4.1938133 -4.2064972 -4.2131238 -4.2307477 -4.2621841][-4.2275429 -4.2178335 -4.2002082 -4.1884604 -4.1794257 -4.1514115 -4.1124072 -4.093401 -4.0984125 -4.1189942 -4.1452103 -4.1673689 -4.1875954 -4.217442 -4.2544374][-4.2174788 -4.2031822 -4.1861067 -4.1749492 -4.161386 -4.1313748 -4.0865383 -4.0585489 -4.0582924 -4.0786848 -4.1088524 -4.1394906 -4.1738381 -4.2157125 -4.2561007][-4.2010708 -4.1808395 -4.16069 -4.1475973 -4.132453 -4.1075788 -4.0720239 -4.0519347 -4.055728 -4.0748529 -4.1053348 -4.1398082 -4.1809673 -4.2259326 -4.2646403][-4.1892395 -4.16525 -4.1421313 -4.1237073 -4.1031609 -4.0826941 -4.065434 -4.0667362 -4.0844312 -4.1047182 -4.1304474 -4.1628261 -4.2013559 -4.2416363 -4.2746143][-4.18918 -4.1661682 -4.1410666 -4.1159315 -4.0861416 -4.0679226 -4.06861 -4.0904164 -4.1204915 -4.1443825 -4.1653123 -4.1941848 -4.2281761 -4.2603216 -4.2860165][-4.1944785 -4.1797543 -4.1598654 -4.1323252 -4.0968943 -4.0785251 -4.088171 -4.1190157 -4.156013 -4.1840105 -4.2052016 -4.2322111 -4.2581067 -4.2788444 -4.29618][-4.20854 -4.2070389 -4.197958 -4.1762886 -4.1453042 -4.1274667 -4.1338625 -4.1576796 -4.1899838 -4.2177358 -4.2385068 -4.2615891 -4.2774463 -4.2872844 -4.2996521][-4.2346969 -4.2419186 -4.2406921 -4.2281661 -4.2074451 -4.189249 -4.1808348 -4.186058 -4.2066708 -4.2292471 -4.2476549 -4.2671342 -4.276495 -4.2807326 -4.2941236][-4.2641392 -4.2723656 -4.2714796 -4.2640882 -4.2526379 -4.232336 -4.2054315 -4.1897149 -4.1977649 -4.2158442 -4.2325468 -4.24937 -4.2550826 -4.2595749 -4.2801414][-4.2800837 -4.2821417 -4.2760434 -4.2697749 -4.2646761 -4.2429876 -4.2010636 -4.1671848 -4.1651964 -4.1825366 -4.2017255 -4.2191882 -4.2253633 -4.2356763 -4.2660427]]...]
INFO - root - 2017-12-06 05:50:24.771771: step 10510, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 77h:06m:05s remains)
INFO - root - 2017-12-06 05:50:33.340439: step 10520, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 80h:42m:09s remains)
INFO - root - 2017-12-06 05:50:41.851661: step 10530, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 74h:26m:33s remains)
INFO - root - 2017-12-06 05:50:50.534842: step 10540, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 78h:18m:33s remains)
INFO - root - 2017-12-06 05:50:59.146502: step 10550, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 75h:37m:47s remains)
INFO - root - 2017-12-06 05:51:07.709160: step 10560, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 76h:39m:15s remains)
INFO - root - 2017-12-06 05:51:16.463903: step 10570, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 76h:51m:35s remains)
INFO - root - 2017-12-06 05:51:24.952200: step 10580, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 76h:05m:53s remains)
INFO - root - 2017-12-06 05:51:33.608330: step 10590, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 77h:41m:58s remains)
INFO - root - 2017-12-06 05:51:42.049743: step 10600, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 77h:53m:59s remains)
2017-12-06 05:51:42.826795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.103796 -4.0115643 -3.9647837 -4.0141997 -4.0891547 -4.1393065 -4.154953 -4.1630669 -4.1768594 -4.1920991 -4.1956763 -4.1923785 -4.1875682 -4.170187 -4.1540875][-4.1032963 -4.0185881 -3.9754806 -4.0143819 -4.0734611 -4.1092 -4.1134496 -4.1171813 -4.1384811 -4.1668897 -4.181951 -4.1892638 -4.1972227 -4.1980672 -4.1939154][-4.1123362 -4.0463123 -4.0159044 -4.0416861 -4.0814095 -4.099051 -4.0959249 -4.1013355 -4.1334271 -4.1725483 -4.1967487 -4.2075453 -4.2147479 -4.2226477 -4.2239537][-4.1433754 -4.1008768 -4.0847878 -4.0999756 -4.121161 -4.1242256 -4.1149297 -4.1223631 -4.1592 -4.1993065 -4.2232533 -4.2310491 -4.2335739 -4.2431846 -4.2447987][-4.1802011 -4.151823 -4.1438761 -4.1508684 -4.1623631 -4.1689796 -4.1699309 -4.1821728 -4.2114563 -4.2354035 -4.2458529 -4.2460938 -4.244401 -4.2509985 -4.2499356][-4.2049909 -4.1765337 -4.1646395 -4.1638269 -4.1689744 -4.1785326 -4.1946864 -4.2200866 -4.2448707 -4.2516408 -4.2447033 -4.2346916 -4.2291861 -4.2368727 -4.2360349][-4.2047744 -4.1593018 -4.131484 -4.1187525 -4.1105986 -4.1121073 -4.1384649 -4.1794229 -4.2121849 -4.2204938 -4.2146454 -4.2068477 -4.2106247 -4.224308 -4.2284336][-4.1804991 -4.1153655 -4.0650029 -4.03317 -4.0095711 -3.9992602 -4.0286813 -4.0831227 -4.1252055 -4.145905 -4.1613321 -4.1750803 -4.2000694 -4.2262287 -4.2340522][-4.1457343 -4.0686445 -4.0096235 -3.9694309 -3.9376576 -3.9163232 -3.9410224 -4.0036726 -4.0508962 -4.0828047 -4.1201296 -4.1605849 -4.2038074 -4.2363248 -4.241466][-4.1264067 -4.0459418 -3.9897418 -3.9549072 -3.9273698 -3.9038405 -3.9197497 -3.9703262 -4.0071597 -4.0459909 -4.1060958 -4.1712337 -4.2226086 -4.2477303 -4.2391734][-4.1381755 -4.0630965 -4.0160103 -3.9953494 -3.9820311 -3.9757526 -3.9841509 -4.0111856 -4.0261965 -4.0616193 -4.1312275 -4.2064562 -4.2534618 -4.25802 -4.2256122][-4.1692586 -4.1121 -4.0831881 -4.0805335 -4.0831952 -4.0929551 -4.0981646 -4.1117067 -4.1195574 -4.1440296 -4.1970758 -4.2515831 -4.276051 -4.2553897 -4.2002358][-4.1946516 -4.1579814 -4.14775 -4.16066 -4.1717811 -4.1784554 -4.1775265 -4.1878695 -4.2011695 -4.2217383 -4.2533045 -4.2788324 -4.2793145 -4.2413168 -4.1805716][-4.2117896 -4.186255 -4.1862311 -4.2072725 -4.2241788 -4.2283983 -4.2226396 -4.2288342 -4.2457981 -4.2653809 -4.2837615 -4.2892723 -4.2736311 -4.2294722 -4.1724062][-4.2192893 -4.1998358 -4.2050328 -4.2295752 -4.2475424 -4.253705 -4.2495928 -4.2527041 -4.2677131 -4.2824507 -4.2931547 -4.2905917 -4.2702236 -4.2286644 -4.1797342]]...]
INFO - root - 2017-12-06 05:51:51.392374: step 10610, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.766 sec/batch; 68h:27m:41s remains)
INFO - root - 2017-12-06 05:51:59.859354: step 10620, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 78h:44m:52s remains)
INFO - root - 2017-12-06 05:52:08.481776: step 10630, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 76h:55m:46s remains)
INFO - root - 2017-12-06 05:52:16.978031: step 10640, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 75h:46m:00s remains)
INFO - root - 2017-12-06 05:52:25.600600: step 10650, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:29m:46s remains)
INFO - root - 2017-12-06 05:52:34.099840: step 10660, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:48m:51s remains)
INFO - root - 2017-12-06 05:52:42.612896: step 10670, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 75h:13m:25s remains)
INFO - root - 2017-12-06 05:52:51.057428: step 10680, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:18m:16s remains)
INFO - root - 2017-12-06 05:52:59.676233: step 10690, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 74h:55m:11s remains)
INFO - root - 2017-12-06 05:53:08.126161: step 10700, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:29m:35s remains)
2017-12-06 05:53:08.876106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3379512 -4.3356233 -4.3306479 -4.3282251 -4.3167882 -4.2875991 -4.2594872 -4.238173 -4.2236733 -4.2323685 -4.25407 -4.2751832 -4.2945127 -4.2938571 -4.2686777][-4.3428907 -4.3456478 -4.3424945 -4.3322158 -4.3049932 -4.2596622 -4.2236924 -4.1924887 -4.1718316 -4.1925321 -4.229929 -4.2628274 -4.2844477 -4.2853503 -4.2586493][-4.3467283 -4.3580117 -4.3598638 -4.3435678 -4.2996597 -4.2377505 -4.1927271 -4.146626 -4.1173224 -4.1497278 -4.2066593 -4.2493267 -4.268795 -4.2656708 -4.2361321][-4.3489108 -4.3672657 -4.36916 -4.3466616 -4.2904091 -4.2121844 -4.1542249 -4.0996175 -4.0749893 -4.1234403 -4.1941838 -4.2376723 -4.2490869 -4.2388382 -4.2045207][-4.3472838 -4.3670578 -4.3617311 -4.3288388 -4.2559614 -4.1543221 -4.0736074 -4.0136809 -4.0078206 -4.0812507 -4.1701436 -4.2124615 -4.2148261 -4.1873703 -4.1380873][-4.3457823 -4.3667383 -4.3545074 -4.3092694 -4.2209668 -4.1007428 -3.99701 -3.9230392 -3.9284904 -4.0271254 -4.1316347 -4.17653 -4.1744461 -4.1289339 -4.0569792][-4.3423276 -4.3624353 -4.3441739 -4.2883148 -4.1850986 -4.0503049 -3.9296904 -3.8304482 -3.8268847 -3.9448195 -4.0692558 -4.1236234 -4.1254878 -4.0709558 -3.9780266][-4.3349848 -4.3507538 -4.3258891 -4.2616577 -4.1497331 -4.0123897 -3.8869531 -3.7638371 -3.7366574 -3.8602872 -3.9999247 -4.0680928 -4.0831428 -4.0317144 -3.937305][-4.3245835 -4.3342485 -4.3052578 -4.2367549 -4.1219172 -3.9937625 -3.8796453 -3.7559991 -3.7137158 -3.8223433 -3.9602132 -4.0360332 -4.0601487 -4.02163 -3.9488478][-4.3133769 -4.3156257 -4.2833323 -4.2153573 -4.1071877 -3.992336 -3.8978143 -3.7949054 -3.7587278 -3.844672 -3.967958 -4.037735 -4.0627942 -4.0405083 -3.9969289][-4.3067775 -4.3022428 -4.2702103 -4.2109704 -4.1196156 -4.0251288 -3.9521716 -3.8733411 -3.8442504 -3.9066591 -4.0044537 -4.0656137 -4.0940423 -4.08928 -4.0624156][-4.3079505 -4.3000121 -4.2722325 -4.2251487 -4.155848 -4.0907316 -4.0453796 -3.9979568 -3.981535 -4.020124 -4.0850205 -4.1262622 -4.1445408 -4.1453252 -4.1295972][-4.3151646 -4.3078022 -4.2842946 -4.2485447 -4.1997929 -4.158669 -4.1382389 -4.1220484 -4.1192875 -4.141799 -4.1775126 -4.2002435 -4.20728 -4.2076616 -4.2027912][-4.3236418 -4.3172655 -4.2987041 -4.2740884 -4.2441535 -4.2228227 -4.2195454 -4.2213397 -4.2234106 -4.2334561 -4.2528391 -4.2640195 -4.2630124 -4.2593679 -4.2609029][-4.3331265 -4.3286576 -4.316443 -4.3024454 -4.2879825 -4.2812119 -4.2881827 -4.2970138 -4.2998433 -4.2999425 -4.3057604 -4.3092155 -4.3046665 -4.3013167 -4.3046393]]...]
INFO - root - 2017-12-06 05:53:17.420069: step 10710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:39m:43s remains)
INFO - root - 2017-12-06 05:53:25.799862: step 10720, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 74h:33m:22s remains)
INFO - root - 2017-12-06 05:53:34.368477: step 10730, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 76h:16m:37s remains)
INFO - root - 2017-12-06 05:53:42.962951: step 10740, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 76h:50m:00s remains)
INFO - root - 2017-12-06 05:53:51.592072: step 10750, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 77h:26m:03s remains)
INFO - root - 2017-12-06 05:54:00.163712: step 10760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 77h:09m:17s remains)
INFO - root - 2017-12-06 05:54:08.742296: step 10770, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 78h:14m:56s remains)
INFO - root - 2017-12-06 05:54:17.350906: step 10780, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 77h:24m:58s remains)
INFO - root - 2017-12-06 05:54:25.883741: step 10790, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 78h:14m:03s remains)
INFO - root - 2017-12-06 05:54:34.455079: step 10800, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:37m:10s remains)
2017-12-06 05:54:35.284694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3227496 -4.3178778 -4.3130012 -4.3093171 -4.3037271 -4.2916703 -4.2784691 -4.2684374 -4.2614346 -4.2588959 -4.2623072 -4.264173 -4.2588925 -4.2470012 -4.222415][-4.3181181 -4.3110104 -4.3028436 -4.2931705 -4.2805538 -4.2646551 -4.2519226 -4.2439504 -4.2389007 -4.2391272 -4.2496843 -4.2563019 -4.2500849 -4.2321534 -4.1969371][-4.3096538 -4.29779 -4.2835274 -4.2696304 -4.25692 -4.2460079 -4.2365465 -4.2301092 -4.2250729 -4.2273903 -4.2408104 -4.2480407 -4.2433434 -4.2239294 -4.1898208][-4.2934084 -4.2805605 -4.26608 -4.2514777 -4.2392006 -4.2251992 -4.2060366 -4.1887279 -4.179718 -4.18376 -4.1996207 -4.2128754 -4.2187071 -4.217031 -4.2022157][-4.2844009 -4.2756977 -4.2618437 -4.2417994 -4.2232585 -4.1947045 -4.1488314 -4.110621 -4.1035 -4.1162663 -4.139729 -4.1623149 -4.1784682 -4.1948862 -4.2005663][-4.2893119 -4.2855816 -4.2681565 -4.236558 -4.2033968 -4.1520395 -4.0734329 -4.0007939 -3.9959929 -4.0308247 -4.0700483 -4.1024537 -4.1298566 -4.1628065 -4.1870985][-4.2978396 -4.2961407 -4.2726068 -4.2256336 -4.1721044 -4.1005521 -3.9951248 -3.8927543 -3.9030137 -3.9752719 -4.028348 -4.0608215 -4.0918832 -4.1373773 -4.1818252][-4.2989745 -4.2989278 -4.2679596 -4.2080693 -4.1405196 -4.0673914 -3.9735229 -3.8893564 -3.922009 -4.0058355 -4.0478959 -4.0650692 -4.0939918 -4.1452618 -4.1988783][-4.2796249 -4.28478 -4.2587862 -4.2058344 -4.145927 -4.0933261 -4.0468664 -4.0171895 -4.0493674 -4.1010933 -4.1124177 -4.1127691 -4.1371069 -4.18599 -4.2312984][-4.2361417 -4.2479534 -4.2379088 -4.2076044 -4.1740332 -4.1496477 -4.1446486 -4.1508827 -4.174706 -4.1938119 -4.1798034 -4.1672668 -4.185842 -4.2240529 -4.2530475][-4.1773324 -4.1976118 -4.2101231 -4.2116251 -4.2054143 -4.2000704 -4.21254 -4.229311 -4.2415075 -4.2389879 -4.2131524 -4.1949272 -4.2035766 -4.2299318 -4.251091][-4.1278396 -4.1664004 -4.1983795 -4.2130847 -4.2162642 -4.2163539 -4.2282991 -4.2431307 -4.249073 -4.241724 -4.2158995 -4.1961689 -4.1958027 -4.2147832 -4.2353768][-4.117157 -4.1654005 -4.1958847 -4.2066145 -4.209909 -4.2085052 -4.214365 -4.2258711 -4.2313056 -4.227438 -4.21224 -4.1993685 -4.1974258 -4.2165561 -4.2407331][-4.1394191 -4.1739564 -4.190134 -4.195375 -4.1940575 -4.1857853 -4.1878867 -4.2015214 -4.2132092 -4.2203412 -4.220046 -4.2189617 -4.2244048 -4.2478533 -4.275454][-4.1598034 -4.1739979 -4.1775331 -4.17879 -4.1797247 -4.1757126 -4.1849585 -4.208632 -4.2299113 -4.2471886 -4.2568512 -4.2608137 -4.2684846 -4.2868915 -4.306066]]...]
INFO - root - 2017-12-06 05:54:43.846687: step 10810, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 78h:13m:00s remains)
INFO - root - 2017-12-06 05:54:52.146901: step 10820, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:44m:27s remains)
INFO - root - 2017-12-06 05:55:00.636937: step 10830, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 74h:43m:03s remains)
INFO - root - 2017-12-06 05:55:09.078829: step 10840, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 78h:35m:22s remains)
INFO - root - 2017-12-06 05:55:17.731413: step 10850, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 77h:09m:14s remains)
INFO - root - 2017-12-06 05:55:26.181820: step 10860, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 77h:55m:48s remains)
INFO - root - 2017-12-06 05:55:34.676615: step 10870, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 74h:54m:29s remains)
INFO - root - 2017-12-06 05:55:43.174422: step 10880, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 79h:18m:50s remains)
INFO - root - 2017-12-06 05:55:51.725281: step 10890, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 76h:42m:43s remains)
INFO - root - 2017-12-06 05:56:00.256763: step 10900, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 77h:18m:45s remains)
2017-12-06 05:56:01.068048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2172012 -4.252625 -4.2700663 -4.2693005 -4.2330093 -4.1628408 -4.1122279 -4.1151214 -4.1618595 -4.2082591 -4.247057 -4.2795644 -4.2892056 -4.2678261 -4.2311974][-4.2177877 -4.253623 -4.2725143 -4.2728825 -4.2376919 -4.1600041 -4.1023641 -4.1069427 -4.1605139 -4.2124448 -4.2524095 -4.2840118 -4.2903934 -4.2638707 -4.2232203][-4.2184362 -4.2573504 -4.2789831 -4.2781048 -4.2397504 -4.1532211 -4.0905371 -4.0982485 -4.1596761 -4.2169261 -4.2569165 -4.2848554 -4.286756 -4.2532272 -4.2070384][-4.2240586 -4.2632904 -4.2847595 -4.27862 -4.2326097 -4.138628 -4.0753026 -4.0896 -4.1590033 -4.2203484 -4.2597032 -4.2830424 -4.2790313 -4.2373405 -4.1859756][-4.234983 -4.2701988 -4.2875838 -4.2735667 -4.2172632 -4.1187034 -4.0591993 -4.08405 -4.159523 -4.2229843 -4.2608428 -4.2815471 -4.2708993 -4.2203441 -4.1643343][-4.2423325 -4.2745867 -4.2864852 -4.2647753 -4.2005539 -4.1036315 -4.0524945 -4.0883293 -4.1650639 -4.227221 -4.2626257 -4.2807045 -4.2641726 -4.204442 -4.1454625][-4.2412233 -4.273375 -4.2823944 -4.2543635 -4.1852531 -4.0958433 -4.0568562 -4.1015577 -4.1746273 -4.2326951 -4.2654743 -4.2808857 -4.2582188 -4.1887646 -4.1297817][-4.2357454 -4.268496 -4.2760229 -4.2412877 -4.1697941 -4.0895448 -4.0616956 -4.1106267 -4.1810708 -4.2371411 -4.2681265 -4.2823405 -4.2535863 -4.1759582 -4.1196127][-4.2340183 -4.2638812 -4.2661023 -4.2233515 -4.1512551 -4.0806675 -4.0613141 -4.1094484 -4.1800752 -4.2369366 -4.2690177 -4.281558 -4.2468953 -4.1642613 -4.1151285][-4.2376661 -4.2594714 -4.253315 -4.2029581 -4.1326289 -4.0728869 -4.0590892 -4.1047893 -4.1760044 -4.2338915 -4.2666955 -4.2752981 -4.2362642 -4.1541338 -4.1165457][-4.2407 -4.2538891 -4.2394133 -4.1829557 -4.118288 -4.0708613 -4.0587 -4.1010008 -4.1720657 -4.2288823 -4.26192 -4.2670989 -4.2269564 -4.1506791 -4.1228094][-4.2411456 -4.2489624 -4.2272367 -4.165741 -4.1081095 -4.0698462 -4.0567527 -4.0962286 -4.1668959 -4.2233229 -4.2564116 -4.2590365 -4.2205625 -4.1531849 -4.1298652][-4.2418604 -4.2474866 -4.2197108 -4.1547904 -4.1017475 -4.0662956 -4.0518289 -4.0906267 -4.1619029 -4.2178802 -4.2508907 -4.2518587 -4.2187085 -4.1589689 -4.13527][-4.2469978 -4.2505789 -4.2179203 -4.150939 -4.0979695 -4.0607214 -4.0437503 -4.0840087 -4.1567383 -4.2130876 -4.2466054 -4.2478204 -4.218586 -4.1638875 -4.140234][-4.2548485 -4.256988 -4.2208939 -4.1536207 -4.0998106 -4.0585556 -4.036624 -4.079092 -4.1541476 -4.2111359 -4.2447619 -4.2459736 -4.2192683 -4.1693311 -4.1475439]]...]
INFO - root - 2017-12-06 05:56:09.659769: step 10910, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 80h:27m:32s remains)
INFO - root - 2017-12-06 05:56:17.991420: step 10920, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 78h:04m:15s remains)
INFO - root - 2017-12-06 05:56:26.641863: step 10930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:59m:11s remains)
INFO - root - 2017-12-06 05:56:35.110335: step 10940, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 76h:17m:28s remains)
INFO - root - 2017-12-06 05:56:43.519969: step 10950, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 77h:55m:16s remains)
INFO - root - 2017-12-06 05:56:51.981266: step 10960, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 75h:02m:24s remains)
INFO - root - 2017-12-06 05:57:00.524230: step 10970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 76h:27m:08s remains)
INFO - root - 2017-12-06 05:57:09.117011: step 10980, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 77h:30m:04s remains)
INFO - root - 2017-12-06 05:57:17.706110: step 10990, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 77h:02m:17s remains)
INFO - root - 2017-12-06 05:57:26.357826: step 11000, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 78h:49m:28s remains)
2017-12-06 05:57:27.160526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3499 -4.3468938 -4.3405957 -4.3342981 -4.3322682 -4.3346996 -4.3380823 -4.3416471 -4.3440328 -4.3459196 -4.34422 -4.3406539 -4.3360739 -4.3338094 -4.3350158][-4.3452373 -4.3365035 -4.3267879 -4.3186469 -4.3181872 -4.3239312 -4.3293109 -4.3311534 -4.3304677 -4.3291645 -4.3248754 -4.3206248 -4.3185458 -4.3175759 -4.3211188][-4.3342881 -4.3192253 -4.3066449 -4.2987695 -4.3014269 -4.3104906 -4.3171492 -4.3162107 -4.3095727 -4.3018217 -4.294899 -4.2907047 -4.2910814 -4.2946544 -4.3045206][-4.312387 -4.2868977 -4.2716403 -4.2664919 -4.2717443 -4.2788677 -4.2838163 -4.2798944 -4.2681646 -4.2583189 -4.2521009 -4.2453256 -4.2460961 -4.2581191 -4.2785764][-4.2810936 -4.2413588 -4.2195525 -4.2159038 -4.2232966 -4.2249594 -4.2240572 -4.21182 -4.1992378 -4.1961861 -4.1997943 -4.194581 -4.1941442 -4.21072 -4.2411141][-4.2495451 -4.1983819 -4.1661563 -4.159616 -4.1641655 -4.1543074 -4.1350055 -4.1059575 -4.0998249 -4.1217904 -4.1500211 -4.1533933 -4.1523538 -4.1692781 -4.2032237][-4.2255874 -4.1675744 -4.1249409 -4.1074195 -4.0988708 -4.066257 -4.0153289 -3.9530659 -3.9615252 -4.0313187 -4.1003127 -4.1295667 -4.1342392 -4.1463346 -4.17214][-4.215642 -4.15727 -4.1088996 -4.0774193 -4.0504246 -3.9961267 -3.912812 -3.8150976 -3.83959 -3.9526343 -4.0586648 -4.117547 -4.1330628 -4.1436739 -4.157948][-4.2161279 -4.1640244 -4.1224494 -4.090385 -4.05452 -3.9944248 -3.9082513 -3.8103678 -3.8441966 -3.9508259 -4.0492873 -4.1177855 -4.1401858 -4.1543813 -4.1643214][-4.2231569 -4.1805606 -4.1518097 -4.1297336 -4.0983968 -4.0527291 -3.9934342 -3.9243219 -3.9504371 -4.0164433 -4.0803146 -4.1337433 -4.1495728 -4.1624851 -4.1744051][-4.233551 -4.2005715 -4.1832609 -4.1736159 -4.1544371 -4.1294537 -4.0976596 -4.0562248 -4.0737152 -4.109252 -4.1421432 -4.1710482 -4.173039 -4.1783795 -4.1894784][-4.2479725 -4.2244334 -4.2154036 -4.215486 -4.2093449 -4.2021465 -4.1886444 -4.1703959 -4.1836691 -4.2019143 -4.2151361 -4.2226119 -4.21222 -4.2104845 -4.2181344][-4.2701268 -4.25323 -4.2488332 -4.2532835 -4.2556276 -4.2587867 -4.2598543 -4.2587647 -4.2720776 -4.2823386 -4.2841845 -4.2776027 -4.2611938 -4.2520709 -4.2559795][-4.2924042 -4.2800322 -4.2782826 -4.2843575 -4.2909365 -4.2971444 -4.3007126 -4.3058033 -4.31798 -4.3254738 -4.323947 -4.3133783 -4.29766 -4.2858825 -4.2860627][-4.3074932 -4.2965388 -4.2952375 -4.3019662 -4.3094659 -4.316082 -4.3191047 -4.3244052 -4.3326139 -4.3377166 -4.3362007 -4.32802 -4.3167381 -4.3075638 -4.3072481]]...]
INFO - root - 2017-12-06 05:57:35.692394: step 11010, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 77h:51m:04s remains)
INFO - root - 2017-12-06 05:57:44.041974: step 11020, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 77h:00m:55s remains)
INFO - root - 2017-12-06 05:57:52.743511: step 11030, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 78h:06m:17s remains)
INFO - root - 2017-12-06 05:58:01.185285: step 11040, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 76h:05m:50s remains)
INFO - root - 2017-12-06 05:58:09.696156: step 11050, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 73h:40m:28s remains)
INFO - root - 2017-12-06 05:58:18.134584: step 11060, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 74h:03m:46s remains)
INFO - root - 2017-12-06 05:58:26.740186: step 11070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:58m:41s remains)
INFO - root - 2017-12-06 05:58:35.333914: step 11080, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 78h:35m:23s remains)
INFO - root - 2017-12-06 05:58:44.003244: step 11090, loss = 2.10, batch loss = 2.05 (9.0 examples/sec; 0.887 sec/batch; 79h:10m:39s remains)
INFO - root - 2017-12-06 05:58:52.827335: step 11100, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 78h:17m:22s remains)
2017-12-06 05:58:54.364962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.204987 -4.2018089 -4.1905217 -4.1604333 -4.1576567 -4.1790605 -4.2010956 -4.221158 -4.2289724 -4.230341 -4.2271013 -4.2331367 -4.2402453 -4.2432661 -4.2397203][-4.2094512 -4.2087555 -4.1918893 -4.1548519 -4.1471586 -4.1615529 -4.1808558 -4.2031155 -4.2146845 -4.2279711 -4.2361517 -4.2446737 -4.2433753 -4.2370329 -4.2263541][-4.21334 -4.2086515 -4.1842718 -4.1419315 -4.1259456 -4.1309819 -4.1450081 -4.1670241 -4.18332 -4.20657 -4.2226605 -4.2279124 -4.2181964 -4.2118168 -4.20784][-4.2230582 -4.2108727 -4.1780834 -4.1260023 -4.097837 -4.0915489 -4.0999985 -4.1189065 -4.13913 -4.1779571 -4.2082186 -4.2136021 -4.207128 -4.2064266 -4.2084479][-4.2219725 -4.208127 -4.1705871 -4.1094728 -4.0731339 -4.0553908 -4.0484328 -4.055655 -4.0802569 -4.1349378 -4.1820669 -4.200027 -4.2000022 -4.196578 -4.1914954][-4.1989388 -4.1932197 -4.1601586 -4.0995836 -4.0558996 -4.0127907 -3.9752958 -3.9575877 -3.9869962 -4.0695333 -4.1408186 -4.1709843 -4.1675053 -4.1568642 -4.1475263][-4.1811738 -4.1790771 -4.1518497 -4.09612 -4.0423527 -3.9708841 -3.9029446 -3.8662286 -3.9060411 -4.016911 -4.1035218 -4.1356125 -4.1344848 -4.1290503 -4.1299782][-4.1708755 -4.1689377 -4.1495585 -4.10268 -4.0482969 -3.9691086 -3.8992271 -3.8742223 -3.9218578 -4.0249448 -4.0988178 -4.125277 -4.1275854 -4.1247725 -4.1301117][-4.1549544 -4.1565351 -4.1530981 -4.1260843 -4.0852566 -4.0248337 -3.9779358 -3.964129 -3.9959683 -4.0608582 -4.1061931 -4.1253 -4.1232882 -4.1168265 -4.1255736][-4.1566043 -4.1644092 -4.1741896 -4.1664124 -4.1431913 -4.1070423 -4.0781841 -4.0635939 -4.074141 -4.1011133 -4.1225629 -4.131762 -4.1242342 -4.1145725 -4.1293616][-4.1812029 -4.1909704 -4.2043009 -4.2056942 -4.1951265 -4.177443 -4.1556463 -4.1387548 -4.1397071 -4.1494966 -4.1554737 -4.1576977 -4.1503825 -4.1465087 -4.166687][-4.2123876 -4.2218952 -4.2343106 -4.2397819 -4.2368908 -4.2282033 -4.2119927 -4.1997814 -4.1978264 -4.199306 -4.1992607 -4.201786 -4.1991363 -4.1984372 -4.2131529][-4.256846 -4.2674341 -4.2773147 -4.2802787 -4.2786145 -4.272377 -4.2608118 -4.2555766 -4.2557092 -4.2551179 -4.2548485 -4.2555213 -4.2536554 -4.2511511 -4.2589417][-4.2963338 -4.3079019 -4.315537 -4.3162937 -4.3117986 -4.3042188 -4.29629 -4.2969518 -4.2993879 -4.2989511 -4.2997708 -4.2988868 -4.2953343 -4.2922244 -4.2979283][-4.3110609 -4.3204703 -4.3258576 -4.3261566 -4.3222003 -4.3163085 -4.3109765 -4.3121381 -4.3144565 -4.3142824 -4.3152876 -4.3141947 -4.3102937 -4.3077269 -4.3118944]]...]
INFO - root - 2017-12-06 05:59:02.981864: step 11110, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.837 sec/batch; 74h:40m:54s remains)
INFO - root - 2017-12-06 05:59:11.330320: step 11120, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 77h:41m:27s remains)
INFO - root - 2017-12-06 05:59:19.949359: step 11130, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 77h:19m:00s remains)
INFO - root - 2017-12-06 05:59:28.466907: step 11140, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 77h:47m:17s remains)
INFO - root - 2017-12-06 05:59:37.130750: step 11150, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 75h:49m:48s remains)
INFO - root - 2017-12-06 05:59:45.614492: step 11160, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 74h:33m:46s remains)
INFO - root - 2017-12-06 05:59:54.162152: step 11170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:12m:07s remains)
INFO - root - 2017-12-06 06:00:02.766430: step 11180, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 77h:25m:33s remains)
INFO - root - 2017-12-06 06:00:11.266020: step 11190, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 74h:15m:41s remains)
INFO - root - 2017-12-06 06:00:19.744938: step 11200, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 75h:02m:43s remains)
2017-12-06 06:00:20.625038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1434221 -4.1450143 -4.1550589 -4.1756306 -4.1958675 -4.2042294 -4.2101555 -4.2162561 -4.2256556 -4.2295947 -4.227787 -4.2278862 -4.2115035 -4.1899 -4.1780815][-4.1684322 -4.1702681 -4.1813779 -4.1985335 -4.2102575 -4.2106524 -4.2111974 -4.2113638 -4.210525 -4.2073436 -4.2014275 -4.2019291 -4.186141 -4.16069 -4.1501751][-4.1795373 -4.1875734 -4.2022119 -4.2159181 -4.2176123 -4.2060175 -4.1993771 -4.1901631 -4.17896 -4.169795 -4.1629548 -4.1690159 -4.1601191 -4.1464462 -4.149929][-4.1839905 -4.1953483 -4.2049346 -4.2118645 -4.2071896 -4.1855936 -4.165278 -4.1477928 -4.1356206 -4.1329312 -4.1376638 -4.1549249 -4.1590314 -4.1621027 -4.17557][-4.1657934 -4.1730003 -4.1781898 -4.1820168 -4.17333 -4.1491776 -4.12303 -4.1050291 -4.1019588 -4.1098 -4.12669 -4.1564021 -4.1704273 -4.1786737 -4.1897688][-4.1357608 -4.14262 -4.1460648 -4.1456275 -4.1320953 -4.1062756 -4.0733275 -4.0543075 -4.0647821 -4.08313 -4.1058331 -4.1432304 -4.1644349 -4.1725912 -4.1806259][-4.116806 -4.1168165 -4.1051927 -4.0892439 -4.0684156 -4.0433793 -4.0073061 -3.9835677 -4.0075045 -4.0408521 -4.0741138 -4.1227794 -4.1474433 -4.1584511 -4.1684766][-4.1030397 -4.0847888 -4.0466652 -4.0068226 -3.9922028 -4.0006113 -3.9873011 -3.9721186 -4.0045457 -4.0464921 -4.0826011 -4.1254997 -4.1445775 -4.155859 -4.1679077][-4.0804753 -4.0482593 -4.0041351 -3.9720528 -3.9945102 -4.0507612 -4.0712762 -4.06132 -4.0781903 -4.10536 -4.1241536 -4.1475382 -4.1521325 -4.1559134 -4.1646333][-4.0730953 -4.0536766 -4.0443439 -4.0535073 -4.0897794 -4.14031 -4.1634464 -4.1511917 -4.1444736 -4.1550264 -4.1649494 -4.175808 -4.1752696 -4.1727209 -4.1776967][-4.1019783 -4.1025305 -4.1176872 -4.1419854 -4.1684361 -4.1953382 -4.2042956 -4.1863308 -4.1780562 -4.1903439 -4.2011409 -4.2103543 -4.2138433 -4.2116141 -4.2106609][-4.1225958 -4.1257019 -4.1492138 -4.1737165 -4.18962 -4.1998224 -4.2016072 -4.1900086 -4.1974854 -4.2185483 -4.2317338 -4.24457 -4.25298 -4.2525897 -4.2478967][-4.1132503 -4.1194572 -4.1548009 -4.181077 -4.1868687 -4.1820827 -4.1794796 -4.1779628 -4.2020683 -4.2345595 -4.257009 -4.2743354 -4.281579 -4.2799883 -4.2743311][-4.1068573 -4.1186666 -4.1492653 -4.1688423 -4.1633849 -4.1493468 -4.1534672 -4.1704006 -4.2059917 -4.2444839 -4.2696986 -4.2841778 -4.2864571 -4.2788639 -4.271255][-4.1038508 -4.1076941 -4.1245284 -4.1368928 -4.1264019 -4.1132903 -4.1305504 -4.1662703 -4.2119193 -4.2501764 -4.2689633 -4.2748866 -4.2708645 -4.25815 -4.2517014]]...]
INFO - root - 2017-12-06 06:00:29.196415: step 11210, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 79h:01m:28s remains)
INFO - root - 2017-12-06 06:00:37.562990: step 11220, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 78h:15m:53s remains)
INFO - root - 2017-12-06 06:00:46.229731: step 11230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 75h:44m:50s remains)
INFO - root - 2017-12-06 06:00:54.699209: step 11240, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:16m:27s remains)
INFO - root - 2017-12-06 06:01:03.389756: step 11250, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.879 sec/batch; 78h:24m:55s remains)
INFO - root - 2017-12-06 06:01:11.784005: step 11260, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 74h:24m:41s remains)
INFO - root - 2017-12-06 06:01:20.430396: step 11270, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 78h:04m:43s remains)
INFO - root - 2017-12-06 06:01:29.015630: step 11280, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 76h:22m:17s remains)
INFO - root - 2017-12-06 06:01:37.516386: step 11290, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 76h:20m:55s remains)
INFO - root - 2017-12-06 06:01:46.150558: step 11300, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 76h:42m:15s remains)
2017-12-06 06:01:46.881491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1761885 -4.183826 -4.1807404 -4.1716204 -4.1714725 -4.1759629 -4.1774144 -4.1874075 -4.1996679 -4.1925116 -4.1573648 -4.1111407 -4.091486 -4.1134362 -4.149128][-4.1884046 -4.1916995 -4.1865005 -4.1781073 -4.1737933 -4.1711664 -4.167891 -4.1775827 -4.1873989 -4.1782455 -4.1424351 -4.0910187 -4.0687351 -4.0977283 -4.1366796][-4.1707673 -4.1702571 -4.1696944 -4.1641154 -4.1578317 -4.1504903 -4.1491833 -4.1620693 -4.1728644 -4.1666508 -4.1329069 -4.075552 -4.0520387 -4.09084 -4.1345267][-4.1351748 -4.1336517 -4.1420941 -4.1436515 -4.13886 -4.1328678 -4.1351533 -4.1478639 -4.1592565 -4.1600757 -4.135325 -4.081079 -4.0592461 -4.1027327 -4.14727][-4.10359 -4.1046815 -4.1201982 -4.1278024 -4.1254606 -4.1202588 -4.1220312 -4.1318436 -4.1413054 -4.1462884 -4.1343589 -4.0972452 -4.0845022 -4.1268673 -4.1668715][-4.0858526 -4.0907454 -4.1099072 -4.1202636 -4.1191268 -4.109026 -4.1032643 -4.1043353 -4.1113954 -4.1216526 -4.1241112 -4.1085796 -4.1099329 -4.1514249 -4.1853471][-4.0856285 -4.0954504 -4.1155171 -4.1236286 -4.11714 -4.0951486 -4.0750256 -4.0645018 -4.0692968 -4.0819192 -4.08813 -4.0769649 -4.0848689 -4.13164 -4.170773][-4.0986328 -4.1130433 -4.1311574 -4.1304016 -4.11071 -4.0737863 -4.0371838 -4.0180044 -4.0222597 -4.0313787 -4.0264359 -3.9984643 -4.0006123 -4.0608306 -4.1139975][-4.1202745 -4.1345859 -4.1481972 -4.13644 -4.1029735 -4.0565333 -4.0112 -3.9902966 -4.00019 -4.0098891 -3.99653 -3.9499681 -3.9362781 -4.0038195 -4.0687132][-4.1334224 -4.1413589 -4.1500888 -4.1320858 -4.0988293 -4.0570421 -4.0165887 -4.0049706 -4.0271974 -4.0476141 -4.0414824 -3.9987819 -3.9822598 -4.0349808 -4.0891309][-4.1377158 -4.1371627 -4.1452756 -4.1319742 -4.106081 -4.0749569 -4.0452895 -4.040534 -4.067729 -4.094388 -4.0965261 -4.0660481 -4.0564051 -4.0947571 -4.1355171][-4.1276083 -4.1243811 -4.1359649 -4.1329751 -4.1187792 -4.1003256 -4.0780444 -4.0708227 -4.0904784 -4.1116843 -4.1177716 -4.1017733 -4.1019783 -4.1343393 -4.16719][-4.12264 -4.1213655 -4.1325321 -4.1365409 -4.1321907 -4.1265249 -4.1118112 -4.1045642 -4.11029 -4.1183453 -4.1210728 -4.1141677 -4.1206779 -4.1512561 -4.1806607][-4.1217709 -4.1225619 -4.1324592 -4.1396961 -4.1433892 -4.1488862 -4.1415186 -4.13421 -4.1318693 -4.1351547 -4.1347032 -4.12827 -4.131741 -4.1586504 -4.1865973][-4.1148753 -4.11685 -4.1298137 -4.1435175 -4.1566825 -4.1700397 -4.1659145 -4.1566005 -4.1536932 -4.1608 -4.164 -4.1597786 -4.1599932 -4.1780906 -4.1977434]]...]
INFO - root - 2017-12-06 06:01:55.420215: step 11310, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 78h:21m:56s remains)
INFO - root - 2017-12-06 06:02:03.971023: step 11320, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:54m:25s remains)
INFO - root - 2017-12-06 06:02:12.392950: step 11330, loss = 2.03, batch loss = 1.97 (9.8 examples/sec; 0.819 sec/batch; 73h:03m:17s remains)
INFO - root - 2017-12-06 06:02:21.102757: step 11340, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 77h:03m:00s remains)
INFO - root - 2017-12-06 06:02:29.802044: step 11350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 77h:46m:57s remains)
INFO - root - 2017-12-06 06:02:38.380688: step 11360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 76h:53m:23s remains)
INFO - root - 2017-12-06 06:02:46.961175: step 11370, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 76h:20m:31s remains)
INFO - root - 2017-12-06 06:02:55.472187: step 11380, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:17m:51s remains)
INFO - root - 2017-12-06 06:03:04.116337: step 11390, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 75h:17m:57s remains)
INFO - root - 2017-12-06 06:03:12.554318: step 11400, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 74h:08m:45s remains)
2017-12-06 06:03:13.393289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3161774 -4.3145952 -4.3144736 -4.3152008 -4.3144388 -4.3138361 -4.3142524 -4.3132849 -4.3111181 -4.3120894 -4.3169355 -4.322382 -4.32987 -4.3382044 -4.3460464][-4.2910185 -4.2931352 -4.296607 -4.2991948 -4.2975249 -4.2955937 -4.2939386 -4.2912269 -4.2886887 -4.2901921 -4.2955832 -4.3028889 -4.313931 -4.3268795 -4.338686][-4.2649541 -4.2633882 -4.2666564 -4.2693982 -4.267128 -4.2630973 -4.2554545 -4.2473612 -4.2458448 -4.2513924 -4.2613788 -4.273035 -4.2887754 -4.3082461 -4.3264961][-4.2351313 -4.2254467 -4.2232423 -4.2211394 -4.215848 -4.208065 -4.1950288 -4.1846085 -4.1848173 -4.1950221 -4.2117014 -4.2303185 -4.2526574 -4.279253 -4.3051972][-4.2075362 -4.1912169 -4.1792107 -4.1664243 -4.1558867 -4.1458015 -4.1339006 -4.1292133 -4.1357403 -4.1465311 -4.161293 -4.1795797 -4.2056437 -4.2386322 -4.2737556][-4.1845522 -4.1656179 -4.1486816 -4.128417 -4.1126833 -4.1043925 -4.1022334 -4.1078725 -4.1140652 -4.1162395 -4.1198292 -4.1309986 -4.1559105 -4.1916766 -4.2344565][-4.1617351 -4.1368694 -4.1148291 -4.0924106 -4.0755968 -4.0775738 -4.0963016 -4.1163712 -4.1202841 -4.1127739 -4.1051264 -4.1080294 -4.1284976 -4.1615143 -4.2040467][-4.1437945 -4.1140571 -4.0896316 -4.0660753 -4.0509882 -4.0644226 -4.0978556 -4.1256957 -4.1301665 -4.1239805 -4.1180706 -4.1196871 -4.1365056 -4.1629734 -4.1980281][-4.1362715 -4.108017 -4.087925 -4.0683727 -4.0572424 -4.0713115 -4.0989547 -4.1209173 -4.1294646 -4.1341238 -4.1422806 -4.1518507 -4.1679521 -4.1896734 -4.2155843][-4.1382275 -4.1211615 -4.115025 -4.1057529 -4.100275 -4.1055551 -4.1135488 -4.1227403 -4.1334863 -4.1505637 -4.17369 -4.1922569 -4.207448 -4.2236423 -4.2416439][-4.1569414 -4.1530976 -4.1571584 -4.1575651 -4.1552567 -4.1512818 -4.1410308 -4.1341934 -4.1435623 -4.1697154 -4.2031817 -4.2264781 -4.2411323 -4.2541661 -4.2689266][-4.1901841 -4.1939 -4.1994905 -4.2006354 -4.2009258 -4.1935134 -4.1745696 -4.1589527 -4.1640363 -4.1925912 -4.2300167 -4.2551737 -4.2707863 -4.283524 -4.2981668][-4.2323556 -4.2374945 -4.2410173 -4.2384243 -4.2363734 -4.2272563 -4.206224 -4.1894655 -4.193172 -4.2215538 -4.2583466 -4.2847176 -4.3018465 -4.3160968 -4.3283086][-4.2771587 -4.2825108 -4.2827148 -4.27598 -4.2674236 -4.2549334 -4.2356963 -4.2211003 -4.2240057 -4.247416 -4.2794352 -4.3065424 -4.3257308 -4.3403611 -4.3497062][-4.2979279 -4.3011222 -4.2995257 -4.2931895 -4.2853756 -4.2766957 -4.2635055 -4.2536016 -4.2555437 -4.270534 -4.292347 -4.3151956 -4.335021 -4.34977 -4.3574114]]...]
INFO - root - 2017-12-06 06:03:21.923057: step 11410, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 77h:27m:05s remains)
INFO - root - 2017-12-06 06:03:30.461033: step 11420, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 78h:36m:33s remains)
INFO - root - 2017-12-06 06:03:38.912282: step 11430, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 75h:58m:28s remains)
INFO - root - 2017-12-06 06:03:47.505205: step 11440, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 78h:01m:05s remains)
INFO - root - 2017-12-06 06:03:56.037070: step 11450, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 74h:35m:03s remains)
INFO - root - 2017-12-06 06:04:04.661285: step 11460, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 77h:45m:07s remains)
INFO - root - 2017-12-06 06:04:13.206009: step 11470, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 78h:20m:31s remains)
INFO - root - 2017-12-06 06:04:21.487256: step 11480, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 74h:37m:16s remains)
INFO - root - 2017-12-06 06:04:30.036460: step 11490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:30m:21s remains)
INFO - root - 2017-12-06 06:04:38.563976: step 11500, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 76h:11m:25s remains)
2017-12-06 06:04:39.335855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.242672 -4.241456 -4.2343493 -4.2134519 -4.1765137 -4.1342554 -4.1194253 -4.1319885 -4.1462517 -4.1727 -4.2030311 -4.2238669 -4.24762 -4.2731638 -4.2907014][-4.2252951 -4.2264085 -4.2239347 -4.2089014 -4.1708031 -4.120945 -4.0944262 -4.1026034 -4.117722 -4.1500497 -4.1910892 -4.2183557 -4.2445054 -4.2727809 -4.2935834][-4.20505 -4.206099 -4.2095461 -4.2050824 -4.1724768 -4.11846 -4.079752 -4.0799212 -4.0931783 -4.1219416 -4.1697888 -4.2081504 -4.2404037 -4.2722983 -4.2962694][-4.1916933 -4.1885309 -4.1965609 -4.1988 -4.1701264 -4.1146421 -4.0613751 -4.0535808 -4.0694118 -4.0925212 -4.1430964 -4.1922493 -4.2337041 -4.2676535 -4.2952924][-4.1828322 -4.1778316 -4.1879916 -4.197269 -4.1689329 -4.1094961 -4.0416441 -4.0308914 -4.0634637 -4.0914268 -4.1386762 -4.1883216 -4.2317715 -4.2641273 -4.2926641][-4.1518173 -4.1538858 -4.1690693 -4.1855869 -4.1581 -4.0901475 -4.0007534 -3.9833829 -4.039525 -4.08603 -4.1354871 -4.1865358 -4.2305503 -4.2608342 -4.2881193][-4.0884242 -4.1066451 -4.1377153 -4.1637669 -4.1409278 -4.0625806 -3.9421103 -3.9018426 -3.9801102 -4.0554056 -4.1152534 -4.1789365 -4.2318673 -4.26409 -4.2877765][-4.0056305 -4.04149 -4.0947447 -4.1363993 -4.1245522 -4.038691 -3.8743029 -3.7874703 -3.8830936 -4.005259 -4.0914712 -4.1756144 -4.2379246 -4.2714262 -4.28986][-3.9464817 -3.9845057 -4.0546331 -4.1131344 -4.1207647 -4.0461078 -3.8595734 -3.7124476 -3.801065 -3.9615402 -4.0747652 -4.1717873 -4.2395372 -4.2751117 -4.2906618][-3.9570973 -3.9850392 -4.052721 -4.1144366 -4.1376619 -4.0891728 -3.936348 -3.7769105 -3.822926 -3.967839 -4.0779247 -4.16787 -4.2329955 -4.2728314 -4.2912955][-4.0282092 -4.0446644 -4.0991716 -4.1453981 -4.1703763 -4.1493196 -4.0533819 -3.9252715 -3.9296072 -4.0233936 -4.1071038 -4.1768947 -4.2313013 -4.2714167 -4.2929821][-4.1182518 -4.123239 -4.1600471 -4.1871142 -4.2067719 -4.206358 -4.1563277 -4.0707431 -4.0550466 -4.1043339 -4.1576958 -4.2057896 -4.2464108 -4.2790141 -4.2985988][-4.1969929 -4.1932554 -4.2154789 -4.2317982 -4.2452288 -4.2540388 -4.2310672 -4.1784353 -4.1560235 -4.1783767 -4.2111239 -4.2421646 -4.270092 -4.2914267 -4.3036127][-4.24978 -4.2402682 -4.253056 -4.2645903 -4.275538 -4.2880087 -4.2802753 -4.2474375 -4.2218814 -4.2270823 -4.2464423 -4.2669954 -4.2876005 -4.30269 -4.3085995][-4.27447 -4.2666712 -4.2726512 -4.282084 -4.2924595 -4.3033581 -4.3025441 -4.2807269 -4.257472 -4.256299 -4.26746 -4.2821045 -4.2977343 -4.308404 -4.3113518]]...]
INFO - root - 2017-12-06 06:04:47.948973: step 11510, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 76h:42m:34s remains)
INFO - root - 2017-12-06 06:04:56.424320: step 11520, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 73h:14m:49s remains)
INFO - root - 2017-12-06 06:05:05.134096: step 11530, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 79h:20m:07s remains)
INFO - root - 2017-12-06 06:05:13.734960: step 11540, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 75h:21m:47s remains)
INFO - root - 2017-12-06 06:05:22.290150: step 11550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:46m:05s remains)
INFO - root - 2017-12-06 06:05:30.924988: step 11560, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:46m:04s remains)
INFO - root - 2017-12-06 06:05:39.523827: step 11570, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.855 sec/batch; 76h:13m:58s remains)
INFO - root - 2017-12-06 06:05:48.020979: step 11580, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.814 sec/batch; 72h:35m:30s remains)
INFO - root - 2017-12-06 06:05:56.580982: step 11590, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 73h:30m:39s remains)
INFO - root - 2017-12-06 06:06:05.174960: step 11600, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 77h:52m:17s remains)
2017-12-06 06:06:05.980790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.298727 -4.2955103 -4.2939167 -4.2943907 -4.2972832 -4.3047562 -4.3095608 -4.3101768 -4.3138576 -4.3215966 -4.3255677 -4.3261976 -4.3270268 -4.3291965 -4.3350434][-4.2554054 -4.2500505 -4.2488456 -4.2465053 -4.2483058 -4.2592354 -4.2670741 -4.2692313 -4.2763529 -4.2878036 -4.2964783 -4.3027558 -4.3089309 -4.3157482 -4.3252487][-4.1939654 -4.185699 -4.1876173 -4.1908741 -4.1942835 -4.2071562 -4.2183275 -4.221756 -4.2327967 -4.2458253 -4.25868 -4.2700324 -4.2789068 -4.2909265 -4.3077378][-4.1380472 -4.121067 -4.1229925 -4.1302748 -4.1327128 -4.1440878 -4.157917 -4.1640029 -4.1820846 -4.2010207 -4.2202406 -4.2369661 -4.2462482 -4.2617064 -4.2858272][-4.1012125 -4.0762753 -4.0701475 -4.0703988 -4.0620437 -4.0654535 -4.0760975 -4.084733 -4.1135178 -4.1441059 -4.1728172 -4.1982441 -4.2131634 -4.23262 -4.2625842][-4.0787034 -4.0472245 -4.0258145 -4.0072036 -3.9804547 -3.9628766 -3.9576366 -3.9691556 -4.016892 -4.0673151 -4.113812 -4.1586347 -4.1882238 -4.2140284 -4.2467933][-4.0821886 -4.0478382 -4.0175867 -3.9813313 -3.9307048 -3.8789339 -3.8418205 -3.8520832 -3.9169426 -3.9899862 -4.0604825 -4.1252737 -4.1680856 -4.2027121 -4.2363653][-4.1127672 -4.0888004 -4.0631723 -4.023819 -3.9642198 -3.8921268 -3.8334508 -3.8355625 -3.8941631 -3.9646509 -4.0395966 -4.1106577 -4.1621418 -4.2025943 -4.2329779][-4.14514 -4.1314545 -4.1188045 -4.0884957 -4.0352926 -3.9678473 -3.9106367 -3.9051781 -3.9431152 -3.9960542 -4.0574217 -4.1220284 -4.173481 -4.2118812 -4.237174][-4.1693177 -4.1601629 -4.1554265 -4.13573 -4.095624 -4.0435052 -3.9961104 -3.9857118 -4.0074844 -4.0425935 -4.0906625 -4.1429629 -4.18717 -4.2227197 -4.2477236][-4.1966228 -4.1873889 -4.1838188 -4.1705503 -4.1425729 -4.1059365 -4.0732985 -4.06503 -4.0776119 -4.1001792 -4.1341085 -4.1717229 -4.2063112 -4.238246 -4.2624564][-4.2378831 -4.2309942 -4.2283235 -4.2194486 -4.2013354 -4.1752896 -4.1506686 -4.1416383 -4.1476774 -4.1630716 -4.1853981 -4.2105341 -4.2382345 -4.2659397 -4.2872233][-4.2847862 -4.283855 -4.2822089 -4.2761316 -4.2646475 -4.2470059 -4.2284579 -4.2199168 -4.2228589 -4.2335148 -4.2465892 -4.2611685 -4.2798266 -4.3008332 -4.3167968][-4.3192825 -4.3218226 -4.3200011 -4.3133755 -4.3059225 -4.2962518 -4.2857504 -4.2832718 -4.2875257 -4.2952218 -4.301456 -4.3086 -4.3178854 -4.3310318 -4.3413472][-4.3357878 -4.3385777 -4.3369536 -4.3326368 -4.3293209 -4.3252778 -4.3206124 -4.3205662 -4.3241954 -4.3287311 -4.3322144 -4.3351469 -4.3399215 -4.3480153 -4.353981]]...]
INFO - root - 2017-12-06 06:06:14.506284: step 11610, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 80h:13m:34s remains)
INFO - root - 2017-12-06 06:06:23.109596: step 11620, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 76h:14m:16s remains)
INFO - root - 2017-12-06 06:06:31.636056: step 11630, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 77h:01m:27s remains)
INFO - root - 2017-12-06 06:06:40.093294: step 11640, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 77h:21m:23s remains)
INFO - root - 2017-12-06 06:06:48.667033: step 11650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 77h:01m:32s remains)
INFO - root - 2017-12-06 06:06:57.196303: step 11660, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 74h:28m:59s remains)
INFO - root - 2017-12-06 06:07:05.698610: step 11670, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.844 sec/batch; 75h:11m:38s remains)
INFO - root - 2017-12-06 06:07:14.383285: step 11680, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 78h:25m:15s remains)
INFO - root - 2017-12-06 06:07:22.943675: step 11690, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 75h:03m:58s remains)
INFO - root - 2017-12-06 06:07:31.468980: step 11700, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 73h:35m:48s remains)
2017-12-06 06:07:32.217755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.300292 -4.2876081 -4.2757034 -4.2615438 -4.2511845 -4.2489185 -4.2506704 -4.2483544 -4.2335067 -4.2056813 -4.1820197 -4.1736984 -4.1823773 -4.1979475 -4.2166867][-4.2877393 -4.2687244 -4.2536 -4.235971 -4.2192068 -4.2155819 -4.2201896 -4.2217932 -4.2101336 -4.1817255 -4.1530838 -4.1386886 -4.1435437 -4.1580558 -4.1774516][-4.2779241 -4.2523112 -4.2312098 -4.2076893 -4.185925 -4.1804266 -4.1857409 -4.1921916 -4.1905866 -4.1728525 -4.1466856 -4.1267939 -4.1281643 -4.1402979 -4.1565633][-4.2683091 -4.2379203 -4.2108192 -4.1818781 -4.1571355 -4.1481581 -4.1534843 -4.1669016 -4.17828 -4.1740689 -4.1527147 -4.1327343 -4.1357079 -4.1476889 -4.159193][-4.259562 -4.2289419 -4.2016554 -4.1697941 -4.139473 -4.1226892 -4.1160712 -4.1235442 -4.1426735 -4.1498666 -4.1387229 -4.1264467 -4.1351857 -4.1525207 -4.1649179][-4.2552533 -4.2259789 -4.20125 -4.1677566 -4.1283073 -4.0917397 -4.056663 -4.0477529 -4.0754032 -4.0993414 -4.10759 -4.1132412 -4.1309242 -4.1515117 -4.1686115][-4.2523627 -4.2217007 -4.1953073 -4.1560755 -4.1046367 -4.0442362 -3.9703536 -3.9397912 -3.9837387 -4.0328045 -4.0646749 -4.0934687 -4.1212 -4.1454945 -4.1679549][-4.2515221 -4.2166181 -4.1841307 -4.1358719 -4.0758371 -3.9997573 -3.8994298 -3.8533533 -3.9164605 -3.9895306 -4.039917 -4.0858841 -4.1212511 -4.1490736 -4.1741347][-4.2559433 -4.2174082 -4.17957 -4.1293077 -4.0763497 -4.012126 -3.9269183 -3.8870006 -3.9423845 -4.011076 -4.0615225 -4.1099911 -4.1442332 -4.1662235 -4.1844215][-4.2627316 -4.2257924 -4.1885242 -4.1472859 -4.1131954 -4.0762777 -4.0288224 -4.0032454 -4.0317664 -4.0719137 -4.1042004 -4.14141 -4.1653876 -4.1754389 -4.1809525][-4.2710915 -4.2410836 -4.2090688 -4.1774035 -4.1570063 -4.1407394 -4.1190104 -4.1018639 -4.1072769 -4.124053 -4.1403451 -4.1640067 -4.1765985 -4.1747427 -4.1700516][-4.2782655 -4.254777 -4.2315831 -4.2109356 -4.1997623 -4.1944566 -4.1830807 -4.1652274 -4.1510549 -4.1504393 -4.1574373 -4.172359 -4.1789351 -4.1734419 -4.1678181][-4.2846422 -4.2639432 -4.2483945 -4.2377224 -4.2335658 -4.2314 -4.2202435 -4.1942725 -4.1639166 -4.1501508 -4.1525459 -4.1627665 -4.170682 -4.1737328 -4.1766133][-4.293643 -4.275394 -4.26423 -4.2578883 -4.252996 -4.2458425 -4.2291136 -4.1963429 -4.1588216 -4.140079 -4.1434026 -4.1542816 -4.1691175 -4.1835833 -4.1936073][-4.3063984 -4.2920551 -4.2823853 -4.2751703 -4.2663369 -4.254158 -4.2348905 -4.2040634 -4.1694031 -4.1524997 -4.157711 -4.1701984 -4.1866646 -4.206038 -4.219327]]...]
INFO - root - 2017-12-06 06:07:40.700760: step 11710, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 76h:53m:24s remains)
INFO - root - 2017-12-06 06:07:49.230943: step 11720, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 77h:04m:51s remains)
INFO - root - 2017-12-06 06:07:57.794089: step 11730, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 77h:11m:51s remains)
INFO - root - 2017-12-06 06:08:06.367372: step 11740, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 77h:07m:55s remains)
INFO - root - 2017-12-06 06:08:14.774804: step 11750, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 75h:52m:32s remains)
INFO - root - 2017-12-06 06:08:23.343096: step 11760, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 76h:04m:25s remains)
INFO - root - 2017-12-06 06:08:31.994735: step 11770, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 79h:58m:45s remains)
INFO - root - 2017-12-06 06:08:40.629304: step 11780, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 75h:02m:00s remains)
INFO - root - 2017-12-06 06:08:49.199036: step 11790, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 76h:11m:39s remains)
INFO - root - 2017-12-06 06:08:57.722132: step 11800, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 78h:58m:44s remains)
2017-12-06 06:08:58.580300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2193861 -4.1973777 -4.1973953 -4.220273 -4.2270789 -4.2247033 -4.2203455 -4.2172332 -4.2173452 -4.22374 -4.23757 -4.2536273 -4.2604065 -4.2592521 -4.2633543][-4.2018542 -4.1835694 -4.1895823 -4.2167125 -4.2259364 -4.2259021 -4.2237563 -4.2226186 -4.2217531 -4.2264223 -4.2337203 -4.2402611 -4.2421527 -4.2401171 -4.2436037][-4.18014 -4.1588936 -4.1685872 -4.1975064 -4.2055907 -4.2050548 -4.2078896 -4.2164922 -4.2237549 -4.2335296 -4.2408838 -4.2454915 -4.2461553 -4.2414155 -4.240211][-4.1685824 -4.1352153 -4.1337638 -4.1545238 -4.1601224 -4.1610622 -4.1689644 -4.1871381 -4.2083254 -4.2271857 -4.2396703 -4.2495279 -4.2544489 -4.2486529 -4.2438498][-4.1607237 -4.1171255 -4.0982838 -4.1046472 -4.10365 -4.1058412 -4.1197629 -4.1442995 -4.1772203 -4.2075062 -4.2274394 -4.2427368 -4.2516513 -4.2485056 -4.2462792][-4.1387777 -4.094749 -4.0692062 -4.0655513 -4.0586729 -4.0536427 -4.0610061 -4.0832586 -4.1216774 -4.1638331 -4.19319 -4.2172785 -4.2320604 -4.23558 -4.2413692][-4.1062317 -4.0771337 -4.0610447 -4.0590959 -4.0556636 -4.0475864 -4.0418396 -4.0498438 -4.0783572 -4.1194167 -4.1491327 -4.1778073 -4.1980338 -4.2108188 -4.2267251][-4.0621028 -4.0426412 -4.0343742 -4.0384789 -4.0442185 -4.040133 -4.0300145 -4.031414 -4.052824 -4.0866547 -4.1084943 -4.1335187 -4.1555243 -4.174129 -4.1983809][-4.0260863 -4.0047383 -3.9974179 -4.0078263 -4.0203404 -4.0208559 -4.0168042 -4.0251727 -4.0473628 -4.0786939 -4.093009 -4.1081934 -4.1235552 -4.1400785 -4.1677227][-4.0235262 -4.0018225 -3.9982171 -4.015461 -4.0277567 -4.0285444 -4.0320034 -4.0464458 -4.0649681 -4.0906177 -4.1027436 -4.1147437 -4.1270037 -4.1401081 -4.16446][-4.0447674 -4.0245175 -4.0256877 -4.045393 -4.0555153 -4.0563602 -4.0633297 -4.0775809 -4.08966 -4.1075563 -4.1200438 -4.134131 -4.1452966 -4.15613 -4.1772046][-4.0785327 -4.0587106 -4.0591297 -4.075 -4.0818739 -4.0842748 -4.0924788 -4.1077905 -4.1185222 -4.1286297 -4.1395578 -4.1529312 -4.1619034 -4.1686544 -4.1873631][-4.1189752 -4.104856 -4.1049032 -4.1147528 -4.1175594 -4.1194129 -4.1270618 -4.1436591 -4.1549516 -4.1612043 -4.1702242 -4.1779747 -4.1791334 -4.1831312 -4.1989441][-4.1696916 -4.1605783 -4.1629529 -4.1704383 -4.1722732 -4.173502 -4.178236 -4.18927 -4.1975155 -4.2018023 -4.208447 -4.2106013 -4.2061458 -4.2076378 -4.2195454][-4.2249837 -4.2196021 -4.2218213 -4.2265573 -4.2284842 -4.2287807 -4.2304068 -4.2349153 -4.2396789 -4.2441125 -4.249835 -4.2509947 -4.2461815 -4.24462 -4.251255]]...]
INFO - root - 2017-12-06 06:09:07.049296: step 11810, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 77h:15m:12s remains)
INFO - root - 2017-12-06 06:09:15.622397: step 11820, loss = 2.04, batch loss = 1.98 (10.2 examples/sec; 0.784 sec/batch; 69h:49m:51s remains)
INFO - root - 2017-12-06 06:09:24.191896: step 11830, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:07m:17s remains)
INFO - root - 2017-12-06 06:09:32.684981: step 11840, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 75h:58m:26s remains)
INFO - root - 2017-12-06 06:09:41.158196: step 11850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:47m:49s remains)
INFO - root - 2017-12-06 06:09:49.679427: step 11860, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 74h:49m:46s remains)
INFO - root - 2017-12-06 06:09:58.227723: step 11870, loss = 2.03, batch loss = 1.97 (9.9 examples/sec; 0.809 sec/batch; 72h:01m:07s remains)
INFO - root - 2017-12-06 06:10:06.759399: step 11880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:47m:04s remains)
INFO - root - 2017-12-06 06:10:15.444227: step 11890, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 77h:28m:14s remains)
INFO - root - 2017-12-06 06:10:24.082462: step 11900, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 74h:27m:30s remains)
2017-12-06 06:10:24.941085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2884378 -4.2773538 -4.2719326 -4.2724838 -4.2741146 -4.2751679 -4.2754512 -4.2756824 -4.2878265 -4.303452 -4.303678 -4.2956285 -4.2902789 -4.2892294 -4.2936649][-4.2817254 -4.2729387 -4.2726455 -4.2809014 -4.2901831 -4.2983675 -4.303421 -4.3056655 -4.3170571 -4.3320851 -4.3337746 -4.3277879 -4.3190713 -4.3110681 -4.3067231][-4.2759643 -4.2645116 -4.2602978 -4.2647796 -4.269053 -4.2764049 -4.2825017 -4.2860489 -4.2997017 -4.3203535 -4.3286138 -4.3289971 -4.32461 -4.3154821 -4.307981][-4.2747617 -4.2582784 -4.2450118 -4.2384739 -4.2344394 -4.23718 -4.242219 -4.2470479 -4.2643867 -4.2916851 -4.3022671 -4.3031611 -4.3031411 -4.3001838 -4.3002682][-4.2816839 -4.2604289 -4.237113 -4.2175879 -4.2025566 -4.1930532 -4.1886716 -4.1896863 -4.2076793 -4.2423482 -4.2585635 -4.2624559 -4.2683783 -4.2737556 -4.2845993][-4.2887607 -4.2618895 -4.2294354 -4.2011938 -4.1745872 -4.1453838 -4.1191974 -4.1033125 -4.1202664 -4.169445 -4.2003956 -4.2151465 -4.2301221 -4.2448797 -4.2638206][-4.2943745 -4.2658577 -4.2311254 -4.1978655 -4.1579361 -4.1039662 -4.04562 -4.0024967 -4.017581 -4.0891328 -4.1424689 -4.1743059 -4.1997337 -4.2196341 -4.2415862][-4.2983708 -4.2721772 -4.2390094 -4.2035422 -4.15372 -4.0784841 -3.9862325 -3.9030452 -3.9103155 -4.0105934 -4.0875106 -4.1356993 -4.1714616 -4.1946774 -4.2160816][-4.2996588 -4.27757 -4.248558 -4.2180219 -4.173182 -4.0999775 -4.0007033 -3.9001079 -3.8910403 -3.9842498 -4.0562158 -4.1037664 -4.1409087 -4.161994 -4.1805038][-4.2971611 -4.2813177 -4.2626729 -4.2476678 -4.2213058 -4.1698241 -4.0995679 -4.0263934 -4.009779 -4.057251 -4.0887814 -4.1100802 -4.1285005 -4.1364069 -4.1441426][-4.2914352 -4.2804847 -4.2718153 -4.2689295 -4.2570872 -4.2261467 -4.1861563 -4.14551 -4.1349969 -4.1557503 -4.1545405 -4.146595 -4.1400547 -4.1286564 -4.1153593][-4.2863917 -4.2756968 -4.2698236 -4.2695103 -4.2618837 -4.2428875 -4.22291 -4.2064271 -4.2085934 -4.2222857 -4.2123914 -4.1920748 -4.1699448 -4.1422715 -4.1086621][-4.2888308 -4.2769451 -4.2724171 -4.2706022 -4.2611494 -4.2421994 -4.2291031 -4.225492 -4.2366872 -4.2501311 -4.2435822 -4.2259746 -4.2031078 -4.173378 -4.1364274][-4.2929139 -4.2779074 -4.2715507 -4.2676058 -4.2579751 -4.2387056 -4.2265668 -4.2272191 -4.239203 -4.2515349 -4.2514505 -4.24349 -4.226264 -4.2008018 -4.1709447][-4.2938313 -4.2752285 -4.2650785 -4.2609272 -4.2551751 -4.24162 -4.2337723 -4.2358828 -4.2439337 -4.2527275 -4.2539334 -4.2480736 -4.2328486 -4.2140217 -4.1960788]]...]
INFO - root - 2017-12-06 06:10:33.376159: step 11910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 76h:40m:43s remains)
INFO - root - 2017-12-06 06:10:41.983671: step 11920, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 75h:35m:36s remains)
INFO - root - 2017-12-06 06:10:50.552244: step 11930, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.874 sec/batch; 77h:52m:17s remains)
INFO - root - 2017-12-06 06:10:59.177983: step 11940, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 72h:27m:04s remains)
INFO - root - 2017-12-06 06:11:07.850307: step 11950, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 79h:58m:25s remains)
INFO - root - 2017-12-06 06:11:16.389357: step 11960, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.836 sec/batch; 74h:27m:54s remains)
INFO - root - 2017-12-06 06:11:24.952771: step 11970, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:44m:27s remains)
INFO - root - 2017-12-06 06:11:33.448931: step 11980, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:29m:06s remains)
INFO - root - 2017-12-06 06:11:42.012153: step 11990, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 77h:14m:02s remains)
INFO - root - 2017-12-06 06:11:50.569549: step 12000, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:13m:09s remains)
2017-12-06 06:11:51.350407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626514 -4.2500362 -4.2444415 -4.2410679 -4.2432804 -4.2467246 -4.2405539 -4.2248011 -4.2090359 -4.2062988 -4.2190156 -4.233017 -4.2396359 -4.2342758 -4.226367][-4.2405615 -4.2231212 -4.2173066 -4.2159667 -4.2230473 -4.2256193 -4.2138214 -4.1868548 -4.1635246 -4.1622591 -4.1793509 -4.1975875 -4.2049093 -4.1943879 -4.1844316][-4.2211452 -4.1980319 -4.1898203 -4.1905107 -4.2016788 -4.2025614 -4.1846008 -4.14989 -4.124845 -4.126677 -4.1452165 -4.1649237 -4.1719918 -4.1593823 -4.1514354][-4.2120695 -4.1883922 -4.17508 -4.1725793 -4.1833367 -4.182899 -4.1594934 -4.1245422 -4.105319 -4.1075664 -4.1189327 -4.1319237 -4.1397557 -4.13248 -4.12976][-4.2158294 -4.1912489 -4.1687169 -4.1551452 -4.1597977 -4.1560035 -4.1293139 -4.1013346 -4.0894403 -4.0848327 -4.0798984 -4.0806003 -4.09422 -4.103312 -4.1153851][-4.22504 -4.1993842 -4.1680722 -4.1410069 -4.1353312 -4.13036 -4.1048226 -4.081388 -4.0709276 -4.0516963 -4.0219808 -4.0065041 -4.0360842 -4.0750084 -4.1089139][-4.238101 -4.215126 -4.1818781 -4.1484427 -4.1365714 -4.1307821 -4.1064234 -4.0807781 -4.0648494 -4.0289879 -3.9757922 -3.9489527 -3.9939268 -4.0581217 -4.1124382][-4.2497077 -4.2311683 -4.2056618 -4.17502 -4.156949 -4.149147 -4.1246562 -4.0965385 -4.0768642 -4.0361257 -3.9823287 -3.9664934 -4.0132375 -4.0748315 -4.1301937][-4.2574463 -4.2398653 -4.2173553 -4.1859 -4.16274 -4.154253 -4.1366954 -4.1148529 -4.099206 -4.0663867 -4.0318236 -4.033659 -4.06868 -4.104362 -4.1416154][-4.2599244 -4.2395806 -4.2119913 -4.1761055 -4.152247 -4.1531549 -4.1508441 -4.1434035 -4.1362295 -4.1178455 -4.1022773 -4.1074867 -4.11943 -4.127059 -4.1435933][-4.2650948 -4.2439041 -4.2087526 -4.16956 -4.1463175 -4.1578283 -4.1699748 -4.1718779 -4.1721044 -4.1642938 -4.1570406 -4.1586604 -4.1522937 -4.1422029 -4.1457067][-4.2852492 -4.2646279 -4.2244573 -4.1827149 -4.1582055 -4.1727633 -4.1900558 -4.19213 -4.1927276 -4.1858873 -4.180172 -4.1801839 -4.1641145 -4.1482286 -4.1506805][-4.3046746 -4.2870073 -4.2503557 -4.212111 -4.1885624 -4.1978979 -4.2100244 -4.2048268 -4.2063937 -4.2018881 -4.1969161 -4.1950073 -4.1773438 -4.161727 -4.1650434][-4.323925 -4.3145013 -4.2863913 -4.2526293 -4.2298565 -4.2293754 -4.2291727 -4.2170119 -4.2165561 -4.2171578 -4.2199354 -4.2263894 -4.2186751 -4.2091179 -4.210362][-4.3397884 -4.3375449 -4.31438 -4.2827749 -4.2583103 -4.2499547 -4.2392168 -4.2268014 -4.2273369 -4.2355342 -4.247695 -4.2643352 -4.2656145 -4.2622881 -4.2624393]]...]
INFO - root - 2017-12-06 06:11:59.888361: step 12010, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 78h:07m:58s remains)
INFO - root - 2017-12-06 06:12:08.532990: step 12020, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 80h:15m:10s remains)
INFO - root - 2017-12-06 06:12:17.040059: step 12030, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 78h:06m:51s remains)
INFO - root - 2017-12-06 06:12:25.630410: step 12040, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.827 sec/batch; 73h:39m:33s remains)
INFO - root - 2017-12-06 06:12:34.130924: step 12050, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 74h:15m:20s remains)
INFO - root - 2017-12-06 06:12:42.640768: step 12060, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 76h:25m:26s remains)
INFO - root - 2017-12-06 06:12:51.139991: step 12070, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 75h:08m:18s remains)
INFO - root - 2017-12-06 06:12:59.744706: step 12080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 74h:44m:08s remains)
INFO - root - 2017-12-06 06:13:08.363157: step 12090, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 81h:01m:15s remains)
INFO - root - 2017-12-06 06:13:16.931512: step 12100, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 75h:16m:42s remains)
2017-12-06 06:13:17.744989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2493715 -4.2525287 -4.2419009 -4.2262745 -4.2215614 -4.2246594 -4.2252045 -4.2075987 -4.1735468 -4.1459165 -4.150034 -4.1849875 -4.223052 -4.2457762 -4.2541213][-4.2237296 -4.2156925 -4.1945858 -4.1770568 -4.1761661 -4.1800113 -4.1736169 -4.1430478 -4.1025653 -4.080565 -4.0983343 -4.144464 -4.1887679 -4.2158461 -4.2294564][-4.1952562 -4.1762433 -4.143465 -4.119514 -4.1202569 -4.1268 -4.1163955 -4.0834665 -4.0508437 -4.0454 -4.0780969 -4.1295629 -4.1741 -4.2022686 -4.2168922][-4.176816 -4.1488318 -4.10661 -4.077775 -4.0778456 -4.0840669 -4.0734229 -4.0501132 -4.0375481 -4.0535932 -4.097939 -4.1501637 -4.1923428 -4.2189927 -4.2287741][-4.1769485 -4.1458497 -4.1017714 -4.0694218 -4.0629582 -4.0628514 -4.0489969 -4.0375547 -4.0500975 -4.0843625 -4.1359377 -4.1863241 -4.2240248 -4.2466903 -4.2499166][-4.18969 -4.157207 -4.1137671 -4.0768642 -4.0637665 -4.0590076 -4.045197 -4.0484843 -4.0800633 -4.1214738 -4.1686258 -4.2101541 -4.2402997 -4.2576041 -4.2577996][-4.1964169 -4.1643186 -4.1229968 -4.0862932 -4.070641 -4.0641589 -4.053834 -4.0670185 -4.1080117 -4.14924 -4.1842294 -4.2117295 -4.231473 -4.2418008 -4.2423329][-4.1798835 -4.1505885 -4.1158652 -4.0867882 -4.0721974 -4.0671206 -4.0658751 -4.0870137 -4.1298275 -4.1646 -4.1880822 -4.202055 -4.2094927 -4.2117686 -4.2138429][-4.1428566 -4.117588 -4.0971227 -4.0841193 -4.078371 -4.0775156 -4.0857906 -4.1097794 -4.1467681 -4.17481 -4.1910477 -4.1961656 -4.1925054 -4.186697 -4.1887741][-4.1180696 -4.0965376 -4.0872774 -4.0904031 -4.0956249 -4.0994682 -4.1131549 -4.1356764 -4.1623621 -4.1836185 -4.1947455 -4.1922126 -4.1765575 -4.1632581 -4.1664066][-4.12244 -4.1050858 -4.1025028 -4.1127739 -4.1226716 -4.1301017 -4.1481047 -4.1677132 -4.1846747 -4.1983256 -4.2040758 -4.1901565 -4.1610966 -4.1431274 -4.15369][-4.1583395 -4.1415873 -4.1367936 -4.1440334 -4.1524525 -4.1626906 -4.1790075 -4.1911912 -4.1983414 -4.2069449 -4.2095027 -4.18824 -4.150825 -4.1350908 -4.1593947][-4.1982431 -4.1784692 -4.1681685 -4.1676641 -4.174736 -4.1863122 -4.1963668 -4.1969056 -4.1944575 -4.1992068 -4.2006187 -4.178225 -4.141881 -4.1389322 -4.176311][-4.2050858 -4.1806335 -4.1722612 -4.1764088 -4.1885867 -4.2016382 -4.2032828 -4.1904964 -4.1805406 -4.1846471 -4.1865149 -4.1663132 -4.1387992 -4.1512194 -4.1962018][-4.1781425 -4.1586208 -4.16385 -4.1807551 -4.1977916 -4.2090917 -4.2039747 -4.18523 -4.1738272 -4.1802611 -4.1878023 -4.1760936 -4.1626415 -4.1887646 -4.2338686]]...]
INFO - root - 2017-12-06 06:13:26.218853: step 12110, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 75h:49m:35s remains)
INFO - root - 2017-12-06 06:13:34.857270: step 12120, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 78h:28m:24s remains)
INFO - root - 2017-12-06 06:13:43.283303: step 12130, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 77h:12m:45s remains)
INFO - root - 2017-12-06 06:13:51.879843: step 12140, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 75h:34m:02s remains)
INFO - root - 2017-12-06 06:14:00.553847: step 12150, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 77h:33m:22s remains)
INFO - root - 2017-12-06 06:14:09.111508: step 12160, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:08m:37s remains)
INFO - root - 2017-12-06 06:14:17.608428: step 12170, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 80h:14m:13s remains)
INFO - root - 2017-12-06 06:14:26.262336: step 12180, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 76h:08m:11s remains)
INFO - root - 2017-12-06 06:14:34.863110: step 12190, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 77h:27m:24s remains)
INFO - root - 2017-12-06 06:14:43.455111: step 12200, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 77h:04m:55s remains)
2017-12-06 06:14:44.312958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2970848 -4.3003936 -4.3009071 -4.3005061 -4.3008432 -4.3013396 -4.3017716 -4.3030925 -4.3026314 -4.2979503 -4.2947187 -4.2968206 -4.3030696 -4.3102255 -4.3142037][-4.2749338 -4.2808838 -4.2838345 -4.2829456 -4.2795358 -4.2753797 -4.2743392 -4.27957 -4.28703 -4.2885289 -4.2904339 -4.2968936 -4.3068194 -4.3149958 -4.3165107][-4.23781 -4.2442746 -4.2492223 -4.2471457 -4.2383146 -4.2258978 -4.2222328 -4.2345085 -4.2538738 -4.2652516 -4.275454 -4.2891459 -4.3042555 -4.3146873 -4.3148417][-4.1910839 -4.1951127 -4.2004619 -4.1958752 -4.1806459 -4.1586261 -4.1482949 -4.1638865 -4.1965151 -4.2200918 -4.2389917 -4.2618809 -4.2860804 -4.3004618 -4.2998447][-4.1425695 -4.1450229 -4.1522183 -4.1476531 -4.1280766 -4.0919695 -4.0632243 -4.0698071 -4.1126413 -4.1503072 -4.1833658 -4.2203026 -4.2558889 -4.2748351 -4.2715755][-4.0931115 -4.0955138 -4.1032152 -4.0965137 -4.0678267 -4.0089808 -3.9457469 -3.9326744 -3.9920158 -4.0567513 -4.112566 -4.1689849 -4.2172503 -4.2423673 -4.23865][-4.0512791 -4.0459528 -4.04457 -4.0320826 -3.9922762 -3.9105234 -3.8084705 -3.7735238 -3.8543434 -3.9556437 -4.0402079 -4.1114688 -4.167532 -4.2003489 -4.2044315][-4.0255179 -4.0065403 -3.9907334 -3.972352 -3.9378541 -3.8642955 -3.7605689 -3.72087 -3.8077531 -3.922765 -4.0147762 -4.0814943 -4.1304135 -4.1629229 -4.1767254][-4.0210857 -3.9940548 -3.9731848 -3.9584816 -3.9476752 -3.91718 -3.8580296 -3.83309 -3.8888669 -3.97398 -4.0440884 -4.090292 -4.1205378 -4.1431527 -4.1597538][-4.0412908 -4.0164075 -4.0024509 -3.997668 -4.006835 -4.0081224 -3.9836752 -3.9681792 -3.9930539 -4.0398254 -4.0816355 -4.1104178 -4.1294742 -4.1431656 -4.1570425][-4.0711865 -4.0551381 -4.0505304 -4.0533104 -4.0682793 -4.080255 -4.0708413 -4.0564833 -4.0632396 -4.0856681 -4.1091957 -4.1300378 -4.147995 -4.1618533 -4.1721349][-4.0969234 -4.0887728 -4.0914836 -4.0979586 -4.1123681 -4.1259174 -4.1221123 -4.11204 -4.1143847 -4.12727 -4.1414323 -4.157423 -4.1753235 -4.1892991 -4.1984949][-4.1209135 -4.11701 -4.12461 -4.1342492 -4.1469836 -4.1573653 -4.1555486 -4.149539 -4.1517029 -4.1616015 -4.1747584 -4.1907377 -4.2074027 -4.2203526 -4.2287545][-4.1541896 -4.1523628 -4.1614985 -4.1716819 -4.1813354 -4.1864476 -4.1836796 -4.1806226 -4.183145 -4.1912861 -4.2031 -4.2181706 -4.233182 -4.2449327 -4.25241][-4.2013431 -4.2002406 -4.2074461 -4.2151952 -4.220932 -4.2221107 -4.2188392 -4.216784 -4.2189894 -4.2247286 -4.2330728 -4.2436347 -4.2542486 -4.2630582 -4.2686672]]...]
INFO - root - 2017-12-06 06:14:52.853519: step 12210, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 76h:04m:35s remains)
INFO - root - 2017-12-06 06:15:01.445612: step 12220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 75h:31m:46s remains)
INFO - root - 2017-12-06 06:15:10.069183: step 12230, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 77h:52m:14s remains)
INFO - root - 2017-12-06 06:15:18.641384: step 12240, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 75h:48m:22s remains)
INFO - root - 2017-12-06 06:15:27.180708: step 12250, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 74h:20m:10s remains)
INFO - root - 2017-12-06 06:15:35.717756: step 12260, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 75h:47m:39s remains)
INFO - root - 2017-12-06 06:15:44.169590: step 12270, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 76h:38m:25s remains)
INFO - root - 2017-12-06 06:15:52.652713: step 12280, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 73h:51m:40s remains)
INFO - root - 2017-12-06 06:16:01.206761: step 12290, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 75h:23m:03s remains)
INFO - root - 2017-12-06 06:16:09.792393: step 12300, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 77h:38m:49s remains)
2017-12-06 06:16:10.559874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2708783 -4.2657857 -4.2626953 -4.2597661 -4.2535653 -4.2413621 -4.231431 -4.2359858 -4.2455769 -4.2541623 -4.2579656 -4.2611794 -4.2627568 -4.2674408 -4.2760758][-4.2519546 -4.2515974 -4.2489619 -4.2450237 -4.2385893 -4.2263045 -4.2157745 -4.220366 -4.2314291 -4.2400188 -4.2424884 -4.2468042 -4.2525616 -4.2635703 -4.2791452][-4.2497077 -4.2524233 -4.2469392 -4.2379694 -4.2292109 -4.2183003 -4.2110662 -4.217998 -4.2284026 -4.2342491 -4.2354546 -4.2412128 -4.247633 -4.2584462 -4.2753997][-4.2401857 -4.2438922 -4.2358193 -4.2222023 -4.2119889 -4.2037835 -4.1992764 -4.2073159 -4.2187123 -4.2231746 -4.2229867 -4.2298121 -4.2367334 -4.2449031 -4.2593985][-4.212862 -4.2223206 -4.2192717 -4.2061138 -4.1975093 -4.1888194 -4.1775675 -4.1790309 -4.1886725 -4.1923933 -4.1906466 -4.198205 -4.2049985 -4.2097287 -4.2168174][-4.1778994 -4.19723 -4.1998682 -4.1877985 -4.1770878 -4.160542 -4.1346021 -4.1255722 -4.1366811 -4.1469851 -4.1457038 -4.1512117 -4.155127 -4.155807 -4.15211][-4.1532755 -4.1788678 -4.1805878 -4.1642475 -4.1446652 -4.1153479 -4.071414 -4.0474324 -4.061904 -4.0840139 -4.0880322 -4.0943131 -4.1000586 -4.0967846 -4.0841193][-4.146595 -4.1670861 -4.1609197 -4.1348786 -4.1038928 -4.06367 -4.0127039 -3.984714 -4.0092735 -4.0456581 -4.0596662 -4.0675273 -4.07152 -4.067863 -4.0563712][-4.1624341 -4.1726613 -4.1646161 -4.1407347 -4.113667 -4.0831895 -4.0502086 -4.0356536 -4.0603824 -4.0956798 -4.1141629 -4.11992 -4.1165929 -4.1092343 -4.1041236][-4.1883035 -4.1910233 -4.1863303 -4.1724558 -4.1567321 -4.1406631 -4.1239638 -4.1189833 -4.13215 -4.1530328 -4.1673493 -4.1697245 -4.1613531 -4.1522303 -4.155117][-4.2037992 -4.2022252 -4.2004976 -4.1959782 -4.1928453 -4.1876173 -4.1807089 -4.1794324 -4.1847911 -4.19548 -4.20511 -4.2064438 -4.1981044 -4.19049 -4.1981606][-4.2247195 -4.2200813 -4.2191582 -4.2211609 -4.2261629 -4.227675 -4.2251115 -4.2244534 -4.2242613 -4.2281256 -4.2327032 -4.2336407 -4.2265034 -4.2194571 -4.2269211][-4.2443237 -4.2397928 -4.240407 -4.2445583 -4.2492747 -4.2514057 -4.2516022 -4.2496881 -4.2449074 -4.2448158 -4.2483768 -4.2501721 -4.2453938 -4.2393169 -4.2425022][-4.2693825 -4.2658072 -4.2661285 -4.268908 -4.2705007 -4.2702913 -4.2702127 -4.2677345 -4.2626925 -4.2613006 -4.2624359 -4.2630634 -4.2599545 -4.2554932 -4.2552443][-4.2862043 -4.2846813 -4.2856011 -4.2861729 -4.2848945 -4.2818761 -4.2796764 -4.2761483 -4.2708883 -4.2684107 -4.2682829 -4.2687168 -4.2679462 -4.2669797 -4.26734]]...]
INFO - root - 2017-12-06 06:16:19.048167: step 12310, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 76h:06m:14s remains)
INFO - root - 2017-12-06 06:16:27.536055: step 12320, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 76h:06m:43s remains)
INFO - root - 2017-12-06 06:16:35.998990: step 12330, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 76h:30m:06s remains)
INFO - root - 2017-12-06 06:16:44.541319: step 12340, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 75h:39m:23s remains)
INFO - root - 2017-12-06 06:16:53.155515: step 12350, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 74h:08m:01s remains)
INFO - root - 2017-12-06 06:17:01.697708: step 12360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:11m:07s remains)
INFO - root - 2017-12-06 06:17:10.223739: step 12370, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 76h:08m:58s remains)
INFO - root - 2017-12-06 06:17:18.672621: step 12380, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 77h:13m:35s remains)
INFO - root - 2017-12-06 06:17:27.245098: step 12390, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 77h:32m:36s remains)
INFO - root - 2017-12-06 06:17:35.853185: step 12400, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 79h:05m:11s remains)
2017-12-06 06:17:36.627262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1824727 -4.2045684 -4.2373719 -4.2638736 -4.2739062 -4.2710834 -4.2555308 -4.2215619 -4.1816173 -4.1648121 -4.1594248 -4.1583338 -4.1607528 -4.1295404 -4.0690231][-4.125042 -4.1705222 -4.2213979 -4.2560563 -4.2658916 -4.2553306 -4.2342305 -4.2041879 -4.173573 -4.1618743 -4.1579142 -4.1577511 -4.1620946 -4.12728 -4.0605679][-4.085845 -4.1560349 -4.2186174 -4.2487407 -4.2494078 -4.229497 -4.2113008 -4.1969032 -4.1879978 -4.1845341 -4.1825147 -4.1831 -4.1871939 -4.1513739 -4.0789709][-4.0656548 -4.1572089 -4.2216458 -4.2376728 -4.2225108 -4.194427 -4.1847496 -4.1897492 -4.1975121 -4.2026377 -4.2007675 -4.2050118 -4.2094107 -4.1775265 -4.1028438][-4.0670924 -4.1662722 -4.2264509 -4.2308378 -4.1976485 -4.1637058 -4.1595283 -4.1760654 -4.1911397 -4.2034688 -4.2087755 -4.2233586 -4.2337708 -4.20342 -4.1222878][-4.0969334 -4.1841106 -4.2338405 -4.2251558 -4.17628 -4.1344724 -4.1289787 -4.1507826 -4.1706085 -4.1884403 -4.2023263 -4.2285795 -4.2466016 -4.2184038 -4.1425858][-4.1583858 -4.2147894 -4.24698 -4.227612 -4.1690769 -4.1208811 -4.107964 -4.1253963 -4.1406918 -4.1563406 -4.1797061 -4.2182312 -4.24378 -4.2200422 -4.1582203][-4.2187524 -4.2446113 -4.2593765 -4.2400064 -4.1853375 -4.1419082 -4.124258 -4.1290936 -4.1294079 -4.13285 -4.1576509 -4.2030153 -4.231101 -4.2120905 -4.1655183][-4.2616277 -4.2658453 -4.2738709 -4.262331 -4.2197247 -4.1890454 -4.1743188 -4.1668987 -4.1506987 -4.1430507 -4.159976 -4.20153 -4.226994 -4.2093649 -4.1716781][-4.2861767 -4.2816963 -4.28393 -4.27836 -4.2516847 -4.2346396 -4.2253718 -4.2157588 -4.1966915 -4.1834908 -4.1890988 -4.2174473 -4.235095 -4.2174888 -4.1856771][-4.2948036 -4.2893867 -4.2902269 -4.2880793 -4.2749448 -4.2674623 -4.2624865 -4.2559261 -4.2432694 -4.2301755 -4.2266984 -4.2408047 -4.2491093 -4.2356958 -4.2103567][-4.3052597 -4.3006592 -4.2993622 -4.2954531 -4.2877879 -4.2848544 -4.28437 -4.2839031 -4.2776728 -4.2683144 -4.2627931 -4.2672391 -4.2699461 -4.2622375 -4.2441487][-4.3192425 -4.3177605 -4.3172154 -4.3133221 -4.3071222 -4.3037777 -4.3030515 -4.3032355 -4.30192 -4.2981553 -4.2944465 -4.2956905 -4.2972612 -4.2933469 -4.281024][-4.32873 -4.3287048 -4.3291807 -4.3277197 -4.3235755 -4.3204632 -4.3189774 -4.3184428 -4.3181524 -4.3181291 -4.3178291 -4.318511 -4.3188024 -4.3166318 -4.3095074][-4.3311229 -4.3310075 -4.3318219 -4.3330545 -4.3325062 -4.3317533 -4.330996 -4.3307304 -4.3309069 -4.3317637 -4.3320355 -4.3311057 -4.3295116 -4.3278804 -4.3252516]]...]
INFO - root - 2017-12-06 06:17:45.132545: step 12410, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 73h:37m:39s remains)
INFO - root - 2017-12-06 06:17:53.684457: step 12420, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 76h:37m:42s remains)
INFO - root - 2017-12-06 06:18:02.274799: step 12430, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 78h:07m:59s remains)
INFO - root - 2017-12-06 06:18:10.887816: step 12440, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 78h:04m:24s remains)
