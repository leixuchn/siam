INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "212"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
111111111 Tensor("siamese_fc/conv5/concat:0", shape=(8, 6, 6, 256), dtype=float32) Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 6, 6), dtype=float32)
sdiufhasudf Tensor("siamese_fc/conv5/b1/BiasAdd:0", shape=(8, 6, 6, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 09:59:30.996277: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:59:30.996314: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:59:30.996321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:59:30.996325: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:59:30.996329: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 09:59:31.617644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-11 09:59:31.617679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 09:59:31.617687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 09:59:31.617695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-11 09:59:37.160663: step 0, loss = 0.22, batch loss = 0.14 (2.0 examples/sec; 3.976 sec/batch; 367h:15m:10s remains)
111111111 Tensor("siamese_fc_1/conv5/concat:0", shape=(8, 20, 20, 256), dtype=float32) Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 20, 20), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/b1/BiasAdd:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-11 09:59:37.808534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1727028 -4.647747 -5.4494467 -6.3287868 -6.8939877 -7.0165739 -6.7712965 -6.4352474 -6.261497 -6.4251103 -6.7710533 -7.0342951 -7.1250343 -6.7778611 -6.0340252][-4.4631462 -5.1326942 -6.186759 -7.1440182 -7.4876862 -7.2241378 -6.6899495 -6.2911739 -6.1880875 -6.5451064 -7.0925255 -7.4985008 -7.7321215 -7.4480195 -6.6391811][-4.5899544 -5.4064684 -6.6046848 -7.3746834 -7.1783652 -6.2484922 -5.2193561 -4.6105795 -4.4936194 -5.0963812 -6.00346 -6.7896957 -7.4474664 -7.4912844 -6.821619][-4.4579091 -5.3488054 -6.5866375 -7.0110316 -6.164052 -4.5090308 -2.8716869 -1.8557484 -1.6210003 -2.5477972 -4.0119424 -5.4142942 -6.6862726 -7.2178612 -6.7960596][-4.0376949 -4.9773793 -6.227736 -6.309505 -4.887167 -2.5617638 -0.18033886 1.5136347 1.9708104 0.62742376 -1.5889087 -3.7829659 -5.7750716 -6.8725891 -6.7476521][-3.4034066 -4.389679 -5.5857668 -5.3292732 -3.4512711 -0.52739668 2.6776786 5.1911449 5.8290005 3.9128494 0.80724669 -2.2060134 -4.801825 -6.3532929 -6.5062218][-2.7859278 -3.7192087 -4.715044 -4.2048817 -2.2081683 0.90291023 4.5226088 7.4824944 8.0523911 5.6186934 1.9084229 -1.5262678 -4.2758193 -5.9218206 -6.1777534][-2.4311585 -3.2809813 -4.0917244 -3.6287429 -2.0368645 0.62020874 3.9113264 6.6079617 6.8894978 4.3775511 0.73358059 -2.4516573 -4.757875 -6.0162477 -6.0770903][-2.5382495 -3.2555771 -3.946039 -3.8013692 -2.8911605 -0.9457171 1.6615438 3.7326603 3.8080044 1.6969843 -1.3598742 -3.9026628 -5.531352 -6.2514219 -6.05097][-2.9473929 -3.4330947 -4.0071025 -4.236824 -3.9378412 -2.5666165 -0.55740595 0.91946554 0.91564751 -0.63464022 -2.9692755 -4.8581867 -5.9239244 -6.2279029 -5.8656449][-3.4750676 -3.7605672 -4.2213659 -4.6882319 -4.7357411 -3.7023208 -2.1810184 -1.2415993 -1.3219218 -2.3628938 -4.023078 -5.3729529 -6.0350876 -6.0581589 -5.6023359][-3.9480305 -4.1575851 -4.5787458 -5.1688981 -5.3306365 -4.4635892 -3.2802143 -2.7146294 -2.8243275 -3.4257107 -4.539638 -5.52367 -5.9888282 -5.9093604 -5.4426351][-3.9523749 -4.245573 -4.7654414 -5.4808288 -5.6710749 -4.9004149 -3.9055252 -3.4892783 -3.5210452 -3.7138653 -4.4348059 -5.24543 -5.7149191 -5.7096906 -5.3137183][-3.7514982 -4.1245508 -4.7153921 -5.4761639 -5.6064458 -4.8364987 -3.8631968 -3.4145932 -3.2858663 -3.1382847 -3.6158252 -4.4179907 -5.0742707 -5.3213511 -5.0945868][-3.2372284 -3.6542511 -4.31714 -5.1224933 -5.194118 -4.323863 -3.2171836 -2.6706533 -2.4461102 -2.1556304 -2.5801094 -3.502475 -4.4127531 -4.9171019 -4.8448806]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 09:59:42.829314: step 10, loss = 0.38, batch loss = 0.30 (17.7 examples/sec; 0.451 sec/batch; 41h:39m:44s remains)
INFO - root - 2017-12-11 09:59:47.306389: step 20, loss = 0.25, batch loss = 0.16 (17.4 examples/sec; 0.459 sec/batch; 42h:25m:39s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut
INFO - root - 2017-12-11 09:59:51.837218: step 30, loss = 0.26, batch loss = 0.18 (16.7 examples/sec; 0.479 sec/batch; 44h:12m:04s remains)
INFO - root - 2017-12-11 09:59:56.354987: step 40, loss = 0.31, batch loss = 0.23 (17.9 examples/sec; 0.448 sec/batch; 41h:20m:27s remains)
INFO - root - 2017-12-11 10:00:00.784043: step 50, loss = 0.33, batch loss = 0.25 (18.1 examples/sec; 0.441 sec/batch; 40h:44m:52s remains)
INFO - root - 2017-12-11 10:00:05.298015: step 60, loss = 0.51, batch loss = 0.43 (17.5 examples/sec; 0.456 sec/batch; 42h:06m:43s remains)
INFO - root - 2017-12-11 10:00:09.735796: step 70, loss = 0.23, batch loss = 0.15 (18.9 examples/sec; 0.424 sec/batch; 39h:07m:54s remains)
INFO - root - 2017-12-11 10:00:14.191233: step 80, loss = 0.26, batch loss = 0.18 (17.5 examples/sec; 0.457 sec/batch; 42h:11m:26s remains)
INFO - root - 2017-12-11 10:00:18.671965: step 90, loss = 0.27, batch loss = 0.18 (18.2 examples/sec; 0.440 sec/batch; 40h:40m:25s remains)
INFO - root - 2017-12-11 10:00:23.191606: step 100, loss = 0.23, batch loss = 0.15 (17.7 examples/sec; 0.451 sec/batch; 41h:38m:52s remains)
2017-12-11 10:00:23.667182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5146508 -3.3309231 -3.832459 -3.5061932 -2.8715363 -2.5658617 -2.3602004 -1.8946626 -1.4542518 -1.5124476 -1.9034631 -2.6414442 -3.1528754 -2.5729408 -2.6224484][-2.8911076 -4.1041789 -4.776526 -4.3833413 -3.5681558 -3.0540886 -2.8412485 -2.5168359 -2.2319157 -2.3183129 -2.6221986 -3.2566192 -3.5785365 -2.9677758 -2.9629054][-3.774605 -5.1741486 -5.8635983 -5.3673749 -4.3538322 -3.5220416 -3.1465366 -2.8295758 -2.5624986 -2.6486368 -2.9236865 -3.4430308 -3.5856097 -3.0581079 -3.124444][-4.5353851 -5.754138 -6.2039995 -5.5239887 -4.2992811 -3.0977521 -2.5068178 -2.2680175 -2.1623771 -2.4042161 -2.8244679 -3.3601232 -3.4458995 -3.07678 -3.2654371][-4.62507 -5.2700038 -5.1544333 -4.0691118 -2.5327682 -0.97468066 -0.28536463 -0.43046212 -0.89278531 -1.6698909 -2.5252705 -3.2678332 -3.3907962 -3.1208711 -3.3601985][-4.0942154 -3.9247489 -3.049221 -1.4775186 0.264915 1.9004312 2.3626237 1.5122995 0.1813035 -1.3339586 -2.7056017 -3.6099908 -3.6478817 -3.2639246 -3.3714163][-3.4090171 -2.5731294 -1.0245781 0.93212509 2.7494984 4.2489672 4.358428 2.8514981 0.78472519 -1.3668971 -3.208838 -4.2301307 -4.125464 -3.5094562 -3.353806][-3.0318637 -1.9251683 -0.059215069 2.0317936 3.7668219 5.0100689 4.809639 2.9754624 0.65481853 -1.6952634 -3.7050741 -4.7289877 -4.5033875 -3.6916709 -3.3045352][-3.2839942 -2.3583343 -0.63231754 1.2800775 2.8197227 3.8135433 3.448935 1.7301726 -0.28079653 -2.328536 -4.2008867 -5.1520929 -4.8459749 -3.8602858 -3.2594302][-4.0581894 -3.55549 -2.2300582 -0.68739057 0.59606266 1.3828158 0.9991436 -0.32236576 -1.7231271 -3.2259746 -4.8219843 -5.6756225 -5.3146648 -4.1786141 -3.3696113][-5.0169683 -4.8244152 -3.8649967 -2.7472978 -1.7619085 -1.1758122 -1.5523715 -2.521069 -3.3854415 -4.3519354 -5.5904794 -6.2292633 -5.6937771 -4.3301268 -3.2121511][-5.3531756 -5.3746448 -4.7491026 -4.0564561 -3.3860142 -2.9732494 -3.303544 -4.0087967 -4.5124421 -5.0793548 -5.9784646 -6.3475485 -5.5690012 -3.995553 -2.6331611][-4.808629 -4.9675155 -4.6685729 -4.3715234 -4.0151892 -3.7375269 -3.9667096 -4.4416747 -4.6959929 -4.9963465 -5.5996861 -5.7043772 -4.7590923 -3.1476009 -1.7913377][-3.9754305 -4.1912141 -4.1347408 -4.0798974 -3.9045217 -3.6807036 -3.7697024 -4.0454354 -4.1564541 -4.3492794 -4.7291059 -4.5898762 -3.5578823 -2.0762608 -0.93922639][-3.4007072 -3.6258631 -3.7048419 -3.7479367 -3.6452966 -3.4389887 -3.3994355 -3.5298181 -3.5832171 -3.759541 -3.9617155 -3.6271944 -2.60738 -1.3652258 -0.523587]]...]
INFO - root - 2017-12-11 10:00:28.105493: step 110, loss = 0.29, batch loss = 0.21 (18.0 examples/sec; 0.444 sec/batch; 40h:58m:22s remains)
INFO - root - 2017-12-11 10:00:32.306424: step 120, loss = 0.31, batch loss = 0.23 (17.4 examples/sec; 0.460 sec/batch; 42h:27m:16s remains)
INFO - root - 2017-12-11 10:00:36.735326: step 130, loss = 0.30, batch loss = 0.22 (18.4 examples/sec; 0.436 sec/batch; 40h:13m:31s remains)
INFO - root - 2017-12-11 10:00:41.155853: step 140, loss = 0.27, batch loss = 0.19 (17.9 examples/sec; 0.446 sec/batch; 41h:11m:00s remains)
INFO - root - 2017-12-11 10:00:45.642911: step 150, loss = 0.37, batch loss = 0.28 (17.4 examples/sec; 0.460 sec/batch; 42h:28m:53s remains)
INFO - root - 2017-12-11 10:00:50.125087: step 160, loss = 0.28, batch loss = 0.20 (18.4 examples/sec; 0.434 sec/batch; 40h:03m:56s remains)
INFO - root - 2017-12-11 10:00:54.613693: step 170, loss = 0.35, batch loss = 0.26 (17.2 examples/sec; 0.465 sec/batch; 42h:53m:09s remains)
INFO - root - 2017-12-11 10:00:59.155215: step 180, loss = 0.26, batch loss = 0.17 (18.1 examples/sec; 0.442 sec/batch; 40h:50m:26s remains)
INFO - root - 2017-12-11 10:01:03.646972: step 190, loss = 0.30, batch loss = 0.22 (17.9 examples/sec; 0.446 sec/batch; 41h:09m:48s remains)
INFO - root - 2017-12-11 10:01:08.159994: step 200, loss = 0.26, batch loss = 0.18 (17.5 examples/sec; 0.457 sec/batch; 42h:09m:30s remains)
2017-12-11 10:01:08.601133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.2907133 -7.4655547 -7.1619015 -6.7493687 -6.1786823 -5.5174265 -5.0488281 -4.6895804 -4.3789635 -4.3166294 -4.4374337 -4.5621691 -4.6208973 -4.7059269 -4.7514462][-7.5365582 -7.7237949 -7.3953896 -6.9306383 -6.1966953 -5.2934532 -4.6663823 -4.2564406 -3.9554977 -3.9313302 -4.0568819 -4.1990261 -4.3373332 -4.4909124 -4.6602373][-6.1102715 -6.2612095 -6.0690646 -5.7236185 -4.9286208 -3.8769755 -3.2597821 -3.0426643 -3.0453939 -3.2746406 -3.429426 -3.5149908 -3.591608 -3.6090713 -3.6740966][-4.5775928 -4.6302948 -4.5322828 -4.2596936 -3.3581285 -2.1077561 -1.4329691 -1.472903 -2.0069768 -2.7057686 -2.9669604 -2.9453921 -2.8065712 -2.5067878 -2.3653214][-3.787972 -3.6575582 -3.4893456 -3.1799583 -2.1518512 -0.58518434 0.48804474 0.52669764 -0.45889282 -1.7049615 -2.2348077 -2.2663016 -2.0369191 -1.5907547 -1.4487317][-2.9396429 -2.3889265 -1.914346 -1.4721169 -0.37508726 1.5215459 3.2145243 3.5874014 2.2323623 0.38170767 -0.6014874 -0.85202622 -0.723006 -0.35290289 -0.2722249][-1.9469609 -0.82966614 0.027740002 0.57225752 1.6426463 3.7544346 5.9388189 6.6248617 4.8960295 2.453846 0.98943853 0.49516487 0.48329353 0.70428896 0.72788668][-1.4563856 -0.067425728 0.93400955 1.4493604 2.4540896 4.6102982 6.9174337 7.7124748 5.805481 3.0601583 1.2611108 0.51451397 0.42123938 0.6857481 0.8708396][-2.487462 -1.2294185 -0.26128054 0.23834562 1.1369872 3.0059443 4.8908978 5.5403013 4.0548553 1.7954631 0.21988297 -0.5628767 -0.64427114 -0.24480915 0.14113855][-4.0193048 -3.0079389 -2.0936158 -1.5681536 -0.83460283 0.49979973 1.6942797 2.1236062 1.2674012 -0.13990736 -1.1439004 -1.793474 -1.8465433 -1.3560197 -0.87303352][-5.0650721 -4.2825055 -3.4731016 -2.9719152 -2.3756571 -1.5142863 -0.90717006 -0.66887069 -1.0766194 -1.8247402 -2.3236048 -2.7277293 -2.6571503 -2.0757947 -1.561908][-5.8813653 -5.2721519 -4.5487466 -4.0559983 -3.5190196 -3.0184946 -2.8413625 -2.7379541 -2.8711362 -3.188611 -3.299052 -3.4254346 -3.2704816 -2.749711 -2.3233192][-6.2859759 -5.845727 -5.2556672 -4.8348136 -4.4528461 -4.2817373 -4.3786831 -4.3807349 -4.4194322 -4.5904045 -4.5253086 -4.4345946 -4.218111 -3.79347 -3.4395912][-6.683383 -6.4935756 -6.0783234 -5.750349 -5.5367908 -5.5882907 -5.7958975 -5.8222303 -5.8109741 -5.9524355 -5.870285 -5.6695786 -5.37597 -4.94728 -4.5750952][-6.7843351 -6.80879 -6.5191231 -6.19236 -5.9770131 -6.0068226 -6.1666589 -6.1966863 -6.1877913 -6.3244967 -6.3012228 -6.1518874 -5.9144878 -5.5667973 -5.23917]]...]
INFO - root - 2017-12-11 10:01:13.074129: step 210, loss = 0.34, batch loss = 0.26 (18.2 examples/sec; 0.440 sec/batch; 40h:34m:59s remains)
INFO - root - 2017-12-11 10:01:17.534675: step 220, loss = 0.31, batch loss = 0.22 (18.2 examples/sec; 0.440 sec/batch; 40h:35m:37s remains)
