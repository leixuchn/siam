INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "191"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-10 07:13:03.487254: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:13:03.487286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:13:03.487293: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:13:03.487297: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:13:03.487300: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:13:04.165769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 07:13:04.165800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 07:13:04.165807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 07:13:04.165815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 07:13:09.176100: step 0, loss = 0.75, batch loss = 0.69 (2.4 examples/sec; 3.298 sec/batch; 304h:36m:24s remains)
2017-12-10 07:13:09.651479: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00019173665 0.0002041433 0.00021593303 0.00022831882 0.00024030718 0.00024995845 0.00025898375 0.00027309271 0.00028612991 0.00029004843 0.00028113145 0.0002644663 0.00024061672 0.0002145114 0.00019040231][0.00020492819 0.00022157014 0.00023685361 0.00025464475 0.00027501085 0.00029530478 0.00031571454 0.00033774727 0.00035408928 0.00035638691 0.00034093185 0.00031209318 0.00027594471 0.00023937765 0.00020688942][0.00021613667 0.00023447713 0.00025235247 0.00027621345 0.00031061858 0.00034586861 0.00037834918 0.00040973944 0.00043289343 0.00043205812 0.00040531941 0.00036101628 0.00030987692 0.000262033 0.00022149077][0.00022710049 0.00024539526 0.00026362325 0.00029397573 0.00033976513 0.00038641563 0.00042926279 0.00047698765 0.00051152217 0.00050893857 0.00047062512 0.00040943827 0.00034377785 0.0002840473 0.00023565898][0.0002374436 0.00025495325 0.00027238362 0.00030768767 0.00036415082 0.00042235889 0.00048153629 0.0005538782 0.00060216163 0.00059286336 0.00053902844 0.00045919712 0.00037639722 0.00030344454 0.00024754231][0.00025108657 0.00026940487 0.00028812583 0.00032657647 0.00038952174 0.0004570765 0.00053329195 0.00063500158 0.00069732359 0.00067088177 0.00059317186 0.0004937367 0.00039661987 0.00031471957 0.00025446439][0.00026773041 0.00028762373 0.00030880756 0.00034957481 0.00041271842 0.00048575629 0.00058134878 0.00071212807 0.00078181509 0.00071960746 0.00061319344 0.00050232979 0.00040149988 0.000316847 0.00025604136][0.00028639319 0.00030971554 0.00033463773 0.00037514188 0.00043276744 0.00050428486 0.00060540868 0.00073752442 0.00078349403 0.00068745494 0.00057398574 0.0004756402 0.00038608554 0.00030812671 0.00025198943][0.00029729283 0.00032368282 0.00035053719 0.00038723662 0.00043398846 0.00049265387 0.0005698945 0.00065755547 0.00066071085 0.00057725707 0.00049712823 0.00042852355 0.00035782874 0.00029233756 0.00024478821][0.00029799511 0.00032656474 0.0003525333 0.00038511289 0.00042253264 0.00046236994 0.00050779904 0.0005514787 0.00053651741 0.00048091426 0.00043237308 0.00038740126 0.00033283647 0.00027852919 0.00023760981][0.00029370043 0.0003217851 0.00034623183 0.00037402663 0.00040159031 0.00042347269 0.00044594405 0.00046372702 0.00044510086 0.00040993924 0.00038266578 0.00035186543 0.00030859475 0.00026417922 0.00022996488][0.0002851515 0.0003093472 0.00032981238 0.000349965 0.00036550703 0.00037337907 0.0003851802 0.00039344467 0.00037841787 0.00035894435 0.00034731955 0.00032449182 0.000288219 0.00025129906 0.00022319991][0.00027573263 0.00029330928 0.00030565693 0.00031650692 0.00032394551 0.00032844991 0.00033911137 0.00035048646 0.00034612286 0.00033865156 0.00033488538 0.00031438548 0.00027964963 0.00024487035 0.00021984724][0.00026635278 0.00027832846 0.00028551847 0.00029209143 0.00029827681 0.00030603312 0.00031974688 0.00033667788 0.00034198546 0.0003431027 0.00034304769 0.00031923104 0.00028148736 0.00024483062 0.00022028261][0.00026441994 0.00027675711 0.00028354468 0.00028934158 0.0002956239 0.00030564374 0.00032269003 0.00034447334 0.00035462881 0.00035862162 0.000357007 0.00032824205 0.00028715565 0.00024788195 0.0002222787]]...]
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/biases:0' shape=(128,) dtype=float32_ref>]
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.1-lastlr0.5-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 07:13:12.864708: step 10, loss = 0.75, batch loss = 0.69 (32.0 examples/sec; 0.250 sec/batch; 23h:04m:06s remains)
INFO - root - 2017-12-10 07:13:15.406189: step 20, loss = 0.76, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:36m:44s remains)
INFO - root - 2017-12-10 07:13:17.872608: step 30, loss = 0.77, batch loss = 0.69 (32.3 examples/sec; 0.247 sec/batch; 22h:51m:22s remains)
INFO - root - 2017-12-10 07:13:20.368738: step 40, loss = 0.79, batch loss = 0.68 (31.8 examples/sec; 0.251 sec/batch; 23h:11m:54s remains)
INFO - root - 2017-12-10 07:13:22.906922: step 50, loss = 17.87, batch loss = 17.74 (32.3 examples/sec; 0.248 sec/batch; 22h:52m:03s remains)
INFO - root - 2017-12-10 07:13:25.424416: step 60, loss = 53.01, batch loss = 52.83 (31.4 examples/sec; 0.255 sec/batch; 23h:32m:47s remains)
INFO - root - 2017-12-10 07:13:27.890167: step 70, loss = 0.92, batch loss = 0.69 (34.0 examples/sec; 0.236 sec/batch; 21h:45m:33s remains)
INFO - root - 2017-12-10 07:13:30.340357: step 80, loss = 0.96, batch loss = 0.69 (32.5 examples/sec; 0.246 sec/batch; 22h:42m:07s remains)
INFO - root - 2017-12-10 07:13:32.763023: step 90, loss = 0.98, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:32m:13s remains)
INFO - root - 2017-12-10 07:13:35.209987: step 100, loss = 0.99, batch loss = 0.69 (31.0 examples/sec; 0.258 sec/batch; 23h:51m:21s remains)
2017-12-10 07:13:35.649305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.021639697 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.021639697 -0.021639697 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.021639697 -0.021639697 -0.0216397 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.021639697 -0.021639697 -0.0216397 -0.0216397 -0.021639701][-0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.021639701 -0.021639701 -0.021639701][-0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397][-0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.021639701 -0.0216397 -0.0216397 -0.0216397 -0.0216397 -0.0216397]]...]
INFO - root - 2017-12-10 07:13:38.079735: step 110, loss = 1.00, batch loss = 0.69 (32.9 examples/sec; 0.243 sec/batch; 22h:28m:04s remains)
INFO - root - 2017-12-10 07:13:40.567231: step 120, loss = 1.01, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:10m:59s remains)
INFO - root - 2017-12-10 07:13:43.028182: step 130, loss = 1.02, batch loss = 0.69 (32.2 examples/sec; 0.248 sec/batch; 22h:55m:44s remains)
INFO - root - 2017-12-10 07:13:45.478105: step 140, loss = 1.02, batch loss = 0.69 (33.0 examples/sec; 0.242 sec/batch; 22h:21m:59s remains)
INFO - root - 2017-12-10 07:13:47.968237: step 150, loss = 1.03, batch loss = 0.69 (32.1 examples/sec; 0.249 sec/batch; 23h:01m:45s remains)
INFO - root - 2017-12-10 07:13:50.420526: step 160, loss = 1.04, batch loss = 0.69 (31.9 examples/sec; 0.251 sec/batch; 23h:10m:31s remains)
INFO - root - 2017-12-10 07:13:52.881816: step 170, loss = 1.05, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:43m:06s remains)
INFO - root - 2017-12-10 07:13:55.377248: step 180, loss = 1.06, batch loss = 0.69 (28.5 examples/sec; 0.281 sec/batch; 25h:56m:59s remains)
INFO - root - 2017-12-10 07:13:57.871721: step 190, loss = 1.06, batch loss = 0.69 (32.0 examples/sec; 0.250 sec/batch; 23h:02m:50s remains)
INFO - root - 2017-12-10 07:14:00.374270: step 200, loss = 1.07, batch loss = 0.69 (31.7 examples/sec; 0.253 sec/batch; 23h:19m:08s remains)
2017-12-10 07:14:00.778130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016993755 -0.0016993749 -0.0016993744 -0.0016993744 -0.0016993744 -0.0016993743 -0.0016993742 -0.0016993746 -0.0016993751 -0.001699375 -0.0016993751 -0.0016993751 -0.0016993752 -0.0016993753 -0.0016993755][-0.0016993755 -0.0016993749 -0.0016993744 -0.0016993738 -0.0016993731 -0.0016993729 -0.0016993723 -0.0016993723 -0.0016993722 -0.0016993724 -0.0016993731 -0.0016993737 -0.0016993743 -0.0016993749 -0.0016993752][-0.0016993755 -0.0016993749 -0.0016993744 -0.0016993737 -0.0016993724 -0.0016993709 -0.0016993685 -0.001699366 -0.001699364 -0.0016993637 -0.0016993653 -0.0016993679 -0.0016993707 -0.001699373 -0.0016993744][-0.0016993755 -0.0016991843 -0.0016989927 -0.0016989856 -0.0016991738 -0.0016991704 -0.0016991621 -0.0016993453 -0.0016993388 -0.001699338 -0.001699343 -0.0016993514 -0.0016993606 -0.0016993679 -0.0016993724][-0.0016993755 -0.0016993749 -0.0016993739 -0.0016991787 -0.0016989764 -0.001698954 -0.0016991243 -0.0016991062 -0.001699093 -0.0016992879 -0.0016993006 -0.0016993204 -0.0016993418 -0.0016993587 -0.0016993688][-0.0016993755 -0.0016993749 -0.0016993729 -0.0016993643 -0.0016993433 -0.0016991163 -0.0016988822 -0.0016990394 -0.0016992145 -0.00169922 -0.0016992446 -0.00169928 -0.0016993177 -0.001699347 -0.0016993642][-0.0016993753 -0.0016993753 -0.0016993715 -0.0016993545 -0.0016993183 -0.0016992653 -0.0016992065 -0.0016989677 -0.0016987581 -0.0016989659 -0.0016991959 -0.0016992454 -0.0016992972 -0.0016993374 -0.0016993604][-0.0016993746 -0.0016993746 -0.0016993677 -0.0016993427 -0.0016992944 -0.0016992296 -0.0016991644 -0.0016991234 -0.0016991154 -0.0016991341 -0.0016991755 -0.0016992316 -0.0016992897 -0.0016993344 -0.0016993594][-0.0016993729 -0.0016993729 -0.001699364 -0.0016993361 -0.001699285 -0.0016992213 -0.0016991635 -0.0016991326 -0.0016991304 -0.0016991504 -0.0016991906 -0.0016992444 -0.0016992987 -0.0016993395 -0.0016993615][-0.0016993677 -0.0016993682 -0.0016993618 -0.0016993396 -0.0016992985 -0.0016992487 -0.0016992069 -0.0016991859 -0.0016991859 -0.0016992028 -0.0016992358 -0.0016992784 -0.0016993202 -0.0016993502 -0.0016993658][-0.0016992622 -0.001699258 -0.0016993067 -0.0016992998 -0.0016992744 -0.0016992945 -0.0016992694 -0.0016992569 -0.0016992578 -0.0016992696 -0.0016992919 -0.001699319 -0.0016993445 -0.0016993616 -0.0016993701][-0.001699355 -0.001699309 -0.0016992626 -0.0016992554 -0.0016992962 -0.0016992859 -0.0016992735 -0.0016993189 -0.0016993196 -0.0016993262 -0.0016993377 -0.0016993509 -0.0016993626 -0.0016993697 -0.001699373][-0.0016993565 -0.0016993599 -0.0016993658 -0.0016993202 -0.0016992689 -0.0016993129 -0.0016993582 -0.0016993561 -0.0016993564 -0.0016993589 -0.0016993628 -0.001699367 -0.0016993703 -0.0016993723 -0.0016993732][-0.0016993647 -0.001699367 -0.0016993707 -0.0016991062 -0.0016988401 -0.001698789 -0.0016990041 -0.0016990523 -0.0016991021 -0.0016993702 -0.0016993709 -0.0016993717 -0.0016993724 -0.0016993729 -0.0016993735][-0.0016993722 -0.001699373 -0.0016993743 -0.001699108 -0.0016988418 -0.0016985743 -0.0016985741 -0.0016985722 -0.0016988378 -0.0016991054 -0.0016991054 -0.0016993731 -0.0016993731 -0.0016993735 -0.0016993737]]...]
INFO - root - 2017-12-10 07:14:03.277622: step 210, loss = 1.09, batch loss = 0.70 (32.2 examples/sec; 0.248 sec/batch; 22h:55m:34s remains)
INFO - root - 2017-12-10 07:14:05.780397: step 220, loss = 1.09, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:32m:03s remains)
INFO - root - 2017-12-10 07:14:08.225456: step 230, loss = 1.10, batch loss = 0.69 (33.9 examples/sec; 0.236 sec/batch; 21h:47m:23s remains)
INFO - root - 2017-12-10 07:14:10.709402: step 240, loss = 1.12, batch loss = 0.69 (31.3 examples/sec; 0.255 sec/batch; 23h:34m:01s remains)
INFO - root - 2017-12-10 07:14:13.202055: step 250, loss = 1.13, batch loss = 0.69 (33.1 examples/sec; 0.242 sec/batch; 22h:17m:37s remains)
INFO - root - 2017-12-10 07:14:15.652550: step 260, loss = 1.14, batch loss = 0.69 (31.7 examples/sec; 0.252 sec/batch; 23h:16m:36s remains)
INFO - root - 2017-12-10 07:14:18.112535: step 270, loss = 1.15, batch loss = 0.69 (32.6 examples/sec; 0.245 sec/batch; 22h:36m:57s remains)
INFO - root - 2017-12-10 07:14:20.606226: step 280, loss = 1.17, batch loss = 0.69 (32.6 examples/sec; 0.245 sec/batch; 22h:39m:19s remains)
INFO - root - 2017-12-10 07:14:23.099176: step 290, loss = 1.19, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:35m:24s remains)
INFO - root - 2017-12-10 07:14:25.597141: step 300, loss = 1.20, batch loss = 0.69 (31.1 examples/sec; 0.257 sec/batch; 23h:43m:25s remains)
2017-12-10 07:14:25.997949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0095019862 -0.00950203 -0.0095020942 -0.0095021408 -0.0095021222 -0.0095018353 -0.0095021473 -0.0095021473 -0.0095021315 -0.0095021324 -0.0095021008 -0.0095020421 -0.0095019378 -0.0095020225 -0.0095021492][-0.0095019322 -0.0095020263 -0.009502029 -0.0095020505 -0.009501881 -0.0095017543 -0.0095019341 -0.00950198 -0.0095018549 -0.0095021734 -0.00950215 -0.0095020393 -0.0095018689 -0.0095018949 -0.0095020151][-0.009501799 -0.009501785 -0.0095017618 -0.0095017022 -0.0095019853 -0.0095019415 -0.0095018232 -0.0095018791 -0.00950201 -0.0095021343 -0.0095021175 -0.0095021669 -0.0095021585 -0.0095020691 -0.0095019192][-0.0095017534 -0.0095018195 -0.00950189 -0.0095017729 -0.0095015205 -0.0095015327 -0.009501867 -0.0095020914 -0.009502138 -0.0095021371 -0.0095021278 -0.0095020942 -0.009502111 -0.0095021008 -0.0095021427][-0.0095018269 -0.0095017776 -0.0095015252 -0.0095016453 -0.0095017757 -0.0095015354 -0.0095013222 -0.0095015792 -0.0095019629 -0.0095021576 -0.009502166 -0.0095021725 -0.0095021492 -0.0095021259 -0.00950212][-0.0095013641 -0.0095014758 -0.0095017422 -0.0095018027 -0.0095014209 -0.0095014088 -0.0095013408 -0.0095013455 -0.0095015848 -0.0095019219 -0.0095021073 -0.0095021743 -0.0095021734 -0.0095021585 -0.00950215][-0.009501813 -0.0095017133 -0.0095016733 -0.0095009487 -0.0095005352 -0.009501243 -0.0095014377 -0.0095013333 -0.009501406 -0.0095016034 -0.0095019229 -0.0095021129 -0.00950217 -0.0095021743 -0.0095021585][-0.0095020151 -0.009501406 -0.0095001906 -0.0095007084 -0.0095020011 -0.0095011527 -0.00950037 -0.0095011108 -0.0095015205 -0.0095016286 -0.0095016621 -0.009501935 -0.0095020458 -0.0095020626 -0.0095021585][-0.0095021315 -0.0095018605 -0.0095017981 -0.0095012113 -0.0094996719 -0.009500361 -0.0095019387 -0.0095012058 -0.0095010921 -0.0095013129 -0.0095019825 -0.0095020346 -0.009502029 -0.0095020253 -0.00950214][-0.0095018558 -0.0095015476 -0.0095012514 -0.0095015829 -0.0095016742 -0.0095013427 -0.0095009357 -0.0095019722 -0.0095020477 -0.00950187 -0.0095016425 -0.0095020821 -0.009502124 -0.0095021268 -0.00950213][-0.0095014228 -0.0095014125 -0.0095014619 -0.0095012495 -0.0095010949 -0.00950124 -0.0095020933 -0.0095020765 -0.0095020114 -0.0095021231 -0.00950213 -0.0095021343 -0.0095021343 -0.0095021343 -0.0095021343][-0.0095011229 -0.0095013082 -0.0095012216 -0.0095011611 -0.0095011778 -0.0095012859 -0.00950153 -0.0095016547 -0.0095019983 -0.009502111 -0.0095021157 -0.0095021185 -0.009502124 -0.0095021287 -0.0095021287][-0.009500742 -0.0095009794 -0.0095011182 -0.0095005669 -0.0095008221 -0.009501474 -0.0095015364 -0.00950159 -0.0095018893 -0.00950207 -0.0095020514 -0.0095021026 -0.0095021063 -0.00950211 -0.00950211][-0.0095014572 -0.0095007308 -0.0095002437 -0.0095006162 -0.0095013427 -0.0094998684 -0.0094993655 -0.0095009133 -0.0095017552 -0.0095018549 -0.0095019639 -0.009502097 -0.0095020952 -0.0095020989 -0.0095020989][-0.0095003089 -0.0095000183 -0.00949982 -0.0094997808 -0.0094993664 -0.0095002707 -0.0095012458 -0.00950078 -0.0094987 -0.0094998544 -0.00950188 -0.009501541 -0.00950163 -0.0095020989 -0.0095020989]]...]
INFO - root - 2017-12-10 07:14:28.452892: step 310, loss = 1.21, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:31m:04s remains)
INFO - root - 2017-12-10 07:14:30.929799: step 320, loss = 1.23, batch loss = 0.69 (32.6 examples/sec; 0.245 sec/batch; 22h:36m:58s remains)
INFO - root - 2017-12-10 07:14:33.389516: step 330, loss = 1.25, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:29m:20s remains)
INFO - root - 2017-12-10 07:14:35.818143: step 340, loss = 1.26, batch loss = 0.69 (33.5 examples/sec; 0.239 sec/batch; 22h:03m:37s remains)
INFO - root - 2017-12-10 07:14:38.323377: step 350, loss = 1.27, batch loss = 0.69 (30.6 examples/sec; 0.261 sec/batch; 24h:07m:13s remains)
INFO - root - 2017-12-10 07:14:40.804465: step 360, loss = 1.28, batch loss = 0.69 (31.2 examples/sec; 0.256 sec/batch; 23h:38m:05s remains)
INFO - root - 2017-12-10 07:14:43.339209: step 370, loss = 1.29, batch loss = 0.69 (32.5 examples/sec; 0.246 sec/batch; 22h:41m:39s remains)
INFO - root - 2017-12-10 07:14:45.801936: step 380, loss = 1.30, batch loss = 0.69 (32.6 examples/sec; 0.245 sec/batch; 22h:38m:20s remains)
INFO - root - 2017-12-10 07:14:48.249115: step 390, loss = 1.31, batch loss = 0.69 (33.0 examples/sec; 0.242 sec/batch; 22h:21m:36s remains)
INFO - root - 2017-12-10 07:14:50.715848: step 400, loss = 1.31, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:35m:30s remains)
2017-12-10 07:14:51.074689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00099316193 -0.00099303538 -0.00099298463 -0.0009922001 -0.00099241314 -0.00099304877 -0.00099308509 -0.00099310663 -0.0009931511 -0.00099317462 -0.00099319906 -0.0009932 -0.00099320314 -0.0009932071 -0.0009931433][-0.00099279673 -0.00099285156 -0.00099286844 -0.00099298626 -0.00099266542 -0.00099211582 -0.00099136622 -0.00099269464 -0.00099312176 -0.00099316658 -0.00099318277 -0.00099320139 -0.00099320279 -0.0009932064 -0.00099311222][-0.00099282281 -0.00099261722 -0.0009925504 -0.00099283294 -0.000992054 -0.00099227554 -0.00099209289 -0.00099272176 -0.0009921582 -0.00099253794 -0.00099309755 -0.000993206 -0.00099320349 -0.00099321466 -0.0009931908][-0.00099304912 -0.00099298311 -0.00099295226 -0.00099300966 -0.0009923852 -0.00099271245 -0.00099104876 -0.00099201966 -0.00099196762 -0.00099232735 -0.00099300221 -0.00099318172 -0.00099320139 -0.00099319348 -0.00099290977][-0.0009924327 -0.000992628 -0.00099232257 -0.00099237158 -0.00099271548 -0.00099243736 -0.00099185773 -0.00099179382 -0.00099234516 -0.00099256658 -0.00099278335 -0.00099299976 -0.00099313341 -0.00099316076 -0.00099309115][-0.00099278358 -0.00099281978 -0.00099276879 -0.00099265249 -0.00099244108 -0.000991403 -0.00099163409 -0.00099151791 -0.00099200069 -0.00099175621 -0.00099238032 -0.00099276716 -0.00099296879 -0.00099304086 -0.00099306053][-0.00099214388 -0.00099237589 -0.00099280872 -0.00099249138 -0.00099239824 -0.00099133572 -0.00099124922 -0.00099057914 -0.00099104876 -0.00099140429 -0.0009923951 -0.00099259708 -0.0009928412 -0.00099297147 -0.00099288556][-0.00099252083 -0.000992191 -0.00099129393 -0.00099182036 -0.00099210872 -0.00099121279 -0.00099063281 -0.00098970672 -0.00098996435 -0.0009891442 -0.00099038845 -0.00099129789 -0.00099255133 -0.00099292735 -0.00099284807][-0.00099226076 -0.00099236623 -0.00099195726 -0.00099189766 -0.00099044316 -0.000989559 -0.00098948588 -0.00098865421 -0.00098817749 -0.00098748691 -0.00098831672 -0.000988655 -0.000990695 -0.00099200313 -0.00099270418][-0.00099111907 -0.00099231955 -0.00099241128 -0.00099188089 -0.00099067879 -0.00098988111 -0.00098894211 -0.00098761381 -0.00098731345 -0.00098676526 -0.00098818541 -0.00098801579 -0.00098935911 -0.00099079858 -0.0009917157][-0.00099196914 -0.00099159474 -0.000991312 -0.00099133025 -0.00099051744 -0.000990847 -0.00099019683 -0.00098914036 -0.0009875 -0.00098786713 -0.000988845 -0.00098987471 -0.00099088019 -0.00099059823 -0.00099187822][-0.00099234551 -0.00099238148 -0.00099267485 -0.00099201186 -0.00099119649 -0.0009916604 -0.00099171686 -0.00099168532 -0.00099080987 -0.00099014409 -0.00099052256 -0.00099158823 -0.00099236751 -0.00099238637 -0.000992521][-0.0009920809 -0.00099199463 -0.00099239871 -0.00099224981 -0.00099161558 -0.00099192164 -0.00099177461 -0.00099235948 -0.00099200429 -0.00099204562 -0.00099184143 -0.000992549 -0.00099251478 -0.00099293783 -0.00099323678][-0.00099258509 -0.00099210476 -0.00099239487 -0.00099245063 -0.000992302 -0.00099208415 -0.00099201081 -0.00099254388 -0.00099267391 -0.00099309371 -0.00099300535 -0.00099314179 -0.00099285215 -0.00099304051 -0.00099323364][-0.0009923554 -0.000992334 -0.00099237938 -0.00099239033 -0.000992387 -0.00099186588 -0.00099157623 -0.0009918313 -0.00099205424 -0.00099270581 -0.00099308556 -0.00099319371 -0.00099322817 -0.000993232 -0.00099323562]]...]
INFO - root - 2017-12-10 07:14:53.556474: step 410, loss = 1.32, batch loss = 0.69 (32.4 examples/sec; 0.247 sec/batch; 22h:47m:25s remains)
INFO - root - 2017-12-10 07:14:55.993437: step 420, loss = 1.32, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:35m:10s remains)
INFO - root - 2017-12-10 07:14:58.479339: step 430, loss = 1.32, batch loss = 0.69 (31.7 examples/sec; 0.253 sec/batch; 23h:18m:24s remains)
INFO - root - 2017-12-10 07:15:00.980081: step 440, loss = 1.33, batch loss = 0.69 (31.5 examples/sec; 0.254 sec/batch; 23h:24m:24s remains)
INFO - root - 2017-12-10 07:15:03.501719: step 450, loss = 1.34, batch loss = 0.69 (32.9 examples/sec; 0.243 sec/batch; 22h:24m:35s remains)
INFO - root - 2017-12-10 07:15:05.975361: step 460, loss = 1.36, batch loss = 0.69 (33.0 examples/sec; 0.242 sec/batch; 22h:19m:38s remains)
INFO - root - 2017-12-10 07:15:08.423696: step 470, loss = 1.37, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:33m:13s remains)
INFO - root - 2017-12-10 07:15:10.904813: step 480, loss = 1.38, batch loss = 0.69 (33.2 examples/sec; 0.241 sec/batch; 22h:11m:48s remains)
INFO - root - 2017-12-10 07:15:13.395868: step 490, loss = 1.39, batch loss = 0.69 (33.4 examples/sec; 0.240 sec/batch; 22h:06m:37s remains)
INFO - root - 2017-12-10 07:15:15.862673: step 500, loss = 1.40, batch loss = 0.69 (32.1 examples/sec; 0.249 sec/batch; 22h:59m:07s remains)
2017-12-10 07:15:16.238731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588][-0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588 -0.00010810588]]...]
INFO - root - 2017-12-10 07:15:18.696320: step 510, loss = 1.41, batch loss = 0.69 (32.8 examples/sec; 0.244 sec/batch; 22h:28m:22s remains)
INFO - root - 2017-12-10 07:15:21.150372: step 520, loss = 1.41, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:35m:34s remains)
INFO - root - 2017-12-10 07:15:23.643264: step 530, loss = 1.42, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:19m:05s remains)
INFO - root - 2017-12-10 07:15:26.177670: step 540, loss = 1.42, batch loss = 0.69 (32.5 examples/sec; 0.246 sec/batch; 22h:41m:33s remains)
INFO - root - 2017-12-10 07:15:28.639146: step 550, loss = 1.43, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:34m:53s remains)
INFO - root - 2017-12-10 07:15:31.223566: step 560, loss = 1.43, batch loss = 0.69 (31.6 examples/sec; 0.253 sec/batch; 23h:19m:44s remains)
INFO - root - 2017-12-10 07:15:33.757594: step 570, loss = 1.44, batch loss = 0.69 (32.7 examples/sec; 0.245 sec/batch; 22h:33m:32s remains)
INFO - root - 2017-12-10 07:15:36.266803: step 580, loss = 1.44, batch loss = 0.69 (32.5 examples/sec; 0.246 sec/batch; 22h:42m:09s remains)
INFO - root - 2017-12-10 07:15:38.767921: step 590, loss = 1.45, batch loss = 0.69 (31.7 examples/sec; 0.252 sec/batch; 23h:14m:42s remains)
INFO - root - 2017-12-10 07:15:41.271817: step 600, loss = 1.46, batch loss = 0.69 (32.1 examples/sec; 0.249 sec/batch; 22h:57m:32s remains)
2017-12-10 07:15:41.592398: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0088979769 0.0083519537 0.00805434 0.00803314 0.0078095472 0.0073258672 0.0064293677 0.0056886291 0.0050755097 0.0046111015 0.003841785 0.0028490743 0.0018370067 0.0009856642 0.00030347329][0.011257613 0.010847057 0.010576108 0.010625022 0.010224561 0.0091851978 0.0077298428 0.0065696929 0.0059466157 0.0056375149 0.0051669297 0.0042473148 0.0029855629 0.0017683146 0.00077517971][0.012817005 0.01291659 0.012976477 0.012981072 0.01238031 0.011056897 0.0093635833 0.0081477221 0.007553902 0.00731626 0.0067596687 0.0055702464 0.0039131711 0.0023398658 0.0010820793][0.011221432 0.011631205 0.011912595 0.012105499 0.011532427 0.010217144 0.00855321 0.0072901635 0.0064990884 0.0060413452 0.0053559653 0.0042786649 0.0029212697 0.0016884905 0.00072645408][0.0058131409 0.0060407477 0.0058131595 0.0058022551 0.0053864196 0.004745008 0.0040162555 0.0033174679 0.002711904 0.0021668677 0.0016275026 0.0010712239 0.00054159318 0.00015267709 -0.0001293082][0.0045508845 0.0037864272 0.0027609477 0.0023268713 0.0017687884 0.0014587379 0.0011895638 0.001027753 0.00089300936 0.00071601942 0.00046008659 0.00019338159 -3.913854e-05 -0.00020899368 -0.00031834841][0.0056093074 0.0049063554 0.0034940278 0.0026570421 0.0015107734 0.0011165369 0.00065269764 0.00055357791 0.000448116 0.00034745343 0.00018444305 4.2451138e-06 -0.00015301548 -0.00026708149 -0.00034070108][0.0027683736 0.0025564786 0.0019799962 0.0015582404 0.0010253172 0.00069878018 0.00034057468 0.00026492085 0.00016531369 8.6785207e-05 -5.7628204e-05 -0.00016654303 -0.00025923463 -0.00032339024 -0.00036435245][0.00045637912 0.00077117805 0.0010372482 0.0010052833 0.00079002674 0.00069077779 0.00053510163 0.00037292505 0.00016499546 1.4384132e-05 -0.00011658797 -0.00022515406 -0.00032499433 -0.00035900471 -0.00037982268][-0.00019768995 -0.0001781843 -0.0001394331 -7.2835392e-05 1.7322018e-06 4.3312059e-05 9.4474788e-05 9.3373441e-05 4.5059569e-05 -5.5833079e-05 -0.00018077015 -0.00025868704 -0.00033172406 -0.00036476442 -0.00038753555][-0.00027145926 -0.00029805937 -0.00031662849 -0.00030724707 -0.0002696384 -0.00021926776 -0.00018193122 -0.00018355914 -0.00021821217 -0.00026877737 -0.00031724898 -0.00035206618 -0.00037623011 -0.0003868342 -0.00039004389][-0.00033332119 -0.00033918914 -0.00035017211 -0.00035962256 -0.00036170069 -0.00035883512 -0.00035673578 -0.00036185549 -0.00036916847 -0.00037605435 -0.00038258644 -0.00038651089 -0.00038960343 -0.00039004395 -0.00039004351][-0.00039004412 -0.00039004412 -0.00039004418 -0.00038591569 -0.00037603086 -0.00037518848 -0.00037729362 -0.00038098416 -0.00038503745 -0.00038591996 -0.00039004415 -0.00039004406 -0.00039004406 -0.00039004395 -0.00039004389][-0.00039004374 -0.00039004372 -0.00039004377 -0.00039004395 -0.00039004404 -0.00039004412 -0.00039004404 -0.00039004406 -0.00039004398 -0.00039004398 -0.00039004389 -0.00039004392 -0.00039004386 -0.00039004389 -0.00039004383][-0.00039004389 -0.00039004404 -0.00039004398 -0.00039004418 -0.00039004412 -0.0003900443 -0.00039004418 -0.0003900443 -0.00039004415 -0.00039004421 -0.000390044 -0.00039004404 -0.00039004386 -0.00039004392 -0.00039004374]]...]
INFO - root - 2017-12-10 07:15:44.085248: step 610, loss = 1.47, batch loss = 0.69 (30.9 examples/sec; 0.259 sec/batch; 23h:53m:59s remains)
INFO - root - 2017-12-10 07:15:46.598666: step 620, loss = 1.47, batch loss = 0.69 (32.0 examples/sec; 0.250 sec/batch; 23h:01m:32s remains)
INFO - root - 2017-12-10 07:15:49.096915: step 630, loss = 1.47, batch loss = 0.69 (32.2 examples/sec; 0.248 sec/batch; 22h:52m:52s remains)
INFO - root - 2017-12-10 07:15:51.594829: step 640, loss = 1.48, batch loss = 0.69 (33.1 examples/sec; 0.242 sec/batch; 22h:17m:13s remains)
INFO - root - 2017-12-10 07:15:54.108743: step 650, loss = 1.52, batch loss = 0.72 (31.9 examples/sec; 0.251 sec/batch; 23h:06m:32s remains)
INFO - root - 2017-12-10 07:15:56.676220: step 660, loss = 1.50, batch loss = 0.69 (33.2 examples/sec; 0.241 sec/batch; 22h:12m:18s remains)
INFO - root - 2017-12-10 07:15:59.177235: step 670, loss = 1.51, batch loss = 0.69 (31.3 examples/sec; 0.256 sec/batch; 23h:33m:21s remains)
