INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "69"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 09:26:14.036664: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 09:26:14.036704: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 09:26:14.036710: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 09:26:14.036715: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 09:26:14.036718: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 09:26:14.634626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 09:26:14.634662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 09:26:14.634668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 09:26:14.634677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-10000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-10000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 09:26:19.422387: step 10010, loss = 1.60, batch loss = 1.54 (38.2 examples/sec; 0.209 sec/batch; 18h:45m:11s remains)
INFO - root - 2017-12-05 09:26:21.556060: step 10020, loss = 1.42, batch loss = 1.36 (34.6 examples/sec; 0.231 sec/batch; 20h:44m:01s remains)
INFO - root - 2017-12-05 09:26:23.798862: step 10030, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:08m:50s remains)
INFO - root - 2017-12-05 09:26:25.964067: step 10040, loss = 1.11, batch loss = 1.05 (37.1 examples/sec; 0.216 sec/batch; 19h:20m:02s remains)
INFO - root - 2017-12-05 09:26:28.111299: step 10050, loss = 1.58, batch loss = 1.52 (37.4 examples/sec; 0.214 sec/batch; 19h:09m:44s remains)
INFO - root - 2017-12-05 09:26:30.252106: step 10060, loss = 1.37, batch loss = 1.31 (37.2 examples/sec; 0.215 sec/batch; 19h:17m:10s remains)
INFO - root - 2017-12-05 09:26:32.395248: step 10070, loss = 1.52, batch loss = 1.47 (38.1 examples/sec; 0.210 sec/batch; 18h:47m:21s remains)
INFO - root - 2017-12-05 09:26:34.509819: step 10080, loss = 1.36, batch loss = 1.30 (37.4 examples/sec; 0.214 sec/batch; 19h:10m:31s remains)
INFO - root - 2017-12-05 09:26:36.649072: step 10090, loss = 1.50, batch loss = 1.44 (37.5 examples/sec; 0.213 sec/batch; 19h:06m:41s remains)
INFO - root - 2017-12-05 09:26:38.757040: step 10100, loss = 1.40, batch loss = 1.35 (37.5 examples/sec; 0.214 sec/batch; 19h:07m:48s remains)
2017-12-05 09:26:39.131671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4163303 -2.0035126 -1.5403218 -1.0841367 -0.71880031 -0.49914718 -0.45209646 -0.55257416 -0.76260638 -1.0653641 -1.4852633 -2.0137563 -2.616797 -3.2152815 -3.7192125][-1.5968726 -1.1509278 -0.68000388 -0.23040867 0.12197828 0.3266468 0.35741949 0.23786783 -0.003241539 -0.35441637 -0.8538835 -1.4967446 -2.236356 -2.9686685 -3.5804605][-0.71583939 -0.29261732 0.11629629 0.48914433 0.7727704 0.92757654 0.92694378 0.78871775 0.52724457 0.14538908 -0.4069829 -1.1295273 -1.9619436 -2.7840292 -3.4691315][0.14285135 0.51170349 0.82055426 1.0874453 1.289114 1.3926167 1.3691039 1.2230096 0.95294 0.55239582 -0.036523819 -0.81584167 -1.7145414 -2.6010029 -3.3454456][0.86554146 1.1766052 1.3925424 1.573194 1.7188377 1.8015447 1.7854486 1.6541376 1.3949671 0.99289846 0.3866992 -0.42791343 -1.3782151 -2.3300428 -3.1507335][1.3406653 1.6082916 1.7588005 1.8981209 2.0412211 2.1563005 2.1938729 2.1103706 1.8848162 1.49618 0.88737583 0.053904533 -0.94019651 -1.9635162 -2.8774142][1.47755 1.7175317 1.8314013 1.9699078 2.1590738 2.3544059 2.4885216 2.4917164 2.3359246 1.9904313 1.3994007 0.56066942 -0.46928668 -1.5601637 -2.5685966][1.2856359 1.4941988 1.561933 1.6986303 1.9363003 2.2267575 2.4838214 2.6131129 2.578516 2.332993 1.8067746 0.99383259 -0.048994541 -1.1881609 -2.2772236][0.87245464 1.0241523 1.0095496 1.090199 1.3174133 1.6586246 2.0170345 2.2870507 2.4179039 2.3347235 1.9421082 1.2174911 0.21849489 -0.92593646 -2.062294][0.35365009 0.40735912 0.26564884 0.23726797 0.38784456 0.70685434 1.1020837 1.4685845 1.7483492 1.8421187 1.6303768 1.0691285 0.19749165 -0.87389088 -1.9868679][-0.25060987 -0.31119013 -0.59011221 -0.75022316 -0.70756149 -0.45774698 -0.084694386 0.31869316 0.69371748 0.92802095 0.89141273 0.52296448 -0.16212368 -1.0820503 -2.0890112][-0.97517276 -1.1201801 -1.4963045 -1.7570498 -1.8178566 -1.6582189 -1.3472879 -0.95958829 -0.54968405 -0.22692823 -0.11764574 -0.29915476 -0.77683091 -1.4990768 -2.3402855][-1.8059325 -1.9839194 -2.3706877 -2.6730275 -2.7991087 -2.7263393 -2.5030322 -2.1780846 -1.7939594 -1.4412947 -1.23523 -1.2588561 -1.5398009 -2.0525758 -2.6967278][-2.6331754 -2.7943144 -3.114217 -3.3880103 -3.5308533 -3.5218387 -3.3874176 -3.1546264 -2.8507733 -2.5360031 -2.3020108 -2.2250378 -2.3482845 -2.6687131 -3.1129851][-3.3404603 -3.4444356 -3.6558688 -3.8554337 -3.9792781 -4.0067706 -3.9452164 -3.8073964 -3.6079068 -3.3803816 -3.186703 -3.0826778 -3.1100187 -3.2773676 -3.5409651]]...]
INFO - root - 2017-12-05 09:26:41.263299: step 10110, loss = 1.42, batch loss = 1.36 (37.0 examples/sec; 0.216 sec/batch; 19h:21m:18s remains)
INFO - root - 2017-12-05 09:26:43.384299: step 10120, loss = 1.51, batch loss = 1.45 (37.1 examples/sec; 0.216 sec/batch; 19h:19m:08s remains)
INFO - root - 2017-12-05 09:26:45.529952: step 10130, loss = 1.21, batch loss = 1.16 (37.5 examples/sec; 0.213 sec/batch; 19h:05m:14s remains)
INFO - root - 2017-12-05 09:26:47.646380: step 10140, loss = 1.27, batch loss = 1.21 (37.5 examples/sec; 0.214 sec/batch; 19h:07m:28s remains)
INFO - root - 2017-12-05 09:26:49.774530: step 10150, loss = 1.18, batch loss = 1.12 (37.9 examples/sec; 0.211 sec/batch; 18h:53m:47s remains)
INFO - root - 2017-12-05 09:26:51.892736: step 10160, loss = 1.21, batch loss = 1.15 (37.6 examples/sec; 0.213 sec/batch; 19h:01m:58s remains)
INFO - root - 2017-12-05 09:26:54.037567: step 10170, loss = 1.32, batch loss = 1.26 (37.5 examples/sec; 0.213 sec/batch; 19h:05m:45s remains)
INFO - root - 2017-12-05 09:26:56.190870: step 10180, loss = 1.37, batch loss = 1.31 (38.2 examples/sec; 0.210 sec/batch; 18h:45m:29s remains)
INFO - root - 2017-12-05 09:26:58.338330: step 10190, loss = 1.66, batch loss = 1.60 (37.1 examples/sec; 0.215 sec/batch; 19h:16m:51s remains)
INFO - root - 2017-12-05 09:27:00.474336: step 10200, loss = 1.29, batch loss = 1.23 (35.9 examples/sec; 0.223 sec/batch; 19h:57m:49s remains)
2017-12-05 09:27:00.746769: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.84440851 1.214612 1.3236365 1.1261263 0.6164031 -0.0407629 -0.6062758 -0.90025926 -0.87264585 -0.55899477 -0.038226128 0.63116455 1.4069471 2.2388453 3.0609365][1.141705 1.4779387 1.5971971 1.4759855 1.0901899 0.53937149 0.00048303604 -0.36981678 -0.50720644 -0.41182017 -0.10527754 0.40289736 1.0832858 1.8709021 2.668427][1.4630194 1.6679888 1.7405648 1.7056627 1.524066 1.1956887 0.79452133 0.4092164 0.098426342 -0.085480213 -0.07423687 0.19650745 0.73220778 1.468657 2.2731423][1.9897966 2.0406404 2.0487919 2.1023111 2.1721811 2.1540408 1.9987655 1.6875114 1.2398863 0.75343132 0.40145874 0.35050249 0.67062092 1.3169136 2.1303258][2.6662459 2.6234169 2.6135778 2.7793784 3.1344194 3.4987969 3.694293 3.5670214 3.053967 2.2697968 1.4909921 1.007668 0.98369122 1.4243698 2.1699748][3.3207927 3.3150535 3.3818793 3.7082777 4.3536749 5.1238823 5.739027 5.8952532 5.40652 4.3626518 3.1208053 2.098031 1.5732446 1.6378641 2.1658649][3.7058573 3.8967075 4.1525927 4.6884413 5.5964413 6.7275019 7.7549067 8.2427187 7.869422 6.6543045 4.98104 3.3774385 2.243638 1.7815604 1.9279256][3.7431421 4.2172308 4.7218037 5.457799 6.5411739 7.9035015 9.2302895 10.015718 9.8287735 8.5730038 6.5987206 4.4895906 2.7522087 1.7016706 1.3626323][3.5658789 4.3119316 5.020277 5.8574829 6.9433069 8.30906 9.7036362 10.636853 10.632814 9.48336 7.4337225 5.0409632 2.8640795 1.3088522 0.49951315][3.4845133 4.3988085 5.1512413 5.9066391 6.7777567 7.877512 9.0573711 9.9153786 9.9971724 9.0324707 7.1366711 4.7563686 2.4226694 0.58133364 -0.55458117][3.7805624 4.7199049 5.3271561 5.7684116 6.1949816 6.7827921 7.5003357 8.0779943 8.1320333 7.3632345 5.7744179 3.6689453 1.4817028 -0.3658452 -1.6123824][4.5502691 5.4194479 5.7272892 5.6755524 5.4813395 5.3902674 5.4867897 5.6404018 5.5554714 4.9422188 3.7178135 2.0498548 0.24561882 -1.3559275 -2.4986851][5.6351309 6.397172 6.3562012 5.7582426 4.8942947 4.07675 3.5010672 3.1690564 2.8716841 2.3438053 1.4579725 0.27109671 -1.0444419 -2.2563381 -3.1528172][6.7563324 7.43441 7.1056538 6.0513258 4.6029158 3.1336079 1.9349351 1.11514 0.555398 0.032113552 -0.6018374 -1.3656034 -2.2077987 -3.0004702 -3.6014295][7.7027893 8.3899517 7.9096775 6.5638351 4.7032175 2.7484407 1.0576882 -0.19184351 -1.0364366 -1.6429563 -2.15373 -2.6344223 -3.1196461 -3.5638514 -3.9040432]]...]
INFO - root - 2017-12-05 09:27:02.889341: step 10210, loss = 1.58, batch loss = 1.53 (36.4 examples/sec; 0.220 sec/batch; 19h:41m:10s remains)
INFO - root - 2017-12-05 09:27:05.037033: step 10220, loss = 1.91, batch loss = 1.85 (37.2 examples/sec; 0.215 sec/batch; 19h:13m:35s remains)
INFO - root - 2017-12-05 09:27:07.174696: step 10230, loss = 1.56, batch loss = 1.50 (37.1 examples/sec; 0.216 sec/batch; 19h:17m:58s remains)
INFO - root - 2017-12-05 09:27:09.330960: step 10240, loss = 1.23, batch loss = 1.17 (37.0 examples/sec; 0.216 sec/batch; 19h:22m:06s remains)
INFO - root - 2017-12-05 09:27:11.465389: step 10250, loss = 1.20, batch loss = 1.14 (37.7 examples/sec; 0.212 sec/batch; 18h:59m:42s remains)
INFO - root - 2017-12-05 09:27:13.606135: step 10260, loss = 1.66, batch loss = 1.60 (37.1 examples/sec; 0.215 sec/batch; 19h:17m:06s remains)
INFO - root - 2017-12-05 09:27:15.793684: step 10270, loss = 1.35, batch loss = 1.29 (38.1 examples/sec; 0.210 sec/batch; 18h:47m:18s remains)
INFO - root - 2017-12-05 09:27:17.948930: step 10280, loss = 1.29, batch loss = 1.23 (37.1 examples/sec; 0.216 sec/batch; 19h:19m:09s remains)
INFO - root - 2017-12-05 09:27:20.094343: step 10290, loss = 1.47, batch loss = 1.41 (36.9 examples/sec; 0.217 sec/batch; 19h:23m:25s remains)
INFO - root - 2017-12-05 09:27:22.249609: step 10300, loss = 1.40, batch loss = 1.34 (37.3 examples/sec; 0.215 sec/batch; 19h:12m:50s remains)
2017-12-05 09:27:22.535949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0991232 -2.0721354 -2.03733 -1.9629266 -1.867013 -1.7490051 -1.6090753 -1.4580078 -1.3212776 -1.2270391 -1.172765 -1.1519141 -1.1555498 -1.2212846 -1.3821075][-1.0159833 -0.98215151 -0.93204832 -0.81764793 -0.66963029 -0.483598 -0.26518679 -0.043931007 0.14147854 0.2580328 0.32384539 0.35613155 0.36065769 0.28052664 0.0586586][0.054974556 0.094892025 0.15459061 0.30504513 0.50661755 0.76396036 1.061204 1.3470612 1.571835 1.7037787 1.7841792 1.8390074 1.8662877 1.7887363 1.5210156][0.86372185 0.91400146 0.97838449 1.1523843 1.3902016 1.7005906 2.0541086 2.379293 2.6214314 2.7615523 2.8640437 2.9563417 3.0237823 2.975862 2.6985536][1.2631316 1.3312516 1.3933473 1.5597405 1.8009586 2.1210475 2.4898357 2.8183823 3.0586667 3.2121468 3.3518395 3.5016971 3.6312952 3.6396227 3.3877592][1.2850671 1.3714991 1.420022 1.5517836 1.756402 2.0432487 2.3902388 2.695055 2.9256091 3.105227 3.3033423 3.5328269 3.7467623 3.8352833 3.6424761][1.0866308 1.1954961 1.2121382 1.2720685 1.3942952 1.6041021 1.8946457 2.1665359 2.3950634 2.6203961 2.9022093 3.2348981 3.5489569 3.7253628 3.6100998][0.80309153 0.93692112 0.91797543 0.8901372 0.9067564 1.006187 1.2095199 1.4393339 1.6737719 1.9671426 2.3518014 2.8015223 3.2193074 3.4809833 3.4463024][0.48751354 0.64740753 0.59405136 0.48421812 0.39263153 0.37327385 0.47582769 0.6622 0.92467165 1.3082304 1.8055749 2.366868 2.875988 3.212369 3.2542148][0.14297438 0.31970072 0.23490715 0.056682587 -0.12586069 -0.25298262 -0.2517767 -0.10883045 0.19219208 0.66974354 1.2737985 1.9283142 2.5139132 2.9179568 3.035583][-0.30468988 -0.11993647 -0.2177887 -0.4272027 -0.66897917 -0.87252879 -0.95371628 -0.84665179 -0.5177083 0.025698662 0.70500135 1.423151 2.0649667 2.527771 2.7247396][-0.9223721 -0.7443378 -0.82870841 -1.0274568 -1.2796333 -1.5159931 -1.6473155 -1.571002 -1.2379878 -0.67447209 0.031048298 0.77259922 1.4429178 1.9516716 2.2272859][-1.6744843 -1.5115261 -1.5686557 -1.7272995 -1.9484899 -2.1797476 -2.3301392 -2.28291 -1.9794443 -1.4483721 -0.77235603 -0.055099487 0.60601711 1.1358123 1.4722986][-2.4532254 -2.3095596 -2.3312953 -2.4374065 -2.6109672 -2.812758 -2.9613485 -2.9424376 -2.6975043 -2.2505591 -1.6644247 -1.0266018 -0.4180541 0.097645283 0.45967722][-3.1252394 -3.0080585 -3.00349 -3.064698 -3.1898875 -3.3503129 -3.4837389 -3.49502 -3.3328242 -3.0086083 -2.5613666 -2.0558598 -1.5483947 -1.09027 -0.74335313]]...]
INFO - root - 2017-12-05 09:27:24.694834: step 10310, loss = 1.78, batch loss = 1.73 (37.2 examples/sec; 0.215 sec/batch; 19h:14m:05s remains)
INFO - root - 2017-12-05 09:27:26.822054: step 10320, loss = 1.27, batch loss = 1.21 (37.3 examples/sec; 0.214 sec/batch; 19h:10m:52s remains)
INFO - root - 2017-12-05 09:27:28.991654: step 10330, loss = 2.03, batch loss = 1.97 (37.2 examples/sec; 0.215 sec/batch; 19h:13m:48s remains)
INFO - root - 2017-12-05 09:27:31.126717: step 10340, loss = 1.11, batch loss = 1.05 (37.1 examples/sec; 0.216 sec/batch; 19h:18m:21s remains)
INFO - root - 2017-12-05 09:27:33.298335: step 10350, loss = 1.25, batch loss = 1.19 (36.6 examples/sec; 0.218 sec/batch; 19h:32m:44s remains)
INFO - root - 2017-12-05 09:27:35.445605: step 10360, loss = 1.31, batch loss = 1.25 (37.5 examples/sec; 0.213 sec/batch; 19h:05m:09s remains)
INFO - root - 2017-12-05 09:27:37.588179: step 10370, loss = 1.74, batch loss = 1.68 (36.4 examples/sec; 0.220 sec/batch; 19h:41m:25s remains)
INFO - root - 2017-12-05 09:27:39.757890: step 10380, loss = 1.49, batch loss = 1.44 (36.4 examples/sec; 0.220 sec/batch; 19h:39m:33s remains)
INFO - root - 2017-12-05 09:27:41.893284: step 10390, loss = 1.61, batch loss = 1.55 (37.7 examples/sec; 0.212 sec/batch; 18h:58m:10s remains)
INFO - root - 2017-12-05 09:27:44.060008: step 10400, loss = 1.23, batch loss = 1.17 (38.1 examples/sec; 0.210 sec/batch; 18h:45m:58s remains)
2017-12-05 09:27:44.331513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.860724 -3.8126736 -3.7784572 -3.74048 -3.6880131 -3.6246631 -3.5661652 -3.5434985 -3.5884035 -3.694921 -3.825748 -3.946044 -4.0432291 -4.1172757 -4.1705585][-3.5975282 -3.5612004 -3.5782208 -3.6161466 -3.649255 -3.6690621 -3.6823843 -3.7106898 -3.7735054 -3.8658624 -3.9623916 -4.0404434 -4.0927072 -4.1202855 -4.130095][-3.2487202 -3.2059526 -3.2603118 -3.3636048 -3.4741631 -3.5677502 -3.6412611 -3.7105598 -3.7883406 -3.8728929 -3.9481838 -4.0015321 -4.0279193 -4.0280061 -4.0125246][-2.8549309 -2.7772136 -2.8414068 -2.9843082 -3.1481342 -3.2914124 -3.4010429 -3.4907122 -3.5732486 -3.6549885 -3.7252245 -3.7743006 -3.7962222 -3.7937119 -3.7809896][-2.5005627 -2.3632164 -2.4072251 -2.5578794 -2.7435153 -2.9070299 -3.0239768 -3.1089735 -3.1839552 -3.2645407 -3.3383203 -3.3933649 -3.424964 -3.4392366 -3.4519947][-2.2770953 -2.0814638 -2.0915339 -2.2281432 -2.410584 -2.5677032 -2.6669078 -2.7270179 -2.7840455 -2.8596497 -2.9349079 -2.9950566 -3.038775 -3.077198 -3.1261361][-2.2354748 -2.017508 -2.0088928 -2.1310811 -2.3021417 -2.4445333 -2.5223749 -2.5572343 -2.5949445 -2.6573892 -2.7224751 -2.7769656 -2.8245611 -2.8817372 -2.9624338][-2.3882127 -2.1913424 -2.1908355 -2.3121192 -2.4797482 -2.6187947 -2.6933541 -2.7229257 -2.7468164 -2.7851243 -2.8244724 -2.8608551 -2.9026392 -2.9679043 -3.0660963][-2.7164302 -2.5690966 -2.5906396 -2.7168036 -2.8852038 -3.0288475 -3.1140089 -3.1506648 -3.1633544 -3.1731343 -3.182713 -3.1968343 -3.2255068 -3.2861662 -3.3848891][-3.1711903 -3.0793009 -3.1174428 -3.2347307 -3.3855658 -3.5172591 -3.6003535 -3.6374907 -3.6422451 -3.6340921 -3.6247284 -3.6229303 -3.6373696 -3.6836951 -3.7656331][-3.645803 -3.5989146 -3.6369531 -3.7254574 -3.8359733 -3.9333959 -3.9963396 -4.0240426 -4.0234861 -4.0097666 -3.9946749 -3.9864998 -3.991549 -4.0205688 -4.0750465][-4.0257168 -4.0066605 -4.0331616 -4.0855145 -4.1487589 -4.2046876 -4.2411065 -4.256247 -4.2540164 -4.2432189 -4.231102 -4.22283 -4.2226892 -4.2369485 -4.2652354][-4.2579341 -4.252521 -4.2662811 -4.2893567 -4.3160682 -4.3397675 -4.3552117 -4.3611789 -4.3596435 -4.3541818 -4.3478208 -4.3426485 -4.3413172 -4.3462281 -4.3568726][-4.3624449 -4.361743 -4.3670964 -4.3747025 -4.382731 -4.3894644 -4.3935919 -4.3948388 -4.3939962 -4.392148 -4.3900366 -4.3882155 -4.3877149 -4.3889608 -4.3918533][-4.3966751 -4.3967509 -4.3980632 -4.3995266 -4.4008431 -4.4017596 -4.4022532 -4.4023519 -4.4020452 -4.4015737 -4.4011106 -4.400764 -4.4006929 -4.4009271 -4.4014812]]...]
INFO - root - 2017-12-05 09:27:46.472613: step 10410, loss = 1.07, batch loss = 1.02 (37.7 examples/sec; 0.212 sec/batch; 18h:58m:06s remains)
INFO - root - 2017-12-05 09:27:48.625657: step 10420, loss = 1.06, batch loss = 1.00 (36.9 examples/sec; 0.217 sec/batch; 19h:24m:41s remains)
INFO - root - 2017-12-05 09:27:50.777926: step 10430, loss = 1.14, batch loss = 1.08 (36.8 examples/sec; 0.218 sec/batch; 19h:28m:06s remains)
INFO - root - 2017-12-05 09:27:52.914562: step 10440, loss = 1.18, batch loss = 1.12 (39.9 examples/sec; 0.200 sec/batch; 17h:55m:39s remains)
INFO - root - 2017-12-05 09:27:55.100887: step 10450, loss = 1.12, batch loss = 1.07 (37.0 examples/sec; 0.216 sec/batch; 19h:19m:09s remains)
INFO - root - 2017-12-05 09:27:57.280982: step 10460, loss = 1.37, batch loss = 1.32 (37.0 examples/sec; 0.216 sec/batch; 19h:20m:48s remains)
INFO - root - 2017-12-05 09:27:59.470949: step 10470, loss = 1.33, batch loss = 1.28 (35.1 examples/sec; 0.228 sec/batch; 20h:24m:46s remains)
INFO - root - 2017-12-05 09:28:01.627259: step 10480, loss = 1.37, batch loss = 1.32 (37.1 examples/sec; 0.215 sec/batch; 19h:16m:11s remains)
INFO - root - 2017-12-05 09:28:03.779430: step 10490, loss = 1.40, batch loss = 1.34 (37.5 examples/sec; 0.213 sec/batch; 19h:05m:40s remains)
INFO - root - 2017-12-05 09:28:05.941780: step 10500, loss = 1.10, batch loss = 1.04 (37.8 examples/sec; 0.212 sec/batch; 18h:55m:09s remains)
2017-12-05 09:28:06.205722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3888211 -4.3681893 -4.3191957 -4.2292371 -4.1121254 -4.0040092 -3.9423246 -3.9481609 -4.013483 -4.1132417 -4.21436 -4.2931371 -4.3442116 -4.3695621 -4.3730407][-4.3831177 -4.362649 -4.31188 -4.2176371 -4.0919094 -3.9686396 -3.888411 -3.8805397 -3.9411066 -4.0468431 -4.1628108 -4.2603459 -4.3276596 -4.3641319 -4.3744922][-4.3788528 -4.359539 -4.3102722 -4.2184196 -4.0938454 -3.9655111 -3.8704422 -3.844636 -3.892982 -3.996634 -4.1201491 -4.2307539 -4.3113246 -4.3575583 -4.3732657][-4.3758821 -4.3580165 -4.3122921 -4.2277136 -4.1113095 -3.9865704 -3.8847871 -3.8439515 -3.8777432 -3.9721329 -4.0934191 -4.2083335 -4.2963233 -4.3494468 -4.3694978][-4.3748217 -4.3567934 -4.3124485 -4.2322717 -4.1197157 -3.9962273 -3.8891747 -3.8375881 -3.8608685 -3.9474306 -4.0650344 -4.1825418 -4.2780886 -4.3390408 -4.3647017][-4.3716 -4.3498435 -4.2992334 -4.2108746 -4.0892124 -3.9548516 -3.8373363 -3.7768412 -3.7951019 -3.8801882 -4.002028 -4.1307335 -4.2413216 -4.3169227 -4.3535457][-4.3598495 -4.3265276 -4.2592535 -4.1506705 -4.0072637 -3.8505869 -3.7171373 -3.6496391 -3.6680918 -3.7581377 -3.892446 -4.0428481 -4.1783366 -4.2776184 -4.332655][-4.3379922 -4.2861376 -4.1947327 -4.06007 -3.8917539 -3.7140346 -3.5709307 -3.5059824 -3.5291898 -3.6241357 -3.768744 -3.9386399 -4.0999637 -4.225594 -4.3025532][-4.315896 -4.245532 -4.1311173 -3.9748416 -3.7917051 -3.6110113 -3.4762096 -3.4236679 -3.4536574 -3.5481865 -3.69066 -3.8623462 -4.0348206 -4.176702 -4.2715688][-4.3036313 -4.2196732 -4.0894284 -3.9250209 -3.7474451 -3.5880713 -3.4810631 -3.4489586 -3.4854455 -3.5726981 -3.7001061 -3.8548045 -4.0166054 -4.1558981 -4.2552829][-4.29804 -4.211071 -4.0816545 -3.9288535 -3.778275 -3.657969 -3.5902126 -3.5831249 -3.6257062 -3.7018259 -3.8041949 -3.9255033 -4.0552473 -4.171896 -4.2589874][-4.2866292 -4.2100468 -4.1029305 -3.98391 -3.8761506 -3.80065 -3.768585 -3.7795317 -3.8210244 -3.8796253 -3.9503069 -4.03308 -4.1236095 -4.2088361 -4.274858][-4.2721448 -4.2184191 -4.1480112 -4.0733871 -4.0106931 -3.97214 -3.9610298 -3.9759848 -4.0075221 -4.0450873 -4.0871959 -4.1370268 -4.1941776 -4.2507434 -4.2957892][-4.2793326 -4.2494879 -4.2131772 -4.1756692 -4.14533 -4.1283145 -4.1249461 -4.135385 -4.1541281 -4.1747255 -4.1967111 -4.223536 -4.2568321 -4.2922726 -4.3211708][-4.3100233 -4.2964563 -4.2805452 -4.2637978 -4.2510161 -4.2443562 -4.2436271 -4.2498088 -4.2598562 -4.2695637 -4.2796645 -4.2929034 -4.3111334 -4.3320012 -4.349124]]...]
INFO - root - 2017-12-05 09:28:08.364781: step 10510, loss = 1.06, batch loss = 1.00 (36.0 examples/sec; 0.222 sec/batch; 19h:53m:49s remains)
INFO - root - 2017-12-05 09:28:10.538873: step 10520, loss = 1.56, batch loss = 1.51 (37.6 examples/sec; 0.213 sec/batch; 19h:01m:42s remains)
INFO - root - 2017-12-05 09:28:12.732336: step 10530, loss = 1.52, batch loss = 1.47 (35.8 examples/sec; 0.223 sec/batch; 19h:59m:16s remains)
INFO - root - 2017-12-05 09:28:14.866524: step 10540, loss = 1.58, batch loss = 1.52 (35.9 examples/sec; 0.223 sec/batch; 19h:54m:30s remains)
INFO - root - 2017-12-05 09:28:17.017704: step 10550, loss = 1.43, batch loss = 1.37 (37.7 examples/sec; 0.212 sec/batch; 18h:59m:02s remains)
INFO - root - 2017-12-05 09:28:19.179338: step 10560, loss = 1.45, batch loss = 1.39 (37.1 examples/sec; 0.216 sec/batch; 19h:16m:33s remains)
INFO - root - 2017-12-05 09:28:21.359807: step 10570, loss = 1.17, batch loss = 1.11 (36.4 examples/sec; 0.220 sec/batch; 19h:40m:43s remains)
INFO - root - 2017-12-05 09:28:23.531682: step 10580, loss = 1.37, batch loss = 1.31 (37.0 examples/sec; 0.216 sec/batch; 19h:21m:25s remains)
INFO - root - 2017-12-05 09:28:25.723384: step 10590, loss = 1.78, batch loss = 1.72 (35.9 examples/sec; 0.223 sec/batch; 19h:56m:30s remains)
INFO - root - 2017-12-05 09:28:27.875439: step 10600, loss = 1.64, batch loss = 1.58 (37.3 examples/sec; 0.214 sec/batch; 19h:09m:34s remains)
2017-12-05 09:28:28.140437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4004016 -4.3997149 -4.3972044 -4.3939843 -4.3924494 -4.3934326 -4.3959656 -4.3983073 -4.4001684 -4.4015017 -4.4023113 -4.4026589 -4.4028559 -4.4030004 -4.4030457][-4.383472 -4.3805895 -4.3676577 -4.349926 -4.3390212 -4.3411183 -4.3532505 -4.3683376 -4.3814421 -4.3911772 -4.3973293 -4.4003425 -4.401556 -4.4019251 -4.4018908][-4.3102374 -4.3068562 -4.2719727 -4.2214584 -4.1854787 -4.1827946 -4.2127471 -4.2605667 -4.3088675 -4.3478117 -4.3735614 -4.3871889 -4.3922853 -4.3917303 -4.3885307][-4.0982747 -4.112421 -4.0571289 -3.9596694 -3.8755407 -3.8497455 -3.8918393 -3.9843431 -4.09663 -4.2021527 -4.2814131 -4.327632 -4.3450813 -4.3410907 -4.324996][-3.6735513 -3.7420363 -3.701726 -3.5765836 -3.4382319 -3.3575177 -3.3730798 -3.4851263 -3.6621768 -3.8597875 -4.0317607 -4.1477289 -4.1994448 -4.1933002 -4.1504078][-3.058521 -3.2151613 -3.2481894 -3.1505313 -2.9854305 -2.8342361 -2.7677035 -2.8294704 -3.0191002 -3.2882261 -3.5616286 -3.7741332 -3.8868995 -3.8927774 -3.8195705][-2.4089365 -2.6494405 -2.7998514 -2.7917147 -2.651648 -2.4494135 -2.2718127 -2.2104869 -2.3234956 -2.5883985 -2.9177346 -3.2158492 -3.4068847 -3.4536052 -3.3727946][-1.980022 -2.2493165 -2.49984 -2.6035442 -2.5338273 -2.3245761 -2.0571773 -1.8504941 -1.8141704 -1.9767823 -2.2736056 -2.6040497 -2.8659935 -2.98238 -2.9359822][-1.9456544 -2.1757989 -2.4581537 -2.6399522 -2.6494613 -2.4761281 -2.1751194 -1.8604951 -1.6670392 -1.6726856 -1.8580217 -2.1485014 -2.441524 -2.63091 -2.6546712][-2.2854977 -2.4380007 -2.6848371 -2.8834891 -2.9434359 -2.8206959 -2.5390506 -2.1876929 -1.8958502 -1.7663682 -1.8233557 -2.0292163 -2.2987328 -2.5234551 -2.6103992][-2.8433957 -2.9148331 -3.0897722 -3.25744 -3.3312111 -3.2532005 -3.0237556 -2.7027001 -2.3937349 -2.1957202 -2.1567535 -2.2696919 -2.4755354 -2.6791239 -2.7818182][-3.419229 -3.4337904 -3.5339189 -3.649003 -3.7117186 -3.6678858 -3.5057378 -3.2570133 -2.993803 -2.7932739 -2.7085891 -2.746511 -2.8681619 -2.9971397 -3.0487275][-3.8746047 -3.8611889 -3.9047253 -3.9691265 -4.0127945 -3.9949446 -3.8996503 -3.7380176 -3.5515108 -3.3924265 -3.30382 -3.2903557 -3.3188345 -3.3276443 -3.2595584][-4.1661806 -4.1457658 -4.1554842 -4.182313 -4.2068939 -4.204638 -4.1616125 -4.0770612 -3.9696 -3.8680644 -3.7970276 -3.7505002 -3.6889334 -3.5613832 -3.3416624][-4.3154759 -4.2993927 -4.2957335 -4.3027 -4.31404 -4.3176222 -4.3039174 -4.269155 -4.2193952 -4.16555 -4.114131 -4.0462761 -3.9166584 -3.6805589 -3.3330164]]...]
INFO - root - 2017-12-05 09:28:30.299672: step 10610, loss = 1.59, batch loss = 1.53 (37.4 examples/sec; 0.214 sec/batch; 19h:06m:56s remains)
INFO - root - 2017-12-05 09:28:32.460007: step 10620, loss = 1.46, batch loss = 1.40 (36.9 examples/sec; 0.217 sec/batch; 19h:24m:00s remains)
INFO - root - 2017-12-05 09:28:34.614381: step 10630, loss = 1.34, batch loss = 1.28 (36.7 examples/sec; 0.218 sec/batch; 19h:29m:11s remains)
INFO - root - 2017-12-05 09:28:36.804871: step 10640, loss = 1.07, batch loss = 1.01 (36.6 examples/sec; 0.219 sec/batch; 19h:33m:29s remains)
INFO - root - 2017-12-05 09:28:38.981084: step 10650, loss = 1.62, batch loss = 1.56 (36.7 examples/sec; 0.218 sec/batch; 19h:29m:35s remains)
INFO - root - 2017-12-05 09:28:41.112365: step 10660, loss = 1.52, batch loss = 1.46 (37.6 examples/sec; 0.213 sec/batch; 19h:00m:55s remains)
INFO - root - 2017-12-05 09:28:43.267849: step 10670, loss = 1.23, batch loss = 1.17 (37.2 examples/sec; 0.215 sec/batch; 19h:14m:09s remains)
INFO - root - 2017-12-05 09:28:45.438173: step 10680, loss = 1.37, batch loss = 1.32 (37.3 examples/sec; 0.214 sec/batch; 19h:09m:56s remains)
INFO - root - 2017-12-05 09:28:47.604853: step 10690, loss = 1.53, batch loss = 1.47 (37.8 examples/sec; 0.211 sec/batch; 18h:54m:19s remains)
INFO - root - 2017-12-05 09:28:49.757485: step 10700, loss = 1.57, batch loss = 1.51 (37.1 examples/sec; 0.215 sec/batch; 19h:15m:01s remains)
2017-12-05 09:28:50.035979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2889147 -4.2575259 -4.225502 -4.1947432 -4.1689024 -4.1429653 -4.1143737 -4.0800033 -4.0302858 -3.9631836 -3.8839974 -3.7907939 -3.673624 -3.5302372 -3.4093583][-4.1714325 -4.054132 -3.923846 -3.7956629 -3.6851366 -3.5953031 -3.5270655 -3.4809237 -3.4479673 -3.4280472 -3.4239202 -3.4299762 -3.427341 -3.4009991 -3.380651][-3.9110851 -3.6248605 -3.3017559 -2.9826546 -2.7084959 -2.4955006 -2.3441563 -2.2553589 -2.2251143 -2.26006 -2.3634956 -2.5312824 -2.7341087 -2.9327486 -3.1180704][-3.4369755 -2.8888381 -2.2766261 -1.678262 -1.1648078 -0.76765037 -0.47920179 -0.29825878 -0.23018169 -0.30126 -0.52492285 -0.90229106 -1.388447 -1.9096596 -2.4048164][-2.6563835 -1.778578 -0.82475281 0.08926487 0.87413454 1.4920483 1.9575925 2.2739859 2.4181814 2.3367715 1.9998064 1.4039702 0.60878372 -0.2753706 -1.1370871][-1.4908485 -0.27228355 1.0043979 2.2025747 3.2347035 4.0678711 4.7202921 5.1857643 5.4186192 5.3439646 4.9181995 4.1428967 3.089541 1.891479 0.68955612][0.039463043 1.528594 3.0260801 4.4063978 5.6047554 6.5994663 7.404541 7.9940958 8.3003845 8.249877 7.8007603 6.955431 5.7826252 4.4079895 2.9701118][1.7603025 3.3884373 4.9464922 6.3567533 7.5949631 8.6472816 9.5163708 10.153921 10.491954 10.488033 10.111617 9.3595152 8.2692814 6.9223633 5.4156246][3.4381318 5.0400639 6.479928 7.746953 8.8599415 9.8114729 10.593152 11.151506 11.455868 11.518821 11.31858 10.827773 10.030684 8.9394045 7.5669432][4.85163 6.2894382 7.4720516 8.4547 9.2803936 9.9459591 10.449135 10.774293 10.961977 11.06799 11.077443 10.934306 10.560534 9.8805332 8.8135281][5.8758011 7.0704451 7.9245138 8.5574741 9.0074844 9.277565 9.384408 9.3626223 9.3240738 9.3657913 9.4877319 9.6199551 9.6341629 9.3763895 8.6810741][6.4722977 7.4043522 7.9221754 8.2070684 8.2776241 8.13835 7.8255663 7.4185467 7.077858 6.9358377 7.0091372 7.2367783 7.4673109 7.5059767 7.1355743][6.6025 7.2914219 7.52182 7.5190763 7.2739344 6.7942095 6.1248045 5.3761473 4.7296505 4.3345461 4.2272196 4.3675165 4.6158705 4.7783041 4.6289253][6.170392 6.6596251 6.67741 6.4742632 6.0237923 5.3311005 4.442749 3.4767227 2.6059408 1.9778256 1.6448832 1.5924215 1.7220731 1.8716488 1.8366714][5.0675573 5.4078817 5.3060122 5.010375 4.4873972 3.7413549 2.8106475 1.8032007 0.85938549 0.101367 -0.41080809 -0.66035748 -0.6975646 -0.64058828 -0.65214443]]...]
INFO - root - 2017-12-05 09:28:52.201023: step 10710, loss = 1.10, batch loss = 1.05 (37.0 examples/sec; 0.216 sec/batch; 19h:20m:43s remains)
INFO - root - 2017-12-05 09:28:54.426291: step 10720, loss = 2.04, batch loss = 1.98 (33.9 examples/sec; 0.236 sec/batch; 21h:06m:28s remains)
INFO - root - 2017-12-05 09:28:56.575945: step 10730, loss = 1.80, batch loss = 1.74 (37.4 examples/sec; 0.214 sec/batch; 19h:05m:46s remains)
INFO - root - 2017-12-05 09:28:58.745816: step 10740, loss = 1.27, batch loss = 1.21 (37.8 examples/sec; 0.212 sec/batch; 18h:55m:10s remains)
INFO - root - 2017-12-05 09:29:00.924796: step 10750, loss = 1.19, batch loss = 1.13 (36.9 examples/sec; 0.217 sec/batch; 19h:21m:25s remains)
INFO - root - 2017-12-05 09:29:03.108218: step 10760, loss = 1.96, batch loss = 1.90 (37.1 examples/sec; 0.216 sec/batch; 19h:16m:36s remains)
INFO - root - 2017-12-05 09:29:05.280998: step 10770, loss = 0.93, batch loss = 0.88 (37.6 examples/sec; 0.213 sec/batch; 19h:01m:47s remains)
INFO - root - 2017-12-05 09:29:07.470960: step 10780, loss = 1.26, batch loss = 1.20 (37.0 examples/sec; 0.216 sec/batch; 19h:20m:14s remains)
INFO - root - 2017-12-05 09:29:09.661352: step 10790, loss = 1.34, batch loss = 1.28 (37.1 examples/sec; 0.216 sec/batch; 19h:16m:20s remains)
INFO - root - 2017-12-05 09:29:11.836463: step 10800, loss = 1.81, batch loss = 1.75 (36.4 examples/sec; 0.220 sec/batch; 19h:38m:56s remains)
2017-12-05 09:29:12.110047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4032407 -4.4026432 -4.4023285 -4.402092 -4.4018545 -4.4016871 -4.4014382 -4.4011092 -4.400775 -4.4004831 -4.40016 -4.400609 -4.4014158 -4.4021649 -4.4027352][-4.3973656 -4.3944459 -4.3928418 -4.3916044 -4.38979 -4.3872223 -4.3840456 -4.3799534 -4.37612 -4.3740053 -4.3739543 -4.377192 -4.3831244 -4.388979 -4.3938079][-4.3799586 -4.3691497 -4.3617187 -4.3550143 -4.3463368 -4.3345675 -4.3198323 -4.3013754 -4.2850633 -4.2774 -4.2798266 -4.2939715 -4.3167496 -4.3417115 -4.3624024][-4.3421769 -4.3124065 -4.2890706 -4.2680936 -4.2427721 -4.20935 -4.1678038 -4.1170521 -4.0708241 -4.0474658 -4.0558157 -4.0970054 -4.1591344 -4.2267466 -4.2846556][-4.2732792 -4.2070794 -4.1501956 -4.1006947 -4.0473638 -3.9805908 -3.8988607 -3.7990975 -3.7066665 -3.657232 -3.6734986 -3.758455 -3.8888071 -4.0295963 -4.1515741][-4.1735868 -4.0495334 -3.93832 -3.8442574 -3.75479 -3.651721 -3.5286188 -3.37732 -3.2338648 -3.1536608 -3.1783416 -3.3182611 -3.5370138 -3.7748694 -3.9826949][-4.06122 -3.8665662 -3.6860688 -3.5367484 -3.4090796 -3.2777483 -3.1272731 -2.9409363 -2.7582998 -2.6539488 -2.6920404 -2.8888845 -3.1974552 -3.532515 -3.8256392][-3.9720056 -3.7155495 -3.4725971 -3.2756145 -3.1207716 -2.9793329 -2.8254309 -2.6291738 -2.4277353 -2.3083839 -2.355072 -2.5914235 -2.9654241 -3.3719416 -3.7261527][-3.941931 -3.6580997 -3.3854637 -3.1695638 -3.0130584 -2.8853891 -2.7488077 -2.5637178 -2.3605886 -2.2289042 -2.2661622 -2.5063996 -2.8999727 -3.3310859 -3.7060881][-3.9817936 -3.7152748 -3.4560025 -3.2532198 -3.1142588 -3.0103974 -2.8989463 -2.7378206 -2.5488682 -2.4093037 -2.4229319 -2.6357617 -3.0038919 -3.40969 -3.762223][-4.0727272 -3.8592746 -3.6505122 -3.4878361 -3.377238 -3.2954845 -3.2034798 -3.0637531 -2.8919029 -2.7540519 -2.7493868 -2.9240766 -3.2371666 -3.5814528 -3.8788776][-4.1827264 -4.0383615 -3.8985002 -3.7916584 -3.7174792 -3.6555619 -3.5761023 -3.4524355 -3.3010664 -3.1797423 -3.1702986 -3.3056505 -3.5480387 -3.8102026 -4.0323529][-4.2800345 -4.200026 -4.1244192 -4.0693626 -4.0296144 -3.9877315 -3.9244635 -3.8250458 -3.7064428 -3.6147752 -3.6075063 -3.7017522 -3.8667877 -4.04046 -4.1833019][-4.3475223 -4.3129663 -4.2819767 -4.26063 -4.2441053 -4.2207332 -4.1804304 -4.1164637 -4.0413251 -3.9850798 -3.9809263 -4.035028 -4.1274896 -4.222024 -4.2969804][-4.38361 -4.3720536 -4.3625994 -4.3563323 -4.3507085 -4.34051 -4.3219085 -4.2916107 -4.2558556 -4.229197 -4.227088 -4.250957 -4.2908573 -4.3306813 -4.3614368]]...]
INFO - root - 2017-12-05 09:29:14.254106: step 10810, loss = 1.73, batch loss = 1.67 (37.7 examples/sec; 0.212 sec/batch; 18h:57m:49s remains)
INFO - root - 2017-12-05 09:29:16.418906: step 10820, loss = 1.81, batch loss = 1.75 (36.2 examples/sec; 0.221 sec/batch; 19h:46m:10s remains)
INFO - root - 2017-12-05 09:29:18.567511: step 10830, loss = 1.34, batch loss = 1.29 (37.3 examples/sec; 0.215 sec/batch; 19h:10m:23s remains)
INFO - root - 2017-12-05 09:29:20.759778: step 10840, loss = 1.09, batch loss = 1.03 (36.8 examples/sec; 0.217 sec/batch; 19h:25m:40s remains)
INFO - root - 2017-12-05 09:29:22.943625: step 10850, loss = 1.42, batch loss = 1.36 (37.6 examples/sec; 0.213 sec/batch; 19h:01m:38s remains)
INFO - root - 2017-12-05 09:29:25.115426: step 10860, loss = 1.18, batch loss = 1.12 (37.8 examples/sec; 0.212 sec/batch; 18h:55m:57s remains)
INFO - root - 2017-12-05 09:29:27.264990: step 10870, loss = 1.15, batch loss = 1.09 (38.1 examples/sec; 0.210 sec/batch; 18h:45m:50s remains)
INFO - root - 2017-12-05 09:29:29.428241: step 10880, loss = 1.36, batch loss = 1.30 (37.2 examples/sec; 0.215 sec/batch; 19h:11m:18s remains)
INFO - root - 2017-12-05 09:29:31.574630: step 10890, loss = 1.31, batch loss = 1.26 (37.7 examples/sec; 0.212 sec/batch; 18h:58m:39s remains)
INFO - root - 2017-12-05 09:29:33.731009: step 10900, loss = 1.41, batch loss = 1.35 (35.4 examples/sec; 0.226 sec/batch; 20h:12m:53s remains)
2017-12-05 09:29:34.012542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1335318 -2.1588919 -2.3147309 -2.5533528 -2.8301783 -3.1123681 -3.3745623 -3.5906906 -3.7404532 -3.8184583 -3.8384738 -3.8214624 -3.8113396 -3.8410397 -3.9147575][-1.7306201 -1.7416098 -1.8891749 -2.1194212 -2.3899136 -2.6715119 -2.9412246 -3.1692371 -3.3281155 -3.4089971 -3.4317942 -3.424988 -3.4410951 -3.5189712 -3.6561041][-1.6357157 -1.6237187 -1.7336724 -1.9110484 -2.1198037 -2.339649 -2.5542412 -2.7388053 -2.8669779 -2.9298744 -2.9523506 -2.9670274 -3.0285108 -3.1732326 -3.3874512][-1.7835627 -1.7467167 -1.8050196 -1.9051266 -2.0185626 -2.1347852 -2.2484648 -2.3497837 -2.4212713 -2.4572229 -2.4830494 -2.5304594 -2.6477361 -2.8621044 -3.1494861][-2.0529785 -1.981292 -1.9766922 -1.9907629 -2.0004203 -2.0048771 -2.0104821 -2.0228658 -2.0366426 -2.0486588 -2.0805392 -2.1597075 -2.3274288 -2.6010506 -2.9505465][-2.3163526 -2.1950021 -2.1230483 -2.0554855 -1.9751518 -1.8906622 -1.814208 -1.758817 -1.7275765 -1.7199798 -1.7527857 -1.8529875 -2.0602422 -2.3828473 -2.7857339][-2.5447021 -2.3751936 -2.2413349 -2.1069458 -1.9611621 -1.8180072 -1.6905253 -1.5932677 -1.5320191 -1.5073502 -1.535454 -1.6477339 -1.8856392 -2.2489874 -2.6951323][-2.7937293 -2.597084 -2.4254856 -2.2548137 -2.0810118 -1.9182243 -1.7750533 -1.6620338 -1.5832982 -1.5445151 -1.5653813 -1.6820409 -1.9348381 -2.313957 -2.7688565][-3.1490622 -2.9666047 -2.8026381 -2.6426945 -2.488018 -2.3477373 -2.2242029 -2.12141 -2.042413 -1.997961 -2.0092745 -2.1120512 -2.3400564 -2.6760509 -3.0689535][-3.5865455 -3.4542561 -3.3325436 -3.2147605 -3.1041927 -3.0047972 -2.9163051 -2.8388147 -2.7761359 -2.7385287 -2.7432275 -2.8180838 -2.9861398 -3.2291245 -3.5054588][-3.9733047 -3.8989599 -3.8291221 -3.7610283 -3.6976423 -3.6402471 -3.5880783 -3.5396507 -3.4992251 -3.4739413 -3.4750164 -3.5184493 -3.6170902 -3.7572787 -3.9129722][-4.2315149 -4.2006254 -4.1709762 -4.1406817 -4.1122675 -4.0855412 -4.0602617 -4.0351911 -4.01412 -4.0008073 -3.9991839 -4.0171418 -4.0610828 -4.1233077 -4.1910353][-4.3523149 -4.3433919 -4.3349276 -4.3252711 -4.3159294 -4.306612 -4.2969785 -4.2868071 -4.277957 -4.2723694 -4.2706027 -4.2753668 -4.28926 -4.309989 -4.3325691][-4.393899 -4.3923073 -4.3907528 -4.3886256 -4.3866158 -4.3844953 -4.3817654 -4.3783941 -4.375248 -4.3735981 -4.3729329 -4.3738437 -4.3769159 -4.3818655 -4.3872128][-4.4022293 -4.402257 -4.4020419 -4.401813 -4.4016128 -4.4012508 -4.4006095 -4.3995652 -4.3984075 -4.3976178 -4.3970141 -4.3968987 -4.3974781 -4.3986158 -4.3998127]]...]
INFO - root - 2017-12-05 09:29:36.163882: step 10910, loss = 1.49, batch loss = 1.43 (38.5 examples/sec; 0.208 sec/batch; 18h:33m:32s remains)
INFO - root - 2017-12-05 09:29:38.328849: step 10920, loss = 1.55, batch loss = 1.49 (36.6 examples/sec; 0.219 sec/batch; 19h:33m:03s remains)
INFO - root - 2017-12-05 09:29:40.553387: step 10930, loss = 1.45, batch loss = 1.39 (37.1 examples/sec; 0.216 sec/batch; 19h:15m:01s remains)
INFO - root - 2017-12-05 09:29:42.753343: step 10940, loss = 1.66, batch loss = 1.61 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:32s remains)
INFO - root - 2017-12-05 09:29:44.937151: step 10950, loss = 1.21, batch loss = 1.15 (36.5 examples/sec; 0.219 sec/batch; 19h:34m:18s remains)
INFO - root - 2017-12-05 09:29:47.120402: step 10960, loss = 2.07, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 19h:30m:33s remains)
INFO - root - 2017-12-05 09:29:49.324400: step 10970, loss = 1.41, batch loss = 1.35 (35.7 examples/sec; 0.224 sec/batch; 20h:00m:13s remains)
INFO - root - 2017-12-05 09:29:51.502861: step 10980, loss = 1.67, batch loss = 1.62 (36.3 examples/sec; 0.220 sec/batch; 19h:40m:48s remains)
INFO - root - 2017-12-05 09:29:53.708972: step 10990, loss = 1.52, batch loss = 1.46 (36.6 examples/sec; 0.219 sec/batch; 19h:31m:13s remains)
INFO - root - 2017-12-05 09:29:55.889994: step 11000, loss = 1.23, batch loss = 1.17 (37.6 examples/sec; 0.213 sec/batch; 19h:01m:02s remains)
2017-12-05 09:29:56.160079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9270689 -3.8141611 -3.7581565 -3.7718115 -3.8512921 -3.9708629 -4.0980849 -4.2104707 -4.2939177 -4.3464532 -4.3729839 -4.3837743 -4.38096 -4.3618212 -4.3265038][-3.3122716 -3.0529475 -2.9095242 -2.9160318 -3.0707412 -3.3198466 -3.5962658 -3.8498468 -4.0495744 -4.1860352 -4.2655687 -4.3058066 -4.31079 -4.2774987 -4.2092948][-2.4479575 -1.9764297 -1.6907759 -1.6532273 -1.86008 -2.2405093 -2.6934509 -3.130734 -3.49933 -3.7743113 -3.9584947 -4.070323 -4.1135478 -4.08585 -3.9964406][-1.459692 -0.7243104 -0.23833609 -0.090102196 -0.2850728 -0.74497271 -1.3433945 -1.959517 -2.518084 -2.9732819 -3.3120773 -3.5468881 -3.6764326 -3.6978564 -3.622551][-0.50730348 0.51558161 1.2560353 1.5984478 1.5169811 1.0763063 0.40628862 -0.34396172 -1.0719414 -1.7104292 -2.2234089 -2.6122162 -2.867857 -2.98833 -2.9874089][0.24690962 1.5316334 2.5474334 3.1497307 3.2865205 2.9914317 2.3683972 1.5758824 0.74747419 -0.025806904 -0.68597746 -1.2181001 -1.609175 -1.8648584 -2.0042911][0.67952108 2.1499958 3.4039631 4.2853823 4.7172279 4.6932678 4.2692595 3.581171 2.7794189 1.9741044 1.2423983 0.61118889 0.090950012 -0.33186722 -0.6793282][0.76937485 2.307303 3.7056723 4.8221407 5.5639572 5.8921623 5.8080683 5.3897028 4.767498 4.0595274 3.3579502 2.6920643 2.0633817 1.4498796 0.84075642][0.62569618 2.1318383 3.562479 4.808814 5.7841129 6.4369802 6.7266788 6.669219 6.3464918 5.8569431 5.2869339 4.6604214 3.9640694 3.1684422 2.2882318][0.36825657 1.7739601 3.1345453 4.3783731 5.443656 6.2919378 6.8650932 7.1320019 7.1246109 6.901227 6.5169964 5.9843178 5.2820263 4.3745584 3.3008738][0.0766778 1.3313227 2.5277233 3.6251745 4.6019955 5.4568729 6.1446075 6.6107073 6.8429337 6.8505898 6.6493883 6.2354507 5.5838556 4.6670485 3.5431833][-0.30316114 0.75867081 1.7270646 2.5805702 3.3341169 4.0322104 4.6688128 5.1909122 5.5511446 5.7143831 5.6636448 5.3724585 4.8152075 3.9808588 2.9474587][-0.85398626 -0.006626606 0.71799564 1.3052592 1.7938762 2.2645564 2.7480783 3.2073865 3.5833173 3.8115616 3.8502522 3.6594706 3.2178745 2.5321722 1.6901822][-1.5249019 -0.8885479 -0.38585091 -0.033438683 0.21671247 0.45704556 0.74243259 1.0604138 1.3606267 1.5721307 1.6413918 1.5219154 1.2012882 0.69777155 0.092467308][-2.2203212 -1.7793677 -1.4657662 -1.2954166 -1.2228875 -1.1648953 -1.0613754 -0.90742254 -0.73458958 -0.59860587 -0.54689884 -0.6225698 -0.83138275 -1.1489546 -1.5144491]]...]
INFO - root - 2017-12-05 09:29:58.381206: step 11010, loss = 2.24, batch loss = 2.19 (35.8 examples/sec; 0.224 sec/batch; 19h:57m:51s remains)
INFO - root - 2017-12-05 09:30:00.575975: step 11020, loss = 1.31, batch loss = 1.25 (37.2 examples/sec; 0.215 sec/batch; 19h:12m:08s remains)
INFO - root - 2017-12-05 09:30:02.773206: step 11030, loss = 1.68, batch loss = 1.62 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:14s remains)
INFO - root - 2017-12-05 09:30:04.975164: step 11040, loss = 1.57, batch loss = 1.51 (37.0 examples/sec; 0.216 sec/batch; 19h:19m:24s remains)
INFO - root - 2017-12-05 09:30:07.171383: step 11050, loss = 1.73, batch loss = 1.67 (37.3 examples/sec; 0.215 sec/batch; 19h:10m:22s remains)
INFO - root - 2017-12-05 09:30:09.345078: step 11060, loss = 1.57, batch loss = 1.51 (36.9 examples/sec; 0.217 sec/batch; 19h:22m:40s remains)
INFO - root - 2017-12-05 09:30:11.520272: step 11070, loss = 1.46, batch loss = 1.40 (34.5 examples/sec; 0.232 sec/batch; 20h:41m:12s remains)
INFO - root - 2017-12-05 09:30:13.714492: step 11080, loss = 1.41, batch loss = 1.36 (35.3 examples/sec; 0.226 sec/batch; 20h:13m:11s remains)
INFO - root - 2017-12-05 09:30:15.882531: step 11090, loss = 1.61, batch loss = 1.55 (36.8 examples/sec; 0.217 sec/batch; 19h:23m:58s remains)
INFO - root - 2017-12-05 09:30:18.062938: step 11100, loss = 0.83, batch loss = 0.77 (37.1 examples/sec; 0.215 sec/batch; 19h:14m:10s remains)
2017-12-05 09:30:18.353277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.61562204 -0.5192492 -0.3843236 0.091111183 0.9540534 2.0624313 3.1395731 3.8516665 3.9651814 3.4434319 2.4274039 1.1042376 -0.35491991 -1.7573838 -2.8906932][-0.089949608 -0.02255106 0.097714424 0.60028839 1.5314665 2.7292457 3.8804331 4.6105862 4.6646404 4.0054235 2.8055544 1.2941198 -0.33065605 -1.8474839 -3.0306375][-0.17383671 -0.11595154 0.025208473 0.56679726 1.5493355 2.8042459 4.0123358 4.7943506 4.8779755 4.206356 2.943028 1.33289 -0.39063406 -1.9718966 -3.1704805][-0.83680606 -0.76319861 -0.56351089 0.035224915 1.0622478 2.3549991 3.6140795 4.4764662 4.6616869 4.0769567 2.8489137 1.2241783 -0.52771759 -2.1166236 -3.2898955][-1.8280985 -1.7326159 -1.4693642 -0.82212067 0.22789621 1.5319819 2.8219891 3.7605114 4.0613136 3.5944996 2.4473667 0.86812353 -0.83430219 -2.3487391 -3.4332275][-2.8425834 -2.7281196 -2.4238331 -1.7639151 -0.73281574 0.54151249 1.8242598 2.805603 3.1953483 2.8281894 1.7654705 0.26961851 -1.3118637 -2.6696544 -3.6066589][-3.6343658 -3.5165076 -3.2068574 -2.5781488 -1.6066902 -0.40161562 0.82497644 1.7904806 2.2080293 1.8931689 0.89629555 -0.49764156 -1.9075775 -3.050036 -3.7994645][-4.109138 -4.0016632 -3.7237997 -3.1671567 -2.2956314 -1.1991289 -0.078539371 0.80856085 1.1914783 0.88760424 -0.051939011 -1.3226094 -2.5287175 -3.4321127 -3.9852498][-4.3173709 -4.2326336 -4.0031028 -3.5351567 -2.7858205 -1.8282125 -0.85507751 -0.10196018 0.19464922 -0.1238246 -0.99331 -2.1062944 -3.0869536 -3.7576725 -4.1370091][-4.3735795 -4.3108344 -4.1306729 -3.7528915 -3.1349611 -2.3359826 -1.5369375 -0.94978809 -0.76400042 -1.0954409 -1.8617694 -2.7789338 -3.5292552 -3.9977882 -4.2419748][-4.3863821 -4.3382707 -4.1986108 -3.903285 -3.417161 -2.7882304 -2.1727037 -1.7494848 -1.6607149 -1.9782133 -2.6084538 -3.3126836 -3.8498127 -4.1589532 -4.3085375][-4.3913755 -4.355073 -4.2513437 -4.0335212 -3.6767225 -3.2176182 -2.7780948 -2.4957333 -2.470741 -2.7414322 -3.2165921 -3.7154562 -4.0720134 -4.2647491 -4.3506966][-4.3951325 -4.37092 -4.30106 -4.1547155 -3.9168286 -3.6125157 -3.3270073 -3.1551995 -3.1613083 -3.3618937 -3.6817658 -3.9990036 -4.2137585 -4.3255363 -4.3730812][-4.3977451 -4.3843627 -4.3436823 -4.2574492 -4.1178555 -3.9407079 -3.7764812 -3.6834173 -3.6973572 -3.8239319 -4.0111351 -4.1860619 -4.3002806 -4.3600874 -4.3857412][-4.3992925 -4.3935394 -4.3740468 -4.3315592 -4.2625561 -4.1759062 -4.0966845 -4.0539727 -4.0647116 -4.1300769 -4.2211366 -4.30179 -4.3531685 -4.3808465 -4.3933053]]...]
INFO - root - 2017-12-05 09:30:20.516992: step 11110, loss = 1.30, batch loss = 1.24 (37.5 examples/sec; 0.213 sec/batch; 19h:01m:18s remains)
INFO - root - 2017-12-05 09:30:22.705117: step 11120, loss = 1.97, batch loss = 1.91 (36.2 examples/sec; 0.221 sec/batch; 19h:42m:59s remains)
INFO - root - 2017-12-05 09:30:24.888990: step 11130, loss = 1.28, batch loss = 1.22 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:07s remains)
INFO - root - 2017-12-05 09:30:27.085556: step 11140, loss = 1.81, batch loss = 1.76 (36.4 examples/sec; 0.220 sec/batch; 19h:36m:17s remains)
INFO - root - 2017-12-05 09:30:29.284947: step 11150, loss = 1.58, batch loss = 1.52 (37.2 examples/sec; 0.215 sec/batch; 19h:10m:56s remains)
INFO - root - 2017-12-05 09:30:31.481108: step 11160, loss = 1.57, batch loss = 1.52 (36.4 examples/sec; 0.220 sec/batch; 19h:37m:05s remains)
INFO - root - 2017-12-05 09:30:33.639764: step 11170, loss = 1.57, batch loss = 1.52 (37.6 examples/sec; 0.213 sec/batch; 18h:59m:29s remains)
INFO - root - 2017-12-05 09:30:35.813221: step 11180, loss = 1.92, batch loss = 1.86 (37.1 examples/sec; 0.215 sec/batch; 19h:13m:38s remains)
INFO - root - 2017-12-05 09:30:37.979107: step 11190, loss = 1.63, batch loss = 1.57 (37.6 examples/sec; 0.213 sec/batch; 19h:00m:14s remains)
INFO - root - 2017-12-05 09:30:40.168941: step 11200, loss = 1.39, batch loss = 1.33 (36.2 examples/sec; 0.221 sec/batch; 19h:44m:42s remains)
2017-12-05 09:30:40.451408: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.1467485 2.9199553 2.4873638 2.09449 1.8485484 1.7833524 1.8295035 1.8665838 1.7404404 1.4000592 0.88756323 0.31083918 -0.22429228 -0.65832376 -1.0142787][3.5904646 3.3831835 2.9502554 2.5606833 2.313561 2.2515278 2.2864614 2.3092456 2.1663861 1.8069177 1.2677422 0.65249777 0.06629467 -0.4151299 -0.807137][3.8414345 3.6583385 3.2155747 2.8016119 2.5215869 2.4281869 2.4387155 2.42794 2.2489204 1.8595161 1.304739 0.6719203 0.057608128 -0.45369053 -0.86904335][4.0946636 3.9519587 3.4803619 2.9921608 2.6376529 2.4779758 2.4368973 2.3798423 2.1656647 1.7487693 1.1732154 0.52055693 -0.10780525 -0.63129592 -1.0524948][4.2860022 4.2170219 3.7259035 3.1535273 2.7009292 2.4549108 2.3484821 2.2421584 1.9942889 1.5477123 0.94721651 0.27849197 -0.34807348 -0.85405517 -1.2424657][4.3645334 4.3869257 3.8863277 3.2473502 2.7039051 2.3622341 2.1929574 2.0508361 1.7870374 1.3213334 0.69787741 0.020710468 -0.58529687 -1.0367157 -1.3429034][4.371398 4.4645872 3.9423027 3.2342892 2.5843706 2.1563454 1.9446812 1.8056254 1.5633216 1.1099238 0.48549938 -0.18823957 -0.76082182 -1.1353648 -1.3268497][4.2568569 4.3899078 3.8390684 3.0625238 2.3165503 1.8096571 1.5711436 1.4699254 1.296566 0.90196085 0.31515884 -0.33094883 -0.86377835 -1.1641455 -1.2443957][3.9439297 4.0761595 3.5024161 2.6684217 1.8585911 1.2978082 1.0468006 0.99451637 0.92068338 0.63317919 0.1318593 -0.453686 -0.93821645 -1.1824772 -1.1843958][3.3099093 3.4278698 2.8614283 2.0296121 1.2124133 0.63620424 0.38851643 0.3781004 0.40461874 0.25181293 -0.12044525 -0.61072707 -1.0370755 -1.2465599 -1.2171123][2.3935227 2.50132 1.9792252 1.2053952 0.43594885 -0.11091423 -0.33967352 -0.3216114 -0.22214079 -0.25364542 -0.48629689 -0.85855079 -1.2153497 -1.4048042 -1.3831384][1.2724547 1.3920946 0.95707846 0.28812933 -0.38226366 -0.86429048 -1.0660512 -1.0348895 -0.90153313 -0.84883738 -0.96020865 -1.2121165 -1.4898248 -1.6620293 -1.6691067][0.041141987 0.17428589 -0.15590906 -0.68268228 -1.2181072 -1.6121826 -1.7819707 -1.7543356 -1.6214297 -1.5277579 -1.5506921 -1.693301 -1.8864686 -2.0297575 -2.0615549][-1.2382529 -1.1060517 -1.3232324 -1.6968687 -2.0862417 -2.3823175 -2.5191669 -2.5038166 -2.3960588 -2.2980473 -2.2693338 -2.32825 -2.4370565 -2.5342803 -2.569406][-2.4584475 -2.3455276 -2.4573674 -2.686554 -2.9404261 -3.143472 -3.2458463 -3.2442784 -3.1749544 -3.0988407 -3.053462 -3.0605807 -3.1025553 -3.1491747 -3.1681852]]...]
INFO - root - 2017-12-05 09:30:42.598151: step 11210, loss = 1.65, batch loss = 1.59 (38.0 examples/sec; 0.210 sec/batch; 18h:46m:54s remains)
INFO - root - 2017-12-05 09:30:44.780175: step 11220, loss = 1.71, batch loss = 1.65 (37.1 examples/sec; 0.216 sec/batch; 19h:13m:58s remains)
INFO - root - 2017-12-05 09:30:46.930734: step 11230, loss = 1.81, batch loss = 1.75 (37.0 examples/sec; 0.216 sec/batch; 19h:19m:03s remains)
INFO - root - 2017-12-05 09:30:49.101468: step 11240, loss = 1.59, batch loss = 1.53 (36.7 examples/sec; 0.218 sec/batch; 19h:27m:46s remains)
INFO - root - 2017-12-05 09:30:51.272194: step 11250, loss = 1.32, batch loss = 1.26 (38.1 examples/sec; 0.210 sec/batch; 18h:43m:17s remains)
INFO - root - 2017-12-05 09:30:53.464623: step 11260, loss = 1.00, batch loss = 0.94 (34.2 examples/sec; 0.234 sec/batch; 20h:52m:27s remains)
INFO - root - 2017-12-05 09:30:55.695342: step 11270, loss = 1.30, batch loss = 1.24 (37.1 examples/sec; 0.215 sec/batch; 19h:13m:38s remains)
INFO - root - 2017-12-05 09:30:57.891883: step 11280, loss = 1.65, batch loss = 1.59 (35.6 examples/sec; 0.225 sec/batch; 20h:04m:44s remains)
INFO - root - 2017-12-05 09:31:00.101602: step 11290, loss = 1.47, batch loss = 1.42 (35.0 examples/sec; 0.228 sec/batch; 20h:22m:31s remains)
INFO - root - 2017-12-05 09:31:02.271282: step 11300, loss = 1.33, batch loss = 1.27 (37.8 examples/sec; 0.211 sec/batch; 18h:51m:44s remains)
2017-12-05 09:31:02.563436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3998356 -4.3959675 -4.3887677 -4.3777685 -4.3627505 -4.3460994 -4.336019 -4.33685 -4.3413858 -4.3440304 -4.3431096 -4.3367257 -4.3265691 -4.3167515 -4.3092504][-4.4006405 -4.3968468 -4.3897886 -4.3790913 -4.3639317 -4.3475747 -4.3393025 -4.3439889 -4.351428 -4.3558769 -4.3570633 -4.352829 -4.3444729 -4.33509 -4.3265533][-4.4012132 -4.3975248 -4.3908429 -4.3810859 -4.3669662 -4.3518324 -4.3451085 -4.3524623 -4.3619428 -4.3673949 -4.369873 -4.3675914 -4.3612056 -4.3524632 -4.3432889][-4.4006996 -4.3963814 -4.3889494 -4.3785954 -4.3638258 -4.3477612 -4.3404441 -4.3490019 -4.3595324 -4.3654037 -4.36912 -4.3690643 -4.3649521 -4.3572483 -4.348321][-4.3994727 -4.3939013 -4.3846674 -4.3715487 -4.3537312 -4.3338075 -4.323904 -4.3334932 -4.3459773 -4.3532696 -4.3595958 -4.3634434 -4.3640766 -4.3596587 -4.3513422][-4.3981442 -4.391335 -4.3797975 -4.3632727 -4.3415422 -4.3164477 -4.302289 -4.3125353 -4.3278465 -4.3379278 -4.3483076 -4.3575783 -4.3649955 -4.365725 -4.3581572][-4.3972249 -4.3898439 -4.3767948 -4.3578119 -4.333508 -4.3052082 -4.2880335 -4.2979584 -4.3152533 -4.3275061 -4.3410511 -4.3544655 -4.3670378 -4.3718944 -4.3644462][-4.3970046 -4.3900895 -4.377326 -4.3581295 -4.3338723 -4.3058844 -4.288393 -4.2970176 -4.3137975 -4.3264275 -4.3399529 -4.3540959 -4.3684826 -4.3746986 -4.3663921][-4.3975506 -4.3919916 -4.3814645 -4.36458 -4.3429804 -4.318212 -4.3021827 -4.3081951 -4.3213153 -4.3319793 -4.3431997 -4.3555632 -4.3694124 -4.3758655 -4.3678722][-4.398489 -4.3945627 -4.3868847 -4.3740668 -4.3569174 -4.33721 -4.323812 -4.3266783 -4.3347077 -4.3415475 -4.3494306 -4.3589687 -4.3714175 -4.3780842 -4.3719563][-4.3994837 -4.3968325 -4.3916759 -4.3826323 -4.36972 -4.3547645 -4.3444867 -4.3463039 -4.3509154 -4.3543258 -4.3587685 -4.3659134 -4.3763738 -4.382864 -4.3793311][-4.4003663 -4.3985982 -4.3952188 -4.3891692 -4.3801556 -4.369287 -4.36238 -4.365171 -4.3685913 -4.3699427 -4.3715935 -4.3761644 -4.3837514 -4.3892865 -4.3880486][-4.4010921 -4.3998904 -4.3976693 -4.3938866 -4.3883414 -4.3813033 -4.37748 -4.381032 -4.3841538 -4.384757 -4.3849587 -4.3872294 -4.3916011 -4.3952432 -4.3950586][-4.4017348 -4.401053 -4.3997893 -4.3976884 -4.3946977 -4.390656 -4.3889785 -4.3921547 -4.3946795 -4.3949475 -4.3945718 -4.3953376 -4.3973713 -4.3991933 -4.399344][-4.402225 -4.4020367 -4.4015455 -4.4005871 -4.3991966 -4.397151 -4.39655 -4.3986239 -4.4001017 -4.400104 -4.3995881 -4.3995619 -4.4002266 -4.4009733 -4.4011517]]...]
INFO - root - 2017-12-05 09:31:04.720886: step 11310, loss = 1.20, batch loss = 1.15 (37.0 examples/sec; 0.216 sec/batch; 19h:17m:40s remains)
INFO - root - 2017-12-05 09:31:06.909702: step 11320, loss = 1.84, batch loss = 1.78 (37.4 examples/sec; 0.214 sec/batch; 19h:04m:01s remains)
INFO - root - 2017-12-05 09:31:09.110076: step 11330, loss = 1.65, batch loss = 1.59 (35.3 examples/sec; 0.227 sec/batch; 20h:12m:49s remains)
INFO - root - 2017-12-05 09:31:11.293074: step 11340, loss = 1.57, batch loss = 1.51 (35.6 examples/sec; 0.224 sec/batch; 20h:01m:36s remains)
INFO - root - 2017-12-05 09:31:13.461679: step 11350, loss = 1.19, batch loss = 1.13 (35.9 examples/sec; 0.223 sec/batch; 19h:53m:25s remains)
INFO - root - 2017-12-05 09:31:15.641110: step 11360, loss = 1.59, batch loss = 1.53 (37.2 examples/sec; 0.215 sec/batch; 19h:09m:42s remains)
INFO - root - 2017-12-05 09:31:17.821154: step 11370, loss = 1.34, batch loss = 1.29 (36.8 examples/sec; 0.217 sec/batch; 19h:23m:08s remains)
INFO - root - 2017-12-05 09:31:20.006536: step 11380, loss = 1.46, batch loss = 1.40 (36.7 examples/sec; 0.218 sec/batch; 19h:25m:47s remains)
INFO - root - 2017-12-05 09:31:22.217725: step 11390, loss = 1.12, batch loss = 1.06 (36.1 examples/sec; 0.222 sec/batch; 19h:46m:48s remains)
INFO - root - 2017-12-05 09:31:24.440828: step 11400, loss = 1.43, batch loss = 1.37 (36.0 examples/sec; 0.222 sec/batch; 19h:48m:07s remains)
2017-12-05 09:31:24.721485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2824593 -4.2624483 -4.2416477 -4.2253394 -4.2194905 -4.2258325 -4.2420006 -4.262188 -4.2774215 -4.2789431 -4.2613587 -4.2223096 -4.1657214 -4.1103725 -4.0782795][-4.2324038 -4.2018747 -4.1738806 -4.1550965 -4.1516042 -4.1634221 -4.1853247 -4.2095804 -4.225996 -4.2241321 -4.2000737 -4.1540203 -4.0910573 -4.034966 -4.0093479][-4.19836 -4.1600904 -4.1290131 -4.1103339 -4.1091828 -4.1233144 -4.1458082 -4.1672716 -4.1772137 -4.16591 -4.1332397 -4.0829997 -4.023952 -3.9789195 -3.9690826][-4.1887345 -4.148035 -4.1174879 -4.1005211 -4.0999045 -4.1117845 -4.127213 -4.1364756 -4.1312447 -4.1062684 -4.0640564 -4.0117836 -3.9623408 -3.9363503 -3.9480386][-4.2010684 -4.1655412 -4.1407194 -4.1276331 -4.1262536 -4.1309443 -4.1319408 -4.1217818 -4.0971894 -4.0572324 -4.0066366 -3.9547911 -3.9165685 -3.9101026 -3.9422314][-4.2222056 -4.1949849 -4.1780019 -4.1703944 -4.1688895 -4.1664047 -4.1536245 -4.1264305 -4.0860553 -4.034061 -3.9780192 -3.9284694 -3.9001021 -3.9077406 -3.952569][-4.2297564 -4.2091 -4.1986756 -4.1963148 -4.1976585 -4.1934948 -4.1738358 -4.1382179 -4.0910316 -4.0348191 -3.9780393 -3.9309912 -3.9077337 -3.9202936 -3.9679375][-4.2048583 -4.1869688 -4.1814289 -4.184751 -4.1909571 -4.1905279 -4.1719217 -4.1369004 -4.0910835 -4.0368176 -3.981638 -3.9361787 -3.9135666 -3.9244347 -3.9683795][-4.1443295 -4.1281781 -4.127461 -4.1353879 -4.1459074 -4.1500297 -4.1370411 -4.1081429 -4.0681148 -4.0191731 -3.9675732 -3.9227278 -3.8984733 -3.9051743 -3.9459524][-4.0634708 -4.0491538 -4.0534916 -4.0662441 -4.0806704 -4.0890985 -4.0826678 -4.0605464 -4.027503 -3.9838459 -3.935143 -3.8902383 -3.8644567 -3.8677986 -3.9074624][-3.9914761 -3.9796088 -3.9866581 -4.000576 -4.0158768 -4.0262556 -4.024333 -4.0074968 -3.9795222 -3.9399793 -3.8936424 -3.8491025 -3.8224573 -3.8241861 -3.8646007][-3.9464021 -3.9379504 -3.9440231 -3.9541333 -3.9649174 -3.9721174 -3.9699326 -3.954175 -3.9271178 -3.8891814 -3.8443179 -3.8009727 -3.773911 -3.7750125 -3.8167396][-3.9287505 -3.9264472 -3.9314795 -3.9353971 -3.9380324 -3.9361773 -3.9267938 -3.9058764 -3.8747115 -3.8354886 -3.7922156 -3.7516606 -3.7258813 -3.7286079 -3.7725034][-3.9350443 -3.9400625 -3.9457119 -3.9445291 -3.9365251 -3.9223998 -3.8997486 -3.8680954 -3.8312452 -3.7916374 -3.7521467 -3.7160776 -3.6924598 -3.6967859 -3.7424717][-3.9714105 -3.9828222 -3.9897344 -3.9823837 -3.9627972 -3.9347167 -3.8984828 -3.8572464 -3.8174884 -3.7822225 -3.7510588 -3.7239504 -3.7057133 -3.7121353 -3.7587438]]...]
INFO - root - 2017-12-05 09:31:26.899114: step 11410, loss = 1.39, batch loss = 1.33 (37.5 examples/sec; 0.213 sec/batch; 19h:01m:55s remains)
INFO - root - 2017-12-05 09:31:29.089454: step 11420, loss = 1.36, batch loss = 1.30 (37.0 examples/sec; 0.216 sec/batch; 19h:16m:00s remains)
INFO - root - 2017-12-05 09:31:31.261137: step 11430, loss = 1.84, batch loss = 1.78 (37.1 examples/sec; 0.216 sec/batch; 19h:13m:25s remains)
INFO - root - 2017-12-05 09:31:33.416949: step 11440, loss = 1.42, batch loss = 1.36 (38.2 examples/sec; 0.209 sec/batch; 18h:39m:43s remains)
INFO - root - 2017-12-05 09:31:35.627864: step 11450, loss = 1.84, batch loss = 1.78 (35.1 examples/sec; 0.228 sec/batch; 20h:18m:08s remains)
INFO - root - 2017-12-05 09:31:37.800346: step 11460, loss = 1.20, batch loss = 1.14 (36.4 examples/sec; 0.220 sec/batch; 19h:37m:10s remains)
INFO - root - 2017-12-05 09:31:39.957927: step 11470, loss = 1.37, batch loss = 1.31 (36.6 examples/sec; 0.219 sec/batch; 19h:29m:18s remains)
INFO - root - 2017-12-05 09:31:42.145901: step 11480, loss = 1.58, batch loss = 1.52 (36.8 examples/sec; 0.217 sec/batch; 19h:22m:23s remains)
INFO - root - 2017-12-05 09:31:44.351477: step 11490, loss = 1.43, batch loss = 1.37 (36.2 examples/sec; 0.221 sec/batch; 19h:41m:57s remains)
INFO - root - 2017-12-05 09:31:46.536836: step 11500, loss = 1.84, batch loss = 1.78 (37.1 examples/sec; 0.215 sec/batch; 19h:12m:27s remains)
2017-12-05 09:31:46.801844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3998737 -4.3974452 -4.3937984 -4.3886628 -4.3827662 -4.3764024 -4.3706827 -4.3678517 -4.3675203 -4.3682723 -4.3698092 -4.37219 -4.3739858 -4.3728271 -4.3692122][-4.3831391 -4.3747387 -4.3621283 -4.3454227 -4.3262544 -4.3055525 -4.2883148 -4.2810678 -4.2837257 -4.2927537 -4.3060369 -4.32043 -4.3294897 -4.3266869 -4.3131795][-4.3208513 -4.2982211 -4.2659688 -4.2246504 -4.1772037 -4.1282096 -4.0910716 -4.0804167 -4.09592 -4.1291451 -4.1727591 -4.214148 -4.23591 -4.224175 -4.1831818][-4.1520386 -4.108387 -4.0493455 -3.9746006 -3.8898389 -3.8050864 -3.7469728 -3.7427294 -3.7899323 -3.8711162 -3.965261 -4.0436044 -4.07208 -4.0291605 -3.9284048][-3.8093667 -3.7544012 -3.6808224 -3.5843048 -3.4758008 -3.3725624 -3.3134155 -3.338109 -3.4386191 -3.5848453 -3.7344956 -3.8381357 -3.8446271 -3.7313881 -3.5300474][-3.2798405 -3.2449198 -3.1956158 -3.1179128 -3.0311356 -2.9577181 -2.9410589 -3.023293 -3.1871505 -3.3906438 -3.5700948 -3.6583679 -3.5953207 -3.3666224 -3.0293899][-2.6555414 -2.6839745 -2.7074838 -2.7011919 -2.6913433 -2.7015095 -2.7690003 -2.92379 -3.140173 -3.3693881 -3.5338981 -3.5582285 -3.3853672 -3.0160697 -2.5358558][-2.1397533 -2.2494073 -2.3614321 -2.4483898 -2.5403051 -2.6575117 -2.8222294 -3.0457163 -3.2939067 -3.5157933 -3.6301494 -3.5626531 -3.270226 -2.7734356 -2.1862512][-1.9254057 -2.0907404 -2.2574515 -2.4105511 -2.5834737 -2.7884486 -3.0289166 -3.2968049 -3.552408 -3.745753 -3.8001187 -3.6511433 -3.2738338 -2.7091248 -2.0913746][-2.0903227 -2.2694428 -2.4380813 -2.6039615 -2.8061805 -3.0470209 -3.313529 -3.5838375 -3.8185146 -3.9721169 -3.9785426 -3.7905395 -3.3976974 -2.85422 -2.295692][-2.586062 -2.7444453 -2.8770685 -3.008934 -3.183387 -3.3980932 -3.6311281 -3.8567903 -4.0413113 -4.1482968 -4.129231 -3.9532771 -3.6171007 -3.1752806 -2.7447052][-3.2259922 -3.3407488 -3.4240284 -3.5056751 -3.6222534 -3.7721181 -3.9348187 -4.0867467 -4.2050905 -4.2668042 -4.2424259 -4.1120834 -3.8744035 -3.5724716 -3.2912133][-3.7856212 -3.8521845 -3.8936157 -3.9332962 -3.9946947 -4.0770397 -4.1666794 -4.2477446 -4.308054 -4.3369646 -4.3190751 -4.241466 -4.1021261 -3.9284809 -3.7723711][-4.1443653 -4.1752486 -4.1915841 -4.2065067 -4.2312632 -4.2662425 -4.304698 -4.3386073 -4.362977 -4.3741951 -4.3651805 -4.3282852 -4.2608113 -4.1766572 -4.1022172][-4.3170385 -4.3288321 -4.3342996 -4.3384714 -4.345407 -4.3563395 -4.3689017 -4.3800807 -4.3881125 -4.3916278 -4.3880162 -4.3739791 -4.347528 -4.3129315 -4.2813821]]...]
INFO - root - 2017-12-05 09:31:49.013136: step 11510, loss = 1.61, batch loss = 1.56 (36.5 examples/sec; 0.219 sec/batch; 19h:31m:02s remains)
INFO - root - 2017-12-05 09:31:51.167627: step 11520, loss = 1.65, batch loss = 1.59 (39.5 examples/sec; 0.203 sec/batch; 18h:03m:32s remains)
INFO - root - 2017-12-05 09:31:53.320633: step 11530, loss = 1.81, batch loss = 1.75 (37.0 examples/sec; 0.216 sec/batch; 19h:17m:31s remains)
INFO - root - 2017-12-05 09:31:55.542748: step 11540, loss = 1.43, batch loss = 1.37 (36.8 examples/sec; 0.218 sec/batch; 19h:24m:25s remains)
INFO - root - 2017-12-05 09:31:57.718532: step 11550, loss = 1.99, batch loss = 1.93 (37.0 examples/sec; 0.216 sec/batch; 19h:16m:07s remains)
INFO - root - 2017-12-05 09:31:59.897157: step 11560, loss = 1.22, batch loss = 1.16 (36.2 examples/sec; 0.221 sec/batch; 19h:40m:53s remains)
INFO - root - 2017-12-05 09:32:02.053603: step 11570, loss = 1.23, batch loss = 1.17 (36.3 examples/sec; 0.220 sec/batch; 19h:38m:26s remains)
INFO - root - 2017-12-05 09:32:04.205003: step 11580, loss = 1.51, batch loss = 1.45 (36.4 examples/sec; 0.220 sec/batch; 19h:35m:50s remains)
INFO - root - 2017-12-05 09:32:06.405492: step 11590, loss = 1.53, batch loss = 1.47 (36.5 examples/sec; 0.219 sec/batch; 19h:32m:09s remains)
INFO - root - 2017-12-05 09:32:08.585501: step 11600, loss = 1.24, batch loss = 1.19 (33.6 examples/sec; 0.238 sec/batch; 21h:12m:22s remains)
2017-12-05 09:32:08.909296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0709062 -3.8632407 -3.5517762 -3.1146536 -2.5576668 -1.9308047 -1.3580668 -0.99673676 -0.98390269 -1.3715546 -2.0696545 -2.86415 -3.5473177 -4.0118623 -4.2603631][-4.09988 -3.8740168 -3.5132625 -2.9913735 -2.3268018 -1.5965185 -0.94223356 -0.52649426 -0.48606348 -0.87758994 -1.6268864 -2.520731 -3.3226972 -3.891423 -4.2080812][-4.15261 -3.9327345 -3.5558171 -2.9845719 -2.2429898 -1.4361279 -0.72778392 -0.28462791 -0.23297691 -0.62116194 -1.3830802 -2.3173482 -3.180716 -3.8079066 -4.1675038][-4.2186241 -4.025434 -3.6677303 -3.0965896 -2.3284931 -1.4836569 -0.74690461 -0.29164505 -0.23331976 -0.60767269 -1.3500257 -2.2717845 -3.1391551 -3.7774639 -4.1484232][-4.2794318 -4.1265526 -3.8208845 -3.3027701 -2.5721869 -1.7406309 -1.0011616 -0.53718352 -0.46132064 -0.80167079 -1.4915597 -2.3583131 -3.1820698 -3.7925687 -4.150763][-4.3278713 -4.2184935 -3.9809444 -3.5489101 -2.9035792 -2.1302788 -1.4114521 -0.9387188 -0.83191752 -1.1214008 -1.7409894 -2.5303063 -3.2841148 -3.8438511 -4.1716123][-4.3601041 -4.2873759 -4.1167092 -3.7810035 -3.2442236 -2.5583646 -1.8790374 -1.3956029 -1.2454348 -1.4731975 -2.0201993 -2.7334018 -3.4138136 -3.9136696 -4.2015061][-4.3714352 -4.324244 -4.2070932 -3.9601655 -3.5381637 -2.9596581 -2.3422828 -1.8576858 -1.6583858 -1.8113151 -2.2777653 -2.9167154 -3.5300307 -3.9744482 -4.2242165][-4.3490744 -4.318862 -4.2417359 -4.0698562 -3.7590945 -3.3014841 -2.7713125 -2.3091097 -2.0688536 -2.1378412 -2.5066724 -3.0584917 -3.6017554 -3.9975469 -4.2190504][-4.2733755 -4.256989 -4.2145281 -4.107604 -3.898984 -3.5655141 -3.1393147 -2.7252188 -2.4613681 -2.4467492 -2.7046163 -3.1534958 -3.6202812 -3.9694273 -4.1714449][-4.1337452 -4.1359224 -4.1304588 -4.0832939 -3.9642057 -3.7438378 -3.4237115 -3.069953 -2.7951522 -2.7043476 -2.8504012 -3.1893511 -3.5759597 -3.8833885 -4.0761843][-3.9634931 -3.9889 -4.0227795 -4.029345 -3.9836431 -3.8546247 -3.6239753 -3.3257129 -3.0453517 -2.8858817 -2.9260283 -3.1589034 -3.4701815 -3.7450004 -3.9402976][-3.8324702 -3.8716319 -3.9307516 -3.9735641 -3.9799054 -3.916641 -3.75122 -3.4959071 -3.2133837 -2.9997525 -2.9468465 -3.0779674 -3.3180232 -3.5685413 -3.7767298][-3.7816322 -3.8203456 -3.8851409 -3.9429955 -3.9795902 -3.9584193 -3.8379686 -3.6142614 -3.3328071 -3.0783081 -2.9459183 -2.9830251 -3.1538565 -3.3821354 -3.6069436][-3.8147979 -3.8422179 -3.8951731 -3.9507065 -4.0005031 -4.0071645 -3.9210591 -3.7252433 -3.4508319 -3.1684942 -2.9713345 -2.9246187 -3.028955 -3.2316372 -3.4675548]]...]
INFO - root - 2017-12-05 09:32:11.099594: step 11610, loss = 1.41, batch loss = 1.35 (37.4 examples/sec; 0.214 sec/batch; 19h:04m:02s remains)
INFO - root - 2017-12-05 09:32:13.320879: step 11620, loss = 1.32, batch loss = 1.26 (35.4 examples/sec; 0.226 sec/batch; 20h:08m:21s remains)
INFO - root - 2017-12-05 09:32:15.539104: step 11630, loss = 1.65, batch loss = 1.59 (36.9 examples/sec; 0.217 sec/batch; 19h:19m:47s remains)
INFO - root - 2017-12-05 09:32:17.706879: step 11640, loss = 1.18, batch loss = 1.12 (37.1 examples/sec; 0.216 sec/batch; 19h:14m:00s remains)
INFO - root - 2017-12-05 09:32:19.893552: step 11650, loss = 1.25, batch loss = 1.19 (36.0 examples/sec; 0.222 sec/batch; 19h:48m:45s remains)
INFO - root - 2017-12-05 09:32:22.109365: step 11660, loss = 1.73, batch loss = 1.68 (36.4 examples/sec; 0.220 sec/batch; 19h:35m:01s remains)
INFO - root - 2017-12-05 09:32:24.335623: step 11670, loss = 1.57, batch loss = 1.51 (35.3 examples/sec; 0.227 sec/batch; 20h:13m:31s remains)
INFO - root - 2017-12-05 09:32:26.509059: step 11680, loss = 1.21, batch loss = 1.15 (36.6 examples/sec; 0.218 sec/batch; 19h:28m:10s remains)
INFO - root - 2017-12-05 09:32:28.688794: step 11690, loss = 1.57, batch loss = 1.52 (37.2 examples/sec; 0.215 sec/batch; 19h:10m:08s remains)
INFO - root - 2017-12-05 09:32:30.898856: step 11700, loss = 1.68, batch loss = 1.62 (37.5 examples/sec; 0.213 sec/batch; 19h:01m:04s remains)
2017-12-05 09:32:31.196595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.39836 -4.3980556 -4.397675 -4.3973756 -4.3971977 -4.3970785 -4.3969445 -4.3968129 -4.3968539 -4.3971534 -4.3976026 -4.3980436 -4.398438 -4.3987741 -4.3990154][-4.3981986 -4.3978558 -4.3973136 -4.3965335 -4.3953071 -4.39352 -4.3914924 -4.3901291 -4.3902721 -4.3918738 -4.394062 -4.3960195 -4.3973827 -4.3982415 -4.3987303][-4.3981476 -4.3977013 -4.3966508 -4.3940868 -4.3888459 -4.3806343 -4.3714824 -4.3653841 -4.3654809 -4.371582 -4.3803692 -4.3883677 -4.3938274 -4.3968329 -4.3982377][-4.3980079 -4.3970957 -4.3945374 -4.3874793 -4.3726277 -4.3498311 -4.3251171 -4.3085952 -4.3083692 -4.32396 -4.3473473 -4.3693247 -4.3846188 -4.3930097 -4.3968315][-4.3971119 -4.3949594 -4.3890495 -4.3735056 -4.3425364 -4.2972622 -4.2497196 -4.2186389 -4.2188773 -4.2497258 -4.2958455 -4.3394513 -4.370038 -4.3868523 -4.3945251][-4.3930669 -4.3876162 -4.3754873 -4.3483191 -4.2986312 -4.2296772 -4.1598983 -4.1167159 -4.1215482 -4.1717429 -4.2433968 -4.3098621 -4.3558097 -4.3807387 -4.3921][-4.3805 -4.3673477 -4.3458281 -4.3071952 -4.2437015 -4.1607966 -4.0808673 -4.0371 -4.0534592 -4.1229043 -4.2142444 -4.2949896 -4.3489442 -4.3775988 -4.3906784][-4.3506212 -4.3241644 -4.292 -4.2476358 -4.1839919 -4.1065335 -4.0375581 -4.0094228 -4.0418267 -4.1228714 -4.219491 -4.3000994 -4.35169 -4.3785167 -4.3908448][-4.2941728 -4.2516165 -4.2124453 -4.17385 -4.1287093 -4.0796251 -4.0431151 -4.0422888 -4.0881886 -4.167233 -4.252439 -4.3196821 -4.3611217 -4.3824053 -4.39221][-4.2068229 -4.1522489 -4.1160603 -4.09772 -4.0884161 -4.084538 -4.091115 -4.1180253 -4.1689587 -4.2344718 -4.2976365 -4.3447242 -4.37299 -4.3874321 -4.3940926][-4.0960817 -4.0415292 -4.02183 -4.0360403 -4.0720253 -4.1168008 -4.162416 -4.2078872 -4.2547889 -4.3008981 -4.3399734 -4.36751 -4.3836513 -4.3919635 -4.3958158][-3.9825709 -3.9444437 -3.9541378 -4.0072579 -4.0859437 -4.1681166 -4.236618 -4.2868524 -4.3232756 -4.350482 -4.3703337 -4.3834362 -4.3911018 -4.3951936 -4.3970928][-3.893384 -3.8878319 -3.9351933 -4.0238366 -4.1304121 -4.227941 -4.2989373 -4.34189 -4.3656096 -4.3789878 -4.38721 -4.3923097 -4.3953905 -4.3970828 -4.397903][-3.8521051 -3.8888867 -3.9722981 -4.0830965 -4.1953707 -4.2857971 -4.3436408 -4.3733988 -4.38639 -4.3919258 -4.3946657 -4.3963075 -4.3973684 -4.3979826 -4.3983326][-3.8720245 -3.9482632 -4.05438 -4.166739 -4.26416 -4.333035 -4.3714385 -4.3884091 -4.39444 -4.3963947 -4.3971324 -4.39762 -4.3979573 -4.398221 -4.3984542]]...]
INFO - root - 2017-12-05 09:32:33.367935: step 11710, loss = 1.65, batch loss = 1.59 (35.6 examples/sec; 0.225 sec/batch; 20h:02m:14s remains)
INFO - root - 2017-12-05 09:32:35.530228: step 11720, loss = 1.69, batch loss = 1.63 (37.2 examples/sec; 0.215 sec/batch; 19h:11m:13s remains)
INFO - root - 2017-12-05 09:32:37.714857: step 11730, loss = 1.39, batch loss = 1.33 (37.4 examples/sec; 0.214 sec/batch; 19h:03m:30s remains)
INFO - root - 2017-12-05 09:32:39.903750: step 11740, loss = 1.21, batch loss = 1.15 (36.6 examples/sec; 0.218 sec/batch; 19h:26m:58s remains)
INFO - root - 2017-12-05 09:32:42.080235: step 11750, loss = 1.25, batch loss = 1.19 (37.4 examples/sec; 0.214 sec/batch; 19h:04m:16s remains)
INFO - root - 2017-12-05 09:32:44.285049: step 11760, loss = 1.28, batch loss = 1.22 (37.1 examples/sec; 0.215 sec/batch; 19h:11m:09s remains)
INFO - root - 2017-12-05 09:32:46.467875: step 11770, loss = 1.51, batch loss = 1.45 (35.8 examples/sec; 0.223 sec/batch; 19h:53m:43s remains)
INFO - root - 2017-12-05 09:32:48.667351: step 11780, loss = 1.24, batch loss = 1.18 (36.3 examples/sec; 0.220 sec/batch; 19h:38m:05s remains)
INFO - root - 2017-12-05 09:32:50.857455: step 11790, loss = 1.54, batch loss = 1.48 (36.6 examples/sec; 0.218 sec/batch; 19h:27m:36s remains)
INFO - root - 2017-12-05 09:32:53.003808: step 11800, loss = 1.39, batch loss = 1.34 (37.6 examples/sec; 0.213 sec/batch; 18h:57m:53s remains)
2017-12-05 09:32:53.321452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2031679 -4.1911793 -4.1768732 -4.1694412 -4.1728563 -4.1871142 -4.2105341 -4.2328782 -4.243784 -4.2382979 -4.2222762 -4.2030144 -4.1868072 -4.1803451 -4.1880193][-4.2062407 -4.1891222 -4.1649303 -4.1460967 -4.1381555 -4.14319 -4.1610942 -4.1826997 -4.1965609 -4.1956725 -4.1838808 -4.1676464 -4.1532989 -4.1464958 -4.1514058][-4.2184625 -4.1964417 -4.1591978 -4.12398 -4.1007466 -4.0941362 -4.1037149 -4.1209273 -4.1346641 -4.1381035 -4.1333404 -4.1253171 -4.1193652 -4.1190653 -4.1262255][-4.2132082 -4.1866593 -4.1383038 -4.0879722 -4.0508347 -4.0331125 -4.0346451 -4.046813 -4.0597425 -4.0664558 -4.0697794 -4.0731163 -4.0801911 -4.091979 -4.1077719][-4.1848893 -4.1581097 -4.1051254 -4.0462027 -3.9984043 -3.9712641 -3.9656539 -3.9732118 -3.9851127 -3.9934437 -4.0025644 -4.0148368 -4.0340767 -4.0598469 -4.088757][-4.1459103 -4.1228924 -4.0707879 -4.008594 -3.9533772 -3.9172606 -3.9041021 -3.9070365 -3.9170923 -3.9249368 -3.9356647 -3.9523532 -3.9790084 -4.0163646 -4.0591][-4.1121864 -4.0938282 -4.046103 -3.9838896 -3.9244137 -3.8804967 -3.8596325 -3.8589535 -3.8671925 -3.8728516 -3.8803363 -3.894444 -3.9217288 -3.9655745 -4.0193839][-4.0991912 -4.0827827 -4.0400205 -3.978864 -3.916786 -3.8665287 -3.8391371 -3.8361082 -3.8442059 -3.8484306 -3.8511569 -3.8592477 -3.8823404 -3.925082 -3.9830949][-4.1109171 -4.0948811 -4.0553312 -3.9948332 -3.9310145 -3.8764212 -3.8440294 -3.8382587 -3.8452079 -3.8491642 -3.8501663 -3.8547187 -3.87318 -3.911217 -3.967169][-4.1458497 -4.1274738 -4.0879259 -4.0268016 -3.9620004 -3.9047127 -3.8681259 -3.8573654 -3.8610034 -3.8646767 -3.8655314 -3.8695564 -3.8854258 -3.9186971 -3.9692583][-4.1997237 -4.1762 -4.1327896 -4.0694885 -4.0026493 -3.9424615 -3.9011238 -3.883667 -3.8826704 -3.8855004 -3.8868661 -3.8912895 -3.9054289 -3.934119 -3.97796][-4.2596822 -4.2325678 -4.1854453 -4.1207237 -4.0520911 -3.9883945 -3.9418287 -3.9179523 -3.9122903 -3.9138427 -3.915164 -3.9191492 -3.9313054 -3.9543252 -3.9896626][-4.3154907 -4.2874842 -4.2409286 -4.1785793 -4.1120329 -4.0483885 -3.9990025 -3.9699376 -3.9589472 -3.9575937 -3.9570954 -3.9575546 -3.9641798 -3.9793942 -4.0055227][-4.35578 -4.3311934 -4.2903123 -4.236021 -4.1778264 -4.1213021 -4.0750232 -4.0448136 -4.0301418 -4.0248218 -4.0207915 -4.0158715 -4.0147705 -4.0207996 -4.0364079][-4.3778234 -4.358583 -4.32659 -4.28427 -4.2392492 -4.19499 -4.157588 -4.1315813 -4.1176748 -4.1115804 -4.1067419 -4.0995512 -4.0940208 -4.0932808 -4.0999875]]...]
INFO - root - 2017-12-05 09:32:55.505597: step 11810, loss = 1.35, batch loss = 1.29 (35.5 examples/sec; 0.225 sec/batch; 20h:03m:10s remains)
INFO - root - 2017-12-05 09:32:57.670021: step 11820, loss = 1.14, batch loss = 1.08 (37.0 examples/sec; 0.216 sec/batch; 19h:14m:11s remains)
INFO - root - 2017-12-05 09:32:59.876524: step 11830, loss = 1.82, batch loss = 1.76 (36.2 examples/sec; 0.221 sec/batch; 19h:39m:33s remains)
INFO - root - 2017-12-05 09:33:02.087364: step 11840, loss = 1.59, batch loss = 1.53 (34.7 examples/sec; 0.230 sec/batch; 20h:30m:59s remains)
INFO - root - 2017-12-05 09:33:04.274199: step 11850, loss = 1.04, batch loss = 0.98 (36.8 examples/sec; 0.217 sec/batch; 19h:22m:04s remains)
INFO - root - 2017-12-05 09:33:06.433253: step 11860, loss = 1.46, batch loss = 1.40 (36.7 examples/sec; 0.218 sec/batch; 19h:25m:34s remains)
INFO - root - 2017-12-05 09:33:08.611502: step 11870, loss = 1.21, batch loss = 1.15 (34.2 examples/sec; 0.234 sec/batch; 20h:49m:23s remains)
INFO - root - 2017-12-05 09:33:10.823314: step 11880, loss = 1.27, batch loss = 1.21 (36.5 examples/sec; 0.219 sec/batch; 19h:30m:43s remains)
INFO - root - 2017-12-05 09:33:12.995187: step 11890, loss = 1.62, batch loss = 1.56 (37.8 examples/sec; 0.211 sec/batch; 18h:49m:41s remains)
INFO - root - 2017-12-05 09:33:15.172496: step 11900, loss = 1.58, batch loss = 1.53 (36.6 examples/sec; 0.219 sec/batch; 19h:27m:35s remains)
2017-12-05 09:33:15.472690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4019513 -4.4020133 -4.4019785 -4.4017911 -4.401504 -4.4009852 -4.3999214 -4.3988357 -4.399044 -4.3997769 -4.3997464 -4.3996615 -4.4000168 -4.40029 -4.4002323][-4.4019089 -4.4021897 -4.4022918 -4.402143 -4.4017811 -4.4011068 -4.3998995 -4.3986588 -4.3987622 -4.3994718 -4.3994493 -4.3993778 -4.3998075 -4.4001627 -4.4001708][-4.401422 -4.4021134 -4.4025717 -4.4026537 -4.4023333 -4.4015765 -4.4002075 -4.3987679 -4.3987041 -4.39935 -4.3993483 -4.3992805 -4.3997459 -4.40015 -4.4001951][-4.4005809 -4.4015851 -4.402451 -4.4028316 -4.4026504 -4.4019532 -4.4004807 -4.3988185 -4.3985872 -4.3992119 -4.3992567 -4.3992081 -4.3997107 -4.4001589 -4.4002361][-4.39964 -4.4008327 -4.4019651 -4.4024277 -4.40231 -4.4017296 -4.4003587 -4.3986778 -4.3984022 -4.3990545 -4.3991733 -4.3991666 -4.3996768 -4.4001484 -4.4002628][-4.3985929 -4.3997331 -4.40089 -4.401372 -4.4013948 -4.4010091 -4.3997731 -4.3982339 -4.3980832 -4.3988638 -4.3991408 -4.3992009 -4.3997126 -4.4001966 -4.4003191][-4.3982468 -4.3988743 -4.3997278 -4.400125 -4.4001689 -4.3999286 -4.3989453 -4.3977394 -4.3978109 -4.3988113 -4.3992729 -4.3993564 -4.3998394 -4.4002948 -4.4003797][-4.398983 -4.3989444 -4.399312 -4.399497 -4.3994493 -4.3992205 -4.3985128 -4.397717 -4.3980961 -4.3992271 -4.399704 -4.3997407 -4.4001274 -4.4004521 -4.4004431][-4.4002476 -4.3996716 -4.3995466 -4.3994207 -4.3992696 -4.3990412 -4.3985572 -4.3981748 -4.3988914 -4.4000831 -4.4004226 -4.4003124 -4.4005117 -4.4006519 -4.4005079][-4.40095 -4.400001 -4.3995118 -4.3991475 -4.3989434 -4.3988156 -4.3986182 -4.3986812 -4.3996325 -4.4008074 -4.4009933 -4.4006844 -4.4006734 -4.4006939 -4.4004974][-4.4011116 -4.4000487 -4.3993907 -4.3989358 -4.3987107 -4.3985929 -4.3985767 -4.3987827 -4.3997526 -4.4008169 -4.4009452 -4.400557 -4.4004717 -4.4005485 -4.4004192][-4.4017382 -4.400826 -4.4001212 -4.3997073 -4.3994756 -4.3992519 -4.3991323 -4.3991833 -4.3999634 -4.4008741 -4.4009318 -4.4004335 -4.4002409 -4.4003339 -4.4002652][-4.4023213 -4.4016628 -4.4011903 -4.4009514 -4.4007335 -4.4004841 -4.4001651 -4.3999219 -4.4003935 -4.4010706 -4.40103 -4.4004269 -4.4000597 -4.400084 -4.4000516][-4.4012575 -4.4009376 -4.4007955 -4.4007668 -4.4006677 -4.4004903 -4.4001451 -4.3997045 -4.399941 -4.4004688 -4.4004903 -4.3999629 -4.3996286 -4.3996916 -4.3998213][-4.400465 -4.4002404 -4.400147 -4.4001665 -4.400034 -4.3999476 -4.39962 -4.3989921 -4.3989048 -4.3993216 -4.3995504 -4.3992476 -4.3990922 -4.3993263 -4.399641]]...]
INFO - root - 2017-12-05 09:33:17.652728: step 11910, loss = 1.26, batch loss = 1.20 (36.0 examples/sec; 0.222 sec/batch; 19h:48m:12s remains)
INFO - root - 2017-12-05 09:33:19.844206: step 11920, loss = 1.38, batch loss = 1.32 (36.6 examples/sec; 0.219 sec/batch; 19h:27m:57s remains)
INFO - root - 2017-12-05 09:33:22.035865: step 11930, loss = 1.27, batch loss = 1.21 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:59s remains)
INFO - root - 2017-12-05 09:33:24.240704: step 11940, loss = 1.49, batch loss = 1.43 (35.2 examples/sec; 0.227 sec/batch; 20h:14m:52s remains)
INFO - root - 2017-12-05 09:33:26.389851: step 11950, loss = 1.45, batch loss = 1.39 (37.2 examples/sec; 0.215 sec/batch; 19h:07m:39s remains)
INFO - root - 2017-12-05 09:33:28.575218: step 11960, loss = 1.32, batch loss = 1.27 (35.5 examples/sec; 0.225 sec/batch; 20h:03m:02s remains)
INFO - root - 2017-12-05 09:33:30.750443: step 11970, loss = 1.64, batch loss = 1.58 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:42s remains)
INFO - root - 2017-12-05 09:33:32.942670: step 11980, loss = 1.81, batch loss = 1.75 (35.9 examples/sec; 0.223 sec/batch; 19h:50m:48s remains)
INFO - root - 2017-12-05 09:33:35.153876: step 11990, loss = 1.94, batch loss = 1.88 (36.6 examples/sec; 0.219 sec/batch; 19h:28m:31s remains)
INFO - root - 2017-12-05 09:33:37.330001: step 12000, loss = 1.18, batch loss = 1.12 (36.3 examples/sec; 0.221 sec/batch; 19h:38m:13s remains)
2017-12-05 09:33:37.629460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1750417 -2.2042625 -2.2702644 -2.3915262 -2.562202 -2.7432618 -2.8690097 -2.8542609 -2.6390572 -2.2298174 -1.6886241 -1.1255052 -0.63070631 -0.25275517 -0.016906261][-1.0312636 -1.1006353 -1.2396758 -1.4612553 -1.7561483 -2.0621724 -2.2923176 -2.3504341 -2.1773036 -1.78069 -1.2309222 -0.64426565 -0.11771154 0.29044676 0.55463219][0.28642464 0.17736673 -0.050347328 -0.3840642 -0.81394553 -1.2572064 -1.6067958 -1.765301 -1.6761923 -1.3492172 -0.84965277 -0.29181051 0.22085619 0.62504864 0.88618183][1.629138 1.4877377 1.1713347 0.72589636 0.17103672 -0.39055061 -0.84829879 -1.1083047 -1.1189251 -0.88768053 -0.4778893 0.00042438507 0.44943237 0.79771805 1.0039587][2.8076434 2.6488476 2.2612152 1.7246332 1.0763307 0.43164396 -0.1029563 -0.44685388 -0.5489769 -0.41919422 -0.12160826 0.2375741 0.56835079 0.80047846 0.89527416][3.6648512 3.5179615 3.0950584 2.510819 1.8115582 1.1329236 0.56319427 0.16931009 -0.0032205582 0.0330019 0.21352196 0.42930555 0.60419655 0.67454338 0.61778736][4.158947 4.0594044 3.6494236 3.0696445 2.3727174 1.7063584 1.1409621 0.73112106 0.51448679 0.47534847 0.54325151 0.61105585 0.61510229 0.50742531 0.28348064][4.3275261 4.3095493 3.9536238 3.4163074 2.772233 2.1642189 1.6476898 1.2584109 1.027658 0.93342543 0.90351677 0.833344 0.66905785 0.38480711 -0.0070109367][4.2365589 4.3005991 4.026947 3.5632572 3.0079551 2.4915571 2.0560651 1.721642 1.5081458 1.3856215 1.2777667 1.0877056 0.77113485 0.32227516 -0.22675037][3.9616604 4.0815659 3.8872738 3.5086727 3.0572844 2.6416707 2.2994943 2.0373049 1.8646641 1.7355418 1.5724058 1.2860994 0.83753538 0.24626923 -0.43480754][3.5426941 3.7002754 3.5600672 3.2702203 2.9218822 2.6043854 2.3534427 2.1646428 2.0363407 1.9099822 1.7058268 1.3400946 0.78265285 0.082502842 -0.68738031][3.0109205 3.1801825 3.0745296 2.8701148 2.6160598 2.3808923 2.2016506 2.0716724 1.9811006 1.858695 1.6190963 1.1914859 0.557271 -0.20428991 -1.0111685][2.3486128 2.5134263 2.4272346 2.286252 2.100203 1.9210954 1.7890673 1.7033911 1.6464858 1.5367446 1.2832885 0.82722139 0.16588449 -0.60384059 -1.3966241][1.5508103 1.6986599 1.62182 1.5176454 1.3720965 1.2305994 1.1328869 1.0788822 1.0483971 0.95748091 0.71064138 0.26532459 -0.3702302 -1.0921774 -1.8257058][0.6069665 0.74312735 0.67716932 0.58988619 0.47010565 0.35538673 0.28253794 0.24752998 0.23010349 0.15193224 -0.074603081 -0.47197652 -1.0288537 -1.6476843 -2.2814384]]...]
INFO - root - 2017-12-05 09:33:39.803630: step 12010, loss = 1.29, batch loss = 1.23 (37.3 examples/sec; 0.214 sec/batch; 19h:05m:21s remains)
INFO - root - 2017-12-05 09:33:41.987491: step 12020, loss = 1.72, batch loss = 1.66 (36.0 examples/sec; 0.222 sec/batch; 19h:45m:26s remains)
INFO - root - 2017-12-05 09:33:44.185843: step 12030, loss = 1.57, batch loss = 1.51 (35.0 examples/sec; 0.229 sec/batch; 20h:21m:09s remains)
INFO - root - 2017-12-05 09:33:46.394283: step 12040, loss = 1.53, batch loss = 1.47 (36.2 examples/sec; 0.221 sec/batch; 19h:38m:46s remains)
INFO - root - 2017-12-05 09:33:48.560102: step 12050, loss = 1.86, batch loss = 1.80 (36.9 examples/sec; 0.217 sec/batch; 19h:17m:18s remains)
INFO - root - 2017-12-05 09:33:50.741548: step 12060, loss = 1.49, batch loss = 1.43 (37.2 examples/sec; 0.215 sec/batch; 19h:09m:56s remains)
INFO - root - 2017-12-05 09:33:52.951563: step 12070, loss = 1.61, batch loss = 1.55 (36.0 examples/sec; 0.222 sec/batch; 19h:46m:55s remains)
INFO - root - 2017-12-05 09:33:55.135450: step 12080, loss = 1.60, batch loss = 1.54 (36.9 examples/sec; 0.217 sec/batch; 19h:16m:25s remains)
INFO - root - 2017-12-05 09:33:57.329474: step 12090, loss = 1.28, batch loss = 1.23 (36.6 examples/sec; 0.219 sec/batch; 19h:27m:25s remains)
INFO - root - 2017-12-05 09:33:59.521028: step 12100, loss = 1.16, batch loss = 1.10 (35.0 examples/sec; 0.229 sec/batch; 20h:21m:11s remains)
2017-12-05 09:33:59.808349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3638458 -4.3626881 -4.3600049 -4.3581247 -4.3604369 -4.3669252 -4.3764873 -4.3861265 -4.3925834 -4.3956509 -4.3970714 -4.3974161 -4.3974395 -4.3977923 -4.3929291][-4.2581434 -4.2605805 -4.2602067 -4.2606225 -4.2688928 -4.2867088 -4.311902 -4.3378844 -4.3574104 -4.367919 -4.3735957 -4.3772674 -4.3806071 -4.3845329 -4.3807955][-4.01734 -4.0233793 -4.0262189 -4.03036 -4.0488935 -4.0897169 -4.1487684 -4.2105575 -4.2612329 -4.2929778 -4.3130116 -4.32891 -4.3438873 -4.3584633 -4.3609838][-3.6163645 -3.6157672 -3.613781 -3.617568 -3.6462295 -3.7183542 -3.8299162 -3.95163 -4.0584269 -4.1350174 -4.1900005 -4.237103 -4.2799592 -4.3170986 -4.3339229][-3.1043887 -3.0711489 -3.047431 -3.0424082 -3.075007 -3.1758137 -3.3472977 -3.5450232 -3.7276559 -3.873878 -3.9914942 -4.095727 -4.1885071 -4.2638879 -4.3051004][-2.6218691 -2.5168433 -2.44246 -2.4111443 -2.4331732 -2.543575 -2.7586474 -3.02867 -3.294188 -3.5274711 -3.7328634 -3.9184554 -4.0796504 -4.2056203 -4.279685][-2.347651 -2.1410186 -1.988987 -1.9091682 -1.9006705 -1.9918287 -2.2100368 -2.5190573 -2.8500278 -3.16703 -3.4661975 -3.7403686 -3.9743564 -4.1529703 -4.2620134][-2.3798831 -2.0808492 -1.853919 -1.7201037 -1.6716557 -1.7235837 -1.9046874 -2.1997871 -2.5528934 -2.9197712 -3.2847905 -3.622577 -3.908592 -4.1235209 -4.2572975][-2.6789243 -2.3319886 -2.0661926 -1.8989143 -1.8194327 -1.8329146 -1.9612052 -2.2052708 -2.5306735 -2.893496 -3.2684221 -3.6176274 -3.9119103 -4.1311555 -4.2691965][-3.1197784 -2.7872553 -2.5282245 -2.3581088 -2.2658365 -2.2546837 -2.3397038 -2.5247798 -2.7898729 -3.0987215 -3.4248533 -3.7296991 -3.9853704 -4.1751571 -4.2949023][-3.5741053 -3.3112109 -3.0981212 -2.9521022 -2.866251 -2.8445346 -2.9001236 -3.0324879 -3.2256703 -3.4523203 -3.6914604 -3.9147296 -4.1008973 -4.2388458 -4.326478][-3.9412415 -3.7722025 -3.6281431 -3.5244913 -3.4602666 -3.4387321 -3.4719548 -3.5572529 -3.680804 -3.8241682 -3.973238 -4.1103415 -4.22277 -4.3051958 -4.357934][-4.18509 -4.0976911 -4.0193653 -3.9596443 -3.9212964 -3.9062181 -3.9228313 -3.9696529 -4.0371161 -4.1142249 -4.1919494 -4.2613292 -4.3166809 -4.3565493 -4.3820906][-4.316545 -4.2812638 -4.2475343 -4.2199354 -4.201478 -4.193553 -4.2009892 -4.2224388 -4.2533212 -4.2877531 -4.3209538 -4.34924 -4.3711181 -4.3865609 -4.39617][-4.3708749 -4.360857 -4.3509207 -4.3415017 -4.334836 -4.3315616 -4.33417 -4.3420072 -4.3535452 -4.3660235 -4.3773127 -4.3862276 -4.3928452 -4.3975954 -4.4005685]]...]
INFO - root - 2017-12-05 09:34:01.957840: step 12110, loss = 1.36, batch loss = 1.30 (36.2 examples/sec; 0.221 sec/batch; 19h:39m:16s remains)
INFO - root - 2017-12-05 09:34:04.161370: step 12120, loss = 1.47, batch loss = 1.41 (36.6 examples/sec; 0.219 sec/batch; 19h:28m:39s remains)
INFO - root - 2017-12-05 09:34:06.356063: step 12130, loss = 1.33, batch loss = 1.27 (36.8 examples/sec; 0.217 sec/batch; 19h:19m:19s remains)
INFO - root - 2017-12-05 09:34:08.588646: step 12140, loss = 1.94, batch loss = 1.88 (34.8 examples/sec; 0.230 sec/batch; 20h:26m:29s remains)
INFO - root - 2017-12-05 09:34:10.789912: step 12150, loss = 1.50, batch loss = 1.44 (37.0 examples/sec; 0.216 sec/batch; 19h:14m:52s remains)
INFO - root - 2017-12-05 09:34:12.991921: step 12160, loss = 1.39, batch loss = 1.33 (37.0 examples/sec; 0.216 sec/batch; 19h:15m:08s remains)
INFO - root - 2017-12-05 09:34:15.169829: step 12170, loss = 1.11, batch loss = 1.06 (38.2 examples/sec; 0.209 sec/batch; 18h:37m:36s remains)
INFO - root - 2017-12-05 09:34:17.354737: step 12180, loss = 1.58, batch loss = 1.52 (35.8 examples/sec; 0.224 sec/batch; 19h:54m:27s remains)
INFO - root - 2017-12-05 09:34:19.573889: step 12190, loss = 1.52, batch loss = 1.46 (35.1 examples/sec; 0.228 sec/batch; 20h:15m:41s remains)
INFO - root - 2017-12-05 09:34:21.783391: step 12200, loss = 1.66, batch loss = 1.60 (34.1 examples/sec; 0.235 sec/batch; 20h:53m:52s remains)
2017-12-05 09:34:22.068848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3076653 -4.2728925 -4.2541914 -4.243464 -4.2345467 -4.2095909 -4.1525307 -4.038425 -3.8614695 -3.6303668 -3.361166 -3.072974 -2.8058846 -2.594553 -2.4682703][-4.2944608 -4.2502832 -4.2208729 -4.1980114 -4.169199 -4.1072388 -3.9915953 -3.7910752 -3.5075483 -3.1675239 -2.8082244 -2.4592409 -2.1533632 -1.9121177 -1.7576647][-4.2691612 -4.2040696 -4.1469431 -4.0900955 -4.0159297 -3.8907931 -3.6925449 -3.3883502 -2.9903715 -2.5371618 -2.0797498 -1.6546376 -1.2979469 -1.0290825 -0.868752][-4.2243915 -4.1182628 -4.0033622 -3.873646 -3.7118587 -3.4851525 -3.1795669 -2.7716217 -2.2823222 -1.7514522 -1.2272933 -0.74429393 -0.34430742 -0.056079865 0.088866711][-4.1604104 -3.9881306 -3.7777569 -3.5256419 -3.2251444 -2.8584747 -2.4342439 -1.947979 -1.4238486 -0.88929462 -0.36723804 0.12305021 0.53622532 0.82709885 0.94482851][-4.0845523 -3.8270264 -3.4925425 -3.0827599 -2.6111834 -2.0891707 -1.5598519 -1.0412314 -0.55338144 -0.09900713 0.3430171 0.77890539 1.1638346 1.4372478 1.5270929][-4.0155506 -3.6752062 -3.2196593 -2.6602378 -2.0345776 -1.3898389 -0.80048966 -0.30211163 0.090967178 0.40262747 0.70595455 1.0401268 1.3679495 1.6158767 1.6931148][-3.9691315 -3.5752676 -3.0460858 -2.4037352 -1.7058716 -1.0280964 -0.4584465 -0.037419796 0.23030329 0.38687754 0.54679918 0.7791419 1.0559487 1.2960839 1.3951416][-3.9417789 -3.5343232 -3.0002604 -2.3712194 -1.7118297 -1.1093543 -0.64783382 -0.358068 -0.22315121 -0.18838024 -0.12706566 0.040786266 0.29496813 0.55278969 0.70619965][-3.9082832 -3.5074465 -3.0110898 -2.460995 -1.9187691 -1.4650161 -1.1656935 -1.034826 -1.0309172 -1.0911787 -1.0960283 -0.97322893 -0.7359252 -0.46095872 -0.25330257][-3.8457825 -3.443011 -2.9833558 -2.5211596 -2.1121905 -1.8187921 -1.686801 -1.712126 -1.8308287 -1.9779956 -2.0526044 -1.99473 -1.8085752 -1.5583217 -1.3333087][-3.7625785 -3.3383768 -2.8879819 -2.4796867 -2.1640172 -1.9839859 -1.9724514 -2.114337 -2.3359063 -2.572772 -2.7397981 -2.7882168 -2.7044868 -2.532547 -2.3375826][-3.7044623 -3.2627168 -2.8064339 -2.40834 -2.115546 -1.9610612 -1.9789553 -2.1590421 -2.4389977 -2.7575929 -3.0318971 -3.2148781 -3.2689054 -3.2095942 -3.0786347][-3.7258892 -3.302587 -2.8565049 -2.4464269 -2.1137877 -1.8933356 -1.8362296 -1.9566381 -2.2238116 -2.5828123 -2.946681 -3.2508721 -3.4356947 -3.4869289 -3.4279313][-3.8422303 -3.4864888 -3.0859923 -2.6709602 -2.2720063 -1.9332714 -1.7298725 -1.7103477 -1.8853481 -2.2162046 -2.6093583 -2.9749537 -3.2366927 -3.3602448 -3.3616848]]...]
INFO - root - 2017-12-05 09:34:24.259744: step 12210, loss = 1.54, batch loss = 1.49 (36.3 examples/sec; 0.220 sec/batch; 19h:35m:00s remains)
INFO - root - 2017-12-05 09:34:26.435196: step 12220, loss = 2.02, batch loss = 1.97 (36.0 examples/sec; 0.222 sec/batch; 19h:44m:36s remains)
INFO - root - 2017-12-05 09:34:28.617248: step 12230, loss = 1.19, batch loss = 1.13 (36.6 examples/sec; 0.219 sec/batch; 19h:27m:04s remains)
INFO - root - 2017-12-05 09:34:30.792717: step 12240, loss = 1.42, batch loss = 1.36 (36.6 examples/sec; 0.218 sec/batch; 19h:25m:34s remains)
INFO - root - 2017-12-05 09:34:32.968247: step 12250, loss = 1.79, batch loss = 1.73 (36.0 examples/sec; 0.222 sec/batch; 19h:47m:14s remains)
INFO - root - 2017-12-05 09:34:35.117141: step 12260, loss = 1.31, batch loss = 1.25 (38.0 examples/sec; 0.211 sec/batch; 18h:44m:57s remains)
INFO - root - 2017-12-05 09:34:37.317535: step 12270, loss = 1.72, batch loss = 1.66 (35.6 examples/sec; 0.225 sec/batch; 19h:58m:12s remains)
INFO - root - 2017-12-05 09:34:39.556244: step 12280, loss = 1.61, batch loss = 1.55 (35.9 examples/sec; 0.223 sec/batch; 19h:48m:44s remains)
INFO - root - 2017-12-05 09:34:41.722086: step 12290, loss = 1.77, batch loss = 1.71 (36.3 examples/sec; 0.221 sec/batch; 19h:37m:25s remains)
INFO - root - 2017-12-05 09:34:43.915886: step 12300, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:11m:33s remains)
2017-12-05 09:34:44.197344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.168622 -4.2330875 -4.2753854 -4.2912183 -4.2871413 -4.282536 -4.2916861 -4.3147645 -4.3434286 -4.3689632 -4.3870993 -4.3971829 -4.4021397 -4.4043126 -4.4050736][-4.0947094 -4.1325784 -4.1332049 -4.0953517 -4.0386724 -4.0063105 -4.0273271 -4.0969658 -4.1879969 -4.2727323 -4.3345714 -4.3715258 -4.390923 -4.3998122 -4.4032693][-3.974611 -3.9461966 -3.8510334 -3.6976929 -3.5342479 -3.4435539 -3.47994 -3.6322427 -3.8430495 -4.0496311 -4.20813 -4.3079491 -4.3627186 -4.3886685 -4.3993955][-3.7956929 -3.6568174 -3.406466 -3.0709748 -2.7405679 -2.5491321 -2.5869102 -2.8454118 -3.2321286 -3.6328201 -3.9579465 -4.1757407 -4.3016372 -4.364573 -4.3910604][-3.5822 -3.3053467 -2.8606625 -2.295408 -1.7449288 -1.3958101 -1.3856726 -1.7313983 -2.3191056 -2.9748631 -3.540859 -3.9424078 -4.1864948 -4.3152108 -4.3717666][-3.3813086 -2.9746072 -2.3397715 -1.5347183 -0.72797561 -0.15556765 -0.01429224 -0.378345 -1.1436801 -2.0807996 -2.9472628 -3.5972004 -4.0099506 -4.2347879 -4.3361974][-3.2442622 -2.75098 -1.9726267 -0.95953846 0.10992718 0.96186495 1.3256264 1.0354047 0.15438938 -1.0426211 -2.2271845 -3.163753 -3.7841187 -4.1306853 -4.2867355][-3.191865 -2.679739 -1.8378563 -0.69157028 0.59479094 1.7314358 2.3694363 2.229713 1.3167791 -0.066880226 -1.5184336 -2.719923 -3.5464416 -4.020093 -4.2329879][-3.2124274 -2.7518594 -1.9402246 -0.772393 0.61921263 1.9561248 2.8366776 2.883575 2.0319791 0.58487558 -1.0110371 -2.3835979 -3.3595061 -3.9317589 -4.1900325][-3.2762868 -2.9206581 -2.2272027 -1.1605904 0.18208218 1.5518303 2.5464735 2.76019 2.0630355 0.70789862 -0.86333728 -2.2599497 -3.2815845 -3.8932381 -4.1715198][-3.3271408 -3.0991395 -2.5874405 -1.7302284 -0.584491 0.64163256 1.5918937 1.8955188 1.3973517 0.2676959 -1.1161807 -2.3875151 -3.339365 -3.9184082 -4.1836786][-3.2887266 -3.1730819 -2.862946 -2.2819684 -1.4405982 -0.48950887 0.28407383 0.57844687 0.24928188 -0.60147285 -1.6941392 -2.7274804 -3.514442 -3.9974232 -4.2198744][-3.0795367 -3.0175757 -2.8830209 -2.598928 -2.1283388 -1.5441954 -1.0411623 -0.84318995 -1.0709088 -1.6646895 -2.4392681 -3.1843238 -3.7561445 -4.105722 -4.2664351][-2.6619129 -2.561286 -2.5338936 -2.5202816 -2.4383426 -2.2740667 -2.1110897 -2.0809717 -2.274214 -2.6694589 -3.1642408 -3.6366186 -3.9959335 -4.21114 -4.3087063][-2.0686958 -1.8333695 -1.8167446 -1.9978964 -2.2614508 -2.5088394 -2.7139421 -2.9084854 -3.1419845 -3.4229889 -3.7196295 -3.9837303 -4.1759281 -4.2863464 -4.3342423]]...]
INFO - root - 2017-12-05 09:34:46.413278: step 12310, loss = 1.33, batch loss = 1.27 (36.0 examples/sec; 0.223 sec/batch; 19h:47m:23s remains)
INFO - root - 2017-12-05 09:34:48.611214: step 12320, loss = 1.18, batch loss = 1.12 (35.7 examples/sec; 0.224 sec/batch; 19h:55m:55s remains)
INFO - root - 2017-12-05 09:34:50.779802: step 12330, loss = 1.59, batch loss = 1.53 (35.3 examples/sec; 0.226 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-05 09:34:52.990929: step 12340, loss = 1.65, batch loss = 1.59 (36.6 examples/sec; 0.219 sec/batch; 19h:26m:18s remains)
INFO - root - 2017-12-05 09:34:55.166050: step 12350, loss = 1.48, batch loss = 1.42 (37.0 examples/sec; 0.216 sec/batch; 19h:12m:55s remains)
INFO - root - 2017-12-05 09:34:57.358175: step 12360, loss = 1.54, batch loss = 1.48 (36.4 examples/sec; 0.220 sec/batch; 19h:31m:28s remains)
INFO - root - 2017-12-05 09:34:59.557106: step 12370, loss = 1.71, batch loss = 1.65 (36.5 examples/sec; 0.219 sec/batch; 19h:28m:07s remains)
INFO - root - 2017-12-05 09:35:01.756499: step 12380, loss = 1.49, batch loss = 1.43 (37.3 examples/sec; 0.214 sec/batch; 19h:02m:54s remains)
INFO - root - 2017-12-05 09:35:03.953709: step 12390, loss = 1.55, batch loss = 1.49 (37.1 examples/sec; 0.216 sec/batch; 19h:11m:54s remains)
INFO - root - 2017-12-05 09:35:06.133087: step 12400, loss = 1.03, batch loss = 0.97 (37.0 examples/sec; 0.216 sec/batch; 19h:14m:38s remains)
2017-12-05 09:35:06.406924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.40377 -4.4036674 -4.4036608 -4.4036651 -4.4036508 -4.403584 -4.4034529 -4.4032693 -4.4031472 -4.4032507 -4.4035692 -4.403935 -4.4042091 -4.4043655 -4.4043765][-4.4020114 -4.4018717 -4.4018283 -4.4016881 -4.4012876 -4.4005237 -4.399425 -4.3983421 -4.3979726 -4.3987546 -4.400423 -4.4021988 -4.403419 -4.4038572 -4.4036021][-4.3946791 -4.3945708 -4.3946438 -4.3941932 -4.392581 -4.3892989 -4.3844624 -4.3797407 -4.3779387 -4.3808622 -4.3873262 -4.3942361 -4.3992171 -4.4015927 -4.40163][-4.3788366 -4.3786831 -4.3787179 -4.3769627 -4.3716412 -4.3616557 -4.3479266 -4.3353906 -4.3310041 -4.3385272 -4.3550649 -4.3733311 -4.3873963 -4.3953643 -4.3978605][-4.35919 -4.3584929 -4.3573961 -4.35242 -4.3399811 -4.3187227 -4.2913637 -4.267612 -4.2593412 -4.2719049 -4.300734 -4.3346839 -4.3635192 -4.3823886 -4.3912396][-4.3466868 -4.3446517 -4.3411036 -4.3317318 -4.3111749 -4.277894 -4.2362676 -4.200141 -4.1856675 -4.1987123 -4.234777 -4.2825804 -4.3281507 -4.3618016 -4.3808322][-4.3446779 -4.3411393 -4.334888 -4.3220029 -4.2963557 -4.2560177 -4.2058783 -4.161068 -4.1389313 -4.1453967 -4.1788034 -4.2321348 -4.290411 -4.33838 -4.3687282][-4.3496661 -4.3452396 -4.337388 -4.3232875 -4.2981734 -4.2597919 -4.2112474 -4.1647863 -4.1362324 -4.1319203 -4.1545143 -4.2024307 -4.2641277 -4.3204989 -4.3592625][-4.3599415 -4.3558917 -4.3480754 -4.334837 -4.3142118 -4.2844963 -4.2463565 -4.2066731 -4.1761713 -4.162147 -4.1717186 -4.2078962 -4.2632942 -4.3182898 -4.3583632][-4.37454 -4.3714719 -4.365099 -4.3544016 -4.3393483 -4.31932 -4.2936859 -4.2646537 -4.23813 -4.220437 -4.2213721 -4.2463098 -4.2898927 -4.3343964 -4.36745][-4.3894782 -4.3875914 -4.38333 -4.3760815 -4.3661666 -4.353723 -4.33813 -4.3195477 -4.3006835 -4.2863946 -4.2852492 -4.3017263 -4.3307295 -4.35996 -4.3811836][-4.3989921 -4.3981113 -4.3958883 -4.39194 -4.3864546 -4.3797069 -4.3713522 -4.3610206 -4.3504767 -4.3425622 -4.34202 -4.3513408 -4.3668613 -4.3818593 -4.3925257][-4.4032888 -4.4029274 -4.4020348 -4.400383 -4.3979263 -4.3947558 -4.3908219 -4.3861232 -4.3816948 -4.378675 -4.3787346 -4.3829126 -4.3892932 -4.3952441 -4.3994722][-4.4042916 -4.4041524 -4.4039092 -4.4034348 -4.4026518 -4.4015532 -4.4000897 -4.3984466 -4.3971143 -4.3963871 -4.3966084 -4.39799 -4.3999739 -4.4017282 -4.4028778][-4.4042072 -4.4041 -4.40403 -4.4039421 -4.4038072 -4.4035969 -4.4032006 -4.4027491 -4.4023709 -4.4021254 -4.4020844 -4.40233 -4.4026871 -4.4029627 -4.4031744]]...]
INFO - root - 2017-12-05 09:35:08.607264: step 12410, loss = 1.32, batch loss = 1.26 (35.6 examples/sec; 0.224 sec/batch; 19h:57m:28s remains)
INFO - root - 2017-12-05 09:35:10.794685: step 12420, loss = 1.35, batch loss = 1.29 (37.4 examples/sec; 0.214 sec/batch; 19h:00m:55s remains)
INFO - root - 2017-12-05 09:35:12.960900: step 12430, loss = 1.36, batch loss = 1.31 (37.9 examples/sec; 0.211 sec/batch; 18h:46m:53s remains)
INFO - root - 2017-12-05 09:35:15.162877: step 12440, loss = 1.67, batch loss = 1.62 (37.0 examples/sec; 0.216 sec/batch; 19h:12m:22s remains)
INFO - root - 2017-12-05 09:35:17.355987: step 12450, loss = 1.61, batch loss = 1.55 (35.2 examples/sec; 0.227 sec/batch; 20h:12m:42s remains)
INFO - root - 2017-12-05 09:35:19.550676: step 12460, loss = 1.46, batch loss = 1.40 (35.1 examples/sec; 0.228 sec/batch; 20h:15m:04s remains)
INFO - root - 2017-12-05 09:35:21.754186: step 12470, loss = 1.22, batch loss = 1.16 (34.8 examples/sec; 0.230 sec/batch; 20h:27m:50s remains)
INFO - root - 2017-12-05 09:35:23.976868: step 12480, loss = 1.50, batch loss = 1.45 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:53s remains)
INFO - root - 2017-12-05 09:35:26.173067: step 12490, loss = 1.34, batch loss = 1.28 (37.6 examples/sec; 0.213 sec/batch; 18h:53m:50s remains)
INFO - root - 2017-12-05 09:35:28.339337: step 12500, loss = 1.37, batch loss = 1.31 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:21s remains)
2017-12-05 09:35:28.640770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4008141 -4.4010568 -4.401772 -4.4026289 -4.4032097 -4.4031582 -4.4022641 -4.4009962 -4.3995056 -4.3984232 -4.3981056 -4.3980503 -4.3979764 -4.3984404 -4.3993311][-4.3988609 -4.3991694 -4.4000807 -4.401289 -4.4019537 -4.4016013 -4.4003673 -4.398982 -4.397831 -4.3971529 -4.3974242 -4.3980174 -4.3985472 -4.3993411 -4.4004388][-4.3966813 -4.3970613 -4.39808 -4.39946 -4.399941 -4.39918 -4.3975344 -4.3960848 -4.3953247 -4.3953085 -4.3961673 -4.3973031 -4.3983841 -4.3995481 -4.4008875][-4.3946881 -4.3951893 -4.3963852 -4.3975954 -4.3978286 -4.3966222 -4.3947415 -4.3932981 -4.3926373 -4.392827 -4.3938828 -4.3953557 -4.3968892 -4.3985052 -4.4002647][-4.3935089 -4.3941126 -4.3955669 -4.3965745 -4.3964572 -4.3948827 -4.3927608 -4.39107 -4.3900776 -4.3900666 -4.3912854 -4.3929877 -4.3947368 -4.39672 -4.3989086][-4.3932981 -4.3941412 -4.395896 -4.3967242 -4.3960176 -4.3939595 -4.3915334 -4.3895254 -4.3883333 -4.3882718 -4.3896432 -4.3914938 -4.3933344 -4.3955154 -4.3980231][-4.3942552 -4.3951173 -4.3966622 -4.39709 -4.3960533 -4.3938541 -4.391315 -4.3893256 -4.3882418 -4.3884239 -4.3898983 -4.3917165 -4.3935156 -4.39561 -4.3980904][-4.3960857 -4.3964753 -4.39737 -4.3974094 -4.396328 -4.394197 -4.3917608 -4.3899279 -4.3891139 -4.3897657 -4.3914862 -4.3932791 -4.3947406 -4.3963408 -4.3984909][-4.3976855 -4.3975234 -4.3977289 -4.3975048 -4.3965769 -4.3948593 -4.392796 -4.3912144 -4.3907766 -4.391819 -4.3935785 -4.3952794 -4.3963728 -4.3972783 -4.3987064][-4.3989058 -4.3983006 -4.3979821 -4.3975167 -4.3967071 -4.3955526 -4.3941703 -4.3931646 -4.3932085 -4.3944917 -4.3962173 -4.3976846 -4.398428 -4.3987818 -4.3994479][-4.3999629 -4.3992577 -4.3986177 -4.3980494 -4.3974543 -4.3969712 -4.3964906 -4.3962741 -4.3966756 -4.3977275 -4.3990183 -4.3998752 -4.4002142 -4.4002738 -4.400476][-4.4012871 -4.4007611 -4.4002171 -4.399868 -4.3996344 -4.3995843 -4.3997068 -4.3999858 -4.4004378 -4.4010553 -4.4016953 -4.4019094 -4.4017534 -4.4015226 -4.401515][-4.402585 -4.4024992 -4.4024858 -4.40263 -4.4026604 -4.4025745 -4.402667 -4.40284 -4.4030266 -4.4032307 -4.4034085 -4.4032407 -4.4028258 -4.4024253 -4.4023333][-4.4035382 -4.4036875 -4.403996 -4.4043355 -4.4044647 -4.4043574 -4.40424 -4.4041767 -4.4040952 -4.4040718 -4.4039721 -4.4036202 -4.4032187 -4.4029593 -4.4029651][-4.4042168 -4.404325 -4.4045329 -4.4046826 -4.4047465 -4.4046273 -4.4043512 -4.4041171 -4.4039488 -4.4039569 -4.4039006 -4.4036326 -4.4034238 -4.4033828 -4.4035454]]...]
INFO - root - 2017-12-05 09:35:30.823604: step 12510, loss = 1.15, batch loss = 1.09 (36.3 examples/sec; 0.220 sec/batch; 19h:35m:11s remains)
INFO - root - 2017-12-05 09:35:33.026860: step 12520, loss = 1.16, batch loss = 1.10 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:07s remains)
INFO - root - 2017-12-05 09:35:35.204604: step 12530, loss = 1.47, batch loss = 1.42 (36.7 examples/sec; 0.218 sec/batch; 19h:22m:29s remains)
INFO - root - 2017-12-05 09:35:37.390299: step 12540, loss = 1.45, batch loss = 1.39 (35.2 examples/sec; 0.227 sec/batch; 20h:11m:44s remains)
INFO - root - 2017-12-05 09:35:39.616474: step 12550, loss = 1.43, batch loss = 1.37 (36.0 examples/sec; 0.222 sec/batch; 19h:43m:56s remains)
INFO - root - 2017-12-05 09:35:41.766394: step 12560, loss = 1.52, batch loss = 1.46 (36.6 examples/sec; 0.218 sec/batch; 19h:24m:34s remains)
INFO - root - 2017-12-05 09:35:43.953971: step 12570, loss = 1.37, batch loss = 1.32 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:33s remains)
INFO - root - 2017-12-05 09:35:46.148365: step 12580, loss = 1.12, batch loss = 1.06 (36.7 examples/sec; 0.218 sec/batch; 19h:22m:42s remains)
INFO - root - 2017-12-05 09:35:48.343581: step 12590, loss = 1.68, batch loss = 1.62 (37.2 examples/sec; 0.215 sec/batch; 19h:05m:22s remains)
INFO - root - 2017-12-05 09:35:50.513561: step 12600, loss = 1.23, batch loss = 1.18 (36.4 examples/sec; 0.220 sec/batch; 19h:30m:55s remains)
2017-12-05 09:35:50.777060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2111111 -4.2086625 -4.1966181 -4.1738439 -4.1507125 -4.1346035 -4.1264081 -4.1163478 -4.09426 -4.0556269 -3.9946978 -3.910048 -3.8223262 -3.7674792 -3.7661214][-4.1262722 -4.0803623 -4.0314789 -3.9828939 -3.9440842 -3.9210732 -3.912991 -3.9068675 -3.8896668 -3.8524082 -3.7905998 -3.7032394 -3.6184738 -3.5768526 -3.5953331][-4.0351124 -3.9567382 -3.8805318 -3.8117702 -3.760776 -3.734767 -3.7303469 -3.7323351 -3.7263331 -3.7000122 -3.6507695 -3.5801888 -3.5213881 -3.5103769 -3.554343][-3.9529355 -3.8577523 -3.7726047 -3.7028003 -3.6561983 -3.638479 -3.6449254 -3.6599841 -3.66649 -3.6534719 -3.6217372 -3.5806649 -3.5596836 -3.5846658 -3.6523874][-3.8869305 -3.7834039 -3.7012696 -3.6464515 -3.6209171 -3.6265211 -3.657434 -3.6959898 -3.721911 -3.7250674 -3.7082057 -3.6887002 -3.6925652 -3.7358561 -3.8106761][-3.8189116 -3.7090373 -3.633532 -3.5965846 -3.5975075 -3.6353688 -3.7024224 -3.7763064 -3.8291526 -3.8500843 -3.8407362 -3.8246489 -3.8273561 -3.8609571 -3.91984][-3.7242951 -3.60763 -3.5357819 -3.5114255 -3.534426 -3.6010532 -3.7027197 -3.8116484 -3.8919368 -3.9274759 -3.9192932 -3.8910341 -3.8698342 -3.8698418 -3.8929513][-3.6039608 -3.4852538 -3.418087 -3.4032431 -3.4410181 -3.5265121 -3.6471777 -3.7736259 -3.8666844 -3.9054031 -3.8917189 -3.8474271 -3.7977986 -3.7579861 -3.7379141][-3.4914713 -3.3792553 -3.3253992 -3.3262448 -3.3792467 -3.4756327 -3.5976806 -3.7186949 -3.8006275 -3.8234091 -3.7936063 -3.7320445 -3.657129 -3.5806103 -3.5187628][-3.4307053 -3.3294463 -3.2932689 -3.3154197 -3.3869231 -3.4940627 -3.6153717 -3.7253528 -3.7878814 -3.7846861 -3.7291336 -3.6415114 -3.5339894 -3.4190784 -3.3154953][-3.4516811 -3.358367 -3.3344953 -3.3703623 -3.4537234 -3.5687463 -3.6940374 -3.8017018 -3.8529634 -3.8314173 -3.7524791 -3.6366603 -3.4933386 -3.3361607 -3.1871729][-3.5346439 -3.4400992 -3.4132025 -3.4439423 -3.5224378 -3.6348777 -3.7629688 -3.8755102 -3.9312615 -3.9141774 -3.8360317 -3.7131772 -3.5515351 -3.362119 -3.1737537][-3.6273222 -3.5237765 -3.4783354 -3.4858859 -3.5411096 -3.6373663 -3.7599695 -3.8760662 -3.9459717 -3.9511676 -3.8972166 -3.7941403 -3.6433825 -3.4517007 -3.247772][-3.665978 -3.5517061 -3.4860156 -3.4654102 -3.489284 -3.5585489 -3.6617289 -3.7700484 -3.85043 -3.882803 -3.8655064 -3.8013353 -3.6866348 -3.5243621 -3.3348522][-3.6162775 -3.5018349 -3.4273152 -3.3857846 -3.3814673 -3.4197576 -3.4935665 -3.5816693 -3.6580887 -3.7064908 -3.7215874 -3.6986389 -3.6291137 -3.5119543 -3.3601499]]...]
INFO - root - 2017-12-05 09:35:52.968774: step 12610, loss = 1.42, batch loss = 1.36 (36.9 examples/sec; 0.217 sec/batch; 19h:16m:35s remains)
INFO - root - 2017-12-05 09:35:55.168749: step 12620, loss = 0.86, batch loss = 0.80 (37.1 examples/sec; 0.216 sec/batch; 19h:09m:38s remains)
INFO - root - 2017-12-05 09:35:57.342118: step 12630, loss = 1.93, batch loss = 1.87 (37.1 examples/sec; 0.216 sec/batch; 19h:10m:15s remains)
INFO - root - 2017-12-05 09:35:59.539734: step 12640, loss = 1.29, batch loss = 1.23 (36.8 examples/sec; 0.218 sec/batch; 19h:19m:39s remains)
INFO - root - 2017-12-05 09:36:01.746019: step 12650, loss = 1.50, batch loss = 1.45 (36.8 examples/sec; 0.218 sec/batch; 19h:20m:22s remains)
INFO - root - 2017-12-05 09:36:03.920734: step 12660, loss = 1.33, batch loss = 1.27 (37.1 examples/sec; 0.215 sec/batch; 19h:08m:42s remains)
INFO - root - 2017-12-05 09:36:06.111953: step 12670, loss = 1.54, batch loss = 1.48 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:03s remains)
INFO - root - 2017-12-05 09:36:08.315846: step 12680, loss = 1.34, batch loss = 1.28 (34.1 examples/sec; 0.234 sec/batch; 20h:49m:16s remains)
INFO - root - 2017-12-05 09:36:10.517167: step 12690, loss = 1.62, batch loss = 1.56 (36.0 examples/sec; 0.222 sec/batch; 19h:45m:19s remains)
INFO - root - 2017-12-05 09:36:12.714954: step 12700, loss = 1.17, batch loss = 1.12 (36.2 examples/sec; 0.221 sec/batch; 19h:37m:19s remains)
2017-12-05 09:36:13.023469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1789966 -3.8152552 -3.1856346 -2.3596549 -1.5145328 -0.86496139 -0.579016 -0.73293447 -1.2989478 -2.1084781 -2.9185705 -3.5397239 -3.9029255 -4.0346036 -4.0259781][-4.0772843 -3.5494127 -2.6263993 -1.3905361 -0.10059929 0.917521 1.4004097 1.2170172 0.40613031 -0.78310418 -2.0024991 -2.9946461 -3.6538994 -4.0011158 -4.1409597][-3.9415963 -3.240901 -2.0186832 -0.35404158 1.4284978 2.8877 3.6409674 3.4789696 2.44732 0.8729496 -0.79209995 -2.2095838 -3.2179503 -3.8198614 -4.1306958][-3.7211618 -2.8270512 -1.3192651 0.73250771 2.9725347 4.8749857 5.9385471 5.8640413 4.6850758 2.7870984 0.70526028 -1.1447625 -2.5343847 -3.4293146 -3.9444394][-3.3496559 -2.2156045 -0.43009233 1.9299626 4.5069609 6.7428141 8.0674076 8.1121111 6.8845549 4.7892523 2.3977752 0.17579556 -1.5907841 -2.8196194 -3.5958288][-2.8064828 -1.3733814 0.687243 3.2588334 5.9920874 8.35216 9.790307 9.9284019 8.74727 6.6268597 4.1033888 1.6407771 -0.44527078 -2.0201573 -3.1065552][-2.1829522 -0.43791485 1.8634429 4.5279274 7.2117052 9.4517736 10.808225 10.972158 9.91234 7.9490738 5.5115042 2.9973249 0.71657753 -1.1552896 -2.5536568][-1.6728671 0.29455519 2.7180076 5.3182087 7.752039 9.6547709 10.751422 10.866746 9.988637 8.35005 6.2154217 3.861021 1.5621037 -0.47570372 -2.1015372][-1.479461 0.51793051 2.8592486 5.2009172 7.2091856 8.62451 9.3482847 9.3669968 8.7221384 7.529398 5.8631563 3.8606958 1.7439232 -0.25925541 -1.9320219][-1.6987822 0.089108944 2.1127725 4.0184956 5.5080395 6.4162712 6.7792711 6.7094364 6.2923145 5.5478888 4.4051714 2.8799286 1.130796 -0.61713362 -2.1224265][-2.2577002 -0.87092042 0.65813446 2.0318861 3.0126462 3.5097256 3.6204119 3.5052013 3.2642622 2.8750319 2.2033582 1.1869507 -0.084337711 -1.421438 -2.5997071][-2.9575365 -2.037688 -1.0429192 -0.18288517 0.38034725 0.60332823 0.58445549 0.46870089 0.34149647 0.17640495 -0.15297222 -0.73157263 -1.5248916 -2.401963 -3.1930211][-3.5932782 -3.0782778 -2.5304284 -2.0730493 -1.798851 -1.726629 -1.7843456 -1.8738151 -1.9382138 -1.9953856 -2.1284287 -2.401063 -2.8100157 -3.2841611 -3.7219772][-4.0388813 -3.7995131 -3.5469303 -3.3423529 -3.2291534 -3.2174397 -3.2671978 -3.3256841 -3.3616376 -3.3818097 -3.4275708 -3.5312996 -3.6998007 -3.902473 -4.0939264][-4.2758713 -4.1853805 -4.0890422 -4.0120964 -3.9717762 -3.9733589 -4.0001392 -4.0301938 -4.0492816 -4.05916 -4.0740137 -4.1067548 -4.1629467 -4.23118 -4.2956095]]...]
INFO - root - 2017-12-05 09:36:15.214535: step 12710, loss = 1.45, batch loss = 1.39 (36.3 examples/sec; 0.221 sec/batch; 19h:35m:26s remains)
INFO - root - 2017-12-05 09:36:17.392957: step 12720, loss = 1.52, batch loss = 1.46 (36.6 examples/sec; 0.218 sec/batch; 19h:23m:54s remains)
INFO - root - 2017-12-05 09:36:19.581263: step 12730, loss = 1.30, batch loss = 1.24 (35.9 examples/sec; 0.223 sec/batch; 19h:47m:01s remains)
INFO - root - 2017-12-05 09:36:21.783474: step 12740, loss = 1.41, batch loss = 1.35 (36.5 examples/sec; 0.219 sec/batch; 19h:27m:04s remains)
INFO - root - 2017-12-05 09:36:24.001602: step 12750, loss = 1.65, batch loss = 1.59 (35.7 examples/sec; 0.224 sec/batch; 19h:53m:57s remains)
INFO - root - 2017-12-05 09:36:26.187214: step 12760, loss = 1.55, batch loss = 1.49 (36.7 examples/sec; 0.218 sec/batch; 19h:22m:52s remains)
INFO - root - 2017-12-05 09:36:28.361731: step 12770, loss = 1.46, batch loss = 1.40 (37.5 examples/sec; 0.214 sec/batch; 18h:58m:16s remains)
INFO - root - 2017-12-05 09:36:30.536443: step 12780, loss = 1.07, batch loss = 1.01 (36.2 examples/sec; 0.221 sec/batch; 19h:38m:04s remains)
INFO - root - 2017-12-05 09:36:32.723262: step 12790, loss = 1.72, batch loss = 1.66 (36.4 examples/sec; 0.220 sec/batch; 19h:31m:25s remains)
INFO - root - 2017-12-05 09:36:34.873041: step 12800, loss = 1.32, batch loss = 1.26 (37.9 examples/sec; 0.211 sec/batch; 18h:45m:57s remains)
2017-12-05 09:36:35.158736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3881989 -4.3866405 -4.3858566 -4.3857584 -4.3860378 -4.3861771 -4.3860054 -4.3856788 -4.385282 -4.384841 -4.3844228 -4.3844705 -4.3854966 -4.3872337 -4.3893][-4.3873625 -4.3859959 -4.3854151 -4.3856964 -4.3861108 -4.3862162 -4.3859115 -4.3854642 -4.3849978 -4.3847432 -4.3846822 -4.3851166 -4.3866844 -4.3890133 -4.3915491][-4.3906727 -4.3898158 -4.3897996 -4.3901405 -4.3903866 -4.3901062 -4.3893518 -4.3886452 -4.3879814 -4.38755 -4.3873587 -4.3878131 -4.3893318 -4.3915834 -4.3941159][-4.3940678 -4.3934436 -4.3936896 -4.3941255 -4.3941965 -4.3938313 -4.393187 -4.3926315 -4.391902 -4.3912883 -4.3908267 -4.391006 -4.3919334 -4.3935814 -4.3955932][-4.3949428 -4.3946719 -4.3948708 -4.3950238 -4.39481 -4.394486 -4.3942018 -4.3940983 -4.39392 -4.3938246 -4.3938832 -4.3942237 -4.3948913 -4.3959074 -4.3970585][-4.3938766 -4.3941603 -4.3947711 -4.3951192 -4.3948517 -4.3944268 -4.3941703 -4.3943038 -4.3945794 -4.3949456 -4.3953042 -4.3956871 -4.3961692 -4.3967662 -4.3973508][-4.3938255 -4.3940806 -4.394558 -4.394876 -4.3946595 -4.3940463 -4.3934383 -4.3933668 -4.3936419 -4.3939767 -4.3942041 -4.3945222 -4.39489 -4.3953567 -4.3958125][-4.3963008 -4.3958158 -4.3954191 -4.3948827 -4.3939981 -4.3928361 -4.3919697 -4.3920383 -4.3927588 -4.3936272 -4.3942933 -4.3949084 -4.3954945 -4.396142 -4.3966527][-4.4004526 -4.4000926 -4.3996954 -4.398953 -4.3978076 -4.396596 -4.3957481 -4.3957705 -4.3964434 -4.3973131 -4.39802 -4.3984866 -4.3987989 -4.3991575 -4.3993688][-4.4020534 -4.4020286 -4.4020457 -4.4017444 -4.4011865 -4.4007244 -4.4005184 -4.4006786 -4.40114 -4.4016848 -4.402041 -4.4021506 -4.4020882 -4.4019346 -4.4016609][-4.4020672 -4.4020371 -4.4021516 -4.4021688 -4.4020553 -4.4020638 -4.4022355 -4.4025221 -4.4028759 -4.4031334 -4.4032512 -4.4030728 -4.4027748 -4.4024682 -4.4020863][-4.4019356 -4.4019065 -4.4020576 -4.4021907 -4.4022689 -4.4023743 -4.40253 -4.4027209 -4.4029813 -4.4031706 -4.4031849 -4.4029093 -4.4026084 -4.4024386 -4.4022474][-4.4018393 -4.401731 -4.4017482 -4.4017549 -4.4017515 -4.4017553 -4.4017887 -4.4019151 -4.4021964 -4.4024577 -4.4025021 -4.4023128 -4.4021864 -4.4021759 -4.4021192][-4.4016232 -4.4013915 -4.4012127 -4.401032 -4.4008131 -4.4006605 -4.4006605 -4.4008493 -4.4012065 -4.4015102 -4.4016762 -4.4017019 -4.401731 -4.4018044 -4.4018192][-4.4013157 -4.4010305 -4.4007559 -4.4004335 -4.400063 -4.3997712 -4.399694 -4.3998585 -4.4002209 -4.40064 -4.4010129 -4.4012632 -4.401422 -4.4015236 -4.4015265]]...]
INFO - root - 2017-12-05 09:36:37.335205: step 12810, loss = 1.73, batch loss = 1.68 (37.2 examples/sec; 0.215 sec/batch; 19h:06m:30s remains)
INFO - root - 2017-12-05 09:36:39.555012: step 12820, loss = 1.39, batch loss = 1.33 (36.4 examples/sec; 0.220 sec/batch; 19h:30m:55s remains)
INFO - root - 2017-12-05 09:36:41.730995: step 12830, loss = 1.87, batch loss = 1.81 (36.8 examples/sec; 0.217 sec/batch; 19h:18m:14s remains)
INFO - root - 2017-12-05 09:36:43.933481: step 12840, loss = 1.75, batch loss = 1.69 (36.6 examples/sec; 0.219 sec/batch; 19h:24m:47s remains)
INFO - root - 2017-12-05 09:36:46.106104: step 12850, loss = 1.42, batch loss = 1.37 (36.4 examples/sec; 0.220 sec/batch; 19h:31m:06s remains)
INFO - root - 2017-12-05 09:36:48.276649: step 12860, loss = 0.97, batch loss = 0.92 (38.1 examples/sec; 0.210 sec/batch; 18h:39m:48s remains)
INFO - root - 2017-12-05 09:36:50.468154: step 12870, loss = 1.06, batch loss = 1.00 (35.2 examples/sec; 0.227 sec/batch; 20h:09m:35s remains)
INFO - root - 2017-12-05 09:36:52.660539: step 12880, loss = 1.16, batch loss = 1.11 (37.2 examples/sec; 0.215 sec/batch; 19h:04m:34s remains)
INFO - root - 2017-12-05 09:36:54.892157: step 12890, loss = 1.07, batch loss = 1.01 (35.5 examples/sec; 0.226 sec/batch; 20h:01m:20s remains)
INFO - root - 2017-12-05 09:36:57.055786: step 12900, loss = 1.55, batch loss = 1.49 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:39s remains)
2017-12-05 09:36:57.346520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0380578 -4.0815077 -4.1517496 -4.2113171 -4.2508445 -4.2731047 -4.2803597 -4.2727332 -4.2527008 -4.2250848 -4.1942019 -4.1610584 -4.1273012 -4.1000257 -4.0944209][-3.9271529 -3.9735844 -4.0500884 -4.1150565 -4.1571288 -4.1805739 -4.1896415 -4.1846633 -4.165616 -4.1365924 -4.1030955 -4.0682778 -4.0339093 -4.0071478 -4.0028229][-3.8062167 -3.8458583 -3.9191043 -3.9822578 -4.0212107 -4.0412216 -4.0491095 -4.0456824 -4.0288324 -4.000823 -3.9681566 -3.9354482 -3.9041584 -3.8806829 -3.8794641][-3.6766698 -3.7019377 -3.765332 -3.820812 -3.8533771 -3.867265 -3.8701048 -3.8645039 -3.8488874 -3.824146 -3.795676 -3.7682862 -3.7441316 -3.7266397 -3.7301178][-3.5433066 -3.5489187 -3.6011405 -3.6498783 -3.6779556 -3.6871533 -3.6843362 -3.6741011 -3.6601098 -3.6418993 -3.6210535 -3.6010282 -3.5853188 -3.5754259 -3.5842624][-3.416095 -3.4003434 -3.4457266 -3.4950032 -3.5269094 -3.5379481 -3.5325303 -3.5183666 -3.5060496 -3.4959369 -3.4846652 -3.4731266 -3.4662175 -3.4644742 -3.4794834][-3.310921 -3.2746668 -3.3212824 -3.3798637 -3.4241388 -3.4438932 -3.4409621 -3.4270411 -3.4176011 -3.4138849 -3.410574 -3.4069929 -3.4094136 -3.4168212 -3.43847][-3.2414727 -3.1922054 -3.2476768 -3.3211036 -3.3809662 -3.4119713 -3.4151587 -3.4070382 -3.4017639 -3.4007931 -3.4014637 -3.40448 -3.4161358 -3.4326582 -3.4586825][-3.2239089 -3.171288 -3.2382836 -3.3248215 -3.3953176 -3.4341166 -3.4445174 -3.4432933 -3.4416323 -3.4410169 -3.4433818 -3.4517574 -3.4706316 -3.4931009 -3.5192709][-3.2750943 -3.2243972 -3.2960625 -3.3836946 -3.4539878 -3.4943347 -3.5098763 -3.5148182 -3.5166011 -3.5162601 -3.5183938 -3.5283055 -3.5480304 -3.5679011 -3.585273][-3.3890169 -3.3456092 -3.4154265 -3.4919558 -3.5511541 -3.5862594 -3.6033511 -3.6120515 -3.6160207 -3.6153493 -3.615238 -3.6210179 -3.6320825 -3.6383433 -3.6381693][-3.544518 -3.5144167 -3.577985 -3.6372371 -3.6787717 -3.7040935 -3.7190681 -3.7288213 -3.7335596 -3.7316709 -3.727113 -3.7237909 -3.7186379 -3.7033734 -3.6815145][-3.7037086 -3.6892927 -3.7462738 -3.7893505 -3.8149447 -3.8302808 -3.8414969 -3.8495107 -3.8524673 -3.8476393 -3.8364489 -3.8209982 -3.79692 -3.7607815 -3.72113][-3.8286026 -3.828506 -3.8822021 -3.9155414 -3.9319036 -3.9406765 -3.9478724 -3.9525778 -3.9522939 -3.9440007 -3.9277694 -3.9043751 -3.869972 -3.8247609 -3.779325][-3.9023986 -3.910804 -3.9649785 -3.9951525 -4.0087996 -4.0153494 -4.0207405 -4.0241051 -4.0234132 -4.0171661 -4.0042515 -3.9836287 -3.9522896 -3.9122372 -3.8731906]]...]
INFO - root - 2017-12-05 09:36:59.547488: step 12910, loss = 1.47, batch loss = 1.41 (36.9 examples/sec; 0.217 sec/batch; 19h:13m:50s remains)
INFO - root - 2017-12-05 09:37:01.731385: step 12920, loss = 1.63, batch loss = 1.57 (36.7 examples/sec; 0.218 sec/batch; 19h:20m:06s remains)
INFO - root - 2017-12-05 09:37:03.901015: step 12930, loss = 1.22, batch loss = 1.16 (35.8 examples/sec; 0.223 sec/batch; 19h:49m:39s remains)
INFO - root - 2017-12-05 09:37:06.096340: step 12940, loss = 1.82, batch loss = 1.76 (34.6 examples/sec; 0.231 sec/batch; 20h:30m:45s remains)
INFO - root - 2017-12-05 09:37:08.292130: step 12950, loss = 1.45, batch loss = 1.39 (36.6 examples/sec; 0.219 sec/batch; 19h:24m:13s remains)
INFO - root - 2017-12-05 09:37:10.471853: step 12960, loss = 1.30, batch loss = 1.24 (36.9 examples/sec; 0.217 sec/batch; 19h:13m:33s remains)
INFO - root - 2017-12-05 09:37:12.665624: step 12970, loss = 1.27, batch loss = 1.21 (36.4 examples/sec; 0.220 sec/batch; 19h:29m:36s remains)
INFO - root - 2017-12-05 09:37:14.872746: step 12980, loss = 1.51, batch loss = 1.45 (36.3 examples/sec; 0.221 sec/batch; 19h:34m:33s remains)
INFO - root - 2017-12-05 09:37:17.056649: step 12990, loss = 1.26, batch loss = 1.20 (37.0 examples/sec; 0.216 sec/batch; 19h:11m:44s remains)
INFO - root - 2017-12-05 09:37:19.251351: step 13000, loss = 1.56, batch loss = 1.50 (37.5 examples/sec; 0.213 sec/batch; 18h:55m:56s remains)
2017-12-05 09:37:19.534248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2239895 -4.1755104 -4.0734549 -3.9182334 -3.7564273 -3.6440458 -3.6168342 -3.6662502 -3.7738481 -3.9209504 -4.0754294 -4.1974492 -4.27056 -4.3053908 -4.3186069][-4.2466321 -4.1456013 -3.9577928 -3.6848211 -3.3961129 -3.1765642 -3.0948732 -3.1586132 -3.3409443 -3.597115 -3.8681703 -4.0915203 -4.2325768 -4.3033953 -4.331924][-4.251018 -4.0746508 -3.7672484 -3.3382449 -2.8828564 -2.5200152 -2.3608027 -2.4408309 -2.725178 -3.1326208 -3.5636909 -3.9273064 -4.1678314 -4.2946324 -4.3474422][-4.2198162 -3.9432075 -3.4841337 -2.8688307 -2.2208936 -1.6902406 -1.4371977 -1.5380945 -1.9476964 -2.5398066 -3.163074 -3.6937861 -4.0576453 -4.2585955 -4.3448329][-4.1488619 -3.7514539 -3.121506 -2.3120422 -1.4677637 -0.76688623 -0.42153168 -0.555995 -1.1042626 -1.8882818 -2.7091036 -3.413219 -3.9111724 -4.1987906 -4.3268843][-4.0432706 -3.5113022 -2.7101009 -1.7231226 -0.71295977 0.12584162 0.53933668 0.3555522 -0.3294816 -1.2807944 -2.2666945 -3.1204133 -3.7436652 -4.12061 -4.2972288][-3.9084854 -3.2512383 -2.3042719 -1.1871619 -0.07459116 0.83193541 1.2640409 1.0259714 0.23881102 -0.82161617 -1.910459 -2.8635898 -3.5824156 -4.0371552 -4.261518][-3.7669516 -3.0233955 -1.9935918 -0.82667375 0.29618597 1.1808052 1.5737309 1.2958779 0.47303486 -0.61093855 -1.7211039 -2.7070491 -3.4721346 -3.9750719 -4.2342834][-3.6559644 -2.8901443 -1.867523 -0.75039983 0.28209639 1.0516562 1.3587356 1.0742917 0.30074978 -0.71253896 -1.7586629 -2.701314 -3.4490919 -3.9543974 -4.2237549][-3.6121116 -2.9011922 -1.9824107 -1.0129292 -0.1544323 0.44509411 0.65283394 0.38927317 -0.27408934 -1.1428261 -2.0483599 -2.8728518 -3.5348222 -3.9896955 -4.2377415][-3.6556592 -3.0662305 -2.3255064 -1.5702376 -0.93223977 -0.51599312 -0.39949083 -0.62673044 -1.1497056 -1.8293278 -2.5394886 -3.1884894 -3.7124062 -4.0748425 -4.2743812][-3.7804029 -3.3511932 -2.8270621 -2.3110421 -1.8941748 -1.6438468 -1.5996675 -1.7821717 -2.1576614 -2.633302 -3.1281302 -3.577323 -3.9375315 -4.184577 -4.320334][-3.967042 -3.7001126 -3.3849926 -3.0876927 -2.8594308 -2.7363 -2.7364249 -2.8676536 -3.1056714 -3.3933454 -3.6868908 -3.9470184 -4.1507349 -4.2870078 -4.3608513][-4.1643782 -4.0273685 -3.8717878 -3.7337229 -3.6358232 -3.5924163 -3.61053 -3.6904469 -3.8150105 -3.9561691 -4.0948005 -4.2119675 -4.299849 -4.3566847 -4.3872132][-4.3064556 -4.2501163 -4.1891084 -4.1396179 -4.1090631 -4.1003265 -4.1161962 -4.1551695 -4.2064252 -4.2594671 -4.3076921 -4.3455296 -4.3728724 -4.3902659 -4.3995776]]...]
INFO - root - 2017-12-05 09:37:21.764971: step 13010, loss = 1.40, batch loss = 1.34 (37.2 examples/sec; 0.215 sec/batch; 19h:04m:45s remains)
INFO - root - 2017-12-05 09:37:23.937963: step 13020, loss = 1.57, batch loss = 1.51 (36.9 examples/sec; 0.217 sec/batch; 19h:15m:42s remains)
INFO - root - 2017-12-05 09:37:26.133441: step 13030, loss = 1.72, batch loss = 1.66 (36.1 examples/sec; 0.222 sec/batch; 19h:40m:22s remains)
INFO - root - 2017-12-05 09:37:28.302734: step 13040, loss = 1.30, batch loss = 1.24 (36.9 examples/sec; 0.217 sec/batch; 19h:15m:09s remains)
INFO - root - 2017-12-05 09:37:30.494198: step 13050, loss = 1.47, batch loss = 1.41 (36.3 examples/sec; 0.221 sec/batch; 19h:34m:08s remains)
INFO - root - 2017-12-05 09:37:32.680705: step 13060, loss = 1.52, batch loss = 1.46 (37.5 examples/sec; 0.213 sec/batch; 18h:55m:25s remains)
INFO - root - 2017-12-05 09:37:34.906125: step 13070, loss = 1.57, batch loss = 1.51 (37.1 examples/sec; 0.216 sec/batch; 19h:09m:11s remains)
INFO - root - 2017-12-05 09:37:37.079740: step 13080, loss = 1.41, batch loss = 1.35 (36.5 examples/sec; 0.219 sec/batch; 19h:25m:38s remains)
INFO - root - 2017-12-05 09:37:39.305958: step 13090, loss = 1.99, batch loss = 1.94 (35.5 examples/sec; 0.225 sec/batch; 19h:58m:30s remains)
INFO - root - 2017-12-05 09:37:41.496750: step 13100, loss = 1.66, batch loss = 1.60 (35.9 examples/sec; 0.223 sec/batch; 19h:46m:45s remains)
2017-12-05 09:37:41.785544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2090454 -4.2082815 -4.2136517 -4.2148733 -4.2069588 -4.1872425 -4.1575074 -4.1231914 -4.093688 -4.076848 -4.0819893 -4.114409 -4.1694889 -4.2354283 -4.29785][-3.9283969 -3.8875804 -3.8602197 -3.83189 -3.7957768 -3.7482066 -3.6874347 -3.6216588 -3.565079 -3.5367296 -3.5591152 -3.6455641 -3.7887714 -3.959446 -4.1226153][-3.5202715 -3.3762989 -3.2523046 -3.139852 -3.0361981 -2.9403384 -2.8407757 -2.7430153 -2.6670184 -2.6434398 -2.7119594 -2.8975549 -3.1875219 -3.5274096 -3.8510046][-3.0880468 -2.7802279 -2.4895499 -2.2241657 -1.995414 -1.8103404 -1.6492472 -1.5112612 -1.4228077 -1.4274991 -1.5798318 -1.9115288 -2.4016881 -2.9670124 -3.5004525][-2.7281647 -2.2399623 -1.7510214 -1.2855353 -0.872628 -0.534744 -0.25500631 -0.035681248 0.078269005 0.027992725 -0.24439383 -0.76564264 -1.5008769 -2.3293278 -3.1015191][-2.4965425 -1.865521 -1.1989546 -0.52541327 0.11838531 0.68400431 1.167532 1.5431871 1.7234516 1.6276584 1.2085085 0.45984459 -0.55648518 -1.6701913 -2.691318][-2.4051595 -1.7138264 -0.94350767 -0.10612917 0.76530123 1.5988946 2.3485651 2.9450498 3.2410131 3.1222925 2.5624146 1.5884194 0.29606581 -1.0870731 -2.3349664][-2.4551 -1.8011615 -1.0316281 -0.12964487 0.88764763 1.9353161 2.9234986 3.7383919 4.1706181 4.0717897 3.4362173 2.3159652 0.837183 -0.72323918 -2.1152644][-2.6606512 -2.1205056 -1.453707 -0.61685014 0.39381409 1.5012259 2.58844 3.513773 4.0407429 4.0048361 3.3986201 2.2832489 0.80581856 -0.74777961 -2.1298954][-3.0022039 -2.6126623 -2.1085968 -1.4369655 -0.5788238 0.40976381 1.4126024 2.2865438 2.8122206 2.8374081 2.3434811 1.3868785 0.11493206 -1.2257485 -2.4229007][-3.4084 -3.1635494 -2.8318949 -2.3640547 -1.7368062 -0.98232436 -0.19543886 0.50302505 0.94095087 0.99790382 0.64763975 -0.064839363 -1.0120585 -2.0129685 -2.9108009][-3.784543 -3.6516652 -3.4641082 -3.1834402 -2.7878585 -2.2936294 -1.765362 -1.2887826 -0.98051357 -0.923722 -1.1419313 -1.6024499 -2.2116416 -2.8568439 -3.4368005][-4.0702758 -4.0090322 -3.9186697 -3.7750163 -3.5621698 -3.2866795 -2.9849963 -2.7085018 -2.5244155 -2.4825323 -2.5994644 -2.8547606 -3.1913533 -3.5484517 -3.8683136][-4.2518177 -4.2287683 -4.1928482 -4.1314878 -4.0352778 -3.9071608 -3.7642848 -3.6322463 -3.5425413 -3.5201788 -3.5728083 -3.6907172 -3.8459094 -4.0101986 -4.1563611][-4.3472857 -4.3409147 -4.3293567 -4.3079929 -4.2725015 -4.2243218 -4.1701479 -4.1199074 -4.0851903 -4.0763407 -4.096025 -4.1401391 -4.1974988 -4.2578988 -4.311152]]...]
INFO - root - 2017-12-05 09:37:43.976631: step 13110, loss = 1.45, batch loss = 1.39 (35.8 examples/sec; 0.223 sec/batch; 19h:49m:11s remains)
INFO - root - 2017-12-05 09:37:46.149835: step 13120, loss = 1.75, batch loss = 1.70 (36.8 examples/sec; 0.217 sec/batch; 19h:17m:29s remains)
INFO - root - 2017-12-05 09:37:48.341063: step 13130, loss = 1.29, batch loss = 1.23 (38.0 examples/sec; 0.211 sec/batch; 18h:41m:21s remains)
INFO - root - 2017-12-05 09:37:50.540219: step 13140, loss = 1.34, batch loss = 1.28 (36.0 examples/sec; 0.222 sec/batch; 19h:43m:55s remains)
INFO - root - 2017-12-05 09:37:52.714256: step 13150, loss = 1.24, batch loss = 1.18 (37.3 examples/sec; 0.215 sec/batch; 19h:02m:40s remains)
INFO - root - 2017-12-05 09:37:54.917407: step 13160, loss = 1.74, batch loss = 1.68 (36.8 examples/sec; 0.218 sec/batch; 19h:17m:42s remains)
INFO - root - 2017-12-05 09:37:57.087145: step 13170, loss = 1.66, batch loss = 1.61 (38.3 examples/sec; 0.209 sec/batch; 18h:30m:22s remains)
INFO - root - 2017-12-05 09:37:59.266799: step 13180, loss = 1.65, batch loss = 1.59 (35.3 examples/sec; 0.227 sec/batch; 20h:07m:34s remains)
INFO - root - 2017-12-05 09:38:01.441059: step 13190, loss = 1.29, batch loss = 1.23 (36.9 examples/sec; 0.217 sec/batch; 19h:12m:47s remains)
INFO - root - 2017-12-05 09:38:03.642545: step 13200, loss = 1.94, batch loss = 1.88 (35.0 examples/sec; 0.229 sec/batch; 20h:16m:01s remains)
2017-12-05 09:38:03.950291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0029678 -4.03442 -4.0824022 -4.1211433 -4.1460848 -4.1538115 -4.1465521 -4.1342459 -4.1308804 -4.1474824 -4.1901879 -4.2534261 -4.3177023 -4.3657336 -4.3913736][-3.6394148 -3.6583812 -3.7029228 -3.7311275 -3.7383788 -3.723052 -3.6918876 -3.6660609 -3.6712418 -3.726018 -3.8414309 -4.0032496 -4.16837 -4.29476 -4.366498][-3.2615819 -3.2252047 -3.2114029 -3.1674571 -3.0965915 -3.0088334 -2.92728 -2.8867383 -2.917697 -3.041641 -3.2726514 -3.5849426 -3.905802 -4.1600051 -4.3130407][-3.0325685 -2.9019823 -2.7677021 -2.5802474 -2.3597543 -2.1433227 -1.9812717 -1.9226832 -1.9941425 -2.210793 -2.5822852 -3.0637751 -3.5583942 -3.9638512 -4.2235737][-3.0667984 -2.8408489 -2.5655656 -2.2057416 -1.8018138 -1.4281514 -1.1652708 -1.0733838 -1.1720312 -1.4658804 -1.9519098 -2.5681343 -3.2047787 -3.7453542 -4.1125469][-3.3274081 -3.0551476 -2.691263 -2.2142818 -1.6774278 -1.1803477 -0.82025576 -0.66611362 -0.74045968 -1.0460534 -1.5656621 -2.2335567 -2.9423015 -3.5693693 -4.0160956][-3.678679 -3.4234571 -3.0593426 -2.575969 -2.0231538 -1.4945669 -1.0800383 -0.848747 -0.83905911 -1.0663881 -1.5194366 -2.1454639 -2.8464644 -3.4959025 -3.9731138][-3.9967444 -3.8026128 -3.5107274 -3.1156704 -2.6506863 -2.1819215 -1.7748771 -1.4865639 -1.3702807 -1.4685521 -1.7960527 -2.3201656 -2.9510212 -3.5566568 -4.0045671][-4.2154326 -4.09435 -3.9031076 -3.6364384 -3.3076611 -2.9494143 -2.5985734 -2.2976749 -2.1060562 -2.0919013 -2.2957177 -2.7002382 -3.2199974 -3.7249532 -4.0919571][-4.3320351 -4.2713795 -4.1695304 -4.0189724 -3.8194098 -3.5772943 -3.3078883 -3.0411 -2.834187 -2.7646444 -2.8826408 -3.1755216 -3.5653257 -3.9373746 -4.1966648][-4.3809257 -4.3567963 -4.3127303 -4.2416744 -4.1363716 -3.9897211 -3.8051076 -3.6035633 -3.43322 -3.360333 -3.4289861 -3.6280632 -3.8906431 -4.130074 -4.2869506][-4.3967619 -4.3895025 -4.3738832 -4.3454924 -4.296042 -4.2158203 -4.1035013 -3.9737761 -3.8617961 -3.81188 -3.8534617 -3.9769835 -4.1329279 -4.2659478 -4.3469191][-4.4010649 -4.3996105 -4.3949285 -4.3848352 -4.36371 -4.3239007 -4.2636566 -4.1931195 -4.1337333 -4.1095228 -4.1351833 -4.20188 -4.2812209 -4.3441553 -4.3796797][-4.4022517 -4.4024477 -4.4010854 -4.3974352 -4.3889093 -4.3714085 -4.3442268 -4.3127522 -4.2876034 -4.2796421 -4.2938085 -4.3238568 -4.3570304 -4.381537 -4.3947396][-4.4025273 -4.4031253 -4.4027047 -4.4014654 -4.39829 -4.3916845 -4.3817663 -4.3710113 -4.3631215 -4.3618536 -4.3678832 -4.3783774 -4.388938 -4.39646 -4.4005213]]...]
INFO - root - 2017-12-05 09:38:06.105073: step 13210, loss = 1.17, batch loss = 1.11 (37.6 examples/sec; 0.213 sec/batch; 18h:53m:25s remains)
INFO - root - 2017-12-05 09:38:08.269039: step 13220, loss = 1.44, batch loss = 1.39 (37.9 examples/sec; 0.211 sec/batch; 18h:43m:33s remains)
INFO - root - 2017-12-05 09:38:10.506482: step 13230, loss = 1.24, batch loss = 1.18 (35.5 examples/sec; 0.225 sec/batch; 19h:57m:41s remains)
INFO - root - 2017-12-05 09:38:12.676431: step 13240, loss = 1.21, batch loss = 1.15 (36.9 examples/sec; 0.217 sec/batch; 19h:14m:23s remains)
INFO - root - 2017-12-05 09:38:14.852796: step 13250, loss = 1.27, batch loss = 1.21 (37.0 examples/sec; 0.216 sec/batch; 19h:10m:05s remains)
INFO - root - 2017-12-05 09:38:17.020998: step 13260, loss = 1.15, batch loss = 1.09 (36.5 examples/sec; 0.219 sec/batch; 19h:25m:25s remains)
INFO - root - 2017-12-05 09:38:19.184147: step 13270, loss = 1.05, batch loss = 0.99 (37.6 examples/sec; 0.213 sec/batch; 18h:52m:44s remains)
INFO - root - 2017-12-05 09:38:21.402550: step 13280, loss = 1.55, batch loss = 1.49 (34.5 examples/sec; 0.232 sec/batch; 20h:32m:33s remains)
INFO - root - 2017-12-05 09:38:23.599766: step 13290, loss = 1.73, batch loss = 1.67 (37.2 examples/sec; 0.215 sec/batch; 19h:04m:06s remains)
INFO - root - 2017-12-05 09:38:25.823551: step 13300, loss = 1.24, batch loss = 1.18 (37.1 examples/sec; 0.216 sec/batch; 19h:08m:15s remains)
2017-12-05 09:38:26.096205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.402945 -4.4022694 -4.4009328 -4.3985133 -4.3955359 -4.3919616 -4.3883324 -4.3847513 -4.3817434 -4.3797374 -4.3785529 -4.378675 -4.378685 -4.37852 -4.3786154][-4.4010682 -4.3979416 -4.3921146 -4.3825846 -4.3702912 -4.3562388 -4.3426514 -4.3302155 -4.320096 -4.3124657 -4.3078589 -4.3066812 -4.30496 -4.3025217 -4.3019733][-4.3962889 -4.3865805 -4.3686843 -4.34012 -4.3033042 -4.2626781 -4.2246909 -4.1927152 -4.1683683 -4.1504703 -4.1388574 -4.1334896 -4.1267018 -4.118535 -4.1159182][-4.3858476 -4.3617077 -4.3180194 -4.2506495 -4.1648755 -4.0724525 -3.9908266 -3.9300237 -3.8904281 -3.8627377 -3.8412695 -3.8272734 -3.811635 -3.7955382 -3.7906387][-4.367187 -4.3181124 -4.2310557 -4.1011853 -3.9374642 -3.7643321 -3.6225612 -3.5337918 -3.4888265 -3.4607015 -3.43329 -3.410038 -3.3850842 -3.3592896 -3.3490984][-4.3411975 -4.2580934 -4.1133814 -3.9001145 -3.6335478 -3.3606129 -3.1550756 -3.0531936 -3.0273957 -3.0226223 -3.0048971 -2.9785242 -2.943747 -2.9046357 -2.8818212][-4.3121696 -4.1930737 -3.9877477 -3.68522 -3.3113794 -2.9429293 -2.6881051 -2.5949893 -2.6156545 -2.6626272 -2.675034 -2.6534679 -2.6076581 -2.5473313 -2.5003848][-4.2884259 -4.1422787 -3.8903389 -3.5178866 -3.0603595 -2.6225991 -2.3385577 -2.264436 -2.338572 -2.4452429 -2.4974129 -2.4868598 -2.4287772 -2.3388035 -2.2528598][-4.2772679 -4.1206384 -3.8510015 -3.4506869 -2.9584541 -2.491755 -2.1929586 -2.1214004 -2.2159765 -2.3515673 -2.426198 -2.4189463 -2.3439531 -2.2206211 -2.087054][-4.2811785 -4.132442 -3.8766565 -3.4949088 -3.0219431 -2.5685935 -2.26369 -2.1631172 -2.2171431 -2.3193092 -2.3743498 -2.3501341 -2.254225 -2.1027765 -1.9293132][-4.2957206 -4.1674485 -3.9465034 -3.6147664 -3.1976392 -2.7821603 -2.4712088 -2.3107891 -2.269897 -2.2786243 -2.2673004 -2.2050815 -2.0903111 -1.9297037 -1.7465796][-4.3136654 -4.2094126 -4.0300121 -3.7592177 -3.4098828 -3.0381575 -2.7161732 -2.4776621 -2.3121026 -2.1910903 -2.0821528 -1.9685493 -1.8408153 -1.6970649 -1.5463088][-4.3294497 -4.2455912 -4.1008873 -3.8809752 -3.5886159 -3.2521245 -2.9138746 -2.5974712 -2.3105655 -2.0599737 -1.8490233 -1.6813433 -1.5491173 -1.4423311 -1.3570733][-4.3394184 -4.2672625 -4.14153 -3.9477539 -3.6848445 -3.3638177 -3.0070486 -2.6293683 -2.2521813 -1.9078927 -1.6216438 -1.4117699 -1.2767217 -1.205662 -1.1885688][-4.3410873 -4.268796 -4.1417227 -3.9442556 -3.6750975 -3.3436422 -2.9673231 -2.5586381 -2.1453257 -1.7660303 -1.4477096 -1.2128916 -1.0692286 -1.0135331 -1.035676]]...]
INFO - root - 2017-12-05 09:38:28.259230: step 13310, loss = 1.39, batch loss = 1.33 (38.0 examples/sec; 0.210 sec/batch; 18h:39m:14s remains)
INFO - root - 2017-12-05 09:38:30.435347: step 13320, loss = 1.79, batch loss = 1.73 (36.5 examples/sec; 0.219 sec/batch; 19h:26m:50s remains)
INFO - root - 2017-12-05 09:38:32.623315: step 13330, loss = 1.57, batch loss = 1.51 (37.0 examples/sec; 0.216 sec/batch; 19h:11m:14s remains)
INFO - root - 2017-12-05 09:38:34.821630: step 13340, loss = 1.08, batch loss = 1.02 (37.0 examples/sec; 0.216 sec/batch; 19h:10m:07s remains)
INFO - root - 2017-12-05 09:38:37.040227: step 13350, loss = 1.42, batch loss = 1.36 (37.3 examples/sec; 0.214 sec/batch; 18h:59m:42s remains)
INFO - root - 2017-12-05 09:38:39.246272: step 13360, loss = 1.83, batch loss = 1.77 (36.7 examples/sec; 0.218 sec/batch; 19h:20m:27s remains)
INFO - root - 2017-12-05 09:38:41.439461: step 13370, loss = 1.69, batch loss = 1.63 (37.4 examples/sec; 0.214 sec/batch; 18h:57m:06s remains)
INFO - root - 2017-12-05 09:38:43.621965: step 13380, loss = 1.39, batch loss = 1.33 (36.1 examples/sec; 0.222 sec/batch; 19h:39m:00s remains)
INFO - root - 2017-12-05 09:38:45.785112: step 13390, loss = 1.08, batch loss = 1.02 (37.4 examples/sec; 0.214 sec/batch; 18h:56m:41s remains)
INFO - root - 2017-12-05 09:38:47.958161: step 13400, loss = 1.29, batch loss = 1.23 (37.0 examples/sec; 0.216 sec/batch; 19h:09m:07s remains)
2017-12-05 09:38:48.254400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3988295 -4.3973188 -4.3966861 -4.3973703 -4.399025 -4.4000258 -4.3999605 -4.3996415 -4.3997903 -4.4003277 -4.4008417 -4.4011469 -4.4014616 -4.401751 -4.4019442][-4.3886037 -4.3839889 -4.3828092 -4.3854828 -4.3900347 -4.3929729 -4.3931708 -4.3924885 -4.3930173 -4.3949642 -4.3973727 -4.3990054 -4.4003954 -4.4014268 -4.4020805][-4.36469 -4.3537207 -4.3511729 -4.3568268 -4.3665433 -4.3730054 -4.37415 -4.3729482 -4.3745637 -4.3797436 -4.3863177 -4.3917184 -4.3961768 -4.3993726 -4.4013929][-4.3209715 -4.2986541 -4.2931976 -4.3032556 -4.3207984 -4.3327041 -4.33543 -4.3336296 -4.3376608 -4.3493519 -4.3640852 -4.3770728 -4.3877044 -4.395154 -4.39971][-4.2528715 -4.2159219 -4.2073522 -4.22336 -4.2508416 -4.2699647 -4.2745209 -4.2726293 -4.2820945 -4.3045473 -4.3321881 -4.356916 -4.3762107 -4.3894634 -4.3971591][-4.1629562 -4.1111765 -4.1005111 -4.1231933 -4.1612344 -4.1884909 -4.1953683 -4.1962652 -4.21554 -4.2543073 -4.2995591 -4.3381882 -4.3662853 -4.3845735 -4.3946257][-4.0610704 -3.9997225 -3.9903338 -4.0189128 -4.0646939 -4.0993485 -4.1109114 -4.1185379 -4.1517615 -4.21169 -4.2768831 -4.3283076 -4.3623724 -4.3826842 -4.39329][-3.9642866 -3.9037986 -3.9009209 -3.9339328 -3.9832962 -4.0240917 -4.0446043 -4.0641618 -4.1138983 -4.1932817 -4.2733412 -4.33136 -4.3661685 -4.3848863 -4.3939877][-3.8937733 -3.8404555 -3.8463097 -3.8817449 -3.9334309 -3.9829717 -4.0187497 -4.0554514 -4.1206355 -4.2102323 -4.2916789 -4.3457117 -4.3752203 -4.3896475 -4.3961811][-3.8655577 -3.8210387 -3.8319945 -3.8672881 -3.9216847 -3.9834344 -4.0391884 -4.095264 -4.1684446 -4.2522383 -4.321012 -4.363327 -4.3846097 -4.3943491 -4.3985248][-3.885638 -3.8461173 -3.8551416 -3.8867176 -3.944129 -4.0185194 -4.0930524 -4.1626563 -4.2331972 -4.2996855 -4.3489742 -4.3776932 -4.3912663 -4.39733 -4.399827][-3.9457037 -3.906672 -3.9086995 -3.9344382 -3.991601 -4.0724769 -4.1565261 -4.2300329 -4.2908411 -4.3382964 -4.3702779 -4.3878679 -4.39573 -4.3989692 -4.4002223][-4.0298691 -3.9893398 -3.9813287 -3.9989686 -4.05084 -4.1282306 -4.2099748 -4.2789822 -4.3291345 -4.3627043 -4.3833966 -4.3940921 -4.3984656 -4.3999138 -4.40043][-4.1151252 -4.0715976 -4.0523963 -4.0590119 -4.099606 -4.1654778 -4.2381 -4.3007283 -4.3447294 -4.3723307 -4.3885159 -4.3968182 -4.3999496 -4.4006143 -4.4007978][-4.1819878 -4.1329927 -4.1013522 -4.0937052 -4.1191373 -4.1710496 -4.2351484 -4.2961011 -4.341146 -4.3700104 -4.3874483 -4.39692 -4.4004035 -4.4010253 -4.401124]]...]
INFO - root - 2017-12-05 09:38:50.432539: step 13410, loss = 1.19, batch loss = 1.13 (35.7 examples/sec; 0.224 sec/batch; 19h:51m:25s remains)
INFO - root - 2017-12-05 09:38:52.632556: step 13420, loss = 1.58, batch loss = 1.52 (36.8 examples/sec; 0.217 sec/batch; 19h:15m:02s remains)
INFO - root - 2017-12-05 09:38:54.856406: step 13430, loss = 1.19, batch loss = 1.13 (36.1 examples/sec; 0.222 sec/batch; 19h:39m:17s remains)
INFO - root - 2017-12-05 09:38:57.018520: step 13440, loss = 1.69, batch loss = 1.64 (37.8 examples/sec; 0.212 sec/batch; 18h:45m:29s remains)
INFO - root - 2017-12-05 09:38:59.244750: step 13450, loss = 1.28, batch loss = 1.23 (35.9 examples/sec; 0.223 sec/batch; 19h:45m:38s remains)
INFO - root - 2017-12-05 09:39:01.422049: step 13460, loss = 1.37, batch loss = 1.31 (36.8 examples/sec; 0.217 sec/batch; 19h:15m:53s remains)
INFO - root - 2017-12-05 09:39:03.618092: step 13470, loss = 1.42, batch loss = 1.36 (37.7 examples/sec; 0.212 sec/batch; 18h:46m:49s remains)
INFO - root - 2017-12-05 09:39:05.811774: step 13480, loss = 1.24, batch loss = 1.18 (36.9 examples/sec; 0.217 sec/batch; 19h:11m:39s remains)
INFO - root - 2017-12-05 09:39:07.980627: step 13490, loss = 1.57, batch loss = 1.51 (36.6 examples/sec; 0.218 sec/batch; 19h:21m:17s remains)
INFO - root - 2017-12-05 09:39:10.189289: step 13500, loss = 1.39, batch loss = 1.33 (37.5 examples/sec; 0.213 sec/batch; 18h:54m:23s remains)
2017-12-05 09:39:10.454291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4029012 -4.4036727 -4.4042697 -4.4044518 -4.4038825 -4.4024277 -4.4004045 -4.3985868 -4.39782 -4.3982334 -4.3992872 -4.4007583 -4.4021683 -4.4029956 -4.4033213][-4.401268 -4.4024777 -4.4034581 -4.4037933 -4.4028358 -4.400393 -4.3973126 -4.3947558 -4.3938012 -4.3945289 -4.3963022 -4.3985028 -4.4006438 -4.4021211 -4.4029164][-4.3998427 -4.401351 -4.4023376 -4.4022884 -4.4006004 -4.3973417 -4.3936658 -4.3908043 -4.3898592 -4.3908448 -4.3932285 -4.3960719 -4.3987322 -4.4007111 -4.402019][-4.3992581 -4.4009938 -4.4017382 -4.4007497 -4.3979793 -4.3939586 -4.3899221 -4.3868423 -4.3860345 -4.3874488 -4.3904119 -4.3939586 -4.3971367 -4.3994937 -4.4011726][-4.399117 -4.4012012 -4.4018388 -4.400034 -4.3962159 -4.3915625 -4.3873849 -4.3842759 -4.3835998 -4.3853559 -4.3889418 -4.393074 -4.396769 -4.399488 -4.4012394][-4.3987026 -4.4011469 -4.4020734 -4.4001431 -4.3958268 -4.3909264 -4.3870087 -4.3843975 -4.3839107 -4.3858061 -4.3894663 -4.3936739 -4.3975596 -4.4004493 -4.4020944][-4.3975954 -4.4001989 -4.4015102 -4.40012 -4.3961334 -4.3917055 -4.3884463 -4.3866467 -4.3866005 -4.3886824 -4.39193 -4.395381 -4.3987179 -4.4013252 -4.4028654][-4.3965087 -4.39889 -4.4002261 -4.3993158 -4.396338 -4.3931079 -4.39105 -4.3902683 -4.3908796 -4.3929834 -4.3953266 -4.397542 -4.3997273 -4.401546 -4.4027915][-4.3961143 -4.3978992 -4.3990068 -4.3984184 -4.3965273 -4.3947916 -4.394196 -4.3944588 -4.3955464 -4.3971624 -4.3983989 -4.3992443 -4.399941 -4.4006505 -4.401494][-4.3959823 -4.3971081 -4.3980074 -4.3979273 -4.3972459 -4.3968368 -4.3971748 -4.397954 -4.3990607 -4.4000449 -4.4004335 -4.4002614 -4.3996181 -4.3992081 -4.3997054][-4.3961277 -4.3967066 -4.3975759 -4.3981395 -4.3985505 -4.3991361 -4.3999214 -4.4007425 -4.4013762 -4.4016209 -4.4015269 -4.4008636 -4.3996062 -4.3987489 -4.3991203][-4.397336 -4.3977075 -4.3984442 -4.3992586 -4.4001155 -4.4010372 -4.4019456 -4.4025855 -4.4027133 -4.4024682 -4.4021525 -4.4015613 -4.4004726 -4.3998151 -4.4001932][-4.3995819 -4.3998752 -4.4003735 -4.400939 -4.4015746 -4.4022503 -4.4029746 -4.4034777 -4.4034948 -4.4031234 -4.4027777 -4.4024305 -4.401792 -4.4015174 -4.4019427][-4.4016356 -4.4017525 -4.40203 -4.4022956 -4.402575 -4.4029183 -4.4033837 -4.4037848 -4.4038162 -4.4034739 -4.4032187 -4.4031639 -4.4029155 -4.4028335 -4.4031906][-4.4028978 -4.4029036 -4.4029946 -4.4030528 -4.4030738 -4.4031057 -4.4032679 -4.4034753 -4.4034166 -4.403091 -4.4030108 -4.4031997 -4.4032354 -4.4031849 -4.4034061]]...]
INFO - root - 2017-12-05 09:39:12.643698: step 13510, loss = 1.25, batch loss = 1.19 (36.2 examples/sec; 0.221 sec/batch; 19h:35m:22s remains)
INFO - root - 2017-12-05 09:39:14.814701: step 13520, loss = 1.44, batch loss = 1.39 (36.9 examples/sec; 0.217 sec/batch; 19h:11m:35s remains)
INFO - root - 2017-12-05 09:39:17.005018: step 13530, loss = 1.47, batch loss = 1.41 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:40s remains)
INFO - root - 2017-12-05 09:39:19.215484: step 13540, loss = 1.10, batch loss = 1.04 (36.9 examples/sec; 0.217 sec/batch; 19h:11m:16s remains)
INFO - root - 2017-12-05 09:39:21.424920: step 13550, loss = 1.78, batch loss = 1.72 (37.3 examples/sec; 0.215 sec/batch; 19h:01m:10s remains)
INFO - root - 2017-12-05 09:39:23.594327: step 13560, loss = 1.26, batch loss = 1.20 (37.6 examples/sec; 0.213 sec/batch; 18h:52m:22s remains)
INFO - root - 2017-12-05 09:39:25.806783: step 13570, loss = 2.14, batch loss = 2.08 (36.7 examples/sec; 0.218 sec/batch; 19h:17m:17s remains)
INFO - root - 2017-12-05 09:39:28.006594: step 13580, loss = 1.40, batch loss = 1.34 (36.6 examples/sec; 0.218 sec/batch; 19h:20m:16s remains)
INFO - root - 2017-12-05 09:39:30.183546: step 13590, loss = 1.18, batch loss = 1.12 (35.1 examples/sec; 0.228 sec/batch; 20h:12m:34s remains)
INFO - root - 2017-12-05 09:39:32.381573: step 13600, loss = 1.64, batch loss = 1.58 (35.6 examples/sec; 0.225 sec/batch; 19h:53m:38s remains)
2017-12-05 09:39:32.668394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4056807 -4.4039989 -4.4005818 -4.3948455 -4.3893394 -4.3863888 -4.3872547 -4.3908134 -4.395371 -4.3997173 -4.4031358 -4.4051847 -4.4061637 -4.4064884 -4.4065342][-4.4042225 -4.3978539 -4.3820248 -4.3559384 -4.3301425 -4.3166108 -4.3204908 -4.3370123 -4.3583574 -4.378335 -4.3931522 -4.4015827 -4.4051776 -4.4062672 -4.4065175][-4.4000521 -4.3819036 -4.3348136 -4.2579646 -4.1820045 -4.1427212 -4.1554604 -4.206007 -4.2692556 -4.3267736 -4.3677258 -4.391118 -4.4014063 -4.4044781 -4.4050922][-4.3852735 -4.3466382 -4.2456326 -4.0820575 -3.9179425 -3.8310604 -3.8565774 -3.9664495 -4.1034055 -4.2262373 -4.3137383 -4.3644252 -4.3874388 -4.39455 -4.3961706][-4.3348842 -4.2768383 -4.1097779 -3.8318 -3.5380533 -3.3657146 -3.3863573 -3.5634563 -3.7997041 -4.0164661 -4.1746182 -4.2666783 -4.3058262 -4.3154659 -4.319427][-4.2005115 -4.1419477 -3.9143314 -3.5098052 -3.0519452 -2.7438097 -2.7148848 -2.9374189 -3.2781248 -3.6062212 -3.8459022 -3.9743426 -4.0131569 -4.011591 -4.0195165][-3.9342713 -3.9049091 -3.6402998 -3.1210127 -2.4903488 -2.009979 -1.8819776 -2.1084192 -2.5275512 -2.9505594 -3.250309 -3.3797958 -3.3775933 -3.3401771 -3.3571248][-3.5527244 -3.5729549 -3.3029597 -2.7124677 -1.9534283 -1.3172569 -1.0742295 -1.2627876 -1.7095923 -2.1779299 -2.48944 -2.5727534 -2.4891195 -2.3949997 -2.4246521][-3.1965787 -3.2712421 -3.0316978 -2.4473662 -1.6609726 -0.95434403 -0.62324858 -0.74338269 -1.153075 -1.6003239 -1.8746336 -1.8872824 -1.7163444 -1.5627584 -1.598984][-3.0694294 -3.1840329 -3.003588 -2.5085397 -1.8192568 -1.1692891 -0.81871819 -0.86381531 -1.1804883 -1.5419819 -1.7437847 -1.698025 -1.485523 -1.3043509 -1.3309629][-3.232161 -3.3558841 -3.2424107 -2.8838205 -2.3680224 -1.8629942 -1.5620668 -1.5569425 -1.765789 -2.0167453 -2.143343 -2.0724244 -1.8768651 -1.7108409 -1.7196569][-3.5671751 -3.6693981 -3.6092095 -3.3859997 -3.0551858 -2.7224672 -2.5096629 -2.4845924 -2.6031919 -2.7529988 -2.8207517 -2.756216 -2.614135 -2.4924207 -2.4881759][-3.9059644 -3.9742768 -3.9487407 -3.8298566 -3.6478362 -3.4606895 -3.3338733 -3.3073494 -3.3631692 -3.4404626 -3.4736502 -3.4320419 -3.3472028 -3.2730229 -3.2617311][-4.1518788 -4.189014 -4.1824245 -4.1304855 -4.0466022 -3.9571869 -3.8927422 -3.8726971 -3.8922386 -3.9242749 -3.9384117 -3.9188275 -3.8779712 -3.8410687 -3.8295362][-4.2920213 -4.3084645 -4.3089094 -4.2912412 -4.2597733 -4.2242389 -4.1964326 -4.1847696 -4.1886187 -4.1986556 -4.2035584 -4.1964011 -4.1809359 -4.1661081 -4.1570888]]...]
INFO - root - 2017-12-05 09:39:34.849175: step 13610, loss = 1.39, batch loss = 1.33 (35.3 examples/sec; 0.227 sec/batch; 20h:04m:58s remains)
INFO - root - 2017-12-05 09:39:37.032141: step 13620, loss = 1.38, batch loss = 1.32 (35.9 examples/sec; 0.223 sec/batch; 19h:44m:45s remains)
INFO - root - 2017-12-05 09:39:39.257668: step 13630, loss = 1.45, batch loss = 1.39 (35.9 examples/sec; 0.223 sec/batch; 19h:45m:00s remains)
INFO - root - 2017-12-05 09:39:41.454611: step 13640, loss = 1.41, batch loss = 1.35 (35.7 examples/sec; 0.224 sec/batch; 19h:52m:24s remains)
INFO - root - 2017-12-05 09:39:43.673556: step 13650, loss = 1.04, batch loss = 0.98 (36.8 examples/sec; 0.218 sec/batch; 19h:16m:38s remains)
INFO - root - 2017-12-05 09:39:45.873809: step 13660, loss = 1.28, batch loss = 1.23 (36.6 examples/sec; 0.219 sec/batch; 19h:22m:26s remains)
INFO - root - 2017-12-05 09:39:48.093898: step 13670, loss = 1.35, batch loss = 1.29 (35.7 examples/sec; 0.224 sec/batch; 19h:49m:43s remains)
INFO - root - 2017-12-05 09:39:50.253532: step 13680, loss = 1.20, batch loss = 1.14 (38.0 examples/sec; 0.210 sec/batch; 18h:37m:23s remains)
INFO - root - 2017-12-05 09:39:52.476726: step 13690, loss = 1.15, batch loss = 1.09 (37.2 examples/sec; 0.215 sec/batch; 19h:03m:32s remains)
INFO - root - 2017-12-05 09:39:54.671526: step 13700, loss = 1.40, batch loss = 1.34 (37.1 examples/sec; 0.215 sec/batch; 19h:04m:48s remains)
2017-12-05 09:39:54.940462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0232491 -3.9332333 -3.8249531 -3.7092459 -3.606694 -3.5444872 -3.5363569 -3.5805316 -3.6708393 -3.7956624 -3.9322896 -4.0601072 -4.1745567 -4.2714643 -4.3393068][-3.6703715 -3.5008583 -3.3014126 -3.0925148 -2.9118738 -2.8032079 -2.7837381 -2.85071 -3.0019414 -3.2254279 -3.4820998 -3.7315874 -3.9584689 -4.1486211 -4.280982][-3.3079305 -3.0490189 -2.7488837 -2.4414244 -2.1831083 -2.0219595 -1.9718583 -2.0371239 -2.2326746 -2.5551052 -2.9463158 -3.3427467 -3.7089953 -4.0129695 -4.2212772][-3.0213928 -2.6847615 -2.295033 -1.9009838 -1.5694962 -1.3420777 -1.2315683 -1.2610478 -1.476655 -1.8849604 -2.4077635 -2.9558907 -3.466207 -3.8856318 -4.1687231][-2.798068 -2.4055026 -1.946779 -1.4806197 -1.0724416 -0.75291395 -0.54286671 -0.5069375 -0.73390245 -1.2285903 -1.8873746 -2.5917997 -3.2465289 -3.7753391 -4.1241908][-2.5922706 -2.17217 -1.6696618 -1.1384516 -0.638324 -0.19540358 0.14811134 0.26495409 0.014097214 -0.58747911 -1.3984168 -2.2639315 -3.0590491 -3.6873674 -4.0907683][-2.4174924 -1.9987168 -1.4756525 -0.88404822 -0.2825942 0.29678774 0.77568674 0.95863819 0.66368008 -0.052843571 -1.0071583 -2.0139928 -2.9259677 -3.6323786 -4.0749073][-2.3350592 -1.930603 -1.3966258 -0.75286627 -0.065373421 0.621161 1.1899672 1.4037375 1.0694246 0.27385426 -0.77215481 -1.8671246 -2.8509436 -3.6043932 -4.069149][-2.3731124 -1.976568 -1.4309092 -0.749284 -0.0049300194 0.739316 1.3387117 1.5522141 1.204958 0.38732862 -0.68218637 -1.8009191 -2.8073573 -3.5792713 -4.0572577][-2.5014558 -2.0986156 -1.5388958 -0.83854389 -0.077142239 0.66875219 1.2477212 1.44349 1.1076841 0.32000351 -0.71322513 -1.7987123 -2.7853689 -3.5523236 -4.0370483][-2.6959081 -2.2849457 -1.7233596 -1.0359247 -0.30634689 0.38642597 0.90784454 1.0780516 0.7781024 0.067754269 -0.87707996 -1.8806159 -2.8075354 -3.5434446 -4.02314][-2.9537759 -2.5555406 -2.0246251 -1.3964672 -0.75186872 -0.16004133 0.27766085 0.42457104 0.18490028 -0.40795374 -1.2200918 -2.0991313 -2.9243999 -3.591572 -4.0385742][-3.2687347 -2.9152181 -2.4560025 -1.9319103 -1.4137247 -0.95332909 -0.6131115 -0.48654437 -0.65346432 -1.1091869 -1.7592726 -2.4770789 -3.1587763 -3.7147069 -4.0918016][-3.5893793 -3.3024511 -2.9409747 -2.5439043 -2.1659648 -1.8375275 -1.5896537 -1.4819067 -1.5819459 -1.9052796 -2.389538 -2.9326959 -3.4508986 -3.8744788 -4.1626253][-3.8636205 -3.6513543 -3.3936129 -3.1206388 -2.8681839 -2.6509714 -2.4791212 -2.3921189 -2.4444709 -2.6565266 -2.9895215 -3.3667777 -3.7275956 -4.023829 -4.2279358]]...]
INFO - root - 2017-12-05 09:39:57.130714: step 13710, loss = 1.52, batch loss = 1.47 (36.2 examples/sec; 0.221 sec/batch; 19h:34m:07s remains)
INFO - root - 2017-12-05 09:39:59.322926: step 13720, loss = 2.49, batch loss = 2.43 (37.4 examples/sec; 0.214 sec/batch; 18h:55m:41s remains)
INFO - root - 2017-12-05 09:40:01.512286: step 13730, loss = 1.64, batch loss = 1.58 (35.5 examples/sec; 0.225 sec/batch; 19h:56m:14s remains)
INFO - root - 2017-12-05 09:40:03.714700: step 13740, loss = 1.36, batch loss = 1.30 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:02s remains)
INFO - root - 2017-12-05 09:40:05.920572: step 13750, loss = 1.58, batch loss = 1.52 (37.1 examples/sec; 0.216 sec/batch; 19h:05m:33s remains)
INFO - root - 2017-12-05 09:40:08.129190: step 13760, loss = 1.65, batch loss = 1.59 (36.8 examples/sec; 0.217 sec/batch; 19h:15m:04s remains)
INFO - root - 2017-12-05 09:40:10.359955: step 13770, loss = 1.38, batch loss = 1.32 (36.2 examples/sec; 0.221 sec/batch; 19h:33m:57s remains)
INFO - root - 2017-12-05 09:40:12.585820: step 13780, loss = 1.53, batch loss = 1.47 (36.7 examples/sec; 0.218 sec/batch; 19h:18m:48s remains)
INFO - root - 2017-12-05 09:40:14.776912: step 13790, loss = 1.23, batch loss = 1.17 (35.6 examples/sec; 0.225 sec/batch; 19h:55m:08s remains)
INFO - root - 2017-12-05 09:40:16.995625: step 13800, loss = 1.49, batch loss = 1.43 (37.3 examples/sec; 0.215 sec/batch; 19h:00m:03s remains)
2017-12-05 09:40:17.305766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3399081 -4.3366675 -4.3459821 -4.3616366 -4.3745384 -4.3811936 -4.380775 -4.3738523 -4.3610625 -4.3492684 -4.347384 -4.3576417 -4.3730927 -4.3858771 -4.3919196][-4.2940946 -4.2809367 -4.291636 -4.3172846 -4.338758 -4.3442135 -4.3303771 -4.2964649 -4.2501912 -4.2145324 -4.2140975 -4.2547469 -4.3115058 -4.3566484 -4.3802323][-4.2478089 -4.2263341 -4.2364016 -4.2635012 -4.2787333 -4.2600517 -4.199791 -4.0984168 -3.9833009 -3.9086823 -3.9252217 -4.0365963 -4.1810102 -4.2945609 -4.3566604][-4.2111998 -4.1855016 -4.18884 -4.1975145 -4.1732826 -4.0876069 -3.9337373 -3.7238855 -3.5181203 -3.4100296 -3.4739954 -3.7000561 -3.9763134 -4.1923223 -4.3138576][-4.1866541 -4.1542435 -4.1362667 -4.0957756 -3.9865017 -3.7819695 -3.4885352 -3.1471865 -2.8607771 -2.757921 -2.9176724 -3.2934885 -3.7244258 -4.0586438 -4.2504382][-4.179059 -4.1289306 -4.0657673 -3.9389572 -3.6998475 -3.3338475 -2.8746495 -2.4095564 -2.0870204 -2.0530088 -2.3595235 -2.9032378 -3.4826858 -3.9245708 -4.1812978][-4.1923766 -4.1096621 -3.9769738 -3.7377617 -3.3489103 -2.8170938 -2.2149096 -1.6781015 -1.3831835 -1.4700232 -1.9405143 -2.6295326 -3.3146467 -3.8279557 -4.1298943][-4.2033896 -4.0778985 -3.8682961 -3.5212417 -3.0098908 -2.3715582 -1.7151091 -1.2002451 -0.99856043 -1.2164867 -1.805727 -2.5624628 -3.2737584 -3.7996502 -4.1124196][-4.1899033 -4.024107 -3.7544305 -3.3391128 -2.7785027 -2.14266 -1.5519233 -1.1559541 -1.0843403 -1.3961236 -2.0091381 -2.7263572 -3.3726707 -3.8455534 -4.130137][-4.1527677 -3.9651687 -3.6739933 -3.2550168 -2.7338462 -2.1934502 -1.7463076 -1.5089126 -1.558357 -1.9091611 -2.4631157 -3.0599606 -3.5755272 -3.9488173 -4.1756134][-4.1154628 -3.9377341 -3.6755145 -3.3175673 -2.898695 -2.496913 -2.2028673 -2.1003304 -2.229692 -2.5692828 -3.0193319 -3.4625587 -3.8260283 -4.0828609 -4.238236][-4.1040597 -3.9668283 -3.7730033 -3.5142262 -3.2174053 -2.9418049 -2.7563863 -2.7264788 -2.8792064 -3.1767898 -3.5198202 -3.8230932 -4.0542736 -4.2088895 -4.2989917][-4.1285787 -4.0412974 -3.9211164 -3.7547817 -3.5542421 -3.3559577 -3.2139809 -3.19183 -3.3269973 -3.5829124 -3.8550377 -4.068768 -4.2123327 -4.2980647 -4.3426232][-4.1808882 -4.1306067 -4.0591736 -3.9478977 -3.7940094 -3.6170354 -3.4636519 -3.4076862 -3.5110488 -3.7464931 -3.9978132 -4.1822586 -4.2893515 -4.3423886 -4.3646245][-4.2431593 -4.21181 -4.1571708 -4.0571389 -3.89962 -3.6987858 -3.5062735 -3.4127491 -3.4968119 -3.7346427 -3.9991643 -4.1939645 -4.3006058 -4.3486385 -4.3678064]]...]
INFO - root - 2017-12-05 09:40:19.480074: step 13810, loss = 1.34, batch loss = 1.28 (37.7 examples/sec; 0.212 sec/batch; 18h:48m:17s remains)
INFO - root - 2017-12-05 09:40:21.672514: step 13820, loss = 1.55, batch loss = 1.49 (35.9 examples/sec; 0.223 sec/batch; 19h:42m:16s remains)
INFO - root - 2017-12-05 09:40:23.851785: step 13830, loss = 1.52, batch loss = 1.46 (35.6 examples/sec; 0.225 sec/batch; 19h:53m:19s remains)
INFO - root - 2017-12-05 09:40:26.061105: step 13840, loss = 1.56, batch loss = 1.50 (36.5 examples/sec; 0.219 sec/batch; 19h:25m:30s remains)
INFO - root - 2017-12-05 09:40:28.273273: step 13850, loss = 1.86, batch loss = 1.80 (36.0 examples/sec; 0.222 sec/batch; 19h:41m:03s remains)
INFO - root - 2017-12-05 09:40:30.455096: step 13860, loss = 1.34, batch loss = 1.28 (36.4 examples/sec; 0.220 sec/batch; 19h:27m:07s remains)
INFO - root - 2017-12-05 09:40:32.643999: step 13870, loss = 1.37, batch loss = 1.31 (36.5 examples/sec; 0.219 sec/batch; 19h:25m:09s remains)
INFO - root - 2017-12-05 09:40:34.808296: step 13880, loss = 1.40, batch loss = 1.34 (36.8 examples/sec; 0.217 sec/batch; 19h:14m:55s remains)
INFO - root - 2017-12-05 09:40:36.980917: step 13890, loss = 1.21, batch loss = 1.15 (37.9 examples/sec; 0.211 sec/batch; 18h:40m:09s remains)
INFO - root - 2017-12-05 09:40:39.191520: step 13900, loss = 1.53, batch loss = 1.47 (35.5 examples/sec; 0.225 sec/batch; 19h:56m:52s remains)
2017-12-05 09:40:39.499695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0028334 -1.4761326 -1.2458189 -1.3672779 -1.7806954 -2.3261988 -2.8022585 -3.0316556 -2.910497 -2.4742181 -1.9059179 -1.4481094 -1.3175673 -1.5559785 -2.0756588][-1.6028426 -1.1164935 -0.93766856 -1.099838 -1.5327349 -2.0700424 -2.518162 -2.7177289 -2.5760922 -2.1335835 -1.5689445 -1.1195819 -0.98652792 -1.2077563 -1.6955409][-0.90884686 -0.48852468 -0.37193537 -0.57091045 -1.0076141 -1.5217841 -1.9381282 -2.1205347 -1.9878438 -1.577481 -1.0505004 -0.6243763 -0.47927737 -0.65536427 -1.0767837][0.024057388 0.36216688 0.40886021 0.17361307 -0.25708342 -0.7427299 -1.1326559 -1.3114846 -1.2071347 -0.84934592 -0.37441683 0.027181625 0.20060873 0.097426414 -0.23168135][1.0185027 1.2691617 1.2390542 0.9620924 0.53608179 0.075767994 -0.29427576 -0.47812796 -0.40990686 -0.11081123 0.30992222 0.69270277 0.90914679 0.89864159 0.6773901][1.850481 2.0184207 1.9148316 1.5998921 1.1753864 0.7368784 0.38533545 0.19551516 0.23084497 0.47896528 0.8562088 1.2284617 1.4872556 1.5616593 1.43572][2.3690605 2.4645586 2.294488 1.9468932 1.5244288 1.1106129 0.78590584 0.60595036 0.62501144 0.83588457 1.1769257 1.5344539 1.8143525 1.9388275 1.8694682][2.5382595 2.5773082 2.3528228 1.9806976 1.5649977 1.1836538 0.89808083 0.7477479 0.77036762 0.95893717 1.2665305 1.5958238 1.8619795 1.9879742 1.9269776][2.4676852 2.4665384 2.1988778 1.8053789 1.3964219 1.0496955 0.80740452 0.69354725 0.72726822 0.89841557 1.1689539 1.4576035 1.6884842 1.7877746 1.7105989][2.3237615 2.2986007 1.995409 1.5812006 1.1709747 0.84503508 0.63189125 0.54246426 0.57844019 0.72584391 0.95199203 1.19273 1.3841605 1.4590693 1.3746624][2.2067585 2.161809 1.8250027 1.3922682 0.97201347 0.64615107 0.43700552 0.34687328 0.3649106 0.47508669 0.65016174 0.84291077 1.0014038 1.0691957 1.0030093][2.129714 2.05836 1.6806283 1.2179174 0.76966476 0.41906214 0.18809891 0.074054718 0.059767246 0.13015509 0.25920057 0.41335487 0.552721 0.63181639 0.60530519][2.0462613 1.9344978 1.500555 0.99125147 0.49812603 0.10302496 -0.16895151 -0.32203627 -0.37401533 -0.33837795 -0.2407856 -0.10843897 0.023967266 0.12027645 0.14118958][1.8373365 1.6880975 1.2118025 0.66056252 0.12632465 -0.31264114 -0.6273849 -0.82135248 -0.91013861 -0.90421438 -0.82941246 -0.71264768 -0.586298 -0.47647572 -0.41136813][1.4402447 1.2832499 0.80775833 0.24728966 -0.30455208 -0.77012396 -1.1134286 -1.3387969 -1.4604883 -1.4856918 -1.4394155 -1.3478384 -1.238831 -1.127672 -1.0281882]]...]
INFO - root - 2017-12-05 09:40:41.679911: step 13910, loss = 1.13, batch loss = 1.08 (36.9 examples/sec; 0.217 sec/batch; 19h:11m:05s remains)
INFO - root - 2017-12-05 09:40:43.898951: step 13920, loss = 1.55, batch loss = 1.49 (35.9 examples/sec; 0.223 sec/batch; 19h:44m:44s remains)
INFO - root - 2017-12-05 09:40:46.083496: step 13930, loss = 1.84, batch loss = 1.78 (36.5 examples/sec; 0.219 sec/batch; 19h:23m:39s remains)
INFO - root - 2017-12-05 09:40:48.266838: step 13940, loss = 1.54, batch loss = 1.48 (36.6 examples/sec; 0.219 sec/batch; 19h:22m:05s remains)
INFO - root - 2017-12-05 09:40:50.471335: step 13950, loss = 1.54, batch loss = 1.48 (36.0 examples/sec; 0.222 sec/batch; 19h:40m:48s remains)
INFO - root - 2017-12-05 09:40:52.645971: step 13960, loss = 1.17, batch loss = 1.11 (36.5 examples/sec; 0.219 sec/batch; 19h:23m:30s remains)
INFO - root - 2017-12-05 09:40:54.853458: step 13970, loss = 1.32, batch loss = 1.26 (33.9 examples/sec; 0.236 sec/batch; 20h:54m:19s remains)
INFO - root - 2017-12-05 09:40:57.083187: step 13980, loss = 1.54, batch loss = 1.48 (36.7 examples/sec; 0.218 sec/batch; 19h:16m:08s remains)
INFO - root - 2017-12-05 09:40:59.281686: step 13990, loss = 1.41, batch loss = 1.36 (37.8 examples/sec; 0.211 sec/batch; 18h:42m:22s remains)
INFO - root - 2017-12-05 09:41:01.484591: step 14000, loss = 1.41, batch loss = 1.36 (36.3 examples/sec; 0.220 sec/batch; 19h:30m:24s remains)
2017-12-05 09:41:01.755904: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.1556168 4.7252121 6.2826643 7.8114624 9.2094431 10.302073 10.934367 10.984607 10.461523 9.5231285 8.4131041 7.5731192 7.3194904 7.69969 8.442605][3.5234723 5.3462257 7.1216469 8.82183 10.339939 11.506068 12.150335 12.128254 11.47704 10.354209 9.0605783 8.0717573 7.753788 8.191431 9.0717945][4.018362 6.1341639 8.1379595 9.9643145 11.518694 12.647215 13.199539 13.02051 12.159313 10.784964 9.2431135 8.0339146 7.5718708 7.9663143 8.9135818][4.6870031 7.1075344 9.345561 11.294527 12.861574 13.921141 14.366411 14.01841 12.921345 11.251689 9.3867617 7.8504066 7.114213 7.33897 8.2572079][5.1259136 7.7753124 10.198566 12.249851 13.828833 14.832941 15.194067 14.727341 13.451735 11.525561 9.3640194 7.4979019 6.4417629 6.4090548 7.1930857][5.086832 7.8054352 10.283539 12.357437 13.934355 14.915567 15.26112 14.779583 13.459309 11.41883 9.0713081 6.9530725 5.6049423 5.29288 5.8889608][4.4416342 7.0418634 9.4258852 11.426519 12.957392 13.930185 14.318007 13.917767 12.687531 10.708927 8.3584337 6.1416206 4.6012964 4.0621529 4.4741211][3.2473063 5.5514879 7.689641 9.5155706 10.962251 11.941566 12.421824 12.200176 11.193609 9.438303 7.2508039 5.0934591 3.4796386 2.778111 3.0205073][1.7239051 3.6148815 5.4014368 6.9800081 8.3200741 9.3248854 9.9375715 9.9495392 9.2391329 7.7992439 5.8777857 3.8900566 2.2997928 1.494113 1.5772567][0.12762499 1.5638185 2.9571934 4.2589922 5.4754391 6.5079994 7.2600975 7.5155783 7.1229134 6.0354538 4.4329176 2.6744533 1.1672807 0.29195547 0.20741415][-1.3445246 -0.33443594 0.6884923 1.7180347 2.7898388 3.8084106 4.6460056 5.0919809 4.9870872 4.252574 3.0003009 1.5139461 0.14045477 -0.76436377 -1.0163028][-2.5619702 -1.9169366 -1.2198846 -0.45140171 0.43991423 1.371314 2.2048659 2.745316 2.8446836 2.4160604 1.5147161 0.33899117 -0.83191609 -1.6908388 -2.0597007][-3.4371576 -3.0733728 -2.6441514 -2.1179223 -1.4379499 -0.66412973 0.074890137 0.60952091 0.81717157 0.61340284 0.019469261 -0.83462644 -1.7410824 -2.4676361 -2.8710632][-3.9643595 -3.7869678 -3.5495386 -3.2242494 -2.7637663 -2.202709 -1.6370137 -1.1987767 -0.97650719 -1.0420203 -1.3891797 -1.9412031 -2.5586956 -3.0901895 -3.4420295][-4.2343979 -4.1622634 -4.0496063 -3.8746953 -3.6087108 -3.2680173 -2.907783 -2.6163614 -2.4437294 -2.4444494 -2.6134174 -2.9130158 -3.2634394 -3.5805879 -3.819665]]...]
INFO - root - 2017-12-05 09:41:03.958696: step 14010, loss = 1.53, batch loss = 1.47 (36.3 examples/sec; 0.220 sec/batch; 19h:28m:29s remains)
INFO - root - 2017-12-05 09:41:06.138097: step 14020, loss = 1.30, batch loss = 1.24 (36.6 examples/sec; 0.218 sec/batch; 19h:19m:16s remains)
INFO - root - 2017-12-05 09:41:08.335570: step 14030, loss = 1.09, batch loss = 1.03 (36.4 examples/sec; 0.220 sec/batch; 19h:26m:32s remains)
INFO - root - 2017-12-05 09:41:10.575989: step 14040, loss = 1.48, batch loss = 1.42 (35.5 examples/sec; 0.225 sec/batch; 19h:54m:35s remains)
INFO - root - 2017-12-05 09:41:12.784234: step 14050, loss = 1.42, batch loss = 1.36 (36.5 examples/sec; 0.219 sec/batch; 19h:23m:39s remains)
INFO - root - 2017-12-05 09:41:14.995111: step 14060, loss = 1.55, batch loss = 1.50 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:32s remains)
INFO - root - 2017-12-05 09:41:17.203946: step 14070, loss = 1.16, batch loss = 1.10 (36.6 examples/sec; 0.218 sec/batch; 19h:19m:17s remains)
INFO - root - 2017-12-05 09:41:19.375863: step 14080, loss = 1.56, batch loss = 1.51 (37.0 examples/sec; 0.216 sec/batch; 19h:06m:49s remains)
INFO - root - 2017-12-05 09:41:21.575912: step 14090, loss = 1.14, batch loss = 1.08 (37.2 examples/sec; 0.215 sec/batch; 19h:00m:45s remains)
INFO - root - 2017-12-05 09:41:23.774547: step 14100, loss = 1.29, batch loss = 1.23 (36.7 examples/sec; 0.218 sec/batch; 19h:18m:14s remains)
2017-12-05 09:41:24.053316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5248713 -3.5109503 -3.50324 -3.5168576 -3.5577729 -3.6244109 -3.6994095 -3.7750597 -3.848156 -3.9191279 -3.9854963 -4.0431404 -4.0879016 -4.1174645 -4.1345868][-2.6185088 -2.5937672 -2.5878863 -2.61636 -2.6841817 -2.7836666 -2.892416 -3.0051284 -3.1209855 -3.2444472 -3.3755374 -3.4997334 -3.6041939 -3.6827121 -3.7400541][-1.6061957 -1.5613232 -1.5493071 -1.582304 -1.6587391 -1.7630987 -1.8751538 -2.0007696 -2.1494584 -2.333149 -2.5500703 -2.7715631 -2.9678419 -3.1251278 -3.2514982][-0.75401568 -0.69523716 -0.68034458 -0.70678926 -0.76136136 -0.82367945 -0.89159274 -0.99843407 -1.1654375 -1.4097271 -1.7237906 -2.0551753 -2.3515127 -2.5936456 -2.7994337][-0.25821924 -0.19400549 -0.17152548 -0.16879034 -0.16062641 -0.13413811 -0.11706257 -0.17575121 -0.34676886 -0.6478622 -1.0555751 -1.487812 -1.8679194 -2.1752031 -2.4411376][-0.20934629 -0.12735176 -0.070299149 0.0016417503 0.11605835 0.26849842 0.40076017 0.41324472 0.25173569 -0.090603828 -0.56722641 -1.0640273 -1.4869599 -1.8196013 -2.1161871][-0.55270362 -0.43085933 -0.29819775 -0.11105299 0.14733839 0.4553647 0.72956991 0.83671522 0.70930767 0.35529757 -0.14998388 -0.6693275 -1.0952718 -1.4203241 -1.7245362][-1.1159832 -0.9381032 -0.69490075 -0.36038113 0.063286781 0.53637743 0.96124744 1.1805034 1.1193342 0.79693413 0.30709982 -0.20242786 -0.61701274 -0.93441772 -1.2515609][-1.6914227 -1.4571762 -1.092916 -0.59997845 -0.010507107 0.61773109 1.1813245 1.5190768 1.5539904 1.3066134 0.8646636 0.37401438 -0.047816277 -0.39186907 -0.75430393][-2.12392 -1.8355098 -1.3524761 -0.71313715 0.014805317 0.76313448 1.4313698 1.8742251 2.01727 1.865983 1.4854693 1.0055785 0.54595566 0.1334939 -0.31252909][-2.3960319 -2.0475729 -1.4512091 -0.69040823 0.1320014 0.94530821 1.6665816 2.1773729 2.4062719 2.341392 2.018117 1.5401783 1.0205445 0.50920868 -0.047671318][-2.5550113 -2.1531966 -1.4810274 -0.65403509 0.20166159 1.0205779 1.7383041 2.2704487 2.5466461 2.5407267 2.2597461 1.7802839 1.1960983 0.57922077 -0.088648796][-2.7094636 -2.2781954 -1.5907199 -0.77371287 0.045578003 0.8071022 1.4642892 1.9638348 2.2402868 2.2638092 2.0130954 1.5384388 0.91560841 0.227777 -0.50536919][-2.9318562 -2.5209064 -1.8951724 -1.1688921 -0.45568776 0.19584513 0.7496829 1.1711307 1.4057565 1.4289517 1.2052941 0.76741362 0.16436291 -0.5152421 -1.2256234][-3.2249982 -2.8839598 -2.3838613 -1.8090889 -1.2475679 -0.737731 -0.30921316 0.014770031 0.19370842 0.20841503 0.023470402 -0.34354687 -0.86500382 -1.4526265 -2.0560269]]...]
INFO - root - 2017-12-05 09:41:26.256976: step 14110, loss = 1.20, batch loss = 1.15 (35.1 examples/sec; 0.228 sec/batch; 20h:10m:38s remains)
INFO - root - 2017-12-05 09:41:28.452384: step 14120, loss = 1.84, batch loss = 1.78 (37.0 examples/sec; 0.216 sec/batch; 19h:06m:09s remains)
INFO - root - 2017-12-05 09:41:30.639503: step 14130, loss = 1.90, batch loss = 1.84 (36.6 examples/sec; 0.218 sec/batch; 19h:19m:09s remains)
INFO - root - 2017-12-05 09:41:32.807014: step 14140, loss = 1.45, batch loss = 1.39 (37.7 examples/sec; 0.212 sec/batch; 18h:44m:28s remains)
INFO - root - 2017-12-05 09:41:34.966121: step 14150, loss = 1.50, batch loss = 1.44 (37.6 examples/sec; 0.213 sec/batch; 18h:49m:51s remains)
INFO - root - 2017-12-05 09:41:37.138808: step 14160, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:27m:27s remains)
INFO - root - 2017-12-05 09:41:39.332687: step 14170, loss = 1.73, batch loss = 1.67 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:28s remains)
INFO - root - 2017-12-05 09:41:41.504990: step 14180, loss = 1.10, batch loss = 1.04 (37.2 examples/sec; 0.215 sec/batch; 19h:00m:20s remains)
INFO - root - 2017-12-05 09:41:43.673982: step 14190, loss = 1.55, batch loss = 1.49 (37.3 examples/sec; 0.214 sec/batch; 18h:57m:00s remains)
INFO - root - 2017-12-05 09:41:45.845840: step 14200, loss = 1.73, batch loss = 1.68 (37.3 examples/sec; 0.214 sec/batch; 18h:57m:23s remains)
2017-12-05 09:41:46.112083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3997636 -4.3949981 -4.3762622 -4.3286772 -4.2377276 -4.0990505 -3.9249959 -3.7482221 -3.6077762 -3.5415254 -3.5662198 -3.6754284 -3.8391759 -4.0116968 -4.1481233][-4.3964677 -4.3853717 -4.3565593 -4.298841 -4.2086134 -4.0916085 -3.9646168 -3.8542824 -3.7854104 -3.7755933 -3.8264804 -3.9247556 -4.0474148 -4.1662397 -4.2570477][-4.38609 -4.3581891 -4.3021493 -4.2133455 -4.1007981 -3.9851863 -3.8935242 -3.8492448 -3.8589287 -3.9149876 -4.0018382 -4.1006508 -4.1950784 -4.27272 -4.3280706][-4.3648977 -4.3077602 -4.2079258 -4.0690327 -3.9138269 -3.7812355 -3.7119522 -3.7236397 -3.8032784 -3.9233577 -4.0547395 -4.172812 -4.2631445 -4.3246179 -4.3632121][-4.3343077 -4.2393174 -4.0853467 -3.8854523 -3.6779132 -3.5187583 -3.4574332 -3.505774 -3.6376333 -3.81343 -3.9931452 -4.1467319 -4.2586865 -4.3298759 -4.3708987][-4.3041439 -4.1736264 -3.9692974 -3.7134118 -3.4581831 -3.2699089 -3.1996202 -3.2579703 -3.4165621 -3.6322515 -3.8589594 -4.0589004 -4.2094331 -4.3071971 -4.3631363][-4.2885771 -4.1396422 -3.907238 -3.6183126 -3.3333688 -3.1204829 -3.0300822 -3.0758262 -3.2343326 -3.4674516 -3.7289095 -3.9707932 -4.1580353 -4.2817516 -4.3530021][-4.2951584 -4.1531444 -3.9265466 -3.6404197 -3.3541632 -3.1309681 -3.0225515 -3.0475278 -3.1904569 -3.4207096 -3.69158 -3.9468765 -4.1445546 -4.2749257 -4.3499002][-4.3182921 -4.2039523 -4.0157061 -3.7689257 -3.5128965 -3.3025513 -3.1899362 -3.1989174 -3.3233819 -3.5337672 -3.7810318 -4.0098391 -4.1827126 -4.2941141 -4.35724][-4.3473282 -4.269556 -4.1360297 -3.9516211 -3.748522 -3.5720775 -3.4725461 -3.4763389 -3.5789905 -3.7506249 -3.9467444 -4.1217523 -4.2483125 -4.3268242 -4.3705177][-4.3723726 -4.3271275 -4.245111 -4.1248951 -3.9835033 -3.8546584 -3.7806802 -3.7848012 -3.8615773 -3.9844217 -4.1188083 -4.2332578 -4.3117433 -4.3583469 -4.3836174][-4.3881626 -4.3661776 -4.322607 -4.2538991 -4.1688046 -4.088974 -4.0420179 -4.0450974 -4.0947733 -4.1714144 -4.2502031 -4.3141413 -4.3563943 -4.3803687 -4.3929162][-4.3957744 -4.3875766 -4.368814 -4.335186 -4.2910376 -4.2481241 -4.2213764 -4.2217259 -4.2491336 -4.2909179 -4.3310246 -4.36184 -4.3815804 -4.392715 -4.3982191][-4.3990393 -4.3967929 -4.3903618 -4.3767614 -4.3575468 -4.3376737 -4.3240724 -4.3229322 -4.3353095 -4.3548417 -4.3728948 -4.385973 -4.3940454 -4.3983846 -4.4002719][-4.4006324 -4.4003015 -4.3985629 -4.3941298 -4.3872561 -4.3794532 -4.3735943 -4.3724971 -4.3768454 -4.3843665 -4.3913198 -4.3962655 -4.3991175 -4.4004955 -4.4010129]]...]
INFO - root - 2017-12-05 09:41:48.304994: step 14210, loss = 1.75, batch loss = 1.69 (36.5 examples/sec; 0.219 sec/batch; 19h:22m:16s remains)
INFO - root - 2017-12-05 09:41:50.500611: step 14220, loss = 1.35, batch loss = 1.29 (37.3 examples/sec; 0.215 sec/batch; 18h:58m:59s remains)
INFO - root - 2017-12-05 09:41:52.669982: step 14230, loss = 1.58, batch loss = 1.53 (37.5 examples/sec; 0.213 sec/batch; 18h:52m:09s remains)
INFO - root - 2017-12-05 09:41:54.827640: step 14240, loss = 1.97, batch loss = 1.91 (38.4 examples/sec; 0.208 sec/batch; 18h:24m:29s remains)
INFO - root - 2017-12-05 09:41:57.025127: step 14250, loss = 1.40, batch loss = 1.34 (37.7 examples/sec; 0.212 sec/batch; 18h:45m:23s remains)
INFO - root - 2017-12-05 09:41:59.242770: step 14260, loss = 1.29, batch loss = 1.23 (34.4 examples/sec; 0.233 sec/batch; 20h:33m:44s remains)
INFO - root - 2017-12-05 09:42:01.434948: step 14270, loss = 1.40, batch loss = 1.34 (35.4 examples/sec; 0.226 sec/batch; 19h:58m:50s remains)
INFO - root - 2017-12-05 09:42:03.581726: step 14280, loss = 1.08, batch loss = 1.02 (37.8 examples/sec; 0.212 sec/batch; 18h:43m:55s remains)
INFO - root - 2017-12-05 09:42:05.760673: step 14290, loss = 1.39, batch loss = 1.33 (36.3 examples/sec; 0.220 sec/batch; 19h:28m:00s remains)
INFO - root - 2017-12-05 09:42:07.952889: step 14300, loss = 1.93, batch loss = 1.87 (36.9 examples/sec; 0.217 sec/batch; 19h:10m:37s remains)
2017-12-05 09:42:08.235672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4016676 -4.3953657 -4.3783374 -4.3465338 -4.3054891 -4.2677402 -4.2439218 -4.2405686 -4.2594986 -4.2936835 -4.3297987 -4.3590512 -4.3799672 -4.3929315 -4.3994303][-4.4004021 -4.391192 -4.3680491 -4.3277144 -4.2770348 -4.2283959 -4.1934857 -4.1827016 -4.200634 -4.2409425 -4.288517 -4.3310256 -4.3639245 -4.3849277 -4.3960209][-4.3988094 -4.3866591 -4.3582458 -4.3111973 -4.2525992 -4.1944175 -4.1498165 -4.1314392 -4.1448917 -4.1863589 -4.2422829 -4.2982445 -4.3441978 -4.3747005 -4.39159][-4.3970985 -4.3824787 -4.3504276 -4.2996712 -4.236989 -4.173255 -4.1221075 -4.09719 -4.10409 -4.1415691 -4.2000594 -4.2658544 -4.3238316 -4.3640795 -4.3867974][-4.395802 -4.379951 -4.3465338 -4.2951279 -4.2319922 -4.1669497 -4.1127677 -4.08294 -4.0836325 -4.1148229 -4.170929 -4.2409506 -4.3073096 -4.3555007 -4.3830948][-4.3950458 -4.3793554 -4.3467932 -4.2971606 -4.2365689 -4.1737719 -4.1202745 -4.088048 -4.083962 -4.1095338 -4.1610904 -4.2300425 -4.2991405 -4.3510771 -4.3811831][-4.3937426 -4.3797393 -4.3501325 -4.3042073 -4.2482481 -4.1906462 -4.1407704 -4.1083765 -4.1011553 -4.1229081 -4.1707091 -4.2350264 -4.3005347 -4.3512349 -4.381206][-4.3896055 -4.3784394 -4.354115 -4.3142366 -4.2647486 -4.2136254 -4.1689448 -4.1375442 -4.1285706 -4.1480336 -4.192246 -4.2500587 -4.3087134 -4.3549409 -4.382534][-4.380199 -4.3727355 -4.355741 -4.3244939 -4.28271 -4.2384915 -4.1993189 -4.1703367 -4.1606059 -4.1774554 -4.2172122 -4.2679853 -4.3193226 -4.3603683 -4.3847032][-4.3648467 -4.3620367 -4.3534241 -4.3320746 -4.2995677 -4.2631936 -4.2298493 -4.2039013 -4.1944633 -4.2077246 -4.2406611 -4.2835793 -4.3282757 -4.3647161 -4.3860755][-4.3469157 -4.3493071 -4.3489494 -4.33743 -4.3147683 -4.287313 -4.2605228 -4.2379627 -4.2288127 -4.2380242 -4.2630477 -4.2971706 -4.335041 -4.366848 -4.3856411][-4.3321586 -4.3397274 -4.3467984 -4.34428 -4.3311067 -4.3125811 -4.292417 -4.2733316 -4.2638412 -4.2686214 -4.28602 -4.3117366 -4.3421397 -4.3684216 -4.3844686][-4.3261609 -4.3381114 -4.3504286 -4.3544493 -4.3492126 -4.3384361 -4.3247213 -4.3099294 -4.300849 -4.301425 -4.311841 -4.3297276 -4.351882 -4.371717 -4.3846693][-4.3327088 -4.3451276 -4.3588448 -4.3664417 -4.3666458 -4.3617082 -4.3540344 -4.3449383 -4.3381214 -4.3362532 -4.3408675 -4.35092 -4.3644147 -4.3775682 -4.387105][-4.348516 -4.3581724 -4.3700395 -4.3783655 -4.3815508 -4.3802814 -4.37697 -4.3727016 -4.3685117 -4.3660574 -4.3666329 -4.3705091 -4.3767633 -4.3843241 -4.3907194]]...]
INFO - root - 2017-12-05 09:42:10.420635: step 14310, loss = 1.31, batch loss = 1.25 (35.9 examples/sec; 0.223 sec/batch; 19h:41m:49s remains)
INFO - root - 2017-12-05 09:42:12.633806: step 14320, loss = 1.64, batch loss = 1.59 (35.8 examples/sec; 0.224 sec/batch; 19h:46m:22s remains)
INFO - root - 2017-12-05 09:42:14.826962: step 14330, loss = 1.21, batch loss = 1.15 (36.1 examples/sec; 0.222 sec/batch; 19h:35m:47s remains)
INFO - root - 2017-12-05 09:42:17.016422: step 14340, loss = 1.50, batch loss = 1.45 (36.3 examples/sec; 0.221 sec/batch; 19h:29m:17s remains)
INFO - root - 2017-12-05 09:42:19.198663: step 14350, loss = 1.92, batch loss = 1.86 (36.0 examples/sec; 0.222 sec/batch; 19h:39m:30s remains)
INFO - root - 2017-12-05 09:42:21.426141: step 14360, loss = 1.31, batch loss = 1.25 (36.5 examples/sec; 0.219 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-05 09:42:23.611661: step 14370, loss = 1.65, batch loss = 1.59 (37.3 examples/sec; 0.215 sec/batch; 18h:57m:54s remains)
INFO - root - 2017-12-05 09:42:25.812741: step 14380, loss = 1.99, batch loss = 1.93 (35.5 examples/sec; 0.225 sec/batch; 19h:54m:34s remains)
INFO - root - 2017-12-05 09:42:27.989245: step 14390, loss = 1.41, batch loss = 1.35 (36.4 examples/sec; 0.220 sec/batch; 19h:24m:26s remains)
INFO - root - 2017-12-05 09:42:30.222399: step 14400, loss = 1.56, batch loss = 1.50 (35.8 examples/sec; 0.224 sec/batch; 19h:46m:09s remains)
2017-12-05 09:42:30.499382: I tensorflow/core/kernels/logging_ops.cc:79] [[[5.2317238 5.18913 4.607131 3.6157913 2.2784691 0.73752022 -0.80571485 -2.1479995 -3.1123652 -3.6635375 -3.8881769 -3.8968189 -3.7945709 -3.6721916 -3.5823765][6.1962891 6.2073088 5.6384907 4.6353607 3.2665815 1.6699781 0.039453506 -1.4264314 -2.534801 -3.2229393 -3.5592914 -3.6532898 -3.618809 -3.5521843 -3.5098403][6.6481819 6.8015051 6.3564672 5.4546309 4.1829157 2.668273 1.0785131 -0.40576839 -1.5986655 -2.4140077 -2.8893366 -3.1244569 -3.2290118 -3.2890978 -3.3522968][6.7369518 7.1343336 6.9415016 6.2567263 5.1941051 3.8591213 2.4033012 0.98205996 -0.24177408 -1.1671548 -1.7953794 -2.2108619 -2.5095494 -2.7593541 -2.9882877][6.4987984 7.1977406 7.3402262 6.973258 6.2014542 5.1298685 3.8913832 2.6154633 1.4344449 0.44297409 -0.32958269 -0.93596244 -1.4497941 -1.9244609 -2.3629267][5.8910875 6.9014044 7.3976097 7.3926363 6.9709415 6.2240629 5.2741928 4.228734 3.1786628 2.1937046 1.3192859 0.54238033 -0.17954063 -0.88015795 -1.5402424][4.9524469 6.1979847 6.9897366 7.3206816 7.2407351 6.8454218 6.2316046 5.48884 4.6648359 3.7726612 2.8517618 1.9472904 1.0558081 0.16154146 -0.69389629][3.7384872 5.0671673 6.0333738 6.6105661 6.815177 6.73524 6.4497805 6.0245314 5.4767933 4.7519293 3.867466 2.9128757 1.9294286 0.91934729 -0.057059765][2.375247 3.6346312 4.6232662 5.3144217 5.6975231 5.8411818 5.796319 5.613121 5.2980862 4.770463 4.0148649 3.1246119 2.1745691 1.1830206 0.21357536][1.1110988 2.1694393 3.0294566 3.6982288 4.1397495 4.38692 4.4699221 4.4251118 4.2531767 3.8899612 3.3089762 2.5795269 1.7807813 0.94357824 0.1159234][0.16067219 0.96867323 1.6190438 2.1599078 2.5503407 2.7993851 2.9207358 2.9339418 2.8351297 2.5829058 2.1602302 1.6110497 1.0081425 0.38625669 -0.22605658][-0.35543394 0.22566175 0.64545345 1.0047269 1.2680631 1.4387264 1.5234127 1.5363784 1.4705815 1.2878098 0.98323965 0.59196138 0.17599964 -0.23965979 -0.64155626][-0.53452206 -0.12311983 0.1055994 0.27976608 0.38751936 0.438663 0.44358635 0.42045546 0.3579607 0.22114706 0.0044269562 -0.26560688 -0.5367744 -0.79809093 -1.0432603][-0.62635279 -0.33044147 -0.23193026 -0.19739485 -0.21625757 -0.26925278 -0.33753872 -0.39751816 -0.46173096 -0.56810737 -0.72490525 -0.91242981 -1.0909147 -1.2584512 -1.4110434][-0.90748215 -0.68118072 -0.64975572 -0.68525577 -0.768131 -0.87338257 -0.9757905 -1.0496225 -1.1101704 -1.1904879 -1.3035219 -1.4430206 -1.5765734 -1.7019897 -1.814589]]...]
INFO - root - 2017-12-05 09:42:32.674143: step 14410, loss = 1.36, batch loss = 1.30 (35.5 examples/sec; 0.225 sec/batch; 19h:54m:29s remains)
INFO - root - 2017-12-05 09:42:34.889709: step 14420, loss = 1.39, batch loss = 1.33 (37.0 examples/sec; 0.216 sec/batch; 19h:05m:30s remains)
INFO - root - 2017-12-05 09:42:37.067895: step 14430, loss = 1.09, batch loss = 1.03 (36.3 examples/sec; 0.220 sec/batch; 19h:27m:54s remains)
INFO - root - 2017-12-05 09:42:39.261711: step 14440, loss = 1.35, batch loss = 1.29 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:06s remains)
INFO - root - 2017-12-05 09:42:41.473444: step 14450, loss = 1.73, batch loss = 1.67 (35.2 examples/sec; 0.227 sec/batch; 20h:05m:25s remains)
INFO - root - 2017-12-05 09:42:43.676774: step 14460, loss = 1.26, batch loss = 1.21 (37.1 examples/sec; 0.216 sec/batch; 19h:03m:58s remains)
INFO - root - 2017-12-05 09:42:45.907468: step 14470, loss = 1.27, batch loss = 1.21 (35.9 examples/sec; 0.223 sec/batch; 19h:40m:11s remains)
INFO - root - 2017-12-05 09:42:48.112489: step 14480, loss = 1.30, batch loss = 1.24 (36.4 examples/sec; 0.220 sec/batch; 19h:25m:22s remains)
INFO - root - 2017-12-05 09:42:50.273400: step 14490, loss = 1.81, batch loss = 1.75 (35.6 examples/sec; 0.225 sec/batch; 19h:52m:06s remains)
INFO - root - 2017-12-05 09:42:52.442064: step 14500, loss = 1.38, batch loss = 1.32 (38.7 examples/sec; 0.207 sec/batch; 18h:15m:51s remains)
2017-12-05 09:42:52.702741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3910289 -4.3794322 -4.3691521 -4.3646674 -4.3653269 -4.368753 -4.3713913 -4.3720307 -4.3720708 -4.3739324 -4.3785772 -4.3850875 -4.3919506 -4.3977604 -4.4015508][-4.3457808 -4.2983689 -4.2564349 -4.2352543 -4.2358174 -4.2475123 -4.2558341 -4.2560086 -4.2553492 -4.2634983 -4.2865434 -4.3190451 -4.350842 -4.3759017 -4.3916192][-4.24589 -4.1213088 -4.0091119 -3.9436479 -3.9288702 -3.9399946 -3.9435513 -3.9337943 -3.9335399 -3.9678824 -4.0454478 -4.1481667 -4.2452717 -4.3193464 -4.3659921][-4.101016 -3.8659816 -3.6466475 -3.4992051 -3.4304976 -3.4000602 -3.3603525 -3.3177619 -3.3276772 -3.4277396 -3.6160452 -3.8480525 -4.0597773 -4.2188759 -4.3199644][-3.9634256 -3.6238904 -3.2948833 -3.0417957 -2.8678565 -2.7273502 -2.5822988 -2.4774783 -2.5147882 -2.7264159 -3.0743895 -3.4771252 -3.8323925 -4.0947332 -4.2621379][-3.8958087 -3.507982 -3.1138926 -2.7611055 -2.4464974 -2.1392915 -1.8427665 -1.6616607 -1.7458396 -2.0930879 -2.6042595 -3.1631851 -3.6425076 -3.9902129 -4.2133341][-3.9198921 -3.5541391 -3.1594777 -2.7522702 -2.3252206 -1.8757682 -1.4644337 -1.2426212 -1.3700337 -1.8064184 -2.4088616 -3.0412192 -3.5726161 -3.9522693 -4.1958241][-4.0120111 -3.7182114 -3.3806148 -2.9879851 -2.5342407 -2.0439303 -1.6172762 -1.4140036 -1.5621829 -1.9978201 -2.5730314 -3.1621447 -3.6528578 -3.9987679 -4.2188773][-4.134306 -3.9324217 -3.6850944 -3.369082 -2.9828076 -2.5637755 -2.216651 -2.0757711 -2.2163119 -2.5680382 -3.0142183 -3.4656522 -3.8420031 -4.1059451 -4.2711163][-4.2476244 -4.1297116 -3.9758461 -3.7646511 -3.4999168 -3.2163 -2.9945071 -2.9206853 -3.0255666 -3.2553923 -3.537149 -3.8208039 -4.0586519 -4.2255106 -4.3271785][-4.3277631 -4.2699814 -4.1905313 -4.074378 -3.9273293 -3.7743042 -3.6629622 -3.6349976 -3.6966269 -3.817986 -3.963098 -4.1100988 -4.2329855 -4.3181939 -4.3685966][-4.3725848 -4.3489385 -4.3154278 -4.2636313 -4.1977477 -4.1314554 -4.08735 -4.0804935 -4.1085167 -4.160717 -4.222671 -4.2859521 -4.3373117 -4.3714018 -4.39123][-4.3933487 -4.3853812 -4.3739576 -4.355443 -4.3321867 -4.3098035 -4.2959619 -4.2943268 -4.303596 -4.3216114 -4.3437128 -4.366365 -4.383594 -4.3945074 -4.4007177][-4.4016004 -4.3995457 -4.396368 -4.391264 -4.3852129 -4.3794708 -4.3760452 -4.3754811 -4.3773379 -4.3820987 -4.3881459 -4.3944674 -4.39909 -4.4019852 -4.4035964][-4.40406 -4.4036713 -4.403141 -4.402194 -4.4011 -4.4000087 -4.399148 -4.3985696 -4.3986835 -4.3997855 -4.4011626 -4.4024372 -4.403316 -4.4039211 -4.404232]]...]
INFO - root - 2017-12-05 09:42:54.905288: step 14510, loss = 1.26, batch loss = 1.20 (37.3 examples/sec; 0.215 sec/batch; 18h:57m:26s remains)
INFO - root - 2017-12-05 09:42:57.101497: step 14520, loss = 1.72, batch loss = 1.66 (36.7 examples/sec; 0.218 sec/batch; 19h:14m:30s remains)
INFO - root - 2017-12-05 09:42:59.297250: step 14530, loss = 1.11, batch loss = 1.05 (37.8 examples/sec; 0.212 sec/batch; 18h:42m:53s remains)
INFO - root - 2017-12-05 09:43:01.467728: step 14540, loss = 1.67, batch loss = 1.62 (36.3 examples/sec; 0.220 sec/batch; 19h:27m:10s remains)
INFO - root - 2017-12-05 09:43:03.686099: step 14550, loss = 1.60, batch loss = 1.55 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:09s remains)
INFO - root - 2017-12-05 09:43:05.880303: step 14560, loss = 1.40, batch loss = 1.34 (37.4 examples/sec; 0.214 sec/batch; 18h:53m:20s remains)
INFO - root - 2017-12-05 09:43:08.081026: step 14570, loss = 1.65, batch loss = 1.59 (35.6 examples/sec; 0.225 sec/batch; 19h:51m:29s remains)
INFO - root - 2017-12-05 09:43:10.280252: step 14580, loss = 1.79, batch loss = 1.74 (37.7 examples/sec; 0.212 sec/batch; 18h:45m:12s remains)
INFO - root - 2017-12-05 09:43:12.486455: step 14590, loss = 1.59, batch loss = 1.53 (36.2 examples/sec; 0.221 sec/batch; 19h:29m:58s remains)
INFO - root - 2017-12-05 09:43:14.665499: step 14600, loss = 1.45, batch loss = 1.39 (37.3 examples/sec; 0.214 sec/batch; 18h:55m:32s remains)
2017-12-05 09:43:15.018963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3830247 -4.3748031 -4.3640671 -4.3535161 -4.3465214 -4.345129 -4.3495331 -4.3579516 -4.3671041 -4.374042 -4.37783 -4.3792839 -4.3798604 -4.3802271 -4.3821859][-4.3850689 -4.3767848 -4.3655996 -4.3538785 -4.3447118 -4.3408818 -4.3431568 -4.3508959 -4.3609757 -4.3697095 -4.3753757 -4.3780565 -4.3788695 -4.3787489 -4.3804641][-4.3898568 -4.382997 -4.3729644 -4.3613262 -4.3508363 -4.3440204 -4.3427973 -4.3483028 -4.3579059 -4.3674064 -4.3743792 -4.3782525 -4.3795877 -4.3795514 -4.3810997][-4.39369 -4.3882942 -4.3795261 -4.3681374 -4.3562837 -4.3471642 -4.3431649 -4.3465977 -4.3555336 -4.3655457 -4.3737164 -4.3788209 -4.3807454 -4.3807688 -4.3822017][-4.3953676 -4.3907094 -4.3824248 -4.3708148 -4.3577914 -4.3469176 -4.3411794 -4.3437243 -4.3528214 -4.3639746 -4.3735123 -4.3793974 -4.3814135 -4.3814578 -4.3829975][-4.39565 -4.3909688 -4.3822002 -4.36965 -4.3557363 -4.3437033 -4.336987 -4.3395672 -4.3496628 -4.3620744 -4.3725958 -4.378777 -4.3807836 -4.3811297 -4.3832865][-4.3955545 -4.3904729 -4.3811197 -4.3677425 -4.3530602 -4.3406887 -4.3341508 -4.3369284 -4.347538 -4.3604445 -4.3709087 -4.3768091 -4.3789458 -4.3800621 -4.3831968][-4.39558 -4.3905096 -4.3812075 -4.3678403 -4.3532915 -4.3415904 -4.3360767 -4.3391676 -4.349205 -4.3608232 -4.3697047 -4.3745909 -4.3768897 -4.3790922 -4.3834581][-4.3957267 -4.3913937 -4.3831463 -4.3711042 -4.3580275 -4.34814 -4.3444061 -4.3478703 -4.3563223 -4.3652687 -4.3715892 -4.3747454 -4.3768659 -4.3802242 -4.3856292][-4.395402 -4.3921018 -4.385488 -4.3756814 -4.3651962 -4.3578181 -4.3560905 -4.3600368 -4.3668513 -4.373054 -4.3768897 -4.3785791 -4.380569 -4.3844943 -4.3899937][-4.3942156 -4.3917155 -4.3867025 -4.3792863 -4.37155 -4.3664656 -4.3661389 -4.3701434 -4.3757262 -4.3803315 -4.383132 -4.3846221 -4.3867292 -4.3904486 -4.3947821][-4.39264 -4.3907485 -4.3871341 -4.3817849 -4.3763447 -4.3729887 -4.3733377 -4.3768978 -4.3815184 -4.3854356 -4.3882575 -4.3903131 -4.3926654 -4.395597 -4.3983216][-4.3925066 -4.3911762 -4.3887572 -4.3851409 -4.3814664 -4.3792443 -4.379703 -4.3825173 -4.3862286 -4.389688 -4.3924971 -4.3947749 -4.3968472 -4.39877 -4.400208][-4.3948183 -4.3940639 -4.3928018 -4.3907905 -4.38863 -4.3872643 -4.3874736 -4.3892207 -4.3917522 -4.3942323 -4.3962784 -4.3979149 -4.39928 -4.4003386 -4.4009757][-4.3978977 -4.397542 -4.3971386 -4.3963461 -4.3953915 -4.3947253 -4.3947763 -4.3956013 -4.3968453 -4.3980851 -4.399107 -4.3999605 -4.4006495 -4.4010949 -4.4013419]]...]
INFO - root - 2017-12-05 09:43:17.280137: step 14610, loss = 2.00, batch loss = 1.94 (36.6 examples/sec; 0.219 sec/batch; 19h:17m:39s remains)
INFO - root - 2017-12-05 09:43:19.482201: step 14620, loss = 1.72, batch loss = 1.66 (36.8 examples/sec; 0.217 sec/batch; 19h:10m:41s remains)
INFO - root - 2017-12-05 09:43:21.692564: step 14630, loss = 1.62, batch loss = 1.56 (35.7 examples/sec; 0.224 sec/batch; 19h:46m:14s remains)
INFO - root - 2017-12-05 09:43:23.870917: step 14640, loss = 1.65, batch loss = 1.59 (34.9 examples/sec; 0.229 sec/batch; 20h:14m:42s remains)
INFO - root - 2017-12-05 09:43:26.036302: step 14650, loss = 1.59, batch loss = 1.53 (36.9 examples/sec; 0.217 sec/batch; 19h:08m:48s remains)
INFO - root - 2017-12-05 09:43:28.222293: step 14660, loss = 1.83, batch loss = 1.77 (36.7 examples/sec; 0.218 sec/batch; 19h:13m:18s remains)
INFO - root - 2017-12-05 09:43:30.406567: step 14670, loss = 1.41, batch loss = 1.35 (36.7 examples/sec; 0.218 sec/batch; 19h:13m:38s remains)
INFO - root - 2017-12-05 09:43:32.630556: step 14680, loss = 1.39, batch loss = 1.33 (36.6 examples/sec; 0.219 sec/batch; 19h:18m:25s remains)
INFO - root - 2017-12-05 09:43:34.834706: step 14690, loss = 1.28, batch loss = 1.22 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:57s remains)
INFO - root - 2017-12-05 09:43:37.025702: step 14700, loss = 1.48, batch loss = 1.42 (36.8 examples/sec; 0.218 sec/batch; 19h:12m:14s remains)
2017-12-05 09:43:37.326935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3266211 -4.3143382 -4.312037 -4.3173914 -4.3274856 -4.3383565 -4.34811 -4.3562202 -4.3631306 -4.368609 -4.3722863 -4.3732915 -4.3693633 -4.3569489 -4.3300042][-4.3022623 -4.2769656 -4.2605972 -4.2540059 -4.2574425 -4.2681842 -4.2825766 -4.2979159 -4.3122783 -4.3242397 -4.3328738 -4.3377075 -4.3361497 -4.3239942 -4.2947836][-4.2813411 -4.2394738 -4.2023225 -4.1750145 -4.1631875 -4.16743 -4.1827316 -4.2034521 -4.2252 -4.2454853 -4.2631006 -4.2773676 -4.2851939 -4.2807903 -4.2566066][-4.2697773 -4.2131529 -4.1549325 -4.1037092 -4.0700231 -4.0579462 -4.0636334 -4.080647 -4.1039753 -4.1313915 -4.1613426 -4.1920962 -4.2185988 -4.2325091 -4.2253103][-4.2727265 -4.2099581 -4.1386657 -4.0676336 -4.0096703 -3.9721723 -3.9555593 -3.9555025 -3.9696913 -3.998209 -4.0397744 -4.0909071 -4.1433468 -4.18549 -4.2064891][-4.2872539 -4.2290878 -4.1579547 -4.0784006 -4.0009956 -3.9354305 -3.8874006 -3.858403 -3.8517597 -3.8720958 -3.9200287 -3.9900665 -4.0697989 -4.1430817 -4.1954632][-4.3026295 -4.2570281 -4.1979475 -4.1234217 -4.0380425 -3.9510438 -3.8724122 -3.8113847 -3.77847 -3.7831132 -3.8284941 -3.9085755 -4.0081182 -4.1062303 -4.1844153][-4.3078294 -4.2768393 -4.2357273 -4.1760511 -4.0963554 -4.0019121 -3.9045615 -3.8196981 -3.7639802 -3.7513418 -3.7872515 -3.8668666 -3.9740236 -4.084549 -4.1759596][-4.2971296 -4.2784977 -4.2548676 -4.2125263 -4.1456652 -4.056201 -3.9559777 -3.8624172 -3.794559 -3.7690508 -3.7940586 -3.8664966 -3.9700534 -4.07965 -4.1709228][-4.272151 -4.2605762 -4.2484608 -4.2182574 -4.162261 -4.0819917 -3.9891989 -3.9005055 -3.8341208 -3.8059464 -3.8249538 -3.8889158 -3.9830863 -4.0841069 -4.1683211][-4.239254 -4.2280364 -4.2188549 -4.1923571 -4.1410279 -4.0681863 -3.9858685 -3.9096081 -3.8546309 -3.8328815 -3.8509598 -3.9076669 -3.991292 -4.0820985 -4.1588941][-4.2048035 -4.188334 -4.1752162 -4.1465063 -4.0957294 -4.0276213 -3.9549818 -3.8924778 -3.8517945 -3.839467 -3.8579273 -3.9073887 -3.9803119 -4.0616612 -4.1329341][-4.1739292 -4.1479177 -4.1279883 -4.0974941 -4.0501184 -3.9893806 -3.927182 -3.8774908 -3.8491275 -3.8432531 -3.8594687 -3.8993721 -3.9595063 -4.0290661 -4.0926218][-4.1599364 -4.1262488 -4.1024346 -4.0750737 -4.0375433 -3.9906185 -3.9429955 -3.9063475 -3.886488 -3.8820167 -3.8918104 -3.9183002 -3.9614141 -4.0138626 -4.0642533][-4.1785855 -4.1448507 -4.1236429 -4.104702 -4.0809374 -4.0510135 -4.0202909 -3.9968414 -3.9831736 -3.9775767 -3.9788814 -3.990226 -4.0137634 -4.0459929 -4.0802073]]...]
INFO - root - 2017-12-05 09:43:39.560213: step 14710, loss = 1.53, batch loss = 1.47 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:38s remains)
INFO - root - 2017-12-05 09:43:41.759850: step 14720, loss = 1.50, batch loss = 1.44 (37.0 examples/sec; 0.216 sec/batch; 19h:05m:01s remains)
INFO - root - 2017-12-05 09:43:43.952404: step 14730, loss = 1.68, batch loss = 1.62 (36.4 examples/sec; 0.220 sec/batch; 19h:22m:56s remains)
INFO - root - 2017-12-05 09:43:46.161666: step 14740, loss = 1.64, batch loss = 1.59 (36.8 examples/sec; 0.218 sec/batch; 19h:12m:16s remains)
INFO - root - 2017-12-05 09:43:48.369548: step 14750, loss = 2.06, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:20m:52s remains)
INFO - root - 2017-12-05 09:43:50.566793: step 14760, loss = 1.47, batch loss = 1.41 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:06s remains)
INFO - root - 2017-12-05 09:43:52.767985: step 14770, loss = 1.44, batch loss = 1.38 (36.6 examples/sec; 0.219 sec/batch; 19h:18m:34s remains)
INFO - root - 2017-12-05 09:43:55.001126: step 14780, loss = 1.61, batch loss = 1.55 (37.0 examples/sec; 0.216 sec/batch; 19h:04m:20s remains)
INFO - root - 2017-12-05 09:43:57.222834: step 14790, loss = 1.56, batch loss = 1.50 (36.9 examples/sec; 0.217 sec/batch; 19h:06m:51s remains)
INFO - root - 2017-12-05 09:43:59.418790: step 14800, loss = 1.63, batch loss = 1.57 (36.5 examples/sec; 0.219 sec/batch; 19h:22m:01s remains)
2017-12-05 09:43:59.697790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4057641 -4.3998537 -4.3728585 -4.2968597 -4.1589994 -3.9854228 -3.8395975 -3.7870164 -3.856766 -4.0095639 -4.1688862 -4.2887106 -4.3557358 -4.378552 -4.3727293][-4.4050522 -4.3980517 -4.3665681 -4.2732162 -4.0951734 -3.8532312 -3.6292329 -3.5234308 -3.5948591 -3.8002989 -4.0329571 -4.2157664 -4.3232608 -4.3675995 -4.3760457][-4.4045715 -4.3952413 -4.353488 -4.2285776 -3.9811716 -3.6276369 -3.2803028 -3.0951362 -3.1765532 -3.4709165 -3.8221192 -4.1039658 -4.2708697 -4.3430214 -4.3642211][-4.4038382 -4.3908019 -4.3322372 -4.1580853 -3.8070498 -3.2965016 -2.7848194 -2.504498 -2.6159806 -3.0393176 -3.5481093 -3.9576061 -4.1982803 -4.2998629 -4.3280015][-4.4031692 -4.3860464 -4.3087807 -4.078824 -3.6121991 -2.9299221 -2.2449751 -1.8754458 -2.0297153 -2.5918436 -3.2614775 -3.8014407 -4.1171947 -4.2464066 -4.275671][-4.4027691 -4.3823161 -4.2873373 -4.0057096 -3.4369621 -2.6095223 -1.7867637 -1.3583207 -1.5581617 -2.2301078 -3.0199981 -3.6572471 -4.029242 -4.1793127 -4.2071142][-4.4028158 -4.3796 -4.2710171 -3.954215 -3.3232369 -2.4156241 -1.5229058 -1.0683496 -1.2891426 -2.0036089 -2.8404408 -3.52147 -3.9250319 -4.0906277 -4.1199336][-4.4029064 -4.3780088 -4.2647419 -3.9425323 -3.3102646 -2.4081528 -1.519563 -1.0521369 -1.230124 -1.8858097 -2.6760023 -3.3402808 -3.75707 -3.9478722 -3.9995496][-4.4028792 -4.3787355 -4.2732768 -3.9779377 -3.4001927 -2.5713348 -1.7326581 -1.2374182 -1.2942412 -1.7897594 -2.4487467 -3.0490732 -3.4751914 -3.7195816 -3.8335338][-4.4029279 -4.3819642 -4.2922707 -4.0398278 -3.5398121 -2.8055634 -2.0218036 -1.4731669 -1.3546562 -1.6236279 -2.097456 -2.6084135 -3.0533552 -3.389679 -3.6147046][-4.4033337 -4.3865137 -4.3130097 -4.1016073 -3.6713836 -3.0146289 -2.2614083 -1.6374335 -1.3275356 -1.356215 -1.6326728 -2.0516768 -2.528703 -2.9859993 -3.3575373][-4.4040151 -4.3906217 -4.3288469 -4.1449695 -3.7572885 -3.1430526 -2.3930986 -1.6904793 -1.2126539 -1.034471 -1.1415601 -1.4888246 -2.0094156 -2.5923924 -3.1101279][-4.4045892 -4.3938012 -4.3385887 -4.169313 -3.805465 -3.2185693 -2.4736063 -1.7210381 -1.1274922 -0.80024314 -0.7799561 -1.0709734 -1.6209636 -2.2934029 -2.9173567][-4.4049478 -4.3963294 -4.34734 -4.1946039 -3.8643615 -3.3275256 -2.6316261 -1.8963089 -1.2666972 -0.86356354 -0.75973964 -0.99021244 -1.5174062 -2.19903 -2.8473768][-4.4051847 -4.39858 -4.3580732 -4.2312512 -3.9566498 -3.5093567 -2.9253464 -2.2897003 -1.7172399 -1.3199799 -1.1765273 -1.3339882 -1.7698507 -2.3628411 -2.9419193]]...]
INFO - root - 2017-12-05 09:44:01.909435: step 14810, loss = 1.59, batch loss = 1.53 (35.5 examples/sec; 0.225 sec/batch; 19h:53m:30s remains)
INFO - root - 2017-12-05 09:44:04.136169: step 14820, loss = 1.79, batch loss = 1.73 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:38s remains)
INFO - root - 2017-12-05 09:44:06.297791: step 14830, loss = 1.34, batch loss = 1.28 (37.6 examples/sec; 0.213 sec/batch; 18h:47m:55s remains)
INFO - root - 2017-12-05 09:44:08.505266: step 14840, loss = 1.31, batch loss = 1.25 (34.3 examples/sec; 0.233 sec/batch; 20h:35m:33s remains)
INFO - root - 2017-12-05 09:44:10.689851: step 14850, loss = 1.10, batch loss = 1.04 (37.0 examples/sec; 0.216 sec/batch; 19h:04m:50s remains)
INFO - root - 2017-12-05 09:44:12.864980: step 14860, loss = 1.62, batch loss = 1.56 (37.2 examples/sec; 0.215 sec/batch; 18h:59m:36s remains)
INFO - root - 2017-12-05 09:44:15.178118: step 14870, loss = 1.88, batch loss = 1.82 (37.4 examples/sec; 0.214 sec/batch; 18h:52m:22s remains)
INFO - root - 2017-12-05 09:44:17.369221: step 14880, loss = 1.73, batch loss = 1.67 (36.0 examples/sec; 0.222 sec/batch; 19h:37m:31s remains)
INFO - root - 2017-12-05 09:44:19.594956: step 14890, loss = 1.40, batch loss = 1.34 (37.1 examples/sec; 0.216 sec/batch; 19h:01m:12s remains)
INFO - root - 2017-12-05 09:44:21.781121: step 14900, loss = 1.26, batch loss = 1.20 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:07s remains)
2017-12-05 09:44:22.101043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3898268 -4.3920684 -4.3890433 -4.376791 -4.3534207 -4.32371 -4.3012071 -4.3009057 -4.3231711 -4.3538165 -4.3767662 -4.3882461 -4.3937669 -4.3975778 -4.4008846][-4.3840675 -4.3864717 -4.3794532 -4.3572226 -4.3172932 -4.2698355 -4.2390747 -4.2453084 -4.283751 -4.3309135 -4.3658872 -4.3846316 -4.3934507 -4.3983026 -4.4017115][-4.3806481 -4.3816066 -4.3676829 -4.3305922 -4.2688942 -4.2013779 -4.1644912 -4.1807685 -4.2379708 -4.30325 -4.3517895 -4.3793225 -4.3926148 -4.3989449 -4.4024029][-4.3803778 -4.3775549 -4.3537641 -4.2988529 -4.2136507 -4.1280518 -4.0885925 -4.1162457 -4.1909556 -4.2730565 -4.335073 -4.3719792 -4.39067 -4.3991332 -4.4027996][-4.3816528 -4.3733778 -4.3384757 -4.2653871 -4.1590643 -4.0607424 -4.0216885 -4.0585065 -4.1459861 -4.2418427 -4.3162947 -4.3627172 -4.387475 -4.3986316 -4.4027181][-4.3822289 -4.3680644 -4.3229432 -4.2343049 -4.1129532 -4.0078068 -3.9696202 -4.0101943 -4.1036386 -4.2091522 -4.2948475 -4.3511615 -4.3826623 -4.3968863 -4.4019165][-4.3800745 -4.3615618 -4.3100042 -4.2133818 -4.0870152 -3.9808445 -3.9408562 -3.9770851 -4.0678525 -4.1770821 -4.271585 -4.3370967 -4.3757329 -4.3938527 -4.4005952][-4.37501 -4.3555121 -4.3043633 -4.211236 -4.09214 -3.9902103 -3.9443324 -3.9677219 -4.0473418 -4.1528921 -4.2509704 -4.3229938 -4.3681345 -4.3907061 -4.3995719][-4.3696909 -4.353076 -4.3090844 -4.2296963 -4.1271315 -4.0338874 -3.9811983 -3.9870663 -4.0488081 -4.1430712 -4.2386827 -4.3136425 -4.3628144 -4.3886623 -4.3992019][-4.3676629 -4.3564262 -4.3239789 -4.2634659 -4.1814775 -4.1000524 -4.043468 -4.0329857 -4.0744772 -4.1522408 -4.2394543 -4.3127522 -4.3622627 -4.3885684 -4.3992543][-4.3700109 -4.3649378 -4.3450217 -4.3039331 -4.2430816 -4.1758547 -4.1207781 -4.0993237 -4.1225762 -4.1823168 -4.2563367 -4.3217773 -4.3664942 -4.3900361 -4.3995395][-4.3755136 -4.3759894 -4.3662071 -4.3419433 -4.3010869 -4.2500749 -4.2026572 -4.1772814 -4.1871281 -4.2295017 -4.2864366 -4.3382225 -4.37375 -4.3923664 -4.4000077][-4.3824553 -4.3859949 -4.3826809 -4.3704715 -4.3465166 -4.3124065 -4.2775035 -4.2549691 -4.2567692 -4.2830205 -4.3211718 -4.3567696 -4.38156 -4.3946929 -4.4005451][-4.3894396 -4.3935823 -4.39311 -4.3876867 -4.3755279 -4.356163 -4.334959 -4.3195806 -4.3178349 -4.3308873 -4.3520293 -4.3725562 -4.3877006 -4.3964438 -4.4009571][-4.394834 -4.398222 -4.3985496 -4.3961515 -4.390502 -4.3813491 -4.3709097 -4.362659 -4.36031 -4.36471 -4.3735504 -4.3833675 -4.3918052 -4.3974209 -4.4009008]]...]
INFO - root - 2017-12-05 09:44:24.315777: step 14910, loss = 1.59, batch loss = 1.53 (36.6 examples/sec; 0.218 sec/batch; 19h:15m:46s remains)
INFO - root - 2017-12-05 09:44:26.541759: step 14920, loss = 1.59, batch loss = 1.53 (36.8 examples/sec; 0.217 sec/batch; 19h:10m:26s remains)
INFO - root - 2017-12-05 09:44:28.739216: step 14930, loss = 1.67, batch loss = 1.61 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:32s remains)
INFO - root - 2017-12-05 09:44:30.934880: step 14940, loss = 1.08, batch loss = 1.02 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:59s remains)
INFO - root - 2017-12-05 09:44:33.114887: step 14950, loss = 1.29, batch loss = 1.23 (36.8 examples/sec; 0.217 sec/batch; 19h:10m:13s remains)
INFO - root - 2017-12-05 09:44:35.319112: step 14960, loss = 1.26, batch loss = 1.21 (36.1 examples/sec; 0.222 sec/batch; 19h:33m:30s remains)
INFO - root - 2017-12-05 09:44:37.506295: step 14970, loss = 1.32, batch loss = 1.26 (36.6 examples/sec; 0.219 sec/batch; 19h:17m:17s remains)
INFO - root - 2017-12-05 09:44:39.689128: step 14980, loss = 1.40, batch loss = 1.34 (38.1 examples/sec; 0.210 sec/batch; 18h:32m:30s remains)
INFO - root - 2017-12-05 09:44:41.869340: step 14990, loss = 1.91, batch loss = 1.85 (37.3 examples/sec; 0.215 sec/batch; 18h:55m:43s remains)
INFO - root - 2017-12-05 09:44:44.073506: step 15000, loss = 1.20, batch loss = 1.14 (37.9 examples/sec; 0.211 sec/batch; 18h:37m:27s remains)
2017-12-05 09:44:44.356598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3599849 -4.3736324 -4.3873811 -4.39594 -4.3999557 -4.4016333 -4.4023986 -4.4029346 -4.4033694 -4.4036994 -4.4038343 -4.4038205 -4.4036989 -4.4036684 -4.4037142][-4.3722682 -4.38331 -4.393487 -4.3989692 -4.4011478 -4.4019694 -4.402504 -4.4029455 -4.4033108 -4.4035397 -4.4036036 -4.4035296 -4.4033623 -4.4033675 -4.4034462][-4.3845954 -4.3921347 -4.3985462 -4.4014287 -4.4021387 -4.4022222 -4.4023786 -4.4026227 -4.4027934 -4.4028053 -4.4026647 -4.4025617 -4.4024835 -4.4026484 -4.402936][-4.3933606 -4.3978772 -4.4010925 -4.4016867 -4.4008708 -4.3996186 -4.3985767 -4.3982282 -4.3985519 -4.3988538 -4.3990717 -4.3993678 -4.39981 -4.4006538 -4.4015341][-4.3970084 -4.3992972 -4.3999481 -4.3979607 -4.3944263 -4.3900676 -4.3862567 -4.3847508 -4.3857212 -4.387712 -4.3896437 -4.3913403 -4.392755 -4.3945222 -4.3962655][-4.3967276 -4.3970065 -4.3948717 -4.3888965 -4.3802309 -4.370172 -4.3620825 -4.3591805 -4.361372 -4.3659658 -4.370996 -4.3751087 -4.3782129 -4.38123 -4.3842931][-4.3942575 -4.3923287 -4.3864627 -4.3746996 -4.3588567 -4.3420572 -4.3301 -4.3268266 -4.3309283 -4.3390265 -4.3477407 -4.3541718 -4.3583927 -4.3617544 -4.3657665][-4.391839 -4.3874025 -4.377882 -4.361445 -4.3404732 -4.3199005 -4.3067226 -4.3041892 -4.3098612 -4.3202357 -4.3310194 -4.338594 -4.3424473 -4.3446722 -4.348597][-4.3917527 -4.3860741 -4.3754473 -4.3583117 -4.3374014 -4.3181214 -4.3066163 -4.3051119 -4.3113394 -4.3220487 -4.3322849 -4.3385892 -4.3406487 -4.3413815 -4.3446436][-4.3948536 -4.389966 -4.3813834 -4.367949 -4.3517308 -4.3373871 -4.3295074 -4.3292174 -4.3348475 -4.3434744 -4.3507719 -4.3545489 -4.3547068 -4.354208 -4.3567162][-4.3986197 -4.3956227 -4.390522 -4.3826895 -4.3732638 -4.3652334 -4.3615088 -4.3622642 -4.366096 -4.3710237 -4.37478 -4.3765097 -4.3758059 -4.3750348 -4.3766937][-4.4011846 -4.3998694 -4.3976626 -4.3943386 -4.3904462 -4.3871908 -4.38582 -4.3864055 -4.3882375 -4.3903022 -4.391696 -4.3922157 -4.3916769 -4.39141 -4.3924856][-4.4025254 -4.402041 -4.4013066 -4.4001784 -4.3989372 -4.3979049 -4.3975673 -4.3977995 -4.39836 -4.3990269 -4.3995581 -4.3997149 -4.3994679 -4.3995709 -4.4002233][-4.4030209 -4.40296 -4.4028687 -4.4025764 -4.4021316 -4.4017706 -4.4016609 -4.4017057 -4.4017773 -4.4019589 -4.4022217 -4.4022918 -4.40216 -4.4022241 -4.4025378][-4.4028425 -4.4029064 -4.4030228 -4.4030476 -4.4029474 -4.4027915 -4.40267 -4.4025879 -4.4025316 -4.4025812 -4.40271 -4.4027643 -4.4026961 -4.4027038 -4.4028506]]...]
INFO - root - 2017-12-05 09:44:46.523886: step 15010, loss = 1.45, batch loss = 1.39 (37.2 examples/sec; 0.215 sec/batch; 18h:58m:14s remains)
INFO - root - 2017-12-05 09:44:48.696127: step 15020, loss = 1.43, batch loss = 1.37 (36.9 examples/sec; 0.217 sec/batch; 19h:07m:04s remains)
INFO - root - 2017-12-05 09:44:50.912585: step 15030, loss = 1.39, batch loss = 1.33 (36.3 examples/sec; 0.220 sec/batch; 19h:26m:09s remains)
INFO - root - 2017-12-05 09:44:53.093892: step 15040, loss = 1.27, batch loss = 1.22 (35.8 examples/sec; 0.223 sec/batch; 19h:41m:52s remains)
INFO - root - 2017-12-05 09:44:55.299693: step 15050, loss = 1.47, batch loss = 1.42 (35.3 examples/sec; 0.227 sec/batch; 19h:59m:19s remains)
INFO - root - 2017-12-05 09:44:57.475860: step 15060, loss = 2.15, batch loss = 2.09 (37.3 examples/sec; 0.214 sec/batch; 18h:54m:19s remains)
INFO - root - 2017-12-05 09:44:59.660501: step 15070, loss = 1.40, batch loss = 1.34 (36.3 examples/sec; 0.220 sec/batch; 19h:24m:24s remains)
INFO - root - 2017-12-05 09:45:01.831839: step 15080, loss = 1.46, batch loss = 1.41 (35.9 examples/sec; 0.223 sec/batch; 19h:39m:10s remains)
INFO - root - 2017-12-05 09:45:04.039409: step 15090, loss = 1.51, batch loss = 1.45 (37.0 examples/sec; 0.216 sec/batch; 19h:02m:30s remains)
INFO - root - 2017-12-05 09:45:06.233022: step 15100, loss = 1.62, batch loss = 1.56 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:49s remains)
2017-12-05 09:45:06.518717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4016871 -4.4017196 -4.4017596 -4.4017873 -4.4018106 -4.4018545 -4.4019132 -4.4019623 -4.4019814 -4.401958 -4.4018774 -4.4017582 -4.4016571 -4.4016027 -4.4015827][-4.4017911 -4.4018397 -4.4019041 -4.4019494 -4.4019871 -4.4020281 -4.4020481 -4.4020472 -4.4020224 -4.40196 -4.4018545 -4.4017334 -4.4016476 -4.4016156 -4.40161][-4.4018941 -4.4019628 -4.4020672 -4.4021497 -4.4022226 -4.4022641 -4.4022307 -4.4021416 -4.4020457 -4.401958 -4.40186 -4.4017591 -4.401701 -4.4016919 -4.4016991][-4.4020267 -4.4021158 -4.4022574 -4.4023628 -4.4024363 -4.4024038 -4.4022522 -4.4019961 -4.4017763 -4.4017019 -4.4017 -4.4017148 -4.4017177 -4.4017358 -4.4017615][-4.4021349 -4.4022684 -4.4024682 -4.4025869 -4.4026175 -4.4024749 -4.4021683 -4.4017196 -4.4013438 -4.4012833 -4.4014359 -4.4016166 -4.4016976 -4.4017534 -4.4017997][-4.402194 -4.40239 -4.402698 -4.4028621 -4.4028769 -4.4026628 -4.4022393 -4.4016528 -4.4011164 -4.4010282 -4.40129 -4.40156 -4.4016819 -4.4017591 -4.4018216][-4.4021373 -4.4024162 -4.4028215 -4.4030638 -4.4031177 -4.4028745 -4.4024062 -4.401793 -4.4011827 -4.4010053 -4.4012465 -4.4015479 -4.4016914 -4.4017515 -4.4018168][-4.4020214 -4.4023218 -4.4027658 -4.4030938 -4.4032178 -4.4029961 -4.4025755 -4.40198 -4.4013534 -4.401104 -4.4012685 -4.4015613 -4.4016962 -4.4017329 -4.4017787][-4.4019046 -4.4021578 -4.4025655 -4.402925 -4.4031115 -4.402988 -4.4026833 -4.4021626 -4.4015665 -4.4013028 -4.4014258 -4.4016581 -4.4017282 -4.401711 -4.4017262][-4.4018393 -4.4020286 -4.402349 -4.4026732 -4.4029226 -4.4029841 -4.4028597 -4.4024625 -4.4019532 -4.4017019 -4.4017515 -4.4018321 -4.4017854 -4.4016981 -4.40168][-4.4018188 -4.4019442 -4.4021616 -4.4023976 -4.4026556 -4.4028811 -4.40296 -4.4027414 -4.4023767 -4.402123 -4.4020448 -4.4019604 -4.4018087 -4.4016776 -4.4016418][-4.4017587 -4.401823 -4.4019322 -4.4020681 -4.4022465 -4.4025068 -4.4027328 -4.4027138 -4.4025283 -4.4023209 -4.40215 -4.4019742 -4.4017878 -4.4016461 -4.401608][-4.4016848 -4.40171 -4.401752 -4.401824 -4.4019327 -4.4021254 -4.4023633 -4.4024644 -4.40239 -4.402247 -4.402082 -4.4019074 -4.4017382 -4.4016213 -4.401588][-4.4016728 -4.4016814 -4.40169 -4.4017181 -4.4017696 -4.4018741 -4.4020362 -4.4021525 -4.4021459 -4.4020486 -4.4019227 -4.4017897 -4.4016776 -4.4016047 -4.4015837][-4.4017215 -4.4017291 -4.4017305 -4.4017191 -4.4017143 -4.4017439 -4.4018221 -4.4018941 -4.4018912 -4.4018269 -4.4017363 -4.4016562 -4.40161 -4.4015861 -4.4015908]]...]
INFO - root - 2017-12-05 09:45:08.673098: step 15110, loss = 1.40, batch loss = 1.34 (36.7 examples/sec; 0.218 sec/batch; 19h:13m:51s remains)
INFO - root - 2017-12-05 09:45:10.883042: step 15120, loss = 1.76, batch loss = 1.70 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:18s remains)
INFO - root - 2017-12-05 09:45:13.095170: step 15130, loss = 1.16, batch loss = 1.10 (34.9 examples/sec; 0.229 sec/batch; 20h:13m:21s remains)
INFO - root - 2017-12-05 09:45:15.273533: step 15140, loss = 1.34, batch loss = 1.28 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:30s remains)
INFO - root - 2017-12-05 09:45:17.439389: step 15150, loss = 1.14, batch loss = 1.08 (37.2 examples/sec; 0.215 sec/batch; 18h:57m:49s remains)
INFO - root - 2017-12-05 09:45:19.642933: step 15160, loss = 1.91, batch loss = 1.85 (36.6 examples/sec; 0.219 sec/batch; 19h:17m:28s remains)
INFO - root - 2017-12-05 09:45:21.867964: step 15170, loss = 2.10, batch loss = 2.04 (36.2 examples/sec; 0.221 sec/batch; 19h:29m:41s remains)
INFO - root - 2017-12-05 09:45:24.063539: step 15180, loss = 1.61, batch loss = 1.55 (36.4 examples/sec; 0.220 sec/batch; 19h:22m:30s remains)
INFO - root - 2017-12-05 09:45:26.250593: step 15190, loss = 1.39, batch loss = 1.33 (36.9 examples/sec; 0.217 sec/batch; 19h:05m:32s remains)
INFO - root - 2017-12-05 09:45:28.443518: step 15200, loss = 1.89, batch loss = 1.83 (36.2 examples/sec; 0.221 sec/batch; 19h:28m:08s remains)
2017-12-05 09:45:28.713242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9947944 -4.0502224 -4.1124492 -4.1491542 -4.1586585 -4.1458964 -4.1203847 -4.0903053 -4.0609365 -4.0445561 -4.0626812 -4.1154318 -4.1860933 -4.2544823 -4.3089795][-3.8843203 -3.935554 -3.9971492 -4.0413308 -4.0634046 -4.065609 -4.053174 -4.0287256 -3.9951692 -3.9717116 -3.9882278 -4.0486503 -4.1342492 -4.217411 -4.2808924][-3.7788167 -3.8211894 -3.8798568 -3.9290185 -3.9634261 -3.9788718 -3.97617 -3.95475 -3.9182138 -3.8904464 -3.9062314 -3.9721894 -4.0697603 -4.16475 -4.2322817][-3.6887858 -3.7128038 -3.7681148 -3.8227775 -3.8687694 -3.8955293 -3.9012811 -3.8848314 -3.8517272 -3.8263044 -3.8403025 -3.902729 -4.0000687 -4.0972781 -4.1624689][-3.6140437 -3.6188674 -3.6654172 -3.7220726 -3.7781951 -3.8184481 -3.8375206 -3.8343935 -3.8140285 -3.795414 -3.8036146 -3.8508446 -3.933737 -4.0229063 -4.0832047][-3.5678914 -3.5420551 -3.5691128 -3.6199584 -3.6813891 -3.7377684 -3.7778566 -3.7967057 -3.7953832 -3.7853742 -3.7863655 -3.8156772 -3.8800125 -3.9585268 -4.0171952][-3.5373373 -3.4787502 -3.4801481 -3.5146437 -3.5744922 -3.6448636 -3.70794 -3.7514462 -3.7702563 -3.7688372 -3.7654393 -3.7833753 -3.8367321 -3.9121938 -3.9786537][-3.5058622 -3.4196441 -3.4000711 -3.4179497 -3.4726851 -3.5510097 -3.6296935 -3.6897969 -3.7227778 -3.7293184 -3.7281117 -3.7442806 -3.7955317 -3.8740437 -3.9539623][-3.4634373 -3.3580527 -3.3298512 -3.3415956 -3.3940625 -3.4737601 -3.554666 -3.616842 -3.6530242 -3.6644957 -3.6683044 -3.6871619 -3.7384093 -3.8193653 -3.90983][-3.4059765 -3.2946916 -3.2737136 -3.2919879 -3.3458204 -3.4210091 -3.4920075 -3.5423143 -3.5710204 -3.5806148 -3.584244 -3.6001191 -3.6446095 -3.7197866 -3.8113403][-3.3453789 -3.2396832 -3.2355535 -3.2659783 -3.3207161 -3.3853567 -3.4392486 -3.4717054 -3.4873083 -3.489043 -3.4849689 -3.4901705 -3.5200088 -3.5794113 -3.6620293][-3.3094277 -3.2140541 -3.2250633 -3.2605257 -3.3098462 -3.3597336 -3.3977187 -3.417084 -3.4234586 -3.417398 -3.4043436 -3.3970606 -3.4099369 -3.4517255 -3.5221915][-3.320137 -3.2365546 -3.2574711 -3.2908077 -3.3309944 -3.369065 -3.4000931 -3.4191482 -3.4265029 -3.42042 -3.4036644 -3.3875895 -3.3869638 -3.4144437 -3.4739182][-3.406121 -3.3349609 -3.3576941 -3.3860092 -3.4195037 -3.4551427 -3.4915888 -3.5224743 -3.5402439 -3.5407667 -3.5272646 -3.5093675 -3.5029104 -3.5216627 -3.5717447][-3.5890772 -3.5337319 -3.550724 -3.5730298 -3.6021476 -3.6392527 -3.6824985 -3.7229288 -3.748189 -3.7540147 -3.7455292 -3.7310743 -3.7255239 -3.7412424 -3.7826221]]...]
INFO - root - 2017-12-05 09:45:30.934892: step 15210, loss = 1.22, batch loss = 1.16 (35.3 examples/sec; 0.226 sec/batch; 19h:56m:57s remains)
INFO - root - 2017-12-05 09:45:33.128068: step 15220, loss = 1.77, batch loss = 1.71 (35.2 examples/sec; 0.227 sec/batch; 20h:01m:26s remains)
INFO - root - 2017-12-05 09:45:35.293181: step 15230, loss = 1.61, batch loss = 1.55 (37.1 examples/sec; 0.215 sec/batch; 18h:59m:10s remains)
INFO - root - 2017-12-05 09:45:37.460791: step 15240, loss = 1.05, batch loss = 0.99 (36.4 examples/sec; 0.219 sec/batch; 19h:20m:36s remains)
INFO - root - 2017-12-05 09:45:39.655309: step 15250, loss = 1.38, batch loss = 1.32 (37.4 examples/sec; 0.214 sec/batch; 18h:50m:21s remains)
INFO - root - 2017-12-05 09:45:41.849526: step 15260, loss = 1.50, batch loss = 1.44 (36.3 examples/sec; 0.221 sec/batch; 19h:26m:06s remains)
INFO - root - 2017-12-05 09:45:44.031497: step 15270, loss = 1.51, batch loss = 1.45 (35.5 examples/sec; 0.225 sec/batch; 19h:51m:43s remains)
INFO - root - 2017-12-05 09:45:46.219256: step 15280, loss = 1.04, batch loss = 0.98 (36.2 examples/sec; 0.221 sec/batch; 19h:27m:24s remains)
INFO - root - 2017-12-05 09:45:48.419740: step 15290, loss = 1.31, batch loss = 1.25 (36.0 examples/sec; 0.222 sec/batch; 19h:35m:32s remains)
INFO - root - 2017-12-05 09:45:50.601661: step 15300, loss = 1.88, batch loss = 1.82 (37.0 examples/sec; 0.216 sec/batch; 19h:02m:00s remains)
2017-12-05 09:45:50.885799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1430893 -4.1201696 -4.0997915 -4.0870404 -4.0781593 -4.0643787 -4.0208554 -3.9054546 -3.6273139 -3.0711 -2.14766 -0.86565137 0.646224 2.2350335 3.6822453][-3.7880237 -3.7131617 -3.6460485 -3.602896 -3.5836689 -3.5740662 -3.5288 -3.3808417 -3.021441 -2.3382022 -1.2750745 0.13083601 1.7346334 3.36158 4.7865686][-3.1411617 -2.9467983 -2.7689543 -2.6533093 -2.6100006 -2.6208327 -2.6114278 -2.4755206 -2.0776989 -1.3101757 -0.15498877 1.3276677 2.9716258 4.5760365 5.9231815][-2.1055219 -1.72035 -1.3709149 -1.1432779 -1.0681226 -1.1255944 -1.2031913 -1.1428545 -0.77422523 0.008769989 1.1943626 2.7012391 4.3429976 5.8966837 7.1454735][-0.70358372 -0.074086189 0.48249912 0.843235 0.95503187 0.839715 0.6497879 0.58899069 0.86463118 1.5800862 2.7081022 4.1564779 5.7266016 7.186842 8.3235054][0.94225359 1.8255792 2.5822606 3.0696726 3.210484 3.0313945 2.7192206 2.5109944 2.6384754 3.2065797 4.1887407 5.4901896 6.9144268 8.2276049 9.236557][2.6669011 3.7617912 4.6669741 5.2446451 5.3981857 5.1605024 4.7387142 4.37963 4.32218 4.6781988 5.4372091 6.5188475 7.7436285 8.8917713 9.7893028][4.1912804 5.4299574 6.4130554 7.0343027 7.1966238 6.9409018 6.4679432 6.003602 5.7753277 5.9002113 6.3935194 7.2109509 8.2050714 9.1881266 9.9991684][5.2646236 6.5585213 7.5555754 8.1840105 8.3665123 8.1475029 7.7010393 7.2138357 6.8774767 6.8080368 7.0559397 7.6185312 8.38843 9.2224627 9.9737425][5.7310457 7.0041533 7.969285 8.6041718 8.8429546 8.72714 8.3826857 7.9529572 7.5844393 7.39159 7.4586697 7.821557 8.4152164 9.1347361 9.8499794][5.6840849 6.8703089 7.7815313 8.429697 8.762578 8.7993059 8.6097107 8.29144 7.9549303 7.7198687 7.6907673 7.9304647 8.4132118 9.0600977 9.7574615][5.3700924 6.4526587 7.2983265 7.9613266 8.4029121 8.6134138 8.6086054 8.4386339 8.1882439 7.9760952 7.9186449 8.0977116 8.5124092 9.107543 9.7792091][5.0124397 6.0135479 6.80424 7.4850626 8.0253334 8.3923349 8.5615807 8.5475111 8.4150076 8.2737093 8.2410107 8.3989515 8.7632236 9.297143 9.9120035][4.835187 5.7649736 6.4950666 7.1748476 7.7709131 8.2411652 8.5372486 8.6558886 8.64234 8.5874405 8.6008129 8.7576408 9.0773973 9.530324 10.050732][4.9171104 5.791357 6.4528379 7.1045628 7.7068605 8.2190237 8.5834084 8.7866488 8.8586092 8.8701878 8.9144373 9.0566683 9.3144474 9.6644125 10.055943]]...]
INFO - root - 2017-12-05 09:45:53.094333: step 15310, loss = 1.24, batch loss = 1.18 (36.4 examples/sec; 0.220 sec/batch; 19h:21m:58s remains)
INFO - root - 2017-12-05 09:45:55.352106: step 15320, loss = 1.37, batch loss = 1.31 (36.9 examples/sec; 0.217 sec/batch; 19h:05m:50s remains)
INFO - root - 2017-12-05 09:45:57.560426: step 15330, loss = 1.84, batch loss = 1.78 (36.2 examples/sec; 0.221 sec/batch; 19h:27m:58s remains)
INFO - root - 2017-12-05 09:45:59.764562: step 15340, loss = 1.89, batch loss = 1.83 (36.7 examples/sec; 0.218 sec/batch; 19h:11m:55s remains)
INFO - root - 2017-12-05 09:46:01.935923: step 15350, loss = 1.46, batch loss = 1.40 (36.2 examples/sec; 0.221 sec/batch; 19h:29m:27s remains)
INFO - root - 2017-12-05 09:46:04.147973: step 15360, loss = 0.91, batch loss = 0.86 (37.4 examples/sec; 0.214 sec/batch; 18h:49m:56s remains)
INFO - root - 2017-12-05 09:46:06.359095: step 15370, loss = 1.22, batch loss = 1.16 (37.4 examples/sec; 0.214 sec/batch; 18h:51m:28s remains)
INFO - root - 2017-12-05 09:46:08.579977: step 15380, loss = 1.56, batch loss = 1.50 (35.5 examples/sec; 0.225 sec/batch; 19h:49m:48s remains)
INFO - root - 2017-12-05 09:46:10.791188: step 15390, loss = 1.39, batch loss = 1.33 (37.1 examples/sec; 0.216 sec/batch; 18h:59m:21s remains)
INFO - root - 2017-12-05 09:46:12.977986: step 15400, loss = 1.35, batch loss = 1.30 (36.7 examples/sec; 0.218 sec/batch; 19h:12m:19s remains)
2017-12-05 09:46:13.253909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7133808 -1.6276147 -1.6915333 -1.863461 -2.0777097 -2.2379029 -2.2799532 -2.2021852 -2.0615597 -1.9357786 -1.877331 -1.8971167 -1.9637218 -2.0303767 -2.0555654][-1.5790248 -1.4506416 -1.4756279 -1.631624 -1.8507221 -2.0387964 -2.1149254 -2.0575283 -1.9115543 -1.7577915 -1.667356 -1.666415 -1.7283034 -1.7979467 -1.8333173][-1.5978086 -1.4431531 -1.4273453 -1.5391362 -1.7297568 -1.9143662 -2.0055256 -1.9659386 -1.8248966 -1.6577344 -1.5428691 -1.5292063 -1.5965898 -1.6867037 -1.7461979][-1.6124864 -1.4389687 -1.3920264 -1.4602056 -1.6099777 -1.7669988 -1.8476088 -1.8069448 -1.6681802 -1.5025933 -1.3899312 -1.3919849 -1.4936197 -1.6266601 -1.7273498][-1.6589613 -1.473304 -1.4082007 -1.4418323 -1.551007 -1.6672289 -1.718328 -1.6654594 -1.5327396 -1.3858733 -1.300447 -1.3377843 -1.484225 -1.6624219 -1.8018334][-1.7640922 -1.5666356 -1.4926167 -1.5106308 -1.5968256 -1.6808984 -1.7024102 -1.6370995 -1.5152001 -1.3963158 -1.347708 -1.419714 -1.5978572 -1.8026686 -1.9612167][-1.9772005 -1.7728734 -1.6968508 -1.7131889 -1.7887056 -1.8482563 -1.8443263 -1.7711763 -1.6645713 -1.5783813 -1.5685987 -1.6670315 -1.8546734 -2.05809 -2.2076778][-2.3432055 -2.1364229 -2.0470483 -2.0500655 -2.1040025 -2.1319582 -2.1029489 -2.0288537 -1.9460719 -1.9003897 -1.9313946 -2.0513635 -2.2322955 -2.4110961 -2.5301571][-2.8158708 -2.6171722 -2.5116417 -2.4822316 -2.4938002 -2.4805074 -2.4249232 -2.3507886 -2.2939529 -2.2899063 -2.3607571 -2.500586 -2.6721444 -2.8209457 -2.9037757][-3.3162816 -3.1383495 -3.0214186 -2.9543433 -2.9157155 -2.8615856 -2.7835846 -2.7095571 -2.6775308 -2.7125459 -2.8192711 -2.975565 -3.1361578 -3.2562861 -3.308877][-3.7467291 -3.5999961 -3.4816055 -3.388442 -3.3133259 -3.2352834 -3.1526909 -3.0880756 -3.0782461 -3.1394541 -3.2644942 -3.4220016 -3.5658908 -3.6601293 -3.6931887][-4.0611105 -3.9531705 -3.8511045 -3.7545667 -3.6690929 -3.5915611 -3.5221069 -3.4754574 -3.4820647 -3.5490677 -3.6636229 -3.7977982 -3.9136937 -3.9834764 -4.0059805][-4.2485046 -4.1806188 -4.1070795 -4.0302153 -3.9601741 -3.9017472 -3.8552642 -3.8290405 -3.8425608 -3.8975115 -3.9816701 -4.0758257 -4.1549029 -4.2012911 -4.2172832][-4.3386726 -4.30237 -4.2582955 -4.2095957 -4.1652093 -4.1313796 -4.10913 -4.0996585 -4.1121016 -4.1459646 -4.1933274 -4.2444329 -4.2870078 -4.3132458 -4.3252339][-4.3765373 -4.3594179 -4.3370538 -4.3118048 -4.2892294 -4.2741647 -4.2664886 -4.2657342 -4.2735853 -4.2886419 -4.3082023 -4.329113 -4.3473239 -4.3604212 -4.3686438]]...]
INFO - root - 2017-12-05 09:46:15.431397: step 15410, loss = 1.29, batch loss = 1.23 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:59s remains)
INFO - root - 2017-12-05 09:46:17.588517: step 15420, loss = 1.19, batch loss = 1.13 (37.5 examples/sec; 0.213 sec/batch; 18h:46m:36s remains)
INFO - root - 2017-12-05 09:46:19.775872: step 15430, loss = 1.50, batch loss = 1.44 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:39s remains)
INFO - root - 2017-12-05 09:46:22.007036: step 15440, loss = 1.61, batch loss = 1.56 (36.7 examples/sec; 0.218 sec/batch; 19h:12m:21s remains)
INFO - root - 2017-12-05 09:46:24.178033: step 15450, loss = 1.11, batch loss = 1.05 (36.8 examples/sec; 0.217 sec/batch; 19h:09m:11s remains)
INFO - root - 2017-12-05 09:46:26.368473: step 15460, loss = 1.59, batch loss = 1.53 (36.8 examples/sec; 0.217 sec/batch; 19h:07m:29s remains)
INFO - root - 2017-12-05 09:46:28.558900: step 15470, loss = 1.64, batch loss = 1.58 (37.1 examples/sec; 0.216 sec/batch; 18h:59m:33s remains)
INFO - root - 2017-12-05 09:46:30.754121: step 15480, loss = 1.86, batch loss = 1.80 (37.5 examples/sec; 0.214 sec/batch; 18h:48m:15s remains)
INFO - root - 2017-12-05 09:46:32.933234: step 15490, loss = 1.16, batch loss = 1.10 (37.1 examples/sec; 0.216 sec/batch; 18h:58m:39s remains)
INFO - root - 2017-12-05 09:46:35.121155: step 15500, loss = 1.51, batch loss = 1.45 (37.0 examples/sec; 0.217 sec/batch; 19h:03m:53s remains)
2017-12-05 09:46:35.438238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9682155 -3.8618512 -3.2872424 -2.1568105 -0.60576129 1.0204363 2.3016915 2.8923697 2.6372409 1.6645455 0.28789568 -1.1259892 -2.3163083 -3.1501708 -3.55806][-3.969347 -3.7000375 -2.8772066 -1.4268575 0.44711018 2.3114715 3.6919441 4.2335358 3.8155041 2.6393909 1.0575213 -0.55161858 -1.9186592 -2.9160118 -3.493818][-3.9455009 -3.5013463 -2.4217734 -0.64836812 1.553143 3.6801758 5.2064495 5.7636776 5.2545366 3.9080191 2.1038136 0.24317837 -1.3754423 -2.6023676 -3.3786397][-3.8964608 -3.2909725 -1.9777944 0.083303452 2.5836482 4.9816427 6.7160568 7.3848143 6.8730659 5.3960667 3.3484416 1.1726899 -0.76907659 -2.2731056 -3.2491813][-3.8468637 -3.0968676 -1.5905118 0.68520069 3.4025989 6.0173454 7.9484529 8.74787 8.2602072 6.6801662 4.4087887 1.9369216 -0.30389547 -2.047966 -3.1744874][-3.8027635 -2.9259186 -1.2722085 1.1350307 3.9611425 6.6775761 8.6994143 9.5462484 9.0467386 7.3770056 4.9460106 2.2807851 -0.14158058 -2.01477 -3.2033243][-3.7558119 -2.7729473 -1.0158162 1.4473805 4.2694511 6.9405994 8.8935366 9.6479673 9.0497265 7.2872515 4.7840357 2.0813189 -0.34392452 -2.1865308 -3.3267708][-3.6947551 -2.6416264 -0.85190749 1.5644994 4.2504005 6.720767 8.4472466 8.9876528 8.2353554 6.4114208 3.9517908 1.3795776 -0.86609817 -2.5232372 -3.5162253][-3.6062486 -2.5232675 -0.78091979 1.4748149 3.8861771 6.0061579 7.371314 7.625865 6.72569 4.9164629 2.6259499 0.33004951 -1.5945911 -2.9568172 -3.7356153][-3.5037868 -2.4385986 -0.82323146 1.1711993 3.2092829 4.89773 5.847436 5.8025112 4.7893543 3.078146 1.0616851 -0.85812807 -2.3879805 -3.4078622 -3.9469793][-3.4256082 -2.4289923 -1.0050979 0.66507578 2.2859707 3.5325632 4.0931835 3.8002052 2.7428131 1.2079039 -0.46495056 -1.965559 -3.0924592 -3.7896 -4.1168303][-3.4234769 -2.5518913 -1.3723602 -0.054265022 1.1547732 1.9991274 2.2478566 1.810092 0.80925655 -0.47881627 -1.776547 -2.8658991 -3.6278663 -4.0567179 -4.2249713][-3.5231636 -2.8266521 -1.9283094 -0.96754932 -0.1366539 0.37900496 0.42437553 -0.033499241 -0.87032461 -1.8562949 -2.7836962 -3.5123062 -3.9848385 -4.2220144 -4.2904625][-3.7165227 -3.2235112 -2.6131051 -1.9858408 -1.472914 -1.1954954 -1.2475464 -1.627017 -2.232619 -2.9002314 -3.4937677 -3.9332457 -4.1974425 -4.3152866 -4.3358369][-3.9708202 -3.6808193 -3.3310366 -2.9810436 -2.7084541 -2.5819952 -2.6516342 -2.9011106 -3.2647743 -3.6452186 -3.9678986 -4.193089 -4.317668 -4.3669338 -4.3700118]]...]
INFO - root - 2017-12-05 09:46:37.618887: step 15510, loss = 2.12, batch loss = 2.06 (37.2 examples/sec; 0.215 sec/batch; 18h:56m:52s remains)
INFO - root - 2017-12-05 09:46:39.829804: step 15520, loss = 1.26, batch loss = 1.20 (37.4 examples/sec; 0.214 sec/batch; 18h:51m:09s remains)
INFO - root - 2017-12-05 09:46:42.028636: step 15530, loss = 1.72, batch loss = 1.66 (35.9 examples/sec; 0.223 sec/batch; 19h:35m:37s remains)
INFO - root - 2017-12-05 09:46:44.213341: step 15540, loss = 1.65, batch loss = 1.59 (37.1 examples/sec; 0.216 sec/batch; 18h:59m:29s remains)
INFO - root - 2017-12-05 09:46:46.432429: step 15550, loss = 1.32, batch loss = 1.26 (35.6 examples/sec; 0.225 sec/batch; 19h:46m:17s remains)
INFO - root - 2017-12-05 09:46:48.618672: step 15560, loss = 1.26, batch loss = 1.20 (37.1 examples/sec; 0.216 sec/batch; 18h:59m:59s remains)
INFO - root - 2017-12-05 09:46:50.797437: step 15570, loss = 1.46, batch loss = 1.40 (37.3 examples/sec; 0.215 sec/batch; 18h:54m:11s remains)
INFO - root - 2017-12-05 09:46:52.984723: step 15580, loss = 1.22, batch loss = 1.16 (36.0 examples/sec; 0.222 sec/batch; 19h:32m:40s remains)
INFO - root - 2017-12-05 09:46:55.189558: step 15590, loss = 1.28, batch loss = 1.22 (36.2 examples/sec; 0.221 sec/batch; 19h:27m:48s remains)
INFO - root - 2017-12-05 09:46:57.381607: step 15600, loss = 1.14, batch loss = 1.08 (36.3 examples/sec; 0.220 sec/batch; 19h:22m:42s remains)
2017-12-05 09:46:57.653718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.97500753 -1.3345006 -1.7977052 -2.2530444 -2.6278465 -2.8841143 -3.0193462 -3.0660286 -3.0618792 -3.0373607 -3.0106306 -2.9937291 -2.9800813 -2.9637711 -2.9759645][-0.19513798 -0.57222652 -1.1304748 -1.729382 -2.24158 -2.6002939 -2.7948756 -2.8640735 -2.8621559 -2.8358002 -2.8156018 -2.8087258 -2.8067074 -2.8003831 -2.8226018][0.30113411 -0.0022807121 -0.5526247 -1.2006381 -1.7880974 -2.2150025 -2.4561796 -2.5531311 -2.5697303 -2.5622115 -2.5687547 -2.5967245 -2.6282244 -2.6505389 -2.6953392][0.50516272 0.32987833 -0.11338234 -0.69169545 -1.2525156 -1.6800218 -1.9344738 -2.0542748 -2.105417 -2.1458941 -2.213604 -2.3109438 -2.4082546 -2.4890797 -2.5766864][0.45208216 0.4037075 0.12511778 -0.28884602 -0.71836519 -1.0633574 -1.2806661 -1.404793 -1.5004697 -1.6209202 -1.7932017 -2.0039425 -2.206748 -2.3767872 -2.5301037][0.21061325 0.25441217 0.1408658 -0.06848526 -0.29672003 -0.48411846 -0.60843253 -0.70733094 -0.84342623 -1.0639825 -1.3775029 -1.7434978 -2.0920222 -2.3804302 -2.6169543][-0.17073536 -0.072333813 -0.039872169 -0.034858227 -0.033251286 -0.023416996 -0.018608093 -0.069925308 -0.23816061 -0.56377721 -1.033246 -1.5742085 -2.0878658 -2.5071411 -2.8305557][-0.70286083 -0.54917 -0.37511349 -0.15682983 0.08127594 0.29272032 0.42506218 0.41858768 0.21467352 -0.21448755 -0.82807541 -1.523684 -2.1806026 -2.7118621 -3.1053345][-1.3852439 -1.1592212 -0.83512878 -0.41020775 0.047597408 0.43577814 0.66406775 0.67166758 0.41473246 -0.106359 -0.82232594 -1.6106982 -2.3452837 -2.9366722 -3.3649647][-2.1134262 -1.8112221 -1.3584268 -0.77465105 -0.16106129 0.34395742 0.62307692 0.61670876 0.3057766 -0.27341223 -1.0290747 -1.8313956 -2.5654931 -3.15385 -3.5742075][-2.7549844 -2.4137912 -1.9002662 -1.2538824 -0.59098434 -0.057975769 0.22405052 0.20124054 -0.12943697 -0.7057941 -1.426182 -2.1681018 -2.8353877 -3.3664532 -3.740376][-3.2250552 -2.9042103 -2.4249136 -1.8304479 -1.2335346 -0.76180983 -0.52354932 -0.56166124 -0.87115955 -1.3787694 -1.9909377 -2.6063759 -3.1520844 -3.5825453 -3.8820965][-3.5306501 -3.2751513 -2.9034657 -2.4521661 -2.0095918 -1.6659522 -1.5051825 -1.5555503 -1.8077247 -2.1947875 -2.6481216 -3.0968137 -3.4894454 -3.7960732 -4.0082765][-3.7374215 -3.5643084 -3.3242767 -3.0418377 -2.7768292 -2.5811176 -2.5058453 -2.5608869 -2.7376957 -2.988596 -3.2748117 -3.554461 -3.7972102 -3.9864647 -4.1181355][-3.9208443 -3.8196616 -3.6892309 -3.5437431 -3.4175947 -3.3353395 -3.3205862 -3.3701801 -3.4779587 -3.6153817 -3.7669153 -3.9130709 -4.0397563 -4.1396952 -4.2102404]]...]
INFO - root - 2017-12-05 09:46:59.822295: step 15610, loss = 1.70, batch loss = 1.64 (37.6 examples/sec; 0.213 sec/batch; 18h:44m:24s remains)
INFO - root - 2017-12-05 09:47:02.065568: step 15620, loss = 2.14, batch loss = 2.08 (36.8 examples/sec; 0.218 sec/batch; 19h:09m:26s remains)
INFO - root - 2017-12-05 09:47:04.242382: step 15630, loss = 1.20, batch loss = 1.14 (36.5 examples/sec; 0.219 sec/batch; 19h:16m:00s remains)
INFO - root - 2017-12-05 09:47:06.452065: step 15640, loss = 1.84, batch loss = 1.78 (37.5 examples/sec; 0.213 sec/batch; 18h:45m:22s remains)
INFO - root - 2017-12-05 09:47:08.637069: step 15650, loss = 1.70, batch loss = 1.64 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:45s remains)
INFO - root - 2017-12-05 09:47:10.823814: step 15660, loss = 1.18, batch loss = 1.13 (36.2 examples/sec; 0.221 sec/batch; 19h:27m:21s remains)
INFO - root - 2017-12-05 09:47:13.015820: step 15670, loss = 1.42, batch loss = 1.36 (37.3 examples/sec; 0.215 sec/batch; 18h:52m:59s remains)
INFO - root - 2017-12-05 09:47:15.214502: step 15680, loss = 1.18, batch loss = 1.12 (36.1 examples/sec; 0.221 sec/batch; 19h:29m:18s remains)
INFO - root - 2017-12-05 09:47:17.405056: step 15690, loss = 1.34, batch loss = 1.28 (37.0 examples/sec; 0.216 sec/batch; 19h:02m:28s remains)
INFO - root - 2017-12-05 09:47:19.618611: step 15700, loss = 1.64, batch loss = 1.58 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:35s remains)
2017-12-05 09:47:19.897180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7507977 -3.800292 -3.7528367 -3.5796909 -3.3024719 -3.000078 -2.7892821 -2.7653399 -2.9471059 -3.2710195 -3.6346488 -3.9439685 -4.1570683 -4.2726855 -4.3119483][-3.6122329 -3.6856546 -3.6514683 -3.4837158 -3.2204165 -2.9563274 -2.8122783 -2.8604305 -3.0812616 -3.4000192 -3.7265708 -3.9912078 -4.1702981 -4.2668204 -4.2987676][-3.5311611 -3.6255851 -3.6094863 -3.4540241 -3.2038314 -2.9646547 -2.864099 -2.9650636 -3.2214441 -3.5411677 -3.8364313 -4.057251 -4.1975651 -4.2696247 -4.2920494][-3.5224814 -3.628485 -3.6270046 -3.4832597 -3.2392488 -3.0016518 -2.9019151 -3.0046272 -3.262918 -3.5773594 -3.8601573 -4.0657749 -4.194262 -4.2621622 -4.2875481][-3.5983372 -3.6968806 -3.6991923 -3.566283 -3.3293936 -3.0843489 -2.9563758 -3.0159695 -3.2351086 -3.5251515 -3.7993236 -4.0096483 -4.1511655 -4.237154 -4.2812142][-3.7448251 -3.813787 -3.8039134 -3.6765361 -3.4472303 -3.1944022 -3.0258718 -3.0153654 -3.1622734 -3.4055839 -3.6667738 -3.8931909 -4.0668511 -4.1895533 -4.2660971][-3.91773 -3.9420435 -3.902842 -3.7644935 -3.5292134 -3.2594008 -3.0455234 -2.9601958 -3.0282917 -3.2206285 -3.4719224 -3.7253549 -3.9460506 -4.1196184 -4.2379513][-4.0629482 -4.0383739 -3.9580717 -3.7892621 -3.5263772 -3.2224116 -2.9580681 -2.8083546 -2.8164611 -2.9769506 -3.2351956 -3.5277467 -3.8038778 -4.0338755 -4.1981535][-4.1521387 -4.0814552 -3.9532123 -3.7390041 -3.4319859 -3.08075 -2.7649393 -2.5674045 -2.5463483 -2.7072155 -2.996695 -3.3390756 -3.671314 -3.9524367 -4.1568236][-4.18472 -4.0735321 -3.8970778 -3.6343522 -3.282515 -2.8903422 -2.5384851 -2.3183105 -2.2969918 -2.4839 -2.8184323 -3.2105196 -3.5864632 -3.9010105 -4.1294785][-4.18321 -4.0353475 -3.815212 -3.5137312 -3.1359739 -2.730932 -2.3773177 -2.1669979 -2.1691494 -2.3906009 -2.7645674 -3.1890543 -3.5834508 -3.9037905 -4.1327634][-4.17159 -3.9956489 -3.7448976 -3.4250658 -3.0539055 -2.6785808 -2.3682494 -2.2017274 -2.2407796 -2.4865904 -2.8723364 -3.2954357 -3.6750298 -3.9710233 -4.1746569][-4.1702657 -3.9842398 -3.7260509 -3.4156623 -3.0843239 -2.7759736 -2.5433204 -2.4431474 -2.5175152 -2.7610173 -3.1164992 -3.4934514 -3.8225543 -4.0698066 -4.2317877][-4.1890945 -4.0161986 -3.7791698 -3.5074062 -3.2406538 -3.019166 -2.8756938 -2.8400354 -2.9327011 -3.1421032 -3.4265175 -3.7194395 -3.9708064 -4.1559787 -4.2730727][-4.2234254 -4.0839286 -3.8915083 -3.6793015 -3.4884636 -3.3533683 -3.2891428 -3.3005576 -3.3888421 -3.5395522 -3.7290177 -3.9198334 -4.08375 -4.2054768 -4.2827106]]...]
INFO - root - 2017-12-05 09:47:22.096177: step 15710, loss = 1.19, batch loss = 1.13 (37.3 examples/sec; 0.214 sec/batch; 18h:51m:43s remains)
INFO - root - 2017-12-05 09:47:24.299691: step 15720, loss = 1.36, batch loss = 1.30 (35.6 examples/sec; 0.225 sec/batch; 19h:46m:07s remains)
INFO - root - 2017-12-05 09:47:26.497206: step 15730, loss = 1.39, batch loss = 1.33 (37.0 examples/sec; 0.216 sec/batch; 19h:02m:37s remains)
INFO - root - 2017-12-05 09:47:28.677000: step 15740, loss = 1.26, batch loss = 1.20 (35.6 examples/sec; 0.225 sec/batch; 19h:46m:30s remains)
INFO - root - 2017-12-05 09:47:30.864851: step 15750, loss = 1.09, batch loss = 1.03 (36.7 examples/sec; 0.218 sec/batch; 19h:10m:51s remains)
INFO - root - 2017-12-05 09:47:33.051774: step 15760, loss = 1.66, batch loss = 1.60 (35.5 examples/sec; 0.225 sec/batch; 19h:48m:05s remains)
INFO - root - 2017-12-05 09:47:35.238039: step 15770, loss = 1.13, batch loss = 1.07 (37.1 examples/sec; 0.216 sec/batch; 18h:59m:32s remains)
INFO - root - 2017-12-05 09:47:37.466882: step 15780, loss = 1.37, batch loss = 1.31 (36.3 examples/sec; 0.220 sec/batch; 19h:22m:53s remains)
INFO - root - 2017-12-05 09:47:39.673380: step 15790, loss = 1.58, batch loss = 1.52 (36.6 examples/sec; 0.218 sec/batch; 19h:12m:26s remains)
INFO - root - 2017-12-05 09:47:41.880319: step 15800, loss = 1.37, batch loss = 1.32 (36.8 examples/sec; 0.217 sec/batch; 19h:07m:22s remains)
2017-12-05 09:47:42.154759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3866043 -4.3870921 -4.385282 -4.3806577 -4.3714161 -4.3564429 -4.3357086 -4.3125944 -4.29317 -4.28507 -4.2907252 -4.3083143 -4.3337064 -4.358891 -4.3773832][-4.3707538 -4.3713846 -4.3686237 -4.3607974 -4.3461137 -4.3258238 -4.302361 -4.2799468 -4.2644696 -4.26185 -4.27354 -4.29714 -4.3274736 -4.3548522 -4.37378][-4.3334384 -4.3377604 -4.3366904 -4.327148 -4.3084197 -4.2841377 -4.2595677 -4.2395597 -4.2288876 -4.2318282 -4.248703 -4.2778711 -4.3132377 -4.3436651 -4.365119][-4.2668552 -4.2804413 -4.2865477 -4.2796082 -4.2596912 -4.23244 -4.2061582 -4.1868424 -4.1791224 -4.1858449 -4.2075777 -4.2435026 -4.2860351 -4.3233004 -4.3513985][-4.1791406 -4.2057443 -4.2236567 -4.2238703 -4.2051163 -4.1735311 -4.1413269 -4.1167831 -4.1063194 -4.1144323 -4.1425519 -4.1886725 -4.2428966 -4.2925076 -4.3325777][-4.103322 -4.1393652 -4.1665955 -4.172647 -4.1537576 -4.1154785 -4.0730228 -4.0369906 -4.0178366 -4.025393 -4.0621662 -4.1225495 -4.1930227 -4.2587295 -4.312675][-4.082541 -4.1173997 -4.144556 -4.1487932 -4.124423 -4.0766392 -4.0218639 -3.9729583 -3.9444337 -3.9521198 -3.99792 -4.0716558 -4.1567626 -4.2365456 -4.3011394][-4.135056 -4.1587262 -4.17535 -4.1704326 -4.1382761 -4.0833921 -4.0205703 -3.963968 -3.930877 -3.9378803 -3.9865582 -4.0647335 -4.1542039 -4.2380319 -4.3046327][-4.2283196 -4.2388744 -4.2428565 -4.2296519 -4.1950779 -4.1425214 -4.0830321 -4.0295997 -3.9981019 -4.0031123 -4.0452957 -4.1139727 -4.1922569 -4.2652359 -4.3226562][-4.311974 -4.3144865 -4.3114243 -4.2967315 -4.2687125 -4.2290549 -4.1847854 -4.1452646 -4.1215806 -4.1239734 -4.1532764 -4.2017817 -4.2568145 -4.3083367 -4.3488278][-4.3655591 -4.3648362 -4.3601055 -4.3490324 -4.3312654 -4.3074813 -4.2815022 -4.2587671 -4.2448993 -4.2458897 -4.2623129 -4.2892833 -4.3201652 -4.3497286 -4.3731341][-4.3915963 -4.3903856 -4.3869834 -4.380713 -4.3716655 -4.3601723 -4.348001 -4.3375807 -4.3310938 -4.3314228 -4.3387194 -4.350636 -4.3647604 -4.3789277 -4.3902225][-4.4012632 -4.4005618 -4.3988166 -4.3959508 -4.3921494 -4.387712 -4.3832917 -4.3796892 -4.3775921 -4.3777952 -4.38012 -4.3840175 -4.3891244 -4.3946428 -4.3991947][-4.4036674 -4.4034014 -4.4027853 -4.401835 -4.4006333 -4.3993435 -4.3980751 -4.3970494 -4.3966341 -4.3968272 -4.397368 -4.3982797 -4.3997583 -4.4015045 -4.4029932][-4.4040723 -4.4039941 -4.4038539 -4.4036093 -4.4032965 -4.4030252 -4.4028115 -4.4026465 -4.4026875 -4.4028478 -4.4029903 -4.403162 -4.4034963 -4.4039435 -4.4042826]]...]
INFO - root - 2017-12-05 09:47:44.338166: step 15810, loss = 1.97, batch loss = 1.91 (35.6 examples/sec; 0.225 sec/batch; 19h:45m:14s remains)
INFO - root - 2017-12-05 09:47:46.507933: step 15820, loss = 2.33, batch loss = 2.28 (36.6 examples/sec; 0.219 sec/batch; 19h:13m:18s remains)
INFO - root - 2017-12-05 09:47:48.700392: step 15830, loss = 1.34, batch loss = 1.28 (36.7 examples/sec; 0.218 sec/batch; 19h:11m:55s remains)
INFO - root - 2017-12-05 09:47:50.896268: step 15840, loss = 1.46, batch loss = 1.40 (37.1 examples/sec; 0.216 sec/batch; 18h:59m:02s remains)
INFO - root - 2017-12-05 09:47:53.068470: step 15850, loss = 1.84, batch loss = 1.78 (37.5 examples/sec; 0.213 sec/batch; 18h:45m:57s remains)
INFO - root - 2017-12-05 09:47:55.272796: step 15860, loss = 0.99, batch loss = 0.93 (35.5 examples/sec; 0.225 sec/batch; 19h:49m:57s remains)
INFO - root - 2017-12-05 09:47:57.448552: step 15870, loss = 1.26, batch loss = 1.20 (37.9 examples/sec; 0.211 sec/batch; 18h:34m:25s remains)
INFO - root - 2017-12-05 09:47:59.644694: step 15880, loss = 1.76, batch loss = 1.70 (37.2 examples/sec; 0.215 sec/batch; 18h:53m:26s remains)
INFO - root - 2017-12-05 09:48:01.828999: step 15890, loss = 1.05, batch loss = 0.99 (37.0 examples/sec; 0.216 sec/batch; 19h:01m:00s remains)
INFO - root - 2017-12-05 09:48:04.018345: step 15900, loss = 1.60, batch loss = 1.54 (35.4 examples/sec; 0.226 sec/batch; 19h:53m:22s remains)
2017-12-05 09:48:04.319437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8820724 -3.0037804 -3.0380592 -2.9497418 -2.7492194 -2.4814236 -2.2021418 -1.9633076 -1.8012402 -1.7244375 -1.7358701 -1.8201368 -1.9599106 -2.1333065 -2.3150966][-3.0026486 -3.1099172 -3.0873404 -2.9263797 -2.6653738 -2.3517828 -2.0470035 -1.8018427 -1.6401651 -1.5554132 -1.5419335 -1.5876431 -1.6911073 -1.8462026 -2.0357218][-3.1404505 -3.1598721 -3.019084 -2.7402859 -2.393388 -2.0294557 -1.717644 -1.4981349 -1.3712275 -1.3102541 -1.2962568 -1.324753 -1.4123955 -1.5705156 -1.7899148][-3.1931057 -3.0539861 -2.74699 -2.3155847 -1.8601098 -1.4429286 -1.1369064 -0.96089745 -0.88648367 -0.86583161 -0.86956692 -0.90176129 -1.001087 -1.1956823 -1.4784255][-3.062233 -2.7301936 -2.2390003 -1.6435375 -1.073611 -0.61104822 -0.32844305 -0.21269083 -0.2002387 -0.22170258 -0.24130535 -0.28054523 -0.39861298 -0.642601 -1.0093706][-2.7052879 -2.1827326 -1.513485 -0.76970005 -0.10269308 0.38459682 0.62601662 0.66616583 0.6053834 0.5362525 0.50349665 0.4688592 0.35004473 0.072057247 -0.36921692][-2.1253932 -1.4358857 -0.62382817 0.22451401 0.94926929 1.433475 1.6191478 1.5792742 1.4427509 1.3214812 1.2669888 1.2337151 1.1216688 0.8302865 0.33950996][-1.4124918 -0.60039186 0.29576683 1.1901979 1.9241285 2.3743949 2.5009384 2.3935905 2.1925006 2.017065 1.9230924 1.866652 1.7399383 1.4293275 0.89681005][-0.71768069 0.15260744 1.0589213 1.9419475 2.643765 3.0489435 3.1270552 2.9687324 2.7164803 2.4866891 2.3293548 2.2084742 2.0227294 1.6606607 1.0742698][-0.13600922 0.71298552 1.5376253 2.3426471 2.9754505 3.3348465 3.3875456 3.2035909 2.9142318 2.6303091 2.3947916 2.179987 1.8964214 1.4520407 0.799768][0.29500818 1.0422282 1.7050486 2.3624816 2.8850698 3.1916122 3.2379265 3.0592847 2.7587433 2.4323044 2.1180406 1.8005242 1.4082155 0.87144375 0.15608692][0.59337759 1.1939988 1.6451411 2.1040268 2.4802222 2.7098875 2.7478566 2.5980058 2.3183308 1.9769421 1.6026278 1.1967001 0.70997906 0.095286369 -0.66355014][0.75738621 1.2266355 1.4850402 1.7468162 1.9626918 2.0960083 2.1087418 1.9873767 1.7491169 1.4212518 1.0177116 0.5472908 -0.014939785 -0.68680859 -1.4618895][0.74668026 1.1402268 1.2826519 1.4068875 1.4965367 1.5446687 1.5261736 1.4211316 1.2234206 0.9266634 0.52234173 0.011440277 -0.60533357 -1.3114893 -2.0725312][0.48163557 0.86945486 0.9933219 1.0656672 1.0926266 1.0891933 1.0490866 0.95477295 0.79210234 0.53518152 0.15602684 -0.36008263 -0.99850035 -1.7081881 -2.4301472]]...]
INFO - root - 2017-12-05 09:48:06.496056: step 15910, loss = 1.40, batch loss = 1.35 (36.8 examples/sec; 0.217 sec/batch; 19h:06m:16s remains)
INFO - root - 2017-12-05 09:48:08.714687: step 15920, loss = 1.90, batch loss = 1.85 (36.7 examples/sec; 0.218 sec/batch; 19h:11m:17s remains)
INFO - root - 2017-12-05 09:48:10.917763: step 15930, loss = 1.18, batch loss = 1.12 (37.1 examples/sec; 0.216 sec/batch; 18h:58m:49s remains)
INFO - root - 2017-12-05 09:48:13.109529: step 15940, loss = 1.24, batch loss = 1.18 (36.5 examples/sec; 0.219 sec/batch; 19h:15m:19s remains)
INFO - root - 2017-12-05 09:48:15.283018: step 15950, loss = 0.97, batch loss = 0.91 (36.9 examples/sec; 0.217 sec/batch; 19h:04m:58s remains)
INFO - root - 2017-12-05 09:48:17.459420: step 15960, loss = 1.34, batch loss = 1.28 (36.8 examples/sec; 0.217 sec/batch; 19h:06m:48s remains)
INFO - root - 2017-12-05 09:48:19.670755: step 15970, loss = 1.48, batch loss = 1.42 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:10s remains)
INFO - root - 2017-12-05 09:48:21.863507: step 15980, loss = 1.47, batch loss = 1.41 (37.3 examples/sec; 0.215 sec/batch; 18h:52m:56s remains)
INFO - root - 2017-12-05 09:48:24.029957: step 15990, loss = 1.15, batch loss = 1.09 (36.6 examples/sec; 0.219 sec/batch; 19h:13m:20s remains)
INFO - root - 2017-12-05 09:48:26.228893: step 16000, loss = 1.25, batch loss = 1.19 (36.8 examples/sec; 0.217 sec/batch; 19h:06m:33s remains)
2017-12-05 09:48:26.537972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4026818 -4.4026589 -4.4026966 -4.402761 -4.4028077 -4.4028192 -4.4028387 -4.4028625 -4.4028759 -4.4029427 -4.4029593 -4.4027982 -4.4022684 -4.4013405 -4.4004054][-4.4031177 -4.4031014 -4.403182 -4.4033036 -4.403388 -4.4034319 -4.4034705 -4.4035053 -4.4035077 -4.4035654 -4.4036255 -4.4034867 -4.4028015 -4.40155 -4.4002171][-4.4035997 -4.4035759 -4.403688 -4.4038687 -4.40401 -4.4040852 -4.4041123 -4.4041104 -4.40406 -4.404067 -4.4041538 -4.4039903 -4.4032927 -4.40194 -4.4002929][-4.4038639 -4.4038739 -4.4040217 -4.4042554 -4.4044328 -4.4045362 -4.404582 -4.4045362 -4.4043941 -4.4042993 -4.4043083 -4.4040041 -4.403172 -4.4017963 -4.3999424][-4.4038973 -4.4038811 -4.4040132 -4.4042754 -4.4045119 -4.4046669 -4.4047251 -4.404634 -4.4044333 -4.4042487 -4.40409 -4.4036255 -4.4027395 -4.4012737 -4.3991823][-4.4037628 -4.4036994 -4.4038076 -4.4040956 -4.4043107 -4.4043884 -4.4043579 -4.4042544 -4.40416 -4.403954 -4.4036613 -4.4031315 -4.4022923 -4.4007797 -4.3985138][-4.403419 -4.4033947 -4.4035363 -4.4038138 -4.4038963 -4.4036932 -4.403398 -4.4032226 -4.4032826 -4.4032702 -4.4029813 -4.4025078 -4.4017735 -4.4002886 -4.3981376][-4.4027786 -4.4027638 -4.4028916 -4.4030528 -4.4028664 -4.4022107 -4.4014721 -4.4011607 -4.4015121 -4.40194 -4.4019566 -4.4017043 -4.4011416 -4.3998308 -4.3979497][-4.4023337 -4.4022489 -4.4022045 -4.4020243 -4.4014578 -4.4004683 -4.3995504 -4.3993492 -4.3999281 -4.4006529 -4.4009666 -4.4009624 -4.4006186 -4.3995686 -4.3980031][-4.4022756 -4.4019942 -4.4015908 -4.4009671 -4.4000773 -4.3990726 -4.3983464 -4.3983088 -4.3988957 -4.3996649 -4.400176 -4.4003429 -4.400187 -4.3993974 -4.3981352][-4.402184 -4.4016647 -4.4009466 -4.4000335 -4.3990188 -4.3981819 -4.3976545 -4.397644 -4.3980517 -4.3987656 -4.3994327 -4.3998284 -4.3998508 -4.3992496 -4.3981829][-4.4006619 -4.3999214 -4.399065 -4.3981295 -4.3973355 -4.3969 -4.3966775 -4.3967562 -4.3970923 -4.3978229 -4.398653 -4.3991652 -4.3992467 -4.3986554 -4.3977866][-4.3979874 -4.3971958 -4.3964853 -4.3959413 -4.3956885 -4.3957386 -4.3958693 -4.3961968 -4.3967109 -4.3975339 -4.3984342 -4.3989615 -4.39893 -4.3982425 -4.3973718][-4.3954482 -4.3949218 -4.3947334 -4.3947287 -4.3948722 -4.3952441 -4.395628 -4.3961792 -4.3970132 -4.3980556 -4.3990693 -4.39955 -4.3993864 -4.3985591 -4.3976016][-4.3951926 -4.39483 -4.3947954 -4.3949404 -4.3953176 -4.3958588 -4.3964105 -4.3971367 -4.3980851 -4.3992109 -4.4002166 -4.4006267 -4.4003963 -4.399581 -4.3986263]]...]
INFO - root - 2017-12-05 09:48:28.725031: step 16010, loss = 1.60, batch loss = 1.54 (34.5 examples/sec; 0.232 sec/batch; 20h:22m:49s remains)
INFO - root - 2017-12-05 09:48:30.899444: step 16020, loss = 2.02, batch loss = 1.96 (37.6 examples/sec; 0.213 sec/batch; 18h:41m:59s remains)
INFO - root - 2017-12-05 09:48:33.081051: step 16030, loss = 1.38, batch loss = 1.32 (37.0 examples/sec; 0.216 sec/batch; 18h:59m:58s remains)
INFO - root - 2017-12-05 09:48:35.257217: step 16040, loss = 2.36, batch loss = 2.30 (36.8 examples/sec; 0.217 sec/batch; 19h:06m:05s remains)
INFO - root - 2017-12-05 09:48:37.450794: step 16050, loss = 1.29, batch loss = 1.23 (36.8 examples/sec; 0.217 sec/batch; 19h:05m:07s remains)
INFO - root - 2017-12-05 09:48:39.652260: step 16060, loss = 1.25, batch loss = 1.19 (37.4 examples/sec; 0.214 sec/batch; 18h:48m:35s remains)
INFO - root - 2017-12-05 09:48:41.874836: step 16070, loss = 1.45, batch loss = 1.39 (35.8 examples/sec; 0.223 sec/batch; 19h:37m:35s remains)
INFO - root - 2017-12-05 09:48:44.063411: step 16080, loss = 2.36, batch loss = 2.31 (36.6 examples/sec; 0.219 sec/batch; 19h:13m:24s remains)
INFO - root - 2017-12-05 09:48:46.246981: step 16090, loss = 1.84, batch loss = 1.78 (35.3 examples/sec; 0.226 sec/batch; 19h:54m:12s remains)
INFO - root - 2017-12-05 09:48:48.463562: step 16100, loss = 1.63, batch loss = 1.58 (36.4 examples/sec; 0.220 sec/batch; 19h:20m:31s remains)
2017-12-05 09:48:48.770409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.403811 -4.4042749 -4.40466 -4.4048872 -4.4049063 -4.4047647 -4.4046326 -4.404644 -4.4046206 -4.4045057 -4.4044356 -4.4044557 -4.4045067 -4.404635 -4.4046574][-4.4034457 -4.403913 -4.4043546 -4.4046474 -4.4047885 -4.4047847 -4.4047108 -4.4046931 -4.4046359 -4.4045277 -4.4044561 -4.4044185 -4.4044104 -4.4044805 -4.4044876][-4.4031792 -4.4036388 -4.4041262 -4.4044662 -4.404624 -4.4046512 -4.4045744 -4.4045329 -4.4045238 -4.4044733 -4.4043746 -4.4042234 -4.4040937 -4.404099 -4.4041185][-4.4030905 -4.4035454 -4.4040294 -4.4043293 -4.4044094 -4.4043574 -4.4042091 -4.4041343 -4.404161 -4.4041591 -4.4040618 -4.4038329 -4.403614 -4.4035554 -4.4036412][-4.402988 -4.4034662 -4.4039125 -4.4041295 -4.4041128 -4.4039917 -4.4037728 -4.4036369 -4.4036665 -4.4037104 -4.4036264 -4.4033856 -4.403162 -4.40311 -4.403254][-4.4027643 -4.4032292 -4.403614 -4.4037914 -4.4037471 -4.4035807 -4.4033089 -4.4031167 -4.403142 -4.4032326 -4.403183 -4.4030004 -4.402895 -4.4029446 -4.4031572][-4.4024758 -4.4029217 -4.4032764 -4.4034762 -4.4034576 -4.4032588 -4.4029121 -4.4026823 -4.40271 -4.4028139 -4.4028068 -4.4027314 -4.402771 -4.4029474 -4.4032269][-4.4021568 -4.4025617 -4.4028792 -4.4030833 -4.4030933 -4.4028959 -4.4025655 -4.4023438 -4.4023962 -4.4025049 -4.4025455 -4.4025669 -4.4027157 -4.4030023 -4.4033737][-4.4018455 -4.4021769 -4.4024482 -4.4026265 -4.402657 -4.4025168 -4.4022722 -4.4021006 -4.402132 -4.402236 -4.4023089 -4.4023871 -4.4026275 -4.403007 -4.4034557][-4.4015808 -4.4018121 -4.4020658 -4.4022479 -4.4023438 -4.4023252 -4.4022155 -4.4021053 -4.4020805 -4.402101 -4.4021468 -4.4022307 -4.40252 -4.4029346 -4.4034252][-4.4013762 -4.4015307 -4.4017816 -4.4019537 -4.4020767 -4.4021215 -4.40208 -4.4019985 -4.4019427 -4.4019284 -4.4019613 -4.4020648 -4.4023871 -4.4028287 -4.403348][-4.4012065 -4.4012861 -4.4015241 -4.4016771 -4.4017854 -4.4018407 -4.4018292 -4.4017754 -4.4017472 -4.4017639 -4.4018369 -4.401978 -4.4023309 -4.4027853 -4.4033303][-4.4010639 -4.4010677 -4.4013057 -4.4014525 -4.4015355 -4.401577 -4.4015822 -4.4015665 -4.401588 -4.4016738 -4.4018207 -4.4020143 -4.4023728 -4.4028149 -4.4033713][-4.4009857 -4.4009533 -4.4011641 -4.4013004 -4.4013624 -4.4013839 -4.4013906 -4.4014039 -4.4014678 -4.401608 -4.4018173 -4.4020538 -4.4023952 -4.4028282 -4.40341][-4.4009762 -4.4009333 -4.4011116 -4.4012237 -4.4012737 -4.40128 -4.4012794 -4.401309 -4.4013991 -4.4015679 -4.4018054 -4.4020596 -4.4023762 -4.4027619 -4.4032969]]...]
INFO - root - 2017-12-05 09:48:50.938131: step 16110, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:03s remains)
INFO - root - 2017-12-05 09:48:53.103566: step 16120, loss = 1.34, batch loss = 1.28 (36.5 examples/sec; 0.219 sec/batch; 19h:17m:07s remains)
INFO - root - 2017-12-05 09:48:55.314698: step 16130, loss = 1.50, batch loss = 1.44 (37.2 examples/sec; 0.215 sec/batch; 18h:54m:32s remains)
INFO - root - 2017-12-05 09:48:57.496802: step 16140, loss = 1.20, batch loss = 1.14 (36.9 examples/sec; 0.217 sec/batch; 19h:03m:07s remains)
INFO - root - 2017-12-05 09:48:59.684814: step 16150, loss = 1.25, batch loss = 1.19 (37.1 examples/sec; 0.216 sec/batch; 18h:56m:31s remains)
INFO - root - 2017-12-05 09:49:01.877517: step 16160, loss = 1.37, batch loss = 1.31 (37.2 examples/sec; 0.215 sec/batch; 18h:55m:00s remains)
INFO - root - 2017-12-05 09:49:04.059668: step 16170, loss = 1.44, batch loss = 1.38 (37.1 examples/sec; 0.216 sec/batch; 18h:56m:38s remains)
INFO - root - 2017-12-05 09:49:06.230856: step 16180, loss = 1.71, batch loss = 1.65 (36.2 examples/sec; 0.221 sec/batch; 19h:23m:38s remains)
INFO - root - 2017-12-05 09:49:08.442457: step 16190, loss = 1.46, batch loss = 1.41 (33.5 examples/sec; 0.239 sec/batch; 20h:58m:02s remains)
INFO - root - 2017-12-05 09:49:10.653381: step 16200, loss = 1.33, batch loss = 1.27 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:06s remains)
2017-12-05 09:49:10.938855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.39625 -4.39585 -4.3955374 -4.3954253 -4.3954954 -4.3956971 -4.3959651 -4.3962793 -4.3966007 -4.3968415 -4.3969707 -4.3971143 -4.3974285 -4.39787 -4.3985043][-4.3766613 -4.3724451 -4.3690305 -4.3667135 -4.3654947 -4.3651614 -4.3654146 -4.3660531 -4.3667192 -4.3673091 -4.3678279 -4.3685255 -4.3699703 -4.3726368 -4.3765769][-4.3266506 -4.3080506 -4.2915368 -4.2786059 -4.2700438 -4.2655396 -4.263917 -4.2642021 -4.265121 -4.2658887 -4.2664814 -4.2684088 -4.2736397 -4.2838206 -4.2997708][-4.2262349 -4.1734467 -4.1249208 -4.0862694 -4.0599451 -4.0455871 -4.0399623 -4.0397916 -4.0409379 -4.041297 -4.04108 -4.0439734 -4.0551395 -4.0799942 -4.1213994][-4.0717921 -3.9600136 -3.8548687 -3.7693722 -3.709636 -3.6756902 -3.6608734 -3.657959 -3.6575973 -3.6551552 -3.6517141 -3.6548007 -3.6747992 -3.7231014 -3.8064766][-3.8761952 -3.6904724 -3.5145981 -3.3704877 -3.2690258 -3.2097907 -3.1824994 -3.174741 -3.1715472 -3.1650834 -3.1573915 -3.1605864 -3.1909063 -3.2678113 -3.4033329][-3.6639671 -3.4118061 -3.1738575 -2.978894 -2.8416033 -2.7610378 -2.7237849 -2.712815 -2.7089748 -2.7006063 -2.6905687 -2.6936417 -2.732141 -2.8346219 -3.0181022][-3.4491818 -3.1674821 -2.9099426 -2.7023501 -2.5591366 -2.4782472 -2.4443364 -2.4377422 -2.4384356 -2.4326851 -2.4239073 -2.4267535 -2.4681325 -2.5823672 -2.7906895][-3.2213049 -2.9641566 -2.7498298 -2.5871506 -2.4833286 -2.4333129 -2.4212551 -2.4287057 -2.4390352 -2.4397106 -2.4350209 -2.4366794 -2.4729104 -2.5794473 -2.7796645][-2.932349 -2.7323327 -2.6001921 -2.5180073 -2.4822941 -2.4830863 -2.5046546 -2.532063 -2.5533571 -2.560823 -2.5602064 -2.5601981 -2.5880318 -2.6791415 -2.8580644][-2.6086864 -2.4786797 -2.4389892 -2.4477625 -2.4890976 -2.5468588 -2.6051035 -2.6529179 -2.6822705 -2.6920331 -2.6890998 -2.6818662 -2.6961303 -2.7680237 -2.9242334][-2.4091761 -2.3452289 -2.3848102 -2.4682496 -2.5715051 -2.6740556 -2.7591145 -2.8190026 -2.8492827 -2.8541431 -2.8406286 -2.8192017 -2.8143196 -2.8627288 -2.9938431][-2.4853394 -2.4730978 -2.5620451 -2.6866169 -2.8206341 -2.9422507 -3.03506 -3.0934129 -3.115006 -3.1070802 -3.0763249 -3.0344558 -3.0064335 -3.0280724 -3.12996][-2.827059 -2.8449078 -2.9466579 -3.0727682 -3.2011082 -3.3132162 -3.3939881 -3.43857 -3.4448473 -3.4210629 -3.3731754 -3.3127353 -3.2663553 -3.2677312 -3.3425436][-3.2693 -3.2937064 -3.3788311 -3.4779227 -3.5761507 -3.6608078 -3.7182851 -3.7428505 -3.7313867 -3.6915176 -3.6304946 -3.5604968 -3.5090868 -3.5059004 -3.5679264]]...]
INFO - root - 2017-12-05 09:49:13.105258: step 16210, loss = 1.19, batch loss = 1.13 (36.0 examples/sec; 0.222 sec/batch; 19h:31m:49s remains)
INFO - root - 2017-12-05 09:49:15.277996: step 16220, loss = 2.01, batch loss = 1.95 (37.1 examples/sec; 0.215 sec/batch; 18h:55m:50s remains)
INFO - root - 2017-12-05 09:49:17.467963: step 16230, loss = 1.40, batch loss = 1.34 (37.8 examples/sec; 0.212 sec/batch; 18h:35m:33s remains)
INFO - root - 2017-12-05 09:49:19.677735: step 16240, loss = 1.51, batch loss = 1.46 (36.3 examples/sec; 0.220 sec/batch; 19h:21m:04s remains)
INFO - root - 2017-12-05 09:49:21.879739: step 16250, loss = 1.51, batch loss = 1.46 (37.0 examples/sec; 0.216 sec/batch; 18h:59m:31s remains)
INFO - root - 2017-12-05 09:49:24.083926: step 16260, loss = 1.70, batch loss = 1.64 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:30s remains)
INFO - root - 2017-12-05 09:49:26.271388: step 16270, loss = 1.48, batch loss = 1.42 (36.0 examples/sec; 0.222 sec/batch; 19h:32m:23s remains)
INFO - root - 2017-12-05 09:49:28.459632: step 16280, loss = 1.10, batch loss = 1.04 (36.8 examples/sec; 0.218 sec/batch; 19h:07m:07s remains)
INFO - root - 2017-12-05 09:49:30.680123: step 16290, loss = 1.42, batch loss = 1.36 (36.8 examples/sec; 0.218 sec/batch; 19h:06m:28s remains)
INFO - root - 2017-12-05 09:49:32.847122: step 16300, loss = 1.71, batch loss = 1.65 (36.7 examples/sec; 0.218 sec/batch; 19h:07m:51s remains)
2017-12-05 09:49:33.124460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0967517 -4.0675573 -4.0640221 -4.0889807 -4.1428375 -4.2139468 -4.28307 -4.3362422 -4.3699141 -4.3883185 -4.39679 -4.399662 -4.3999228 -4.3990912 -4.3979535][-4.125463 -4.1316032 -4.1534882 -4.1858149 -4.228785 -4.2767944 -4.3198586 -4.3519874 -4.3731556 -4.3864107 -4.3941231 -4.3982515 -4.40005 -4.4001312 -4.3989458][-4.2040763 -4.2268991 -4.2537422 -4.2785072 -4.3014851 -4.3219748 -4.3372712 -4.3479443 -4.3568764 -4.3667974 -4.3777351 -4.387464 -4.3939686 -4.3967252 -4.396214][-4.2878227 -4.3079419 -4.323885 -4.330749 -4.3287354 -4.3184361 -4.3033004 -4.2925653 -4.29299 -4.3073111 -4.3315139 -4.3570037 -4.3767204 -4.3877177 -4.391139][-4.3481851 -4.3575525 -4.3583183 -4.3444591 -4.3134961 -4.267561 -4.2189646 -4.1885133 -4.1883068 -4.2179675 -4.2661538 -4.3155293 -4.3539314 -4.3768215 -4.3865581][-4.3810148 -4.3820982 -4.3714027 -4.34021 -4.2843266 -4.21001 -4.1402674 -4.1050448 -4.1147971 -4.1624565 -4.2300553 -4.2949991 -4.3437982 -4.3727951 -4.3861046][-4.3956418 -4.3929486 -4.3782787 -4.3417706 -4.2799678 -4.2036572 -4.1406941 -4.1181517 -4.1386175 -4.1902218 -4.2545838 -4.3126655 -4.3544841 -4.3786507 -4.3900471][-4.4008303 -4.3978815 -4.3857703 -4.3563051 -4.3084145 -4.2534957 -4.2142196 -4.2066312 -4.228004 -4.2669692 -4.3109469 -4.3481507 -4.3738103 -4.3883834 -4.3952065][-4.4025965 -4.4006691 -4.3935351 -4.3763671 -4.3493056 -4.3201618 -4.3014045 -4.3005958 -4.3140779 -4.3353686 -4.3578877 -4.3762231 -4.3887544 -4.3959603 -4.3989739][-4.4029894 -4.4020238 -4.39886 -4.3910747 -4.3785691 -4.3644385 -4.35423 -4.352046 -4.3568659 -4.3664832 -4.3776221 -4.3873429 -4.3943748 -4.3985524 -4.3999062][-4.4028206 -4.402379 -4.4011083 -4.3972836 -4.3893566 -4.377768 -4.3656106 -4.3579493 -4.3578329 -4.3653369 -4.3763189 -4.3860879 -4.3925543 -4.3959856 -4.397038][-4.4014454 -4.4013128 -4.4008508 -4.3972945 -4.3872919 -4.3699012 -4.349309 -4.33433 -4.3325496 -4.3449097 -4.3628983 -4.3774352 -4.385489 -4.3885746 -4.3896394][-4.3996015 -4.3996453 -4.3993812 -4.3949203 -4.3808661 -4.3551354 -4.3243237 -4.3019381 -4.2992177 -4.3175449 -4.3441944 -4.3652544 -4.3757243 -4.3789463 -4.3809533][-4.3989277 -4.3993421 -4.399271 -4.3943062 -4.377533 -4.3463135 -4.3081274 -4.2790771 -4.2734323 -4.2937241 -4.3264832 -4.3539381 -4.3681326 -4.3731656 -4.3770485][-4.4001589 -4.4006934 -4.4005489 -4.3957849 -4.3791327 -4.3470631 -4.3061371 -4.2726989 -4.2633696 -4.2825394 -4.3173337 -4.3491111 -4.3673983 -4.3752804 -4.3810229]]...]
INFO - root - 2017-12-05 09:49:35.334851: step 16310, loss = 1.82, batch loss = 1.76 (36.2 examples/sec; 0.221 sec/batch; 19h:23m:23s remains)
INFO - root - 2017-12-05 09:49:37.510473: step 16320, loss = 1.52, batch loss = 1.46 (37.4 examples/sec; 0.214 sec/batch; 18h:47m:44s remains)
INFO - root - 2017-12-05 09:49:39.730344: step 16330, loss = 1.43, batch loss = 1.37 (34.4 examples/sec; 0.232 sec/batch; 20h:24m:31s remains)
INFO - root - 2017-12-05 09:49:41.927968: step 16340, loss = 1.40, batch loss = 1.35 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:49s remains)
INFO - root - 2017-12-05 09:49:44.125270: step 16350, loss = 1.48, batch loss = 1.42 (35.7 examples/sec; 0.224 sec/batch; 19h:41m:42s remains)
INFO - root - 2017-12-05 09:49:46.288231: step 16360, loss = 1.57, batch loss = 1.51 (37.1 examples/sec; 0.216 sec/batch; 18h:56m:14s remains)
INFO - root - 2017-12-05 09:49:48.501876: step 16370, loss = 1.71, batch loss = 1.65 (36.2 examples/sec; 0.221 sec/batch; 19h:23m:48s remains)
INFO - root - 2017-12-05 09:49:50.715007: step 16380, loss = 1.81, batch loss = 1.75 (36.6 examples/sec; 0.219 sec/batch; 19h:12m:51s remains)
INFO - root - 2017-12-05 09:49:52.901143: step 16390, loss = 1.41, batch loss = 1.35 (36.5 examples/sec; 0.219 sec/batch; 19h:15m:04s remains)
INFO - root - 2017-12-05 09:49:55.072902: step 16400, loss = 1.54, batch loss = 1.49 (37.3 examples/sec; 0.215 sec/batch; 18h:51m:02s remains)
2017-12-05 09:49:55.369073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.992233 -1.3116691 -0.55506539 0.2218833 0.86838055 1.2511621 1.2973514 1.0047898 0.45246935 -0.22109127 -0.92466259 -1.5647964 -2.0206633 -2.1819882 -2.0224249][-2.4947319 -1.8932464 -1.2059555 -0.47783542 0.16584158 0.60170746 0.75423193 0.61730433 0.25860596 -0.22194672 -0.78360152 -1.3528864 -1.8099434 -2.0069146 -1.8850455][-3.0943365 -2.6261511 -2.0675414 -1.4581342 -0.88951588 -0.45134902 -0.21125364 -0.18069458 -0.31445312 -0.56079078 -0.92951322 -1.3838975 -1.8051963 -2.0242047 -1.9489229][-3.5973964 -3.26584 -2.8602843 -2.4041455 -1.9551733 -1.5640626 -1.2831728 -1.1275957 -1.0747652 -1.118922 -1.3080978 -1.6396008 -2.0073071 -2.2359238 -2.2066216][-3.9614193 -3.7481444 -3.4800396 -3.1713955 -2.8484936 -2.5335326 -2.2603219 -2.0409868 -1.8789732 -1.796905 -1.8575869 -2.0814269 -2.3855944 -2.6076849 -2.6194909][-4.1719174 -4.0391922 -3.8649135 -3.6623249 -3.4398093 -3.204082 -2.9700761 -2.7524633 -2.5722435 -2.4580822 -2.4620454 -2.6102848 -2.8471875 -3.0397007 -3.0728216][-4.2369895 -4.1364436 -4.0011163 -3.8469768 -3.6830757 -3.5139866 -3.3399639 -3.1750264 -3.0487118 -2.9842587 -3.0017939 -3.1127429 -3.2860732 -3.4308076 -3.4610884][-4.1858954 -4.0700593 -3.9196949 -3.7574916 -3.6085334 -3.4780221 -3.3600633 -3.2700772 -3.2386761 -3.2723284 -3.3505418 -3.4636493 -3.596633 -3.7000303 -3.7234077][-4.0606108 -3.8936038 -3.6861918 -3.4707675 -3.2872348 -3.1503186 -3.0545373 -3.0202937 -3.0787067 -3.2210932 -3.389822 -3.5478687 -3.69148 -3.8020761 -3.8522618][-3.9114292 -3.6850214 -3.4087274 -3.1191025 -2.8674242 -2.6763229 -2.5430791 -2.4978933 -2.5867138 -2.7969427 -3.0491602 -3.287426 -3.511987 -3.7106695 -3.8524258][-3.79417 -3.5242755 -3.1946557 -2.8385 -2.5111797 -2.2343209 -2.0085869 -1.8788438 -1.9116697 -2.1081541 -2.3926508 -2.7113943 -3.0646348 -3.423094 -3.7218075][-3.7495537 -3.466789 -3.1143889 -2.7151918 -2.3210833 -1.9492998 -1.6043384 -1.3412721 -1.2480083 -1.3561726 -1.6140449 -1.9835656 -2.4639945 -3.0056677 -3.4953065][-3.7760198 -3.5075674 -3.1629333 -2.7510121 -2.3131256 -1.8607354 -1.3985572 -0.98829484 -0.73265386 -0.69857955 -0.87981629 -1.2624247 -1.8429375 -2.5515215 -3.2292836][-3.8485796 -3.6116772 -3.2973418 -2.8979201 -2.4424462 -1.9359148 -1.3835557 -0.84534812 -0.43377328 -0.24760389 -0.32904768 -0.69032764 -1.3302124 -2.162734 -2.9926481][-3.94277 -3.7435935 -3.4674346 -3.0960712 -2.6471643 -2.1229012 -1.5282762 -0.92068744 -0.41116834 -0.1128459 -0.102705 -0.41785812 -1.0613067 -1.9385498 -2.8435102]]...]
INFO - root - 2017-12-05 09:49:57.549801: step 16410, loss = 1.45, batch loss = 1.39 (35.5 examples/sec; 0.226 sec/batch; 19h:48m:28s remains)
INFO - root - 2017-12-05 09:49:59.754675: step 16420, loss = 1.54, batch loss = 1.48 (36.6 examples/sec; 0.218 sec/batch; 19h:10m:38s remains)
INFO - root - 2017-12-05 09:50:01.962459: step 16430, loss = 1.45, batch loss = 1.39 (36.1 examples/sec; 0.221 sec/batch; 19h:26m:00s remains)
INFO - root - 2017-12-05 09:50:04.143556: step 16440, loss = 1.50, batch loss = 1.44 (36.1 examples/sec; 0.222 sec/batch; 19h:27m:16s remains)
INFO - root - 2017-12-05 09:50:06.334323: step 16450, loss = 1.07, batch loss = 1.01 (36.6 examples/sec; 0.219 sec/batch; 19h:11m:02s remains)
INFO - root - 2017-12-05 09:50:08.497517: step 16460, loss = 1.59, batch loss = 1.53 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:47s remains)
INFO - root - 2017-12-05 09:50:10.680713: step 16470, loss = 1.28, batch loss = 1.23 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:46s remains)
INFO - root - 2017-12-05 09:50:12.897553: step 16480, loss = 1.71, batch loss = 1.66 (36.2 examples/sec; 0.221 sec/batch; 19h:23m:35s remains)
INFO - root - 2017-12-05 09:50:15.093835: step 16490, loss = 1.39, batch loss = 1.33 (37.3 examples/sec; 0.215 sec/batch; 18h:51m:07s remains)
INFO - root - 2017-12-05 09:50:17.297980: step 16500, loss = 1.46, batch loss = 1.41 (36.9 examples/sec; 0.217 sec/batch; 19h:00m:59s remains)
2017-12-05 09:50:17.567584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4052858 -4.405262 -4.4052439 -4.4051838 -4.4049859 -4.4046531 -4.4043307 -4.4041023 -4.4039607 -4.4041066 -4.4044352 -4.4048028 -4.4050703 -4.4051647 -4.4052434][-4.4051414 -4.4050164 -4.4047637 -4.4039965 -4.4023323 -4.3998232 -4.3970637 -4.3949571 -4.3944511 -4.3960028 -4.3988214 -4.4017453 -4.4037404 -4.4045968 -4.4049807][-4.4048362 -4.4042382 -4.4028807 -4.3994231 -4.3922739 -4.3814516 -4.3689318 -4.3591976 -4.3569846 -4.3634849 -4.375793 -4.3889823 -4.3985057 -4.4029055 -4.4044714][-4.4038363 -4.4014931 -4.3960443 -4.3841867 -4.361835 -4.3287268 -4.2910795 -4.2625055 -4.2567267 -4.2767029 -4.3141155 -4.3538136 -4.3829331 -4.3976483 -4.4030476][-4.4005923 -4.3923264 -4.3752041 -4.3428383 -4.2891436 -4.2155313 -4.1357584 -4.0781431 -4.0713587 -4.11937 -4.2020555 -4.2875905 -4.35126 -4.3857236 -4.3996][-4.3933611 -4.3728995 -4.3330607 -4.2651463 -4.1636505 -4.0350494 -3.9047356 -3.8185189 -3.820787 -3.912246 -4.0537934 -4.1963663 -4.3041658 -4.3658996 -4.3931084][-4.3831735 -4.345542 -4.27499 -4.1628666 -4.00808 -3.826617 -3.65828 -3.5620456 -3.5875583 -3.7259436 -3.9197781 -4.1101294 -4.2555795 -4.3431492 -4.3846745][-4.3746629 -4.3232427 -4.229259 -4.0862503 -3.9003289 -3.6970105 -3.5240703 -3.4413936 -3.4893272 -3.6511307 -3.865406 -4.0716181 -4.2305665 -4.3296022 -4.3787994][-4.3739347 -4.3225574 -4.2300911 -4.0934529 -3.9249024 -3.7517366 -3.6149178 -3.5586298 -3.6100833 -3.7529008 -3.9376531 -4.114511 -4.2512608 -4.3372579 -4.3806129][-4.3811326 -4.3429232 -4.2747068 -4.177074 -4.0631924 -3.9524126 -3.869369 -3.840065 -3.8798416 -3.9769495 -4.0992732 -4.215631 -4.3053007 -4.36104 -4.3887243][-4.3906393 -4.3692989 -4.3314023 -4.2791796 -4.2210345 -4.1666307 -4.127233 -4.1152983 -4.1385169 -4.1902647 -4.2531471 -4.3119283 -4.3564763 -4.3834953 -4.3966556][-4.3978181 -4.3892694 -4.3736248 -4.3524818 -4.3297458 -4.3093066 -4.2948217 -4.2910037 -4.301455 -4.3226395 -4.3473616 -4.3698096 -4.386569 -4.3966079 -4.4013634][-4.4018717 -4.3995333 -4.3946929 -4.38819 -4.3814487 -4.375648 -4.3716621 -4.3708925 -4.3744874 -4.3809519 -4.3880377 -4.3943858 -4.3991327 -4.4018908 -4.4030552][-4.4035935 -4.4032211 -4.4021149 -4.4006114 -4.3991218 -4.3978815 -4.3971562 -4.3972011 -4.3981853 -4.3995571 -4.4007826 -4.4017172 -4.4024391 -4.4028497 -4.402925][-4.404202 -4.4042187 -4.4040928 -4.4038544 -4.4035907 -4.4033546 -4.4032035 -4.4032826 -4.403451 -4.4034157 -4.4028754 -4.4021578 -4.4018636 -4.4018197 -4.4019656]]...]
INFO - root - 2017-12-05 09:50:19.745732: step 16510, loss = 1.43, batch loss = 1.37 (35.4 examples/sec; 0.226 sec/batch; 19h:50m:22s remains)
INFO - root - 2017-12-05 09:50:21.914545: step 16520, loss = 1.80, batch loss = 1.74 (37.8 examples/sec; 0.212 sec/batch; 18h:35m:41s remains)
INFO - root - 2017-12-05 09:50:24.121068: step 16530, loss = 1.90, batch loss = 1.85 (35.5 examples/sec; 0.225 sec/batch; 19h:46m:41s remains)
INFO - root - 2017-12-05 09:50:26.326673: step 16540, loss = 1.51, batch loss = 1.45 (36.8 examples/sec; 0.218 sec/batch; 19h:05m:33s remains)
INFO - root - 2017-12-05 09:50:28.513253: step 16550, loss = 1.13, batch loss = 1.07 (36.1 examples/sec; 0.222 sec/batch; 19h:28m:31s remains)
INFO - root - 2017-12-05 09:50:30.683296: step 16560, loss = 1.13, batch loss = 1.07 (35.8 examples/sec; 0.223 sec/batch; 19h:36m:18s remains)
INFO - root - 2017-12-05 09:50:32.875215: step 16570, loss = 1.79, batch loss = 1.73 (36.7 examples/sec; 0.218 sec/batch; 19h:08m:35s remains)
INFO - root - 2017-12-05 09:50:35.064094: step 16580, loss = 1.59, batch loss = 1.53 (37.4 examples/sec; 0.214 sec/batch; 18h:46m:06s remains)
INFO - root - 2017-12-05 09:50:37.284951: step 16590, loss = 1.36, batch loss = 1.30 (36.4 examples/sec; 0.220 sec/batch; 19h:17m:21s remains)
INFO - root - 2017-12-05 09:50:39.503822: step 16600, loss = 1.30, batch loss = 1.24 (37.4 examples/sec; 0.214 sec/batch; 18h:46m:20s remains)
2017-12-05 09:50:39.791747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4030623 -4.4020815 -4.3996773 -4.3941126 -4.3815579 -4.3577013 -4.3223367 -4.2859759 -4.2653723 -4.2687654 -4.2922382 -4.3246293 -4.3551946 -4.3776917 -4.3912048][-4.4028749 -4.4014659 -4.3984418 -4.3911648 -4.3752522 -4.3460031 -4.3055372 -4.2678475 -4.2500257 -4.2593389 -4.2888374 -4.32525 -4.35755 -4.3798532 -4.3928018][-4.40069 -4.3958392 -4.3863449 -4.3670745 -4.3317146 -4.2762785 -4.2091131 -4.1517978 -4.1276574 -4.1459217 -4.1977358 -4.2623034 -4.3206606 -4.3617334 -4.3855257][-4.3937721 -4.37974 -4.3541455 -4.3093534 -4.2383852 -4.1395864 -4.0287032 -3.9385798 -3.9018738 -3.9344482 -4.0239615 -4.137475 -4.2422862 -4.3195 -4.3667784][-4.3802433 -4.3494511 -4.2970138 -4.2141294 -4.0957279 -3.9447956 -3.7856331 -3.6607041 -3.6118193 -3.6628981 -3.7985888 -3.9725223 -4.1361971 -4.2604494 -4.3395238][-4.3629093 -4.3115621 -4.2269454 -4.1008687 -3.934351 -3.7387147 -3.5434709 -3.3939593 -3.3381097 -3.4068861 -3.583576 -3.8128581 -4.0321517 -4.201509 -4.3116193][-4.3494363 -4.2820787 -4.1735749 -4.0169878 -3.8189683 -3.5994177 -3.3906693 -3.2336812 -3.1764097 -3.2551184 -3.4550507 -3.7173247 -3.970521 -4.1671247 -4.2952051][-4.3484292 -4.2795415 -4.1697297 -4.0127888 -3.8156531 -3.6000018 -3.3978906 -3.2471738 -3.1922355 -3.2709556 -3.4708853 -3.7325988 -3.9836986 -4.1761508 -4.2998519][-4.3601918 -4.3052421 -4.2177572 -4.0912447 -3.9278193 -3.7438648 -3.566596 -3.4328375 -3.3841293 -3.4539871 -3.6299784 -3.8556352 -4.0664229 -4.2240024 -4.3225255][-4.3766394 -4.3423753 -4.2863827 -4.2023406 -4.0877671 -3.9521987 -3.8169162 -3.713742 -3.677428 -3.7323227 -3.8656859 -4.0310588 -4.1796808 -4.2868919 -4.3514409][-4.39048 -4.3737206 -4.3447285 -4.2981858 -4.2295537 -4.1433287 -4.0550318 -3.9885151 -3.9676869 -4.0056438 -4.0907021 -4.1921749 -4.2798715 -4.3407044 -4.3756781][-4.3985081 -4.391994 -4.3799829 -4.3590078 -4.325119 -4.2784476 -4.228971 -4.19293 -4.1840997 -4.2071424 -4.2526679 -4.3039341 -4.3465309 -4.3749571 -4.3907328][-4.4019604 -4.399909 -4.3958936 -4.38813 -4.3748407 -4.354589 -4.3317132 -4.3156738 -4.3131852 -4.3244004 -4.3440127 -4.3647938 -4.3814979 -4.3924203 -4.3984842][-4.40321 -4.4026284 -4.4016051 -4.3992558 -4.3951106 -4.3883576 -4.380054 -4.374372 -4.3738384 -4.3777213 -4.3841448 -4.3907814 -4.3961916 -4.3998384 -4.4019156][-4.4035025 -4.4032474 -4.4029613 -4.402319 -4.4012103 -4.3993993 -4.3970027 -4.3954396 -4.3953681 -4.3964863 -4.398231 -4.4000854 -4.4016261 -4.4025908 -4.4030523]]...]
INFO - root - 2017-12-05 09:50:41.949951: step 16610, loss = 1.50, batch loss = 1.44 (37.3 examples/sec; 0.215 sec/batch; 18h:49m:55s remains)
INFO - root - 2017-12-05 09:50:44.114894: step 16620, loss = 1.31, batch loss = 1.25 (37.2 examples/sec; 0.215 sec/batch; 18h:51m:02s remains)
INFO - root - 2017-12-05 09:50:46.305054: step 16630, loss = 1.87, batch loss = 1.81 (36.2 examples/sec; 0.221 sec/batch; 19h:23m:46s remains)
INFO - root - 2017-12-05 09:50:48.473639: step 16640, loss = 1.67, batch loss = 1.61 (36.4 examples/sec; 0.220 sec/batch; 19h:16m:00s remains)
INFO - root - 2017-12-05 09:50:50.671411: step 16650, loss = 1.26, batch loss = 1.20 (36.4 examples/sec; 0.220 sec/batch; 19h:15m:41s remains)
INFO - root - 2017-12-05 09:50:52.827522: step 16660, loss = 1.70, batch loss = 1.64 (36.7 examples/sec; 0.218 sec/batch; 19h:06m:38s remains)
INFO - root - 2017-12-05 09:50:55.025721: step 16670, loss = 1.41, batch loss = 1.35 (37.9 examples/sec; 0.211 sec/batch; 18h:31m:49s remains)
INFO - root - 2017-12-05 09:50:57.202752: step 16680, loss = 1.30, batch loss = 1.25 (37.5 examples/sec; 0.213 sec/batch; 18h:43m:44s remains)
INFO - root - 2017-12-05 09:50:59.373263: step 16690, loss = 1.65, batch loss = 1.59 (36.1 examples/sec; 0.222 sec/batch; 19h:26m:50s remains)
INFO - root - 2017-12-05 09:51:01.550408: step 16700, loss = 1.00, batch loss = 0.94 (35.7 examples/sec; 0.224 sec/batch; 19h:38m:54s remains)
2017-12-05 09:51:01.841416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3110914 -4.2931666 -4.2819505 -4.2750182 -4.2699976 -4.2664766 -4.264626 -4.2624049 -4.2573514 -4.24877 -4.2369242 -4.2214613 -4.2042379 -4.1881657 -4.1809783][-4.1863184 -4.1426306 -4.1115618 -4.0884261 -4.0696688 -4.0545578 -4.044292 -4.0374393 -4.0321841 -4.0271988 -4.0207305 -4.0092878 -3.9937584 -3.9770627 -3.97393][-4.0157471 -3.9343863 -3.8716946 -3.8207941 -3.7763762 -3.7383771 -3.7108533 -3.6967397 -3.6976726 -3.7107968 -3.7279768 -3.7389374 -3.7409093 -3.7360454 -3.7472382][-3.8293962 -3.7040474 -3.6027479 -3.5169897 -3.4406786 -3.374934 -3.3284016 -3.3094668 -3.3241692 -3.3675146 -3.4218998 -3.4672573 -3.4943819 -3.5067358 -3.5371079][-3.6559765 -3.4853873 -3.3430681 -3.2209535 -3.1134114 -3.0232787 -2.9636664 -2.9469795 -2.9820361 -3.0602837 -3.1544886 -3.2347367 -3.285219 -3.313813 -3.3625631][-3.5104086 -3.29662 -3.1145248 -2.9595771 -2.8285198 -2.725024 -2.6622133 -2.6538675 -2.7093105 -2.8153505 -2.9383643 -3.0425103 -3.1098852 -3.1541891 -3.2234688][-3.3931503 -3.1409326 -2.9241567 -2.7444987 -2.6019483 -2.4990454 -2.444633 -2.4478574 -2.5155637 -2.6318111 -2.7625322 -2.8735378 -2.9506207 -3.0126276 -3.1073737][-3.2933602 -3.010798 -2.7698243 -2.5778623 -2.4373631 -2.3467236 -2.309087 -2.3247926 -2.3939095 -2.5012321 -2.6192527 -2.7236657 -2.8061683 -2.8872604 -3.0094204][-3.2114563 -2.9112453 -2.6604595 -2.4709876 -2.34526 -2.2763438 -2.2606537 -2.2887471 -2.3541098 -2.4441853 -2.5407143 -2.6313443 -2.7130656 -2.8055232 -2.9453826][-3.1708455 -2.8705852 -2.6295838 -2.4609418 -2.363384 -2.323633 -2.3320861 -2.3720458 -2.4342504 -2.5090728 -2.586535 -2.6613631 -2.7333734 -2.8209777 -2.9559226][-3.2220283 -2.9470143 -2.7378855 -2.6052432 -2.5433435 -2.5348239 -2.5659173 -2.6172547 -2.678699 -2.7426577 -2.8037992 -2.85979 -2.9123158 -2.9782274 -3.0847373][-3.3964968 -3.1734171 -3.0132074 -2.92297 -2.8936594 -2.9089224 -2.9544168 -3.011198 -3.0690317 -3.1213875 -3.165652 -3.2000601 -3.227695 -3.2634184 -3.3310313][-3.6585426 -3.501395 -3.3952157 -3.3430817 -3.3357716 -3.3610551 -3.4070175 -3.4579961 -3.5053802 -3.543642 -3.5715556 -3.5879831 -3.5961115 -3.6074214 -3.6411047][-3.9302638 -3.8349073 -3.7742944 -3.7484491 -3.7507839 -3.7739437 -3.8098333 -3.847507 -3.8808155 -3.9058466 -3.9217298 -3.9274886 -3.9247074 -3.9218571 -3.9330821][-4.1506944 -4.1006079 -4.070322 -4.0594158 -4.0640926 -4.080164 -4.102859 -4.1263108 -4.1469364 -4.1618395 -4.1697488 -4.1700497 -4.1635437 -4.155633 -4.1554856]]...]
INFO - root - 2017-12-05 09:51:04.011924: step 16710, loss = 1.31, batch loss = 1.25 (37.7 examples/sec; 0.212 sec/batch; 18h:36m:06s remains)
INFO - root - 2017-12-05 09:51:06.195329: step 16720, loss = 1.75, batch loss = 1.69 (35.4 examples/sec; 0.226 sec/batch; 19h:49m:12s remains)
INFO - root - 2017-12-05 09:51:08.387373: step 16730, loss = 1.62, batch loss = 1.56 (34.7 examples/sec; 0.231 sec/batch; 20h:14m:01s remains)
INFO - root - 2017-12-05 09:51:10.612551: step 16740, loss = 1.63, batch loss = 1.57 (36.0 examples/sec; 0.222 sec/batch; 19h:28m:31s remains)
INFO - root - 2017-12-05 09:51:12.778940: step 16750, loss = 1.29, batch loss = 1.23 (37.2 examples/sec; 0.215 sec/batch; 18h:53m:05s remains)
INFO - root - 2017-12-05 09:51:14.969142: step 16760, loss = 1.41, batch loss = 1.35 (35.5 examples/sec; 0.226 sec/batch; 19h:47m:26s remains)
INFO - root - 2017-12-05 09:51:17.178865: step 16770, loss = 1.22, batch loss = 1.16 (37.1 examples/sec; 0.215 sec/batch; 18h:53m:53s remains)
INFO - root - 2017-12-05 09:51:19.366031: step 16780, loss = 1.31, batch loss = 1.25 (37.6 examples/sec; 0.213 sec/batch; 18h:38m:40s remains)
INFO - root - 2017-12-05 09:51:21.556868: step 16790, loss = 1.46, batch loss = 1.40 (37.5 examples/sec; 0.213 sec/batch; 18h:42m:39s remains)
INFO - root - 2017-12-05 09:51:23.767997: step 16800, loss = 1.06, batch loss = 1.01 (35.8 examples/sec; 0.224 sec/batch; 19h:36m:01s remains)
2017-12-05 09:51:24.060264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2301757 -2.0320148 -1.9136586 -1.8535001 -1.8476601 -1.8686225 -1.8664935 -1.8382139 -1.7934799 -1.7367315 -1.659143 -1.5886593 -1.5535502 -1.5614095 -1.6176162][-2.1847835 -2.0102262 -1.9287984 -1.9064884 -1.9224186 -1.9370375 -1.9133413 -1.851594 -1.7558057 -1.6392109 -1.5051522 -1.3876133 -1.3120663 -1.2853901 -1.3213203][-2.1217775 -1.9702525 -1.9252636 -1.9401841 -1.9780583 -1.9929142 -1.9555902 -1.8698781 -1.7391489 -1.5775859 -1.4001803 -1.2375348 -1.1170807 -1.0489693 -1.0443802][-1.9162827 -1.7725651 -1.7489512 -1.7932999 -1.8499336 -1.8735518 -1.8399625 -1.752439 -1.6150994 -1.441463 -1.2475393 -1.0616925 -0.91076064 -0.80947924 -0.7633276][-1.597518 -1.4502809 -1.4392993 -1.5022247 -1.5797927 -1.619143 -1.5978296 -1.5253725 -1.4050627 -1.2440758 -1.0554621 -0.87025118 -0.71381211 -0.60169482 -0.53262424][-1.233408 -1.0831177 -1.0880177 -1.1738985 -1.2810366 -1.345453 -1.3415747 -1.2861371 -1.1894355 -1.0531893 -0.88570642 -0.71918297 -0.5802896 -0.48350286 -0.41993284][-0.90499735 -0.76647282 -0.80194879 -0.92001843 -1.067014 -1.1665068 -1.1887815 -1.1538944 -1.0804007 -0.96916938 -0.829041 -0.68922162 -0.57918119 -0.51131797 -0.47430182][-0.69584513 -0.58237886 -0.66859913 -0.83195257 -1.0237315 -1.161725 -1.2146332 -1.2024448 -1.1478558 -1.0563583 -0.94192123 -0.83082175 -0.75314736 -0.71643996 -0.70940614][-0.66720605 -0.59569478 -0.74815536 -0.96197653 -1.1915855 -1.3584859 -1.4326155 -1.4332528 -1.3881412 -1.3096211 -1.219023 -1.1396754 -1.0991607 -1.0950024 -1.1125946][-0.86170506 -0.83515239 -1.0443039 -1.2942266 -1.5395634 -1.711683 -1.7867491 -1.7832408 -1.7378237 -1.6679499 -1.6035054 -1.5631216 -1.5647964 -1.5930367 -1.6280158][-1.2619815 -1.2680397 -1.4963787 -1.7486699 -1.9740896 -2.121089 -2.170084 -2.1411891 -2.0810764 -2.0177329 -1.9837394 -1.9862468 -2.0296328 -2.0870159 -2.136766][-1.7970488 -1.8042355 -1.9966552 -2.1958818 -2.354816 -2.441082 -2.443176 -2.3794837 -2.3007677 -2.2429469 -2.2362275 -2.2745562 -2.3503206 -2.4313922 -2.497793][-2.3184993 -2.2807577 -2.3787408 -2.4744411 -2.529057 -2.531682 -2.4792454 -2.3871868 -2.3024254 -2.2577486 -2.2772779 -2.3437076 -2.438705 -2.5357747 -2.6175685][-2.6301579 -2.5095105 -2.4808192 -2.4488516 -2.3936386 -2.3182671 -2.2269819 -2.1323342 -2.0666373 -2.0509584 -2.1000104 -2.1911895 -2.3014989 -2.4103436 -2.5047517][-2.611753 -2.3851509 -2.230602 -2.0867774 -1.9494023 -1.8305109 -1.7396352 -1.6759193 -1.6571035 -1.6916788 -1.7807419 -1.9010434 -2.0286412 -2.1511657 -2.2603371]]...]
INFO - root - 2017-12-05 09:51:26.260089: step 16810, loss = 1.20, batch loss = 1.14 (36.3 examples/sec; 0.220 sec/batch; 19h:18m:11s remains)
INFO - root - 2017-12-05 09:51:28.509939: step 16820, loss = 1.38, batch loss = 1.32 (35.6 examples/sec; 0.225 sec/batch; 19h:43m:32s remains)
INFO - root - 2017-12-05 09:51:30.729551: step 16830, loss = 1.16, batch loss = 1.10 (37.3 examples/sec; 0.214 sec/batch; 18h:48m:22s remains)
INFO - root - 2017-12-05 09:51:32.947831: step 16840, loss = 1.21, batch loss = 1.15 (37.8 examples/sec; 0.211 sec/batch; 18h:32m:20s remains)
INFO - root - 2017-12-05 09:51:35.147176: step 16850, loss = 1.60, batch loss = 1.54 (35.5 examples/sec; 0.226 sec/batch; 19h:47m:11s remains)
INFO - root - 2017-12-05 09:51:37.362159: step 16860, loss = 1.36, batch loss = 1.30 (36.5 examples/sec; 0.219 sec/batch; 19h:13m:43s remains)
INFO - root - 2017-12-05 09:51:39.583588: step 16870, loss = 1.00, batch loss = 0.94 (36.5 examples/sec; 0.219 sec/batch; 19h:11m:52s remains)
INFO - root - 2017-12-05 09:51:41.772216: step 16880, loss = 1.23, batch loss = 1.18 (36.1 examples/sec; 0.222 sec/batch; 19h:26m:36s remains)
INFO - root - 2017-12-05 09:51:43.945113: step 16890, loss = 1.81, batch loss = 1.75 (38.0 examples/sec; 0.210 sec/batch; 18h:26m:50s remains)
INFO - root - 2017-12-05 09:51:46.167037: step 16900, loss = 1.40, batch loss = 1.34 (36.5 examples/sec; 0.219 sec/batch; 19h:11m:46s remains)
2017-12-05 09:51:46.455704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5604262 -3.2249551 -2.887691 -2.6071579 -2.4725418 -2.4215579 -2.2925448 -1.979888 -1.4658251 -0.84015727 -0.20489216 0.40776873 1.0481992 1.75208 2.4902225][-3.405035 -2.9944105 -2.5810108 -2.2182276 -2.007149 -1.8757217 -1.6509926 -1.2276235 -0.60371637 0.09066534 0.74861574 1.3392234 1.9588847 2.6856279 3.5238733][-3.238713 -2.7298427 -2.2026515 -1.7101531 -1.3753557 -1.1410575 -0.84327388 -0.38057375 0.23770905 0.85753393 1.3976021 1.8437853 2.344523 3.0261989 3.935523][-2.9885907 -2.3200741 -1.5969589 -0.888711 -0.36423254 -0.00884819 0.31896067 0.72976017 1.2098126 1.61482 1.9240632 2.1577663 2.519371 3.1720648 4.1849213][-2.6376162 -1.7358177 -0.72594619 0.28870583 1.070827 1.5703659 1.8988729 2.178328 2.4157176 2.5170674 2.5558181 2.6000328 2.892601 3.6044836 4.7825508][-2.2410066 -1.0620732 0.28342247 1.6489191 2.721169 3.3796496 3.6977978 3.8028555 3.7562141 3.5491376 3.3687987 3.3248539 3.6689415 4.5364161 5.9120083][-1.9157534 -0.48483896 1.1669827 2.8487225 4.1905327 5.0040636 5.3146667 5.2497778 4.946888 4.5027533 4.2072554 4.1964464 4.6873236 5.7519417 7.2918167][-1.7935722 -0.21945953 1.6095638 3.4734659 4.9790487 5.8858414 6.1773672 5.9772472 5.495224 4.9303465 4.6415014 4.7676153 5.4583864 6.7148495 8.365387][-1.9530351 -0.40689945 1.3971958 3.2308965 4.7142038 5.5830889 5.795785 5.4730921 4.9049568 4.3577871 4.1973715 4.5376854 5.453105 6.8925285 8.6395016][-2.3771884 -1.0413241 0.52753878 2.1128941 3.3842216 4.0897284 4.1812887 3.7813435 3.2257137 2.8044395 2.8336263 3.3937206 4.5088568 6.1060762 7.9676247][-2.9430079 -1.9390147 -0.74882007 0.44415617 1.3849616 1.8701501 1.8688626 1.4821901 1.0266171 0.76233435 0.94336605 1.6471334 2.8718243 4.5639572 6.5384789][-3.50304 -2.8572111 -2.082372 -1.3156586 -0.72358441 -0.44809222 -0.49560308 -0.79878974 -1.1131485 -1.2416058 -0.98608255 -0.25260019 0.96844482 2.6634755 4.6864443][-3.9438148 -3.5998449 -3.1792381 -2.7659311 -2.4476795 -2.3116441 -2.3488672 -2.5341399 -2.7148347 -2.7580614 -2.5132012 -1.8650815 -0.77954483 0.77746058 2.700644][-4.2146606 -4.0654411 -3.8774724 -3.6903348 -3.539057 -3.4684896 -3.4669113 -3.5458717 -3.6253185 -3.6322408 -3.4627237 -2.9908409 -2.1563683 -0.88790822 0.7529273][-4.3413267 -4.2910318 -4.2220659 -4.147995 -4.0789223 -4.0375886 -4.0144668 -4.0281343 -4.045083 -4.0383458 -3.9407785 -3.6507819 -3.1080532 -2.2218702 -1.0094719]]...]
INFO - root - 2017-12-05 09:51:48.621702: step 16910, loss = 1.31, batch loss = 1.25 (37.0 examples/sec; 0.216 sec/batch; 18h:57m:31s remains)
INFO - root - 2017-12-05 09:51:50.801084: step 16920, loss = 1.37, batch loss = 1.31 (37.3 examples/sec; 0.215 sec/batch; 18h:48m:13s remains)
INFO - root - 2017-12-05 09:51:52.989715: step 16930, loss = 1.51, batch loss = 1.46 (35.8 examples/sec; 0.223 sec/batch; 19h:34m:57s remains)
INFO - root - 2017-12-05 09:51:55.179895: step 16940, loss = 1.78, batch loss = 1.72 (36.9 examples/sec; 0.217 sec/batch; 19h:00m:00s remains)
INFO - root - 2017-12-05 09:51:57.351485: step 16950, loss = 1.95, batch loss = 1.89 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:49s remains)
INFO - root - 2017-12-05 09:51:59.552951: step 16960, loss = 1.33, batch loss = 1.27 (34.9 examples/sec; 0.229 sec/batch; 20h:05m:53s remains)
INFO - root - 2017-12-05 09:52:01.754346: step 16970, loss = 1.51, batch loss = 1.45 (37.1 examples/sec; 0.216 sec/batch; 18h:53m:30s remains)
INFO - root - 2017-12-05 09:52:03.947916: step 16980, loss = 1.39, batch loss = 1.34 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:23s remains)
INFO - root - 2017-12-05 09:52:06.167355: step 16990, loss = 1.90, batch loss = 1.84 (36.4 examples/sec; 0.220 sec/batch; 19h:15m:15s remains)
INFO - root - 2017-12-05 09:52:08.347230: step 17000, loss = 1.33, batch loss = 1.27 (35.8 examples/sec; 0.224 sec/batch; 19h:36m:02s remains)
2017-12-05 09:52:08.655647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3878436 -3.0078621 -2.4683595 -1.8213375 -1.1840336 -0.65345025 -0.31192827 -0.16636324 -0.17652845 -0.30401897 -0.51870561 -0.8030076 -1.1199057 -1.4351587 -1.6979659][-2.7954772 -2.1639957 -1.3052557 -0.29734135 0.68287039 1.4951625 2.0204105 2.2621717 2.2855973 2.1398573 1.8721004 1.5120568 1.1163044 0.71949005 0.38927317][-2.2129703 -1.278717 -0.039912224 1.3886333 2.7701783 3.9196978 4.678256 5.05912 5.1529455 5.0317783 4.7633677 4.3923006 3.9889631 3.5761795 3.2204666][-1.7685528 -0.52137971 1.118607 3.003624 4.8418455 6.4016705 7.4733362 8.0665 8.28734 8.2458239 8.0384 7.7374029 7.420104 7.0767908 6.7400322][-1.5132856 0.0058050156 2.0112934 4.331481 6.6357374 8.6425829 10.085278 10.957533 11.366426 11.464341 11.384995 11.241286 11.10768 10.923219 10.650774][-1.4230919 0.29557562 2.5854845 5.2690687 7.9910507 10.421608 12.244968 13.426706 14.059118 14.329638 14.419415 14.488926 14.600815 14.643232 14.479231][-1.450666 0.38924456 2.8678503 5.8114805 8.8616991 11.651478 13.823883 15.303261 16.160145 16.60322 16.862738 17.140558 17.49173 17.768141 17.728672][-1.5288529 0.36741543 2.9483676 6.0531254 9.3264608 12.372164 14.811665 16.527559 17.560472 18.137341 18.53075 18.979204 19.531942 20.021286 20.123055][-1.5954204 0.30233622 2.9069676 6.0729961 9.4564085 12.641367 15.237492 17.095181 18.239634 18.898981 19.377611 19.940022 20.64266 21.314108 21.578491][-1.5963373 0.25915384 2.8152847 5.944396 9.3198824 12.525345 15.171337 17.083258 18.283949 19.001825 19.558256 20.217207 21.04084 21.858685 22.276808][-1.4640512 0.32360029 2.7712073 5.7720938 9.0284376 12.14661 14.752441 16.658907 17.8947 18.683392 19.345158 20.118137 21.054173 21.974323 22.496403][-1.132688 0.56252 2.8459334 5.6380529 8.6849575 11.630466 14.131266 16.009621 17.300606 18.210693 19.029606 19.947779 20.977236 21.913469 22.424032][-0.58680654 0.99010468 3.0500083 5.5595808 8.3216019 11.03325 13.398258 15.252642 16.629618 17.700108 18.697084 19.743286 20.793139 21.630157 21.99234][0.0734663 1.5063562 3.2941771 5.456851 7.8685293 10.293206 12.498302 14.337212 15.826023 17.068935 18.212282 19.298376 20.235996 20.82366 20.900051][0.619895 1.8921485 3.390409 5.1819725 7.2066641 9.3045864 11.316524 13.115965 14.684715 16.045622 17.244371 18.245934 18.938196 19.174402 18.910862]]...]
INFO - root - 2017-12-05 09:52:10.846837: step 17010, loss = 1.83, batch loss = 1.78 (36.2 examples/sec; 0.221 sec/batch; 19h:21m:05s remains)
INFO - root - 2017-12-05 09:52:13.103456: step 17020, loss = 1.18, batch loss = 1.12 (33.9 examples/sec; 0.236 sec/batch; 20h:39m:50s remains)
INFO - root - 2017-12-05 09:52:15.394592: step 17030, loss = 1.47, batch loss = 1.41 (36.9 examples/sec; 0.217 sec/batch; 19h:00m:51s remains)
INFO - root - 2017-12-05 09:52:17.608682: step 17040, loss = 1.77, batch loss = 1.72 (33.3 examples/sec; 0.240 sec/batch; 21h:04m:17s remains)
INFO - root - 2017-12-05 09:52:19.801318: step 17050, loss = 1.38, batch loss = 1.32 (36.8 examples/sec; 0.217 sec/batch; 19h:03m:07s remains)
INFO - root - 2017-12-05 09:52:22.000503: step 17060, loss = 1.17, batch loss = 1.11 (36.5 examples/sec; 0.219 sec/batch; 19h:11m:06s remains)
INFO - root - 2017-12-05 09:52:24.175864: step 17070, loss = 1.63, batch loss = 1.58 (36.6 examples/sec; 0.219 sec/batch; 19h:10m:17s remains)
INFO - root - 2017-12-05 09:52:26.369230: step 17080, loss = 1.50, batch loss = 1.44 (36.7 examples/sec; 0.218 sec/batch; 19h:06m:08s remains)
INFO - root - 2017-12-05 09:52:28.587807: step 17090, loss = 1.22, batch loss = 1.16 (35.7 examples/sec; 0.224 sec/batch; 19h:38m:57s remains)
INFO - root - 2017-12-05 09:52:30.817209: step 17100, loss = 1.06, batch loss = 1.00 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:28s remains)
2017-12-05 09:52:31.101389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8970013 -4.0346313 -4.003902 -3.7605238 -3.3898487 -3.0136783 -2.7348258 -2.6273391 -2.7336991 -3.0421991 -3.4621665 -3.8418088 -4.0449252 -4.0126448 -3.7837152][-3.5367415 -3.7514768 -3.7508855 -3.46975 -3.0250344 -2.5743752 -2.2518506 -2.1509361 -2.316076 -2.7134664 -3.2272012 -3.6821127 -3.9168677 -3.8588517 -3.5615196][-3.1294625 -3.405746 -3.4538693 -3.1839604 -2.723105 -2.2519991 -1.9309375 -1.863683 -2.0906119 -2.5550103 -3.1222777 -3.6076055 -3.8457234 -3.7648783 -3.4295359][-2.7544153 -3.0688562 -3.1747305 -2.9616628 -2.5456553 -2.1113794 -1.8340447 -1.8158677 -2.089525 -2.5843658 -3.1590977 -3.6321127 -3.8543527 -3.7674751 -3.4375134][-2.5058193 -2.8372645 -2.9988604 -2.865242 -2.5306182 -2.1713362 -1.9622052 -1.9951713 -2.2908452 -2.7747309 -3.3127298 -3.7401011 -3.9356294 -3.860539 -3.5776052][-2.4363298 -2.7753549 -2.9780281 -2.9151492 -2.6633775 -2.3863301 -2.2463346 -2.3169317 -2.6078043 -3.0496011 -3.5227017 -3.8870237 -4.0521331 -4.0010381 -3.790719][-2.5535526 -2.8958774 -3.1187372 -3.0981488 -2.9067109 -2.695673 -2.607203 -2.6927762 -2.9536426 -3.3338413 -3.731349 -4.0306492 -4.1667161 -4.1425934 -4.0076156][-2.8264639 -3.1548615 -3.3700156 -3.3582578 -3.1967626 -3.026515 -2.9652019 -3.043962 -3.2593508 -3.5727043 -3.9001889 -4.143652 -4.25631 -4.2548256 -4.1813793][-3.1928391 -3.4809792 -3.6574616 -3.6267767 -3.4717476 -3.3191507 -3.2638421 -3.3224564 -3.4901602 -3.7440653 -4.0165491 -4.2195196 -4.3156424 -4.3284569 -4.2958169][-3.5792172 -3.7963479 -3.9104261 -3.8494091 -3.6919448 -3.5483797 -3.4939172 -3.5337863 -3.6610849 -3.8650186 -4.0933266 -4.2662554 -4.3498716 -4.3694139 -4.3588104][-3.9066935 -4.0380487 -4.0853634 -3.9994514 -3.846199 -3.7184384 -3.67496 -3.7093911 -3.8085947 -3.9677689 -4.1521916 -4.2957778 -4.3670268 -4.3883233 -4.3883638][-4.1310592 -4.1848969 -4.1787686 -4.0829811 -3.9468076 -3.8463709 -3.8251595 -3.8663778 -3.9495778 -4.0696168 -4.2071071 -4.3168869 -4.3735118 -4.3933868 -4.3980718][-4.2496595 -4.2477636 -4.2114453 -4.1230135 -4.0168481 -3.9515555 -3.9572513 -4.0092621 -4.0819426 -4.1679354 -4.2593403 -4.33294 -4.3731532 -4.3901877 -4.3976803][-4.2960787 -4.2637262 -4.2179165 -4.1491628 -4.0794916 -4.0487881 -4.0749564 -4.1317067 -4.1914859 -4.2466526 -4.2983279 -4.3394957 -4.3642612 -4.3791866 -4.3907051][-4.3102975 -4.2678623 -4.2272167 -4.18336 -4.1482596 -4.1437325 -4.1775074 -4.2261353 -4.2659841 -4.2933688 -4.3152618 -4.3331342 -4.3466682 -4.3608718 -4.3784466]]...]
INFO - root - 2017-12-05 09:52:33.285834: step 17110, loss = 1.45, batch loss = 1.39 (36.5 examples/sec; 0.219 sec/batch; 19h:13m:02s remains)
INFO - root - 2017-12-05 09:52:35.482256: step 17120, loss = 1.44, batch loss = 1.38 (37.4 examples/sec; 0.214 sec/batch; 18h:44m:58s remains)
INFO - root - 2017-12-05 09:52:37.661078: step 17130, loss = 1.57, batch loss = 1.51 (37.4 examples/sec; 0.214 sec/batch; 18h:43m:55s remains)
INFO - root - 2017-12-05 09:52:39.875222: step 17140, loss = 1.84, batch loss = 1.78 (35.9 examples/sec; 0.223 sec/batch; 19h:31m:33s remains)
INFO - root - 2017-12-05 09:52:42.068586: step 17150, loss = 1.43, batch loss = 1.38 (37.2 examples/sec; 0.215 sec/batch; 18h:49m:51s remains)
INFO - root - 2017-12-05 09:52:44.253177: step 17160, loss = 1.41, batch loss = 1.35 (37.1 examples/sec; 0.215 sec/batch; 18h:52m:03s remains)
INFO - root - 2017-12-05 09:52:46.455251: step 17170, loss = 1.53, batch loss = 1.47 (35.9 examples/sec; 0.223 sec/batch; 19h:29m:32s remains)
INFO - root - 2017-12-05 09:52:48.659097: step 17180, loss = 1.47, batch loss = 1.42 (36.4 examples/sec; 0.220 sec/batch; 19h:14m:55s remains)
INFO - root - 2017-12-05 09:52:50.843157: step 17190, loss = 1.37, batch loss = 1.32 (35.9 examples/sec; 0.223 sec/batch; 19h:31m:46s remains)
INFO - root - 2017-12-05 09:52:53.043088: step 17200, loss = 1.46, batch loss = 1.40 (36.4 examples/sec; 0.220 sec/batch; 19h:15m:55s remains)
2017-12-05 09:52:53.466337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4004359 -4.4006605 -4.3992863 -4.3962088 -4.391233 -4.384584 -4.3767381 -4.3673234 -4.3543487 -4.3360629 -4.3150682 -4.2997775 -4.3001885 -4.3161316 -4.3399744][-4.4015737 -4.4012027 -4.3983045 -4.3922982 -4.3825436 -4.3697171 -4.3561764 -4.3447857 -4.3358421 -4.3268557 -4.3174357 -4.3120241 -4.3184471 -4.3366551 -4.3589435][-4.4019403 -4.399632 -4.3929405 -4.3803649 -4.3604264 -4.3356681 -4.3128986 -4.2999611 -4.3002396 -4.3097644 -4.3234739 -4.3386755 -4.3553886 -4.3715572 -4.3849692][-4.4005013 -4.3947749 -4.3812122 -4.3560071 -4.3169589 -4.2707009 -4.2314978 -4.2146049 -4.2276349 -4.26168 -4.3031144 -4.3407011 -4.3697248 -4.3879657 -4.3970361][-4.3977656 -4.3869905 -4.3624749 -4.3185945 -4.2539 -4.1804676 -4.1215243 -4.1002865 -4.128087 -4.190671 -4.26251 -4.3231864 -4.3654089 -4.3885903 -4.398036][-4.3945174 -4.3772397 -4.3396807 -4.2760057 -4.1877389 -4.0934787 -4.0231867 -4.003756 -4.0474539 -4.13449 -4.2304592 -4.3086529 -4.360446 -4.3870482 -4.3970871][-4.3907957 -4.3664713 -4.3164973 -4.2382689 -4.1382751 -4.03893 -3.97051 -3.9593811 -4.0171165 -4.1196876 -4.226665 -4.3098483 -4.3624496 -4.3884106 -4.3975716][-4.3870597 -4.3561225 -4.2970452 -4.2126479 -4.1145816 -4.0255594 -3.9706116 -3.9719448 -4.03942 -4.1460381 -4.249804 -4.3258157 -4.370863 -4.3922324 -4.3989735][-4.3844967 -4.3502779 -4.2876525 -4.2045107 -4.117167 -4.0465112 -4.010572 -4.0246067 -4.0952067 -4.1946335 -4.2846608 -4.3466773 -4.3808279 -4.3961234 -4.4003162][-4.3842893 -4.3511524 -4.2917185 -4.2148967 -4.1392441 -4.0845737 -4.0644689 -4.0887613 -4.1574831 -4.2434282 -4.3160362 -4.3634462 -4.3881292 -4.3984861 -4.4009252][-4.3858862 -4.3578959 -4.307826 -4.2423553 -4.1773281 -4.1311955 -4.1178508 -4.1454406 -4.2079892 -4.2800789 -4.33758 -4.3736978 -4.3919125 -4.3990278 -4.4005957][-4.3874593 -4.3660131 -4.3280954 -4.2776012 -4.2249484 -4.1845016 -4.1709986 -4.1937923 -4.2454863 -4.3037958 -4.349227 -4.3777218 -4.39217 -4.3975348 -4.3985281][-4.3886752 -4.373168 -4.3466296 -4.3102417 -4.2692595 -4.2340131 -4.2179437 -4.2315788 -4.2708521 -4.3172255 -4.3538318 -4.3771 -4.3889713 -4.3932328 -4.3936019][-4.3903961 -4.3795385 -4.361619 -4.3360438 -4.3035436 -4.2719903 -4.2524986 -4.2561917 -4.2836246 -4.3209248 -4.3517108 -4.3712578 -4.3811817 -4.3841271 -4.3832912][-4.3924747 -4.3853493 -4.3740129 -4.3568254 -4.3315444 -4.30287 -4.2793627 -4.2731791 -4.2895117 -4.3181672 -4.3435097 -4.3594375 -4.3665404 -4.3668451 -4.3631778]]...]
INFO - root - 2017-12-05 09:52:55.678826: step 17210, loss = 1.23, batch loss = 1.17 (36.8 examples/sec; 0.217 sec/batch; 19h:01m:31s remains)
INFO - root - 2017-12-05 09:52:58.299488: step 17220, loss = 1.29, batch loss = 1.23 (20.6 examples/sec; 0.388 sec/batch; 33h:56m:56s remains)
INFO - root - 2017-12-05 09:53:02.146743: step 17230, loss = 1.24, batch loss = 1.18 (20.9 examples/sec; 0.383 sec/batch; 33h:33m:35s remains)
INFO - root - 2017-12-05 09:53:05.988705: step 17240, loss = 1.27, batch loss = 1.21 (21.3 examples/sec; 0.376 sec/batch; 32h:55m:44s remains)
INFO - root - 2017-12-05 09:53:09.852324: step 17250, loss = 1.34, batch loss = 1.29 (21.3 examples/sec; 0.376 sec/batch; 32h:54m:22s remains)
INFO - root - 2017-12-05 09:53:13.689991: step 17260, loss = 1.49, batch loss = 1.43 (20.8 examples/sec; 0.385 sec/batch; 33h:45m:05s remains)
INFO - root - 2017-12-05 09:53:17.509288: step 17270, loss = 2.03, batch loss = 1.97 (21.2 examples/sec; 0.377 sec/batch; 32h:59m:34s remains)
INFO - root - 2017-12-05 09:53:21.391235: step 17280, loss = 1.52, batch loss = 1.46 (20.1 examples/sec; 0.399 sec/batch; 34h:53m:39s remains)
INFO - root - 2017-12-05 09:53:25.274069: step 17290, loss = 1.47, batch loss = 1.41 (20.7 examples/sec; 0.386 sec/batch; 33h:50m:03s remains)
INFO - root - 2017-12-05 09:53:29.093951: step 17300, loss = 2.04, batch loss = 1.99 (20.5 examples/sec; 0.390 sec/batch; 34h:09m:47s remains)
2017-12-05 09:53:29.489728: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.0539546 2.4654994 2.5319195 2.3049636 1.8166938 1.1974607 0.56825733 0.05908823 -0.27311468 -0.42400002 -0.51496005 -0.68904877 -1.0486178 -1.609544 -2.3067465][3.4081688 4.0437164 4.2802696 4.1274776 3.5996027 2.8504577 2.0523582 1.3954391 0.97244215 0.79642105 0.70409536 0.50188541 0.04696846 -0.6877017 -1.6078186][3.8376551 4.7098522 5.1984863 5.2480021 4.8294182 4.0794315 3.2016168 2.44129 1.935811 1.716578 1.6083064 1.3778596 0.85122633 -0.0087423325 -1.0901144][3.6393204 4.7276659 5.5050192 5.844533 5.6639509 5.0391407 4.1734781 3.3429422 2.7298393 2.3967018 2.1874747 1.8630171 1.249475 0.30346632 -0.8620553][3.3861065 4.5856695 5.5810318 6.2071457 6.3257332 5.9335308 5.1843405 4.3340111 3.5797968 3.0231957 2.5663261 2.0181756 1.2432494 0.21271563 -0.97000718][3.5295691 4.7111959 5.7670097 6.5752964 6.9693928 6.8764896 6.3557053 5.5729742 4.6855059 3.8271751 2.9885406 2.0763373 1.032896 -0.13035727 -1.3172169][4.1256933 5.1711349 6.1070309 6.9411154 7.5086918 7.703043 7.4792223 6.8633404 5.9083948 4.7548828 3.4961905 2.1633825 0.795733 -0.53662205 -1.7403228][4.7208414 5.5831785 6.27923 6.9813728 7.5649557 7.9473352 7.9955492 7.6064348 6.7047577 5.3982306 3.8452282 2.1771522 0.53350925 -0.94645143 -2.1604774][4.6590738 5.3484983 5.7894068 6.2831526 6.7564955 7.1691256 7.3614607 7.170248 6.4517 5.2426596 3.6800938 1.9344506 0.20572329 -1.3205478 -2.5110931][3.7203302 4.254508 4.47439 4.7560053 5.0600891 5.3875389 5.60606 5.5568256 5.0945835 4.1889038 2.9058738 1.3824635 -0.18733072 -1.6104686 -2.7241981][2.1226182 2.5327196 2.610971 2.7289085 2.8739266 3.068749 3.23803 3.274704 3.0661535 2.5463543 1.7103438 0.60747147 -0.61990571 -1.8119013 -2.7987294][0.31678009 0.6239934 0.63323069 0.65278721 0.68751955 0.76310492 0.8572135 0.92751312 0.90833759 0.73423433 0.34932756 -0.28395081 -1.0978048 -1.990721 -2.8107519][-1.3482225 -1.1237011 -1.1376023 -1.1624947 -1.1823573 -1.1798258 -1.145294 -1.0747359 -0.99248385 -0.942333 -0.99320579 -1.238209 -1.6731446 -2.2608466 -2.8836834][-2.6325192 -2.4781046 -2.4969234 -2.5353346 -2.5707316 -2.5986724 -2.5988142 -2.5467458 -2.4387593 -2.2966042 -2.1778955 -2.1810393 -2.336864 -2.6599891 -3.0787034][-3.4756203 -3.3781734 -3.3883276 -3.4224904 -3.4574959 -3.4874308 -3.4987805 -3.4678559 -3.3785124 -3.2339177 -3.0774012 -2.9858165 -2.9948039 -3.133321 -3.3744431]]...]
INFO - root - 2017-12-05 09:53:33.259751: step 17310, loss = 1.53, batch loss = 1.47 (21.7 examples/sec; 0.368 sec/batch; 32h:12m:25s remains)
INFO - root - 2017-12-05 09:53:37.019401: step 17320, loss = 1.94, batch loss = 1.89 (21.0 examples/sec; 0.380 sec/batch; 33h:18m:25s remains)
INFO - root - 2017-12-05 09:53:40.788629: step 17330, loss = 1.24, batch loss = 1.18 (20.8 examples/sec; 0.384 sec/batch; 33h:39m:32s remains)
INFO - root - 2017-12-05 09:53:44.538007: step 17340, loss = 1.08, batch loss = 1.02 (20.9 examples/sec; 0.384 sec/batch; 33h:35m:18s remains)
INFO - root - 2017-12-05 09:53:48.238828: step 17350, loss = 1.91, batch loss = 1.85 (22.4 examples/sec; 0.358 sec/batch; 31h:18m:16s remains)
INFO - root - 2017-12-05 09:53:51.979504: step 17360, loss = 1.92, batch loss = 1.86 (21.3 examples/sec; 0.375 sec/batch; 32h:48m:31s remains)
INFO - root - 2017-12-05 09:53:55.745156: step 17370, loss = 1.50, batch loss = 1.44 (20.9 examples/sec; 0.382 sec/batch; 33h:27m:14s remains)
INFO - root - 2017-12-05 09:53:59.627143: step 17380, loss = 1.32, batch loss = 1.26 (21.0 examples/sec; 0.381 sec/batch; 33h:19m:56s remains)
INFO - root - 2017-12-05 09:54:03.437060: step 17390, loss = 1.32, batch loss = 1.26 (21.0 examples/sec; 0.381 sec/batch; 33h:19m:49s remains)
INFO - root - 2017-12-05 09:54:07.308739: step 17400, loss = 1.44, batch loss = 1.38 (21.2 examples/sec; 0.378 sec/batch; 33h:04m:32s remains)
2017-12-05 09:54:07.700283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3974571 -4.3979745 -4.3980646 -4.3977151 -4.3972168 -4.3970957 -4.3973012 -4.3974218 -4.3975468 -4.3976774 -4.3980083 -4.398663 -4.3993392 -4.3996143 -4.3995247][-4.3971562 -4.3978515 -4.398253 -4.3982954 -4.398108 -4.3979654 -4.3979959 -4.398056 -4.3980603 -4.3979545 -4.397965 -4.3983707 -4.3989348 -4.3991413 -4.3989229][-4.3975306 -4.3981214 -4.398622 -4.3989587 -4.3990622 -4.399004 -4.3988767 -4.3987546 -4.3985572 -4.3981605 -4.3978615 -4.3980303 -4.39849 -4.3986559 -4.3982058][-4.3982034 -4.3986492 -4.3991504 -4.3996525 -4.3999138 -4.3998995 -4.3996825 -4.3994241 -4.399056 -4.3983974 -4.3978539 -4.3979592 -4.3985128 -4.3986773 -4.397995][-4.3988004 -4.3991685 -4.3996844 -4.4003215 -4.4007173 -4.4006939 -4.4003754 -4.4000182 -4.3995662 -4.398767 -4.3980141 -4.3981466 -4.3989882 -4.399282 -4.3986621][-4.3991361 -4.3994665 -4.3999872 -4.4007459 -4.4012318 -4.4011016 -4.400672 -4.4002891 -4.3998094 -4.3989434 -4.398078 -4.3982024 -4.3992286 -4.3997049 -4.3994346][-4.3991895 -4.3994193 -4.3997512 -4.40046 -4.4008842 -4.4006009 -4.4000854 -4.3997674 -4.3993111 -4.3983946 -4.39746 -4.3974819 -4.3984118 -4.3990722 -4.3992357][-4.3989277 -4.3986678 -4.3984504 -4.398818 -4.3990541 -4.39867 -4.3982148 -4.3981538 -4.397934 -4.3970661 -4.396039 -4.3958845 -4.396605 -4.3973718 -4.3978629][-4.398169 -4.3971467 -4.3959646 -4.3956308 -4.3956141 -4.3951793 -4.394959 -4.3954315 -4.3957338 -4.3950572 -4.3940434 -4.3937526 -4.3941216 -4.3947043 -4.3953161][-4.3972793 -4.3955913 -4.3936582 -4.3927493 -4.3923931 -4.3918839 -4.3920875 -4.3930526 -4.3937454 -4.393384 -4.3926029 -4.3921919 -4.392067 -4.3922644 -4.3927732][-4.39705 -4.3953853 -4.393549 -4.3925109 -4.3919349 -4.3914142 -4.3916821 -4.392704 -4.3934135 -4.3933449 -4.3929195 -4.3924427 -4.3919 -4.3917413 -4.3919959][-4.3974919 -4.3965693 -4.3955183 -4.3947983 -4.3942409 -4.3937111 -4.3937283 -4.3942823 -4.3947406 -4.3948736 -4.3947129 -4.3942571 -4.39365 -4.3933506 -4.3933349][-4.398232 -4.3979259 -4.3975744 -4.3972721 -4.3969584 -4.3966241 -4.3964362 -4.3965206 -4.3966889 -4.3967814 -4.3967032 -4.3964128 -4.3960519 -4.3958731 -4.3958097][-4.3987355 -4.3986406 -4.3985763 -4.3985128 -4.3984609 -4.3983731 -4.3982635 -4.3982224 -4.3982248 -4.3982105 -4.3981323 -4.3979592 -4.3977833 -4.3976879 -4.3976612][-4.3989515 -4.3988628 -4.3988647 -4.3988771 -4.3988976 -4.3989086 -4.3988929 -4.3988719 -4.3988481 -4.3988256 -4.3987937 -4.3987317 -4.3986635 -4.3986163 -4.3985972]]...]
INFO - root - 2017-12-05 09:54:11.510734: step 17410, loss = 1.05, batch loss = 0.99 (20.3 examples/sec; 0.394 sec/batch; 34h:28m:01s remains)
INFO - root - 2017-12-05 09:54:15.261343: step 17420, loss = 1.57, batch loss = 1.51 (20.8 examples/sec; 0.384 sec/batch; 33h:36m:00s remains)
INFO - root - 2017-12-05 09:54:19.040496: step 17430, loss = 1.39, batch loss = 1.33 (20.3 examples/sec; 0.394 sec/batch; 34h:31m:16s remains)
INFO - root - 2017-12-05 09:54:22.936032: step 17440, loss = 1.26, batch loss = 1.20 (20.6 examples/sec; 0.389 sec/batch; 34h:00m:02s remains)
INFO - root - 2017-12-05 09:54:26.798085: step 17450, loss = 1.67, batch loss = 1.61 (21.8 examples/sec; 0.367 sec/batch; 32h:09m:07s remains)
INFO - root - 2017-12-05 09:54:30.652125: step 17460, loss = 1.44, batch loss = 1.38 (20.7 examples/sec; 0.386 sec/batch; 33h:46m:18s remains)
INFO - root - 2017-12-05 09:54:34.572601: step 17470, loss = 1.50, batch loss = 1.44 (19.2 examples/sec; 0.417 sec/batch; 36h:31m:50s remains)
INFO - root - 2017-12-05 09:54:38.528244: step 17480, loss = 1.03, batch loss = 0.97 (21.5 examples/sec; 0.372 sec/batch; 32h:31m:14s remains)
INFO - root - 2017-12-05 09:54:42.409478: step 17490, loss = 1.86, batch loss = 1.80 (19.9 examples/sec; 0.403 sec/batch; 35h:15m:45s remains)
INFO - root - 2017-12-05 09:54:46.403487: step 17500, loss = 1.64, batch loss = 1.58 (20.5 examples/sec; 0.391 sec/batch; 34h:13m:39s remains)
2017-12-05 09:54:46.813390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2030067 -1.074959 -1.0065351 -0.95683551 -0.92946053 -0.95391893 -1.0515347 -1.2335396 -1.4989135 -1.8230405 -2.1912897 -2.5763922 -2.9630775 -3.3205717 -3.6387708][0.30363321 0.50534153 0.62452316 0.71309376 0.76654673 0.74500847 0.61461306 0.35698032 -0.032272816 -0.51292872 -1.0623667 -1.6378858 -2.21938 -2.7601242 -3.2415824][1.8428593 2.1283569 2.308198 2.4487185 2.5383992 2.5326486 2.3802714 2.0553288 1.5477438 0.91315556 0.18303633 -0.58370876 -1.3678708 -2.1070278 -2.7727437][3.1832128 3.5538497 3.7981043 3.9887838 4.11407 4.1256266 3.9576998 3.5798283 2.9826317 2.2323437 1.3653765 0.44705963 -0.50601435 -1.4243314 -2.2696221][4.0684929 4.4950333 4.793519 5.0221739 5.1776857 5.2096863 5.0458555 4.6518507 4.0206852 3.220777 2.2924733 1.2951941 0.23694277 -0.81100392 -1.7995811][4.4377146 4.896471 5.2164354 5.4586267 5.631999 5.6927338 5.5622034 5.2040186 4.6042709 3.8293409 2.9213171 1.9247389 0.83626223 -0.28604841 -1.3756785][4.3843727 4.8425035 5.1520319 5.3888197 5.5754528 5.676609 5.6126642 5.3382854 4.8267612 4.1341238 3.3111081 2.3825865 1.3255129 0.18282557 -0.96926951][4.0785675 4.5021868 4.7683849 4.956213 5.1357031 5.2739182 5.2857456 5.1233368 4.7450781 4.1872635 3.504334 2.6963592 1.7206225 0.60011435 -0.5831387][3.5959988 3.9583693 4.1457205 4.2547655 4.3938503 4.5375443 4.6087713 4.5603142 4.3458338 3.9792576 3.4859633 2.8409672 1.9821715 0.9162612 -0.27022886][3.0289841 3.2861118 3.3407083 3.3522587 3.4266486 3.5578713 3.6850262 3.760601 3.7300863 3.5760307 3.2901287 2.8223915 2.0928922 1.0980291 -0.069128036][2.4301519 2.5626493 2.4728866 2.3908825 2.4099479 2.533946 2.7181 2.9129958 3.055191 3.0942497 2.9907846 2.6791625 2.0700593 1.1517925 0.017746449][1.9142075 1.9459796 1.7339907 1.5862203 1.5731807 1.7022138 1.9388189 2.22604 2.494658 2.6730494 2.6992636 2.4980626 1.974853 1.1176958 0.01500845][1.5900111 1.5642443 1.2799482 1.0910125 1.0621819 1.1958489 1.4641123 1.8053584 2.1453905 2.4029164 2.5013762 2.3525367 1.8686376 1.0448537 -0.034611702][1.4133801 1.3702497 1.0681686 0.86656189 0.8381958 0.97297525 1.24751 1.6032529 1.9655657 2.2487245 2.366353 2.2307034 1.7548513 0.9444046 -0.11833763][1.2158813 1.1956768 0.9115386 0.72970772 0.71773767 0.85684633 1.1249676 1.468317 1.8152075 2.0812821 2.178771 2.0265064 1.5414948 0.74466848 -0.28646708]]...]
INFO - root - 2017-12-05 09:54:50.701814: step 17510, loss = 1.57, batch loss = 1.51 (19.2 examples/sec; 0.417 sec/batch; 36h:30m:33s remains)
INFO - root - 2017-12-05 09:54:54.651317: step 17520, loss = 1.22, batch loss = 1.16 (20.0 examples/sec; 0.400 sec/batch; 34h:58m:43s remains)
INFO - root - 2017-12-05 09:54:58.597949: step 17530, loss = 1.17, batch loss = 1.12 (20.2 examples/sec; 0.396 sec/batch; 34h:40m:53s remains)
INFO - root - 2017-12-05 09:55:02.492476: step 17540, loss = 1.27, batch loss = 1.21 (20.7 examples/sec; 0.386 sec/batch; 33h:45m:48s remains)
INFO - root - 2017-12-05 09:55:06.331726: step 17550, loss = 1.55, batch loss = 1.49 (21.6 examples/sec; 0.370 sec/batch; 32h:20m:56s remains)
INFO - root - 2017-12-05 09:55:10.275998: step 17560, loss = 1.05, batch loss = 0.99 (21.5 examples/sec; 0.371 sec/batch; 32h:28m:53s remains)
INFO - root - 2017-12-05 09:55:14.168473: step 17570, loss = 1.52, batch loss = 1.46 (20.6 examples/sec; 0.388 sec/batch; 33h:56m:52s remains)
INFO - root - 2017-12-05 09:55:17.959132: step 17580, loss = 1.31, batch loss = 1.25 (21.0 examples/sec; 0.382 sec/batch; 33h:23m:04s remains)
INFO - root - 2017-12-05 09:55:21.833789: step 17590, loss = 2.07, batch loss = 2.01 (19.6 examples/sec; 0.408 sec/batch; 35h:40m:38s remains)
INFO - root - 2017-12-05 09:55:25.660332: step 17600, loss = 1.44, batch loss = 1.38 (20.8 examples/sec; 0.385 sec/batch; 33h:39m:24s remains)
2017-12-05 09:55:26.061369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6615596 -0.18080521 1.3904643 2.7894244 3.7832928 4.3159513 4.4719286 4.3927355 4.2298546 4.0855389 4.0037909 3.9427404 3.8459921 3.6915197 3.489799][-1.4509697 0.1143527 1.7587528 3.2109838 4.2334938 4.7633047 4.8834572 4.7208004 4.4108009 4.0594559 3.7368841 3.4390349 3.1469321 2.8549657 2.5773602][-1.3161302 0.29756069 1.9770131 3.4414353 4.4539728 4.949254 5.0121775 4.7636814 4.3261337 3.8111029 3.3073955 2.8376832 2.407577 2.0063386 1.6307402][-1.2118387 0.44195557 2.1482258 3.6144934 4.6029272 5.0502186 5.0523472 4.7416468 4.239769 3.6604691 3.0906825 2.5635571 2.0868535 1.6289935 1.1621881][-1.1811748 0.48158884 2.1870036 3.6322832 4.584816 4.9811192 4.9348164 4.5932655 4.0864983 3.5317788 3.0071554 2.5434995 2.1233191 1.6856241 1.1705866][-1.2466476 0.38848639 2.0636911 3.466692 4.3750582 4.7262459 4.6376076 4.2730303 3.7881327 3.3081188 2.9032335 2.5866022 2.3051472 1.9558506 1.4429336][-1.429538 0.12350798 1.719048 3.0504289 3.9057236 4.2144156 4.0851307 3.6993179 3.2395673 2.8452234 2.5784001 2.4302874 2.3084884 2.0729632 1.6039186][-1.7668877 -0.37509584 1.0617781 2.2643394 3.0353589 3.2959738 3.1423697 2.7606297 2.3463855 2.0449996 1.9029045 1.891562 1.8976736 1.7659831 1.3681774][-2.2402437 -1.0869586 0.10995579 1.1138716 1.751894 1.9460096 1.7891212 1.455616 1.1238179 0.92014837 0.87018728 0.93395519 1.0009279 0.93012333 0.60627222][-2.7810903 -1.9067545 -0.99701262 -0.23629761 0.24225807 0.37151814 0.23707962 -0.017159462 -0.24898958 -0.3634901 -0.36105967 -0.28514671 -0.22666168 -0.29000425 -0.55598736][-3.302489 -2.6993062 -2.0704961 -1.5453985 -1.2163262 -1.1333175 -1.2337077 -1.4080875 -1.5571849 -1.6193423 -1.6110289 -1.5708966 -1.5609419 -1.6426654 -1.8553112][-3.7529411 -3.3889103 -3.0074482 -2.6912479 -2.4937041 -2.4465191 -2.5120654 -2.62096 -2.7137151 -2.7522607 -2.7584453 -2.756273 -2.7792358 -2.8541818 -2.9941182][-4.0899796 -3.9100809 -3.7210259 -3.5658879 -3.46818 -3.4460111 -3.4836123 -3.544246 -3.5972567 -3.6224575 -3.6364522 -3.6481726 -3.6709878 -3.711906 -3.773427][-4.2879429 -4.2175808 -4.1425467 -4.0827289 -4.045156 -4.0372849 -4.054801 -4.0846739 -4.1131716 -4.1281219 -4.1383586 -4.1456575 -4.1552715 -4.166646 -4.1781473][-4.3766241 -4.3572359 -4.335403 -4.3184443 -4.3074064 -4.3052759 -4.3105712 -4.3227158 -4.3361449 -4.3438935 -4.3482862 -4.348402 -4.3470721 -4.3439732 -4.3383036]]...]
INFO - root - 2017-12-05 09:55:29.965055: step 17610, loss = 1.45, batch loss = 1.39 (21.2 examples/sec; 0.377 sec/batch; 32h:59m:36s remains)
INFO - root - 2017-12-05 09:55:33.848168: step 17620, loss = 1.97, batch loss = 1.91 (21.4 examples/sec; 0.374 sec/batch; 32h:44m:40s remains)
INFO - root - 2017-12-05 09:55:37.738517: step 17630, loss = 1.09, batch loss = 1.04 (22.2 examples/sec; 0.360 sec/batch; 31h:28m:39s remains)
INFO - root - 2017-12-05 09:55:41.661056: step 17640, loss = 1.40, batch loss = 1.35 (20.3 examples/sec; 0.395 sec/batch; 34h:32m:10s remains)
INFO - root - 2017-12-05 09:55:45.590199: step 17650, loss = 1.54, batch loss = 1.48 (19.6 examples/sec; 0.408 sec/batch; 35h:38m:46s remains)
INFO - root - 2017-12-05 09:55:49.512026: step 17660, loss = 1.99, batch loss = 1.93 (20.4 examples/sec; 0.392 sec/batch; 34h:16m:38s remains)
INFO - root - 2017-12-05 09:55:53.422267: step 17670, loss = 1.35, batch loss = 1.30 (21.4 examples/sec; 0.373 sec/batch; 32h:38m:12s remains)
INFO - root - 2017-12-05 09:55:57.257030: step 17680, loss = 2.09, batch loss = 2.03 (20.1 examples/sec; 0.398 sec/batch; 34h:46m:31s remains)
INFO - root - 2017-12-05 09:56:01.174780: step 17690, loss = 1.44, batch loss = 1.38 (20.3 examples/sec; 0.393 sec/batch; 34h:22m:51s remains)
INFO - root - 2017-12-05 09:56:04.988726: step 17700, loss = 2.01, batch loss = 1.95 (23.7 examples/sec; 0.337 sec/batch; 29h:29m:11s remains)
2017-12-05 09:56:05.372291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6088309 -3.4645352 -3.1659985 -2.660955 -2.0094435 -1.3587554 -0.90585423 -0.78377318 -1.0123281 -1.4952605 -2.0518682 -2.5155959 -2.8155346 -2.9872177 -3.099483][-3.7387204 -3.4900124 -2.9986336 -2.211565 -1.2237306 -0.24636078 0.44140291 0.62669325 0.26062393 -0.52322173 -1.4632363 -2.3101819 -2.9265013 -3.3105259 -3.5300868][-3.8865809 -3.5012515 -2.76424 -1.6369326 -0.25948381 1.0867143 2.0366874 2.288208 1.7730169 0.67487288 -0.66391706 -1.9111798 -2.8598323 -3.4735365 -3.815516][-3.9572361 -3.4072523 -2.3931453 -0.90539646 0.86561441 2.5721107 3.7659025 4.0672321 3.4065723 2.0193982 0.31584215 -1.3026898 -2.5692782 -3.4107947 -3.8818059][-3.9294538 -3.2051117 -1.9159362 -0.090407372 2.0311704 4.0413456 5.4263144 5.7479887 4.953043 3.3232031 1.3170371 -0.61674213 -2.1668017 -3.2218845 -3.821645][-3.8331957 -2.9393046 -1.3999667 0.71185207 3.1023641 5.3176966 6.8130703 7.1205053 6.2113314 4.4072866 2.1922665 0.026624203 -1.7539356 -2.9934781 -3.7112386][-3.6987762 -2.6658282 -0.941643 1.3579955 3.8948641 6.1910191 7.7048283 7.9697733 6.9778357 5.0848484 2.7756433 0.4902606 -1.4308789 -2.7987771 -3.6066093][-3.5565109 -2.4479158 -0.6469326 1.6995149 4.2262936 6.4650621 7.9107294 8.1206837 7.0983171 5.2122908 2.9283962 0.64800739 -1.3002753 -2.7117572 -3.557765][-3.4543576 -2.3578331 -0.61477828 1.6107402 3.9555626 5.9856462 7.259902 7.410387 6.4394827 4.6817904 2.5567427 0.42258549 -1.4194965 -2.76927 -3.5852165][-3.4401469 -2.4546862 -0.90812397 1.0383382 3.0531569 4.7577152 5.7924223 5.8858843 5.0405016 3.5218825 1.6787186 -0.18036842 -1.7940497 -2.9808292 -3.6978219][-3.5203066 -2.7181704 -1.4687357 0.089674473 1.6834998 3.0044341 3.779561 3.819067 3.1334419 1.9201493 0.44626617 -1.044311 -2.3425486 -3.2972338 -3.870466][-3.6754131 -3.0888853 -2.1803243 -1.0533481 0.088924885 1.0197096 1.5453143 1.5390487 1.0180416 0.12433672 -0.94904542 -2.0264757 -2.9604928 -3.6442091 -4.0523262][-3.877851 -3.4972379 -2.9112213 -2.1916866 -1.4715476 -0.8953135 -0.58785391 -0.62872648 -0.996248 -1.5875533 -2.2812145 -2.9621863 -3.5421517 -3.9600978 -4.2057185][-4.089067 -3.8739104 -3.5448203 -3.1492286 -2.7653897 -2.4697034 -2.3308086 -2.3876152 -2.6164727 -2.9525707 -3.3303771 -3.6887372 -3.9844491 -4.1925 -4.3119521][-4.2580314 -4.155993 -3.9981728 -3.8142204 -3.6445234 -3.5240395 -3.4810624 -3.5296443 -3.6503711 -3.8093095 -3.976563 -4.1267824 -4.244905 -4.325192 -4.3701057]]...]
INFO - root - 2017-12-05 09:56:09.286353: step 17710, loss = 1.82, batch loss = 1.76 (20.5 examples/sec; 0.390 sec/batch; 34h:04m:24s remains)
INFO - root - 2017-12-05 09:56:13.223697: step 17720, loss = 1.46, batch loss = 1.40 (20.4 examples/sec; 0.393 sec/batch; 34h:20m:22s remains)
INFO - root - 2017-12-05 09:56:17.138303: step 17730, loss = 1.55, batch loss = 1.49 (21.1 examples/sec; 0.380 sec/batch; 33h:12m:15s remains)
INFO - root - 2017-12-05 09:56:21.043423: step 17740, loss = 1.33, batch loss = 1.27 (20.9 examples/sec; 0.383 sec/batch; 33h:27m:11s remains)
INFO - root - 2017-12-05 09:56:24.868205: step 17750, loss = 1.41, batch loss = 1.35 (21.6 examples/sec; 0.371 sec/batch; 32h:27m:16s remains)
INFO - root - 2017-12-05 09:56:28.744842: step 17760, loss = 1.08, batch loss = 1.02 (20.9 examples/sec; 0.382 sec/batch; 33h:24m:08s remains)
INFO - root - 2017-12-05 09:56:32.498084: step 17770, loss = 1.78, batch loss = 1.72 (20.6 examples/sec; 0.389 sec/batch; 34h:00m:42s remains)
INFO - root - 2017-12-05 09:56:36.348154: step 17780, loss = 1.54, batch loss = 1.48 (21.5 examples/sec; 0.372 sec/batch; 32h:29m:15s remains)
INFO - root - 2017-12-05 09:56:40.189488: step 17790, loss = 1.44, batch loss = 1.38 (20.8 examples/sec; 0.385 sec/batch; 33h:38m:01s remains)
INFO - root - 2017-12-05 09:56:44.040589: step 17800, loss = 1.17, batch loss = 1.11 (20.4 examples/sec; 0.392 sec/batch; 34h:14m:33s remains)
2017-12-05 09:56:44.431434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3746223 -4.3818979 -4.3894453 -4.3936567 -4.3916636 -4.3836575 -4.3727136 -4.3627663 -4.3578582 -4.359767 -4.36736 -4.3778195 -4.3887782 -4.3969045 -4.4007845][-4.371654 -4.377718 -4.384376 -4.3863149 -4.3788381 -4.3622608 -4.3411355 -4.3221912 -4.3134971 -4.3188133 -4.3348975 -4.35606 -4.3772788 -4.3925858 -4.3998241][-4.3740077 -4.377614 -4.3813844 -4.3788323 -4.364006 -4.337162 -4.3043051 -4.275166 -4.26266 -4.2725267 -4.29888 -4.3324294 -4.3648133 -4.3878317 -4.3987474][-4.3794608 -4.3804841 -4.3802652 -4.3716974 -4.3485422 -4.3111391 -4.2665572 -4.2276812 -4.2119932 -4.2265997 -4.2634892 -4.30957 -4.3527212 -4.383 -4.3974309][-4.3849907 -4.3836989 -4.37915 -4.3635955 -4.3311982 -4.2825761 -4.2259541 -4.1781573 -4.1604347 -4.180521 -4.2278414 -4.2862511 -4.3398523 -4.3773565 -4.3955727][-4.3888497 -4.3848853 -4.3751512 -4.350965 -4.3075895 -4.24545 -4.1750336 -4.1185017 -4.1004038 -4.1278486 -4.1869183 -4.2585907 -4.323669 -4.369699 -4.3929219][-4.38821 -4.3811193 -4.3644686 -4.329401 -4.2716522 -4.1923728 -4.1057329 -4.0394988 -4.0229225 -4.0611086 -4.1358247 -4.2238607 -4.3031197 -4.359899 -4.3894858][-4.3796768 -4.3683977 -4.3433208 -4.29512 -4.2199349 -4.1208229 -4.0158138 -3.94062 -3.9279182 -3.9807394 -4.0752077 -4.1828232 -4.2789516 -4.3484688 -4.3854022][-4.3631611 -4.3477392 -4.3140354 -4.2522693 -4.1597 -4.0419397 -3.9211204 -3.8409381 -3.8352404 -3.9034867 -4.0170026 -4.1426892 -4.2547159 -4.3366323 -4.3810782][-4.3488588 -4.3301682 -4.2895989 -4.2182841 -4.1147289 -3.9873207 -3.8614542 -3.7839508 -3.7867799 -3.8650947 -3.9880545 -4.121357 -4.240653 -4.3293824 -4.378376][-4.346921 -4.3262796 -4.2834711 -4.2112455 -4.1096249 -3.9890361 -3.8741264 -3.8071189 -3.8163805 -3.8937333 -4.0096889 -4.1339383 -4.2462564 -4.331697 -4.3792262][-4.3566618 -4.3367667 -4.2974739 -4.2342086 -4.1483021 -4.050343 -3.959595 -3.9081085 -3.9193649 -3.9844928 -4.078773 -4.1799426 -4.2726846 -4.344151 -4.3836155][-4.3695526 -4.3533878 -4.3227048 -4.2752051 -4.2121415 -4.1419849 -4.0776954 -4.0416121 -4.0512533 -4.0987158 -4.1655045 -4.2384892 -4.3070183 -4.3599238 -4.3886676][-4.3803864 -4.3692679 -4.3485718 -4.3167553 -4.274622 -4.2281628 -4.1861405 -4.1627564 -4.1698122 -4.2005258 -4.2428784 -4.2906761 -4.3369007 -4.3731585 -4.392983][-4.3892117 -4.3825431 -4.3699512 -4.3500781 -4.3233738 -4.2939634 -4.2679605 -4.2536092 -4.2577906 -4.2762561 -4.3014469 -4.3310757 -4.3603525 -4.3836837 -4.3964324]]...]
INFO - root - 2017-12-05 09:56:48.257996: step 17810, loss = 1.28, batch loss = 1.22 (20.0 examples/sec; 0.400 sec/batch; 34h:56m:36s remains)
INFO - root - 2017-12-05 09:56:52.181468: step 17820, loss = 1.89, batch loss = 1.83 (20.1 examples/sec; 0.398 sec/batch; 34h:46m:02s remains)
INFO - root - 2017-12-05 09:56:55.986542: step 17830, loss = 1.71, batch loss = 1.65 (20.6 examples/sec; 0.389 sec/batch; 33h:58m:18s remains)
INFO - root - 2017-12-05 09:56:59.857380: step 17840, loss = 1.76, batch loss = 1.70 (21.8 examples/sec; 0.368 sec/batch; 32h:08m:37s remains)
INFO - root - 2017-12-05 09:57:03.816978: step 17850, loss = 1.21, batch loss = 1.15 (20.1 examples/sec; 0.399 sec/batch; 34h:52m:10s remains)
INFO - root - 2017-12-05 09:57:07.672828: step 17860, loss = 1.25, batch loss = 1.19 (20.8 examples/sec; 0.385 sec/batch; 33h:40m:31s remains)
INFO - root - 2017-12-05 09:57:11.602609: step 17870, loss = 1.60, batch loss = 1.54 (20.5 examples/sec; 0.390 sec/batch; 34h:04m:03s remains)
INFO - root - 2017-12-05 09:57:15.542225: step 17880, loss = 1.38, batch loss = 1.33 (20.5 examples/sec; 0.391 sec/batch; 34h:11m:05s remains)
INFO - root - 2017-12-05 09:57:19.377227: step 17890, loss = 1.24, batch loss = 1.18 (20.8 examples/sec; 0.384 sec/batch; 33h:33m:31s remains)
INFO - root - 2017-12-05 09:57:23.293776: step 17900, loss = 1.34, batch loss = 1.29 (21.2 examples/sec; 0.377 sec/batch; 32h:54m:29s remains)
2017-12-05 09:57:23.737407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.15914965 0.28164244 0.57237577 0.74409437 0.71649122 0.5286932 0.2136364 -0.18544865 -0.68673038 -1.282922 -1.9674301 -2.6918449 -3.3620191 -3.8727398 -4.181941][1.287919 1.7641735 2.0363164 2.1750717 2.0962696 1.8597879 1.4962964 1.0341606 0.43765259 -0.29700136 -1.1676402 -2.1039312 -2.9815073 -3.6637614 -4.0890255][2.683322 3.1246648 3.3024745 3.3556032 3.2162213 2.9531155 2.5868211 2.1110907 1.459991 0.61457968 -0.41747904 -1.5445459 -2.6128378 -3.4554043 -3.9931004][3.7518578 4.0815716 4.090127 3.9981737 3.78905 3.5253944 3.1988249 2.7559829 2.1046796 1.2101541 0.088243961 -1.153501 -2.3422306 -3.2958159 -3.9153214][4.3641806 4.5333967 4.3197985 4.0534592 3.7691612 3.5049634 3.2294674 2.8340201 2.2138791 1.3274975 0.20548487 -1.0366647 -2.2389505 -3.2235928 -3.8752046][4.5902748 4.606883 4.1908951 3.7749243 3.4271598 3.1714664 2.9418044 2.5794911 1.9812121 1.1189432 0.042460918 -1.1336446 -2.2761936 -3.2317271 -3.8724108][4.626503 4.5409231 3.9801779 3.4651737 3.0930119 2.8556356 2.6560097 2.3023534 1.6950574 0.83724451 -0.19464445 -1.2956517 -2.3619962 -3.26786 -3.8829412][4.5061741 4.4010859 3.777873 3.2242837 2.8632493 2.6460133 2.452621 2.0791817 1.4364204 0.55963612 -0.44601655 -1.4793403 -2.4677868 -3.3160949 -3.8961265][4.1183434 4.0610523 3.4520593 2.922524 2.5917106 2.3949194 2.1919565 1.785892 1.1076612 0.22227383 -0.74983454 -1.7137902 -2.6181617 -3.3942616 -3.924911][3.3800144 3.388588 2.8583446 2.4062858 2.1254644 1.9439869 1.7237291 1.3017554 0.62508011 -0.22862864 -1.136708 -2.0068238 -2.8086915 -3.4968071 -3.9690771][2.3152447 2.3918505 1.9799304 1.6313853 1.418323 1.2641206 1.0450349 0.64080334 0.014674187 -0.75935483 -1.5696807 -2.3340211 -3.022774 -3.6106052 -4.0169616][1.0731792 1.1842766 0.8813262 0.63943195 0.50484419 0.40158844 0.22246599 -0.12094498 -0.65472794 -1.3182416 -2.0127501 -2.6636271 -3.2371073 -3.7260132 -4.0662851][-0.26280928 -0.15709352 -0.36480188 -0.51721978 -0.57600522 -0.61401629 -0.725811 -0.97820163 -1.3912964 -1.9215517 -2.4902358 -3.019856 -3.4754434 -3.8590732 -4.1236882][-1.6225441 -1.5410078 -1.6751919 -1.7598124 -1.7669287 -1.7532613 -1.7952437 -1.9481673 -2.2260475 -2.6019416 -3.0170522 -3.4094212 -3.7438879 -4.0161657 -4.199213][-2.8416064 -2.7778292 -2.8552721 -2.8946805 -2.8810558 -2.8486524 -2.8494754 -2.920826 -3.0724349 -3.2926464 -3.5440183 -3.7857494 -3.9911847 -4.1574488 -4.2692184]]...]
INFO - root - 2017-12-05 09:57:27.664935: step 17910, loss = 1.33, batch loss = 1.27 (20.1 examples/sec; 0.398 sec/batch; 34h:45m:18s remains)
INFO - root - 2017-12-05 09:57:31.608170: step 17920, loss = 1.32, batch loss = 1.26 (20.6 examples/sec; 0.389 sec/batch; 33h:59m:28s remains)
INFO - root - 2017-12-05 09:57:35.522989: step 17930, loss = 1.49, batch loss = 1.43 (21.0 examples/sec; 0.381 sec/batch; 33h:15m:30s remains)
INFO - root - 2017-12-05 09:57:39.425999: step 17940, loss = 1.62, batch loss = 1.56 (21.1 examples/sec; 0.379 sec/batch; 33h:05m:26s remains)
INFO - root - 2017-12-05 09:57:43.287710: step 17950, loss = 1.53, batch loss = 1.47 (21.7 examples/sec; 0.368 sec/batch; 32h:10m:56s remains)
INFO - root - 2017-12-05 09:57:46.850823: step 17960, loss = 1.28, batch loss = 1.22 (36.8 examples/sec; 0.217 sec/batch; 18h:58m:32s remains)
INFO - root - 2017-12-05 09:57:50.737631: step 17970, loss = 1.37, batch loss = 1.31 (20.2 examples/sec; 0.395 sec/batch; 34h:32m:07s remains)
INFO - root - 2017-12-05 09:57:54.642079: step 17980, loss = 1.80, batch loss = 1.74 (20.2 examples/sec; 0.395 sec/batch; 34h:32m:33s remains)
INFO - root - 2017-12-05 09:57:58.597781: step 17990, loss = 1.89, batch loss = 1.84 (19.5 examples/sec; 0.410 sec/batch; 35h:49m:03s remains)
INFO - root - 2017-12-05 09:58:02.493766: step 18000, loss = 1.44, batch loss = 1.38 (19.7 examples/sec; 0.406 sec/batch; 35h:29m:54s remains)
2017-12-05 09:58:02.939035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9242077 -1.9116533 -1.8782125 -1.8016498 -1.6900883 -1.5777452 -1.507695 -1.5223243 -1.6537526 -1.9136939 -2.2819664 -2.6888313 -3.0546918 -3.3227093 -3.4771297][-2.0993855 -2.0430121 -1.9474916 -1.8000336 -1.6218433 -1.4578674 -1.3662426 -1.3949668 -1.5728524 -1.8970616 -2.3255582 -2.7758512 -3.165741 -3.4395516 -3.5860071][-2.3571079 -2.2651269 -2.1198225 -1.9151328 -1.6789629 -1.4651015 -1.3439989 -1.372148 -1.5741224 -1.9328604 -2.3875432 -2.850811 -3.2376604 -3.4933293 -3.6135468][-2.6091843 -2.4912424 -2.3132291 -2.073694 -1.80112 -1.551048 -1.3973343 -1.4035487 -1.5964079 -1.9492943 -2.3901598 -2.82999 -3.1856008 -3.4056787 -3.4911325][-2.8299925 -2.6907434 -2.4922342 -2.2363687 -1.9464879 -1.6732206 -1.4848089 -1.4466836 -1.5951889 -1.9038951 -2.2980738 -2.6876519 -2.9978166 -3.1801476 -3.2397223][-3.0038376 -2.8443456 -2.6296628 -2.3664405 -2.0711396 -1.7834189 -1.5606203 -1.4661648 -1.5479996 -1.7922375 -2.1267159 -2.464762 -2.7392037 -2.9039946 -2.964802][-3.1208415 -2.9337509 -2.7006974 -2.4300539 -2.1335948 -1.837918 -1.5859935 -1.4375455 -1.4546685 -1.6395173 -1.9270289 -2.2360053 -2.5063624 -2.6894553 -2.7853732][-3.1864924 -2.9614768 -2.7044561 -2.4220517 -2.1234767 -1.8258262 -1.5577536 -1.372375 -1.3445058 -1.4929366 -1.7601373 -2.0701401 -2.3659747 -2.5955997 -2.7454541][-3.2324958 -2.9625392 -2.6727619 -2.3714607 -2.0661538 -1.7728751 -1.5098877 -1.3187652 -1.2757127 -1.4114485 -1.6779327 -2.0042639 -2.3332927 -2.6112578 -2.810956][-3.3003397 -2.9897604 -2.6632504 -2.3357115 -2.0199857 -1.7390971 -1.5063982 -1.3470888 -1.3201318 -1.4594429 -1.7278461 -2.06165 -2.4040344 -2.7017746 -2.9200196][-3.414252 -3.0828328 -2.7351599 -2.3943584 -2.08271 -1.829633 -1.646244 -1.5404782 -1.5475664 -1.6943521 -1.9534352 -2.2702036 -2.5915151 -2.8662252 -3.0551963][-3.578166 -3.2562928 -2.9193075 -2.596101 -2.3155684 -2.1073525 -1.9791739 -1.9266012 -1.9658942 -2.1111536 -2.3423796 -2.6158409 -2.8844779 -3.1005387 -3.2233231][-3.7690084 -3.4851413 -3.1868353 -2.906496 -2.6763835 -2.5208313 -2.4427333 -2.4307628 -2.4875078 -2.6170726 -2.8058925 -3.0197842 -3.2201731 -3.3635182 -3.4105124][-3.9561896 -3.7267826 -3.4807611 -3.252578 -3.0751512 -2.9670362 -2.9274633 -2.9415565 -3.0028248 -3.1092329 -3.2508497 -3.4024363 -3.5342593 -3.6079612 -3.5884695][-4.1124783 -3.9386194 -3.7438502 -3.5612595 -3.4245372 -3.3512526 -3.3388247 -3.3688715 -3.4303772 -3.5166755 -3.6192079 -3.7177167 -3.7910714 -3.8077941 -3.738894]]...]
INFO - root - 2017-12-05 09:58:06.795152: step 18010, loss = 1.23, batch loss = 1.17 (21.6 examples/sec; 0.370 sec/batch; 32h:16m:51s remains)
INFO - root - 2017-12-05 09:58:10.636176: step 18020, loss = 1.65, batch loss = 1.59 (21.4 examples/sec; 0.373 sec/batch; 32h:37m:03s remains)
INFO - root - 2017-12-05 09:58:14.487455: step 18030, loss = 1.48, batch loss = 1.43 (21.2 examples/sec; 0.377 sec/batch; 32h:54m:45s remains)
INFO - root - 2017-12-05 09:58:18.298064: step 18040, loss = 1.13, batch loss = 1.07 (21.1 examples/sec; 0.379 sec/batch; 33h:05m:47s remains)
INFO - root - 2017-12-05 09:58:22.127384: step 18050, loss = 1.12, batch loss = 1.06 (20.7 examples/sec; 0.386 sec/batch; 33h:41m:01s remains)
INFO - root - 2017-12-05 09:58:25.946907: step 18060, loss = 1.97, batch loss = 1.91 (21.1 examples/sec; 0.379 sec/batch; 33h:08m:49s remains)
INFO - root - 2017-12-05 09:58:29.811578: step 18070, loss = 1.32, batch loss = 1.26 (20.7 examples/sec; 0.386 sec/batch; 33h:44m:35s remains)
INFO - root - 2017-12-05 09:58:33.703464: step 18080, loss = 1.62, batch loss = 1.56 (20.4 examples/sec; 0.392 sec/batch; 34h:16m:02s remains)
INFO - root - 2017-12-05 09:58:37.579316: step 18090, loss = 1.73, batch loss = 1.67 (20.3 examples/sec; 0.393 sec/batch; 34h:20m:57s remains)
INFO - root - 2017-12-05 09:58:41.532157: step 18100, loss = 2.07, batch loss = 2.01 (19.5 examples/sec; 0.409 sec/batch; 35h:45m:37s remains)
2017-12-05 09:58:41.924643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2170234 -4.1952891 -4.2016559 -4.2365718 -4.2864885 -4.3277555 -4.3471508 -4.3432646 -4.3236442 -4.3018727 -4.2920136 -4.2999372 -4.3210282 -4.34086 -4.3500557][-4.2256312 -4.2017603 -4.2053003 -4.2378354 -4.2859788 -4.3265834 -4.347208 -4.3465948 -4.330966 -4.3118038 -4.3014717 -4.3056397 -4.3210511 -4.3360481 -4.3430138][-4.2363262 -4.2119646 -4.2125864 -4.2401285 -4.281858 -4.3166533 -4.3336 -4.3319454 -4.3169975 -4.2988048 -4.2887383 -4.2918205 -4.304194 -4.3169951 -4.3238015][-4.2425752 -4.2193775 -4.2177572 -4.2385259 -4.2698631 -4.2935648 -4.3009748 -4.2927423 -4.2733703 -4.2528653 -4.2425084 -4.2470903 -4.2607503 -4.2759333 -4.2870636][-4.2408791 -4.2207813 -4.2181306 -4.23228 -4.2519627 -4.2618637 -4.2562289 -4.2370791 -4.2090878 -4.183713 -4.1730518 -4.1806822 -4.1993089 -4.220809 -4.2388015][-4.2298107 -4.2149253 -4.2136779 -4.2229881 -4.2323761 -4.2293792 -4.2104259 -4.179122 -4.1417685 -4.1111078 -4.1001172 -4.1115694 -4.1366792 -4.1653218 -4.1893444][-4.2132678 -4.20535 -4.208446 -4.2162619 -4.2190733 -4.2064595 -4.1767659 -4.1350851 -4.0900288 -4.0547314 -4.0425172 -4.0562849 -4.0858521 -4.1189833 -4.1456256][-4.2007251 -4.2004037 -4.2101731 -4.2204733 -4.2222123 -4.2052221 -4.168366 -4.11855 -4.0670619 -4.0273032 -4.0120835 -4.0248423 -4.054472 -4.0873113 -4.1118536][-4.204299 -4.2102385 -4.2261295 -4.2404342 -4.2444615 -4.22712 -4.1871014 -4.1324129 -4.0758467 -4.0311928 -4.0108438 -4.0184436 -4.0431871 -4.0706592 -4.089385][-4.2287526 -4.2375326 -4.2558932 -4.2719808 -4.2774296 -4.2609568 -4.2212267 -4.1654029 -4.1055574 -4.0562358 -4.02948 -4.0291576 -4.0459609 -4.0660195 -4.0786][-4.2652125 -4.2735076 -4.2904921 -4.3051515 -4.3105354 -4.2955947 -4.2577815 -4.2021875 -4.1408658 -4.0882807 -4.0551105 -4.0450134 -4.0522637 -4.0652823 -4.0741043][-4.303164 -4.3094897 -4.323132 -4.3351274 -4.3393564 -4.3258405 -4.2895794 -4.2334771 -4.1705322 -4.1145473 -4.0749 -4.0557384 -4.0542765 -4.0623622 -4.0713277][-4.3382311 -4.3428216 -4.352778 -4.3610253 -4.362536 -4.34902 -4.3134017 -4.2567725 -4.192029 -4.1330571 -4.0882564 -4.0619583 -4.0540762 -4.05966 -4.0722895][-4.3678665 -4.3711109 -4.37725 -4.3811774 -4.3791566 -4.3647022 -4.3305721 -4.2750087 -4.2096324 -4.1487226 -4.1012864 -4.0713129 -4.0598316 -4.0649137 -4.0827985][-4.3886142 -4.3903046 -4.3928981 -4.3933582 -4.3888636 -4.3747191 -4.3444386 -4.2941413 -4.2319641 -4.1728516 -4.1263609 -4.0960765 -4.0835714 -4.0888834 -4.1104784]]...]
INFO - root - 2017-12-05 09:58:45.822988: step 18110, loss = 1.43, batch loss = 1.37 (20.3 examples/sec; 0.393 sec/batch; 34h:21m:01s remains)
INFO - root - 2017-12-05 09:58:49.692078: step 18120, loss = 1.12, batch loss = 1.06 (20.5 examples/sec; 0.390 sec/batch; 34h:02m:28s remains)
INFO - root - 2017-12-05 09:58:53.515234: step 18130, loss = 1.12, batch loss = 1.07 (22.0 examples/sec; 0.363 sec/batch; 31h:41m:23s remains)
INFO - root - 2017-12-05 09:58:57.420105: step 18140, loss = 1.43, batch loss = 1.37 (20.3 examples/sec; 0.394 sec/batch; 34h:22m:37s remains)
INFO - root - 2017-12-05 09:59:01.272473: step 18150, loss = 1.73, batch loss = 1.67 (20.6 examples/sec; 0.388 sec/batch; 33h:54m:13s remains)
INFO - root - 2017-12-05 09:59:05.122612: step 18160, loss = 1.55, batch loss = 1.49 (21.6 examples/sec; 0.370 sec/batch; 32h:17m:28s remains)
INFO - root - 2017-12-05 09:59:08.996397: step 18170, loss = 1.89, batch loss = 1.83 (20.1 examples/sec; 0.397 sec/batch; 34h:40m:40s remains)
INFO - root - 2017-12-05 09:59:12.926955: step 18180, loss = 1.53, batch loss = 1.47 (20.7 examples/sec; 0.386 sec/batch; 33h:42m:46s remains)
INFO - root - 2017-12-05 09:59:16.736931: step 18190, loss = 1.36, batch loss = 1.30 (21.0 examples/sec; 0.381 sec/batch; 33h:18m:25s remains)
INFO - root - 2017-12-05 09:59:20.704084: step 18200, loss = 1.24, batch loss = 1.18 (20.0 examples/sec; 0.399 sec/batch; 34h:52m:32s remains)
2017-12-05 09:59:21.083060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4013386 -4.401454 -4.4015279 -4.4015 -4.4014173 -4.4014111 -4.4015303 -4.4016209 -4.4017229 -4.4017367 -4.4016194 -4.401433 -4.4012671 -4.401206 -4.4012284][-4.4014144 -4.4014359 -4.401351 -4.4011397 -4.4007311 -4.4004974 -4.4006295 -4.4010158 -4.4014339 -4.4016352 -4.4016123 -4.4014196 -4.4011755 -4.4010425 -4.4010305][-4.4016333 -4.4015427 -4.4012346 -4.4005785 -4.3994865 -4.3986697 -4.3985133 -4.3991137 -4.399961 -4.4005704 -4.4008584 -4.4007797 -4.400671 -4.4006538 -4.40077][-4.4019194 -4.4017453 -4.4011807 -4.4001284 -4.3984561 -4.3968472 -4.3961339 -4.3964915 -4.3975163 -4.3983822 -4.3988514 -4.3990006 -4.3991985 -4.3995647 -4.4001317][-4.4022894 -4.4020114 -4.4011889 -4.3998795 -4.398066 -4.395978 -4.3946514 -4.3946052 -4.3956666 -4.3965392 -4.3969178 -4.3970947 -4.3974671 -4.3981004 -4.39911][-4.4024553 -4.4020333 -4.4009562 -4.3994508 -4.3977265 -4.3955407 -4.3938584 -4.3935657 -4.3945622 -4.3954539 -4.3957143 -4.3958087 -4.3962317 -4.3969831 -4.3981781][-4.4025149 -4.402 -4.4007359 -4.3991318 -4.397409 -4.3953919 -4.3937449 -4.3933754 -4.3941946 -4.3950524 -4.3951716 -4.3952851 -4.3957148 -4.3965144 -4.397788][-4.4024472 -4.4020119 -4.4008646 -4.3993306 -4.3976784 -4.3959484 -4.3946071 -4.3942337 -4.3947515 -4.3954911 -4.3956971 -4.395864 -4.3961835 -4.39682 -4.3979454][-4.4023762 -4.4021039 -4.4012523 -4.4000025 -4.3985705 -4.397131 -4.3961077 -4.39583 -4.396059 -4.3965855 -4.3967781 -4.3968582 -4.3969975 -4.3974833 -4.3983955][-4.4022737 -4.4021978 -4.4016819 -4.4007759 -4.3995881 -4.398334 -4.3974338 -4.39721 -4.3974695 -4.3978648 -4.3979034 -4.39787 -4.3979921 -4.3983693 -4.3990078][-4.4021997 -4.4023795 -4.4022489 -4.4017053 -4.400806 -4.3997855 -4.399004 -4.3986969 -4.3988447 -4.3991504 -4.3991113 -4.3990169 -4.3991156 -4.3994393 -4.399796][-4.4021277 -4.4024954 -4.402761 -4.4027076 -4.402256 -4.4015722 -4.4009733 -4.4006104 -4.4005971 -4.4006896 -4.4005666 -4.4004107 -4.4003949 -4.4005542 -4.4006348][-4.4020305 -4.402473 -4.4029779 -4.4033027 -4.403275 -4.4029622 -4.4025588 -4.4022489 -4.4021993 -4.4021478 -4.4019675 -4.4017515 -4.4015427 -4.4013896 -4.4012017][-4.4018245 -4.4021759 -4.4026361 -4.4029937 -4.4030867 -4.402946 -4.4026952 -4.40252 -4.4025164 -4.4024911 -4.4024434 -4.4023285 -4.4020963 -4.4017768 -4.4014311][-4.4015627 -4.4017539 -4.4020405 -4.4022522 -4.4022956 -4.4021888 -4.4020267 -4.401938 -4.4019628 -4.4019985 -4.4020329 -4.4020228 -4.4019074 -4.4016767 -4.4013996]]...]
INFO - root - 2017-12-05 09:59:25.032494: step 18210, loss = 1.54, batch loss = 1.48 (20.3 examples/sec; 0.395 sec/batch; 34h:29m:10s remains)
INFO - root - 2017-12-05 09:59:28.856066: step 18220, loss = 1.37, batch loss = 1.32 (21.7 examples/sec; 0.369 sec/batch; 32h:13m:24s remains)
INFO - root - 2017-12-05 09:59:32.778989: step 18230, loss = 1.59, batch loss = 1.53 (20.4 examples/sec; 0.393 sec/batch; 34h:16m:44s remains)
INFO - root - 2017-12-05 09:59:36.672290: step 18240, loss = 1.21, batch loss = 1.15 (19.9 examples/sec; 0.402 sec/batch; 35h:07m:15s remains)
INFO - root - 2017-12-05 09:59:40.526068: step 18250, loss = 1.66, batch loss = 1.61 (21.6 examples/sec; 0.370 sec/batch; 32h:16m:40s remains)
INFO - root - 2017-12-05 09:59:44.385191: step 18260, loss = 1.55, batch loss = 1.49 (20.4 examples/sec; 0.392 sec/batch; 34h:10m:42s remains)
INFO - root - 2017-12-05 09:59:48.330106: step 18270, loss = 1.78, batch loss = 1.72 (19.1 examples/sec; 0.419 sec/batch; 36h:33m:55s remains)
INFO - root - 2017-12-05 09:59:52.169914: step 18280, loss = 1.52, batch loss = 1.46 (20.7 examples/sec; 0.386 sec/batch; 33h:39m:27s remains)
INFO - root - 2017-12-05 09:59:56.064477: step 18290, loss = 1.33, batch loss = 1.27 (20.4 examples/sec; 0.392 sec/batch; 34h:12m:28s remains)
INFO - root - 2017-12-05 09:59:59.932660: step 18300, loss = 1.37, batch loss = 1.31 (20.8 examples/sec; 0.384 sec/batch; 33h:29m:38s remains)
2017-12-05 10:00:00.302365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2772651 -4.2072406 -4.1425252 -4.1006265 -4.0903382 -4.1132097 -4.1609497 -4.2184243 -4.2735343 -4.3189316 -4.352562 -4.3751917 -4.3888059 -4.3963571 -4.4002328][-4.0533032 -3.8700309 -3.6990218 -3.5805709 -3.5400419 -3.5848958 -3.6967685 -3.8414667 -3.9890029 -4.1192694 -4.2234292 -4.2980833 -4.3462663 -4.3747959 -4.3901324][-3.6724296 -3.3038838 -2.9544947 -2.6987548 -2.5881054 -2.6405764 -2.8283882 -3.0979233 -3.3959038 -3.6800852 -3.9233537 -4.1095233 -4.2381282 -4.3187895 -4.3643689][-3.1762178 -2.5713096 -1.9838049 -1.5271816 -1.2841651 -1.2900021 -1.5223055 -1.917402 -2.4024665 -2.9035196 -3.3638387 -3.7409635 -4.0178146 -4.2015123 -4.3095636][-2.6737959 -1.8320127 -0.98760247 -0.28504515 0.16510057 0.30302286 0.11618137 -0.34696102 -1.0072958 -1.7545571 -2.4920878 -3.1387105 -3.6429272 -3.9946475 -4.2099805][-2.2857931 -1.2699566 -0.2175231 0.720314 1.4233031 1.8060293 1.797276 1.3893113 0.64036989 -0.31663513 -1.3404634 -2.301609 -3.0975249 -3.6797416 -4.0512905][-2.0798459 -0.990891 0.16646242 1.261714 2.1898394 2.8585935 3.1405172 2.9383001 2.2483807 1.1935372 -0.054240227 -1.314523 -2.4222376 -3.2713993 -3.8351729][-2.0431249 -0.97405887 0.17740297 1.3174214 2.377728 3.2856588 3.8803272 3.9796267 3.4912682 2.4840145 1.1302476 -0.3496871 -1.7276621 -2.8321462 -3.5925281][-2.1230371 -1.1293221 -0.054468632 1.0414567 2.1264262 3.1560259 3.9722776 4.3537064 4.1239471 3.2715015 1.940414 0.36882782 -1.1719949 -2.4587593 -3.3757286][-2.3041239 -1.41676 -0.45535636 0.5399518 1.5581069 2.5769639 3.4657617 4.0165005 4.0096445 3.3577285 2.1553435 0.63489866 -0.92169976 -2.2646596 -3.2513275][-2.5910454 -1.8382196 -1.021019 -0.1693387 0.70752907 1.5964403 2.4105935 2.9899907 3.1223922 2.6729403 1.6832633 0.35227585 -1.0610161 -2.3176284 -3.2661486][-2.9778996 -2.3869588 -1.7441998 -1.0752451 -0.39272594 0.29107475 0.92839 1.4203424 1.5990696 1.3263211 0.60033083 -0.43457651 -1.5703003 -2.6118107 -3.4189737][-3.4141803 -2.999789 -2.5463555 -2.0763319 -1.6038775 -1.1408534 -0.70993114 -0.36332369 -0.21306133 -0.36275816 -0.83150625 -1.5332398 -2.3241739 -3.0710893 -3.6657658][-3.8242111 -3.5743113 -3.2973619 -3.0099754 -2.7245355 -2.4510145 -2.1979067 -1.9928169 -1.9015861 -1.9813945 -2.24644 -2.6514597 -3.1182585 -3.5703518 -3.9378426][-4.1372986 -4.0145783 -3.8741565 -3.726532 -3.5802603 -3.4422002 -3.3139639 -3.2107217 -3.1682 -3.2083669 -3.3352342 -3.5288262 -3.7565277 -3.9811177 -4.1651549]]...]
INFO - root - 2017-12-05 10:00:04.139327: step 18310, loss = 1.07, batch loss = 1.01 (21.2 examples/sec; 0.378 sec/batch; 32h:57m:30s remains)
INFO - root - 2017-12-05 10:00:08.085841: step 18320, loss = 1.56, batch loss = 1.50 (19.6 examples/sec; 0.407 sec/batch; 35h:32m:49s remains)
INFO - root - 2017-12-05 10:00:11.978101: step 18330, loss = 1.56, batch loss = 1.50 (20.1 examples/sec; 0.399 sec/batch; 34h:47m:36s remains)
INFO - root - 2017-12-05 10:00:15.866434: step 18340, loss = 1.47, batch loss = 1.41 (20.7 examples/sec; 0.386 sec/batch; 33h:41m:23s remains)
INFO - root - 2017-12-05 10:00:19.799908: step 18350, loss = 1.85, batch loss = 1.80 (20.7 examples/sec; 0.387 sec/batch; 33h:47m:24s remains)
INFO - root - 2017-12-05 10:00:23.651157: step 18360, loss = 1.58, batch loss = 1.52 (21.7 examples/sec; 0.368 sec/batch; 32h:07m:53s remains)
INFO - root - 2017-12-05 10:00:27.535345: step 18370, loss = 1.41, batch loss = 1.35 (20.5 examples/sec; 0.391 sec/batch; 34h:05m:01s remains)
INFO - root - 2017-12-05 10:00:31.469680: step 18380, loss = 1.46, batch loss = 1.40 (20.5 examples/sec; 0.390 sec/batch; 34h:02m:22s remains)
INFO - root - 2017-12-05 10:00:35.397658: step 18390, loss = 1.87, batch loss = 1.82 (22.3 examples/sec; 0.359 sec/batch; 31h:19m:01s remains)
INFO - root - 2017-12-05 10:00:39.137324: step 18400, loss = 1.24, batch loss = 1.18 (21.3 examples/sec; 0.376 sec/batch; 32h:50m:25s remains)
2017-12-05 10:00:39.561606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4023962 -4.4020796 -4.4005313 -4.3964729 -4.3900051 -4.3832712 -4.37906 -4.3795738 -4.384078 -4.3897204 -4.3939233 -4.3948116 -4.3929305 -4.39047 -4.3888407][-4.4021864 -4.4006157 -4.39438 -4.3792534 -4.3546391 -4.3269057 -4.3072734 -4.3050923 -4.3200364 -4.3422956 -4.3620234 -4.3741832 -4.3789468 -4.3807898 -4.3823953][-4.4019694 -4.397316 -4.3798866 -4.33872 -4.2714658 -4.1940141 -4.1359305 -4.1234026 -4.1593342 -4.2215919 -4.2831383 -4.3283458 -4.354866 -4.3692012 -4.3774819][-4.4011841 -4.3909307 -4.3539238 -4.2674856 -4.1248832 -3.9562397 -3.8242407 -3.7872615 -3.857579 -3.9928029 -4.13639 -4.2488618 -4.3190556 -4.3564153 -4.3744822][-4.3994803 -4.3807392 -4.3156104 -4.1654038 -3.9158971 -3.6145315 -3.3697779 -3.2912707 -3.4093952 -3.6541576 -3.9244118 -4.1410217 -4.2759328 -4.3443089 -4.3737507][-4.3962035 -4.36684 -4.2688518 -4.0453033 -3.6729403 -3.2160954 -2.8333204 -2.6995454 -2.8728518 -3.2518744 -3.6776578 -4.0204782 -4.2317395 -4.33505 -4.3753862][-4.3914404 -4.3516479 -4.2236996 -3.9355569 -3.4547708 -2.8572032 -2.3421333 -2.1499689 -2.3757968 -2.8858957 -3.4598031 -3.9192073 -4.198916 -4.3312225 -4.3785815][-4.386384 -4.3391409 -4.1928978 -3.8687842 -3.3288326 -2.6529212 -2.0590239 -1.825686 -2.0856762 -2.6822844 -3.3471329 -3.8735485 -4.1895232 -4.3343921 -4.3824372][-4.3814979 -4.3323846 -4.1866217 -3.8674631 -3.3379045 -2.6767361 -2.092737 -1.8604817 -2.1230991 -2.7230144 -3.3844404 -3.9019306 -4.2076454 -4.3434534 -4.3859305][-4.3765059 -4.3330173 -4.2060628 -3.9302049 -3.4742568 -2.9095588 -2.4146962 -2.2245035 -2.4602609 -2.9838688 -3.5537229 -3.9919403 -4.2461324 -4.3558741 -4.3883486][-4.3713923 -4.33908 -4.2426753 -4.0326562 -3.6868591 -3.2634392 -2.8992879 -2.7691035 -2.9558849 -3.3556986 -3.7843518 -4.1076617 -4.2912078 -4.3686066 -4.3898134][-4.3687067 -4.3492765 -4.2862248 -4.1475534 -3.919183 -3.6426039 -3.4104533 -3.3350153 -3.4638419 -3.7289662 -4.008821 -4.2160807 -4.331532 -4.3793526 -4.3907361][-4.3694568 -4.3608403 -4.3259959 -4.2482066 -4.1199188 -3.9659884 -3.8400345 -3.8035338 -3.8782935 -4.0276747 -4.183712 -4.2978249 -4.3607388 -4.3863006 -4.3908243][-4.372817 -4.3716297 -4.3564677 -4.320159 -4.2604332 -4.1894355 -4.1325374 -4.1180086 -4.1535721 -4.2227731 -4.2948651 -4.348012 -4.3779778 -4.3899317 -4.3908854][-4.3790011 -4.3816733 -4.3775973 -4.3643875 -4.3417845 -4.3151045 -4.2941875 -4.2894578 -4.3029661 -4.3286252 -4.3555536 -4.3758154 -4.3881617 -4.393034 -4.3926878]]...]
INFO - root - 2017-12-05 10:00:43.406080: step 18410, loss = 1.39, batch loss = 1.34 (20.7 examples/sec; 0.386 sec/batch; 33h:40m:55s remains)
INFO - root - 2017-12-05 10:00:47.195018: step 18420, loss = 1.31, batch loss = 1.25 (20.8 examples/sec; 0.385 sec/batch; 33h:32m:59s remains)
INFO - root - 2017-12-05 10:00:51.077954: step 18430, loss = 1.41, batch loss = 1.35 (20.3 examples/sec; 0.394 sec/batch; 34h:21m:05s remains)
INFO - root - 2017-12-05 10:00:54.837041: step 18440, loss = 1.40, batch loss = 1.34 (21.2 examples/sec; 0.377 sec/batch; 32h:53m:58s remains)
INFO - root - 2017-12-05 10:00:58.683756: step 18450, loss = 1.24, batch loss = 1.18 (20.4 examples/sec; 0.392 sec/batch; 34h:10m:21s remains)
INFO - root - 2017-12-05 10:01:02.524969: step 18460, loss = 1.42, batch loss = 1.36 (20.8 examples/sec; 0.385 sec/batch; 33h:33m:59s remains)
INFO - root - 2017-12-05 10:01:06.395524: step 18470, loss = 1.47, batch loss = 1.41 (21.1 examples/sec; 0.380 sec/batch; 33h:08m:01s remains)
INFO - root - 2017-12-05 10:01:10.181698: step 18480, loss = 1.46, batch loss = 1.40 (21.2 examples/sec; 0.377 sec/batch; 32h:51m:57s remains)
INFO - root - 2017-12-05 10:01:14.030354: step 18490, loss = 1.86, batch loss = 1.80 (20.6 examples/sec; 0.389 sec/batch; 33h:56m:08s remains)
INFO - root - 2017-12-05 10:01:17.886553: step 18500, loss = 1.76, batch loss = 1.70 (20.9 examples/sec; 0.383 sec/batch; 33h:22m:12s remains)
2017-12-05 10:01:18.267061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2442007 -4.1503549 -4.0501885 -3.9537735 -3.876807 -3.8278832 -3.8041229 -3.7953961 -3.7866104 -3.7754672 -3.7547226 -3.7303209 -3.709831 -3.7049129 -3.7226682][-3.9948614 -3.7744906 -3.5350869 -3.2989225 -3.0973728 -2.9525471 -2.8670049 -2.82171 -2.7910328 -2.7670383 -2.7376828 -2.7125587 -2.6950717 -2.7050056 -2.7552004][-3.5539153 -3.151973 -2.7191272 -2.2914853 -1.9184575 -1.6392787 -1.4604356 -1.3557973 -1.2945802 -1.2672935 -1.2511723 -1.2505014 -1.2559586 -1.2929325 -1.3762305][-2.9358053 -2.3357189 -1.7054021 -1.0920868 -0.55734682 -0.14845848 0.12977123 0.30534744 0.40596342 0.43696117 0.42640877 0.38026428 0.32754946 0.24670839 0.13397169][-2.2432876 -1.4768007 -0.68720603 0.076575756 0.75200558 1.2857966 1.668777 1.9231181 2.067771 2.1042895 2.0610542 1.9609561 1.8545718 1.7339587 1.6092496][-1.6019614 -0.731786 0.15542412 1.0285873 1.8311296 2.5033941 3.0249705 3.400332 3.6322241 3.7143316 3.6639347 3.5162477 3.3457713 3.1736431 3.0272069][-1.1041334 -0.19002962 0.74801016 1.7098656 2.6453571 3.4877782 4.1924629 4.73458 5.0977221 5.2703295 5.2566013 5.0925426 4.8646526 4.6245279 4.4196806][-0.75243258 0.18774891 1.1678557 2.2210865 3.3042769 4.3365393 5.2394238 5.9573393 6.4582853 6.7289 6.7652807 6.6011 6.3163452 5.9798908 5.6639938][-0.48186445 0.50936317 1.5559545 2.7108078 3.9347181 5.1293955 6.1813765 7.0140934 7.5958672 7.9193125 7.9751158 7.7866726 7.4229078 6.9604492 6.4928102][-0.2477026 0.82607603 1.9611793 3.2084494 4.52783 5.8117638 6.92785 7.7828569 8.3514566 8.6373663 8.6311388 8.3465328 7.8495607 7.2291622 6.6000347][-0.055519581 1.0976558 2.302331 3.5931473 4.9268103 6.19718 7.2644749 8.0343008 8.4891253 8.6398029 8.48219 8.0240335 7.3451347 6.5546484 5.7932882][0.015332699 1.1885142 2.3923712 3.6464529 4.9019194 6.0534744 6.9689627 7.5678968 7.8417997 7.8073158 7.4634857 6.818841 5.9694033 5.050004 4.2213297][-0.1302762 0.97545528 2.0888681 3.2244287 4.3266211 5.2953224 6.0121908 6.4156084 6.5086288 6.3080873 5.8158121 5.0430412 4.09653 3.1271296 2.3021407][-0.49184632 0.46907759 1.4185939 2.3798504 3.2968764 4.0773573 4.6155329 4.8661728 4.8340225 4.5365582 3.9797764 3.1768308 2.23658 1.3070431 0.5428443][-1.001085 -0.2245307 0.52472162 1.2861223 2.0111685 2.6206355 3.0243258 3.187654 3.1140738 2.8156562 2.2985163 1.5778475 0.74765968 -0.062541485 -0.72523236]]...]
INFO - root - 2017-12-05 10:01:22.138311: step 18510, loss = 1.51, batch loss = 1.45 (20.6 examples/sec; 0.389 sec/batch; 33h:55m:14s remains)
INFO - root - 2017-12-05 10:01:26.019956: step 18520, loss = 1.35, batch loss = 1.30 (21.1 examples/sec; 0.380 sec/batch; 33h:07m:28s remains)
INFO - root - 2017-12-05 10:01:29.904850: step 18530, loss = 1.27, batch loss = 1.21 (20.7 examples/sec; 0.387 sec/batch; 33h:43m:41s remains)
INFO - root - 2017-12-05 10:01:33.724633: step 18540, loss = 1.26, batch loss = 1.20 (19.9 examples/sec; 0.402 sec/batch; 35h:01m:59s remains)
INFO - root - 2017-12-05 10:01:37.533047: step 18550, loss = 1.20, batch loss = 1.14 (22.1 examples/sec; 0.362 sec/batch; 31h:35m:53s remains)
INFO - root - 2017-12-05 10:01:41.333860: step 18560, loss = 1.41, batch loss = 1.35 (20.2 examples/sec; 0.396 sec/batch; 34h:30m:42s remains)
INFO - root - 2017-12-05 10:01:45.160499: step 18570, loss = 1.32, batch loss = 1.26 (20.4 examples/sec; 0.393 sec/batch; 34h:16m:31s remains)
INFO - root - 2017-12-05 10:01:49.051801: step 18580, loss = 1.55, batch loss = 1.49 (20.4 examples/sec; 0.392 sec/batch; 34h:09m:29s remains)
INFO - root - 2017-12-05 10:01:52.948305: step 18590, loss = 1.28, batch loss = 1.22 (20.0 examples/sec; 0.400 sec/batch; 34h:54m:24s remains)
INFO - root - 2017-12-05 10:01:56.783996: step 18600, loss = 1.63, batch loss = 1.57 (20.7 examples/sec; 0.386 sec/batch; 33h:39m:58s remains)
2017-12-05 10:01:57.209676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4007659 -4.4002118 -4.4000821 -4.4000978 -4.4002552 -4.4008737 -4.4017949 -4.4024248 -4.4028149 -4.4027524 -4.4022031 -4.4019012 -4.4020724 -4.4023647 -4.4026227][-4.4021626 -4.4016566 -4.4011 -4.4005647 -4.4000893 -4.4003024 -4.4011579 -4.4018083 -4.4021616 -4.4020844 -4.4016485 -4.4013476 -4.4013681 -4.4013844 -4.4014363][-4.4024615 -4.4019375 -4.4011188 -4.40029 -4.3996673 -4.3996544 -4.4002194 -4.400599 -4.4008512 -4.4008274 -4.4005628 -4.4004669 -4.4004669 -4.4002538 -4.4001393][-4.4024844 -4.4019814 -4.4012442 -4.4004693 -4.3998528 -4.3995972 -4.3996172 -4.3996596 -4.3997746 -4.3996329 -4.3994651 -4.3995733 -4.3995419 -4.3992667 -4.3992305][-4.402844 -4.4024096 -4.4017177 -4.400857 -4.4000998 -4.3995318 -4.3990765 -4.3988867 -4.3989482 -4.398788 -4.3984013 -4.3982754 -4.3980355 -4.3974967 -4.39761][-4.4031205 -4.4024658 -4.4016113 -4.400496 -4.3995357 -4.3987412 -4.3979273 -4.3975248 -4.3977942 -4.3979177 -4.39751 -4.3969517 -4.3963647 -4.3957205 -4.3960309][-4.4018836 -4.40087 -4.399786 -4.3985987 -4.3974729 -4.3962049 -4.3947268 -4.3939676 -4.394588 -4.3952146 -4.3953857 -4.3949542 -4.3942623 -4.3939095 -4.3946166][-4.396368 -4.3947167 -4.3933716 -4.3921142 -4.3906541 -4.388864 -4.3869667 -4.3861613 -4.3872614 -4.38876 -4.3896866 -4.3896589 -4.3892031 -4.38932 -4.3904448][-4.3888149 -4.3867888 -4.385417 -4.3842854 -4.3830671 -4.38164 -4.3803439 -4.38003 -4.3811345 -4.3826666 -4.3839049 -4.384491 -4.3846927 -4.3853707 -4.3866658][-4.38042 -4.3776536 -4.3758984 -4.3748984 -4.3741441 -4.3733711 -4.3726439 -4.3724632 -4.3730049 -4.3741441 -4.3753228 -4.3762975 -4.3771195 -4.3785605 -4.3803964][-4.3693714 -4.36586 -4.3638382 -4.3627248 -4.3619752 -4.3613405 -4.3608685 -4.3605766 -4.3605657 -4.3611403 -4.3621445 -4.363009 -4.3639174 -4.3656411 -4.3677015][-4.3625903 -4.3586493 -4.3561249 -4.3546987 -4.3537555 -4.3531632 -4.3529453 -4.3526921 -4.3523149 -4.3522015 -4.352612 -4.3528152 -4.35317 -4.3543715 -4.3559113][-4.3634462 -4.3593993 -4.3567076 -4.3551726 -4.3541856 -4.3536434 -4.3534193 -4.353065 -4.3523932 -4.3516822 -4.3513923 -4.3509359 -4.3507185 -4.3515286 -4.3524342][-4.3728371 -4.3692579 -4.3668242 -4.3653274 -4.3643122 -4.36374 -4.3633947 -4.3629017 -4.3619337 -4.3608975 -4.3601642 -4.35927 -4.3585477 -4.3584671 -4.3583465][-4.3849812 -4.3821311 -4.38005 -4.3788657 -4.3779912 -4.3774767 -4.3771982 -4.3768959 -4.3761597 -4.37524 -4.3745279 -4.3735991 -4.3726435 -4.3718247 -4.3710327]]...]
INFO - root - 2017-12-05 10:02:01.089616: step 18610, loss = 1.22, batch loss = 1.16 (21.2 examples/sec; 0.378 sec/batch; 32h:57m:24s remains)
INFO - root - 2017-12-05 10:02:04.961304: step 18620, loss = 1.37, batch loss = 1.32 (20.6 examples/sec; 0.388 sec/batch; 33h:49m:02s remains)
INFO - root - 2017-12-05 10:02:08.782586: step 18630, loss = 1.51, batch loss = 1.46 (21.4 examples/sec; 0.375 sec/batch; 32h:39m:16s remains)
INFO - root - 2017-12-05 10:02:12.689909: step 18640, loss = 1.31, batch loss = 1.25 (20.5 examples/sec; 0.390 sec/batch; 33h:59m:53s remains)
INFO - root - 2017-12-05 10:02:16.590066: step 18650, loss = 1.14, batch loss = 1.09 (20.9 examples/sec; 0.384 sec/batch; 33h:26m:40s remains)
INFO - root - 2017-12-05 10:02:20.450269: step 18660, loss = 2.20, batch loss = 2.14 (24.2 examples/sec; 0.331 sec/batch; 28h:49m:23s remains)
INFO - root - 2017-12-05 10:02:24.321930: step 18670, loss = 1.61, batch loss = 1.55 (21.7 examples/sec; 0.369 sec/batch; 32h:08m:47s remains)
INFO - root - 2017-12-05 10:02:28.232298: step 18680, loss = 1.29, batch loss = 1.23 (21.2 examples/sec; 0.378 sec/batch; 32h:55m:48s remains)
INFO - root - 2017-12-05 10:02:32.046807: step 18690, loss = 1.00, batch loss = 0.95 (21.6 examples/sec; 0.371 sec/batch; 32h:19m:34s remains)
INFO - root - 2017-12-05 10:02:35.866001: step 18700, loss = 1.81, batch loss = 1.75 (20.3 examples/sec; 0.394 sec/batch; 34h:20m:43s remains)
2017-12-05 10:02:36.267138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1533022 -3.9189734 -3.5186517 -3.006948 -2.4954391 -2.1036572 -1.9038043 -1.8876421 -1.9760635 -2.0874958 -2.2291682 -2.4637537 -2.8061061 -3.2125797 -3.6129007][-4.1396246 -3.8868504 -3.4487031 -2.8722463 -2.2781868 -1.815141 -1.5916681 -1.6158092 -1.7943313 -2.0157168 -2.2562449 -2.555124 -2.9180307 -3.3147092 -3.6884668][-4.1335216 -3.8562145 -3.3755021 -2.7343907 -2.0652003 -1.54894 -1.3267179 -1.4196074 -1.7133985 -2.0622246 -2.40625 -2.7572508 -3.1190462 -3.4809413 -3.8069873][-4.1296096 -3.8183084 -3.2851627 -2.5764954 -1.8407364 -1.2881167 -1.0839112 -1.2589386 -1.6749337 -2.1505351 -2.5902712 -2.9839654 -3.3381262 -3.6585822 -3.9316375][-4.126946 -3.7785268 -3.1895876 -2.4152925 -1.6214521 -1.0395088 -0.85466838 -1.104424 -1.6299846 -2.2176564 -2.7419138 -3.1755331 -3.5283751 -3.81594 -4.0419073][-4.1255956 -3.7465873 -3.1113086 -2.2825546 -1.440774 -0.829911 -0.65089536 -0.94979692 -1.5557439 -2.2326598 -2.8316746 -3.3088727 -3.6714418 -3.9397666 -4.1306963][-4.1298642 -3.7368324 -3.078773 -2.2182977 -1.3440943 -0.70540237 -0.5126543 -0.82299662 -1.4688485 -2.2067294 -2.8685207 -3.3903975 -3.7708964 -4.0304494 -4.1972][-4.1466479 -3.7627933 -3.1158848 -2.2608492 -1.3810284 -0.72441053 -0.50338578 -0.78698039 -1.4269385 -2.189291 -2.8904407 -3.44579 -3.8420417 -4.0959115 -4.244844][-4.1782 -3.8290727 -3.2314537 -2.4281039 -1.5850794 -0.93677092 -0.69124961 -0.92456174 -1.517108 -2.2568331 -2.9563622 -3.5162015 -3.9102168 -4.1506214 -4.281671][-4.2197609 -3.9256744 -3.4112768 -2.7053058 -1.9465175 -1.3465931 -1.0988028 -1.2806265 -1.7986906 -2.4677062 -3.1134105 -3.6332297 -3.9934096 -4.2037973 -4.3126287][-4.2649274 -4.0364656 -3.6267531 -3.0509057 -2.4168751 -1.9061365 -1.6860139 -1.8270636 -2.256125 -2.8175247 -3.3613365 -3.7964435 -4.0901895 -4.2559619 -4.3388243][-4.308104 -4.1456156 -3.8451622 -3.409539 -2.9173369 -2.5160623 -2.3423584 -2.4514024 -2.7869911 -3.2242725 -3.6430407 -3.9709084 -4.1843381 -4.3012762 -4.3583484][-4.3431334 -4.2367516 -4.0328569 -3.7277236 -3.373282 -3.0810289 -2.9568243 -3.0407233 -3.2880554 -3.6035769 -3.8983321 -4.1212916 -4.2603211 -4.3343472 -4.3704925][-4.3665037 -4.3022528 -4.1737576 -3.9747076 -3.7386889 -3.5428467 -3.4638195 -3.5277672 -3.6981428 -3.9069297 -4.0941491 -4.2300463 -4.3116879 -4.354775 -4.3770757][-4.3777218 -4.341269 -4.2654057 -4.1445575 -3.9997823 -3.8814812 -3.8395665 -3.8874588 -3.9963477 -4.1204748 -4.2256713 -4.2991562 -4.3428245 -4.3669481 -4.3810639]]...]
INFO - root - 2017-12-05 10:02:40.246905: step 18710, loss = 1.38, batch loss = 1.32 (20.0 examples/sec; 0.400 sec/batch; 34h:51m:39s remains)
INFO - root - 2017-12-05 10:02:44.157980: step 18720, loss = 2.10, batch loss = 2.04 (20.6 examples/sec; 0.389 sec/batch; 33h:53m:36s remains)
INFO - root - 2017-12-05 10:02:48.050882: step 18730, loss = 1.03, batch loss = 0.97 (20.1 examples/sec; 0.398 sec/batch; 34h:40m:43s remains)
INFO - root - 2017-12-05 10:02:51.966355: step 18740, loss = 1.20, batch loss = 1.14 (20.6 examples/sec; 0.389 sec/batch; 33h:53m:47s remains)
INFO - root - 2017-12-05 10:02:55.879749: step 18750, loss = 1.46, batch loss = 1.41 (21.0 examples/sec; 0.381 sec/batch; 33h:11m:23s remains)
INFO - root - 2017-12-05 10:02:59.683512: step 18760, loss = 1.61, batch loss = 1.56 (22.6 examples/sec; 0.354 sec/batch; 30h:49m:18s remains)
INFO - root - 2017-12-05 10:03:03.525194: step 18770, loss = 1.90, batch loss = 1.84 (21.0 examples/sec; 0.381 sec/batch; 33h:13m:54s remains)
INFO - root - 2017-12-05 10:03:07.336817: step 18780, loss = 1.87, batch loss = 1.82 (21.3 examples/sec; 0.375 sec/batch; 32h:41m:31s remains)
INFO - root - 2017-12-05 10:03:11.217442: step 18790, loss = 1.08, batch loss = 1.03 (20.2 examples/sec; 0.397 sec/batch; 34h:35m:31s remains)
INFO - root - 2017-12-05 10:03:15.171812: step 18800, loss = 1.61, batch loss = 1.56 (20.2 examples/sec; 0.395 sec/batch; 34h:27m:32s remains)
2017-12-05 10:03:15.545547: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.1570926 2.0665278 1.7924948 1.3911648 0.93386984 0.48383284 0.084294319 -0.22596741 -0.42535996 -0.50111723 -0.45662022 -0.3341465 -0.20587492 -0.19684935 -0.40794277][2.8691058 2.8389792 2.6118436 2.2445884 1.8180614 1.3910522 1.0045433 0.694304 0.48020697 0.37998486 0.39442253 0.489985 0.59589958 0.58664989 0.35006428][2.8222184 2.9009118 2.8077779 2.5849319 2.2990503 1.9908953 1.6866074 1.4150338 1.2017126 1.0738888 1.0456858 1.1001019 1.1722965 1.1429577 0.89185333][2.242486 2.4542112 2.5610776 2.5794835 2.5301142 2.4106131 2.2239633 1.9977937 1.7784204 1.6141996 1.5415874 1.5657182 1.6192212 1.5833473 1.3255877][1.4004292 1.7401538 2.069591 2.3723054 2.6095376 2.7202754 2.6729469 2.4935718 2.2572298 2.048943 1.9356122 1.9446716 2.0017238 1.9726114 1.7095966][0.60407734 1.0446095 1.5607095 2.1049037 2.611052 2.9492722 3.0503912 2.925036 2.6694436 2.4131284 2.263113 2.2658587 2.338263 2.3202114 2.0438609][0.027173519 0.52957344 1.1642289 1.8735056 2.5732694 3.0943484 3.3327165 3.2727966 3.0130038 2.7217946 2.5413141 2.5326896 2.6089635 2.5924106 2.2936072][-0.26625633 0.25343704 0.92337275 1.7079077 2.5078444 3.14537 3.4884872 3.4962869 3.2477951 2.9395375 2.734652 2.6964488 2.7449846 2.7052207 2.3753471][-0.37403774 0.13003874 0.775136 1.5623288 2.382803 3.066978 3.4642148 3.5138984 3.2757535 2.9512167 2.7076178 2.6071081 2.58844 2.49934 2.1386442][-0.47407269 -0.014636993 0.56255817 1.3013835 2.0829306 2.7521825 3.1478124 3.2001805 2.9566483 2.6042829 2.3018894 2.1152806 2.0062828 1.8558259 1.4822712][-0.76001811 -0.36277246 0.12275076 0.77696943 1.4782429 2.0840611 2.4347363 2.4657474 2.2107792 1.8265777 1.4592056 1.1850896 0.99429274 0.798954 0.44929886][-1.332267 -1.0096948 -0.62592125 -0.0828743 0.5109868 1.0238814 1.31111 1.318943 1.0632358 0.66659021 0.25592947 -0.077981949 -0.3160243 -0.52052712 -0.80645847][-2.1322711 -1.8938003 -1.6142454 -1.1981819 -0.73349595 -0.32847404 -0.10489655 -0.11065865 -0.34436512 -0.7147963 -1.1179194 -1.4580207 -1.6968048 -1.8744869 -2.0704494][-2.9893782 -2.832762 -2.6525521 -2.3720157 -2.0466897 -1.7584565 -1.598371 -1.6062114 -1.7893224 -2.0838902 -2.4138942 -2.6966853 -2.8893561 -3.0164635 -3.1259236][-3.6916449 -3.604851 -3.510186 -3.3558793 -3.1686523 -2.9959798 -2.8961935 -2.8953946 -3.0058484 -3.187814 -3.3981767 -3.5807931 -3.7034245 -3.7795298 -3.83064]]...]
INFO - root - 2017-12-05 10:03:19.469122: step 18810, loss = 2.07, batch loss = 2.01 (21.4 examples/sec; 0.374 sec/batch; 32h:36m:51s remains)
INFO - root - 2017-12-05 10:03:23.270466: step 18820, loss = 2.01, batch loss = 1.95 (20.3 examples/sec; 0.394 sec/batch; 34h:21m:35s remains)
INFO - root - 2017-12-05 10:03:27.109354: step 18830, loss = 1.67, batch loss = 1.61 (21.1 examples/sec; 0.379 sec/batch; 33h:00m:00s remains)
INFO - root - 2017-12-05 10:03:30.932857: step 18840, loss = 1.84, batch loss = 1.78 (20.5 examples/sec; 0.391 sec/batch; 34h:04m:44s remains)
INFO - root - 2017-12-05 10:03:34.808893: step 18850, loss = 1.26, batch loss = 1.20 (20.0 examples/sec; 0.399 sec/batch; 34h:47m:42s remains)
INFO - root - 2017-12-05 10:03:38.591710: step 18860, loss = 1.45, batch loss = 1.39 (23.7 examples/sec; 0.338 sec/batch; 29h:26m:48s remains)
INFO - root - 2017-12-05 10:03:42.391923: step 18870, loss = 1.34, batch loss = 1.28 (21.3 examples/sec; 0.376 sec/batch; 32h:44m:29s remains)
INFO - root - 2017-12-05 10:03:46.187393: step 18880, loss = 1.48, batch loss = 1.42 (20.6 examples/sec; 0.388 sec/batch; 33h:48m:08s remains)
INFO - root - 2017-12-05 10:03:49.990545: step 18890, loss = 1.58, batch loss = 1.52 (20.8 examples/sec; 0.384 sec/batch; 33h:28m:40s remains)
INFO - root - 2017-12-05 10:03:53.764692: step 18900, loss = 1.21, batch loss = 1.15 (21.1 examples/sec; 0.380 sec/batch; 33h:04m:38s remains)
2017-12-05 10:03:54.146056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3893552 -4.3877544 -4.3849196 -4.3814826 -4.375217 -4.3658829 -4.3538542 -4.3404584 -4.3292642 -4.3225236 -4.3206034 -4.3195472 -4.3144536 -4.3063674 -4.3012447][-4.378057 -4.3726068 -4.3638687 -4.3527207 -4.3366842 -4.3155251 -4.2907457 -4.2663894 -4.2489648 -4.2426519 -4.2469559 -4.2538157 -4.2547235 -4.2492943 -4.2413979][-4.3544707 -4.3377714 -4.3148732 -4.2885184 -4.2567 -4.2193193 -4.178648 -4.1416674 -4.1172776 -4.1103296 -4.1192112 -4.13321 -4.1418486 -4.1406837 -4.13056][-4.30496 -4.2675967 -4.2211208 -4.1725378 -4.1220393 -4.0677662 -4.0112958 -3.9615879 -3.9290733 -3.9188004 -3.9287858 -3.9481606 -3.9661562 -3.9743843 -3.9693115][-4.2225218 -4.1606245 -4.0881782 -4.015686 -3.9467223 -3.8764002 -3.80461 -3.7439291 -3.7069533 -3.69494 -3.7058685 -3.7303658 -3.7600172 -3.784117 -3.7945461][-4.1248322 -4.0458837 -3.9559517 -3.8640256 -3.7780755 -3.6925244 -3.6071067 -3.541368 -3.5084352 -3.5008667 -3.5153952 -3.5462036 -3.5882683 -3.631567 -3.6658177][-4.05053 -3.9703465 -3.8782163 -3.777194 -3.678695 -3.5816739 -3.4893808 -3.4256608 -3.4020724 -3.4033306 -3.4236634 -3.4601107 -3.5127172 -3.5736547 -3.630796][-4.0348868 -3.9652762 -3.8817992 -3.7812691 -3.6785657 -3.5794291 -3.4919922 -3.4375536 -3.424093 -3.4354417 -3.4626918 -3.5041728 -3.5625837 -3.6321497 -3.6997876][-4.0752358 -4.01544 -3.9397178 -3.8430488 -3.7426364 -3.6506393 -3.5792572 -3.5430789 -3.5464098 -3.5744028 -3.6134253 -3.6601896 -3.7166572 -3.7789924 -3.8372052][-4.1386333 -4.0791287 -4.0020394 -3.9048228 -3.807441 -3.7267485 -3.6768064 -3.6664302 -3.6950181 -3.7470269 -3.8043396 -3.8593469 -3.9108481 -3.9570382 -3.9936206][-4.1951804 -4.1268525 -4.0388579 -3.9353681 -3.8383486 -3.7656953 -3.7325902 -3.7460186 -3.8001032 -3.8770719 -3.9570937 -4.0271015 -4.0802374 -4.1156235 -4.1349292][-4.235754 -4.157021 -4.0562162 -3.944912 -3.8460855 -3.7755222 -3.74796 -3.7722037 -3.8403454 -3.934092 -4.034112 -4.1231194 -4.1879311 -4.2257719 -4.2407742][-4.2653155 -4.1833706 -4.0785179 -3.9665604 -3.868366 -3.7946863 -3.7599845 -3.7763267 -3.8410776 -3.9382534 -4.0491514 -4.1538792 -4.2341423 -4.2834716 -4.3047848][-4.2928963 -4.2198076 -4.1260567 -4.0264387 -3.9352946 -3.8589792 -3.8118396 -3.8079386 -3.8526516 -3.9364929 -4.0420122 -4.1494045 -4.238986 -4.3000412 -4.33168][-4.3202596 -4.2658238 -4.1952605 -4.11881 -4.0427814 -3.9706774 -3.9150231 -3.8919725 -3.9103088 -3.9669914 -4.0492325 -4.1417956 -4.2271304 -4.2914839 -4.3309565]]...]
INFO - root - 2017-12-05 10:03:57.929556: step 18910, loss = 1.52, batch loss = 1.47 (22.1 examples/sec; 0.362 sec/batch; 31h:33m:36s remains)
INFO - root - 2017-12-05 10:04:01.838992: step 18920, loss = 1.42, batch loss = 1.36 (21.1 examples/sec; 0.379 sec/batch; 33h:02m:40s remains)
INFO - root - 2017-12-05 10:04:05.736442: step 18930, loss = 1.29, batch loss = 1.24 (20.5 examples/sec; 0.390 sec/batch; 33h:56m:51s remains)
INFO - root - 2017-12-05 10:04:09.555720: step 18940, loss = 1.59, batch loss = 1.54 (22.0 examples/sec; 0.363 sec/batch; 31h:37m:18s remains)
INFO - root - 2017-12-05 10:04:13.417394: step 18950, loss = 1.37, batch loss = 1.31 (22.7 examples/sec; 0.352 sec/batch; 30h:40m:12s remains)
INFO - root - 2017-12-05 10:04:17.305788: step 18960, loss = 1.08, batch loss = 1.02 (20.0 examples/sec; 0.400 sec/batch; 34h:48m:35s remains)
INFO - root - 2017-12-05 10:04:21.174088: step 18970, loss = 1.50, batch loss = 1.44 (22.0 examples/sec; 0.363 sec/batch; 31h:37m:20s remains)
INFO - root - 2017-12-05 10:04:25.122360: step 18980, loss = 1.50, batch loss = 1.44 (19.8 examples/sec; 0.405 sec/batch; 35h:14m:39s remains)
INFO - root - 2017-12-05 10:04:29.044134: step 18990, loss = 1.69, batch loss = 1.63 (20.9 examples/sec; 0.382 sec/batch; 33h:18m:28s remains)
INFO - root - 2017-12-05 10:04:32.983208: step 19000, loss = 1.52, batch loss = 1.46 (20.2 examples/sec; 0.396 sec/batch; 34h:31m:33s remains)
2017-12-05 10:04:33.403482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1772423 -4.1044683 -4.0147047 -3.8952317 -3.7389898 -3.5527344 -3.3645296 -3.2119203 -3.1111617 -3.0470567 -2.9758508 -2.8571725 -2.6852598 -2.5112314 -2.4174185][-4.0174236 -3.8611512 -3.6459889 -3.3528771 -2.9800129 -2.5593417 -2.1538634 -1.8325958 -1.627892 -1.5177085 -1.4279737 -1.3017733 -1.1282563 -0.98102 -0.97895169][-3.8084242 -3.5078015 -3.0744126 -2.488497 -1.7696762 -0.99483395 -0.27721834 0.27715206 0.61427021 0.762475 0.82562256 0.87270784 0.92194176 0.89105511 0.64641333][-3.545619 -3.0227358 -2.263061 -1.262181 -0.085258007 1.1291165 2.2123094 3.0245876 3.4893298 3.6346178 3.5866966 3.4262967 3.185328 2.7996721 2.1548324][-3.222827 -2.4006772 -1.2261584 0.267272 1.9489045 3.6088133 5.0291796 6.0499649 6.5757618 6.6399117 6.3891735 5.9155693 5.266923 4.4106016 3.2859616][-2.83737 -1.6697984 -0.050840378 1.9253316 4.0486593 6.04669 7.6769686 8.7744274 9.2420177 9.13844 8.6382456 7.8448477 6.8063107 5.5129051 3.9620905][-2.4206386 -0.91976643 1.0837846 3.4218564 5.8122635 7.9434805 9.5731153 10.556941 10.832033 10.512718 9.8049746 8.8084755 7.5484524 6.0091057 4.2134953][-2.0700624 -0.331841 1.8958855 4.3804893 6.7872057 8.7870054 10.165069 10.834575 10.817801 10.302331 9.507473 8.50883 7.2801466 5.7711887 3.9842892][-1.9387126 -0.147933 2.06044 4.4195204 6.5807304 8.2171936 9.1587 9.4037409 9.0949316 8.4729862 7.7379 6.9270191 5.949316 4.7050915 3.1546626][-2.1199996 -0.50205493 1.4303608 3.4183736 5.1415548 6.3075876 6.7959604 6.6760349 6.167841 5.5338182 4.9478083 4.4100809 3.7801104 2.9135437 1.7365437][-2.5811381 -1.3114908 0.17095757 1.647089 2.8589282 3.5750852 3.7213697 3.3960528 2.831522 2.2709022 1.8581734 1.5736804 1.2597661 0.7567668 -0.017765522][-3.1700048 -2.3160877 -1.3303061 -0.37793636 0.36067438 0.72687197 0.68347216 0.321908 -0.16770744 -0.59647846 -0.85736465 -0.97427535 -1.079767 -1.3119917 -1.7409465][-3.7180779 -3.2382522 -2.6838136 -2.1638377 -1.7831035 -1.6337943 -1.7330942 -2.0146954 -2.3551621 -2.6344268 -2.7878561 -2.8301411 -2.8442845 -2.9214754 -3.1075883][-4.1064286 -3.8903193 -3.6341667 -3.398452 -3.2344813 -3.1857462 -3.2607746 -3.4233749 -3.6105638 -3.7617438 -3.8452916 -3.8669457 -3.8631487 -3.8785503 -3.936502][-4.3075137 -4.2318668 -4.1357026 -4.046936 -3.9875228 -3.9727826 -4.0071397 -4.0767279 -4.1570477 -4.2232566 -4.2639432 -4.2801042 -4.2819858 -4.284606 -4.2967105]]...]
INFO - root - 2017-12-05 10:04:37.352526: step 19010, loss = 1.30, batch loss = 1.24 (19.9 examples/sec; 0.402 sec/batch; 34h:59m:54s remains)
INFO - root - 2017-12-05 10:04:41.161873: step 19020, loss = 1.06, batch loss = 1.00 (20.0 examples/sec; 0.401 sec/batch; 34h:54m:10s remains)
INFO - root - 2017-12-05 10:04:45.090231: step 19030, loss = 1.03, batch loss = 0.97 (20.5 examples/sec; 0.391 sec/batch; 34h:03m:30s remains)
INFO - root - 2017-12-05 10:04:49.014675: step 19040, loss = 1.55, batch loss = 1.49 (21.2 examples/sec; 0.378 sec/batch; 32h:55m:01s remains)
INFO - root - 2017-12-05 10:04:52.947560: step 19050, loss = 1.13, batch loss = 1.07 (19.7 examples/sec; 0.405 sec/batch; 35h:17m:23s remains)
INFO - root - 2017-12-05 10:04:56.786220: step 19060, loss = 1.56, batch loss = 1.50 (20.8 examples/sec; 0.385 sec/batch; 33h:30m:46s remains)
INFO - root - 2017-12-05 10:05:00.780875: step 19070, loss = 1.46, batch loss = 1.40 (20.3 examples/sec; 0.394 sec/batch; 34h:20m:02s remains)
INFO - root - 2017-12-05 10:05:04.655646: step 19080, loss = 2.02, batch loss = 1.96 (21.3 examples/sec; 0.376 sec/batch; 32h:45m:00s remains)
INFO - root - 2017-12-05 10:05:08.450868: step 19090, loss = 1.88, batch loss = 1.82 (22.7 examples/sec; 0.353 sec/batch; 30h:42m:29s remains)
INFO - root - 2017-12-05 10:05:12.182856: step 19100, loss = 2.28, batch loss = 2.22 (21.4 examples/sec; 0.374 sec/batch; 32h:34m:53s remains)
2017-12-05 10:05:12.595397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7267616 -2.5765364 -2.5345712 -2.5744715 -2.6479015 -2.7043304 -2.7133095 -2.659965 -2.5428987 -2.3759136 -2.2005095 -2.07342 -2.0596979 -2.2018805 -2.4968443][-1.9362891 -1.7010062 -1.6092923 -1.6243882 -1.6878335 -1.737587 -1.7306693 -1.6430023 -1.4710665 -1.2352719 -0.99887323 -0.84693241 -0.86576796 -1.1107304 -1.5742252][-1.1680012 -0.83744049 -0.67819977 -0.64584088 -0.67683482 -0.69914746 -0.65893054 -0.52280211 -0.28610039 0.020758152 0.31085396 0.47041559 0.395535 0.023933411 -0.62763858][-0.5627079 -0.13417482 0.10875845 0.21063232 0.23470116 0.25924158 0.34687376 0.538672 0.83710718 1.1970754 1.5122914 1.647325 1.4919696 0.98516178 0.15882778][-0.21470547 0.29829741 0.62732697 0.81230593 0.90891695 0.997736 1.1470046 1.3975487 1.739697 2.1124554 2.4030657 2.4753036 2.2261181 1.601604 0.64917946][-0.090462208 0.47626591 0.87106705 1.1316528 1.3082829 1.4746556 1.6952434 1.9999957 2.3602843 2.7061539 2.9289274 2.9080658 2.5567331 1.8402495 0.81694794][-0.074663639 0.50769615 0.93207407 1.2430301 1.4905457 1.7361169 2.0263572 2.3644724 2.7057538 2.9893417 3.1209946 3.0000563 2.5579929 1.7809715 0.73518419][-0.11616039 0.45340633 0.87932587 1.2220035 1.5300522 1.8512092 2.2010641 2.5451961 2.8267503 3.0114665 3.0376444 2.8255463 2.3181696 1.5172229 0.48889637][-0.29800272 0.24404192 0.66451025 1.0393891 1.4125814 1.8083076 2.2078047 2.5445137 2.7570348 2.8387289 2.76537 2.4795094 1.9333487 1.1331043 0.13784838][-0.70638657 -0.21643257 0.19113111 0.5973258 1.0333886 1.4928212 1.9286556 2.2620444 2.4385586 2.468266 2.3463712 2.0281186 1.4739032 0.69164324 -0.26681185][-1.3349118 -0.920074 -0.53743911 -0.11248112 0.36326408 0.85933733 1.312603 1.6484828 1.8229127 1.852603 1.7377496 1.4347029 0.90199947 0.15380621 -0.75380826][-2.1127887 -1.783416 -1.4406319 -1.0245595 -0.54545355 -0.045454502 0.40658665 0.74566555 0.93649817 0.99482775 0.9182663 0.65869284 0.17331028 -0.51595736 -1.3391142][-2.9145217 -2.6731815 -2.3881841 -2.0178859 -1.5796416 -1.1155586 -0.68579745 -0.34790325 -0.13632345 -0.041471958 -0.073085308 -0.27973986 -0.69907141 -1.2980161 -1.9975712][-3.5960083 -3.4365926 -3.2219322 -2.92723 -2.5702515 -2.1834149 -1.8094757 -1.497154 -1.2829204 -1.1660981 -1.1627607 -1.3178768 -1.6523204 -2.127315 -2.6684661][-4.04862 -3.9566286 -3.8178606 -3.6185427 -3.3686552 -3.0888844 -2.8057575 -2.558105 -2.378262 -2.2693317 -2.2509053 -2.3596723 -2.5976958 -2.9262211 -3.2906318]]...]
INFO - root - 2017-12-05 10:05:16.431015: step 19110, loss = 1.21, batch loss = 1.15 (21.3 examples/sec; 0.376 sec/batch; 32h:45m:06s remains)
INFO - root - 2017-12-05 10:05:20.231525: step 19120, loss = 0.86, batch loss = 0.80 (22.6 examples/sec; 0.353 sec/batch; 30h:45m:29s remains)
INFO - root - 2017-12-05 10:05:24.030654: step 19130, loss = 1.52, batch loss = 1.46 (21.6 examples/sec; 0.371 sec/batch; 32h:17m:46s remains)
INFO - root - 2017-12-05 10:05:27.853764: step 19140, loss = 1.80, batch loss = 1.75 (21.0 examples/sec; 0.382 sec/batch; 33h:13m:44s remains)
INFO - root - 2017-12-05 10:05:31.715183: step 19150, loss = 1.31, batch loss = 1.25 (21.4 examples/sec; 0.374 sec/batch; 32h:35m:08s remains)
INFO - root - 2017-12-05 10:05:35.585362: step 19160, loss = 1.79, batch loss = 1.73 (19.8 examples/sec; 0.403 sec/batch; 35h:04m:50s remains)
INFO - root - 2017-12-05 10:05:39.504425: step 19170, loss = 1.43, batch loss = 1.37 (21.2 examples/sec; 0.377 sec/batch; 32h:50m:48s remains)
INFO - root - 2017-12-05 10:05:43.343469: step 19180, loss = 1.45, batch loss = 1.39 (21.0 examples/sec; 0.382 sec/batch; 33h:12m:25s remains)
INFO - root - 2017-12-05 10:05:47.271021: step 19190, loss = 1.51, batch loss = 1.45 (20.1 examples/sec; 0.398 sec/batch; 34h:37m:27s remains)
INFO - root - 2017-12-05 10:05:51.074305: step 19200, loss = 1.56, batch loss = 1.51 (21.2 examples/sec; 0.378 sec/batch; 32h:53m:58s remains)
2017-12-05 10:05:51.451156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.38771 -4.3692384 -4.3464756 -4.3269844 -4.3157144 -4.3122506 -4.3147006 -4.32086 -4.32768 -4.3328505 -4.3363414 -4.3385959 -4.3426919 -4.3509412 -4.3625808][-4.3495374 -4.2859607 -4.2061267 -4.1362391 -4.0964246 -4.0881143 -4.0990753 -4.1155219 -4.1302533 -4.1431746 -4.15819 -4.1776643 -4.20597 -4.2439656 -4.2866588][-4.2744083 -4.1178341 -3.9162793 -3.7320967 -3.61913 -3.5859275 -3.60068 -3.6273181 -3.6510615 -3.68017 -3.7292516 -3.8039734 -3.9048507 -4.0211287 -4.1355648][-4.161355 -3.8606246 -3.4616122 -3.0812693 -2.8275998 -2.7261446 -2.7213502 -2.7432792 -2.7665098 -2.8166945 -2.9320059 -3.1247139 -3.3769395 -3.6489553 -3.8974247][-4.0373278 -3.5689824 -2.92752 -2.2864692 -1.8205092 -1.5872905 -1.5137842 -1.4957366 -1.4982622 -1.5721991 -1.7869446 -2.1640451 -2.6536818 -3.1617126 -3.6022418][-3.9488764 -3.347486 -2.4930224 -1.5918951 -0.879488 -0.45464563 -0.24937153 -0.14310694 -0.096209049 -0.18640184 -0.51484752 -1.1132388 -1.8906028 -2.6775641 -3.3326447][-3.9352939 -3.294106 -2.3453894 -1.2815206 -0.36552 0.26508188 0.6462698 0.8862071 1.0127869 0.92530537 0.49792385 -0.30305767 -1.3376689 -2.361975 -3.1856661][-4.0016112 -3.4302125 -2.5485005 -1.4982347 -0.51202583 0.25679636 0.80003977 1.1804695 1.3980923 1.3355188 0.85970783 -0.05107832 -1.2142847 -2.3420527 -3.2203562][-4.1153908 -3.6872935 -3.0000715 -2.1330183 -1.2488797 -0.47932959 0.13390684 0.60068464 0.88560009 0.86406326 0.41312265 -0.4654355 -1.5740929 -2.6277864 -3.4254174][-4.2311606 -3.962676 -3.5141344 -2.9165831 -2.2575078 -1.6259661 -1.0688925 -0.61177731 -0.31415081 -0.29310369 -0.65029311 -1.368109 -2.2665143 -3.103765 -3.7196674][-4.3190284 -4.1802177 -3.9384007 -3.5969968 -3.1914051 -2.767535 -2.3586545 -1.9966264 -1.7423117 -1.6929181 -1.9245968 -2.4181805 -3.0355048 -3.6007791 -4.0047474][-4.3697028 -4.31113 -4.2052417 -4.0473375 -3.8463292 -3.617815 -3.3775496 -3.1477933 -2.9727447 -2.9199605 -3.0404077 -3.3217506 -3.6747179 -3.9916489 -4.2109818][-4.3920174 -4.3710551 -4.332243 -4.2724156 -4.1930804 -4.0978537 -3.9902747 -3.8794284 -3.7877929 -3.7526941 -3.802063 -3.9302256 -4.0914826 -4.2335196 -4.3282781][-4.40043 -4.3936148 -4.3808937 -4.3609185 -4.3345623 -4.3029628 -4.2671423 -4.2294393 -4.1966262 -4.1826968 -4.1989164 -4.2440691 -4.3008003 -4.3498683 -4.3814435][-4.403162 -4.4011478 -4.3971572 -4.3909054 -4.3833723 -4.37515 -4.3663435 -4.3576727 -4.3505673 -4.3480573 -4.3527508 -4.364429 -4.379127 -4.391511 -4.3992715]]...]
INFO - root - 2017-12-05 10:05:55.317887: step 19210, loss = 2.25, batch loss = 2.20 (20.4 examples/sec; 0.391 sec/batch; 34h:03m:03s remains)
INFO - root - 2017-12-05 10:05:59.182456: step 19220, loss = 1.20, batch loss = 1.14 (21.1 examples/sec; 0.379 sec/batch; 32h:59m:30s remains)
INFO - root - 2017-12-05 10:06:03.080162: step 19230, loss = 1.55, batch loss = 1.49 (20.4 examples/sec; 0.391 sec/batch; 34h:03m:58s remains)
INFO - root - 2017-12-05 10:06:06.969626: step 19240, loss = 1.45, batch loss = 1.40 (21.1 examples/sec; 0.379 sec/batch; 32h:57m:59s remains)
INFO - root - 2017-12-05 10:06:10.803759: step 19250, loss = 1.09, batch loss = 1.03 (20.0 examples/sec; 0.400 sec/batch; 34h:46m:10s remains)
INFO - root - 2017-12-05 10:06:14.626833: step 19260, loss = 1.60, batch loss = 1.54 (20.9 examples/sec; 0.383 sec/batch; 33h:21m:04s remains)
INFO - root - 2017-12-05 10:06:18.353018: step 19270, loss = 1.52, batch loss = 1.46 (21.0 examples/sec; 0.381 sec/batch; 33h:07m:26s remains)
INFO - root - 2017-12-05 10:06:22.168409: step 19280, loss = 1.35, batch loss = 1.29 (21.0 examples/sec; 0.381 sec/batch; 33h:10m:57s remains)
INFO - root - 2017-12-05 10:06:26.036214: step 19290, loss = 1.52, batch loss = 1.46 (20.4 examples/sec; 0.392 sec/batch; 34h:08m:06s remains)
INFO - root - 2017-12-05 10:06:29.863583: step 19300, loss = 0.94, batch loss = 0.88 (21.0 examples/sec; 0.381 sec/batch; 33h:09m:09s remains)
2017-12-05 10:06:30.223124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2658124 -4.1053057 -3.8405597 -3.4509439 -2.9495745 -2.4001281 -1.9359758 -1.7210782 -1.8701704 -2.340009 -2.9553318 -3.5317581 -3.958499 -4.2148805 -4.338613][-4.216321 -3.986969 -3.6025932 -3.0350881 -2.315506 -1.537061 -0.88002014 -0.57539892 -0.78293943 -1.4448047 -2.3219841 -3.1485443 -3.7642651 -4.1345763 -4.3125238][-4.1857843 -3.902827 -3.4176881 -2.6933794 -1.7780776 -0.79679394 0.024349213 0.40201044 0.14218473 -0.68561029 -1.7857511 -2.8270717 -3.6052048 -4.0713921 -4.29353][-4.1857629 -3.880672 -3.3406529 -2.5190272 -1.4700634 -0.34324121 0.5966115 1.0291796 0.737762 -0.19648838 -1.4399507 -2.6207047 -3.5053658 -4.0344119 -4.2842641][-4.2109952 -3.9181614 -3.3773942 -2.5304773 -1.423893 -0.221066 0.78558064 1.2516961 0.94904327 -0.025458336 -1.3194654 -2.5480328 -3.4702635 -4.0215588 -4.2811613][-4.2479439 -3.9914145 -3.4940248 -2.6851785 -1.5972359 -0.39312458 0.62428617 1.0997243 0.80325556 -0.15591621 -1.4198389 -2.6126685 -3.5059488 -4.03737 -4.28591][-4.2862959 -4.0795302 -3.6565328 -2.9384141 -1.9394815 -0.80652881 0.17116213 0.64153862 0.37487698 -0.51359177 -1.6800299 -2.7752564 -3.5914824 -4.073832 -4.2977405][-4.3225441 -4.1705875 -3.8416643 -3.2590547 -2.4195294 -1.4357617 -0.5529573 -0.09224987 -0.28571415 -1.0321915 -2.0313385 -2.9785309 -3.689091 -4.110518 -4.3080978][-4.3532863 -4.2535944 -4.0255556 -3.6052935 -2.9795475 -2.2175057 -1.4999659 -1.0792327 -1.1639018 -1.6986325 -2.4624686 -3.21435 -3.7933254 -4.1460261 -4.3172584][-4.376297 -4.3201408 -4.1840382 -3.9229863 -3.52182 -3.011868 -2.5045438 -2.1683109 -2.1571739 -2.4656143 -2.9650078 -3.4911623 -3.9171295 -4.1890497 -4.3286309][-4.3907852 -4.3646 -4.2977104 -4.1645393 -3.9529774 -3.6702259 -3.3702931 -3.1447294 -3.0875568 -3.2205029 -3.4887016 -3.7985613 -4.0660982 -4.2470651 -4.3459139][-4.398459 -4.3890529 -4.3636827 -4.3118868 -4.2265639 -4.1040583 -3.9619062 -3.8399017 -3.7839613 -3.8174739 -3.9281025 -4.0754876 -4.2124228 -4.3106222 -4.3672738][-4.4017029 -4.3992515 -4.392406 -4.3789406 -4.35635 -4.3194652 -4.2703233 -4.2210784 -4.1876521 -4.1839147 -4.2136607 -4.2654428 -4.319026 -4.3606486 -4.3862643][-4.4032044 -4.4029789 -4.4019012 -4.400187 -4.3972588 -4.3912911 -4.3814535 -4.3690329 -4.3569441 -4.3501573 -4.3526688 -4.3633957 -4.377193 -4.3896008 -4.3980274][-4.40388 -4.4040661 -4.4038992 -4.4036746 -4.4033585 -4.4026394 -4.4013424 -4.3995075 -4.3973036 -4.3958631 -4.3955622 -4.3966107 -4.39848 -4.4007559 -4.4026613]]...]
INFO - root - 2017-12-05 10:06:34.173482: step 19310, loss = 1.21, batch loss = 1.15 (20.5 examples/sec; 0.391 sec/batch; 33h:58m:42s remains)
INFO - root - 2017-12-05 10:06:38.046431: step 19320, loss = 1.64, batch loss = 1.58 (21.1 examples/sec; 0.379 sec/batch; 32h:58m:43s remains)
INFO - root - 2017-12-05 10:06:41.930773: step 19330, loss = 1.56, batch loss = 1.50 (20.5 examples/sec; 0.390 sec/batch; 33h:56m:58s remains)
INFO - root - 2017-12-05 10:06:45.814298: step 19340, loss = 1.63, batch loss = 1.57 (20.3 examples/sec; 0.395 sec/batch; 34h:21m:42s remains)
INFO - root - 2017-12-05 10:06:49.645101: step 19350, loss = 1.30, batch loss = 1.25 (22.3 examples/sec; 0.359 sec/batch; 31h:15m:42s remains)
INFO - root - 2017-12-05 10:06:53.479649: step 19360, loss = 1.35, batch loss = 1.29 (22.0 examples/sec; 0.364 sec/batch; 31h:41m:56s remains)
INFO - root - 2017-12-05 10:06:57.377150: step 19370, loss = 1.18, batch loss = 1.13 (21.6 examples/sec; 0.371 sec/batch; 32h:16m:43s remains)
INFO - root - 2017-12-05 10:07:01.171059: step 19380, loss = 1.59, batch loss = 1.53 (22.2 examples/sec; 0.361 sec/batch; 31h:23m:04s remains)
INFO - root - 2017-12-05 10:07:05.001540: step 19390, loss = 1.17, batch loss = 1.11 (20.3 examples/sec; 0.394 sec/batch; 34h:17m:51s remains)
INFO - root - 2017-12-05 10:07:08.914429: step 19400, loss = 1.50, batch loss = 1.44 (20.2 examples/sec; 0.397 sec/batch; 34h:29m:38s remains)
2017-12-05 10:07:09.348930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2325053 -4.1590853 -4.1200447 -4.1365123 -4.1967359 -4.2693262 -4.3239441 -4.3515291 -4.3588209 -4.354917 -4.3452377 -4.3278227 -4.3014555 -4.274066 -4.2538362][-4.1702724 -4.0848112 -4.04224 -4.0634127 -4.1297765 -4.2054229 -4.259099 -4.2853765 -4.2940364 -4.2946019 -4.2912474 -4.2797556 -4.2565713 -4.2279644 -4.202477][-4.1074705 -4.0222807 -3.9853523 -4.0053186 -4.0588551 -4.1127844 -4.1447349 -4.1578531 -4.1672478 -4.180707 -4.1973743 -4.2097182 -4.2102418 -4.1992574 -4.18359][-4.0553951 -3.9825673 -3.956876 -3.96792 -3.99009 -3.999028 -3.9901056 -3.9793642 -3.9875183 -4.0200911 -4.0672894 -4.1178465 -4.159596 -4.1853218 -4.1967511][-4.0265045 -3.9731748 -3.9570324 -3.9518812 -3.93011 -3.8806033 -3.8207693 -3.7809122 -3.787986 -3.8422997 -3.9237313 -4.01661 -4.1049957 -4.1754932 -4.2224483][-4.0257382 -3.9934773 -3.9807892 -3.952251 -3.8832269 -3.7777085 -3.6714406 -3.6078463 -3.6163976 -3.6896544 -3.7990065 -3.9258966 -4.0536184 -4.1636934 -4.2413425][-4.0461435 -4.0299206 -4.0127764 -3.9576628 -3.8483891 -3.7031918 -3.5686879 -3.4939792 -3.5062964 -3.5901537 -3.7144856 -3.8602066 -4.0115104 -4.14629 -4.2422776][-4.0719829 -4.0621285 -4.0349631 -3.9581628 -3.8257065 -3.6650505 -3.5253835 -3.4519129 -3.4672012 -3.5514998 -3.6759934 -3.8235142 -3.979912 -4.1215072 -4.2228441][-4.0923934 -4.0777283 -4.0385208 -3.9510069 -3.8179498 -3.6694593 -3.5474176 -3.48599 -3.5005803 -3.5741169 -3.68368 -3.8160834 -3.9595287 -4.0921416 -4.1890187][-4.1187959 -4.0917621 -4.0416846 -3.9541876 -3.8370318 -3.7174258 -3.6250176 -3.5807633 -3.5936527 -3.6518207 -3.7380362 -3.8438683 -3.9616768 -4.0739474 -4.1600003][-4.1667891 -4.1285129 -4.0726838 -3.9936028 -3.9009476 -3.8153152 -3.7538404 -3.7273748 -3.7403247 -3.7845159 -3.8460658 -3.9198425 -4.0039058 -4.0874829 -4.156755][-4.2355208 -4.1950397 -4.1426048 -4.0781736 -4.0109825 -3.9551327 -3.920078 -3.9085717 -3.9217572 -3.9538789 -3.9932399 -4.0367055 -4.0868883 -4.1393609 -4.1884208][-4.3059368 -4.2741213 -4.2343736 -4.1889653 -4.1441917 -4.1098113 -4.0922847 -4.0907454 -4.1033587 -4.125308 -4.1470761 -4.1673355 -4.1912622 -4.2191133 -4.2497129][-4.3588991 -4.3400192 -4.3163834 -4.28975 -4.2635193 -4.2441525 -4.2364006 -4.2386532 -4.2482653 -4.2617197 -4.2720542 -4.2787743 -4.2876277 -4.300344 -4.3169756][-4.3878164 -4.3792129 -4.3682723 -4.3559241 -4.3434849 -4.3345027 -4.3317857 -4.3342695 -4.3399115 -4.3466277 -4.3503952 -4.3513465 -4.3534703 -4.3583622 -4.365972]]...]
INFO - root - 2017-12-05 10:07:13.203787: step 19410, loss = 1.40, batch loss = 1.34 (21.9 examples/sec; 0.365 sec/batch; 31h:46m:21s remains)
INFO - root - 2017-12-05 10:07:17.058153: step 19420, loss = 1.23, batch loss = 1.17 (20.7 examples/sec; 0.386 sec/batch; 33h:35m:38s remains)
INFO - root - 2017-12-05 10:07:20.814424: step 19430, loss = 1.37, batch loss = 1.32 (20.6 examples/sec; 0.389 sec/batch; 33h:47m:45s remains)
INFO - root - 2017-12-05 10:07:24.686141: step 19440, loss = 1.37, batch loss = 1.32 (20.6 examples/sec; 0.388 sec/batch; 33h:44m:41s remains)
INFO - root - 2017-12-05 10:07:28.536429: step 19450, loss = 1.17, batch loss = 1.11 (19.5 examples/sec; 0.411 sec/batch; 35h:42m:27s remains)
INFO - root - 2017-12-05 10:07:32.346384: step 19460, loss = 1.06, batch loss = 1.00 (20.3 examples/sec; 0.393 sec/batch; 34h:12m:42s remains)
