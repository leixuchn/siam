INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "139"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 09:13:05.086537: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 09:13:05.086572: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 09:13:05.086579: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 09:13:05.086583: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 09:13:05.086587: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-07 09:13:10.848310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 4.02GiB
2017-12-07 09:13:10.848350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 09:13:10.848357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 09:13:10.848364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 09:13:40.658958: step 0, loss = 2.04, batch loss = 1.99 (0.4 examples/sec; 19.769 sec/batch; 1825h:54m:05s remains)
2017-12-07 09:13:41.994533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3351674 -4.3331385 -4.3350124 -4.3383994 -4.3367233 -4.3263392 -4.3069773 -4.2818456 -4.2584896 -4.2509136 -4.2573705 -4.271997 -4.2927332 -4.3157558 -4.3329687][-4.3321395 -4.3303294 -4.3332443 -4.3348227 -4.327456 -4.3053293 -4.2705097 -4.2307253 -4.199966 -4.1970963 -4.2143049 -4.2389584 -4.268281 -4.2983575 -4.3208075][-4.3260121 -4.3249526 -4.3284974 -4.3253636 -4.3040295 -4.2625666 -4.205061 -4.1447244 -4.1054997 -4.1107936 -4.1450653 -4.1869917 -4.2320185 -4.2741141 -4.3054147][-4.3137865 -4.3152256 -4.32095 -4.3106704 -4.2711878 -4.2085142 -4.1265812 -4.040328 -3.9876587 -4.0048332 -4.062305 -4.1258116 -4.1932578 -4.250617 -4.2922964][-4.2973928 -4.3023186 -4.3087358 -4.2921467 -4.2380118 -4.1566057 -4.0498161 -3.9305742 -3.8612902 -3.9000802 -3.9855373 -4.072947 -4.1617651 -4.2323327 -4.2838778][-4.2787614 -4.2868233 -4.2919588 -4.2682734 -4.199616 -4.0976443 -3.9609668 -3.8038068 -3.7238851 -3.8002996 -3.9187627 -4.0297203 -4.1368294 -4.2184706 -4.2794414][-4.2621012 -4.2694383 -4.2695494 -4.2363667 -4.1563997 -4.0372157 -3.8745503 -3.6848164 -3.6164365 -3.7474036 -3.8975053 -4.022965 -4.1350527 -4.2191172 -4.2820687][-4.2522759 -4.2538242 -4.2449059 -4.2047882 -4.1244078 -4.0067053 -3.8473394 -3.6711946 -3.6521273 -3.8127246 -3.957799 -4.0671387 -4.1615982 -4.2359457 -4.2924395][-4.2432284 -4.235991 -4.2219577 -4.185256 -4.1243811 -4.0371685 -3.9228132 -3.8135407 -3.8355739 -3.9628191 -4.0628619 -4.1321487 -4.1994767 -4.2580886 -4.3044844][-4.2345009 -4.2208004 -4.2047887 -4.1763167 -4.1390991 -4.0884514 -4.0256076 -3.9724247 -4.00274 -4.08417 -4.1421809 -4.1776838 -4.2234626 -4.2714634 -4.3119268][-4.2262988 -4.2101259 -4.192523 -4.17147 -4.1508508 -4.1267548 -4.0984321 -4.0766344 -4.1012845 -4.1494627 -4.1809516 -4.1974626 -4.2340951 -4.2791133 -4.3174491][-4.2235856 -4.2103386 -4.1949253 -4.18097 -4.1703215 -4.1634932 -4.1553802 -4.1491814 -4.1648178 -4.186883 -4.2012115 -4.209837 -4.244957 -4.2896595 -4.3241134][-4.2349052 -4.2294893 -4.2210178 -4.2140212 -4.2109241 -4.2137866 -4.2120953 -4.2059917 -4.20833 -4.208045 -4.2102404 -4.215498 -4.2510557 -4.2959023 -4.3276849][-4.2530289 -4.2548575 -4.2547288 -4.2540684 -4.2547517 -4.2582068 -4.2518578 -4.2369623 -4.2257214 -4.21276 -4.2086158 -4.2141604 -4.2505774 -4.2961979 -4.3272157][-4.2577477 -4.263226 -4.2690496 -4.2727065 -4.2746344 -4.2767153 -4.263566 -4.2371655 -4.2143192 -4.1977415 -4.1950626 -4.2049723 -4.244091 -4.2909989 -4.3228736]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 09:13:52.941328: step 10, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 88h:43m:56s remains)
INFO - root - 2017-12-07 09:14:02.829033: step 20, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 92h:58m:48s remains)
INFO - root - 2017-12-07 09:14:12.482068: step 30, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 90h:19m:59s remains)
INFO - root - 2017-12-07 09:14:22.130182: step 40, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 90h:02m:10s remains)
INFO - root - 2017-12-07 09:14:32.004273: step 50, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 92h:13m:04s remains)
INFO - root - 2017-12-07 09:14:41.759923: step 60, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 91h:08m:34s remains)
INFO - root - 2017-12-07 09:14:51.371660: step 70, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 84h:33m:35s remains)
INFO - root - 2017-12-07 09:15:01.215955: step 80, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 89h:37m:19s remains)
INFO - root - 2017-12-07 09:15:10.779879: step 90, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.003 sec/batch; 92h:38m:46s remains)
INFO - root - 2017-12-07 09:15:20.588195: step 100, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 89h:40m:39s remains)
2017-12-07 09:15:21.540514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2453279 -4.2026882 -4.1529317 -4.1247768 -4.130249 -4.1563921 -4.1830673 -4.2092009 -4.2285337 -4.2338815 -4.233613 -4.2355528 -4.2285275 -4.2121005 -4.2011518][-4.249207 -4.2006569 -4.1445546 -4.1109715 -4.1119409 -4.1364918 -4.1609311 -4.1828465 -4.2022319 -4.2124476 -4.2167392 -4.21938 -4.213973 -4.1976528 -4.1881576][-4.2513614 -4.2015858 -4.1467185 -4.1128454 -4.1115966 -4.1324606 -4.1535344 -4.1730785 -4.193923 -4.2095728 -4.2156129 -4.2148666 -4.2067266 -4.1892676 -4.178205][-4.2506242 -4.2019849 -4.1498137 -4.1154432 -4.1103573 -4.1269236 -4.1457758 -4.1637812 -4.1851625 -4.2052407 -4.2138219 -4.2114148 -4.2021356 -4.1870584 -4.1763482][-4.2491407 -4.2021747 -4.1526942 -4.1153245 -4.1035004 -4.1132927 -4.1267715 -4.1374145 -4.1547008 -4.1796346 -4.1953039 -4.1982656 -4.19354 -4.183814 -4.177218][-4.2476768 -4.2028389 -4.1543946 -4.113873 -4.0965838 -4.0997486 -4.10355 -4.0989757 -4.1055331 -4.1357331 -4.1631832 -4.1768713 -4.1806831 -4.1761479 -4.1739521][-4.2425685 -4.1964917 -4.144196 -4.0996242 -4.079288 -4.0807209 -4.0799351 -4.0659204 -4.06623 -4.1002674 -4.1352277 -4.1541214 -4.1630535 -4.162343 -4.16247][-4.239449 -4.1910667 -4.1359873 -4.0911455 -4.0720997 -4.0754056 -4.0770726 -4.0664105 -4.0665016 -4.0952368 -4.1247811 -4.1383839 -4.1451588 -4.1468406 -4.1483026][-4.2388835 -4.1898642 -4.1344647 -4.0918565 -4.0753765 -4.079102 -4.0828233 -4.0820909 -4.0887251 -4.1105857 -4.1289973 -4.1338129 -4.1335025 -4.1316876 -4.13188][-4.2401094 -4.1902294 -4.1356983 -4.0954041 -4.0789647 -4.0799284 -4.0843263 -4.0897059 -4.1034551 -4.123848 -4.1374784 -4.1357408 -4.1286459 -4.1209412 -4.1199126][-4.2468295 -4.198226 -4.1480384 -4.1131992 -4.1003251 -4.1020126 -4.1081586 -4.1165156 -4.1283579 -4.1451879 -4.1560297 -4.1515875 -4.1405163 -4.1299229 -4.127634][-4.2577229 -4.21287 -4.1689892 -4.1395288 -4.1311221 -4.1373415 -4.1499295 -4.1634989 -4.1726089 -4.1829715 -4.188714 -4.1826887 -4.1703343 -4.1589804 -4.156086][-4.2729378 -4.2350283 -4.1988044 -4.173861 -4.1674762 -4.1777773 -4.1974545 -4.2161684 -4.223877 -4.2282681 -4.2273612 -4.2190661 -4.2062407 -4.196322 -4.1947746][-4.2954092 -4.2689338 -4.2433634 -4.22396 -4.2177153 -4.22691 -4.2458529 -4.2634454 -4.270472 -4.2722445 -4.268384 -4.2591267 -4.2475796 -4.2413597 -4.24267][-4.3172679 -4.3021984 -4.2878032 -4.2757678 -4.2705197 -4.2754073 -4.2873087 -4.2994556 -4.3045092 -4.3052826 -4.3021741 -4.296061 -4.2891927 -4.286819 -4.2885289]]...]
INFO - root - 2017-12-07 09:15:31.348516: step 110, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 83h:37m:58s remains)
INFO - root - 2017-12-07 09:15:41.206733: step 120, loss = 2.05, batch loss = 1.99 (7.7 examples/sec; 1.033 sec/batch; 95h:23m:07s remains)
INFO - root - 2017-12-07 09:15:51.036014: step 130, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.026 sec/batch; 94h:44m:16s remains)
INFO - root - 2017-12-07 09:16:00.730778: step 140, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 89h:39m:25s remains)
INFO - root - 2017-12-07 09:16:10.565706: step 150, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.948 sec/batch; 87h:32m:34s remains)
INFO - root - 2017-12-07 09:16:20.348424: step 160, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 89h:43m:01s remains)
INFO - root - 2017-12-07 09:16:30.289153: step 170, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 90h:44m:26s remains)
INFO - root - 2017-12-07 09:16:39.967544: step 180, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 91h:18m:34s remains)
INFO - root - 2017-12-07 09:16:49.550559: step 190, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 93h:15m:22s remains)
INFO - root - 2017-12-07 09:16:59.503413: step 200, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 88h:57m:24s remains)
2017-12-07 09:17:00.444505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2906132 -4.2792225 -4.2594209 -4.2447133 -4.2418075 -4.2459688 -4.2450247 -4.2427149 -4.2330966 -4.2253809 -4.2288179 -4.2342172 -4.2347183 -4.2328734 -4.2349334][-4.296236 -4.2799387 -4.2633505 -4.2517257 -4.2462373 -4.2475481 -4.244772 -4.2392125 -4.2293034 -4.2235341 -4.2280226 -4.2287269 -4.2242508 -4.2214074 -4.2259769][-4.2751307 -4.2551417 -4.2499933 -4.2527752 -4.2489853 -4.2455192 -4.237329 -4.2280641 -4.2215209 -4.22539 -4.2347136 -4.2312293 -4.2182131 -4.2098064 -4.2104015][-4.2437963 -4.2264071 -4.2345042 -4.249928 -4.2505383 -4.2441931 -4.2311292 -4.2113934 -4.2037563 -4.2199268 -4.2386394 -4.2344236 -4.2137423 -4.1971626 -4.18941][-4.2242379 -4.2047644 -4.214901 -4.2372961 -4.2426171 -4.234962 -4.213243 -4.1771522 -4.1616588 -4.1932168 -4.2257414 -4.226912 -4.2035804 -4.1810865 -4.166707][-4.2171369 -4.1922178 -4.1952629 -4.2177343 -4.2270942 -4.2137003 -4.1723113 -4.1001329 -4.0704317 -4.1279697 -4.1854692 -4.2042556 -4.1853008 -4.16199 -4.1497536][-4.2107968 -4.1763644 -4.1665621 -4.1875467 -4.2000918 -4.1818523 -4.108439 -3.9805732 -3.9227033 -4.0232835 -4.1245842 -4.1703978 -4.1628923 -4.1348991 -4.1264791][-4.189362 -4.1548247 -4.1415086 -4.1691294 -4.1930861 -4.1758976 -4.0877323 -3.9337139 -3.8589506 -3.9829478 -4.1097517 -4.1712132 -4.1715136 -4.1381636 -4.1283255][-4.181859 -4.1580219 -4.1496134 -4.1792831 -4.2095079 -4.2030149 -4.1420178 -4.0388584 -3.9902875 -4.0694017 -4.1632166 -4.21468 -4.2185826 -4.1889453 -4.1751575][-4.2086182 -4.1946921 -4.1883779 -4.2136908 -4.2409377 -4.2430887 -4.2077303 -4.1474 -4.1141686 -4.1519895 -4.2132268 -4.2549219 -4.260231 -4.2368197 -4.2183514][-4.2455282 -4.2366586 -4.2276216 -4.2423086 -4.2616129 -4.2653971 -4.2403474 -4.1980362 -4.1727343 -4.1915407 -4.2335453 -4.2694349 -4.27395 -4.2580256 -4.2383895][-4.2684836 -4.2610893 -4.2556148 -4.2664118 -4.2746034 -4.2758074 -4.2594819 -4.2286353 -4.2088256 -4.2176352 -4.2440057 -4.2731514 -4.2779818 -4.2691083 -4.2510695][-4.2877169 -4.2806072 -4.2792859 -4.2878156 -4.2872243 -4.2822876 -4.2733841 -4.2500763 -4.2366447 -4.2424579 -4.2592382 -4.2823954 -4.2873745 -4.280355 -4.2672887][-4.2922697 -4.2903633 -4.2932944 -4.3000579 -4.2976546 -4.2898912 -4.2856417 -4.2679911 -4.2597566 -4.2634778 -4.2726703 -4.2904587 -4.2938347 -4.2878957 -4.2761602][-4.2865992 -4.2879248 -4.292778 -4.2981424 -4.2948947 -4.2888861 -4.2870841 -4.2759995 -4.2702355 -4.2702136 -4.2721314 -4.282702 -4.2876163 -4.2826519 -4.272727]]...]
INFO - root - 2017-12-07 09:17:10.297918: step 210, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 93h:39m:51s remains)
INFO - root - 2017-12-07 09:17:20.209023: step 220, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 90h:51m:20s remains)
INFO - root - 2017-12-07 09:17:30.010176: step 230, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 90h:16m:06s remains)
INFO - root - 2017-12-07 09:17:39.893874: step 240, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.028 sec/batch; 94h:52m:11s remains)
INFO - root - 2017-12-07 09:17:49.755748: step 250, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 91h:23m:31s remains)
INFO - root - 2017-12-07 09:17:59.544316: step 260, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 84h:11m:21s remains)
INFO - root - 2017-12-07 09:18:09.330263: step 270, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 89h:20m:03s remains)
INFO - root - 2017-12-07 09:18:19.091577: step 280, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 89h:38m:17s remains)
INFO - root - 2017-12-07 09:18:28.947672: step 290, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.026 sec/batch; 94h:38m:36s remains)
INFO - root - 2017-12-07 09:18:38.821222: step 300, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.031 sec/batch; 95h:09m:07s remains)
2017-12-07 09:18:39.741879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21422 -4.2051873 -4.1883883 -4.18065 -4.177392 -4.1819644 -4.1867766 -4.1795053 -4.1724777 -4.1850295 -4.2066569 -4.2216067 -4.2145882 -4.1933923 -4.1738539][-4.2079968 -4.2075982 -4.2069664 -4.2088532 -4.2013321 -4.1949697 -4.1874151 -4.1745014 -4.1739011 -4.1889086 -4.2073226 -4.2167625 -4.206583 -4.1908655 -4.1855278][-4.1906691 -4.1999454 -4.2152252 -4.2255116 -4.2190175 -4.2042031 -4.1861448 -4.1686049 -4.1659741 -4.1752558 -4.1861434 -4.1924667 -4.1857462 -4.1825461 -4.192627][-4.1693921 -4.182004 -4.2084594 -4.2290435 -4.2255564 -4.2049713 -4.1785169 -4.151104 -4.1355324 -4.13022 -4.1354036 -4.147387 -4.1573052 -4.1744142 -4.1981578][-4.169271 -4.1779623 -4.205327 -4.2318373 -4.2296929 -4.201385 -4.1619968 -4.1182208 -4.0818529 -4.0606656 -4.064486 -4.0911312 -4.1238823 -4.1605773 -4.1966338][-4.1939049 -4.1956334 -4.2168818 -4.2407 -4.2359176 -4.200233 -4.145772 -4.0844932 -4.0311337 -3.9949174 -3.9888861 -4.0219312 -4.0729017 -4.1303306 -4.1802831][-4.2171345 -4.2173138 -4.2340064 -4.254427 -4.2511511 -4.2138891 -4.1526918 -4.0844975 -4.0247836 -3.9769385 -3.9530909 -3.9715018 -4.0214972 -4.0942249 -4.1595817][-4.2381644 -4.236958 -4.2481637 -4.2635674 -4.2636003 -4.2344885 -4.1832442 -4.1250329 -4.0703945 -4.0210505 -3.986444 -3.9784908 -4.0066237 -4.0752034 -4.1453662][-4.2686996 -4.2658162 -4.2714715 -4.2805996 -4.2797184 -4.2613993 -4.2264743 -4.185318 -4.1415806 -4.0973182 -4.06013 -4.0349402 -4.0355911 -4.0807438 -4.1384068][-4.2976236 -4.2973208 -4.3020444 -4.308187 -4.3080754 -4.2993431 -4.2807341 -4.2552357 -4.2221766 -4.1825027 -4.1435061 -4.1079721 -4.0883865 -4.1051316 -4.1387682][-4.31192 -4.3176455 -4.3258023 -4.3328156 -4.3360019 -4.33402 -4.32433 -4.3072119 -4.2826347 -4.2488661 -4.2122421 -4.1719828 -4.1422062 -4.1376209 -4.1447186][-4.2989664 -4.3123364 -4.3238482 -4.3341446 -4.3428435 -4.3482447 -4.3471794 -4.336657 -4.3173203 -4.2894197 -4.2589149 -4.2235684 -4.1952991 -4.1762719 -4.1590056][-4.2848692 -4.3024025 -4.3153286 -4.3258023 -4.3373289 -4.3496633 -4.3561721 -4.3536429 -4.3399215 -4.3181157 -4.2966394 -4.2696753 -4.2446957 -4.2180343 -4.1825423][-4.2920685 -4.3092828 -4.3205957 -4.3258896 -4.3321233 -4.34246 -4.3523111 -4.3550715 -4.346384 -4.3339944 -4.3224936 -4.30628 -4.2855535 -4.254425 -4.2103186][-4.3016391 -4.3182149 -4.3264332 -4.3253922 -4.3241749 -4.32818 -4.33706 -4.3423014 -4.337358 -4.3329549 -4.32885 -4.322371 -4.3101463 -4.2798686 -4.2322459]]...]
INFO - root - 2017-12-07 09:18:49.537265: step 310, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 90h:41m:40s remains)
INFO - root - 2017-12-07 09:18:59.508576: step 320, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 88h:59m:50s remains)
INFO - root - 2017-12-07 09:19:09.200452: step 330, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 91h:41m:54s remains)
INFO - root - 2017-12-07 09:19:19.105893: step 340, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 88h:15m:50s remains)
INFO - root - 2017-12-07 09:19:28.768790: step 350, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 87h:14m:10s remains)
INFO - root - 2017-12-07 09:19:38.717169: step 360, loss = 2.09, batch loss = 2.04 (7.6 examples/sec; 1.047 sec/batch; 96h:37m:16s remains)
INFO - root - 2017-12-07 09:19:48.452646: step 370, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 91h:54m:09s remains)
INFO - root - 2017-12-07 09:19:58.089694: step 380, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 92h:25m:50s remains)
INFO - root - 2017-12-07 09:20:07.933176: step 390, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.009 sec/batch; 93h:03m:28s remains)
INFO - root - 2017-12-07 09:20:17.561334: step 400, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 91h:46m:24s remains)
2017-12-07 09:20:18.460174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2644806 -4.2554617 -4.25628 -4.2659173 -4.2788973 -4.2940035 -4.2953434 -4.2903547 -4.280282 -4.2758217 -4.2685747 -4.2629304 -4.2598062 -4.2720518 -4.2846508][-4.2667766 -4.2606959 -4.2621694 -4.2723207 -4.2888403 -4.3057513 -4.3042669 -4.2939334 -4.2775903 -4.26693 -4.2568793 -4.2457914 -4.2344661 -4.2421966 -4.2522063][-4.2759757 -4.2704325 -4.2691469 -4.275043 -4.2897587 -4.3050785 -4.2988348 -4.2855339 -4.2675285 -4.2550926 -4.2445869 -4.2303109 -4.2139707 -4.2170854 -4.222487][-4.2689319 -4.2567534 -4.2500505 -4.2530189 -4.2662153 -4.2777052 -4.2660136 -4.2528839 -4.2405529 -4.2334828 -4.2288127 -4.2183475 -4.2035027 -4.2052078 -4.205914][-4.2541566 -4.2280769 -4.2083211 -4.2067485 -4.2187977 -4.2291055 -4.2165918 -4.2070274 -4.2033758 -4.203136 -4.205853 -4.2057061 -4.197196 -4.1997037 -4.1990533][-4.2496982 -4.2144189 -4.1777191 -4.1608195 -4.1601524 -4.1624751 -4.148129 -4.1461945 -4.1557026 -4.1674457 -4.1785035 -4.1884151 -4.1882639 -4.1950321 -4.196362][-4.2629161 -4.2346792 -4.1942635 -4.1631389 -4.1491175 -4.1358204 -4.104733 -4.0896754 -4.0991073 -4.1208491 -4.1408949 -4.1593819 -4.16793 -4.1832771 -4.191505][-4.2676816 -4.2505255 -4.2206087 -4.1961856 -4.189971 -4.1812372 -4.1391621 -4.0978293 -4.0857406 -4.0948844 -4.107717 -4.1262312 -4.1418376 -4.1665211 -4.1834259][-4.2504072 -4.2385149 -4.2174735 -4.2073607 -4.2201939 -4.2299542 -4.197402 -4.1520944 -4.12309 -4.1103058 -4.1030536 -4.1101451 -4.1237988 -4.1501069 -4.1734586][-4.2079973 -4.1944761 -4.1770229 -4.1804008 -4.2127433 -4.2424865 -4.2280731 -4.1993032 -4.1745811 -4.1515155 -4.1301556 -4.123651 -4.1288419 -4.14969 -4.1739464][-4.1658354 -4.1493616 -4.1306973 -4.1413593 -4.1873121 -4.2321486 -4.2337561 -4.2216234 -4.2100954 -4.1914654 -4.1717496 -4.1612391 -4.1599889 -4.1725988 -4.1919265][-4.1606703 -4.1398768 -4.1139078 -4.1214037 -4.1689548 -4.2190614 -4.2327323 -4.2317867 -4.2296157 -4.2186179 -4.2083673 -4.2041669 -4.2027221 -4.2092695 -4.2212825][-4.2017317 -4.1820107 -4.1544485 -4.1517439 -4.1847982 -4.2237577 -4.2371221 -4.2400718 -4.2419434 -4.2382441 -4.236064 -4.2387819 -4.241066 -4.2462163 -4.2518282][-4.2587066 -4.2460794 -4.22277 -4.2114739 -4.2241192 -4.2434788 -4.2503185 -4.2537384 -4.2581367 -4.2591596 -4.2616 -4.268054 -4.2732244 -4.2773695 -4.27974][-4.2996521 -4.2923384 -4.2765379 -4.2647152 -4.2663374 -4.2742805 -4.2765265 -4.2777209 -4.2805738 -4.2833652 -4.2860894 -4.290339 -4.293623 -4.2959323 -4.2971377]]...]
INFO - root - 2017-12-07 09:20:28.210900: step 410, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 85h:46m:31s remains)
INFO - root - 2017-12-07 09:20:37.902066: step 420, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 86h:40m:13s remains)
INFO - root - 2017-12-07 09:20:47.723479: step 430, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 87h:46m:15s remains)
INFO - root - 2017-12-07 09:20:57.420391: step 440, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 92h:26m:06s remains)
INFO - root - 2017-12-07 09:21:07.237980: step 450, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 1.003 sec/batch; 92h:31m:38s remains)
INFO - root - 2017-12-07 09:21:17.011456: step 460, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 86h:54m:08s remains)
INFO - root - 2017-12-07 09:21:26.770357: step 470, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 87h:40m:28s remains)
INFO - root - 2017-12-07 09:21:36.380406: step 480, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 91h:16m:09s remains)
INFO - root - 2017-12-07 09:21:45.984437: step 490, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 91h:27m:56s remains)
INFO - root - 2017-12-07 09:21:55.826308: step 500, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 89h:24m:11s remains)
2017-12-07 09:21:56.778807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25937 -4.2497449 -4.243464 -4.2334204 -4.2174735 -4.2133193 -4.220252 -4.2273049 -4.2345748 -4.2342072 -4.2250853 -4.2048264 -4.1734223 -4.1428914 -4.1123166][-4.2315383 -4.2195635 -4.2089596 -4.191833 -4.1725445 -4.1659904 -4.1743331 -4.1930785 -4.2146134 -4.21964 -4.2076812 -4.1803918 -4.1430411 -4.1139035 -4.08563][-4.2091327 -4.1924181 -4.1716452 -4.1419306 -4.1165853 -4.1123543 -4.1290746 -4.1633978 -4.1981845 -4.2021093 -4.1797032 -4.1476636 -4.1090283 -4.0806031 -4.06149][-4.1972485 -4.172617 -4.1359911 -4.092576 -4.0629382 -4.0608897 -4.084609 -4.131135 -4.1761527 -4.180603 -4.1510606 -4.1187758 -4.0861444 -4.061213 -4.0537271][-4.178544 -4.1466742 -4.1002364 -4.051682 -4.0219808 -4.0150056 -4.0342135 -4.0829787 -4.1341424 -4.1388025 -4.1103735 -4.0888767 -4.0679469 -4.0579371 -4.0684814][-4.1609378 -4.1331453 -4.0909181 -4.0400181 -3.99807 -3.96661 -3.9586246 -3.992234 -4.0425444 -4.0565224 -4.039434 -4.0401621 -4.0424085 -4.0523281 -4.0772028][-4.1470041 -4.1323214 -4.10347 -4.0549312 -3.9940665 -3.9223559 -3.8604698 -3.8625269 -3.922461 -3.9670839 -3.9798129 -4.005271 -4.0235033 -4.0456982 -4.0772462][-4.1267676 -4.1169844 -4.1000361 -4.0583925 -3.9933393 -3.9047458 -3.8062344 -3.7766798 -3.8474231 -3.922982 -3.9670167 -4.0098972 -4.0369139 -4.0554962 -4.0827065][-4.1081634 -4.0901175 -4.0752726 -4.0474091 -4.0029421 -3.940876 -3.8636425 -3.8363619 -3.8879952 -3.9509859 -3.9969604 -4.0456295 -4.0763531 -4.0890121 -4.104661][-4.0917397 -4.076108 -4.070025 -4.0623932 -4.04687 -4.0180883 -3.976455 -3.9607053 -3.9829791 -4.0098834 -4.0389562 -4.084034 -4.115953 -4.1236677 -4.1257172][-4.0905037 -4.0975895 -4.1080465 -4.112978 -4.11079 -4.0991726 -4.0778065 -4.0652018 -4.0651326 -4.0613337 -4.0652819 -4.0973058 -4.1260357 -4.1330705 -4.1298385][-4.1268144 -4.1445718 -4.1578364 -4.1593747 -4.1534886 -4.1446724 -4.1280451 -4.1111436 -4.1000223 -4.0846543 -4.0787024 -4.1016321 -4.1262808 -4.1375084 -4.1429095][-4.16444 -4.1766081 -4.1840153 -4.1783681 -4.16721 -4.1576657 -4.1431003 -4.125855 -4.1157117 -4.1058083 -4.1046219 -4.1223874 -4.139657 -4.1533532 -4.1648564][-4.1864867 -4.1916761 -4.1925011 -4.1840596 -4.1736088 -4.1661134 -4.1548395 -4.1450124 -4.1453285 -4.1468329 -4.1495118 -4.1540647 -4.156786 -4.1649108 -4.1751266][-4.1952209 -4.1974492 -4.2005806 -4.1961064 -4.1899772 -4.1842036 -4.1742382 -4.1711931 -4.1808095 -4.1889668 -4.1905823 -4.1832619 -4.1747336 -4.1747885 -4.1779165]]...]
INFO - root - 2017-12-07 09:22:06.619586: step 510, loss = 2.09, batch loss = 2.04 (7.9 examples/sec; 1.018 sec/batch; 93h:54m:17s remains)
INFO - root - 2017-12-07 09:22:16.435137: step 520, loss = 2.10, batch loss = 2.04 (7.7 examples/sec; 1.035 sec/batch; 95h:29m:09s remains)
INFO - root - 2017-12-07 09:22:26.206800: step 530, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 1.006 sec/batch; 92h:45m:56s remains)
INFO - root - 2017-12-07 09:22:36.113002: step 540, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 88h:08m:53s remains)
INFO - root - 2017-12-07 09:22:45.961197: step 550, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 92h:28m:28s remains)
INFO - root - 2017-12-07 09:22:55.820498: step 560, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.006 sec/batch; 92h:46m:06s remains)
INFO - root - 2017-12-07 09:23:05.330889: step 570, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 87h:56m:29s remains)
INFO - root - 2017-12-07 09:23:15.008488: step 580, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.009 sec/batch; 93h:04m:10s remains)
INFO - root - 2017-12-07 09:23:24.910732: step 590, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 91h:27m:25s remains)
INFO - root - 2017-12-07 09:23:34.696112: step 600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 87h:13m:06s remains)
2017-12-07 09:23:35.627871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2268596 -4.2148457 -4.209033 -4.1995335 -4.1919003 -4.1888347 -4.1925068 -4.2106438 -4.2448292 -4.2750115 -4.2937837 -4.3064203 -4.3006964 -4.2739062 -4.2450361][-4.2207336 -4.2126923 -4.2107868 -4.2073112 -4.2012548 -4.1998181 -4.2012186 -4.2164712 -4.2473021 -4.2740064 -4.2874475 -4.2945185 -4.2889271 -4.2694283 -4.25148][-4.2096262 -4.1991496 -4.1997685 -4.2059116 -4.2108269 -4.215281 -4.2144942 -4.2264967 -4.2498755 -4.2675591 -4.2708778 -4.2718496 -4.2673473 -4.2542887 -4.2455306][-4.1973495 -4.1835928 -4.1834168 -4.1941485 -4.2025809 -4.2029786 -4.1943913 -4.2015924 -4.2173839 -4.228334 -4.2299705 -4.2338076 -4.2365603 -4.2282782 -4.2241096][-4.1754274 -4.1668921 -4.1685557 -4.1822271 -4.1881065 -4.1752424 -4.1543756 -4.1549473 -4.1675129 -4.179122 -4.1907449 -4.204639 -4.2123623 -4.2042923 -4.1962333][-4.1501656 -4.1466269 -4.1483355 -4.1568766 -4.1519723 -4.1210976 -4.087882 -4.0851707 -4.1058717 -4.1351652 -4.1648583 -4.1864052 -4.1909728 -4.1755943 -4.1553774][-4.1472917 -4.146544 -4.1420941 -4.1312594 -4.1018882 -4.0475235 -3.9998667 -3.9986377 -4.0392866 -4.0936565 -4.1331859 -4.1513343 -4.1481466 -4.1214952 -4.0886669][-4.1524911 -4.1536155 -4.1447024 -4.1198497 -4.0696478 -3.9971509 -3.9401591 -3.9414892 -3.9934669 -4.0537429 -4.086566 -4.0929708 -4.0828953 -4.0490584 -4.0113821][-4.1577082 -4.1555257 -4.1483431 -4.1273379 -4.0744395 -4.0017819 -3.9473448 -3.9426756 -3.9810398 -4.020844 -4.0375953 -4.0346832 -4.022748 -3.9946895 -3.9710417][-4.1792455 -4.1815758 -4.1834106 -4.1734309 -4.1326079 -4.0747867 -4.0310354 -4.0189271 -4.0349956 -4.0485272 -4.0474648 -4.0367985 -4.0217557 -3.9973612 -3.9880891][-4.20036 -4.2085724 -4.2174044 -4.2174072 -4.19453 -4.16003 -4.1352825 -4.125102 -4.1271238 -4.1274352 -4.1173868 -4.1019111 -4.0840764 -4.0593252 -4.0532479][-4.2262712 -4.229959 -4.2335515 -4.2343907 -4.224349 -4.210659 -4.2054915 -4.2046595 -4.2056346 -4.2033734 -4.1943769 -4.1804662 -4.164012 -4.1429405 -4.1356997][-4.235033 -4.232729 -4.229876 -4.2325382 -4.2335362 -4.2366295 -4.2462258 -4.255044 -4.2584209 -4.2574577 -4.2519989 -4.242311 -4.2305388 -4.2149973 -4.2070966][-4.2263994 -4.222733 -4.2147689 -4.21587 -4.2233276 -4.2396235 -4.2590113 -4.2734661 -4.2807527 -4.2818975 -4.2784 -4.2710681 -4.2628355 -4.2508664 -4.2436423][-4.2243 -4.2223086 -4.2104549 -4.2073984 -4.2174506 -4.237905 -4.2596579 -4.2752161 -4.282671 -4.2830071 -4.2790222 -4.2725105 -4.2658072 -4.2556887 -4.2499213]]...]
INFO - root - 2017-12-07 09:23:45.306021: step 610, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 90h:55m:40s remains)
INFO - root - 2017-12-07 09:23:55.255928: step 620, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.036 sec/batch; 95h:29m:42s remains)
INFO - root - 2017-12-07 09:24:04.973028: step 630, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.006 sec/batch; 92h:46m:11s remains)
INFO - root - 2017-12-07 09:24:14.882476: step 640, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 85h:23m:51s remains)
INFO - root - 2017-12-07 09:24:24.619960: step 650, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 87h:25m:38s remains)
INFO - root - 2017-12-07 09:24:34.438051: step 660, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.040 sec/batch; 95h:52m:11s remains)
INFO - root - 2017-12-07 09:24:44.205911: step 670, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.953 sec/batch; 87h:48m:50s remains)
INFO - root - 2017-12-07 09:24:54.022447: step 680, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 89h:28m:40s remains)
INFO - root - 2017-12-07 09:25:03.858927: step 690, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 85h:50m:32s remains)
INFO - root - 2017-12-07 09:25:13.790936: step 700, loss = 2.07, batch loss = 2.02 (7.7 examples/sec; 1.043 sec/batch; 96h:08m:05s remains)
2017-12-07 09:25:14.797967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2189813 -4.2123361 -4.2083807 -4.2052021 -4.2032914 -4.2011247 -4.197237 -4.1921611 -4.18927 -4.1898828 -4.1916466 -4.1905947 -4.1828046 -4.17166 -4.159605][-4.2429876 -4.2357993 -4.2302666 -4.224371 -4.2196383 -4.2143083 -4.2068248 -4.1973262 -4.1907725 -4.1886 -4.1887674 -4.1869965 -4.1805463 -4.1724067 -4.1627836][-4.2629471 -4.2556047 -4.2496066 -4.2431111 -4.2386131 -4.2330961 -4.2248669 -4.2143264 -4.2066 -4.2032466 -4.2015471 -4.1985049 -4.1933641 -4.1886859 -4.1822281][-4.2542734 -4.2470794 -4.2414846 -4.2373996 -4.2365556 -4.2345295 -4.2282043 -4.2199273 -4.2153649 -4.2167645 -4.2178044 -4.215714 -4.210588 -4.205379 -4.1979914][-4.2271085 -4.2150736 -4.206953 -4.2062883 -4.2112894 -4.2141571 -4.2107639 -4.2054906 -4.2080307 -4.2188768 -4.2283506 -4.2318697 -4.2297525 -4.2237797 -4.2134428][-4.1924982 -4.1727753 -4.1606436 -4.162971 -4.1730423 -4.17798 -4.1722183 -4.166316 -4.175025 -4.1974573 -4.21972 -4.23411 -4.239809 -4.2363548 -4.22618][-4.1702647 -4.1459169 -4.1313558 -4.1345978 -4.1450081 -4.1444874 -4.1299715 -4.1183157 -4.1294985 -4.1610312 -4.1945696 -4.2214 -4.2369289 -4.2394457 -4.2326322][-4.1657357 -4.1459579 -4.1355262 -4.1395979 -4.1455841 -4.1374922 -4.1137133 -4.0931172 -4.0988951 -4.129168 -4.1650629 -4.1973763 -4.2192578 -4.2273006 -4.2251735][-4.1728606 -4.1630688 -4.1602445 -4.1668038 -4.1711912 -4.1618505 -4.1401653 -4.1210065 -4.1212993 -4.1416578 -4.1691427 -4.1949072 -4.21197 -4.21824 -4.2160544][-4.1916895 -4.1895814 -4.1897855 -4.19477 -4.1964369 -4.1880527 -4.1725173 -4.1616645 -4.164176 -4.1796265 -4.2002168 -4.2181139 -4.2262926 -4.22496 -4.2167883][-4.2189517 -4.21643 -4.2122884 -4.2100229 -4.2041755 -4.1922073 -4.1791825 -4.175982 -4.1832023 -4.1990657 -4.218154 -4.2338872 -4.2393422 -4.2341485 -4.2209845][-4.2440004 -4.2376 -4.22937 -4.2229261 -4.2133131 -4.1981587 -4.183785 -4.1820865 -4.1909251 -4.2064753 -4.2232709 -4.2366452 -4.2411451 -4.2352877 -4.2216597][-4.2559881 -4.2484293 -4.2403502 -4.2357755 -4.2294412 -4.2167721 -4.2027278 -4.1985888 -4.2034297 -4.214045 -4.2257304 -4.2350249 -4.2375107 -4.2320042 -4.2201977][-4.2541895 -4.2467866 -4.2409515 -4.2402067 -4.2390118 -4.2333727 -4.2251554 -4.2214527 -4.2229395 -4.227375 -4.2322059 -4.2359996 -4.2354236 -4.2290306 -4.2188687][-4.2466054 -4.2398896 -4.2350965 -4.2352505 -4.2355475 -4.2344751 -4.2323132 -4.2313857 -4.2321615 -4.2328105 -4.233449 -4.2340403 -4.2326603 -4.2266197 -4.2182097]]...]
INFO - root - 2017-12-07 09:25:24.628709: step 710, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 83h:39m:54s remains)
INFO - root - 2017-12-07 09:25:34.579577: step 720, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.036 sec/batch; 95h:27m:45s remains)
INFO - root - 2017-12-07 09:25:44.404423: step 730, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 90h:53m:37s remains)
INFO - root - 2017-12-07 09:25:54.206804: step 740, loss = 2.10, batch loss = 2.04 (7.7 examples/sec; 1.032 sec/batch; 95h:07m:55s remains)
INFO - root - 2017-12-07 09:26:04.143769: step 750, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.982 sec/batch; 90h:27m:15s remains)
INFO - root - 2017-12-07 09:26:13.902294: step 760, loss = 2.11, batch loss = 2.05 (7.8 examples/sec; 1.032 sec/batch; 95h:05m:01s remains)
INFO - root - 2017-12-07 09:26:23.706592: step 770, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 87h:09m:21s remains)
INFO - root - 2017-12-07 09:26:33.517517: step 780, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.008 sec/batch; 92h:51m:28s remains)
INFO - root - 2017-12-07 09:26:43.246407: step 790, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.045 sec/batch; 96h:18m:11s remains)
INFO - root - 2017-12-07 09:26:53.050983: step 800, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.951 sec/batch; 87h:38m:42s remains)
2017-12-07 09:26:54.091618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1285706 -4.1600714 -4.1896167 -4.189497 -4.1799974 -4.1605959 -4.1594558 -4.1834717 -4.2133336 -4.2370791 -4.2502217 -4.2513638 -4.2546291 -4.2577219 -4.2501407][-4.1545892 -4.1922836 -4.222003 -4.2157116 -4.2023497 -4.1778021 -4.1654644 -4.1783981 -4.2016258 -4.2181964 -4.2203603 -4.2169805 -4.2175527 -4.2215204 -4.2254272][-4.1923466 -4.2333379 -4.2611556 -4.2521534 -4.2376318 -4.212388 -4.1886349 -4.187758 -4.2008004 -4.2098336 -4.2073984 -4.2024317 -4.198606 -4.2052541 -4.2194805][-4.2234478 -4.2635012 -4.2910185 -4.2875566 -4.2779202 -4.2591729 -4.2365851 -4.2271605 -4.2288103 -4.2300172 -4.2220397 -4.21022 -4.1999893 -4.2059164 -4.223464][-4.253387 -4.2870846 -4.3111262 -4.3106523 -4.3027511 -4.2881813 -4.2676873 -4.2493744 -4.2433429 -4.23892 -4.2257376 -4.2087984 -4.1962156 -4.2038302 -4.2247124][-4.2790694 -4.29706 -4.3077669 -4.3047857 -4.2970576 -4.2828345 -4.2591629 -4.2292857 -4.2229304 -4.2191548 -4.2075224 -4.193038 -4.1809182 -4.1910629 -4.2166457][-4.2812877 -4.2783704 -4.2702422 -4.2615523 -4.2531056 -4.2377329 -4.2106652 -4.1736021 -4.175539 -4.1778965 -4.1707497 -4.1619954 -4.1546249 -4.1671286 -4.1976442][-4.2528744 -4.2325664 -4.2109385 -4.1954947 -4.1864552 -4.1681471 -4.1287665 -4.0851827 -4.1052847 -4.1234612 -4.1279731 -4.1279931 -4.1273327 -4.1444945 -4.1799293][-4.2145987 -4.1933522 -4.1739779 -4.1629457 -4.1577444 -4.1355333 -4.0842276 -4.0351405 -4.0653834 -4.0947318 -4.111455 -4.1245708 -4.129591 -4.1515412 -4.1868353][-4.1909485 -4.1861491 -4.1873875 -4.1957788 -4.19927 -4.1789923 -4.1390219 -4.1008472 -4.1177292 -4.1319141 -4.1445646 -4.1566525 -4.1597795 -4.1801405 -4.2105403][-4.1964388 -4.2129097 -4.2335324 -4.2559414 -4.2662406 -4.25434 -4.2284527 -4.2015433 -4.2006445 -4.1989641 -4.1986833 -4.197341 -4.1933928 -4.2083988 -4.2338572][-4.2140169 -4.2469921 -4.2756371 -4.30184 -4.3166409 -4.314013 -4.295239 -4.2725663 -4.2647576 -4.2546625 -4.2424078 -4.2255955 -4.2119808 -4.2234311 -4.2462783][-4.2265816 -4.2658925 -4.2931995 -4.3170547 -4.3338194 -4.3347225 -4.3169823 -4.2946982 -4.2842565 -4.2736282 -4.2578979 -4.2320819 -4.2174783 -4.2310328 -4.2513232][-4.2233667 -4.2590823 -4.2784443 -4.2947598 -4.3115029 -4.3147569 -4.298593 -4.2784271 -4.2679768 -4.2605557 -4.2495008 -4.2315125 -4.2258539 -4.2424927 -4.2614045][-4.2192435 -4.2435904 -4.2521157 -4.2612953 -4.2771211 -4.282722 -4.2699952 -4.255055 -4.247963 -4.245975 -4.2422647 -4.2368927 -4.2417121 -4.2559566 -4.2721825]]...]
INFO - root - 2017-12-07 09:27:03.972391: step 810, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 86h:29m:10s remains)
INFO - root - 2017-12-07 09:27:13.702623: step 820, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 86h:20m:06s remains)
INFO - root - 2017-12-07 09:27:23.532271: step 830, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 90h:43m:53s remains)
INFO - root - 2017-12-07 09:27:33.356286: step 840, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.032 sec/batch; 95h:04m:06s remains)
INFO - root - 2017-12-07 09:27:43.176642: step 850, loss = 2.07, batch loss = 2.01 (7.5 examples/sec; 1.066 sec/batch; 98h:13m:27s remains)
INFO - root - 2017-12-07 09:27:52.806173: step 860, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 91h:56m:12s remains)
INFO - root - 2017-12-07 09:28:02.602287: step 870, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 88h:45m:44s remains)
INFO - root - 2017-12-07 09:28:12.400253: step 880, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.975 sec/batch; 89h:48m:04s remains)
INFO - root - 2017-12-07 09:28:22.144210: step 890, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 91h:54m:26s remains)
INFO - root - 2017-12-07 09:28:32.062473: step 900, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 91h:25m:02s remains)
2017-12-07 09:28:32.990505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1435447 -4.1454315 -4.1497693 -4.1309204 -4.1070662 -4.1131964 -4.1318235 -4.1423106 -4.1554523 -4.1596055 -4.1534925 -4.14098 -4.120059 -4.1204143 -4.13593][-4.1179571 -4.1503739 -4.1698332 -4.1514077 -4.1156607 -4.1108956 -4.1312213 -4.14735 -4.1650896 -4.1745138 -4.1673069 -4.1509128 -4.1285567 -4.1223826 -4.135231][-4.0889573 -4.1574044 -4.194838 -4.1753516 -4.1259589 -4.1015425 -4.112514 -4.1263318 -4.1478882 -4.1661835 -4.1646128 -4.1528797 -4.1316381 -4.1216927 -4.135242][-4.1083145 -4.1778879 -4.2174783 -4.1953812 -4.1322713 -4.0858979 -4.069221 -4.0767155 -4.1050777 -4.1337109 -4.1404028 -4.1410646 -4.1291194 -4.1135583 -4.1270723][-4.158792 -4.2024918 -4.2253575 -4.1965938 -4.1225452 -4.0600691 -4.014545 -4.0190506 -4.0657511 -4.1082339 -4.1233063 -4.1290507 -4.1218853 -4.1001019 -4.1087012][-4.1940441 -4.2054706 -4.2092962 -4.1712103 -4.09027 -4.0133138 -3.9450028 -3.9564948 -4.0347781 -4.0937171 -4.1126447 -4.1093488 -4.0965242 -4.0782108 -4.0845189][-4.2078395 -4.1993542 -4.1874952 -4.1400471 -4.0493593 -3.9605005 -3.8816271 -3.9080176 -4.0239081 -4.1010756 -4.1239867 -4.110364 -4.0841126 -4.0564585 -4.0578322][-4.2109351 -4.1932755 -4.1704006 -4.119092 -4.0415215 -3.9806623 -3.9303403 -3.9639258 -4.0789504 -4.151792 -4.1756229 -4.1620893 -4.1198049 -4.0795159 -4.0725708][-4.2199445 -4.2073627 -4.1820383 -4.1371675 -4.0831871 -4.0560765 -4.0373607 -4.0698109 -4.1591387 -4.2089262 -4.2228951 -4.2166128 -4.1753435 -4.1349454 -4.1262379][-4.2317624 -4.2327008 -4.2200618 -4.1914821 -4.1593285 -4.1475887 -4.1401391 -4.1567268 -4.206593 -4.2270217 -4.2306743 -4.2319283 -4.2038441 -4.1732283 -4.1633258][-4.2361407 -4.2460122 -4.2456322 -4.2298121 -4.2134867 -4.2106938 -4.2059493 -4.2051792 -4.2240262 -4.22334 -4.2170024 -4.2217894 -4.2031703 -4.1721835 -4.1633987][-4.2371283 -4.2464771 -4.249002 -4.2388358 -4.2282176 -4.2278347 -4.2258072 -4.2216845 -4.2284117 -4.2201233 -4.2069926 -4.2052922 -4.1882448 -4.1557794 -4.1465645][-4.2428055 -4.242732 -4.244719 -4.2397528 -4.2304358 -4.2282891 -4.2284279 -4.2304525 -4.2347941 -4.2299132 -4.2226324 -4.2197289 -4.2034826 -4.1713591 -4.1584244][-4.2583022 -4.252459 -4.2517304 -4.2521729 -4.2508225 -4.2515097 -4.253746 -4.25791 -4.2572265 -4.2522583 -4.2485003 -4.2463589 -4.22987 -4.1992593 -4.1872735][-4.2671866 -4.2634025 -4.2651706 -4.26877 -4.2717137 -4.2754755 -4.2799072 -4.2853236 -4.2816424 -4.2710829 -4.2612858 -4.2572923 -4.2458086 -4.223628 -4.2180619]]...]
INFO - root - 2017-12-07 09:28:42.762266: step 910, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.020 sec/batch; 93h:58m:20s remains)
INFO - root - 2017-12-07 09:28:52.512219: step 920, loss = 2.11, batch loss = 2.05 (7.9 examples/sec; 1.013 sec/batch; 93h:17m:20s remains)
INFO - root - 2017-12-07 09:29:02.312162: step 930, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 90h:53m:22s remains)
INFO - root - 2017-12-07 09:29:12.003360: step 940, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 91h:17m:59s remains)
INFO - root - 2017-12-07 09:29:21.648809: step 950, loss = 2.12, batch loss = 2.06 (8.0 examples/sec; 0.997 sec/batch; 91h:48m:38s remains)
INFO - root - 2017-12-07 09:29:31.439488: step 960, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 87h:23m:46s remains)
INFO - root - 2017-12-07 09:29:41.328255: step 970, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 89h:40m:21s remains)
INFO - root - 2017-12-07 09:29:51.230711: step 980, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 92h:28m:33s remains)
INFO - root - 2017-12-07 09:30:01.055808: step 990, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 87h:02m:45s remains)
INFO - root - 2017-12-07 09:30:10.912405: step 1000, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.021 sec/batch; 94h:01m:22s remains)
2017-12-07 09:30:11.883225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1140914 -4.1192188 -4.1334028 -4.1596041 -4.1837358 -4.1903334 -4.1774287 -4.1491923 -4.1252356 -4.1220522 -4.1315956 -4.150548 -4.1774845 -4.2158384 -4.25225][-4.102849 -4.1128254 -4.1303029 -4.1599321 -4.1862845 -4.1928983 -4.1777306 -4.1446 -4.119596 -4.1227055 -4.1389227 -4.1634064 -4.1920652 -4.2278872 -4.2619967][-4.0866327 -4.1003766 -4.1237268 -4.1553564 -4.1822848 -4.190187 -4.1749973 -4.1387715 -4.110961 -4.11617 -4.1376867 -4.1656671 -4.1933646 -4.226728 -4.2599974][-4.0889153 -4.1057296 -4.1277556 -4.1541324 -4.1757054 -4.181426 -4.1660213 -4.1281996 -4.0961905 -4.0987968 -4.120975 -4.1502624 -4.1802716 -4.2166772 -4.2533846][-4.1130171 -4.1266217 -4.1391654 -4.1520462 -4.1598153 -4.156518 -4.1364164 -4.0949416 -4.06071 -4.0683203 -4.0960307 -4.1292906 -4.1662359 -4.2087812 -4.2500439][-4.1528144 -4.1572633 -4.1544509 -4.1490088 -4.1404691 -4.1229377 -4.0914149 -4.0425658 -4.0094681 -4.0315166 -4.0736089 -4.1145811 -4.1586118 -4.2073946 -4.2516317][-4.1723375 -4.1673684 -4.152256 -4.1354675 -4.117125 -4.0886879 -4.0443096 -3.9845202 -3.9531207 -3.9966159 -4.0600162 -4.1127763 -4.1610374 -4.2118578 -4.2563982][-4.1719823 -4.1597123 -4.1403222 -4.1216664 -4.1025167 -4.0730872 -4.0257554 -3.9609342 -3.9325924 -3.9895263 -4.0638132 -4.1210284 -4.1690745 -4.2184038 -4.2609968][-4.1686893 -4.1497912 -4.1299152 -4.1171074 -4.1071548 -4.0878329 -4.0520463 -3.9987061 -3.9735744 -4.0189018 -4.0798793 -4.1270485 -4.1700635 -4.2168159 -4.2590179][-4.1648655 -4.1441431 -4.1254392 -4.1181927 -4.1187334 -4.1132121 -4.0933042 -4.0551844 -4.0286274 -4.0500045 -4.0886745 -4.1210475 -4.1582704 -4.204514 -4.2498][-4.160171 -4.141397 -4.1247616 -4.1194162 -4.1247158 -4.1283889 -4.1214995 -4.0963154 -4.068922 -4.0739388 -4.0963573 -4.1182456 -4.1518922 -4.1981034 -4.2450514][-4.1653085 -4.150013 -4.1344438 -4.1292477 -4.1360731 -4.1446037 -4.1467762 -4.1307449 -4.1046119 -4.100863 -4.1135969 -4.1305075 -4.1624222 -4.2075129 -4.2525663][-4.1854506 -4.1716619 -4.1563072 -4.1526251 -4.1618676 -4.1724544 -4.1772084 -4.1645117 -4.1401391 -4.1318245 -4.1405635 -4.156424 -4.1887603 -4.2319031 -4.2726951][-4.2212267 -4.2095523 -4.1976838 -4.1960979 -4.2045579 -4.212966 -4.2153673 -4.2026191 -4.1794868 -4.1712523 -4.1802411 -4.1967239 -4.226799 -4.2649727 -4.2985363][-4.2617626 -4.2529464 -4.244319 -4.2427988 -4.2482424 -4.2537537 -4.2541037 -4.2430234 -4.2244725 -4.2184014 -4.2263122 -4.24183 -4.2666059 -4.2958913 -4.3201942]]...]
INFO - root - 2017-12-07 09:30:21.638881: step 1010, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 89h:24m:23s remains)
INFO - root - 2017-12-07 09:30:31.374598: step 1020, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 89h:58m:20s remains)
INFO - root - 2017-12-07 09:30:41.158949: step 1030, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 86h:50m:45s remains)
INFO - root - 2017-12-07 09:30:50.786393: step 1040, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 88h:46m:17s remains)
INFO - root - 2017-12-07 09:31:00.475518: step 1050, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 91h:37m:48s remains)
INFO - root - 2017-12-07 09:31:10.295923: step 1060, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 91h:59m:49s remains)
INFO - root - 2017-12-07 09:31:20.171270: step 1070, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 92h:18m:35s remains)
INFO - root - 2017-12-07 09:31:29.965327: step 1080, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 88h:50m:41s remains)
INFO - root - 2017-12-07 09:31:39.664770: step 1090, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 87h:38m:58s remains)
INFO - root - 2017-12-07 09:31:49.456402: step 1100, loss = 2.07, batch loss = 2.02 (7.7 examples/sec; 1.032 sec/batch; 95h:02m:03s remains)
2017-12-07 09:31:50.333589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.240294 -4.2668142 -4.2674742 -4.2615933 -4.2585835 -4.2496443 -4.2344012 -4.2413483 -4.2612643 -4.2798963 -4.2821164 -4.2798324 -4.2796736 -4.2845588 -4.2945337][-4.2449903 -4.2673912 -4.26855 -4.2642746 -4.2637768 -4.263329 -4.2527237 -4.2493863 -4.2540689 -4.2631803 -4.2625208 -4.2647319 -4.2727165 -4.2851353 -4.3036275][-4.23735 -4.2621017 -4.2709036 -4.2660789 -4.2603192 -4.2545705 -4.2303877 -4.2080574 -4.2034578 -4.2191472 -4.2317104 -4.2425075 -4.2549148 -4.2691331 -4.29265][-4.2251258 -4.2533779 -4.2697325 -4.2622418 -4.2487912 -4.2276397 -4.1792502 -4.132606 -4.1273084 -4.1600709 -4.1914539 -4.212606 -4.2312512 -4.250875 -4.2773733][-4.217422 -4.244339 -4.2601242 -4.2491817 -4.227355 -4.1859469 -4.107655 -4.036891 -4.0422769 -4.1045508 -4.1587086 -4.1860476 -4.2073922 -4.2318134 -4.258986][-4.2193751 -4.2423224 -4.2536635 -4.236475 -4.2017679 -4.1356921 -4.0226727 -3.9314227 -3.9640155 -4.0676703 -4.1409721 -4.1669731 -4.1860771 -4.2106709 -4.2356863][-4.2249136 -4.2413535 -4.2471023 -4.2217689 -4.1724873 -4.0823579 -3.9439888 -3.852308 -3.920387 -4.0557995 -4.1352997 -4.155766 -4.1687522 -4.1893888 -4.2111821][-4.2216396 -4.2322803 -4.2313375 -4.2044888 -4.149487 -4.0603251 -3.9401026 -3.881922 -3.9629965 -4.0835705 -4.1439643 -4.1537275 -4.1590643 -4.1730108 -4.1892095][-4.1937571 -4.1963997 -4.1929765 -4.1735682 -4.1361794 -4.0791979 -4.0033822 -3.9804168 -4.0411873 -4.116375 -4.1469326 -4.1495719 -4.1526566 -4.1616974 -4.1719413][-4.1617966 -4.1552939 -4.1530719 -4.1492453 -4.1389513 -4.115099 -4.0725422 -4.0631456 -4.0954781 -4.1358681 -4.1512718 -4.1526508 -4.1546764 -4.1620922 -4.1704197][-4.1456504 -4.1316967 -4.1299076 -4.1370106 -4.1479278 -4.1494541 -4.1288152 -4.1193004 -4.1315536 -4.1512303 -4.1597476 -4.1624851 -4.1667552 -4.17473 -4.1832976][-4.164865 -4.1518316 -4.1502423 -4.1599579 -4.1814566 -4.1969786 -4.187573 -4.1751847 -4.1749411 -4.182662 -4.1882362 -4.1938839 -4.2013459 -4.2086415 -4.2164912][-4.2152534 -4.2037249 -4.1992464 -4.2061858 -4.2275896 -4.2469978 -4.2425256 -4.228066 -4.2221975 -4.2288704 -4.2400885 -4.2525744 -4.257287 -4.2572279 -4.2605934][-4.2752113 -4.2727461 -4.2693377 -4.2705483 -4.2825365 -4.2942209 -4.2888741 -4.274466 -4.26706 -4.2723022 -4.2858849 -4.300797 -4.3030148 -4.2981057 -4.2955527][-4.3164167 -4.3214774 -4.3176508 -4.3155427 -4.3197827 -4.3225503 -4.3177824 -4.3083854 -4.3031096 -4.3067946 -4.3166895 -4.3277893 -4.3271575 -4.3199763 -4.3145251]]...]
INFO - root - 2017-12-07 09:32:00.224571: step 1110, loss = 2.06, batch loss = 2.01 (7.8 examples/sec; 1.023 sec/batch; 94h:07m:56s remains)
INFO - root - 2017-12-07 09:32:09.998044: step 1120, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 91h:15m:22s remains)
INFO - root - 2017-12-07 09:32:19.674893: step 1130, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 90h:13m:56s remains)
INFO - root - 2017-12-07 09:32:29.352084: step 1140, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 90h:29m:41s remains)
INFO - root - 2017-12-07 09:32:39.070482: step 1150, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 91h:47m:46s remains)
INFO - root - 2017-12-07 09:32:48.838231: step 1160, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 86h:22m:53s remains)
INFO - root - 2017-12-07 09:32:58.679098: step 1170, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 91h:42m:10s remains)
INFO - root - 2017-12-07 09:33:08.556857: step 1180, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 93h:34m:34s remains)
INFO - root - 2017-12-07 09:33:18.386010: step 1190, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 88h:27m:45s remains)
INFO - root - 2017-12-07 09:33:28.389265: step 1200, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 88h:48m:25s remains)
2017-12-07 09:33:29.442555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2736487 -4.2578869 -4.2482815 -4.245729 -4.2501144 -4.261404 -4.2765832 -4.2934265 -4.2926192 -4.2727032 -4.2482104 -4.2436562 -4.2625027 -4.2840137 -4.2951384][-4.2623434 -4.2447295 -4.2261095 -4.2086172 -4.2005992 -4.2090034 -4.2318377 -4.26267 -4.2738533 -4.2644877 -4.2490664 -4.2461848 -4.2599754 -4.2726579 -4.273664][-4.2262988 -4.2095537 -4.1817403 -4.1466961 -4.1247473 -4.1306834 -4.1601825 -4.2041612 -4.2371039 -4.2529325 -4.2571406 -4.260972 -4.2687707 -4.268415 -4.2546082][-4.1784625 -4.166667 -4.1326628 -4.076333 -4.0382528 -4.0427656 -4.0697937 -4.1080804 -4.1591587 -4.2104506 -4.2445979 -4.2653685 -4.2757711 -4.2699866 -4.2481818][-4.1397038 -4.137692 -4.0999327 -4.0258536 -3.9688253 -3.9590211 -3.9573843 -3.9586112 -4.019908 -4.1161761 -4.1886625 -4.2390046 -4.2669597 -4.270721 -4.2531433][-4.1250129 -4.1384115 -4.1076422 -4.0275154 -3.9509346 -3.8991878 -3.8301351 -3.7571969 -3.8275812 -3.9837115 -4.1049986 -4.18569 -4.2351351 -4.255342 -4.2501163][-4.1311188 -4.1608949 -4.151967 -4.0883765 -3.9995072 -3.8973408 -3.7449734 -3.5811398 -3.6566567 -3.8692207 -4.024 -4.1211724 -4.1868205 -4.2244811 -4.2366033][-4.129127 -4.1797991 -4.2029462 -4.1747246 -4.103157 -3.9966366 -3.8202403 -3.6289153 -3.683418 -3.8672895 -4.0001884 -4.0817156 -4.1449342 -4.1939464 -4.2179756][-4.126914 -4.18925 -4.2374015 -4.2459154 -4.2111731 -4.1444798 -4.0139 -3.8675964 -3.8789775 -3.9718668 -4.0431571 -4.0881958 -4.1367888 -4.1857729 -4.212779][-4.1401453 -4.1975102 -4.2488046 -4.2724304 -4.2676568 -4.240818 -4.1623979 -4.0668168 -4.0585165 -4.09441 -4.1245031 -4.1458611 -4.1792145 -4.2173562 -4.234396][-4.1744995 -4.2132521 -4.2461 -4.267839 -4.2767315 -4.271183 -4.2291522 -4.1728683 -4.164824 -4.1847396 -4.2043362 -4.2188773 -4.2365031 -4.2514319 -4.2541037][-4.2040014 -4.2210493 -4.2349892 -4.2516837 -4.2671666 -4.272265 -4.2479477 -4.2113614 -4.203712 -4.2204113 -4.2423835 -4.2544394 -4.2575994 -4.2527614 -4.2452512][-4.2119293 -4.2131596 -4.2134109 -4.2290626 -4.2493973 -4.2559743 -4.2359858 -4.2051291 -4.1942849 -4.2056217 -4.2253432 -4.2338929 -4.2290773 -4.21449 -4.2077951][-4.2181277 -4.2103381 -4.2011304 -4.2093048 -4.2277122 -4.2357979 -4.219924 -4.1927204 -4.1725144 -4.1624751 -4.1654515 -4.1666293 -4.1587186 -4.1471376 -4.1556182][-4.2367921 -4.21941 -4.199851 -4.1951962 -4.2046075 -4.2136111 -4.2060337 -4.1845274 -4.1536236 -4.1164503 -4.0919285 -4.0793314 -4.0736694 -4.0790205 -4.11652]]...]
INFO - root - 2017-12-07 09:33:39.268234: step 1210, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.014 sec/batch; 93h:16m:48s remains)
INFO - root - 2017-12-07 09:33:49.131135: step 1220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 86h:23m:19s remains)
INFO - root - 2017-12-07 09:33:59.026498: step 1230, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 91h:59m:37s remains)
INFO - root - 2017-12-07 09:34:08.706102: step 1240, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 92h:31m:05s remains)
INFO - root - 2017-12-07 09:34:18.436763: step 1250, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 87h:18m:06s remains)
INFO - root - 2017-12-07 09:34:28.347922: step 1260, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 88h:46m:52s remains)
INFO - root - 2017-12-07 09:34:38.209348: step 1270, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 93h:01m:55s remains)
INFO - root - 2017-12-07 09:34:48.151903: step 1280, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 90h:44m:28s remains)
INFO - root - 2017-12-07 09:34:57.927110: step 1290, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 90h:59m:27s remains)
INFO - root - 2017-12-07 09:35:07.762035: step 1300, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 88h:45m:32s remains)
2017-12-07 09:35:08.785171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2784262 -4.2898154 -4.2767682 -4.2525973 -4.2282844 -4.2005458 -4.1836081 -4.1892605 -4.2174087 -4.2409868 -4.2461691 -4.2398229 -4.2308655 -4.2323089 -4.2494378][-4.2880664 -4.2943673 -4.2783747 -4.2488875 -4.2118587 -4.1680517 -4.1371379 -4.1355495 -4.1672645 -4.2044287 -4.22548 -4.2285571 -4.2228241 -4.2219 -4.2315979][-4.2642689 -4.2650409 -4.2498245 -4.2225718 -4.1808157 -4.1306057 -4.0947428 -4.0924463 -4.1253524 -4.1688862 -4.1995039 -4.2063603 -4.199151 -4.1943846 -4.1968746][-4.2214446 -4.2183537 -4.2053528 -4.1829205 -4.1448922 -4.09955 -4.0667596 -4.0656757 -4.098033 -4.1398048 -4.1715751 -4.1788449 -4.170342 -4.1631837 -4.161994][-4.1789594 -4.1743221 -4.1663132 -4.150104 -4.1181617 -4.0763927 -4.0382242 -4.0303621 -4.0673742 -4.1131086 -4.1478915 -4.1612182 -4.1564889 -4.1500354 -4.1485305][-4.1490741 -4.1433592 -4.1419921 -4.1297522 -4.09971 -4.0538449 -4.0026469 -3.980938 -4.0259752 -4.0898275 -4.1389451 -4.1645193 -4.1685648 -4.1642022 -4.1632452][-4.1478539 -4.139967 -4.139112 -4.1244955 -4.0889935 -4.0385222 -3.978271 -3.9439964 -3.9973037 -4.0805163 -4.1467891 -4.1856284 -4.1974845 -4.198348 -4.2006555][-4.1642733 -4.1552939 -4.1513653 -4.1319814 -4.0942039 -4.047401 -3.9985642 -3.97147 -4.0203114 -4.1001086 -4.1688976 -4.212297 -4.2285414 -4.2365532 -4.2429061][-4.16608 -4.1553621 -4.1513147 -4.1375651 -4.107223 -4.0736523 -4.0497723 -4.0422697 -4.0771427 -4.1336918 -4.18925 -4.2288275 -4.2481008 -4.26071 -4.2699423][-4.1622477 -4.1539316 -4.1533666 -4.1520553 -4.1316328 -4.1081052 -4.0982032 -4.0986967 -4.1200986 -4.1540418 -4.1964169 -4.2312107 -4.2526197 -4.2678185 -4.2760854][-4.1680012 -4.167356 -4.1768565 -4.1859593 -4.1706896 -4.1497221 -4.141129 -4.1396985 -4.1484237 -4.163847 -4.1956606 -4.2260008 -4.2443576 -4.2571979 -4.2606888][-4.1815023 -4.1863546 -4.2042112 -4.2150178 -4.2030487 -4.1872082 -4.1809855 -4.1746206 -4.1757541 -4.1818452 -4.202877 -4.2277617 -4.241653 -4.249836 -4.242909][-4.2003212 -4.2057166 -4.2251544 -4.2344766 -4.225606 -4.2152243 -4.21143 -4.2037435 -4.1981931 -4.1950264 -4.203979 -4.2202721 -4.2289581 -4.234385 -4.2261543][-4.210793 -4.2142215 -4.2372756 -4.2513194 -4.2479854 -4.2400112 -4.2353282 -4.2268019 -4.2169065 -4.2050443 -4.199275 -4.1980848 -4.197629 -4.2054515 -4.2077093][-4.2229238 -4.2202535 -4.2414842 -4.259593 -4.2628479 -4.2580481 -4.2554436 -4.2490916 -4.2390137 -4.2239461 -4.2073836 -4.1922359 -4.1803293 -4.1876554 -4.2010131]]...]
INFO - root - 2017-12-07 09:35:18.543773: step 1310, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 91h:03m:19s remains)
INFO - root - 2017-12-07 09:35:28.282230: step 1320, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 90h:22m:05s remains)
INFO - root - 2017-12-07 09:35:38.022621: step 1330, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 84h:50m:19s remains)
INFO - root - 2017-12-07 09:35:47.865163: step 1340, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 86h:47m:03s remains)
INFO - root - 2017-12-07 09:35:57.655253: step 1350, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 93h:03m:02s remains)
INFO - root - 2017-12-07 09:36:07.529203: step 1360, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 86h:07m:17s remains)
INFO - root - 2017-12-07 09:36:17.431199: step 1370, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 92h:05m:32s remains)
INFO - root - 2017-12-07 09:36:27.297104: step 1380, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 89h:59m:42s remains)
INFO - root - 2017-12-07 09:36:37.065987: step 1390, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 88h:31m:07s remains)
INFO - root - 2017-12-07 09:36:46.846171: step 1400, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.981 sec/batch; 90h:10m:56s remains)
2017-12-07 09:36:47.743476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2096405 -4.2168164 -4.2195897 -4.2180238 -4.2119031 -4.2091808 -4.2084355 -4.2015629 -4.1845303 -4.1700621 -4.1670222 -4.1715517 -4.1792808 -4.1884341 -4.2065454][-4.2124581 -4.2218304 -4.2291837 -4.2312822 -4.2251787 -4.2205095 -4.2191558 -4.2128963 -4.19855 -4.1892419 -4.1906004 -4.1944523 -4.196157 -4.1988411 -4.2116728][-4.2165751 -4.2252932 -4.233602 -4.23763 -4.2335954 -4.2286539 -4.2280025 -4.22158 -4.2090812 -4.2030129 -4.2086182 -4.2146907 -4.2152762 -4.2155514 -4.2223926][-4.2080545 -4.2111683 -4.2158594 -4.2193274 -4.2168446 -4.2121911 -4.2133512 -4.2100067 -4.2015014 -4.1991305 -4.2098546 -4.2218132 -4.2257557 -4.2255154 -4.2287736][-4.1837573 -4.1776638 -4.1776824 -4.1796732 -4.1763644 -4.1682563 -4.1656127 -4.1632071 -4.1597557 -4.1647053 -4.1832414 -4.2055597 -4.2162647 -4.2185907 -4.2201166][-4.158699 -4.1401944 -4.1307831 -4.1281486 -4.1195292 -4.1049242 -4.0965672 -4.0954456 -4.1004992 -4.1175179 -4.1439419 -4.1735311 -4.1921234 -4.1992583 -4.1995783][-4.1667042 -4.1355886 -4.1107769 -4.0946479 -4.071116 -4.0405521 -4.0220795 -4.02089 -4.0379329 -4.0731897 -4.1114483 -4.147377 -4.1720858 -4.1849337 -4.1836987][-4.2098861 -4.1697226 -4.1314292 -4.1019859 -4.0641875 -4.0160937 -3.9828255 -3.9789705 -4.0028696 -4.0564318 -4.1106658 -4.155642 -4.1870241 -4.2044339 -4.2023215][-4.2519388 -4.2159715 -4.1790748 -4.1498938 -4.1118875 -4.0616307 -4.0256653 -4.0171642 -4.0382047 -4.0896382 -4.1409965 -4.1840997 -4.2156949 -4.2357121 -4.2370358][-4.27754 -4.2496109 -4.2212462 -4.1992445 -4.1699977 -4.1329985 -4.1077018 -4.1026149 -4.1189313 -4.1514335 -4.1812644 -4.2059464 -4.2225571 -4.2345304 -4.2390413][-4.2853184 -4.2672668 -4.2464871 -4.2297978 -4.2089062 -4.1841755 -4.1671824 -4.1622753 -4.1701646 -4.1862721 -4.1994138 -4.2085505 -4.2100754 -4.2124949 -4.2168746][-4.2755251 -4.2689395 -4.2569885 -4.2452555 -4.2313752 -4.2125487 -4.1937284 -4.1795211 -4.174758 -4.1790123 -4.1832471 -4.1887708 -4.189467 -4.1942563 -4.2031665][-4.2570057 -4.2646112 -4.2633133 -4.2561755 -4.2469888 -4.23129 -4.2099066 -4.1854382 -4.1713805 -4.1679 -4.1696095 -4.1793165 -4.1887326 -4.2022271 -4.2194571][-4.2281933 -4.2477908 -4.2574673 -4.25749 -4.25391 -4.244442 -4.2255874 -4.2011743 -4.1836085 -4.1746964 -4.1734319 -4.184691 -4.2010446 -4.2221813 -4.2452531][-4.1955829 -4.2138987 -4.2282062 -4.2358613 -4.2420387 -4.2437334 -4.2363725 -4.222611 -4.2084975 -4.1980267 -4.1953473 -4.2055564 -4.2206941 -4.2384834 -4.2574677]]...]
INFO - root - 2017-12-07 09:36:57.555676: step 1410, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 85h:11m:22s remains)
INFO - root - 2017-12-07 09:37:07.545111: step 1420, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 93h:09m:28s remains)
INFO - root - 2017-12-07 09:37:17.150334: step 1430, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 87h:36m:58s remains)
INFO - root - 2017-12-07 09:37:26.925554: step 1440, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 90h:15m:34s remains)
INFO - root - 2017-12-07 09:37:36.739129: step 1450, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 91h:49m:14s remains)
INFO - root - 2017-12-07 09:37:46.561676: step 1460, loss = 2.10, batch loss = 2.05 (7.9 examples/sec; 1.008 sec/batch; 92h:42m:22s remains)
INFO - root - 2017-12-07 09:37:56.377492: step 1470, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 91h:32m:21s remains)
INFO - root - 2017-12-07 09:38:06.256620: step 1480, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.045 sec/batch; 96h:04m:20s remains)
INFO - root - 2017-12-07 09:38:16.222260: step 1490, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.013 sec/batch; 93h:06m:27s remains)
INFO - root - 2017-12-07 09:38:26.075238: step 1500, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 90h:08m:46s remains)
2017-12-07 09:38:27.065128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2420878 -4.2470078 -4.2622471 -4.2647343 -4.2612658 -4.2633643 -4.2686253 -4.2684979 -4.2565718 -4.2354813 -4.2395282 -4.2664051 -4.2823033 -4.28165 -4.2699294][-4.1766505 -4.196609 -4.2328043 -4.2406182 -4.2361417 -4.2390766 -4.24662 -4.2453442 -4.229907 -4.2085781 -4.2143464 -4.2466559 -4.2670803 -4.2694874 -4.2569594][-4.1214533 -4.1539984 -4.2053361 -4.2148814 -4.2068167 -4.2046165 -4.2118268 -4.2117057 -4.2005 -4.186285 -4.1941133 -4.2261643 -4.2498775 -4.2565827 -4.2447762][-4.09691 -4.1351051 -4.1900291 -4.1963034 -4.1783075 -4.1678891 -4.173059 -4.1773505 -4.1748929 -4.1728225 -4.1857076 -4.2161946 -4.2420874 -4.2533054 -4.2453933][-4.1009116 -4.1406493 -4.1906409 -4.1907196 -4.1627331 -4.1462908 -4.1521807 -4.1626267 -4.1674414 -4.1717057 -4.1918492 -4.2218046 -4.2483492 -4.260963 -4.2565722][-4.1244917 -4.1599922 -4.1965365 -4.1864271 -4.1469426 -4.1273022 -4.1381421 -4.156 -4.1641746 -4.1670885 -4.19165 -4.2236376 -4.25212 -4.2680674 -4.2710414][-4.1497664 -4.1768994 -4.193727 -4.1710958 -4.1206927 -4.0957193 -4.1078448 -4.13414 -4.1461039 -4.145864 -4.1678905 -4.2036757 -4.2365184 -4.2593918 -4.2727771][-4.1478119 -4.1651149 -4.1650209 -4.133296 -4.0764346 -4.0423675 -4.0490065 -4.0798693 -4.0960965 -4.0911269 -4.1087694 -4.15231 -4.1999259 -4.2344861 -4.2552738][-4.1311 -4.1421785 -4.1358037 -4.1047373 -4.0470753 -4.00355 -4.0015159 -4.0301018 -4.0459785 -4.0345535 -4.0434127 -4.0887909 -4.146986 -4.1934595 -4.220521][-4.1134129 -4.1201096 -4.1153154 -4.0964451 -4.051158 -4.0093937 -3.9985089 -4.0161214 -4.0262632 -4.0112591 -4.0137415 -4.0532565 -4.11179 -4.160985 -4.1886024][-4.1058249 -4.1144009 -4.1148453 -4.1112924 -4.0873308 -4.0558643 -4.0380716 -4.0420656 -4.0452967 -4.0310531 -4.0307059 -4.0600038 -4.1076159 -4.14882 -4.1749868][-4.1236434 -4.1342912 -4.1394324 -4.1453071 -4.1397076 -4.1195064 -4.0982976 -4.090786 -4.0898981 -4.0818424 -4.0813522 -4.1004248 -4.1360464 -4.169147 -4.1902008][-4.166986 -4.180263 -4.1862874 -4.1927204 -4.1956029 -4.1873307 -4.1704311 -4.1572394 -4.1540818 -4.1509142 -4.1495123 -4.1579342 -4.1820021 -4.210535 -4.2294483][-4.2183919 -4.2318764 -4.2386761 -4.2446675 -4.2491083 -4.2484612 -4.2406526 -4.2299628 -4.22537 -4.2234306 -4.2218232 -4.2235994 -4.2378411 -4.2600636 -4.2766056][-4.2693477 -4.2790008 -4.2849813 -4.2901034 -4.2946172 -4.2969437 -4.296113 -4.2912407 -4.28837 -4.2862325 -4.2846856 -4.2862587 -4.2955327 -4.3094697 -4.3189249]]...]
INFO - root - 2017-12-07 09:38:36.702244: step 1510, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.045 sec/batch; 96h:02m:09s remains)
INFO - root - 2017-12-07 09:38:46.428701: step 1520, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 83h:28m:04s remains)
INFO - root - 2017-12-07 09:38:56.181432: step 1530, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 92h:01m:05s remains)
INFO - root - 2017-12-07 09:39:06.074718: step 1540, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.021 sec/batch; 93h:51m:08s remains)
INFO - root - 2017-12-07 09:39:15.835367: step 1550, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 92h:14m:19s remains)
INFO - root - 2017-12-07 09:39:25.712900: step 1560, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 89h:55m:26s remains)
INFO - root - 2017-12-07 09:39:35.561358: step 1570, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 88h:08m:18s remains)
INFO - root - 2017-12-07 09:39:45.449815: step 1580, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 92h:32m:16s remains)
INFO - root - 2017-12-07 09:39:55.286333: step 1590, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 87h:09m:12s remains)
INFO - root - 2017-12-07 09:40:05.240666: step 1600, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.028 sec/batch; 94h:27m:53s remains)
2017-12-07 09:40:06.170635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3380828 -4.3358393 -4.3342972 -4.3284225 -4.3196158 -4.3099957 -4.3055992 -4.3094745 -4.3139257 -4.3196287 -4.327817 -4.337543 -4.3444347 -4.3471932 -4.3466039][-4.3263755 -4.326663 -4.3289757 -4.32458 -4.3161054 -4.3063841 -4.3025 -4.3099346 -4.3163214 -4.3239503 -4.3343019 -4.3424835 -4.3438172 -4.3391776 -4.3337951][-4.3080583 -4.3103585 -4.3147573 -4.3103456 -4.3018107 -4.292563 -4.2896504 -4.2995515 -4.3079562 -4.3180823 -4.3311791 -4.3390751 -4.3356228 -4.3227882 -4.3115067][-4.2909145 -4.2954135 -4.2983871 -4.2911711 -4.2801428 -4.2695975 -4.2684364 -4.2836313 -4.2977309 -4.3134913 -4.3299165 -4.3379569 -4.3312387 -4.3128963 -4.2973056][-4.27691 -4.2773156 -4.2737293 -4.2588558 -4.2402339 -4.223516 -4.221384 -4.2419462 -4.2659874 -4.2944846 -4.3196425 -4.3305645 -4.323545 -4.3065243 -4.2908893][-4.262289 -4.2500186 -4.2348647 -4.2109776 -4.18358 -4.1572418 -4.1464534 -4.1702924 -4.2094603 -4.255085 -4.2943687 -4.3115654 -4.3096 -4.2990661 -4.2868671][-4.2561607 -4.2317472 -4.2022729 -4.1613 -4.1140108 -4.0636363 -4.0305939 -4.0563931 -4.1192031 -4.1901712 -4.2486868 -4.2795887 -4.2892509 -4.2885156 -4.2807956][-4.2609382 -4.2293506 -4.1898274 -4.1320014 -4.058322 -3.9697018 -3.8977611 -3.9219983 -4.0074706 -4.1032543 -4.1871786 -4.2401328 -4.2660232 -4.2772627 -4.2768345][-4.2711353 -4.2478294 -4.2204938 -4.1743331 -4.1081724 -4.0194621 -3.9391496 -3.948796 -4.0169296 -4.1015015 -4.1835604 -4.2371407 -4.265882 -4.278141 -4.2798762][-4.2870731 -4.2799778 -4.2720337 -4.2454929 -4.1969991 -4.1282773 -4.0646987 -4.062232 -4.095542 -4.1453424 -4.2057295 -4.2470217 -4.2688389 -4.2783504 -4.2823954][-4.2770691 -4.2784281 -4.2810926 -4.2648277 -4.2250824 -4.1711197 -4.1277642 -4.1299815 -4.147274 -4.1748 -4.2168679 -4.245945 -4.263382 -4.2714825 -4.2771058][-4.2566614 -4.2609677 -4.2701139 -4.2626052 -4.2328906 -4.1953177 -4.16921 -4.1778283 -4.1953468 -4.2164874 -4.2441607 -4.2624059 -4.2747765 -4.2797103 -4.2827425][-4.2393041 -4.2422094 -4.250854 -4.2477093 -4.2272849 -4.2032819 -4.1882291 -4.1999969 -4.2178 -4.234622 -4.2552004 -4.27268 -4.2858372 -4.2922869 -4.2941451][-4.2252135 -4.2218571 -4.2245154 -4.2225423 -4.2103887 -4.1965475 -4.1879573 -4.2020726 -4.2224083 -4.2400012 -4.2596807 -4.2784767 -4.2933035 -4.2998247 -4.2998319][-4.2244296 -4.2167306 -4.2168818 -4.2147083 -4.206964 -4.1978087 -4.1936855 -4.2089148 -4.2279429 -4.2447734 -4.2635441 -4.2831192 -4.2992449 -4.3058429 -4.3041968]]...]
INFO - root - 2017-12-07 09:40:16.075439: step 1610, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 91h:05m:23s remains)
INFO - root - 2017-12-07 09:40:25.812687: step 1620, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 90h:01m:37s remains)
INFO - root - 2017-12-07 09:40:35.727485: step 1630, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 93h:01m:01s remains)
INFO - root - 2017-12-07 09:40:45.732901: step 1640, loss = 2.07, batch loss = 2.02 (7.2 examples/sec; 1.116 sec/batch; 102h:32m:20s remains)
INFO - root - 2017-12-07 09:40:55.626297: step 1650, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 88h:31m:14s remains)
INFO - root - 2017-12-07 09:41:05.498875: step 1660, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.036 sec/batch; 95h:13m:55s remains)
INFO - root - 2017-12-07 09:41:15.493650: step 1670, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 89h:35m:22s remains)
INFO - root - 2017-12-07 09:41:25.259959: step 1680, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 90h:14m:19s remains)
INFO - root - 2017-12-07 09:41:35.331682: step 1690, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 88h:33m:58s remains)
INFO - root - 2017-12-07 09:41:45.071143: step 1700, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 87h:20m:07s remains)
2017-12-07 09:41:45.955588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1462889 -4.0814018 -4.0663605 -4.1219873 -4.2044172 -4.2758179 -4.3175631 -4.3202615 -4.3031707 -4.2982378 -4.2969723 -4.3059425 -4.3213372 -4.3325815 -4.3183007][-4.1213436 -4.0443482 -4.0315137 -4.1004748 -4.1980109 -4.2733054 -4.3055077 -4.2981892 -4.2778192 -4.2755594 -4.2800784 -4.2973547 -4.3226171 -4.334549 -4.3166943][-4.1439543 -4.0750446 -4.0686574 -4.1327443 -4.2243266 -4.28829 -4.3040271 -4.2793584 -4.2510014 -4.2474642 -4.257278 -4.2860932 -4.3246164 -4.3362479 -4.3126092][-4.1873655 -4.1505342 -4.1617622 -4.2117252 -4.2755952 -4.3119655 -4.3006454 -4.2496762 -4.2102494 -4.2103829 -4.2335434 -4.2774835 -4.3263755 -4.3357296 -4.3075929][-4.2278314 -4.2215343 -4.2453637 -4.2725811 -4.2998133 -4.2990589 -4.252665 -4.16751 -4.1157722 -4.133862 -4.1891265 -4.2566242 -4.3134704 -4.3216562 -4.2945557][-4.2546959 -4.2645283 -4.285109 -4.2904181 -4.2834773 -4.2404261 -4.1425557 -4.0082707 -3.9466093 -4.0055857 -4.1111321 -4.2107129 -4.277257 -4.290822 -4.2694182][-4.2716 -4.2897944 -4.3049245 -4.295464 -4.260097 -4.1791472 -4.0381789 -3.8645732 -3.8081298 -3.9144106 -4.0578237 -4.1757379 -4.2457891 -4.2626982 -4.2441435][-4.293169 -4.30938 -4.3163753 -4.2979836 -4.25358 -4.1702332 -4.0409689 -3.9007993 -3.8768158 -3.9762366 -4.0994554 -4.1957741 -4.2481556 -4.2544961 -4.2306552][-4.3129592 -4.3196959 -4.3166456 -4.2956963 -4.2577934 -4.1973877 -4.1141109 -4.0406508 -4.0444016 -4.1146584 -4.1954312 -4.254807 -4.2776461 -4.2645454 -4.2326055][-4.3188496 -4.3231192 -4.3199906 -4.3004851 -4.2718 -4.2339005 -4.1867819 -4.1549578 -4.1713376 -4.2221007 -4.2749305 -4.3089137 -4.3089323 -4.280066 -4.2445436][-4.3196068 -4.3260241 -4.32569 -4.3113461 -4.2939544 -4.2723231 -4.2435637 -4.2270665 -4.2428665 -4.2802949 -4.3144407 -4.3325834 -4.3242116 -4.2973628 -4.269556][-4.3232141 -4.3263183 -4.3262916 -4.3176479 -4.3095827 -4.297904 -4.2780128 -4.267487 -4.2783365 -4.3030505 -4.3226767 -4.331356 -4.3253536 -4.3094287 -4.292305][-4.3275447 -4.3244734 -4.3202357 -4.3141623 -4.3117743 -4.3087263 -4.3012419 -4.2984958 -4.3037167 -4.3137469 -4.318912 -4.3200502 -4.3163261 -4.3097963 -4.301548][-4.3338852 -4.3274827 -4.3211017 -4.31645 -4.3166857 -4.3196111 -4.3229775 -4.326273 -4.3284569 -4.3286567 -4.3229747 -4.3184071 -4.3148704 -4.3118596 -4.3080897][-4.3336549 -4.3299541 -4.3265309 -4.3242393 -4.3238811 -4.3256478 -4.3302317 -4.3350329 -4.3368006 -4.3335714 -4.3264527 -4.3227763 -4.3216133 -4.3221116 -4.3222785]]...]
INFO - root - 2017-12-07 09:41:55.723975: step 1710, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.957 sec/batch; 87h:53m:51s remains)
INFO - root - 2017-12-07 09:42:05.503581: step 1720, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 90h:09m:16s remains)
INFO - root - 2017-12-07 09:42:15.292749: step 1730, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 90h:05m:08s remains)
INFO - root - 2017-12-07 09:42:25.143786: step 1740, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 86h:27m:27s remains)
INFO - root - 2017-12-07 09:42:34.928450: step 1750, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 89h:10m:13s remains)
INFO - root - 2017-12-07 09:42:44.838411: step 1760, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 90h:52m:02s remains)
INFO - root - 2017-12-07 09:42:54.616129: step 1770, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 93h:23m:02s remains)
INFO - root - 2017-12-07 09:43:04.527077: step 1780, loss = 2.11, batch loss = 2.05 (7.9 examples/sec; 1.015 sec/batch; 93h:14m:42s remains)
INFO - root - 2017-12-07 09:43:14.239441: step 1790, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 89h:17m:21s remains)
INFO - root - 2017-12-07 09:43:24.148047: step 1800, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 86h:57m:36s remains)
2017-12-07 09:43:25.119429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3190618 -4.3216729 -4.2936172 -4.2496614 -4.21416 -4.1940784 -4.1884375 -4.2150006 -4.2419152 -4.2277875 -4.1944346 -4.18052 -4.19854 -4.2251863 -4.2418008][-4.3245115 -4.3274794 -4.2996387 -4.2548375 -4.2162094 -4.1878181 -4.17639 -4.2052617 -4.2363911 -4.2266259 -4.1948166 -4.1796241 -4.2010965 -4.234468 -4.2611036][-4.3262668 -4.3273473 -4.3005033 -4.2566376 -4.2120476 -4.175066 -4.1600137 -4.19418 -4.22761 -4.2142372 -4.1745558 -4.1634841 -4.2015429 -4.2433867 -4.2748823][-4.3247819 -4.3240166 -4.29776 -4.250464 -4.1980615 -4.1548028 -4.1391768 -4.1740775 -4.2047777 -4.1833153 -4.1420512 -4.1501679 -4.2065473 -4.2536268 -4.2809343][-4.3255873 -4.3251281 -4.2986526 -4.24574 -4.1845613 -4.1366763 -4.1178384 -4.1482878 -4.1704431 -4.1405497 -4.112452 -4.1477494 -4.2168832 -4.2628684 -4.2819796][-4.3275414 -4.3286238 -4.30101 -4.2442794 -4.1753607 -4.121717 -4.098947 -4.1188612 -4.1231327 -4.08243 -4.0826669 -4.1492529 -4.2244072 -4.265295 -4.2793365][-4.3280964 -4.3291597 -4.3013396 -4.24407 -4.175817 -4.11998 -4.0910807 -4.0956559 -4.0768509 -4.0334845 -4.071528 -4.158534 -4.2311573 -4.2644715 -4.27455][-4.3264923 -4.3266063 -4.299314 -4.2480478 -4.1873956 -4.1336761 -4.0993314 -4.0917044 -4.0708013 -4.0530529 -4.1129212 -4.1961985 -4.254559 -4.2776012 -4.2765222][-4.3217707 -4.3214006 -4.2963409 -4.2520089 -4.2017746 -4.1564536 -4.123373 -4.112431 -4.1053977 -4.1176038 -4.177938 -4.2423725 -4.2845578 -4.2932529 -4.2759166][-4.3163443 -4.3160043 -4.2945037 -4.2579269 -4.2179132 -4.1808686 -4.1506395 -4.1365738 -4.139318 -4.1721387 -4.2316947 -4.2819796 -4.3138533 -4.3125396 -4.28393][-4.311367 -4.3120604 -4.2949672 -4.2656608 -4.2295809 -4.1934237 -4.1602607 -4.1425991 -4.1592941 -4.2070956 -4.2664361 -4.3137341 -4.3409815 -4.3321104 -4.3028278][-4.3108468 -4.3120008 -4.2960906 -4.268249 -4.231534 -4.1904364 -4.1520667 -4.140451 -4.1751223 -4.2309914 -4.2877254 -4.3327732 -4.3513184 -4.3365741 -4.3083596][-4.3157744 -4.3177338 -4.3007174 -4.2659311 -4.2202339 -4.1693754 -4.1335888 -4.1393261 -4.190084 -4.248951 -4.3025236 -4.3423862 -4.3508558 -4.3299 -4.2997093][-4.3251696 -4.3271718 -4.3055077 -4.2546506 -4.1892352 -4.1243005 -4.1006689 -4.1342568 -4.2006769 -4.2654448 -4.316268 -4.3453374 -4.3432641 -4.3148255 -4.2815361][-4.3364964 -4.3382587 -4.308001 -4.2384405 -4.1496825 -4.0767217 -4.0740871 -4.1378736 -4.2159948 -4.2811284 -4.32668 -4.3443017 -4.3319683 -4.2984648 -4.2688937]]...]
INFO - root - 2017-12-07 09:43:34.819612: step 1810, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 91h:43m:23s remains)
INFO - root - 2017-12-07 09:43:44.553709: step 1820, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 86h:58m:24s remains)
INFO - root - 2017-12-07 09:43:54.233333: step 1830, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 88h:54m:30s remains)
INFO - root - 2017-12-07 09:44:03.985987: step 1840, loss = 2.08, batch loss = 2.02 (7.6 examples/sec; 1.059 sec/batch; 97h:14m:05s remains)
INFO - root - 2017-12-07 09:44:13.645738: step 1850, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 89h:59m:56s remains)
INFO - root - 2017-12-07 09:44:23.357620: step 1860, loss = 2.03, batch loss = 1.97 (8.2 examples/sec; 0.971 sec/batch; 89h:13m:33s remains)
INFO - root - 2017-12-07 09:44:33.289707: step 1870, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.020 sec/batch; 93h:39m:54s remains)
INFO - root - 2017-12-07 09:44:43.079534: step 1880, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 90h:51m:30s remains)
INFO - root - 2017-12-07 09:44:52.979297: step 1890, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 92h:06m:50s remains)
INFO - root - 2017-12-07 09:45:02.602877: step 1900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 87h:54m:56s remains)
2017-12-07 09:45:03.539920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393641 -4.2237496 -4.2301497 -4.250813 -4.2639813 -4.269526 -4.2839265 -4.294838 -4.3025551 -4.2979136 -4.2755041 -4.2527428 -4.2349119 -4.2276163 -4.2337422][-4.2198262 -4.1846967 -4.1795282 -4.2052388 -4.2345204 -4.2513695 -4.2680311 -4.285027 -4.3018627 -4.3018937 -4.2782722 -4.2555819 -4.2374487 -4.2274919 -4.2287068][-4.1991386 -4.1446433 -4.12342 -4.1485419 -4.1947103 -4.2299862 -4.2473989 -4.2637744 -4.2813106 -4.2838068 -4.2627549 -4.251039 -4.2390394 -4.2267747 -4.2194452][-4.1791019 -4.1179652 -4.0784984 -4.0849323 -4.1333065 -4.18598 -4.2155519 -4.2348318 -4.2551508 -4.2588239 -4.2477655 -4.2507334 -4.2475123 -4.2359157 -4.2203355][-4.1703405 -4.1119423 -4.0580788 -4.0412779 -4.0734115 -4.1277781 -4.1688967 -4.2020988 -4.2365208 -4.2458425 -4.2415543 -4.2507739 -4.2530537 -4.242023 -4.2216415][-4.1647525 -4.1126733 -4.0565424 -4.0199709 -4.0296688 -4.0678182 -4.1092606 -4.1536903 -4.2023807 -4.2246165 -4.2272534 -4.2364249 -4.2373037 -4.2275348 -4.2091856][-4.1457186 -4.1057148 -4.0539856 -4.0085249 -3.9963214 -4.0124617 -4.044661 -4.0899849 -4.1443257 -4.1742086 -4.1807051 -4.1960173 -4.2038 -4.2055616 -4.1978083][-4.1129241 -4.0841312 -4.0426474 -4.0011144 -3.9795456 -3.9759328 -3.9942348 -4.0313845 -4.0862503 -4.1171327 -4.1235323 -4.1428747 -4.1634846 -4.18205 -4.1854324][-4.0803189 -4.0585265 -4.0260553 -3.994628 -3.9721751 -3.9559071 -3.9566317 -3.9778 -4.0245409 -4.0549822 -4.0663648 -4.095284 -4.131041 -4.162035 -4.1727915][-4.0698009 -4.0492172 -4.0209823 -3.9936643 -3.9732466 -3.9523129 -3.9392107 -3.9454556 -3.9783723 -4.0098519 -4.0314765 -4.0717349 -4.1152053 -4.1508 -4.1662087][-4.0888057 -4.0679321 -4.0432 -4.0173879 -3.9964926 -3.9750443 -3.9574378 -3.9544964 -3.9746203 -3.9981275 -4.0184193 -4.0602565 -4.1052265 -4.141479 -4.1599503][-4.1440835 -4.1267109 -4.1060386 -4.0856209 -4.0687733 -4.0525832 -4.0387931 -4.0324922 -4.039391 -4.0443497 -4.0475521 -4.0776024 -4.116281 -4.14961 -4.1656938][-4.2245207 -4.213378 -4.1983395 -4.1824956 -4.1700745 -4.1595016 -4.1498418 -4.1446185 -4.144012 -4.1358852 -4.1233625 -4.1333957 -4.1552749 -4.1797385 -4.1933575][-4.2977037 -4.2921453 -4.2830434 -4.2730646 -4.26493 -4.2583089 -4.25203 -4.2482705 -4.2470474 -4.2385521 -4.2223663 -4.21776 -4.2232318 -4.2335539 -4.24149][-4.3421497 -4.3415179 -4.3375325 -4.332675 -4.3294435 -4.3266435 -4.32398 -4.3224697 -4.3220406 -4.3161044 -4.3027139 -4.2942047 -4.2905755 -4.2905517 -4.2919903]]...]
INFO - root - 2017-12-07 09:45:13.130816: step 1910, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.027 sec/batch; 94h:19m:48s remains)
INFO - root - 2017-12-07 09:45:22.987856: step 1920, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.006 sec/batch; 92h:22m:59s remains)
INFO - root - 2017-12-07 09:45:32.806412: step 1930, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.029 sec/batch; 94h:29m:04s remains)
INFO - root - 2017-12-07 09:45:42.608779: step 1940, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 88h:04m:40s remains)
INFO - root - 2017-12-07 09:45:52.474903: step 1950, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.031 sec/batch; 94h:41m:43s remains)
INFO - root - 2017-12-07 09:46:02.189523: step 1960, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 88h:37m:51s remains)
INFO - root - 2017-12-07 09:46:12.035780: step 1970, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 88h:25m:24s remains)
INFO - root - 2017-12-07 09:46:21.761312: step 1980, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 92h:53m:37s remains)
INFO - root - 2017-12-07 09:46:31.450644: step 1990, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 88h:41m:54s remains)
INFO - root - 2017-12-07 09:46:41.180983: step 2000, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 82h:19m:14s remains)
2017-12-07 09:46:42.124318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2511826 -4.2474947 -4.2553053 -4.260035 -4.2555351 -4.2493114 -4.2491956 -4.2472038 -4.244483 -4.2508307 -4.2530932 -4.253015 -4.25664 -4.2569385 -4.2559443][-4.24546 -4.2393603 -4.2460227 -4.2534394 -4.2489562 -4.2408895 -4.2417374 -4.2424 -4.2388272 -4.2380309 -4.2341213 -4.2299819 -4.2343063 -4.23918 -4.2425][-4.2449231 -4.2400908 -4.2454624 -4.2538915 -4.2469296 -4.2329125 -4.2329741 -4.2398815 -4.2410669 -4.2366385 -4.2241068 -4.21013 -4.2068615 -4.209362 -4.2127137][-4.2415261 -4.2412982 -4.2470083 -4.2557187 -4.2454982 -4.2233343 -4.21654 -4.2286782 -4.2407322 -4.2379365 -4.2186694 -4.1923914 -4.177125 -4.1722112 -4.1747551][-4.2223258 -4.2226334 -4.2274065 -4.2355695 -4.22553 -4.1942768 -4.1738172 -4.187346 -4.2156162 -4.2228785 -4.2049561 -4.1759238 -4.1531653 -4.1419315 -4.1457539][-4.2069888 -4.1998959 -4.2006269 -4.20676 -4.1945162 -4.145133 -4.0971828 -4.1036129 -4.1570487 -4.1933093 -4.1940737 -4.1723413 -4.1481237 -4.1361737 -4.1436915][-4.2203884 -4.2018342 -4.188447 -4.18478 -4.1642284 -4.0926704 -4.0067477 -3.9983709 -4.0783882 -4.1507492 -4.1775031 -4.1703429 -4.1522756 -4.145864 -4.1599221][-4.2373052 -4.2121181 -4.1874881 -4.1737781 -4.1485858 -4.0699158 -3.9644823 -3.9406061 -4.0239086 -4.1117425 -4.1572981 -4.1634827 -4.1502428 -4.1453595 -4.165318][-4.2467508 -4.2255187 -4.201735 -4.1888604 -4.1741343 -4.1183238 -4.0333219 -4.0010147 -4.0499086 -4.1130109 -4.1519585 -4.1610126 -4.1492314 -4.1420493 -4.1607327][-4.23419 -4.22506 -4.2195148 -4.2232089 -4.2270188 -4.2007656 -4.1453824 -4.1095581 -4.1177945 -4.1428189 -4.16324 -4.1683183 -4.1600943 -4.1532989 -4.1686459][-4.2052155 -4.2063789 -4.219583 -4.2400675 -4.2592387 -4.2520604 -4.21794 -4.1854653 -4.1751661 -4.1789322 -4.187438 -4.1911983 -4.186214 -4.1794329 -4.1893234][-4.1894441 -4.193748 -4.2149382 -4.2418575 -4.2673063 -4.2721238 -4.2538 -4.2308025 -4.2187619 -4.2165394 -4.2206273 -4.2243924 -4.2208338 -4.212872 -4.2177925][-4.19837 -4.1969051 -4.2144132 -4.2382088 -4.2633219 -4.2756829 -4.269752 -4.2545595 -4.2436929 -4.2403364 -4.242063 -4.2464819 -4.2461443 -4.2413187 -4.2445121][-4.2205939 -4.2116833 -4.2179661 -4.2311182 -4.249877 -4.2654681 -4.2687292 -4.2612162 -4.2531629 -4.2500677 -4.2512493 -4.2567463 -4.2605944 -4.26016 -4.263905][-4.2467494 -4.2348633 -4.2326779 -4.2360163 -4.2468519 -4.2613873 -4.2704563 -4.2699294 -4.2664413 -4.2639241 -4.263876 -4.2688937 -4.27423 -4.2753444 -4.2785211]]...]
INFO - root - 2017-12-07 09:46:51.904884: step 2010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 86h:12m:09s remains)
INFO - root - 2017-12-07 09:47:01.843517: step 2020, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.975 sec/batch; 89h:31m:43s remains)
INFO - root - 2017-12-07 09:47:11.686897: step 2030, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 91h:25m:20s remains)
INFO - root - 2017-12-07 09:47:21.646340: step 2040, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 91h:39m:12s remains)
INFO - root - 2017-12-07 09:47:31.506048: step 2050, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 92h:53m:32s remains)
INFO - root - 2017-12-07 09:47:41.340269: step 2060, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 85h:37m:33s remains)
INFO - root - 2017-12-07 09:47:51.212585: step 2070, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 91h:05m:49s remains)
INFO - root - 2017-12-07 09:48:01.189015: step 2080, loss = 2.08, batch loss = 2.03 (7.7 examples/sec; 1.043 sec/batch; 95h:44m:08s remains)
INFO - root - 2017-12-07 09:48:10.971929: step 2090, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 90h:23m:58s remains)
INFO - root - 2017-12-07 09:48:20.678839: step 2100, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 89h:28m:03s remains)
2017-12-07 09:48:21.639340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1683145 -4.1664557 -4.1758347 -4.1988516 -4.2124434 -4.2144794 -4.222383 -4.2279339 -4.2357349 -4.2648644 -4.2955194 -4.3220634 -4.3302 -4.3227448 -4.3018951][-4.1708322 -4.1787596 -4.195086 -4.2189217 -4.2305546 -4.2371645 -4.2495174 -4.2630057 -4.2800326 -4.3114309 -4.333786 -4.3513722 -4.3557291 -4.3452578 -4.3217797][-4.1802096 -4.1853404 -4.2021713 -4.2238226 -4.23365 -4.2397919 -4.2526383 -4.268981 -4.2937694 -4.3280015 -4.3484993 -4.3628197 -4.3651175 -4.3506002 -4.3239069][-4.1966944 -4.1945624 -4.210989 -4.2312388 -4.2341409 -4.2288575 -4.2323413 -4.2435689 -4.2749934 -4.3171239 -4.3403354 -4.3562064 -4.3613052 -4.3472776 -4.3204074][-4.21137 -4.2065821 -4.2208505 -4.2355013 -4.2217431 -4.1923718 -4.1793919 -4.1879659 -4.2302442 -4.2830677 -4.3156352 -4.3392687 -4.3495626 -4.3382568 -4.3159766][-4.2301726 -4.2268419 -4.2352071 -4.2364459 -4.1994147 -4.1359754 -4.0984397 -4.105793 -4.16325 -4.231523 -4.2807279 -4.3182144 -4.3399734 -4.3381948 -4.3253269][-4.24722 -4.2525506 -4.2555537 -4.2428 -4.1897087 -4.0995092 -4.02714 -4.0205207 -4.0926633 -4.1803389 -4.2469144 -4.299335 -4.3314729 -4.3378043 -4.3311377][-4.2635813 -4.2766829 -4.2775922 -4.2595758 -4.2041011 -4.1096239 -4.0180831 -3.9913769 -4.0613914 -4.1550345 -4.22843 -4.290072 -4.3244815 -4.3280869 -4.3205547][-4.2638474 -4.2829514 -4.2835617 -4.26573 -4.2180223 -4.1335611 -4.0403776 -4.0047088 -4.0578732 -4.1452832 -4.2183313 -4.2836671 -4.3183608 -4.3177161 -4.3026156][-4.253026 -4.2734041 -4.2731013 -4.2580023 -4.22123 -4.1499786 -4.0655742 -4.0325046 -4.0741458 -4.1498332 -4.21704 -4.2791071 -4.3130331 -4.3095441 -4.2920208][-4.2538185 -4.2679687 -4.2627621 -4.250937 -4.2249975 -4.1669831 -4.0960183 -4.0709572 -4.1102438 -4.1745796 -4.2299228 -4.275795 -4.3012748 -4.3003864 -4.2874956][-4.2700987 -4.2732353 -4.2601552 -4.24829 -4.2288876 -4.187315 -4.139761 -4.1277876 -4.1642022 -4.216207 -4.2538919 -4.2769132 -4.2855492 -4.2831249 -4.2804146][-4.2971463 -4.2981997 -4.2811761 -4.2623453 -4.2425756 -4.2148633 -4.1880374 -4.18857 -4.2216935 -4.2585716 -4.2765055 -4.2699556 -4.2505832 -4.2399 -4.2446938][-4.3064985 -4.31417 -4.3039732 -4.2874513 -4.2714653 -4.254539 -4.2404418 -4.2470961 -4.2725182 -4.2943354 -4.2967925 -4.2663279 -4.2191029 -4.1940265 -4.203558][-4.2960696 -4.3072467 -4.3079596 -4.3051696 -4.2980237 -4.2895551 -4.2847996 -4.2948022 -4.3121853 -4.3198214 -4.3091855 -4.2653894 -4.2059894 -4.170568 -4.1782107]]...]
INFO - root - 2017-12-07 09:48:31.499744: step 2110, loss = 2.10, batch loss = 2.04 (7.6 examples/sec; 1.047 sec/batch; 96h:03m:22s remains)
INFO - root - 2017-12-07 09:48:41.459402: step 2120, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.028 sec/batch; 94h:18m:10s remains)
INFO - root - 2017-12-07 09:48:51.371743: step 2130, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 86h:49m:11s remains)
INFO - root - 2017-12-07 09:49:01.276852: step 2140, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 91h:09m:28s remains)
INFO - root - 2017-12-07 09:49:11.011951: step 2150, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 92h:08m:11s remains)
INFO - root - 2017-12-07 09:49:20.822144: step 2160, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 91h:09m:06s remains)
INFO - root - 2017-12-07 09:49:30.693984: step 2170, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 91h:01m:11s remains)
INFO - root - 2017-12-07 09:49:40.483847: step 2180, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 91h:36m:21s remains)
INFO - root - 2017-12-07 09:49:50.387149: step 2190, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 88h:15m:34s remains)
INFO - root - 2017-12-07 09:50:00.103585: step 2200, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 88h:16m:42s remains)
2017-12-07 09:50:01.005043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.216259 -4.2278013 -4.2314649 -4.2342038 -4.2354236 -4.237505 -4.2406397 -4.2445683 -4.2470174 -4.2461619 -4.2431579 -4.2416444 -4.2416663 -4.2403035 -4.2393689][-4.212811 -4.2243891 -4.2287793 -4.2326345 -4.2353482 -4.2388535 -4.243216 -4.2470455 -4.2482195 -4.2467079 -4.2442875 -4.24371 -4.2434077 -4.2406588 -4.2371812][-4.2451 -4.2569866 -4.2605848 -4.2617679 -4.261981 -4.2629814 -4.265707 -4.2676258 -4.26732 -4.2662497 -4.2663 -4.2679152 -4.2685037 -4.2652411 -4.2596579][-4.2806063 -4.2906408 -4.2900124 -4.2856874 -4.2821455 -4.2794919 -4.2784405 -4.2750154 -4.2709613 -4.2713819 -4.2764959 -4.281785 -4.2864661 -4.2885218 -4.2875967][-4.2904248 -4.2944112 -4.286417 -4.2748761 -4.2674875 -4.2619419 -4.2550864 -4.2416244 -4.230741 -4.235188 -4.2504635 -4.2624111 -4.2728543 -4.2826929 -4.2921853][-4.251369 -4.2474985 -4.2300167 -4.2091031 -4.1957235 -4.1830468 -4.164628 -4.1349983 -4.1170673 -4.1333523 -4.1658764 -4.1910558 -4.2117791 -4.2333837 -4.2574573][-4.1817703 -4.16698 -4.1404195 -4.11013 -4.0869584 -4.0603576 -4.025949 -3.9783792 -3.9567199 -3.9902325 -4.043622 -4.0872784 -4.1223984 -4.1570387 -4.1950407][-4.1532412 -4.1344042 -4.1104331 -4.0818357 -4.0543594 -4.0188341 -3.9757185 -3.9212332 -3.8978834 -3.9371295 -3.9955359 -4.042491 -4.0811887 -4.1174288 -4.160562][-4.18386 -4.1738544 -4.1654749 -4.1521277 -4.1342597 -4.1066065 -4.0749021 -4.036099 -4.0158143 -4.0380116 -4.0742264 -4.1038027 -4.1255703 -4.1481333 -4.1834111][-4.2413435 -4.2423844 -4.2463231 -4.2449741 -4.2366252 -4.218894 -4.199523 -4.1787882 -4.166358 -4.1737065 -4.1881628 -4.2001662 -4.206686 -4.2154841 -4.2363825][-4.2935381 -4.3020306 -4.3115826 -4.3141065 -4.3089542 -4.2973981 -4.2863021 -4.2778864 -4.2736597 -4.2768145 -4.2825885 -4.2880487 -4.2894411 -4.2904954 -4.2969522][-4.3167863 -4.3230224 -4.3283596 -4.3279414 -4.3235021 -4.3164616 -4.3105068 -4.307436 -4.3077731 -4.3117762 -4.3172245 -4.322865 -4.3253088 -4.3269615 -4.3292046][-4.3285108 -4.3306522 -4.3314075 -4.329936 -4.3270488 -4.3226929 -4.318315 -4.3158555 -4.3165965 -4.3215752 -4.3277764 -4.3329549 -4.3359995 -4.3385878 -4.3407016][-4.3447556 -4.3465667 -4.3464365 -4.3450265 -4.3430715 -4.3394394 -4.3349519 -4.3308539 -4.329843 -4.3330755 -4.3390112 -4.3419394 -4.342339 -4.3442335 -4.3479156][-4.3556457 -4.3580828 -4.3572216 -4.35567 -4.3527617 -4.3476963 -4.3412795 -4.3350873 -4.3331075 -4.335206 -4.33896 -4.339272 -4.3376732 -4.3391457 -4.3443289]]...]
INFO - root - 2017-12-07 09:50:10.932264: step 2210, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 89h:58m:06s remains)
INFO - root - 2017-12-07 09:50:20.735886: step 2220, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 92h:01m:51s remains)
INFO - root - 2017-12-07 09:50:30.723301: step 2230, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 91h:33m:16s remains)
INFO - root - 2017-12-07 09:50:40.577683: step 2240, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 87h:38m:44s remains)
INFO - root - 2017-12-07 09:50:50.466436: step 2250, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 90h:12m:32s remains)
INFO - root - 2017-12-07 09:51:00.175166: step 2260, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 88h:31m:51s remains)
INFO - root - 2017-12-07 09:51:10.083909: step 2270, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.044 sec/batch; 95h:47m:52s remains)
INFO - root - 2017-12-07 09:51:19.820889: step 2280, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 92h:48m:08s remains)
INFO - root - 2017-12-07 09:51:29.447789: step 2290, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 90h:31m:10s remains)
INFO - root - 2017-12-07 09:51:39.199225: step 2300, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 89h:22m:29s remains)
2017-12-07 09:51:40.149395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2853036 -4.2881904 -4.2830386 -4.2648773 -4.2420254 -4.2202229 -4.1961966 -4.1873021 -4.1825604 -4.1747746 -4.1488423 -4.1204424 -4.1028442 -4.1224337 -4.1656966][-4.2899661 -4.2853036 -4.2725835 -4.2471752 -4.2168908 -4.1845427 -4.1509538 -4.1366467 -4.1224918 -4.1066818 -4.0949063 -4.09987 -4.114924 -4.15083 -4.2002478][-4.2843065 -4.2634287 -4.2326465 -4.1988444 -4.1704926 -4.1390772 -4.107832 -4.0882826 -4.0590897 -4.038362 -4.0538464 -4.0916376 -4.1351247 -4.1860075 -4.2396741][-4.2674217 -4.235508 -4.1917152 -4.1559558 -4.1391239 -4.1177568 -4.0898385 -4.0671721 -4.0376887 -4.0250621 -4.05359 -4.1009536 -4.1544023 -4.2078409 -4.2616267][-4.25716 -4.2250967 -4.1806569 -4.1399918 -4.1196642 -4.1022849 -4.0730238 -4.0425906 -4.0302377 -4.0500665 -4.0884285 -4.134522 -4.1841908 -4.2314606 -4.2760015][-4.2503324 -4.2255354 -4.1884236 -4.1415477 -4.1050014 -4.0681148 -4.0178866 -3.9756484 -4.0007935 -4.067441 -4.124227 -4.1774149 -4.224781 -4.2628808 -4.291935][-4.2288623 -4.2181993 -4.1928267 -4.1424708 -4.0809479 -4.0105858 -3.9243734 -3.8657889 -3.9393883 -4.0597353 -4.1398654 -4.2028685 -4.2525172 -4.2857542 -4.3047404][-4.2045975 -4.1940355 -4.1746879 -4.1253548 -4.0537682 -3.9740987 -3.8877873 -3.8508723 -3.9507985 -4.080287 -4.161849 -4.22306 -4.2683425 -4.2971869 -4.3092837][-4.1638155 -4.1555104 -4.1482425 -4.12295 -4.0808225 -4.0384517 -4.0016317 -3.9998748 -4.0691013 -4.1508508 -4.2043438 -4.2478857 -4.2804112 -4.3008041 -4.3082061][-4.1222563 -4.13727 -4.1527061 -4.1496439 -4.1352825 -4.121789 -4.1139512 -4.1299706 -4.1773386 -4.2257733 -4.2563882 -4.2806244 -4.2981563 -4.306097 -4.3104224][-4.1258168 -4.1583586 -4.1790376 -4.1785631 -4.1705184 -4.1645365 -4.1664968 -4.1894464 -4.2312908 -4.2668524 -4.2892804 -4.3066998 -4.3148503 -4.3140006 -4.3162565][-4.1554031 -4.184691 -4.200428 -4.1974268 -4.1937642 -4.1902413 -4.1922064 -4.2151885 -4.2513289 -4.2788715 -4.2991209 -4.3169694 -4.325119 -4.3250122 -4.3270259][-4.1881633 -4.2078381 -4.2174478 -4.2172728 -4.2167306 -4.2108145 -4.2078867 -4.2263613 -4.2553873 -4.2789922 -4.3005586 -4.3201027 -4.3293905 -4.3316736 -4.3333235][-4.2020149 -4.2170658 -4.2227259 -4.2246809 -4.2298384 -4.2296963 -4.224966 -4.2367826 -4.2632861 -4.2862582 -4.30863 -4.3271255 -4.3339577 -4.3350782 -4.3353591][-4.2047606 -4.2227273 -4.228786 -4.2274041 -4.2330728 -4.2455397 -4.2522955 -4.2647166 -4.2873039 -4.3058023 -4.3230696 -4.3347821 -4.3368559 -4.336504 -4.33656]]...]
INFO - root - 2017-12-07 09:51:49.892946: step 2310, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.024 sec/batch; 93h:54m:25s remains)
INFO - root - 2017-12-07 09:51:59.820029: step 2320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 88h:50m:11s remains)
INFO - root - 2017-12-07 09:52:09.773102: step 2330, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 91h:22m:18s remains)
INFO - root - 2017-12-07 09:52:19.647816: step 2340, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 90h:17m:53s remains)
INFO - root - 2017-12-07 09:52:29.392353: step 2350, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 85h:43m:09s remains)
INFO - root - 2017-12-07 09:52:39.159077: step 2360, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.021 sec/batch; 93h:36m:20s remains)
INFO - root - 2017-12-07 09:52:48.898675: step 2370, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 90h:45m:30s remains)
INFO - root - 2017-12-07 09:52:58.829081: step 2380, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.018 sec/batch; 93h:19m:34s remains)
INFO - root - 2017-12-07 09:53:08.538089: step 2390, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.019 sec/batch; 93h:23m:52s remains)
INFO - root - 2017-12-07 09:53:18.364206: step 2400, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 89h:21m:17s remains)
2017-12-07 09:53:19.377379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1220651 -4.1427107 -4.1880856 -4.2211957 -4.2349992 -4.2413492 -4.2431226 -4.2398491 -4.2405314 -4.2472873 -4.2527494 -4.2566266 -4.2487507 -4.2396731 -4.2466917][-4.1062865 -4.1258531 -4.1717873 -4.2090559 -4.2313781 -4.2472124 -4.2534223 -4.2504373 -4.2490044 -4.2514606 -4.2546368 -4.2572312 -4.2582464 -4.258666 -4.2701278][-4.1267352 -4.139535 -4.179728 -4.2181268 -4.2437239 -4.2614169 -4.2653937 -4.261796 -4.2579603 -4.2573128 -4.2570629 -4.2566528 -4.2627068 -4.2666235 -4.2767854][-4.1543531 -4.1659913 -4.2029138 -4.2379751 -4.2598877 -4.2696786 -4.2641268 -4.2542405 -4.246861 -4.2478848 -4.2500968 -4.2540565 -4.2630334 -4.2640753 -4.2654877][-4.1751657 -4.1889539 -4.2214332 -4.24696 -4.2566285 -4.2487373 -4.2255635 -4.2042837 -4.1951 -4.2054806 -4.2249355 -4.2473354 -4.2650452 -4.2633076 -4.2548318][-4.1765676 -4.191874 -4.2200589 -4.2354078 -4.2267365 -4.1900115 -4.1381879 -4.0984035 -4.0935793 -4.1298037 -4.1806927 -4.22781 -4.2607403 -4.265625 -4.2526574][-4.160037 -4.1749363 -4.2009897 -4.2058158 -4.1737895 -4.1032739 -4.0168624 -3.9567022 -3.9644487 -4.0381913 -4.1285148 -4.20211 -4.2509441 -4.2682815 -4.2560248][-4.1519995 -4.1674728 -4.1901288 -4.1848173 -4.1311731 -4.0364771 -3.9294305 -3.861568 -3.8857391 -3.9817309 -4.0924973 -4.1797738 -4.240416 -4.2679391 -4.2597446][-4.1628428 -4.1794949 -4.1990061 -4.1924496 -4.1392918 -4.0536866 -3.9672632 -3.9194896 -3.9413285 -4.0148015 -4.1054611 -4.1810985 -4.23933 -4.270824 -4.2667255][-4.1879058 -4.1989207 -4.2122355 -4.2092805 -4.1731305 -4.1199636 -4.0746856 -4.0522742 -4.0608454 -4.0954614 -4.1493187 -4.2004457 -4.2463527 -4.2761292 -4.2763515][-4.2212777 -4.2196474 -4.219697 -4.2181435 -4.2029119 -4.1824121 -4.1704893 -4.16419 -4.1624603 -4.1685667 -4.190496 -4.2176719 -4.2500691 -4.27374 -4.2773681][-4.2456121 -4.2351046 -4.2249513 -4.2245793 -4.2266917 -4.2296162 -4.2336583 -4.2298408 -4.2190037 -4.2063074 -4.2039042 -4.2135081 -4.2342329 -4.2501655 -4.256413][-4.2520056 -4.2421541 -4.2321134 -4.2334404 -4.2434974 -4.2564273 -4.2626014 -4.2507429 -4.2285094 -4.2022133 -4.1826549 -4.1781335 -4.1899652 -4.2018461 -4.2133861][-4.2400126 -4.2352252 -4.2321777 -4.2395496 -4.2530203 -4.2677445 -4.2706289 -4.2486978 -4.2148151 -4.1785507 -4.1456637 -4.1273208 -4.1290159 -4.1388063 -4.1580391][-4.2075505 -4.2078624 -4.2134933 -4.2261839 -4.2398677 -4.2519388 -4.2522192 -4.2276807 -4.1909518 -4.1552672 -4.1214085 -4.0985928 -4.09502 -4.105742 -4.1338382]]...]
INFO - root - 2017-12-07 09:53:29.291927: step 2410, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.023 sec/batch; 93h:45m:32s remains)
INFO - root - 2017-12-07 09:53:39.187128: step 2420, loss = 2.10, batch loss = 2.04 (7.7 examples/sec; 1.041 sec/batch; 95h:27m:48s remains)
INFO - root - 2017-12-07 09:53:48.989781: step 2430, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.015 sec/batch; 93h:01m:11s remains)
INFO - root - 2017-12-07 09:53:58.671269: step 2440, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 84h:53m:36s remains)
INFO - root - 2017-12-07 09:54:08.279638: step 2450, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 79h:37m:09s remains)
INFO - root - 2017-12-07 09:54:18.051745: step 2460, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 87h:37m:11s remains)
INFO - root - 2017-12-07 09:54:28.003661: step 2470, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 90h:02m:33s remains)
INFO - root - 2017-12-07 09:54:37.697539: step 2480, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 89h:57m:18s remains)
INFO - root - 2017-12-07 09:54:47.411360: step 2490, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 87h:34m:45s remains)
INFO - root - 2017-12-07 09:54:57.184609: step 2500, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 82h:55m:41s remains)
2017-12-07 09:54:58.124130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2967696 -4.2990165 -4.2959824 -4.2935019 -4.2905583 -4.2870836 -4.2872868 -4.2942572 -4.3050327 -4.3128285 -4.3129292 -4.3053589 -4.2987461 -4.2974811 -4.2998319][-4.2928352 -4.29541 -4.2915473 -4.2844186 -4.2760458 -4.2678103 -4.2617311 -4.2672796 -4.2844052 -4.2989955 -4.3016224 -4.2936954 -4.2846551 -4.2783632 -4.2759953][-4.2818842 -4.2841406 -4.2777 -4.2617512 -4.2465954 -4.2339053 -4.2228384 -4.2310205 -4.2577176 -4.2799711 -4.288372 -4.283061 -4.2739973 -4.2631421 -4.2561636][-4.2619605 -4.2584825 -4.2470179 -4.2235069 -4.2056251 -4.1901846 -4.17834 -4.1890011 -4.2239394 -4.2507195 -4.2643685 -4.263495 -4.2598457 -4.2476735 -4.238009][-4.2463112 -4.2367744 -4.2192416 -4.1931767 -4.1747851 -4.1565208 -4.1408095 -4.1483746 -4.1899948 -4.2205119 -4.2383146 -4.2419438 -4.2427626 -4.2325063 -4.2231212][-4.230722 -4.2185855 -4.1940289 -4.1625896 -4.1378875 -4.11323 -4.0869541 -4.0854731 -4.1311264 -4.1735563 -4.1995735 -4.2068052 -4.2153468 -4.2168961 -4.2161112][-4.2180057 -4.2072506 -4.1811433 -4.1406875 -4.1002126 -4.0590167 -4.012763 -3.985745 -4.0293555 -4.0968218 -4.1460028 -4.1665349 -4.1868463 -4.200191 -4.2097611][-4.2168641 -4.2031097 -4.1790333 -4.1295061 -4.0634041 -3.9996459 -3.9306009 -3.8714194 -3.910799 -4.0054116 -4.0782418 -4.1139417 -4.146781 -4.1698551 -4.1854324][-4.2236085 -4.2057967 -4.1811171 -4.1240954 -4.0434 -3.978606 -3.9174998 -3.8600774 -3.89851 -3.9881968 -4.0484686 -4.0758429 -4.1084003 -4.1355238 -4.1508675][-4.2270408 -4.2132316 -4.1919737 -4.1396456 -4.0634847 -4.0047131 -3.9578922 -3.9146891 -3.94456 -4.0078096 -4.045599 -4.0611634 -4.0839195 -4.103981 -4.1177564][-4.2133579 -4.2027712 -4.19149 -4.15714 -4.1013827 -4.05625 -4.0176115 -3.9858613 -3.9991167 -4.0341883 -4.0509682 -4.0605631 -4.0783443 -4.0945306 -4.10912][-4.1990194 -4.1889648 -4.1876488 -4.1744652 -4.1452527 -4.1171656 -4.0871606 -4.0655227 -4.0676041 -4.0808716 -4.0779786 -4.077631 -4.0944633 -4.1123962 -4.1278129][-4.2068048 -4.2009487 -4.2046661 -4.2007604 -4.18474 -4.1686864 -4.148737 -4.1324663 -4.1325097 -4.1371031 -4.1249275 -4.1170359 -4.1359377 -4.1570582 -4.1736865][-4.2283139 -4.2283139 -4.2368879 -4.2390409 -4.2278843 -4.2174883 -4.2070336 -4.1972566 -4.1983609 -4.1993518 -4.1871233 -4.1813393 -4.1966066 -4.2135868 -4.2273889][-4.259686 -4.2633095 -4.2729688 -4.2787309 -4.2733555 -4.2697134 -4.2655931 -4.2612529 -4.2653747 -4.2677083 -4.2597542 -4.2559557 -4.2650852 -4.2747817 -4.2828088]]...]
INFO - root - 2017-12-07 09:55:08.065983: step 2510, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 92h:17m:58s remains)
INFO - root - 2017-12-07 09:55:17.944686: step 2520, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 88h:23m:32s remains)
INFO - root - 2017-12-07 09:55:27.760505: step 2530, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.011 sec/batch; 92h:42m:37s remains)
INFO - root - 2017-12-07 09:55:37.589176: step 2540, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 89h:05m:23s remains)
INFO - root - 2017-12-07 09:55:47.528841: step 2550, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 89h:43m:44s remains)
INFO - root - 2017-12-07 09:55:57.176062: step 2560, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 90h:00m:33s remains)
INFO - root - 2017-12-07 09:56:07.054934: step 2570, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 88h:24m:27s remains)
INFO - root - 2017-12-07 09:56:16.821372: step 2580, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 91h:54m:03s remains)
INFO - root - 2017-12-07 09:56:26.735415: step 2590, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.041 sec/batch; 95h:24m:13s remains)
INFO - root - 2017-12-07 09:56:36.340206: step 2600, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 78h:54m:38s remains)
2017-12-07 09:56:37.248286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2059231 -4.2086844 -4.2140026 -4.2175093 -4.2211561 -4.223094 -4.2228231 -4.214931 -4.1970577 -4.1746697 -4.1545291 -4.1348977 -4.1341619 -4.1576104 -4.1873226][-4.1911483 -4.1915307 -4.2015319 -4.211482 -4.2222638 -4.2302461 -4.2347946 -4.2300735 -4.2071857 -4.1727796 -4.1394691 -4.1148739 -4.1126533 -4.129674 -4.1584878][-4.1861162 -4.1906271 -4.2053123 -4.2212534 -4.2338576 -4.2385054 -4.2367105 -4.22908 -4.2060504 -4.1671214 -4.1294971 -4.1017346 -4.09005 -4.0919404 -4.1122732][-4.1919556 -4.1990557 -4.2142739 -4.2314172 -4.2400408 -4.2361078 -4.2270422 -4.2146616 -4.1950674 -4.1632476 -4.129777 -4.0985651 -4.0746741 -4.0605273 -4.0693913][-4.1988544 -4.2070174 -4.2217159 -4.2368636 -4.2377634 -4.2243695 -4.2078586 -4.1878729 -4.1700292 -4.1498666 -4.1282358 -4.097568 -4.0662808 -4.0405607 -4.0405326][-4.2007093 -4.2076035 -4.2210112 -4.2332478 -4.228056 -4.2077508 -4.1793604 -4.1480222 -4.1277423 -4.1200442 -4.1122541 -4.0887928 -4.0622797 -4.0390205 -4.0415592][-4.1877661 -4.1987791 -4.2133942 -4.2241607 -4.2168946 -4.1918097 -4.1527686 -4.1109457 -4.0902958 -4.0916576 -4.09611 -4.0855241 -4.0706515 -4.0561218 -4.0622826][-4.1760688 -4.1972752 -4.2165694 -4.2264843 -4.2185493 -4.1897511 -4.1467624 -4.104733 -4.0883307 -4.0906873 -4.0985003 -4.1008067 -4.0974736 -4.0880542 -4.091435][-4.1726789 -4.2049055 -4.2273755 -4.2356648 -4.2284522 -4.199914 -4.1579866 -4.1211252 -4.1088662 -4.112669 -4.1237068 -4.1353817 -4.139864 -4.1322713 -4.1330876][-4.1754894 -4.2138586 -4.2362537 -4.2426314 -4.237319 -4.2127409 -4.1759095 -4.1467395 -4.1416216 -4.153214 -4.1656051 -4.1736894 -4.1737909 -4.1675014 -4.1689839][-4.1870484 -4.22291 -4.2443576 -4.2520895 -4.2488656 -4.2268958 -4.195982 -4.1759253 -4.1800461 -4.1956615 -4.2026205 -4.200233 -4.1917753 -4.1871095 -4.1947947][-4.2015405 -4.2337646 -4.2546258 -4.2648506 -4.2640681 -4.2463956 -4.22172 -4.2087588 -4.2174363 -4.2324414 -4.2359023 -4.2269087 -4.2138133 -4.209837 -4.2194176][-4.2176938 -4.2422695 -4.2628121 -4.2761097 -4.2795339 -4.2702293 -4.254828 -4.2483091 -4.2551112 -4.2642756 -4.2636542 -4.2523909 -4.2401733 -4.2383804 -4.2465525][-4.2475438 -4.2598 -4.2750587 -4.2883458 -4.2957773 -4.2927165 -4.2848282 -4.2818308 -4.28616 -4.2910171 -4.2889786 -4.2800517 -4.2722397 -4.2733979 -4.2790661][-4.2823296 -4.2843332 -4.29144 -4.2992954 -4.3049316 -4.3047328 -4.3013649 -4.3010135 -4.3048573 -4.3095098 -4.3096457 -4.3053718 -4.3007455 -4.3010139 -4.303659]]...]
INFO - root - 2017-12-07 09:56:47.140271: step 2610, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 89h:02m:50s remains)
INFO - root - 2017-12-07 09:56:56.935310: step 2620, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 91h:39m:08s remains)
INFO - root - 2017-12-07 09:57:06.764335: step 2630, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 86h:21m:59s remains)
INFO - root - 2017-12-07 09:57:16.400213: step 2640, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 86h:42m:41s remains)
INFO - root - 2017-12-07 09:57:26.315939: step 2650, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 90h:09m:39s remains)
INFO - root - 2017-12-07 09:57:35.999166: step 2660, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 91h:48m:48s remains)
INFO - root - 2017-12-07 09:57:45.643591: step 2670, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.971 sec/batch; 88h:59m:35s remains)
INFO - root - 2017-12-07 09:57:55.453004: step 2680, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.994 sec/batch; 91h:04m:59s remains)
INFO - root - 2017-12-07 09:58:05.198135: step 2690, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 90h:36m:17s remains)
INFO - root - 2017-12-07 09:58:15.099747: step 2700, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 91h:06m:52s remains)
2017-12-07 09:58:16.040253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3074303 -4.3057523 -4.3045611 -4.3039317 -4.3038735 -4.3018489 -4.2964177 -4.2892928 -4.2844229 -4.2820358 -4.2814097 -4.2820349 -4.2812676 -4.2745838 -4.2618484][-4.3352404 -4.3337364 -4.3336687 -4.3345952 -4.3353415 -4.3337216 -4.327673 -4.3195386 -4.3150535 -4.312727 -4.311852 -4.3126183 -4.3138714 -4.3109636 -4.3039131][-4.3509445 -4.3519683 -4.3531146 -4.3545237 -4.3529344 -4.3465805 -4.3346329 -4.3203039 -4.3118167 -4.308517 -4.3100395 -4.3151712 -4.3229542 -4.3257127 -4.3233624][-4.3579721 -4.3591356 -4.3588905 -4.3570275 -4.34833 -4.3311653 -4.3079081 -4.2864251 -4.275363 -4.2735395 -4.281826 -4.296102 -4.3115606 -4.3196049 -4.3236027][-4.3505521 -4.3486495 -4.3433213 -4.3335371 -4.3123674 -4.2771721 -4.2348347 -4.199038 -4.1833758 -4.1884408 -4.212141 -4.2444448 -4.2751937 -4.2938118 -4.3072581][-4.3291674 -4.3237386 -4.3112345 -4.2890172 -4.25173 -4.1941586 -4.121285 -4.0578184 -4.03608 -4.0595131 -4.1111946 -4.1666465 -4.2144756 -4.2450032 -4.269166][-4.290617 -4.2833967 -4.2637725 -4.2271261 -4.1711016 -4.0879836 -3.9802516 -3.8782682 -3.8516927 -3.9141736 -4.0083623 -4.0902214 -4.1496143 -4.1872792 -4.2189713][-4.23493 -4.2268329 -4.1983309 -4.14782 -4.0766172 -3.9756968 -3.8433223 -3.7066574 -3.6807783 -3.7989073 -3.9402609 -4.0419989 -4.1030107 -4.1384177 -4.1717863][-4.1786175 -4.1700053 -4.1405411 -4.08893 -4.0158825 -3.9207954 -3.8066418 -3.6961007 -3.6865766 -3.802376 -3.9329002 -4.0245762 -4.0785108 -4.1066213 -4.1335506][-4.1583996 -4.1525073 -4.1320014 -4.0915704 -4.0390568 -3.9820926 -3.9240837 -3.8687408 -3.8602965 -3.9187162 -3.9914989 -4.0515809 -4.09423 -4.1157489 -4.132741][-4.1875372 -4.1842895 -4.1772223 -4.1592007 -4.1356192 -4.1145797 -4.0946126 -4.0690551 -4.0539713 -4.0668087 -4.0955505 -4.1288624 -4.156817 -4.1712446 -4.1783829][-4.2435327 -4.2382388 -4.2361593 -4.2336712 -4.2321858 -4.2305784 -4.2253304 -4.2121115 -4.19794 -4.1966853 -4.2080441 -4.2248826 -4.2395258 -4.247366 -4.2474251][-4.2679944 -4.2609487 -4.2608795 -4.2687244 -4.2817216 -4.2914333 -4.2939463 -4.2887459 -4.2792082 -4.2761655 -4.2819381 -4.2921085 -4.30122 -4.3073 -4.3070078][-4.25248 -4.2408142 -4.2422819 -4.2588859 -4.2818718 -4.2999506 -4.3102245 -4.3132319 -4.3087769 -4.3038774 -4.3043394 -4.3129878 -4.3255482 -4.3361959 -4.3389206][-4.2293272 -4.2084322 -4.2045341 -4.2214236 -4.2500172 -4.2766118 -4.2966814 -4.30629 -4.3023334 -4.2932677 -4.2897038 -4.29852 -4.3172936 -4.3360238 -4.3445444]]...]
INFO - root - 2017-12-07 09:58:25.834672: step 2710, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.933 sec/batch; 85h:26m:40s remains)
INFO - root - 2017-12-07 09:58:35.707970: step 2720, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 91h:16m:19s remains)
INFO - root - 2017-12-07 09:58:45.473463: step 2730, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.030 sec/batch; 94h:21m:59s remains)
INFO - root - 2017-12-07 09:58:55.263994: step 2740, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.984 sec/batch; 90h:08m:02s remains)
INFO - root - 2017-12-07 09:59:05.011614: step 2750, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 87h:37m:07s remains)
INFO - root - 2017-12-07 09:59:14.741177: step 2760, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 85h:38m:46s remains)
INFO - root - 2017-12-07 09:59:24.406255: step 2770, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 91h:10m:30s remains)
INFO - root - 2017-12-07 09:59:34.087462: step 2780, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 89h:39m:15s remains)
INFO - root - 2017-12-07 09:59:43.909510: step 2790, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.990 sec/batch; 90h:37m:39s remains)
INFO - root - 2017-12-07 09:59:53.636330: step 2800, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 83h:36m:57s remains)
2017-12-07 09:59:54.625790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1502371 -4.1463928 -4.1456423 -4.1541262 -4.1685996 -4.1637864 -4.1224136 -4.0720377 -4.0542927 -4.0938148 -4.165288 -4.2313623 -4.2769508 -4.3122411 -4.3291578][-4.1571455 -4.1363568 -4.1229887 -4.1246681 -4.1408386 -4.1444044 -4.1180911 -4.0769439 -4.0599065 -4.1011734 -4.1710539 -4.23076 -4.2735138 -4.3061771 -4.3216739][-4.1905594 -4.156517 -4.1272054 -4.1121879 -4.116559 -4.1234164 -4.1140471 -4.0879178 -4.0751233 -4.1220117 -4.191309 -4.24147 -4.2788606 -4.3097777 -4.3242226][-4.2248445 -4.1841941 -4.1442132 -4.1149011 -4.1002874 -4.0943384 -4.0896931 -4.0716329 -4.0638237 -4.1204815 -4.1973314 -4.2458696 -4.2794247 -4.3118324 -4.3278332][-4.2514191 -4.2075844 -4.1559734 -4.1084285 -4.0692439 -4.0443573 -4.0328417 -4.0177145 -4.0161681 -4.0863185 -4.1783981 -4.2337346 -4.2686286 -4.3047304 -4.3253903][-4.259675 -4.2161942 -4.15628 -4.0847654 -4.0172205 -3.9850409 -3.9720919 -3.9552341 -3.9583189 -4.0445809 -4.1499009 -4.21687 -4.2563124 -4.2974048 -4.3230734][-4.2553654 -4.2149024 -4.1509924 -4.063519 -3.9713802 -3.9351115 -3.9259167 -3.9099612 -3.9215484 -4.0166454 -4.1282449 -4.2036815 -4.2494612 -4.293117 -4.3228769][-4.2489738 -4.210351 -4.1512341 -4.0728951 -3.9812598 -3.9465039 -3.9341326 -3.9182711 -3.9292126 -4.0200729 -4.1281605 -4.2056413 -4.2533522 -4.2957206 -4.3245621][-4.2360477 -4.1990256 -4.1559734 -4.1164351 -4.0704169 -4.0459576 -4.0210123 -3.9899874 -3.9842446 -4.0494714 -4.1425648 -4.2150989 -4.2591405 -4.2974749 -4.3233528][-4.2128735 -4.1762161 -4.1473832 -4.1399407 -4.1338396 -4.126359 -4.1040668 -4.0670228 -4.0485144 -4.09247 -4.1681681 -4.2303443 -4.2670889 -4.2994833 -4.3225293][-4.18379 -4.1461873 -4.1274328 -4.1335869 -4.1419744 -4.1452341 -4.1294413 -4.10211 -4.09131 -4.129128 -4.1920133 -4.2439141 -4.2746959 -4.3048744 -4.3256197][-4.1672788 -4.1333251 -4.1246114 -4.1336746 -4.1399975 -4.1408348 -4.127367 -4.112886 -4.1149158 -4.1485744 -4.2015438 -4.2474852 -4.2778378 -4.3092074 -4.3305569][-4.1734228 -4.148139 -4.1487789 -4.1608825 -4.161129 -4.1527362 -4.136199 -4.1264358 -4.1358776 -4.1658173 -4.21164 -4.2504716 -4.2770553 -4.3101063 -4.3336749][-4.1838617 -4.1564293 -4.1590462 -4.1728711 -4.17345 -4.165463 -4.1487942 -4.142077 -4.1547341 -4.18302 -4.2222323 -4.2531009 -4.2759619 -4.3098006 -4.3350253][-4.1798134 -4.1509352 -4.1500263 -4.1634703 -4.1744795 -4.1801319 -4.1694074 -4.1649785 -4.1791153 -4.2027078 -4.2340407 -4.2604551 -4.2798924 -4.3113832 -4.3358669]]...]
INFO - root - 2017-12-07 10:00:04.475235: step 2810, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 91h:37m:59s remains)
INFO - root - 2017-12-07 10:00:14.188251: step 2820, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 89h:35m:14s remains)
INFO - root - 2017-12-07 10:00:24.097979: step 2830, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 91h:08m:58s remains)
INFO - root - 2017-12-07 10:00:33.958199: step 2840, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 91h:32m:43s remains)
INFO - root - 2017-12-07 10:00:43.689314: step 2850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 87h:56m:46s remains)
INFO - root - 2017-12-07 10:00:53.225529: step 2860, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 83h:03m:13s remains)
INFO - root - 2017-12-07 10:01:02.907278: step 2870, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 89h:45m:48s remains)
INFO - root - 2017-12-07 10:01:12.781200: step 2880, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 87h:09m:04s remains)
INFO - root - 2017-12-07 10:01:22.490952: step 2890, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.019 sec/batch; 93h:16m:12s remains)
INFO - root - 2017-12-07 10:01:32.422672: step 2900, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 88h:18m:34s remains)
2017-12-07 10:01:33.411323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3227005 -4.3243742 -4.3269677 -4.3293991 -4.3289833 -4.324604 -4.3174143 -4.3138127 -4.3180304 -4.3265862 -4.334631 -4.3382125 -4.3372784 -4.333261 -4.3282208][-4.3263121 -4.3281631 -4.3301349 -4.3313742 -4.3298888 -4.3256073 -4.3216462 -4.3224359 -4.3291497 -4.3386931 -4.3457227 -4.3467069 -4.3434663 -4.3375168 -4.3307514][-4.329874 -4.3303595 -4.3290186 -4.3235369 -4.3127975 -4.29876 -4.2884583 -4.287519 -4.2989125 -4.3177452 -4.3343434 -4.3426695 -4.3446045 -4.3412237 -4.3343163][-4.330586 -4.3289495 -4.3218646 -4.303977 -4.2752495 -4.2419634 -4.2166338 -4.2100568 -4.230124 -4.2668023 -4.3017926 -4.3270617 -4.3412395 -4.3444262 -4.338522][-4.3275042 -4.3222961 -4.306428 -4.2702074 -4.21378 -4.1488214 -4.09812 -4.0825596 -4.1181335 -4.183136 -4.2469478 -4.2982306 -4.3300905 -4.3426671 -4.3389745][-4.3221688 -4.3120165 -4.2846746 -4.2245688 -4.1320143 -4.0226307 -3.9326937 -3.9025078 -3.9626317 -4.07079 -4.1729836 -4.2550721 -4.3079481 -4.3324075 -4.3335724][-4.3185544 -4.3033752 -4.2651653 -4.1835623 -4.05626 -3.9010916 -3.7673366 -3.720295 -3.8106358 -3.9662342 -4.1074595 -4.2164574 -4.2862768 -4.3200374 -4.326107][-4.3197713 -4.3036189 -4.262888 -4.1758466 -4.0369964 -3.8657007 -3.7162094 -3.662601 -3.7639563 -3.9327674 -4.0847621 -4.1998768 -4.2748389 -4.311944 -4.3211122][-4.3245888 -4.3126802 -4.2816191 -4.2120252 -4.1008573 -3.9692206 -3.8605161 -3.8243709 -3.8926687 -4.011497 -4.1261506 -4.2192554 -4.2836752 -4.3146071 -4.3214288][-4.32277 -4.31565 -4.3008585 -4.2643814 -4.2027907 -4.1296968 -4.0703325 -4.0493274 -4.0810127 -4.1418352 -4.20787 -4.2672367 -4.3103137 -4.3285451 -4.327949][-4.2951827 -4.2861428 -4.2858138 -4.2846775 -4.2710104 -4.248404 -4.2275667 -4.2200384 -4.2308712 -4.2527471 -4.2814817 -4.312058 -4.3354483 -4.3424692 -4.3360305][-4.2339191 -4.2154479 -4.225955 -4.2567511 -4.2842546 -4.3016124 -4.3094926 -4.3131285 -4.3151541 -4.3167634 -4.3240929 -4.33774 -4.3489895 -4.3492866 -4.3406138][-4.1253934 -4.0873919 -4.1081204 -4.1739783 -4.2431555 -4.2985892 -4.3320413 -4.3482423 -4.3500681 -4.3436294 -4.3403745 -4.3450885 -4.3499732 -4.3476887 -4.3395386][-3.9704242 -3.8986104 -3.9293303 -4.0398431 -4.1556053 -4.2481084 -4.3081565 -4.3404951 -4.3492846 -4.3437939 -4.3383689 -4.3405714 -4.3435469 -4.3409553 -4.3349113][-3.8231723 -3.7161317 -3.7562156 -3.9104607 -4.0677233 -4.1899939 -4.2711196 -4.3167887 -4.333005 -4.3318233 -4.3278084 -4.3303971 -4.3334432 -4.3317175 -4.3278737]]...]
INFO - root - 2017-12-07 10:01:42.975574: step 2910, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.026 sec/batch; 93h:55m:33s remains)
INFO - root - 2017-12-07 10:01:52.826100: step 2920, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.970 sec/batch; 88h:45m:33s remains)
INFO - root - 2017-12-07 10:02:02.503748: step 2930, loss = 2.12, batch loss = 2.06 (7.8 examples/sec; 1.020 sec/batch; 93h:22m:01s remains)
INFO - root - 2017-12-07 10:02:12.392920: step 2940, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 85h:56m:47s remains)
INFO - root - 2017-12-07 10:02:22.201691: step 2950, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.929 sec/batch; 85h:00m:27s remains)
INFO - root - 2017-12-07 10:02:31.920134: step 2960, loss = 2.11, batch loss = 2.05 (7.8 examples/sec; 1.020 sec/batch; 93h:19m:34s remains)
INFO - root - 2017-12-07 10:02:41.691298: step 2970, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 84h:46m:14s remains)
INFO - root - 2017-12-07 10:02:51.704491: step 2980, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.043 sec/batch; 95h:30m:16s remains)
INFO - root - 2017-12-07 10:03:01.515736: step 2990, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 90h:16m:25s remains)
INFO - root - 2017-12-07 10:03:11.411656: step 3000, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 85h:12m:17s remains)
2017-12-07 10:03:12.382638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3443766 -4.3441973 -4.3397202 -4.3291435 -4.3065934 -4.2682037 -4.2148681 -4.1526084 -4.0855622 -4.0064082 -3.9187324 -3.8507996 -3.8514705 -3.9118998 -3.9908552][-4.34829 -4.3503656 -4.3474865 -4.3385615 -4.3185143 -4.2832408 -4.2335362 -4.1769743 -4.11602 -4.038981 -3.9533043 -3.8895905 -3.8839216 -3.9285173 -3.9937763][-4.352839 -4.3570166 -4.3567262 -4.3509064 -4.335484 -4.3086038 -4.2723351 -4.2348771 -4.1968136 -4.1445956 -4.0810695 -4.0305223 -4.0161629 -4.0369225 -4.0784492][-4.3559551 -4.3611569 -4.3620324 -4.35646 -4.3410234 -4.3168273 -4.2909083 -4.2708597 -4.2592082 -4.2424645 -4.2133718 -4.18657 -4.1738644 -4.1780567 -4.1942015][-4.3547578 -4.3590717 -4.3578897 -4.3472924 -4.3238506 -4.2934594 -4.2668362 -4.2525816 -4.2582474 -4.2742596 -4.2820287 -4.2816658 -4.2790623 -4.2799916 -4.2830343][-4.3506122 -4.3513451 -4.342309 -4.3183627 -4.2769308 -4.2290297 -4.1897531 -4.1731791 -4.1919837 -4.2395363 -4.2871737 -4.3160672 -4.3285966 -4.3327279 -4.3313389][-4.346911 -4.3426809 -4.3215466 -4.2785106 -4.2115612 -4.133317 -4.0650954 -4.0319772 -4.0624895 -4.1419449 -4.2313929 -4.2948112 -4.3304105 -4.3454738 -4.3467655][-4.3454356 -4.3388438 -4.3098259 -4.2547784 -4.1696939 -4.0616217 -3.9540181 -3.887459 -3.9234505 -4.0324721 -4.1575356 -4.2519 -4.3090253 -4.3369422 -4.3456316][-4.3517475 -4.3454823 -4.3169174 -4.2638125 -4.1801414 -4.068274 -3.9538615 -3.8751016 -3.908355 -4.019865 -4.1473145 -4.24496 -4.3027062 -4.331183 -4.3419228][-4.3579817 -4.3524265 -4.3312078 -4.2951756 -4.2374148 -4.1592569 -4.0827613 -4.0320535 -4.0515289 -4.1228743 -4.2117271 -4.2812109 -4.3201275 -4.3385682 -4.3440828][-4.3384314 -4.3296885 -4.3156047 -4.2973781 -4.2710681 -4.2339916 -4.1965408 -4.1707 -4.1773429 -4.2128334 -4.2644615 -4.3067641 -4.3315582 -4.3445997 -4.3471212][-4.2883925 -4.2765374 -4.2696638 -4.2668982 -4.2651372 -4.2571268 -4.2432117 -4.2274041 -4.2243671 -4.2386146 -4.270134 -4.3023114 -4.3276019 -4.3446789 -4.3508992][-4.2254105 -4.211988 -4.2115769 -4.2206607 -4.2356219 -4.2461066 -4.2418 -4.2255621 -4.21309 -4.2154417 -4.2374673 -4.2685609 -4.30014 -4.3269057 -4.34309][-4.1654058 -4.1499224 -4.1490645 -4.1617045 -4.1848893 -4.2050853 -4.2050576 -4.1865835 -4.16556 -4.1590195 -4.1755877 -4.2077537 -4.2454429 -4.2827048 -4.3099108][-4.1352324 -4.1245956 -4.1228685 -4.1295824 -4.146596 -4.1632314 -4.1607304 -4.1410623 -4.1145697 -4.1015363 -4.114284 -4.1453133 -4.1831293 -4.2231679 -4.254292]]...]
INFO - root - 2017-12-07 10:03:22.016815: step 3010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 86h:15m:32s remains)
INFO - root - 2017-12-07 10:03:31.718171: step 3020, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 86h:22m:57s remains)
INFO - root - 2017-12-07 10:03:41.810894: step 3030, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 92h:09m:34s remains)
INFO - root - 2017-12-07 10:03:51.759080: step 3040, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 91h:33m:26s remains)
INFO - root - 2017-12-07 10:04:01.474240: step 3050, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 89h:17m:15s remains)
INFO - root - 2017-12-07 10:04:11.022961: step 3060, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 91h:09m:47s remains)
INFO - root - 2017-12-07 10:04:21.118836: step 3070, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 88h:54m:31s remains)
INFO - root - 2017-12-07 10:04:30.833964: step 3080, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.005 sec/batch; 92h:00m:00s remains)
INFO - root - 2017-12-07 10:04:40.772537: step 3090, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 87h:14m:26s remains)
INFO - root - 2017-12-07 10:04:50.499274: step 3100, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 85h:15m:17s remains)
2017-12-07 10:04:51.457150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3289528 -4.3279972 -4.3305607 -4.331275 -4.3290682 -4.3258457 -4.3231807 -4.321701 -4.3151431 -4.3099771 -4.3076429 -4.3045912 -4.3026419 -4.3009658 -4.2995462][-4.3498745 -4.3535638 -4.3559332 -4.3548379 -4.3495889 -4.3427467 -4.3353043 -4.3284206 -4.3152823 -4.3042588 -4.2987113 -4.2920365 -4.2878833 -4.2805905 -4.2742753][-4.3545432 -4.3629212 -4.3641553 -4.3588057 -4.3471107 -4.3337083 -4.3182354 -4.3050718 -4.2895985 -4.2811928 -4.2756052 -4.2654886 -4.2602916 -4.2471194 -4.2320991][-4.3268967 -4.340755 -4.3419814 -4.3324456 -4.3143716 -4.2938595 -4.268415 -4.2468781 -4.2360539 -4.237021 -4.2326517 -4.2154717 -4.20524 -4.1894493 -4.16746][-4.2841377 -4.3012123 -4.299974 -4.2841072 -4.2579641 -4.2263737 -4.1841593 -4.1495357 -4.1495571 -4.1654286 -4.1628685 -4.139708 -4.1281295 -4.1186337 -4.100543][-4.2499957 -4.2647824 -4.25432 -4.2247519 -4.1840372 -4.1303129 -4.0591273 -4.0034842 -4.022769 -4.0651889 -4.0742693 -4.0549221 -4.0560613 -4.0716906 -4.0739751][-4.2445288 -4.2501192 -4.2224584 -4.1714315 -4.1101718 -4.0285587 -3.91949 -3.8374648 -3.887429 -3.968168 -3.9990184 -4.0010662 -4.027698 -4.0756054 -4.1084771][-4.2635365 -4.2605467 -4.2204142 -4.1539154 -4.0773935 -3.9814436 -3.8541656 -3.7585547 -3.8316905 -3.938798 -3.9874036 -4.0130215 -4.0636058 -4.130671 -4.1830015][-4.2893205 -4.2862592 -4.2481041 -4.1852188 -4.1134014 -4.0295849 -3.9293909 -3.860374 -3.9167595 -4.0014968 -4.04521 -4.073875 -4.1252627 -4.1912184 -4.243279][-4.3162236 -4.3173771 -4.2893944 -4.2438855 -4.1906009 -4.1302638 -4.0673289 -4.0286832 -4.0578246 -4.1054764 -4.132081 -4.1522336 -4.1905346 -4.2420568 -4.284821][-4.3367939 -4.3408527 -4.3246903 -4.2975445 -4.2650971 -4.22782 -4.1928067 -4.1734705 -4.1859646 -4.2099862 -4.2243185 -4.2347178 -4.2551723 -4.28501 -4.314055][-4.3454189 -4.3501487 -4.3430419 -4.3293133 -4.312736 -4.2950435 -4.2791138 -4.2705226 -4.2764406 -4.287869 -4.2959857 -4.2984104 -4.3045187 -4.3170671 -4.3345332][-4.3430476 -4.3473043 -4.345407 -4.3400888 -4.3333116 -4.3275776 -4.323122 -4.3220377 -4.32684 -4.334064 -4.3388238 -4.3366756 -4.3351345 -4.3391814 -4.3508234][-4.3352032 -4.3400435 -4.3415394 -4.3421631 -4.3405957 -4.3396573 -4.3417358 -4.3450923 -4.3499651 -4.355186 -4.3578606 -4.3552127 -4.3521338 -4.3524265 -4.3602867][-4.3277988 -4.332058 -4.3349175 -4.3383555 -4.3391633 -4.3397012 -4.3440704 -4.3487792 -4.353138 -4.3568349 -4.358295 -4.3555746 -4.3526254 -4.3520746 -4.3579183]]...]
INFO - root - 2017-12-07 10:05:01.294670: step 3110, loss = 2.06, batch loss = 2.00 (7.6 examples/sec; 1.046 sec/batch; 95h:43m:23s remains)
INFO - root - 2017-12-07 10:05:11.165997: step 3120, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 91h:54m:10s remains)
INFO - root - 2017-12-07 10:05:20.911131: step 3130, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 85h:03m:11s remains)
INFO - root - 2017-12-07 10:05:30.880873: step 3140, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.026 sec/batch; 93h:53m:06s remains)
INFO - root - 2017-12-07 10:05:40.521285: step 3150, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 89h:22m:15s remains)
INFO - root - 2017-12-07 10:05:50.443256: step 3160, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.009 sec/batch; 92h:20m:59s remains)
INFO - root - 2017-12-07 10:06:00.337268: step 3170, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 86h:54m:25s remains)
INFO - root - 2017-12-07 10:06:10.139452: step 3180, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.983 sec/batch; 89h:54m:05s remains)
INFO - root - 2017-12-07 10:06:19.837036: step 3190, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.004 sec/batch; 91h:51m:32s remains)
INFO - root - 2017-12-07 10:06:29.788119: step 3200, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 90h:28m:03s remains)
2017-12-07 10:06:30.691671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2948346 -4.27629 -4.2680464 -4.2733679 -4.2876773 -4.2943659 -4.2921624 -4.2888374 -4.2858891 -4.2834883 -4.2837358 -4.284081 -4.2906222 -4.3002844 -4.3057585][-4.2666388 -4.2388067 -4.2253895 -4.22699 -4.2453556 -4.2555552 -4.2498827 -4.2451086 -4.2442331 -4.2450581 -4.2513084 -4.2590113 -4.2712507 -4.2860813 -4.2920775][-4.2375193 -4.2022066 -4.1867056 -4.1838551 -4.1988239 -4.2016878 -4.1861119 -4.1826177 -4.1870737 -4.1929288 -4.2057133 -4.2231688 -4.2430925 -4.2648563 -4.2755175][-4.2082009 -4.1668959 -4.1474352 -4.1355882 -4.1416759 -4.1349959 -4.1118226 -4.111105 -4.1187005 -4.127336 -4.1444287 -4.1734724 -4.2064886 -4.2404056 -4.258872][-4.189115 -4.1372037 -4.1071358 -4.0814829 -4.0737157 -4.0611291 -4.0398793 -4.0425386 -4.0441761 -4.0484762 -4.0658765 -4.1084466 -4.1647778 -4.2170825 -4.2479095][-4.170239 -4.1085539 -4.0669031 -4.03197 -4.0158896 -4.0004425 -3.9774468 -3.9690769 -3.9536068 -3.9429936 -3.9583173 -4.0206761 -4.1084485 -4.1877141 -4.2370462][-4.1301947 -4.0595093 -4.0075479 -3.9681854 -3.9528275 -3.9417064 -3.9176245 -3.9006763 -3.8777363 -3.8590217 -3.874788 -3.955287 -4.0669413 -4.1664548 -4.2293549][-4.1044416 -4.03856 -3.9885771 -3.9543722 -3.9403839 -3.9260604 -3.897285 -3.8842835 -3.8738701 -3.8662188 -3.8907447 -3.9727852 -4.0829992 -4.1787024 -4.2416353][-4.1303577 -4.0806565 -4.0439372 -4.0185885 -4.00484 -3.9845209 -3.9574218 -3.9542992 -3.9586973 -3.9663081 -3.9959724 -4.0628362 -4.149251 -4.2210493 -4.2702227][-4.179637 -4.1474028 -4.1266279 -4.11011 -4.0964751 -4.080008 -4.0633955 -4.0649805 -4.0761852 -4.0887127 -4.1142583 -4.1608977 -4.2206593 -4.2690907 -4.3022494][-4.2230425 -4.2049952 -4.1969161 -4.1910744 -4.1815739 -4.1710124 -4.16562 -4.1724129 -4.1836085 -4.1944408 -4.2122669 -4.2434654 -4.2820215 -4.3117366 -4.3314424][-4.2624655 -4.2562366 -4.2558427 -4.2549071 -4.2494044 -4.2440572 -4.2442904 -4.2528877 -4.2629213 -4.271822 -4.2845259 -4.3047853 -4.3254542 -4.3399911 -4.3485932][-4.2903309 -4.2915668 -4.294292 -4.2959776 -4.2945948 -4.2926731 -4.2927594 -4.2980213 -4.3058538 -4.3124127 -4.320909 -4.3349447 -4.3458018 -4.3513083 -4.3543029][-4.316421 -4.3196573 -4.3234444 -4.3257403 -4.3252716 -4.3252378 -4.3253956 -4.3281803 -4.3331933 -4.3376541 -4.3425488 -4.3500381 -4.3551173 -4.3565483 -4.3574734][-4.3423119 -4.343863 -4.343852 -4.3445868 -4.345202 -4.34558 -4.3452206 -4.3468623 -4.3490305 -4.3514333 -4.3545685 -4.3585844 -4.3611836 -4.3615603 -4.3614054]]...]
INFO - root - 2017-12-07 10:06:40.576643: step 3210, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.032 sec/batch; 94h:21m:29s remains)
INFO - root - 2017-12-07 10:06:50.443889: step 3220, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 87h:48m:33s remains)
INFO - root - 2017-12-07 10:07:00.193097: step 3230, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 90h:58m:24s remains)
INFO - root - 2017-12-07 10:07:10.162654: step 3240, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 88h:56m:12s remains)
INFO - root - 2017-12-07 10:07:19.883717: step 3250, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 89h:27m:18s remains)
INFO - root - 2017-12-07 10:07:29.835801: step 3260, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.007 sec/batch; 92h:04m:00s remains)
INFO - root - 2017-12-07 10:07:39.712952: step 3270, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 89h:22m:58s remains)
INFO - root - 2017-12-07 10:07:49.626151: step 3280, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 90h:49m:59s remains)
INFO - root - 2017-12-07 10:07:59.384152: step 3290, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 87h:34m:11s remains)
INFO - root - 2017-12-07 10:08:09.225579: step 3300, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.006 sec/batch; 92h:00m:32s remains)
2017-12-07 10:08:10.162867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2488065 -4.24882 -4.2653394 -4.288085 -4.3104711 -4.3277063 -4.3395805 -4.3402133 -4.329752 -4.3141713 -4.2970924 -4.2798734 -4.2705688 -4.2750087 -4.2851615][-4.2317219 -4.2438087 -4.2688656 -4.2934608 -4.3131156 -4.3275347 -4.3402667 -4.3464437 -4.3457785 -4.3407569 -4.3339272 -4.3256922 -4.3176589 -4.3091488 -4.2976675][-4.2180748 -4.23645 -4.2643404 -4.2873845 -4.3017006 -4.310915 -4.3199553 -4.3258729 -4.3307834 -4.3372226 -4.3441315 -4.3489141 -4.3472548 -4.3346419 -4.3105564][-4.2367687 -4.2506218 -4.2675757 -4.2776361 -4.2780037 -4.2741356 -4.2717838 -4.2705178 -4.2769451 -4.2930994 -4.3146172 -4.3358049 -4.3488736 -4.345252 -4.3246632][-4.2726965 -4.2752972 -4.2738004 -4.2634096 -4.2411122 -4.2143583 -4.1911793 -4.1762257 -4.1816945 -4.2083387 -4.2476177 -4.2902451 -4.3253331 -4.3393426 -4.3324623][-4.2970357 -4.2926865 -4.27625 -4.2450042 -4.197814 -4.1431785 -4.0909853 -4.0553784 -4.0593357 -4.1016216 -4.1621819 -4.2256103 -4.2822828 -4.3139997 -4.3201718][-4.3082042 -4.307713 -4.2865605 -4.2412896 -4.1720319 -4.0905347 -4.0066123 -3.945085 -3.948781 -4.01043 -4.0941038 -4.1747179 -4.2440453 -4.2850924 -4.29765][-4.3171616 -4.3248758 -4.308135 -4.2617 -4.1836696 -4.0877314 -3.9827912 -3.9004483 -3.9017453 -3.9749479 -4.0704417 -4.158319 -4.2288947 -4.2699056 -4.2835231][-4.328259 -4.3399053 -4.3297462 -4.2908468 -4.2214437 -4.1344461 -4.0388021 -3.9659615 -3.9663925 -4.0290384 -4.1106076 -4.1852717 -4.243186 -4.2768621 -4.289392][-4.3443832 -4.3534265 -4.3453856 -4.3139825 -4.2605553 -4.1978841 -4.1343374 -4.0922728 -4.0970931 -4.1373782 -4.1885786 -4.2364588 -4.2745819 -4.3005524 -4.3140011][-4.3532009 -4.35559 -4.3448281 -4.3190331 -4.283608 -4.248601 -4.2199492 -4.2083673 -4.2167 -4.2352738 -4.2563505 -4.2773056 -4.298502 -4.3199244 -4.3376937][-4.3466582 -4.3415728 -4.3275065 -4.3089557 -4.2917576 -4.2808781 -4.2787118 -4.2875209 -4.2973175 -4.299685 -4.2961631 -4.2927384 -4.2988343 -4.3159018 -4.3376827][-4.3313403 -4.32082 -4.3035789 -4.2919788 -4.2908182 -4.2991052 -4.3146143 -4.3341508 -4.3434205 -4.3358006 -4.314384 -4.2903414 -4.2809567 -4.2915263 -4.3158145][-4.3155789 -4.3062692 -4.290381 -4.2826295 -4.2894573 -4.30687 -4.3287678 -4.3493381 -4.3578629 -4.3483353 -4.3201571 -4.2844963 -4.2637472 -4.2667165 -4.2893338][-4.3012295 -4.2985435 -4.2906923 -4.2887931 -4.2988672 -4.3169169 -4.3349757 -4.349874 -4.3560686 -4.3485613 -4.3236356 -4.288744 -4.2645535 -4.2616253 -4.2805529]]...]
INFO - root - 2017-12-07 10:08:19.861952: step 3310, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 86h:19m:40s remains)
INFO - root - 2017-12-07 10:08:27.650085: step 3320, loss = 2.11, batch loss = 2.06 (10.7 examples/sec; 0.745 sec/batch; 68h:06m:51s remains)
INFO - root - 2017-12-07 10:08:34.989861: step 3330, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 0.780 sec/batch; 71h:18m:48s remains)
INFO - root - 2017-12-07 10:08:42.293409: step 3340, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 66h:19m:10s remains)
INFO - root - 2017-12-07 10:08:49.615984: step 3350, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.735 sec/batch; 67h:10m:27s remains)
INFO - root - 2017-12-07 10:08:56.973863: step 3360, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 64h:14m:22s remains)
INFO - root - 2017-12-07 10:09:04.404580: step 3370, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 68h:29m:31s remains)
INFO - root - 2017-12-07 10:09:11.756681: step 3380, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.753 sec/batch; 68h:49m:16s remains)
INFO - root - 2017-12-07 10:09:19.052378: step 3390, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 68h:44m:32s remains)
INFO - root - 2017-12-07 10:09:26.290357: step 3400, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 0.776 sec/batch; 70h:55m:12s remains)
2017-12-07 10:09:27.119785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2737055 -4.2999654 -4.3228631 -4.3339849 -4.3406816 -4.3368278 -4.322722 -4.3080683 -4.2996831 -4.29615 -4.2975731 -4.3006439 -4.2939124 -4.2689633 -4.2309256][-4.2672114 -4.2912292 -4.3110476 -4.3185272 -4.3221784 -4.3151612 -4.2965589 -4.2783179 -4.2671919 -4.2614055 -4.2649846 -4.2706537 -4.2620349 -4.2282505 -4.1718521][-4.2373629 -4.2576685 -4.2791829 -4.2892838 -4.2918158 -4.2784691 -4.2528472 -4.2296104 -4.2168207 -4.2141771 -4.2257543 -4.2400231 -4.2346969 -4.1998358 -4.1359878][-4.1887174 -4.203567 -4.2317328 -4.2490644 -4.2495747 -4.2271829 -4.1911187 -4.1608047 -4.1505475 -4.1586914 -4.188477 -4.2189894 -4.2229657 -4.1960268 -4.1380572][-4.1231084 -4.1355357 -4.1790347 -4.2072759 -4.2041821 -4.1709075 -4.1199245 -4.0802817 -4.0792389 -4.1099119 -4.1603451 -4.2038755 -4.21885 -4.2049036 -4.1688027][-4.0694222 -4.0858779 -4.1390295 -4.1690421 -4.1596026 -4.1122808 -4.0364542 -3.9846067 -4.0041533 -4.0672669 -4.1373773 -4.1891308 -4.2117505 -4.2123055 -4.2034817][-4.0758042 -4.0992446 -4.1454029 -4.1574736 -4.1269388 -4.0567346 -3.9496887 -3.8878825 -3.9371107 -4.0312095 -4.1135554 -4.166657 -4.1912374 -4.2030139 -4.2157063][-4.1291966 -4.1558566 -4.1794887 -4.1617441 -4.1065497 -4.0127039 -3.8850539 -3.824852 -3.9064775 -4.0215049 -4.1044173 -4.1546354 -4.1777973 -4.192 -4.2088795][-4.1817994 -4.2052193 -4.2113624 -4.1769614 -4.106153 -4.0045075 -3.8869853 -3.8498235 -3.9429436 -4.0504017 -4.1222229 -4.166441 -4.1887989 -4.198627 -4.2040038][-4.2139759 -4.2305918 -4.2313075 -4.1996465 -4.1331339 -4.0484776 -3.9741256 -3.9699581 -4.0445509 -4.1213984 -4.1724896 -4.2057204 -4.222795 -4.2239671 -4.2142472][-4.2536559 -4.2614121 -4.2589498 -4.2375736 -4.1888638 -4.1324916 -4.0975757 -4.1087112 -4.1543064 -4.1985731 -4.2319098 -4.2540269 -4.2603731 -4.2519608 -4.235208][-4.3006535 -4.2987156 -4.2870007 -4.2698584 -4.2406092 -4.2101851 -4.1974387 -4.2090154 -4.2351956 -4.2645607 -4.2886291 -4.298789 -4.29152 -4.2768326 -4.2593856][-4.3343434 -4.3266654 -4.3093505 -4.2944474 -4.2776895 -4.2633862 -4.2598305 -4.2688255 -4.2862639 -4.3095651 -4.3290215 -4.3320541 -4.3190589 -4.3012538 -4.2841444][-4.3383026 -4.3317432 -4.3177128 -4.3061194 -4.2966905 -4.2915707 -4.2957673 -4.3052888 -4.3194184 -4.3377504 -4.350606 -4.3483992 -4.3343391 -4.3171744 -4.3033652][-4.3231516 -4.320096 -4.3155208 -4.3114667 -4.3063955 -4.3037543 -4.3112321 -4.3227315 -4.333405 -4.3437748 -4.3479571 -4.3423734 -4.3295493 -4.3161778 -4.3078952]]...]
INFO - root - 2017-12-07 10:09:34.341409: step 3410, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 66h:58m:37s remains)
INFO - root - 2017-12-07 10:09:41.701595: step 3420, loss = 2.07, batch loss = 2.02 (10.6 examples/sec; 0.757 sec/batch; 69h:12m:35s remains)
INFO - root - 2017-12-07 10:09:49.209018: step 3430, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.731 sec/batch; 66h:51m:04s remains)
INFO - root - 2017-12-07 10:09:56.467518: step 3440, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.754 sec/batch; 68h:55m:59s remains)
INFO - root - 2017-12-07 10:10:03.861914: step 3450, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 0.794 sec/batch; 72h:34m:29s remains)
INFO - root - 2017-12-07 10:10:11.111063: step 3460, loss = 2.08, batch loss = 2.03 (10.7 examples/sec; 0.748 sec/batch; 68h:20m:33s remains)
INFO - root - 2017-12-07 10:10:18.422459: step 3470, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 68h:04m:45s remains)
INFO - root - 2017-12-07 10:10:25.744017: step 3480, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 67h:28m:47s remains)
INFO - root - 2017-12-07 10:10:32.982192: step 3490, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 62h:46m:43s remains)
INFO - root - 2017-12-07 10:10:40.183554: step 3500, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 64h:33m:52s remains)
2017-12-07 10:10:40.950480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2367134 -4.2387991 -4.2462316 -4.2609916 -4.2734032 -4.2754054 -4.273643 -4.2737603 -4.2668047 -4.2661219 -4.2674985 -4.2551603 -4.2320776 -4.2052774 -4.1805406][-4.2204223 -4.21654 -4.2177916 -4.2348514 -4.2523475 -4.2553749 -4.2484818 -4.2462816 -4.2428913 -4.2477031 -4.2516475 -4.235621 -4.2057333 -4.1731911 -4.1476297][-4.1800485 -4.1744528 -4.1775551 -4.1996431 -4.2210188 -4.2229071 -4.211 -4.2085814 -4.2117205 -4.2216187 -4.2289996 -4.2143669 -4.1807203 -4.1452579 -4.1210375][-4.1420612 -4.1385617 -4.1491084 -4.1746764 -4.1928587 -4.1877713 -4.1685157 -4.1686454 -4.1809149 -4.1962562 -4.20378 -4.189538 -4.1530337 -4.1119528 -4.0888996][-4.1408834 -4.1331353 -4.1465774 -4.1680984 -4.1783671 -4.1668067 -4.1443677 -4.1490703 -4.1673918 -4.1862068 -4.1936092 -4.1807981 -4.1436753 -4.099288 -4.0797029][-4.1686072 -4.1498613 -4.1526914 -4.1593876 -4.1579008 -4.1408758 -4.1213303 -4.1328478 -4.1609573 -4.1871538 -4.1978498 -4.1890717 -4.1581483 -4.1135979 -4.0893469][-4.1706996 -4.1481051 -4.1396651 -4.1315866 -4.1198387 -4.0999289 -4.0822034 -4.0945206 -4.1291766 -4.1640806 -4.1844792 -4.1872282 -4.1622381 -4.1147289 -4.0818915][-4.1602769 -4.1384535 -4.1257668 -4.1126814 -4.0943627 -4.0701332 -4.0508404 -4.0578961 -4.0874271 -4.11527 -4.1376395 -4.149518 -4.132164 -4.09111 -4.0616775][-4.1594152 -4.1443272 -4.1379957 -4.1302261 -4.117393 -4.0980086 -4.0811977 -4.0820866 -4.0959172 -4.1071029 -4.1186256 -4.1259866 -4.1136584 -4.0863886 -4.066359][-4.1694975 -4.1638794 -4.1706276 -4.17794 -4.1809664 -4.1733913 -4.1647305 -4.1629634 -4.1656876 -4.1655278 -4.1665177 -4.166131 -4.1542044 -4.1363907 -4.1249089][-4.1906471 -4.191689 -4.20475 -4.217927 -4.23051 -4.2333336 -4.2335305 -4.2355452 -4.2368045 -4.2352343 -4.2324533 -4.2267089 -4.2124958 -4.1992879 -4.1943049][-4.213305 -4.2174206 -4.2307038 -4.2440491 -4.2611871 -4.2717104 -4.2799182 -4.2878604 -4.2929773 -4.2931781 -4.2901711 -4.282383 -4.2680616 -4.25576 -4.2509575][-4.2326803 -4.2358904 -4.2495546 -4.2646241 -4.2831845 -4.2982588 -4.3083591 -4.3156295 -4.3226852 -4.3265843 -4.3257346 -4.3212657 -4.3105969 -4.299777 -4.2914538][-4.2349119 -4.2377486 -4.2485485 -4.262424 -4.2801561 -4.2955561 -4.303225 -4.3073554 -4.3115506 -4.3150196 -4.3164568 -4.3168464 -4.3148394 -4.3100896 -4.301538][-4.2319436 -4.2336154 -4.2454586 -4.2622585 -4.281496 -4.2972765 -4.3038359 -4.3043218 -4.3045115 -4.3065848 -4.3099279 -4.3130918 -4.3158579 -4.315587 -4.309936]]...]
INFO - root - 2017-12-07 10:10:48.258264: step 3510, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.699 sec/batch; 63h:52m:16s remains)
INFO - root - 2017-12-07 10:10:55.552295: step 3520, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 68h:09m:22s remains)
INFO - root - 2017-12-07 10:11:02.917209: step 3530, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 0.775 sec/batch; 70h:47m:53s remains)
INFO - root - 2017-12-07 10:11:10.198858: step 3540, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.750 sec/batch; 68h:30m:28s remains)
INFO - root - 2017-12-07 10:11:17.516899: step 3550, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 64h:27m:09s remains)
INFO - root - 2017-12-07 10:11:24.757867: step 3560, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 62h:16m:23s remains)
INFO - root - 2017-12-07 10:11:32.219415: step 3570, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.735 sec/batch; 67h:06m:40s remains)
INFO - root - 2017-12-07 10:11:39.453831: step 3580, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.754 sec/batch; 68h:52m:49s remains)
INFO - root - 2017-12-07 10:11:46.628927: step 3590, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 63h:54m:02s remains)
INFO - root - 2017-12-07 10:11:53.734186: step 3600, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 64h:27m:39s remains)
2017-12-07 10:11:54.500781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2667165 -4.2530451 -4.2424345 -4.2197909 -4.1875658 -4.1654367 -4.1639729 -4.17977 -4.1953983 -4.2153459 -4.2292237 -4.2220016 -4.1962233 -4.1640639 -4.1381087][-4.2482562 -4.2370658 -4.2368045 -4.22655 -4.1966133 -4.1641808 -4.1538496 -4.1739268 -4.1978397 -4.2204251 -4.2378316 -4.2339783 -4.2083573 -4.1738253 -4.1445775][-4.2304468 -4.229301 -4.2411122 -4.2451172 -4.2234068 -4.1839113 -4.1637607 -4.182312 -4.2118025 -4.2362947 -4.2551131 -4.2571731 -4.2375751 -4.2075315 -4.18061][-4.2111988 -4.2213445 -4.2377777 -4.2508268 -4.2388358 -4.1985779 -4.1739497 -4.1908851 -4.2232161 -4.2487626 -4.2682557 -4.2771621 -4.2671456 -4.245048 -4.2221651][-4.1864944 -4.2025375 -4.2199616 -4.2343144 -4.2293816 -4.1951756 -4.1722131 -4.1905332 -4.2233071 -4.2478123 -4.2691579 -4.2856493 -4.2847424 -4.2676635 -4.2468333][-4.1603184 -4.1746535 -4.1906915 -4.2043819 -4.204216 -4.1811628 -4.1652761 -4.1858759 -4.2162151 -4.2346439 -4.2524457 -4.2714448 -4.2778172 -4.26661 -4.2493291][-4.1441388 -4.1454554 -4.1575322 -4.1730471 -4.1775331 -4.165411 -4.1605482 -4.1830544 -4.2088351 -4.21922 -4.2256207 -4.2387214 -4.2474408 -4.242703 -4.2342539][-4.1472635 -4.1328793 -4.1398659 -4.1607323 -4.1744432 -4.1723328 -4.1754746 -4.1973724 -4.2132287 -4.210012 -4.2002692 -4.2032547 -4.2119446 -4.2167253 -4.2213092][-4.17225 -4.1503506 -4.1490369 -4.1676564 -4.1870975 -4.1917295 -4.1989164 -4.2157669 -4.2210689 -4.2042041 -4.1845775 -4.1811752 -4.1889944 -4.2000065 -4.2152729][-4.2009277 -4.1807017 -4.1728339 -4.1834555 -4.1988788 -4.2049489 -4.2121119 -4.2229333 -4.2198644 -4.1979918 -4.1786666 -4.1767592 -4.1824503 -4.1933703 -4.212709][-4.222425 -4.2066994 -4.1956553 -4.1980424 -4.2061224 -4.2101088 -4.2132874 -4.2158775 -4.2064805 -4.1862593 -4.1736374 -4.1784062 -4.1859436 -4.1941981 -4.2104316][-4.2340965 -4.2220521 -4.210454 -4.2101092 -4.2152872 -4.2161145 -4.21512 -4.2106829 -4.1974978 -4.1799159 -4.1728163 -4.182838 -4.1928086 -4.1988382 -4.2109442][-4.2385783 -4.2310567 -4.2218266 -4.2210503 -4.2266064 -4.2282939 -4.2248273 -4.2153311 -4.199122 -4.1820216 -4.1778779 -4.1887841 -4.1996708 -4.2043004 -4.2135777][-4.24882 -4.2461143 -4.2416439 -4.243279 -4.2492461 -4.2513757 -4.2478189 -4.2379231 -4.2226324 -4.2069731 -4.2025738 -4.2118049 -4.2217827 -4.2244406 -4.2296958][-4.2655663 -4.2665339 -4.2659588 -4.270267 -4.27664 -4.2795706 -4.2775936 -4.2702188 -4.2581491 -4.245667 -4.24138 -4.2473335 -4.2538686 -4.2543464 -4.2553134]]...]
INFO - root - 2017-12-07 10:12:01.654170: step 3610, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.737 sec/batch; 67h:19m:43s remains)
INFO - root - 2017-12-07 10:12:08.775966: step 3620, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 63h:39m:27s remains)
INFO - root - 2017-12-07 10:12:15.876438: step 3630, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 63h:03m:42s remains)
INFO - root - 2017-12-07 10:12:23.071430: step 3640, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 64h:50m:25s remains)
INFO - root - 2017-12-07 10:12:30.301060: step 3650, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.693 sec/batch; 63h:18m:43s remains)
INFO - root - 2017-12-07 10:12:37.339965: step 3660, loss = 2.11, batch loss = 2.05 (11.1 examples/sec; 0.718 sec/batch; 65h:35m:01s remains)
INFO - root - 2017-12-07 10:12:44.401316: step 3670, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 66h:53m:37s remains)
INFO - root - 2017-12-07 10:12:51.502995: step 3680, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.739 sec/batch; 67h:31m:58s remains)
INFO - root - 2017-12-07 10:12:58.652208: step 3690, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.681 sec/batch; 62h:10m:06s remains)
INFO - root - 2017-12-07 10:13:05.824231: step 3700, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 62h:49m:59s remains)
2017-12-07 10:13:06.533654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2689433 -4.2808514 -4.275218 -4.2497153 -4.2224078 -4.2068596 -4.2020249 -4.2029409 -4.2008805 -4.1926556 -4.1859536 -4.1838655 -4.1929584 -4.2078614 -4.2217484][-4.2479949 -4.25944 -4.2535458 -4.2313375 -4.2083917 -4.1978259 -4.2038302 -4.207912 -4.1961651 -4.1737494 -4.1592493 -4.160131 -4.1797657 -4.2051206 -4.2205071][-4.2223983 -4.2297435 -4.222796 -4.2010794 -4.1846509 -4.1821842 -4.1959038 -4.201159 -4.18273 -4.1541233 -4.1408815 -4.1511612 -4.1819243 -4.2127438 -4.2262053][-4.1824732 -4.1837153 -4.1763034 -4.1591563 -4.1554308 -4.1654091 -4.1880569 -4.1953368 -4.1773529 -4.1577072 -4.1599231 -4.1788783 -4.2096834 -4.23182 -4.2330103][-4.1506958 -4.1424894 -4.132699 -4.12216 -4.1318965 -4.1526031 -4.1748247 -4.1817603 -4.17165 -4.1700573 -4.1895547 -4.2130694 -4.23758 -4.244173 -4.2245822][-4.1354642 -4.119544 -4.1021433 -4.0909734 -4.0987663 -4.112886 -4.1203251 -4.1209774 -4.1226025 -4.1366992 -4.1654935 -4.1905451 -4.2094421 -4.2110348 -4.1866961][-4.1053882 -4.0841713 -4.0582485 -4.0400615 -4.0366774 -4.0350776 -4.0217695 -4.0149918 -4.0293751 -4.0618634 -4.1036463 -4.1332483 -4.1504674 -4.1568518 -4.1438522][-4.0418272 -4.0022383 -3.9518056 -3.9115229 -3.893955 -3.8811762 -3.8540251 -3.849807 -3.8888125 -3.9477527 -4.008389 -4.0492382 -4.0750203 -4.0926566 -4.1017771][-4.0226512 -3.96232 -3.8892756 -3.8304381 -3.8073351 -3.7968736 -3.7744653 -3.7788639 -3.82866 -3.8952789 -3.9577348 -3.9992416 -4.0239763 -4.0436893 -4.0670896][-4.1106634 -4.0620413 -4.006371 -3.9638283 -3.9490533 -3.9464755 -3.9399283 -3.9449472 -3.9719963 -4.0077643 -4.04089 -4.0604911 -4.06823 -4.0750155 -4.089817][-4.2119379 -4.1857085 -4.1581173 -4.1382961 -4.1334038 -4.1358595 -4.1354795 -4.1362491 -4.1434507 -4.1527863 -4.161222 -4.1623783 -4.1556339 -4.1515045 -4.1534157][-4.2779045 -4.2647986 -4.251596 -4.2412062 -4.2386189 -4.2403979 -4.2397881 -4.2377763 -4.2372427 -4.2374434 -4.2372622 -4.2320161 -4.2237654 -4.2193065 -4.2194519][-4.3156929 -4.3077908 -4.2986913 -4.2899051 -4.2851143 -4.2832694 -4.280293 -4.2779164 -4.2769341 -4.2757521 -4.2746229 -4.2709684 -4.2676649 -4.2687726 -4.2733822][-4.3336096 -4.3283939 -4.3226542 -4.3158875 -4.3105206 -4.3071218 -4.3038578 -4.3013058 -4.29961 -4.2984157 -4.2987432 -4.298893 -4.2998028 -4.302815 -4.3083715][-4.339982 -4.3362036 -4.3319607 -4.326684 -4.3214827 -4.3188868 -4.3173962 -4.3161392 -4.314229 -4.3121653 -4.311336 -4.3116536 -4.31356 -4.3162618 -4.3199277]]...]
INFO - root - 2017-12-07 10:13:13.669563: step 3710, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 66h:35m:01s remains)
INFO - root - 2017-12-07 10:13:20.697309: step 3720, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 64h:26m:00s remains)
INFO - root - 2017-12-07 10:13:27.725464: step 3730, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 65h:29m:37s remains)
INFO - root - 2017-12-07 10:13:34.916055: step 3740, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 63h:42m:05s remains)
INFO - root - 2017-12-07 10:13:42.076548: step 3750, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.718 sec/batch; 65h:31m:43s remains)
INFO - root - 2017-12-07 10:13:49.217093: step 3760, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 66h:03m:14s remains)
INFO - root - 2017-12-07 10:13:56.426420: step 3770, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 64h:39m:23s remains)
INFO - root - 2017-12-07 10:14:03.549368: step 3780, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 66h:44m:30s remains)
INFO - root - 2017-12-07 10:14:11.106125: step 3790, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.743 sec/batch; 67h:48m:44s remains)
INFO - root - 2017-12-07 10:14:21.750192: step 3800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 86h:43m:19s remains)
2017-12-07 10:14:22.663820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34462 -4.3373756 -4.3276234 -4.3214722 -4.3185768 -4.3180885 -4.3198919 -4.320086 -4.3157439 -4.3093119 -4.3028865 -4.2973003 -4.2947707 -4.2987514 -4.3061161][-4.3464675 -4.3358006 -4.3213944 -4.3106494 -4.3064241 -4.3057427 -4.3087864 -4.30752 -4.3012567 -4.2913795 -4.28435 -4.2797 -4.2763238 -4.281436 -4.2906446][-4.3420091 -4.3272724 -4.3077712 -4.29248 -4.2891307 -4.2925858 -4.2986846 -4.2967653 -4.288115 -4.27603 -4.26973 -4.266469 -4.2630906 -4.2698412 -4.2792778][-4.3317981 -4.3087087 -4.2816234 -4.2614269 -4.2626348 -4.2756462 -4.288527 -4.2893605 -4.2811232 -4.2722554 -4.2675209 -4.2644262 -4.260952 -4.2663932 -4.2725849][-4.313221 -4.2794013 -4.240303 -4.2077603 -4.2011967 -4.2145929 -4.2307978 -4.2382493 -4.2389126 -4.2440219 -4.2465558 -4.2462149 -4.2467437 -4.2530375 -4.2569847][-4.2948227 -4.2531462 -4.2020779 -4.1559386 -4.1288366 -4.1257677 -4.1302118 -4.135561 -4.1518083 -4.1833162 -4.2052341 -4.2152047 -4.2273059 -4.2412052 -4.2480507][-4.2797041 -4.2358246 -4.1800127 -4.1229081 -4.0738311 -4.0405087 -4.0078058 -3.982944 -4.0092406 -4.0783687 -4.1311965 -4.1639423 -4.1948166 -4.2245483 -4.2425008][-4.2706132 -4.2306361 -4.1776762 -4.118721 -4.0575137 -3.9942658 -3.9060204 -3.8187099 -3.83648 -3.9420433 -4.0313249 -4.0956411 -4.1502676 -4.1992826 -4.2344933][-4.2787685 -4.2525058 -4.2144065 -4.1681337 -4.1163573 -4.0507517 -3.9427056 -3.8214252 -3.8024724 -3.8912277 -3.9851551 -4.0665922 -4.1340523 -4.1928582 -4.2389421][-4.2995734 -4.2887287 -4.2703991 -4.2429533 -4.2107081 -4.1611056 -4.0798306 -3.9926491 -3.9629269 -4.0034304 -4.0587606 -4.1140313 -4.1647658 -4.2166357 -4.2581906][-4.3168511 -4.3134279 -4.3094621 -4.2992487 -4.2834525 -4.2496419 -4.2006121 -4.1610432 -4.1442571 -4.1571665 -4.176394 -4.1968217 -4.2209 -4.2566996 -4.2877164][-4.3171816 -4.3167639 -4.3218889 -4.3248129 -4.3254294 -4.3095474 -4.2830372 -4.2743888 -4.27109 -4.2739358 -4.2757316 -4.2757378 -4.2828732 -4.3020959 -4.3192506][-4.3113546 -4.3122587 -4.3205218 -4.3303289 -4.3408222 -4.3387761 -4.3287497 -4.3328433 -4.334199 -4.3350191 -4.3325543 -4.3271832 -4.3264313 -4.3343444 -4.3412228][-4.3151703 -4.3147449 -4.3215389 -4.3312545 -4.340703 -4.3455796 -4.3460984 -4.3525796 -4.3535624 -4.3525743 -4.3501592 -4.3451996 -4.3429832 -4.3455839 -4.34809][-4.3255825 -4.3240986 -4.3263249 -4.3318233 -4.337553 -4.3416324 -4.3429837 -4.3458958 -4.34548 -4.3446765 -4.3444486 -4.3432078 -4.3427329 -4.3447919 -4.3466315]]...]
INFO - root - 2017-12-07 10:14:31.952344: step 3810, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 82h:54m:45s remains)
INFO - root - 2017-12-07 10:14:41.322482: step 3820, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 84h:51m:22s remains)
INFO - root - 2017-12-07 10:14:50.803146: step 3830, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 85h:53m:58s remains)
INFO - root - 2017-12-07 10:15:00.246103: step 3840, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.959 sec/batch; 87h:32m:42s remains)
INFO - root - 2017-12-07 10:15:09.548774: step 3850, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 86h:11m:53s remains)
INFO - root - 2017-12-07 10:15:18.726535: step 3860, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 85h:46m:10s remains)
INFO - root - 2017-12-07 10:15:28.088160: step 3870, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 86h:30m:02s remains)
INFO - root - 2017-12-07 10:15:37.489290: step 3880, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 86h:28m:15s remains)
INFO - root - 2017-12-07 10:15:46.843588: step 3890, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 84h:17m:34s remains)
INFO - root - 2017-12-07 10:15:56.204919: step 3900, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 85h:24m:35s remains)
2017-12-07 10:15:57.057985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2591438 -4.1952033 -4.1334691 -4.09877 -4.1119795 -4.1508031 -4.1918607 -4.2161465 -4.2204642 -4.21173 -4.199338 -4.1891656 -4.1736631 -4.1637425 -4.174367][-4.2545853 -4.1801443 -4.1033254 -4.0637045 -4.0911455 -4.1448164 -4.1891828 -4.2132611 -4.2176776 -4.2070274 -4.1951571 -4.1883845 -4.178369 -4.1697044 -4.176836][-4.2537727 -4.1709414 -4.0816035 -4.0376778 -4.0798287 -4.1506615 -4.197803 -4.22011 -4.22606 -4.2142377 -4.2010927 -4.193953 -4.1813912 -4.1695042 -4.1703548][-4.2567697 -4.1687031 -4.0695548 -4.0209217 -4.0757337 -4.159956 -4.2101197 -4.2288156 -4.2297573 -4.2145 -4.2012887 -4.1932716 -4.1795969 -4.171977 -4.173038][-4.2648835 -4.1736817 -4.0673456 -4.011539 -4.0697861 -4.16352 -4.2175016 -4.2323432 -4.228498 -4.2120824 -4.1998596 -4.1928287 -4.18621 -4.1889443 -4.1939197][-4.2799358 -4.1925964 -4.0856538 -4.0219159 -4.0700121 -4.1639614 -4.2180548 -4.2289329 -4.22521 -4.2112961 -4.1992397 -4.1948929 -4.1960735 -4.2067223 -4.2135029][-4.2920308 -4.2143612 -4.1171846 -4.0533643 -4.089047 -4.1746087 -4.2238979 -4.2290096 -4.2243905 -4.2141104 -4.1980495 -4.1902204 -4.1944938 -4.2110572 -4.2213931][-4.2960525 -4.2269559 -4.1430144 -4.0866027 -4.1151195 -4.1919646 -4.2404847 -4.2467356 -4.2389145 -4.227354 -4.2044077 -4.1850896 -4.1826196 -4.200079 -4.2172761][-4.2978096 -4.2352037 -4.1638174 -4.1183286 -4.1448121 -4.213841 -4.2623482 -4.271565 -4.2632203 -4.2520032 -4.2302642 -4.2032981 -4.1885653 -4.1973987 -4.2140346][-4.2972841 -4.2405624 -4.179955 -4.1436734 -4.1683087 -4.2283397 -4.2748947 -4.2858658 -4.2800775 -4.2748723 -4.2605162 -4.2366924 -4.2161951 -4.2119131 -4.2179728][-4.291553 -4.2383642 -4.1881466 -4.1621437 -4.1858959 -4.236577 -4.2799664 -4.2924047 -4.28612 -4.2822571 -4.2741656 -4.25694 -4.2383723 -4.2299047 -4.2314639][-4.2858081 -4.2380786 -4.1981158 -4.18212 -4.2050853 -4.2461481 -4.2829437 -4.2940025 -4.2885017 -4.2853994 -4.2799354 -4.2655735 -4.251256 -4.2471218 -4.2513866][-4.281496 -4.2429743 -4.2136111 -4.2051668 -4.2261291 -4.2561822 -4.2833467 -4.2919464 -4.2877469 -4.2850847 -4.2803259 -4.2682443 -4.2570491 -4.2577362 -4.2676191][-4.2794747 -4.2513909 -4.23205 -4.2277575 -4.2451472 -4.2660437 -4.2860518 -4.2928257 -4.2882071 -4.2844 -4.280653 -4.272397 -4.2650118 -4.2663665 -4.2778592][-4.2851658 -4.2656531 -4.2537179 -4.2512021 -4.2637467 -4.27876 -4.293561 -4.2977476 -4.2920313 -4.2859516 -4.2811308 -4.2768931 -4.275094 -4.2792654 -4.2906027]]...]
INFO - root - 2017-12-07 10:16:06.506601: step 3910, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 80h:49m:22s remains)
INFO - root - 2017-12-07 10:16:16.007450: step 3920, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 84h:15m:33s remains)
INFO - root - 2017-12-07 10:16:25.410939: step 3930, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.974 sec/batch; 88h:54m:03s remains)
INFO - root - 2017-12-07 10:16:34.832336: step 3940, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 88h:31m:18s remains)
INFO - root - 2017-12-07 10:16:44.255119: step 3950, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.975 sec/batch; 88h:56m:50s remains)
INFO - root - 2017-12-07 10:16:53.660578: step 3960, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:08m:31s remains)
INFO - root - 2017-12-07 10:17:02.953253: step 3970, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 84h:39m:59s remains)
INFO - root - 2017-12-07 10:17:12.692512: step 3980, loss = 2.11, batch loss = 2.05 (7.8 examples/sec; 1.023 sec/batch; 93h:22m:47s remains)
INFO - root - 2017-12-07 10:17:22.446539: step 3990, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.024 sec/batch; 93h:24m:06s remains)
INFO - root - 2017-12-07 10:17:32.102126: step 4000, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 86h:42m:47s remains)
2017-12-07 10:17:33.090276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3051147 -4.3068905 -4.3120871 -4.3293791 -4.3434324 -4.3243818 -4.2630887 -4.1838551 -4.128509 -4.1282516 -4.1612477 -4.1664138 -4.1407595 -4.1210761 -4.141829][-4.3165717 -4.3169022 -4.3194218 -4.3322864 -4.341022 -4.31655 -4.2511778 -4.1718845 -4.130116 -4.1482153 -4.1849084 -4.1843586 -4.1502409 -4.1215091 -4.1421294][-4.317894 -4.3192825 -4.3228045 -4.3305082 -4.3333597 -4.3023963 -4.2318282 -4.149169 -4.1204648 -4.1598973 -4.2037554 -4.2034297 -4.1647725 -4.1248755 -4.1358962][-4.3093557 -4.3178148 -4.3264575 -4.3301759 -4.3217692 -4.2792187 -4.2014318 -4.1171989 -4.1035557 -4.1652222 -4.2184319 -4.2243705 -4.1865749 -4.1376457 -4.1402416][-4.3046851 -4.322175 -4.3378348 -4.3355441 -4.3124833 -4.251698 -4.157 -4.0649586 -4.0629268 -4.1502743 -4.2185979 -4.2327023 -4.1971865 -4.145237 -4.1466951][-4.3064795 -4.3318124 -4.350596 -4.3387318 -4.295598 -4.2097917 -4.0912991 -3.9837499 -3.9912753 -4.1046 -4.1954074 -4.223197 -4.1905589 -4.1432452 -4.1502342][-4.3068662 -4.3390021 -4.3581605 -4.3333459 -4.2650981 -4.151125 -4.006701 -3.8812039 -3.8972483 -4.0397391 -4.1558208 -4.1973977 -4.1712255 -4.13194 -4.1469049][-4.307457 -4.3425508 -4.358099 -4.3231854 -4.2345943 -4.1008382 -3.9392715 -3.8007774 -3.8292341 -3.9954491 -4.12656 -4.1746192 -4.1574435 -4.1316581 -4.1538515][-4.3073587 -4.3388367 -4.3486352 -4.3118663 -4.2172346 -4.0829983 -3.92881 -3.809715 -3.853478 -4.0114985 -4.1325908 -4.178246 -4.1719737 -4.1605134 -4.187264][-4.3077922 -4.3338141 -4.3389945 -4.3064189 -4.2189174 -4.1006403 -3.9759119 -3.8916891 -3.9408095 -4.0712576 -4.167613 -4.2055407 -4.2084908 -4.2084155 -4.2338552][-4.3106561 -4.3300147 -4.333765 -4.3060803 -4.2306924 -4.133914 -4.041657 -3.9878209 -4.03733 -4.1404829 -4.2134194 -4.24372 -4.252069 -4.2576532 -4.2771854][-4.3127203 -4.3243818 -4.3277578 -4.3039894 -4.241941 -4.1628742 -4.0936775 -4.0590792 -4.1072879 -4.1908007 -4.2474079 -4.27404 -4.2867746 -4.2939196 -4.3077126][-4.3135638 -4.3179584 -4.320837 -4.2986546 -4.2445254 -4.1748171 -4.1172838 -4.0961685 -4.141643 -4.2125826 -4.2616482 -4.2912269 -4.309278 -4.31691 -4.3263292][-4.3138332 -4.3114376 -4.3127809 -4.2925153 -4.2442193 -4.1804314 -4.1303 -4.1215568 -4.1653681 -4.2283792 -4.2758803 -4.3090196 -4.32847 -4.334362 -4.3399515][-4.3125811 -4.3058944 -4.3079262 -4.2942915 -4.2553658 -4.2021937 -4.1629615 -4.1624675 -4.1998239 -4.2547455 -4.2997551 -4.3306122 -4.344605 -4.3459663 -4.3481536]]...]
INFO - root - 2017-12-07 10:17:42.682869: step 4010, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 84h:34m:38s remains)
INFO - root - 2017-12-07 10:17:52.172038: step 4020, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 85h:38m:08s remains)
INFO - root - 2017-12-07 10:18:01.881099: step 4030, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 89h:39m:17s remains)
INFO - root - 2017-12-07 10:18:11.528178: step 4040, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 80h:38m:40s remains)
INFO - root - 2017-12-07 10:18:21.226488: step 4050, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.970 sec/batch; 88h:28m:05s remains)
INFO - root - 2017-12-07 10:18:30.914123: step 4060, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.026 sec/batch; 93h:36m:03s remains)
INFO - root - 2017-12-07 10:18:40.473137: step 4070, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 91h:09m:55s remains)
INFO - root - 2017-12-07 10:18:50.283914: step 4080, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 92h:38m:34s remains)
INFO - root - 2017-12-07 10:19:00.115644: step 4090, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 85h:56m:08s remains)
INFO - root - 2017-12-07 10:19:09.766103: step 4100, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 91h:27m:47s remains)
2017-12-07 10:19:10.706803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3394079 -4.3410211 -4.3413339 -4.3407412 -4.340106 -4.3412933 -4.3412037 -4.340673 -4.3437014 -4.3451872 -4.3419857 -4.3368177 -4.332 -4.3279457 -4.3285766][-4.3277292 -4.3232551 -4.3224363 -4.3231249 -4.3268018 -4.332047 -4.3333955 -4.3350759 -4.3406239 -4.3388052 -4.3278565 -4.3173351 -4.3093333 -4.3057761 -4.3101082][-4.3029466 -4.291883 -4.2891412 -4.2934251 -4.3039551 -4.3150411 -4.3196 -4.3246021 -4.3299737 -4.3203573 -4.3006344 -4.2865639 -4.2786765 -4.27893 -4.2895131][-4.264421 -4.2536187 -4.2563472 -4.2692275 -4.2857943 -4.299499 -4.3037124 -4.3071632 -4.3056211 -4.2866807 -4.2641516 -4.2548909 -4.2554541 -4.264462 -4.2770753][-4.2293005 -4.2243967 -4.2363958 -4.2560134 -4.2745438 -4.2835884 -4.2777858 -4.2688565 -4.25555 -4.2318668 -4.2190213 -4.2258162 -4.237875 -4.2537923 -4.2626104][-4.2153044 -4.2155709 -4.2316804 -4.2526422 -4.2648187 -4.2606893 -4.2344894 -4.2038112 -4.1842914 -4.1747169 -4.1891212 -4.2154675 -4.2328444 -4.2478375 -4.2493033][-4.226171 -4.2242327 -4.231842 -4.2382231 -4.2309647 -4.2016649 -4.1462541 -4.0941978 -4.0940566 -4.131031 -4.1861877 -4.2275457 -4.2438474 -4.2499728 -4.2408538][-4.2369957 -4.2312617 -4.2276855 -4.2193069 -4.1931005 -4.1371145 -4.0490727 -3.9844849 -4.0287108 -4.1201458 -4.199079 -4.2386532 -4.2445264 -4.2386937 -4.2193065][-4.2361703 -4.22773 -4.2205453 -4.2102408 -4.1830044 -4.1292 -4.0518661 -4.0170379 -4.0838037 -4.17611 -4.2351203 -4.2507496 -4.2363744 -4.2178488 -4.1945138][-4.2383456 -4.2285504 -4.223269 -4.2127881 -4.193604 -4.163641 -4.1285405 -4.1306343 -4.1830606 -4.2368522 -4.2597523 -4.2472434 -4.2156477 -4.1973009 -4.1889009][-4.2355957 -4.2321477 -4.2334256 -4.224896 -4.2129316 -4.2025232 -4.1921048 -4.204514 -4.2367854 -4.258636 -4.2553172 -4.2252007 -4.1919856 -4.1892433 -4.2041106][-4.2318745 -4.2341046 -4.2367682 -4.2296543 -4.222568 -4.2180204 -4.216414 -4.2311726 -4.2512488 -4.2562146 -4.2376094 -4.2000103 -4.1778097 -4.1934662 -4.2255836][-4.2144732 -4.2209358 -4.2237 -4.2170157 -4.2137208 -4.2098341 -4.2155547 -4.2345219 -4.2485371 -4.2475133 -4.2213068 -4.1841817 -4.1724682 -4.1982288 -4.2351909][-4.174962 -4.1784759 -4.1799273 -4.176806 -4.1786742 -4.181201 -4.1927447 -4.2127318 -4.2216749 -4.2189956 -4.2000279 -4.1737657 -4.1688309 -4.1944103 -4.2312117][-4.1410961 -4.1380391 -4.1362824 -4.1356182 -4.1412029 -4.1485558 -4.1639662 -4.1823654 -4.192296 -4.1934824 -4.1861267 -4.1708283 -4.1688161 -4.1896448 -4.2222152]]...]
INFO - root - 2017-12-07 10:19:20.399524: step 4110, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 86h:01m:36s remains)
INFO - root - 2017-12-07 10:19:30.023301: step 4120, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 88h:31m:53s remains)
INFO - root - 2017-12-07 10:19:39.716975: step 4130, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 88h:40m:41s remains)
INFO - root - 2017-12-07 10:19:49.230982: step 4140, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.991 sec/batch; 90h:22m:02s remains)
INFO - root - 2017-12-07 10:19:58.797218: step 4150, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 85h:05m:55s remains)
INFO - root - 2017-12-07 10:20:08.471345: step 4160, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 88h:19m:28s remains)
INFO - root - 2017-12-07 10:20:18.166559: step 4170, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.977 sec/batch; 89h:06m:16s remains)
INFO - root - 2017-12-07 10:20:27.681262: step 4180, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 87h:37m:06s remains)
INFO - root - 2017-12-07 10:20:37.130866: step 4190, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 83h:08m:38s remains)
INFO - root - 2017-12-07 10:20:46.648784: step 4200, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 86h:16m:21s remains)
2017-12-07 10:20:47.582855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2542934 -4.2465773 -4.2433014 -4.2469125 -4.2506065 -4.2471819 -4.2366238 -4.2185979 -4.1988716 -4.1771593 -4.1573291 -4.1391854 -4.1340203 -4.1525769 -4.189539][-4.2328262 -4.2306209 -4.2312665 -4.2311282 -4.2232008 -4.2020903 -4.1719632 -4.1368661 -4.1055222 -4.0760856 -4.0516219 -4.0288997 -4.0214362 -4.0456052 -4.0951157][-4.2256279 -4.2308874 -4.2332897 -4.225668 -4.2035522 -4.1633534 -4.1139956 -4.0646963 -4.027245 -3.9985459 -3.9762561 -3.9560552 -3.9514718 -3.9814775 -4.0366149][-4.2106867 -4.2201037 -4.2227483 -4.2099214 -4.1799469 -4.1312251 -4.0762258 -4.0264287 -3.9937654 -3.9776113 -3.97156 -3.9682808 -3.9772592 -4.0095854 -4.0543203][-4.1634173 -4.1798062 -4.1873751 -4.175344 -4.1440015 -4.0975714 -4.0507689 -4.0176945 -4.0056987 -4.0105519 -4.0242281 -4.039506 -4.0588756 -4.0849271 -4.1113019][-4.0907288 -4.1204376 -4.1379094 -4.1293936 -4.1008763 -4.0618758 -4.0279541 -4.0157657 -4.0274391 -4.0509267 -4.0770392 -4.1018534 -4.124824 -4.1444225 -4.159667][-4.0194688 -4.0623426 -4.0904818 -4.08932 -4.0676951 -4.0394015 -4.01779 -4.0191693 -4.044703 -4.0767236 -4.1077428 -4.1348939 -4.1579328 -4.1765051 -4.1919956][-3.9859359 -4.0347166 -4.0695343 -4.0773983 -4.0700927 -4.0625572 -4.059474 -4.0684857 -4.092051 -4.1144094 -4.1348109 -4.1544333 -4.1760073 -4.2002149 -4.2258582][-4.0179343 -4.0631461 -4.0978017 -4.1146097 -4.1234527 -4.1339703 -4.1412687 -4.1489153 -4.1561317 -4.1558237 -4.1568575 -4.1654372 -4.1834235 -4.21293 -4.2485738][-4.0805888 -4.1154613 -4.1474152 -4.1696787 -4.1875682 -4.2042994 -4.2140436 -4.2166438 -4.2064142 -4.1843371 -4.1659422 -4.1592607 -4.1686211 -4.1976004 -4.24074][-4.1304603 -4.1546111 -4.18153 -4.2047119 -4.2261009 -4.2458615 -4.2604151 -4.263793 -4.2449722 -4.2068915 -4.1692991 -4.1435866 -4.1380959 -4.1621833 -4.2104259][-4.1702571 -4.1862035 -4.2076459 -4.2301574 -4.253201 -4.275918 -4.2947264 -4.3003154 -4.2807517 -4.2376652 -4.189271 -4.1488037 -4.1310744 -4.1483507 -4.194128][-4.2219181 -4.2308283 -4.245007 -4.2624373 -4.2821369 -4.3013878 -4.3169212 -4.3227983 -4.3098879 -4.2786279 -4.2396688 -4.2040043 -4.1859679 -4.1939244 -4.2213311][-4.2817707 -4.2862115 -4.2930269 -4.3029056 -4.3154087 -4.3270698 -4.3363042 -4.3407159 -4.3353252 -4.31977 -4.2974653 -4.27375 -4.2573752 -4.2513161 -4.250793][-4.3333116 -4.3340311 -4.3347411 -4.3374429 -4.3422022 -4.3458948 -4.3485985 -4.3504624 -4.3483977 -4.3406987 -4.3273654 -4.3101549 -4.2909765 -4.2689972 -4.2438412]]...]
INFO - root - 2017-12-07 10:20:57.269708: step 4210, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 88h:53m:24s remains)
INFO - root - 2017-12-07 10:21:06.801965: step 4220, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.990 sec/batch; 90h:15m:25s remains)
INFO - root - 2017-12-07 10:21:16.388444: step 4230, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 87h:08m:38s remains)
INFO - root - 2017-12-07 10:21:26.157399: step 4240, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 86h:45m:41s remains)
INFO - root - 2017-12-07 10:21:35.898527: step 4250, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 89h:52m:17s remains)
INFO - root - 2017-12-07 10:21:45.526682: step 4260, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.008 sec/batch; 91h:54m:34s remains)
INFO - root - 2017-12-07 10:21:55.194986: step 4270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 88h:42m:43s remains)
INFO - root - 2017-12-07 10:22:04.904582: step 4280, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 84h:01m:21s remains)
INFO - root - 2017-12-07 10:22:14.658110: step 4290, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.027 sec/batch; 93h:38m:15s remains)
INFO - root - 2017-12-07 10:22:24.189549: step 4300, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 87h:09m:05s remains)
2017-12-07 10:22:25.070519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3735375 -4.3683481 -4.3628693 -4.3574715 -4.353734 -4.3527184 -4.3529458 -4.354826 -4.3590331 -4.3620386 -4.3616462 -4.3600926 -4.360414 -4.3629451 -4.3653374][-4.3763814 -4.36733 -4.35618 -4.3454714 -4.3377271 -4.3330731 -4.3317609 -4.333982 -4.3397236 -4.3466291 -4.3506522 -4.3525825 -4.3553705 -4.3605618 -4.3649578][-4.3699055 -4.35441 -4.3364921 -4.3203678 -4.3074951 -4.2953677 -4.28425 -4.2824569 -4.2902718 -4.3038168 -4.3195581 -4.3323174 -4.3426266 -4.352901 -4.3621616][-4.3515334 -4.3274212 -4.3034282 -4.2831793 -4.2627821 -4.2402759 -4.2191653 -4.2122369 -4.2180753 -4.234344 -4.2612014 -4.2869806 -4.3088551 -4.328815 -4.347918][-4.3279462 -4.2921171 -4.2577019 -4.2288551 -4.1973104 -4.1583238 -4.1227126 -4.1112485 -4.1153507 -4.1343446 -4.1722565 -4.21661 -4.25667 -4.2931418 -4.3251724][-4.3086343 -4.2604103 -4.2103124 -4.1623416 -4.1097803 -4.0485821 -3.9957805 -3.9769719 -3.9792674 -4.005043 -4.0569611 -4.1194587 -4.1837239 -4.24561 -4.2985][-4.2940893 -4.2358623 -4.174583 -4.1126862 -4.0469379 -3.9720092 -3.9008324 -3.8621635 -3.8535404 -3.8807969 -3.9453559 -4.0268722 -4.1159859 -4.2053027 -4.2803431][-4.2937317 -4.2323503 -4.1709423 -4.1115761 -4.0477638 -3.9702113 -3.8886468 -3.8309462 -3.8122342 -3.8438158 -3.9186361 -4.0124054 -4.1119246 -4.2079678 -4.2858596][-4.3102612 -4.2574539 -4.2070575 -4.1587949 -4.1074867 -4.0422764 -3.9666514 -3.90926 -3.8943682 -3.931808 -4.0030589 -4.0873556 -4.1751862 -4.2559152 -4.3159342][-4.3366995 -4.2985764 -4.2626963 -4.2277522 -4.1920271 -4.1479483 -4.0945859 -4.0542741 -4.0492249 -4.0822577 -4.1349978 -4.1949005 -4.2584605 -4.3137712 -4.3503][-4.361505 -4.338697 -4.3156796 -4.2908487 -4.2676558 -4.2441406 -4.2160926 -4.1936135 -4.1930752 -4.2155261 -4.2490115 -4.2855229 -4.3250966 -4.3581491 -4.3746648][-4.378787 -4.3655987 -4.3486724 -4.3295736 -4.3161225 -4.3065839 -4.294301 -4.28403 -4.2844372 -4.2964168 -4.317349 -4.3400383 -4.3607578 -4.3753481 -4.3800259][-4.3868403 -4.3807993 -4.3698277 -4.3549309 -4.3438869 -4.3377643 -4.3321376 -4.3290896 -4.3314018 -4.3386016 -4.3501787 -4.3613358 -4.3688226 -4.3726649 -4.3726244][-4.3866124 -4.3850384 -4.3808246 -4.3729806 -4.3643932 -4.3566256 -4.3499303 -4.3466921 -4.348453 -4.3534131 -4.35916 -4.3638878 -4.3659759 -4.3665032 -4.3658295][-4.3836832 -4.3830566 -4.382422 -4.3801341 -4.37658 -4.3718581 -4.3657804 -4.3609438 -4.360188 -4.3628273 -4.3657489 -4.3677707 -4.3679237 -4.3666854 -4.3650994]]...]
INFO - root - 2017-12-07 10:22:34.760582: step 4310, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 88h:24m:57s remains)
INFO - root - 2017-12-07 10:22:44.489005: step 4320, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 92h:33m:38s remains)
INFO - root - 2017-12-07 10:22:54.200396: step 4330, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 90h:07m:36s remains)
INFO - root - 2017-12-07 10:23:03.693782: step 4340, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 79h:22m:24s remains)
INFO - root - 2017-12-07 10:23:13.311634: step 4350, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 86h:18m:58s remains)
INFO - root - 2017-12-07 10:23:23.033801: step 4360, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 90h:56m:49s remains)
INFO - root - 2017-12-07 10:23:32.759504: step 4370, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 90h:52m:12s remains)
INFO - root - 2017-12-07 10:23:42.454795: step 4380, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 88h:03m:27s remains)
INFO - root - 2017-12-07 10:23:52.041681: step 4390, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 88h:30m:50s remains)
INFO - root - 2017-12-07 10:24:01.795814: step 4400, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 86h:08m:07s remains)
2017-12-07 10:24:02.743733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2014089 -4.2125559 -4.2131543 -4.2053475 -4.2020216 -4.2097907 -4.2196493 -4.2143173 -4.1895909 -4.1785555 -4.1863184 -4.2075095 -4.2348523 -4.2400231 -4.2260222][-4.2002654 -4.2074428 -4.20354 -4.1954508 -4.1990013 -4.2110157 -4.222436 -4.2210016 -4.2032433 -4.1929307 -4.1975346 -4.2108612 -4.2194624 -4.2105064 -4.200016][-4.174387 -4.1773481 -4.1692939 -4.1596031 -4.1625328 -4.1721721 -4.1839409 -4.1947813 -4.1931171 -4.1879692 -4.1912374 -4.1913557 -4.1839609 -4.1722326 -4.17266][-4.1079574 -4.1150808 -4.1140232 -4.103457 -4.1055679 -4.1125865 -4.1257095 -4.1452436 -4.156096 -4.1541047 -4.1562867 -4.1483316 -4.133491 -4.1222434 -4.1238489][-4.0432081 -4.0586991 -4.0660176 -4.0588145 -4.0609746 -4.0684004 -4.0797491 -4.0947266 -4.1071639 -4.107327 -4.1103382 -4.1015558 -4.0882826 -4.0795183 -4.074512][-4.0276747 -4.0434361 -4.0439358 -4.030776 -4.0310979 -4.0338459 -4.0294242 -4.0311155 -4.0510659 -4.0624557 -4.0649662 -4.0595942 -4.0501623 -4.045506 -4.0404329][-4.0215917 -4.0295873 -4.0182261 -3.99569 -3.9836047 -3.9596691 -3.9182336 -3.9003246 -3.9471097 -3.9848099 -3.996829 -4.0033336 -4.0048108 -4.0087523 -4.0197225][-4.0330219 -4.0285211 -4.0064683 -3.9733319 -3.9389625 -3.8811421 -3.8051999 -3.7788286 -3.8645487 -3.9343848 -3.9607382 -3.9833107 -4.0037475 -4.0234165 -4.0538335][-4.083334 -4.0672345 -4.0433316 -4.0120311 -3.9735413 -3.9149258 -3.8483243 -3.8368776 -3.9211798 -3.9885917 -4.0149784 -4.0412359 -4.0707588 -4.0967827 -4.1341171][-4.13698 -4.1193209 -4.1031871 -4.0850019 -4.0624228 -4.0266266 -3.9877892 -3.9817362 -4.0386109 -4.0848789 -4.1017981 -4.1212606 -4.1481891 -4.1713591 -4.2034645][-4.1799922 -4.1708851 -4.1625676 -4.1531777 -4.1409078 -4.1232367 -4.1048708 -4.1023564 -4.1358633 -4.1646404 -4.1754103 -4.1872125 -4.2063742 -4.222023 -4.2444692][-4.2128015 -4.2103491 -4.2081771 -4.2037373 -4.1965384 -4.1907086 -4.1831203 -4.1817646 -4.1980529 -4.21473 -4.2219205 -4.22795 -4.2386513 -4.2429886 -4.2505217][-4.241931 -4.239027 -4.2382221 -4.2360115 -4.2325878 -4.2316108 -4.2282805 -4.2266674 -4.2317142 -4.2399635 -4.2428651 -4.2415681 -4.2387223 -4.228507 -4.2235174][-4.2721014 -4.2667918 -4.26334 -4.2608247 -4.2588148 -4.2585812 -4.2563481 -4.2528925 -4.2521358 -4.2531495 -4.249526 -4.2407 -4.2253108 -4.2042956 -4.1883545][-4.2983251 -4.2939219 -4.2898917 -4.2880011 -4.2858286 -4.2836752 -4.2811351 -4.2792249 -4.2782707 -4.2773037 -4.2713757 -4.259037 -4.2377744 -4.2085795 -4.1815448]]...]
INFO - root - 2017-12-07 10:24:12.359782: step 4410, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.993 sec/batch; 90h:32m:25s remains)
INFO - root - 2017-12-07 10:24:22.098469: step 4420, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 87h:25m:46s remains)
INFO - root - 2017-12-07 10:24:31.791894: step 4430, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 90h:17m:37s remains)
INFO - root - 2017-12-07 10:24:41.697872: step 4440, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.019 sec/batch; 92h:52m:42s remains)
INFO - root - 2017-12-07 10:24:51.400260: step 4450, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 86h:40m:15s remains)
INFO - root - 2017-12-07 10:25:00.946945: step 4460, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 84h:40m:46s remains)
INFO - root - 2017-12-07 10:25:10.511997: step 4470, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 88h:11m:44s remains)
INFO - root - 2017-12-07 10:25:20.095721: step 4480, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 89h:10m:38s remains)
INFO - root - 2017-12-07 10:25:29.829659: step 4490, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 86h:12m:28s remains)
INFO - root - 2017-12-07 10:25:39.349649: step 4500, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 83h:19m:02s remains)
2017-12-07 10:25:40.299025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3368335 -4.3324375 -4.32976 -4.3229146 -4.3099909 -4.3013806 -4.2910109 -4.2775035 -4.2792697 -4.2857008 -4.2850442 -4.2898459 -4.3027263 -4.3149056 -4.3226051][-4.3430872 -4.3390203 -4.3356638 -4.3243027 -4.3040938 -4.2875838 -4.2658672 -4.2420049 -4.24381 -4.2561536 -4.2566323 -4.268085 -4.290648 -4.3076558 -4.3190174][-4.3483496 -4.3424482 -4.3348994 -4.3148303 -4.2845511 -4.2568359 -4.2190514 -4.1838279 -4.1917 -4.2218122 -4.2335324 -4.2521839 -4.2820864 -4.3060193 -4.3212605][-4.3433976 -4.3387389 -4.3312006 -4.3060765 -4.2650337 -4.2224536 -4.1642661 -4.1179476 -4.1396432 -4.1958704 -4.2214074 -4.2432957 -4.2750559 -4.30301 -4.3222232][-4.3334274 -4.3282967 -4.3174877 -4.2841659 -4.232739 -4.1748304 -4.093678 -4.0400052 -4.0807714 -4.1629596 -4.2024322 -4.2295046 -4.2652459 -4.2975621 -4.3202424][-4.32101 -4.3142557 -4.3008132 -4.2581658 -4.1958461 -4.1165318 -4.0041113 -3.944108 -4.0173364 -4.1251559 -4.1788759 -4.2142558 -4.2571483 -4.2947016 -4.3185506][-4.3112941 -4.3063579 -4.2913117 -4.2397738 -4.1689062 -4.0632458 -3.9059205 -3.8346577 -3.9507754 -4.0889444 -4.1572542 -4.2017069 -4.2530708 -4.297586 -4.3235569][-4.3079095 -4.30977 -4.2966619 -4.2486157 -4.1788216 -4.0620351 -3.8773687 -3.7938144 -3.9364665 -4.091188 -4.1636734 -4.2107029 -4.2630281 -4.3088121 -4.3338962][-4.3072023 -4.3186531 -4.3090487 -4.2684956 -4.2097855 -4.1118193 -3.9532259 -3.8759532 -3.9949775 -4.1299353 -4.1923518 -4.2362967 -4.281085 -4.318614 -4.3385205][-4.3034687 -4.3137097 -4.3074126 -4.2786369 -4.2380781 -4.1682315 -4.0577197 -4.01116 -4.0909762 -4.1822057 -4.2234831 -4.2581778 -4.2915659 -4.3186255 -4.331974][-4.2964754 -4.3001637 -4.2926321 -4.2727647 -4.246232 -4.1978493 -4.1285896 -4.1084661 -4.1659551 -4.2288175 -4.2567306 -4.2765679 -4.2974887 -4.3159533 -4.3236694][-4.2814865 -4.2815771 -4.2761455 -4.2619123 -4.2427611 -4.2093349 -4.1654367 -4.1594067 -4.2065783 -4.2585921 -4.2822838 -4.2896647 -4.2961516 -4.3046083 -4.3068][-4.2613511 -4.26435 -4.2666492 -4.2576222 -4.2414532 -4.220089 -4.1969819 -4.197762 -4.22863 -4.2644982 -4.2830629 -4.2839918 -4.2818136 -4.2837529 -4.2810464][-4.2470455 -4.2531624 -4.259532 -4.2530804 -4.2416415 -4.2324038 -4.2256227 -4.2310605 -4.2458081 -4.2626543 -4.2720122 -4.2695684 -4.2640848 -4.2643275 -4.2607532][-4.2472444 -4.259728 -4.2657018 -4.2587023 -4.2499485 -4.2449851 -4.2454844 -4.2523985 -4.2598038 -4.2644339 -4.2657213 -4.2599454 -4.2529221 -4.2489719 -4.2404718]]...]
INFO - root - 2017-12-07 10:25:49.761792: step 4510, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.016 sec/batch; 92h:33m:18s remains)
INFO - root - 2017-12-07 10:25:59.466416: step 4520, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.018 sec/batch; 92h:45m:53s remains)
INFO - root - 2017-12-07 10:26:09.059331: step 4530, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 85h:44m:38s remains)
INFO - root - 2017-12-07 10:26:18.705134: step 4540, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 88h:36m:55s remains)
INFO - root - 2017-12-07 10:26:28.520948: step 4550, loss = 2.07, batch loss = 2.02 (7.8 examples/sec; 1.024 sec/batch; 93h:15m:55s remains)
INFO - root - 2017-12-07 10:26:38.059114: step 4560, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.935 sec/batch; 85h:11m:08s remains)
INFO - root - 2017-12-07 10:26:47.665323: step 4570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 85h:08m:39s remains)
INFO - root - 2017-12-07 10:26:57.350346: step 4580, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 89h:45m:58s remains)
INFO - root - 2017-12-07 10:27:07.190420: step 4590, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 91h:25m:09s remains)
INFO - root - 2017-12-07 10:27:16.852929: step 4600, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 87h:00m:40s remains)
2017-12-07 10:27:17.873325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3121185 -4.3105421 -4.3085742 -4.3042908 -4.2999382 -4.2960091 -4.2950935 -4.2998857 -4.3046217 -4.3040938 -4.2989335 -4.2958221 -4.2979355 -4.3041134 -4.3086009][-4.2870693 -4.2857413 -4.2826104 -4.275598 -4.267313 -4.2592282 -4.2590833 -4.2676086 -4.2738585 -4.2721362 -4.2631006 -4.2561855 -4.2564006 -4.2640166 -4.2702522][-4.268322 -4.2677422 -4.26322 -4.2523351 -4.2396941 -4.2292461 -4.2300258 -4.2399006 -4.2470264 -4.2443132 -4.230556 -4.2195282 -4.218646 -4.22795 -4.2352691][-4.2472043 -4.2479362 -4.2442122 -4.23071 -4.2156053 -4.2065139 -4.2081256 -4.21401 -4.2160778 -4.213624 -4.2019563 -4.1931429 -4.1972446 -4.2079124 -4.2145448][-4.2168083 -4.2197495 -4.2199311 -4.2066884 -4.1914158 -4.1840878 -4.1825762 -4.1722584 -4.1608806 -4.1609235 -4.1607718 -4.165936 -4.1825504 -4.1952252 -4.1992741][-4.1767859 -4.1810784 -4.1853437 -4.1718593 -4.1542015 -4.1472449 -4.1382637 -4.1066494 -4.0753093 -4.0764637 -4.0978332 -4.1254539 -4.1568065 -4.174541 -4.1771803][-4.1399436 -4.1456351 -4.1506996 -4.1352086 -4.1135449 -4.1026273 -4.0833082 -4.0290914 -3.9692194 -3.9725196 -4.0275369 -4.0834756 -4.1268516 -4.1470795 -4.15016][-4.135 -4.1404753 -4.1438174 -4.1277661 -4.1048508 -4.08953 -4.0621076 -3.9949071 -3.9231646 -3.9312506 -4.0102682 -4.0835142 -4.1265459 -4.1430588 -4.1416755][-4.1694736 -4.177156 -4.1829019 -4.1693492 -4.1504607 -4.1342258 -4.1054296 -4.0457773 -3.9908309 -4.0029206 -4.0722694 -4.1334963 -4.1636052 -4.1637506 -4.151083][-4.1945295 -4.2077708 -4.2216382 -4.2169862 -4.2052073 -4.1906466 -4.16283 -4.1170592 -4.0775394 -4.0860815 -4.1368732 -4.1811967 -4.1939821 -4.1797171 -4.1571736][-4.1815572 -4.2006679 -4.2230072 -4.2284851 -4.2246456 -4.2147393 -4.1893544 -4.1501212 -4.1178169 -4.1234746 -4.1624379 -4.195271 -4.2018223 -4.187685 -4.16495][-4.1578188 -4.1786084 -4.2063937 -4.2204266 -4.224009 -4.2178493 -4.1927156 -4.1567149 -4.1328616 -4.1406908 -4.1728311 -4.1996222 -4.2079859 -4.1988955 -4.1787724][-4.1566706 -4.1728687 -4.1985717 -4.2141 -4.2210894 -4.2143297 -4.1886959 -4.1568832 -4.1420193 -4.1535211 -4.1806483 -4.2058377 -4.2161636 -4.2107205 -4.195118][-4.1669445 -4.1797085 -4.1966228 -4.2084126 -4.2160568 -4.2054768 -4.1777706 -4.1500897 -4.1437588 -4.1598072 -4.1865273 -4.2118487 -4.22235 -4.2179179 -4.2033982][-4.1745472 -4.1870513 -4.1915879 -4.1944337 -4.1980443 -4.1801991 -4.1454873 -4.1230116 -4.130023 -4.1524878 -4.1797924 -4.2064915 -4.21604 -4.2082791 -4.1963725]]...]
INFO - root - 2017-12-07 10:27:27.398364: step 4610, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.979 sec/batch; 89h:11m:30s remains)
INFO - root - 2017-12-07 10:27:37.086702: step 4620, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 89h:15m:52s remains)
INFO - root - 2017-12-07 10:27:46.754138: step 4630, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 81h:16m:52s remains)
INFO - root - 2017-12-07 10:27:56.241488: step 4640, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 87h:32m:09s remains)
INFO - root - 2017-12-07 10:28:05.735006: step 4650, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 90h:54m:23s remains)
INFO - root - 2017-12-07 10:28:15.505091: step 4660, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 88h:25m:13s remains)
INFO - root - 2017-12-07 10:28:25.359821: step 4670, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 89h:51m:34s remains)
INFO - root - 2017-12-07 10:28:35.031772: step 4680, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 85h:05m:30s remains)
INFO - root - 2017-12-07 10:28:44.809755: step 4690, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 87h:10m:35s remains)
INFO - root - 2017-12-07 10:28:54.532486: step 4700, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 90h:41m:43s remains)
2017-12-07 10:28:55.384527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2660828 -4.2799215 -4.3013778 -4.30521 -4.2864819 -4.2703075 -4.2730074 -4.2852974 -4.2953773 -4.2920508 -4.2677317 -4.2369328 -4.2322474 -4.2535658 -4.2629724][-4.2673721 -4.291234 -4.3153663 -4.3136997 -4.2839489 -4.2605028 -4.2630515 -4.274889 -4.2839 -4.2853971 -4.2701917 -4.2444644 -4.2437711 -4.2698064 -4.2844396][-4.245831 -4.2833552 -4.3141041 -4.3120365 -4.2746062 -4.2432022 -4.2472076 -4.2660847 -4.277648 -4.2785721 -4.2619076 -4.2409573 -4.2433262 -4.27337 -4.29159][-4.2164083 -4.2653356 -4.3025684 -4.301651 -4.2562194 -4.2134991 -4.2182231 -4.2487693 -4.2687383 -4.2675776 -4.2480531 -4.2355237 -4.2442684 -4.275269 -4.2965574][-4.2060027 -4.25499 -4.2916141 -4.2811718 -4.2151279 -4.1510911 -4.1584187 -4.2126474 -4.2497978 -4.2520456 -4.233367 -4.2269154 -4.2422256 -4.2777343 -4.3043456][-4.2124066 -4.255734 -4.2792006 -4.2414947 -4.1393938 -4.0486751 -4.0725651 -4.1620445 -4.2165504 -4.222116 -4.2010131 -4.1978579 -4.2209783 -4.2659636 -4.3017678][-4.2233706 -4.2566051 -4.2590489 -4.1880164 -4.0483093 -3.9395564 -3.9895964 -4.1155915 -4.1840968 -4.185761 -4.1549411 -4.1501145 -4.1812172 -4.2373023 -4.2856984][-4.2462153 -4.2699971 -4.2552652 -4.1680026 -4.0204363 -3.9183457 -3.9750097 -4.1084809 -4.1755204 -4.1668458 -4.1250153 -4.119452 -4.1610293 -4.2276073 -4.2810006][-4.2790117 -4.2937055 -4.2702293 -4.1881628 -4.0616374 -3.9765706 -4.0201006 -4.1338058 -4.1913371 -4.1734734 -4.1271071 -4.122509 -4.1680527 -4.2369804 -4.2907109][-4.3089209 -4.316124 -4.289197 -4.2192044 -4.1183424 -4.0463128 -4.0729728 -4.1643825 -4.2114658 -4.1884718 -4.1392241 -4.1341786 -4.1829967 -4.2530041 -4.3018808][-4.3214679 -4.3238811 -4.2986064 -4.2426939 -4.1677847 -4.1117659 -4.1257639 -4.1960282 -4.2344151 -4.2112617 -4.1617327 -4.1515832 -4.1994524 -4.2662597 -4.3077826][-4.3190212 -4.3191805 -4.3007627 -4.2614136 -4.211205 -4.174788 -4.1811543 -4.2287784 -4.2581024 -4.2406073 -4.1935544 -4.17728 -4.2198725 -4.27922 -4.3132143][-4.3144197 -4.3148394 -4.3035512 -4.2788496 -4.2490449 -4.2300591 -4.2335129 -4.2636986 -4.2804208 -4.265152 -4.2256346 -4.2114153 -4.2473607 -4.295279 -4.321207][-4.318583 -4.3202705 -4.3138261 -4.3010778 -4.2835178 -4.2738733 -4.2762737 -4.2942624 -4.2992 -4.2835155 -4.2561808 -4.2501121 -4.2783513 -4.311058 -4.3271632][-4.3263125 -4.328949 -4.3278055 -4.3230596 -4.3123822 -4.305069 -4.3059311 -4.3156133 -4.3143597 -4.2995276 -4.2824087 -4.2820444 -4.3008337 -4.3197112 -4.328742]]...]
INFO - root - 2017-12-07 10:29:05.091187: step 4710, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 89h:27m:31s remains)
INFO - root - 2017-12-07 10:29:14.849387: step 4720, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.009 sec/batch; 91h:51m:29s remains)
INFO - root - 2017-12-07 10:29:24.634039: step 4730, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 84h:08m:51s remains)
INFO - root - 2017-12-07 10:29:34.241217: step 4740, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 85h:22m:26s remains)
INFO - root - 2017-12-07 10:29:43.769431: step 4750, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 81h:26m:04s remains)
INFO - root - 2017-12-07 10:29:53.293840: step 4760, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 87h:57m:07s remains)
INFO - root - 2017-12-07 10:30:02.951419: step 4770, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 87h:23m:16s remains)
INFO - root - 2017-12-07 10:30:12.503289: step 4780, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 89h:38m:45s remains)
INFO - root - 2017-12-07 10:30:22.107455: step 4790, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 88h:10m:08s remains)
INFO - root - 2017-12-07 10:30:31.807527: step 4800, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 88h:49m:13s remains)
2017-12-07 10:30:32.660602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2115459 -4.2127538 -4.2198219 -4.2282877 -4.2310648 -4.2260652 -4.2195044 -4.2216158 -4.229775 -4.2367315 -4.2358012 -4.2320375 -4.2363796 -4.2517085 -4.2701478][-4.2241459 -4.2195578 -4.2201114 -4.2262845 -4.2314849 -4.2325416 -4.2326064 -4.2361827 -4.2433152 -4.2492166 -4.2482672 -4.2484927 -4.2575474 -4.272429 -4.2861681][-4.2390008 -4.2326293 -4.2314796 -4.2370205 -4.243216 -4.2456117 -4.2454062 -4.2462964 -4.2497354 -4.2507486 -4.2462721 -4.246623 -4.257236 -4.2716641 -4.281961][-4.252903 -4.2466431 -4.2476797 -4.25409 -4.2594981 -4.2578635 -4.250062 -4.2434731 -4.2417417 -4.2379 -4.2298465 -4.2266221 -4.2348886 -4.2476888 -4.2555752][-4.2539797 -4.2489586 -4.2521081 -4.2596016 -4.2630005 -4.2515473 -4.2277765 -4.2083459 -4.2026138 -4.1993237 -4.1920295 -4.1860294 -4.1923122 -4.2041917 -4.2122822][-4.2342916 -4.2292142 -4.2308021 -4.2390637 -4.2389193 -4.213088 -4.1672745 -4.1355286 -4.133986 -4.1410327 -4.1400647 -4.1348724 -4.1422896 -4.1550932 -4.1653748][-4.1980658 -4.1930218 -4.1926751 -4.2003837 -4.1952214 -4.1509156 -4.078619 -4.0363941 -4.0520692 -4.0838265 -4.0961137 -4.0983868 -4.1127138 -4.1283507 -4.139123][-4.1546369 -4.15287 -4.152503 -4.1603265 -4.1520758 -4.0927119 -3.9954782 -3.9449592 -3.9867878 -4.0467582 -4.0733514 -4.0858583 -4.1072659 -4.1268792 -4.1395655][-4.1170034 -4.1181283 -4.1158032 -4.1262765 -4.1268883 -4.0743322 -3.9803548 -3.9361653 -3.9930029 -4.0624976 -4.0927134 -4.1076269 -4.12731 -4.145998 -4.1590261][-4.0922995 -4.092226 -4.0905347 -4.1111593 -4.1335258 -4.1098137 -4.0515728 -4.0297976 -4.078383 -4.132268 -4.1560936 -4.1659937 -4.1751046 -4.1841455 -4.1918221][-4.0830855 -4.0756373 -4.0780945 -4.1140757 -4.1558323 -4.159164 -4.1357193 -4.129281 -4.1619115 -4.1989932 -4.2192335 -4.227355 -4.2311716 -4.2327929 -4.2342877][-4.0938325 -4.0787745 -4.0889459 -4.1350985 -4.1830435 -4.1985717 -4.1945786 -4.1951923 -4.2154169 -4.2413235 -4.2598224 -4.2702308 -4.2752814 -4.2762017 -4.275012][-4.1187468 -4.1039624 -4.1223044 -4.1682358 -4.2093534 -4.2253566 -4.2281842 -4.2324615 -4.2471337 -4.2665105 -4.2826257 -4.2941403 -4.300632 -4.3015342 -4.2996283][-4.16072 -4.1497431 -4.1681581 -4.2019444 -4.2292576 -4.24033 -4.2444854 -4.2503428 -4.2619324 -4.275043 -4.2867746 -4.2957358 -4.3011713 -4.3003664 -4.2961612][-4.1970711 -4.1900172 -4.2042775 -4.2262645 -4.242229 -4.2490988 -4.2535543 -4.25902 -4.2647333 -4.2675395 -4.2707834 -4.2766824 -4.2808385 -4.2769442 -4.2693844]]...]
INFO - root - 2017-12-07 10:30:42.163457: step 4810, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 85h:57m:49s remains)
INFO - root - 2017-12-07 10:30:51.780828: step 4820, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 89h:48m:40s remains)
INFO - root - 2017-12-07 10:31:01.450558: step 4830, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 86h:45m:19s remains)
INFO - root - 2017-12-07 10:31:11.169239: step 4840, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 87h:40m:06s remains)
INFO - root - 2017-12-07 10:31:20.805866: step 4850, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 89h:04m:31s remains)
INFO - root - 2017-12-07 10:31:30.239117: step 4860, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 86h:29m:49s remains)
INFO - root - 2017-12-07 10:31:39.965369: step 4870, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 90h:10m:15s remains)
INFO - root - 2017-12-07 10:31:49.616691: step 4880, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 87h:06m:38s remains)
INFO - root - 2017-12-07 10:31:59.205613: step 4890, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.970 sec/batch; 88h:14m:42s remains)
INFO - root - 2017-12-07 10:32:08.820830: step 4900, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 88h:30m:28s remains)
2017-12-07 10:32:09.708673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2144542 -4.2466578 -4.2513204 -4.2239861 -4.1938119 -4.1851482 -4.1908703 -4.2122259 -4.2489414 -4.2890172 -4.3185282 -4.3307967 -4.3253894 -4.2902851 -4.2603612][-4.2253275 -4.258759 -4.2635994 -4.2341704 -4.1995611 -4.1846895 -4.1904697 -4.2184534 -4.2614884 -4.3012643 -4.3250375 -4.3363853 -4.3335104 -4.2996283 -4.2649369][-4.217186 -4.2530141 -4.2602987 -4.2300682 -4.1906867 -4.1691051 -4.1768494 -4.2156057 -4.2663784 -4.3073587 -4.3257146 -4.332437 -4.3307285 -4.3005719 -4.2647977][-4.1939721 -4.2343307 -4.2438788 -4.2133279 -4.1684632 -4.1397233 -4.1488242 -4.1994953 -4.2603788 -4.3048558 -4.3221354 -4.3262286 -4.3238173 -4.2936869 -4.2565422][-4.1692786 -4.2106419 -4.2225733 -4.1940985 -4.1439328 -4.1045871 -4.1056218 -4.1641512 -4.2378554 -4.2890091 -4.3122044 -4.3162003 -4.3085504 -4.2694983 -4.2251706][-4.1610169 -4.1969037 -4.2097974 -4.1876292 -4.1359825 -4.0803642 -4.0583944 -4.1142259 -4.2012763 -4.2648749 -4.2980962 -4.3053412 -4.2911549 -4.2405362 -4.1847463][-4.175108 -4.1993623 -4.2088556 -4.1940184 -4.1482048 -4.0842032 -4.0420389 -4.0887451 -4.1792231 -4.250782 -4.2926574 -4.3015323 -4.2822356 -4.2259641 -4.1614227][-4.1895261 -4.1984787 -4.1970186 -4.1830249 -4.14766 -4.0955014 -4.0619559 -4.1041665 -4.1843691 -4.2514462 -4.2939587 -4.3041577 -4.2846065 -4.2312131 -4.1670547][-4.1870651 -4.1777296 -4.1623206 -4.1441174 -4.1217303 -4.0939865 -4.0870361 -4.1310315 -4.1986175 -4.2578983 -4.295742 -4.3051205 -4.2889996 -4.2464342 -4.1936307][-4.1677184 -4.1444411 -4.1195326 -4.099247 -4.0889621 -4.0900326 -4.1126604 -4.1593142 -4.2160792 -4.2640181 -4.293582 -4.2989812 -4.2865171 -4.2559552 -4.2197642][-4.1465592 -4.1189084 -4.092474 -4.0749907 -4.0742579 -4.097455 -4.1359878 -4.1821175 -4.2312522 -4.267693 -4.2865343 -4.2866893 -4.2753162 -4.2568212 -4.239397][-4.14699 -4.119647 -4.09473 -4.0796785 -4.0850835 -4.1185923 -4.164 -4.2079053 -4.2493992 -4.2741594 -4.2828493 -4.2788095 -4.2693944 -4.260747 -4.2567034][-4.169208 -4.1452484 -4.1237292 -4.1098886 -4.1173477 -4.1520386 -4.1963243 -4.2356339 -4.26866 -4.2844024 -4.2858381 -4.2785678 -4.2689629 -4.2630534 -4.2648549][-4.2071118 -4.1902175 -4.1741643 -4.1630931 -4.1690817 -4.1971831 -4.2327724 -4.2616458 -4.2848206 -4.2943292 -4.2919426 -4.2861433 -4.2781954 -4.273447 -4.2763691][-4.2422733 -4.232933 -4.2246995 -4.2189178 -4.2216578 -4.2380919 -4.2606015 -4.2774863 -4.2905884 -4.294107 -4.2895131 -4.2838149 -4.2764664 -4.27261 -4.2775869]]...]
INFO - root - 2017-12-07 10:32:19.496497: step 4910, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 90h:18m:54s remains)
INFO - root - 2017-12-07 10:32:29.068715: step 4920, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 90h:00m:57s remains)
INFO - root - 2017-12-07 10:32:38.653116: step 4930, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 89h:11m:06s remains)
INFO - root - 2017-12-07 10:32:48.262247: step 4940, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 90h:40m:11s remains)
INFO - root - 2017-12-07 10:32:57.799241: step 4950, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 87h:05m:15s remains)
INFO - root - 2017-12-07 10:33:07.504305: step 4960, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 85h:15m:49s remains)
INFO - root - 2017-12-07 10:33:17.148054: step 4970, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 90h:06m:49s remains)
INFO - root - 2017-12-07 10:33:26.748581: step 4980, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.918 sec/batch; 83h:31m:41s remains)
INFO - root - 2017-12-07 10:33:36.291799: step 4990, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 87h:49m:41s remains)
INFO - root - 2017-12-07 10:33:45.723563: step 5000, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 86h:05m:56s remains)
2017-12-07 10:33:46.655133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2552567 -4.2656417 -4.2540374 -4.2053986 -4.1468835 -4.1135616 -4.1200137 -4.1521254 -4.1823015 -4.2105107 -4.2337823 -4.2523255 -4.271358 -4.2874236 -4.2859039][-4.2491879 -4.26319 -4.2561336 -4.214592 -4.1594753 -4.1310048 -4.1388907 -4.1633072 -4.1836805 -4.2009482 -4.2177014 -4.2372293 -4.25811 -4.2740669 -4.2775569][-4.2331858 -4.2541075 -4.2496915 -4.2134523 -4.1683908 -4.1478786 -4.1525879 -4.1673751 -4.1783552 -4.1860023 -4.2029238 -4.2272015 -4.2481346 -4.2645669 -4.2741818][-4.2053695 -4.2377996 -4.2439704 -4.2183719 -4.1850748 -4.165782 -4.1529303 -4.1551423 -4.1661806 -4.1714115 -4.192996 -4.2205758 -4.2416449 -4.2581587 -4.2712669][-4.1754165 -4.2220559 -4.24147 -4.2246952 -4.1990585 -4.1718946 -4.1288433 -4.1188893 -4.1419754 -4.1569104 -4.1777067 -4.2030478 -4.223032 -4.2413921 -4.2610016][-4.1682897 -4.2175274 -4.237524 -4.2182536 -4.1852183 -4.1332808 -4.0500312 -4.0378027 -4.0933623 -4.1342344 -4.1629105 -4.186811 -4.21115 -4.2352877 -4.2573676][-4.1747327 -4.2177815 -4.228581 -4.1957517 -4.1414814 -4.0452456 -3.9085002 -3.9125419 -4.0219626 -4.10078 -4.1432743 -4.1615696 -4.1883745 -4.2177176 -4.243073][-4.1897492 -4.2185478 -4.217329 -4.1685119 -4.0881104 -3.9415057 -3.7550821 -3.7943971 -3.9636014 -4.077683 -4.12887 -4.1339025 -4.1517425 -4.1804843 -4.2108736][-4.226707 -4.2405195 -4.222755 -4.1644983 -4.0869231 -3.9522271 -3.7913013 -3.8394217 -3.9975348 -4.0978956 -4.1300545 -4.1069579 -4.1024055 -4.1237807 -4.1564994][-4.2723389 -4.2764049 -4.250515 -4.1941957 -4.1376657 -4.0520291 -3.9503248 -3.9773116 -4.0812869 -4.1429033 -4.150537 -4.1147556 -4.0935278 -4.1024904 -4.12833][-4.2938194 -4.2932038 -4.2727022 -4.2305641 -4.19102 -4.138392 -4.0776324 -4.0843983 -4.1426611 -4.1753893 -4.1678762 -4.1311221 -4.1063576 -4.1159849 -4.1363411][-4.3025141 -4.3017077 -4.2905774 -4.2617345 -4.2341838 -4.204505 -4.1679983 -4.1613674 -4.1887994 -4.2027745 -4.1890182 -4.15888 -4.131619 -4.1362886 -4.1495581][-4.3081098 -4.3094387 -4.3070107 -4.2879047 -4.2666631 -4.2527237 -4.2342334 -4.2236128 -4.2334409 -4.2366405 -4.2255068 -4.2053089 -4.1784148 -4.1688557 -4.1700268][-4.3133059 -4.3140821 -4.3127031 -4.3000407 -4.285203 -4.2799788 -4.2715535 -4.2638359 -4.2669272 -4.2651911 -4.2589726 -4.2499971 -4.2311325 -4.213923 -4.20638][-4.3247313 -4.3225532 -4.318821 -4.308063 -4.2957435 -4.2916784 -4.2882214 -4.2853951 -4.2864728 -4.2871122 -4.2859149 -4.2831459 -4.272481 -4.2564678 -4.2480979]]...]
INFO - root - 2017-12-07 10:33:56.263541: step 5010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 83h:46m:27s remains)
INFO - root - 2017-12-07 10:34:05.903542: step 5020, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 87h:03m:51s remains)
INFO - root - 2017-12-07 10:34:15.595964: step 5030, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 83h:22m:06s remains)
INFO - root - 2017-12-07 10:34:25.201522: step 5040, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 91h:07m:39s remains)
INFO - root - 2017-12-07 10:34:34.858649: step 5050, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.014 sec/batch; 92h:16m:33s remains)
INFO - root - 2017-12-07 10:34:44.518846: step 5060, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 83h:36m:17s remains)
INFO - root - 2017-12-07 10:34:54.118103: step 5070, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 85h:24m:16s remains)
INFO - root - 2017-12-07 10:35:03.838745: step 5080, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 90h:05m:20s remains)
INFO - root - 2017-12-07 10:35:13.546564: step 5090, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 88h:07m:50s remains)
INFO - root - 2017-12-07 10:35:23.236785: step 5100, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 88h:37m:32s remains)
2017-12-07 10:35:24.154343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2844448 -4.2883887 -4.2949271 -4.2870603 -4.2728148 -4.2552624 -4.2422328 -4.2205548 -4.2014956 -4.1840134 -4.1793895 -4.1801558 -4.1835241 -4.2027426 -4.2348652][-4.2900991 -4.2962785 -4.3002105 -4.2905226 -4.2791653 -4.2687435 -4.25492 -4.2240934 -4.1939654 -4.1670742 -4.1539726 -4.157485 -4.1723428 -4.200995 -4.2362967][-4.2844186 -4.2940841 -4.2937474 -4.2855625 -4.2777758 -4.2731476 -4.2558703 -4.2166419 -4.1768408 -4.1460195 -4.1302972 -4.1413121 -4.1674361 -4.205802 -4.2423186][-4.2681046 -4.2831655 -4.2864785 -4.2820678 -4.2769771 -4.2695723 -4.2464571 -4.204267 -4.1654153 -4.13556 -4.1207891 -4.13802 -4.1744108 -4.2176585 -4.2519283][-4.258729 -4.2739763 -4.2797666 -4.2787657 -4.2740049 -4.258378 -4.2259831 -4.1904979 -4.162611 -4.13999 -4.1295934 -4.1462741 -4.1835408 -4.2231932 -4.2528715][-4.2531524 -4.2648506 -4.2730021 -4.2748823 -4.2643819 -4.2341561 -4.1885805 -4.1596403 -4.1474991 -4.1410379 -4.1400728 -4.156651 -4.1868186 -4.2172236 -4.2411289][-4.2432055 -4.2547517 -4.2609272 -4.2517586 -4.2226343 -4.170588 -4.11439 -4.0931268 -4.1051121 -4.1221561 -4.1380024 -4.1598444 -4.1832519 -4.2048163 -4.2223511][-4.2429171 -4.2539525 -4.2486911 -4.2155132 -4.1587892 -4.0802817 -4.0047398 -3.9913409 -4.0341082 -4.0835748 -4.1251426 -4.1546721 -4.1737232 -4.1869817 -4.1961718][-4.2496557 -4.2572255 -4.2398229 -4.1835546 -4.0946822 -3.9843092 -3.8922839 -3.898102 -3.9743977 -4.0547156 -4.1150384 -4.1475134 -4.1612411 -4.1656842 -4.1645074][-4.2563057 -4.2548952 -4.2278109 -4.159059 -4.0509439 -3.9300711 -3.857331 -3.8964357 -3.9869695 -4.0670986 -4.1219139 -4.1522775 -4.1614504 -4.1589885 -4.14945][-4.2630162 -4.2500076 -4.212039 -4.1444125 -4.0465727 -3.9578676 -3.9334528 -3.9928603 -4.066277 -4.1206613 -4.1558213 -4.1765962 -4.1816339 -4.1769886 -4.167397][-4.2738733 -4.2435493 -4.19697 -4.1371207 -4.0666189 -4.0190787 -4.03152 -4.0967965 -4.1540771 -4.1886349 -4.2087979 -4.2207532 -4.2226777 -4.220304 -4.216598][-4.2871308 -4.2404346 -4.1843867 -4.1221981 -4.0711694 -4.0548315 -4.0921764 -4.1576619 -4.206171 -4.2327108 -4.2470055 -4.256134 -4.260397 -4.2644868 -4.2669849][-4.2888923 -4.2280235 -4.1576724 -4.0909452 -4.0544596 -4.0642295 -4.1193066 -4.1816444 -4.2219133 -4.2463727 -4.2590585 -4.269321 -4.2778354 -4.287528 -4.2956457][-4.2729115 -4.2002053 -4.1159778 -4.0467486 -4.0253282 -4.0583496 -4.1287732 -4.1883779 -4.2221723 -4.2432542 -4.2561555 -4.2693563 -4.2817583 -4.29468 -4.3058829]]...]
INFO - root - 2017-12-07 10:35:33.812079: step 5110, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 91h:08m:02s remains)
INFO - root - 2017-12-07 10:35:43.253951: step 5120, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 88h:52m:48s remains)
INFO - root - 2017-12-07 10:35:52.938717: step 5130, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 83h:34m:39s remains)
INFO - root - 2017-12-07 10:36:02.619369: step 5140, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 81h:12m:29s remains)
INFO - root - 2017-12-07 10:36:12.319468: step 5150, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.984 sec/batch; 89h:30m:34s remains)
INFO - root - 2017-12-07 10:36:21.971563: step 5160, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 86h:56m:56s remains)
INFO - root - 2017-12-07 10:36:31.592778: step 5170, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 89h:37m:42s remains)
INFO - root - 2017-12-07 10:36:41.170173: step 5180, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 84h:09m:44s remains)
INFO - root - 2017-12-07 10:36:50.702641: step 5190, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 79h:46m:12s remains)
INFO - root - 2017-12-07 10:37:00.385043: step 5200, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.020 sec/batch; 92h:44m:57s remains)
2017-12-07 10:37:01.267801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3070407 -4.2933474 -4.2846403 -4.2774053 -4.2686281 -4.2514744 -4.2282124 -4.2052522 -4.1963873 -4.2037024 -4.2146754 -4.2310133 -4.2315316 -4.2304435 -4.2224836][-4.3065419 -4.2925463 -4.2847204 -4.2798128 -4.27388 -4.2586555 -4.2318296 -4.2058139 -4.1996574 -4.2084403 -4.2130804 -4.2184739 -4.2079659 -4.203012 -4.1977186][-4.3052964 -4.2924066 -4.284409 -4.2793074 -4.2736816 -4.2582054 -4.2255411 -4.1937418 -4.1854324 -4.1951761 -4.1997814 -4.2004991 -4.1856112 -4.1802626 -4.1817536][-4.3045268 -4.2934709 -4.2838993 -4.2735443 -4.2640181 -4.2473397 -4.210568 -4.1704965 -4.1550746 -4.1671839 -4.1795988 -4.1830134 -4.169148 -4.1643057 -4.1702228][-4.2989092 -4.2892542 -4.2789884 -4.2644663 -4.2501006 -4.2312956 -4.1933455 -4.1455436 -4.1235428 -4.1401329 -4.1663852 -4.1805806 -4.1693482 -4.162025 -4.1659203][-4.2886209 -4.2806454 -4.2703137 -4.252296 -4.2347846 -4.21444 -4.180172 -4.1300683 -4.10066 -4.1207714 -4.1592836 -4.1802144 -4.1706481 -4.1629558 -4.1656103][-4.2728605 -4.2623816 -4.2473173 -4.2230997 -4.2009792 -4.1824112 -4.1586051 -4.1174464 -4.0830922 -4.1007586 -4.1470394 -4.169199 -4.1575122 -4.1481962 -4.1481152][-4.2548962 -4.2382388 -4.2150521 -4.1839581 -4.1601591 -4.1483479 -4.1412544 -4.1170855 -4.0831308 -4.091074 -4.1348486 -4.1560469 -4.1418867 -4.1263623 -4.11883][-4.2385535 -4.2161379 -4.1895719 -4.1589446 -4.1383142 -4.1329122 -4.1390085 -4.1327949 -4.1056881 -4.1028476 -4.136301 -4.1549392 -4.1430736 -4.1287317 -4.1234508][-4.2283134 -4.2097073 -4.1931906 -4.1742225 -4.1637173 -4.1621308 -4.1703548 -4.1755972 -4.1615119 -4.1565843 -4.17773 -4.1911092 -4.1837544 -4.1766868 -4.1767154][-4.2237387 -4.2171235 -4.2186408 -4.21405 -4.2142458 -4.2176776 -4.227128 -4.2402658 -4.239027 -4.2358413 -4.2455187 -4.2533402 -4.2470737 -4.2400751 -4.2365303][-4.2245336 -4.229032 -4.2433667 -4.2488914 -4.2567606 -4.2652225 -4.2755551 -4.2914634 -4.2954388 -4.2938576 -4.2977195 -4.3021164 -4.2966428 -4.2881021 -4.2812753][-4.2430305 -4.24578 -4.2585325 -4.2655416 -4.2781115 -4.2885852 -4.298697 -4.3136463 -4.318315 -4.3165865 -4.3178368 -4.320972 -4.318378 -4.3113484 -4.303153][-4.2744608 -4.2672467 -4.2688451 -4.2743039 -4.2923679 -4.3049746 -4.3112 -4.32199 -4.32523 -4.322783 -4.3211541 -4.3195028 -4.3146882 -4.3095255 -4.302855][-4.3013244 -4.285038 -4.2769561 -4.2783117 -4.2948418 -4.3073359 -4.3085151 -4.3131962 -4.3145409 -4.3140588 -4.3117151 -4.3077564 -4.30256 -4.2990789 -4.2953663]]...]
INFO - root - 2017-12-07 10:37:10.959122: step 5210, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 89h:51m:50s remains)
INFO - root - 2017-12-07 10:37:20.616439: step 5220, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 90h:22m:18s remains)
INFO - root - 2017-12-07 10:37:30.295699: step 5230, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 88h:30m:12s remains)
INFO - root - 2017-12-07 10:37:39.852302: step 5240, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.952 sec/batch; 86h:34m:47s remains)
INFO - root - 2017-12-07 10:37:49.506741: step 5250, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 83h:54m:23s remains)
INFO - root - 2017-12-07 10:37:59.193536: step 5260, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 89h:46m:50s remains)
INFO - root - 2017-12-07 10:38:08.875304: step 5270, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 88h:09m:02s remains)
INFO - root - 2017-12-07 10:38:18.522850: step 5280, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 85h:16m:54s remains)
INFO - root - 2017-12-07 10:38:28.043825: step 5290, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 87h:39m:04s remains)
INFO - root - 2017-12-07 10:38:37.696563: step 5300, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 82h:13m:01s remains)
2017-12-07 10:38:38.727174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2959146 -4.2983804 -4.3028755 -4.3035579 -4.2962952 -4.2819633 -4.2736177 -4.2678571 -4.2644091 -4.2655025 -4.2668128 -4.2746925 -4.28575 -4.2962179 -4.3060942][-4.2788134 -4.28302 -4.2906218 -4.2921691 -4.2806807 -4.2582192 -4.2455707 -4.2383051 -4.2343826 -4.2349024 -4.2337666 -4.2423797 -4.2565832 -4.2707691 -4.2853017][-4.2514968 -4.2581964 -4.26788 -4.268961 -4.2542667 -4.2242184 -4.2067618 -4.1998158 -4.2008567 -4.2043023 -4.2018046 -4.2090859 -4.2241178 -4.2409835 -4.2580528][-4.2111392 -4.2205734 -4.2328577 -4.2347131 -4.2164493 -4.1809273 -4.1588268 -4.1540823 -4.1649446 -4.175703 -4.1729703 -4.1772594 -4.1910939 -4.207849 -4.2246614][-4.1712379 -4.1815019 -4.1966186 -4.2011847 -4.1812153 -4.1396456 -4.1085162 -4.1029892 -4.1225 -4.1433482 -4.1442347 -4.1475177 -4.162324 -4.1799145 -4.1958723][-4.1403966 -4.14867 -4.1629562 -4.1705413 -4.1505089 -4.1037121 -4.0642576 -4.0585136 -4.0852704 -4.115746 -4.1238718 -4.1284027 -4.14363 -4.1638494 -4.1796331][-4.1202559 -4.1288767 -4.1420283 -4.1511383 -4.1283903 -4.0717254 -4.0209889 -4.0149932 -4.0486608 -4.0887094 -4.1076207 -4.1161408 -4.1315031 -4.1533337 -4.1681833][-4.1205454 -4.1363645 -4.152422 -4.1602507 -4.130116 -4.0585947 -3.9907577 -3.9766743 -4.0138478 -4.06513 -4.098865 -4.1154995 -4.1334643 -4.1555076 -4.1675835][-4.1294837 -4.1532903 -4.1721115 -4.1791873 -4.1476731 -4.0745845 -4.0035572 -3.9868293 -4.0222917 -4.0705657 -4.107101 -4.1250858 -4.1413717 -4.1617947 -4.1749077][-4.1243134 -4.1534653 -4.1750193 -4.1826949 -4.1580186 -4.0990653 -4.0421071 -4.0311952 -4.0590858 -4.0931549 -4.1185122 -4.1279578 -4.1369529 -4.1537819 -4.169652][-4.1194491 -4.1464415 -4.1689186 -4.1798577 -4.1646614 -4.12385 -4.0845571 -4.0764446 -4.092361 -4.1121387 -4.1251116 -4.1274467 -4.1305981 -4.1450872 -4.1658354][-4.1188941 -4.1413269 -4.1613135 -4.1736517 -4.1675634 -4.1428022 -4.1189055 -4.1134024 -4.1226573 -4.1314359 -4.135643 -4.1357174 -4.1381116 -4.1526179 -4.1747065][-4.1186328 -4.1364374 -4.154593 -4.167778 -4.1677575 -4.1524973 -4.1392827 -4.1386962 -4.1482868 -4.152822 -4.1519136 -4.1508055 -4.1534352 -4.169313 -4.190443][-4.1238647 -4.1384583 -4.1526861 -4.1642227 -4.1666651 -4.1562986 -4.1498952 -4.1547823 -4.1666727 -4.1714506 -4.1693192 -4.1684446 -4.1721749 -4.1872692 -4.2060223][-4.1288309 -4.1423807 -4.1539631 -4.1629453 -4.1662722 -4.160821 -4.1596966 -4.1675072 -4.180625 -4.1871028 -4.1857548 -4.1868854 -4.1918373 -4.2059293 -4.2226915]]...]
INFO - root - 2017-12-07 10:38:48.336216: step 5310, loss = 2.12, batch loss = 2.06 (8.5 examples/sec; 0.938 sec/batch; 85h:12m:30s remains)
INFO - root - 2017-12-07 10:38:57.884361: step 5320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 87h:50m:19s remains)
INFO - root - 2017-12-07 10:39:07.463400: step 5330, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 86h:42m:26s remains)
INFO - root - 2017-12-07 10:39:16.971938: step 5340, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 84h:45m:13s remains)
INFO - root - 2017-12-07 10:39:26.530624: step 5350, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.026 sec/batch; 93h:16m:46s remains)
INFO - root - 2017-12-07 10:39:36.138582: step 5360, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 89h:12m:57s remains)
INFO - root - 2017-12-07 10:39:45.839440: step 5370, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 84h:19m:17s remains)
INFO - root - 2017-12-07 10:39:55.372328: step 5380, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 85h:14m:14s remains)
INFO - root - 2017-12-07 10:40:04.861708: step 5390, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 86h:44m:03s remains)
INFO - root - 2017-12-07 10:40:14.444206: step 5400, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 87h:07m:02s remains)
2017-12-07 10:40:15.404993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3218665 -4.3211923 -4.3206363 -4.3180814 -4.3101659 -4.3079762 -4.3077297 -4.3078136 -4.30701 -4.2991123 -4.2916894 -4.2851448 -4.2832212 -4.28662 -4.2925882][-4.3221884 -4.327271 -4.3333287 -4.3331876 -4.323236 -4.3197479 -4.3203697 -4.3219533 -4.3216262 -4.31077 -4.2987857 -4.2856174 -4.2770615 -4.2766461 -4.2794089][-4.3105683 -4.3175011 -4.3256125 -4.32386 -4.3108845 -4.3055253 -4.3085279 -4.31498 -4.3191657 -4.3101959 -4.2960081 -4.2779279 -4.2657495 -4.2635221 -4.2639985][-4.2977905 -4.3018351 -4.309 -4.3051424 -4.2881894 -4.2779098 -4.2799797 -4.2921829 -4.3059664 -4.3025451 -4.2902794 -4.2729225 -4.2593737 -4.2554226 -4.2551284][-4.2844658 -4.2780285 -4.2776284 -4.2673888 -4.2461782 -4.2251763 -4.219964 -4.2397227 -4.2698379 -4.2808342 -4.2775621 -4.2673655 -4.258419 -4.2545609 -4.25415][-4.2734623 -4.2569146 -4.2427912 -4.224122 -4.1946 -4.1534028 -4.1273556 -4.1505957 -4.204566 -4.2401805 -4.2535706 -4.2533684 -4.2536597 -4.2558866 -4.2575703][-4.2652383 -4.2426023 -4.2181253 -4.1869273 -4.1398177 -4.0709662 -4.0130024 -4.0279555 -4.1009259 -4.1621008 -4.1991525 -4.2185807 -4.2337532 -4.2484527 -4.2581191][-4.2538366 -4.2302852 -4.201334 -4.1543527 -4.082118 -3.9847252 -3.8932762 -3.8842025 -3.9670877 -4.0526724 -4.1219673 -4.1721239 -4.2051597 -4.2283316 -4.242444][-4.2611055 -4.2491755 -4.2341881 -4.1950235 -4.1268935 -4.0377164 -3.9458952 -3.9140613 -3.9706647 -4.0427365 -4.1108003 -4.165019 -4.1946144 -4.2124457 -4.2226124][-4.2714305 -4.2721114 -4.2733426 -4.2527075 -4.2094374 -4.1551952 -4.0907726 -4.0521593 -4.0675006 -4.0983863 -4.1356297 -4.1682973 -4.1845136 -4.19392 -4.2019849][-4.2673044 -4.270287 -4.2773142 -4.267982 -4.2453585 -4.2209449 -4.1850138 -4.1558309 -4.1479926 -4.1484661 -4.1562634 -4.1669617 -4.1718197 -4.1785316 -4.18965][-4.2557545 -4.256433 -4.2655597 -4.2640171 -4.2534084 -4.2459173 -4.2338057 -4.22077 -4.2088637 -4.1937804 -4.1854463 -4.1837959 -4.1828485 -4.190279 -4.2032142][-4.2520685 -4.24818 -4.2534733 -4.2539349 -4.2514639 -4.2566657 -4.2618618 -4.264039 -4.2582316 -4.2427626 -4.2307811 -4.2245197 -4.2203784 -4.22612 -4.2365756][-4.2678366 -4.2606673 -4.2620859 -4.2615857 -4.2638507 -4.2738862 -4.28395 -4.292212 -4.2947321 -4.2881012 -4.2785349 -4.2712951 -4.2652769 -4.2642593 -4.2686419][-4.2837629 -4.2753181 -4.2759213 -4.2784891 -4.2843823 -4.2934036 -4.3004608 -4.3057709 -4.3097911 -4.3086743 -4.302063 -4.2945161 -4.2895341 -4.2880287 -4.291975]]...]
INFO - root - 2017-12-07 10:40:25.238895: step 5410, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 90h:40m:05s remains)
INFO - root - 2017-12-07 10:40:34.863898: step 5420, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 88h:45m:48s remains)
INFO - root - 2017-12-07 10:40:44.450119: step 5430, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 86h:35m:19s remains)
INFO - root - 2017-12-07 10:40:53.959051: step 5440, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 84h:33m:11s remains)
INFO - root - 2017-12-07 10:41:03.509121: step 5450, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 84h:48m:39s remains)
INFO - root - 2017-12-07 10:41:13.085830: step 5460, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.002 sec/batch; 91h:02m:54s remains)
INFO - root - 2017-12-07 10:41:22.693791: step 5470, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 84h:24m:16s remains)
INFO - root - 2017-12-07 10:41:32.157193: step 5480, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.927 sec/batch; 84h:14m:33s remains)
INFO - root - 2017-12-07 10:41:41.832799: step 5490, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.003 sec/batch; 91h:08m:53s remains)
INFO - root - 2017-12-07 10:41:51.520605: step 5500, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 89h:01m:37s remains)
2017-12-07 10:41:52.414337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2169824 -4.217989 -4.2133808 -4.203548 -4.205543 -4.2102118 -4.2196836 -4.2283125 -4.2373996 -4.23928 -4.2340374 -4.23177 -4.2377882 -4.2362094 -4.2341886][-4.2038031 -4.2061653 -4.2060986 -4.1973467 -4.20194 -4.208046 -4.2141361 -4.2174087 -4.2233858 -4.2260232 -4.2181664 -4.2145133 -4.2271919 -4.232954 -4.2328205][-4.1867228 -4.1912861 -4.195425 -4.1899981 -4.1956844 -4.2027812 -4.2124233 -4.2180963 -4.2250385 -4.2244954 -4.20991 -4.1990814 -4.2149477 -4.2294621 -4.2344456][-4.1733208 -4.1766429 -4.1776772 -4.1756878 -4.1815796 -4.1907558 -4.20252 -4.2098336 -4.2156277 -4.2130337 -4.195395 -4.1753197 -4.1878586 -4.2125015 -4.2292876][-4.1515179 -4.1536803 -4.1500683 -4.1482677 -4.155632 -4.1592541 -4.1668563 -4.1786757 -4.1902738 -4.1930513 -4.1770329 -4.1504469 -4.153511 -4.1767254 -4.2014971][-4.136466 -4.1246386 -4.1128731 -4.109179 -4.1078563 -4.0867929 -4.0696859 -4.0949011 -4.1364417 -4.1591511 -4.1525707 -4.1245131 -4.1221361 -4.1410203 -4.1619453][-4.1243134 -4.1004853 -4.0753655 -4.058846 -4.0284081 -3.9558191 -3.8803549 -3.9219766 -4.0296731 -4.098175 -4.1111 -4.0942779 -4.0991783 -4.1238642 -4.1440773][-4.105165 -4.0804219 -4.0532503 -4.029635 -3.9819884 -3.8767369 -3.7368226 -3.7689123 -3.9308045 -4.0382214 -4.07105 -4.0690117 -4.09105 -4.12628 -4.1493297][-4.108357 -4.0960674 -4.0854659 -4.0787048 -4.0483656 -3.9753959 -3.8687863 -3.8615825 -3.9792247 -4.0608759 -4.0889168 -4.0945659 -4.1231284 -4.1599731 -4.1784797][-4.1102834 -4.1129851 -4.1247654 -4.1406479 -4.1372037 -4.106452 -4.05331 -4.0342817 -4.0878959 -4.1301775 -4.1453805 -4.1473703 -4.170404 -4.199079 -4.2116261][-4.1235318 -4.1331234 -4.161943 -4.1908031 -4.2043552 -4.201735 -4.1782775 -4.1576567 -4.1682172 -4.1785116 -4.1754832 -4.1658168 -4.1871376 -4.2145772 -4.2297416][-4.1555948 -4.1736145 -4.2061334 -4.2311473 -4.2446876 -4.2521272 -4.2429862 -4.223701 -4.2138071 -4.2036924 -4.1761093 -4.1421046 -4.1594753 -4.1992388 -4.2262273][-4.1796803 -4.1991835 -4.2227087 -4.2381148 -4.2474847 -4.2562895 -4.2548804 -4.2420068 -4.2214808 -4.1948042 -4.1472769 -4.0999737 -4.119699 -4.1721349 -4.2110815][-4.1894388 -4.205266 -4.2228646 -4.23301 -4.2439113 -4.2537432 -4.2557664 -4.2478466 -4.2287769 -4.2005081 -4.1590009 -4.1278682 -4.155118 -4.2040315 -4.2314849][-4.2124987 -4.2149429 -4.2230358 -4.2284322 -4.2376451 -4.2489123 -4.2537904 -4.2517214 -4.2411942 -4.2245812 -4.2030926 -4.1929307 -4.21883 -4.2501569 -4.2649913]]...]
INFO - root - 2017-12-07 10:42:01.987830: step 5510, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 85h:50m:29s remains)
INFO - root - 2017-12-07 10:42:11.824780: step 5520, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 91h:12m:03s remains)
INFO - root - 2017-12-07 10:42:21.480948: step 5530, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 90h:35m:18s remains)
INFO - root - 2017-12-07 10:42:31.109973: step 5540, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 86h:14m:27s remains)
INFO - root - 2017-12-07 10:42:40.823199: step 5550, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 84h:41m:46s remains)
INFO - root - 2017-12-07 10:42:50.476727: step 5560, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 87h:46m:34s remains)
INFO - root - 2017-12-07 10:43:00.166790: step 5570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 84h:50m:38s remains)
INFO - root - 2017-12-07 10:43:09.696257: step 5580, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 90h:26m:35s remains)
INFO - root - 2017-12-07 10:43:19.292615: step 5590, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 90h:40m:35s remains)
INFO - root - 2017-12-07 10:43:28.983021: step 5600, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 81h:07m:23s remains)
2017-12-07 10:43:29.920960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2890859 -4.2787194 -4.2685118 -4.2571769 -4.2448077 -4.2381568 -4.2373767 -4.2416377 -4.2519331 -4.2663364 -4.2815995 -4.2891717 -4.2894254 -4.2916832 -4.2980518][-4.2746797 -4.2591486 -4.2465653 -4.233417 -4.2166309 -4.2035995 -4.1938882 -4.1902208 -4.19866 -4.220427 -4.2470098 -4.2629309 -4.2683616 -4.2727904 -4.2800236][-4.2634034 -4.2420545 -4.2291293 -4.2157702 -4.191771 -4.1653533 -4.1371574 -4.1197791 -4.1269784 -4.1607842 -4.2030277 -4.2320533 -4.2450428 -4.2506824 -4.2557535][-4.2552447 -4.2306671 -4.2207713 -4.2094722 -4.17557 -4.1281028 -4.0733967 -4.0394969 -4.0445724 -4.0909743 -4.1528535 -4.1983576 -4.2209496 -4.230444 -4.2343359][-4.2456574 -4.2229128 -4.220202 -4.2136607 -4.1730494 -4.1073065 -4.0261636 -3.9690776 -3.9654613 -4.0192242 -4.0981903 -4.1609335 -4.1968184 -4.2137132 -4.2193794][-4.2276607 -4.2087097 -4.2156153 -4.2178345 -4.17902 -4.1043215 -4.002614 -3.9146154 -3.8806736 -3.9306509 -4.0318241 -4.1169748 -4.1675043 -4.1924448 -4.202415][-4.2031136 -4.1847224 -4.1986094 -4.2125182 -4.1872139 -4.1182842 -4.0052805 -3.8792121 -3.7994542 -3.8412051 -3.9710312 -4.0755935 -4.1348066 -4.1635118 -4.1788564][-4.1913543 -4.1707325 -4.1848116 -4.2076731 -4.2005029 -4.1498995 -4.0408449 -3.899035 -3.7965384 -3.8351789 -3.9723644 -4.0710292 -4.1202545 -4.1423316 -4.1571445][-4.1986365 -4.1749015 -4.1871514 -4.216063 -4.2253008 -4.1976457 -4.1097784 -3.9821711 -3.8893538 -3.9167416 -4.0262966 -4.0954027 -4.1208138 -4.1299973 -4.1401825][-4.2257 -4.1994348 -4.2055869 -4.2331982 -4.2525663 -4.2436762 -4.1821146 -4.0795794 -4.0021677 -4.0113935 -4.0846372 -4.1277204 -4.1349154 -4.1340809 -4.1412983][-4.2706475 -4.24697 -4.24496 -4.262876 -4.281795 -4.2857823 -4.2496071 -4.1763263 -4.1136513 -4.1059537 -4.1491213 -4.1760516 -4.1751885 -4.1709342 -4.1759634][-4.3101807 -4.2924609 -4.2858338 -4.2918711 -4.3042989 -4.3132496 -4.2933125 -4.2421808 -4.1925945 -4.1797414 -4.2041655 -4.2226229 -4.2225661 -4.2196436 -4.2221][-4.3268981 -4.3150029 -4.3085523 -4.3102884 -4.3177323 -4.3254352 -4.3138046 -4.2795582 -4.2441239 -4.2323961 -4.2436624 -4.2558556 -4.2588124 -4.2590094 -4.262013][-4.3270636 -4.32179 -4.3185334 -4.31861 -4.3212581 -4.3241215 -4.3169255 -4.2960262 -4.2731304 -4.2629871 -4.2670145 -4.2741938 -4.2805815 -4.2865334 -4.2918758][-4.3248596 -4.3224378 -4.3217058 -4.3214111 -4.3186417 -4.315125 -4.3098869 -4.2999682 -4.2875667 -4.2799935 -4.280385 -4.2862859 -4.2962852 -4.3070092 -4.3131075]]...]
INFO - root - 2017-12-07 10:43:39.425118: step 5610, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 87h:49m:06s remains)
INFO - root - 2017-12-07 10:43:48.848679: step 5620, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 82h:33m:08s remains)
INFO - root - 2017-12-07 10:43:58.630365: step 5630, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 90h:07m:42s remains)
INFO - root - 2017-12-07 10:44:08.294493: step 5640, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 86h:54m:18s remains)
INFO - root - 2017-12-07 10:44:17.727830: step 5650, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 83h:01m:32s remains)
INFO - root - 2017-12-07 10:44:27.157985: step 5660, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.926 sec/batch; 84h:02m:13s remains)
INFO - root - 2017-12-07 10:44:36.760748: step 5670, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.933 sec/batch; 84h:40m:01s remains)
INFO - root - 2017-12-07 10:44:46.117940: step 5680, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 83h:13m:59s remains)
INFO - root - 2017-12-07 10:44:55.594956: step 5690, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 89h:10m:34s remains)
INFO - root - 2017-12-07 10:45:05.268222: step 5700, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.026 sec/batch; 93h:07m:01s remains)
2017-12-07 10:45:06.254516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24036 -4.2104573 -4.1815562 -4.1584892 -4.1513181 -4.1519895 -4.138649 -4.1303868 -4.1242156 -4.1172295 -4.1110511 -4.10852 -4.0953355 -4.0711055 -4.063055][-4.2370005 -4.1986833 -4.1564045 -4.1190262 -4.1019316 -4.0989294 -4.083622 -4.0703845 -4.0556459 -4.0457006 -4.0453053 -4.0463557 -4.0335188 -4.010448 -4.0062227][-4.2400756 -4.1958709 -4.1444578 -4.0990171 -4.0760922 -4.0729294 -4.059896 -4.0451207 -4.027184 -4.0165977 -4.0189357 -4.0204196 -4.0078058 -3.9893074 -3.9876363][-4.2457762 -4.2036877 -4.1551285 -4.1137424 -4.0929203 -4.09052 -4.0822697 -4.0695362 -4.0550394 -4.0475674 -4.0472841 -4.0419674 -4.0257196 -4.0080347 -4.00364][-4.2521391 -4.2199235 -4.1854897 -4.1591148 -4.1467862 -4.1476583 -4.1457777 -4.1385479 -4.1290956 -4.123549 -4.1191511 -4.1067634 -4.0872221 -4.0686784 -4.0598145][-4.2575178 -4.2368283 -4.2150211 -4.2006555 -4.1961055 -4.2011423 -4.2054887 -4.2043247 -4.1993809 -4.1938586 -4.1875792 -4.1756158 -4.1577406 -4.1404214 -4.1321964][-4.2589021 -4.2445636 -4.2274609 -4.2160149 -4.2117476 -4.2167015 -4.2231965 -4.2264729 -4.2262888 -4.2234583 -4.22031 -4.2173557 -4.2093239 -4.200809 -4.1989312][-4.2568669 -4.2428432 -4.2225285 -4.2076664 -4.2008791 -4.20316 -4.2083488 -4.2117519 -4.2136078 -4.2133136 -4.2169056 -4.2261424 -4.2313466 -4.2354155 -4.2412186][-4.2509794 -4.2321768 -4.206512 -4.1868958 -4.1771336 -4.1776929 -4.1805577 -4.180233 -4.1791162 -4.1795287 -4.1888914 -4.2089491 -4.2271724 -4.2419863 -4.2543855][-4.2429357 -4.219789 -4.1904163 -4.1680565 -4.1572018 -4.1563087 -4.1550164 -4.1491404 -4.1450515 -4.1463013 -4.1596804 -4.1860561 -4.2130489 -4.2351742 -4.2520061][-4.2370634 -4.2128725 -4.1838751 -4.1598525 -4.1478362 -4.1457839 -4.1424541 -4.133419 -4.1289883 -4.1333551 -4.1491241 -4.1777444 -4.2074189 -4.2325263 -4.250721][-4.2359614 -4.2141972 -4.1881962 -4.1635022 -4.1497378 -4.1477594 -4.1473036 -4.1406865 -4.139535 -4.1460009 -4.1613932 -4.1885953 -4.2165856 -4.2399564 -4.2557082][-4.2399359 -4.2208815 -4.1993828 -4.1770444 -4.16415 -4.1642823 -4.1695518 -4.1675725 -4.1690812 -4.1754618 -4.18867 -4.2111845 -4.2334237 -4.2513671 -4.2620654][-4.2472663 -4.2304897 -4.2137046 -4.1960769 -4.1871028 -4.1903706 -4.2014327 -4.20398 -4.2080231 -4.213603 -4.2227468 -4.2377119 -4.2520385 -4.2632036 -4.2680087][-4.2556667 -4.2419105 -4.2289987 -4.2175641 -4.2139955 -4.2206969 -4.2352972 -4.24179 -4.2477751 -4.2515273 -4.2556019 -4.2632866 -4.2706909 -4.2754121 -4.2755957]]...]
INFO - root - 2017-12-07 10:45:15.862308: step 5710, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 87h:44m:48s remains)
INFO - root - 2017-12-07 10:45:25.411185: step 5720, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.018 sec/batch; 92h:26m:27s remains)
INFO - root - 2017-12-07 10:45:34.985897: step 5730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 84h:11m:03s remains)
INFO - root - 2017-12-07 10:45:44.615977: step 5740, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 86h:18m:07s remains)
INFO - root - 2017-12-07 10:45:54.198447: step 5750, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 83h:05m:08s remains)
INFO - root - 2017-12-07 10:46:03.726894: step 5760, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.975 sec/batch; 88h:28m:58s remains)
INFO - root - 2017-12-07 10:46:13.511712: step 5770, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 90h:33m:27s remains)
INFO - root - 2017-12-07 10:46:23.112429: step 5780, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 82h:57m:14s remains)
INFO - root - 2017-12-07 10:46:32.680030: step 5790, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 88h:17m:23s remains)
INFO - root - 2017-12-07 10:46:42.378171: step 5800, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 88h:36m:34s remains)
2017-12-07 10:46:43.326845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2192812 -4.2239356 -4.2239404 -4.2301431 -4.2460527 -4.2683392 -4.2900796 -4.2998233 -4.2844229 -4.2468467 -4.2022943 -4.1783686 -4.1830964 -4.1919823 -4.189374][-4.1774731 -4.1910882 -4.2049732 -4.2167692 -4.23084 -4.250349 -4.2697535 -4.276577 -4.26274 -4.2314825 -4.1911354 -4.1682892 -4.1749306 -4.1881495 -4.1907306][-4.140111 -4.1629963 -4.1986446 -4.2256684 -4.2402616 -4.2511306 -4.2604108 -4.259665 -4.245944 -4.2233891 -4.1964273 -4.1801519 -4.1874738 -4.2012744 -4.2024083][-4.1340394 -4.15732 -4.2051253 -4.2377486 -4.2455621 -4.2384648 -4.2354083 -4.230978 -4.2215934 -4.2138495 -4.20347 -4.1939626 -4.1967616 -4.204618 -4.20284][-4.143343 -4.1610065 -4.2024374 -4.2259569 -4.2165427 -4.1881227 -4.1760292 -4.1761651 -4.1752563 -4.1772079 -4.1821537 -4.1827841 -4.1866369 -4.1867485 -4.1800909][-4.1227031 -4.1362395 -4.1608706 -4.167367 -4.1358376 -4.0808845 -4.057241 -4.0713096 -4.0880027 -4.1023912 -4.1260762 -4.1477327 -4.1611414 -4.15625 -4.1410065][-4.1116343 -4.1128488 -4.106863 -4.0790381 -4.015265 -3.9278252 -3.8829987 -3.9212649 -3.9774749 -4.0175447 -4.0644684 -4.1145239 -4.1464534 -4.146472 -4.1296434][-4.1206169 -4.0999184 -4.0648656 -4.0027633 -3.908416 -3.7864947 -3.6992731 -3.7454321 -3.8516469 -3.9290609 -4.0014634 -4.0789962 -4.1301141 -4.1406126 -4.1317821][-4.1506524 -4.1178584 -4.0745912 -4.0062838 -3.9219599 -3.8191712 -3.7250376 -3.7448313 -3.8521645 -3.9363875 -4.0039907 -4.0760489 -4.1266284 -4.1449647 -4.1487532][-4.1959777 -4.1670933 -4.1347861 -4.0848756 -4.0310597 -3.9771378 -3.9216232 -3.9221067 -3.9872062 -4.0465837 -4.0914736 -4.1338158 -4.1615262 -4.1716576 -4.1740193][-4.2388368 -4.2161465 -4.1952634 -4.1657195 -4.1333065 -4.1102867 -4.089591 -4.0897937 -4.1264191 -4.1642847 -4.1930628 -4.2171245 -4.2291694 -4.2244225 -4.2131019][-4.2684975 -4.2505651 -4.238215 -4.2238526 -4.208405 -4.2050843 -4.2041597 -4.207232 -4.2264748 -4.2486296 -4.2679605 -4.2827787 -4.2871208 -4.2758894 -4.2560911][-4.2902703 -4.2781196 -4.269702 -4.2626042 -4.2588305 -4.2643957 -4.2714062 -4.2752314 -4.2837081 -4.2943897 -4.3052983 -4.3127289 -4.3144693 -4.3059359 -4.2897291][-4.30228 -4.2956691 -4.2895632 -4.2855821 -4.2854395 -4.2903957 -4.2971892 -4.3007612 -4.3042345 -4.3083267 -4.3122931 -4.3143196 -4.3161254 -4.3141294 -4.3067427][-4.304575 -4.3010192 -4.2984967 -4.2974281 -4.2993331 -4.3035679 -4.3091035 -4.3136473 -4.3153677 -4.3149152 -4.3133478 -4.3111873 -4.3116341 -4.3124328 -4.3104043]]...]
INFO - root - 2017-12-07 10:46:52.859324: step 5810, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 90h:57m:58s remains)
INFO - root - 2017-12-07 10:47:02.277930: step 5820, loss = 2.12, batch loss = 2.06 (8.8 examples/sec; 0.913 sec/batch; 82h:50m:35s remains)
INFO - root - 2017-12-07 10:47:11.974279: step 5830, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 86h:02m:32s remains)
INFO - root - 2017-12-07 10:47:21.607369: step 5840, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 86h:51m:46s remains)
INFO - root - 2017-12-07 10:47:31.172834: step 5850, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 87h:16m:55s remains)
INFO - root - 2017-12-07 10:47:40.659197: step 5860, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 87h:26m:28s remains)
INFO - root - 2017-12-07 10:47:50.381893: step 5870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 87h:57m:25s remains)
INFO - root - 2017-12-07 10:47:59.901343: step 5880, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 83h:57m:31s remains)
INFO - root - 2017-12-07 10:48:09.464342: step 5890, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 86h:44m:36s remains)
INFO - root - 2017-12-07 10:48:19.285753: step 5900, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.032 sec/batch; 93h:35m:28s remains)
2017-12-07 10:48:20.195664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2080574 -4.2107568 -4.2208343 -4.2337885 -4.2369781 -4.2306371 -4.2280421 -4.232439 -4.2395921 -4.2458215 -4.2525415 -4.2592611 -4.2596188 -4.2570124 -4.2574077][-4.2082691 -4.2094445 -4.2195945 -4.2307758 -4.2314639 -4.2210736 -4.2162652 -4.2241926 -4.2381229 -4.2502613 -4.2605872 -4.2687 -4.2685909 -4.2640457 -4.2599912][-4.202775 -4.2043996 -4.2192078 -4.2324467 -4.2327476 -4.217886 -4.207408 -4.2155623 -4.233345 -4.2491026 -4.2625418 -4.2720838 -4.2720885 -4.2676735 -4.2600813][-4.1932678 -4.1956353 -4.2198267 -4.2407684 -4.244379 -4.2262979 -4.2075982 -4.2114134 -4.2293906 -4.2470164 -4.2627826 -4.2738218 -4.2754021 -4.2723589 -4.2614985][-4.1809983 -4.1806607 -4.2125454 -4.2410812 -4.249104 -4.2301416 -4.2075634 -4.2076015 -4.2246885 -4.2426538 -4.2578039 -4.269279 -4.2749548 -4.2749386 -4.2626266][-4.1606927 -4.1588893 -4.1975684 -4.2310796 -4.2395 -4.2198253 -4.196701 -4.1953473 -4.2102475 -4.2273436 -4.2390842 -4.2483397 -4.2567534 -4.2589011 -4.2471914][-4.1394696 -4.1412792 -4.1869926 -4.2226353 -4.2294221 -4.20856 -4.1872368 -4.1851645 -4.1961088 -4.2096238 -4.2190146 -4.229331 -4.2426577 -4.2481136 -4.2389622][-4.1211348 -4.131053 -4.1822562 -4.2161803 -4.2177343 -4.1935315 -4.1737432 -4.1724463 -4.1800385 -4.1917691 -4.2022758 -4.2166109 -4.2345586 -4.2431645 -4.2369][-4.11898 -4.1366172 -4.1901088 -4.2225142 -4.21978 -4.1927915 -4.1742516 -4.1761484 -4.1824408 -4.1915431 -4.1996045 -4.2134452 -4.2306046 -4.2385445 -4.232892][-4.1462736 -4.1652842 -4.213479 -4.2405233 -4.2330289 -4.2044921 -4.1891222 -4.1973991 -4.20586 -4.2116461 -4.214273 -4.2237754 -4.2365956 -4.2422462 -4.23685][-4.1719623 -4.1891437 -4.2277665 -4.2476053 -4.2374482 -4.2099066 -4.1987362 -4.2121482 -4.2227468 -4.22732 -4.2281671 -4.2344875 -4.2438054 -4.248199 -4.2440395][-4.1929693 -4.205534 -4.2331662 -4.2474446 -4.239511 -4.2183065 -4.2129936 -4.2277431 -4.2361822 -4.23856 -4.2386441 -4.243865 -4.2523055 -4.2572703 -4.2559495][-4.2163987 -4.2239566 -4.242322 -4.2539644 -4.2509303 -4.2393312 -4.2390013 -4.2510443 -4.2547016 -4.2533875 -4.2519894 -4.2560544 -4.2637382 -4.2697544 -4.2723684][-4.2464957 -4.2491941 -4.2602568 -4.2703476 -4.2730451 -4.26971 -4.2719965 -4.2794156 -4.2788944 -4.2752457 -4.2729187 -4.2757483 -4.2816024 -4.2871208 -4.2918282][-4.2759204 -4.2748981 -4.2806811 -4.2888384 -4.295074 -4.2976151 -4.3014841 -4.3051772 -4.3029189 -4.2994614 -4.2973256 -4.2984095 -4.3013482 -4.3049979 -4.3088388]]...]
INFO - root - 2017-12-07 10:48:29.792162: step 5910, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 88h:50m:55s remains)
INFO - root - 2017-12-07 10:48:39.390748: step 5920, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 85h:17m:48s remains)
INFO - root - 2017-12-07 10:48:49.104098: step 5930, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.025 sec/batch; 92h:58m:07s remains)
INFO - root - 2017-12-07 10:48:58.598932: step 5940, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 87h:53m:37s remains)
INFO - root - 2017-12-07 10:49:08.230715: step 5950, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 84h:21m:08s remains)
INFO - root - 2017-12-07 10:49:17.818718: step 5960, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 87h:08m:07s remains)
INFO - root - 2017-12-07 10:49:27.397175: step 5970, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 91h:13m:05s remains)
INFO - root - 2017-12-07 10:49:37.062411: step 5980, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 82h:54m:17s remains)
INFO - root - 2017-12-07 10:49:46.503093: step 5990, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 83h:26m:39s remains)
INFO - root - 2017-12-07 10:49:56.057084: step 6000, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 90h:20m:32s remains)
2017-12-07 10:49:57.025214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.204504 -4.2075353 -4.2290211 -4.2528758 -4.2627468 -4.2563281 -4.2551832 -4.2689958 -4.2850561 -4.2959681 -4.2981935 -4.2821484 -4.2511616 -4.2141643 -4.1833763][-4.158803 -4.1575994 -4.1879687 -4.2305388 -4.2551904 -4.2578588 -4.2616854 -4.2741742 -4.2868276 -4.2911253 -4.2895384 -4.2713418 -4.2342486 -4.1921 -4.1614046][-4.1439042 -4.1425519 -4.1746821 -4.2190785 -4.2462249 -4.2535748 -4.2601132 -4.2690535 -4.2791834 -4.2842503 -4.2806463 -4.2609811 -4.2204909 -4.1742005 -4.1448326][-4.1552753 -4.1575212 -4.1817741 -4.2129545 -4.2316637 -4.236629 -4.2444596 -4.2555685 -4.2662292 -4.2726889 -4.2696652 -4.2484927 -4.2089481 -4.166091 -4.1430902][-4.1790981 -4.1826458 -4.1925063 -4.2041674 -4.2083645 -4.205925 -4.2145562 -4.2327614 -4.2472639 -4.2547951 -4.2544913 -4.2376652 -4.2024722 -4.1643448 -4.14636][-4.2157617 -4.218894 -4.2145982 -4.2117329 -4.20443 -4.18889 -4.1903963 -4.20891 -4.2221141 -4.2289267 -4.2334409 -4.2320638 -4.2124133 -4.1801839 -4.1593866][-4.2501054 -4.2528911 -4.2390723 -4.2239518 -4.2026711 -4.17148 -4.1626315 -4.1761637 -4.1846442 -4.1865196 -4.1959515 -4.2142787 -4.2198625 -4.2039409 -4.1875081][-4.2563004 -4.2582703 -4.2465868 -4.2341375 -4.2115092 -4.1708269 -4.151587 -4.1544313 -4.1463776 -4.1288362 -4.1330185 -4.1659379 -4.1977339 -4.2074847 -4.2105889][-4.2317038 -4.2397375 -4.2404389 -4.2456551 -4.2370558 -4.2016959 -4.1721058 -4.1526356 -4.1150765 -4.0636845 -4.0491533 -4.0928936 -4.1520219 -4.1942806 -4.2236686][-4.1907268 -4.2112551 -4.2316866 -4.2608681 -4.2715564 -4.2476883 -4.2138686 -4.1723528 -4.1002841 -4.007854 -3.9682248 -4.0171366 -4.0978012 -4.1689763 -4.2213182][-4.1508183 -4.1916614 -4.2360168 -4.286406 -4.3119788 -4.2953758 -4.2562375 -4.1965084 -4.1002011 -3.9853759 -3.9334149 -3.9791636 -4.0667768 -4.1484866 -4.2104697][-4.1139631 -4.1779685 -4.2392187 -4.2975206 -4.3327537 -4.3248787 -4.2846847 -4.2158146 -4.1151419 -4.0014882 -3.9519305 -3.9929454 -4.0739412 -4.1502523 -4.2078648][-4.073257 -4.1547413 -4.2226067 -4.2816367 -4.3224959 -4.3227162 -4.2846923 -4.2195077 -4.1355157 -4.0458198 -4.0078382 -4.0372806 -4.09689 -4.15707 -4.20559][-4.0404172 -4.1198978 -4.1766591 -4.2339182 -4.2787385 -4.2893176 -4.2615891 -4.2085552 -4.1484652 -4.09202 -4.0699668 -4.0869856 -4.1192646 -4.1572976 -4.1931171][-4.0250468 -4.0866718 -4.1232409 -4.1728058 -4.2219057 -4.2417879 -4.2271976 -4.1905036 -4.150187 -4.1213436 -4.1159649 -4.1249452 -4.1353526 -4.15333 -4.1774135]]...]
INFO - root - 2017-12-07 10:50:06.689266: step 6010, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 83h:04m:56s remains)
INFO - root - 2017-12-07 10:50:16.326961: step 6020, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 86h:51m:08s remains)
INFO - root - 2017-12-07 10:50:25.953743: step 6030, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.001 sec/batch; 90h:45m:05s remains)
INFO - root - 2017-12-07 10:50:35.543542: step 6040, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 87h:02m:01s remains)
INFO - root - 2017-12-07 10:50:45.175343: step 6050, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 85h:45m:06s remains)
INFO - root - 2017-12-07 10:50:54.894985: step 6060, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 85h:10m:20s remains)
INFO - root - 2017-12-07 10:51:04.415942: step 6070, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 88h:15m:43s remains)
INFO - root - 2017-12-07 10:51:14.019548: step 6080, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 86h:55m:03s remains)
INFO - root - 2017-12-07 10:51:23.620431: step 6090, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 86h:57m:06s remains)
INFO - root - 2017-12-07 10:51:33.205183: step 6100, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.880 sec/batch; 79h:46m:26s remains)
2017-12-07 10:51:34.159220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3343339 -4.3399816 -4.3464193 -4.3487725 -4.3435826 -4.3353877 -4.3286705 -4.3249121 -4.3249311 -4.3306375 -4.3379803 -4.343482 -4.3428922 -4.3380537 -4.33212][-4.3243933 -4.3271933 -4.3320336 -4.3328023 -4.3238225 -4.3119717 -4.3022823 -4.2980266 -4.2959528 -4.30305 -4.3163638 -4.3270578 -4.326508 -4.3207541 -4.3161697][-4.3159075 -4.3148794 -4.31793 -4.3165731 -4.3040228 -4.2879138 -4.2763948 -4.2756381 -4.2743254 -4.2792606 -4.2949905 -4.3098931 -4.3087373 -4.3009467 -4.2982521][-4.31012 -4.3052835 -4.3058491 -4.3032618 -4.289804 -4.2707205 -4.2573256 -4.2591305 -4.2601275 -4.2623954 -4.2757692 -4.2938881 -4.2929921 -4.2832437 -4.2812972][-4.3034687 -4.2949691 -4.2916937 -4.28651 -4.2724233 -4.2516518 -4.2345104 -4.2367382 -4.2446327 -4.2486486 -4.25652 -4.2690473 -4.2645555 -4.2545485 -4.2551446][-4.2980013 -4.2865319 -4.2797861 -4.2727723 -4.2592959 -4.2374177 -4.2166896 -4.220325 -4.2370405 -4.2457132 -4.25053 -4.2542973 -4.2405996 -4.2246184 -4.2238045][-4.2951064 -4.2811656 -4.272521 -4.2625771 -4.2470746 -4.2247434 -4.2033939 -4.2083297 -4.2280974 -4.2431827 -4.2522449 -4.2528548 -4.2321472 -4.2092471 -4.2023597][-4.2934322 -4.2773743 -4.2688584 -4.2566009 -4.234664 -4.2094574 -4.1859818 -4.1856132 -4.2013931 -4.2240705 -4.2461014 -4.2528152 -4.2346187 -4.2167363 -4.208909][-4.2963428 -4.2817526 -4.274096 -4.2589836 -4.2298865 -4.2010016 -4.17752 -4.1726727 -4.1808133 -4.2048283 -4.2336445 -4.246098 -4.2367344 -4.22858 -4.224328][-4.3009796 -4.2918191 -4.2865825 -4.2700133 -4.2374268 -4.2055893 -4.1834583 -4.1767087 -4.1811743 -4.2004724 -4.2270689 -4.2427812 -4.2442055 -4.2439795 -4.2403278][-4.3005986 -4.2940588 -4.291966 -4.2758636 -4.2425122 -4.2104063 -4.1874528 -4.178822 -4.1859846 -4.2047019 -4.2287216 -4.2450109 -4.2515917 -4.2531362 -4.2482457][-4.2954335 -4.2866812 -4.283968 -4.2693596 -4.2419257 -4.2171054 -4.1966743 -4.1907387 -4.2024961 -4.2215033 -4.2415962 -4.2555313 -4.2613816 -4.2627945 -4.2601128][-4.2932348 -4.2827334 -4.2803497 -4.2703404 -4.2556958 -4.2442336 -4.2308493 -4.2264438 -4.2369637 -4.2516623 -4.263876 -4.2705317 -4.2708473 -4.2704344 -4.2702084][-4.2923317 -4.2820349 -4.2822752 -4.2795849 -4.2779527 -4.278553 -4.2735262 -4.2708879 -4.2773871 -4.2849236 -4.288671 -4.28882 -4.2856312 -4.2840919 -4.2853079][-4.2950792 -4.2864294 -4.2873907 -4.2878122 -4.29094 -4.2960825 -4.296946 -4.2974172 -4.3008285 -4.3033533 -4.302547 -4.3000212 -4.2968736 -4.2959371 -4.2967911]]...]
INFO - root - 2017-12-07 10:51:43.692710: step 6110, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 88h:58m:02s remains)
INFO - root - 2017-12-07 10:51:53.193696: step 6120, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.921 sec/batch; 83h:30m:04s remains)
INFO - root - 2017-12-07 10:52:03.169409: step 6130, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 90h:27m:16s remains)
INFO - root - 2017-12-07 10:52:12.681619: step 6140, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 85h:57m:04s remains)
INFO - root - 2017-12-07 10:52:22.035030: step 6150, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 76h:04m:06s remains)
INFO - root - 2017-12-07 10:52:31.548307: step 6160, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 89h:51m:57s remains)
INFO - root - 2017-12-07 10:52:41.017515: step 6170, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 85h:59m:20s remains)
INFO - root - 2017-12-07 10:52:50.702119: step 6180, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 82h:57m:50s remains)
INFO - root - 2017-12-07 10:53:00.459778: step 6190, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 89h:28m:38s remains)
INFO - root - 2017-12-07 10:53:10.142431: step 6200, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 85h:42m:49s remains)
2017-12-07 10:53:11.077466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533517 -4.2429457 -4.2367897 -4.240274 -4.2443767 -4.2375493 -4.2127085 -4.1777053 -4.1424155 -4.122447 -4.1386971 -4.173141 -4.2056112 -4.2344012 -4.272882][-4.2594652 -4.2532139 -4.2521677 -4.2556376 -4.2571993 -4.2432103 -4.2089763 -4.1695061 -4.1399288 -4.1243668 -4.13811 -4.1755843 -4.2180061 -4.259819 -4.3045421][-4.2681723 -4.2706976 -4.2759008 -4.2828474 -4.279933 -4.2547474 -4.2085752 -4.1589594 -4.1279683 -4.1182404 -4.1367483 -4.1827526 -4.2309117 -4.2789731 -4.321569][-4.2785888 -4.2892814 -4.2993469 -4.3074236 -4.2953162 -4.2584295 -4.199255 -4.1323776 -4.0939956 -4.0949478 -4.1288815 -4.1890693 -4.2463322 -4.2968316 -4.3305364][-4.2899942 -4.3049097 -4.3169813 -4.3195915 -4.295506 -4.2481937 -4.1738071 -4.0835333 -4.0396028 -4.0648532 -4.1255832 -4.2018747 -4.2679267 -4.3166566 -4.3371091][-4.3049536 -4.3186235 -4.3275027 -4.3189788 -4.2834406 -4.219892 -4.1248956 -4.018785 -3.9851711 -4.0488348 -4.1413612 -4.2277226 -4.2943664 -4.3325157 -4.335866][-4.3072176 -4.3177671 -4.3215461 -4.3036475 -4.2554107 -4.1715736 -4.0608706 -3.9645822 -3.9645245 -4.0632133 -4.1668773 -4.250771 -4.306582 -4.3301663 -4.3184261][-4.2882967 -4.2955475 -4.2971935 -4.2732439 -4.2137218 -4.1201944 -4.0204015 -3.9669228 -4.0065136 -4.1069636 -4.1981344 -4.2708588 -4.3116779 -4.3195739 -4.2939568][-4.2574358 -4.2669554 -4.2722416 -4.2470407 -4.1823192 -4.0921373 -4.0178566 -4.0098152 -4.0693126 -4.1559095 -4.2308259 -4.2895088 -4.3144579 -4.3041472 -4.268219][-4.2271075 -4.2430696 -4.2521162 -4.2243266 -4.1566677 -4.0767651 -4.0332532 -4.0598893 -4.1262264 -4.1978583 -4.2597876 -4.304852 -4.314436 -4.2920518 -4.2472053][-4.2025757 -4.2227259 -4.2283154 -4.1969748 -4.13345 -4.0719733 -4.0577955 -4.1091251 -4.1775732 -4.2351103 -4.2835112 -4.316144 -4.3152957 -4.2860775 -4.2339911][-4.1914949 -4.2120605 -4.2109427 -4.178483 -4.1272388 -4.0876131 -4.0929346 -4.149838 -4.213882 -4.2595043 -4.2982206 -4.3218722 -4.3115873 -4.2724237 -4.2133718][-4.2032666 -4.2166572 -4.2074122 -4.176487 -4.1405497 -4.1183658 -4.1317244 -4.1828513 -4.2391796 -4.2811546 -4.3136349 -4.3281269 -4.3091211 -4.2598224 -4.1932626][-4.218811 -4.2265468 -4.2166762 -4.1903081 -4.1675491 -4.1574254 -4.1716647 -4.2127318 -4.2568417 -4.2907796 -4.3135228 -4.3190093 -4.2944417 -4.2404757 -4.1728106][-4.237031 -4.24659 -4.2423077 -4.2216306 -4.2050252 -4.1985674 -4.2089453 -4.2395439 -4.2683334 -4.2876997 -4.2974815 -4.2970357 -4.2716112 -4.21782 -4.1575565]]...]
INFO - root - 2017-12-07 10:53:20.739823: step 6210, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 81h:06m:25s remains)
INFO - root - 2017-12-07 10:53:30.336296: step 6220, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.982 sec/batch; 88h:57m:40s remains)
INFO - root - 2017-12-07 10:53:40.127833: step 6230, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 91h:02m:20s remains)
INFO - root - 2017-12-07 10:53:49.770059: step 6240, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 86h:20m:34s remains)
INFO - root - 2017-12-07 10:53:59.361348: step 6250, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 89h:39m:38s remains)
INFO - root - 2017-12-07 10:54:08.971889: step 6260, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.987 sec/batch; 89h:28m:52s remains)
INFO - root - 2017-12-07 10:54:18.567061: step 6270, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.016 sec/batch; 92h:05m:22s remains)
INFO - root - 2017-12-07 10:54:28.150522: step 6280, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 86h:24m:53s remains)
INFO - root - 2017-12-07 10:54:37.792726: step 6290, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 84h:21m:22s remains)
INFO - root - 2017-12-07 10:54:47.490955: step 6300, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 84h:50m:39s remains)
2017-12-07 10:54:48.457588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3342519 -4.3057075 -4.2624393 -4.2005439 -4.1251168 -4.0791621 -4.1021109 -4.1444278 -4.1632805 -4.1719046 -4.1780739 -4.1837435 -4.1887269 -4.1966329 -4.21064][-4.3374968 -4.3090224 -4.2618532 -4.1926956 -4.1050425 -4.041369 -4.0605965 -4.1201153 -4.1544533 -4.169302 -4.1733208 -4.1777029 -4.1852751 -4.1988254 -4.2186503][-4.3425508 -4.3154421 -4.2686033 -4.1936374 -4.0944333 -4.0089111 -4.0139079 -4.0888028 -4.1421824 -4.1663985 -4.1695275 -4.1712594 -4.1793613 -4.1985788 -4.2253423][-4.346911 -4.3234625 -4.2812057 -4.2056823 -4.1014938 -3.9957623 -3.9755259 -4.0546989 -4.1256423 -4.1610107 -4.1640038 -4.1604004 -4.168591 -4.1930451 -4.2269235][-4.3473196 -4.3283205 -4.2923536 -4.2209759 -4.12055 -4.0037832 -3.954906 -4.026422 -4.1093597 -4.154109 -4.1599855 -4.1527252 -4.1597433 -4.1859097 -4.2218552][-4.3452106 -4.3300343 -4.3034 -4.2415237 -4.1520767 -4.0345879 -3.9633331 -4.0151582 -4.099606 -4.1502724 -4.1613569 -4.1547556 -4.1608367 -4.1848531 -4.2150221][-4.3428164 -4.3301682 -4.3121352 -4.2607784 -4.1812592 -4.0654497 -3.9792917 -4.0048232 -4.0809512 -4.1390538 -4.1595836 -4.1595368 -4.1629043 -4.1791353 -4.1980963][-4.3406343 -4.3298807 -4.3185091 -4.2765121 -4.2076 -4.0991192 -4.0081615 -4.0092955 -4.0703354 -4.1340442 -4.1679711 -4.1779232 -4.1795855 -4.1834006 -4.1854787][-4.3392653 -4.3291192 -4.3217192 -4.2894444 -4.2336349 -4.1409521 -4.058795 -4.0426855 -4.0831246 -4.1423917 -4.1858759 -4.2067542 -4.2147207 -4.2091227 -4.1956434][-4.3396587 -4.3291368 -4.3233638 -4.299324 -4.25744 -4.18727 -4.1224356 -4.0997934 -4.1185217 -4.1636906 -4.2069025 -4.2355804 -4.2512875 -4.2431335 -4.2212625][-4.3424654 -4.3319874 -4.3265123 -4.3079576 -4.2772479 -4.2298861 -4.1818085 -4.1584063 -4.1634159 -4.1918325 -4.2292027 -4.264432 -4.2905841 -4.2863483 -4.2619767][-4.346611 -4.3363986 -4.3295946 -4.3130655 -4.2911987 -4.2647104 -4.2330136 -4.2075028 -4.2012868 -4.2140021 -4.2483621 -4.2923784 -4.3274765 -4.3280487 -4.3045173][-4.3509212 -4.3409271 -4.3330679 -4.3192258 -4.3049273 -4.2930923 -4.2773933 -4.2529631 -4.2412291 -4.2439618 -4.2729511 -4.3185773 -4.3581529 -4.3628168 -4.342916][-4.35585 -4.3463888 -4.3377237 -4.3244171 -4.3146133 -4.3108187 -4.3062987 -4.2871909 -4.2753649 -4.2775135 -4.2996879 -4.3380356 -4.3730063 -4.3794589 -4.3643947][-4.3573995 -4.3484025 -4.33867 -4.3271465 -4.3214879 -4.3239202 -4.3264756 -4.3136845 -4.3012061 -4.2990727 -4.3115468 -4.3396111 -4.3684425 -4.3775296 -4.3696446]]...]
INFO - root - 2017-12-07 10:54:58.162379: step 6310, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 86h:51m:09s remains)
INFO - root - 2017-12-07 10:55:07.652917: step 6320, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 90h:00m:15s remains)
INFO - root - 2017-12-07 10:55:17.297420: step 6330, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 85h:38m:01s remains)
INFO - root - 2017-12-07 10:55:26.874573: step 6340, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.923 sec/batch; 83h:37m:51s remains)
INFO - root - 2017-12-07 10:55:36.527092: step 6350, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 89h:21m:40s remains)
INFO - root - 2017-12-07 10:55:46.029934: step 6360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 87h:00m:52s remains)
INFO - root - 2017-12-07 10:55:55.790947: step 6370, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 84h:53m:49s remains)
INFO - root - 2017-12-07 10:56:05.485581: step 6380, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 89h:59m:57s remains)
INFO - root - 2017-12-07 10:56:15.137083: step 6390, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 90h:21m:11s remains)
INFO - root - 2017-12-07 10:56:24.727395: step 6400, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 84h:12m:20s remains)
2017-12-07 10:56:25.703581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2205667 -4.2277083 -4.2323995 -4.2286873 -4.2156162 -4.2022982 -4.1947904 -4.1941471 -4.1973867 -4.2001271 -4.2004104 -4.2042766 -4.2054877 -4.1944942 -4.183485][-4.2383847 -4.2348518 -4.2292671 -4.2183414 -4.1997113 -4.1863284 -4.181211 -4.1861286 -4.1949215 -4.2008791 -4.2029366 -4.2082019 -4.2142196 -4.205224 -4.1925807][-4.2433748 -4.2332888 -4.2234797 -4.2090511 -4.1864438 -4.1674862 -4.1581416 -4.1648183 -4.1808434 -4.1965332 -4.2028065 -4.20655 -4.2083359 -4.1981683 -4.1837416][-4.2417006 -4.2290893 -4.2175822 -4.2025251 -4.1784544 -4.1501365 -4.1300316 -4.135159 -4.156631 -4.1829557 -4.1930904 -4.1934185 -4.1884446 -4.1777258 -4.1645384][-4.2355952 -4.224102 -4.2155142 -4.2008295 -4.1740489 -4.1373491 -4.1037025 -4.101675 -4.12862 -4.1635475 -4.1742454 -4.1698332 -4.1601095 -4.1517839 -4.1457019][-4.2210178 -4.2130227 -4.2103233 -4.1995287 -4.1733875 -4.13091 -4.0849009 -4.0752378 -4.1063762 -4.1459732 -4.1596613 -4.1572461 -4.1497192 -4.1481504 -4.1519666][-4.2061729 -4.1979356 -4.1934915 -4.1863079 -4.1635695 -4.1211238 -4.0714803 -4.0568976 -4.0907397 -4.1346908 -4.1578875 -4.1678715 -4.1726856 -4.1804848 -4.188201][-4.1931281 -4.1804171 -4.1697412 -4.1673217 -4.1546688 -4.1203656 -4.0768933 -4.0613317 -4.0926065 -4.134151 -4.1580777 -4.1790733 -4.1984205 -4.21769 -4.2283845][-4.195262 -4.17782 -4.1603317 -4.1596322 -4.1609545 -4.1449809 -4.1150641 -4.0981164 -4.1159973 -4.1437039 -4.1627054 -4.187263 -4.2175927 -4.2459054 -4.2557907][-4.2083521 -4.18495 -4.1605411 -4.1614208 -4.1780109 -4.1810789 -4.1667047 -4.1487374 -4.1510415 -4.1632023 -4.1785979 -4.201941 -4.2313704 -4.257339 -4.2590985][-4.219708 -4.1918883 -4.16359 -4.1640577 -4.1871128 -4.206214 -4.2106323 -4.1987381 -4.1917157 -4.1923018 -4.2014971 -4.2151523 -4.2307038 -4.2465587 -4.2415609][-4.2323151 -4.2076559 -4.1808062 -4.1773791 -4.1964526 -4.2218223 -4.2385435 -4.2351375 -4.229167 -4.2257557 -4.2250166 -4.2229791 -4.2213597 -4.22607 -4.2196965][-4.246974 -4.2261577 -4.2030745 -4.1950278 -4.2048092 -4.2276154 -4.2463613 -4.2498913 -4.2504373 -4.2509236 -4.2441978 -4.2315426 -4.2179446 -4.2151937 -4.2100019][-4.2655611 -4.2469397 -4.2268171 -4.2142987 -4.2149706 -4.2325692 -4.2498937 -4.2574983 -4.2628179 -4.2650938 -4.2550941 -4.237999 -4.2233624 -4.2227268 -4.2217007][-4.2833419 -4.2675848 -4.2505574 -4.2359071 -4.2310219 -4.2419052 -4.2536097 -4.25867 -4.2656674 -4.2687292 -4.2582593 -4.2433391 -4.235858 -4.2416191 -4.244916]]...]
INFO - root - 2017-12-07 10:56:35.431433: step 6410, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 89h:24m:33s remains)
INFO - root - 2017-12-07 10:56:45.016289: step 6420, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 84h:33m:26s remains)
INFO - root - 2017-12-07 10:56:54.602449: step 6430, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 89h:23m:55s remains)
INFO - root - 2017-12-07 10:57:04.324527: step 6440, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 89h:44m:36s remains)
INFO - root - 2017-12-07 10:57:14.013651: step 6450, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 86h:29m:51s remains)
INFO - root - 2017-12-07 10:57:23.560393: step 6460, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 85h:46m:48s remains)
INFO - root - 2017-12-07 10:57:33.347169: step 6470, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 87h:04m:15s remains)
INFO - root - 2017-12-07 10:57:43.018283: step 6480, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 88h:08m:52s remains)
INFO - root - 2017-12-07 10:57:52.441906: step 6490, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 88h:47m:09s remains)
INFO - root - 2017-12-07 10:58:02.176815: step 6500, loss = 2.11, batch loss = 2.05 (7.7 examples/sec; 1.040 sec/batch; 94h:08m:07s remains)
2017-12-07 10:58:03.087182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3409381 -4.3598576 -4.3751879 -4.3814769 -4.3811646 -4.36298 -4.3270788 -4.2885075 -4.2484212 -4.20441 -4.1612148 -4.1468568 -4.1730556 -4.2205319 -4.2649417][-4.343123 -4.37187 -4.3937664 -4.3991804 -4.3918233 -4.3636117 -4.315567 -4.2699924 -4.2264128 -4.1795335 -4.1361475 -4.1273723 -4.1548142 -4.209393 -4.261374][-4.3412838 -4.3781095 -4.4022808 -4.40249 -4.3836074 -4.3416605 -4.28607 -4.2353106 -4.186234 -4.1418681 -4.1091843 -4.1128325 -4.1426167 -4.20035 -4.2565107][-4.3414259 -4.381052 -4.40335 -4.396069 -4.3660207 -4.3122678 -4.2504082 -4.1924968 -4.1411514 -4.1041064 -4.0810561 -4.0945811 -4.1261559 -4.1900611 -4.253438][-4.3449373 -4.3826485 -4.4001055 -4.3884449 -4.3490095 -4.2868094 -4.2158375 -4.1531138 -4.1000018 -4.0660834 -4.0505958 -4.0676827 -4.105144 -4.1794739 -4.2509608][-4.3525624 -4.3891058 -4.4016876 -4.3825631 -4.3290105 -4.2539248 -4.1749287 -4.1084843 -4.0545406 -4.0255423 -4.02163 -4.0471964 -4.0999923 -4.1873736 -4.2591314][-4.3628459 -4.3986206 -4.4050303 -4.3748026 -4.3048344 -4.2126474 -4.1265283 -4.0497637 -3.989671 -3.977778 -4.0000577 -4.041481 -4.1152067 -4.2084761 -4.2770429][-4.3695536 -4.4012561 -4.4005418 -4.3612924 -4.2783794 -4.1674786 -4.0564675 -3.9562562 -3.9093521 -3.9337873 -3.9905009 -4.0554252 -4.146081 -4.2356725 -4.2951488][-4.3736649 -4.3976789 -4.386476 -4.3386779 -4.2475939 -4.1131415 -3.9652009 -3.8427522 -3.8273568 -3.8974307 -3.9846506 -4.0738964 -4.1780472 -4.2579269 -4.3029919][-4.3760071 -4.3871379 -4.3625016 -4.3049755 -4.2092266 -4.0701408 -3.9060278 -3.7797966 -3.79038 -3.8825951 -3.9804363 -4.08398 -4.1934962 -4.2632217 -4.2968793][-4.3760052 -4.3698983 -4.3322697 -4.2668443 -4.1756172 -4.05328 -3.9151359 -3.8166034 -3.8389647 -3.9228358 -4.0080724 -4.10875 -4.2088819 -4.265996 -4.2909107][-4.368166 -4.348804 -4.3037128 -4.2360864 -4.1583743 -4.0715146 -3.9810708 -3.926734 -3.9499433 -4.0059676 -4.068 -4.1487112 -4.22689 -4.2684307 -4.2852025][-4.3518157 -4.3276024 -4.284482 -4.224926 -4.1667271 -4.1129775 -4.0605421 -4.0380812 -4.0587549 -4.0936408 -4.135417 -4.1924963 -4.2475009 -4.2743092 -4.2837772][-4.3376789 -4.3153572 -4.2824831 -4.2352762 -4.1915917 -4.1554294 -4.1254091 -4.1182771 -4.1353579 -4.1583881 -4.1889334 -4.2309275 -4.2663226 -4.2821779 -4.2865291][-4.3366437 -4.3205667 -4.3010712 -4.2679305 -4.2358131 -4.2112088 -4.1932759 -4.1910233 -4.1988816 -4.2136278 -4.2373371 -4.269393 -4.2907281 -4.2980452 -4.3000116]]...]
INFO - root - 2017-12-07 10:58:12.731136: step 6510, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.990 sec/batch; 89h:40m:31s remains)
INFO - root - 2017-12-07 10:58:22.365072: step 6520, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 89h:31m:23s remains)
INFO - root - 2017-12-07 10:58:32.086093: step 6530, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 86h:27m:55s remains)
INFO - root - 2017-12-07 10:58:41.712653: step 6540, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 84h:08m:20s remains)
INFO - root - 2017-12-07 10:58:51.456177: step 6550, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 1.004 sec/batch; 90h:55m:26s remains)
INFO - root - 2017-12-07 10:59:01.027967: step 6560, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 85h:18m:34s remains)
INFO - root - 2017-12-07 10:59:10.735928: step 6570, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 84h:45m:49s remains)
INFO - root - 2017-12-07 10:59:20.454897: step 6580, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 88h:16m:27s remains)
INFO - root - 2017-12-07 10:59:29.972109: step 6590, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 84h:52m:46s remains)
INFO - root - 2017-12-07 10:59:39.585224: step 6600, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 90h:34m:08s remains)
2017-12-07 10:59:40.542029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3169327 -4.3007607 -4.2762403 -4.23951 -4.2058306 -4.1790576 -4.153831 -4.1229172 -4.1022835 -4.0899029 -4.0958157 -4.113699 -4.1351728 -4.1617169 -4.1663904][-4.3245206 -4.3085032 -4.2825947 -4.2454243 -4.2073412 -4.1788716 -4.1579885 -4.1321573 -4.1116428 -4.1003313 -4.1026025 -4.122323 -4.1480794 -4.1763883 -4.1803627][-4.3262687 -4.3090758 -4.2818179 -4.246511 -4.210629 -4.1823726 -4.1607342 -4.1411653 -4.12855 -4.122951 -4.1223025 -4.1372023 -4.1624565 -4.1843076 -4.1883435][-4.3225422 -4.3012462 -4.2689695 -4.2333646 -4.1977997 -4.1720171 -4.1526785 -4.1418939 -4.139255 -4.139267 -4.1397624 -4.1497846 -4.166481 -4.1827788 -4.18674][-4.3144784 -4.2858834 -4.24588 -4.2082667 -4.1748896 -4.1546087 -4.1333914 -4.1188879 -4.1187644 -4.1262088 -4.1343646 -4.1422796 -4.1550522 -4.1735415 -4.1867228][-4.3008151 -4.2618871 -4.2111921 -4.1721869 -4.1433387 -4.1224709 -4.0892067 -4.064816 -4.0693541 -4.0889134 -4.1104693 -4.122705 -4.1353712 -4.1559057 -4.1753306][-4.2857561 -4.2360978 -4.1740737 -4.1292877 -4.0981469 -4.06758 -4.020668 -3.9926286 -4.0103078 -4.0461063 -4.07969 -4.0994797 -4.115128 -4.1387186 -4.1604438][-4.2769313 -4.223033 -4.1580658 -4.1084824 -4.0707078 -4.0330215 -3.9818184 -3.9542675 -3.9767177 -4.0172534 -4.0576324 -4.0818477 -4.0982776 -4.1164746 -4.1335049][-4.2774086 -4.2302351 -4.1809182 -4.1438084 -4.1106191 -4.0757289 -4.0274043 -3.9978924 -4.00869 -4.0406322 -4.078548 -4.10088 -4.1065979 -4.1055684 -4.1046453][-4.2817287 -4.2457175 -4.2124429 -4.18761 -4.1616244 -4.1342864 -4.0942869 -4.0635324 -4.0655675 -4.0888453 -4.1190157 -4.1332645 -4.1242375 -4.1038785 -4.0860739][-4.2841787 -4.2546191 -4.2304583 -4.2138877 -4.1951637 -4.1759839 -4.1455641 -4.1190763 -4.1156187 -4.132503 -4.1533141 -4.1579871 -4.1408443 -4.1137304 -4.0942378][-4.286634 -4.261764 -4.2443838 -4.2346692 -4.2226996 -4.2097311 -4.191565 -4.1738443 -4.1661019 -4.1689982 -4.175077 -4.1684375 -4.1463375 -4.1177421 -4.1062021][-4.2939644 -4.27357 -4.256114 -4.2443805 -4.2346497 -4.2260008 -4.2140746 -4.2008858 -4.1920972 -4.1890621 -4.186646 -4.1761761 -4.1566334 -4.1333256 -4.1294575][-4.30643 -4.2903147 -4.2736111 -4.2611256 -4.2547264 -4.2487822 -4.2402453 -4.2303863 -4.2233968 -4.2185926 -4.2128692 -4.2032156 -4.1891594 -4.170548 -4.1666389][-4.3177772 -4.30523 -4.2911625 -4.282557 -4.2806478 -4.2808509 -4.2774916 -4.26996 -4.2631474 -4.2561154 -4.2478566 -4.24122 -4.233871 -4.2224646 -4.2181377]]...]
INFO - root - 2017-12-07 10:59:50.163336: step 6610, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 86h:58m:27s remains)
INFO - root - 2017-12-07 10:59:59.802111: step 6620, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 86h:03m:52s remains)
INFO - root - 2017-12-07 11:00:09.552865: step 6630, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 88h:24m:17s remains)
INFO - root - 2017-12-07 11:00:19.379498: step 6640, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 87h:19m:54s remains)
INFO - root - 2017-12-07 11:00:28.934047: step 6650, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 84h:40m:11s remains)
INFO - root - 2017-12-07 11:00:38.469520: step 6660, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 84h:59m:23s remains)
INFO - root - 2017-12-07 11:00:48.245355: step 6670, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 89h:24m:24s remains)
INFO - root - 2017-12-07 11:00:57.876844: step 6680, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 83h:56m:09s remains)
INFO - root - 2017-12-07 11:01:07.602496: step 6690, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 86h:23m:26s remains)
INFO - root - 2017-12-07 11:01:17.263352: step 6700, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 84h:56m:13s remains)
2017-12-07 11:01:18.217139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3187647 -4.3060184 -4.2790422 -4.2605548 -4.2504215 -4.2482672 -4.2515383 -4.2593336 -4.2545614 -4.237905 -4.2218604 -4.2187691 -4.2285943 -4.2442989 -4.2566857][-4.3238354 -4.3149896 -4.2935209 -4.2790718 -4.2709894 -4.2691779 -4.269454 -4.2682614 -4.2548056 -4.2308655 -4.2089977 -4.2040663 -4.2161632 -4.2375026 -4.2518783][-4.3305407 -4.32439 -4.3037362 -4.2871509 -4.2759929 -4.2703767 -4.2632332 -4.250474 -4.233058 -4.2120504 -4.1968117 -4.1980553 -4.2102933 -4.2333 -4.2484241][-4.3373079 -4.3330035 -4.3102374 -4.2855382 -4.2623854 -4.2443571 -4.2206798 -4.1928759 -4.179173 -4.1734104 -4.1765442 -4.1878004 -4.2005649 -4.2243629 -4.2416883][-4.340414 -4.3354111 -4.3069882 -4.2689524 -4.2314906 -4.1957707 -4.14562 -4.100256 -4.0940013 -4.111177 -4.1382885 -4.1661248 -4.1913776 -4.2192669 -4.2390795][-4.337327 -4.32717 -4.2887006 -4.2336254 -4.1830025 -4.1253543 -4.0402713 -3.975039 -3.9888844 -4.045083 -4.0997162 -4.1465425 -4.1837463 -4.2162681 -4.2360344][-4.3279095 -4.3108821 -4.2622685 -4.1921463 -4.1241283 -4.0369263 -3.9127581 -3.8392572 -3.8930748 -3.9930923 -4.0705624 -4.1294417 -4.172853 -4.2062421 -4.2277408][-4.3183241 -4.2934613 -4.2338152 -4.1512251 -4.0635738 -3.9428024 -3.7928634 -3.7365665 -3.8370872 -3.9691832 -4.0600152 -4.1216526 -4.1626396 -4.1945319 -4.2205567][-4.3117862 -4.2808094 -4.2135782 -4.1285467 -4.0368791 -3.911047 -3.7838593 -3.7613153 -3.8748615 -4.0020905 -4.0810866 -4.1327939 -4.1660314 -4.1920738 -4.2168484][-4.3089151 -4.2794914 -4.2187791 -4.1484089 -4.0765839 -3.9833624 -3.9076357 -3.9058616 -3.9896541 -4.0764661 -4.129034 -4.1662512 -4.189508 -4.2074924 -4.2280107][-4.3075819 -4.2852645 -4.2389426 -4.1907134 -4.1449518 -4.0894547 -4.0536695 -4.0603375 -4.1090174 -4.1579404 -4.1894078 -4.2149391 -4.2280641 -4.2364807 -4.250268][-4.3068833 -4.2901216 -4.2586226 -4.2304697 -4.206357 -4.1789846 -4.1642661 -4.1728559 -4.2007508 -4.229569 -4.2488427 -4.2649369 -4.268117 -4.2688956 -4.2744422][-4.3102098 -4.2974796 -4.27549 -4.2586336 -4.2480235 -4.2414355 -4.2402272 -4.2500067 -4.2673597 -4.2859745 -4.2963719 -4.3033853 -4.29982 -4.295661 -4.2927032][-4.3176584 -4.3091588 -4.2907434 -4.2772202 -4.2719917 -4.2763042 -4.2838559 -4.294827 -4.3085384 -4.3231945 -4.3292027 -4.3300872 -4.3216062 -4.3136511 -4.3063645][-4.3211212 -4.3133497 -4.2926073 -4.2785511 -4.2750082 -4.2824545 -4.2961259 -4.31147 -4.3255596 -4.3399272 -4.3459315 -4.3441296 -4.3349304 -4.325747 -4.3166246]]...]
INFO - root - 2017-12-07 11:01:27.994079: step 6710, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 89h:56m:46s remains)
INFO - root - 2017-12-07 11:01:37.628322: step 6720, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 87h:09m:14s remains)
INFO - root - 2017-12-07 11:01:47.185067: step 6730, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 86h:49m:47s remains)
INFO - root - 2017-12-07 11:01:56.749355: step 6740, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 88h:48m:21s remains)
INFO - root - 2017-12-07 11:02:06.444350: step 6750, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.016 sec/batch; 91h:58m:14s remains)
INFO - root - 2017-12-07 11:02:16.046996: step 6760, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 84h:26m:01s remains)
INFO - root - 2017-12-07 11:02:25.813673: step 6770, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 81h:33m:14s remains)
INFO - root - 2017-12-07 11:02:35.538265: step 6780, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.025 sec/batch; 92h:43m:44s remains)
INFO - root - 2017-12-07 11:02:45.402097: step 6790, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.024 sec/batch; 92h:40m:05s remains)
INFO - root - 2017-12-07 11:02:55.092749: step 6800, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 85h:42m:49s remains)
2017-12-07 11:02:56.028594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2700119 -4.2435508 -4.2276373 -4.2229795 -4.2431021 -4.2675347 -4.27319 -4.2702003 -4.2733784 -4.2672844 -4.2595453 -4.2436008 -4.2409143 -4.2357926 -4.2219095][-4.2657843 -4.2338367 -4.2083955 -4.1962433 -4.2101874 -4.2310114 -4.233315 -4.2306337 -4.2397442 -4.2460356 -4.2439165 -4.2272596 -4.2220254 -4.2205858 -4.20311][-4.2674513 -4.2349896 -4.2039638 -4.1834011 -4.1864157 -4.1966681 -4.1900663 -4.1824446 -4.1960568 -4.2171874 -4.2198863 -4.2020936 -4.1936469 -4.1900711 -4.1643434][-4.2686195 -4.2373796 -4.203424 -4.1747875 -4.1649227 -4.1665373 -4.1532083 -4.1420426 -4.1577005 -4.189467 -4.1949935 -4.1721063 -4.1590996 -4.15011 -4.12156][-4.2738562 -4.2441845 -4.2117429 -4.1795735 -4.1630125 -4.1588874 -4.146975 -4.1332607 -4.1453791 -4.1812053 -4.1880903 -4.1635275 -4.1540689 -4.1446619 -4.1182313][-4.2743645 -4.2447453 -4.2148643 -4.1862326 -4.1726537 -4.1665659 -4.1563597 -4.1430621 -4.1523576 -4.1867495 -4.1993923 -4.1875777 -4.1838245 -4.1761127 -4.1559448][-4.2624025 -4.2282805 -4.1980262 -4.176012 -4.1653919 -4.1537085 -4.1423144 -4.1365604 -4.1503682 -4.1801152 -4.2011609 -4.2034554 -4.2046423 -4.2018065 -4.189096][-4.2432575 -4.2030616 -4.1722 -4.157033 -4.1473503 -4.1247168 -4.10495 -4.0985684 -4.1150942 -4.1413751 -4.1654344 -4.1795654 -4.1876378 -4.1902385 -4.1866589][-4.2210889 -4.1717567 -4.1358852 -4.1207447 -4.1088061 -4.0773234 -4.043828 -4.0274134 -4.044323 -4.0718641 -4.1007261 -4.1285729 -4.14752 -4.1572437 -4.1629205][-4.1973886 -4.1393547 -4.0955453 -4.0763865 -4.0673413 -4.040513 -4.0034518 -3.9783204 -3.9920704 -4.021656 -4.0541611 -4.0904889 -4.1186395 -4.1383452 -4.1534691][-4.1913838 -4.1326628 -4.0848255 -4.0643206 -4.0599518 -4.0473752 -4.0240021 -4.0061011 -4.0176978 -4.0416765 -4.0694661 -4.103951 -4.1352882 -4.1629872 -4.1849132][-4.2165046 -4.1697311 -4.1312194 -4.1143293 -4.1145782 -4.1127729 -4.1047916 -4.0944414 -4.0996394 -4.117558 -4.1389632 -4.1638818 -4.1911077 -4.2172904 -4.2362447][-4.2613668 -4.2297759 -4.2082376 -4.201479 -4.2055435 -4.2068877 -4.20526 -4.1988044 -4.1956539 -4.2046165 -4.2186027 -4.2323971 -4.2480364 -4.2652836 -4.2768917][-4.2993093 -4.2784581 -4.2655811 -4.2626729 -4.267158 -4.269474 -4.2700257 -4.2649407 -4.259912 -4.2634091 -4.2708817 -4.2781267 -4.2874894 -4.2984996 -4.3061242][-4.3191633 -4.3049726 -4.2958136 -4.2947674 -4.2988825 -4.3014755 -4.3028331 -4.2992139 -4.2945905 -4.2942009 -4.2966914 -4.3002853 -4.3061247 -4.3145618 -4.3220387]]...]
INFO - root - 2017-12-07 11:03:05.479517: step 6810, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 85h:17m:14s remains)
INFO - root - 2017-12-07 11:03:15.090746: step 6820, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 86h:31m:37s remains)
INFO - root - 2017-12-07 11:03:24.672471: step 6830, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 84h:46m:27s remains)
INFO - root - 2017-12-07 11:03:34.359141: step 6840, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 84h:39m:38s remains)
INFO - root - 2017-12-07 11:03:44.001679: step 6850, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 89h:10m:13s remains)
INFO - root - 2017-12-07 11:03:53.783697: step 6860, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 89h:06m:05s remains)
INFO - root - 2017-12-07 11:04:03.502116: step 6870, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 82h:16m:30s remains)
INFO - root - 2017-12-07 11:04:13.158908: step 6880, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 87h:43m:23s remains)
INFO - root - 2017-12-07 11:04:22.874323: step 6890, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 91h:30m:21s remains)
INFO - root - 2017-12-07 11:04:32.683645: step 6900, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 88h:39m:04s remains)
2017-12-07 11:04:33.698059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2602043 -4.2381458 -4.2202139 -4.2099686 -4.2039795 -4.1858668 -4.1657887 -4.1523228 -4.1691704 -4.2051611 -4.2274561 -4.2353048 -4.2409444 -4.2374506 -4.212317][-4.2670612 -4.2378654 -4.2121768 -4.2013841 -4.1997275 -4.1819043 -4.152864 -4.1293006 -4.1497321 -4.196125 -4.2257857 -4.2360272 -4.2424035 -4.2389455 -4.2081871][-4.2700124 -4.2339454 -4.2039037 -4.1946464 -4.1985016 -4.1865869 -4.1534104 -4.1212573 -4.139514 -4.1906228 -4.2234268 -4.239563 -4.249227 -4.2463088 -4.2113419][-4.2698984 -4.2325296 -4.2038445 -4.1953931 -4.197587 -4.1876163 -4.1479478 -4.0997572 -4.1105862 -4.1660113 -4.2056012 -4.2292466 -4.2441797 -4.2457447 -4.2127151][-4.2704177 -4.2381406 -4.21192 -4.1960139 -4.1871228 -4.17316 -4.1156015 -4.0371408 -4.0390735 -4.1169581 -4.1755862 -4.2074614 -4.2292552 -4.2419887 -4.2204151][-4.2733865 -4.2481475 -4.2222281 -4.1910129 -4.1583948 -4.1226974 -4.0292697 -3.9005084 -3.9000728 -4.0304503 -4.1329927 -4.1888733 -4.2297053 -4.2568507 -4.247581][-4.2741008 -4.2543631 -4.2301931 -4.1928673 -4.1423478 -4.0699792 -3.9215302 -3.7352152 -3.7564077 -3.950254 -4.0960331 -4.1773782 -4.24116 -4.2816911 -4.2799287][-4.2752886 -4.2587519 -4.2366476 -4.2115297 -4.1695752 -4.089622 -3.9337497 -3.7561321 -3.7956061 -3.9727743 -4.1009817 -4.1800022 -4.2475152 -4.2914958 -4.2916632][-4.2747669 -4.2556086 -4.2339048 -4.2247043 -4.1982193 -4.1385231 -4.0352783 -3.9338698 -3.9613662 -4.0677586 -4.1435714 -4.1947994 -4.2418389 -4.27697 -4.2746444][-4.2793956 -4.2585015 -4.2362494 -4.2288055 -4.2038555 -4.15499 -4.0929518 -4.0438323 -4.0676317 -4.1345115 -4.1779847 -4.2099237 -4.2382989 -4.2550864 -4.2467966][-4.2902727 -4.2727804 -4.2558885 -4.249939 -4.2243667 -4.1733432 -4.1282949 -4.0995455 -4.1191072 -4.1634636 -4.1920714 -4.2178688 -4.2370267 -4.2405181 -4.2288547][-4.2980266 -4.286562 -4.2755575 -4.2713323 -4.2508035 -4.2111115 -4.1812286 -4.1591182 -4.1718235 -4.2011461 -4.2214546 -4.2398725 -4.2501888 -4.2476625 -4.23409][-4.3014536 -4.2959776 -4.2913203 -4.2884879 -4.2766976 -4.2513685 -4.2294226 -4.2143259 -4.2329879 -4.2521963 -4.262289 -4.2724147 -4.2724066 -4.2640915 -4.2497988][-4.3002496 -4.2995248 -4.3024774 -4.3021741 -4.2929354 -4.2753868 -4.2569556 -4.2506471 -4.2729521 -4.288672 -4.2937636 -4.301475 -4.299222 -4.292666 -4.2807856][-4.3003764 -4.304575 -4.3146729 -4.31884 -4.3115368 -4.3007703 -4.2876859 -4.2828012 -4.2954063 -4.3056602 -4.3130894 -4.3261538 -4.3285465 -4.32866 -4.3240461]]...]
INFO - root - 2017-12-07 11:04:43.273983: step 6910, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 85h:01m:39s remains)
INFO - root - 2017-12-07 11:04:53.003688: step 6920, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 89h:20m:51s remains)
INFO - root - 2017-12-07 11:05:02.616708: step 6930, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 87h:08m:44s remains)
INFO - root - 2017-12-07 11:05:12.227311: step 6940, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 84h:23m:58s remains)
INFO - root - 2017-12-07 11:05:21.835073: step 6950, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 88h:09m:40s remains)
INFO - root - 2017-12-07 11:05:31.487396: step 6960, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 88h:50m:00s remains)
INFO - root - 2017-12-07 11:05:41.188821: step 6970, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 88h:00m:55s remains)
INFO - root - 2017-12-07 11:05:50.685113: step 6980, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 82h:41m:03s remains)
INFO - root - 2017-12-07 11:06:00.140805: step 6990, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 78h:56m:36s remains)
INFO - root - 2017-12-07 11:06:09.870884: step 7000, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 85h:07m:39s remains)
2017-12-07 11:06:10.839570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.242342 -4.2262349 -4.2217336 -4.2192121 -4.2156591 -4.2184396 -4.2210288 -4.2099013 -4.1910567 -4.1755447 -4.170188 -4.187995 -4.2134104 -4.2357039 -4.259604][-4.2141223 -4.1916347 -4.1872077 -4.1896973 -4.1887846 -4.1920619 -4.194303 -4.1798015 -4.1584883 -4.143527 -4.1411867 -4.1636462 -4.1911125 -4.2145081 -4.2398543][-4.1802912 -4.1562462 -4.1560345 -4.1661019 -4.1686268 -4.17329 -4.1751008 -4.1588964 -4.1387148 -4.1288075 -4.1328435 -4.1557922 -4.1801996 -4.1999831 -4.2262707][-4.1476912 -4.1211152 -4.1198606 -4.1306 -4.132905 -4.1359529 -4.1339712 -4.1174016 -4.1038704 -4.1080351 -4.1267576 -4.1545553 -4.1762261 -4.1932116 -4.221262][-4.1219773 -4.08659 -4.079916 -4.0855174 -4.0826697 -4.0762653 -4.0600142 -4.033875 -4.0295758 -4.0586157 -4.0994725 -4.1383691 -4.1641579 -4.1852746 -4.2168336][-4.0956783 -4.0496287 -4.0366936 -4.0379939 -4.0256438 -4.003777 -3.9628742 -3.9199393 -3.931108 -3.9916124 -4.0550585 -4.1081462 -4.1441255 -4.1724806 -4.210309][-4.0732355 -4.0238652 -4.0057192 -3.9975963 -3.9728358 -3.9332933 -3.866823 -3.8085036 -3.8411665 -3.9294102 -4.00741 -4.0716324 -4.12115 -4.1601653 -4.2029138][-4.0960875 -4.0589767 -4.043447 -4.0306282 -4.0013318 -3.9566636 -3.8900352 -3.8427114 -3.8787398 -3.9582508 -4.0212235 -4.0776711 -4.1280808 -4.166048 -4.2040467][-4.145278 -4.1288323 -4.1238208 -4.1185603 -4.0971828 -4.0601664 -4.0100946 -3.9810264 -4.0042772 -4.0499 -4.0845537 -4.1242867 -4.164207 -4.1916175 -4.2188888][-4.2035546 -4.2043648 -4.2111931 -4.2156239 -4.2056761 -4.1798391 -4.1464467 -4.1307707 -4.1403446 -4.15861 -4.170157 -4.1932478 -4.2209854 -4.2354918 -4.2500396][-4.2551074 -4.2632027 -4.2736812 -4.2816353 -4.2791777 -4.26538 -4.2443805 -4.2360587 -4.2390366 -4.243474 -4.2436323 -4.2548857 -4.2717423 -4.2773333 -4.2829576][-4.2865067 -4.2915177 -4.2966533 -4.301477 -4.3013597 -4.2951164 -4.2833152 -4.2792206 -4.2807231 -4.2825766 -4.2800913 -4.2863121 -4.2989988 -4.3040094 -4.3061314][-4.2854671 -4.286355 -4.2869878 -4.2892542 -4.2907219 -4.2897258 -4.2827411 -4.2796063 -4.2810764 -4.282938 -4.2822514 -4.2880878 -4.3006225 -4.3090453 -4.3128185][-4.2780647 -4.27804 -4.2775807 -4.2790785 -4.2804394 -4.2802444 -4.2760167 -4.2737975 -4.2747636 -4.2765164 -4.2764964 -4.2823939 -4.294672 -4.305325 -4.3119435][-4.2878909 -4.28875 -4.2890568 -4.2908311 -4.2920723 -4.2912607 -4.2885351 -4.2874708 -4.2871733 -4.2868433 -4.286799 -4.29101 -4.299377 -4.3076982 -4.3141708]]...]
INFO - root - 2017-12-07 11:06:20.350306: step 7010, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 85h:51m:11s remains)
INFO - root - 2017-12-07 11:06:30.068513: step 7020, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 88h:25m:49s remains)
INFO - root - 2017-12-07 11:06:39.984902: step 7030, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 88h:54m:16s remains)
INFO - root - 2017-12-07 11:06:49.571185: step 7040, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 91h:16m:34s remains)
INFO - root - 2017-12-07 11:06:59.398994: step 7050, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.005 sec/batch; 90h:48m:42s remains)
INFO - root - 2017-12-07 11:07:09.169295: step 7060, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 82h:58m:29s remains)
INFO - root - 2017-12-07 11:07:18.821963: step 7070, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 87h:39m:51s remains)
INFO - root - 2017-12-07 11:07:28.415820: step 7080, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 88h:24m:23s remains)
INFO - root - 2017-12-07 11:07:38.188607: step 7090, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 87h:12m:12s remains)
INFO - root - 2017-12-07 11:07:47.954855: step 7100, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.930 sec/batch; 84h:04m:33s remains)
2017-12-07 11:07:48.850119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3167071 -4.3209405 -4.3199444 -4.313674 -4.2992039 -4.274229 -4.2464867 -4.2248378 -4.215858 -4.2288074 -4.2511568 -4.2709246 -4.2809806 -4.2762728 -4.2551117][-4.3182874 -4.3216982 -4.3218184 -4.3113408 -4.2856278 -4.2456226 -4.2055678 -4.1796989 -4.1752625 -4.1993861 -4.2357349 -4.2649264 -4.2818794 -4.282629 -4.2655177][-4.3093004 -4.3108416 -4.309823 -4.29274 -4.2544136 -4.1987329 -4.1468425 -4.118062 -4.1227403 -4.1570044 -4.205934 -4.2468872 -4.2694545 -4.2739358 -4.2608576][-4.3061934 -4.3058667 -4.3006878 -4.2749181 -4.2210655 -4.1451397 -4.0789456 -4.0484967 -4.0652995 -4.112504 -4.1742826 -4.2281904 -4.2561994 -4.2615814 -4.2513113][-4.316731 -4.3135734 -4.3032455 -4.2700448 -4.2020721 -4.1074557 -4.0272274 -3.9945068 -4.0195975 -4.0796862 -4.1494803 -4.2109027 -4.2422876 -4.2501044 -4.2449875][-4.320961 -4.3145909 -4.2975068 -4.2555223 -4.1768155 -4.0718832 -3.98438 -3.9486926 -3.9762015 -4.0458593 -4.1180382 -4.1803493 -4.2176995 -4.2354441 -4.2425661][-4.3141627 -4.3048873 -4.2805014 -4.2295027 -4.1442494 -4.038856 -3.9525197 -3.9135745 -3.9404244 -4.0165629 -4.0916648 -4.151825 -4.1957712 -4.2267771 -4.2462277][-4.3095851 -4.298471 -4.2689314 -4.211978 -4.1318345 -4.0418272 -3.9657388 -3.92546 -3.9466341 -4.0207253 -4.0949593 -4.1538696 -4.202415 -4.2369246 -4.2563238][-4.2947731 -4.2837729 -4.2505732 -4.1927209 -4.1273408 -4.0678577 -4.01737 -3.9833891 -3.9937322 -4.0521178 -4.1172194 -4.1726851 -4.2191081 -4.2472854 -4.2526035][-4.2692885 -4.2618556 -4.2314568 -4.1816754 -4.1361623 -4.1056452 -4.080482 -4.0574212 -4.0595675 -4.0943642 -4.1451855 -4.1951928 -4.2309065 -4.2417717 -4.223711][-4.2605429 -4.257021 -4.2327161 -4.1970577 -4.1716847 -4.1582518 -4.1497793 -4.1406384 -4.1393237 -4.153101 -4.1835303 -4.2191563 -4.2379317 -4.22961 -4.1919394][-4.2689157 -4.269609 -4.2530732 -4.2303958 -4.2184725 -4.2137246 -4.2142806 -4.2166576 -4.2159615 -4.2167807 -4.2308779 -4.2505651 -4.2531967 -4.2292814 -4.1810503][-4.2852941 -4.2893486 -4.2826791 -4.2718449 -4.2679362 -4.2676368 -4.2703633 -4.274075 -4.2733 -4.2702732 -4.2753172 -4.2817688 -4.2741332 -4.2431264 -4.1952963][-4.2951117 -4.3010817 -4.3003259 -4.2991977 -4.298882 -4.300097 -4.303194 -4.3048787 -4.3028579 -4.2982616 -4.2976937 -4.2965312 -4.2875733 -4.2619209 -4.22419][-4.3013239 -4.3048906 -4.3051853 -4.3071213 -4.3082294 -4.308877 -4.3098989 -4.3106413 -4.3094583 -4.3068624 -4.3052998 -4.3028865 -4.2970424 -4.28142 -4.2599273]]...]
INFO - root - 2017-12-07 11:07:58.476958: step 7110, loss = 2.12, batch loss = 2.06 (8.0 examples/sec; 0.998 sec/batch; 90h:11m:12s remains)
INFO - root - 2017-12-07 11:08:08.323269: step 7120, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 86h:12m:06s remains)
INFO - root - 2017-12-07 11:08:17.975010: step 7130, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 83h:27m:40s remains)
INFO - root - 2017-12-07 11:08:27.676384: step 7140, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 88h:35m:08s remains)
INFO - root - 2017-12-07 11:08:37.224132: step 7150, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 88h:37m:31s remains)
INFO - root - 2017-12-07 11:08:46.748625: step 7160, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 88h:56m:46s remains)
INFO - root - 2017-12-07 11:08:56.350947: step 7170, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 91h:05m:10s remains)
INFO - root - 2017-12-07 11:09:06.050913: step 7180, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 84h:07m:31s remains)
INFO - root - 2017-12-07 11:09:15.638665: step 7190, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 89h:35m:59s remains)
INFO - root - 2017-12-07 11:09:25.209928: step 7200, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 88h:12m:02s remains)
2017-12-07 11:09:26.131184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2857857 -4.248816 -4.2146826 -4.1952353 -4.20175 -4.2168527 -4.2255621 -4.2210865 -4.2207012 -4.215373 -4.20957 -4.2191467 -4.2337737 -4.2393847 -4.2148738][-4.3025732 -4.276135 -4.2512236 -4.2343421 -4.2285266 -4.2284117 -4.2332211 -4.2368422 -4.2416739 -4.2357268 -4.224328 -4.2245564 -4.2267408 -4.2238011 -4.1994438][-4.3205237 -4.3041258 -4.2856355 -4.2646656 -4.242785 -4.2294226 -4.233253 -4.2476878 -4.263732 -4.2661142 -4.2592349 -4.256918 -4.2490048 -4.2372489 -4.215055][-4.3343949 -4.3303881 -4.3156114 -4.2858567 -4.2437906 -4.213151 -4.2110085 -4.2337308 -4.2651196 -4.282505 -4.2901139 -4.2948403 -4.2849703 -4.2674518 -4.2477889][-4.3436079 -4.3514047 -4.3376775 -4.2956438 -4.224668 -4.1613169 -4.1365948 -4.1623387 -4.2220917 -4.2669425 -4.2956843 -4.3119564 -4.306118 -4.2883015 -4.2731347][-4.3376813 -4.3536081 -4.3418779 -4.2907014 -4.19162 -4.0796022 -4.0064058 -4.0186248 -4.1134377 -4.1997709 -4.2578583 -4.2908583 -4.2994032 -4.2939305 -4.2851558][-4.3198223 -4.3426766 -4.3331223 -4.2761521 -4.1582432 -4.0035381 -3.86806 -3.8459675 -3.9671142 -4.0912147 -4.175343 -4.2312794 -4.2679257 -4.2902293 -4.2950354][-4.3055296 -4.3298569 -4.3210807 -4.2640119 -4.143322 -3.9800959 -3.8207741 -3.7696877 -3.8832979 -4.0064259 -4.0930705 -4.1622877 -4.2199106 -4.2659307 -4.2849379][-4.3128104 -4.3348565 -4.325254 -4.271112 -4.168087 -4.0389986 -3.918011 -3.8754585 -3.9411461 -4.0127745 -4.0677786 -4.1242032 -4.1835756 -4.2370505 -4.2610431][-4.3356619 -4.3536763 -4.3445249 -4.3020043 -4.230742 -4.1521158 -4.08517 -4.0573587 -4.0768127 -4.0951056 -4.1098409 -4.1373634 -4.1798511 -4.2243795 -4.2449918][-4.355031 -4.3673339 -4.3595462 -4.3340845 -4.2984438 -4.2619543 -4.2334743 -4.2158213 -4.207509 -4.1961279 -4.18561 -4.1899147 -4.2114568 -4.23553 -4.2444458][-4.3596725 -4.3628769 -4.3538461 -4.3424215 -4.3360658 -4.329874 -4.3254509 -4.3172669 -4.2995019 -4.2770977 -4.2602086 -4.2569861 -4.2656636 -4.2694807 -4.2607388][-4.3505926 -4.3438015 -4.3357496 -4.3343945 -4.34178 -4.3523483 -4.3635864 -4.3648252 -4.3507094 -4.3304977 -4.3143644 -4.3112607 -4.3169184 -4.3112674 -4.2907739][-4.3317742 -4.322155 -4.3197384 -4.3263435 -4.3397412 -4.3585715 -4.3781648 -4.3851581 -4.3759623 -4.3603764 -4.3468103 -4.3439531 -4.3470988 -4.3392539 -4.3126607][-4.2993007 -4.2923493 -4.29711 -4.3105197 -4.3293815 -4.3553009 -4.3808155 -4.3917089 -4.3873272 -4.3761148 -4.3662677 -4.3619747 -4.3586106 -4.3450947 -4.3132048]]...]
INFO - root - 2017-12-07 11:09:35.942475: step 7210, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 90h:40m:44s remains)
INFO - root - 2017-12-07 11:09:45.538202: step 7220, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 86h:23m:31s remains)
INFO - root - 2017-12-07 11:09:55.188972: step 7230, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 88h:57m:39s remains)
INFO - root - 2017-12-07 11:10:04.891270: step 7240, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 0.998 sec/batch; 90h:11m:14s remains)
INFO - root - 2017-12-07 11:10:14.662919: step 7250, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 87h:13m:57s remains)
INFO - root - 2017-12-07 11:10:24.483748: step 7260, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.044 sec/batch; 94h:19m:57s remains)
INFO - root - 2017-12-07 11:10:34.220638: step 7270, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 87h:45m:24s remains)
INFO - root - 2017-12-07 11:10:43.751341: step 7280, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 84h:30m:00s remains)
INFO - root - 2017-12-07 11:10:53.566274: step 7290, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 88h:24m:01s remains)
INFO - root - 2017-12-07 11:11:03.386389: step 7300, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 88h:01m:55s remains)
2017-12-07 11:11:04.367951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2893238 -4.2800488 -4.2871118 -4.2982073 -4.3050075 -4.3020525 -4.2965517 -4.3024158 -4.3097277 -4.3092394 -4.2969389 -4.2813888 -4.277657 -4.282443 -4.2952375][-4.24103 -4.2217636 -4.2283854 -4.2439775 -4.2539234 -4.2547812 -4.25076 -4.2559953 -4.2626605 -4.2594686 -4.2432046 -4.2292595 -4.231173 -4.2430234 -4.2603459][-4.20447 -4.1771617 -4.1833453 -4.2055712 -4.2205563 -4.2244806 -4.22555 -4.2293477 -4.2333679 -4.22447 -4.2067037 -4.2013674 -4.2132244 -4.2308893 -4.2485571][-4.1929564 -4.1656294 -4.1716995 -4.1935496 -4.2082644 -4.2126722 -4.2155805 -4.2171845 -4.222373 -4.2168169 -4.2087336 -4.2147145 -4.2281957 -4.2416978 -4.2545695][-4.195869 -4.1741385 -4.1805654 -4.1954265 -4.2018452 -4.1952229 -4.18801 -4.1834664 -4.1956873 -4.211482 -4.2214341 -4.2344666 -4.2415562 -4.2438049 -4.2481351][-4.2069178 -4.191999 -4.2010365 -4.2110457 -4.2050996 -4.1797762 -4.1418347 -4.1145811 -4.138742 -4.191371 -4.2268028 -4.244925 -4.2476649 -4.2414322 -4.2391281][-4.2241526 -4.2144866 -4.2255096 -4.2336216 -4.2165265 -4.1675844 -4.0838308 -4.0130825 -4.0508642 -4.1537609 -4.215229 -4.2424541 -4.25001 -4.2459731 -4.2415733][-4.2361617 -4.2293453 -4.2400146 -4.249557 -4.2336197 -4.1763668 -4.0726514 -3.9805033 -4.0156727 -4.1325564 -4.2056541 -4.236455 -4.2526116 -4.25654 -4.2525311][-4.2436819 -4.24322 -4.2600942 -4.2766213 -4.270009 -4.225152 -4.1485853 -4.0853992 -4.1015077 -4.1724987 -4.2215109 -4.23983 -4.2537889 -4.2581282 -4.2544093][-4.2447252 -4.2475796 -4.2685614 -4.2926426 -4.295258 -4.2713408 -4.2308249 -4.1976728 -4.203021 -4.2343707 -4.2509727 -4.2501574 -4.2518029 -4.2490573 -4.2437124][-4.2398076 -4.2414017 -4.2635593 -4.2879815 -4.2932048 -4.2844238 -4.2737327 -4.26259 -4.2671852 -4.2802849 -4.2822623 -4.275877 -4.2684956 -4.2586918 -4.249197][-4.2475753 -4.2468786 -4.2661881 -4.2829885 -4.2856669 -4.2821264 -4.2846842 -4.2904935 -4.29702 -4.307313 -4.3100171 -4.3121052 -4.309237 -4.2987528 -4.2840772][-4.2720857 -4.2721739 -4.2845144 -4.2936606 -4.2960868 -4.2926192 -4.2953105 -4.3049097 -4.3149815 -4.3272519 -4.3347054 -4.3400865 -4.3397489 -4.33235 -4.3172469][-4.2924662 -4.2943616 -4.3025236 -4.30579 -4.3059225 -4.3019691 -4.3044748 -4.3145456 -4.325284 -4.3374844 -4.3454385 -4.350379 -4.3509417 -4.3451986 -4.3305893][-4.3050227 -4.3060012 -4.3089871 -4.3086104 -4.308599 -4.3074007 -4.30772 -4.312633 -4.3239031 -4.3374085 -4.34401 -4.3445778 -4.3452535 -4.3436351 -4.3319736]]...]
INFO - root - 2017-12-07 11:11:14.130191: step 7310, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.912 sec/batch; 82h:23m:05s remains)
INFO - root - 2017-12-07 11:11:23.984296: step 7320, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 88h:41m:12s remains)
INFO - root - 2017-12-07 11:11:33.512645: step 7330, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 84h:57m:02s remains)
INFO - root - 2017-12-07 11:11:43.232560: step 7340, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.006 sec/batch; 90h:50m:07s remains)
INFO - root - 2017-12-07 11:11:52.925625: step 7350, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 85h:32m:04s remains)
INFO - root - 2017-12-07 11:12:02.639912: step 7360, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 88h:48m:01s remains)
INFO - root - 2017-12-07 11:12:12.221985: step 7370, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 90h:29m:41s remains)
INFO - root - 2017-12-07 11:12:21.953726: step 7380, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 88h:01m:20s remains)
INFO - root - 2017-12-07 11:12:31.686030: step 7390, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 88h:18m:07s remains)
INFO - root - 2017-12-07 11:12:41.315266: step 7400, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 86h:28m:35s remains)
2017-12-07 11:12:42.272964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3329654 -4.3327703 -4.322772 -4.3061676 -4.2649746 -4.2054315 -4.1495848 -4.1275325 -4.1459446 -4.191669 -4.2323866 -4.2556043 -4.2665129 -4.268961 -4.2698417][-4.328198 -4.3251863 -4.3190446 -4.3080845 -4.2713132 -4.217052 -4.1631112 -4.136672 -4.1485448 -4.191277 -4.2292061 -4.2542181 -4.271554 -4.2783089 -4.2812185][-4.3255458 -4.3171248 -4.3105197 -4.3035188 -4.2779341 -4.2307286 -4.1735573 -4.1357493 -4.1420565 -4.1849318 -4.2241015 -4.2527323 -4.27808 -4.2938128 -4.30397][-4.3267016 -4.3135023 -4.3032241 -4.2909651 -4.2617655 -4.2100277 -4.143836 -4.1007352 -4.115386 -4.1701241 -4.2178397 -4.2531376 -4.2845755 -4.3065777 -4.3195314][-4.3341551 -4.3194375 -4.3053231 -4.2757616 -4.2211084 -4.1393251 -4.0473938 -4.013978 -4.0661917 -4.1475248 -4.2057714 -4.2461858 -4.281909 -4.308466 -4.3218551][-4.3350568 -4.3244362 -4.304625 -4.2538457 -4.1605573 -4.0268059 -3.8911116 -3.8836122 -4.0013657 -4.1203632 -4.1908364 -4.2363763 -4.2784829 -4.312386 -4.3270464][-4.3299718 -4.3246613 -4.3035293 -4.238893 -4.1167545 -3.9422436 -3.7705603 -3.7923498 -3.9621892 -4.1043229 -4.1811748 -4.2333965 -4.2815671 -4.3183479 -4.3328228][-4.3231778 -4.3211346 -4.3028822 -4.2418709 -4.1307721 -3.9777837 -3.830663 -3.8370697 -3.977134 -4.1000776 -4.1732793 -4.2313242 -4.2827468 -4.3183384 -4.3328309][-4.3217325 -4.3209748 -4.3077946 -4.262857 -4.184052 -4.0844903 -3.9854782 -3.9660606 -4.0349827 -4.1121883 -4.1734428 -4.2334185 -4.2859592 -4.3177462 -4.3314581][-4.323442 -4.3207045 -4.3107052 -4.2813492 -4.2289538 -4.1696148 -4.1085148 -4.0804224 -4.1034751 -4.1451993 -4.1951694 -4.2510381 -4.2973633 -4.3232155 -4.3348022][-4.3289757 -4.3232355 -4.312007 -4.2896948 -4.2537856 -4.212965 -4.1700687 -4.146378 -4.1567669 -4.1866684 -4.2302446 -4.2784443 -4.3136826 -4.3326979 -4.3403063][-4.3323226 -4.3250518 -4.3129482 -4.2920527 -4.260725 -4.224546 -4.1896224 -4.1732683 -4.1851344 -4.217299 -4.2628946 -4.3040123 -4.3296776 -4.341547 -4.3438821][-4.3332534 -4.3280911 -4.3173046 -4.2957277 -4.2654591 -4.2320223 -4.2038832 -4.1940708 -4.2074447 -4.2427297 -4.2866225 -4.32042 -4.3379388 -4.3437133 -4.3415318][-4.32936 -4.3281078 -4.3206744 -4.3006773 -4.273365 -4.2427506 -4.2196145 -4.21476 -4.2290654 -4.2610626 -4.2982683 -4.3232193 -4.3338609 -4.3361883 -4.3337164][-4.3242159 -4.3253865 -4.32044 -4.3045111 -4.2831779 -4.2600117 -4.2430019 -4.2420268 -4.2559371 -4.2800069 -4.3047667 -4.319838 -4.3252273 -4.3264017 -4.3254409]]...]
INFO - root - 2017-12-07 11:12:52.034238: step 7410, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 85h:35m:52s remains)
INFO - root - 2017-12-07 11:13:01.594359: step 7420, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 82h:33m:30s remains)
INFO - root - 2017-12-07 11:13:11.168935: step 7430, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 90h:16m:56s remains)
INFO - root - 2017-12-07 11:13:20.692662: step 7440, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 84h:29m:17s remains)
INFO - root - 2017-12-07 11:13:30.440152: step 7450, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.952 sec/batch; 85h:56m:01s remains)
INFO - root - 2017-12-07 11:13:40.207083: step 7460, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.927 sec/batch; 83h:43m:21s remains)
INFO - root - 2017-12-07 11:13:49.853573: step 7470, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 84h:32m:11s remains)
INFO - root - 2017-12-07 11:13:59.491800: step 7480, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 86h:34m:47s remains)
INFO - root - 2017-12-07 11:14:09.201726: step 7490, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 89h:20m:37s remains)
INFO - root - 2017-12-07 11:14:18.591788: step 7500, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 84h:05m:50s remains)
2017-12-07 11:14:19.518617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.343646 -4.3376956 -4.334177 -4.3275681 -4.3081684 -4.2837124 -4.272151 -4.2797933 -4.2883883 -4.2924051 -4.2908883 -4.2868638 -4.2750907 -4.2610354 -4.2570214][-4.3372641 -4.3299842 -4.323246 -4.3054376 -4.2710419 -4.231442 -4.2147012 -4.2312756 -4.2555089 -4.2728848 -4.2777972 -4.2729359 -4.2533946 -4.2276616 -4.2163043][-4.3321528 -4.3275204 -4.3173451 -4.2864094 -4.2327285 -4.1728306 -4.1472654 -4.1758976 -4.2227859 -4.2565026 -4.2718139 -4.2691369 -4.2429471 -4.2028127 -4.1828194][-4.3295078 -4.3285952 -4.3147225 -4.2699747 -4.1954961 -4.1096044 -4.0671563 -4.1103768 -4.1876545 -4.2400322 -4.2636166 -4.2601242 -4.2281346 -4.1797419 -4.1562395][-4.3237457 -4.3218455 -4.3003669 -4.2434883 -4.1472573 -4.0358281 -3.9758701 -4.0278692 -4.1369095 -4.2165179 -4.2554183 -4.2549405 -4.2191248 -4.1648731 -4.1451511][-4.3165727 -4.3067379 -4.2776785 -4.2129831 -4.1037579 -3.9678955 -3.8770103 -3.9344606 -4.0807748 -4.19051 -4.2439737 -4.2489982 -4.2138219 -4.163507 -4.1562257][-4.3039684 -4.2841778 -4.2474251 -4.183075 -4.0798345 -3.9367385 -3.8163519 -3.866226 -4.0362568 -4.1612043 -4.2162919 -4.2188711 -4.1821995 -4.1477408 -4.1645226][-4.2798767 -4.2563415 -4.2219872 -4.1777363 -4.1110535 -4.0111585 -3.9186807 -3.9401417 -4.0642433 -4.1585011 -4.1959224 -4.1881342 -4.1484027 -4.1273651 -4.1663933][-4.2542629 -4.2340674 -4.2092924 -4.1927562 -4.1669331 -4.1155467 -4.0632262 -4.0667133 -4.13026 -4.1827531 -4.2028461 -4.1918273 -4.1507263 -4.1285772 -4.1698742][-4.2315559 -4.21683 -4.2023053 -4.2030711 -4.200696 -4.1763506 -4.1451235 -4.1428032 -4.1785331 -4.2074976 -4.2146621 -4.2005696 -4.1643634 -4.1421967 -4.177917][-4.2085266 -4.202528 -4.1997442 -4.2100568 -4.2204847 -4.2166033 -4.2009311 -4.1953182 -4.2165132 -4.2315779 -4.2336936 -4.2230353 -4.1956043 -4.1775174 -4.2021527][-4.1987143 -4.1992922 -4.2030382 -4.2150607 -4.2331743 -4.2440629 -4.2401123 -4.23697 -4.2476435 -4.2552776 -4.2587285 -4.2553635 -4.2420921 -4.2282653 -4.2400293][-4.213253 -4.2187204 -4.2258568 -4.2377439 -4.25672 -4.2714272 -4.2721133 -4.271873 -4.2771921 -4.2820373 -4.2868285 -4.2898216 -4.2893515 -4.2828131 -4.2888513][-4.2495494 -4.2576942 -4.2663488 -4.2744455 -4.2883964 -4.3013167 -4.3048849 -4.307642 -4.3117037 -4.3137355 -4.3185744 -4.3230219 -4.3262725 -4.32463 -4.3287973][-4.2916994 -4.2974639 -4.3012605 -4.304049 -4.3115592 -4.3204746 -4.3256817 -4.33012 -4.3328252 -4.3349953 -4.3393111 -4.3420534 -4.3428965 -4.3418202 -4.3434095]]...]
INFO - root - 2017-12-07 11:14:29.337930: step 7510, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 91h:13m:50s remains)
INFO - root - 2017-12-07 11:14:39.065004: step 7520, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 89h:10m:41s remains)
INFO - root - 2017-12-07 11:14:48.560838: step 7530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 86h:52m:22s remains)
INFO - root - 2017-12-07 11:14:58.238419: step 7540, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 88h:20m:51s remains)
INFO - root - 2017-12-07 11:15:08.098765: step 7550, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 88h:27m:40s remains)
INFO - root - 2017-12-07 11:15:17.797450: step 7560, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 84h:54m:22s remains)
INFO - root - 2017-12-07 11:15:27.339860: step 7570, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.976 sec/batch; 88h:07m:36s remains)
INFO - root - 2017-12-07 11:15:36.930821: step 7580, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 91h:28m:24s remains)
INFO - root - 2017-12-07 11:15:46.465075: step 7590, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 86h:14m:28s remains)
INFO - root - 2017-12-07 11:15:56.083558: step 7600, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 87h:07m:02s remains)
2017-12-07 11:15:56.994085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3272176 -4.324419 -4.3166013 -4.3096371 -4.3081284 -4.3069687 -4.3087931 -4.3104239 -4.3113875 -4.3128419 -4.3138318 -4.3125324 -4.3161554 -4.3252645 -4.3326488][-4.3068185 -4.3006859 -4.2886696 -4.277864 -4.2733545 -4.2656522 -4.2629194 -4.2630849 -4.265604 -4.2710109 -4.2783456 -4.2821941 -4.2912536 -4.3052411 -4.315959][-4.2834926 -4.2733283 -4.2570472 -4.2446895 -4.2374558 -4.2213359 -4.2073927 -4.19998 -4.2034717 -4.2137 -4.2295384 -4.2437897 -4.2608614 -4.2795248 -4.2945056][-4.2616496 -4.2463856 -4.2282 -4.2168727 -4.2096691 -4.187242 -4.1597586 -4.1412616 -4.1455631 -4.1574063 -4.1754923 -4.1981654 -4.22601 -4.2505231 -4.2692266][-4.244041 -4.2228646 -4.2031717 -4.1923857 -4.1842155 -4.1563635 -4.11717 -4.0903969 -4.0960488 -4.1073804 -4.1206985 -4.1431479 -4.1759529 -4.2034955 -4.227386][-4.2326255 -4.2045975 -4.1805491 -4.1670156 -4.1548481 -4.1228828 -4.0730162 -4.0390344 -4.052237 -4.0671363 -4.073946 -4.0892963 -4.1186314 -4.1464353 -4.1751986][-4.2292695 -4.1951485 -4.1650066 -4.1472549 -4.12637 -4.0845456 -4.0174141 -3.9716167 -3.9995351 -4.0305095 -4.04265 -4.0563078 -4.0777559 -4.1014652 -4.135231][-4.234642 -4.1977119 -4.1648154 -4.1479211 -4.1219277 -4.0662613 -3.9734626 -3.9100571 -3.9512949 -4.0040164 -4.0297551 -4.0457373 -4.06016 -4.076735 -4.1122518][-4.2466087 -4.2116737 -4.1831503 -4.1728168 -4.1512351 -4.0966458 -4.0036111 -3.9343491 -3.965282 -4.017252 -4.0476913 -4.0626268 -4.0701861 -4.0760689 -4.1051235][-4.2584295 -4.2282352 -4.2079921 -4.2047477 -4.1921163 -4.1531315 -4.0859065 -4.0346513 -4.0440497 -4.0676718 -4.0844688 -4.0988278 -4.10846 -4.1135983 -4.1308737][-4.2686415 -4.2418089 -4.2283478 -4.2276683 -4.220037 -4.1948018 -4.152051 -4.1208272 -4.1219273 -4.1247306 -4.1254177 -4.140039 -4.1579013 -4.1685777 -4.1778455][-4.2737803 -4.2473707 -4.2375326 -4.2381043 -4.2311597 -4.2141633 -4.1875935 -4.1717606 -4.1759229 -4.1730394 -4.1625175 -4.1708007 -4.1915622 -4.2055945 -4.2134027][-4.270494 -4.2421741 -4.2325764 -4.232739 -4.22804 -4.2200089 -4.2075405 -4.207633 -4.2192526 -4.2159467 -4.1996927 -4.1997066 -4.21596 -4.2301278 -4.2392473][-4.2645073 -4.2334309 -4.2174854 -4.2107306 -4.2064452 -4.2029438 -4.2010193 -4.2138252 -4.2369213 -4.2385378 -4.2229624 -4.217721 -4.227591 -4.2387018 -4.2470436][-4.262455 -4.2293944 -4.2065396 -4.1907663 -4.1808529 -4.1759872 -4.1777692 -4.1985984 -4.2300682 -4.2403512 -4.2298408 -4.2214141 -4.226367 -4.2345185 -4.2414522]]...]
INFO - root - 2017-12-07 11:16:06.622181: step 7610, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 89h:53m:49s remains)
INFO - root - 2017-12-07 11:16:16.267585: step 7620, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 84h:29m:51s remains)
INFO - root - 2017-12-07 11:16:25.854142: step 7630, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 87h:33m:07s remains)
INFO - root - 2017-12-07 11:16:35.341107: step 7640, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 85h:00m:41s remains)
INFO - root - 2017-12-07 11:16:45.071306: step 7650, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 80h:00m:13s remains)
INFO - root - 2017-12-07 11:16:54.784805: step 7660, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 83h:46m:33s remains)
INFO - root - 2017-12-07 11:17:04.387499: step 7670, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 89h:51m:14s remains)
INFO - root - 2017-12-07 11:17:14.068399: step 7680, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 87h:08m:41s remains)
INFO - root - 2017-12-07 11:17:23.863607: step 7690, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 87h:05m:42s remains)
INFO - root - 2017-12-07 11:17:33.531575: step 7700, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 87h:30m:38s remains)
2017-12-07 11:17:34.441081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2161255 -4.2375684 -4.2452893 -4.2413812 -4.2303 -4.221839 -4.2305884 -4.2585125 -4.2858949 -4.3035059 -4.3168869 -4.3167291 -4.2987232 -4.2768869 -4.2656217][-4.1415892 -4.1775694 -4.1957541 -4.1943593 -4.1841187 -4.1769342 -4.1880636 -4.2199106 -4.2500606 -4.2641807 -4.2735677 -4.2709146 -4.2520652 -4.229949 -4.2201104][-4.09216 -4.1317868 -4.157 -4.1559415 -4.1501293 -4.1495738 -4.1613503 -4.1917286 -4.2173834 -4.225244 -4.2311525 -4.2308946 -4.2177892 -4.2051287 -4.2033424][-4.1028614 -4.1320696 -4.1514454 -4.1518292 -4.1488991 -4.1490145 -4.1519179 -4.1755667 -4.19629 -4.1980104 -4.2022967 -4.2102456 -4.2122846 -4.2139454 -4.2208934][-4.1697135 -4.1799111 -4.1844497 -4.1772823 -4.1622958 -4.1473465 -4.1313529 -4.147718 -4.167819 -4.168427 -4.1717458 -4.1866584 -4.20052 -4.2118797 -4.2287722][-4.2295756 -4.2205191 -4.2019167 -4.1744666 -4.1379123 -4.1017842 -4.0685587 -4.0852027 -4.1113143 -4.1204152 -4.1296978 -4.1522756 -4.1731462 -4.1932817 -4.22648][-4.2489409 -4.2320719 -4.1951151 -4.1438589 -4.0841665 -4.034936 -4.0031052 -4.0245614 -4.0564051 -4.0724335 -4.0904684 -4.1181436 -4.1447406 -4.1760831 -4.220952][-4.2540727 -4.2402534 -4.1971345 -4.1364985 -4.0649476 -4.0187373 -3.9956863 -4.0138769 -4.0452962 -4.0752525 -4.0998993 -4.1240778 -4.1550851 -4.1962142 -4.2407708][-4.2541323 -4.2474957 -4.2140141 -4.1662579 -4.1081581 -4.0761037 -4.0613089 -4.0710874 -4.0995817 -4.1390896 -4.1653528 -4.1843672 -4.2136369 -4.2509832 -4.2824559][-4.2537017 -4.2534642 -4.2353854 -4.2113361 -4.1822162 -4.1698861 -4.1651049 -4.1728177 -4.1959782 -4.2299976 -4.2514281 -4.2634325 -4.2833853 -4.3050375 -4.3192935][-4.238647 -4.2454429 -4.2460256 -4.2483006 -4.2485323 -4.2531323 -4.2579188 -4.267086 -4.2834959 -4.302206 -4.3106756 -4.3153472 -4.3252163 -4.3367777 -4.34231][-4.2055087 -4.2198362 -4.2420268 -4.2694111 -4.2943277 -4.3115125 -4.3214355 -4.3284459 -4.3373666 -4.3447733 -4.3442988 -4.3437681 -4.3484488 -4.3540878 -4.3550844][-4.1634431 -4.1839075 -4.2219 -4.267406 -4.3063126 -4.33 -4.3400207 -4.3435326 -4.3472781 -4.3505025 -4.3489642 -4.3475609 -4.3499188 -4.3525643 -4.352324][-4.1378508 -4.1601968 -4.2009244 -4.2511711 -4.2955041 -4.3218112 -4.3319445 -4.3337545 -4.3350315 -4.3372178 -4.336771 -4.335484 -4.3358927 -4.3365827 -4.3364825][-4.1405 -4.1615691 -4.1988573 -4.2436647 -4.2828569 -4.3072038 -4.3185358 -4.3226581 -4.3245983 -4.3264627 -4.3264527 -4.3253736 -4.3245754 -4.3239541 -4.3230853]]...]
INFO - root - 2017-12-07 11:17:44.226852: step 7710, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 86h:02m:39s remains)
INFO - root - 2017-12-07 11:17:53.798206: step 7720, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 82h:54m:53s remains)
INFO - root - 2017-12-07 11:18:03.440301: step 7730, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 87h:32m:40s remains)
INFO - root - 2017-12-07 11:18:13.040013: step 7740, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.007 sec/batch; 90h:49m:42s remains)
INFO - root - 2017-12-07 11:18:22.794508: step 7750, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 86h:24m:47s remains)
INFO - root - 2017-12-07 11:18:32.581703: step 7760, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 88h:53m:52s remains)
INFO - root - 2017-12-07 11:18:42.108997: step 7770, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 88h:32m:57s remains)
INFO - root - 2017-12-07 11:18:51.565035: step 7780, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 88h:51m:50s remains)
INFO - root - 2017-12-07 11:19:01.099758: step 7790, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 86h:40m:19s remains)
INFO - root - 2017-12-07 11:19:10.836407: step 7800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 85h:29m:41s remains)
2017-12-07 11:19:11.751620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2856979 -4.269649 -4.2657537 -4.2616472 -4.2544494 -4.2510195 -4.2543197 -4.254086 -4.2509274 -4.249001 -4.2478681 -4.2516694 -4.2677655 -4.2860365 -4.2957883][-4.2553282 -4.2357979 -4.2386532 -4.2421165 -4.2379546 -4.2386312 -4.2436204 -4.2433186 -4.2399383 -4.2360182 -4.2341094 -4.2361269 -4.2505274 -4.2678833 -4.277976][-4.2247982 -4.2038717 -4.2129483 -4.2278652 -4.2312608 -4.2341714 -4.2337193 -4.2272034 -4.2211518 -4.221005 -4.2242618 -4.2283492 -4.2412877 -4.2540717 -4.2614422][-4.2145271 -4.1958776 -4.2053523 -4.2204919 -4.2222271 -4.2167296 -4.2006674 -4.1814842 -4.1797485 -4.1955628 -4.2100215 -4.2226229 -4.2358384 -4.2439876 -4.252615][-4.2194495 -4.2049108 -4.2111211 -4.2212157 -4.2201986 -4.2066031 -4.1776428 -4.1445656 -4.1475067 -4.183218 -4.209188 -4.2285204 -4.2419066 -4.2448039 -4.2527876][-4.2119746 -4.2007213 -4.2059908 -4.2127657 -4.2042737 -4.1789665 -4.1377711 -4.0842009 -4.0871434 -4.1501827 -4.1978168 -4.2285857 -4.2478848 -4.2512121 -4.2586322][-4.2010202 -4.1926451 -4.1941376 -4.1924987 -4.1701975 -4.1223254 -4.053606 -3.9595428 -3.9490724 -4.0518465 -4.1400027 -4.1919479 -4.2225018 -4.2336245 -4.2441072][-4.20678 -4.2057691 -4.2043867 -4.1881881 -4.1452222 -4.0725837 -3.9715161 -3.8323476 -3.7936029 -3.9341352 -4.0602312 -4.1342425 -4.1750884 -4.1926055 -4.2066422][-4.2278914 -4.2361145 -4.2366157 -4.2193022 -4.1788607 -4.1124821 -4.0284266 -3.9167812 -3.8792493 -3.9861827 -4.0923491 -4.1533308 -4.1867127 -4.2002087 -4.2065883][-4.2550893 -4.2668176 -4.2716589 -4.2591567 -4.230514 -4.1848717 -4.1313043 -4.0654826 -4.0372796 -4.1017146 -4.1775413 -4.2190652 -4.2400646 -4.2478428 -4.2463937][-4.2688847 -4.282516 -4.288805 -4.2785645 -4.2604346 -4.2321143 -4.1963735 -4.155787 -4.1322856 -4.1668792 -4.22134 -4.255343 -4.2744503 -4.2827835 -4.2816706][-4.277185 -4.2950516 -4.3063793 -4.3022337 -4.2897768 -4.2691116 -4.2389159 -4.2101412 -4.1945243 -4.2115331 -4.2456741 -4.2703156 -4.2844448 -4.2924981 -4.2929788][-4.2822385 -4.3042493 -4.32034 -4.3201647 -4.3097277 -4.2924752 -4.2701454 -4.2539582 -4.2488203 -4.2580919 -4.2762942 -4.2896786 -4.2953753 -4.2976336 -4.2952871][-4.2856565 -4.3053927 -4.32075 -4.3228979 -4.3140035 -4.3027463 -4.2917047 -4.2887273 -4.2907104 -4.2948012 -4.3009853 -4.3073616 -4.3103447 -4.310184 -4.3071537][-4.2844286 -4.3024807 -4.3169518 -4.32093 -4.3153334 -4.3083477 -4.3046904 -4.3080106 -4.3127661 -4.3142176 -4.3150339 -4.3181772 -4.321291 -4.3224082 -4.3222151]]...]
INFO - root - 2017-12-07 11:19:21.372225: step 7810, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 88h:56m:22s remains)
INFO - root - 2017-12-07 11:19:30.953674: step 7820, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 89h:21m:18s remains)
INFO - root - 2017-12-07 11:19:40.470498: step 7830, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 84h:32m:15s remains)
INFO - root - 2017-12-07 11:19:50.085476: step 7840, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 86h:44m:36s remains)
INFO - root - 2017-12-07 11:19:59.790249: step 7850, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.017 sec/batch; 91h:41m:43s remains)
INFO - root - 2017-12-07 11:20:09.626080: step 7860, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.004 sec/batch; 90h:30m:13s remains)
INFO - root - 2017-12-07 11:20:19.155199: step 7870, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 86h:07m:41s remains)
INFO - root - 2017-12-07 11:20:28.717048: step 7880, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 83h:41m:32s remains)
INFO - root - 2017-12-07 11:20:38.375215: step 7890, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.941 sec/batch; 84h:49m:09s remains)
INFO - root - 2017-12-07 11:20:48.165184: step 7900, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 87h:29m:13s remains)
2017-12-07 11:20:49.131540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3339972 -4.3317122 -4.3311319 -4.3313208 -4.3319736 -4.3324761 -4.3333573 -4.3346643 -4.3331385 -4.3284931 -4.3253131 -4.3259377 -4.3260913 -4.3215528 -4.31556][-4.3160539 -4.31056 -4.307075 -4.3047929 -4.30348 -4.3040037 -4.3079267 -4.313046 -4.3153419 -4.3139544 -4.3133121 -4.3131185 -4.3090758 -4.301888 -4.2953644][-4.296154 -4.2867374 -4.2795506 -4.2740221 -4.2717185 -4.2738614 -4.2815371 -4.2907243 -4.2970576 -4.3004317 -4.3023772 -4.2987542 -4.2878604 -4.2790818 -4.2715983][-4.2838149 -4.272809 -4.2635937 -4.2561097 -4.2532277 -4.25743 -4.268393 -4.2800374 -4.2894511 -4.2972908 -4.3029156 -4.29848 -4.2819295 -4.2685814 -4.2585483][-4.2671776 -4.2554126 -4.2445226 -4.2328925 -4.2253852 -4.2267385 -4.2366085 -4.2495742 -4.2661867 -4.2838535 -4.2959809 -4.2944951 -4.2777605 -4.26273 -4.2504649][-4.2355623 -4.2203655 -4.2036886 -4.1837115 -4.1678877 -4.1615992 -4.1668572 -4.1812253 -4.2110915 -4.24562 -4.2708859 -4.2817516 -4.2736573 -4.2608967 -4.2488279][-4.1961279 -4.1780243 -4.1567707 -4.1285605 -4.102262 -4.08218 -4.0733757 -4.0841894 -4.1286616 -4.1847963 -4.2257295 -4.2494154 -4.2526712 -4.25029 -4.2455754][-4.1986542 -4.1844978 -4.1665573 -4.1381388 -4.1040835 -4.0651674 -4.0293756 -4.0209064 -4.0696936 -4.13953 -4.1927986 -4.2237425 -4.2352581 -4.2445149 -4.2495246][-4.2494874 -4.2393379 -4.226594 -4.2061849 -4.1782465 -4.1402678 -4.0991349 -4.0764823 -4.10645 -4.1609817 -4.2065296 -4.2307658 -4.238524 -4.2491322 -4.2586951][-4.278594 -4.2695284 -4.259594 -4.246563 -4.2295847 -4.2046237 -4.1762762 -4.1547732 -4.1654139 -4.1968336 -4.2271433 -4.2417145 -4.2433619 -4.2521548 -4.2628188][-4.2742038 -4.26516 -4.2552156 -4.2464685 -4.2381687 -4.2248492 -4.2095561 -4.1950421 -4.1988769 -4.2161856 -4.2363858 -4.245707 -4.2432885 -4.2512846 -4.2621546][-4.2707033 -4.2587657 -4.2453732 -4.2371769 -4.2340784 -4.2282858 -4.2226362 -4.2165961 -4.2209458 -4.2313976 -4.2428379 -4.2476673 -4.2437477 -4.2511411 -4.2617006][-4.2790828 -4.2647 -4.2477136 -4.2357607 -4.2318721 -4.22885 -4.2283707 -4.2246728 -4.2276793 -4.2329731 -4.2351279 -4.2348142 -4.2324305 -4.244216 -4.2576151][-4.2944365 -4.276237 -4.2532697 -4.2341852 -4.2259064 -4.2225218 -4.2245846 -4.2213578 -4.221209 -4.2214546 -4.2170868 -4.2122874 -4.2147608 -4.2339792 -4.2510552][-4.3200932 -4.3007526 -4.2728162 -4.2452831 -4.2284102 -4.2216449 -4.2236738 -4.219327 -4.2174463 -4.2151546 -4.2071266 -4.1997585 -4.2050438 -4.2275443 -4.2456656]]...]
INFO - root - 2017-12-07 11:20:58.652412: step 7910, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 84h:25m:07s remains)
INFO - root - 2017-12-07 11:21:08.239510: step 7920, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 85h:01m:50s remains)
INFO - root - 2017-12-07 11:21:18.138458: step 7930, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.021 sec/batch; 92h:02m:52s remains)
INFO - root - 2017-12-07 11:21:27.839113: step 7940, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 82h:57m:06s remains)
INFO - root - 2017-12-07 11:21:37.394250: step 7950, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 83h:44m:14s remains)
INFO - root - 2017-12-07 11:21:47.078119: step 7960, loss = 2.10, batch loss = 2.04 (7.6 examples/sec; 1.052 sec/batch; 94h:48m:27s remains)
INFO - root - 2017-12-07 11:21:56.703835: step 7970, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 91h:30m:45s remains)
INFO - root - 2017-12-07 11:22:06.377399: step 7980, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 86h:49m:05s remains)
INFO - root - 2017-12-07 11:22:16.048196: step 7990, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 87h:15m:14s remains)
INFO - root - 2017-12-07 11:22:25.565337: step 8000, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 83h:45m:03s remains)
2017-12-07 11:22:26.581488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2238975 -4.2322168 -4.25097 -4.2561846 -4.2470541 -4.2386093 -4.2392049 -4.2406864 -4.25066 -4.2728496 -4.2869859 -4.2898426 -4.2876339 -4.2814579 -4.2692194][-4.2198954 -4.2258987 -4.2377973 -4.2340832 -4.2186718 -4.2074347 -4.2051682 -4.21027 -4.2287636 -4.2586021 -4.2824297 -4.2917485 -4.2901216 -4.28038 -4.2653913][-4.2168422 -4.2178941 -4.2197361 -4.2044396 -4.1781464 -4.1560707 -4.1477442 -4.1621332 -4.1966462 -4.2386327 -4.2720976 -4.2891512 -4.2921877 -4.2841539 -4.2702756][-4.2180233 -4.2123718 -4.2033839 -4.1765385 -4.136343 -4.0985918 -4.0851989 -4.1108327 -4.1634078 -4.2167315 -4.2559009 -4.2778277 -4.2859197 -4.2811179 -4.2712126][-4.2221785 -4.21247 -4.1950092 -4.1606226 -4.1081753 -4.0515208 -4.02833 -4.0628262 -4.1309605 -4.1935186 -4.2355871 -4.26121 -4.2745895 -4.2742429 -4.2675767][-4.225811 -4.2101703 -4.1834769 -4.1398568 -4.0747218 -3.9964564 -3.9588706 -4.0029659 -4.0869703 -4.1608191 -4.2090793 -4.2408409 -4.2607064 -4.2641339 -4.261095][-4.2278357 -4.2026467 -4.1617894 -4.1052675 -4.0285726 -3.9342566 -3.8822393 -3.9369304 -4.033864 -4.11569 -4.1723657 -4.2158351 -4.2460189 -4.25655 -4.25968][-4.2279849 -4.1977978 -4.1497254 -4.0889869 -4.0197673 -3.9403133 -3.8958855 -3.9410534 -4.0212731 -4.0894108 -4.1432571 -4.1943054 -4.2317719 -4.2492747 -4.2593112][-4.22355 -4.1924443 -4.1475229 -4.0961537 -4.0523443 -4.0122695 -3.9899788 -4.0132017 -4.0544682 -4.0929494 -4.1299853 -4.1711807 -4.2027097 -4.2232103 -4.24071][-4.2128992 -4.179111 -4.1358614 -4.0940986 -4.0755477 -4.0714259 -4.0689287 -4.0774956 -4.0923576 -4.11007 -4.1275649 -4.1493206 -4.1723118 -4.1937661 -4.2166667][-4.1998506 -4.1613564 -4.1162677 -4.0830946 -4.0855536 -4.1075778 -4.1183109 -4.1187429 -4.119391 -4.1251683 -4.1297836 -4.1392965 -4.1609178 -4.1842709 -4.2045922][-4.1961436 -4.1551476 -4.1107917 -4.0890503 -4.1088762 -4.1450772 -4.1592121 -4.1523538 -4.1432643 -4.1437497 -4.1417007 -4.1472154 -4.1679325 -4.1863322 -4.1989951][-4.20811 -4.16878 -4.1299419 -4.1197476 -4.1486411 -4.1841125 -4.1903982 -4.1755528 -4.1605039 -4.1585579 -4.1575904 -4.1655831 -4.180892 -4.1881132 -4.1921411][-4.2373376 -4.2018189 -4.1694269 -4.1656919 -4.1949072 -4.2194357 -4.2143912 -4.1914606 -4.1692576 -4.1629677 -4.1667886 -4.1843491 -4.1991224 -4.1967478 -4.19376][-4.2672362 -4.2381973 -4.21337 -4.2136812 -4.2384176 -4.2498584 -4.231986 -4.2014961 -4.1740441 -4.1649923 -4.1734357 -4.201992 -4.2203512 -4.2143922 -4.210072]]...]
INFO - root - 2017-12-07 11:22:36.064081: step 8010, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 80h:40m:26s remains)
INFO - root - 2017-12-07 11:22:45.697892: step 8020, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 84h:48m:30s remains)
INFO - root - 2017-12-07 11:22:55.293990: step 8030, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 90h:16m:46s remains)
INFO - root - 2017-12-07 11:23:05.160918: step 8040, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 87h:53m:09s remains)
INFO - root - 2017-12-07 11:23:14.842170: step 8050, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 86h:28m:57s remains)
INFO - root - 2017-12-07 11:23:24.453120: step 8060, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 87h:41m:49s remains)
INFO - root - 2017-12-07 11:23:33.969659: step 8070, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 88h:27m:45s remains)
INFO - root - 2017-12-07 11:23:43.624894: step 8080, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.995 sec/batch; 89h:38m:50s remains)
INFO - root - 2017-12-07 11:23:53.304139: step 8090, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 87h:00m:27s remains)
INFO - root - 2017-12-07 11:24:02.860514: step 8100, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 81h:25m:29s remains)
2017-12-07 11:24:03.803441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3166575 -4.3124938 -4.3066659 -4.298316 -4.286253 -4.2787504 -4.2728672 -4.2656946 -4.264349 -4.2695718 -4.2779784 -4.2871084 -4.2993417 -4.3118277 -4.3230052][-4.3202095 -4.3153396 -4.3053756 -4.2907982 -4.2692127 -4.2544336 -4.2437539 -4.2307668 -4.2239828 -4.2292209 -4.2406921 -4.2538075 -4.2726793 -4.2925277 -4.3134704][-4.3284106 -4.3191772 -4.30031 -4.2745042 -4.2412124 -4.2205429 -4.2071595 -4.19156 -4.1796584 -4.1832023 -4.1974225 -4.2159405 -4.2399879 -4.26549 -4.29627][-4.3334293 -4.321063 -4.2939811 -4.2569818 -4.2111306 -4.1833396 -4.1713581 -4.1553593 -4.138134 -4.138484 -4.1514397 -4.1749063 -4.2029486 -4.232513 -4.2718296][-4.3300214 -4.3142986 -4.2800422 -4.23163 -4.1685057 -4.1311245 -4.1187272 -4.1034722 -4.0856886 -4.0848212 -4.0950055 -4.1220245 -4.1573606 -4.1957507 -4.2431574][-4.3174338 -4.294013 -4.2536912 -4.2010221 -4.1294188 -4.0875459 -4.07348 -4.0550647 -4.0356731 -4.0285215 -4.0324674 -4.0614104 -4.1109028 -4.1637769 -4.2201695][-4.2982225 -4.2663221 -4.2204962 -4.16661 -4.1002555 -4.0634723 -4.0512643 -4.0260577 -3.9935341 -3.9696498 -3.9621861 -3.9928517 -4.0617027 -4.1343031 -4.203413][-4.27761 -4.2438955 -4.196866 -4.1433415 -4.0854721 -4.0518079 -4.0388393 -4.0070992 -3.9626722 -3.9256644 -3.909421 -3.9448628 -4.0292878 -4.1185079 -4.1967411][-4.2750025 -4.2466211 -4.20496 -4.1562543 -4.1058321 -4.0738907 -4.0588746 -4.0256858 -3.9811144 -3.9429722 -3.9276075 -3.9665585 -4.0488725 -4.136447 -4.2100639][-4.288394 -4.2749834 -4.2467456 -4.2103267 -4.1690812 -4.1398249 -4.1242189 -4.0904388 -4.04898 -4.0120306 -3.9991629 -4.0367484 -4.1070724 -4.1814837 -4.240097][-4.3133526 -4.313262 -4.2997665 -4.2753539 -4.2423115 -4.2155733 -4.198247 -4.1665549 -4.1335306 -4.1036668 -4.0918021 -4.12214 -4.179698 -4.2371383 -4.2769389][-4.3358121 -4.3426194 -4.33913 -4.322042 -4.2968626 -4.2730308 -4.2561979 -4.2312665 -4.2110338 -4.1938224 -4.1840291 -4.2069006 -4.2502394 -4.2883973 -4.3101225][-4.3448758 -4.3566694 -4.3604989 -4.350863 -4.3351803 -4.3171949 -4.3028116 -4.2843595 -4.2710562 -4.2614274 -4.2540345 -4.2700138 -4.2985239 -4.3199344 -4.3300257][-4.3365 -4.3511524 -4.359199 -4.3557582 -4.3479438 -4.3382864 -4.3290548 -4.3165936 -4.3078246 -4.3026834 -4.2975616 -4.3059583 -4.322103 -4.3336573 -4.3387551][-4.3233261 -4.336154 -4.3422475 -4.3416233 -4.3400879 -4.3374252 -4.3338084 -4.3285747 -4.3244567 -4.3234029 -4.3219471 -4.3255 -4.3339128 -4.3401718 -4.3432231]]...]
INFO - root - 2017-12-07 11:24:13.187528: step 8110, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 82h:34m:26s remains)
INFO - root - 2017-12-07 11:24:23.052278: step 8120, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 90h:46m:35s remains)
INFO - root - 2017-12-07 11:24:32.607570: step 8130, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 82h:01m:09s remains)
INFO - root - 2017-12-07 11:24:42.478442: step 8140, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.975 sec/batch; 87h:52m:21s remains)
INFO - root - 2017-12-07 11:24:52.096190: step 8150, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 89h:28m:28s remains)
INFO - root - 2017-12-07 11:25:01.659314: step 8160, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 85h:10m:24s remains)
INFO - root - 2017-12-07 11:25:11.362598: step 8170, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 88h:32m:50s remains)
INFO - root - 2017-12-07 11:25:21.081605: step 8180, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.021 sec/batch; 92h:00m:02s remains)
INFO - root - 2017-12-07 11:25:30.699647: step 8190, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 86h:50m:43s remains)
INFO - root - 2017-12-07 11:25:40.434038: step 8200, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 91h:23m:27s remains)
2017-12-07 11:25:41.350373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3359036 -4.3422551 -4.3327127 -4.3028364 -4.2644281 -4.2283058 -4.2098269 -4.229507 -4.254282 -4.2611585 -4.2636862 -4.2703776 -4.2871962 -4.3083959 -4.329329][-4.3346071 -4.337697 -4.3220334 -4.2835374 -4.2358804 -4.1976919 -4.1832628 -4.2101803 -4.2419028 -4.2566013 -4.2635674 -4.2710347 -4.28521 -4.3020687 -4.3175168][-4.3294454 -4.3274617 -4.3056555 -4.2609725 -4.2057581 -4.1640744 -4.1546178 -4.1945233 -4.2378564 -4.26223 -4.2739592 -4.2816634 -4.2914915 -4.3004866 -4.3068337][-4.3240581 -4.3189144 -4.2929134 -4.2445049 -4.1837888 -4.1384354 -4.1326895 -4.1821241 -4.2350383 -4.2672763 -4.283463 -4.2921534 -4.2982078 -4.3007226 -4.30058][-4.3229818 -4.3186059 -4.2932458 -4.2428679 -4.1765494 -4.125145 -4.1132488 -4.1591606 -4.2148213 -4.2561526 -4.2810473 -4.2947822 -4.3011761 -4.3024116 -4.301095][-4.3232732 -4.3207045 -4.2977028 -4.246201 -4.1757259 -4.1117435 -4.0767913 -4.1024065 -4.15564 -4.2082996 -4.2500172 -4.2781992 -4.2938223 -4.2997079 -4.30029][-4.3245254 -4.322722 -4.2989974 -4.2435331 -4.1652822 -4.0798297 -4.0080695 -4.0054388 -4.0547614 -4.1192 -4.1808429 -4.2296734 -4.2629771 -4.2816577 -4.2896819][-4.327662 -4.3275733 -4.3032589 -4.2409792 -4.1492248 -4.0372071 -3.9295077 -3.9080663 -3.9586849 -4.0321441 -4.1067634 -4.1709208 -4.2188478 -4.2488704 -4.2647381][-4.3272777 -4.3290758 -4.3065281 -4.2443218 -4.1524944 -4.0361805 -3.9225683 -3.9059358 -3.9628825 -4.0324035 -4.095891 -4.1510291 -4.1948147 -4.2219839 -4.2377234][-4.3213115 -4.3235779 -4.304903 -4.2506666 -4.1757216 -4.0804272 -3.9929268 -3.99681 -4.0569324 -4.1153321 -4.1566572 -4.1866779 -4.2081552 -4.21912 -4.2265229][-4.3163519 -4.3183966 -4.3046274 -4.264801 -4.2162771 -4.1517267 -4.0943284 -4.1099496 -4.1619306 -4.2053671 -4.2278595 -4.2361727 -4.238759 -4.2339926 -4.2321496][-4.3172984 -4.316679 -4.306706 -4.2816429 -4.2533126 -4.2145605 -4.1797423 -4.1957955 -4.2329731 -4.2595234 -4.26835 -4.2668829 -4.2623229 -4.2503786 -4.24381][-4.3232923 -4.3202147 -4.31142 -4.2932992 -4.2762117 -4.2539244 -4.2347136 -4.2483506 -4.2732477 -4.2861662 -4.285006 -4.2790442 -4.2717366 -4.2581868 -4.2515125][-4.3303256 -4.3259029 -4.3165517 -4.2971916 -4.2804561 -4.2657704 -4.2563233 -4.2713351 -4.2919073 -4.2978487 -4.292541 -4.2822857 -4.2725849 -4.2603745 -4.2566872][-4.3328848 -4.3281779 -4.3171372 -4.2934566 -4.2718554 -4.2546873 -4.2473617 -4.2658176 -4.2903452 -4.3001356 -4.2995086 -4.2901912 -4.2798038 -4.26917 -4.2665572]]...]
INFO - root - 2017-12-07 11:25:51.004569: step 8210, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.982 sec/batch; 88h:24m:53s remains)
INFO - root - 2017-12-07 11:26:00.654155: step 8220, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 86h:24m:13s remains)
INFO - root - 2017-12-07 11:26:10.221804: step 8230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 83h:10m:18s remains)
INFO - root - 2017-12-07 11:26:19.880411: step 8240, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 89h:50m:12s remains)
INFO - root - 2017-12-07 11:26:29.645654: step 8250, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 84h:57m:06s remains)
INFO - root - 2017-12-07 11:26:39.331387: step 8260, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 90h:04m:02s remains)
INFO - root - 2017-12-07 11:26:48.903547: step 8270, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 85h:28m:00s remains)
INFO - root - 2017-12-07 11:26:58.611313: step 8280, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 83h:46m:19s remains)
INFO - root - 2017-12-07 11:27:08.256144: step 8290, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 88h:56m:28s remains)
INFO - root - 2017-12-07 11:27:17.910766: step 8300, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 88h:16m:08s remains)
2017-12-07 11:27:18.963430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2113352 -4.2060742 -4.216053 -4.21741 -4.2079268 -4.2075458 -4.2200322 -4.2364526 -4.2490854 -4.2534313 -4.252912 -4.2459397 -4.2451067 -4.2410131 -4.2379823][-4.210865 -4.2014203 -4.212985 -4.2158813 -4.2039719 -4.196208 -4.206027 -4.2266345 -4.2395325 -4.24192 -4.24204 -4.2329473 -4.2245841 -4.2076707 -4.1944227][-4.200675 -4.1839256 -4.1948514 -4.2018237 -4.194221 -4.1858764 -4.19438 -4.2142625 -4.2287655 -4.2346764 -4.2410021 -4.233201 -4.2161336 -4.1814833 -4.1550832][-4.1931124 -4.1680355 -4.1715784 -4.1758223 -4.1707592 -4.16558 -4.1759439 -4.1948915 -4.2113705 -4.2233725 -4.2355323 -4.2312431 -4.2084055 -4.1609735 -4.1248288][-4.1966639 -4.16323 -4.1499419 -4.140759 -4.1304154 -4.1238 -4.1333 -4.1490421 -4.1690769 -4.1915007 -4.21213 -4.215765 -4.1945934 -4.145607 -4.1098285][-4.2096767 -4.1667385 -4.1312828 -4.1007156 -4.0794024 -4.0667815 -4.0678205 -4.0736728 -4.0925765 -4.1299477 -4.1634307 -4.1778169 -4.1667418 -4.1306224 -4.1057453][-4.2229028 -4.1776457 -4.12798 -4.0811744 -4.0478406 -4.0252848 -4.0092068 -3.9968226 -4.0078435 -4.0610123 -4.1147079 -4.1403875 -4.1425176 -4.1267886 -4.1211219][-4.233634 -4.1976161 -4.1510057 -4.1038537 -4.0674262 -4.0409431 -4.0112081 -3.9813476 -3.9820924 -4.0404024 -4.10238 -4.1281309 -4.1350574 -4.1309128 -4.1432037][-4.2398233 -4.2191081 -4.19104 -4.1602173 -4.1298451 -4.1044989 -4.0718107 -4.0324912 -4.0201163 -4.0718465 -4.1230011 -4.1381016 -4.14052 -4.1359677 -4.1535749][-4.2427425 -4.2308526 -4.2216415 -4.2077227 -4.1865044 -4.1650076 -4.1344414 -4.0916514 -4.0666203 -4.102253 -4.1397772 -4.1461005 -4.1430306 -4.1359954 -4.1473246][-4.2507372 -4.2387309 -4.2354007 -4.2300835 -4.2153807 -4.1942563 -4.1643662 -4.12516 -4.0970182 -4.1170354 -4.1416821 -4.1457896 -4.1437521 -4.1322746 -4.1312656][-4.2534719 -4.2369995 -4.2362247 -4.2374883 -4.2257562 -4.2042837 -4.1760263 -4.1432834 -4.1226139 -4.1335907 -4.1471267 -4.1483116 -4.1485505 -4.1315308 -4.1224127][-4.2419133 -4.2238374 -4.2288332 -4.2391834 -4.232532 -4.2111077 -4.182323 -4.1561961 -4.1475663 -4.1547289 -4.1601491 -4.1592221 -4.1619954 -4.1414318 -4.1265564][-4.2279506 -4.209897 -4.21813 -4.235528 -4.2367644 -4.2162442 -4.1854539 -4.1640115 -4.1668611 -4.1793108 -4.181129 -4.1795034 -4.18157 -4.1647043 -4.1483197][-4.223805 -4.2042427 -4.2095685 -4.2269478 -4.2331018 -4.2151623 -4.1852517 -4.1721272 -4.1873884 -4.2094574 -4.2170997 -4.2136774 -4.2091966 -4.1898527 -4.1671519]]...]
INFO - root - 2017-12-07 11:27:28.624354: step 8310, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 85h:28m:39s remains)
INFO - root - 2017-12-07 11:27:38.264035: step 8320, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 87h:19m:28s remains)
INFO - root - 2017-12-07 11:27:47.876293: step 8330, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 86h:35m:13s remains)
INFO - root - 2017-12-07 11:27:57.295705: step 8340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 82h:21m:43s remains)
INFO - root - 2017-12-07 11:28:07.107272: step 8350, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.977 sec/batch; 87h:57m:04s remains)
INFO - root - 2017-12-07 11:28:16.831379: step 8360, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 91h:13m:24s remains)
INFO - root - 2017-12-07 11:28:26.423429: step 8370, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 84h:53m:59s remains)
INFO - root - 2017-12-07 11:28:36.064033: step 8380, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 85h:38m:34s remains)
INFO - root - 2017-12-07 11:28:45.894330: step 8390, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 91h:12m:11s remains)
INFO - root - 2017-12-07 11:28:55.466077: step 8400, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 83h:11m:17s remains)
2017-12-07 11:28:56.378512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3660026 -4.3808784 -4.3853741 -4.3569202 -4.2914371 -4.2066622 -4.1400037 -4.1154284 -4.1403027 -4.1903763 -4.2427273 -4.2811127 -4.3017154 -4.3045459 -4.2972097][-4.3750324 -4.3915238 -4.4004173 -4.3806372 -4.3295631 -4.2555652 -4.1861324 -4.1430016 -4.1404519 -4.1671872 -4.209578 -4.2518668 -4.2852163 -4.3017297 -4.3045049][-4.3842845 -4.4011207 -4.4105625 -4.3947992 -4.352509 -4.2850418 -4.2099757 -4.151042 -4.1257133 -4.13314 -4.1715703 -4.2212605 -4.2694678 -4.3040233 -4.3209867][-4.3896289 -4.4055533 -4.4138885 -4.4022489 -4.3644466 -4.3002219 -4.2214966 -4.1502738 -4.1051965 -4.0955129 -4.1335707 -4.1895823 -4.2515106 -4.3033071 -4.3358665][-4.3918796 -4.4069695 -4.4141612 -4.405592 -4.3698549 -4.3080773 -4.2276516 -4.1442842 -4.075645 -4.0457058 -4.0841575 -4.14811 -4.2222228 -4.2888703 -4.3362217][-4.3934479 -4.4065104 -4.4088068 -4.3953934 -4.355773 -4.2908025 -4.2070656 -4.10928 -4.01657 -3.9702048 -4.0168004 -4.0951772 -4.1822462 -4.2618837 -4.3216085][-4.3930707 -4.40359 -4.3989792 -4.3742509 -4.3227029 -4.24814 -4.1572037 -4.0452118 -3.9305966 -3.8770671 -3.9433975 -4.0403576 -4.1391463 -4.2296681 -4.301405][-4.390645 -4.397068 -4.3838582 -4.3460088 -4.2816763 -4.2005229 -4.1101589 -3.997947 -3.8802514 -3.8331285 -3.9089088 -4.011138 -4.1104827 -4.2025809 -4.281301][-4.38551 -4.3862915 -4.365519 -4.3178506 -4.2470255 -4.16911 -4.0920444 -4.0011029 -3.9096091 -3.8821454 -3.9446614 -4.027462 -4.1119475 -4.1955276 -4.272851][-4.3795762 -4.3747211 -4.3497295 -4.2988753 -4.2287393 -4.1587772 -4.0997157 -4.0364494 -3.9780674 -3.9686391 -4.0125046 -4.0710945 -4.1378722 -4.2109928 -4.2807531][-4.3737588 -4.3654318 -4.3410339 -4.2940168 -4.2323818 -4.1739011 -4.1318293 -4.0959282 -4.0688581 -4.0716081 -4.0995679 -4.1385674 -4.1908741 -4.2521448 -4.3070817][-4.3706136 -4.3629718 -4.344563 -4.3084807 -4.2616735 -4.2181168 -4.1906586 -4.1761365 -4.1697254 -4.1764994 -4.1914687 -4.2173944 -4.2560592 -4.3009872 -4.3362494][-4.3700337 -4.3662043 -4.3565955 -4.3348579 -4.3046718 -4.2763581 -4.2609124 -4.25741 -4.2597756 -4.2654481 -4.2738423 -4.2914853 -4.3159447 -4.3416319 -4.3566189][-4.36843 -4.3681498 -4.3657293 -4.3566165 -4.3416719 -4.3266315 -4.3187718 -4.3193455 -4.3240809 -4.3291869 -4.3352909 -4.3468614 -4.3590884 -4.3678341 -4.3666296][-4.3648911 -4.3661385 -4.3672991 -4.3659239 -4.3613672 -4.3551655 -4.352 -4.3540535 -4.3588548 -4.3632984 -4.36771 -4.373529 -4.3760929 -4.3724995 -4.3605108]]...]
INFO - root - 2017-12-07 11:29:05.935138: step 8410, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 83h:58m:01s remains)
INFO - root - 2017-12-07 11:29:15.595896: step 8420, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.002 sec/batch; 90h:11m:34s remains)
INFO - root - 2017-12-07 11:29:25.507270: step 8430, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 90h:23m:02s remains)
INFO - root - 2017-12-07 11:29:35.096342: step 8440, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 83h:32m:47s remains)
INFO - root - 2017-12-07 11:29:44.704031: step 8450, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.991 sec/batch; 89h:13m:53s remains)
INFO - root - 2017-12-07 11:29:54.461089: step 8460, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 86h:45m:55s remains)
INFO - root - 2017-12-07 11:30:04.285336: step 8470, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 86h:19m:25s remains)
INFO - root - 2017-12-07 11:30:13.792872: step 8480, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 83h:29m:41s remains)
INFO - root - 2017-12-07 11:30:23.322181: step 8490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 86h:47m:19s remains)
INFO - root - 2017-12-07 11:30:32.829178: step 8500, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 85h:43m:17s remains)
2017-12-07 11:30:33.723825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33718 -4.3364692 -4.3320985 -4.3289323 -4.3280754 -4.328896 -4.3296161 -4.329402 -4.3286724 -4.3293967 -4.3316374 -4.3332734 -4.3342385 -4.3355327 -4.3353596][-4.3392282 -4.338326 -4.3330693 -4.3291221 -4.3280592 -4.3283396 -4.32805 -4.3269806 -4.3257432 -4.3255486 -4.3269978 -4.3287797 -4.3307142 -4.3334436 -4.3347583][-4.3309646 -4.3283706 -4.3211632 -4.3161578 -4.3152237 -4.3157535 -4.3153667 -4.3152623 -4.315486 -4.3164105 -4.3189073 -4.3221383 -4.3259625 -4.3304243 -4.332962][-4.3200154 -4.3128366 -4.299911 -4.2897434 -4.2852497 -4.2831116 -4.2809486 -4.2805915 -4.2814622 -4.2843952 -4.2907095 -4.2988281 -4.3069792 -4.3139591 -4.3171258][-4.3015308 -4.28665 -4.2652936 -4.2469954 -4.2363105 -4.2303867 -4.2258329 -4.2233028 -4.2230258 -4.2280931 -4.2407737 -4.256845 -4.2697806 -4.2780352 -4.2801671][-4.2782192 -4.2570214 -4.2262716 -4.195869 -4.172946 -4.1578817 -4.1458569 -4.1364431 -4.1327863 -4.1404133 -4.1632237 -4.1915684 -4.2124405 -4.2229848 -4.2242417][-4.273313 -4.2529478 -4.2143931 -4.1695228 -4.131639 -4.1035805 -4.0778003 -4.0560565 -4.0447865 -4.0514212 -4.08058 -4.1186948 -4.1454825 -4.157021 -4.1578183][-4.2664504 -4.2544804 -4.2187867 -4.1745176 -4.1374903 -4.1110578 -4.083714 -4.0597982 -4.0447741 -4.0456028 -4.0681715 -4.1013203 -4.1235366 -4.1302891 -4.1278186][-4.2419772 -4.2378807 -4.2114234 -4.1788721 -4.1558647 -4.1448579 -4.1337185 -4.1252451 -4.1203246 -4.12177 -4.1360979 -4.1591282 -4.1744986 -4.179121 -4.178494][-4.2542849 -4.2537785 -4.2354121 -4.2136946 -4.200388 -4.1971116 -4.1973367 -4.2017388 -4.2061234 -4.2104321 -4.2189708 -4.2322564 -4.241364 -4.2446737 -4.2451949][-4.2893963 -4.2918291 -4.2819891 -4.2698884 -4.261879 -4.2594938 -4.261878 -4.2691503 -4.2762213 -4.2823105 -4.2882252 -4.2952418 -4.2997613 -4.3015804 -4.3023515][-4.3206139 -4.3228173 -4.3183846 -4.3129973 -4.3082042 -4.3051524 -4.3056335 -4.3103266 -4.3161893 -4.321867 -4.3255153 -4.3285179 -4.3307791 -4.3323283 -4.3332391][-4.3391147 -4.3400979 -4.3377891 -4.3345585 -4.33038 -4.3264093 -4.3242035 -4.3251042 -4.3282008 -4.331687 -4.3332577 -4.3343568 -4.3357191 -4.3364778 -4.3369541][-4.3387938 -4.3384728 -4.3362989 -4.3345413 -4.3317575 -4.3277044 -4.323987 -4.3223176 -4.3231759 -4.3240261 -4.3233762 -4.3227234 -4.3223228 -4.3216734 -4.3221579][-4.325789 -4.3244209 -4.3219028 -4.3221226 -4.3225188 -4.3211713 -4.3191442 -4.318007 -4.318161 -4.3175941 -4.3156466 -4.3134403 -4.3110042 -4.3090143 -4.3092413]]...]
INFO - root - 2017-12-07 11:30:43.356072: step 8510, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 84h:23m:26s remains)
INFO - root - 2017-12-07 11:30:53.017385: step 8520, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 87h:48m:36s remains)
INFO - root - 2017-12-07 11:31:02.503427: step 8530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 87h:12m:21s remains)
INFO - root - 2017-12-07 11:31:12.136311: step 8540, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.011 sec/batch; 90h:57m:50s remains)
INFO - root - 2017-12-07 11:31:21.801423: step 8550, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 86h:00m:59s remains)
INFO - root - 2017-12-07 11:31:31.454922: step 8560, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 83h:35m:16s remains)
INFO - root - 2017-12-07 11:31:40.996743: step 8570, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 81h:46m:43s remains)
INFO - root - 2017-12-07 11:31:50.499186: step 8580, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 80h:30m:41s remains)
INFO - root - 2017-12-07 11:32:00.264069: step 8590, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.026 sec/batch; 92h:20m:36s remains)
INFO - root - 2017-12-07 11:32:09.957014: step 8600, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 82h:40m:07s remains)
2017-12-07 11:32:10.836358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2837858 -4.2818284 -4.2810483 -4.2807689 -4.2797575 -4.2772093 -4.2740145 -4.2720432 -4.2717113 -4.2727723 -4.2744136 -4.2747226 -4.2722211 -4.2657475 -4.252749][-4.2926178 -4.2917495 -4.2918873 -4.2934604 -4.2936754 -4.2897077 -4.2831893 -4.278254 -4.2771873 -4.2795458 -4.2829413 -4.2844234 -4.2813644 -4.27245 -4.2563562][-4.3036752 -4.3019161 -4.3036065 -4.3086219 -4.3113523 -4.3059545 -4.2955685 -4.2876854 -4.2869334 -4.2919626 -4.2982163 -4.3013511 -4.2974038 -4.2872629 -4.2689848][-4.2988739 -4.2961178 -4.2997651 -4.3083138 -4.3136005 -4.3064528 -4.2914729 -4.2818131 -4.2865081 -4.2978 -4.3064232 -4.3096557 -4.3047466 -4.2941561 -4.2776933][-4.280128 -4.2730436 -4.27556 -4.2841415 -4.2855406 -4.2704697 -4.2475142 -4.2367406 -4.2517257 -4.2754726 -4.290514 -4.2961464 -4.292069 -4.282197 -4.2705932][-4.2371583 -4.2316403 -4.2387476 -4.2492328 -4.2438545 -4.2138386 -4.1701407 -4.1440492 -4.1681404 -4.2122555 -4.2423553 -4.2572761 -4.2602148 -4.2544308 -4.2497768][-4.1798668 -4.1762066 -4.1900773 -4.2082515 -4.20065 -4.1534567 -4.0761108 -4.0183449 -4.0454597 -4.113833 -4.1624346 -4.1913943 -4.2052312 -4.2061257 -4.2100353][-4.15121 -4.1447005 -4.1550827 -4.1718631 -4.16096 -4.1027942 -4.0063529 -3.9294574 -3.9584489 -4.0397673 -4.0987706 -4.1348734 -4.15307 -4.1589856 -4.1685662][-4.1487556 -4.1388187 -4.1445169 -4.1603494 -4.1531034 -4.1087828 -4.0407424 -3.9915469 -4.0162711 -4.0753546 -4.1200356 -4.1474681 -4.15852 -4.1577649 -4.1621552][-4.15181 -4.1434155 -4.14568 -4.1611323 -4.1599445 -4.1365428 -4.1067948 -4.0901213 -4.1086073 -4.1423354 -4.1696467 -4.1884384 -4.1908789 -4.1801448 -4.1735883][-4.1478243 -4.1441112 -4.1445131 -4.15784 -4.1595554 -4.1485686 -4.1426115 -4.1429849 -4.153697 -4.172811 -4.1943283 -4.2132316 -4.217123 -4.2033291 -4.1880875][-4.1494136 -4.1481152 -4.1496172 -4.1600943 -4.1646242 -4.1638079 -4.1695595 -4.1769328 -4.1799784 -4.1916628 -4.211863 -4.2300344 -4.2338204 -4.2199345 -4.2036548][-4.1724358 -4.1702929 -4.1745 -4.1845121 -4.192121 -4.1966066 -4.203548 -4.2093754 -4.2080312 -4.2137675 -4.2296491 -4.2418523 -4.2437725 -4.2346969 -4.2254004][-4.2089357 -4.204812 -4.2096443 -4.2158561 -4.2223439 -4.2262192 -4.2307234 -4.2332106 -4.2311373 -4.2338963 -4.2423868 -4.2488384 -4.2499313 -4.2452812 -4.2417345][-4.2487411 -4.2433043 -4.2434015 -4.2454066 -4.2494497 -4.2502027 -4.2506475 -4.2488909 -4.244504 -4.24332 -4.2453866 -4.2472463 -4.2467256 -4.2440972 -4.2419486]]...]
INFO - root - 2017-12-07 11:32:20.427357: step 8610, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 87h:04m:38s remains)
INFO - root - 2017-12-07 11:32:30.094025: step 8620, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 86h:31m:11s remains)
INFO - root - 2017-12-07 11:32:39.700707: step 8630, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 87h:37m:51s remains)
INFO - root - 2017-12-07 11:32:49.276283: step 8640, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 86h:53m:29s remains)
INFO - root - 2017-12-07 11:32:58.923312: step 8650, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 88h:38m:03s remains)
INFO - root - 2017-12-07 11:33:08.531206: step 8660, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 85h:48m:19s remains)
INFO - root - 2017-12-07 11:33:18.092432: step 8670, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 78h:25m:40s remains)
INFO - root - 2017-12-07 11:33:27.681723: step 8680, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 82h:16m:22s remains)
INFO - root - 2017-12-07 11:33:37.387389: step 8690, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 86h:39m:41s remains)
INFO - root - 2017-12-07 11:33:47.001936: step 8700, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.012 sec/batch; 91h:00m:27s remains)
2017-12-07 11:33:47.963786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275178 -4.2745581 -4.2712178 -4.268405 -4.2640443 -4.2609639 -4.2613673 -4.2629957 -4.2664952 -4.2702842 -4.27155 -4.2733259 -4.2748885 -4.2754436 -4.276526][-4.2603555 -4.2570076 -4.2500677 -4.2427888 -4.2366767 -4.2341461 -4.2368326 -4.2415524 -4.247808 -4.252717 -4.2536912 -4.2568521 -4.2620268 -4.2670188 -4.2720785][-4.2439 -4.2347579 -4.2205563 -4.2053981 -4.195837 -4.1942129 -4.198802 -4.2084231 -4.2195206 -4.2274013 -4.2291684 -4.2352548 -4.2468252 -4.25768 -4.2658958][-4.2310524 -4.2148638 -4.1922526 -4.1661243 -4.1488733 -4.1405377 -4.1399579 -4.1523356 -4.1701803 -4.1871953 -4.1979456 -4.2125721 -4.2340255 -4.2520008 -4.2627239][-4.2180462 -4.1919551 -4.1600251 -4.1206479 -4.0882816 -4.064353 -4.0541258 -4.0701003 -4.0960817 -4.1236105 -4.1499343 -4.1761122 -4.2089357 -4.2367697 -4.2538018][-4.2065864 -4.1673059 -4.1224952 -4.069828 -4.0178 -3.970685 -3.9487157 -3.9723966 -4.0108557 -4.0490351 -4.0905757 -4.1293278 -4.170352 -4.2042823 -4.2303944][-4.1955853 -4.1454444 -4.08911 -4.0270033 -3.958097 -3.882139 -3.841933 -3.8744419 -3.9362226 -3.99381 -4.0469136 -4.0891008 -4.1262841 -4.161551 -4.1960249][-4.1936784 -4.1392589 -4.0823917 -4.0272536 -3.962687 -3.8861661 -3.8406639 -3.8685441 -3.930203 -3.9886599 -4.0316253 -4.0617933 -4.0906725 -4.12711 -4.1688571][-4.20014 -4.1525674 -4.1090269 -4.0712552 -4.028451 -3.9764409 -3.9484286 -3.9657381 -3.998131 -4.0311003 -4.0558224 -4.0690842 -4.0864153 -4.1148758 -4.1542144][-4.2173743 -4.1885357 -4.162715 -4.136879 -4.1069608 -4.0724034 -4.0630784 -4.0765009 -4.0844736 -4.090189 -4.0925741 -4.0865273 -4.0925784 -4.112339 -4.1471882][-4.2440691 -4.2274485 -4.2095718 -4.1863713 -4.1610765 -4.1353817 -4.1335158 -4.1443276 -4.1441965 -4.1320148 -4.1122818 -4.0926723 -4.0974178 -4.1126165 -4.1449213][-4.2727237 -4.2600265 -4.2420077 -4.2143865 -4.1870775 -4.1647396 -4.1602812 -4.1655312 -4.167541 -4.1559768 -4.1310759 -4.1089549 -4.1169853 -4.13342 -4.1591053][-4.2876596 -4.2748637 -4.2515612 -4.2179422 -4.1880112 -4.1662989 -4.1569195 -4.1607881 -4.1730337 -4.1733704 -4.1567883 -4.1395845 -4.1428251 -4.1527972 -4.1669121][-4.2768497 -4.2624984 -4.2367978 -4.2057891 -4.1796503 -4.1596103 -4.1435208 -4.1383367 -4.1503606 -4.1587658 -4.1523166 -4.1407619 -4.1418252 -4.1463962 -4.1519465][-4.250494 -4.2350435 -4.2109318 -4.1881528 -4.170372 -4.1543083 -4.1372304 -4.1229634 -4.120677 -4.1255779 -4.128727 -4.1263409 -4.1341853 -4.1418247 -4.1372242]]...]
INFO - root - 2017-12-07 11:33:57.653158: step 8710, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 89h:54m:59s remains)
INFO - root - 2017-12-07 11:34:07.354313: step 8720, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 89h:41m:42s remains)
INFO - root - 2017-12-07 11:34:17.049053: step 8730, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:54m:42s remains)
INFO - root - 2017-12-07 11:34:26.668698: step 8740, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 81h:35m:48s remains)
INFO - root - 2017-12-07 11:34:36.295356: step 8750, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.969 sec/batch; 87h:07m:04s remains)
INFO - root - 2017-12-07 11:34:46.025380: step 8760, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 90h:22m:54s remains)
INFO - root - 2017-12-07 11:34:55.692338: step 8770, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 87h:16m:05s remains)
INFO - root - 2017-12-07 11:35:05.148505: step 8780, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.926 sec/batch; 83h:15m:39s remains)
INFO - root - 2017-12-07 11:35:14.818690: step 8790, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 81h:04m:37s remains)
INFO - root - 2017-12-07 11:35:24.476066: step 8800, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 89h:25m:10s remains)
2017-12-07 11:35:25.407468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1951208 -4.2123752 -4.2220082 -4.2216635 -4.2204924 -4.2132263 -4.2053304 -4.2015505 -4.2086897 -4.2310667 -4.2543569 -4.2815108 -4.3034654 -4.3190241 -4.3245592][-4.1771712 -4.2054005 -4.2248158 -4.2334943 -4.2397637 -4.2365603 -4.2268143 -4.2203393 -4.2221317 -4.236043 -4.2550907 -4.2753739 -4.2932835 -4.3082185 -4.3153448][-4.1788626 -4.216433 -4.2400885 -4.2520881 -4.2561827 -4.2502022 -4.2384706 -4.2260485 -4.21726 -4.2213974 -4.23745 -4.2535696 -4.2695808 -4.2877946 -4.2988348][-4.2001891 -4.2394323 -4.2627959 -4.2718215 -4.2655783 -4.2477756 -4.2254772 -4.198709 -4.1791945 -4.1823273 -4.2045984 -4.2259688 -4.24911 -4.274025 -4.2879281][-4.234396 -4.273447 -4.291398 -4.2888322 -4.2665396 -4.2308893 -4.1852589 -4.1310277 -4.09927 -4.1157284 -4.1631575 -4.20405 -4.2398176 -4.2694626 -4.2813148][-4.26484 -4.3026714 -4.3152294 -4.3011756 -4.2666011 -4.2118068 -4.1332693 -4.0340524 -3.9816163 -4.0263124 -4.1195216 -4.1920719 -4.2409997 -4.2698441 -4.2752528][-4.28111 -4.3118186 -4.3205857 -4.3027625 -4.2642136 -4.1951504 -4.0817156 -3.9321666 -3.8612332 -3.9494076 -4.0900478 -4.1903234 -4.2467108 -4.271801 -4.271287][-4.2884374 -4.3081522 -4.3141232 -4.298811 -4.2641916 -4.1919193 -4.060113 -3.8906307 -3.8201752 -3.9425077 -4.0995245 -4.2020712 -4.254169 -4.2725687 -4.26792][-4.2839575 -4.2936459 -4.3039279 -4.2988605 -4.2743764 -4.2098522 -4.0882416 -3.946131 -3.9017763 -4.009584 -4.13626 -4.2166805 -4.2561679 -4.2658119 -4.2607813][-4.2731285 -4.2735581 -4.2881608 -4.2977552 -4.2900019 -4.2451468 -4.1528764 -4.054255 -4.0300913 -4.0973511 -4.17766 -4.2313437 -4.2570024 -4.2593141 -4.256165][-4.2575412 -4.2510328 -4.270885 -4.2980633 -4.310781 -4.2909117 -4.2297096 -4.164638 -4.1402807 -4.1665754 -4.2108006 -4.2464705 -4.2627387 -4.2594466 -4.2576051][-4.2449007 -4.2375388 -4.2618542 -4.3023458 -4.329958 -4.3275247 -4.2897115 -4.2415447 -4.2104344 -4.2111859 -4.2362256 -4.2619181 -4.271193 -4.2661595 -4.2643447][-4.2411423 -4.24021 -4.2653327 -4.307004 -4.3413987 -4.3466792 -4.3231959 -4.2841535 -4.2503028 -4.2402487 -4.2544751 -4.2728949 -4.2782903 -4.2753263 -4.2735157][-4.2479105 -4.2533164 -4.276772 -4.3136811 -4.3445015 -4.3524365 -4.3381267 -4.3074374 -4.2721534 -4.2546206 -4.2596006 -4.2719536 -4.2749453 -4.2700253 -4.2690687][-4.2691011 -4.2763243 -4.296279 -4.3237877 -4.3449903 -4.3511677 -4.3393641 -4.311079 -4.2726908 -4.2470751 -4.2452993 -4.2563925 -4.2606616 -4.2521067 -4.2475991]]...]
INFO - root - 2017-12-07 11:35:34.972689: step 8810, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 83h:13m:02s remains)
INFO - root - 2017-12-07 11:35:44.479538: step 8820, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 85h:20m:08s remains)
INFO - root - 2017-12-07 11:35:54.293016: step 8830, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 91h:13m:49s remains)
INFO - root - 2017-12-07 11:36:03.867328: step 8840, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 90h:47m:28s remains)
INFO - root - 2017-12-07 11:36:13.550541: step 8850, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.948 sec/batch; 85h:15m:14s remains)
INFO - root - 2017-12-07 11:36:23.218555: step 8860, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:47m:32s remains)
INFO - root - 2017-12-07 11:36:32.832400: step 8870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 86h:15m:13s remains)
INFO - root - 2017-12-07 11:36:42.581390: step 8880, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 84h:54m:37s remains)
INFO - root - 2017-12-07 11:36:51.876852: step 8890, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 81h:57m:05s remains)
INFO - root - 2017-12-07 11:37:01.436380: step 8900, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 86h:47m:57s remains)
2017-12-07 11:37:02.446154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1800942 -4.1811686 -4.1926847 -4.2121992 -4.2257891 -4.2280021 -4.2349873 -4.24255 -4.247149 -4.2542582 -4.2531815 -4.2324805 -4.1993995 -4.168324 -4.1535263][-4.1197157 -4.1383028 -4.1689239 -4.206944 -4.2357569 -4.246973 -4.2497511 -4.2460833 -4.2427831 -4.2438173 -4.2352405 -4.2016611 -4.1512213 -4.1087737 -4.091259][-4.0408659 -4.0852795 -4.1449137 -4.2038846 -4.2398186 -4.2460756 -4.2324991 -4.215517 -4.2038145 -4.2003541 -4.1904006 -4.1537333 -4.0991163 -4.0598788 -4.0475049][-4.0169892 -4.0871663 -4.1652303 -4.222 -4.2430992 -4.2261095 -4.1874194 -4.1523833 -4.1329746 -4.1329212 -4.1357965 -4.1161628 -4.082921 -4.0667295 -4.0688577][-4.0978222 -4.16623 -4.2232513 -4.2474422 -4.23933 -4.1917806 -4.1195936 -4.0631685 -4.0478163 -4.0685573 -4.0966687 -4.109901 -4.1114693 -4.1152825 -4.1251011][-4.18237 -4.2268481 -4.2515869 -4.2416444 -4.2008939 -4.1247959 -4.0165629 -3.9441726 -3.9620256 -4.02479 -4.0843043 -4.1269608 -4.1527367 -4.1635547 -4.1651545][-4.2301054 -4.249217 -4.2399583 -4.1926918 -4.1163907 -4.0118628 -3.8746004 -3.7991867 -3.8771904 -3.9900079 -4.0751505 -4.1381183 -4.17614 -4.1811528 -4.1636009][-4.2518325 -4.2495093 -4.2137518 -4.1407862 -4.044528 -3.9368622 -3.8201091 -3.7807615 -3.8931046 -4.0152488 -4.0937347 -4.1543469 -4.1918674 -4.1870141 -4.1561341][-4.2536349 -4.2379117 -4.1912785 -4.1148806 -4.0325193 -3.9566524 -3.89855 -3.9003043 -3.9917789 -4.0834484 -4.1362939 -4.1801419 -4.2099223 -4.1996531 -4.170301][-4.2401433 -4.2203941 -4.1700082 -4.1018314 -4.0408249 -4.0002241 -3.985445 -4.0074687 -4.071413 -4.13128 -4.1651139 -4.1931915 -4.2133837 -4.2037225 -4.1833272][-4.2365108 -4.2115941 -4.164206 -4.1130419 -4.0757542 -4.0604053 -4.0692091 -4.0980673 -4.1411777 -4.1756458 -4.1954937 -4.2108946 -4.2196259 -4.2132463 -4.2017527][-4.2463627 -4.2243643 -4.1931381 -4.1678557 -4.1517406 -4.1489854 -4.1632605 -4.1842303 -4.2048745 -4.22084 -4.2323966 -4.2408843 -4.2446532 -4.2436576 -4.2397423][-4.2751417 -4.2585917 -4.2408462 -4.2333426 -4.2287383 -4.2298841 -4.242496 -4.2562404 -4.2634211 -4.2690053 -4.2766213 -4.2824721 -4.28636 -4.2883983 -4.2874532][-4.3007321 -4.290565 -4.2831469 -4.284718 -4.2857008 -4.2875662 -4.2980194 -4.3076138 -4.3100538 -4.3111973 -4.3155065 -4.3191719 -4.3215556 -4.3225183 -4.3215475][-4.31349 -4.3099694 -4.3087096 -4.3118362 -4.3129768 -4.3133669 -4.31801 -4.3225336 -4.3213706 -4.319726 -4.3212209 -4.3233838 -4.3245535 -4.3256321 -4.3253231]]...]
INFO - root - 2017-12-07 11:37:12.044628: step 8910, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 87h:46m:15s remains)
INFO - root - 2017-12-07 11:37:21.780533: step 8920, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 86h:33m:15s remains)
INFO - root - 2017-12-07 11:37:31.466414: step 8930, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 84h:55m:10s remains)
INFO - root - 2017-12-07 11:37:41.319615: step 8940, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.019 sec/batch; 91h:35m:14s remains)
INFO - root - 2017-12-07 11:37:50.901971: step 8950, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 87h:55m:54s remains)
INFO - root - 2017-12-07 11:38:00.576246: step 8960, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 86h:38m:26s remains)
INFO - root - 2017-12-07 11:38:10.313903: step 8970, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 85h:42m:17s remains)
INFO - root - 2017-12-07 11:38:19.827288: step 8980, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 83h:54m:46s remains)
INFO - root - 2017-12-07 11:38:29.477484: step 8990, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 88h:37m:11s remains)
INFO - root - 2017-12-07 11:38:39.176198: step 9000, loss = 2.09, batch loss = 2.04 (7.9 examples/sec; 1.009 sec/batch; 90h:42m:31s remains)
2017-12-07 11:38:40.107851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1888952 -4.1747656 -4.1606216 -4.1535048 -4.1492891 -4.1542392 -4.1559916 -4.1521864 -4.1556544 -4.1617928 -4.1643744 -4.1656942 -4.1615949 -4.1451206 -4.1260381][-4.166533 -4.1485095 -4.133112 -4.1270823 -4.1237593 -4.1265125 -4.1258855 -4.1224113 -4.1325 -4.1493168 -4.1606879 -4.1664071 -4.1631255 -4.144865 -4.1200123][-4.14492 -4.12702 -4.1131372 -4.1111512 -4.1105337 -4.1131077 -4.11322 -4.1118546 -4.1272817 -4.1523037 -4.1696281 -4.1757059 -4.1712108 -4.1526923 -4.1283083][-4.1217117 -4.1080017 -4.0948658 -4.0897202 -4.0864644 -4.0852365 -4.0831866 -4.08559 -4.1107492 -4.14611 -4.1720042 -4.1820483 -4.1795359 -4.1653476 -4.1480451][-4.0882483 -4.077919 -4.0586715 -4.0413642 -4.0278091 -4.0161471 -4.007442 -4.01442 -4.0554614 -4.1074667 -4.1481628 -4.1687679 -4.1730003 -4.1661987 -4.15774][-4.0661578 -4.0517459 -4.0236053 -3.9951429 -3.9710884 -3.9474542 -3.9258342 -3.9320934 -3.9859505 -4.0516958 -4.1054554 -4.1381311 -4.1522322 -4.1512079 -4.1479173][-4.0900126 -4.0739603 -4.0475383 -4.0242929 -4.005054 -3.9823959 -3.9521949 -3.9459443 -3.9875374 -4.0440946 -4.0934367 -4.1245308 -4.1370482 -4.1330023 -4.1266766][-4.1264939 -4.1139522 -4.0974517 -4.0888276 -4.0840812 -4.0768437 -4.0592093 -4.0532703 -4.076849 -4.1107459 -4.1395607 -4.1544166 -4.1568651 -4.1470838 -4.1327748][-4.1596975 -4.1515808 -4.1457734 -4.1469779 -4.1488252 -4.1500425 -4.1454859 -4.1470766 -4.1618829 -4.1796756 -4.1925092 -4.1973209 -4.1979356 -4.1914091 -4.1768427][-4.1809716 -4.1750288 -4.1751909 -4.1821761 -4.188735 -4.1961694 -4.1999192 -4.2054882 -4.2141433 -4.222476 -4.2290115 -4.2304387 -4.2323503 -4.2313457 -4.2235889][-4.1942711 -4.1870065 -4.186182 -4.1920538 -4.1998081 -4.2130876 -4.2235231 -4.2322388 -4.237618 -4.2413 -4.245295 -4.2457404 -4.2470617 -4.2493305 -4.2497659][-4.2051482 -4.1974144 -4.1922688 -4.1918292 -4.1959553 -4.2107859 -4.2256217 -4.2352819 -4.2372503 -4.2373805 -4.2408552 -4.2428694 -4.2434855 -4.2450452 -4.2486358][-4.2134581 -4.207375 -4.2015929 -4.1961708 -4.1957397 -4.2073016 -4.2203932 -4.2285275 -4.2283297 -4.2263293 -4.2276173 -4.2303205 -4.231544 -4.2323446 -4.2342014][-4.2140288 -4.2097154 -4.2064233 -4.2014089 -4.201591 -4.2111244 -4.2214332 -4.2279353 -4.2278538 -4.2247114 -4.2230573 -4.2241797 -4.2262893 -4.228539 -4.2305236][-4.213903 -4.2112718 -4.210813 -4.2080674 -4.2086668 -4.2154727 -4.2227459 -4.2271647 -4.2274132 -4.2241635 -4.2207518 -4.2213883 -4.2254443 -4.2306976 -4.2346082]]...]
INFO - root - 2017-12-07 11:38:49.563293: step 9010, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 89h:22m:11s remains)
INFO - root - 2017-12-07 11:38:59.318077: step 9020, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 89h:43m:13s remains)
INFO - root - 2017-12-07 11:39:09.114830: step 9030, loss = 2.07, batch loss = 2.02 (7.7 examples/sec; 1.038 sec/batch; 93h:14m:04s remains)
INFO - root - 2017-12-07 11:39:18.783997: step 9040, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.925 sec/batch; 83h:04m:17s remains)
INFO - root - 2017-12-07 11:39:28.304926: step 9050, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 74h:36m:41s remains)
INFO - root - 2017-12-07 11:39:37.978394: step 9060, loss = 2.12, batch loss = 2.06 (8.1 examples/sec; 0.990 sec/batch; 88h:56m:17s remains)
INFO - root - 2017-12-07 11:39:47.609341: step 9070, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 85h:40m:43s remains)
INFO - root - 2017-12-07 11:39:57.181734: step 9080, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 83h:49m:01s remains)
INFO - root - 2017-12-07 11:40:06.661718: step 9090, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 89h:11m:25s remains)
INFO - root - 2017-12-07 11:40:16.233888: step 9100, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 85h:47m:15s remains)
2017-12-07 11:40:17.202306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3027496 -4.2955832 -4.2942214 -4.3009558 -4.3127341 -4.3217053 -4.3287368 -4.3357887 -4.3384762 -4.3390851 -4.3402419 -4.3447332 -4.3507195 -4.3544965 -4.3538265][-4.2584209 -4.2454405 -4.2444582 -4.259398 -4.2797532 -4.2916942 -4.2987919 -4.3089051 -4.3136225 -4.3155584 -4.3210688 -4.3316565 -4.34174 -4.3481331 -4.3481517][-4.2105465 -4.1910758 -4.1919904 -4.2203031 -4.249455 -4.2594891 -4.2608061 -4.2689872 -4.2727814 -4.2763066 -4.2886128 -4.3070617 -4.3242755 -4.3342881 -4.335187][-4.1764164 -4.1523314 -4.1539192 -4.1891479 -4.22016 -4.220746 -4.2084503 -4.2120934 -4.2152305 -4.2224731 -4.2420735 -4.2678142 -4.2920704 -4.309196 -4.3160596][-4.1714344 -4.1465588 -4.1419954 -4.1644998 -4.1775737 -4.1623926 -4.1358442 -4.134016 -4.1427541 -4.1586361 -4.1834745 -4.2146869 -4.249774 -4.2771292 -4.2936397][-4.18045 -4.1542106 -4.1402588 -4.142941 -4.1300755 -4.0920095 -4.0483837 -4.0415916 -4.0596194 -4.0858703 -4.1176591 -4.1562662 -4.2023959 -4.2422571 -4.270875][-4.1796913 -4.1478057 -4.1217847 -4.1041164 -4.0641747 -3.9987798 -3.936995 -3.9348149 -3.973489 -4.0203466 -4.0648074 -4.1104445 -4.1634092 -4.2111144 -4.2487068][-4.1685133 -4.1292443 -4.095665 -4.0665693 -4.0116806 -3.9242365 -3.8460593 -3.8511281 -3.9212246 -3.9981561 -4.0570369 -4.10466 -4.1518288 -4.1951232 -4.2344351][-4.1640449 -4.1260853 -4.097012 -4.0710549 -4.0209174 -3.9397302 -3.8750389 -3.890285 -3.965591 -4.0432243 -4.0976658 -4.1360927 -4.1703835 -4.2045441 -4.2371173][-4.18124 -4.1506033 -4.1303506 -4.1152163 -4.08342 -4.0328212 -3.9981921 -4.0132875 -4.0625262 -4.1133261 -4.1523342 -4.1796837 -4.2008157 -4.223619 -4.2494116][-4.2189765 -4.1970491 -4.1824718 -4.1743622 -4.1587505 -4.1354 -4.1188745 -4.1228037 -4.1450367 -4.1739511 -4.202704 -4.2229533 -4.2350826 -4.2501211 -4.271894][-4.25922 -4.245131 -4.2346745 -4.2305989 -4.22421 -4.2146788 -4.2064023 -4.2044573 -4.2133994 -4.2293553 -4.2468834 -4.257381 -4.2631183 -4.2751193 -4.2947206][-4.2857785 -4.2778587 -4.272233 -4.2702494 -4.2678366 -4.2650967 -4.260355 -4.2573991 -4.261899 -4.2703238 -4.2780533 -4.2818594 -4.2858453 -4.2967839 -4.3135056][-4.306735 -4.3029118 -4.3014741 -4.3009629 -4.2988229 -4.2976074 -4.2954044 -4.29405 -4.2974095 -4.3023524 -4.3065224 -4.3076229 -4.3106861 -4.3187022 -4.3300591][-4.3260713 -4.3261662 -4.3273277 -4.3262429 -4.3230562 -4.3209276 -4.3197269 -4.3197112 -4.322053 -4.325779 -4.3284931 -4.3295293 -4.3316593 -4.3364549 -4.3422337]]...]
INFO - root - 2017-12-07 11:40:26.889585: step 9110, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 84h:48m:00s remains)
INFO - root - 2017-12-07 11:40:36.563317: step 9120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 81h:38m:32s remains)
INFO - root - 2017-12-07 11:40:46.120105: step 9130, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 87h:37m:22s remains)
INFO - root - 2017-12-07 11:40:55.700127: step 9140, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 88h:19m:38s remains)
INFO - root - 2017-12-07 11:41:05.373485: step 9150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 85h:09m:55s remains)
INFO - root - 2017-12-07 11:41:15.101631: step 9160, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.004 sec/batch; 90h:10m:57s remains)
INFO - root - 2017-12-07 11:41:24.710798: step 9170, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 83h:35m:28s remains)
INFO - root - 2017-12-07 11:41:34.346997: step 9180, loss = 2.11, batch loss = 2.05 (9.5 examples/sec; 0.840 sec/batch; 75h:24m:32s remains)
INFO - root - 2017-12-07 11:41:44.154820: step 9190, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.017 sec/batch; 91h:20m:08s remains)
INFO - root - 2017-12-07 11:41:53.778936: step 9200, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 89h:50m:30s remains)
2017-12-07 11:41:54.748020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626429 -4.2668462 -4.2755947 -4.2677059 -4.2504954 -4.2347975 -4.2201829 -4.2100778 -4.2194257 -4.2429075 -4.2665234 -4.2775917 -4.2909942 -4.297709 -4.2924805][-4.2672758 -4.2736082 -4.286706 -4.2853494 -4.2774634 -4.267024 -4.2575517 -4.2483869 -4.2517471 -4.265954 -4.2791133 -4.2869849 -4.3005157 -4.304255 -4.2980852][-4.2579851 -4.2655058 -4.2806535 -4.2839332 -4.282567 -4.2777896 -4.2730327 -4.2665739 -4.2700019 -4.2807312 -4.2892594 -4.2936044 -4.302845 -4.3033295 -4.2988391][-4.2417831 -4.2533846 -4.269361 -4.2716427 -4.2681723 -4.2614436 -4.2560158 -4.2561979 -4.271884 -4.2927666 -4.30736 -4.315608 -4.3211412 -4.3166714 -4.3090477][-4.2159829 -4.2312732 -4.2464924 -4.2435808 -4.2297397 -4.21175 -4.2007341 -4.2040172 -4.2352157 -4.2811732 -4.3157983 -4.3326206 -4.33695 -4.329998 -4.3176212][-4.1731682 -4.1912937 -4.20758 -4.2011065 -4.1747575 -4.1391234 -4.1101336 -4.1049604 -4.1462588 -4.2219849 -4.2847271 -4.315536 -4.326333 -4.32129 -4.3062959][-4.141819 -4.159081 -4.1711516 -4.1588569 -4.1188483 -4.0606751 -4.0041218 -3.9717903 -4.0055895 -4.1062603 -4.2018428 -4.2566938 -4.2872486 -4.2945895 -4.2810774][-4.1363468 -4.1510444 -4.1579757 -4.1428561 -4.0960546 -4.0230165 -3.9348903 -3.8571365 -3.857393 -3.9640994 -4.086895 -4.1710787 -4.226891 -4.2539034 -4.252418][-4.1531191 -4.1604934 -4.1630392 -4.1498928 -4.1136832 -4.0496149 -3.954752 -3.8579078 -3.8341606 -3.9136224 -4.0273714 -4.1209335 -4.192234 -4.2333136 -4.2430482][-4.1897125 -4.1914124 -4.1913195 -4.1833735 -4.1663547 -4.1274657 -4.0561314 -3.9847322 -3.9759629 -4.0276151 -4.1004796 -4.1643343 -4.2189512 -4.2554116 -4.26592][-4.2485962 -4.2433677 -4.2379708 -4.2316747 -4.2265859 -4.2120976 -4.1716709 -4.13073 -4.1331253 -4.1638126 -4.2046132 -4.2423792 -4.2793307 -4.3069363 -4.3137059][-4.2938757 -4.28409 -4.2789116 -4.2775993 -4.2804956 -4.2807832 -4.2622495 -4.2429628 -4.247797 -4.264327 -4.2855096 -4.306962 -4.332273 -4.3512173 -4.3536744][-4.3196387 -4.31263 -4.3089085 -4.3095055 -4.31384 -4.3180304 -4.3122287 -4.3062086 -4.3110414 -4.3203917 -4.3310404 -4.3409791 -4.3543191 -4.3643475 -4.3651242][-4.3367848 -4.33277 -4.3298664 -4.3303328 -4.3345542 -4.33965 -4.3401289 -4.3393512 -4.3425765 -4.348515 -4.3533707 -4.3570509 -4.362638 -4.366652 -4.3670063][-4.3503227 -4.3488007 -4.346952 -4.3464265 -4.3484907 -4.35257 -4.3551059 -4.3561273 -4.358542 -4.3618279 -4.3642673 -4.3654685 -4.3667994 -4.3683195 -4.369338]]...]
INFO - root - 2017-12-07 11:42:04.534206: step 9210, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 0.996 sec/batch; 89h:28m:58s remains)
INFO - root - 2017-12-07 11:42:14.186771: step 9220, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 84h:39m:09s remains)
INFO - root - 2017-12-07 11:42:23.745640: step 9230, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 86h:43m:11s remains)
INFO - root - 2017-12-07 11:42:33.484490: step 9240, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.918 sec/batch; 82h:24m:26s remains)
INFO - root - 2017-12-07 11:42:43.142975: step 9250, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 89h:20m:49s remains)
INFO - root - 2017-12-07 11:42:52.830502: step 9260, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:44m:29s remains)
INFO - root - 2017-12-07 11:43:02.568819: step 9270, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 85h:26m:05s remains)
INFO - root - 2017-12-07 11:43:12.186089: step 9280, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.955 sec/batch; 85h:45m:41s remains)
INFO - root - 2017-12-07 11:43:21.919386: step 9290, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.027 sec/batch; 92h:10m:22s remains)
INFO - root - 2017-12-07 11:43:31.468313: step 9300, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 89h:18m:20s remains)
2017-12-07 11:43:32.344952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.270051 -4.2622914 -4.2523079 -4.2305417 -4.2126408 -4.1935306 -4.1748538 -4.1616507 -4.1535435 -4.1406164 -4.1427879 -4.1743369 -4.2049713 -4.2257276 -4.2386961][-4.2547059 -4.2533641 -4.2501254 -4.2319112 -4.21439 -4.1962504 -4.1816788 -4.1802573 -4.1848459 -4.1728411 -4.1653805 -4.1812792 -4.197753 -4.2155342 -4.2332644][-4.2323542 -4.2321143 -4.2360425 -4.2236848 -4.2066045 -4.1921287 -4.1842718 -4.1964564 -4.2128596 -4.2073879 -4.1921697 -4.1910396 -4.1925755 -4.2006783 -4.216104][-4.2225852 -4.2155805 -4.2179918 -4.2062616 -4.18404 -4.1673164 -4.1656346 -4.1906824 -4.2205863 -4.2276731 -4.2133465 -4.2010903 -4.1864376 -4.1838579 -4.1944652][-4.2238722 -4.209712 -4.20602 -4.1889424 -4.1580248 -4.1274991 -4.1195569 -4.1510983 -4.194912 -4.2174687 -4.2122412 -4.1942792 -4.1676168 -4.1591287 -4.1715045][-4.2225766 -4.2043991 -4.19341 -4.1673141 -4.1260133 -4.0744009 -4.0469704 -4.0825043 -4.147748 -4.192965 -4.2035527 -4.1859431 -4.1530218 -4.1399345 -4.15533][-4.1979594 -4.1793447 -4.1653519 -4.1329312 -4.0855627 -4.0148916 -3.9591169 -3.9914503 -4.0764985 -4.1460795 -4.1767073 -4.169929 -4.1431227 -4.1336656 -4.1506224][-4.1736584 -4.1534667 -4.1390424 -4.1071668 -4.0649662 -4.0010881 -3.9376674 -3.954998 -4.0356259 -4.1079092 -4.1457238 -4.1467309 -4.1267796 -4.1243262 -4.1406755][-4.1509209 -4.1271782 -4.1148262 -4.0922279 -4.0663414 -4.0360217 -4.0019989 -4.007443 -4.0559192 -4.1036439 -4.1267891 -4.12451 -4.1110163 -4.112092 -4.1223164][-4.1449337 -4.1148558 -4.0989041 -4.0829334 -4.0703993 -4.068161 -4.063333 -4.0688691 -4.0922046 -4.1117063 -4.1137366 -4.1032696 -4.0942459 -4.0967765 -4.1014366][-4.1681151 -4.1350651 -4.115603 -4.1061535 -4.1043649 -4.115757 -4.1267471 -4.1350546 -4.1446176 -4.1459656 -4.1339779 -4.1171679 -4.110425 -4.1111054 -4.1093841][-4.1880465 -4.1600914 -4.1445575 -4.1415896 -4.1487174 -4.1665864 -4.1822705 -4.1896315 -4.1921568 -4.1855583 -4.1687417 -4.1505332 -4.1435461 -4.1418185 -4.1379724][-4.2080317 -4.1860008 -4.1758051 -4.1780524 -4.1909647 -4.2098346 -4.2268205 -4.2355342 -4.2374139 -4.230473 -4.2180696 -4.2053747 -4.1987286 -4.1951962 -4.1905613][-4.2369337 -4.2220597 -4.2175956 -4.2221518 -4.2338367 -4.2474976 -4.2609072 -4.2687392 -4.2700071 -4.2651882 -4.2584548 -4.2520118 -4.247982 -4.24598 -4.2428865][-4.2737064 -4.2672582 -4.2665396 -4.2696257 -4.2757659 -4.2838893 -4.2924676 -4.29788 -4.2985926 -4.2959366 -4.2934756 -4.2906542 -4.28837 -4.2866263 -4.2845287]]...]
INFO - root - 2017-12-07 11:43:41.890160: step 9310, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 82h:03m:24s remains)
INFO - root - 2017-12-07 11:43:51.692400: step 9320, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 89h:07m:24s remains)
INFO - root - 2017-12-07 11:44:01.338915: step 9330, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 87h:18m:19s remains)
INFO - root - 2017-12-07 11:44:11.133386: step 9340, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.978 sec/batch; 87h:49m:03s remains)
INFO - root - 2017-12-07 11:44:20.485302: step 9350, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 86h:59m:16s remains)
INFO - root - 2017-12-07 11:44:30.231451: step 9360, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 88h:56m:49s remains)
INFO - root - 2017-12-07 11:44:39.753802: step 9370, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.967 sec/batch; 86h:45m:43s remains)
INFO - root - 2017-12-07 11:44:49.278350: step 9380, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 87h:39m:38s remains)
INFO - root - 2017-12-07 11:44:59.017523: step 9390, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 83h:50m:52s remains)
INFO - root - 2017-12-07 11:45:08.647027: step 9400, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 86h:56m:51s remains)
2017-12-07 11:45:09.554756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3451972 -4.3432212 -4.3418989 -4.3407083 -4.3399911 -4.3392458 -4.3373542 -4.3351283 -4.3325295 -4.329885 -4.3298554 -4.332479 -4.3345594 -4.3355336 -4.3343849][-4.347404 -4.3479662 -4.3480749 -4.3464894 -4.3435688 -4.3388734 -4.3325038 -4.3280244 -4.3261843 -4.3260717 -4.3283052 -4.3324871 -4.3345442 -4.3346076 -4.3330889][-4.3474312 -4.3508072 -4.3517876 -4.3493357 -4.3440681 -4.3355956 -4.3255596 -4.3175206 -4.3151703 -4.3175845 -4.3228145 -4.329864 -4.33273 -4.3323588 -4.32999][-4.33964 -4.3454752 -4.3472767 -4.3446527 -4.3367252 -4.3234429 -4.3083358 -4.2942085 -4.2900109 -4.2956967 -4.3072958 -4.3204312 -4.3265977 -4.3276615 -4.3257375][-4.3210073 -4.3299727 -4.3328743 -4.3300109 -4.31914 -4.3005104 -4.2788172 -4.2569318 -4.2515435 -4.2610292 -4.2796717 -4.302186 -4.3157997 -4.3217506 -4.3220768][-4.2888279 -4.2985706 -4.3015366 -4.297431 -4.2811003 -4.25238 -4.2196827 -4.1897078 -4.191267 -4.21418 -4.2439442 -4.2758861 -4.2982345 -4.3119555 -4.3168535][-4.2433281 -4.2495918 -4.2498074 -4.243259 -4.2190466 -4.176034 -4.1229382 -4.0819173 -4.1027861 -4.1539688 -4.201767 -4.2434664 -4.2731743 -4.2952733 -4.3058596][-4.1985207 -4.1975451 -4.1921206 -4.1816049 -4.1527538 -4.0974627 -4.0158772 -3.9531419 -3.9961212 -4.0848022 -4.1577463 -4.2115455 -4.2466011 -4.2742591 -4.2899256][-4.18329 -4.1740417 -4.1605248 -4.1419568 -4.1069527 -4.0405903 -3.9328206 -3.8425913 -3.8999379 -4.02313 -4.1224627 -4.1865196 -4.2254882 -4.2573385 -4.2779393][-4.2024369 -4.1889906 -4.1740513 -4.1529946 -4.1167827 -4.053215 -3.9501786 -3.8598847 -3.9091525 -4.0279708 -4.12728 -4.189363 -4.2271523 -4.2590122 -4.2798433][-4.2334871 -4.2202339 -4.2093596 -4.1924491 -4.1601086 -4.1095462 -4.0360789 -3.9719441 -4.0021896 -4.0875268 -4.1671281 -4.2192335 -4.2522659 -4.2799568 -4.2965508][-4.2724662 -4.2603235 -4.2532511 -4.2420297 -4.2176232 -4.1779709 -4.1263571 -4.0832129 -4.100966 -4.1566133 -4.2158294 -4.25706 -4.2846565 -4.3070831 -4.3179321][-4.3128 -4.3016973 -4.2957096 -4.28765 -4.2692909 -4.2370811 -4.197742 -4.1687441 -4.1784225 -4.2138214 -4.2565732 -4.2879434 -4.3106537 -4.32816 -4.3345947][-4.3422952 -4.3346004 -4.32837 -4.3208981 -4.3063383 -4.2809834 -4.2518492 -4.2345209 -4.2419558 -4.2649484 -4.2935762 -4.3140793 -4.3303308 -4.3419189 -4.3444605][-4.3577881 -4.3549423 -4.3512306 -4.3460088 -4.3374443 -4.3211131 -4.3016911 -4.2910237 -4.2950339 -4.3070707 -4.3224678 -4.3347921 -4.344512 -4.350378 -4.3492875]]...]
INFO - root - 2017-12-07 11:45:19.358748: step 9410, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 84h:35m:41s remains)
INFO - root - 2017-12-07 11:45:29.102449: step 9420, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 87h:15m:12s remains)
INFO - root - 2017-12-07 11:45:38.704445: step 9430, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 82h:18m:36s remains)
INFO - root - 2017-12-07 11:45:48.454895: step 9440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 86h:56m:11s remains)
INFO - root - 2017-12-07 11:45:58.013291: step 9450, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 88h:36m:35s remains)
INFO - root - 2017-12-07 11:46:07.533501: step 9460, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 82h:06m:51s remains)
INFO - root - 2017-12-07 11:46:17.305308: step 9470, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 87h:44m:00s remains)
INFO - root - 2017-12-07 11:46:26.981660: step 9480, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 86h:58m:08s remains)
INFO - root - 2017-12-07 11:46:36.571188: step 9490, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.947 sec/batch; 84h:56m:14s remains)
INFO - root - 2017-12-07 11:46:46.307907: step 9500, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 87h:20m:20s remains)
2017-12-07 11:46:47.299509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2157869 -4.2502828 -4.2707386 -4.2472887 -4.175797 -4.0911655 -4.0449061 -4.064918 -4.140264 -4.2267585 -4.2668204 -4.2856774 -4.2947807 -4.29419 -4.2870965][-4.2350779 -4.2647262 -4.281496 -4.2505655 -4.1736441 -4.0879264 -4.0404882 -4.0587325 -4.1317267 -4.2157488 -4.2612834 -4.287859 -4.3049121 -4.3089762 -4.3046479][-4.2564907 -4.2721319 -4.279716 -4.243875 -4.1664052 -4.0868807 -4.0399666 -4.05079 -4.1155715 -4.1882448 -4.2330151 -4.269804 -4.2998734 -4.3131223 -4.3172784][-4.2688284 -4.26753 -4.2581582 -4.2148037 -4.1482019 -4.08309 -4.0442209 -4.0525117 -4.105813 -4.1610827 -4.2014403 -4.2444987 -4.2838039 -4.3076663 -4.3189807][-4.2548947 -4.24236 -4.2177954 -4.1670341 -4.1105485 -4.0656948 -4.0388393 -4.0454149 -4.0856571 -4.1260195 -4.1633081 -4.2097487 -4.2534232 -4.2850757 -4.3019557][-4.2257795 -4.2079916 -4.1687226 -4.1111746 -4.0666981 -4.0428319 -4.0255466 -4.0258961 -4.04962 -4.0828924 -4.1245141 -4.17321 -4.2164083 -4.2525158 -4.2744074][-4.2064509 -4.1893673 -4.1419969 -4.0824666 -4.0475917 -4.0396047 -4.0325866 -4.0276775 -4.0349059 -4.0654 -4.113873 -4.1633635 -4.201777 -4.2361565 -4.260407][-4.2136579 -4.2007461 -4.1580324 -4.1040492 -4.0712848 -4.0682912 -4.0644341 -4.0624914 -4.0634074 -4.0907927 -4.1422939 -4.1928525 -4.2276306 -4.2520828 -4.2693706][-4.2430277 -4.2326727 -4.1996446 -4.1521025 -4.1197462 -4.1143794 -4.111165 -4.1134744 -4.1188974 -4.1441255 -4.1934333 -4.2402043 -4.26982 -4.2836642 -4.2919736][-4.2751789 -4.2698069 -4.2489548 -4.2145815 -4.1876659 -4.1840315 -4.1848087 -4.1872067 -4.1908913 -4.20887 -4.2454553 -4.28142 -4.3050518 -4.3127022 -4.3148317][-4.2934313 -4.2931175 -4.2826676 -4.2616887 -4.2416368 -4.242867 -4.249455 -4.2534485 -4.2537065 -4.2616987 -4.2827363 -4.3066182 -4.3236384 -4.3298264 -4.3305941][-4.2964158 -4.3004041 -4.2988176 -4.2872796 -4.2750707 -4.2785316 -4.2900653 -4.2946205 -4.2898917 -4.2941651 -4.3063459 -4.3181162 -4.3272629 -4.3319292 -4.3326182][-4.2901778 -4.2937279 -4.2959995 -4.2892261 -4.2819481 -4.2866197 -4.2969112 -4.30122 -4.2973523 -4.3003111 -4.3094959 -4.3170156 -4.3221922 -4.3266363 -4.328258][-4.2926455 -4.2932544 -4.2947311 -4.2869167 -4.2785816 -4.2812595 -4.288774 -4.2935967 -4.2926188 -4.2958188 -4.3028746 -4.3088741 -4.314497 -4.32017 -4.3222075][-4.3015685 -4.3006806 -4.3005171 -4.294559 -4.2875347 -4.287221 -4.2903891 -4.2938356 -4.2956171 -4.2999997 -4.30634 -4.3109355 -4.3156662 -4.3197856 -4.3216872]]...]
INFO - root - 2017-12-07 11:46:56.872199: step 9510, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 82h:57m:48s remains)
INFO - root - 2017-12-07 11:47:06.547602: step 9520, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 87h:21m:25s remains)
INFO - root - 2017-12-07 11:47:16.131209: step 9530, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 85h:28m:06s remains)
INFO - root - 2017-12-07 11:47:25.937364: step 9540, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 87h:30m:50s remains)
INFO - root - 2017-12-07 11:47:35.643013: step 9550, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 84h:39m:08s remains)
INFO - root - 2017-12-07 11:47:45.360995: step 9560, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 84h:33m:32s remains)
INFO - root - 2017-12-07 11:47:54.812019: step 9570, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 83h:43m:36s remains)
INFO - root - 2017-12-07 11:48:04.553075: step 9580, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 86h:41m:12s remains)
INFO - root - 2017-12-07 11:48:14.102516: step 9590, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 85h:44m:01s remains)
INFO - root - 2017-12-07 11:48:23.850816: step 9600, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.006 sec/batch; 90h:15m:02s remains)
2017-12-07 11:48:24.899210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3216448 -4.3249636 -4.3281288 -4.3224435 -4.3066912 -4.2831106 -4.258667 -4.2436519 -4.2319641 -4.2174745 -4.1951194 -4.1729655 -4.1402078 -4.0946183 -4.0635777][-4.30571 -4.310288 -4.3195543 -4.3182058 -4.3018537 -4.2739773 -4.2443495 -4.2299275 -4.2205925 -4.2069774 -4.1818304 -4.1602211 -4.1264668 -4.0696187 -4.0246854][-4.2809267 -4.2896681 -4.3071656 -4.3100047 -4.295866 -4.270401 -4.2429261 -4.2332411 -4.2278805 -4.2163095 -4.19247 -4.1780925 -4.1542859 -4.1030192 -4.0591812][-4.2466764 -4.2608223 -4.2848039 -4.2939568 -4.2860851 -4.2649617 -4.2434168 -4.2424974 -4.2444754 -4.2375178 -4.2210097 -4.2179246 -4.2081318 -4.1725693 -4.1411142][-4.20515 -4.2227235 -4.2535405 -4.2701588 -4.2706947 -4.2550321 -4.2398834 -4.2447777 -4.2522631 -4.2530985 -4.2475605 -4.2552958 -4.2564788 -4.2365203 -4.2194662][-4.1552076 -4.1753697 -4.2104955 -4.2371235 -4.2445369 -4.2316489 -4.2199507 -4.22308 -4.2325239 -4.240591 -4.2468214 -4.26567 -4.2809052 -4.2747955 -4.2686596][-4.1128325 -4.1371913 -4.1688814 -4.1956058 -4.2045021 -4.1956296 -4.1861873 -4.183496 -4.1937447 -4.2106018 -4.22571 -4.2514815 -4.2766562 -4.2803373 -4.2845931][-4.1081638 -4.1268983 -4.1406169 -4.1514196 -4.1545997 -4.1462688 -4.1341014 -4.1240411 -4.1329093 -4.1590571 -4.18522 -4.2221513 -4.2563796 -4.270277 -4.28677][-4.1208739 -4.1251593 -4.116477 -4.100667 -4.0823131 -4.0606475 -4.0406237 -4.0238523 -4.0356669 -4.0778632 -4.1212368 -4.1742296 -4.2221 -4.2500191 -4.2781234][-4.126061 -4.1146355 -4.0876546 -4.0500913 -4.0089254 -3.9748855 -3.9552095 -3.941601 -3.9613106 -4.0220942 -4.0823865 -4.1437397 -4.1962404 -4.2344751 -4.2701812][-4.1379447 -4.115859 -4.082613 -4.0411372 -4.00015 -3.9738615 -3.9689159 -3.9642138 -3.9833615 -4.0429897 -4.1007185 -4.1497869 -4.1923704 -4.23028 -4.2647586][-4.1624479 -4.1400328 -4.1136642 -4.0827618 -4.0596867 -4.0514321 -4.0566411 -4.058373 -4.0705738 -4.115562 -4.1619573 -4.194983 -4.2210612 -4.2473021 -4.2714424][-4.1987286 -4.1794071 -4.1677885 -4.1529346 -4.1474667 -4.1550026 -4.1650996 -4.1703391 -4.1775846 -4.2055106 -4.2368431 -4.2544527 -4.2671428 -4.2804933 -4.2918544][-4.2520566 -4.2381067 -4.2368464 -4.2351856 -4.2395988 -4.2512784 -4.2602735 -4.2632618 -4.2671008 -4.2811122 -4.2979031 -4.3053412 -4.3089447 -4.3103471 -4.3080678][-4.2967958 -4.288734 -4.294714 -4.3006196 -4.3084021 -4.3183832 -4.3212953 -4.3181491 -4.3142014 -4.3177152 -4.3250318 -4.328373 -4.3283386 -4.3227105 -4.310286]]...]
INFO - root - 2017-12-07 11:48:34.557940: step 9610, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 88h:34m:04s remains)
INFO - root - 2017-12-07 11:48:44.011571: step 9620, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 87h:28m:57s remains)
INFO - root - 2017-12-07 11:48:53.478425: step 9630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 81h:56m:59s remains)
INFO - root - 2017-12-07 11:49:03.152567: step 9640, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 88h:52m:03s remains)
INFO - root - 2017-12-07 11:49:12.845210: step 9650, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 85h:25m:21s remains)
INFO - root - 2017-12-07 11:49:22.603153: step 9660, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 86h:08m:22s remains)
INFO - root - 2017-12-07 11:49:32.081999: step 9670, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 87h:23m:33s remains)
INFO - root - 2017-12-07 11:49:41.685130: step 9680, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 86h:21m:47s remains)
INFO - root - 2017-12-07 11:49:51.395781: step 9690, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.941 sec/batch; 84h:22m:45s remains)
INFO - root - 2017-12-07 11:50:01.088473: step 9700, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 89h:45m:10s remains)
2017-12-07 11:50:02.066955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2939472 -4.2993526 -4.3000288 -4.2956839 -4.2976065 -4.3043032 -4.3023171 -4.2989631 -4.2988715 -4.30278 -4.3114204 -4.3199363 -4.3195758 -4.3123794 -4.3069415][-4.2762485 -4.2794785 -4.2779632 -4.2697239 -4.2685313 -4.2735672 -4.2627783 -4.2600317 -4.267662 -4.2813392 -4.3005466 -4.3176904 -4.3193312 -4.3097467 -4.3033376][-4.2578793 -4.257411 -4.2459207 -4.2305861 -4.2221241 -4.2186627 -4.1963191 -4.197566 -4.218369 -4.244801 -4.2774439 -4.3022313 -4.3071289 -4.2987652 -4.2928886][-4.2559738 -4.247407 -4.2210026 -4.1959171 -4.1759987 -4.1531219 -4.1090193 -4.1127295 -4.1525512 -4.19563 -4.2436333 -4.2776132 -4.2898731 -4.2851038 -4.2820334][-4.253932 -4.232861 -4.1935043 -4.1602268 -4.130342 -4.0790849 -4.0038972 -4.0127006 -4.07616 -4.1383495 -4.2015023 -4.2505121 -4.2718596 -4.2732353 -4.2734795][-4.2498684 -4.2215519 -4.1766062 -4.1354427 -4.0907722 -3.9991763 -3.8798704 -3.9052095 -4.0164394 -4.1066351 -4.1798139 -4.2367725 -4.2631588 -4.2665367 -4.2682385][-4.24533 -4.2199311 -4.1793771 -4.1263 -4.0531564 -3.9079521 -3.7277474 -3.7842751 -3.9647467 -4.0872793 -4.1675911 -4.2293348 -4.2590313 -4.2642832 -4.2665663][-4.2348189 -4.2183819 -4.1881804 -4.13261 -4.0390263 -3.8712635 -3.6820159 -3.7648561 -3.9693422 -4.0955305 -4.1705995 -4.2319446 -4.2633719 -4.269186 -4.2695942][-4.23752 -4.2312689 -4.21759 -4.1702957 -4.0817056 -3.9420567 -3.8088098 -3.882421 -4.046587 -4.1420479 -4.1976314 -4.246933 -4.2738566 -4.2778172 -4.2748313][-4.2271566 -4.2245736 -4.2223673 -4.1892281 -4.1155157 -4.002358 -3.9114554 -3.9776068 -4.1074567 -4.1821437 -4.2248425 -4.2626257 -4.2830372 -4.2833686 -4.2782145][-4.213912 -4.2196846 -4.22976 -4.2146292 -4.1606555 -4.0744448 -4.0122094 -4.0678082 -4.1693096 -4.2262774 -4.2571921 -4.2841716 -4.2972069 -4.2904544 -4.2841763][-4.2155466 -4.2292886 -4.2493753 -4.2512116 -4.224617 -4.16869 -4.1257563 -4.1647334 -4.2342935 -4.2753325 -4.2950039 -4.3117013 -4.3139176 -4.2993336 -4.290525][-4.2315936 -4.2495484 -4.2731543 -4.2859983 -4.2797 -4.2449155 -4.2128158 -4.2358022 -4.2802391 -4.3088069 -4.3183365 -4.3274927 -4.3240342 -4.3050194 -4.2947626][-4.2355647 -4.2587848 -4.2847972 -4.3022246 -4.3048754 -4.285584 -4.2636089 -4.2752213 -4.3044543 -4.3233109 -4.3239832 -4.3260894 -4.3200359 -4.3026347 -4.295083][-4.2565856 -4.2801614 -4.3051338 -4.32246 -4.3273749 -4.3149948 -4.3002892 -4.3045735 -4.3206954 -4.3276367 -4.320755 -4.3176346 -4.3098478 -4.2963204 -4.2926207]]...]
INFO - root - 2017-12-07 11:50:11.711634: step 9710, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 86h:30m:59s remains)
INFO - root - 2017-12-07 11:50:21.326921: step 9720, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 85h:40m:08s remains)
INFO - root - 2017-12-07 11:50:31.220263: step 9730, loss = 2.09, batch loss = 2.04 (7.9 examples/sec; 1.018 sec/batch; 91h:18m:46s remains)
INFO - root - 2017-12-07 11:50:40.875304: step 9740, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 85h:56m:31s remains)
INFO - root - 2017-12-07 11:50:50.567237: step 9750, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 87h:31m:02s remains)
INFO - root - 2017-12-07 11:51:00.403221: step 9760, loss = 2.05, batch loss = 1.99 (7.8 examples/sec; 1.023 sec/batch; 91h:42m:57s remains)
INFO - root - 2017-12-07 11:51:09.990794: step 9770, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 83h:48m:42s remains)
INFO - root - 2017-12-07 11:51:19.620202: step 9780, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 86h:09m:14s remains)
INFO - root - 2017-12-07 11:51:29.222821: step 9790, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 81h:38m:58s remains)
INFO - root - 2017-12-07 11:51:38.919361: step 9800, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 85h:43m:47s remains)
2017-12-07 11:51:39.814508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1897235 -4.1791639 -4.2037978 -4.2344713 -4.2565112 -4.2667341 -4.25957 -4.2390075 -4.21561 -4.2048368 -4.2150292 -4.2313623 -4.24202 -4.241281 -4.2326236][-4.2704983 -4.2584233 -4.2634625 -4.2716269 -4.2828865 -4.289465 -4.2830467 -4.263144 -4.2388539 -4.2288804 -4.2391853 -4.252408 -4.2598953 -4.2538018 -4.2349749][-4.3142338 -4.3019104 -4.2982726 -4.29453 -4.2951956 -4.2905746 -4.2784796 -4.2598095 -4.242126 -4.2414742 -4.2544937 -4.2637396 -4.2702188 -4.2623754 -4.239789][-4.3310351 -4.3191457 -4.3095093 -4.2971721 -4.2871056 -4.2709103 -4.2518167 -4.2347465 -4.2264752 -4.2370467 -4.2542763 -4.2646794 -4.2745728 -4.271122 -4.2529197][-4.3349657 -4.3226967 -4.3061318 -4.2861977 -4.2676868 -4.2400494 -4.2135658 -4.1972241 -4.1969867 -4.2165742 -4.2415872 -4.262095 -4.2807527 -4.28796 -4.2819123][-4.3411875 -4.3289256 -4.3046103 -4.274179 -4.2430706 -4.2019048 -4.16737 -4.1551223 -4.1657438 -4.195353 -4.2312331 -4.26485 -4.2914162 -4.3055344 -4.3077478][-4.3249631 -4.3103147 -4.2798924 -4.2449031 -4.2081246 -4.1613631 -4.1249352 -4.1216259 -4.1467113 -4.1888547 -4.232707 -4.2680554 -4.290309 -4.3015146 -4.3052073][-4.2983913 -4.2854543 -4.2586341 -4.2260914 -4.1900406 -4.1415358 -4.1015248 -4.0968313 -4.1288776 -4.1770725 -4.2252269 -4.2592297 -4.2772384 -4.2880464 -4.291718][-4.2810845 -4.2675323 -4.2436452 -4.2150917 -4.1811056 -4.1323867 -4.0868707 -4.07543 -4.1078229 -4.1589189 -4.2058496 -4.2348013 -4.2491484 -4.2614889 -4.2645946][-4.2616873 -4.2489877 -4.2286954 -4.2037072 -4.1751595 -4.1297078 -4.0808549 -4.0634 -4.090096 -4.13656 -4.1781578 -4.202508 -4.2168489 -4.233088 -4.2376442][-4.238421 -4.2297974 -4.21305 -4.1920733 -4.1715517 -4.1351185 -4.0922346 -4.0772533 -4.0996742 -4.1382928 -4.1696777 -4.1918516 -4.2088308 -4.2276754 -4.2330756][-4.2165365 -4.216208 -4.2046852 -4.1909637 -4.1836405 -4.1634903 -4.1376519 -4.13256 -4.1524138 -4.18002 -4.199502 -4.2131138 -4.2234464 -4.2338948 -4.2325292][-4.2065315 -4.2146173 -4.210618 -4.2082562 -4.215734 -4.2098479 -4.1967349 -4.193933 -4.2036214 -4.2149205 -4.2228022 -4.2262836 -4.2269311 -4.22618 -4.2188029][-4.2045422 -4.2078161 -4.2015142 -4.2049603 -4.2232351 -4.2298989 -4.2271881 -4.2245793 -4.2253647 -4.2257252 -4.22603 -4.2213421 -4.2163067 -4.2100821 -4.1990337][-4.1961493 -4.183177 -4.1695333 -4.1786194 -4.2043772 -4.2192011 -4.2259226 -4.2285714 -4.2306867 -4.2312274 -4.2303891 -4.2217922 -4.2112832 -4.1991067 -4.1817026]]...]
INFO - root - 2017-12-07 11:51:49.302443: step 9810, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.907 sec/batch; 81h:17m:41s remains)
INFO - root - 2017-12-07 11:51:58.974582: step 9820, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 85h:36m:46s remains)
INFO - root - 2017-12-07 11:52:08.693708: step 9830, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.045 sec/batch; 93h:41m:11s remains)
INFO - root - 2017-12-07 11:52:18.198951: step 9840, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 80h:48m:45s remains)
INFO - root - 2017-12-07 11:52:27.684468: step 9850, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 83h:15m:14s remains)
INFO - root - 2017-12-07 11:52:37.333703: step 9860, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 80h:14m:10s remains)
INFO - root - 2017-12-07 11:52:46.958009: step 9870, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.927 sec/batch; 83h:03m:22s remains)
INFO - root - 2017-12-07 11:52:56.643225: step 9880, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 89h:32m:03s remains)
INFO - root - 2017-12-07 11:53:06.328584: step 9890, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 85h:50m:15s remains)
INFO - root - 2017-12-07 11:53:15.967117: step 9900, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 84h:58m:39s remains)
2017-12-07 11:53:16.966749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.273294 -4.2781625 -4.2874823 -4.2942886 -4.2980175 -4.2996082 -4.2994351 -4.2977138 -4.2976904 -4.2984319 -4.2893348 -4.2734513 -4.2563028 -4.2394562 -4.2277327][-4.2784824 -4.2801523 -4.284687 -4.286243 -4.2858734 -4.2848639 -4.2822247 -4.2780657 -4.2774105 -4.2799959 -4.27065 -4.252594 -4.2309422 -4.207583 -4.1948237][-4.2800198 -4.28204 -4.2801681 -4.2748156 -4.2665553 -4.2594781 -4.2512255 -4.2438564 -4.2442842 -4.2500982 -4.2444286 -4.2294602 -4.2039261 -4.1721697 -4.1561995][-4.2788663 -4.2760348 -4.2653584 -4.2533731 -4.2404528 -4.2273583 -4.2114859 -4.2002807 -4.2080293 -4.2224088 -4.2240529 -4.2144551 -4.1863794 -4.1491003 -4.1270242][-4.279851 -4.2717533 -4.2514167 -4.2308021 -4.2143092 -4.1983528 -4.1775107 -4.163249 -4.1779051 -4.2037826 -4.2136974 -4.2125149 -4.1872039 -4.1482315 -4.1125684][-4.2805233 -4.2728763 -4.243361 -4.2136531 -4.1912937 -4.1716919 -4.14691 -4.1301293 -4.1501617 -4.1825757 -4.2005773 -4.2106624 -4.1950459 -4.1599007 -4.1141572][-4.2696986 -4.2624159 -4.2252021 -4.18819 -4.1582427 -4.1346464 -4.1113644 -4.0986977 -4.121335 -4.155797 -4.1817102 -4.2010655 -4.1961694 -4.1698446 -4.1286364][-4.2405014 -4.2296963 -4.1949077 -4.1563082 -4.12229 -4.0947709 -4.0782132 -4.0768852 -4.1035428 -4.1380372 -4.1686735 -4.1896262 -4.1874366 -4.1694107 -4.1421142][-4.2099929 -4.196445 -4.1645083 -4.1249423 -4.0905504 -4.0633316 -4.0535741 -4.0651493 -4.098742 -4.1365809 -4.1674714 -4.1840906 -4.180748 -4.1649523 -4.1442552][-4.191813 -4.1714997 -4.1376467 -4.0979471 -4.0705485 -4.0516882 -4.052979 -4.0680571 -4.1032286 -4.1427121 -4.1710391 -4.1837173 -4.1801176 -4.162179 -4.1400547][-4.19065 -4.1627007 -4.116889 -4.0781951 -4.0651417 -4.0642905 -4.0760469 -4.0899615 -4.1181564 -4.148797 -4.1726227 -4.186491 -4.185895 -4.1664095 -4.1387343][-4.1848803 -4.1536574 -4.1054468 -4.0710859 -4.0686483 -4.0866833 -4.1093621 -4.1203871 -4.1307182 -4.1485333 -4.1724758 -4.1911812 -4.1927524 -4.1728826 -4.1399541][-4.1766505 -4.1485496 -4.1070971 -4.0796041 -4.083364 -4.1128707 -4.1415653 -4.149549 -4.1496143 -4.1624303 -4.18653 -4.20409 -4.202879 -4.1802845 -4.1422868][-4.1838765 -4.1591992 -4.1310019 -4.115901 -4.1252766 -4.1536083 -4.1802115 -4.1905265 -4.1942081 -4.2039838 -4.2207975 -4.2290325 -4.2246861 -4.2034507 -4.1677365][-4.2255788 -4.2077494 -4.1918826 -4.1870193 -4.1977496 -4.2180257 -4.2389293 -4.2494216 -4.2558217 -4.262589 -4.2692428 -4.2696109 -4.2650943 -4.2512097 -4.2266278]]...]
INFO - root - 2017-12-07 11:53:26.637925: step 9910, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 86h:38m:50s remains)
INFO - root - 2017-12-07 11:53:36.264980: step 9920, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 83h:37m:11s remains)
INFO - root - 2017-12-07 11:53:45.887548: step 9930, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 77h:10m:06s remains)
INFO - root - 2017-12-07 11:53:55.582680: step 9940, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 88h:54m:33s remains)
INFO - root - 2017-12-07 11:54:05.211969: step 9950, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 87h:43m:40s remains)
INFO - root - 2017-12-07 11:54:14.677377: step 9960, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 82h:32m:12s remains)
INFO - root - 2017-12-07 11:54:24.353238: step 9970, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 83h:19m:09s remains)
INFO - root - 2017-12-07 11:54:34.181878: step 9980, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 89h:57m:57s remains)
INFO - root - 2017-12-07 11:54:43.822367: step 9990, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 89h:05m:49s remains)
INFO - root - 2017-12-07 11:54:53.179538: step 10000, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 87h:40m:35s remains)
2017-12-07 11:54:54.119416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2322993 -4.2379694 -4.2480364 -4.2606745 -4.2699513 -4.2704034 -4.2606921 -4.2420235 -4.2247443 -4.2140541 -4.20596 -4.2086487 -4.2247782 -4.2454062 -4.2586985][-4.2413788 -4.25355 -4.2717257 -4.2886357 -4.2986226 -4.2987676 -4.2891226 -4.2716646 -4.2584386 -4.252388 -4.2474494 -4.2465439 -4.252264 -4.2635741 -4.274394][-4.24851 -4.2657409 -4.2882977 -4.3047075 -4.3098264 -4.3037391 -4.2928815 -4.2815151 -4.2763424 -4.2763891 -4.2776775 -4.2786384 -4.2795291 -4.2859464 -4.2943854][-4.2475977 -4.2651219 -4.2867389 -4.3002691 -4.2947488 -4.2786503 -4.2610192 -4.248548 -4.2429657 -4.249939 -4.2650185 -4.2776346 -4.285943 -4.2965231 -4.3047419][-4.2377396 -4.2506657 -4.2648516 -4.266583 -4.2451386 -4.2112665 -4.17909 -4.1542215 -4.14813 -4.174201 -4.2103848 -4.2418184 -4.2607131 -4.2785687 -4.291966][-4.2250638 -4.230948 -4.2304778 -4.2128329 -4.1744671 -4.1186604 -4.0584846 -4.0033383 -4.0046806 -4.0727005 -4.142693 -4.1965756 -4.2266383 -4.2544184 -4.2753577][-4.2241669 -4.2150559 -4.1905565 -4.1421256 -4.0792828 -3.9993651 -3.901906 -3.8052542 -3.8267441 -3.9573531 -4.0672936 -4.1405106 -4.1843452 -4.2257013 -4.259275][-4.2337079 -4.2221055 -4.1827397 -4.1177373 -4.0418487 -3.9466369 -3.8293214 -3.7224932 -3.7675359 -3.9146638 -4.0214591 -4.0847039 -4.1367922 -4.1938844 -4.2401376][-4.2377787 -4.2388 -4.2145119 -4.1681938 -4.1081462 -4.0300131 -3.945154 -3.8849666 -3.9225156 -4.0056438 -4.0545864 -4.0811062 -4.1175089 -4.1717572 -4.2210836][-4.2188754 -4.2352128 -4.2373328 -4.2213979 -4.1938953 -4.15006 -4.1028166 -4.0703764 -4.0852203 -4.1122589 -4.120348 -4.1266494 -4.1538267 -4.1996942 -4.2392063][-4.1864324 -4.2191429 -4.2451224 -4.2512546 -4.2437177 -4.2206664 -4.191977 -4.1726723 -4.1722879 -4.1715136 -4.1647944 -4.169405 -4.2013512 -4.2470493 -4.2773871][-4.1642957 -4.2086167 -4.2440672 -4.2568331 -4.25207 -4.2285552 -4.2020731 -4.1889997 -4.1827645 -4.1724429 -4.1655293 -4.1805363 -4.2247543 -4.2753177 -4.3029566][-4.1759305 -4.2182894 -4.24633 -4.2518353 -4.2415233 -4.2126522 -4.1825056 -4.1680679 -4.163053 -4.1567955 -4.1564684 -4.178277 -4.223805 -4.274014 -4.3041091][-4.1939425 -4.2272282 -4.2491794 -4.2498169 -4.2357097 -4.2028494 -4.1709595 -4.156785 -4.1550794 -4.1536584 -4.1548834 -4.1720271 -4.2090559 -4.252512 -4.2871346][-4.2003 -4.2278366 -4.248086 -4.2501922 -4.236763 -4.2068133 -4.1743007 -4.1611738 -4.160377 -4.1555324 -4.1481123 -4.1566181 -4.1893845 -4.231144 -4.2674384]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 11:55:04.215569: step 10010, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 87h:49m:29s remains)
INFO - root - 2017-12-07 11:55:13.602596: step 10020, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.935 sec/batch; 83h:43m:03s remains)
INFO - root - 2017-12-07 11:55:23.395697: step 10030, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 89h:20m:26s remains)
INFO - root - 2017-12-07 11:55:32.962556: step 10040, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 83h:14m:18s remains)
INFO - root - 2017-12-07 11:55:42.541746: step 10050, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 86h:43m:12s remains)
INFO - root - 2017-12-07 11:55:52.094278: step 10060, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.009 sec/batch; 90h:20m:13s remains)
INFO - root - 2017-12-07 11:56:01.768199: step 10070, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 85h:29m:52s remains)
INFO - root - 2017-12-07 11:56:11.379075: step 10080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 80h:54m:12s remains)
INFO - root - 2017-12-07 11:56:20.952267: step 10090, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 84h:44m:16s remains)
INFO - root - 2017-12-07 11:56:30.597835: step 10100, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:31m:53s remains)
2017-12-07 11:56:31.563169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2147846 -4.21486 -4.2400627 -4.2754526 -4.302609 -4.3251963 -4.3366361 -4.3320885 -4.31812 -4.29612 -4.2776556 -4.2770104 -4.2917566 -4.3094592 -4.31728][-4.2087522 -4.1991038 -4.2136235 -4.2417541 -4.2686739 -4.2956018 -4.3150153 -4.3156075 -4.3034 -4.2797861 -4.2592425 -4.2559366 -4.2688732 -4.286943 -4.2973261][-4.2069273 -4.1875181 -4.188118 -4.2034092 -4.2252846 -4.2521906 -4.2769184 -4.2838025 -4.2791848 -4.262958 -4.2465568 -4.24067 -4.2468643 -4.25813 -4.2697759][-4.2045403 -4.1772852 -4.1640992 -4.1643791 -4.1741128 -4.1913605 -4.2126231 -4.2246442 -4.2328377 -4.2338738 -4.2308464 -4.2283883 -4.229301 -4.2348433 -4.2472334][-4.195395 -4.1594491 -4.135366 -4.1234307 -4.1164694 -4.1136589 -4.12187 -4.1338549 -4.1558876 -4.1784134 -4.1964493 -4.2076912 -4.2160878 -4.2262955 -4.2413807][-4.1938124 -4.1465249 -4.1099491 -4.0834141 -4.0543189 -4.0229063 -4.0058241 -4.01198 -4.0542164 -4.107954 -4.1550245 -4.1903348 -4.2160158 -4.2363772 -4.2514644][-4.2020216 -4.1477323 -4.1011524 -4.060595 -4.0079851 -3.9405227 -3.8859451 -3.8811872 -3.9465561 -4.0350323 -4.1135712 -4.1755877 -4.2213149 -4.2510228 -4.2640667][-4.21592 -4.1653829 -4.118206 -4.0719481 -4.0056505 -3.9119329 -3.8237431 -3.8043542 -3.8794584 -3.9851942 -4.0819254 -4.1602116 -4.219624 -4.2560754 -4.2697887][-4.217608 -4.1792479 -4.1434588 -4.1077728 -4.0546217 -3.9758103 -3.8981531 -3.8686447 -3.9130542 -3.993453 -4.0781932 -4.1529613 -4.2138677 -4.2507763 -4.2621813][-4.1991291 -4.1776557 -4.1623349 -4.1486955 -4.121665 -4.0773787 -4.02979 -4.0002527 -4.0099516 -4.0534124 -4.1097889 -4.1635432 -4.2104092 -4.2366452 -4.2387362][-4.169507 -4.161479 -4.1666913 -4.1779552 -4.1761465 -4.1633987 -4.1434913 -4.123322 -4.1180549 -4.1347666 -4.1600895 -4.1847849 -4.2080483 -4.2167277 -4.2047224][-4.1466112 -4.145844 -4.166585 -4.1980228 -4.2129731 -4.2166157 -4.2143245 -4.2042279 -4.1981707 -4.2017088 -4.2079763 -4.2110796 -4.2121959 -4.2020879 -4.1735086][-4.1490674 -4.154779 -4.1849289 -4.2267704 -4.2476315 -4.2541809 -4.2539921 -4.2451944 -4.2379389 -4.2356043 -4.2323589 -4.2225909 -4.2098269 -4.1904531 -4.1562505][-4.1850634 -4.1960473 -4.2282515 -4.2677097 -4.28397 -4.28102 -4.2692652 -4.250958 -4.2353034 -4.2243371 -4.2130938 -4.1994667 -4.182683 -4.1644998 -4.1406217][-4.2367153 -4.2479916 -4.276516 -4.305624 -4.3118873 -4.2989836 -4.2771196 -4.2480288 -4.218133 -4.1911969 -4.1695037 -4.1543827 -4.1406827 -4.132278 -4.128315]]...]
INFO - root - 2017-12-07 11:56:41.044336: step 10110, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 85h:39m:03s remains)
INFO - root - 2017-12-07 11:56:50.592733: step 10120, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 84h:20m:51s remains)
INFO - root - 2017-12-07 11:57:00.276522: step 10130, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 90h:41m:40s remains)
INFO - root - 2017-12-07 11:57:09.901407: step 10140, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 85h:39m:04s remains)
INFO - root - 2017-12-07 11:57:19.511682: step 10150, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 82h:27m:53s remains)
INFO - root - 2017-12-07 11:57:28.958424: step 10160, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 85h:24m:19s remains)
INFO - root - 2017-12-07 11:57:38.542980: step 10170, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.019 sec/batch; 91h:15m:24s remains)
INFO - root - 2017-12-07 11:57:48.374664: step 10180, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 88h:13m:31s remains)
INFO - root - 2017-12-07 11:57:57.894952: step 10190, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 86h:19m:25s remains)
INFO - root - 2017-12-07 11:58:07.334226: step 10200, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.925 sec/batch; 82h:46m:16s remains)
2017-12-07 11:58:08.256098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3084145 -4.3116169 -4.3173594 -4.3262095 -4.3311992 -4.3310618 -4.3287587 -4.3246818 -4.3221359 -4.3227582 -4.3283682 -4.3346534 -4.3380094 -4.3352861 -4.329215][-4.3225808 -4.3256469 -4.3304834 -4.3373256 -4.337019 -4.3279247 -4.3161049 -4.3066845 -4.3031826 -4.3052416 -4.3128853 -4.3205123 -4.3254094 -4.322886 -4.3160281][-4.3263459 -4.328958 -4.3325315 -4.335391 -4.3282385 -4.3093681 -4.2874112 -4.2735353 -4.2736306 -4.282733 -4.2950726 -4.3033948 -4.3083415 -4.304606 -4.2964768][-4.3227429 -4.3268628 -4.328546 -4.3255682 -4.3099089 -4.2804351 -4.2473106 -4.2284513 -4.234374 -4.2553945 -4.2743449 -4.283123 -4.2850032 -4.2783108 -4.2703338][-4.3180223 -4.3208981 -4.3159404 -4.3030453 -4.2760797 -4.2345614 -4.18846 -4.1669049 -4.1851282 -4.22355 -4.25356 -4.2668033 -4.26615 -4.256568 -4.2476978][-4.30847 -4.30566 -4.2896466 -4.2624307 -4.2201548 -4.1623149 -4.101779 -4.0840073 -4.1239614 -4.1867461 -4.2357411 -4.2593679 -4.2595472 -4.2478857 -4.2354226][-4.2956977 -4.2846494 -4.2556596 -4.2115278 -4.153419 -4.0818748 -4.012804 -4.0102 -4.0783081 -4.1643448 -4.2290711 -4.2608418 -4.2609091 -4.2452602 -4.2270885][-4.282073 -4.2626805 -4.2244663 -4.170342 -4.1082582 -4.0411472 -3.9834545 -4.0024843 -4.0864649 -4.1770129 -4.2419109 -4.269495 -4.2631512 -4.2429018 -4.22367][-4.2804117 -4.255898 -4.2146363 -4.16299 -4.1152525 -4.07466 -4.0478945 -4.075933 -4.1455927 -4.2170534 -4.2655973 -4.2789559 -4.2611809 -4.2396779 -4.2259555][-4.2957106 -4.2711654 -4.2352676 -4.195241 -4.1664958 -4.1518888 -4.146831 -4.1700411 -4.2137012 -4.2572279 -4.2840729 -4.2838879 -4.2601123 -4.24201 -4.2347927][-4.3150611 -4.29524 -4.269681 -4.2438908 -4.2281075 -4.2270861 -4.2302308 -4.2438726 -4.2673235 -4.2910719 -4.3024564 -4.295557 -4.2757616 -4.2647138 -4.2599378][-4.3266573 -4.31383 -4.2987466 -4.2848492 -4.276475 -4.27815 -4.2816358 -4.2882495 -4.3022256 -4.3173447 -4.3228035 -4.3176603 -4.308012 -4.3040538 -4.2947679][-4.3294215 -4.3208 -4.3104997 -4.3017426 -4.2964969 -4.2974 -4.3010178 -4.3058152 -4.3167973 -4.3287168 -4.3367491 -4.3394184 -4.3385906 -4.3392749 -4.326932][-4.320827 -4.3125963 -4.3015451 -4.2927051 -4.2885556 -4.29039 -4.2961435 -4.3017149 -4.3108244 -4.3211331 -4.3334684 -4.3426919 -4.3469367 -4.3495688 -4.34042][-4.2972021 -4.2836375 -4.26577 -4.2504115 -4.2444954 -4.2491379 -4.2598681 -4.2708511 -4.282197 -4.2946124 -4.3127704 -4.3288746 -4.3358936 -4.3397479 -4.3361106]]...]
INFO - root - 2017-12-07 11:58:17.827418: step 10210, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 88h:02m:46s remains)
INFO - root - 2017-12-07 11:58:27.592431: step 10220, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 87h:45m:09s remains)
INFO - root - 2017-12-07 11:58:37.406108: step 10230, loss = 2.12, batch loss = 2.06 (7.5 examples/sec; 1.064 sec/batch; 95h:16m:41s remains)
INFO - root - 2017-12-07 11:58:47.013830: step 10240, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 84h:50m:05s remains)
INFO - root - 2017-12-07 11:58:56.412444: step 10250, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 85h:12m:15s remains)
INFO - root - 2017-12-07 11:59:06.076286: step 10260, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 86h:33m:50s remains)
INFO - root - 2017-12-07 11:59:15.615282: step 10270, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 81h:53m:56s remains)
INFO - root - 2017-12-07 11:59:25.142431: step 10280, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 82h:16m:59s remains)
INFO - root - 2017-12-07 11:59:34.721415: step 10290, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 87h:35m:24s remains)
INFO - root - 2017-12-07 11:59:44.347403: step 10300, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 86h:26m:13s remains)
2017-12-07 11:59:45.316721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2726746 -4.27613 -4.2794623 -4.280767 -4.2802444 -4.2799764 -4.2801638 -4.2786436 -4.2714005 -4.2597756 -4.2495852 -4.2452097 -4.2474384 -4.2629547 -4.2853251][-4.2624884 -4.2658257 -4.2706046 -4.2740769 -4.2735968 -4.2718349 -4.2721381 -4.272068 -4.2673092 -4.2582507 -4.2526121 -4.2537751 -4.2591567 -4.2731361 -4.290597][-4.2612734 -4.2659721 -4.2742958 -4.2802963 -4.2786937 -4.2717896 -4.2695889 -4.2696137 -4.2700973 -4.2671723 -4.2663121 -4.2711005 -4.2760825 -4.2852254 -4.29577][-4.2494879 -4.2563038 -4.2692513 -4.2764096 -4.2708921 -4.2588053 -4.2556195 -4.2614393 -4.2722378 -4.2759352 -4.2767363 -4.2799139 -4.2794814 -4.2833352 -4.2922544][-4.2089119 -4.2211342 -4.2372522 -4.2429357 -4.2302289 -4.2097549 -4.2042704 -4.220829 -4.2505927 -4.2691846 -4.2731223 -4.2700443 -4.2605448 -4.2611284 -4.2745028][-4.1611223 -4.1853724 -4.2059722 -4.2069321 -4.177 -4.1349154 -4.1226954 -4.1553555 -4.2131119 -4.2543755 -4.2646055 -4.25534 -4.2369018 -4.2353716 -4.2551022][-4.1213241 -4.1636996 -4.1892214 -4.17752 -4.1177249 -4.0413404 -4.0111208 -4.0579343 -4.1472826 -4.2144866 -4.2374043 -4.2317142 -4.2148385 -4.2176886 -4.2456288][-4.1039457 -4.1654086 -4.1994724 -4.1824937 -4.1041021 -4.0015421 -3.9494624 -3.9950004 -4.0940614 -4.1728725 -4.2101488 -4.2173996 -4.2143216 -4.2263494 -4.258862][-4.1079226 -4.1801009 -4.2229643 -4.2163291 -4.1523 -4.0672722 -4.0200534 -4.043417 -4.1098394 -4.1698575 -4.2088313 -4.2280178 -4.2423739 -4.2640042 -4.294466][-4.1354604 -4.2041354 -4.2487874 -4.2487016 -4.2027793 -4.1474004 -4.1182637 -4.1270657 -4.1617579 -4.1998496 -4.2330813 -4.2545576 -4.2755914 -4.3007021 -4.3270154][-4.1798811 -4.2385945 -4.2802587 -4.2843695 -4.2517457 -4.2147779 -4.1974058 -4.1972342 -4.20634 -4.2231674 -4.2485542 -4.2700357 -4.2928977 -4.3199186 -4.344697][-4.2044268 -4.2575207 -4.2995858 -4.3106117 -4.2925863 -4.2700982 -4.2588973 -4.2510643 -4.2408185 -4.2364807 -4.2486753 -4.2654467 -4.2862329 -4.3132005 -4.3405771][-4.2043815 -4.2572355 -4.3041911 -4.3243318 -4.3167715 -4.3062782 -4.3009067 -4.2920861 -4.2747488 -4.2589893 -4.2595367 -4.2680731 -4.283124 -4.3058205 -4.3317285][-4.1969342 -4.2498178 -4.3059354 -4.3383589 -4.3378353 -4.3323846 -4.3287287 -4.3215766 -4.3049746 -4.2884669 -4.2815514 -4.2807078 -4.2878075 -4.3056374 -4.329][-4.186378 -4.2364779 -4.2966981 -4.3388844 -4.3460288 -4.3424358 -4.3374162 -4.3296485 -4.314352 -4.2995496 -4.2884731 -4.2810345 -4.2827992 -4.2986274 -4.3233747]]...]
INFO - root - 2017-12-07 11:59:54.957360: step 10310, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 86h:16m:28s remains)
INFO - root - 2017-12-07 12:00:04.697802: step 10320, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 86h:51m:31s remains)
INFO - root - 2017-12-07 12:00:14.323618: step 10330, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 83h:52m:18s remains)
INFO - root - 2017-12-07 12:00:23.967465: step 10340, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 85h:16m:20s remains)
INFO - root - 2017-12-07 12:00:33.437403: step 10350, loss = 2.11, batch loss = 2.06 (9.8 examples/sec; 0.819 sec/batch; 73h:16m:55s remains)
INFO - root - 2017-12-07 12:00:43.145927: step 10360, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 90h:32m:23s remains)
INFO - root - 2017-12-07 12:00:52.673014: step 10370, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 87h:57m:01s remains)
INFO - root - 2017-12-07 12:01:02.281152: step 10380, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 82h:07m:27s remains)
INFO - root - 2017-12-07 12:01:11.920504: step 10390, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 88h:07m:15s remains)
INFO - root - 2017-12-07 12:01:21.663367: step 10400, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 89h:44m:19s remains)
2017-12-07 12:01:22.694267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.358479 -4.3595119 -4.353447 -4.3366432 -4.3102431 -4.2863789 -4.2757506 -4.2843652 -4.3096948 -4.3347683 -4.3506179 -4.3584337 -4.3605566 -4.3594689 -4.3581047][-4.3615766 -4.3610153 -4.3504577 -4.3270078 -4.2944264 -4.2665429 -4.2558084 -4.2682838 -4.2987103 -4.3276877 -4.3473573 -4.3586292 -4.3616085 -4.36046 -4.3593116][-4.362731 -4.3601484 -4.3446622 -4.3135891 -4.2728248 -4.2402892 -4.2306228 -4.2469573 -4.2813363 -4.3142529 -4.3375459 -4.3529096 -4.3589783 -4.359375 -4.3594656][-4.3554788 -4.3502145 -4.32662 -4.2832408 -4.2310357 -4.19584 -4.1913085 -4.2141995 -4.2546663 -4.2948027 -4.3242741 -4.3446331 -4.3554807 -4.3587651 -4.3600035][-4.331737 -4.3203888 -4.2821412 -4.22171 -4.1597805 -4.126502 -4.1316161 -4.1613789 -4.2065854 -4.2562351 -4.2959843 -4.3247185 -4.3441024 -4.3538871 -4.358624][-4.2852221 -4.2622714 -4.2066183 -4.1327033 -4.0708275 -4.048295 -4.0644903 -4.0969572 -4.1395731 -4.1938443 -4.2469044 -4.2899508 -4.3213043 -4.3403454 -4.3511844][-4.2289348 -4.1916189 -4.1207523 -4.039969 -3.9884691 -3.9817696 -4.0080037 -4.0388746 -4.0697923 -4.120842 -4.1858897 -4.2446508 -4.290555 -4.32168 -4.3401713][-4.189908 -4.1428781 -4.0660076 -3.9887903 -3.9510729 -3.9562325 -3.9855089 -4.0085588 -4.0241046 -4.0666018 -4.1363554 -4.2043705 -4.2629309 -4.3060317 -4.3321218][-4.1764207 -4.1285 -4.0600076 -3.9978673 -3.9730449 -3.9818087 -4.00561 -4.0179181 -4.0210867 -4.0551057 -4.1215258 -4.1895919 -4.2530856 -4.3021746 -4.3321266][-4.1824512 -4.1404209 -4.0883093 -4.0440388 -4.03037 -4.0387549 -4.0536385 -4.0548835 -4.0497093 -4.0758619 -4.1328249 -4.1948762 -4.256803 -4.306757 -4.3368082][-4.2083521 -4.1752005 -4.1397891 -4.1131454 -4.1082535 -4.1171212 -4.1269903 -4.122447 -4.1120067 -4.1278 -4.1683192 -4.2177444 -4.269742 -4.3140984 -4.3410535][-4.2450747 -4.2242932 -4.2048721 -4.1934996 -4.1961136 -4.2058811 -4.2120504 -4.2054715 -4.1943893 -4.2002792 -4.2246294 -4.2595983 -4.2973137 -4.3292575 -4.3468089][-4.277102 -4.2650509 -4.2559242 -4.254992 -4.2636085 -4.2744584 -4.2797823 -4.27558 -4.2669215 -4.2663813 -4.2787056 -4.3012147 -4.3256025 -4.3442607 -4.3513236][-4.3040276 -4.2974439 -4.2935867 -4.2964544 -4.3047762 -4.3130984 -4.3169122 -4.3142157 -4.3078322 -4.3046236 -4.3100414 -4.3240981 -4.339221 -4.3488455 -4.349225][-4.3256307 -4.3223348 -4.3201356 -4.3214388 -4.325264 -4.3292766 -4.3313351 -4.3295894 -4.3254051 -4.3228393 -4.324645 -4.3320909 -4.3397741 -4.3433795 -4.3414416]]...]
INFO - root - 2017-12-07 12:01:32.397417: step 10410, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 85h:50m:35s remains)
INFO - root - 2017-12-07 12:01:42.168399: step 10420, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 86h:13m:12s remains)
INFO - root - 2017-12-07 12:01:51.894032: step 10430, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 87h:24m:46s remains)
INFO - root - 2017-12-07 12:02:01.511614: step 10440, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 83h:26m:03s remains)
INFO - root - 2017-12-07 12:02:11.080191: step 10450, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 86h:15m:05s remains)
INFO - root - 2017-12-07 12:02:20.663859: step 10460, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 85h:37m:33s remains)
INFO - root - 2017-12-07 12:02:30.225959: step 10470, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 89h:02m:26s remains)
INFO - root - 2017-12-07 12:02:39.854188: step 10480, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 90h:18m:47s remains)
INFO - root - 2017-12-07 12:02:49.660293: step 10490, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.006 sec/batch; 90h:01m:08s remains)
INFO - root - 2017-12-07 12:02:59.327770: step 10500, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 84h:09m:44s remains)
2017-12-07 12:03:00.318707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2957554 -4.2873878 -4.2832704 -4.2801504 -4.2842221 -4.2947259 -4.3058829 -4.3142681 -4.3135424 -4.3055692 -4.2986693 -4.2953205 -4.2886677 -4.2706385 -4.2371535][-4.2647877 -4.2569904 -4.256238 -4.2565494 -4.2651267 -4.2798095 -4.2931976 -4.3015628 -4.2979503 -4.2882643 -4.2828865 -4.2820477 -4.2733164 -4.2505474 -4.2144279][-4.2295494 -4.2276287 -4.2322111 -4.2354589 -4.2455363 -4.2609496 -4.2736645 -4.2805653 -4.2753911 -4.2665462 -4.2628031 -4.26365 -4.2541904 -4.2290421 -4.1941905][-4.1929955 -4.2043219 -4.2178035 -4.2246442 -4.2366352 -4.2509379 -4.2610283 -4.263257 -4.25368 -4.2432404 -4.2401772 -4.240355 -4.2307577 -4.2068777 -4.1792464][-4.155158 -4.18109 -4.2039828 -4.2169738 -4.2335916 -4.249186 -4.2552986 -4.2454367 -4.2247505 -4.2109423 -4.2093067 -4.2091756 -4.2015634 -4.1844621 -4.172019][-4.13001 -4.1658988 -4.1929321 -4.2107491 -4.2307062 -4.2451453 -4.2404613 -4.2103338 -4.1745963 -4.1624041 -4.1688275 -4.1748066 -4.1752353 -4.1721673 -4.1793184][-4.1397681 -4.1699543 -4.1897793 -4.2028794 -4.2182803 -4.2256427 -4.2051115 -4.15265 -4.1069207 -4.1064963 -4.1298466 -4.1500225 -4.1656971 -4.1805825 -4.2033453][-4.178545 -4.1907196 -4.19374 -4.1940379 -4.1962328 -4.1928205 -4.157856 -4.0912271 -4.0511379 -4.0743489 -4.1177454 -4.1533413 -4.1843548 -4.2102389 -4.2377434][-4.2140179 -4.2087064 -4.1984673 -4.1856656 -4.1756549 -4.1643648 -4.1245017 -4.0638289 -4.0474453 -4.0967059 -4.1517429 -4.1924763 -4.2253666 -4.2494106 -4.2720594][-4.2229629 -4.2097683 -4.1975465 -4.1813011 -4.168726 -4.1614103 -4.1338387 -4.0949779 -4.1029277 -4.1585708 -4.2077889 -4.2379808 -4.2599773 -4.2753358 -4.290751][-4.2132854 -4.2003984 -4.19351 -4.1835089 -4.1800804 -4.1860085 -4.1776533 -4.1601086 -4.1737676 -4.2140317 -4.244627 -4.2610655 -4.2720995 -4.2816262 -4.2919631][-4.2028561 -4.1955972 -4.194777 -4.1951122 -4.2027535 -4.2200322 -4.2224665 -4.214241 -4.2184238 -4.2365575 -4.2500353 -4.2561841 -4.2625284 -4.2702465 -4.2768788][-4.204349 -4.2022738 -4.2075195 -4.213798 -4.2248716 -4.2417974 -4.2470212 -4.2404394 -4.233686 -4.2343493 -4.239233 -4.2420311 -4.2445512 -4.2490778 -4.2505193][-4.2114277 -4.2104812 -4.2174587 -4.2238445 -4.2328191 -4.2432957 -4.2469897 -4.2423172 -4.2318487 -4.2245531 -4.2269478 -4.2282786 -4.22622 -4.2262082 -4.2232623][-4.213727 -4.2124429 -4.2199922 -4.2250466 -4.2287621 -4.2299933 -4.2310781 -4.2274613 -4.2173419 -4.2110872 -4.2163348 -4.21876 -4.2147779 -4.2111559 -4.2046881]]...]
INFO - root - 2017-12-07 12:03:10.054530: step 10510, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.970 sec/batch; 86h:45m:23s remains)
INFO - root - 2017-12-07 12:03:19.748899: step 10520, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 86h:15m:22s remains)
INFO - root - 2017-12-07 12:03:29.400422: step 10530, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.023 sec/batch; 91h:30m:52s remains)
INFO - root - 2017-12-07 12:03:38.957584: step 10540, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 73h:22m:10s remains)
INFO - root - 2017-12-07 12:03:48.528130: step 10550, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 83h:50m:05s remains)
INFO - root - 2017-12-07 12:03:58.062712: step 10560, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 83h:41m:32s remains)
INFO - root - 2017-12-07 12:04:07.913568: step 10570, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 90h:50m:28s remains)
INFO - root - 2017-12-07 12:04:17.595760: step 10580, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 87h:04m:56s remains)
INFO - root - 2017-12-07 12:04:27.208774: step 10590, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 83h:53m:41s remains)
INFO - root - 2017-12-07 12:04:36.656267: step 10600, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.960 sec/batch; 85h:50m:53s remains)
2017-12-07 12:04:37.724521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1815844 -4.1169782 -4.0625882 -4.0424333 -4.08969 -4.131999 -4.1383543 -4.125701 -4.1102948 -4.1171231 -4.1577897 -4.1838737 -4.1990266 -4.2133436 -4.2179446][-4.1900291 -4.1398096 -4.1028328 -4.0922194 -4.1249266 -4.1442485 -4.1351261 -4.1171932 -4.1020007 -4.1136613 -4.1634922 -4.2021117 -4.2237577 -4.2365103 -4.2446122][-4.217577 -4.1861053 -4.1690412 -4.1677089 -4.1852474 -4.1864781 -4.1719089 -4.1566725 -4.1432891 -4.1544533 -4.1980653 -4.233736 -4.2503815 -4.2569137 -4.2624497][-4.2467871 -4.2242718 -4.2172155 -4.2211089 -4.2299161 -4.2257671 -4.2207389 -4.2232504 -4.2222295 -4.2262807 -4.2430339 -4.2577934 -4.2610183 -4.2621474 -4.2662277][-4.2662053 -4.2429004 -4.2322097 -4.2328944 -4.2344446 -4.2263112 -4.2275982 -4.2456875 -4.2620292 -4.2671251 -4.2608871 -4.2518616 -4.2435651 -4.2415981 -4.251369][-4.2703433 -4.2358756 -4.2122235 -4.2018881 -4.1904593 -4.1714516 -4.1680694 -4.1911278 -4.225172 -4.2401104 -4.2311254 -4.218832 -4.2133155 -4.2198138 -4.2427278][-4.2511411 -4.2050667 -4.1693788 -4.1415863 -4.111762 -4.0813627 -4.0682321 -4.0880032 -4.134738 -4.1578164 -4.1544733 -4.1558919 -4.1711664 -4.2016358 -4.2448525][-4.2160683 -4.1606412 -4.1203709 -4.0864058 -4.050025 -4.0127707 -3.9845991 -3.9910877 -4.0416613 -4.068964 -4.068593 -4.09054 -4.1401272 -4.1979632 -4.2547469][-4.1959939 -4.1364961 -4.0939555 -4.059536 -4.026258 -3.9960158 -3.9660552 -3.9584982 -3.9952481 -4.0093756 -4.0070834 -4.0557394 -4.1425734 -4.2178068 -4.271884][-4.200047 -4.144742 -4.1069345 -4.0769773 -4.05439 -4.0390577 -4.0247107 -4.0185614 -4.0353246 -4.028492 -4.0221233 -4.0825119 -4.1855841 -4.25895 -4.2919908][-4.2154779 -4.1725764 -4.1492534 -4.1329117 -4.12566 -4.126338 -4.1265945 -4.1238947 -4.1308656 -4.1243711 -4.1193 -4.1650095 -4.2447624 -4.2935905 -4.2955174][-4.2312236 -4.1984606 -4.1902018 -4.1908188 -4.1959634 -4.2018423 -4.2039843 -4.1999068 -4.205174 -4.2095585 -4.216578 -4.2452326 -4.2886953 -4.3066311 -4.2871618][-4.24464 -4.2201843 -4.2198248 -4.2298574 -4.2413821 -4.2472658 -4.2480297 -4.2429476 -4.2470012 -4.2571664 -4.2735715 -4.2941065 -4.3111663 -4.3076425 -4.2765112][-4.2511454 -4.2306781 -4.2341084 -4.2496037 -4.2635655 -4.2677617 -4.2693534 -4.2685642 -4.2738333 -4.2843904 -4.29837 -4.3134217 -4.3173542 -4.3051176 -4.2728667][-4.2481976 -4.2279449 -4.2325387 -4.24944 -4.2657371 -4.2689123 -4.270586 -4.2768531 -4.2871675 -4.2979174 -4.3077269 -4.3184109 -4.3184223 -4.3039737 -4.2751093]]...]
INFO - root - 2017-12-07 12:04:47.491119: step 10610, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 88h:37m:28s remains)
INFO - root - 2017-12-07 12:04:57.230991: step 10620, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 84h:02m:34s remains)
INFO - root - 2017-12-07 12:05:07.043858: step 10630, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.991 sec/batch; 88h:35m:59s remains)
INFO - root - 2017-12-07 12:05:16.667550: step 10640, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.997 sec/batch; 89h:09m:19s remains)
INFO - root - 2017-12-07 12:05:26.379544: step 10650, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 83h:44m:08s remains)
INFO - root - 2017-12-07 12:05:35.914967: step 10660, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 87h:28m:10s remains)
INFO - root - 2017-12-07 12:05:45.481356: step 10670, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 82h:31m:57s remains)
INFO - root - 2017-12-07 12:05:55.288513: step 10680, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 87h:22m:10s remains)
INFO - root - 2017-12-07 12:06:04.704147: step 10690, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 88h:47m:54s remains)
INFO - root - 2017-12-07 12:06:14.526007: step 10700, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 90h:10m:17s remains)
2017-12-07 12:06:15.431517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17829 -4.1931014 -4.2167091 -4.241468 -4.2622089 -4.2752695 -4.2836618 -4.2872534 -4.2845526 -4.2790694 -4.2720847 -4.26583 -4.2619376 -4.2587128 -4.2552848][-4.2017465 -4.21694 -4.2385683 -4.2600389 -4.2760539 -4.2846494 -4.2892156 -4.2902255 -4.28666 -4.280756 -4.2725439 -4.2645149 -4.2594013 -4.2564826 -4.2546368][-4.2317309 -4.2453465 -4.2617359 -4.2757626 -4.2837734 -4.2864375 -4.2872686 -4.2863221 -4.2822866 -4.2764468 -4.267828 -4.2592216 -4.2542176 -4.2525997 -4.2527523][-4.25485 -4.263061 -4.27079 -4.2750826 -4.27481 -4.2729287 -4.2715778 -4.2702756 -4.2681661 -4.2650938 -4.2587748 -4.2517571 -4.2480268 -4.2480631 -4.2501454][-4.2690153 -4.2689295 -4.2663045 -4.2603378 -4.2523904 -4.2464175 -4.2435846 -4.2440529 -4.2467442 -4.2498665 -4.2495522 -4.2470393 -4.2464595 -4.2489004 -4.252707][-4.2781267 -4.2717867 -4.2617507 -4.2475567 -4.2329273 -4.2230959 -4.2194095 -4.22244 -4.2304049 -4.23946 -4.2454238 -4.2482285 -4.2514319 -4.25623 -4.2614441][-4.28462 -4.276402 -4.2636175 -4.2465978 -4.2299252 -4.2190151 -4.21578 -4.2208695 -4.2313657 -4.2418776 -4.2490182 -4.2531676 -4.2574229 -4.2631531 -4.2697644][-4.2885261 -4.281693 -4.2699327 -4.2539878 -4.2386293 -4.2287931 -4.2263985 -4.23162 -4.2404952 -4.2478886 -4.2516823 -4.253551 -4.2567205 -4.26181 -4.2689676][-4.2899823 -4.2842846 -4.2741733 -4.2606173 -4.2478981 -4.24049 -4.2396317 -4.2440033 -4.2495074 -4.2522888 -4.2520084 -4.2508087 -4.2517247 -4.2547626 -4.2604928][-4.290195 -4.2852082 -4.277029 -4.2666183 -4.2572575 -4.2522159 -4.2523146 -4.2551913 -4.2575617 -4.257091 -4.2543063 -4.25112 -4.2499738 -4.250917 -4.2544751][-4.2913084 -4.2871871 -4.2814136 -4.2744417 -4.2684059 -4.2650008 -4.2651119 -4.2666736 -4.2670927 -4.264915 -4.2611103 -4.257174 -4.255043 -4.2548208 -4.2570052][-4.2927246 -4.2894092 -4.2855678 -4.2813663 -4.27777 -4.2755589 -4.2755437 -4.27634 -4.2760577 -4.2736435 -4.2701936 -4.2665567 -4.264101 -4.2632947 -4.2645631][-4.2934489 -4.2904487 -4.2876935 -4.2852473 -4.2832541 -4.2820096 -4.2820678 -4.2825136 -4.2823205 -4.2807221 -4.2785068 -4.2760434 -4.2739425 -4.2728076 -4.2730541][-4.2935276 -4.2907896 -4.2886009 -4.2870593 -4.2859359 -4.285233 -4.2851996 -4.2851739 -4.2848434 -4.2839022 -4.2828732 -4.2817068 -4.2802076 -4.2789927 -4.2783794][-4.2934222 -4.2913766 -4.2900848 -4.2894635 -4.2891593 -4.2889051 -4.2886834 -4.2881279 -4.287169 -4.285996 -4.2850313 -4.2839556 -4.2824507 -4.2808 -4.279614]]...]
INFO - root - 2017-12-07 12:06:25.104020: step 10710, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 88h:07m:44s remains)
INFO - root - 2017-12-07 12:06:34.873516: step 10720, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.005 sec/batch; 89h:50m:55s remains)
INFO - root - 2017-12-07 12:06:44.299313: step 10730, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 84h:56m:42s remains)
INFO - root - 2017-12-07 12:06:54.021056: step 10740, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 88h:17m:01s remains)
INFO - root - 2017-12-07 12:07:03.861525: step 10750, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 88h:46m:49s remains)
INFO - root - 2017-12-07 12:07:13.617768: step 10760, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.028 sec/batch; 91h:53m:15s remains)
INFO - root - 2017-12-07 12:07:23.287875: step 10770, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 83h:47m:02s remains)
INFO - root - 2017-12-07 12:07:32.940476: step 10780, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 85h:51m:26s remains)
INFO - root - 2017-12-07 12:07:42.505575: step 10790, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 88h:57m:07s remains)
INFO - root - 2017-12-07 12:07:52.203195: step 10800, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 84h:11m:22s remains)
2017-12-07 12:07:53.248949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3296986 -4.32554 -4.3221397 -4.3189979 -4.3141365 -4.306087 -4.2968545 -4.2869081 -4.283462 -4.2860966 -4.2874851 -4.2835016 -4.2715616 -4.2546406 -4.2262497][-4.3250217 -4.318749 -4.3129668 -4.30431 -4.2931128 -4.2808623 -4.2693782 -4.2600107 -4.2608089 -4.2701497 -4.2768784 -4.2739062 -4.2597947 -4.2346792 -4.1926308][-4.3184748 -4.3097849 -4.300384 -4.2884235 -4.2759733 -4.2655425 -4.2550545 -4.24635 -4.248888 -4.2618656 -4.2696648 -4.2653761 -4.2509055 -4.2231131 -4.1786084][-4.3107576 -4.3004322 -4.2889204 -4.276361 -4.2660666 -4.2537484 -4.2335348 -4.2181025 -4.218987 -4.2334256 -4.2422543 -4.2435613 -4.2411423 -4.226696 -4.198442][-4.3016109 -4.2937016 -4.2804656 -4.2620368 -4.243433 -4.2152815 -4.170958 -4.1425667 -4.1479673 -4.1692905 -4.1865082 -4.2019887 -4.218833 -4.2246294 -4.2165904][-4.2989278 -4.2948561 -4.2750974 -4.2453823 -4.2128129 -4.1593966 -4.0797143 -4.0336471 -4.0490742 -4.0887384 -4.1193738 -4.147594 -4.1783485 -4.2004037 -4.2089205][-4.3061876 -4.3027692 -4.2739158 -4.2273054 -4.1742144 -4.0942965 -3.9801087 -3.9101379 -3.9466884 -4.0171576 -4.0654583 -4.1018257 -4.1430626 -4.1807785 -4.2061195][-4.305171 -4.3019643 -4.2678652 -4.2090793 -4.1410537 -4.0574951 -3.9519691 -3.8934789 -3.9543633 -4.036746 -4.0789661 -4.1073937 -4.1469717 -4.1906543 -4.2264462][-4.2893744 -4.2895994 -4.2605128 -4.2091537 -4.149848 -4.0936894 -4.0414472 -4.02143 -4.0648046 -4.1180983 -4.1362882 -4.1515617 -4.1840296 -4.2237744 -4.2567983][-4.2559218 -4.2635708 -4.2515907 -4.221314 -4.1839237 -4.1615968 -4.1561127 -4.1612625 -4.1824489 -4.1997457 -4.1953673 -4.2018409 -4.2256193 -4.2527032 -4.2732553][-4.2035942 -4.220541 -4.2320251 -4.2272229 -4.2140079 -4.2142763 -4.2305346 -4.245708 -4.2525039 -4.2464685 -4.229579 -4.2289605 -4.2442288 -4.2614551 -4.2731042][-4.159606 -4.1909943 -4.2179947 -4.2250428 -4.2237997 -4.2292027 -4.2459011 -4.2605767 -4.2626061 -4.2535162 -4.2398567 -4.236692 -4.245542 -4.2597284 -4.270452][-4.1482096 -4.1863956 -4.2099724 -4.2149639 -4.2139931 -4.2170839 -4.2304239 -4.2457466 -4.24977 -4.2452908 -4.24068 -4.2410569 -4.2491322 -4.2657366 -4.2783556][-4.1617208 -4.1885467 -4.2025223 -4.2017512 -4.1957688 -4.1942534 -4.2070546 -4.2250361 -4.2355223 -4.240675 -4.2468381 -4.253407 -4.2659569 -4.2883267 -4.3041062][-4.17822 -4.1892538 -4.1950183 -4.1939507 -4.1900125 -4.1909881 -4.2082696 -4.2324543 -4.251163 -4.2664719 -4.2779741 -4.2861853 -4.2972951 -4.3153305 -4.3265924]]...]
INFO - root - 2017-12-07 12:08:02.852824: step 10810, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.947 sec/batch; 84h:35m:37s remains)
INFO - root - 2017-12-07 12:08:12.444292: step 10820, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.004 sec/batch; 89h:41m:08s remains)
INFO - root - 2017-12-07 12:08:22.171105: step 10830, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.006 sec/batch; 89h:52m:09s remains)
INFO - root - 2017-12-07 12:08:31.786987: step 10840, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 84h:48m:17s remains)
INFO - root - 2017-12-07 12:08:41.472010: step 10850, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.971 sec/batch; 86h:46m:00s remains)
INFO - root - 2017-12-07 12:08:51.010401: step 10860, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.994 sec/batch; 88h:47m:37s remains)
INFO - root - 2017-12-07 12:09:00.701376: step 10870, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 82h:38m:06s remains)
INFO - root - 2017-12-07 12:09:10.476450: step 10880, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 86h:24m:43s remains)
INFO - root - 2017-12-07 12:09:20.155885: step 10890, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 88h:17m:47s remains)
INFO - root - 2017-12-07 12:09:29.906586: step 10900, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 85h:27m:40s remains)
2017-12-07 12:09:30.879244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3508673 -4.3469014 -4.339644 -4.33332 -4.3274784 -4.321589 -4.3186874 -4.3194828 -4.3227634 -4.3251915 -4.3279481 -4.334322 -4.3405862 -4.3448882 -4.3475518][-4.3494043 -4.3410311 -4.3294611 -4.3218579 -4.3146238 -4.3038735 -4.2935839 -4.2916675 -4.2982836 -4.3061609 -4.313 -4.3221016 -4.3312449 -4.3367286 -4.3411078][-4.3427486 -4.3310184 -4.3174973 -4.3085938 -4.2984023 -4.28163 -4.262969 -4.2527657 -4.2589478 -4.2739873 -4.2880239 -4.3017073 -4.3152194 -4.3274336 -4.3344693][-4.3360305 -4.3226161 -4.3080173 -4.2962 -4.2800789 -4.2538352 -4.2214437 -4.19514 -4.1992188 -4.2268047 -4.2514334 -4.2727308 -4.2929344 -4.3117676 -4.3210392][-4.327095 -4.3124847 -4.2970853 -4.2826157 -4.2628579 -4.2283311 -4.1784439 -4.1263885 -4.1248131 -4.1739268 -4.2145824 -4.2468977 -4.2718368 -4.2891526 -4.2982354][-4.3152976 -4.2970891 -4.2756648 -4.2543736 -4.2255816 -4.1778073 -4.1047907 -4.0173392 -4.0095019 -4.0939155 -4.1662769 -4.2145691 -4.2470546 -4.26595 -4.2742519][-4.302475 -4.2771473 -4.2417221 -4.2025661 -4.1546497 -4.0861387 -3.9840217 -3.8590527 -3.8396115 -3.9657226 -4.0859256 -4.1629167 -4.2134161 -4.244422 -4.2588029][-4.2896123 -4.2558985 -4.2074237 -4.1493459 -4.079248 -3.9892125 -3.8662014 -3.719748 -3.6868653 -3.83688 -3.9983797 -4.1104293 -4.1843128 -4.2321038 -4.2557721][-4.28012 -4.2414613 -4.1842613 -4.1116891 -4.0281029 -3.9331853 -3.8278151 -3.7133265 -3.6909351 -3.8242989 -3.9839833 -4.1049981 -4.1843357 -4.2368593 -4.2620068][-4.2734284 -4.2324381 -4.1715484 -4.0913496 -4.0054083 -3.9212391 -3.8485026 -3.782197 -3.7841611 -3.8857841 -4.0169492 -4.1267076 -4.1998439 -4.2474318 -4.268733][-4.2720284 -4.2306538 -4.1689835 -4.0882015 -4.0059037 -3.9319179 -3.8735564 -3.8370662 -3.8598623 -3.94171 -4.0487938 -4.1464858 -4.2147 -4.2585359 -4.278532][-4.2804847 -4.2414351 -4.1840777 -4.1098161 -4.0370603 -3.9759009 -3.9262242 -3.9102178 -3.9468899 -4.0116897 -4.0964932 -4.1787353 -4.2399988 -4.2796535 -4.2974529][-4.3003097 -4.2695842 -4.2254734 -4.1712456 -4.1208625 -4.0797129 -4.0477314 -4.0422812 -4.0755591 -4.119935 -4.176249 -4.232058 -4.2770476 -4.3074517 -4.3202095][-4.3177056 -4.2989025 -4.2731562 -4.241704 -4.2155576 -4.198637 -4.1862545 -4.1860414 -4.2064161 -4.2303753 -4.2595429 -4.2882528 -4.3107395 -4.3287683 -4.3362656][-4.3279123 -4.3163319 -4.3026919 -4.2874784 -4.2775197 -4.27498 -4.2742414 -4.2768965 -4.2873964 -4.2983069 -4.3108487 -4.32204 -4.3302846 -4.3390102 -4.3431416]]...]
INFO - root - 2017-12-07 12:09:40.600995: step 10910, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 84h:12m:14s remains)
INFO - root - 2017-12-07 12:09:50.214526: step 10920, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 87h:08m:28s remains)
INFO - root - 2017-12-07 12:09:59.872234: step 10930, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 88h:31m:05s remains)
INFO - root - 2017-12-07 12:10:09.438783: step 10940, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 87h:03m:54s remains)
INFO - root - 2017-12-07 12:10:19.105142: step 10950, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 81h:51m:21s remains)
INFO - root - 2017-12-07 12:10:28.704744: step 10960, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 84h:17m:40s remains)
INFO - root - 2017-12-07 12:10:38.330569: step 10970, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 87h:21m:27s remains)
INFO - root - 2017-12-07 12:10:47.989000: step 10980, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 86h:48m:02s remains)
INFO - root - 2017-12-07 12:10:57.692547: step 10990, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 86h:20m:43s remains)
INFO - root - 2017-12-07 12:11:07.273059: step 11000, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 81h:45m:29s remains)
2017-12-07 12:11:08.268148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3458033 -4.3395958 -4.3323517 -4.3286672 -4.3316245 -4.337378 -4.3405061 -4.3411326 -4.3400741 -4.3376145 -4.3343167 -4.3322239 -4.3304167 -4.3316979 -4.336359][-4.3371325 -4.3269539 -4.319128 -4.3166142 -4.3214865 -4.328248 -4.3293791 -4.3263054 -4.3224053 -4.3178515 -4.3145876 -4.31488 -4.3153977 -4.3196607 -4.3291621][-4.3191309 -4.305088 -4.29893 -4.2985787 -4.302249 -4.3063655 -4.303905 -4.29889 -4.2933435 -4.2873793 -4.2817688 -4.2845173 -4.2925491 -4.3051724 -4.3226438][-4.2866764 -4.2648163 -4.2585912 -4.2604637 -4.2597384 -4.2579241 -4.2514868 -4.2481408 -4.247642 -4.2453465 -4.2372403 -4.238842 -4.2535348 -4.2771263 -4.3050485][-4.2470193 -4.2147131 -4.2054172 -4.2077742 -4.199688 -4.1835032 -4.1645112 -4.1694098 -4.1891994 -4.2013683 -4.1985378 -4.2010694 -4.2176943 -4.2462988 -4.2784834][-4.2136955 -4.1713657 -4.1542211 -4.1475215 -4.1199584 -4.0761642 -4.03266 -4.0540509 -4.1142907 -4.1565919 -4.17139 -4.1794724 -4.1948361 -4.2202249 -4.2486267][-4.1949878 -4.1475887 -4.1177478 -4.092907 -4.0381556 -3.956681 -3.8804715 -3.9232335 -4.0305028 -4.1134458 -4.1537781 -4.1697412 -4.1810527 -4.1948147 -4.2168903][-4.1912527 -4.1481819 -4.1140943 -4.0782833 -4.0117836 -3.9121063 -3.8245258 -3.8802154 -3.9969563 -4.09151 -4.1426921 -4.1631055 -4.1756043 -4.1840577 -4.2009892][-4.2002168 -4.1671734 -4.140729 -4.109611 -4.0563164 -3.9805455 -3.9164245 -3.9586997 -4.0360489 -4.1053691 -4.1483178 -4.1653767 -4.1793866 -4.1892705 -4.2042584][-4.2164764 -4.1946282 -4.1809816 -4.163415 -4.1313963 -4.0883484 -4.0506105 -4.0781236 -4.1177711 -4.1533465 -4.1754575 -4.1813178 -4.1911573 -4.2024293 -4.2155924][-4.2393861 -4.2264872 -4.2225776 -4.2184849 -4.2064576 -4.1884928 -4.1705389 -4.18794 -4.205955 -4.2179165 -4.220006 -4.2144222 -4.2199736 -4.2306728 -4.2399378][-4.2680488 -4.2606292 -4.2624836 -4.2650695 -4.2653775 -4.2633128 -4.2612762 -4.2752976 -4.2852311 -4.2843189 -4.2739005 -4.263164 -4.2620277 -4.2704391 -4.2769837][-4.292923 -4.2886815 -4.2929935 -4.2987671 -4.3022003 -4.3052869 -4.3102517 -4.3238888 -4.3318114 -4.3277731 -4.3144588 -4.302248 -4.2967796 -4.3008084 -4.3059444][-4.3068409 -4.3031573 -4.3071413 -4.3140893 -4.3190289 -4.322937 -4.3272948 -4.3366375 -4.3411694 -4.3374014 -4.3277855 -4.3193464 -4.3142123 -4.3169994 -4.3225055][-4.3193069 -4.3141 -4.3146281 -4.3185434 -4.322526 -4.32663 -4.3311763 -4.3370638 -4.3395724 -4.3383775 -4.3328366 -4.328145 -4.3247352 -4.32591 -4.3305612]]...]
INFO - root - 2017-12-07 12:11:17.939615: step 11010, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 85h:06m:39s remains)
INFO - root - 2017-12-07 12:11:27.705553: step 11020, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.888 sec/batch; 79h:17m:11s remains)
INFO - root - 2017-12-07 12:11:37.273762: step 11030, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 86h:12m:49s remains)
INFO - root - 2017-12-07 12:11:47.015846: step 11040, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 86h:51m:54s remains)
INFO - root - 2017-12-07 12:11:56.649774: step 11050, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 84h:56m:26s remains)
INFO - root - 2017-12-07 12:12:06.403715: step 11060, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 84h:18m:53s remains)
INFO - root - 2017-12-07 12:12:16.129449: step 11070, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 86h:14m:00s remains)
INFO - root - 2017-12-07 12:12:25.649233: step 11080, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 86h:01m:21s remains)
INFO - root - 2017-12-07 12:12:35.441011: step 11090, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.975 sec/batch; 87h:01m:28s remains)
INFO - root - 2017-12-07 12:12:45.217854: step 11100, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 81h:41m:52s remains)
2017-12-07 12:12:46.208883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2196007 -4.2208862 -4.2129107 -4.1931577 -4.1976762 -4.2183905 -4.2414603 -4.2564168 -4.254045 -4.2477822 -4.2444148 -4.2527757 -4.2616582 -4.2643628 -4.25914][-4.2187839 -4.2232561 -4.2074642 -4.1776085 -4.180829 -4.2001877 -4.2253528 -4.2424874 -4.2445207 -4.2492185 -4.25576 -4.2628679 -4.2646518 -4.260334 -4.2486868][-4.2253094 -4.22946 -4.20518 -4.1688237 -4.1634879 -4.1739073 -4.1951714 -4.2150388 -4.2246804 -4.2405415 -4.25224 -4.252574 -4.246345 -4.2402754 -4.2352533][-4.2320452 -4.2318106 -4.1992011 -4.1552582 -4.1370993 -4.1334796 -4.14896 -4.167582 -4.1808281 -4.2147894 -4.2376838 -4.2373095 -4.2320013 -4.2308726 -4.2339725][-4.2336626 -4.2319775 -4.1925139 -4.1403651 -4.1136594 -4.0975685 -4.1015468 -4.1091423 -4.1267352 -4.1807957 -4.21935 -4.2270794 -4.2226577 -4.2158165 -4.2155347][-4.2141056 -4.2189846 -4.1817737 -4.1293411 -4.0958343 -4.0585055 -4.0377445 -4.0183067 -4.0416155 -4.1235638 -4.1853766 -4.2023816 -4.1966481 -4.1813498 -4.1750288][-4.1954451 -4.2052941 -4.17573 -4.1266732 -4.0805559 -4.01777 -3.9645958 -3.9168961 -3.9559608 -4.0695906 -4.1501427 -4.1688681 -4.1644921 -4.1538997 -4.1556692][-4.184515 -4.192256 -4.1732955 -4.1342254 -4.0824485 -4.0087323 -3.945194 -3.903019 -3.9596438 -4.0728459 -4.1434741 -4.1554403 -4.1526041 -4.1483979 -4.1535811][-4.1728439 -4.1803722 -4.1764765 -4.15587 -4.1141534 -4.0543289 -4.003633 -3.9778826 -4.0236025 -4.1002879 -4.1482077 -4.1547079 -4.1486521 -4.1432419 -4.1530209][-4.1795168 -4.1902514 -4.2005558 -4.1958752 -4.1711588 -4.1353 -4.0976911 -4.076458 -4.0983839 -4.135468 -4.1623888 -4.1622348 -4.1509638 -4.1450577 -4.1619368][-4.2031136 -4.2146783 -4.2285175 -4.23157 -4.2210269 -4.2038212 -4.1747384 -4.15576 -4.1627545 -4.1772909 -4.1897635 -4.1869159 -4.1770215 -4.176827 -4.1983075][-4.2322984 -4.2428765 -4.2555447 -4.2612567 -4.2582054 -4.2491422 -4.2277422 -4.21541 -4.2173986 -4.2207842 -4.22501 -4.2233324 -4.2171144 -4.2191324 -4.2381024][-4.2722087 -4.2835922 -4.2934246 -4.2968841 -4.2941751 -4.2873182 -4.2724724 -4.2680578 -4.2701817 -4.2699337 -4.270875 -4.2701864 -4.2636337 -4.2622528 -4.2756433][-4.3074708 -4.31992 -4.3275704 -4.3286009 -4.3242931 -4.3165431 -4.3067679 -4.3074389 -4.3105488 -4.3112392 -4.3131776 -4.3107 -4.303494 -4.3012605 -4.3114004][-4.3193274 -4.3298588 -4.336267 -4.3373628 -4.3339553 -4.3279057 -4.3219051 -4.3223553 -4.3238525 -4.3250465 -4.3268256 -4.3244362 -4.3182244 -4.3168182 -4.3244762]]...]
INFO - root - 2017-12-07 12:12:56.011127: step 11110, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.989 sec/batch; 88h:18m:07s remains)
INFO - root - 2017-12-07 12:13:05.609087: step 11120, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 81h:49m:07s remains)
INFO - root - 2017-12-07 12:13:15.108643: step 11130, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 86h:41m:57s remains)
INFO - root - 2017-12-07 12:13:24.657402: step 11140, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 83h:12m:04s remains)
INFO - root - 2017-12-07 12:13:34.310681: step 11150, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 88h:01m:01s remains)
INFO - root - 2017-12-07 12:13:43.994180: step 11160, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 85h:03m:46s remains)
INFO - root - 2017-12-07 12:13:53.770252: step 11170, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 85h:03m:41s remains)
INFO - root - 2017-12-07 12:14:03.440670: step 11180, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.982 sec/batch; 87h:37m:48s remains)
INFO - root - 2017-12-07 12:14:13.149854: step 11190, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 89h:22m:52s remains)
INFO - root - 2017-12-07 12:14:22.906295: step 11200, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 87h:38m:08s remains)
2017-12-07 12:14:23.843766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3240714 -4.3292255 -4.3299751 -4.3276248 -4.327219 -4.3259082 -4.3217645 -4.31319 -4.3043013 -4.2964878 -4.2908669 -4.2881308 -4.2919364 -4.3015938 -4.3106222][-4.3182111 -4.32721 -4.3295441 -4.3267221 -4.324934 -4.3231068 -4.315217 -4.2997885 -4.2874107 -4.2781672 -4.2739482 -4.2758708 -4.283669 -4.2971225 -4.3063817][-4.3033738 -4.31527 -4.31757 -4.3137321 -4.3109808 -4.3076916 -4.295435 -4.2734895 -4.2615404 -4.2567153 -4.2560039 -4.2650709 -4.2786274 -4.2943058 -4.3011942][-4.2906833 -4.3028641 -4.3030934 -4.2937407 -4.283494 -4.27725 -4.2665858 -4.2435007 -4.2350388 -4.236002 -4.2393427 -4.2531295 -4.2731562 -4.2902565 -4.2958689][-4.2752242 -4.2870784 -4.2856684 -4.2687235 -4.2451997 -4.2297134 -4.2140326 -4.1894412 -4.1900563 -4.2046547 -4.2152195 -4.2331223 -4.2583947 -4.2791057 -4.2862206][-4.2518916 -4.2627935 -4.2595372 -4.2333765 -4.1940813 -4.1589036 -4.1275058 -4.09527 -4.1091347 -4.1461711 -4.1718812 -4.1985264 -4.2338324 -4.2602696 -4.2722464][-4.2185612 -4.2336531 -4.2273955 -4.1922145 -4.13659 -4.0739923 -4.011764 -3.9601016 -3.9851336 -4.0545006 -4.1040168 -4.1448383 -4.1908741 -4.2212563 -4.241313][-4.17864 -4.2056589 -4.2042694 -4.16157 -4.0897703 -4.0021973 -3.9096251 -3.8379662 -3.8772385 -3.9819272 -4.052743 -4.1018686 -4.146914 -4.1759825 -4.2041497][-4.1598554 -4.1996937 -4.2120275 -4.1721425 -4.1007595 -4.014112 -3.922183 -3.8536224 -3.8983059 -4.0020876 -4.0676394 -4.101872 -4.1275964 -4.1471453 -4.1782026][-4.1739273 -4.216136 -4.2340674 -4.2056384 -4.1542783 -4.0943341 -4.0328603 -3.9821546 -4.017849 -4.0941939 -4.1369452 -4.147604 -4.1526952 -4.1629558 -4.1849651][-4.213192 -4.246006 -4.2589526 -4.240593 -4.2131968 -4.1837482 -4.1528354 -4.1222653 -4.1441545 -4.1886234 -4.2086258 -4.2046704 -4.2007589 -4.2044716 -4.2121887][-4.2498636 -4.2729425 -4.2818136 -4.271791 -4.2639518 -4.2551088 -4.2425494 -4.2268739 -4.2358608 -4.2579637 -4.2640095 -4.253499 -4.2413859 -4.235364 -4.233068][-4.2885938 -4.305254 -4.3110561 -4.3025017 -4.3006344 -4.2998772 -4.2969794 -4.2910175 -4.2932692 -4.3040533 -4.3053327 -4.2959194 -4.2812328 -4.2709355 -4.2673545][-4.3187509 -4.3299584 -4.3304963 -4.3216391 -4.3176851 -4.3136516 -4.3113136 -4.3109932 -4.3153644 -4.3235922 -4.323698 -4.3192844 -4.3082385 -4.3023295 -4.3036971][-4.3335185 -4.339591 -4.3373785 -4.3268709 -4.319468 -4.3136406 -4.3098459 -4.3105536 -4.315527 -4.3202672 -4.319911 -4.3204894 -4.3169127 -4.3185539 -4.3259988]]...]
INFO - root - 2017-12-07 12:14:33.400091: step 11210, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 84h:38m:00s remains)
INFO - root - 2017-12-07 12:14:42.983443: step 11220, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 82h:31m:03s remains)
INFO - root - 2017-12-07 12:14:52.648876: step 11230, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 87h:08m:32s remains)
INFO - root - 2017-12-07 12:15:02.352283: step 11240, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 82h:55m:38s remains)
INFO - root - 2017-12-07 12:15:12.004665: step 11250, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.013 sec/batch; 90h:22m:27s remains)
INFO - root - 2017-12-07 12:15:21.645095: step 11260, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 86h:01m:52s remains)
INFO - root - 2017-12-07 12:15:31.496526: step 11270, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 89h:07m:15s remains)
INFO - root - 2017-12-07 12:15:41.110515: step 11280, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 80h:04m:44s remains)
INFO - root - 2017-12-07 12:15:50.723579: step 11290, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 84h:38m:36s remains)
INFO - root - 2017-12-07 12:16:00.444120: step 11300, loss = 2.05, batch loss = 1.99 (7.8 examples/sec; 1.022 sec/batch; 91h:11m:14s remains)
2017-12-07 12:16:01.346266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2116108 -4.217062 -4.2160845 -4.2049103 -4.1972742 -4.1935263 -4.196137 -4.1995149 -4.2028165 -4.2007914 -4.1900239 -4.1776972 -4.1740842 -4.1798477 -4.2009487][-4.208034 -4.2221441 -4.230722 -4.216351 -4.191112 -4.1750174 -4.1782141 -4.1895614 -4.1977873 -4.1981149 -4.1871204 -4.1748915 -4.1773677 -4.1891093 -4.2110295][-4.2027555 -4.2221322 -4.237505 -4.2258005 -4.1929126 -4.1647305 -4.1618609 -4.1740503 -4.1845522 -4.1819077 -4.1642652 -4.1476688 -4.1591849 -4.1822886 -4.2106276][-4.1868978 -4.2076516 -4.2259688 -4.2221951 -4.1908903 -4.1476016 -4.1330643 -4.1468496 -4.1689243 -4.1699734 -4.145853 -4.126174 -4.14517 -4.1773176 -4.20901][-4.160934 -4.184679 -4.2113037 -4.21643 -4.1831131 -4.1260471 -4.095839 -4.1170306 -4.1643796 -4.1823134 -4.1541486 -4.1274834 -4.1430445 -4.1743164 -4.2033443][-4.1570458 -4.1812348 -4.2085519 -4.2102833 -4.1633153 -4.0893159 -4.0456491 -4.0794773 -4.1583939 -4.1984849 -4.1702437 -4.1365352 -4.1407042 -4.1635747 -4.1905437][-4.1761236 -4.1927571 -4.2108459 -4.1999407 -4.129921 -4.0287147 -3.9670937 -4.0175815 -4.1304436 -4.1929097 -4.175632 -4.1468215 -4.1470737 -4.1604939 -4.182333][-4.2080655 -4.214304 -4.2187476 -4.1919961 -4.1045108 -3.9720156 -3.8800693 -3.9420185 -4.0859456 -4.1714025 -4.1728153 -4.1597023 -4.1657529 -4.1734333 -4.1879926][-4.2367315 -4.2381463 -4.2339149 -4.2017174 -4.119031 -3.9894204 -3.8914552 -3.9424579 -4.0795975 -4.1649671 -4.1729712 -4.1696081 -4.1794982 -4.1860971 -4.1982284][-4.2530031 -4.251451 -4.2466278 -4.2230544 -4.1657615 -4.0741558 -4.0033927 -4.0291085 -4.1179938 -4.177042 -4.181818 -4.1800809 -4.1877828 -4.1955013 -4.2057762][-4.2455988 -4.2401452 -4.2416973 -4.2342291 -4.2066793 -4.1493616 -4.1015692 -4.1124167 -4.1633835 -4.2020216 -4.2082181 -4.2058954 -4.20586 -4.2062306 -4.2103009][-4.22549 -4.2144051 -4.2198515 -4.2252784 -4.2217565 -4.1909242 -4.1551385 -4.1555266 -4.1866622 -4.2159491 -4.2294841 -4.2283139 -4.2206421 -4.2117362 -4.2046947][-4.2069368 -4.1927166 -4.1980906 -4.2076745 -4.2137117 -4.1971803 -4.1693869 -4.1653576 -4.1873198 -4.2134528 -4.2302232 -4.231987 -4.2201343 -4.2010822 -4.1847978][-4.2010503 -4.1888194 -4.192811 -4.2017145 -4.206574 -4.190649 -4.1654854 -4.1641712 -4.1854463 -4.2108502 -4.229558 -4.2337294 -4.2191906 -4.1942225 -4.1726866][-4.2127619 -4.2074385 -4.21237 -4.2166572 -4.2148533 -4.1965714 -4.172389 -4.1741796 -4.1982756 -4.2244086 -4.2433114 -4.2452264 -4.2300582 -4.2050118 -4.181942]]...]
INFO - root - 2017-12-07 12:16:11.065414: step 11310, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 88h:30m:47s remains)
INFO - root - 2017-12-07 12:16:20.652032: step 11320, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 86h:08m:09s remains)
INFO - root - 2017-12-07 12:16:30.479807: step 11330, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 84h:30m:36s remains)
INFO - root - 2017-12-07 12:16:40.123471: step 11340, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 80h:57m:55s remains)
INFO - root - 2017-12-07 12:16:49.818827: step 11350, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 85h:45m:28s remains)
INFO - root - 2017-12-07 12:16:59.493165: step 11360, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.025 sec/batch; 91h:26m:24s remains)
INFO - root - 2017-12-07 12:17:09.182884: step 11370, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 81h:12m:45s remains)
INFO - root - 2017-12-07 12:17:18.917203: step 11380, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.947 sec/batch; 84h:29m:03s remains)
INFO - root - 2017-12-07 12:17:28.626825: step 11390, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 88h:56m:52s remains)
INFO - root - 2017-12-07 12:17:38.350982: step 11400, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 86h:23m:13s remains)
2017-12-07 12:17:39.249716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.328486 -4.3273654 -4.3261685 -4.3265028 -4.3291054 -4.326489 -4.3195291 -4.3106012 -4.3054056 -4.30697 -4.3178129 -4.3313503 -4.3399348 -4.3433785 -4.3423886][-4.3104286 -4.3053021 -4.30165 -4.3001757 -4.3028784 -4.2992258 -4.2908382 -4.2822266 -4.2792435 -4.2831411 -4.2992115 -4.3210626 -4.3352613 -4.3388948 -4.3361549][-4.2863808 -4.2772412 -4.269259 -4.2652574 -4.2678385 -4.2603149 -4.2478094 -4.238924 -4.2420678 -4.2517004 -4.2718863 -4.3012495 -4.3221569 -4.328 -4.3247623][-4.2544723 -4.2409363 -4.2257824 -4.2143626 -4.2145042 -4.20393 -4.1863537 -4.1740742 -4.1879292 -4.2072172 -4.2314363 -4.2662706 -4.2916369 -4.3007879 -4.3018889][-4.2173352 -4.1988039 -4.1718287 -4.1429596 -4.1275511 -4.1072364 -4.0843592 -4.0780473 -4.115396 -4.1534085 -4.1832643 -4.2181687 -4.24299 -4.2558508 -4.26678][-4.1899052 -4.167068 -4.12978 -4.07503 -4.0195827 -3.9615312 -3.9168718 -3.9248309 -4.005827 -4.0790968 -4.124661 -4.162262 -4.1899738 -4.2105327 -4.2334657][-4.1759558 -4.1511507 -4.1123548 -4.0462389 -3.9518137 -3.8311479 -3.727427 -3.7374477 -3.8742743 -3.9970808 -4.0718989 -4.1223903 -4.1610355 -4.1905842 -4.2194095][-4.1633868 -4.1369891 -4.108315 -4.0518546 -3.9529009 -3.803124 -3.6505795 -3.6442711 -3.8062282 -3.9595141 -4.0573325 -4.1204028 -4.167305 -4.2005844 -4.2266][-4.1611824 -4.1377106 -4.1263785 -4.0967388 -4.0310259 -3.9131389 -3.7846413 -3.7639649 -3.8785071 -4.0040274 -4.092895 -4.1525416 -4.1975174 -4.2275405 -4.2461681][-4.1637163 -4.1446123 -4.1473227 -4.1441488 -4.1162949 -4.0452104 -3.9568093 -3.9337518 -3.9986095 -4.0796318 -4.1446047 -4.1916747 -4.2289991 -4.2520485 -4.2632256][-4.1743422 -4.1586237 -4.1709867 -4.1873803 -4.1867161 -4.1499138 -4.0938187 -4.0768008 -4.1117678 -4.1606064 -4.2021084 -4.2322974 -4.2575865 -4.2713132 -4.2764473][-4.205761 -4.1900067 -4.2012181 -4.2227774 -4.2369132 -4.2254829 -4.1957541 -4.1863503 -4.2030973 -4.2283039 -4.2502565 -4.2679768 -4.2832608 -4.2909184 -4.29202][-4.2483578 -4.237617 -4.2435913 -4.2598333 -4.2721233 -4.2690926 -4.2541547 -4.25017 -4.2602892 -4.2726631 -4.28174 -4.2919621 -4.3002248 -4.3036003 -4.3049808][-4.2751207 -4.2671714 -4.2703509 -4.2813215 -4.2884674 -4.2867875 -4.279686 -4.2787123 -4.286375 -4.293016 -4.2957797 -4.3007822 -4.3064256 -4.3112192 -4.3151369][-4.2944908 -4.2871485 -4.28884 -4.2945585 -4.2970395 -4.2965651 -4.2950845 -4.2943778 -4.2988086 -4.3040485 -4.3063846 -4.310462 -4.3154325 -4.3210187 -4.3250809]]...]
INFO - root - 2017-12-07 12:17:48.750139: step 11410, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 85h:18m:56s remains)
INFO - root - 2017-12-07 12:17:58.470044: step 11420, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 87h:44m:03s remains)
INFO - root - 2017-12-07 12:18:08.219818: step 11430, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 84h:43m:36s remains)
INFO - root - 2017-12-07 12:18:18.016052: step 11440, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 88h:15m:40s remains)
INFO - root - 2017-12-07 12:18:27.703804: step 11450, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.990 sec/batch; 88h:15m:18s remains)
INFO - root - 2017-12-07 12:18:37.510621: step 11460, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.006 sec/batch; 89h:42m:19s remains)
INFO - root - 2017-12-07 12:18:47.253833: step 11470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 84h:53m:28s remains)
INFO - root - 2017-12-07 12:18:56.789402: step 11480, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 87h:12m:21s remains)
INFO - root - 2017-12-07 12:19:06.471536: step 11490, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 87h:05m:51s remains)
INFO - root - 2017-12-07 12:19:16.260808: step 11500, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 85h:19m:41s remains)
2017-12-07 12:19:17.242323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3273249 -4.3259363 -4.3197737 -4.3114262 -4.2995596 -4.2880349 -4.2794881 -4.2743587 -4.2756886 -4.2767882 -4.2790103 -4.282196 -4.2888184 -4.29671 -4.3044186][-4.325119 -4.3219981 -4.3138309 -4.3034048 -4.2870297 -4.2688556 -4.25514 -4.2478261 -4.2524023 -4.2595611 -4.2724185 -4.2850375 -4.29577 -4.302577 -4.3059535][-4.3225513 -4.3171759 -4.306324 -4.2933598 -4.2735314 -4.2485662 -4.2283497 -4.2169032 -4.2253089 -4.2413306 -4.2674675 -4.2925768 -4.3087821 -4.314064 -4.3120322][-4.3220468 -4.3161545 -4.3034315 -4.2874141 -4.2621779 -4.2272573 -4.1961818 -4.1801748 -4.1896429 -4.2146726 -4.255713 -4.2933474 -4.3154297 -4.3196454 -4.3162956][-4.3234544 -4.3166914 -4.3006582 -4.2775874 -4.2418785 -4.1921291 -4.1482282 -4.1286955 -4.1435761 -4.1804595 -4.2341347 -4.2784681 -4.3042293 -4.3120813 -4.3116803][-4.3191876 -4.3052573 -4.2775874 -4.2426038 -4.1944952 -4.12991 -4.068738 -4.0450397 -4.0786905 -4.1345606 -4.2002978 -4.2496643 -4.2785268 -4.2916512 -4.2925587][-4.2994504 -4.272049 -4.2275763 -4.1781664 -4.1182051 -4.032218 -3.9405653 -3.9020629 -3.9659586 -4.0586133 -4.1406827 -4.1982026 -4.2372031 -4.25989 -4.2642088][-4.2678204 -4.22534 -4.1635385 -4.1004786 -4.0300245 -3.9294121 -3.8133969 -3.7530835 -3.8448656 -3.974442 -4.0733485 -4.1420937 -4.1955667 -4.2301793 -4.2411566][-4.2406216 -4.190608 -4.1217508 -4.0549297 -3.98897 -3.9048688 -3.811197 -3.7681336 -3.8494511 -3.9682021 -4.0601807 -4.1264963 -4.1854744 -4.2294989 -4.2497907][-4.2396321 -4.1917119 -4.1311684 -4.0772171 -4.0286393 -3.9789493 -3.9338868 -3.9209366 -3.9698453 -4.0470815 -4.1140542 -4.1679382 -4.2180591 -4.2592688 -4.2820954][-4.2640224 -4.2246284 -4.1738615 -4.1328616 -4.1053686 -4.0857544 -4.077467 -4.0836844 -4.1113353 -4.1510377 -4.1920214 -4.2328558 -4.2714553 -4.3025713 -4.3194985][-4.2938552 -4.2645459 -4.2253332 -4.1964889 -4.1861577 -4.1852856 -4.19335 -4.2054262 -4.2212658 -4.2368126 -4.25524 -4.2832727 -4.3123813 -4.3342514 -4.3443551][-4.3153086 -4.2989097 -4.2757473 -4.2604508 -4.2603588 -4.2664785 -4.2766504 -4.2868681 -4.2951527 -4.298274 -4.3037395 -4.3195095 -4.3381996 -4.3503618 -4.3547564][-4.3286991 -4.3240094 -4.3140621 -4.30717 -4.310185 -4.3172035 -4.3250523 -4.3315053 -4.3339806 -4.3330073 -4.3319721 -4.3381405 -4.3471956 -4.3529129 -4.3534737][-4.3356595 -4.3367209 -4.3327179 -4.3283291 -4.3287654 -4.3328838 -4.3377028 -4.3409276 -4.341527 -4.3399 -4.3385262 -4.3402395 -4.3443546 -4.3475003 -4.3477097]]...]
INFO - root - 2017-12-07 12:19:27.003768: step 11510, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.018 sec/batch; 90h:46m:45s remains)
INFO - root - 2017-12-07 12:19:36.662193: step 11520, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 79h:13m:56s remains)
INFO - root - 2017-12-07 12:19:46.098184: step 11530, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 83h:59m:36s remains)
INFO - root - 2017-12-07 12:19:55.803756: step 11540, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 88h:36m:11s remains)
INFO - root - 2017-12-07 12:20:05.534715: step 11550, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 86h:57m:17s remains)
INFO - root - 2017-12-07 12:20:15.178754: step 11560, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 86h:25m:03s remains)
INFO - root - 2017-12-07 12:20:24.865514: step 11570, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 85h:33m:27s remains)
INFO - root - 2017-12-07 12:20:34.543656: step 11580, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 83h:56m:14s remains)
INFO - root - 2017-12-07 12:20:44.480223: step 11590, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.011 sec/batch; 90h:05m:00s remains)
INFO - root - 2017-12-07 12:20:54.046994: step 11600, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 86h:20m:13s remains)
2017-12-07 12:20:54.991832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2953839 -4.2929759 -4.2909217 -4.2927246 -4.2942109 -4.2957087 -4.301621 -4.3075986 -4.309938 -4.3128386 -4.31999 -4.321619 -4.3226604 -4.3238726 -4.3260722][-4.257041 -4.25023 -4.2456942 -4.2457356 -4.2445011 -4.2447052 -4.249835 -4.2593007 -4.2668076 -4.274425 -4.2882242 -4.2950783 -4.3008909 -4.3057489 -4.3115797][-4.2085814 -4.1908545 -4.1834521 -4.1855168 -4.1884604 -4.1914215 -4.19599 -4.2081604 -4.217474 -4.229403 -4.2471085 -4.2591081 -4.2697167 -4.2774563 -4.2879229][-4.1642351 -4.1371341 -4.1233878 -4.1235056 -4.1287184 -4.1307817 -4.1323676 -4.1448283 -4.1557813 -4.1751971 -4.1990576 -4.2195311 -4.23575 -4.2454863 -4.2593579][-4.1309862 -4.0984445 -4.0800734 -4.0724049 -4.0731173 -4.0638485 -4.0530596 -4.0576005 -4.0664096 -4.0983596 -4.137876 -4.1691327 -4.1953583 -4.212863 -4.2321348][-4.1102495 -4.0728517 -4.0480561 -4.0301919 -4.0156689 -3.9891317 -3.9567845 -3.9376917 -3.9399328 -3.9922116 -4.0558915 -4.1060548 -4.1502395 -4.1851454 -4.2126546][-4.1157784 -4.0760436 -4.0468569 -4.0227809 -3.9955523 -3.9520543 -3.8948755 -3.8414795 -3.8309362 -3.8982415 -3.9836793 -4.0544991 -4.1149793 -4.1640449 -4.2005849][-4.1377578 -4.1018329 -4.0804281 -4.063797 -4.0373077 -3.9909472 -3.9243338 -3.8543587 -3.8332715 -3.8900318 -3.9694026 -4.040031 -4.1056533 -4.1629338 -4.2043085][-4.1574373 -4.1270633 -4.1135616 -4.1087432 -4.0919585 -4.05259 -3.9915028 -3.927129 -3.902554 -3.9383848 -3.997273 -4.0558357 -4.1166358 -4.1735368 -4.2141037][-4.1782055 -4.1520729 -4.1394563 -4.1393304 -4.1300898 -4.1015868 -4.05539 -4.0031276 -3.9771006 -3.9955263 -4.0369406 -4.0832515 -4.1346493 -4.1839452 -4.2233133][-4.202456 -4.1810122 -4.168026 -4.1666975 -4.1596689 -4.1410546 -4.1103354 -4.0721045 -4.0506144 -4.0589285 -4.0880766 -4.1226244 -4.1604505 -4.1992521 -4.2351871][-4.2413468 -4.22795 -4.2183967 -4.215991 -4.2111387 -4.1995349 -4.1777387 -4.1477919 -4.1295409 -4.1324911 -4.1534171 -4.1763816 -4.2000756 -4.2298188 -4.2617216][-4.2852383 -4.2806883 -4.2775469 -4.2761488 -4.2725029 -4.2641053 -4.2486439 -4.2267385 -4.2132969 -4.216279 -4.230444 -4.2431531 -4.2558208 -4.274879 -4.2976727][-4.3171678 -4.3183174 -4.3181491 -4.316874 -4.3125677 -4.3057575 -4.2962036 -4.2841725 -4.279326 -4.284668 -4.2940016 -4.3008714 -4.3060117 -4.3145571 -4.3263178][-4.3342037 -4.335494 -4.33449 -4.3330069 -4.33037 -4.3271437 -4.3233938 -4.3190918 -4.3185797 -4.3217726 -4.3262081 -4.329371 -4.330812 -4.334085 -4.3402991]]...]
INFO - root - 2017-12-07 12:21:04.528530: step 11610, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 86h:19m:10s remains)
INFO - root - 2017-12-07 12:21:14.111756: step 11620, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 86h:37m:03s remains)
INFO - root - 2017-12-07 12:21:23.762284: step 11630, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 83h:32m:22s remains)
INFO - root - 2017-12-07 12:21:33.367740: step 11640, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 86h:10m:48s remains)
INFO - root - 2017-12-07 12:21:43.032764: step 11650, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 86h:44m:35s remains)
INFO - root - 2017-12-07 12:21:52.774605: step 11660, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 87h:03m:04s remains)
INFO - root - 2017-12-07 12:22:02.362158: step 11670, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.966 sec/batch; 86h:03m:34s remains)
INFO - root - 2017-12-07 12:22:12.131150: step 11680, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 85h:05m:48s remains)
INFO - root - 2017-12-07 12:22:21.726524: step 11690, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 84h:59m:22s remains)
INFO - root - 2017-12-07 12:22:31.174187: step 11700, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 88h:54m:33s remains)
2017-12-07 12:22:32.120722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3188257 -4.3049612 -4.2934694 -4.2837949 -4.2793107 -4.2797718 -4.2809935 -4.274992 -4.2588453 -4.2378144 -4.2265034 -4.2277179 -4.240252 -4.2561607 -4.2700071][-4.3049912 -4.2857795 -4.2728152 -4.2603235 -4.2508759 -4.248085 -4.2489109 -4.2427077 -4.2274609 -4.206357 -4.1941614 -4.1943078 -4.2071462 -4.2254748 -4.2400761][-4.2969513 -4.2724409 -4.2545686 -4.2364464 -4.220943 -4.2153382 -4.2162304 -4.2151966 -4.2092919 -4.1952257 -4.1829586 -4.1807709 -4.1896315 -4.20539 -4.2172265][-4.2903457 -4.2616191 -4.236867 -4.2116714 -4.1934738 -4.1838174 -4.18259 -4.1865325 -4.1939845 -4.1897182 -4.1777592 -4.1736393 -4.1816406 -4.1980023 -4.2081003][-4.2816272 -4.2519813 -4.2244616 -4.1947379 -4.1707854 -4.1521649 -4.141305 -4.1436529 -4.1616616 -4.1693859 -4.1646314 -4.1657333 -4.1789784 -4.1963139 -4.2060156][-4.2737961 -4.2424164 -4.21519 -4.1833224 -4.1498041 -4.113472 -4.082036 -4.07867 -4.106348 -4.1292825 -4.1403179 -4.1524076 -4.1716628 -4.1900568 -4.20393][-4.2668467 -4.2317266 -4.2012124 -4.16224 -4.114593 -4.0520282 -3.9904456 -3.9823482 -4.0324173 -4.0792074 -4.1101055 -4.1376934 -4.161335 -4.1829448 -4.2023463][-4.2633066 -4.2246966 -4.1879811 -4.1417413 -4.0850277 -4.0039563 -3.9185157 -3.910078 -3.9822814 -4.0483451 -4.0943089 -4.1317844 -4.1553078 -4.1775026 -4.1995907][-4.2654619 -4.2266407 -4.1860118 -4.139585 -4.0916038 -4.024991 -3.9534543 -3.9468498 -4.0103979 -4.070147 -4.1126814 -4.1483755 -4.1665959 -4.1827235 -4.2007737][-4.2717991 -4.2355165 -4.1977787 -4.161912 -4.1347866 -4.0993915 -4.0596256 -4.0506716 -4.0840297 -4.1184359 -4.1452684 -4.1708131 -4.1820068 -4.1893635 -4.197185][-4.2835164 -4.2536445 -4.2217264 -4.1947222 -4.180687 -4.1663232 -4.1460724 -4.1335192 -4.1414914 -4.1548991 -4.1715736 -4.188211 -4.1909838 -4.1893396 -4.1896453][-4.2941232 -4.2707839 -4.2476182 -4.2299614 -4.2236819 -4.2185431 -4.205503 -4.1860032 -4.1719642 -4.171308 -4.1838231 -4.1954956 -4.1959524 -4.1922626 -4.1921268][-4.3020463 -4.2834411 -4.2679939 -4.2578807 -4.2549162 -4.250248 -4.2356334 -4.2056389 -4.1776519 -4.1705437 -4.1810679 -4.1899247 -4.195672 -4.2007675 -4.2065935][-4.3122382 -4.2961135 -4.2842765 -4.2765031 -4.2701459 -4.2603192 -4.2405081 -4.2063031 -4.1769819 -4.1701069 -4.1809411 -4.1908369 -4.2038403 -4.217762 -4.2279105][-4.3260884 -4.3131433 -4.3024468 -4.293087 -4.2823553 -4.2681065 -4.2472816 -4.2166986 -4.1924348 -4.1880522 -4.198194 -4.2090454 -4.2246661 -4.2422895 -4.2516508]]...]
INFO - root - 2017-12-07 12:22:41.714176: step 11710, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 85h:30m:32s remains)
INFO - root - 2017-12-07 12:22:51.402860: step 11720, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 86h:46m:35s remains)
INFO - root - 2017-12-07 12:23:01.158389: step 11730, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 85h:10m:19s remains)
INFO - root - 2017-12-07 12:23:10.758330: step 11740, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 87h:11m:59s remains)
INFO - root - 2017-12-07 12:23:20.392083: step 11750, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 83h:43m:49s remains)
INFO - root - 2017-12-07 12:23:30.138189: step 11760, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 86h:50m:39s remains)
INFO - root - 2017-12-07 12:23:39.854798: step 11770, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.995 sec/batch; 88h:37m:25s remains)
INFO - root - 2017-12-07 12:23:49.601073: step 11780, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 84h:25m:36s remains)
INFO - root - 2017-12-07 12:23:59.260040: step 11790, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 87h:43m:32s remains)
INFO - root - 2017-12-07 12:24:08.925792: step 11800, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 90h:24m:32s remains)
2017-12-07 12:24:09.937244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2418022 -4.2229686 -4.2195234 -4.2389035 -4.2447119 -4.2447052 -4.24321 -4.2391839 -4.2401547 -4.2510252 -4.2625461 -4.2680097 -4.2677097 -4.271554 -4.2769032][-4.2194347 -4.2030358 -4.2045927 -4.2282572 -4.2376742 -4.241931 -4.2442174 -4.241611 -4.2433815 -4.2515545 -4.2544737 -4.2530694 -4.2507944 -4.2516413 -4.2580037][-4.2068567 -4.183795 -4.1845255 -4.2091327 -4.2184181 -4.2233582 -4.2308636 -4.2389874 -4.2490473 -4.2611284 -4.263495 -4.2609692 -4.2547221 -4.2495422 -4.2524152][-4.19761 -4.15715 -4.1456666 -4.1635175 -4.1682911 -4.1737638 -4.1886597 -4.2118883 -4.2361627 -4.2577977 -4.2655597 -4.2664342 -4.2596369 -4.2524848 -4.254077][-4.1812625 -4.1328969 -4.1116033 -4.1175146 -4.1148615 -4.1182723 -4.1371193 -4.1680388 -4.2036066 -4.2339554 -4.2477903 -4.25624 -4.2545133 -4.2529378 -4.2582984][-4.1513147 -4.1110086 -4.0908217 -4.0898566 -4.0842829 -4.0837259 -4.0960374 -4.11983 -4.1535344 -4.1879354 -4.2088375 -4.2253008 -4.2322683 -4.2411027 -4.2541757][-4.10738 -4.0803809 -4.064014 -4.0642591 -4.0660696 -4.0664515 -4.0698895 -4.0877738 -4.1154394 -4.1459184 -4.1664429 -4.184761 -4.1978297 -4.2160673 -4.2389083][-4.0762367 -4.0531712 -4.03621 -4.0403271 -4.0500097 -4.0525455 -4.0580149 -4.0803776 -4.1095228 -4.1343141 -4.14911 -4.1597772 -4.1681685 -4.1883183 -4.2169275][-4.0748019 -4.0508051 -4.032351 -4.0396056 -4.0518403 -4.0555034 -4.0634775 -4.0873618 -4.1186895 -4.1421509 -4.1529732 -4.1567345 -4.1607 -4.179286 -4.2069144][-4.0836911 -4.0605206 -4.0482159 -4.0610681 -4.0719872 -4.0747228 -4.085669 -4.1074648 -4.1328831 -4.1537132 -4.1616197 -4.1624122 -4.1659226 -4.1839681 -4.2090163][-4.103447 -4.0822554 -4.0748143 -4.0884852 -4.0980506 -4.1030207 -4.1159458 -4.1307855 -4.1458068 -4.1601181 -4.16561 -4.168541 -4.1742744 -4.1921291 -4.2165561][-4.1354871 -4.1186905 -4.1111813 -4.120245 -4.1257772 -4.1308479 -4.1456132 -4.1594839 -4.1705184 -4.1803546 -4.1823297 -4.1822953 -4.1871839 -4.2015481 -4.2240362][-4.1773262 -4.1655941 -4.16082 -4.1641626 -4.1662822 -4.1716514 -4.184927 -4.19754 -4.2055521 -4.2115679 -4.2116113 -4.2078848 -4.2086883 -4.2184997 -4.2366858][-4.2248178 -4.2165151 -4.2145796 -4.216289 -4.2173142 -4.2199988 -4.2270923 -4.2356668 -4.2426853 -4.2488022 -4.249999 -4.2457857 -4.2431884 -4.2478 -4.2599072][-4.262609 -4.2580209 -4.25757 -4.2580938 -4.2582836 -4.2587576 -4.2607937 -4.2646079 -4.2701902 -4.2766895 -4.280293 -4.2796412 -4.2775874 -4.2793183 -4.2874017]]...]
INFO - root - 2017-12-07 12:24:19.315774: step 11810, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 82h:34m:17s remains)
INFO - root - 2017-12-07 12:24:29.128783: step 11820, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 85h:15m:40s remains)
INFO - root - 2017-12-07 12:24:38.897272: step 11830, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 85h:31m:33s remains)
INFO - root - 2017-12-07 12:24:48.535222: step 11840, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.929 sec/batch; 82h:43m:00s remains)
INFO - root - 2017-12-07 12:24:58.173994: step 11850, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 83h:12m:45s remains)
INFO - root - 2017-12-07 12:25:07.668087: step 11860, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 80h:21m:56s remains)
INFO - root - 2017-12-07 12:25:17.255346: step 11870, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.993 sec/batch; 88h:24m:56s remains)
INFO - root - 2017-12-07 12:25:26.510103: step 11880, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 88h:03m:32s remains)
INFO - root - 2017-12-07 12:25:36.284046: step 11890, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:33m:52s remains)
INFO - root - 2017-12-07 12:25:45.896991: step 11900, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 79h:47m:58s remains)
2017-12-07 12:25:46.850143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3102293 -4.3119845 -4.3135362 -4.3099875 -4.3077688 -4.3070941 -4.3058496 -4.3154531 -4.3326154 -4.3400068 -4.33857 -4.3360543 -4.3321848 -4.3307652 -4.3326011][-4.3059521 -4.3085322 -4.3117981 -4.3112011 -4.31105 -4.3119521 -4.310307 -4.3184133 -4.33622 -4.3470025 -4.3503032 -4.3513188 -4.3480773 -4.3437524 -4.3389196][-4.3007464 -4.3002162 -4.2976313 -4.2918062 -4.2856817 -4.2824383 -4.2788391 -4.288444 -4.3105488 -4.3273931 -4.3358631 -4.3409781 -4.3404803 -4.3373656 -4.3325958][-4.2994294 -4.292057 -4.2774739 -4.2607994 -4.2464886 -4.2385464 -4.2335 -4.2467833 -4.2772632 -4.3015981 -4.3122578 -4.3176141 -4.316463 -4.315414 -4.3153324][-4.3019347 -4.2846441 -4.2556405 -4.2272215 -4.2015076 -4.1811938 -4.16781 -4.1793513 -4.219286 -4.2517877 -4.2679348 -4.2775702 -4.280261 -4.2862649 -4.2946124][-4.3045969 -4.2788076 -4.2387781 -4.200664 -4.1604929 -4.1175485 -4.0837626 -4.0890388 -4.1440248 -4.1911058 -4.217134 -4.2352986 -4.2444477 -4.2579808 -4.2761812][-4.3070292 -4.2771206 -4.2316313 -4.1837482 -4.1247125 -4.0503931 -3.9830465 -3.9762657 -4.0564814 -4.1288443 -4.17281 -4.2043276 -4.2195053 -4.2369924 -4.2600069][-4.30729 -4.2769485 -4.2308354 -4.1781158 -4.1074648 -4.0082045 -3.9024305 -3.8733606 -3.9775562 -4.07598 -4.1357069 -4.1766429 -4.1976304 -4.2165704 -4.2369089][-4.308042 -4.28247 -4.2445216 -4.2026758 -4.1425576 -4.0487447 -3.943187 -3.9084091 -3.9973681 -4.0794544 -4.1269641 -4.1617179 -4.179913 -4.1946459 -4.2101431][-4.309258 -4.2928076 -4.2700715 -4.2461281 -4.2068624 -4.1389627 -4.0627432 -4.038084 -4.0932579 -4.1348963 -4.15134 -4.1638813 -4.1691976 -4.1759911 -4.183116][-4.3090944 -4.3005447 -4.290184 -4.2793636 -4.2584462 -4.2150068 -4.1679974 -4.1554346 -4.1905422 -4.2045689 -4.194313 -4.1842442 -4.174911 -4.170207 -4.1649723][-4.3068829 -4.3045063 -4.30103 -4.2934723 -4.2795043 -4.2557645 -4.2302608 -4.2283239 -4.2554502 -4.2580571 -4.2355423 -4.2116446 -4.1917663 -4.17462 -4.1600914][-4.3089652 -4.3092504 -4.3091307 -4.3002825 -4.285553 -4.2694659 -4.2552171 -4.2584076 -4.2809539 -4.2822824 -4.26001 -4.2341628 -4.2115879 -4.1893888 -4.1713347][-4.3130221 -4.3105478 -4.3075938 -4.2969041 -4.2832246 -4.2704959 -4.2613921 -4.266511 -4.2845254 -4.2854271 -4.267622 -4.2475138 -4.2287016 -4.206811 -4.1913972][-4.3151131 -4.3089795 -4.3013492 -4.2898865 -4.278501 -4.2695794 -4.2659321 -4.2737684 -4.2874451 -4.2876534 -4.2735772 -4.2586894 -4.2442393 -4.224956 -4.2141204]]...]
INFO - root - 2017-12-07 12:25:56.405495: step 11910, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 83h:40m:57s remains)
INFO - root - 2017-12-07 12:26:06.124295: step 11920, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 86h:15m:48s remains)
INFO - root - 2017-12-07 12:26:15.932696: step 11930, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.999 sec/batch; 88h:58m:25s remains)
INFO - root - 2017-12-07 12:26:25.639154: step 11940, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 88h:21m:08s remains)
INFO - root - 2017-12-07 12:26:35.247618: step 11950, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 86h:26m:31s remains)
INFO - root - 2017-12-07 12:26:45.046383: step 11960, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 85h:47m:50s remains)
INFO - root - 2017-12-07 12:26:54.714854: step 11970, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 86h:43m:11s remains)
INFO - root - 2017-12-07 12:27:04.358142: step 11980, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.014 sec/batch; 90h:15m:35s remains)
INFO - root - 2017-12-07 12:27:13.987954: step 11990, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 87h:18m:02s remains)
INFO - root - 2017-12-07 12:27:23.560300: step 12000, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 87h:08m:04s remains)
2017-12-07 12:27:24.466809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2640696 -4.2538581 -4.2494855 -4.2538123 -4.2616558 -4.2567773 -4.2453213 -4.2336516 -4.2306232 -4.2314963 -4.2395735 -4.2493238 -4.2487321 -4.2442379 -4.2427583][-4.2410688 -4.2291894 -4.2242332 -4.2324853 -4.2392592 -4.22632 -4.2030907 -4.1845617 -4.18475 -4.1951842 -4.2091951 -4.2193937 -4.215302 -4.2041845 -4.2036452][-4.2165966 -4.2014318 -4.1945324 -4.206347 -4.2122436 -4.1901779 -4.1560841 -4.135169 -4.1425681 -4.162549 -4.1808553 -4.1902561 -4.1824622 -4.1678562 -4.1696148][-4.2039995 -4.1836314 -4.1739116 -4.1881623 -4.1929741 -4.1651421 -4.1279984 -4.1096587 -4.1217151 -4.1399584 -4.1546636 -4.1675038 -4.1638727 -4.1517563 -4.1571555][-4.2090058 -4.1806993 -4.1628413 -4.174881 -4.1748219 -4.1473913 -4.114109 -4.1028857 -4.1148677 -4.120482 -4.1246238 -4.1422634 -4.1528821 -4.1533093 -4.1615386][-4.2201438 -4.1834497 -4.1544342 -4.1552629 -4.1478353 -4.1232224 -4.0977106 -4.08931 -4.0904565 -4.0791111 -4.072525 -4.097343 -4.1284132 -4.1493549 -4.1636281][-4.2321672 -4.1929374 -4.1594672 -4.1492209 -4.13805 -4.1150074 -4.0917892 -4.0784464 -4.061265 -4.0290694 -4.0112672 -4.0451455 -4.0997639 -4.1442561 -4.1695294][-4.2401433 -4.2061152 -4.1778393 -4.1640944 -4.1499767 -4.1230288 -4.0986614 -4.0811887 -4.0492563 -4.0047121 -3.9860315 -4.0252376 -4.0895109 -4.1430197 -4.1798677][-4.242692 -4.2148333 -4.1947041 -4.179924 -4.1601148 -4.1293869 -4.1076136 -4.0901175 -4.0572357 -4.0215011 -4.0226984 -4.0611525 -4.1082206 -4.149086 -4.1879873][-4.2433443 -4.2125554 -4.19014 -4.1773763 -4.1639853 -4.141654 -4.1307125 -4.1219549 -4.1013541 -4.0820265 -4.0925694 -4.11606 -4.1312537 -4.1500854 -4.1817436][-4.2405691 -4.2003846 -4.1703205 -4.1635275 -4.16543 -4.1592774 -4.1631656 -4.1660194 -4.1594882 -4.1500506 -4.1541214 -4.1560755 -4.1481457 -4.1508474 -4.169682][-4.2432523 -4.1960015 -4.1599617 -4.1571774 -4.1708388 -4.1782932 -4.1914163 -4.1973786 -4.1938434 -4.1854606 -4.1853094 -4.1749506 -4.1583152 -4.1550574 -4.1653819][-4.2655869 -4.2173157 -4.1794419 -4.1727796 -4.1876531 -4.1993589 -4.211421 -4.2153583 -4.2055974 -4.1954184 -4.1931167 -4.1785197 -4.1603875 -4.1576853 -4.1641178][-4.2977061 -4.2577157 -4.219399 -4.2058339 -4.215157 -4.2204151 -4.2198396 -4.2160869 -4.2061644 -4.2017422 -4.2017651 -4.1939096 -4.1824918 -4.1841516 -4.1860938][-4.3258605 -4.2967658 -4.2626009 -4.2457113 -4.2425075 -4.2372417 -4.2252855 -4.21611 -4.2132382 -4.2213836 -4.2338285 -4.2383776 -4.2341886 -4.2372389 -4.2365608]]...]
INFO - root - 2017-12-07 12:27:34.312810: step 12010, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.009 sec/batch; 89h:51m:19s remains)
INFO - root - 2017-12-07 12:27:43.791366: step 12020, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 87h:38m:24s remains)
INFO - root - 2017-12-07 12:27:53.484116: step 12030, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 82h:51m:50s remains)
INFO - root - 2017-12-07 12:28:03.303926: step 12040, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.010 sec/batch; 89h:53m:02s remains)
INFO - root - 2017-12-07 12:28:12.878644: step 12050, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 84h:11m:42s remains)
INFO - root - 2017-12-07 12:28:22.635611: step 12060, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 83h:26m:08s remains)
INFO - root - 2017-12-07 12:28:32.445953: step 12070, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 85h:32m:19s remains)
INFO - root - 2017-12-07 12:28:42.125391: step 12080, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 86h:26m:13s remains)
INFO - root - 2017-12-07 12:28:51.921178: step 12090, loss = 2.12, batch loss = 2.06 (8.0 examples/sec; 1.006 sec/batch; 89h:33m:31s remains)
INFO - root - 2017-12-07 12:29:01.504897: step 12100, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 86h:36m:37s remains)
2017-12-07 12:29:02.422877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2204041 -4.2211304 -4.2211566 -4.2108068 -4.1913233 -4.1682086 -4.1708817 -4.2011142 -4.2391763 -4.2642937 -4.27353 -4.2684584 -4.2591281 -4.2563891 -4.2609825][-4.2087283 -4.2126393 -4.2124748 -4.2033639 -4.1858029 -4.1680169 -4.1746645 -4.206912 -4.2468424 -4.2715096 -4.2770739 -4.271667 -4.264832 -4.2631974 -4.2659774][-4.2047973 -4.2089677 -4.2088733 -4.2037911 -4.1915483 -4.1758914 -4.18113 -4.2133636 -4.2543349 -4.2812204 -4.2864609 -4.2809744 -4.27208 -4.2664366 -4.2629747][-4.2221241 -4.2232761 -4.2197671 -4.2162127 -4.20726 -4.1839585 -4.1729379 -4.1967797 -4.2388077 -4.2741513 -4.2863312 -4.2830729 -4.2743344 -4.2653084 -4.2581048][-4.2550282 -4.2556858 -4.2485828 -4.2410421 -4.2279773 -4.1918125 -4.155138 -4.1542168 -4.1924152 -4.2438908 -4.2702856 -4.2740173 -4.2678847 -4.2599459 -4.2543468][-4.2762432 -4.2774196 -4.2712383 -4.2604094 -4.2368641 -4.1858311 -4.1202154 -4.0835333 -4.1111984 -4.181994 -4.2324095 -4.2493639 -4.2520375 -4.2526121 -4.2520909][-4.2942204 -4.295846 -4.2906733 -4.2768488 -4.2455797 -4.1828389 -4.0915561 -4.0127091 -4.0114026 -4.0955935 -4.1786857 -4.2194223 -4.2378478 -4.2481074 -4.2530289][-4.307188 -4.3164144 -4.3158555 -4.299561 -4.267179 -4.2073555 -4.1169615 -4.0136209 -3.9663343 -4.0259018 -4.1166687 -4.1801071 -4.2181692 -4.241014 -4.2513633][-4.3111782 -4.3294773 -4.3354006 -4.3207588 -4.2915072 -4.2468204 -4.1807418 -4.0943475 -4.0315537 -4.0382018 -4.0892525 -4.1454916 -4.1907825 -4.2214031 -4.2386446][-4.3056622 -4.3282371 -4.3411365 -4.3357239 -4.3174524 -4.289083 -4.2462697 -4.1854005 -4.1280622 -4.1091938 -4.1244693 -4.1558342 -4.1854234 -4.2062287 -4.2205472][-4.2864885 -4.3121433 -4.3326092 -4.3408766 -4.3356342 -4.3193502 -4.2920275 -4.2511587 -4.2073617 -4.184515 -4.1858091 -4.2012296 -4.2170191 -4.22405 -4.2255321][-4.2731776 -4.2986975 -4.3203483 -4.3360529 -4.3403883 -4.3310905 -4.3085289 -4.2758074 -4.2425771 -4.2240229 -4.2263789 -4.2398758 -4.2511382 -4.2529297 -4.248116][-4.2734132 -4.2948294 -4.3106995 -4.3245997 -4.33088 -4.3229556 -4.300478 -4.2674847 -4.2368813 -4.2221808 -4.2306404 -4.2503824 -4.2663383 -4.2695765 -4.264905][-4.2810054 -4.2924809 -4.29996 -4.3079424 -4.3135152 -4.3059072 -4.2815242 -4.2458348 -4.20963 -4.19259 -4.2063446 -4.2356057 -4.2584128 -4.2651839 -4.2655611][-4.2856746 -4.2854729 -4.2862406 -4.2894444 -4.2928629 -4.2872424 -4.2663655 -4.2301297 -4.1908827 -4.1729856 -4.1894054 -4.2205815 -4.244729 -4.2518034 -4.254251]]...]
INFO - root - 2017-12-07 12:29:12.285989: step 12110, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 89h:13m:52s remains)
INFO - root - 2017-12-07 12:29:21.994627: step 12120, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 87h:51m:08s remains)
INFO - root - 2017-12-07 12:29:31.624597: step 12130, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 87h:58m:20s remains)
INFO - root - 2017-12-07 12:29:41.347143: step 12140, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 86h:40m:58s remains)
INFO - root - 2017-12-07 12:29:50.853480: step 12150, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 81h:36m:15s remains)
INFO - root - 2017-12-07 12:30:00.623643: step 12160, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 88h:05m:37s remains)
INFO - root - 2017-12-07 12:30:10.300903: step 12170, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 87h:12m:35s remains)
INFO - root - 2017-12-07 12:30:19.956749: step 12180, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 84h:28m:59s remains)
INFO - root - 2017-12-07 12:30:29.730364: step 12190, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 85h:52m:18s remains)
INFO - root - 2017-12-07 12:30:39.297416: step 12200, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.031 sec/batch; 91h:45m:40s remains)
2017-12-07 12:30:40.220427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2941875 -4.3011608 -4.3058457 -4.3055511 -4.3054643 -4.3065457 -4.3064981 -4.3066773 -4.307488 -4.3043 -4.3021412 -4.3043509 -4.3101859 -4.3172035 -4.3203316][-4.2745152 -4.2841382 -4.2924852 -4.291707 -4.2869959 -4.282938 -4.2800694 -4.2830815 -4.2914028 -4.2950811 -4.2983484 -4.3053584 -4.3147554 -4.3229685 -4.3232675][-4.2427049 -4.2535019 -4.2640467 -4.2624612 -4.2515297 -4.2389245 -4.2307177 -4.2392683 -4.2590861 -4.2724276 -4.2827783 -4.2972555 -4.3120747 -4.322824 -4.3223424][-4.2033534 -4.2117281 -4.2216687 -4.21839 -4.1995444 -4.1774769 -4.1611466 -4.1725841 -4.2013712 -4.22427 -4.2427039 -4.2662187 -4.2890344 -4.3033152 -4.3037][-4.1625967 -4.1663566 -4.1751161 -4.1716866 -4.1454558 -4.1101093 -4.0766106 -4.0815287 -4.1190691 -4.1558976 -4.1896505 -4.2256389 -4.2564869 -4.270936 -4.2676649][-4.1218395 -4.1187477 -4.1243291 -4.1157784 -4.0764222 -4.0169659 -3.953409 -3.9455898 -4.0031924 -4.0688324 -4.1263571 -4.178762 -4.217761 -4.233254 -4.2261896][-4.0854669 -4.0708833 -4.0650063 -4.0460992 -3.9935522 -3.909744 -3.81328 -3.7928143 -3.8795416 -3.9823933 -4.0662541 -4.1285443 -4.1719809 -4.192524 -4.1886859][-4.06203 -4.0331793 -4.0142646 -3.9888737 -3.9423666 -3.8676403 -3.7789657 -3.7614965 -3.8552985 -3.9679091 -4.0543752 -4.1078553 -4.1422262 -4.1627693 -4.1668892][-4.0583153 -4.0214825 -3.9993138 -3.9779751 -3.9553006 -3.9226458 -3.8801641 -3.8729284 -3.9341705 -4.0172844 -4.0815592 -4.1155195 -4.1352057 -4.1485372 -4.158113][-4.075655 -4.0424886 -4.02534 -4.0157084 -4.0132742 -4.0112638 -3.9986165 -3.9946103 -4.02322 -4.0702596 -4.109345 -4.1308126 -4.1432981 -4.1511054 -4.1596427][-4.1007233 -4.0766773 -4.0687156 -4.0674348 -4.0720568 -4.0811234 -4.0798187 -4.0745897 -4.0845876 -4.1096034 -4.132977 -4.1494651 -4.1632371 -4.1722584 -4.1793742][-4.124764 -4.1079 -4.1071515 -4.1106668 -4.1180582 -4.1297169 -4.1328378 -4.1286969 -4.1337028 -4.1482997 -4.1643205 -4.1791887 -4.194355 -4.2042933 -4.2111607][-4.1512251 -4.1385646 -4.1426535 -4.1499462 -4.1586022 -4.168745 -4.17152 -4.1691031 -4.1728792 -4.1831722 -4.1975703 -4.2130017 -4.2268806 -4.2365417 -4.2424245][-4.1907048 -4.1826024 -4.1878824 -4.1961379 -4.2040453 -4.2102785 -4.2106924 -4.2087011 -4.2104621 -4.2169809 -4.2280846 -4.2411127 -4.2524986 -4.2603498 -4.2643862][-4.234889 -4.2310262 -4.2349024 -4.2404637 -4.2451172 -4.24769 -4.2466011 -4.244812 -4.2456255 -4.249372 -4.2556987 -4.263536 -4.2709732 -4.2766309 -4.2795391]]...]
INFO - root - 2017-12-07 12:30:49.885311: step 12210, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.025 sec/batch; 91h:13m:11s remains)
INFO - root - 2017-12-07 12:30:59.545247: step 12220, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 88h:33m:46s remains)
INFO - root - 2017-12-07 12:31:09.291798: step 12230, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 84h:19m:43s remains)
INFO - root - 2017-12-07 12:31:18.764356: step 12240, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:45m:55s remains)
INFO - root - 2017-12-07 12:31:28.374928: step 12250, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.960 sec/batch; 85h:22m:19s remains)
INFO - root - 2017-12-07 12:31:37.977872: step 12260, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 89h:42m:00s remains)
INFO - root - 2017-12-07 12:31:47.659762: step 12270, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 87h:40m:43s remains)
INFO - root - 2017-12-07 12:31:57.193688: step 12280, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 86h:13m:24s remains)
INFO - root - 2017-12-07 12:32:06.783488: step 12290, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 83h:18m:51s remains)
INFO - root - 2017-12-07 12:32:16.474724: step 12300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 83h:47m:07s remains)
2017-12-07 12:32:17.481529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3892126 -4.3668156 -4.3186908 -4.2586513 -4.1990304 -4.1790576 -4.2112527 -4.2609448 -4.2951665 -4.3062372 -4.2915592 -4.2721663 -4.2642021 -4.2809095 -4.3170304][-4.3973441 -4.3672476 -4.3111339 -4.2378244 -4.1647172 -4.1436944 -4.18787 -4.2445755 -4.2814107 -4.2928076 -4.2811394 -4.2679729 -4.2703753 -4.2900891 -4.3260651][-4.396379 -4.3575463 -4.2971873 -4.2198477 -4.1431413 -4.120563 -4.1661234 -4.2181931 -4.2488871 -4.2559891 -4.2491741 -4.2482934 -4.2658834 -4.2929683 -4.3286004][-4.3864703 -4.3397379 -4.2798462 -4.2095909 -4.1415472 -4.1174774 -4.148551 -4.1793938 -4.1956639 -4.1980748 -4.1988163 -4.2123165 -4.2467227 -4.2859325 -4.3234367][-4.3771744 -4.3278232 -4.2685146 -4.2072492 -4.150207 -4.1247387 -4.13105 -4.12884 -4.1307139 -4.1321368 -4.1432991 -4.1769648 -4.2276535 -4.2750106 -4.3133936][-4.3756003 -4.3362989 -4.2838373 -4.2278423 -4.1725788 -4.1321459 -4.09738 -4.0592051 -4.057642 -4.0744004 -4.1075516 -4.1650786 -4.2285419 -4.2781811 -4.3128719][-4.3824844 -4.362587 -4.3254609 -4.2694983 -4.2017593 -4.12827 -4.0401511 -3.9668019 -3.984062 -4.0393782 -4.1040287 -4.1783342 -4.2461123 -4.2927203 -4.3216715][-4.3908591 -4.3843803 -4.3582897 -4.3031125 -4.2201538 -4.11335 -3.9843104 -3.893013 -3.9411607 -4.0381846 -4.1270089 -4.2045712 -4.2664623 -4.3078084 -4.331821][-4.4009414 -4.4022727 -4.3805838 -4.3237243 -4.2346635 -4.1179094 -3.9839983 -3.905211 -3.9706931 -4.0794773 -4.1678815 -4.2353916 -4.28389 -4.3166142 -4.3365016][-4.4106359 -4.4177885 -4.3960176 -4.3361459 -4.2504292 -4.1448331 -4.0368514 -3.9902608 -4.0539351 -4.146451 -4.2203093 -4.2726727 -4.3042836 -4.3244085 -4.3386588][-4.4135756 -4.4247375 -4.4039888 -4.3464451 -4.2633133 -4.1739097 -4.1030569 -4.0901103 -4.1435213 -4.2111421 -4.2659693 -4.3013945 -4.3177743 -4.3271046 -4.3379993][-4.4057894 -4.4199171 -4.4055576 -4.360239 -4.2909656 -4.2199554 -4.1767378 -4.180222 -4.2189245 -4.2625213 -4.2985392 -4.3177404 -4.3197784 -4.3221941 -4.3326883][-4.3933778 -4.4070177 -4.3968086 -4.3654785 -4.316525 -4.2669005 -4.24312 -4.2532654 -4.27892 -4.3034182 -4.3203278 -4.32108 -4.3102818 -4.3097782 -4.3235044][-4.3766522 -4.3903761 -4.3828139 -4.3616371 -4.3309879 -4.3001938 -4.29099 -4.3033767 -4.320375 -4.3313246 -4.3330755 -4.3203607 -4.3031564 -4.3040705 -4.3223767][-4.3614068 -4.3767242 -4.37367 -4.3579607 -4.3379555 -4.3204556 -4.3178844 -4.3268251 -4.3379321 -4.3382878 -4.3283563 -4.308188 -4.2915583 -4.2980032 -4.320704]]...]
INFO - root - 2017-12-07 12:32:27.164262: step 12310, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 86h:22m:50s remains)
INFO - root - 2017-12-07 12:32:36.922979: step 12320, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 83h:20m:46s remains)
INFO - root - 2017-12-07 12:32:46.727782: step 12330, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 89h:46m:40s remains)
INFO - root - 2017-12-07 12:32:56.337196: step 12340, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 88h:59m:05s remains)
INFO - root - 2017-12-07 12:33:05.879610: step 12350, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 86h:47m:16s remains)
INFO - root - 2017-12-07 12:33:15.493468: step 12360, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 87h:44m:34s remains)
INFO - root - 2017-12-07 12:33:25.004113: step 12370, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 84h:46m:54s remains)
INFO - root - 2017-12-07 12:33:34.890976: step 12380, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.011 sec/batch; 89h:53m:40s remains)
INFO - root - 2017-12-07 12:33:44.587245: step 12390, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.969 sec/batch; 86h:08m:59s remains)
INFO - root - 2017-12-07 12:33:54.182595: step 12400, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 85h:21m:17s remains)
2017-12-07 12:33:55.193585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2058468 -4.2502732 -4.2795343 -4.2865796 -4.2805562 -4.2681484 -4.2441006 -4.2088947 -4.1818266 -4.1798086 -4.1964831 -4.1986213 -4.1981335 -4.1743269 -4.1189876][-4.15764 -4.2276621 -4.2665668 -4.273726 -4.261013 -4.2407703 -4.2221613 -4.1984363 -4.1831117 -4.1823406 -4.1975484 -4.2004008 -4.2030716 -4.1840496 -4.1278419][-4.1297474 -4.2202606 -4.2606211 -4.2574105 -4.2342482 -4.2087536 -4.2005386 -4.1959863 -4.1992254 -4.204618 -4.2177548 -4.222578 -4.2264266 -4.2096586 -4.1556764][-4.1239057 -4.2209125 -4.2538543 -4.2385383 -4.2064075 -4.17747 -4.1780038 -4.1885123 -4.2037868 -4.2159085 -4.2294655 -4.2392912 -4.2461982 -4.2290649 -4.1753988][-4.1390443 -4.22548 -4.2475076 -4.2215643 -4.1840239 -4.1590996 -4.1679487 -4.1871018 -4.2037191 -4.216104 -4.23096 -4.2467523 -4.26032 -4.2407327 -4.1813121][-4.1690178 -4.2373695 -4.242548 -4.2022204 -4.1618404 -4.140533 -4.1535931 -4.1754003 -4.1882219 -4.1953855 -4.210649 -4.2346978 -4.2585626 -4.2417612 -4.1777124][-4.209641 -4.2515087 -4.2412415 -4.1929584 -4.150948 -4.1303315 -4.1402388 -4.1557083 -4.1628823 -4.1673684 -4.1856327 -4.2189951 -4.2502661 -4.2373204 -4.1764026][-4.2454834 -4.2641273 -4.246748 -4.2033234 -4.1683826 -4.1480556 -4.148725 -4.1510592 -4.1442223 -4.1428814 -4.1637087 -4.2034712 -4.2351723 -4.2253518 -4.1729922][-4.2725925 -4.2803969 -4.2658587 -4.2344112 -4.208992 -4.1917424 -4.1841516 -4.1729422 -4.1533089 -4.1423473 -4.1605029 -4.2007608 -4.23042 -4.2198038 -4.1730757][-4.2875991 -4.2941303 -4.2854786 -4.2662787 -4.2491527 -4.2386646 -4.2314539 -4.2143273 -4.1859756 -4.1669588 -4.1768527 -4.2102304 -4.2361093 -4.2255044 -4.1824584][-4.2918429 -4.2979779 -4.291749 -4.2794075 -4.2689943 -4.2651043 -4.2630334 -4.2516379 -4.2272563 -4.2072778 -4.2083645 -4.2296348 -4.2479053 -4.2387109 -4.2009459][-4.3014245 -4.3054533 -4.2992949 -4.2899776 -4.2813053 -4.2783632 -4.2808609 -4.2788348 -4.2632823 -4.2473993 -4.2459555 -4.2568541 -4.2664919 -4.2598562 -4.2314811][-4.313848 -4.317883 -4.312911 -4.3044558 -4.2967091 -4.2927532 -4.2947836 -4.2953758 -4.2877569 -4.2786436 -4.2790718 -4.2851992 -4.289391 -4.2848349 -4.2658491][-4.3231878 -4.3263717 -4.3237014 -4.317286 -4.3117313 -4.3071675 -4.306839 -4.3075695 -4.3053255 -4.3027925 -4.3046575 -4.3085079 -4.3106794 -4.3080807 -4.298563][-4.330317 -4.3323445 -4.332242 -4.3300576 -4.327929 -4.3252578 -4.3242249 -4.3244529 -4.3247886 -4.3249784 -4.3260193 -4.327116 -4.3268762 -4.325408 -4.3229108]]...]
INFO - root - 2017-12-07 12:34:05.060067: step 12410, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.977 sec/batch; 86h:54m:05s remains)
INFO - root - 2017-12-07 12:34:14.823839: step 12420, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 84h:41m:50s remains)
INFO - root - 2017-12-07 12:34:24.411212: step 12430, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 78h:41m:25s remains)
INFO - root - 2017-12-07 12:34:34.226045: step 12440, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 82h:11m:04s remains)
INFO - root - 2017-12-07 12:34:43.925955: step 12450, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 82h:38m:06s remains)
INFO - root - 2017-12-07 12:34:53.552754: step 12460, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.024 sec/batch; 91h:03m:55s remains)
INFO - root - 2017-12-07 12:35:03.228350: step 12470, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 87h:27m:01s remains)
INFO - root - 2017-12-07 12:35:12.903842: step 12480, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 89h:32m:22s remains)
INFO - root - 2017-12-07 12:35:22.541033: step 12490, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 89h:16m:59s remains)
INFO - root - 2017-12-07 12:35:32.221421: step 12500, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 88h:30m:31s remains)
2017-12-07 12:35:33.161758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2688217 -4.260397 -4.2378888 -4.2065587 -4.1782155 -4.1597981 -4.16261 -4.1863236 -4.2128963 -4.2264953 -4.2453985 -4.2651072 -4.2755122 -4.274344 -4.2623696][-4.2279291 -4.2219949 -4.2061796 -4.182014 -4.1580129 -4.1368546 -4.1382895 -4.1645117 -4.1971922 -4.2185225 -4.2445974 -4.2686834 -4.279562 -4.2759318 -4.2606273][-4.1895514 -4.1882806 -4.1793103 -4.1656518 -4.1544304 -4.1384444 -4.1349297 -4.1539235 -4.1823416 -4.2080016 -4.2422638 -4.2765017 -4.2938566 -4.295526 -4.2805266][-4.1779256 -4.1787958 -4.1726351 -4.1674066 -4.1640539 -4.1532478 -4.1422081 -4.1491861 -4.1717305 -4.1987576 -4.2369661 -4.2787666 -4.3017116 -4.3105097 -4.2990065][-4.2010255 -4.2001071 -4.1918893 -4.1832838 -4.1750622 -4.1597977 -4.1428242 -4.1398067 -4.1563249 -4.1820579 -4.2202439 -4.2670593 -4.294693 -4.3070393 -4.2974048][-4.231771 -4.2299623 -4.2223282 -4.2074137 -4.1862698 -4.1586671 -4.1342735 -4.1211576 -4.1330771 -4.1613975 -4.1996717 -4.2499533 -4.2805238 -4.29225 -4.282155][-4.2658715 -4.2636743 -4.2536235 -4.2276735 -4.1902757 -4.1473188 -4.1140962 -4.0932746 -4.1031647 -4.137639 -4.181035 -4.2364879 -4.2701077 -4.2789712 -4.2663851][-4.2813873 -4.2769051 -4.2635407 -4.2309217 -4.1853642 -4.1335449 -4.0918031 -4.0639014 -4.07287 -4.1153564 -4.1664829 -4.2259121 -4.2631326 -4.27563 -4.2648563][-4.2724667 -4.262589 -4.2466741 -4.2149673 -4.1738377 -4.1236215 -4.0786834 -4.0479779 -4.0583653 -4.1108241 -4.1673307 -4.2240634 -4.2639327 -4.2812343 -4.2747397][-4.2389088 -4.2229357 -4.2052889 -4.1777039 -4.1473908 -4.110548 -4.0774522 -4.0622077 -4.0863481 -4.1443553 -4.1968193 -4.2395406 -4.2695274 -4.282692 -4.27709][-4.1755166 -4.1585264 -4.147068 -4.1298227 -4.1124048 -4.0923305 -4.0828185 -4.0989676 -4.1447062 -4.20634 -4.2486725 -4.2695627 -4.2778664 -4.274899 -4.2635355][-4.112824 -4.1025257 -4.1045866 -4.1023245 -4.0979109 -4.09851 -4.11777 -4.16324 -4.2225709 -4.2778783 -4.3056779 -4.3054109 -4.2895827 -4.2675014 -4.2478447][-4.1032009 -4.1088309 -4.1267905 -4.1397543 -4.1487617 -4.1646261 -4.196125 -4.2465782 -4.2994452 -4.3366423 -4.3445 -4.3255358 -4.2948532 -4.2608271 -4.2307768][-4.1436148 -4.1617351 -4.187943 -4.2078171 -4.2165961 -4.2311773 -4.2564335 -4.2954407 -4.33539 -4.3555775 -4.3482342 -4.3229437 -4.29111 -4.25496 -4.2203903][-4.2056208 -4.2268872 -4.2531223 -4.2691278 -4.2683673 -4.268692 -4.2792034 -4.3033552 -4.3306689 -4.3381104 -4.3283858 -4.3077312 -4.2844162 -4.2582045 -4.2298422]]...]
INFO - root - 2017-12-07 12:35:42.973451: step 12510, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 89h:08m:38s remains)
INFO - root - 2017-12-07 12:35:52.602775: step 12520, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 85h:08m:07s remains)
INFO - root - 2017-12-07 12:36:02.242205: step 12530, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 83h:04m:12s remains)
INFO - root - 2017-12-07 12:36:11.823277: step 12540, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 86h:00m:19s remains)
INFO - root - 2017-12-07 12:36:21.568016: step 12550, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.975 sec/batch; 86h:39m:49s remains)
INFO - root - 2017-12-07 12:36:31.361813: step 12560, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 87h:48m:20s remains)
INFO - root - 2017-12-07 12:36:40.942986: step 12570, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 85h:35m:17s remains)
INFO - root - 2017-12-07 12:36:50.556697: step 12580, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 84h:00m:00s remains)
INFO - root - 2017-12-07 12:36:59.988303: step 12590, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 83h:21m:37s remains)
INFO - root - 2017-12-07 12:37:09.619376: step 12600, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.018 sec/batch; 90h:25m:21s remains)
2017-12-07 12:37:10.510782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2947974 -4.2917356 -4.2973933 -4.3048491 -4.3133121 -4.3178115 -4.3152103 -4.3087769 -4.3006248 -4.2937789 -4.2912283 -4.2890325 -4.2888384 -4.2845621 -4.2805686][-4.2840967 -4.2791333 -4.284359 -4.2949343 -4.3075747 -4.315815 -4.3150463 -4.3054333 -4.2953215 -4.2914777 -4.295784 -4.3018603 -4.301888 -4.2924514 -4.2835197][-4.2571607 -4.253263 -4.2600226 -4.2722306 -4.2849946 -4.2924776 -4.2921391 -4.28272 -4.2744851 -4.2781553 -4.2935061 -4.3106446 -4.314714 -4.3047776 -4.2936883][-4.2050405 -4.202672 -4.2136369 -4.2259426 -4.2348208 -4.2356353 -4.2324252 -4.2278495 -4.2315969 -4.2522068 -4.2805843 -4.3080616 -4.3203068 -4.3166504 -4.3056936][-4.1364741 -4.1354709 -4.1518574 -4.1630931 -4.1667681 -4.15545 -4.1460567 -4.146544 -4.16434 -4.2046804 -4.2484369 -4.2855606 -4.3085732 -4.3182082 -4.3130941][-4.0862112 -4.0802593 -4.0951424 -4.1006932 -4.092802 -4.0658188 -4.0467105 -4.0492296 -4.0754805 -4.130383 -4.1873913 -4.2343802 -4.2708397 -4.2961311 -4.3039536][-4.0933113 -4.080884 -4.0841823 -4.0805473 -4.059854 -4.0205626 -3.991688 -3.9840679 -4.0019865 -4.0561805 -4.12062 -4.1775808 -4.2281222 -4.2671976 -4.2879167][-4.1494317 -4.1315722 -4.1217742 -4.1114254 -4.0912986 -4.0586691 -4.0295062 -4.0053978 -3.9980245 -4.0346017 -4.09542 -4.1564035 -4.214344 -4.2578564 -4.2814579][-4.2134991 -4.1921005 -4.1734905 -4.1604462 -4.1462231 -4.127512 -4.109941 -4.0834036 -4.06099 -4.0796185 -4.1298413 -4.1847925 -4.2356563 -4.272028 -4.2886715][-4.2694845 -4.252264 -4.2321105 -4.2190084 -4.20854 -4.1968594 -4.187993 -4.1676908 -4.1442003 -4.1532617 -4.1910334 -4.2331839 -4.2676539 -4.2911673 -4.2983475][-4.3062596 -4.2982731 -4.2847748 -4.2781487 -4.2745171 -4.2674766 -4.2611189 -4.2463117 -4.2268386 -4.2291193 -4.251061 -4.2749887 -4.2909846 -4.3018031 -4.3018551][-4.3258691 -4.3271174 -4.3199048 -4.3193941 -4.3218074 -4.3184085 -4.3139844 -4.3051372 -4.2918687 -4.2894778 -4.2945075 -4.29887 -4.2988796 -4.3004985 -4.2965693][-4.3358064 -4.3407936 -4.338026 -4.3401713 -4.3449049 -4.3429213 -4.3398719 -4.3366179 -4.3298373 -4.3260221 -4.3213019 -4.3122292 -4.3012748 -4.2961869 -4.2893376][-4.3365612 -4.3441782 -4.344017 -4.3458915 -4.3488612 -4.3477931 -4.3477674 -4.3490381 -4.3475723 -4.3444085 -4.335288 -4.3202739 -4.3043046 -4.2949667 -4.2857561][-4.3275676 -4.3373752 -4.3396091 -4.34183 -4.3436365 -4.34409 -4.346694 -4.3503604 -4.3507538 -4.3481293 -4.3375325 -4.3216319 -4.3063483 -4.2969961 -4.2867208]]...]
INFO - root - 2017-12-07 12:37:20.210278: step 12610, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 84h:27m:29s remains)
INFO - root - 2017-12-07 12:37:29.913924: step 12620, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 86h:33m:01s remains)
INFO - root - 2017-12-07 12:37:39.497460: step 12630, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 82h:46m:31s remains)
INFO - root - 2017-12-07 12:37:49.143077: step 12640, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 82h:29m:49s remains)
INFO - root - 2017-12-07 12:37:58.852757: step 12650, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 84h:42m:45s remains)
INFO - root - 2017-12-07 12:38:08.593569: step 12660, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 85h:05m:31s remains)
INFO - root - 2017-12-07 12:38:18.288696: step 12670, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 85h:14m:55s remains)
INFO - root - 2017-12-07 12:38:27.922754: step 12680, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.986 sec/batch; 87h:35m:22s remains)
INFO - root - 2017-12-07 12:38:37.701574: step 12690, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 84h:05m:52s remains)
INFO - root - 2017-12-07 12:38:47.306628: step 12700, loss = 2.10, batch loss = 2.05 (9.0 examples/sec; 0.887 sec/batch; 78h:50m:06s remains)
2017-12-07 12:38:48.402480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3141 -4.3276839 -4.3329206 -4.3301191 -4.3252931 -4.3250623 -4.3283253 -4.3339067 -4.3427482 -4.3495626 -4.3523521 -4.3406525 -4.3053184 -4.2747564 -4.2513528][-4.3196354 -4.3335323 -4.3393197 -4.3364334 -4.3311558 -4.329854 -4.3283062 -4.3280854 -4.3312917 -4.3337836 -4.3331456 -4.3225293 -4.2898431 -4.2628608 -4.2399254][-4.3182244 -4.3349752 -4.3445268 -4.3440456 -4.3324142 -4.3223023 -4.3104386 -4.3014774 -4.2997408 -4.2984295 -4.2964535 -4.2881818 -4.2615128 -4.2413058 -4.2284942][-4.3225269 -4.3371477 -4.3445172 -4.3382277 -4.3144994 -4.2883 -4.2613487 -4.2387643 -4.2341061 -4.2319045 -4.2304034 -4.2282138 -4.213912 -4.2074156 -4.2094588][-4.3208508 -4.3266277 -4.320281 -4.2982936 -4.2576737 -4.2145071 -4.1660929 -4.1254587 -4.131217 -4.1443839 -4.1513696 -4.1617064 -4.1614385 -4.164916 -4.1776094][-4.2978244 -4.2879148 -4.2643404 -4.2248664 -4.1734552 -4.1201415 -4.042098 -3.9749231 -4.0016308 -4.0421882 -4.0693612 -4.1009045 -4.1168666 -4.1245751 -4.1411142][-4.2527733 -4.2214794 -4.1755385 -4.1208639 -4.0655165 -4.0042415 -3.8969238 -3.8037515 -3.8745837 -3.9641449 -4.0189977 -4.0701847 -4.0980988 -4.105545 -4.10991][-4.1954217 -4.1418452 -4.08409 -4.0271416 -3.977813 -3.9270325 -3.8460565 -3.7952764 -3.895684 -3.995482 -4.05073 -4.1011558 -4.1313763 -4.1302161 -4.1159048][-4.1484995 -4.0913229 -4.046268 -4.0075259 -3.9811614 -3.9715967 -3.9623938 -3.9720478 -4.0447369 -4.1060309 -4.1419468 -4.1814413 -4.2043877 -4.1959577 -4.1755352][-4.1334882 -4.1017613 -4.0911226 -4.0799828 -4.0720286 -4.0851169 -4.1073637 -4.1314082 -4.1722388 -4.201045 -4.2189207 -4.2431774 -4.2592435 -4.2522879 -4.2398872][-4.1560435 -4.1531806 -4.1712546 -4.1790314 -4.18112 -4.1993556 -4.2232871 -4.2387629 -4.2557487 -4.2676497 -4.274013 -4.28689 -4.2990737 -4.2984476 -4.2960663][-4.1783137 -4.1945381 -4.2281804 -4.2468138 -4.2545104 -4.2703052 -4.2868376 -4.2947683 -4.3012514 -4.3032165 -4.2994871 -4.3006759 -4.3064394 -4.3067069 -4.3080387][-4.1617403 -4.1903276 -4.2328057 -4.2584891 -4.2689939 -4.2786603 -4.2874823 -4.2890248 -4.2877569 -4.2827377 -4.2749529 -4.2700109 -4.2685747 -4.2676558 -4.27353][-4.1069789 -4.1392875 -4.1856532 -4.2128887 -4.2235923 -4.2283139 -4.231864 -4.2291312 -4.2239342 -4.2163568 -4.2071648 -4.1985226 -4.1933293 -4.1953716 -4.2054706][-4.0491138 -4.0766554 -4.1203022 -4.1441646 -4.1527634 -4.1532807 -4.1539569 -4.1508989 -4.146554 -4.1403751 -4.1331348 -4.1252327 -4.1201162 -4.1250429 -4.1367927]]...]
INFO - root - 2017-12-07 12:38:58.102128: step 12710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 79h:31m:47s remains)
INFO - root - 2017-12-07 12:39:07.757172: step 12720, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 82h:37m:43s remains)
INFO - root - 2017-12-07 12:39:17.317993: step 12730, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 84h:46m:43s remains)
INFO - root - 2017-12-07 12:39:27.066309: step 12740, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.018 sec/batch; 90h:22m:48s remains)
INFO - root - 2017-12-07 12:39:36.755321: step 12750, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 85h:50m:49s remains)
INFO - root - 2017-12-07 12:39:46.407304: step 12760, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.968 sec/batch; 85h:58m:08s remains)
INFO - root - 2017-12-07 12:39:56.271615: step 12770, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.027 sec/batch; 91h:15m:16s remains)
INFO - root - 2017-12-07 12:40:05.797222: step 12780, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.994 sec/batch; 88h:14m:57s remains)
INFO - root - 2017-12-07 12:40:15.559669: step 12790, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 87h:11m:26s remains)
INFO - root - 2017-12-07 12:40:25.113269: step 12800, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 86h:56m:34s remains)
2017-12-07 12:40:26.107203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2263389 -4.1640253 -4.0842328 -4.0005903 -3.9588256 -3.9906363 -4.0661473 -4.1474905 -4.2215309 -4.2802372 -4.3166766 -4.3313684 -4.3332806 -4.3325529 -4.3362956][-4.2414446 -4.2151041 -4.1750069 -4.1248426 -4.0881081 -4.0847778 -4.1196756 -4.1715345 -4.2303572 -4.2829213 -4.3185163 -4.3319297 -4.3320394 -4.3287487 -4.3312216][-4.2448034 -4.2433658 -4.23295 -4.2140312 -4.1945457 -4.183177 -4.1955261 -4.2234359 -4.2630682 -4.3026814 -4.3302331 -4.338253 -4.3338747 -4.3271723 -4.3265657][-4.2461138 -4.2552953 -4.25652 -4.2497439 -4.23472 -4.2184415 -4.2238278 -4.2464309 -4.2809825 -4.3153377 -4.3405166 -4.3457465 -4.33856 -4.3281837 -4.3225818][-4.2653222 -4.2645264 -4.2531834 -4.2354679 -4.2097464 -4.1866455 -4.1942263 -4.2267871 -4.2695165 -4.308538 -4.3373241 -4.3473125 -4.3428569 -4.3319817 -4.3229861][-4.292562 -4.275383 -4.2366195 -4.1874595 -4.1402612 -4.1067691 -4.1180377 -4.1657057 -4.2229733 -4.2758288 -4.3161263 -4.3388767 -4.3432484 -4.3353782 -4.3253493][-4.3066111 -4.2820778 -4.2288733 -4.1486769 -4.0633488 -4.0005012 -3.99975 -4.0607843 -4.1411018 -4.213891 -4.2705312 -4.311028 -4.3310819 -4.3324094 -4.3260975][-4.2997103 -4.2863083 -4.2422357 -4.15976 -4.0567117 -3.9565821 -3.9087129 -3.9540744 -4.0475407 -4.1391306 -4.2129455 -4.2708092 -4.3081408 -4.3240848 -4.324986][-4.2652721 -4.2712374 -4.257822 -4.2114615 -4.1387539 -4.0510349 -3.9765182 -3.9682155 -4.0251837 -4.1034589 -4.1779675 -4.2436347 -4.290659 -4.3161082 -4.3222027][-4.2179418 -4.2402983 -4.2589321 -4.2568841 -4.2302804 -4.1842885 -4.1321812 -4.1077914 -4.1205244 -4.1557713 -4.2017255 -4.2524476 -4.2919436 -4.3149972 -4.3205552][-4.1824822 -4.2168469 -4.2537365 -4.27819 -4.2827225 -4.269815 -4.2437944 -4.2273297 -4.2293782 -4.243556 -4.2651258 -4.2931027 -4.315547 -4.326612 -4.3255653][-4.1881142 -4.227088 -4.2665796 -4.2924542 -4.3017449 -4.2970037 -4.27877 -4.2657747 -4.2713318 -4.2884965 -4.3084145 -4.3307 -4.3448009 -4.3465919 -4.3373709][-4.2410359 -4.2715182 -4.2954121 -4.3032413 -4.2930608 -4.2741642 -4.2495437 -4.2358203 -4.248518 -4.2780704 -4.3080034 -4.337955 -4.3585763 -4.3619375 -4.3515267][-4.2910967 -4.3051386 -4.3097453 -4.2958646 -4.2605762 -4.2192883 -4.1810136 -4.1671386 -4.1889992 -4.2311444 -4.2776074 -4.3198962 -4.3509235 -4.3626356 -4.3573976][-4.3047628 -4.3020778 -4.2961516 -4.2724133 -4.222692 -4.1579947 -4.0947595 -4.0671463 -4.0946636 -4.15173 -4.2164869 -4.2760539 -4.32277 -4.347703 -4.3516293]]...]
INFO - root - 2017-12-07 12:40:35.921944: step 12810, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 87h:57m:45s remains)
INFO - root - 2017-12-07 12:40:45.491324: step 12820, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 87h:48m:04s remains)
INFO - root - 2017-12-07 12:40:55.143714: step 12830, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 84h:16m:18s remains)
INFO - root - 2017-12-07 12:41:04.794084: step 12840, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 82h:08m:21s remains)
INFO - root - 2017-12-07 12:41:14.532703: step 12850, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 89h:44m:51s remains)
INFO - root - 2017-12-07 12:41:24.285607: step 12860, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 87h:49m:23s remains)
INFO - root - 2017-12-07 12:41:33.717853: step 12870, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 84h:54m:10s remains)
INFO - root - 2017-12-07 12:41:43.402562: step 12880, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 83h:22m:55s remains)
INFO - root - 2017-12-07 12:41:53.245176: step 12890, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 85h:51m:25s remains)
INFO - root - 2017-12-07 12:42:02.918727: step 12900, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 83h:40m:12s remains)
2017-12-07 12:42:03.915624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2881637 -4.3015704 -4.3105903 -4.310914 -4.305409 -4.305469 -4.3098373 -4.3173418 -4.3221445 -4.3236928 -4.3178363 -4.3005767 -4.2723837 -4.2482519 -4.243053][-4.2683926 -4.2902756 -4.3057289 -4.30513 -4.2946124 -4.290554 -4.2904668 -4.2938285 -4.3029866 -4.3107862 -4.307035 -4.2840023 -4.24478 -4.2128882 -4.2092452][-4.2383704 -4.266892 -4.2885165 -4.290309 -4.2760153 -4.2650414 -4.2557669 -4.2492485 -4.26072 -4.2787905 -4.2845054 -4.2643046 -4.2211065 -4.1875691 -4.1837077][-4.2098255 -4.2404842 -4.2646637 -4.2660327 -4.2442203 -4.2198367 -4.1953378 -4.1775994 -4.1989121 -4.2364192 -4.2569761 -4.2456384 -4.2070456 -4.1788721 -4.1746168][-4.1914244 -4.2217312 -4.2432961 -4.2394791 -4.2081079 -4.1651349 -4.1152315 -4.0835423 -4.12409 -4.1891618 -4.2292833 -4.2315874 -4.202795 -4.1813035 -4.1745791][-4.1848145 -4.2149882 -4.2325487 -4.223423 -4.1833453 -4.1170788 -4.0323305 -3.9813292 -4.0450091 -4.1381512 -4.1990867 -4.219111 -4.2039824 -4.189013 -4.1794572][-4.1855888 -4.2179308 -4.2372737 -4.2286549 -4.1849122 -4.1042948 -3.9934657 -3.9227042 -4.0011487 -4.1072526 -4.1798415 -4.2139735 -4.2088223 -4.1956286 -4.1822333][-4.18791 -4.2246571 -4.2518411 -4.250402 -4.2109613 -4.132699 -4.0235181 -3.9534225 -4.018508 -4.1134677 -4.1811213 -4.2175512 -4.2187295 -4.2092485 -4.1969023][-4.1895237 -4.2257695 -4.2591691 -4.27051 -4.2437258 -4.1829786 -4.0992861 -4.0483303 -4.0889874 -4.1560893 -4.2068014 -4.2343817 -4.2386503 -4.2320361 -4.2215824][-4.1959372 -4.2242408 -4.2535839 -4.269825 -4.2594056 -4.2230411 -4.1707511 -4.1389885 -4.1589317 -4.1998773 -4.2304788 -4.2482162 -4.2545767 -4.2525387 -4.244863][-4.2062216 -4.2230792 -4.236146 -4.2448683 -4.2442069 -4.230494 -4.2056103 -4.1882491 -4.19749 -4.2206216 -4.2363958 -4.2440858 -4.2475858 -4.246532 -4.2411547][-4.2144189 -4.2208157 -4.2214565 -4.2219634 -4.2272334 -4.2279115 -4.222352 -4.2148824 -4.2147708 -4.2220287 -4.22546 -4.2252359 -4.2275591 -4.229147 -4.2279][-4.2145481 -4.2152739 -4.2197151 -4.2230434 -4.2302332 -4.2377915 -4.241065 -4.2392321 -4.2331262 -4.2246437 -4.217339 -4.21531 -4.2189641 -4.2234573 -4.2253466][-4.2018628 -4.2001209 -4.217494 -4.2356224 -4.2475133 -4.2549047 -4.2577157 -4.258513 -4.2502403 -4.23283 -4.2197838 -4.2174096 -4.2241569 -4.2319179 -4.23652][-4.1854711 -4.1764326 -4.2010612 -4.232564 -4.2506585 -4.25575 -4.2549253 -4.2589884 -4.2542367 -4.2382584 -4.2257247 -4.2248883 -4.231998 -4.2397361 -4.2451992]]...]
INFO - root - 2017-12-07 12:42:13.535157: step 12910, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 83h:19m:50s remains)
INFO - root - 2017-12-07 12:42:23.208173: step 12920, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 86h:40m:43s remains)
INFO - root - 2017-12-07 12:42:32.840646: step 12930, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 89h:37m:54s remains)
INFO - root - 2017-12-07 12:42:42.524438: step 12940, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 87h:55m:48s remains)
INFO - root - 2017-12-07 12:42:52.417014: step 12950, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 83h:15m:16s remains)
INFO - root - 2017-12-07 12:43:02.036203: step 12960, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.979 sec/batch; 86h:54m:16s remains)
INFO - root - 2017-12-07 12:43:11.653555: step 12970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 75h:58m:21s remains)
INFO - root - 2017-12-07 12:43:21.270254: step 12980, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 86h:05m:40s remains)
INFO - root - 2017-12-07 12:43:31.094669: step 12990, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 87h:39m:23s remains)
INFO - root - 2017-12-07 12:43:40.734682: step 13000, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.958 sec/batch; 85h:03m:38s remains)
2017-12-07 12:43:41.711085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2964859 -4.2967095 -4.3013873 -4.3079948 -4.313468 -4.317152 -4.3162227 -4.3063903 -4.2829256 -4.2565837 -4.2367153 -4.2283692 -4.2342734 -4.2475677 -4.2609334][-4.2917547 -4.2966275 -4.302897 -4.3095236 -4.3145943 -4.315619 -4.3097048 -4.2945986 -4.2647114 -4.2307262 -4.2015395 -4.1885676 -4.1970181 -4.2148581 -4.2376041][-4.2739153 -4.2872024 -4.2971668 -4.3020806 -4.3030453 -4.2975373 -4.2855353 -4.2682352 -4.2367287 -4.2006578 -4.1681662 -4.1528163 -4.16057 -4.1795325 -4.2097058][-4.252039 -4.2731323 -4.2848048 -4.2876968 -4.2844911 -4.2705455 -4.2534075 -4.2356977 -4.2068229 -4.1783705 -4.1506376 -4.1364403 -4.142529 -4.1606078 -4.1931176][-4.2328863 -4.2578096 -4.268507 -4.2692556 -4.2611613 -4.2405024 -4.2177024 -4.1979594 -4.1748834 -4.1580548 -4.1417 -4.1343031 -4.1419611 -4.1604991 -4.1920538][-4.2141404 -4.24087 -4.249526 -4.245338 -4.230185 -4.1997857 -4.1640258 -4.1373558 -4.1174908 -4.1121716 -4.1117172 -4.1209497 -4.1410422 -4.1673336 -4.2009964][-4.1931047 -4.2211432 -4.224298 -4.21004 -4.1839724 -4.1408019 -4.0919747 -4.0569563 -4.0396452 -4.0480514 -4.0655088 -4.0939693 -4.1300511 -4.1677418 -4.2069521][-4.1780953 -4.20333 -4.2023749 -4.1774616 -4.1320362 -4.0726957 -4.0124493 -3.973249 -3.9641345 -3.990571 -4.0278454 -4.0733237 -4.1215134 -4.1670861 -4.2077193][-4.1781235 -4.2008052 -4.1995397 -4.168705 -4.1113448 -4.0421066 -3.9788966 -3.9484639 -3.958771 -3.9980643 -4.03833 -4.0809464 -4.1259832 -4.1708679 -4.2104721][-4.1804757 -4.2028279 -4.2092156 -4.1896505 -4.1416545 -4.08021 -4.0239878 -4.001461 -4.016942 -4.0472784 -4.0720315 -4.1006484 -4.1359863 -4.178133 -4.2168584][-4.1813416 -4.204854 -4.2213011 -4.2200351 -4.1916075 -4.1447282 -4.0982571 -4.0762649 -4.0815516 -4.0920997 -4.0998688 -4.118557 -4.1517639 -4.1932316 -4.2294264][-4.1852531 -4.20549 -4.2248278 -4.2341852 -4.2221041 -4.1936622 -4.1614203 -4.1449041 -4.1457977 -4.1430349 -4.1388254 -4.1502991 -4.179985 -4.217586 -4.2471151][-4.1923337 -4.2070446 -4.2205052 -4.2268696 -4.2179532 -4.2028761 -4.1927748 -4.1948366 -4.2015429 -4.1966906 -4.1889057 -4.194263 -4.21545 -4.2436495 -4.2627506][-4.1960993 -4.2056346 -4.2062907 -4.2018209 -4.1904149 -4.1824737 -4.1922941 -4.2135968 -4.2315345 -4.2360659 -4.2336349 -4.2370133 -4.2500596 -4.2669683 -4.274333][-4.1885953 -4.1963983 -4.190731 -4.1822648 -4.1750412 -4.1731133 -4.1903195 -4.2200842 -4.248549 -4.2652788 -4.2707677 -4.2738743 -4.2804365 -4.286921 -4.2836418]]...]
INFO - root - 2017-12-07 12:43:51.313733: step 13010, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 84h:17m:42s remains)
INFO - root - 2017-12-07 12:44:01.182999: step 13020, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 81h:52m:34s remains)
INFO - root - 2017-12-07 12:44:10.694604: step 13030, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 86h:53m:22s remains)
INFO - root - 2017-12-07 12:44:20.209891: step 13040, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 82h:35m:55s remains)
INFO - root - 2017-12-07 12:44:29.842371: step 13050, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 85h:22m:34s remains)
INFO - root - 2017-12-07 12:44:39.597213: step 13060, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.031 sec/batch; 91h:26m:36s remains)
INFO - root - 2017-12-07 12:44:49.121348: step 13070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 82h:53m:39s remains)
INFO - root - 2017-12-07 12:44:58.906769: step 13080, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.014 sec/batch; 89h:56m:36s remains)
INFO - root - 2017-12-07 12:45:08.649728: step 13090, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 87h:59m:32s remains)
INFO - root - 2017-12-07 12:45:18.355031: step 13100, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 83h:47m:03s remains)
2017-12-07 12:45:19.288690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3377385 -4.3238416 -4.3090572 -4.3004093 -4.2963629 -4.2917237 -4.287374 -4.2861018 -4.2853994 -4.2773128 -4.2700672 -4.2689581 -4.2743211 -4.2780218 -4.27924][-4.3207903 -4.3010464 -4.2800794 -4.2639103 -4.2542868 -4.2455397 -4.2375441 -4.2377453 -4.2439923 -4.2378373 -4.2280974 -4.2291369 -4.23905 -4.2445 -4.2413149][-4.2936945 -4.2643223 -4.2317605 -4.2019553 -4.1799059 -4.1651616 -4.1552238 -4.1626115 -4.1781645 -4.1769667 -4.1674528 -4.1716137 -4.1870275 -4.1966863 -4.1943226][-4.2623639 -4.220037 -4.1730256 -4.1308951 -4.0965433 -4.074759 -4.0692539 -4.0863571 -4.104867 -4.1019282 -4.0946622 -4.1026521 -4.1283178 -4.1475739 -4.1508379][-4.2328415 -4.176517 -4.116334 -4.0647984 -4.018292 -3.9876282 -3.9850945 -4.0098152 -4.0287189 -4.0232029 -4.0109043 -4.0199938 -4.0537024 -4.0853047 -4.0993781][-4.2106175 -4.1458282 -4.0723338 -4.0108171 -3.9529188 -3.9118824 -3.9054966 -3.9342692 -3.9540455 -3.9491329 -3.9405229 -3.9569061 -3.998981 -4.038816 -4.0605707][-4.198504 -4.1237268 -4.040442 -3.9690928 -3.8984385 -3.8373008 -3.8117144 -3.8407664 -3.8734856 -3.8775258 -3.8861849 -3.9252992 -3.97985 -4.0219517 -4.0411291][-4.1844859 -4.0964503 -4.0035648 -3.9237559 -3.8393862 -3.7589841 -3.7230997 -3.7704601 -3.8341305 -3.8633692 -3.8934984 -3.9436307 -3.9980612 -4.0375967 -4.0510669][-4.1701326 -4.07332 -3.9788122 -3.903873 -3.8303876 -3.7619655 -3.7437634 -3.8140435 -3.89151 -3.9272232 -3.9633529 -4.0059748 -4.0548139 -4.0922322 -4.1035213][-4.1714272 -4.0866961 -4.0097837 -3.9536552 -3.9089692 -3.8680081 -3.86147 -3.9186516 -3.9744749 -3.9988861 -4.0290456 -4.06634 -4.11618 -4.1572704 -4.1766424][-4.1967 -4.1392 -4.0882826 -4.0494246 -4.0213375 -3.9931989 -3.9849222 -4.0188503 -4.0488892 -4.0592084 -4.0800433 -4.1179709 -4.1682444 -4.2102861 -4.23622][-4.2356281 -4.204391 -4.1756353 -4.14931 -4.1281419 -4.1098852 -4.1022115 -4.1226058 -4.1449285 -4.1551528 -4.17265 -4.204164 -4.2419739 -4.271091 -4.2892323][-4.2723193 -4.253582 -4.236228 -4.2173848 -4.2047358 -4.1992469 -4.1991229 -4.2181468 -4.240202 -4.2512751 -4.2638822 -4.2845054 -4.3039732 -4.3158097 -4.32431][-4.2919269 -4.2775269 -4.2671204 -4.2577248 -4.2542071 -4.258697 -4.2705112 -4.2917643 -4.309185 -4.3128238 -4.3142633 -4.3198404 -4.3257952 -4.3303814 -4.3358955][-4.3054051 -4.2917647 -4.2844319 -4.281302 -4.2832932 -4.2907596 -4.3029375 -4.3201051 -4.3320942 -4.3337326 -4.3324647 -4.3318882 -4.3320403 -4.33486 -4.3408375]]...]
INFO - root - 2017-12-07 12:45:29.037839: step 13110, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.020 sec/batch; 90h:30m:52s remains)
INFO - root - 2017-12-07 12:45:38.695921: step 13120, loss = 2.10, batch loss = 2.05 (8.0 examples/sec; 0.997 sec/batch; 88h:26m:38s remains)
INFO - root - 2017-12-07 12:45:48.317963: step 13130, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 81h:54m:24s remains)
INFO - root - 2017-12-07 12:45:57.948083: step 13140, loss = 2.11, batch loss = 2.06 (8.6 examples/sec; 0.934 sec/batch; 82h:50m:09s remains)
INFO - root - 2017-12-07 12:46:07.778919: step 13150, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 85h:47m:40s remains)
INFO - root - 2017-12-07 12:46:17.478602: step 13160, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 86h:19m:26s remains)
INFO - root - 2017-12-07 12:46:27.126594: step 13170, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 83h:36m:29s remains)
INFO - root - 2017-12-07 12:46:36.835530: step 13180, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 86h:10m:27s remains)
INFO - root - 2017-12-07 12:46:46.628538: step 13190, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 87h:06m:49s remains)
INFO - root - 2017-12-07 12:46:56.418374: step 13200, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.935 sec/batch; 82h:56m:10s remains)
2017-12-07 12:46:57.344727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2999439 -4.2959208 -4.2945871 -4.2930665 -4.2945271 -4.2985239 -4.3030825 -4.3030462 -4.2960424 -4.2920356 -4.2919211 -4.2951164 -4.2978272 -4.2998171 -4.3054347][-4.2883754 -4.28093 -4.2788172 -4.2760916 -4.2788672 -4.2847376 -4.287437 -4.2864528 -4.2800751 -4.2785587 -4.2799745 -4.2826538 -4.283371 -4.2846003 -4.2916136][-4.2640367 -4.2502608 -4.2428322 -4.2361078 -4.2400165 -4.2470307 -4.2492719 -4.2456427 -4.2449841 -4.2517152 -4.2563376 -4.2555218 -4.2532392 -4.2547975 -4.2608795][-4.2277069 -4.2066364 -4.1917391 -4.181293 -4.1820474 -4.1911793 -4.1967688 -4.1931891 -4.1994143 -4.2190614 -4.2322445 -4.2256866 -4.2169242 -4.2132144 -4.2191973][-4.1939945 -4.1671872 -4.1446433 -4.1303234 -4.1248336 -4.1354933 -4.1438632 -4.1420622 -4.1492567 -4.1790938 -4.2030959 -4.1940393 -4.1770449 -4.1682696 -4.1727409][-4.1728415 -4.1429429 -4.1112056 -4.0847917 -4.0682 -4.0778022 -4.09448 -4.0984721 -4.1004167 -4.13437 -4.168633 -4.1600981 -4.1366434 -4.1320724 -4.143271][-4.17079 -4.1416745 -4.1005449 -4.0626669 -4.0341063 -4.0374937 -4.0597963 -4.0722904 -4.0722842 -4.101656 -4.1384463 -4.1244774 -4.0962195 -4.1056414 -4.1268854][-4.1822152 -4.1623392 -4.1221032 -4.0789871 -4.044889 -4.0398636 -4.0598083 -4.079071 -4.0723658 -4.0866795 -4.1133919 -4.08683 -4.05329 -4.0795813 -4.1171584][-4.1938939 -4.1878905 -4.1593666 -4.1269436 -4.0972385 -4.0849957 -4.0970411 -4.1139169 -4.1006708 -4.1006603 -4.1095691 -4.0718212 -4.0404091 -4.0844469 -4.134841][-4.1943135 -4.1994219 -4.1922417 -4.1769738 -4.1586661 -4.1411018 -4.1436567 -4.1533647 -4.1389523 -4.1344366 -4.1315675 -4.1004524 -4.0811806 -4.1222553 -4.166472][-4.1940312 -4.2001109 -4.2032251 -4.2048326 -4.1993504 -4.1796389 -4.1704993 -4.1712489 -4.1618934 -4.1600866 -4.1573973 -4.1386223 -4.1265559 -4.15369 -4.1859083][-4.20065 -4.2023721 -4.205091 -4.2115111 -4.2109432 -4.1874309 -4.1683226 -4.1682553 -4.1707296 -4.17823 -4.1779118 -4.1646256 -4.1500964 -4.1683888 -4.187911][-4.20972 -4.2082052 -4.2080126 -4.2122288 -4.2135339 -4.1902061 -4.1728134 -4.1774178 -4.1873207 -4.1990414 -4.1981592 -4.1827736 -4.1629772 -4.1709285 -4.1754589][-4.2203503 -4.2209907 -4.2204394 -4.2213273 -4.2202582 -4.2029595 -4.1956635 -4.2065644 -4.2203512 -4.2313347 -4.2312627 -4.2162724 -4.1925268 -4.1898613 -4.1881385][-4.2355146 -4.2412343 -4.2456121 -4.2493372 -4.2474236 -4.235734 -4.2353215 -4.24815 -4.2616315 -4.2713032 -4.2740531 -4.2641454 -4.2429729 -4.2356143 -4.2315593]]...]
INFO - root - 2017-12-07 12:47:07.005290: step 13210, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 85h:41m:00s remains)
INFO - root - 2017-12-07 12:47:16.748818: step 13220, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.979 sec/batch; 86h:49m:45s remains)
INFO - root - 2017-12-07 12:47:26.432413: step 13230, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 85h:25m:19s remains)
INFO - root - 2017-12-07 12:47:36.104617: step 13240, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 87h:01m:56s remains)
INFO - root - 2017-12-07 12:47:45.824823: step 13250, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 86h:55m:51s remains)
INFO - root - 2017-12-07 12:47:55.476958: step 13260, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.017 sec/batch; 90h:09m:29s remains)
INFO - root - 2017-12-07 12:48:04.975088: step 13270, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 84h:16m:06s remains)
INFO - root - 2017-12-07 12:48:14.669129: step 13280, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 86h:07m:59s remains)
INFO - root - 2017-12-07 12:48:24.302071: step 13290, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.037 sec/batch; 91h:56m:38s remains)
INFO - root - 2017-12-07 12:48:33.981173: step 13300, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 82h:20m:36s remains)
2017-12-07 12:48:34.960004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2431812 -4.2439833 -4.238452 -4.227294 -4.2115946 -4.1891694 -4.1467061 -4.0771046 -4.0202003 -4.0129623 -4.0497627 -4.0944405 -4.1185112 -4.1074009 -4.0525417][-4.2675061 -4.2660389 -4.2558908 -4.2395396 -4.2202687 -4.2028766 -4.17424 -4.1229253 -4.0819454 -4.0746751 -4.092679 -4.11538 -4.1259356 -4.1199808 -4.0751243][-4.2715616 -4.2677946 -4.2543817 -4.2331471 -4.2096338 -4.1955781 -4.1783228 -4.1428924 -4.1166224 -4.1081934 -4.1084471 -4.1123915 -4.113513 -4.1139264 -4.0902667][-4.2605457 -4.2567568 -4.240664 -4.2122059 -4.18348 -4.1676416 -4.1512589 -4.1201692 -4.1024036 -4.09713 -4.0951257 -4.100534 -4.1069493 -4.113194 -4.1038704][-4.2467384 -4.2467833 -4.23365 -4.1982884 -4.1628604 -4.1423135 -4.1191759 -4.0845942 -4.0645747 -4.0568609 -4.0653958 -4.0878787 -4.1091843 -4.1202321 -4.1153603][-4.2350965 -4.2329078 -4.2163792 -4.1726046 -4.1374965 -4.11952 -4.0925384 -4.0571938 -4.0321012 -4.0143619 -4.0358582 -4.0803232 -4.1153193 -4.1334057 -4.1299667][-4.2041392 -4.1990929 -4.1760106 -4.1281543 -4.0991974 -4.093626 -4.0771804 -4.0454583 -4.0115585 -3.9836221 -4.0136151 -4.0675049 -4.1035814 -4.1263008 -4.13015][-4.1644697 -4.1534252 -4.1280403 -4.0846677 -4.0700073 -4.0766072 -4.073421 -4.0513177 -4.0200911 -3.9937167 -4.0180016 -4.0547528 -4.0790386 -4.1020889 -4.1152563][-4.1487246 -4.1383109 -4.112442 -4.0761833 -4.0765948 -4.0911593 -4.0978703 -4.0870304 -4.0652604 -4.0461345 -4.0548983 -4.0628948 -4.0657544 -4.0792184 -4.0968151][-4.160851 -4.1521506 -4.1222754 -4.0892253 -4.0966539 -4.1163511 -4.1278615 -4.1272297 -4.1194139 -4.112309 -4.1069241 -4.0897822 -4.0711246 -4.0691447 -4.0858674][-4.1750011 -4.1627612 -4.1287804 -4.1028605 -4.1154518 -4.1305089 -4.1369586 -4.1395631 -4.1441946 -4.1471229 -4.1380935 -4.1112862 -4.0856543 -4.0757461 -4.0902181][-4.1823306 -4.166625 -4.1336837 -4.108469 -4.113358 -4.1171288 -4.1182375 -4.12763 -4.146524 -4.15849 -4.1491079 -4.1244159 -4.1035185 -4.0948272 -4.1033816][-4.1982532 -4.1777387 -4.1452065 -4.1162462 -4.1047993 -4.0978394 -4.0998755 -4.1173177 -4.1471758 -4.1629605 -4.15246 -4.1317749 -4.1187139 -4.1136379 -4.1182237][-4.2123241 -4.1929736 -4.1646724 -4.1337256 -4.1073017 -4.0897417 -4.0894094 -4.1107082 -4.1441627 -4.1608443 -4.1535873 -4.1388016 -4.1300111 -4.1267424 -4.1315103][-4.2115393 -4.1954603 -4.1731982 -4.145402 -4.115273 -4.09343 -4.0928111 -4.1134143 -4.1422272 -4.1573586 -4.1568027 -4.1506548 -4.1446519 -4.1428623 -4.1490793]]...]
INFO - root - 2017-12-07 12:48:44.676329: step 13310, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.019 sec/batch; 90h:19m:47s remains)
INFO - root - 2017-12-07 12:48:54.379196: step 13320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 85h:00m:02s remains)
INFO - root - 2017-12-07 12:49:04.040067: step 13330, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.921 sec/batch; 81h:39m:53s remains)
INFO - root - 2017-12-07 12:49:13.830419: step 13340, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 85h:27m:29s remains)
INFO - root - 2017-12-07 12:49:23.576296: step 13350, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 89h:07m:57s remains)
INFO - root - 2017-12-07 12:49:33.175984: step 13360, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 76h:54m:46s remains)
INFO - root - 2017-12-07 12:49:42.789524: step 13370, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 86h:37m:52s remains)
INFO - root - 2017-12-07 12:49:52.288922: step 13380, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 87h:26m:08s remains)
INFO - root - 2017-12-07 12:50:01.836770: step 13390, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 85h:50m:49s remains)
INFO - root - 2017-12-07 12:50:11.381511: step 13400, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 84h:09m:37s remains)
2017-12-07 12:50:12.322941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3788204 -4.3834081 -4.3769107 -4.3562522 -4.3267746 -4.29462 -4.2600517 -4.2213964 -4.1827688 -4.1505203 -4.1403861 -4.1598978 -4.1971297 -4.2507629 -4.3102069][-4.3809924 -4.3824573 -4.36946 -4.3398371 -4.3015327 -4.2613411 -4.2239628 -4.1860495 -4.1551061 -4.1478972 -4.1617332 -4.1906042 -4.22604 -4.2732134 -4.3209867][-4.382731 -4.3800106 -4.3610291 -4.3235331 -4.2770948 -4.2317905 -4.1943154 -4.1596088 -4.1377091 -4.1507359 -4.1818461 -4.2163134 -4.2528872 -4.2975836 -4.3367324][-4.3830628 -4.3772454 -4.3524046 -4.3070049 -4.2533669 -4.2032175 -4.16999 -4.141819 -4.1339622 -4.1610708 -4.1987462 -4.2372694 -4.2767534 -4.3197188 -4.3512816][-4.3827777 -4.3755765 -4.3456306 -4.2923856 -4.2281036 -4.170476 -4.1328421 -4.106493 -4.1130648 -4.1531472 -4.1953564 -4.2405276 -4.2864728 -4.32807 -4.3544416][-4.3815041 -4.3733644 -4.3375511 -4.2738547 -4.1935153 -4.1181426 -4.0657997 -4.0386114 -4.0635004 -4.1218452 -4.1728821 -4.227499 -4.282969 -4.3265586 -4.353199][-4.3767781 -4.365694 -4.3219662 -4.2433691 -4.142858 -4.0418634 -3.9710846 -3.9543664 -4.012754 -4.09938 -4.1628494 -4.2229171 -4.2797594 -4.3237419 -4.3513846][-4.3717222 -4.3587937 -4.3077035 -4.2163696 -4.0990276 -3.9758677 -3.8937619 -3.8948462 -3.9859786 -4.0919681 -4.1666174 -4.228992 -4.2835979 -4.3251858 -4.3521671][-4.3715186 -4.3599138 -4.3085337 -4.2147994 -4.0908532 -3.959233 -3.878603 -3.8964009 -3.9989965 -4.1062555 -4.1820288 -4.2431221 -4.2956138 -4.3341255 -4.3586235][-4.3728552 -4.3637476 -4.3186951 -4.2312727 -4.1110225 -3.9853427 -3.9157372 -3.9446321 -4.0412288 -4.1397405 -4.21255 -4.2692394 -4.3164358 -4.3480735 -4.3670964][-4.3750958 -4.3688192 -4.3321137 -4.2575412 -4.1518521 -4.0459623 -3.9953656 -4.0280447 -4.1070175 -4.1880221 -4.2553263 -4.3054214 -4.3426785 -4.3649654 -4.3771672][-4.376914 -4.3726859 -4.3460708 -4.288826 -4.2021203 -4.1204538 -4.0908132 -4.1223722 -4.1773434 -4.2391005 -4.2971244 -4.3395333 -4.36649 -4.3802414 -4.3849835][-4.3746943 -4.3712687 -4.3512282 -4.3078337 -4.2413383 -4.1835618 -4.1731091 -4.2045145 -4.2451987 -4.2909393 -4.3335223 -4.3647666 -4.381464 -4.3873849 -4.3863497][-4.3671021 -4.3633757 -4.3485055 -4.3155951 -4.2660184 -4.2282381 -4.2305107 -4.261826 -4.2951765 -4.3290477 -4.3584843 -4.377974 -4.3860598 -4.3864641 -4.3812203][-4.3611212 -4.3579149 -4.3481932 -4.324749 -4.2892675 -4.2653213 -4.2722149 -4.2998462 -4.3276863 -4.3517365 -4.3701482 -4.3801756 -4.3819675 -4.379931 -4.3745141]]...]
INFO - root - 2017-12-07 12:50:22.083281: step 13410, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 85h:03m:52s remains)
INFO - root - 2017-12-07 12:50:31.693862: step 13420, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 86h:49m:28s remains)
INFO - root - 2017-12-07 12:50:41.294018: step 13430, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 86h:28m:55s remains)
INFO - root - 2017-12-07 12:50:50.949641: step 13440, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 83h:45m:13s remains)
INFO - root - 2017-12-07 12:51:00.712788: step 13450, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 86h:24m:17s remains)
INFO - root - 2017-12-07 12:51:10.361678: step 13460, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.030 sec/batch; 91h:18m:38s remains)
INFO - root - 2017-12-07 12:51:20.091459: step 13470, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 85h:25m:50s remains)
INFO - root - 2017-12-07 12:51:29.707901: step 13480, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 85h:22m:57s remains)
INFO - root - 2017-12-07 12:51:39.434509: step 13490, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 83h:41m:46s remains)
INFO - root - 2017-12-07 12:51:49.159921: step 13500, loss = 2.12, batch loss = 2.06 (7.9 examples/sec; 1.011 sec/batch; 89h:34m:40s remains)
2017-12-07 12:51:50.132936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3243809 -4.3187141 -4.3093672 -4.2993331 -4.2906718 -4.2845597 -4.2759781 -4.2598329 -4.246789 -4.2498455 -4.2594738 -4.2739153 -4.2888131 -4.2944283 -4.2929296][-4.3137355 -4.3105588 -4.3043532 -4.2990808 -4.2929382 -4.2849016 -4.2743182 -4.2523675 -4.2331319 -4.238131 -4.2486963 -4.2624736 -4.2764797 -4.2786188 -4.2738605][-4.2895231 -4.2947669 -4.3005843 -4.3044958 -4.3018484 -4.29386 -4.2791038 -4.2481036 -4.2216353 -4.2269535 -4.2399616 -4.2528811 -4.2625446 -4.2594724 -4.2506313][-4.2653241 -4.2811036 -4.3001432 -4.310967 -4.3105335 -4.298439 -4.2736831 -4.2308755 -4.191483 -4.1936975 -4.2120547 -4.232121 -4.2447748 -4.2423983 -4.2361546][-4.246171 -4.27068 -4.29611 -4.3064055 -4.3014989 -4.2839308 -4.2493849 -4.1958175 -4.1456394 -4.1420937 -4.1683636 -4.20387 -4.2277203 -4.2316656 -4.22825][-4.2337 -4.2586827 -4.2851377 -4.2932243 -4.2806911 -4.2520628 -4.2083154 -4.146543 -4.090467 -4.0820923 -4.1123791 -4.1661096 -4.2106657 -4.2324514 -4.2378192][-4.2327886 -4.2541237 -4.2792959 -4.2857208 -4.2613139 -4.2174392 -4.158607 -4.0843196 -4.0253868 -4.0111561 -4.0443239 -4.1166959 -4.1870184 -4.2345648 -4.2545738][-4.2456031 -4.2631984 -4.2829342 -4.2814221 -4.2441053 -4.1825428 -4.10634 -4.0191164 -3.95986 -3.9422638 -3.9751024 -4.0584369 -4.1500506 -4.219264 -4.2530303][-4.2659893 -4.2807307 -4.2926726 -4.2778792 -4.2269177 -4.1528149 -4.0684114 -3.9825583 -3.9361045 -3.9281297 -3.9567628 -4.0374718 -4.1343379 -4.2113414 -4.2549067][-4.2867956 -4.3035884 -4.3102403 -4.2842193 -4.22471 -4.1461411 -4.0610032 -3.9873791 -3.958869 -3.9666896 -3.9989018 -4.0727692 -4.15844 -4.2289748 -4.2701049][-4.3054471 -4.3231936 -4.3288355 -4.3018064 -4.2465496 -4.1767259 -4.100513 -4.0420318 -4.0260043 -4.0439067 -4.0793285 -4.1390557 -4.2036271 -4.25461 -4.2800622][-4.3063374 -4.320859 -4.3254404 -4.3043451 -4.25997 -4.2072911 -4.1509447 -4.1120358 -4.1045995 -4.1252255 -4.1605129 -4.2049923 -4.2458782 -4.2697778 -4.2722335][-4.2842221 -4.2941771 -4.29776 -4.2855849 -4.253036 -4.2183857 -4.1846342 -4.1683578 -4.1713209 -4.1948762 -4.2295365 -4.2630935 -4.2849736 -4.2827873 -4.2554622][-4.2432609 -4.2539978 -4.2594857 -4.2513881 -4.2276769 -4.2080159 -4.1927557 -4.1943622 -4.2134681 -4.2457705 -4.2787104 -4.3016167 -4.305553 -4.2809887 -4.2296066][-4.2046256 -4.2181435 -4.2233915 -4.212791 -4.1904182 -4.1775851 -4.1754174 -4.191987 -4.2259974 -4.2688823 -4.3011422 -4.3162956 -4.3048458 -4.2609277 -4.1944671]]...]
INFO - root - 2017-12-07 12:51:59.825892: step 13510, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 82h:12m:17s remains)
INFO - root - 2017-12-07 12:52:09.436282: step 13520, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 86h:03m:49s remains)
INFO - root - 2017-12-07 12:52:19.148373: step 13530, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 88h:14m:09s remains)
INFO - root - 2017-12-07 12:52:28.772859: step 13540, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 81h:15m:08s remains)
INFO - root - 2017-12-07 12:52:38.405559: step 13550, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 86h:43m:53s remains)
INFO - root - 2017-12-07 12:52:47.984366: step 13560, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 84h:19m:45s remains)
INFO - root - 2017-12-07 12:52:57.829072: step 13570, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 86h:38m:49s remains)
INFO - root - 2017-12-07 12:53:07.561556: step 13580, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.010 sec/batch; 89h:27m:20s remains)
INFO - root - 2017-12-07 12:53:17.095932: step 13590, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 87h:18m:21s remains)
INFO - root - 2017-12-07 12:53:26.681838: step 13600, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 81h:56m:41s remains)
2017-12-07 12:53:27.678557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9707572 -4.0357232 -4.1164856 -4.18931 -4.2400155 -4.2661152 -4.2763295 -4.2746391 -4.2464852 -4.1822691 -4.1052856 -4.0181441 -3.9477057 -3.9373698 -3.9833128][-4.0269551 -4.0676165 -4.1210995 -4.1648951 -4.1970911 -4.2189603 -4.2371759 -4.2556429 -4.260263 -4.2364836 -4.1918421 -4.1199102 -4.0546064 -4.0354276 -4.0533895][-4.118506 -4.1373572 -4.15415 -4.15751 -4.1545467 -4.1564589 -4.1782632 -4.2198529 -4.2590389 -4.2755437 -4.2619519 -4.2129946 -4.1651649 -4.1488924 -4.1481266][-4.1975694 -4.2042 -4.1905756 -4.1536288 -4.1099339 -4.0859113 -4.1061683 -4.1653986 -4.2329597 -4.2825656 -4.2956228 -4.2682285 -4.23625 -4.2250633 -4.2146735][-4.2578454 -4.2482162 -4.2052965 -4.1337357 -4.0609455 -4.01662 -4.0257845 -4.089344 -4.175776 -4.2504773 -4.2945528 -4.2970767 -4.2812481 -4.2726612 -4.2555661][-4.2978439 -4.26746 -4.196229 -4.1031284 -4.0181375 -3.9586351 -3.9468658 -3.9984949 -4.0866733 -4.17612 -4.247879 -4.2887726 -4.3019571 -4.3019333 -4.2786641][-4.3067093 -4.2654271 -4.1862364 -4.0937467 -4.0109224 -3.9461806 -3.9161992 -3.945611 -4.0170794 -4.1057968 -4.191494 -4.2590051 -4.2956386 -4.3056345 -4.2846961][-4.3054819 -4.2691789 -4.2046003 -4.1320586 -4.0612922 -4.0042429 -3.9750876 -3.9907346 -4.03592 -4.1017809 -4.1710587 -4.2346864 -4.2792444 -4.3002329 -4.2952037][-4.3087888 -4.2874656 -4.246201 -4.1956458 -4.1383238 -4.0933714 -4.0750456 -4.0864573 -4.1071434 -4.1372328 -4.1716008 -4.2110014 -4.2523093 -4.285058 -4.30239][-4.3080335 -4.3039255 -4.2858424 -4.2567534 -4.2145877 -4.1825457 -4.1741095 -4.181026 -4.1810384 -4.1769586 -4.17219 -4.1787419 -4.2071104 -4.2487669 -4.2897563][-4.2973123 -4.3063159 -4.306015 -4.2956176 -4.2712746 -4.251452 -4.2466803 -4.244091 -4.2210641 -4.1858068 -4.1474428 -4.1251345 -4.1407347 -4.1897798 -4.2484593][-4.2791743 -4.2946768 -4.3065104 -4.3110723 -4.3046527 -4.296576 -4.2915111 -4.275486 -4.2267632 -4.1609244 -4.0967016 -4.0547042 -4.0607014 -4.1147218 -4.1860723][-4.2647371 -4.2794161 -4.2957854 -4.3079114 -4.3111181 -4.3109708 -4.3033957 -4.2693462 -4.1958261 -4.1084766 -4.0321732 -3.9807448 -3.9757118 -4.0272756 -4.0988421][-4.2632689 -4.2727084 -4.2873292 -4.30229 -4.3100781 -4.3118711 -4.2993641 -4.2513962 -4.1621375 -4.0668416 -3.9864724 -3.9267311 -3.9051864 -3.9425275 -4.0055742][-4.2766461 -4.2799497 -4.2889643 -4.3004026 -4.309526 -4.31124 -4.2981539 -4.2513447 -4.16989 -4.0845957 -4.0078626 -3.9458609 -3.9124422 -3.930367 -3.9755731]]...]
INFO - root - 2017-12-07 12:53:37.513385: step 13610, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.006 sec/batch; 89h:04m:38s remains)
INFO - root - 2017-12-07 12:53:47.256949: step 13620, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 81h:41m:06s remains)
INFO - root - 2017-12-07 12:53:57.016394: step 13630, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 82h:37m:50s remains)
INFO - root - 2017-12-07 12:54:06.632465: step 13640, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 86h:12m:12s remains)
INFO - root - 2017-12-07 12:54:16.273048: step 13650, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 86h:27m:59s remains)
INFO - root - 2017-12-07 12:54:25.869121: step 13660, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 82h:39m:31s remains)
INFO - root - 2017-12-07 12:54:35.557939: step 13670, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 83h:34m:32s remains)
INFO - root - 2017-12-07 12:54:45.339073: step 13680, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.991 sec/batch; 87h:46m:16s remains)
INFO - root - 2017-12-07 12:54:55.198221: step 13690, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 89h:36m:57s remains)
INFO - root - 2017-12-07 12:55:04.895073: step 13700, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.986 sec/batch; 87h:20m:52s remains)
2017-12-07 12:55:05.900845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1702709 -4.17243 -4.1794124 -4.1998825 -4.2176261 -4.2243233 -4.228653 -4.233428 -4.2304192 -4.2250943 -4.23135 -4.232944 -4.2210388 -4.2072186 -4.2116618][-4.1513805 -4.1581326 -4.1708026 -4.1968861 -4.2158737 -4.2206812 -4.2243414 -4.2290154 -4.2222095 -4.2120786 -4.2082372 -4.2067752 -4.202076 -4.1964993 -4.2055311][-4.1355295 -4.1531105 -4.1720681 -4.2015734 -4.2220092 -4.2246938 -4.2261634 -4.2276564 -4.2170382 -4.1994119 -4.18077 -4.1720181 -4.1768446 -4.1784096 -4.1892343][-4.1269641 -4.1580529 -4.1857681 -4.2151079 -4.2311487 -4.2285419 -4.2204871 -4.2130961 -4.2000813 -4.1830745 -4.1600213 -4.1513219 -4.1612921 -4.1715813 -4.1852226][-4.1357517 -4.1690812 -4.199245 -4.2240434 -4.231545 -4.2233753 -4.2058687 -4.1838427 -4.1664567 -4.1564708 -4.1393466 -4.13386 -4.1447043 -4.1612906 -4.1815181][-4.1619616 -4.1855917 -4.2127204 -4.2327118 -4.2280445 -4.20499 -4.1696591 -4.129868 -4.1123967 -4.1145706 -4.1082697 -4.1135082 -4.1296282 -4.1510892 -4.1786852][-4.1967468 -4.2080393 -4.2254763 -4.2354136 -4.214488 -4.1657047 -4.0960569 -4.0292635 -4.0251245 -4.0596118 -4.0744805 -4.0949721 -4.1141758 -4.13463 -4.1657529][-4.2059016 -4.2054152 -4.2156777 -4.2192559 -4.1840882 -4.1103868 -4.0029244 -3.9146938 -3.9459944 -4.0299244 -4.0745878 -4.0971231 -4.1070075 -4.1180315 -4.1446066][-4.2007093 -4.1927304 -4.197732 -4.1998372 -4.1642385 -4.0971956 -4.0052238 -3.9372129 -3.9875603 -4.0781527 -4.1257644 -4.1382713 -4.1329584 -4.1322818 -4.1484251][-4.2090964 -4.2013664 -4.20525 -4.210135 -4.187675 -4.1502209 -4.1028333 -4.0722723 -4.10495 -4.1623597 -4.1913218 -4.1919913 -4.1779284 -4.169239 -4.1758704][-4.2224579 -4.2224331 -4.2304335 -4.2352314 -4.2240853 -4.2086492 -4.1883903 -4.1738825 -4.1871672 -4.2127175 -4.2225947 -4.21867 -4.20337 -4.1919374 -4.1966524][-4.2365332 -4.2409444 -4.2454791 -4.2449026 -4.2379432 -4.2296457 -4.2152081 -4.2041626 -4.2078996 -4.2155871 -4.2193689 -4.2205939 -4.2117162 -4.2021894 -4.2030039][-4.2487292 -4.2509208 -4.2467079 -4.2381129 -4.2279377 -4.2176766 -4.2041368 -4.1968465 -4.1991787 -4.2052827 -4.2166915 -4.2282915 -4.2257152 -4.2178092 -4.2148771][-4.2559767 -4.2504277 -4.2368407 -4.2205548 -4.2075977 -4.1960607 -4.1879091 -4.1882157 -4.1947131 -4.206893 -4.2250142 -4.2419682 -4.24554 -4.2417803 -4.2371473][-4.2515197 -4.2375431 -4.2201424 -4.2026782 -4.1932878 -4.1904111 -4.1886687 -4.1927619 -4.2033548 -4.2154994 -4.22982 -4.242352 -4.2478428 -4.2483706 -4.2446361]]...]
INFO - root - 2017-12-07 12:55:15.736825: step 13710, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.028 sec/batch; 91h:03m:24s remains)
INFO - root - 2017-12-07 12:55:25.455490: step 13720, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 85h:25m:35s remains)
INFO - root - 2017-12-07 12:55:35.080259: step 13730, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 82h:32m:05s remains)
INFO - root - 2017-12-07 12:55:44.720507: step 13740, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 85h:19m:22s remains)
INFO - root - 2017-12-07 12:55:54.276563: step 13750, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 80h:38m:55s remains)
INFO - root - 2017-12-07 12:56:03.894200: step 13760, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.020 sec/batch; 90h:17m:54s remains)
INFO - root - 2017-12-07 12:56:13.609679: step 13770, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 86h:08m:08s remains)
INFO - root - 2017-12-07 12:56:23.392268: step 13780, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 85h:40m:30s remains)
INFO - root - 2017-12-07 12:56:32.964862: step 13790, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 83h:31m:25s remains)
INFO - root - 2017-12-07 12:56:42.511382: step 13800, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 85h:07m:09s remains)
2017-12-07 12:56:43.490207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2694912 -4.2566504 -4.2558331 -4.2661266 -4.2702017 -4.2586308 -4.2457757 -4.2403641 -4.2422633 -4.2585049 -4.288434 -4.309392 -4.3134732 -4.3066607 -4.2946653][-4.2502894 -4.2246151 -4.2157574 -4.232079 -4.2428603 -4.23416 -4.216033 -4.1954803 -4.1822958 -4.2006903 -4.2462635 -4.27838 -4.2843351 -4.279882 -4.272253][-4.23328 -4.2006817 -4.1901112 -4.2060504 -4.2154846 -4.2081747 -4.1844769 -4.1464443 -4.1089039 -4.1255627 -4.1933308 -4.2430162 -4.2582183 -4.2600355 -4.2558913][-4.2196512 -4.186676 -4.17672 -4.1843734 -4.1857061 -4.1757727 -4.1453671 -4.0807204 -4.0092874 -4.0259075 -4.1256037 -4.1991043 -4.22753 -4.2382994 -4.2392321][-4.2124586 -4.1777158 -4.1652956 -4.1647744 -4.1586852 -4.1470737 -4.1092806 -4.009676 -3.8967357 -3.915163 -4.0562229 -4.1600146 -4.2069764 -4.228332 -4.2310085][-4.2217288 -4.1903529 -4.1761723 -4.1649165 -4.1489506 -4.1371732 -4.0949783 -3.9695127 -3.8194118 -3.8287587 -4.0015292 -4.132391 -4.1995611 -4.2304859 -4.2305861][-4.2457452 -4.2257891 -4.2139196 -4.1875968 -4.1575055 -4.1390781 -4.1021323 -3.97976 -3.827009 -3.821691 -3.9900324 -4.1267157 -4.2024918 -4.2329049 -4.2301][-4.26781 -4.2612886 -4.2550836 -4.2206054 -4.1786971 -4.1561022 -4.1376882 -4.0448322 -3.9340537 -3.9287117 -4.0499697 -4.1528893 -4.2127671 -4.2339644 -4.2263293][-4.2805271 -4.2832804 -4.2813983 -4.2453165 -4.1990318 -4.1801944 -4.1857777 -4.1272383 -4.0603104 -4.0634518 -4.1371036 -4.1952596 -4.2248464 -4.23051 -4.2199278][-4.2867832 -4.297286 -4.3049421 -4.2711406 -4.2276397 -4.2102423 -4.229229 -4.2049732 -4.1693697 -4.1768641 -4.2177787 -4.24017 -4.236609 -4.2239685 -4.2127013][-4.2963629 -4.3100672 -4.32318 -4.3017173 -4.2705522 -4.2555051 -4.2738233 -4.2722511 -4.2575936 -4.2612505 -4.2777996 -4.2721367 -4.245935 -4.221343 -4.2142673][-4.3102593 -4.3219533 -4.3305578 -4.3156276 -4.2968793 -4.2876253 -4.3006263 -4.3049607 -4.3009977 -4.3003697 -4.299861 -4.27963 -4.2407861 -4.2128553 -4.2183347][-4.31496 -4.3231707 -4.3276672 -4.3164968 -4.30565 -4.3026328 -4.3129082 -4.3219657 -4.3274851 -4.3301868 -4.324245 -4.2982326 -4.2563257 -4.234602 -4.2502251][-4.3104162 -4.3109593 -4.3071008 -4.2991991 -4.2975965 -4.3051796 -4.318481 -4.3322277 -4.3419452 -4.3495054 -4.3417373 -4.3149757 -4.2807741 -4.264452 -4.278121][-4.3025632 -4.2935185 -4.2816572 -4.2755847 -4.2810807 -4.2958975 -4.3129659 -4.3281269 -4.3375039 -4.3445659 -4.341259 -4.3223357 -4.2985892 -4.2863326 -4.2936]]...]
INFO - root - 2017-12-07 12:56:53.187388: step 13810, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 84h:35m:13s remains)
INFO - root - 2017-12-07 12:57:02.971645: step 13820, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 86h:32m:22s remains)
INFO - root - 2017-12-07 12:57:12.643182: step 13830, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 89h:50m:51s remains)
INFO - root - 2017-12-07 12:57:22.349842: step 13840, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 86h:17m:41s remains)
INFO - root - 2017-12-07 12:57:31.907123: step 13850, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 84h:21m:04s remains)
INFO - root - 2017-12-07 12:57:41.633143: step 13860, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 88h:18m:39s remains)
INFO - root - 2017-12-07 12:57:51.291639: step 13870, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 85h:36m:07s remains)
INFO - root - 2017-12-07 12:58:00.686367: step 13880, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 83h:30m:28s remains)
INFO - root - 2017-12-07 12:58:10.468211: step 13890, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 87h:09m:27s remains)
INFO - root - 2017-12-07 12:58:20.279568: step 13900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 84h:37m:16s remains)
2017-12-07 12:58:21.267076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2495294 -4.2490325 -4.2493649 -4.2515397 -4.2616239 -4.274066 -4.2815661 -4.2805047 -4.2765636 -4.2695589 -4.2643147 -4.27154 -4.2835808 -4.2917666 -4.2930765][-4.2446957 -4.2442775 -4.2406135 -4.2371483 -4.24335 -4.2550297 -4.2662849 -4.2712011 -4.2722316 -4.2681952 -4.266356 -4.2742739 -4.2857084 -4.2955885 -4.2975521][-4.2489057 -4.2432303 -4.2299519 -4.216167 -4.2172775 -4.2289166 -4.2455959 -4.2612066 -4.2707658 -4.2726512 -4.2764063 -4.284451 -4.2911482 -4.2985134 -4.2996869][-4.2485943 -4.2361479 -4.2077742 -4.1739144 -4.1581593 -4.1596203 -4.175149 -4.2005634 -4.2244229 -4.2419434 -4.2625031 -4.2792258 -4.2865252 -4.293777 -4.2947931][-4.2504411 -4.2286329 -4.182107 -4.124507 -4.083324 -4.0615721 -4.062964 -4.0879512 -4.1283631 -4.1730342 -4.2233591 -4.2626004 -4.2806692 -4.2908177 -4.2931519][-4.2539792 -4.2262878 -4.1713524 -4.1017447 -4.0396433 -3.9891641 -3.9632008 -3.9721241 -4.018858 -4.0894246 -4.1697965 -4.2348404 -4.2690988 -4.2838125 -4.2869992][-4.2605472 -4.2378902 -4.1920142 -4.1294866 -4.06304 -3.9969075 -3.9499693 -3.9364386 -3.9731107 -4.046895 -4.1364269 -4.21442 -4.2575164 -4.2747378 -4.2789845][-4.2652678 -4.2533684 -4.2247238 -4.1825051 -4.1336751 -4.0798588 -4.0314045 -4.0013885 -4.0141811 -4.0678692 -4.140471 -4.2106576 -4.2522116 -4.2674942 -4.2729373][-4.2601624 -4.2607303 -4.2486067 -4.2278557 -4.2051988 -4.1771092 -4.1443787 -4.1106863 -4.1045241 -4.1337428 -4.1804624 -4.2307887 -4.2611294 -4.2695484 -4.2737842][-4.2437949 -4.2521553 -4.2500229 -4.2437477 -4.240252 -4.23312 -4.2173486 -4.1950207 -4.1871362 -4.2032089 -4.2298074 -4.2603292 -4.279799 -4.2812362 -4.280333][-4.219337 -4.2227206 -4.220345 -4.2198353 -4.2277231 -4.2370191 -4.238142 -4.2319837 -4.2317138 -4.2437415 -4.2578855 -4.2742009 -4.2849035 -4.2804856 -4.272471][-4.2024603 -4.1957126 -4.1858077 -4.1842036 -4.1959944 -4.2148938 -4.2275157 -4.232173 -4.2393007 -4.2523503 -4.2625074 -4.2713366 -4.2760592 -4.267982 -4.2561235][-4.2065516 -4.1943431 -4.1783576 -4.173017 -4.1825 -4.2011452 -4.2149048 -4.2222347 -4.2309189 -4.2418995 -4.2486477 -4.2531805 -4.2545686 -4.2464752 -4.2357965][-4.2361364 -4.2287831 -4.2139492 -4.20706 -4.211462 -4.2216949 -4.2281885 -4.2325935 -4.23953 -4.248208 -4.2528219 -4.2559338 -4.2568703 -4.2510128 -4.2431111][-4.273077 -4.2757449 -4.2687125 -4.2639294 -4.2640514 -4.2659168 -4.2651882 -4.2658186 -4.2690263 -4.2733154 -4.2756333 -4.2780733 -4.2792559 -4.2762651 -4.2716823]]...]
INFO - root - 2017-12-07 12:58:30.690713: step 13910, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 82h:14m:50s remains)
INFO - root - 2017-12-07 12:58:40.417929: step 13920, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 87h:09m:45s remains)
INFO - root - 2017-12-07 12:58:50.114471: step 13930, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 86h:29m:37s remains)
INFO - root - 2017-12-07 12:58:59.704814: step 13940, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:17m:51s remains)
INFO - root - 2017-12-07 12:59:09.145886: step 13950, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 87h:34m:47s remains)
INFO - root - 2017-12-07 12:59:18.754446: step 13960, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 85h:46m:07s remains)
INFO - root - 2017-12-07 12:59:28.458238: step 13970, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 78h:57m:42s remains)
INFO - root - 2017-12-07 12:59:38.112409: step 13980, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.022 sec/batch; 90h:23m:06s remains)
INFO - root - 2017-12-07 12:59:47.835841: step 13990, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 88h:12m:08s remains)
INFO - root - 2017-12-07 12:59:57.468034: step 14000, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.970 sec/batch; 85h:46m:37s remains)
2017-12-07 12:59:58.383780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.267036 -4.2653284 -4.260582 -4.25265 -4.2460227 -4.2444258 -4.2620249 -4.2817388 -4.2853842 -4.2673173 -4.2334261 -4.205265 -4.2008305 -4.2131476 -4.2237859][-4.2643709 -4.2631292 -4.2568455 -4.2481174 -4.2459626 -4.2504025 -4.2679858 -4.2815294 -4.2720523 -4.2387323 -4.1905513 -4.1558371 -4.1526589 -4.1690531 -4.1898093][-4.2506037 -4.2492313 -4.2419 -4.2345243 -4.2369037 -4.2456112 -4.2569766 -4.259963 -4.2391257 -4.1866274 -4.1236267 -4.0901365 -4.0975137 -4.1284328 -4.1640058][-4.2377243 -4.2400975 -4.2397752 -4.2383938 -4.2451272 -4.2506652 -4.2483912 -4.2307444 -4.1895 -4.1219926 -4.0591211 -4.0449114 -4.0763197 -4.1232481 -4.1662545][-4.2285371 -4.2340975 -4.2361841 -4.2362561 -4.2430553 -4.2376766 -4.2126856 -4.1707425 -4.115046 -4.0505605 -4.0186243 -4.0467854 -4.0989275 -4.1461968 -4.1811881][-4.2238955 -4.227499 -4.2241015 -4.2179542 -4.212337 -4.1872764 -4.1280475 -4.0510573 -3.9855342 -3.9523413 -3.9814641 -4.0532193 -4.1150408 -4.155499 -4.1761436][-4.2144337 -4.2110486 -4.1993957 -4.1785069 -4.1492829 -4.0925822 -3.9890885 -3.8764737 -3.8273034 -3.8564656 -3.9379191 -4.0260878 -4.0866628 -4.1191888 -4.128356][-4.2041435 -4.1940484 -4.1731992 -4.1361628 -4.084897 -4.0023446 -3.8714304 -3.7623112 -3.7748122 -3.8587346 -3.950156 -4.0251455 -4.070919 -4.0862975 -4.0846376][-4.2280831 -4.2231979 -4.2028174 -4.1608582 -4.1006703 -4.0181661 -3.9059596 -3.8478162 -3.8960748 -3.9727731 -4.0322242 -4.0787587 -4.1031752 -4.1079178 -4.1048937][-4.2651935 -4.26521 -4.2456818 -4.2068233 -4.1511874 -4.0893731 -4.0228939 -4.0171371 -4.0688629 -4.1170893 -4.146502 -4.1677432 -4.1744814 -4.1722374 -4.168469][-4.2981839 -4.3021049 -4.2876797 -4.2568879 -4.2139158 -4.1767321 -4.149704 -4.16668 -4.2046056 -4.2264352 -4.2341905 -4.2395911 -4.2401137 -4.2391362 -4.2359385][-4.3240328 -4.3290534 -4.3212433 -4.3016815 -4.2716331 -4.2504826 -4.2432981 -4.263495 -4.28751 -4.2934184 -4.2891188 -4.2877383 -4.2881355 -4.2888775 -4.2865248][-4.3351097 -4.3380094 -4.3343482 -4.3235569 -4.3029661 -4.2906747 -4.2919583 -4.3077397 -4.3228116 -4.3235707 -4.3162456 -4.3136721 -4.3124013 -4.31037 -4.3069072][-4.332644 -4.3324757 -4.3307414 -4.3232193 -4.3100729 -4.3032427 -4.3073912 -4.3168259 -4.3245039 -4.3234529 -4.3175244 -4.3138747 -4.3108263 -4.3063307 -4.3022003][-4.3252006 -4.3209953 -4.3175583 -4.3113647 -4.303545 -4.3012152 -4.3044786 -4.3086224 -4.3114977 -4.31135 -4.3100858 -4.3089685 -4.3076158 -4.3044043 -4.3014088]]...]
INFO - root - 2017-12-07 13:00:07.886765: step 14010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 83h:27m:57s remains)
INFO - root - 2017-12-07 13:00:17.442889: step 14020, loss = 2.11, batch loss = 2.05 (7.9 examples/sec; 1.013 sec/batch; 89h:35m:12s remains)
INFO - root - 2017-12-07 13:00:27.080095: step 14030, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 85h:04m:10s remains)
INFO - root - 2017-12-07 13:00:36.605244: step 14040, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 83h:27m:00s remains)
INFO - root - 2017-12-07 13:00:45.909523: step 14050, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 83h:41m:37s remains)
INFO - root - 2017-12-07 13:00:55.484420: step 14060, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.970 sec/batch; 85h:45m:36s remains)
INFO - root - 2017-12-07 13:01:05.191650: step 14070, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 87h:51m:27s remains)
INFO - root - 2017-12-07 13:01:14.870272: step 14080, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 82h:35m:50s remains)
INFO - root - 2017-12-07 13:01:24.445580: step 14090, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 86h:18m:14s remains)
INFO - root - 2017-12-07 13:01:34.191204: step 14100, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 83h:59m:57s remains)
2017-12-07 13:01:35.144771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2543721 -4.2511454 -4.2464552 -4.2453866 -4.246985 -4.24834 -4.2448916 -4.2349625 -4.2193246 -4.2080011 -4.207613 -4.2176743 -4.2355924 -4.2535415 -4.2696609][-4.2187929 -4.2079859 -4.2004032 -4.2015929 -4.2092781 -4.2154264 -4.2120686 -4.199214 -4.178566 -4.1655951 -4.1665249 -4.1823716 -4.2064524 -4.2296138 -4.2494845][-4.1782618 -4.1608882 -4.1526008 -4.159842 -4.1748233 -4.185533 -4.1843266 -4.1698923 -4.1483979 -4.1390786 -4.1444693 -4.1641011 -4.1884027 -4.211925 -4.233849][-4.1434941 -4.1204538 -4.1073914 -4.1158667 -4.1333957 -4.1444206 -4.1456428 -4.1321826 -4.1133766 -4.1110668 -4.1278286 -4.1542525 -4.1815557 -4.204926 -4.2263203][-4.1195669 -4.0919781 -4.071867 -4.0723534 -4.0843873 -4.0891004 -4.085053 -4.0715928 -4.0581722 -4.0690517 -4.1056647 -4.1435213 -4.1773486 -4.2034497 -4.2252436][-4.101131 -4.0665631 -4.0401845 -4.0276589 -4.025506 -4.0202479 -4.0063114 -3.980881 -3.9671783 -3.9988251 -4.0611639 -4.1186109 -4.1656241 -4.1991897 -4.2246537][-4.0697703 -4.0291848 -3.9996297 -3.9705679 -3.9507585 -3.9339623 -3.9099216 -3.8674374 -3.8486507 -3.9067917 -3.9999545 -4.0790143 -4.1415358 -4.1884594 -4.2221937][-4.0565572 -4.02011 -3.9945502 -3.9588239 -3.929214 -3.9062161 -3.8762574 -3.8305941 -3.8156796 -3.8843136 -3.9841712 -4.0623388 -4.1282911 -4.1836014 -4.2206607][-4.0772767 -4.0575914 -4.0451307 -4.0168591 -3.9936571 -3.9755692 -3.9500384 -3.9179397 -3.9117141 -3.96507 -4.0358071 -4.0882177 -4.13991 -4.189517 -4.2216191][-4.1406164 -4.1370239 -4.1359587 -4.1189632 -4.1051836 -4.0946569 -4.0747862 -4.053092 -4.0501351 -4.0818906 -4.1181307 -4.1430941 -4.1780119 -4.2159619 -4.2372508][-4.2132506 -4.2178426 -4.2244134 -4.2171135 -4.2113056 -4.2064519 -4.1900215 -4.1744461 -4.1732793 -4.1893668 -4.2041559 -4.212513 -4.2335682 -4.2569451 -4.2672133][-4.2691517 -4.2730289 -4.2805657 -4.2792063 -4.2769284 -4.2742414 -4.2622213 -4.2508774 -4.2514386 -4.2600646 -4.2656865 -4.2690725 -4.2798872 -4.2917323 -4.2954516][-4.281004 -4.2812033 -4.2856226 -4.2864337 -4.2862816 -4.2862663 -4.279644 -4.2715945 -4.2714391 -4.2772903 -4.2833042 -4.2881522 -4.2970719 -4.3053474 -4.3077016][-4.2657328 -4.2651353 -4.26675 -4.2684193 -4.269228 -4.2702918 -4.267417 -4.26145 -4.2593484 -4.2634726 -4.2718887 -4.2794609 -4.2894082 -4.3002615 -4.3062716][-4.2658467 -4.2671185 -4.2681108 -4.269948 -4.2713208 -4.2717795 -4.2690706 -4.2651343 -4.2633862 -4.2655997 -4.2720346 -4.278954 -4.2875628 -4.2976594 -4.3061566]]...]
INFO - root - 2017-12-07 13:01:44.733692: step 14110, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 84h:59m:59s remains)
INFO - root - 2017-12-07 13:01:54.424915: step 14120, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 86h:16m:02s remains)
INFO - root - 2017-12-07 13:02:04.187468: step 14130, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 86h:49m:26s remains)
INFO - root - 2017-12-07 13:02:13.715325: step 14140, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.970 sec/batch; 85h:44m:50s remains)
INFO - root - 2017-12-07 13:02:23.402023: step 14150, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 85h:33m:52s remains)
INFO - root - 2017-12-07 13:02:32.903416: step 14160, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 84h:16m:21s remains)
INFO - root - 2017-12-07 13:02:42.602406: step 14170, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 84h:20m:55s remains)
INFO - root - 2017-12-07 13:02:52.283514: step 14180, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 86h:50m:51s remains)
INFO - root - 2017-12-07 13:03:01.716832: step 14190, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 82h:56m:48s remains)
INFO - root - 2017-12-07 13:03:11.377357: step 14200, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 84h:47m:11s remains)
2017-12-07 13:03:12.341479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3290892 -4.32584 -4.3243403 -4.3226051 -4.3169527 -4.3073 -4.3000216 -4.3000154 -4.3043451 -4.3095231 -4.3157053 -4.3185339 -4.3191214 -4.3196549 -4.3222861][-4.3114638 -4.3047514 -4.303247 -4.29957 -4.2918248 -4.2829838 -4.27475 -4.2731667 -4.2763987 -4.2813854 -4.288456 -4.2942605 -4.2987485 -4.3010292 -4.3043332][-4.2806911 -4.2713423 -4.2710953 -4.2672434 -4.2574954 -4.2507715 -4.2447238 -4.2402759 -4.2381811 -4.237596 -4.2454576 -4.257895 -4.2684975 -4.2737632 -4.2782664][-4.2393107 -4.2272987 -4.2293463 -4.2272816 -4.217627 -4.213532 -4.2117186 -4.2062693 -4.1974096 -4.1915379 -4.1984258 -4.2155328 -4.2302694 -4.2388921 -4.2444506][-4.2126946 -4.199656 -4.200058 -4.1947317 -4.1791396 -4.1698966 -4.1639204 -4.1504197 -4.1330752 -4.1287866 -4.1440954 -4.1693521 -4.18921 -4.1986866 -4.2036347][-4.20538 -4.1944523 -4.1905437 -4.1737666 -4.1457844 -4.1213245 -4.094903 -4.0597439 -4.0260921 -4.0230842 -4.0529032 -4.0962896 -4.1314173 -4.1476917 -4.1529193][-4.2091579 -4.2001534 -4.19435 -4.1702142 -4.1375685 -4.1012683 -4.0512662 -3.9878328 -3.9340458 -3.9281709 -3.9694233 -4.0276313 -4.0764461 -4.1029115 -4.1133552][-4.21956 -4.2148709 -4.212256 -4.19128 -4.1642447 -4.1320715 -4.0751953 -3.9989953 -3.9365292 -3.9240797 -3.9614742 -4.0172868 -4.0655894 -4.0957594 -4.1083288][-4.2313328 -4.2330446 -4.2339578 -4.2196112 -4.2030191 -4.1823044 -4.138948 -4.0793552 -4.0309882 -4.0143003 -4.0339766 -4.0707293 -4.104104 -4.1264114 -4.1362371][-4.231719 -4.2364268 -4.2413564 -4.2351727 -4.228478 -4.2190905 -4.1932135 -4.1575766 -4.1308784 -4.1181426 -4.1252437 -4.1461473 -4.1620383 -4.1698494 -4.1693935][-4.2225232 -4.2257853 -4.2349133 -4.2376218 -4.2387953 -4.2389908 -4.2283893 -4.2119045 -4.2020645 -4.1985149 -4.202323 -4.2116947 -4.2136893 -4.2070074 -4.19593][-4.2139783 -4.2109613 -4.2199512 -4.2290187 -4.237071 -4.2422962 -4.2406783 -4.2351203 -4.2349815 -4.2402382 -4.2462692 -4.2497897 -4.2441258 -4.2287188 -4.2077518][-4.2187181 -4.2063117 -4.2104282 -4.2217245 -4.2330661 -4.2379889 -4.2338715 -4.2282333 -4.2286372 -4.2381263 -4.24594 -4.2505732 -4.2463379 -4.2284431 -4.201375][-4.2322097 -4.2148061 -4.2152572 -4.22339 -4.2315035 -4.23303 -4.2244587 -4.214951 -4.2108827 -4.21776 -4.2250743 -4.2321539 -4.235527 -4.222702 -4.19314][-4.2565818 -4.23926 -4.2375183 -4.2397113 -4.2397714 -4.2365036 -4.2254915 -4.2115827 -4.2012796 -4.2027287 -4.2092485 -4.2207975 -4.2335696 -4.2285771 -4.2008457]]...]
INFO - root - 2017-12-07 13:03:21.977716: step 14210, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.907 sec/batch; 80h:09m:37s remains)
INFO - root - 2017-12-07 13:03:31.475091: step 14220, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 86h:33m:25s remains)
INFO - root - 2017-12-07 13:03:40.929713: step 14230, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 82h:54m:25s remains)
INFO - root - 2017-12-07 13:03:50.408463: step 14240, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 87h:31m:26s remains)
INFO - root - 2017-12-07 13:04:00.102070: step 14250, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 86h:05m:13s remains)
INFO - root - 2017-12-07 13:04:09.589057: step 14260, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 84h:21m:35s remains)
INFO - root - 2017-12-07 13:04:19.200031: step 14270, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 83h:30m:21s remains)
INFO - root - 2017-12-07 13:04:28.941291: step 14280, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 86h:48m:38s remains)
INFO - root - 2017-12-07 13:04:38.570358: step 14290, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 84h:33m:04s remains)
INFO - root - 2017-12-07 13:04:48.412065: step 14300, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 85h:43m:21s remains)
2017-12-07 13:04:49.302234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3038855 -4.2940917 -4.2995338 -4.3025918 -4.3009419 -4.3029051 -4.30637 -4.305881 -4.2888112 -4.2671671 -4.2481408 -4.2354951 -4.2323456 -4.2407212 -4.2462416][-4.3131995 -4.3019109 -4.3018727 -4.2989578 -4.2933064 -4.2925687 -4.2926731 -4.2907786 -4.2726312 -4.2509942 -4.2371206 -4.2271929 -4.2203846 -4.2278566 -4.2350049][-4.3184848 -4.3029232 -4.292491 -4.2794409 -4.2679014 -4.2661262 -4.2670836 -4.2661147 -4.2492352 -4.2314868 -4.2264256 -4.217629 -4.2076163 -4.2161722 -4.22658][-4.319109 -4.2982454 -4.2777362 -4.2583327 -4.2434773 -4.2390127 -4.2377305 -4.2376647 -4.2237549 -4.2138844 -4.2174115 -4.2113657 -4.2046161 -4.218802 -4.2347965][-4.3122749 -4.2862816 -4.2586651 -4.237793 -4.22117 -4.207756 -4.1910429 -4.1807795 -4.1734352 -4.1856403 -4.2086411 -4.2201576 -4.2256765 -4.2439141 -4.2585344][-4.3054419 -4.2790594 -4.2502761 -4.2283087 -4.2071967 -4.1756396 -4.1250238 -4.0883646 -4.0968585 -4.1480279 -4.2024436 -4.2362466 -4.2543292 -4.2710738 -4.2772551][-4.3051591 -4.2831964 -4.256948 -4.2318206 -4.2006025 -4.1393776 -4.0355844 -3.9610918 -3.9977858 -4.1018252 -4.1925578 -4.2462039 -4.27451 -4.291563 -4.2908506][-4.3116307 -4.2949448 -4.272573 -4.247004 -4.20542 -4.1186976 -3.970196 -3.8570943 -3.9185588 -4.0696158 -4.1859636 -4.2493596 -4.2824645 -4.2961645 -4.2876005][-4.3180017 -4.3048816 -4.2871766 -4.2654519 -4.226522 -4.1446295 -4.0065932 -3.8952036 -3.9526384 -4.0950437 -4.2023253 -4.2601075 -4.2882605 -4.2914715 -4.2717781][-4.3217192 -4.3082166 -4.2913489 -4.2713547 -4.2415547 -4.1869936 -4.0995083 -4.0222197 -4.0533705 -4.1469908 -4.225214 -4.2705193 -4.2933574 -4.2904725 -4.2631493][-4.3193326 -4.2991648 -4.2771277 -4.257174 -4.2345047 -4.2050276 -4.1582832 -4.1088595 -4.1250196 -4.1857009 -4.2412491 -4.2810092 -4.3023357 -4.2961287 -4.2655849][-4.3142605 -4.2869763 -4.2567968 -4.2344265 -4.2181005 -4.2035232 -4.1758823 -4.1403055 -4.1525459 -4.1998167 -4.2446265 -4.280704 -4.30163 -4.2969627 -4.2670622][-4.3100872 -4.2802339 -4.2455144 -4.2228565 -4.21203 -4.2032161 -4.1814647 -4.1514182 -4.1596513 -4.2010779 -4.2402406 -4.2734489 -4.2918797 -4.2849422 -4.2569776][-4.3098679 -4.2844934 -4.2546463 -4.2347946 -4.223206 -4.2106352 -4.1880255 -4.1592526 -4.1641378 -4.2007422 -4.2346716 -4.2633028 -4.2777619 -4.2707858 -4.2513046][-4.3116493 -4.292522 -4.27093 -4.2541928 -4.2390957 -4.2210507 -4.1958065 -4.1706715 -4.1762266 -4.2100749 -4.2392278 -4.2622328 -4.2710509 -4.2639875 -4.252409]]...]
INFO - root - 2017-12-07 13:04:58.827518: step 14310, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 85h:13m:38s remains)
INFO - root - 2017-12-07 13:05:08.506916: step 14320, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 86h:23m:27s remains)
INFO - root - 2017-12-07 13:05:18.136968: step 14330, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 84h:42m:07s remains)
INFO - root - 2017-12-07 13:05:27.673245: step 14340, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 87h:04m:36s remains)
INFO - root - 2017-12-07 13:05:37.362208: step 14350, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 81h:20m:25s remains)
INFO - root - 2017-12-07 13:05:46.975718: step 14360, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 87h:55m:27s remains)
INFO - root - 2017-12-07 13:05:56.522460: step 14370, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 87h:00m:36s remains)
INFO - root - 2017-12-07 13:06:06.050580: step 14380, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.936 sec/batch; 82h:40m:40s remains)
INFO - root - 2017-12-07 13:06:15.583297: step 14390, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 84h:34m:09s remains)
INFO - root - 2017-12-07 13:06:25.248133: step 14400, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 88h:06m:13s remains)
2017-12-07 13:06:26.243311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32684 -4.3104444 -4.2931576 -4.2852263 -4.2858 -4.282989 -4.2920885 -4.3074317 -4.3225317 -4.3290567 -4.3312726 -4.3286552 -4.3267875 -4.32598 -4.3193192][-4.3266077 -4.3079872 -4.2891006 -4.2752757 -4.2660847 -4.2552781 -4.259582 -4.2742209 -4.2919407 -4.302043 -4.3080835 -4.3109493 -4.310462 -4.3059411 -4.2972026][-4.3296957 -4.3071966 -4.2832379 -4.2621613 -4.2414112 -4.2199616 -4.2181129 -4.2309446 -4.2517357 -4.2691851 -4.2846346 -4.2965517 -4.2971597 -4.28797 -4.2726488][-4.3249531 -4.29782 -4.2729011 -4.2479377 -4.2182932 -4.1892056 -4.1810288 -4.1927824 -4.22088 -4.250226 -4.2759156 -4.2914586 -4.2885332 -4.2745943 -4.2518864][-4.3185544 -4.2886257 -4.2644467 -4.2411814 -4.2092152 -4.1778016 -4.1622882 -4.169301 -4.20556 -4.2457857 -4.2750335 -4.2838168 -4.27277 -4.2514977 -4.2196884][-4.3193893 -4.2891521 -4.2662034 -4.2468925 -4.2176385 -4.1845684 -4.1543918 -4.1447706 -4.1848793 -4.2360463 -4.2644339 -4.2638083 -4.2451239 -4.2180104 -4.1806569][-4.3224568 -4.2921453 -4.2663369 -4.243237 -4.2100821 -4.1655922 -4.1072693 -4.072083 -4.11992 -4.1907721 -4.2274113 -4.2270203 -4.2052412 -4.1783638 -4.1426482][-4.3111491 -4.2749949 -4.2379222 -4.1991286 -4.1497111 -4.0787692 -3.9765153 -3.9064283 -3.9687109 -4.0760984 -4.139904 -4.1587067 -4.1487761 -4.1292191 -4.0998068][-4.2922 -4.2481976 -4.2027068 -4.1526814 -4.090188 -3.9985602 -3.8599479 -3.7601938 -3.8312309 -3.9670048 -4.0583887 -4.1019993 -4.1090322 -4.1004891 -4.0776005][-4.29417 -4.2545571 -4.2154837 -4.1762247 -4.1279488 -4.0540485 -3.9400208 -3.8591614 -3.9079394 -4.0144086 -4.0913663 -4.1326003 -4.1385827 -4.1256351 -4.099267][-4.305171 -4.2747841 -4.2451329 -4.2181749 -4.1883855 -4.1423798 -4.065968 -4.0108418 -4.0399513 -4.1081719 -4.1588707 -4.1853423 -4.1828456 -4.1639624 -4.135756][-4.3081508 -4.2805676 -4.2528825 -4.2299724 -4.2095861 -4.1814981 -4.13229 -4.0967383 -4.1151376 -4.1568489 -4.1875868 -4.2054706 -4.2019544 -4.1829762 -4.1578588][-4.308414 -4.2800097 -4.25078 -4.2283573 -4.2132878 -4.1972914 -4.1702223 -4.1530952 -4.1658573 -4.1913605 -4.2089515 -4.2178683 -4.2121868 -4.1932893 -4.1721244][-4.3069897 -4.2776346 -4.2481046 -4.2279882 -4.2190642 -4.2147088 -4.2071681 -4.2030959 -4.2108669 -4.2248435 -4.2328992 -4.233994 -4.2244368 -4.207293 -4.1913166][-4.2966475 -4.2649188 -4.2358904 -4.2192435 -4.2161007 -4.2232327 -4.2324748 -4.2396674 -4.2457924 -4.2514153 -4.2515721 -4.2464671 -4.2354317 -4.2218442 -4.2100954]]...]
INFO - root - 2017-12-07 13:06:36.112574: step 14410, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 88h:14m:26s remains)
INFO - root - 2017-12-07 13:06:45.776287: step 14420, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 86h:52m:52s remains)
INFO - root - 2017-12-07 13:06:55.455504: step 14430, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 77h:01m:19s remains)
INFO - root - 2017-12-07 13:07:05.255550: step 14440, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 83h:08m:00s remains)
INFO - root - 2017-12-07 13:07:14.868970: step 14450, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 79h:55m:30s remains)
INFO - root - 2017-12-07 13:07:24.507515: step 14460, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 87h:22m:57s remains)
INFO - root - 2017-12-07 13:07:34.190184: step 14470, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 88h:46m:33s remains)
INFO - root - 2017-12-07 13:07:43.775228: step 14480, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 82h:33m:49s remains)
INFO - root - 2017-12-07 13:07:53.469335: step 14490, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 84h:02m:19s remains)
INFO - root - 2017-12-07 13:08:03.123058: step 14500, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 86h:54m:19s remains)
2017-12-07 13:08:04.237887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2444806 -4.2395129 -4.2360644 -4.2049952 -4.1651459 -4.1279798 -4.1196408 -4.1352243 -4.1531072 -4.1703358 -4.1955843 -4.2179656 -4.2260175 -4.2222 -4.2029281][-4.2707562 -4.2633677 -4.2572188 -4.2291431 -4.1887569 -4.1474633 -4.1231508 -4.121316 -4.1257524 -4.1340289 -4.1610489 -4.1962147 -4.2130013 -4.2116046 -4.1919084][-4.2828131 -4.2735362 -4.2644339 -4.2396278 -4.2025714 -4.1587372 -4.1160789 -4.0958185 -4.0951567 -4.1053071 -4.1354866 -4.18284 -4.2107949 -4.2106323 -4.1887226][-4.2763233 -4.2663856 -4.2527452 -4.233243 -4.2056365 -4.1634336 -4.1025333 -4.064332 -4.0692782 -4.0874767 -4.1166544 -4.1665635 -4.20524 -4.2105513 -4.19659][-4.2635427 -4.2555833 -4.2406273 -4.2239089 -4.2039404 -4.161777 -4.080307 -4.0223346 -4.0436783 -4.0798407 -4.1043177 -4.1446419 -4.1899128 -4.202271 -4.2029781][-4.2453666 -4.2396235 -4.2267079 -4.2101407 -4.1893158 -4.1437883 -4.044795 -3.9752135 -4.024816 -4.0911131 -4.1104383 -4.1305275 -4.1656322 -4.1771588 -4.1875639][-4.2211752 -4.2169609 -4.2089882 -4.1911478 -4.1719627 -4.1282883 -4.0188928 -3.9397359 -4.0106907 -4.1053219 -4.1246033 -4.1204853 -4.1363387 -4.1422381 -4.1554933][-4.2097869 -4.2084818 -4.2066913 -4.1910396 -4.1721334 -4.1304398 -4.0174904 -3.9255528 -3.99674 -4.1035132 -4.1270747 -4.1079531 -4.1078882 -4.1090384 -4.1246724][-4.2205114 -4.221118 -4.2244244 -4.2137365 -4.1933517 -4.1522641 -4.0465064 -3.9506419 -3.9987178 -4.0960226 -4.1225133 -4.09898 -4.092371 -4.0941277 -4.112793][-4.2388525 -4.2405934 -4.2446628 -4.2384963 -4.2191634 -4.179595 -4.0907941 -4.0036697 -4.0221148 -4.0946088 -4.1177316 -4.0981765 -4.0921297 -4.0992866 -4.122036][-4.255796 -4.2539639 -4.2538986 -4.2490416 -4.232295 -4.1983166 -4.1275592 -4.0530519 -4.0487576 -4.0967493 -4.1190038 -4.1096387 -4.1083045 -4.1155329 -4.13729][-4.2743611 -4.2677989 -4.2583141 -4.2472696 -4.2308183 -4.2031984 -4.1487451 -4.0895934 -4.0753307 -4.1027493 -4.12308 -4.12576 -4.13015 -4.1352825 -4.1520209][-4.2940679 -4.2840652 -4.2656255 -4.2451177 -4.2259574 -4.20326 -4.1653247 -4.1267176 -4.1101918 -4.1200938 -4.135273 -4.1435037 -4.1513991 -4.1547642 -4.1661091][-4.3106189 -4.3012433 -4.2790914 -4.2522206 -4.230516 -4.2122517 -4.1884766 -4.1668029 -4.1496744 -4.141675 -4.1456203 -4.1509571 -4.1608162 -4.1675858 -4.1789675][-4.3215203 -4.316771 -4.2970071 -4.2694435 -4.2461476 -4.22737 -4.2098017 -4.1943665 -4.1720853 -4.1464996 -4.1360526 -4.1344934 -4.1447792 -4.1578093 -4.1755095]]...]
INFO - root - 2017-12-07 13:08:14.032874: step 14510, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 80h:17m:16s remains)
INFO - root - 2017-12-07 13:08:23.743259: step 14520, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 84h:53m:28s remains)
INFO - root - 2017-12-07 13:08:33.260649: step 14530, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 88h:39m:39s remains)
INFO - root - 2017-12-07 13:08:42.859360: step 14540, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 84h:28m:52s remains)
INFO - root - 2017-12-07 13:08:52.239623: step 14550, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 81h:46m:24s remains)
INFO - root - 2017-12-07 13:09:01.954101: step 14560, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 87h:59m:49s remains)
INFO - root - 2017-12-07 13:09:11.635781: step 14570, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 85h:58m:06s remains)
INFO - root - 2017-12-07 13:09:21.217752: step 14580, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 86h:45m:28s remains)
INFO - root - 2017-12-07 13:09:30.946025: step 14590, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 88h:18m:46s remains)
INFO - root - 2017-12-07 13:09:40.578659: step 14600, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 86h:47m:34s remains)
2017-12-07 13:09:41.520289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2383294 -4.2682662 -4.2944112 -4.3046312 -4.2825646 -4.238821 -4.1955576 -4.1548882 -4.1284552 -4.1352048 -4.1559157 -4.1713638 -4.1837778 -4.18531 -4.1831937][-4.2630444 -4.2930355 -4.3181424 -4.3282194 -4.3104181 -4.2704787 -4.2270236 -4.1831141 -4.142211 -4.12743 -4.131041 -4.1345434 -4.1395197 -4.1460562 -4.158587][-4.274775 -4.2972236 -4.3188629 -4.3292422 -4.3119383 -4.2698069 -4.2201848 -4.1758857 -4.1430264 -4.1312256 -4.12972 -4.1264868 -4.1213474 -4.1273365 -4.1510315][-4.2775345 -4.2960839 -4.3138938 -4.3218122 -4.2962928 -4.2452512 -4.1881475 -4.1443434 -4.1305308 -4.1437292 -4.1513405 -4.149169 -4.1426325 -4.1452761 -4.166431][-4.2820511 -4.2981348 -4.3132944 -4.310946 -4.2655644 -4.1945243 -4.1258874 -4.080771 -4.08678 -4.135293 -4.1670475 -4.1793523 -4.1855884 -4.1918521 -4.2016258][-4.2934489 -4.3052649 -4.3148789 -4.2973943 -4.2296004 -4.1336622 -4.0454683 -3.989881 -4.0098386 -4.0940118 -4.1532917 -4.1823421 -4.2081528 -4.2272511 -4.2364941][-4.3083224 -4.3129067 -4.3119974 -4.2772365 -4.1897469 -4.0673594 -3.9548373 -3.8792987 -3.8981125 -4.0120964 -4.1033192 -4.1546507 -4.2020836 -4.236783 -4.2552619][-4.3043461 -4.3043046 -4.2989984 -4.2591891 -4.1727781 -4.0486283 -3.9346509 -3.85587 -3.8653917 -3.9749818 -4.0763292 -4.1422935 -4.2020307 -4.2463121 -4.2731009][-4.2650924 -4.2730684 -4.2813292 -4.262394 -4.2022715 -4.1041451 -4.0159287 -3.9563975 -3.9615297 -4.0359473 -4.1150231 -4.1725459 -4.2244363 -4.2663932 -4.2940993][-4.2073255 -4.2335057 -4.2652817 -4.2766275 -4.2518697 -4.1902919 -4.1307163 -4.0884743 -4.0873523 -4.13069 -4.1849623 -4.225493 -4.2602119 -4.2891283 -4.3106585][-4.1560893 -4.1988368 -4.252995 -4.2897291 -4.2961349 -4.2700191 -4.2362976 -4.2084465 -4.2015381 -4.2214117 -4.2540431 -4.2783504 -4.2962942 -4.312037 -4.3263903][-4.1301756 -4.1880527 -4.2561679 -4.304925 -4.3277359 -4.3249083 -4.3111434 -4.2933183 -4.2839069 -4.2902312 -4.3046527 -4.3175068 -4.3257422 -4.3319082 -4.3391447][-4.1405444 -4.2024603 -4.2660456 -4.3113837 -4.3393621 -4.3499746 -4.3475032 -4.337204 -4.3295231 -4.3285613 -4.3310552 -4.33551 -4.3384991 -4.3403811 -4.343327][-4.18575 -4.2390409 -4.2851157 -4.3167019 -4.3391781 -4.3520355 -4.353385 -4.3486743 -4.343482 -4.3409266 -4.3402348 -4.3403792 -4.3418093 -4.3431053 -4.3440619][-4.2411556 -4.2794571 -4.3038297 -4.3183517 -4.3302107 -4.3382006 -4.339138 -4.3373847 -4.3345404 -4.3330569 -4.3323307 -4.3319969 -4.332561 -4.3337526 -4.3345275]]...]
INFO - root - 2017-12-07 13:09:51.135245: step 14610, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 83h:00m:14s remains)
INFO - root - 2017-12-07 13:10:00.837214: step 14620, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 86h:34m:29s remains)
INFO - root - 2017-12-07 13:10:10.409942: step 14630, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 87h:27m:28s remains)
INFO - root - 2017-12-07 13:10:20.094981: step 14640, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.993 sec/batch; 87h:38m:21s remains)
INFO - root - 2017-12-07 13:10:29.659256: step 14650, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 85h:38m:00s remains)
INFO - root - 2017-12-07 13:10:39.211735: step 14660, loss = 2.10, batch loss = 2.05 (9.2 examples/sec; 0.865 sec/batch; 76h:23m:05s remains)
INFO - root - 2017-12-07 13:10:48.989372: step 14670, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 87h:58m:15s remains)
INFO - root - 2017-12-07 13:10:58.691811: step 14680, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 87h:56m:35s remains)
INFO - root - 2017-12-07 13:11:08.355553: step 14690, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 88h:38m:40s remains)
INFO - root - 2017-12-07 13:11:18.015140: step 14700, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 87h:39m:16s remains)
2017-12-07 13:11:18.925685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2978516 -4.2994385 -4.3115029 -4.3204608 -4.32123 -4.3192949 -4.3233676 -4.3246379 -4.3186183 -4.3149285 -4.3170991 -4.3230095 -4.3271518 -4.3325424 -4.3353791][-4.3077106 -4.307961 -4.3198233 -4.3292913 -4.3324351 -4.3308234 -4.3333564 -4.3341365 -4.3275757 -4.3214483 -4.3220787 -4.3267846 -4.329916 -4.3341665 -4.3355336][-4.2717447 -4.2671018 -4.2768607 -4.2870073 -4.2910213 -4.2904873 -4.29245 -4.292316 -4.2854977 -4.2791109 -4.2778749 -4.2783914 -4.2781343 -4.2810616 -4.2822456][-4.1998868 -4.1876793 -4.1939831 -4.2046452 -4.2124405 -4.213356 -4.2130446 -4.2110415 -4.2032518 -4.1939044 -4.1884775 -4.1832867 -4.1782837 -4.1790738 -4.1808562][-4.1216407 -4.09464 -4.0904255 -4.0980468 -4.1067824 -4.1107321 -4.1120844 -4.1089516 -4.0981607 -4.0846291 -4.0727911 -4.0585241 -4.0456915 -4.0427361 -4.0406322][-4.0455422 -3.9996116 -3.9804039 -3.9808002 -3.9874358 -3.9915457 -3.9939425 -3.9908979 -3.9824233 -3.9705193 -3.9568582 -3.9379263 -3.9213054 -3.9156253 -3.9113817][-4.0421429 -3.98909 -3.9585409 -3.950006 -3.9473641 -3.9422271 -3.9390929 -3.9384167 -3.9432852 -3.9473617 -3.9443939 -3.9321489 -3.9200966 -3.9150591 -3.9102559][-4.1148529 -4.0726271 -4.04222 -4.0281119 -4.0147467 -3.9960864 -3.9807026 -3.9803097 -3.999871 -4.02481 -4.0408549 -4.0438695 -4.0422287 -4.0412054 -4.0381742][-4.1788344 -4.1483455 -4.1238279 -4.1102505 -4.0933957 -4.0691757 -4.04546 -4.0426059 -4.0664816 -4.0993414 -4.1270537 -4.1425376 -4.1490407 -4.1499887 -4.1471691][-4.203774 -4.1788039 -4.1585374 -4.1507173 -4.1398368 -4.1220422 -4.102572 -4.0982709 -4.116611 -4.1450858 -4.17262 -4.18919 -4.1963191 -4.1950569 -4.1920719][-4.2051554 -4.1786795 -4.1582351 -4.1545053 -4.1522765 -4.1453142 -4.1361456 -4.132308 -4.1395526 -4.1566381 -4.1767817 -4.1895595 -4.1941929 -4.1912746 -4.1890159][-4.2089424 -4.1789322 -4.154911 -4.1509309 -4.1557293 -4.1592627 -4.1605077 -4.1564407 -4.1509476 -4.1538949 -4.1641464 -4.1732478 -4.1781778 -4.1758437 -4.1760049][-4.2250938 -4.1931844 -4.16569 -4.1586795 -4.1652188 -4.1716847 -4.1780071 -4.1724958 -4.1566458 -4.147357 -4.1470027 -4.1510606 -4.1566658 -4.1581583 -4.16213][-4.2462063 -4.21295 -4.1847062 -4.175848 -4.1781287 -4.1825023 -4.1882782 -4.1814342 -4.1621623 -4.1479983 -4.1406474 -4.1380215 -4.1408234 -4.1446433 -4.1523275][-4.2600513 -4.2258191 -4.1984487 -4.1898141 -4.1897559 -4.1920643 -4.1981449 -4.1929307 -4.1770468 -4.1628242 -4.1513529 -4.1452522 -4.1454921 -4.1500535 -4.1610684]]...]
INFO - root - 2017-12-07 13:11:28.471229: step 14710, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.975 sec/batch; 86h:04m:44s remains)
INFO - root - 2017-12-07 13:11:37.977376: step 14720, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 84h:42m:47s remains)
INFO - root - 2017-12-07 13:11:47.634238: step 14730, loss = 2.10, batch loss = 2.05 (8.0 examples/sec; 1.000 sec/batch; 88h:16m:13s remains)
INFO - root - 2017-12-07 13:11:57.425944: step 14740, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 84h:47m:17s remains)
INFO - root - 2017-12-07 13:12:07.143050: step 14750, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 86h:06m:49s remains)
INFO - root - 2017-12-07 13:12:16.740799: step 14760, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 82h:59m:11s remains)
INFO - root - 2017-12-07 13:12:26.258233: step 14770, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.997 sec/batch; 88h:01m:19s remains)
INFO - root - 2017-12-07 13:12:35.882115: step 14780, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 82h:27m:18s remains)
INFO - root - 2017-12-07 13:12:45.610429: step 14790, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 85h:19m:26s remains)
INFO - root - 2017-12-07 13:12:55.285534: step 14800, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 87h:10m:41s remains)
2017-12-07 13:12:56.291796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3225946 -4.3156214 -4.3094854 -4.3073449 -4.3101921 -4.314908 -4.3147821 -4.3082132 -4.301 -4.2914643 -4.2812843 -4.27517 -4.274241 -4.2752891 -4.2794657][-4.3160338 -4.3084955 -4.3022871 -4.2998104 -4.303401 -4.309876 -4.3128662 -4.3081837 -4.3019567 -4.2933865 -4.2855377 -4.2818689 -4.2823744 -4.2843013 -4.2880626][-4.2899475 -4.2864089 -4.2807961 -4.2761211 -4.2761354 -4.2798133 -4.2805467 -4.2727418 -4.2638841 -4.2567334 -4.2535157 -4.2539911 -4.2588062 -4.2652593 -4.2731638][-4.2434216 -4.2473211 -4.2428317 -4.234602 -4.229188 -4.2261119 -4.2200003 -4.2041855 -4.1909404 -4.1854296 -4.1895657 -4.1993718 -4.2108016 -4.2238927 -4.2392178][-4.1845984 -4.1948514 -4.1900373 -4.17774 -4.1681151 -4.1586628 -4.14325 -4.1150422 -4.0930371 -4.0906196 -4.1066709 -4.1307845 -4.153019 -4.1752281 -4.1994815][-4.1334186 -4.1393905 -4.12693 -4.1085167 -4.0950027 -4.0825548 -4.0618725 -4.0247951 -3.9944062 -3.997705 -4.0281205 -4.0667067 -4.1005073 -4.1326795 -4.1652713][-4.0940328 -4.0851412 -4.0596128 -4.03691 -4.022438 -4.0122857 -3.9971676 -3.9652984 -3.9371767 -3.9490538 -3.9871714 -4.0327368 -4.070807 -4.1070881 -4.1437268][-4.0837994 -4.0650735 -4.0366092 -4.018414 -4.0074673 -4.0032673 -4.00135 -3.9857051 -3.9691575 -3.9848537 -4.016048 -4.0529242 -4.0851197 -4.1161485 -4.1472979][-4.1193047 -4.1035619 -4.0837231 -4.0740404 -4.0666566 -4.0660429 -4.0725312 -4.0686808 -4.0596247 -4.0717587 -4.0906467 -4.1154017 -4.1390352 -4.16057 -4.1817708][-4.1761532 -4.1667213 -4.1546135 -4.1502604 -4.1458869 -4.146667 -4.1560717 -4.1591287 -4.1571188 -4.1656981 -4.1768312 -4.1933155 -4.2095194 -4.2216473 -4.2329445][-4.2323971 -4.2297459 -4.2250476 -4.2235465 -4.2211428 -4.2206607 -4.2264791 -4.2305551 -4.2322135 -4.2381439 -4.2453346 -4.2574134 -4.2698545 -4.2774181 -4.2811871][-4.2709765 -4.272356 -4.2711797 -4.2704215 -4.269062 -4.2676535 -4.2696571 -4.272428 -4.2750654 -4.2797542 -4.286077 -4.2967272 -4.3080263 -4.3143954 -4.3148637][-4.2813921 -4.2823915 -4.2822285 -4.2825952 -4.283422 -4.2838607 -4.2850924 -4.2876782 -4.291121 -4.295475 -4.3015237 -4.3117471 -4.3235154 -4.3306742 -4.331903][-4.2769494 -4.2786574 -4.2796106 -4.2805214 -4.2827315 -4.28574 -4.2878466 -4.2898917 -4.2926955 -4.2963533 -4.3014417 -4.3107181 -4.3234024 -4.3322058 -4.3351326][-4.2683654 -4.2713623 -4.2723293 -4.2723441 -4.2750297 -4.2791862 -4.2812977 -4.2824583 -4.2838054 -4.2859406 -4.2884288 -4.2964339 -4.3099852 -4.3206491 -4.3262062]]...]
INFO - root - 2017-12-07 13:13:05.760023: step 14810, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 84h:23m:32s remains)
INFO - root - 2017-12-07 13:13:15.495147: step 14820, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 80h:33m:42s remains)
INFO - root - 2017-12-07 13:13:25.190131: step 14830, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 84h:54m:32s remains)
INFO - root - 2017-12-07 13:13:34.689141: step 14840, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 84h:17m:41s remains)
INFO - root - 2017-12-07 13:13:44.317901: step 14850, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 77h:29m:47s remains)
INFO - root - 2017-12-07 13:13:53.979396: step 14860, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 85h:22m:41s remains)
INFO - root - 2017-12-07 13:14:03.631937: step 14870, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 86h:10m:53s remains)
INFO - root - 2017-12-07 13:14:13.321557: step 14880, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 82h:54m:16s remains)
INFO - root - 2017-12-07 13:14:22.967633: step 14890, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 86h:11m:05s remains)
INFO - root - 2017-12-07 13:14:32.595887: step 14900, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 84h:51m:30s remains)
2017-12-07 13:14:33.505488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2669234 -4.2494912 -4.20922 -4.1741257 -4.1784964 -4.2066693 -4.2389135 -4.2568145 -4.2249393 -4.1804776 -4.18075 -4.2022762 -4.2058587 -4.2031341 -4.2152591][-4.2492328 -4.229856 -4.182611 -4.1477242 -4.1672149 -4.2115197 -4.2491369 -4.2701063 -4.2414284 -4.1961169 -4.1881976 -4.2028427 -4.2099848 -4.2096934 -4.2204852][-4.2191496 -4.1953278 -4.1425977 -4.1115584 -4.1455436 -4.2013659 -4.2386832 -4.2562952 -4.2404776 -4.2134614 -4.2111263 -4.21651 -4.2236748 -4.2252464 -4.2331214][-4.1967549 -4.1703544 -4.1209702 -4.0961952 -4.1280179 -4.174387 -4.1990108 -4.2109451 -4.2159572 -4.2169161 -4.2273393 -4.2326732 -4.2365594 -4.240211 -4.2452912][-4.2000422 -4.1721592 -4.1312528 -4.1084852 -4.1142592 -4.118773 -4.113205 -4.11988 -4.1503673 -4.1921511 -4.2249 -4.2387438 -4.2427912 -4.2478385 -4.2555485][-4.2339 -4.2097626 -4.1760321 -4.1448259 -4.1110263 -4.0558028 -3.9970481 -3.9926867 -4.0593204 -4.1490855 -4.2117538 -4.2377033 -4.245657 -4.2581234 -4.2744][-4.2761626 -4.2585249 -4.2320247 -4.19851 -4.1421256 -4.0441165 -3.9308891 -3.8994904 -3.9916334 -4.116169 -4.1984978 -4.2313247 -4.2450614 -4.2652645 -4.2898526][-4.300149 -4.2888374 -4.2730508 -4.2476459 -4.1973548 -4.1014113 -3.9720087 -3.9044924 -3.9736643 -4.0961723 -4.1813655 -4.2168794 -4.2337728 -4.2572021 -4.2876968][-4.3030591 -4.2958422 -4.2901268 -4.2773242 -4.2454033 -4.1762881 -4.0676293 -3.9764662 -3.9907472 -4.0815659 -4.1633277 -4.2008977 -4.2189484 -4.2402492 -4.2710109][-4.2977929 -4.2959261 -4.2974286 -4.2930894 -4.2721314 -4.2273059 -4.1479664 -4.0588851 -4.0317259 -4.0804114 -4.149075 -4.1890192 -4.20823 -4.2253423 -4.2500973][-4.2863679 -4.2857432 -4.2907481 -4.2921023 -4.28132 -4.2542844 -4.2039838 -4.1333089 -4.0910187 -4.1040525 -4.1514769 -4.1877694 -4.203856 -4.2133741 -4.23078][-4.2685804 -4.2670684 -4.2683678 -4.2738495 -4.2702641 -4.2566051 -4.2308517 -4.18404 -4.1466608 -4.1424327 -4.1694226 -4.1977797 -4.2072377 -4.2093945 -4.2188487][-4.2538385 -4.2500839 -4.2419438 -4.2395673 -4.2365055 -4.230773 -4.2265739 -4.2050905 -4.183207 -4.1765223 -4.1936245 -4.216619 -4.2196465 -4.2110648 -4.2096376][-4.2543979 -4.2488022 -4.2304816 -4.2119164 -4.1986403 -4.1886907 -4.1933427 -4.1901593 -4.1864958 -4.187686 -4.2061005 -4.2278833 -4.2276888 -4.2120719 -4.1991138][-4.2639275 -4.2599955 -4.2379179 -4.2047052 -4.175971 -4.1605048 -4.1660247 -4.1708231 -4.1740565 -4.1821842 -4.2032204 -4.2261786 -4.228066 -4.2132 -4.1921725]]...]
INFO - root - 2017-12-07 13:14:43.168951: step 14910, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 83h:41m:39s remains)
INFO - root - 2017-12-07 13:14:52.701666: step 14920, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 83h:21m:07s remains)
INFO - root - 2017-12-07 13:15:02.245969: step 14930, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.990 sec/batch; 87h:21m:44s remains)
INFO - root - 2017-12-07 13:15:11.832076: step 14940, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 86h:39m:19s remains)
INFO - root - 2017-12-07 13:15:21.502203: step 14950, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 84h:10m:18s remains)
INFO - root - 2017-12-07 13:15:31.019970: step 14960, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 79h:54m:52s remains)
INFO - root - 2017-12-07 13:15:40.622337: step 14970, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 88h:15m:04s remains)
INFO - root - 2017-12-07 13:15:50.347001: step 14980, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 88h:12m:06s remains)
INFO - root - 2017-12-07 13:16:00.019518: step 14990, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 83h:22m:43s remains)
INFO - root - 2017-12-07 13:16:09.555784: step 15000, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 85h:04m:26s remains)
2017-12-07 13:16:10.584814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2473092 -4.2494969 -4.2509007 -4.2607269 -4.2538195 -4.2176023 -4.1510687 -4.08616 -4.0852137 -4.1280813 -4.1618886 -4.2005582 -4.2437992 -4.2833681 -4.3091974][-4.2737985 -4.28504 -4.2862463 -4.2853384 -4.268878 -4.2338986 -4.1743603 -4.1113548 -4.1059637 -4.1522374 -4.1914878 -4.2273536 -4.2618432 -4.2947392 -4.3153272][-4.2686305 -4.2887 -4.2959785 -4.2939911 -4.2801895 -4.2539358 -4.2132525 -4.168077 -4.1605268 -4.1990814 -4.2358375 -4.2644119 -4.2883835 -4.3110757 -4.3258195][-4.2423234 -4.2653041 -4.2830625 -4.2859373 -4.2763062 -4.2580819 -4.2354188 -4.2134037 -4.2083392 -4.2332177 -4.2649279 -4.2897744 -4.3080945 -4.3231797 -4.3348141][-4.2074838 -4.2281208 -4.2515545 -4.259253 -4.2455354 -4.2206368 -4.203805 -4.2028279 -4.20808 -4.2255225 -4.25749 -4.2861581 -4.3079867 -4.3235345 -4.3369255][-4.1714382 -4.188375 -4.2072039 -4.2117929 -4.183033 -4.1299448 -4.0987577 -4.1171141 -4.1467342 -4.1713634 -4.2088866 -4.2504659 -4.2861562 -4.3129544 -4.3319445][-4.1384077 -4.1464071 -4.1560555 -4.1479783 -4.0955968 -3.9967315 -3.9241881 -3.9683559 -4.0431647 -4.0885262 -4.1364317 -4.19732 -4.25435 -4.2963648 -4.3232508][-4.1272626 -4.1142559 -4.1039758 -4.0857716 -4.02199 -3.8852012 -3.7593472 -3.8258967 -3.9502895 -4.0218554 -4.0773149 -4.1514044 -4.2259073 -4.279891 -4.3144][-4.1638255 -4.1290026 -4.0991707 -4.0790286 -4.0345621 -3.9158258 -3.7816763 -3.8218877 -3.941242 -4.0172606 -4.0751357 -4.1464906 -4.2195463 -4.2733922 -4.3096137][-4.2185616 -4.1756592 -4.1399274 -4.1241918 -4.1084371 -4.0409412 -3.9448676 -3.9410186 -4.0107188 -4.0722141 -4.1257119 -4.1842093 -4.2394586 -4.2796955 -4.3099031][-4.2687492 -4.2281375 -4.19203 -4.1746988 -4.1731067 -4.1425509 -4.0842156 -4.0626049 -4.0952435 -4.14282 -4.1911712 -4.2380157 -4.2714181 -4.2948227 -4.315804][-4.3009238 -4.2720156 -4.2412004 -4.2250409 -4.2262979 -4.2132559 -4.1810865 -4.1618361 -4.1766772 -4.2118926 -4.2506714 -4.2855668 -4.3035679 -4.313704 -4.3245912][-4.3191619 -4.3057742 -4.2868886 -4.2764735 -4.2796879 -4.2738466 -4.255909 -4.2424703 -4.2523017 -4.275527 -4.2996764 -4.3202233 -4.3284359 -4.329834 -4.3324976][-4.3250189 -4.32034 -4.310842 -4.3069105 -4.3146486 -4.3132892 -4.30215 -4.2920966 -4.3013029 -4.3182044 -4.3286734 -4.3374066 -4.3403206 -4.3382936 -4.3368268][-4.3244524 -4.3204308 -4.3147845 -4.3135614 -4.3205032 -4.3203049 -4.312202 -4.3060737 -4.3159561 -4.3308163 -4.3360982 -4.3390117 -4.33941 -4.3387432 -4.3386865]]...]
INFO - root - 2017-12-07 13:16:20.296690: step 15010, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.018 sec/batch; 89h:45m:03s remains)
INFO - root - 2017-12-07 13:16:29.956477: step 15020, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 85h:40m:13s remains)
INFO - root - 2017-12-07 13:16:39.557727: step 15030, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 85h:04m:39s remains)
INFO - root - 2017-12-07 13:16:49.273237: step 15040, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 83h:24m:13s remains)
INFO - root - 2017-12-07 13:16:58.931788: step 15050, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 85h:55m:51s remains)
INFO - root - 2017-12-07 13:17:08.452926: step 15060, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 84h:08m:36s remains)
INFO - root - 2017-12-07 13:17:18.150626: step 15070, loss = 2.12, batch loss = 2.07 (8.2 examples/sec; 0.971 sec/batch; 85h:36m:39s remains)
INFO - root - 2017-12-07 13:17:27.892495: step 15080, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 82h:39m:21s remains)
INFO - root - 2017-12-07 13:17:37.615611: step 15090, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.997 sec/batch; 87h:54m:52s remains)
INFO - root - 2017-12-07 13:17:47.227029: step 15100, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 80h:47m:03s remains)
2017-12-07 13:17:48.244653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3201609 -4.303699 -4.2880664 -4.2731729 -4.266634 -4.2635059 -4.2581429 -4.2495165 -4.2545991 -4.2696333 -4.2772164 -4.27192 -4.268734 -4.2722378 -4.2753978][-4.3099918 -4.2872849 -4.2696924 -4.2538018 -4.2397318 -4.2240715 -4.212647 -4.2060704 -4.2201304 -4.2454767 -4.2619 -4.2570853 -4.246407 -4.2449894 -4.247889][-4.30203 -4.2760906 -4.2582574 -4.2434525 -4.223453 -4.1906981 -4.1626253 -4.1589346 -4.18719 -4.2274561 -4.255827 -4.2550783 -4.2400436 -4.2320681 -4.2341628][-4.2913203 -4.2647629 -4.2471929 -4.2322617 -4.2078648 -4.1594934 -4.1015711 -4.09788 -4.1503811 -4.2092204 -4.24816 -4.2539935 -4.2379484 -4.2258706 -4.2259731][-4.2729468 -4.2473197 -4.2293119 -4.2106342 -4.1802363 -4.1152735 -4.0253091 -4.0101342 -4.0901518 -4.1731439 -4.2263813 -4.2430949 -4.2332568 -4.2214522 -4.2182922][-4.2516632 -4.2266321 -4.2087016 -4.1852036 -4.1460295 -4.06638 -3.9521503 -3.9147654 -4.0097818 -4.1210976 -4.194695 -4.2208877 -4.2203956 -4.2157426 -4.21002][-4.235517 -4.2128983 -4.193675 -4.1625237 -4.1122732 -4.0165892 -3.8822186 -3.8246336 -3.922554 -4.0557752 -4.1486993 -4.1858358 -4.2003422 -4.2067323 -4.2045178][-4.2292657 -4.2137461 -4.1982207 -4.1690879 -4.1174736 -4.0187593 -3.8851175 -3.827812 -3.9083819 -4.0306792 -4.1186233 -4.160912 -4.1865726 -4.1995192 -4.2025781][-4.2323637 -4.2230544 -4.2143741 -4.1962352 -4.1611781 -4.0893497 -3.9922104 -3.9550531 -4.0047851 -4.0804005 -4.1372137 -4.1686783 -4.1926866 -4.2049112 -4.2096453][-4.2381263 -4.2376776 -4.2333856 -4.2190027 -4.1950011 -4.1527739 -4.0974383 -4.0837297 -4.110961 -4.1499205 -4.1833754 -4.1998034 -4.2135596 -4.2237906 -4.2279115][-4.2467012 -4.2536 -4.2534895 -4.2447081 -4.2293324 -4.2005253 -4.1672006 -4.1661119 -4.1855474 -4.2123027 -4.2334657 -4.2403612 -4.2429342 -4.2482872 -4.2505441][-4.2607441 -4.2718325 -4.2735109 -4.272007 -4.2655182 -4.2410355 -4.2151303 -4.2218156 -4.2421794 -4.2600908 -4.2720156 -4.2755976 -4.2744446 -4.2766113 -4.2769403][-4.2710347 -4.2859821 -4.2933311 -4.2952852 -4.2920651 -4.2709541 -4.2545567 -4.26656 -4.2824368 -4.2901607 -4.2949014 -4.29917 -4.301311 -4.3033571 -4.3013086][-4.2778153 -4.2922592 -4.3033681 -4.3072934 -4.30734 -4.29584 -4.2916164 -4.3041987 -4.3099904 -4.3090973 -4.3110676 -4.3161092 -4.3197513 -4.3205228 -4.3168826][-4.2856107 -4.2926712 -4.2989368 -4.3017898 -4.3039174 -4.301744 -4.3045173 -4.3148866 -4.3173251 -4.3162985 -4.3191423 -4.3232856 -4.3250794 -4.3245759 -4.3218007]]...]
INFO - root - 2017-12-07 13:17:58.028524: step 15110, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 82h:25m:51s remains)
INFO - root - 2017-12-07 13:18:07.719636: step 15120, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 86h:50m:38s remains)
INFO - root - 2017-12-07 13:18:17.501972: step 15130, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 87h:59m:07s remains)
INFO - root - 2017-12-07 13:18:27.306251: step 15140, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 86h:31m:23s remains)
INFO - root - 2017-12-07 13:18:36.947090: step 15150, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 87h:09m:05s remains)
INFO - root - 2017-12-07 13:18:46.655648: step 15160, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 83h:30m:00s remains)
INFO - root - 2017-12-07 13:18:56.163639: step 15170, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 86h:01m:43s remains)
INFO - root - 2017-12-07 13:19:05.755647: step 15180, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 81h:46m:31s remains)
INFO - root - 2017-12-07 13:19:15.399461: step 15190, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 84h:46m:01s remains)
INFO - root - 2017-12-07 13:19:25.215180: step 15200, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.028 sec/batch; 90h:35m:54s remains)
2017-12-07 13:19:26.157266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2746425 -4.2754383 -4.2649527 -4.2613316 -4.2629194 -4.2702894 -4.2757559 -4.2641068 -4.2550569 -4.263 -4.2769742 -4.2918649 -4.303195 -4.3067141 -4.3066692][-4.2822652 -4.2871523 -4.2771497 -4.2675381 -4.2623138 -4.2648449 -4.2716084 -4.2662077 -4.2592983 -4.2702856 -4.2861953 -4.2971859 -4.3027997 -4.3038921 -4.3051515][-4.2729039 -4.2826571 -4.2780466 -4.265893 -4.2544031 -4.2504864 -4.255197 -4.2516522 -4.2424994 -4.2478809 -4.2624645 -4.2743716 -4.2822309 -4.2910581 -4.2985435][-4.2488985 -4.2635994 -4.2678313 -4.2569647 -4.2404442 -4.2297359 -4.2284756 -4.2218571 -4.2078376 -4.2083144 -4.2241 -4.2395921 -4.2541833 -4.2724791 -4.2898068][-4.2203593 -4.2355895 -4.2438827 -4.2358265 -4.2206435 -4.2060046 -4.1967859 -4.1782618 -4.1553593 -4.148344 -4.1640639 -4.1927648 -4.224484 -4.2567363 -4.2839937][-4.1902037 -4.2027912 -4.2129645 -4.2099 -4.1977482 -4.1801796 -4.1605916 -4.1309605 -4.10096 -4.0900164 -4.1108532 -4.1619582 -4.21374 -4.2562733 -4.2865891][-4.1603594 -4.1716042 -4.1853004 -4.190661 -4.183455 -4.1648479 -4.1327014 -4.0901489 -4.0536427 -4.0460954 -4.084002 -4.1571712 -4.2219234 -4.2686138 -4.2960181][-4.133357 -4.1434402 -4.1582346 -4.1689353 -4.1685786 -4.1515245 -4.1103315 -4.056004 -4.0165434 -4.0240216 -4.0841193 -4.168612 -4.236938 -4.282053 -4.305335][-4.1179833 -4.12539 -4.1380148 -4.1471896 -4.1433 -4.122252 -4.076952 -4.0232086 -4.0002909 -4.0348954 -4.10905 -4.1893311 -4.2499104 -4.289928 -4.310338][-4.1260409 -4.1309214 -4.1403852 -4.1393509 -4.1245804 -4.1019 -4.0643716 -4.0359888 -4.048985 -4.1021814 -4.1679573 -4.2246809 -4.2679524 -4.2978816 -4.3124242][-4.1574955 -4.1629214 -4.1678114 -4.1563077 -4.1331377 -4.1109667 -4.0904031 -4.093338 -4.127583 -4.1797214 -4.2271047 -4.2606368 -4.289885 -4.3098569 -4.3184][-4.2029133 -4.2068105 -4.2040915 -4.185801 -4.1632056 -4.1472549 -4.1427956 -4.16131 -4.1959968 -4.2408557 -4.2710781 -4.2905536 -4.3119597 -4.3262987 -4.3288884][-4.2609954 -4.2598014 -4.2510042 -4.2313123 -4.2159905 -4.2104559 -4.2171149 -4.234642 -4.2614393 -4.29398 -4.312469 -4.3223519 -4.336175 -4.3442035 -4.3401318][-4.3080683 -4.30344 -4.2921982 -4.2747645 -4.2650466 -4.2652826 -4.2747931 -4.288928 -4.3083153 -4.3318129 -4.345201 -4.3495502 -4.3557153 -4.356174 -4.3475528][-4.3354516 -4.32894 -4.3159027 -4.2990818 -4.2896066 -4.2902994 -4.2997627 -4.3150129 -4.3335786 -4.3539119 -4.3649516 -4.3670077 -4.3666315 -4.3605814 -4.3482194]]...]
INFO - root - 2017-12-07 13:19:35.724791: step 15210, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 86h:07m:22s remains)
INFO - root - 2017-12-07 13:19:45.390155: step 15220, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 74h:00m:25s remains)
INFO - root - 2017-12-07 13:19:55.126341: step 15230, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 85h:01m:55s remains)
INFO - root - 2017-12-07 13:20:04.700408: step 15240, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 86h:18m:25s remains)
INFO - root - 2017-12-07 13:20:14.442892: step 15250, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 83h:05m:14s remains)
INFO - root - 2017-12-07 13:20:24.015534: step 15260, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.958 sec/batch; 84h:26m:45s remains)
INFO - root - 2017-12-07 13:20:33.697525: step 15270, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.993 sec/batch; 87h:28m:44s remains)
INFO - root - 2017-12-07 13:20:43.470195: step 15280, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 86h:37m:08s remains)
INFO - root - 2017-12-07 13:20:53.259087: step 15290, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 86h:33m:38s remains)
INFO - root - 2017-12-07 13:21:03.001808: step 15300, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 86h:51m:01s remains)
2017-12-07 13:21:04.045545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2386909 -4.2401352 -4.2452278 -4.2601275 -4.2706904 -4.2702274 -4.2620225 -4.22918 -4.2004008 -4.1731739 -4.1345344 -4.1197662 -4.1435428 -4.1620965 -4.1539478][-4.2306018 -4.2381582 -4.2433205 -4.250783 -4.2535334 -4.2478681 -4.2327561 -4.2071376 -4.1855979 -4.1607308 -4.1292534 -4.128871 -4.1747723 -4.2114863 -4.2181721][-4.2211232 -4.2342029 -4.2394829 -4.2385364 -4.2318149 -4.2222824 -4.2062187 -4.1863265 -4.1656752 -4.1377873 -4.1117344 -4.120924 -4.1781349 -4.225965 -4.2467208][-4.212328 -4.2266045 -4.2249489 -4.2153668 -4.2104263 -4.20849 -4.1991806 -4.1787887 -4.1554461 -4.1289616 -4.1040044 -4.1091805 -4.1581516 -4.207005 -4.2385535][-4.1868606 -4.1981974 -4.18956 -4.1787529 -4.1834459 -4.1903763 -4.1818242 -4.1609073 -4.1453152 -4.1284733 -4.1100254 -4.1103997 -4.148448 -4.1962566 -4.2344923][-4.1490922 -4.1607718 -4.1485062 -4.1365385 -4.1445937 -4.1518421 -4.1380787 -4.1219435 -4.1179047 -4.1166024 -4.1099768 -4.1135197 -4.147048 -4.1967783 -4.2407289][-4.1187348 -4.1358976 -4.1207323 -4.1037788 -4.1082997 -4.1021214 -4.0759211 -4.0621924 -4.0731139 -4.0953455 -4.111361 -4.1234145 -4.1543922 -4.20051 -4.2417517][-4.1008677 -4.1228271 -4.1050763 -4.0807481 -4.0830836 -4.0659876 -4.0305538 -4.0162659 -4.039237 -4.0850506 -4.1236324 -4.1421332 -4.16244 -4.1965356 -4.2291069][-4.1002111 -4.1145716 -4.0891113 -4.0597577 -4.0627537 -4.04264 -4.0082936 -4.000886 -4.0318303 -4.0893068 -4.1380086 -4.1610651 -4.1708994 -4.190557 -4.2122755][-4.1152945 -4.1209321 -4.0913081 -4.0638971 -4.0663233 -4.0483136 -4.0245996 -4.0320678 -4.06759 -4.1217957 -4.1632605 -4.1765437 -4.1779203 -4.1847658 -4.1946707][-4.1571593 -4.1600356 -4.1349697 -4.114511 -4.1166296 -4.1017079 -4.0900187 -4.1064706 -4.1351647 -4.1725683 -4.1964688 -4.190702 -4.1821985 -4.1804233 -4.1791282][-4.2020259 -4.2059116 -4.1901503 -4.1810284 -4.1867757 -4.1778727 -4.1716733 -4.1868706 -4.2049046 -4.22234 -4.2274413 -4.2085381 -4.1921892 -4.1865611 -4.1830921][-4.2212057 -4.2282071 -4.2246184 -4.2246232 -4.2340975 -4.23381 -4.2317863 -4.244616 -4.2560034 -4.2614131 -4.25629 -4.2350206 -4.2171426 -4.2103944 -4.2098012][-4.2185616 -4.23003 -4.2346263 -4.2420759 -4.2555151 -4.2610016 -4.2649131 -4.2795515 -4.2928615 -4.2950177 -4.2870216 -4.269125 -4.2545342 -4.2492342 -4.249598][-4.2074871 -4.2219687 -4.2333126 -4.247191 -4.2651429 -4.2766447 -4.2848644 -4.2987294 -4.3119807 -4.3139606 -4.3041849 -4.2897148 -4.2805567 -4.2784224 -4.2796922]]...]
INFO - root - 2017-12-07 13:21:13.600944: step 15310, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 83h:00m:03s remains)
INFO - root - 2017-12-07 13:21:23.281150: step 15320, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.997 sec/batch; 87h:51m:29s remains)
INFO - root - 2017-12-07 13:21:33.010263: step 15330, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.997 sec/batch; 87h:51m:00s remains)
INFO - root - 2017-12-07 13:21:42.583085: step 15340, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 85h:47m:34s remains)
INFO - root - 2017-12-07 13:21:52.315486: step 15350, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 84h:48m:55s remains)
INFO - root - 2017-12-07 13:22:01.886192: step 15360, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 88h:21m:45s remains)
INFO - root - 2017-12-07 13:22:11.495362: step 15370, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 83h:53m:58s remains)
INFO - root - 2017-12-07 13:22:21.163547: step 15380, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 85h:23m:25s remains)
INFO - root - 2017-12-07 13:22:30.805157: step 15390, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 84h:02m:49s remains)
INFO - root - 2017-12-07 13:22:40.367110: step 15400, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 84h:52m:28s remains)
2017-12-07 13:22:41.358824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3446627 -4.3466797 -4.3466964 -4.3454204 -4.3421006 -4.3353348 -4.3262267 -4.3176355 -4.3170805 -4.3266892 -4.3392787 -4.3406143 -4.3334856 -4.3176937 -4.2859735][-4.3459406 -4.3470707 -4.3450222 -4.3408256 -4.3331132 -4.322504 -4.3094463 -4.2960987 -4.2939105 -4.3051372 -4.3212714 -4.3255796 -4.3196912 -4.2997203 -4.2533579][-4.3493781 -4.3494163 -4.3440838 -4.3352695 -4.3224192 -4.3042459 -4.2819314 -4.2621522 -4.2589564 -4.2771511 -4.3040648 -4.3144221 -4.3071361 -4.2766986 -4.2043891][-4.3517537 -4.3510294 -4.3413858 -4.3276405 -4.3081245 -4.2765608 -4.2332158 -4.1980104 -4.19481 -4.2274351 -4.2754378 -4.2982559 -4.29026 -4.2457352 -4.1433544][-4.3501115 -4.348628 -4.3363109 -4.3172812 -4.2889943 -4.2410398 -4.1699696 -4.1098623 -4.1116343 -4.1745119 -4.2501454 -4.2880292 -4.2808485 -4.2268038 -4.1168866][-4.3437181 -4.3405056 -4.3239202 -4.2970891 -4.257885 -4.1855969 -4.0726318 -3.9754267 -3.9903176 -4.1017857 -4.2119846 -4.2713614 -4.2755346 -4.2332835 -4.1542468][-4.3367028 -4.3300805 -4.305769 -4.2683225 -4.2157 -4.1203213 -3.9687271 -3.8274758 -3.8626707 -4.0289087 -4.1705108 -4.2532349 -4.2775583 -4.255517 -4.2133727][-4.3344674 -4.3245239 -4.2947664 -4.248908 -4.1888928 -4.0976167 -3.956583 -3.8262343 -3.8685832 -4.03927 -4.178813 -4.259306 -4.2864194 -4.2784319 -4.2593164][-4.3337317 -4.325305 -4.2991152 -4.25556 -4.2050834 -4.145031 -4.0619025 -3.996721 -4.02987 -4.1398911 -4.23378 -4.2860956 -4.2997184 -4.2914438 -4.2739916][-4.3318214 -4.3280363 -4.3132563 -4.2826962 -4.2447028 -4.2112908 -4.1774397 -4.1590118 -4.178381 -4.2285433 -4.2725711 -4.2968245 -4.3038578 -4.2938046 -4.2654724][-4.3374267 -4.3353596 -4.3287029 -4.3100123 -4.2843814 -4.2661152 -4.26113 -4.2651167 -4.2740631 -4.2862134 -4.2973623 -4.3038993 -4.2992649 -4.2739124 -4.2229462][-4.3428807 -4.3414268 -4.3379736 -4.3246818 -4.3019943 -4.2819848 -4.2810636 -4.2959681 -4.3071651 -4.3130903 -4.3178554 -4.3144851 -4.2897921 -4.2307835 -4.141489][-4.3393812 -4.338768 -4.3379731 -4.3237076 -4.2949252 -4.2634335 -4.2539558 -4.2716193 -4.2961745 -4.3152204 -4.3245058 -4.3136411 -4.2605686 -4.15643 -4.0159011][-4.3286166 -4.3286328 -4.3243423 -4.2996445 -4.2549124 -4.2035751 -4.1824007 -4.2081304 -4.2573147 -4.2982774 -4.3130879 -4.2891922 -4.2173204 -4.0912056 -3.9274874][-4.318603 -4.3187742 -4.304544 -4.2600636 -4.183023 -4.0964756 -4.0612807 -4.115931 -4.2045093 -4.2697639 -4.2942739 -4.2705269 -4.2049017 -4.1064715 -3.997185]]...]
INFO - root - 2017-12-07 13:22:50.895682: step 15410, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 79h:52m:49s remains)
INFO - root - 2017-12-07 13:23:00.653556: step 15420, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 85h:11m:08s remains)
INFO - root - 2017-12-07 13:23:10.175743: step 15430, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 85h:47m:06s remains)
INFO - root - 2017-12-07 13:23:19.807326: step 15440, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.015 sec/batch; 89h:24m:12s remains)
INFO - root - 2017-12-07 13:23:29.496749: step 15450, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 86h:18m:37s remains)
INFO - root - 2017-12-07 13:23:39.186113: step 15460, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.990 sec/batch; 87h:13m:13s remains)
INFO - root - 2017-12-07 13:23:48.929815: step 15470, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 87h:18m:54s remains)
INFO - root - 2017-12-07 13:23:58.473472: step 15480, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 86h:28m:24s remains)
INFO - root - 2017-12-07 13:24:08.060832: step 15490, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.973 sec/batch; 85h:43m:26s remains)
INFO - root - 2017-12-07 13:24:17.687579: step 15500, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 77h:10m:11s remains)
2017-12-07 13:24:18.668079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1929674 -4.1940541 -4.1863446 -4.1864309 -4.183753 -4.1809568 -4.2028756 -4.22137 -4.2359095 -4.2562132 -4.2632713 -4.2571359 -4.2445664 -4.2293816 -4.2097983][-4.1469836 -4.1504693 -4.1462049 -4.1640491 -4.1797023 -4.1830406 -4.2111387 -4.2347465 -4.2410097 -4.2562151 -4.2656088 -4.2556143 -4.2370467 -4.2213225 -4.2027235][-4.1342468 -4.144258 -4.1527543 -4.1922174 -4.2238879 -4.2304511 -4.2534318 -4.2712364 -4.2646813 -4.2703481 -4.2777371 -4.2640638 -4.2452803 -4.2314687 -4.2127805][-4.1667995 -4.186111 -4.2085738 -4.2523146 -4.280942 -4.2800879 -4.2850189 -4.28557 -4.2640214 -4.2601256 -4.2701516 -4.2621689 -4.2520623 -4.242228 -4.222672][-4.2325363 -4.2559719 -4.2808833 -4.306592 -4.3140755 -4.2917304 -4.2642727 -4.243783 -4.2174969 -4.2156744 -4.2334852 -4.235281 -4.2353292 -4.2332392 -4.223269][-4.2915812 -4.3041673 -4.3175955 -4.3161941 -4.2932181 -4.2410769 -4.1745806 -4.14204 -4.1344409 -4.1477981 -4.1804814 -4.1957369 -4.2104864 -4.2281775 -4.2414742][-4.3219891 -4.3195777 -4.3130565 -4.2873096 -4.2343812 -4.1409531 -4.0209084 -3.978544 -4.0152249 -4.0634642 -4.1157942 -4.1568518 -4.1936016 -4.2348814 -4.2718811][-4.3372846 -4.3221464 -4.2937927 -4.24056 -4.1584582 -4.0302563 -3.8735106 -3.8445687 -3.9433279 -4.0307121 -4.1039777 -4.16471 -4.2113147 -4.2572646 -4.2999434][-4.341248 -4.314517 -4.2691231 -4.1952329 -4.1054821 -4.0003891 -3.8887858 -3.8957369 -4.0020328 -4.0905676 -4.1643438 -4.2259164 -4.2651772 -4.2987423 -4.3279467][-4.33613 -4.2992997 -4.2503862 -4.1842036 -4.11682 -4.0599771 -4.0140257 -4.0482664 -4.1328163 -4.2003818 -4.2549963 -4.2976909 -4.31958 -4.3342829 -4.345211][-4.32297 -4.2784648 -4.2317042 -4.1807737 -4.1405468 -4.1181107 -4.11723 -4.1677413 -4.2318335 -4.2750945 -4.3071909 -4.3311653 -4.3413191 -4.344027 -4.3454843][-4.2967429 -4.2456636 -4.1934204 -4.1511593 -4.13118 -4.1319761 -4.1600604 -4.220367 -4.2728267 -4.3011975 -4.32074 -4.3312283 -4.3329682 -4.3274369 -4.322247][-4.2548323 -4.1951537 -4.1347151 -4.1000037 -4.1026893 -4.1280618 -4.1761432 -4.2417521 -4.2865558 -4.3013444 -4.308641 -4.3055034 -4.2971053 -4.2861171 -4.2760363][-4.2136745 -4.1546936 -4.1006131 -4.0809779 -4.1065006 -4.1489143 -4.1975241 -4.2481618 -4.2766962 -4.27944 -4.2743955 -4.2560339 -4.2385044 -4.22906 -4.2202997][-4.21411 -4.171073 -4.1361513 -4.1296849 -4.1633477 -4.2068453 -4.237855 -4.2597518 -4.263238 -4.2511125 -4.237627 -4.2099967 -4.1906137 -4.1914992 -4.1928663]]...]
INFO - root - 2017-12-07 13:24:28.379463: step 15510, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 83h:41m:10s remains)
INFO - root - 2017-12-07 13:24:37.834401: step 15520, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 83h:46m:53s remains)
INFO - root - 2017-12-07 13:24:47.403462: step 15530, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 80h:41m:27s remains)
INFO - root - 2017-12-07 13:24:56.995706: step 15540, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 83h:02m:24s remains)
INFO - root - 2017-12-07 13:25:06.811436: step 15550, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 88h:46m:02s remains)
INFO - root - 2017-12-07 13:25:16.449547: step 15560, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 85h:24m:20s remains)
INFO - root - 2017-12-07 13:25:26.190742: step 15570, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 84h:05m:35s remains)
INFO - root - 2017-12-07 13:25:35.965244: step 15580, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.975 sec/batch; 85h:48m:15s remains)
INFO - root - 2017-12-07 13:25:45.584231: step 15590, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 82h:09m:12s remains)
INFO - root - 2017-12-07 13:25:54.983893: step 15600, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 85h:33m:54s remains)
2017-12-07 13:25:55.985357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1485004 -4.1819873 -4.2192626 -4.2491603 -4.2543125 -4.2375679 -4.2176161 -4.2060947 -4.1958003 -4.1836972 -4.1793656 -4.1932731 -4.20995 -4.2166157 -4.2026362][-4.1455364 -4.176333 -4.218699 -4.2511692 -4.2572017 -4.2369661 -4.2107115 -4.1960406 -4.1897173 -4.1866336 -4.1917048 -4.2051806 -4.2149873 -4.218071 -4.2020478][-4.1667924 -4.1883574 -4.221539 -4.243 -4.2398591 -4.2114086 -4.1793308 -4.1681948 -4.1784592 -4.1913223 -4.2050586 -4.2158546 -4.2205744 -4.2244568 -4.2113829][-4.1905646 -4.2003579 -4.2179108 -4.2188325 -4.19947 -4.1607161 -4.1269565 -4.130002 -4.1633406 -4.1949706 -4.2139339 -4.219595 -4.2188897 -4.2214456 -4.2115951][-4.2011967 -4.2026815 -4.2001104 -4.1753283 -4.1395769 -4.0957727 -4.0713873 -4.0973296 -4.1540217 -4.2011104 -4.2233934 -4.2202644 -4.20883 -4.200551 -4.1893215][-4.1915774 -4.1850095 -4.1631489 -4.1180744 -4.06858 -4.02396 -4.0175323 -4.0683079 -4.1434531 -4.2039886 -4.2309313 -4.2197552 -4.1937513 -4.1696877 -4.1550283][-4.1681423 -4.1513233 -4.119031 -4.0651169 -4.00948 -3.9680772 -3.9761634 -4.0381312 -4.1228933 -4.1950665 -4.2240148 -4.2043614 -4.1664319 -4.1351981 -4.1233807][-4.1355391 -4.1072807 -4.0747986 -4.0261583 -3.9785862 -3.9468634 -3.954941 -4.006536 -4.0918226 -4.1664371 -4.191771 -4.1645179 -4.1220284 -4.0965114 -4.09232][-4.1186767 -4.0845408 -4.0573759 -4.0238285 -3.9927 -3.9698529 -3.974221 -4.0101123 -4.0817761 -4.1468673 -4.1583729 -4.1171603 -4.0719256 -4.0547915 -4.0592217][-4.137959 -4.1036496 -4.0809031 -4.0624022 -4.0428514 -4.0278692 -4.0308743 -4.0507178 -4.1002169 -4.1465755 -4.1417155 -4.0917692 -4.0491724 -4.0325608 -4.0365458][-4.1759243 -4.1487904 -4.1356421 -4.1278448 -4.1153669 -4.1071668 -4.1067624 -4.1126537 -4.1431279 -4.1704316 -4.1555457 -4.1082573 -4.0685453 -4.0427852 -4.0327392][-4.2106609 -4.2008204 -4.198102 -4.1976094 -4.192359 -4.1878233 -4.183176 -4.1805468 -4.1950307 -4.207015 -4.1884212 -4.1494293 -4.1130443 -4.0836711 -4.060638][-4.24301 -4.246098 -4.2480984 -4.2483039 -4.2452788 -4.2415252 -4.2375355 -4.2334929 -4.2414627 -4.2467794 -4.2298408 -4.1971245 -4.164207 -4.1382532 -4.1146631][-4.2721977 -4.2752166 -4.2736492 -4.2710795 -4.2699437 -4.2691979 -4.2687111 -4.2681494 -4.272182 -4.2694845 -4.2499509 -4.2216191 -4.1958666 -4.1783733 -4.16288][-4.2747993 -4.2711759 -4.2664008 -4.2679076 -4.2741885 -4.2792287 -4.2807665 -4.2801528 -4.2784505 -4.2667441 -4.2440624 -4.221674 -4.2038374 -4.1915584 -4.1821146]]...]
INFO - root - 2017-12-07 13:26:05.841326: step 15610, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 88h:52m:28s remains)
INFO - root - 2017-12-07 13:26:15.601191: step 15620, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 87h:51m:42s remains)
INFO - root - 2017-12-07 13:26:25.251183: step 15630, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 79h:04m:14s remains)
INFO - root - 2017-12-07 13:26:34.793209: step 15640, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 80h:45m:13s remains)
INFO - root - 2017-12-07 13:26:44.405774: step 15650, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 81h:59m:54s remains)
INFO - root - 2017-12-07 13:26:54.148192: step 15660, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.029 sec/batch; 90h:35m:24s remains)
INFO - root - 2017-12-07 13:27:03.672059: step 15670, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 86h:59m:46s remains)
INFO - root - 2017-12-07 13:27:13.436125: step 15680, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 84h:35m:58s remains)
INFO - root - 2017-12-07 13:27:22.972108: step 15690, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 84h:23m:48s remains)
INFO - root - 2017-12-07 13:27:32.474003: step 15700, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 85h:14m:58s remains)
2017-12-07 13:27:33.459630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3490458 -4.3486962 -4.3476443 -4.3453913 -4.3435125 -4.3379192 -4.3276024 -4.3159332 -4.3104239 -4.3081741 -4.3095756 -4.3125033 -4.316566 -4.3223543 -4.3269687][-4.3478327 -4.3489423 -4.3477044 -4.342452 -4.3362603 -4.3237705 -4.3058853 -4.2872558 -4.2797384 -4.2770939 -4.2778468 -4.281249 -4.2877994 -4.2973728 -4.3071313][-4.344275 -4.345715 -4.3426361 -4.3326225 -4.3187685 -4.2978983 -4.2756562 -4.252317 -4.246007 -4.2434788 -4.2408237 -4.2428327 -4.2523623 -4.26802 -4.2832155][-4.3354397 -4.3314333 -4.3226175 -4.3071809 -4.28512 -4.2561555 -4.2299662 -4.2058854 -4.2053094 -4.2072873 -4.2040257 -4.2059441 -4.2184153 -4.237669 -4.2577066][-4.32738 -4.31545 -4.2983565 -4.2763743 -4.2477856 -4.2107491 -4.1703634 -4.1380486 -4.1518412 -4.17452 -4.1837788 -4.189774 -4.2019215 -4.2190924 -4.2375593][-4.3170257 -4.2982965 -4.2733116 -4.241672 -4.1971836 -4.1415591 -4.0698929 -4.0158978 -4.0492644 -4.1047354 -4.1406755 -4.1605453 -4.1789007 -4.1972952 -4.2165985][-4.2983484 -4.26989 -4.2352142 -4.1889086 -4.1239071 -4.0347533 -3.9137545 -3.8256207 -3.8840125 -3.9816296 -4.0551085 -4.0995674 -4.135066 -4.1631594 -4.1895943][-4.2631774 -4.22661 -4.1838 -4.1289225 -4.0546222 -3.9428451 -3.7791469 -3.6495852 -3.7226388 -3.8584647 -3.9642687 -4.0331788 -4.0879779 -4.1303377 -4.1647458][-4.2281957 -4.1857519 -4.1408687 -4.0894527 -4.0311604 -3.9443529 -3.8037958 -3.6799021 -3.7234213 -3.8397963 -3.9368396 -4.0085564 -4.0732765 -4.1255579 -4.1634722][-4.2040272 -4.1598029 -4.1212263 -4.0872197 -4.0619211 -4.021122 -3.9389007 -3.8571658 -3.8693335 -3.9344327 -3.9969537 -4.0500517 -4.1062222 -4.1542373 -4.1894379][-4.1888223 -4.1454306 -4.1139545 -4.0993567 -4.1022768 -4.0970821 -4.060173 -4.0141006 -4.0131178 -4.0449276 -4.0786862 -4.11227 -4.1549463 -4.19635 -4.2280312][-4.1970592 -4.160017 -4.139751 -4.1400223 -4.1553812 -4.1655316 -4.1545115 -4.1337805 -4.1296329 -4.1466837 -4.1675372 -4.1897373 -4.2183242 -4.2493019 -4.273726][-4.2401834 -4.2161813 -4.2081733 -4.2135835 -4.22802 -4.2384615 -4.2360959 -4.2267509 -4.2243671 -4.2348623 -4.2490749 -4.265336 -4.283843 -4.3034081 -4.3179946][-4.2880268 -4.2744455 -4.2718334 -4.2785316 -4.289854 -4.2983351 -4.2991705 -4.2953167 -4.2940679 -4.2994723 -4.3068485 -4.316751 -4.3282833 -4.3408203 -4.348403][-4.3215919 -4.3151827 -4.3146486 -4.3208647 -4.3283195 -4.3336835 -4.3339825 -4.331902 -4.3308783 -4.3332477 -4.3364758 -4.34164 -4.3487258 -4.3562822 -4.35917]]...]
INFO - root - 2017-12-07 13:27:43.165168: step 15710, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 82h:39m:43s remains)
INFO - root - 2017-12-07 13:27:52.626418: step 15720, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.916 sec/batch; 80h:36m:51s remains)
INFO - root - 2017-12-07 13:28:02.171720: step 15730, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 84h:16m:01s remains)
INFO - root - 2017-12-07 13:28:11.765810: step 15740, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 86h:29m:18s remains)
INFO - root - 2017-12-07 13:28:21.460541: step 15750, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 84h:50m:37s remains)
INFO - root - 2017-12-07 13:28:31.216233: step 15760, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 84h:34m:06s remains)
INFO - root - 2017-12-07 13:28:40.837579: step 15770, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 87h:04m:47s remains)
INFO - root - 2017-12-07 13:28:50.561549: step 15780, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 81h:42m:47s remains)
INFO - root - 2017-12-07 13:29:00.332779: step 15790, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 88h:49m:48s remains)
INFO - root - 2017-12-07 13:29:09.883000: step 15800, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 80h:40m:26s remains)
2017-12-07 13:29:10.876271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3535028 -4.3344393 -4.3013253 -4.2740188 -4.263103 -4.2568021 -4.2556791 -4.2645106 -4.2737384 -4.275034 -4.2705188 -4.2649903 -4.2589736 -4.2579389 -4.263278][-4.3384976 -4.3079305 -4.2652369 -4.2282124 -4.2085581 -4.1932817 -4.1872425 -4.1957135 -4.2069254 -4.21161 -4.2080307 -4.2000484 -4.1923485 -4.1918035 -4.1978207][-4.32255 -4.2822428 -4.231648 -4.1841893 -4.1531119 -4.12598 -4.1160226 -4.1273088 -4.1397982 -4.1492066 -4.1478624 -4.1390285 -4.1348557 -4.1423249 -4.1517282][-4.3108716 -4.2645516 -4.2040815 -4.141521 -4.0902653 -4.0437021 -4.0313916 -4.0547209 -4.077352 -4.0972395 -4.1049066 -4.1028757 -4.1116438 -4.134387 -4.1499252][-4.3022261 -4.2514033 -4.1810246 -4.1061068 -4.0269036 -3.9411564 -3.9029589 -3.9360838 -3.9851673 -4.026897 -4.0577025 -4.0795145 -4.1066694 -4.147965 -4.1733189][-4.2956848 -4.2444563 -4.1673107 -4.0791073 -3.9754369 -3.840148 -3.7506223 -3.7817194 -3.873528 -3.9469395 -4.0036945 -4.0554624 -4.1059914 -4.1644988 -4.208138][-4.2940617 -4.2457643 -4.1652603 -4.0632863 -3.9364781 -3.7540362 -3.5885062 -3.6055515 -3.7576354 -3.8731933 -3.9542193 -4.0310712 -4.1060939 -4.1765571 -4.2351522][-4.2912669 -4.2524638 -4.1801438 -4.0771317 -3.9405122 -3.7430525 -3.5325456 -3.5294678 -3.7267485 -3.8703716 -3.9588385 -4.0459018 -4.1325359 -4.2070131 -4.2668686][-4.2963362 -4.2755404 -4.2277908 -4.1508613 -4.043 -3.8965588 -3.7428789 -3.7403517 -3.8913035 -3.9979825 -4.05707 -4.1266003 -4.2012215 -4.2635155 -4.3080482][-4.3082452 -4.3049355 -4.2839108 -4.2440124 -4.1790676 -4.0907984 -4.0067339 -4.0086756 -4.0919852 -4.1499691 -4.1796603 -4.2250247 -4.2746482 -4.3163195 -4.342957][-4.3212929 -4.3245754 -4.3180189 -4.3005986 -4.2673593 -4.2240782 -4.1874504 -4.1910362 -4.2315221 -4.2554479 -4.2655745 -4.2890334 -4.3181491 -4.3467979 -4.3609352][-4.3280888 -4.3294811 -4.3267617 -4.3162341 -4.2996883 -4.2830095 -4.2696991 -4.2740569 -4.2953148 -4.3049464 -4.3072786 -4.3168883 -4.335566 -4.3552976 -4.3620524][-4.3315654 -4.32935 -4.323523 -4.3118877 -4.30194 -4.298038 -4.2963886 -4.3006744 -4.3145032 -4.3209643 -4.321867 -4.3284192 -4.341176 -4.3530827 -4.3550348][-4.3378086 -4.33447 -4.3269587 -4.3169136 -4.3122058 -4.3140745 -4.3173127 -4.3202167 -4.3288155 -4.3331 -4.3344045 -4.3395963 -4.3467116 -4.3517404 -4.3499484][-4.3435678 -4.3431478 -4.3393679 -4.3347073 -4.333499 -4.3351173 -4.3370357 -4.3380637 -4.3405457 -4.3412271 -4.3415651 -4.3444619 -4.347569 -4.3488231 -4.3468728]]...]
INFO - root - 2017-12-07 13:29:20.485130: step 15810, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 82h:02m:05s remains)
INFO - root - 2017-12-07 13:29:30.114686: step 15820, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 85h:10m:41s remains)
INFO - root - 2017-12-07 13:29:39.660687: step 15830, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 81h:20m:16s remains)
INFO - root - 2017-12-07 13:29:49.264862: step 15840, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 85h:02m:19s remains)
INFO - root - 2017-12-07 13:29:58.933115: step 15850, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 85h:49m:28s remains)
INFO - root - 2017-12-07 13:30:08.761419: step 15860, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 86h:05m:54s remains)
INFO - root - 2017-12-07 13:30:18.355956: step 15870, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 84h:03m:56s remains)
INFO - root - 2017-12-07 13:30:27.879196: step 15880, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.896 sec/batch; 78h:48m:20s remains)
INFO - root - 2017-12-07 13:30:37.425288: step 15890, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 74h:36m:27s remains)
INFO - root - 2017-12-07 13:30:46.990168: step 15900, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.016 sec/batch; 89h:22m:00s remains)
2017-12-07 13:30:47.905890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3359466 -4.3416233 -4.341073 -4.3341632 -4.3324084 -4.3370895 -4.3435235 -4.3470817 -4.3423562 -4.3321815 -4.320169 -4.3126874 -4.3106489 -4.3151011 -4.3254547][-4.3099689 -4.3190403 -4.3226867 -4.3160987 -4.3128843 -4.3187494 -4.3286014 -4.3364511 -4.3320889 -4.3167863 -4.2951279 -4.278975 -4.27312 -4.2799478 -4.2952762][-4.2685728 -4.277997 -4.2828951 -4.2765765 -4.2730818 -4.2807975 -4.2938743 -4.305656 -4.3039589 -4.2872143 -4.2611957 -4.2411218 -4.23464 -4.2447877 -4.26347][-4.2158942 -4.222199 -4.2220559 -4.2141471 -4.2117057 -4.2205577 -4.2338295 -4.2497745 -4.2572956 -4.249084 -4.2307968 -4.2180891 -4.2170868 -4.2286892 -4.2457685][-4.1723471 -4.1737852 -4.1655755 -4.153471 -4.146656 -4.1464963 -4.1491351 -4.165206 -4.1895342 -4.2047791 -4.2088637 -4.2125168 -4.2210269 -4.2325807 -4.2437415][-4.1559792 -4.1565447 -4.1420717 -4.1194196 -4.0952086 -4.0656872 -4.0357742 -4.0443273 -4.0957479 -4.1497016 -4.1868491 -4.2095013 -4.2267208 -4.2388029 -4.2460775][-4.1635351 -4.1684546 -4.153419 -4.1207027 -4.072998 -4.0025964 -3.9222827 -3.9112859 -3.9951162 -4.0917816 -4.15362 -4.1853256 -4.2083578 -4.2242656 -4.2346525][-4.1772261 -4.19158 -4.1859965 -4.1510286 -4.0902195 -3.9933906 -3.8808022 -3.8482628 -3.945822 -4.0580645 -4.1192856 -4.1417761 -4.1634431 -4.1872754 -4.2083497][-4.1825619 -4.2129412 -4.2250824 -4.20163 -4.1479473 -4.0592976 -3.9673319 -3.9417341 -4.0066676 -4.0762529 -4.1018524 -4.0993996 -4.1102438 -4.1386805 -4.1727815][-4.1790819 -4.2240281 -4.2522044 -4.247736 -4.2144494 -4.1538291 -4.0970526 -4.0808816 -4.1072621 -4.124989 -4.1138229 -4.0891395 -4.0878444 -4.1140332 -4.1500635][-4.1861205 -4.2309265 -4.2651854 -4.2758794 -4.2630124 -4.2314072 -4.2019181 -4.1892457 -4.1926503 -4.1832218 -4.1578584 -4.1274476 -4.1209159 -4.1379957 -4.1611938][-4.2028942 -4.2354383 -4.2709537 -4.2929826 -4.2950583 -4.2833037 -4.2695165 -4.2561631 -4.2500567 -4.2372417 -4.2195268 -4.1988831 -4.1925006 -4.1976252 -4.2029381][-4.2318645 -4.2492709 -4.2807803 -4.3075728 -4.3174615 -4.3145533 -4.307529 -4.2958488 -4.2886677 -4.2818265 -4.2762256 -4.2676268 -4.2635322 -4.2623124 -4.2580962][-4.2714882 -4.2796335 -4.3048019 -4.3285856 -4.3400221 -4.3391261 -4.3322315 -4.3228927 -4.3199749 -4.321991 -4.3253517 -4.3246384 -4.3217845 -4.3173981 -4.3086696][-4.3119187 -4.3180933 -4.3359413 -4.3504138 -4.3579121 -4.3572979 -4.3502765 -4.3416305 -4.3410254 -4.3484364 -4.3571844 -4.3609648 -4.3589621 -4.3530731 -4.3430672]]...]
INFO - root - 2017-12-07 13:30:57.533690: step 15910, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.970 sec/batch; 85h:19m:45s remains)
INFO - root - 2017-12-07 13:31:07.352871: step 15920, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 88h:01m:33s remains)
INFO - root - 2017-12-07 13:31:17.177984: step 15930, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 86h:49m:07s remains)
INFO - root - 2017-12-07 13:31:26.637506: step 15940, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 84h:34m:25s remains)
INFO - root - 2017-12-07 13:31:36.250407: step 15950, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 82h:51m:38s remains)
INFO - root - 2017-12-07 13:31:45.835674: step 15960, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 85h:21m:40s remains)
INFO - root - 2017-12-07 13:31:55.549762: step 15970, loss = 2.07, batch loss = 2.02 (7.8 examples/sec; 1.025 sec/batch; 90h:07m:39s remains)
INFO - root - 2017-12-07 13:32:05.149787: step 15980, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 85h:08m:42s remains)
INFO - root - 2017-12-07 13:32:14.504079: step 15990, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 83h:57m:24s remains)
INFO - root - 2017-12-07 13:32:24.095612: step 16000, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 80h:18m:00s remains)
2017-12-07 13:32:24.948715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2053785 -4.1865377 -4.1851854 -4.1976042 -4.2261586 -4.2430182 -4.2429395 -4.244359 -4.2474022 -4.2614012 -4.27055 -4.2515306 -4.2334962 -4.2329407 -4.2417164][-4.182229 -4.1646852 -4.1689472 -4.1915612 -4.22601 -4.2460084 -4.2451587 -4.2486272 -4.2560325 -4.2676597 -4.2714562 -4.2503786 -4.2259445 -4.221427 -4.2363386][-4.1881819 -4.1797667 -4.1887813 -4.2148161 -4.2483206 -4.26067 -4.253726 -4.2469335 -4.24872 -4.2560372 -4.2549458 -4.2352777 -4.2044449 -4.1893764 -4.2045465][-4.2201343 -4.2167664 -4.2301307 -4.2580862 -4.2854185 -4.2802768 -4.2534661 -4.22146 -4.2061296 -4.2043676 -4.2092524 -4.2065787 -4.1829357 -4.1609421 -4.1642108][-4.223671 -4.2208886 -4.2403679 -4.2733827 -4.2915535 -4.2640629 -4.2066 -4.1372814 -4.09309 -4.0879521 -4.12039 -4.1488047 -4.1436906 -4.1220655 -4.1112986][-4.1882892 -4.1776724 -4.1959133 -4.2371688 -4.2524633 -4.2049184 -4.117465 -4.0109382 -3.9319181 -3.9288857 -4.0048132 -4.0693936 -4.0844345 -4.0705953 -4.0560775][-4.1129622 -4.0788651 -4.0929804 -4.1482968 -4.1749983 -4.1261535 -4.0242743 -3.9038038 -3.8068862 -3.8154948 -3.9333715 -4.0233135 -4.0536456 -4.0435367 -4.0269942][-4.014287 -3.9584284 -3.9641602 -4.0364523 -4.0950155 -4.0736794 -3.9946837 -3.903801 -3.8369229 -3.8651853 -3.9704318 -4.0474305 -4.0763335 -4.0647492 -4.0461884][-3.9826682 -3.9180868 -3.914155 -3.9885571 -4.0703034 -4.0868053 -4.0502009 -4.0052361 -3.9768627 -4.0064917 -4.0754013 -4.1261506 -4.147315 -4.1346073 -4.1144962][-4.0575891 -4.0122914 -4.0046806 -4.051661 -4.1151767 -4.1427064 -4.1335173 -4.1168756 -4.1110988 -4.1380653 -4.1822548 -4.2157083 -4.2295642 -4.2139435 -4.1909747][-4.1497059 -4.1285343 -4.1222916 -4.140348 -4.1658573 -4.1791739 -4.1790171 -4.1767049 -4.1872053 -4.2178297 -4.2527781 -4.2747412 -4.2794437 -4.2604804 -4.2383556][-4.20157 -4.19142 -4.1887517 -4.1892266 -4.18684 -4.1883225 -4.1925564 -4.1970973 -4.21301 -4.2410917 -4.2670856 -4.281795 -4.2824054 -4.2665343 -4.251338][-4.2215281 -4.2077312 -4.2061715 -4.2028351 -4.193573 -4.1943083 -4.2041273 -4.2124128 -4.222477 -4.2397857 -4.2527494 -4.2611 -4.2613869 -4.2518687 -4.2435269][-4.2163806 -4.2020769 -4.2018385 -4.20438 -4.2047882 -4.2129583 -4.2255554 -4.2308717 -4.2313962 -4.2329721 -4.2325964 -4.2370796 -4.2385197 -4.231545 -4.2256117][-4.2054462 -4.197063 -4.1990094 -4.2067556 -4.2140393 -4.2269983 -4.2379069 -4.2378411 -4.2341065 -4.228169 -4.2182493 -4.2195582 -4.2225094 -4.2178049 -4.2125435]]...]
INFO - root - 2017-12-07 13:32:34.670811: step 16010, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 83h:58m:58s remains)
INFO - root - 2017-12-07 13:32:44.442662: step 16020, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 77h:34m:21s remains)
INFO - root - 2017-12-07 13:32:54.055286: step 16030, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 81h:26m:21s remains)
INFO - root - 2017-12-07 13:33:03.866501: step 16040, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 87h:48m:31s remains)
INFO - root - 2017-12-07 13:33:13.444766: step 16050, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 85h:02m:31s remains)
INFO - root - 2017-12-07 13:33:23.185507: step 16060, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 83h:49m:56s remains)
INFO - root - 2017-12-07 13:33:32.716519: step 16070, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 82h:16m:47s remains)
INFO - root - 2017-12-07 13:33:42.331929: step 16080, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 86h:33m:57s remains)
INFO - root - 2017-12-07 13:33:51.944771: step 16090, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.031 sec/batch; 90h:36m:13s remains)
INFO - root - 2017-12-07 13:34:01.755755: step 16100, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.002 sec/batch; 88h:06m:10s remains)
2017-12-07 13:34:02.711233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3162851 -4.319181 -4.3203998 -4.3210397 -4.3209167 -4.3202605 -4.3203588 -4.3207235 -4.3203225 -4.3196878 -4.3198071 -4.3196177 -4.3222189 -4.3293161 -4.3381619][-4.2957287 -4.2992058 -4.3019323 -4.3046527 -4.306417 -4.3080635 -4.3107224 -4.3126955 -4.3128457 -4.3121076 -4.3119473 -4.3105211 -4.3118925 -4.3194909 -4.3305154][-4.2751937 -4.2763052 -4.2763696 -4.2774515 -4.2789516 -4.2825155 -4.2886577 -4.2934275 -4.2960176 -4.2975364 -4.2988496 -4.2977886 -4.2975378 -4.3046994 -4.3169494][-4.2506723 -4.2450933 -4.2375631 -4.2317324 -4.2283678 -4.2294593 -4.2352066 -4.2399378 -4.2449589 -4.2533221 -4.2626724 -4.2672505 -4.2670555 -4.2729487 -4.2865462][-4.2145257 -4.1992021 -4.1813951 -4.1667471 -4.1552606 -4.148356 -4.146018 -4.1442294 -4.1535163 -4.1759653 -4.2009592 -4.2165022 -4.2193723 -4.2259741 -4.2429423][-4.1776252 -4.1529393 -4.1233783 -4.097363 -4.0744433 -4.0541472 -4.03199 -4.0133672 -4.0295 -4.0739779 -4.1187792 -4.14922 -4.1618943 -4.1742196 -4.1977029][-4.1640973 -4.1333537 -4.0929852 -4.0532665 -4.0154972 -3.9772148 -3.9269242 -3.8795409 -3.9026489 -3.9767973 -4.0409069 -4.0843978 -4.1115746 -4.1348877 -4.1668015][-4.1841288 -4.15648 -4.1168942 -4.0723619 -4.02169 -3.9666302 -3.8921583 -3.8208318 -3.8453636 -3.9354682 -4.0061054 -4.053278 -4.0888219 -4.1196828 -4.155097][-4.2078567 -4.1915951 -4.1684861 -4.1389651 -4.0942941 -4.0417271 -3.972477 -3.9088926 -3.9223969 -3.9868193 -4.0356297 -4.0731478 -4.1065254 -4.13669 -4.1687078][-4.2132578 -4.2108192 -4.2095933 -4.2035155 -4.1799054 -4.1473227 -4.1017437 -4.0608826 -4.0659094 -4.0948133 -4.11145 -4.1300697 -4.1526413 -4.1746445 -4.1991639][-4.1968055 -4.205451 -4.2229638 -4.2387786 -4.2372465 -4.2275472 -4.2059765 -4.183538 -4.1819344 -4.186337 -4.18002 -4.1795673 -4.1890092 -4.2026134 -4.2229385][-4.1653218 -4.1815429 -4.2133126 -4.2440619 -4.2591996 -4.2707314 -4.2702107 -4.2611408 -4.255754 -4.246006 -4.2284479 -4.2148156 -4.2121668 -4.2196612 -4.2370744][-4.1646237 -4.1808424 -4.21376 -4.2449441 -4.2667408 -4.2918739 -4.3050737 -4.3050117 -4.3005128 -4.2878957 -4.2677855 -4.2486067 -4.2386451 -4.2410445 -4.2548504][-4.2173409 -4.22773 -4.2488012 -4.267755 -4.2830839 -4.3072 -4.3232365 -4.3270965 -4.3261318 -4.3187976 -4.3050981 -4.2899041 -4.279243 -4.278151 -4.2856288][-4.2806511 -4.2862535 -4.296052 -4.3037119 -4.3107328 -4.3253756 -4.3360777 -4.3395343 -4.3409176 -4.3386397 -4.3323116 -4.32446 -4.3176823 -4.314971 -4.3162761]]...]
INFO - root - 2017-12-07 13:34:12.166708: step 16110, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 80h:31m:28s remains)
INFO - root - 2017-12-07 13:34:21.659627: step 16120, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 84h:41m:06s remains)
INFO - root - 2017-12-07 13:34:31.353362: step 16130, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.995 sec/batch; 87h:26m:24s remains)
INFO - root - 2017-12-07 13:34:41.072479: step 16140, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 80h:33m:52s remains)
INFO - root - 2017-12-07 13:34:50.590572: step 16150, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 84h:20m:39s remains)
INFO - root - 2017-12-07 13:35:00.387566: step 16160, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.031 sec/batch; 90h:35m:25s remains)
INFO - root - 2017-12-07 13:35:10.120038: step 16170, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 86h:10m:16s remains)
INFO - root - 2017-12-07 13:35:19.624533: step 16180, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 81h:33m:44s remains)
INFO - root - 2017-12-07 13:35:29.118288: step 16190, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 85h:15m:07s remains)
INFO - root - 2017-12-07 13:35:38.744465: step 16200, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 88h:57m:16s remains)
2017-12-07 13:35:39.634931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1423316 -4.1332941 -4.151372 -4.1833439 -4.2044549 -4.2125297 -4.2005963 -4.174911 -4.1457839 -4.1271329 -4.1185093 -4.121047 -4.1470819 -4.176053 -4.1886811][-4.1349788 -4.1243911 -4.1342907 -4.1586161 -4.1835275 -4.192832 -4.1857815 -4.1647038 -4.1428332 -4.1243362 -4.1139317 -4.1144848 -4.1377063 -4.1656179 -4.177762][-4.1340427 -4.125824 -4.1290941 -4.1438165 -4.1655746 -4.1745639 -4.1693726 -4.1529331 -4.141861 -4.134376 -4.1295757 -4.1300983 -4.1430445 -4.1627078 -4.1731887][-4.1353564 -4.1332493 -4.1396527 -4.1503954 -4.1607337 -4.1629105 -4.1527977 -4.13468 -4.1270976 -4.1262927 -4.1289945 -4.1312838 -4.1366544 -4.1502223 -4.1627483][-4.1255131 -4.1330366 -4.1505289 -4.1651497 -4.1676059 -4.1584907 -4.1334529 -4.1028576 -4.09244 -4.0998659 -4.1109381 -4.1162448 -4.1191311 -4.1263728 -4.1386657][-4.1092196 -4.1207685 -4.150054 -4.17608 -4.1764784 -4.1523919 -4.1052761 -4.0538049 -4.0431757 -4.0675693 -4.0902095 -4.1045403 -4.1115069 -4.1143346 -4.1190019][-4.0817747 -4.0952334 -4.134481 -4.1670728 -4.1658416 -4.1278663 -4.0572419 -3.9880209 -3.9858158 -4.0365739 -4.0774994 -4.1013927 -4.1087923 -4.10602 -4.0996947][-4.065948 -4.0815077 -4.1231236 -4.1510878 -4.1440253 -4.0934405 -4.0047026 -3.9253125 -3.9356461 -4.0094709 -4.0675211 -4.1004372 -4.1053882 -4.0999665 -4.0918746][-4.0828633 -4.1000609 -4.133728 -4.1483645 -4.1299815 -4.0753121 -3.9903173 -3.9261072 -3.9438288 -4.01596 -4.0760021 -4.1103535 -4.1153321 -4.1100855 -4.102335][-4.1173115 -4.1330595 -4.1535454 -4.1557908 -4.1323409 -4.0847096 -4.0236244 -3.98836 -4.0081291 -4.0585103 -4.1030331 -4.13288 -4.1403208 -4.1364317 -4.1289172][-4.1600528 -4.1719995 -4.1786714 -4.1712956 -4.1490016 -4.1157637 -4.0823784 -4.0707068 -4.0896535 -4.125524 -4.1550088 -4.1757517 -4.1801329 -4.1757812 -4.1684055][-4.1952634 -4.2019587 -4.1990466 -4.1864481 -4.1695271 -4.1535535 -4.1452017 -4.1514812 -4.1684608 -4.1941171 -4.2133136 -4.2252364 -4.2259378 -4.2221971 -4.2139506][-4.2277827 -4.230608 -4.2244205 -4.2130027 -4.2034149 -4.1971965 -4.1991234 -4.2106538 -4.226284 -4.2455072 -4.2593431 -4.2657981 -4.2656364 -4.2631006 -4.2563214][-4.2639446 -4.2598391 -4.2528911 -4.2464776 -4.2407479 -4.2365665 -4.2403431 -4.2505369 -4.2641878 -4.2794123 -4.2902074 -4.2959247 -4.2974358 -4.2968755 -4.292954][-4.28453 -4.2776394 -4.2713609 -4.2687721 -4.266243 -4.2648458 -4.2675238 -4.2755079 -4.2864561 -4.2982655 -4.3064017 -4.3116241 -4.3131452 -4.3127956 -4.3107605]]...]
INFO - root - 2017-12-07 13:35:49.274215: step 16210, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 81h:14m:13s remains)
INFO - root - 2017-12-07 13:35:58.911956: step 16220, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 83h:54m:46s remains)
INFO - root - 2017-12-07 13:36:08.299059: step 16230, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 83h:38m:33s remains)
INFO - root - 2017-12-07 13:36:17.915001: step 16240, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 87h:43m:57s remains)
INFO - root - 2017-12-07 13:36:27.711893: step 16250, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 84h:53m:07s remains)
INFO - root - 2017-12-07 13:36:37.334307: step 16260, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 84h:17m:19s remains)
INFO - root - 2017-12-07 13:36:47.060393: step 16270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 85h:57m:19s remains)
INFO - root - 2017-12-07 13:36:56.605502: step 16280, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 73h:31m:53s remains)
INFO - root - 2017-12-07 13:37:06.211848: step 16290, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 86h:23m:38s remains)
INFO - root - 2017-12-07 13:37:15.720138: step 16300, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 81h:31m:28s remains)
2017-12-07 13:37:16.700259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3395042 -4.3347468 -4.3316026 -4.3274574 -4.320498 -4.3129449 -4.3091784 -4.3104386 -4.3167543 -4.3244281 -4.3279309 -4.3233514 -4.3113675 -4.3003 -4.2968359][-4.3298922 -4.3178582 -4.3122587 -4.3072972 -4.2975912 -4.2854853 -4.2759418 -4.2725277 -4.2779632 -4.2893782 -4.2985153 -4.2949753 -4.2780809 -4.2593951 -4.2529411][-4.3079619 -4.2899261 -4.28515 -4.2844725 -4.2756767 -4.2569504 -4.2355003 -4.2212529 -4.2242236 -4.2410512 -4.2580247 -4.2583313 -4.2372661 -4.2103305 -4.1975651][-4.2840371 -4.26465 -4.2656469 -4.2739463 -4.2690105 -4.242887 -4.2041311 -4.1741023 -4.1713791 -4.197288 -4.2273989 -4.2357159 -4.2141356 -4.1773205 -4.1515522][-4.2629695 -4.244576 -4.2518849 -4.2689142 -4.2666874 -4.233603 -4.17799 -4.1265225 -4.1134186 -4.1520662 -4.2038193 -4.2264876 -4.21281 -4.1725111 -4.1325836][-4.2404232 -4.2250118 -4.237896 -4.2607327 -4.2591648 -4.218308 -4.144537 -4.0674024 -4.0383058 -4.0921679 -4.1734738 -4.2197032 -4.2238817 -4.1914978 -4.1423564][-4.2132683 -4.2024536 -4.2217345 -4.2481136 -4.2462373 -4.1998634 -4.1098843 -4.0076971 -3.9571393 -4.0206761 -4.1335411 -4.2115993 -4.2389765 -4.2191072 -4.1657357][-4.1886482 -4.1805468 -4.2054157 -4.2368879 -4.2376075 -4.190145 -4.0967827 -3.9828277 -3.9118218 -3.9688306 -4.0982924 -4.2044497 -4.2550807 -4.247788 -4.1954274][-4.1734238 -4.1633196 -4.1910882 -4.2290683 -4.2410917 -4.2065353 -4.1299477 -4.031467 -3.958328 -3.9880409 -4.0977564 -4.207418 -4.2720714 -4.2755394 -4.2281122][-4.1739745 -4.159018 -4.1857314 -4.2278514 -4.2530632 -4.2415786 -4.1931262 -4.119535 -4.0539179 -4.0519738 -4.1207714 -4.2118669 -4.2770557 -4.2878695 -4.2459874][-4.1902843 -4.1691518 -4.1881857 -4.2266755 -4.2587461 -4.2679591 -4.247026 -4.1982851 -4.1438408 -4.1242423 -4.1575675 -4.219811 -4.274837 -4.2871737 -4.2533488][-4.2236714 -4.199142 -4.2078824 -4.2394028 -4.2729325 -4.2935185 -4.2935834 -4.2674384 -4.2263889 -4.2019324 -4.2124286 -4.2484627 -4.2892952 -4.3025136 -4.2777958][-4.2660785 -4.2422881 -4.2424622 -4.26791 -4.3005486 -4.3227096 -4.3308516 -4.3206778 -4.2940512 -4.2720275 -4.27018 -4.2879086 -4.3162451 -4.3280311 -4.3097215][-4.3029404 -4.2853274 -4.2819896 -4.2999411 -4.3262744 -4.3445382 -4.3529329 -4.3506012 -4.3371978 -4.3216686 -4.3140955 -4.317678 -4.3342385 -4.3419485 -4.327867][-4.3232517 -4.3128862 -4.3087225 -4.31802 -4.3341451 -4.34472 -4.3503895 -4.3535433 -4.3497639 -4.3404098 -4.3316112 -4.3287897 -4.3358016 -4.3379159 -4.3275232]]...]
INFO - root - 2017-12-07 13:37:26.498532: step 16310, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.936 sec/batch; 82h:12m:55s remains)
INFO - root - 2017-12-07 13:37:36.100770: step 16320, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 81h:36m:53s remains)
INFO - root - 2017-12-07 13:37:45.629970: step 16330, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 86h:53m:20s remains)
INFO - root - 2017-12-07 13:37:55.266287: step 16340, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 87h:52m:32s remains)
INFO - root - 2017-12-07 13:38:04.908415: step 16350, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 86h:24m:05s remains)
INFO - root - 2017-12-07 13:38:14.524149: step 16360, loss = 2.12, batch loss = 2.07 (8.4 examples/sec; 0.949 sec/batch; 83h:19m:52s remains)
INFO - root - 2017-12-07 13:38:24.133672: step 16370, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 84h:06m:47s remains)
INFO - root - 2017-12-07 13:38:33.761095: step 16380, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.010 sec/batch; 88h:41m:34s remains)
INFO - root - 2017-12-07 13:38:43.516584: step 16390, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 86h:06m:54s remains)
INFO - root - 2017-12-07 13:38:52.950612: step 16400, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 84h:03m:22s remains)
2017-12-07 13:38:53.911702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2754622 -4.2667451 -4.2464318 -4.2295704 -4.2021804 -4.15324 -4.10491 -4.0749049 -4.0868311 -4.1213026 -4.1622453 -4.197576 -4.2207441 -4.2388625 -4.2708178][-4.2613053 -4.2561712 -4.2413683 -4.2298617 -4.1988592 -4.1442628 -4.0940595 -4.0665312 -4.0826759 -4.118228 -4.1592331 -4.1917181 -4.2116389 -4.22977 -4.2614655][-4.2428565 -4.2412395 -4.2370739 -4.2403789 -4.2219062 -4.1735549 -4.1246252 -4.0962706 -4.1049504 -4.1344719 -4.167892 -4.1906495 -4.2048969 -4.2212176 -4.2540374][-4.2194161 -4.2162576 -4.219368 -4.2401304 -4.2416167 -4.2057261 -4.1500459 -4.1033087 -4.1002154 -4.129334 -4.1675019 -4.1929297 -4.2066641 -4.2232132 -4.255372][-4.1900086 -4.1849656 -4.1915865 -4.2216039 -4.2317381 -4.1969552 -4.1216321 -4.0475936 -4.0446887 -4.0966444 -4.1557436 -4.1962605 -4.2155223 -4.2319317 -4.2613459][-4.1518216 -4.147438 -4.1600509 -4.1925578 -4.1981878 -4.1510539 -4.0451789 -3.9263403 -3.9269049 -4.0239019 -4.1214623 -4.1853204 -4.2167397 -4.2392917 -4.2681165][-4.1170974 -4.1214828 -4.1401548 -4.1695004 -4.163681 -4.0931768 -3.9460602 -3.775419 -3.788049 -3.9463453 -4.0884542 -4.1735468 -4.2158904 -4.2419481 -4.2703757][-4.11423 -4.1249495 -4.1472125 -4.1659269 -4.1456137 -4.055923 -3.8889937 -3.7094841 -3.7511742 -3.9452765 -4.0975447 -4.181706 -4.2225657 -4.2444673 -4.2689457][-4.15412 -4.1677823 -4.186358 -4.1947441 -4.1708646 -4.0942636 -3.9717114 -3.8619123 -3.9083042 -4.0507207 -4.1551523 -4.2115207 -4.2361836 -4.24993 -4.2703509][-4.2040062 -4.21863 -4.2332864 -4.2404623 -4.2204037 -4.1605954 -4.081727 -4.0227122 -4.0563173 -4.1406889 -4.1994038 -4.2330489 -4.24663 -4.2582593 -4.2753162][-4.2335529 -4.2438059 -4.2540827 -4.2587485 -4.2417479 -4.1970472 -4.1479869 -4.1171079 -4.1420517 -4.1914687 -4.2281885 -4.2504759 -4.2590933 -4.2691793 -4.2832193][-4.2280793 -4.2376571 -4.245965 -4.247025 -4.231122 -4.198297 -4.16378 -4.1434894 -4.1611967 -4.1906633 -4.2185054 -4.24096 -4.2572246 -4.2716489 -4.2871995][-4.2104425 -4.2182436 -4.222456 -4.2224207 -4.207231 -4.1776533 -4.1488781 -4.1305175 -4.1399264 -4.1591659 -4.1902661 -4.226006 -4.2557869 -4.2753806 -4.293211][-4.20697 -4.2108474 -4.208158 -4.2081037 -4.2029195 -4.184629 -4.1660872 -4.151247 -4.1561871 -4.1696634 -4.2038808 -4.2496533 -4.281106 -4.2969403 -4.3111124][-4.2129779 -4.2112346 -4.2049789 -4.2076154 -4.2133741 -4.20915 -4.2015772 -4.1928887 -4.1960979 -4.2092252 -4.2461133 -4.2928476 -4.3173447 -4.3255 -4.332058]]...]
INFO - root - 2017-12-07 13:39:03.543758: step 16410, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 84h:09m:24s remains)
INFO - root - 2017-12-07 13:39:13.233484: step 16420, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 84h:35m:00s remains)
INFO - root - 2017-12-07 13:39:22.828532: step 16430, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 77h:04m:42s remains)
INFO - root - 2017-12-07 13:39:32.464263: step 16440, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 81h:31m:18s remains)
INFO - root - 2017-12-07 13:39:42.156967: step 16450, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 86h:45m:34s remains)
INFO - root - 2017-12-07 13:39:51.915823: step 16460, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 86h:30m:19s remains)
INFO - root - 2017-12-07 13:40:01.682106: step 16470, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 85h:07m:47s remains)
INFO - root - 2017-12-07 13:40:11.244114: step 16480, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 81h:15m:04s remains)
INFO - root - 2017-12-07 13:40:20.876121: step 16490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 84h:59m:30s remains)
INFO - root - 2017-12-07 13:40:30.512600: step 16500, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.986 sec/batch; 86h:32m:30s remains)
2017-12-07 13:40:31.487683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2593684 -4.2617016 -4.2658005 -4.2700548 -4.2753177 -4.2788634 -4.2798743 -4.2794919 -4.2757421 -4.2684937 -4.2611251 -4.2568569 -4.252295 -4.2431259 -4.231802][-4.2444835 -4.247633 -4.2491636 -4.2488356 -4.252111 -4.2570472 -4.2633486 -4.2698517 -4.2721782 -4.2693791 -4.2649403 -4.263238 -4.2601686 -4.2497439 -4.2396517][-4.2305832 -4.2380409 -4.2389517 -4.2338037 -4.2314539 -4.2317362 -4.2376614 -4.2483206 -4.2565923 -4.2582684 -4.2576146 -4.2616072 -4.2650042 -4.2582388 -4.248991][-4.2048531 -4.2162724 -4.2168994 -4.2082 -4.1980171 -4.1882424 -4.1878505 -4.1959248 -4.206326 -4.2123218 -4.2176495 -4.2326751 -4.2473369 -4.2467155 -4.2353168][-4.1617942 -4.1737432 -4.1724262 -4.1629157 -4.151669 -4.1394362 -4.1373181 -4.1407018 -4.1467137 -4.1507955 -4.1561427 -4.1742697 -4.196074 -4.2030668 -4.1936831][-4.117413 -4.1299343 -4.1302266 -4.1255569 -4.1240487 -4.1207719 -4.1189585 -4.1125135 -4.1037221 -4.0950513 -4.0920081 -4.1050487 -4.1271596 -4.1413369 -4.1425505][-4.1043715 -4.1151333 -4.1174569 -4.1176157 -4.1224709 -4.1248131 -4.1219559 -4.1034369 -4.0740027 -4.0491304 -4.0359659 -4.0403991 -4.0586176 -4.078475 -4.0942883][-4.1150908 -4.1218262 -4.1220517 -4.1245713 -4.13055 -4.1315413 -4.1268191 -4.0999832 -4.0561466 -4.0201726 -3.9958031 -3.9856083 -3.9926341 -4.019887 -4.0528674][-4.1281042 -4.1296935 -4.1253023 -4.1261225 -4.1330638 -4.1379914 -4.1361847 -4.10705 -4.0638938 -4.032155 -4.0102615 -3.9936094 -3.9896626 -4.0119567 -4.0443382][-4.141243 -4.1385951 -4.1276188 -4.1245308 -4.1284585 -4.1344995 -4.1339221 -4.1094556 -4.0816903 -4.0697503 -4.0669856 -4.0623531 -4.0587091 -4.0641379 -4.0743279][-4.1640053 -4.157124 -4.1418786 -4.1312852 -4.1242657 -4.1225772 -4.1185789 -4.1010542 -4.0909982 -4.0963712 -4.1112046 -4.1218777 -4.1258116 -4.118176 -4.1069665][-4.1799526 -4.172689 -4.157867 -4.1437011 -4.1293445 -4.1206512 -4.1097593 -4.0922241 -4.0891104 -4.1052151 -4.1320376 -4.1525145 -4.1617985 -4.1509943 -4.136476][-4.176549 -4.1775479 -4.1722031 -4.165575 -4.1556082 -4.1419215 -4.1227069 -4.1040859 -4.1024709 -4.1196256 -4.14646 -4.1715322 -4.1864848 -4.1830373 -4.1827602][-4.1679568 -4.1774621 -4.1822958 -4.1856809 -4.1793046 -4.16133 -4.1399817 -4.1254416 -4.1271029 -4.1429472 -4.1646171 -4.190124 -4.20999 -4.2192297 -4.23663][-4.174542 -4.1848865 -4.186501 -4.1889205 -4.1809182 -4.1612821 -4.1437731 -4.1384926 -4.1489644 -4.1666775 -4.1850986 -4.20694 -4.2251463 -4.2435775 -4.2735062]]...]
INFO - root - 2017-12-07 13:40:41.277749: step 16510, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 82h:04m:50s remains)
INFO - root - 2017-12-07 13:40:50.989008: step 16520, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 83h:26m:43s remains)
INFO - root - 2017-12-07 13:41:00.546777: step 16530, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 87h:13m:29s remains)
INFO - root - 2017-12-07 13:41:10.167584: step 16540, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 84h:15m:45s remains)
INFO - root - 2017-12-07 13:41:19.882593: step 16550, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 84h:00m:48s remains)
INFO - root - 2017-12-07 13:41:29.555097: step 16560, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.986 sec/batch; 86h:33m:17s remains)
INFO - root - 2017-12-07 13:41:38.864159: step 16570, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.010 sec/batch; 88h:38m:25s remains)
INFO - root - 2017-12-07 13:41:48.286109: step 16580, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 82h:58m:52s remains)
INFO - root - 2017-12-07 13:41:57.772017: step 16590, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 87h:05m:33s remains)
INFO - root - 2017-12-07 13:42:07.414268: step 16600, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 82h:05m:22s remains)
2017-12-07 13:42:08.338379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3447862 -4.3439813 -4.3417077 -4.3428688 -4.3364868 -4.319562 -4.2920918 -4.2620749 -4.2424197 -4.2458482 -4.2767773 -4.3131275 -4.3412104 -4.3590908 -4.3695593][-4.3534942 -4.3485017 -4.3399496 -4.3349786 -4.3205957 -4.2959256 -4.2641411 -4.2317038 -4.2091985 -4.2166352 -4.25894 -4.30037 -4.3316827 -4.35389 -4.3649669][-4.3599591 -4.3485665 -4.3282328 -4.3146534 -4.2955146 -4.2693477 -4.2366347 -4.2053156 -4.1844158 -4.1983972 -4.2521043 -4.2959027 -4.3264337 -4.3497858 -4.361022][-4.3567224 -4.3365545 -4.30221 -4.2776189 -4.2527957 -4.2218394 -4.1860752 -4.1588864 -4.1497107 -4.1765571 -4.2402215 -4.2881846 -4.3208079 -4.3456869 -4.3576317][-4.3351336 -4.3049927 -4.2550883 -4.2135954 -4.1776171 -4.1416159 -4.1096897 -4.0941334 -4.1014819 -4.1442528 -4.2094679 -4.257206 -4.2973971 -4.3292351 -4.3471565][-4.2926893 -4.2516384 -4.1842241 -4.1168404 -4.0632057 -4.0297704 -4.0167022 -4.0177665 -4.0414133 -4.0911331 -4.1414423 -4.1863074 -4.243042 -4.2904038 -4.3201632][-4.2353764 -4.1858487 -4.0993814 -4.0037928 -3.9297333 -3.9027309 -3.9182487 -3.9405921 -3.9744425 -4.0130348 -4.0409441 -4.0865006 -4.1618633 -4.2253518 -4.2709818][-4.1889167 -4.1385989 -4.0399156 -3.9176862 -3.8225327 -3.8014996 -3.841424 -3.8762827 -3.8993897 -3.9071062 -3.9179564 -3.9788036 -4.0790014 -4.1596451 -4.222898][-4.1781282 -4.1337028 -4.0376854 -3.9086204 -3.805141 -3.7898493 -3.8309536 -3.8529782 -3.8489687 -3.8281024 -3.8344347 -3.9146013 -4.0340762 -4.1287856 -4.2035971][-4.2063184 -4.1715961 -4.0941596 -3.9852974 -3.9000082 -3.890728 -3.91093 -3.9088352 -3.8902481 -3.8666124 -3.8800795 -3.961858 -4.0706711 -4.1601486 -4.230494][-4.2572489 -4.2348995 -4.18227 -4.1095796 -4.059443 -4.0568142 -4.0643234 -4.0481625 -4.03024 -4.0195627 -4.0339675 -4.0979838 -4.1754065 -4.2422466 -4.2942929][-4.3106632 -4.3006 -4.2713065 -4.2316322 -4.2102175 -4.2128773 -4.2151523 -4.2017341 -4.1920233 -4.1915574 -4.20235 -4.2415037 -4.2884641 -4.3292456 -4.3590407][-4.3487387 -4.3453851 -4.3325319 -4.3160877 -4.311357 -4.3152108 -4.3170357 -4.3108678 -4.3073492 -4.30873 -4.3132148 -4.3329968 -4.3607373 -4.3835096 -4.3971319][-4.368175 -4.3685641 -4.3636971 -4.3585243 -4.3588867 -4.3620458 -4.3636956 -4.3630543 -4.3639679 -4.3646603 -4.3644819 -4.3728356 -4.3876839 -4.4001551 -4.4060674][-4.3771544 -4.3802176 -4.379045 -4.3777785 -4.378633 -4.3806129 -4.3815694 -4.38137 -4.382236 -4.3826904 -4.3828211 -4.3863568 -4.3927121 -4.3974619 -4.3994212]]...]
INFO - root - 2017-12-07 13:42:18.184035: step 16610, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 87h:28m:34s remains)
INFO - root - 2017-12-07 13:42:27.925946: step 16620, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 83h:48m:31s remains)
INFO - root - 2017-12-07 13:42:37.394577: step 16630, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 82h:54m:08s remains)
INFO - root - 2017-12-07 13:42:47.009091: step 16640, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 84h:06m:25s remains)
INFO - root - 2017-12-07 13:42:56.721515: step 16650, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 86h:41m:48s remains)
INFO - root - 2017-12-07 13:43:06.426469: step 16660, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 86h:21m:59s remains)
INFO - root - 2017-12-07 13:43:15.922032: step 16670, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 85h:49m:29s remains)
INFO - root - 2017-12-07 13:43:25.616518: step 16680, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 85h:11m:35s remains)
INFO - root - 2017-12-07 13:43:35.293095: step 16690, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 84h:39m:23s remains)
INFO - root - 2017-12-07 13:43:45.020436: step 16700, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 86h:09m:56s remains)
2017-12-07 13:43:45.970050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3582549 -4.35542 -4.35527 -4.3557143 -4.3542542 -4.35111 -4.3480296 -4.3477035 -4.3467135 -4.3434105 -4.3360319 -4.3264017 -4.3128805 -4.2997556 -4.2910433][-4.3591957 -4.3533363 -4.3508239 -4.3503056 -4.3491592 -4.3462367 -4.34404 -4.3443465 -4.3440289 -4.3411646 -4.3341618 -4.3258767 -4.3156462 -4.3025765 -4.2904348][-4.3636131 -4.3551421 -4.3490429 -4.3453093 -4.3419075 -4.3371396 -4.3346038 -4.3354969 -4.3351583 -4.3322735 -4.3263984 -4.3193412 -4.3126531 -4.3016572 -4.2888522][-4.3691068 -4.3570132 -4.3433194 -4.3309536 -4.3204842 -4.3093777 -4.3005791 -4.2985625 -4.3009768 -4.3012781 -4.2989964 -4.2964745 -4.2972589 -4.2925506 -4.282825][-4.3641353 -4.343142 -4.3170838 -4.2907729 -4.2662868 -4.2374368 -4.2062521 -4.1932969 -4.2055531 -4.2161984 -4.2212157 -4.2282863 -4.2422395 -4.2515411 -4.2525234][-4.3333125 -4.2972331 -4.256587 -4.2190065 -4.1780157 -4.1187162 -4.0437083 -4.0122881 -4.0515385 -4.0888648 -4.1098819 -4.1294346 -4.1550021 -4.1794004 -4.1940894][-4.2879996 -4.24532 -4.2044425 -4.1709604 -4.1248827 -4.0415335 -3.9268277 -3.8841836 -3.957273 -4.02713 -4.0658665 -4.0901937 -4.1129513 -4.1387877 -4.1572871][-4.2641683 -4.2285862 -4.1997643 -4.1840868 -4.1569619 -4.0891118 -3.990324 -3.9575787 -4.0236459 -4.0856605 -4.111156 -4.1184878 -4.1222253 -4.1334291 -4.1433015][-4.258832 -4.2293634 -4.2080355 -4.2048664 -4.1995611 -4.1634068 -4.1022825 -4.079535 -4.1183839 -4.1552582 -4.1634183 -4.1557708 -4.1476607 -4.1488419 -4.1538806][-4.2567134 -4.2299027 -4.2130795 -4.2149673 -4.2201872 -4.2036958 -4.1662374 -4.1479197 -4.1668048 -4.1882358 -4.1888862 -4.1826925 -4.1796646 -4.1829042 -4.190731][-4.2652316 -4.2408524 -4.2270136 -4.228735 -4.2333703 -4.2231212 -4.1973715 -4.1820436 -4.1929741 -4.2061915 -4.2067604 -4.2069716 -4.2127275 -4.22236 -4.2369704][-4.2860017 -4.266119 -4.2545128 -4.2542496 -4.2589111 -4.2550735 -4.2417088 -4.2343173 -4.24306 -4.2525411 -4.254168 -4.256072 -4.2626162 -4.2705293 -4.2822137][-4.3134542 -4.2994628 -4.2914929 -4.2932281 -4.3024311 -4.3068581 -4.3044224 -4.3037343 -4.310585 -4.3172011 -4.3175421 -4.3168244 -4.31863 -4.3210778 -4.326592][-4.3380156 -4.3291464 -4.3245912 -4.3279486 -4.3386836 -4.3462744 -4.3485775 -4.3494816 -4.3551006 -4.361187 -4.363513 -4.363894 -4.3639765 -4.3637371 -4.3641329][-4.3498874 -4.3444223 -4.3416519 -4.345026 -4.3522663 -4.35654 -4.3577728 -4.35768 -4.3609514 -4.3655148 -4.3692737 -4.372252 -4.3733463 -4.3735542 -4.3727379]]...]
INFO - root - 2017-12-07 13:43:55.508600: step 16710, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 81h:43m:35s remains)
INFO - root - 2017-12-07 13:44:05.201055: step 16720, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 82h:43m:13s remains)
INFO - root - 2017-12-07 13:44:14.657583: step 16730, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 82h:47m:22s remains)
INFO - root - 2017-12-07 13:44:24.232334: step 16740, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 87h:47m:35s remains)
INFO - root - 2017-12-07 13:44:33.971255: step 16750, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 82h:39m:56s remains)
INFO - root - 2017-12-07 13:44:43.707765: step 16760, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 85h:47m:06s remains)
INFO - root - 2017-12-07 13:44:53.270337: step 16770, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 85h:27m:54s remains)
INFO - root - 2017-12-07 13:45:02.866467: step 16780, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 83h:17m:29s remains)
INFO - root - 2017-12-07 13:45:12.708227: step 16790, loss = 2.12, batch loss = 2.06 (8.1 examples/sec; 0.990 sec/batch; 86h:46m:40s remains)
INFO - root - 2017-12-07 13:45:22.410742: step 16800, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.961 sec/batch; 84h:15m:07s remains)
2017-12-07 13:45:23.358176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2911897 -4.2812514 -4.2710147 -4.2633448 -4.2483521 -4.2255893 -4.1978769 -4.182332 -4.1910138 -4.2065439 -4.2223539 -4.2358651 -4.2395434 -4.2473063 -4.2476063][-4.2909904 -4.2812009 -4.2711725 -4.260994 -4.2402005 -4.2071381 -4.1691127 -4.14954 -4.1685581 -4.1986918 -4.2256284 -4.2391629 -4.2402287 -4.244978 -4.2432346][-4.2882481 -4.2765083 -4.2669053 -4.2613158 -4.2410979 -4.19867 -4.1556063 -4.1382971 -4.1697068 -4.2109561 -4.2431827 -4.2539892 -4.25102 -4.2500515 -4.2424316][-4.274713 -4.2577782 -4.2511086 -4.2551289 -4.2421174 -4.200839 -4.1572776 -4.1449909 -4.1878147 -4.2312503 -4.2597761 -4.2661572 -4.2623482 -4.2598577 -4.2460093][-4.2494173 -4.2275977 -4.2281513 -4.2424674 -4.2351227 -4.196053 -4.1520677 -4.1479278 -4.1956544 -4.2383509 -4.2604451 -4.2576656 -4.2522593 -4.2490854 -4.2359505][-4.21744 -4.1882162 -4.1903505 -4.2086239 -4.2037816 -4.1577768 -4.1007414 -4.0952835 -4.150991 -4.2023396 -4.222518 -4.2164903 -4.2155633 -4.2140121 -4.2048206][-4.180233 -4.1425714 -4.1428628 -4.1589556 -4.149756 -4.0869813 -4.0006127 -3.9797995 -4.0501208 -4.1237822 -4.1549644 -4.160605 -4.1696763 -4.1721711 -4.1670728][-4.1438355 -4.09846 -4.0946937 -4.103466 -4.0830121 -3.9959919 -3.876497 -3.8459079 -3.9441216 -4.0504427 -4.1041489 -4.1216407 -4.1391211 -4.1483197 -4.1499925][-4.1331577 -4.0897603 -4.0844593 -4.0839515 -4.0503049 -3.9511395 -3.8263426 -3.81054 -3.9301085 -4.0473208 -4.1061697 -4.1246624 -4.1418395 -4.1531405 -4.1583533][-4.1627049 -4.1338053 -4.1307135 -4.1255789 -4.0926309 -4.0161195 -3.9366446 -3.9429145 -4.033277 -4.1202641 -4.16474 -4.1756406 -4.1873083 -4.1952224 -4.2006955][-4.2097106 -4.1970668 -4.2011452 -4.1976933 -4.1729112 -4.1251407 -4.0861177 -4.0974426 -4.150465 -4.2053318 -4.2334528 -4.241034 -4.2512197 -4.2554989 -4.2557712][-4.255825 -4.2549162 -4.2676573 -4.2674646 -4.249773 -4.2163296 -4.1934557 -4.2007346 -4.2303052 -4.2660236 -4.2877364 -4.2974706 -4.3063173 -4.3062754 -4.3036642][-4.2909689 -4.2960863 -4.311594 -4.3139811 -4.299911 -4.2743745 -4.2575178 -4.25783 -4.2733903 -4.2987041 -4.3158612 -4.3245468 -4.3298988 -4.3277326 -4.3238463][-4.3047123 -4.3119283 -4.3263841 -4.3314481 -4.3226047 -4.3032889 -4.2903957 -4.2864957 -4.2949376 -4.31234 -4.3262768 -4.3334713 -4.3358355 -4.3320708 -4.3276572][-4.3061776 -4.3109446 -4.3226113 -4.3294115 -4.3255968 -4.3119478 -4.3005652 -4.2971478 -4.3026023 -4.315227 -4.3255386 -4.3313966 -4.3341017 -4.3321571 -4.3286133]]...]
INFO - root - 2017-12-07 13:45:33.080743: step 16810, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 88h:10m:03s remains)
INFO - root - 2017-12-07 13:45:42.847738: step 16820, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 83h:37m:02s remains)
INFO - root - 2017-12-07 13:45:52.587019: step 16830, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 85h:51m:15s remains)
INFO - root - 2017-12-07 13:46:02.329501: step 16840, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 86h:29m:13s remains)
INFO - root - 2017-12-07 13:46:12.151310: step 16850, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 87h:03m:36s remains)
INFO - root - 2017-12-07 13:46:21.787789: step 16860, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 84h:59m:47s remains)
INFO - root - 2017-12-07 13:46:31.473356: step 16870, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 82h:36m:13s remains)
INFO - root - 2017-12-07 13:46:41.194322: step 16880, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.984 sec/batch; 86h:18m:40s remains)
INFO - root - 2017-12-07 13:46:50.959529: step 16890, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 85h:13m:36s remains)
INFO - root - 2017-12-07 13:47:00.564104: step 16900, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 80h:58m:06s remains)
2017-12-07 13:47:01.509027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3290005 -4.3187437 -4.3093252 -4.3031826 -4.299324 -4.2994051 -4.30523 -4.3110094 -4.3112497 -4.3088555 -4.3094835 -4.3093147 -4.3108273 -4.3157396 -4.3201475][-4.3181829 -4.3024912 -4.2905197 -4.2829113 -4.2785068 -4.27901 -4.2838912 -4.286799 -4.2810159 -4.2691727 -4.2630219 -4.2612963 -4.264245 -4.272656 -4.2826805][-4.305182 -4.2853618 -4.2701406 -4.2629585 -4.2611747 -4.2655883 -4.276731 -4.2830267 -4.2770033 -4.2602296 -4.24501 -4.2364464 -4.2340741 -4.2392797 -4.2511635][-4.2788796 -4.2487288 -4.22312 -4.2170992 -4.222755 -4.2347569 -4.2559986 -4.2701774 -4.269011 -4.253232 -4.2367067 -4.2241769 -4.214746 -4.2155867 -4.2271686][-4.2641153 -4.2192512 -4.1816688 -4.1736403 -4.1859717 -4.2020049 -4.2246437 -4.2418327 -4.2403107 -4.2208238 -4.2013645 -4.1865635 -4.1779013 -4.1812396 -4.1978269][-4.2641907 -4.2148786 -4.1753979 -4.1629272 -4.1717291 -4.1829405 -4.1974673 -4.2093225 -4.2047162 -4.1807084 -4.1521382 -4.1300464 -4.1218948 -4.1279454 -4.1474409][-4.2593422 -4.2137723 -4.1767197 -4.1605115 -4.1629596 -4.1660891 -4.1711707 -4.1776562 -4.1753769 -4.1549931 -4.1273813 -4.1003752 -4.08732 -4.088593 -4.10269][-4.2528906 -4.2121725 -4.1753535 -4.1519823 -4.1455812 -4.1411743 -4.1416779 -4.1501331 -4.1542525 -4.1436033 -4.1248517 -4.1012983 -4.083776 -4.0823951 -4.0909896][-4.2526331 -4.217051 -4.18361 -4.1565628 -4.1416078 -4.1295762 -4.1233149 -4.1239047 -4.129178 -4.1275535 -4.1203547 -4.1069727 -4.092948 -4.092473 -4.0969992][-4.2509956 -4.2203293 -4.1951685 -4.1771421 -4.1649003 -4.1538925 -4.1460562 -4.1429372 -4.1439433 -4.1431179 -4.137351 -4.1213031 -4.1054459 -4.1066337 -4.1117148][-4.2464447 -4.2227583 -4.2098069 -4.205924 -4.20112 -4.1940026 -4.1888962 -4.183023 -4.177043 -4.17439 -4.1681533 -4.1473851 -4.1285534 -4.1327481 -4.1392193][-4.2364697 -4.2204843 -4.2189665 -4.2247958 -4.2243028 -4.2214122 -4.2215939 -4.2152858 -4.2033253 -4.1977 -4.1912541 -4.1718597 -4.1540322 -4.15958 -4.1669841][-4.2327566 -4.2227473 -4.2245808 -4.23 -4.2289338 -4.2305765 -4.2365456 -4.2348857 -4.22484 -4.2231741 -4.2191162 -4.2014136 -4.1831026 -4.1846251 -4.1873145][-4.231205 -4.2297239 -4.2330275 -4.2361832 -4.2311883 -4.2338915 -4.2409291 -4.2425318 -4.2364445 -4.2383471 -4.2370825 -4.2243819 -4.2116256 -4.2096453 -4.2068982][-4.2283716 -4.2303963 -4.2354851 -4.240077 -4.2363873 -4.2402043 -4.2475653 -4.2503204 -4.2445979 -4.2426915 -4.2388892 -4.2293658 -4.2235842 -4.2223754 -4.2167029]]...]
INFO - root - 2017-12-07 13:47:11.091469: step 16910, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 87h:14m:57s remains)
INFO - root - 2017-12-07 13:47:20.977934: step 16920, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 87h:21m:59s remains)
INFO - root - 2017-12-07 13:47:30.574463: step 16930, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 77h:47m:52s remains)
INFO - root - 2017-12-07 13:47:40.246131: step 16940, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 86h:38m:13s remains)
INFO - root - 2017-12-07 13:47:50.073806: step 16950, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 83h:01m:12s remains)
INFO - root - 2017-12-07 13:47:59.696908: step 16960, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 79h:41m:16s remains)
INFO - root - 2017-12-07 13:48:09.259210: step 16970, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 82h:32m:52s remains)
INFO - root - 2017-12-07 13:48:19.040141: step 16980, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 87h:19m:34s remains)
INFO - root - 2017-12-07 13:48:28.751052: step 16990, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 84h:18m:46s remains)
INFO - root - 2017-12-07 13:48:38.465354: step 17000, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 82h:09m:19s remains)
2017-12-07 13:48:39.329388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2896051 -4.2678547 -4.2379622 -4.1974649 -4.1607542 -4.1480632 -4.1716452 -4.2041717 -4.2239509 -4.2257118 -4.2292104 -4.2525158 -4.2735825 -4.2790132 -4.2741318][-4.2771711 -4.2417974 -4.1934958 -4.1296797 -4.0735321 -4.05664 -4.09176 -4.132689 -4.1565781 -4.1660171 -4.1842237 -4.2268472 -4.2611251 -4.2726374 -4.2692475][-4.2677193 -4.2185788 -4.1466732 -4.0517082 -3.9680696 -3.9495752 -4.0040112 -4.0627632 -4.0937333 -4.1136374 -4.1516342 -4.21358 -4.2611842 -4.2824821 -4.2835107][-4.2664251 -4.2069039 -4.1174641 -4.0012527 -3.9002151 -3.8786664 -3.9467924 -4.0274868 -4.0713305 -4.103056 -4.1535015 -4.2179685 -4.2669182 -4.2943916 -4.2986073][-4.2699828 -4.2099652 -4.1199241 -4.0095959 -3.9105992 -3.8755379 -3.9263732 -4.0183086 -4.0869231 -4.1337628 -4.1849561 -4.2369933 -4.2746482 -4.2966366 -4.2999973][-4.2817774 -4.2270031 -4.1454864 -4.0463705 -3.9439759 -3.8682683 -3.872932 -3.9785666 -4.0856018 -4.1550927 -4.2031813 -4.2363377 -4.2525411 -4.258194 -4.2612848][-4.2936077 -4.2439823 -4.167295 -4.0707021 -3.9471927 -3.8146276 -3.7669897 -3.8953686 -4.0490646 -4.1401305 -4.1814489 -4.1939149 -4.1864977 -4.1732831 -4.1823397][-4.3000531 -4.2549868 -4.1814537 -4.0837703 -3.9506891 -3.802983 -3.7455449 -3.8770044 -4.0346241 -4.117547 -4.136415 -4.1200662 -4.08445 -4.058672 -4.0871444][-4.3007402 -4.2597475 -4.1956072 -4.1102843 -3.9999642 -3.8931415 -3.8706026 -3.9772186 -4.0857263 -4.1274705 -4.1107359 -4.059691 -4.0020638 -3.9830136 -4.03867][-4.3018727 -4.2660575 -4.2183385 -4.1572971 -4.0810089 -4.0214767 -4.0278625 -4.1027083 -4.1595855 -4.1670928 -4.1278996 -4.060575 -4.0067959 -4.0095429 -4.0770297][-4.2987366 -4.2706938 -4.2396555 -4.2038012 -4.160574 -4.1363149 -4.1562285 -4.2025251 -4.22575 -4.2165966 -4.1757064 -4.1167026 -4.0827255 -4.1049609 -4.1656728][-4.2944536 -4.2730947 -4.2533855 -4.2382183 -4.224133 -4.2241664 -4.2472067 -4.2740483 -4.2811408 -4.2683368 -4.2340012 -4.1936793 -4.1795945 -4.2079949 -4.252914][-4.3015351 -4.2847595 -4.2725439 -4.2712574 -4.2754135 -4.2876287 -4.3075762 -4.3233032 -4.3248835 -4.315805 -4.2908125 -4.262888 -4.2591519 -4.282371 -4.3115039][-4.315527 -4.3035507 -4.2984571 -4.305644 -4.3188696 -4.3330517 -4.34515 -4.353189 -4.351902 -4.3422718 -4.3235607 -4.3056412 -4.3048716 -4.3208919 -4.3405662][-4.3283954 -4.3217359 -4.32102 -4.33037 -4.34421 -4.35597 -4.3633394 -4.367425 -4.36516 -4.3557453 -4.3419766 -4.3293586 -4.3276691 -4.3376813 -4.3497767]]...]
INFO - root - 2017-12-07 13:48:48.919090: step 17010, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.006 sec/batch; 88h:10m:32s remains)
INFO - root - 2017-12-07 13:48:58.572143: step 17020, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.997 sec/batch; 87h:20m:03s remains)
INFO - root - 2017-12-07 13:49:08.379051: step 17030, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 86h:14m:25s remains)
INFO - root - 2017-12-07 13:49:18.145777: step 17040, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 84h:39m:13s remains)
INFO - root - 2017-12-07 13:49:27.940864: step 17050, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 87h:23m:49s remains)
INFO - root - 2017-12-07 13:49:37.283569: step 17060, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 84h:16m:28s remains)
INFO - root - 2017-12-07 13:49:46.980240: step 17070, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.039 sec/batch; 91h:01m:30s remains)
INFO - root - 2017-12-07 13:49:56.611457: step 17080, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 85h:16m:32s remains)
INFO - root - 2017-12-07 13:50:06.524690: step 17090, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.030 sec/batch; 90h:16m:51s remains)
INFO - root - 2017-12-07 13:50:16.145312: step 17100, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 84h:55m:23s remains)
2017-12-07 13:50:17.015420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2338476 -4.2250814 -4.2146382 -4.20384 -4.2045197 -4.2071185 -4.2030339 -4.1954842 -4.1844893 -4.1768112 -4.17701 -4.1789184 -4.1771507 -4.1740794 -4.1778841][-4.1972294 -4.1925011 -4.187448 -4.1810246 -4.1867623 -4.1932139 -4.1925383 -4.1841278 -4.1668935 -4.1528125 -4.15269 -4.1583872 -4.1568031 -4.1476274 -4.1441774][-4.1831779 -4.182086 -4.178555 -4.1725087 -4.177186 -4.1815243 -4.1822047 -4.1766391 -4.1596255 -4.1439352 -4.1409135 -4.1480427 -4.1484985 -4.1380372 -4.1323957][-4.1888809 -4.1896091 -4.1838098 -4.1732607 -4.1699781 -4.166007 -4.1595211 -4.1495361 -4.1325321 -4.1194148 -4.1206851 -4.1291351 -4.1323261 -4.12451 -4.1215553][-4.2096028 -4.2096915 -4.2012634 -4.1836228 -4.1691742 -4.1554103 -4.1364388 -4.1138334 -4.0947175 -4.0896387 -4.1000738 -4.1091971 -4.1069875 -4.0953107 -4.0912585][-4.2219148 -4.2263184 -4.2195563 -4.1973276 -4.1724663 -4.1471796 -4.1156259 -4.0850935 -4.0753331 -4.0920076 -4.1180964 -4.1257296 -4.1119623 -4.0882211 -4.075233][-4.220427 -4.2292147 -4.2232766 -4.1920485 -4.1491394 -4.1021485 -4.0514321 -4.013824 -4.0248384 -4.083683 -4.1429658 -4.1674047 -4.1600451 -4.1371951 -4.1201024][-4.21552 -4.2206488 -4.2116137 -4.1727667 -4.117116 -4.0518756 -3.9827983 -3.9355657 -3.9567416 -4.0456357 -4.1314254 -4.1750307 -4.183639 -4.1742868 -4.1676545][-4.2243657 -4.2236266 -4.2119217 -4.172977 -4.1164289 -4.0492611 -3.9858704 -3.9522731 -3.9811027 -4.0652761 -4.1420455 -4.1818814 -4.1923189 -4.190834 -4.1944752][-4.2482758 -4.246944 -4.2364054 -4.2010579 -4.149519 -4.0914149 -4.0450039 -4.034287 -4.0702033 -4.1368575 -4.1899638 -4.214252 -4.2157445 -4.211421 -4.2153425][-4.2782145 -4.2768207 -4.2660036 -4.2366276 -4.1949582 -4.1523147 -4.1218691 -4.1261845 -4.1615796 -4.2091126 -4.2392964 -4.25005 -4.2456264 -4.2400513 -4.2421603][-4.3023725 -4.3031693 -4.2931929 -4.2707176 -4.2420592 -4.2173386 -4.2031603 -4.212152 -4.2385626 -4.2668562 -4.2773094 -4.2730951 -4.2587867 -4.2485981 -4.2451272][-4.3059325 -4.3106508 -4.3043261 -4.2890825 -4.2733836 -4.2659736 -4.2663484 -4.2748275 -4.2860265 -4.2932925 -4.2852888 -4.2655025 -4.2388906 -4.2204771 -4.2100596][-4.2980156 -4.3040075 -4.298028 -4.2862267 -4.27758 -4.2811947 -4.2891841 -4.2966895 -4.2980027 -4.2917976 -4.2727046 -4.2458391 -4.2146544 -4.19348 -4.1834373][-4.2889295 -4.2941866 -4.2845573 -4.2704968 -4.261663 -4.265028 -4.2736964 -4.2813396 -4.2792006 -4.268364 -4.2470155 -4.2205906 -4.1906018 -4.1709485 -4.1637011]]...]
INFO - root - 2017-12-07 13:50:26.594487: step 17110, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 84h:10m:19s remains)
INFO - root - 2017-12-07 13:50:36.223273: step 17120, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 80h:39m:22s remains)
INFO - root - 2017-12-07 13:50:45.831483: step 17130, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 87h:33m:45s remains)
INFO - root - 2017-12-07 13:50:55.257016: step 17140, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.955 sec/batch; 83h:37m:14s remains)
INFO - root - 2017-12-07 13:51:05.022170: step 17150, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 82h:29m:19s remains)
INFO - root - 2017-12-07 13:51:14.587443: step 17160, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 85h:58m:20s remains)
INFO - root - 2017-12-07 13:51:24.276148: step 17170, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.022 sec/batch; 89h:31m:15s remains)
INFO - root - 2017-12-07 13:51:33.858512: step 17180, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 81h:32m:29s remains)
INFO - root - 2017-12-07 13:51:43.459948: step 17190, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 85h:25m:49s remains)
INFO - root - 2017-12-07 13:51:52.990365: step 17200, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 81h:38m:03s remains)
2017-12-07 13:51:54.079029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25511 -4.2468457 -4.2558246 -4.2801557 -4.310173 -4.3322644 -4.3425765 -4.3356113 -4.3143225 -4.2877131 -4.2632909 -4.2475367 -4.2406373 -4.2401929 -4.2457848][-4.2817087 -4.264286 -4.2654147 -4.2856483 -4.3168535 -4.3435154 -4.3584518 -4.3532963 -4.3348174 -4.3127766 -4.2940125 -4.2855563 -4.2812777 -4.2793565 -4.2834277][-4.2906795 -4.2680426 -4.2638388 -4.2822595 -4.3140373 -4.3461771 -4.3663135 -4.3629007 -4.3454909 -4.3262658 -4.3131256 -4.3120861 -4.3124166 -4.3104095 -4.3117118][-4.287848 -4.2570729 -4.2460842 -4.26352 -4.2964864 -4.3315177 -4.3583164 -4.358376 -4.3409581 -4.3242817 -4.3180594 -4.3247619 -4.3322868 -4.3344216 -4.3338885][-4.2606611 -4.2152085 -4.1942172 -4.21108 -4.2515907 -4.2910528 -4.322382 -4.3287592 -4.3119655 -4.2961979 -4.2959862 -4.309526 -4.3251562 -4.3355865 -4.3364773][-4.2181411 -4.1537185 -4.1138506 -4.1246586 -4.1765537 -4.2255607 -4.2613 -4.2744236 -4.2601843 -4.2453532 -4.2531729 -4.2767568 -4.3005748 -4.3178797 -4.321517][-4.1824632 -4.1030335 -4.0402212 -4.0380583 -4.0996628 -4.1630125 -4.2061143 -4.2233372 -4.2136097 -4.1994009 -4.20993 -4.2384825 -4.2676721 -4.2916307 -4.2973862][-4.1793542 -4.1043744 -4.032969 -4.0154843 -4.0709338 -4.1373634 -4.1829476 -4.2001619 -4.1929221 -4.1770525 -4.1825385 -4.2070837 -4.2338362 -4.2568979 -4.2644835][-4.2032127 -4.1485753 -4.0891542 -4.0643535 -4.0950356 -4.1432447 -4.1780524 -4.189878 -4.1827178 -4.1697822 -4.1729574 -4.1931458 -4.217207 -4.2360282 -4.2413545][-4.2324648 -4.2043123 -4.1670041 -4.1402097 -4.1436768 -4.1646652 -4.1828675 -4.1871924 -4.1819453 -4.1761031 -4.182817 -4.2017617 -4.2238855 -4.2369046 -4.2349682][-4.2567019 -4.2483711 -4.2281833 -4.1979356 -4.1809196 -4.1795597 -4.1843243 -4.1860533 -4.18807 -4.1942539 -4.2088566 -4.2257705 -4.2429295 -4.25273 -4.2464833][-4.2720575 -4.2762361 -4.2678695 -4.2411904 -4.2165208 -4.20128 -4.1960254 -4.1949196 -4.2033019 -4.2205958 -4.2436996 -4.2585316 -4.2686462 -4.2741179 -4.2647285][-4.2781219 -4.2929029 -4.29721 -4.280437 -4.2557063 -4.2339077 -4.2202024 -4.2167139 -4.2232289 -4.2394867 -4.2640953 -4.2788725 -4.2864819 -4.2893362 -4.278039][-4.2820435 -4.3005004 -4.3127851 -4.304688 -4.2854853 -4.2645621 -4.2471642 -4.2403069 -4.2410841 -4.2495308 -4.26953 -4.2852068 -4.2953954 -4.2999048 -4.2912741][-4.2940784 -4.3084569 -4.3197632 -4.3162365 -4.3028464 -4.285965 -4.2711697 -4.2617702 -4.2577028 -4.2607212 -4.2760186 -4.290585 -4.3003817 -4.305 -4.3008289]]...]
INFO - root - 2017-12-07 13:52:03.590090: step 17210, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 79h:34m:59s remains)
INFO - root - 2017-12-07 13:52:13.286106: step 17220, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 82h:38m:56s remains)
INFO - root - 2017-12-07 13:52:23.119738: step 17230, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 85h:18m:04s remains)
INFO - root - 2017-12-07 13:52:32.561455: step 17240, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.994 sec/batch; 87h:00m:58s remains)
INFO - root - 2017-12-07 13:52:42.295402: step 17250, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 84h:43m:57s remains)
INFO - root - 2017-12-07 13:52:51.895706: step 17260, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 85h:46m:37s remains)
INFO - root - 2017-12-07 13:53:01.486404: step 17270, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 81h:30m:51s remains)
INFO - root - 2017-12-07 13:53:11.094470: step 17280, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 81h:23m:43s remains)
INFO - root - 2017-12-07 13:53:20.636338: step 17290, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 84h:23m:37s remains)
INFO - root - 2017-12-07 13:53:30.433003: step 17300, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 86h:26m:31s remains)
2017-12-07 13:53:31.330120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2697496 -4.2706289 -4.2698922 -4.2615066 -4.2506752 -4.25314 -4.2656198 -4.2751884 -4.2799158 -4.2877483 -4.2926674 -4.2898068 -4.2891517 -4.29286 -4.2896767][-4.2859073 -4.2807622 -4.2751 -4.2650533 -4.2534761 -4.2555428 -4.2692428 -4.2801514 -4.2808924 -4.2810764 -4.2799897 -4.2707229 -4.2624135 -4.259635 -4.2515821][-4.2938151 -4.2868648 -4.2782769 -4.2654061 -4.2508574 -4.2471547 -4.2569914 -4.2667503 -4.2692647 -4.2674155 -4.2628922 -4.2488275 -4.2344275 -4.2244024 -4.2121816][-4.2910714 -4.2841673 -4.2720723 -4.2562079 -4.2382073 -4.228405 -4.2334814 -4.243175 -4.2504282 -4.2523985 -4.2486854 -4.2335362 -4.2171144 -4.2025995 -4.1875362][-4.2749281 -4.2671952 -4.2543483 -4.2403 -4.225316 -4.2143793 -4.2153473 -4.2258286 -4.2350922 -4.2385106 -4.2348552 -4.2182026 -4.2010179 -4.1848884 -4.168715][-4.2448649 -4.233129 -4.2159176 -4.20286 -4.1940269 -4.1863041 -4.185782 -4.1952839 -4.2075458 -4.2143826 -4.2155371 -4.206315 -4.194181 -4.1813169 -4.1681523][-4.2185049 -4.1964827 -4.1670661 -4.1429195 -4.1312943 -4.1241918 -4.1215677 -4.1320324 -4.148891 -4.1616268 -4.1719027 -4.1768708 -4.1780577 -4.1764355 -4.1723814][-4.2087789 -4.175076 -4.1307731 -4.0888934 -4.0643044 -4.0525875 -4.0466628 -4.0537858 -4.074307 -4.0979557 -4.1247268 -4.1487851 -4.1703677 -4.1858339 -4.1943903][-4.205863 -4.1650691 -4.1112938 -4.0557494 -4.0206947 -4.0020185 -3.9925432 -3.9935858 -4.0138845 -4.0523748 -4.1011429 -4.1436863 -4.1831951 -4.2143855 -4.2359042][-4.2092047 -4.1719027 -4.1243658 -4.0701208 -4.0322614 -4.011632 -4.0051613 -4.0031362 -4.0135407 -4.0504503 -4.1041203 -4.1519103 -4.1974344 -4.2350698 -4.2619982][-4.2199717 -4.1943913 -4.16316 -4.1251855 -4.0976148 -4.0832911 -4.0812569 -4.0809903 -4.0837035 -4.1032491 -4.1422434 -4.1803007 -4.2181015 -4.249671 -4.273078][-4.2327027 -4.2185183 -4.1984634 -4.175087 -4.1629686 -4.1561275 -4.1585603 -4.165062 -4.1664896 -4.1704512 -4.1865158 -4.2052355 -4.22591 -4.2438612 -4.25762][-4.2252531 -4.2210026 -4.2130914 -4.2044187 -4.2036834 -4.2001095 -4.2059655 -4.2207479 -4.2255478 -4.22161 -4.2220964 -4.2225771 -4.2266183 -4.2332697 -4.2375927][-4.1871128 -4.1945963 -4.2012224 -4.2030458 -4.2084742 -4.2134342 -4.22436 -4.243948 -4.2513742 -4.2479119 -4.2428527 -4.23344 -4.2251587 -4.2211447 -4.2158055][-4.1414843 -4.1597342 -4.1776314 -4.1852689 -4.1934271 -4.2008348 -4.2126389 -4.2339811 -4.242775 -4.2403135 -4.2352395 -4.2255459 -4.2124963 -4.2016807 -4.1896038]]...]
INFO - root - 2017-12-07 13:53:40.843776: step 17310, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 82h:19m:11s remains)
INFO - root - 2017-12-07 13:53:50.331757: step 17320, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 83h:07m:41s remains)
INFO - root - 2017-12-07 13:54:00.049888: step 17330, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 84h:48m:57s remains)
INFO - root - 2017-12-07 13:54:09.654849: step 17340, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 85h:35m:50s remains)
INFO - root - 2017-12-07 13:54:19.230913: step 17350, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 78h:26m:24s remains)
INFO - root - 2017-12-07 13:54:28.942139: step 17360, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 84h:19m:24s remains)
INFO - root - 2017-12-07 13:54:38.674879: step 17370, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.018 sec/batch; 89h:04m:20s remains)
INFO - root - 2017-12-07 13:54:48.377050: step 17380, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 84h:58m:46s remains)
INFO - root - 2017-12-07 13:54:58.138134: step 17390, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 86h:31m:33s remains)
INFO - root - 2017-12-07 13:55:07.815412: step 17400, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.035 sec/batch; 90h:33m:11s remains)
2017-12-07 13:55:08.747176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2094083 -4.2053552 -4.1996827 -4.1918497 -4.190763 -4.2032852 -4.2285633 -4.2499 -4.260488 -4.2748275 -4.2861137 -4.2873716 -4.2856731 -4.2846789 -4.29008][-4.2340097 -4.2259083 -4.2217021 -4.2164855 -4.2160149 -4.2244153 -4.242332 -4.2620316 -4.276022 -4.287004 -4.2940207 -4.2954254 -4.296401 -4.2968941 -4.3036065][-4.2809443 -4.2705307 -4.2636123 -4.2582269 -4.2546139 -4.2531915 -4.2583318 -4.2682467 -4.277389 -4.2861624 -4.2918897 -4.2930408 -4.2924228 -4.2929015 -4.3012705][-4.328084 -4.3128614 -4.3002629 -4.2942381 -4.2855372 -4.2665482 -4.2496 -4.2428188 -4.2490611 -4.2631025 -4.27785 -4.2830863 -4.2777863 -4.2723 -4.2747808][-4.3423576 -4.321681 -4.3052964 -4.2988615 -4.2803941 -4.2401109 -4.1965122 -4.1664138 -4.175746 -4.2166672 -4.2545338 -4.2655516 -4.2552052 -4.2397938 -4.231348][-4.3212595 -4.2959132 -4.27598 -4.2642117 -4.2294359 -4.1623559 -4.0764756 -4.0128379 -4.0389566 -4.1266446 -4.1993904 -4.2215905 -4.2117419 -4.1882963 -4.1714354][-4.2848511 -4.2567592 -4.2334223 -4.2102695 -4.1554041 -4.0565481 -3.9185324 -3.8151684 -3.8845103 -4.037466 -4.1380072 -4.164351 -4.1517878 -4.1182046 -4.1001005][-4.249898 -4.2245879 -4.2000961 -4.1630068 -4.0903435 -3.9743531 -3.8064311 -3.6751904 -3.7954772 -3.9948761 -4.1082625 -4.1320119 -4.1094718 -4.0713844 -4.0669651][-4.2449865 -4.2322726 -4.2117734 -4.1702747 -4.1026411 -4.0050545 -3.8727541 -3.7864528 -3.8992894 -4.0602708 -4.141593 -4.1471405 -4.1099644 -4.0760641 -4.0924678][-4.2591634 -4.2535348 -4.2414527 -4.2143908 -4.1694779 -4.1052756 -4.0309629 -3.9930389 -4.0682731 -4.1624255 -4.20546 -4.1910915 -4.1416612 -4.1151032 -4.1427269][-4.2678409 -4.2653522 -4.2585363 -4.24899 -4.2300229 -4.1970692 -4.165606 -4.156024 -4.1951027 -4.2377677 -4.256321 -4.231482 -4.1836238 -4.16303 -4.1859283][-4.2644243 -4.2591558 -4.255959 -4.2587824 -4.2606273 -4.2526555 -4.245894 -4.2483764 -4.26485 -4.2799549 -4.2808237 -4.2556391 -4.2198515 -4.2075157 -4.2218313][-4.2537537 -4.2489042 -4.2495403 -4.2565861 -4.2666841 -4.2746053 -4.281467 -4.2893767 -4.2954168 -4.2990704 -4.2948136 -4.2774572 -4.2584672 -4.252955 -4.2597065][-4.245616 -4.2434931 -4.247088 -4.2550211 -4.2664165 -4.277916 -4.2922344 -4.3063359 -4.3118496 -4.3113232 -4.3081946 -4.2984095 -4.2902222 -4.2893095 -4.2918682][-4.2606411 -4.2605715 -4.2637453 -4.2689648 -4.2765775 -4.2871194 -4.3030152 -4.3186 -4.3219962 -4.3182187 -4.3176918 -4.3169041 -4.3161159 -4.3171062 -4.3153849]]...]
INFO - root - 2017-12-07 13:55:18.260521: step 17410, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 79h:13m:44s remains)
INFO - root - 2017-12-07 13:55:27.907661: step 17420, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 85h:39m:28s remains)
INFO - root - 2017-12-07 13:55:37.352739: step 17430, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 81h:56m:55s remains)
INFO - root - 2017-12-07 13:55:46.981544: step 17440, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 86h:50m:00s remains)
INFO - root - 2017-12-07 13:55:56.772857: step 17450, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 89h:00m:21s remains)
INFO - root - 2017-12-07 13:56:06.525623: step 17460, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 86h:22m:39s remains)
INFO - root - 2017-12-07 13:56:16.177178: step 17470, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 86h:02m:03s remains)
INFO - root - 2017-12-07 13:56:25.840729: step 17480, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 84h:39m:39s remains)
INFO - root - 2017-12-07 13:56:35.593306: step 17490, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 87h:51m:42s remains)
INFO - root - 2017-12-07 13:56:45.364760: step 17500, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 82h:59m:28s remains)
2017-12-07 13:56:46.367785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2205391 -4.22656 -4.2351689 -4.2423878 -4.2502179 -4.2545223 -4.2520571 -4.2535992 -4.2591214 -4.2635446 -4.2646489 -4.2636971 -4.2588539 -4.2496309 -4.2395353][-4.1764779 -4.1907549 -4.2089267 -4.2214603 -4.2315063 -4.2283916 -4.2177672 -4.2117028 -4.2133055 -4.2214174 -4.2296953 -4.2270503 -4.2190604 -4.2076316 -4.2014723][-4.1342678 -4.1593165 -4.1895275 -4.2121086 -4.2216926 -4.2037964 -4.1734028 -4.1505675 -4.1482425 -4.1668754 -4.1861382 -4.184536 -4.1701088 -4.1543589 -4.1497602][-4.0882783 -4.1232343 -4.1715736 -4.205081 -4.206563 -4.1688213 -4.113162 -4.0633731 -4.0579753 -4.0998368 -4.1392426 -4.1433234 -4.1211448 -4.0969782 -4.0870528][-4.061357 -4.0923753 -4.1513538 -4.1931257 -4.1862712 -4.1206913 -4.0228376 -3.9314394 -3.934504 -4.0263739 -4.1016297 -4.1222315 -4.0974369 -4.0624633 -4.0445237][-4.065866 -4.0893712 -4.1434107 -4.1804895 -4.1635532 -4.0652876 -3.9024513 -3.7469854 -3.777946 -3.9445977 -4.070045 -4.1183419 -4.1026092 -4.0561 -4.0257726][-4.103054 -4.1217766 -4.1658592 -4.1872535 -4.153645 -4.0188632 -3.7836137 -3.563262 -3.6501627 -3.8904197 -4.0554481 -4.1256056 -4.12218 -4.0676646 -4.0257897][-4.1446567 -4.1649427 -4.1980352 -4.2029567 -4.1567788 -4.0062032 -3.7449276 -3.5152981 -3.6462586 -3.8991339 -4.0634322 -4.1329789 -4.1309066 -4.0743065 -4.0342627][-4.172647 -4.1930757 -4.2171063 -4.2140269 -4.16954 -4.0423412 -3.8396902 -3.6956534 -3.8044975 -3.9876962 -4.1095181 -4.1590118 -4.142283 -4.0842605 -4.0496888][-4.1770577 -4.1987634 -4.2229261 -4.2171588 -4.1788116 -4.0912151 -3.9705229 -3.9060612 -3.9768639 -4.0853453 -4.1648431 -4.193532 -4.1609716 -4.1002254 -4.06726][-4.1604233 -4.184391 -4.2104087 -4.2021914 -4.1690307 -4.1174483 -4.0609775 -4.0454388 -4.0941858 -4.1562157 -4.2058773 -4.2150016 -4.1731539 -4.1108532 -4.0795808][-4.1337957 -4.1683578 -4.192822 -4.17986 -4.1520095 -4.1300831 -4.118609 -4.1339755 -4.1710939 -4.2020893 -4.224905 -4.217803 -4.1748466 -4.1173716 -4.0892696][-4.1167531 -4.1606727 -4.1828642 -4.16875 -4.1460295 -4.139122 -4.1552162 -4.1897435 -4.2169456 -4.2250328 -4.2249475 -4.2127314 -4.1791368 -4.130672 -4.1075416][-4.1388884 -4.1880679 -4.2071671 -4.1933479 -4.1747527 -4.1777282 -4.2038965 -4.2383757 -4.2532158 -4.2448039 -4.2304587 -4.2189074 -4.193912 -4.1567969 -4.1447492][-4.1899686 -4.2318239 -4.2537436 -4.2472968 -4.237318 -4.2482376 -4.2713032 -4.2931395 -4.2946987 -4.2756162 -4.2563281 -4.2453814 -4.2313843 -4.2098761 -4.2076726]]...]
INFO - root - 2017-12-07 13:56:56.075759: step 17510, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 84h:45m:33s remains)
INFO - root - 2017-12-07 13:57:05.794389: step 17520, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 82h:58m:43s remains)
INFO - root - 2017-12-07 13:57:15.208172: step 17530, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 80h:54m:48s remains)
INFO - root - 2017-12-07 13:57:24.868843: step 17540, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 84h:51m:06s remains)
INFO - root - 2017-12-07 13:57:34.688049: step 17550, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 81h:54m:00s remains)
INFO - root - 2017-12-07 13:57:44.405282: step 17560, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 87h:31m:41s remains)
INFO - root - 2017-12-07 13:57:54.173154: step 17570, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 81h:24m:54s remains)
INFO - root - 2017-12-07 13:58:03.875151: step 17580, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 83h:50m:12s remains)
INFO - root - 2017-12-07 13:58:13.473308: step 17590, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 88h:15m:34s remains)
INFO - root - 2017-12-07 13:58:23.233691: step 17600, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.981 sec/batch; 85h:51m:02s remains)
2017-12-07 13:58:24.288119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2216606 -4.2350073 -4.2495952 -4.2590003 -4.2645388 -4.2680073 -4.2714105 -4.2722626 -4.2744184 -4.2771678 -4.2795148 -4.2808657 -4.2800813 -4.2802043 -4.2807994][-4.2287827 -4.2437954 -4.2606559 -4.2700591 -4.2733231 -4.274725 -4.2764306 -4.2769666 -4.2810221 -4.2844415 -4.2849441 -4.2835941 -4.2793126 -4.27524 -4.2741866][-4.2509885 -4.2630558 -4.2752352 -4.2798786 -4.2786021 -4.2747936 -4.2718534 -4.2715268 -4.2768483 -4.2829161 -4.2839503 -4.2804785 -4.272799 -4.2661376 -4.2659988][-4.2880173 -4.2926121 -4.2941294 -4.288713 -4.2810507 -4.2704892 -4.257411 -4.2496619 -4.2539759 -4.2645984 -4.2707644 -4.2674012 -4.2566113 -4.2492108 -4.2503772][-4.30067 -4.2962518 -4.287127 -4.2719588 -4.2574568 -4.2387867 -4.2093282 -4.1867914 -4.1903496 -4.2114177 -4.229394 -4.2294283 -4.219316 -4.2130432 -4.2166696][-4.2637572 -4.2451291 -4.2212639 -4.1943421 -4.1671295 -4.1323938 -4.0794716 -4.0402775 -4.0554566 -4.103816 -4.1471767 -4.1587639 -4.1510348 -4.1439219 -4.1464958][-4.1935711 -4.1618938 -4.1253176 -4.0829148 -4.0321035 -3.9710643 -3.8911729 -3.8353791 -3.8736506 -3.9578841 -4.0303793 -4.0565486 -4.0552626 -4.053062 -4.0618739][-4.1316867 -4.0965381 -4.0579619 -4.0158334 -3.9599724 -3.8946474 -3.8159075 -3.7638903 -3.8118758 -3.8970761 -3.967165 -3.995651 -4.0002484 -4.0047812 -4.0253673][-4.1242085 -4.100668 -4.0778809 -4.0561066 -4.0238814 -3.9886179 -3.9474707 -3.9210927 -3.9510775 -4.0000439 -4.038691 -4.0526123 -4.0506516 -4.0547113 -4.0724282][-4.166173 -4.158124 -4.1527777 -4.1491461 -4.1395435 -4.1287632 -4.1127906 -4.1002874 -4.1135259 -4.1352878 -4.1518178 -4.1568632 -4.1518831 -4.1531029 -4.160501][-4.2280068 -4.2313094 -4.2354774 -4.2394896 -4.2397089 -4.2410469 -4.2381077 -4.2322893 -4.2355232 -4.2439375 -4.2517414 -4.2536983 -4.2503672 -4.250494 -4.25123][-4.2732959 -4.2769094 -4.2819963 -4.2880144 -4.2912431 -4.2948346 -4.2959709 -4.2935677 -4.2937527 -4.2958865 -4.2994475 -4.3008533 -4.2981386 -4.2959013 -4.2918792][-4.2963357 -4.2986083 -4.3029733 -4.3080325 -4.3104992 -4.3131571 -4.3144321 -4.3122191 -4.3112912 -4.3120866 -4.3136582 -4.3140221 -4.3116179 -4.3094382 -4.3060236][-4.2995648 -4.3019609 -4.3046541 -4.3069248 -4.3072104 -4.3079486 -4.3085575 -4.3078766 -4.3078055 -4.3084989 -4.3094034 -4.3100634 -4.3093967 -4.3087406 -4.3078265][-4.2662196 -4.267776 -4.26849 -4.2683954 -4.26705 -4.2665129 -4.2669516 -4.2672133 -4.2682071 -4.2696791 -4.2706776 -4.2712774 -4.271471 -4.2716861 -4.2726917]]...]
INFO - root - 2017-12-07 13:58:33.874188: step 17610, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.929 sec/batch; 81h:18m:02s remains)
INFO - root - 2017-12-07 13:58:43.462813: step 17620, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 81h:56m:05s remains)
INFO - root - 2017-12-07 13:58:53.283288: step 17630, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 84h:06m:21s remains)
INFO - root - 2017-12-07 13:59:03.043880: step 17640, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 84h:23m:00s remains)
INFO - root - 2017-12-07 13:59:12.626111: step 17650, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 83h:17m:06s remains)
INFO - root - 2017-12-07 13:59:22.336002: step 17660, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 86h:33m:15s remains)
INFO - root - 2017-12-07 13:59:32.052770: step 17670, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 84h:53m:23s remains)
INFO - root - 2017-12-07 13:59:41.677175: step 17680, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 83h:11m:58s remains)
INFO - root - 2017-12-07 13:59:51.144378: step 17690, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 76h:47m:59s remains)
INFO - root - 2017-12-07 14:00:00.783134: step 17700, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.962 sec/batch; 84h:05m:37s remains)
2017-12-07 14:00:01.809320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2538776 -4.258513 -4.2550793 -4.2405791 -4.2294741 -4.2276959 -4.2375875 -4.2591372 -4.2703257 -4.2583408 -4.2422781 -4.2441978 -4.2663956 -4.277709 -4.2692738][-4.25212 -4.2523227 -4.2459207 -4.228334 -4.2139578 -4.21326 -4.2263145 -4.2448707 -4.2446146 -4.2277794 -4.2198186 -4.2295651 -4.2603693 -4.2790427 -4.2738252][-4.2514534 -4.244556 -4.2350025 -4.2203741 -4.210701 -4.2136436 -4.2239804 -4.2319713 -4.2227974 -4.21289 -4.21799 -4.2368317 -4.2690454 -4.2902513 -4.2870684][-4.2428608 -4.2323666 -4.2200136 -4.2065263 -4.2027783 -4.2085195 -4.214406 -4.2127733 -4.2036719 -4.2065 -4.2260261 -4.2497931 -4.2760415 -4.2953825 -4.2917981][-4.238863 -4.2282643 -4.2112436 -4.1922092 -4.1897326 -4.1973972 -4.1986547 -4.19244 -4.190011 -4.2056012 -4.2365332 -4.2623119 -4.2791276 -4.2856803 -4.277524][-4.2397695 -4.2275271 -4.2050643 -4.1817408 -4.1764708 -4.1867142 -4.1903329 -4.1860776 -4.189219 -4.209559 -4.236383 -4.2552972 -4.2676 -4.2708454 -4.2625594][-4.237339 -4.2189732 -4.1930771 -4.1700473 -4.16439 -4.174706 -4.1827774 -4.1831717 -4.1924062 -4.20999 -4.2226605 -4.22981 -4.2464561 -4.2638459 -4.2648382][-4.2139587 -4.1932912 -4.1730638 -4.1569648 -4.1518459 -4.1589012 -4.1693764 -4.1774163 -4.1919837 -4.2047105 -4.2087669 -4.2096248 -4.2322197 -4.2564726 -4.2607565][-4.1869631 -4.1678791 -4.1617393 -4.15449 -4.1467957 -4.1504397 -4.162055 -4.1756082 -4.1938267 -4.2041683 -4.2109046 -4.215116 -4.2391558 -4.2613349 -4.2637167][-4.1771593 -4.1608129 -4.1660933 -4.1636019 -4.1558042 -4.1576395 -4.1627388 -4.1752038 -4.1954894 -4.207211 -4.217926 -4.2260132 -4.2453384 -4.2618022 -4.2628703][-4.1873336 -4.1750474 -4.1820865 -4.1799746 -4.17342 -4.1726265 -4.1713667 -4.1787987 -4.1995125 -4.2138405 -4.2244105 -4.2292652 -4.2390409 -4.2516131 -4.2551265][-4.1956077 -4.1949983 -4.2036161 -4.1983051 -4.1917195 -4.1907787 -4.1861353 -4.1879916 -4.2071438 -4.2229371 -4.2287903 -4.2262425 -4.2270608 -4.235889 -4.2358809][-4.19059 -4.2050409 -4.2153449 -4.2088017 -4.2036977 -4.2010913 -4.1961126 -4.1951857 -4.2086558 -4.2223706 -4.2237315 -4.2129097 -4.2012596 -4.2017131 -4.19891][-4.1650162 -4.191072 -4.2057281 -4.2041259 -4.2047262 -4.2054977 -4.2017465 -4.1981459 -4.199923 -4.20535 -4.2006969 -4.1861792 -4.1701469 -4.16318 -4.1581054][-4.1420031 -4.1719775 -4.187448 -4.1881857 -4.1960444 -4.2023435 -4.2000523 -4.1932373 -4.183176 -4.1764593 -4.1695852 -4.1601844 -4.1500177 -4.1401215 -4.1334605]]...]
INFO - root - 2017-12-07 14:00:11.485633: step 17710, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 84h:04m:36s remains)
INFO - root - 2017-12-07 14:00:21.241309: step 17720, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 84h:52m:09s remains)
INFO - root - 2017-12-07 14:00:30.819742: step 17730, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 86h:55m:16s remains)
INFO - root - 2017-12-07 14:00:40.419279: step 17740, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 86h:08m:02s remains)
INFO - root - 2017-12-07 14:00:49.966293: step 17750, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.007 sec/batch; 88h:01m:29s remains)
INFO - root - 2017-12-07 14:00:59.666940: step 17760, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 78h:11m:25s remains)
INFO - root - 2017-12-07 14:01:09.394344: step 17770, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 80h:52m:56s remains)
INFO - root - 2017-12-07 14:01:18.988762: step 17780, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 86h:14m:49s remains)
INFO - root - 2017-12-07 14:01:28.497682: step 17790, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 86h:16m:09s remains)
INFO - root - 2017-12-07 14:01:38.186071: step 17800, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 84h:35m:04s remains)
2017-12-07 14:01:39.184191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2948427 -4.2669353 -4.2417736 -4.2284021 -4.2283959 -4.2405334 -4.2567868 -4.271471 -4.2820435 -4.2896748 -4.2972617 -4.3004789 -4.29468 -4.2839651 -4.2764044][-4.2853765 -4.2544265 -4.2299056 -4.2221975 -4.2287612 -4.240881 -4.2530313 -4.2628174 -4.2704544 -4.276845 -4.2810669 -4.2776713 -4.2656083 -4.2535658 -4.2504497][-4.2827539 -4.2548637 -4.2369528 -4.2366142 -4.2429175 -4.2468939 -4.2484527 -4.2487688 -4.2500544 -4.2526484 -4.2519073 -4.2433615 -4.2313375 -4.2271695 -4.2345386][-4.2880397 -4.2662282 -4.256289 -4.2588787 -4.2591472 -4.2494063 -4.2340851 -4.2216072 -4.2153659 -4.2173305 -4.2175179 -4.2107825 -4.208118 -4.2189808 -4.2376018][-4.2985182 -4.2843103 -4.2790241 -4.27803 -4.2666316 -4.2385116 -4.2025619 -4.1750221 -4.166995 -4.1786685 -4.1892838 -4.1942286 -4.2067552 -4.2301884 -4.2531524][-4.3068705 -4.2966342 -4.2905464 -4.2824945 -4.2581058 -4.2094007 -4.150208 -4.1103549 -4.1168718 -4.1535721 -4.1808672 -4.1988583 -4.2223625 -4.2491546 -4.2677741][-4.3076305 -4.2971082 -4.2856679 -4.2677417 -4.2279453 -4.1603332 -4.084167 -4.0491047 -4.0900517 -4.1536083 -4.1898928 -4.2107062 -4.2330871 -4.2554812 -4.2669821][-4.30159 -4.2872853 -4.2689433 -4.2409768 -4.1897173 -4.1156044 -4.0459056 -4.0393953 -4.1089711 -4.1766763 -4.2062187 -4.2187319 -4.2323966 -4.2501559 -4.26003][-4.3007855 -4.2826343 -4.2568426 -4.2206578 -4.1673884 -4.1062822 -4.06968 -4.0936122 -4.1568146 -4.2035217 -4.2167859 -4.2167282 -4.2212744 -4.2375464 -4.2522526][-4.2990947 -4.2761226 -4.2425618 -4.2029586 -4.15849 -4.123095 -4.1197906 -4.1535921 -4.1958771 -4.2178779 -4.2166457 -4.2063413 -4.2038655 -4.22001 -4.2425113][-4.2893138 -4.2598605 -4.22161 -4.1837888 -4.1507998 -4.1374197 -4.1536179 -4.1866412 -4.2124224 -4.2173648 -4.2065029 -4.1906972 -4.1841955 -4.2005038 -4.2279367][-4.2677693 -4.2349439 -4.1977925 -4.1642728 -4.1390543 -4.1365075 -4.1599178 -4.1907411 -4.2104235 -4.2081666 -4.1921668 -4.1742444 -4.1673436 -4.1817374 -4.2075253][-4.2421107 -4.2106237 -4.179841 -4.1507049 -4.1276126 -4.1266823 -4.1494522 -4.1784649 -4.1965957 -4.1935992 -4.1775103 -4.1614432 -4.155457 -4.1655149 -4.1841722][-4.2258363 -4.1979427 -4.1720538 -4.1455688 -4.1249375 -4.1247559 -4.1442995 -4.1694961 -4.1862669 -4.1852584 -4.1731782 -4.1617241 -4.1565585 -4.161974 -4.1720295][-4.2285843 -4.2047434 -4.1826077 -4.1595845 -4.1425028 -4.142746 -4.1560969 -4.1759524 -4.1908264 -4.1914186 -4.1837854 -4.1766529 -4.172369 -4.1744719 -4.1783543]]...]
INFO - root - 2017-12-07 14:01:48.895551: step 17810, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 87h:57m:22s remains)
INFO - root - 2017-12-07 14:01:58.520948: step 17820, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 82h:32m:24s remains)
INFO - root - 2017-12-07 14:02:08.218535: step 17830, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 85h:05m:16s remains)
INFO - root - 2017-12-07 14:02:17.823239: step 17840, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 82h:17m:55s remains)
INFO - root - 2017-12-07 14:02:27.550723: step 17850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 82h:36m:04s remains)
INFO - root - 2017-12-07 14:02:37.023957: step 17860, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 82h:26m:30s remains)
INFO - root - 2017-12-07 14:02:46.679916: step 17870, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 82h:28m:58s remains)
INFO - root - 2017-12-07 14:02:56.419971: step 17880, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 83h:45m:20s remains)
INFO - root - 2017-12-07 14:03:05.928332: step 17890, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 86h:05m:35s remains)
INFO - root - 2017-12-07 14:03:15.748756: step 17900, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 86h:18m:34s remains)
2017-12-07 14:03:16.772309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21019 -4.2385135 -4.2591 -4.2587619 -4.2545204 -4.2537537 -4.2562871 -4.2599659 -4.2679944 -4.2690406 -4.262352 -4.2597914 -4.2547693 -4.2511749 -4.2565832][-4.2050257 -4.2445722 -4.2717743 -4.2722864 -4.2684827 -4.2629108 -4.2647972 -4.2654867 -4.2736797 -4.2795873 -4.2817221 -4.2870502 -4.2888336 -4.2903805 -4.3047051][-4.2131615 -4.2480912 -4.2709427 -4.2653909 -4.2539353 -4.2482944 -4.2516365 -4.2521863 -4.2597036 -4.2712274 -4.2820616 -4.2942333 -4.303143 -4.311276 -4.3297153][-4.2276793 -4.2438397 -4.2529025 -4.2407603 -4.2243776 -4.2148986 -4.21621 -4.2179646 -4.2257004 -4.2406006 -4.2585144 -4.2821975 -4.3025961 -4.3184643 -4.3403044][-4.2440867 -4.2433758 -4.2371483 -4.216176 -4.1927538 -4.1719427 -4.1560435 -4.145225 -4.150249 -4.1725984 -4.2018323 -4.2418938 -4.2796731 -4.3069444 -4.3311639][-4.2446971 -4.2382197 -4.2226925 -4.1926761 -4.1599 -4.118247 -4.0646682 -4.0254245 -4.0258784 -4.0657334 -4.1202941 -4.183136 -4.240726 -4.2799215 -4.3064628][-4.215868 -4.2099695 -4.1945615 -4.1634655 -4.1274195 -4.0711546 -3.9804697 -3.8974824 -3.8846543 -3.945086 -4.0284925 -4.1163573 -4.1929865 -4.2423058 -4.2696304][-4.1762705 -4.1699595 -4.1613235 -4.1398044 -4.1084776 -4.0545549 -3.9602249 -3.8582094 -3.8286655 -3.8918788 -3.9819374 -4.0742431 -4.1581736 -4.2109308 -4.2341738][-4.1542616 -4.1419764 -4.1406608 -4.133215 -4.1143122 -4.081697 -4.0206866 -3.9469297 -3.9164138 -3.9551501 -4.0174327 -4.0872488 -4.1581798 -4.2045484 -4.2211838][-4.1748166 -4.1608171 -4.1605663 -4.1596584 -4.1541553 -4.1469245 -4.1259704 -4.0900669 -4.0666037 -4.08226 -4.1114516 -4.1497512 -4.1961393 -4.2280488 -4.2372112][-4.2319169 -4.2226624 -4.2213621 -4.2213125 -4.221468 -4.2282138 -4.2299962 -4.2176352 -4.2011356 -4.202559 -4.2119951 -4.2269645 -4.2497249 -4.2669916 -4.2709913][-4.2935686 -4.2876887 -4.2856588 -4.2849026 -4.2849145 -4.2914429 -4.2987552 -4.2954521 -4.2829924 -4.27652 -4.2759552 -4.2783036 -4.2868009 -4.2945628 -4.2963915][-4.3252225 -4.3206391 -4.3183107 -4.3172956 -4.3161035 -4.3189688 -4.3249664 -4.325254 -4.3168831 -4.3093944 -4.3059082 -4.3034444 -4.30635 -4.3112988 -4.3137732][-4.3383155 -4.3353424 -4.3337049 -4.3323555 -4.330574 -4.3304687 -4.3336644 -4.3343573 -4.3308568 -4.3264236 -4.3246026 -4.3224406 -4.324141 -4.3287859 -4.3330197][-4.3437548 -4.3436146 -4.3436847 -4.3425846 -4.3407226 -4.3401117 -4.3409085 -4.3407936 -4.3393817 -4.3381705 -4.3387094 -4.3384528 -4.339427 -4.3419094 -4.3445792]]...]
INFO - root - 2017-12-07 14:03:26.343211: step 17910, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 84h:36m:25s remains)
INFO - root - 2017-12-07 14:03:36.028201: step 17920, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 84h:37m:20s remains)
INFO - root - 2017-12-07 14:03:45.693420: step 17930, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.994 sec/batch; 86h:48m:49s remains)
INFO - root - 2017-12-07 14:03:55.272505: step 17940, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 86h:44m:16s remains)
INFO - root - 2017-12-07 14:04:04.987576: step 17950, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 85h:06m:01s remains)
INFO - root - 2017-12-07 14:04:14.548043: step 17960, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 83h:50m:44s remains)
INFO - root - 2017-12-07 14:04:24.082886: step 17970, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 79h:42m:54s remains)
INFO - root - 2017-12-07 14:04:33.765236: step 17980, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 86h:57m:08s remains)
INFO - root - 2017-12-07 14:04:43.479378: step 17990, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 82h:55m:08s remains)
INFO - root - 2017-12-07 14:04:53.137727: step 18000, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 86h:03m:15s remains)
2017-12-07 14:04:54.108814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2864814 -4.2859011 -4.2789183 -4.2727375 -4.2681332 -4.2609239 -4.2529902 -4.2515221 -4.2555513 -4.2574811 -4.2563634 -4.2504907 -4.2487669 -4.26286 -4.2843146][-4.2599669 -4.260067 -4.2532058 -4.2473946 -4.2414246 -4.231266 -4.220376 -4.2196007 -4.2252712 -4.2239652 -4.2208962 -4.2153468 -4.2144017 -4.2342477 -4.2637515][-4.2390337 -4.2380462 -4.2256207 -4.2129569 -4.2073469 -4.1998682 -4.1863756 -4.1831336 -4.1870556 -4.1826081 -4.1774774 -4.1691031 -4.1654754 -4.1884565 -4.2258639][-4.2176194 -4.2114449 -4.1906557 -4.1714535 -4.1690674 -4.1666536 -4.1503534 -4.1404047 -4.1425786 -4.14027 -4.1356888 -4.1228237 -4.1147747 -4.1374989 -4.1819758][-4.2029967 -4.1905746 -4.1662521 -4.1396317 -4.1318007 -4.12676 -4.1024361 -4.0832205 -4.0822663 -4.0881009 -4.09428 -4.0832877 -4.0739431 -4.0991197 -4.1484213][-4.1794667 -4.165803 -4.1453924 -4.1178188 -4.1007447 -4.0868683 -4.04734 -4.0085058 -4.0045204 -4.0316567 -4.0618615 -4.0598793 -4.0567126 -4.0872507 -4.1384635][-4.1546235 -4.141933 -4.126966 -4.1047163 -4.0802703 -4.0550985 -3.9919124 -3.9142187 -3.9090977 -3.9761462 -4.039001 -4.0553603 -4.0623784 -4.0951529 -4.1429815][-4.1475458 -4.1334043 -4.1245914 -4.1116419 -4.0858116 -4.0512428 -3.9641228 -3.8423057 -3.83569 -3.9482749 -4.03948 -4.0713267 -4.0826454 -4.1084914 -4.1489339][-4.152873 -4.1428237 -4.1455212 -4.147141 -4.1273155 -4.0927544 -4.0105033 -3.8923354 -3.8834722 -3.9869719 -4.0706725 -4.1035295 -4.1108232 -4.1232452 -4.1533833][-4.1697884 -4.1656275 -4.1761956 -4.1909347 -4.1799793 -4.1525888 -4.0921817 -4.0119433 -4.00278 -4.0665112 -4.121233 -4.140173 -4.1388063 -4.1398964 -4.1589127][-4.1791644 -4.177424 -4.196218 -4.2167373 -4.2131038 -4.1909332 -4.1470647 -4.0895791 -4.08321 -4.1211925 -4.1525207 -4.1633787 -4.1583934 -4.1533394 -4.1632414][-4.1724606 -4.1686358 -4.1948447 -4.222415 -4.2267766 -4.2062764 -4.168366 -4.1239905 -4.1228728 -4.149282 -4.1682644 -4.1734447 -4.1685371 -4.1634 -4.1705384][-4.1720448 -4.1577177 -4.1801424 -4.2132087 -4.2232594 -4.2028866 -4.1696858 -4.1391382 -4.1431065 -4.1666527 -4.180162 -4.1804032 -4.1786432 -4.1792126 -4.1862159][-4.1788397 -4.156683 -4.1719928 -4.1984906 -4.2080321 -4.19134 -4.1663876 -4.1458354 -4.1502233 -4.1737757 -4.1899467 -4.1900911 -4.1879072 -4.1939154 -4.2016273][-4.2065434 -4.1854143 -4.1899586 -4.1979589 -4.2091026 -4.19611 -4.17443 -4.1619225 -4.1618009 -4.186451 -4.2091956 -4.2093797 -4.2061305 -4.2121234 -4.220994]]...]
INFO - root - 2017-12-07 14:05:03.833316: step 18010, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 82h:56m:38s remains)
INFO - root - 2017-12-07 14:05:13.627440: step 18020, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.927 sec/batch; 80h:57m:55s remains)
INFO - root - 2017-12-07 14:05:23.159535: step 18030, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 78h:59m:44s remains)
INFO - root - 2017-12-07 14:05:32.868627: step 18040, loss = 2.11, batch loss = 2.05 (7.8 examples/sec; 1.019 sec/batch; 89h:03m:04s remains)
INFO - root - 2017-12-07 14:05:42.523731: step 18050, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 81h:54m:46s remains)
INFO - root - 2017-12-07 14:05:52.223811: step 18060, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 86h:04m:09s remains)
INFO - root - 2017-12-07 14:06:01.872231: step 18070, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 87h:39m:07s remains)
INFO - root - 2017-12-07 14:06:11.489447: step 18080, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 84h:46m:33s remains)
INFO - root - 2017-12-07 14:06:21.108202: step 18090, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 85h:59m:28s remains)
INFO - root - 2017-12-07 14:06:30.717034: step 18100, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 82h:21m:05s remains)
2017-12-07 14:06:31.661228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2771125 -4.2698421 -4.268034 -4.2652893 -4.26428 -4.2671714 -4.2654076 -4.2653632 -4.2698936 -4.2663803 -4.2580628 -4.2520175 -4.2487535 -4.2473707 -4.248107][-4.236958 -4.2293625 -4.2252078 -4.2183738 -4.2176785 -4.2232752 -4.2205939 -4.218749 -4.2233062 -4.2235689 -4.2176056 -4.2091589 -4.2019258 -4.1995039 -4.2007613][-4.2191868 -4.2134447 -4.2097287 -4.2052865 -4.2070589 -4.2112722 -4.2014961 -4.19198 -4.1938524 -4.2003617 -4.2000165 -4.19537 -4.1910353 -4.1906285 -4.1925378][-4.2189307 -4.2156529 -4.2165561 -4.2172837 -4.2210345 -4.2199368 -4.2029157 -4.1883082 -4.189703 -4.1994848 -4.2021384 -4.2028694 -4.204093 -4.2066922 -4.2117095][-4.2297182 -4.2273064 -4.2272882 -4.2273097 -4.2283535 -4.2229695 -4.2052765 -4.190444 -4.1896424 -4.1921797 -4.1901159 -4.1944418 -4.2015204 -4.2062964 -4.2125545][-4.2347617 -4.2303629 -4.2240591 -4.2166696 -4.2107167 -4.2018456 -4.1849995 -4.1697435 -4.1602964 -4.1515331 -4.146503 -4.1522441 -4.1594896 -4.1643224 -4.1697211][-4.2103062 -4.2047005 -4.1934609 -4.1800351 -4.1675448 -4.1524911 -4.1356621 -4.1196008 -4.1038475 -4.0900621 -4.0899749 -4.0991383 -4.1025229 -4.0983615 -4.0956364][-4.1719074 -4.1673031 -4.1585641 -4.1452661 -4.126853 -4.1084318 -4.0901427 -4.075316 -4.0605574 -4.0503049 -4.0598884 -4.0770297 -4.0825195 -4.0765324 -4.0651693][-4.1589327 -4.1548605 -4.148838 -4.1396036 -4.1251087 -4.1109629 -4.0964851 -4.0867624 -4.0784965 -4.0753679 -4.0886874 -4.1090279 -4.1176605 -4.1127896 -4.1021729][-4.1750526 -4.1724143 -4.1685319 -4.1656909 -4.1583824 -4.151032 -4.1375937 -4.1305385 -4.1268435 -4.1250238 -4.1323214 -4.1511483 -4.1629667 -4.1587968 -4.1502123][-4.2120681 -4.2149119 -4.2171173 -4.2228966 -4.2238579 -4.2228608 -4.2092953 -4.2003589 -4.1949892 -4.1894984 -4.1868963 -4.1940837 -4.197648 -4.1905341 -4.1853981][-4.2200937 -4.2299538 -4.2403126 -4.2536592 -4.2639909 -4.2724633 -4.2665591 -4.26007 -4.2528706 -4.2425346 -4.2317133 -4.2264547 -4.2182393 -4.2020168 -4.1933][-4.1959233 -4.212925 -4.2306514 -4.2478123 -4.2615948 -4.275043 -4.2782192 -4.2773776 -4.2709985 -4.2576571 -4.2423334 -4.2298379 -4.2180157 -4.1986294 -4.1879325][-4.1745896 -4.1895041 -4.2047796 -4.2175379 -4.2279558 -4.2407217 -4.2480073 -4.2523317 -4.2508354 -4.2427926 -4.2316051 -4.2193346 -4.2038331 -4.179318 -4.1645889][-4.1933813 -4.1994681 -4.2052054 -4.2094526 -4.2126808 -4.2190833 -4.2239456 -4.2319427 -4.2361927 -4.2340522 -4.2315941 -4.2282143 -4.2166386 -4.1909223 -4.1739388]]...]
INFO - root - 2017-12-07 14:06:41.371544: step 18110, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 81h:51m:11s remains)
INFO - root - 2017-12-07 14:06:51.124160: step 18120, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 84h:58m:59s remains)
INFO - root - 2017-12-07 14:07:00.549777: step 18130, loss = 2.03, batch loss = 1.97 (8.3 examples/sec; 0.969 sec/batch; 84h:36m:41s remains)
INFO - root - 2017-12-07 14:07:10.167769: step 18140, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 85h:55m:34s remains)
INFO - root - 2017-12-07 14:07:19.901840: step 18150, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 86h:41m:49s remains)
INFO - root - 2017-12-07 14:07:29.574230: step 18160, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 84h:01m:28s remains)
INFO - root - 2017-12-07 14:07:39.227436: step 18170, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 82h:31m:34s remains)
INFO - root - 2017-12-07 14:07:48.811811: step 18180, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 86h:37m:37s remains)
INFO - root - 2017-12-07 14:07:58.494394: step 18190, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.023 sec/batch; 89h:17m:31s remains)
INFO - root - 2017-12-07 14:08:08.184127: step 18200, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 86h:18m:42s remains)
2017-12-07 14:08:09.156689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.297317 -4.2858582 -4.2869568 -4.2879186 -4.2779679 -4.2734017 -4.2781811 -4.2854128 -4.2935629 -4.3006134 -4.3062778 -4.3076754 -4.30346 -4.2964454 -4.294589][-4.2842312 -4.2702956 -4.2724624 -4.2714391 -4.2549806 -4.2466593 -4.2524142 -4.2637463 -4.2771454 -4.286725 -4.2975507 -4.3041196 -4.3023543 -4.2935691 -4.2899628][-4.26635 -4.2506685 -4.2482414 -4.2401609 -4.2195592 -4.2089424 -4.2141156 -4.2278905 -4.2494078 -4.2662759 -4.2819123 -4.2926364 -4.2939348 -4.2878857 -4.2858329][-4.2465119 -4.2285028 -4.2154312 -4.2042866 -4.1869435 -4.1740346 -4.1726642 -4.1876044 -4.2185135 -4.244391 -4.2634125 -4.2777596 -4.2826648 -4.281034 -4.2801185][-4.22004 -4.19379 -4.1658955 -4.1541576 -4.1461444 -4.1340585 -4.121851 -4.133616 -4.1746941 -4.2101288 -4.2338724 -4.2567654 -4.2695937 -4.2731476 -4.2712831][-4.1996441 -4.1567388 -4.1080351 -4.089004 -4.0945415 -4.0887671 -4.0672684 -4.0719867 -4.1237793 -4.1655483 -4.1939139 -4.222445 -4.2453942 -4.2606411 -4.2633314][-4.1930275 -4.1315889 -4.0581527 -4.0260077 -4.0387983 -4.0403767 -4.0108242 -4.0022469 -4.0593705 -4.1104455 -4.1420951 -4.1745992 -4.2054429 -4.2307067 -4.2459488][-4.2047634 -4.1348481 -4.0471749 -4.0047293 -4.0164552 -4.0168238 -3.9735205 -3.9458907 -4.0011978 -4.0574441 -4.08827 -4.1222892 -4.1604609 -4.1932526 -4.2229347][-4.22572 -4.1647258 -4.0844574 -4.0380926 -4.0378923 -4.0291252 -3.9796963 -3.9421272 -3.9811678 -4.0319252 -4.0506463 -4.0802879 -4.1195812 -4.1571393 -4.1980615][-4.2327261 -4.1876359 -4.1296268 -4.0915947 -4.0860524 -4.0753794 -4.0343437 -3.9989555 -4.0154533 -4.0440292 -4.0463376 -4.0651488 -4.0958333 -4.1298676 -4.1716609][-4.2228417 -4.1855159 -4.1498919 -4.1300368 -4.1298232 -4.1220436 -4.0942883 -4.0752296 -4.0799584 -4.0810232 -4.0665069 -4.0706663 -4.0900168 -4.1149445 -4.1467509][-4.2049847 -4.1676364 -4.1454458 -4.1394577 -4.1425414 -4.1409822 -4.1310253 -4.1305556 -4.1389923 -4.1268654 -4.098568 -4.0871181 -4.0956626 -4.1111455 -4.1358843][-4.1907611 -4.1546297 -4.1406388 -4.1423154 -4.1473308 -4.1525965 -4.1559925 -4.1637659 -4.1712608 -4.1588578 -4.1296234 -4.1099768 -4.1116786 -4.1254716 -4.1518731][-4.199975 -4.1698961 -4.1631341 -4.1702228 -4.18001 -4.1886077 -4.1933632 -4.1979756 -4.2028718 -4.194057 -4.171495 -4.1549835 -4.1573553 -4.17231 -4.196393][-4.23632 -4.2136283 -4.2121115 -4.2222409 -4.2326317 -4.2395778 -4.2425694 -4.2427936 -4.2423491 -4.2353892 -4.2244558 -4.2178349 -4.2245045 -4.239327 -4.2566414]]...]
INFO - root - 2017-12-07 14:08:18.778929: step 18210, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 82h:17m:00s remains)
INFO - root - 2017-12-07 14:08:28.404192: step 18220, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 85h:51m:32s remains)
INFO - root - 2017-12-07 14:08:37.938397: step 18230, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 84h:36m:21s remains)
INFO - root - 2017-12-07 14:08:47.716331: step 18240, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 82h:12m:07s remains)
INFO - root - 2017-12-07 14:08:57.265341: step 18250, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.969 sec/batch; 84h:37m:06s remains)
INFO - root - 2017-12-07 14:09:06.889011: step 18260, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.010 sec/batch; 88h:09m:41s remains)
INFO - root - 2017-12-07 14:09:16.551372: step 18270, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 80h:45m:56s remains)
INFO - root - 2017-12-07 14:09:26.212638: step 18280, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 83h:21m:15s remains)
INFO - root - 2017-12-07 14:09:35.943484: step 18290, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 84h:38m:30s remains)
INFO - root - 2017-12-07 14:09:45.594299: step 18300, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.947 sec/batch; 82h:37m:21s remains)
2017-12-07 14:09:46.521372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2118049 -4.239274 -4.2570438 -4.271524 -4.2881017 -4.287817 -4.2739224 -4.2551126 -4.2472639 -4.2544136 -4.2671375 -4.2804093 -4.2924185 -4.3047895 -4.3176694][-4.2371788 -4.2695785 -4.2834244 -4.2858977 -4.2859697 -4.2658019 -4.2307286 -4.1989436 -4.1925855 -4.2128034 -4.241044 -4.2690616 -4.2905397 -4.306716 -4.3202629][-4.251574 -4.2834554 -4.2913237 -4.28312 -4.267272 -4.2255607 -4.1659184 -4.1179347 -4.1135588 -4.150147 -4.1976075 -4.2436237 -4.2795372 -4.304461 -4.3224535][-4.2635679 -4.2936797 -4.2963243 -4.2791915 -4.2486329 -4.1862278 -4.0998178 -4.0326147 -4.0299883 -4.0847759 -4.1528549 -4.2173753 -4.2672582 -4.2995415 -4.3223915][-4.2690792 -4.2975492 -4.2968354 -4.2722535 -4.2280569 -4.1462088 -4.0340114 -3.9454086 -3.9438457 -4.0211916 -4.1128545 -4.1955018 -4.2566977 -4.2939434 -4.3180532][-4.2643905 -4.2903423 -4.286582 -4.2539344 -4.1974015 -4.1006155 -3.9658797 -3.8518734 -3.8479471 -3.9527197 -4.0745106 -4.1766443 -4.2487025 -4.2906389 -4.3129487][-4.2514563 -4.27209 -4.2634578 -4.2224832 -4.1560059 -4.0512285 -3.9008913 -3.7616522 -3.749969 -3.8849885 -4.0391884 -4.1594963 -4.2399549 -4.2866325 -4.3097224][-4.2387767 -4.2531934 -4.2437663 -4.2031307 -4.1374946 -4.0386415 -3.8898635 -3.73859 -3.7121072 -3.8560326 -4.0240703 -4.1494727 -4.2323427 -4.280797 -4.303997][-4.23556 -4.2475495 -4.2447982 -4.2145276 -4.1643438 -4.090116 -3.9703557 -3.8434906 -3.8090632 -3.9181135 -4.05786 -4.1612883 -4.2325597 -4.27707 -4.2958703][-4.2286615 -4.2441559 -4.2524533 -4.2404423 -4.214736 -4.1740785 -4.0979772 -4.0136552 -3.9812725 -4.0438309 -4.1328039 -4.194778 -4.239193 -4.2730355 -4.2867975][-4.2055907 -4.2250447 -4.2412596 -4.2468271 -4.2459612 -4.2349858 -4.1958265 -4.1472511 -4.1214905 -4.1492753 -4.1951766 -4.2207003 -4.2371025 -4.2553616 -4.265347][-4.1616025 -4.1788092 -4.1984129 -4.2181888 -4.2394385 -4.2524996 -4.2414293 -4.219162 -4.2007923 -4.2078285 -4.22051 -4.2132936 -4.2045484 -4.2089939 -4.2157588][-4.1078744 -4.121058 -4.1420512 -4.1724806 -4.2101107 -4.2404528 -4.2475719 -4.2406006 -4.2265539 -4.2217932 -4.2122321 -4.1816549 -4.1568079 -4.1539965 -4.1593938][-4.0824137 -4.0953794 -4.1132975 -4.1437941 -4.1897879 -4.2305484 -4.2501373 -4.2534537 -4.2394133 -4.2231688 -4.1987319 -4.1586838 -4.134016 -4.1341953 -4.142365][-4.1071849 -4.1199965 -4.1331744 -4.1561575 -4.2003126 -4.2436051 -4.2672367 -4.2728157 -4.2589936 -4.2408051 -4.2147746 -4.17702 -4.1597128 -4.1670537 -4.1784754]]...]
INFO - root - 2017-12-07 14:09:56.144504: step 18310, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 83h:27m:27s remains)
INFO - root - 2017-12-07 14:10:05.843350: step 18320, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.952 sec/batch; 83h:02m:51s remains)
INFO - root - 2017-12-07 14:10:15.334507: step 18330, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 84h:38m:44s remains)
INFO - root - 2017-12-07 14:10:24.956664: step 18340, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 80h:29m:58s remains)
INFO - root - 2017-12-07 14:10:34.618723: step 18350, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 79h:47m:00s remains)
INFO - root - 2017-12-07 14:10:44.436256: step 18360, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.001 sec/batch; 87h:19m:35s remains)
INFO - root - 2017-12-07 14:10:54.022244: step 18370, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 84h:23m:27s remains)
INFO - root - 2017-12-07 14:11:03.683984: step 18380, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.948 sec/batch; 82h:42m:48s remains)
INFO - root - 2017-12-07 14:11:13.412087: step 18390, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 88h:02m:25s remains)
INFO - root - 2017-12-07 14:11:23.038579: step 18400, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 83h:04m:35s remains)
2017-12-07 14:11:23.991293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2040577 -4.1821733 -4.1635723 -4.1587548 -4.166409 -4.1925673 -4.2045827 -4.2136278 -4.2225828 -4.2372165 -4.2493882 -4.2531676 -4.2479043 -4.2541027 -4.2708573][-4.245245 -4.2277346 -4.2082062 -4.1981258 -4.2015738 -4.2228055 -4.2340894 -4.2472353 -4.2617517 -4.275887 -4.2763109 -4.2644587 -4.247108 -4.24711 -4.2609086][-4.2598219 -4.2575727 -4.246479 -4.2389627 -4.2357478 -4.2430439 -4.2445912 -4.2537084 -4.2659144 -4.2741461 -4.2652574 -4.2447777 -4.2247276 -4.226048 -4.2449474][-4.2438917 -4.2560463 -4.2559924 -4.2547188 -4.2498813 -4.2467237 -4.2401991 -4.2442665 -4.2490883 -4.2474551 -4.2310071 -4.2117295 -4.1959515 -4.205359 -4.2346506][-4.2078757 -4.2247 -4.23296 -4.2389417 -4.2374272 -4.2324862 -4.2207584 -4.2194114 -4.2180686 -4.2073245 -4.190783 -4.177279 -4.1731119 -4.1961327 -4.2351232][-4.1706324 -4.1926742 -4.2038989 -4.210063 -4.2085176 -4.2048078 -4.1896629 -4.1855927 -4.1787968 -4.1680264 -4.1600618 -4.1561565 -4.1683741 -4.2045946 -4.2448797][-4.1438046 -4.1682038 -4.1767597 -4.1745982 -4.1684561 -4.1613574 -4.1423049 -4.1329322 -4.1191449 -4.1146193 -4.1267443 -4.1439142 -4.1752772 -4.2178569 -4.2541986][-4.133492 -4.148293 -4.1436033 -4.1250458 -4.1067314 -4.0936909 -4.0757227 -4.0673041 -4.0542545 -4.0547171 -4.0871921 -4.1305041 -4.1794944 -4.2257161 -4.2608557][-4.1258278 -4.1289463 -4.1107116 -4.0788155 -4.0493555 -4.0332794 -4.0231528 -4.0262961 -4.0202179 -4.0231462 -4.0670881 -4.1236582 -4.1792088 -4.2267504 -4.2630129][-4.127655 -4.1290641 -4.1097183 -4.0748997 -4.0403318 -4.0263338 -4.0286856 -4.0413055 -4.039989 -4.0440626 -4.0873618 -4.138515 -4.1843805 -4.2259059 -4.2642241][-4.1365786 -4.1416535 -4.1275783 -4.0993137 -4.0699434 -4.0642161 -4.0813022 -4.1018443 -4.1041584 -4.105742 -4.1358666 -4.169383 -4.1992049 -4.2325096 -4.2699766][-4.14425 -4.1520424 -4.1460309 -4.1331177 -4.1202407 -4.127367 -4.152596 -4.1741357 -4.1749072 -4.1708746 -4.1842394 -4.1998734 -4.2170777 -4.2448754 -4.279089][-4.1614833 -4.1693439 -4.1719136 -4.1738472 -4.1754508 -4.1888938 -4.2120953 -4.2283611 -4.2270532 -4.2173705 -4.2185225 -4.222857 -4.2358885 -4.2613211 -4.2914295][-4.2023067 -4.2082782 -4.2132792 -4.2203817 -4.2276106 -4.2382526 -4.2513103 -4.2600102 -4.2560554 -4.2436876 -4.241816 -4.2438288 -4.2540207 -4.2778125 -4.3044577][-4.2512431 -4.2519197 -4.2552772 -4.2626281 -4.2696514 -4.2768016 -4.2827768 -4.2849116 -4.277864 -4.2625728 -4.2589245 -4.2589531 -4.266984 -4.2890129 -4.313756]]...]
INFO - root - 2017-12-07 14:11:33.729091: step 18410, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 84h:33m:49s remains)
INFO - root - 2017-12-07 14:11:43.217368: step 18420, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 75h:13m:25s remains)
INFO - root - 2017-12-07 14:11:52.809359: step 18430, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 85h:14m:21s remains)
INFO - root - 2017-12-07 14:12:02.491791: step 18440, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.022 sec/batch; 89h:08m:54s remains)
INFO - root - 2017-12-07 14:12:12.258143: step 18450, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 86h:22m:40s remains)
INFO - root - 2017-12-07 14:12:21.953275: step 18460, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 84h:50m:01s remains)
INFO - root - 2017-12-07 14:12:31.495146: step 18470, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 85h:37m:32s remains)
INFO - root - 2017-12-07 14:12:41.445696: step 18480, loss = 2.10, batch loss = 2.05 (7.9 examples/sec; 1.008 sec/batch; 87h:52m:56s remains)
INFO - root - 2017-12-07 14:12:51.124952: step 18490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 83h:51m:30s remains)
INFO - root - 2017-12-07 14:13:00.881004: step 18500, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 87h:26m:37s remains)
2017-12-07 14:13:01.792930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2619662 -4.2736206 -4.2874274 -4.3004746 -4.3110704 -4.318706 -4.3223104 -4.3245492 -4.3260117 -4.324934 -4.3208618 -4.3121834 -4.297514 -4.2752962 -4.256856][-4.2543907 -4.2661185 -4.2798357 -4.2927594 -4.3043618 -4.3130302 -4.3180137 -4.3244252 -4.3323541 -4.3374395 -4.3368583 -4.3291421 -4.3123522 -4.2866874 -4.2643266][-4.2469654 -4.2537155 -4.2621427 -4.2699671 -4.2798347 -4.2876773 -4.2933269 -4.3046122 -4.3209124 -4.3340588 -4.3390579 -4.3347588 -4.3183408 -4.2916989 -4.2677708][-4.2385716 -4.2343159 -4.2314777 -4.2295794 -4.2350249 -4.240551 -4.2443094 -4.2584906 -4.2837591 -4.3064184 -4.3188109 -4.3200932 -4.3079519 -4.2844925 -4.2626152][-4.2276258 -4.2090559 -4.1913829 -4.1773348 -4.1756859 -4.1758165 -4.1725893 -4.1827445 -4.2139673 -4.2475562 -4.2694974 -4.2788124 -4.2755313 -4.2608185 -4.2464633][-4.2130146 -4.1824875 -4.1524572 -4.127748 -4.1173744 -4.1084514 -4.092052 -4.0875425 -4.11743 -4.1620879 -4.1960554 -4.2155609 -4.2246022 -4.2227435 -4.2201567][-4.1921997 -4.1558375 -4.1195869 -4.0882435 -4.0687647 -4.0495005 -4.018364 -3.9932323 -4.0138693 -4.068099 -4.1154108 -4.1460409 -4.1679692 -4.1806479 -4.1918235][-4.1707363 -4.1347308 -4.0983896 -4.0651069 -4.0402508 -4.0119057 -3.9700916 -3.9292207 -3.9385369 -3.9974227 -4.0557346 -4.0951428 -4.1258593 -4.1497316 -4.17239][-4.1586261 -4.1250954 -4.0915179 -4.0600557 -4.0357165 -4.0065928 -3.9652262 -3.9240644 -3.9243124 -3.9762068 -4.0360141 -4.0789628 -4.112432 -4.1403189 -4.1683893][-4.1621914 -4.130209 -4.0986156 -4.071064 -4.052063 -4.0319662 -4.00212 -3.9717987 -3.9685428 -4.0059891 -4.0571218 -4.0964823 -4.1259971 -4.1510859 -4.1782646][-4.1868219 -4.1583171 -4.129446 -4.1064992 -4.0935063 -4.0843215 -4.068974 -4.0510263 -4.0473819 -4.0701914 -4.1076202 -4.1378284 -4.1575413 -4.1748233 -4.1955223][-4.2240362 -4.2047434 -4.1827869 -4.1654248 -4.1571522 -4.1547852 -4.1491208 -4.1384721 -4.1333838 -4.1428494 -4.1642852 -4.1823583 -4.1924119 -4.2014332 -4.212749][-4.256351 -4.2534218 -4.2455783 -4.2364812 -4.2309585 -4.2301149 -4.2271857 -4.2183022 -4.2098961 -4.2084327 -4.2155595 -4.2217355 -4.2233591 -4.223712 -4.2231483][-4.2583957 -4.274941 -4.285172 -4.2877893 -4.2877955 -4.288146 -4.2847614 -4.2745504 -4.2626524 -4.2536306 -4.2500353 -4.2474275 -4.2426758 -4.235321 -4.2219162][-4.22066 -4.2538953 -4.2813439 -4.2981153 -4.3076973 -4.3132138 -4.3122444 -4.3040314 -4.2923055 -4.2792988 -4.2678084 -4.2575078 -4.2466221 -4.2317386 -4.2065649]]...]
INFO - root - 2017-12-07 14:13:11.425233: step 18510, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 86h:14m:02s remains)
INFO - root - 2017-12-07 14:13:20.973773: step 18520, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 84h:12m:59s remains)
INFO - root - 2017-12-07 14:13:30.581898: step 18530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 84h:23m:55s remains)
INFO - root - 2017-12-07 14:13:40.223316: step 18540, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 83h:46m:53s remains)
INFO - root - 2017-12-07 14:13:49.920649: step 18550, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 85h:23m:52s remains)
INFO - root - 2017-12-07 14:13:59.547443: step 18560, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 84h:12m:37s remains)
INFO - root - 2017-12-07 14:14:09.127576: step 18570, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 83h:31m:21s remains)
INFO - root - 2017-12-07 14:14:18.806902: step 18580, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 85h:59m:07s remains)
INFO - root - 2017-12-07 14:14:28.313053: step 18590, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 80h:43m:27s remains)
INFO - root - 2017-12-07 14:14:38.018436: step 18600, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 84h:17m:28s remains)
2017-12-07 14:14:39.024516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2977552 -4.281743 -4.2581062 -4.2420306 -4.245069 -4.25223 -4.2591333 -4.2628455 -4.2676239 -4.2786026 -4.2899289 -4.2985849 -4.3087487 -4.3147283 -4.3109131][-4.2892942 -4.26989 -4.2411623 -4.2227941 -4.2253947 -4.2286272 -4.2311311 -4.2299371 -4.233325 -4.2483587 -4.2624378 -4.270895 -4.284462 -4.2916756 -4.2875166][-4.2951117 -4.2749243 -4.2447362 -4.2295384 -4.2323732 -4.22998 -4.2225714 -4.2087793 -4.2048073 -4.2205935 -4.2414918 -4.256053 -4.2759323 -4.287003 -4.2834358][-4.3047056 -4.2832885 -4.2550654 -4.2420588 -4.2401958 -4.2275128 -4.2056909 -4.1750259 -4.1643777 -4.1853371 -4.2181435 -4.2480044 -4.2775164 -4.295702 -4.2950411][-4.3206372 -4.3031864 -4.2794871 -4.264257 -4.2525678 -4.2254848 -4.186111 -4.1369367 -4.125936 -4.1569543 -4.2025967 -4.247643 -4.2880473 -4.3107305 -4.3101611][-4.313508 -4.3050022 -4.2888851 -4.2694163 -4.2463837 -4.2071743 -4.1518841 -4.09299 -4.0899034 -4.1339531 -4.1910386 -4.2464786 -4.2955079 -4.3221674 -4.3228679][-4.2702637 -4.2698331 -4.2654805 -4.2476845 -4.2124586 -4.1584244 -4.0876169 -4.0266585 -4.0409851 -4.1026855 -4.17043 -4.2321815 -4.2847137 -4.3124762 -4.31333][-4.2030239 -4.2093525 -4.2185659 -4.2044897 -4.1634064 -4.09438 -4.0050955 -3.9451058 -3.983464 -4.0681124 -4.1492929 -4.2151585 -4.2685871 -4.2932572 -4.2893066][-4.1292305 -4.1351218 -4.1564889 -4.1543589 -4.1180916 -4.04096 -3.9357235 -3.8798223 -3.93963 -4.0406747 -4.1305103 -4.1978679 -4.2501926 -4.2699261 -4.2618089][-4.0780692 -4.0848761 -4.115634 -4.1288252 -4.1065941 -4.0398693 -3.9392629 -3.8903105 -3.95514 -4.0524044 -4.1342077 -4.1924009 -4.2344484 -4.2469878 -4.2356267][-4.0817146 -4.0906043 -4.1255393 -4.1487589 -4.1433349 -4.0997744 -4.0281415 -3.9928331 -4.0472145 -4.1193042 -4.1744628 -4.2096486 -4.2320676 -4.2338772 -4.22093][-4.1283722 -4.1344595 -4.1634674 -4.18361 -4.1867361 -4.1650538 -4.1270227 -4.1089315 -4.1440639 -4.1859312 -4.2146378 -4.2311325 -4.2388878 -4.23225 -4.2162728][-4.1917086 -4.1933837 -4.2111626 -4.2252941 -4.2317524 -4.2248898 -4.206543 -4.1956105 -4.2131429 -4.2327337 -4.2471514 -4.2562389 -4.259901 -4.253314 -4.2390518][-4.2306628 -4.2332134 -4.2475414 -4.2612882 -4.2678752 -4.2669716 -4.2589536 -4.2528925 -4.2600102 -4.2678633 -4.2770491 -4.2836852 -4.288806 -4.2882485 -4.2799873][-4.2480588 -4.2542 -4.2682934 -4.282979 -4.291204 -4.2920294 -4.28739 -4.282896 -4.2855377 -4.2912836 -4.3002558 -4.3081346 -4.3169365 -4.3225551 -4.3187609]]...]
INFO - root - 2017-12-07 14:14:48.570898: step 18610, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 80h:37m:03s remains)
INFO - root - 2017-12-07 14:14:57.960233: step 18620, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 82h:33m:58s remains)
INFO - root - 2017-12-07 14:15:07.683209: step 18630, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 86h:58m:15s remains)
INFO - root - 2017-12-07 14:15:17.361550: step 18640, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 83h:36m:30s remains)
INFO - root - 2017-12-07 14:15:26.956022: step 18650, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.015 sec/batch; 88h:30m:33s remains)
INFO - root - 2017-12-07 14:15:36.542075: step 18660, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 82h:39m:11s remains)
INFO - root - 2017-12-07 14:15:46.354263: step 18670, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.014 sec/batch; 88h:26m:12s remains)
INFO - root - 2017-12-07 14:15:56.018777: step 18680, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.029 sec/batch; 89h:43m:45s remains)
INFO - root - 2017-12-07 14:16:05.630607: step 18690, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 86h:21m:14s remains)
INFO - root - 2017-12-07 14:16:15.210071: step 18700, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 86h:27m:03s remains)
2017-12-07 14:16:16.225197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3519831 -4.3306131 -4.2976623 -4.2611661 -4.2306323 -4.2103949 -4.2149444 -4.215754 -4.1998949 -4.2079415 -4.2130728 -4.2029462 -4.19413 -4.2093811 -4.2350526][-4.3389206 -4.3132668 -4.2789388 -4.2398338 -4.20459 -4.1803427 -4.1870818 -4.1941414 -4.1923151 -4.2116117 -4.2250476 -4.2271833 -4.224854 -4.2344031 -4.2489691][-4.3145542 -4.2831349 -4.2479105 -4.2088423 -4.170198 -4.14572 -4.1581483 -4.1732721 -4.18217 -4.2083373 -4.2261953 -4.2386885 -4.2426066 -4.24631 -4.2513533][-4.2999854 -4.2689233 -4.2392812 -4.2094 -4.1796284 -4.1661253 -4.1841116 -4.1975121 -4.2030873 -4.22324 -4.23565 -4.2484446 -4.2525725 -4.25406 -4.25667][-4.2971349 -4.2747984 -4.2605305 -4.2470965 -4.232564 -4.2319422 -4.2499709 -4.2502217 -4.24203 -4.2537608 -4.2565651 -4.258213 -4.2606235 -4.2642469 -4.2680779][-4.2932954 -4.2766166 -4.2726412 -4.2710814 -4.2672019 -4.2724328 -4.2838326 -4.271317 -4.2557483 -4.2662129 -4.2669358 -4.2614579 -4.25825 -4.2619457 -4.2705369][-4.2854223 -4.2702394 -4.2667918 -4.267777 -4.2644992 -4.266593 -4.2666812 -4.2381229 -4.21333 -4.2288518 -4.2391977 -4.2379289 -4.2359009 -4.2408891 -4.2559791][-4.2716513 -4.2583466 -4.2524619 -4.2523603 -4.2447724 -4.2322569 -4.2098103 -4.1556773 -4.1186266 -4.1505313 -4.1863513 -4.2024884 -4.2117915 -4.2195125 -4.23627][-4.2553654 -4.2444911 -4.2372303 -4.2342281 -4.2198286 -4.1906323 -4.1408787 -4.0562716 -4.0067444 -4.0608397 -4.1303644 -4.1713982 -4.1997542 -4.215436 -4.2299695][-4.245976 -4.2380815 -4.2300296 -4.2256246 -4.2093363 -4.17543 -4.1167445 -4.0294771 -3.9810283 -4.0390944 -4.1182556 -4.1702046 -4.2074218 -4.2270927 -4.2342267][-4.2537141 -4.2490921 -4.2417126 -4.2390227 -4.2278275 -4.205153 -4.16496 -4.1047635 -4.0706987 -4.1071568 -4.1669841 -4.2079649 -4.23652 -4.249248 -4.2448521][-4.2796249 -4.274931 -4.2663369 -4.2638822 -4.2577796 -4.2473917 -4.2282858 -4.1956253 -4.173955 -4.1924324 -4.2266607 -4.2512016 -4.2669926 -4.27238 -4.2634988][-4.2997713 -4.2927217 -4.2819395 -4.2809539 -4.2811432 -4.27903 -4.2703786 -4.2505374 -4.2325759 -4.2406168 -4.2607279 -4.272819 -4.2815351 -4.2868333 -4.284286][-4.3003159 -4.291554 -4.279881 -4.2784748 -4.2810497 -4.2833562 -4.2815318 -4.269815 -4.2576303 -4.2623587 -4.2740712 -4.2796497 -4.2858357 -4.2950964 -4.3011856][-4.2922277 -4.2832665 -4.2710371 -4.2657366 -4.2665987 -4.27058 -4.273407 -4.2721887 -4.2696848 -4.2743855 -4.2816882 -4.2869773 -4.2948847 -4.306756 -4.3161149]]...]
INFO - root - 2017-12-07 14:16:26.001052: step 18710, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.942 sec/batch; 82h:08m:18s remains)
INFO - root - 2017-12-07 14:16:35.433917: step 18720, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 83h:18m:26s remains)
INFO - root - 2017-12-07 14:16:45.152726: step 18730, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 80h:28m:35s remains)
INFO - root - 2017-12-07 14:16:54.964040: step 18740, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 84h:42m:41s remains)
INFO - root - 2017-12-07 14:17:04.436001: step 18750, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.823 sec/batch; 71h:44m:06s remains)
INFO - root - 2017-12-07 14:17:14.170395: step 18760, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 86h:27m:27s remains)
INFO - root - 2017-12-07 14:17:24.030372: step 18770, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 80h:33m:50s remains)
INFO - root - 2017-12-07 14:17:33.643875: step 18780, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.032 sec/batch; 89h:56m:34s remains)
INFO - root - 2017-12-07 14:17:43.307098: step 18790, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 85h:40m:37s remains)
INFO - root - 2017-12-07 14:17:53.013214: step 18800, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 80h:36m:52s remains)
2017-12-07 14:17:53.997598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2342334 -4.2564015 -4.2800241 -4.2844625 -4.2729092 -4.2497158 -4.2243285 -4.1993418 -4.1726756 -4.1460285 -4.1218352 -4.1138458 -4.1135726 -4.1342869 -4.1706381][-4.2246051 -4.2372742 -4.2539816 -4.2547913 -4.2470708 -4.2286716 -4.2044668 -4.1761093 -4.1474557 -4.1286192 -4.1180191 -4.1171594 -4.1157937 -4.1232038 -4.1442995][-4.2203221 -4.2177925 -4.2222152 -4.2172208 -4.2086821 -4.1896906 -4.1629324 -4.1312056 -4.1069436 -4.1042433 -4.1107254 -4.1209445 -4.12025 -4.1184368 -4.1248169][-4.2236748 -4.2085686 -4.2021403 -4.1878395 -4.16726 -4.137073 -4.0991464 -4.059968 -4.0424018 -4.0593438 -4.0844707 -4.1085615 -4.1163611 -4.1189117 -4.1256285][-4.2403831 -4.2228642 -4.2107868 -4.1838484 -4.1404753 -4.088192 -4.0326495 -3.9786022 -3.9583633 -3.9938502 -4.0402355 -4.0804172 -4.1021972 -4.1194553 -4.1361041][-4.2626982 -4.2521243 -4.2393236 -4.2003956 -4.1319818 -4.0496593 -3.9654622 -3.8813286 -3.8489478 -3.9094703 -3.98501 -4.0473938 -4.0896759 -4.1235552 -4.1506057][-4.2667508 -4.264761 -4.2544508 -4.2137265 -4.1335783 -4.0322719 -3.9236691 -3.8112371 -3.761596 -3.8503518 -3.9546742 -4.0376153 -4.097928 -4.145802 -4.1821589][-4.2663341 -4.27081 -4.2617588 -4.2233148 -4.1514277 -4.0614042 -3.9664211 -3.8696461 -3.8262877 -3.8976936 -3.9871881 -4.0597339 -4.1173916 -4.1670218 -4.2040343][-4.26499 -4.2753758 -4.2658935 -4.2308159 -4.1760936 -4.1161075 -4.0579762 -4.0042772 -3.9753933 -4.0077357 -4.0559697 -4.0972419 -4.1337476 -4.17422 -4.2058239][-4.2465234 -4.2632513 -4.2570944 -4.2293921 -4.1936374 -4.1620688 -4.1375632 -4.1145372 -4.0950861 -4.1017742 -4.1179857 -4.1321449 -4.1507344 -4.1835055 -4.2085934][-4.2264085 -4.2499104 -4.2522554 -4.2374239 -4.2204456 -4.2101526 -4.2060251 -4.1972294 -4.1816406 -4.1726217 -4.16818 -4.1662693 -4.1709008 -4.1941848 -4.2115159][-4.231607 -4.2606931 -4.2674913 -4.2612953 -4.2549014 -4.2562628 -4.2575908 -4.2506127 -4.2341747 -4.2171354 -4.2015114 -4.1927438 -4.1941371 -4.215086 -4.230299][-4.2471738 -4.27864 -4.2853785 -4.2800283 -4.2749248 -4.2791672 -4.2776022 -4.2670112 -4.2491951 -4.2316418 -4.2187042 -4.2149663 -4.2279339 -4.2544703 -4.2708144][-4.2682991 -4.299468 -4.3050013 -4.3009567 -4.2961435 -4.29925 -4.2949996 -4.2825456 -4.2683096 -4.2578259 -4.2527037 -4.2556152 -4.2745585 -4.2999749 -4.3118505][-4.2874861 -4.3138614 -4.3169823 -4.3148947 -4.3136253 -4.316144 -4.3129697 -4.3041296 -4.2972956 -4.2951307 -4.2965097 -4.3011546 -4.3160667 -4.3333282 -4.3407545]]...]
INFO - root - 2017-12-07 14:18:03.599301: step 18810, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 81h:06m:45s remains)
INFO - root - 2017-12-07 14:18:13.273277: step 18820, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 84h:32m:36s remains)
INFO - root - 2017-12-07 14:18:22.902449: step 18830, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 84h:52m:34s remains)
INFO - root - 2017-12-07 14:18:32.713272: step 18840, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 84h:08m:39s remains)
INFO - root - 2017-12-07 14:18:42.466903: step 18850, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.043 sec/batch; 90h:50m:38s remains)
INFO - root - 2017-12-07 14:18:52.175792: step 18860, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 84h:48m:41s remains)
INFO - root - 2017-12-07 14:19:01.864553: step 18870, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 85h:26m:10s remains)
INFO - root - 2017-12-07 14:19:11.665026: step 18880, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 79h:57m:42s remains)
INFO - root - 2017-12-07 14:19:21.158921: step 18890, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 83h:10m:39s remains)
INFO - root - 2017-12-07 14:19:30.776295: step 18900, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 84h:57m:29s remains)
2017-12-07 14:19:31.654387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2890453 -4.2981715 -4.3136344 -4.325768 -4.3198195 -4.29227 -4.2665534 -4.2450876 -4.2428718 -4.2520366 -4.2682848 -4.2831159 -4.2916775 -4.2957773 -4.2958374][-4.26087 -4.2763252 -4.2997108 -4.3176017 -4.3117423 -4.2798457 -4.2456017 -4.2141557 -4.2097325 -4.2225566 -4.2454405 -4.2627769 -4.2707829 -4.2753162 -4.2803979][-4.2299929 -4.2482152 -4.2786031 -4.3033195 -4.2932277 -4.2569194 -4.2119884 -4.1714029 -4.1698556 -4.1898918 -4.219501 -4.2408986 -4.2508655 -4.2574449 -4.2724776][-4.1937795 -4.2077031 -4.2423911 -4.2746243 -4.2597718 -4.2184043 -4.1617546 -4.1181965 -4.1298389 -4.1623907 -4.2006369 -4.2304373 -4.2479486 -4.2585735 -4.2781587][-4.1619921 -4.1572862 -4.1881671 -4.2223248 -4.2017932 -4.1550527 -4.0951471 -4.0676079 -4.1031404 -4.1459322 -4.19097 -4.228282 -4.2513146 -4.2660422 -4.2841225][-4.1610069 -4.1343088 -4.1427078 -4.1619787 -4.1346807 -4.0675135 -3.9912865 -3.9887407 -4.0691013 -4.1399384 -4.1972904 -4.2396927 -4.2619 -4.2716508 -4.2809653][-4.1874156 -4.1541762 -4.1429906 -4.1366196 -4.0907173 -3.9845231 -3.8648381 -3.8923759 -4.0364227 -4.1502914 -4.2181487 -4.2615786 -4.281076 -4.2771244 -4.2766485][-4.2102695 -4.1821256 -4.1648855 -4.1464095 -4.0964146 -3.9860282 -3.8595967 -3.8943651 -4.0473766 -4.172955 -4.243103 -4.2838011 -4.297226 -4.2817435 -4.2754326][-4.2174916 -4.1925664 -4.1706934 -4.1550403 -4.1222906 -4.05348 -3.9820895 -4.0016451 -4.1005735 -4.1961784 -4.2546377 -4.2922373 -4.3040433 -4.2857676 -4.2801361][-4.2100058 -4.1897211 -4.1690722 -4.1589651 -4.1437244 -4.1137633 -4.0860019 -4.0969596 -4.1514158 -4.2095828 -4.2482791 -4.2816887 -4.2942772 -4.2844281 -4.28681][-4.1953983 -4.1892219 -4.1821208 -4.1745768 -4.168036 -4.1607814 -4.1542449 -4.1682081 -4.2046523 -4.2351046 -4.2519045 -4.2752814 -4.2857041 -4.2811308 -4.2918906][-4.1889257 -4.1928935 -4.2006116 -4.1999178 -4.1996226 -4.2040362 -4.2065096 -4.2206736 -4.2409897 -4.2517581 -4.2513947 -4.2685013 -4.2779422 -4.2793593 -4.2989264][-4.1888251 -4.1997104 -4.2204185 -4.2306194 -4.2396207 -4.2510285 -4.254776 -4.26187 -4.26696 -4.2658072 -4.2575512 -4.2679434 -4.2770042 -4.2825804 -4.3089204][-4.1996679 -4.2089005 -4.2342887 -4.2511449 -4.2679877 -4.2817197 -4.28317 -4.2844129 -4.2839775 -4.2727871 -4.2578979 -4.2612562 -4.269217 -4.2823486 -4.3123422][-4.2099867 -4.2149816 -4.2386475 -4.2578597 -4.2761965 -4.2862387 -4.2849622 -4.2831216 -4.2833676 -4.2703161 -4.2521639 -4.2479115 -4.2541065 -4.2707095 -4.2982645]]...]
INFO - root - 2017-12-07 14:19:41.191899: step 18910, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 84h:57m:16s remains)
INFO - root - 2017-12-07 14:19:50.662831: step 18920, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 86h:55m:04s remains)
INFO - root - 2017-12-07 14:20:00.257902: step 18930, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 85h:44m:18s remains)
INFO - root - 2017-12-07 14:20:09.720892: step 18940, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 85h:31m:04s remains)
INFO - root - 2017-12-07 14:20:19.409345: step 18950, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 81h:31m:27s remains)
INFO - root - 2017-12-07 14:20:29.238810: step 18960, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 83h:02m:30s remains)
INFO - root - 2017-12-07 14:20:38.880633: step 18970, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 85h:16m:24s remains)
INFO - root - 2017-12-07 14:20:48.435184: step 18980, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 80h:04m:35s remains)
INFO - root - 2017-12-07 14:20:58.204724: step 18990, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 84h:25m:36s remains)
INFO - root - 2017-12-07 14:21:07.761283: step 19000, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 78h:43m:15s remains)
2017-12-07 14:21:08.796276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2895203 -4.2752953 -4.2514877 -4.2204514 -4.1917934 -4.1685023 -4.1433449 -4.1376481 -4.1616311 -4.1792116 -4.1924162 -4.2042394 -4.21198 -4.2257862 -4.2471008][-4.2858095 -4.2696357 -4.2451787 -4.2105751 -4.1811 -4.1551208 -4.127315 -4.1133094 -4.1280327 -4.14115 -4.1563826 -4.1775541 -4.1993761 -4.224586 -4.2501512][-4.2963271 -4.2803245 -4.2521524 -4.2145915 -4.1835785 -4.1588426 -4.1300321 -4.105247 -4.1050034 -4.1122465 -4.133297 -4.164299 -4.1964674 -4.2278914 -4.2542496][-4.3056359 -4.29145 -4.2628293 -4.226706 -4.195684 -4.1664433 -4.130693 -4.0939336 -4.083797 -4.0882998 -4.1185141 -4.1588383 -4.1951213 -4.22692 -4.2523546][-4.313755 -4.3026757 -4.274188 -4.2363234 -4.1984835 -4.1542974 -4.10451 -4.0597696 -4.0502143 -4.0635648 -4.1072083 -4.158371 -4.1962876 -4.2246909 -4.2492471][-4.3153276 -4.3032036 -4.268858 -4.2242889 -4.174561 -4.1109662 -4.0405903 -3.9906704 -3.9980996 -4.0394211 -4.1024566 -4.1632419 -4.2023363 -4.2276077 -4.2505403][-4.3103065 -4.2929206 -4.2519369 -4.1968966 -4.1293368 -4.0377984 -3.9364398 -3.8846509 -3.9362147 -4.0232482 -4.107801 -4.1736879 -4.2105565 -4.2308793 -4.2503829][-4.3032479 -4.2805839 -4.2323985 -4.167387 -4.0805693 -3.9591632 -3.8256814 -3.7827077 -3.8989587 -4.03104 -4.1262021 -4.1871076 -4.2144341 -4.2282877 -4.2458444][-4.3006816 -4.27248 -4.2195368 -4.1483827 -4.057271 -3.9411168 -3.8300505 -3.8324928 -3.9633117 -4.0860744 -4.1626644 -4.2053151 -4.2190042 -4.2289524 -4.2464156][-4.2968345 -4.2647448 -4.2172728 -4.1562262 -4.0884428 -4.0183029 -3.9661226 -3.9893417 -4.0817089 -4.1601491 -4.2052021 -4.227119 -4.2327671 -4.2417135 -4.2579207][-4.2899551 -4.2586608 -4.2220926 -4.1813 -4.1444635 -4.1139269 -4.091526 -4.1114907 -4.16886 -4.2137022 -4.2365065 -4.2472558 -4.2514505 -4.260179 -4.2730293][-4.2818308 -4.252183 -4.2248907 -4.2017961 -4.1869626 -4.1749959 -4.1605148 -4.1736851 -4.211729 -4.2408376 -4.2550244 -4.2629061 -4.2680607 -4.2753878 -4.2841806][-4.2768145 -4.2481718 -4.2248516 -4.2106462 -4.2082829 -4.2043624 -4.1930232 -4.2011104 -4.2292128 -4.2512007 -4.263309 -4.2718186 -4.2773476 -4.2831283 -4.2905893][-4.2751403 -4.2475276 -4.2270083 -4.2169819 -4.2192769 -4.2209849 -4.2151308 -4.222302 -4.2445436 -4.26463 -4.2757111 -4.2831659 -4.2877178 -4.2924137 -4.300055][-4.2762313 -4.2497959 -4.2323036 -4.2265849 -4.2318158 -4.23941 -4.2401881 -4.2473779 -4.2638869 -4.2803173 -4.2892189 -4.2950563 -4.2998409 -4.3054771 -4.3132014]]...]
INFO - root - 2017-12-07 14:21:18.522478: step 19010, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 87h:19m:05s remains)
INFO - root - 2017-12-07 14:21:28.227368: step 19020, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 85h:54m:50s remains)
INFO - root - 2017-12-07 14:21:37.799278: step 19030, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 86h:03m:54s remains)
INFO - root - 2017-12-07 14:21:47.416448: step 19040, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 82h:34m:12s remains)
INFO - root - 2017-12-07 14:21:57.081145: step 19050, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 84h:33m:54s remains)
INFO - root - 2017-12-07 14:22:06.742428: step 19060, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 82h:53m:20s remains)
INFO - root - 2017-12-07 14:22:16.336462: step 19070, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 84h:06m:32s remains)
INFO - root - 2017-12-07 14:22:25.923060: step 19080, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 83h:46m:38s remains)
INFO - root - 2017-12-07 14:22:35.576025: step 19090, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 87h:41m:02s remains)
INFO - root - 2017-12-07 14:22:45.335895: step 19100, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 86h:15m:38s remains)
2017-12-07 14:22:46.335395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3116364 -4.3077173 -4.2996149 -4.2954688 -4.2930846 -4.2907877 -4.2876592 -4.2834558 -4.2839832 -4.2918019 -4.3011546 -4.3127017 -4.3295059 -4.3408909 -4.34602][-4.29654 -4.2877483 -4.2750921 -4.2686033 -4.2656031 -4.2640185 -4.2575541 -4.24498 -4.2435956 -4.2537189 -4.2639928 -4.2813463 -4.3094478 -4.3282433 -4.335968][-4.2719955 -4.2558408 -4.2372942 -4.2280931 -4.22384 -4.2231007 -4.2137179 -4.1919537 -4.1880579 -4.1994863 -4.2115941 -4.2384186 -4.2777267 -4.3058076 -4.3203554][-4.2383862 -4.2175264 -4.1963086 -4.1869922 -4.1844215 -4.1820173 -4.1656914 -4.1308112 -4.1211233 -4.1354914 -4.1542668 -4.1935234 -4.2430615 -4.2792015 -4.3008451][-4.197268 -4.1735554 -4.1491561 -4.1396618 -4.1402292 -4.1347814 -4.1080031 -4.0586271 -4.0495448 -4.0765405 -4.111783 -4.1672668 -4.2227764 -4.2592816 -4.2836542][-4.1547666 -4.1225886 -4.0929441 -4.0816245 -4.0825844 -4.0722113 -4.0366797 -3.9799888 -3.98364 -4.0322976 -4.0890183 -4.15618 -4.2128925 -4.2473645 -4.2715411][-4.1334419 -4.086935 -4.0403252 -4.0163722 -4.0103493 -3.9911888 -3.9564619 -3.9201186 -3.9509525 -4.0225635 -4.0930357 -4.1627645 -4.2162395 -4.2486806 -4.2719502][-4.1573529 -4.103332 -4.039463 -4.0036111 -3.98886 -3.96356 -3.94178 -3.9376779 -3.9863355 -4.0615835 -4.1281033 -4.1873889 -4.232708 -4.2615228 -4.2828312][-4.1909833 -4.1411138 -4.0789351 -4.0474491 -4.0365505 -4.0148692 -4.0024967 -4.0082779 -4.0509062 -4.114265 -4.1700659 -4.2175894 -4.2566085 -4.2829041 -4.2993431][-4.2176809 -4.1810207 -4.1368294 -4.1250625 -4.1243134 -4.1078348 -4.1000848 -4.0997562 -4.1258726 -4.1733274 -4.2162614 -4.2564321 -4.2890444 -4.3082857 -4.3176284][-4.2528753 -4.2320547 -4.2103882 -4.2150354 -4.2198606 -4.2074952 -4.2000437 -4.1912603 -4.2018504 -4.2319531 -4.2627625 -4.2971926 -4.3215575 -4.3334723 -4.3369827][-4.2934012 -4.288209 -4.2832518 -4.2927575 -4.2969232 -4.2888455 -4.2848516 -4.2732716 -4.2694683 -4.2821856 -4.3000369 -4.3231421 -4.3409715 -4.3497629 -4.3518248][-4.3126636 -4.3140926 -4.3165064 -4.32718 -4.333169 -4.3322926 -4.3334527 -4.3251996 -4.31596 -4.3166871 -4.3224025 -4.3348927 -4.3481374 -4.3577833 -4.3613253][-4.320282 -4.3216105 -4.3246942 -4.3334217 -4.3395171 -4.3421741 -4.3446674 -4.3412247 -4.3347921 -4.3335915 -4.336 -4.3424716 -4.3514991 -4.3603725 -4.3641367][-4.3364382 -4.337018 -4.339334 -4.3446589 -4.34807 -4.3498631 -4.3511648 -4.3500819 -4.3476081 -4.3468146 -4.3472648 -4.3511052 -4.3559895 -4.361608 -4.3644338]]...]
INFO - root - 2017-12-07 14:22:55.768872: step 19110, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 83h:38m:41s remains)
INFO - root - 2017-12-07 14:23:05.537467: step 19120, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.932 sec/batch; 81h:09m:58s remains)
INFO - root - 2017-12-07 14:23:15.137524: step 19130, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 82h:09m:58s remains)
INFO - root - 2017-12-07 14:23:24.965492: step 19140, loss = 2.06, batch loss = 2.01 (7.8 examples/sec; 1.022 sec/batch; 88h:59m:39s remains)
INFO - root - 2017-12-07 14:23:34.663674: step 19150, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 80h:31m:16s remains)
INFO - root - 2017-12-07 14:23:44.274721: step 19160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 81h:17m:01s remains)
INFO - root - 2017-12-07 14:23:53.992531: step 19170, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.033 sec/batch; 89h:54m:02s remains)
INFO - root - 2017-12-07 14:24:03.696758: step 19180, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.970 sec/batch; 84h:27m:18s remains)
INFO - root - 2017-12-07 14:24:13.313658: step 19190, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 86h:55m:38s remains)
INFO - root - 2017-12-07 14:24:22.891416: step 19200, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 83h:17m:57s remains)
2017-12-07 14:24:23.902260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3214808 -4.3142519 -4.2998176 -4.282814 -4.2554975 -4.2208915 -4.2012553 -4.2079768 -4.2207489 -4.2369308 -4.2616196 -4.29201 -4.3141232 -4.3280325 -4.3421712][-4.31091 -4.2992954 -4.278933 -4.2550015 -4.2221532 -4.1890554 -4.1729374 -4.1833358 -4.2040076 -4.230998 -4.2625151 -4.29674 -4.3188109 -4.3306236 -4.3439951][-4.3014064 -4.2869754 -4.2628574 -4.2327385 -4.1975217 -4.1694036 -4.1548791 -4.1641321 -4.1903796 -4.2273579 -4.2648754 -4.3001008 -4.3204088 -4.3306994 -4.3441105][-4.2875705 -4.275322 -4.2516189 -4.2170177 -4.182723 -4.1580849 -4.1410732 -4.1458879 -4.1742821 -4.2177792 -4.2618666 -4.2993279 -4.3194618 -4.3288808 -4.3431273][-4.2668414 -4.2602277 -4.2448015 -4.2128158 -4.1840296 -4.1615667 -4.1418066 -4.1390653 -4.1654196 -4.2135196 -4.2613335 -4.2992749 -4.3182006 -4.3272929 -4.3413672][-4.2509522 -4.2529726 -4.2498941 -4.2242904 -4.1991506 -4.1719055 -4.1449542 -4.133337 -4.1555829 -4.2054482 -4.257597 -4.2985191 -4.3186784 -4.3282886 -4.3412828][-4.23389 -4.2434306 -4.2518721 -4.2366152 -4.2134118 -4.1766653 -4.1333733 -4.1131744 -4.1353893 -4.1917663 -4.2515392 -4.2970819 -4.3214512 -4.3317537 -4.3433819][-4.213419 -4.2293792 -4.24533 -4.2375407 -4.2161436 -4.1748276 -4.1174068 -4.0900259 -4.11589 -4.1826739 -4.2493005 -4.297637 -4.3247819 -4.3354545 -4.3458805][-4.1846051 -4.2105241 -4.2328587 -4.2322006 -4.2150612 -4.1695228 -4.1001706 -4.0696578 -4.1021996 -4.1798182 -4.2504177 -4.3004413 -4.3291163 -4.3391595 -4.3481283][-4.1598129 -4.1939135 -4.2198 -4.2238035 -4.2108502 -4.1629 -4.0897174 -4.0628023 -4.1025796 -4.1840677 -4.2552786 -4.3050723 -4.3335457 -4.3425555 -4.3501835][-4.1465721 -4.1823516 -4.210269 -4.2158761 -4.2067165 -4.1622186 -4.0985737 -4.0860877 -4.1279883 -4.1995063 -4.2631483 -4.3106546 -4.337739 -4.34512 -4.3515291][-4.1530666 -4.1825652 -4.2083225 -4.2172766 -4.2150426 -4.1802635 -4.1354628 -4.137536 -4.1703558 -4.2217288 -4.2726984 -4.3154435 -4.3410106 -4.3464265 -4.3518505][-4.1865449 -4.2084603 -4.2264581 -4.2334795 -4.2309723 -4.2018328 -4.173667 -4.1859245 -4.2075891 -4.2409778 -4.28366 -4.3200397 -4.342454 -4.3473077 -4.3520322][-4.2177405 -4.2321568 -4.240685 -4.2406125 -4.2332182 -4.2078586 -4.1917214 -4.2108121 -4.2279673 -4.2531486 -4.2921124 -4.323945 -4.343544 -4.3477612 -4.3521585][-4.2375579 -4.2525129 -4.2543855 -4.2455812 -4.2331839 -4.2105455 -4.1998091 -4.2209005 -4.2362094 -4.2584109 -4.2955446 -4.3256598 -4.3429155 -4.34636 -4.3511815]]...]
INFO - root - 2017-12-07 14:24:33.507772: step 19210, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 81h:06m:24s remains)
INFO - root - 2017-12-07 14:24:43.075920: step 19220, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 79h:08m:02s remains)
INFO - root - 2017-12-07 14:24:52.837894: step 19230, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 85h:57m:48s remains)
INFO - root - 2017-12-07 14:25:02.578810: step 19240, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 84h:32m:38s remains)
INFO - root - 2017-12-07 14:25:12.205432: step 19250, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.977 sec/batch; 85h:02m:18s remains)
INFO - root - 2017-12-07 14:25:21.733377: step 19260, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 80h:50m:52s remains)
INFO - root - 2017-12-07 14:25:31.350390: step 19270, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 80h:34m:54s remains)
INFO - root - 2017-12-07 14:25:40.921611: step 19280, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 81h:50m:38s remains)
INFO - root - 2017-12-07 14:25:50.466624: step 19290, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 85h:44m:29s remains)
INFO - root - 2017-12-07 14:26:00.022752: step 19300, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 85h:15m:11s remains)
2017-12-07 14:26:01.004161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3263378 -4.3177786 -4.3130941 -4.3074026 -4.2928815 -4.2653904 -4.2490211 -4.2600775 -4.2793307 -4.2865767 -4.2890773 -4.2933903 -4.2883005 -4.2662911 -4.2403126][-4.3302789 -4.3226871 -4.3146935 -4.3024187 -4.2820024 -4.2454844 -4.2244287 -4.2400727 -4.2707224 -4.2868757 -4.2936091 -4.2962756 -4.2833338 -4.2478185 -4.207108][-4.3326592 -4.325407 -4.3140984 -4.2970223 -4.2713733 -4.2280231 -4.2031431 -4.22262 -4.2600083 -4.2818837 -4.2949171 -4.3050237 -4.2904086 -4.2422853 -4.184339][-4.3316946 -4.3230166 -4.3092217 -4.2882891 -4.2587113 -4.2129626 -4.1846538 -4.2002339 -4.2358208 -4.2601309 -4.2817793 -4.305501 -4.3018246 -4.2532644 -4.1857696][-4.3269453 -4.3158007 -4.2995315 -4.2761855 -4.2408419 -4.1892042 -4.1467438 -4.147058 -4.1794033 -4.2109346 -4.2460656 -4.2893 -4.3059897 -4.2722282 -4.2086091][-4.3216262 -4.309536 -4.2921948 -4.2662745 -4.22394 -4.1596446 -4.0956378 -4.0778522 -4.1072068 -4.1486397 -4.1979718 -4.2582746 -4.2946911 -4.2802372 -4.232007][-4.3141618 -4.3034396 -4.2862706 -4.2570925 -4.2095804 -4.1355453 -4.052177 -4.0171924 -4.0445528 -4.0970755 -4.1580262 -4.2224069 -4.2627358 -4.2692904 -4.2471805][-4.2976818 -4.2837205 -4.2622786 -4.2309837 -4.1893964 -4.1246271 -4.0445824 -4.0044894 -4.0282865 -4.0812178 -4.1421094 -4.1938915 -4.2244177 -4.2382574 -4.239759][-4.2804394 -4.2612238 -4.2378616 -4.2084002 -4.179955 -4.1350975 -4.0798244 -4.0529761 -4.0821481 -4.1301122 -4.1774635 -4.2019815 -4.2100792 -4.21527 -4.2202482][-4.2728686 -4.2538652 -4.2345719 -4.2119508 -4.1930857 -4.1592526 -4.1212416 -4.1091352 -4.1475015 -4.191988 -4.2255445 -4.2298841 -4.2240062 -4.2188988 -4.2146053][-4.2713814 -4.2592239 -4.2496238 -4.2360058 -4.2213821 -4.1921358 -4.162786 -4.1572185 -4.1931496 -4.2350159 -4.2592378 -4.2556648 -4.2435656 -4.2354288 -4.2256212][-4.2813244 -4.2747393 -4.2763438 -4.2750921 -4.2686434 -4.2465315 -4.2238226 -4.2217469 -4.2485051 -4.2796268 -4.2921252 -4.2841082 -4.2696877 -4.2602019 -4.2500253][-4.298008 -4.2961783 -4.3054028 -4.3134556 -4.314363 -4.3009911 -4.2822671 -4.2779951 -4.2940516 -4.3107219 -4.3136797 -4.30328 -4.2899919 -4.2827535 -4.2751527][-4.3076458 -4.3057089 -4.3164482 -4.3292522 -4.3343081 -4.3280964 -4.3153777 -4.31059 -4.315558 -4.318584 -4.3145113 -4.3032007 -4.2905335 -4.2847209 -4.281117][-4.3067956 -4.3023868 -4.3101606 -4.3204789 -4.3250046 -4.3215885 -4.3135166 -4.3088808 -4.3087969 -4.3068419 -4.3015108 -4.2937379 -4.28585 -4.2818646 -4.2805123]]...]
INFO - root - 2017-12-07 14:26:10.600539: step 19310, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 81h:11m:32s remains)
INFO - root - 2017-12-07 14:26:20.251091: step 19320, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 82h:38m:27s remains)
INFO - root - 2017-12-07 14:26:29.820331: step 19330, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 83h:47m:50s remains)
INFO - root - 2017-12-07 14:26:39.681273: step 19340, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.995 sec/batch; 86h:34m:00s remains)
INFO - root - 2017-12-07 14:26:49.404201: step 19350, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 81h:40m:53s remains)
INFO - root - 2017-12-07 14:26:58.956801: step 19360, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 82h:46m:26s remains)
INFO - root - 2017-12-07 14:27:08.723043: step 19370, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 85h:16m:00s remains)
INFO - root - 2017-12-07 14:27:18.377600: step 19380, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 83h:43m:37s remains)
INFO - root - 2017-12-07 14:27:28.017479: step 19390, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 79h:40m:19s remains)
INFO - root - 2017-12-07 14:27:37.544346: step 19400, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 82h:06m:04s remains)
2017-12-07 14:27:38.613691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23622 -4.242969 -4.2608571 -4.2709608 -4.2593131 -4.2479715 -4.240315 -4.2330651 -4.2279363 -4.2317238 -4.2276163 -4.2138548 -4.2120008 -4.2154455 -4.2187977][-4.2208076 -4.2331386 -4.2545948 -4.2676044 -4.2596669 -4.2502022 -4.2421412 -4.2383513 -4.23707 -4.2398086 -4.23554 -4.225112 -4.2211585 -4.2162547 -4.2122][-4.2034903 -4.2262974 -4.2548108 -4.2695055 -4.2595129 -4.2437239 -4.2297173 -4.2267628 -4.231647 -4.2376304 -4.2384081 -4.23575 -4.2305412 -4.2154388 -4.1982784][-4.1847715 -4.2145815 -4.2507048 -4.2690949 -4.2616467 -4.2434392 -4.2219629 -4.2137995 -4.2201147 -4.2282357 -4.2375922 -4.2502732 -4.2516947 -4.235496 -4.2070432][-4.1930618 -4.2139473 -4.2415152 -4.2530661 -4.2443829 -4.2243876 -4.2021708 -4.1943784 -4.2015109 -4.2098951 -4.2277045 -4.2563529 -4.2718134 -4.2673659 -4.2438636][-4.2287321 -4.2327981 -4.2389531 -4.2341585 -4.2151814 -4.1868243 -4.1632719 -4.1615644 -4.1733775 -4.1850438 -4.2098088 -4.2479234 -4.2746739 -4.2853942 -4.2779756][-4.2563624 -4.2476087 -4.2332077 -4.2145691 -4.185935 -4.1479249 -4.1183314 -4.118494 -4.1339874 -4.1512213 -4.182838 -4.2273989 -4.2650552 -4.2919326 -4.3015223][-4.2656465 -4.2501321 -4.2264485 -4.1972589 -4.1573377 -4.1077337 -4.0642977 -4.0608511 -4.0820575 -4.1108546 -4.1497474 -4.19763 -4.2451019 -4.2837715 -4.3048549][-4.2604032 -4.2418723 -4.2196159 -4.1910868 -4.1445332 -4.0833282 -4.021172 -4.0001025 -4.0195742 -4.0604177 -4.1122308 -4.16667 -4.2188354 -4.2620573 -4.2877145][-4.2645535 -4.2442336 -4.2266493 -4.2060127 -4.1664405 -4.1097093 -4.0458159 -4.00905 -4.0165114 -4.0562463 -4.1094112 -4.1642776 -4.2155447 -4.25483 -4.2774086][-4.2883868 -4.2663746 -4.2537947 -4.2440572 -4.2191997 -4.1811304 -4.138629 -4.1095829 -4.1059637 -4.1301322 -4.1687083 -4.2126131 -4.2542524 -4.2836037 -4.2974544][-4.3131881 -4.2944918 -4.2861867 -4.2844133 -4.2724795 -4.2530732 -4.2317624 -4.21679 -4.2137852 -4.2260118 -4.2479653 -4.2764821 -4.3046813 -4.321907 -4.3266206][-4.3286595 -4.3153334 -4.3099065 -4.3110113 -4.305131 -4.2956009 -4.287302 -4.2829638 -4.2831059 -4.2894063 -4.300436 -4.3160419 -4.3317666 -4.3405962 -4.3426056][-4.3380303 -4.3298268 -4.3259883 -4.3267031 -4.3236322 -4.318244 -4.3143029 -4.3140516 -4.3151817 -4.3173013 -4.3217053 -4.3301268 -4.3391037 -4.3441238 -4.3461251][-4.3429961 -4.3396688 -4.3372493 -4.3359075 -4.333427 -4.330482 -4.328239 -4.3279114 -4.3282685 -4.3288713 -4.3309784 -4.3359671 -4.3415108 -4.3451085 -4.3472881]]...]
INFO - root - 2017-12-07 14:27:48.184509: step 19410, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 85h:46m:52s remains)
INFO - root - 2017-12-07 14:27:57.827307: step 19420, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 84h:26m:52s remains)
INFO - root - 2017-12-07 14:28:07.410045: step 19430, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 82h:10m:13s remains)
INFO - root - 2017-12-07 14:28:17.143109: step 19440, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 81h:58m:46s remains)
INFO - root - 2017-12-07 14:28:26.651462: step 19450, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 84h:28m:55s remains)
INFO - root - 2017-12-07 14:28:36.211616: step 19460, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 82h:45m:41s remains)
INFO - root - 2017-12-07 14:28:45.806145: step 19470, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 87h:05m:19s remains)
INFO - root - 2017-12-07 14:28:55.219937: step 19480, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 82h:02m:36s remains)
INFO - root - 2017-12-07 14:29:04.876772: step 19490, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 81h:51m:07s remains)
INFO - root - 2017-12-07 14:29:14.585517: step 19500, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 85h:52m:51s remains)
2017-12-07 14:29:15.534196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18831 -4.1841049 -4.1802392 -4.1710181 -4.1648765 -4.1695824 -4.1846142 -4.19828 -4.2085228 -4.202795 -4.1817927 -4.1589589 -4.1522775 -4.1512847 -4.1498384][-4.1956878 -4.1950808 -4.1956944 -4.1923747 -4.1887131 -4.1926918 -4.2050014 -4.2143865 -4.217207 -4.2101607 -4.1921391 -4.1728249 -4.1681147 -4.1664963 -4.1612763][-4.2199016 -4.2258229 -4.2319622 -4.2338886 -4.2300811 -4.2309251 -4.2354946 -4.2328768 -4.2241178 -4.2162175 -4.2033505 -4.1887426 -4.1805329 -4.1738486 -4.1629124][-4.2525024 -4.2648211 -4.27331 -4.2747936 -4.2671714 -4.2610064 -4.2539029 -4.2364922 -4.2167792 -4.2065787 -4.20091 -4.1930757 -4.1819582 -4.167428 -4.1485839][-4.2689633 -4.2833333 -4.2899609 -4.2893496 -4.2783275 -4.2628016 -4.2420721 -4.2128334 -4.1880274 -4.1786132 -4.18034 -4.1801639 -4.1704206 -4.1504636 -4.1287894][-4.2624197 -4.2724338 -4.2743993 -4.2703705 -4.2565775 -4.2321873 -4.198102 -4.16026 -4.13817 -4.1367545 -4.1495943 -4.161417 -4.1595783 -4.1423316 -4.1269946][-4.2398052 -4.2417259 -4.2350726 -4.2232704 -4.2033963 -4.1727848 -4.1320434 -4.0943489 -4.0827942 -4.1004467 -4.1306806 -4.1554012 -4.1645617 -4.1580582 -4.1500444][-4.2060342 -4.1948271 -4.1785984 -4.1595931 -4.1383753 -4.1126804 -4.0809622 -4.0555677 -4.05921 -4.0911994 -4.1310811 -4.16089 -4.1724691 -4.1732736 -4.1714649][-4.1667242 -4.1405611 -4.1188245 -4.102849 -4.0932112 -4.08486 -4.0739865 -4.0665145 -4.0780687 -4.1088572 -4.1423483 -4.1638727 -4.167767 -4.1682091 -4.1721683][-4.1253476 -4.0917854 -4.0760589 -4.0744891 -4.0832796 -4.0934324 -4.0970883 -4.1016417 -4.1159687 -4.13825 -4.1591558 -4.1661682 -4.1583886 -4.1560068 -4.1634231][-4.1035185 -4.0778232 -4.0761805 -4.0877304 -4.1056709 -4.1198835 -4.1238346 -4.1266227 -4.1381383 -4.155654 -4.1707048 -4.17141 -4.159924 -4.1557732 -4.1616726][-4.1146121 -4.1013412 -4.1054893 -4.1173854 -4.12991 -4.1390676 -4.1415849 -4.1417503 -4.1497221 -4.1665268 -4.18083 -4.1824226 -4.1740441 -4.1714926 -4.1746855][-4.1490126 -4.1418753 -4.1399646 -4.1418982 -4.1453056 -4.1510053 -4.1569557 -4.1596937 -4.1668935 -4.1806726 -4.1936502 -4.1961226 -4.1911054 -4.1896977 -4.1902204][-4.1785531 -4.169817 -4.1572576 -4.1493936 -4.148632 -4.1552534 -4.1647949 -4.1720915 -4.1797891 -4.1903777 -4.2008839 -4.2025971 -4.1992679 -4.1989832 -4.1978769][-4.1879764 -4.1742587 -4.1523747 -4.1417961 -4.1454716 -4.1556606 -4.1666183 -4.1746888 -4.1811194 -4.1880937 -4.194901 -4.1947713 -4.1930385 -4.1946087 -4.192265]]...]
INFO - root - 2017-12-07 14:29:25.102826: step 19510, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 82h:31m:30s remains)
INFO - root - 2017-12-07 14:29:34.683599: step 19520, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 83h:48m:03s remains)
INFO - root - 2017-12-07 14:29:44.262108: step 19530, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 85h:48m:14s remains)
INFO - root - 2017-12-07 14:29:53.838193: step 19540, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 85h:42m:55s remains)
INFO - root - 2017-12-07 14:30:03.509436: step 19550, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 87h:16m:06s remains)
INFO - root - 2017-12-07 14:30:13.273835: step 19560, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 82h:23m:40s remains)
INFO - root - 2017-12-07 14:30:22.865740: step 19570, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 82h:50m:39s remains)
INFO - root - 2017-12-07 14:30:32.555016: step 19580, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 82h:48m:18s remains)
INFO - root - 2017-12-07 14:30:42.017737: step 19590, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 75h:36m:39s remains)
INFO - root - 2017-12-07 14:30:51.600960: step 19600, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 76h:01m:51s remains)
2017-12-07 14:30:52.556251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.232141 -4.2418613 -4.2429342 -4.2393308 -4.2454567 -4.2578864 -4.2756262 -4.2803521 -4.2792077 -4.2764387 -4.272481 -4.2720823 -4.2698011 -4.2644234 -4.2538567][-4.2109075 -4.2324624 -4.247407 -4.2533221 -4.264029 -4.2766342 -4.288497 -4.2833457 -4.2737417 -4.2729959 -4.273674 -4.2709427 -4.2627044 -4.2492776 -4.2355433][-4.2031507 -4.2312369 -4.2514954 -4.2576227 -4.2636204 -4.2693014 -4.2682657 -4.2554293 -4.2466793 -4.2540007 -4.2619476 -4.2585483 -4.2476454 -4.2298012 -4.2130337][-4.2167096 -4.2382131 -4.2501636 -4.2510457 -4.2490005 -4.2452145 -4.2345819 -4.2200155 -4.2151 -4.2305107 -4.2474732 -4.2460256 -4.2339711 -4.212801 -4.1947155][-4.2293673 -4.2366309 -4.2347941 -4.2256155 -4.2157397 -4.2070127 -4.1974611 -4.1906857 -4.1910219 -4.2095928 -4.2310615 -4.2300153 -4.2191386 -4.2015157 -4.1877394][-4.2267919 -4.2196307 -4.2044 -4.1852622 -4.1704645 -4.1603103 -4.1597047 -4.167439 -4.178031 -4.19795 -4.2191181 -4.2186956 -4.2080531 -4.1903219 -4.1759319][-4.2212086 -4.2031112 -4.1830354 -4.1591034 -4.1385741 -4.1233439 -4.1279478 -4.1451116 -4.1656928 -4.1941929 -4.2161126 -4.2130938 -4.1942697 -4.1699371 -4.1489544][-4.22483 -4.2044158 -4.1858087 -4.1629481 -4.1425276 -4.1264615 -4.1310906 -4.1495929 -4.17003 -4.2017212 -4.2212391 -4.2123413 -4.1846337 -4.1562338 -4.1339669][-4.239933 -4.224472 -4.2092123 -4.1889806 -4.1720009 -4.1618972 -4.1681623 -4.1821003 -4.194335 -4.2176433 -4.2281408 -4.2142358 -4.183403 -4.157928 -4.1399078][-4.2523866 -4.2448692 -4.2346139 -4.2178254 -4.2036028 -4.1963496 -4.1995306 -4.2092032 -4.2175241 -4.2293644 -4.227859 -4.21017 -4.1816697 -4.163806 -4.1523929][-4.2566476 -4.259409 -4.2584763 -4.2497044 -4.2411823 -4.2350287 -4.2326412 -4.2363415 -4.2383618 -4.23682 -4.2239776 -4.203259 -4.1792555 -4.1708617 -4.1703067][-4.2596207 -4.2683043 -4.2735457 -4.2735348 -4.2716303 -4.2670503 -4.2601175 -4.2580242 -4.251327 -4.2390947 -4.2193475 -4.199429 -4.1824965 -4.1808591 -4.1896191][-4.262085 -4.27033 -4.2742934 -4.2795262 -4.2846069 -4.2834287 -4.2758956 -4.2700396 -4.2597795 -4.2442174 -4.2254863 -4.2103324 -4.2004156 -4.2037191 -4.2162962][-4.25082 -4.2573109 -4.2583456 -4.264492 -4.2740359 -4.28009 -4.278542 -4.27553 -4.2667885 -4.2541656 -4.2406154 -4.2348671 -4.2368312 -4.2465353 -4.2606826][-4.2150393 -4.2138095 -4.210063 -4.2172127 -4.2344823 -4.2539244 -4.26443 -4.2675638 -4.2622309 -4.2533722 -4.2463965 -4.2515984 -4.26813 -4.2855568 -4.2985039]]...]
INFO - root - 2017-12-07 14:31:02.276252: step 19610, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 83h:48m:40s remains)
INFO - root - 2017-12-07 14:31:11.847545: step 19620, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 81h:42m:22s remains)
INFO - root - 2017-12-07 14:31:21.433253: step 19630, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 84h:21m:09s remains)
INFO - root - 2017-12-07 14:31:30.937940: step 19640, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 84h:20m:06s remains)
INFO - root - 2017-12-07 14:31:40.513556: step 19650, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 83h:06m:45s remains)
INFO - root - 2017-12-07 14:31:50.215069: step 19660, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.996 sec/batch; 86h:30m:47s remains)
INFO - root - 2017-12-07 14:31:59.612854: step 19670, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.898 sec/batch; 78h:01m:42s remains)
INFO - root - 2017-12-07 14:32:09.344434: step 19680, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.016 sec/batch; 88h:16m:49s remains)
INFO - root - 2017-12-07 14:32:18.947669: step 19690, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 87h:53m:50s remains)
INFO - root - 2017-12-07 14:32:28.631132: step 19700, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 82h:50m:33s remains)
2017-12-07 14:32:29.596609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22569 -4.2126083 -4.2078795 -4.2024126 -4.1951571 -4.1899748 -4.1968327 -4.2191997 -4.2465591 -4.2683868 -4.2777443 -4.2818904 -4.2793527 -4.2772503 -4.2729216][-4.18743 -4.178545 -4.1814904 -4.1871009 -4.1921687 -4.2031779 -4.2174191 -4.2359037 -4.2538443 -4.2678032 -4.2717066 -4.2730021 -4.2655182 -4.256186 -4.2481666][-4.1314034 -4.1311536 -4.1391711 -4.1530666 -4.1692858 -4.1909747 -4.2175107 -4.244463 -4.2653356 -4.2747307 -4.2710185 -4.2611394 -4.2412267 -4.2240562 -4.2116766][-4.0964537 -4.1037626 -4.1125393 -4.1223106 -4.1334615 -4.1508079 -4.1882973 -4.2334132 -4.2658892 -4.2785525 -4.2740431 -4.2554164 -4.224236 -4.1978879 -4.1814241][-4.0920563 -4.1010108 -4.105413 -4.0968623 -4.0803885 -4.0781593 -4.1206489 -4.1844654 -4.23445 -4.2621193 -4.2684827 -4.2511091 -4.2142072 -4.1796808 -4.1568823][-4.1157317 -4.1108589 -4.1032171 -4.0658412 -4.0050521 -3.9667106 -4.0058022 -4.0875263 -4.15976 -4.2121034 -4.2404809 -4.2353768 -4.2014647 -4.1682682 -4.14766][-4.1417751 -4.11258 -4.0844622 -4.0194635 -3.9101346 -3.8169661 -3.8328979 -3.9287574 -4.0232892 -4.1083336 -4.1711631 -4.1929493 -4.1781645 -4.1648307 -4.1584969][-4.1636372 -4.1130214 -4.0650969 -3.9838328 -3.843734 -3.7049706 -3.6837578 -3.7761147 -3.8755009 -3.9848833 -4.0832176 -4.1383286 -4.1518059 -4.1679287 -4.1848636][-4.1953197 -4.1471262 -4.1017056 -4.0320811 -3.9154787 -3.7997551 -3.7645063 -3.8083076 -3.8636975 -3.95285 -4.0540147 -4.1204944 -4.1511841 -4.1844392 -4.2153211][-4.2548332 -4.2249665 -4.1889477 -4.1350174 -4.0541248 -3.9796159 -3.9505446 -3.9562688 -3.969311 -4.0204763 -4.0983639 -4.1550131 -4.1858239 -4.2177229 -4.2465091][-4.3103609 -4.3017087 -4.2812304 -4.2407618 -4.1817923 -4.1315365 -4.1088285 -4.099678 -4.0949187 -4.121419 -4.1748815 -4.2177725 -4.2395449 -4.2626033 -4.2808914][-4.3351064 -4.3440948 -4.3383603 -4.3114057 -4.2699957 -4.2352219 -4.2167115 -4.2045517 -4.1979432 -4.2112508 -4.2442021 -4.2718587 -4.2820349 -4.2935514 -4.30422][-4.3236485 -4.3456335 -4.3550749 -4.3461175 -4.3234329 -4.3024659 -4.2906132 -4.2803288 -4.2716494 -4.274878 -4.2904792 -4.3009987 -4.2992597 -4.300457 -4.3044124][-4.2992187 -4.3241959 -4.3406115 -4.3444009 -4.3363471 -4.3269925 -4.3201675 -4.3126035 -4.3053479 -4.300106 -4.3021836 -4.3008814 -4.2918477 -4.2861862 -4.2859726][-4.2867761 -4.3070621 -4.3201804 -4.3255186 -4.32168 -4.3166833 -4.3117757 -4.3069367 -4.3021545 -4.29544 -4.2923503 -4.2882242 -4.2796359 -4.2719522 -4.269227]]...]
INFO - root - 2017-12-07 14:32:39.299384: step 19710, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 86h:33m:38s remains)
INFO - root - 2017-12-07 14:32:48.974564: step 19720, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.034 sec/batch; 89h:52m:07s remains)
INFO - root - 2017-12-07 14:32:58.535333: step 19730, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.013 sec/batch; 88h:00m:15s remains)
INFO - root - 2017-12-07 14:33:08.110463: step 19740, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 82h:06m:41s remains)
INFO - root - 2017-12-07 14:33:17.912116: step 19750, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 84h:43m:29s remains)
INFO - root - 2017-12-07 14:33:27.556525: step 19760, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.982 sec/batch; 85h:16m:14s remains)
INFO - root - 2017-12-07 14:33:37.203448: step 19770, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 79h:04m:11s remains)
INFO - root - 2017-12-07 14:33:46.938184: step 19780, loss = 2.13, batch loss = 2.07 (8.4 examples/sec; 0.955 sec/batch; 82h:56m:50s remains)
INFO - root - 2017-12-07 14:33:56.514804: step 19790, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 81h:39m:20s remains)
INFO - root - 2017-12-07 14:34:06.172664: step 19800, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.935 sec/batch; 81h:14m:33s remains)
2017-12-07 14:34:07.102393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2032 -4.2053051 -4.2272472 -4.2373581 -4.2410645 -4.2443018 -4.250083 -4.2633219 -4.2801633 -4.2911439 -4.2873917 -4.2613616 -4.2228813 -4.1916227 -4.1882124][-4.2019124 -4.2008491 -4.2243104 -4.2363071 -4.2408938 -4.245841 -4.25058 -4.2570195 -4.2713742 -4.2815757 -4.2774744 -4.2516289 -4.2082639 -4.1643362 -4.1509838][-4.219265 -4.2153397 -4.2331953 -4.2420411 -4.2445245 -4.2478518 -4.2483878 -4.2424951 -4.2467623 -4.253077 -4.2484741 -4.2268896 -4.1796684 -4.1212454 -4.0973616][-4.2595911 -4.25649 -4.2625151 -4.2607174 -4.2542353 -4.2487445 -4.2396936 -4.215971 -4.20517 -4.2113895 -4.2184186 -4.210259 -4.163672 -4.0957608 -4.0643044][-4.2957754 -4.2903543 -4.2823739 -4.2673244 -4.2513542 -4.2369971 -4.2199669 -4.1764288 -4.1423244 -4.1474371 -4.1750298 -4.1916552 -4.1592507 -4.0920186 -4.0524611][-4.314115 -4.3015809 -4.2793894 -4.2521877 -4.2272434 -4.2094474 -4.188992 -4.13081 -4.0703764 -4.0656857 -4.1143789 -4.1620984 -4.1574283 -4.1044636 -4.0578628][-4.3206244 -4.3039427 -4.2718978 -4.2347693 -4.2037125 -4.1846228 -4.159687 -4.0942221 -4.017221 -3.9992874 -4.0626931 -4.1359448 -4.1577954 -4.1230087 -4.0727181][-4.3206925 -4.306613 -4.2712035 -4.2289152 -4.1914759 -4.1673179 -4.1382127 -4.0733314 -3.9981234 -3.975462 -4.0428839 -4.1292286 -4.1640148 -4.1417017 -4.0896578][-4.3152332 -4.3048692 -4.2746186 -4.23615 -4.1968436 -4.1630592 -4.1275926 -4.0658684 -4.0017681 -3.981487 -4.0411448 -4.131989 -4.1791797 -4.1649694 -4.1122241][-4.3145943 -4.3046346 -4.2783732 -4.2458396 -4.2082562 -4.1667829 -4.1243706 -4.0703917 -4.022614 -4.0082145 -4.0568237 -4.1418223 -4.196105 -4.189374 -4.1407666][-4.3154063 -4.3086414 -4.2843313 -4.2572145 -4.2241063 -4.1808047 -4.1373711 -4.0950327 -4.0637293 -4.056036 -4.0953355 -4.1650114 -4.2135282 -4.2060332 -4.1600881][-4.3069286 -4.3088112 -4.2926335 -4.2728267 -4.2474952 -4.2079239 -4.1651163 -4.1280322 -4.1085515 -4.1123505 -4.1483645 -4.2029305 -4.2381024 -4.2229042 -4.1739936][-4.2901917 -4.29928 -4.2921062 -4.2812519 -4.2669668 -4.2372189 -4.2007494 -4.1699886 -4.1568556 -4.1675186 -4.1997256 -4.2420406 -4.2666464 -4.2411327 -4.1856375][-4.27712 -4.2909379 -4.2898936 -4.2892671 -4.2855773 -4.265872 -4.2390518 -4.2148018 -4.2037997 -4.2126031 -4.2362866 -4.2656937 -4.2797608 -4.2445483 -4.1846433][-4.2692633 -4.2849355 -4.2867632 -4.2921715 -4.2976322 -4.2887554 -4.2734632 -4.2576423 -4.2469387 -4.2482963 -4.2594657 -4.2747369 -4.2789321 -4.2375903 -4.1738491]]...]
INFO - root - 2017-12-07 14:34:16.831290: step 19810, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 81h:58m:33s remains)
INFO - root - 2017-12-07 14:34:26.415945: step 19820, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 83h:24m:01s remains)
INFO - root - 2017-12-07 14:34:36.021669: step 19830, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 78h:52m:20s remains)
INFO - root - 2017-12-07 14:34:45.542374: step 19840, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 82h:45m:25s remains)
INFO - root - 2017-12-07 14:34:55.276797: step 19850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.970 sec/batch; 84h:11m:59s remains)
INFO - root - 2017-12-07 14:35:04.998593: step 19860, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 84h:23m:32s remains)
INFO - root - 2017-12-07 14:35:14.755159: step 19870, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 86h:58m:43s remains)
INFO - root - 2017-12-07 14:35:24.052621: step 19880, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 80h:59m:20s remains)
INFO - root - 2017-12-07 14:35:33.646223: step 19890, loss = 2.11, batch loss = 2.06 (8.2 examples/sec; 0.981 sec/batch; 85h:10m:08s remains)
INFO - root - 2017-12-07 14:35:43.407599: step 19900, loss = 2.10, batch loss = 2.05 (8.0 examples/sec; 0.995 sec/batch; 86h:26m:06s remains)
2017-12-07 14:35:44.428917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.192802 -4.1340561 -4.1081276 -4.1187267 -4.1508751 -4.183176 -4.1999397 -4.2031846 -4.2004147 -4.2110982 -4.2222939 -4.22411 -4.2175331 -4.2088256 -4.2046642][-4.1899304 -4.1398053 -4.1316991 -4.1462431 -4.1687145 -4.1864033 -4.1949086 -4.1977291 -4.1932936 -4.1986861 -4.2082672 -4.2158756 -4.2172842 -4.2147264 -4.2180791][-4.2006283 -4.1617994 -4.1601353 -4.1698675 -4.1831813 -4.1945229 -4.1990528 -4.1968632 -4.1913853 -4.1949515 -4.1983333 -4.2013483 -4.2019162 -4.2022786 -4.2109456][-4.2148833 -4.1861553 -4.1837091 -4.1862154 -4.1931825 -4.2015643 -4.2016482 -4.1887622 -4.1727252 -4.1744103 -4.1697669 -4.161499 -4.1560068 -4.159358 -4.172626][-4.2345915 -4.2116265 -4.2039828 -4.2008052 -4.2003064 -4.1977787 -4.1874337 -4.1661987 -4.1404543 -4.1327076 -4.1182547 -4.0990944 -4.0918622 -4.1049695 -4.1256185][-4.241859 -4.2173009 -4.2052703 -4.1989627 -4.1890631 -4.170908 -4.1491847 -4.1251163 -4.0958562 -4.0782561 -4.057827 -4.0385818 -4.03776 -4.0591078 -4.0823774][-4.2322984 -4.206028 -4.1923647 -4.1844273 -4.163435 -4.1312943 -4.0982842 -4.06848 -4.0413322 -4.0237679 -4.0145583 -4.0120139 -4.0243936 -4.0465965 -4.0651031][-4.2212529 -4.1932769 -4.1726952 -4.155355 -4.1246495 -4.0864406 -4.0452881 -4.01372 -4.0009384 -4.0003238 -4.011735 -4.0284357 -4.0505953 -4.0682445 -4.0798855][-4.2110806 -4.1749105 -4.1454568 -4.1222486 -4.091857 -4.0575709 -4.01937 -4.00054 -4.0081892 -4.0282807 -4.0529556 -4.07372 -4.092423 -4.1028996 -4.1087117][-4.1880836 -4.1466961 -4.1184826 -4.1024776 -4.0846982 -4.0658512 -4.0446134 -4.0445743 -4.0593114 -4.0810518 -4.1015 -4.112608 -4.118865 -4.1214385 -4.1240053][-4.160882 -4.1215 -4.1082711 -4.1081696 -4.103766 -4.0993686 -4.0926404 -4.1006284 -4.108726 -4.1173458 -4.124167 -4.1251831 -4.1238422 -4.11986 -4.1186624][-4.1501126 -4.1215982 -4.1229539 -4.1342764 -4.1360703 -4.1358838 -4.1322913 -4.1421337 -4.1432691 -4.137392 -4.1316385 -4.1250625 -4.1192255 -4.1134725 -4.1133423][-4.1715341 -4.1536646 -4.1627517 -4.1790552 -4.184494 -4.18162 -4.1736689 -4.1805658 -4.1801386 -4.1699996 -4.1595178 -4.1489382 -4.1416616 -4.138339 -4.139236][-4.2186818 -4.2114873 -4.220933 -4.2330489 -4.2381783 -4.2348843 -4.2260027 -4.2285275 -4.2292008 -4.22087 -4.2125368 -4.2011142 -4.1929264 -4.1904483 -4.190434][-4.2687464 -4.2687292 -4.2744055 -4.2804222 -4.2829313 -4.2802305 -4.2726164 -4.273119 -4.2733464 -4.2685437 -4.2629328 -4.2546897 -4.2492657 -4.2472897 -4.2472715]]...]
INFO - root - 2017-12-07 14:35:54.022758: step 19910, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 79h:56m:40s remains)
INFO - root - 2017-12-07 14:36:03.507435: step 19920, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 78h:33m:05s remains)
INFO - root - 2017-12-07 14:36:13.113370: step 19930, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 79h:53m:55s remains)
INFO - root - 2017-12-07 14:36:22.610363: step 19940, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 82h:54m:35s remains)
INFO - root - 2017-12-07 14:36:32.234800: step 19950, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 85h:56m:26s remains)
INFO - root - 2017-12-07 14:36:41.838987: step 19960, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 84h:55m:55s remains)
INFO - root - 2017-12-07 14:36:51.486451: step 19970, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 82h:24m:07s remains)
INFO - root - 2017-12-07 14:37:01.177632: step 19980, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 83h:56m:18s remains)
INFO - root - 2017-12-07 14:37:10.761786: step 19990, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 82h:20m:32s remains)
INFO - root - 2017-12-07 14:37:20.373536: step 20000, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 1.001 sec/batch; 86h:52m:28s remains)
2017-12-07 14:37:21.287065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3140507 -4.3099003 -4.311904 -4.31812 -4.3232455 -4.3266668 -4.3246503 -4.320806 -4.3169112 -4.3109465 -4.3016453 -4.2968593 -4.3017607 -4.309711 -4.3182554][-4.2970405 -4.288136 -4.2883182 -4.2965755 -4.3068633 -4.3133559 -4.3083515 -4.3010244 -4.2956047 -4.2861819 -4.2723894 -4.2656832 -4.2726092 -4.285275 -4.2976918][-4.2698803 -4.2568655 -4.2543869 -4.2632012 -4.2774839 -4.2862673 -4.27674 -4.264101 -4.2561927 -4.2428989 -4.2281241 -4.2232494 -4.2335534 -4.2519274 -4.2700248][-4.238977 -4.223155 -4.2181544 -4.2240748 -4.233602 -4.2392154 -4.2174053 -4.1927037 -4.18333 -4.1740465 -4.1676388 -4.1706305 -4.1885672 -4.2173815 -4.2450418][-4.2179108 -4.1997662 -4.1915884 -4.192368 -4.1885133 -4.1810522 -4.1406374 -4.0990133 -4.0937085 -4.1006083 -4.1096144 -4.1262655 -4.15657 -4.1955118 -4.230454][-4.1984277 -4.1807857 -4.1719608 -4.1600041 -4.1326613 -4.1053281 -4.0378628 -3.9620931 -3.9611197 -3.9975624 -4.0274205 -4.0660043 -4.1180754 -4.1689997 -4.2138143][-4.1860185 -4.1641984 -4.1478515 -4.1145573 -4.0561829 -4.0029583 -3.8910563 -3.7615585 -3.7840269 -3.8757441 -3.9269567 -3.9884815 -4.0699749 -4.1360006 -4.1938491][-4.1964459 -4.1673613 -4.1373081 -4.0757408 -3.9872196 -3.9109056 -3.7519023 -3.564373 -3.6322868 -3.7840729 -3.8471122 -3.9247079 -4.0327435 -4.1091714 -4.1774178][-4.2203937 -4.189332 -4.1534696 -4.0878005 -4.0025716 -3.9374554 -3.7980578 -3.6340857 -3.7032149 -3.8387172 -3.8808014 -3.9390125 -4.041677 -4.111495 -4.1764345][-4.2507634 -4.2248468 -4.1962132 -4.14465 -4.0854964 -4.0527148 -3.9656947 -3.8610187 -3.8947182 -3.9772255 -3.9942393 -4.0241847 -4.0967755 -4.1479487 -4.1993132][-4.2764611 -4.2560573 -4.2387352 -4.2056723 -4.170403 -4.1611638 -4.1175127 -4.058176 -4.0647516 -4.10577 -4.1141205 -4.1262922 -4.1695585 -4.2000337 -4.234416][-4.2953806 -4.2818727 -4.2725382 -4.2548714 -4.2377777 -4.2428703 -4.2276492 -4.19251 -4.1771059 -4.1920958 -4.1965551 -4.1997128 -4.2231507 -4.2412944 -4.2648716][-4.3075047 -4.3012161 -4.2947631 -4.2855215 -4.27751 -4.2839661 -4.2804732 -4.2607532 -4.2392955 -4.2394729 -4.2424064 -4.2423997 -4.2517486 -4.2624974 -4.2806931][-4.3072371 -4.3022 -4.2934608 -4.2853518 -4.2788262 -4.2834496 -4.2844982 -4.2767382 -4.2643685 -4.2622275 -4.2645888 -4.2631593 -4.2677045 -4.2756538 -4.2898159][-4.3111215 -4.3056836 -4.2960505 -4.2871518 -4.2810974 -4.2827587 -4.2844539 -4.2851968 -4.2811632 -4.28009 -4.2814789 -4.2821708 -4.2874022 -4.2931027 -4.3030434]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 14:37:31.810848: step 20010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 78h:41m:57s remains)
INFO - root - 2017-12-07 14:37:41.652924: step 20020, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 84h:38m:58s remains)
INFO - root - 2017-12-07 14:37:51.198454: step 20030, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 84h:53m:45s remains)
INFO - root - 2017-12-07 14:38:00.860800: step 20040, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 85h:52m:59s remains)
INFO - root - 2017-12-07 14:38:10.644926: step 20050, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 84h:17m:31s remains)
INFO - root - 2017-12-07 14:38:20.253124: step 20060, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 85h:24m:43s remains)
INFO - root - 2017-12-07 14:38:29.909532: step 20070, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 84h:51m:34s remains)
INFO - root - 2017-12-07 14:38:39.615200: step 20080, loss = 2.09, batch loss = 2.04 (7.8 examples/sec; 1.029 sec/batch; 89h:20m:18s remains)
INFO - root - 2017-12-07 14:38:49.295974: step 20090, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 85h:45m:27s remains)
INFO - root - 2017-12-07 14:38:58.646750: step 20100, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 83h:37m:24s remains)
2017-12-07 14:38:59.650102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2936234 -4.2755103 -4.2725267 -4.2764854 -4.2765007 -4.2737932 -4.2744884 -4.2736936 -4.2681274 -4.266799 -4.2850823 -4.3105464 -4.3281531 -4.336031 -4.3347826][-4.2689872 -4.2398858 -4.23647 -4.2449174 -4.2461066 -4.2419863 -4.2435231 -4.2451391 -4.2399797 -4.2411747 -4.2652664 -4.2968321 -4.3183556 -4.3268805 -4.3247733][-4.2501059 -4.2153158 -4.2071023 -4.2112308 -4.2096438 -4.2046556 -4.2088189 -4.2196016 -4.2209506 -4.2267404 -4.2527909 -4.28809 -4.3104033 -4.3180742 -4.3162184][-4.2327042 -4.1989441 -4.1903434 -4.1889019 -4.1815825 -4.1743722 -4.1798997 -4.1992645 -4.2103896 -4.2228317 -4.2524304 -4.2883992 -4.3065934 -4.3111167 -4.308877][-4.2088752 -4.1770525 -4.1727896 -4.1738472 -4.1646385 -4.1520748 -4.1504254 -4.1688046 -4.1889429 -4.2097306 -4.2417693 -4.2771297 -4.2951875 -4.3009052 -4.3007631][-4.2081342 -4.1754889 -4.1671157 -4.1679173 -4.1584988 -4.1368976 -4.1214356 -4.137466 -4.1682925 -4.1954179 -4.2253823 -4.2579055 -4.2766209 -4.2849755 -4.2882819][-4.2086239 -4.1722221 -4.1562824 -4.1598563 -4.1532702 -4.12625 -4.09276 -4.1053014 -4.1470327 -4.184123 -4.21572 -4.2476439 -4.2679181 -4.2776175 -4.2794595][-4.1934032 -4.160502 -4.1459112 -4.1539168 -4.1517725 -4.1308613 -4.0916677 -4.0977011 -4.1464643 -4.1890845 -4.2184024 -4.2483954 -4.2710028 -4.2808833 -4.28146][-4.1785769 -4.150219 -4.1353726 -4.143311 -4.1522946 -4.1479092 -4.117671 -4.1262712 -4.1783695 -4.2204142 -4.2406497 -4.261055 -4.2815232 -4.2887392 -4.2882171][-4.1792388 -4.1498423 -4.1291857 -4.1334844 -4.1513886 -4.1590466 -4.1360168 -4.1445036 -4.1963391 -4.240191 -4.2591 -4.2753286 -4.2938671 -4.2984605 -4.2965422][-4.2013359 -4.1698203 -4.1414833 -4.138514 -4.1555223 -4.1601334 -4.1372995 -4.145215 -4.1945806 -4.2393303 -4.2663722 -4.28543 -4.304482 -4.3118725 -4.3116684][-4.2334623 -4.2073374 -4.1836057 -4.1794186 -4.1897769 -4.1872487 -4.158967 -4.1553769 -4.196012 -4.2410645 -4.2774491 -4.3002567 -4.3203416 -4.3299408 -4.3285131][-4.2447867 -4.2259541 -4.2152724 -4.2185059 -4.2267385 -4.2181125 -4.188549 -4.1797123 -4.2113466 -4.2507524 -4.2883091 -4.3140726 -4.3327451 -4.3383937 -4.3350453][-4.2460012 -4.2348652 -4.2342081 -4.2426214 -4.2545309 -4.2493868 -4.2246017 -4.2186332 -4.2412381 -4.2676663 -4.2983537 -4.3235836 -4.3405185 -4.3427081 -4.3359857][-4.249711 -4.2452583 -4.2510095 -4.2646065 -4.2795825 -4.2788372 -4.260859 -4.25815 -4.2719173 -4.2876644 -4.3094811 -4.3302312 -4.344028 -4.346251 -4.3382888]]...]
INFO - root - 2017-12-07 14:39:09.433896: step 20110, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 84h:31m:37s remains)
INFO - root - 2017-12-07 14:39:18.976751: step 20120, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.919 sec/batch; 79h:44m:36s remains)
INFO - root - 2017-12-07 14:39:28.698185: step 20130, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 80h:26m:22s remains)
INFO - root - 2017-12-07 14:39:38.473445: step 20140, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 84h:59m:24s remains)
INFO - root - 2017-12-07 14:39:48.156406: step 20150, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 84h:55m:09s remains)
INFO - root - 2017-12-07 14:39:57.753889: step 20160, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 81h:54m:19s remains)
INFO - root - 2017-12-07 14:40:07.412198: step 20170, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 81h:59m:29s remains)
INFO - root - 2017-12-07 14:40:17.075457: step 20180, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 87h:07m:20s remains)
INFO - root - 2017-12-07 14:40:26.667054: step 20190, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 80h:23m:31s remains)
INFO - root - 2017-12-07 14:40:36.325543: step 20200, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 83h:55m:24s remains)
2017-12-07 14:40:37.272473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2522173 -4.2600703 -4.276032 -4.2875385 -4.2891169 -4.2817259 -4.2647424 -4.2428551 -4.22689 -4.2225308 -4.228498 -4.2364516 -4.2423358 -4.2464943 -4.2467542][-4.2853117 -4.2906923 -4.2981763 -4.2996936 -4.2925677 -4.2769213 -4.2525582 -4.2259169 -4.2093563 -4.2081313 -4.2153611 -4.2222047 -4.225843 -4.2293043 -4.2305393][-4.3047915 -4.3065491 -4.3070579 -4.3018775 -4.2907748 -4.2727623 -4.2485051 -4.2248411 -4.21458 -4.2187243 -4.2250376 -4.2268386 -4.2260003 -4.2254438 -4.2257476][-4.2902241 -4.2894278 -4.28838 -4.283246 -4.2735033 -4.259438 -4.2401109 -4.2228165 -4.2200937 -4.2301154 -4.2371588 -4.2376227 -4.2365208 -4.2348127 -4.2339234][-4.2466965 -4.2428904 -4.2415848 -4.2386718 -4.2317834 -4.2203403 -4.2047 -4.1920257 -4.1963792 -4.2161436 -4.2301188 -4.2333879 -4.2319751 -4.2311945 -4.2317009][-4.1980371 -4.1879082 -4.1840277 -4.1803341 -4.1715851 -4.15757 -4.138061 -4.1243548 -4.1358438 -4.1708088 -4.1985145 -4.2081137 -4.2089891 -4.2128143 -4.2183123][-4.1677432 -4.1472816 -4.1338615 -4.1183853 -4.0946198 -4.0668178 -4.0373077 -4.0187573 -4.0407224 -4.0988522 -4.1440516 -4.1622114 -4.1699038 -4.1813426 -4.1962576][-4.1648026 -4.1370025 -4.1148853 -4.0854011 -4.0428257 -3.9940312 -3.9433563 -3.9073877 -3.9367924 -4.0191774 -4.0813713 -4.105752 -4.1178656 -4.1372457 -4.1622238][-4.1885972 -4.1625524 -4.1416764 -4.1104574 -4.066196 -4.0161004 -3.9610443 -3.9121034 -3.9186239 -3.9823289 -4.0373983 -4.0600433 -4.0721421 -4.0954866 -4.1281967][-4.2155008 -4.196413 -4.1844654 -4.1631827 -4.1317225 -4.0977559 -4.0579109 -4.0137196 -3.9984303 -4.0220861 -4.0470433 -4.0530472 -4.0599203 -4.0832 -4.116056][-4.24022 -4.2291603 -4.2242446 -4.2107673 -4.1888328 -4.1662779 -4.1384478 -4.1033177 -4.0841284 -4.0888505 -4.0951128 -4.0884609 -4.088223 -4.104445 -4.129807][-4.2711935 -4.26459 -4.2607083 -4.2494192 -4.2337322 -4.2202778 -4.2023783 -4.1762114 -4.1569314 -4.1509619 -4.1466289 -4.1364565 -4.1339483 -4.1449537 -4.1619248][-4.2984428 -4.2926068 -4.2874279 -4.2763572 -4.2647872 -4.2593417 -4.2516761 -4.2377048 -4.2254243 -4.2186117 -4.2108774 -4.2000437 -4.1950665 -4.1999846 -4.2095737][-4.317956 -4.3118787 -4.3058324 -4.2962213 -4.28781 -4.2847943 -4.281744 -4.2750778 -4.2681994 -4.2640667 -4.2596359 -4.2515697 -4.2453642 -4.2448416 -4.2471995][-4.3335695 -4.3275375 -4.3213143 -4.3130331 -4.3061352 -4.3021059 -4.2983437 -4.2921143 -4.285697 -4.2816181 -4.2788568 -4.2746077 -4.2701187 -4.2671666 -4.2659788]]...]
INFO - root - 2017-12-07 14:40:47.047277: step 20210, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 85h:32m:08s remains)
INFO - root - 2017-12-07 14:40:56.670620: step 20220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 81h:38m:27s remains)
INFO - root - 2017-12-07 14:41:06.365892: step 20230, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 82h:14m:58s remains)
INFO - root - 2017-12-07 14:41:15.858462: step 20240, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.925 sec/batch; 80h:12m:06s remains)
INFO - root - 2017-12-07 14:41:25.731739: step 20250, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.009 sec/batch; 87h:29m:39s remains)
INFO - root - 2017-12-07 14:41:35.291477: step 20260, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.814 sec/batch; 70h:33m:58s remains)
INFO - root - 2017-12-07 14:41:44.833499: step 20270, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.999 sec/batch; 86h:37m:06s remains)
INFO - root - 2017-12-07 14:41:54.570026: step 20280, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 81h:06m:53s remains)
INFO - root - 2017-12-07 14:42:04.283399: step 20290, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 87h:08m:26s remains)
INFO - root - 2017-12-07 14:42:13.825301: step 20300, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.967 sec/batch; 83h:52m:29s remains)
2017-12-07 14:42:14.887420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1933069 -4.1912136 -4.1935573 -4.1947374 -4.1944065 -4.1923428 -4.1871672 -4.1905222 -4.2030158 -4.2131047 -4.2172208 -4.21575 -4.2091169 -4.2046247 -4.2071791][-4.1842546 -4.1879034 -4.20017 -4.2109289 -4.2206535 -4.2251463 -4.2231393 -4.2262559 -4.2348728 -4.2436314 -4.2507696 -4.2522235 -4.2448697 -4.2335591 -4.2234359][-4.1822538 -4.1882668 -4.2051249 -4.2207117 -4.2359414 -4.2453523 -4.2462187 -4.2503881 -4.2569246 -4.2653666 -4.2745771 -4.2792897 -4.2726831 -4.2586746 -4.2421513][-4.1870532 -4.1947579 -4.2119055 -4.2275505 -4.244668 -4.2556005 -4.2565956 -4.2598615 -4.2641993 -4.2721343 -4.2832136 -4.2913232 -4.28688 -4.2745991 -4.2578993][-4.1890817 -4.1947289 -4.2052679 -4.21482 -4.2278023 -4.23503 -4.2311616 -4.2299867 -4.2318196 -4.2407842 -4.25712 -4.2731462 -4.2755108 -4.2695293 -4.2587061][-4.1963191 -4.1928368 -4.1913342 -4.1876593 -4.1863518 -4.1831055 -4.1707077 -4.161417 -4.1555567 -4.1625485 -4.1886277 -4.2198696 -4.23786 -4.2448678 -4.2451787][-4.2108164 -4.1966515 -4.1806803 -4.1607027 -4.1428328 -4.1291714 -4.1078305 -4.0838413 -4.0580745 -4.0516281 -4.0863957 -4.1370964 -4.179522 -4.2082853 -4.2248335][-4.2298975 -4.2092266 -4.1874113 -4.1605854 -4.1372638 -4.1208034 -4.0974569 -4.062273 -4.0111451 -3.9743013 -3.9988337 -4.0559359 -4.1155081 -4.1647024 -4.1960082][-4.2550182 -4.237359 -4.2209158 -4.2014132 -4.1865945 -4.1773882 -4.1619358 -4.1332445 -4.0802393 -4.0267167 -4.0208645 -4.0491724 -4.0915146 -4.135159 -4.1659751][-4.2634983 -4.2505484 -4.2396984 -4.229156 -4.2240629 -4.2227216 -4.2187791 -4.2056046 -4.1706233 -4.1232152 -4.09828 -4.0922542 -4.1043777 -4.1304359 -4.1504917][-4.2563457 -4.2486238 -4.2418036 -4.2369318 -4.2364249 -4.23761 -4.2404003 -4.2411003 -4.2255096 -4.1948209 -4.1722631 -4.1542273 -4.14778 -4.1587553 -4.1685786][-4.2322035 -4.2300673 -4.2282834 -4.2283425 -4.2318006 -4.2372489 -4.24387 -4.2524548 -4.2482157 -4.2323923 -4.2208652 -4.2083044 -4.201148 -4.20686 -4.2110162][-4.214056 -4.2113647 -4.2088642 -4.2099867 -4.2172046 -4.2286391 -4.2412128 -4.255652 -4.2587123 -4.251503 -4.2476048 -4.2413788 -4.23918 -4.2441306 -4.2455659][-4.2171044 -4.2087626 -4.2021713 -4.2006745 -4.2088 -4.2234979 -4.240252 -4.2580767 -4.2661028 -4.2657824 -4.2674785 -4.2657456 -4.267529 -4.271338 -4.2707577][-4.23944 -4.226222 -4.2154174 -4.2084484 -4.2122264 -4.2250018 -4.2425337 -4.2610078 -4.2721925 -4.2779031 -4.2846403 -4.2867074 -4.28867 -4.2894053 -4.2867017]]...]
INFO - root - 2017-12-07 14:42:24.717989: step 20310, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.019 sec/batch; 88h:20m:52s remains)
INFO - root - 2017-12-07 14:42:34.237008: step 20320, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 85h:01m:27s remains)
INFO - root - 2017-12-07 14:42:43.954931: step 20330, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 83h:39m:06s remains)
INFO - root - 2017-12-07 14:42:53.609715: step 20340, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 84h:47m:20s remains)
INFO - root - 2017-12-07 14:43:03.147800: step 20350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 80h:55m:57s remains)
INFO - root - 2017-12-07 14:43:12.746239: step 20360, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 84h:54m:55s remains)
INFO - root - 2017-12-07 14:43:22.191928: step 20370, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 86h:11m:35s remains)
INFO - root - 2017-12-07 14:43:31.992224: step 20380, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 85h:47m:51s remains)
INFO - root - 2017-12-07 14:43:41.555196: step 20390, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 85h:05m:01s remains)
INFO - root - 2017-12-07 14:43:51.091743: step 20400, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 80h:07m:40s remains)
2017-12-07 14:43:51.993721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2005515 -4.2121944 -4.2217951 -4.2240281 -4.2200418 -4.2167082 -4.2145739 -4.2114792 -4.2090755 -4.2128558 -4.2278051 -4.2498164 -4.2684627 -4.2739959 -4.2705789][-4.1646061 -4.1740065 -4.1872835 -4.1919527 -4.1858091 -4.1822572 -4.1805439 -4.1769524 -4.1737008 -4.1736479 -4.1830235 -4.2019525 -4.2216058 -4.2379117 -4.2519093][-4.1337166 -4.1332355 -4.1453118 -4.1513338 -4.1441884 -4.1416488 -4.1450334 -4.1480112 -4.1491294 -4.1463413 -4.1444926 -4.148313 -4.1570296 -4.1738462 -4.1997385][-4.1156278 -4.10882 -4.1227756 -4.136447 -4.1349726 -4.1370811 -4.14533 -4.15133 -4.1502075 -4.1396976 -4.1236587 -4.1117306 -4.1076479 -4.1175447 -4.1447043][-4.1229744 -4.1135564 -4.1298084 -4.15216 -4.162303 -4.171658 -4.179513 -4.1777334 -4.1618371 -4.134912 -4.1067605 -4.0901394 -4.0888534 -4.1021976 -4.1328616][-4.16917 -4.1585226 -4.1723108 -4.1958823 -4.2097449 -4.2169032 -4.2146759 -4.1954207 -4.1585402 -4.1125097 -4.074553 -4.0645962 -4.0847445 -4.1186833 -4.1603484][-4.2414174 -4.2327647 -4.2414827 -4.2583122 -4.2657638 -4.262095 -4.2449479 -4.2072811 -4.1510687 -4.0875611 -4.0438595 -4.0487714 -4.0965767 -4.1522856 -4.2008781][-4.2995915 -4.2934675 -4.2987847 -4.3093309 -4.3109465 -4.2979083 -4.2682776 -4.2189126 -4.1538019 -4.0874271 -4.0492859 -4.0698581 -4.1343341 -4.1986947 -4.2448239][-4.3276095 -4.3255429 -4.3287206 -4.3338494 -4.3305607 -4.3127303 -4.2803206 -4.2340527 -4.1776981 -4.1251378 -4.098568 -4.1246123 -4.1863213 -4.2436657 -4.2801261][-4.3244629 -4.327692 -4.3315697 -4.3346014 -4.3315296 -4.31758 -4.2947149 -4.26387 -4.2279091 -4.1949744 -4.1768346 -4.1926312 -4.2342 -4.2729449 -4.2959294][-4.2938972 -4.3050404 -4.31493 -4.3224 -4.3259234 -4.3226013 -4.3144026 -4.3017187 -4.2850251 -4.2662106 -4.2496433 -4.2501645 -4.2677631 -4.2867141 -4.2970366][-4.2626657 -4.2827072 -4.2994204 -4.3119769 -4.3204594 -4.3242569 -4.3258862 -4.3247395 -4.3204365 -4.3111377 -4.296874 -4.2878642 -4.2891617 -4.2936459 -4.2952504][-4.2438631 -4.2682061 -4.2888956 -4.3037024 -4.3126249 -4.3172231 -4.3212957 -4.3247066 -4.3258243 -4.3216085 -4.3104954 -4.3007565 -4.2965217 -4.2938623 -4.2913117][-4.2403679 -4.265655 -4.2871513 -4.3003778 -4.3055072 -4.306211 -4.30666 -4.3071942 -4.307056 -4.304153 -4.2976761 -4.2925019 -4.2897668 -4.287096 -4.2866659][-4.2500892 -4.2724571 -4.2916203 -4.3020229 -4.3037767 -4.300838 -4.2964845 -4.29176 -4.28757 -4.284101 -4.2814875 -4.2807508 -4.281074 -4.2809706 -4.2840343]]...]
INFO - root - 2017-12-07 14:44:01.541505: step 20410, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 83h:33m:41s remains)
INFO - root - 2017-12-07 14:44:11.150172: step 20420, loss = 2.12, batch loss = 2.06 (8.5 examples/sec; 0.943 sec/batch; 81h:44m:56s remains)
INFO - root - 2017-12-07 14:44:20.651538: step 20430, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 81h:31m:08s remains)
INFO - root - 2017-12-07 14:44:30.293061: step 20440, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.009 sec/batch; 87h:28m:04s remains)
INFO - root - 2017-12-07 14:44:39.875932: step 20450, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 82h:28m:00s remains)
INFO - root - 2017-12-07 14:44:49.499084: step 20460, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 86h:53m:35s remains)
INFO - root - 2017-12-07 14:44:59.061975: step 20470, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 86h:20m:59s remains)
INFO - root - 2017-12-07 14:45:08.706504: step 20480, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 85h:26m:24s remains)
INFO - root - 2017-12-07 14:45:18.396605: step 20490, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 86h:06m:07s remains)
INFO - root - 2017-12-07 14:45:28.096649: step 20500, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 84h:00m:27s remains)
2017-12-07 14:45:29.105952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2015104 -4.2104 -4.2057176 -4.2173996 -4.2354641 -4.2413464 -4.2286425 -4.1996574 -4.1695585 -4.1787152 -4.2040625 -4.216373 -4.2213764 -4.2154164 -4.2075448][-4.17234 -4.1897936 -4.1943884 -4.2143903 -4.2356319 -4.246727 -4.248445 -4.2368402 -4.2141619 -4.2171135 -4.230042 -4.2319975 -4.229188 -4.21401 -4.2048435][-4.1566443 -4.1749945 -4.1880908 -4.2135758 -4.2305675 -4.2367978 -4.243732 -4.242363 -4.2272253 -4.2248178 -4.2289543 -4.2249455 -4.2188535 -4.2029247 -4.1960697][-4.1762471 -4.1869249 -4.2002859 -4.2153182 -4.21603 -4.2090755 -4.2097282 -4.2143912 -4.2123766 -4.2115631 -4.2073169 -4.1923451 -4.1773748 -4.1646504 -4.168951][-4.2066383 -4.2109466 -4.2197247 -4.224586 -4.2089763 -4.1804795 -4.1603785 -4.1659412 -4.1826906 -4.195466 -4.1907983 -4.1703091 -4.1463032 -4.1305437 -4.14092][-4.2109694 -4.2169948 -4.2222266 -4.2165642 -4.1878853 -4.14065 -4.0898314 -4.0821981 -4.1225805 -4.1604242 -4.1672487 -4.1514254 -4.1296062 -4.1137085 -4.1241145][-4.1985993 -4.2080235 -4.2120256 -4.195312 -4.1536131 -4.0836048 -3.9909267 -3.9515455 -4.0155954 -4.0896306 -4.1253767 -4.128674 -4.1182489 -4.1061444 -4.1149073][-4.2066622 -4.2140131 -4.2096157 -4.1809969 -4.1224251 -4.0250235 -3.8823938 -3.7900548 -3.8759413 -3.9989493 -4.0748339 -4.1009994 -4.1014905 -4.090313 -4.0944176][-4.227798 -4.235786 -4.2283034 -4.2003217 -4.1480827 -4.0575767 -3.9128337 -3.793565 -3.8385382 -3.9510972 -4.037612 -4.0770388 -4.0793839 -4.0689893 -4.0704007][-4.2338443 -4.24393 -4.2453246 -4.233037 -4.2037034 -4.1434441 -4.0436168 -3.9453845 -3.9324167 -3.9771914 -4.0295992 -4.0647774 -4.0640635 -4.0549936 -4.0546021][-4.22311 -4.23042 -4.2425637 -4.2522812 -4.2478509 -4.2189097 -4.163054 -4.093708 -4.050828 -4.0469079 -4.0669737 -4.09102 -4.0882239 -4.0790615 -4.0747118][-4.2106767 -4.21205 -4.232367 -4.2560439 -4.2715321 -4.2676077 -4.2459493 -4.2055 -4.1622367 -4.1348858 -4.1328077 -4.1439004 -4.1411972 -4.1340933 -4.1270933][-4.20187 -4.1960158 -4.218051 -4.2484994 -4.2757316 -4.2874022 -4.2829823 -4.2656331 -4.2365251 -4.2081547 -4.1985974 -4.2045088 -4.2060504 -4.1998749 -4.1893597][-4.1999445 -4.1851764 -4.1992931 -4.2318254 -4.2664008 -4.2853885 -4.2846332 -4.2781978 -4.2656851 -4.2478695 -4.2389641 -4.24596 -4.2573924 -4.2598324 -4.2487326][-4.2125721 -4.1901903 -4.1902623 -4.2124925 -4.2427578 -4.2593122 -4.2607465 -4.2639017 -4.2642016 -4.2577896 -4.25605 -4.2648149 -4.2803817 -4.2896233 -4.2843208]]...]
INFO - root - 2017-12-07 14:45:38.807435: step 20510, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 86h:29m:11s remains)
INFO - root - 2017-12-07 14:45:48.703224: step 20520, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 85h:24m:50s remains)
INFO - root - 2017-12-07 14:45:58.244459: step 20530, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 83h:00m:37s remains)
INFO - root - 2017-12-07 14:46:07.648957: step 20540, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 74h:51m:52s remains)
INFO - root - 2017-12-07 14:46:17.376650: step 20550, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 84h:19m:46s remains)
INFO - root - 2017-12-07 14:46:27.048791: step 20560, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 83h:23m:38s remains)
INFO - root - 2017-12-07 14:46:36.619224: step 20570, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 81h:29m:03s remains)
INFO - root - 2017-12-07 14:46:46.243411: step 20580, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 79h:05m:50s remains)
INFO - root - 2017-12-07 14:46:55.930487: step 20590, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 82h:59m:45s remains)
INFO - root - 2017-12-07 14:47:05.321137: step 20600, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.948 sec/batch; 82h:08m:30s remains)
2017-12-07 14:47:06.230061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3402791 -4.3346372 -4.32756 -4.3233876 -4.3216095 -4.3213596 -4.3239613 -4.3262916 -4.3258772 -4.3246312 -4.3237104 -4.3239913 -4.3252883 -4.3276873 -4.3298569][-4.3311996 -4.3201747 -4.3076191 -4.2989764 -4.2953753 -4.2971787 -4.3054152 -4.3106103 -4.3074093 -4.3009243 -4.2983751 -4.2994914 -4.3034573 -4.3091474 -4.3151097][-4.3097448 -4.2896624 -4.2693586 -4.2549224 -4.2514567 -4.2594323 -4.2760386 -4.2836685 -4.2769804 -4.2670884 -4.2664638 -4.2713966 -4.2783637 -4.2887549 -4.2993331][-4.2802606 -4.2482195 -4.2162819 -4.19288 -4.1893854 -4.2072849 -4.2325726 -4.239418 -4.2258158 -4.2151728 -4.2212591 -4.2344327 -4.247292 -4.2636595 -4.2808104][-4.26076 -4.2176137 -4.1720924 -4.1374779 -4.1297374 -4.1532416 -4.1860051 -4.1928291 -4.1785469 -4.1734638 -4.187263 -4.2069087 -4.225955 -4.2463427 -4.2634206][-4.2571898 -4.2072883 -4.1522555 -4.1070514 -4.0904374 -4.1107574 -4.1432638 -4.1461997 -4.1372652 -4.1455774 -4.170186 -4.1953988 -4.2160363 -4.2352829 -4.249176][-4.2626777 -4.2130947 -4.160089 -4.1154032 -4.0938725 -4.1009369 -4.1161222 -4.1075306 -4.1028709 -4.1267138 -4.1611147 -4.1891928 -4.2097583 -4.2279439 -4.2384953][-4.2744617 -4.2329011 -4.1909423 -4.1509924 -4.12857 -4.1265979 -4.1258554 -4.1085415 -4.1036043 -4.1298409 -4.1642876 -4.1901536 -4.2055449 -4.2161245 -4.2224717][-4.2879148 -4.2538953 -4.2226582 -4.1956925 -4.1801171 -4.1769958 -4.1702838 -4.1506286 -4.1410012 -4.1571956 -4.1802421 -4.1938033 -4.1982875 -4.20037 -4.2007589][-4.2970362 -4.2679725 -4.2430263 -4.2251325 -4.215661 -4.2157054 -4.2120318 -4.1942363 -4.1805463 -4.1824508 -4.1891813 -4.1910582 -4.1880426 -4.1826944 -4.1798949][-4.3038478 -4.2769356 -4.2499 -4.2302232 -4.2210174 -4.2253289 -4.2264218 -4.2109876 -4.1911311 -4.1800027 -4.1774712 -4.1820445 -4.1810813 -4.1731253 -4.1707606][-4.3053269 -4.2769256 -4.2450142 -4.2203727 -4.2096272 -4.2175512 -4.2226954 -4.2059364 -4.180057 -4.1640391 -4.1666055 -4.18184 -4.1880665 -4.1824675 -4.180192][-4.2966809 -4.2641625 -4.2282338 -4.2014656 -4.1892872 -4.1950631 -4.2027283 -4.189487 -4.1639266 -4.1481152 -4.1597853 -4.1909461 -4.2092547 -4.2047253 -4.1969419][-4.2890167 -4.2540507 -4.2169089 -4.1876655 -4.1720037 -4.1790566 -4.1941686 -4.1863151 -4.1597962 -4.1445503 -4.1621032 -4.2041564 -4.2315006 -4.2298884 -4.2224121][-4.2903304 -4.2583957 -4.2228112 -4.1940126 -4.1825967 -4.1952171 -4.2138515 -4.2061992 -4.1781592 -4.165606 -4.1874108 -4.2319312 -4.2587605 -4.2597713 -4.2528872]]...]
INFO - root - 2017-12-07 14:47:15.888256: step 20610, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 80h:39m:04s remains)
INFO - root - 2017-12-07 14:47:25.701735: step 20620, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 87h:17m:45s remains)
INFO - root - 2017-12-07 14:47:35.382206: step 20630, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 86h:48m:23s remains)
INFO - root - 2017-12-07 14:47:45.021532: step 20640, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 85h:32m:56s remains)
INFO - root - 2017-12-07 14:47:54.571918: step 20650, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 83h:53m:05s remains)
INFO - root - 2017-12-07 14:48:04.054865: step 20660, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 80h:15m:44s remains)
INFO - root - 2017-12-07 14:48:13.702723: step 20670, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 83h:55m:30s remains)
INFO - root - 2017-12-07 14:48:23.424877: step 20680, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.011 sec/batch; 87h:35m:25s remains)
INFO - root - 2017-12-07 14:48:33.029816: step 20690, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 84h:46m:02s remains)
INFO - root - 2017-12-07 14:48:42.628983: step 20700, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 85h:31m:02s remains)
2017-12-07 14:48:43.570132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2857437 -4.2693248 -4.2642817 -4.2697253 -4.2871737 -4.3124771 -4.3352704 -4.3472691 -4.3484807 -4.3420563 -4.3325877 -4.3304014 -4.3383274 -4.3461323 -4.3460712][-4.3054891 -4.2827172 -4.2674489 -4.2633324 -4.2729468 -4.2946272 -4.3152275 -4.3286185 -4.3316717 -4.3300776 -4.3278742 -4.3293376 -4.3376713 -4.3480411 -4.3523259][-4.3011732 -4.2655129 -4.2371054 -4.2214966 -4.2240906 -4.2452497 -4.2683363 -4.2859769 -4.2916026 -4.2963495 -4.3051157 -4.3143735 -4.3251562 -4.33811 -4.3462534][-4.26773 -4.2105513 -4.1607237 -4.1275787 -4.1231294 -4.1529841 -4.1898608 -4.2182722 -4.23138 -4.2431469 -4.2622275 -4.2814641 -4.2983847 -4.3149023 -4.325665][-4.21902 -4.1334372 -4.0519552 -3.99061 -3.9767268 -4.0210466 -4.0819764 -4.1301045 -4.1565747 -4.1781454 -4.2094164 -4.2430162 -4.2701659 -4.2900348 -4.3022308][-4.176199 -4.0669022 -3.9550405 -3.8646824 -3.8373461 -3.8889723 -3.971168 -4.0364466 -4.0727553 -4.10569 -4.1530666 -4.1996861 -4.2397842 -4.2679019 -4.2841949][-4.1670904 -4.0543718 -3.9291334 -3.8186362 -3.7678084 -3.8070357 -3.8911982 -3.9624248 -4.003963 -4.0480919 -4.1082835 -4.1648011 -4.2172861 -4.257122 -4.2805519][-4.19671 -4.1006365 -3.987577 -3.8832822 -3.8197434 -3.8240466 -3.8820686 -3.9463019 -3.9905026 -4.0395074 -4.1027708 -4.1617465 -4.2181129 -4.2625027 -4.288908][-4.254272 -4.1835928 -4.0969458 -4.0162883 -3.9576962 -3.9366565 -3.960228 -4.0041847 -4.0399137 -4.0787578 -4.1347222 -4.1902342 -4.2430763 -4.2842093 -4.3069758][-4.3090353 -4.2609868 -4.1993532 -4.1403551 -4.0958252 -4.070847 -4.0768805 -4.1023278 -4.1247635 -4.14996 -4.1919851 -4.23724 -4.2775578 -4.3077755 -4.3232293][-4.3376489 -4.3076725 -4.26439 -4.2208967 -4.1889997 -4.1727476 -4.17684 -4.1903591 -4.2038007 -4.2180972 -4.2417226 -4.2704396 -4.295023 -4.3118534 -4.3172431][-4.3410096 -4.3229108 -4.2932496 -4.2618022 -4.2432 -4.239264 -4.2458863 -4.249732 -4.2530704 -4.2581692 -4.2681713 -4.2805209 -4.2919884 -4.2979808 -4.2959342][-4.3268876 -4.3146248 -4.2975321 -4.2813644 -4.2771144 -4.28295 -4.2919841 -4.2916512 -4.2866654 -4.282752 -4.2827153 -4.2830353 -4.2842512 -4.2816086 -4.2723484][-4.2998157 -4.2904119 -4.286345 -4.2867284 -4.29661 -4.3107557 -4.3230166 -4.3230357 -4.3151221 -4.3051343 -4.2979355 -4.29165 -4.2860184 -4.2754679 -4.2574954][-4.27034 -4.2607675 -4.2673826 -4.2824736 -4.3018537 -4.3217468 -4.3374195 -4.3410521 -4.3343611 -4.3228846 -4.31398 -4.3064022 -4.297864 -4.2819643 -4.2567539]]...]
INFO - root - 2017-12-07 14:48:53.384017: step 20710, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 87h:04m:56s remains)
INFO - root - 2017-12-07 14:49:03.012340: step 20720, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 82h:07m:31s remains)
INFO - root - 2017-12-07 14:49:12.603676: step 20730, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 80h:03m:32s remains)
INFO - root - 2017-12-07 14:49:22.332536: step 20740, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 81h:37m:31s remains)
INFO - root - 2017-12-07 14:49:32.026378: step 20750, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 85h:12m:10s remains)
INFO - root - 2017-12-07 14:49:41.686035: step 20760, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.010 sec/batch; 87h:25m:23s remains)
INFO - root - 2017-12-07 14:49:51.056339: step 20770, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 84h:46m:26s remains)
INFO - root - 2017-12-07 14:50:00.700425: step 20780, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 78h:00m:59s remains)
INFO - root - 2017-12-07 14:50:10.406337: step 20790, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.947 sec/batch; 82h:01m:08s remains)
INFO - root - 2017-12-07 14:50:20.113680: step 20800, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 85h:23m:06s remains)
2017-12-07 14:50:21.021077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2902675 -4.2790227 -4.2712374 -4.2696748 -4.2702489 -4.2720513 -4.2722383 -4.2660327 -4.2715888 -4.296411 -4.3151321 -4.3104997 -4.28978 -4.2554684 -4.217104][-4.2723875 -4.2603326 -4.2551484 -4.257453 -4.2612262 -4.2640572 -4.2599063 -4.2480268 -4.2536955 -4.2838397 -4.3079844 -4.3061509 -4.2832961 -4.2510438 -4.21865][-4.2423797 -4.2281742 -4.2269878 -4.2318573 -4.2348552 -4.2333884 -4.2253103 -4.21275 -4.2195263 -4.256042 -4.2888994 -4.2922268 -4.2708759 -4.2419729 -4.2150807][-4.2129259 -4.1964993 -4.2018943 -4.2099686 -4.2047467 -4.1917233 -4.1749196 -4.1580472 -4.16611 -4.2120919 -4.2570014 -4.2748394 -4.2636161 -4.2401533 -4.2164927][-4.19491 -4.1727448 -4.1807427 -4.1929579 -4.1845036 -4.1607056 -4.1291237 -4.0977635 -4.1026359 -4.1552639 -4.2102394 -4.2445941 -4.2518034 -4.2410989 -4.2275786][-4.1835628 -4.1572261 -4.1685309 -4.1840076 -4.1745796 -4.1389446 -4.0838575 -4.0306497 -4.0311313 -4.0910664 -4.1585994 -4.2072654 -4.2338805 -4.24232 -4.2469454][-4.158742 -4.1380939 -4.1583614 -4.180038 -4.1701608 -4.1228037 -4.0446372 -3.9735901 -3.968076 -4.0348091 -4.11337 -4.172729 -4.211916 -4.2346616 -4.2565465][-4.1221075 -4.1127577 -4.1449208 -4.1761975 -4.1704783 -4.1241159 -4.04288 -3.9705603 -3.9549291 -4.0123749 -4.0918827 -4.156631 -4.2010779 -4.2315097 -4.2618718][-4.0867872 -4.0934515 -4.135777 -4.177649 -4.1812935 -4.143158 -4.0721335 -4.0125833 -3.99465 -4.0339274 -4.1015491 -4.1629438 -4.2057948 -4.2379971 -4.2732654][-4.0784845 -4.0981064 -4.147048 -4.1929026 -4.2047753 -4.1740866 -4.115644 -4.0712824 -4.0609903 -4.0870519 -4.1373396 -4.1870937 -4.2187777 -4.2448645 -4.281426][-4.1074147 -4.1338248 -4.18144 -4.22476 -4.2400012 -4.2150826 -4.1679683 -4.1398606 -4.1370416 -4.1516604 -4.18135 -4.2135277 -4.232595 -4.2496734 -4.2808118][-4.1528134 -4.1807532 -4.21775 -4.2523808 -4.266788 -4.2472296 -4.213635 -4.19843 -4.1975694 -4.2008142 -4.2103658 -4.2251034 -4.2349129 -4.2497244 -4.2777424][-4.2030087 -4.2277894 -4.2520337 -4.2713561 -4.281086 -4.2667241 -4.2443457 -4.2331738 -4.2240338 -4.2150178 -4.2109776 -4.2120733 -4.2168145 -4.2324915 -4.2602334][-4.2437005 -4.2616777 -4.2742352 -4.2796316 -4.281363 -4.2665424 -4.2515025 -4.2417631 -4.2254 -4.2084637 -4.199563 -4.1941428 -4.1953187 -4.2119842 -4.2398844][-4.262022 -4.2721334 -4.2781773 -4.2740417 -4.2656021 -4.2461138 -4.2304268 -4.2218356 -4.208293 -4.1977773 -4.1927962 -4.1886153 -4.1904268 -4.2066274 -4.2307158]]...]
INFO - root - 2017-12-07 14:50:30.548768: step 20810, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 78h:55m:30s remains)
INFO - root - 2017-12-07 14:50:40.137696: step 20820, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 84h:08m:49s remains)
INFO - root - 2017-12-07 14:50:49.946341: step 20830, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 83h:23m:49s remains)
INFO - root - 2017-12-07 14:50:59.748186: step 20840, loss = 2.07, batch loss = 2.01 (7.5 examples/sec; 1.060 sec/batch; 91h:45m:23s remains)
INFO - root - 2017-12-07 14:51:09.220648: step 20850, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 81h:03m:13s remains)
INFO - root - 2017-12-07 14:51:18.934363: step 20860, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 82h:46m:57s remains)
INFO - root - 2017-12-07 14:51:28.633515: step 20870, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 82h:48m:41s remains)
INFO - root - 2017-12-07 14:51:38.284671: step 20880, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 87h:19m:13s remains)
INFO - root - 2017-12-07 14:51:47.826146: step 20890, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 82h:36m:49s remains)
INFO - root - 2017-12-07 14:51:57.522568: step 20900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 82h:36m:09s remains)
2017-12-07 14:51:58.530402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2986956 -4.301579 -4.3022332 -4.3010225 -4.2975292 -4.2919221 -4.2858438 -4.2797565 -4.2748885 -4.2727017 -4.27163 -4.2723479 -4.2756882 -4.2803488 -4.2843943][-4.3005724 -4.302556 -4.3027525 -4.301537 -4.2988925 -4.294579 -4.2893238 -4.282732 -4.2751551 -4.2685819 -4.264061 -4.2652922 -4.2722759 -4.280592 -4.2860432][-4.3001022 -4.3013783 -4.3017826 -4.3016815 -4.3011508 -4.2988372 -4.2941651 -4.2859478 -4.2730312 -4.2585831 -4.2486706 -4.250452 -4.2623491 -4.275672 -4.2836261][-4.2980714 -4.2993326 -4.3011136 -4.303483 -4.3061771 -4.3058405 -4.3007808 -4.2882943 -4.2660828 -4.2404761 -4.2246881 -4.2282887 -4.246954 -4.2671723 -4.2790127][-4.2955022 -4.2980533 -4.3023548 -4.3080597 -4.3136249 -4.3140917 -4.306644 -4.2865152 -4.2518854 -4.2147465 -4.1948676 -4.2028289 -4.2299738 -4.2577744 -4.2742181][-4.2948384 -4.3000064 -4.3074389 -4.3161063 -4.3231587 -4.3227816 -4.3106289 -4.2807546 -4.2340555 -4.1884689 -4.1683307 -4.18313 -4.2185869 -4.251997 -4.2717776][-4.296874 -4.3051076 -4.3152714 -4.3256273 -4.3323765 -4.3294711 -4.3111858 -4.2719936 -4.2169566 -4.1694546 -4.1547337 -4.1776648 -4.2183619 -4.2536039 -4.274075][-4.3009615 -4.3114185 -4.3227978 -4.3333097 -4.3389444 -4.3333087 -4.31012 -4.264596 -4.2070355 -4.164289 -4.1586614 -4.188 -4.2295747 -4.2626638 -4.2812014][-4.3051443 -4.3162522 -4.3273387 -4.3370881 -4.341743 -4.3348069 -4.3094592 -4.2630014 -4.20922 -4.1756783 -4.1790185 -4.21051 -4.247642 -4.2748418 -4.2898326][-4.3070068 -4.3175464 -4.3275027 -4.3364472 -4.34121 -4.3354111 -4.3120484 -4.2703576 -4.2251682 -4.2016191 -4.2101092 -4.2383142 -4.2666984 -4.2860374 -4.2968845][-4.3061428 -4.3157721 -4.3251033 -4.3345547 -4.3414598 -4.3392649 -4.3211288 -4.28651 -4.25118 -4.2349091 -4.2428946 -4.2633657 -4.2820821 -4.2939658 -4.3005219][-4.3040304 -4.3131061 -4.3231068 -4.3351178 -4.3459368 -4.3484092 -4.33644 -4.3089952 -4.280479 -4.2660584 -4.2693486 -4.2812653 -4.2920227 -4.2985425 -4.3013434][-4.3024058 -4.3114486 -4.3226428 -4.3368893 -4.3501768 -4.3556066 -4.3482132 -4.32638 -4.3020835 -4.2871485 -4.2854433 -4.2911768 -4.2971697 -4.300662 -4.3004346][-4.3017087 -4.3107171 -4.32196 -4.3357959 -4.3486328 -4.3546338 -4.3493447 -4.3317223 -4.3111081 -4.2963672 -4.2919855 -4.2941527 -4.2977319 -4.2991161 -4.296896][-4.3013153 -4.30944 -4.3188982 -4.3302402 -4.3408642 -4.3455653 -4.3411641 -4.3271704 -4.3101277 -4.2971611 -4.291934 -4.2919922 -4.2935581 -4.2934275 -4.2909126]]...]
INFO - root - 2017-12-07 14:52:08.223898: step 20910, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.013 sec/batch; 87h:41m:44s remains)
INFO - root - 2017-12-07 14:52:17.819017: step 20920, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 81h:16m:40s remains)
INFO - root - 2017-12-07 14:52:27.563015: step 20930, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 82h:45m:48s remains)
INFO - root - 2017-12-07 14:52:37.202615: step 20940, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 86h:52m:43s remains)
INFO - root - 2017-12-07 14:52:46.750560: step 20950, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.895 sec/batch; 77h:29m:20s remains)
INFO - root - 2017-12-07 14:52:56.372874: step 20960, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.961 sec/batch; 83h:07m:18s remains)
INFO - root - 2017-12-07 14:53:06.038365: step 20970, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 85h:23m:29s remains)
INFO - root - 2017-12-07 14:53:15.819892: step 20980, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 79h:13m:15s remains)
INFO - root - 2017-12-07 14:53:25.453816: step 20990, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 85h:27m:42s remains)
INFO - root - 2017-12-07 14:53:35.223090: step 21000, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 87h:48m:10s remains)
2017-12-07 14:53:36.261951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3230829 -4.3013468 -4.2808714 -4.2632236 -4.2482719 -4.24094 -4.2371964 -4.2399821 -4.2503285 -4.2669067 -4.2787447 -4.2802925 -4.2724252 -4.2554584 -4.2410159][-4.3219137 -4.2975144 -4.2729287 -4.2475829 -4.2209587 -4.2014279 -4.1874609 -4.1795406 -4.1838565 -4.2078962 -4.23907 -4.258194 -4.2658343 -4.2652946 -4.2646232][-4.3214912 -4.29813 -4.2736807 -4.2448568 -4.2076216 -4.17236 -4.1414542 -4.1135263 -4.1034322 -4.1320114 -4.1863666 -4.2284112 -4.2538786 -4.2654095 -4.2719622][-4.3246646 -4.3057008 -4.2849026 -4.2561264 -4.2124395 -4.1639833 -4.1160831 -4.0667367 -4.0355444 -4.0638938 -4.1393628 -4.2038484 -4.2444468 -4.2642021 -4.2723832][-4.3308191 -4.3168211 -4.298481 -4.2683988 -4.2191205 -4.16203 -4.1020322 -4.0357828 -3.9854202 -4.0053024 -4.0933771 -4.1757984 -4.2286782 -4.2522454 -4.2575855][-4.33642 -4.3259153 -4.3077073 -4.2748237 -4.2196064 -4.15615 -4.0869522 -4.0077705 -3.9472902 -3.9592586 -4.0486851 -4.1405935 -4.1992431 -4.2226057 -4.2228093][-4.339848 -4.3307705 -4.3122921 -4.2805939 -4.2257266 -4.161447 -4.0882196 -4.0051875 -3.9494376 -3.9620504 -4.0404263 -4.1249247 -4.177434 -4.191463 -4.1808581][-4.3386793 -4.3296919 -4.3115087 -4.2864919 -4.2431979 -4.1898184 -4.1264133 -4.0564876 -4.0229769 -4.0431795 -4.0972776 -4.1501813 -4.17635 -4.1681352 -4.1416912][-4.3357754 -4.3269649 -4.3101306 -4.292851 -4.2637243 -4.2258449 -4.1802912 -4.1318908 -4.1208358 -4.1478176 -4.1769123 -4.1924777 -4.1855111 -4.1518192 -4.1074824][-4.3332844 -4.3244529 -4.30849 -4.2949495 -4.2743845 -4.2505155 -4.2210736 -4.188992 -4.1884289 -4.2166061 -4.23257 -4.2297511 -4.2037959 -4.1544032 -4.0994353][-4.3306489 -4.3205142 -4.3029547 -4.28726 -4.2691674 -4.2555323 -4.2395506 -4.218801 -4.2224541 -4.2507029 -4.2654524 -4.2598758 -4.2282562 -4.1734128 -4.1168985][-4.327033 -4.3138127 -4.2928228 -4.27159 -4.2499294 -4.2401295 -4.2327142 -4.2193 -4.226778 -4.2571545 -4.2782135 -4.2767706 -4.2453752 -4.19089 -4.1349716][-4.324409 -4.3093977 -4.2865167 -4.2584 -4.2271371 -4.2103724 -4.2016368 -4.1908021 -4.2044029 -4.2404733 -4.269217 -4.2764277 -4.254364 -4.2057276 -4.1519055][-4.32511 -4.3120227 -4.2912745 -4.2577214 -4.2148108 -4.1869297 -4.1712732 -4.1609411 -4.1807833 -4.2210479 -4.2548051 -4.2712493 -4.2600465 -4.2207913 -4.1725416][-4.3305607 -4.3235664 -4.3092704 -4.2771258 -4.228498 -4.1903691 -4.1675673 -4.1542339 -4.1727948 -4.210453 -4.2431774 -4.2638726 -4.2617812 -4.2332959 -4.1924257]]...]
INFO - root - 2017-12-07 14:53:46.018991: step 21010, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 85h:34m:59s remains)
INFO - root - 2017-12-07 14:53:55.614105: step 21020, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 83h:26m:29s remains)
INFO - root - 2017-12-07 14:54:05.304418: step 21030, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 84h:16m:56s remains)
INFO - root - 2017-12-07 14:54:15.007962: step 21040, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 83h:00m:39s remains)
INFO - root - 2017-12-07 14:54:24.645077: step 21050, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 86h:19m:10s remains)
INFO - root - 2017-12-07 14:54:34.238992: step 21060, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.011 sec/batch; 87h:27m:26s remains)
INFO - root - 2017-12-07 14:54:43.796906: step 21070, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 82h:12m:50s remains)
INFO - root - 2017-12-07 14:54:53.688799: step 21080, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 83h:41m:25s remains)
INFO - root - 2017-12-07 14:55:03.223355: step 21090, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 85h:24m:31s remains)
INFO - root - 2017-12-07 14:55:12.785619: step 21100, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 80h:56m:52s remains)
2017-12-07 14:55:13.738507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2751164 -4.2794166 -4.2789841 -4.2727427 -4.2607856 -4.2485614 -4.2369766 -4.2256312 -4.2183008 -4.2163515 -4.2180696 -4.2225103 -4.2285509 -4.2314749 -4.2323966][-4.3014908 -4.3043518 -4.3024468 -4.2948828 -4.2830086 -4.2681918 -4.2524109 -4.2344475 -4.2196918 -4.2110505 -4.209321 -4.2118797 -4.2189841 -4.2251053 -4.2257051][-4.3223844 -4.32348 -4.3206506 -4.3121 -4.3000493 -4.2816329 -4.2626553 -4.2447557 -4.2310424 -4.2213926 -4.2176805 -4.2164536 -4.2160554 -4.2189794 -4.2179441][-4.335299 -4.3295135 -4.319016 -4.3012733 -4.2775354 -4.2484922 -4.2241917 -4.2122312 -4.2135882 -4.219131 -4.2291918 -4.2345328 -4.230545 -4.2265415 -4.2209578][-4.3352962 -4.3209515 -4.2977176 -4.258811 -4.2115126 -4.1644125 -4.1290994 -4.1236458 -4.1433005 -4.1747417 -4.2087669 -4.2314706 -4.2378764 -4.2380633 -4.2361088][-4.3219314 -4.2967381 -4.25241 -4.1768522 -4.0889053 -4.0142922 -3.9702272 -3.9774518 -4.0226221 -4.0870843 -4.15617 -4.2050638 -4.2323289 -4.2479405 -4.2534781][-4.3024979 -4.2568679 -4.1828556 -4.0674996 -3.9356809 -3.830636 -3.7823043 -3.8091731 -3.8806181 -3.9790084 -4.0834079 -4.160449 -4.2129769 -4.2469473 -4.2620525][-4.2964005 -4.2403145 -4.1561155 -4.0300641 -3.892184 -3.7913043 -3.756691 -3.7948654 -3.8716245 -3.9675756 -4.0678592 -4.147275 -4.2064867 -4.2459841 -4.2637744][-4.3149958 -4.2705126 -4.2086029 -4.1153307 -4.0200295 -3.9641171 -3.9527662 -3.9803317 -4.0270214 -4.0829396 -4.1446691 -4.1946759 -4.2316546 -4.259779 -4.273459][-4.3286595 -4.3006673 -4.2641788 -4.2116351 -4.1585507 -4.1349592 -4.1357207 -4.1480074 -4.1691775 -4.1964812 -4.2251496 -4.2480741 -4.2636909 -4.274601 -4.2792311][-4.3228078 -4.3045397 -4.2815361 -4.2540994 -4.2301974 -4.2214327 -4.2204189 -4.2197161 -4.2259197 -4.2381763 -4.252882 -4.2632065 -4.2679839 -4.270472 -4.2703776][-4.3064694 -4.29102 -4.2725 -4.2542424 -4.2406058 -4.23091 -4.2243309 -4.2143974 -4.2092371 -4.2139006 -4.2234983 -4.2299809 -4.2334228 -4.2370458 -4.2414184][-4.2789526 -4.2627206 -4.247005 -4.2322626 -4.217381 -4.2010155 -4.1852646 -4.1666393 -4.1571751 -4.1624851 -4.1725492 -4.1789856 -4.1814404 -4.1895666 -4.2042603][-4.2601147 -4.2444525 -4.229815 -4.213932 -4.1949458 -4.1742616 -4.1525249 -4.1312056 -4.1202979 -4.1220379 -4.1299472 -4.1352773 -4.1350427 -4.1464505 -4.174293][-4.2565327 -4.2453041 -4.2321754 -4.2154694 -4.1940827 -4.1720409 -4.1509156 -4.1352181 -4.1274266 -4.12673 -4.1303372 -4.1313906 -4.1263404 -4.136889 -4.1707397]]...]
INFO - root - 2017-12-07 14:55:23.251023: step 21110, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 81h:41m:21s remains)
INFO - root - 2017-12-07 14:55:32.895343: step 21120, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 84h:37m:53s remains)
INFO - root - 2017-12-07 14:55:42.553537: step 21130, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 83h:06m:55s remains)
INFO - root - 2017-12-07 14:55:52.110861: step 21140, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 84h:04m:19s remains)
INFO - root - 2017-12-07 14:56:01.846129: step 21150, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 81h:35m:02s remains)
INFO - root - 2017-12-07 14:56:11.544022: step 21160, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 85h:10m:37s remains)
INFO - root - 2017-12-07 14:56:21.144699: step 21170, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 83h:14m:41s remains)
INFO - root - 2017-12-07 14:56:30.857999: step 21180, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 84h:59m:56s remains)
INFO - root - 2017-12-07 14:56:40.575194: step 21190, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.994 sec/batch; 85h:55m:27s remains)
INFO - root - 2017-12-07 14:56:50.228917: step 21200, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 80h:54m:44s remains)
2017-12-07 14:56:51.112900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1966867 -4.1590137 -4.1411028 -4.1449671 -4.1620464 -4.1699591 -4.1655879 -4.1550913 -4.1578035 -4.1719646 -4.1988697 -4.227005 -4.2383056 -4.2370868 -4.232924][-4.1745472 -4.1390371 -4.1241212 -4.1326585 -4.1596041 -4.1759863 -4.1774626 -4.1723847 -4.1742039 -4.1852717 -4.2116432 -4.2397423 -4.2473407 -4.2417331 -4.2315168][-4.1565585 -4.1294422 -4.1248207 -4.1411481 -4.1745806 -4.1968293 -4.2002487 -4.1974792 -4.1973829 -4.2060232 -4.2299919 -4.2519808 -4.2544203 -4.2427969 -4.2305965][-4.1488972 -4.1397824 -4.1472764 -4.1636863 -4.1897063 -4.2017655 -4.1959686 -4.1918945 -4.1982012 -4.2137384 -4.2350187 -4.2479239 -4.2463946 -4.2314739 -4.2164445][-4.1564617 -4.1708136 -4.1875448 -4.1946764 -4.1983728 -4.1854396 -4.1553621 -4.1424961 -4.161427 -4.1960206 -4.2263684 -4.2300482 -4.2214136 -4.1997848 -4.1784077][-4.1823359 -4.2099328 -4.2230253 -4.2100978 -4.1803021 -4.132689 -4.071104 -4.0541248 -4.0958581 -4.1646714 -4.2172046 -4.2206831 -4.2070441 -4.1730981 -4.1400337][-4.2309823 -4.2575655 -4.2572966 -4.2168493 -4.1427493 -4.0480118 -3.9512873 -3.9405239 -4.01949 -4.1350732 -4.21436 -4.2227697 -4.2040977 -4.1574855 -4.1159163][-4.2807832 -4.3030896 -4.2920504 -4.2293305 -4.1119781 -3.9668872 -3.837436 -3.8399043 -3.9521189 -4.1051083 -4.20444 -4.21513 -4.1903815 -4.1348634 -4.0891571][-4.3127589 -4.3306222 -4.3168564 -4.250329 -4.1207833 -3.9627252 -3.8370285 -3.8534346 -3.9627111 -4.1044173 -4.2012382 -4.2127805 -4.1806693 -4.124805 -4.0742874][-4.3254151 -4.3392754 -4.3279119 -4.2762747 -4.1736221 -4.051971 -3.9740176 -4.0029607 -4.0774431 -4.1635356 -4.2258673 -4.2276778 -4.1880403 -4.1305523 -4.0772648][-4.3237133 -4.3340716 -4.3243642 -4.2868757 -4.2167678 -4.1376462 -4.1045361 -4.1372323 -4.1785555 -4.2165618 -4.241457 -4.232914 -4.1900749 -4.1280951 -4.0753613][-4.3121166 -4.3182988 -4.307375 -4.273911 -4.2228389 -4.1704211 -4.1633558 -4.2006369 -4.2283473 -4.2433186 -4.2433343 -4.2272897 -4.184752 -4.1179361 -4.0664725][-4.3094044 -4.3124676 -4.2961535 -4.2621431 -4.2223058 -4.1885448 -4.1967173 -4.2341657 -4.254396 -4.2578459 -4.2449484 -4.2195225 -4.1782341 -4.118432 -4.0770674][-4.3178978 -4.3173032 -4.295598 -4.2651219 -4.2361674 -4.2168489 -4.2334647 -4.2689872 -4.2834239 -4.2802162 -4.25931 -4.2291527 -4.1920505 -4.1405039 -4.1114063][-4.3161139 -4.3131394 -4.2898831 -4.2649312 -4.2475419 -4.2422428 -4.2654152 -4.2966461 -4.3061724 -4.3015866 -4.2791214 -4.2471428 -4.2050328 -4.1539578 -4.1305513]]...]
INFO - root - 2017-12-07 14:57:00.814095: step 21210, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 78h:18m:09s remains)
INFO - root - 2017-12-07 14:57:10.547967: step 21220, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.970 sec/batch; 83h:52m:15s remains)
INFO - root - 2017-12-07 14:57:20.291685: step 21230, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 83h:57m:20s remains)
INFO - root - 2017-12-07 14:57:30.063394: step 21240, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 83h:37m:59s remains)
INFO - root - 2017-12-07 14:57:39.584929: step 21250, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 1.006 sec/batch; 86h:58m:07s remains)
INFO - root - 2017-12-07 14:57:49.306534: step 21260, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 80h:30m:45s remains)
INFO - root - 2017-12-07 14:57:58.850200: step 21270, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 84h:11m:54s remains)
INFO - root - 2017-12-07 14:58:08.403175: step 21280, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 85h:08m:15s remains)
INFO - root - 2017-12-07 14:58:17.982877: step 21290, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 83h:53m:07s remains)
INFO - root - 2017-12-07 14:58:27.809613: step 21300, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 84h:41m:33s remains)
2017-12-07 14:58:28.857470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.317977 -4.3058743 -4.3002834 -4.2928052 -4.2825055 -4.2771521 -4.2873831 -4.2973781 -4.2983613 -4.2976494 -4.3033323 -4.3122778 -4.320014 -4.3272481 -4.3373833][-4.2886233 -4.2726269 -4.2665195 -4.25728 -4.2385888 -4.2234159 -4.2348585 -4.24735 -4.2444453 -4.2410522 -4.2499585 -4.2652054 -4.2771511 -4.28704 -4.3030438][-4.2660623 -4.2452173 -4.235642 -4.2239037 -4.1974778 -4.1753774 -4.1883831 -4.2024384 -4.1964593 -4.19046 -4.2044716 -4.2249126 -4.2380037 -4.2478495 -4.2674422][-4.2586412 -4.2320518 -4.2149639 -4.1968207 -4.1575279 -4.1252832 -4.1416817 -4.16106 -4.1529341 -4.1440563 -4.1612473 -4.1865573 -4.2026868 -4.2138038 -4.2367973][-4.2606 -4.2297449 -4.2043724 -4.1773281 -4.1256495 -4.0771217 -4.0880032 -4.1105766 -4.1044464 -4.0979156 -4.1207237 -4.153697 -4.1767406 -4.1920929 -4.2202635][-4.2654514 -4.23536 -4.2043042 -4.1695724 -4.1045389 -4.0356045 -4.0222483 -4.0387654 -4.04192 -4.0515261 -4.0897713 -4.1346354 -4.1654425 -4.1844988 -4.2144561][-4.2727308 -4.2493029 -4.2191763 -4.1802197 -4.1081848 -4.0229 -3.9749265 -3.9724407 -3.9855158 -4.0149436 -4.0733967 -4.1298504 -4.1655865 -4.1874766 -4.2157378][-4.2759395 -4.2620611 -4.2453742 -4.2152238 -4.1567774 -4.0873365 -4.0273585 -3.9934201 -3.9941444 -4.0263 -4.092134 -4.1501074 -4.1835556 -4.2048383 -4.2280059][-4.2541389 -4.2428861 -4.2378974 -4.2204657 -4.1850142 -4.1456933 -4.1028543 -4.0566092 -4.0343566 -4.0521092 -4.1098146 -4.1596956 -4.1888747 -4.2106056 -4.2352366][-4.2225366 -4.2082353 -4.2098775 -4.2067375 -4.1910968 -4.1768556 -4.1572189 -4.1197143 -4.0900531 -4.0976963 -4.1463733 -4.1895666 -4.2143922 -4.2316093 -4.252214][-4.1987872 -4.1779327 -4.1807246 -4.1860113 -4.1820984 -4.18633 -4.1884694 -4.1655293 -4.1375179 -4.1384373 -4.1784468 -4.2165217 -4.2370791 -4.2487731 -4.2671208][-4.1922812 -4.1710706 -4.1750736 -4.1860843 -4.1899242 -4.2043304 -4.2179923 -4.20531 -4.1843133 -4.1836247 -4.2156539 -4.2453022 -4.2576728 -4.2657266 -4.28264][-4.2120838 -4.1985455 -4.2063966 -4.220614 -4.2269096 -4.2432303 -4.2617221 -4.2561264 -4.2415514 -4.2408276 -4.2663045 -4.2881918 -4.2930603 -4.2958932 -4.3080921][-4.2554312 -4.2499876 -4.2589803 -4.270957 -4.2731137 -4.2836871 -4.2982078 -4.2936735 -4.28091 -4.2780581 -4.2960153 -4.3120456 -4.3162031 -4.3197317 -4.3310122][-4.2983618 -4.298665 -4.3088613 -4.3177543 -4.3157353 -4.3186188 -4.3251939 -4.3174987 -4.3048477 -4.3006883 -4.3113151 -4.3206964 -4.3256488 -4.3338747 -4.3462749]]...]
INFO - root - 2017-12-07 14:58:38.523435: step 21310, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 84h:28m:58s remains)
INFO - root - 2017-12-07 14:58:48.124126: step 21320, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 79h:32m:36s remains)
INFO - root - 2017-12-07 14:58:57.903979: step 21330, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 85h:35m:04s remains)
INFO - root - 2017-12-07 14:59:07.401275: step 21340, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 78h:09m:20s remains)
INFO - root - 2017-12-07 14:59:17.076983: step 21350, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 85h:03m:42s remains)
INFO - root - 2017-12-07 14:59:26.804136: step 21360, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 84h:25m:17s remains)
INFO - root - 2017-12-07 14:59:36.397010: step 21370, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 76h:52m:05s remains)
INFO - root - 2017-12-07 14:59:46.067682: step 21380, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 80h:52m:08s remains)
INFO - root - 2017-12-07 14:59:55.736307: step 21390, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.018 sec/batch; 87h:57m:52s remains)
INFO - root - 2017-12-07 15:00:05.517410: step 21400, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 84h:37m:58s remains)
2017-12-07 15:00:06.511003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2532916 -4.2388096 -4.2417479 -4.2538605 -4.2738032 -4.2895904 -4.2965808 -4.2985458 -4.3009391 -4.2978926 -4.2753363 -4.236896 -4.2030263 -4.1981015 -4.2224026][-4.2512493 -4.2354217 -4.2395244 -4.2498121 -4.2666993 -4.2799058 -4.28705 -4.29144 -4.2978988 -4.2968397 -4.277895 -4.2406354 -4.2017503 -4.1894984 -4.2122579][-4.2349348 -4.218327 -4.223146 -4.2376137 -4.2577195 -4.2680855 -4.2718081 -4.2747545 -4.2843256 -4.2901959 -4.2841158 -4.2618175 -4.2318044 -4.2172918 -4.231688][-4.2064419 -4.178515 -4.1764746 -4.194715 -4.2248979 -4.2427745 -4.2498679 -4.2539992 -4.264184 -4.2762384 -4.2900314 -4.2907124 -4.2766786 -4.2630672 -4.26383][-4.1857734 -4.133842 -4.1098118 -4.1210322 -4.1584592 -4.192306 -4.2150011 -4.2309113 -4.2440124 -4.2592325 -4.2859254 -4.3091679 -4.3144436 -4.3049612 -4.2933583][-4.1839466 -4.1143637 -4.0667496 -4.0524664 -4.0718012 -4.1039553 -4.1414604 -4.1807847 -4.2145267 -4.2412071 -4.2727017 -4.3076015 -4.3272429 -4.3242869 -4.3071165][-4.2025442 -4.1322155 -4.0754304 -4.04047 -4.0232925 -4.0164719 -4.033946 -4.0903997 -4.1588821 -4.2125669 -4.2545242 -4.291347 -4.3167982 -4.3207927 -4.3030152][-4.2401958 -4.1797256 -4.12548 -4.0858183 -4.0420952 -3.987536 -3.9559493 -3.9998658 -4.0930967 -4.173564 -4.2300987 -4.2714777 -4.2991495 -4.307744 -4.2931514][-4.2708025 -4.2251844 -4.1798315 -4.1470451 -4.1062517 -4.043066 -3.9837995 -3.9977946 -4.0760255 -4.1529684 -4.2130647 -4.2584391 -4.2879663 -4.2978344 -4.2858858][-4.2784686 -4.2469473 -4.2129307 -4.1872907 -4.1682081 -4.1292329 -4.0787883 -4.0720944 -4.1158919 -4.16412 -4.201807 -4.236958 -4.266407 -4.2806778 -4.2749548][-4.2702227 -4.2467079 -4.2218361 -4.2040949 -4.2047539 -4.1952486 -4.1666803 -4.1551867 -4.1692076 -4.1895676 -4.2026424 -4.2155905 -4.2332954 -4.2492161 -4.2536063][-4.25785 -4.2380495 -4.2188649 -4.2083373 -4.2188826 -4.2241926 -4.2122602 -4.2041712 -4.204257 -4.2079339 -4.2107763 -4.212183 -4.2186642 -4.2268119 -4.2309432][-4.2599139 -4.2423639 -4.2212873 -4.2089739 -4.2184925 -4.2296596 -4.2271919 -4.2240911 -4.2190981 -4.2121367 -4.209425 -4.2115269 -4.2214217 -4.2270808 -4.2233238][-4.2751021 -4.263978 -4.245203 -4.230566 -4.2324181 -4.2445869 -4.2477894 -4.2417073 -4.23181 -4.2162266 -4.2038794 -4.2071643 -4.2243757 -4.2385359 -4.2361331][-4.2923226 -4.2889152 -4.2770309 -4.26506 -4.2633777 -4.2720652 -4.2755589 -4.2668662 -4.2497768 -4.2253942 -4.2045622 -4.2000785 -4.2156024 -4.233295 -4.2374616]]...]
INFO - root - 2017-12-07 15:00:16.104660: step 21410, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 85h:23m:39s remains)
INFO - root - 2017-12-07 15:00:25.842735: step 21420, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 83h:16m:06s remains)
INFO - root - 2017-12-07 15:00:35.595841: step 21430, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 84h:42m:06s remains)
INFO - root - 2017-12-07 15:00:45.090689: step 21440, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 83h:34m:25s remains)
INFO - root - 2017-12-07 15:00:54.699066: step 21450, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 84h:37m:55s remains)
INFO - root - 2017-12-07 15:01:04.305927: step 21460, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 85h:15m:13s remains)
INFO - root - 2017-12-07 15:01:14.139998: step 21470, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.992 sec/batch; 85h:41m:45s remains)
INFO - root - 2017-12-07 15:01:23.749408: step 21480, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 80h:18m:52s remains)
INFO - root - 2017-12-07 15:01:33.392029: step 21490, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 85h:31m:47s remains)
INFO - root - 2017-12-07 15:01:43.233976: step 21500, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 79h:05m:07s remains)
2017-12-07 15:01:44.122580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1930456 -4.1938853 -4.1986513 -4.2027483 -4.1958113 -4.1864142 -4.1891479 -4.2008538 -4.2105384 -4.2218361 -4.240418 -4.2534885 -4.2651911 -4.2761197 -4.2874527][-4.1732516 -4.184813 -4.1906881 -4.1871986 -4.1771903 -4.1704679 -4.1736469 -4.1828413 -4.1933651 -4.2069316 -4.2289071 -4.2448936 -4.2620163 -4.278399 -4.2925329][-4.13486 -4.1483812 -4.1505733 -4.1414289 -4.1367836 -4.1396656 -4.1440482 -4.1522388 -4.1703763 -4.1938233 -4.2171783 -4.236279 -4.2574048 -4.2775812 -4.2944851][-4.1070786 -4.1099815 -4.0934534 -4.0767751 -4.0860057 -4.1055412 -4.1168318 -4.1283484 -4.1513748 -4.1782918 -4.2019916 -4.2258444 -4.2514877 -4.2739434 -4.2919784][-4.0845604 -4.0771365 -4.0476608 -4.0249543 -4.0427556 -4.0751867 -4.0912437 -4.1034155 -4.1280441 -4.1531315 -4.1795154 -4.213614 -4.2466545 -4.2709074 -4.2882113][-4.0674124 -4.0555329 -4.0234342 -3.9975073 -4.0120296 -4.0419588 -4.0501742 -4.0565042 -4.0872555 -4.1235213 -4.1637535 -4.2095227 -4.2486176 -4.273921 -4.2896929][-4.0872588 -4.0715094 -4.038352 -4.0065646 -4.0062151 -4.0167804 -4.0078068 -4.0108647 -4.0544367 -4.1115108 -4.1696854 -4.2230911 -4.2633233 -4.28603 -4.2972279][-4.147438 -4.12416 -4.0882311 -4.0515804 -4.0347304 -4.0272431 -4.0098519 -4.0150027 -4.0639286 -4.1273503 -4.1851907 -4.2325559 -4.2670584 -4.2870488 -4.298811][-4.1986771 -4.179378 -4.1517825 -4.1236205 -4.1013846 -4.0830383 -4.0608935 -4.0642667 -4.107336 -4.1595697 -4.2002459 -4.23296 -4.2591877 -4.2776284 -4.2931952][-4.2443461 -4.233953 -4.2208362 -4.2060561 -4.1873488 -4.160953 -4.1277976 -4.1262631 -4.1625671 -4.1977148 -4.2227635 -4.244216 -4.261241 -4.2753668 -4.2902231][-4.288826 -4.2842789 -4.2795167 -4.2698975 -4.2491431 -4.2143364 -4.171195 -4.1660767 -4.196322 -4.2223458 -4.242022 -4.2605243 -4.2733912 -4.2818637 -4.2901773][-4.3101807 -4.3104453 -4.307889 -4.2968683 -4.2697005 -4.2288532 -4.1867843 -4.1881933 -4.2182074 -4.2382684 -4.2562208 -4.2737045 -4.2858033 -4.2908764 -4.2936811][-4.2938647 -4.2987757 -4.2983384 -4.2851486 -4.2546043 -4.2182846 -4.1909118 -4.2073874 -4.2408414 -4.25515 -4.2674727 -4.28158 -4.2940121 -4.2995672 -4.297998][-4.2453456 -4.2522984 -4.2527466 -4.2381377 -4.2085972 -4.1809316 -4.1689048 -4.1974826 -4.2340565 -4.2509871 -4.266191 -4.2830772 -4.298327 -4.3051643 -4.3011045][-4.2075424 -4.2164803 -4.2170868 -4.2034698 -4.1789823 -4.1560731 -4.1505556 -4.1815228 -4.2202911 -4.2425294 -4.2650342 -4.2870231 -4.3029823 -4.3097863 -4.30422]]...]
INFO - root - 2017-12-07 15:01:53.635780: step 21510, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 80h:13m:19s remains)
INFO - root - 2017-12-07 15:02:03.248552: step 21520, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 85h:03m:34s remains)
INFO - root - 2017-12-07 15:02:13.123149: step 21530, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.037 sec/batch; 89h:32m:52s remains)
INFO - root - 2017-12-07 15:02:22.519651: step 21540, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 82h:01m:09s remains)
INFO - root - 2017-12-07 15:02:32.149326: step 21550, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 78h:50m:49s remains)
INFO - root - 2017-12-07 15:02:41.846042: step 21560, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 80h:25m:19s remains)
INFO - root - 2017-12-07 15:02:51.610139: step 21570, loss = 2.10, batch loss = 2.05 (7.8 examples/sec; 1.027 sec/batch; 88h:42m:00s remains)
INFO - root - 2017-12-07 15:03:01.239918: step 21580, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 82h:49m:28s remains)
INFO - root - 2017-12-07 15:03:10.910908: step 21590, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 82h:06m:04s remains)
INFO - root - 2017-12-07 15:03:20.635514: step 21600, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 78h:17m:36s remains)
2017-12-07 15:03:21.648466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2264509 -4.2314143 -4.2392864 -4.2432632 -4.2392197 -4.2272153 -4.2174349 -4.2144256 -4.2147722 -4.2132287 -4.2154169 -4.2232866 -4.2303205 -4.2401686 -4.2554388][-4.2282567 -4.2306285 -4.2340155 -4.2334127 -4.2247734 -4.2095189 -4.1981392 -4.1938267 -4.194634 -4.1960812 -4.2043324 -4.218998 -4.2297192 -4.2398167 -4.253716][-4.1951337 -4.1929021 -4.1899543 -4.1826334 -4.1685476 -4.1505318 -4.1407022 -4.1439009 -4.1540089 -4.1658764 -4.1847663 -4.2085457 -4.2248278 -4.2359262 -4.2485271][-4.150209 -4.1441889 -4.1374607 -4.1274424 -4.11203 -4.0939326 -4.0883694 -4.1027136 -4.12484 -4.1476984 -4.1752558 -4.2034292 -4.2230206 -4.2340593 -4.2454481][-4.1034083 -4.09481 -4.0883126 -4.0819464 -4.0702019 -4.0537415 -4.0515285 -4.0741439 -4.1064005 -4.1394138 -4.1742425 -4.2038469 -4.2231388 -4.2332106 -4.2441387][-4.0711474 -4.0607514 -4.05387 -4.0484052 -4.036653 -4.0191889 -4.0165596 -4.0424719 -4.0796437 -4.1197863 -4.1626644 -4.1965246 -4.2183638 -4.2307887 -4.2435989][-4.0601478 -4.0521908 -4.0460896 -4.03849 -4.022388 -3.998878 -3.9885774 -4.0091829 -4.044435 -4.0860772 -4.1348028 -4.1757779 -4.2055264 -4.2249479 -4.2419353][-4.0574679 -4.0533042 -4.0488191 -4.0376439 -4.0153937 -3.9842775 -3.96666 -3.980293 -4.0122666 -4.0547853 -4.10728 -4.1535397 -4.19011 -4.2157378 -4.2375855][-4.066864 -4.0632415 -4.05789 -4.042881 -4.0175209 -3.987005 -3.970402 -3.97895 -4.0050774 -4.0460215 -4.0987992 -4.1453605 -4.182354 -4.2097707 -4.2345953][-4.0864005 -4.08517 -4.081408 -4.0705 -4.0515513 -4.0246377 -4.00658 -4.0071378 -4.023067 -4.0555511 -4.1017113 -4.1446738 -4.1798711 -4.2077017 -4.2345257][-4.1100469 -4.1112132 -4.1100025 -4.1057444 -4.0950456 -4.0742922 -4.0566421 -4.051446 -4.0611286 -4.0857434 -4.1218443 -4.1563187 -4.1858692 -4.212111 -4.2388062][-4.1521173 -4.152379 -4.1517811 -4.1502237 -4.1447864 -4.1323118 -4.1211424 -4.117506 -4.123713 -4.1396122 -4.1624846 -4.18475 -4.2041636 -4.225678 -4.2485418][-4.193141 -4.1954241 -4.1974115 -4.1984067 -4.1964188 -4.1898627 -4.1844106 -4.18304 -4.1872506 -4.1952806 -4.2064986 -4.2175088 -4.2277584 -4.2430077 -4.2605019][-4.1931276 -4.1984735 -4.2028565 -4.2060828 -4.2046027 -4.1985965 -4.1956773 -4.1994247 -4.2085018 -4.218339 -4.2289505 -4.2390451 -4.2477279 -4.2595611 -4.272316][-4.1546307 -4.1607518 -4.1655583 -4.1680746 -4.1638093 -4.155159 -4.1535721 -4.1635437 -4.1813521 -4.1997585 -4.21972 -4.2388868 -4.2538109 -4.26759 -4.27925]]...]
INFO - root - 2017-12-07 15:03:31.161129: step 21610, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 82h:10m:43s remains)
INFO - root - 2017-12-07 15:03:40.764554: step 21620, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 80h:30m:05s remains)
INFO - root - 2017-12-07 15:03:50.588640: step 21630, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 86h:17m:00s remains)
INFO - root - 2017-12-07 15:04:00.147710: step 21640, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.986 sec/batch; 85h:10m:00s remains)
INFO - root - 2017-12-07 15:04:09.877557: step 21650, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 82h:35m:44s remains)
INFO - root - 2017-12-07 15:04:19.464282: step 21660, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 85h:41m:09s remains)
INFO - root - 2017-12-07 15:04:29.125710: step 21670, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 80h:16m:46s remains)
INFO - root - 2017-12-07 15:04:39.027685: step 21680, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 87h:05m:05s remains)
INFO - root - 2017-12-07 15:04:48.569754: step 21690, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 84h:44m:50s remains)
INFO - root - 2017-12-07 15:04:58.092691: step 21700, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 79h:13m:32s remains)
2017-12-07 15:04:59.046945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2273922 -4.2255011 -4.2270818 -4.2290735 -4.2251034 -4.2107267 -4.1937547 -4.181664 -4.17253 -4.1913319 -4.2085714 -4.2207527 -4.2384377 -4.2487288 -4.2563915][-4.2209826 -4.2127366 -4.2117758 -4.2137613 -4.2068968 -4.1857915 -4.1620789 -4.1431661 -4.1347437 -4.1573305 -4.1777887 -4.1918645 -4.2133489 -4.2311673 -4.2434068][-4.2173128 -4.2076249 -4.2076025 -4.20734 -4.1952491 -4.1712904 -4.14335 -4.1108723 -4.0965152 -4.1237288 -4.1483965 -4.1683331 -4.1971493 -4.2227483 -4.2398891][-4.2126594 -4.208241 -4.2131653 -4.2110882 -4.1972742 -4.1761689 -4.1469936 -4.1074553 -4.0914717 -4.1226335 -4.1530066 -4.1766362 -4.20715 -4.232882 -4.248806][-4.201869 -4.2025509 -4.2110162 -4.2009611 -4.181613 -4.1590862 -4.126358 -4.0829482 -4.0709639 -4.1159964 -4.1594086 -4.1907368 -4.2211723 -4.2427106 -4.2576461][-4.2020459 -4.2003369 -4.2064152 -4.190383 -4.1627522 -4.1223936 -4.0656528 -4.0020881 -4.0008583 -4.078279 -4.1479559 -4.1938443 -4.2266979 -4.2469592 -4.2621655][-4.2109494 -4.2023582 -4.2025819 -4.1841 -4.1394339 -4.067822 -3.9671309 -3.870738 -3.8861351 -4.0138159 -4.1202078 -4.1849294 -4.23092 -4.2576218 -4.2730575][-4.2206922 -4.2095985 -4.2048182 -4.1841736 -4.1289358 -4.0363245 -3.9042039 -3.7806311 -3.8139246 -3.9782906 -4.1046662 -4.1803827 -4.2380857 -4.2708044 -4.2844877][-4.2340531 -4.22417 -4.2227721 -4.2133913 -4.1690283 -4.0869431 -3.9645879 -3.8572528 -3.8846531 -4.0250411 -4.140357 -4.207078 -4.258502 -4.2866907 -4.296258][-4.2455611 -4.2391405 -4.2441936 -4.2468581 -4.2201877 -4.1591964 -4.0624924 -3.9833508 -3.9961367 -4.092845 -4.1861691 -4.2420087 -4.2813764 -4.301537 -4.3048458][-4.2571197 -4.2544956 -4.263165 -4.2695479 -4.2508287 -4.2021141 -4.1189942 -4.0555997 -4.0627813 -4.1298189 -4.2072711 -4.258656 -4.2961321 -4.3118558 -4.30936][-4.2682481 -4.2691126 -4.2751646 -4.2778983 -4.2590055 -4.2146678 -4.1415739 -4.0900416 -4.1000209 -4.1522551 -4.2172589 -4.2655177 -4.3039055 -4.3191829 -4.3143754][-4.2712674 -4.2710819 -4.2739463 -4.2725368 -4.2556782 -4.219831 -4.1656365 -4.1316214 -4.1436563 -4.1808729 -4.2330647 -4.2774453 -4.3118086 -4.3248434 -4.3194485][-4.2775049 -4.2748928 -4.2789907 -4.2767129 -4.2637963 -4.2348623 -4.1982856 -4.17867 -4.1896839 -4.2160592 -4.2565813 -4.2943029 -4.3224459 -4.3303437 -4.3244357][-4.2793741 -4.2787547 -4.287262 -4.2868829 -4.2755933 -4.2518816 -4.2248034 -4.2113757 -4.2199683 -4.2397218 -4.2716327 -4.30297 -4.3248758 -4.3299742 -4.3252072]]...]
INFO - root - 2017-12-07 15:05:08.819221: step 21710, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 80h:01m:18s remains)
INFO - root - 2017-12-07 15:05:18.563858: step 21720, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 84h:37m:38s remains)
INFO - root - 2017-12-07 15:05:28.170972: step 21730, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 79h:20m:08s remains)
INFO - root - 2017-12-07 15:05:37.864304: step 21740, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.969 sec/batch; 83h:39m:12s remains)
INFO - root - 2017-12-07 15:05:47.586554: step 21750, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 82h:58m:58s remains)
INFO - root - 2017-12-07 15:05:57.254608: step 21760, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 81h:46m:19s remains)
INFO - root - 2017-12-07 15:06:06.888101: step 21770, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 83h:00m:08s remains)
INFO - root - 2017-12-07 15:06:16.430536: step 21780, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 84h:06m:43s remains)
INFO - root - 2017-12-07 15:06:26.129722: step 21790, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 80h:59m:21s remains)
INFO - root - 2017-12-07 15:06:35.900238: step 21800, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 84h:23m:42s remains)
2017-12-07 15:06:36.806541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1803 -4.1666894 -4.1628404 -4.158462 -4.1430674 -4.1177797 -4.1025491 -4.1142292 -4.1460772 -4.1710219 -4.19639 -4.2165189 -4.222611 -4.2256217 -4.2258196][-4.159132 -4.1467957 -4.1468821 -4.1466966 -4.1339579 -4.110642 -4.0943303 -4.1070237 -4.1409802 -4.1761479 -4.2129216 -4.2438946 -4.2575006 -4.2681136 -4.2746868][-4.1228881 -4.1203032 -4.1313167 -4.1409364 -4.1367221 -4.1173391 -4.1034355 -4.1169109 -4.1516337 -4.19395 -4.2315784 -4.2626023 -4.2788091 -4.2923889 -4.3018265][-4.084621 -4.0928392 -4.1134357 -4.1313329 -4.134994 -4.1263638 -4.1218257 -4.1404862 -4.1765075 -4.2177343 -4.2504435 -4.2738476 -4.2888837 -4.3020663 -4.3093863][-4.07123 -4.0877676 -4.1146183 -4.1349869 -4.1413178 -4.141191 -4.143249 -4.1648808 -4.2023277 -4.2420316 -4.2729554 -4.2904005 -4.3004227 -4.305768 -4.3040414][-4.1009364 -4.1225929 -4.1465235 -4.1574078 -4.1554747 -4.1518693 -4.1511207 -4.1706352 -4.2115264 -4.2551475 -4.2871413 -4.3032703 -4.3101115 -4.3075552 -4.292984][-4.1502724 -4.1727114 -4.1902738 -4.1872191 -4.1694021 -4.1494 -4.1387925 -4.1550488 -4.2000623 -4.2518277 -4.2879543 -4.3055778 -4.313158 -4.3059378 -4.2816596][-4.1908479 -4.2045817 -4.2134118 -4.2012405 -4.1751533 -4.1412354 -4.1227913 -4.1415839 -4.1877661 -4.2384853 -4.2753892 -4.296454 -4.3044024 -4.2922039 -4.2643671][-4.21198 -4.2057791 -4.2026405 -4.1863523 -4.1579738 -4.1205697 -4.1043525 -4.1263313 -4.1667557 -4.2076263 -4.2410483 -4.2634606 -4.2703915 -4.2568026 -4.2358608][-4.212739 -4.1847048 -4.1639948 -4.1437311 -4.1180573 -4.0907164 -4.0870209 -4.1087489 -4.1381555 -4.1691132 -4.197073 -4.2161202 -4.219645 -4.2104273 -4.2038016][-4.1948409 -4.1518836 -4.120244 -4.0993257 -4.0814118 -4.0715017 -4.0853763 -4.1104245 -4.1325731 -4.156919 -4.1773772 -4.1902518 -4.1890707 -4.1847029 -4.1894374][-4.1719804 -4.1300359 -4.1000576 -4.0847268 -4.0798388 -4.087913 -4.1157942 -4.1465907 -4.1686254 -4.1882977 -4.2029333 -4.2094526 -4.2042365 -4.1994076 -4.2039113][-4.1475382 -4.1207433 -4.1089673 -4.1111345 -4.1202559 -4.1381235 -4.1702194 -4.2043271 -4.2298222 -4.2500639 -4.26237 -4.26619 -4.2583613 -4.2475114 -4.2408605][-4.1269622 -4.1180692 -4.1286588 -4.1512556 -4.1728449 -4.1960759 -4.2277465 -4.2605109 -4.2876282 -4.3086438 -4.3215036 -4.3248806 -4.3161798 -4.2996264 -4.2806406][-4.1253881 -4.126564 -4.1513696 -4.1844749 -4.215127 -4.2459288 -4.2747121 -4.2991738 -4.320642 -4.3384814 -4.34959 -4.3519015 -4.3457117 -4.3308048 -4.3094034]]...]
INFO - root - 2017-12-07 15:06:46.489757: step 21810, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 79h:38m:01s remains)
INFO - root - 2017-12-07 15:06:56.223962: step 21820, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 78h:06m:53s remains)
INFO - root - 2017-12-07 15:07:05.682914: step 21830, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 83h:53m:08s remains)
INFO - root - 2017-12-07 15:07:15.299793: step 21840, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 84h:58m:53s remains)
INFO - root - 2017-12-07 15:07:24.989687: step 21850, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 82h:12m:10s remains)
INFO - root - 2017-12-07 15:07:34.758740: step 21860, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.040 sec/batch; 89h:42m:24s remains)
INFO - root - 2017-12-07 15:07:44.470589: step 21870, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 84h:02m:11s remains)
INFO - root - 2017-12-07 15:07:54.254072: step 21880, loss = 2.12, batch loss = 2.06 (8.4 examples/sec; 0.948 sec/batch; 81h:47m:41s remains)
INFO - root - 2017-12-07 15:08:03.903351: step 21890, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 84h:45m:45s remains)
INFO - root - 2017-12-07 15:08:13.536787: step 21900, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.023 sec/batch; 88h:17m:14s remains)
2017-12-07 15:08:14.450359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3168335 -4.3251519 -4.3314362 -4.3349457 -4.3369222 -4.338171 -4.3373709 -4.3364286 -4.3363066 -4.3340268 -4.3268332 -4.3197312 -4.314486 -4.305418 -4.2952681][-4.321147 -4.3290477 -4.333313 -4.3325195 -4.3296652 -4.3269219 -4.3234043 -4.3220077 -4.3243685 -4.3269029 -4.3253036 -4.3214049 -4.3172765 -4.3075075 -4.2958879][-4.3236623 -4.3287253 -4.3277993 -4.3201275 -4.3092589 -4.2983932 -4.2890339 -4.2865534 -4.2923393 -4.3031564 -4.3125629 -4.3164959 -4.3159938 -4.3065023 -4.2937183][-4.3295693 -4.329752 -4.32126 -4.3049817 -4.2842073 -4.2625027 -4.2448173 -4.2397432 -4.24918 -4.2689247 -4.2902489 -4.3017979 -4.305933 -4.2983937 -4.2857828][-4.3346391 -4.3301625 -4.3152852 -4.2914243 -4.2624216 -4.2298045 -4.20203 -4.1922016 -4.204443 -4.2320976 -4.2644114 -4.2819309 -4.289022 -4.2841907 -4.2733936][-4.3324137 -4.3267245 -4.3093529 -4.2819872 -4.2479944 -4.2077179 -4.1726561 -4.1589136 -4.1715913 -4.2038136 -4.2418904 -4.2607732 -4.2666869 -4.26223 -4.2533827][-4.324965 -4.3210597 -4.3063512 -4.2809 -4.2462716 -4.202282 -4.16235 -4.1429257 -4.1506872 -4.1808114 -4.2157283 -4.2317853 -4.2344136 -4.2285357 -4.2231188][-4.3008637 -4.2987156 -4.2906847 -4.2741194 -4.2472868 -4.2073336 -4.1658955 -4.1390963 -4.1368837 -4.1570339 -4.1809335 -4.191762 -4.1940994 -4.1909871 -4.1917644][-4.2607617 -4.2604871 -4.2584405 -4.252666 -4.2383561 -4.2089391 -4.1702409 -4.1394176 -4.1292195 -4.1384683 -4.1505303 -4.156518 -4.1616774 -4.1657171 -4.174005][-4.2298889 -4.2349825 -4.2390475 -4.2417622 -4.2354817 -4.2126889 -4.1749473 -4.1388106 -4.1195564 -4.1192555 -4.1254611 -4.1318145 -4.1438079 -4.1570649 -4.1714411][-4.2231879 -4.230885 -4.2379761 -4.2441363 -4.2402291 -4.2187438 -4.1812181 -4.1420269 -4.1177778 -4.1138124 -4.1205549 -4.1310983 -4.1499996 -4.1698446 -4.1867003][-4.243104 -4.2508721 -4.2575836 -4.2636976 -4.2623658 -4.2450843 -4.2133422 -4.1785316 -4.1564784 -4.1539078 -4.1634607 -4.177567 -4.1964073 -4.2132483 -4.2235365][-4.2803268 -4.2866631 -4.2921 -4.2976422 -4.2992668 -4.2906189 -4.2707443 -4.2473993 -4.2313104 -4.2288394 -4.2360024 -4.2464194 -4.2575579 -4.2628956 -4.2619519][-4.3154058 -4.319313 -4.3215408 -4.32399 -4.3258252 -4.3236752 -4.3155637 -4.3045092 -4.294786 -4.2915215 -4.2934256 -4.2963772 -4.2981505 -4.292829 -4.28379][-4.3292818 -4.3317146 -4.3318753 -4.3318152 -4.3325062 -4.3326945 -4.3303652 -4.3266058 -4.321907 -4.3190308 -4.3172984 -4.3153763 -4.3118234 -4.3017445 -4.289803]]...]
INFO - root - 2017-12-07 15:08:24.069556: step 21910, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 82h:17m:34s remains)
INFO - root - 2017-12-07 15:08:33.822174: step 21920, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 80h:26m:06s remains)
INFO - root - 2017-12-07 15:08:43.384960: step 21930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.982 sec/batch; 84h:40m:52s remains)
INFO - root - 2017-12-07 15:08:53.114022: step 21940, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 83h:58m:23s remains)
INFO - root - 2017-12-07 15:09:02.650201: step 21950, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 82h:30m:42s remains)
INFO - root - 2017-12-07 15:09:12.385501: step 21960, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 84h:00m:39s remains)
INFO - root - 2017-12-07 15:09:22.047527: step 21970, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.981 sec/batch; 84h:34m:50s remains)
INFO - root - 2017-12-07 15:09:31.762788: step 21980, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 78h:10m:18s remains)
INFO - root - 2017-12-07 15:09:41.437420: step 21990, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 82h:18m:43s remains)
INFO - root - 2017-12-07 15:09:50.985492: step 22000, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 80h:32m:26s remains)
2017-12-07 15:09:51.953217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2749939 -4.2498379 -4.2127185 -4.1901312 -4.18655 -4.1850729 -4.1608729 -4.1467772 -4.1625123 -4.1624746 -4.1417379 -4.1215954 -4.1127386 -4.1172175 -4.117106][-4.2821126 -4.2598453 -4.2280827 -4.211791 -4.2110195 -4.2092066 -4.1779032 -4.1502142 -4.1525011 -4.1486673 -4.1330338 -4.1247792 -4.1187754 -4.1184225 -4.1161547][-4.2877517 -4.2678494 -4.2401485 -4.226213 -4.2254267 -4.2238131 -4.1920285 -4.1509533 -4.1333728 -4.11995 -4.1124945 -4.1234794 -4.1294889 -4.129 -4.1246405][-4.2900991 -4.2699814 -4.2443042 -4.2271953 -4.217186 -4.2059345 -4.1708951 -4.1363554 -4.123107 -4.1088934 -4.1058407 -4.1283932 -4.145124 -4.1474361 -4.144855][-4.2882485 -4.2648177 -4.234004 -4.2063646 -4.1748881 -4.1371922 -4.0846276 -4.0694623 -4.0941758 -4.1056538 -4.1143103 -4.1385074 -4.1567097 -4.1621842 -4.1623964][-4.2797842 -4.2507162 -4.210844 -4.1663957 -4.1076279 -4.0285788 -3.9417679 -3.9438248 -4.0224586 -4.0823064 -4.1196709 -4.14453 -4.1583891 -4.1639328 -4.1684995][-4.2663341 -4.2317209 -4.1826262 -4.1225572 -4.0421529 -3.9255576 -3.8070252 -3.8280964 -3.9558296 -4.0560632 -4.1177759 -4.1425157 -4.1498165 -4.1550374 -4.165513][-4.2549195 -4.2149129 -4.158618 -4.0990715 -4.0223694 -3.9107513 -3.8077302 -3.8346305 -3.9541879 -4.0512066 -4.11404 -4.1349268 -4.1350665 -4.1335354 -4.1470771][-4.2476511 -4.2073007 -4.1517568 -4.106957 -4.0623407 -3.9997666 -3.9424188 -3.9528551 -4.0269351 -4.0916643 -4.137691 -4.1534276 -4.1491976 -4.137289 -4.1442728][-4.2458482 -4.2044721 -4.1541872 -4.1228867 -4.1079507 -4.0910883 -4.0719962 -4.0765524 -4.1154876 -4.1529727 -4.1845412 -4.1980028 -4.1916184 -4.1719351 -4.168891][-4.2538424 -4.2146449 -4.1722851 -4.1513247 -4.1521344 -4.1600575 -4.1640577 -4.1667647 -4.1861892 -4.2091484 -4.2303991 -4.2418447 -4.2347393 -4.2125454 -4.2036633][-4.2723312 -4.24145 -4.209156 -4.1901178 -4.18695 -4.1963143 -4.2066159 -4.2141581 -4.2330775 -4.2531552 -4.2718797 -4.2807083 -4.2729392 -4.2543039 -4.2444959][-4.2914233 -4.2680969 -4.2437963 -4.2276573 -4.2205658 -4.2247224 -4.2359881 -4.246892 -4.2678094 -4.2871222 -4.3018818 -4.3078246 -4.3029385 -4.2890563 -4.2789822][-4.30456 -4.2852321 -4.2663603 -4.2573485 -4.2577791 -4.2645545 -4.2750969 -4.2834587 -4.29794 -4.3123665 -4.321023 -4.3241329 -4.321991 -4.3147683 -4.3069358][-4.31505 -4.2991242 -4.2860827 -4.28374 -4.2904949 -4.3002248 -4.3085437 -4.3128824 -4.3189931 -4.326293 -4.3300586 -4.3316064 -4.3305173 -4.3278613 -4.3230925]]...]
INFO - root - 2017-12-07 15:10:01.506158: step 22010, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 83h:32m:58s remains)
INFO - root - 2017-12-07 15:10:11.090243: step 22020, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 73h:19m:05s remains)
INFO - root - 2017-12-07 15:10:20.859922: step 22030, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 84h:46m:18s remains)
INFO - root - 2017-12-07 15:10:30.558422: step 22040, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 85h:00m:41s remains)
INFO - root - 2017-12-07 15:10:40.223292: step 22050, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 84h:32m:46s remains)
INFO - root - 2017-12-07 15:10:49.923625: step 22060, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 85h:04m:53s remains)
INFO - root - 2017-12-07 15:10:59.653660: step 22070, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 79h:49m:09s remains)
INFO - root - 2017-12-07 15:11:09.442220: step 22080, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 85h:22m:46s remains)
INFO - root - 2017-12-07 15:11:19.094185: step 22090, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.002 sec/batch; 86h:23m:24s remains)
INFO - root - 2017-12-07 15:11:28.820827: step 22100, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.965 sec/batch; 83h:10m:30s remains)
2017-12-07 15:11:29.814435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2762475 -4.255477 -4.2469854 -4.2500243 -4.2619238 -4.2692962 -4.2556677 -4.224205 -4.1866708 -4.1733589 -4.1819968 -4.2037125 -4.2422228 -4.2796574 -4.3064556][-4.2361164 -4.220799 -4.2195907 -4.2320442 -4.2516322 -4.2624044 -4.2471876 -4.2137203 -4.1788034 -4.1738882 -4.1889806 -4.2125063 -4.2476883 -4.2820888 -4.3084702][-4.1894884 -4.1803393 -4.1863894 -4.2050285 -4.22301 -4.2271137 -4.2118764 -4.1901383 -4.1732616 -4.1818767 -4.206861 -4.2329841 -4.2616043 -4.2887278 -4.3116212][-4.1584234 -4.1582508 -4.1673779 -4.1807451 -4.1859984 -4.1766305 -4.1604486 -4.1526642 -4.1572719 -4.1814032 -4.2188196 -4.2502227 -4.2746735 -4.2974887 -4.3159347][-4.13901 -4.150682 -4.1669168 -4.175878 -4.1657085 -4.1361237 -4.1105905 -4.1074462 -4.1270213 -4.1680584 -4.2173862 -4.2519183 -4.2790627 -4.3039141 -4.3194633][-4.1306844 -4.156662 -4.1803703 -4.18495 -4.1587567 -4.1059237 -4.0626879 -4.0580196 -4.090127 -4.1500654 -4.2107286 -4.2502494 -4.281188 -4.3084478 -4.3230491][-4.1453657 -4.1810884 -4.2099185 -4.2043304 -4.1625853 -4.0937271 -4.0368872 -4.0308909 -4.0713859 -4.1402297 -4.2033682 -4.2452168 -4.278193 -4.3066435 -4.3224463][-4.1911688 -4.2236385 -4.24783 -4.2356739 -4.1893096 -4.1242228 -4.0707712 -4.0612268 -4.0956922 -4.1530905 -4.204514 -4.2452612 -4.2785311 -4.3062658 -4.3213983][-4.23662 -4.259 -4.2779078 -4.2697091 -4.2365022 -4.1936245 -4.1534104 -4.1430883 -4.1638393 -4.1960626 -4.2290044 -4.2611609 -4.2907319 -4.3134379 -4.325057][-4.2650681 -4.2823091 -4.296577 -4.2959976 -4.2799053 -4.2541976 -4.22404 -4.21662 -4.2303734 -4.2459092 -4.2669587 -4.2901711 -4.3130813 -4.328567 -4.3333826][-4.2913523 -4.3027673 -4.3084722 -4.30478 -4.2973328 -4.2823458 -4.2612348 -4.2610221 -4.2761536 -4.288765 -4.3051209 -4.322422 -4.3382111 -4.3440852 -4.3414292][-4.3125815 -4.321022 -4.3178544 -4.3063412 -4.2972078 -4.2876248 -4.2737389 -4.2786326 -4.2963915 -4.3113708 -4.3269939 -4.340776 -4.3508191 -4.3499269 -4.34287][-4.3260074 -4.3322654 -4.3247671 -4.3119268 -4.2994447 -4.2909889 -4.2845392 -4.292882 -4.3061595 -4.3174877 -4.329535 -4.3405638 -4.3469353 -4.3446689 -4.3388844][-4.3232918 -4.3289814 -4.325068 -4.3190536 -4.31242 -4.3060331 -4.3019791 -4.306366 -4.3119912 -4.3176627 -4.3253231 -4.3325319 -4.3373089 -4.3372254 -4.3351264][-4.3169045 -4.3222823 -4.322813 -4.3223042 -4.3207812 -4.3184242 -4.3156953 -4.3170896 -4.3183432 -4.31901 -4.32194 -4.326232 -4.3310623 -4.3333797 -4.3342304]]...]
INFO - root - 2017-12-07 15:11:39.380282: step 22110, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 75h:16m:02s remains)
INFO - root - 2017-12-07 15:11:49.024951: step 22120, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 82h:26m:57s remains)
INFO - root - 2017-12-07 15:11:58.559422: step 22130, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 81h:02m:12s remains)
INFO - root - 2017-12-07 15:12:08.178437: step 22140, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 83h:12m:10s remains)
INFO - root - 2017-12-07 15:12:17.869418: step 22150, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 86h:33m:39s remains)
INFO - root - 2017-12-07 15:12:27.619919: step 22160, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 82h:01m:07s remains)
INFO - root - 2017-12-07 15:12:37.451751: step 22170, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 83h:19m:20s remains)
INFO - root - 2017-12-07 15:12:46.983059: step 22180, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 81h:17m:37s remains)
INFO - root - 2017-12-07 15:12:56.706435: step 22190, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 87h:08m:04s remains)
INFO - root - 2017-12-07 15:13:06.215878: step 22200, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.981 sec/batch; 84h:35m:22s remains)
2017-12-07 15:13:07.286651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2172289 -4.1983805 -4.1820154 -4.1752925 -4.1880937 -4.2132978 -4.2418394 -4.2693725 -4.2902551 -4.30019 -4.2998805 -4.2900004 -4.2776322 -4.2717609 -4.2742925][-4.2032743 -4.1884418 -4.1705108 -4.1615844 -4.1792765 -4.2105632 -4.2414846 -4.2692375 -4.2940392 -4.3106461 -4.3151703 -4.3060722 -4.2937374 -4.288157 -4.2904][-4.1833014 -4.1779375 -4.1604981 -4.1513767 -4.1755662 -4.2113681 -4.2437954 -4.2690773 -4.2948966 -4.3163943 -4.3254051 -4.318893 -4.3068905 -4.2994094 -4.2978144][-4.1643867 -4.1666026 -4.1508431 -4.1438994 -4.1709433 -4.2071033 -4.2372203 -4.2589564 -4.2845345 -4.3089533 -4.3219647 -4.3196287 -4.3085651 -4.2986488 -4.2928991][-4.162899 -4.1637521 -4.1465254 -4.1424136 -4.1680779 -4.2007236 -4.224709 -4.2398062 -4.2637749 -4.2909837 -4.3075542 -4.3102541 -4.3002243 -4.2873111 -4.2783461][-4.1798506 -4.1742539 -4.1551266 -4.1497836 -4.1684341 -4.1900229 -4.2020822 -4.208025 -4.2321162 -4.2655263 -4.288929 -4.2984285 -4.2905455 -4.2728686 -4.2584271][-4.2048783 -4.1932054 -4.1710725 -4.1594291 -4.1656852 -4.1723218 -4.1675811 -4.1609745 -4.1857419 -4.2287526 -4.2623682 -4.280622 -4.2772131 -4.257319 -4.2383523][-4.2272434 -4.2099934 -4.1840148 -4.1652641 -4.1569571 -4.1447873 -4.1194296 -4.0948777 -4.1176047 -4.173285 -4.2196536 -4.2475872 -4.250504 -4.234416 -4.2160759][-4.2399817 -4.2188869 -4.1940627 -4.17528 -4.1577592 -4.1263089 -4.0765557 -4.0290518 -4.0443263 -4.1112561 -4.1697445 -4.2074347 -4.2202969 -4.2128448 -4.1989565][-4.2373924 -4.218298 -4.2023015 -4.1930914 -4.177043 -4.1354837 -4.0674396 -3.9992459 -3.9975278 -4.0622029 -4.1245914 -4.169229 -4.1941166 -4.1989446 -4.1936507][-4.2214813 -4.2122941 -4.2077379 -4.2108736 -4.2050328 -4.171051 -4.1048269 -4.0326753 -4.0139847 -4.0609045 -4.1121893 -4.1538463 -4.1856771 -4.2007217 -4.2049351][-4.2119427 -4.2151322 -4.2212391 -4.2340274 -4.2391038 -4.221189 -4.1707544 -4.1083751 -4.0825596 -4.1083522 -4.1415367 -4.1717377 -4.2018638 -4.2193465 -4.2244349][-4.2304268 -4.2423592 -4.2544317 -4.2699594 -4.2782369 -4.27024 -4.2358994 -4.1884456 -4.1641226 -4.1748652 -4.1914911 -4.207366 -4.2289186 -4.2413907 -4.239368][-4.2673168 -4.2821422 -4.294785 -4.3066411 -4.3124495 -4.3107381 -4.2886052 -4.2530913 -4.2328477 -4.2358794 -4.2422762 -4.2483921 -4.2609143 -4.2661743 -4.2571521][-4.3034325 -4.3130083 -4.3205214 -4.3264604 -4.3318663 -4.3368912 -4.326664 -4.3024683 -4.2848134 -4.2822032 -4.283114 -4.2849541 -4.2913976 -4.2910209 -4.2759337]]...]
INFO - root - 2017-12-07 15:13:17.097047: step 22210, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 80h:22m:35s remains)
INFO - root - 2017-12-07 15:13:26.709666: step 22220, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 82h:40m:07s remains)
INFO - root - 2017-12-07 15:13:36.312045: step 22230, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 78h:13m:17s remains)
INFO - root - 2017-12-07 15:13:45.876771: step 22240, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 80h:53m:38s remains)
INFO - root - 2017-12-07 15:13:55.384907: step 22250, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 82h:19m:59s remains)
INFO - root - 2017-12-07 15:14:05.086568: step 22260, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 82h:44m:49s remains)
INFO - root - 2017-12-07 15:14:14.847763: step 22270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 83h:54m:55s remains)
INFO - root - 2017-12-07 15:14:24.473196: step 22280, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 83h:01m:41s remains)
INFO - root - 2017-12-07 15:14:34.078003: step 22290, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 82h:44m:41s remains)
INFO - root - 2017-12-07 15:14:43.850149: step 22300, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 79h:49m:36s remains)
2017-12-07 15:14:44.828704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3163452 -4.3072939 -4.2985153 -4.2914844 -4.2873359 -4.2836466 -4.2822237 -4.2857995 -4.2887673 -4.2919493 -4.2970285 -4.2985497 -4.2938552 -4.2894831 -4.2913151][-4.3070254 -4.293222 -4.2793465 -4.2662959 -4.2555933 -4.2447662 -4.2393093 -4.2454181 -4.25173 -4.2619376 -4.276495 -4.2836461 -4.2773929 -4.2683935 -4.2694678][-4.2982979 -4.2826333 -4.2643962 -4.2462826 -4.2273488 -4.20811 -4.1969347 -4.2014122 -4.2086768 -4.2271996 -4.2522769 -4.2638726 -4.2550111 -4.2416539 -4.2403116][-4.2918196 -4.2737656 -4.2525182 -4.2294326 -4.2055812 -4.1824341 -4.165689 -4.1662197 -4.17329 -4.1970649 -4.2265973 -4.2353296 -4.2226329 -4.2091532 -4.2088361][-4.280468 -4.2594671 -4.2319965 -4.1996269 -4.1680069 -4.1420984 -4.1235776 -4.1269755 -4.1395464 -4.1680646 -4.1960354 -4.2005286 -4.1862011 -4.1751056 -4.1759095][-4.2654648 -4.2379618 -4.1982183 -4.1493077 -4.0991306 -4.0577536 -4.0271091 -4.0422893 -4.0778832 -4.1198535 -4.1506877 -4.1586785 -4.148489 -4.1410918 -4.1404858][-4.2498789 -4.216465 -4.1648016 -4.0988784 -4.0255051 -3.954273 -3.8962085 -3.9270666 -3.9971893 -4.0634284 -4.1080565 -4.1295238 -4.1277785 -4.12284 -4.1189027][-4.2427869 -4.2071929 -4.1560087 -4.09106 -4.0203147 -3.950633 -3.8930006 -3.9229703 -3.9856148 -4.0484638 -4.0954361 -4.1214404 -4.1244612 -4.1218548 -4.1157451][-4.2471695 -4.2159882 -4.1746907 -4.1240268 -4.0751739 -4.0328197 -4.0053062 -4.0273948 -4.0573893 -4.0919242 -4.1183496 -4.1310768 -4.13249 -4.1327248 -4.126853][-4.2611337 -4.2327628 -4.1960654 -4.1535239 -4.1169205 -4.0895009 -4.0804725 -4.1025763 -4.122261 -4.146821 -4.1601973 -4.1630039 -4.1620808 -4.1606374 -4.1529193][-4.2759528 -4.252593 -4.2221832 -4.1854429 -4.1516109 -4.1243162 -4.1185603 -4.1386232 -4.1586838 -4.1838441 -4.1950488 -4.1966987 -4.1942306 -4.1925783 -4.1834006][-4.2915478 -4.2723742 -4.2450633 -4.2121515 -4.1846247 -4.1628833 -4.1581016 -4.1735411 -4.1919646 -4.2124729 -4.2216768 -4.2266383 -4.2262 -4.2254348 -4.2177038][-4.2951226 -4.2786674 -4.2556853 -4.2314243 -4.215085 -4.2040262 -4.205029 -4.2167168 -4.2250109 -4.233717 -4.2369523 -4.2461782 -4.2522745 -4.2527757 -4.2454181][-4.2973547 -4.28338 -4.2657232 -4.2492037 -4.241034 -4.2387643 -4.2426944 -4.2504478 -4.2524505 -4.251298 -4.2479911 -4.2564721 -4.2658291 -4.2677479 -4.2573795][-4.2980194 -4.28703 -4.2746415 -4.2633557 -4.2587638 -4.2619071 -4.26822 -4.2735662 -4.271975 -4.2647452 -4.2578626 -4.2609425 -4.2690892 -4.2735939 -4.2675605]]...]
INFO - root - 2017-12-07 15:14:54.525293: step 22310, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 86h:35m:36s remains)
INFO - root - 2017-12-07 15:15:04.043962: step 22320, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 82h:25m:17s remains)
INFO - root - 2017-12-07 15:15:13.775009: step 22330, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 82h:58m:40s remains)
INFO - root - 2017-12-07 15:15:23.467262: step 22340, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.990 sec/batch; 85h:17m:05s remains)
INFO - root - 2017-12-07 15:15:33.123002: step 22350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 80h:23m:36s remains)
INFO - root - 2017-12-07 15:15:42.862209: step 22360, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 81h:45m:26s remains)
INFO - root - 2017-12-07 15:15:52.482271: step 22370, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 82h:57m:26s remains)
INFO - root - 2017-12-07 15:16:02.219832: step 22380, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.994 sec/batch; 85h:39m:38s remains)
INFO - root - 2017-12-07 15:16:11.927589: step 22390, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 86h:05m:04s remains)
INFO - root - 2017-12-07 15:16:21.644111: step 22400, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 82h:30m:34s remains)
2017-12-07 15:16:22.644813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3042789 -4.3150406 -4.3216648 -4.3191743 -4.3124328 -4.3030982 -4.2944179 -4.2906733 -4.2956753 -4.3005705 -4.3017712 -4.2995381 -4.2956815 -4.291532 -4.2940192][-4.2999988 -4.3140078 -4.3220534 -4.3184114 -4.3101645 -4.2994442 -4.2916856 -4.2918372 -4.3030553 -4.3095689 -4.3053517 -4.2952585 -4.2862082 -4.2800984 -4.2825265][-4.2940016 -4.3065853 -4.3111644 -4.3023915 -4.2906551 -4.277667 -4.2718992 -4.278163 -4.2965903 -4.3041763 -4.2902555 -4.2681208 -4.2521553 -4.2432995 -4.2477651][-4.2877131 -4.2930527 -4.2895913 -4.273396 -4.2580442 -4.2422829 -4.2340355 -4.2429285 -4.2635422 -4.2680645 -4.24167 -4.2078404 -4.1877608 -4.1794791 -4.1910038][-4.2851644 -4.2820039 -4.2692137 -4.2463446 -4.2271214 -4.2058239 -4.19121 -4.1970472 -4.2176118 -4.217833 -4.1793513 -4.1355324 -4.11403 -4.1110964 -4.1305814][-4.2864995 -4.2748327 -4.253499 -4.2236218 -4.1941986 -4.1595325 -4.1327515 -4.1330981 -4.1550517 -4.1592197 -4.1205163 -4.0756731 -4.0550218 -4.0566769 -4.0824113][-4.284389 -4.2637916 -4.23509 -4.197998 -4.15557 -4.1058149 -4.0677328 -4.0652556 -4.0955915 -4.114759 -4.0874953 -4.0461111 -4.0239673 -4.0280595 -4.0577469][-4.2752 -4.2489247 -4.217052 -4.1776438 -4.1296496 -4.069201 -4.021317 -4.01658 -4.0526495 -4.0845828 -4.0726686 -4.0384283 -4.0177207 -4.0231276 -4.0557904][-4.261344 -4.23603 -4.2101169 -4.1792107 -4.1383433 -4.0786886 -4.024478 -4.0112319 -4.0418968 -4.0733714 -4.07014 -4.0473309 -4.0359273 -4.0434866 -4.0768576][-4.2415586 -4.2217555 -4.2078857 -4.190958 -4.1647811 -4.1156459 -4.0603175 -4.0369821 -4.0559773 -4.0800266 -4.0791221 -4.0664406 -4.0660267 -4.0768013 -4.1079659][-4.2233539 -4.2080464 -4.2052126 -4.2051573 -4.1955752 -4.1590552 -4.1085258 -4.0783091 -4.0852561 -4.1013684 -4.0991292 -4.0906148 -4.0926189 -4.1021833 -4.1250534][-4.2160692 -4.2001591 -4.20209 -4.2116561 -4.2130318 -4.1899848 -4.1488681 -4.1188369 -4.1178846 -4.125937 -4.1201029 -4.1118846 -4.1110649 -4.1168442 -4.134037][-4.2231174 -4.2037015 -4.2045493 -4.2159634 -4.224256 -4.2129641 -4.1860509 -4.1643782 -4.163362 -4.1688509 -4.1605492 -4.1471777 -4.1399956 -4.1415992 -4.1550493][-4.23749 -4.2159734 -4.2151823 -4.224967 -4.2351251 -4.2335458 -4.2212567 -4.2088847 -4.2086058 -4.2128563 -4.2043443 -4.1881709 -4.1790323 -4.1819744 -4.1979403][-4.2549224 -4.2343 -4.2332463 -4.2430172 -4.2531481 -4.256434 -4.2514548 -4.2447505 -4.2438717 -4.2447991 -4.2358141 -4.2211628 -4.2145414 -4.223835 -4.2435842]]...]
INFO - root - 2017-12-07 15:16:32.286033: step 22410, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 87h:06m:59s remains)
INFO - root - 2017-12-07 15:16:41.996242: step 22420, loss = 2.11, batch loss = 2.05 (7.9 examples/sec; 1.007 sec/batch; 86h:42m:16s remains)
INFO - root - 2017-12-07 15:16:51.716375: step 22430, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.952 sec/batch; 81h:58m:31s remains)
INFO - root - 2017-12-07 15:17:01.275329: step 22440, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 79h:58m:26s remains)
INFO - root - 2017-12-07 15:17:10.853219: step 22450, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 84h:49m:44s remains)
INFO - root - 2017-12-07 15:17:20.418642: step 22460, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 81h:08m:57s remains)
INFO - root - 2017-12-07 15:17:30.088111: step 22470, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 85h:05m:30s remains)
INFO - root - 2017-12-07 15:17:39.766809: step 22480, loss = 2.11, batch loss = 2.06 (8.5 examples/sec; 0.938 sec/batch; 80h:46m:00s remains)
INFO - root - 2017-12-07 15:17:49.537761: step 22490, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 82h:01m:52s remains)
INFO - root - 2017-12-07 15:17:59.210532: step 22500, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 81h:02m:58s remains)
2017-12-07 15:18:00.203710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3079271 -4.3068867 -4.3047829 -4.3028107 -4.3025765 -4.3050151 -4.3113117 -4.3176446 -4.3191524 -4.3131051 -4.3005753 -4.2826128 -4.2667379 -4.258647 -4.2569642][-4.3087778 -4.3072629 -4.3031774 -4.2975287 -4.2931976 -4.2934914 -4.2997932 -4.3074121 -4.3092375 -4.302423 -4.2904534 -4.2760668 -4.265873 -4.2629571 -4.2655077][-4.3099566 -4.3078785 -4.3015332 -4.2912917 -4.2816525 -4.2781835 -4.2816396 -4.286983 -4.2862082 -4.2796063 -4.2725759 -4.2673364 -4.2667608 -4.2679186 -4.2704515][-4.3154583 -4.3132687 -4.305088 -4.290391 -4.2750473 -4.2665992 -4.2658052 -4.2660379 -4.2593818 -4.2513938 -4.2482471 -4.2502828 -4.2568188 -4.2617345 -4.2657652][-4.3213615 -4.3199954 -4.3118057 -4.29435 -4.2741675 -4.260716 -4.2539334 -4.24734 -4.2338119 -4.2221904 -4.2180538 -4.2208419 -4.2299724 -4.2400384 -4.248105][-4.3249469 -4.3250918 -4.3176951 -4.2997 -4.2774825 -4.2601109 -4.2468438 -4.23273 -4.2126865 -4.1947465 -4.1837459 -4.1825347 -4.19346 -4.2105217 -4.22377][-4.3264456 -4.3277359 -4.3208275 -4.3039041 -4.2823486 -4.2641282 -4.2460613 -4.2250314 -4.2002454 -4.1747007 -4.1530304 -4.1434374 -4.1540203 -4.1782055 -4.196291][-4.3255544 -4.3268628 -4.3191662 -4.3031526 -4.283534 -4.2664838 -4.2465615 -4.2213216 -4.1931515 -4.1601033 -4.1284461 -4.1113868 -4.123539 -4.1535077 -4.1744418][-4.3229804 -4.3239918 -4.3145285 -4.2983103 -4.2792158 -4.2619548 -4.2406168 -4.211843 -4.1781821 -4.138433 -4.0998855 -4.0824471 -4.1026368 -4.1388412 -4.1604433][-4.3192253 -4.3196654 -4.3064408 -4.2874932 -4.2663841 -4.2466664 -4.2243629 -4.1956444 -4.15759 -4.1126256 -4.0696855 -4.0566716 -4.0862517 -4.1261177 -4.1471105][-4.3156824 -4.3154063 -4.2987061 -4.2771397 -4.2543907 -4.2352905 -4.2167091 -4.1925035 -4.1541362 -4.10639 -4.0590963 -4.0462823 -4.0777974 -4.1154685 -4.1359076][-4.3091574 -4.3095112 -4.2918777 -4.2674794 -4.2430997 -4.2265348 -4.215641 -4.1993833 -4.1632371 -4.114388 -4.0663118 -4.0478644 -4.0727849 -4.1065831 -4.1265993][-4.2975159 -4.3005815 -4.2847319 -4.2599621 -4.2366028 -4.2228065 -4.2174034 -4.2070141 -4.1771021 -4.137476 -4.09688 -4.0752773 -4.0888634 -4.1154671 -4.1306958][-4.2833853 -4.28769 -4.27595 -4.2545619 -4.2347307 -4.2252417 -4.2283764 -4.2297873 -4.2123227 -4.1838489 -4.1509686 -4.1249576 -4.1240335 -4.1370368 -4.1404638][-4.2610307 -4.2653813 -4.2595077 -4.2453065 -4.2312579 -4.2282925 -4.240355 -4.2525697 -4.2452779 -4.22555 -4.1967921 -4.1653595 -4.1494994 -4.1496544 -4.1437674]]...]
INFO - root - 2017-12-07 15:18:09.892558: step 22510, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.035 sec/batch; 89h:08m:22s remains)
INFO - root - 2017-12-07 15:18:19.551827: step 22520, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 85h:52m:19s remains)
INFO - root - 2017-12-07 15:18:29.230101: step 22530, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 80h:19m:08s remains)
INFO - root - 2017-12-07 15:18:38.872249: step 22540, loss = 2.10, batch loss = 2.05 (7.9 examples/sec; 1.007 sec/batch; 86h:43m:37s remains)
INFO - root - 2017-12-07 15:18:48.606567: step 22550, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 87h:05m:39s remains)
INFO - root - 2017-12-07 15:18:58.460054: step 22560, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.990 sec/batch; 85h:13m:12s remains)
INFO - root - 2017-12-07 15:19:08.109827: step 22570, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.993 sec/batch; 85h:28m:42s remains)
INFO - root - 2017-12-07 15:19:17.818955: step 22580, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 81h:49m:04s remains)
INFO - root - 2017-12-07 15:19:27.623998: step 22590, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 83h:01m:49s remains)
INFO - root - 2017-12-07 15:19:37.155853: step 22600, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 84h:42m:45s remains)
2017-12-07 15:19:38.102047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2028542 -4.1798086 -4.1563325 -4.137352 -4.130496 -4.1495976 -4.1625886 -4.1653857 -4.1757903 -4.1889782 -4.2015758 -4.1966934 -4.1874256 -4.1867108 -4.1891174][-4.2083249 -4.1797409 -4.143393 -4.11348 -4.10896 -4.1388507 -4.1615934 -4.1731868 -4.1892886 -4.2062569 -4.2167535 -4.2056413 -4.18412 -4.1681485 -4.1614842][-4.2179484 -4.1928806 -4.1543794 -4.1166444 -4.1100183 -4.1341763 -4.1524277 -4.1645355 -4.1842556 -4.2061133 -4.2222881 -4.2179279 -4.1920638 -4.1639256 -4.1481438][-4.2277565 -4.2193866 -4.1858854 -4.1478004 -4.134973 -4.1388125 -4.1307006 -4.1243906 -4.1503944 -4.1925745 -4.225986 -4.2337766 -4.2134671 -4.1851053 -4.168633][-4.2320185 -4.243556 -4.2179675 -4.1833692 -4.1677489 -4.1482692 -4.0979214 -4.0530391 -4.0880589 -4.1586256 -4.2119322 -4.23646 -4.2285767 -4.2149878 -4.2141404][-4.2352872 -4.2582774 -4.2395973 -4.2071509 -4.1847491 -4.1359963 -4.0352879 -3.9455056 -3.9954205 -4.1019559 -4.1816263 -4.2247353 -4.2343593 -4.2394128 -4.2518573][-4.2255106 -4.2540627 -4.2424412 -4.2133479 -4.1859155 -4.1183157 -3.9889426 -3.884835 -3.9526057 -4.0769882 -4.168663 -4.2204165 -4.2448649 -4.2593102 -4.2778273][-4.1955557 -4.2344379 -4.2351356 -4.2183108 -4.1992536 -4.13462 -4.0299926 -3.9568751 -4.0123911 -4.1134491 -4.1843657 -4.2298803 -4.2597656 -4.2806091 -4.3000674][-4.1544204 -4.2086945 -4.2292528 -4.2285976 -4.2194042 -4.1662388 -4.0942373 -4.0533013 -4.0951848 -4.1660419 -4.2162156 -4.2496319 -4.2755275 -4.2939882 -4.3104305][-4.1200042 -4.1875062 -4.2269211 -4.2390952 -4.2352743 -4.1964464 -4.1488523 -4.134738 -4.1696362 -4.2172408 -4.249702 -4.2683573 -4.2829623 -4.2934322 -4.3049889][-4.101264 -4.1778808 -4.2279954 -4.2477903 -4.2516546 -4.2302976 -4.2093959 -4.2153091 -4.242516 -4.26899 -4.2805014 -4.285614 -4.2896867 -4.2942648 -4.3016925][-4.1070313 -4.1785502 -4.2292056 -4.25552 -4.2703376 -4.2659979 -4.2630825 -4.2725058 -4.2890491 -4.3021321 -4.3018012 -4.2998176 -4.299809 -4.3025336 -4.3082957][-4.125452 -4.1843538 -4.2291131 -4.2588687 -4.28117 -4.2890329 -4.2928319 -4.2992115 -4.3080325 -4.3143077 -4.31118 -4.3099794 -4.31038 -4.3137517 -4.3194003][-4.14625 -4.1920381 -4.2292848 -4.2595158 -4.2852411 -4.2980423 -4.3026996 -4.3060708 -4.3114543 -4.3146257 -4.3140659 -4.3160076 -4.3170958 -4.3200712 -4.3249793][-4.1596775 -4.2008991 -4.2361126 -4.2669039 -4.2919483 -4.305696 -4.3125353 -4.3140845 -4.31515 -4.3169436 -4.3183336 -4.3214726 -4.3235855 -4.3258681 -4.3287168]]...]
INFO - root - 2017-12-07 15:19:47.538595: step 22610, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 79h:55m:44s remains)
INFO - root - 2017-12-07 15:19:56.985908: step 22620, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 82h:32m:29s remains)
INFO - root - 2017-12-07 15:20:06.639304: step 22630, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 84h:06m:55s remains)
INFO - root - 2017-12-07 15:20:16.406743: step 22640, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 83h:53m:09s remains)
INFO - root - 2017-12-07 15:20:26.008901: step 22650, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 80h:19m:33s remains)
INFO - root - 2017-12-07 15:20:35.705725: step 22660, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 82h:50m:20s remains)
INFO - root - 2017-12-07 15:20:45.431476: step 22670, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 85h:33m:16s remains)
INFO - root - 2017-12-07 15:20:55.020761: step 22680, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 85h:19m:53s remains)
INFO - root - 2017-12-07 15:21:04.763367: step 22690, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 86h:29m:54s remains)
INFO - root - 2017-12-07 15:21:14.369731: step 22700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 74h:10m:00s remains)
2017-12-07 15:21:15.292772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3064561 -4.2920446 -4.2748022 -4.2543616 -4.2381916 -4.2267084 -4.2211518 -4.2203736 -4.2243147 -4.2366319 -4.2510152 -4.2590981 -4.255435 -4.2518039 -4.25144][-4.2962761 -4.2778053 -4.2541695 -4.2251854 -4.2035522 -4.1881752 -4.180048 -4.1798224 -4.183497 -4.1939225 -4.2085 -4.2198014 -4.2148781 -4.2075944 -4.205564][-4.2861466 -4.2628846 -4.2295966 -4.1873608 -4.1544771 -4.1337695 -4.1280379 -4.1360955 -4.1497564 -4.1636457 -4.1826077 -4.1939588 -4.1823249 -4.1615005 -4.1512804][-4.2744379 -4.2433691 -4.1961551 -4.1352897 -4.0835571 -4.0530243 -4.0525913 -4.0781436 -4.1134343 -4.1418295 -4.17111 -4.1833019 -4.1654329 -4.1333094 -4.1074319][-4.2617831 -4.2224779 -4.1615849 -4.0808773 -4.0059109 -3.9510286 -3.9447639 -3.9934359 -4.0644283 -4.114686 -4.1540833 -4.1703782 -4.1552553 -4.1237378 -4.0881891][-4.2498126 -4.2020769 -4.131403 -4.0362387 -3.9413023 -3.8505673 -3.8137579 -3.8808458 -3.9927876 -4.0692639 -4.1178265 -4.1453462 -4.1410155 -4.1149416 -4.0761728][-4.2450452 -4.1904173 -4.1125374 -4.0106244 -3.9039705 -3.7769184 -3.683394 -3.7440214 -3.8925617 -3.9982386 -4.0515847 -4.0834565 -4.0938144 -4.0840216 -4.0548615][-4.248404 -4.1928978 -4.1123672 -4.0130491 -3.9108205 -3.7773325 -3.6494193 -3.6816468 -3.8353314 -3.9503031 -3.9997358 -4.0278316 -4.0435352 -4.0524969 -4.039887][-4.25711 -4.2103338 -4.1397634 -4.0586982 -3.985615 -3.8914094 -3.7921546 -3.7949419 -3.8946376 -3.9775853 -4.0085583 -4.0215459 -4.030179 -4.0479631 -4.0552487][-4.26639 -4.232862 -4.1787319 -4.1215935 -4.0814009 -4.0303173 -3.9749732 -3.96495 -4.0022249 -4.0432854 -4.0615983 -4.0681429 -4.0629821 -4.0762 -4.0909567][-4.2765779 -4.2537656 -4.2145467 -4.1757989 -4.1576943 -4.1348958 -4.1072588 -4.0919456 -4.091125 -4.103394 -4.1186366 -4.1261654 -4.116992 -4.1225219 -4.1339011][-4.2880621 -4.2713227 -4.2443438 -4.2187538 -4.2077246 -4.1989608 -4.1878133 -4.1754394 -4.1640906 -4.1653776 -4.1756473 -4.1846719 -4.1771369 -4.1743679 -4.1778316][-4.2967272 -4.2832808 -4.264605 -4.2489381 -4.2418308 -4.2404637 -4.2405181 -4.238039 -4.2305293 -4.2241459 -4.2249646 -4.2307696 -4.2282677 -4.2217026 -4.2166381][-4.3055668 -4.2954292 -4.2827353 -4.2729335 -4.2674007 -4.2677808 -4.2713046 -4.2739797 -4.2731676 -4.2674603 -4.2637887 -4.2661381 -4.2683616 -4.2670321 -4.2630582][-4.3159256 -4.3086944 -4.3000879 -4.2931528 -4.2886925 -4.2881265 -4.2916393 -4.295774 -4.2980704 -4.2956138 -4.293704 -4.2958407 -4.2989478 -4.3002734 -4.2988696]]...]
INFO - root - 2017-12-07 15:21:24.933092: step 22710, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 82h:10m:03s remains)
INFO - root - 2017-12-07 15:21:34.591095: step 22720, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 84h:16m:00s remains)
INFO - root - 2017-12-07 15:21:44.188740: step 22730, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 76h:09m:23s remains)
INFO - root - 2017-12-07 15:21:53.836104: step 22740, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.924 sec/batch; 79h:29m:12s remains)
INFO - root - 2017-12-07 15:22:03.450221: step 22750, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 82h:08m:59s remains)
INFO - root - 2017-12-07 15:22:13.343840: step 22760, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 80h:13m:14s remains)
INFO - root - 2017-12-07 15:22:23.022779: step 22770, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 79h:24m:31s remains)
INFO - root - 2017-12-07 15:22:32.648048: step 22780, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 83h:02m:18s remains)
INFO - root - 2017-12-07 15:22:42.172490: step 22790, loss = 2.06, batch loss = 2.01 (7.8 examples/sec; 1.024 sec/batch; 88h:03m:42s remains)
INFO - root - 2017-12-07 15:22:51.824985: step 22800, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 77h:25m:59s remains)
2017-12-07 15:22:52.831111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2352362 -4.239819 -4.2301641 -4.2195725 -4.2138233 -4.2159057 -4.2228723 -4.2323451 -4.2398391 -4.2443242 -4.2497864 -4.2634106 -4.2769732 -4.2859278 -4.2936115][-4.2195272 -4.2236357 -4.2097573 -4.1979475 -4.1992483 -4.2090917 -4.2201781 -4.2286377 -4.2363663 -4.2395558 -4.243917 -4.2587924 -4.2728062 -4.2805943 -4.2870507][-4.2251105 -4.2259078 -4.2013812 -4.1801391 -4.1813779 -4.1962833 -4.2113142 -4.2232943 -4.2380767 -4.2432523 -4.2475348 -4.2606192 -4.2720785 -4.2775993 -4.28228][-4.2227678 -4.2146854 -4.1870222 -4.15854 -4.1560764 -4.1747985 -4.1938133 -4.2098713 -4.2359085 -4.2512693 -4.2587976 -4.2689543 -4.2747631 -4.27547 -4.2771158][-4.2006168 -4.1906447 -4.1683464 -4.1366539 -4.1257992 -4.137393 -4.1540575 -4.1725435 -4.209177 -4.2410207 -4.2602129 -4.2753124 -4.2787714 -4.2739182 -4.2712789][-4.1767249 -4.1678491 -4.1557689 -4.1232166 -4.0986571 -4.0919371 -4.0937977 -4.1062531 -4.1530695 -4.2079277 -4.247633 -4.2740068 -4.2813268 -4.2751789 -4.267447][-4.1692338 -4.1562948 -4.1464586 -4.1135879 -4.0763083 -4.0619073 -4.0489225 -4.0456066 -4.0906167 -4.1632252 -4.2258096 -4.2646275 -4.2780166 -4.2766337 -4.2684426][-4.1816416 -4.16512 -4.1538658 -4.1250548 -4.0921283 -4.0776324 -4.049037 -4.019887 -4.0512218 -4.1316051 -4.2105188 -4.2553992 -4.2702351 -4.2757268 -4.2723484][-4.2127786 -4.1980152 -4.1948614 -4.1759343 -4.1497197 -4.1364961 -4.09616 -4.0451961 -4.0504251 -4.1201496 -4.2069688 -4.2551231 -4.2685828 -4.2773323 -4.2797413][-4.2547736 -4.2430754 -4.2451191 -4.236527 -4.2161064 -4.199615 -4.1578565 -4.1052694 -4.093482 -4.1437597 -4.22426 -4.2724347 -4.2837348 -4.2895255 -4.293395][-4.3072691 -4.2947025 -4.29667 -4.2921062 -4.2735262 -4.2560124 -4.22056 -4.1815262 -4.1692653 -4.2033529 -4.2661829 -4.3087449 -4.3171315 -4.3158951 -4.3142309][-4.3525572 -4.3385305 -4.3359628 -4.331048 -4.3145471 -4.2970114 -4.2702942 -4.2472897 -4.2404041 -4.2610192 -4.3058281 -4.3397865 -4.345202 -4.3388023 -4.3287759][-4.361249 -4.3501916 -4.3453369 -4.3389077 -4.3284292 -4.3137894 -4.2945414 -4.284265 -4.2830491 -4.2957082 -4.3233118 -4.3466005 -4.3498755 -4.3405676 -4.3274069][-4.3412843 -4.3339257 -4.3287997 -4.3214817 -4.3121018 -4.302032 -4.293314 -4.2920909 -4.2970943 -4.31006 -4.3253088 -4.3384457 -4.3402267 -4.33124 -4.3195448][-4.3118863 -4.3080044 -4.3026834 -4.2973685 -4.2900791 -4.282475 -4.2822413 -4.2887535 -4.2992058 -4.3121109 -4.3204193 -4.3259678 -4.325469 -4.3194103 -4.3126779]]...]
INFO - root - 2017-12-07 15:23:02.459765: step 22810, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 78h:54m:44s remains)
INFO - root - 2017-12-07 15:23:12.259222: step 22820, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 83h:07m:03s remains)
INFO - root - 2017-12-07 15:23:22.037277: step 22830, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 81h:08m:48s remains)
INFO - root - 2017-12-07 15:23:31.854162: step 22840, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 85h:53m:53s remains)
INFO - root - 2017-12-07 15:23:41.738866: step 22850, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 84h:30m:42s remains)
INFO - root - 2017-12-07 15:23:51.305078: step 22860, loss = 2.12, batch loss = 2.06 (8.4 examples/sec; 0.957 sec/batch; 82h:18m:35s remains)
INFO - root - 2017-12-07 15:24:00.872137: step 22870, loss = 2.03, batch loss = 1.98 (8.3 examples/sec; 0.964 sec/batch; 82h:55m:12s remains)
INFO - root - 2017-12-07 15:24:10.573081: step 22880, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 83h:36m:21s remains)
INFO - root - 2017-12-07 15:24:20.322739: step 22890, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.959 sec/batch; 82h:27m:58s remains)
INFO - root - 2017-12-07 15:24:29.931700: step 22900, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 82h:39m:35s remains)
2017-12-07 15:24:31.031542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3292947 -4.3263435 -4.3227139 -4.3173695 -4.3110151 -4.3061237 -4.3026581 -4.3001232 -4.2997804 -4.3008142 -4.3020086 -4.3032374 -4.3059864 -4.3098497 -4.3134317][-4.3286252 -4.3222427 -4.315659 -4.3070769 -4.2985783 -4.2916718 -4.2848086 -4.2790132 -4.2778268 -4.2811928 -4.2854958 -4.2910371 -4.2985678 -4.3058615 -4.3115058][-4.3201394 -4.3091321 -4.2986908 -4.2870502 -4.2752032 -4.2642126 -4.2520628 -4.2444005 -4.2442703 -4.2502818 -4.25818 -4.2692213 -4.2821755 -4.2928891 -4.3012695][-4.2976818 -4.2788014 -4.26005 -4.2400331 -4.2194428 -4.20306 -4.1919627 -4.1922097 -4.2008643 -4.2133188 -4.227128 -4.2454104 -4.2625551 -4.2745514 -4.2840056][-4.2488632 -4.2206831 -4.1929107 -4.1624813 -4.1293144 -4.1006117 -4.0816875 -4.089776 -4.1199427 -4.1521616 -4.1801596 -4.2097812 -4.2321391 -4.2450094 -4.2549386][-4.1719828 -4.1387258 -4.10439 -4.0643616 -4.0182629 -3.9699059 -3.9318001 -3.9473367 -4.005713 -4.0635271 -4.1124434 -4.1542869 -4.1803932 -4.1932559 -4.2038512][-4.08314 -4.04857 -4.0113516 -3.9710732 -3.918313 -3.844636 -3.7747037 -3.796392 -3.8831749 -3.9629192 -4.0313277 -4.0854964 -4.1124358 -4.12517 -4.1381664][-4.0231743 -3.9918644 -3.9575944 -3.9319737 -3.8987768 -3.8350744 -3.7632122 -3.7776084 -3.8459978 -3.9113727 -3.9764833 -4.0310373 -4.057219 -4.07172 -4.0859051][-4.0341754 -4.01388 -3.9932861 -3.9905503 -3.9915566 -3.9662414 -3.9263031 -3.9274154 -3.9512875 -3.9785392 -4.0199895 -4.0594606 -4.0769448 -4.0880814 -4.09839][-4.1108332 -4.105237 -4.0962138 -4.100955 -4.1116962 -4.106226 -4.0867496 -4.0794244 -4.0805068 -4.08826 -4.1132612 -4.1401458 -4.1496286 -4.1546707 -4.1578069][-4.1985779 -4.2012029 -4.1961722 -4.196497 -4.2024131 -4.2015185 -4.1888747 -4.1760616 -4.1682677 -4.1707439 -4.187706 -4.20725 -4.2163477 -4.2229395 -4.2267747][-4.2482767 -4.253283 -4.2507339 -4.2485332 -4.249907 -4.2500834 -4.2438107 -4.2346315 -4.2282252 -4.22867 -4.2369962 -4.2492695 -4.2559133 -4.2618446 -4.2664061][-4.2546873 -4.2588944 -4.2577953 -4.255136 -4.2556415 -4.2580023 -4.2568603 -4.2519794 -4.2492466 -4.2495289 -4.2504759 -4.2519288 -4.2518506 -4.2534571 -4.2574868][-4.2313008 -4.233511 -4.2334738 -4.2312837 -4.2319379 -4.2359152 -4.238132 -4.2369328 -4.2364655 -4.23545 -4.2344055 -4.2313972 -4.2263608 -4.2241697 -4.2278156][-4.2289047 -4.2283363 -4.2275481 -4.2259665 -4.2256241 -4.2272859 -4.2288733 -4.2295904 -4.2297859 -4.2281313 -4.2267146 -4.22327 -4.2187996 -4.2176056 -4.2211165]]...]
INFO - root - 2017-12-07 15:24:40.715781: step 22910, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 82h:23m:05s remains)
INFO - root - 2017-12-07 15:24:50.291193: step 22920, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 83h:55m:35s remains)
INFO - root - 2017-12-07 15:24:59.815956: step 22930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 83h:56m:48s remains)
INFO - root - 2017-12-07 15:25:09.511687: step 22940, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.930 sec/batch; 79h:58m:39s remains)
INFO - root - 2017-12-07 15:25:19.071050: step 22950, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 77h:53m:24s remains)
INFO - root - 2017-12-07 15:25:28.645168: step 22960, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.011 sec/batch; 86h:58m:02s remains)
INFO - root - 2017-12-07 15:25:38.371190: step 22970, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.994 sec/batch; 85h:27m:52s remains)
INFO - root - 2017-12-07 15:25:48.003343: step 22980, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 80h:34m:12s remains)
INFO - root - 2017-12-07 15:25:57.665688: step 22990, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.926 sec/batch; 79h:36m:06s remains)
INFO - root - 2017-12-07 15:26:07.196427: step 23000, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 86h:10m:56s remains)
2017-12-07 15:26:08.162003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2225118 -4.2362576 -4.2364874 -4.2150769 -4.1790709 -4.1421204 -4.125186 -4.134851 -4.1579943 -4.1774654 -4.1923552 -4.1968122 -4.1881866 -4.1719847 -4.1695204][-4.246757 -4.2603984 -4.2624631 -4.2445416 -4.2123814 -4.1739941 -4.1536684 -4.1640558 -4.1908965 -4.2160769 -4.2351375 -4.2426834 -4.2312412 -4.2002816 -4.1740966][-4.2460322 -4.2639589 -4.2718129 -4.2565818 -4.2266164 -4.1898017 -4.1682825 -4.1728139 -4.19568 -4.2183542 -4.2419739 -4.2566862 -4.2519317 -4.2238722 -4.1869764][-4.237102 -4.2508955 -4.2563138 -4.2431374 -4.2183704 -4.1834335 -4.1575308 -4.1512785 -4.1660094 -4.1895876 -4.2205949 -4.2438369 -4.24802 -4.2342138 -4.2043548][-4.2343979 -4.2398577 -4.2384191 -4.2302132 -4.210135 -4.1799955 -4.1535363 -4.1376042 -4.1391683 -4.1619525 -4.2008319 -4.2345643 -4.2458911 -4.2429008 -4.2298312][-4.223722 -4.2183666 -4.2133088 -4.2109895 -4.2020288 -4.1810493 -4.1608958 -4.14325 -4.13961 -4.1555452 -4.1915636 -4.2217259 -4.234148 -4.23951 -4.2421765][-4.2036362 -4.1901326 -4.1865273 -4.1884656 -4.1842875 -4.1689854 -4.1571975 -4.1467609 -4.14697 -4.1587992 -4.1843419 -4.2048492 -4.2098413 -4.2168465 -4.2272806][-4.2040682 -4.1822019 -4.1716118 -4.1680584 -4.158093 -4.1364293 -4.1274066 -4.1340423 -4.1521778 -4.1695414 -4.1880555 -4.1973834 -4.1959567 -4.2002215 -4.2104053][-4.206099 -4.1779737 -4.1604848 -4.1493387 -4.12849 -4.0997696 -4.0946431 -4.1200852 -4.1626697 -4.1957145 -4.2202058 -4.2287464 -4.2222877 -4.2187896 -4.2203774][-4.1830788 -4.1572089 -4.1445293 -4.1395226 -4.1229329 -4.0974908 -4.094255 -4.1252232 -4.1772861 -4.2209945 -4.2511058 -4.26279 -4.2644949 -4.2622 -4.2549462][-4.1769962 -4.1529732 -4.1510792 -4.167388 -4.1665764 -4.1499329 -4.1456475 -4.1621943 -4.1977749 -4.23243 -4.2575884 -4.2723465 -4.2840934 -4.28906 -4.2829471][-4.1739883 -4.1485176 -4.151535 -4.1839514 -4.2007489 -4.195456 -4.1959834 -4.2012458 -4.2128386 -4.230773 -4.2467518 -4.2591772 -4.2756705 -4.2858982 -4.2854667][-4.1563182 -4.1298575 -4.1319685 -4.1682844 -4.1911707 -4.1969023 -4.2039022 -4.205884 -4.2119193 -4.2265673 -4.23789 -4.2437634 -4.2577772 -4.2705665 -4.2755961][-4.1216521 -4.0969424 -4.1003366 -4.1435075 -4.1810522 -4.1967058 -4.2039337 -4.2039018 -4.2132907 -4.2279577 -4.2359886 -4.2374477 -4.2479339 -4.2613425 -4.2708974][-4.0761662 -4.0518508 -4.0541286 -4.1058364 -4.163806 -4.1917028 -4.1997209 -4.1983266 -4.2062039 -4.2145376 -4.2157059 -4.2158427 -4.2240477 -4.2367897 -4.2472949]]...]
INFO - root - 2017-12-07 15:26:17.840680: step 23010, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 82h:17m:39s remains)
INFO - root - 2017-12-07 15:26:27.479418: step 23020, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 80h:28m:40s remains)
INFO - root - 2017-12-07 15:26:37.129676: step 23030, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.993 sec/batch; 85h:21m:20s remains)
INFO - root - 2017-12-07 15:26:46.716764: step 23040, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 85h:18m:45s remains)
INFO - root - 2017-12-07 15:26:56.436321: step 23050, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 82h:46m:41s remains)
INFO - root - 2017-12-07 15:27:06.153265: step 23060, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 82h:47m:58s remains)
INFO - root - 2017-12-07 15:27:15.665965: step 23070, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 85h:32m:24s remains)
INFO - root - 2017-12-07 15:27:25.308333: step 23080, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 86h:10m:18s remains)
INFO - root - 2017-12-07 15:27:35.037515: step 23090, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 81h:24m:46s remains)
INFO - root - 2017-12-07 15:27:44.640975: step 23100, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 78h:06m:12s remains)
2017-12-07 15:27:45.655810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1973124 -4.2002611 -4.1960998 -4.1876698 -4.1754136 -4.1633286 -4.1591415 -4.1708097 -4.194581 -4.2203426 -4.2490544 -4.2739334 -4.2914891 -4.3024478 -4.3068643][-4.164609 -4.1797137 -4.1832943 -4.1791844 -4.1659255 -4.147203 -4.1320348 -4.1386356 -4.1626134 -4.1918507 -4.2322206 -4.26809 -4.2893338 -4.3002181 -4.3064003][-4.1332445 -4.1557527 -4.1636591 -4.1611366 -4.1474009 -4.1300774 -4.1155615 -4.1217852 -4.1489654 -4.1830368 -4.2280107 -4.263227 -4.279038 -4.2864671 -4.2943764][-4.0993843 -4.121676 -4.1307516 -4.1306572 -4.1263671 -4.1191559 -4.1089363 -4.1115918 -4.1366096 -4.1730909 -4.2121754 -4.2384043 -4.247036 -4.2509055 -4.2606449][-4.0717998 -4.0886831 -4.099349 -4.1045613 -4.1159716 -4.1172895 -4.1015563 -4.0853596 -4.0989828 -4.1408467 -4.1804137 -4.1993761 -4.2051864 -4.21055 -4.2221546][-4.071084 -4.0776048 -4.0840387 -4.0940585 -4.1105747 -4.1067448 -4.0662866 -4.0107579 -4.0039563 -4.0603237 -4.1179085 -4.1515341 -4.16998 -4.1833148 -4.1976929][-4.0933928 -4.0908585 -4.0916348 -4.0980372 -4.1028943 -4.0722656 -3.9852874 -3.8641772 -3.8316841 -3.9329152 -4.038939 -4.1025238 -4.1385775 -4.1608529 -4.179235][-4.1195955 -4.1144972 -4.10668 -4.1007752 -4.0909133 -4.0353065 -3.9173069 -3.7587166 -3.7226136 -3.8770132 -4.0232897 -4.1050148 -4.1470966 -4.1704082 -4.1870022][-4.1268773 -4.11902 -4.1030211 -4.0943313 -4.0898581 -4.0459495 -3.9572732 -3.8503666 -3.8421545 -3.9730878 -4.0949969 -4.1564846 -4.1812849 -4.1932368 -4.2011065][-4.122148 -4.1098375 -4.0914178 -4.0882335 -4.0986004 -4.0859671 -4.0434775 -3.9907994 -3.9910722 -4.0782185 -4.1640887 -4.2015872 -4.2076106 -4.2084117 -4.2068872][-4.1265244 -4.1072168 -4.0860057 -4.0860863 -4.1063 -4.1185966 -4.104064 -4.0735607 -4.0660305 -4.1239433 -4.1883087 -4.2173944 -4.2156234 -4.2087808 -4.1998463][-4.1488104 -4.1211224 -4.091166 -4.0835624 -4.1042337 -4.128159 -4.127223 -4.0971918 -4.0776491 -4.114038 -4.1741104 -4.2082291 -4.2121091 -4.2069678 -4.1974235][-4.1727076 -4.1451454 -4.1075206 -4.0871239 -4.1010594 -4.1251569 -4.1261168 -4.0971856 -4.073246 -4.0932269 -4.1488304 -4.1902208 -4.2019186 -4.2004094 -4.1906686][-4.1949563 -4.173737 -4.136754 -4.1046762 -4.1096077 -4.1306162 -4.1261048 -4.1050539 -4.0888729 -4.0942531 -4.1358218 -4.1751804 -4.1923194 -4.1959291 -4.1912737][-4.2058454 -4.1897306 -4.158762 -4.128696 -4.1335907 -4.1559572 -4.1524329 -4.1391096 -4.1259971 -4.1143026 -4.1340542 -4.1670809 -4.1914959 -4.2029819 -4.2025266]]...]
INFO - root - 2017-12-07 15:27:55.371798: step 23110, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 84h:42m:03s remains)
INFO - root - 2017-12-07 15:28:04.926598: step 23120, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 83h:05m:31s remains)
INFO - root - 2017-12-07 15:28:14.643019: step 23130, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 85h:30m:12s remains)
INFO - root - 2017-12-07 15:28:24.212740: step 23140, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 81h:46m:26s remains)
INFO - root - 2017-12-07 15:28:33.908800: step 23150, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 83h:03m:17s remains)
INFO - root - 2017-12-07 15:28:43.684613: step 23160, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 87h:05m:14s remains)
INFO - root - 2017-12-07 15:28:53.168575: step 23170, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.811 sec/batch; 69h:41m:27s remains)
INFO - root - 2017-12-07 15:29:02.713340: step 23180, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 81h:15m:15s remains)
INFO - root - 2017-12-07 15:29:12.133274: step 23190, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 82h:33m:53s remains)
INFO - root - 2017-12-07 15:29:21.861698: step 23200, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.975 sec/batch; 83h:46m:34s remains)
2017-12-07 15:29:22.822492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3156548 -4.3024178 -4.2921948 -4.2910614 -4.2996545 -4.3020744 -4.2877946 -4.25531 -4.2228608 -4.2112975 -4.231895 -4.2635007 -4.2851906 -4.2895393 -4.2828426][-4.317153 -4.2996373 -4.2892208 -4.2897124 -4.2990661 -4.2983246 -4.2770648 -4.2388945 -4.2006207 -4.1903596 -4.2209935 -4.2599645 -4.2844214 -4.2900367 -4.2782745][-4.3146152 -4.2955217 -4.2861896 -4.2877049 -4.2933531 -4.2852545 -4.25693 -4.2105432 -4.1644874 -4.1539822 -4.1972075 -4.2520328 -4.2838573 -4.2885127 -4.2672467][-4.3142023 -4.2942619 -4.28555 -4.2880654 -4.2921929 -4.2807941 -4.2476864 -4.1926684 -4.1346111 -4.1169405 -4.1675143 -4.2342987 -4.276154 -4.2808604 -4.2532768][-4.3183513 -4.2995653 -4.2899976 -4.2869086 -4.2827687 -4.2636685 -4.22179 -4.1607695 -4.100647 -4.0853992 -4.145978 -4.2232432 -4.2672276 -4.2652578 -4.2263126][-4.3247709 -4.3080463 -4.2924776 -4.2757792 -4.256084 -4.2280979 -4.1818347 -4.1139936 -4.0533705 -4.0521 -4.13271 -4.2241478 -4.2675009 -4.2542987 -4.20446][-4.3292866 -4.3156018 -4.2947626 -4.2673573 -4.2359824 -4.2029285 -4.1560655 -4.0860267 -4.021297 -4.03126 -4.1261945 -4.2322674 -4.2787981 -4.2607622 -4.2062035][-4.3352451 -4.32324 -4.3005428 -4.2685714 -4.2333078 -4.1992712 -4.1554728 -4.0860653 -4.0142493 -4.0204115 -4.1186247 -4.2324777 -4.2853136 -4.2679324 -4.2129521][-4.3334002 -4.3198652 -4.2945671 -4.2620239 -4.228826 -4.1959152 -4.1555367 -4.0942826 -4.0254622 -4.0290732 -4.1216736 -4.2293987 -4.280129 -4.2624583 -4.2086248][-4.330802 -4.3164506 -4.2858415 -4.2542386 -4.2227774 -4.1916556 -4.1584706 -4.1145177 -4.0655746 -4.0779161 -4.1590905 -4.2443514 -4.2815351 -4.259366 -4.2075171][-4.3251066 -4.3149 -4.2807841 -4.2459874 -4.2134843 -4.1898432 -4.1685734 -4.1414776 -4.1168828 -4.141542 -4.2083268 -4.2701063 -4.2933507 -4.2710524 -4.2296104][-4.3211193 -4.311614 -4.2788863 -4.2486577 -4.2228522 -4.2068372 -4.1932569 -4.177217 -4.171979 -4.20389 -4.2547097 -4.2909608 -4.3016024 -4.2840319 -4.2588906][-4.3208685 -4.3139658 -4.2839746 -4.2604303 -4.2439747 -4.233429 -4.222919 -4.2093105 -4.2158294 -4.25097 -4.2884674 -4.3087254 -4.31082 -4.2931976 -4.2734313][-4.3200636 -4.3185763 -4.2940516 -4.2742758 -4.2658148 -4.2620964 -4.2543631 -4.2413964 -4.2498741 -4.2829165 -4.311244 -4.323657 -4.3185072 -4.29972 -4.280674][-4.332119 -4.3312812 -4.3104119 -4.2925744 -4.2915988 -4.2978706 -4.294929 -4.2833929 -4.2877064 -4.3113036 -4.3308458 -4.3373146 -4.3264804 -4.3105817 -4.2978563]]...]
INFO - root - 2017-12-07 15:29:32.447291: step 23210, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 84h:56m:47s remains)
INFO - root - 2017-12-07 15:29:42.102661: step 23220, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 85h:33m:20s remains)
INFO - root - 2017-12-07 15:29:51.857807: step 23230, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 82h:16m:42s remains)
INFO - root - 2017-12-07 15:30:01.602564: step 23240, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 82h:10m:40s remains)
INFO - root - 2017-12-07 15:30:11.238625: step 23250, loss = 2.03, batch loss = 1.98 (8.2 examples/sec; 0.974 sec/batch; 83h:38m:50s remains)
INFO - root - 2017-12-07 15:30:20.918144: step 23260, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 81h:44m:44s remains)
INFO - root - 2017-12-07 15:30:30.649172: step 23270, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.024 sec/batch; 87h:58m:43s remains)
INFO - root - 2017-12-07 15:30:40.294462: step 23280, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 79h:29m:42s remains)
INFO - root - 2017-12-07 15:30:49.641365: step 23290, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 83h:01m:41s remains)
INFO - root - 2017-12-07 15:30:59.170734: step 23300, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.999 sec/batch; 85h:49m:06s remains)
2017-12-07 15:31:00.119550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2503262 -4.2566786 -4.26192 -4.2631545 -4.2517791 -4.2640276 -4.2941217 -4.3137608 -4.3127465 -4.3077149 -4.2924261 -4.266099 -4.2619591 -4.2792244 -4.3028364][-4.2330036 -4.24113 -4.2493272 -4.2524996 -4.2435279 -4.2539945 -4.28266 -4.3008871 -4.3002052 -4.2937517 -4.2754049 -4.2480903 -4.2457142 -4.2673249 -4.296433][-4.2555127 -4.2613516 -4.2644253 -4.2604771 -4.2470961 -4.249311 -4.2720213 -4.2925749 -4.2973676 -4.291872 -4.273767 -4.2497973 -4.2468514 -4.2665958 -4.2933097][-4.2822938 -4.2780666 -4.2639556 -4.2437811 -4.2176666 -4.2052116 -4.2247729 -4.2571568 -4.2792392 -4.2862597 -4.2773933 -4.2625489 -4.2614579 -4.2763228 -4.2952852][-4.2922578 -4.2678256 -4.2296176 -4.1842141 -4.128612 -4.0942392 -4.1225138 -4.1868348 -4.2421861 -4.2701211 -4.2730203 -4.264946 -4.2650442 -4.2775364 -4.293623][-4.2765064 -4.2280011 -4.1601348 -4.0764074 -3.9746053 -3.9125514 -3.9605732 -4.0717368 -4.170403 -4.228137 -4.2442784 -4.24136 -4.2485723 -4.266077 -4.2879777][-4.2559776 -4.1899056 -4.1002774 -3.9866285 -3.8509092 -3.767417 -3.8372357 -3.9769306 -4.1022854 -4.181674 -4.2076426 -4.2121162 -4.2338071 -4.2612734 -4.2878723][-4.2466583 -4.1862736 -4.1093965 -4.0161252 -3.9138513 -3.8623586 -3.9275303 -4.0310163 -4.1261744 -4.1917429 -4.2088289 -4.2151837 -4.2454491 -4.2764521 -4.3005033][-4.2592521 -4.2144485 -4.1633029 -4.1074371 -4.0506721 -4.0328631 -4.0817785 -4.1419129 -4.195569 -4.2343464 -4.239768 -4.2433167 -4.2717233 -4.2980423 -4.3148384][-4.2846613 -4.256989 -4.2287006 -4.1965661 -4.1674123 -4.1671424 -4.2015977 -4.2329721 -4.2608328 -4.2820292 -4.2793703 -4.2786703 -4.3008819 -4.3203664 -4.3285122][-4.3012028 -4.2850943 -4.2699952 -4.25111 -4.2385335 -4.2451587 -4.2669654 -4.2835436 -4.3002987 -4.3098111 -4.3013759 -4.293777 -4.307848 -4.3221579 -4.3264132][-4.3021383 -4.2934079 -4.2855406 -4.274405 -4.2678013 -4.2714715 -4.2832084 -4.2946134 -4.3071346 -4.3086123 -4.2952118 -4.2840376 -4.2939053 -4.3070092 -4.3119636][-4.3016176 -4.2971191 -4.2900195 -4.2789264 -4.2707119 -4.26948 -4.2743912 -4.2835269 -4.2939744 -4.2928019 -4.2771211 -4.264771 -4.2764306 -4.2933965 -4.30203][-4.3192124 -4.3115387 -4.2998304 -4.285306 -4.2727032 -4.2649503 -4.2629886 -4.2680912 -4.2758 -4.2734103 -4.2564797 -4.2455292 -4.2619643 -4.2832637 -4.2987294][-4.3343935 -4.3210044 -4.3026667 -4.2825794 -4.2652631 -4.252542 -4.2482285 -4.2523952 -4.2604618 -4.2588506 -4.2415996 -4.22795 -4.2441869 -4.2683997 -4.2913866]]...]
INFO - root - 2017-12-07 15:31:09.956831: step 23310, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 83h:57m:39s remains)
INFO - root - 2017-12-07 15:31:19.661220: step 23320, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 79h:41m:18s remains)
INFO - root - 2017-12-07 15:31:29.205951: step 23330, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 84h:14m:11s remains)
INFO - root - 2017-12-07 15:31:38.845888: step 23340, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 78h:41m:11s remains)
INFO - root - 2017-12-07 15:31:48.465211: step 23350, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 78h:59m:50s remains)
INFO - root - 2017-12-07 15:31:58.116562: step 23360, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 82h:20m:37s remains)
INFO - root - 2017-12-07 15:32:07.629479: step 23370, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 84h:56m:37s remains)
INFO - root - 2017-12-07 15:32:17.309049: step 23380, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 84h:01m:40s remains)
INFO - root - 2017-12-07 15:32:26.974407: step 23390, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 83h:19m:10s remains)
INFO - root - 2017-12-07 15:32:36.392091: step 23400, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 81h:47m:09s remains)
2017-12-07 15:32:37.464317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2708359 -4.2944145 -4.30745 -4.3154635 -4.3075285 -4.2744746 -4.2425876 -4.2238235 -4.2245884 -4.2445645 -4.2734723 -4.2918515 -4.2859941 -4.2731552 -4.2661481][-4.2621861 -4.2872725 -4.3060951 -4.3173733 -4.3016658 -4.2600045 -4.2206259 -4.2030559 -4.2135115 -4.2410803 -4.2729526 -4.2953053 -4.2992096 -4.2916226 -4.2863703][-4.2475491 -4.279294 -4.30314 -4.317471 -4.2991667 -4.2535086 -4.2056189 -4.1836867 -4.1962976 -4.2281122 -4.2618003 -4.2859516 -4.2977719 -4.2986846 -4.2982073][-4.2362094 -4.2717438 -4.296164 -4.3073473 -4.2841272 -4.2363138 -4.1874056 -4.1651931 -4.1731453 -4.2038083 -4.2414384 -4.2715726 -4.2905092 -4.3032012 -4.308785][-4.2302566 -4.266614 -4.2853065 -4.2872314 -4.2546487 -4.2024603 -4.1557074 -4.1364169 -4.1436286 -4.1755652 -4.2190886 -4.2572384 -4.28557 -4.3072348 -4.3177176][-4.2274156 -4.262579 -4.2740664 -4.2657518 -4.2244916 -4.1645904 -4.1153812 -4.0995564 -4.11388 -4.1532712 -4.2005782 -4.2423325 -4.279335 -4.3071094 -4.3189631][-4.2306304 -4.2638268 -4.2714338 -4.2559695 -4.2093387 -4.1433249 -4.0865431 -4.06884 -4.0916362 -4.1398749 -4.1894789 -4.2294092 -4.2698531 -4.3019905 -4.3147464][-4.2384019 -4.2628732 -4.2672162 -4.2486897 -4.203414 -4.1382036 -4.0765815 -4.0547309 -4.0787263 -4.129652 -4.1799889 -4.2194104 -4.2613945 -4.2953296 -4.3116927][-4.2408462 -4.2563219 -4.2567606 -4.2371254 -4.1972623 -4.1400208 -4.082932 -4.0599351 -4.0779676 -4.1230936 -4.1731839 -4.2143068 -4.2545 -4.2888894 -4.3072095][-4.233675 -4.2346497 -4.2290697 -4.2128835 -4.1849971 -4.1427064 -4.0976248 -4.0790768 -4.0895038 -4.1246529 -4.1706505 -4.2127476 -4.2492456 -4.2802086 -4.2985172][-4.2217021 -4.2080073 -4.1942167 -4.1832151 -4.1740136 -4.1518083 -4.125021 -4.11242 -4.1169887 -4.1417613 -4.1794243 -4.2188849 -4.2504649 -4.2770619 -4.2936788][-4.2122974 -4.1916518 -4.1770954 -4.171 -4.1753044 -4.1714797 -4.1608381 -4.1513782 -4.1525083 -4.1701689 -4.2008786 -4.233501 -4.2570591 -4.2786455 -4.2958646][-4.2184691 -4.2026739 -4.1942277 -4.1964517 -4.2092543 -4.2142057 -4.20939 -4.2010212 -4.198215 -4.2070794 -4.2301702 -4.2549748 -4.2704139 -4.2862954 -4.3024497][-4.243082 -4.2334127 -4.2318182 -4.2414403 -4.2588682 -4.2683482 -4.26526 -4.2575126 -4.2509623 -4.2501869 -4.259388 -4.2731853 -4.2835388 -4.2963729 -4.3122234][-4.2778535 -4.2730236 -4.2746048 -4.2866154 -4.3060775 -4.3183351 -4.3169713 -4.307065 -4.2929683 -4.2830486 -4.279449 -4.2818871 -4.286942 -4.2973051 -4.3149695]]...]
INFO - root - 2017-12-07 15:32:47.112965: step 23410, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 78h:23m:49s remains)
INFO - root - 2017-12-07 15:32:56.720088: step 23420, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 83h:11m:19s remains)
INFO - root - 2017-12-07 15:33:06.345207: step 23430, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 84h:46m:53s remains)
INFO - root - 2017-12-07 15:33:16.027160: step 23440, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 84h:23m:33s remains)
INFO - root - 2017-12-07 15:33:25.615348: step 23450, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 81h:06m:01s remains)
INFO - root - 2017-12-07 15:33:35.201836: step 23460, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 84h:19m:43s remains)
INFO - root - 2017-12-07 15:33:44.759880: step 23470, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 82h:57m:42s remains)
INFO - root - 2017-12-07 15:33:54.291317: step 23480, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 74h:34m:21s remains)
INFO - root - 2017-12-07 15:34:04.086435: step 23490, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 84h:18m:19s remains)
INFO - root - 2017-12-07 15:34:13.782612: step 23500, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 82h:36m:37s remains)
2017-12-07 15:34:14.718764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2583723 -4.2508287 -4.245791 -4.24517 -4.2383761 -4.224822 -4.2204218 -4.21746 -4.2188034 -4.2164145 -4.2109795 -4.2135243 -4.2159195 -4.2139759 -4.2201266][-4.2314715 -4.2274666 -4.2282882 -4.2306628 -4.2261605 -4.2132974 -4.2055049 -4.199604 -4.1978059 -4.1910424 -4.1802936 -4.1827831 -4.191371 -4.1928768 -4.1996379][-4.2107024 -4.2078829 -4.2111168 -4.2102809 -4.2059908 -4.2013512 -4.1973686 -4.1923647 -4.1939244 -4.1911826 -4.1776695 -4.1730609 -4.1808047 -4.1870995 -4.1924119][-4.1991749 -4.1928458 -4.1941581 -4.1868057 -4.1794429 -4.178997 -4.17624 -4.1688495 -4.1786289 -4.1921787 -4.1817679 -4.167963 -4.1690416 -4.1826181 -4.1871524][-4.2177558 -4.2033324 -4.1959252 -4.1818995 -4.1688414 -4.1627359 -4.1483407 -4.124825 -4.1375232 -4.1792536 -4.1869149 -4.1712265 -4.16282 -4.1757789 -4.1797709][-4.2499795 -4.230082 -4.2158279 -4.1959238 -4.1779981 -4.1628881 -4.1310053 -4.0724034 -4.0653925 -4.13874 -4.1821175 -4.1768923 -4.1638613 -4.17191 -4.1738849][-4.2452068 -4.2229991 -4.2147994 -4.1996541 -4.1821685 -4.1649251 -4.1166563 -4.0118632 -3.9614477 -4.0641942 -4.1561213 -4.1758685 -4.168561 -4.1754818 -4.1738987][-4.2213917 -4.1983213 -4.198163 -4.1931338 -4.1810064 -4.1728377 -4.1281734 -4.0021644 -3.9093466 -4.01711 -4.1423039 -4.1810474 -4.1818805 -4.1901851 -4.1847005][-4.2042007 -4.1829414 -4.1942167 -4.1987109 -4.1927233 -4.1971607 -4.1742268 -4.0760574 -3.9881294 -4.0594745 -4.1646972 -4.1999831 -4.2046061 -4.2137213 -4.2064066][-4.203651 -4.1844859 -4.1987472 -4.2071142 -4.2025409 -4.2119751 -4.2136407 -4.156673 -4.0949965 -4.130743 -4.1967382 -4.21868 -4.2252178 -4.2372293 -4.232172][-4.2137356 -4.1953015 -4.2066312 -4.22122 -4.2183723 -4.2291903 -4.245296 -4.22047 -4.1810584 -4.1921539 -4.2232862 -4.2312746 -4.241694 -4.2587605 -4.2588773][-4.2300658 -4.2171168 -4.2264752 -4.2455173 -4.2466378 -4.2534609 -4.2702489 -4.2615738 -4.2394228 -4.2402945 -4.2506642 -4.2500458 -4.2619662 -4.2778468 -4.2821159][-4.2607069 -4.2543216 -4.262342 -4.2785435 -4.2836933 -4.2854552 -4.2948008 -4.29285 -4.2820945 -4.2795563 -4.2794881 -4.2777729 -4.2883277 -4.2990651 -4.3035235][-4.2871213 -4.2864919 -4.2952056 -4.3080459 -4.3144937 -4.3119278 -4.314158 -4.3156247 -4.3140941 -4.3110237 -4.3067236 -4.3059688 -4.3138905 -4.3169904 -4.3179431][-4.3079085 -4.3080773 -4.3137217 -4.3234634 -4.328434 -4.3237467 -4.3225236 -4.3262157 -4.329145 -4.3273511 -4.3230724 -4.3241453 -4.32911 -4.3281288 -4.3269353]]...]
INFO - root - 2017-12-07 15:34:24.385666: step 23510, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.994 sec/batch; 85h:20m:08s remains)
INFO - root - 2017-12-07 15:34:34.062824: step 23520, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 78h:13m:00s remains)
INFO - root - 2017-12-07 15:34:43.724191: step 23530, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 83h:28m:52s remains)
INFO - root - 2017-12-07 15:34:53.368578: step 23540, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 78h:56m:02s remains)
INFO - root - 2017-12-07 15:35:03.064316: step 23550, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 81h:59m:36s remains)
INFO - root - 2017-12-07 15:35:12.548802: step 23560, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 79h:03m:39s remains)
INFO - root - 2017-12-07 15:35:22.096896: step 23570, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 81h:08m:16s remains)
INFO - root - 2017-12-07 15:35:31.695983: step 23580, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 82h:57m:23s remains)
INFO - root - 2017-12-07 15:35:41.315934: step 23590, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 84h:12m:57s remains)
INFO - root - 2017-12-07 15:35:51.013819: step 23600, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 83h:27m:37s remains)
2017-12-07 15:35:51.899171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546015 -4.2516775 -4.2538633 -4.2613587 -4.264133 -4.2647243 -4.2624 -4.2537265 -4.2379665 -4.2272191 -4.2196631 -4.2170105 -4.2214556 -4.2288704 -4.228581][-4.2557106 -4.251317 -4.2512865 -4.2589183 -4.2610044 -4.2601981 -4.2559333 -4.2434258 -4.2247157 -4.2147021 -4.2099748 -4.2147417 -4.2270961 -4.2388411 -4.2361946][-4.2597647 -4.2509828 -4.2455831 -4.2484126 -4.2477288 -4.2449727 -4.2375913 -4.2224293 -4.2053661 -4.200696 -4.2034636 -4.2182894 -4.2374153 -4.2506614 -4.2455091][-4.2639422 -4.2505 -4.2391539 -4.2333317 -4.2254753 -4.2143307 -4.2022548 -4.1852546 -4.17402 -4.1780758 -4.1902113 -4.2139797 -4.2381811 -4.2527633 -4.2474546][-4.2626019 -4.2465811 -4.2289486 -4.211267 -4.1912374 -4.1688957 -4.1469054 -4.1250563 -4.1215019 -4.1388559 -4.1618567 -4.1962624 -4.229671 -4.2498908 -4.2474484][-4.2590823 -4.2399187 -4.2153797 -4.1849551 -4.1498928 -4.1117043 -4.0696664 -4.0372658 -4.0462704 -4.0847063 -4.1238065 -4.171391 -4.21869 -4.2472196 -4.2491875][-4.25386 -4.230279 -4.1983452 -4.157526 -4.1091042 -4.053165 -3.9889858 -3.9438553 -3.9688354 -4.0334873 -4.0928841 -4.1531096 -4.2107105 -4.2454877 -4.2502751][-4.2490964 -4.2251616 -4.1921563 -4.1481204 -4.0963249 -4.0352654 -3.9661524 -3.91795 -3.9508584 -4.02757 -4.0960188 -4.1576357 -4.2142963 -4.2482562 -4.25244][-4.2525411 -4.2305765 -4.1990623 -4.1576548 -4.1135683 -4.0669117 -4.0193868 -3.983573 -4.0095854 -4.0745749 -4.1345763 -4.1870317 -4.2346787 -4.2619796 -4.26249][-4.261313 -4.2423339 -4.2151995 -4.1779475 -4.1428089 -4.1123838 -4.086709 -4.0650244 -4.083375 -4.131918 -4.1782951 -4.2205982 -4.2585506 -4.2781663 -4.2748537][-4.2654548 -4.2493081 -4.227941 -4.1969104 -4.1693306 -4.1495466 -4.13612 -4.12296 -4.1388435 -4.1750793 -4.2089782 -4.2421875 -4.272429 -4.2859788 -4.2816734][-4.25928 -4.2462845 -4.2305017 -4.2065368 -4.1850019 -4.1699858 -4.1606617 -4.1510139 -4.1659575 -4.1941576 -4.2199974 -4.2468615 -4.2711091 -4.2813516 -4.2790418][-4.24812 -4.2357631 -4.2229738 -4.204277 -4.1875739 -4.1753297 -4.1677423 -4.1615987 -4.17548 -4.1974277 -4.218153 -4.2414188 -4.262701 -4.2716393 -4.272553][-4.2367411 -4.2248578 -4.2141256 -4.2003307 -4.1884322 -4.1787992 -4.1725535 -4.1694188 -4.1804667 -4.197587 -4.2149467 -4.2344522 -4.2530804 -4.2614145 -4.2646751][-4.2290392 -4.2184105 -4.209796 -4.2004962 -4.1923561 -4.1860156 -4.18278 -4.1822309 -4.1911459 -4.2040672 -4.217907 -4.23448 -4.2514625 -4.2596235 -4.264163]]...]
INFO - root - 2017-12-07 15:36:01.567084: step 23610, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 83h:46m:00s remains)
INFO - root - 2017-12-07 15:36:11.265488: step 23620, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 83h:04m:37s remains)
INFO - root - 2017-12-07 15:36:20.814113: step 23630, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 82h:52m:51s remains)
INFO - root - 2017-12-07 15:36:30.319242: step 23640, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 71h:18m:17s remains)
INFO - root - 2017-12-07 15:36:39.967394: step 23650, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 83h:04m:07s remains)
INFO - root - 2017-12-07 15:36:49.618562: step 23660, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 84h:13m:56s remains)
INFO - root - 2017-12-07 15:36:59.210780: step 23670, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 83h:11m:30s remains)
INFO - root - 2017-12-07 15:37:08.570292: step 23680, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 77h:16m:45s remains)
INFO - root - 2017-12-07 15:37:18.183962: step 23690, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 86h:16m:50s remains)
INFO - root - 2017-12-07 15:37:27.722519: step 23700, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 79h:52m:52s remains)
2017-12-07 15:37:28.703976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3006883 -4.3051252 -4.3117714 -4.3139954 -4.3123736 -4.3103342 -4.3096156 -4.3101249 -4.3100238 -4.3099122 -4.3117075 -4.3125672 -4.3097115 -4.3059864 -4.3047953][-4.2732682 -4.2782307 -4.2862687 -4.2891755 -4.2882528 -4.2871766 -4.2881017 -4.2898369 -4.2898 -4.2899914 -4.293189 -4.2945595 -4.28916 -4.2823744 -4.2813549][-4.2517171 -4.2548566 -4.2620373 -4.2643495 -4.2631721 -4.2621036 -4.2623005 -4.2643809 -4.2661538 -4.2674894 -4.2706542 -4.2715573 -4.2653289 -4.257483 -4.255693][-4.2368851 -4.237134 -4.2420077 -4.2427254 -4.2404609 -4.2382956 -4.2347465 -4.2339163 -4.2368422 -4.2404327 -4.244277 -4.2468691 -4.243289 -4.2368269 -4.2337079][-4.2236872 -4.2200384 -4.2206736 -4.2183847 -4.213316 -4.2065272 -4.1964383 -4.1926641 -4.1974478 -4.2055011 -4.2143626 -4.2232428 -4.22632 -4.223002 -4.216073][-4.2197924 -4.2103939 -4.2035971 -4.195961 -4.1864219 -4.1717825 -4.150526 -4.13535 -4.1349397 -4.1495471 -4.1705112 -4.1903811 -4.2034497 -4.2060432 -4.1982317][-4.2190123 -4.2026 -4.1880479 -4.1755075 -4.1618118 -4.1397305 -4.10489 -4.0682039 -4.0492296 -4.0649719 -4.1016583 -4.1364355 -4.1626582 -4.1764874 -4.1753664][-4.2165575 -4.1951308 -4.17675 -4.1632533 -4.1503272 -4.1283364 -4.0894732 -4.0380807 -3.997906 -4.0027852 -4.0431437 -4.0866132 -4.1234016 -4.147984 -4.1561894][-4.222127 -4.2000456 -4.1840496 -4.1772828 -4.1744242 -4.1625562 -4.1348891 -4.0944886 -4.0580997 -4.0525131 -4.0777369 -4.1099195 -4.1399908 -4.1625428 -4.1718345][-4.2302241 -4.2097692 -4.1985455 -4.1988459 -4.2050405 -4.2039628 -4.189126 -4.1651597 -4.1428165 -4.1357532 -4.1470423 -4.1638813 -4.1822877 -4.1974373 -4.2057929][-4.2326756 -4.2156239 -4.2082653 -4.2121239 -4.2231507 -4.2303281 -4.2263603 -4.2153416 -4.2051339 -4.2000675 -4.2024384 -4.20762 -4.21478 -4.2221189 -4.2286582][-4.2330055 -4.2207217 -4.2161069 -4.2195554 -4.2309265 -4.2408857 -4.2440052 -4.242146 -4.2392588 -4.2361617 -4.2357516 -4.2355905 -4.2347031 -4.2349992 -4.2374911][-4.2237697 -4.2150006 -4.212532 -4.21467 -4.2228184 -4.2307358 -4.2345734 -4.2359586 -4.2369204 -4.2374678 -4.2381949 -4.2378387 -4.2363205 -4.2351189 -4.2366714][-4.2168641 -4.2072425 -4.203907 -4.2026463 -4.2062922 -4.2107434 -4.2129269 -4.2151237 -4.2184186 -4.2219005 -4.2241244 -4.2231393 -4.2209487 -4.2213798 -4.2246675][-4.2315726 -4.2194242 -4.211894 -4.2045827 -4.2010741 -4.1995378 -4.1987514 -4.1999454 -4.2032876 -4.2077541 -4.2103014 -4.2088442 -4.2060337 -4.2058587 -4.2088356]]...]
INFO - root - 2017-12-07 15:37:38.333039: step 23710, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 82h:34m:11s remains)
INFO - root - 2017-12-07 15:37:47.892725: step 23720, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 84h:54m:32s remains)
INFO - root - 2017-12-07 15:37:57.538322: step 23730, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 82h:19m:10s remains)
INFO - root - 2017-12-07 15:38:07.304238: step 23740, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 85h:40m:09s remains)
INFO - root - 2017-12-07 15:38:17.039012: step 23750, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 82h:06m:45s remains)
INFO - root - 2017-12-07 15:38:26.536129: step 23760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 78h:12m:37s remains)
INFO - root - 2017-12-07 15:38:36.007139: step 23770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:33m:05s remains)
INFO - root - 2017-12-07 15:38:45.490866: step 23780, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 80h:48m:38s remains)
INFO - root - 2017-12-07 15:38:55.226547: step 23790, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 85h:41m:11s remains)
INFO - root - 2017-12-07 15:39:04.832479: step 23800, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 83h:01m:11s remains)
2017-12-07 15:39:05.778434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16618 -4.1420689 -4.1331348 -4.133842 -4.144299 -4.1433334 -4.1422372 -4.1430669 -4.1364546 -4.136363 -4.14816 -4.1572022 -4.164515 -4.1778464 -4.1997051][-4.161274 -4.1430182 -4.1407218 -4.1447988 -4.1530209 -4.1522527 -4.1545262 -4.1580591 -4.1508241 -4.14664 -4.1536489 -4.1616335 -4.1673517 -4.1753244 -4.189014][-4.1523986 -4.135345 -4.1337495 -4.1374359 -4.1445141 -4.1455107 -4.1494646 -4.1564503 -4.1578789 -4.1586943 -4.163311 -4.165236 -4.1632752 -4.164834 -4.1696033][-4.16159 -4.142036 -4.1341286 -4.1325722 -4.139184 -4.1420255 -4.1415086 -4.1467476 -4.1571536 -4.1699591 -4.1782012 -4.176219 -4.1715145 -4.170392 -4.1701226][-4.1914449 -4.1702724 -4.1544642 -4.1458721 -4.1495056 -4.1485958 -4.1350708 -4.1326041 -4.1492605 -4.1743007 -4.1922379 -4.1958804 -4.1947956 -4.1954632 -4.1929865][-4.2207294 -4.2011657 -4.1812048 -4.1671677 -4.1647511 -4.1536703 -4.1197472 -4.0944452 -4.1117158 -4.15601 -4.1923604 -4.2104349 -4.2212033 -4.2272711 -4.2208319][-4.232152 -4.213726 -4.1917753 -4.1730981 -4.1616993 -4.1395149 -4.0864673 -4.0309291 -4.0443354 -4.1141891 -4.1762629 -4.2133269 -4.2371907 -4.2473464 -4.2365975][-4.2266717 -4.2090154 -4.1874123 -4.16795 -4.1489568 -4.1190944 -4.0610347 -3.9941344 -4.0019617 -4.0851741 -4.16133 -4.2129159 -4.2484779 -4.262145 -4.25111][-4.2160883 -4.2060552 -4.1901755 -4.172543 -4.1516747 -4.1263514 -4.0848207 -4.0355244 -4.0372019 -4.0975075 -4.1640663 -4.21459 -4.2533197 -4.2679248 -4.257617][-4.2081933 -4.2136602 -4.2053189 -4.18879 -4.1709981 -4.1569138 -4.1347895 -4.1053185 -4.0999541 -4.1330938 -4.1831632 -4.2239752 -4.2556491 -4.2651892 -4.2548265][-4.2044754 -4.2223754 -4.2222443 -4.2103076 -4.1984143 -4.1902175 -4.176507 -4.1580949 -4.1468139 -4.1623988 -4.2006989 -4.2345467 -4.2591505 -4.2644234 -4.2529874][-4.2048836 -4.2255545 -4.2311764 -4.2266626 -4.2212615 -4.2164354 -4.2066636 -4.1923294 -4.1762891 -4.1789455 -4.2078633 -4.2392416 -4.2602239 -4.2618408 -4.2487378][-4.2049356 -4.2256365 -4.2364316 -4.23811 -4.2371655 -4.2358866 -4.2287297 -4.2144761 -4.196033 -4.192174 -4.2116752 -4.2383571 -4.2566452 -4.2565632 -4.2476439][-4.2087851 -4.2267151 -4.2416358 -4.2475991 -4.2501068 -4.2527108 -4.2481437 -4.2375283 -4.2248759 -4.2198343 -4.2266893 -4.2438917 -4.257925 -4.2579403 -4.2509437][-4.2168255 -4.2279758 -4.2448835 -4.2530718 -4.25654 -4.2619033 -4.263083 -4.2608638 -4.256505 -4.2508507 -4.2476983 -4.2531233 -4.2604866 -4.2596636 -4.2543874]]...]
INFO - root - 2017-12-07 15:39:15.427426: step 23810, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 83h:56m:04s remains)
INFO - root - 2017-12-07 15:39:25.135667: step 23820, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 84h:28m:30s remains)
INFO - root - 2017-12-07 15:39:34.596068: step 23830, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:33m:25s remains)
INFO - root - 2017-12-07 15:39:44.138262: step 23840, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 78h:02m:56s remains)
INFO - root - 2017-12-07 15:39:53.846310: step 23850, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 83h:12m:26s remains)
INFO - root - 2017-12-07 15:40:03.567137: step 23860, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 85h:49m:34s remains)
INFO - root - 2017-12-07 15:40:13.132251: step 23870, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 79h:35m:44s remains)
INFO - root - 2017-12-07 15:40:22.952941: step 23880, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 81h:48m:30s remains)
INFO - root - 2017-12-07 15:40:32.582336: step 23890, loss = 2.08, batch loss = 2.02 (7.6 examples/sec; 1.058 sec/batch; 90h:44m:03s remains)
INFO - root - 2017-12-07 15:40:42.133867: step 23900, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 80h:55m:35s remains)
2017-12-07 15:40:43.110460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1821885 -4.1680651 -4.1503315 -4.1371341 -4.1416879 -4.1447859 -4.1373453 -4.13554 -4.1379037 -4.1397843 -4.1471429 -4.15222 -4.1513882 -4.150692 -4.1380467][-4.1994824 -4.1803889 -4.1608324 -4.1492343 -4.1565652 -4.1632013 -4.158124 -4.1549625 -4.1563668 -4.1585526 -4.1651678 -4.1654906 -4.1633892 -4.1609564 -4.1397839][-4.2144794 -4.1936049 -4.17497 -4.1684513 -4.1793022 -4.1875148 -4.1816559 -4.1741214 -4.1724796 -4.176199 -4.1848783 -4.185277 -4.1849318 -4.1806612 -4.1571279][-4.2232747 -4.200284 -4.1858821 -4.1854253 -4.198359 -4.2050781 -4.1977305 -4.1848955 -4.17637 -4.17909 -4.1927657 -4.1983962 -4.2026982 -4.1980543 -4.17462][-4.2037158 -4.1782331 -4.1674142 -4.1742282 -4.1887531 -4.1912937 -4.1789417 -4.1589537 -4.1431446 -4.1484213 -4.1733832 -4.1936207 -4.2091603 -4.2080894 -4.1868711][-4.15968 -4.1373873 -4.1339612 -4.1477685 -4.16407 -4.1620817 -4.14218 -4.1122069 -4.0922604 -4.1040874 -4.1437 -4.181602 -4.2106757 -4.2158966 -4.1990471][-4.1170754 -4.1011572 -4.1084409 -4.1277785 -4.1412196 -4.1334271 -4.1032643 -4.063983 -4.0427818 -4.06312 -4.1166825 -4.1706638 -4.2109013 -4.2216291 -4.2073979][-4.1102052 -4.0980859 -4.1108518 -4.1311378 -4.1394405 -4.1237764 -4.0805254 -4.0306568 -4.0073919 -4.0346265 -4.0951824 -4.1570539 -4.2037396 -4.2202315 -4.2117305][-4.1315308 -4.1162906 -4.1271186 -4.1454124 -4.1540947 -4.1374817 -4.0928106 -4.0423069 -4.0221558 -4.0502005 -4.1060886 -4.1643825 -4.2122307 -4.233067 -4.2278376][-4.156745 -4.1364889 -4.139348 -4.1536837 -4.1640344 -4.1525183 -4.1222916 -4.0898528 -4.0826755 -4.1080184 -4.1476603 -4.1911445 -4.2317877 -4.2512379 -4.2449756][-4.1785946 -4.156096 -4.1525669 -4.1627636 -4.1696844 -4.1616797 -4.1454053 -4.1342368 -4.1422253 -4.1657448 -4.1890092 -4.2165818 -4.2443933 -4.257493 -4.2507687][-4.1916976 -4.1722283 -4.1656203 -4.1686563 -4.1668181 -4.1595163 -4.1516347 -4.1534348 -4.171864 -4.1958203 -4.210649 -4.2257824 -4.24172 -4.2468252 -4.240983][-4.2100191 -4.1963115 -4.1897974 -4.1859255 -4.1748209 -4.1668162 -4.1669326 -4.1783857 -4.2014213 -4.222569 -4.2297115 -4.2326407 -4.2365813 -4.2352867 -4.2299981][-4.2143211 -4.2055664 -4.2007804 -4.193511 -4.1763258 -4.1671662 -4.1737418 -4.1905384 -4.2135978 -4.228858 -4.2306175 -4.228364 -4.22692 -4.2248287 -4.2219505][-4.2180576 -4.2131228 -4.209034 -4.1979551 -4.1760736 -4.162446 -4.1679983 -4.1823835 -4.1999321 -4.2073274 -4.2067041 -4.2055364 -4.2066255 -4.2110291 -4.2176895]]...]
INFO - root - 2017-12-07 15:40:52.744180: step 23910, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.984 sec/batch; 84h:20m:46s remains)
INFO - root - 2017-12-07 15:41:02.457056: step 23920, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 86h:28m:56s remains)
INFO - root - 2017-12-07 15:41:12.028557: step 23930, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.009 sec/batch; 86h:31m:23s remains)
INFO - root - 2017-12-07 15:41:21.717878: step 23940, loss = 2.04, batch loss = 1.98 (8.0 examples/sec; 1.006 sec/batch; 86h:12m:18s remains)
INFO - root - 2017-12-07 15:41:31.583120: step 23950, loss = 2.09, batch loss = 2.03 (7.5 examples/sec; 1.070 sec/batch; 91h:41m:21s remains)
INFO - root - 2017-12-07 15:41:41.052383: step 23960, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 82h:50m:50s remains)
INFO - root - 2017-12-07 15:41:50.487160: step 23970, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 79h:30m:56s remains)
INFO - root - 2017-12-07 15:42:00.307897: step 23980, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 86h:42m:21s remains)
INFO - root - 2017-12-07 15:42:10.025044: step 23990, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 80h:31m:31s remains)
INFO - root - 2017-12-07 15:42:19.846938: step 24000, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 83h:17m:18s remains)
2017-12-07 15:42:20.791534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2263594 -4.2161169 -4.2114058 -4.2179294 -4.2239947 -4.2167382 -4.1952443 -4.1667366 -4.1584578 -4.1730938 -4.2068462 -4.2411809 -4.2591729 -4.2456403 -4.2018733][-4.2267256 -4.2195959 -4.2163153 -4.2148786 -4.212 -4.1982656 -4.1744728 -4.1505604 -4.155756 -4.1822166 -4.2156153 -4.2431297 -4.2527776 -4.231585 -4.1868067][-4.2141266 -4.2060609 -4.2021089 -4.1941605 -4.1836834 -4.1665492 -4.1442275 -4.1269927 -4.1446142 -4.1820025 -4.2166705 -4.2385678 -4.2444267 -4.225997 -4.186554][-4.1814961 -4.1780634 -4.1789351 -4.1717534 -4.1576147 -4.13674 -4.1132288 -4.100163 -4.1281495 -4.1735382 -4.2107344 -4.2297578 -4.2369432 -4.2281694 -4.1984091][-4.1533914 -4.1636739 -4.1824679 -4.18111 -4.1578331 -4.1222782 -4.0868816 -4.0775366 -4.1177855 -4.1709223 -4.2096624 -4.2271576 -4.2359509 -4.231842 -4.2068396][-4.1301208 -4.1592913 -4.1972127 -4.2039833 -4.1687946 -4.1063228 -4.0540991 -4.0592542 -4.1177931 -4.1796012 -4.2166142 -4.2304764 -4.2400403 -4.2385511 -4.2160149][-4.119185 -4.1599913 -4.2058296 -4.2100644 -4.1533136 -4.0601239 -3.9889162 -4.0214143 -4.110795 -4.1858845 -4.2205691 -4.2291718 -4.235486 -4.236187 -4.2164512][-4.1408997 -4.1846471 -4.2215228 -4.2109284 -4.1325135 -4.0162892 -3.9329422 -3.9899168 -4.1019917 -4.1821284 -4.2111297 -4.2145262 -4.2200203 -4.2243314 -4.2060757][-4.1777062 -4.2165327 -4.2419505 -4.2254748 -4.1518211 -4.0519857 -3.9904346 -4.0355811 -4.1237469 -4.1851921 -4.2048039 -4.2057467 -4.2135835 -4.2233458 -4.2094154][-4.200346 -4.235785 -4.2555866 -4.2432494 -4.1889224 -4.1208682 -4.0845 -4.1065197 -4.155303 -4.1903939 -4.2038994 -4.2093363 -4.2184658 -4.2274003 -4.2148905][-4.203084 -4.2380095 -4.2610312 -4.2567434 -4.2194619 -4.1703167 -4.1460557 -4.1526914 -4.1750865 -4.19111 -4.2006378 -4.2118654 -4.2217255 -4.2268181 -4.2123857][-4.2095027 -4.2420931 -4.269588 -4.2738132 -4.2484446 -4.2110138 -4.1907921 -4.1901293 -4.1995683 -4.2034769 -4.2071009 -4.216907 -4.2235074 -4.2246528 -4.2087359][-4.2295275 -4.255 -4.2784038 -4.2826676 -4.2633972 -4.2366557 -4.2225657 -4.2223186 -4.2274261 -4.2277217 -4.2263145 -4.2294006 -4.230978 -4.2289643 -4.2154202][-4.246139 -4.2628593 -4.2800932 -4.2821474 -4.2697716 -4.253273 -4.2465763 -4.2475085 -4.2518997 -4.2537384 -4.2512097 -4.2492433 -4.2464867 -4.244544 -4.2372108][-4.2597632 -4.2696323 -4.2810593 -4.2791772 -4.2696228 -4.2621117 -4.2628202 -4.2654271 -4.2693214 -4.2723594 -4.2716985 -4.270896 -4.269105 -4.2673874 -4.2621961]]...]
INFO - root - 2017-12-07 15:42:30.709709: step 24010, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 81h:37m:31s remains)
INFO - root - 2017-12-07 15:42:40.352323: step 24020, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.977 sec/batch; 83h:42m:57s remains)
INFO - root - 2017-12-07 15:42:50.080304: step 24030, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 84h:00m:05s remains)
INFO - root - 2017-12-07 15:42:59.836839: step 24040, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 84h:36m:06s remains)
INFO - root - 2017-12-07 15:43:09.395867: step 24050, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 79h:25m:17s remains)
INFO - root - 2017-12-07 15:43:19.009490: step 24060, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 81h:15m:59s remains)
INFO - root - 2017-12-07 15:43:28.588692: step 24070, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 86h:46m:45s remains)
INFO - root - 2017-12-07 15:43:38.402785: step 24080, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 84h:58m:40s remains)
INFO - root - 2017-12-07 15:43:48.055411: step 24090, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 82h:03m:48s remains)
INFO - root - 2017-12-07 15:43:57.591805: step 24100, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.007 sec/batch; 86h:16m:22s remains)
2017-12-07 15:43:58.537402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3103089 -4.3114734 -4.3142076 -4.3201489 -4.3265495 -4.320859 -4.3013415 -4.2793379 -4.2718434 -4.2765541 -4.289979 -4.3077555 -4.3230615 -4.3272657 -4.3179789][-4.3120289 -4.3160539 -4.3211732 -4.32769 -4.3284397 -4.3104315 -4.2736716 -4.2382464 -4.2244773 -4.2310266 -4.2527533 -4.2816472 -4.306097 -4.3147039 -4.3090024][-4.3063812 -4.3133893 -4.3207793 -4.3265977 -4.3219342 -4.2927947 -4.2400007 -4.1912184 -4.1721048 -4.180306 -4.2098279 -4.251833 -4.2874 -4.3010907 -4.2989264][-4.292531 -4.3048067 -4.3153405 -4.3211546 -4.311265 -4.2717004 -4.2058406 -4.1475606 -4.123323 -4.1307049 -4.1650538 -4.2192783 -4.2661762 -4.2861104 -4.2907057][-4.2692533 -4.2878318 -4.3007336 -4.3057957 -4.292336 -4.2477217 -4.1771297 -4.116477 -4.091167 -4.0989132 -4.1364784 -4.1951232 -4.2449427 -4.2660508 -4.2761235][-4.24594 -4.2654362 -4.2782488 -4.282836 -4.2700853 -4.22926 -4.1645346 -4.1101403 -4.0915341 -4.1017904 -4.1374393 -4.1868711 -4.2251077 -4.2400012 -4.2506514][-4.2433519 -4.2567391 -4.2653823 -4.2697053 -4.2608776 -4.2271705 -4.1736221 -4.1301947 -4.1192904 -4.1312222 -4.1597886 -4.1911912 -4.2104936 -4.2158213 -4.2246585][-4.2635674 -4.2690749 -4.271008 -4.2720451 -4.2649503 -4.2375689 -4.1947846 -4.1610465 -4.1546974 -4.1667786 -4.187561 -4.20216 -4.2060795 -4.2042389 -4.2114124][-4.28838 -4.2874813 -4.282949 -4.2790685 -4.2717762 -4.2502027 -4.2189512 -4.1942625 -4.1915746 -4.2047777 -4.2216563 -4.2284365 -4.2252092 -4.2190084 -4.2227168][-4.3046222 -4.3001919 -4.2915292 -4.2849865 -4.2792273 -4.2656159 -4.2463818 -4.2298646 -4.22967 -4.2412972 -4.2556872 -4.2628074 -4.2610331 -4.2554789 -4.2559257][-4.3153605 -4.3109112 -4.3021593 -4.2959042 -4.2937207 -4.2877212 -4.2779593 -4.2677007 -4.2659516 -4.2713518 -4.2810345 -4.2893624 -4.2927084 -4.2910414 -4.291461][-4.3223615 -4.3198013 -4.3138008 -4.310185 -4.3117418 -4.311296 -4.3068719 -4.299562 -4.2940779 -4.292325 -4.2962236 -4.3045449 -4.3124571 -4.3170319 -4.3200865][-4.32806 -4.3284807 -4.3257976 -4.3249469 -4.3275576 -4.3281374 -4.3250461 -4.318047 -4.3097715 -4.3039579 -4.3037968 -4.3100343 -4.3187175 -4.3275156 -4.3340631][-4.3297224 -4.3318629 -4.332027 -4.3329749 -4.3353176 -4.33464 -4.3298378 -4.3214602 -4.3122883 -4.3052754 -4.3036661 -4.307621 -4.3147473 -4.3243208 -4.3331032][-4.3251348 -4.3276 -4.3291588 -4.3306317 -4.3316493 -4.3289447 -4.3225007 -4.3142524 -4.3068433 -4.3012509 -4.2997169 -4.3021564 -4.3074241 -4.315877 -4.3251042]]...]
INFO - root - 2017-12-07 15:44:08.312798: step 24110, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 84h:53m:09s remains)
INFO - root - 2017-12-07 15:44:17.928246: step 24120, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 82h:26m:30s remains)
INFO - root - 2017-12-07 15:44:27.451313: step 24130, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 78h:24m:00s remains)
INFO - root - 2017-12-07 15:44:37.072011: step 24140, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 82h:18m:17s remains)
INFO - root - 2017-12-07 15:44:46.791743: step 24150, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 82h:35m:48s remains)
INFO - root - 2017-12-07 15:44:56.434159: step 24160, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 75h:33m:11s remains)
INFO - root - 2017-12-07 15:45:06.143996: step 24170, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 77h:22m:27s remains)
INFO - root - 2017-12-07 15:45:15.876469: step 24180, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 81h:35m:16s remains)
INFO - root - 2017-12-07 15:45:25.447266: step 24190, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.004 sec/batch; 86h:01m:32s remains)
INFO - root - 2017-12-07 15:45:34.991366: step 24200, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 86h:08m:43s remains)
2017-12-07 15:45:35.933446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2445836 -4.2211266 -4.202055 -4.18731 -4.193377 -4.213213 -4.2379737 -4.2518964 -4.2651982 -4.2741895 -4.2740903 -4.2481675 -4.2298284 -4.20936 -4.193152][-4.2308536 -4.2049932 -4.1886587 -4.1764636 -4.1916103 -4.2171445 -4.2404737 -4.2588468 -4.2718062 -4.281024 -4.2693825 -4.2284269 -4.2036924 -4.1749716 -4.15285][-4.2235208 -4.1855311 -4.1638932 -4.1555247 -4.1798506 -4.2085605 -4.2320671 -4.2546792 -4.268744 -4.2823706 -4.268239 -4.2177567 -4.1836548 -4.1452861 -4.1166062][-4.211658 -4.1586251 -4.1268911 -4.124454 -4.1490722 -4.1697264 -4.1938977 -4.2230668 -4.2489362 -4.2719321 -4.2637644 -4.2131748 -4.1737394 -4.1284409 -4.0978508][-4.2032127 -4.137866 -4.0984969 -4.0919156 -4.1040077 -4.1060376 -4.1130557 -4.1453619 -4.1960363 -4.2400064 -4.247654 -4.211504 -4.1777639 -4.1322536 -4.1017089][-4.2078004 -4.1399956 -4.0914679 -4.0688376 -4.0615168 -4.0375652 -4.0040932 -4.0130386 -4.0927529 -4.17866 -4.2190304 -4.2159977 -4.2028394 -4.1628656 -4.136548][-4.2087035 -4.1506114 -4.1029162 -4.0645738 -4.0334444 -3.978472 -3.8790483 -3.8333859 -3.9452119 -4.0828137 -4.1632934 -4.1961889 -4.2143807 -4.1936793 -4.1780477][-4.1847291 -4.1437426 -4.1088343 -4.0696707 -4.0287189 -3.9575853 -3.7986155 -3.6993735 -3.8321352 -3.9936082 -4.0902948 -4.1436429 -4.184855 -4.1835365 -4.189774][-4.1332393 -4.10683 -4.0946565 -4.0750155 -4.0490961 -3.9886575 -3.8343034 -3.7328413 -3.8324223 -3.954957 -4.0296512 -4.0872784 -4.1374049 -4.1497793 -4.1716237][-4.10621 -4.08378 -4.0893059 -4.0926552 -4.0936193 -4.0675397 -3.9697547 -3.8939376 -3.9350619 -3.9901426 -4.0302582 -4.0781636 -4.1218276 -4.1367421 -4.1542039][-4.1370926 -4.114635 -4.1257029 -4.1396542 -4.1563668 -4.1635137 -4.1166091 -4.0681472 -4.0788851 -4.0966988 -4.1173916 -4.1473351 -4.1724482 -4.1761661 -4.1801853][-4.2069626 -4.1929779 -4.2058468 -4.2208776 -4.2347894 -4.251327 -4.2398362 -4.2163877 -4.22018 -4.2258425 -4.2394738 -4.2517428 -4.25837 -4.2495904 -4.2437196][-4.2849822 -4.2787766 -4.2908735 -4.3037786 -4.3099456 -4.3231473 -4.3296342 -4.3234363 -4.3306746 -4.3339815 -4.3373108 -4.3330269 -4.3292708 -4.31734 -4.31127][-4.3435597 -4.3423829 -4.3527317 -4.3603354 -4.361002 -4.3662167 -4.3731174 -4.3765593 -4.3857894 -4.3873 -4.3831277 -4.3708067 -4.3625536 -4.3525143 -4.34997][-4.3704 -4.3715835 -4.3762603 -4.3782845 -4.3788795 -4.3811769 -4.3860135 -4.388021 -4.3929605 -4.3922534 -4.3860521 -4.3753061 -4.3667836 -4.362236 -4.3612165]]...]
INFO - root - 2017-12-07 15:45:45.650423: step 24210, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 82h:45m:29s remains)
INFO - root - 2017-12-07 15:45:55.286438: step 24220, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 83h:22m:41s remains)
INFO - root - 2017-12-07 15:46:04.809624: step 24230, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 80h:36m:18s remains)
INFO - root - 2017-12-07 15:46:14.514987: step 24240, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.967 sec/batch; 82h:49m:36s remains)
INFO - root - 2017-12-07 15:46:24.275094: step 24250, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 81h:06m:28s remains)
INFO - root - 2017-12-07 15:46:33.938447: step 24260, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.947 sec/batch; 81h:03m:24s remains)
INFO - root - 2017-12-07 15:46:43.720164: step 24270, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 84h:40m:14s remains)
INFO - root - 2017-12-07 15:46:53.438459: step 24280, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 79h:37m:18s remains)
INFO - root - 2017-12-07 15:47:03.092470: step 24290, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 83h:45m:37s remains)
INFO - root - 2017-12-07 15:47:12.779498: step 24300, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 82h:43m:49s remains)
2017-12-07 15:47:13.797723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2570643 -4.2281075 -4.203351 -4.176034 -4.14931 -4.1344862 -4.14184 -4.1672091 -4.1982183 -4.2243929 -4.230216 -4.2128639 -4.18816 -4.1744928 -4.1747832][-4.265811 -4.2410421 -4.218349 -4.1879215 -4.1586709 -4.1453562 -4.1547527 -4.1797972 -4.2061238 -4.2293406 -4.2361364 -4.2191329 -4.1956468 -4.1830511 -4.1802006][-4.2752686 -4.2559495 -4.2402792 -4.2141652 -4.1904469 -4.1830597 -4.1943331 -4.2112226 -4.2229428 -4.2351241 -4.2335005 -4.2162013 -4.20179 -4.1993904 -4.2016563][-4.2730184 -4.2570052 -4.2456522 -4.2273474 -4.2104526 -4.2090034 -4.2207642 -4.2320971 -4.231863 -4.2266207 -4.2116323 -4.1956825 -4.1962824 -4.2124462 -4.2237778][-4.257534 -4.2397213 -4.2306514 -4.214376 -4.1989918 -4.1984348 -4.2139935 -4.2262263 -4.2220454 -4.2038431 -4.1765623 -4.1563206 -4.1661897 -4.1966543 -4.2160339][-4.235373 -4.2056203 -4.1888375 -4.1663642 -4.1439 -4.1450019 -4.1736007 -4.1940041 -4.1880803 -4.162148 -4.1318827 -4.1086531 -4.1158466 -4.1446109 -4.1664047][-4.2183633 -4.174736 -4.1467781 -4.120265 -4.0924821 -4.0890489 -4.1213565 -4.1448627 -4.1401844 -4.1153436 -4.0936279 -4.0793653 -4.0747862 -4.0792875 -4.08873][-4.2164717 -4.1624174 -4.1238451 -4.089035 -4.0503349 -4.0331669 -4.052494 -4.07132 -4.0723224 -4.0598774 -4.0560017 -4.0571117 -4.0543675 -4.0429482 -4.0341444][-4.211163 -4.1500883 -4.1026397 -4.0587912 -4.0107465 -3.9828248 -3.9897842 -4.0012059 -4.0094543 -4.0162334 -4.0379186 -4.0604277 -4.0715685 -4.068368 -4.0627432][-4.2041655 -4.1457067 -4.102808 -4.0672774 -4.0293179 -4.0025644 -4.0043473 -4.0102172 -4.0219188 -4.041399 -4.0799994 -4.1172552 -4.1373186 -4.1414661 -4.1409316][-4.2123389 -4.1690764 -4.14402 -4.1258311 -4.1046705 -4.0872674 -4.0928159 -4.0986886 -4.1086807 -4.1305833 -4.1679659 -4.20253 -4.2219172 -4.2264514 -4.2251973][-4.2298574 -4.2048559 -4.2001982 -4.2000976 -4.1925116 -4.1846437 -4.1897907 -4.1936326 -4.2019639 -4.2180157 -4.2411876 -4.2611237 -4.2707181 -4.272058 -4.2722349][-4.2453694 -4.2345524 -4.243876 -4.2559476 -4.2589769 -4.2553244 -4.2528973 -4.2489014 -4.252861 -4.265254 -4.2785678 -4.2871308 -4.2897096 -4.2898984 -4.2923613][-4.2583475 -4.2529917 -4.2648311 -4.2811794 -4.2901053 -4.2906852 -4.2866426 -4.2797732 -4.2825127 -4.2929788 -4.3026614 -4.3077321 -4.3081174 -4.306345 -4.3080649][-4.2709122 -4.2663608 -4.276103 -4.2904487 -4.2997413 -4.3035316 -4.3023238 -4.298213 -4.3015914 -4.3097672 -4.3163686 -4.3190846 -4.3169379 -4.3122983 -4.3109174]]...]
INFO - root - 2017-12-07 15:47:23.622237: step 24310, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 82h:21m:11s remains)
INFO - root - 2017-12-07 15:47:33.310292: step 24320, loss = 2.08, batch loss = 2.03 (7.6 examples/sec; 1.058 sec/batch; 90h:32m:24s remains)
INFO - root - 2017-12-07 15:47:43.057411: step 24330, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 83h:25m:19s remains)
INFO - root - 2017-12-07 15:47:52.839460: step 24340, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.976 sec/batch; 83h:33m:23s remains)
INFO - root - 2017-12-07 15:48:02.501884: step 24350, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 81h:11m:47s remains)
INFO - root - 2017-12-07 15:48:12.065507: step 24360, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.032 sec/batch; 88h:20m:49s remains)
INFO - root - 2017-12-07 15:48:21.675841: step 24370, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 81h:48m:16s remains)
INFO - root - 2017-12-07 15:48:31.355259: step 24380, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 84h:24m:55s remains)
INFO - root - 2017-12-07 15:48:41.207880: step 24390, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 81h:13m:20s remains)
INFO - root - 2017-12-07 15:48:50.828891: step 24400, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 82h:56m:00s remains)
2017-12-07 15:48:51.777962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2860537 -4.2892261 -4.3024731 -4.3230739 -4.3403163 -4.3434391 -4.3449516 -4.3455148 -4.343719 -4.3384285 -4.3310108 -4.3059239 -4.2566605 -4.2277412 -4.2311463][-4.2727408 -4.2796607 -4.297327 -4.3195906 -4.3368979 -4.338696 -4.33496 -4.3309689 -4.3280878 -4.3280306 -4.32659 -4.30621 -4.2549925 -4.2134142 -4.2029276][-4.2426705 -4.2572737 -4.2771034 -4.3022704 -4.3174777 -4.3140736 -4.3081388 -4.3044868 -4.3058128 -4.3157296 -4.31912 -4.3010993 -4.2525587 -4.2115774 -4.193264][-4.1993213 -4.2247472 -4.2483025 -4.2824883 -4.2929649 -4.2784796 -4.2691364 -4.27236 -4.283639 -4.3027725 -4.3111567 -4.2953482 -4.2570329 -4.22368 -4.204874][-4.1540542 -4.189805 -4.2181911 -4.2551851 -4.2616992 -4.2335615 -4.2121458 -4.2183108 -4.2483306 -4.2836404 -4.3004584 -4.2913146 -4.264627 -4.2390656 -4.2208447][-4.1318331 -4.1726322 -4.2061563 -4.2316322 -4.22461 -4.1714444 -4.1147175 -4.1029768 -4.1596227 -4.2344122 -4.2708597 -4.2716317 -4.2546463 -4.2378187 -4.2204752][-4.1399069 -4.1779132 -4.2010884 -4.2022743 -4.178709 -4.1065707 -3.9999058 -3.9369485 -4.0190768 -4.1497254 -4.2151966 -4.224544 -4.2139492 -4.2066431 -4.1966472][-4.1383152 -4.1669641 -4.1829844 -4.1723948 -4.1436443 -4.0657606 -3.9186387 -3.7807963 -3.8667512 -4.0474343 -4.1375961 -4.1531334 -4.1478343 -4.1528616 -4.1582727][-4.1470056 -4.1696749 -4.1823158 -4.1756105 -4.1522684 -4.0911655 -3.9580648 -3.8076599 -3.843641 -4.002636 -4.09057 -4.1051722 -4.1019249 -4.1139979 -4.1342869][-4.1805 -4.1961989 -4.2089405 -4.2100916 -4.2036934 -4.1755948 -4.0931969 -3.9939861 -3.9816971 -4.0578289 -4.1114197 -4.1159854 -4.1125216 -4.1247253 -4.14557][-4.2230582 -4.2309313 -4.2391257 -4.2411942 -4.2429638 -4.2404633 -4.2056646 -4.1540995 -4.12617 -4.1511035 -4.1720119 -4.1618347 -4.1552196 -4.1665082 -4.184391][-4.2628145 -4.2666931 -4.26772 -4.2660871 -4.269609 -4.275569 -4.2653942 -4.2430878 -4.2204642 -4.22627 -4.2342582 -4.2188549 -4.2066469 -4.2129345 -4.2273464][-4.2956486 -4.2968092 -4.2964149 -4.2958679 -4.2995181 -4.3064256 -4.3066835 -4.2978253 -4.2848849 -4.2842288 -4.2878456 -4.2756886 -4.2609334 -4.2615027 -4.2711911][-4.3205819 -4.3204622 -4.3199959 -4.3197937 -4.3225417 -4.3276372 -4.33069 -4.3289523 -4.3251133 -4.3262081 -4.3275294 -4.3180218 -4.3054 -4.3042645 -4.3111095][-4.3349013 -4.3354588 -4.3348908 -4.334486 -4.3356571 -4.3375487 -4.3385377 -4.3382378 -4.3378344 -4.3390007 -4.3391371 -4.3329635 -4.3241367 -4.3237338 -4.3285637]]...]
INFO - root - 2017-12-07 15:49:01.446117: step 24410, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.006 sec/batch; 86h:07m:25s remains)
INFO - root - 2017-12-07 15:49:11.203174: step 24420, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 84h:03m:50s remains)
INFO - root - 2017-12-07 15:49:20.826154: step 24430, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 84h:08m:15s remains)
INFO - root - 2017-12-07 15:49:30.435945: step 24440, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 79h:31m:26s remains)
INFO - root - 2017-12-07 15:49:40.282455: step 24450, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 83h:00m:07s remains)
INFO - root - 2017-12-07 15:49:49.869980: step 24460, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 79h:19m:22s remains)
INFO - root - 2017-12-07 15:49:59.362760: step 24470, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 80h:55m:37s remains)
INFO - root - 2017-12-07 15:50:09.237583: step 24480, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.023 sec/batch; 87h:34m:10s remains)
INFO - root - 2017-12-07 15:50:18.930592: step 24490, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 83h:02m:51s remains)
INFO - root - 2017-12-07 15:50:28.698289: step 24500, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 78h:11m:50s remains)
2017-12-07 15:50:29.803856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3268023 -4.3335438 -4.3357587 -4.3352036 -4.3362484 -4.3361673 -4.3400126 -4.3456616 -4.3430371 -4.3367324 -4.3300524 -4.3221116 -4.3148947 -4.3112698 -4.3128381][-4.3257484 -4.3355503 -4.3420024 -4.3445878 -4.3483238 -4.3474078 -4.350245 -4.355484 -4.3459921 -4.3304825 -4.3154907 -4.29809 -4.2828903 -4.280807 -4.2899795][-4.3132663 -4.323729 -4.330471 -4.3320832 -4.3345113 -4.32891 -4.3279881 -4.3309655 -4.315515 -4.2954783 -4.2774258 -4.256103 -4.2371645 -4.2390642 -4.2583203][-4.2949376 -4.3054075 -4.3120656 -4.3109055 -4.3096952 -4.2972732 -4.2887874 -4.2869935 -4.2692342 -4.2473073 -4.2285919 -4.2090039 -4.1944146 -4.2060237 -4.2347317][-4.2828584 -4.2918224 -4.295332 -4.2886839 -4.2792072 -4.2587304 -4.2410822 -4.2312369 -4.2124219 -4.1859269 -4.1651716 -4.1523566 -4.1518512 -4.1778374 -4.2138095][-4.2756381 -4.2817488 -4.2815819 -4.2673421 -4.2440009 -4.2097974 -4.17984 -4.15832 -4.133944 -4.1035666 -4.0867796 -4.0900273 -4.1107168 -4.14962 -4.19194][-4.27608 -4.2779098 -4.271409 -4.2481465 -4.2111044 -4.1585951 -4.1113544 -4.0725117 -4.0362887 -4.0064864 -4.00293 -4.0275884 -4.0696087 -4.1204333 -4.1706653][-4.2690377 -4.2678609 -4.2583189 -4.2296038 -4.1851935 -4.1200175 -4.0651741 -4.0250525 -3.9917946 -3.9742522 -3.9820533 -4.0135651 -4.0604591 -4.1137395 -4.1669927][-4.2684665 -4.2671995 -4.2599626 -4.2388034 -4.2023683 -4.1486683 -4.1089492 -4.0886359 -4.068624 -4.0603824 -4.0639968 -4.0789995 -4.1106367 -4.1495385 -4.1927314][-4.2874393 -4.2941151 -4.2951703 -4.2882833 -4.2675 -4.229785 -4.1990237 -4.1837621 -4.1659484 -4.1559386 -4.1520524 -4.1557417 -4.1745987 -4.1999817 -4.2311549][-4.3152704 -4.3271446 -4.3329496 -4.3322554 -4.3221097 -4.2990866 -4.2768097 -4.2614293 -4.2442074 -4.233294 -4.2255244 -4.223412 -4.2323217 -4.2454958 -4.2661676][-4.3343821 -4.3458595 -4.3521395 -4.354063 -4.3508124 -4.3387618 -4.3245974 -4.3123302 -4.2977667 -4.2858911 -4.27489 -4.2684765 -4.2707295 -4.2763443 -4.28945][-4.345861 -4.3548431 -4.3594055 -4.3597541 -4.3571234 -4.3501377 -4.3419785 -4.332974 -4.3225589 -4.3119092 -4.3014941 -4.2933893 -4.2928514 -4.2954717 -4.3032303][-4.3504148 -4.35478 -4.35619 -4.355834 -4.3543153 -4.3521624 -4.3480062 -4.34247 -4.3355522 -4.3274703 -4.3191357 -4.3123178 -4.3103223 -4.3098116 -4.3138213][-4.3526206 -4.3531351 -4.3524165 -4.3514867 -4.3509226 -4.3507557 -4.3491373 -4.3464694 -4.3436875 -4.3405457 -4.3360405 -4.3306613 -4.3267941 -4.3237562 -4.3245382]]...]
INFO - root - 2017-12-07 15:50:39.525317: step 24510, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 84h:51m:34s remains)
INFO - root - 2017-12-07 15:50:49.197837: step 24520, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 82h:30m:14s remains)
INFO - root - 2017-12-07 15:50:58.833575: step 24530, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.944 sec/batch; 80h:47m:31s remains)
INFO - root - 2017-12-07 15:51:08.562085: step 24540, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.006 sec/batch; 86h:04m:16s remains)
INFO - root - 2017-12-07 15:51:18.336995: step 24550, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 85h:55m:07s remains)
INFO - root - 2017-12-07 15:51:28.088946: step 24560, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 81h:03m:03s remains)
INFO - root - 2017-12-07 15:51:37.784435: step 24570, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 84h:32m:23s remains)
INFO - root - 2017-12-07 15:51:47.506434: step 24580, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 80h:17m:02s remains)
INFO - root - 2017-12-07 15:51:57.065307: step 24590, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 84h:19m:29s remains)
INFO - root - 2017-12-07 15:52:06.697691: step 24600, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 83h:39m:39s remains)
2017-12-07 15:52:07.769095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3158669 -4.3093057 -4.3047976 -4.3054371 -4.3084784 -4.3087845 -4.3087635 -4.3130965 -4.321991 -4.3300395 -4.3349566 -4.3370852 -4.3391151 -4.3408628 -4.343101][-4.2877908 -4.2799187 -4.2764874 -4.279757 -4.2847052 -4.2824197 -4.2808232 -4.2898979 -4.3086448 -4.325489 -4.3362641 -4.3407354 -4.3421812 -4.3418736 -4.3420982][-4.2546391 -4.2473917 -4.248116 -4.2548962 -4.2594943 -4.25117 -4.2423325 -4.2535481 -4.284596 -4.3114767 -4.3260508 -4.3330736 -4.3373742 -4.3371892 -4.3354526][-4.22094 -4.2144914 -4.2181058 -4.2259932 -4.2265167 -4.20753 -4.1878018 -4.2017231 -4.2469316 -4.2830763 -4.3002596 -4.31002 -4.3178191 -4.3178267 -4.3153358][-4.1992836 -4.1912107 -4.1929569 -4.1938124 -4.1827569 -4.1458244 -4.1132531 -4.1342278 -4.1937914 -4.2383943 -4.2592764 -4.2738323 -4.2870936 -4.2874 -4.2822919][-4.1970382 -4.1812725 -4.1685791 -4.1489868 -4.1119018 -4.0458417 -4.0009069 -4.0364151 -4.1149631 -4.1702585 -4.2008705 -4.2281427 -4.25167 -4.2556429 -4.2491932][-4.2036781 -4.1768365 -4.1437435 -4.0957346 -4.0260506 -3.930917 -3.8790705 -3.9341352 -4.0339513 -4.1017184 -4.1448083 -4.1840982 -4.2179208 -4.229744 -4.2272215][-4.2122555 -4.180809 -4.13943 -4.07561 -3.9879975 -3.8847408 -3.835844 -3.895159 -3.9929569 -4.056376 -4.1002741 -4.1479115 -4.1927824 -4.2117023 -4.2143297][-4.22444 -4.1984882 -4.1643062 -4.1087551 -4.0383234 -3.9632597 -3.9263213 -3.9628172 -4.0260534 -4.0610924 -4.08973 -4.1408429 -4.1932082 -4.2173891 -4.225677][-4.2366543 -4.217484 -4.1946239 -4.1585 -4.1177907 -4.0758815 -4.0520763 -4.0688057 -4.0986943 -4.1100969 -4.1216574 -4.1657481 -4.2130556 -4.2357683 -4.2457662][-4.2559 -4.2415314 -4.2242422 -4.2021565 -4.1820669 -4.158658 -4.1433721 -4.15265 -4.1670957 -4.1668329 -4.1655197 -4.1947703 -4.2312932 -4.249887 -4.2605348][-4.2806921 -4.267868 -4.2491984 -4.229867 -4.2160525 -4.2002068 -4.1905608 -4.2005334 -4.212194 -4.2082915 -4.2005486 -4.2178707 -4.2454538 -4.2604065 -4.2696061][-4.3059831 -4.2945318 -4.2744246 -4.2522764 -4.2352214 -4.2181282 -4.2108932 -4.2229471 -4.2348232 -4.2343059 -4.2327518 -4.2470012 -4.268939 -4.28009 -4.2874746][-4.3256822 -4.3169284 -4.2999945 -4.2806048 -4.2643714 -4.250349 -4.2465258 -4.2564974 -4.2643857 -4.2648873 -4.2683125 -4.2799435 -4.2942853 -4.3013883 -4.3070993][-4.3430738 -4.3390856 -4.328917 -4.3172579 -4.307457 -4.30032 -4.299325 -4.3036885 -4.3050594 -4.3031774 -4.3043737 -4.310142 -4.3171225 -4.3207054 -4.3246183]]...]
INFO - root - 2017-12-07 15:52:17.449458: step 24610, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 82h:33m:35s remains)
INFO - root - 2017-12-07 15:52:27.085180: step 24620, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 83h:25m:14s remains)
INFO - root - 2017-12-07 15:52:36.739798: step 24630, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 85h:03m:28s remains)
INFO - root - 2017-12-07 15:52:46.269251: step 24640, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 82h:32m:37s remains)
INFO - root - 2017-12-07 15:52:56.027997: step 24650, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 82h:34m:45s remains)
INFO - root - 2017-12-07 15:53:05.605777: step 24660, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 81h:19m:29s remains)
INFO - root - 2017-12-07 15:53:15.087341: step 24670, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 81h:37m:09s remains)
INFO - root - 2017-12-07 15:53:24.888639: step 24680, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 80h:42m:30s remains)
INFO - root - 2017-12-07 15:53:34.612028: step 24690, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.003 sec/batch; 85h:44m:53s remains)
INFO - root - 2017-12-07 15:53:44.415673: step 24700, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 83h:21m:37s remains)
2017-12-07 15:53:45.344609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3016176 -4.3039188 -4.3062487 -4.3082523 -4.309607 -4.3083944 -4.3056917 -4.3028355 -4.3002191 -4.2989297 -4.2997408 -4.3030329 -4.3076372 -4.3109307 -4.3139458][-4.3194451 -4.3203769 -4.3211403 -4.3221622 -4.3233457 -4.3227067 -4.3206787 -4.318573 -4.3167858 -4.3163576 -4.3167129 -4.3190651 -4.3233404 -4.3263674 -4.329936][-4.3346 -4.3326354 -4.330071 -4.3280988 -4.3258743 -4.3202219 -4.3118725 -4.3047013 -4.3008132 -4.300848 -4.3018675 -4.3055959 -4.3119464 -4.3166032 -4.3209066][-4.3320665 -4.323936 -4.31441 -4.3049364 -4.2949805 -4.2807813 -4.2624826 -4.2478952 -4.2441378 -4.248898 -4.2537069 -4.2635765 -4.2759452 -4.2831612 -4.287581][-4.3115888 -4.2951622 -4.2748046 -4.2535534 -4.2337151 -4.2111192 -4.1828117 -4.1612234 -4.1622219 -4.1788268 -4.1928935 -4.2134628 -4.2315044 -4.2384653 -4.2384472][-4.2810078 -4.2534103 -4.2165775 -4.1783319 -4.1461558 -4.1152411 -4.0792718 -4.054852 -4.0666432 -4.0997586 -4.1283617 -4.1597013 -4.1824961 -4.1867466 -4.1827345][-4.2503662 -4.2075782 -4.1508117 -4.0943251 -4.0521049 -4.0150366 -3.9710107 -3.9480276 -3.9827554 -4.0392079 -4.0837784 -4.1234951 -4.148994 -4.1491852 -4.1374116][-4.2340627 -4.1802144 -4.1134005 -4.0521154 -4.0131121 -3.981456 -3.9425442 -3.929666 -3.980772 -4.04799 -4.0962725 -4.1368194 -4.1602597 -4.1588984 -4.1428876][-4.2440453 -4.192461 -4.1347065 -4.0870361 -4.0629554 -4.0469379 -4.0267148 -4.0243292 -4.06739 -4.1202707 -4.1579642 -4.1892128 -4.2043052 -4.2044277 -4.1938119][-4.2724738 -4.2331705 -4.1924677 -4.1626067 -4.1514554 -4.1473589 -4.1402006 -4.1409588 -4.167243 -4.1998367 -4.2260537 -4.2473373 -4.2541885 -4.2510529 -4.2436032][-4.3020267 -4.2767644 -4.25246 -4.2369103 -4.234304 -4.2365322 -4.237361 -4.2403517 -4.25473 -4.2704363 -4.2835164 -4.2927465 -4.2940445 -4.2897115 -4.284616][-4.3255506 -4.3092093 -4.2953825 -4.2877994 -4.287817 -4.2904234 -4.2920833 -4.2952785 -4.3026633 -4.3104482 -4.3171124 -4.3228951 -4.3247256 -4.3228583 -4.3198795][-4.3436027 -4.33343 -4.3258533 -4.3213067 -4.3206625 -4.3228221 -4.3252358 -4.3286381 -4.3324656 -4.3363361 -4.34024 -4.3434467 -4.3438873 -4.3418064 -4.3391652][-4.3525524 -4.3444448 -4.3386335 -4.3336887 -4.3313975 -4.3324647 -4.3349633 -4.3371153 -4.3387551 -4.3404851 -4.3409095 -4.3400908 -4.3386884 -4.3372564 -4.3366938][-4.3499889 -4.34022 -4.3319626 -4.3242064 -4.3200593 -4.3204269 -4.3235869 -4.3260875 -4.32762 -4.3292222 -4.3289385 -4.3269777 -4.3259144 -4.3268275 -4.3297448]]...]
INFO - root - 2017-12-07 15:53:54.928483: step 24710, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 80h:39m:07s remains)
INFO - root - 2017-12-07 15:54:04.616639: step 24720, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.977 sec/batch; 83h:30m:41s remains)
INFO - root - 2017-12-07 15:54:14.332371: step 24730, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.986 sec/batch; 84h:16m:52s remains)
INFO - root - 2017-12-07 15:54:24.119626: step 24740, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 85h:13m:49s remains)
INFO - root - 2017-12-07 15:54:33.733387: step 24750, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 83h:05m:33s remains)
INFO - root - 2017-12-07 15:54:43.294769: step 24760, loss = 2.11, batch loss = 2.06 (8.2 examples/sec; 0.980 sec/batch; 83h:48m:38s remains)
INFO - root - 2017-12-07 15:54:52.812857: step 24770, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 78h:07m:12s remains)
INFO - root - 2017-12-07 15:55:02.496903: step 24780, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 80h:11m:08s remains)
INFO - root - 2017-12-07 15:55:12.196511: step 24790, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 82h:26m:29s remains)
INFO - root - 2017-12-07 15:55:21.638878: step 24800, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 74h:58m:28s remains)
2017-12-07 15:55:22.681662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2289271 -4.2401285 -4.2601967 -4.2739916 -4.2782922 -4.2731047 -4.2641025 -4.2543831 -4.2460461 -4.2400565 -4.2378421 -4.2315712 -4.2221074 -4.2177858 -4.2183366][-4.2241855 -4.2401414 -4.263659 -4.2799039 -4.2818108 -4.2686605 -4.2517605 -4.237144 -4.2304759 -4.2291 -4.2335277 -4.2349911 -4.2340813 -4.2403574 -4.25324][-4.2237 -4.2406921 -4.2638731 -4.2795339 -4.2768946 -4.2578545 -4.2378445 -4.2246437 -4.2266145 -4.2366824 -4.25219 -4.2645922 -4.2713346 -4.2794986 -4.2933564][-4.2324386 -4.2408195 -4.2559404 -4.2683711 -4.2644262 -4.2479224 -4.2322259 -4.2257757 -4.2362466 -4.2582741 -4.2841549 -4.3012414 -4.3061624 -4.3074656 -4.31318][-4.2646661 -4.2591257 -4.2639236 -4.2698188 -4.2610083 -4.2429132 -4.2268405 -4.2203979 -4.2316422 -4.2620869 -4.2912064 -4.3072872 -4.3054743 -4.2952089 -4.2926726][-4.2887664 -4.2780809 -4.2776666 -4.2783146 -4.2622848 -4.2385912 -4.2146158 -4.2005191 -4.2082458 -4.2408047 -4.2644072 -4.2739081 -4.2645407 -4.2447667 -4.2359047][-4.2833443 -4.2700381 -4.2643132 -4.2569528 -4.233645 -4.1993141 -4.1577191 -4.1284633 -4.1339421 -4.1741567 -4.1982517 -4.2078667 -4.1973977 -4.1730013 -4.1613469][-4.2607574 -4.2474651 -4.2338562 -4.2157078 -4.1834469 -4.1380062 -4.0733318 -4.0182366 -4.0218778 -4.07457 -4.1098428 -4.1280088 -4.1241922 -4.1115355 -4.1088014][-4.2281332 -4.2222633 -4.2090693 -4.1892457 -4.1563458 -4.1159592 -4.0605583 -4.0057654 -4.0100746 -4.0611162 -4.0973964 -4.1182094 -4.1254964 -4.1316762 -4.1409931][-4.2168589 -4.2152529 -4.2063041 -4.1888237 -4.1646557 -4.148932 -4.1329207 -4.1097183 -4.1155252 -4.1437359 -4.1696415 -4.1894183 -4.2037921 -4.2142005 -4.2172146][-4.2269025 -4.226572 -4.222218 -4.2121959 -4.202476 -4.2075386 -4.2163262 -4.2137346 -4.2196908 -4.2327437 -4.2479153 -4.2598314 -4.2681656 -4.2704372 -4.2635303][-4.221786 -4.2263775 -4.2329874 -4.2316475 -4.2331262 -4.2447014 -4.2582436 -4.2619772 -4.2638779 -4.2660017 -4.2705245 -4.274507 -4.2773046 -4.2722807 -4.2604885][-4.1956339 -4.1997824 -4.2072225 -4.2063317 -4.213172 -4.2268939 -4.2390804 -4.2416844 -4.2395444 -4.237988 -4.2392168 -4.2414865 -4.24221 -4.2337618 -4.2224841][-4.1634083 -4.1605358 -4.1617441 -4.15311 -4.1567874 -4.1699209 -4.181211 -4.1852851 -4.1848164 -4.1838856 -4.1836696 -4.1847134 -4.1858153 -4.1821895 -4.1769776][-4.1565375 -4.1452236 -4.1363344 -4.12119 -4.1188984 -4.1278281 -4.1390724 -4.1481581 -4.1511755 -4.1518984 -4.1481419 -4.1462188 -4.1463614 -4.1481924 -4.1537628]]...]
INFO - root - 2017-12-07 15:55:32.348580: step 24810, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 78h:57m:46s remains)
INFO - root - 2017-12-07 15:55:42.086458: step 24820, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 80h:36m:48s remains)
INFO - root - 2017-12-07 15:55:51.792584: step 24830, loss = 2.05, batch loss = 2.00 (7.8 examples/sec; 1.028 sec/batch; 87h:50m:02s remains)
INFO - root - 2017-12-07 15:56:01.449220: step 24840, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 76h:54m:14s remains)
INFO - root - 2017-12-07 15:56:11.092482: step 24850, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 82h:39m:52s remains)
INFO - root - 2017-12-07 15:56:20.617975: step 24860, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 78h:58m:04s remains)
INFO - root - 2017-12-07 15:56:30.280273: step 24870, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 81h:04m:08s remains)
INFO - root - 2017-12-07 15:56:40.050538: step 24880, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 84h:28m:20s remains)
INFO - root - 2017-12-07 15:56:49.730698: step 24890, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 83h:15m:54s remains)
INFO - root - 2017-12-07 15:56:59.529667: step 24900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 81h:42m:18s remains)
2017-12-07 15:57:00.488791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3083138 -4.302053 -4.2953062 -4.29139 -4.2895465 -4.2874608 -4.286243 -4.2778707 -4.26685 -4.25875 -4.2493715 -4.24566 -4.2439203 -4.240314 -4.234334][-4.3113341 -4.3086815 -4.3034749 -4.3023119 -4.3024864 -4.3002763 -4.2984362 -4.2886753 -4.2752976 -4.2673354 -4.2612872 -4.2583537 -4.2538762 -4.2497191 -4.2490339][-4.314889 -4.3141932 -4.3089461 -4.3088636 -4.308557 -4.3066068 -4.3037419 -4.2916603 -4.2787385 -4.2766066 -4.2805815 -4.2836137 -4.2781506 -4.2690487 -4.2674246][-4.3109241 -4.3109918 -4.3049655 -4.3045707 -4.3048363 -4.3015308 -4.294558 -4.2777557 -4.2684212 -4.2782 -4.2951965 -4.3055897 -4.3020768 -4.2905746 -4.2862558][-4.2922406 -4.2900791 -4.2792587 -4.2732711 -4.2703705 -4.2627039 -4.247139 -4.2212248 -4.2213221 -4.248867 -4.2808833 -4.3022962 -4.3041196 -4.2960186 -4.292686][-4.2617993 -4.25985 -4.2463174 -4.2306428 -4.2162375 -4.1932268 -4.1508074 -4.1023378 -4.1195493 -4.174624 -4.2280259 -4.262424 -4.2710981 -4.2701297 -4.2715964][-4.2265368 -4.2294559 -4.2171125 -4.1908531 -4.1596994 -4.1130314 -4.0302486 -3.9459376 -3.9863014 -4.0745296 -4.1495786 -4.1977353 -4.2131495 -4.216795 -4.221755][-4.2032495 -4.2145662 -4.2091341 -4.1816397 -4.1431947 -4.0833497 -3.9854536 -3.8889141 -3.9332194 -4.0242915 -4.0993438 -4.1468487 -4.1635313 -4.1706834 -4.1811676][-4.209341 -4.2268 -4.2271547 -4.2060332 -4.17149 -4.1184025 -4.0439463 -3.9840546 -4.0170517 -4.0736923 -4.1207113 -4.152317 -4.1603179 -4.1651578 -4.1784525][-4.2357492 -4.2535739 -4.2553935 -4.2422624 -4.2187595 -4.18228 -4.1370025 -4.1127877 -4.1383128 -4.1664162 -4.1876507 -4.2026448 -4.2035661 -4.2037439 -4.2133265][-4.2663608 -4.2823005 -4.2820954 -4.27467 -4.2630577 -4.245698 -4.2255778 -4.2213149 -4.2398839 -4.2515411 -4.2579446 -4.2596631 -4.2547636 -4.2513318 -4.2588911][-4.2874155 -4.3007836 -4.2996831 -4.2964272 -4.2931137 -4.289762 -4.2869821 -4.2909303 -4.30447 -4.3081932 -4.3051343 -4.30049 -4.2933421 -4.2883282 -4.2934179][-4.2952757 -4.3053493 -4.3041067 -4.3015347 -4.3031859 -4.3066549 -4.3097062 -4.3157973 -4.3250537 -4.3244362 -4.3189464 -4.3149853 -4.31006 -4.3063912 -4.3095555][-4.2961287 -4.3010283 -4.2998233 -4.2972355 -4.2981811 -4.3036046 -4.3093853 -4.3144989 -4.3189297 -4.315618 -4.3104763 -4.3087168 -4.3078766 -4.3086824 -4.3131084][-4.2945929 -4.2935233 -4.29061 -4.286026 -4.2839522 -4.2900577 -4.2987437 -4.3042808 -4.3061242 -4.3028731 -4.2984152 -4.2978311 -4.3001103 -4.3056974 -4.3130627]]...]
INFO - root - 2017-12-07 15:57:10.069089: step 24910, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 81h:27m:18s remains)
INFO - root - 2017-12-07 15:57:19.635386: step 24920, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 79h:22m:40s remains)
INFO - root - 2017-12-07 15:57:29.179022: step 24930, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 77h:25m:14s remains)
INFO - root - 2017-12-07 15:57:38.821044: step 24940, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 82h:38m:17s remains)
INFO - root - 2017-12-07 15:57:48.432548: step 24950, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 83h:56m:58s remains)
INFO - root - 2017-12-07 15:57:58.047004: step 24960, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 82h:23m:36s remains)
INFO - root - 2017-12-07 15:58:07.517473: step 24970, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 79h:40m:00s remains)
INFO - root - 2017-12-07 15:58:17.261686: step 24980, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 83h:18m:46s remains)
INFO - root - 2017-12-07 15:58:26.919906: step 24990, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 83h:06m:52s remains)
INFO - root - 2017-12-07 15:58:36.554873: step 25000, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 84h:10m:40s remains)
2017-12-07 15:58:37.591023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2951064 -4.2892661 -4.29017 -4.2939925 -4.2965736 -4.2965469 -4.3005824 -4.3060021 -4.3085322 -4.3056693 -4.3014884 -4.2916636 -4.2566929 -4.1993942 -4.1412487][-4.2901444 -4.2798524 -4.2740803 -4.2728548 -4.2704048 -4.2671328 -4.2712612 -4.2744193 -4.2784729 -4.2834344 -4.2824745 -4.2711625 -4.2344122 -4.1738892 -4.11391][-4.2866526 -4.2689567 -4.25192 -4.2390413 -4.2234097 -4.2118669 -4.2107992 -4.2071662 -4.2146134 -4.2362328 -4.2505293 -4.2478561 -4.2200232 -4.1685815 -4.1142354][-4.2860541 -4.2585588 -4.229207 -4.2015142 -4.1708603 -4.145678 -4.1281796 -4.1096792 -4.12311 -4.1717267 -4.2114463 -4.221848 -4.2045832 -4.1638069 -4.1189461][-4.2826271 -4.24624 -4.2056618 -4.165513 -4.1215525 -4.0820584 -4.0377588 -3.9921126 -4.0195103 -4.1093144 -4.1740236 -4.195672 -4.1848984 -4.1492038 -4.1074724][-4.2813358 -4.2403884 -4.1871614 -4.1315355 -4.073308 -4.0139589 -3.9281223 -3.8397355 -3.8903866 -4.0368829 -4.1306105 -4.1625681 -4.1557069 -4.121582 -4.0866518][-4.2674613 -4.225317 -4.1629019 -4.0991559 -4.0326037 -3.9573121 -3.8326972 -3.6953912 -3.7662661 -3.9599483 -4.0806737 -4.1282125 -4.1327605 -4.1087322 -4.0903172][-4.2444291 -4.210825 -4.1584153 -4.1075144 -4.0584097 -3.9985583 -3.8865573 -3.760216 -3.8227756 -3.9901893 -4.0941939 -4.1427565 -4.1503711 -4.1380768 -4.1373377][-4.229218 -4.2073274 -4.1727762 -4.1454086 -4.1277394 -4.1000581 -4.030479 -3.9539306 -3.9943166 -4.1002955 -4.1598172 -4.1886778 -4.184473 -4.1736712 -4.1800971][-4.2236681 -4.2070165 -4.1871619 -4.1813512 -4.18938 -4.1845303 -4.14853 -4.1099343 -4.1326394 -4.192441 -4.2221451 -4.2324276 -4.2232213 -4.217258 -4.223712][-4.215837 -4.1981058 -4.1913643 -4.2048235 -4.2288213 -4.238327 -4.2275534 -4.2124119 -4.2231359 -4.2503772 -4.2611213 -4.258532 -4.2524667 -4.2506561 -4.2539716][-4.2153559 -4.1945653 -4.1957431 -4.2217431 -4.2504039 -4.2666259 -4.2738509 -4.2755041 -4.2795959 -4.2914653 -4.29292 -4.2857962 -4.2831354 -4.2827678 -4.2791314][-4.2346997 -4.210454 -4.2094941 -4.2345929 -4.2610784 -4.2806726 -4.3007903 -4.3143206 -4.3204489 -4.3251462 -4.3225765 -4.3136187 -4.3070107 -4.3028908 -4.2967119][-4.2601924 -4.236712 -4.2333183 -4.2538629 -4.2782125 -4.2990232 -4.3201022 -4.335866 -4.341404 -4.3377361 -4.3290906 -4.3195229 -4.3131104 -4.3094211 -4.3071213][-4.2809353 -4.2651596 -4.2657471 -4.2833409 -4.3040133 -4.3205504 -4.3344374 -4.3429952 -4.3432031 -4.3350439 -4.3251014 -4.3172803 -4.313345 -4.3126993 -4.3147526]]...]
INFO - root - 2017-12-07 15:58:47.242243: step 25010, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 82h:13m:05s remains)
INFO - root - 2017-12-07 15:58:56.691195: step 25020, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 81h:14m:39s remains)
INFO - root - 2017-12-07 15:59:06.230343: step 25030, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 83h:30m:43s remains)
INFO - root - 2017-12-07 15:59:15.821555: step 25040, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 81h:38m:08s remains)
INFO - root - 2017-12-07 15:59:25.466039: step 25050, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 81h:41m:50s remains)
INFO - root - 2017-12-07 15:59:35.002991: step 25060, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 82h:28m:03s remains)
INFO - root - 2017-12-07 15:59:44.512423: step 25070, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 79h:41m:28s remains)
INFO - root - 2017-12-07 15:59:54.159482: step 25080, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 79h:55m:34s remains)
INFO - root - 2017-12-07 16:00:03.747575: step 25090, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 84h:41m:43s remains)
INFO - root - 2017-12-07 16:00:13.369528: step 25100, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 81h:04m:34s remains)
2017-12-07 16:00:14.364828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3304043 -4.3225513 -4.31452 -4.3095231 -4.3035307 -4.29398 -4.2820473 -4.2753158 -4.2850924 -4.300714 -4.297122 -4.2812057 -4.2719817 -4.2696857 -4.2722073][-4.3132162 -4.303257 -4.2944593 -4.290092 -4.284492 -4.2674861 -4.2421823 -4.2250834 -4.2352796 -4.2610817 -4.2626252 -4.2433057 -4.2302666 -4.2261825 -4.229394][-4.2908878 -4.2742591 -4.2613487 -4.2576356 -4.25264 -4.2306852 -4.1943588 -4.1678424 -4.1749797 -4.2042928 -4.2084713 -4.1893158 -4.1718235 -4.1659303 -4.1736522][-4.2724013 -4.2519865 -4.2395105 -4.2370496 -4.2315583 -4.2024722 -4.1495752 -4.1059012 -4.1128826 -4.1491814 -4.159574 -4.1494131 -4.138011 -4.1343889 -4.1452475][-4.2566929 -4.2359304 -4.22367 -4.2212696 -4.2103472 -4.1679149 -4.0891123 -4.0156846 -4.0269432 -4.081306 -4.1049352 -4.1119418 -4.1146336 -4.1231236 -4.1368394][-4.238482 -4.2144403 -4.2006149 -4.1966982 -4.1746011 -4.1089778 -3.9910073 -3.8688254 -3.8787613 -3.9730833 -4.0377817 -4.0766029 -4.0990481 -4.1169529 -4.131753][-4.2252555 -4.1979294 -4.1811986 -4.1709089 -4.1341257 -4.0393934 -3.8823047 -3.7092688 -3.7244418 -3.885932 -4.0039997 -4.0733361 -4.1008258 -4.1138315 -4.1231332][-4.2158055 -4.1843719 -4.1642976 -4.1482763 -4.1107225 -4.0223265 -3.8881817 -3.7521234 -3.7790184 -3.9313486 -4.0358129 -4.0992908 -4.1214566 -4.1268363 -4.1323895][-4.2159443 -4.1851306 -4.1684351 -4.1552281 -4.1319227 -4.0787005 -4.0074468 -3.9373674 -3.9462154 -4.0332355 -4.0941315 -4.1354942 -4.1504822 -4.1561275 -4.1621][-4.2244859 -4.1968169 -4.1878285 -4.1818595 -4.17411 -4.1514769 -4.1186218 -4.0767303 -4.0698557 -4.1141391 -4.1480179 -4.1719809 -4.1779957 -4.1821561 -4.1892781][-4.2307696 -4.2058353 -4.2046752 -4.2116022 -4.2179255 -4.2092915 -4.1873178 -4.1546707 -4.1432748 -4.1682291 -4.1889215 -4.2025661 -4.2032471 -4.2067494 -4.217411][-4.2402797 -4.2150421 -4.2181745 -4.2345161 -4.2492156 -4.24605 -4.2281528 -4.2005162 -4.1884322 -4.2021918 -4.2114244 -4.2162175 -4.217927 -4.2290545 -4.2439017][-4.258184 -4.2331772 -4.2327075 -4.2472482 -4.2622166 -4.2623053 -4.2515092 -4.2351341 -4.2267375 -4.2302384 -4.2305264 -4.2303305 -4.2367072 -4.2537608 -4.2694945][-4.2881322 -4.2693639 -4.2679024 -4.2795558 -4.2936912 -4.2984891 -4.29243 -4.2807908 -4.270432 -4.2641058 -4.2605605 -4.2625208 -4.2735834 -4.2909284 -4.3031507][-4.3215833 -4.3114548 -4.3100839 -4.3171968 -4.3263354 -4.3308 -4.3269229 -4.3178396 -4.3088241 -4.3026109 -4.3010154 -4.3052506 -4.3125238 -4.322628 -4.3287287]]...]
INFO - root - 2017-12-07 16:00:24.046222: step 25110, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 83h:10m:27s remains)
INFO - root - 2017-12-07 16:00:33.619712: step 25120, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 81h:34m:11s remains)
INFO - root - 2017-12-07 16:00:43.329372: step 25130, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 79h:36m:27s remains)
INFO - root - 2017-12-07 16:00:52.815719: step 25140, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 83h:51m:22s remains)
INFO - root - 2017-12-07 16:01:02.522950: step 25150, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.982 sec/batch; 83h:47m:55s remains)
INFO - root - 2017-12-07 16:01:12.162525: step 25160, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.939 sec/batch; 80h:08m:47s remains)
INFO - root - 2017-12-07 16:01:21.673083: step 25170, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 81h:34m:04s remains)
INFO - root - 2017-12-07 16:01:31.491768: step 25180, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 86h:13m:14s remains)
INFO - root - 2017-12-07 16:01:41.136867: step 25190, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 81h:58m:53s remains)
INFO - root - 2017-12-07 16:01:50.615129: step 25200, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 78h:31m:35s remains)
2017-12-07 16:01:51.502771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2904153 -4.2806253 -4.2732782 -4.2525053 -4.2239413 -4.1922221 -4.1707687 -4.1750631 -4.1958036 -4.2052412 -4.1984506 -4.2070036 -4.2286959 -4.2456427 -4.2499747][-4.2907457 -4.281796 -4.2731194 -4.2462616 -4.20479 -4.1546774 -4.1178107 -4.1217895 -4.1514492 -4.171813 -4.1709404 -4.18131 -4.2076473 -4.2287378 -4.2355318][-4.2914796 -4.2790189 -4.2649717 -4.2327781 -4.1862068 -4.1241479 -4.0662727 -4.0632639 -4.1031675 -4.1333122 -4.1366096 -4.15311 -4.1929488 -4.218534 -4.2252836][-4.2894135 -4.2712955 -4.2538185 -4.2240458 -4.1850657 -4.1230984 -4.046483 -4.0275025 -4.068428 -4.1036997 -4.1134572 -4.1415453 -4.1967907 -4.2254214 -4.2306771][-4.2872772 -4.2665572 -4.2499962 -4.2278872 -4.200223 -4.142241 -4.0582457 -4.0283465 -4.0651932 -4.1032538 -4.1223617 -4.1584892 -4.2161679 -4.2391386 -4.2367563][-4.2868586 -4.2667489 -4.2509627 -4.229928 -4.2035313 -4.1428709 -4.0479321 -4.0038466 -4.049068 -4.1101127 -4.1456919 -4.1856 -4.2296877 -4.2346725 -4.2236032][-4.2840753 -4.2605324 -4.2418571 -4.2162123 -4.1773152 -4.0974126 -3.9644468 -3.8859186 -3.9648073 -4.0765944 -4.1323872 -4.1701875 -4.1990547 -4.1857204 -4.1636834][-4.2840261 -4.2585988 -4.2384439 -4.2052264 -4.1486549 -4.0476775 -3.8825779 -3.7786052 -3.8979702 -4.046474 -4.1069326 -4.1286836 -4.1376328 -4.1060934 -4.0682759][-4.2917724 -4.2690935 -4.2477255 -4.2056656 -4.1431608 -4.0533819 -3.9225831 -3.854552 -3.9574578 -4.0722365 -4.1084523 -4.1044188 -4.0929146 -4.0467978 -4.0005174][-4.3025165 -4.2828631 -4.2618194 -4.220439 -4.1671128 -4.1048684 -4.0248303 -3.9881129 -4.0494204 -4.1148748 -4.1228442 -4.1006818 -4.081418 -4.0406842 -4.00867][-4.31063 -4.2932014 -4.2736149 -4.2404356 -4.2005177 -4.1542583 -4.1045613 -4.081593 -4.1133523 -4.1447935 -4.1379294 -4.1164145 -4.1062665 -4.0845628 -4.0688195][-4.3139544 -4.2971249 -4.2800736 -4.2573175 -4.2292285 -4.1930585 -4.1612468 -4.1456366 -4.1609707 -4.1719046 -4.1604614 -4.144805 -4.1410861 -4.1309996 -4.1255188][-4.315587 -4.3022246 -4.2898483 -4.2771373 -4.2598996 -4.231627 -4.2094378 -4.2018127 -4.2082176 -4.2077765 -4.1966667 -4.18181 -4.1753573 -4.1672831 -4.1631355][-4.3152008 -4.3053503 -4.2996464 -4.2982559 -4.2916565 -4.2712026 -4.2522678 -4.2456608 -4.2473435 -4.2413235 -4.2297435 -4.2179904 -4.2114077 -4.19868 -4.1918826][-4.3133698 -4.3062181 -4.3051691 -4.310658 -4.3113828 -4.29793 -4.2791786 -4.2723179 -4.2762628 -4.2688966 -4.2571983 -4.2464104 -4.2380304 -4.2207313 -4.2077904]]...]
INFO - root - 2017-12-07 16:02:01.215867: step 25210, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.006 sec/batch; 85h:52m:26s remains)
INFO - root - 2017-12-07 16:02:10.933074: step 25220, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 80h:34m:19s remains)
INFO - root - 2017-12-07 16:02:20.427834: step 25230, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.982 sec/batch; 83h:47m:05s remains)
INFO - root - 2017-12-07 16:02:30.108012: step 25240, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 85h:04m:02s remains)
INFO - root - 2017-12-07 16:02:39.867436: step 25250, loss = 2.06, batch loss = 2.01 (7.8 examples/sec; 1.028 sec/batch; 87h:42m:33s remains)
INFO - root - 2017-12-07 16:02:49.403377: step 25260, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 81h:56m:47s remains)
INFO - root - 2017-12-07 16:02:58.883507: step 25270, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 81h:34m:53s remains)
INFO - root - 2017-12-07 16:03:08.512913: step 25280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 78h:38m:17s remains)
INFO - root - 2017-12-07 16:03:18.173037: step 25290, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 82h:10m:31s remains)
INFO - root - 2017-12-07 16:03:27.788335: step 25300, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 74h:00m:57s remains)
2017-12-07 16:03:28.710255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2426991 -4.2297897 -4.21801 -4.2058258 -4.1958928 -4.20376 -4.2225933 -4.2282815 -4.2284789 -4.2265344 -4.22031 -4.2165027 -4.2168374 -4.2152834 -4.2162237][-4.2584147 -4.2460694 -4.2341213 -4.2214241 -4.2103477 -4.2167358 -4.2353969 -4.2399058 -4.2396684 -4.2375288 -4.2305317 -4.2238593 -4.2199168 -4.2105603 -4.2041645][-4.244524 -4.229815 -4.2173886 -4.2047691 -4.1935439 -4.1992459 -4.2151375 -4.2184873 -4.2181511 -4.219749 -4.2188158 -4.2175727 -4.2138872 -4.2004557 -4.1883965][-4.2080779 -4.1870828 -4.1751919 -4.1653776 -4.1579814 -4.1647587 -4.1762958 -4.17525 -4.1735253 -4.1814394 -4.19172 -4.2032189 -4.2092443 -4.2016211 -4.1873817][-4.1679344 -4.1405344 -4.1293397 -4.1247449 -4.1240449 -4.1316962 -4.1367621 -4.12764 -4.1239653 -4.1400509 -4.1626081 -4.186789 -4.205605 -4.207562 -4.193975][-4.1151376 -4.078896 -4.0641708 -4.0608826 -4.0639634 -4.0672803 -4.0583568 -4.0375419 -4.0404687 -4.0743241 -4.1155214 -4.153821 -4.1816554 -4.19448 -4.1868992][-4.07574 -4.0304794 -4.0081134 -3.9996655 -3.9956405 -3.979986 -3.9422998 -3.9015663 -3.9141579 -3.9700084 -4.0294132 -4.0789084 -4.1093049 -4.1257682 -4.1254039][-4.0584812 -4.0072556 -3.9749033 -3.9558694 -3.936738 -3.9003463 -3.8352079 -3.778919 -3.8039911 -3.8772941 -3.9468415 -3.9974625 -4.0181866 -4.0255804 -4.0278559][-4.04964 -3.9978294 -3.9588518 -3.9331675 -3.9066825 -3.8711395 -3.814764 -3.769819 -3.7967582 -3.8632782 -3.9224014 -3.959506 -3.963624 -3.9606709 -3.969161][-4.0739522 -4.0322003 -3.9991477 -3.9784856 -3.959106 -3.9406233 -3.9136524 -3.8902373 -3.9078424 -3.9506063 -3.987303 -4.0052881 -3.996733 -3.9848826 -3.9938998][-4.1551342 -4.1292682 -4.1074543 -4.0940971 -4.0851755 -4.0829277 -4.0787759 -4.0718393 -4.0804992 -4.1006045 -4.1169806 -4.121242 -4.1090937 -4.0962162 -4.1009517][-4.2524557 -4.2412548 -4.2289596 -4.2199888 -4.2173834 -4.2224588 -4.2282004 -4.2284751 -4.2310753 -4.2374926 -4.2430696 -4.2419953 -4.2324996 -4.2220111 -4.2221141][-4.3221231 -4.3190484 -4.3116355 -4.3044119 -4.3034754 -4.3079839 -4.3133411 -4.3138828 -4.3141012 -4.3159094 -4.3172832 -4.3151789 -4.3098826 -4.3044329 -4.3025136][-4.3540912 -4.3528924 -4.3477869 -4.342473 -4.3410559 -4.3426504 -4.3447981 -4.3451262 -4.3454623 -4.346838 -4.3477407 -4.3460627 -4.34293 -4.3405342 -4.33957][-4.3645587 -4.3635111 -4.3598032 -4.3568559 -4.3561831 -4.3566613 -4.3570924 -4.3567214 -4.3564858 -4.3573565 -4.3583837 -4.3582926 -4.3571491 -4.3558645 -4.3557467]]...]
INFO - root - 2017-12-07 16:03:38.274177: step 25310, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 78h:28m:54s remains)
INFO - root - 2017-12-07 16:03:48.065933: step 25320, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 84h:00m:19s remains)
INFO - root - 2017-12-07 16:03:57.644850: step 25330, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 80h:57m:50s remains)
INFO - root - 2017-12-07 16:04:07.179287: step 25340, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 78h:20m:43s remains)
INFO - root - 2017-12-07 16:04:16.854085: step 25350, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 83h:20m:19s remains)
INFO - root - 2017-12-07 16:04:26.639538: step 25360, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.041 sec/batch; 88h:48m:50s remains)
INFO - root - 2017-12-07 16:04:36.226062: step 25370, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 84h:32m:00s remains)
INFO - root - 2017-12-07 16:04:45.823652: step 25380, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 80h:26m:27s remains)
INFO - root - 2017-12-07 16:04:55.476457: step 25390, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 81h:41m:41s remains)
INFO - root - 2017-12-07 16:05:05.163610: step 25400, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 85h:37m:56s remains)
2017-12-07 16:05:06.087565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1866345 -4.1867609 -4.1926188 -4.1752014 -4.1527343 -4.1583157 -4.1777997 -4.1887565 -4.1941814 -4.179347 -4.1593451 -4.1447926 -4.1452303 -4.1641359 -4.1761885][-4.210176 -4.2033482 -4.2070317 -4.1958361 -4.1874557 -4.1949143 -4.2136245 -4.2180042 -4.2142749 -4.1977425 -4.1758528 -4.1582541 -4.1523438 -4.1673355 -4.1766634][-4.2378826 -4.2236247 -4.226892 -4.2249036 -4.2331038 -4.2424822 -4.2547593 -4.2478538 -4.2342424 -4.2095218 -4.18337 -4.1627955 -4.1560822 -4.1687284 -4.1756568][-4.2539191 -4.2370267 -4.2413592 -4.2441297 -4.2588344 -4.270093 -4.2737746 -4.2607226 -4.24516 -4.2171717 -4.1864028 -4.1610222 -4.1546445 -4.1699824 -4.1770873][-4.2256951 -4.2054315 -4.2150779 -4.2256026 -4.241631 -4.2521973 -4.2542796 -4.24413 -4.2323313 -4.2101436 -4.1823406 -4.1593347 -4.15356 -4.1678033 -4.1745572][-4.162303 -4.1457896 -4.1635628 -4.180191 -4.1984978 -4.2077 -4.2091088 -4.2022533 -4.1945052 -4.1835585 -4.1682329 -4.1514273 -4.1470337 -4.156528 -4.1665797][-4.0994081 -4.090734 -4.1211171 -4.1462154 -4.1602731 -4.1616263 -4.1491785 -4.1293793 -4.1247568 -4.13949 -4.1446524 -4.1307893 -4.12273 -4.1290855 -4.1387281][-4.0732317 -4.0638485 -4.0955062 -4.1240349 -4.1337476 -4.118392 -4.0798936 -4.0379853 -4.0381818 -4.08768 -4.1168828 -4.10996 -4.0969496 -4.0931921 -4.095912][-4.0898938 -4.0727139 -4.0987082 -4.1316547 -4.1418743 -4.113502 -4.0628061 -4.0169621 -4.0227127 -4.0838566 -4.1169372 -4.112206 -4.0998335 -4.086401 -4.0775356][-4.1413441 -4.1261592 -4.1439719 -4.1738081 -4.1867976 -4.1619205 -4.1240778 -4.0954919 -4.1007547 -4.1366673 -4.1516752 -4.137393 -4.1271305 -4.1196008 -4.1083269][-4.1894283 -4.1885252 -4.2062349 -4.2272649 -4.2354164 -4.2190733 -4.1888046 -4.174274 -4.1763549 -4.1902051 -4.1923227 -4.1750689 -4.1655421 -4.1638684 -4.15835][-4.2219796 -4.2298903 -4.2539248 -4.2740188 -4.2789187 -4.2637992 -4.2399673 -4.2299528 -4.2298393 -4.2311444 -4.2241096 -4.2057304 -4.1939054 -4.189714 -4.1904411][-4.24806 -4.2584686 -4.2809587 -4.2999449 -4.304554 -4.2921224 -4.2726078 -4.2624235 -4.2595987 -4.2558117 -4.2472572 -4.22565 -4.2071486 -4.2014785 -4.203083][-4.2611961 -4.2731776 -4.2922335 -4.3071408 -4.3082294 -4.297545 -4.2832603 -4.2713728 -4.26416 -4.2602081 -4.2515759 -4.2297015 -4.2085209 -4.1980448 -4.1983805][-4.25932 -4.2709789 -4.287622 -4.2976646 -4.2952685 -4.2851987 -4.2744141 -4.262876 -4.2511382 -4.2439222 -4.2326427 -4.2082691 -4.1843448 -4.1775641 -4.179697]]...]
INFO - root - 2017-12-07 16:05:15.799900: step 25410, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 81h:23m:32s remains)
INFO - root - 2017-12-07 16:05:25.504056: step 25420, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 81h:56m:02s remains)
INFO - root - 2017-12-07 16:05:35.168010: step 25430, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.001 sec/batch; 85h:22m:42s remains)
INFO - root - 2017-12-07 16:05:44.876775: step 25440, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 79h:59m:00s remains)
INFO - root - 2017-12-07 16:05:54.527290: step 25450, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.976 sec/batch; 83h:14m:44s remains)
INFO - root - 2017-12-07 16:06:04.134784: step 25460, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 81h:35m:19s remains)
INFO - root - 2017-12-07 16:06:13.590495: step 25470, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 80h:58m:03s remains)
INFO - root - 2017-12-07 16:06:23.253056: step 25480, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 83h:50m:16s remains)
INFO - root - 2017-12-07 16:06:32.921268: step 25490, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 83h:06m:20s remains)
INFO - root - 2017-12-07 16:06:42.628786: step 25500, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 81h:14m:51s remains)
2017-12-07 16:06:43.547515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28251 -4.2751365 -4.268765 -4.2651072 -4.2662268 -4.2727909 -4.2755289 -4.2784724 -4.2789245 -4.28103 -4.2850385 -4.28996 -4.2945266 -4.3017597 -4.3077979][-4.2850289 -4.2837267 -4.2825217 -4.283145 -4.2865176 -4.2927847 -4.292304 -4.2938957 -4.2952752 -4.2986078 -4.2993703 -4.299593 -4.3007913 -4.3021512 -4.3010883][-4.2747893 -4.2796659 -4.2876296 -4.2939262 -4.2997112 -4.3064733 -4.3030257 -4.3034029 -4.3042164 -4.3083191 -4.3073874 -4.3058567 -4.304635 -4.2998962 -4.2921157][-4.2411432 -4.2448096 -4.2562551 -4.2682757 -4.2756376 -4.2834072 -4.2823958 -4.2840095 -4.2872844 -4.2948842 -4.2973852 -4.3014965 -4.3006778 -4.29302 -4.2825727][-4.1892004 -4.1930528 -4.2074289 -4.22371 -4.2301383 -4.2337513 -4.236608 -4.2427325 -4.2521796 -4.2617817 -4.2712722 -4.2821074 -4.2802668 -4.2717829 -4.265862][-4.1436625 -4.146512 -4.1582603 -4.1679268 -4.1670885 -4.1643534 -4.1669478 -4.1776547 -4.196032 -4.21238 -4.2305803 -4.2490416 -4.2501616 -4.244236 -4.2440414][-4.1090479 -4.1026449 -4.0985785 -4.0890579 -4.0731859 -4.057023 -4.0524688 -4.0681129 -4.1028261 -4.1346951 -4.1637583 -4.1934252 -4.2059689 -4.2065244 -4.2130418][-4.0784063 -4.0529504 -4.0228763 -3.9841065 -3.9429955 -3.907212 -3.892055 -3.9124222 -3.9678397 -4.0228505 -4.0666342 -4.1096759 -4.1411233 -4.1565347 -4.1770358][-4.0788207 -4.0453396 -4.0056853 -3.9601023 -3.913115 -3.8693414 -3.8484108 -3.8643942 -3.9207015 -3.9796417 -4.0257168 -4.0717635 -4.1113133 -4.1369929 -4.1656656][-4.1103473 -4.0915251 -4.068779 -4.0457811 -4.0204043 -3.9937091 -3.9817822 -3.9931159 -4.0280023 -4.0654516 -4.0944428 -4.1257038 -4.1542072 -4.1715307 -4.1928458][-4.1514354 -4.1486344 -4.1437764 -4.1350155 -4.1228485 -4.1095471 -4.1046906 -4.1111665 -4.1301737 -4.150094 -4.1667905 -4.1887035 -4.2079525 -4.2159433 -4.2266827][-4.1860075 -4.1917014 -4.1996107 -4.201683 -4.1961479 -4.187561 -4.1832457 -4.1873746 -4.1966376 -4.209549 -4.2225485 -4.2412195 -4.2551227 -4.2591314 -4.2626171][-4.2260928 -4.2363605 -4.2507358 -4.2603612 -4.2603641 -4.2537928 -4.2496061 -4.2516794 -4.2558551 -4.2644291 -4.2750158 -4.2906294 -4.3024521 -4.3066978 -4.3052149][-4.2713971 -4.2828174 -4.29688 -4.3051534 -4.3051252 -4.2977052 -4.2918468 -4.2910457 -4.2932081 -4.2998734 -4.3084846 -4.3197527 -4.3296175 -4.3342304 -4.3326182][-4.3010168 -4.310267 -4.3198423 -4.324162 -4.3224444 -4.3162293 -4.3117089 -4.3107405 -4.3118119 -4.31619 -4.3219991 -4.3299742 -4.3379865 -4.3419805 -4.3407969]]...]
INFO - root - 2017-12-07 16:06:53.258374: step 25510, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 80h:36m:37s remains)
INFO - root - 2017-12-07 16:07:02.721703: step 25520, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 74h:07m:03s remains)
INFO - root - 2017-12-07 16:07:12.305120: step 25530, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 81h:31m:07s remains)
INFO - root - 2017-12-07 16:07:22.155679: step 25540, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 84h:39m:53s remains)
INFO - root - 2017-12-07 16:07:31.777663: step 25550, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 86h:33m:07s remains)
INFO - root - 2017-12-07 16:07:41.224668: step 25560, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 82h:59m:06s remains)
INFO - root - 2017-12-07 16:07:50.924196: step 25570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 79h:46m:09s remains)
INFO - root - 2017-12-07 16:08:00.682735: step 25580, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 83h:57m:01s remains)
INFO - root - 2017-12-07 16:08:10.367591: step 25590, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 81h:47m:27s remains)
INFO - root - 2017-12-07 16:08:20.272577: step 25600, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 84h:52m:42s remains)
2017-12-07 16:08:21.207609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464762 -4.2436585 -4.2418575 -4.2329469 -4.2118034 -4.1821938 -4.1536088 -4.1521578 -4.1886678 -4.2423615 -4.2908273 -4.3226147 -4.3369961 -4.3443694 -4.3458834][-4.2167344 -4.211709 -4.2173839 -4.2178359 -4.2035055 -4.174633 -4.14222 -4.1421251 -4.1818237 -4.2397785 -4.2907748 -4.3244529 -4.338201 -4.344717 -4.3458734][-4.1906204 -4.1831408 -4.1935153 -4.1989517 -4.1889982 -4.1574349 -4.1212411 -4.12593 -4.1708007 -4.2315421 -4.2859163 -4.3227358 -4.3367405 -4.3426518 -4.3443441][-4.1789346 -4.1648922 -4.1708055 -4.1729088 -4.1606417 -4.1243029 -4.0877995 -4.1028228 -4.1557159 -4.2205811 -4.2797775 -4.3210073 -4.3354783 -4.3401055 -4.3423114][-4.1861229 -4.1623154 -4.1543555 -4.1410871 -4.1159983 -4.0746317 -4.0445027 -4.0754223 -4.1404986 -4.2095189 -4.2720752 -4.3182616 -4.3346753 -4.338594 -4.3409672][-4.2148862 -4.184545 -4.1610475 -4.124474 -4.0777316 -4.0269394 -4.0052981 -4.0501671 -4.1265864 -4.1998563 -4.2642841 -4.3147559 -4.3339071 -4.3380785 -4.3406968][-4.2548518 -4.2254367 -4.1928678 -4.1393332 -4.0714688 -4.00749 -3.9842076 -4.0324945 -4.1151266 -4.1938329 -4.2597384 -4.3123517 -4.3338928 -4.3385115 -4.341011][-4.2930784 -4.2726159 -4.24439 -4.1894989 -4.1085773 -4.026144 -3.9839027 -4.0215 -4.1050034 -4.1889119 -4.2574196 -4.3108091 -4.3335867 -4.3392477 -4.3415151][-4.3200865 -4.3111143 -4.2936554 -4.2480674 -4.166328 -4.0674834 -3.9967465 -4.0127964 -4.0924263 -4.1812506 -4.2533693 -4.3076262 -4.3316627 -4.3395767 -4.3422508][-4.3323331 -4.3321524 -4.3230772 -4.2892594 -4.2160535 -4.11012 -4.0162086 -4.0083447 -4.0800462 -4.1714869 -4.24776 -4.3031888 -4.3286304 -4.3388138 -4.3430014][-4.3343678 -4.3398132 -4.3371167 -4.3134189 -4.2525611 -4.1506925 -4.0447259 -4.0146728 -4.0746064 -4.165102 -4.24392 -4.30019 -4.3267956 -4.3378415 -4.3434715][-4.3318624 -4.3404527 -4.3427219 -4.3269863 -4.2782307 -4.1868696 -4.0790224 -4.0310779 -4.076303 -4.1622567 -4.241818 -4.2994685 -4.3268776 -4.337543 -4.3439927][-4.3281832 -4.3367414 -4.3422589 -4.3335848 -4.2967472 -4.2185926 -4.1170244 -4.057498 -4.085268 -4.1629128 -4.2419119 -4.3004894 -4.3278747 -4.3384242 -4.3452768][-4.3243871 -4.3306808 -4.3378649 -4.3365111 -4.3118782 -4.2492681 -4.1620884 -4.1005769 -4.1105285 -4.1718211 -4.2443142 -4.3017759 -4.3291726 -4.3405914 -4.3473735][-4.3215408 -4.3250761 -4.3334694 -4.3376517 -4.3218584 -4.2716374 -4.2006421 -4.1462922 -4.144351 -4.1880279 -4.2505937 -4.304234 -4.3313103 -4.3436494 -4.3498316]]...]
INFO - root - 2017-12-07 16:08:30.999541: step 25610, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 85h:59m:30s remains)
INFO - root - 2017-12-07 16:08:40.269251: step 25620, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 83h:58m:46s remains)
INFO - root - 2017-12-07 16:08:49.914169: step 25630, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.962 sec/batch; 81h:59m:58s remains)
INFO - root - 2017-12-07 16:08:59.426724: step 25640, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 83h:29m:11s remains)
INFO - root - 2017-12-07 16:09:09.209363: step 25650, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.025 sec/batch; 87h:20m:01s remains)
INFO - root - 2017-12-07 16:09:18.835757: step 25660, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 81h:28m:49s remains)
INFO - root - 2017-12-07 16:09:28.532236: step 25670, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.948 sec/batch; 80h:47m:39s remains)
INFO - root - 2017-12-07 16:09:38.144437: step 25680, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 83h:34m:01s remains)
INFO - root - 2017-12-07 16:09:47.737192: step 25690, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 79h:05m:58s remains)
INFO - root - 2017-12-07 16:09:57.499339: step 25700, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 82h:43m:32s remains)
2017-12-07 16:09:58.426591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2996407 -4.2956915 -4.2851849 -4.2701077 -4.2693725 -4.2807837 -4.2928281 -4.2994738 -4.2942657 -4.2740006 -4.2382708 -4.1888 -4.1507921 -4.1488271 -4.1797686][-4.2759151 -4.27002 -4.2552438 -4.2372737 -4.2334924 -4.239336 -4.25164 -4.2690282 -4.280314 -4.27545 -4.2491851 -4.2019019 -4.1598639 -4.1498137 -4.172081][-4.2532463 -4.2386661 -4.2119789 -4.182147 -4.1672521 -4.1652579 -4.1780529 -4.2099037 -4.2417784 -4.2595024 -4.2562509 -4.2283478 -4.1904907 -4.1679549 -4.172936][-4.2293162 -4.203464 -4.1633821 -4.1213822 -4.0910511 -4.0734715 -4.0820551 -4.1281562 -4.1860232 -4.2297244 -4.2512 -4.245554 -4.2158809 -4.181366 -4.166091][-4.2033663 -4.1657095 -4.118125 -4.0737915 -4.0342793 -3.9965851 -3.9928916 -4.0468788 -4.1290655 -4.1986408 -4.2425003 -4.2537603 -4.2302232 -4.1878161 -4.1563683][-4.1847677 -4.1335077 -4.0822177 -4.045404 -4.0068464 -3.9539323 -3.9278126 -3.9716482 -4.0679379 -4.1619482 -4.22625 -4.2514453 -4.2355151 -4.1966777 -4.1613965][-4.1875858 -4.1257291 -4.0733166 -4.0471058 -4.0184693 -3.9628594 -3.9172986 -3.9375937 -4.0259075 -4.1283021 -4.2075572 -4.24685 -4.2453642 -4.2200942 -4.191803][-4.2171588 -4.155839 -4.1091743 -4.0921688 -4.0737 -4.0272541 -3.9765313 -3.9735181 -4.034987 -4.1219187 -4.1996832 -4.2464571 -4.2594266 -4.249939 -4.2323613][-4.2584825 -4.2108941 -4.1753306 -4.1632128 -4.152751 -4.1169715 -4.0695496 -4.0560889 -4.0941095 -4.1580157 -4.2198534 -4.2610517 -4.2782431 -4.2767892 -4.2668161][-4.2855434 -4.2572408 -4.2357121 -4.23095 -4.229393 -4.20475 -4.1664023 -4.1488533 -4.1693139 -4.2107172 -4.2548475 -4.2843847 -4.2965627 -4.2941284 -4.2860537][-4.2771587 -4.2657995 -4.2588353 -4.2659197 -4.2741852 -4.2605357 -4.23516 -4.2201242 -4.2292352 -4.2536926 -4.2811408 -4.2983246 -4.302959 -4.2966228 -4.2870588][-4.2420616 -4.2397618 -4.2452879 -4.2628288 -4.2803049 -4.2749143 -4.257514 -4.2456007 -4.2490058 -4.2612495 -4.2755818 -4.2835646 -4.2845879 -4.2766738 -4.2665815][-4.2022967 -4.2041121 -4.2167287 -4.2398829 -4.2608933 -4.2579713 -4.2416406 -4.2291384 -4.2296767 -4.2394528 -4.2484913 -4.252748 -4.2555342 -4.2508311 -4.2427659][-4.1910777 -4.1951046 -4.2084579 -4.2284355 -4.243813 -4.2372169 -4.217412 -4.2024174 -4.2023873 -4.2156057 -4.2272515 -4.2324972 -4.2385788 -4.24025 -4.2363849][-4.222281 -4.22864 -4.2408824 -4.2536697 -4.2595687 -4.2472992 -4.2242875 -4.2052813 -4.2020078 -4.2159686 -4.2320504 -4.2421155 -4.2521253 -4.2592649 -4.2597957]]...]
INFO - root - 2017-12-07 16:10:08.077107: step 25710, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 78h:44m:14s remains)
INFO - root - 2017-12-07 16:10:17.750388: step 25720, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 82h:14m:45s remains)
INFO - root - 2017-12-07 16:10:27.272949: step 25730, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.971 sec/batch; 82h:46m:31s remains)
INFO - root - 2017-12-07 16:10:36.948880: step 25740, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 81h:12m:19s remains)
INFO - root - 2017-12-07 16:10:46.602353: step 25750, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 77h:06m:53s remains)
INFO - root - 2017-12-07 16:10:56.325828: step 25760, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 82h:02m:52s remains)
INFO - root - 2017-12-07 16:11:06.012266: step 25770, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 85h:54m:07s remains)
INFO - root - 2017-12-07 16:11:15.788488: step 25780, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 78h:22m:38s remains)
INFO - root - 2017-12-07 16:11:25.394194: step 25790, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 80h:44m:14s remains)
INFO - root - 2017-12-07 16:11:34.912049: step 25800, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 82h:40m:24s remains)
2017-12-07 16:11:35.844141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3479595 -4.3320355 -4.3018661 -4.2474785 -4.1824579 -4.1347957 -4.1317849 -4.1691108 -4.1986876 -4.2076545 -4.2145872 -4.2273865 -4.2256594 -4.2144642 -4.2196226][-4.3460841 -4.3316531 -4.3034949 -4.2510614 -4.1890364 -4.1441588 -4.1448526 -4.1870494 -4.2214241 -4.2331352 -4.2381811 -4.2456703 -4.2468071 -4.2358608 -4.2374072][-4.3445444 -4.3289928 -4.2997036 -4.2446308 -4.1812325 -4.1330838 -4.1333632 -4.1835279 -4.2275372 -4.2480345 -4.2571836 -4.2623224 -4.2611547 -4.2489791 -4.25003][-4.3431773 -4.3247275 -4.2915297 -4.2289176 -4.1582103 -4.1018157 -4.0988517 -4.1611156 -4.2239056 -4.2571616 -4.270196 -4.2738237 -4.2683911 -4.2546721 -4.2531714][-4.3433685 -4.3238335 -4.2887206 -4.2218366 -4.1453323 -4.0827031 -4.073976 -4.1419859 -4.2145991 -4.2531748 -4.26556 -4.2687678 -4.2626386 -4.2488737 -4.2457886][-4.3437066 -4.3235068 -4.2892232 -4.2252574 -4.1501393 -4.0822372 -4.0631781 -4.1220946 -4.1913571 -4.2279625 -4.2395549 -4.2450371 -4.24425 -4.2364421 -4.236752][-4.3422275 -4.322082 -4.289464 -4.2316217 -4.1593285 -4.0828848 -4.0470681 -4.0865507 -4.145782 -4.1830359 -4.2015986 -4.2144537 -4.2220097 -4.219646 -4.2214923][-4.33884 -4.3186727 -4.2865267 -4.2322369 -4.1605806 -4.07642 -4.02751 -4.0525737 -4.1039987 -4.1460085 -4.1742473 -4.1929975 -4.2019315 -4.1949039 -4.1911793][-4.3325496 -4.3099241 -4.2758079 -4.2232671 -4.1561022 -4.0757542 -4.0288196 -4.0519676 -4.0999517 -4.1404181 -4.1653109 -4.1786571 -4.1839695 -4.172493 -4.1610122][-4.3249435 -4.298614 -4.262444 -4.2103777 -4.1484551 -4.0791659 -4.0431175 -4.0692525 -4.1146822 -4.1496086 -4.1687212 -4.178906 -4.1853728 -4.1756678 -4.1567941][-4.319387 -4.2904162 -4.2514362 -4.1983662 -4.138279 -4.0814042 -4.0579357 -4.086853 -4.133008 -4.169837 -4.1898136 -4.1987309 -4.2033916 -4.19412 -4.1704454][-4.3163137 -4.2859492 -4.2469358 -4.1944013 -4.1392083 -4.0962777 -4.0848441 -4.1154456 -4.1601772 -4.1970305 -4.2135558 -4.2171726 -4.2161932 -4.2081785 -4.185905][-4.3171186 -4.2874188 -4.2497864 -4.1989789 -4.1497736 -4.117362 -4.1173687 -4.1520233 -4.1963587 -4.2313085 -4.2449341 -4.2465582 -4.2421432 -4.2343349 -4.2150064][-4.3231974 -4.2957911 -4.2592754 -4.2086678 -4.1602621 -4.1324654 -4.1384468 -4.1770806 -4.2194977 -4.2513018 -4.263927 -4.2639389 -4.2571974 -4.2479396 -4.2319021][-4.3283563 -4.3022208 -4.26587 -4.2148027 -4.1646023 -4.1327996 -4.1382408 -4.1786838 -4.2201228 -4.2486434 -4.2586432 -4.2553425 -4.2483134 -4.2410755 -4.228406]]...]
INFO - root - 2017-12-07 16:11:45.488453: step 25810, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.004 sec/batch; 85h:31m:54s remains)
INFO - root - 2017-12-07 16:11:55.115948: step 25820, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 77h:14m:20s remains)
INFO - root - 2017-12-07 16:12:04.813652: step 25830, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 84h:41m:19s remains)
INFO - root - 2017-12-07 16:12:14.571487: step 25840, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 84h:29m:17s remains)
INFO - root - 2017-12-07 16:12:24.186407: step 25850, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 83h:28m:59s remains)
INFO - root - 2017-12-07 16:12:33.819024: step 25860, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 83h:17m:40s remains)
INFO - root - 2017-12-07 16:12:43.431322: step 25870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 82h:23m:58s remains)
INFO - root - 2017-12-07 16:12:53.206103: step 25880, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 82h:18m:04s remains)
INFO - root - 2017-12-07 16:13:02.954246: step 25890, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 83h:31m:52s remains)
INFO - root - 2017-12-07 16:13:12.577450: step 25900, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 79h:59m:39s remains)
2017-12-07 16:13:13.560584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32136 -4.3097048 -4.2953215 -4.2823868 -4.2775259 -4.2779341 -4.2829256 -4.2903857 -4.2951083 -4.3003578 -4.3117914 -4.3164945 -4.3196568 -4.3244653 -4.33285][-4.3040428 -4.285 -4.2641788 -4.2442584 -4.2350726 -4.2334437 -4.2399821 -4.2512674 -4.2580252 -4.2645979 -4.2819142 -4.2861333 -4.2896624 -4.2975225 -4.3100204][-4.3039126 -4.2796183 -4.2529922 -4.2241549 -4.2066774 -4.2020655 -4.2079291 -4.2181282 -4.2230725 -4.2272964 -4.2474375 -4.2498817 -4.2538943 -4.2659965 -4.2825994][-4.3063531 -4.2815261 -4.2532635 -4.2201061 -4.1975031 -4.1935911 -4.1951036 -4.1987071 -4.1973977 -4.1923504 -4.20632 -4.2063646 -4.2093949 -4.224463 -4.2439837][-4.3093276 -4.2878695 -4.2615709 -4.2300944 -4.2059884 -4.1963296 -4.1845241 -4.1771588 -4.1657591 -4.1479063 -4.1519861 -4.1480055 -4.1469836 -4.1640739 -4.190218][-4.3026915 -4.2856007 -4.2624021 -4.2338452 -4.20526 -4.1830368 -4.1548252 -4.1375909 -4.1126194 -4.0821567 -4.0839977 -4.0812025 -4.081645 -4.1032162 -4.1372728][-4.2785416 -4.2599716 -4.2339344 -4.2003617 -4.1606255 -4.1207347 -4.079021 -4.0548239 -4.0215082 -3.9854605 -3.9883158 -3.9916525 -4.0034556 -4.0387988 -4.0873232][-4.2377906 -4.2067757 -4.1700258 -4.1222649 -4.0662174 -4.0064516 -3.9569592 -3.9335055 -3.9037793 -3.873384 -3.884007 -3.9044273 -3.9384334 -3.996253 -4.060854][-4.1854382 -4.1362405 -4.0854912 -4.0257678 -3.96237 -3.8991218 -3.8621507 -3.8543487 -3.8426385 -3.8327916 -3.8569732 -3.8916569 -3.9401124 -4.0121579 -4.0830088][-4.1739407 -4.1217451 -4.0744481 -4.0243773 -3.977905 -3.935111 -3.9213178 -3.9268332 -3.9247057 -3.9282095 -3.9538302 -3.9902091 -4.0381942 -4.1025171 -4.1632156][-4.2289376 -4.1969552 -4.1705003 -4.141398 -4.1166935 -4.0942764 -4.0907292 -4.0952005 -4.0922546 -4.0979891 -4.1163392 -4.1456146 -4.182827 -4.2270174 -4.2672415][-4.2943954 -4.2842827 -4.2780738 -4.2676778 -4.2597218 -4.2515607 -4.2494555 -4.2506361 -4.24721 -4.2510109 -4.2621665 -4.279747 -4.3015547 -4.3242621 -4.3416977][-4.3374853 -4.3356242 -4.3362956 -4.3335671 -4.3320484 -4.3296452 -4.3278236 -4.3274832 -4.3249836 -4.3273673 -4.3340969 -4.3424072 -4.3517547 -4.3612037 -4.367455][-4.3600912 -4.3579416 -4.3586645 -4.3584924 -4.3592281 -4.3592033 -4.358079 -4.3585105 -4.3573666 -4.3581471 -4.3610473 -4.3635373 -4.3663931 -4.3699274 -4.37297][-4.3732295 -4.3721323 -4.3719916 -4.3721719 -4.37237 -4.3725019 -4.3722677 -4.3726115 -4.3721132 -4.3722029 -4.3732419 -4.3738432 -4.3746858 -4.3759274 -4.3771935]]...]
INFO - root - 2017-12-07 16:13:23.321001: step 25910, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 82h:51m:45s remains)
INFO - root - 2017-12-07 16:13:32.941871: step 25920, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 80h:40m:52s remains)
INFO - root - 2017-12-07 16:13:42.623332: step 25930, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 85h:04m:35s remains)
INFO - root - 2017-12-07 16:13:52.254368: step 25940, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 80h:23m:26s remains)
INFO - root - 2017-12-07 16:14:02.104695: step 25950, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 84h:22m:18s remains)
INFO - root - 2017-12-07 16:14:11.856636: step 25960, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 80h:03m:12s remains)
INFO - root - 2017-12-07 16:14:21.553125: step 25970, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 84h:26m:00s remains)
INFO - root - 2017-12-07 16:14:31.162403: step 25980, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 85h:13m:49s remains)
INFO - root - 2017-12-07 16:14:40.832932: step 25990, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 84h:43m:45s remains)
INFO - root - 2017-12-07 16:14:50.290944: step 26000, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 85h:02m:36s remains)
2017-12-07 16:14:51.218288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.132319 -4.1381989 -4.1519184 -4.1589332 -4.1536341 -4.1508665 -4.1581073 -4.1703563 -4.1800938 -4.1860533 -4.1857347 -4.1781788 -4.1577568 -4.1296525 -4.1092982][-4.1834788 -4.1839504 -4.1912813 -4.1928692 -4.1838083 -4.1760616 -4.1753 -4.1808472 -4.186861 -4.1909056 -4.1906319 -4.1844206 -4.1652737 -4.1397109 -4.1239905][-4.2444324 -4.2406087 -4.2426424 -4.2412467 -4.2352815 -4.2265821 -4.2189722 -4.2180667 -4.221693 -4.2260828 -4.2281179 -4.2254868 -4.2103262 -4.1889749 -4.1764793][-4.28915 -4.2812476 -4.2761083 -4.2718453 -4.2675953 -4.2594924 -4.2495885 -4.2436423 -4.2458081 -4.2546778 -4.2647691 -4.2707443 -4.2633514 -4.2469864 -4.2375064][-4.2967625 -4.2800617 -4.2669983 -4.2592835 -4.2556462 -4.2495022 -4.2392721 -4.2296815 -4.2313046 -4.2468104 -4.2689018 -4.2856016 -4.2877712 -4.2778554 -4.2722263][-4.2840276 -4.2566333 -4.2303367 -4.2100997 -4.1962962 -4.1864076 -4.1747618 -4.1633615 -4.1682806 -4.1962161 -4.2343345 -4.261692 -4.2724628 -4.2698135 -4.2692127][-4.269032 -4.2324438 -4.1913357 -4.1547937 -4.1215596 -4.091579 -4.067914 -4.0541224 -4.0653009 -4.1076078 -4.1639357 -4.2086954 -4.2328634 -4.2407632 -4.2479544][-4.2642646 -4.2240095 -4.1720786 -4.1241665 -4.0768628 -4.0235128 -3.9758825 -3.9517827 -3.9650445 -4.0168333 -4.0880637 -4.147429 -4.1851978 -4.2084446 -4.2269068][-4.2785149 -4.2408442 -4.1887445 -4.1427584 -4.0985703 -4.04165 -3.9859657 -3.95296 -3.9543927 -3.9945748 -4.0593948 -4.1178279 -4.1546526 -4.1830153 -4.209321][-4.3033934 -4.2772346 -4.2369089 -4.2035813 -4.1762724 -4.1367059 -4.09685 -4.0683427 -4.0570469 -4.0702348 -4.1043472 -4.1374793 -4.1539412 -4.1682239 -4.1936569][-4.3260107 -4.3116446 -4.28723 -4.2670336 -4.2554426 -4.2371974 -4.2191091 -4.202282 -4.1860981 -4.1766186 -4.1811547 -4.1857009 -4.1799655 -4.1756248 -4.1950583][-4.3401947 -4.3314528 -4.3201466 -4.3127813 -4.3095822 -4.3017483 -4.2968698 -4.2899518 -4.2763391 -4.2592669 -4.2479095 -4.2340717 -4.2148786 -4.2018862 -4.2175674][-4.3477 -4.3410811 -4.3394747 -4.3433642 -4.3442469 -4.3385692 -4.3359566 -4.3335223 -4.3253937 -4.3104248 -4.2972422 -4.2750196 -4.2480016 -4.2312756 -4.247736][-4.342752 -4.3375 -4.3410873 -4.3504286 -4.3562722 -4.3534241 -4.351604 -4.3495579 -4.3427558 -4.3312311 -4.32074 -4.3015261 -4.2760706 -4.2603803 -4.2759156][-4.3180819 -4.3134751 -4.3176708 -4.32867 -4.3375936 -4.3395505 -4.3414855 -4.3402572 -4.3324142 -4.323699 -4.3175588 -4.3068428 -4.2904997 -4.2803273 -4.2926016]]...]
INFO - root - 2017-12-07 16:15:00.848664: step 26010, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.971 sec/batch; 82h:37m:44s remains)
INFO - root - 2017-12-07 16:15:10.531219: step 26020, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 85h:06m:59s remains)
INFO - root - 2017-12-07 16:15:20.259952: step 26030, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.970 sec/batch; 82h:34m:07s remains)
INFO - root - 2017-12-07 16:15:29.966909: step 26040, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 82h:33m:29s remains)
INFO - root - 2017-12-07 16:15:39.586580: step 26050, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.008 sec/batch; 85h:46m:06s remains)
INFO - root - 2017-12-07 16:15:49.346930: step 26060, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 81h:18m:14s remains)
INFO - root - 2017-12-07 16:15:59.132666: step 26070, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 82h:37m:20s remains)
INFO - root - 2017-12-07 16:16:08.769623: step 26080, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 78h:17m:15s remains)
INFO - root - 2017-12-07 16:16:18.379634: step 26090, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 83h:38m:18s remains)
INFO - root - 2017-12-07 16:16:27.936467: step 26100, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 81h:54m:08s remains)
2017-12-07 16:16:28.837352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584939 -4.2723718 -4.2958727 -4.3110642 -4.3264489 -4.3326168 -4.313303 -4.2753539 -4.2501187 -4.26309 -4.2907143 -4.3071675 -4.3110576 -4.3099666 -4.3154359][-4.2743778 -4.2828746 -4.3041625 -4.3163614 -4.3281846 -4.3332653 -4.3204942 -4.2979741 -4.2867436 -4.2989674 -4.3171849 -4.3279653 -4.3324838 -4.33074 -4.3316178][-4.2959356 -4.300807 -4.315701 -4.322485 -4.3275051 -4.3224669 -4.3025813 -4.2821507 -4.2819967 -4.3010902 -4.3163195 -4.3310995 -4.3395462 -4.3412371 -4.3398633][-4.3125124 -4.3183932 -4.3266573 -4.3220568 -4.3084955 -4.2856331 -4.2535028 -4.2355466 -4.2550659 -4.2861195 -4.3022718 -4.3182187 -4.3308411 -4.3361468 -4.3365474][-4.3247256 -4.3302674 -4.3317566 -4.3094339 -4.2659392 -4.2106366 -4.1511168 -4.1273727 -4.1741958 -4.2322893 -4.2620525 -4.2863 -4.3052015 -4.3152108 -4.3178606][-4.3288083 -4.3328414 -4.3301239 -4.2925539 -4.2212849 -4.1265697 -4.0182366 -3.9731603 -4.0516691 -4.1434345 -4.1974907 -4.2372193 -4.2641168 -4.2795262 -4.2831831][-4.32227 -4.3313274 -4.330615 -4.286643 -4.2042713 -4.0915732 -3.9510407 -3.890918 -3.9876258 -4.0997663 -4.1679749 -4.2132049 -4.238904 -4.2523274 -4.2517304][-4.3112612 -4.3292818 -4.337008 -4.3004584 -4.2292094 -4.1304345 -4.0080395 -3.9577384 -4.034616 -4.1294155 -4.1906753 -4.2313356 -4.2504835 -4.2558541 -4.2431226][-4.3019352 -4.3270149 -4.33885 -4.3151755 -4.2643085 -4.1943145 -4.1123424 -4.0803556 -4.1279063 -4.1895819 -4.2307444 -4.259954 -4.2727084 -4.2725968 -4.2494469][-4.3079576 -4.3305717 -4.3401523 -4.3239994 -4.291924 -4.2509937 -4.2047167 -4.182725 -4.2043605 -4.2395992 -4.2664433 -4.2871585 -4.29641 -4.2923131 -4.2635183][-4.3189883 -4.334774 -4.3411436 -4.3289685 -4.307085 -4.2804618 -4.2520885 -4.2295823 -4.2358818 -4.261168 -4.2873583 -4.3048663 -4.3083534 -4.3044 -4.2824283][-4.316185 -4.3295431 -4.3361926 -4.325932 -4.3076177 -4.2861514 -4.2631702 -4.2394881 -4.2362027 -4.2567058 -4.28695 -4.3068986 -4.3109732 -4.3134484 -4.3053503][-4.3104248 -4.3215094 -4.329113 -4.321672 -4.3044558 -4.2849979 -4.2612934 -4.236783 -4.2264285 -4.2428536 -4.2751379 -4.2981081 -4.3078032 -4.3184071 -4.321146][-4.3018422 -4.3088641 -4.3168912 -4.3148451 -4.2984586 -4.2768359 -4.2478213 -4.222681 -4.207942 -4.2212315 -4.2518773 -4.2784553 -4.2971668 -4.3157606 -4.3248444][-4.286767 -4.29055 -4.2995515 -4.3021579 -4.2905626 -4.2708321 -4.2438903 -4.2192445 -4.2021637 -4.2122478 -4.2401414 -4.2669568 -4.289979 -4.3114696 -4.3205442]]...]
INFO - root - 2017-12-07 16:16:38.365576: step 26110, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 79h:30m:05s remains)
INFO - root - 2017-12-07 16:16:47.894608: step 26120, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 82h:39m:50s remains)
INFO - root - 2017-12-07 16:16:57.659028: step 26130, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 82h:48m:10s remains)
INFO - root - 2017-12-07 16:17:07.020672: step 26140, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 69h:57m:49s remains)
INFO - root - 2017-12-07 16:17:16.668389: step 26150, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 81h:51m:16s remains)
INFO - root - 2017-12-07 16:17:26.371402: step 26160, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 81h:04m:07s remains)
INFO - root - 2017-12-07 16:17:36.125651: step 26170, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 82h:20m:23s remains)
INFO - root - 2017-12-07 16:17:45.708685: step 26180, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 80h:10m:11s remains)
INFO - root - 2017-12-07 16:17:55.274698: step 26190, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 80h:24m:33s remains)
INFO - root - 2017-12-07 16:18:04.902574: step 26200, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.995 sec/batch; 84h:39m:23s remains)
2017-12-07 16:18:05.920718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.308054 -4.3210187 -4.3314586 -4.33165 -4.3255224 -4.318748 -4.3162537 -4.3187828 -4.3228607 -4.3253245 -4.3253188 -4.3211684 -4.317296 -4.3137693 -4.3102064][-4.3094153 -4.3237982 -4.334178 -4.3295879 -4.3183193 -4.3083076 -4.3031769 -4.307992 -4.31536 -4.3204718 -4.3247175 -4.3241878 -4.3227487 -4.3160357 -4.306437][-4.2954216 -4.3077617 -4.3149414 -4.3041134 -4.2860775 -4.2683058 -4.255466 -4.2585163 -4.2724919 -4.2857504 -4.2986827 -4.3045812 -4.3061152 -4.2974081 -4.2823148][-4.2548842 -4.2614636 -4.2678623 -4.256732 -4.2324171 -4.2025161 -4.1764307 -4.1755962 -4.2006383 -4.2284594 -4.2552934 -4.2709503 -4.2760305 -4.2646842 -4.2422032][-4.2073479 -4.20374 -4.2073941 -4.1941714 -4.1606259 -4.1076965 -4.0567241 -4.0581384 -4.1083083 -4.1626415 -4.2088208 -4.2353649 -4.2428746 -4.2266841 -4.1920128][-4.1836729 -4.1653128 -4.1565132 -4.1314292 -4.0795741 -3.9930801 -3.9056768 -3.9130826 -4.0022259 -4.0869842 -4.1472187 -4.1779242 -4.1885848 -4.17266 -4.1269722][-4.1982841 -4.1692114 -4.1482272 -4.1081004 -4.040472 -3.9360998 -3.8255548 -3.8398404 -3.9559817 -4.0532618 -4.1108751 -4.1341348 -4.1450362 -4.1280951 -4.07208][-4.2204871 -4.1921754 -4.1703391 -4.1369128 -4.0855694 -4.0133147 -3.9416504 -3.95652 -4.0363617 -4.1034036 -4.139185 -4.1463861 -4.1435003 -4.1150532 -4.0521235][-4.232585 -4.2114115 -4.1956038 -4.1828308 -4.1637955 -4.1283436 -4.093183 -4.1052461 -4.1430469 -4.1793828 -4.1973157 -4.1901054 -4.17304 -4.1389756 -4.0858564][-4.228188 -4.207727 -4.1969533 -4.2072597 -4.214922 -4.2064829 -4.1918859 -4.1970949 -4.2131143 -4.237412 -4.24999 -4.2395043 -4.2202268 -4.195231 -4.1607127][-4.20691 -4.1866875 -4.1799345 -4.2022219 -4.2267947 -4.2378974 -4.2346053 -4.2347546 -4.2467995 -4.2731934 -4.2850704 -4.2770534 -4.2622781 -4.2512155 -4.2340555][-4.1853123 -4.1661234 -4.1586456 -4.1810765 -4.2138472 -4.238925 -4.245945 -4.24835 -4.2644191 -4.2933464 -4.3050966 -4.2962728 -4.2833753 -4.2796826 -4.2737627][-4.1796889 -4.1609244 -4.1506419 -4.1672516 -4.1975112 -4.2270088 -4.2412148 -4.2505393 -4.2754073 -4.3078947 -4.3188329 -4.3081703 -4.2975864 -4.2944951 -4.294436][-4.1912069 -4.1751533 -4.1658468 -4.175663 -4.19693 -4.222692 -4.24 -4.2573104 -4.2878466 -4.3168154 -4.32521 -4.3164935 -4.3082013 -4.3054132 -4.30779][-4.2014236 -4.1891227 -4.1846805 -4.1930828 -4.2105541 -4.2346821 -4.2562079 -4.2784815 -4.3065429 -4.3290439 -4.33547 -4.3322315 -4.32796 -4.3265524 -4.327723]]...]
INFO - root - 2017-12-07 16:18:15.451933: step 26210, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.997 sec/batch; 84h:49m:38s remains)
INFO - root - 2017-12-07 16:18:24.956910: step 26220, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 75h:57m:02s remains)
INFO - root - 2017-12-07 16:18:34.629813: step 26230, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 83h:28m:49s remains)
INFO - root - 2017-12-07 16:18:44.335048: step 26240, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.033 sec/batch; 87h:54m:04s remains)
INFO - root - 2017-12-07 16:18:53.793835: step 26250, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 80h:11m:37s remains)
INFO - root - 2017-12-07 16:19:03.360318: step 26260, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 80h:39m:27s remains)
INFO - root - 2017-12-07 16:19:13.005495: step 26270, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.946 sec/batch; 80h:28m:13s remains)
INFO - root - 2017-12-07 16:19:22.721508: step 26280, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.037 sec/batch; 88h:12m:30s remains)
INFO - root - 2017-12-07 16:19:32.261407: step 26290, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 79h:25m:35s remains)
INFO - root - 2017-12-07 16:19:41.878981: step 26300, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 84h:57m:16s remains)
2017-12-07 16:19:42.789088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2015133 -4.2344885 -4.2577591 -4.26954 -4.2766628 -4.2807674 -4.2659636 -4.2403455 -4.2202678 -4.2295361 -4.2552452 -4.2798886 -4.2877727 -4.2890081 -4.2869062][-4.1821733 -4.22447 -4.2530394 -4.2655797 -4.2743134 -4.279418 -4.2594137 -4.2259512 -4.2019787 -4.2126713 -4.2458911 -4.2766242 -4.2878428 -4.2907147 -4.2908311][-4.1822543 -4.2286377 -4.2616758 -4.2785664 -4.2858391 -4.2837553 -4.2564511 -4.2168031 -4.1899238 -4.2009697 -4.2408323 -4.2782717 -4.2938461 -4.3002558 -4.305697][-4.2191868 -4.2640872 -4.2996182 -4.3160424 -4.3134942 -4.2930503 -4.251925 -4.1981044 -4.1650743 -4.1751642 -4.2183318 -4.2649374 -4.2950449 -4.3101063 -4.32062][-4.270895 -4.3047333 -4.3343239 -4.3416052 -4.32333 -4.2838993 -4.2228403 -4.1493835 -4.1059709 -4.1142278 -4.163229 -4.21987 -4.2642603 -4.2918372 -4.3140674][-4.30454 -4.3265038 -4.3435669 -4.3426261 -4.31672 -4.2655263 -4.1829176 -4.0818167 -4.0182934 -4.0266476 -4.0872469 -4.1558867 -4.2132182 -4.2537336 -4.2884789][-4.30842 -4.3252826 -4.3375764 -4.3385572 -4.3126931 -4.2528009 -4.1505246 -4.0226526 -3.9389923 -3.9466126 -4.018703 -4.0989208 -4.1658044 -4.2159271 -4.2557306][-4.2990746 -4.3173895 -4.3330083 -4.3394504 -4.3159628 -4.2555218 -4.1552496 -4.0250764 -3.9329607 -3.9384136 -4.0124569 -4.0916915 -4.156641 -4.2061996 -4.2400165][-4.2849431 -4.3059607 -4.3275623 -4.33467 -4.3089757 -4.2515655 -4.1672239 -4.0583467 -3.9804573 -3.9921587 -4.0691829 -4.1387396 -4.1878204 -4.2256532 -4.24493][-4.2735782 -4.2920475 -4.3128805 -4.3191195 -4.293128 -4.2411442 -4.1732697 -4.0939388 -4.0438933 -4.0656061 -4.1372504 -4.1890507 -4.2175136 -4.2365575 -4.2417564][-4.2648282 -4.2824769 -4.3026662 -4.31204 -4.296772 -4.2556181 -4.20606 -4.1551914 -4.1269588 -4.143034 -4.1901755 -4.2166495 -4.222281 -4.2254477 -4.2234688][-4.2596993 -4.2782478 -4.2995648 -4.3122487 -4.3080192 -4.2805257 -4.2485528 -4.2198496 -4.205204 -4.2099562 -4.2214928 -4.2244072 -4.2162719 -4.2164373 -4.2220912][-4.2566366 -4.2750716 -4.2988644 -4.3152733 -4.3174591 -4.2993975 -4.2769585 -4.2629442 -4.2539177 -4.2457256 -4.2359934 -4.2263036 -4.2185364 -4.2235694 -4.2418189][-4.2544723 -4.2680154 -4.2904916 -4.3132143 -4.3198628 -4.3048906 -4.280355 -4.2656794 -4.25918 -4.2474012 -4.2313848 -4.2209392 -4.2216959 -4.2332835 -4.2566915][-4.2575655 -4.2609262 -4.2784762 -4.3055711 -4.3141308 -4.2961645 -4.2627063 -4.2386274 -4.2316537 -4.2282195 -4.223454 -4.2213445 -4.2317429 -4.2490044 -4.272058]]...]
INFO - root - 2017-12-07 16:19:52.216264: step 26310, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 82h:45m:46s remains)
INFO - root - 2017-12-07 16:20:01.924401: step 26320, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.000 sec/batch; 85h:03m:05s remains)
INFO - root - 2017-12-07 16:20:11.528247: step 26330, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 79h:15m:35s remains)
INFO - root - 2017-12-07 16:20:21.234443: step 26340, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 79h:57m:45s remains)
INFO - root - 2017-12-07 16:20:30.983064: step 26350, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.999 sec/batch; 84h:56m:07s remains)
INFO - root - 2017-12-07 16:20:40.740500: step 26360, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 84h:22m:27s remains)
INFO - root - 2017-12-07 16:20:50.378426: step 26370, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 79h:56m:34s remains)
INFO - root - 2017-12-07 16:21:00.117708: step 26380, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 81h:24m:19s remains)
INFO - root - 2017-12-07 16:21:09.790576: step 26390, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 84h:06m:25s remains)
INFO - root - 2017-12-07 16:21:19.231395: step 26400, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 81h:30m:20s remains)
2017-12-07 16:21:20.198945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3271947 -4.3231268 -4.3121681 -4.3006482 -4.2953477 -4.2998075 -4.3143144 -4.3315268 -4.3444338 -4.3532372 -4.3564262 -4.351944 -4.3433995 -4.3339462 -4.3259568][-4.326582 -4.3132515 -4.2890768 -4.2640214 -4.2507343 -4.2551222 -4.2755923 -4.3007984 -4.3234596 -4.343152 -4.3568254 -4.3614793 -4.3592882 -4.3518286 -4.3430014][-4.3228173 -4.299891 -4.2611413 -4.2188683 -4.19134 -4.1891904 -4.2113905 -4.2431645 -4.2765374 -4.3107271 -4.3392382 -4.3576851 -4.3660989 -4.36374 -4.3551564][-4.3191495 -4.2917337 -4.2431893 -4.185914 -4.1399107 -4.1226449 -4.1359243 -4.1658916 -4.2079926 -4.2577829 -4.3027444 -4.3381014 -4.3608255 -4.3675246 -4.3620167][-4.3153071 -4.2903914 -4.2420554 -4.1779404 -4.1147289 -4.073122 -4.0612783 -4.0757513 -4.1237545 -4.190187 -4.250711 -4.3011155 -4.3376389 -4.3563137 -4.3585286][-4.31104 -4.292449 -4.2521448 -4.1912947 -4.118845 -4.0511074 -4.0016603 -3.9859219 -4.0323977 -4.1125569 -4.1859794 -4.248013 -4.2971539 -4.3298688 -4.3436604][-4.3091617 -4.2976408 -4.2673626 -4.2165861 -4.1456461 -4.06328 -3.9816909 -3.9302213 -3.9608359 -4.0403223 -4.11799 -4.1853042 -4.2441993 -4.291729 -4.319777][-4.3105216 -4.3047333 -4.2835178 -4.2459354 -4.1869287 -4.1077008 -4.0167613 -3.9461808 -3.9464827 -3.9996819 -4.0640697 -4.1268034 -4.1898241 -4.2490473 -4.2906814][-4.3133698 -4.3106976 -4.2976513 -4.2740064 -4.2327275 -4.1703 -4.0911145 -4.0200353 -3.9923236 -4.0062661 -4.043282 -4.0913286 -4.1503234 -4.2138643 -4.2640572][-4.31517 -4.3136172 -4.3065867 -4.2948885 -4.2710633 -4.230783 -4.1746554 -4.1150756 -4.0708723 -4.0513263 -4.0586104 -4.0863614 -4.1331229 -4.191905 -4.2439408][-4.3145294 -4.3142018 -4.3110337 -4.3067741 -4.2953329 -4.2743664 -4.2430854 -4.2033005 -4.1601157 -4.12537 -4.1108232 -4.1173005 -4.1437244 -4.1878691 -4.2342358][-4.3126421 -4.3153782 -4.3154192 -4.3140116 -4.3084493 -4.2999411 -4.28763 -4.2681079 -4.2391882 -4.2091331 -4.1877918 -4.178762 -4.1846371 -4.2085013 -4.2407713][-4.3102331 -4.3185697 -4.3222022 -4.3217039 -4.3184195 -4.3152995 -4.3123608 -4.3063068 -4.2932687 -4.2763734 -4.260149 -4.2470655 -4.2411304 -4.2480021 -4.2636771][-4.3027468 -4.31685 -4.3238978 -4.3239107 -4.3211083 -4.3193874 -4.3193507 -4.3200722 -4.3181648 -4.3129878 -4.3055177 -4.2967143 -4.2895308 -4.2880831 -4.2919483][-4.2929959 -4.3098011 -4.3179307 -4.3175154 -4.3144517 -4.3123441 -4.3117504 -4.3144078 -4.3181262 -4.3197536 -4.3194227 -4.3177695 -4.3151269 -4.3130078 -4.3124466]]...]
INFO - root - 2017-12-07 16:21:29.688836: step 26410, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 80h:11m:16s remains)
INFO - root - 2017-12-07 16:21:39.281510: step 26420, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 84h:13m:36s remains)
INFO - root - 2017-12-07 16:21:49.068078: step 26430, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.004 sec/batch; 85h:22m:41s remains)
INFO - root - 2017-12-07 16:21:58.733449: step 26440, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 83h:54m:01s remains)
INFO - root - 2017-12-07 16:22:08.418056: step 26450, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 80h:47m:50s remains)
INFO - root - 2017-12-07 16:22:18.032523: step 26460, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 83h:35m:30s remains)
INFO - root - 2017-12-07 16:22:27.428138: step 26470, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 83h:51m:31s remains)
INFO - root - 2017-12-07 16:22:36.969196: step 26480, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.964 sec/batch; 81h:55m:06s remains)
INFO - root - 2017-12-07 16:22:46.639529: step 26490, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 80h:10m:15s remains)
INFO - root - 2017-12-07 16:22:55.995833: step 26500, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 81h:42m:54s remains)
2017-12-07 16:22:56.924959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32001 -4.3255134 -4.3173132 -4.2933788 -4.2644887 -4.2370086 -4.2197304 -4.2083755 -4.2004776 -4.2072287 -4.24002 -4.2809358 -4.3130875 -4.3247075 -4.3145638][-4.3437085 -4.3437686 -4.322227 -4.2837138 -4.2448249 -4.2220492 -4.2222261 -4.2259951 -4.2236972 -4.2292385 -4.2523808 -4.2809267 -4.3035073 -4.313139 -4.299036][-4.3491254 -4.3429255 -4.3091879 -4.2569127 -4.2130923 -4.2007756 -4.2176256 -4.2316847 -4.2282491 -4.2246614 -4.235 -4.2565536 -4.2776065 -4.2845421 -4.2630377][-4.3375573 -4.3258524 -4.2825036 -4.2246733 -4.1828856 -4.1803761 -4.2027578 -4.209794 -4.1917486 -4.1788721 -4.1897664 -4.2194738 -4.2477012 -4.2521343 -4.2243519][-4.3094254 -4.2915659 -4.247509 -4.1969562 -4.1672449 -4.16972 -4.1787653 -4.1526871 -4.1019235 -4.0787268 -4.1080313 -4.1581326 -4.1926088 -4.1939816 -4.1657467][-4.2778029 -4.2590957 -4.2239523 -4.1883545 -4.1668329 -4.1549063 -4.1244946 -4.0415716 -3.9496536 -3.9453366 -4.0174336 -4.0858669 -4.1211791 -4.1211915 -4.10452][-4.2518172 -4.2403412 -4.2143717 -4.1883168 -4.1605139 -4.1215506 -4.042109 -3.9136708 -3.8194942 -3.8890233 -4.0112987 -4.089262 -4.1258912 -4.1349082 -4.138679][-4.23416 -4.2286625 -4.213748 -4.1970863 -4.1693535 -4.1263413 -4.0497103 -3.9548697 -3.9298086 -4.0244288 -4.1293011 -4.1868558 -4.213058 -4.2190857 -4.2246137][-4.2418809 -4.2382827 -4.2329774 -4.227231 -4.2099309 -4.1830535 -4.1442327 -4.1117625 -4.1238632 -4.1842003 -4.2415533 -4.2704263 -4.2821937 -4.2843041 -4.2869058][-4.2604542 -4.2536144 -4.2534766 -4.2558584 -4.2495418 -4.2373071 -4.21794 -4.2060103 -4.2182236 -4.2505116 -4.278008 -4.29108 -4.2997084 -4.3054113 -4.3092232][-4.2600222 -4.2534943 -4.2551889 -4.2581329 -4.2526503 -4.2406788 -4.2213454 -4.2077379 -4.216917 -4.2391939 -4.2576275 -4.2674012 -4.2803879 -4.2956386 -4.3067713][-4.2379217 -4.2338085 -4.23411 -4.2334051 -4.2265706 -4.2080379 -4.1817226 -4.1697917 -4.1834645 -4.208324 -4.2304707 -4.2449059 -4.2629361 -4.2849226 -4.3004041][-4.2323155 -4.2223859 -4.2169685 -4.2100477 -4.1996045 -4.1762156 -4.1501656 -4.1462059 -4.1652951 -4.1923079 -4.2166228 -4.23509 -4.2555785 -4.2762089 -4.2891908][-4.246644 -4.230238 -4.2161956 -4.2015982 -4.1861305 -4.1623459 -4.1419787 -4.1439404 -4.1644416 -4.1892385 -4.2097912 -4.2268395 -4.244143 -4.2593713 -4.267921][-4.2615471 -4.2387834 -4.2178726 -4.2051463 -4.1974134 -4.1812391 -4.167963 -4.1693635 -4.1821294 -4.1957407 -4.2045035 -4.2097354 -4.2173209 -4.2267051 -4.2311807]]...]
INFO - root - 2017-12-07 16:23:06.522286: step 26510, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 84h:01m:57s remains)
INFO - root - 2017-12-07 16:23:16.247416: step 26520, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.006 sec/batch; 85h:32m:21s remains)
INFO - root - 2017-12-07 16:23:25.977712: step 26530, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 80h:20m:45s remains)
INFO - root - 2017-12-07 16:23:35.605535: step 26540, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 81h:12m:08s remains)
INFO - root - 2017-12-07 16:23:45.228816: step 26550, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 84h:36m:33s remains)
INFO - root - 2017-12-07 16:23:54.954289: step 26560, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 82h:21m:58s remains)
INFO - root - 2017-12-07 16:24:04.535586: step 26570, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 80h:41m:29s remains)
INFO - root - 2017-12-07 16:24:14.169803: step 26580, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 77h:04m:52s remains)
INFO - root - 2017-12-07 16:24:23.864624: step 26590, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 82h:44m:41s remains)
INFO - root - 2017-12-07 16:24:33.498034: step 26600, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 79h:53m:35s remains)
2017-12-07 16:24:34.424943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0857921 -4.1472955 -4.2076941 -4.248529 -4.2665849 -4.2691412 -4.2687445 -4.2675529 -4.2672486 -4.249701 -4.2236609 -4.205193 -4.1881986 -4.1536889 -4.1078115][-4.0970268 -4.1523066 -4.2069716 -4.2436457 -4.254662 -4.2494092 -4.2441297 -4.2463913 -4.2575483 -4.2543688 -4.2424884 -4.2399526 -4.235301 -4.2117987 -4.1776953][-4.1463227 -4.1773114 -4.215312 -4.2403078 -4.2406311 -4.2244887 -4.2061319 -4.20479 -4.225275 -4.2414932 -4.2533846 -4.2692575 -4.2746487 -4.2595286 -4.2357883][-4.1865416 -4.1960869 -4.216783 -4.2291012 -4.2178736 -4.1848068 -4.1423607 -4.1306229 -4.1640334 -4.2042184 -4.2436471 -4.2812042 -4.2986975 -4.2914929 -4.2768641][-4.21707 -4.2171474 -4.2224889 -4.2197781 -4.1961365 -4.1415167 -4.0686707 -4.0380411 -4.0856023 -4.1525836 -4.2135487 -4.2703481 -4.3024836 -4.3064475 -4.2998157][-4.2512507 -4.2462072 -4.2334275 -4.2113872 -4.1738744 -4.0959949 -3.9863083 -3.9232538 -3.988261 -4.0873914 -4.1675138 -4.2386827 -4.2855124 -4.3007374 -4.3016357][-4.2854295 -4.2743149 -4.2432156 -4.2008448 -4.1452923 -4.0507545 -3.9168558 -3.8266215 -3.90891 -4.0326495 -4.1273007 -4.2104793 -4.2678967 -4.2922044 -4.2995443][-4.3147135 -4.2938876 -4.2467384 -4.1891246 -4.128613 -4.0505753 -3.9433348 -3.8726482 -3.9439149 -4.0479307 -4.1271586 -4.206181 -4.2649155 -4.2887049 -4.295423][-4.33204 -4.3059025 -4.2549257 -4.1982226 -4.1514788 -4.1065779 -4.0443115 -4.005209 -4.0486503 -4.108911 -4.1574273 -4.2186046 -4.2716403 -4.2909722 -4.2925115][-4.3479414 -4.3245883 -4.2820296 -4.2361712 -4.2043471 -4.1808548 -4.1471596 -4.1240349 -4.1433339 -4.1725616 -4.2001891 -4.2448325 -4.2866993 -4.3005557 -4.2955914][-4.3547359 -4.3382144 -4.3076839 -4.2761 -4.25891 -4.2489343 -4.2277923 -4.209177 -4.2143316 -4.2317824 -4.2518463 -4.2831531 -4.3113527 -4.318716 -4.3074503][-4.35221 -4.3426342 -4.3258853 -4.3101149 -4.3037643 -4.2972484 -4.2805767 -4.2642064 -4.2668977 -4.2824254 -4.2998 -4.320683 -4.3372478 -4.3393135 -4.3240662][-4.3496275 -4.3468485 -4.3400826 -4.3343253 -4.3316402 -4.3247714 -4.3117957 -4.30125 -4.3052368 -4.3193831 -4.33191 -4.3443317 -4.352376 -4.3515148 -4.3369303][-4.3448238 -4.3445721 -4.3422461 -4.3406367 -4.3391471 -4.33378 -4.3258533 -4.3212132 -4.3267512 -4.3374486 -4.34654 -4.3533058 -4.3574662 -4.355463 -4.3433051][-4.3416781 -4.3417053 -4.341104 -4.3408294 -4.3410358 -4.3388376 -4.3348818 -4.3326511 -4.3363557 -4.3434243 -4.3501611 -4.35456 -4.3565383 -4.3540096 -4.3449283]]...]
INFO - root - 2017-12-07 16:24:44.116063: step 26610, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 82h:15m:15s remains)
INFO - root - 2017-12-07 16:24:53.702512: step 26620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 78h:19m:02s remains)
INFO - root - 2017-12-07 16:25:03.341351: step 26630, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 83h:14m:42s remains)
INFO - root - 2017-12-07 16:25:12.917932: step 26640, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 80h:04m:50s remains)
INFO - root - 2017-12-07 16:25:22.445935: step 26650, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 79h:40m:08s remains)
INFO - root - 2017-12-07 16:25:32.211656: step 26660, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 84h:55m:47s remains)
INFO - root - 2017-12-07 16:25:41.900299: step 26670, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 81h:02m:18s remains)
INFO - root - 2017-12-07 16:25:51.456903: step 26680, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 78h:25m:12s remains)
INFO - root - 2017-12-07 16:26:01.095116: step 26690, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 81h:58m:52s remains)
INFO - root - 2017-12-07 16:26:10.445100: step 26700, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 81h:04m:32s remains)
2017-12-07 16:26:11.451252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3059325 -4.3055429 -4.3010125 -4.2937145 -4.2883458 -4.2915888 -4.2928443 -4.292563 -4.2952123 -4.2950559 -4.2907329 -4.2894835 -4.2887673 -4.2802773 -4.2698116][-4.2911983 -4.2905583 -4.2858214 -4.2757907 -4.2659774 -4.268353 -4.2678 -4.2615423 -4.2581086 -4.2560372 -4.2535825 -4.2558923 -4.2557154 -4.2456975 -4.2372518][-4.27296 -4.2742362 -4.2710042 -4.2608166 -4.2477264 -4.246006 -4.2384109 -4.2197156 -4.2048368 -4.2009149 -4.2040806 -4.2113862 -4.21334 -4.2076807 -4.2075615][-4.2536945 -4.2541852 -4.2496676 -4.2403069 -4.2249432 -4.2156544 -4.2005892 -4.1700068 -4.1458125 -4.143549 -4.1580791 -4.1702871 -4.1748672 -4.1801996 -4.191433][-4.2424259 -4.2372274 -4.2274375 -4.21535 -4.1933641 -4.1702337 -4.1436749 -4.1041517 -4.0799189 -4.092886 -4.1288457 -4.147234 -4.1534352 -4.1659861 -4.1846104][-4.2317977 -4.22192 -4.2036419 -4.1876168 -4.1608477 -4.12022 -4.0731096 -4.0226192 -4.0092716 -4.0534797 -4.1164894 -4.1477551 -4.1556625 -4.1673851 -4.1865206][-4.2096076 -4.199182 -4.1725011 -4.1525531 -4.1267853 -4.0700588 -3.9880633 -3.910852 -3.9198771 -4.0087271 -4.1016722 -4.1518235 -4.1720982 -4.1845536 -4.1978121][-4.18595 -4.174448 -4.1457911 -4.1287856 -4.1067972 -4.0392494 -3.9285262 -3.8233204 -3.8594949 -3.9861851 -4.0975966 -4.1600094 -4.1882877 -4.1979189 -4.2024088][-4.1697369 -4.1570535 -4.134058 -4.1271863 -4.1125889 -4.04975 -3.9471831 -3.8585699 -3.9053419 -4.0284605 -4.1268082 -4.1797338 -4.2041373 -4.203898 -4.1963735][-4.1793785 -4.1678343 -4.1477795 -4.1437192 -4.1343994 -4.0872 -4.0175319 -3.9695532 -4.0123005 -4.10137 -4.1702213 -4.2075396 -4.21859 -4.2062025 -4.1872454][-4.2043176 -4.1987829 -4.1821642 -4.1757855 -4.17157 -4.1380391 -4.09039 -4.06233 -4.1000528 -4.1651249 -4.2161241 -4.240921 -4.2429376 -4.2204347 -4.1977916][-4.2295394 -4.2281966 -4.2160883 -4.2119431 -4.2133989 -4.18907 -4.1511822 -4.1325812 -4.1662722 -4.2209778 -4.2586708 -4.2767835 -4.277648 -4.2492447 -4.2216678][-4.2485456 -4.2554455 -4.2519183 -4.2534437 -4.2569175 -4.2390771 -4.2110481 -4.199614 -4.2242308 -4.264483 -4.2885933 -4.30256 -4.3044381 -4.2789254 -4.2466497][-4.2717204 -4.2853804 -4.2877197 -4.2914677 -4.2923203 -4.2752371 -4.2555485 -4.2542648 -4.2729821 -4.2973404 -4.3077025 -4.3128581 -4.309566 -4.2878966 -4.261857][-4.2907267 -4.3064437 -4.3116307 -4.313549 -4.3085546 -4.2912421 -4.2762446 -4.2794557 -4.2937956 -4.3071389 -4.3097095 -4.3081565 -4.3012714 -4.2852764 -4.2695823]]...]
INFO - root - 2017-12-07 16:26:21.050119: step 26710, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 82h:41m:23s remains)
INFO - root - 2017-12-07 16:26:30.595621: step 26720, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 78h:08m:22s remains)
INFO - root - 2017-12-07 16:26:40.152098: step 26730, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 80h:12m:39s remains)
INFO - root - 2017-12-07 16:26:49.605198: step 26740, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 83h:06m:15s remains)
INFO - root - 2017-12-07 16:26:59.277439: step 26750, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.017 sec/batch; 86h:21m:11s remains)
INFO - root - 2017-12-07 16:27:08.877680: step 26760, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 80h:38m:49s remains)
INFO - root - 2017-12-07 16:27:18.536324: step 26770, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 83h:53m:44s remains)
INFO - root - 2017-12-07 16:27:28.224890: step 26780, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 83h:14m:11s remains)
INFO - root - 2017-12-07 16:27:37.707459: step 26790, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 83h:41m:41s remains)
INFO - root - 2017-12-07 16:27:47.321443: step 26800, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.991 sec/batch; 84h:09m:21s remains)
2017-12-07 16:27:48.252675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3179073 -4.3026438 -4.2797875 -4.2595754 -4.2468944 -4.2322292 -4.2124491 -4.1929479 -4.1845074 -4.2068415 -4.2296896 -4.2340903 -4.2217641 -4.1952105 -4.1525331][-4.3143458 -4.2997603 -4.2804766 -4.2634187 -4.2510176 -4.2331104 -4.2059135 -4.1813583 -4.171051 -4.1953111 -4.2165942 -4.2152171 -4.1964321 -4.1658311 -4.1195278][-4.3113618 -4.2976985 -4.2798433 -4.2616453 -4.2473407 -4.2269073 -4.1911192 -4.1607437 -4.1512046 -4.1775684 -4.199501 -4.195837 -4.1737585 -4.1415725 -4.09688][-4.3072681 -4.2929912 -4.274219 -4.25234 -4.2365675 -4.2144928 -4.1711011 -4.1369028 -4.127214 -4.1496387 -4.1739979 -4.1778283 -4.1621385 -4.13776 -4.0974226][-4.3040385 -4.2889457 -4.2688704 -4.242837 -4.225368 -4.1985388 -4.1472778 -4.105267 -4.0910921 -4.1056576 -4.1323452 -4.15028 -4.1490526 -4.1375427 -4.1071453][-4.3018422 -4.2853055 -4.263165 -4.2313461 -4.2088757 -4.1726003 -4.1050749 -4.0450096 -4.033669 -4.0527482 -4.0886569 -4.1220088 -4.1310034 -4.1279974 -4.1096606][-4.2991228 -4.279911 -4.2542281 -4.2160316 -4.1858926 -4.1355815 -4.0388737 -3.9427209 -3.9389622 -3.9901366 -4.0579562 -4.113451 -4.133801 -4.1344495 -4.1253562][-4.2963862 -4.2738638 -4.2455721 -4.2026968 -4.163043 -4.0929003 -3.9527328 -3.8005347 -3.8193426 -3.9360602 -4.0494666 -4.1230626 -4.1532264 -4.15673 -4.1508627][-4.2941432 -4.2686596 -4.2371769 -4.1904 -4.1431494 -4.059402 -3.8973594 -3.7229619 -3.7716694 -3.9322176 -4.0650868 -4.1436348 -4.1775494 -4.1865807 -4.184639][-4.2912483 -4.2635903 -4.2297316 -4.1829977 -4.1367497 -4.0628328 -3.9398057 -3.8261368 -3.8732278 -4.0091615 -4.1223588 -4.1862226 -4.2114315 -4.2209392 -4.2172112][-4.2866707 -4.2599115 -4.2288933 -4.1886024 -4.1526933 -4.0983911 -4.02314 -3.9629645 -3.9981337 -4.098258 -4.1800075 -4.2220025 -4.2346249 -4.2419305 -4.2397342][-4.2826324 -4.2595029 -4.2374964 -4.2094665 -4.187067 -4.1492176 -4.1003289 -4.0674028 -4.0928292 -4.1645441 -4.21731 -4.2441273 -4.2558708 -4.2626791 -4.2624683][-4.2809639 -4.2614841 -4.2488365 -4.2329812 -4.2204509 -4.1954365 -4.1592555 -4.1364708 -4.1580257 -4.2093778 -4.2385249 -4.2554359 -4.2640243 -4.2666125 -4.2619348][-4.2806778 -4.2635832 -4.2571855 -4.2488136 -4.2431192 -4.2245345 -4.1926603 -4.16633 -4.1809578 -4.2161703 -4.2339611 -4.2462111 -4.2519197 -4.2491217 -4.2414517][-4.2824225 -4.2644162 -4.2588067 -4.2508888 -4.2449646 -4.2250881 -4.1911154 -4.1559005 -4.1633911 -4.1953106 -4.2112546 -4.2194619 -4.2240458 -4.2225504 -4.2177463]]...]
INFO - root - 2017-12-07 16:27:58.002823: step 26810, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.045 sec/batch; 88h:45m:19s remains)
INFO - root - 2017-12-07 16:28:07.417871: step 26820, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 83h:19m:37s remains)
INFO - root - 2017-12-07 16:28:17.046983: step 26830, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 78h:33m:21s remains)
INFO - root - 2017-12-07 16:28:26.541045: step 26840, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 79h:13m:18s remains)
INFO - root - 2017-12-07 16:28:36.012414: step 26850, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 82h:01m:18s remains)
INFO - root - 2017-12-07 16:28:45.595434: step 26860, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 82h:43m:43s remains)
INFO - root - 2017-12-07 16:28:55.235855: step 26870, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 84h:47m:38s remains)
INFO - root - 2017-12-07 16:29:04.868635: step 26880, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 81h:41m:35s remains)
INFO - root - 2017-12-07 16:29:14.451081: step 26890, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 81h:36m:57s remains)
INFO - root - 2017-12-07 16:29:24.006048: step 26900, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 78h:56m:41s remains)
2017-12-07 16:29:24.940325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3034372 -4.3030181 -4.3029118 -4.3024478 -4.301949 -4.3012156 -4.2996345 -4.2977071 -4.2966657 -4.2960081 -4.2947268 -4.2926865 -4.2904286 -4.2881732 -4.2856727][-4.3312378 -4.3329535 -4.333096 -4.3320889 -4.3302231 -4.3278203 -4.3244576 -4.321557 -4.3208585 -4.3213267 -4.321703 -4.3219657 -4.3218842 -4.3211846 -4.3196306][-4.3547668 -4.3566866 -4.3554568 -4.3517966 -4.3469796 -4.3420463 -4.338068 -4.3367748 -4.3388376 -4.3425035 -4.3466969 -4.3509803 -4.3535829 -4.3542409 -4.3530746][-4.3514757 -4.3526721 -4.348876 -4.3408866 -4.3316488 -4.3236761 -4.3186064 -4.320159 -4.3267765 -4.3347259 -4.3431439 -4.3509812 -4.3564272 -4.3584194 -4.3574991][-4.3307471 -4.3315592 -4.324563 -4.3107557 -4.2950387 -4.2819457 -4.2746348 -4.2800326 -4.2940035 -4.3091183 -4.3227115 -4.3336692 -4.3408952 -4.34338 -4.3417983][-4.2735305 -4.2731781 -4.2626047 -4.2413898 -4.2143812 -4.18966 -4.1780925 -4.1907587 -4.2180824 -4.2458014 -4.2674289 -4.282464 -4.2921929 -4.2957745 -4.2949762][-4.1851206 -4.186254 -4.1738977 -4.144177 -4.1010461 -4.0567751 -4.0340056 -4.0536304 -4.099771 -4.1453352 -4.1769361 -4.1964869 -4.2088051 -4.21441 -4.2158518][-4.1005616 -4.1034255 -4.0904145 -4.0532093 -3.9935293 -3.9265587 -3.886925 -3.9122851 -3.9754114 -4.0339513 -4.0717874 -4.0931287 -4.1056619 -4.1144943 -4.1213613][-4.0884027 -4.0960627 -4.0910387 -4.0603261 -4.0054212 -3.9403262 -3.9010258 -3.9221683 -3.9776764 -4.0252309 -4.0496845 -4.0554342 -4.0551729 -4.0594511 -4.0688095][-4.1552119 -4.1664267 -4.171834 -4.1582603 -4.1282158 -4.0915575 -4.0703869 -4.0824494 -4.1129494 -4.135488 -4.1384959 -4.1240926 -4.1076093 -4.1026134 -4.1100864][-4.2181144 -4.2258353 -4.2342176 -4.2341895 -4.2288508 -4.2196302 -4.2153449 -4.2225823 -4.2344165 -4.2377324 -4.2268162 -4.2035432 -4.1817303 -4.1742721 -4.181623][-4.2529826 -4.2551036 -4.2608094 -4.2650518 -4.2694416 -4.2711897 -4.2748904 -4.2800384 -4.2850013 -4.2830553 -4.27152 -4.2523026 -4.2359886 -4.2312989 -4.2378707][-4.2707224 -4.2657161 -4.2631879 -4.2634187 -4.2655573 -4.2690797 -4.2770228 -4.2833724 -4.2879658 -4.2882433 -4.2833629 -4.2736778 -4.2646303 -4.2623839 -4.2663579][-4.2850633 -4.2743344 -4.2631478 -4.2553787 -4.2509046 -4.2518167 -4.2604485 -4.2666259 -4.2709055 -4.2752519 -4.2793322 -4.2807851 -4.2803564 -4.2812858 -4.28408][-4.3018522 -4.2868018 -4.2672825 -4.2500839 -4.2382903 -4.2335749 -4.2360291 -4.23656 -4.2372651 -4.2437286 -4.2568655 -4.273181 -4.2874932 -4.2987881 -4.3058515]]...]
INFO - root - 2017-12-07 16:29:34.649497: step 26910, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 79h:36m:11s remains)
INFO - root - 2017-12-07 16:29:44.247463: step 26920, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 79h:54m:05s remains)
INFO - root - 2017-12-07 16:29:53.833758: step 26930, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.998 sec/batch; 84h:44m:14s remains)
INFO - root - 2017-12-07 16:30:03.236112: step 26940, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 79h:18m:01s remains)
INFO - root - 2017-12-07 16:30:13.007143: step 26950, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 84h:15m:29s remains)
INFO - root - 2017-12-07 16:30:22.748797: step 26960, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 83h:00m:51s remains)
INFO - root - 2017-12-07 16:30:32.404394: step 26970, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 79h:51m:07s remains)
INFO - root - 2017-12-07 16:30:42.154894: step 26980, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 81h:41m:56s remains)
INFO - root - 2017-12-07 16:30:51.711290: step 26990, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.958 sec/batch; 81h:19m:49s remains)
INFO - root - 2017-12-07 16:31:01.454501: step 27000, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 81h:04m:37s remains)
2017-12-07 16:31:02.431205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2977781 -4.28676 -4.2779012 -4.2677426 -4.2548032 -4.23948 -4.2293015 -4.2226305 -4.22142 -4.2278881 -4.2393584 -4.2533908 -4.2644191 -4.2677588 -4.2703657][-4.3097749 -4.3049307 -4.300734 -4.286993 -4.2658038 -4.2441416 -4.2320752 -4.2228808 -4.2217512 -4.2346458 -4.2527966 -4.2686267 -4.2765093 -4.2764659 -4.2749982][-4.32268 -4.3222923 -4.3147583 -4.2921171 -4.2592869 -4.22759 -4.2146935 -4.2054806 -4.2048011 -4.2274861 -4.256218 -4.2755747 -4.2820759 -4.2813048 -4.2774515][-4.3342376 -4.3385763 -4.327837 -4.2961092 -4.251842 -4.20214 -4.1764197 -4.1630559 -4.1660786 -4.1997957 -4.2370605 -4.2608042 -4.2708855 -4.2741408 -4.2679338][-4.337832 -4.3447943 -4.3307157 -4.293787 -4.2382407 -4.1638279 -4.1115875 -4.0790606 -4.086338 -4.1373076 -4.1829634 -4.210391 -4.2270222 -4.241374 -4.2418761][-4.3383775 -4.3456793 -4.3294644 -4.2910943 -4.225955 -4.1245866 -4.01928 -3.9449186 -3.9637375 -4.0545444 -4.1191406 -4.1497231 -4.1703873 -4.1963463 -4.2100177][-4.3375435 -4.3453474 -4.3249073 -4.2796569 -4.2048593 -4.0839372 -3.9192817 -3.7827902 -3.8230131 -3.9717975 -4.0704565 -4.1160069 -4.1403418 -4.1690564 -4.1913381][-4.3346014 -4.3460917 -4.32815 -4.2813244 -4.2094231 -4.099431 -3.9377272 -3.781122 -3.8089635 -3.9613864 -4.0758562 -4.1349025 -4.160624 -4.1826172 -4.2012658][-4.3213706 -4.3391042 -4.3379841 -4.3074155 -4.2525272 -4.1768436 -4.0736213 -3.9596081 -3.9443264 -4.0289359 -4.1203003 -4.1767726 -4.1985536 -4.2138186 -4.2234592][-4.3033719 -4.3292246 -4.3499212 -4.3424988 -4.3059168 -4.2546287 -4.196331 -4.12336 -4.0846424 -4.1093221 -4.1604261 -4.1990452 -4.2134457 -4.2253537 -4.2322063][-4.2875452 -4.3165784 -4.351058 -4.3601141 -4.3361745 -4.3004656 -4.2658539 -4.2195663 -4.1773839 -4.1646023 -4.1770868 -4.19792 -4.2102065 -4.2238784 -4.233974][-4.2755032 -4.3018045 -4.3375082 -4.350421 -4.3332844 -4.3090286 -4.2881408 -4.2626081 -4.2314115 -4.2013931 -4.1851406 -4.1910634 -4.2047496 -4.2235484 -4.2394686][-4.2709742 -4.2901053 -4.3191662 -4.3289247 -4.3136244 -4.2940116 -4.2791505 -4.2663221 -4.2510061 -4.2229605 -4.1988525 -4.1971993 -4.21181 -4.2340713 -4.2533011][-4.2768764 -4.2880349 -4.3071227 -4.3129621 -4.2989392 -4.2815866 -4.2692966 -4.263062 -4.2583389 -4.2414131 -4.2270708 -4.2289238 -4.2420983 -4.2605319 -4.2748079][-4.2917166 -4.2982693 -4.3086948 -4.3116555 -4.302042 -4.2892451 -4.2819448 -4.2801666 -4.278224 -4.2697792 -4.2656388 -4.2710776 -4.2803636 -4.2900305 -4.2964997]]...]
INFO - root - 2017-12-07 16:31:12.141035: step 27010, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 85h:12m:16s remains)
INFO - root - 2017-12-07 16:31:21.796218: step 27020, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 81h:21m:23s remains)
INFO - root - 2017-12-07 16:31:31.348897: step 27030, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 81h:45m:55s remains)
INFO - root - 2017-12-07 16:31:41.128016: step 27040, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.967 sec/batch; 82h:02m:00s remains)
INFO - root - 2017-12-07 16:31:50.733187: step 27050, loss = 2.05, batch loss = 2.00 (7.8 examples/sec; 1.023 sec/batch; 86h:49m:09s remains)
INFO - root - 2017-12-07 16:32:00.398971: step 27060, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 82h:52m:12s remains)
INFO - root - 2017-12-07 16:32:10.116899: step 27070, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 84h:20m:37s remains)
INFO - root - 2017-12-07 16:32:19.864055: step 27080, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.980 sec/batch; 83h:10m:11s remains)
INFO - root - 2017-12-07 16:32:29.481727: step 27090, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 80h:44m:42s remains)
INFO - root - 2017-12-07 16:32:39.018409: step 27100, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 81h:25m:09s remains)
2017-12-07 16:32:39.899830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1721916 -4.1772943 -4.2048831 -4.2328982 -4.2510509 -4.262188 -4.2761059 -4.2881174 -4.2918243 -4.2869596 -4.2758169 -4.2669964 -4.2693443 -4.2738705 -4.287149][-4.1740265 -4.1819487 -4.2084093 -4.2344818 -4.2438865 -4.2477455 -4.2599468 -4.2724266 -4.2736454 -4.2666082 -4.2533832 -4.2429352 -4.2479715 -4.2550011 -4.2709012][-4.1958027 -4.2071443 -4.2293897 -4.2479467 -4.2454171 -4.2343254 -4.2381024 -4.2491164 -4.25044 -4.2429161 -4.2270823 -4.212245 -4.2194104 -4.229393 -4.2487469][-4.2094741 -4.2216463 -4.2365813 -4.24638 -4.2393703 -4.2179327 -4.21778 -4.2315264 -4.2384062 -4.2333412 -4.2122183 -4.1879444 -4.1896105 -4.2000113 -4.2213244][-4.2144613 -4.2226591 -4.229805 -4.233727 -4.2224722 -4.1949277 -4.1925426 -4.2108479 -4.2256575 -4.2281165 -4.2105994 -4.1810737 -4.1751828 -4.1820703 -4.2013474][-4.2172079 -4.2235341 -4.2239876 -4.2194929 -4.19735 -4.15856 -4.144136 -4.16415 -4.1943312 -4.2131543 -4.2104712 -4.18835 -4.1829071 -4.1901307 -4.2037349][-4.2165036 -4.2218385 -4.21799 -4.2064939 -4.1807256 -4.1316576 -4.0891528 -4.0945134 -4.1380424 -4.1798906 -4.1984038 -4.1938577 -4.1927795 -4.2011151 -4.210731][-4.1977386 -4.2017546 -4.2004433 -4.1921725 -4.1711717 -4.1228805 -4.05732 -4.036068 -4.0727839 -4.1242504 -4.1584797 -4.1769333 -4.1878328 -4.1993117 -4.2126174][-4.1682124 -4.1736612 -4.1782856 -4.1785526 -4.1623745 -4.121551 -4.0610671 -4.028657 -4.0488014 -4.0894103 -4.1247315 -4.1547604 -4.178195 -4.1991568 -4.2207561][-4.1697516 -4.1749525 -4.1778927 -4.1790395 -4.1678209 -4.1407819 -4.1063519 -4.0924354 -4.1077256 -4.1268411 -4.1424842 -4.1644917 -4.1856742 -4.2053676 -4.2308455][-4.202579 -4.2047524 -4.2043929 -4.2030177 -4.1904535 -4.1722641 -4.1558528 -4.1581564 -4.1749597 -4.1808662 -4.1783 -4.1843143 -4.1962719 -4.21107 -4.2377777][-4.2290606 -4.2264719 -4.2239819 -4.2185836 -4.2076349 -4.1937041 -4.1822281 -4.1860747 -4.1983128 -4.1967611 -4.18561 -4.1845074 -4.1955385 -4.2070513 -4.2330074][-4.24538 -4.2373309 -4.2344909 -4.2268023 -4.2157469 -4.2010789 -4.190309 -4.1942863 -4.2017264 -4.19121 -4.1713333 -4.1678443 -4.1798463 -4.19414 -4.2207808][-4.2603197 -4.2524471 -4.2500186 -4.2440104 -4.2319703 -4.2191992 -4.2109003 -4.2172008 -4.2220306 -4.203548 -4.1759949 -4.1673231 -4.1775465 -4.1912885 -4.21555][-4.2628994 -4.2565875 -4.253191 -4.247292 -4.2379975 -4.2309141 -4.2276487 -4.23461 -4.2397227 -4.2213321 -4.1929474 -4.184144 -4.1917858 -4.1994271 -4.214797]]...]
INFO - root - 2017-12-07 16:32:49.610366: step 27110, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 81h:03m:42s remains)
INFO - root - 2017-12-07 16:32:59.258982: step 27120, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 82h:51m:09s remains)
INFO - root - 2017-12-07 16:33:08.897818: step 27130, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 83h:19m:16s remains)
INFO - root - 2017-12-07 16:33:18.591426: step 27140, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 78h:34m:20s remains)
INFO - root - 2017-12-07 16:33:28.211637: step 27150, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 70h:49m:39s remains)
INFO - root - 2017-12-07 16:33:37.780935: step 27160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 79h:06m:28s remains)
INFO - root - 2017-12-07 16:33:47.413601: step 27170, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 85h:55m:42s remains)
INFO - root - 2017-12-07 16:33:57.038160: step 27180, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 83h:54m:54s remains)
INFO - root - 2017-12-07 16:34:06.712223: step 27190, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.940 sec/batch; 79h:42m:37s remains)
INFO - root - 2017-12-07 16:34:16.274836: step 27200, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 82h:23m:19s remains)
2017-12-07 16:34:17.192494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2991505 -4.3026328 -4.3106208 -4.3179536 -4.3206272 -4.3242507 -4.3284597 -4.3309631 -4.3328514 -4.3345852 -4.3358665 -4.3363795 -4.3360505 -4.3342376 -4.3323193][-4.270597 -4.2800512 -4.2925448 -4.3001785 -4.2987289 -4.2984128 -4.3008161 -4.3042321 -4.3118033 -4.3211231 -4.3283129 -4.331799 -4.3323331 -4.3295217 -4.3253713][-4.2516832 -4.2694745 -4.2813148 -4.2794943 -4.2636137 -4.2502484 -4.2429247 -4.2434711 -4.2611833 -4.2856874 -4.3038821 -4.3136859 -4.3169832 -4.3143997 -4.3101549][-4.2460084 -4.2670746 -4.2693825 -4.2487364 -4.2115469 -4.177402 -4.1497116 -4.1414862 -4.1734233 -4.2229743 -4.2611523 -4.2839952 -4.292923 -4.2928133 -4.2912726][-4.2501073 -4.2685566 -4.2579293 -4.2166624 -4.1550531 -4.0915089 -4.0284882 -3.9963892 -4.0417323 -4.1283836 -4.1989841 -4.243505 -4.2628126 -4.2663994 -4.2697868][-4.2572556 -4.2713819 -4.2516017 -4.1990032 -4.1206346 -4.0246673 -3.9130008 -3.8390539 -3.8936384 -4.0232139 -4.1308675 -4.1973643 -4.2256083 -4.2325339 -4.2403092][-4.262167 -4.2738347 -4.2532611 -4.2020259 -4.1211796 -4.0071054 -3.853931 -3.7389851 -3.7964611 -3.9588525 -4.0897107 -4.1636591 -4.1899872 -4.1962748 -4.2060866][-4.2720876 -4.28365 -4.2643037 -4.2211771 -4.1525812 -4.0456114 -3.894557 -3.7801986 -3.8345032 -3.9879839 -4.1050754 -4.1612988 -4.1707454 -4.1652093 -4.1699324][-4.2822614 -4.2913904 -4.2742963 -4.2444186 -4.1988006 -4.122097 -4.0146618 -3.9394593 -3.980689 -4.0814333 -4.1510057 -4.1714377 -4.1552238 -4.1290827 -4.1230316][-4.2804108 -4.2877965 -4.2748957 -4.2554331 -4.2323818 -4.1897082 -4.1320591 -4.0905008 -4.1160984 -4.1612682 -4.1754656 -4.1579537 -4.116807 -4.0698853 -4.0549235][-4.2732258 -4.2832875 -4.274827 -4.2606668 -4.2490196 -4.2281737 -4.2038488 -4.1843834 -4.19641 -4.2011886 -4.17002 -4.1194324 -4.0550151 -3.9915454 -3.9728389][-4.2616825 -4.2746944 -4.2710438 -4.2622757 -4.2584476 -4.2506318 -4.2415643 -4.2290077 -4.2289782 -4.2163386 -4.1668015 -4.1002836 -4.019877 -3.9445589 -3.9207797][-4.2566772 -4.2688522 -4.2668142 -4.2644672 -4.2699089 -4.2707644 -4.263485 -4.2497 -4.2413511 -4.2214847 -4.1713676 -4.10643 -4.03069 -3.9653654 -3.9436388][-4.2628684 -4.2697134 -4.267365 -4.2704105 -4.2804718 -4.2833323 -4.2724056 -4.2547693 -4.2445893 -4.2246175 -4.1813416 -4.1300783 -4.0759344 -4.0356126 -4.0243988][-4.2752 -4.2728214 -4.270483 -4.2794638 -4.29081 -4.290091 -4.2749758 -4.2533188 -4.2457156 -4.2307668 -4.1985722 -4.1654606 -4.1314688 -4.1118693 -4.1097145]]...]
INFO - root - 2017-12-07 16:34:26.781037: step 27210, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 81h:06m:42s remains)
INFO - root - 2017-12-07 16:34:36.470262: step 27220, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 81h:09m:22s remains)
INFO - root - 2017-12-07 16:34:46.236464: step 27230, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 80h:23m:30s remains)
INFO - root - 2017-12-07 16:34:55.800616: step 27240, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.975 sec/batch; 82h:38m:11s remains)
INFO - root - 2017-12-07 16:35:05.617295: step 27250, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.984 sec/batch; 83h:26m:47s remains)
INFO - root - 2017-12-07 16:35:15.307523: step 27260, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 78h:45m:34s remains)
INFO - root - 2017-12-07 16:35:24.871463: step 27270, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 83h:21m:47s remains)
INFO - root - 2017-12-07 16:35:34.391477: step 27280, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 83h:27m:16s remains)
INFO - root - 2017-12-07 16:35:44.078516: step 27290, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 83h:50m:14s remains)
INFO - root - 2017-12-07 16:35:53.820478: step 27300, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 81h:02m:11s remains)
2017-12-07 16:35:54.788605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3312039 -4.3266072 -4.3199897 -4.3072596 -4.2911191 -4.2804646 -4.2739463 -4.2659163 -4.2677364 -4.2784338 -4.29061 -4.3032088 -4.31414 -4.3121824 -4.2964716][-4.3360829 -4.3375854 -4.3384891 -4.331501 -4.3162308 -4.3054571 -4.2998509 -4.2943511 -4.2962074 -4.3050609 -4.31418 -4.322649 -4.33185 -4.327055 -4.30789][-4.3387914 -4.3384752 -4.3409996 -4.3353968 -4.3212857 -4.3134532 -4.3128209 -4.3132043 -4.3158913 -4.323432 -4.3265762 -4.3274126 -4.3314743 -4.3212953 -4.2959538][-4.3239565 -4.3224897 -4.32437 -4.3201985 -4.3098197 -4.3057652 -4.3083081 -4.31071 -4.311408 -4.3174906 -4.3175511 -4.3126683 -4.312861 -4.3009667 -4.2708197][-4.2818322 -4.2846017 -4.2920017 -4.2916079 -4.284996 -4.2824244 -4.2816472 -4.2784944 -4.2772417 -4.28482 -4.2871537 -4.28066 -4.2804766 -4.2701979 -4.231966][-4.2211461 -4.2260623 -4.2352948 -4.2312589 -4.2189417 -4.2094464 -4.1963387 -4.1758456 -4.172143 -4.1921725 -4.2071719 -4.2025895 -4.2045646 -4.2021337 -4.164175][-4.1608596 -4.1520386 -4.1428266 -4.1170273 -4.0853992 -4.0567403 -4.0178585 -3.9660275 -3.9615281 -4.0094543 -4.0486684 -4.0564728 -4.0701222 -4.0865588 -4.0731258][-4.1489272 -4.1150174 -4.071209 -4.0128021 -3.9554844 -3.9048762 -3.8374515 -3.7519448 -3.7497478 -3.8348203 -3.9089198 -3.944937 -3.9817088 -4.0244703 -4.0423183][-4.203105 -4.1639242 -4.1068683 -4.0397429 -3.9811945 -3.9396973 -3.8896976 -3.8300016 -3.8219132 -3.8849566 -3.950192 -3.9902964 -4.0291986 -4.0737677 -4.1014657][-4.2566166 -4.2327371 -4.1955824 -4.1537881 -4.1145015 -4.0893846 -4.0641727 -4.031846 -4.01866 -4.0417547 -4.0737743 -4.0998597 -4.1280255 -4.1574283 -4.1730409][-4.2669706 -4.258975 -4.2440014 -4.2259169 -4.2055478 -4.1923923 -4.1788244 -4.1616335 -4.15061 -4.1491871 -4.15212 -4.1596689 -4.1693387 -4.1766195 -4.1749277][-4.221777 -4.2271657 -4.2315912 -4.2338896 -4.2285233 -4.2245479 -4.2171702 -4.208209 -4.19977 -4.1888371 -4.1782331 -4.1748481 -4.1713781 -4.168633 -4.1644783][-4.1391144 -4.1582031 -4.1819935 -4.2053905 -4.2162433 -4.2247286 -4.2264609 -4.2273846 -4.2239141 -4.212296 -4.2020245 -4.198494 -4.1952395 -4.1962814 -4.1968393][-4.0806379 -4.1017027 -4.1365895 -4.16915 -4.1900926 -4.2125659 -4.2289028 -4.2428937 -4.2459955 -4.2414966 -4.2426367 -4.2477202 -4.2516437 -4.2555127 -4.2545724][-4.058598 -4.0728855 -4.1071744 -4.1408472 -4.1653633 -4.19538 -4.2231364 -4.246942 -4.2582817 -4.264401 -4.2759547 -4.2893772 -4.3001595 -4.3051543 -4.2979517]]...]
INFO - root - 2017-12-07 16:36:04.367190: step 27310, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 83h:12m:55s remains)
INFO - root - 2017-12-07 16:36:13.694569: step 27320, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 78h:25m:25s remains)
INFO - root - 2017-12-07 16:36:23.459606: step 27330, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 81h:05m:01s remains)
INFO - root - 2017-12-07 16:36:33.139514: step 27340, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 82h:31m:08s remains)
INFO - root - 2017-12-07 16:36:42.734915: step 27350, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.936 sec/batch; 79h:18m:19s remains)
INFO - root - 2017-12-07 16:36:52.476545: step 27360, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 84h:03m:10s remains)
INFO - root - 2017-12-07 16:37:02.097588: step 27370, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 79h:07m:36s remains)
INFO - root - 2017-12-07 16:37:11.714390: step 27380, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 80h:55m:59s remains)
INFO - root - 2017-12-07 16:37:21.325118: step 27390, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 82h:47m:01s remains)
INFO - root - 2017-12-07 16:37:31.116672: step 27400, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.997 sec/batch; 84h:30m:57s remains)
2017-12-07 16:37:32.144323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2499952 -4.1962347 -4.1288366 -4.0778618 -4.06892 -4.1089706 -4.1728125 -4.2271638 -4.2394252 -4.2213011 -4.1848326 -4.1467667 -4.1333613 -4.1450748 -4.1536083][-4.26601 -4.2199049 -4.148839 -4.0898337 -4.0821018 -4.1283307 -4.18873 -4.2361116 -4.2510481 -4.2436347 -4.2238917 -4.1986041 -4.1848235 -4.182672 -4.1798763][-4.2688909 -4.2353721 -4.1801395 -4.1289473 -4.124548 -4.1631846 -4.20933 -4.2420249 -4.2544541 -4.2535272 -4.2464409 -4.2342715 -4.2222266 -4.2116356 -4.2044334][-4.2601633 -4.2382069 -4.199574 -4.1617842 -4.1563931 -4.1784372 -4.2041049 -4.2284951 -4.2463622 -4.2564883 -4.2606564 -4.2606239 -4.2535691 -4.2412462 -4.231081][-4.246635 -4.2321415 -4.2052326 -4.1772509 -4.1706071 -4.1698346 -4.1762238 -4.1950827 -4.2207265 -4.24649 -4.2668042 -4.2775011 -4.2756777 -4.2623191 -4.247026][-4.2362843 -4.2302151 -4.2121444 -4.1912012 -4.1772604 -4.1493258 -4.1361442 -4.1487145 -4.1792 -4.219007 -4.2566442 -4.2766037 -4.2789779 -4.2700515 -4.2535305][-4.2307763 -4.2248363 -4.2135658 -4.1990352 -4.1764097 -4.1254563 -4.0922847 -4.1071439 -4.1491575 -4.1985087 -4.2400408 -4.2623549 -4.2662463 -4.2641191 -4.2520051][-4.2317276 -4.221067 -4.2052765 -4.1902261 -4.155489 -4.083499 -4.0400796 -4.0724826 -4.1333275 -4.185286 -4.2174153 -4.2346044 -4.2407894 -4.2476974 -4.2481451][-4.2348385 -4.2210107 -4.2045183 -4.1798987 -4.1294155 -4.0495105 -4.0104456 -4.061213 -4.1356053 -4.1827936 -4.2002835 -4.2073083 -4.2122769 -4.2238646 -4.2368975][-4.2366042 -4.2265711 -4.2190747 -4.1918416 -4.1362963 -4.0645709 -4.0316591 -4.0774078 -4.1448088 -4.180655 -4.1865134 -4.1864376 -4.1861262 -4.2018132 -4.2236562][-4.2397246 -4.2400107 -4.2425079 -4.2201476 -4.171999 -4.1142631 -4.0796232 -4.0998182 -4.14175 -4.1589227 -4.1531048 -4.1515846 -4.1585021 -4.18809 -4.218482][-4.2463913 -4.255702 -4.2633176 -4.246273 -4.2048631 -4.1559434 -4.1200085 -4.1162624 -4.1327987 -4.1346579 -4.1172791 -4.1146855 -4.1364145 -4.1786938 -4.2135448][-4.2518334 -4.2610378 -4.2692308 -4.2573981 -4.2234421 -4.1806812 -4.148324 -4.1347084 -4.1362419 -4.1218224 -4.0933876 -4.0896859 -4.12021 -4.1645069 -4.1981936][-4.2525077 -4.2576413 -4.2635255 -4.2567644 -4.2318115 -4.1986713 -4.1745572 -4.1623573 -4.16064 -4.1447015 -4.1216755 -4.1185389 -4.1404576 -4.1692567 -4.1891046][-4.2478647 -4.2497811 -4.2554379 -4.2516923 -4.2340078 -4.2081957 -4.1885581 -4.1753917 -4.1731453 -4.1666045 -4.1617584 -4.1667385 -4.1793437 -4.1900926 -4.1936336]]...]
INFO - root - 2017-12-07 16:37:41.649652: step 27410, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 80h:08m:50s remains)
INFO - root - 2017-12-07 16:37:51.256128: step 27420, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 85h:59m:51s remains)
INFO - root - 2017-12-07 16:38:01.020882: step 27430, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.019 sec/batch; 86h:22m:21s remains)
INFO - root - 2017-12-07 16:38:10.642968: step 27440, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 80h:33m:58s remains)
INFO - root - 2017-12-07 16:38:20.372449: step 27450, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 83h:04m:53s remains)
INFO - root - 2017-12-07 16:38:30.124127: step 27460, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 81h:50m:49s remains)
INFO - root - 2017-12-07 16:38:39.683395: step 27470, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 80h:33m:56s remains)
INFO - root - 2017-12-07 16:38:49.265925: step 27480, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 81h:44m:10s remains)
INFO - root - 2017-12-07 16:38:58.942584: step 27490, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 83h:34m:21s remains)
INFO - root - 2017-12-07 16:39:08.574917: step 27500, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 81h:24m:02s remains)
2017-12-07 16:39:09.442700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3037796 -4.2845144 -4.2525282 -4.21738 -4.1941266 -4.2019019 -4.222784 -4.2384343 -4.2595329 -4.2731581 -4.2767529 -4.2739453 -4.2711339 -4.2702456 -4.2646189][-4.29748 -4.2770543 -4.242691 -4.2036333 -4.1776891 -4.18311 -4.2000728 -4.2113972 -4.2349925 -4.2551212 -4.2604308 -4.2575688 -4.2559681 -4.2560859 -4.2479439][-4.293705 -4.2766347 -4.2446833 -4.2067952 -4.1782274 -4.1736164 -4.1755843 -4.1789627 -4.2028646 -4.2288256 -4.2378078 -4.2358613 -4.2371125 -4.2436948 -4.2355494][-4.2989225 -4.2888551 -4.262145 -4.2254648 -4.1897311 -4.1650643 -4.1420064 -4.1322575 -4.1576433 -4.1950831 -4.214323 -4.2210183 -4.2311287 -4.2435107 -4.2344217][-4.3093791 -4.3045464 -4.2811828 -4.2422028 -4.1937723 -4.1398878 -4.0801063 -4.0485759 -4.0821314 -4.1432052 -4.1849809 -4.2101259 -4.2322083 -4.2474027 -4.2383895][-4.3183908 -4.3164806 -4.2929654 -4.248229 -4.1860089 -4.1035886 -4.0002093 -3.9344881 -3.9806182 -4.0802794 -4.1603441 -4.2091618 -4.23849 -4.2530503 -4.2445703][-4.3244672 -4.3243542 -4.2973261 -4.2452517 -4.1710529 -4.067822 -3.9287796 -3.824162 -3.8874693 -4.0335903 -4.1533751 -4.2184415 -4.2466702 -4.255209 -4.2416887][-4.3269024 -4.3256054 -4.2961564 -4.2410359 -4.1655197 -4.0602169 -3.9148419 -3.7989519 -3.8733895 -4.0399308 -4.1680536 -4.2299137 -4.2511716 -4.2512679 -4.2359304][-4.3305612 -4.3272171 -4.299345 -4.2485428 -4.1846809 -4.096849 -3.9784608 -3.8904502 -3.9522276 -4.0914583 -4.1949105 -4.2389731 -4.24686 -4.2418656 -4.2331285][-4.3349786 -4.3316865 -4.3055654 -4.2654586 -4.2211537 -4.1587443 -4.0762029 -4.016336 -4.0509334 -4.1432195 -4.21623 -4.24384 -4.242435 -4.2382021 -4.2401023][-4.3349538 -4.3321238 -4.3092 -4.2828827 -4.2620368 -4.22525 -4.1727138 -4.1300454 -4.1396575 -4.1899805 -4.2370586 -4.2560449 -4.2536774 -4.2546358 -4.2633181][-4.330626 -4.3283215 -4.3097711 -4.2972841 -4.2972589 -4.2842903 -4.2528753 -4.2193933 -4.2122946 -4.2336736 -4.2656479 -4.2808666 -4.2801957 -4.2828684 -4.2900472][-4.3271194 -4.326376 -4.3137541 -4.3116689 -4.3249907 -4.3270478 -4.3072991 -4.28019 -4.2672043 -4.2755651 -4.2978244 -4.3110714 -4.3115315 -4.3140993 -4.317873][-4.3256326 -4.3262615 -4.3189006 -4.3230853 -4.3424306 -4.3514876 -4.338521 -4.3180661 -4.3065963 -4.3110662 -4.3263364 -4.334753 -4.3360872 -4.3390207 -4.3412771][-4.3234239 -4.3253098 -4.32192 -4.3282738 -4.3453913 -4.355144 -4.3474431 -4.3317385 -4.3233228 -4.3260431 -4.334177 -4.3380651 -4.3401661 -4.3447804 -4.34635]]...]
INFO - root - 2017-12-07 16:39:19.064534: step 27510, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 80h:56m:29s remains)
INFO - root - 2017-12-07 16:39:28.741173: step 27520, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.013 sec/batch; 85h:50m:38s remains)
INFO - root - 2017-12-07 16:39:38.431549: step 27530, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.995 sec/batch; 84h:17m:39s remains)
INFO - root - 2017-12-07 16:39:48.036583: step 27540, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 80h:01m:48s remains)
INFO - root - 2017-12-07 16:39:57.541486: step 27550, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 83h:37m:50s remains)
INFO - root - 2017-12-07 16:40:07.181735: step 27560, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 83h:23m:01s remains)
INFO - root - 2017-12-07 16:40:16.837188: step 27570, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 83h:18m:32s remains)
INFO - root - 2017-12-07 16:40:26.445158: step 27580, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 82h:25m:31s remains)
INFO - root - 2017-12-07 16:40:35.985842: step 27590, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 79h:23m:40s remains)
INFO - root - 2017-12-07 16:40:45.682810: step 27600, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.935 sec/batch; 79h:12m:01s remains)
2017-12-07 16:40:46.672971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1885428 -4.2042155 -4.214602 -4.2128983 -4.2201834 -4.2376728 -4.25488 -4.2696595 -4.2576714 -4.2075667 -4.1657462 -4.1721191 -4.1985903 -4.2306676 -4.2401][-4.190011 -4.2037864 -4.20843 -4.2043467 -4.2123508 -4.2334538 -4.2552714 -4.277246 -4.2771306 -4.2459655 -4.21474 -4.2198081 -4.2436595 -4.26988 -4.27666][-4.184875 -4.1916857 -4.1882982 -4.1822824 -4.189115 -4.2091951 -4.2356663 -4.2657995 -4.2797046 -4.2719412 -4.2537918 -4.2525349 -4.2644134 -4.2772017 -4.2827396][-4.179987 -4.171649 -4.1659575 -4.1611123 -4.16568 -4.1801891 -4.2077847 -4.2384033 -4.2579532 -4.2605562 -4.24651 -4.2344851 -4.23568 -4.2443666 -4.2545762][-4.1854343 -4.1649003 -4.1558752 -4.1506844 -4.1512828 -4.1597877 -4.180521 -4.2030778 -4.2192283 -4.2179961 -4.1957579 -4.1707945 -4.1647968 -4.1754613 -4.1965322][-4.1857591 -4.1618919 -4.1492887 -4.1414552 -4.1371765 -4.1398191 -4.160337 -4.1796637 -4.1853991 -4.1665182 -4.1259689 -4.0880585 -4.0802031 -4.0989642 -4.1344943][-4.1716237 -4.1543369 -4.1440549 -4.1365013 -4.1285815 -4.1315112 -4.1598206 -4.1761756 -4.1668258 -4.1281314 -4.0662632 -4.010087 -4.0031705 -4.0363526 -4.0865874][-4.1538558 -4.14859 -4.1447763 -4.1422806 -4.1346283 -4.1407089 -4.1727142 -4.1846833 -4.1642866 -4.1133008 -4.0403514 -3.9745865 -3.9734023 -4.0215955 -4.0823636][-4.1309457 -4.1450543 -4.1580887 -4.1643262 -4.1629329 -4.1725454 -4.1968455 -4.2012749 -4.1786613 -4.13022 -4.0665655 -4.0120049 -4.0138683 -4.0620108 -4.1169562][-4.1271253 -4.1527791 -4.1755471 -4.1871233 -4.1923881 -4.2043214 -4.2191787 -4.2194118 -4.2069592 -4.1761165 -4.1344595 -4.0998025 -4.1013875 -4.1333814 -4.1683092][-4.163269 -4.1875768 -4.2086225 -4.2200489 -4.225657 -4.2338529 -4.2432241 -4.2448044 -4.2415557 -4.22996 -4.2096343 -4.1896458 -4.1878791 -4.2014651 -4.2155066][-4.2137351 -4.2282338 -4.2392316 -4.2457294 -4.2518978 -4.2637591 -4.2754183 -4.279994 -4.2812681 -4.2788248 -4.2714453 -4.2616334 -4.2547321 -4.2526083 -4.2527752][-4.2609458 -4.2651629 -4.2704234 -4.2728357 -4.2805004 -4.2920089 -4.3043513 -4.3092527 -4.3106642 -4.3101978 -4.306663 -4.3005981 -4.2924247 -4.2855439 -4.2796683][-4.2917519 -4.2927632 -4.2954 -4.2979684 -4.3019652 -4.306252 -4.3130074 -4.3167334 -4.3167048 -4.3178635 -4.3172646 -4.3140483 -4.3078938 -4.301775 -4.2952538][-4.3067093 -4.3059807 -4.3069978 -4.3087077 -4.308311 -4.3082509 -4.3099556 -4.3119907 -4.3109856 -4.3103266 -4.3094378 -4.3065372 -4.3038907 -4.302577 -4.3014636]]...]
INFO - root - 2017-12-07 16:40:56.336389: step 27610, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 82h:21m:52s remains)
INFO - root - 2017-12-07 16:41:05.959987: step 27620, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.947 sec/batch; 80h:13m:13s remains)
INFO - root - 2017-12-07 16:41:15.711852: step 27630, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 80h:57m:59s remains)
INFO - root - 2017-12-07 16:41:25.253621: step 27640, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 82h:04m:37s remains)
INFO - root - 2017-12-07 16:41:34.982802: step 27650, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.999 sec/batch; 84h:35m:15s remains)
INFO - root - 2017-12-07 16:41:44.613234: step 27660, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 84h:02m:58s remains)
INFO - root - 2017-12-07 16:41:54.365898: step 27670, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.020 sec/batch; 86h:20m:57s remains)
INFO - root - 2017-12-07 16:42:03.993151: step 27680, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 85h:46m:42s remains)
INFO - root - 2017-12-07 16:42:13.544773: step 27690, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 81h:26m:57s remains)
INFO - root - 2017-12-07 16:42:23.284467: step 27700, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 84h:30m:06s remains)
2017-12-07 16:42:24.235450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281631 -4.3271589 -4.3282747 -4.3306451 -4.3339305 -4.3370028 -4.3384042 -4.33782 -4.3362613 -4.3291011 -4.3183 -4.3108487 -4.3037467 -4.2971711 -4.2912965][-4.3115945 -4.3099251 -4.3102674 -4.3133621 -4.3206968 -4.3287835 -4.3344588 -4.33714 -4.3372054 -4.3287969 -4.3156052 -4.3053126 -4.2947111 -4.2858891 -4.2795343][-4.2690716 -4.2625303 -4.2574391 -4.2586703 -4.2685161 -4.2810707 -4.2918844 -4.2998571 -4.3047357 -4.2992334 -4.2888727 -4.280376 -4.2695131 -4.2603722 -4.2551389][-4.2125 -4.2000346 -4.1908894 -4.1936808 -4.208189 -4.2267337 -4.2418661 -4.2520761 -4.2581635 -4.2540011 -4.2459908 -4.2386293 -4.2289715 -4.2218566 -4.2227488][-4.1453371 -4.1263351 -4.1122603 -4.1158204 -4.1382704 -4.1698661 -4.1920877 -4.2047472 -4.2124496 -4.20981 -4.2025447 -4.1975455 -4.1946063 -4.1948342 -4.204278][-4.0568137 -4.0249467 -4.0009146 -4.0063691 -4.0423412 -4.0918045 -4.1268592 -4.1473322 -4.1617475 -4.1621633 -4.1580482 -4.1610017 -4.169661 -4.1815228 -4.2001343][-3.9602368 -3.9034991 -3.8574209 -3.8606281 -3.9078031 -3.9744387 -4.0271397 -4.0658965 -4.0937424 -4.1000304 -4.1024761 -4.1172309 -4.1382332 -4.1637621 -4.1953974][-3.9047146 -3.8353739 -3.7801042 -3.7843442 -3.8295698 -3.8939028 -3.9467731 -3.9924974 -4.0261183 -4.0334878 -4.0406384 -4.0683436 -4.1012344 -4.1386018 -4.1865563][-3.9523578 -3.9031956 -3.8717003 -3.8888483 -3.9301586 -3.9790168 -4.0135403 -4.0395374 -4.0556965 -4.0516973 -4.0511913 -4.0766144 -4.1086464 -4.1442256 -4.1920848][-4.0495238 -4.0214796 -4.0123425 -4.0395203 -4.0807252 -4.1212072 -4.1458945 -4.1590457 -4.1622 -4.1517544 -4.141819 -4.1525908 -4.1693 -4.189496 -4.2211633][-4.1421728 -4.1283026 -4.1287608 -4.153512 -4.186089 -4.2146235 -4.2322431 -4.2395616 -4.2387815 -4.2285028 -4.214613 -4.2127714 -4.2156534 -4.22158 -4.2374163][-4.2166929 -4.2119455 -4.2160668 -4.2328377 -4.2519922 -4.2657647 -4.2753124 -4.2790036 -4.2781463 -4.2712312 -4.2572131 -4.2475462 -4.2408786 -4.2368379 -4.2427092][-4.2616096 -4.2599826 -4.2633944 -4.2739797 -4.2855563 -4.293035 -4.2988458 -4.3008871 -4.301301 -4.2985458 -4.28806 -4.2770219 -4.2669315 -4.2592597 -4.2616172][-4.3034182 -4.3023615 -4.3023605 -4.3085542 -4.3158355 -4.3197827 -4.3219271 -4.3233085 -4.3249259 -4.3238845 -4.3175616 -4.3093953 -4.300386 -4.2940497 -4.2966161][-4.3371148 -4.3380508 -4.3369904 -4.3394403 -4.3416071 -4.3419595 -4.3420763 -4.3438206 -4.3462586 -4.3456955 -4.3420935 -4.3372288 -4.3314795 -4.3284235 -4.3317194]]...]
INFO - root - 2017-12-07 16:42:34.055905: step 27710, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.030 sec/batch; 87h:14m:41s remains)
INFO - root - 2017-12-07 16:42:43.784574: step 27720, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 78h:07m:27s remains)
INFO - root - 2017-12-07 16:42:53.394920: step 27730, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 79h:31m:27s remains)
INFO - root - 2017-12-07 16:43:03.107147: step 27740, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 80h:11m:37s remains)
INFO - root - 2017-12-07 16:43:12.844714: step 27750, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 83h:04m:07s remains)
INFO - root - 2017-12-07 16:43:22.364955: step 27760, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 73h:41m:20s remains)
INFO - root - 2017-12-07 16:43:31.930097: step 27770, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 80h:51m:03s remains)
INFO - root - 2017-12-07 16:43:41.592042: step 27780, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 76h:59m:52s remains)
INFO - root - 2017-12-07 16:43:51.334390: step 27790, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 84h:44m:15s remains)
INFO - root - 2017-12-07 16:44:00.881088: step 27800, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 79h:07m:53s remains)
2017-12-07 16:44:01.842036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2710805 -4.2662649 -4.2691674 -4.2875509 -4.3149471 -4.3219371 -4.3184338 -4.3189249 -4.3194818 -4.3174648 -4.3162313 -4.319665 -4.3197393 -4.3052306 -4.2891488][-4.2429013 -4.241631 -4.243422 -4.2600908 -4.2903166 -4.3020229 -4.3073559 -4.3157687 -4.3233795 -4.3260279 -4.3261395 -4.3303318 -4.33263 -4.3093357 -4.28551][-4.1956367 -4.208045 -4.2197609 -4.2415504 -4.2741466 -4.2829895 -4.2880273 -4.2971644 -4.3040109 -4.3113689 -4.3122072 -4.3181214 -4.3298216 -4.3133755 -4.2874532][-4.1523376 -4.181601 -4.21159 -4.244379 -4.2747965 -4.2702808 -4.2611766 -4.2678323 -4.2750154 -4.2852764 -4.2865376 -4.2925506 -4.312573 -4.3139682 -4.2970767][-4.1414189 -4.1816616 -4.223804 -4.2545772 -4.2605972 -4.2284203 -4.1938248 -4.1983438 -4.2198277 -4.242094 -4.2539749 -4.2656116 -4.2914968 -4.311204 -4.3077893][-4.1547542 -4.1893549 -4.2281361 -4.2472706 -4.2243977 -4.1556821 -4.0802851 -4.0747976 -4.1239586 -4.1766858 -4.2179003 -4.2484589 -4.2783422 -4.3058257 -4.3114662][-4.1770916 -4.1918545 -4.2124805 -4.2094889 -4.1575532 -4.0505795 -3.9310803 -3.9123113 -4.0003929 -4.0981569 -4.1731443 -4.2297268 -4.2687864 -4.2981129 -4.3095179][-4.1910081 -4.1939816 -4.1950808 -4.1717529 -4.1051879 -3.9861331 -3.8542016 -3.8295155 -3.9376786 -4.0586925 -4.1519337 -4.2194905 -4.2616944 -4.2887716 -4.2975206][-4.1892385 -4.19578 -4.1914587 -4.1625528 -4.1095443 -4.0273418 -3.9367235 -3.9150763 -3.9895558 -4.0862393 -4.1683683 -4.2257423 -4.26011 -4.2776237 -4.2811794][-4.1857004 -4.1969433 -4.1934462 -4.1693683 -4.1398821 -4.103014 -4.0640683 -4.0542784 -4.0946093 -4.1524782 -4.2079911 -4.2476144 -4.2678962 -4.2739911 -4.2727642][-4.1902032 -4.1990676 -4.2001042 -4.1841559 -4.1651573 -4.1571093 -4.1571732 -4.163867 -4.1879549 -4.2194214 -4.2505169 -4.2741084 -4.2835188 -4.27495 -4.266665][-4.2158251 -4.2189946 -4.2189312 -4.2040811 -4.187696 -4.1946082 -4.2137737 -4.2298441 -4.2480545 -4.2652078 -4.278244 -4.2871704 -4.2862673 -4.2660055 -4.2492743][-4.2553582 -4.2538548 -4.2488289 -4.2314191 -4.2149305 -4.2290583 -4.2531524 -4.26779 -4.2794185 -4.2890296 -4.2916365 -4.2880464 -4.2777429 -4.2567329 -4.2360234][-4.2856846 -4.2827811 -4.2762814 -4.2581282 -4.2434578 -4.2596593 -4.2827482 -4.2915754 -4.2925954 -4.29658 -4.2952118 -4.2885075 -4.2780371 -4.2614446 -4.2438459][-4.3019247 -4.2996049 -4.2972693 -4.2843666 -4.2742176 -4.2866516 -4.3031859 -4.30674 -4.2983232 -4.2970147 -4.2949853 -4.2882376 -4.2788057 -4.2663941 -4.2549577]]...]
INFO - root - 2017-12-07 16:44:11.359687: step 27810, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 80h:19m:40s remains)
INFO - root - 2017-12-07 16:44:20.966527: step 27820, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 84h:44m:06s remains)
INFO - root - 2017-12-07 16:44:30.393020: step 27830, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 84h:25m:24s remains)
INFO - root - 2017-12-07 16:44:39.951755: step 27840, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 82h:46m:10s remains)
INFO - root - 2017-12-07 16:44:49.639051: step 27850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 76h:43m:53s remains)
INFO - root - 2017-12-07 16:44:59.166913: step 27860, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 83h:43m:36s remains)
INFO - root - 2017-12-07 16:45:08.809439: step 27870, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 83h:13m:03s remains)
INFO - root - 2017-12-07 16:45:18.222473: step 27880, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 80h:37m:44s remains)
INFO - root - 2017-12-07 16:45:27.896194: step 27890, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 81h:58m:15s remains)
INFO - root - 2017-12-07 16:45:37.586983: step 27900, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.901 sec/batch; 76h:12m:32s remains)
2017-12-07 16:45:38.569768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3213363 -4.3212471 -4.3195534 -4.3151407 -4.3133717 -4.313138 -4.3117394 -4.3095312 -4.308516 -4.3095403 -4.3118773 -4.3138628 -4.3173537 -4.3217287 -4.3257012][-4.3120842 -4.3122344 -4.3082047 -4.3002815 -4.2966661 -4.2972593 -4.2952809 -4.2898974 -4.2837763 -4.2830787 -4.2892246 -4.2962022 -4.3027239 -4.3088107 -4.3156781][-4.29186 -4.2904463 -4.2819481 -4.2703576 -4.2635589 -4.2636251 -4.2626185 -4.2527828 -4.2403054 -4.2392368 -4.2527757 -4.2685566 -4.2795253 -4.2859769 -4.293026][-4.2509112 -4.2432113 -4.2266593 -4.2069507 -4.1956563 -4.196166 -4.1966214 -4.1841879 -4.164782 -4.1609859 -4.1852984 -4.2137694 -4.2344394 -4.2462568 -4.2582078][-4.2011275 -4.180861 -4.1527758 -4.1220832 -4.1090713 -4.1061554 -4.1031365 -4.0851316 -4.052702 -4.042666 -4.0742869 -4.1122503 -4.1507888 -4.1823153 -4.2119384][-4.1655312 -4.1369166 -4.0993252 -4.0607882 -4.0463877 -4.0397806 -4.0243115 -3.9861057 -3.9339862 -3.9195137 -3.9580903 -4.0026774 -4.0604858 -4.112875 -4.1590776][-4.1338563 -4.1056 -4.0638914 -4.0194292 -4.005621 -4.0002742 -3.9713655 -3.9112744 -3.8471532 -3.8496671 -3.9108553 -3.9666355 -4.0246358 -4.0711322 -4.114912][-4.1049461 -4.0825086 -4.0463848 -4.0023012 -3.9951727 -4.0001893 -3.972928 -3.90786 -3.854732 -3.8802259 -3.9545891 -4.0149078 -4.0583344 -4.0839911 -4.112186][-4.1138535 -4.10047 -4.0805793 -4.0463524 -4.0457664 -4.0611515 -4.0428219 -3.9923267 -3.9627135 -3.9885123 -4.0478125 -4.0949764 -4.1172218 -4.1235518 -4.1398435][-4.1416206 -4.1410265 -4.1374941 -4.1181335 -4.1247258 -4.1478329 -4.1365833 -4.1053753 -4.0948296 -4.1129761 -4.1492052 -4.1784067 -4.1874123 -4.1854429 -4.1939554][-4.169281 -4.175746 -4.1842346 -4.1798162 -4.1902966 -4.2154036 -4.2147732 -4.1979771 -4.1924381 -4.2067609 -4.2274251 -4.2473063 -4.2538414 -4.2511282 -4.2524514][-4.2049642 -4.2135482 -4.2260251 -4.2270546 -4.2360363 -4.2544656 -4.2593031 -4.2527008 -4.2538958 -4.267571 -4.2812519 -4.294692 -4.2967811 -4.2911582 -4.2888994][-4.2405539 -4.2501268 -4.2597842 -4.2630205 -4.2682004 -4.2790904 -4.2858624 -4.286036 -4.2899094 -4.3000388 -4.3101149 -4.3189421 -4.3172746 -4.3111324 -4.308382][-4.2745962 -4.28179 -4.2893395 -4.2938185 -4.2984786 -4.3061805 -4.3097563 -4.3124042 -4.3163018 -4.3210688 -4.3262963 -4.3302922 -4.3270736 -4.3215022 -4.3188572][-4.2973366 -4.301403 -4.3056 -4.3090868 -4.3135052 -4.3186936 -4.3211713 -4.3232627 -4.3265061 -4.32928 -4.3304529 -4.330936 -4.3280005 -4.3239741 -4.32217]]...]
INFO - root - 2017-12-07 16:45:48.303896: step 27910, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 79h:58m:04s remains)
INFO - root - 2017-12-07 16:45:58.009726: step 27920, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 83h:48m:34s remains)
INFO - root - 2017-12-07 16:46:07.697900: step 27930, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.040 sec/batch; 88h:00m:59s remains)
INFO - root - 2017-12-07 16:46:17.231077: step 27940, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 77h:12m:10s remains)
INFO - root - 2017-12-07 16:46:26.814205: step 27950, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 81h:08m:42s remains)
INFO - root - 2017-12-07 16:46:36.289549: step 27960, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 84h:21m:15s remains)
INFO - root - 2017-12-07 16:46:45.931180: step 27970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 77h:15m:46s remains)
INFO - root - 2017-12-07 16:46:55.577278: step 27980, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 83h:52m:48s remains)
INFO - root - 2017-12-07 16:47:05.067078: step 27990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 71h:21m:49s remains)
INFO - root - 2017-12-07 16:47:14.862741: step 28000, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 82h:05m:52s remains)
2017-12-07 16:47:15.845658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.36654 -4.366343 -4.3620181 -4.3586531 -4.3592963 -4.3579211 -4.3518744 -4.3441486 -4.3400054 -4.3409534 -4.3429012 -4.3445082 -4.3429074 -4.3385367 -4.3352513][-4.3824053 -4.3846498 -4.382771 -4.377934 -4.3739114 -4.3655643 -4.3518152 -4.3367982 -4.326848 -4.3258181 -4.328712 -4.3327522 -4.3314152 -4.3277216 -4.325985][-4.3837624 -4.3882103 -4.3889313 -4.3828983 -4.3749065 -4.3616524 -4.3430309 -4.3239093 -4.311336 -4.3092446 -4.3108788 -4.3156533 -4.3167048 -4.3148451 -4.3147326][-4.3674378 -4.3718882 -4.3732433 -4.3651094 -4.3536015 -4.3357368 -4.3134685 -4.2920666 -4.2831826 -4.2873092 -4.2907391 -4.296288 -4.3006034 -4.302146 -4.3045826][-4.33463 -4.3392367 -4.3380213 -4.3278866 -4.31373 -4.2907863 -4.2618475 -4.23399 -4.2276788 -4.2431073 -4.2567716 -4.2695212 -4.28046 -4.28747 -4.2947736][-4.2843795 -4.2898769 -4.285583 -4.2731133 -4.2548332 -4.2241025 -4.1826215 -4.145597 -4.1465015 -4.1829581 -4.2154541 -4.2372251 -4.2551112 -4.270021 -4.2848592][-4.2219605 -4.2251987 -4.216444 -4.2011971 -4.1758485 -4.1293516 -4.0632386 -4.0059981 -4.0173569 -4.0890212 -4.1516714 -4.1877246 -4.215488 -4.2432938 -4.2707825][-4.1630406 -4.16603 -4.1570172 -4.142735 -4.1167054 -4.0602832 -3.9700723 -3.8897221 -3.90646 -4.00812 -4.1000261 -4.1526651 -4.1908789 -4.2279778 -4.2632928][-4.1359739 -4.1457453 -4.1505942 -4.1516438 -4.1415691 -4.1031389 -4.0305743 -3.9632485 -3.9736657 -4.0580788 -4.1378579 -4.1847124 -4.2174897 -4.2470679 -4.273859][-4.1601949 -4.1746516 -4.1910748 -4.204998 -4.2118444 -4.19918 -4.1593165 -4.1176019 -4.1201453 -4.170619 -4.2214837 -4.2504196 -4.2696366 -4.2838063 -4.2948704][-4.2193055 -4.2275929 -4.239018 -4.2508655 -4.2612934 -4.2596579 -4.2377629 -4.213685 -4.2142243 -4.2444992 -4.2769246 -4.2953086 -4.3052 -4.3086429 -4.3091106][-4.2727008 -4.27376 -4.2771 -4.2834716 -4.2882824 -4.282701 -4.2646003 -4.2521529 -4.2571149 -4.2802796 -4.3038588 -4.3181539 -4.3227649 -4.3194847 -4.31361][-4.3106189 -4.30965 -4.3094878 -4.3110104 -4.3073287 -4.294805 -4.27831 -4.2740226 -4.2839437 -4.3039684 -4.3230653 -4.3332291 -4.3321924 -4.3224831 -4.3129215][-4.3338714 -4.3323946 -4.328938 -4.3252735 -4.3173938 -4.3037667 -4.2910323 -4.2920661 -4.3060942 -4.3250017 -4.3379383 -4.3411098 -4.3356609 -4.3212838 -4.3088021][-4.3448315 -4.3424544 -4.3357072 -4.3274508 -4.3175311 -4.3042927 -4.2938228 -4.2967548 -4.313302 -4.3322759 -4.3413925 -4.3403058 -4.3327484 -4.3164077 -4.3035679]]...]
INFO - root - 2017-12-07 16:47:25.372989: step 28010, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 77h:34m:49s remains)
INFO - root - 2017-12-07 16:47:34.901550: step 28020, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 78h:32m:47s remains)
INFO - root - 2017-12-07 16:47:44.668573: step 28030, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 77h:28m:09s remains)
INFO - root - 2017-12-07 16:47:54.367210: step 28040, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 80h:31m:49s remains)
INFO - root - 2017-12-07 16:48:04.039018: step 28050, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.022 sec/batch; 86h:26m:18s remains)
INFO - root - 2017-12-07 16:48:13.536556: step 28060, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.981 sec/batch; 82h:57m:14s remains)
INFO - root - 2017-12-07 16:48:23.069476: step 28070, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 84h:07m:06s remains)
INFO - root - 2017-12-07 16:48:32.585094: step 28080, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 78h:58m:26s remains)
INFO - root - 2017-12-07 16:48:42.162140: step 28090, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 82h:58m:19s remains)
INFO - root - 2017-12-07 16:48:51.661596: step 28100, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 81h:15m:16s remains)
2017-12-07 16:48:52.556770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2935214 -4.2880206 -4.2923465 -4.2985563 -4.3024659 -4.3027368 -4.2998924 -4.2978811 -4.2967644 -4.2960267 -4.2958727 -4.2958503 -4.2955341 -4.2957649 -4.2984114][-4.2756495 -4.2693186 -4.2750816 -4.2855229 -4.2919145 -4.2920117 -4.2871714 -4.28286 -4.2794361 -4.2765079 -4.2752142 -4.2756777 -4.2767472 -4.2781839 -4.2822766][-4.2605767 -4.2534456 -4.2592163 -4.2731156 -4.2819452 -4.2810059 -4.2743678 -4.2689619 -4.2645736 -4.2607007 -4.25911 -4.2609015 -4.2636142 -4.2657685 -4.2707348][-4.2525496 -4.2429442 -4.2467532 -4.262569 -4.2741051 -4.272068 -4.2629342 -4.254776 -4.24741 -4.2425532 -4.243134 -4.25041 -4.2572408 -4.2605462 -4.2656679][-4.242743 -4.2321491 -4.2327919 -4.2469664 -4.2593813 -4.255825 -4.2417679 -4.2243524 -4.2078233 -4.1991544 -4.2025867 -4.2191749 -4.2364469 -4.2456255 -4.2536483][-4.2186985 -4.2105827 -4.2117691 -4.2235365 -4.2346678 -4.2306447 -4.2109342 -4.1798496 -4.1469955 -4.1283064 -4.1297579 -4.1543026 -4.1845217 -4.2043853 -4.2207575][-4.18672 -4.182013 -4.1868949 -4.19791 -4.2085781 -4.20642 -4.1857214 -4.1462197 -4.1029205 -4.0733662 -4.0668025 -4.0907397 -4.127111 -4.1544662 -4.1811533][-4.154171 -4.1466126 -4.1537323 -4.1671958 -4.1810079 -4.1824689 -4.1664133 -4.131804 -4.092402 -4.0623646 -4.053133 -4.0718207 -4.1015649 -4.1233764 -4.1495972][-4.1378827 -4.1260123 -4.1322651 -4.1487088 -4.1690388 -4.1760416 -4.1635695 -4.134573 -4.1031141 -4.0805764 -4.0747776 -4.0903859 -4.1128178 -4.1271343 -4.1438761][-4.1450229 -4.12913 -4.1330471 -4.1498594 -4.1720495 -4.1817532 -4.1691771 -4.1398449 -4.1105609 -4.09346 -4.0942245 -4.1125469 -4.1342859 -4.1485591 -4.1624117][-4.1633735 -4.146059 -4.148665 -4.1657524 -4.1846809 -4.1927662 -4.1795082 -4.1503487 -4.1221118 -4.1058364 -4.1096559 -4.1307454 -4.1534948 -4.1701484 -4.18674][-4.1759405 -4.15697 -4.1597962 -4.1814394 -4.201426 -4.2102847 -4.1996813 -4.1735268 -4.1485095 -4.12999 -4.1331954 -4.1531806 -4.1753583 -4.1921954 -4.2097955][-4.1840944 -4.1621008 -4.164134 -4.1907535 -4.2147365 -4.2306485 -4.2287035 -4.2082243 -4.1834354 -4.1621575 -4.1629543 -4.179059 -4.1964769 -4.2104969 -4.2253814][-4.1926908 -4.1694121 -4.1663022 -4.1913056 -4.2151003 -4.2379313 -4.2473469 -4.2348504 -4.2123404 -4.1892438 -4.186172 -4.1964455 -4.2070117 -4.2177095 -4.2298279][-4.1995497 -4.180799 -4.1767678 -4.1948571 -4.2120676 -4.232904 -4.2475381 -4.2420659 -4.2254186 -4.2048922 -4.1969166 -4.1995449 -4.2049594 -4.2128897 -4.2238836]]...]
INFO - root - 2017-12-07 16:49:02.330271: step 28110, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 76h:37m:17s remains)
INFO - root - 2017-12-07 16:49:11.918034: step 28120, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 80h:49m:51s remains)
INFO - root - 2017-12-07 16:49:21.546999: step 28130, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.994 sec/batch; 84h:00m:45s remains)
INFO - root - 2017-12-07 16:49:31.204619: step 28140, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 77h:39m:47s remains)
INFO - root - 2017-12-07 16:49:40.832708: step 28150, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 78h:41m:00s remains)
INFO - root - 2017-12-07 16:49:50.240318: step 28160, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 85h:32m:23s remains)
INFO - root - 2017-12-07 16:49:59.814381: step 28170, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 80h:30m:39s remains)
INFO - root - 2017-12-07 16:50:09.562684: step 28180, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 83h:49m:56s remains)
INFO - root - 2017-12-07 16:50:19.260483: step 28190, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 79h:33m:26s remains)
INFO - root - 2017-12-07 16:50:28.908048: step 28200, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 80h:39m:09s remains)
2017-12-07 16:50:29.952842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2959957 -4.2915277 -4.2877073 -4.283988 -4.2715974 -4.2539458 -4.2457323 -4.2387748 -4.2272353 -4.2284131 -4.2378845 -4.2413688 -4.2425709 -4.2424822 -4.2377777][-4.2571621 -4.2539115 -4.2558007 -4.2598844 -4.2527013 -4.2335925 -4.2216024 -4.2120566 -4.2012477 -4.2087932 -4.22308 -4.2300344 -4.231719 -4.2317319 -4.2295961][-4.2200522 -4.2207894 -4.23043 -4.2445955 -4.24632 -4.2267323 -4.2095737 -4.1917238 -4.1749249 -4.18646 -4.2067208 -4.2179022 -4.2167139 -4.214355 -4.2181206][-4.1886148 -4.1920919 -4.2044859 -4.2205839 -4.2243152 -4.2066479 -4.1864386 -4.159339 -4.1298785 -4.1398745 -4.1667137 -4.1846681 -4.1871767 -4.1850724 -4.1973763][-4.1755991 -4.180007 -4.1894822 -4.1958637 -4.1862612 -4.1598229 -4.1378231 -4.1092224 -4.0767012 -4.0795197 -4.1039476 -4.1293697 -4.1468906 -4.1551676 -4.1794162][-4.1695013 -4.1744308 -4.1820393 -4.1790233 -4.1506071 -4.1011868 -4.0664597 -4.0417328 -4.0283146 -4.0445604 -4.0677495 -4.0927362 -4.1209974 -4.1405625 -4.1666965][-4.1621938 -4.1639204 -4.1695023 -4.1674771 -4.1336737 -4.0625119 -3.9925067 -3.950351 -3.9620852 -4.024404 -4.0706978 -4.0960746 -4.1231275 -4.1407337 -4.1633415][-4.1494679 -4.149066 -4.157 -4.1597381 -4.131732 -4.06125 -3.9656818 -3.8824396 -3.8980615 -4.0003495 -4.0780344 -4.110599 -4.128849 -4.1364679 -4.1545358][-4.14114 -4.1423507 -4.1477885 -4.1468825 -4.1212611 -4.0650268 -3.9837551 -3.9083033 -3.91959 -4.012598 -4.0829983 -4.1120896 -4.1267319 -4.1296697 -4.1454644][-4.1310172 -4.1402373 -4.1495647 -4.1404943 -4.1069832 -4.0642576 -4.0176315 -3.9871712 -4.0106759 -4.07063 -4.1114535 -4.1311407 -4.1417546 -4.1411085 -4.1568942][-4.1186609 -4.1397691 -4.16273 -4.1590738 -4.1275873 -4.0908356 -4.0656013 -4.066112 -4.0996637 -4.1429396 -4.1647291 -4.1754169 -4.1816959 -4.1788511 -4.1913524][-4.1349406 -4.1593447 -4.19177 -4.2020655 -4.1854072 -4.1576214 -4.1359258 -4.1417494 -4.1775208 -4.2146721 -4.2276921 -4.2306223 -4.2315893 -4.226748 -4.233674][-4.1868262 -4.2081819 -4.2397666 -4.2561431 -4.2525425 -4.2353234 -4.21747 -4.2184629 -4.2425423 -4.2688284 -4.277801 -4.2759509 -4.2722435 -4.2672057 -4.2710686][-4.2521791 -4.2668457 -4.2893786 -4.3040895 -4.3047647 -4.2950506 -4.2820797 -4.27822 -4.2879252 -4.3015366 -4.3066692 -4.3061891 -4.3045316 -4.3021746 -4.305779][-4.3026776 -4.3110147 -4.3224716 -4.3305 -4.3325338 -4.3285866 -4.3209972 -4.316102 -4.3180957 -4.3234286 -4.3267694 -4.3292894 -4.3308792 -4.3308945 -4.3336196]]...]
INFO - root - 2017-12-07 16:50:39.646809: step 28210, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 81h:37m:36s remains)
INFO - root - 2017-12-07 16:50:49.227889: step 28220, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 82h:13m:03s remains)
INFO - root - 2017-12-07 16:50:58.838234: step 28230, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 79h:12m:39s remains)
INFO - root - 2017-12-07 16:51:08.559400: step 28240, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 80h:39m:43s remains)
INFO - root - 2017-12-07 16:51:18.102398: step 28250, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 82h:02m:57s remains)
INFO - root - 2017-12-07 16:51:27.801050: step 28260, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 83h:17m:20s remains)
INFO - root - 2017-12-07 16:51:37.211095: step 28270, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 80h:14m:20s remains)
INFO - root - 2017-12-07 16:51:46.865772: step 28280, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 85h:21m:09s remains)
INFO - root - 2017-12-07 16:51:56.591675: step 28290, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 83h:34m:16s remains)
INFO - root - 2017-12-07 16:52:06.167518: step 28300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 79h:43m:59s remains)
2017-12-07 16:52:07.123161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3459983 -4.36257 -4.3699331 -4.3670931 -4.3542705 -4.3240376 -4.2802086 -4.2425475 -4.226779 -4.227057 -4.2380366 -4.25601 -4.2635555 -4.2445059 -4.224472][-4.3504252 -4.3635063 -4.3675079 -4.3607063 -4.3425412 -4.3043437 -4.2532859 -4.2132459 -4.1947584 -4.198348 -4.2229271 -4.2503743 -4.2594867 -4.2429705 -4.2248235][-4.3541861 -4.3626337 -4.3595524 -4.3424 -4.3149467 -4.271441 -4.2170987 -4.1739206 -4.150424 -4.1622629 -4.2004809 -4.2333112 -4.2453718 -4.2357621 -4.223906][-4.3538728 -4.35713 -4.3445148 -4.3183413 -4.2837434 -4.2355661 -4.1797075 -4.1331444 -4.1099 -4.1401439 -4.1883821 -4.2185059 -4.2292509 -4.2237749 -4.2167974][-4.3518023 -4.3513827 -4.33411 -4.3040781 -4.2635612 -4.2095003 -4.1459289 -4.09114 -4.07586 -4.1284237 -4.1822619 -4.2161355 -4.2338843 -4.2307363 -4.2251234][-4.3508482 -4.3481522 -4.328999 -4.294199 -4.24665 -4.1843939 -4.1088967 -4.0390387 -4.0318174 -4.1062446 -4.1727796 -4.2211146 -4.2512741 -4.2540817 -4.248179][-4.3435841 -4.33982 -4.3199692 -4.2812395 -4.2212429 -4.1434579 -4.0436912 -3.9482541 -3.9522278 -4.0630097 -4.1583157 -4.227077 -4.2721176 -4.2783051 -4.2758245][-4.3350248 -4.3312154 -4.3075242 -4.2589889 -4.1817131 -4.0822468 -3.9533389 -3.8349497 -3.8661246 -4.0213962 -4.1397524 -4.2226853 -4.2769742 -4.2926221 -4.2966185][-4.3269649 -4.3221345 -4.2931004 -4.2336082 -4.1474714 -4.0439405 -3.9173181 -3.8227823 -3.8786657 -4.0263114 -4.1360731 -4.2128482 -4.2668056 -4.2915792 -4.30829][-4.3183084 -4.3154411 -4.2862182 -4.224771 -4.1459684 -4.057704 -3.9563723 -3.8991227 -3.9577646 -4.0681672 -4.1523376 -4.2154403 -4.2604156 -4.2862115 -4.3119736][-4.316545 -4.316009 -4.2944951 -4.2441196 -4.1815619 -4.1135449 -4.0366154 -4.0043497 -4.05498 -4.1290355 -4.1898 -4.239202 -4.271924 -4.2907748 -4.3133597][-4.3176117 -4.3201694 -4.3058476 -4.2686405 -4.22238 -4.1759834 -4.1257181 -4.1115918 -4.1474915 -4.1918511 -4.23162 -4.2650118 -4.2875495 -4.3008547 -4.3134217][-4.3169203 -4.3176751 -4.3069215 -4.2810936 -4.2510862 -4.2253842 -4.1995416 -4.1965823 -4.2184815 -4.2394295 -4.257812 -4.2763691 -4.2910862 -4.3016138 -4.3072691][-4.3144932 -4.3121357 -4.3035955 -4.2875218 -4.2705126 -4.2605982 -4.2517242 -4.2531147 -4.262373 -4.2674704 -4.271739 -4.2803311 -4.2883911 -4.2954021 -4.2985768][-4.31391 -4.3089504 -4.3010149 -4.2900214 -4.2792153 -4.2761073 -4.27542 -4.2770829 -4.27881 -4.2797775 -4.2816119 -4.2863927 -4.2904859 -4.2948208 -4.2975979]]...]
INFO - root - 2017-12-07 16:52:16.784749: step 28310, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 81h:38m:48s remains)
INFO - root - 2017-12-07 16:52:26.296005: step 28320, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 79h:30m:52s remains)
INFO - root - 2017-12-07 16:52:36.006625: step 28330, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.966 sec/batch; 81h:35m:01s remains)
INFO - root - 2017-12-07 16:52:45.470664: step 28340, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 80h:21m:35s remains)
INFO - root - 2017-12-07 16:52:54.861024: step 28350, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 82h:37m:26s remains)
INFO - root - 2017-12-07 16:53:04.312896: step 28360, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 81h:05m:25s remains)
INFO - root - 2017-12-07 16:53:13.795054: step 28370, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 81h:42m:11s remains)
INFO - root - 2017-12-07 16:53:23.461563: step 28380, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 84h:42m:58s remains)
INFO - root - 2017-12-07 16:53:33.149531: step 28390, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 80h:46m:31s remains)
INFO - root - 2017-12-07 16:53:42.838588: step 28400, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 79h:50m:30s remains)
2017-12-07 16:53:43.861769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3418007 -4.3371096 -4.3330784 -4.3284478 -4.3235435 -4.3190107 -4.3121381 -4.3059316 -4.3025188 -4.2967839 -4.2916484 -4.2960243 -4.3058176 -4.3206625 -4.3346448][-4.3302231 -4.3261261 -4.3270593 -4.3263392 -4.3225632 -4.317533 -4.3068709 -4.2993417 -4.2958937 -4.2892818 -4.2833786 -4.289897 -4.3000717 -4.3152294 -4.3316436][-4.3119321 -4.3058763 -4.3095717 -4.3136969 -4.3095679 -4.3000727 -4.2826538 -4.2743292 -4.2748923 -4.26987 -4.268374 -4.2807045 -4.2952065 -4.3109541 -4.3285303][-4.3004136 -4.2940617 -4.2982769 -4.3026242 -4.2929249 -4.2685266 -4.2358689 -4.22251 -4.2295861 -4.2342916 -4.2436767 -4.2657819 -4.2901297 -4.3091869 -4.3251729][-4.2932796 -4.2832084 -4.2821708 -4.2777076 -4.2555981 -4.2072783 -4.1459703 -4.1155629 -4.132339 -4.1593747 -4.1884527 -4.2296524 -4.2697239 -4.2975636 -4.3161678][-4.283175 -4.2621942 -4.2464676 -4.2256565 -4.1842012 -4.1101894 -4.014832 -3.9527216 -3.9842215 -4.052031 -4.1141949 -4.177835 -4.2335525 -4.2724476 -4.2981529][-4.2744851 -4.2360487 -4.2000394 -4.159215 -4.0985069 -4.0012584 -3.8754945 -3.7745788 -3.817518 -3.9307141 -4.0304337 -4.1196432 -4.192245 -4.242372 -4.276926][-4.2777929 -4.2376418 -4.1963978 -4.1503549 -4.0911021 -4.0080404 -3.904407 -3.8104088 -3.8348272 -3.935462 -4.0305662 -4.1198816 -4.1903439 -4.2362804 -4.2683883][-4.28852 -4.2561746 -4.2260108 -4.1922393 -4.1516132 -4.0972395 -4.0344863 -3.9756277 -3.9810722 -4.0431795 -4.10815 -4.17477 -4.2247691 -4.255693 -4.2777472][-4.2956805 -4.2728958 -4.2569904 -4.2402687 -4.2181883 -4.1864934 -4.1518664 -4.1199942 -4.1190152 -4.1521544 -4.1900535 -4.2354674 -4.2650414 -4.2808323 -4.2918735][-4.2963967 -4.2819362 -4.2769823 -4.2723541 -4.2609134 -4.2415547 -4.222599 -4.2085733 -4.210259 -4.2255812 -4.241293 -4.2711487 -4.2918377 -4.3016996 -4.3073759][-4.2944961 -4.2832556 -4.2817335 -4.2774429 -4.2702513 -4.2584085 -4.2459836 -4.2376585 -4.2423334 -4.2512302 -4.2582321 -4.2806621 -4.3002453 -4.3096185 -4.3170676][-4.299438 -4.2901192 -4.2874808 -4.2793536 -4.2706265 -4.2625289 -4.255764 -4.2523913 -4.2579861 -4.264513 -4.2698646 -4.2882481 -4.3071232 -4.31876 -4.3285604][-4.3064337 -4.2970104 -4.2930479 -4.2848959 -4.2781076 -4.2724767 -4.2688484 -4.2681355 -4.2718849 -4.2763033 -4.2814484 -4.2981935 -4.31593 -4.329062 -4.3386221][-4.3176379 -4.3089776 -4.304244 -4.2997041 -4.2969155 -4.2943025 -4.292809 -4.2923288 -4.2930179 -4.2949262 -4.299449 -4.3115582 -4.3238716 -4.3338609 -4.3411493]]...]
INFO - root - 2017-12-07 16:53:53.393384: step 28410, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 74h:54m:59s remains)
INFO - root - 2017-12-07 16:54:03.076722: step 28420, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 84h:18m:05s remains)
INFO - root - 2017-12-07 16:54:12.731921: step 28430, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 83h:12m:25s remains)
INFO - root - 2017-12-07 16:54:22.452461: step 28440, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 85h:00m:58s remains)
INFO - root - 2017-12-07 16:54:32.033814: step 28450, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 77h:29m:01s remains)
INFO - root - 2017-12-07 16:54:41.793227: step 28460, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 79h:50m:30s remains)
INFO - root - 2017-12-07 16:54:51.213904: step 28470, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 80h:28m:48s remains)
INFO - root - 2017-12-07 16:55:00.880696: step 28480, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.976 sec/batch; 82h:23m:10s remains)
INFO - root - 2017-12-07 16:55:10.523825: step 28490, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 80h:35m:28s remains)
INFO - root - 2017-12-07 16:55:20.002806: step 28500, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 79h:41m:05s remains)
2017-12-07 16:55:20.904759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3063722 -4.2982497 -4.2978206 -4.3037677 -4.3098555 -4.3109922 -4.3080225 -4.3027034 -4.2925239 -4.2803454 -4.2806144 -4.2851944 -4.2820191 -4.2813354 -4.2984734][-4.3043528 -4.2926493 -4.2894759 -4.2929745 -4.2963495 -4.2937031 -4.2857018 -4.2774467 -4.2671118 -4.2539983 -4.257237 -4.2684417 -4.2701187 -4.2704282 -4.2886138][-4.2976336 -4.2833486 -4.2796364 -4.28289 -4.2836 -4.278131 -4.2652445 -4.2556071 -4.246592 -4.2345333 -4.2373586 -4.254528 -4.2639256 -4.2680221 -4.2861805][-4.291502 -4.2746587 -4.26595 -4.2671294 -4.2621517 -4.2489886 -4.229538 -4.2211471 -4.2183871 -4.2096348 -4.20984 -4.227551 -4.2466755 -4.260891 -4.2820873][-4.2769246 -4.2558603 -4.240417 -4.2409844 -4.2304692 -4.20428 -4.1742978 -4.170248 -4.178874 -4.1716557 -4.1683373 -4.1839814 -4.2078638 -4.2340217 -4.2648177][-4.249073 -4.2225413 -4.2013674 -4.1950917 -4.1766434 -4.1321297 -4.0857081 -4.0856938 -4.1057739 -4.1038265 -4.1004443 -4.11721 -4.147954 -4.1916018 -4.2378411][-4.2153826 -4.1793389 -4.148169 -4.1270757 -4.0900936 -4.0205927 -3.9559178 -3.9596786 -3.9943316 -4.0103135 -4.0264187 -4.0677967 -4.1206841 -4.17841 -4.2306061][-4.1900663 -4.1442409 -4.1006641 -4.0589828 -3.9974835 -3.9123917 -3.84699 -3.8646355 -3.9271028 -3.9836488 -4.0338507 -4.0953584 -4.1585288 -4.2151861 -4.2597213][-4.1876268 -4.1380763 -4.0950356 -4.0589352 -4.0073175 -3.938659 -3.8933902 -3.92471 -3.9955273 -4.0648823 -4.1179547 -4.1711507 -4.2252588 -4.270184 -4.3008933][-4.2219548 -4.1850872 -4.1554918 -4.1354442 -4.101645 -4.058639 -4.0337896 -4.0663342 -4.1257572 -4.1828728 -4.22327 -4.2576418 -4.2937412 -4.3223681 -4.3377414][-4.2607574 -4.2446828 -4.2346182 -4.224247 -4.2018027 -4.1776042 -4.1664 -4.1916485 -4.2321024 -4.2686925 -4.2938924 -4.3124981 -4.332767 -4.3469954 -4.3534136][-4.2795248 -4.2738285 -4.2778525 -4.2784548 -4.2674618 -4.2561255 -4.2520318 -4.2664351 -4.2893033 -4.3103333 -4.3247657 -4.3353405 -4.3473377 -4.3541441 -4.3552947][-4.2909985 -4.2866068 -4.2964845 -4.3071218 -4.3074589 -4.3040476 -4.3042431 -4.3111739 -4.3226986 -4.333662 -4.34054 -4.3445034 -4.349915 -4.3532953 -4.3536787][-4.3089223 -4.3049517 -4.3133845 -4.3251553 -4.331336 -4.333364 -4.3348708 -4.3371096 -4.3410044 -4.3440728 -4.3446903 -4.3447218 -4.3466182 -4.3498435 -4.3521829][-4.3294196 -4.3256569 -4.32915 -4.33663 -4.3428216 -4.345809 -4.3469138 -4.3468847 -4.3469052 -4.3464808 -4.3464971 -4.3476133 -4.3500404 -4.3531017 -4.3560424]]...]
INFO - root - 2017-12-07 16:55:30.668634: step 28510, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.992 sec/batch; 83h:46m:16s remains)
INFO - root - 2017-12-07 16:55:40.261946: step 28520, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 75h:09m:06s remains)
INFO - root - 2017-12-07 16:55:49.868820: step 28530, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 80h:06m:08s remains)
INFO - root - 2017-12-07 16:55:59.505025: step 28540, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 80h:48m:43s remains)
INFO - root - 2017-12-07 16:56:09.176598: step 28550, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 81h:28m:17s remains)
INFO - root - 2017-12-07 16:56:18.761918: step 28560, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 78h:48m:07s remains)
INFO - root - 2017-12-07 16:56:28.417032: step 28570, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 82h:21m:33s remains)
INFO - root - 2017-12-07 16:56:37.983809: step 28580, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 78h:31m:08s remains)
INFO - root - 2017-12-07 16:56:47.475066: step 28590, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 78h:25m:16s remains)
INFO - root - 2017-12-07 16:56:57.067697: step 28600, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 81h:38m:48s remains)
2017-12-07 16:56:58.105985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2596869 -4.2840905 -4.2862678 -4.266139 -4.2357283 -4.1853 -4.1321974 -4.0969667 -4.1077738 -4.1384583 -4.1851373 -4.237361 -4.2629924 -4.2704005 -4.2690392][-4.2537489 -4.2802367 -4.2817435 -4.2595439 -4.2271614 -4.1795096 -4.1252756 -4.084197 -4.0884924 -4.12246 -4.1806774 -4.2358055 -4.2602496 -4.2639942 -4.2596984][-4.240891 -4.2712111 -4.2768989 -4.2594504 -4.2250819 -4.1692953 -4.1068006 -4.0614657 -4.0604744 -4.1038923 -4.175138 -4.2368503 -4.2642827 -4.2637539 -4.2528605][-4.2379279 -4.2688684 -4.2784472 -4.266336 -4.2289305 -4.1589065 -4.0847011 -4.0337915 -4.0306849 -4.0823979 -4.1645055 -4.2323084 -4.2628222 -4.2589278 -4.2441387][-4.2503915 -4.2766285 -4.2867842 -4.2776318 -4.2360649 -4.1548018 -4.0642982 -3.9951768 -3.9853 -4.0482626 -4.1440835 -4.2181005 -4.2507977 -4.247376 -4.2336011][-4.2602048 -4.282042 -4.2951326 -4.2888484 -4.2453394 -4.1545248 -4.0417428 -3.9441183 -3.9237437 -4.0045133 -4.1172733 -4.1979985 -4.2346311 -4.2356462 -4.22395][-4.2691445 -4.2896824 -4.3059621 -4.3042941 -4.2615891 -4.1652513 -4.0330887 -3.9089167 -3.8838286 -3.9807582 -4.1050954 -4.1858454 -4.2193918 -4.223217 -4.2114048][-4.2855239 -4.306633 -4.3263335 -4.3295374 -4.2890587 -4.1913862 -4.0551691 -3.9266229 -3.8995898 -3.9938207 -4.1107678 -4.1808782 -4.2067056 -4.2109814 -4.1983232][-4.2967863 -4.3187308 -4.3397212 -4.34837 -4.3123865 -4.2175493 -4.0903859 -3.9742689 -3.9384916 -4.0079083 -4.1100249 -4.1734118 -4.1955781 -4.2002048 -4.1906939][-4.2917533 -4.3148441 -4.3347745 -4.3486428 -4.3206005 -4.2387419 -4.1316223 -4.0343313 -3.9923174 -4.0326715 -4.1175866 -4.1764665 -4.1996422 -4.20763 -4.2000337][-4.2687349 -4.2949224 -4.3144083 -4.3339109 -4.3189664 -4.2579622 -4.1699214 -4.0833492 -4.0376687 -4.0608611 -4.1301422 -4.1820774 -4.2076607 -4.2164731 -4.2101626][-4.238626 -4.2750149 -4.2986593 -4.3222184 -4.3174 -4.2716703 -4.1986265 -4.1252589 -4.0826473 -4.0983658 -4.1478739 -4.1894884 -4.217248 -4.2249823 -4.2187452][-4.2140613 -4.2615085 -4.293458 -4.32055 -4.3235717 -4.2908764 -4.2338209 -4.1785612 -4.1452022 -4.1518154 -4.1813869 -4.2108126 -4.234962 -4.2411041 -4.2397423][-4.19172 -4.2486739 -4.2936568 -4.3231788 -4.33112 -4.3104172 -4.2671638 -4.224195 -4.1977439 -4.1973734 -4.2129984 -4.2346692 -4.2573438 -4.26744 -4.2736855][-4.1701841 -4.2345791 -4.2907372 -4.3245168 -4.3374724 -4.3273349 -4.2955775 -4.2604241 -4.2361879 -4.2315168 -4.2416143 -4.2605639 -4.2831378 -4.2983432 -4.3115411]]...]
INFO - root - 2017-12-07 16:57:07.718748: step 28610, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 81h:59m:47s remains)
INFO - root - 2017-12-07 16:57:17.205028: step 28620, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.960 sec/batch; 81h:01m:47s remains)
INFO - root - 2017-12-07 16:57:26.840754: step 28630, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 80h:25m:24s remains)
INFO - root - 2017-12-07 16:57:36.446561: step 28640, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 83h:22m:01s remains)
INFO - root - 2017-12-07 16:57:46.039606: step 28650, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 80h:14m:23s remains)
INFO - root - 2017-12-07 16:57:55.654259: step 28660, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 80h:50m:02s remains)
INFO - root - 2017-12-07 16:58:05.140245: step 28670, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 79h:32m:31s remains)
INFO - root - 2017-12-07 16:58:14.913769: step 28680, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 79h:16m:13s remains)
INFO - root - 2017-12-07 16:58:24.518237: step 28690, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.994 sec/batch; 83h:50m:52s remains)
INFO - root - 2017-12-07 16:58:34.174854: step 28700, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 81h:03m:25s remains)
2017-12-07 16:58:35.233180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3261638 -4.3292875 -4.3302989 -4.3281684 -4.3216958 -4.3127813 -4.3075566 -4.3085356 -4.3135457 -4.3180256 -4.3201971 -4.3200374 -4.3197432 -4.3207827 -4.3220315][-4.3241811 -4.3226776 -4.3171439 -4.3065042 -4.291461 -4.2779903 -4.27104 -4.2755203 -4.2874479 -4.2980819 -4.3050356 -4.307539 -4.3092136 -4.31185 -4.314168][-4.3142619 -4.3052597 -4.2890973 -4.2647009 -4.2397146 -4.2210045 -4.2113256 -4.2171192 -4.234149 -4.2517185 -4.2626047 -4.2686715 -4.2765336 -4.2843943 -4.2911553][-4.2972879 -4.2803831 -4.2517309 -4.2134051 -4.1811085 -4.1584997 -4.1430845 -4.1441741 -4.1614156 -4.1857796 -4.2001152 -4.207459 -4.2232628 -4.2401533 -4.256423][-4.277 -4.2542419 -4.2174335 -4.1680846 -4.130548 -4.10624 -4.0819759 -4.0692129 -4.0827465 -4.1146231 -4.1324425 -4.1402225 -4.1631756 -4.19154 -4.2195439][-4.2566891 -4.2319064 -4.192862 -4.1336656 -4.0883722 -4.057632 -4.0160732 -3.9764173 -3.9882865 -4.0392547 -4.0673628 -4.0785122 -4.1097016 -4.1522918 -4.191925][-4.2409649 -4.2165809 -4.1784682 -4.1144156 -4.0587721 -4.0155849 -3.9482024 -3.8686333 -3.8810685 -3.9644842 -4.0123811 -4.0368891 -4.0793414 -4.1345687 -4.1847615][-4.23317 -4.2086782 -4.173564 -4.1132197 -4.0516186 -3.9978542 -3.9059443 -3.7834473 -3.7910118 -3.90674 -3.9784079 -4.0241575 -4.081615 -4.146275 -4.2015996][-4.2370596 -4.2104049 -4.1786146 -4.1268258 -4.0681176 -4.0171576 -3.9301522 -3.8076053 -3.8036034 -3.9151082 -3.9963841 -4.0560865 -4.119072 -4.1827087 -4.232399][-4.2519617 -4.2237229 -4.191884 -4.146472 -4.0945907 -4.0562997 -3.997406 -3.9175141 -3.9144602 -3.993628 -4.0670552 -4.1280837 -4.1823721 -4.2304668 -4.2651749][-4.2694221 -4.2419724 -4.2089057 -4.1656685 -4.117548 -4.0897141 -4.0587091 -4.0230432 -4.0314026 -4.086369 -4.1474185 -4.1992259 -4.2384443 -4.2689061 -4.2883835][-4.2851357 -4.2625971 -4.2330389 -4.1957645 -4.1563282 -4.1355038 -4.1204634 -4.1122088 -4.1295457 -4.1686549 -4.2118478 -4.2480817 -4.2735572 -4.2911596 -4.3010821][-4.2971339 -4.2833023 -4.2625742 -4.2361441 -4.2116117 -4.1989622 -4.1894269 -4.1891551 -4.20505 -4.2298994 -4.25537 -4.2766142 -4.2918611 -4.3007612 -4.3062038][-4.3054385 -4.2997785 -4.2894344 -4.2745972 -4.262507 -4.2587643 -4.2532358 -4.2505684 -4.256731 -4.2703161 -4.2828884 -4.29287 -4.3002043 -4.3043051 -4.3083711][-4.3115716 -4.3102164 -4.3073125 -4.300818 -4.2958288 -4.2965021 -4.29407 -4.2888331 -4.2874107 -4.2926626 -4.2985072 -4.3023758 -4.3045578 -4.3065567 -4.3103347]]...]
INFO - root - 2017-12-07 16:58:44.938136: step 28710, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 80h:16m:34s remains)
INFO - root - 2017-12-07 16:58:54.648456: step 28720, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 84h:22m:32s remains)
INFO - root - 2017-12-07 16:59:04.289360: step 28730, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 80h:27m:36s remains)
INFO - root - 2017-12-07 16:59:13.873804: step 28740, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 81h:17m:23s remains)
INFO - root - 2017-12-07 16:59:23.560597: step 28750, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.010 sec/batch; 85h:13m:47s remains)
INFO - root - 2017-12-07 16:59:33.078659: step 28760, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 80h:14m:51s remains)
INFO - root - 2017-12-07 16:59:42.633639: step 28770, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 77h:25m:44s remains)
INFO - root - 2017-12-07 16:59:52.291997: step 28780, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 77h:31m:53s remains)
INFO - root - 2017-12-07 17:00:01.912257: step 28790, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 79h:17m:13s remains)
INFO - root - 2017-12-07 17:00:11.628007: step 28800, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.020 sec/batch; 86h:04m:15s remains)
2017-12-07 17:00:12.564128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2488575 -4.2401137 -4.241518 -4.2545786 -4.2685156 -4.2816286 -4.2897673 -4.2867303 -4.2685475 -4.2414994 -4.2306762 -4.2370114 -4.2342639 -4.2164879 -4.2024894][-4.2483506 -4.2401214 -4.2442431 -4.2615771 -4.2767949 -4.2877216 -4.293098 -4.2886205 -4.2688317 -4.2412534 -4.2293568 -4.2357316 -4.2326059 -4.2126551 -4.1935911][-4.2509832 -4.2460227 -4.2551465 -4.2739739 -4.28802 -4.2938409 -4.2926087 -4.286119 -4.2672749 -4.2386832 -4.228653 -4.2371845 -4.2387996 -4.2201285 -4.199091][-4.251194 -4.2495656 -4.2601819 -4.2754483 -4.28504 -4.2847395 -4.2775588 -4.2708106 -4.2572904 -4.23415 -4.2278814 -4.2398114 -4.2453389 -4.2255764 -4.2004671][-4.2538509 -4.2513733 -4.2586637 -4.2693124 -4.2722869 -4.2625895 -4.2462296 -4.2346234 -4.2240233 -4.211297 -4.2142348 -4.2359381 -4.2494226 -4.2289248 -4.1994338][-4.2612052 -4.2525434 -4.248363 -4.2498121 -4.2460103 -4.2279387 -4.2013259 -4.1835241 -4.1744637 -4.1707087 -4.1841669 -4.220469 -4.2472467 -4.2328625 -4.2049875][-4.2668638 -4.2524638 -4.23704 -4.2280488 -4.2156067 -4.188014 -4.151 -4.1284084 -4.1190896 -4.1223154 -4.1445727 -4.1944957 -4.2370825 -4.2388477 -4.2165327][-4.2587051 -4.2411623 -4.2247066 -4.2149758 -4.2015691 -4.1702375 -4.1236525 -4.0942845 -4.0814142 -4.083427 -4.1087427 -4.1677456 -4.2248993 -4.2410007 -4.225564][-4.2429895 -4.229248 -4.2203217 -4.2195187 -4.2148504 -4.1899161 -4.1451421 -4.1134305 -4.0981922 -4.0943084 -4.110642 -4.1632462 -4.2202778 -4.2406969 -4.2277384][-4.2271285 -4.2141156 -4.2133331 -4.2271233 -4.2366748 -4.2258773 -4.1957183 -4.1707497 -4.1553183 -4.1434565 -4.1424127 -4.1757069 -4.2174668 -4.2298207 -4.2112007][-4.2170625 -4.2040458 -4.2104664 -4.2309618 -4.24924 -4.2458162 -4.2262373 -4.2128019 -4.20303 -4.1877789 -4.1753054 -4.19104 -4.2142329 -4.2138963 -4.1871691][-4.2221861 -4.2063613 -4.211318 -4.2282028 -4.2426333 -4.236371 -4.2203026 -4.2146993 -4.2106619 -4.1972184 -4.1840539 -4.1923819 -4.2064457 -4.1997271 -4.1723876][-4.23018 -4.213273 -4.2160254 -4.2247076 -4.2276282 -4.2180386 -4.206862 -4.20503 -4.2044005 -4.1938438 -4.1841483 -4.1893578 -4.199707 -4.1918769 -4.1729522][-4.2509446 -4.23693 -4.2371688 -4.2365236 -4.2270136 -4.2107353 -4.2014451 -4.2010956 -4.2029781 -4.1956992 -4.1889358 -4.192946 -4.2003584 -4.1954069 -4.1857133][-4.2756405 -4.26832 -4.2712488 -4.2660422 -4.2530932 -4.2375178 -4.2298837 -4.22968 -4.2305951 -4.2246242 -4.2183471 -4.218627 -4.2207904 -4.2153072 -4.2109256]]...]
INFO - root - 2017-12-07 17:00:22.127138: step 28810, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 81h:50m:33s remains)
INFO - root - 2017-12-07 17:00:31.741411: step 28820, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 79h:56m:43s remains)
INFO - root - 2017-12-07 17:00:41.078028: step 28830, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 76h:36m:04s remains)
INFO - root - 2017-12-07 17:00:50.819732: step 28840, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 82h:21m:05s remains)
INFO - root - 2017-12-07 17:01:00.473206: step 28850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 79h:49m:50s remains)
INFO - root - 2017-12-07 17:01:10.035772: step 28860, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 82h:52m:23s remains)
INFO - root - 2017-12-07 17:01:19.800382: step 28870, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.025 sec/batch; 86h:26m:49s remains)
INFO - root - 2017-12-07 17:01:29.606151: step 28880, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 84h:29m:32s remains)
INFO - root - 2017-12-07 17:01:39.215402: step 28890, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 83h:55m:05s remains)
INFO - root - 2017-12-07 17:01:48.801142: step 28900, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 78h:20m:06s remains)
2017-12-07 17:01:49.687248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3553905 -4.3574209 -4.3574204 -4.3557653 -4.3530078 -4.3487515 -4.3441343 -4.3413887 -4.342586 -4.3471646 -4.3524828 -4.3570895 -4.3604317 -4.3616447 -4.36084][-4.3575058 -4.3602843 -4.3593197 -4.3542004 -4.3454995 -4.3338528 -4.3226395 -4.3153043 -4.3151174 -4.3222647 -4.3329339 -4.3451152 -4.3565369 -4.3632793 -4.3640208][-4.3591633 -4.3613248 -4.3561759 -4.3419142 -4.3198848 -4.2931404 -4.2687788 -4.2539711 -4.252521 -4.2654562 -4.2888284 -4.3165088 -4.3418865 -4.3572955 -4.3620973][-4.358674 -4.357367 -4.3426328 -4.3110862 -4.2637577 -4.2100425 -4.1640811 -4.13823 -4.1398273 -4.1684847 -4.2177367 -4.2721152 -4.3181062 -4.3457041 -4.3568377][-4.3515382 -4.3411345 -4.3090944 -4.2518997 -4.172565 -4.0877171 -4.0173507 -3.9788392 -3.9841723 -4.0337267 -4.1139855 -4.2000604 -4.2727313 -4.3205557 -4.3439713][-4.3399625 -4.3197036 -4.2706079 -4.189208 -4.0834126 -3.9783323 -3.8946848 -3.8553455 -3.8711603 -3.9418297 -4.0438023 -4.1482406 -4.2364845 -4.2971706 -4.3310661][-4.3360353 -4.314034 -4.260849 -4.1707983 -4.0575089 -3.9534883 -3.879462 -3.8520422 -3.8781929 -3.9575417 -4.0614119 -4.161448 -4.2423525 -4.297678 -4.330132][-4.3388357 -4.3229008 -4.2804165 -4.2051191 -4.1112561 -4.0307355 -3.9793625 -3.9623823 -3.9854465 -4.0528526 -4.1388311 -4.2166991 -4.2775126 -4.3173676 -4.3400016][-4.3434782 -4.3363237 -4.3109889 -4.2621174 -4.201263 -4.1504326 -4.1202159 -4.111783 -4.1266074 -4.1699357 -4.2254887 -4.2754993 -4.3132453 -4.3362226 -4.3487887][-4.33719 -4.339756 -4.332006 -4.3088942 -4.2785082 -4.2522845 -4.2373867 -4.2337732 -4.2409863 -4.2625637 -4.2897849 -4.3157797 -4.3357792 -4.3478942 -4.3541064][-4.3153453 -4.3278165 -4.3339972 -4.329299 -4.3194656 -4.3097348 -4.3044086 -4.303874 -4.3061967 -4.3150091 -4.3272147 -4.3398829 -4.3497028 -4.354166 -4.3550334][-4.2707906 -4.2939768 -4.3126674 -4.3240085 -4.3309813 -4.3326807 -4.3335876 -4.3347306 -4.3352757 -4.3392429 -4.3454781 -4.3513689 -4.354876 -4.3545275 -4.3530087][-4.2156515 -4.2439537 -4.2677617 -4.2891622 -4.3069224 -4.3180194 -4.3265753 -4.3320475 -4.33471 -4.339035 -4.3441782 -4.3476315 -4.3481717 -4.3465037 -4.3451643][-4.1790228 -4.1976752 -4.2162151 -4.2401814 -4.2638397 -4.28166 -4.2973676 -4.307992 -4.3140497 -4.3208385 -4.3266759 -4.3292036 -4.3288069 -4.3280678 -4.3295765][-4.2120366 -4.2113032 -4.2110572 -4.2206144 -4.234798 -4.2489448 -4.2654519 -4.2786446 -4.2864313 -4.2940311 -4.2982707 -4.2977324 -4.2952881 -4.2963305 -4.3027363]]...]
INFO - root - 2017-12-07 17:01:59.397245: step 28910, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 80h:19m:58s remains)
INFO - root - 2017-12-07 17:02:09.074763: step 28920, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 83h:52m:34s remains)
INFO - root - 2017-12-07 17:02:18.631676: step 28930, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 82h:01m:44s remains)
INFO - root - 2017-12-07 17:02:28.340996: step 28940, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 78h:12m:05s remains)
INFO - root - 2017-12-07 17:02:37.994370: step 28950, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 82h:58m:19s remains)
INFO - root - 2017-12-07 17:02:47.748674: step 28960, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 81h:01m:03s remains)
INFO - root - 2017-12-07 17:02:57.405032: step 28970, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 82h:27m:41s remains)
INFO - root - 2017-12-07 17:03:07.073638: step 28980, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 81h:55m:00s remains)
INFO - root - 2017-12-07 17:03:16.878122: step 28990, loss = 2.04, batch loss = 1.98 (7.9 examples/sec; 1.007 sec/batch; 84h:52m:19s remains)
INFO - root - 2017-12-07 17:03:26.336492: step 29000, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 80h:00m:36s remains)
2017-12-07 17:03:27.286291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3133116 -4.2900276 -4.2582016 -4.22278 -4.2116356 -4.2359209 -4.2587724 -4.2704344 -4.2764196 -4.2788324 -4.2698388 -4.2654023 -4.2668877 -4.2634878 -4.2653623][-4.2915463 -4.2695594 -4.2424231 -4.2075953 -4.1916113 -4.2179041 -4.2486963 -4.26007 -4.2603397 -4.2593193 -4.253376 -4.2530823 -4.2591252 -4.2581849 -4.2617097][-4.2942786 -4.2780204 -4.2549925 -4.2215962 -4.19995 -4.215518 -4.2417412 -4.2480168 -4.2412934 -4.2380781 -4.23922 -4.2507458 -4.2604423 -4.2605848 -4.26465][-4.3020167 -4.292593 -4.2758751 -4.2475796 -4.2181568 -4.2171006 -4.2346463 -4.2368803 -4.2263389 -4.2229033 -4.2297292 -4.2510028 -4.2669587 -4.2735448 -4.2810125][-4.2959933 -4.2942657 -4.2829981 -4.2599139 -4.2297692 -4.2133985 -4.2166567 -4.2116427 -4.2020092 -4.2046814 -4.216876 -4.2405624 -4.26139 -4.2772379 -4.290741][-4.2807164 -4.2817793 -4.2703438 -4.2480702 -4.2252474 -4.2044492 -4.1939235 -4.1869183 -4.1804886 -4.1933804 -4.2121344 -4.2319984 -4.2502084 -4.2717681 -4.289876][-4.2554789 -4.2551761 -4.2456388 -4.227869 -4.2111254 -4.1940365 -4.1841869 -4.1821842 -4.1791811 -4.1947269 -4.219707 -4.2389593 -4.252037 -4.2702003 -4.289124][-4.2414656 -4.2348585 -4.2287817 -4.2200089 -4.2079625 -4.1937742 -4.1849632 -4.1860981 -4.1840806 -4.2008314 -4.22995 -4.2547674 -4.2663989 -4.2768793 -4.2877989][-4.2352414 -4.2264042 -4.2208505 -4.2213664 -4.2176137 -4.2118134 -4.202744 -4.2000375 -4.1990767 -4.2170362 -4.2493443 -4.2806435 -4.2929654 -4.2926717 -4.2891426][-4.2318258 -4.2341337 -4.2334437 -4.2367063 -4.2390013 -4.2376261 -4.2281656 -4.2169352 -4.2141995 -4.2329507 -4.2662253 -4.2963581 -4.3094292 -4.3058567 -4.2919517][-4.2448387 -4.2577124 -4.2602549 -4.259357 -4.2569895 -4.2523966 -4.2409229 -4.2232966 -4.2193031 -4.239675 -4.2727666 -4.2988091 -4.3096714 -4.3073325 -4.2901106][-4.2572618 -4.2680292 -4.2666683 -4.262702 -4.2593074 -4.251883 -4.2380095 -4.2194309 -4.2132316 -4.233541 -4.2655244 -4.2910728 -4.305131 -4.3052692 -4.2896776][-4.2592998 -4.2665329 -4.2636633 -4.2576032 -4.2491627 -4.236587 -4.2226429 -4.2066731 -4.1992059 -4.217845 -4.2502089 -4.2752924 -4.293695 -4.2993383 -4.2887511][-4.2473407 -4.2580032 -4.2572174 -4.2540178 -4.242826 -4.2246418 -4.2098794 -4.1953907 -4.186945 -4.2033753 -4.2399559 -4.2682314 -4.2888064 -4.2951846 -4.2836218][-4.23566 -4.2457676 -4.2464294 -4.2503681 -4.2442937 -4.2266297 -4.212532 -4.1982884 -4.1815233 -4.1881037 -4.226789 -4.2570858 -4.2781157 -4.2829609 -4.2675595]]...]
INFO - root - 2017-12-07 17:03:36.902947: step 29010, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 77h:46m:33s remains)
INFO - root - 2017-12-07 17:03:46.715053: step 29020, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 83h:13m:09s remains)
INFO - root - 2017-12-07 17:03:56.384128: step 29030, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 82h:58m:09s remains)
INFO - root - 2017-12-07 17:04:06.029361: step 29040, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 81h:14m:25s remains)
INFO - root - 2017-12-07 17:04:15.834688: step 29050, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 84h:22m:45s remains)
INFO - root - 2017-12-07 17:04:25.565324: step 29060, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 81h:55m:56s remains)
INFO - root - 2017-12-07 17:04:35.195543: step 29070, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 79h:31m:10s remains)
INFO - root - 2017-12-07 17:04:44.981370: step 29080, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.999 sec/batch; 84h:12m:49s remains)
INFO - root - 2017-12-07 17:04:54.658798: step 29090, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 78h:20m:59s remains)
INFO - root - 2017-12-07 17:05:04.430520: step 29100, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 83h:36m:37s remains)
2017-12-07 17:05:05.383544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2834673 -4.2830348 -4.2827654 -4.2841372 -4.2839231 -4.28234 -4.2809796 -4.2820873 -4.2897053 -4.30119 -4.3089113 -4.3078132 -4.3011112 -4.2941103 -4.2904134][-4.2707744 -4.2685938 -4.2679157 -4.26726 -4.2648454 -4.2620153 -4.2612767 -4.2662029 -4.2803726 -4.298687 -4.3109841 -4.3116503 -4.3074889 -4.3020105 -4.2959828][-4.2505808 -4.2447104 -4.2395034 -4.2323713 -4.2244759 -4.2181306 -4.2178493 -4.2272739 -4.2478766 -4.2731686 -4.2923317 -4.3016338 -4.3069372 -4.3081031 -4.3026581][-4.2226429 -4.2098503 -4.196403 -4.1804757 -4.1656485 -4.1556315 -4.155798 -4.1680574 -4.1922536 -4.2234583 -4.2530394 -4.277245 -4.297996 -4.3099756 -4.3091488][-4.1976094 -4.1779866 -4.1536565 -4.130085 -4.1085844 -4.0946393 -4.0923109 -4.10202 -4.1237421 -4.1586504 -4.2004566 -4.2418003 -4.2760787 -4.2985582 -4.3047867][-4.1798744 -4.159771 -4.1324534 -4.1053848 -4.07809 -4.055203 -4.040206 -4.0320187 -4.0434008 -4.0868196 -4.1465883 -4.2029934 -4.2471666 -4.2754827 -4.2868438][-4.1640291 -4.1482315 -4.1257486 -4.1016612 -4.0736589 -4.0385151 -3.9957614 -3.9550848 -3.9562292 -4.0135159 -4.0913391 -4.1600385 -4.2088771 -4.2392278 -4.2546678][-4.15813 -4.1438103 -4.1262083 -4.1095843 -4.0870719 -4.0426259 -3.9668126 -3.8890324 -3.8798895 -3.9488957 -4.0374246 -4.1139545 -4.16573 -4.1957788 -4.2132516][-4.1693473 -4.1623054 -4.1551008 -4.1449094 -4.1268454 -4.0822105 -3.9959116 -3.8995538 -3.8678298 -3.9241235 -4.0089617 -4.0860357 -4.1373806 -4.1683259 -4.1860881][-4.185627 -4.1894422 -4.1966944 -4.1947927 -4.181869 -4.1496129 -4.0868025 -4.0123329 -3.9717572 -3.9899902 -4.0394278 -4.094842 -4.137423 -4.1705413 -4.1898713][-4.2065973 -4.2154579 -4.2296157 -4.23537 -4.2309384 -4.2114577 -4.171226 -4.1219549 -4.0872049 -4.0807953 -4.0978689 -4.12548 -4.1552668 -4.18857 -4.2108736][-4.2243586 -4.2338452 -4.2456226 -4.2522259 -4.2539849 -4.2470932 -4.22444 -4.1879525 -4.1541562 -4.13542 -4.1320767 -4.1405458 -4.1645126 -4.1996703 -4.2269611][-4.2230315 -4.226738 -4.2318821 -4.2368121 -4.2449665 -4.2492175 -4.2381377 -4.2119069 -4.1819744 -4.1625915 -4.1549244 -4.1531234 -4.1755424 -4.2126384 -4.2448421][-4.2059708 -4.2008529 -4.2007365 -4.2078261 -4.2210937 -4.2328444 -4.2249074 -4.2041974 -4.1816797 -4.1724396 -4.1703653 -4.1732478 -4.1999564 -4.2395954 -4.2737336][-4.1954818 -4.1835761 -4.1781964 -4.1855364 -4.1995087 -4.2068763 -4.2010007 -4.1887388 -4.1802421 -4.1819377 -4.1872416 -4.19837 -4.2268133 -4.2626314 -4.2951407]]...]
INFO - root - 2017-12-07 17:05:15.072032: step 29110, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 80h:38m:24s remains)
INFO - root - 2017-12-07 17:05:24.864581: step 29120, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 82h:44m:49s remains)
INFO - root - 2017-12-07 17:05:34.538331: step 29130, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 81h:33m:53s remains)
INFO - root - 2017-12-07 17:05:44.194862: step 29140, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.986 sec/batch; 83h:06m:46s remains)
INFO - root - 2017-12-07 17:05:53.986672: step 29150, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 77h:56m:59s remains)
INFO - root - 2017-12-07 17:06:03.745322: step 29160, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 83h:09m:28s remains)
INFO - root - 2017-12-07 17:06:13.277275: step 29170, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 78h:31m:25s remains)
INFO - root - 2017-12-07 17:06:23.018252: step 29180, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 75h:37m:37s remains)
INFO - root - 2017-12-07 17:06:32.634363: step 29190, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 79h:42m:00s remains)
INFO - root - 2017-12-07 17:06:42.216645: step 29200, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 80h:46m:04s remains)
2017-12-07 17:06:43.242529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3744307 -4.3793211 -4.36789 -4.3379478 -4.290741 -4.2487316 -4.2075768 -4.1770663 -4.2139363 -4.261188 -4.2709136 -4.2469306 -4.1949067 -4.1519756 -4.1341348][-4.3707786 -4.3746386 -4.360682 -4.3235865 -4.2680445 -4.2199049 -4.1756945 -4.1508346 -4.201633 -4.2593622 -4.275228 -4.2506366 -4.1949296 -4.150435 -4.1262007][-4.3664932 -4.36778 -4.3463655 -4.2962117 -4.2261052 -4.1606846 -4.1040006 -4.0876203 -4.16349 -4.2409267 -4.2736483 -4.2587428 -4.2069597 -4.1619091 -4.1292238][-4.3643618 -4.3636742 -4.3360205 -4.2727714 -4.1874094 -4.0966249 -4.0117993 -3.9943662 -4.1024532 -4.2089586 -4.2640195 -4.2682486 -4.231895 -4.1998396 -4.173708][-4.3632841 -4.360908 -4.3301792 -4.2569017 -4.155477 -4.0346656 -3.9062693 -3.8670907 -4.0055447 -4.1489911 -4.2322516 -4.2645555 -4.2564549 -4.24858 -4.2410207][-4.3624597 -4.3569813 -4.3225121 -4.2403016 -4.12269 -3.9769971 -3.8064463 -3.7293549 -3.8872302 -4.0626554 -4.1747031 -4.2397089 -4.2662082 -4.280571 -4.2836657][-4.3612165 -4.3520274 -4.3149195 -4.2315216 -4.1087623 -3.9597073 -3.7723606 -3.6563053 -3.8004487 -3.9816835 -4.1077814 -4.1994772 -4.2556934 -4.2868576 -4.2942429][-4.3611279 -4.3504844 -4.3143511 -4.2365837 -4.1243429 -3.9962735 -3.8376179 -3.7262132 -3.8216572 -3.9695327 -4.0836968 -4.1805525 -4.2497721 -4.2873368 -4.2969542][-4.3632321 -4.3542943 -4.3217807 -4.2523994 -4.1554961 -4.0569477 -3.9397626 -3.8536973 -3.9100492 -4.015975 -4.1087446 -4.1965742 -4.2633004 -4.299849 -4.3112807][-4.3669963 -4.361167 -4.3325357 -4.2717404 -4.1871305 -4.1112213 -4.027782 -3.9652843 -4.001924 -4.0745878 -4.1456747 -4.2229757 -4.2862692 -4.3220096 -4.3357053][-4.3721328 -4.370698 -4.3486977 -4.3010826 -4.23355 -4.1777344 -4.1218195 -4.0794005 -4.1039581 -4.1491332 -4.1945205 -4.2557406 -4.31221 -4.3451438 -4.3609285][-4.3763084 -4.3792348 -4.3643465 -4.3290339 -4.2766814 -4.2368822 -4.2025833 -4.1786866 -4.1968384 -4.2204561 -4.2427349 -4.285409 -4.3306847 -4.3567004 -4.3717718][-4.37592 -4.3804359 -4.3687239 -4.339323 -4.2938251 -4.2627125 -4.2438397 -4.233994 -4.250607 -4.2596903 -4.264246 -4.2923794 -4.3266492 -4.3443761 -4.3572674][-4.3728776 -4.3749442 -4.360486 -4.3283868 -4.2802663 -4.24905 -4.2377019 -4.2377367 -4.2589121 -4.2632279 -4.2569122 -4.2736421 -4.2964044 -4.3064179 -4.3187919][-4.3687897 -4.365715 -4.3437085 -4.3016462 -4.2421074 -4.2047515 -4.1939592 -4.2005157 -4.2301326 -4.2353988 -4.2197909 -4.2252097 -4.2383022 -4.2429829 -4.2558131]]...]
INFO - root - 2017-12-07 17:06:52.927824: step 29210, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 83h:33m:07s remains)
INFO - root - 2017-12-07 17:07:02.491800: step 29220, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 76h:06m:07s remains)
INFO - root - 2017-12-07 17:07:12.193338: step 29230, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 82h:26m:12s remains)
INFO - root - 2017-12-07 17:07:21.786754: step 29240, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 81h:08m:24s remains)
INFO - root - 2017-12-07 17:07:31.306773: step 29250, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 80h:17m:37s remains)
INFO - root - 2017-12-07 17:07:41.032260: step 29260, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 79h:15m:40s remains)
INFO - root - 2017-12-07 17:07:50.840691: step 29270, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.016 sec/batch; 85h:34m:17s remains)
INFO - root - 2017-12-07 17:08:00.549868: step 29280, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 80h:56m:17s remains)
INFO - root - 2017-12-07 17:08:10.219626: step 29290, loss = 2.09, batch loss = 2.04 (7.9 examples/sec; 1.011 sec/batch; 85h:10m:27s remains)
INFO - root - 2017-12-07 17:08:19.606912: step 29300, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 81h:31m:38s remains)
2017-12-07 17:08:20.520693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3065214 -4.305572 -4.3066511 -4.3061886 -4.303791 -4.2994637 -4.2875171 -4.2736568 -4.2696261 -4.2750478 -4.2821045 -4.2881384 -4.2928319 -4.3014026 -4.3165932][-4.2810535 -4.2854214 -4.2919559 -4.294054 -4.2916737 -4.2839675 -4.2625513 -4.2411084 -4.236506 -4.24513 -4.2559252 -4.2642903 -4.2732668 -4.2889981 -4.311305][-4.2489557 -4.2610703 -4.2740993 -4.2788029 -4.2757382 -4.2627397 -4.2307706 -4.2027273 -4.202394 -4.2186961 -4.2346554 -4.2459979 -4.259768 -4.2820292 -4.3094649][-4.218925 -4.2400074 -4.2592411 -4.2648182 -4.2585931 -4.2357049 -4.1877742 -4.1495743 -4.1580305 -4.1891932 -4.212894 -4.2283096 -4.2467575 -4.2764006 -4.3084474][-4.2010612 -4.228219 -4.2491703 -4.2507186 -4.2352891 -4.19435 -4.1237106 -4.0713949 -4.0925622 -4.1430225 -4.1779237 -4.200881 -4.2255335 -4.26439 -4.3028188][-4.1913915 -4.2206507 -4.2380633 -4.2326674 -4.2033067 -4.1398559 -4.0449419 -3.9795501 -4.0174766 -4.0901079 -4.1407776 -4.1741514 -4.2049313 -4.2515149 -4.2969851][-4.1947865 -4.2251487 -4.2374973 -4.2207065 -4.1710019 -4.0847573 -3.9729152 -3.9071302 -3.9647841 -4.0541272 -4.1167707 -4.158534 -4.1947436 -4.24615 -4.2950191][-4.2084827 -4.2388568 -4.2441273 -4.2116518 -4.1424484 -4.0461903 -3.9435451 -3.900651 -3.9695063 -4.0605049 -4.1240511 -4.1683526 -4.20399 -4.2537127 -4.3014193][-4.243258 -4.2746277 -4.2734222 -4.2280736 -4.1490479 -4.0623045 -3.9949207 -3.9862723 -4.0510798 -4.1248255 -4.1770496 -4.2130094 -4.240314 -4.2796135 -4.3176813][-4.2726827 -4.3050108 -4.3006 -4.2503796 -4.173233 -4.1072812 -4.0753307 -4.0921011 -4.1458759 -4.1974478 -4.236094 -4.2622256 -4.2798138 -4.3058758 -4.3320947][-4.281745 -4.312232 -4.3079858 -4.2637634 -4.20215 -4.1611896 -4.1543593 -4.1822767 -4.2209229 -4.250289 -4.2755766 -4.2936912 -4.3037891 -4.3207235 -4.3385968][-4.2813096 -4.3067956 -4.3027797 -4.2673488 -4.2256246 -4.210218 -4.2219067 -4.2516084 -4.2748265 -4.2849431 -4.2984395 -4.3109469 -4.3153729 -4.3259273 -4.3383236][-4.2871561 -4.3066316 -4.3010244 -4.2729979 -4.2484465 -4.2519193 -4.2752609 -4.3001494 -4.3110704 -4.3097072 -4.3150921 -4.3226686 -4.3232174 -4.3293376 -4.3378015][-4.3064718 -4.3199577 -4.3137159 -4.2917356 -4.278604 -4.2896209 -4.3143373 -4.3315644 -4.335721 -4.3296533 -4.329947 -4.3327942 -4.3304014 -4.3333235 -4.3391385][-4.3305631 -4.3407621 -4.3350806 -4.3176255 -4.3090949 -4.31882 -4.3375831 -4.3500657 -4.3522344 -4.3455858 -4.3434987 -4.3425093 -4.33761 -4.3384418 -4.3424754]]...]
INFO - root - 2017-12-07 17:08:30.256947: step 29310, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 79h:51m:03s remains)
INFO - root - 2017-12-07 17:08:39.759673: step 29320, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 81h:34m:29s remains)
INFO - root - 2017-12-07 17:08:49.492957: step 29330, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 80h:55m:03s remains)
INFO - root - 2017-12-07 17:08:59.126450: step 29340, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.016 sec/batch; 85h:32m:28s remains)
INFO - root - 2017-12-07 17:09:08.793637: step 29350, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 81h:35m:58s remains)
INFO - root - 2017-12-07 17:09:18.521492: step 29360, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 81h:53m:37s remains)
INFO - root - 2017-12-07 17:09:28.126287: step 29370, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 83h:23m:07s remains)
INFO - root - 2017-12-07 17:09:37.777897: step 29380, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 82h:02m:22s remains)
INFO - root - 2017-12-07 17:09:47.355058: step 29390, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 77h:27m:09s remains)
INFO - root - 2017-12-07 17:09:56.955436: step 29400, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 78h:09m:24s remains)
2017-12-07 17:09:57.892576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2963986 -4.3043709 -4.3167295 -4.324903 -4.326262 -4.3205748 -4.304637 -4.2785463 -4.2478213 -4.2184029 -4.2001529 -4.1910019 -4.1895394 -4.1895428 -4.1930943][-4.2992563 -4.3034649 -4.3116984 -4.3164029 -4.3165932 -4.3122797 -4.2982826 -4.2723603 -4.2389235 -4.2094536 -4.1955833 -4.1939836 -4.2017579 -4.207695 -4.2150297][-4.266324 -4.2709527 -4.2800412 -4.2889295 -4.2963662 -4.2995081 -4.2932596 -4.275012 -4.2488041 -4.2265906 -4.2177782 -4.2187281 -4.22517 -4.2284737 -4.2352195][-4.2000794 -4.2074251 -4.2217045 -4.2394161 -4.2589812 -4.2742362 -4.2790551 -4.2731218 -4.2612257 -4.2526612 -4.2500653 -4.25147 -4.2527862 -4.2504082 -4.2554922][-4.1236396 -4.1334257 -4.1514888 -4.1760554 -4.203588 -4.22759 -4.2434072 -4.249969 -4.2516794 -4.2557082 -4.2634745 -4.2696352 -4.2702336 -4.2679133 -4.2728519][-4.0727854 -4.0811071 -4.0965934 -4.120369 -4.1471124 -4.1701422 -4.1894722 -4.2034564 -4.2150464 -4.2295165 -4.248765 -4.2645464 -4.2731552 -4.2774353 -4.2844787][-4.076355 -4.077558 -4.08183 -4.094008 -4.1096077 -4.1221023 -4.1353626 -4.1459594 -4.1540151 -4.1681962 -4.1963019 -4.226645 -4.2515821 -4.2701669 -4.28314][-4.1182995 -4.1150656 -4.1107368 -4.1094551 -4.1104345 -4.1103411 -4.1145225 -4.1147418 -4.1097827 -4.1112962 -4.1384935 -4.1789446 -4.2193427 -4.2537622 -4.2751217][-4.1685739 -4.1644068 -4.1581044 -4.1500163 -4.1419182 -4.1339612 -4.131577 -4.1258845 -4.1093459 -4.0973034 -4.1173224 -4.1580577 -4.205451 -4.2472906 -4.2722979][-4.2157226 -4.2143035 -4.2120852 -4.20499 -4.1952806 -4.1838155 -4.1772642 -4.1683588 -4.1464524 -4.1284857 -4.1410284 -4.174974 -4.2165027 -4.2538924 -4.2757316][-4.2495093 -4.2524681 -4.2570729 -4.2565713 -4.2513542 -4.2436867 -4.2377338 -4.2283368 -4.2073293 -4.1889224 -4.1937075 -4.2145123 -4.2416496 -4.2662663 -4.2797909][-4.2713804 -4.2805448 -4.2915568 -4.297204 -4.2966776 -4.29313 -4.288384 -4.2789731 -4.2618871 -4.2475157 -4.2464371 -4.2546849 -4.2661033 -4.2750664 -4.2788229][-4.2749739 -4.28919 -4.3045969 -4.314692 -4.3162937 -4.3139653 -4.3113475 -4.3061404 -4.2973723 -4.2901473 -4.2854595 -4.2844386 -4.2827768 -4.2769408 -4.2714419][-4.2605324 -4.2759533 -4.2920485 -4.3033695 -4.3062859 -4.3058324 -4.3077455 -4.3101406 -4.3093891 -4.3070111 -4.301692 -4.2962842 -4.2874646 -4.2735662 -4.2626748][-4.2408147 -4.256156 -4.2704983 -4.2802939 -4.2832046 -4.2850738 -4.2918143 -4.3000064 -4.3053308 -4.3066316 -4.3016305 -4.2931929 -4.2794857 -4.262907 -4.252521]]...]
INFO - root - 2017-12-07 17:10:07.619409: step 29410, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 79h:01m:44s remains)
INFO - root - 2017-12-07 17:10:17.131472: step 29420, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 78h:52m:55s remains)
INFO - root - 2017-12-07 17:10:26.983313: step 29430, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 83h:49m:39s remains)
INFO - root - 2017-12-07 17:10:36.614563: step 29440, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.959 sec/batch; 80h:44m:11s remains)
INFO - root - 2017-12-07 17:10:46.333426: step 29450, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 83h:17m:31s remains)
INFO - root - 2017-12-07 17:10:56.168651: step 29460, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 83h:46m:09s remains)
INFO - root - 2017-12-07 17:11:05.719105: step 29470, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 83h:21m:00s remains)
INFO - root - 2017-12-07 17:11:15.341353: step 29480, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 78h:33m:47s remains)
INFO - root - 2017-12-07 17:11:25.039147: step 29490, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 79h:42m:32s remains)
INFO - root - 2017-12-07 17:11:34.707830: step 29500, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 77h:47m:58s remains)
2017-12-07 17:11:35.629794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2450862 -4.2326035 -4.2187395 -4.2041507 -4.217123 -4.2278986 -4.2091312 -4.1848483 -4.1603751 -4.1637959 -4.1795616 -4.177721 -4.1620884 -4.141912 -4.1325617][-4.1999245 -4.1857839 -4.1678562 -4.150157 -4.1651449 -4.1798759 -4.1641417 -4.1393476 -4.115622 -4.1252885 -4.1433892 -4.139781 -4.1257782 -4.1094832 -4.1040463][-4.1472254 -4.130477 -4.1047649 -4.0755281 -4.0843048 -4.1068354 -4.1049404 -4.0923238 -4.0815845 -4.101634 -4.1195226 -4.1086245 -4.0944037 -4.0934734 -4.1001515][-4.1079211 -4.0868726 -4.0530362 -4.008584 -4.0104284 -4.0454249 -4.0705695 -4.074399 -4.0756645 -4.0992937 -4.1137738 -4.0993519 -4.0916038 -4.1054287 -4.1213717][-4.0836678 -4.065105 -4.0313296 -3.9794402 -3.9715521 -4.006165 -4.0444417 -4.0515108 -4.0571895 -4.0901184 -4.1120739 -4.0984783 -4.0949674 -4.1191092 -4.1429729][-4.0800109 -4.0714769 -4.0433769 -3.988379 -3.9570413 -3.9559953 -3.9743834 -3.9764156 -3.9932363 -4.0474653 -4.0836544 -4.0792594 -4.0768819 -4.1069112 -4.1387844][-4.0837188 -4.0819974 -4.0540843 -3.9982836 -3.9461434 -3.9020817 -3.8846917 -3.8870006 -3.9252715 -4.0002031 -4.0393391 -4.0344448 -4.0327878 -4.0649695 -4.1020732][-4.0951834 -4.0898619 -4.0625467 -4.0122032 -3.9558029 -3.8924797 -3.8603926 -3.8734744 -3.9175572 -3.9833531 -4.0102234 -3.9991858 -3.9962385 -4.0246015 -4.0648322][-4.12199 -4.1106668 -4.085393 -4.0453548 -4.0022473 -3.9561458 -3.9327772 -3.9493821 -3.9754162 -4.01132 -4.0192437 -3.9993854 -3.99494 -4.0201902 -4.061152][-4.1497359 -4.1399097 -4.1202135 -4.0876884 -4.057507 -4.0306234 -4.0198569 -4.0370226 -4.0478196 -4.0594311 -4.0534086 -4.0335875 -4.03138 -4.0562797 -4.0982561][-4.1894221 -4.1850424 -4.1729631 -4.1444106 -4.1193147 -4.104526 -4.1022406 -4.1164422 -4.1198087 -4.1194739 -4.1106834 -4.0965233 -4.0984697 -4.1224155 -4.15693][-4.2372985 -4.2390003 -4.2318859 -4.2065759 -4.1844764 -4.173902 -4.17526 -4.1872597 -4.1898456 -4.1880765 -4.1799717 -4.1723471 -4.1750445 -4.1906118 -4.2081146][-4.2635775 -4.2705331 -4.2706351 -4.2532377 -4.2380581 -4.232151 -4.2335677 -4.2422924 -4.2454228 -4.2421584 -4.2336564 -4.2281842 -4.229681 -4.2392149 -4.2482924][-4.2710648 -4.2795138 -4.2834496 -4.2762523 -4.2692318 -4.2664828 -4.2679315 -4.2743063 -4.276804 -4.2750049 -4.2685781 -4.2631645 -4.261714 -4.2668905 -4.27303][-4.2749591 -4.2817578 -4.2843261 -4.2793221 -4.2761903 -4.2751279 -4.276197 -4.28161 -4.2856216 -4.2865448 -4.2841568 -4.2803526 -4.2792287 -4.2811933 -4.2842908]]...]
INFO - root - 2017-12-07 17:11:45.177433: step 29510, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.971 sec/batch; 81h:45m:28s remains)
INFO - root - 2017-12-07 17:11:54.747289: step 29520, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 75h:42m:11s remains)
INFO - root - 2017-12-07 17:12:04.518159: step 29530, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 82h:33m:04s remains)
INFO - root - 2017-12-07 17:12:14.101312: step 29540, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 82h:07m:53s remains)
INFO - root - 2017-12-07 17:12:23.677141: step 29550, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 81h:58m:41s remains)
INFO - root - 2017-12-07 17:12:33.208882: step 29560, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 79h:40m:35s remains)
INFO - root - 2017-12-07 17:12:42.950992: step 29570, loss = 2.12, batch loss = 2.06 (8.1 examples/sec; 0.983 sec/batch; 82h:43m:33s remains)
INFO - root - 2017-12-07 17:12:52.816737: step 29580, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 77h:57m:37s remains)
INFO - root - 2017-12-07 17:13:02.409622: step 29590, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 79h:54m:30s remains)
INFO - root - 2017-12-07 17:13:12.013618: step 29600, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 82h:15m:18s remains)
2017-12-07 17:13:12.989610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2556329 -4.2663655 -4.2730904 -4.2731318 -4.2725954 -4.271337 -4.2720385 -4.2798553 -4.2891626 -4.2905707 -4.2874804 -4.2755632 -4.2492361 -4.21669 -4.2005382][-4.2941914 -4.3020182 -4.3052745 -4.299551 -4.2884436 -4.2765932 -4.2695394 -4.272759 -4.2821908 -4.2860966 -4.2860961 -4.2834291 -4.2671757 -4.2392378 -4.2256651][-4.3012447 -4.30847 -4.3112197 -4.3001556 -4.2782564 -4.25633 -4.2407084 -4.2387638 -4.2482843 -4.2572627 -4.2655625 -4.2777452 -4.2797894 -4.2663412 -4.2575765][-4.2979412 -4.3093486 -4.312552 -4.2926326 -4.2556639 -4.2193308 -4.18944 -4.1758223 -4.1860561 -4.2074142 -4.2342048 -4.2666082 -4.28944 -4.2908707 -4.2842937][-4.2820592 -4.2978954 -4.2983837 -4.2689919 -4.215867 -4.1569505 -4.1000667 -4.067523 -4.08169 -4.1285353 -4.1848965 -4.2416244 -4.2841635 -4.2999029 -4.2977872][-4.2483716 -4.260787 -4.2533565 -4.2111487 -4.1396956 -4.04767 -3.9451628 -3.8787963 -3.9048905 -3.9971929 -4.0938745 -4.1772227 -4.2438793 -4.2809873 -4.2922034][-4.2281785 -4.2247987 -4.1925025 -4.1236362 -4.0187955 -3.8796556 -3.7191231 -3.6007147 -3.6418948 -3.7955022 -3.9448206 -4.0637164 -4.1619964 -4.2278976 -4.25983][-4.2450943 -4.2285924 -4.1817808 -4.1023684 -3.9891756 -3.8405144 -3.6642916 -3.5192075 -3.5438304 -3.6982388 -3.8545833 -3.9845548 -4.0971184 -4.1825533 -4.2303267][-4.2843428 -4.2712135 -4.2382455 -4.185173 -4.111062 -4.0123491 -3.8966651 -3.7968473 -3.8009756 -3.8821914 -3.971498 -4.0511451 -4.1264448 -4.1872621 -4.2220073][-4.3166285 -4.3106813 -4.2941971 -4.26638 -4.2250257 -4.1684546 -4.1027541 -4.04846 -4.0502553 -4.0866218 -4.1239586 -4.1536403 -4.1870914 -4.2168307 -4.2311563][-4.3155155 -4.3097429 -4.3001652 -4.2848454 -4.25714 -4.2204857 -4.1860285 -4.1670885 -4.178103 -4.1989884 -4.2081637 -4.2049575 -4.2090859 -4.2186465 -4.2198024][-4.280139 -4.2718196 -4.2637048 -4.2530465 -4.2315006 -4.2062397 -4.193397 -4.2004547 -4.2222486 -4.2407584 -4.2428174 -4.2290125 -4.22084 -4.2218833 -4.2171769][-4.2527456 -4.2391143 -4.2283025 -4.2214069 -4.2034092 -4.1861148 -4.190125 -4.2123117 -4.2397194 -4.2603579 -4.2642565 -4.251811 -4.2379818 -4.2332635 -4.2301846][-4.2534251 -4.2399549 -4.2309747 -4.2255411 -4.20825 -4.1931725 -4.2030616 -4.2300882 -4.2574263 -4.2782311 -4.2846122 -4.2775269 -4.2627106 -4.24894 -4.2422547][-4.2740388 -4.265893 -4.2610922 -4.2553248 -4.2361717 -4.2198935 -4.2248864 -4.2459927 -4.2675962 -4.2834029 -4.2840838 -4.2736373 -4.2567363 -4.2370105 -4.2226429]]...]
INFO - root - 2017-12-07 17:13:22.560080: step 29610, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 83h:08m:09s remains)
INFO - root - 2017-12-07 17:13:32.328310: step 29620, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 84h:34m:33s remains)
INFO - root - 2017-12-07 17:13:41.909429: step 29630, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 84h:02m:40s remains)
INFO - root - 2017-12-07 17:13:51.681970: step 29640, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.945 sec/batch; 79h:30m:41s remains)
INFO - root - 2017-12-07 17:14:01.395618: step 29650, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 81h:16m:04s remains)
INFO - root - 2017-12-07 17:14:11.066715: step 29660, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.969 sec/batch; 81h:32m:31s remains)
INFO - root - 2017-12-07 17:14:20.472798: step 29670, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 72h:57m:11s remains)
INFO - root - 2017-12-07 17:14:29.984940: step 29680, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 80h:20m:01s remains)
INFO - root - 2017-12-07 17:14:39.736022: step 29690, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 83h:42m:52s remains)
INFO - root - 2017-12-07 17:14:49.511936: step 29700, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 82h:40m:16s remains)
2017-12-07 17:14:50.479796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3536968 -4.3578372 -4.3576632 -4.342803 -4.3133264 -4.2687359 -4.2230806 -4.1927562 -4.18235 -4.1846309 -4.174161 -4.1621327 -4.1790695 -4.2195864 -4.24943][-4.3535333 -4.359 -4.3614516 -4.3476925 -4.3149033 -4.2594752 -4.2000914 -4.1707897 -4.1746039 -4.1937151 -4.1948366 -4.1818581 -4.1940317 -4.2333364 -4.263103][-4.3527279 -4.3586164 -4.3623142 -4.3494062 -4.3141561 -4.2472296 -4.1729083 -4.1460319 -4.1666055 -4.2055573 -4.2204561 -4.213171 -4.2238264 -4.2561321 -4.2780981][-4.3521881 -4.3568311 -4.36069 -4.3476653 -4.311317 -4.2342734 -4.1469827 -4.1240864 -4.1630955 -4.2234626 -4.2504482 -4.2486968 -4.2591581 -4.2832656 -4.29681][-4.3523145 -4.3554277 -4.3578124 -4.3447547 -4.3052044 -4.2207751 -4.1228538 -4.1022243 -4.1595426 -4.2372046 -4.2729259 -4.274745 -4.285162 -4.3043394 -4.3115511][-4.3530331 -4.3549714 -4.3547812 -4.3415704 -4.2974911 -4.20328 -4.0962877 -4.0754337 -4.1476049 -4.2418613 -4.2851357 -4.2920351 -4.3023853 -4.317554 -4.32176][-4.3538623 -4.3551145 -4.3518658 -4.3375292 -4.2892056 -4.1846476 -4.0645852 -4.0376329 -4.121192 -4.23337 -4.2897506 -4.3040442 -4.3113027 -4.3213811 -4.3225269][-4.3543277 -4.3541775 -4.3483877 -4.333745 -4.2854881 -4.1808367 -4.0539603 -4.01127 -4.0890236 -4.2095523 -4.2814484 -4.3069181 -4.3140311 -4.3175573 -4.3138161][-4.3533111 -4.352385 -4.3458128 -4.3327589 -4.2896609 -4.1947789 -4.0746632 -4.0142708 -4.0667677 -4.1786518 -4.2590466 -4.2977748 -4.3114858 -4.3137217 -4.3084555][-4.3508153 -4.3492961 -4.3444562 -4.3350272 -4.3003073 -4.2193947 -4.1162767 -4.0529232 -4.0749946 -4.1581554 -4.2318749 -4.2802057 -4.3051023 -4.3093276 -4.3032346][-4.3474073 -4.3432455 -4.3388686 -4.333137 -4.3088808 -4.2459249 -4.1675825 -4.1173038 -4.1199136 -4.1665573 -4.2184253 -4.2700715 -4.3057103 -4.3123074 -4.3008575][-4.3438287 -4.3362055 -4.3302183 -4.3266764 -4.3106813 -4.2656636 -4.2111177 -4.1802998 -4.1772866 -4.1982088 -4.2247305 -4.2694368 -4.3088918 -4.316319 -4.2984023][-4.3412795 -4.3287969 -4.3202105 -4.3178267 -4.3063416 -4.2726507 -4.2337265 -4.2192869 -4.2219429 -4.2356739 -4.2443466 -4.2738209 -4.310853 -4.3175898 -4.2958159][-4.3392959 -4.3223319 -4.3105 -4.3072104 -4.2982473 -4.2697673 -4.2390437 -4.2323928 -4.2435117 -4.2600656 -4.26266 -4.2785158 -4.30842 -4.3167639 -4.2977157][-4.3395047 -4.3191195 -4.3028593 -4.2959156 -4.2867475 -4.2614136 -4.2353849 -4.2290182 -4.2426753 -4.2625203 -4.27141 -4.2825274 -4.3027377 -4.3110857 -4.2967582]]...]
INFO - root - 2017-12-07 17:15:00.013179: step 29710, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 79h:16m:12s remains)
INFO - root - 2017-12-07 17:15:09.683797: step 29720, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.974 sec/batch; 81h:55m:39s remains)
INFO - root - 2017-12-07 17:15:19.428390: step 29730, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 81h:25m:44s remains)
INFO - root - 2017-12-07 17:15:28.965414: step 29740, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 84h:24m:36s remains)
INFO - root - 2017-12-07 17:15:38.603767: step 29750, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.935 sec/batch; 78h:40m:17s remains)
INFO - root - 2017-12-07 17:15:48.321734: step 29760, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 84h:11m:26s remains)
INFO - root - 2017-12-07 17:15:57.776528: step 29770, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 79h:25m:50s remains)
INFO - root - 2017-12-07 17:16:07.644286: step 29780, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 83h:30m:30s remains)
INFO - root - 2017-12-07 17:16:17.289177: step 29790, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 81h:34m:33s remains)
INFO - root - 2017-12-07 17:16:26.919254: step 29800, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 77h:58m:49s remains)
2017-12-07 17:16:27.942012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3030396 -4.2972217 -4.2918987 -4.2866845 -4.2847338 -4.2866068 -4.2908235 -4.2965455 -4.2994633 -4.2962456 -4.2867622 -4.2753806 -4.2629046 -4.2509732 -4.2414293][-4.2777634 -4.2720757 -4.2657504 -4.257175 -4.2529631 -4.2540054 -4.2584195 -4.2661285 -4.2718086 -4.2721081 -4.2676792 -4.2598066 -4.2498107 -4.2377458 -4.2248855][-4.2584367 -4.25169 -4.243228 -4.2324767 -4.2260761 -4.2249761 -4.2255936 -4.2318907 -4.2393265 -4.2442346 -4.2461071 -4.2441421 -4.2394867 -4.2311621 -4.2207069][-4.2513175 -4.2371917 -4.2234159 -4.211257 -4.2037506 -4.19926 -4.1939154 -4.1955123 -4.2038512 -4.215693 -4.2268176 -4.2334366 -4.2366476 -4.236712 -4.236352][-4.2578053 -4.2324352 -4.2053986 -4.1864662 -4.1771245 -4.1646938 -4.148262 -4.141377 -4.1492872 -4.1679621 -4.1858521 -4.1992574 -4.2127094 -4.226645 -4.2427096][-4.2617292 -4.2228069 -4.1739311 -4.1369586 -4.11206 -4.077383 -4.040144 -4.0274997 -4.042747 -4.0761933 -4.1097784 -4.1361766 -4.1608925 -4.1908193 -4.2234197][-4.2551928 -4.2116265 -4.1466546 -4.0849686 -4.029685 -3.9623504 -3.90248 -3.8912387 -3.92429 -3.9755049 -4.0226946 -4.0625987 -4.1055112 -4.1555943 -4.1993742][-4.2450228 -4.2150412 -4.1584516 -4.0913773 -4.0193553 -3.9358091 -3.8713906 -3.868037 -3.9081497 -3.9534838 -3.9934525 -4.0355759 -4.090807 -4.1501255 -4.1970706][-4.2323003 -4.2258596 -4.1934824 -4.1462321 -4.089664 -4.0232606 -3.9761324 -3.9767199 -4.0042677 -4.0274081 -4.0485287 -4.0831966 -4.1359863 -4.1864624 -4.2224622][-4.20613 -4.2275324 -4.2236738 -4.2050347 -4.1760664 -4.13632 -4.110641 -4.1122971 -4.1241145 -4.1319861 -4.144619 -4.1694279 -4.2064395 -4.2363758 -4.2544708][-4.1836181 -4.2243471 -4.2391896 -4.243391 -4.2396822 -4.228713 -4.2190394 -4.2186327 -4.2234926 -4.2294168 -4.2430434 -4.2595873 -4.281342 -4.2936077 -4.2957435][-4.1973877 -4.2412634 -4.2611494 -4.27638 -4.2873416 -4.2912416 -4.2889495 -4.2887807 -4.2936029 -4.300313 -4.3110967 -4.3214588 -4.3303542 -4.3316755 -4.3289762][-4.2452354 -4.2763143 -4.29156 -4.3103008 -4.3260369 -4.3323922 -4.33112 -4.3294158 -4.3330612 -4.338264 -4.342947 -4.3465939 -4.348155 -4.3469639 -4.3487792][-4.2887111 -4.3052235 -4.3148127 -4.33234 -4.3472729 -4.3534679 -4.3524904 -4.3498416 -4.3517108 -4.3554087 -4.3565769 -4.3563671 -4.3556132 -4.3553557 -4.3592148][-4.3291435 -4.3333836 -4.3369927 -4.3485861 -4.3599696 -4.3645763 -4.3631153 -4.36035 -4.3598404 -4.3606296 -4.3613148 -4.3618679 -4.36151 -4.362359 -4.3644524]]...]
INFO - root - 2017-12-07 17:16:37.540781: step 29810, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 83h:06m:22s remains)
INFO - root - 2017-12-07 17:16:47.293670: step 29820, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 82h:20m:25s remains)
INFO - root - 2017-12-07 17:16:57.064996: step 29830, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 81h:18m:05s remains)
INFO - root - 2017-12-07 17:17:06.392900: step 29840, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 77h:54m:04s remains)
INFO - root - 2017-12-07 17:17:16.141055: step 29850, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.999 sec/batch; 83h:57m:32s remains)
INFO - root - 2017-12-07 17:17:25.910315: step 29860, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 82h:55m:26s remains)
INFO - root - 2017-12-07 17:17:35.482930: step 29870, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 78h:50m:16s remains)
INFO - root - 2017-12-07 17:17:45.166109: step 29880, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 83h:10m:38s remains)
INFO - root - 2017-12-07 17:17:54.971162: step 29890, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 81h:17m:46s remains)
INFO - root - 2017-12-07 17:18:04.697616: step 29900, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 81h:56m:48s remains)
2017-12-07 17:18:05.733284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2722898 -4.2474904 -4.2483721 -4.2581649 -4.2669148 -4.2697635 -4.2550745 -4.2351522 -4.216382 -4.2220941 -4.23695 -4.2564316 -4.2781067 -4.2878695 -4.2959509][-4.2352958 -4.2191844 -4.2233224 -4.2369571 -4.2531834 -4.2665949 -4.2617188 -4.2482667 -4.2299004 -4.23056 -4.2408538 -4.2539244 -4.2619915 -4.2631149 -4.2619667][-4.1993842 -4.1870451 -4.1918406 -4.2090421 -4.2280283 -4.2455182 -4.2469625 -4.2447214 -4.2413378 -4.2484937 -4.2539635 -4.2563529 -4.2519741 -4.2408938 -4.2259][-4.163208 -4.1552 -4.1656628 -4.187849 -4.2036119 -4.2156682 -4.2126393 -4.2079124 -4.2202725 -4.23925 -4.245657 -4.243907 -4.2323203 -4.2126722 -4.1869884][-4.1341591 -4.1305547 -4.1503463 -4.177568 -4.1851978 -4.1831417 -4.15981 -4.1380119 -4.1572104 -4.1953411 -4.2129951 -4.2141886 -4.2028565 -4.1759214 -4.1416388][-4.1340876 -4.1158438 -4.1269317 -4.1486964 -4.1557517 -4.14397 -4.0874257 -4.0228786 -4.0368261 -4.1051817 -4.1445518 -4.1604466 -4.1638107 -4.1419153 -4.1069984][-4.1432514 -4.0921316 -4.07185 -4.0747261 -4.0783386 -4.0551791 -3.9604266 -3.8305 -3.8326769 -3.9516308 -4.0236373 -4.0615363 -4.0954585 -4.0983505 -4.075808][-4.1518278 -4.06809 -4.0155916 -4.0021677 -3.9937093 -3.9584558 -3.8495839 -3.6811061 -3.6715498 -3.8356586 -3.9365404 -3.9893231 -4.04399 -4.072783 -4.0711527][-4.1830277 -4.0991049 -4.0392485 -4.0260172 -4.0154629 -3.9821086 -3.9021389 -3.7855604 -3.7750869 -3.8968618 -3.9853249 -4.0250654 -4.0690837 -4.1034551 -4.1173458][-4.2287035 -4.1743336 -4.1313276 -4.1154222 -4.103168 -4.0761652 -4.0279531 -3.9714394 -3.963558 -4.0283875 -4.0860062 -4.1097569 -4.1391191 -4.1686091 -4.1841969][-4.2593465 -4.2390337 -4.2221861 -4.2087421 -4.1941133 -4.1702209 -4.1398711 -4.1182251 -4.1173382 -4.1468682 -4.1802011 -4.1973825 -4.2129817 -4.228982 -4.2398195][-4.2523193 -4.2551169 -4.2621818 -4.26306 -4.2538204 -4.2325745 -4.2135506 -4.2053752 -4.2086964 -4.2232771 -4.2396679 -4.246944 -4.2505846 -4.2536922 -4.2555027][-4.2263288 -4.2391481 -4.2582006 -4.2697539 -4.26683 -4.2536454 -4.2436428 -4.2388687 -4.2418013 -4.2487593 -4.2532859 -4.2507439 -4.249115 -4.246789 -4.2416897][-4.2063355 -4.2200627 -4.2376833 -4.2508664 -4.2527552 -4.2445455 -4.2379675 -4.233778 -4.2348709 -4.2345567 -4.2337728 -4.2295046 -4.2276258 -4.2245512 -4.2191129][-4.2200193 -4.2278738 -4.2360821 -4.2407875 -4.2405767 -4.2319708 -4.225039 -4.2225118 -4.2225437 -4.2212543 -4.2203674 -4.2190065 -4.2197833 -4.2208867 -4.2212505]]...]
INFO - root - 2017-12-07 17:18:15.248721: step 29910, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.983 sec/batch; 82h:39m:27s remains)
INFO - root - 2017-12-07 17:18:25.045183: step 29920, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 77h:35m:47s remains)
INFO - root - 2017-12-07 17:18:34.764826: step 29930, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 78h:54m:46s remains)
INFO - root - 2017-12-07 17:18:44.380883: step 29940, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 80h:16m:16s remains)
INFO - root - 2017-12-07 17:18:53.879820: step 29950, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 84h:35m:20s remains)
INFO - root - 2017-12-07 17:19:03.601526: step 29960, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.014 sec/batch; 85h:12m:15s remains)
INFO - root - 2017-12-07 17:19:13.234753: step 29970, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 81h:17m:53s remains)
INFO - root - 2017-12-07 17:19:22.820685: step 29980, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 79h:20m:29s remains)
INFO - root - 2017-12-07 17:19:32.420068: step 29990, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 80h:18m:16s remains)
INFO - root - 2017-12-07 17:19:42.107019: step 30000, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 76h:49m:03s remains)
2017-12-07 17:19:43.105795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2434688 -4.2073731 -4.1715069 -4.138494 -4.1129551 -4.1059365 -4.1253147 -4.1640682 -4.2002144 -4.2241678 -4.2374978 -4.2438912 -4.2472425 -4.2480783 -4.2485175][-4.221808 -4.168159 -4.1129084 -4.06371 -4.0217896 -3.9999514 -4.02429 -4.0901408 -4.1536903 -4.1920886 -4.2124672 -4.2236166 -4.231668 -4.2376447 -4.2425122][-4.2004585 -4.1276784 -4.0534825 -3.9852438 -3.9206164 -3.8732173 -3.8992646 -4.0014296 -4.09856 -4.1562009 -4.1872387 -4.2032642 -4.2169042 -4.2298651 -4.2430487][-4.1913676 -4.1117344 -4.0252705 -3.9381685 -3.850574 -3.7793846 -3.80331 -3.9298656 -4.0529871 -4.1253991 -4.1654863 -4.1883497 -4.2060714 -4.2244749 -4.2430267][-4.1895609 -4.1031876 -4.0071774 -3.9023862 -3.7983313 -3.7163665 -3.7435474 -3.8786783 -4.0163326 -4.0977349 -4.14307 -4.1733446 -4.1971416 -4.2181048 -4.2370806][-4.1884408 -4.0926733 -3.9884355 -3.8814032 -3.7854967 -3.7221837 -3.7530153 -3.8737414 -4.0023189 -4.0802288 -4.1263833 -4.1601424 -4.1847992 -4.2071877 -4.2264643][-4.193161 -4.0998778 -4.00008 -3.9088769 -3.8463326 -3.8235621 -3.857198 -3.9418693 -4.0334816 -4.0913391 -4.1283288 -4.1580787 -4.1799207 -4.2038479 -4.224864][-4.2041121 -4.1298704 -4.0540285 -3.9897323 -3.9585204 -3.9572701 -3.977633 -4.0239053 -4.0880537 -4.1310349 -4.1530223 -4.1717157 -4.1921306 -4.221354 -4.2437744][-4.2039866 -4.1516747 -4.102026 -4.0676775 -4.0547791 -4.0537624 -4.0562978 -4.0790248 -4.1296344 -4.1680689 -4.1824389 -4.1961484 -4.2169342 -4.247386 -4.2671323][-4.2012868 -4.1616125 -4.130753 -4.1182542 -4.1118608 -4.09946 -4.0868912 -4.0975418 -4.1348672 -4.1734047 -4.1941662 -4.212225 -4.236794 -4.2649622 -4.281538][-4.2109962 -4.182405 -4.15485 -4.1429152 -4.1285348 -4.0952477 -4.066874 -4.0725603 -4.1088448 -4.159884 -4.1998105 -4.2290621 -4.2566514 -4.2840743 -4.2974415][-4.2448015 -4.2244344 -4.1981297 -4.1719918 -4.1354432 -4.0816908 -4.0413661 -4.0508938 -4.0966034 -4.1606531 -4.2204428 -4.2579613 -4.2844248 -4.3090568 -4.31284][-4.2720957 -4.2557979 -4.2277622 -4.1862974 -4.1322622 -4.0708561 -4.0361018 -4.0577488 -4.1108847 -4.1760521 -4.2422428 -4.2841778 -4.3081336 -4.3228426 -4.3128891][-4.28771 -4.2702103 -4.2387481 -4.1859379 -4.1239324 -4.0653219 -4.0459743 -4.0800543 -4.1343703 -4.1945333 -4.2569041 -4.2953067 -4.3117986 -4.3117948 -4.2874608][-4.2877746 -4.26826 -4.2387848 -4.1905127 -4.1322393 -4.0855484 -4.08268 -4.1237316 -4.1731334 -4.2244582 -4.2737112 -4.3040786 -4.3083277 -4.289587 -4.2537332]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 17:19:53.519442: step 30010, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 82h:43m:05s remains)
INFO - root - 2017-12-07 17:20:03.308449: step 30020, loss = 2.09, batch loss = 2.04 (7.8 examples/sec; 1.020 sec/batch; 85h:44m:27s remains)
INFO - root - 2017-12-07 17:20:13.152189: step 30030, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 80h:53m:51s remains)
INFO - root - 2017-12-07 17:20:22.781755: step 30040, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 80h:12m:20s remains)
INFO - root - 2017-12-07 17:20:32.352311: step 30050, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 79h:00m:51s remains)
INFO - root - 2017-12-07 17:20:41.986591: step 30060, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 79h:49m:36s remains)
INFO - root - 2017-12-07 17:20:51.617324: step 30070, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 80h:24m:19s remains)
INFO - root - 2017-12-07 17:21:01.337451: step 30080, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 81h:19m:10s remains)
INFO - root - 2017-12-07 17:21:11.023533: step 30090, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 81h:28m:41s remains)
INFO - root - 2017-12-07 17:21:20.440803: step 30100, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 82h:30m:58s remains)
2017-12-07 17:21:21.423072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2711492 -4.2753639 -4.2804165 -4.2826252 -4.2783904 -4.2780361 -4.2845168 -4.2920871 -4.291903 -4.2854147 -4.2719054 -4.2553048 -4.2435479 -4.2416081 -4.2458878][-4.2470646 -4.2567477 -4.2696972 -4.280375 -4.2858467 -4.2945151 -4.3082972 -4.3227749 -4.32721 -4.3196836 -4.3060856 -4.2878323 -4.2698526 -4.2603188 -4.2608356][-4.2166634 -4.2305336 -4.2546029 -4.2755065 -4.2902751 -4.3058271 -4.3238707 -4.3394556 -4.3421249 -4.3309383 -4.3144712 -4.29547 -4.272758 -4.2573719 -4.259768][-4.19262 -4.2008452 -4.2240071 -4.2438455 -4.2622008 -4.2845254 -4.3110676 -4.3328881 -4.3377819 -4.3267689 -4.309864 -4.2891951 -4.262846 -4.2402864 -4.2404151][-4.1851425 -4.17447 -4.1769171 -4.1799765 -4.1930246 -4.2207065 -4.2595158 -4.29544 -4.3109531 -4.3071809 -4.2959166 -4.2804008 -4.2564011 -4.2232108 -4.2136836][-4.2001047 -4.1705728 -4.1438732 -4.1165056 -4.1096764 -4.1357226 -4.1848092 -4.2355504 -4.2652664 -4.2733526 -4.2687211 -4.2609086 -4.2456865 -4.208499 -4.18782][-4.2331219 -4.2023625 -4.1608772 -4.10858 -4.0704069 -4.0722528 -4.1113577 -4.1662197 -4.2065678 -4.2262492 -4.2317963 -4.2293735 -4.2202477 -4.1835413 -4.1532812][-4.2697382 -4.2500911 -4.2150717 -4.1631408 -4.1102824 -4.0810823 -4.0830317 -4.115221 -4.1481037 -4.1726174 -4.18943 -4.1955824 -4.1871943 -4.1518888 -4.1164188][-4.3003483 -4.2925568 -4.2738967 -4.2402153 -4.1955118 -4.1549397 -4.1315327 -4.131145 -4.1388364 -4.1544056 -4.1737227 -4.1861739 -4.1828461 -4.1554632 -4.1184311][-4.3065047 -4.3058577 -4.301167 -4.2906656 -4.2645278 -4.23133 -4.2059183 -4.1933141 -4.1864257 -4.1905026 -4.203804 -4.2163181 -4.2189631 -4.20448 -4.1763234][-4.2848463 -4.2826657 -4.2865448 -4.2927022 -4.2853065 -4.2702727 -4.2612085 -4.2569027 -4.2523351 -4.2480974 -4.2531042 -4.263216 -4.2661934 -4.2566476 -4.2393632][-4.251843 -4.2434645 -4.2483268 -4.2598391 -4.2652874 -4.2656474 -4.2739267 -4.2846036 -4.2901387 -4.2884073 -4.2913966 -4.2980485 -4.2974453 -4.287324 -4.2776523][-4.2231665 -4.2100282 -4.2103624 -4.2141495 -4.2198367 -4.2291389 -4.24967 -4.2694116 -4.280406 -4.2880106 -4.295629 -4.30279 -4.3049912 -4.3019667 -4.3028822][-4.2215142 -4.2033029 -4.194921 -4.1830473 -4.1804686 -4.1872764 -4.2123604 -4.2383175 -4.2528849 -4.2693992 -4.2857246 -4.2931385 -4.2951894 -4.2995882 -4.3087063][-4.2394414 -4.2197652 -4.2038531 -4.1838975 -4.1729856 -4.1757669 -4.2003474 -4.2273135 -4.2420359 -4.2590733 -4.2768044 -4.2850852 -4.2889056 -4.2979012 -4.307343]]...]
INFO - root - 2017-12-07 17:21:31.276331: step 30110, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 84h:32m:53s remains)
INFO - root - 2017-12-07 17:21:40.909826: step 30120, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 77h:57m:46s remains)
INFO - root - 2017-12-07 17:21:50.701214: step 30130, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 83h:14m:53s remains)
INFO - root - 2017-12-07 17:22:00.490108: step 30140, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 84h:41m:06s remains)
INFO - root - 2017-12-07 17:22:10.124998: step 30150, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 79h:44m:03s remains)
INFO - root - 2017-12-07 17:22:19.610215: step 30160, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 77h:12m:10s remains)
INFO - root - 2017-12-07 17:22:29.210980: step 30170, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 78h:42m:12s remains)
INFO - root - 2017-12-07 17:22:38.825202: step 30180, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.935 sec/batch; 78h:33m:31s remains)
INFO - root - 2017-12-07 17:22:48.583987: step 30190, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 82h:59m:01s remains)
INFO - root - 2017-12-07 17:22:58.076253: step 30200, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 78h:58m:18s remains)
2017-12-07 17:22:59.055663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2794046 -4.2489414 -4.2298436 -4.20873 -4.197969 -4.2073388 -4.2274976 -4.2254224 -4.2075534 -4.2037349 -4.2095132 -4.2058415 -4.2039804 -4.2188158 -4.2310314][-4.2892413 -4.256835 -4.2357578 -4.21251 -4.1949463 -4.200768 -4.2296443 -4.2357397 -4.2178707 -4.2131367 -4.21811 -4.2171988 -4.219429 -4.2383714 -4.257977][-4.3018985 -4.2723765 -4.2492876 -4.2239189 -4.1926036 -4.1879745 -4.2195172 -4.2365522 -4.2283182 -4.224195 -4.226932 -4.2270107 -4.2342696 -4.2563457 -4.273571][-4.3105354 -4.2842107 -4.25734 -4.22124 -4.165051 -4.1426983 -4.1795874 -4.2103195 -4.2214665 -4.2308764 -4.2415133 -4.2424431 -4.2475147 -4.2653332 -4.2784691][-4.315053 -4.2877297 -4.2527733 -4.20021 -4.1061883 -4.0517249 -4.10118 -4.1600275 -4.1981926 -4.2309494 -4.2525234 -4.2527823 -4.2530193 -4.2705398 -4.283587][-4.3176241 -4.2887497 -4.24464 -4.1737413 -4.0464144 -3.9546633 -4.0213156 -4.120244 -4.1840353 -4.2300153 -4.2523103 -4.2429109 -4.2365351 -4.2565913 -4.2802544][-4.3180513 -4.289156 -4.2421069 -4.1662726 -4.0394883 -3.95111 -4.0192046 -4.1259546 -4.189642 -4.230062 -4.2455716 -4.2232 -4.2077293 -4.2303972 -4.2673674][-4.3146734 -4.2896471 -4.246541 -4.1781812 -4.0854239 -4.0389738 -4.0980268 -4.1730075 -4.2119737 -4.2348247 -4.2374325 -4.2040129 -4.1810765 -4.2076778 -4.2527943][-4.3069682 -4.2891588 -4.2570477 -4.2054634 -4.15182 -4.1390285 -4.1904397 -4.2368159 -4.2530861 -4.2625504 -4.2552481 -4.2158918 -4.1843328 -4.2038965 -4.2456632][-4.3045545 -4.29522 -4.2763834 -4.2456951 -4.2199473 -4.2227292 -4.2600389 -4.288671 -4.2965431 -4.3017678 -4.2925172 -4.2571759 -4.2244596 -4.2364845 -4.2661095][-4.3095331 -4.3060656 -4.295855 -4.2811 -4.2695684 -4.2748337 -4.2954717 -4.3120027 -4.3184366 -4.32485 -4.3219824 -4.299829 -4.2749872 -4.2786694 -4.2971811][-4.3165984 -4.3162451 -4.310545 -4.3042235 -4.298759 -4.30132 -4.3109083 -4.3211546 -4.3303804 -4.3385038 -4.3387237 -4.3273888 -4.3142967 -4.3142447 -4.3241568][-4.3205819 -4.321126 -4.3179793 -4.3156834 -4.3120627 -4.3105187 -4.3144484 -4.3205752 -4.3298812 -4.3388333 -4.3444214 -4.3425579 -4.3360095 -4.3344779 -4.3379498][-4.3290148 -4.32936 -4.3264952 -4.3243141 -4.3224106 -4.3206997 -4.3220239 -4.3270178 -4.3352661 -4.3416519 -4.3462219 -4.3482256 -4.3469481 -4.3448267 -4.3443561][-4.3395009 -4.33986 -4.3384175 -4.337008 -4.335844 -4.3338375 -4.3333416 -4.3363838 -4.3411555 -4.3447347 -4.3476281 -4.349874 -4.349864 -4.3483267 -4.3475413]]...]
INFO - root - 2017-12-07 17:23:08.847114: step 30210, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 84h:44m:24s remains)
INFO - root - 2017-12-07 17:23:18.601872: step 30220, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 80h:45m:27s remains)
INFO - root - 2017-12-07 17:23:28.279838: step 30230, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 80h:57m:11s remains)
INFO - root - 2017-12-07 17:23:37.848424: step 30240, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.025 sec/batch; 86h:03m:19s remains)
INFO - root - 2017-12-07 17:23:47.489769: step 30250, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 80h:03m:12s remains)
INFO - root - 2017-12-07 17:23:57.188332: step 30260, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 78h:34m:59s remains)
INFO - root - 2017-12-07 17:24:06.712388: step 30270, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 82h:37m:44s remains)
INFO - root - 2017-12-07 17:24:16.478654: step 30280, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.954 sec/batch; 80h:06m:29s remains)
INFO - root - 2017-12-07 17:24:26.132255: step 30290, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 82h:28m:46s remains)
INFO - root - 2017-12-07 17:24:35.708800: step 30300, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 79h:09m:24s remains)
2017-12-07 17:24:36.673637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3461504 -4.345952 -4.3450623 -4.3450527 -4.34572 -4.3460827 -4.3452668 -4.3435574 -4.3399858 -4.3361983 -4.3343954 -4.3369894 -4.3418679 -4.3480206 -4.3554111][-4.3322053 -4.3322763 -4.3310719 -4.3313441 -4.3329988 -4.3323774 -4.3288021 -4.3244114 -4.3180676 -4.311573 -4.3083453 -4.3135204 -4.3207703 -4.331007 -4.3424907][-4.3187885 -4.3180747 -4.3140945 -4.3103638 -4.3068924 -4.2994933 -4.2890625 -4.2791758 -4.2724037 -4.2675967 -4.2672024 -4.2764211 -4.2880564 -4.3029695 -4.3198071][-4.3068838 -4.3035975 -4.291719 -4.2770977 -4.2610745 -4.2389708 -4.2173252 -4.2032819 -4.2001181 -4.203454 -4.2130427 -4.2323937 -4.2505093 -4.2702112 -4.2925458][-4.2922568 -4.285275 -4.2624207 -4.2314587 -4.1979733 -4.1603856 -4.1323247 -4.1201439 -4.1252341 -4.1424079 -4.1664429 -4.1985626 -4.2229586 -4.2444019 -4.2695985][-4.2757382 -4.2635522 -4.2295713 -4.1800408 -4.1265469 -4.0796509 -4.0617666 -4.0671206 -4.0856586 -4.1141882 -4.1441026 -4.1810408 -4.2075114 -4.2282319 -4.2539835][-4.2576737 -4.2391844 -4.1958423 -4.128778 -4.0608816 -4.0132065 -4.0200329 -4.0527897 -4.0894032 -4.1223807 -4.1471457 -4.1779528 -4.201519 -4.2211041 -4.2460418][-4.2406192 -4.2187295 -4.1704631 -4.0958052 -4.0217147 -3.9750581 -3.9994898 -4.0559082 -4.1069741 -4.1424842 -4.1624484 -4.1846981 -4.2041574 -4.22293 -4.2460809][-4.2350941 -4.2144814 -4.1680007 -4.0971193 -4.0278573 -3.9800222 -4.0030332 -4.0674963 -4.1268282 -4.162838 -4.1804848 -4.1967373 -4.2100716 -4.2273417 -4.2494249][-4.2440228 -4.2274184 -4.1879759 -4.1284466 -4.0732384 -4.0332727 -4.046896 -4.0996914 -4.1536727 -4.1841507 -4.2012453 -4.2145691 -4.2218933 -4.2368236 -4.2576165][-4.2627311 -4.2490511 -4.2184544 -4.1732011 -4.1363425 -4.1097317 -4.11351 -4.1456795 -4.1850529 -4.2072244 -4.2241859 -4.2370396 -4.2451611 -4.2586493 -4.2780547][-4.282155 -4.2708764 -4.2491665 -4.218298 -4.1969137 -4.1810741 -4.1743979 -4.1883135 -4.2148447 -4.2307048 -4.2442288 -4.256753 -4.2686176 -4.2838264 -4.3016696][-4.29941 -4.2912416 -4.2769485 -4.2579775 -4.24713 -4.2391138 -4.2274227 -4.2276177 -4.2425609 -4.2508931 -4.2600975 -4.2754807 -4.2935972 -4.3112617 -4.3249879][-4.3201213 -4.3155284 -4.30705 -4.2953558 -4.289083 -4.2854424 -4.2775455 -4.2735491 -4.279418 -4.281652 -4.2875714 -4.3035388 -4.3226247 -4.3378754 -4.3441987][-4.333859 -4.3319569 -4.3273144 -4.319736 -4.3137364 -4.3112411 -4.3093352 -4.3081732 -4.3123879 -4.3135242 -4.318141 -4.33063 -4.3450503 -4.3555617 -4.3565555]]...]
INFO - root - 2017-12-07 17:24:46.283997: step 30310, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 80h:07m:14s remains)
INFO - root - 2017-12-07 17:24:55.965970: step 30320, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 80h:02m:20s remains)
INFO - root - 2017-12-07 17:25:05.683881: step 30330, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 83h:58m:15s remains)
INFO - root - 2017-12-07 17:25:15.281581: step 30340, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 75h:42m:47s remains)
INFO - root - 2017-12-07 17:25:24.937264: step 30350, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 83h:43m:23s remains)
INFO - root - 2017-12-07 17:25:34.560216: step 30360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 81h:18m:35s remains)
INFO - root - 2017-12-07 17:25:44.204538: step 30370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 81h:26m:09s remains)
INFO - root - 2017-12-07 17:25:53.740782: step 30380, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 78h:01m:43s remains)
INFO - root - 2017-12-07 17:26:03.374893: step 30390, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 78h:26m:27s remains)
INFO - root - 2017-12-07 17:26:13.150821: step 30400, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 82h:50m:53s remains)
2017-12-07 17:26:14.208334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2294292 -4.2371974 -4.2428246 -4.2452917 -4.2466493 -4.2477078 -4.2478995 -4.2459888 -4.2429833 -4.2401061 -4.2379036 -4.2366967 -4.2364011 -4.2355351 -4.2338963][-4.256671 -4.2599959 -4.2633719 -4.2646413 -4.2642646 -4.2631688 -4.2618241 -4.25946 -4.256763 -4.2544351 -4.2522426 -4.2505794 -4.2500339 -4.2493839 -4.248282][-4.2761178 -4.2760897 -4.2773223 -4.2749319 -4.2699838 -4.2660222 -4.2651286 -4.2653689 -4.2664161 -4.2679286 -4.2690239 -4.2696891 -4.2707996 -4.2716308 -4.2720933][-4.2737107 -4.2709179 -4.2675838 -4.2573338 -4.2440329 -4.2353916 -4.2358465 -4.2416434 -4.2496467 -4.258647 -4.2670641 -4.2732759 -4.278121 -4.2815804 -4.28432][-4.245234 -4.240541 -4.2332439 -4.2162871 -4.1951618 -4.1816893 -4.1810217 -4.1898069 -4.2050276 -4.2229962 -4.2403049 -4.25264 -4.260551 -4.2654743 -4.2690573][-4.2083678 -4.2016139 -4.1932449 -4.1759272 -4.1538825 -4.1373272 -4.1305819 -4.1353078 -4.1529026 -4.1773329 -4.2001657 -4.21616 -4.2258286 -4.2310872 -4.2351184][-4.1916385 -4.1817145 -4.1713033 -4.1556897 -4.1341753 -4.1122527 -4.0941157 -4.08941 -4.1070075 -4.1364384 -4.1606932 -4.1761618 -4.1849122 -4.189858 -4.1952443][-4.2047338 -4.1930962 -4.1813908 -4.1679997 -4.1461964 -4.1161332 -4.0843077 -4.0651884 -4.0771489 -4.1061273 -4.1278238 -4.13729 -4.1405983 -4.144002 -4.1525068][-4.2329054 -4.2197776 -4.2063856 -4.1950779 -4.1773157 -4.1487412 -4.1136308 -4.0884151 -4.0925994 -4.1134357 -4.1268024 -4.127306 -4.122345 -4.119648 -4.1268959][-4.2588706 -4.2464352 -4.23209 -4.2206063 -4.2062163 -4.1830239 -4.1537704 -4.1317515 -4.1334696 -4.1459236 -4.1512418 -4.1469021 -4.1382074 -4.131484 -4.1352639][-4.276444 -4.2675524 -4.2553492 -4.2469845 -4.2385254 -4.223896 -4.2041478 -4.1891065 -4.1889782 -4.1930041 -4.190331 -4.1818256 -4.17113 -4.1624241 -4.1628976][-4.2838178 -4.2802482 -4.2723351 -4.2675467 -4.2637873 -4.2562232 -4.245564 -4.2372222 -4.2369866 -4.237247 -4.2330675 -4.2250013 -4.2141724 -4.2046537 -4.2020731][-4.2736912 -4.275291 -4.2735376 -4.2730684 -4.273241 -4.2716269 -4.2676821 -4.2630434 -4.26078 -4.2586021 -4.2565255 -4.2513304 -4.2407231 -4.2308617 -4.2266407][-4.2527404 -4.2574339 -4.2591925 -4.2601018 -4.2606544 -4.2612505 -4.2589159 -4.2540197 -4.249506 -4.247632 -4.2482781 -4.2452455 -4.2357469 -4.2271757 -4.2250714][-4.23622 -4.2421055 -4.2455788 -4.2468095 -4.2459555 -4.244205 -4.2372465 -4.2261763 -4.2173209 -4.2160816 -4.2198291 -4.2203679 -4.2143426 -4.2084455 -4.2088366]]...]
INFO - root - 2017-12-07 17:26:24.001151: step 30410, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 83h:30m:55s remains)
INFO - root - 2017-12-07 17:26:33.568568: step 30420, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 81h:12m:20s remains)
INFO - root - 2017-12-07 17:26:43.244998: step 30430, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 78h:35m:25s remains)
INFO - root - 2017-12-07 17:26:52.974998: step 30440, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 80h:54m:15s remains)
INFO - root - 2017-12-07 17:27:02.679880: step 30450, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 82h:39m:58s remains)
INFO - root - 2017-12-07 17:27:12.445369: step 30460, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 81h:02m:54s remains)
INFO - root - 2017-12-07 17:27:22.093888: step 30470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 80h:11m:27s remains)
INFO - root - 2017-12-07 17:27:31.888234: step 30480, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 83h:12m:32s remains)
INFO - root - 2017-12-07 17:27:41.425559: step 30490, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 80h:31m:15s remains)
INFO - root - 2017-12-07 17:27:50.929725: step 30500, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 78h:10m:22s remains)
2017-12-07 17:27:51.943373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3061962 -4.3108649 -4.317822 -4.3246069 -4.3286214 -4.3262129 -4.3060703 -4.2725096 -4.2531285 -4.2569222 -4.2685528 -4.2430677 -4.172667 -4.1285162 -4.1274662][-4.3176112 -4.3190036 -4.3224478 -4.3240538 -4.3220797 -4.3157797 -4.2981672 -4.2693343 -4.25396 -4.2667403 -4.2899675 -4.273417 -4.2037072 -4.151557 -4.1417656][-4.3243747 -4.3221626 -4.3220119 -4.3165927 -4.3041716 -4.286552 -4.2598834 -4.22028 -4.2074943 -4.2419629 -4.2862277 -4.2868004 -4.2323503 -4.1852179 -4.171103][-4.32492 -4.3195271 -4.3137021 -4.2970581 -4.2712436 -4.2419872 -4.1999369 -4.1351995 -4.1181602 -4.1849208 -4.2582803 -4.281354 -4.2533755 -4.2205238 -4.2026706][-4.3309379 -4.3194818 -4.3001213 -4.2626796 -4.2180328 -4.1761708 -4.1223431 -4.0314436 -4.0060668 -4.109571 -4.2164626 -4.2642593 -4.2634974 -4.2401538 -4.2120295][-4.3361888 -4.3176923 -4.28633 -4.233314 -4.1689682 -4.1114836 -4.0364218 -3.9054146 -3.8679521 -4.0183406 -4.1653666 -4.2412791 -4.2609859 -4.2388053 -4.1936927][-4.3289523 -4.3063803 -4.2672243 -4.2064381 -4.1229334 -4.0432935 -3.9321787 -3.7360339 -3.6834345 -3.8997488 -4.1020932 -4.2069535 -4.2392893 -4.2116709 -4.14141][-4.3142757 -4.2897491 -4.2452178 -4.1804271 -4.0889273 -4.0013733 -3.8775134 -3.6489096 -3.5906782 -3.8385715 -4.0668011 -4.1811957 -4.2135496 -4.1723733 -4.0734754][-4.2930093 -4.2764592 -4.2366409 -4.1766958 -4.0976448 -4.0306253 -3.9456398 -3.7812812 -3.7434597 -3.936866 -4.1197996 -4.2055221 -4.2188716 -4.1574559 -4.0305457][-4.26178 -4.259583 -4.2370415 -4.1954851 -4.1371946 -4.0899725 -4.0402913 -3.9392393 -3.9223897 -4.0671897 -4.1999316 -4.252182 -4.2490144 -4.1757326 -4.0334349][-4.2324443 -4.2448235 -4.2388716 -4.215333 -4.1706572 -4.1291137 -4.0918207 -4.0171738 -4.010601 -4.1315813 -4.2407084 -4.2841864 -4.2835484 -4.2148118 -4.0766821][-4.211237 -4.23381 -4.2390132 -4.2265968 -4.1906734 -4.1532278 -4.121376 -4.0630522 -4.0590582 -4.15464 -4.2434731 -4.2902126 -4.3015518 -4.2480078 -4.1343551][-4.2014303 -4.2222824 -4.2342868 -4.23189 -4.2077589 -4.1826019 -4.165185 -4.1311936 -4.131084 -4.1927972 -4.2532392 -4.2896543 -4.2994208 -4.2615156 -4.1809211][-4.201385 -4.2146368 -4.2295637 -4.2378902 -4.2296796 -4.214632 -4.2046833 -4.1915107 -4.1967373 -4.2327662 -4.2715478 -4.2968154 -4.2993932 -4.2703466 -4.2116385][-4.2059879 -4.2206011 -4.2406716 -4.2608428 -4.2649546 -4.2504349 -4.2356019 -4.226954 -4.229907 -4.2523403 -4.280334 -4.2980719 -4.2970581 -4.2759705 -4.2365313]]...]
INFO - root - 2017-12-07 17:28:01.630696: step 30510, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 81h:31m:59s remains)
INFO - root - 2017-12-07 17:28:11.413890: step 30520, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 82h:43m:53s remains)
INFO - root - 2017-12-07 17:28:20.971872: step 30530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 80h:59m:32s remains)
INFO - root - 2017-12-07 17:28:30.700582: step 30540, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 78h:42m:15s remains)
INFO - root - 2017-12-07 17:28:40.408930: step 30550, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 84h:49m:29s remains)
INFO - root - 2017-12-07 17:28:50.095582: step 30560, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 82h:03m:57s remains)
INFO - root - 2017-12-07 17:28:59.818367: step 30570, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 83h:04m:22s remains)
INFO - root - 2017-12-07 17:29:09.618283: step 30580, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 80h:25m:34s remains)
INFO - root - 2017-12-07 17:29:19.175763: step 30590, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 77h:36m:40s remains)
INFO - root - 2017-12-07 17:29:28.850526: step 30600, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 80h:06m:45s remains)
2017-12-07 17:29:29.753430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3188534 -4.3187842 -4.3156662 -4.3111272 -4.307272 -4.3001633 -4.2916627 -4.2875471 -4.2821083 -4.2688694 -4.2519455 -4.2418833 -4.2461333 -4.2619848 -4.2789655][-4.3425932 -4.3454041 -4.3443809 -4.33594 -4.3225594 -4.3037367 -4.2874846 -4.2833652 -4.2836452 -4.2758403 -4.259685 -4.2465243 -4.24727 -4.2660227 -4.2882328][-4.349762 -4.3544569 -4.3546348 -4.3371291 -4.3061795 -4.2694659 -4.2431269 -4.239512 -4.249908 -4.2549205 -4.2498941 -4.2425485 -4.2417741 -4.2632957 -4.2928524][-4.3358822 -4.3407445 -4.3406892 -4.3163042 -4.2705493 -4.2187548 -4.1837735 -4.186934 -4.2075315 -4.2227154 -4.2290759 -4.2292027 -4.2278261 -4.248632 -4.2827935][-4.3170071 -4.3188581 -4.3174119 -4.2906351 -4.2395196 -4.1731839 -4.1261406 -4.1383119 -4.1808667 -4.2050791 -4.2168832 -4.2176671 -4.2119532 -4.2256012 -4.2598734][-4.3006077 -4.2918692 -4.281148 -4.2469077 -4.1822309 -4.0981231 -4.033772 -4.0630226 -4.1478958 -4.1990228 -4.2172418 -4.2195272 -4.2084289 -4.2093263 -4.2380681][-4.2814646 -4.2596259 -4.2301493 -4.1769195 -4.0896583 -3.9712 -3.8811331 -3.9382925 -4.0821614 -4.1753187 -4.2100058 -4.2177835 -4.2047048 -4.1996779 -4.2270188][-4.2648621 -4.236279 -4.1942358 -4.1288486 -4.01728 -3.8510747 -3.7117939 -3.7842252 -3.9824123 -4.1240931 -4.1786547 -4.1971259 -4.1921268 -4.1872883 -4.2147579][-4.2775607 -4.2599807 -4.222383 -4.1598854 -4.0537572 -3.8827913 -3.7260697 -3.7650356 -3.9500289 -4.1016965 -4.1667929 -4.19235 -4.1946268 -4.18792 -4.2079887][-4.3005791 -4.3050857 -4.2797656 -4.2314939 -4.1539688 -4.0300021 -3.9127681 -3.9061136 -4.0199275 -4.1410389 -4.2041121 -4.2297373 -4.2326708 -4.2227793 -4.2317333][-4.3042097 -4.3314042 -4.3213959 -4.2858739 -4.2289038 -4.1457486 -4.0659447 -4.0414586 -4.1017752 -4.1892786 -4.2467613 -4.2757168 -4.282948 -4.2812362 -4.2878881][-4.2827916 -4.3233294 -4.3285556 -4.3056288 -4.2669444 -4.2084255 -4.1474252 -4.1187468 -4.1515942 -4.2185221 -4.2735009 -4.3054538 -4.3191328 -4.329608 -4.339889][-4.2552147 -4.2956371 -4.3149428 -4.3060474 -4.2850571 -4.2486978 -4.1997004 -4.1673079 -4.1852365 -4.2411952 -4.2940164 -4.324533 -4.3413444 -4.3596663 -4.3718662][-4.2559276 -4.2784095 -4.2998924 -4.3028026 -4.2912326 -4.2707787 -4.2356095 -4.2037354 -4.2088242 -4.2513757 -4.3006077 -4.33443 -4.35639 -4.3764639 -4.3854103][-4.27145 -4.2681971 -4.279201 -4.2875409 -4.278851 -4.2632303 -4.2407746 -4.2156181 -4.2147355 -4.2463703 -4.2932496 -4.3304181 -4.3563352 -4.3732781 -4.3753486]]...]
INFO - root - 2017-12-07 17:29:39.402556: step 30610, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 81h:00m:07s remains)
INFO - root - 2017-12-07 17:29:49.026692: step 30620, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 79h:55m:24s remains)
INFO - root - 2017-12-07 17:29:58.703424: step 30630, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 81h:02m:12s remains)
INFO - root - 2017-12-07 17:30:08.285257: step 30640, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 80h:23m:31s remains)
INFO - root - 2017-12-07 17:30:17.911119: step 30650, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 82h:15m:35s remains)
INFO - root - 2017-12-07 17:30:27.629581: step 30660, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 78h:55m:02s remains)
INFO - root - 2017-12-07 17:30:37.326316: step 30670, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 82h:11m:34s remains)
INFO - root - 2017-12-07 17:30:47.058193: step 30680, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 74h:59m:06s remains)
INFO - root - 2017-12-07 17:30:56.877178: step 30690, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 81h:23m:54s remains)
INFO - root - 2017-12-07 17:31:06.514431: step 30700, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 77h:59m:59s remains)
2017-12-07 17:31:07.446162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3002291 -4.2876468 -4.2827282 -4.2889204 -4.2909951 -4.2853794 -4.2826738 -4.2848959 -4.2880249 -4.2869487 -4.2826185 -4.2799306 -4.27832 -4.2772312 -4.2783442][-4.2770233 -4.264173 -4.26211 -4.2703505 -4.2686586 -4.2569733 -4.2522831 -4.254849 -4.2623405 -4.2655234 -4.2635469 -4.26386 -4.2636352 -4.2617064 -4.263742][-4.2556319 -4.2450166 -4.24711 -4.25173 -4.2402515 -4.2191682 -4.2085013 -4.2072186 -4.2209978 -4.2334461 -4.2398863 -4.2489095 -4.2530732 -4.2520719 -4.2544637][-4.2176881 -4.20636 -4.20784 -4.2085428 -4.1912141 -4.1660447 -4.1518049 -4.1522384 -4.1735997 -4.1974659 -4.2143917 -4.2303252 -4.2394423 -4.2383013 -4.2382536][-4.1646571 -4.1532397 -4.1570921 -4.1609817 -4.149282 -4.1303596 -4.1173325 -4.1150508 -4.1371593 -4.1681304 -4.1900191 -4.2006946 -4.206418 -4.2033534 -4.2024822][-4.1328697 -4.124989 -4.135469 -4.1475248 -4.1407681 -4.1214681 -4.0946951 -4.0719762 -4.0852041 -4.1239023 -4.1513214 -4.1575933 -4.1574559 -4.1555629 -4.1617408][-4.1337261 -4.1291409 -4.1370678 -4.1459222 -4.1325006 -4.1004744 -4.0469937 -3.9883199 -3.9898884 -4.048635 -4.0956793 -4.1095119 -4.1098871 -4.1156688 -4.1339855][-4.1469727 -4.1383748 -4.1365213 -4.1300211 -4.1052332 -4.0619197 -3.9912167 -3.9160025 -3.9254279 -4.0121741 -4.0809011 -4.0977058 -4.0947967 -4.1047955 -4.1307626][-4.1462722 -4.1247983 -4.1122189 -4.1005135 -4.0818505 -4.0526323 -4.0115972 -3.9790537 -4.0043497 -4.0787597 -4.1337047 -4.1394162 -4.1246576 -4.1264458 -4.1487904][-4.135406 -4.10958 -4.1062913 -4.1110353 -4.106369 -4.091475 -4.07488 -4.0657253 -4.0883746 -4.1406145 -4.1763029 -4.1703763 -4.1473188 -4.1429005 -4.164228][-4.1483145 -4.1290359 -4.1401386 -4.1587486 -4.1597581 -4.1475368 -4.1351433 -4.1291938 -4.1449552 -4.1795921 -4.2040453 -4.1937423 -4.1685123 -4.164207 -4.1862593][-4.1901979 -4.1779628 -4.1893 -4.2080917 -4.2135382 -4.2044425 -4.18984 -4.1826892 -4.1931639 -4.2155447 -4.2328992 -4.2282896 -4.2106385 -4.2069631 -4.2271409][-4.242857 -4.2372379 -4.2457433 -4.25789 -4.2622519 -4.2533727 -4.2426705 -4.2401233 -4.2469978 -4.2587271 -4.2690549 -4.2686615 -4.2600269 -4.2598014 -4.275671][-4.2826986 -4.2797194 -4.2865539 -4.2962828 -4.299788 -4.2942095 -4.2894378 -4.291729 -4.2962875 -4.3006635 -4.3040376 -4.3020463 -4.2977395 -4.2993541 -4.3085976][-4.3103223 -4.3061185 -4.3095841 -4.3169513 -4.3202333 -4.318974 -4.3181233 -4.3218179 -4.3258715 -4.3275342 -4.3272791 -4.3242812 -4.3202162 -4.3192649 -4.3226089]]...]
INFO - root - 2017-12-07 17:31:16.988217: step 30710, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 82h:27m:29s remains)
INFO - root - 2017-12-07 17:31:26.628452: step 30720, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 82h:40m:30s remains)
INFO - root - 2017-12-07 17:31:36.332135: step 30730, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 81h:27m:59s remains)
INFO - root - 2017-12-07 17:31:45.992097: step 30740, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 84h:01m:42s remains)
INFO - root - 2017-12-07 17:31:55.704065: step 30750, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 82h:23m:04s remains)
INFO - root - 2017-12-07 17:32:05.441716: step 30760, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.974 sec/batch; 81h:35m:55s remains)
INFO - root - 2017-12-07 17:32:14.972719: step 30770, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 78h:48m:51s remains)
INFO - root - 2017-12-07 17:32:24.366298: step 30780, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.871 sec/batch; 73h:00m:55s remains)
INFO - root - 2017-12-07 17:32:34.100853: step 30790, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 82h:38m:42s remains)
INFO - root - 2017-12-07 17:32:43.913208: step 30800, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 80h:15m:40s remains)
2017-12-07 17:32:44.870148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3121562 -4.3004074 -4.2900023 -4.2839551 -4.2775927 -4.2718868 -4.2654552 -4.2556376 -4.2415385 -4.2350707 -4.238306 -4.2398739 -4.2379966 -4.2381592 -4.2467661][-4.2987542 -4.2826548 -4.2716894 -4.2635794 -4.2527781 -4.2420511 -4.2320242 -4.2166882 -4.198946 -4.1935177 -4.2036109 -4.2146358 -4.2141523 -4.2122269 -4.2202611][-4.288867 -4.2725015 -4.2625365 -4.2525864 -4.2367554 -4.2183156 -4.1999745 -4.1780429 -4.1603289 -4.1589108 -4.1762805 -4.1975927 -4.1975985 -4.1902461 -4.1943812][-4.2823234 -4.26717 -4.2574382 -4.2452903 -4.2241836 -4.1990767 -4.1728172 -4.1435976 -4.1259112 -4.1297951 -4.1557384 -4.18531 -4.1841793 -4.1710019 -4.1685562][-4.2839355 -4.2714252 -4.2622871 -4.2498364 -4.2237849 -4.189662 -4.1559181 -4.1166883 -4.0933266 -4.1062765 -4.1435547 -4.1784739 -4.177196 -4.1622157 -4.1507421][-4.2870932 -4.2774138 -4.2686796 -4.2562637 -4.22648 -4.1824975 -4.1297045 -4.061204 -4.0200553 -4.0449872 -4.1049185 -4.1528168 -4.1585279 -4.1439118 -4.1250567][-4.2825313 -4.2744322 -4.2630649 -4.247683 -4.2124772 -4.1600776 -4.0896072 -3.9884317 -3.9200752 -3.9595113 -4.0472736 -4.1139493 -4.1318 -4.1218386 -4.1014485][-4.2690654 -4.2584419 -4.2440343 -4.2272143 -4.1905046 -4.1404614 -4.0760403 -3.9726744 -3.8932381 -3.93764 -4.0326695 -4.0993333 -4.1235337 -4.1175823 -4.1003842][-4.2475123 -4.2320666 -4.2190084 -4.2041659 -4.1696806 -4.1322732 -4.0910058 -4.023962 -3.9751828 -4.0133586 -4.0810204 -4.1287107 -4.1442313 -4.1375856 -4.1228213][-4.2433362 -4.2279367 -4.2206969 -4.20762 -4.1793447 -4.15244 -4.1292262 -4.0931454 -4.0674863 -4.0966096 -4.1391354 -4.1710491 -4.1796393 -4.1696067 -4.1569481][-4.2620416 -4.2507696 -4.2474556 -4.2368159 -4.2149243 -4.1966124 -4.1825733 -4.1600404 -4.1429868 -4.1617689 -4.1909041 -4.2110486 -4.2152543 -4.2072973 -4.1989651][-4.2880764 -4.2797589 -4.278347 -4.2694721 -4.2529597 -4.2377725 -4.2276239 -4.2088122 -4.1954908 -4.2155623 -4.2385969 -4.2498879 -4.2506776 -4.2454348 -4.2379541][-4.3031635 -4.2982383 -4.3006711 -4.2969851 -4.2854819 -4.2740259 -4.2675648 -4.2541022 -4.2460217 -4.2643285 -4.2821364 -4.2875109 -4.2853527 -4.2824416 -4.2742624][-4.3116226 -4.3109536 -4.31803 -4.3202081 -4.3135591 -4.3070741 -4.3063712 -4.3016291 -4.2976255 -4.3068767 -4.3194785 -4.3227143 -4.31878 -4.3164816 -4.3097243][-4.3162079 -4.3147373 -4.320632 -4.325758 -4.3241968 -4.3219566 -4.3247685 -4.3236685 -4.3206506 -4.3233442 -4.3300133 -4.3336639 -4.3317747 -4.3296928 -4.325387]]...]
INFO - root - 2017-12-07 17:32:54.498029: step 30810, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 79h:43m:15s remains)
INFO - root - 2017-12-07 17:33:04.084881: step 30820, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 79h:24m:31s remains)
INFO - root - 2017-12-07 17:33:13.890548: step 30830, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 80h:51m:13s remains)
INFO - root - 2017-12-07 17:33:23.606532: step 30840, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 77h:02m:55s remains)
INFO - root - 2017-12-07 17:33:33.256080: step 30850, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.045 sec/batch; 87h:33m:40s remains)
INFO - root - 2017-12-07 17:33:43.073683: step 30860, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 82h:16m:16s remains)
INFO - root - 2017-12-07 17:33:52.780991: step 30870, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 80h:32m:50s remains)
INFO - root - 2017-12-07 17:34:02.380180: step 30880, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.018 sec/batch; 85h:17m:53s remains)
INFO - root - 2017-12-07 17:34:11.862896: step 30890, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 80h:41m:30s remains)
INFO - root - 2017-12-07 17:34:21.488746: step 30900, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 81h:05m:44s remains)
2017-12-07 17:34:22.418734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393131 -4.24706 -4.2650266 -4.2744732 -4.2705178 -4.2569923 -4.2508583 -4.2594919 -4.2646527 -4.2668428 -4.2691493 -4.2772732 -4.2780132 -4.2721858 -4.2697654][-4.2412891 -4.2415862 -4.2539082 -4.2638164 -4.2617297 -4.250927 -4.2443194 -4.2477884 -4.2542267 -4.2683053 -4.2794118 -4.2868485 -4.28085 -4.2648745 -4.2557378][-4.2402415 -4.23716 -4.2432914 -4.2486606 -4.2445083 -4.2325678 -4.2247519 -4.2221293 -4.2333808 -4.2641177 -4.2873611 -4.2959747 -4.2819242 -4.258985 -4.2521682][-4.2386856 -4.2351789 -4.2377825 -4.2400079 -4.2331986 -4.21547 -4.1998391 -4.19171 -4.2064338 -4.2509427 -4.2810836 -4.2890682 -4.272223 -4.2505536 -4.2505445][-4.229301 -4.2238145 -4.2235761 -4.2221828 -4.2150006 -4.1923909 -4.1693072 -4.1567063 -4.1737533 -4.2257614 -4.2579918 -4.2665277 -4.2519517 -4.235043 -4.2441406][-4.2270741 -4.2235541 -4.2227716 -4.2200193 -4.2101769 -4.1781054 -4.1387405 -4.1153193 -4.1302104 -4.185956 -4.2257128 -4.2372341 -4.2236547 -4.2057381 -4.2229338][-4.2523336 -4.2521973 -4.2537889 -4.2476273 -4.2267227 -4.1798 -4.1174083 -4.0780997 -4.089139 -4.1474552 -4.1962466 -4.2112856 -4.1931138 -4.1666527 -4.1861324][-4.2836266 -4.2848463 -4.2877026 -4.2776833 -4.2465034 -4.1891861 -4.116044 -4.0682487 -4.0788331 -4.1348367 -4.1848526 -4.1939392 -4.1653075 -4.1252966 -4.1382518][-4.3173418 -4.3183107 -4.3204474 -4.3070464 -4.2719421 -4.2141604 -4.1429129 -4.0968146 -4.1049423 -4.1523824 -4.1921797 -4.1909823 -4.1559639 -4.1017241 -4.1022739][-4.3393173 -4.3396645 -4.3412905 -4.3259306 -4.2911983 -4.2389369 -4.1764159 -4.1335311 -4.1357465 -4.1690726 -4.1963768 -4.1909828 -4.15955 -4.1072416 -4.0980744][-4.3521194 -4.3490458 -4.3466811 -4.333055 -4.3029976 -4.2564549 -4.2020617 -4.1607013 -4.1521425 -4.1689754 -4.1860404 -4.183382 -4.1624503 -4.1217117 -4.1092448][-4.3609667 -4.356184 -4.351728 -4.3419027 -4.3195844 -4.2806635 -4.2320666 -4.1892486 -4.1677661 -4.1681619 -4.1803675 -4.1851873 -4.1773849 -4.1515379 -4.1382675][-4.3646317 -4.3602419 -4.3563004 -4.3506289 -4.3365383 -4.3090377 -4.2695379 -4.230629 -4.2036095 -4.1929398 -4.2002063 -4.2069616 -4.2064033 -4.190423 -4.1772242][-4.3629193 -4.3602543 -4.3578434 -4.3561811 -4.3495493 -4.3327823 -4.3052177 -4.2769241 -4.2565055 -4.2443118 -4.2462459 -4.2487059 -4.2442431 -4.2292519 -4.2156229][-4.3630095 -4.3625097 -4.3619227 -4.3630595 -4.3618569 -4.3526349 -4.3354411 -4.3177366 -4.3069854 -4.2996931 -4.2990112 -4.2972693 -4.2872272 -4.2708788 -4.2567129]]...]
INFO - root - 2017-12-07 17:34:32.149500: step 30910, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 78h:28m:19s remains)
INFO - root - 2017-12-07 17:34:41.886636: step 30920, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 83h:36m:31s remains)
INFO - root - 2017-12-07 17:34:51.629103: step 30930, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 83h:33m:11s remains)
INFO - root - 2017-12-07 17:35:01.159222: step 30940, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 80h:27m:57s remains)
INFO - root - 2017-12-07 17:35:10.894424: step 30950, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 76h:29m:46s remains)
INFO - root - 2017-12-07 17:35:20.640521: step 30960, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 80h:15m:55s remains)
INFO - root - 2017-12-07 17:35:30.217476: step 30970, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.979 sec/batch; 82h:02m:18s remains)
INFO - root - 2017-12-07 17:35:39.717130: step 30980, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 80h:23m:37s remains)
INFO - root - 2017-12-07 17:35:49.320629: step 30990, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 82h:31m:53s remains)
INFO - root - 2017-12-07 17:35:59.126395: step 31000, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 80h:47m:55s remains)
2017-12-07 17:36:00.049809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.334404 -4.3352876 -4.3345118 -4.333477 -4.3325014 -4.3316622 -4.3310075 -4.330749 -4.3297462 -4.3281374 -4.3266754 -4.3267183 -4.3277316 -4.3285341 -4.3283796][-4.3386707 -4.3367567 -4.3348317 -4.3311362 -4.3282232 -4.3269663 -4.3259969 -4.3259311 -4.3237624 -4.3191509 -4.3154798 -4.3148828 -4.3177152 -4.3209333 -4.3224154][-4.3209276 -4.3208084 -4.3181405 -4.3083773 -4.3011446 -4.2953739 -4.2903757 -4.2896276 -4.2887182 -4.2841425 -4.2802935 -4.2815418 -4.2896843 -4.2998714 -4.306663][-4.286592 -4.2923355 -4.2865858 -4.26505 -4.2464452 -4.2268476 -4.2099524 -4.2100258 -4.2182136 -4.2202849 -4.2207937 -4.22959 -4.2499394 -4.272387 -4.2868714][-4.2285609 -4.2466207 -4.2383037 -4.2002115 -4.1575251 -4.1091475 -4.0708961 -4.0763178 -4.1058559 -4.1235356 -4.13648 -4.1626639 -4.202055 -4.2401929 -4.2639918][-4.1568651 -4.19599 -4.1942253 -4.1458344 -4.0712914 -3.9783812 -3.9057181 -3.9217107 -3.9867179 -4.0337491 -4.072494 -4.1242514 -4.1807394 -4.2261462 -4.2524862][-4.1082029 -4.1719966 -4.1829319 -4.1331186 -4.0333047 -3.8988891 -3.7953253 -3.823606 -3.9250104 -4.0074978 -4.0746741 -4.1442494 -4.2035651 -4.2413626 -4.2585988][-4.1143146 -4.1840835 -4.2006273 -4.1550937 -4.0533071 -3.9208856 -3.8312862 -3.8674223 -3.9701169 -4.0573692 -4.128963 -4.1968985 -4.2455626 -4.26942 -4.274909][-4.1586237 -4.217813 -4.2337694 -4.1969805 -4.1111994 -4.0088325 -3.9550364 -3.9886467 -4.0658584 -4.1342468 -4.1928787 -4.2485576 -4.2829485 -4.2935772 -4.2904282][-4.2161484 -4.2638755 -4.2781229 -4.2519259 -4.1890821 -4.1192541 -4.090734 -4.1145906 -4.1611633 -4.2060275 -4.246531 -4.2838216 -4.3040357 -4.3054404 -4.2988482][-4.2630625 -4.2991009 -4.3110704 -4.2944031 -4.2535582 -4.2108006 -4.1971478 -4.2106 -4.2334352 -4.258554 -4.2819796 -4.3023014 -4.3119345 -4.3092065 -4.3030124][-4.2935233 -4.3164105 -4.3219118 -4.3103695 -4.2868366 -4.2647285 -4.260685 -4.268405 -4.2777853 -4.2891583 -4.3008289 -4.3105764 -4.3129086 -4.3083215 -4.3029423][-4.3098593 -4.3219557 -4.3214588 -4.313406 -4.30102 -4.2922964 -4.2937956 -4.2977924 -4.2999811 -4.3033781 -4.3086925 -4.31326 -4.3126845 -4.3082304 -4.3031797][-4.3133483 -4.3179379 -4.314847 -4.3108048 -4.3061938 -4.30399 -4.3059478 -4.306674 -4.3053861 -4.3050814 -4.3072596 -4.309958 -4.3087707 -4.3049068 -4.3000331][-4.3137326 -4.3140693 -4.3094478 -4.3067913 -4.3051939 -4.3052549 -4.3065147 -4.3056717 -4.3034678 -4.301959 -4.3025961 -4.3037806 -4.3017182 -4.2979593 -4.2938275]]...]
INFO - root - 2017-12-07 17:36:09.615236: step 31010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 78h:33m:16s remains)
INFO - root - 2017-12-07 17:36:19.101309: step 31020, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 83h:17m:05s remains)
INFO - root - 2017-12-07 17:36:28.743495: step 31030, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 78h:06m:34s remains)
INFO - root - 2017-12-07 17:36:38.394635: step 31040, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 77h:53m:05s remains)
INFO - root - 2017-12-07 17:36:48.201478: step 31050, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 83h:51m:49s remains)
INFO - root - 2017-12-07 17:36:57.911818: step 31060, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 78h:50m:46s remains)
INFO - root - 2017-12-07 17:37:07.459675: step 31070, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 72h:55m:44s remains)
INFO - root - 2017-12-07 17:37:17.065381: step 31080, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 82h:21m:50s remains)
INFO - root - 2017-12-07 17:37:26.669824: step 31090, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 79h:32m:59s remains)
INFO - root - 2017-12-07 17:37:36.414689: step 31100, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 78h:20m:26s remains)
2017-12-07 17:37:37.361910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.166925 -4.1722164 -4.1713815 -4.1615219 -4.1520867 -4.17332 -4.2307053 -4.2787585 -4.2893176 -4.2743306 -4.2376227 -4.2017989 -4.1869473 -4.210835 -4.2394094][-4.1670256 -4.1675844 -4.1594949 -4.1494746 -4.1449227 -4.1663475 -4.2132683 -4.2539792 -4.2686615 -4.267014 -4.2452488 -4.2175984 -4.2007527 -4.2138839 -4.2349215][-4.1789608 -4.1725078 -4.1584187 -4.1450043 -4.1452055 -4.162631 -4.1902342 -4.2149029 -4.2320871 -4.2432508 -4.2423935 -4.2307506 -4.2157664 -4.2143865 -4.2255435][-4.1901073 -4.1793933 -4.1605268 -4.1454678 -4.1476021 -4.1553559 -4.1621366 -4.1712208 -4.19187 -4.2133079 -4.2305894 -4.2352085 -4.2261147 -4.2161236 -4.2212462][-4.1919475 -4.183157 -4.1639719 -4.1509619 -4.1477337 -4.1393213 -4.1246142 -4.1229987 -4.1498728 -4.1851339 -4.2168407 -4.2319026 -4.2286568 -4.214138 -4.2202549][-4.1974807 -4.1919041 -4.17621 -4.161962 -4.1461515 -4.1185336 -4.0828953 -4.0740814 -4.116456 -4.1683159 -4.2060356 -4.2186775 -4.2131486 -4.2008739 -4.2162609][-4.2058945 -4.2053437 -4.1945248 -4.1762404 -4.1433611 -4.0938005 -4.0453067 -4.0421343 -4.103056 -4.1683755 -4.2056522 -4.2065516 -4.1915364 -4.18236 -4.20771][-4.224256 -4.2268553 -4.2192249 -4.1955113 -4.1394172 -4.0646849 -4.0182462 -4.0368896 -4.1166468 -4.1881056 -4.2218885 -4.2115579 -4.188796 -4.1782627 -4.2068763][-4.2483487 -4.2558093 -4.2485127 -4.2180619 -4.1443005 -4.049439 -4.0128088 -4.0579133 -4.1467266 -4.2126513 -4.2374229 -4.2198405 -4.1974382 -4.1926484 -4.22134][-4.2616835 -4.2741346 -4.265636 -4.2315149 -4.1538525 -4.0597453 -4.0367336 -4.0967526 -4.1781259 -4.2285457 -4.2387557 -4.2188225 -4.2063055 -4.21463 -4.2432404][-4.2683325 -4.2839775 -4.2743998 -4.2420716 -4.1717138 -4.0965366 -4.090313 -4.1488886 -4.2103233 -4.2419624 -4.2362938 -4.2151418 -4.2129016 -4.233407 -4.2615962][-4.275094 -4.2887559 -4.2773051 -4.2488151 -4.1938229 -4.1438246 -4.1516328 -4.19851 -4.2407284 -4.2539878 -4.2355037 -4.2152672 -4.2223482 -4.2490582 -4.2721491][-4.2624936 -4.2728133 -4.2620425 -4.2413254 -4.2063427 -4.179563 -4.1939359 -4.2265859 -4.2527542 -4.25299 -4.2323565 -4.2174516 -4.230042 -4.2554941 -4.2735109][-4.2472467 -4.2555127 -4.2483521 -4.2352338 -4.21636 -4.2047029 -4.2202849 -4.2402844 -4.2531905 -4.2461123 -4.2284517 -4.2208204 -4.2350593 -4.2547688 -4.2686315][-4.2484889 -4.255703 -4.25048 -4.2404432 -4.228579 -4.2234874 -4.2373505 -4.2513809 -4.257091 -4.2480087 -4.2358236 -4.2333088 -4.243845 -4.2566452 -4.26567]]...]
INFO - root - 2017-12-07 17:37:46.975911: step 31110, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 80h:34m:14s remains)
INFO - root - 2017-12-07 17:37:56.754851: step 31120, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 82h:37m:11s remains)
INFO - root - 2017-12-07 17:38:06.399976: step 31130, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 75h:06m:58s remains)
INFO - root - 2017-12-07 17:38:16.040263: step 31140, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 82h:58m:46s remains)
INFO - root - 2017-12-07 17:38:25.607105: step 31150, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 83h:34m:40s remains)
INFO - root - 2017-12-07 17:38:35.209170: step 31160, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.993 sec/batch; 83h:09m:00s remains)
INFO - root - 2017-12-07 17:38:44.683476: step 31170, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 81h:00m:05s remains)
INFO - root - 2017-12-07 17:38:54.167215: step 31180, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 81h:26m:18s remains)
INFO - root - 2017-12-07 17:39:03.619142: step 31190, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.903 sec/batch; 75h:32m:22s remains)
INFO - root - 2017-12-07 17:39:13.342615: step 31200, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 78h:43m:39s remains)
2017-12-07 17:39:14.334609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3044906 -4.3122144 -4.3119297 -4.3078909 -4.3023472 -4.2943544 -4.2872491 -4.2821083 -4.2787061 -4.2793784 -4.2845192 -4.2796659 -4.2597094 -4.2426238 -4.224164][-4.2964206 -4.3006487 -4.2961965 -4.2850032 -4.2713823 -4.2541704 -4.2383957 -4.2282333 -4.227901 -4.2415276 -4.258729 -4.2619104 -4.2451563 -4.2309184 -4.2139082][-4.2886138 -4.288722 -4.2807341 -4.2621441 -4.2402821 -4.2123265 -4.1862698 -4.16739 -4.1697974 -4.2011847 -4.2404552 -4.2576275 -4.2489867 -4.235558 -4.2172008][-4.2839122 -4.2866726 -4.2811456 -4.2577081 -4.2241287 -4.1818924 -4.1398134 -4.1030416 -4.1049185 -4.1620359 -4.2337337 -4.2733765 -4.2752042 -4.2575574 -4.2331095][-4.2807131 -4.2894688 -4.2926903 -4.2700653 -4.2231088 -4.1555266 -4.08029 -4.0145931 -4.0191765 -4.1136079 -4.2241478 -4.2889338 -4.3016405 -4.280067 -4.25007][-4.2756371 -4.2909203 -4.30155 -4.2813764 -4.2184515 -4.1164827 -3.9868145 -3.8723135 -3.8834796 -4.0334711 -4.1927519 -4.2854872 -4.3136268 -4.2980552 -4.2643681][-4.2709556 -4.2919955 -4.307313 -4.2869945 -4.2081347 -4.0674715 -3.8759356 -3.7049294 -3.7256143 -3.9387734 -4.1462173 -4.2671962 -4.3139987 -4.3058462 -4.2680387][-4.2585835 -4.2885141 -4.3096666 -4.2921066 -4.2092261 -4.0556526 -3.8348231 -3.6312823 -3.6566515 -3.9011168 -4.1271362 -4.2566667 -4.31312 -4.3079162 -4.2618704][-4.2424035 -4.2828279 -4.3125753 -4.3061156 -4.23917 -4.1110959 -3.9233623 -3.7524345 -3.7682452 -3.9708176 -4.1632833 -4.2702613 -4.3142614 -4.3043251 -4.2505393][-4.2163453 -4.2614412 -4.2980089 -4.305151 -4.265419 -4.1812439 -4.0512505 -3.9362724 -3.9442322 -4.0796442 -4.2152348 -4.286797 -4.3107128 -4.2901392 -4.2338552][-4.2013803 -4.2416258 -4.2794642 -4.2955942 -4.2796659 -4.2316751 -4.1491776 -4.0781307 -4.0851932 -4.1706262 -4.2567968 -4.2971187 -4.303021 -4.275878 -4.2230573][-4.2173824 -4.2490654 -4.2811975 -4.2971921 -4.2937918 -4.2674627 -4.2158051 -4.1708789 -4.1775579 -4.232563 -4.2895789 -4.313787 -4.3113623 -4.2837439 -4.2381287][-4.2479277 -4.2714305 -4.2973652 -4.3093395 -4.3075891 -4.2928915 -4.26253 -4.2355771 -4.2411213 -4.2760916 -4.31276 -4.3272729 -4.3219123 -4.2988029 -4.2633033][-4.2868977 -4.3033319 -4.3221273 -4.3300405 -4.3285332 -4.3203073 -4.3057752 -4.2919927 -4.2940207 -4.3135571 -4.3338709 -4.3381166 -4.329875 -4.312037 -4.2881894][-4.3120027 -4.3212428 -4.3306842 -4.3332582 -4.3327904 -4.3318844 -4.3295264 -4.3249846 -4.3250675 -4.3342361 -4.3422904 -4.3396215 -4.3303165 -4.31786 -4.3039546]]...]
INFO - root - 2017-12-07 17:39:23.951505: step 31210, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 82h:05m:38s remains)
INFO - root - 2017-12-07 17:39:33.558670: step 31220, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 79h:20m:39s remains)
INFO - root - 2017-12-07 17:39:43.129705: step 31230, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 79h:50m:33s remains)
INFO - root - 2017-12-07 17:39:52.872996: step 31240, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.004 sec/batch; 83h:59m:17s remains)
INFO - root - 2017-12-07 17:40:02.484069: step 31250, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 79h:15m:53s remains)
INFO - root - 2017-12-07 17:40:12.156888: step 31260, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.946 sec/batch; 79h:11m:56s remains)
INFO - root - 2017-12-07 17:40:21.684764: step 31270, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 80h:51m:57s remains)
INFO - root - 2017-12-07 17:40:31.350566: step 31280, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 79h:07m:50s remains)
INFO - root - 2017-12-07 17:40:41.080816: step 31290, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 80h:52m:33s remains)
INFO - root - 2017-12-07 17:40:50.750870: step 31300, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 80h:03m:14s remains)
2017-12-07 17:40:51.776012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1701684 -4.1692219 -4.1654387 -4.1457729 -4.1342683 -4.1468296 -4.1708541 -4.1900849 -4.1927667 -4.1853585 -4.1718526 -4.1728821 -4.189321 -4.1967506 -4.1863461][-4.1295948 -4.1317096 -4.1421776 -4.1378427 -4.1369548 -4.1511059 -4.1697493 -4.1827035 -4.18298 -4.174819 -4.1626806 -4.1664848 -4.1800857 -4.1841016 -4.172627][-4.1132135 -4.1147304 -4.1381578 -4.1525679 -4.1657572 -4.1835475 -4.1948795 -4.1969714 -4.1904535 -4.1798606 -4.1698585 -4.1763229 -4.1849761 -4.1848822 -4.1753125][-4.1125937 -4.1167555 -4.1456108 -4.170702 -4.1886511 -4.2041159 -4.2082815 -4.1974959 -4.1832166 -4.1727967 -4.1699448 -4.1866069 -4.2008691 -4.2057624 -4.2025876][-4.1275373 -4.1350865 -4.1641951 -4.1920075 -4.2077088 -4.2133632 -4.2057543 -4.1800346 -4.1552038 -4.1455803 -4.1529756 -4.1843548 -4.2159834 -4.2342463 -4.238276][-4.1595654 -4.1705208 -4.1948013 -4.2180882 -4.2290554 -4.2207041 -4.1927247 -4.1475134 -4.1141772 -4.1077771 -4.1268787 -4.1711326 -4.2178435 -4.2460284 -4.2513065][-4.1857414 -4.1990972 -4.2176566 -4.2355642 -4.2403531 -4.2180376 -4.1721663 -4.1111145 -4.071991 -4.0681257 -4.097137 -4.1532016 -4.2131944 -4.2464838 -4.2449741][-4.2187643 -4.2285767 -4.238409 -4.2509146 -4.2474017 -4.2107821 -4.1492152 -4.0808959 -4.0416875 -4.0410309 -4.0798869 -4.1434841 -4.2116213 -4.2489052 -4.2453508][-4.2505984 -4.2545972 -4.2578597 -4.2641368 -4.2542262 -4.2120814 -4.1444883 -4.0798359 -4.0518947 -4.0602441 -4.0993838 -4.156086 -4.2177429 -4.2540655 -4.2519703][-4.2700691 -4.26589 -4.2607555 -4.2635689 -4.2588267 -4.2274384 -4.1731596 -4.1248331 -4.1125336 -4.1230149 -4.1482973 -4.1845574 -4.2271767 -4.2552352 -4.249692][-4.2760072 -4.2633348 -4.2490945 -4.2474518 -4.2504168 -4.2371254 -4.2080374 -4.1811142 -4.1773667 -4.1836691 -4.1958947 -4.2152634 -4.237555 -4.2503128 -4.2400594][-4.2711725 -4.2538719 -4.2364283 -4.2324953 -4.2376728 -4.2349181 -4.2241583 -4.2127957 -4.2109823 -4.2126384 -4.2182169 -4.231133 -4.2402425 -4.2411366 -4.2281747][-4.2516079 -4.231802 -4.2135382 -4.2072206 -4.2103944 -4.2100639 -4.2080421 -4.2061291 -4.2045026 -4.2000861 -4.2003965 -4.2106152 -4.21449 -4.209146 -4.1952953][-4.2216144 -4.2024307 -4.1867 -4.1814833 -4.1844387 -4.1855907 -4.1884379 -4.1905165 -4.187604 -4.179678 -4.1773977 -4.1833978 -4.1823835 -4.1721168 -4.1581569][-4.2209525 -4.20539 -4.1951337 -4.1925817 -4.1961951 -4.1989632 -4.2024946 -4.2020311 -4.1959472 -4.1881428 -4.1857328 -4.1884036 -4.1822128 -4.1675539 -4.1543169]]...]
INFO - root - 2017-12-07 17:41:01.371448: step 31310, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 81h:25m:16s remains)
INFO - root - 2017-12-07 17:41:10.902214: step 31320, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.924 sec/batch; 77h:18m:36s remains)
INFO - root - 2017-12-07 17:41:20.587256: step 31330, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 81h:59m:23s remains)
INFO - root - 2017-12-07 17:41:30.405001: step 31340, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.018 sec/batch; 85h:11m:01s remains)
INFO - root - 2017-12-07 17:41:39.940718: step 31350, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 79h:48m:36s remains)
INFO - root - 2017-12-07 17:41:49.644788: step 31360, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 78h:23m:32s remains)
INFO - root - 2017-12-07 17:41:59.256406: step 31370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 81h:58m:36s remains)
INFO - root - 2017-12-07 17:42:09.063786: step 31380, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 83h:04m:29s remains)
INFO - root - 2017-12-07 17:42:18.583590: step 31390, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 80h:39m:02s remains)
INFO - root - 2017-12-07 17:42:28.353311: step 31400, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.000 sec/batch; 83h:39m:10s remains)
2017-12-07 17:42:29.315108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3316064 -4.33415 -4.3375535 -4.3318787 -4.3195195 -4.3050489 -4.2876544 -4.2783093 -4.2770114 -4.2837453 -4.3006735 -4.3207417 -4.3341842 -4.3280048 -4.3045516][-4.3430195 -4.3436341 -4.3413544 -4.3302417 -4.3136582 -4.2948365 -4.27392 -4.2628403 -4.262126 -4.2689934 -4.2880588 -4.31149 -4.3259869 -4.3209672 -4.2984982][-4.3531761 -4.3531485 -4.3462911 -4.3287349 -4.3076978 -4.28554 -4.2611761 -4.2503548 -4.2516322 -4.2591949 -4.278677 -4.3021288 -4.3145189 -4.307631 -4.2821431][-4.3538485 -4.3501368 -4.3380527 -4.3141956 -4.2887292 -4.2625403 -4.2341852 -4.2235518 -4.2278919 -4.2374349 -4.2573342 -4.28086 -4.2892318 -4.2812638 -4.2592411][-4.3453074 -4.3333368 -4.3135214 -4.2850804 -4.2569046 -4.2261586 -4.1904755 -4.17754 -4.1890469 -4.2040968 -4.2243056 -4.2468424 -4.2517056 -4.245738 -4.2363195][-4.3342381 -4.3139539 -4.2871013 -4.2493591 -4.2107172 -4.171669 -4.1267118 -4.1106458 -4.1360068 -4.167356 -4.1971197 -4.225369 -4.2323503 -4.2254219 -4.222827][-4.3199911 -4.2945042 -4.2619843 -4.2124424 -4.1581903 -4.108047 -4.0564513 -4.0409932 -4.0852151 -4.1404929 -4.1818781 -4.2136559 -4.2229729 -4.2129474 -4.2071252][-4.3060608 -4.2817879 -4.2494016 -4.1980333 -4.1367779 -4.0803776 -4.0318508 -4.0181727 -4.066474 -4.1316757 -4.17451 -4.1988606 -4.20011 -4.1834903 -4.1683221][-4.2950616 -4.2729235 -4.2474174 -4.2092648 -4.1586461 -4.1059217 -4.0691524 -4.0616994 -4.0975356 -4.1470003 -4.1768332 -4.1849418 -4.1700397 -4.1455441 -4.1227241][-4.2841907 -4.2634635 -4.2464261 -4.2240429 -4.1893816 -4.1449404 -4.1186705 -4.1148529 -4.1369476 -4.1641784 -4.1762042 -4.1683068 -4.1393518 -4.1064563 -4.0796547][-4.2746954 -4.2541647 -4.2420306 -4.228292 -4.2058377 -4.1705546 -4.1482406 -4.1404195 -4.1501784 -4.1616445 -4.16207 -4.1468806 -4.1120887 -4.0754776 -4.0515852][-4.2721233 -4.2534895 -4.24716 -4.239881 -4.2284164 -4.2028742 -4.1791286 -4.1639967 -4.1627593 -4.1597033 -4.1509013 -4.1345625 -4.1042151 -4.0702219 -4.0489883][-4.2773128 -4.2642622 -4.2663317 -4.2650275 -4.2574139 -4.2370267 -4.2137694 -4.1967397 -4.18959 -4.180479 -4.165164 -4.1474648 -4.1260462 -4.101429 -4.0838041][-4.2883348 -4.2785025 -4.2845812 -4.2848415 -4.2760282 -4.25651 -4.2379212 -4.22326 -4.215178 -4.2078204 -4.1946468 -4.1822138 -4.1742353 -4.1636968 -4.1531816][-4.3006082 -4.2903991 -4.2949266 -4.2927775 -4.2806158 -4.2607312 -4.2474279 -4.2382836 -4.2341809 -4.2320294 -4.2256894 -4.2230949 -4.2259455 -4.2275853 -4.2270017]]...]
INFO - root - 2017-12-07 17:42:38.990605: step 31410, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 81h:28m:53s remains)
INFO - root - 2017-12-07 17:42:48.542608: step 31420, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 77h:37m:41s remains)
INFO - root - 2017-12-07 17:42:58.093692: step 31430, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 83h:19m:13s remains)
INFO - root - 2017-12-07 17:43:07.859864: step 31440, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 79h:26m:53s remains)
INFO - root - 2017-12-07 17:43:17.496493: step 31450, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 79h:48m:17s remains)
INFO - root - 2017-12-07 17:43:27.076078: step 31460, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 79h:12m:50s remains)
INFO - root - 2017-12-07 17:43:36.754062: step 31470, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 81h:23m:07s remains)
INFO - root - 2017-12-07 17:43:46.323858: step 31480, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 80h:09m:19s remains)
INFO - root - 2017-12-07 17:43:55.851139: step 31490, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.985 sec/batch; 82h:23m:10s remains)
INFO - root - 2017-12-07 17:44:05.347581: step 31500, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 80h:40m:21s remains)
2017-12-07 17:44:06.271857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2308426 -4.2484751 -4.2541776 -4.2459245 -4.2248025 -4.1952834 -4.1701007 -4.1620116 -4.1786814 -4.2131395 -4.2569823 -4.2894897 -4.3067527 -4.3120928 -4.3042636][-4.2652555 -4.273591 -4.2717547 -4.2518678 -4.2106872 -4.16459 -4.1271648 -4.1162934 -4.1443477 -4.1993475 -4.2616296 -4.3044434 -4.326818 -4.3361263 -4.3304868][-4.2965336 -4.2977734 -4.2888637 -4.2588968 -4.1983943 -4.1305704 -4.0722327 -4.0508723 -4.0878353 -4.1658812 -4.2485676 -4.3025103 -4.3325262 -4.3469591 -4.3450046][-4.3282661 -4.3204913 -4.3032713 -4.26449 -4.1913524 -4.1061382 -4.0261812 -3.9893923 -4.0296164 -4.1221261 -4.2207251 -4.2832203 -4.3229094 -4.3419151 -4.3412261][-4.3455815 -4.3302069 -4.3058929 -4.2608261 -4.1782494 -4.0797677 -3.9799109 -3.9292583 -3.9719505 -4.0753169 -4.1848512 -4.2560878 -4.3035131 -4.3260593 -4.324739][-4.3448195 -4.3254061 -4.2973118 -4.2486591 -4.159276 -4.048574 -3.9323921 -3.8710215 -3.9146466 -4.0291538 -4.1517005 -4.2350917 -4.2885518 -4.3147 -4.314086][-4.3363838 -4.313838 -4.2839503 -4.235076 -4.1434751 -4.0261388 -3.900691 -3.8322628 -3.8754587 -4.00177 -4.1355519 -4.2288284 -4.2852693 -4.3130183 -4.3145308][-4.3263235 -4.3019037 -4.2692156 -4.2199345 -4.1336474 -4.0205917 -3.900846 -3.8358827 -3.8766084 -4.0006714 -4.134902 -4.2327323 -4.29142 -4.3190064 -4.3225312][-4.3154058 -4.2914662 -4.2571316 -4.2089272 -4.1328568 -4.0342155 -3.9324653 -3.8813207 -3.9226947 -4.0331478 -4.1533875 -4.2462711 -4.3023477 -4.3269892 -4.3303533][-4.3068566 -4.2863374 -4.2530413 -4.204298 -4.1381936 -4.0576649 -3.9814439 -3.9484763 -3.9903948 -4.0853934 -4.1861954 -4.2668147 -4.3154554 -4.3352051 -4.3366075][-4.3071904 -4.2916965 -4.2614579 -4.2161956 -4.1607189 -4.1006536 -4.0515132 -4.0377727 -4.0778046 -4.1571703 -4.2384462 -4.3022532 -4.3364282 -4.3471308 -4.3444681][-4.3175187 -4.3068933 -4.2834969 -4.2496319 -4.2097354 -4.1692 -4.1399388 -4.1359711 -4.1670685 -4.226604 -4.2887745 -4.3358703 -4.3566384 -4.3584819 -4.3519726][-4.3309221 -4.3254318 -4.31036 -4.2892847 -4.2671995 -4.2448573 -4.2285256 -4.2284422 -4.2513876 -4.2906632 -4.3316665 -4.3609219 -4.3712182 -4.3671308 -4.3579555][-4.3422546 -4.3406291 -4.3326068 -4.321434 -4.3119721 -4.3024182 -4.2946525 -4.2959175 -4.311264 -4.3350024 -4.35942 -4.3748426 -4.37792 -4.3712244 -4.362143][-4.3493838 -4.3494091 -4.3464904 -4.3423743 -4.340775 -4.3395867 -4.338573 -4.339469 -4.3466845 -4.3586011 -4.3700228 -4.3764715 -4.3758841 -4.3703017 -4.3638864]]...]
INFO - root - 2017-12-07 17:44:15.985366: step 31510, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 81h:18m:20s remains)
INFO - root - 2017-12-07 17:44:25.470935: step 31520, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 78h:40m:36s remains)
INFO - root - 2017-12-07 17:44:35.223893: step 31530, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 82h:24m:37s remains)
INFO - root - 2017-12-07 17:44:45.037930: step 31540, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 81h:57m:21s remains)
INFO - root - 2017-12-07 17:44:54.743886: step 31550, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 84h:33m:48s remains)
INFO - root - 2017-12-07 17:45:04.318477: step 31560, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.969 sec/batch; 81h:01m:02s remains)
INFO - root - 2017-12-07 17:45:13.939277: step 31570, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 81h:12m:55s remains)
INFO - root - 2017-12-07 17:45:23.700922: step 31580, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 81h:38m:34s remains)
INFO - root - 2017-12-07 17:45:33.493431: step 31590, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 79h:27m:12s remains)
INFO - root - 2017-12-07 17:45:43.068353: step 31600, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 80h:12m:03s remains)
2017-12-07 17:45:43.951961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.236722 -4.2512403 -4.259037 -4.2688041 -4.2805653 -4.2986169 -4.315856 -4.3295879 -4.3270526 -4.3007164 -4.2662268 -4.2470117 -4.2554884 -4.291204 -4.3350945][-4.237709 -4.237999 -4.2299728 -4.2348995 -4.2454252 -4.2676711 -4.2953777 -4.3170233 -4.3154569 -4.28912 -4.2547045 -4.234241 -4.2434988 -4.2803631 -4.3225689][-4.2492437 -4.2346587 -4.206213 -4.1971078 -4.1983709 -4.220098 -4.2523251 -4.2790942 -4.2830763 -4.2643552 -4.2386589 -4.2242818 -4.2356129 -4.2683644 -4.3015089][-4.256556 -4.2282944 -4.1787591 -4.1460142 -4.1325364 -4.15132 -4.190938 -4.2269154 -4.2446818 -4.2431989 -4.233263 -4.2310367 -4.24175 -4.2611675 -4.2774196][-4.2515903 -4.2128782 -4.1446347 -4.0854983 -4.0554471 -4.0713487 -4.1160069 -4.1577506 -4.1880641 -4.205709 -4.2191887 -4.2298689 -4.2392311 -4.2444925 -4.246263][-4.2473836 -4.208169 -4.1323204 -4.0504222 -4.0000567 -4.0091777 -4.0481796 -4.0789709 -4.1049566 -4.1404395 -4.1843829 -4.209928 -4.2191215 -4.2156391 -4.2111349][-4.2554145 -4.2264915 -4.1549139 -4.0658188 -4.0023623 -3.9959826 -4.0130453 -4.0146356 -4.0179477 -4.0661321 -4.1415138 -4.1873765 -4.1992836 -4.1920633 -4.1893239][-4.2713475 -4.2585244 -4.2038555 -4.1286559 -4.0727873 -4.0513611 -4.037992 -4.0068269 -3.9849343 -4.034761 -4.1281967 -4.1888442 -4.2050672 -4.1966496 -4.1936936][-4.282567 -4.2852755 -4.2516823 -4.2022195 -4.1646876 -4.1419392 -4.1155462 -4.0724063 -4.0369477 -4.0731425 -4.1583161 -4.2190752 -4.2350807 -4.2252011 -4.2212272][-4.2864041 -4.2972307 -4.282001 -4.2571645 -4.2405434 -4.2275591 -4.2035828 -4.1678171 -4.1373963 -4.1576605 -4.2124443 -4.2562461 -4.2665977 -4.2558169 -4.2507968][-4.2862959 -4.2965016 -4.2907948 -4.2817445 -4.2801 -4.2769861 -4.2605004 -4.2390947 -4.2213798 -4.2288446 -4.2558537 -4.2806907 -4.2856789 -4.2770758 -4.2718887][-4.2948828 -4.3013115 -4.2981591 -4.2952514 -4.2978487 -4.2998323 -4.2915177 -4.2821689 -4.2740078 -4.2750697 -4.287219 -4.3008981 -4.3044305 -4.2985196 -4.2934084][-4.3095918 -4.3119111 -4.3091984 -4.3081512 -4.3111835 -4.3141632 -4.3124447 -4.3097715 -4.3053036 -4.3042603 -4.3091 -4.31553 -4.3181453 -4.31651 -4.3124647][-4.3332062 -4.3329391 -4.3302484 -4.3298798 -4.3319759 -4.3333573 -4.3323913 -4.3316374 -4.3298221 -4.3288288 -4.3302231 -4.3323936 -4.3340836 -4.3346848 -4.332931][-4.3533435 -4.3528481 -4.3512011 -4.3505769 -4.3512182 -4.3510842 -4.3500934 -4.3493114 -4.3484783 -4.3484936 -4.34887 -4.3497043 -4.35091 -4.3524079 -4.3526769]]...]
INFO - root - 2017-12-07 17:45:53.484553: step 31610, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 77h:19m:41s remains)
INFO - root - 2017-12-07 17:46:03.097636: step 31620, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 82h:43m:05s remains)
INFO - root - 2017-12-07 17:46:12.936512: step 31630, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 82h:08m:19s remains)
INFO - root - 2017-12-07 17:46:22.715882: step 31640, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 80h:21m:11s remains)
INFO - root - 2017-12-07 17:46:32.349431: step 31650, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 80h:18m:39s remains)
INFO - root - 2017-12-07 17:46:41.965990: step 31660, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 81h:48m:08s remains)
INFO - root - 2017-12-07 17:46:51.820724: step 31670, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 83h:39m:00s remains)
INFO - root - 2017-12-07 17:47:01.457789: step 31680, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 81h:43m:31s remains)
INFO - root - 2017-12-07 17:47:10.965397: step 31690, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 80h:26m:00s remains)
INFO - root - 2017-12-07 17:47:20.642648: step 31700, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.959 sec/batch; 80h:06m:52s remains)
2017-12-07 17:47:21.700036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1550193 -4.1379194 -4.1332483 -4.1494894 -4.1669688 -4.1767383 -4.1730938 -4.1717114 -4.1758862 -4.1815486 -4.1859894 -4.1828136 -4.17744 -4.1805754 -4.191164][-4.1184826 -4.1128569 -4.12895 -4.1670341 -4.1912351 -4.1968975 -4.18723 -4.1787596 -4.175015 -4.1719522 -4.1684327 -4.1652889 -4.1647115 -4.1732011 -4.1882648][-4.108397 -4.1010342 -4.11924 -4.1646252 -4.1921129 -4.1985383 -4.1885686 -4.1777558 -4.1725874 -4.1676016 -4.1607208 -4.1547976 -4.1586719 -4.1746821 -4.1931829][-4.1218648 -4.101634 -4.1109371 -4.1532817 -4.1795964 -4.184422 -4.1757841 -4.1672149 -4.1626 -4.1649656 -4.1672444 -4.1653943 -4.1696944 -4.1869826 -4.205842][-4.1335611 -4.1084938 -4.1144428 -4.1568789 -4.1839337 -4.179831 -4.15709 -4.1428246 -4.1447878 -4.162394 -4.1768446 -4.1824045 -4.1869221 -4.1994209 -4.2155213][-4.1127353 -4.091764 -4.1031017 -4.145864 -4.1718273 -4.1532755 -4.1084356 -4.0826135 -4.097343 -4.1334496 -4.16215 -4.1772838 -4.1821876 -4.1919971 -4.2073612][-4.08442 -4.0723882 -4.088881 -4.1273589 -4.143805 -4.0979977 -4.0206523 -3.9810958 -4.01349 -4.0761852 -4.1236405 -4.1465106 -4.1554942 -4.1708589 -4.1919909][-4.0853906 -4.0795059 -4.0951414 -4.1223855 -4.1176476 -4.0366583 -3.9344316 -3.8984022 -3.9530506 -4.0410151 -4.1013026 -4.1267633 -4.1407924 -4.1641345 -4.1898046][-4.1355047 -4.1298246 -4.1406283 -4.1577425 -4.1457076 -4.0667357 -3.981215 -3.959059 -4.0078993 -4.0802412 -4.1243792 -4.1400919 -4.1534204 -4.1800833 -4.2067971][-4.1970119 -4.1914425 -4.197197 -4.2084742 -4.2005982 -4.1518536 -4.105247 -4.0971265 -4.1204782 -4.1563449 -4.1704288 -4.1707578 -4.1783886 -4.20463 -4.2298789][-4.241631 -4.2380719 -4.2401857 -4.2485886 -4.2431107 -4.216217 -4.1949353 -4.194109 -4.2054505 -4.2212729 -4.217257 -4.2062259 -4.2113185 -4.23209 -4.2519517][-4.2766628 -4.2714305 -4.2742848 -4.2835884 -4.2764306 -4.2629066 -4.2548709 -4.2534823 -4.257916 -4.265089 -4.2568536 -4.2460213 -4.2489915 -4.2648044 -4.2790694][-4.3330913 -4.3259044 -4.3293352 -4.3337564 -4.3228331 -4.3113465 -4.3066373 -4.3034139 -4.3011456 -4.3034682 -4.2982869 -4.2917175 -4.2907562 -4.2999125 -4.3072739][-4.3502264 -4.3443265 -4.3446422 -4.343492 -4.3348384 -4.3285413 -4.3286347 -4.3270049 -4.3236051 -4.3239784 -4.322227 -4.3165259 -4.313179 -4.317081 -4.3206797][-4.323122 -4.3171544 -4.3152804 -4.3129086 -4.3098826 -4.3104177 -4.315146 -4.31891 -4.3200769 -4.3213477 -4.3203478 -4.3179626 -4.3150511 -4.31663 -4.3214736]]...]
INFO - root - 2017-12-07 17:47:31.475535: step 31710, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 83h:22m:31s remains)
INFO - root - 2017-12-07 17:47:41.192339: step 31720, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 77h:34m:38s remains)
INFO - root - 2017-12-07 17:47:50.983141: step 31730, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.032 sec/batch; 86h:12m:44s remains)
INFO - root - 2017-12-07 17:48:00.677234: step 31740, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 83h:25m:43s remains)
INFO - root - 2017-12-07 17:48:10.390627: step 31750, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 81h:50m:08s remains)
INFO - root - 2017-12-07 17:48:19.896005: step 31760, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 80h:37m:28s remains)
INFO - root - 2017-12-07 17:48:29.621716: step 31770, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 83h:27m:29s remains)
INFO - root - 2017-12-07 17:48:39.330721: step 31780, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 83h:12m:19s remains)
INFO - root - 2017-12-07 17:48:48.942912: step 31790, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 79h:36m:25s remains)
INFO - root - 2017-12-07 17:48:58.570618: step 31800, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 81h:55m:15s remains)
2017-12-07 17:48:59.516504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3101869 -4.3023791 -4.2972989 -4.273263 -4.235168 -4.2095194 -4.201118 -4.1994238 -4.2053103 -4.2299023 -4.2500167 -4.2565947 -4.2521524 -4.2255588 -4.1770325][-4.2853665 -4.2835813 -4.2862406 -4.2677522 -4.2303176 -4.2004557 -4.1793485 -4.1649909 -4.173491 -4.2126489 -4.2473221 -4.2604117 -4.2467556 -4.2061639 -4.1418886][-4.263371 -4.2667689 -4.2753239 -4.2610488 -4.2260318 -4.1939 -4.1567807 -4.1212354 -4.1281567 -4.1838417 -4.2347741 -4.2553463 -4.2415109 -4.2012496 -4.1364722][-4.2472129 -4.2496982 -4.2577868 -4.2456603 -4.2146435 -4.1826496 -4.1344528 -4.0742 -4.0720406 -4.1390376 -4.2065048 -4.2437477 -4.245338 -4.2202277 -4.1678495][-4.2394872 -4.2415509 -4.2463312 -4.2350321 -4.2063241 -4.1658564 -4.0954428 -4.0041027 -3.9922566 -4.0789242 -4.16759 -4.2250218 -4.2452369 -4.2382879 -4.2036037][-4.236722 -4.2441463 -4.2492719 -4.2373285 -4.2070041 -4.1543517 -4.0542035 -3.9311802 -3.9112651 -4.0129962 -4.1238556 -4.2011647 -4.2374029 -4.2415919 -4.2200532][-4.2301173 -4.2405415 -4.247426 -4.2403097 -4.2134967 -4.1612926 -4.0544386 -3.9224646 -3.8952177 -3.9854894 -4.0895696 -4.1724343 -4.2214065 -4.2334428 -4.2169943][-4.2178645 -4.2326684 -4.2442842 -4.2438674 -4.2254753 -4.1807327 -4.0862923 -3.9733953 -3.9476428 -4.0106821 -4.0820918 -4.1510825 -4.204576 -4.2212343 -4.2093043][-4.2015038 -4.2256546 -4.2457447 -4.2541342 -4.2443128 -4.2105885 -4.1370778 -4.056047 -4.0391588 -4.076942 -4.1125803 -4.1570797 -4.2059827 -4.2267227 -4.2199049][-4.1902905 -4.2196422 -4.2471905 -4.2642212 -4.2630792 -4.24269 -4.1856394 -4.1261821 -4.1133757 -4.1283112 -4.1371703 -4.1610346 -4.2032189 -4.2279325 -4.2292757][-4.2073421 -4.2272553 -4.2553396 -4.2729206 -4.2747526 -4.2634721 -4.2194104 -4.1738014 -4.1625152 -4.1648965 -4.16533 -4.1817765 -4.21738 -4.2439227 -4.2506919][-4.2461109 -4.2520037 -4.2700596 -4.27953 -4.2782688 -4.2709093 -4.2421503 -4.2157493 -4.2153893 -4.2191567 -4.2164226 -4.2243485 -4.2477946 -4.267808 -4.2753191][-4.2856693 -4.2803483 -4.2886229 -4.2899976 -4.2833905 -4.276855 -4.2586079 -4.2483354 -4.257565 -4.2631464 -4.2613673 -4.2647352 -4.2757573 -4.2857814 -4.2891684][-4.3153896 -4.3081269 -4.310463 -4.3039007 -4.2925682 -4.2864842 -4.2764592 -4.2740912 -4.2854557 -4.290874 -4.2900071 -4.29035 -4.2923903 -4.2950115 -4.2936416][-4.3318729 -4.32669 -4.3262019 -4.3147693 -4.3031631 -4.3010173 -4.298945 -4.3025756 -4.3162909 -4.3239532 -4.3227181 -4.3179836 -4.3102427 -4.3032222 -4.2952852]]...]
INFO - root - 2017-12-07 17:49:09.333613: step 31810, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.999 sec/batch; 83h:27m:45s remains)
INFO - root - 2017-12-07 17:49:19.093633: step 31820, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 80h:43m:02s remains)
INFO - root - 2017-12-07 17:49:28.822357: step 31830, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 77h:45m:31s remains)
INFO - root - 2017-12-07 17:49:38.592264: step 31840, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 81h:35m:29s remains)
INFO - root - 2017-12-07 17:49:48.183581: step 31850, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 82h:34m:15s remains)
INFO - root - 2017-12-07 17:49:57.735992: step 31860, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 82h:29m:49s remains)
INFO - root - 2017-12-07 17:50:07.524409: step 31870, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.930 sec/batch; 77h:37m:34s remains)
INFO - root - 2017-12-07 17:50:17.272356: step 31880, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 82h:28m:42s remains)
INFO - root - 2017-12-07 17:50:26.913033: step 31890, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 80h:20m:18s remains)
INFO - root - 2017-12-07 17:50:36.566416: step 31900, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 81h:48m:48s remains)
2017-12-07 17:50:37.508540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2486205 -4.2501187 -4.250021 -4.242033 -4.2247014 -4.2073164 -4.1999269 -4.2072268 -4.2164154 -4.2254062 -4.2262025 -4.2217121 -4.2283149 -4.2394567 -4.2466764][-4.2257652 -4.2247434 -4.2257485 -4.2194662 -4.2023044 -4.1818047 -4.1665354 -4.1738024 -4.19209 -4.2052236 -4.2018356 -4.1919227 -4.198102 -4.207902 -4.2135744][-4.2061505 -4.2028818 -4.204289 -4.195159 -4.1750779 -4.1501527 -4.1251206 -4.1294217 -4.1585279 -4.1757855 -4.1690273 -4.1551137 -4.1596322 -4.1703234 -4.1777773][-4.196909 -4.187223 -4.1840086 -4.1645346 -4.1338611 -4.1033297 -4.0706944 -4.0761466 -4.1103849 -4.1260414 -4.122611 -4.115747 -4.122941 -4.1358118 -4.1430817][-4.1955791 -4.1775274 -4.1649475 -4.1292963 -4.0867348 -4.05836 -4.0323877 -4.0462694 -4.0800791 -4.0855989 -4.0883217 -4.0931854 -4.1024661 -4.1140141 -4.1182437][-4.1961269 -4.1734333 -4.1492643 -4.0932889 -4.03667 -4.0199747 -4.018023 -4.0472307 -4.0786452 -4.0730429 -4.0737009 -4.0854335 -4.0911422 -4.09585 -4.0940981][-4.2016053 -4.1786485 -4.14189 -4.0630083 -3.9918563 -3.9881208 -4.0114651 -4.05604 -4.0848789 -4.0666051 -4.0568695 -4.0719471 -4.0808744 -4.0834832 -4.0794659][-4.2154074 -4.1944809 -4.1544166 -4.0652547 -3.9894576 -3.9892366 -4.0233321 -4.07757 -4.1029677 -4.0740924 -4.0528545 -4.0684137 -4.0840659 -4.0923538 -4.0877852][-4.2312336 -4.2155538 -4.1837506 -4.1070724 -4.0358047 -4.0260997 -4.0491676 -4.1002746 -4.1245618 -4.093514 -4.0722523 -4.08885 -4.1108317 -4.1205473 -4.1112242][-4.2492857 -4.2390766 -4.2139716 -4.1585507 -4.0988479 -4.0743728 -4.0795078 -4.115942 -4.1327209 -4.1070714 -4.0977778 -4.12073 -4.1447992 -4.1500664 -4.129468][-4.2670145 -4.2612019 -4.2410746 -4.1984105 -4.1474094 -4.1126165 -4.1002827 -4.1147137 -4.1258216 -4.1211224 -4.1296191 -4.1545782 -4.1722388 -4.1630297 -4.1270781][-4.2768192 -4.2712049 -4.250041 -4.2099004 -4.1669517 -4.1356778 -4.1118517 -4.1106381 -4.1260219 -4.139883 -4.1534448 -4.1724381 -4.1819 -4.1667676 -4.1277013][-4.2756457 -4.2700448 -4.2448249 -4.200788 -4.164083 -4.1432505 -4.1190519 -4.1147251 -4.1350117 -4.1523094 -4.1616406 -4.1738596 -4.1834459 -4.1765518 -4.1467252][-4.2719817 -4.2675176 -4.2424083 -4.1957717 -4.1590195 -4.1432462 -4.1225753 -4.1195478 -4.1406574 -4.1550488 -4.1570268 -4.1654119 -4.1806197 -4.1872497 -4.1718984][-4.2749238 -4.2731795 -4.253078 -4.2084546 -4.170126 -4.1497912 -4.1279073 -4.1296191 -4.1488137 -4.1637287 -4.1666737 -4.1726985 -4.1908131 -4.2043271 -4.1979256]]...]
INFO - root - 2017-12-07 17:50:47.191741: step 31910, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.925 sec/batch; 77h:14m:56s remains)
INFO - root - 2017-12-07 17:50:56.922159: step 31920, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 82h:49m:43s remains)
INFO - root - 2017-12-07 17:51:06.418618: step 31930, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 75h:00m:34s remains)
INFO - root - 2017-12-07 17:51:16.067556: step 31940, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 81h:09m:02s remains)
INFO - root - 2017-12-07 17:51:25.760241: step 31950, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 82h:08m:57s remains)
INFO - root - 2017-12-07 17:51:35.334614: step 31960, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 79h:20m:25s remains)
INFO - root - 2017-12-07 17:51:45.070049: step 31970, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 84h:02m:28s remains)
INFO - root - 2017-12-07 17:51:54.821708: step 31980, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 75h:50m:16s remains)
INFO - root - 2017-12-07 17:52:04.457150: step 31990, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 82h:44m:52s remains)
INFO - root - 2017-12-07 17:52:14.079402: step 32000, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 80h:40m:51s remains)
2017-12-07 17:52:15.053657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3551383 -4.3608985 -4.362011 -4.3543935 -4.335042 -4.3025851 -4.2697148 -4.2452793 -4.2327032 -4.2290759 -4.22308 -4.2191687 -4.2235742 -4.2477221 -4.2781639][-4.3652306 -4.3735347 -4.3732328 -4.3568726 -4.3264346 -4.282331 -4.2380815 -4.2088251 -4.2006254 -4.2078385 -4.2089877 -4.207962 -4.2125812 -4.2375908 -4.2732272][-4.3723788 -4.3837523 -4.3820429 -4.359818 -4.3194852 -4.2626352 -4.2070675 -4.1742115 -4.1740494 -4.1936417 -4.1989145 -4.2005558 -4.2082872 -4.2357121 -4.2766037][-4.3800406 -4.3921685 -4.3866057 -4.3560858 -4.3019843 -4.2321005 -4.1667862 -4.1367736 -4.1500049 -4.1827927 -4.193233 -4.1996145 -4.2127547 -4.2430253 -4.2874355][-4.3901992 -4.3964887 -4.3795691 -4.3347945 -4.2665768 -4.1826072 -4.1092019 -4.0895233 -4.1204576 -4.1650171 -4.1804962 -4.1902404 -4.2108083 -4.2450733 -4.2918186][-4.3966389 -4.3916268 -4.3569551 -4.2946458 -4.2108603 -4.1118112 -4.0302758 -4.0210104 -4.0693469 -4.12592 -4.1464462 -4.1601372 -4.1931143 -4.2385597 -4.2899804][-4.3855782 -4.3647928 -4.3060584 -4.2216048 -4.1182642 -4.0039349 -3.919179 -3.9233506 -3.9864395 -4.0587444 -4.0946188 -4.1266289 -4.1812439 -4.2396965 -4.2938695][-4.3516278 -4.3083377 -4.2198725 -4.1070795 -3.9853294 -3.8673048 -3.7933376 -3.8159859 -3.8893495 -3.9809291 -4.0495739 -4.1130085 -4.1872749 -4.2533312 -4.3044844][-4.302783 -4.2384338 -4.1256194 -3.9934409 -3.8764079 -3.7943954 -3.7557719 -3.7905354 -3.8679178 -3.9686892 -4.0612574 -4.1418929 -4.2177577 -4.2748718 -4.313159][-4.2580857 -4.1896291 -4.078146 -3.9598434 -3.87864 -3.8531888 -3.8490829 -3.8863578 -3.9589121 -4.04801 -4.1298723 -4.198668 -4.2549934 -4.2902179 -4.3132906][-4.2291393 -4.1722593 -4.0866666 -4.0051689 -3.9637995 -3.972002 -3.9846735 -4.0224223 -4.0855656 -4.1538897 -4.20788 -4.2475529 -4.273005 -4.2857714 -4.2959428][-4.2182369 -4.1770821 -4.1193118 -4.0719633 -4.0589356 -4.0832176 -4.1049514 -4.1399078 -4.1887622 -4.2337151 -4.2600369 -4.2703943 -4.2673016 -4.2630143 -4.2687597][-4.221416 -4.1952853 -4.1604638 -4.1333137 -4.1311221 -4.154057 -4.177331 -4.2078915 -4.2421203 -4.2659669 -4.2674584 -4.2516141 -4.2275209 -4.2149343 -4.2265244][-4.2396379 -4.222887 -4.2004685 -4.1833262 -4.181788 -4.1946478 -4.2089577 -4.2272677 -4.2455511 -4.2505546 -4.22871 -4.1913233 -4.1574206 -4.1515622 -4.1805248][-4.263207 -4.2530069 -4.2381349 -4.2242813 -4.2172651 -4.21603 -4.2166753 -4.2205977 -4.2214293 -4.2070236 -4.1666822 -4.1215577 -4.0950813 -4.1080317 -4.1545572]]...]
INFO - root - 2017-12-07 17:52:24.886306: step 32010, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 81h:31m:00s remains)
INFO - root - 2017-12-07 17:52:34.551850: step 32020, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 84h:15m:43s remains)
INFO - root - 2017-12-07 17:52:44.142994: step 32030, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.994 sec/batch; 82h:56m:21s remains)
INFO - root - 2017-12-07 17:52:53.968894: step 32040, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 82h:20m:07s remains)
INFO - root - 2017-12-07 17:53:03.391647: step 32050, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 80h:37m:38s remains)
INFO - root - 2017-12-07 17:53:13.078640: step 32060, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 84h:43m:08s remains)
INFO - root - 2017-12-07 17:53:22.791606: step 32070, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 80h:42m:08s remains)
INFO - root - 2017-12-07 17:53:32.397210: step 32080, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 79h:11m:59s remains)
INFO - root - 2017-12-07 17:53:42.000318: step 32090, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 77h:12m:40s remains)
INFO - root - 2017-12-07 17:53:51.745503: step 32100, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 83h:33m:41s remains)
2017-12-07 17:53:52.674955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2738681 -4.2708883 -4.2569833 -4.2422752 -4.2381916 -4.2419777 -4.2490206 -4.2548747 -4.2573643 -4.2626777 -4.2786989 -4.296618 -4.308857 -4.3174429 -4.3215384][-4.2722373 -4.2616334 -4.2403636 -4.223475 -4.2215867 -4.2292266 -4.2390175 -4.2495604 -4.2571225 -4.2655716 -4.2832079 -4.3001628 -4.3074331 -4.311821 -4.3133917][-4.266387 -4.2505951 -4.2256093 -4.2055078 -4.2014647 -4.2058229 -4.2170606 -4.235086 -4.2528181 -4.2684517 -4.2858076 -4.2980351 -4.2997246 -4.2965388 -4.2932038][-4.2552481 -4.238678 -4.2111979 -4.1870246 -4.1758547 -4.17547 -4.190999 -4.2171607 -4.240386 -4.260623 -4.2780967 -4.2853107 -4.2818432 -4.2739196 -4.2671981][-4.2339306 -4.2245073 -4.1990366 -4.1726403 -4.1536384 -4.1477184 -4.1661458 -4.1967878 -4.2212267 -4.240036 -4.252964 -4.2564735 -4.2505465 -4.2407961 -4.2330894][-4.2048426 -4.2083454 -4.1918516 -4.1672025 -4.1442928 -4.1343145 -4.1523728 -4.1796145 -4.2009048 -4.2154217 -4.2220411 -4.2236671 -4.219645 -4.2109947 -4.2010226][-4.1744485 -4.1875963 -4.1837244 -4.1666331 -4.1460629 -4.1352649 -4.1488247 -4.1699162 -4.1837749 -4.1946154 -4.2016759 -4.2042813 -4.2015457 -4.193326 -4.1809115][-4.1469135 -4.1619267 -4.1680737 -4.1615982 -4.1505227 -4.141706 -4.1478825 -4.1574712 -4.16273 -4.1727953 -4.1903019 -4.2037134 -4.2070394 -4.2006721 -4.1864934][-4.1413894 -4.1507998 -4.1594849 -4.1611886 -4.1589165 -4.1500311 -4.1433883 -4.1396985 -4.1352148 -4.1418042 -4.169343 -4.1999183 -4.2175355 -4.2185993 -4.2084155][-4.163 -4.166451 -4.1721716 -4.1743417 -4.1716337 -4.1583 -4.139945 -4.1230707 -4.1088486 -4.1116838 -4.1420259 -4.1794105 -4.2052112 -4.2128572 -4.2114477][-4.1940584 -4.1977615 -4.2005339 -4.1997046 -4.1930313 -4.1764846 -4.15336 -4.1283574 -4.1076946 -4.1015677 -4.1185274 -4.1461053 -4.1683264 -4.1765394 -4.1814146][-4.205811 -4.219142 -4.2285438 -4.2317753 -4.2231388 -4.2048392 -4.1841273 -4.1615629 -4.1399336 -4.125175 -4.1200228 -4.1214972 -4.1283154 -4.1351819 -4.1463618][-4.1947312 -4.2157431 -4.2355251 -4.2464933 -4.2424893 -4.2306619 -4.2184696 -4.2048182 -4.1892943 -4.1734786 -4.1566544 -4.1361623 -4.1231227 -4.1226463 -4.1327453][-4.1686759 -4.1926541 -4.2202716 -4.2375164 -4.2384768 -4.2343359 -4.2337155 -4.2339745 -4.2285247 -4.217988 -4.2010264 -4.1740189 -4.15006 -4.1406054 -4.1418157][-4.1456494 -4.1691775 -4.197484 -4.214076 -4.2169633 -4.2158794 -4.22253 -4.2368116 -4.2452555 -4.2461185 -4.23571 -4.2102509 -4.1833491 -4.167315 -4.1611824]]...]
INFO - root - 2017-12-07 17:54:02.280355: step 32110, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 82h:15m:00s remains)
INFO - root - 2017-12-07 17:54:11.738358: step 32120, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 74h:50m:02s remains)
INFO - root - 2017-12-07 17:54:21.218046: step 32130, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 81h:07m:11s remains)
INFO - root - 2017-12-07 17:54:30.807253: step 32140, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 80h:13m:53s remains)
INFO - root - 2017-12-07 17:54:40.601114: step 32150, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 82h:34m:54s remains)
INFO - root - 2017-12-07 17:54:50.279646: step 32160, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 82h:59m:45s remains)
INFO - root - 2017-12-07 17:54:59.966112: step 32170, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 80h:10m:24s remains)
INFO - root - 2017-12-07 17:55:09.815653: step 32180, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.998 sec/batch; 83h:13m:52s remains)
INFO - root - 2017-12-07 17:55:19.374755: step 32190, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 76h:15m:03s remains)
INFO - root - 2017-12-07 17:55:28.993936: step 32200, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 79h:42m:18s remains)
2017-12-07 17:55:29.914818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32772 -4.3296165 -4.3295822 -4.3278079 -4.3252883 -4.3224735 -4.3201556 -4.3193941 -4.3199749 -4.3221169 -4.3248343 -4.3280067 -4.3320551 -4.3359771 -4.3384871][-4.3285871 -4.3317814 -4.3321185 -4.3298583 -4.3258858 -4.3214536 -4.31849 -4.3181386 -4.3202004 -4.3250108 -4.3310075 -4.3359818 -4.3398919 -4.3422985 -4.3432031][-4.3281813 -4.3304315 -4.328227 -4.3213277 -4.3114228 -4.301621 -4.2963367 -4.2970047 -4.3035097 -4.3149495 -4.328156 -4.3385396 -4.3443718 -4.3451385 -4.3435488][-4.3255868 -4.3244638 -4.3154683 -4.2973046 -4.2725811 -4.2495284 -4.236577 -4.2374506 -4.251689 -4.2769384 -4.304862 -4.3273821 -4.3406963 -4.3436689 -4.3413115][-4.31737 -4.3098764 -4.2906981 -4.2554817 -4.2080669 -4.1635375 -4.1359496 -4.1347623 -4.1600752 -4.2073255 -4.2587972 -4.2998395 -4.3252349 -4.334167 -4.3333821][-4.3076067 -4.2947307 -4.2665119 -4.2144384 -4.1423516 -4.0705657 -4.020967 -4.0139017 -4.0519223 -4.1242046 -4.2016778 -4.262063 -4.2999544 -4.3161664 -4.3184042][-4.3046527 -4.2908025 -4.2603035 -4.2023687 -4.1172075 -4.0247993 -3.9536757 -3.937933 -3.9846377 -4.0733638 -4.1669621 -4.2381048 -4.2821636 -4.3016109 -4.3059263][-4.3098264 -4.2996917 -4.2753558 -4.2280345 -4.1517248 -4.0606885 -3.9846702 -3.9627018 -4.0040421 -4.0855284 -4.1722326 -4.2379737 -4.2775664 -4.2940049 -4.2975225][-4.3185158 -4.3145022 -4.3008184 -4.2717838 -4.2198949 -4.1517229 -4.0909977 -4.0683417 -4.0925689 -4.1472073 -4.2074637 -4.2526474 -4.2780128 -4.2868257 -4.2876792][-4.3276262 -4.3279996 -4.322063 -4.3074846 -4.279901 -4.2397332 -4.2004228 -4.1825304 -4.1916914 -4.2200556 -4.2514448 -4.2720671 -4.279592 -4.2785935 -4.2748909][-4.3335381 -4.3339581 -4.3279095 -4.3165507 -4.3010688 -4.2805171 -4.2613573 -4.2542515 -4.2598486 -4.2744021 -4.2881236 -4.2922454 -4.2870178 -4.2775288 -4.268774][-4.3335404 -4.3289933 -4.3135839 -4.2914343 -4.2716618 -4.258738 -4.2549515 -4.2629204 -4.2766118 -4.2931828 -4.3040309 -4.3047619 -4.2971334 -4.2859769 -4.2759261][-4.3181911 -4.3102179 -4.2838182 -4.2434464 -4.2071018 -4.1913929 -4.199955 -4.2260723 -4.2578068 -4.2879677 -4.3058295 -4.3118849 -4.3095455 -4.3025818 -4.2947278][-4.2878156 -4.2843895 -4.2540159 -4.2004037 -4.1485815 -4.1280394 -4.1459293 -4.1878295 -4.2361069 -4.2777958 -4.303371 -4.3158531 -4.3193755 -4.3173623 -4.3126822][-4.2563143 -4.2685137 -4.2484355 -4.1983013 -4.1445618 -4.1241465 -4.1468353 -4.194222 -4.2447867 -4.2858348 -4.3112907 -4.3251352 -4.3315105 -4.3331113 -4.3312578]]...]
INFO - root - 2017-12-07 17:55:39.729496: step 32210, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.023 sec/batch; 85h:18m:13s remains)
INFO - root - 2017-12-07 17:55:49.512902: step 32220, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 80h:16m:03s remains)
INFO - root - 2017-12-07 17:55:59.163837: step 32230, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.947 sec/batch; 78h:58m:40s remains)
INFO - root - 2017-12-07 17:56:08.700290: step 32240, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 82h:02m:29s remains)
INFO - root - 2017-12-07 17:56:18.405155: step 32250, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.021 sec/batch; 85h:08m:34s remains)
INFO - root - 2017-12-07 17:56:28.052361: step 32260, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 76h:43m:40s remains)
INFO - root - 2017-12-07 17:56:37.852454: step 32270, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 82h:48m:58s remains)
INFO - root - 2017-12-07 17:56:47.581669: step 32280, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.894 sec/batch; 74h:34m:52s remains)
INFO - root - 2017-12-07 17:56:57.227179: step 32290, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 81h:06m:45s remains)
INFO - root - 2017-12-07 17:57:06.911844: step 32300, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 84h:41m:49s remains)
2017-12-07 17:57:07.846560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2695961 -4.2530084 -4.2434921 -4.2313786 -4.2154288 -4.2053676 -4.2058077 -4.2165833 -4.2331581 -4.2392693 -4.224371 -4.2019625 -4.1880746 -4.1894045 -4.2050266][-4.2686043 -4.2562261 -4.2545862 -4.2439985 -4.2226124 -4.2049785 -4.2009363 -4.211822 -4.2288685 -4.2432904 -4.24146 -4.22632 -4.2077394 -4.1947331 -4.1988268][-4.26301 -4.2572522 -4.261405 -4.2513618 -4.2246528 -4.2007089 -4.1931076 -4.2034664 -4.2284312 -4.2537289 -4.2644062 -4.2538857 -4.2311697 -4.2059245 -4.1953073][-4.2572513 -4.2510619 -4.2564068 -4.2493191 -4.2243876 -4.1959128 -4.1800814 -4.1904416 -4.2239642 -4.260253 -4.280365 -4.2766433 -4.2541823 -4.225606 -4.2049918][-4.2527223 -4.2386994 -4.242744 -4.2419934 -4.2214751 -4.1830392 -4.1557493 -4.1691241 -4.2137227 -4.2590513 -4.2857294 -4.2894125 -4.2712283 -4.2460065 -4.2255273][-4.2452636 -4.2205129 -4.2227798 -4.2278337 -4.2025743 -4.1446104 -4.1026635 -4.1322622 -4.1994195 -4.2513614 -4.2816348 -4.2899742 -4.275835 -4.2574549 -4.2455373][-4.2364216 -4.2022643 -4.2016325 -4.2065687 -4.1679206 -4.0795374 -4.0133018 -4.0700908 -4.1682196 -4.2334843 -4.2712 -4.2807937 -4.2695684 -4.2600064 -4.2586451][-4.2307353 -4.1994805 -4.2004285 -4.205442 -4.1583538 -4.0453238 -3.9474916 -4.019392 -4.1406078 -4.2187476 -4.2595243 -4.2692428 -4.2590122 -4.2554045 -4.2603788][-4.229413 -4.2087989 -4.2164989 -4.2233214 -4.1881557 -4.0905275 -3.9985282 -4.0411196 -4.1425118 -4.2160621 -4.2538934 -4.259316 -4.2474227 -4.244154 -4.2507091][-4.2281866 -4.2193208 -4.2290564 -4.2348275 -4.2156887 -4.153008 -4.09002 -4.1062126 -4.1736059 -4.2319288 -4.2641387 -4.2645054 -4.2447481 -4.2328925 -4.2365351][-4.2272863 -4.2260771 -4.2342224 -4.2372756 -4.2275233 -4.1941152 -4.1599278 -4.1686192 -4.2100263 -4.252624 -4.279882 -4.276968 -4.2482452 -4.2234707 -4.2208314][-4.2325854 -4.2282877 -4.2298908 -4.2330332 -4.2313666 -4.2154112 -4.196609 -4.2060819 -4.233851 -4.2647915 -4.2864075 -4.2847242 -4.2544641 -4.2176604 -4.2036576][-4.2439508 -4.2288008 -4.2178574 -4.2161531 -4.2168345 -4.2106104 -4.199779 -4.2105803 -4.2362862 -4.2630391 -4.2838464 -4.2870708 -4.2650251 -4.2244697 -4.1998124][-4.2575088 -4.2340951 -4.2125897 -4.2022386 -4.19869 -4.1944361 -4.1868324 -4.1984649 -4.2244453 -4.2540951 -4.2760539 -4.2829657 -4.2731447 -4.2402148 -4.2148976][-4.2697062 -4.245667 -4.2218342 -4.2077589 -4.1974525 -4.1872616 -4.1778774 -4.1899462 -4.2169266 -4.2471576 -4.2693481 -4.2783175 -4.2780609 -4.2559848 -4.2345572]]...]
INFO - root - 2017-12-07 17:57:17.661571: step 32310, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 78h:34m:18s remains)
INFO - root - 2017-12-07 17:57:27.439610: step 32320, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 82h:10m:32s remains)
INFO - root - 2017-12-07 17:57:37.083237: step 32330, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.966 sec/batch; 80h:31m:21s remains)
INFO - root - 2017-12-07 17:57:46.838199: step 32340, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 81h:57m:11s remains)
INFO - root - 2017-12-07 17:57:56.591869: step 32350, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 78h:42m:54s remains)
INFO - root - 2017-12-07 17:58:06.154362: step 32360, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.006 sec/batch; 83h:50m:54s remains)
INFO - root - 2017-12-07 17:58:15.822755: step 32370, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 75h:25m:43s remains)
INFO - root - 2017-12-07 17:58:25.505661: step 32380, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 79h:02m:58s remains)
INFO - root - 2017-12-07 17:58:35.270767: step 32390, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.019 sec/batch; 84h:57m:52s remains)
INFO - root - 2017-12-07 17:58:44.952585: step 32400, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 80h:40m:34s remains)
2017-12-07 17:58:46.048315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.329205 -4.3063264 -4.2830415 -4.2728891 -4.273983 -4.2845135 -4.2953558 -4.3004656 -4.2974629 -4.2865915 -4.2708073 -4.25566 -4.2541242 -4.2666421 -4.2846284][-4.3113813 -4.2775278 -4.2447939 -4.2328496 -4.2353792 -4.2489061 -4.2605252 -4.2634859 -4.2613416 -4.2493048 -4.2276554 -4.2074213 -4.2064619 -4.2239881 -4.2464204][-4.288506 -4.2444763 -4.2073741 -4.1971421 -4.1996613 -4.2098508 -4.2118697 -4.2038035 -4.2008533 -4.1972375 -4.1837788 -4.1691427 -4.1733875 -4.1953721 -4.218379][-4.2607489 -4.2115474 -4.174726 -4.1683464 -4.1685643 -4.16839 -4.15405 -4.1338139 -4.1361666 -4.1525025 -4.1608982 -4.1601691 -4.1696796 -4.19074 -4.2082505][-4.2378731 -4.1882849 -4.15111 -4.1451168 -4.1393771 -4.1212206 -4.0827241 -4.0458741 -4.0565143 -4.102726 -4.1413155 -4.1598811 -4.1730218 -4.1887059 -4.196887][-4.2244844 -4.1775184 -4.138288 -4.1247983 -4.1082067 -4.0628109 -3.9895129 -3.9272451 -3.9483535 -4.0246286 -4.0937982 -4.1311097 -4.1507845 -4.1670775 -4.1755424][-4.2151527 -4.1731758 -4.1333323 -4.1073909 -4.069509 -3.9886212 -3.8728929 -3.7788424 -3.8199055 -3.9304395 -4.0261254 -4.0827961 -4.1135473 -4.1365347 -4.1515136][-4.2168169 -4.1845813 -4.1469021 -4.1111307 -4.0575252 -3.9615054 -3.8366945 -3.7403829 -3.7908602 -3.9027829 -3.9962478 -4.0574756 -4.0979643 -4.1326184 -4.1569686][-4.2345057 -4.2154918 -4.1862121 -4.1528983 -4.1074314 -4.0336957 -3.9451287 -3.8851867 -3.9202123 -3.9896615 -4.0444136 -4.0837693 -4.1197085 -4.1584673 -4.1909723][-4.2587976 -4.2481956 -4.2274857 -4.2022524 -4.1758304 -4.13142 -4.0821447 -4.0508213 -4.0660987 -4.0958638 -4.114491 -4.1304259 -4.1532555 -4.1897411 -4.2239084][-4.2849741 -4.2766123 -4.26141 -4.244246 -4.2302122 -4.2035093 -4.1776814 -4.1621637 -4.1640205 -4.1719379 -4.1708884 -4.1735253 -4.1855683 -4.2137284 -4.242979][-4.3083377 -4.2992468 -4.2847304 -4.2709103 -4.2636609 -4.2495351 -4.2374091 -4.2322993 -4.2311077 -4.2289724 -4.2181153 -4.2149959 -4.2212691 -4.240571 -4.2642493][-4.3243332 -4.3134656 -4.299654 -4.2886853 -4.28487 -4.2811646 -4.281343 -4.2840552 -4.2828932 -4.2760763 -4.261507 -4.2568994 -4.2617588 -4.275619 -4.2931938][-4.3377171 -4.3265095 -4.3135986 -4.3037238 -4.300076 -4.3019748 -4.3093934 -4.317121 -4.3174205 -4.3096824 -4.2974505 -4.2952504 -4.3012271 -4.3117428 -4.3229508][-4.3507137 -4.3432159 -4.3336749 -4.3262243 -4.3227663 -4.3248563 -4.3330903 -4.3409438 -4.3415308 -4.336257 -4.3301463 -4.330729 -4.3353462 -4.341135 -4.3462582]]...]
INFO - root - 2017-12-07 17:58:55.609477: step 32410, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 77h:54m:32s remains)
INFO - root - 2017-12-07 17:59:05.246706: step 32420, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 81h:46m:24s remains)
INFO - root - 2017-12-07 17:59:14.869084: step 32430, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 80h:17m:47s remains)
INFO - root - 2017-12-07 17:59:24.572947: step 32440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 78h:41m:59s remains)
INFO - root - 2017-12-07 17:59:34.241998: step 32450, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 78h:44m:15s remains)
INFO - root - 2017-12-07 17:59:43.936911: step 32460, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 82h:33m:45s remains)
INFO - root - 2017-12-07 17:59:53.646980: step 32470, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 83h:26m:11s remains)
INFO - root - 2017-12-07 18:00:03.299330: step 32480, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 78h:27m:50s remains)
INFO - root - 2017-12-07 18:00:13.059101: step 32490, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 75h:54m:30s remains)
INFO - root - 2017-12-07 18:00:22.753492: step 32500, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 77h:54m:55s remains)
2017-12-07 18:00:23.785882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2891364 -4.2830033 -4.2809634 -4.2794733 -4.2800689 -4.2831006 -4.2865334 -4.2870989 -4.2848892 -4.28956 -4.3003531 -4.3101277 -4.3152142 -4.3160038 -4.3152275][-4.2483821 -4.2375731 -4.2324538 -4.2297873 -4.2298856 -4.2338967 -4.2385907 -4.2386761 -4.2346869 -4.2374773 -4.2512712 -4.2649546 -4.2704616 -4.272 -4.27202][-4.2162008 -4.2032571 -4.1984606 -4.1993871 -4.1987677 -4.1982822 -4.199264 -4.1998968 -4.1964307 -4.1976066 -4.2115293 -4.2263494 -4.2307453 -4.2328076 -4.2357559][-4.1957564 -4.1849608 -4.1819053 -4.1847997 -4.1804953 -4.1716175 -4.167603 -4.1699367 -4.1721005 -4.1780734 -4.1904254 -4.2011418 -4.2043982 -4.2090373 -4.2162538][-4.1708312 -4.1650009 -4.1650691 -4.1688094 -4.1557341 -4.1303406 -4.1123095 -4.1108284 -4.1236658 -4.1461 -4.1629033 -4.1719308 -4.1793003 -4.1890149 -4.2009845][-4.144979 -4.1420846 -4.14405 -4.1449203 -4.1166577 -4.0628538 -4.0155821 -4.0008492 -4.0286098 -4.074605 -4.1017184 -4.1139889 -4.1291208 -4.1475596 -4.1679325][-4.1084766 -4.1023917 -4.1009378 -4.094646 -4.0518484 -3.9698296 -3.8860507 -3.853157 -3.89991 -3.9719191 -4.0104918 -4.0278893 -4.050818 -4.0791788 -4.1070423][-4.0849905 -4.0713463 -4.064837 -4.0514126 -3.9976873 -3.9000025 -3.794874 -3.7515407 -3.8077843 -3.8900013 -3.9335 -3.9536271 -3.9805474 -4.0132976 -4.0433445][-4.1064391 -4.0874991 -4.081481 -4.0680838 -4.0189118 -3.936816 -3.8514891 -3.8210397 -3.8610103 -3.9172473 -3.9455321 -3.9619992 -3.9867241 -4.013319 -4.0368237][-4.1404839 -4.1237054 -4.1245823 -4.1187749 -4.085042 -4.0300875 -3.9765577 -3.9616401 -3.981601 -4.0086684 -4.0215716 -4.0338626 -4.0531526 -4.0720663 -4.0899763][-4.1619687 -4.1507239 -4.1605124 -4.1684003 -4.1547823 -4.1252818 -4.0986261 -4.0944471 -4.1035409 -4.1114321 -4.113606 -4.1193938 -4.13153 -4.1434326 -4.1563215][-4.1604152 -4.1539803 -4.1741571 -4.1965981 -4.2003536 -4.1900954 -4.1821442 -4.1858282 -4.1910267 -4.1910548 -4.187418 -4.1874757 -4.1928864 -4.1993465 -4.2069612][-4.1535797 -4.1510844 -4.177918 -4.2076092 -4.2218909 -4.2215543 -4.2220039 -4.2297306 -4.23518 -4.2349691 -4.2313466 -4.228044 -4.2291079 -4.2305088 -4.2321539][-4.1658249 -4.1676254 -4.1932878 -4.222331 -4.2392254 -4.2426186 -4.2450137 -4.250679 -4.2541614 -4.2532444 -4.2487607 -4.242837 -4.2390857 -4.23655 -4.232379][-4.195755 -4.2009997 -4.2215524 -4.2427197 -4.2545815 -4.2560267 -4.2547259 -4.254251 -4.2543235 -4.2518415 -4.2459664 -4.2375641 -4.2291102 -4.2216811 -4.2086782]]...]
INFO - root - 2017-12-07 18:00:33.505432: step 32510, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 78h:07m:39s remains)
INFO - root - 2017-12-07 18:00:43.185890: step 32520, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.994 sec/batch; 82h:47m:37s remains)
INFO - root - 2017-12-07 18:00:52.825438: step 32530, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 80h:52m:17s remains)
INFO - root - 2017-12-07 18:01:02.476706: step 32540, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 78h:58m:31s remains)
INFO - root - 2017-12-07 18:01:11.995508: step 32550, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 80h:06m:52s remains)
INFO - root - 2017-12-07 18:01:21.929989: step 32560, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 81h:06m:41s remains)
INFO - root - 2017-12-07 18:01:31.495383: step 32570, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 82h:06m:28s remains)
INFO - root - 2017-12-07 18:01:41.112304: step 32580, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 82h:43m:41s remains)
INFO - root - 2017-12-07 18:01:50.721734: step 32590, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 83h:07m:29s remains)
INFO - root - 2017-12-07 18:02:00.421874: step 32600, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 83h:08m:23s remains)
2017-12-07 18:02:01.416809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2330804 -4.2570119 -4.2823319 -4.2886028 -4.2875414 -4.2603359 -4.2087398 -4.17668 -4.1959882 -4.2206244 -4.2322092 -4.2311649 -4.2258415 -4.2283568 -4.2487421][-4.2242465 -4.2489419 -4.2711954 -4.2731919 -4.2628326 -4.2356353 -4.1877251 -4.1638989 -4.1875839 -4.2153411 -4.2275977 -4.2254772 -4.2251105 -4.2330127 -4.2595925][-4.196795 -4.2189732 -4.2378359 -4.2439861 -4.235024 -4.2137723 -4.1801243 -4.1649256 -4.179409 -4.2004151 -4.2156978 -4.2138872 -4.2196517 -4.2391357 -4.2737136][-4.1764193 -4.1907864 -4.2074218 -4.2171817 -4.2105827 -4.1965852 -4.17256 -4.157526 -4.1590638 -4.1700239 -4.1952515 -4.2075534 -4.2272272 -4.2575359 -4.2894163][-4.1649661 -4.171927 -4.1898775 -4.2054482 -4.204843 -4.1956773 -4.17849 -4.1583333 -4.1496816 -4.1559157 -4.1943479 -4.2266607 -4.2595344 -4.2875333 -4.3007455][-4.15946 -4.1562862 -4.1677008 -4.1808515 -4.181839 -4.1735649 -4.1599584 -4.1452761 -4.1356244 -4.1491432 -4.1966243 -4.2401047 -4.2809629 -4.3043857 -4.3039985][-4.1775284 -4.1575 -4.15234 -4.1533103 -4.1429291 -4.1260347 -4.1166606 -4.1169271 -4.1211782 -4.14465 -4.1914144 -4.2338061 -4.2762146 -4.302772 -4.2982283][-4.19842 -4.1712475 -4.1590896 -4.1495023 -4.1248579 -4.0954175 -4.0888147 -4.1081505 -4.1266966 -4.1519203 -4.1927986 -4.2316532 -4.2669568 -4.2884121 -4.2798834][-4.201448 -4.1805005 -4.1725163 -4.1576171 -4.1218214 -4.0818791 -4.08094 -4.115036 -4.1466908 -4.1726308 -4.2071967 -4.2378945 -4.2567048 -4.2641973 -4.2527542][-4.206326 -4.1987753 -4.1940346 -4.1722388 -4.1305504 -4.0931945 -4.1012125 -4.1398029 -4.1738987 -4.1993518 -4.2260904 -4.2441049 -4.2447581 -4.2386742 -4.2296748][-4.2309165 -4.2341361 -4.2336416 -4.2117243 -4.1775808 -4.1538987 -4.157856 -4.1810427 -4.203455 -4.2216387 -4.2348003 -4.2409973 -4.232625 -4.2224746 -4.2155585][-4.253356 -4.2597771 -4.2599678 -4.2398233 -4.2173157 -4.2086649 -4.2121572 -4.2224011 -4.2321634 -4.2398567 -4.2433014 -4.2458978 -4.2366538 -4.2256184 -4.2169528][-4.2586331 -4.2642064 -4.2628741 -4.2472057 -4.235662 -4.2358127 -4.2406182 -4.2423582 -4.2406163 -4.2383194 -4.237514 -4.2422509 -4.2396369 -4.2355614 -4.2325988][-4.2514796 -4.2590718 -4.2614093 -4.24981 -4.2407403 -4.2422371 -4.245738 -4.2423387 -4.2342792 -4.2259884 -4.2259307 -4.235364 -4.243649 -4.2501936 -4.2573681][-4.2441878 -4.2519469 -4.25351 -4.2416735 -4.230268 -4.230464 -4.23537 -4.2336869 -4.2314644 -4.23218 -4.2396207 -4.2493687 -4.2581515 -4.2660608 -4.2714105]]...]
INFO - root - 2017-12-07 18:02:11.097386: step 32610, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.005 sec/batch; 83h:42m:32s remains)
INFO - root - 2017-12-07 18:02:20.767244: step 32620, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 78h:32m:23s remains)
INFO - root - 2017-12-07 18:02:30.140301: step 32630, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 75h:33m:13s remains)
INFO - root - 2017-12-07 18:02:39.882970: step 32640, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 81h:29m:19s remains)
INFO - root - 2017-12-07 18:02:49.383381: step 32650, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 77h:59m:35s remains)
INFO - root - 2017-12-07 18:02:59.020243: step 32660, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.041 sec/batch; 86h:43m:42s remains)
INFO - root - 2017-12-07 18:03:08.801117: step 32670, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 82h:41m:29s remains)
INFO - root - 2017-12-07 18:03:18.397050: step 32680, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 76h:09m:51s remains)
INFO - root - 2017-12-07 18:03:28.202863: step 32690, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 77h:52m:45s remains)
INFO - root - 2017-12-07 18:03:37.768265: step 32700, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 81h:41m:54s remains)
2017-12-07 18:03:38.820027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30513 -4.2970104 -4.2960052 -4.295784 -4.2993722 -4.2822113 -4.2602434 -4.2446628 -4.2311921 -4.2280612 -4.2241712 -4.2276258 -4.2175751 -4.1976628 -4.1916251][-4.2967157 -4.2888045 -4.2870502 -4.2850976 -4.2877855 -4.2693219 -4.2442217 -4.2251554 -4.2039704 -4.1924796 -4.1830139 -4.1843204 -4.1709414 -4.1474748 -4.13835][-4.2982183 -4.2949848 -4.2928061 -4.2862873 -4.2852907 -4.2658691 -4.2347307 -4.2028646 -4.1716585 -4.1572952 -4.1498032 -4.1546617 -4.1458168 -4.122498 -4.1084943][-4.3124919 -4.3132772 -4.3090749 -4.2932467 -4.2785592 -4.2461357 -4.2060857 -4.1698608 -4.1372147 -4.1295333 -4.131012 -4.1403747 -4.1364417 -4.1215224 -4.1128254][-4.3254075 -4.3264918 -4.3201842 -4.2957582 -4.2604342 -4.2018185 -4.142148 -4.1006355 -4.082016 -4.1009626 -4.1222057 -4.1374 -4.1423011 -4.1411328 -4.1441555][-4.332108 -4.3335857 -4.3277521 -4.2976193 -4.2420096 -4.155879 -4.0605836 -3.9895902 -3.9901731 -4.0555482 -4.1124368 -4.1447525 -4.1634922 -4.1760235 -4.1897311][-4.3291559 -4.3298583 -4.3270559 -4.2958136 -4.2328186 -4.1255741 -3.9966669 -3.8882465 -3.9035175 -4.0209503 -4.1181331 -4.1746268 -4.2123742 -4.2321658 -4.2477555][-4.3225012 -4.3210611 -4.3188386 -4.2885418 -4.2275434 -4.1233821 -3.9984286 -3.8987498 -3.9217753 -4.0460548 -4.1497488 -4.2096987 -4.2493472 -4.2694712 -4.28331][-4.3172789 -4.31245 -4.3081264 -4.2816191 -4.2307796 -4.1413493 -4.0460515 -3.9872251 -4.0142112 -4.1147628 -4.1959496 -4.2400036 -4.2713642 -4.2896786 -4.3015876][-4.312077 -4.3032861 -4.2972503 -4.2753458 -4.2356973 -4.1668372 -4.1057272 -4.0807681 -4.1068926 -4.1807723 -4.2368803 -4.2642355 -4.2885642 -4.3029194 -4.3127956][-4.3088641 -4.2948112 -4.2862048 -4.2698059 -4.2405329 -4.1885471 -4.1530724 -4.1482592 -4.17301 -4.2276225 -4.2648978 -4.2817187 -4.3017669 -4.3130136 -4.3190827][-4.3070197 -4.2882462 -4.2776723 -4.2682052 -4.248208 -4.21028 -4.1914091 -4.1992264 -4.2200513 -4.2592411 -4.2823286 -4.2922192 -4.3078432 -4.3177619 -4.3239236][-4.3067112 -4.2865658 -4.2756376 -4.2726378 -4.2622142 -4.234921 -4.2254024 -4.2362089 -4.2490873 -4.2736187 -4.2820258 -4.28421 -4.2975011 -4.310802 -4.3226681][-4.3079038 -4.28885 -4.2800384 -4.281507 -4.2780242 -4.2594595 -4.2558208 -4.2643523 -4.2644811 -4.2710757 -4.2653561 -4.2598944 -4.27209 -4.2923837 -4.3123474][-4.3096337 -4.2912321 -4.2844853 -4.2890406 -4.2891035 -4.2753396 -4.2731833 -4.2781835 -4.2638364 -4.2511511 -4.2259722 -4.2108254 -4.2279706 -4.2618694 -4.2936211]]...]
INFO - root - 2017-12-07 18:03:48.412320: step 32710, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.994 sec/batch; 82h:44m:28s remains)
INFO - root - 2017-12-07 18:03:58.005368: step 32720, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.030 sec/batch; 85h:46m:51s remains)
INFO - root - 2017-12-07 18:04:07.603467: step 32730, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 82h:11m:56s remains)
INFO - root - 2017-12-07 18:04:17.394360: step 32740, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 80h:17m:35s remains)
INFO - root - 2017-12-07 18:04:27.087740: step 32750, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 83h:18m:20s remains)
INFO - root - 2017-12-07 18:04:36.719308: step 32760, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 80h:25m:48s remains)
INFO - root - 2017-12-07 18:04:46.515129: step 32770, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 79h:58m:15s remains)
INFO - root - 2017-12-07 18:04:56.054427: step 32780, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 79h:04m:27s remains)
INFO - root - 2017-12-07 18:05:05.583380: step 32790, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 76h:48m:11s remains)
INFO - root - 2017-12-07 18:05:15.268669: step 32800, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 82h:49m:07s remains)
2017-12-07 18:05:16.224854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2206221 -4.2159333 -4.2005634 -4.1824727 -4.1782789 -4.1873679 -4.196434 -4.1983857 -4.1955376 -4.2008238 -4.2190051 -4.2376757 -4.2405415 -4.2318578 -4.2200179][-4.2149363 -4.2151704 -4.2044559 -4.1901407 -4.1892681 -4.20041 -4.2124872 -4.2196383 -4.2208662 -4.225173 -4.2349734 -4.2461267 -4.2470775 -4.2401505 -4.2273517][-4.2131696 -4.214787 -4.2065835 -4.1944108 -4.1941071 -4.2049289 -4.2211347 -4.2353773 -4.2390513 -4.2388196 -4.2399492 -4.2429113 -4.2436633 -4.2394753 -4.2312851][-4.2056184 -4.2058654 -4.1961408 -4.1821074 -4.1770177 -4.1811266 -4.1965261 -4.2146721 -4.2194037 -4.2149539 -4.2093649 -4.209794 -4.216939 -4.2206707 -4.2210121][-4.1922832 -4.1944675 -4.185616 -4.168819 -4.1535573 -4.1421709 -4.1532445 -4.1778841 -4.1836381 -4.1791444 -4.178453 -4.1830177 -4.1947412 -4.2008805 -4.2066193][-4.1754413 -4.1802297 -4.1691971 -4.141768 -4.1056151 -4.0782309 -4.0939207 -4.1383171 -4.1591282 -4.1667733 -4.1787052 -4.1872692 -4.1940713 -4.1929317 -4.1939249][-4.15601 -4.1587806 -4.1412921 -4.1025825 -4.0545487 -4.024229 -4.0496941 -4.1117916 -4.1484332 -4.1690192 -4.1886587 -4.1980944 -4.198894 -4.19084 -4.1841264][-4.15196 -4.1561747 -4.1427932 -4.1139121 -4.0775743 -4.0535855 -4.0676117 -4.11211 -4.1422849 -4.1649756 -4.1885004 -4.2019448 -4.203229 -4.1983848 -4.1953206][-4.1696038 -4.1780744 -4.1741486 -4.1597843 -4.1356421 -4.1091232 -4.0987225 -4.111444 -4.1242938 -4.1403742 -4.1631727 -4.1834126 -4.1933408 -4.2015042 -4.20952][-4.1851344 -4.1935229 -4.1945004 -4.190237 -4.1733251 -4.1432371 -4.116498 -4.1066265 -4.103775 -4.1113558 -4.1319017 -4.1575279 -4.177702 -4.1978903 -4.2138567][-4.2028003 -4.2083745 -4.21047 -4.2124925 -4.2030635 -4.1792874 -4.1530752 -4.1357822 -4.122613 -4.120729 -4.1341057 -4.155983 -4.177104 -4.19669 -4.2121196][-4.2347236 -4.2350564 -4.2361741 -4.2405 -4.2381153 -4.2235312 -4.2049313 -4.1884179 -4.1730623 -4.16575 -4.1694436 -4.1810741 -4.1956925 -4.2113333 -4.224637][-4.2722869 -4.2698412 -4.2699795 -4.274436 -4.275166 -4.2668676 -4.2544394 -4.2415991 -4.227798 -4.2179151 -4.2148108 -4.2176785 -4.2255878 -4.2362928 -4.2460918][-4.3060269 -4.302351 -4.3012109 -4.3027263 -4.3034263 -4.2991385 -4.2923794 -4.2849846 -4.2759724 -4.26865 -4.2643251 -4.2629008 -4.264256 -4.2669678 -4.2692552][-4.3326597 -4.3285904 -4.3271985 -4.3265996 -4.3265419 -4.3242726 -4.3215532 -4.3195419 -4.3163328 -4.3124371 -4.3076124 -4.3028188 -4.2983751 -4.2942719 -4.2898507]]...]
INFO - root - 2017-12-07 18:05:25.947276: step 32810, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 79h:37m:22s remains)
INFO - root - 2017-12-07 18:05:35.528574: step 32820, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 77h:36m:55s remains)
INFO - root - 2017-12-07 18:05:45.119114: step 32830, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 79h:02m:28s remains)
INFO - root - 2017-12-07 18:05:54.892949: step 32840, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.983 sec/batch; 81h:49m:54s remains)
INFO - root - 2017-12-07 18:06:04.407164: step 32850, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 81h:25m:32s remains)
INFO - root - 2017-12-07 18:06:14.112006: step 32860, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 80h:32m:18s remains)
INFO - root - 2017-12-07 18:06:23.526182: step 32870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 80h:02m:04s remains)
INFO - root - 2017-12-07 18:06:33.004425: step 32880, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 77h:45m:01s remains)
INFO - root - 2017-12-07 18:06:42.881343: step 32890, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 80h:52m:51s remains)
INFO - root - 2017-12-07 18:06:52.552319: step 32900, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 78h:54m:13s remains)
2017-12-07 18:06:53.587341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.205318 -4.2021832 -4.20421 -4.2190971 -4.2355337 -4.2509813 -4.2691278 -4.2798204 -4.2767782 -4.2619181 -4.24544 -4.2491961 -4.2537036 -4.2533731 -4.2587638][-4.1788783 -4.1776457 -4.1776586 -4.1961355 -4.2185307 -4.2351809 -4.2553945 -4.2698679 -4.2698426 -4.2581258 -4.2426314 -4.2419567 -4.242003 -4.2410369 -4.2493949][-4.1636305 -4.1639395 -4.1652389 -4.1870027 -4.2131448 -4.2235026 -4.2410688 -4.2601304 -4.2657413 -4.2609143 -4.2524767 -4.252635 -4.2507372 -4.2482538 -4.2526321][-4.147644 -4.1477118 -4.153007 -4.1757889 -4.1966085 -4.2003646 -4.2109489 -4.2257476 -4.2347841 -4.242908 -4.2437663 -4.2469373 -4.247354 -4.2457752 -4.246809][-4.1342168 -4.1305566 -4.1381612 -4.1573892 -4.1672392 -4.1626639 -4.1612096 -4.1583753 -4.1654868 -4.19132 -4.2069678 -4.2133188 -4.2164073 -4.2199349 -4.2254004][-4.1261611 -4.1147017 -4.1189904 -4.1299419 -4.1279883 -4.1118484 -4.0893235 -4.0524678 -4.0558629 -4.1093678 -4.1490088 -4.162653 -4.16924 -4.1797595 -4.1951184][-4.1154051 -4.0965638 -4.0907631 -4.0897231 -4.0815549 -4.0535731 -3.9980667 -3.9125061 -3.9082541 -4.00349 -4.0750403 -4.1028872 -4.1152534 -4.1373134 -4.1683173][-4.1250696 -4.1024947 -4.0934272 -4.0883422 -4.0793295 -4.0462775 -3.9686141 -3.847187 -3.8258212 -3.9399242 -4.0259981 -4.0592771 -4.0767484 -4.1085415 -4.1515336][-4.163507 -4.1447291 -4.14221 -4.1418242 -4.134408 -4.1103511 -4.0528016 -3.9598944 -3.9335165 -4.0011797 -4.0577374 -4.0828056 -4.0938368 -4.1180229 -4.154398][-4.1902061 -4.1755919 -4.1770487 -4.1810308 -4.1806011 -4.1706896 -4.1401033 -4.08679 -4.0648851 -4.0918303 -4.1206069 -4.1366291 -4.1362929 -4.1433868 -4.163682][-4.2020726 -4.1912241 -4.1921597 -4.1962171 -4.2007251 -4.2001739 -4.1865568 -4.1590805 -4.1425562 -4.1495953 -4.1639042 -4.1740103 -4.1656265 -4.1633434 -4.1718893][-4.2236834 -4.2155418 -4.2155347 -4.2193108 -4.2266316 -4.2303433 -4.2257328 -4.2118788 -4.1973109 -4.1919837 -4.1950045 -4.2003431 -4.1907287 -4.1875486 -4.1928463][-4.2588406 -4.2529168 -4.2525382 -4.2553015 -4.2613211 -4.2642431 -4.2621484 -4.2556 -4.2460423 -4.2382064 -4.237093 -4.2404537 -4.2351594 -4.2348394 -4.2388082][-4.2968135 -4.2930031 -4.2922473 -4.2934909 -4.2965508 -4.2977982 -4.2962494 -4.2927246 -4.2865257 -4.2791457 -4.2771239 -4.2811508 -4.2812114 -4.2841253 -4.287046][-4.3190823 -4.3173718 -4.3158555 -4.3153453 -4.3154435 -4.3153706 -4.3146482 -4.31349 -4.3104486 -4.3054247 -4.3041787 -4.3090963 -4.3116722 -4.3139806 -4.3155236]]...]
INFO - root - 2017-12-07 18:07:03.409035: step 32910, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 79h:34m:18s remains)
INFO - root - 2017-12-07 18:07:12.920344: step 32920, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 74h:01m:21s remains)
INFO - root - 2017-12-07 18:07:22.597285: step 32930, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 83h:07m:04s remains)
INFO - root - 2017-12-07 18:07:32.279527: step 32940, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 78h:55m:05s remains)
INFO - root - 2017-12-07 18:07:41.977318: step 32950, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 80h:50m:08s remains)
INFO - root - 2017-12-07 18:07:51.533680: step 32960, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 81h:21m:20s remains)
INFO - root - 2017-12-07 18:08:01.005253: step 32970, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 77h:21m:49s remains)
INFO - root - 2017-12-07 18:08:10.839660: step 32980, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 77h:33m:46s remains)
INFO - root - 2017-12-07 18:08:20.519469: step 32990, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 79h:59m:26s remains)
INFO - root - 2017-12-07 18:08:30.135545: step 33000, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 78h:50m:04s remains)
2017-12-07 18:08:31.083824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29169 -4.2538247 -4.2390618 -4.2380357 -4.2466831 -4.2526641 -4.2542486 -4.2566605 -4.2553263 -4.2641587 -4.2761545 -4.2751217 -4.2662764 -4.2640553 -4.2712412][-4.2652335 -4.219316 -4.2053285 -4.2006025 -4.2048974 -4.2104635 -4.211206 -4.2111659 -4.2075963 -4.227272 -4.2521429 -4.2536492 -4.243104 -4.2414074 -4.2523227][-4.2420411 -4.195085 -4.1823878 -4.1703529 -4.1668286 -4.1727891 -4.1653194 -4.1515212 -4.1456637 -4.1818533 -4.2239027 -4.232461 -4.2260146 -4.2271729 -4.2401891][-4.2266593 -4.1812387 -4.1682425 -4.1459432 -4.1290979 -4.1298928 -4.1081471 -4.0751715 -4.0730586 -4.1322269 -4.1891966 -4.2102709 -4.2119989 -4.2189131 -4.2339048][-4.2168255 -4.1762366 -4.1631413 -4.1331925 -4.1042848 -4.0899005 -4.0383844 -3.9729404 -3.9831576 -4.0746274 -4.1481171 -4.1849475 -4.1979537 -4.2116337 -4.2262607][-4.2247567 -4.1904368 -4.1742887 -4.1402869 -4.1053357 -4.0573282 -3.9407234 -3.8148584 -3.8535905 -4.0034242 -4.1102147 -4.1670113 -4.1916518 -4.207324 -4.219532][-4.24712 -4.21634 -4.1912718 -4.1536632 -4.1095834 -4.0125012 -3.8088977 -3.6073871 -3.6994021 -3.9298129 -4.0782194 -4.1573544 -4.1950669 -4.2080226 -4.2141161][-4.2697248 -4.2375593 -4.1986074 -4.1504717 -4.0932302 -3.9587533 -3.6950746 -3.4509695 -3.6106412 -3.8935938 -4.06324 -4.1563315 -4.19989 -4.2078724 -4.2078133][-4.2921629 -4.2583365 -4.2170453 -4.169044 -4.1141071 -3.992779 -3.7605886 -3.567616 -3.711509 -3.9476118 -4.08514 -4.1655188 -4.2051196 -4.2087617 -4.206646][-4.3165956 -4.2914667 -4.2618675 -4.2270117 -4.1798444 -4.0832853 -3.9100809 -3.7847896 -3.8872824 -4.0450554 -4.1309543 -4.1857071 -4.2141423 -4.2173514 -4.2156711][-4.3316083 -4.3185434 -4.3041735 -4.2832928 -4.2425179 -4.1621456 -4.0320592 -3.9599652 -4.0346131 -4.1323094 -4.1798453 -4.2123909 -4.2301278 -4.2345324 -4.2336659][-4.3354239 -4.3288894 -4.3203912 -4.30309 -4.265676 -4.201273 -4.103766 -4.063765 -4.1244779 -4.190011 -4.2208438 -4.2407765 -4.2527943 -4.2555904 -4.2545509][-4.3309779 -4.3253078 -4.3192353 -4.2997427 -4.2653904 -4.2195249 -4.1537724 -4.1375079 -4.1861286 -4.2275915 -4.2486725 -4.2629285 -4.2711277 -4.2718616 -4.2701073][-4.3203449 -4.313828 -4.3090715 -4.2918615 -4.2658243 -4.2388248 -4.2019916 -4.1977878 -4.2309713 -4.2554164 -4.2696581 -4.2761512 -4.2808604 -4.2819481 -4.2827682][-4.3135548 -4.3056297 -4.3005271 -4.2886124 -4.2739463 -4.2588224 -4.2435517 -4.2460418 -4.2656217 -4.2784672 -4.285841 -4.2850919 -4.2880635 -4.2936387 -4.3006549]]...]
INFO - root - 2017-12-07 18:08:40.669707: step 33010, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 81h:56m:08s remains)
INFO - root - 2017-12-07 18:08:50.201318: step 33020, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 78h:55m:02s remains)
INFO - root - 2017-12-07 18:08:59.989569: step 33030, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 79h:33m:22s remains)
INFO - root - 2017-12-07 18:09:09.684949: step 33040, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 81h:03m:26s remains)
INFO - root - 2017-12-07 18:09:19.324026: step 33050, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 82h:55m:46s remains)
INFO - root - 2017-12-07 18:09:28.778346: step 33060, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.029 sec/batch; 85h:37m:08s remains)
INFO - root - 2017-12-07 18:09:38.582741: step 33070, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.010 sec/batch; 83h:59m:48s remains)
INFO - root - 2017-12-07 18:09:48.227155: step 33080, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 79h:29m:33s remains)
INFO - root - 2017-12-07 18:09:57.778729: step 33090, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 80h:20m:47s remains)
INFO - root - 2017-12-07 18:10:07.491367: step 33100, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 78h:47m:27s remains)
2017-12-07 18:10:08.426655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3006864 -4.2850475 -4.2850461 -4.2934275 -4.2974806 -4.2943225 -4.2825303 -4.2545109 -4.2188148 -4.1881204 -4.1705441 -4.1789584 -4.2101688 -4.2430649 -4.2615805][-4.2830286 -4.26428 -4.2651243 -4.2777009 -4.2849579 -4.2807689 -4.2610197 -4.2199421 -4.1801591 -4.1582303 -4.1548276 -4.1760588 -4.208313 -4.2345924 -4.2481294][-4.2693634 -4.2480521 -4.2504478 -4.2671218 -4.2770109 -4.2688241 -4.23682 -4.1808362 -4.1415787 -4.1390824 -4.157968 -4.1934695 -4.221169 -4.232244 -4.2375555][-4.2707272 -4.2411594 -4.2395716 -4.2551866 -4.265389 -4.2499609 -4.2076468 -4.1439409 -4.1136975 -4.1369863 -4.1770616 -4.2180905 -4.2374949 -4.2391458 -4.2453446][-4.2767096 -4.2373075 -4.2283444 -4.2387938 -4.24216 -4.2196269 -4.1745648 -4.1171379 -4.105566 -4.1474695 -4.1946368 -4.2310653 -4.2451043 -4.2455506 -4.2529321][-4.2767458 -4.23344 -4.2204165 -4.2249928 -4.2201772 -4.1958022 -4.1567593 -4.1118779 -4.1161671 -4.1670513 -4.2134147 -4.2422934 -4.2496467 -4.2503591 -4.2539492][-4.2672887 -4.2234936 -4.211936 -4.2115731 -4.2057214 -4.1873665 -4.157433 -4.1232996 -4.1307707 -4.1756768 -4.2194371 -4.245307 -4.2541351 -4.2577929 -4.2556534][-4.2564335 -4.2189579 -4.2147436 -4.2147937 -4.213264 -4.2004862 -4.175878 -4.1419773 -4.1346478 -4.1608858 -4.1976523 -4.2244644 -4.24081 -4.2500834 -4.2475266][-4.2482533 -4.2166467 -4.216146 -4.2195024 -4.2230673 -4.2151361 -4.1920347 -4.1529465 -4.1313696 -4.1474838 -4.1820292 -4.2099566 -4.2279887 -4.2344203 -4.2290568][-4.2357087 -4.2051039 -4.2071776 -4.2181005 -4.2279544 -4.224751 -4.198638 -4.1540813 -4.1278973 -4.1413007 -4.1742277 -4.2014489 -4.2157106 -4.2174525 -4.2118592][-4.2268643 -4.1938725 -4.1888518 -4.2021513 -4.2186527 -4.2205753 -4.1935358 -4.1508346 -4.1327491 -4.1509519 -4.179925 -4.2022219 -4.2087011 -4.2062969 -4.2007184][-4.2143359 -4.1745906 -4.1608233 -4.1635718 -4.1788263 -4.1893563 -4.1743178 -4.1487532 -4.1483746 -4.1740804 -4.1966209 -4.2092347 -4.2098484 -4.2037926 -4.2030034][-4.2082496 -4.1706104 -4.1544809 -4.1413026 -4.134675 -4.1421609 -4.1440835 -4.1430516 -4.1601896 -4.191977 -4.2066827 -4.21105 -4.2168188 -4.2153769 -4.2185073][-4.2269611 -4.2023616 -4.1942759 -4.1739955 -4.1393003 -4.1242361 -4.1354804 -4.1580014 -4.184052 -4.2092257 -4.210803 -4.2108569 -4.2277594 -4.2377429 -4.2422957][-4.2451782 -4.2326169 -4.2330446 -4.2157865 -4.1763544 -4.1533031 -4.1631646 -4.1913977 -4.216351 -4.2277675 -4.2177844 -4.2111278 -4.2305117 -4.2452588 -4.2529235]]...]
INFO - root - 2017-12-07 18:10:18.009070: step 33110, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 77h:04m:46s remains)
INFO - root - 2017-12-07 18:10:27.555988: step 33120, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 80h:39m:39s remains)
INFO - root - 2017-12-07 18:10:37.189092: step 33130, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 81h:01m:26s remains)
INFO - root - 2017-12-07 18:10:46.843083: step 33140, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 83h:12m:22s remains)
INFO - root - 2017-12-07 18:10:56.546993: step 33150, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.994 sec/batch; 82h:37m:11s remains)
INFO - root - 2017-12-07 18:11:06.056519: step 33160, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 76h:14m:37s remains)
INFO - root - 2017-12-07 18:11:15.563824: step 33170, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:59m:43s remains)
INFO - root - 2017-12-07 18:11:25.471407: step 33180, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.033 sec/batch; 85h:54m:28s remains)
INFO - root - 2017-12-07 18:11:35.039102: step 33190, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 80h:54m:09s remains)
INFO - root - 2017-12-07 18:11:44.442189: step 33200, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 75h:44m:51s remains)
2017-12-07 18:11:45.422919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2957892 -4.2933049 -4.29348 -4.2846074 -4.2697568 -4.2428031 -4.2034807 -4.1644254 -4.1401663 -4.1374192 -4.1480222 -4.1662011 -4.1851087 -4.1953926 -4.2150631][-4.2869468 -4.2871909 -4.2898903 -4.281867 -4.2664828 -4.2286525 -4.1745715 -4.1260648 -4.1001439 -4.1063323 -4.130188 -4.158854 -4.1799841 -4.19322 -4.2174697][-4.2770967 -4.2816787 -4.2855892 -4.276052 -4.25623 -4.2119489 -4.1515179 -4.1026416 -4.0848341 -4.1056037 -4.13641 -4.1652446 -4.1852031 -4.2021723 -4.2292762][-4.2535024 -4.2690754 -4.2806635 -4.2723975 -4.2481046 -4.2021866 -4.1390643 -4.0962324 -4.0923696 -4.1277428 -4.1575584 -4.176362 -4.1898708 -4.207438 -4.2358289][-4.2201176 -4.2487741 -4.2719474 -4.2719827 -4.2478886 -4.1986265 -4.1285777 -4.0854073 -4.0950246 -4.1433821 -4.1733241 -4.1842942 -4.191339 -4.2056675 -4.2338386][-4.1884646 -4.2217474 -4.2514505 -4.2579222 -4.2301979 -4.16573 -4.0747867 -4.016068 -4.0446215 -4.118156 -4.1610341 -4.1732121 -4.1814895 -4.1996341 -4.2298613][-4.1524086 -4.1834812 -4.219183 -4.2280245 -4.1902213 -4.1001997 -3.9680791 -3.8686352 -3.9271879 -4.0492415 -4.1190557 -4.1449823 -4.164854 -4.1949124 -4.2307029][-4.0952039 -4.1253672 -4.1716003 -4.187716 -4.1510305 -4.0473022 -3.8793988 -3.7426264 -3.8293147 -3.9928944 -4.0787535 -4.1155624 -4.1487041 -4.1903715 -4.2315931][-4.0532789 -4.0814357 -4.1382041 -4.1696553 -4.1500258 -4.0666347 -3.9233663 -3.8121476 -3.8879857 -4.0283689 -4.0888109 -4.1083426 -4.1349206 -4.1812911 -4.2246008][-4.0926576 -4.1111379 -4.1554122 -4.1844997 -4.1771922 -4.1275086 -4.0406122 -3.9790132 -4.0244341 -4.1074109 -4.1323214 -4.1293654 -4.1415639 -4.1838555 -4.2207189][-4.175437 -4.181663 -4.2035933 -4.2191362 -4.2139292 -4.1869211 -4.1424446 -4.1095967 -4.1319752 -4.1727815 -4.1788807 -4.16895 -4.1759729 -4.2113094 -4.23658][-4.2396727 -4.2362766 -4.241168 -4.2456245 -4.2397485 -4.2214394 -4.197885 -4.1777582 -4.1889691 -4.2092285 -4.2085948 -4.2010674 -4.2081356 -4.2371798 -4.2549376][-4.270885 -4.2610455 -4.2557845 -4.2537651 -4.2445426 -4.2301488 -4.2170515 -4.2064176 -4.2166028 -4.2286649 -4.226368 -4.2162766 -4.2231741 -4.2503943 -4.2678289][-4.2924128 -4.2810559 -4.2704339 -4.2627692 -4.2507524 -4.2395368 -4.2311239 -4.2271128 -4.2378221 -4.2482495 -4.2465615 -4.23868 -4.2481685 -4.2713962 -4.2886882][-4.3069391 -4.2933636 -4.279449 -4.2695518 -4.2615266 -4.2554436 -4.2500792 -4.2487011 -4.2570305 -4.2679429 -4.2715635 -4.2696037 -4.2815948 -4.300488 -4.3150315]]...]
INFO - root - 2017-12-07 18:11:55.078539: step 33210, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 81h:35m:07s remains)
INFO - root - 2017-12-07 18:12:04.887729: step 33220, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 80h:48m:59s remains)
INFO - root - 2017-12-07 18:12:14.672751: step 33230, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 77h:29m:37s remains)
INFO - root - 2017-12-07 18:12:24.463009: step 33240, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 83h:15m:07s remains)
INFO - root - 2017-12-07 18:12:34.183773: step 33250, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 77h:38m:35s remains)
INFO - root - 2017-12-07 18:12:43.576952: step 33260, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 79h:23m:36s remains)
INFO - root - 2017-12-07 18:12:53.137590: step 33270, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 79h:56m:40s remains)
INFO - root - 2017-12-07 18:13:02.829553: step 33280, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 80h:02m:17s remains)
INFO - root - 2017-12-07 18:13:12.338631: step 33290, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 77h:34m:07s remains)
INFO - root - 2017-12-07 18:13:22.069985: step 33300, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 82h:14m:54s remains)
2017-12-07 18:13:23.112007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2768745 -4.290658 -4.3005271 -4.2860646 -4.2522459 -4.2178988 -4.1795053 -4.1423411 -4.1229191 -4.1430993 -4.188952 -4.2396808 -4.2875028 -4.3182836 -4.3332663][-4.2938805 -4.3097558 -4.3202863 -4.3006921 -4.2597589 -4.2201476 -4.1732736 -4.1248264 -4.0988269 -4.1238065 -4.1739316 -4.23048 -4.2848115 -4.3177943 -4.3325086][-4.3005619 -4.3177042 -4.3305745 -4.3093753 -4.2643647 -4.2188053 -4.159946 -4.0983777 -4.0695653 -4.1046762 -4.1608629 -4.2228651 -4.284287 -4.3179913 -4.3316569][-4.2882643 -4.3057032 -4.3210349 -4.3012066 -4.2563529 -4.2031074 -4.1323948 -4.0632925 -4.0385895 -4.0853896 -4.1498528 -4.2187729 -4.2851796 -4.3190813 -4.3313437][-4.2714872 -4.2882996 -4.3026385 -4.2813172 -4.2347956 -4.171977 -4.0916443 -4.021307 -4.0086565 -4.0703998 -4.144361 -4.2237091 -4.2939277 -4.32547 -4.3337641][-4.2610555 -4.2752795 -4.2827449 -4.2572832 -4.206593 -4.1315794 -4.0432606 -3.9848604 -3.9917402 -4.06649 -4.1471786 -4.2344708 -4.3052545 -4.3336315 -4.3385625][-4.2471447 -4.2544942 -4.2519145 -4.2197375 -4.1581588 -4.0641518 -3.970314 -3.9445314 -3.9841464 -4.06991 -4.1537356 -4.2431812 -4.3128638 -4.340179 -4.3431888][-4.2353582 -4.235013 -4.2243748 -4.1877828 -4.1162448 -4.0031681 -3.9028802 -3.9136367 -3.9851141 -4.0793433 -4.1651888 -4.2512612 -4.3169665 -4.3436718 -4.3472381][-4.2375693 -4.2337813 -4.2185674 -4.1858835 -4.1196642 -4.0116754 -3.9207921 -3.9436524 -4.0139403 -4.0961938 -4.1734896 -4.2521119 -4.3149323 -4.3440518 -4.3501043][-4.2469578 -4.2438455 -4.2290192 -4.2018542 -4.1481118 -4.0584812 -3.9868183 -4.0035982 -4.05358 -4.1116734 -4.1762609 -4.2486138 -4.3108654 -4.3416944 -4.3498225][-4.2560925 -4.2537084 -4.2409916 -4.217135 -4.1746168 -4.1050816 -4.0519366 -4.0591226 -4.0906286 -4.1315928 -4.184721 -4.2490029 -4.3085489 -4.3390856 -4.3485575][-4.2694931 -4.2635937 -4.2490287 -4.2261767 -4.1935148 -4.1404729 -4.0989671 -4.0971913 -4.1187749 -4.1530375 -4.2000818 -4.2560129 -4.3093572 -4.33841 -4.3480129][-4.2831798 -4.2752433 -4.2612243 -4.2414923 -4.2184806 -4.1778502 -4.1421814 -4.1303186 -4.1430421 -4.174159 -4.2177391 -4.2672772 -4.314198 -4.33951 -4.3483005][-4.2970271 -4.2890215 -4.2777314 -4.2646179 -4.247869 -4.214747 -4.1835437 -4.1687713 -4.1746292 -4.1994081 -4.2347426 -4.2771497 -4.3175449 -4.3405237 -4.348526][-4.3044882 -4.2933369 -4.2826557 -4.2718253 -4.2555261 -4.2285256 -4.2046733 -4.1929874 -4.1959534 -4.2163138 -4.2449393 -4.2827621 -4.3193722 -4.3411098 -4.3486772]]...]
INFO - root - 2017-12-07 18:13:32.748069: step 33310, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 83h:07m:38s remains)
INFO - root - 2017-12-07 18:13:42.532739: step 33320, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 80h:35m:21s remains)
INFO - root - 2017-12-07 18:13:52.134198: step 33330, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 76h:03m:51s remains)
INFO - root - 2017-12-07 18:14:01.778831: step 33340, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 83h:44m:24s remains)
INFO - root - 2017-12-07 18:14:11.372583: step 33350, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 82h:31m:33s remains)
INFO - root - 2017-12-07 18:14:21.014536: step 33360, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 80h:12m:24s remains)
INFO - root - 2017-12-07 18:14:30.535430: step 33370, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 77h:04m:09s remains)
INFO - root - 2017-12-07 18:14:40.019607: step 33380, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 79h:28m:00s remains)
INFO - root - 2017-12-07 18:14:49.666990: step 33390, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 82h:20m:03s remains)
INFO - root - 2017-12-07 18:14:59.513143: step 33400, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 84h:04m:58s remains)
2017-12-07 18:15:00.525661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2057343 -4.2059789 -4.2042952 -4.1953545 -4.1904235 -4.1948905 -4.2063689 -4.21934 -4.2356353 -4.2537246 -4.2567759 -4.2495117 -4.2426715 -4.2371707 -4.2239594][-4.1633983 -4.1695166 -4.1788173 -4.1878867 -4.1926947 -4.2042513 -4.2226024 -4.2349482 -4.242682 -4.2582779 -4.2608209 -4.2502422 -4.2402072 -4.2307749 -4.2152734][-4.1498737 -4.1653 -4.1894374 -4.2183824 -4.2323604 -4.2425809 -4.2587729 -4.267561 -4.2658134 -4.2741313 -4.2780685 -4.2685056 -4.2555647 -4.2428346 -4.2259774][-4.182333 -4.2082696 -4.240387 -4.2684717 -4.2806344 -4.2841039 -4.2896786 -4.2859378 -4.2711887 -4.2696438 -4.2786822 -4.2805705 -4.2719455 -4.2615223 -4.2445827][-4.2411795 -4.2647362 -4.2882733 -4.2986045 -4.3014197 -4.2933874 -4.281146 -4.2624393 -4.2399487 -4.2341275 -4.2471685 -4.2600436 -4.2615576 -4.25686 -4.2425876][-4.2871866 -4.297286 -4.3012609 -4.2960787 -4.2837329 -4.2556181 -4.2203746 -4.1934829 -4.1833339 -4.1811919 -4.1966262 -4.2253704 -4.2393155 -4.2429914 -4.2455125][-4.3008895 -4.2905459 -4.2777953 -4.2609096 -4.2356267 -4.1846242 -4.1125612 -4.0764422 -4.0915384 -4.1081805 -4.1338334 -4.1800456 -4.2127647 -4.2349467 -4.2621098][-4.3002439 -4.2790461 -4.2500486 -4.2175174 -4.179275 -4.1051679 -3.9938421 -3.9393353 -3.9869847 -4.0360427 -4.08114 -4.1415095 -4.1899948 -4.2320037 -4.2756557][-4.3021607 -4.2798181 -4.2355042 -4.1843014 -4.1358047 -4.0612326 -3.9516637 -3.9009128 -3.9601085 -4.025034 -4.0858588 -4.1509104 -4.2027912 -4.2484727 -4.2902007][-4.3014245 -4.276897 -4.2318668 -4.1793604 -4.1345921 -4.0834308 -4.021585 -4.0050926 -4.05139 -4.1054897 -4.1636367 -4.2210159 -4.2632961 -4.2949591 -4.3180761][-4.2932725 -4.2631855 -4.224853 -4.1852713 -4.1550937 -4.1332421 -4.1221619 -4.1412115 -4.1761489 -4.2123513 -4.2540989 -4.29497 -4.3202119 -4.3348427 -4.340919][-4.2621851 -4.219234 -4.1829429 -4.1608467 -4.1514068 -4.15998 -4.1878047 -4.227726 -4.2557025 -4.27533 -4.2991076 -4.3224115 -4.3356223 -4.3417525 -4.3400393][-4.2164111 -4.1604624 -4.1220379 -4.1120868 -4.1260219 -4.1610332 -4.2134566 -4.2645497 -4.2891717 -4.2962489 -4.3035274 -4.31424 -4.3231535 -4.3242126 -4.3182988][-4.1666389 -4.1021833 -4.0608931 -4.0645771 -4.1065631 -4.1637025 -4.2260122 -4.2724042 -4.2875686 -4.2859421 -4.2820978 -4.2829385 -4.2900109 -4.2915173 -4.2798991][-4.1479568 -4.0946083 -4.0631127 -4.0755143 -4.1249204 -4.1817384 -4.2332392 -4.2627096 -4.2659945 -4.25737 -4.2451982 -4.2362971 -4.2417426 -4.2447009 -4.2318845]]...]
INFO - root - 2017-12-07 18:15:10.091555: step 33410, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 81h:50m:51s remains)
INFO - root - 2017-12-07 18:15:19.639969: step 33420, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 74h:36m:36s remains)
INFO - root - 2017-12-07 18:15:29.279354: step 33430, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 81h:38m:18s remains)
INFO - root - 2017-12-07 18:15:38.950732: step 33440, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 81h:32m:41s remains)
INFO - root - 2017-12-07 18:15:48.581040: step 33450, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 80h:36m:06s remains)
INFO - root - 2017-12-07 18:15:58.213321: step 33460, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 80h:40m:58s remains)
INFO - root - 2017-12-07 18:16:07.900721: step 33470, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 84h:01m:14s remains)
INFO - root - 2017-12-07 18:16:17.620998: step 33480, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 78h:01m:52s remains)
INFO - root - 2017-12-07 18:16:27.246339: step 33490, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 79h:11m:38s remains)
INFO - root - 2017-12-07 18:16:37.016766: step 33500, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 80h:28m:16s remains)
2017-12-07 18:16:37.984336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.339303 -4.3351736 -4.3306451 -4.3242478 -4.3155808 -4.3039279 -4.2868276 -4.2749639 -4.277885 -4.2899032 -4.2990212 -4.2990074 -4.2897892 -4.2763176 -4.265769][-4.3494191 -4.34814 -4.341177 -4.32704 -4.3051748 -4.2805047 -4.2523494 -4.23079 -4.2287 -4.2445068 -4.2631316 -4.2720165 -4.2677565 -4.2588925 -4.2513285][-4.3597937 -4.3598461 -4.3473063 -4.3233337 -4.2886696 -4.2504911 -4.2116485 -4.1788111 -4.1709156 -4.1910944 -4.2210631 -4.2406869 -4.2433743 -4.2421479 -4.2412453][-4.3660197 -4.3662548 -4.3495331 -4.3183041 -4.2723455 -4.2221928 -4.1733475 -4.1300044 -4.1197505 -4.1474762 -4.1879616 -4.2137179 -4.2169223 -4.2185297 -4.2218604][-4.3683076 -4.3679104 -4.3477521 -4.3098207 -4.25419 -4.1924505 -4.13449 -4.0855141 -4.08074 -4.1229134 -4.1735692 -4.2011495 -4.2007933 -4.1996088 -4.2036276][-4.3662062 -4.3622756 -4.3367152 -4.2908468 -4.2264805 -4.1538887 -4.09126 -4.0438166 -4.0512538 -4.1104054 -4.1735897 -4.2053318 -4.2041292 -4.2004824 -4.2045765][-4.3604631 -4.350697 -4.3182621 -4.265027 -4.1923046 -4.1129723 -4.0519505 -4.0129409 -4.0346141 -4.1086025 -4.1836786 -4.2214189 -4.2234993 -4.2215333 -4.2292128][-4.3537774 -4.3392444 -4.30242 -4.2468705 -4.1714745 -4.09353 -4.0437851 -4.023695 -4.062161 -4.1414318 -4.216023 -4.2519188 -4.2546453 -4.2549539 -4.2655358][-4.3493247 -4.33268 -4.2955027 -4.24379 -4.1769137 -4.1138673 -4.08214 -4.0813479 -4.1284528 -4.2003031 -4.2618837 -4.2903056 -4.2915373 -4.2908421 -4.2982907][-4.3479567 -4.3308263 -4.295927 -4.2525659 -4.2012405 -4.1584487 -4.1419272 -4.1502204 -4.195158 -4.2541528 -4.2995048 -4.3196645 -4.3201036 -4.3195796 -4.325099][-4.3443627 -4.3264375 -4.2948709 -4.2613888 -4.2258272 -4.2034621 -4.1993637 -4.2121658 -4.2508721 -4.2959652 -4.3263941 -4.3383703 -4.33753 -4.3378992 -4.3410883][-4.3381205 -4.3192854 -4.29224 -4.267591 -4.2473555 -4.2445722 -4.2535558 -4.2704916 -4.2988667 -4.3285851 -4.3446341 -4.3472829 -4.34248 -4.3399391 -4.3390846][-4.3316879 -4.3134656 -4.2920375 -4.2774816 -4.2731295 -4.2845407 -4.29998 -4.3148685 -4.3317604 -4.3466368 -4.3497643 -4.3443894 -4.3364959 -4.3299074 -4.3230019][-4.3264213 -4.3106027 -4.2964964 -4.2921462 -4.2999992 -4.3180141 -4.3312616 -4.3376136 -4.3430386 -4.347682 -4.3439412 -4.3350077 -4.3247266 -4.3137431 -4.2975054][-4.3247237 -4.3121228 -4.3055544 -4.3089948 -4.322278 -4.3400669 -4.3489304 -4.34871 -4.3456364 -4.3429074 -4.3348484 -4.3255882 -4.3138018 -4.2995243 -4.2757483]]...]
INFO - root - 2017-12-07 18:16:47.773903: step 33510, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 79h:57m:32s remains)
INFO - root - 2017-12-07 18:16:57.603395: step 33520, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 80h:44m:00s remains)
INFO - root - 2017-12-07 18:17:07.029830: step 33530, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 78h:55m:23s remains)
INFO - root - 2017-12-07 18:17:16.619183: step 33540, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 80h:51m:58s remains)
INFO - root - 2017-12-07 18:17:26.174830: step 33550, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.017 sec/batch; 84h:26m:04s remains)
INFO - root - 2017-12-07 18:17:35.751502: step 33560, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 77h:20m:04s remains)
INFO - root - 2017-12-07 18:17:45.355254: step 33570, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 79h:11m:21s remains)
INFO - root - 2017-12-07 18:17:55.011332: step 33580, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.025 sec/batch; 85h:08m:34s remains)
INFO - root - 2017-12-07 18:18:04.647429: step 33590, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 82h:23m:57s remains)
INFO - root - 2017-12-07 18:18:14.148738: step 33600, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 69h:25m:49s remains)
2017-12-07 18:18:15.107303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3118815 -4.2954464 -4.2798643 -4.2707887 -4.2660465 -4.2705722 -4.2813315 -4.2860622 -4.2890205 -4.2925143 -4.2965808 -4.3047366 -4.3116422 -4.3140497 -4.3130574][-4.3048077 -4.2854633 -4.2664428 -4.2546821 -4.24981 -4.2580686 -4.2761021 -4.2881303 -4.2953148 -4.3006849 -4.30182 -4.3054996 -4.3102632 -4.3123446 -4.3121953][-4.2942705 -4.26923 -4.2440081 -4.2257977 -4.217958 -4.229044 -4.2518692 -4.2690315 -4.2826695 -4.2934666 -4.294632 -4.2951164 -4.29903 -4.3027649 -4.3058763][-4.2859221 -4.2545385 -4.2214341 -4.1952448 -4.1815424 -4.191494 -4.2161455 -4.2388625 -4.2628541 -4.2828317 -4.2873631 -4.2872944 -4.2900009 -4.2949753 -4.3008256][-4.285 -4.2497611 -4.2086272 -4.1720533 -4.1471291 -4.1492071 -4.1701703 -4.1951575 -4.2318239 -4.2641678 -4.2748756 -4.2790985 -4.2830887 -4.2871609 -4.294342][-4.295157 -4.2620831 -4.2168922 -4.168992 -4.1239047 -4.1007996 -4.101366 -4.1182184 -4.1663842 -4.2158003 -4.2388105 -4.2521496 -4.2610435 -4.266685 -4.2772236][-4.3116889 -4.2858195 -4.2447386 -4.1925015 -4.1304531 -4.07442 -4.0320768 -4.0166936 -4.063704 -4.1318169 -4.1748538 -4.2029572 -4.2208581 -4.23164 -4.2470179][-4.3220763 -4.3029437 -4.2707763 -4.2235374 -4.1581249 -4.0809565 -3.9961479 -3.926863 -3.9401813 -4.009922 -4.0736752 -4.1248026 -4.1589632 -4.1807547 -4.2033057][-4.3252621 -4.3144531 -4.2966018 -4.2642841 -4.21032 -4.1350808 -4.0366783 -3.9332893 -3.8939803 -3.9306068 -3.9920504 -4.0542145 -4.10201 -4.1331086 -4.1606526][-4.3272848 -4.32398 -4.3192887 -4.3033981 -4.2680826 -4.2116213 -4.1268654 -4.0278568 -3.9636271 -3.9586079 -3.9889171 -4.0355544 -4.0803847 -4.1108518 -4.138051][-4.3301635 -4.3282037 -4.3270106 -4.3220706 -4.3032551 -4.268373 -4.2099171 -4.1336374 -4.0726223 -4.0480695 -4.0495515 -4.0687666 -4.0965495 -4.1194429 -4.1415639][-4.3376718 -4.3363709 -4.3343935 -4.3334942 -4.326364 -4.3082891 -4.2722135 -4.216938 -4.1665831 -4.1400237 -4.1303258 -4.1343222 -4.1479921 -4.1616673 -4.1761618][-4.34669 -4.3477855 -4.3447289 -4.3444114 -4.3421469 -4.3337812 -4.3139467 -4.2771869 -4.2404208 -4.220293 -4.2106748 -4.2121248 -4.221549 -4.2305803 -4.239604][-4.3495703 -4.3531027 -4.3504639 -4.3499069 -4.349462 -4.3471546 -4.3388324 -4.319716 -4.3002248 -4.2908497 -4.2859216 -4.2872028 -4.2944207 -4.3007207 -4.306077][-4.3474674 -4.3513818 -4.3504 -4.3497448 -4.350492 -4.3516107 -4.3495297 -4.3412557 -4.3337326 -4.3319607 -4.3315921 -4.3338466 -4.3394985 -4.3444972 -4.3479]]...]
INFO - root - 2017-12-07 18:18:24.680331: step 33610, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 81h:09m:12s remains)
INFO - root - 2017-12-07 18:18:34.239161: step 33620, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 80h:01m:04s remains)
INFO - root - 2017-12-07 18:18:43.906124: step 33630, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 77h:46m:12s remains)
INFO - root - 2017-12-07 18:18:53.614537: step 33640, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 78h:11m:00s remains)
INFO - root - 2017-12-07 18:19:03.179685: step 33650, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 81h:52m:01s remains)
INFO - root - 2017-12-07 18:19:13.037982: step 33660, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 79h:14m:19s remains)
INFO - root - 2017-12-07 18:19:22.598363: step 33670, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 78h:48m:43s remains)
INFO - root - 2017-12-07 18:19:32.334237: step 33680, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 82h:50m:13s remains)
INFO - root - 2017-12-07 18:19:41.866719: step 33690, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 78h:21m:59s remains)
INFO - root - 2017-12-07 18:19:51.377808: step 33700, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 81h:31m:54s remains)
2017-12-07 18:19:52.323156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2814765 -4.2872195 -4.3073406 -4.32959 -4.3329825 -4.3170471 -4.3092532 -4.3115387 -4.3060541 -4.2953219 -4.2890024 -4.2941313 -4.3059874 -4.3222275 -4.3399353][-4.2360587 -4.2446051 -4.2725978 -4.2972 -4.295155 -4.2726049 -4.2659788 -4.273066 -4.2669148 -4.2558842 -4.2516255 -4.2580938 -4.2714486 -4.292325 -4.317956][-4.1808491 -4.1980357 -4.2385182 -4.2627015 -4.2503014 -4.2218428 -4.2163591 -4.2301335 -4.2277226 -4.2215891 -4.2242012 -4.2354655 -4.2505531 -4.2727447 -4.3004651][-4.1431074 -4.1684241 -4.21391 -4.230608 -4.2053328 -4.1705923 -4.1628222 -4.1771703 -4.1801853 -4.1858478 -4.1995749 -4.2204556 -4.2463856 -4.2726235 -4.2957964][-4.1389647 -4.1597285 -4.1960731 -4.1994915 -4.157918 -4.1132588 -4.0969243 -4.1078877 -4.1160669 -4.13543 -4.1645293 -4.1974239 -4.2367916 -4.2714353 -4.2927918][-4.150938 -4.1606627 -4.1799045 -4.1660075 -4.1110425 -4.0463157 -4.0136366 -4.02981 -4.0504732 -4.0738072 -4.10951 -4.1552572 -4.2098351 -4.2569351 -4.2792797][-4.1748104 -4.1714435 -4.16851 -4.1347117 -4.0660133 -3.9846907 -3.9385152 -3.9564528 -3.979413 -4.0006886 -4.0425048 -4.1031885 -4.1705489 -4.2258062 -4.2533665][-4.2003331 -4.179884 -4.1576371 -4.1143961 -4.0467567 -3.9672601 -3.9159551 -3.9192326 -3.9171295 -3.9202363 -3.9694412 -4.0470109 -4.122035 -4.1816621 -4.21795][-4.2189517 -4.19193 -4.1635413 -4.1231828 -4.0683641 -4.00794 -3.9679282 -3.9632759 -3.9431634 -3.9233766 -3.9641476 -4.0412197 -4.1117415 -4.1643615 -4.2006903][-4.2352409 -4.2130203 -4.1911535 -4.1618824 -4.1228218 -4.0862646 -4.0663886 -4.06384 -4.0443416 -4.0173736 -4.0380745 -4.0945392 -4.1439786 -4.18437 -4.2138829][-4.2511606 -4.2367806 -4.22626 -4.2118387 -4.1899176 -4.1715508 -4.1642637 -4.1612091 -4.1422229 -4.11865 -4.1265311 -4.1608248 -4.1911221 -4.2209244 -4.2469778][-4.2681141 -4.2590961 -4.2557173 -4.2517796 -4.2437477 -4.2387362 -4.2386308 -4.2357926 -4.2190657 -4.1996846 -4.1999826 -4.2175922 -4.2362905 -4.25835 -4.2810678][-4.2883525 -4.2806964 -4.2798538 -4.2808628 -4.2788324 -4.2790933 -4.2817163 -4.281436 -4.2692 -4.2540288 -4.2501197 -4.25848 -4.2712684 -4.2865133 -4.3058467][-4.3095841 -4.3061976 -4.3061423 -4.3069339 -4.3053913 -4.3051257 -4.306313 -4.3062711 -4.2999105 -4.2916851 -4.2883034 -4.2930527 -4.3026962 -4.3146095 -4.3294077][-4.3279686 -4.3277597 -4.328784 -4.3294053 -4.3285351 -4.3282881 -4.328927 -4.3286338 -4.3268428 -4.3249655 -4.3245716 -4.3275156 -4.3332357 -4.3407683 -4.3488588]]...]
INFO - root - 2017-12-07 18:20:01.912149: step 33710, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.972 sec/batch; 80h:40m:02s remains)
INFO - root - 2017-12-07 18:20:11.504694: step 33720, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 79h:30m:03s remains)
INFO - root - 2017-12-07 18:20:21.069012: step 33730, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 78h:00m:14s remains)
INFO - root - 2017-12-07 18:20:30.727076: step 33740, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 77h:40m:11s remains)
INFO - root - 2017-12-07 18:20:40.661500: step 33750, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.017 sec/batch; 84h:22m:41s remains)
INFO - root - 2017-12-07 18:20:50.311606: step 33760, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 79h:14m:00s remains)
INFO - root - 2017-12-07 18:20:59.934735: step 33770, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 80h:52m:51s remains)
INFO - root - 2017-12-07 18:21:09.626764: step 33780, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 76h:01m:54s remains)
INFO - root - 2017-12-07 18:21:19.291416: step 33790, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 79h:01m:17s remains)
INFO - root - 2017-12-07 18:21:28.939912: step 33800, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.991 sec/batch; 82h:12m:31s remains)
2017-12-07 18:21:29.947513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2934127 -4.2651224 -4.2439251 -4.2248096 -4.1956792 -4.16824 -4.1703844 -4.1960421 -4.2212663 -4.2438631 -4.2702308 -4.28364 -4.2783551 -4.2697849 -4.2729969][-4.2924032 -4.2685165 -4.2439156 -4.2166076 -4.1804581 -4.1449585 -4.1434512 -4.1759729 -4.21142 -4.2433424 -4.2689381 -4.2772918 -4.26944 -4.2637224 -4.26676][-4.300137 -4.2816854 -4.2565107 -4.2236419 -4.1828446 -4.1389 -4.1254106 -4.1571684 -4.2045708 -4.2414451 -4.2664647 -4.275178 -4.2684922 -4.263936 -4.2639828][-4.2987957 -4.2820163 -4.256906 -4.2255211 -4.1855235 -4.1354818 -4.112556 -4.1420317 -4.1993847 -4.243474 -4.2707448 -4.2789645 -4.27701 -4.2752481 -4.274158][-4.2871971 -4.271183 -4.2464476 -4.2186337 -4.1853189 -4.139812 -4.1134014 -4.1362381 -4.1885443 -4.2375488 -4.2694974 -4.2780619 -4.2763672 -4.2804203 -4.2857366][-4.27613 -4.2640209 -4.2394805 -4.2079148 -4.173419 -4.1295605 -4.1000848 -4.108726 -4.1528258 -4.2108531 -4.2534561 -4.2647905 -4.2609553 -4.2682891 -4.2825894][-4.272079 -4.2641292 -4.2370281 -4.195333 -4.1465244 -4.0892615 -4.0359812 -4.0126271 -4.0592966 -4.1509457 -4.2177477 -4.2440877 -4.2468753 -4.256741 -4.2755532][-4.2742896 -4.272131 -4.243227 -4.193007 -4.1308293 -4.0531936 -3.9617648 -3.8954024 -3.9542005 -4.0896759 -4.1800175 -4.2204318 -4.2349167 -4.2501183 -4.2712436][-4.283021 -4.2885 -4.2604027 -4.2042222 -4.1400642 -4.0603652 -3.9624877 -3.8934028 -3.9568677 -4.0910378 -4.1726403 -4.2101669 -4.2242293 -4.242496 -4.2662482][-4.2939911 -4.3027663 -4.2756376 -4.2147589 -4.1546288 -4.089344 -4.0148306 -3.9788284 -4.04288 -4.1430259 -4.194303 -4.2145782 -4.2210741 -4.238914 -4.2621827][-4.3013473 -4.3135076 -4.2899852 -4.2334652 -4.1795435 -4.1282563 -4.0690618 -4.0538831 -4.118597 -4.1919484 -4.221571 -4.231801 -4.2312202 -4.2422757 -4.2594924][-4.3070879 -4.3214087 -4.30399 -4.2567377 -4.2111163 -4.1748133 -4.1234832 -4.1106186 -4.1672606 -4.2229066 -4.2435961 -4.25233 -4.2526555 -4.2573462 -4.2619896][-4.3135328 -4.3249331 -4.3123517 -4.2764726 -4.2426076 -4.2224326 -4.184164 -4.1714659 -4.2125988 -4.2499208 -4.2647 -4.2736249 -4.2763662 -4.2784324 -4.2732553][-4.3234987 -4.3288584 -4.320013 -4.2954521 -4.27411 -4.2648344 -4.2437868 -4.2347317 -4.2588139 -4.2784314 -4.2871666 -4.2953625 -4.298779 -4.298265 -4.2906342][-4.3317614 -4.331296 -4.3249621 -4.3090348 -4.2955265 -4.2895813 -4.2781963 -4.2721162 -4.2844062 -4.2964029 -4.3038807 -4.3120127 -4.3164678 -4.3156209 -4.309453]]...]
INFO - root - 2017-12-07 18:21:39.611868: step 33810, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 78h:20m:44s remains)
INFO - root - 2017-12-07 18:21:49.213735: step 33820, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 80h:00m:51s remains)
INFO - root - 2017-12-07 18:21:58.803405: step 33830, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 75h:47m:46s remains)
INFO - root - 2017-12-07 18:22:08.488722: step 33840, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 81h:10m:26s remains)
INFO - root - 2017-12-07 18:22:18.308894: step 33850, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 77h:42m:17s remains)
INFO - root - 2017-12-07 18:22:27.935671: step 33860, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 82h:45m:58s remains)
INFO - root - 2017-12-07 18:22:37.577453: step 33870, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.810 sec/batch; 67h:11m:07s remains)
INFO - root - 2017-12-07 18:22:47.302247: step 33880, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.942 sec/batch; 78h:10m:23s remains)
INFO - root - 2017-12-07 18:22:57.028425: step 33890, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 82h:31m:22s remains)
INFO - root - 2017-12-07 18:23:06.532392: step 33900, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.980 sec/batch; 81h:17m:33s remains)
2017-12-07 18:23:07.459728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3239231 -4.3245459 -4.3234468 -4.3218393 -4.31917 -4.3132963 -4.3044767 -4.2970481 -4.2941127 -4.2937841 -4.2956305 -4.2981906 -4.3011045 -4.3045344 -4.3100219][-4.3252826 -4.3268709 -4.3236418 -4.3182812 -4.3101435 -4.2976084 -4.2831836 -4.2739534 -4.274044 -4.27753 -4.2802534 -4.2844944 -4.2903004 -4.2953176 -4.3013992][-4.3205495 -4.31806 -4.30875 -4.2950315 -4.2784142 -4.2577424 -4.2378478 -4.2296824 -4.2377372 -4.2479668 -4.2533383 -4.2599325 -4.268631 -4.2756686 -4.2840557][-4.3060217 -4.29467 -4.2784557 -4.2567635 -4.2304096 -4.1988211 -4.1695743 -4.164969 -4.1908555 -4.2143893 -4.2252707 -4.2349863 -4.2447219 -4.2526507 -4.2626581][-4.2860384 -4.2651792 -4.242147 -4.2105675 -4.1687245 -4.1168671 -4.0657024 -4.0627894 -4.1203742 -4.1737962 -4.201654 -4.21772 -4.2272348 -4.2325144 -4.2398338][-4.2553339 -4.2271824 -4.1989574 -4.1555848 -4.0927687 -4.0082126 -3.9165735 -3.9069433 -4.0048881 -4.0992565 -4.1562023 -4.1859436 -4.2013235 -4.2071037 -4.2126365][-4.2232347 -4.1859384 -4.1522412 -4.0944986 -4.0096869 -3.8868623 -3.7425678 -3.7196705 -3.8664734 -4.0086222 -4.0993977 -4.1476374 -4.1738844 -4.1855516 -4.190208][-4.1919584 -4.1464643 -4.1123314 -4.0539 -3.9675431 -3.839612 -3.6709332 -3.6333544 -3.7993169 -3.9643135 -4.068202 -4.1260638 -4.163476 -4.18089 -4.1848722][-4.163825 -4.1173644 -4.0925083 -4.0588431 -4.0101242 -3.9313164 -3.8161833 -3.7826366 -3.8959489 -4.0169172 -4.0910778 -4.134841 -4.1699982 -4.1913519 -4.196497][-4.1415749 -4.0987082 -4.0895429 -4.0880246 -4.0823655 -4.0531588 -3.9992726 -3.9755538 -4.0345063 -4.0997338 -4.1365557 -4.1606522 -4.1877894 -4.2085962 -4.214869][-4.124639 -4.0849905 -4.0897727 -4.1140628 -4.13633 -4.1381874 -4.1210871 -4.1070809 -4.1337285 -4.163558 -4.1785212 -4.1881924 -4.2069016 -4.22634 -4.2350955][-4.1275563 -4.0950036 -4.1085057 -4.1428623 -4.1734109 -4.1889267 -4.19309 -4.1871705 -4.1959944 -4.2099533 -4.2196722 -4.2248631 -4.2373114 -4.2541142 -4.263905][-4.1577349 -4.1390395 -4.1566215 -4.189024 -4.2132492 -4.2282209 -4.2393341 -4.2381287 -4.239058 -4.2462344 -4.2568908 -4.2635918 -4.272121 -4.2849865 -4.2953143][-4.2049074 -4.1973395 -4.2162385 -4.2407441 -4.255619 -4.2639141 -4.274786 -4.2768059 -4.2757616 -4.278564 -4.2868762 -4.2940269 -4.2990575 -4.3079638 -4.3177924][-4.24456 -4.2432332 -4.2592373 -4.2744932 -4.2831306 -4.288404 -4.2958736 -4.3001547 -4.3003345 -4.3024316 -4.3087153 -4.3153839 -4.320066 -4.326272 -4.33321]]...]
INFO - root - 2017-12-07 18:23:17.221692: step 33910, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.932 sec/batch; 77h:15m:55s remains)
INFO - root - 2017-12-07 18:23:26.794648: step 33920, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 78h:09m:50s remains)
INFO - root - 2017-12-07 18:23:36.366019: step 33930, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 81h:07m:13s remains)
INFO - root - 2017-12-07 18:23:46.068694: step 33940, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 80h:14m:04s remains)
INFO - root - 2017-12-07 18:23:55.764293: step 33950, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 79h:53m:36s remains)
INFO - root - 2017-12-07 18:24:05.319543: step 33960, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 78h:23m:35s remains)
INFO - root - 2017-12-07 18:24:15.041233: step 33970, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.012 sec/batch; 83h:54m:26s remains)
INFO - root - 2017-12-07 18:24:24.680533: step 33980, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 82h:45m:24s remains)
INFO - root - 2017-12-07 18:24:34.232876: step 33990, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.950 sec/batch; 78h:45m:58s remains)
INFO - root - 2017-12-07 18:24:43.803387: step 34000, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 77h:11m:52s remains)
2017-12-07 18:24:44.775749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3465276 -4.3476543 -4.3447309 -4.32979 -4.2919469 -4.2435546 -4.2117205 -4.20442 -4.2293134 -4.2672215 -4.2962766 -4.3125491 -4.3189554 -4.3132873 -4.2914281][-4.3479681 -4.3489366 -4.34628 -4.3314233 -4.2923579 -4.2399116 -4.2023296 -4.1905165 -4.2145467 -4.2560296 -4.2897897 -4.3112922 -4.3228121 -4.320591 -4.3006449][-4.3485537 -4.3498034 -4.3477669 -4.333261 -4.293468 -4.2372189 -4.1934109 -4.1766419 -4.2001591 -4.245645 -4.2846732 -4.3118382 -4.3283062 -4.3299432 -4.3140588][-4.3490524 -4.3506637 -4.3491483 -4.3345742 -4.2933178 -4.2329731 -4.1824975 -4.1609573 -4.1855283 -4.235508 -4.2799258 -4.3123722 -4.3329129 -4.3375134 -4.3258362][-4.3493576 -4.3512673 -4.3502846 -4.3358808 -4.2932234 -4.2288451 -4.1715703 -4.145503 -4.1720514 -4.2253385 -4.2736526 -4.3094115 -4.3322859 -4.3391576 -4.331089][-4.3493557 -4.3515811 -4.3511624 -4.3372226 -4.2940092 -4.2265596 -4.1636047 -4.1343346 -4.1640797 -4.219975 -4.270155 -4.3062525 -4.3290024 -4.3371468 -4.3307457][-4.3493438 -4.3516426 -4.3515196 -4.3377628 -4.2940226 -4.2239013 -4.1566281 -4.1252241 -4.1579924 -4.2166414 -4.2679977 -4.3033319 -4.3245373 -4.3325291 -4.3258247][-4.3490696 -4.3513012 -4.3505683 -4.3359842 -4.2908006 -4.2186723 -4.1492043 -4.1164188 -4.1504393 -4.211669 -4.2648029 -4.2996092 -4.31875 -4.3248434 -4.3160963][-4.3485765 -4.350421 -4.3486519 -4.3330531 -4.2871227 -4.2156138 -4.1487889 -4.1172276 -4.1490803 -4.2096519 -4.26242 -4.2959914 -4.313302 -4.3176885 -4.3068466][-4.3479486 -4.3492866 -4.346797 -4.3309932 -4.2862291 -4.217741 -4.1566658 -4.1285305 -4.1565266 -4.2135553 -4.2635784 -4.2947559 -4.3101935 -4.3131442 -4.3012285][-4.3471937 -4.3482237 -4.3456798 -4.33066 -4.289093 -4.2258482 -4.1714768 -4.1473789 -4.1714354 -4.2229614 -4.2680731 -4.2958612 -4.3091097 -4.3106804 -4.2988224][-4.3468919 -4.3478518 -4.3458915 -4.3329539 -4.2969379 -4.2413306 -4.1942167 -4.1740742 -4.1938653 -4.2379355 -4.2766881 -4.3006949 -4.3116179 -4.3120308 -4.3010859][-4.3472509 -4.3483167 -4.3472247 -4.3370728 -4.3080344 -4.262732 -4.2237806 -4.2074823 -4.2228146 -4.2586355 -4.2901931 -4.3095684 -4.318006 -4.3173246 -4.3076096][-4.3476858 -4.3487821 -4.3485088 -4.341619 -4.3202076 -4.2858582 -4.2553139 -4.2426558 -4.2539034 -4.2812643 -4.3054886 -4.3201036 -4.3262672 -4.3249326 -4.3169565][-4.3479905 -4.3488517 -4.3486981 -4.344234 -4.32966 -4.3052583 -4.2824559 -4.2731805 -4.281517 -4.3017244 -4.3191714 -4.3294516 -4.3331418 -4.3312383 -4.3250494]]...]
INFO - root - 2017-12-07 18:24:54.463038: step 34010, loss = 2.11, batch loss = 2.06 (8.7 examples/sec; 0.922 sec/batch; 76h:26m:28s remains)
INFO - root - 2017-12-07 18:25:04.063673: step 34020, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 77h:21m:25s remains)
INFO - root - 2017-12-07 18:25:13.633061: step 34030, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 79h:08m:12s remains)
INFO - root - 2017-12-07 18:25:23.101309: step 34040, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.894 sec/batch; 74h:06m:06s remains)
INFO - root - 2017-12-07 18:25:32.754672: step 34050, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 77h:53m:13s remains)
INFO - root - 2017-12-07 18:25:42.421893: step 34060, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 80h:03m:16s remains)
INFO - root - 2017-12-07 18:25:52.185810: step 34070, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 78h:35m:10s remains)
INFO - root - 2017-12-07 18:26:01.843714: step 34080, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 76h:19m:10s remains)
INFO - root - 2017-12-07 18:26:11.491514: step 34090, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 82h:52m:59s remains)
INFO - root - 2017-12-07 18:26:21.095806: step 34100, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.009 sec/batch; 83h:38m:38s remains)
2017-12-07 18:26:22.015635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3446865 -4.344059 -4.3435273 -4.3405809 -4.3340311 -4.3245516 -4.3150649 -4.3108864 -4.3133478 -4.3196507 -4.3262954 -4.3302159 -4.3315978 -4.3323641 -4.332561][-4.3544469 -4.3537164 -4.3530483 -4.3502808 -4.3455725 -4.3397527 -4.3348961 -4.3338847 -4.3374629 -4.3435 -4.348855 -4.351007 -4.3499727 -4.3479352 -4.3446784][-4.3571167 -4.3568068 -4.3563805 -4.3528528 -4.3477173 -4.34294 -4.3395338 -4.3384223 -4.3402009 -4.3460727 -4.3533363 -4.3574362 -4.3574095 -4.3545127 -4.3474183][-4.3498383 -4.3480582 -4.3459873 -4.3396072 -4.3313313 -4.3242 -4.317801 -4.3119197 -4.308557 -4.3138709 -4.3261218 -4.3366756 -4.3420982 -4.3419409 -4.3323231][-4.3348746 -4.3298583 -4.3244505 -4.3120947 -4.2945647 -4.2787442 -4.2641029 -4.2499022 -4.24191 -4.2485061 -4.2698545 -4.2913938 -4.3065438 -4.3102283 -4.2968221][-4.3122306 -4.3029661 -4.2901816 -4.2643952 -4.2270932 -4.1909118 -4.1591506 -4.1329441 -4.12452 -4.14163 -4.1830053 -4.2249837 -4.2567549 -4.26851 -4.2532988][-4.2883582 -4.2726173 -4.246943 -4.199698 -4.1356196 -4.0737453 -4.0167909 -3.9731762 -3.9699526 -4.0095673 -4.0780215 -4.1448188 -4.1958089 -4.2207069 -4.2127471][-4.271204 -4.2501521 -4.2158847 -4.1561489 -4.075913 -3.9921186 -3.9074697 -3.8450432 -3.8470592 -3.909961 -4.0027452 -4.0876045 -4.1510534 -4.1880383 -4.1915][-4.261302 -4.2396584 -4.206049 -4.1508603 -4.08002 -4.0046115 -3.9266677 -3.8744106 -3.884712 -3.9492154 -4.0342355 -4.1079507 -4.1617169 -4.1946878 -4.2017636][-4.2571683 -4.2428732 -4.220654 -4.1843929 -4.1397438 -4.0948191 -4.050662 -4.0283871 -4.0461612 -4.0928612 -4.1474123 -4.1914353 -4.2187247 -4.2334843 -4.2339506][-4.2665305 -4.2639365 -4.256845 -4.2414184 -4.2208 -4.2003155 -4.1815338 -4.1755395 -4.1906195 -4.2191215 -4.2481589 -4.2676492 -4.2748957 -4.2747006 -4.2663984][-4.2882051 -4.292625 -4.2949791 -4.2926106 -4.2869339 -4.2796655 -4.2724905 -4.270927 -4.2792959 -4.2937846 -4.3078966 -4.3161211 -4.3166647 -4.3125172 -4.3034849][-4.31095 -4.317502 -4.323504 -4.3279591 -4.3297343 -4.3286963 -4.3266263 -4.3259044 -4.3288178 -4.3344173 -4.3404012 -4.343163 -4.3419461 -4.3385224 -4.3321624][-4.3228927 -4.3272896 -4.3324137 -4.3379664 -4.3422675 -4.3441858 -4.34433 -4.3438706 -4.3440886 -4.3453021 -4.3467278 -4.3467803 -4.3455973 -4.3449965 -4.34323][-4.320673 -4.3204756 -4.3229847 -4.3283143 -4.3346438 -4.3392286 -4.3415813 -4.3419805 -4.3410387 -4.3404045 -4.3401647 -4.3399124 -4.3397584 -4.3406405 -4.3413258]]...]
INFO - root - 2017-12-07 18:26:31.794188: step 34110, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 80h:48m:59s remains)
INFO - root - 2017-12-07 18:26:41.519358: step 34120, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 81h:42m:45s remains)
INFO - root - 2017-12-07 18:26:51.137779: step 34130, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 80h:42m:59s remains)
INFO - root - 2017-12-07 18:27:00.962068: step 34140, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 77h:25m:55s remains)
INFO - root - 2017-12-07 18:27:10.545333: step 34150, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 81h:03m:15s remains)
INFO - root - 2017-12-07 18:27:20.197529: step 34160, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.989 sec/batch; 81h:57m:43s remains)
INFO - root - 2017-12-07 18:27:29.956850: step 34170, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 81h:57m:33s remains)
INFO - root - 2017-12-07 18:27:39.690018: step 34180, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 82h:16m:58s remains)
INFO - root - 2017-12-07 18:27:49.294348: step 34190, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.927 sec/batch; 76h:49m:28s remains)
INFO - root - 2017-12-07 18:27:59.027517: step 34200, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 76h:52m:09s remains)
2017-12-07 18:28:00.011212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2455506 -4.24834 -4.2619257 -4.2737179 -4.27598 -4.2666736 -4.2547979 -4.2569637 -4.2714753 -4.2847152 -4.285821 -4.2823944 -4.282104 -4.2776423 -4.2691364][-4.2417307 -4.2411265 -4.2526717 -4.2662606 -4.27329 -4.2704315 -4.2599454 -4.2607031 -4.2732863 -4.2899704 -4.297225 -4.2983932 -4.2969584 -4.2880421 -4.277986][-4.2508473 -4.2518129 -4.2594013 -4.2669544 -4.2695813 -4.2657967 -4.2562876 -4.252697 -4.25749 -4.2705746 -4.2825556 -4.2959623 -4.2998533 -4.2902689 -4.2815676][-4.2521567 -4.2554007 -4.2593369 -4.2583551 -4.2493343 -4.2358546 -4.2237239 -4.21964 -4.2237048 -4.236629 -4.2541432 -4.2840409 -4.29509 -4.2878308 -4.2786813][-4.2286072 -4.2314572 -4.2328534 -4.2234845 -4.1994472 -4.1721811 -4.15306 -4.1536069 -4.1713948 -4.1964173 -4.2236223 -4.2655458 -4.2820306 -4.2780213 -4.2652097][-4.1907072 -4.1899471 -4.1839967 -4.1656466 -4.1281595 -4.0850768 -4.0596819 -4.0727525 -4.1127439 -4.1558414 -4.1968045 -4.2479825 -4.27139 -4.2691345 -4.2506986][-4.1663876 -4.1613178 -4.148818 -4.1201673 -4.0698271 -4.0103869 -3.9747796 -3.9989393 -4.0608144 -4.1222706 -4.1743832 -4.2257123 -4.25216 -4.2519255 -4.2343483][-4.1722541 -4.1661906 -4.155117 -4.1276665 -4.07581 -4.0088568 -3.964273 -3.9865024 -4.0512848 -4.1147947 -4.1619811 -4.20401 -4.2251272 -4.2280769 -4.216208][-4.2001314 -4.1973329 -4.1939783 -4.1777534 -4.1386309 -4.0823703 -4.0419984 -4.0527158 -4.0981226 -4.142406 -4.1739745 -4.200264 -4.214591 -4.2184753 -4.2141323][-4.2196212 -4.2245622 -4.2332635 -4.2323308 -4.2115078 -4.1737885 -4.1444874 -4.1440306 -4.164495 -4.1867728 -4.202538 -4.2140961 -4.2202415 -4.2256045 -4.2310915][-4.221499 -4.2277865 -4.242671 -4.2558475 -4.2548757 -4.2388716 -4.2214303 -4.2162857 -4.2241907 -4.2336292 -4.2397051 -4.239141 -4.2353072 -4.2385073 -4.2488527][-4.2212596 -4.2226725 -4.2359366 -4.2551103 -4.2677484 -4.2681136 -4.2616172 -4.2580047 -4.260221 -4.261673 -4.2603483 -4.2515521 -4.2388153 -4.23688 -4.248096][-4.2320533 -4.2292676 -4.2359338 -4.2490067 -4.2628126 -4.2719955 -4.2749591 -4.2751236 -4.2763391 -4.2747793 -4.2652936 -4.2466116 -4.2276025 -4.2227955 -4.2366576][-4.2540755 -4.247499 -4.2464137 -4.2518554 -4.2614655 -4.2735662 -4.2848845 -4.2912636 -4.29316 -4.2845116 -4.2645345 -4.2363071 -4.212842 -4.2060957 -4.2207508][-4.2702703 -4.2609363 -4.2539721 -4.25488 -4.2616634 -4.2739863 -4.2904553 -4.3028564 -4.3061323 -4.2927551 -4.2662497 -4.2324972 -4.2050123 -4.1934667 -4.2052817]]...]
INFO - root - 2017-12-07 18:28:09.658477: step 34210, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.006 sec/batch; 83h:23m:09s remains)
INFO - root - 2017-12-07 18:28:19.179202: step 34220, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 77h:46m:38s remains)
INFO - root - 2017-12-07 18:28:28.698220: step 34230, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 79h:48m:50s remains)
INFO - root - 2017-12-07 18:28:38.405012: step 34240, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 80h:29m:39s remains)
INFO - root - 2017-12-07 18:28:47.999985: step 34250, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 78h:30m:54s remains)
INFO - root - 2017-12-07 18:28:57.720013: step 34260, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 75h:14m:49s remains)
INFO - root - 2017-12-07 18:29:07.289589: step 34270, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 77h:58m:45s remains)
INFO - root - 2017-12-07 18:29:17.000838: step 34280, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.004 sec/batch; 83h:10m:34s remains)
INFO - root - 2017-12-07 18:29:26.694282: step 34290, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 80h:45m:15s remains)
INFO - root - 2017-12-07 18:29:36.453765: step 34300, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 80h:30m:59s remains)
2017-12-07 18:29:37.530784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2344213 -4.241231 -4.2661338 -4.2901578 -4.3132358 -4.3305135 -4.3385696 -4.3340836 -4.3285093 -4.3303323 -4.3418851 -4.3560362 -4.3634272 -4.3613706 -4.35342][-4.1926584 -4.1987166 -4.2293262 -4.2624917 -4.2945752 -4.3164997 -4.3260708 -4.3236933 -4.3217063 -4.3265085 -4.3409319 -4.3550515 -4.35963 -4.3489347 -4.331121][-4.175 -4.17613 -4.2094388 -4.2513461 -4.2884922 -4.3090091 -4.318943 -4.3169026 -4.313189 -4.3183022 -4.3327346 -4.3457346 -4.3487411 -4.3339667 -4.306047][-4.1789417 -4.1750841 -4.2067652 -4.2485266 -4.2834587 -4.2943325 -4.2979159 -4.2902222 -4.2821369 -4.2883506 -4.3054509 -4.3256984 -4.3364649 -4.3220224 -4.2874494][-4.1904392 -4.1836739 -4.2100434 -4.2441416 -4.2696919 -4.2651787 -4.2493811 -4.2206583 -4.2031388 -4.2147493 -4.2444034 -4.2814927 -4.3060207 -4.2975941 -4.2665987][-4.187851 -4.1810327 -4.1966662 -4.2145929 -4.222559 -4.1976275 -4.158741 -4.1077771 -4.0835161 -4.1090274 -4.1602383 -4.2153611 -4.2533579 -4.2569876 -4.2363195][-4.1703749 -4.1682229 -4.1707535 -4.1668043 -4.1515741 -4.1078987 -4.0500445 -3.9770367 -3.9489374 -3.9963541 -4.07345 -4.1456885 -4.1980658 -4.2183371 -4.21374][-4.159574 -4.1580024 -4.1425066 -4.1183195 -4.0865803 -4.036314 -3.9770875 -3.90347 -3.8812904 -3.9483011 -4.0416417 -4.121016 -4.1781259 -4.2077689 -4.2129683][-4.1596742 -4.1502514 -4.1179872 -4.0830164 -4.0521626 -4.0189209 -3.9876306 -3.9414365 -3.9298563 -3.9872396 -4.0666656 -4.1334014 -4.1856508 -4.2178159 -4.2229376][-4.1752872 -4.1564846 -4.1190128 -4.0859628 -4.0699186 -4.0624838 -4.0590711 -4.0379233 -4.0264893 -4.057065 -4.1098337 -4.1573591 -4.1973295 -4.22512 -4.2252159][-4.2000666 -4.1755729 -4.1419926 -4.1178403 -4.1132135 -4.1193948 -4.1260138 -4.1172614 -4.1030807 -4.1077204 -4.1356072 -4.164669 -4.1875448 -4.2022552 -4.1898551][-4.2251005 -4.2007046 -4.1738019 -4.1555119 -4.1550031 -4.1616497 -4.1651349 -4.1574712 -4.1413131 -4.1303492 -4.1389456 -4.1536942 -4.1596375 -4.1578865 -4.1328883][-4.2487392 -4.2267427 -4.2057152 -4.1916642 -4.1904011 -4.1934667 -4.1887169 -4.1768761 -4.1626849 -4.1497164 -4.1493182 -4.1523776 -4.1441832 -4.1277208 -4.09721][-4.2626567 -4.2432041 -4.2294974 -4.2193718 -4.2174969 -4.2188077 -4.2115474 -4.1984587 -4.1872072 -4.17619 -4.1714864 -4.1651292 -4.1458912 -4.1200032 -4.09521][-4.2623005 -4.2496777 -4.2443686 -4.2400589 -4.2398152 -4.241641 -4.2369308 -4.2264895 -4.2185292 -4.2100196 -4.20222 -4.188457 -4.1628938 -4.1334577 -4.114203]]...]
INFO - root - 2017-12-07 18:29:47.218961: step 34310, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.989 sec/batch; 81h:53m:33s remains)
INFO - root - 2017-12-07 18:29:56.825276: step 34320, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 77h:20m:30s remains)
INFO - root - 2017-12-07 18:30:06.455001: step 34330, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 81h:48m:22s remains)
INFO - root - 2017-12-07 18:30:16.251024: step 34340, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 83h:34m:56s remains)
INFO - root - 2017-12-07 18:30:25.958807: step 34350, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 78h:57m:47s remains)
INFO - root - 2017-12-07 18:30:35.531825: step 34360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 79h:56m:18s remains)
INFO - root - 2017-12-07 18:30:45.165069: step 34370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 81h:06m:49s remains)
INFO - root - 2017-12-07 18:30:54.667230: step 34380, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.957 sec/batch; 79h:14m:59s remains)
INFO - root - 2017-12-07 18:31:04.465932: step 34390, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 77h:53m:21s remains)
INFO - root - 2017-12-07 18:31:14.076822: step 34400, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 80h:27m:14s remains)
2017-12-07 18:31:15.018118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.252008 -4.2353611 -4.2117906 -4.1899381 -4.1727309 -4.1594672 -4.1431875 -4.1177778 -4.0994954 -4.1064043 -4.1116505 -4.1166706 -4.1447458 -4.1836991 -4.2143879][-4.2530432 -4.2331676 -4.2000046 -4.1760793 -4.1541033 -4.1379929 -4.124661 -4.1072416 -4.0935869 -4.1022167 -4.107626 -4.1047678 -4.1276836 -4.1630921 -4.1920204][-4.2353845 -4.213614 -4.1729736 -4.1514044 -4.1381049 -4.1270823 -4.1131096 -4.1007648 -4.0916524 -4.0988975 -4.1110797 -4.1081772 -4.1224737 -4.1527786 -4.1829572][-4.1996994 -4.1831727 -4.1456041 -4.1338158 -4.1375422 -4.1301494 -4.1121383 -4.1023684 -4.091445 -4.0921936 -4.106925 -4.1084208 -4.1189003 -4.1505537 -4.1851382][-4.1749988 -4.1637864 -4.1360083 -4.1342683 -4.1439505 -4.1325808 -4.1083918 -4.1000118 -4.0956788 -4.0942712 -4.1050034 -4.1074963 -4.1174593 -4.1527309 -4.1912961][-4.16537 -4.1636004 -4.14462 -4.1442008 -4.1464186 -4.1179738 -4.0762949 -4.0676246 -4.084518 -4.0983591 -4.1078358 -4.1074276 -4.1134691 -4.1465626 -4.1909742][-4.1604371 -4.1705222 -4.1601162 -4.1577387 -4.1432953 -4.087306 -4.0213628 -4.0144033 -4.0649848 -4.1057091 -4.1192741 -4.118608 -4.1201887 -4.1462641 -4.1903658][-4.161397 -4.1799364 -4.1754966 -4.1626487 -4.1311255 -4.0518165 -3.97114 -3.9729266 -4.0558677 -4.120615 -4.133606 -4.1274219 -4.1274462 -4.1533556 -4.196259][-4.1666017 -4.1891489 -4.1868095 -4.1653671 -4.1257949 -4.0554395 -3.9979215 -4.0044646 -4.0737991 -4.1267853 -4.1323104 -4.126359 -4.134655 -4.1625195 -4.2050543][-4.1820693 -4.2042408 -4.2087173 -4.1884027 -4.1517234 -4.108047 -4.0767984 -4.0756488 -4.1056714 -4.1261659 -4.1233559 -4.1267762 -4.1500373 -4.1845832 -4.222785][-4.2065835 -4.2292953 -4.2376575 -4.2221 -4.191184 -4.1589289 -4.135458 -4.1273923 -4.1310105 -4.1266727 -4.1218667 -4.1357732 -4.172596 -4.2157178 -4.248744][-4.2324467 -4.253233 -4.2608542 -4.2495394 -4.2228065 -4.1931372 -4.1698737 -4.1635222 -4.1624274 -4.1494851 -4.1428547 -4.1636262 -4.2047687 -4.24716 -4.2756152][-4.2544107 -4.2699919 -4.2755175 -4.266655 -4.2433796 -4.2147565 -4.194788 -4.1941156 -4.1956296 -4.1849389 -4.1805677 -4.1994033 -4.2381983 -4.2730227 -4.2939954][-4.2740817 -4.2864628 -4.287302 -4.2767072 -4.2589254 -4.2356153 -4.2205148 -4.2240543 -4.2262039 -4.2177315 -4.2141776 -4.228374 -4.2603364 -4.2859888 -4.2997775][-4.29532 -4.305747 -4.3018527 -4.2896261 -4.2742896 -4.2562828 -4.2447257 -4.2441516 -4.2422776 -4.2353897 -4.235003 -4.2491097 -4.2748652 -4.292098 -4.2990036]]...]
INFO - root - 2017-12-07 18:31:24.751680: step 34410, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 77h:15m:30s remains)
INFO - root - 2017-12-07 18:31:34.412228: step 34420, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 78h:34m:50s remains)
INFO - root - 2017-12-07 18:31:44.053195: step 34430, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 80h:43m:10s remains)
INFO - root - 2017-12-07 18:31:53.816254: step 34440, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 81h:49m:28s remains)
INFO - root - 2017-12-07 18:32:03.546500: step 34450, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 82h:00m:23s remains)
INFO - root - 2017-12-07 18:32:13.200566: step 34460, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.940 sec/batch; 77h:49m:01s remains)
INFO - root - 2017-12-07 18:32:22.723626: step 34470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 79h:03m:33s remains)
INFO - root - 2017-12-07 18:32:32.284879: step 34480, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 79h:12m:19s remains)
INFO - root - 2017-12-07 18:32:42.089936: step 34490, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 79h:47m:29s remains)
INFO - root - 2017-12-07 18:32:51.803385: step 34500, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 81h:39m:56s remains)
2017-12-07 18:32:52.766469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23939 -4.2245903 -4.2106514 -4.2021713 -4.199234 -4.2019892 -4.207099 -4.2117987 -4.2066588 -4.1812563 -4.1460342 -4.1184235 -4.1122212 -4.1414404 -4.1902475][-4.2564044 -4.2425361 -4.2312193 -4.2252245 -4.2250156 -4.2302337 -4.2364993 -4.2407894 -4.2352681 -4.2106452 -4.1749439 -4.1456246 -4.1399608 -4.1692066 -4.2154756][-4.2737665 -4.26579 -4.2595444 -4.2566323 -4.2570705 -4.2613978 -4.2664528 -4.27073 -4.2681618 -4.2514482 -4.2256756 -4.2029753 -4.1978965 -4.2163758 -4.2477608][-4.2890892 -4.2866306 -4.2840171 -4.2834654 -4.2837996 -4.284925 -4.2868919 -4.2906609 -4.2934718 -4.2886972 -4.2778287 -4.2661238 -4.2620177 -4.2682724 -4.2817721][-4.2989173 -4.2999387 -4.30001 -4.3001943 -4.2971487 -4.289928 -4.2815294 -4.2789578 -4.2852387 -4.2947249 -4.3037772 -4.3078895 -4.3083858 -4.3072209 -4.30682][-4.2984238 -4.2992587 -4.2980695 -4.2930255 -4.2779288 -4.2535915 -4.227457 -4.2145758 -4.2249665 -4.2545285 -4.2894549 -4.3135672 -4.3224483 -4.3191905 -4.3109331][-4.28228 -4.2778392 -4.2692447 -4.2513919 -4.2177396 -4.171782 -4.1252923 -4.1013408 -4.11785 -4.1714783 -4.23507 -4.2824092 -4.3045092 -4.3053007 -4.2966938][-4.2592072 -4.2461047 -4.2259889 -4.1921749 -4.1393847 -4.0744433 -4.0123377 -3.9817324 -4.0109563 -4.09214 -4.1819572 -4.24695 -4.2791462 -4.2858324 -4.2811022][-4.2325959 -4.212935 -4.1844473 -4.1416531 -4.0820179 -4.013515 -3.9510918 -3.9254067 -3.9712393 -4.0696135 -4.1690712 -4.2379556 -4.2702365 -4.2774515 -4.274714][-4.2121558 -4.1939878 -4.1687851 -4.1331868 -4.0854912 -4.0337534 -3.990005 -3.9788134 -4.0271163 -4.1146617 -4.199357 -4.2542329 -4.2765651 -4.2789054 -4.2755446][-4.2090664 -4.2037134 -4.192441 -4.1727662 -4.1421185 -4.108624 -4.083674 -4.0830078 -4.1235833 -4.1881313 -4.2469792 -4.280355 -4.28918 -4.2855058 -4.281127][-4.2173591 -4.225873 -4.2286234 -4.2233582 -4.2072177 -4.187983 -4.1759572 -4.1818404 -4.2122512 -4.2529459 -4.2852426 -4.2975564 -4.2937284 -4.286047 -4.2824578][-4.2354393 -4.2485256 -4.2576747 -4.2612348 -4.2557864 -4.2478843 -4.2453771 -4.2538395 -4.2723117 -4.2913327 -4.3008065 -4.2956524 -4.2838831 -4.2755651 -4.2745028][-4.2564054 -4.2654839 -4.2726464 -4.2771454 -4.2774568 -4.2772284 -4.2810764 -4.2900372 -4.2989197 -4.3028 -4.2980113 -4.2843337 -4.2711825 -4.2644696 -4.2654662][-4.2738762 -4.2742515 -4.2745481 -4.27596 -4.27838 -4.2826805 -4.2893381 -4.2968011 -4.30066 -4.298615 -4.2901793 -4.2774615 -4.2677979 -4.2634144 -4.2656193]]...]
INFO - root - 2017-12-07 18:33:02.454522: step 34510, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 81h:15m:10s remains)
INFO - root - 2017-12-07 18:33:12.344434: step 34520, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.009 sec/batch; 83h:32m:50s remains)
INFO - root - 2017-12-07 18:33:21.903245: step 34530, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 76h:31m:55s remains)
INFO - root - 2017-12-07 18:33:31.568834: step 34540, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 76h:54m:26s remains)
INFO - root - 2017-12-07 18:33:41.221272: step 34550, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 81h:09m:37s remains)
INFO - root - 2017-12-07 18:33:50.710458: step 34560, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 78h:45m:13s remains)
INFO - root - 2017-12-07 18:34:00.337482: step 34570, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 79h:08m:12s remains)
INFO - root - 2017-12-07 18:34:09.657171: step 34580, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 76h:34m:24s remains)
INFO - root - 2017-12-07 18:34:19.257737: step 34590, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 75h:33m:12s remains)
INFO - root - 2017-12-07 18:34:29.008412: step 34600, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 80h:33m:34s remains)
2017-12-07 18:34:30.020363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2917747 -4.3030281 -4.3120971 -4.3165531 -4.3134069 -4.3064919 -4.3018212 -4.3026991 -4.3143387 -4.3291855 -4.333118 -4.3265171 -4.3174415 -4.3086281 -4.2990665][-4.3029919 -4.3172727 -4.3247933 -4.3293314 -4.325758 -4.3153815 -4.30188 -4.2962241 -4.3016973 -4.3103814 -4.3120313 -4.3112106 -4.3093271 -4.3005319 -4.2811174][-4.3173971 -4.3359208 -4.3436074 -4.3454218 -4.3386459 -4.3217139 -4.2946906 -4.2768049 -4.2756433 -4.2825522 -4.2838268 -4.2864089 -4.2884312 -4.2741008 -4.2378192][-4.3337073 -4.3515048 -4.3557405 -4.3521924 -4.338531 -4.3105369 -4.2675586 -4.2323246 -4.2219186 -4.2300858 -4.234436 -4.2444086 -4.2561455 -4.2370391 -4.1811166][-4.3436174 -4.3578024 -4.3574972 -4.3449883 -4.314436 -4.26276 -4.1917219 -4.132812 -4.113811 -4.1324129 -4.1588864 -4.1927652 -4.2234058 -4.2091446 -4.1487207][-4.3462644 -4.3569756 -4.3493686 -4.320313 -4.2642107 -4.1794076 -4.0695057 -3.9747405 -3.9547002 -4.0121427 -4.0843449 -4.1551175 -4.206264 -4.2095833 -4.1712141][-4.3378119 -4.3422079 -4.3291116 -4.2859263 -4.2092552 -4.0990725 -3.9568703 -3.838213 -3.8364973 -3.947402 -4.0651793 -4.1565356 -4.217833 -4.2390723 -4.2279992][-4.3144364 -4.3145041 -4.3018012 -4.2595425 -4.1824918 -4.0774927 -3.9502437 -3.8571305 -3.8790672 -4.000536 -4.1165261 -4.1973186 -4.2468271 -4.2656078 -4.263361][-4.2728195 -4.26587 -4.2594194 -4.2387094 -4.195013 -4.1351209 -4.0643086 -4.0142741 -4.0330424 -4.1106482 -4.1852932 -4.2353487 -4.2619166 -4.2691073 -4.2697334][-4.2275324 -4.2112622 -4.2113061 -4.2173924 -4.2147956 -4.2015586 -4.1764879 -4.1515064 -4.1543107 -4.18992 -4.2292285 -4.2571225 -4.2676044 -4.2700152 -4.2764974][-4.2032938 -4.1760674 -4.1717477 -4.1894927 -4.2080469 -4.2248869 -4.2302966 -4.2231126 -4.222578 -4.237092 -4.2603426 -4.2757454 -4.2814813 -4.2850022 -4.2951465][-4.2074852 -4.1703711 -4.1570783 -4.1750221 -4.2035255 -4.2373061 -4.2637763 -4.2747464 -4.2779789 -4.2859211 -4.2989922 -4.3081083 -4.3125453 -4.3156223 -4.3221436][-4.2354603 -4.197381 -4.180624 -4.1942916 -4.2255764 -4.265131 -4.3001018 -4.3199 -4.3269882 -4.327867 -4.3312221 -4.3336825 -4.3321733 -4.3307004 -4.3304734][-4.2759728 -4.2501516 -4.237998 -4.2438836 -4.2652321 -4.2937584 -4.321887 -4.3408237 -4.3463564 -4.3408966 -4.3348389 -4.3313627 -4.3259568 -4.3203244 -4.3158946][-4.3126779 -4.2981739 -4.2900529 -4.2884879 -4.2932138 -4.3046145 -4.3203444 -4.3309393 -4.3303161 -4.3188057 -4.3073955 -4.302762 -4.2991414 -4.2938375 -4.288403]]...]
INFO - root - 2017-12-07 18:34:39.834938: step 34610, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 80h:57m:16s remains)
INFO - root - 2017-12-07 18:34:49.556661: step 34620, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 79h:17m:59s remains)
INFO - root - 2017-12-07 18:34:59.251409: step 34630, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 79h:33m:34s remains)
INFO - root - 2017-12-07 18:35:09.030896: step 34640, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 80h:52m:42s remains)
INFO - root - 2017-12-07 18:35:18.663441: step 34650, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 78h:13m:00s remains)
INFO - root - 2017-12-07 18:35:28.463768: step 34660, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 83h:18m:42s remains)
INFO - root - 2017-12-07 18:35:37.969828: step 34670, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 82h:59m:18s remains)
INFO - root - 2017-12-07 18:35:47.651144: step 34680, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 78h:36m:15s remains)
INFO - root - 2017-12-07 18:35:57.270190: step 34690, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 77h:53m:15s remains)
INFO - root - 2017-12-07 18:36:06.925075: step 34700, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.019 sec/batch; 84h:15m:35s remains)
2017-12-07 18:36:07.902627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3429103 -4.3325238 -4.3196979 -4.296174 -4.253891 -4.2124434 -4.1864305 -4.1853189 -4.2073097 -4.2335067 -4.2439628 -4.2344074 -4.2114353 -4.1885471 -4.1732235][-4.3404045 -4.329227 -4.315721 -4.2919397 -4.2497883 -4.2059569 -4.1776109 -4.1748734 -4.1923904 -4.2157474 -4.2286711 -4.2228823 -4.2023511 -4.1832 -4.1745682][-4.3387427 -4.326735 -4.3119464 -4.2872329 -4.2451415 -4.2006545 -4.171298 -4.1639543 -4.1741438 -4.193172 -4.2048821 -4.1976194 -4.1750884 -4.15733 -4.1559172][-4.336606 -4.3229342 -4.3062873 -4.2800636 -4.23655 -4.1932058 -4.1689105 -4.1663876 -4.1783361 -4.1941862 -4.1976128 -4.1836238 -4.1577845 -4.1450768 -4.1537781][-4.335289 -4.3191166 -4.2987309 -4.2683048 -4.2201385 -4.1790509 -4.1654878 -4.1747813 -4.1962032 -4.2136 -4.2094445 -4.1859503 -4.1595607 -4.1538959 -4.1692142][-4.3351617 -4.3175611 -4.2932954 -4.2575808 -4.2037878 -4.1620827 -4.1543851 -4.1713319 -4.201694 -4.2247233 -4.2195129 -4.194242 -4.172 -4.1718569 -4.18799][-4.3362803 -4.3182893 -4.2927327 -4.2558274 -4.201633 -4.1586528 -4.1470485 -4.1593881 -4.1890016 -4.2185893 -4.2208 -4.2049317 -4.1899719 -4.1913252 -4.2012773][-4.337357 -4.3187513 -4.2919717 -4.2563086 -4.2030478 -4.1592503 -4.1393566 -4.1415353 -4.16781 -4.2047772 -4.2159872 -4.2123032 -4.2063618 -4.2071958 -4.2081761][-4.3374 -4.3187656 -4.2930284 -4.2594147 -4.2095795 -4.169075 -4.1505723 -4.1504707 -4.1787386 -4.2181273 -4.2298894 -4.2287912 -4.2270694 -4.22476 -4.2161741][-4.3367743 -4.319459 -4.2960305 -4.2663374 -4.2229486 -4.1891737 -4.1760635 -4.1810889 -4.2133436 -4.2466187 -4.2515931 -4.2497711 -4.2478189 -4.2407012 -4.2234583][-4.3360147 -4.3202882 -4.2982149 -4.2693076 -4.2278523 -4.1972909 -4.1888075 -4.1961584 -4.226202 -4.2539186 -4.2569194 -4.2570634 -4.2568583 -4.2480512 -4.2285652][-4.3361378 -4.320549 -4.299789 -4.2688403 -4.2215433 -4.1899734 -4.185214 -4.19169 -4.2132063 -4.2385283 -4.24719 -4.2525163 -4.2558393 -4.2480206 -4.2318196][-4.3379035 -4.3224745 -4.3026576 -4.2699523 -4.2190356 -4.1887336 -4.1852441 -4.184278 -4.19354 -4.2148356 -4.2290478 -4.2418494 -4.2519641 -4.2492237 -4.2373872][-4.3403225 -4.3264666 -4.3095446 -4.279407 -4.2342005 -4.2095366 -4.203258 -4.1888518 -4.1850791 -4.1990681 -4.2160282 -4.2341523 -4.2485137 -4.251689 -4.2444611][-4.3417668 -4.3300877 -4.3157182 -4.2895184 -4.2524066 -4.2301431 -4.2123594 -4.1831217 -4.1681876 -4.1782188 -4.1969767 -4.2175851 -4.2353992 -4.2476168 -4.2463341]]...]
INFO - root - 2017-12-07 18:36:17.410086: step 34710, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.912 sec/batch; 75h:25m:20s remains)
INFO - root - 2017-12-07 18:36:26.990132: step 34720, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 80h:56m:06s remains)
INFO - root - 2017-12-07 18:36:36.744740: step 34730, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 82h:21m:38s remains)
INFO - root - 2017-12-07 18:36:46.420773: step 34740, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 78h:26m:31s remains)
INFO - root - 2017-12-07 18:36:56.037335: step 34750, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 79h:23m:46s remains)
INFO - root - 2017-12-07 18:37:05.821594: step 34760, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 82h:06m:49s remains)
INFO - root - 2017-12-07 18:37:15.288328: step 34770, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 77h:55m:28s remains)
INFO - root - 2017-12-07 18:37:25.034923: step 34780, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 78h:49m:41s remains)
INFO - root - 2017-12-07 18:37:34.685933: step 34790, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 82h:31m:46s remains)
INFO - root - 2017-12-07 18:37:44.354485: step 34800, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 77h:21m:32s remains)
2017-12-07 18:37:45.318336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14209 -4.1547093 -4.1569772 -4.1377053 -4.1049271 -4.0872421 -4.1018543 -4.1271496 -4.1300192 -4.1311421 -4.1561189 -4.1837487 -4.2001858 -4.2061872 -4.1850519][-4.1152015 -4.1366673 -4.1524715 -4.1391282 -4.1042461 -4.0843606 -4.1046457 -4.1426644 -4.1621046 -4.1727228 -4.1934938 -4.2082062 -4.2089744 -4.198617 -4.1620393][-4.1472774 -4.1728983 -4.18825 -4.1762571 -4.141068 -4.1167431 -4.1268792 -4.1587224 -4.1814528 -4.2003088 -4.22558 -4.2376142 -4.2301116 -4.2070394 -4.1671042][-4.1940842 -4.2211747 -4.2328191 -4.2217312 -4.189033 -4.1564412 -4.1475554 -4.1572537 -4.1720152 -4.1961837 -4.2319469 -4.2517171 -4.2494493 -4.2310376 -4.2007508][-4.2197237 -4.2441196 -4.2506757 -4.2368608 -4.2024765 -4.1600389 -4.1312575 -4.119031 -4.1264143 -4.1643691 -4.2186813 -4.2540183 -4.2600265 -4.2504311 -4.2305179][-4.2206631 -4.2346196 -4.2337213 -4.2151 -4.17883 -4.1257243 -4.0676341 -4.02234 -4.030827 -4.0982265 -4.1768503 -4.2304249 -4.2481956 -4.254065 -4.24676][-4.2078695 -4.201323 -4.1868119 -4.1614604 -4.1216521 -4.0468445 -3.9413137 -3.8576524 -3.8904786 -4.0050197 -4.110095 -4.1805224 -4.217207 -4.2394829 -4.2441397][-4.2005029 -4.1740646 -4.1425347 -4.1107678 -4.0748658 -3.9903827 -3.856281 -3.7672858 -3.835458 -3.9699643 -4.076992 -4.1463103 -4.1919541 -4.2217941 -4.2310734][-4.2051077 -4.1702261 -4.1289434 -4.103231 -4.0862746 -4.0244508 -3.9219983 -3.8711016 -3.9298811 -4.0242186 -4.0981359 -4.1498456 -4.1899924 -4.2184558 -4.2271242][-4.19692 -4.1587491 -4.1151352 -4.0978971 -4.0989819 -4.0710816 -4.0169368 -3.9974642 -4.0315738 -4.0847349 -4.12953 -4.1686912 -4.2040682 -4.224957 -4.2280359][-4.1866179 -4.1483636 -4.1074162 -4.0965986 -4.1063323 -4.0992947 -4.0770769 -4.0776486 -4.1016016 -4.1330543 -4.1608543 -4.191999 -4.2200961 -4.2329116 -4.2307281][-4.1939096 -4.160821 -4.1296453 -4.1229782 -4.13314 -4.134654 -4.1272507 -4.1337614 -4.1517506 -4.1734428 -4.1913381 -4.2140064 -4.2352653 -4.2419224 -4.2370567][-4.2216039 -4.197722 -4.1747828 -4.1649394 -4.1674728 -4.1710124 -4.1713986 -4.1805148 -4.1962905 -4.2132311 -4.2241864 -4.23683 -4.2480755 -4.2493515 -4.2458534][-4.2566323 -4.2418675 -4.225636 -4.2133932 -4.2095346 -4.2127457 -4.2165656 -4.2258296 -4.2363014 -4.2454052 -4.25215 -4.2591763 -4.2632656 -4.2630281 -4.2627554][-4.2801609 -4.2720456 -4.2631731 -4.2550545 -4.2507811 -4.251658 -4.2545986 -4.2613149 -4.2660103 -4.2689219 -4.2718568 -4.2761059 -4.2786207 -4.2789159 -4.279767]]...]
INFO - root - 2017-12-07 18:37:54.842326: step 34810, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 80h:05m:16s remains)
INFO - root - 2017-12-07 18:38:04.408919: step 34820, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 76h:03m:13s remains)
INFO - root - 2017-12-07 18:38:13.960592: step 34830, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 82h:06m:15s remains)
INFO - root - 2017-12-07 18:38:23.613582: step 34840, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.013 sec/batch; 83h:43m:53s remains)
INFO - root - 2017-12-07 18:38:33.396498: step 34850, loss = 2.09, batch loss = 2.04 (7.8 examples/sec; 1.021 sec/batch; 84h:24m:24s remains)
INFO - root - 2017-12-07 18:38:42.949327: step 34860, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 80h:51m:12s remains)
INFO - root - 2017-12-07 18:38:52.405238: step 34870, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 74h:56m:55s remains)
INFO - root - 2017-12-07 18:39:01.875711: step 34880, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 80h:46m:24s remains)
INFO - root - 2017-12-07 18:39:11.626728: step 34890, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 83h:01m:20s remains)
INFO - root - 2017-12-07 18:39:21.290259: step 34900, loss = 2.05, batch loss = 2.00 (7.8 examples/sec; 1.027 sec/batch; 84h:55m:35s remains)
2017-12-07 18:39:22.227016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.196733 -4.1551485 -4.1180596 -4.1075029 -4.1400414 -4.1896281 -4.2203374 -4.2151732 -4.1712046 -4.1183863 -4.103806 -4.1396613 -4.1790295 -4.2099581 -4.2449789][-4.2221546 -4.1793976 -4.1362157 -4.1226411 -4.1519737 -4.1952643 -4.2218132 -4.2096276 -4.1529551 -4.0871668 -4.069109 -4.1088662 -4.1544948 -4.1944003 -4.2391272][-4.2444181 -4.1990356 -4.1512504 -4.1357145 -4.1650977 -4.2043672 -4.22956 -4.2176809 -4.1612287 -4.0969496 -4.0780768 -4.1128707 -4.1607561 -4.2075086 -4.2576928][-4.2490387 -4.2023072 -4.1528764 -4.1323833 -4.15484 -4.191113 -4.2214503 -4.2229404 -4.181294 -4.1323829 -4.1156621 -4.1409588 -4.1865969 -4.2360907 -4.2865844][-4.2337723 -4.1844611 -4.13569 -4.1099095 -4.1213837 -4.1580691 -4.2001619 -4.2206326 -4.2022095 -4.1735759 -4.1593995 -4.1717505 -4.2071161 -4.2511525 -4.2986388][-4.22236 -4.1691728 -4.1193361 -4.0850267 -4.0873728 -4.1235104 -4.1746149 -4.2112246 -4.2135086 -4.2000804 -4.1848693 -4.18194 -4.2009945 -4.23638 -4.2809873][-4.2216005 -4.1660123 -4.112083 -4.0692587 -4.059906 -4.0927439 -4.1486764 -4.1943779 -4.2096992 -4.2049909 -4.1888471 -4.17526 -4.1790733 -4.2024264 -4.2435164][-4.2301531 -4.1798973 -4.128686 -4.0813527 -4.058053 -4.0780191 -4.1285753 -4.1770315 -4.1982079 -4.1963177 -4.1793075 -4.1589622 -4.1518321 -4.1648736 -4.2049947][-4.2382951 -4.201304 -4.1624665 -4.1207662 -4.0890565 -4.0888753 -4.12116 -4.1616621 -4.1817641 -4.1800027 -4.1624255 -4.13945 -4.1255641 -4.1329226 -4.1756845][-4.2428722 -4.2194223 -4.1970358 -4.1724839 -4.144093 -4.1263456 -4.1312838 -4.1531196 -4.1676588 -4.1649027 -4.1477771 -4.1249394 -4.1087708 -4.1168489 -4.1647639][-4.231936 -4.2184706 -4.2121878 -4.2102661 -4.1955266 -4.1700511 -4.1536961 -4.1567721 -4.1636863 -4.1600409 -4.1478529 -4.1319551 -4.1197939 -4.1294866 -4.1734014][-4.2001462 -4.1965642 -4.2035036 -4.2222085 -4.2235837 -4.1979132 -4.1718359 -4.1668768 -4.1741614 -4.176579 -4.1769176 -4.1740732 -4.1685424 -4.1761417 -4.2060127][-4.1611109 -4.1675205 -4.1864429 -4.2183409 -4.2311039 -4.2093096 -4.1806912 -4.1756549 -4.1907787 -4.20475 -4.2198248 -4.2303557 -4.230628 -4.2342706 -4.2433286][-4.1349792 -4.1504307 -4.179862 -4.2193413 -4.2343392 -4.2130232 -4.1845794 -4.1821666 -4.2047257 -4.226994 -4.2496285 -4.2674713 -4.2741313 -4.2765765 -4.2720218][-4.1328912 -4.1512146 -4.1849046 -4.2226481 -4.2338476 -4.2096763 -4.1815391 -4.1836452 -4.2150388 -4.2426124 -4.2663765 -4.2846866 -4.2929454 -4.2942486 -4.2827826]]...]
INFO - root - 2017-12-07 18:39:32.003144: step 34910, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 82h:14m:51s remains)
INFO - root - 2017-12-07 18:39:41.851298: step 34920, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 83h:45m:44s remains)
INFO - root - 2017-12-07 18:39:51.540894: step 34930, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 81h:03m:05s remains)
INFO - root - 2017-12-07 18:40:00.992903: step 34940, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 75h:24m:06s remains)
INFO - root - 2017-12-07 18:40:10.690193: step 34950, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.970 sec/batch; 80h:08m:53s remains)
INFO - root - 2017-12-07 18:40:20.421853: step 34960, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 80h:45m:07s remains)
INFO - root - 2017-12-07 18:40:30.050818: step 34970, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 78h:35m:26s remains)
INFO - root - 2017-12-07 18:40:39.780398: step 34980, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 82h:04m:24s remains)
INFO - root - 2017-12-07 18:40:49.607390: step 34990, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 80h:45m:02s remains)
INFO - root - 2017-12-07 18:40:59.214642: step 35000, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 81h:08m:42s remains)
2017-12-07 18:41:00.199881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.361608 -4.3639855 -4.3642678 -4.3620949 -4.3586354 -4.3555984 -4.3506269 -4.3455014 -4.3435588 -4.3483248 -4.3568664 -4.3607569 -4.3558674 -4.342731 -4.3117418][-4.3613138 -4.3628631 -4.3613071 -4.3572106 -4.3523583 -4.3470035 -4.3377371 -4.3279662 -4.3225889 -4.3292065 -4.3416491 -4.3493328 -4.3458076 -4.3297586 -4.285965][-4.3596449 -4.3593373 -4.3566074 -4.3501229 -4.3418112 -4.3295426 -4.3120556 -4.2963662 -4.2886329 -4.2974205 -4.3145952 -4.3271513 -4.32655 -4.3057184 -4.2493033][-4.3595209 -4.3571706 -4.3517823 -4.3388586 -4.3216634 -4.2968 -4.2676725 -4.2487164 -4.2407541 -4.2531629 -4.2763 -4.2941418 -4.2980514 -4.2767224 -4.214437][-4.3636017 -4.357306 -4.3447409 -4.3197451 -4.2887769 -4.2476773 -4.2080774 -4.1895113 -4.1910663 -4.2144132 -4.2446318 -4.2679515 -4.2788043 -4.2613897 -4.1994166][-4.3619461 -4.3518276 -4.3324432 -4.293509 -4.2426677 -4.1799374 -4.1256332 -4.1087003 -4.1309137 -4.1739445 -4.2187958 -4.2529883 -4.2740211 -4.2648916 -4.2108965][-4.3527536 -4.34033 -4.313026 -4.2567873 -4.1810169 -4.0933304 -4.0216208 -4.0140185 -4.0656943 -4.1351089 -4.2005854 -4.2488408 -4.2776718 -4.2751918 -4.2337995][-4.3355336 -4.32064 -4.2827911 -4.2100296 -4.1115069 -3.9964945 -3.9106417 -3.9251778 -4.0121021 -4.1074982 -4.1898808 -4.2461872 -4.2737536 -4.2719917 -4.2430439][-4.30921 -4.2929263 -4.2476039 -4.1630235 -4.0478897 -3.9146843 -3.8396208 -3.8896489 -4.0082059 -4.1173387 -4.1945639 -4.2377129 -4.2545633 -4.2528481 -4.2401018][-4.2848907 -4.2730956 -4.2262526 -4.1400652 -4.0314083 -3.9180775 -3.8757474 -3.9490297 -4.0718169 -4.1680851 -4.2173939 -4.2299728 -4.2305403 -4.23269 -4.2391844][-4.2736278 -4.2723551 -4.2312021 -4.1570468 -4.0770683 -4.0112343 -4.0008802 -4.0665417 -4.1587644 -4.2192969 -4.2343717 -4.2235904 -4.2171488 -4.2291141 -4.2536969][-4.276063 -4.2860012 -4.2545953 -4.1985445 -4.1505079 -4.1244726 -4.1319561 -4.1771941 -4.2260504 -4.245091 -4.2351751 -4.2198648 -4.2213264 -4.2457581 -4.2810869][-4.2798443 -4.2941933 -4.2727766 -4.236289 -4.2147307 -4.2161126 -4.2299533 -4.2500277 -4.2568803 -4.2432923 -4.2241564 -4.2202115 -4.2391505 -4.274611 -4.3111858][-4.2709866 -4.284339 -4.2714643 -4.2540107 -4.2551136 -4.2711706 -4.2791076 -4.2726026 -4.2501893 -4.2231851 -4.2127581 -4.2279925 -4.2631674 -4.3040366 -4.3347526][-4.2556329 -4.2628331 -4.2599759 -4.2596278 -4.2738204 -4.2893386 -4.2813683 -4.2534437 -4.2214608 -4.204751 -4.215282 -4.2486229 -4.2910161 -4.3268628 -4.3472514]]...]
INFO - root - 2017-12-07 18:41:09.834797: step 35010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 77h:39m:27s remains)
INFO - root - 2017-12-07 18:41:19.572137: step 35020, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 77h:30m:20s remains)
INFO - root - 2017-12-07 18:41:29.304269: step 35030, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 78h:41m:40s remains)
INFO - root - 2017-12-07 18:41:38.960756: step 35040, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 79h:38m:00s remains)
INFO - root - 2017-12-07 18:41:48.571843: step 35050, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 80h:02m:22s remains)
INFO - root - 2017-12-07 18:41:58.185484: step 35060, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 80h:14m:54s remains)
INFO - root - 2017-12-07 18:42:07.784628: step 35070, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.964 sec/batch; 79h:40m:03s remains)
INFO - root - 2017-12-07 18:42:17.489227: step 35080, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 80h:58m:39s remains)
INFO - root - 2017-12-07 18:42:27.153487: step 35090, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 79h:57m:07s remains)
INFO - root - 2017-12-07 18:42:36.957159: step 35100, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 79h:44m:16s remains)
2017-12-07 18:42:38.145661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3052683 -4.2903452 -4.28192 -4.2821655 -4.2875175 -4.2936206 -4.297698 -4.2976046 -4.2929392 -4.2912569 -4.2945995 -4.2993789 -4.3050566 -4.3080497 -4.3087034][-4.2815843 -4.2658343 -4.2634625 -4.2725921 -4.28739 -4.3018947 -4.3109865 -4.3115387 -4.3030543 -4.2945189 -4.2920012 -4.2923164 -4.297967 -4.3029213 -4.302855][-4.24427 -4.2240825 -4.2285805 -4.2488208 -4.2744789 -4.2976465 -4.3136516 -4.3160958 -4.3020077 -4.283566 -4.2743959 -4.2723308 -4.279346 -4.2866645 -4.2878408][-4.2064509 -4.1767821 -4.1864939 -4.2199092 -4.2583928 -4.2905879 -4.3127589 -4.3154974 -4.2931271 -4.2647676 -4.25464 -4.2564077 -4.2678995 -4.2796359 -4.2822165][-4.1750469 -4.1288695 -4.1390257 -4.1856184 -4.2367978 -4.2778211 -4.302278 -4.3011942 -4.267189 -4.2323022 -4.227407 -4.2393909 -4.2572279 -4.275188 -4.2829909][-4.1605868 -4.0961714 -4.1015987 -4.1592488 -4.218935 -4.2630157 -4.2799444 -4.262033 -4.2071929 -4.1591086 -4.16295 -4.1951962 -4.2276373 -4.2574673 -4.2785969][-4.1663818 -4.0853539 -4.0769849 -4.1342468 -4.195991 -4.23655 -4.24005 -4.1960135 -4.1026139 -4.0317507 -4.0524011 -4.12185 -4.1784668 -4.2228446 -4.2577562][-4.1911397 -4.101182 -4.0670214 -4.1044331 -4.1607738 -4.1983938 -4.1949158 -4.133019 -3.9966819 -3.8926649 -3.9353685 -4.0508118 -4.1376333 -4.1961088 -4.2390881][-4.2234197 -4.1431432 -4.0950217 -4.1051769 -4.1481709 -4.18248 -4.1812444 -4.1215878 -3.9772127 -3.8575819 -3.9027975 -4.0271692 -4.1195006 -4.1797419 -4.2256141][-4.2345815 -4.1661549 -4.1189718 -4.1132469 -4.1464224 -4.1852794 -4.1937747 -4.1494937 -4.0399866 -3.940397 -3.9576943 -4.0511107 -4.1289511 -4.1798997 -4.2181883][-4.2131319 -4.1477981 -4.1076345 -4.1014209 -4.1340127 -4.1784296 -4.2001195 -4.1816707 -4.1181512 -4.0479507 -4.0389576 -4.0919981 -4.1512413 -4.1952844 -4.2271643][-4.1811724 -4.1143646 -4.0814505 -4.0863652 -4.1261892 -4.1722927 -4.2015042 -4.2042637 -4.1740527 -4.1273437 -4.1068697 -4.1310744 -4.1746631 -4.216692 -4.2474365][-4.1664863 -4.0998425 -4.0745425 -4.0933414 -4.14018 -4.1854453 -4.2137113 -4.2231879 -4.2095146 -4.1773796 -4.1527371 -4.158905 -4.1895285 -4.2302556 -4.2658529][-4.18571 -4.125823 -4.1072803 -4.1329608 -4.180542 -4.2184129 -4.23992 -4.2483649 -4.2421165 -4.2206721 -4.1995797 -4.1962452 -4.2129755 -4.2467031 -4.28167][-4.2280498 -4.17906 -4.1613035 -4.1823568 -4.2179909 -4.2435722 -4.2577782 -4.2632518 -4.2603116 -4.24669 -4.235 -4.2303767 -4.23846 -4.2628126 -4.2914152]]...]
INFO - root - 2017-12-07 18:42:47.942687: step 35110, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 77h:43m:53s remains)
INFO - root - 2017-12-07 18:42:57.674848: step 35120, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 79h:56m:11s remains)
INFO - root - 2017-12-07 18:43:07.404687: step 35130, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 82h:59m:48s remains)
INFO - root - 2017-12-07 18:43:17.101362: step 35140, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 78h:04m:48s remains)
INFO - root - 2017-12-07 18:43:26.723276: step 35150, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 76h:05m:39s remains)
INFO - root - 2017-12-07 18:43:36.184038: step 35160, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 82h:14m:36s remains)
INFO - root - 2017-12-07 18:43:45.892784: step 35170, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 79h:36m:56s remains)
INFO - root - 2017-12-07 18:43:55.588250: step 35180, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 77h:52m:49s remains)
INFO - root - 2017-12-07 18:44:05.230936: step 35190, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 77h:36m:25s remains)
INFO - root - 2017-12-07 18:44:14.863660: step 35200, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.925 sec/batch; 76h:24m:25s remains)
2017-12-07 18:44:15.816221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2824349 -4.2955956 -4.293467 -4.2719502 -4.2458887 -4.2219014 -4.1968307 -4.1835566 -4.18365 -4.1592293 -4.134263 -4.1303463 -4.1355743 -4.1436167 -4.1508842][-4.2844672 -4.3002229 -4.2951727 -4.272192 -4.2428575 -4.2163329 -4.1961441 -4.1847773 -4.1814837 -4.1618991 -4.1414042 -4.1286693 -4.11484 -4.1050606 -4.1092262][-4.284831 -4.3030071 -4.2978597 -4.2719846 -4.2377639 -4.2048721 -4.1860647 -4.1733952 -4.1704993 -4.1612368 -4.1522064 -4.1358771 -4.1079698 -4.0793734 -4.0748081][-4.2807317 -4.2989507 -4.292944 -4.2639031 -4.2238445 -4.1849308 -4.1640887 -4.1493821 -4.1537557 -4.1601849 -4.1664991 -4.1548543 -4.1225848 -4.08497 -4.0729313][-4.276361 -4.2963848 -4.2918286 -4.2632394 -4.22072 -4.1762276 -4.1474094 -4.1244073 -4.1312742 -4.1511207 -4.1715231 -4.1697211 -4.1432238 -4.1069326 -4.0907617][-4.2748823 -4.2951622 -4.2917223 -4.2659855 -4.2272806 -4.1793914 -4.1345687 -4.0933342 -4.0954504 -4.1282654 -4.1666279 -4.1812124 -4.1691356 -4.1410036 -4.1207743][-4.2719965 -4.2899642 -4.2885551 -4.2668958 -4.2332497 -4.180892 -4.1136851 -4.0378714 -4.0329266 -4.0921054 -4.1559272 -4.194418 -4.1985688 -4.1810308 -4.161716][-4.2686934 -4.2844644 -4.2844515 -4.2642593 -4.2268648 -4.1657166 -4.0738931 -3.9612978 -3.9570563 -4.0558963 -4.1499891 -4.2086773 -4.2288961 -4.2232466 -4.2103796][-4.2658353 -4.2783289 -4.2753258 -4.2525568 -4.2095885 -4.1462941 -4.0517797 -3.9407406 -3.9523129 -4.0598307 -4.155468 -4.2190609 -4.2495804 -4.2570386 -4.2524309][-4.2569971 -4.2622418 -4.2530107 -4.2305551 -4.1942019 -4.1480832 -4.081892 -4.0130506 -4.0275321 -4.098877 -4.1659021 -4.2202668 -4.2537861 -4.2702732 -4.2749634][-4.2406068 -4.2375412 -4.221076 -4.2032676 -4.1843848 -4.1628881 -4.1299982 -4.0945807 -4.1025248 -4.1392856 -4.1812038 -4.2225518 -4.2533703 -4.2709551 -4.2774091][-4.221664 -4.2146378 -4.1976743 -4.1844068 -4.1785822 -4.1775126 -4.1668935 -4.149035 -4.1491218 -4.1707687 -4.2028956 -4.2345047 -4.2569642 -4.2680321 -4.2671242][-4.2057953 -4.2031803 -4.1933522 -4.1851463 -4.1826992 -4.1898937 -4.188756 -4.1766043 -4.1717019 -4.188066 -4.2171702 -4.2422252 -4.2571206 -4.2591724 -4.24979][-4.1948977 -4.2017193 -4.2017813 -4.1948967 -4.1896472 -4.196136 -4.1958504 -4.1867785 -4.1811218 -4.1940331 -4.2207565 -4.2418571 -4.2521787 -4.2497029 -4.2369404][-4.1905746 -4.205348 -4.2129169 -4.2032557 -4.1966219 -4.1996245 -4.1975083 -4.1905904 -4.186944 -4.1972604 -4.2200675 -4.2376366 -4.2447643 -4.2448211 -4.2334929]]...]
INFO - root - 2017-12-07 18:44:25.539280: step 35210, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 78h:05m:58s remains)
INFO - root - 2017-12-07 18:44:35.111193: step 35220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 77h:28m:59s remains)
INFO - root - 2017-12-07 18:44:44.785910: step 35230, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 82h:31m:44s remains)
INFO - root - 2017-12-07 18:44:54.358565: step 35240, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 80h:53m:38s remains)
INFO - root - 2017-12-07 18:45:03.988823: step 35250, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 82h:44m:25s remains)
INFO - root - 2017-12-07 18:45:13.699696: step 35260, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.005 sec/batch; 82h:56m:21s remains)
INFO - root - 2017-12-07 18:45:23.314350: step 35270, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.969 sec/batch; 80h:02m:34s remains)
INFO - root - 2017-12-07 18:45:32.820564: step 35280, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 78h:26m:13s remains)
INFO - root - 2017-12-07 18:45:42.504512: step 35290, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 80h:45m:07s remains)
INFO - root - 2017-12-07 18:45:52.289407: step 35300, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 81h:03m:14s remains)
2017-12-07 18:45:53.204356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2509203 -4.2406149 -4.231914 -4.2278752 -4.2276192 -4.2236428 -4.2160182 -4.1995339 -4.1970525 -4.2164707 -4.2441344 -4.2722478 -4.3011212 -4.3310428 -4.3489223][-4.2445784 -4.226758 -4.2136831 -4.2068 -4.2041221 -4.1953173 -4.1837559 -4.1649294 -4.1627245 -4.1897426 -4.2279816 -4.262423 -4.2926087 -4.3244362 -4.3438954][-4.2382746 -4.218173 -4.204246 -4.1963096 -4.1925116 -4.1828928 -4.1686053 -4.1470566 -4.1412444 -4.1703811 -4.2144065 -4.2522235 -4.28364 -4.3161077 -4.3357086][-4.2272754 -4.2112975 -4.1997013 -4.193964 -4.19332 -4.1876464 -4.1729245 -4.145381 -4.1308932 -4.157445 -4.2038622 -4.2423625 -4.2737451 -4.3041282 -4.3226094][-4.2022758 -4.190609 -4.1805086 -4.1784649 -4.184886 -4.1878753 -4.1753464 -4.1450005 -4.1236548 -4.143672 -4.1873674 -4.2253222 -4.2567067 -4.2860742 -4.3050566][-4.1717854 -4.1552196 -4.1385384 -4.13496 -4.1460571 -4.1538444 -4.1415586 -4.1097789 -4.0839939 -4.1004372 -4.1432371 -4.1886959 -4.2271924 -4.2609825 -4.2839003][-4.1485324 -4.1183939 -4.0848641 -4.0711966 -4.0797224 -4.0885634 -4.0762444 -4.0436478 -4.0166621 -4.0315137 -4.0780139 -4.1347642 -4.1863689 -4.2305393 -4.2600203][-4.1385856 -4.0918841 -4.0411887 -4.0156841 -4.01965 -4.0311904 -4.0240412 -3.9964175 -3.9730248 -3.9888353 -4.039228 -4.10254 -4.162612 -4.211915 -4.243988][-4.1533918 -4.10621 -4.0544004 -4.0246959 -4.0266647 -4.040904 -4.0412607 -4.0234156 -4.0067387 -4.0204654 -4.0625911 -4.118578 -4.1734767 -4.2170396 -4.2446957][-4.1846371 -4.1510644 -4.1159468 -4.097415 -4.1008077 -4.1119442 -4.1122689 -4.101624 -4.0908718 -4.1013842 -4.1324196 -4.1754889 -4.21914 -4.2507915 -4.2671785][-4.210907 -4.1912732 -4.1715651 -4.1624045 -4.1667624 -4.1727672 -4.1711063 -4.1644735 -4.1581817 -4.1671343 -4.1918726 -4.227499 -4.2618384 -4.283092 -4.2908468][-4.2137733 -4.2048059 -4.196281 -4.1932764 -4.1965852 -4.1980715 -4.1927867 -4.1855345 -4.1808505 -4.1904769 -4.2147646 -4.2483764 -4.27817 -4.2959881 -4.304173][-4.1964688 -4.1966882 -4.1999836 -4.2067165 -4.212595 -4.2109027 -4.1983857 -4.1845956 -4.17772 -4.1885591 -4.2145395 -4.24849 -4.2781196 -4.298727 -4.3125029][-4.1801448 -4.1865253 -4.2005095 -4.2153721 -4.2233176 -4.2185087 -4.19949 -4.18073 -4.1725464 -4.1838126 -4.2111039 -4.2445784 -4.2740736 -4.2976036 -4.3157272][-4.183002 -4.1922331 -4.2089319 -4.2252493 -4.2326169 -4.224925 -4.2016668 -4.1788921 -4.1690683 -4.1813531 -4.2093658 -4.240139 -4.2671256 -4.2906065 -4.3101559]]...]
INFO - root - 2017-12-07 18:46:02.930635: step 35310, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 74h:49m:42s remains)
INFO - root - 2017-12-07 18:46:12.661627: step 35320, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 1.006 sec/batch; 83h:03m:30s remains)
INFO - root - 2017-12-07 18:46:22.310287: step 35330, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 80h:40m:29s remains)
INFO - root - 2017-12-07 18:46:31.989678: step 35340, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 79h:03m:45s remains)
INFO - root - 2017-12-07 18:46:41.490085: step 35350, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 71h:02m:59s remains)
INFO - root - 2017-12-07 18:46:51.070097: step 35360, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 82h:23m:43s remains)
INFO - root - 2017-12-07 18:47:00.708473: step 35370, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.991 sec/batch; 81h:46m:34s remains)
INFO - root - 2017-12-07 18:47:10.363309: step 35380, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 78h:42m:26s remains)
INFO - root - 2017-12-07 18:47:19.881966: step 35390, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 82h:41m:29s remains)
INFO - root - 2017-12-07 18:47:29.592235: step 35400, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 78h:20m:27s remains)
2017-12-07 18:47:30.544176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.322124 -4.3268042 -4.308229 -4.2776203 -4.2488961 -4.2269855 -4.2147574 -4.2142611 -4.2136011 -4.2104006 -4.2081447 -4.2077155 -4.2210474 -4.2409163 -4.2560649][-4.32028 -4.3178287 -4.2885485 -4.249156 -4.210947 -4.1791639 -4.165626 -4.1742558 -4.1847796 -4.18665 -4.1856394 -4.1825671 -4.1928821 -4.2120018 -4.2271829][-4.3191066 -4.3105536 -4.274672 -4.226265 -4.1748543 -4.1328511 -4.114481 -4.1290817 -4.1522121 -4.16301 -4.1641793 -4.1627989 -4.1730533 -4.1926107 -4.2084551][-4.3209386 -4.3078561 -4.2703648 -4.2172856 -4.1569076 -4.1045012 -4.0773191 -4.09195 -4.1257324 -4.1484017 -4.1585937 -4.1633945 -4.1722946 -4.185801 -4.1982403][-4.3204613 -4.3084092 -4.2763343 -4.22466 -4.1565208 -4.0891752 -4.0419397 -4.0465508 -4.0927267 -4.1391358 -4.1715708 -4.190136 -4.1973815 -4.1995316 -4.2003098][-4.3150954 -4.3046179 -4.2755322 -4.2234583 -4.1418266 -4.047719 -3.9662066 -3.9650595 -4.0366817 -4.1162491 -4.173 -4.2096157 -4.2222266 -4.2168064 -4.2077785][-4.3074241 -4.2922435 -4.2581968 -4.1947513 -4.0950332 -3.9668827 -3.8522987 -3.8706827 -3.9875095 -4.0953269 -4.1608181 -4.2040968 -4.2182641 -4.2132382 -4.2061839][-4.2974596 -4.2765102 -4.2355285 -4.1632576 -4.0556555 -3.9284167 -3.8364694 -3.8899002 -4.0149188 -4.11232 -4.1604538 -4.1878905 -4.1967497 -4.1964149 -4.1969213][-4.2910657 -4.2687311 -4.2261529 -4.1591821 -4.0767288 -4.0006633 -3.9628677 -4.0111175 -4.0945945 -4.1544647 -4.1773181 -4.184267 -4.1840515 -4.1853147 -4.1928377][-4.2925634 -4.2712288 -4.2368369 -4.1879005 -4.1394272 -4.1055202 -4.0931764 -4.1199841 -4.1618333 -4.1901464 -4.1935186 -4.18497 -4.1800437 -4.187748 -4.2049255][-4.2992926 -4.2821274 -4.2558641 -4.22425 -4.1968656 -4.1790438 -4.1741576 -4.1868763 -4.2047162 -4.2141609 -4.208704 -4.1949782 -4.1940227 -4.207952 -4.2266374][-4.3047452 -4.2925959 -4.2723818 -4.2524934 -4.2360845 -4.2244329 -4.221159 -4.22982 -4.2397089 -4.2390218 -4.2293096 -4.2186036 -4.2202616 -4.2350864 -4.2488065][-4.3093171 -4.3012729 -4.2839146 -4.2697248 -4.2583623 -4.2485771 -4.2470231 -4.2565975 -4.2638426 -4.2567625 -4.247117 -4.2396269 -4.2408028 -4.2510734 -4.2600765][-4.3114562 -4.3065395 -4.2871037 -4.2719126 -4.26164 -4.2529407 -4.2526693 -4.2627134 -4.2661366 -4.2562227 -4.24705 -4.2425637 -4.2463822 -4.2566929 -4.26371][-4.3126554 -4.3093834 -4.2869654 -4.2654853 -4.2500052 -4.2376447 -4.235961 -4.2472243 -4.2522669 -4.2426438 -4.2328711 -4.2331762 -4.2440786 -4.2581034 -4.2643642]]...]
INFO - root - 2017-12-07 18:47:40.077887: step 35410, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 77h:46m:54s remains)
INFO - root - 2017-12-07 18:47:49.790430: step 35420, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 78h:53m:15s remains)
INFO - root - 2017-12-07 18:47:59.550297: step 35430, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.930 sec/batch; 76h:42m:33s remains)
INFO - root - 2017-12-07 18:48:09.371662: step 35440, loss = 2.10, batch loss = 2.04 (7.7 examples/sec; 1.037 sec/batch; 85h:32m:34s remains)
INFO - root - 2017-12-07 18:48:18.813245: step 35450, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 81h:11m:59s remains)
INFO - root - 2017-12-07 18:48:28.342338: step 35460, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 76h:30m:28s remains)
INFO - root - 2017-12-07 18:48:37.911137: step 35470, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 76h:59m:24s remains)
INFO - root - 2017-12-07 18:48:47.481813: step 35480, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.002 sec/batch; 82h:39m:55s remains)
INFO - root - 2017-12-07 18:48:57.319450: step 35490, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 81h:01m:00s remains)
INFO - root - 2017-12-07 18:49:06.987430: step 35500, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 76h:31m:40s remains)
2017-12-07 18:49:07.979173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2748852 -4.2830987 -4.2847028 -4.2873521 -4.2953868 -4.2993813 -4.2949352 -4.2939658 -4.29774 -4.3125038 -4.3292418 -4.33425 -4.3255949 -4.3154182 -4.3090382][-4.2728243 -4.2901845 -4.2994323 -4.3083243 -4.3159361 -4.3150926 -4.303586 -4.294857 -4.2944736 -4.309557 -4.3288536 -4.3356452 -4.3266888 -4.3169441 -4.314199][-4.2759862 -4.3028078 -4.3204565 -4.3313427 -4.3323 -4.3177466 -4.2926865 -4.2744865 -4.2708526 -4.2903409 -4.3178153 -4.329237 -4.3192372 -4.3092794 -4.3095284][-4.284122 -4.3115215 -4.3305683 -4.33644 -4.323597 -4.2897496 -4.2481351 -4.2157917 -4.2102122 -4.2399516 -4.2857561 -4.3122921 -4.3083005 -4.302299 -4.3063526][-4.2945867 -4.3120337 -4.3247018 -4.3226686 -4.295886 -4.2429767 -4.1792879 -4.1257939 -4.116466 -4.1658783 -4.2408934 -4.2886624 -4.2988472 -4.3002892 -4.3052425][-4.30657 -4.3093352 -4.3077431 -4.2901721 -4.2463231 -4.1728058 -4.0825071 -3.9981866 -3.9890072 -4.0760741 -4.1859384 -4.2508116 -4.276165 -4.2854781 -4.2941294][-4.3064079 -4.2973452 -4.2787218 -4.2396326 -4.1757989 -4.0868549 -3.9756184 -3.8620965 -3.8572137 -3.9960055 -4.1411128 -4.2152128 -4.2428784 -4.2561007 -4.2736139][-4.2960477 -4.2770929 -4.2414451 -4.1825 -4.1025991 -4.0061069 -3.8818936 -3.7405329 -3.7403932 -3.9184592 -4.0865159 -4.1636982 -4.1832647 -4.1909986 -4.2153072][-4.2925849 -4.2685456 -4.2200956 -4.1491094 -4.0654902 -3.9790204 -3.8682325 -3.7377586 -3.7487283 -3.9112737 -4.0633664 -4.1347218 -4.1342664 -4.12705 -4.1537747][-4.3064833 -4.2840562 -4.2314272 -4.1588287 -4.0865359 -4.02524 -3.9472311 -3.8632903 -3.8844872 -3.9932628 -4.0989113 -4.14939 -4.1347556 -4.117528 -4.1376519][-4.3242455 -4.3038325 -4.2511435 -4.1832118 -4.1275268 -4.090601 -4.0461154 -4.0025225 -4.0228376 -4.0851808 -4.1464887 -4.1715131 -4.1549888 -4.1409049 -4.1550455][-4.3377457 -4.3211322 -4.2776036 -4.2226672 -4.1841407 -4.1669254 -4.1488247 -4.1318388 -4.1422734 -4.1710215 -4.20272 -4.2125764 -4.1980929 -4.1893463 -4.19853][-4.3406715 -4.3274341 -4.2980237 -4.2630706 -4.2418623 -4.2376246 -4.2334142 -4.2271829 -4.2284918 -4.2391257 -4.2569494 -4.2616463 -4.2496028 -4.2414618 -4.2468424][-4.3402262 -4.3320913 -4.3164668 -4.2981381 -4.2866983 -4.2869997 -4.2878704 -4.2845521 -4.2811513 -4.2862697 -4.2991948 -4.306036 -4.29872 -4.2911386 -4.2952719][-4.3398819 -4.3348122 -4.32821 -4.3213305 -4.3180537 -4.3197942 -4.32072 -4.3172297 -4.3129807 -4.3133273 -4.3194847 -4.3230386 -4.320106 -4.318543 -4.3245821]]...]
INFO - root - 2017-12-07 18:49:17.572008: step 35510, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 80h:08m:22s remains)
INFO - root - 2017-12-07 18:49:27.283803: step 35520, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 78h:17m:18s remains)
INFO - root - 2017-12-07 18:49:36.979537: step 35530, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 76h:05m:27s remains)
INFO - root - 2017-12-07 18:49:46.759545: step 35540, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 78h:03m:32s remains)
INFO - root - 2017-12-07 18:49:56.174977: step 35550, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 81h:58m:45s remains)
INFO - root - 2017-12-07 18:50:06.073126: step 35560, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 81h:07m:28s remains)
INFO - root - 2017-12-07 18:50:15.672561: step 35570, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.931 sec/batch; 76h:46m:31s remains)
INFO - root - 2017-12-07 18:50:25.323497: step 35580, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 81h:43m:34s remains)
INFO - root - 2017-12-07 18:50:35.200624: step 35590, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 82h:07m:16s remains)
INFO - root - 2017-12-07 18:50:44.694412: step 35600, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 75h:44m:11s remains)
2017-12-07 18:50:45.693234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3700948 -4.3679104 -4.3488383 -4.3062806 -4.2410622 -4.161047 -4.1111088 -4.1230049 -4.1656013 -4.2137322 -4.2561536 -4.2970953 -4.3268938 -4.3408561 -4.3436546][-4.3732138 -4.3680005 -4.3460236 -4.2994962 -4.2242155 -4.1283422 -4.0744386 -4.0995283 -4.1594243 -4.21263 -4.2560844 -4.2979689 -4.326683 -4.3408222 -4.3450608][-4.3735213 -4.3650441 -4.3364859 -4.2839394 -4.1991787 -4.0918679 -4.0351052 -4.0752797 -4.1544971 -4.2142668 -4.259954 -4.3017812 -4.32777 -4.3406458 -4.3446765][-4.3702517 -4.35893 -4.3252835 -4.269803 -4.1776495 -4.0604877 -3.9930677 -4.0384517 -4.1341529 -4.2087836 -4.2616477 -4.306108 -4.3306708 -4.3408055 -4.3421717][-4.3648381 -4.3529434 -4.3182344 -4.2632728 -4.1681013 -4.0472469 -3.9661567 -4.0067911 -4.1154757 -4.203454 -4.2649703 -4.3107872 -4.3348818 -4.3425908 -4.3391604][-4.3606105 -4.3520436 -4.3180208 -4.2598562 -4.1587906 -4.0349655 -3.9410794 -3.9756866 -4.0977306 -4.1987309 -4.2680416 -4.3135357 -4.3373437 -4.3439493 -4.3363452][-4.3564343 -4.3532343 -4.321599 -4.254859 -4.1433134 -4.0101118 -3.9037569 -3.9302108 -4.0642157 -4.1801562 -4.2576008 -4.3042479 -4.3311181 -4.3402014 -4.3302126][-4.3504009 -4.3502955 -4.3222151 -4.2497406 -4.1268969 -3.9799061 -3.856319 -3.8682833 -4.0110426 -4.1458693 -4.2351251 -4.2874789 -4.3161449 -4.3291211 -4.3187542][-4.3444982 -4.3447375 -4.3195186 -4.2509012 -4.1286831 -3.9826741 -3.8506012 -3.8373125 -3.9686618 -4.1152325 -4.2162662 -4.2741365 -4.3039513 -4.3206334 -4.311305][-4.3369827 -4.3376722 -4.3181005 -4.2636218 -4.1589642 -4.03238 -3.9105306 -3.8736234 -3.9722054 -4.1113997 -4.2122593 -4.2691679 -4.2967086 -4.3115549 -4.3043585][-4.3310905 -4.331573 -4.3164067 -4.274837 -4.1933279 -4.0934205 -3.9938271 -3.9501488 -4.0165043 -4.133348 -4.2246671 -4.2746177 -4.2975521 -4.3098607 -4.3043847][-4.3281927 -4.32883 -4.31813 -4.2876234 -4.2277541 -4.1543264 -4.0811648 -4.0484328 -4.0934243 -4.182179 -4.2554536 -4.2930317 -4.3073988 -4.3148942 -4.3091025][-4.3237419 -4.3272033 -4.3236752 -4.3073745 -4.2680683 -4.2190247 -4.169661 -4.14841 -4.1773438 -4.2394714 -4.2923408 -4.3162246 -4.3205957 -4.3211541 -4.3125715][-4.3192158 -4.3248954 -4.326417 -4.3211985 -4.2997241 -4.2695594 -4.2386889 -4.2250762 -4.242269 -4.2822361 -4.3159647 -4.33004 -4.329248 -4.3249488 -4.3158422][-4.3154755 -4.3218102 -4.326 -4.32708 -4.3176961 -4.29942 -4.2800894 -4.2727904 -4.283155 -4.3073392 -4.3263221 -4.3337007 -4.3313885 -4.3255334 -4.3171759]]...]
INFO - root - 2017-12-07 18:50:55.305593: step 35610, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 81h:39m:33s remains)
INFO - root - 2017-12-07 18:51:05.107506: step 35620, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.024 sec/batch; 84h:27m:36s remains)
INFO - root - 2017-12-07 18:51:14.869015: step 35630, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 78h:32m:05s remains)
INFO - root - 2017-12-07 18:51:24.631051: step 35640, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 79h:18m:49s remains)
INFO - root - 2017-12-07 18:51:34.221607: step 35650, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.005 sec/batch; 82h:50m:41s remains)
INFO - root - 2017-12-07 18:51:43.854433: step 35660, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 79h:04m:27s remains)
INFO - root - 2017-12-07 18:51:53.439396: step 35670, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 78h:35m:43s remains)
INFO - root - 2017-12-07 18:52:02.962057: step 35680, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 76h:52m:51s remains)
INFO - root - 2017-12-07 18:52:12.586645: step 35690, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 78h:06m:11s remains)
INFO - root - 2017-12-07 18:52:22.188799: step 35700, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 78h:25m:54s remains)
2017-12-07 18:52:23.214470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2117629 -4.1820192 -4.1487331 -4.1422458 -4.1772819 -4.2157702 -4.2335181 -4.2203183 -4.1887331 -4.1837473 -4.214457 -4.255208 -4.2873054 -4.2911263 -4.2795119][-4.2256789 -4.1867223 -4.1423974 -4.1278992 -4.1574359 -4.1900167 -4.2031 -4.1894116 -4.167161 -4.1703682 -4.2076449 -4.2518368 -4.2852778 -4.291554 -4.2816758][-4.2488365 -4.2149415 -4.1760373 -4.1613469 -4.1762748 -4.1862493 -4.1775308 -4.1533461 -4.1305285 -4.1381807 -4.1874566 -4.239615 -4.2734556 -4.2808714 -4.276494][-4.2537351 -4.2321758 -4.2099905 -4.2031407 -4.2074671 -4.1980815 -4.1647315 -4.1209636 -4.0824943 -4.0871491 -4.1546264 -4.2228646 -4.2593803 -4.2647085 -4.2636695][-4.2313061 -4.2249804 -4.2262115 -4.2362819 -4.2426176 -4.21925 -4.1618967 -4.0930967 -4.0315962 -4.0341511 -4.1171665 -4.1983795 -4.2414808 -4.2499561 -4.2515][-4.191329 -4.1986933 -4.2256041 -4.258287 -4.2703443 -4.2381606 -4.1639128 -4.071013 -3.9917085 -3.9987979 -4.0888786 -4.175189 -4.2260494 -4.242703 -4.2467127][-4.1511774 -4.171659 -4.2187638 -4.2659063 -4.2801437 -4.2414422 -4.1561894 -4.0534225 -3.9760125 -3.9952126 -4.0859733 -4.1715903 -4.2249789 -4.2454538 -4.250114][-4.1412339 -4.1708841 -4.2240882 -4.2686243 -4.2754993 -4.2301064 -4.1383195 -4.0361638 -3.9731588 -4.0071287 -4.1009979 -4.1868329 -4.2385621 -4.2561517 -4.259213][-4.1570587 -4.1931839 -4.2424622 -4.2724495 -4.2657909 -4.2139058 -4.1205153 -4.0247164 -3.9795141 -4.023901 -4.1165938 -4.2028451 -4.253418 -4.2704 -4.2753191][-4.1786504 -4.2186303 -4.2592521 -4.2729125 -4.2520308 -4.1979818 -4.1108308 -4.0288262 -4.0041928 -4.0527329 -4.1351519 -4.2130551 -4.260375 -4.2785931 -4.2863255][-4.2022653 -4.2424359 -4.2727489 -4.2729597 -4.2415175 -4.1880765 -4.1132503 -4.0569248 -4.05608 -4.1047058 -4.16857 -4.223002 -4.2592692 -4.2787852 -4.2905154][-4.2238412 -4.26205 -4.2813997 -4.270669 -4.2328634 -4.1799288 -4.1183572 -4.0904393 -4.111958 -4.1637039 -4.2076507 -4.2327452 -4.2503343 -4.2675128 -4.2836342][-4.2546692 -4.2868357 -4.2913513 -4.266634 -4.2195334 -4.16253 -4.1142063 -4.113533 -4.1513038 -4.2062082 -4.2371078 -4.2423906 -4.2430272 -4.251483 -4.2673464][-4.2951622 -4.3172903 -4.3055396 -4.2628331 -4.2011218 -4.1411719 -4.1094546 -4.1293826 -4.174922 -4.2306709 -4.26058 -4.2613592 -4.2544231 -4.25453 -4.2640486][-4.3303847 -4.3398452 -4.3110938 -4.2523289 -4.1804895 -4.123498 -4.1109853 -4.1453705 -4.1938791 -4.2476654 -4.2783718 -4.2807884 -4.2752404 -4.2718329 -4.27236]]...]
INFO - root - 2017-12-07 18:52:32.754761: step 35710, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 81h:24m:29s remains)
INFO - root - 2017-12-07 18:52:42.302060: step 35720, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 80h:09m:24s remains)
INFO - root - 2017-12-07 18:52:52.101796: step 35730, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 81h:32m:20s remains)
INFO - root - 2017-12-07 18:53:01.742848: step 35740, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 73h:22m:19s remains)
INFO - root - 2017-12-07 18:53:11.599985: step 35750, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 80h:24m:57s remains)
INFO - root - 2017-12-07 18:53:21.241603: step 35760, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.904 sec/batch; 74h:32m:36s remains)
INFO - root - 2017-12-07 18:53:31.205217: step 35770, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 82h:16m:11s remains)
INFO - root - 2017-12-07 18:53:40.863228: step 35780, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.988 sec/batch; 81h:23m:41s remains)
INFO - root - 2017-12-07 18:53:50.397248: step 35790, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 75h:04m:13s remains)
INFO - root - 2017-12-07 18:53:59.971315: step 35800, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 79h:36m:04s remains)
2017-12-07 18:54:01.011372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1163836 -4.1671796 -4.251008 -4.3086495 -4.3374658 -4.3519554 -4.3465018 -4.3063517 -4.2380295 -4.1674647 -4.1149144 -4.1199594 -4.1577649 -4.2025738 -4.2471366][-4.1958179 -4.2318997 -4.2940984 -4.329597 -4.3405633 -4.3419538 -4.3325982 -4.3060069 -4.2756443 -4.25628 -4.2447371 -4.2488637 -4.2623472 -4.2820129 -4.3028827][-4.264019 -4.291739 -4.3358765 -4.352674 -4.3430629 -4.3240986 -4.3041039 -4.2802405 -4.269659 -4.2778873 -4.2930927 -4.30866 -4.317667 -4.3256989 -4.3356729][-4.3066077 -4.3320909 -4.362319 -4.3694191 -4.3442006 -4.3011293 -4.2578926 -4.2269921 -4.2334952 -4.2662964 -4.2981882 -4.3251 -4.3422642 -4.3452792 -4.3456464][-4.3346596 -4.353796 -4.3714566 -4.3616114 -4.3124452 -4.2348614 -4.1540928 -4.1101832 -4.142839 -4.2154112 -4.2719421 -4.3156462 -4.343842 -4.3511987 -4.3476863][-4.3505039 -4.3605328 -4.3627048 -4.3267417 -4.2380371 -4.1115189 -3.9738102 -3.8990457 -3.9649637 -4.1052437 -4.2065544 -4.2767696 -4.3253527 -4.3449783 -4.3483005][-4.3559203 -4.3567715 -4.3420949 -4.275527 -4.1443863 -3.9664791 -3.7617819 -3.6235914 -3.7096574 -3.9239664 -4.0900249 -4.2007074 -4.2810755 -4.3236156 -4.3415222][-4.34921 -4.3443303 -4.3189945 -4.2422018 -4.1086354 -3.9242206 -3.6847572 -3.47583 -3.5248828 -3.762398 -3.9662633 -4.1065836 -4.2162294 -4.2842803 -4.3189154][-4.3505683 -4.3476791 -4.3206391 -4.2556477 -4.1571484 -4.0259948 -3.8374724 -3.6464071 -3.6233728 -3.7663383 -3.9358108 -4.0675449 -4.1805205 -4.2568483 -4.2969627][-4.3511772 -4.3537583 -4.3321667 -4.2849917 -4.2227192 -4.1483793 -4.0370388 -3.9110067 -3.8669391 -3.9180729 -4.0223322 -4.1164322 -4.2042036 -4.2671781 -4.2976046][-4.346312 -4.3538055 -4.3430247 -4.309773 -4.2739286 -4.2339845 -4.1720052 -4.0986495 -4.0565481 -4.0588017 -4.1171026 -4.1809492 -4.2436643 -4.2894492 -4.3097167][-4.3345938 -4.34903 -4.3517628 -4.3379889 -4.3201532 -4.2998734 -4.2607369 -4.211031 -4.1684437 -4.1414776 -4.161087 -4.2014928 -4.2496934 -4.2918735 -4.3137007][-4.32 -4.3391557 -4.3525968 -4.3555641 -4.3528142 -4.344449 -4.3175917 -4.2738557 -4.2230029 -4.1696529 -4.1528783 -4.1698952 -4.211668 -4.2621956 -4.2976785][-4.3102527 -4.331953 -4.3527427 -4.3665113 -4.3712707 -4.3674521 -4.3469739 -4.3082504 -4.2475767 -4.170352 -4.1170263 -4.1065307 -4.1415925 -4.2064948 -4.2618361][-4.30724 -4.3239331 -4.3414721 -4.35503 -4.3622246 -4.3646421 -4.3535757 -4.3213687 -4.260088 -4.1740985 -4.1007643 -4.0681367 -4.085268 -4.1524467 -4.2272511]]...]
INFO - root - 2017-12-07 18:54:10.794707: step 35810, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 0.994 sec/batch; 81h:56m:59s remains)
INFO - root - 2017-12-07 18:54:20.458371: step 35820, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 76h:11m:27s remains)
INFO - root - 2017-12-07 18:54:30.047213: step 35830, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 73h:56m:37s remains)
INFO - root - 2017-12-07 18:54:39.544034: step 35840, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 80h:44m:51s remains)
INFO - root - 2017-12-07 18:54:49.131490: step 35850, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.981 sec/batch; 80h:51m:56s remains)
INFO - root - 2017-12-07 18:54:58.961195: step 35860, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 83h:37m:06s remains)
INFO - root - 2017-12-07 18:55:08.775874: step 35870, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.997 sec/batch; 82h:08m:23s remains)
INFO - root - 2017-12-07 18:55:18.400143: step 35880, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 76h:11m:31s remains)
INFO - root - 2017-12-07 18:55:28.156445: step 35890, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 81h:53m:33s remains)
INFO - root - 2017-12-07 18:55:37.903367: step 35900, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 82h:27m:08s remains)
2017-12-07 18:55:38.937267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1903253 -4.1932287 -4.18998 -4.1941595 -4.2072449 -4.2128339 -4.2066727 -4.1903286 -4.1835914 -4.2076149 -4.2434864 -4.2665448 -4.282877 -4.285069 -4.2626061][-4.2023864 -4.1968584 -4.1901078 -4.1830187 -4.1834526 -4.1835637 -4.1832123 -4.1763115 -4.1687374 -4.1856976 -4.2318687 -4.2696548 -4.2936978 -4.301651 -4.2798519][-4.2215261 -4.2010736 -4.1865911 -4.1718273 -4.1563454 -4.1479688 -4.1533542 -4.1587329 -4.1531267 -4.1685057 -4.2221289 -4.2723842 -4.3018241 -4.3124146 -4.2934365][-4.2369661 -4.2023997 -4.1803184 -4.1597667 -4.1289139 -4.1085858 -4.1133671 -4.1259465 -4.1329803 -4.158886 -4.2182622 -4.2702866 -4.3002725 -4.3114653 -4.298389][-4.2556005 -4.2092109 -4.1802964 -4.1568046 -4.1168137 -4.0767694 -4.0559244 -4.05936 -4.0888062 -4.1433578 -4.2094917 -4.25485 -4.2847114 -4.3007154 -4.2955375][-4.273303 -4.2256403 -4.1951694 -4.1660228 -4.1177764 -4.055294 -3.9888208 -3.961081 -4.0053535 -4.1013036 -4.1856551 -4.2344851 -4.2667956 -4.2852345 -4.2893848][-4.2869034 -4.2493839 -4.2195959 -4.1797214 -4.1193247 -4.040977 -3.9381313 -3.87665 -3.9293928 -4.0614738 -4.1614747 -4.2179875 -4.2526007 -4.2699728 -4.2770953][-4.2857785 -4.2648807 -4.23718 -4.187181 -4.1175756 -4.0360169 -3.9385543 -3.8895743 -3.9430361 -4.0610781 -4.14781 -4.2018404 -4.2329087 -4.2408247 -4.2471161][-4.2655716 -4.2569904 -4.2345119 -4.1864715 -4.1189961 -4.0548296 -3.9903872 -3.9678273 -4.0026412 -4.070919 -4.1270628 -4.1779346 -4.2091746 -4.2131867 -4.2216468][-4.2312274 -4.2313824 -4.2168274 -4.1852827 -4.1376858 -4.0956054 -4.0587215 -4.0434418 -4.0527868 -4.0740104 -4.1055484 -4.1572409 -4.1913295 -4.1973534 -4.2049747][-4.1929131 -4.2002139 -4.1950989 -4.1807108 -4.1565132 -4.130795 -4.1082811 -4.0937977 -4.0845561 -4.0839782 -4.1084189 -4.1602654 -4.1896191 -4.1903324 -4.1859365][-4.1691022 -4.1740417 -4.1743741 -4.1665173 -4.1549711 -4.1410322 -4.1275969 -4.1145973 -4.1028371 -4.1047449 -4.1326084 -4.174859 -4.18551 -4.1679754 -4.1504216][-4.1689825 -4.168056 -4.1614947 -4.1471462 -4.1397104 -4.1400027 -4.1400962 -4.1343894 -4.1313143 -4.141293 -4.1648631 -4.1801291 -4.1650281 -4.1338634 -4.11369][-4.18732 -4.1798434 -4.1600804 -4.1359429 -4.13272 -4.1473756 -4.1575108 -4.16178 -4.1662745 -4.1755118 -4.19055 -4.1853685 -4.1525054 -4.1172204 -4.100678][-4.209023 -4.195045 -4.1682572 -4.1440868 -4.1480255 -4.1689835 -4.1843729 -4.1918778 -4.2009277 -4.2054968 -4.2106047 -4.1924548 -4.1554875 -4.1250467 -4.1131544]]...]
INFO - root - 2017-12-07 18:55:48.695560: step 35910, loss = 2.11, batch loss = 2.05 (7.9 examples/sec; 1.008 sec/batch; 83h:00m:21s remains)
INFO - root - 2017-12-07 18:55:58.354728: step 35920, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 77h:00m:31s remains)
INFO - root - 2017-12-07 18:56:07.880630: step 35930, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 81h:32m:02s remains)
INFO - root - 2017-12-07 18:56:17.482240: step 35940, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 76h:45m:03s remains)
INFO - root - 2017-12-07 18:56:27.303990: step 35950, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 82h:56m:26s remains)
INFO - root - 2017-12-07 18:56:37.084664: step 35960, loss = 2.09, batch loss = 2.04 (7.9 examples/sec; 1.012 sec/batch; 83h:22m:13s remains)
INFO - root - 2017-12-07 18:56:46.859962: step 35970, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 79h:31m:44s remains)
INFO - root - 2017-12-07 18:56:56.520146: step 35980, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 80h:01m:47s remains)
INFO - root - 2017-12-07 18:57:06.319676: step 35990, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 83h:17m:38s remains)
INFO - root - 2017-12-07 18:57:15.907921: step 36000, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 79h:19m:33s remains)
2017-12-07 18:57:16.839442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3379083 -4.3167815 -4.277441 -4.2298722 -4.1941381 -4.1824107 -4.1818986 -4.1798573 -4.1776419 -4.1744485 -4.1783414 -4.1879654 -4.196269 -4.1997089 -4.1978149][-4.3170819 -4.2936506 -4.2493095 -4.1958928 -4.1573653 -4.1427674 -4.1434865 -4.1500664 -4.1595049 -4.1712346 -4.1864276 -4.2010493 -4.2094059 -4.2103362 -4.2083144][-4.2917156 -4.2715526 -4.225246 -4.1708379 -4.1342587 -4.1211224 -4.1291761 -4.1506562 -4.1734443 -4.1964788 -4.2133989 -4.221715 -4.2246046 -4.2255039 -4.2274175][-4.26671 -4.249176 -4.2018251 -4.1481757 -4.1128058 -4.1028652 -4.1199265 -4.1570864 -4.1940308 -4.2237415 -4.2374029 -4.2359161 -4.2298536 -4.2325149 -4.2429504][-4.2362885 -4.2202115 -4.1749444 -4.122077 -4.0848241 -4.0756145 -4.1000395 -4.1523209 -4.2029438 -4.2351918 -4.2428236 -4.2331877 -4.222877 -4.2297044 -4.2495747][-4.2031922 -4.191741 -4.1503968 -4.095366 -4.0515919 -4.0434012 -4.0757322 -4.1363187 -4.1934257 -4.2214656 -4.2214718 -4.208024 -4.2032847 -4.2199583 -4.2482243][-4.1839633 -4.1764631 -4.1375647 -4.0779152 -4.0274615 -4.0158854 -4.0506907 -4.1146331 -4.172965 -4.1991005 -4.19687 -4.188941 -4.1938376 -4.2152505 -4.2456889][-4.1771631 -4.173707 -4.1392255 -4.0777664 -4.0212808 -3.9969811 -4.0234084 -4.088026 -4.150414 -4.1821442 -4.1880984 -4.18815 -4.1973348 -4.2171192 -4.2418323][-4.1740689 -4.1723294 -4.1444526 -4.0909824 -4.0343866 -3.9980745 -4.0123348 -4.0741825 -4.138967 -4.176682 -4.1930208 -4.1981087 -4.2035046 -4.2172613 -4.23395][-4.1708961 -4.1712284 -4.1520734 -4.1101241 -4.05867 -4.0150566 -4.0143857 -4.0626984 -4.1227117 -4.1664658 -4.1921034 -4.2053041 -4.2117777 -4.2209134 -4.2296677][-4.1670895 -4.1746211 -4.1665449 -4.1365228 -4.0907969 -4.0453005 -4.0314689 -4.0650883 -4.113565 -4.1561551 -4.1881475 -4.2115192 -4.2248063 -4.2302518 -4.2288909][-4.1658487 -4.1783581 -4.1780591 -4.1576133 -4.1221771 -4.0884666 -4.0778456 -4.1000791 -4.131547 -4.1620936 -4.1890464 -4.2142491 -4.2313266 -4.2338309 -4.2269206][-4.1577926 -4.1690497 -4.1716218 -4.158906 -4.137136 -4.1202474 -4.1215677 -4.1389494 -4.1590252 -4.1763315 -4.1924181 -4.2118111 -4.2272997 -4.2270422 -4.2192988][-4.1348386 -4.1417155 -4.1472368 -4.1470003 -4.1413231 -4.1393456 -4.1484823 -4.1621337 -4.1743093 -4.1848612 -4.1945992 -4.2083325 -4.2214541 -4.2208214 -4.2136579][-4.1212716 -4.1262646 -4.1337433 -4.1421413 -4.147521 -4.1548982 -4.1660247 -4.1738629 -4.1805878 -4.1872735 -4.195787 -4.2082214 -4.2186189 -4.2182412 -4.2128706]]...]
INFO - root - 2017-12-07 18:57:26.576985: step 36010, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 79h:36m:32s remains)
INFO - root - 2017-12-07 18:57:36.255643: step 36020, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 80h:24m:58s remains)
INFO - root - 2017-12-07 18:57:45.929292: step 36030, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 80h:52m:47s remains)
INFO - root - 2017-12-07 18:57:55.587719: step 36040, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 79h:39m:38s remains)
INFO - root - 2017-12-07 18:58:05.243955: step 36050, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 77h:44m:42s remains)
INFO - root - 2017-12-07 18:58:14.654066: step 36060, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 76h:11m:44s remains)
INFO - root - 2017-12-07 18:58:24.272603: step 36070, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 80h:08m:49s remains)
INFO - root - 2017-12-07 18:58:34.058791: step 36080, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 79h:53m:01s remains)
INFO - root - 2017-12-07 18:58:43.797943: step 36090, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 74h:47m:49s remains)
INFO - root - 2017-12-07 18:58:53.437591: step 36100, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 82h:48m:15s remains)
2017-12-07 18:58:54.391198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533641 -4.2357416 -4.2218981 -4.2169485 -4.2092466 -4.196578 -4.1801577 -4.1734018 -4.1798673 -4.2068219 -4.24076 -4.2722139 -4.2946157 -4.2968392 -4.2800808][-4.2384624 -4.217514 -4.208734 -4.211844 -4.214015 -4.212728 -4.2050066 -4.2023082 -4.2066889 -4.225451 -4.2530537 -4.281045 -4.3025537 -4.3041024 -4.2855353][-4.2167377 -4.1938219 -4.1896453 -4.2031283 -4.21523 -4.2225142 -4.2189951 -4.2172542 -4.2201991 -4.2309275 -4.2516079 -4.276381 -4.2961879 -4.297842 -4.2787714][-4.1934614 -4.16784 -4.1624627 -4.1788192 -4.1938591 -4.2036471 -4.2022309 -4.199903 -4.2055612 -4.2164197 -4.2373304 -4.2612929 -4.2777929 -4.2787023 -4.2611661][-4.1812229 -4.1560297 -4.1489234 -4.1619439 -4.1743078 -4.1816196 -4.1784644 -4.1761727 -4.1899028 -4.2035308 -4.22253 -4.2437539 -4.2562819 -4.25615 -4.2416182][-4.1667814 -4.1396322 -4.1290793 -4.1320052 -4.1348095 -4.1415176 -4.1425991 -4.1456184 -4.1702089 -4.188838 -4.2070556 -4.2262731 -4.2354722 -4.2344289 -4.2204432][-4.1425452 -4.1114473 -4.094202 -4.0830793 -4.0733004 -4.0775557 -4.0850334 -4.0965033 -4.1294789 -4.154654 -4.174283 -4.1924205 -4.2020216 -4.1994481 -4.1851683][-4.1126838 -4.0720611 -4.0410023 -4.0112185 -3.9856944 -3.9845762 -3.9972968 -4.023005 -4.0649252 -4.0943112 -4.1149144 -4.1319108 -4.1432967 -4.1442757 -4.1356692][-4.1037612 -4.0537591 -4.010601 -3.9686224 -3.9380534 -3.9338567 -3.9468608 -3.9823868 -4.030211 -4.0622444 -4.0853906 -4.1041231 -4.1190071 -4.1255178 -4.1237516][-4.1335421 -4.0906215 -4.0554905 -4.0237007 -4.0050611 -4.002573 -4.006969 -4.0372362 -4.0768905 -4.0995975 -4.1148138 -4.1288953 -4.14239 -4.1502576 -4.15365][-4.1833553 -4.1548643 -4.1372623 -4.1267648 -4.1232705 -4.1242003 -4.1232052 -4.1403427 -4.1606154 -4.1660557 -4.1695337 -4.1773887 -4.1867127 -4.1932974 -4.1986656][-4.2302947 -4.2116294 -4.2047625 -4.2064304 -4.2100573 -4.212142 -4.2098894 -4.2162418 -4.2215981 -4.2182479 -4.2155919 -4.2187705 -4.2248244 -4.23041 -4.2368727][-4.2670736 -4.2546606 -4.2526865 -4.2570167 -4.262825 -4.2663941 -4.2648463 -4.2642531 -4.2619004 -4.2562184 -4.2522049 -4.2522368 -4.2560997 -4.2617526 -4.2682381][-4.295743 -4.2874441 -4.2864594 -4.2910261 -4.2967763 -4.3001013 -4.2997413 -4.2983093 -4.2957358 -4.2919235 -4.2891197 -4.2893128 -4.2923284 -4.29563 -4.2984385][-4.3180132 -4.3126636 -4.3121285 -4.3150687 -4.3187728 -4.3212543 -4.32157 -4.3209467 -4.3194914 -4.3182907 -4.3177905 -4.3182354 -4.3195391 -4.3206215 -4.3212409]]...]
INFO - root - 2017-12-07 18:59:04.117890: step 36110, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 77h:03m:07s remains)
INFO - root - 2017-12-07 18:59:13.874360: step 36120, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 80h:52m:45s remains)
INFO - root - 2017-12-07 18:59:23.484014: step 36130, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 78h:29m:10s remains)
INFO - root - 2017-12-07 18:59:33.166791: step 36140, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 80h:57m:33s remains)
INFO - root - 2017-12-07 18:59:42.977653: step 36150, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 77h:39m:01s remains)
INFO - root - 2017-12-07 18:59:52.726261: step 36160, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 81h:28m:11s remains)
INFO - root - 2017-12-07 19:00:02.482365: step 36170, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 81h:57m:55s remains)
INFO - root - 2017-12-07 19:00:12.104890: step 36180, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 76h:44m:49s remains)
INFO - root - 2017-12-07 19:00:21.741844: step 36190, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 78h:42m:38s remains)
INFO - root - 2017-12-07 19:00:31.336120: step 36200, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.018 sec/batch; 83h:46m:47s remains)
2017-12-07 19:00:32.374770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2832079 -4.2857604 -4.274086 -4.24387 -4.1838803 -4.0722489 -3.9402921 -3.8633089 -3.9308026 -4.0521641 -4.1639638 -4.2544327 -4.3128738 -4.3399625 -4.3496943][-4.2789288 -4.2749786 -4.2592363 -4.2313819 -4.1798949 -4.0807056 -3.9627223 -3.896107 -3.9605646 -4.0735273 -4.1781192 -4.2635765 -4.318265 -4.3429203 -4.3506193][-4.2754121 -4.2600794 -4.2403717 -4.217905 -4.1791239 -4.097703 -4.0027542 -3.9558911 -4.0163069 -4.1162577 -4.2090373 -4.2822447 -4.3282557 -4.3477426 -4.3521976][-4.2755466 -4.2527146 -4.2288136 -4.2100439 -4.1808181 -4.1126456 -4.0326638 -3.9980168 -4.0557003 -4.150393 -4.2360144 -4.2987204 -4.3359137 -4.3507214 -4.353188][-4.2795072 -4.2568197 -4.2326 -4.2137022 -4.1845803 -4.1200781 -4.0428963 -4.0099535 -4.0687613 -4.1636677 -4.2481046 -4.3074236 -4.3402786 -4.35198 -4.3531804][-4.2817483 -4.265595 -4.2457023 -4.2245207 -4.1895761 -4.1200995 -4.0370989 -4.0004807 -4.0624514 -4.158978 -4.2457223 -4.307446 -4.3418326 -4.3527713 -4.3530288][-4.2773757 -4.2704897 -4.2591496 -4.2374992 -4.1961069 -4.1188602 -4.0275559 -3.9849653 -4.0475316 -4.1458564 -4.2359943 -4.3023062 -4.3406792 -4.3524132 -4.3531985][-4.2716923 -4.2724652 -4.2680511 -4.2480726 -4.2017646 -4.1166244 -4.016993 -3.9667983 -4.0297794 -4.1317191 -4.2264028 -4.2970109 -4.3378472 -4.351254 -4.3532863][-4.2686529 -4.2726989 -4.2728119 -4.2550936 -4.2073359 -4.1194186 -4.0164223 -3.9639902 -4.026866 -4.1286864 -4.2244287 -4.2960291 -4.3373394 -4.3514624 -4.353673][-4.2681036 -4.2721186 -4.27479 -4.2603669 -4.2148051 -4.1296196 -4.0313759 -3.983011 -4.0436826 -4.1386809 -4.2299581 -4.2990441 -4.3386712 -4.3523083 -4.3540158][-4.2661915 -4.2700052 -4.2730913 -4.2624526 -4.2213964 -4.1428566 -4.0536995 -4.0116234 -4.0692716 -4.1539311 -4.238256 -4.3038726 -4.34106 -4.3533039 -4.3540549][-4.2600713 -4.263238 -4.2655869 -4.26027 -4.2280807 -4.156364 -4.0739322 -4.0339127 -4.0869 -4.1628466 -4.2424817 -4.3071494 -4.3431849 -4.3539128 -4.3537984][-4.2447429 -4.2458682 -4.2453985 -4.2437558 -4.2198825 -4.1548152 -4.0765 -4.0362091 -4.0861797 -4.1605053 -4.2417731 -4.3091803 -4.3450408 -4.3541265 -4.3533821][-4.2309346 -4.2284236 -4.2239647 -4.2238183 -4.2053819 -4.1454234 -4.0682721 -4.0296392 -4.0806737 -4.1573319 -4.2422013 -4.3117185 -4.3459244 -4.353601 -4.352778][-4.2287631 -4.2254772 -4.2193604 -4.2206216 -4.20476 -4.1478996 -4.071362 -4.035954 -4.0880675 -4.1668773 -4.2506695 -4.3173141 -4.347013 -4.3524613 -4.3518167]]...]
INFO - root - 2017-12-07 19:00:41.997916: step 36210, loss = 2.12, batch loss = 2.06 (8.8 examples/sec; 0.913 sec/batch; 75h:08m:17s remains)
INFO - root - 2017-12-07 19:00:51.313384: step 36220, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 70h:09m:26s remains)
INFO - root - 2017-12-07 19:01:00.877937: step 36230, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 82h:27m:45s remains)
INFO - root - 2017-12-07 19:01:10.652640: step 36240, loss = 2.12, batch loss = 2.07 (7.7 examples/sec; 1.040 sec/batch; 85h:36m:17s remains)
INFO - root - 2017-12-07 19:01:20.196592: step 36250, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 76h:49m:58s remains)
INFO - root - 2017-12-07 19:01:29.751609: step 36260, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 76h:25m:43s remains)
INFO - root - 2017-12-07 19:01:39.337824: step 36270, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 80h:18m:39s remains)
INFO - root - 2017-12-07 19:01:48.979952: step 36280, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.008 sec/batch; 82h:56m:46s remains)
INFO - root - 2017-12-07 19:01:58.751068: step 36290, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 79h:30m:36s remains)
INFO - root - 2017-12-07 19:02:08.409510: step 36300, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 77h:41m:52s remains)
2017-12-07 19:02:09.405718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3192644 -4.30949 -4.3025184 -4.2969437 -4.291018 -4.2869415 -4.2838011 -4.2826433 -4.2814226 -4.2815351 -4.2861624 -4.2923245 -4.2977371 -4.3076997 -4.3200917][-4.3006768 -4.2831564 -4.2702832 -4.2614202 -4.2521043 -4.2427588 -4.2344794 -4.2305417 -4.2263517 -4.2261662 -4.2361517 -4.2519608 -4.2676029 -4.2874327 -4.3074622][-4.275146 -4.2487216 -4.2296381 -4.2185993 -4.207974 -4.192009 -4.1759248 -4.1692867 -4.16833 -4.17449 -4.191256 -4.2158704 -4.2422895 -4.2712216 -4.2944098][-4.2458611 -4.211885 -4.1897073 -4.1802154 -4.1705742 -4.149229 -4.1271868 -4.1262665 -4.1403089 -4.1563706 -4.1744308 -4.2032824 -4.2356043 -4.2641268 -4.285542][-4.218008 -4.1796346 -4.1572747 -4.1499553 -4.1372423 -4.1046729 -4.0763645 -4.0884714 -4.1255574 -4.1531329 -4.1660304 -4.1910515 -4.226346 -4.2515244 -4.269917][-4.1942849 -4.1522169 -4.1285052 -4.1180325 -4.0966907 -4.0461631 -4.0059729 -4.0341306 -4.1005907 -4.1419668 -4.1486597 -4.1662335 -4.202105 -4.2265873 -4.2493577][-4.1701331 -4.1223416 -4.0951691 -4.0778704 -4.0419126 -3.9642537 -3.906183 -3.9599841 -4.0610757 -4.1268506 -4.1330543 -4.1434412 -4.1784487 -4.20794 -4.23785][-4.15828 -4.1110473 -4.0848989 -4.06738 -4.0282793 -3.941103 -3.8786173 -3.9488964 -4.0696549 -4.1442943 -4.1503682 -4.1548848 -4.186471 -4.2161303 -4.2429738][-4.1657453 -4.1251183 -4.1034727 -4.0946312 -4.0716662 -4.0132561 -3.9725873 -4.0305672 -4.1371069 -4.2039557 -4.2078567 -4.2085562 -4.230834 -4.25314 -4.2702909][-4.1889191 -4.1578407 -4.1411667 -4.1386847 -4.13011 -4.1037436 -4.0845952 -4.1257091 -4.2064242 -4.2603688 -4.2627573 -4.2647367 -4.2823687 -4.2978449 -4.3026209][-4.2146726 -4.1900058 -4.174159 -4.1734633 -4.1754456 -4.1712627 -4.169559 -4.2001371 -4.2572465 -4.2965183 -4.2954884 -4.302896 -4.3204832 -4.330555 -4.3261333][-4.2421336 -4.2219238 -4.2057748 -4.2052469 -4.2127647 -4.2192492 -4.2237673 -4.2420268 -4.2773876 -4.3029089 -4.304431 -4.3169188 -4.3377767 -4.348753 -4.3408237][-4.2684307 -4.2507768 -4.2347941 -4.233139 -4.2418642 -4.252233 -4.2580347 -4.2643051 -4.280879 -4.2942009 -4.2984123 -4.3139257 -4.3361025 -4.3474064 -4.3408489][-4.2952094 -4.2822108 -4.2683592 -4.2635312 -4.2680879 -4.2746978 -4.2768393 -4.275928 -4.2793055 -4.2834611 -4.2871056 -4.3021374 -4.3212237 -4.3334451 -4.3326683][-4.3191915 -4.3123937 -4.303843 -4.2981763 -4.2976346 -4.2975969 -4.2949929 -4.2919421 -4.2913175 -4.2924781 -4.295454 -4.3055892 -4.3173728 -4.3271484 -4.3314323]]...]
INFO - root - 2017-12-07 19:02:19.118107: step 36310, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 79h:27m:00s remains)
INFO - root - 2017-12-07 19:02:28.834714: step 36320, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 80h:01m:48s remains)
INFO - root - 2017-12-07 19:02:38.517899: step 36330, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 83h:03m:00s remains)
INFO - root - 2017-12-07 19:02:48.080306: step 36340, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 76h:58m:29s remains)
INFO - root - 2017-12-07 19:02:57.601397: step 36350, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 82h:02m:24s remains)
INFO - root - 2017-12-07 19:03:07.339820: step 36360, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 80h:56m:06s remains)
INFO - root - 2017-12-07 19:03:16.986456: step 36370, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.970 sec/batch; 79h:45m:03s remains)
INFO - root - 2017-12-07 19:03:26.749543: step 36380, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.019 sec/batch; 83h:49m:19s remains)
INFO - root - 2017-12-07 19:03:36.189948: step 36390, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 77h:04m:46s remains)
INFO - root - 2017-12-07 19:03:45.824058: step 36400, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 78h:49m:12s remains)
2017-12-07 19:03:46.855582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2270579 -4.1964073 -4.1892805 -4.2061009 -4.2268686 -4.2455487 -4.2552948 -4.2410707 -4.2104969 -4.177732 -4.161109 -4.1694326 -4.1991243 -4.233151 -4.2481217][-4.2261429 -4.1888914 -4.1803751 -4.2007194 -4.2258892 -4.2430806 -4.2506347 -4.2392793 -4.2137208 -4.1810584 -4.1524725 -4.1434922 -4.1634369 -4.2047157 -4.2341628][-4.2303004 -4.1937132 -4.1840205 -4.2026391 -4.2265258 -4.2397547 -4.2453208 -4.2412767 -4.227561 -4.2038183 -4.1715608 -4.1475039 -4.1535025 -4.190321 -4.2240133][-4.2411284 -4.2119637 -4.2029743 -4.21496 -4.22884 -4.233973 -4.2391095 -4.2480679 -4.2540917 -4.2493854 -4.2284446 -4.202878 -4.1965508 -4.2156129 -4.23794][-4.2557712 -4.2371922 -4.2293329 -4.2316976 -4.2268748 -4.2173786 -4.2201471 -4.2386642 -4.2630463 -4.2792273 -4.28018 -4.2691526 -4.2626872 -4.2696533 -4.2804646][-4.2672219 -4.2606244 -4.2552991 -4.2469487 -4.2214851 -4.1924229 -4.1829333 -4.1993184 -4.2329936 -4.2634678 -4.27784 -4.279541 -4.2779512 -4.2843208 -4.298203][-4.2703557 -4.2720985 -4.269609 -4.2540374 -4.2114172 -4.1599979 -4.1297045 -4.1330709 -4.1719351 -4.2137394 -4.2340751 -4.2406197 -4.243114 -4.2522373 -4.2742434][-4.2651825 -4.2715292 -4.2738037 -4.256094 -4.2050648 -4.1377811 -4.0836596 -4.0629826 -4.0934558 -4.1349926 -4.1517067 -4.1502919 -4.1510878 -4.1666174 -4.2036567][-4.2567396 -4.2624745 -4.2689896 -4.2546115 -4.2064457 -4.1389618 -4.0721655 -4.0269656 -4.0345244 -4.0644321 -4.0774302 -4.0705509 -4.0630393 -4.0769758 -4.1222639][-4.2605429 -4.2618361 -4.2701778 -4.2646604 -4.2329035 -4.184948 -4.1328797 -4.0887785 -4.0779533 -4.0905638 -4.097259 -4.0896792 -4.077951 -4.0838127 -4.1170588][-4.27992 -4.2783346 -4.2867513 -4.2915559 -4.280417 -4.2580571 -4.2295394 -4.1996627 -4.1862745 -4.1889596 -4.1914072 -4.1886039 -4.1813087 -4.1823621 -4.1984315][-4.2907104 -4.2865853 -4.2946954 -4.3065777 -4.3084664 -4.302793 -4.2913895 -4.27269 -4.2599411 -4.2603564 -4.2656035 -4.269196 -4.2674737 -4.2670765 -4.2721915][-4.2879181 -4.2801833 -4.2866678 -4.303443 -4.3117137 -4.3122425 -4.3087373 -4.2950883 -4.2825985 -4.2793984 -4.2847738 -4.2923684 -4.2946105 -4.2945271 -4.2957172][-4.2698116 -4.2553806 -4.2595444 -4.2795191 -4.2931194 -4.2964025 -4.2935925 -4.2795377 -4.2648044 -4.2581329 -4.2623491 -4.2712107 -4.2753825 -4.2756076 -4.2745571][-4.249464 -4.2250924 -4.2220731 -4.240375 -4.2574134 -4.264751 -4.2612724 -4.2463689 -4.2315083 -4.2235808 -4.2264643 -4.2361875 -4.2422686 -4.2430449 -4.2410231]]...]
INFO - root - 2017-12-07 19:03:56.525787: step 36410, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 79h:06m:15s remains)
INFO - root - 2017-12-07 19:04:06.149435: step 36420, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 71h:56m:58s remains)
INFO - root - 2017-12-07 19:04:15.736174: step 36430, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 77h:12m:49s remains)
INFO - root - 2017-12-07 19:04:25.312482: step 36440, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.966 sec/batch; 79h:25m:22s remains)
INFO - root - 2017-12-07 19:04:35.145314: step 36450, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 78h:06m:52s remains)
INFO - root - 2017-12-07 19:04:44.801756: step 36460, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 79h:02m:33s remains)
INFO - root - 2017-12-07 19:04:54.397411: step 36470, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.982 sec/batch; 80h:43m:00s remains)
INFO - root - 2017-12-07 19:05:04.195342: step 36480, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 79h:54m:37s remains)
INFO - root - 2017-12-07 19:05:13.758129: step 36490, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 77h:32m:55s remains)
INFO - root - 2017-12-07 19:05:23.534639: step 36500, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.041 sec/batch; 85h:34m:40s remains)
2017-12-07 19:05:24.453879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2580142 -4.266799 -4.2851143 -4.3065548 -4.3226995 -4.3256006 -4.3039956 -4.2638879 -4.2369027 -4.2374163 -4.2661285 -4.3008022 -4.3312917 -4.3488407 -4.3554397][-4.2363343 -4.2466846 -4.2689929 -4.2929888 -4.312387 -4.319489 -4.301795 -4.2648687 -4.2408867 -4.2441545 -4.2724304 -4.3047438 -4.3326569 -4.3488269 -4.3547044][-4.2086678 -4.2158484 -4.2396398 -4.2636976 -4.282721 -4.2909617 -4.2742772 -4.2396736 -4.2221665 -4.2351031 -4.2684565 -4.3009787 -4.3283029 -4.3458419 -4.3522305][-4.1742058 -4.1767607 -4.1991506 -4.2243366 -4.2440743 -4.250843 -4.2292848 -4.1909132 -4.1794567 -4.2059841 -4.2472782 -4.2807961 -4.3117466 -4.3352284 -4.3447824][-4.14402 -4.1365809 -4.1522813 -4.1754327 -4.1970868 -4.2018452 -4.1693745 -4.1214352 -4.1160173 -4.1577005 -4.2083077 -4.246788 -4.2863359 -4.3186312 -4.3336987][-4.1171494 -4.0973897 -4.10128 -4.1209273 -4.1448655 -4.1456008 -4.1014204 -4.039577 -4.0397415 -4.1015954 -4.1694336 -4.2184486 -4.267818 -4.3063345 -4.3235388][-4.1030579 -4.0724983 -4.0642605 -4.0750284 -4.0931997 -4.0848031 -4.0304952 -3.9640243 -3.974504 -4.0580292 -4.1444974 -4.2092719 -4.2663078 -4.3064084 -4.3217235][-4.1273651 -4.095067 -4.0822811 -4.0821733 -4.0891933 -4.0715952 -4.0204573 -3.9697294 -3.989218 -4.0729237 -4.1593003 -4.2270741 -4.2829876 -4.3188124 -4.3294463][-4.1789365 -4.1576266 -4.150342 -4.14437 -4.1409965 -4.1196337 -4.0839953 -4.0555539 -4.0707145 -4.1274347 -4.1905255 -4.2469778 -4.295464 -4.3264465 -4.3342566][-4.2122641 -4.2008266 -4.1967554 -4.188107 -4.176713 -4.1566272 -4.1379561 -4.1253753 -4.1314549 -4.1621246 -4.2037864 -4.2504153 -4.2937088 -4.3232908 -4.332962][-4.2134266 -4.2059922 -4.2009234 -4.1903615 -4.1771617 -4.1622071 -4.1515183 -4.1409059 -4.137876 -4.1565371 -4.190959 -4.2363782 -4.2807817 -4.3111763 -4.3244576][-4.1834679 -4.1777673 -4.171699 -4.16057 -4.1475329 -4.1342025 -4.1240468 -4.1128325 -4.1064181 -4.1219864 -4.1583881 -4.208478 -4.2590814 -4.2955103 -4.3140082][-4.1422997 -4.1361184 -4.1315608 -4.1225219 -4.1097589 -4.0955482 -4.0855618 -4.0775828 -4.0739212 -4.09202 -4.13466 -4.1908593 -4.24595 -4.2871985 -4.3089724][-4.1285572 -4.1234183 -4.1228843 -4.1201415 -4.1123862 -4.10161 -4.0946307 -4.0887904 -4.0866642 -4.1057677 -4.1485167 -4.204493 -4.2575479 -4.2952061 -4.3139935][-4.1697464 -4.1681437 -4.1704493 -4.17269 -4.1717458 -4.1660991 -4.1623993 -4.1579533 -4.15675 -4.1735106 -4.2081943 -4.2520385 -4.2917681 -4.3171592 -4.3280644]]...]
INFO - root - 2017-12-07 19:05:34.172601: step 36510, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.989 sec/batch; 81h:20m:31s remains)
INFO - root - 2017-12-07 19:05:43.604161: step 36520, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 80h:30m:28s remains)
INFO - root - 2017-12-07 19:05:53.143069: step 36530, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 79h:32m:05s remains)
INFO - root - 2017-12-07 19:06:02.780880: step 36540, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 76h:50m:00s remains)
INFO - root - 2017-12-07 19:06:12.479677: step 36550, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 78h:18m:22s remains)
INFO - root - 2017-12-07 19:06:22.013538: step 36560, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 81h:16m:01s remains)
INFO - root - 2017-12-07 19:06:31.671249: step 36570, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 79h:43m:54s remains)
INFO - root - 2017-12-07 19:06:41.262888: step 36580, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 77h:29m:01s remains)
INFO - root - 2017-12-07 19:06:50.899404: step 36590, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 78h:18m:56s remains)
INFO - root - 2017-12-07 19:07:00.470716: step 36600, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 78h:24m:01s remains)
2017-12-07 19:07:01.351421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2205367 -4.2203507 -4.2205567 -4.2155519 -4.21274 -4.21447 -4.2112856 -4.1974673 -4.1842256 -4.1826444 -4.1778097 -4.1783066 -4.1800246 -4.1874518 -4.1893172][-4.1998239 -4.1977167 -4.2010503 -4.2047343 -4.2131848 -4.2209687 -4.2211356 -4.212543 -4.1993041 -4.19442 -4.1925106 -4.193563 -4.1941133 -4.1948972 -4.1859322][-4.2040348 -4.2034278 -4.2109237 -4.2234735 -4.2376037 -4.2449031 -4.2473569 -4.2449636 -4.237639 -4.2329535 -4.2308831 -4.2293658 -4.2241311 -4.2163463 -4.1996465][-4.2138009 -4.212461 -4.2200942 -4.2371736 -4.2532415 -4.2593489 -4.2621517 -4.261198 -4.2562537 -4.2511854 -4.24759 -4.2412744 -4.2277765 -4.212605 -4.1955109][-4.2249422 -4.2187681 -4.2246113 -4.2423773 -4.2587485 -4.2609415 -4.2585449 -4.25037 -4.2396793 -4.2363806 -4.2386765 -4.2353177 -4.2178316 -4.1971989 -4.1814117][-4.2446551 -4.233253 -4.235074 -4.2497716 -4.2634888 -4.2593431 -4.2449489 -4.2204952 -4.1995487 -4.2023368 -4.2189527 -4.2277656 -4.2137938 -4.1860766 -4.1663513][-4.25308 -4.2400146 -4.2393947 -4.2526021 -4.26085 -4.2466912 -4.2142477 -4.1672897 -4.1330061 -4.1411829 -4.1722383 -4.19483 -4.1860676 -4.1551437 -4.1346083][-4.2416344 -4.226 -4.2239528 -4.2394791 -4.2484379 -4.2284617 -4.1816626 -4.1145287 -4.0664024 -4.0754485 -4.11563 -4.1483583 -4.1531935 -4.1379261 -4.1261849][-4.2155523 -4.1922927 -4.1894975 -4.213182 -4.2325358 -4.220161 -4.1756244 -4.1071138 -4.0561523 -4.0635939 -4.1028657 -4.1402841 -4.1598353 -4.1625061 -4.1596479][-4.1847234 -4.1554365 -4.1539264 -4.1866436 -4.2196693 -4.2252502 -4.2000675 -4.1523376 -4.1140876 -4.1169877 -4.1407528 -4.1666265 -4.1822619 -4.1860123 -4.1829863][-4.1733704 -4.1460695 -4.1473374 -4.1768966 -4.2103281 -4.2259889 -4.2199187 -4.1954551 -4.17256 -4.1691132 -4.1768484 -4.1894155 -4.1968088 -4.19765 -4.1957841][-4.1762609 -4.1571164 -4.1595378 -4.1771646 -4.2010937 -4.2171807 -4.2202969 -4.213367 -4.2030668 -4.1990123 -4.2018356 -4.2097244 -4.2126756 -4.2139678 -4.2180233][-4.1833034 -4.1719956 -4.1748352 -4.1837945 -4.19897 -4.213098 -4.2215462 -4.227345 -4.2295189 -4.2307739 -4.2347903 -4.2410984 -4.2444038 -4.2489309 -4.2573895][-4.220037 -4.2124863 -4.214057 -4.2187738 -4.22787 -4.2391911 -4.2501111 -4.260839 -4.2679086 -4.2725115 -4.2774024 -4.2818527 -4.2850485 -4.2910261 -4.2987914][-4.2711749 -4.2676206 -4.2687149 -4.2706428 -4.27637 -4.2846394 -4.2932253 -4.302166 -4.308393 -4.3128905 -4.3154912 -4.317122 -4.3181329 -4.3206377 -4.3248796]]...]
INFO - root - 2017-12-07 19:07:11.001598: step 36610, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 79h:58m:44s remains)
INFO - root - 2017-12-07 19:07:20.717363: step 36620, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 82h:16m:56s remains)
INFO - root - 2017-12-07 19:07:30.277627: step 36630, loss = 2.12, batch loss = 2.07 (8.0 examples/sec; 0.999 sec/batch; 82h:08m:23s remains)
INFO - root - 2017-12-07 19:07:39.957784: step 36640, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 80h:12m:52s remains)
INFO - root - 2017-12-07 19:07:49.700509: step 36650, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 78h:17m:51s remains)
INFO - root - 2017-12-07 19:07:59.312690: step 36660, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 80h:55m:02s remains)
INFO - root - 2017-12-07 19:08:08.893938: step 36670, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 79h:54m:54s remains)
INFO - root - 2017-12-07 19:08:18.560579: step 36680, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 81h:38m:13s remains)
INFO - root - 2017-12-07 19:08:28.268107: step 36690, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 77h:49m:00s remains)
INFO - root - 2017-12-07 19:08:37.857201: step 36700, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 72h:14m:27s remains)
2017-12-07 19:08:38.830107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2510667 -4.2486243 -4.2321906 -4.1947446 -4.1566329 -4.128819 -4.1163082 -4.1213555 -4.132905 -4.1395535 -4.1555228 -4.1861458 -4.2187419 -4.2375665 -4.2449756][-4.2513885 -4.2514234 -4.2445087 -4.2141085 -4.1762304 -4.1434608 -4.1230364 -4.1221013 -4.1323404 -4.138176 -4.1559081 -4.190825 -4.226573 -4.2453618 -4.251514][-4.2451115 -4.2413287 -4.2394495 -4.2224374 -4.1941824 -4.1650863 -4.1390185 -4.1297135 -4.1336155 -4.1363792 -4.1559796 -4.1919918 -4.228054 -4.242445 -4.2416463][-4.2324734 -4.2185283 -4.215127 -4.2074561 -4.1941075 -4.1775107 -4.1564221 -4.1461115 -4.1486545 -4.1503797 -4.1648221 -4.1920781 -4.2209511 -4.2305422 -4.2194481][-4.2205853 -4.1956668 -4.1843462 -4.1776767 -4.1722817 -4.1664262 -4.1558065 -4.1499004 -4.1556416 -4.1646786 -4.1752968 -4.1930966 -4.2126684 -4.2154903 -4.1937408][-4.214325 -4.1826315 -4.1630354 -4.1511507 -4.1407242 -4.128634 -4.1208038 -4.1209564 -4.1406231 -4.1648397 -4.1791272 -4.19289 -4.2048688 -4.2045517 -4.1756616][-4.2148714 -4.1822357 -4.1581526 -4.1407313 -4.1177292 -4.0848465 -4.0567188 -4.0564942 -4.0945897 -4.1412425 -4.1696572 -4.1907048 -4.2053843 -4.2064729 -4.179781][-4.2228971 -4.1964755 -4.1748891 -4.1556044 -4.1191096 -4.0607972 -3.9996276 -3.9882116 -4.034256 -4.0959244 -4.1403303 -4.1737967 -4.2005825 -4.210485 -4.1970944][-4.2368226 -4.2212391 -4.206913 -4.1900644 -4.152144 -4.0841136 -4.002028 -3.9737887 -4.0088577 -4.0642061 -4.1102414 -4.1498837 -4.1882043 -4.2117143 -4.2152624][-4.2461281 -4.2461829 -4.2420173 -4.2317691 -4.2004862 -4.1399727 -4.0650377 -4.0272236 -4.0378423 -4.0686564 -4.1033726 -4.1394706 -4.1788859 -4.2072964 -4.2199025][-4.2376342 -4.2526593 -4.2561808 -4.2524428 -4.23228 -4.1869183 -4.13214 -4.0962477 -4.0913506 -4.1035824 -4.128643 -4.1594005 -4.1896405 -4.2063451 -4.2098904][-4.2175207 -4.2422762 -4.2510691 -4.2499704 -4.2376313 -4.2110572 -4.1796308 -4.1565065 -4.148016 -4.1503825 -4.1672511 -4.1944952 -4.2180929 -4.2218771 -4.2096968][-4.1987653 -4.2273388 -4.2376852 -4.2331529 -4.2223172 -4.2115951 -4.20216 -4.1931248 -4.1840715 -4.1802921 -4.1905646 -4.2159739 -4.2372351 -4.2364674 -4.2207551][-4.1940293 -4.218812 -4.2267361 -4.2204337 -4.2089143 -4.2061296 -4.210216 -4.2088861 -4.1970272 -4.1902413 -4.1984997 -4.220737 -4.2413287 -4.2439537 -4.2336454][-4.2109265 -4.222332 -4.2210097 -4.2133451 -4.2047105 -4.209857 -4.2224822 -4.2250347 -4.2134438 -4.2053328 -4.2112427 -4.2282219 -4.2432857 -4.248219 -4.2467437]]...]
INFO - root - 2017-12-07 19:08:48.251322: step 36710, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 74h:30m:40s remains)
INFO - root - 2017-12-07 19:08:57.776858: step 36720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 75h:22m:19s remains)
INFO - root - 2017-12-07 19:09:07.435323: step 36730, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.987 sec/batch; 81h:07m:13s remains)
INFO - root - 2017-12-07 19:09:17.056360: step 36740, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 81h:50m:49s remains)
INFO - root - 2017-12-07 19:09:26.503220: step 36750, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 79h:29m:00s remains)
INFO - root - 2017-12-07 19:09:36.131530: step 36760, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 82h:30m:00s remains)
INFO - root - 2017-12-07 19:09:45.742314: step 36770, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 76h:32m:38s remains)
INFO - root - 2017-12-07 19:09:55.288460: step 36780, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 79h:39m:51s remains)
INFO - root - 2017-12-07 19:10:04.934547: step 36790, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 83h:13m:54s remains)
INFO - root - 2017-12-07 19:10:14.574487: step 36800, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 81h:27m:10s remains)
2017-12-07 19:10:15.559565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2489653 -4.2161565 -4.1904988 -4.1960883 -4.226161 -4.2615967 -4.2853174 -4.2889004 -4.2807074 -4.2681623 -4.2447124 -4.2141676 -4.1742272 -4.1323748 -4.1110315][-4.2079945 -4.1677513 -4.142581 -4.157074 -4.1966295 -4.2356205 -4.256649 -4.2583618 -4.2512455 -4.2418737 -4.2172794 -4.1847029 -4.1360917 -4.0813951 -4.057384][-4.175519 -4.1354527 -4.1149111 -4.1351647 -4.1738319 -4.2014956 -4.2055249 -4.1986341 -4.1944714 -4.1939836 -4.1784768 -4.1554971 -4.1125774 -4.0586214 -4.034893][-4.1919045 -4.160028 -4.1472154 -4.1575837 -4.1717424 -4.1693397 -4.1411929 -4.1152062 -4.11602 -4.1376152 -4.1497803 -4.1533704 -4.1310167 -4.0910378 -4.0726085][-4.2178535 -4.1933532 -4.1851749 -4.1748023 -4.1498551 -4.1007543 -4.0259666 -3.9689612 -3.9806056 -4.0442352 -4.1053748 -4.150866 -4.1628394 -4.1450238 -4.1345997][-4.2345185 -4.2109947 -4.1974144 -4.1637506 -4.1038456 -4.0041022 -3.8646915 -3.7591703 -3.7948856 -3.9254525 -4.0448332 -4.1338196 -4.1802268 -4.1840611 -4.1825333][-4.2458134 -4.2224703 -4.2019167 -4.1564384 -4.08509 -3.9626925 -3.7829955 -3.6375618 -3.68995 -3.8634775 -4.0176435 -4.1286983 -4.1896381 -4.2049861 -4.2108665][-4.244823 -4.2321258 -4.2172551 -4.1816049 -4.13055 -4.0416293 -3.8998804 -3.7785971 -3.8097084 -3.9431093 -4.0639091 -4.1520834 -4.1972766 -4.2112756 -4.2193522][-4.2276096 -4.2303319 -4.2303023 -4.2146363 -4.1902618 -4.1442533 -4.0570498 -3.9759569 -3.9918785 -4.0707045 -4.13613 -4.1809878 -4.1951523 -4.1934109 -4.1951632][-4.1873169 -4.1995134 -4.2155447 -4.2229662 -4.2278409 -4.2147555 -4.1693649 -4.1225228 -4.1322608 -4.16991 -4.189899 -4.1933179 -4.1756387 -4.1505523 -4.1422844][-4.1688366 -4.1834283 -4.2094221 -4.2332211 -4.2556472 -4.2637019 -4.2452374 -4.2222905 -4.2232723 -4.2274861 -4.2140813 -4.1889772 -4.150773 -4.1119404 -4.0954132][-4.1857386 -4.2011404 -4.2273026 -4.253521 -4.2812667 -4.299674 -4.2974195 -4.2867727 -4.2803926 -4.2628169 -4.2268729 -4.1851578 -4.1406436 -4.1017289 -4.084516][-4.2131863 -4.2231426 -4.2435589 -4.2644863 -4.2918162 -4.3148823 -4.3220892 -4.3175106 -4.3082156 -4.2822909 -4.2395158 -4.195447 -4.156342 -4.1290035 -4.11714][-4.2379413 -4.2423673 -4.2568283 -4.2700734 -4.28961 -4.3111672 -4.3242865 -4.3245373 -4.3155322 -4.2917786 -4.256506 -4.2242928 -4.1977439 -4.180315 -4.1688809][-4.27036 -4.271153 -4.2761788 -4.2798467 -4.2875347 -4.3014464 -4.314672 -4.3182254 -4.3147321 -4.30157 -4.2806926 -4.2614217 -4.2437792 -4.2309346 -4.2175221]]...]
INFO - root - 2017-12-07 19:10:25.257539: step 36810, loss = 2.11, batch loss = 2.05 (8.8 examples/sec; 0.908 sec/batch; 74h:32m:43s remains)
INFO - root - 2017-12-07 19:10:34.736189: step 36820, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 79h:53m:40s remains)
INFO - root - 2017-12-07 19:10:44.630667: step 36830, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 76h:19m:05s remains)
INFO - root - 2017-12-07 19:10:54.457993: step 36840, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 79h:53m:20s remains)
INFO - root - 2017-12-07 19:11:04.238497: step 36850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 79h:20m:41s remains)
INFO - root - 2017-12-07 19:11:13.858157: step 36860, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 81h:48m:12s remains)
INFO - root - 2017-12-07 19:11:23.510278: step 36870, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 79h:20m:46s remains)
INFO - root - 2017-12-07 19:11:33.215033: step 36880, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 82h:32m:14s remains)
INFO - root - 2017-12-07 19:11:42.913801: step 36890, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 77h:18m:12s remains)
INFO - root - 2017-12-07 19:11:52.424541: step 36900, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 78h:58m:03s remains)
2017-12-07 19:11:53.359258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1681781 -4.2113953 -4.2419472 -4.2539511 -4.2521372 -4.2383313 -4.2088203 -4.18513 -4.1783285 -4.1652532 -4.1465535 -4.1403556 -4.1449046 -4.15118 -4.1430907][-4.1991649 -4.2390103 -4.26653 -4.2725286 -4.2598929 -4.2354512 -4.1958332 -4.164464 -4.1577816 -4.1410751 -4.1121683 -4.0970902 -4.1085072 -4.1300125 -4.1339521][-4.2231207 -4.2597823 -4.2790751 -4.2729321 -4.2455897 -4.210433 -4.1666107 -4.1413493 -4.1461196 -4.1331229 -4.0963902 -4.0685544 -4.080884 -4.1155386 -4.1290812][-4.2313237 -4.26535 -4.2772179 -4.2550554 -4.2022777 -4.1470714 -4.0996323 -4.096076 -4.1327353 -4.1436911 -4.1177187 -4.0868044 -4.0912914 -4.1234446 -4.1355181][-4.2200842 -4.2495475 -4.262197 -4.2328835 -4.1552811 -4.0610447 -3.9882543 -4.0016294 -4.0866652 -4.1456409 -4.150095 -4.1299844 -4.1276407 -4.1481485 -4.156703][-4.1807818 -4.2017326 -4.2232451 -4.1998897 -4.1065664 -3.9651036 -3.8417625 -3.8533962 -3.9906323 -4.10985 -4.1605711 -4.1645246 -4.1680818 -4.1826 -4.1879044][-4.1233826 -4.1301737 -4.1657867 -4.1677217 -4.087522 -3.9322169 -3.7665708 -3.7536945 -3.919801 -4.0812159 -4.1700335 -4.1996908 -4.2136073 -4.2244568 -4.2255607][-4.0589042 -4.05172 -4.1057558 -4.1506295 -4.1260281 -4.0218792 -3.8863873 -3.8517466 -3.9762716 -4.1173687 -4.20374 -4.2429166 -4.2610111 -4.2651625 -4.2597742][-4.0312843 -4.015264 -4.0788012 -4.1537447 -4.1798763 -4.1397738 -4.0580635 -4.0166235 -4.0778131 -4.1710515 -4.2364454 -4.2734137 -4.2892036 -4.2849708 -4.2688293][-4.0550075 -4.0425825 -4.0959024 -4.172895 -4.2229176 -4.221839 -4.1843843 -4.1524997 -4.1758776 -4.2317071 -4.276885 -4.3011394 -4.3026242 -4.2829647 -4.2560248][-4.1155591 -4.1121383 -4.1484132 -4.2066531 -4.2538733 -4.2689567 -4.2599311 -4.2460546 -4.2594051 -4.294713 -4.3174229 -4.3214707 -4.3077855 -4.2740049 -4.2375178][-4.2004595 -4.2050986 -4.2255707 -4.2588348 -4.2885141 -4.3010736 -4.3015733 -4.2974172 -4.3094563 -4.3324485 -4.33961 -4.3348346 -4.3194113 -4.2849107 -4.2481742][-4.271884 -4.2789116 -4.2932076 -4.307272 -4.31837 -4.3214917 -4.3208041 -4.3197131 -4.3291054 -4.3423882 -4.3444409 -4.3443651 -4.3369389 -4.3078594 -4.271523][-4.3052993 -4.3109694 -4.3224034 -4.3286853 -4.3309121 -4.3308749 -4.3293018 -4.3272424 -4.3307481 -4.3381023 -4.3430457 -4.3493161 -4.3463688 -4.3192964 -4.282146][-4.3151622 -4.3197546 -4.3289247 -4.3333268 -4.3337073 -4.3323612 -4.32979 -4.3266292 -4.325232 -4.3256145 -4.3277707 -4.3321643 -4.3284144 -4.305346 -4.2726583]]...]
INFO - root - 2017-12-07 19:12:03.065837: step 36910, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 82h:35m:23s remains)
INFO - root - 2017-12-07 19:12:12.714224: step 36920, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 79h:11m:08s remains)
INFO - root - 2017-12-07 19:12:22.485890: step 36930, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 77h:23m:43s remains)
INFO - root - 2017-12-07 19:12:32.167980: step 36940, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 80h:58m:11s remains)
INFO - root - 2017-12-07 19:12:41.719485: step 36950, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 79h:53m:14s remains)
INFO - root - 2017-12-07 19:12:51.316223: step 36960, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 77h:14m:34s remains)
INFO - root - 2017-12-07 19:13:01.094607: step 36970, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 79h:48m:42s remains)
INFO - root - 2017-12-07 19:13:10.869484: step 36980, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 81h:50m:42s remains)
INFO - root - 2017-12-07 19:13:20.525184: step 36990, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 74h:54m:12s remains)
INFO - root - 2017-12-07 19:13:30.209807: step 37000, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.985 sec/batch; 80h:48m:47s remains)
2017-12-07 19:13:31.132772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2770162 -4.2134509 -4.1536131 -4.1101112 -4.1012683 -4.1287446 -4.1587811 -4.1820259 -4.20293 -4.2262073 -4.2426724 -4.2549419 -4.2671461 -4.2703362 -4.2563596][-4.284164 -4.2333794 -4.1895838 -4.1623645 -4.1541972 -4.1704354 -4.18939 -4.1970134 -4.199708 -4.2082205 -4.2197623 -4.2375755 -4.260767 -4.2723007 -4.2663422][-4.2838426 -4.2465305 -4.2200632 -4.2109365 -4.2063022 -4.2098312 -4.2132835 -4.20344 -4.1897283 -4.1855416 -4.1900005 -4.2088251 -4.2399421 -4.2584076 -4.2632356][-4.2697973 -4.2395391 -4.2277021 -4.2358623 -4.2371459 -4.2316537 -4.222671 -4.201571 -4.17675 -4.1617856 -4.1538191 -4.167202 -4.2019796 -4.22715 -4.24291][-4.2413278 -4.2079797 -4.20398 -4.225687 -4.2387152 -4.232409 -4.2178845 -4.1927714 -4.1619291 -4.1312847 -4.1038795 -4.1026173 -4.1398439 -4.181118 -4.2144647][-4.2220058 -4.1773286 -4.1644268 -4.1869197 -4.2060819 -4.203867 -4.1913462 -4.1673913 -4.1300611 -4.0822411 -4.0325284 -4.0169578 -4.061482 -4.1317983 -4.1946812][-4.2111025 -4.1559629 -4.1252332 -4.1314793 -4.1392474 -4.1298752 -4.119875 -4.1050544 -4.0773306 -4.03195 -3.9720483 -3.9439113 -3.994978 -4.0892878 -4.175159][-4.2126837 -4.1475248 -4.0952177 -4.0747294 -4.0627851 -4.0468054 -4.0444646 -4.0510979 -4.04603 -4.0209408 -3.9719853 -3.9410768 -3.983923 -4.0764084 -4.16508][-4.2242961 -4.1539526 -4.0913434 -4.0586123 -4.0444913 -4.0359898 -4.0467649 -4.0699692 -4.0849633 -4.0801296 -4.0487256 -4.0216808 -4.0427022 -4.1028709 -4.1673517][-4.2440095 -4.1783304 -4.1241503 -4.100606 -4.0975823 -4.1006513 -4.1200266 -4.1490846 -4.1715374 -4.1745143 -4.1515341 -4.1266975 -4.1251373 -4.150897 -4.1854324][-4.2573338 -4.20725 -4.17329 -4.1641846 -4.1676517 -4.1741295 -4.1925321 -4.21774 -4.2388196 -4.2460313 -4.2297025 -4.2085381 -4.197176 -4.1979294 -4.2080355][-4.2705064 -4.2362962 -4.2167311 -4.2149606 -4.2211504 -4.2280369 -4.2415304 -4.2596221 -4.2757535 -4.2837572 -4.2744832 -4.2594275 -4.245224 -4.2352557 -4.2360711][-4.2827229 -4.2602558 -4.2504158 -4.2542286 -4.26156 -4.2694907 -4.2777734 -4.2881403 -4.2978582 -4.3049855 -4.3040023 -4.2954221 -4.2832284 -4.2726154 -4.2692409][-4.2976308 -4.2850723 -4.283298 -4.2889395 -4.2952104 -4.2995911 -4.3026137 -4.3062305 -4.3127341 -4.3206234 -4.3254852 -4.3227172 -4.314508 -4.3061967 -4.3011789][-4.3092127 -4.3020754 -4.3043184 -4.3098717 -4.3144474 -4.3148932 -4.3129382 -4.3133912 -4.3176608 -4.323606 -4.3278675 -4.3271074 -4.3231034 -4.3186073 -4.3139486]]...]
INFO - root - 2017-12-07 19:13:40.858864: step 37010, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 77h:26m:59s remains)
INFO - root - 2017-12-07 19:13:50.446761: step 37020, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 77h:32m:50s remains)
INFO - root - 2017-12-07 19:14:00.055138: step 37030, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 78h:04m:09s remains)
INFO - root - 2017-12-07 19:14:09.656064: step 37040, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 81h:11m:04s remains)
INFO - root - 2017-12-07 19:14:19.478239: step 37050, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 81h:01m:36s remains)
INFO - root - 2017-12-07 19:14:29.072658: step 37060, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.834 sec/batch; 68h:26m:21s remains)
INFO - root - 2017-12-07 19:14:38.780554: step 37070, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 79h:44m:45s remains)
INFO - root - 2017-12-07 19:14:48.520154: step 37080, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 79h:59m:29s remains)
INFO - root - 2017-12-07 19:14:58.373740: step 37090, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.041 sec/batch; 85h:26m:03s remains)
INFO - root - 2017-12-07 19:15:07.965621: step 37100, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 80h:40m:21s remains)
2017-12-07 19:15:08.877378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2286463 -4.244946 -4.2632861 -4.2855229 -4.3024907 -4.3177032 -4.3290715 -4.3357677 -4.3351655 -4.3278704 -4.3177342 -4.3081083 -4.3004761 -4.296257 -4.2925472][-4.2333903 -4.2426152 -4.2562971 -4.2770267 -4.2952805 -4.3140688 -4.3255887 -4.3288674 -4.323009 -4.3104138 -4.2977934 -4.2873712 -4.2798133 -4.278368 -4.2821155][-4.2572231 -4.2601733 -4.2699423 -4.2856221 -4.2992449 -4.3120055 -4.3161082 -4.3109813 -4.3001013 -4.2851791 -4.2742105 -4.2665997 -4.2633739 -4.2659822 -4.2784748][-4.2837677 -4.2794213 -4.2844491 -4.2921863 -4.2974949 -4.2980633 -4.2884941 -4.2700491 -4.2517657 -4.2367148 -4.2299585 -4.2282095 -4.2338963 -4.2432671 -4.2630119][-4.2926478 -4.2833223 -4.2822385 -4.2830424 -4.2816072 -4.2692833 -4.2427249 -4.2069387 -4.1803641 -4.1692991 -4.1753416 -4.1880832 -4.2076488 -4.2245317 -4.2461967][-4.2976589 -4.2799358 -4.2676058 -4.2603855 -4.2512517 -4.2260838 -4.1806989 -4.1253972 -4.0923853 -4.097609 -4.1294537 -4.1660013 -4.20043 -4.2237773 -4.2410188][-4.2860765 -4.2513242 -4.2229347 -4.2043591 -4.1877737 -4.156 -4.097754 -4.0255938 -3.9881396 -4.0146089 -4.0752378 -4.1313391 -4.1753683 -4.2027607 -4.2155213][-4.2442746 -4.1888747 -4.1443195 -4.1190352 -4.1023731 -4.0768127 -4.0193949 -3.9379487 -3.9001322 -3.943022 -4.0241709 -4.0959077 -4.1485295 -4.1808834 -4.19294][-4.1924591 -4.1242108 -4.0763979 -4.0538807 -4.0437336 -4.0349479 -3.9983931 -3.9363258 -3.916688 -3.9712324 -4.0558853 -4.1279011 -4.1758041 -4.2027121 -4.2108212][-4.1786218 -4.1182833 -4.0788617 -4.0626335 -4.0616217 -4.0723815 -4.064395 -4.03457 -4.0343165 -4.0826268 -4.1478229 -4.2008004 -4.233695 -4.2503796 -4.252965][-4.2068906 -4.1664176 -4.1398206 -4.1277766 -4.130877 -4.1456766 -4.1493421 -4.1398277 -4.1478491 -4.1827765 -4.2254252 -4.26103 -4.2829051 -4.290669 -4.2876582][-4.2486343 -4.2268806 -4.2119803 -4.2017341 -4.2018509 -4.209003 -4.2128191 -4.2135859 -4.2242146 -4.248724 -4.27652 -4.3005238 -4.3147736 -4.3166513 -4.3096404][-4.2817197 -4.2755184 -4.2726789 -4.26922 -4.2689195 -4.2705193 -4.2707167 -4.2705483 -4.2769923 -4.2918777 -4.3084054 -4.323307 -4.3309088 -4.329227 -4.3206248][-4.3002529 -4.3012552 -4.3038874 -4.306457 -4.3099561 -4.3133073 -4.3149538 -4.3157997 -4.3190107 -4.3261294 -4.3342576 -4.3398852 -4.3399758 -4.3339763 -4.3241062][-4.3095145 -4.3126187 -4.3159304 -4.320869 -4.3263083 -4.3305731 -4.3321285 -4.3321404 -4.3319645 -4.3333178 -4.3346066 -4.333725 -4.3302755 -4.3231096 -4.3133383]]...]
INFO - root - 2017-12-07 19:15:18.432454: step 37110, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 80h:55m:43s remains)
INFO - root - 2017-12-07 19:15:28.195349: step 37120, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 81h:19m:43s remains)
INFO - root - 2017-12-07 19:15:38.018745: step 37130, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 80h:50m:27s remains)
INFO - root - 2017-12-07 19:15:47.771807: step 37140, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 80h:52m:22s remains)
INFO - root - 2017-12-07 19:15:57.419120: step 37150, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 80h:22m:10s remains)
INFO - root - 2017-12-07 19:16:07.276100: step 37160, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 80h:56m:53s remains)
INFO - root - 2017-12-07 19:16:16.982405: step 37170, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 76h:34m:18s remains)
INFO - root - 2017-12-07 19:16:26.669531: step 37180, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 77h:53m:30s remains)
INFO - root - 2017-12-07 19:16:36.288047: step 37190, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 80h:29m:57s remains)
INFO - root - 2017-12-07 19:16:45.928463: step 37200, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 77h:38m:17s remains)
2017-12-07 19:16:46.863700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3046021 -4.3046179 -4.302156 -4.3005276 -4.3041773 -4.312109 -4.3153987 -4.3115621 -4.3050108 -4.3015523 -4.3030367 -4.3083239 -4.3126082 -4.3132038 -4.31277][-4.2735405 -4.2644315 -4.2531109 -4.2476435 -4.2553911 -4.2712359 -4.2780108 -4.274507 -4.267539 -4.2638278 -4.2658696 -4.2739811 -4.2795839 -4.2795153 -4.2806077][-4.2416992 -4.2237654 -4.2041287 -4.1961408 -4.2095218 -4.2352743 -4.2470455 -4.2441883 -4.23673 -4.2310529 -4.2315192 -4.2398577 -4.2457042 -4.244586 -4.2469277][-4.2233863 -4.2000175 -4.1775732 -4.171134 -4.1903038 -4.2229772 -4.2354355 -4.2305808 -4.220778 -4.2132463 -4.2124372 -4.2198052 -4.2235184 -4.2196836 -4.221828][-4.2215219 -4.1959324 -4.1758518 -4.1742496 -4.1957283 -4.2252092 -4.2332597 -4.2253156 -4.2144928 -4.2084212 -4.2092714 -4.2169228 -4.2183738 -4.2124553 -4.2135434][-4.2232032 -4.2009091 -4.187511 -4.1916022 -4.2109733 -4.2325273 -4.2354016 -4.2273316 -4.2180386 -4.2133656 -4.2161045 -4.2224841 -4.2207623 -4.2137113 -4.2139425][-4.213738 -4.1965141 -4.1893487 -4.1983733 -4.2179651 -4.2348294 -4.235908 -4.2289677 -4.222095 -4.2206292 -4.2248855 -4.2283907 -4.2238479 -4.2159681 -4.2153249][-4.1866503 -4.1722441 -4.1661992 -4.1759481 -4.1981039 -4.2175884 -4.2212605 -4.2164316 -4.2138457 -4.2184987 -4.2258487 -4.2272062 -4.221941 -4.2148986 -4.2145905][-4.1620684 -4.1492782 -4.141273 -4.1489429 -4.1725378 -4.1967311 -4.2029109 -4.1988525 -4.1996131 -4.2103043 -4.2199435 -4.221704 -4.2170153 -4.2106667 -4.2103653][-4.1635194 -4.1540914 -4.1452622 -4.14899 -4.1698437 -4.1936116 -4.19997 -4.1973696 -4.2002916 -4.2113204 -4.221786 -4.2230091 -4.2190571 -4.2128968 -4.2119789][-4.1973071 -4.1917863 -4.18563 -4.1871514 -4.201406 -4.2186737 -4.2228985 -4.2193375 -4.2194862 -4.2275743 -4.2345433 -4.2366877 -4.2350068 -4.2281146 -4.2256842][-4.2445812 -4.2415938 -4.2382107 -4.2393994 -4.2476673 -4.2571979 -4.2582374 -4.25189 -4.246037 -4.2478118 -4.2513237 -4.2534375 -4.2527275 -4.2463341 -4.2435951][-4.2872343 -4.2857924 -4.2836709 -4.2834997 -4.2875204 -4.2915254 -4.2909975 -4.2846589 -4.276576 -4.2746987 -4.2760334 -4.2772164 -4.2764025 -4.2707925 -4.2687473][-4.3156171 -4.3152714 -4.3134346 -4.3114057 -4.3116384 -4.3124142 -4.3121138 -4.3093581 -4.3046536 -4.3037848 -4.3055315 -4.3064179 -4.3042846 -4.2995381 -4.2977562][-4.3285155 -4.3283458 -4.3263292 -4.3235235 -4.3215575 -4.3209896 -4.3219814 -4.3226614 -4.3223653 -4.3237815 -4.3256392 -4.325345 -4.3222747 -4.3184376 -4.3172603]]...]
INFO - root - 2017-12-07 19:16:56.502995: step 37210, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 78h:31m:02s remains)
INFO - root - 2017-12-07 19:17:06.072663: step 37220, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 78h:30m:28s remains)
INFO - root - 2017-12-07 19:17:15.507935: step 37230, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 80h:27m:56s remains)
INFO - root - 2017-12-07 19:17:25.312998: step 37240, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.947 sec/batch; 77h:41m:16s remains)
INFO - root - 2017-12-07 19:17:35.014811: step 37250, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 79h:59m:34s remains)
INFO - root - 2017-12-07 19:17:44.685857: step 37260, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.984 sec/batch; 80h:41m:44s remains)
INFO - root - 2017-12-07 19:17:54.508012: step 37270, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.045 sec/batch; 85h:44m:12s remains)
INFO - root - 2017-12-07 19:18:04.243954: step 37280, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 78h:31m:42s remains)
INFO - root - 2017-12-07 19:18:13.751222: step 37290, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 77h:57m:35s remains)
INFO - root - 2017-12-07 19:18:23.379480: step 37300, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 78h:22m:04s remains)
2017-12-07 19:18:24.416172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3448753 -4.3447704 -4.3452039 -4.3458853 -4.3456092 -4.3471766 -4.3491488 -4.3505945 -4.35024 -4.3480034 -4.3455477 -4.3440723 -4.343811 -4.3439612 -4.3443327][-4.34954 -4.3455825 -4.3430705 -4.3409119 -4.3395176 -4.3437362 -4.3485012 -4.35092 -4.3477106 -4.341465 -4.3375049 -4.335268 -4.33501 -4.3355007 -4.3376522][-4.3394914 -4.3266406 -4.3184886 -4.3115659 -4.3103423 -4.3171134 -4.3231072 -4.3243289 -4.3178549 -4.3077531 -4.3018484 -4.3002234 -4.3042684 -4.3099709 -4.3167257][-4.3107896 -4.2895088 -4.2762666 -4.2657757 -4.2645273 -4.2674747 -4.2700138 -4.2687774 -4.2576504 -4.2452393 -4.2385116 -4.2413187 -4.2557678 -4.2714014 -4.2853127][-4.2689505 -4.2405338 -4.2248216 -4.2126446 -4.2035942 -4.1888766 -4.1771946 -4.1690235 -4.151063 -4.1399722 -4.1454711 -4.1692104 -4.2010474 -4.2236843 -4.2429595][-4.2245049 -4.1885104 -4.1736703 -4.1620188 -4.1412668 -4.1022034 -4.0701036 -4.0498424 -4.0280485 -4.0250182 -4.0530214 -4.1067934 -4.155973 -4.1857548 -4.210166][-4.1841068 -4.1468105 -4.1403742 -4.1336951 -4.101613 -4.0351343 -3.980829 -3.9504247 -3.9383535 -3.9561727 -4.0043068 -4.0741391 -4.1301103 -4.1653781 -4.1992126][-4.1662955 -4.1322808 -4.1313062 -4.1244369 -4.0797777 -4.0000186 -3.9352303 -3.9027689 -3.9055245 -3.9479651 -4.00995 -4.08476 -4.1421738 -4.1824827 -4.2196627][-4.1846695 -4.1592746 -4.1585007 -4.1515331 -4.1103287 -4.0442338 -3.9905877 -3.9599445 -3.9621911 -4.0088696 -4.0731993 -4.1442566 -4.1977463 -4.2326112 -4.2654514][-4.2343531 -4.2154436 -4.2143164 -4.2096262 -4.1830826 -4.1403108 -4.1075411 -4.0780659 -4.068779 -4.1000776 -4.1550379 -4.21978 -4.2656054 -4.2862659 -4.3091669][-4.2861438 -4.2737751 -4.2749672 -4.2701521 -4.2491479 -4.2219687 -4.2036657 -4.1796145 -4.1698604 -4.1911979 -4.234005 -4.2862473 -4.3178344 -4.3273025 -4.3411784][-4.3266153 -4.3223033 -4.3209929 -4.3092904 -4.2841225 -4.2641459 -4.2558684 -4.2454453 -4.2455254 -4.2658057 -4.2938333 -4.3274674 -4.3465667 -4.3525133 -4.3585567][-4.3466287 -4.3480592 -4.3446751 -4.3279734 -4.3021212 -4.2830329 -4.2799048 -4.27956 -4.2891159 -4.3126969 -4.3335376 -4.3520136 -4.3630524 -4.3646755 -4.3659487][-4.3593411 -4.3653855 -4.361845 -4.3444347 -4.3191671 -4.2987509 -4.2917991 -4.2930465 -4.3067179 -4.3318396 -4.3518143 -4.3666873 -4.373704 -4.3710155 -4.3676543][-4.36503 -4.3715396 -4.3666062 -4.3490939 -4.3235817 -4.3015981 -4.2908854 -4.2917967 -4.307271 -4.3314986 -4.35334 -4.3691378 -4.3735552 -4.3668842 -4.3603325]]...]
INFO - root - 2017-12-07 19:18:34.125086: step 37310, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.984 sec/batch; 80h:40m:59s remains)
INFO - root - 2017-12-07 19:18:43.816948: step 37320, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 81h:07m:43s remains)
INFO - root - 2017-12-07 19:18:53.560208: step 37330, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 76h:07m:06s remains)
INFO - root - 2017-12-07 19:19:03.212104: step 37340, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.940 sec/batch; 77h:04m:42s remains)
INFO - root - 2017-12-07 19:19:12.877035: step 37350, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 77h:28m:46s remains)
INFO - root - 2017-12-07 19:19:22.587715: step 37360, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 79h:40m:10s remains)
INFO - root - 2017-12-07 19:19:32.280443: step 37370, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 82h:47m:33s remains)
INFO - root - 2017-12-07 19:19:41.913536: step 37380, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 79h:50m:17s remains)
INFO - root - 2017-12-07 19:19:51.566902: step 37390, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 81h:08m:30s remains)
INFO - root - 2017-12-07 19:20:00.729966: step 37400, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 77h:53m:19s remains)
2017-12-07 19:20:01.761942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2871051 -4.281599 -4.2821579 -4.2947836 -4.3116956 -4.3235331 -4.3303146 -4.3262687 -4.3156085 -4.3031468 -4.290586 -4.2729135 -4.2418613 -4.214128 -4.1997304][-4.3135557 -4.3127141 -4.3135366 -4.321032 -4.3319373 -4.3393722 -4.3420343 -4.3357153 -4.3230443 -4.3075504 -4.2891922 -4.2675643 -4.2312121 -4.2026448 -4.1882668][-4.3223929 -4.3224897 -4.3232484 -4.3263144 -4.3307252 -4.3318238 -4.3274307 -4.3178205 -4.3063941 -4.2933359 -4.2770267 -4.2562318 -4.2208152 -4.19346 -4.1783662][-4.3160033 -4.3119607 -4.3105612 -4.3103824 -4.3075504 -4.2989039 -4.2844 -4.26872 -4.262598 -4.26012 -4.2558336 -4.2416859 -4.2131228 -4.1901402 -4.1771045][-4.2880564 -4.2751255 -4.2695775 -4.2658553 -4.2526603 -4.22872 -4.1996627 -4.1794662 -4.1895156 -4.2057958 -4.2185826 -4.2135758 -4.1929278 -4.184145 -4.182044][-4.2371087 -4.2074757 -4.1965547 -4.1929469 -4.1673212 -4.118783 -4.0601296 -4.0334568 -4.0800514 -4.1312 -4.1598544 -4.1602988 -4.1399069 -4.1471643 -4.1648088][-4.1751986 -4.1240358 -4.1121993 -4.1193104 -4.0870662 -4.0093012 -3.8968964 -3.8520198 -3.9574811 -4.0570946 -4.0977621 -4.0901089 -4.0567641 -4.0742397 -4.116004][-4.1234012 -4.0536103 -4.047832 -4.074584 -4.046277 -3.9533682 -3.8004146 -3.7380855 -3.888623 -4.0249238 -4.0722518 -4.050323 -4.0021725 -4.0219169 -4.0762792][-4.1046996 -4.047101 -4.0553532 -4.086225 -4.0664864 -3.9984107 -3.8800886 -3.83081 -3.9498634 -4.0727673 -4.1216106 -4.0982771 -4.0486078 -4.0444503 -4.0781903][-4.1211853 -4.0969839 -4.1162786 -4.1413355 -4.1302385 -4.0991869 -4.0401258 -4.0086689 -4.0754652 -4.158989 -4.2007785 -4.1841736 -4.1403365 -4.1154261 -4.12273][-4.1594467 -4.1602383 -4.1800637 -4.1987944 -4.1987362 -4.1909342 -4.1659374 -4.1465149 -4.1776037 -4.226676 -4.2577419 -4.2510657 -4.223794 -4.19426 -4.1903472][-4.2101769 -4.2161965 -4.2283168 -4.2444243 -4.2542772 -4.2559276 -4.2458858 -4.2383189 -4.2534242 -4.280086 -4.300107 -4.2994514 -4.283421 -4.2613163 -4.2567339][-4.2545438 -4.2592664 -4.266675 -4.279552 -4.292942 -4.29792 -4.2931714 -4.2922263 -4.3018169 -4.3158274 -4.3262606 -4.325232 -4.3148928 -4.3025045 -4.3008876][-4.2829108 -4.2870884 -4.2915339 -4.3004994 -4.3119812 -4.3182869 -4.3167396 -4.3167672 -4.3207994 -4.3265386 -4.3325653 -4.33176 -4.3243408 -4.3175759 -4.3185906][-4.2980614 -4.3022628 -4.3084493 -4.3166103 -4.3244257 -4.326704 -4.3229251 -4.320508 -4.3214221 -4.3234673 -4.3269825 -4.3284769 -4.3243418 -4.3210793 -4.3232956]]...]
INFO - root - 2017-12-07 19:20:11.481525: step 37410, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.009 sec/batch; 82h:44m:18s remains)
INFO - root - 2017-12-07 19:20:21.225682: step 37420, loss = 2.10, batch loss = 2.05 (7.9 examples/sec; 1.011 sec/batch; 82h:53m:24s remains)
INFO - root - 2017-12-07 19:20:31.038132: step 37430, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 82h:19m:33s remains)
INFO - root - 2017-12-07 19:20:40.655738: step 37440, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 81h:15m:12s remains)
INFO - root - 2017-12-07 19:20:50.271252: step 37450, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 77h:16m:49s remains)
INFO - root - 2017-12-07 19:21:00.025271: step 37460, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 80h:10m:47s remains)
INFO - root - 2017-12-07 19:21:09.669344: step 37470, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 80h:42m:24s remains)
INFO - root - 2017-12-07 19:21:19.387773: step 37480, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.007 sec/batch; 82h:31m:01s remains)
INFO - root - 2017-12-07 19:21:29.039380: step 37490, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 74h:58m:06s remains)
INFO - root - 2017-12-07 19:21:38.705966: step 37500, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 80h:04m:45s remains)
2017-12-07 19:21:39.664237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1846747 -4.173532 -4.1635375 -4.1633258 -4.1685491 -4.1781697 -4.1891174 -4.20179 -4.2089 -4.2100663 -4.2113929 -4.2051334 -4.1998849 -4.1929817 -4.186151][-4.1558385 -4.1438265 -4.1428657 -4.1594596 -4.1815104 -4.202323 -4.2117963 -4.2136397 -4.2131691 -4.2059393 -4.204289 -4.1997061 -4.1913695 -4.1849442 -4.184123][-4.148562 -4.1354051 -4.1362357 -4.158402 -4.1907258 -4.2198706 -4.229002 -4.2227607 -4.2147055 -4.1993713 -4.1913557 -4.1873174 -4.1798482 -4.1758265 -4.1790776][-4.1575384 -4.1400924 -4.1363192 -4.1583223 -4.19657 -4.229332 -4.2371564 -4.2214785 -4.2049241 -4.1856318 -4.17743 -4.177948 -4.17656 -4.1767612 -4.1789074][-4.1498561 -4.13324 -4.1367092 -4.1640387 -4.2038679 -4.2294922 -4.22773 -4.2008471 -4.1763391 -4.1588907 -4.1640191 -4.1807752 -4.1936927 -4.2021852 -4.2018228][-4.133419 -4.1251059 -4.1454244 -4.179255 -4.2093549 -4.2145023 -4.1941752 -4.1559644 -4.1284184 -4.1233397 -4.1543064 -4.1956549 -4.2225103 -4.2333145 -4.2288113][-4.1257877 -4.1273942 -4.1565185 -4.1836667 -4.1915579 -4.168159 -4.1219006 -4.0693445 -4.0502424 -4.077467 -4.1409683 -4.1989636 -4.2305737 -4.2388711 -4.2322922][-4.1240864 -4.1337814 -4.1646705 -4.1801686 -4.1644459 -4.1162529 -4.0486679 -3.9854753 -3.9807234 -4.0397983 -4.1206522 -4.180254 -4.2126226 -4.22338 -4.2206621][-4.1264105 -4.1474175 -4.1790385 -4.1884513 -4.16192 -4.1039491 -4.0337648 -3.97702 -3.983469 -4.0469694 -4.121655 -4.1724386 -4.2028432 -4.2157331 -4.2150283][-4.1497931 -4.1787353 -4.2083211 -4.2144132 -4.1889014 -4.1381545 -4.0806527 -4.0356922 -4.0380526 -4.0868807 -4.146183 -4.1847987 -4.2077217 -4.2172408 -4.2169375][-4.1987138 -4.2233624 -4.2405 -4.2374835 -4.2138739 -4.17573 -4.1353407 -4.1042628 -4.1072807 -4.1449094 -4.1881027 -4.2119188 -4.2220793 -4.223156 -4.2183237][-4.2255373 -4.2392378 -4.2455425 -4.2375507 -4.2180681 -4.1932149 -4.1715746 -4.1597443 -4.1702003 -4.2009435 -4.2269444 -4.2339163 -4.2304077 -4.2237244 -4.2154851][-4.2308664 -4.2318716 -4.2295246 -4.2203813 -4.2102838 -4.2004576 -4.195282 -4.1984673 -4.2121072 -4.2311077 -4.239141 -4.2322969 -4.2223921 -4.2175684 -4.2169418][-4.2310424 -4.2231083 -4.2162967 -4.2103171 -4.2119126 -4.2158785 -4.2211895 -4.2275038 -4.2343349 -4.240262 -4.2380104 -4.2283039 -4.2219248 -4.2243829 -4.2324033][-4.2124114 -4.1944275 -4.1862211 -4.1918669 -4.2113757 -4.2317219 -4.245636 -4.2488394 -4.24338 -4.2386246 -4.2331591 -4.2280679 -4.2300997 -4.240335 -4.2521453]]...]
INFO - root - 2017-12-07 19:21:49.417437: step 37510, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 81h:46m:38s remains)
INFO - root - 2017-12-07 19:21:59.112481: step 37520, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 79h:44m:36s remains)
INFO - root - 2017-12-07 19:22:08.730206: step 37530, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 78h:18m:12s remains)
INFO - root - 2017-12-07 19:22:18.521080: step 37540, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 83h:01m:26s remains)
INFO - root - 2017-12-07 19:22:28.270114: step 37550, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 80h:11m:39s remains)
INFO - root - 2017-12-07 19:22:38.000474: step 37560, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 81h:27m:34s remains)
INFO - root - 2017-12-07 19:22:47.496435: step 37570, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.947 sec/batch; 77h:32m:45s remains)
INFO - root - 2017-12-07 19:22:57.273164: step 37580, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 80h:59m:20s remains)
INFO - root - 2017-12-07 19:23:06.882746: step 37590, loss = 2.07, batch loss = 2.01 (7.6 examples/sec; 1.058 sec/batch; 86h:37m:49s remains)
INFO - root - 2017-12-07 19:23:16.630365: step 37600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 77h:21m:15s remains)
2017-12-07 19:23:17.594023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2473884 -4.2591057 -4.2457433 -4.2095037 -4.1743622 -4.1532245 -4.1622038 -4.1869063 -4.2235327 -4.2559514 -4.2748284 -4.2909684 -4.3088465 -4.3287258 -4.34839][-4.2323537 -4.2401714 -4.2170696 -4.1742692 -4.1389632 -4.1297483 -4.1510787 -4.1835203 -4.2208786 -4.2486515 -4.2651391 -4.2821131 -4.3035512 -4.3276081 -4.3493476][-4.2555661 -4.2539377 -4.2217946 -4.1716442 -4.1269536 -4.1127772 -4.130528 -4.1625075 -4.1982718 -4.2287812 -4.2501407 -4.2722621 -4.2984476 -4.326056 -4.3490062][-4.2942696 -4.285923 -4.2514749 -4.1989727 -4.1408114 -4.1010842 -4.0994596 -4.1248293 -4.1617393 -4.2028074 -4.2356234 -4.2670512 -4.2988858 -4.3278046 -4.3500843][-4.3229485 -4.31158 -4.2800503 -4.2327104 -4.1673474 -4.1024885 -4.0794916 -4.0948057 -4.1331964 -4.1853967 -4.2290354 -4.2685318 -4.3035951 -4.3312488 -4.3521152][-4.3304348 -4.3196921 -4.2917442 -4.2525582 -4.1877823 -4.1152682 -4.08254 -4.0904655 -4.1303921 -4.1870136 -4.2338805 -4.2755861 -4.3095388 -4.3349628 -4.3539224][-4.3237343 -4.3131585 -4.2888737 -4.2575841 -4.2001023 -4.1354446 -4.1046157 -4.109405 -4.1473112 -4.2016182 -4.24445 -4.2819953 -4.3127742 -4.3368831 -4.3545327][-4.3146334 -4.3041277 -4.2840838 -4.2598648 -4.2089624 -4.1547713 -4.1308413 -4.1345286 -4.1699014 -4.2186565 -4.2540092 -4.2856841 -4.3129783 -4.3366108 -4.3538446][-4.2896814 -4.2794724 -4.2656302 -4.2494216 -4.2081084 -4.1662483 -4.1508522 -4.1537466 -4.1878767 -4.2305865 -4.2610025 -4.288713 -4.3127637 -4.3356571 -4.3529196][-4.2523251 -4.235858 -4.2201705 -4.2036648 -4.1701083 -4.1457758 -4.1442733 -4.1548476 -4.1936855 -4.2344379 -4.2646203 -4.2910166 -4.3129258 -4.3353438 -4.3528061][-4.2034659 -4.1751781 -4.1510739 -4.1248665 -4.0960989 -4.0964642 -4.1176186 -4.1454144 -4.1930156 -4.2354689 -4.2683311 -4.2942448 -4.3142166 -4.336071 -4.353313][-4.1551018 -4.1182923 -4.0856185 -4.0499821 -4.0263166 -4.0506639 -4.0952597 -4.1409545 -4.1939096 -4.2390156 -4.273562 -4.29911 -4.3170457 -4.3370266 -4.3537827][-4.1323667 -4.1027813 -4.0719228 -4.0337148 -4.0158415 -4.0512724 -4.1050406 -4.1567016 -4.2071557 -4.2493806 -4.2804804 -4.3038473 -4.3194885 -4.3377328 -4.3541441][-4.1472869 -4.132998 -4.1091928 -4.0751495 -4.0626049 -4.0916319 -4.1369662 -4.1806087 -4.2231474 -4.2583437 -4.2825694 -4.3036275 -4.3186951 -4.3370681 -4.3540244][-4.1805029 -4.18089 -4.162292 -4.132019 -4.1155844 -4.1276774 -4.1608176 -4.1971807 -4.2354074 -4.2649627 -4.2828708 -4.3014455 -4.3154964 -4.3349981 -4.3537745]]...]
INFO - root - 2017-12-07 19:23:27.132437: step 37610, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 79h:34m:30s remains)
INFO - root - 2017-12-07 19:23:36.848892: step 37620, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 78h:20m:09s remains)
INFO - root - 2017-12-07 19:23:46.736152: step 37630, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 78h:18m:46s remains)
INFO - root - 2017-12-07 19:23:56.298345: step 37640, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 74h:08m:51s remains)
INFO - root - 2017-12-07 19:24:05.765560: step 37650, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.942 sec/batch; 77h:09m:53s remains)
INFO - root - 2017-12-07 19:24:15.453141: step 37660, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 82h:21m:54s remains)
INFO - root - 2017-12-07 19:24:25.052473: step 37670, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 81h:33m:55s remains)
INFO - root - 2017-12-07 19:24:34.657950: step 37680, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 76h:15m:07s remains)
INFO - root - 2017-12-07 19:24:44.332176: step 37690, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 80h:14m:38s remains)
INFO - root - 2017-12-07 19:24:54.039268: step 37700, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.975 sec/batch; 79h:50m:22s remains)
2017-12-07 19:24:54.978200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2505054 -4.2578139 -4.267427 -4.2798977 -4.2885208 -4.2902703 -4.2881541 -4.2847753 -4.2713752 -4.2501311 -4.2243004 -4.179986 -4.1285877 -4.106535 -4.1176057][-4.2492089 -4.2560558 -4.2685456 -4.2837648 -4.2928023 -4.2947021 -4.2952271 -4.291667 -4.27415 -4.2506895 -4.2286596 -4.1910028 -4.1492276 -4.1352978 -4.1501527][-4.2533445 -4.2551737 -4.270287 -4.2898893 -4.301753 -4.3034739 -4.3022203 -4.2939873 -4.271904 -4.25291 -4.243865 -4.2234387 -4.1966705 -4.1878066 -4.1981759][-4.2361922 -4.234735 -4.2526612 -4.2750645 -4.2841773 -4.2803965 -4.2707844 -4.2567816 -4.2361927 -4.2293577 -4.237885 -4.2383981 -4.2273574 -4.2210031 -4.2268081][-4.2053118 -4.2060828 -4.2225404 -4.2398214 -4.2405753 -4.22453 -4.1997628 -4.1774707 -4.1643453 -4.1750255 -4.19964 -4.2159896 -4.2203665 -4.220819 -4.2257323][-4.1704726 -4.1785874 -4.1898017 -4.1981406 -4.1877527 -4.1558657 -4.1122918 -4.0750966 -4.0697017 -4.1010113 -4.1418929 -4.1691089 -4.1827788 -4.185462 -4.1932969][-4.1252575 -4.1379356 -4.1435847 -4.1432705 -4.1239252 -4.0734811 -4.0014071 -3.94062 -3.9438963 -4.0045958 -4.0694323 -4.1100292 -4.1310315 -4.1370306 -4.1473632][-4.0878887 -4.1005898 -4.1009679 -4.0962639 -4.0719032 -4.0093036 -3.90836 -3.8239679 -3.8347645 -3.9236715 -4.0113964 -4.0669069 -4.095427 -4.1056247 -4.1162553][-4.1021485 -4.1089897 -4.1100364 -4.1109157 -4.0932088 -4.0357952 -3.9402192 -3.8666041 -3.8846624 -3.9705331 -4.0523934 -4.1026144 -4.1262445 -4.1327658 -4.1345296][-4.1398816 -4.1446118 -4.1507134 -4.1572223 -4.1473703 -4.1065168 -4.0433593 -4.0057268 -4.0287971 -4.0876765 -4.1406755 -4.1685715 -4.1747112 -4.1720066 -4.1623907][-4.19298 -4.1981931 -4.2066431 -4.2153716 -4.2106462 -4.1848116 -4.1484613 -4.1356859 -4.1568475 -4.1887741 -4.2102957 -4.2094145 -4.1973419 -4.1860371 -4.1678004][-4.2410812 -4.2502532 -4.2569842 -4.2641606 -4.2620072 -4.2497854 -4.2345862 -4.23495 -4.2506561 -4.2600136 -4.2556729 -4.2326674 -4.2053466 -4.1845875 -4.1652517][-4.2732115 -4.2852883 -4.2889891 -4.2921929 -4.2895088 -4.2847667 -4.2815289 -4.2897239 -4.3038697 -4.3024316 -4.2835469 -4.2462225 -4.20661 -4.1768074 -4.1593847][-4.2756391 -4.2913618 -4.29492 -4.2974982 -4.2974524 -4.29686 -4.2963095 -4.3036442 -4.3140779 -4.3076868 -4.2841754 -4.2394519 -4.1936369 -4.1668296 -4.1582718][-4.2728696 -4.2899756 -4.2919135 -4.2928271 -4.2939553 -4.2953873 -4.2951035 -4.3000622 -4.305675 -4.2969275 -4.2715683 -4.2242217 -4.1769872 -4.156189 -4.1603255]]...]
INFO - root - 2017-12-07 19:25:04.662657: step 37710, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 72h:16m:28s remains)
INFO - root - 2017-12-07 19:25:14.447184: step 37720, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 80h:56m:14s remains)
INFO - root - 2017-12-07 19:25:24.208971: step 37730, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.014 sec/batch; 83h:01m:52s remains)
INFO - root - 2017-12-07 19:25:33.844560: step 37740, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 77h:09m:01s remains)
INFO - root - 2017-12-07 19:25:43.530227: step 37750, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 79h:17m:39s remains)
INFO - root - 2017-12-07 19:25:53.067276: step 37760, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 76h:17m:38s remains)
INFO - root - 2017-12-07 19:26:02.640006: step 37770, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 78h:29m:48s remains)
INFO - root - 2017-12-07 19:26:12.230699: step 37780, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 78h:05m:46s remains)
INFO - root - 2017-12-07 19:26:21.834379: step 37790, loss = 2.08, batch loss = 2.02 (7.6 examples/sec; 1.047 sec/batch; 85h:43m:14s remains)
INFO - root - 2017-12-07 19:26:31.465340: step 37800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 77h:55m:12s remains)
2017-12-07 19:26:32.422683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2252154 -4.2060046 -4.2107391 -4.2285657 -4.2458429 -4.2549014 -4.2638049 -4.2617955 -4.254405 -4.2586 -4.2681971 -4.2779222 -4.28741 -4.2900248 -4.2869196][-4.1796761 -4.1500878 -4.1607494 -4.1881838 -4.2111311 -4.22035 -4.2249956 -4.2170153 -4.2037597 -4.2089958 -4.2268209 -4.2471752 -4.2671194 -4.277276 -4.2763057][-4.1353416 -4.1020484 -4.1265125 -4.16202 -4.1824269 -4.1839252 -4.1657505 -4.1356072 -4.1192183 -4.1332011 -4.162466 -4.1977921 -4.2324581 -4.254231 -4.2575774][-4.1150222 -4.0782723 -4.1057873 -4.1428938 -4.1528044 -4.1308522 -4.0760603 -4.0119514 -3.9897895 -4.0160341 -4.0628057 -4.1206236 -4.1757007 -4.2124271 -4.2289686][-4.14616 -4.1035767 -4.1089506 -4.1270313 -4.1155047 -4.0672965 -3.9737754 -3.8724623 -3.8429532 -3.8844705 -3.95674 -4.0474668 -4.1288948 -4.18246 -4.21041][-4.2048731 -4.1607771 -4.1351891 -4.1243052 -4.0926485 -4.0279341 -3.9136405 -3.7809319 -3.7320204 -3.7797871 -3.8792648 -4.001205 -4.1041789 -4.1702156 -4.2046247][-4.2536464 -4.215405 -4.1820369 -4.155889 -4.1192989 -4.053185 -3.9404325 -3.801986 -3.743165 -3.7788079 -3.8788733 -4.0008373 -4.0992002 -4.1580019 -4.1886435][-4.2887244 -4.2606931 -4.2321415 -4.210237 -4.1799927 -4.1257458 -4.0263515 -3.9110663 -3.8607397 -3.8782611 -3.9465871 -4.0337095 -4.0970421 -4.1280408 -4.151186][-4.3232164 -4.304513 -4.2806125 -4.2621846 -4.2408462 -4.2049861 -4.1335716 -4.0537639 -4.0263686 -4.0348468 -4.061543 -4.0992022 -4.11296 -4.1095562 -4.1221361][-4.3511715 -4.3402762 -4.3196821 -4.3075948 -4.2946091 -4.2712197 -4.2286911 -4.1855187 -4.1725526 -4.1738024 -4.16457 -4.1607814 -4.1411519 -4.1141768 -4.1123219][-4.3617053 -4.354404 -4.3373513 -4.3267517 -4.3152552 -4.2970419 -4.2699165 -4.2483091 -4.2453208 -4.2388172 -4.2150755 -4.1953759 -4.1686668 -4.1355376 -4.121069][-4.3537254 -4.3474069 -4.3330908 -4.3245845 -4.315105 -4.2994385 -4.2779522 -4.2661691 -4.2661867 -4.2525854 -4.2274089 -4.2087522 -4.1898623 -4.1601949 -4.1394343][-4.3394394 -4.3361635 -4.3261189 -4.3195357 -4.3110738 -4.2984128 -4.2821341 -4.2787905 -4.2809663 -4.2689724 -4.24909 -4.2391262 -4.2277875 -4.2054515 -4.1861258][-4.3327656 -4.330884 -4.3268285 -4.3225918 -4.3171096 -4.3074975 -4.2977018 -4.2983189 -4.3020964 -4.2972841 -4.2873063 -4.2837706 -4.278213 -4.2645969 -4.2513194][-4.3209109 -4.3218842 -4.3223243 -4.3221159 -4.31887 -4.3108306 -4.3056989 -4.3071594 -4.31089 -4.3102946 -4.3087673 -4.3079324 -4.3042226 -4.2985034 -4.2925429]]...]
INFO - root - 2017-12-07 19:26:42.256621: step 37810, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 79h:19m:49s remains)
INFO - root - 2017-12-07 19:26:52.013197: step 37820, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 80h:09m:10s remains)
INFO - root - 2017-12-07 19:27:01.651223: step 37830, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 76h:31m:56s remains)
INFO - root - 2017-12-07 19:27:11.309926: step 37840, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 77h:57m:54s remains)
INFO - root - 2017-12-07 19:27:20.995734: step 37850, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 82h:53m:45s remains)
INFO - root - 2017-12-07 19:27:30.668302: step 37860, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 80h:09m:22s remains)
INFO - root - 2017-12-07 19:27:40.364058: step 37870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 79h:19m:20s remains)
INFO - root - 2017-12-07 19:27:49.968204: step 37880, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 68h:46m:25s remains)
INFO - root - 2017-12-07 19:27:59.608489: step 37890, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 78h:17m:19s remains)
INFO - root - 2017-12-07 19:28:09.226556: step 37900, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 77h:36m:42s remains)
2017-12-07 19:28:10.132410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981358 -4.2903609 -4.2857366 -4.282907 -4.2743196 -4.2594495 -4.2358308 -4.2068281 -4.184546 -4.190474 -4.2072582 -4.2227993 -4.2450466 -4.2774739 -4.3050804][-4.3237906 -4.3174019 -4.3135071 -4.3021283 -4.27994 -4.2613492 -4.248467 -4.2258663 -4.2066693 -4.2134848 -4.2285838 -4.2411976 -4.2580752 -4.2840395 -4.3032517][-4.3313718 -4.3223104 -4.3153362 -4.29901 -4.2709761 -4.2532787 -4.2488661 -4.2297983 -4.2121358 -4.2212749 -4.2390394 -4.2528253 -4.2686572 -4.2907867 -4.3032274][-4.318121 -4.3102441 -4.3041396 -4.288363 -4.2596269 -4.2423673 -4.2370205 -4.2120638 -4.1981645 -4.2172523 -4.2428536 -4.261014 -4.2791061 -4.2995653 -4.307652][-4.2772689 -4.2727747 -4.2778435 -4.2723188 -4.2473564 -4.2263274 -4.2061739 -4.1666083 -4.1518135 -4.1847072 -4.2235656 -4.2520313 -4.2762203 -4.2984748 -4.3090577][-4.2232003 -4.221931 -4.2345314 -4.2366676 -4.2126083 -4.1830869 -4.1389308 -4.0710568 -4.061769 -4.1257887 -4.1861472 -4.2297382 -4.2669458 -4.2968636 -4.3111358][-4.2054353 -4.206398 -4.2163925 -4.2122116 -4.1792865 -4.1307077 -4.045186 -3.9303617 -3.9325118 -4.0471125 -4.1416545 -4.2003417 -4.2496948 -4.2905259 -4.3103294][-4.2114429 -4.211307 -4.2161527 -4.2052884 -4.1649346 -4.0988951 -3.9775295 -3.8220909 -3.8319938 -3.9795721 -4.0947056 -4.1611691 -4.2226992 -4.2738109 -4.3008084][-4.2300467 -4.2284846 -4.2281928 -4.2168074 -4.1838503 -4.1268158 -4.0216355 -3.8936367 -3.8957818 -4.0020976 -4.0929666 -4.1505456 -4.2105737 -4.2613187 -4.2897072][-4.2422032 -4.2444377 -4.2479739 -4.2449389 -4.2279468 -4.1873169 -4.1108727 -4.0143585 -4.0000682 -4.0551558 -4.1135263 -4.1567512 -4.2096047 -4.255867 -4.2839885][-4.24786 -4.25653 -4.2655649 -4.2660031 -4.2574239 -4.2310505 -4.1766343 -4.0994258 -4.0692968 -4.0925574 -4.1305194 -4.1663513 -4.2179108 -4.2641506 -4.2904897][-4.2591796 -4.2700391 -4.2805452 -4.2784724 -4.2704549 -4.2542691 -4.2170453 -4.1577005 -4.1260824 -4.1347723 -4.1590934 -4.1884208 -4.2328448 -4.2740088 -4.2976503][-4.2765059 -4.2823462 -4.2873707 -4.2826214 -4.2727246 -4.2597246 -4.2333159 -4.1935649 -4.1703019 -4.1750441 -4.19068 -4.2132468 -4.2485027 -4.2825756 -4.3045235][-4.2840977 -4.285399 -4.2865062 -4.2820172 -4.272963 -4.2624125 -4.2467332 -4.2265449 -4.2178888 -4.2266436 -4.2362423 -4.2478132 -4.270288 -4.2965531 -4.3166337][-4.2963319 -4.289012 -4.2842574 -4.2782974 -4.2731361 -4.2670307 -4.2591004 -4.2520003 -4.2520814 -4.2613053 -4.2690425 -4.276947 -4.2927289 -4.3138671 -4.3321166]]...]
INFO - root - 2017-12-07 19:28:19.752442: step 37910, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.949 sec/batch; 77h:41m:37s remains)
INFO - root - 2017-12-07 19:28:29.551202: step 37920, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 81h:08m:55s remains)
INFO - root - 2017-12-07 19:28:39.312845: step 37930, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 78h:51m:29s remains)
INFO - root - 2017-12-07 19:28:49.011633: step 37940, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.994 sec/batch; 81h:21m:32s remains)
INFO - root - 2017-12-07 19:28:58.853002: step 37950, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 80h:05m:44s remains)
INFO - root - 2017-12-07 19:29:08.571423: step 37960, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 80h:55m:45s remains)
INFO - root - 2017-12-07 19:29:18.458260: step 37970, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.008 sec/batch; 82h:29m:41s remains)
INFO - root - 2017-12-07 19:29:27.979166: step 37980, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 77h:20m:25s remains)
INFO - root - 2017-12-07 19:29:37.648451: step 37990, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 76h:41m:53s remains)
INFO - root - 2017-12-07 19:29:47.264028: step 38000, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 78h:49m:33s remains)
2017-12-07 19:29:48.244778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3136973 -4.3065424 -4.2967229 -4.2884417 -4.2853608 -4.2891655 -4.29728 -4.3028274 -4.3082552 -4.3134074 -4.3189411 -4.3241453 -4.327857 -4.32992 -4.331336][-4.2939219 -4.2810636 -4.2672887 -4.2565675 -4.2527823 -4.2577267 -4.2663279 -4.2711072 -4.2785096 -4.2857318 -4.2928529 -4.299108 -4.3049545 -4.3091063 -4.3119245][-4.2580791 -4.2423635 -4.2297726 -4.2237749 -4.2250195 -4.23256 -4.237946 -4.2374458 -4.2426591 -4.2500348 -4.257266 -4.2644138 -4.2727118 -4.2794971 -4.2846251][-4.2190428 -4.206121 -4.2001419 -4.2046471 -4.2119904 -4.2196064 -4.2187271 -4.2099648 -4.2097797 -4.216145 -4.2213941 -4.2266479 -4.2349811 -4.2427478 -4.2495933][-4.1831794 -4.174686 -4.1744394 -4.188704 -4.2029252 -4.2112288 -4.2032027 -4.1853132 -4.181941 -4.1878448 -4.1888223 -4.1881223 -4.1948729 -4.205483 -4.2154975][-4.1448469 -4.14163 -4.1454558 -4.16606 -4.1828327 -4.1873732 -4.17114 -4.1448178 -4.1431942 -4.1516447 -4.1518459 -4.1512671 -4.1599922 -4.1734233 -4.1874108][-4.1076236 -4.1070476 -4.1122422 -4.1332335 -4.146811 -4.142848 -4.123333 -4.0973248 -4.0957823 -4.1033607 -4.10105 -4.1012225 -4.1156325 -4.1340857 -4.1474686][-4.0882549 -4.0851321 -4.0873904 -4.1010904 -4.1062822 -4.0946236 -4.0762792 -4.0569396 -4.0547495 -4.0548525 -4.0454788 -4.044137 -4.0613589 -4.0810461 -4.0844254][-4.1154466 -4.1089463 -4.1064558 -4.1139865 -4.115047 -4.1044321 -4.096674 -4.0882759 -4.0823708 -4.0658875 -4.0421834 -4.0349174 -4.0486164 -4.058291 -4.0443835][-4.15722 -4.146769 -4.1432929 -4.1499062 -4.1559672 -4.1557617 -4.1596723 -4.1624069 -4.1538548 -4.1256046 -4.0929055 -4.0803828 -4.0855145 -4.082871 -4.0559163][-4.1763272 -4.1599383 -4.1541519 -4.1625433 -4.1748824 -4.1842337 -4.1940327 -4.1999154 -4.1909227 -4.1651039 -4.1404686 -4.1325669 -4.1326241 -4.122354 -4.0966196][-4.1749635 -4.1571412 -4.1517906 -4.1615252 -4.1748405 -4.1838889 -4.1932168 -4.198216 -4.1925197 -4.1786423 -4.1699076 -4.1701913 -4.1684189 -4.15569 -4.1372857][-4.1826839 -4.1702056 -4.169385 -4.1777062 -4.1880913 -4.1933007 -4.1974864 -4.1999769 -4.1980953 -4.1954389 -4.1981554 -4.2032328 -4.2023697 -4.1937423 -4.1832242][-4.2335887 -4.2272825 -4.227335 -4.2305837 -4.2354951 -4.2375312 -4.237412 -4.2360454 -4.23609 -4.2392535 -4.2438517 -4.2469296 -4.2469473 -4.24333 -4.2386107][-4.2882433 -4.2854719 -4.2848597 -4.2849169 -4.2858119 -4.2857571 -4.2846932 -4.2831554 -4.285316 -4.2898693 -4.29189 -4.2910738 -4.2897606 -4.2884164 -4.2865944]]...]
INFO - root - 2017-12-07 19:29:57.952041: step 38010, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 78h:39m:43s remains)
INFO - root - 2017-12-07 19:30:07.690177: step 38020, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 77h:21m:15s remains)
INFO - root - 2017-12-07 19:30:17.231504: step 38030, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.984 sec/batch; 80h:30m:52s remains)
INFO - root - 2017-12-07 19:30:26.799969: step 38040, loss = 2.12, batch loss = 2.06 (8.4 examples/sec; 0.956 sec/batch; 78h:11m:37s remains)
INFO - root - 2017-12-07 19:30:36.638743: step 38050, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 79h:30m:00s remains)
INFO - root - 2017-12-07 19:30:46.269899: step 38060, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 78h:43m:34s remains)
INFO - root - 2017-12-07 19:30:55.849104: step 38070, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 80h:10m:33s remains)
INFO - root - 2017-12-07 19:31:05.473984: step 38080, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.006 sec/batch; 82h:18m:44s remains)
INFO - root - 2017-12-07 19:31:15.086412: step 38090, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.925 sec/batch; 75h:37m:54s remains)
INFO - root - 2017-12-07 19:31:25.014767: step 38100, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 81h:48m:55s remains)
2017-12-07 19:31:26.020198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3436689 -4.3429227 -4.3418736 -4.342792 -4.34535 -4.3461304 -4.3413148 -4.3324761 -4.3230605 -4.31443 -4.313118 -4.3159165 -4.3178353 -4.3166637 -4.3150582][-4.3167157 -4.3133335 -4.3119268 -4.3147225 -4.3191304 -4.3173475 -4.3066468 -4.2945123 -4.2818289 -4.2703037 -4.2683845 -4.2727518 -4.2757792 -4.2777495 -4.2787266][-4.29101 -4.2844 -4.2805996 -4.2803164 -4.2809067 -4.2729859 -4.2549195 -4.2414818 -4.2330356 -4.2234 -4.2222185 -4.226346 -4.2303391 -4.2347059 -4.2373724][-4.2651615 -4.2552238 -4.2479315 -4.2402058 -4.2331681 -4.2119431 -4.1802549 -4.167201 -4.1708512 -4.1741943 -4.182374 -4.1875706 -4.1871328 -4.1927032 -4.1983275][-4.2375736 -4.225461 -4.2166524 -4.201045 -4.1798415 -4.1342626 -4.0770121 -4.0671415 -4.0988097 -4.1263018 -4.1485538 -4.1562357 -4.1559763 -4.16359 -4.1712193][-4.2141533 -4.1997032 -4.1862431 -4.159668 -4.1155467 -4.0323386 -3.9366868 -3.9402583 -4.02101 -4.0864167 -4.1215873 -4.125298 -4.1202435 -4.128787 -4.1408353][-4.2074513 -4.1896253 -4.1706314 -4.1294847 -4.0564361 -3.9294481 -3.79501 -3.8174908 -3.9526486 -4.0514855 -4.0963426 -4.0961533 -4.083602 -4.092308 -4.1110177][-4.2182803 -4.204443 -4.1915731 -4.154213 -4.0868573 -3.9710245 -3.8508656 -3.8692153 -3.9867327 -4.0683737 -4.1011677 -4.0982094 -4.0899525 -4.1014853 -4.1272483][-4.2362204 -4.2268438 -4.226738 -4.2070661 -4.1627226 -4.0818963 -3.9924874 -3.9965997 -4.0743208 -4.126368 -4.1438046 -4.1400352 -4.1334543 -4.1489005 -4.1820264][-4.2429581 -4.2352777 -4.2424636 -4.2379217 -4.2115545 -4.1557155 -4.0843182 -4.0777369 -4.1333323 -4.1669154 -4.1792989 -4.1783218 -4.1736007 -4.1902966 -4.2269988][-4.239181 -4.23342 -4.2398396 -4.2406898 -4.2249079 -4.1869221 -4.1289859 -4.1165419 -4.1591058 -4.1811333 -4.1938877 -4.1967554 -4.1965337 -4.2156348 -4.2489662][-4.2394037 -4.2358985 -4.2407336 -4.2374697 -4.2246847 -4.1986589 -4.1546392 -4.1401711 -4.1679869 -4.1821904 -4.196547 -4.2063742 -4.2127185 -4.2320094 -4.260016][-4.2477193 -4.2454324 -4.2497253 -4.245069 -4.2357564 -4.21774 -4.1881766 -4.17472 -4.1849465 -4.1900911 -4.2050519 -4.2232747 -4.2347174 -4.2497425 -4.2714925][-4.2575278 -4.2560878 -4.2593565 -4.2550187 -4.2475557 -4.237721 -4.2207828 -4.2131395 -4.2172279 -4.2158504 -4.2238054 -4.2432332 -4.2543392 -4.262723 -4.2731209][-4.2748308 -4.2681155 -4.2634048 -4.2540178 -4.2412505 -4.2335439 -4.2260218 -4.2250476 -4.2303996 -4.2293992 -4.2354193 -4.2530732 -4.2627954 -4.2668681 -4.2724528]]...]
INFO - root - 2017-12-07 19:31:35.490616: step 38110, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 78h:07m:14s remains)
INFO - root - 2017-12-07 19:31:45.252716: step 38120, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 78h:36m:17s remains)
INFO - root - 2017-12-07 19:31:54.941079: step 38130, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 80h:50m:10s remains)
INFO - root - 2017-12-07 19:32:04.612982: step 38140, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 78h:59m:33s remains)
INFO - root - 2017-12-07 19:32:14.352381: step 38150, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.930 sec/batch; 76h:03m:15s remains)
INFO - root - 2017-12-07 19:32:23.952245: step 38160, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 78h:17m:27s remains)
INFO - root - 2017-12-07 19:32:33.657226: step 38170, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 82h:59m:32s remains)
INFO - root - 2017-12-07 19:32:43.203827: step 38180, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 76h:33m:57s remains)
INFO - root - 2017-12-07 19:32:52.965699: step 38190, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.015 sec/batch; 82h:58m:41s remains)
INFO - root - 2017-12-07 19:33:02.698939: step 38200, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 74h:56m:54s remains)
2017-12-07 19:33:03.659912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3235145 -4.3250608 -4.3264828 -4.3250284 -4.3216033 -4.3179741 -4.3175082 -4.3181505 -4.314539 -4.3121209 -4.3090038 -4.3041735 -4.2995505 -4.3007851 -4.3101244][-4.3084779 -4.3101392 -4.3127203 -4.3111978 -4.3058877 -4.3007388 -4.2999353 -4.3017974 -4.2986641 -4.2953911 -4.2911425 -4.2836504 -4.2760067 -4.2777615 -4.2902174][-4.2891269 -4.2899017 -4.2912416 -4.2862372 -4.2763324 -4.2688942 -4.2665739 -4.2677507 -4.2675595 -4.2665586 -4.263948 -4.2560725 -4.2482109 -4.2516842 -4.2663994][-4.2615671 -4.2591534 -4.258862 -4.2486453 -4.2314262 -4.2148337 -4.2030621 -4.2010026 -4.2076836 -4.2149811 -4.218513 -4.2151561 -4.210887 -4.2188945 -4.2386909][-4.2262292 -4.2183251 -4.2140856 -4.1982393 -4.1729984 -4.1418123 -4.1133456 -4.1049342 -4.1237564 -4.1478043 -4.1606035 -4.1629143 -4.1614528 -4.1746893 -4.2019038][-4.1924415 -4.1767631 -4.1658077 -4.1446362 -4.1103749 -4.0604315 -4.0034652 -3.9792805 -4.0154672 -4.0717669 -4.1030526 -4.1108255 -4.1114087 -4.1301174 -4.1647615][-4.1744366 -4.14887 -4.1278524 -4.0984225 -4.0514588 -3.9779203 -3.8791246 -3.8129549 -3.8530426 -3.9483359 -4.0075159 -4.0310521 -4.0462232 -4.0802732 -4.1317039][-4.1752906 -4.1455154 -4.1182814 -4.0829873 -4.0288558 -3.9473057 -3.8254368 -3.7158396 -3.7292972 -3.8330884 -3.9065692 -3.9454663 -3.9773672 -4.0279436 -4.0984969][-4.20559 -4.1857066 -4.167275 -4.1404619 -4.0991864 -4.0366883 -3.9400115 -3.8410518 -3.8166916 -3.8660941 -3.9113731 -3.9417052 -3.9693067 -4.020421 -4.0937848][-4.2391739 -4.2329159 -4.2291794 -4.217875 -4.1943374 -4.1556993 -4.09468 -4.0293283 -3.9948936 -4.0006428 -4.0155096 -4.0325627 -4.0521994 -4.0880427 -4.1406617][-4.2697253 -4.2707882 -4.27464 -4.2750959 -4.2671113 -4.2477479 -4.2156253 -4.1790385 -4.1497054 -4.1382127 -4.135139 -4.1409926 -4.1534042 -4.1784296 -4.2113862][-4.3042722 -4.3073406 -4.3125973 -4.315485 -4.31447 -4.307445 -4.2930245 -4.27622 -4.2590194 -4.2468061 -4.2379956 -4.2358685 -4.2428656 -4.2608972 -4.2789412][-4.32585 -4.3281565 -4.3324766 -4.3366051 -4.3391962 -4.3387375 -4.3325315 -4.3244748 -4.314383 -4.305603 -4.2989759 -4.2955394 -4.2980046 -4.3100142 -4.3209834][-4.339685 -4.3406096 -4.3434453 -4.3473921 -4.3516951 -4.3542695 -4.3529243 -4.3500843 -4.3435822 -4.3360295 -4.3300228 -4.3257289 -4.3233166 -4.3283381 -4.3349771][-4.347271 -4.3462939 -4.3467703 -4.3479843 -4.3501582 -4.3525238 -4.354125 -4.354744 -4.3528123 -4.3487706 -4.3439522 -4.3401103 -4.3373747 -4.338943 -4.34301]]...]
INFO - root - 2017-12-07 19:33:13.353952: step 38210, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 77h:52m:38s remains)
INFO - root - 2017-12-07 19:33:23.075837: step 38220, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 79h:26m:59s remains)
INFO - root - 2017-12-07 19:33:32.494836: step 38230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 74h:50m:19s remains)
INFO - root - 2017-12-07 19:33:42.100733: step 38240, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 80h:17m:14s remains)
INFO - root - 2017-12-07 19:33:51.963046: step 38250, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.014 sec/batch; 82h:54m:46s remains)
INFO - root - 2017-12-07 19:34:01.577843: step 38260, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 75h:31m:58s remains)
INFO - root - 2017-12-07 19:34:11.289438: step 38270, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 78h:01m:20s remains)
INFO - root - 2017-12-07 19:34:21.090901: step 38280, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 81h:53m:21s remains)
INFO - root - 2017-12-07 19:34:30.888349: step 38290, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.006 sec/batch; 82h:15m:07s remains)
INFO - root - 2017-12-07 19:34:40.413619: step 38300, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 76h:29m:04s remains)
2017-12-07 19:34:41.377203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3026519 -4.2945747 -4.2762284 -4.2563004 -4.2464409 -4.2388539 -4.2297726 -4.2261996 -4.2169976 -4.1842093 -4.149415 -4.135323 -4.1346021 -4.14104 -4.1470022][-4.2981853 -4.2916765 -4.274332 -4.2553744 -4.2471414 -4.2416248 -4.2350683 -4.23689 -4.2364478 -4.2067561 -4.1697264 -4.1552687 -4.157146 -4.1605606 -4.1649885][-4.2886176 -4.2801657 -4.2652369 -4.2491922 -4.2433534 -4.2392473 -4.2302432 -4.2301216 -4.2335672 -4.2083936 -4.175302 -4.1647487 -4.1725407 -4.1778131 -4.1845107][-4.2683764 -4.2611794 -4.248426 -4.2325048 -4.2216005 -4.2088661 -4.1888151 -4.1795831 -4.1837249 -4.1726561 -4.1546807 -4.156755 -4.1745191 -4.1879349 -4.2016187][-4.2512646 -4.2415686 -4.2238417 -4.1960392 -4.1666927 -4.1393495 -4.1065774 -4.0882931 -4.0966716 -4.1086235 -4.12054 -4.1451259 -4.1714268 -4.1905618 -4.2098775][-4.2053528 -4.19256 -4.1695995 -4.1291881 -4.079381 -4.0347123 -3.9885838 -3.9679246 -3.9963429 -4.0468073 -4.098412 -4.1510415 -4.1867466 -4.2080531 -4.2269478][-4.1363511 -4.1209841 -4.0995107 -4.0589213 -4.0028305 -3.9491591 -3.8977559 -3.8875329 -3.9511967 -4.040153 -4.1173368 -4.1813631 -4.216476 -4.2314658 -4.2451973][-4.0810971 -4.0737743 -4.0657382 -4.0416327 -4.0047803 -3.9733186 -3.9443111 -3.9513564 -4.0166707 -4.0931735 -4.1554961 -4.2047386 -4.2295556 -4.2399268 -4.2504086][-4.0800381 -4.0896544 -4.0991993 -4.0933805 -4.0769773 -4.0696478 -4.0633726 -4.0711794 -4.1105566 -4.1501932 -4.1776037 -4.2036061 -4.216888 -4.2268028 -4.2381153][-4.1151309 -4.1330066 -4.1481242 -4.14937 -4.1399646 -4.1398239 -4.1435256 -4.1507878 -4.1693492 -4.1783776 -4.1751251 -4.1792192 -4.1842155 -4.1981049 -4.2169824][-4.1422687 -4.1591134 -4.1725817 -4.1763978 -4.1729755 -4.17679 -4.18437 -4.189589 -4.1944079 -4.182663 -4.1597228 -4.1528192 -4.1575451 -4.1767883 -4.2030711][-4.1640539 -4.1774621 -4.188457 -4.1917796 -4.1880236 -4.18781 -4.1916327 -4.191689 -4.1860805 -4.1675792 -4.1473022 -4.14838 -4.1636953 -4.1885247 -4.2154713][-4.1891971 -4.1925774 -4.1973238 -4.1964412 -4.1888437 -4.1849904 -4.1839371 -4.1806169 -4.1741462 -4.1622024 -4.154717 -4.1636963 -4.1827593 -4.2066469 -4.2298689][-4.2055354 -4.2012825 -4.2019324 -4.2010937 -4.1948581 -4.189611 -4.1863065 -4.1836729 -4.1800494 -4.1745625 -4.1718292 -4.1759958 -4.1851239 -4.1963639 -4.2115703][-4.2016053 -4.1950488 -4.1911049 -4.1934543 -4.1949983 -4.1936679 -4.1903419 -4.1864591 -4.18303 -4.1789412 -4.175921 -4.1709294 -4.1657181 -4.1653004 -4.1721382]]...]
INFO - root - 2017-12-07 19:34:51.166484: step 38310, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 79h:26m:23s remains)
INFO - root - 2017-12-07 19:35:00.930462: step 38320, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 78h:18m:28s remains)
INFO - root - 2017-12-07 19:35:10.584063: step 38330, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 78h:55m:11s remains)
INFO - root - 2017-12-07 19:35:20.268794: step 38340, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.991 sec/batch; 80h:56m:59s remains)
INFO - root - 2017-12-07 19:35:30.044310: step 38350, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 78h:06m:02s remains)
INFO - root - 2017-12-07 19:35:39.716899: step 38360, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 80h:15m:32s remains)
INFO - root - 2017-12-07 19:35:49.292398: step 38370, loss = 2.10, batch loss = 2.05 (8.8 examples/sec; 0.908 sec/batch; 74h:09m:11s remains)
INFO - root - 2017-12-07 19:35:58.929577: step 38380, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 78h:09m:36s remains)
INFO - root - 2017-12-07 19:36:08.610005: step 38390, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.035 sec/batch; 84h:31m:06s remains)
INFO - root - 2017-12-07 19:36:18.243497: step 38400, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 81h:40m:06s remains)
2017-12-07 19:36:19.280072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2229414 -4.2224984 -4.2293282 -4.234983 -4.24047 -4.2482924 -4.2585912 -4.2687535 -4.279397 -4.2765918 -4.2483578 -4.1906738 -4.1297636 -4.0842237 -4.0512547][-4.2295923 -4.225637 -4.2233109 -4.2207937 -4.2237043 -4.2371063 -4.2586508 -4.2785678 -4.2934694 -4.2951794 -4.2768416 -4.2348118 -4.1862493 -4.1494379 -4.1255417][-4.2562733 -4.2482028 -4.2346563 -4.2209048 -4.2171016 -4.2282276 -4.2519345 -4.2760067 -4.2943773 -4.30307 -4.2980652 -4.2745142 -4.2422118 -4.2136345 -4.1953206][-4.2927527 -4.2796383 -4.2539396 -4.2287378 -4.2172852 -4.2233486 -4.2437429 -4.2701325 -4.2935281 -4.3081279 -4.3129959 -4.3051643 -4.2861147 -4.2608995 -4.2350316][-4.3089094 -4.2947869 -4.260561 -4.2237663 -4.2025595 -4.2013216 -4.2160258 -4.243042 -4.2736444 -4.2981076 -4.3121638 -4.3168621 -4.3131552 -4.2971926 -4.2667823][-4.2974348 -4.2818675 -4.2412558 -4.1925731 -4.1593041 -4.1455979 -4.1533422 -4.1838703 -4.2254567 -4.2639985 -4.2936783 -4.3108191 -4.3195167 -4.3157172 -4.2913218][-4.2786307 -4.256382 -4.2044611 -4.1395879 -4.0869646 -4.0523229 -4.0516667 -4.0952482 -4.1575923 -4.2149625 -4.26009 -4.2909837 -4.3128233 -4.323173 -4.3114529][-4.2757483 -4.2430887 -4.1815691 -4.1066189 -4.0378108 -3.9827943 -3.9669895 -4.0139933 -4.0929947 -4.16678 -4.2247934 -4.2668619 -4.2994156 -4.3210239 -4.3220186][-4.2858062 -4.2504649 -4.1906147 -4.1202679 -4.0550337 -3.9998713 -3.9778695 -4.0144506 -4.0864024 -4.1545172 -4.2094164 -4.2522984 -4.2886639 -4.313715 -4.321475][-4.2956142 -4.267086 -4.2200866 -4.1660519 -4.119873 -4.0872054 -4.0766058 -4.1004391 -4.1474352 -4.1897144 -4.2266407 -4.2599683 -4.2901583 -4.3102369 -4.3167448][-4.301887 -4.2796383 -4.2465324 -4.2109118 -4.18438 -4.1729441 -4.1774707 -4.1968813 -4.2242894 -4.2443542 -4.2625513 -4.2831483 -4.3027563 -4.3152943 -4.3183622][-4.3081055 -4.2908988 -4.2696486 -4.2492728 -4.2352791 -4.2315683 -4.2384205 -4.2543712 -4.2716994 -4.2831464 -4.2924018 -4.3038473 -4.3144903 -4.3212461 -4.3243842][-4.3154988 -4.3006 -4.2850733 -4.272409 -4.2644367 -4.2620244 -4.266264 -4.275394 -4.2866635 -4.2960858 -4.30431 -4.3129592 -4.3197103 -4.3237944 -4.3282957][-4.3229475 -4.310987 -4.2981763 -4.2870016 -4.2789989 -4.274704 -4.2736773 -4.2763495 -4.2814546 -4.2876019 -4.2948561 -4.304584 -4.313921 -4.3214545 -4.3281255][-4.3287935 -4.3208151 -4.3120146 -4.3039045 -4.2975054 -4.2930765 -4.2910061 -4.2912841 -4.2934 -4.2964783 -4.30078 -4.3066506 -4.3132243 -4.3205447 -4.3266263]]...]
INFO - root - 2017-12-07 19:36:28.942007: step 38410, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 79h:24m:37s remains)
INFO - root - 2017-12-07 19:36:38.594719: step 38420, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 78h:22m:29s remains)
INFO - root - 2017-12-07 19:36:48.279773: step 38430, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 74h:56m:40s remains)
INFO - root - 2017-12-07 19:36:57.976320: step 38440, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 78h:11m:28s remains)
INFO - root - 2017-12-07 19:37:07.651890: step 38450, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 79h:26m:05s remains)
INFO - root - 2017-12-07 19:37:17.369615: step 38460, loss = 2.10, batch loss = 2.05 (7.6 examples/sec; 1.048 sec/batch; 85h:35m:58s remains)
INFO - root - 2017-12-07 19:37:27.047683: step 38470, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 77h:08m:50s remains)
INFO - root - 2017-12-07 19:37:36.763168: step 38480, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 81h:13m:13s remains)
INFO - root - 2017-12-07 19:37:46.471833: step 38490, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 81h:02m:40s remains)
INFO - root - 2017-12-07 19:37:56.262071: step 38500, loss = 2.09, batch loss = 2.04 (7.8 examples/sec; 1.029 sec/batch; 84h:02m:47s remains)
2017-12-07 19:37:57.255554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3128161 -4.3200903 -4.3285456 -4.3329182 -4.3303795 -4.3220005 -4.3131232 -4.3123679 -4.3172879 -4.3234234 -4.3298044 -4.3357258 -4.3409038 -4.34546 -4.3487172][-4.277915 -4.2863717 -4.2942877 -4.2990007 -4.2942095 -4.2830153 -4.2751455 -4.278161 -4.2857008 -4.2940049 -4.3023763 -4.3115392 -4.3225455 -4.3331676 -4.3406172][-4.22555 -4.233644 -4.2423463 -4.2503176 -4.2450476 -4.2360392 -4.2347417 -4.24288 -4.25141 -4.2561493 -4.26433 -4.2783575 -4.2958894 -4.31316 -4.3268795][-4.17275 -4.1802192 -4.1940446 -4.2066379 -4.2013583 -4.1937847 -4.1961408 -4.2086158 -4.2160039 -4.2163558 -4.2237363 -4.2407641 -4.2624865 -4.2861271 -4.3061438][-4.1468945 -4.1465964 -4.1605663 -4.1736941 -4.1674404 -4.15558 -4.1513376 -4.1647234 -4.1729937 -4.1715546 -4.1786375 -4.1949339 -4.2187033 -4.2501569 -4.2785807][-4.1540666 -4.1426592 -4.1469646 -4.1521912 -4.1361618 -4.1059284 -4.0793271 -4.0912414 -4.1071982 -4.1114268 -4.1181192 -4.1336269 -4.1634946 -4.206615 -4.2486563][-4.1810412 -4.1664281 -4.1595678 -4.1454053 -4.1029778 -4.0368004 -3.9767976 -3.986587 -4.0244651 -4.0483003 -4.0615191 -4.08179 -4.1212134 -4.1776319 -4.2297831][-4.2121482 -4.1993632 -4.1874084 -4.1571918 -4.0849071 -3.9867814 -3.9011159 -3.9076235 -3.9599986 -4.0036893 -4.0309734 -4.058682 -4.1060886 -4.167542 -4.2230458][-4.225009 -4.2141185 -4.2037182 -4.1712918 -4.087955 -3.9855475 -3.9021723 -3.9136877 -3.9674187 -4.0180073 -4.0550232 -4.0873508 -4.1313524 -4.1844563 -4.2337427][-4.2259121 -4.2183414 -4.2110505 -4.1842847 -4.1151195 -4.0405927 -3.9826455 -3.9936025 -4.0365472 -4.0809264 -4.1151705 -4.1443849 -4.1818147 -4.2220182 -4.2595305][-4.2269406 -4.2261782 -4.2263265 -4.2090969 -4.1595292 -4.1119461 -4.0745173 -4.0843682 -4.110312 -4.1443815 -4.1675458 -4.1884952 -4.2186375 -4.2527905 -4.2809649][-4.2355628 -4.2411966 -4.2453794 -4.232327 -4.1985903 -4.1658325 -4.1403871 -4.1497135 -4.1666803 -4.18872 -4.1990352 -4.2093668 -4.2370191 -4.2692356 -4.2927928][-4.2449484 -4.2503695 -4.2544541 -4.2449722 -4.2205935 -4.1956477 -4.1776905 -4.1895661 -4.2023287 -4.21336 -4.2148304 -4.2204847 -4.2467327 -4.2781339 -4.3008337][-4.2537203 -4.2526507 -4.2507234 -4.2448759 -4.2306981 -4.2131352 -4.2025867 -4.2138534 -4.2251949 -4.2320914 -4.2308164 -4.2352214 -4.2592111 -4.2872519 -4.3087831][-4.2666364 -4.2614603 -4.2559977 -4.2538552 -4.24704 -4.2353234 -4.2291846 -4.2376194 -4.2455292 -4.251565 -4.2522554 -4.2579408 -4.276865 -4.2990117 -4.3174343]]...]
INFO - root - 2017-12-07 19:38:06.900477: step 38510, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 78h:09m:46s remains)
INFO - root - 2017-12-07 19:38:16.576695: step 38520, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 81h:26m:48s remains)
INFO - root - 2017-12-07 19:38:26.402633: step 38530, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.981 sec/batch; 80h:04m:32s remains)
INFO - root - 2017-12-07 19:38:36.127714: step 38540, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 76h:50m:13s remains)
INFO - root - 2017-12-07 19:38:45.863776: step 38550, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 79h:32m:58s remains)
INFO - root - 2017-12-07 19:38:55.657295: step 38560, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 71h:31m:30s remains)
INFO - root - 2017-12-07 19:39:05.348832: step 38570, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 77h:57m:33s remains)
INFO - root - 2017-12-07 19:39:14.809324: step 38580, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 77h:25m:05s remains)
INFO - root - 2017-12-07 19:39:24.631280: step 38590, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 78h:39m:46s remains)
INFO - root - 2017-12-07 19:39:34.364312: step 38600, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 1.005 sec/batch; 82h:01m:21s remains)
2017-12-07 19:39:35.294451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3252339 -4.3041224 -4.2493405 -4.164145 -4.0786848 -4.0044723 -3.9765439 -4.0331955 -4.1343632 -4.2058339 -4.2326088 -4.2446232 -4.2493916 -4.2543497 -4.2631388][-4.3310137 -4.3104711 -4.2578983 -4.1827483 -4.1143394 -4.0580788 -4.0400133 -4.0901461 -4.172585 -4.2297454 -4.2529888 -4.261116 -4.2603922 -4.2598567 -4.2643166][-4.3357816 -4.3222904 -4.2799358 -4.2192683 -4.1727471 -4.1436095 -4.1428871 -4.1757174 -4.2243776 -4.2601156 -4.2783413 -4.2878423 -4.2878261 -4.2830744 -4.2784891][-4.3278136 -4.3230166 -4.2906332 -4.2418184 -4.2088237 -4.1997566 -4.2117929 -4.2320814 -4.2566066 -4.2721448 -4.283083 -4.2956429 -4.3017006 -4.2954645 -4.2837615][-4.3081417 -4.3073721 -4.2822165 -4.2359939 -4.2061706 -4.200954 -4.2153149 -4.2328486 -4.2531376 -4.2627573 -4.2674465 -4.2793984 -4.2877016 -4.2782717 -4.2596936][-4.2906075 -4.2919827 -4.2629342 -4.2081914 -4.1652517 -4.1416802 -4.1426029 -4.1647825 -4.2027688 -4.2274761 -4.2360468 -4.2444062 -4.2472653 -4.2358575 -4.2113929][-4.275641 -4.2801013 -4.2430453 -4.1708932 -4.0981984 -4.0371175 -4.0085406 -4.0419478 -4.1231356 -4.1811175 -4.2024388 -4.2070088 -4.2023849 -4.1882458 -4.1597281][-4.2631269 -4.2712831 -4.2298965 -4.1450787 -4.0473981 -3.9520473 -3.8921375 -3.9391162 -4.0612636 -4.1453724 -4.1763592 -4.1833692 -4.17871 -4.1660309 -4.1393147][-4.2612352 -4.271843 -4.2319188 -4.1501927 -4.0533195 -3.9523962 -3.8875527 -3.9325435 -4.0505686 -4.1308985 -4.1703625 -4.1895933 -4.1943803 -4.1873546 -4.1692319][-4.2712932 -4.2804604 -4.2454829 -4.1785431 -4.098527 -4.0164113 -3.9652934 -3.9963267 -4.0812197 -4.1464052 -4.1874223 -4.2158041 -4.2325859 -4.2350173 -4.2265897][-4.2864776 -4.2896328 -4.2616119 -4.2080545 -4.1448083 -4.0863233 -4.0539551 -4.0789809 -4.1407409 -4.1920357 -4.2257681 -4.2525158 -4.2765512 -4.28738 -4.2868867][-4.2985926 -4.3003778 -4.2801886 -4.2379222 -4.1879582 -4.148767 -4.1332641 -4.1613955 -4.2124591 -4.2529335 -4.2786145 -4.2995529 -4.3208733 -4.3345075 -4.3375988][-4.308465 -4.3123264 -4.3001041 -4.2676935 -4.2306986 -4.2064648 -4.2043428 -4.2337594 -4.27761 -4.3114324 -4.3290772 -4.3400674 -4.3507571 -4.3591957 -4.3582392][-4.3192205 -4.3237944 -4.3161416 -4.2930956 -4.2675514 -4.2524014 -4.2556524 -4.2797661 -4.3148942 -4.3428278 -4.3547482 -4.3559713 -4.3561835 -4.354434 -4.34504][-4.3262944 -4.3299661 -4.3244939 -4.308157 -4.2891707 -4.278091 -4.2804313 -4.2964544 -4.3221264 -4.3446846 -4.3549752 -4.352232 -4.3448925 -4.3323755 -4.31373]]...]
INFO - root - 2017-12-07 19:39:45.046159: step 38610, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 80h:02m:50s remains)
INFO - root - 2017-12-07 19:39:54.872267: step 38620, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.034 sec/batch; 84h:24m:38s remains)
INFO - root - 2017-12-07 19:40:04.550521: step 38630, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 78h:17m:41s remains)
INFO - root - 2017-12-07 19:40:14.424576: step 38640, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 81h:36m:23s remains)
INFO - root - 2017-12-07 19:40:24.152459: step 38650, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.934 sec/batch; 76h:16m:39s remains)
INFO - root - 2017-12-07 19:40:33.801842: step 38660, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 76h:53m:24s remains)
INFO - root - 2017-12-07 19:40:43.404670: step 38670, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 79h:11m:26s remains)
INFO - root - 2017-12-07 19:40:53.136927: step 38680, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 78h:38m:11s remains)
INFO - root - 2017-12-07 19:41:02.899751: step 38690, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 79h:21m:17s remains)
INFO - root - 2017-12-07 19:41:12.428786: step 38700, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 76h:25m:07s remains)
2017-12-07 19:41:13.403284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2195492 -4.2071323 -4.193903 -4.1821609 -4.1770949 -4.1690426 -4.163331 -4.1691861 -4.1798911 -4.1812382 -4.1682506 -4.1462135 -4.1300917 -4.1274896 -4.1264148][-4.2050991 -4.1975594 -4.1842923 -4.1644549 -4.1516218 -4.139504 -4.1332073 -4.1516805 -4.178339 -4.1898837 -4.182899 -4.1541967 -4.1266074 -4.11949 -4.119287][-4.1917415 -4.191349 -4.1792159 -4.157784 -4.1406384 -4.1260557 -4.1214404 -4.1482906 -4.181613 -4.19501 -4.1902695 -4.1637745 -4.1367669 -4.1313252 -4.13583][-4.1838517 -4.1936827 -4.1835828 -4.158164 -4.1302309 -4.1004539 -4.0850964 -4.1174059 -4.1596937 -4.17836 -4.1857467 -4.1740432 -4.1589141 -4.1608019 -4.1713691][-4.1732016 -4.1914043 -4.1874051 -4.1641226 -4.1211853 -4.0558643 -4.0065193 -4.0471945 -4.1153 -4.1486616 -4.1708717 -4.1759658 -4.1729565 -4.1829762 -4.1976957][-4.1680098 -4.1862512 -4.1824055 -4.1550012 -4.0974307 -3.9856827 -3.8802552 -3.9326785 -4.0413318 -4.0997119 -4.1369157 -4.1576257 -4.1692176 -4.1912317 -4.2070332][-4.1632719 -4.1747723 -4.1645617 -4.1288738 -4.0639563 -3.9250944 -3.7712922 -3.8302789 -3.9775326 -4.0595155 -4.1124716 -4.1467695 -4.169558 -4.198842 -4.211134][-4.156415 -4.1672668 -4.1611118 -4.1354337 -4.0937462 -3.9925315 -3.8783503 -3.9132667 -4.0252876 -4.0904894 -4.1307349 -4.1614861 -4.1828475 -4.2075553 -4.2225652][-4.1633306 -4.1703444 -4.1674666 -4.1565218 -4.1417971 -4.0838275 -4.0169482 -4.0306149 -4.0947251 -4.1396766 -4.1687241 -4.1916122 -4.2072625 -4.2191262 -4.2328839][-4.1863337 -4.1811738 -4.1739397 -4.1681423 -4.1638675 -4.1324778 -4.101377 -4.1071525 -4.1399317 -4.1737852 -4.1950483 -4.2090173 -4.2218885 -4.2261391 -4.2354422][-4.1933832 -4.176425 -4.1642723 -4.1665435 -4.173933 -4.1607342 -4.1542253 -4.160603 -4.1731148 -4.1915817 -4.2022228 -4.2053628 -4.2145653 -4.2201791 -4.2301588][-4.1995807 -4.178812 -4.1659966 -4.1684833 -4.1831846 -4.1788964 -4.1835232 -4.1950727 -4.1983328 -4.2048988 -4.2067647 -4.2010503 -4.2063131 -4.2159986 -4.2293186][-4.22867 -4.2110519 -4.1966047 -4.1969175 -4.2110548 -4.2068996 -4.2129903 -4.2258377 -4.2230887 -4.2194519 -4.2115808 -4.1978807 -4.1959028 -4.2042227 -4.2185292][-4.2469535 -4.2315173 -4.2177668 -4.2129703 -4.2216268 -4.2201724 -4.2274733 -4.2420754 -4.2411885 -4.2341304 -4.2220016 -4.2039728 -4.1942186 -4.19871 -4.2104778][-4.2501097 -4.2421193 -4.2358141 -4.2269669 -4.2263684 -4.2273092 -4.2382488 -4.2550349 -4.2553506 -4.2470932 -4.2363515 -4.2187858 -4.2054906 -4.20813 -4.2175026]]...]
INFO - root - 2017-12-07 19:41:23.145643: step 38710, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 79h:39m:51s remains)
INFO - root - 2017-12-07 19:41:32.870238: step 38720, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.948 sec/batch; 77h:19m:26s remains)
INFO - root - 2017-12-07 19:41:42.733302: step 38730, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.985 sec/batch; 80h:23m:56s remains)
INFO - root - 2017-12-07 19:41:52.367576: step 38740, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 79h:28m:23s remains)
INFO - root - 2017-12-07 19:42:02.159947: step 38750, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 82h:53m:06s remains)
INFO - root - 2017-12-07 19:42:11.729963: step 38760, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 78h:03m:09s remains)
INFO - root - 2017-12-07 19:42:21.421989: step 38770, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 75h:45m:45s remains)
INFO - root - 2017-12-07 19:42:31.172675: step 38780, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 81h:48m:23s remains)
INFO - root - 2017-12-07 19:42:40.850343: step 38790, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.006 sec/batch; 82h:04m:34s remains)
INFO - root - 2017-12-07 19:42:50.347734: step 38800, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 75h:19m:44s remains)
2017-12-07 19:42:51.387503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516336 -4.2557116 -4.2638388 -4.2695436 -4.2668166 -4.2568579 -4.248476 -4.2470174 -4.2528939 -4.2615356 -4.2638545 -4.264482 -4.2648354 -4.2604775 -4.2493896][-4.2617054 -4.26787 -4.274981 -4.2785139 -4.2742805 -4.2647853 -4.26055 -4.2632709 -4.2708969 -4.2788763 -4.2809734 -4.2814345 -4.281312 -4.2747769 -4.2631164][-4.2682614 -4.277348 -4.2850008 -4.2874088 -4.2820835 -4.2734289 -4.2724915 -4.2775288 -4.2853069 -4.2917833 -4.2941222 -4.2955036 -4.29603 -4.289475 -4.280211][-4.264545 -4.2749619 -4.2822747 -4.283649 -4.2774043 -4.2691503 -4.2693281 -4.2743025 -4.2814794 -4.2877321 -4.2921586 -4.2962594 -4.2986751 -4.29398 -4.2871761][-4.2490153 -4.2568851 -4.2608261 -4.2594028 -4.2508235 -4.2413311 -4.2389641 -4.2388425 -4.24244 -4.2502751 -4.25925 -4.2675114 -4.271832 -4.2693152 -4.2649031][-4.2277136 -4.2313004 -4.2313542 -4.2247987 -4.2086577 -4.1898284 -4.17519 -4.1618242 -4.160171 -4.1718483 -4.1873026 -4.1993842 -4.2055726 -4.2070661 -4.2083063][-4.2122631 -4.211576 -4.2085838 -4.1969805 -4.1709747 -4.1363091 -4.10135 -4.0708375 -4.0663776 -4.086092 -4.1083207 -4.1223779 -4.1295629 -4.1352348 -4.1428523][-4.2114038 -4.2090297 -4.2058468 -4.1941614 -4.1648588 -4.1208844 -4.0728769 -4.0344181 -4.0314345 -4.0597787 -4.0869226 -4.1004844 -4.1062722 -4.1113892 -4.119257][-4.221776 -4.2202773 -4.2201819 -4.2151046 -4.1939077 -4.1565189 -4.1140919 -4.0825472 -4.0854011 -4.1164594 -4.1410832 -4.1516786 -4.1547046 -4.1561251 -4.1576719][-4.2295446 -4.2270813 -4.2284484 -4.2284665 -4.2163949 -4.1903758 -4.1591525 -4.1378527 -4.1456127 -4.1744456 -4.1941233 -4.2004261 -4.1996732 -4.194766 -4.1872807][-4.2355509 -4.2312536 -4.2321906 -4.2339644 -4.2285652 -4.2115245 -4.1895266 -4.1748405 -4.1808691 -4.2002649 -4.2116008 -4.2132277 -4.210032 -4.2006569 -4.18599][-4.2480416 -4.2435541 -4.2447639 -4.2470284 -4.2449274 -4.23398 -4.2206683 -4.2117019 -4.2132778 -4.2209163 -4.2229695 -4.2206459 -4.2167287 -4.2067013 -4.1906433][-4.2620831 -4.258924 -4.2613058 -4.2645149 -4.2642846 -4.25829 -4.2519021 -4.2475047 -4.2461314 -4.2469172 -4.2448149 -4.2414236 -4.2376609 -4.2294912 -4.2166443][-4.2724791 -4.2709384 -4.2738953 -4.2771554 -4.2774267 -4.2740831 -4.2714005 -4.2688112 -4.2666345 -4.2655039 -4.2637467 -4.2621021 -4.2604418 -4.2563033 -4.2495108][-4.2749414 -4.2731166 -4.2742572 -4.2765765 -4.2771697 -4.2759418 -4.2751117 -4.2734361 -4.2710624 -4.2696557 -4.2691894 -4.2693148 -4.2694454 -4.2684684 -4.2666178]]...]
INFO - root - 2017-12-07 19:43:01.270688: step 38810, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 79h:12m:44s remains)
INFO - root - 2017-12-07 19:43:10.996573: step 38820, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 79h:05m:15s remains)
INFO - root - 2017-12-07 19:43:20.583207: step 38830, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 78h:19m:35s remains)
INFO - root - 2017-12-07 19:43:30.118583: step 38840, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 73h:29m:15s remains)
INFO - root - 2017-12-07 19:43:39.702203: step 38850, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 77h:40m:04s remains)
INFO - root - 2017-12-07 19:43:49.418013: step 38860, loss = 2.05, batch loss = 1.99 (7.8 examples/sec; 1.029 sec/batch; 83h:55m:24s remains)
INFO - root - 2017-12-07 19:43:59.263036: step 38870, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.040 sec/batch; 84h:52m:01s remains)
INFO - root - 2017-12-07 19:44:08.814526: step 38880, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 75h:38m:49s remains)
INFO - root - 2017-12-07 19:44:18.445640: step 38890, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 79h:50m:39s remains)
INFO - root - 2017-12-07 19:44:28.224582: step 38900, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 79h:59m:52s remains)
2017-12-07 19:44:29.192866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1754308 -4.1541181 -4.1349764 -4.1317511 -4.1430883 -4.1463056 -4.1326861 -4.1225109 -4.1319814 -4.15199 -4.1576653 -4.1432395 -4.1165051 -4.0923386 -4.0998063][-4.1514068 -4.1364126 -4.1220407 -4.1169906 -4.1228209 -4.1270108 -4.122787 -4.1190987 -4.1304541 -4.1478815 -4.1503448 -4.1395211 -4.1219378 -4.1062808 -4.1162958][-4.1606169 -4.1452127 -4.1264868 -4.1163311 -4.1197262 -4.1343942 -4.1432304 -4.1467481 -4.1604123 -4.1756835 -4.1786146 -4.1744637 -4.1697145 -4.1633244 -4.1723943][-4.1857409 -4.1666088 -4.14166 -4.1288409 -4.132184 -4.1534696 -4.16807 -4.1746626 -4.1897154 -4.2076778 -4.2200379 -4.2290783 -4.2355118 -4.2360716 -4.2393861][-4.2020931 -4.1810894 -4.1559653 -4.1444235 -4.1495767 -4.1687961 -4.1764483 -4.1750655 -4.1890159 -4.2142596 -4.2395558 -4.2622137 -4.2767634 -4.2804794 -4.2764044][-4.2016425 -4.1797743 -4.1612887 -4.1527452 -4.1549306 -4.1567507 -4.14223 -4.1255717 -4.1412735 -4.1803336 -4.2197838 -4.2514992 -4.2688274 -4.2712984 -4.2637858][-4.1831756 -4.1628356 -4.1534805 -4.1463771 -4.13609 -4.1060371 -4.054419 -4.0209165 -4.0535517 -4.1197848 -4.176856 -4.2112103 -4.2228103 -4.2180767 -4.20755][-4.1642594 -4.1477537 -4.1462955 -4.13936 -4.1138573 -4.0513749 -3.9608526 -3.9138367 -3.976572 -4.0717869 -4.137897 -4.1629305 -4.1604781 -4.1459 -4.1373825][-4.1520438 -4.1419649 -4.1458988 -4.1431174 -4.1172137 -4.0492263 -3.9552634 -3.9159045 -3.9868555 -4.0758781 -4.1260495 -4.1284304 -4.1085792 -4.0922365 -4.0944324][-4.1610718 -4.1549745 -4.15948 -4.1620703 -4.1504383 -4.1045814 -4.0385456 -4.0147471 -4.0616474 -4.1136565 -4.1299953 -4.1071696 -4.0753789 -4.0644908 -4.07642][-4.1755681 -4.1736345 -4.1779866 -4.1861076 -4.1868525 -4.1607447 -4.1233754 -4.1098666 -4.1329646 -4.1509519 -4.1410904 -4.1034842 -4.0719123 -4.0696616 -4.0848017][-4.1775918 -4.1811395 -4.1878057 -4.2000875 -4.20741 -4.1949062 -4.1788845 -4.1769314 -4.1899409 -4.1921864 -4.1725926 -4.134028 -4.1064281 -4.1065106 -4.1178327][-4.174325 -4.1851563 -4.1958938 -4.2127695 -4.22661 -4.22809 -4.2257605 -4.2302747 -4.2413597 -4.2417917 -4.2248363 -4.195415 -4.1737032 -4.1704516 -4.1754203][-4.1715679 -4.1914062 -4.2108197 -4.2319927 -4.2494717 -4.2595582 -4.2644348 -4.2708092 -4.2790852 -4.2801867 -4.2711892 -4.2537427 -4.2373934 -4.2316823 -4.2297297][-4.1807466 -4.2040424 -4.2263885 -4.2474174 -4.2638021 -4.273097 -4.277101 -4.2825685 -4.288835 -4.2921906 -4.290659 -4.2830281 -4.2729492 -4.2665567 -4.26215]]...]
INFO - root - 2017-12-07 19:44:38.750122: step 38910, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 75h:49m:29s remains)
INFO - root - 2017-12-07 19:44:48.564589: step 38920, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 80h:21m:25s remains)
INFO - root - 2017-12-07 19:44:58.427392: step 38930, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 79h:59m:17s remains)
INFO - root - 2017-12-07 19:45:08.110151: step 38940, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 75h:12m:04s remains)
INFO - root - 2017-12-07 19:45:17.887077: step 38950, loss = 2.10, batch loss = 2.05 (8.0 examples/sec; 1.003 sec/batch; 81h:48m:58s remains)
INFO - root - 2017-12-07 19:45:27.683887: step 38960, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 79h:44m:16s remains)
INFO - root - 2017-12-07 19:45:37.305190: step 38970, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 82h:27m:41s remains)
INFO - root - 2017-12-07 19:45:47.020629: step 38980, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.981 sec/batch; 79h:57m:33s remains)
INFO - root - 2017-12-07 19:45:57.053059: step 38990, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 80h:28m:27s remains)
INFO - root - 2017-12-07 19:46:06.793021: step 39000, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 80h:40m:37s remains)
2017-12-07 19:46:07.799651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3531981 -4.3488655 -4.3412395 -4.3330026 -4.3258886 -4.3219156 -4.3236947 -4.3308377 -4.3392262 -4.3449612 -4.3472552 -4.347621 -4.3467908 -4.3449535 -4.3434515][-4.3544884 -4.3502345 -4.3394532 -4.3245468 -4.3091545 -4.2983747 -4.29621 -4.3046055 -4.3174925 -4.3302059 -4.3403616 -4.346622 -4.3495021 -4.3492532 -4.3473778][-4.3351922 -4.3324647 -4.3183489 -4.29698 -4.2739768 -4.2568512 -4.2528687 -4.2634335 -4.28082 -4.2996521 -4.3174868 -4.3324671 -4.3432803 -4.3490319 -4.3498917][-4.3036461 -4.29873 -4.2819452 -4.2556124 -4.2242136 -4.1983624 -4.1912427 -4.2045479 -4.22563 -4.2491465 -4.2762303 -4.3011141 -4.3223124 -4.3385444 -4.3470011][-4.2745748 -4.2766542 -4.2608142 -4.2298803 -4.1859164 -4.1442809 -4.1205482 -4.1158872 -4.1263595 -4.1549449 -4.2014966 -4.2488656 -4.2881155 -4.3184853 -4.3371987][-4.2345009 -4.2591538 -4.2605977 -4.2410326 -4.2023716 -4.1521211 -4.0985622 -4.047626 -4.0153613 -4.02917 -4.0922527 -4.1662912 -4.2322 -4.2850556 -4.3208475][-4.1702247 -4.2102504 -4.2332382 -4.237895 -4.2254825 -4.1901956 -4.1262717 -4.0410843 -3.9668074 -3.9534853 -4.0059891 -4.0834117 -4.1614461 -4.2323623 -4.28749][-4.1356592 -4.1656265 -4.1932712 -4.213995 -4.21938 -4.1986895 -4.1483307 -4.0757604 -4.0081139 -3.9810195 -4.0045676 -4.0555983 -4.1193542 -4.1899309 -4.25361][-4.1622968 -4.169621 -4.1863313 -4.1999507 -4.2008786 -4.1822004 -4.1517186 -4.1151409 -4.0791259 -4.0594912 -4.0616198 -4.0792294 -4.1147294 -4.1716881 -4.2338996][-4.1972861 -4.1912184 -4.1983347 -4.2039886 -4.1979575 -4.1779375 -4.1618919 -4.1491356 -4.1335945 -4.1223378 -4.11948 -4.1206098 -4.1343904 -4.1735792 -4.2275982][-4.2049088 -4.1844244 -4.1770697 -4.1820812 -4.1903696 -4.1919775 -4.1969404 -4.1976333 -4.1895957 -4.1797581 -4.173306 -4.1665845 -4.1673856 -4.1907072 -4.2334013][-4.2099948 -4.1652365 -4.1338716 -4.1328197 -4.1580319 -4.1862011 -4.2139592 -4.235517 -4.2406683 -4.2353764 -4.2274141 -4.2137852 -4.20426 -4.2161922 -4.2470551][-4.2399578 -4.1847463 -4.1313114 -4.1134086 -4.1355181 -4.1722894 -4.2104836 -4.2420154 -4.2580075 -4.2619629 -4.2617207 -4.2520423 -4.2402711 -4.2437024 -4.2656779][-4.2696323 -4.2305546 -4.184401 -4.1565452 -4.1538558 -4.1723113 -4.2008929 -4.2266793 -4.2408042 -4.2494373 -4.2578263 -4.2615037 -4.2596564 -4.2652016 -4.2823915][-4.2808528 -4.263422 -4.2367716 -4.2125568 -4.1914668 -4.1767159 -4.1715164 -4.1725063 -4.1767197 -4.191041 -4.2145319 -4.23942 -4.2591553 -4.2752094 -4.2913136]]...]
INFO - root - 2017-12-07 19:46:17.461368: step 39010, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 75h:40m:48s remains)
INFO - root - 2017-12-07 19:46:27.109110: step 39020, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 79h:32m:34s remains)
INFO - root - 2017-12-07 19:46:36.893049: step 39030, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 80h:11m:15s remains)
INFO - root - 2017-12-07 19:46:46.458769: step 39040, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 80h:13m:31s remains)
INFO - root - 2017-12-07 19:46:56.158298: step 39050, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 77h:03m:51s remains)
INFO - root - 2017-12-07 19:47:05.903897: step 39060, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 77h:46m:53s remains)
INFO - root - 2017-12-07 19:47:15.608655: step 39070, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 80h:50m:56s remains)
INFO - root - 2017-12-07 19:47:25.213139: step 39080, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 78h:45m:03s remains)
INFO - root - 2017-12-07 19:47:34.787820: step 39090, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 78h:41m:21s remains)
INFO - root - 2017-12-07 19:47:44.335382: step 39100, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.975 sec/batch; 79h:29m:58s remains)
2017-12-07 19:47:45.350302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2649908 -4.2627306 -4.2457542 -4.2364488 -4.2257295 -4.1911478 -4.1821532 -4.2300816 -4.253088 -4.2493372 -4.2410579 -4.2262244 -4.2046704 -4.2039952 -4.2097597][-4.26395 -4.2569795 -4.2415833 -4.2305555 -4.218698 -4.1875091 -4.1770377 -4.2219639 -4.2440596 -4.2402887 -4.2358251 -4.2258444 -4.2122774 -4.214344 -4.2217946][-4.2622609 -4.2518573 -4.2360749 -4.2211704 -4.2085133 -4.1800218 -4.165906 -4.2084355 -4.2351451 -4.2388959 -4.2405033 -4.2320504 -4.2233405 -4.2258096 -4.232645][-4.2698474 -4.260952 -4.2463179 -4.2279139 -4.2119365 -4.1827841 -4.163312 -4.2015057 -4.2362013 -4.2524939 -4.2583742 -4.25068 -4.24584 -4.2502351 -4.2554722][-4.2728295 -4.2629037 -4.2465997 -4.2255316 -4.2052956 -4.1714082 -4.1464095 -4.1791315 -4.221034 -4.2453113 -4.254199 -4.245389 -4.2386913 -4.2434535 -4.2476363][-4.2684636 -4.25624 -4.2337837 -4.2073417 -4.181282 -4.1408997 -4.1060557 -4.1297374 -4.1731477 -4.2020726 -4.2169518 -4.2079525 -4.1969118 -4.1998262 -4.2068572][-4.2506542 -4.2339168 -4.2057195 -4.1739626 -4.1406493 -4.0910697 -4.0434542 -4.0539756 -4.0874486 -4.1210155 -4.1512 -4.1532855 -4.1418915 -4.14379 -4.1557522][-4.2319779 -4.2101321 -4.1772885 -4.1393018 -4.0925717 -4.0286369 -3.9676108 -3.9572496 -3.960149 -3.9909611 -4.0534983 -4.0887113 -4.0939889 -4.1041479 -4.121913][-4.2364182 -4.22096 -4.1940422 -4.1578808 -4.1006112 -4.0254312 -3.9611619 -3.9393363 -3.9168177 -3.9355826 -4.01593 -4.0774922 -4.1027713 -4.1193166 -4.1306968][-4.2560825 -4.2510152 -4.2364521 -4.2115288 -4.1635022 -4.0988426 -4.0531168 -4.0475516 -4.033534 -4.0399942 -4.09241 -4.1405029 -4.1652517 -4.1782389 -4.1803212][-4.2731657 -4.2761416 -4.2703562 -4.2534032 -4.2179003 -4.1717758 -4.1487918 -4.1586866 -4.152926 -4.1506538 -4.1735282 -4.2015543 -4.2179451 -4.2250533 -4.2224536][-4.2904115 -4.2961254 -4.2903843 -4.277833 -4.252532 -4.2200909 -4.2115207 -4.2322092 -4.2314239 -4.2235785 -4.2275743 -4.2393994 -4.24682 -4.2475419 -4.2442307][-4.2992454 -4.3062377 -4.2988381 -4.2904716 -4.2715192 -4.245729 -4.2434177 -4.2661886 -4.265532 -4.2546606 -4.2495823 -4.2493782 -4.2476678 -4.242445 -4.2373443][-4.3003683 -4.3053608 -4.2946405 -4.2903495 -4.279767 -4.2574215 -4.2557974 -4.27708 -4.2766395 -4.2668719 -4.2587972 -4.2500629 -4.2422061 -4.2330561 -4.2247276][-4.2948065 -4.2948823 -4.28336 -4.2805533 -4.2761765 -4.2560525 -4.2520866 -4.2708406 -4.270946 -4.2634835 -4.25688 -4.2452421 -4.2354841 -4.2275004 -4.2198429]]...]
INFO - root - 2017-12-07 19:47:55.075659: step 39110, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 76h:47m:29s remains)
INFO - root - 2017-12-07 19:48:04.728281: step 39120, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 75h:54m:57s remains)
INFO - root - 2017-12-07 19:48:14.488286: step 39130, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 77h:29m:34s remains)
INFO - root - 2017-12-07 19:48:24.224954: step 39140, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.043 sec/batch; 84h:57m:50s remains)
INFO - root - 2017-12-07 19:48:33.818694: step 39150, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 78h:55m:45s remains)
INFO - root - 2017-12-07 19:48:43.551218: step 39160, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 77h:30m:18s remains)
INFO - root - 2017-12-07 19:48:53.183355: step 39170, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 79h:24m:49s remains)
INFO - root - 2017-12-07 19:49:02.855478: step 39180, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 78h:37m:12s remains)
INFO - root - 2017-12-07 19:49:12.543947: step 39190, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.930 sec/batch; 75h:47m:13s remains)
INFO - root - 2017-12-07 19:49:22.426819: step 39200, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.009 sec/batch; 82h:10m:54s remains)
2017-12-07 19:49:23.494457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2484889 -4.2508078 -4.2551312 -4.2563119 -4.2632618 -4.2667594 -4.2660508 -4.2633915 -4.2658358 -4.2804832 -4.2923307 -4.2972035 -4.2986259 -4.2913251 -4.2698674][-4.2575836 -4.2523317 -4.2430873 -4.2234797 -4.21568 -4.2183113 -4.2240844 -4.2294207 -4.2419119 -4.2645512 -4.2742548 -4.2715712 -4.2699566 -4.2576694 -4.2238865][-4.2652459 -4.2546015 -4.2308574 -4.191884 -4.1707883 -4.1732931 -4.1833549 -4.1953168 -4.2199426 -4.253705 -4.2645316 -4.2541571 -4.2454257 -4.2283826 -4.1847472][-4.2763896 -4.2643504 -4.2329469 -4.187149 -4.1601419 -4.15399 -4.152647 -4.1605072 -4.1974292 -4.2464066 -4.2634926 -4.249105 -4.2335467 -4.2138906 -4.1661987][-4.2783246 -4.2661653 -4.2288079 -4.1813745 -4.1504779 -4.1266823 -4.0981421 -4.0979114 -4.1542349 -4.2255354 -4.2554026 -4.2446775 -4.2279835 -4.2093825 -4.1660562][-4.2632942 -4.2482553 -4.2046661 -4.1572433 -4.1183791 -4.0645227 -3.9929481 -3.9722652 -4.0506587 -4.1546726 -4.2048097 -4.2067842 -4.1994958 -4.1874251 -4.1515307][-4.2279377 -4.2112203 -4.1663709 -4.1176405 -4.0690141 -3.9736915 -3.8359976 -3.7739735 -3.8793485 -4.0273476 -4.1047239 -4.128109 -4.1329627 -4.1278658 -4.1022868][-4.1864538 -4.1734314 -4.1410518 -4.103971 -4.0581875 -3.9463973 -3.7714977 -3.6656346 -3.7662737 -3.9307413 -4.0221148 -4.063704 -4.0855613 -4.0888724 -4.0744414][-4.1558042 -4.1604309 -4.1517749 -4.13986 -4.1209579 -4.0512385 -3.9322324 -3.8479328 -3.888278 -3.9858747 -4.0442128 -4.078229 -4.1023865 -4.1119156 -4.1052856][-4.1592665 -4.1781092 -4.1867409 -4.1916046 -4.1910758 -4.1653962 -4.10843 -4.0581465 -4.0600686 -4.09163 -4.1099377 -4.128993 -4.1487031 -4.1606264 -4.1590734][-4.1977324 -4.2231417 -4.23523 -4.2441645 -4.24938 -4.2467184 -4.2263174 -4.1992407 -4.1840458 -4.1792617 -4.1764579 -4.1860132 -4.2027345 -4.2145648 -4.2171488][-4.233448 -4.26038 -4.2750769 -4.2849135 -4.2911363 -4.2946911 -4.2890015 -4.2751279 -4.2593069 -4.2454758 -4.2381759 -4.2418823 -4.25177 -4.2603321 -4.2648058][-4.2547727 -4.2795129 -4.2957573 -4.3067131 -4.3144846 -4.3203974 -4.3214421 -4.3157072 -4.3049684 -4.2945471 -4.2882404 -4.2867374 -4.2884216 -4.2913132 -4.2956209][-4.27724 -4.2950234 -4.3087726 -4.3174314 -4.3242307 -4.3308563 -4.335773 -4.3358464 -4.331665 -4.3265147 -4.3228989 -4.3199673 -4.3176904 -4.3174763 -4.3201823][-4.3042521 -4.313767 -4.32203 -4.3268557 -4.3304133 -4.3338046 -4.3382077 -4.3412871 -4.3419657 -4.3416305 -4.3396749 -4.3358316 -4.3324413 -4.33201 -4.3345995]]...]
INFO - root - 2017-12-07 19:49:33.293894: step 39210, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 79h:55m:37s remains)
INFO - root - 2017-12-07 19:49:43.004802: step 39220, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 80h:59m:05s remains)
INFO - root - 2017-12-07 19:49:52.590535: step 39230, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 80h:23m:21s remains)
INFO - root - 2017-12-07 19:50:02.099158: step 39240, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 72h:55m:07s remains)
INFO - root - 2017-12-07 19:50:11.729630: step 39250, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 77h:34m:00s remains)
INFO - root - 2017-12-07 19:50:21.465937: step 39260, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 81h:40m:22s remains)
INFO - root - 2017-12-07 19:50:31.169826: step 39270, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 78h:22m:00s remains)
INFO - root - 2017-12-07 19:50:40.877169: step 39280, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 76h:36m:28s remains)
INFO - root - 2017-12-07 19:50:50.669815: step 39290, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 79h:54m:03s remains)
INFO - root - 2017-12-07 19:51:00.358912: step 39300, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 76h:30m:46s remains)
2017-12-07 19:51:01.352222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2592978 -4.238759 -4.2240758 -4.2158575 -4.2128506 -4.2177143 -4.2306638 -4.2407155 -4.2444849 -4.2571778 -4.267849 -4.2669134 -4.2519145 -4.2395205 -4.2302203][-4.2604117 -4.2488804 -4.2473965 -4.2503281 -4.2487569 -4.249217 -4.2561975 -4.2578897 -4.254683 -4.2605543 -4.2639985 -4.2607412 -4.2452259 -4.2335567 -4.22715][-4.2620258 -4.2581511 -4.2661729 -4.2785058 -4.2780061 -4.2736959 -4.2751842 -4.2688031 -4.2577639 -4.2557774 -4.2522883 -4.2442584 -4.2284813 -4.2172241 -4.2143955][-4.2639012 -4.2626262 -4.2704964 -4.2803268 -4.2766094 -4.2672534 -4.2641163 -4.2535148 -4.2415323 -4.2365894 -4.2311773 -4.2199526 -4.2016783 -4.1925378 -4.1949444][-4.2718687 -4.2628455 -4.2593565 -4.2582178 -4.248558 -4.2359147 -4.2286835 -4.2175336 -4.2087207 -4.2051888 -4.19733 -4.1828732 -4.1643314 -4.1594224 -4.1670022][-4.2748342 -4.251636 -4.2325521 -4.2191448 -4.2037835 -4.1888428 -4.1804152 -4.1674471 -4.1539721 -4.1530294 -4.1499538 -4.1407881 -4.1314912 -4.1320305 -4.1471124][-4.2671533 -4.2290716 -4.1968632 -4.17572 -4.1569648 -4.1355934 -4.1128068 -4.0853376 -4.061862 -4.0746679 -4.0924306 -4.10393 -4.1111984 -4.1201987 -4.1449995][-4.2491217 -4.2030029 -4.1595926 -4.1295748 -4.1027322 -4.0718837 -4.0373192 -3.9936516 -3.9683154 -4.0113134 -4.0602808 -4.0963264 -4.1191 -4.1400657 -4.1711802][-4.2256126 -4.1833858 -4.1397161 -4.105268 -4.0719271 -4.0420132 -4.0198922 -3.9982147 -3.9916649 -4.038888 -4.0888352 -4.1298146 -4.1578155 -4.1821718 -4.2070966][-4.2124186 -4.1826782 -4.14773 -4.1215391 -4.0978456 -4.0844669 -4.0876312 -4.0924206 -4.0958505 -4.1202679 -4.1498065 -4.1854696 -4.212903 -4.2344236 -4.2468376][-4.2126384 -4.1956911 -4.1741586 -4.1588359 -4.1490912 -4.1513038 -4.1664739 -4.1798682 -4.1813946 -4.1885443 -4.2029252 -4.2274113 -4.2459388 -4.2606211 -4.2660275][-4.2119904 -4.2022943 -4.1931257 -4.1875715 -4.1860428 -4.1943383 -4.2066545 -4.2143216 -4.2132111 -4.2160344 -4.2227135 -4.2348704 -4.2392812 -4.2461472 -4.2535963][-4.2134695 -4.2087512 -4.2069373 -4.2073779 -4.2110057 -4.2194667 -4.2268329 -4.2321477 -4.2338166 -4.2377586 -4.2379212 -4.23789 -4.2310228 -4.2342968 -4.2486453][-4.2487941 -4.2439222 -4.2404537 -4.2407417 -4.2468271 -4.25286 -4.2571716 -4.2594094 -4.2603869 -4.2646446 -4.2630997 -4.2578149 -4.2462754 -4.2453189 -4.2595983][-4.2967072 -4.2913265 -4.287087 -4.2866969 -4.2916965 -4.2954826 -4.2974625 -4.2951403 -4.2931046 -4.2948823 -4.2917747 -4.2864814 -4.2745461 -4.2690849 -4.2771945]]...]
INFO - root - 2017-12-07 19:51:11.029187: step 39310, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 78h:05m:10s remains)
INFO - root - 2017-12-07 19:51:20.887088: step 39320, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 81h:12m:32s remains)
INFO - root - 2017-12-07 19:51:30.435965: step 39330, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 76h:11m:48s remains)
INFO - root - 2017-12-07 19:51:40.046021: step 39340, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 78h:08m:44s remains)
INFO - root - 2017-12-07 19:51:49.781428: step 39350, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.012 sec/batch; 82h:26m:15s remains)
INFO - root - 2017-12-07 19:51:59.501509: step 39360, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 78h:47m:48s remains)
INFO - root - 2017-12-07 19:52:09.065968: step 39370, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 77h:53m:34s remains)
INFO - root - 2017-12-07 19:52:18.752018: step 39380, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 80h:47m:26s remains)
INFO - root - 2017-12-07 19:52:28.561536: step 39390, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 81h:36m:34s remains)
INFO - root - 2017-12-07 19:52:38.276650: step 39400, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 80h:39m:29s remains)
2017-12-07 19:52:39.270763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3212829 -4.3154721 -4.3115492 -4.3063388 -4.3026633 -4.2965069 -4.2923021 -4.2904172 -4.28982 -4.2943835 -4.2977843 -4.2975483 -4.2960348 -4.2959366 -4.2974296][-4.3144627 -4.306386 -4.3009286 -4.2946296 -4.2913189 -4.2844338 -4.2777338 -4.2714062 -4.2680311 -4.2770023 -4.2849951 -4.2868881 -4.2839131 -4.281291 -4.2802539][-4.3040266 -4.2952123 -4.2881446 -4.2813082 -4.2779622 -4.2685094 -4.2563972 -4.2402053 -4.2327552 -4.2486811 -4.2621036 -4.2669911 -4.2660775 -4.2615547 -4.2567444][-4.2939615 -4.2818136 -4.2705832 -4.2610817 -4.25295 -4.2358027 -4.2132959 -4.1841669 -4.172029 -4.1995878 -4.2253079 -4.2366462 -4.2440562 -4.2439656 -4.2393727][-4.2832766 -4.2647724 -4.2464366 -4.2300353 -4.2116971 -4.1816835 -4.1426911 -4.0946703 -4.0766559 -4.1273689 -4.177465 -4.1995649 -4.2176108 -4.2235279 -4.2209067][-4.2728477 -4.2451611 -4.2159991 -4.1883054 -4.157269 -4.1165533 -4.0581965 -3.9791005 -3.9522963 -4.034368 -4.114675 -4.1513953 -4.1797037 -4.1919937 -4.1939783][-4.2667103 -4.2288523 -4.1849656 -4.1452065 -4.1055136 -4.065134 -3.9989269 -3.8946691 -3.8536849 -3.9583149 -4.0684757 -4.1255989 -4.1626472 -4.1769552 -4.1802511][-4.2678394 -4.2251124 -4.1730142 -4.1274605 -4.0888391 -4.0589433 -4.0060287 -3.9107707 -3.8685453 -3.9557979 -4.0671196 -4.1318889 -4.1726041 -4.1876955 -4.1890659][-4.277689 -4.2387757 -4.1926131 -4.1554103 -4.130373 -4.1155305 -4.0853262 -4.0181522 -3.9828823 -4.0319891 -4.1130323 -4.1680174 -4.2049594 -4.2201557 -4.2201691][-4.2903838 -4.2604394 -4.2273941 -4.2036142 -4.1932769 -4.1909533 -4.1789446 -4.1392283 -4.1138172 -4.1359577 -4.1857 -4.2210932 -4.2472677 -4.2599273 -4.25966][-4.302989 -4.2820559 -4.26073 -4.247838 -4.2472181 -4.25253 -4.2518334 -4.233285 -4.2205477 -4.2311096 -4.2595067 -4.2759643 -4.2880239 -4.2951307 -4.2931943][-4.3167539 -4.30402 -4.2918324 -4.2875729 -4.2924843 -4.3017821 -4.3061604 -4.2973742 -4.2900443 -4.292625 -4.3040323 -4.3076148 -4.312418 -4.3179955 -4.3156466][-4.3266759 -4.3217888 -4.3166265 -4.3168912 -4.3242846 -4.3352256 -4.341291 -4.3342843 -4.3232942 -4.3161378 -4.3139939 -4.3115311 -4.315351 -4.321825 -4.3219018][-4.3313046 -4.3297682 -4.3278093 -4.32869 -4.3342404 -4.3419056 -4.3461146 -4.340322 -4.32873 -4.3181148 -4.3111906 -4.3076272 -4.3118095 -4.3179994 -4.3196507][-4.3342037 -4.3334303 -4.3316488 -4.3308516 -4.3321972 -4.3350077 -4.3357491 -4.3305893 -4.3217049 -4.3133774 -4.3086538 -4.3071384 -4.3100452 -4.313674 -4.3157454]]...]
INFO - root - 2017-12-07 19:52:49.031736: step 39410, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 71h:09m:52s remains)
INFO - root - 2017-12-07 19:52:58.710311: step 39420, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 82h:24m:40s remains)
INFO - root - 2017-12-07 19:53:08.476169: step 39430, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 81h:08m:51s remains)
INFO - root - 2017-12-07 19:53:18.012925: step 39440, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 78h:39m:51s remains)
INFO - root - 2017-12-07 19:53:27.820498: step 39450, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 77h:38m:17s remains)
INFO - root - 2017-12-07 19:53:37.573638: step 39460, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.002 sec/batch; 81h:35m:02s remains)
INFO - root - 2017-12-07 19:53:47.332886: step 39470, loss = 2.10, batch loss = 2.05 (7.9 examples/sec; 1.016 sec/batch; 82h:40m:49s remains)
INFO - root - 2017-12-07 19:53:56.977005: step 39480, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 76h:26m:19s remains)
INFO - root - 2017-12-07 19:54:06.703457: step 39490, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 79h:12m:46s remains)
INFO - root - 2017-12-07 19:54:16.225335: step 39500, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 79h:49m:43s remains)
2017-12-07 19:54:17.156094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2719007 -4.26029 -4.269186 -4.2760496 -4.2837605 -4.2880936 -4.2782826 -4.269279 -4.27781 -4.2972426 -4.3090935 -4.3067355 -4.2964182 -4.2858424 -4.2801585][-4.2862997 -4.2790389 -4.2912116 -4.29903 -4.3071537 -4.309516 -4.2991152 -4.2916417 -4.2995129 -4.3200507 -4.3293185 -4.3230853 -4.3066978 -4.2871108 -4.2741323][-4.2971187 -4.2932472 -4.3054109 -4.3149519 -4.3206038 -4.3185945 -4.3057647 -4.2970328 -4.3037825 -4.3230758 -4.331491 -4.3281136 -4.3117557 -4.2861586 -4.2679825][-4.2957311 -4.29991 -4.3113918 -4.3160725 -4.3137617 -4.3028741 -4.2831774 -4.2712469 -4.2787018 -4.3011351 -4.3132615 -4.3143287 -4.304616 -4.2828503 -4.2674241][-4.2711573 -4.281796 -4.2893529 -4.2882 -4.280983 -4.2609963 -4.22909 -4.2063646 -4.2128944 -4.2426715 -4.2643185 -4.2759342 -4.2787671 -4.2723556 -4.2728152][-4.2236328 -4.2335434 -4.2375388 -4.2293634 -4.2185144 -4.19467 -4.1508813 -4.112906 -4.1167569 -4.1630611 -4.201417 -4.2257652 -4.2432957 -4.2603545 -4.28515][-4.157135 -4.1620965 -4.1622424 -4.1503625 -4.1405783 -4.1190925 -4.066915 -4.0089417 -4.0055132 -4.0705991 -4.1318254 -4.1719832 -4.2041054 -4.2416377 -4.2861085][-4.1168642 -4.1067195 -4.0968294 -4.0805511 -4.0734749 -4.0608659 -4.0107374 -3.9429975 -3.9314198 -4.0017376 -4.0764704 -4.1313076 -4.1733294 -4.2197104 -4.273252][-4.1520872 -4.1302881 -4.1071305 -4.0845981 -4.0738206 -4.06243 -4.0216465 -3.961966 -3.9502881 -4.0060477 -4.0691242 -4.123765 -4.16766 -4.2119851 -4.2621703][-4.2371445 -4.2135806 -4.1815906 -4.1539969 -4.1366024 -4.1198363 -4.0922375 -4.0536022 -4.0467 -4.0784822 -4.1163921 -4.1580372 -4.1976008 -4.2312274 -4.2656069][-4.308507 -4.28806 -4.2592368 -4.236691 -4.2185078 -4.1993656 -4.1846709 -4.1691766 -4.1668777 -4.1823726 -4.2010317 -4.2295322 -4.2578163 -4.2746243 -4.2894497][-4.3426037 -4.3275666 -4.3099189 -4.2991614 -4.28928 -4.2759366 -4.2717109 -4.2722735 -4.2713857 -4.2780471 -4.28861 -4.3021231 -4.3134093 -4.3155608 -4.3147864][-4.3465853 -4.3357368 -4.32898 -4.3311119 -4.3291469 -4.3209891 -4.3206949 -4.328969 -4.3292842 -4.3338327 -4.3429708 -4.3485565 -4.3451433 -4.336977 -4.3247409][-4.3276138 -4.320457 -4.3223972 -4.3332138 -4.3366528 -4.3320417 -4.3322392 -4.3395138 -4.3400793 -4.345016 -4.3560314 -4.3600388 -4.3501554 -4.3378973 -4.3214731][-4.3019462 -4.2979712 -4.3034282 -4.31694 -4.3246446 -4.3240247 -4.3237529 -4.3280568 -4.3269577 -4.3294158 -4.3378692 -4.3410888 -4.3321433 -4.3215094 -4.3073416]]...]
INFO - root - 2017-12-07 19:54:26.985509: step 39510, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 77h:36m:41s remains)
INFO - root - 2017-12-07 19:54:36.716512: step 39520, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 77h:44m:51s remains)
INFO - root - 2017-12-07 19:54:46.238241: step 39530, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.958 sec/batch; 77h:58m:36s remains)
INFO - root - 2017-12-07 19:54:55.853860: step 39540, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 80h:22m:00s remains)
INFO - root - 2017-12-07 19:55:05.486960: step 39550, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 76h:45m:28s remains)
INFO - root - 2017-12-07 19:55:15.350894: step 39560, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.990 sec/batch; 80h:33m:03s remains)
INFO - root - 2017-12-07 19:55:24.979682: step 39570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 75h:48m:18s remains)
INFO - root - 2017-12-07 19:55:34.535048: step 39580, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.905 sec/batch; 73h:39m:23s remains)
INFO - root - 2017-12-07 19:55:44.310036: step 39590, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 81h:48m:14s remains)
INFO - root - 2017-12-07 19:55:54.121285: step 39600, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.959 sec/batch; 78h:01m:41s remains)
2017-12-07 19:55:55.152391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2577348 -4.2640362 -4.2737074 -4.2719541 -4.2651687 -4.2645831 -4.265789 -4.2458072 -4.1975584 -4.1503558 -4.1204395 -4.1203084 -4.1596236 -4.2120085 -4.2419047][-4.2589707 -4.2671547 -4.2790208 -4.2790484 -4.2722173 -4.270226 -4.2673664 -4.2457547 -4.2014756 -4.1551533 -4.1296978 -4.129447 -4.1591511 -4.2025127 -4.2231464][-4.2437811 -4.2554054 -4.2691689 -4.2712336 -4.2648869 -4.2578573 -4.2484469 -4.231883 -4.2025418 -4.1658635 -4.1495667 -4.1510653 -4.1682081 -4.1964397 -4.2025509][-4.217958 -4.2299542 -4.2406864 -4.2393365 -4.231792 -4.22224 -4.2114053 -4.2023673 -4.1917458 -4.1758084 -4.1736197 -4.1789112 -4.1871171 -4.2023721 -4.2020226][-4.1888423 -4.1902785 -4.1894026 -4.1802993 -4.17006 -4.164855 -4.1598849 -4.1583447 -4.1676493 -4.1700821 -4.1804128 -4.191803 -4.2045169 -4.2178464 -4.2225389][-4.1587939 -4.1373014 -4.1182079 -4.1027436 -4.0935092 -4.0961862 -4.1031365 -4.1113558 -4.1357765 -4.1501026 -4.1640744 -4.1878662 -4.214613 -4.2338057 -4.245738][-4.1402063 -4.1020842 -4.070477 -4.0573626 -4.0548749 -4.0607843 -4.0759177 -4.0868888 -4.1133928 -4.1329718 -4.1500144 -4.183352 -4.2196312 -4.2441163 -4.2561255][-4.147337 -4.1118917 -4.0868983 -4.0879931 -4.0954847 -4.0975723 -4.1075759 -4.1113739 -4.1260452 -4.1432524 -4.1607442 -4.1973438 -4.2374048 -4.2602305 -4.2663479][-4.1797109 -4.1577225 -4.1474442 -4.1584959 -4.1670914 -4.1636987 -4.1651611 -4.1574039 -4.1535163 -4.160738 -4.1716876 -4.2054534 -4.249249 -4.2758451 -4.2801895][-4.2203779 -4.2105861 -4.2101917 -4.2203512 -4.2266722 -4.2205663 -4.2156692 -4.2025003 -4.1825366 -4.1768389 -4.1819081 -4.2077613 -4.2481294 -4.2780886 -4.2860985][-4.2634377 -4.2580552 -4.2601919 -4.2691078 -4.2744555 -4.268033 -4.2605724 -4.2467189 -4.2256193 -4.2123728 -4.2109418 -4.2258215 -4.2527156 -4.2769122 -4.2866731][-4.286695 -4.2861276 -4.2902761 -4.2979727 -4.2984648 -4.2904396 -4.2842779 -4.27485 -4.2589078 -4.2432227 -4.23834 -4.2435904 -4.2573166 -4.2700791 -4.2788548][-4.2947183 -4.2987103 -4.3039041 -4.3078322 -4.3017545 -4.2924829 -4.2850614 -4.2813296 -4.27322 -4.2643948 -4.2643957 -4.2663054 -4.2679296 -4.2694573 -4.27347][-4.3052406 -4.3107009 -4.3124375 -4.3099146 -4.2987242 -4.2875595 -4.2780209 -4.2793 -4.2798572 -4.2824631 -4.2901692 -4.2918458 -4.2842188 -4.2751193 -4.2698307][-4.3107567 -4.3175654 -4.317349 -4.3100538 -4.2974863 -4.2859211 -4.2770162 -4.2813387 -4.2846923 -4.2911291 -4.3002667 -4.3034973 -4.2949734 -4.2831116 -4.2715864]]...]
INFO - root - 2017-12-07 19:56:04.791999: step 39610, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 77h:34m:06s remains)
INFO - root - 2017-12-07 19:56:14.507454: step 39620, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 76h:52m:55s remains)
INFO - root - 2017-12-07 19:56:24.160248: step 39630, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 80h:46m:34s remains)
INFO - root - 2017-12-07 19:56:33.818971: step 39640, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.947 sec/batch; 77h:00m:07s remains)
INFO - root - 2017-12-07 19:56:43.423533: step 39650, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 80h:07m:53s remains)
INFO - root - 2017-12-07 19:56:53.203466: step 39660, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 80h:22m:00s remains)
INFO - root - 2017-12-07 19:57:02.936287: step 39670, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 77h:12m:26s remains)
INFO - root - 2017-12-07 19:57:12.646519: step 39680, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 80h:18m:32s remains)
INFO - root - 2017-12-07 19:57:22.418405: step 39690, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 81h:22m:27s remains)
INFO - root - 2017-12-07 19:57:32.214521: step 39700, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 81h:31m:26s remains)
2017-12-07 19:57:33.179097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2527323 -4.2573972 -4.2499218 -4.23872 -4.2377963 -4.2547164 -4.2830276 -4.3106327 -4.3293481 -4.339623 -4.34333 -4.3444071 -4.3442416 -4.3401122 -4.3291764][-4.2817278 -4.2817321 -4.2676687 -4.2463803 -4.2337623 -4.2404308 -4.262567 -4.2890854 -4.3110237 -4.3269954 -4.336359 -4.3410392 -4.3414106 -4.3331656 -4.3138595][-4.30812 -4.3082938 -4.2943492 -4.2700453 -4.2496986 -4.2453265 -4.256073 -4.2751336 -4.2956953 -4.3154063 -4.3303962 -4.3386531 -4.3396645 -4.3279819 -4.3023839][-4.3209953 -4.3243585 -4.3162837 -4.2975574 -4.27727 -4.264111 -4.2606516 -4.2669215 -4.2819462 -4.3035293 -4.32443 -4.3377037 -4.3413262 -4.3302369 -4.3051224][-4.3216248 -4.3285389 -4.3271441 -4.3156877 -4.2967615 -4.27364 -4.252285 -4.2423773 -4.2515903 -4.2762856 -4.3058634 -4.3284569 -4.3398108 -4.3368068 -4.3204775][-4.3102679 -4.3202515 -4.3240137 -4.316525 -4.2939982 -4.2563043 -4.2148376 -4.1892924 -4.194561 -4.22535 -4.2662 -4.300446 -4.32227 -4.3304796 -4.3269882][-4.2944489 -4.3040237 -4.3072639 -4.2953911 -4.26088 -4.2041903 -4.1427083 -4.1052446 -4.1120906 -4.15247 -4.20678 -4.2545075 -4.2870722 -4.3054128 -4.3129363][-4.28284 -4.2870536 -4.2827868 -4.26062 -4.2113266 -4.1351442 -4.0550389 -4.0079527 -4.0181603 -4.0690117 -4.1349134 -4.1948638 -4.2387753 -4.2682819 -4.2860179][-4.2763643 -4.2734466 -4.2587194 -4.2246161 -4.16308 -4.0728521 -3.9809258 -3.9289184 -3.9442461 -4.0039005 -4.0776863 -4.1444597 -4.1951003 -4.2304196 -4.2538261][-4.2651887 -4.25832 -4.2385206 -4.199254 -4.1347446 -4.0459056 -3.9592655 -3.914572 -3.9340317 -3.9958909 -4.0692554 -4.13494 -4.1836896 -4.21515 -4.2347846][-4.2503414 -4.244957 -4.2277961 -4.1948042 -4.1431246 -4.0746951 -4.0106688 -3.9812031 -4.0012984 -4.0555348 -4.1177535 -4.1724305 -4.2107606 -4.2319083 -4.2422581][-4.2339587 -4.2326112 -4.2251244 -4.2093482 -4.1821818 -4.1447935 -4.11064 -4.0989571 -4.1180515 -4.1576862 -4.2000356 -4.23495 -4.25631 -4.2645235 -4.2658629][-4.2200165 -4.2235947 -4.227993 -4.2319217 -4.2311854 -4.2237425 -4.2146349 -4.2139144 -4.2279816 -4.2510519 -4.2728562 -4.2881594 -4.2950325 -4.2936134 -4.2900352][-4.2208705 -4.225194 -4.2347383 -4.2496333 -4.2643294 -4.2740049 -4.2777662 -4.2802472 -4.2863107 -4.2954063 -4.3032694 -4.3058515 -4.3040032 -4.2996454 -4.2971535][-4.2321386 -4.2320461 -4.2384095 -4.2532244 -4.2707825 -4.2848282 -4.2920532 -4.2949152 -4.2962432 -4.2968326 -4.2953954 -4.2910385 -4.2861362 -4.2835846 -4.2852736]]...]
INFO - root - 2017-12-07 19:57:42.893009: step 39710, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 78h:08m:34s remains)
INFO - root - 2017-12-07 19:57:52.496911: step 39720, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 80h:39m:21s remains)
INFO - root - 2017-12-07 19:58:02.123907: step 39730, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 77h:05m:26s remains)
INFO - root - 2017-12-07 19:58:11.799465: step 39740, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 80h:38m:12s remains)
INFO - root - 2017-12-07 19:58:21.441797: step 39750, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 80h:23m:39s remains)
INFO - root - 2017-12-07 19:58:31.085954: step 39760, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 79h:53m:46s remains)
INFO - root - 2017-12-07 19:58:40.661775: step 39770, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.944 sec/batch; 76h:43m:39s remains)
INFO - root - 2017-12-07 19:58:50.261824: step 39780, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 76h:41m:28s remains)
INFO - root - 2017-12-07 19:58:59.852885: step 39790, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 73h:16m:26s remains)
INFO - root - 2017-12-07 19:59:09.399078: step 39800, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 78h:35m:06s remains)
2017-12-07 19:59:10.327589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2853765 -4.2767997 -4.28108 -4.2983718 -4.3282828 -4.3484988 -4.3472958 -4.3209023 -4.2853484 -4.2508817 -4.2303934 -4.2229743 -4.2260475 -4.2406759 -4.253139][-4.283854 -4.2821631 -4.2936492 -4.3190918 -4.3519812 -4.3660173 -4.34986 -4.3073406 -4.2606716 -4.223773 -4.205287 -4.2046113 -4.2166767 -4.2333679 -4.2479405][-4.2892723 -4.2962403 -4.3151612 -4.3437419 -4.3691149 -4.3681951 -4.3347607 -4.2806554 -4.2297668 -4.2016444 -4.1939845 -4.2025824 -4.2211261 -4.2376733 -4.2510915][-4.3121505 -4.3316078 -4.3562074 -4.3780088 -4.38598 -4.3647761 -4.3118539 -4.243135 -4.1883593 -4.1757131 -4.1861916 -4.2073503 -4.2318234 -4.2473783 -4.2594042][-4.3455434 -4.3754759 -4.3976994 -4.4009337 -4.38525 -4.3414173 -4.2646027 -4.1741123 -4.1164784 -4.1305156 -4.1687574 -4.208899 -4.2445354 -4.2616968 -4.2703233][-4.3739271 -4.4039841 -4.4136157 -4.3937769 -4.3551326 -4.2892914 -4.186182 -4.0711985 -4.0123925 -4.0621681 -4.1406517 -4.2090807 -4.2571139 -4.2751684 -4.277976][-4.3838725 -4.4069686 -4.4010239 -4.3633022 -4.31094 -4.2286286 -4.1018572 -3.956223 -3.8964059 -3.988694 -4.1108551 -4.2087965 -4.2656479 -4.2801929 -4.2796822][-4.380302 -4.3913035 -4.3691416 -4.3183031 -4.2592692 -4.1698537 -4.026298 -3.8609707 -3.8202775 -3.9559796 -4.1071024 -4.2172475 -4.272769 -4.2828574 -4.2821445][-4.3705487 -4.3666949 -4.3335071 -4.2809558 -4.2260418 -4.1405005 -3.9920332 -3.8323286 -3.8292108 -3.9884543 -4.1373577 -4.235323 -4.283679 -4.291503 -4.2918615][-4.3561082 -4.3433952 -4.3082738 -4.2639551 -4.2183766 -4.1440763 -4.0185041 -3.8971043 -3.9242578 -4.0645142 -4.1825919 -4.258081 -4.2953787 -4.3009977 -4.3028712][-4.343339 -4.3280005 -4.2957935 -4.2593341 -4.2208986 -4.1651711 -4.0756011 -4.0006309 -4.0392909 -4.13986 -4.2241569 -4.2796187 -4.3027005 -4.3035131 -4.3052449][-4.3279672 -4.3138046 -4.2861347 -4.2588558 -4.23226 -4.1956525 -4.1352673 -4.0902524 -4.1233845 -4.19307 -4.2539716 -4.291801 -4.3018274 -4.2995996 -4.2996559][-4.3108168 -4.2987127 -4.2787228 -4.2636237 -4.2514815 -4.2319455 -4.1945548 -4.1659517 -4.1860991 -4.2308121 -4.27357 -4.2954807 -4.2957797 -4.2904258 -4.2845531][-4.2955322 -4.2862487 -4.2757006 -4.2708454 -4.271142 -4.2678885 -4.2500558 -4.2337685 -4.243814 -4.2671909 -4.289669 -4.2987151 -4.2942371 -4.2837687 -4.2717476][-4.289701 -4.279263 -4.2696781 -4.270164 -4.2808466 -4.2901177 -4.2876611 -4.2811193 -4.2862644 -4.2975264 -4.3060155 -4.3082614 -4.3038898 -4.291996 -4.2764287]]...]
INFO - root - 2017-12-07 19:59:20.080338: step 39810, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.970 sec/batch; 78h:53m:55s remains)
INFO - root - 2017-12-07 19:59:29.754085: step 39820, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 79h:09m:06s remains)
INFO - root - 2017-12-07 19:59:39.204302: step 39830, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 79h:32m:20s remains)
INFO - root - 2017-12-07 19:59:48.791338: step 39840, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 78h:36m:15s remains)
INFO - root - 2017-12-07 19:59:58.421943: step 39850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 78h:33m:23s remains)
INFO - root - 2017-12-07 20:00:08.002455: step 39860, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 77h:23m:59s remains)
INFO - root - 2017-12-07 20:00:17.668578: step 39870, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 80h:13m:51s remains)
INFO - root - 2017-12-07 20:00:27.387749: step 39880, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 78h:41m:18s remains)
INFO - root - 2017-12-07 20:00:36.960384: step 39890, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 75h:54m:25s remains)
INFO - root - 2017-12-07 20:00:46.482704: step 39900, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 78h:28m:33s remains)
2017-12-07 20:00:47.468386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1237717 -4.0847373 -4.0568862 -4.0645862 -4.1129131 -4.1646595 -4.2001987 -4.2166519 -4.2074103 -4.1819448 -4.1516318 -4.1210694 -4.0979381 -4.08934 -4.1144485][-4.1274 -4.0916996 -4.0639153 -4.0705042 -4.1173787 -4.1663246 -4.2017031 -4.21996 -4.2142591 -4.186986 -4.1520228 -4.1158204 -4.0891008 -4.0863776 -4.1173229][-4.1375885 -4.1082063 -4.0780993 -4.0764308 -4.1142259 -4.1595464 -4.197948 -4.22125 -4.2217941 -4.2010131 -4.1676726 -4.1354489 -4.1182394 -4.1352062 -4.174902][-4.1409445 -4.112855 -4.0809813 -4.0727487 -4.0978637 -4.1382742 -4.1827312 -4.2103539 -4.215312 -4.1967559 -4.165689 -4.1432009 -4.1454973 -4.1811781 -4.2226958][-4.14614 -4.1211052 -4.0925703 -4.0763307 -4.0879478 -4.1282125 -4.1768694 -4.2019372 -4.20183 -4.1788487 -4.1504374 -4.1385021 -4.1591859 -4.2065144 -4.2393079][-4.1521425 -4.1296177 -4.0995216 -4.0765982 -4.085454 -4.1309514 -4.1817565 -4.2007089 -4.1872334 -4.1515036 -4.1273613 -4.1288514 -4.1599064 -4.2067251 -4.2281761][-4.1469207 -4.1243448 -4.0856929 -4.0634413 -4.082675 -4.1357327 -4.1869493 -4.2008133 -4.1780457 -4.1314154 -4.1109705 -4.1202631 -4.1505551 -4.1863737 -4.196548][-4.1265707 -4.1000123 -4.0532088 -4.0351815 -4.06767 -4.1292043 -4.1820173 -4.1919837 -4.1632514 -4.1136022 -4.1045156 -4.1232805 -4.1492057 -4.1695924 -4.1681533][-4.1177487 -4.0844212 -4.0329885 -4.0194588 -4.0573087 -4.1206322 -4.1695151 -4.1698895 -4.1327209 -4.0872941 -4.1000657 -4.1355281 -4.1600232 -4.16682 -4.1601954][-4.155869 -4.1208014 -4.0705538 -4.0578122 -4.0862117 -4.1330276 -4.1665487 -4.1556673 -4.1079588 -4.0695868 -4.0997624 -4.1448035 -4.1682119 -4.1686935 -4.1640325][-4.2103963 -4.1804657 -4.1395192 -4.1264205 -4.1397481 -4.1639957 -4.1801267 -4.165369 -4.1189728 -4.0916147 -4.1247854 -4.1618981 -4.17991 -4.17947 -4.1779075][-4.2335534 -4.2146573 -4.19073 -4.1819291 -4.1860385 -4.197711 -4.2081442 -4.19878 -4.1664624 -4.150373 -4.1750717 -4.1985874 -4.2080488 -4.2038932 -4.201189][-4.2309923 -4.2256885 -4.2184768 -4.2181549 -4.2237844 -4.2318335 -4.2412424 -4.2376466 -4.219573 -4.2102938 -4.2250681 -4.23692 -4.2385917 -4.2277312 -4.2193007][-4.2423787 -4.2410326 -4.240118 -4.2444696 -4.2513404 -4.259542 -4.2667651 -4.2647419 -4.2566118 -4.2536149 -4.261764 -4.2673235 -4.2639976 -4.2506676 -4.2368441][-4.26872 -4.2641 -4.260519 -4.263258 -4.2658596 -4.2702689 -4.2757254 -4.2755723 -4.2759681 -4.2796993 -4.2861686 -4.2909522 -4.28965 -4.2783189 -4.2619357]]...]
INFO - root - 2017-12-07 20:00:57.211126: step 39910, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 78h:37m:55s remains)
INFO - root - 2017-12-07 20:01:06.606235: step 39920, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 73h:28m:32s remains)
INFO - root - 2017-12-07 20:01:16.214107: step 39930, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 79h:08m:50s remains)
INFO - root - 2017-12-07 20:01:25.609037: step 39940, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 76h:37m:29s remains)
INFO - root - 2017-12-07 20:01:35.202798: step 39950, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 80h:13m:46s remains)
INFO - root - 2017-12-07 20:01:44.797339: step 39960, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 78h:44m:46s remains)
INFO - root - 2017-12-07 20:01:54.570019: step 39970, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 80h:37m:13s remains)
INFO - root - 2017-12-07 20:02:04.172138: step 39980, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 78h:29m:21s remains)
INFO - root - 2017-12-07 20:02:13.739459: step 39990, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 78h:16m:11s remains)
INFO - root - 2017-12-07 20:02:23.525671: step 40000, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 80h:25m:53s remains)
2017-12-07 20:02:24.458134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2456222 -4.2524109 -4.2354012 -4.1702218 -4.0889025 -4.0760374 -4.1306973 -4.1877694 -4.2106738 -4.2124805 -4.2149639 -4.2235794 -4.2306046 -4.2287397 -4.2173452][-4.2249508 -4.2339025 -4.2346063 -4.194521 -4.1148787 -4.0649519 -4.1019168 -4.1702824 -4.2071414 -4.2144208 -4.2152691 -4.2167988 -4.22206 -4.2256107 -4.2162671][-4.2121706 -4.2174449 -4.22966 -4.2194796 -4.159091 -4.084012 -4.0810885 -4.1477685 -4.2020535 -4.2193747 -4.2193661 -4.2144985 -4.2173829 -4.2236285 -4.2172394][-4.2146726 -4.2150321 -4.2312126 -4.2404156 -4.2023635 -4.1172295 -4.0644026 -4.1082759 -4.17938 -4.2164221 -4.2228513 -4.2157278 -4.2145114 -4.2206006 -4.2171555][-4.2297354 -4.2278757 -4.2403026 -4.2531075 -4.2311616 -4.1490469 -4.0546837 -4.0498252 -4.128068 -4.1970644 -4.225153 -4.2262721 -4.2244539 -4.2260318 -4.2208443][-4.2430563 -4.2419777 -4.2489319 -4.2577653 -4.2425156 -4.1710582 -4.05531 -3.9869437 -4.0506029 -4.1549864 -4.2214284 -4.2440243 -4.2460585 -4.2423091 -4.2354875][-4.2455716 -4.2448626 -4.2475262 -4.2510252 -4.2383881 -4.1836638 -4.0704293 -3.9568336 -3.976579 -4.0992966 -4.2029743 -4.2528858 -4.263433 -4.2590427 -4.2509923][-4.2415318 -4.2414117 -4.2400961 -4.2403011 -4.2325168 -4.1993513 -4.117732 -4.0039997 -3.9648697 -4.0598269 -4.1773129 -4.2492566 -4.2719116 -4.2707391 -4.2613821][-4.2429318 -4.2443337 -4.2359033 -4.2284231 -4.2268887 -4.2163835 -4.1788898 -4.0964866 -4.0282035 -4.0583935 -4.1555824 -4.235692 -4.2720475 -4.27975 -4.2710509][-4.2530274 -4.2545018 -4.2388005 -4.2170572 -4.2109647 -4.2166219 -4.2152824 -4.1748643 -4.1099515 -4.0840321 -4.1379175 -4.2119808 -4.2611279 -4.2791944 -4.2759862][-4.2657971 -4.2665677 -4.2450891 -4.2103486 -4.1926255 -4.2058759 -4.2282004 -4.2235866 -4.1789408 -4.1284475 -4.1318145 -4.1855106 -4.2390976 -4.2658324 -4.271049][-4.2724128 -4.2760196 -4.2555127 -4.2174511 -4.1905379 -4.2025952 -4.2364087 -4.2515383 -4.2285504 -4.1740031 -4.1400509 -4.1605568 -4.2059159 -4.2419538 -4.2568469][-4.2657743 -4.2750525 -4.2612004 -4.2341208 -4.2105579 -4.2172923 -4.2484236 -4.26662 -4.2571521 -4.2088938 -4.1591835 -4.145925 -4.1748366 -4.2146115 -4.23807][-4.2494912 -4.2658668 -4.2616558 -4.2489138 -4.2336621 -4.2361856 -4.2578516 -4.2701964 -4.2653351 -4.2289066 -4.1799107 -4.1483188 -4.1597953 -4.1967988 -4.223947][-4.2374034 -4.2536254 -4.2576423 -4.2547827 -4.2463031 -4.2455459 -4.2540593 -4.2572107 -4.2522879 -4.2276416 -4.1922 -4.1634703 -4.1647139 -4.1939573 -4.2188444]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 20:02:34.460672: step 40010, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 79h:16m:00s remains)
INFO - root - 2017-12-07 20:02:43.881034: step 40020, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 76h:08m:31s remains)
INFO - root - 2017-12-07 20:02:53.514469: step 40030, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.947 sec/batch; 76h:53m:51s remains)
INFO - root - 2017-12-07 20:03:03.161488: step 40040, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.014 sec/batch; 82h:24m:58s remains)
INFO - root - 2017-12-07 20:03:12.836788: step 40050, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 77h:08m:25s remains)
INFO - root - 2017-12-07 20:03:22.497020: step 40060, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 80h:26m:59s remains)
INFO - root - 2017-12-07 20:03:32.140057: step 40070, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 71h:46m:28s remains)
INFO - root - 2017-12-07 20:03:41.826428: step 40080, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 74h:17m:31s remains)
INFO - root - 2017-12-07 20:03:51.321363: step 40090, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 78h:39m:21s remains)
INFO - root - 2017-12-07 20:04:01.028911: step 40100, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.006 sec/batch; 81h:43m:41s remains)
2017-12-07 20:04:01.994614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2828941 -4.2715621 -4.2634859 -4.2630744 -4.2546797 -4.2307057 -4.213222 -4.2104845 -4.2133646 -4.2143917 -4.2043562 -4.1987176 -4.2081656 -4.2248993 -4.2418351][-4.2934675 -4.2865233 -4.2837753 -4.2876158 -4.279521 -4.2545371 -4.2332616 -4.2239566 -4.2202992 -4.2119222 -4.1901412 -4.1737657 -4.1738682 -4.1863408 -4.2026052][-4.2811728 -4.2828522 -4.2883477 -4.2981038 -4.2945461 -4.2742276 -4.2540464 -4.2416763 -4.2337556 -4.2201815 -4.194581 -4.1747394 -4.1697335 -4.1794481 -4.1932592][-4.2528472 -4.2600508 -4.2710676 -4.2824965 -4.2821422 -4.2677436 -4.2524605 -4.2428565 -4.237009 -4.22638 -4.2058721 -4.1918912 -4.1905928 -4.2005591 -4.2124624][-4.2156148 -4.2234077 -4.2339411 -4.2418504 -4.240365 -4.2309504 -4.2245774 -4.2229862 -4.22356 -4.2214565 -4.2094364 -4.2037315 -4.20767 -4.2169862 -4.2271395][-4.1740746 -4.18094 -4.18896 -4.1920414 -4.1889949 -4.1813741 -4.179441 -4.185071 -4.1920028 -4.1975131 -4.190866 -4.1899452 -4.1966724 -4.2044287 -4.2134209][-4.1377058 -4.1412978 -4.1482859 -4.148519 -4.1428518 -4.1306596 -4.1265011 -4.1322632 -4.1420064 -4.1549683 -4.15513 -4.1573029 -4.1634173 -4.1680465 -4.1775331][-4.11755 -4.1143432 -4.1195555 -4.1148639 -4.0962563 -4.0661421 -4.0504432 -4.0524259 -4.0674939 -4.0944409 -4.1096644 -4.1200347 -4.1282253 -4.132874 -4.1462121][-4.1357126 -4.1265116 -4.1269073 -4.1146455 -4.0805326 -4.0295358 -3.9983692 -3.9939613 -4.0130334 -4.0526271 -4.0836563 -4.1059856 -4.1190143 -4.1251197 -4.1391864][-4.1875453 -4.1764231 -4.1705418 -4.1531253 -4.1120434 -4.0521035 -4.0135984 -4.0051413 -4.02596 -4.0690856 -4.1099119 -4.1415591 -4.1577282 -4.1636038 -4.174418][-4.2390494 -4.2281203 -4.2199373 -4.20253 -4.1625175 -4.1067872 -4.0729437 -4.07055 -4.0960345 -4.1380763 -4.1807766 -4.2141705 -4.2297606 -4.2328682 -4.2380877][-4.2700319 -4.2582688 -4.2500648 -4.2352891 -4.20132 -4.1587477 -4.1321015 -4.134203 -4.1634121 -4.2028618 -4.2427034 -4.2736893 -4.286859 -4.2877245 -4.29029][-4.2692938 -4.2588277 -4.2528682 -4.2432685 -4.2185841 -4.1907506 -4.1718225 -4.1762166 -4.2053909 -4.2402692 -4.2743707 -4.2996778 -4.30961 -4.3101325 -4.3128915][-4.2516055 -4.2396374 -4.2348385 -4.2287607 -4.2155342 -4.2020774 -4.1948209 -4.2042551 -4.2318435 -4.2623672 -4.2881074 -4.3052735 -4.3114796 -4.312861 -4.3176079][-4.2475777 -4.2325392 -4.2239313 -4.2167878 -4.213069 -4.2136612 -4.2177305 -4.2315273 -4.255662 -4.2794352 -4.2962174 -4.3048234 -4.3072114 -4.3092527 -4.3155837]]...]
INFO - root - 2017-12-07 20:04:11.352977: step 40110, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 77h:10m:22s remains)
INFO - root - 2017-12-07 20:04:20.759163: step 40120, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 77h:10m:56s remains)
INFO - root - 2017-12-07 20:04:30.481193: step 40130, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 81h:06m:45s remains)
INFO - root - 2017-12-07 20:04:40.179899: step 40140, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 78h:31m:07s remains)
INFO - root - 2017-12-07 20:04:49.732214: step 40150, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 74h:57m:50s remains)
INFO - root - 2017-12-07 20:04:59.315275: step 40160, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 76h:28m:54s remains)
INFO - root - 2017-12-07 20:05:08.795222: step 40170, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 76h:38m:53s remains)
INFO - root - 2017-12-07 20:05:18.608793: step 40180, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.010 sec/batch; 81h:58m:59s remains)
INFO - root - 2017-12-07 20:05:28.160671: step 40190, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 78h:40m:20s remains)
INFO - root - 2017-12-07 20:05:37.803719: step 40200, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 77h:22m:08s remains)
2017-12-07 20:05:38.800343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32503 -4.3160367 -4.3016748 -4.2834029 -4.2715168 -4.2702518 -4.2666464 -4.2612095 -4.2655711 -4.2776685 -4.2892809 -4.2956815 -4.3028941 -4.3085117 -4.3126445][-4.3273163 -4.3202939 -4.3077645 -4.2883644 -4.2707005 -4.2629805 -4.25766 -4.2557054 -4.2656112 -4.2795134 -4.2904391 -4.2941103 -4.296236 -4.2961869 -4.2965221][-4.3305779 -4.3279748 -4.3185344 -4.2979145 -4.2746038 -4.2573051 -4.245595 -4.24285 -4.2574759 -4.2773023 -4.2917686 -4.296988 -4.2960749 -4.2894483 -4.2861953][-4.3356857 -4.3383102 -4.332303 -4.309989 -4.27811 -4.2453794 -4.2215104 -4.2135286 -4.2306538 -4.2605305 -4.2850394 -4.2957292 -4.2938714 -4.2854371 -4.2788639][-4.3410015 -4.3456345 -4.3409882 -4.3168464 -4.273416 -4.2214794 -4.1791568 -4.1637 -4.1848054 -4.2277761 -4.2650762 -4.2830663 -4.2836428 -4.2754483 -4.2649813][-4.3457689 -4.3495536 -4.3430419 -4.3136821 -4.2597365 -4.1924467 -4.1298795 -4.1052132 -4.1344161 -4.1912317 -4.2386246 -4.2632346 -4.2696395 -4.2623825 -4.2478509][-4.3497934 -4.35185 -4.3414431 -4.3059287 -4.2459974 -4.1701126 -4.0930166 -4.060318 -4.0972152 -4.1663432 -4.2210717 -4.2488465 -4.2599874 -4.2560058 -4.239471][-4.3514857 -4.3524933 -4.3382449 -4.298254 -4.2361012 -4.1585126 -4.0778151 -4.0410151 -4.0805531 -4.1544423 -4.2127748 -4.243372 -4.2587795 -4.2572126 -4.2371459][-4.3517065 -4.3528218 -4.338644 -4.2997322 -4.2419105 -4.1711607 -4.0945024 -4.0553 -4.0854855 -4.1523709 -4.2104068 -4.2449508 -4.2622657 -4.2571573 -4.2282534][-4.3527236 -4.3536887 -4.3424344 -4.3087273 -4.2584729 -4.1986437 -4.1332235 -4.0945463 -4.1101432 -4.16053 -4.2112732 -4.246973 -4.2656155 -4.2561889 -4.220468][-4.3551345 -4.3566151 -4.3482423 -4.3200197 -4.2771754 -4.2262282 -4.1753693 -4.14258 -4.1472235 -4.1804938 -4.2191892 -4.2492456 -4.2651629 -4.2541509 -4.2159729][-4.3566213 -4.3596654 -4.3541045 -4.3310461 -4.2928624 -4.2483835 -4.2102351 -4.186512 -4.1844716 -4.2035971 -4.229135 -4.2499194 -4.2586274 -4.2456827 -4.2098041][-4.3565927 -4.3614645 -4.3590188 -4.338758 -4.3015509 -4.2599258 -4.2288251 -4.2132735 -4.2115569 -4.2239909 -4.2403569 -4.2472668 -4.2403097 -4.2259 -4.1984453][-4.3562279 -4.3631659 -4.3640828 -4.3466444 -4.3098512 -4.2704062 -4.2419357 -4.229197 -4.22791 -4.2360435 -4.242188 -4.23463 -4.2147703 -4.2008681 -4.1853509][-4.3558745 -4.3644404 -4.3683395 -4.3525414 -4.3162732 -4.2771406 -4.2491055 -4.23817 -4.2363024 -4.2393918 -4.2360282 -4.2172003 -4.1901517 -4.17663 -4.165874]]...]
INFO - root - 2017-12-07 20:05:48.332151: step 40210, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 71h:38m:08s remains)
INFO - root - 2017-12-07 20:05:58.149927: step 40220, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 81h:31m:41s remains)
INFO - root - 2017-12-07 20:06:07.880128: step 40230, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.944 sec/batch; 76h:40m:37s remains)
INFO - root - 2017-12-07 20:06:17.458760: step 40240, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 76h:33m:01s remains)
INFO - root - 2017-12-07 20:06:27.105510: step 40250, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 77h:21m:40s remains)
INFO - root - 2017-12-07 20:06:36.778884: step 40260, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 77h:30m:13s remains)
INFO - root - 2017-12-07 20:06:46.294076: step 40270, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 75h:42m:25s remains)
INFO - root - 2017-12-07 20:06:56.079933: step 40280, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 80h:32m:49s remains)
INFO - root - 2017-12-07 20:07:05.770189: step 40290, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.956 sec/batch; 77h:37m:45s remains)
INFO - root - 2017-12-07 20:07:15.441082: step 40300, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 80h:04m:41s remains)
2017-12-07 20:07:16.430477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2949076 -4.2900205 -4.2876782 -4.2840753 -4.2823086 -4.27957 -4.2714043 -4.2556348 -4.2319489 -4.21635 -4.2261515 -4.2425871 -4.2467079 -4.2463226 -4.2504492][-4.2850504 -4.2844748 -4.2846675 -4.2781754 -4.2728434 -4.271256 -4.2594371 -4.2329178 -4.1965513 -4.1765828 -4.1949263 -4.220902 -4.2271981 -4.2224336 -4.2250552][-4.2814355 -4.2881293 -4.2905331 -4.2832074 -4.2764854 -4.2731142 -4.2544751 -4.2164168 -4.1686931 -4.1485381 -4.17785 -4.2140203 -4.2224126 -4.2111583 -4.2082834][-4.2856512 -4.2961063 -4.2973752 -4.2912254 -4.2856374 -4.2792354 -4.2532978 -4.204917 -4.1493325 -4.1296639 -4.169414 -4.2131782 -4.2243805 -4.2074761 -4.1977754][-4.2897573 -4.3004913 -4.2994528 -4.2953076 -4.2896109 -4.2755804 -4.2336645 -4.1701083 -4.1050262 -4.0898695 -4.1442127 -4.2021852 -4.2216024 -4.2034235 -4.18993][-4.2886734 -4.2965302 -4.294652 -4.2880363 -4.2730627 -4.2403641 -4.1712904 -4.0859203 -4.01725 -4.0161152 -4.0913997 -4.1721225 -4.2083945 -4.1985331 -4.1840239][-4.2808433 -4.2861142 -4.280776 -4.2651196 -4.233736 -4.1790638 -4.0839806 -3.9808047 -3.9120388 -3.9252276 -4.0279942 -4.1334853 -4.1880784 -4.1910934 -4.1788077][-4.277617 -4.2804704 -4.265625 -4.2364373 -4.1906867 -4.119133 -4.0072083 -3.9049993 -3.8515208 -3.8751748 -4.0029368 -4.1260939 -4.1866007 -4.1936836 -4.1831222][-4.273344 -4.2719207 -4.2473369 -4.2102008 -4.1604166 -4.0815277 -3.9581416 -3.8649342 -3.8427107 -3.8938622 -4.03235 -4.1466012 -4.1962872 -4.1966071 -4.1852083][-4.2728868 -4.26956 -4.2395172 -4.2022567 -4.1596274 -4.0839658 -3.951978 -3.8643508 -3.8807018 -3.9680452 -4.0892515 -4.1724644 -4.2072859 -4.2036009 -4.19112][-4.2785544 -4.2734642 -4.2454019 -4.2175584 -4.1895614 -4.1298594 -4.01186 -3.9404285 -3.9796829 -4.066287 -4.1498137 -4.2011294 -4.2226081 -4.2166696 -4.207972][-4.2861357 -4.28036 -4.257709 -4.2423091 -4.228034 -4.1898165 -4.1075044 -4.0638685 -4.1022696 -4.161562 -4.2043982 -4.2292614 -4.2368064 -4.229497 -4.2253332][-4.2928224 -4.2884145 -4.2728124 -4.2668567 -4.2619729 -4.2419477 -4.1973367 -4.1761775 -4.2047944 -4.2325029 -4.2430348 -4.24693 -4.2456765 -4.2402172 -4.2424045][-4.3020811 -4.2968664 -4.2870951 -4.2846746 -4.2845449 -4.2748766 -4.2534246 -4.2456751 -4.2626133 -4.2697954 -4.2643409 -4.2590675 -4.2570424 -4.2580438 -4.2652664][-4.3135157 -4.30958 -4.3034754 -4.300436 -4.298049 -4.29124 -4.2811503 -4.2800431 -4.2897038 -4.2897859 -4.2799335 -4.275002 -4.2775946 -4.2831922 -4.2905974]]...]
INFO - root - 2017-12-07 20:07:25.809879: step 40310, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 76h:58m:50s remains)
INFO - root - 2017-12-07 20:07:35.418007: step 40320, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 81h:36m:19s remains)
INFO - root - 2017-12-07 20:07:44.984284: step 40330, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 73h:35m:41s remains)
INFO - root - 2017-12-07 20:07:54.652817: step 40340, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 74h:38m:50s remains)
INFO - root - 2017-12-07 20:08:04.088296: step 40350, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 77h:40m:43s remains)
INFO - root - 2017-12-07 20:08:13.492306: step 40360, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 77h:50m:05s remains)
INFO - root - 2017-12-07 20:08:23.059920: step 40370, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 80h:11m:24s remains)
INFO - root - 2017-12-07 20:08:32.811960: step 40380, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 79h:45m:30s remains)
INFO - root - 2017-12-07 20:08:42.406865: step 40390, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 78h:01m:23s remains)
INFO - root - 2017-12-07 20:08:51.900686: step 40400, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.992 sec/batch; 80h:30m:11s remains)
2017-12-07 20:08:52.957694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3294177 -4.3412304 -4.3506603 -4.3501883 -4.3477259 -4.3465829 -4.345324 -4.3437486 -4.3425179 -4.3431273 -4.3454432 -4.3486204 -4.3504043 -4.3509207 -4.3506565][-4.3180952 -4.3261127 -4.3337793 -4.3319135 -4.32972 -4.3296585 -4.3287783 -4.3253951 -4.3223624 -4.3242388 -4.3298721 -4.33657 -4.340055 -4.3408546 -4.3383861][-4.2971277 -4.2904968 -4.29195 -4.2929306 -4.2967677 -4.3041053 -4.3075485 -4.305912 -4.3031321 -4.3039293 -4.3105845 -4.3194127 -4.3238897 -4.3233619 -4.3173485][-4.2707109 -4.2407069 -4.2276163 -4.2321877 -4.2483015 -4.2678714 -4.2805281 -4.2859588 -4.2882047 -4.290308 -4.2986379 -4.30873 -4.3136039 -4.3115964 -4.303297][-4.2538066 -4.2060704 -4.1779695 -4.1803012 -4.2029333 -4.2300024 -4.2493448 -4.26086 -4.272078 -4.283751 -4.2977262 -4.3090925 -4.314548 -4.312264 -4.3044934][-4.2516913 -4.2011385 -4.1675 -4.1653237 -4.1796613 -4.1955085 -4.2019353 -4.2079678 -4.2278886 -4.2582545 -4.2869811 -4.3041315 -4.3123336 -4.3121662 -4.3103905][-4.2458477 -4.2087908 -4.188035 -4.1809092 -4.1713562 -4.1533079 -4.1224837 -4.0971537 -4.1162395 -4.1751394 -4.2325449 -4.2647238 -4.2832217 -4.2918997 -4.2980065][-4.2360644 -4.2199135 -4.2177539 -4.2070451 -4.1694865 -4.1100206 -4.0244145 -3.9410684 -3.949286 -4.0509806 -4.1453185 -4.1967554 -4.2227507 -4.2367053 -4.2485967][-4.2287812 -4.2396173 -4.2545991 -4.2486329 -4.2037163 -4.1235704 -4.0065489 -3.8825326 -3.8744082 -3.9910119 -4.0975451 -4.147707 -4.1627994 -4.1684089 -4.1748838][-4.2263021 -4.2533555 -4.2760682 -4.28244 -4.2538395 -4.1916242 -4.1029444 -4.0111017 -3.9963822 -4.0649023 -4.1309724 -4.1512361 -4.1414409 -4.1295609 -4.123621][-4.211112 -4.237493 -4.2703862 -4.2945771 -4.2902684 -4.254952 -4.2044034 -4.1519079 -4.13648 -4.16199 -4.1843781 -4.1781468 -4.1574411 -4.134409 -4.1111984][-4.1815681 -4.2019129 -4.2434492 -4.2796893 -4.2894859 -4.2726851 -4.2463088 -4.221138 -4.2095604 -4.2136378 -4.2136064 -4.201314 -4.1836843 -4.1557312 -4.1213703][-4.16582 -4.1740704 -4.20886 -4.2427845 -4.25866 -4.2549281 -4.2487545 -4.2458992 -4.2373881 -4.2281375 -4.2208776 -4.2164469 -4.2081094 -4.1855621 -4.1539497][-4.1859589 -4.1798444 -4.1949062 -4.2140102 -4.2298827 -4.2358365 -4.2475076 -4.2604752 -4.2524681 -4.2337518 -4.2239943 -4.223084 -4.2212787 -4.2122889 -4.2001042][-4.2332363 -4.2249427 -4.2278194 -4.2332006 -4.2417092 -4.2485676 -4.2680163 -4.2844715 -4.2745075 -4.2494593 -4.2354584 -4.2349019 -4.2369709 -4.2414494 -4.2476239]]...]
INFO - root - 2017-12-07 20:09:02.366816: step 40410, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 74h:49m:36s remains)
INFO - root - 2017-12-07 20:09:11.809933: step 40420, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 74h:35m:50s remains)
INFO - root - 2017-12-07 20:09:21.593249: step 40430, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 79h:56m:34s remains)
INFO - root - 2017-12-07 20:09:31.051648: step 40440, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 78h:10m:45s remains)
INFO - root - 2017-12-07 20:09:40.614300: step 40450, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 76h:14m:22s remains)
INFO - root - 2017-12-07 20:09:50.354560: step 40460, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.962 sec/batch; 78h:02m:00s remains)
INFO - root - 2017-12-07 20:09:59.931166: step 40470, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 77h:58m:04s remains)
INFO - root - 2017-12-07 20:10:09.458485: step 40480, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 77h:37m:25s remains)
INFO - root - 2017-12-07 20:10:19.026723: step 40490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 75h:40m:10s remains)
INFO - root - 2017-12-07 20:10:28.715660: step 40500, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 74h:29m:17s remains)
2017-12-07 20:10:29.679924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1694689 -4.1161318 -4.123333 -4.1819305 -4.2307835 -4.2566752 -4.296349 -4.3272448 -4.3208909 -4.2903891 -4.2515893 -4.2310319 -4.2282124 -4.2444515 -4.2512689][-4.146863 -4.093699 -4.0997043 -4.1591754 -4.20619 -4.2276411 -4.2645154 -4.3001842 -4.3009424 -4.2781434 -4.2450237 -4.229938 -4.2301764 -4.2457261 -4.2519503][-4.1540895 -4.1073914 -4.1125588 -4.1622386 -4.19623 -4.2045469 -4.2311373 -4.265111 -4.2733068 -4.266695 -4.2476459 -4.2420626 -4.2420263 -4.2497845 -4.2488184][-4.1744351 -4.1435509 -4.1464882 -4.1784263 -4.19482 -4.1874866 -4.1989703 -4.2269249 -4.2420611 -4.2559562 -4.2539053 -4.2536893 -4.2463231 -4.2431326 -4.2323947][-4.1872587 -4.1771832 -4.1809983 -4.1958604 -4.1936321 -4.1706724 -4.162643 -4.1781845 -4.1971664 -4.2288527 -4.2426486 -4.2483392 -4.2350411 -4.2237687 -4.2070656][-4.1894813 -4.1988888 -4.2066922 -4.208405 -4.1908016 -4.1541791 -4.1295204 -4.1312823 -4.1499496 -4.1925497 -4.21798 -4.231287 -4.2152987 -4.1974125 -4.176537][-4.1989098 -4.2217555 -4.2314773 -4.225626 -4.1964331 -4.1487145 -4.1090703 -4.0977726 -4.1132927 -4.161716 -4.19222 -4.2108579 -4.1939831 -4.1700234 -4.14594][-4.2211871 -4.2508698 -4.2594967 -4.2477331 -4.2092066 -4.1503768 -4.0980282 -4.0772095 -4.0915775 -4.1420193 -4.1730576 -4.1936965 -4.1747375 -4.1408653 -4.1118584][-4.2471876 -4.27624 -4.28163 -4.2633319 -4.2180042 -4.1507263 -4.0902691 -4.067843 -4.0818348 -4.1320572 -4.1640515 -4.1867909 -4.1634531 -4.1158571 -4.0833039][-4.2713094 -4.2949815 -4.2956877 -4.2709718 -4.2187858 -4.1434431 -4.0804086 -4.0596547 -4.0738358 -4.1247783 -4.1601496 -4.1865077 -4.1604033 -4.1032305 -4.0771213][-4.2877097 -4.3006144 -4.2957096 -4.2693958 -4.2131705 -4.1328321 -4.0715594 -4.0558419 -4.0715518 -4.120378 -4.1556721 -4.1796918 -4.1496181 -4.0893087 -4.08048][-4.2913537 -4.2942152 -4.2860203 -4.2631474 -4.2108622 -4.1358209 -4.0829368 -4.0734162 -4.0879545 -4.1262569 -4.1486683 -4.1567478 -4.1155052 -4.0533643 -4.0667238][-4.2879977 -4.2832289 -4.2760582 -4.2590857 -4.2163157 -4.1556063 -4.1157408 -4.1125708 -4.1218824 -4.1422348 -4.1437011 -4.1266012 -4.0652819 -3.9994655 -4.0318985][-4.2841592 -4.2742229 -4.2693329 -4.2582197 -4.2278118 -4.1850581 -4.1575875 -4.1561422 -4.1587796 -4.1593132 -4.1372871 -4.0943208 -4.0184464 -3.953042 -3.9953611][-4.2873888 -4.2742891 -4.2690868 -4.261271 -4.2433391 -4.2188673 -4.2006063 -4.1964154 -4.1909089 -4.1758542 -4.1372218 -4.0787563 -3.9993694 -3.9401429 -3.9804821]]...]
INFO - root - 2017-12-07 20:10:39.114187: step 40510, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 79h:45m:51s remains)
INFO - root - 2017-12-07 20:10:48.656785: step 40520, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 77h:37m:13s remains)
INFO - root - 2017-12-07 20:10:58.383978: step 40530, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 75h:17m:48s remains)
INFO - root - 2017-12-07 20:11:08.052355: step 40540, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 78h:25m:38s remains)
INFO - root - 2017-12-07 20:11:17.725075: step 40550, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 77h:19m:01s remains)
INFO - root - 2017-12-07 20:11:27.417432: step 40560, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 80h:45m:28s remains)
INFO - root - 2017-12-07 20:11:36.963980: step 40570, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 73h:21m:32s remains)
INFO - root - 2017-12-07 20:11:46.525678: step 40580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 67h:56m:22s remains)
INFO - root - 2017-12-07 20:11:56.002764: step 40590, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 79h:11m:20s remains)
INFO - root - 2017-12-07 20:12:05.517604: step 40600, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 79h:31m:21s remains)
2017-12-07 20:12:06.438575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2990761 -4.3062239 -4.2992969 -4.2723646 -4.2399769 -4.2182422 -4.2168522 -4.2383327 -4.2571964 -4.26551 -4.2600517 -4.2427444 -4.2289648 -4.2231917 -4.2338066][-4.2913156 -4.2986932 -4.2925415 -4.2706289 -4.2457132 -4.2318072 -4.2338872 -4.2489257 -4.2581515 -4.2568321 -4.2427154 -4.2194047 -4.2058277 -4.2005172 -4.2132392][-4.2764139 -4.2869263 -4.2851915 -4.2704158 -4.2551613 -4.2475476 -4.24709 -4.2495856 -4.2490163 -4.2417574 -4.22071 -4.1887965 -4.1709685 -4.1670785 -4.1824861][-4.2547917 -4.2709603 -4.27704 -4.2707791 -4.2632918 -4.2587295 -4.2523694 -4.2428722 -4.2374372 -4.2322531 -4.2108049 -4.1724319 -4.14661 -4.1390805 -4.152597][-4.2347074 -4.2569466 -4.270896 -4.2706103 -4.26616 -4.2589569 -4.2412829 -4.2228775 -4.21853 -4.2185345 -4.20085 -4.1654677 -4.1338415 -4.1223936 -4.1336083][-4.2204905 -4.2454195 -4.2637877 -4.2629118 -4.2507424 -4.2291765 -4.1928105 -4.1653914 -4.1648612 -4.1725445 -4.1659431 -4.1444106 -4.1212077 -4.1149888 -4.12756][-4.1990433 -4.2224278 -4.2398577 -4.23076 -4.2003741 -4.1524749 -4.0946984 -4.0632749 -4.0744581 -4.096374 -4.1096139 -4.1072578 -4.1025143 -4.1103125 -4.1245556][-4.1607866 -4.1802526 -4.1936612 -4.1721959 -4.1212115 -4.0514145 -3.9814823 -3.9556 -3.9856067 -4.0317025 -4.067277 -4.0834284 -4.1009316 -4.1248527 -4.1377568][-4.1093221 -4.1227336 -4.13071 -4.1014419 -4.0445232 -3.9745963 -3.9168818 -3.9105582 -3.9594369 -4.0230703 -4.0704217 -4.0947089 -4.12576 -4.1602688 -4.1742754][-4.0771418 -4.087431 -4.0958562 -4.0747609 -4.0369306 -3.9953351 -3.9708509 -3.9830148 -4.0312014 -4.0848522 -4.120141 -4.1351995 -4.1653895 -4.198051 -4.2102904][-4.0926347 -4.1054473 -4.1181965 -4.114573 -4.1043396 -4.0937362 -4.0917811 -4.1033988 -4.13184 -4.163425 -4.1796732 -4.1808305 -4.2007909 -4.2215376 -4.229322][-4.1327548 -4.1514368 -4.1716323 -4.1839304 -4.1936784 -4.20108 -4.206501 -4.2067642 -4.2126746 -4.2259569 -4.225174 -4.214602 -4.2206736 -4.2292767 -4.2302508][-4.1865907 -4.2083592 -4.232419 -4.2507548 -4.2645335 -4.2756605 -4.2811303 -4.2729874 -4.2659168 -4.2658496 -4.2565637 -4.2425771 -4.2427559 -4.2451653 -4.2411408][-4.2440219 -4.2641358 -4.2851534 -4.298912 -4.3083305 -4.3176651 -4.3208852 -4.3101254 -4.2996531 -4.2954926 -4.2889237 -4.2809711 -4.2848868 -4.2870345 -4.2796931][-4.2913876 -4.3041806 -4.3186312 -4.3260694 -4.3309293 -4.3377085 -4.3403821 -4.3316259 -4.3222928 -4.3183389 -4.3168278 -4.3149934 -4.3233805 -4.3241777 -4.3163137]]...]
INFO - root - 2017-12-07 20:12:16.078524: step 40610, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 76h:49m:13s remains)
INFO - root - 2017-12-07 20:12:25.735964: step 40620, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 80h:40m:32s remains)
INFO - root - 2017-12-07 20:12:35.198004: step 40630, loss = 2.12, batch loss = 2.06 (8.1 examples/sec; 0.986 sec/batch; 79h:58m:02s remains)
INFO - root - 2017-12-07 20:12:44.551352: step 40640, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 76h:26m:15s remains)
INFO - root - 2017-12-07 20:12:54.057572: step 40650, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 76h:57m:06s remains)
INFO - root - 2017-12-07 20:13:03.672713: step 40660, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 73h:30m:21s remains)
INFO - root - 2017-12-07 20:13:13.495830: step 40670, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 79h:55m:24s remains)
INFO - root - 2017-12-07 20:13:23.189753: step 40680, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.014 sec/batch; 82h:12m:15s remains)
INFO - root - 2017-12-07 20:13:32.787991: step 40690, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 76h:02m:54s remains)
INFO - root - 2017-12-07 20:13:42.353572: step 40700, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 74h:25m:45s remains)
2017-12-07 20:13:43.381557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1379333 -4.1713061 -4.2043686 -4.2274294 -4.2284861 -4.2080197 -4.1887493 -4.1850657 -4.1978931 -4.2117682 -4.2166924 -4.2239285 -4.2329893 -4.2519169 -4.28228][-4.1449709 -4.1813359 -4.2132874 -4.2293925 -4.2190285 -4.192389 -4.1754594 -4.1786423 -4.1934919 -4.2036695 -4.2089686 -4.2189422 -4.2283244 -4.2400813 -4.2672544][-4.1812367 -4.207355 -4.2301545 -4.233707 -4.2137718 -4.1857848 -4.1746626 -4.1835475 -4.198215 -4.2038469 -4.2063766 -4.2191358 -4.2268205 -4.2326374 -4.2527032][-4.2186246 -4.2331443 -4.2373648 -4.2276392 -4.2060318 -4.1810913 -4.17451 -4.1892815 -4.2089076 -4.2206244 -4.2271008 -4.2415371 -4.2503791 -4.2509174 -4.2585163][-4.2420139 -4.2459683 -4.2363544 -4.2175875 -4.1961241 -4.1718674 -4.1635513 -4.17983 -4.2108946 -4.2350922 -4.2523251 -4.2701368 -4.280437 -4.2780447 -4.2771316][-4.2477202 -4.2462988 -4.2281032 -4.1990523 -4.1657248 -4.1344233 -4.1160231 -4.127614 -4.17131 -4.2141671 -4.2456236 -4.2707543 -4.28343 -4.2844157 -4.2860022][-4.2422767 -4.2390389 -4.2140188 -4.1744905 -4.125473 -4.0744181 -4.035511 -4.0362797 -4.0900278 -4.153851 -4.2002697 -4.232851 -4.2490864 -4.2591152 -4.2707233][-4.2216535 -4.2190552 -4.1952925 -4.1532669 -4.092011 -4.0087347 -3.93406 -3.91505 -3.9743798 -4.0643444 -4.1362286 -4.1805258 -4.2003741 -4.2177606 -4.2402806][-4.1878076 -4.1981492 -4.1896076 -4.158505 -4.1077185 -4.0219817 -3.934319 -3.898782 -3.9416213 -4.0239959 -4.0990148 -4.1512341 -4.1809406 -4.206955 -4.2338185][-4.1726475 -4.2012267 -4.2111726 -4.1956325 -4.1690936 -4.1118412 -4.0467939 -4.0185361 -4.0402646 -4.0853939 -4.1273732 -4.165297 -4.1988521 -4.2283764 -4.2537894][-4.1899438 -4.2197003 -4.2347836 -4.2272944 -4.2170811 -4.1750507 -4.1228175 -4.1077127 -4.1249137 -4.1456442 -4.1614151 -4.1858416 -4.2196312 -4.25101 -4.2753677][-4.218595 -4.2298455 -4.2335997 -4.2285614 -4.2282357 -4.1934233 -4.1497507 -4.1460371 -4.15851 -4.1614056 -4.1598783 -4.1813941 -4.2240205 -4.25847 -4.2819581][-4.2309351 -4.2156043 -4.2027068 -4.1948595 -4.1969867 -4.1710572 -4.1416159 -4.1473436 -4.1527357 -4.1364355 -4.1179729 -4.1367421 -4.1870227 -4.2248244 -4.2514296][-4.2236795 -4.1929789 -4.1710377 -4.1629057 -4.1686468 -4.1582394 -4.1465974 -4.1613846 -4.1641459 -4.135829 -4.1065054 -4.1210232 -4.1705661 -4.2044196 -4.2289362][-4.2140775 -4.1797757 -4.1603117 -4.1557436 -4.1645622 -4.1681442 -4.1725397 -4.1925197 -4.1961823 -4.1716857 -4.1497793 -4.1672192 -4.208941 -4.2286463 -4.2397795]]...]
INFO - root - 2017-12-07 20:13:52.959649: step 40710, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.947 sec/batch; 76h:46m:13s remains)
INFO - root - 2017-12-07 20:14:02.577061: step 40720, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 75h:04m:39s remains)
INFO - root - 2017-12-07 20:14:12.446094: step 40730, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 79h:35m:32s remains)
INFO - root - 2017-12-07 20:14:22.165484: step 40740, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 78h:10m:25s remains)
INFO - root - 2017-12-07 20:14:31.795325: step 40750, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 78h:40m:06s remains)
INFO - root - 2017-12-07 20:14:41.229677: step 40760, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.977 sec/batch; 79h:09m:52s remains)
INFO - root - 2017-12-07 20:14:51.061934: step 40770, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 78h:48m:20s remains)
INFO - root - 2017-12-07 20:15:00.729016: step 40780, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 77h:28m:29s remains)
INFO - root - 2017-12-07 20:15:10.430818: step 40790, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 79h:43m:58s remains)
INFO - root - 2017-12-07 20:15:20.145020: step 40800, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 77h:56m:05s remains)
2017-12-07 20:15:21.150676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2973723 -4.2830696 -4.2695951 -4.2601767 -4.2543983 -4.2564082 -4.2621918 -4.2696571 -4.2811718 -4.2950525 -4.3031917 -4.3018112 -4.2958059 -4.2921934 -4.2929955][-4.2954278 -4.2776642 -4.2637072 -4.2569385 -4.2548337 -4.2608356 -4.2696643 -4.2778287 -4.2878494 -4.3013849 -4.3111734 -4.3161116 -4.3185 -4.3207603 -4.3205938][-4.2774425 -4.2532716 -4.2371 -4.2311163 -4.23111 -4.2411714 -4.2581768 -4.2716174 -4.2796021 -4.2921176 -4.3030686 -4.3133731 -4.323853 -4.3340716 -4.3353915][-4.2519822 -4.2229886 -4.2046094 -4.1992693 -4.1999969 -4.2132497 -4.2370205 -4.254365 -4.2632923 -4.2770629 -4.2897182 -4.3006577 -4.314775 -4.3295436 -4.3325372][-4.221138 -4.184535 -4.1616483 -4.1555924 -4.1578646 -4.1721816 -4.1996045 -4.2209063 -4.232511 -4.2492366 -4.2671127 -4.2803059 -4.2963271 -4.3096795 -4.3122587][-4.1902823 -4.1453967 -4.1111116 -4.0995617 -4.10252 -4.1137595 -4.1363716 -4.1547694 -4.1634674 -4.1820722 -4.2104983 -4.2368255 -4.2619905 -4.2781811 -4.2843122][-4.1804104 -4.1260281 -4.0771213 -4.0549641 -4.0450063 -4.0406055 -4.05093 -4.062089 -4.0662155 -4.0895352 -4.13377 -4.1782346 -4.2184162 -4.24699 -4.2615042][-4.1906896 -4.1339688 -4.0801067 -4.050137 -4.0221214 -3.9907384 -3.9783578 -3.9787931 -3.980171 -4.0080943 -4.067246 -4.1288176 -4.1822791 -4.2219048 -4.245935][-4.2191205 -4.1734138 -4.1338921 -4.1095018 -4.076014 -4.0236974 -3.985003 -3.9647069 -3.9547384 -3.9757946 -4.0367379 -4.1077709 -4.1706915 -4.2155833 -4.2448244][-4.2600212 -4.2316828 -4.2138171 -4.2037272 -4.1797528 -4.1317778 -4.0845933 -4.0490766 -4.0249949 -4.026957 -4.0671887 -4.13001 -4.1905389 -4.233007 -4.2614083][-4.2903819 -4.2796879 -4.2814713 -4.2834268 -4.2675295 -4.2305036 -4.189373 -4.1562696 -4.1340055 -4.1302366 -4.1508708 -4.1909904 -4.2335925 -4.2664452 -4.2889247][-4.3062778 -4.3064466 -4.3163371 -4.3214092 -4.311286 -4.2864089 -4.2571254 -4.2356935 -4.2247686 -4.2243381 -4.2355661 -4.2570033 -4.2805037 -4.3015223 -4.3169961][-4.3153429 -4.3180337 -4.3261724 -4.33027 -4.3221254 -4.3029523 -4.2792945 -4.2645245 -4.2639627 -4.2726436 -4.2854991 -4.3013959 -4.3149166 -4.3274851 -4.3352437][-4.3229079 -4.3269858 -4.332922 -4.334702 -4.32493 -4.3078275 -4.2885847 -4.2771935 -4.2798476 -4.2911873 -4.3055816 -4.321034 -4.3325763 -4.3402581 -4.3426843][-4.3303747 -4.3333273 -4.3365297 -4.3355446 -4.3254628 -4.3117275 -4.2979097 -4.2904496 -4.29242 -4.3027558 -4.3159852 -4.3303442 -4.3405948 -4.3462324 -4.345304]]...]
INFO - root - 2017-12-07 20:15:30.805435: step 40810, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 76h:48m:35s remains)
INFO - root - 2017-12-07 20:15:40.606946: step 40820, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 79h:58m:50s remains)
INFO - root - 2017-12-07 20:15:50.358496: step 40830, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 76h:54m:13s remains)
INFO - root - 2017-12-07 20:15:59.983212: step 40840, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 77h:13m:10s remains)
INFO - root - 2017-12-07 20:16:09.466308: step 40850, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 75h:22m:31s remains)
INFO - root - 2017-12-07 20:16:19.226544: step 40860, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.014 sec/batch; 82h:07m:13s remains)
INFO - root - 2017-12-07 20:16:28.942687: step 40870, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 79h:06m:12s remains)
INFO - root - 2017-12-07 20:16:38.493933: step 40880, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 77h:18m:04s remains)
INFO - root - 2017-12-07 20:16:48.187697: step 40890, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 78h:58m:57s remains)
INFO - root - 2017-12-07 20:16:57.892375: step 40900, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 76h:36m:18s remains)
2017-12-07 20:16:58.883379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2066779 -4.2084374 -4.1866679 -4.1676183 -4.1762037 -4.207233 -4.23129 -4.2366829 -4.2267833 -4.2151933 -4.204236 -4.190784 -4.1716881 -4.157546 -4.1621242][-4.2096009 -4.2017794 -4.1715646 -4.1501331 -4.1621075 -4.195838 -4.2258019 -4.2368455 -4.228538 -4.2142324 -4.2023754 -4.1905723 -4.1733837 -4.16024 -4.1693974][-4.2152934 -4.1984138 -4.1570344 -4.1286325 -4.1407337 -4.1782913 -4.2130079 -4.2257209 -4.223731 -4.2132421 -4.2045245 -4.1986818 -4.1828456 -4.1649685 -4.1646972][-4.2132196 -4.1964006 -4.1493707 -4.1115909 -4.1202812 -4.1568446 -4.190814 -4.2020717 -4.2022028 -4.1984477 -4.1946692 -4.1976738 -4.1874709 -4.168736 -4.1587076][-4.1963296 -4.1928072 -4.153337 -4.1169667 -4.1169987 -4.1429043 -4.1647739 -4.165751 -4.1609588 -4.1541257 -4.1543164 -4.1691551 -4.1710024 -4.1583877 -4.1429262][-4.1592965 -4.1757774 -4.15403 -4.1267357 -4.1180797 -4.1264367 -4.1256466 -4.106791 -4.0860186 -4.0696254 -4.0771861 -4.1098442 -4.1322861 -4.1333046 -4.1252937][-4.11465 -4.1503725 -4.1491723 -4.1324477 -4.1166425 -4.1064782 -4.0823078 -4.0363417 -3.9859655 -3.9507267 -3.9673896 -4.0229821 -4.0679364 -4.0893264 -4.101656][-4.1040435 -4.1428466 -4.1553292 -4.1480193 -4.1321626 -4.1140137 -4.0820222 -4.0281825 -3.9605865 -3.9089251 -3.9232359 -3.9810228 -4.0260968 -4.0580397 -4.0871687][-4.1223116 -4.1539922 -4.1740513 -4.1733422 -4.1532593 -4.1326389 -4.1107359 -4.0791445 -4.022747 -3.9640322 -3.9628167 -4.0110297 -4.0489631 -4.0729632 -4.0989933][-4.1331468 -4.1641731 -4.19129 -4.1946516 -4.1715636 -4.1503506 -4.1379166 -4.1275473 -4.091465 -4.0356808 -4.0155644 -4.0485935 -4.0725465 -4.0817504 -4.094285][-4.1264567 -4.155859 -4.1866765 -4.1997857 -4.1886096 -4.1687818 -4.1600142 -4.1599822 -4.1426148 -4.1018367 -4.0736604 -4.083621 -4.0849361 -4.0829091 -4.0860019][-4.1101413 -4.1395903 -4.1683693 -4.1835637 -4.185998 -4.1805191 -4.1714878 -4.1659193 -4.1597404 -4.1388741 -4.11777 -4.1117334 -4.0978012 -4.0845385 -4.0846338][-4.10676 -4.1238685 -4.1411161 -4.1533632 -4.1675158 -4.1806107 -4.1796551 -4.1723475 -4.1649246 -4.154532 -4.1446624 -4.1375766 -4.1217103 -4.1056738 -4.1031909][-4.1050243 -4.1140885 -4.1232638 -4.1252317 -4.13909 -4.1618943 -4.1707854 -4.1655717 -4.1576176 -4.1553574 -4.1617975 -4.1690984 -4.1632886 -4.1526527 -4.1460233][-4.0989866 -4.1143246 -4.1165695 -4.1045637 -4.1138391 -4.1447105 -4.1553669 -4.14817 -4.1433535 -4.1495891 -4.1618457 -4.1803107 -4.1904416 -4.1937866 -4.1906667]]...]
INFO - root - 2017-12-07 20:17:08.486038: step 40910, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 75h:58m:44s remains)
INFO - root - 2017-12-07 20:17:18.094054: step 40920, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 80h:48m:46s remains)
INFO - root - 2017-12-07 20:17:27.724446: step 40930, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 78h:42m:15s remains)
INFO - root - 2017-12-07 20:17:37.458576: step 40940, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 74h:50m:12s remains)
INFO - root - 2017-12-07 20:17:47.039455: step 40950, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 77h:50m:58s remains)
INFO - root - 2017-12-07 20:17:56.703808: step 40960, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 78h:08m:54s remains)
INFO - root - 2017-12-07 20:18:06.309550: step 40970, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 79h:57m:39s remains)
INFO - root - 2017-12-07 20:18:16.100254: step 40980, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 79h:59m:02s remains)
INFO - root - 2017-12-07 20:18:25.940426: step 40990, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 77h:20m:36s remains)
INFO - root - 2017-12-07 20:18:35.537934: step 41000, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 76h:22m:41s remains)
2017-12-07 20:18:36.525753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1869593 -4.1733465 -4.1837034 -4.2145133 -4.2339005 -4.2260351 -4.1894636 -4.1253123 -4.0582128 -4.0600686 -4.1409273 -4.2258968 -4.27378 -4.2966986 -4.3151708][-4.2038841 -4.1890683 -4.1884732 -4.2059765 -4.2139258 -4.1979008 -4.16147 -4.1020212 -4.0431833 -4.0523849 -4.1389704 -4.2251067 -4.2731814 -4.2950878 -4.3129811][-4.2235279 -4.2131653 -4.2063184 -4.2088556 -4.1980925 -4.1665277 -4.1255803 -4.0690603 -4.0201979 -4.0351009 -4.1264772 -4.2196674 -4.2745452 -4.2971978 -4.314322][-4.2676415 -4.2657957 -4.2566395 -4.239933 -4.2016649 -4.146739 -4.0867281 -4.018116 -3.9769964 -4.009706 -4.1129093 -4.2128611 -4.2737036 -4.2998681 -4.3164663][-4.3067226 -4.3117919 -4.301528 -4.2677183 -4.2079749 -4.1346292 -4.0532961 -3.9716544 -3.9377728 -3.9916735 -4.1044431 -4.2066016 -4.2719779 -4.3013148 -4.3163342][-4.3279228 -4.3357 -4.3238912 -4.2763634 -4.2066813 -4.1251349 -4.0284023 -3.9349756 -3.9040627 -3.9707313 -4.0903735 -4.1943245 -4.2683825 -4.3030033 -4.3151274][-4.3325009 -4.3409376 -4.32623 -4.2741308 -4.2032442 -4.1192966 -4.0151706 -3.9146743 -3.8799169 -3.948482 -4.0751534 -4.1852446 -4.2659721 -4.304904 -4.3148632][-4.3331881 -4.3396835 -4.3243809 -4.274404 -4.2031007 -4.1212807 -4.0241122 -3.9365716 -3.9070728 -3.9688435 -4.086607 -4.1879182 -4.2648106 -4.3043995 -4.3149548][-4.3312831 -4.3351445 -4.322535 -4.2809672 -4.2134724 -4.1352305 -4.044816 -3.9737005 -3.9526572 -4.0046306 -4.1020684 -4.1907115 -4.2589498 -4.2977166 -4.3122077][-4.3242488 -4.3319983 -4.3261127 -4.2959638 -4.2393761 -4.1657495 -4.079957 -4.0179262 -3.9956849 -4.0351787 -4.1174555 -4.1991491 -4.2589812 -4.2925248 -4.3094578][-4.3160944 -4.3292928 -4.3289113 -4.3063474 -4.2579823 -4.193305 -4.1165185 -4.0585618 -4.0308976 -4.0606141 -4.1381874 -4.2194595 -4.2711215 -4.2963562 -4.3127251][-4.3012929 -4.3221545 -4.3291836 -4.3139205 -4.2719984 -4.2132568 -4.1422052 -4.0835204 -4.0488982 -4.0737882 -4.1546011 -4.2376189 -4.2826552 -4.3017573 -4.3165221][-4.280457 -4.3114343 -4.3295174 -4.3256397 -4.2935514 -4.2391014 -4.1688547 -4.1047206 -4.0577216 -4.07846 -4.1619091 -4.2444763 -4.2862062 -4.3029327 -4.318326][-4.2671528 -4.3052244 -4.33268 -4.3399 -4.3218493 -4.2739806 -4.2054439 -4.1389279 -4.0839324 -4.0994716 -4.1772022 -4.252099 -4.2902942 -4.3063092 -4.3214445][-4.2530847 -4.2922864 -4.3261719 -4.34314 -4.3375187 -4.3017774 -4.2437987 -4.182703 -4.1272717 -4.1346207 -4.1987309 -4.2626038 -4.2975698 -4.3145185 -4.3284545]]...]
INFO - root - 2017-12-07 20:18:46.269259: step 41010, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 78h:13m:22s remains)
INFO - root - 2017-12-07 20:18:56.197316: step 41020, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 78h:06m:45s remains)
INFO - root - 2017-12-07 20:19:05.764308: step 41030, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 77h:52m:28s remains)
INFO - root - 2017-12-07 20:19:15.397313: step 41040, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 78h:57m:17s remains)
INFO - root - 2017-12-07 20:19:25.119883: step 41050, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.028 sec/batch; 83h:13m:15s remains)
INFO - root - 2017-12-07 20:19:34.717735: step 41060, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 80h:42m:23s remains)
INFO - root - 2017-12-07 20:19:44.337162: step 41070, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 77h:04m:05s remains)
INFO - root - 2017-12-07 20:19:54.098121: step 41080, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 79h:11m:23s remains)
INFO - root - 2017-12-07 20:20:03.681060: step 41090, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.024 sec/batch; 82h:53m:00s remains)
INFO - root - 2017-12-07 20:20:13.342477: step 41100, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 80h:20m:50s remains)
2017-12-07 20:20:14.284252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1895452 -4.1992955 -4.2249584 -4.2448292 -4.2497859 -4.2449327 -4.2319341 -4.2206731 -4.2165475 -4.2163019 -4.2167883 -4.222127 -4.2427535 -4.2660074 -4.2880869][-4.1821666 -4.1908512 -4.2112622 -4.2265549 -4.2239804 -4.2118278 -4.1942949 -4.1838322 -4.186491 -4.1944461 -4.2033491 -4.2126737 -4.2348542 -4.26084 -4.2892246][-4.2063255 -4.2078667 -4.2150931 -4.2173519 -4.1974244 -4.1643853 -4.1352758 -4.1272645 -4.1458468 -4.1750293 -4.201055 -4.2218022 -4.247849 -4.2779007 -4.3085203][-4.2469196 -4.240273 -4.2277827 -4.2039385 -4.148994 -4.0817933 -4.0429239 -4.0508404 -4.0975614 -4.1537156 -4.2036018 -4.2405157 -4.2733221 -4.3052912 -4.3320661][-4.2870355 -4.2745104 -4.2399211 -4.1776543 -4.0727444 -3.9644914 -3.9292459 -3.9751482 -4.058445 -4.1394305 -4.209065 -4.2608695 -4.2983041 -4.3268847 -4.3454595][-4.3167076 -4.300303 -4.2471609 -4.1450448 -3.9895246 -3.8430116 -3.8238356 -3.9260118 -4.0529151 -4.1532865 -4.2282805 -4.2830329 -4.3204656 -4.34108 -4.3479476][-4.3283663 -4.3071032 -4.2415676 -4.1197348 -3.9504647 -3.8089762 -3.8220997 -3.961412 -4.1013856 -4.1973691 -4.2615967 -4.3079329 -4.3356905 -4.3455729 -4.3401508][-4.3255172 -4.3001146 -4.2324486 -4.1216736 -3.9891987 -3.909071 -3.9542608 -4.0772009 -4.1874971 -4.2544408 -4.2964129 -4.3239765 -4.3352661 -4.3321414 -4.3166614][-4.3172216 -4.2969513 -4.243865 -4.1636615 -4.0849586 -4.0591016 -4.1081152 -4.1952395 -4.2649026 -4.3009005 -4.3200612 -4.3286023 -4.322988 -4.3087978 -4.2896976][-4.3114486 -4.3054175 -4.274271 -4.2261372 -4.1882849 -4.188457 -4.2258143 -4.2781496 -4.3169646 -4.3322611 -4.3346033 -4.3290577 -4.3122621 -4.2937441 -4.2741313][-4.31969 -4.3245816 -4.30835 -4.2808809 -4.2659965 -4.2750788 -4.2998514 -4.329782 -4.3465309 -4.3453703 -4.3369942 -4.325613 -4.30534 -4.2872572 -4.2714753][-4.33465 -4.3407335 -4.3294258 -4.3111763 -4.3072414 -4.3172474 -4.3332782 -4.3458686 -4.3448648 -4.335258 -4.3262939 -4.3176637 -4.2992845 -4.2850904 -4.2727022][-4.3448086 -4.3420029 -4.3268929 -4.3132324 -4.3165073 -4.3246603 -4.3320551 -4.3312449 -4.3210111 -4.311698 -4.3076515 -4.3030124 -4.2887645 -4.2761426 -4.2661543][-4.347857 -4.3348279 -4.3172913 -4.308063 -4.3162484 -4.3180346 -4.3123107 -4.3008885 -4.2865644 -4.2821507 -4.2834339 -4.2817011 -4.2708554 -4.2609773 -4.2566648][-4.34572 -4.3304319 -4.3174019 -4.3130331 -4.3190227 -4.308229 -4.2871156 -4.267951 -4.2550693 -4.2535667 -4.257906 -4.2575879 -4.2516108 -4.2454286 -4.2447834]]...]
INFO - root - 2017-12-07 20:20:23.892202: step 41110, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.995 sec/batch; 80h:31m:17s remains)
INFO - root - 2017-12-07 20:20:33.608284: step 41120, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 79h:24m:50s remains)
INFO - root - 2017-12-07 20:20:43.432523: step 41130, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 78h:40m:33s remains)
INFO - root - 2017-12-07 20:20:53.139381: step 41140, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 77h:24m:40s remains)
INFO - root - 2017-12-07 20:21:02.721105: step 41150, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 80h:28m:21s remains)
INFO - root - 2017-12-07 20:21:12.410933: step 41160, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.997 sec/batch; 80h:38m:46s remains)
INFO - root - 2017-12-07 20:21:22.222896: step 41170, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 79h:54m:25s remains)
INFO - root - 2017-12-07 20:21:31.869240: step 41180, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 73h:39m:04s remains)
INFO - root - 2017-12-07 20:21:41.463768: step 41190, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 78h:50m:46s remains)
INFO - root - 2017-12-07 20:21:51.053441: step 41200, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 74h:41m:15s remains)
2017-12-07 20:21:51.991130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2732773 -4.26104 -4.2547183 -4.25593 -4.2624121 -4.27183 -4.2831955 -4.2958493 -4.3076978 -4.319171 -4.3305073 -4.339551 -4.3450031 -4.347929 -4.3493028][-4.3041124 -4.29271 -4.28466 -4.2820811 -4.2837667 -4.2886744 -4.2962146 -4.3053784 -4.3142457 -4.3235469 -4.3335085 -4.34185 -4.3470669 -4.3501329 -4.3520074][-4.3342133 -4.3275766 -4.3201976 -4.3136759 -4.309113 -4.3074493 -4.3090711 -4.3130593 -4.3184977 -4.3260417 -4.3351879 -4.3429332 -4.3479786 -4.3510818 -4.3530917][-4.3532052 -4.3495607 -4.33984 -4.3261819 -4.3123751 -4.3021803 -4.2972813 -4.2978778 -4.3034816 -4.3138185 -4.326726 -4.3378429 -4.3453779 -4.3501873 -4.3532367][-4.3527107 -4.3482232 -4.3330855 -4.3092022 -4.2826433 -4.2611427 -4.2497087 -4.2490282 -4.2583876 -4.2756906 -4.2966871 -4.3156157 -4.3302717 -4.3408756 -4.3482][-4.3222589 -4.3145685 -4.2942729 -4.2614107 -4.22306 -4.1908207 -4.1738281 -4.1725922 -4.1860185 -4.2115912 -4.2426867 -4.2723069 -4.2979364 -4.3185034 -4.3339658][-4.2700672 -4.26318 -4.2440424 -4.209425 -4.1641464 -4.12199 -4.0954528 -4.0869975 -4.0976377 -4.126616 -4.1666665 -4.2075691 -4.2453995 -4.2784672 -4.305057][-4.213264 -4.2192974 -4.21378 -4.1896653 -4.1467996 -4.099586 -4.0609426 -4.0367742 -4.0335903 -4.0551586 -4.0954571 -4.1424274 -4.1879983 -4.2305851 -4.2665086][-4.1713057 -4.1922922 -4.203866 -4.1979818 -4.1719561 -4.1355791 -4.0966563 -4.060813 -4.0403051 -4.04226 -4.0665479 -4.106122 -4.1492491 -4.1927705 -4.2315526][-4.1635141 -4.1913133 -4.2117186 -4.2184162 -4.2093368 -4.190619 -4.164 -4.1321449 -4.1063566 -4.0927043 -4.0954456 -4.1144953 -4.1428728 -4.1772738 -4.211607][-4.1946936 -4.2203856 -4.24223 -4.2549424 -4.2568612 -4.251946 -4.2394009 -4.2184081 -4.1958976 -4.1759186 -4.1630864 -4.1617732 -4.171165 -4.1895628 -4.2123423][-4.2389688 -4.2592888 -4.27956 -4.2952404 -4.3043346 -4.3082914 -4.3057117 -4.2942996 -4.2779727 -4.2594671 -4.2420921 -4.2300673 -4.2254577 -4.2290473 -4.2384219][-4.2764444 -4.2904181 -4.307467 -4.3229966 -4.3349333 -4.3428211 -4.3452282 -4.3410592 -4.3314819 -4.3178 -4.3025331 -4.2894926 -4.28118 -4.2786784 -4.2810845][-4.3012185 -4.3092685 -4.3213649 -4.332747 -4.3424377 -4.350502 -4.3555112 -4.356739 -4.3543143 -4.3478732 -4.3391337 -4.3309331 -4.3247094 -4.321291 -4.321022][-4.31446 -4.3184347 -4.3261852 -4.3344564 -4.3419657 -4.3488355 -4.3543839 -4.3580556 -4.3595858 -4.3585567 -4.3562317 -4.3535862 -4.3510284 -4.349226 -4.3487206]]...]
INFO - root - 2017-12-07 20:22:01.775751: step 41210, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 76h:17m:44s remains)
INFO - root - 2017-12-07 20:22:11.547644: step 41220, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 80h:54m:28s remains)
INFO - root - 2017-12-07 20:22:21.207641: step 41230, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 76h:39m:37s remains)
INFO - root - 2017-12-07 20:22:30.994694: step 41240, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 79h:57m:22s remains)
INFO - root - 2017-12-07 20:22:40.801726: step 41250, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 73h:40m:43s remains)
INFO - root - 2017-12-07 20:22:50.367526: step 41260, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 76h:51m:21s remains)
INFO - root - 2017-12-07 20:23:00.142034: step 41270, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.021 sec/batch; 82h:36m:39s remains)
INFO - root - 2017-12-07 20:23:09.896454: step 41280, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 77h:09m:31s remains)
INFO - root - 2017-12-07 20:23:19.348042: step 41290, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 77h:19m:59s remains)
INFO - root - 2017-12-07 20:23:29.098361: step 41300, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 80h:19m:30s remains)
2017-12-07 20:23:30.048044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3156934 -4.3208623 -4.3226843 -4.3192725 -4.3153486 -4.3118377 -4.3063688 -4.2992783 -4.296628 -4.2955976 -4.2887826 -4.2759514 -4.2709618 -4.2778921 -4.2878652][-4.2855382 -4.2967634 -4.3035908 -4.299427 -4.2916541 -4.2830267 -4.2728362 -4.2636814 -4.2639289 -4.2660818 -4.25423 -4.2297878 -4.2212453 -4.231698 -4.2467675][-4.2391233 -4.2562218 -4.2700624 -4.2685914 -4.2568727 -4.2380204 -4.21514 -4.1969094 -4.1995945 -4.2105112 -4.1988177 -4.1698871 -4.1637163 -4.1807775 -4.2016392][-4.2109571 -4.2279348 -4.2422714 -4.2401237 -4.2190475 -4.18682 -4.1492229 -4.12173 -4.1303244 -4.1576133 -4.1564751 -4.1325502 -4.1341124 -4.1591492 -4.1827078][-4.1976404 -4.2050757 -4.2157049 -4.2064915 -4.1673098 -4.1091623 -4.047791 -4.0065236 -4.0277848 -4.0860772 -4.1117783 -4.1075191 -4.1245871 -4.1607456 -4.1881871][-4.1746817 -4.1734996 -4.1777039 -4.1599488 -4.097868 -4.0022535 -3.897572 -3.8260474 -3.866643 -3.9742157 -4.0444241 -4.0715537 -4.108892 -4.1578732 -4.1960654][-4.1291146 -4.1269026 -4.1302023 -4.1080313 -4.033514 -3.9141681 -3.7664642 -3.6558337 -3.7147207 -3.872628 -3.9879892 -4.0434918 -4.0925021 -4.1457624 -4.1934609][-4.0810647 -4.0878406 -4.0983896 -4.084558 -4.0293608 -3.9368169 -3.8114874 -3.7113919 -3.7626557 -3.9098952 -4.0229826 -4.0801225 -4.1205225 -4.1646 -4.2080393][-4.0591044 -4.0764112 -4.0971308 -4.1023116 -4.0832624 -4.0411959 -3.972297 -3.9165673 -3.9498127 -4.0418878 -4.1167908 -4.1552458 -4.1828561 -4.21507 -4.2458544][-4.0859823 -4.096611 -4.1169376 -4.1392326 -4.1493983 -4.1432166 -4.1147628 -4.0911822 -4.1093578 -4.1537771 -4.1879735 -4.2064905 -4.2299237 -4.2574606 -4.2798305][-4.150939 -4.1534829 -4.1681285 -4.1926255 -4.2126155 -4.2203155 -4.2106743 -4.20165 -4.2092366 -4.22284 -4.227222 -4.2302446 -4.2509241 -4.2784019 -4.2989411][-4.1998773 -4.2028174 -4.2150497 -4.2378917 -4.2609067 -4.272058 -4.2719703 -4.2673845 -4.2670546 -4.2610097 -4.2453437 -4.2375703 -4.2570782 -4.2842927 -4.3053808][-4.2346067 -4.2438912 -4.2597885 -4.2777138 -4.2934918 -4.3003 -4.3019338 -4.298584 -4.2931652 -4.2770405 -4.249042 -4.2312975 -4.2456779 -4.2722564 -4.2966113][-4.2607431 -4.2738309 -4.29085 -4.3036757 -4.3103361 -4.3108335 -4.3111262 -4.3083787 -4.3004146 -4.2788568 -4.2436833 -4.2210383 -4.2315383 -4.2566147 -4.2842379][-4.28162 -4.2928672 -4.3100433 -4.319108 -4.3201151 -4.3186903 -4.317452 -4.3153949 -4.308176 -4.2854548 -4.2505713 -4.2297387 -4.2388759 -4.2612858 -4.2863078]]...]
INFO - root - 2017-12-07 20:23:39.685207: step 41310, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 75h:52m:55s remains)
INFO - root - 2017-12-07 20:23:49.295526: step 41320, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 74h:22m:31s remains)
INFO - root - 2017-12-07 20:23:58.905114: step 41330, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 77h:21m:13s remains)
INFO - root - 2017-12-07 20:24:08.645791: step 41340, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 77h:21m:05s remains)
INFO - root - 2017-12-07 20:24:18.296614: step 41350, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.917 sec/batch; 74h:12m:02s remains)
INFO - root - 2017-12-07 20:24:27.855422: step 41360, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 73h:37m:27s remains)
INFO - root - 2017-12-07 20:24:37.678637: step 41370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 78h:34m:15s remains)
INFO - root - 2017-12-07 20:24:47.131111: step 41380, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 73h:33m:12s remains)
INFO - root - 2017-12-07 20:24:56.761120: step 41390, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 79h:41m:17s remains)
INFO - root - 2017-12-07 20:25:06.540215: step 41400, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.014 sec/batch; 81h:57m:50s remains)
2017-12-07 20:25:07.470058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3251562 -4.3150678 -4.3071556 -4.3038859 -4.3044496 -4.3048158 -4.3064728 -4.3102918 -4.31322 -4.3146076 -4.3150821 -4.3142772 -4.3147306 -4.3170967 -4.3206873][-4.3192158 -4.3082271 -4.2999773 -4.298245 -4.3018847 -4.3032103 -4.3061194 -4.3135686 -4.3184886 -4.3196473 -4.3180137 -4.3125906 -4.3077712 -4.3057132 -4.3053517][-4.3108258 -4.29699 -4.285686 -4.2834477 -4.2871943 -4.2873821 -4.2925835 -4.3064628 -4.3168416 -4.3203354 -4.317502 -4.3052664 -4.2944536 -4.2872949 -4.2818613][-4.2891974 -4.2688975 -4.2515168 -4.2470241 -4.2474184 -4.2413111 -4.2450418 -4.2660861 -4.2846756 -4.2926521 -4.2930746 -4.2783113 -4.2638373 -4.2535748 -4.2451096][-4.2556329 -4.2248335 -4.1973882 -4.1841593 -4.1766696 -4.159862 -4.156199 -4.1844893 -4.2108812 -4.2221742 -4.2294774 -4.2233081 -4.2134485 -4.20752 -4.2019057][-4.2211418 -4.1752272 -4.1279821 -4.0932808 -4.0669317 -4.0286326 -4.0108757 -4.0548329 -4.0971136 -4.1115575 -4.1242008 -4.1309161 -4.1327887 -4.1396494 -4.1474524][-4.1917953 -4.1356339 -4.0747218 -4.0179906 -3.9681273 -3.8969495 -3.8557868 -3.9243741 -3.9917114 -4.01372 -4.033587 -4.0486479 -4.05706 -4.0767646 -4.1020856][-4.1803703 -4.1260195 -4.0689435 -4.0089278 -3.9474549 -3.8549407 -3.7979674 -3.8845329 -3.9667485 -3.993077 -4.0158091 -4.02857 -4.0306277 -4.0512261 -4.0835819][-4.1887755 -4.1402922 -4.0946321 -4.052649 -4.0086594 -3.9378068 -3.9037406 -3.9747725 -4.0356293 -4.0498519 -4.0679064 -4.075551 -4.0643954 -4.0728655 -4.096303][-4.203218 -4.1608086 -4.124846 -4.1017809 -4.0782752 -4.0347567 -4.0186043 -4.0690689 -4.10612 -4.1084986 -4.1215625 -4.1251693 -4.1026711 -4.097362 -4.11031][-4.2200317 -4.1846 -4.1548667 -4.1421709 -4.1301384 -4.103044 -4.094574 -4.1283131 -4.1492186 -4.1462088 -4.1545515 -4.1540761 -4.1293869 -4.1211381 -4.1314454][-4.2380309 -4.2121477 -4.1916041 -4.1836624 -4.1735058 -4.1563392 -4.155026 -4.177927 -4.1893415 -4.1829634 -4.1863232 -4.1831493 -4.1645365 -4.1639996 -4.175612][-4.25962 -4.2417135 -4.2288337 -4.2223082 -4.2113218 -4.1998749 -4.2031231 -4.2194366 -4.224865 -4.2171702 -4.2168722 -4.2143192 -4.2044539 -4.2098861 -4.2227154][-4.2818613 -4.2711129 -4.2640152 -4.2585764 -4.2490416 -4.2407656 -4.2452826 -4.2577171 -4.2593708 -4.2536826 -4.2530794 -4.2543693 -4.2507696 -4.2570024 -4.2686834][-4.3050704 -4.2990823 -4.2950969 -4.29132 -4.2852411 -4.2804136 -4.2840958 -4.2918625 -4.2917695 -4.2898159 -4.2903032 -4.2921934 -4.2921805 -4.297061 -4.3034258]]...]
INFO - root - 2017-12-07 20:25:17.177104: step 41410, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.946 sec/batch; 76h:28m:19s remains)
INFO - root - 2017-12-07 20:25:26.880592: step 41420, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 76h:16m:55s remains)
INFO - root - 2017-12-07 20:25:36.314643: step 41430, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 72h:24m:40s remains)
INFO - root - 2017-12-07 20:25:46.001323: step 41440, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 79h:08m:38s remains)
INFO - root - 2017-12-07 20:25:55.714069: step 41450, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 80h:02m:15s remains)
INFO - root - 2017-12-07 20:26:05.227460: step 41460, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 78h:40m:22s remains)
INFO - root - 2017-12-07 20:26:14.988301: step 41470, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.985 sec/batch; 79h:38m:44s remains)
INFO - root - 2017-12-07 20:26:24.465760: step 41480, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 79h:15m:12s remains)
INFO - root - 2017-12-07 20:26:34.117010: step 41490, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 79h:29m:41s remains)
INFO - root - 2017-12-07 20:26:43.742809: step 41500, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.012 sec/batch; 81h:47m:05s remains)
2017-12-07 20:26:44.736400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1925359 -4.207828 -4.2260022 -4.2517161 -4.2859192 -4.3073673 -4.2926893 -4.2572365 -4.233696 -4.2296643 -4.240706 -4.2416129 -4.23286 -4.21869 -4.189476][-4.191772 -4.2127814 -4.2361736 -4.2659187 -4.2982893 -4.310781 -4.288096 -4.2488427 -4.225 -4.2266254 -4.2458396 -4.2528272 -4.2469177 -4.2384772 -4.2155118][-4.184546 -4.2160854 -4.2506537 -4.2848587 -4.3089209 -4.3057609 -4.2684312 -4.2181077 -4.1910052 -4.1999369 -4.2328491 -4.2544889 -4.2643766 -4.2689824 -4.2578554][-4.1922135 -4.2366443 -4.2760882 -4.299192 -4.3019772 -4.2800875 -4.2298508 -4.1696587 -4.1411562 -4.1627016 -4.2137618 -4.2544961 -4.2811365 -4.3014922 -4.3072534][-4.2172771 -4.2557187 -4.2828431 -4.2879782 -4.2688336 -4.2292933 -4.1642766 -4.097939 -4.0812249 -4.1272635 -4.199254 -4.2525597 -4.2868128 -4.3147411 -4.3315535][-4.2439151 -4.2634177 -4.271246 -4.2591953 -4.2239347 -4.1636648 -4.0722437 -3.9991941 -4.0155311 -4.0990443 -4.1898937 -4.246726 -4.2779846 -4.302886 -4.3228736][-4.2485228 -4.2477016 -4.2373791 -4.2155061 -4.1660643 -4.0835085 -3.9616127 -3.8811848 -3.9476607 -4.0723524 -4.1762705 -4.2316904 -4.255733 -4.2761583 -4.2929416][-4.2188482 -4.2058058 -4.194797 -4.1807113 -4.1328764 -4.0418921 -3.9114842 -3.8413887 -3.943316 -4.0777149 -4.1742778 -4.2210383 -4.2371659 -4.2505636 -4.2611322][-4.1801748 -4.16657 -4.1668949 -4.1631465 -4.1296778 -4.0613151 -3.9722447 -3.9459527 -4.0318084 -4.1318035 -4.1999369 -4.2287307 -4.2361994 -4.2407417 -4.2453208][-4.1554675 -4.1468554 -4.1550279 -4.1604552 -4.1444697 -4.1079845 -4.0665784 -4.0702949 -4.1331406 -4.1991796 -4.2427597 -4.2577596 -4.2583823 -4.257648 -4.2581496][-4.1493921 -4.149579 -4.1621246 -4.1724052 -4.1713405 -4.159441 -4.1453328 -4.15878 -4.2038832 -4.2481623 -4.2769046 -4.2844815 -4.2827082 -4.2809057 -4.2823048][-4.1567926 -4.1616554 -4.1746073 -4.1874995 -4.1968017 -4.1992555 -4.1970277 -4.208663 -4.2407079 -4.2730789 -4.2912283 -4.2940364 -4.2901063 -4.2895594 -4.2957268][-4.1826677 -4.1881514 -4.1960373 -4.20717 -4.2195616 -4.226542 -4.2280903 -4.2378154 -4.2628012 -4.2868552 -4.2959304 -4.2925143 -4.2837734 -4.2835298 -4.2945981][-4.2206035 -4.2265582 -4.2282753 -4.2307763 -4.2382693 -4.2440534 -4.2456269 -4.2536016 -4.2749748 -4.2905693 -4.2899961 -4.2782035 -4.2643952 -4.2654071 -4.2809653][-4.2564788 -4.2655725 -4.2673678 -4.2638936 -4.2620492 -4.2596469 -4.2563534 -4.2616043 -4.2784791 -4.2860117 -4.2766905 -4.2584457 -4.2417684 -4.2451634 -4.2656865]]...]
INFO - root - 2017-12-07 20:26:54.395332: step 41510, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.977 sec/batch; 78h:56m:31s remains)
INFO - root - 2017-12-07 20:27:04.271935: step 41520, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 79h:33m:35s remains)
INFO - root - 2017-12-07 20:27:13.712188: step 41530, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 80h:31m:33s remains)
INFO - root - 2017-12-07 20:27:23.277521: step 41540, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 78h:43m:22s remains)
INFO - root - 2017-12-07 20:27:32.963625: step 41550, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 77h:12m:04s remains)
INFO - root - 2017-12-07 20:27:42.649270: step 41560, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 77h:34m:37s remains)
INFO - root - 2017-12-07 20:27:52.258577: step 41570, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 77h:44m:36s remains)
INFO - root - 2017-12-07 20:28:01.787583: step 41580, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 80h:52m:50s remains)
INFO - root - 2017-12-07 20:28:11.475215: step 41590, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 80h:25m:29s remains)
INFO - root - 2017-12-07 20:28:20.943152: step 41600, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.982 sec/batch; 79h:23m:05s remains)
2017-12-07 20:28:21.964234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3337121 -4.3318105 -4.3256278 -4.3221378 -4.3199077 -4.3171935 -4.3144236 -4.3135843 -4.3152881 -4.3171606 -4.318943 -4.3233838 -4.3293877 -4.3353024 -4.340549][-4.3325529 -4.3293896 -4.318738 -4.3081222 -4.2994342 -4.2922511 -4.2883635 -4.2891116 -4.2947264 -4.3000526 -4.3041053 -4.3115392 -4.3208375 -4.3306293 -4.3398309][-4.3213382 -4.3146434 -4.2977848 -4.2783751 -4.2607665 -4.2473 -4.2434325 -4.2487698 -4.2633615 -4.2791681 -4.2910042 -4.3025556 -4.3129897 -4.3229966 -4.3331547][-4.3000374 -4.2887883 -4.2663407 -4.2398415 -4.2130647 -4.1936588 -4.1890178 -4.1978512 -4.2208562 -4.2489476 -4.2709613 -4.2869058 -4.2981958 -4.3067536 -4.315732][-4.2790213 -4.2621827 -4.2354579 -4.2023935 -4.1667495 -4.1396003 -4.1299448 -4.1400228 -4.1713815 -4.2091513 -4.2378678 -4.2564063 -4.268641 -4.2778478 -4.287498][-4.2666316 -4.2447348 -4.213408 -4.1728525 -4.1233487 -4.0771179 -4.0467672 -4.055028 -4.10363 -4.1591635 -4.1989884 -4.2221246 -4.2379169 -4.2510853 -4.2643924][-4.2697854 -4.2487774 -4.2136812 -4.1620059 -4.0910449 -4.0110931 -3.9438264 -3.9488194 -4.02838 -4.1109285 -4.1644421 -4.1957097 -4.2191973 -4.2377844 -4.2564435][-4.2693896 -4.2574015 -4.2254977 -4.1690359 -4.0787139 -3.9603937 -3.8452506 -3.8403561 -3.9559031 -4.0645933 -4.1308212 -4.1701412 -4.2020893 -4.2296896 -4.2551961][-4.2517939 -4.2542558 -4.2398071 -4.1972218 -4.1169233 -4.0025382 -3.8812881 -3.8617764 -3.962616 -4.0542479 -4.1105237 -4.1480923 -4.1815472 -4.2143683 -4.24559][-4.2215166 -4.2402611 -4.2479258 -4.2293162 -4.182044 -4.1145382 -4.0452709 -4.0218139 -4.0553851 -4.0882792 -4.1160359 -4.1445131 -4.171072 -4.2008185 -4.2345738][-4.1921053 -4.2285891 -4.2553511 -4.2520161 -4.2255063 -4.1960526 -4.1721854 -4.15055 -4.1428237 -4.1411519 -4.1512437 -4.1659193 -4.1765757 -4.1993632 -4.2334681][-4.1774287 -4.2197375 -4.2568035 -4.2624488 -4.24598 -4.2343411 -4.2408366 -4.2314272 -4.2117109 -4.1960111 -4.1954131 -4.2005758 -4.1978779 -4.2115641 -4.2441883][-4.1886692 -4.2128468 -4.2459111 -4.2554989 -4.2500043 -4.2508583 -4.2681127 -4.2700028 -4.2560558 -4.2385411 -4.2320867 -4.2302933 -4.2237425 -4.2357545 -4.2643905][-4.2327318 -4.2300777 -4.244801 -4.2541151 -4.2554555 -4.26067 -4.2736659 -4.2731886 -4.2682209 -4.2585335 -4.2546525 -4.2556639 -4.2576084 -4.2708111 -4.2919483][-4.2800541 -4.2573872 -4.2500958 -4.2541113 -4.2613859 -4.2685251 -4.2766595 -4.2753115 -4.2741494 -4.2729082 -4.2762408 -4.2856083 -4.2973242 -4.3098412 -4.324151]]...]
INFO - root - 2017-12-07 20:28:31.631638: step 41610, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 80h:48m:44s remains)
INFO - root - 2017-12-07 20:28:41.292777: step 41620, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.022 sec/batch; 82h:36m:53s remains)
INFO - root - 2017-12-07 20:28:50.915749: step 41630, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 75h:44m:41s remains)
INFO - root - 2017-12-07 20:29:00.504811: step 41640, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 76h:56m:56s remains)
INFO - root - 2017-12-07 20:29:10.119543: step 41650, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.030 sec/batch; 83h:12m:13s remains)
INFO - root - 2017-12-07 20:29:19.802059: step 41660, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 80h:42m:58s remains)
INFO - root - 2017-12-07 20:29:29.460314: step 41670, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 76h:46m:32s remains)
INFO - root - 2017-12-07 20:29:38.962072: step 41680, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.917 sec/batch; 74h:05m:44s remains)
INFO - root - 2017-12-07 20:29:48.616933: step 41690, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 80h:34m:13s remains)
INFO - root - 2017-12-07 20:29:58.211217: step 41700, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 78h:37m:43s remains)
2017-12-07 20:29:59.300695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.182601 -4.1636906 -4.146893 -4.1311636 -4.1266232 -4.1379676 -4.170155 -4.221827 -4.2503505 -4.2438736 -4.2358356 -4.2429132 -4.2471724 -4.2429643 -4.2285247][-4.1907969 -4.1762891 -4.1698165 -4.1656322 -4.1654639 -4.1669269 -4.1871519 -4.2266989 -4.2429686 -4.2300344 -4.2292042 -4.2424774 -4.2497411 -4.2529159 -4.2514505][-4.1904116 -4.1804848 -4.1854715 -4.1915736 -4.1896772 -4.1870995 -4.2000375 -4.2249031 -4.227386 -4.2098722 -4.214262 -4.2310839 -4.2411451 -4.2519078 -4.2622161][-4.1669483 -4.1671691 -4.1804457 -4.1843185 -4.17851 -4.17682 -4.189322 -4.2054009 -4.1987314 -4.1837993 -4.1917229 -4.2106252 -4.2286978 -4.2490482 -4.2647243][-4.1341238 -4.1535778 -4.1764064 -4.1763577 -4.1683021 -4.167408 -4.1802964 -4.1863427 -4.1699367 -4.1552491 -4.1663222 -4.1920896 -4.2203555 -4.2471108 -4.2650623][-4.1135216 -4.1541748 -4.1839895 -4.1804152 -4.1662865 -4.1650071 -4.1727939 -4.1687474 -4.1448779 -4.1340828 -4.1543412 -4.1891308 -4.2225575 -4.2493758 -4.2668142][-4.1217928 -4.172431 -4.202199 -4.1950755 -4.1751771 -4.1676259 -4.1599016 -4.1424913 -4.1232162 -4.1250491 -4.1554432 -4.1986928 -4.2318454 -4.2535715 -4.2662272][-4.1675062 -4.2139692 -4.2332282 -4.2188315 -4.1940026 -4.1792555 -4.1585054 -4.1343703 -4.1238666 -4.1366858 -4.1742806 -4.2206821 -4.2492933 -4.265799 -4.273212][-4.2234945 -4.2548614 -4.2571406 -4.2296124 -4.2008448 -4.1861253 -4.1655755 -4.1441207 -4.1387172 -4.1581421 -4.19788 -4.2424316 -4.2696419 -4.2839775 -4.2889533][-4.2668877 -4.2814164 -4.2689276 -4.2353945 -4.2064939 -4.1927071 -4.1730881 -4.1575217 -4.1566463 -4.1806035 -4.2166829 -4.2534957 -4.2765546 -4.2899928 -4.2949462][-4.28736 -4.2841821 -4.2644081 -4.2361336 -4.2163372 -4.2064443 -4.19199 -4.1858568 -4.19377 -4.2135777 -4.2346134 -4.255897 -4.2716956 -4.2822685 -4.2878666][-4.2795181 -4.2650042 -4.2443104 -4.2306509 -4.2275834 -4.2276397 -4.220612 -4.2209496 -4.2306604 -4.2396383 -4.2434011 -4.2499771 -4.2605958 -4.2702093 -4.2772751][-4.2505374 -4.2271018 -4.2096114 -4.2166333 -4.2349486 -4.2472758 -4.2458105 -4.2454376 -4.2454062 -4.2396746 -4.2329826 -4.2337985 -4.2485785 -4.2628708 -4.267374][-4.2092347 -4.1794376 -4.1666594 -4.1936493 -4.2317157 -4.2563663 -4.2604284 -4.2573724 -4.247333 -4.2273607 -4.213 -4.2147141 -4.2396665 -4.262898 -4.2705483][-4.1701217 -4.1394548 -4.1309667 -4.1684308 -4.2166643 -4.2459164 -4.252665 -4.2519331 -4.2404213 -4.2142115 -4.192709 -4.2010169 -4.2370863 -4.2664685 -4.276741]]...]
INFO - root - 2017-12-07 20:30:08.975400: step 41710, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 80h:01m:16s remains)
INFO - root - 2017-12-07 20:30:18.748311: step 41720, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.021 sec/batch; 82h:30m:12s remains)
INFO - root - 2017-12-07 20:30:28.427824: step 41730, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 80h:10m:23s remains)
INFO - root - 2017-12-07 20:30:37.903682: step 41740, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 72h:36m:34s remains)
INFO - root - 2017-12-07 20:30:47.515235: step 41750, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 75h:51m:18s remains)
INFO - root - 2017-12-07 20:30:57.280780: step 41760, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 80h:17m:37s remains)
INFO - root - 2017-12-07 20:31:06.743536: step 41770, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 75h:10m:42s remains)
INFO - root - 2017-12-07 20:31:16.281661: step 41780, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 72h:48m:24s remains)
INFO - root - 2017-12-07 20:31:26.065022: step 41790, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 75h:17m:38s remains)
INFO - root - 2017-12-07 20:31:35.757478: step 41800, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.993 sec/batch; 80h:12m:05s remains)
2017-12-07 20:31:36.849831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2293062 -4.2382026 -4.257462 -4.2685685 -4.2678227 -4.2576885 -4.243588 -4.2377162 -4.2434006 -4.2531528 -4.2579184 -4.2520723 -4.2428317 -4.2430024 -4.252924][-4.2303333 -4.2456617 -4.2679377 -4.2767096 -4.2709689 -4.2570052 -4.2416062 -4.2381206 -4.2461982 -4.2550573 -4.2563329 -4.24626 -4.235023 -4.2365651 -4.2500281][-4.248343 -4.2686009 -4.29084 -4.2954679 -4.2835083 -4.2654352 -4.249527 -4.2497983 -4.2619281 -4.2719903 -4.27038 -4.2566381 -4.2430239 -4.2423782 -4.2535043][-4.2471452 -4.2706161 -4.2930379 -4.2958465 -4.2808189 -4.2611542 -4.2483764 -4.2534633 -4.2705264 -4.2828374 -4.2820134 -4.2697892 -4.2559881 -4.2517481 -4.2565928][-4.2295809 -4.253953 -4.2758608 -4.2752657 -4.2533503 -4.2264371 -4.2114286 -4.2229996 -4.2494073 -4.2682815 -4.2749114 -4.2710209 -4.2612605 -4.2558007 -4.2564678][-4.1892776 -4.2118731 -4.2325168 -4.22811 -4.1961985 -4.1543374 -4.1290526 -4.1490045 -4.1936049 -4.226717 -4.2451348 -4.2506104 -4.2461786 -4.2411561 -4.2388773][-4.1285348 -4.1497641 -4.1695848 -4.1616893 -4.1176429 -4.0564308 -4.0165529 -4.048945 -4.1204147 -4.1729622 -4.2022805 -4.2138028 -4.2110572 -4.2029943 -4.1944375][-4.0980973 -4.1193318 -4.1392479 -4.1320744 -4.0859966 -4.0192556 -3.9725251 -4.0071735 -4.0882974 -4.1478634 -4.1794815 -4.192306 -4.1888022 -4.1766434 -4.1583972][-4.1397557 -4.1632919 -4.1822886 -4.177722 -4.1463561 -4.1032295 -4.07306 -4.0937686 -4.1470351 -4.18386 -4.1985283 -4.2001228 -4.1902127 -4.1733861 -4.1482906][-4.1866922 -4.2095313 -4.2250953 -4.2201118 -4.2015533 -4.1796079 -4.1627445 -4.1698885 -4.194541 -4.2143397 -4.2215853 -4.2206836 -4.2132325 -4.2000804 -4.1743069][-4.2176185 -4.2310891 -4.23687 -4.2306538 -4.2217693 -4.2159853 -4.2129078 -4.2155442 -4.2231903 -4.2303915 -4.2349916 -4.2382755 -4.2402306 -4.2368679 -4.2168269][-4.2097931 -4.2103577 -4.2102485 -4.206986 -4.2111254 -4.2233543 -4.2345486 -4.2410212 -4.2408772 -4.2392144 -4.2408829 -4.2464967 -4.255558 -4.2626052 -4.2519789][-4.1775885 -4.1662879 -4.1643658 -4.1641 -4.1754384 -4.1950488 -4.210012 -4.2158608 -4.2092628 -4.2016163 -4.2018046 -4.2121811 -4.2291479 -4.2434087 -4.2404962][-4.1234617 -4.1064081 -4.1059127 -4.1112928 -4.1280441 -4.1498637 -4.1591997 -4.1584358 -4.1455579 -4.1307359 -4.1281786 -4.1427245 -4.1674085 -4.1915684 -4.1998034][-4.0617485 -4.0463724 -4.05154 -4.0645347 -4.085916 -4.1072378 -4.1098585 -4.1001616 -4.0830574 -4.0692787 -4.0711594 -4.0893464 -4.1170058 -4.1446161 -4.1593285]]...]
INFO - root - 2017-12-07 20:31:46.665683: step 41810, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 74h:52m:09s remains)
INFO - root - 2017-12-07 20:31:56.342250: step 41820, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 80h:29m:47s remains)
INFO - root - 2017-12-07 20:32:06.035917: step 41830, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.015 sec/batch; 81h:57m:31s remains)
INFO - root - 2017-12-07 20:32:15.730533: step 41840, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 79h:08m:10s remains)
INFO - root - 2017-12-07 20:32:25.381763: step 41850, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 75h:41m:22s remains)
INFO - root - 2017-12-07 20:32:35.134824: step 41860, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 79h:23m:59s remains)
INFO - root - 2017-12-07 20:32:44.911303: step 41870, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 80h:29m:19s remains)
INFO - root - 2017-12-07 20:32:54.534244: step 41880, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 78h:40m:35s remains)
INFO - root - 2017-12-07 20:33:04.168991: step 41890, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 81h:04m:43s remains)
INFO - root - 2017-12-07 20:33:13.991059: step 41900, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.027 sec/batch; 82h:51m:48s remains)
2017-12-07 20:33:14.916217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2244864 -4.2201877 -4.2159138 -4.2151132 -4.2171278 -4.2169929 -4.2154078 -4.2146845 -4.220149 -4.2271538 -4.228056 -4.2268257 -4.2301121 -4.233089 -4.2363963][-4.2397208 -4.2264438 -4.2152572 -4.2085447 -4.2071371 -4.2068973 -4.2040014 -4.2014956 -4.2023268 -4.207098 -4.2125921 -4.2186351 -4.224587 -4.2285781 -4.2354717][-4.2657566 -4.2482305 -4.2336965 -4.222611 -4.2122068 -4.2039323 -4.1898847 -4.1772914 -4.1698871 -4.171731 -4.1830564 -4.1980109 -4.2057438 -4.2101707 -4.2273779][-4.2728324 -4.251164 -4.2340679 -4.2215586 -4.2035937 -4.1824546 -4.1526437 -4.1272631 -4.1143503 -4.1169968 -4.1404762 -4.1626091 -4.167336 -4.1734796 -4.2040391][-4.2496691 -4.2226481 -4.2011733 -4.1871462 -4.1660094 -4.1376276 -4.0936208 -4.0528669 -4.0352364 -4.0459747 -4.0856009 -4.1117063 -4.1118546 -4.1200995 -4.1623783][-4.197588 -4.1685061 -4.1423316 -4.1265979 -4.1080637 -4.0773149 -4.0217342 -3.9613726 -3.9354889 -3.959378 -4.0217447 -4.0564723 -4.0531464 -4.0580845 -4.1081424][-4.1333761 -4.1074028 -4.0772152 -4.0558233 -4.0374146 -4.0087972 -3.9443891 -3.8592384 -3.819159 -3.8647592 -3.9596882 -4.0100694 -4.0071511 -4.0106344 -4.0600634][-4.1062403 -4.0788016 -4.048213 -4.0264392 -4.0076013 -3.9800253 -3.9184449 -3.8246436 -3.7738028 -3.8283083 -3.9370041 -3.9952574 -3.9937339 -3.9933953 -4.0326486][-4.1391897 -4.108696 -4.0814805 -4.0617666 -4.0426297 -4.0224752 -3.9803817 -3.9103298 -3.8646636 -3.9019597 -3.9867384 -4.0295911 -4.0236478 -4.0193105 -4.0497627][-4.1864839 -4.1580029 -4.1397891 -4.1256385 -4.111156 -4.1006651 -4.080267 -4.0390811 -4.0050898 -4.0226331 -4.0711656 -4.08788 -4.0770693 -4.0759559 -4.1020021][-4.21538 -4.1949148 -4.1865811 -4.1779723 -4.1700416 -4.1674004 -4.1620636 -4.1407385 -4.120544 -4.131588 -4.155724 -4.1534448 -4.1369705 -4.1388292 -4.162787][-4.2336426 -4.2240429 -4.2218051 -4.2130985 -4.2100415 -4.2140303 -4.2143416 -4.20538 -4.1962762 -4.2064185 -4.2197847 -4.2112284 -4.1954703 -4.1985493 -4.2191014][-4.2428975 -4.2399344 -4.2382364 -4.2282782 -4.2290835 -4.2381458 -4.2437534 -4.2439423 -4.244143 -4.2538257 -4.261065 -4.2525477 -4.2397137 -4.2421603 -4.2592077][-4.2447386 -4.2448659 -4.2396984 -4.2243471 -4.2233706 -4.2372165 -4.2482343 -4.2560768 -4.2648835 -4.2769341 -4.2832446 -4.27693 -4.2651844 -4.2637324 -4.2758222][-4.2319112 -4.2348928 -4.2259917 -4.2065096 -4.2009168 -4.212986 -4.2240076 -4.23464 -4.2498946 -4.2663364 -4.2740064 -4.2710705 -4.261076 -4.2574596 -4.2650228]]...]
INFO - root - 2017-12-07 20:33:24.549545: step 41910, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 76h:54m:46s remains)
INFO - root - 2017-12-07 20:33:34.275961: step 41920, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 80h:32m:30s remains)
INFO - root - 2017-12-07 20:33:43.835808: step 41930, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 75h:53m:22s remains)
INFO - root - 2017-12-07 20:33:53.480580: step 41940, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 77h:53m:34s remains)
INFO - root - 2017-12-07 20:34:03.270805: step 41950, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 77h:16m:56s remains)
INFO - root - 2017-12-07 20:34:12.989587: step 41960, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 78h:14m:37s remains)
INFO - root - 2017-12-07 20:34:22.671054: step 41970, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 81h:39m:48s remains)
INFO - root - 2017-12-07 20:34:32.325669: step 41980, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 78h:00m:19s remains)
INFO - root - 2017-12-07 20:34:42.172438: step 41990, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 80h:10m:02s remains)
INFO - root - 2017-12-07 20:34:51.609172: step 42000, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 75h:30m:15s remains)
2017-12-07 20:34:52.534953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2388358 -4.2331219 -4.2346725 -4.2383056 -4.24363 -4.2462454 -4.2300916 -4.2013512 -4.1653337 -4.1302238 -4.1297812 -4.1547308 -4.1773419 -4.1894512 -4.2110724][-4.2293134 -4.2127342 -4.2126327 -4.2252421 -4.2403374 -4.2493744 -4.2323031 -4.1998196 -4.1638265 -4.1358414 -4.1424856 -4.16269 -4.173718 -4.174108 -4.1901116][-4.2189436 -4.1885276 -4.1835279 -4.19909 -4.2189312 -4.230423 -4.2159686 -4.1892657 -4.1707106 -4.1616273 -4.1709957 -4.1764946 -4.163938 -4.14719 -4.1521134][-4.200254 -4.1600204 -4.1485486 -4.1609855 -4.1739807 -4.1790934 -4.1616416 -4.1389842 -4.1458797 -4.1615491 -4.1779737 -4.1728225 -4.1380706 -4.1077495 -4.1062875][-4.1816835 -4.141449 -4.1263137 -4.1316066 -4.1232219 -4.1023755 -4.067378 -4.0510006 -4.090868 -4.1353474 -4.159574 -4.1499472 -4.1023674 -4.0672407 -4.0651269][-4.1673546 -4.130404 -4.112782 -4.1021385 -4.0569015 -3.98487 -3.908025 -3.9003725 -3.9979281 -4.0845847 -4.1284719 -4.1303005 -4.0931811 -4.06645 -4.0672917][-4.1505995 -4.1166468 -4.0911841 -4.0545607 -3.9677331 -3.8217778 -3.6693356 -3.6778388 -3.8678863 -4.0169396 -4.0878134 -4.1070662 -4.091042 -4.0843244 -4.0957146][-4.1504116 -4.1210775 -4.0874171 -4.0321121 -3.9238698 -3.7398279 -3.5505023 -3.582571 -3.8228192 -3.9916003 -4.064393 -4.0845132 -4.0796666 -4.0894594 -4.1132226][-4.154561 -4.1323013 -4.1034307 -4.058351 -3.9807315 -3.8501382 -3.7291687 -3.760818 -3.9225478 -4.0325189 -4.0732555 -4.0826883 -4.0809441 -4.0964842 -4.1243782][-4.1660762 -4.1520662 -4.1354289 -4.1108251 -4.0670834 -3.9952877 -3.9379458 -3.955615 -4.0375037 -4.0920658 -4.1082168 -4.108067 -4.1024289 -4.1118064 -4.135159][-4.1929955 -4.1819935 -4.1731148 -4.1635523 -4.140595 -4.1008677 -4.0747995 -4.0816488 -4.1172462 -4.1400518 -4.1447816 -4.1419034 -4.1343179 -4.1349735 -4.1513324][-4.2270522 -4.2165561 -4.2128315 -4.2125812 -4.2021618 -4.1800895 -4.1655235 -4.164989 -4.1804733 -4.1889954 -4.1881042 -4.1817522 -4.1730266 -4.1712255 -4.1840515][-4.2683959 -4.2611132 -4.260818 -4.264225 -4.259305 -4.2476311 -4.23753 -4.2350411 -4.2429695 -4.2475972 -4.244236 -4.23568 -4.2264748 -4.223774 -4.2334871][-4.3026342 -4.3012524 -4.3058643 -4.3127313 -4.3119617 -4.3066907 -4.3008847 -4.2989793 -4.3027992 -4.3058004 -4.3038092 -4.2964892 -4.2872171 -4.2815342 -4.2839909][-4.3245921 -4.3260775 -4.3315706 -4.3380227 -4.3395939 -4.3375258 -4.3346324 -4.3326035 -4.3343844 -4.3362184 -4.3367705 -4.334271 -4.3288345 -4.322226 -4.3188868]]...]
INFO - root - 2017-12-07 20:35:02.111949: step 42010, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 78h:09m:08s remains)
INFO - root - 2017-12-07 20:35:11.884311: step 42020, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 74h:59m:54s remains)
INFO - root - 2017-12-07 20:35:21.576596: step 42030, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 79h:37m:21s remains)
INFO - root - 2017-12-07 20:35:31.208524: step 42040, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 79h:09m:27s remains)
INFO - root - 2017-12-07 20:35:40.952726: step 42050, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 79h:13m:21s remains)
INFO - root - 2017-12-07 20:35:50.650296: step 42060, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 69h:46m:38s remains)
INFO - root - 2017-12-07 20:36:00.383408: step 42070, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 75h:24m:44s remains)
INFO - root - 2017-12-07 20:36:10.137712: step 42080, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 77h:50m:17s remains)
INFO - root - 2017-12-07 20:36:19.900159: step 42090, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 75h:54m:11s remains)
INFO - root - 2017-12-07 20:36:29.486779: step 42100, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 76h:36m:30s remains)
2017-12-07 20:36:30.492494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2035909 -4.186615 -4.1727653 -4.1660323 -4.1638956 -4.1703115 -4.190412 -4.2127385 -4.2295928 -4.2290845 -4.2113147 -4.2061796 -4.2145481 -4.2345853 -4.2616334][-4.1659784 -4.1372638 -4.1186528 -4.1133213 -4.1233664 -4.1448421 -4.172502 -4.2001553 -4.2252784 -4.2380648 -4.2352242 -4.2377691 -4.2466855 -4.2592845 -4.27575][-4.1381664 -4.101 -4.0795417 -4.0759087 -4.0941467 -4.1251335 -4.1581249 -4.1868844 -4.213356 -4.2344003 -4.2472715 -4.260016 -4.2709765 -4.2780981 -4.2832623][-4.1311293 -4.0907974 -4.0706239 -4.0712876 -4.0907488 -4.1184807 -4.1451354 -4.1654811 -4.1835818 -4.2066851 -4.2284513 -4.2488585 -4.2630439 -4.2693739 -4.2716231][-4.1436081 -4.1041141 -4.0884843 -4.0982594 -4.1191483 -4.136776 -4.1481357 -4.1538157 -4.1618557 -4.1795983 -4.1960716 -4.2108293 -4.2243805 -4.2374816 -4.2495961][-4.1549978 -4.1207256 -4.1144714 -4.1372633 -4.1606231 -4.1696734 -4.1699023 -4.1658635 -4.1650076 -4.1719217 -4.17225 -4.1714749 -4.1838288 -4.2110558 -4.2411284][-4.1537309 -4.1283817 -4.1355052 -4.1653905 -4.1895814 -4.1978431 -4.1971431 -4.1894393 -4.1811538 -4.1706381 -4.1492658 -4.1364331 -4.1548214 -4.196558 -4.2404923][-4.154737 -4.142612 -4.1632113 -4.1980004 -4.2192984 -4.2259603 -4.22652 -4.2163897 -4.1968656 -4.1642156 -4.1289988 -4.118681 -4.1478753 -4.1975517 -4.2416363][-4.159265 -4.1666746 -4.1985984 -4.2315297 -4.242497 -4.2429752 -4.2427335 -4.2307544 -4.2045655 -4.1620593 -4.1232166 -4.1244745 -4.15961 -4.2029734 -4.233119][-4.1740379 -4.19605 -4.2293382 -4.2532482 -4.254602 -4.2496023 -4.249217 -4.2370262 -4.2105026 -4.170033 -4.1367669 -4.1425681 -4.1731863 -4.2011862 -4.212605][-4.1932898 -4.2150221 -4.2381353 -4.2494721 -4.2413092 -4.2325006 -4.2339625 -4.229887 -4.2150126 -4.182435 -4.1540952 -4.1590819 -4.1786046 -4.1882277 -4.1886396][-4.203054 -4.2194896 -4.2301855 -4.2308111 -4.2169662 -4.207427 -4.2101064 -4.2132874 -4.2105842 -4.1857862 -4.15707 -4.1570058 -4.1693082 -4.1768594 -4.1817393][-4.2064757 -4.2234178 -4.2307005 -4.2268271 -4.2098494 -4.1973119 -4.1965985 -4.1972814 -4.19942 -4.1799507 -4.1503587 -4.1461248 -4.1624122 -4.1805925 -4.1987338][-4.2095718 -4.2279119 -4.238811 -4.2378931 -4.2226548 -4.2088242 -4.2009206 -4.1963682 -4.1961122 -4.180881 -4.1533766 -4.1488776 -4.1689882 -4.1995535 -4.2322359][-4.2139692 -4.2320523 -4.2456355 -4.2506275 -4.2431622 -4.2320366 -4.2220573 -4.2140775 -4.2110219 -4.2001896 -4.1777878 -4.1701131 -4.1865735 -4.2164927 -4.24984]]...]
INFO - root - 2017-12-07 20:36:40.142382: step 42110, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 77h:40m:37s remains)
INFO - root - 2017-12-07 20:36:49.715275: step 42120, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 75h:54m:29s remains)
INFO - root - 2017-12-07 20:36:59.281409: step 42130, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 76h:36m:20s remains)
INFO - root - 2017-12-07 20:37:08.911904: step 42140, loss = 2.13, batch loss = 2.07 (8.3 examples/sec; 0.968 sec/batch; 78h:06m:29s remains)
INFO - root - 2017-12-07 20:37:18.680563: step 42150, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 81h:10m:57s remains)
INFO - root - 2017-12-07 20:37:28.418040: step 42160, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 77h:56m:51s remains)
INFO - root - 2017-12-07 20:37:38.091330: step 42170, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 78h:28m:42s remains)
INFO - root - 2017-12-07 20:37:47.826798: step 42180, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.012 sec/batch; 81h:38m:18s remains)
INFO - root - 2017-12-07 20:37:57.485701: step 42190, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 78h:19m:43s remains)
INFO - root - 2017-12-07 20:38:07.038696: step 42200, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 77h:05m:26s remains)
2017-12-07 20:38:08.029735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3144612 -4.3141656 -4.3138943 -4.3144031 -4.3145561 -4.314702 -4.313468 -4.312489 -4.3146682 -4.3175559 -4.3226528 -4.327261 -4.330759 -4.3333073 -4.3354611][-4.3019719 -4.30204 -4.302218 -4.3037872 -4.3045883 -4.3027658 -4.2977958 -4.2956095 -4.3019938 -4.3086019 -4.3178425 -4.3241692 -4.3267059 -4.3269548 -4.3279362][-4.286139 -4.2871385 -4.2897749 -4.2949095 -4.2958732 -4.290401 -4.277492 -4.2741394 -4.2883015 -4.2993855 -4.3122869 -4.3178444 -4.3165479 -4.311718 -4.3086767][-4.2626534 -4.2619133 -4.2654324 -4.27338 -4.2752972 -4.2651134 -4.24178 -4.2368503 -4.2613616 -4.2796507 -4.2966537 -4.3021545 -4.2964087 -4.285603 -4.2794466][-4.236383 -4.2315869 -4.2322059 -4.2379861 -4.2360473 -4.2191887 -4.1875005 -4.1813569 -4.215724 -4.2417922 -4.2633882 -4.2698536 -4.2625074 -4.2473626 -4.242415][-4.2111964 -4.2011709 -4.1953278 -4.1930957 -4.1825528 -4.1587682 -4.1199331 -4.1129408 -4.1571131 -4.1938257 -4.2222128 -4.2323036 -4.2262015 -4.2092886 -4.2067342][-4.1872358 -4.1700673 -4.15005 -4.1312 -4.1042948 -4.0714474 -4.027421 -4.0270576 -4.0892668 -4.1419773 -4.18084 -4.1920414 -4.1860857 -4.1683822 -4.1697726][-4.1829119 -4.1608672 -4.1216841 -4.0719061 -4.0146437 -3.9684374 -3.9278879 -3.9462337 -4.0326076 -4.103003 -4.1516037 -4.1640463 -4.1622176 -4.1495609 -4.1539955][-4.1977458 -4.1827364 -4.1525345 -4.1061382 -4.0441833 -3.987143 -3.9364769 -3.9475493 -4.0274534 -4.0944843 -4.1420431 -4.1536808 -4.1509724 -4.1428628 -4.1510134][-4.2077618 -4.2021117 -4.18855 -4.1598334 -4.114913 -4.0673504 -4.0201845 -4.0205464 -4.0760465 -4.1242757 -4.1586242 -4.1667418 -4.1611538 -4.1518083 -4.159657][-4.19772 -4.1938548 -4.187799 -4.1697569 -4.1378713 -4.1015563 -4.0675321 -4.0694156 -4.1094565 -4.1452107 -4.1721921 -4.1830912 -4.1797676 -4.173295 -4.1803589][-4.19449 -4.1882615 -4.1836762 -4.170372 -4.1457944 -4.1152844 -4.08805 -4.0901484 -4.1202817 -4.1494107 -4.1732545 -4.1856866 -4.1861153 -4.1870594 -4.1985669][-4.2161517 -4.2122388 -4.2108912 -4.2009926 -4.1795883 -4.1516232 -4.1261573 -4.1241531 -4.1458983 -4.1722078 -4.1945558 -4.2063541 -4.2106147 -4.2165666 -4.2285213][-4.25976 -4.2616739 -4.2654366 -4.2614393 -4.2468047 -4.2254734 -4.2060738 -4.2026372 -4.214921 -4.2341385 -4.2528768 -4.2629361 -4.2659192 -4.2690058 -4.2751236][-4.2988081 -4.304738 -4.3106341 -4.3092384 -4.3004 -4.2889061 -4.2796764 -4.278152 -4.2846169 -4.2966142 -4.3089428 -4.31583 -4.3164239 -4.31525 -4.3153973]]...]
INFO - root - 2017-12-07 20:38:17.722967: step 42210, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 78h:32m:13s remains)
INFO - root - 2017-12-07 20:38:27.368676: step 42220, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 78h:17m:02s remains)
INFO - root - 2017-12-07 20:38:37.048633: step 42230, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 76h:21m:43s remains)
INFO - root - 2017-12-07 20:38:46.728385: step 42240, loss = 2.03, batch loss = 1.97 (8.0 examples/sec; 1.000 sec/batch; 80h:39m:27s remains)
INFO - root - 2017-12-07 20:38:56.418968: step 42250, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 73h:07m:37s remains)
INFO - root - 2017-12-07 20:39:05.957974: step 42260, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 75h:30m:48s remains)
INFO - root - 2017-12-07 20:39:15.506461: step 42270, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 78h:54m:17s remains)
INFO - root - 2017-12-07 20:39:25.136673: step 42280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 74h:19m:30s remains)
INFO - root - 2017-12-07 20:39:34.973096: step 42290, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.025 sec/batch; 82h:37m:07s remains)
INFO - root - 2017-12-07 20:39:44.604359: step 42300, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.012 sec/batch; 81h:33m:47s remains)
2017-12-07 20:39:45.553509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2864842 -4.2762341 -4.277142 -4.2857985 -4.2892504 -4.2911725 -4.2904658 -4.2912035 -4.2891397 -4.2905874 -4.2968855 -4.3037686 -4.3070836 -4.3115945 -4.320951][-4.2672038 -4.2572002 -4.2570839 -4.2627811 -4.2623596 -4.2597008 -4.2542062 -4.2542744 -4.2559686 -4.258605 -4.2671213 -4.2791553 -4.2834783 -4.287941 -4.2984214][-4.2509403 -4.2477937 -4.248621 -4.2498574 -4.2440081 -4.232749 -4.2207918 -4.2223244 -4.2324433 -4.2421727 -4.252717 -4.2668796 -4.2703238 -4.2708111 -4.2788076][-4.2316551 -4.2321811 -4.2310109 -4.2264853 -4.2144213 -4.1957026 -4.1804962 -4.1882095 -4.2104759 -4.2334313 -4.2508917 -4.2662597 -4.2689896 -4.2669663 -4.271749][-4.1947103 -4.1980977 -4.1937513 -4.1855292 -4.1696415 -4.14869 -4.1340613 -4.1463571 -4.1783686 -4.2137103 -4.2401891 -4.2597423 -4.2659416 -4.2671947 -4.2729926][-4.1567845 -4.1591172 -4.1542439 -4.1434579 -4.124536 -4.1035872 -4.0927114 -4.1115479 -4.15038 -4.1940308 -4.2278833 -4.2508826 -4.2618208 -4.2660975 -4.2711859][-4.1381278 -4.1340413 -4.12635 -4.10454 -4.0720778 -4.0460281 -4.0418277 -4.0712042 -4.1236806 -4.1794863 -4.220974 -4.2496824 -4.2642155 -4.2679319 -4.2681231][-4.15573 -4.1344461 -4.1112022 -4.0667167 -4.0116057 -3.9749756 -3.9717176 -4.0094857 -4.0792613 -4.1488285 -4.2004919 -4.2387285 -4.2567859 -4.2595782 -4.2571826][-4.1716614 -4.1380959 -4.106636 -4.0514994 -3.9942172 -3.965219 -3.9669218 -4.0025711 -4.0722828 -4.1422334 -4.1926303 -4.2281675 -4.2447567 -4.2447462 -4.2410388][-4.1631703 -4.1342292 -4.1090422 -4.0689521 -4.0401855 -4.0402775 -4.0522156 -4.0790257 -4.1301446 -4.1778045 -4.2120094 -4.2351422 -4.2441087 -4.2376928 -4.2298479][-4.1422443 -4.129477 -4.1192045 -4.1056252 -4.1076088 -4.1265149 -4.1393032 -4.1559939 -4.1864662 -4.2111177 -4.2292724 -4.2427688 -4.2496462 -4.2411809 -4.2301035][-4.1354551 -4.1388111 -4.1435213 -4.1479435 -4.1639013 -4.1878772 -4.1975842 -4.2053757 -4.2228794 -4.2355113 -4.2430496 -4.249548 -4.2518153 -4.2414861 -4.2313356][-4.1792564 -4.1845417 -4.1912737 -4.1950297 -4.2066412 -4.2265892 -4.234508 -4.2405729 -4.2534089 -4.2620783 -4.2640572 -4.2641191 -4.2618456 -4.2488513 -4.23922][-4.2411404 -4.2380052 -4.2379785 -4.2356634 -4.2402735 -4.2516422 -4.2554245 -4.2612262 -4.2735529 -4.284133 -4.2860022 -4.2826047 -4.2786903 -4.2673807 -4.26039][-4.2869906 -4.2779779 -4.2732482 -4.2645373 -4.2597857 -4.2622256 -4.262886 -4.2696247 -4.2847915 -4.2971048 -4.2983527 -4.2933359 -4.2907615 -4.2827907 -4.2786012]]...]
INFO - root - 2017-12-07 20:39:55.320378: step 42310, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 80h:32m:36s remains)
INFO - root - 2017-12-07 20:40:05.057047: step 42320, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 75h:22m:08s remains)
INFO - root - 2017-12-07 20:40:14.726312: step 42330, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.994 sec/batch; 80h:05m:22s remains)
INFO - root - 2017-12-07 20:40:24.472080: step 42340, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 76h:29m:35s remains)
INFO - root - 2017-12-07 20:40:34.111206: step 42350, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 73h:59m:21s remains)
INFO - root - 2017-12-07 20:40:43.775355: step 42360, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.988 sec/batch; 79h:36m:03s remains)
INFO - root - 2017-12-07 20:40:53.511198: step 42370, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 78h:54m:47s remains)
INFO - root - 2017-12-07 20:41:03.208859: step 42380, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 78h:47m:06s remains)
INFO - root - 2017-12-07 20:41:12.944022: step 42390, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.010 sec/batch; 81h:21m:33s remains)
INFO - root - 2017-12-07 20:41:22.528178: step 42400, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.970 sec/batch; 78h:07m:43s remains)
2017-12-07 20:41:23.571727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2744961 -4.2907395 -4.3024039 -4.2924795 -4.2767105 -4.2620292 -4.2507958 -4.2558284 -4.2610745 -4.2550054 -4.2412252 -4.225503 -4.2080569 -4.2039986 -4.2212868][-4.2669153 -4.2764869 -4.281456 -4.2669511 -4.2441578 -4.2284389 -4.2218914 -4.2365093 -4.2480192 -4.2479515 -4.2345357 -4.218205 -4.2029409 -4.1930451 -4.2051005][-4.2376213 -4.2333851 -4.2323461 -4.2156172 -4.1906929 -4.1774912 -4.1806121 -4.2127838 -4.2397666 -4.2524366 -4.2472916 -4.235805 -4.2226114 -4.2049727 -4.2063][-4.2004008 -4.1886187 -4.1869912 -4.1746697 -4.1560416 -4.1426582 -4.1442213 -4.1938591 -4.2395477 -4.2633638 -4.2653413 -4.2593675 -4.2463946 -4.2222137 -4.2156973][-4.1623154 -4.1521196 -4.1540966 -4.1465297 -4.1275072 -4.0992408 -4.0725365 -4.121665 -4.1861072 -4.2201076 -4.2303543 -4.2289124 -4.2226834 -4.2079458 -4.2128916][-4.1274633 -4.124897 -4.12749 -4.1206636 -4.0893779 -4.020792 -3.9350657 -3.9705 -4.0619349 -4.1166425 -4.14293 -4.1480923 -4.1510277 -4.1584487 -4.187911][-4.072021 -4.0727682 -4.0723181 -4.0614395 -4.0158434 -3.909502 -3.7573097 -3.7719393 -3.9039772 -3.9936774 -4.0528297 -4.0784483 -4.0881906 -4.1084108 -4.1527743][-4.0321674 -4.0294256 -4.020247 -4.0087137 -3.9667015 -3.8619554 -3.7105105 -3.7050965 -3.8377523 -3.9366481 -4.0098209 -4.0518479 -4.0670109 -4.083744 -4.1267767][-4.0544386 -4.0533829 -4.0463023 -4.036581 -4.0080132 -3.942899 -3.8592005 -3.8546109 -3.9277573 -3.9893012 -4.0441761 -4.084969 -4.0934877 -4.0970411 -4.1286039][-4.1116867 -4.1172309 -4.11168 -4.1072955 -4.0931568 -4.0636873 -4.0303559 -4.02944 -4.0619421 -4.0940337 -4.1235552 -4.1497774 -4.151567 -4.1443567 -4.1616845][-4.1795139 -4.1859093 -4.1817102 -4.1840434 -4.1812658 -4.166265 -4.1531191 -4.1540275 -4.171257 -4.1936426 -4.2126079 -4.2256546 -4.2182436 -4.2054968 -4.2112455][-4.2482572 -4.2520342 -4.2515121 -4.2612681 -4.263906 -4.2498188 -4.2442 -4.2480912 -4.2595363 -4.2749968 -4.2906542 -4.2993536 -4.2861962 -4.2680206 -4.2628522][-4.3104582 -4.3126578 -4.3129249 -4.3201375 -4.3210673 -4.3091598 -4.3024611 -4.3049679 -4.3121271 -4.3218675 -4.3340526 -4.3419275 -4.3317957 -4.3152514 -4.3040938][-4.342907 -4.3445778 -4.3439279 -4.3472142 -4.3466625 -4.3383622 -4.330853 -4.3291993 -4.3323035 -4.3387761 -4.3473587 -4.355617 -4.3528752 -4.3429947 -4.3300843][-4.3436813 -4.3436494 -4.3417721 -4.3418822 -4.3404016 -4.3351769 -4.3294425 -4.3270259 -4.3280687 -4.3317103 -4.3364477 -4.3434963 -4.3479981 -4.3453135 -4.3357553]]...]
INFO - root - 2017-12-07 20:41:33.280229: step 42410, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 77h:57m:21s remains)
INFO - root - 2017-12-07 20:41:43.134468: step 42420, loss = 2.04, batch loss = 1.98 (7.7 examples/sec; 1.044 sec/batch; 84h:05m:07s remains)
INFO - root - 2017-12-07 20:41:52.960501: step 42430, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 78h:32m:15s remains)
INFO - root - 2017-12-07 20:42:02.351865: step 42440, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 75h:46m:06s remains)
INFO - root - 2017-12-07 20:42:11.750965: step 42450, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 75h:15m:44s remains)
INFO - root - 2017-12-07 20:42:21.318498: step 42460, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 77h:49m:29s remains)
INFO - root - 2017-12-07 20:42:30.868984: step 42470, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.008 sec/batch; 81h:12m:41s remains)
INFO - root - 2017-12-07 20:42:40.460037: step 42480, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 77h:59m:26s remains)
INFO - root - 2017-12-07 20:42:50.109767: step 42490, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 76h:42m:36s remains)
INFO - root - 2017-12-07 20:42:59.825165: step 42500, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 76h:55m:31s remains)
2017-12-07 20:43:00.885099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.162744 -4.1545048 -4.14977 -4.1472378 -4.1435113 -4.1452708 -4.1529827 -4.1570973 -4.1510382 -4.1442161 -4.1387796 -4.1380124 -4.1459494 -4.1635985 -4.1865587][-4.1647882 -4.1539721 -4.1457133 -4.1410365 -4.1387672 -4.1441522 -4.1579361 -4.170115 -4.1701407 -4.1615248 -4.1491408 -4.1387296 -4.1383095 -4.1469955 -4.1631866][-4.1838388 -4.1662192 -4.1544452 -4.1500421 -4.1561127 -4.1706171 -4.1901093 -4.2036061 -4.2027574 -4.1915255 -4.1744256 -4.1588988 -4.1491785 -4.1431532 -4.1479735][-4.1923704 -4.168417 -4.1543717 -4.1515584 -4.1669359 -4.191216 -4.2170153 -4.2334743 -4.2315817 -4.2170386 -4.1957073 -4.1789155 -4.1667576 -4.1526628 -4.146009][-4.1841049 -4.1585541 -4.1465697 -4.1444941 -4.1590147 -4.1850815 -4.2151775 -4.2343092 -4.2327452 -4.2153397 -4.1905384 -4.1725268 -4.1634264 -4.1520152 -4.1427464][-4.1732097 -4.1495342 -4.1421475 -4.141418 -4.1478682 -4.164423 -4.1892414 -4.2046027 -4.1998305 -4.1809163 -4.1557803 -4.1422224 -4.1441088 -4.1463208 -4.1439033][-4.1795096 -4.1563053 -4.1516428 -4.1541681 -4.153821 -4.1554446 -4.1642404 -4.1682234 -4.1577864 -4.1392145 -4.1210055 -4.1169167 -4.1331296 -4.1470633 -4.1508536][-4.2130623 -4.1965022 -4.1942267 -4.195179 -4.1889729 -4.1794834 -4.1744628 -4.1662588 -4.148726 -4.1311646 -4.1202774 -4.1241097 -4.146018 -4.1591096 -4.1605706][-4.2546391 -4.24381 -4.2415843 -4.2401776 -4.2296271 -4.2150183 -4.2033024 -4.1911511 -4.1728911 -4.1578436 -4.1513343 -4.1569486 -4.1730237 -4.1779819 -4.1744075][-4.2915115 -4.2855315 -4.280839 -4.2774534 -4.2653351 -4.248795 -4.235498 -4.2270288 -4.2178659 -4.21078 -4.2062521 -4.2065578 -4.2099628 -4.2038531 -4.1948018][-4.3167076 -4.3138876 -4.3086267 -4.304491 -4.2928996 -4.2767963 -4.2649913 -4.2603841 -4.2579842 -4.2570844 -4.2536092 -4.2475805 -4.239234 -4.2239146 -4.2106929][-4.3134184 -4.3115191 -4.3086791 -4.3078804 -4.2996006 -4.2863975 -4.2767711 -4.2747264 -4.275507 -4.2783284 -4.27724 -4.2678905 -4.2539907 -4.2359796 -4.22295][-4.28804 -4.2853742 -4.28483 -4.28793 -4.2845531 -4.2758312 -4.2664642 -4.264061 -4.2659 -4.2708759 -4.2732882 -4.2677436 -4.2557411 -4.2410674 -4.2320662][-4.2647452 -4.2623963 -4.264348 -4.2699838 -4.2701273 -4.2638006 -4.25391 -4.2485695 -4.2483616 -4.2522349 -4.2562728 -4.2557793 -4.2502289 -4.2429872 -4.2389646][-4.2508707 -4.2498403 -4.2531114 -4.2591114 -4.2606 -4.256218 -4.248117 -4.2421675 -4.2407546 -4.2434731 -4.2479544 -4.2504826 -4.2511764 -4.2513256 -4.2520857]]...]
INFO - root - 2017-12-07 20:43:10.572129: step 42510, loss = 2.12, batch loss = 2.06 (8.4 examples/sec; 0.957 sec/batch; 77h:07m:11s remains)
INFO - root - 2017-12-07 20:43:20.196747: step 42520, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 80h:57m:13s remains)
INFO - root - 2017-12-07 20:43:29.866211: step 42530, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 76h:08m:19s remains)
INFO - root - 2017-12-07 20:43:39.490206: step 42540, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 79h:11m:09s remains)
INFO - root - 2017-12-07 20:43:48.948122: step 42550, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 79h:07m:44s remains)
INFO - root - 2017-12-07 20:43:58.500695: step 42560, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 73h:20m:03s remains)
INFO - root - 2017-12-07 20:44:08.365534: step 42570, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.027 sec/batch; 82h:43m:06s remains)
INFO - root - 2017-12-07 20:44:18.025970: step 42580, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 74h:24m:52s remains)
INFO - root - 2017-12-07 20:44:27.662064: step 42590, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 75h:14m:04s remains)
INFO - root - 2017-12-07 20:44:37.139889: step 42600, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 75h:10m:14s remains)
2017-12-07 20:44:38.267054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3050756 -4.2951078 -4.2892575 -4.2860689 -4.2896748 -4.298964 -4.3089018 -4.3126473 -4.310585 -4.3033504 -4.296948 -4.2994084 -4.3100286 -4.3222136 -4.3333859][-4.243526 -4.2338624 -4.234827 -4.2416921 -4.2553372 -4.2701263 -4.27848 -4.2748671 -4.2615242 -4.2459011 -4.2387223 -4.247334 -4.2678237 -4.290163 -4.3102269][-4.1721683 -4.1660876 -4.1807556 -4.2035842 -4.2264848 -4.24026 -4.237916 -4.2217531 -4.1960068 -4.1750264 -4.1724477 -4.1917229 -4.2229848 -4.2555709 -4.2864938][-4.1075168 -4.1058383 -4.1329575 -4.170846 -4.2000637 -4.2069225 -4.190237 -4.1602454 -4.1250558 -4.1045 -4.1127954 -4.1452637 -4.1851525 -4.2253308 -4.26474][-4.0676041 -4.0772443 -4.1162243 -4.1603503 -4.1832647 -4.1724463 -4.1351924 -4.0907993 -4.0568323 -4.0520067 -4.0807071 -4.1255269 -4.1675138 -4.2088127 -4.2502413][-4.0787954 -4.1067319 -4.1472569 -4.1739855 -4.1713419 -4.1313906 -4.06466 -4.0108232 -3.9954743 -4.0286231 -4.0863266 -4.1402526 -4.17914 -4.2134218 -4.2480927][-4.1376486 -4.1717782 -4.195755 -4.1902595 -4.1507821 -4.0693784 -3.9696932 -3.9226377 -3.9564073 -4.0403748 -4.1211967 -4.1763062 -4.2086334 -4.2300858 -4.2494221][-4.1921492 -4.2156296 -4.2199755 -4.1851387 -4.1111412 -3.9874279 -3.8702352 -3.8681786 -3.9713206 -4.0890493 -4.1742215 -4.22044 -4.2396011 -4.2436914 -4.2467041][-4.2229095 -4.2285228 -4.2119575 -4.15545 -4.0599213 -3.9304035 -3.8509507 -3.9137006 -4.0489526 -4.1631546 -4.2338495 -4.260138 -4.2591472 -4.2458243 -4.238575][-4.2319989 -4.2244072 -4.1960149 -4.13061 -4.0368228 -3.9454634 -3.929879 -4.0201015 -4.1414967 -4.2308149 -4.2747984 -4.2778449 -4.2575083 -4.2321072 -4.2223859][-4.2287836 -4.2148533 -4.1868916 -4.1265635 -4.0534706 -4.0085936 -4.0344152 -4.1209855 -4.2119594 -4.2687573 -4.2843547 -4.2642031 -4.2282352 -4.1969938 -4.1901422][-4.2251573 -4.2079148 -4.1836739 -4.1392136 -4.0923066 -4.0859 -4.1312938 -4.20221 -4.2595005 -4.2849736 -4.2724657 -4.2289619 -4.1768718 -4.14306 -4.1473126][-4.2245646 -4.2071481 -4.1872272 -4.1565104 -4.131321 -4.1502748 -4.2021513 -4.2540259 -4.2826276 -4.2821751 -4.2466569 -4.1815891 -4.1182504 -4.0914245 -4.1162806][-4.2371974 -4.2237239 -4.2053 -4.1816821 -4.1712809 -4.1992593 -4.2456264 -4.2799892 -4.286962 -4.2656903 -4.2119327 -4.1345987 -4.071311 -4.06404 -4.112576][-4.25384 -4.2443347 -4.2300344 -4.2154722 -4.2150273 -4.2419024 -4.276504 -4.2917557 -4.2820649 -4.2439213 -4.1792741 -4.1013641 -4.047709 -4.0653071 -4.1340094]]...]
INFO - root - 2017-12-07 20:44:47.785937: step 42610, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 77h:58m:55s remains)
INFO - root - 2017-12-07 20:44:57.390854: step 42620, loss = 2.07, batch loss = 2.01 (7.6 examples/sec; 1.055 sec/batch; 84h:56m:21s remains)
INFO - root - 2017-12-07 20:45:07.091792: step 42630, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 80h:11m:24s remains)
INFO - root - 2017-12-07 20:45:16.906892: step 42640, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.019 sec/batch; 82h:03m:06s remains)
INFO - root - 2017-12-07 20:45:26.498114: step 42650, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 79h:10m:20s remains)
INFO - root - 2017-12-07 20:45:36.170651: step 42660, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 77h:21m:06s remains)
INFO - root - 2017-12-07 20:45:46.007993: step 42670, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 77h:55m:51s remains)
INFO - root - 2017-12-07 20:45:55.535132: step 42680, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 75h:54m:37s remains)
INFO - root - 2017-12-07 20:46:05.255422: step 42690, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 79h:16m:45s remains)
INFO - root - 2017-12-07 20:46:14.905751: step 42700, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 80h:48m:02s remains)
2017-12-07 20:46:15.897847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.289093 -4.2690282 -4.244607 -4.2224383 -4.2047477 -4.2039208 -4.2251692 -4.24892 -4.2495141 -4.2273569 -4.1953154 -4.168828 -4.1600986 -4.1807613 -4.2221351][-4.282433 -4.2614317 -4.2339807 -4.2126331 -4.1946692 -4.1893415 -4.2110052 -4.2358689 -4.2379885 -4.2151928 -4.1818719 -4.1558671 -4.1467996 -4.1717815 -4.2244129][-4.2735524 -4.2498403 -4.2200351 -4.2006559 -4.1834011 -4.1770153 -4.19469 -4.213562 -4.2203574 -4.206193 -4.1844449 -4.1697226 -4.1669035 -4.1958671 -4.2515478][-4.256144 -4.2299824 -4.2042336 -4.1862993 -4.1691685 -4.1603503 -4.171885 -4.1902847 -4.20904 -4.214251 -4.2100182 -4.2097054 -4.2124815 -4.2353039 -4.2803817][-4.2380414 -4.219286 -4.1970682 -4.1743121 -4.1495438 -4.1358352 -4.148406 -4.1738772 -4.2059093 -4.2267218 -4.2308178 -4.2347651 -4.2375546 -4.2521796 -4.288537][-4.2258153 -4.2098041 -4.1827307 -4.1435909 -4.1078873 -4.0971146 -4.1197677 -4.1562948 -4.19197 -4.2115326 -4.2105517 -4.2114019 -4.2158523 -4.2358103 -4.2789607][-4.2081752 -4.1893587 -4.1501861 -4.0945725 -4.0489588 -4.0443225 -4.0745821 -4.1107635 -4.1388879 -4.1474319 -4.1400352 -4.1423249 -4.1588178 -4.196732 -4.2512312][-4.1860719 -4.1550045 -4.0982466 -4.0278478 -3.9774723 -3.9809546 -4.0134649 -4.0402455 -4.0572968 -4.0589709 -4.0535221 -4.0653315 -4.0974064 -4.1524563 -4.2126617][-4.1606817 -4.1157494 -4.0421252 -3.9608984 -3.9089861 -3.9205632 -3.9544876 -3.9754913 -3.9862545 -3.9887834 -3.9888146 -4.0052066 -4.0440845 -4.1076255 -4.173975][-4.1340075 -4.0851955 -4.0115633 -3.9385328 -3.8992064 -3.9158473 -3.9493952 -3.9651132 -3.9709733 -3.9739759 -3.9780946 -3.9952524 -4.0299835 -4.0900378 -4.1591344][-4.1391153 -4.0960627 -4.0365238 -3.9852946 -3.9660323 -3.9857361 -4.013165 -4.0262513 -4.0281076 -4.0275536 -4.0332646 -4.0505185 -4.0809145 -4.1305161 -4.1931977][-4.1876593 -4.1593671 -4.1189528 -4.0868015 -4.0793123 -4.0975184 -4.1187086 -4.1308761 -4.1327763 -4.1303425 -4.1334352 -4.1454782 -4.1660504 -4.2002439 -4.2469096][-4.2438831 -4.2315245 -4.2105875 -4.1935024 -4.1924386 -4.2079329 -4.2238879 -4.2348804 -4.2365909 -4.2335253 -4.23283 -4.2386417 -4.2488666 -4.2682743 -4.2983027][-4.2898045 -4.2856407 -4.2780118 -4.2711868 -4.2728829 -4.2825561 -4.2914681 -4.2989054 -4.30141 -4.3015294 -4.302278 -4.3041682 -4.3078876 -4.3179212 -4.336103][-4.3223515 -4.3217521 -4.3200026 -4.3170776 -4.3172526 -4.3211112 -4.3253393 -4.3291144 -4.3308425 -4.3330479 -4.335382 -4.3364291 -4.3376369 -4.3426447 -4.3530335]]...]
INFO - root - 2017-12-07 20:46:25.505646: step 42710, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 80h:13m:18s remains)
INFO - root - 2017-12-07 20:46:35.232406: step 42720, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.034 sec/batch; 83h:14m:44s remains)
INFO - root - 2017-12-07 20:46:44.878099: step 42730, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.969 sec/batch; 77h:57m:46s remains)
INFO - root - 2017-12-07 20:46:54.335519: step 42740, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.023 sec/batch; 82h:21m:36s remains)
INFO - root - 2017-12-07 20:47:03.911980: step 42750, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 75h:09m:10s remains)
INFO - root - 2017-12-07 20:47:13.638765: step 42760, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 79h:11m:40s remains)
INFO - root - 2017-12-07 20:47:23.276226: step 42770, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 74h:45m:57s remains)
INFO - root - 2017-12-07 20:47:32.860463: step 42780, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 75h:32m:08s remains)
INFO - root - 2017-12-07 20:47:42.441295: step 42790, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 73h:09m:13s remains)
INFO - root - 2017-12-07 20:47:52.122831: step 42800, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 79h:44m:14s remains)
2017-12-07 20:47:53.204939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1991148 -4.2002978 -4.20789 -4.2009053 -4.1941767 -4.1824837 -4.1758962 -4.18772 -4.2140937 -4.2333174 -4.240272 -4.2428637 -4.2412419 -4.2372894 -4.2398372][-4.222867 -4.2227159 -4.2320237 -4.227787 -4.2177906 -4.2003565 -4.1868105 -4.1832514 -4.1936784 -4.203187 -4.2061677 -4.208179 -4.2108541 -4.2137551 -4.2226539][-4.2390723 -4.2370353 -4.2514529 -4.256525 -4.252481 -4.2382689 -4.2223568 -4.2035227 -4.1886859 -4.1740546 -4.1591225 -4.1519241 -4.1571341 -4.1676631 -4.1875663][-4.2499504 -4.2487707 -4.2648816 -4.2741017 -4.2730341 -4.2558994 -4.231338 -4.1952844 -4.1562448 -4.121253 -4.0934391 -4.0828066 -4.0955005 -4.1166348 -4.1474833][-4.2466974 -4.2452235 -4.263906 -4.27302 -4.2679214 -4.2429132 -4.2034397 -4.1522093 -4.1015553 -4.0680208 -4.0481024 -4.0490785 -4.0762053 -4.1043315 -4.1310821][-4.2360892 -4.2340965 -4.251565 -4.2571979 -4.2457428 -4.2150702 -4.1726 -4.1202221 -4.071733 -4.0531449 -4.0519915 -4.0679884 -4.1034141 -4.126153 -4.13794][-4.2304788 -4.2303891 -4.2413673 -4.2418323 -4.2264304 -4.1976557 -4.1640463 -4.1246891 -4.0919161 -4.0867682 -4.0926504 -4.1106238 -4.1424074 -4.1557264 -4.1526985][-4.2387729 -4.2351227 -4.2353153 -4.2300873 -4.2142115 -4.1943526 -4.1760874 -4.1562095 -4.1405559 -4.1402478 -4.1440225 -4.1592546 -4.1829247 -4.1896105 -4.1810474][-4.2535586 -4.2432413 -4.2328963 -4.2254896 -4.2166328 -4.2122512 -4.212872 -4.2081895 -4.2018776 -4.1985178 -4.1965284 -4.2036543 -4.2168055 -4.2215633 -4.2150059][-4.257875 -4.2478728 -4.2379031 -4.2374439 -4.2413907 -4.2492995 -4.2582035 -4.2594051 -4.2526054 -4.2433653 -4.2400236 -4.2442007 -4.2514405 -4.2545033 -4.2480354][-4.2462759 -4.2451653 -4.2466016 -4.2576528 -4.2693572 -4.2794104 -4.2892241 -4.2930646 -4.2866316 -4.2760286 -4.2755003 -4.2803035 -4.283823 -4.2818556 -4.2719655][-4.2196422 -4.2344394 -4.2526245 -4.2744 -4.2933688 -4.306077 -4.3185129 -4.3252544 -4.3181605 -4.30602 -4.302072 -4.3021879 -4.2995777 -4.2941308 -4.28315][-4.2003493 -4.2310982 -4.2620792 -4.2915063 -4.31753 -4.3341064 -4.34669 -4.350369 -4.3396378 -4.3224812 -4.3114133 -4.3064046 -4.3004675 -4.2934594 -4.2834992][-4.1972365 -4.2348614 -4.2708669 -4.30162 -4.3277779 -4.3441172 -4.3541684 -4.3538508 -4.3387022 -4.3186913 -4.3035593 -4.2946467 -4.2882528 -4.2830911 -4.276217][-4.202105 -4.2352815 -4.2671857 -4.2950964 -4.3173065 -4.3308349 -4.338419 -4.3373008 -4.3239107 -4.306663 -4.2918491 -4.2813096 -4.2752385 -4.2724938 -4.2693553]]...]
INFO - root - 2017-12-07 20:48:02.907408: step 42810, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 77h:20m:26s remains)
INFO - root - 2017-12-07 20:48:12.578924: step 42820, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 75h:53m:01s remains)
INFO - root - 2017-12-07 20:48:22.242297: step 42830, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 1.003 sec/batch; 80h:41m:15s remains)
INFO - root - 2017-12-07 20:48:31.937245: step 42840, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.015 sec/batch; 81h:40m:37s remains)
INFO - root - 2017-12-07 20:48:41.731299: step 42850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 76h:00m:44s remains)
INFO - root - 2017-12-07 20:48:51.340835: step 42860, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 76h:36m:09s remains)
INFO - root - 2017-12-07 20:49:00.965813: step 42870, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 78h:24m:07s remains)
INFO - root - 2017-12-07 20:49:10.669116: step 42880, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.955 sec/batch; 76h:48m:33s remains)
INFO - root - 2017-12-07 20:49:20.432048: step 42890, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 77h:51m:50s remains)
INFO - root - 2017-12-07 20:49:30.076212: step 42900, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 75h:15m:08s remains)
2017-12-07 20:49:31.020075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.359035 -4.3482738 -4.3429809 -4.3455067 -4.3475528 -4.3520613 -4.3566976 -4.3516836 -4.3392735 -4.3251224 -4.3108144 -4.2994552 -4.2955279 -4.2998767 -4.3088679][-4.3545628 -4.33207 -4.318686 -4.3178968 -4.3252292 -4.3397217 -4.3483019 -4.3421607 -4.3262839 -4.3108654 -4.299686 -4.2910943 -4.2889137 -4.2970061 -4.3100219][-4.3280473 -4.2959542 -4.2774887 -4.2786317 -4.2948542 -4.3159919 -4.3223982 -4.3100719 -4.2914438 -4.2756248 -4.2643762 -4.2560196 -4.2555552 -4.2694068 -4.28934][-4.2637553 -4.2274737 -4.2162552 -4.2310905 -4.2554765 -4.27123 -4.2626 -4.2419553 -4.229455 -4.2256088 -4.2231259 -4.220686 -4.2264953 -4.2444367 -4.2673364][-4.1754165 -4.1447392 -4.1521363 -4.1849957 -4.210021 -4.2074246 -4.1751509 -4.14613 -4.147922 -4.1685891 -4.1861405 -4.2016335 -4.2193618 -4.2399578 -4.2585917][-4.0796437 -4.0578942 -4.0810933 -4.1237187 -4.138371 -4.1077938 -4.0429969 -4.0091419 -4.0401931 -4.09961 -4.1494794 -4.1881232 -4.2202711 -4.2435541 -4.2556391][-4.0189548 -4.0008788 -4.0279889 -4.0677528 -4.0633559 -3.993715 -3.8890834 -3.8602438 -3.9327559 -4.0291257 -4.1069622 -4.1617584 -4.2031116 -4.2289042 -4.2394123][-4.0250878 -4.0011239 -4.02096 -4.0451484 -4.0261059 -3.9439676 -3.8393986 -3.8251133 -3.9072542 -4.0025697 -4.0822811 -4.1387005 -4.1809354 -4.2063942 -4.2206907][-4.0897727 -4.0623431 -4.0708523 -4.0835633 -4.0740113 -4.0313044 -3.9717705 -3.9546392 -3.9929376 -4.049964 -4.1088419 -4.1546273 -4.1891394 -4.212172 -4.2317357][-4.1435051 -4.1246367 -4.1363339 -4.1568394 -4.1705828 -4.1651788 -4.1397319 -4.1158581 -4.1128736 -4.1319227 -4.1689219 -4.2043552 -4.2299075 -4.2494149 -4.2700071][-4.1648769 -4.1638021 -4.1886854 -4.2188926 -4.2430992 -4.2538943 -4.2462511 -4.2281518 -4.2115731 -4.2090435 -4.2274156 -4.2529769 -4.2730193 -4.2894392 -4.3068624][-4.1790013 -4.1914878 -4.22274 -4.2555165 -4.2821312 -4.2974315 -4.2972097 -4.285862 -4.2671566 -4.2556338 -4.2633305 -4.281631 -4.2996378 -4.313941 -4.3248739][-4.1930089 -4.2109666 -4.2389593 -4.2699318 -4.2966361 -4.3135953 -4.3130288 -4.3001041 -4.278511 -4.2644119 -4.27143 -4.2894559 -4.3057022 -4.3144112 -4.3164253][-4.2231708 -4.2363124 -4.2544408 -4.2776361 -4.2996149 -4.3121281 -4.3069754 -4.2908554 -4.2706146 -4.2598062 -4.2672949 -4.2829123 -4.2954011 -4.2995086 -4.2963958][-4.2623415 -4.2641912 -4.26982 -4.2837863 -4.2999382 -4.3083906 -4.3028193 -4.2885461 -4.2758904 -4.27107 -4.2785649 -4.2877617 -4.2918081 -4.2904339 -4.2865992]]...]
INFO - root - 2017-12-07 20:49:40.521010: step 42910, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 72h:51m:34s remains)
INFO - root - 2017-12-07 20:49:50.192672: step 42920, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 76h:24m:38s remains)
INFO - root - 2017-12-07 20:49:59.961346: step 42930, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 79h:18m:05s remains)
INFO - root - 2017-12-07 20:50:09.337463: step 42940, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 76h:31m:49s remains)
INFO - root - 2017-12-07 20:50:18.934918: step 42950, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 81h:02m:53s remains)
INFO - root - 2017-12-07 20:50:28.612309: step 42960, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 81h:26m:13s remains)
INFO - root - 2017-12-07 20:50:38.317938: step 42970, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.017 sec/batch; 81h:49m:24s remains)
INFO - root - 2017-12-07 20:50:47.935327: step 42980, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 76h:08m:49s remains)
INFO - root - 2017-12-07 20:50:57.599887: step 42990, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 77h:45m:19s remains)
INFO - root - 2017-12-07 20:51:07.281925: step 43000, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.985 sec/batch; 79h:13m:10s remains)
2017-12-07 20:51:08.315371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2339764 -4.2442455 -4.2462878 -4.2480459 -4.2416139 -4.2121043 -4.1671662 -4.1352777 -4.12153 -4.1381564 -4.1872044 -4.2407565 -4.2848763 -4.3122144 -4.3292618][-4.2456717 -4.2534232 -4.2580638 -4.261878 -4.2576528 -4.2321334 -4.1862817 -4.138629 -4.10488 -4.1173143 -4.1721487 -4.2350755 -4.2859015 -4.3149419 -4.3298655][-4.2728209 -4.2784672 -4.2814469 -4.2791657 -4.2674475 -4.2376161 -4.1880984 -4.1251979 -4.07958 -4.09503 -4.1626334 -4.2361026 -4.2918739 -4.3198681 -4.330462][-4.3112144 -4.3090773 -4.3010855 -4.2897062 -4.2687459 -4.2302303 -4.1707478 -4.0931034 -4.0425935 -4.0676532 -4.1505451 -4.2349019 -4.2955256 -4.322752 -4.3312497][-4.3389316 -4.329577 -4.3100982 -4.2890768 -4.2595639 -4.2096167 -4.1360388 -4.0473 -4.0056863 -4.0504532 -4.1467013 -4.2388182 -4.2980714 -4.323369 -4.331387][-4.346674 -4.3369193 -4.310576 -4.2780485 -4.241919 -4.1841874 -4.0942931 -4.000267 -3.9782858 -4.0487452 -4.1573052 -4.249299 -4.3026142 -4.3240023 -4.3319759][-4.3416967 -4.3388586 -4.3136024 -4.27943 -4.2411814 -4.1783323 -4.0754194 -3.9778779 -3.9715638 -4.0574932 -4.1714773 -4.2598443 -4.3067479 -4.3252854 -4.3335519][-4.328023 -4.3391089 -4.3296533 -4.305223 -4.2675691 -4.1982627 -4.0896921 -3.9916568 -3.9881742 -4.0705204 -4.1772332 -4.2618032 -4.3073416 -4.325305 -4.3336225][-4.3118868 -4.3391242 -4.3453417 -4.3313041 -4.2962928 -4.2284594 -4.1289515 -4.04096 -4.0354223 -4.0988154 -4.186491 -4.2622743 -4.3068705 -4.3249364 -4.3336234][-4.2907958 -4.333261 -4.3530755 -4.34898 -4.3217125 -4.261343 -4.1768084 -4.1053147 -4.0958185 -4.139091 -4.2056074 -4.2689352 -4.3078995 -4.3245263 -4.3346276][-4.2605467 -4.3173513 -4.3526282 -4.3623238 -4.3482647 -4.3006144 -4.231328 -4.1728816 -4.1583724 -4.1854587 -4.2340169 -4.2847042 -4.3166471 -4.3292408 -4.3378367][-4.2244968 -4.2952051 -4.3465271 -4.3697596 -4.3679662 -4.3294916 -4.2696533 -4.2194428 -4.2051034 -4.2242222 -4.261198 -4.3023953 -4.3278732 -4.3362341 -4.3413415][-4.1787357 -4.264554 -4.3291259 -4.3634734 -4.37048 -4.3394518 -4.2866564 -4.2414865 -4.23008 -4.2457166 -4.2781763 -4.3144774 -4.33532 -4.3411665 -4.3439426][-4.1328311 -4.2319765 -4.3063965 -4.3480887 -4.3619943 -4.3398719 -4.2947979 -4.2516036 -4.2369113 -4.2496023 -4.2822809 -4.3177834 -4.336524 -4.3415575 -4.3451271][-4.0873432 -4.1977615 -4.2829723 -4.331646 -4.347506 -4.3282862 -4.28942 -4.2483234 -4.2337542 -4.2472496 -4.2827067 -4.3190784 -4.3357348 -4.339397 -4.3443379]]...]
INFO - root - 2017-12-07 20:51:18.137628: step 43010, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 78h:20m:47s remains)
INFO - root - 2017-12-07 20:51:28.037050: step 43020, loss = 2.10, batch loss = 2.05 (7.9 examples/sec; 1.007 sec/batch; 80h:56m:51s remains)
INFO - root - 2017-12-07 20:51:37.753872: step 43030, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 73h:41m:20s remains)
INFO - root - 2017-12-07 20:51:47.345881: step 43040, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 77h:02m:05s remains)
INFO - root - 2017-12-07 20:51:57.070225: step 43050, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 75h:38m:56s remains)
INFO - root - 2017-12-07 20:52:06.595989: step 43060, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 74h:59m:52s remains)
INFO - root - 2017-12-07 20:52:16.242609: step 43070, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 75h:33m:33s remains)
INFO - root - 2017-12-07 20:52:25.889245: step 43080, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 76h:51m:28s remains)
INFO - root - 2017-12-07 20:52:35.580070: step 43090, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 76h:23m:41s remains)
INFO - root - 2017-12-07 20:52:45.326910: step 43100, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.008 sec/batch; 81h:04m:18s remains)
2017-12-07 20:52:46.301970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.145113 -4.1318779 -4.1336913 -4.15986 -4.1993322 -4.2301 -4.239593 -4.2349839 -4.2294846 -4.2290173 -4.2329383 -4.2486143 -4.2646365 -4.2551994 -4.2295871][-4.1782837 -4.1415391 -4.0995026 -4.0922527 -4.1211653 -4.1555166 -4.1746073 -4.18581 -4.1977596 -4.2099304 -4.2166505 -4.2320766 -4.2488384 -4.2434411 -4.2203121][-4.2098117 -4.1702991 -4.102777 -4.0479956 -4.0373921 -4.0511084 -4.0758505 -4.108418 -4.1418629 -4.1679649 -4.1849704 -4.2089872 -4.2245283 -4.2154179 -4.1953182][-4.2238793 -4.1993685 -4.1306357 -4.0494757 -3.9892719 -3.9559653 -3.9694362 -4.0250692 -4.0798516 -4.115664 -4.145802 -4.1887493 -4.2135324 -4.2052107 -4.1847181][-4.2198133 -4.2144094 -4.1601644 -4.0808148 -4.0005169 -3.9292936 -3.908021 -3.9685259 -4.0337672 -4.0753794 -4.1174874 -4.179235 -4.2181168 -4.2184219 -4.2015061][-4.21056 -4.2169037 -4.1808715 -4.1211257 -4.0534415 -3.9785943 -3.9374776 -3.9729326 -4.0229335 -4.0574894 -4.107173 -4.1764789 -4.2253389 -4.2368026 -4.225934][-4.2147784 -4.2248769 -4.1981626 -4.1587973 -4.1159124 -4.0582604 -4.0177841 -4.021668 -4.0373411 -4.0574508 -4.1075988 -4.1771178 -4.2285094 -4.2483044 -4.2443738][-4.2317295 -4.2371931 -4.2144542 -4.1848054 -4.1627321 -4.1311531 -4.100771 -4.0849791 -4.0719652 -4.076282 -4.1209249 -4.1821361 -4.2295218 -4.2523718 -4.2511177][-4.24682 -4.24434 -4.2253847 -4.200747 -4.1867409 -4.1744361 -4.1546803 -4.1294355 -4.1007342 -4.0874162 -4.1168623 -4.1698461 -4.2159119 -4.23989 -4.2437119][-4.2506442 -4.2426291 -4.22664 -4.2073 -4.1982489 -4.19658 -4.1836624 -4.1555548 -4.11546 -4.0813165 -4.0898905 -4.1351213 -4.1815572 -4.2110314 -4.2207503][-4.2622113 -4.2563667 -4.2424774 -4.2268577 -4.2195506 -4.2182164 -4.2059164 -4.1749725 -4.1253581 -4.0697703 -4.0541368 -4.0873604 -4.1382933 -4.1778827 -4.1929865][-4.2852254 -4.282917 -4.2739134 -4.2600508 -4.2512307 -4.2492523 -4.2399416 -4.2043667 -4.1432152 -4.0715637 -4.038012 -4.0533748 -4.0999479 -4.1479788 -4.168808][-4.3072815 -4.3045464 -4.2977242 -4.2842612 -4.27648 -4.2758651 -4.2724905 -4.2413397 -4.1775246 -4.1042209 -4.0655909 -4.0649056 -4.0959973 -4.1341982 -4.1545868][-4.31324 -4.305675 -4.2960734 -4.283927 -4.2810311 -4.2850676 -4.2863374 -4.2664866 -4.2087336 -4.1465874 -4.1158428 -4.1086311 -4.1208816 -4.1396918 -4.1540771][-4.3059988 -4.2933273 -4.2779708 -4.2634935 -4.2669153 -4.2789836 -4.2848988 -4.2705145 -4.2245388 -4.1800904 -4.1591787 -4.1495733 -4.1486087 -4.1532049 -4.166245]]...]
INFO - root - 2017-12-07 20:52:55.762619: step 43110, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 75h:53m:02s remains)
INFO - root - 2017-12-07 20:53:05.392236: step 43120, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.008 sec/batch; 81h:01m:56s remains)
INFO - root - 2017-12-07 20:53:15.056274: step 43130, loss = 2.10, batch loss = 2.04 (7.7 examples/sec; 1.044 sec/batch; 83h:54m:22s remains)
INFO - root - 2017-12-07 20:53:24.830912: step 43140, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 78h:37m:02s remains)
INFO - root - 2017-12-07 20:53:34.389640: step 43150, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.919 sec/batch; 73h:52m:27s remains)
INFO - root - 2017-12-07 20:53:44.248354: step 43160, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.994 sec/batch; 79h:51m:25s remains)
INFO - root - 2017-12-07 20:53:53.910328: step 43170, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.026 sec/batch; 82h:26m:59s remains)
INFO - root - 2017-12-07 20:54:03.617282: step 43180, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.999 sec/batch; 80h:17m:44s remains)
INFO - root - 2017-12-07 20:54:13.343383: step 43190, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 75h:44m:28s remains)
INFO - root - 2017-12-07 20:54:23.112455: step 43200, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.026 sec/batch; 82h:25m:20s remains)
2017-12-07 20:54:24.159835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2326155 -4.226016 -4.215744 -4.214839 -4.223958 -4.2398167 -4.2613525 -4.2756357 -4.27823 -4.2749109 -4.278244 -4.28548 -4.282198 -4.2620564 -4.2286234][-4.2147393 -4.2254977 -4.2223926 -4.2181511 -4.220674 -4.23302 -4.2561874 -4.2779493 -4.28713 -4.2882495 -4.2936635 -4.2986746 -4.2942152 -4.2786369 -4.2547064][-4.2203512 -4.2344975 -4.2294993 -4.2183042 -4.2142582 -4.2206373 -4.2433209 -4.2734241 -4.2903633 -4.2953162 -4.3002028 -4.2975831 -4.2896466 -4.2778678 -4.2675138][-4.2401752 -4.241672 -4.2268419 -4.2077255 -4.19613 -4.1991243 -4.221221 -4.2532444 -4.2746639 -4.2890558 -4.2997723 -4.295126 -4.2825217 -4.2740583 -4.2734351][-4.2569451 -4.2437406 -4.2168961 -4.1904211 -4.1759281 -4.1760435 -4.1946092 -4.2235322 -4.2487354 -4.2737255 -4.2915969 -4.2911234 -4.2814484 -4.2754321 -4.2789178][-4.2649622 -4.2455797 -4.215138 -4.1860118 -4.1674633 -4.1618338 -4.1711788 -4.1920514 -4.2202215 -4.2494669 -4.2710891 -4.2805409 -4.2816825 -4.2806983 -4.2843447][-4.2664189 -4.2570367 -4.2339735 -4.2039089 -4.1771092 -4.1605854 -4.1551638 -4.16248 -4.1875129 -4.2173953 -4.2451177 -4.2700315 -4.2865715 -4.2897444 -4.2892237][-4.2541227 -4.261775 -4.2551651 -4.2302985 -4.1973372 -4.1658859 -4.144536 -4.1414452 -4.1625786 -4.1921992 -4.2250733 -4.2637963 -4.2908545 -4.2945261 -4.289855][-4.2318244 -4.2532992 -4.2609048 -4.2435026 -4.2085762 -4.1694288 -4.1421118 -4.134654 -4.1495934 -4.175324 -4.2086363 -4.2570786 -4.2937312 -4.2990661 -4.2906313][-4.2267008 -4.2512341 -4.2626925 -4.2488108 -4.21744 -4.1823277 -4.15956 -4.1490746 -4.1528606 -4.17246 -4.2071967 -4.2590575 -4.3005404 -4.3062553 -4.2932954][-4.2500625 -4.2687492 -4.2769847 -4.2657967 -4.2404428 -4.2103763 -4.19278 -4.1831245 -4.1806574 -4.1959167 -4.2277794 -4.2718244 -4.3059497 -4.3129282 -4.3015852][-4.2823753 -4.293448 -4.2995391 -4.2926292 -4.2766814 -4.2527013 -4.2335291 -4.2207966 -4.2164841 -4.2251763 -4.2458005 -4.2771 -4.30507 -4.3132148 -4.3072562][-4.3060694 -4.3110051 -4.3150291 -4.3100648 -4.2996459 -4.2804914 -4.2617073 -4.2500744 -4.247479 -4.2499871 -4.2597542 -4.28001 -4.3010011 -4.3074 -4.3063345][-4.3096471 -4.3090925 -4.30678 -4.2999697 -4.2922807 -4.2775 -4.2622247 -4.2589359 -4.2645311 -4.2684565 -4.27157 -4.2826076 -4.295752 -4.300848 -4.3056159][-4.2894707 -4.28653 -4.2828236 -4.2757621 -4.2654018 -4.2504611 -4.2366519 -4.2412982 -4.2580075 -4.266335 -4.2673326 -4.2716689 -4.28097 -4.2903361 -4.3000865]]...]
INFO - root - 2017-12-07 20:54:33.649108: step 43210, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 69h:50m:10s remains)
INFO - root - 2017-12-07 20:54:43.209375: step 43220, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 76h:13m:57s remains)
INFO - root - 2017-12-07 20:54:52.737554: step 43230, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 80h:06m:23s remains)
INFO - root - 2017-12-07 20:55:02.578603: step 43240, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.027 sec/batch; 82h:30m:13s remains)
INFO - root - 2017-12-07 20:55:12.247247: step 43250, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 77h:40m:46s remains)
INFO - root - 2017-12-07 20:55:21.916824: step 43260, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 77h:46m:54s remains)
INFO - root - 2017-12-07 20:55:31.658427: step 43270, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.994 sec/batch; 79h:50m:26s remains)
INFO - root - 2017-12-07 20:55:41.243805: step 43280, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.976 sec/batch; 78h:24m:43s remains)
INFO - root - 2017-12-07 20:55:50.920730: step 43290, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 78h:50m:44s remains)
INFO - root - 2017-12-07 20:56:00.499147: step 43300, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.927 sec/batch; 74h:27m:39s remains)
2017-12-07 20:56:01.577971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2275653 -4.2522888 -4.2683659 -4.2708387 -4.2609825 -4.2544913 -4.2488394 -4.2409639 -4.2356882 -4.22971 -4.2225251 -4.2129588 -4.1896162 -4.1599278 -4.1594996][-4.2129169 -4.2492414 -4.2730203 -4.2761455 -4.25931 -4.2424684 -4.2315063 -4.2187114 -4.2093863 -4.2002749 -4.1939664 -4.1884518 -4.1751614 -4.152853 -4.1535773][-4.2129493 -4.2577276 -4.2873163 -4.288836 -4.2643852 -4.2354412 -4.212575 -4.1922617 -4.1825686 -4.1719031 -4.1623769 -4.1639347 -4.1669068 -4.1551008 -4.1529551][-4.2106972 -4.2623544 -4.2977738 -4.2940578 -4.2592154 -4.2175055 -4.1810527 -4.1564813 -4.1528988 -4.1512303 -4.143507 -4.14767 -4.1538424 -4.149158 -4.1501036][-4.2101541 -4.2641072 -4.3028731 -4.2940245 -4.2512503 -4.1989942 -4.1471825 -4.1151509 -4.1214104 -4.1367273 -4.1400356 -4.1454749 -4.1439371 -4.1333871 -4.1324716][-4.2210422 -4.2718067 -4.3024735 -4.2831039 -4.2291059 -4.1581388 -4.0842724 -4.0443864 -4.0677314 -4.1095848 -4.1343732 -4.1513319 -4.1458178 -4.12212 -4.109129][-4.2310858 -4.2733841 -4.2910151 -4.2605348 -4.1923728 -4.0935354 -3.982151 -3.9310699 -3.9779947 -4.0576735 -4.1155834 -4.1535864 -4.1556911 -4.1275663 -4.1070309][-4.234694 -4.2668104 -4.2786312 -4.2456117 -4.170938 -4.0557957 -3.9212365 -3.8643394 -3.9270036 -4.0246539 -4.1017065 -4.1561179 -4.16936 -4.1477914 -4.1332197][-4.236834 -4.2599797 -4.2695241 -4.2430496 -4.1795912 -4.0772505 -3.9567244 -3.9081273 -3.9587376 -4.0361843 -4.1059456 -4.1652923 -4.1888032 -4.17891 -4.1704373][-4.2348566 -4.2482595 -4.2572432 -4.2432265 -4.2031207 -4.1338444 -4.0497217 -4.0135541 -4.0341411 -4.0716228 -4.1169658 -4.169383 -4.2014065 -4.203917 -4.1989384][-4.2315822 -4.2411022 -4.251483 -4.2499237 -4.2287164 -4.1873875 -4.1311865 -4.102046 -4.1019878 -4.1125164 -4.1342578 -4.174747 -4.2072868 -4.2161241 -4.2129054][-4.2244158 -4.23621 -4.2493978 -4.2562604 -4.248888 -4.227046 -4.187469 -4.1562595 -4.140132 -4.1359034 -4.1452737 -4.170578 -4.1962752 -4.2104144 -4.2119393][-4.2130733 -4.2297039 -4.2442446 -4.2576346 -4.2642245 -4.2598896 -4.2316647 -4.1981616 -4.1698508 -4.152977 -4.1488624 -4.1508102 -4.1646557 -4.1796317 -4.1850381][-4.2137885 -4.2346063 -4.2497721 -4.2621984 -4.276794 -4.285295 -4.2689304 -4.2408934 -4.2087493 -4.1828413 -4.1636176 -4.1434183 -4.1445246 -4.1568336 -4.1609631][-4.2188325 -4.2412891 -4.2530589 -4.2601113 -4.2746024 -4.2864442 -4.2806153 -4.2611938 -4.2352161 -4.2142377 -4.1951728 -4.1669278 -4.160583 -4.1716971 -4.1693935]]...]
INFO - root - 2017-12-07 20:56:11.283889: step 43310, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 75h:52m:48s remains)
INFO - root - 2017-12-07 20:56:20.962669: step 43320, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 75h:11m:24s remains)
INFO - root - 2017-12-07 20:56:30.451993: step 43330, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 79h:47m:23s remains)
INFO - root - 2017-12-07 20:56:40.109527: step 43340, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 74h:31m:33s remains)
INFO - root - 2017-12-07 20:56:49.884615: step 43350, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 78h:00m:48s remains)
INFO - root - 2017-12-07 20:56:59.587472: step 43360, loss = 2.10, batch loss = 2.05 (8.9 examples/sec; 0.897 sec/batch; 72h:01m:33s remains)
INFO - root - 2017-12-07 20:57:09.258975: step 43370, loss = 2.12, batch loss = 2.06 (8.5 examples/sec; 0.941 sec/batch; 75h:32m:54s remains)
INFO - root - 2017-12-07 20:57:18.914109: step 43380, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 79h:59m:00s remains)
INFO - root - 2017-12-07 20:57:28.695836: step 43390, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 77h:19m:16s remains)
INFO - root - 2017-12-07 20:57:38.517956: step 43400, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 80h:04m:29s remains)
2017-12-07 20:57:39.506234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1006413 -4.0783095 -4.1008053 -4.1643267 -4.1984076 -4.1883178 -4.1474671 -4.1393008 -4.1679182 -4.16334 -4.1304803 -4.0963831 -4.0742197 -4.1125593 -4.1677847][-4.1054783 -4.0752997 -4.1023493 -4.1729164 -4.2017922 -4.1816607 -4.1317492 -4.1286674 -4.1718535 -4.1711507 -4.1295867 -4.0789661 -4.0502295 -4.0999584 -4.1650753][-4.10836 -4.0761924 -4.1097932 -4.18096 -4.1971745 -4.1614385 -4.101438 -4.1035762 -4.1651888 -4.1777654 -4.1399474 -4.0858822 -4.0560684 -4.1099248 -4.1751585][-4.0975075 -4.0704775 -4.11037 -4.1783142 -4.1778793 -4.1181388 -4.0442309 -4.0596581 -4.1463108 -4.1810837 -4.1579208 -4.1072354 -4.0824208 -4.1318431 -4.1872172][-4.069932 -4.0475073 -4.0933614 -4.15747 -4.1435108 -4.0583396 -3.9664609 -4.0017877 -4.1131 -4.1724563 -4.1722717 -4.1353326 -4.11704 -4.1521778 -4.1899233][-4.0390406 -4.0185814 -4.0703611 -4.1324339 -4.1090961 -4.001709 -3.8874071 -3.9395397 -4.0700502 -4.1431918 -4.1614203 -4.13918 -4.1236825 -4.1475625 -4.1718235][-4.0410428 -4.0196962 -4.0709929 -4.1305714 -4.1019368 -3.9783204 -3.8464577 -3.9029012 -4.0350523 -4.1068063 -4.1273346 -4.1077876 -4.0899434 -4.1093383 -4.1291928][-4.0793371 -4.0525227 -4.0969934 -4.1522231 -4.1272368 -4.0140343 -3.9001832 -3.9461584 -4.0497251 -4.1010056 -4.1067104 -4.0749025 -4.0518074 -4.072217 -4.0910487][-4.1243048 -4.0937605 -4.1316357 -4.1821713 -4.1673985 -4.0793266 -3.9999294 -4.0393887 -4.1085835 -4.1347895 -4.1174431 -4.0748076 -4.0561385 -4.077353 -4.090044][-4.1641059 -4.1370373 -4.1710854 -4.2125659 -4.2007852 -4.1294227 -4.0782142 -4.123672 -4.1762443 -4.1889658 -4.1566467 -4.1096735 -4.100565 -4.121706 -4.1240454][-4.1868525 -4.1621971 -4.1932235 -4.2276807 -4.2180529 -4.1642289 -4.1338983 -4.1847248 -4.2328267 -4.2411308 -4.2060871 -4.1631041 -4.1589823 -4.1734982 -4.1659036][-4.1818914 -4.1575289 -4.1917357 -4.22864 -4.2297192 -4.1953568 -4.1749077 -4.2216811 -4.2622676 -4.2694526 -4.2371626 -4.2001472 -4.1935596 -4.198092 -4.1808047][-4.1640658 -4.1421642 -4.1778326 -4.2148204 -4.2223582 -4.2009363 -4.1914082 -4.2355642 -4.26918 -4.2758985 -4.2452111 -4.2083521 -4.1922855 -4.18591 -4.1629033][-4.1553378 -4.1391144 -4.17229 -4.2041225 -4.2135334 -4.19881 -4.1985826 -4.23749 -4.2642593 -4.2657518 -4.2311816 -4.1896629 -4.1651487 -4.1546192 -4.137907][-4.1621413 -4.1571183 -4.1830463 -4.2035494 -4.2093639 -4.1958866 -4.1948137 -4.2284617 -4.2519269 -4.2431951 -4.2026343 -4.1557784 -4.1259632 -4.1175723 -4.11507]]...]
INFO - root - 2017-12-07 20:57:48.926678: step 43410, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 76h:57m:40s remains)
INFO - root - 2017-12-07 20:57:58.458646: step 43420, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 0.798 sec/batch; 64h:04m:35s remains)
INFO - root - 2017-12-07 20:58:08.125633: step 43430, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 74h:59m:06s remains)
INFO - root - 2017-12-07 20:58:17.642530: step 43440, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.886 sec/batch; 71h:09m:33s remains)
INFO - root - 2017-12-07 20:58:27.210367: step 43450, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 78h:11m:27s remains)
INFO - root - 2017-12-07 20:58:36.808213: step 43460, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 77h:08m:51s remains)
INFO - root - 2017-12-07 20:58:46.631957: step 43470, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 80h:15m:52s remains)
INFO - root - 2017-12-07 20:58:56.253555: step 43480, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 73h:58m:40s remains)
INFO - root - 2017-12-07 20:59:05.919496: step 43490, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 80h:22m:50s remains)
INFO - root - 2017-12-07 20:59:15.668268: step 43500, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.026 sec/batch; 82h:23m:48s remains)
2017-12-07 20:59:16.673754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2849479 -4.280108 -4.2781425 -4.2728615 -4.2659688 -4.26271 -4.2645583 -4.2648816 -4.252171 -4.2245159 -4.193049 -4.1805439 -4.18785 -4.2110171 -4.2228031][-4.2694507 -4.2647872 -4.2633581 -4.2580843 -4.2502313 -4.246057 -4.2506695 -4.2542434 -4.244493 -4.2234516 -4.1972542 -4.1791787 -4.17538 -4.1886039 -4.2029166][-4.2516437 -4.2460318 -4.2443113 -4.2395034 -4.2314487 -4.2266755 -4.2308946 -4.2352023 -4.2303109 -4.2168236 -4.1982274 -4.1768165 -4.1650195 -4.1707039 -4.1885839][-4.2308464 -4.2224951 -4.2198639 -4.2140927 -4.207366 -4.2008643 -4.1976051 -4.1966834 -4.1956458 -4.1923652 -4.1855383 -4.1723256 -4.1617107 -4.1693769 -4.1966467][-4.2073741 -4.196775 -4.1953712 -4.1884503 -4.1784377 -4.1663513 -4.1506906 -4.1374483 -4.1341805 -4.1412272 -4.1507878 -4.1503086 -4.1489506 -4.164793 -4.2028108][-4.185358 -4.1726594 -4.1692772 -4.1624966 -4.1491976 -4.1303639 -4.0989118 -4.0638995 -4.0485191 -4.0646172 -4.0929036 -4.1167145 -4.1364117 -4.1649261 -4.2082529][-4.1712155 -4.1549711 -4.1456671 -4.1336837 -4.1120729 -4.0791159 -4.0235496 -3.9541194 -3.9236298 -3.9712811 -4.04084 -4.1036053 -4.1470633 -4.1824236 -4.2184305][-4.1654296 -4.1458282 -4.1287947 -4.1080089 -4.0779462 -4.0313368 -3.9573164 -3.8689892 -3.8486569 -3.9483671 -4.0529513 -4.1291246 -4.1743088 -4.2027607 -4.2215753][-4.1690693 -4.1497235 -4.1274166 -4.1043077 -4.0783334 -4.0362225 -3.9795265 -3.9307339 -3.9427021 -4.0299034 -4.1106853 -4.163753 -4.1949153 -4.2139106 -4.21926][-4.1840072 -4.1698456 -4.1491041 -4.1328521 -4.1231308 -4.0999417 -4.0679283 -4.0516152 -4.06657 -4.1096449 -4.1511841 -4.1787958 -4.1989374 -4.2146096 -4.2173605][-4.1993351 -4.1863446 -4.1700778 -4.1622248 -4.1661687 -4.1576881 -4.1374369 -4.1299667 -4.1415606 -4.1559577 -4.1721392 -4.1873307 -4.2019639 -4.215106 -4.2151942][-4.2027521 -4.1863489 -4.1734829 -4.170907 -4.18102 -4.1818891 -4.1709456 -4.170722 -4.1799955 -4.1811366 -4.186059 -4.198638 -4.2122779 -4.2218957 -4.2204275][-4.1977072 -4.1761174 -4.1617131 -4.1609993 -4.1747766 -4.1845427 -4.1842146 -4.189487 -4.1979933 -4.1994257 -4.2079473 -4.2214561 -4.2330942 -4.2427015 -4.2427216][-4.2005949 -4.1782002 -4.1620431 -4.1637034 -4.17825 -4.1903071 -4.1889515 -4.18994 -4.1987472 -4.2125912 -4.2306461 -4.2452874 -4.2545166 -4.2635584 -4.2652793][-4.2269 -4.2131319 -4.2008038 -4.201735 -4.2070041 -4.207602 -4.1986294 -4.19371 -4.2034383 -4.2294531 -4.2502804 -4.2619553 -4.2678089 -4.2742577 -4.2773829]]...]
INFO - root - 2017-12-07 20:59:26.375879: step 43510, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.997 sec/batch; 80h:00m:02s remains)
INFO - root - 2017-12-07 20:59:35.872969: step 43520, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 76h:05m:16s remains)
INFO - root - 2017-12-07 20:59:45.497256: step 43530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 77h:04m:13s remains)
INFO - root - 2017-12-07 20:59:55.171644: step 43540, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 74h:37m:25s remains)
INFO - root - 2017-12-07 21:00:04.830873: step 43550, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.909 sec/batch; 72h:59m:07s remains)
INFO - root - 2017-12-07 21:00:14.530549: step 43560, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.999 sec/batch; 80h:11m:35s remains)
INFO - root - 2017-12-07 21:00:24.321866: step 43570, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 75h:57m:19s remains)
INFO - root - 2017-12-07 21:00:33.903624: step 43580, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.922 sec/batch; 74h:00m:36s remains)
INFO - root - 2017-12-07 21:00:43.521230: step 43590, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 78h:48m:07s remains)
INFO - root - 2017-12-07 21:00:53.189761: step 43600, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.033 sec/batch; 82h:52m:27s remains)
2017-12-07 21:00:54.077509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2697196 -4.257143 -4.249094 -4.2403855 -4.236681 -4.2406 -4.2523165 -4.2725077 -4.2915649 -4.3060145 -4.31015 -4.3082128 -4.3054514 -4.2948174 -4.2766609][-4.2725406 -4.268465 -4.263413 -4.255641 -4.254261 -4.2582808 -4.2651038 -4.27634 -4.2903528 -4.3026705 -4.3066034 -4.3071923 -4.307229 -4.3001084 -4.2829781][-4.2512436 -4.25002 -4.2464828 -4.2455254 -4.254045 -4.2657866 -4.2739949 -4.2803841 -4.2885489 -4.2938881 -4.2943845 -4.2960668 -4.2990785 -4.297493 -4.2866635][-4.2156906 -4.2127886 -4.2091165 -4.218524 -4.2399173 -4.2606373 -4.272315 -4.2784653 -4.2851467 -4.2862244 -4.282527 -4.283463 -4.2881188 -4.28985 -4.285007][-4.187295 -4.1795363 -4.1736178 -4.1904716 -4.221056 -4.2440991 -4.2533669 -4.255188 -4.258728 -4.258955 -4.254847 -4.2555504 -4.2637806 -4.2703767 -4.2707539][-4.1597629 -4.147728 -4.1413383 -4.15806 -4.18451 -4.1980758 -4.1971736 -4.1916509 -4.1934066 -4.1953344 -4.1915174 -4.1880832 -4.196321 -4.2086229 -4.2149305][-4.1182871 -4.1057897 -4.1022868 -4.1132569 -4.1225276 -4.1175208 -4.09761 -4.0821776 -4.0869694 -4.0961347 -4.0952096 -4.0865431 -4.0924377 -4.1094956 -4.1229916][-4.099268 -4.09257 -4.0947595 -4.1015673 -4.09788 -4.0815759 -4.0530696 -4.0337095 -4.0383615 -4.0492764 -4.048439 -4.035018 -4.0333118 -4.047533 -4.063333][-4.1250772 -4.1218758 -4.1301355 -4.1388307 -4.1352978 -4.1254764 -4.10901 -4.0964155 -4.0976477 -4.1021748 -4.0988526 -4.085434 -4.078999 -4.0855546 -4.0948229][-4.1739 -4.1696849 -4.1780443 -4.1862249 -4.1845474 -4.1828184 -4.1807184 -4.177599 -4.1771507 -4.1756454 -4.1724997 -4.1665406 -4.1621804 -4.1609845 -4.1595988][-4.2220964 -4.2144384 -4.2180133 -4.2205992 -4.2182169 -4.2196503 -4.2239685 -4.2241082 -4.2214985 -4.2176147 -4.2147365 -4.2147937 -4.2165785 -4.2133203 -4.2049017][-4.2471576 -4.2366643 -4.2332006 -4.2300143 -4.225544 -4.2285471 -4.2355094 -4.2366309 -4.2337923 -4.2313061 -4.2312603 -4.2365575 -4.2449803 -4.2439041 -4.2349291][-4.2604151 -4.2542195 -4.2505946 -4.2464867 -4.242064 -4.2429752 -4.2481003 -4.2497039 -4.2479277 -4.2472682 -4.2475352 -4.2523227 -4.2619433 -4.2637968 -4.2562013][-4.2736416 -4.2729869 -4.2736893 -4.2739043 -4.2732787 -4.2736959 -4.2769194 -4.2783642 -4.2770643 -4.2757497 -4.2728629 -4.2729053 -4.2783685 -4.27841 -4.2705412][-4.300117 -4.3020372 -4.302989 -4.3033719 -4.3032293 -4.3032455 -4.3055534 -4.30628 -4.3040123 -4.3008432 -4.2961988 -4.2942104 -4.2973766 -4.2967396 -4.2867613]]...]
INFO - root - 2017-12-07 21:01:03.659544: step 43610, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 75h:00m:45s remains)
INFO - root - 2017-12-07 21:01:13.200338: step 43620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 73h:54m:16s remains)
INFO - root - 2017-12-07 21:01:22.887697: step 43630, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 79h:42m:50s remains)
INFO - root - 2017-12-07 21:01:32.551467: step 43640, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 77h:14m:13s remains)
INFO - root - 2017-12-07 21:01:42.243555: step 43650, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.905 sec/batch; 72h:36m:30s remains)
INFO - root - 2017-12-07 21:01:51.880296: step 43660, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 76h:47m:15s remains)
INFO - root - 2017-12-07 21:02:01.491187: step 43670, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.027 sec/batch; 82h:25m:25s remains)
INFO - root - 2017-12-07 21:02:11.114539: step 43680, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 79h:35m:01s remains)
INFO - root - 2017-12-07 21:02:20.834567: step 43690, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 77h:39m:53s remains)
INFO - root - 2017-12-07 21:02:30.465490: step 43700, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 73h:55m:40s remains)
2017-12-07 21:02:31.433674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2247677 -4.1891522 -4.1597247 -4.1500759 -4.1637826 -4.2037039 -4.2489557 -4.2830911 -4.3017411 -4.3101282 -4.3197389 -4.3293152 -4.3356886 -4.3387327 -4.3411622][-4.1772423 -4.1370778 -4.1078286 -4.1152248 -4.151134 -4.2038231 -4.25406 -4.2858872 -4.2973871 -4.2975755 -4.306251 -4.31719 -4.3268776 -4.33369 -4.3372097][-4.1165161 -4.0898728 -4.0812044 -4.1100965 -4.1637316 -4.2195539 -4.2595482 -4.2778344 -4.2780876 -4.26676 -4.2704821 -4.2809653 -4.299437 -4.3156505 -4.32177][-4.0900574 -4.0864305 -4.1000896 -4.1401172 -4.196352 -4.24173 -4.2615137 -4.2605391 -4.2482939 -4.2261252 -4.223978 -4.2388363 -4.2686486 -4.29429 -4.304502][-4.1127024 -4.1246023 -4.1450939 -4.1775579 -4.2179146 -4.2420626 -4.244597 -4.2252398 -4.1978831 -4.1666565 -4.1613541 -4.1828055 -4.2210627 -4.254478 -4.2702112][-4.1577926 -4.1701813 -4.1798511 -4.1903181 -4.2050967 -4.2051191 -4.1856394 -4.1458378 -4.1082993 -4.0794349 -4.07597 -4.1019464 -4.1524687 -4.1950636 -4.2156024][-4.213028 -4.2142859 -4.2026854 -4.1867156 -4.1674681 -4.1324577 -4.0821018 -4.023654 -3.9909444 -3.978905 -3.9894519 -4.0258808 -4.0933075 -4.1468844 -4.1731606][-4.2560153 -4.240799 -4.2130604 -4.1777935 -4.1315432 -4.0656977 -3.9867442 -3.926024 -3.9197514 -3.9379873 -3.9749165 -4.0294662 -4.1067853 -4.1624227 -4.1861019][-4.2815948 -4.2590694 -4.2280979 -4.1903763 -4.1392884 -4.070477 -3.9927585 -3.9539943 -3.97822 -4.0192227 -4.0729289 -4.13001 -4.188221 -4.2230325 -4.2335367][-4.2978373 -4.27778 -4.2519612 -4.2246647 -4.1900125 -4.1467509 -4.1032104 -4.0903153 -4.1150794 -4.1500487 -4.1977677 -4.2389965 -4.265305 -4.2774138 -4.2819834][-4.3045816 -4.2945428 -4.2799439 -4.2643619 -4.2453427 -4.22437 -4.20701 -4.2060351 -4.220778 -4.24041 -4.273747 -4.2991691 -4.3041005 -4.3039269 -4.30756][-4.3102984 -4.3082323 -4.3027072 -4.2943921 -4.2806587 -4.2639718 -4.2531066 -4.2531838 -4.2642984 -4.2802877 -4.3010941 -4.3147144 -4.3098235 -4.3029714 -4.3068056][-4.3214431 -4.3228121 -4.3218589 -4.3177357 -4.3056307 -4.2874961 -4.2730637 -4.2688503 -4.2763581 -4.2899356 -4.3041453 -4.3121362 -4.3062143 -4.2982731 -4.3028445][-4.3256969 -4.3279381 -4.3300385 -4.328639 -4.3173504 -4.296102 -4.2777867 -4.2683887 -4.2708778 -4.2816076 -4.2935643 -4.3011761 -4.2975969 -4.290843 -4.2948751][-4.3103871 -4.3141203 -4.3184342 -4.3193355 -4.311749 -4.2908144 -4.271626 -4.2611761 -4.2581406 -4.2627974 -4.2724314 -4.2788959 -4.2773094 -4.2738523 -4.27675]]...]
INFO - root - 2017-12-07 21:02:41.082740: step 43710, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 77h:14m:30s remains)
INFO - root - 2017-12-07 21:02:50.649258: step 43720, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 74h:09m:33s remains)
INFO - root - 2017-12-07 21:03:00.390741: step 43730, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 76h:34m:44s remains)
INFO - root - 2017-12-07 21:03:09.963860: step 43740, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 77h:20m:17s remains)
INFO - root - 2017-12-07 21:03:19.467785: step 43750, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 76h:25m:13s remains)
INFO - root - 2017-12-07 21:03:29.039035: step 43760, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.982 sec/batch; 78h:46m:28s remains)
INFO - root - 2017-12-07 21:03:38.750568: step 43770, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 79h:10m:44s remains)
INFO - root - 2017-12-07 21:03:48.362807: step 43780, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 75h:59m:44s remains)
INFO - root - 2017-12-07 21:03:58.022237: step 43790, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 77h:30m:32s remains)
INFO - root - 2017-12-07 21:04:07.533081: step 43800, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 80h:03m:14s remains)
2017-12-07 21:04:08.584897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3031693 -4.3094172 -4.3104324 -4.31178 -4.3117518 -4.3113651 -4.3110261 -4.3083305 -4.3039236 -4.2999063 -4.2994332 -4.2999935 -4.2906623 -4.2677164 -4.2424908][-4.3214765 -4.32602 -4.3262353 -4.3261952 -4.3243561 -4.3230114 -4.3233624 -4.3204346 -4.3123817 -4.3049183 -4.3051028 -4.3065896 -4.2967434 -4.2701292 -4.234201][-4.3374748 -4.3401709 -4.3375812 -4.3331966 -4.3271365 -4.3233981 -4.323473 -4.3194542 -4.3083563 -4.3006754 -4.3032708 -4.3108749 -4.3099046 -4.291563 -4.2573147][-4.3451567 -4.3459711 -4.3371935 -4.326004 -4.3131609 -4.3040614 -4.3030672 -4.295094 -4.2788134 -4.2728 -4.2792945 -4.2943773 -4.3069034 -4.3039122 -4.2795706][-4.337079 -4.3328066 -4.3176622 -4.3009124 -4.2806859 -4.2645669 -4.261807 -4.2499638 -4.2305317 -4.2268772 -4.2363262 -4.2576919 -4.2812581 -4.2919922 -4.2775531][-4.316514 -4.3040395 -4.283473 -4.2613091 -4.2309861 -4.2029409 -4.1948895 -4.18146 -4.162221 -4.1614947 -4.176321 -4.202848 -4.2370243 -4.2585196 -4.256649][-4.2859097 -4.2632256 -4.2324352 -4.1997919 -4.1489782 -4.097198 -4.0819936 -4.074657 -4.0657678 -4.0740237 -4.1000247 -4.1357241 -4.1825995 -4.2142673 -4.2198911][-4.2496219 -4.2176957 -4.1763659 -4.125217 -4.042603 -3.9622526 -3.9423862 -3.9481208 -3.9666681 -4.0007486 -4.0448508 -4.094142 -4.1512833 -4.1888871 -4.1912403][-4.2339573 -4.1986551 -4.1548944 -4.094655 -4.0010738 -3.9145098 -3.901967 -3.9228086 -3.9611483 -4.0069275 -4.0608163 -4.1144104 -4.1697607 -4.2022233 -4.202127][-4.2494993 -4.2208228 -4.1846361 -4.1340575 -4.059576 -3.998306 -3.9995542 -4.0220313 -4.0550985 -4.0924034 -4.1363454 -4.1782928 -4.219584 -4.243484 -4.2429996][-4.2688575 -4.2513595 -4.22658 -4.1926937 -4.14745 -4.1176848 -4.1287184 -4.1476879 -4.1727929 -4.1985826 -4.2266717 -4.2512331 -4.2761855 -4.2928457 -4.2909513][-4.2780461 -4.2720642 -4.2580104 -4.2423053 -4.2235065 -4.2152481 -4.2276678 -4.2414031 -4.2585149 -4.2734194 -4.2892137 -4.302094 -4.3138919 -4.322154 -4.32223][-4.2787709 -4.2867136 -4.2849402 -4.2836704 -4.2825041 -4.2834482 -4.2897153 -4.2949758 -4.30382 -4.3136573 -4.32257 -4.325562 -4.3270183 -4.3307281 -4.3324647][-4.2843761 -4.2981377 -4.3000946 -4.3011942 -4.3039765 -4.30605 -4.3064771 -4.3074493 -4.3139386 -4.3229795 -4.3317256 -4.3343754 -4.3328509 -4.3316779 -4.3313293][-4.2980309 -4.3084278 -4.3071914 -4.304235 -4.3029742 -4.3018475 -4.2996249 -4.2983785 -4.3049312 -4.3158612 -4.3262844 -4.3308678 -4.3303723 -4.3303957 -4.330976]]...]
INFO - root - 2017-12-07 21:04:18.258199: step 43810, loss = 2.12, batch loss = 2.06 (8.4 examples/sec; 0.953 sec/batch; 76h:27m:23s remains)
INFO - root - 2017-12-07 21:04:27.891468: step 43820, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 75h:12m:17s remains)
INFO - root - 2017-12-07 21:04:37.566441: step 43830, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 78h:26m:19s remains)
INFO - root - 2017-12-07 21:04:47.116563: step 43840, loss = 2.12, batch loss = 2.06 (8.4 examples/sec; 0.950 sec/batch; 76h:08m:50s remains)
INFO - root - 2017-12-07 21:04:56.876014: step 43850, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 74h:29m:00s remains)
INFO - root - 2017-12-07 21:05:06.469627: step 43860, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 79h:28m:46s remains)
INFO - root - 2017-12-07 21:05:16.224609: step 43870, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 81h:24m:22s remains)
INFO - root - 2017-12-07 21:05:25.797170: step 43880, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 79h:25m:58s remains)
INFO - root - 2017-12-07 21:05:35.456120: step 43890, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 75h:31m:42s remains)
INFO - root - 2017-12-07 21:05:45.077029: step 43900, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 76h:38m:02s remains)
2017-12-07 21:05:46.056715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2174349 -4.2227645 -4.2309976 -4.2310624 -4.2243075 -4.2161336 -4.214004 -4.2194281 -4.2303433 -4.2439361 -4.2524276 -4.2607775 -4.2667708 -4.2670674 -4.2542934][-4.2198191 -4.2236466 -4.2309327 -4.2316246 -4.2259493 -4.2177873 -4.2133756 -4.2162728 -4.2270851 -4.2420926 -4.24999 -4.2578392 -4.2640538 -4.2650003 -4.252306][-4.2173057 -4.2216144 -4.2305546 -4.235508 -4.2327123 -4.2234263 -4.2133584 -4.210084 -4.2200828 -4.2377071 -4.2481966 -4.2582226 -4.2660384 -4.2668743 -4.2520604][-4.2161012 -4.2219286 -4.2328081 -4.2418709 -4.2415428 -4.2308154 -4.2137518 -4.2035627 -4.2113075 -4.2306852 -4.2443981 -4.2581563 -4.268836 -4.2702918 -4.2539196][-4.209053 -4.2170715 -4.2310443 -4.2448111 -4.2475014 -4.2368407 -4.214396 -4.1975193 -4.2012887 -4.2207346 -4.237977 -4.2566996 -4.2716002 -4.2745676 -4.2570481][-4.1943483 -4.2053213 -4.2233958 -4.2430425 -4.2512059 -4.2425866 -4.2161679 -4.19208 -4.1908751 -4.2092104 -4.2300277 -4.2539544 -4.2730412 -4.2769928 -4.2587051][-4.1763778 -4.18944 -4.2111335 -4.2365417 -4.2510581 -4.2463036 -4.21863 -4.1886835 -4.1818542 -4.1980829 -4.2217512 -4.2496595 -4.2715206 -4.2760282 -4.2580147][-4.1637931 -4.1775112 -4.2010341 -4.2294464 -4.2476115 -4.2466054 -4.220118 -4.1868238 -4.17482 -4.1888776 -4.2143364 -4.2444324 -4.2677774 -4.2724891 -4.2548814][-4.1584277 -4.1709542 -4.1940885 -4.22249 -4.2414136 -4.2431307 -4.2191515 -4.1846228 -4.1687431 -4.1810455 -4.207541 -4.2388496 -4.2632542 -4.2684984 -4.2522058][-4.1553855 -4.1657515 -4.1873255 -4.2138772 -4.2328596 -4.2372208 -4.2166443 -4.1828218 -4.1643906 -4.1749063 -4.20142 -4.233129 -4.2586536 -4.2656655 -4.2518911][-4.1531563 -4.1604338 -4.1809292 -4.2065811 -4.2263589 -4.233222 -4.2162418 -4.1846662 -4.1655731 -4.17409 -4.1993361 -4.2297378 -4.255074 -4.2638607 -4.2532849][-4.1570535 -4.161376 -4.1812859 -4.2064605 -4.2267394 -4.2349749 -4.2209067 -4.1924534 -4.1741447 -4.1809816 -4.2038813 -4.2309475 -4.2540879 -4.2635875 -4.2561626][-4.1671238 -4.1688027 -4.1877017 -4.2115755 -4.231514 -4.24001 -4.2280426 -4.2035928 -4.1874776 -4.1932664 -4.213202 -4.2359667 -4.2557607 -4.2645893 -4.2590618][-4.180212 -4.1799431 -4.1970649 -4.2189136 -4.2376475 -4.246078 -4.2360926 -4.215507 -4.2018194 -4.2067361 -4.2235723 -4.2425308 -4.2591629 -4.2670069 -4.2627172][-4.1930542 -4.1911707 -4.2053404 -4.2243137 -4.2415595 -4.2500072 -4.2424111 -4.2255969 -4.2147236 -4.2189007 -4.2329154 -4.2490115 -4.263031 -4.2697272 -4.2665315]]...]
INFO - root - 2017-12-07 21:05:55.577022: step 43910, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 74h:03m:24s remains)
INFO - root - 2017-12-07 21:06:05.288816: step 43920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 73h:45m:49s remains)
INFO - root - 2017-12-07 21:06:15.116083: step 43930, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 77h:00m:53s remains)
INFO - root - 2017-12-07 21:06:24.677096: step 43940, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 72h:17m:41s remains)
INFO - root - 2017-12-07 21:06:34.319076: step 43950, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 74h:25m:16s remains)
INFO - root - 2017-12-07 21:06:44.013769: step 43960, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 75h:38m:55s remains)
INFO - root - 2017-12-07 21:06:53.594957: step 43970, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 74h:40m:12s remains)
INFO - root - 2017-12-07 21:07:03.324044: step 43980, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 79h:29m:11s remains)
INFO - root - 2017-12-07 21:07:13.227606: step 43990, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 75h:25m:28s remains)
INFO - root - 2017-12-07 21:07:22.927645: step 44000, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 77h:25m:08s remains)
2017-12-07 21:07:23.984672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1878829 -4.1444316 -4.1057048 -4.08597 -4.0929976 -4.112762 -4.1202221 -4.1333094 -4.1645894 -4.2158079 -4.2620955 -4.2888651 -4.3065548 -4.31527 -4.3237953][-4.1844053 -4.1427908 -4.1019826 -4.0800853 -4.0872226 -4.1134958 -4.1254725 -4.1384726 -4.167346 -4.2164359 -4.2598429 -4.2823553 -4.2985439 -4.3053136 -4.3127413][-4.1924877 -4.1613522 -4.1289763 -4.1121211 -4.1228328 -4.1533313 -4.1699305 -4.1799288 -4.1992745 -4.2390285 -4.2730594 -4.2882237 -4.3000197 -4.3030262 -4.3082981][-4.203177 -4.1768284 -4.1549082 -4.1495576 -4.1634665 -4.1926532 -4.20949 -4.2129421 -4.2189159 -4.2457213 -4.2721004 -4.2864814 -4.2978816 -4.3008976 -4.3054042][-4.2166324 -4.1850252 -4.1686187 -4.1744804 -4.1894708 -4.2110605 -4.2196836 -4.209177 -4.2017736 -4.2181768 -4.2414756 -4.2618504 -4.279665 -4.2885265 -4.296834][-4.2307968 -4.1924415 -4.1779766 -4.1867685 -4.1948562 -4.2009721 -4.1933966 -4.1624775 -4.14021 -4.1524668 -4.1800923 -4.2127953 -4.2443624 -4.2632666 -4.2795329][-4.2339578 -4.1954975 -4.1809368 -4.1819673 -4.1775808 -4.1695328 -4.1482415 -4.0990462 -4.0635443 -4.0740919 -4.1133122 -4.1627483 -4.2091241 -4.2378483 -4.26253][-4.2230697 -4.1880465 -4.1721568 -4.1619377 -4.1475987 -4.1358309 -4.1132941 -4.0596018 -4.0202622 -4.0331068 -4.0847535 -4.1473556 -4.2012286 -4.2350988 -4.2622809][-4.2017512 -4.1742067 -4.1588883 -4.1443071 -4.1282759 -4.1215272 -4.1081481 -4.0668087 -4.0380859 -4.0581079 -4.1115994 -4.1743631 -4.2283163 -4.259964 -4.2805061][-4.181993 -4.1604195 -4.1497703 -4.1378913 -4.1261559 -4.129178 -4.1277819 -4.1031837 -4.0898376 -4.1140385 -4.1609521 -4.2159386 -4.2629085 -4.28745 -4.2998171][-4.1890564 -4.1706843 -4.1619096 -4.1514835 -4.1475 -4.1583824 -4.1641731 -4.1524692 -4.1508393 -4.1732087 -4.2067108 -4.249063 -4.2854066 -4.3018117 -4.3092604][-4.2190633 -4.2050228 -4.1957588 -4.1817803 -4.1767087 -4.1869922 -4.1935205 -4.1932631 -4.2030506 -4.2218518 -4.242115 -4.269289 -4.2942271 -4.3039117 -4.3108768][-4.2422056 -4.2350683 -4.228354 -4.2118325 -4.2022214 -4.2075853 -4.2119784 -4.2187347 -4.2368321 -4.255188 -4.2651448 -4.2791181 -4.2928391 -4.2977629 -4.3065944][-4.2412381 -4.2385406 -4.2366452 -4.2252073 -4.2173109 -4.21824 -4.217412 -4.2247963 -4.2435989 -4.2629557 -4.266767 -4.2699714 -4.2752461 -4.2800732 -4.2950521][-4.2162566 -4.2118998 -4.2101512 -4.2081885 -4.2090783 -4.2100487 -4.2058158 -4.2112269 -4.227582 -4.246973 -4.2485571 -4.2448068 -4.2463017 -4.2563286 -4.2801523]]...]
INFO - root - 2017-12-07 21:07:33.599388: step 44010, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.041 sec/batch; 83h:25m:35s remains)
INFO - root - 2017-12-07 21:07:43.220844: step 44020, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 75h:16m:41s remains)
INFO - root - 2017-12-07 21:07:52.855167: step 44030, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 76h:24m:02s remains)
INFO - root - 2017-12-07 21:08:02.588584: step 44040, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 76h:47m:06s remains)
INFO - root - 2017-12-07 21:08:12.146556: step 44050, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 76h:07m:13s remains)
INFO - root - 2017-12-07 21:08:21.826739: step 44060, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 77h:00m:59s remains)
INFO - root - 2017-12-07 21:08:31.396518: step 44070, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 77h:51m:38s remains)
INFO - root - 2017-12-07 21:08:41.071090: step 44080, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 78h:05m:54s remains)
INFO - root - 2017-12-07 21:08:50.787276: step 44090, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 77h:25m:39s remains)
INFO - root - 2017-12-07 21:09:00.320404: step 44100, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 76h:14m:08s remains)
2017-12-07 21:09:01.250261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2605047 -4.2247691 -4.1942606 -4.1921506 -4.2042542 -4.200448 -4.1809664 -4.14941 -4.1435308 -4.1668153 -4.1947455 -4.2202926 -4.2394395 -4.2501721 -4.2607965][-4.2455025 -4.2183137 -4.19193 -4.1811042 -4.1887045 -4.1850929 -4.1693993 -4.13699 -4.1376476 -4.1791182 -4.2205229 -4.2550721 -4.2755532 -4.2736282 -4.2722311][-4.2401204 -4.2210917 -4.2088776 -4.2015247 -4.2087994 -4.1964512 -4.1703076 -4.1327558 -4.1392303 -4.1946898 -4.2489104 -4.2848272 -4.2950935 -4.2796907 -4.2713785][-4.229763 -4.2148671 -4.2199621 -4.2275333 -4.2356091 -4.2071033 -4.1629596 -4.1076341 -4.1136217 -4.1844659 -4.249073 -4.2796125 -4.2726493 -4.2469764 -4.234736][-4.2330203 -4.2202024 -4.2338648 -4.2415953 -4.2368703 -4.1858397 -4.1077089 -4.0239863 -4.0393887 -4.1238294 -4.192503 -4.2190561 -4.2044196 -4.1807208 -4.1694589][-4.2290154 -4.2184281 -4.2298961 -4.2241745 -4.2035379 -4.1265349 -4.0084538 -3.9002895 -3.940383 -4.0492048 -4.1258106 -4.1586289 -4.1498642 -4.1295896 -4.1179128][-4.1996121 -4.188426 -4.1931839 -4.1728892 -4.1314106 -4.0376406 -3.8985567 -3.7823954 -3.8517528 -3.9820859 -4.0686889 -4.1154923 -4.124877 -4.1190944 -4.1123505][-4.1881928 -4.1687455 -4.1645474 -4.1408763 -4.099709 -4.0213752 -3.911092 -3.826014 -3.8888245 -4.0018854 -4.0816422 -4.1263061 -4.1382565 -4.1435947 -4.150888][-4.2221785 -4.1974545 -4.181798 -4.1552606 -4.1235948 -4.0824642 -4.0220227 -3.9755826 -4.01553 -4.0949912 -4.1551714 -4.1861758 -4.187726 -4.1925907 -4.206419][-4.2745833 -4.2446861 -4.2214713 -4.1944795 -4.1753011 -4.1578393 -4.1255288 -4.0994015 -4.1235132 -4.1777678 -4.2221432 -4.2436543 -4.2405105 -4.2400818 -4.2506247][-4.3056965 -4.2779846 -4.2574043 -4.2383752 -4.230701 -4.2268486 -4.2073555 -4.193018 -4.2052937 -4.2406244 -4.2690964 -4.2848997 -4.2808185 -4.2786708 -4.2840624][-4.3156619 -4.2937765 -4.2731557 -4.2629328 -4.2648487 -4.268455 -4.2581649 -4.2497406 -4.2552528 -4.2763329 -4.2911134 -4.2993288 -4.2968884 -4.2951636 -4.3019743][-4.3090248 -4.2893758 -4.2678785 -4.2631822 -4.2695761 -4.2777729 -4.2770877 -4.2721486 -4.2738895 -4.2849917 -4.2887216 -4.2866464 -4.282711 -4.2832427 -4.2909155][-4.3042 -4.2870069 -4.2651606 -4.25747 -4.2612848 -4.2703943 -4.2746382 -4.2736535 -4.2755723 -4.2815113 -4.2815123 -4.2748942 -4.2684131 -4.2674441 -4.2725234][-4.3080449 -4.2974463 -4.280272 -4.2696142 -4.2692552 -4.275033 -4.27897 -4.2806249 -4.282373 -4.2849293 -4.2829943 -4.2765455 -4.2694216 -4.2667913 -4.2701011]]...]
INFO - root - 2017-12-07 21:09:10.755814: step 44110, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 78h:34m:57s remains)
INFO - root - 2017-12-07 21:09:20.351784: step 44120, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.024 sec/batch; 82h:03m:53s remains)
INFO - root - 2017-12-07 21:09:30.119579: step 44130, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 75h:47m:05s remains)
INFO - root - 2017-12-07 21:09:39.800694: step 44140, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.981 sec/batch; 78h:35m:39s remains)
INFO - root - 2017-12-07 21:09:49.421187: step 44150, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 78h:26m:13s remains)
INFO - root - 2017-12-07 21:09:59.196668: step 44160, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 77h:25m:25s remains)
INFO - root - 2017-12-07 21:10:08.794816: step 44170, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 72h:35m:27s remains)
INFO - root - 2017-12-07 21:10:18.390422: step 44180, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 72h:31m:28s remains)
INFO - root - 2017-12-07 21:10:28.038163: step 44190, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 77h:03m:49s remains)
INFO - root - 2017-12-07 21:10:37.699371: step 44200, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 69h:04m:18s remains)
2017-12-07 21:10:38.784069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3371263 -4.3411713 -4.3464961 -4.3497434 -4.3494997 -4.3463931 -4.3424377 -4.3399839 -4.337184 -4.3323913 -4.3262115 -4.3230548 -4.3254 -4.3310871 -4.3396463][-4.3258138 -4.3304982 -4.33644 -4.3389163 -4.33581 -4.3302321 -4.3247046 -4.3209758 -4.3185539 -4.315587 -4.3105354 -4.3069725 -4.3095174 -4.3162646 -4.3259869][-4.3037429 -4.3113694 -4.3212419 -4.3248329 -4.3168521 -4.3038931 -4.2940574 -4.28711 -4.28538 -4.2869496 -4.2871208 -4.2863722 -4.2894969 -4.2992058 -4.3113828][-4.2613854 -4.2719932 -4.2877197 -4.2921796 -4.277864 -4.2518692 -4.23211 -4.2229571 -4.2265458 -4.2383652 -4.2496552 -4.256659 -4.2636166 -4.27717 -4.2938361][-4.2104568 -4.2215009 -4.2433147 -4.2489057 -4.2249575 -4.1765752 -4.1354671 -4.1206579 -4.1385694 -4.1749487 -4.2080364 -4.2285743 -4.2408614 -4.2557292 -4.2738857][-4.1655765 -4.1752639 -4.2010951 -4.2065163 -4.1704717 -4.091969 -4.0136652 -3.9809694 -4.0166316 -4.09101 -4.1551027 -4.19606 -4.2178288 -4.2340631 -4.2538743][-4.1311221 -4.139215 -4.1676655 -4.174149 -4.1272988 -4.0199256 -3.8964353 -3.8306987 -3.8834622 -3.9998164 -4.10049 -4.1641254 -4.1966004 -4.2145891 -4.2355433][-4.1056342 -4.1145678 -4.1451015 -4.1575708 -4.1101351 -3.9906273 -3.8425674 -3.7486262 -3.8041139 -3.9422026 -4.0644412 -4.14102 -4.1782603 -4.1961541 -4.2183943][-4.0920382 -4.1021733 -4.1337719 -4.1501322 -4.113133 -4.0130887 -3.8886261 -3.8063352 -3.8421671 -3.9572349 -4.066071 -4.1342921 -4.1653018 -4.1825371 -4.2077122][-4.0841746 -4.0935354 -4.1250439 -4.14562 -4.1296043 -4.07074 -3.9929235 -3.9388454 -3.9527321 -4.0208011 -4.0930414 -4.1414557 -4.165288 -4.183342 -4.2104745][-4.0766416 -4.0827365 -4.1136789 -4.1411738 -4.148314 -4.128767 -4.0953245 -4.0701509 -4.0717163 -4.1012292 -4.1360574 -4.1626616 -4.1811895 -4.2007613 -4.2290454][-4.080894 -4.0862226 -4.1197681 -4.1521893 -4.1725659 -4.1782994 -4.1741939 -4.1709895 -4.17117 -4.1813931 -4.1936679 -4.2044864 -4.21654 -4.2348247 -4.261272][-4.1264834 -4.1281667 -4.1588769 -4.1893897 -4.2119222 -4.2239389 -4.232069 -4.2394419 -4.2439413 -4.2489552 -4.2527728 -4.2564931 -4.2639856 -4.278708 -4.2993913][-4.2120733 -4.21121 -4.230607 -4.2507935 -4.2655 -4.2742014 -4.2824488 -4.2914715 -4.2975283 -4.302774 -4.305407 -4.3061843 -4.3086605 -4.317234 -4.3300571][-4.2898836 -4.2886496 -4.2975483 -4.3044577 -4.3095217 -4.3139505 -4.3200054 -4.3254952 -4.3284607 -4.3319359 -4.334013 -4.3347034 -4.3362384 -4.340632 -4.3477373]]...]
INFO - root - 2017-12-07 21:10:48.389545: step 44210, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 75h:59m:41s remains)
INFO - root - 2017-12-07 21:10:58.176748: step 44220, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 80h:16m:17s remains)
INFO - root - 2017-12-07 21:11:07.957076: step 44230, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 79h:22m:37s remains)
INFO - root - 2017-12-07 21:11:17.497390: step 44240, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.958 sec/batch; 76h:43m:17s remains)
INFO - root - 2017-12-07 21:11:27.228438: step 44250, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 80h:59m:06s remains)
INFO - root - 2017-12-07 21:11:36.982580: step 44260, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 77h:46m:47s remains)
INFO - root - 2017-12-07 21:11:46.589017: step 44270, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:20m:14s remains)
INFO - root - 2017-12-07 21:11:56.159066: step 44280, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.975 sec/batch; 78h:02m:59s remains)
INFO - root - 2017-12-07 21:12:05.769020: step 44290, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 76h:59m:15s remains)
INFO - root - 2017-12-07 21:12:15.419954: step 44300, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.969 sec/batch; 77h:32m:18s remains)
2017-12-07 21:12:16.378176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2330709 -4.2256 -4.2309308 -4.2476521 -4.2566066 -4.2569928 -4.2519155 -4.2544756 -4.2572322 -4.2544689 -4.2465997 -4.241385 -4.24137 -4.2455111 -4.2511859][-4.2389727 -4.2292447 -4.2341809 -4.2518654 -4.2622008 -4.2635756 -4.2555327 -4.2525949 -4.2487173 -4.23727 -4.2209358 -4.2155585 -4.2231956 -4.2302332 -4.2347474][-4.2447925 -4.2326021 -4.2340984 -4.2458534 -4.2528834 -4.2577891 -4.2523003 -4.2487783 -4.2397785 -4.2201796 -4.1985035 -4.1969976 -4.2163124 -4.2306132 -4.2342277][-4.2502084 -4.2343965 -4.2244177 -4.2232852 -4.2229605 -4.2275834 -4.2239075 -4.2226706 -4.215857 -4.1960254 -4.1717234 -4.1725779 -4.198648 -4.2189164 -4.2257776][-4.2614818 -4.2433066 -4.2191081 -4.2046571 -4.1940303 -4.1852417 -4.1701522 -4.1658006 -4.1663728 -4.1580253 -4.1423211 -4.1441917 -4.1678391 -4.1915421 -4.2043076][-4.2749443 -4.2565002 -4.2258234 -4.200223 -4.1744213 -4.1328468 -4.0790386 -4.0556269 -4.0772715 -4.1030455 -4.1118784 -4.1204338 -4.1376247 -4.1603088 -4.1769476][-4.265645 -4.2550116 -4.2281752 -4.19327 -4.1421933 -4.0530143 -3.9377327 -3.8890839 -3.9582107 -4.0431952 -4.086688 -4.1075506 -4.1215 -4.1408315 -4.157589][-4.2344322 -4.2312956 -4.2114 -4.1715531 -4.102242 -3.977386 -3.8067784 -3.7357168 -3.8638992 -4.0037441 -4.075098 -4.1072559 -4.1208158 -4.1361742 -4.1519918][-4.20293 -4.2070894 -4.19588 -4.1591768 -4.0901427 -3.978786 -3.83636 -3.778585 -3.8855395 -4.0137882 -4.08283 -4.1142836 -4.1319351 -4.1465111 -4.1617041][-4.1891861 -4.1914663 -4.1821423 -4.1462545 -4.0916514 -4.0271831 -3.9613862 -3.9414306 -4.0021458 -4.0805097 -4.1217823 -4.1350636 -4.1496925 -4.1654558 -4.179513][-4.20372 -4.1963768 -4.1802969 -4.1468239 -4.1100588 -4.0907955 -4.0827503 -4.0908837 -4.1245174 -4.1610518 -4.1738238 -4.1690464 -4.1744037 -4.1922092 -4.2066174][-4.2356472 -4.2197094 -4.1983967 -4.1718154 -4.1528711 -4.1608963 -4.1805763 -4.1996112 -4.2177172 -4.2262206 -4.2203946 -4.2065353 -4.2072592 -4.2246652 -4.2375374][-4.2739983 -4.2553368 -4.2353153 -4.2181487 -4.211123 -4.2260828 -4.2487097 -4.269094 -4.2785797 -4.2732244 -4.2572713 -4.2415485 -4.2398481 -4.2516379 -4.26386][-4.3001533 -4.2807922 -4.2607975 -4.2480326 -4.2473664 -4.26161 -4.2805557 -4.2964988 -4.2997017 -4.2897453 -4.271843 -4.2608676 -4.25916 -4.2660289 -4.2757821][-4.29888 -4.2789555 -4.2618756 -4.2577372 -4.2641816 -4.2750368 -4.2844505 -4.2918835 -4.289072 -4.2795844 -4.2652655 -4.259696 -4.2610631 -4.2697344 -4.2777815]]...]
INFO - root - 2017-12-07 21:12:26.224976: step 44310, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 78h:25m:18s remains)
INFO - root - 2017-12-07 21:12:35.975049: step 44320, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 78h:02m:01s remains)
INFO - root - 2017-12-07 21:12:45.715862: step 44330, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 77h:34m:29s remains)
INFO - root - 2017-12-07 21:12:55.375377: step 44340, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 78h:04m:35s remains)
INFO - root - 2017-12-07 21:13:05.028945: step 44350, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 78h:50m:49s remains)
INFO - root - 2017-12-07 21:13:15.023260: step 44360, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 79h:07m:27s remains)
INFO - root - 2017-12-07 21:13:24.590354: step 44370, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 77h:00m:27s remains)
INFO - root - 2017-12-07 21:13:34.310413: step 44380, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 76h:29m:48s remains)
INFO - root - 2017-12-07 21:13:44.089561: step 44390, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.029 sec/batch; 82h:20m:22s remains)
INFO - root - 2017-12-07 21:13:53.617142: step 44400, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 72h:36m:14s remains)
2017-12-07 21:13:54.576905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546725 -4.2470036 -4.2388711 -4.2276073 -4.2246208 -4.2263188 -4.2235308 -4.218123 -4.2157454 -4.2165112 -4.2225022 -4.2308607 -4.2360377 -4.2391276 -4.2393985][-4.2400184 -4.2302608 -4.2219911 -4.2131114 -4.2147202 -4.221415 -4.2214403 -4.21598 -4.2106037 -4.2071047 -4.209085 -4.2156172 -4.22066 -4.2250633 -4.2291093][-4.2292037 -4.2158766 -4.2051172 -4.1956658 -4.1994729 -4.211318 -4.2176123 -4.2181678 -4.2159271 -4.2114539 -4.2073007 -4.206131 -4.2060518 -4.20901 -4.21599][-4.2263145 -4.2095404 -4.1952052 -4.1823063 -4.1843095 -4.1974916 -4.2086568 -4.2171187 -4.2225566 -4.2217369 -4.2140789 -4.20493 -4.1973529 -4.1960592 -4.2032909][-4.2335372 -4.2144876 -4.1968336 -4.1793804 -4.1763043 -4.1862135 -4.1975746 -4.2096276 -4.2224317 -4.2288356 -4.2235365 -4.2114415 -4.1992884 -4.1926823 -4.1953559][-4.2456808 -4.2277341 -4.2103977 -4.1905642 -4.1809092 -4.1821036 -4.1848521 -4.1912122 -4.2070594 -4.2221084 -4.2249389 -4.2168818 -4.2052231 -4.1953444 -4.1920834][-4.254518 -4.2408924 -4.2268424 -4.206986 -4.1911812 -4.1809306 -4.1698618 -4.1654372 -4.1802793 -4.2022305 -4.2143769 -4.2135787 -4.2056575 -4.1962924 -4.1904759][-4.258666 -4.2492924 -4.238884 -4.2203188 -4.2016325 -4.1842942 -4.16417 -4.1516891 -4.163219 -4.1869874 -4.2030668 -4.2073164 -4.2024484 -4.1945395 -4.189744][-4.2559376 -4.2481074 -4.2405109 -4.225194 -4.2085848 -4.1944327 -4.1788239 -4.1683288 -4.1752882 -4.1924682 -4.2037773 -4.2062383 -4.2007236 -4.1936 -4.191474][-4.2438512 -4.2336335 -4.2278957 -4.2181854 -4.2098541 -4.2069192 -4.2033668 -4.20038 -4.2052631 -4.2153749 -4.2194295 -4.2153497 -4.2049074 -4.1946597 -4.1913548][-4.2318568 -4.21522 -4.2077322 -4.2020683 -4.2022767 -4.2102084 -4.2166276 -4.2211504 -4.2270961 -4.23506 -4.2359214 -4.2268243 -4.21109 -4.1946645 -4.1859317][-4.2278829 -4.2036834 -4.189395 -4.1820908 -4.184123 -4.1966124 -4.2072868 -4.2160249 -4.2240396 -4.2333755 -4.2363534 -4.2288485 -4.2150712 -4.1967683 -4.1829424][-4.2378492 -4.2092319 -4.1877341 -4.1746006 -4.1714463 -4.18006 -4.1877394 -4.196249 -4.2056656 -4.2165093 -4.222116 -4.2199631 -4.2129731 -4.1977472 -4.1829972][-4.25312 -4.2242937 -4.1988764 -4.1800518 -4.1707611 -4.1721134 -4.1740122 -4.17951 -4.1871409 -4.1954679 -4.1997762 -4.2011786 -4.2025471 -4.1966391 -4.1892877][-4.2600231 -4.2327051 -4.2079444 -4.1884856 -4.1778693 -4.1768351 -4.17597 -4.1782703 -4.181448 -4.1841598 -4.1844649 -4.1873789 -4.1959229 -4.20176 -4.2056532]]...]
INFO - root - 2017-12-07 21:14:04.133310: step 44410, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 73h:19m:39s remains)
INFO - root - 2017-12-07 21:14:13.967583: step 44420, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.019 sec/batch; 81h:32m:41s remains)
INFO - root - 2017-12-07 21:14:23.670943: step 44430, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 79h:32m:46s remains)
INFO - root - 2017-12-07 21:14:33.313025: step 44440, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.990 sec/batch; 79h:15m:10s remains)
INFO - root - 2017-12-07 21:14:42.971661: step 44450, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 76h:00m:28s remains)
INFO - root - 2017-12-07 21:14:52.651986: step 44460, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 80h:04m:06s remains)
INFO - root - 2017-12-07 21:15:02.267741: step 44470, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 78h:35m:32s remains)
INFO - root - 2017-12-07 21:15:11.957216: step 44480, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 75h:19m:07s remains)
INFO - root - 2017-12-07 21:15:21.622110: step 44490, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.958 sec/batch; 76h:39m:27s remains)
INFO - root - 2017-12-07 21:15:31.255084: step 44500, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 72h:18m:12s remains)
2017-12-07 21:15:32.275623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3509574 -4.3527479 -4.3527107 -4.3523831 -4.3533535 -4.3544645 -4.3554306 -4.35494 -4.3523655 -4.3478975 -4.34452 -4.3446107 -4.3464451 -4.3478918 -4.349308][-4.3468237 -4.3480263 -4.3485079 -4.3483448 -4.34973 -4.3503556 -4.3479257 -4.3409672 -4.330575 -4.3200169 -4.3155985 -4.3182397 -4.3238754 -4.3278155 -4.3305798][-4.342185 -4.3422532 -4.3411264 -4.3411651 -4.3444257 -4.3443956 -4.3336382 -4.3117013 -4.2856288 -4.2629113 -4.2558718 -4.2638469 -4.2779312 -4.2907887 -4.3017731][-4.3354645 -4.3280621 -4.3191566 -4.3164053 -4.3207784 -4.3191686 -4.2952476 -4.2507725 -4.2042532 -4.1697264 -4.1611204 -4.17566 -4.2011318 -4.22956 -4.2595592][-4.3233638 -4.3002706 -4.2742414 -4.2589612 -4.2545304 -4.2397652 -4.1936526 -4.1228385 -4.0627 -4.03123 -4.03909 -4.0700927 -4.1114712 -4.1578159 -4.2118607][-4.29772 -4.249403 -4.197165 -4.1579275 -4.1306028 -4.0959091 -4.0266762 -3.9302018 -3.8687217 -3.8644907 -3.9091518 -3.9690301 -4.0283108 -4.0903292 -4.1635375][-4.247612 -4.1643982 -4.081347 -4.0154409 -3.9656289 -3.9157751 -3.8316228 -3.7178903 -3.6749253 -3.7259378 -3.8204529 -3.9093838 -3.985302 -4.0589895 -4.14314][-4.1781569 -4.0584135 -3.9490621 -3.8756526 -3.8323212 -3.793829 -3.7181411 -3.6131415 -3.60359 -3.7040248 -3.8279777 -3.9254916 -4.009 -4.0867105 -4.1674471][-4.1267686 -3.9876754 -3.8780651 -3.8314795 -3.8262439 -3.8240881 -3.7901764 -3.7306776 -3.7382252 -3.8299055 -3.9331052 -4.00952 -4.0795312 -4.1476941 -4.2139721][-4.1381903 -4.0144529 -3.9366109 -3.9295125 -3.9595296 -3.985038 -3.9862607 -3.9706125 -3.9807632 -4.0319867 -4.0942554 -4.1404805 -4.1848235 -4.2308378 -4.2747455][-4.199904 -4.1187367 -4.0806785 -4.0942826 -4.1278691 -4.1523762 -4.1629467 -4.16826 -4.1802607 -4.2035527 -4.2329941 -4.2578 -4.2843871 -4.3106894 -4.3325996][-4.2680612 -4.2276635 -4.2156568 -4.2343216 -4.2603636 -4.2780046 -4.2855387 -4.2923222 -4.298111 -4.3056974 -4.316236 -4.3287859 -4.3446794 -4.3581028 -4.3663864][-4.317503 -4.3013339 -4.3004627 -4.3163176 -4.3357878 -4.3492589 -4.3544955 -4.3594227 -4.360733 -4.3609838 -4.3634868 -4.3687224 -4.37639 -4.3797345 -4.3784261][-4.3452163 -4.3408518 -4.3417416 -4.3514848 -4.3636913 -4.3718243 -4.3751197 -4.3773375 -4.376893 -4.3750591 -4.3743143 -4.3747325 -4.3751135 -4.3737273 -4.3709354][-4.3561525 -4.3571496 -4.3583674 -4.3620491 -4.366179 -4.3689246 -4.3707213 -4.3715158 -4.3715186 -4.3703532 -4.368916 -4.3675733 -4.3659329 -4.3641329 -4.3628044]]...]
INFO - root - 2017-12-07 21:15:41.830294: step 44510, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 74h:27m:39s remains)
INFO - root - 2017-12-07 21:15:51.567397: step 44520, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 76h:10m:43s remains)
INFO - root - 2017-12-07 21:16:01.291918: step 44530, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 76h:42m:01s remains)
INFO - root - 2017-12-07 21:16:10.871629: step 44540, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 76h:53m:57s remains)
INFO - root - 2017-12-07 21:16:20.626399: step 44550, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 78h:27m:20s remains)
INFO - root - 2017-12-07 21:16:30.380718: step 44560, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 78h:20m:24s remains)
INFO - root - 2017-12-07 21:16:40.217545: step 44570, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 75h:07m:59s remains)
INFO - root - 2017-12-07 21:16:49.815436: step 44580, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.012 sec/batch; 80h:54m:18s remains)
INFO - root - 2017-12-07 21:16:59.343385: step 44590, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.967 sec/batch; 77h:20m:34s remains)
INFO - root - 2017-12-07 21:17:09.002634: step 44600, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 77h:55m:01s remains)
2017-12-07 21:17:09.964561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2477717 -4.2586851 -4.2717271 -4.2776852 -4.2797503 -4.2794685 -4.2719822 -4.2633362 -4.2549548 -4.2505808 -4.2545395 -4.2571497 -4.2523365 -4.2370777 -4.2125349][-4.2534065 -4.2634726 -4.2812786 -4.2964678 -4.3059359 -4.3105578 -4.3061757 -4.2985382 -4.2881923 -4.280549 -4.2792239 -4.2706 -4.2541776 -4.2322593 -4.2043324][-4.2601585 -4.2687583 -4.2854013 -4.3002844 -4.3103251 -4.3152347 -4.3091903 -4.2968965 -4.2811518 -4.2710314 -4.2691369 -4.2586827 -4.2396812 -4.2163005 -4.1912313][-4.2688007 -4.2715616 -4.2777319 -4.2839684 -4.2907739 -4.294919 -4.2872148 -4.269907 -4.2514939 -4.2427621 -4.2435875 -4.2376041 -4.2240233 -4.2078366 -4.1919656][-4.2620416 -4.2583628 -4.254 -4.2516022 -4.2560682 -4.2597136 -4.2492886 -4.2244444 -4.2010078 -4.1938806 -4.2020035 -4.2088456 -4.2070875 -4.2006359 -4.1972632][-4.249826 -4.2400961 -4.2273464 -4.2164245 -4.2147756 -4.2136478 -4.196876 -4.1630397 -4.1344361 -4.1314831 -4.1504092 -4.1746869 -4.1900563 -4.193574 -4.1983891][-4.2500176 -4.2396183 -4.22411 -4.20553 -4.1928439 -4.1801429 -4.1535063 -4.1141386 -4.0830846 -4.0821872 -4.1124554 -4.1548166 -4.1857514 -4.1951332 -4.2019749][-4.2644792 -4.2573833 -4.2419605 -4.2173953 -4.1974845 -4.1802769 -4.1510482 -4.110343 -4.0755315 -4.0719995 -4.1055775 -4.1550412 -4.1889324 -4.2013659 -4.2112007][-4.2702794 -4.2680655 -4.2570043 -4.2353616 -4.2161312 -4.2025852 -4.177299 -4.1401272 -4.102447 -4.0914526 -4.1167283 -4.1549625 -4.1811466 -4.1973915 -4.2120686][-4.2644272 -4.2712221 -4.2699943 -4.2570162 -4.244143 -4.2337341 -4.2118073 -4.177691 -4.1366439 -4.1146116 -4.1207557 -4.1409922 -4.1603837 -4.1803718 -4.2000279][-4.245708 -4.2618103 -4.2708368 -4.2682576 -4.2647729 -4.2575417 -4.23712 -4.204947 -4.1623354 -4.1334691 -4.1247821 -4.1293163 -4.1417885 -4.1650925 -4.1892138][-4.21469 -4.2364206 -4.2544823 -4.2630396 -4.2697544 -4.2669225 -4.2506037 -4.2241664 -4.1864009 -4.1595688 -4.1462803 -4.1411462 -4.1432419 -4.1598468 -4.1823845][-4.1870575 -4.2093792 -4.2327271 -4.2488022 -4.2597551 -4.2604856 -4.2518311 -4.2379165 -4.2139864 -4.1935811 -4.179821 -4.1683254 -4.1578388 -4.1584835 -4.1722493][-4.1799626 -4.19796 -4.220232 -4.2350869 -4.2430382 -4.2444606 -4.24384 -4.2430549 -4.2321405 -4.2157636 -4.1992788 -4.184443 -4.1666117 -4.1533537 -4.156806][-4.1953278 -4.2058415 -4.2200947 -4.2270041 -4.2303739 -4.2317572 -4.2364368 -4.2435694 -4.2418456 -4.2291679 -4.2100363 -4.189065 -4.164763 -4.1429734 -4.1394691]]...]
INFO - root - 2017-12-07 21:17:19.686667: step 44610, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 80h:15m:45s remains)
INFO - root - 2017-12-07 21:17:29.194498: step 44620, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 77h:42m:01s remains)
INFO - root - 2017-12-07 21:17:39.007869: step 44630, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 74h:59m:48s remains)
INFO - root - 2017-12-07 21:17:48.766493: step 44640, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.980 sec/batch; 78h:21m:24s remains)
INFO - root - 2017-12-07 21:17:58.283604: step 44650, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 78h:45m:21s remains)
INFO - root - 2017-12-07 21:18:07.973687: step 44660, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 76h:39m:02s remains)
INFO - root - 2017-12-07 21:18:17.885262: step 44670, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 75h:58m:07s remains)
INFO - root - 2017-12-07 21:18:27.508523: step 44680, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 80h:31m:59s remains)
INFO - root - 2017-12-07 21:18:37.107133: step 44690, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 74h:04m:30s remains)
INFO - root - 2017-12-07 21:18:46.892748: step 44700, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 77h:45m:01s remains)
2017-12-07 21:18:47.932154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2916374 -4.2944608 -4.2980952 -4.3017268 -4.3042197 -4.30344 -4.298563 -4.2911506 -4.2830939 -4.2795067 -4.2793751 -4.281074 -4.2844267 -4.290309 -4.2996988][-4.2655654 -4.2711396 -4.2770219 -4.2800121 -4.2798676 -4.276114 -4.2635884 -4.2464409 -4.2332797 -4.2311444 -4.236825 -4.2417803 -4.247262 -4.2576985 -4.2727871][-4.2478313 -4.2563572 -4.262928 -4.2626524 -4.2564435 -4.2458172 -4.2211385 -4.1916618 -4.1716685 -4.1751504 -4.1912756 -4.2016091 -4.2100348 -4.2238231 -4.244935][-4.2304039 -4.2401743 -4.2454863 -4.2413011 -4.2278161 -4.2086873 -4.1713867 -4.1330986 -4.1150336 -4.12678 -4.1480546 -4.16213 -4.1741705 -4.1930718 -4.2184982][-4.2014585 -4.2128949 -4.2158532 -4.2083831 -4.1873531 -4.1604867 -4.1168184 -4.0812688 -4.0758991 -4.0953765 -4.1145334 -4.1269317 -4.1430469 -4.16676 -4.1959858][-4.1609244 -4.1770954 -4.1755137 -4.1606922 -4.1330619 -4.1008377 -4.0515313 -4.0164657 -4.02671 -4.0592542 -4.0746961 -4.0807552 -4.1005845 -4.1322331 -4.1680255][-4.1321664 -4.1448612 -4.133215 -4.1109033 -4.0857186 -4.0507216 -3.9887824 -3.9476469 -3.9693947 -4.0106459 -4.0228586 -4.0245328 -4.0495811 -4.0916915 -4.1361389][-4.1256094 -4.1248336 -4.1064134 -4.0838289 -4.0678186 -4.038034 -3.9672611 -3.9157491 -3.9320467 -3.9654417 -3.9688501 -3.9705904 -4.0043864 -4.0568247 -4.1096597][-4.1319532 -4.1225333 -4.1088 -4.1039076 -4.1073127 -4.0918756 -4.0304489 -3.979183 -3.9778235 -3.9862134 -3.9703522 -3.9714866 -4.0088873 -4.0537791 -4.0997505][-4.1350789 -4.1269712 -4.1260309 -4.1433578 -4.1706672 -4.1783957 -4.1419382 -4.1050158 -4.0944276 -4.0787392 -4.0444613 -4.0411015 -4.0714746 -4.0977354 -4.1235747][-4.1349025 -4.1323123 -4.1470089 -4.1801977 -4.2193818 -4.2356262 -4.2184715 -4.2005324 -4.1913829 -4.1664958 -4.1249037 -4.114984 -4.1366735 -4.151979 -4.1630573][-4.1389 -4.1426697 -4.16758 -4.2099528 -4.2501545 -4.2630258 -4.2530818 -4.2441068 -4.2386847 -4.2144413 -4.174181 -4.1592937 -4.1754208 -4.1890936 -4.201623][-4.1621013 -4.1690598 -4.1920958 -4.2316446 -4.2675233 -4.2789059 -4.2726126 -4.2665944 -4.2628913 -4.2441344 -4.2049656 -4.184032 -4.1958489 -4.2121892 -4.2316594][-4.1994658 -4.2080884 -4.2283525 -4.2594171 -4.2879362 -4.2967906 -4.2918792 -4.2879648 -4.2880359 -4.2799587 -4.2488332 -4.2241659 -4.2277007 -4.2415142 -4.262218][-4.2416325 -4.2482095 -4.2661476 -4.2885513 -4.3076906 -4.3125358 -4.3056712 -4.3010139 -4.3033152 -4.3032894 -4.2852139 -4.2678771 -4.2680221 -4.2765312 -4.2909255]]...]
INFO - root - 2017-12-07 21:18:57.567134: step 44710, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 78h:33m:12s remains)
INFO - root - 2017-12-07 21:19:07.128176: step 44720, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 80h:46m:58s remains)
INFO - root - 2017-12-07 21:19:16.845628: step 44730, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.976 sec/batch; 78h:03m:24s remains)
INFO - root - 2017-12-07 21:19:26.631773: step 44740, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 79h:06m:17s remains)
INFO - root - 2017-12-07 21:19:36.315115: step 44750, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 78h:43m:43s remains)
INFO - root - 2017-12-07 21:19:46.119610: step 44760, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 78h:31m:53s remains)
INFO - root - 2017-12-07 21:19:55.831304: step 44770, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 75h:27m:44s remains)
INFO - root - 2017-12-07 21:20:05.503379: step 44780, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 76h:21m:31s remains)
INFO - root - 2017-12-07 21:20:14.985536: step 44790, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.007 sec/batch; 80h:27m:20s remains)
INFO - root - 2017-12-07 21:20:24.598501: step 44800, loss = 2.10, batch loss = 2.04 (7.7 examples/sec; 1.033 sec/batch; 82h:31m:07s remains)
2017-12-07 21:20:25.603813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2177172 -4.2103577 -4.2150578 -4.2195764 -4.2133331 -4.2033324 -4.2006183 -4.1952543 -4.186594 -4.1952147 -4.2219472 -4.2538137 -4.2752681 -4.2863 -4.2934456][-4.2419991 -4.2329097 -4.2367682 -4.2384892 -4.229394 -4.2199922 -4.2242374 -4.2277513 -4.2221947 -4.231751 -4.2525964 -4.272963 -4.2845697 -4.2930188 -4.2978325][-4.2592149 -4.2492852 -4.252645 -4.2535877 -4.2445331 -4.2377963 -4.2436357 -4.2493649 -4.2439184 -4.253222 -4.269455 -4.2831345 -4.2916689 -4.2970834 -4.2987671][-4.2562423 -4.2443724 -4.2482343 -4.2503042 -4.2438974 -4.2379837 -4.2421937 -4.2492442 -4.2458906 -4.2553234 -4.269186 -4.2797017 -4.285315 -4.2888346 -4.2909913][-4.2439389 -4.2281609 -4.2321186 -4.2368565 -4.2341528 -4.2276506 -4.224133 -4.2276845 -4.2280831 -4.243103 -4.2617254 -4.2706466 -4.2717662 -4.2733383 -4.2770996][-4.2264729 -4.2051921 -4.2083611 -4.21502 -4.2150183 -4.2054968 -4.1904821 -4.1830659 -4.1851621 -4.2090192 -4.2364 -4.2519407 -4.2583337 -4.263011 -4.2708211][-4.2036333 -4.1723032 -4.1657596 -4.1698871 -4.1660724 -4.1484985 -4.1153765 -4.0900121 -4.0966539 -4.1406593 -4.1900263 -4.2252707 -4.2464523 -4.2594323 -4.2709689][-4.1848683 -4.1444869 -4.1247787 -4.1194248 -4.1078196 -4.0790162 -4.020906 -3.9663489 -3.968729 -4.0356016 -4.116971 -4.1803961 -4.220437 -4.2460718 -4.2620273][-4.1722207 -4.1363411 -4.1177011 -4.1108012 -4.10814 -4.092412 -4.0378714 -3.9744449 -3.9619665 -4.0152454 -4.092196 -4.1564302 -4.2001824 -4.2330146 -4.2536783][-4.1895828 -4.1717696 -4.1629744 -4.1582046 -4.1686234 -4.1701279 -4.1360993 -4.0920982 -4.07655 -4.1026173 -4.1488419 -4.1879759 -4.2156043 -4.2409873 -4.26004][-4.2274594 -4.2250929 -4.221529 -4.2199306 -4.2351036 -4.2382812 -4.2173204 -4.193018 -4.1820469 -4.1915064 -4.2141495 -4.2342443 -4.2477965 -4.2626009 -4.2768555][-4.2460728 -4.2503562 -4.2513137 -4.2526522 -4.26653 -4.266058 -4.2546883 -4.2449541 -4.2422671 -4.2463984 -4.2559795 -4.2664475 -4.27274 -4.279923 -4.2894626][-4.2347231 -4.2447667 -4.2535291 -4.2578964 -4.2662587 -4.2613893 -4.2543077 -4.2520037 -4.2570357 -4.2647548 -4.2728109 -4.2820988 -4.2867484 -4.2920847 -4.3015761][-4.2178621 -4.231708 -4.2482123 -4.2584491 -4.2679367 -4.2627964 -4.2557116 -4.252142 -4.258707 -4.2672086 -4.2768121 -4.2912254 -4.3011951 -4.308054 -4.3169165][-4.2212887 -4.2369418 -4.2525797 -4.2605176 -4.2684574 -4.2643695 -4.2563815 -4.2503285 -4.2549429 -4.2624578 -4.2749968 -4.2945585 -4.3097353 -4.3200006 -4.3290687]]...]
INFO - root - 2017-12-07 21:20:35.183617: step 44810, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 77h:16m:31s remains)
INFO - root - 2017-12-07 21:20:44.672586: step 44820, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 77h:35m:38s remains)
INFO - root - 2017-12-07 21:20:54.477180: step 44830, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 79h:13m:32s remains)
INFO - root - 2017-12-07 21:21:04.243501: step 44840, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.982 sec/batch; 78h:29m:50s remains)
INFO - root - 2017-12-07 21:21:13.818962: step 44850, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 73h:17m:47s remains)
INFO - root - 2017-12-07 21:21:23.459342: step 44860, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 79h:09m:33s remains)
INFO - root - 2017-12-07 21:21:33.160826: step 44870, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 76h:59m:15s remains)
INFO - root - 2017-12-07 21:21:42.569160: step 44880, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 65h:41m:59s remains)
INFO - root - 2017-12-07 21:21:52.363846: step 44890, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 79h:09m:28s remains)
INFO - root - 2017-12-07 21:22:02.095868: step 44900, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 75h:24m:24s remains)
2017-12-07 21:22:03.130981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3009634 -4.2874837 -4.2739697 -4.2653351 -4.2603207 -4.2508235 -4.2401743 -4.2375226 -4.2419462 -4.2501736 -4.26178 -4.2719669 -4.2773256 -4.2786183 -4.2833118][-4.2758183 -4.2617931 -4.2488031 -4.2441435 -4.2421641 -4.2302403 -4.21548 -4.2116761 -4.2154903 -4.2235909 -4.2366285 -4.2511358 -4.2595549 -4.2601376 -4.2642689][-4.240293 -4.2269168 -4.2130852 -4.2126608 -4.2132983 -4.1994381 -4.1809196 -4.1773357 -4.1845803 -4.1955233 -4.2094159 -4.2269807 -4.2354722 -4.2342663 -4.2381439][-4.2147269 -4.2019234 -4.1834149 -4.1834641 -4.185183 -4.1641483 -4.1381464 -4.1384091 -4.1601253 -4.1836085 -4.2013164 -4.2218904 -4.2308216 -4.2268567 -4.2290545][-4.1961927 -4.1879368 -4.1679831 -4.161787 -4.152885 -4.1052408 -4.0554681 -4.0564737 -4.1018138 -4.1457319 -4.1751881 -4.2065744 -4.2228045 -4.2242737 -4.228168][-4.1695013 -4.170321 -4.1581774 -4.1430807 -4.1060376 -4.0102983 -3.9170647 -3.9256296 -4.0052032 -4.0742955 -4.1256022 -4.1766291 -4.2069192 -4.219811 -4.2263842][-4.1374569 -4.1466961 -4.1437297 -4.1175275 -4.0388331 -3.8768158 -3.7309628 -3.7793322 -3.9111271 -4.0048208 -4.0777555 -4.1481037 -4.1915808 -4.2114015 -4.2202091][-4.1332874 -4.1474733 -4.1440082 -4.1046224 -3.999413 -3.7983804 -3.6235094 -3.7187893 -3.8901396 -3.995003 -4.0690446 -4.1393042 -4.1884236 -4.2065787 -4.212605][-4.1613526 -4.1771097 -4.1725717 -4.1336679 -4.0461392 -3.8906105 -3.7721882 -3.8500319 -3.9861083 -4.06739 -4.1195083 -4.1683693 -4.2093968 -4.2190008 -4.2175531][-4.1990309 -4.2124639 -4.212739 -4.1874261 -4.1362147 -4.0498648 -3.9982648 -4.0376244 -4.1075277 -4.15617 -4.1895013 -4.2181878 -4.2416468 -4.2427273 -4.2330604][-4.2420707 -4.248138 -4.248034 -4.2326245 -4.204535 -4.1591659 -4.1397529 -4.1570706 -4.1889887 -4.2206616 -4.2481375 -4.266839 -4.275178 -4.2700953 -4.2535548][-4.2756109 -4.2735705 -4.2650571 -4.2476082 -4.22407 -4.1956463 -4.1853848 -4.1974659 -4.220952 -4.2523589 -4.2801065 -4.2945142 -4.2979093 -4.2901711 -4.2724977][-4.2947364 -4.2848091 -4.2693176 -4.2466125 -4.220891 -4.1948447 -4.1872492 -4.2011695 -4.2260323 -4.25718 -4.2844911 -4.2985859 -4.3008776 -4.29668 -4.2870507][-4.2895107 -4.2773352 -4.2640486 -4.2432961 -4.2176447 -4.1924367 -4.1855187 -4.1977487 -4.2250891 -4.2529888 -4.2762146 -4.2913389 -4.2969513 -4.2970939 -4.296195][-4.2770381 -4.2688851 -4.2632875 -4.25219 -4.23271 -4.2081032 -4.1962724 -4.2064815 -4.2340727 -4.2569089 -4.2741661 -4.2888055 -4.2965751 -4.3003063 -4.3035679]]...]
INFO - root - 2017-12-07 21:22:12.999179: step 44910, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 76h:11m:08s remains)
INFO - root - 2017-12-07 21:22:22.734680: step 44920, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 74h:37m:19s remains)
INFO - root - 2017-12-07 21:22:32.463980: step 44930, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 79h:15m:11s remains)
INFO - root - 2017-12-07 21:22:42.145366: step 44940, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 75h:08m:39s remains)
INFO - root - 2017-12-07 21:22:51.835027: step 44950, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 73h:15m:48s remains)
INFO - root - 2017-12-07 21:23:01.233838: step 44960, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 73h:03m:59s remains)
INFO - root - 2017-12-07 21:23:10.874338: step 44970, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 80h:14m:03s remains)
INFO - root - 2017-12-07 21:23:20.503617: step 44980, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 78h:20m:29s remains)
INFO - root - 2017-12-07 21:23:30.139978: step 44990, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 74h:17m:29s remains)
INFO - root - 2017-12-07 21:23:39.689836: step 45000, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 75h:52m:38s remains)
2017-12-07 21:23:40.644901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2741394 -4.2574654 -4.2322598 -4.22004 -4.2229233 -4.233088 -4.2401872 -4.2460871 -4.2508245 -4.2593532 -4.2686105 -4.274621 -4.2779803 -4.2829237 -4.2892814][-4.2615266 -4.2432675 -4.2185397 -4.2095428 -4.2160172 -4.2227969 -4.2240014 -4.2257957 -4.2259831 -4.2308812 -4.2392592 -4.2467675 -4.250453 -4.2549968 -4.264401][-4.25261 -4.2303343 -4.2045569 -4.1972766 -4.2056618 -4.2070026 -4.202806 -4.2033868 -4.2065949 -4.2146788 -4.2199097 -4.2268467 -4.2350383 -4.2433739 -4.2543159][-4.2353373 -4.2087755 -4.1863689 -4.1788173 -4.1801643 -4.1765666 -4.174108 -4.1801562 -4.1916471 -4.20777 -4.2128134 -4.2155895 -4.2187719 -4.2282958 -4.2397208][-4.2283525 -4.1977396 -4.1793809 -4.173398 -4.1669669 -4.1552672 -4.1546612 -4.166223 -4.1829515 -4.2069306 -4.2123733 -4.2047267 -4.1967058 -4.2028408 -4.216454][-4.2400265 -4.2091951 -4.1917181 -4.1792083 -4.1604447 -4.1390119 -4.1373773 -4.1526027 -4.1711864 -4.1897745 -4.1897531 -4.1746264 -4.1644669 -4.1753416 -4.1932278][-4.2474093 -4.2146788 -4.1905656 -4.1608486 -4.119628 -4.0904593 -4.0930662 -4.11356 -4.1337409 -4.1454773 -4.1398935 -4.1260281 -4.1213083 -4.1418343 -4.1663961][-4.2379165 -4.2035737 -4.1769862 -4.1396065 -4.08354 -4.0471463 -4.0535774 -4.0782433 -4.1052451 -4.1238933 -4.1228423 -4.1135106 -4.1110668 -4.1326189 -4.1542807][-4.2239227 -4.1941862 -4.178196 -4.1509805 -4.1003056 -4.0653682 -4.069397 -4.0892692 -4.1158514 -4.1389589 -4.1474433 -4.1444821 -4.1419635 -4.1537523 -4.1694622][-4.2360005 -4.2207413 -4.224618 -4.2143803 -4.17752 -4.1414409 -4.1323333 -4.1404915 -4.1571264 -4.1739311 -4.1797385 -4.1782751 -4.1754813 -4.1886053 -4.2084422][-4.2555628 -4.2519875 -4.270946 -4.274394 -4.2491417 -4.2122517 -4.1880054 -4.1853561 -4.1964269 -4.2088428 -4.2108083 -4.2050176 -4.2017584 -4.2197227 -4.2446408][-4.2571025 -4.258112 -4.2840767 -4.2943263 -4.2784195 -4.24952 -4.2250323 -4.2179127 -4.225862 -4.2360034 -4.2355251 -4.2284808 -4.22856 -4.2452655 -4.2650037][-4.2506323 -4.2512059 -4.2747092 -4.2847204 -4.2757473 -4.2610483 -4.2480154 -4.2464232 -4.2507429 -4.2496758 -4.2422023 -4.2352686 -4.2351823 -4.2448535 -4.2588143][-4.2533412 -4.2528315 -4.268321 -4.2742662 -4.2679996 -4.2629423 -4.2582989 -4.2582417 -4.2566981 -4.2481155 -4.23745 -4.2274723 -4.2254839 -4.231144 -4.2421775][-4.2696962 -4.27165 -4.2808652 -4.2790551 -4.2677889 -4.2608352 -4.258266 -4.2584233 -4.2546577 -4.2437987 -4.2309041 -4.2219849 -4.2217674 -4.2268448 -4.2366557]]...]
INFO - root - 2017-12-07 21:23:50.384322: step 45010, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.993 sec/batch; 79h:19m:10s remains)
INFO - root - 2017-12-07 21:24:00.126657: step 45020, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 78h:06m:19s remains)
INFO - root - 2017-12-07 21:24:09.674285: step 45030, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 75h:14m:45s remains)
INFO - root - 2017-12-07 21:24:19.313692: step 45040, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 78h:56m:15s remains)
INFO - root - 2017-12-07 21:24:29.092219: step 45050, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 75h:37m:54s remains)
INFO - root - 2017-12-07 21:24:38.686979: step 45060, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.956 sec/batch; 76h:21m:36s remains)
INFO - root - 2017-12-07 21:24:48.307268: step 45070, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 79h:42m:28s remains)
INFO - root - 2017-12-07 21:24:57.498689: step 45080, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 76h:11m:53s remains)
INFO - root - 2017-12-07 21:25:07.056976: step 45090, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.970 sec/batch; 77h:26m:48s remains)
INFO - root - 2017-12-07 21:25:16.681455: step 45100, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.022 sec/batch; 81h:35m:21s remains)
2017-12-07 21:25:17.704082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1571436 -4.1414523 -4.1488471 -4.159307 -4.1756768 -4.1996112 -4.2011042 -4.1843066 -4.1730618 -4.1511092 -4.1248341 -4.1159658 -4.1432018 -4.1819448 -4.195961][-4.0380163 -4.0224628 -4.0402517 -4.0635948 -4.1018391 -4.1610475 -4.2017827 -4.2138605 -4.2138824 -4.1930432 -4.1540103 -4.1182203 -4.1187158 -4.144516 -4.1565957][-3.9359746 -3.9229264 -3.9516585 -3.987591 -4.0467086 -4.1289754 -4.1894674 -4.2121849 -4.2137384 -4.1874022 -4.1297455 -4.0661569 -4.0413122 -4.0555143 -4.0685892][-3.9412234 -3.9340875 -3.9681449 -4.0089474 -4.0677485 -4.1396604 -4.1947837 -4.2151985 -4.2114134 -4.1805787 -4.1101332 -4.02093 -3.9706547 -3.9668558 -3.9783506][-4.0413928 -4.0356174 -4.0603385 -4.0887046 -4.1228991 -4.1666226 -4.2051015 -4.2197938 -4.2147741 -4.1902175 -4.1281648 -4.0474186 -3.9967637 -3.9821115 -3.9853086][-4.1536336 -4.140203 -4.1435924 -4.1453094 -4.1460619 -4.1642885 -4.1914277 -4.2045512 -4.1997185 -4.1897235 -4.1650252 -4.127358 -4.1045365 -4.0930882 -4.086484][-4.2304196 -4.2030191 -4.1825304 -4.1558347 -4.1270413 -4.1257987 -4.1452532 -4.1554747 -4.1499372 -4.1642218 -4.1851683 -4.18991 -4.1932421 -4.1897211 -4.1827755][-4.2683816 -4.225183 -4.1820364 -4.1251454 -4.062541 -4.0385108 -4.0467243 -4.0554023 -4.0574942 -4.1012096 -4.1610389 -4.19273 -4.209599 -4.2200885 -4.2242908][-4.2798376 -4.2249451 -4.1615119 -4.0768256 -3.985311 -3.9424429 -3.9443128 -3.9577343 -3.9765947 -4.0461955 -4.1300678 -4.1745362 -4.2005963 -4.2228522 -4.2364411][-4.2908106 -4.2354107 -4.1670074 -4.075047 -3.9795651 -3.935106 -3.9391 -3.9607284 -3.9910207 -4.0600066 -4.1361685 -4.1787157 -4.2065034 -4.2337079 -4.2521415][-4.2976723 -4.2526016 -4.2009473 -4.1307907 -4.0595922 -4.028718 -4.0362558 -4.0568566 -4.082129 -4.1307874 -4.185607 -4.2189121 -4.2425966 -4.2648468 -4.277945][-4.3026948 -4.2733722 -4.2453003 -4.2077284 -4.1693149 -4.1544042 -4.1589317 -4.1696196 -4.1829171 -4.2117596 -4.2439289 -4.2647171 -4.2802987 -4.2937441 -4.3005123][-4.3035707 -4.2882829 -4.2785487 -4.2661619 -4.2506018 -4.2450428 -4.2486591 -4.2525039 -4.2580023 -4.2759829 -4.2959938 -4.3068833 -4.3152318 -4.3219543 -4.3223109][-4.3039131 -4.2963257 -4.2973723 -4.3004026 -4.3000803 -4.3018942 -4.30527 -4.307013 -4.3100228 -4.3210487 -4.3320971 -4.3379636 -4.34099 -4.3399439 -4.3322997][-4.3051205 -4.2997484 -4.3056922 -4.3159204 -4.3234425 -4.3294206 -4.3332882 -4.333684 -4.3345795 -4.3403277 -4.3465428 -4.3480811 -4.3472695 -4.3426347 -4.3308611]]...]
INFO - root - 2017-12-07 21:25:27.393460: step 45110, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.967 sec/batch; 77h:12m:59s remains)
INFO - root - 2017-12-07 21:25:36.943401: step 45120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 67h:52m:26s remains)
INFO - root - 2017-12-07 21:25:46.431083: step 45130, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 74h:22m:47s remains)
INFO - root - 2017-12-07 21:25:55.990678: step 45140, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 75h:23m:17s remains)
INFO - root - 2017-12-07 21:26:05.664047: step 45150, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 79h:49m:55s remains)
INFO - root - 2017-12-07 21:26:15.172337: step 45160, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 73h:30m:44s remains)
INFO - root - 2017-12-07 21:26:24.952804: step 45170, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:05m:17s remains)
INFO - root - 2017-12-07 21:26:34.368593: step 45180, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 76h:04m:29s remains)
INFO - root - 2017-12-07 21:26:43.885344: step 45190, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 74h:28m:27s remains)
INFO - root - 2017-12-07 21:26:53.718245: step 45200, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.014 sec/batch; 80h:53m:41s remains)
2017-12-07 21:26:54.682032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1894732 -4.1881251 -4.2133417 -4.2456756 -4.2678261 -4.2750754 -4.274158 -4.2586608 -4.2366004 -4.2159858 -4.200901 -4.1914377 -4.1831884 -4.1770148 -4.1699109][-4.1804438 -4.1901312 -4.2245526 -4.2622747 -4.2805443 -4.2771087 -4.2686539 -4.2529383 -4.2342119 -4.2152591 -4.19726 -4.1803136 -4.1655807 -4.1583052 -4.1586947][-4.1922941 -4.2119646 -4.2466593 -4.2804642 -4.2907872 -4.2782979 -4.2654576 -4.25226 -4.2386856 -4.2239051 -4.2034826 -4.1835566 -4.1703911 -4.1710157 -4.1809864][-4.2124071 -4.2360311 -4.2617984 -4.2821827 -4.2831292 -4.2679396 -4.2585797 -4.2487044 -4.2373648 -4.2231579 -4.2013407 -4.1830478 -4.1775484 -4.191771 -4.2137547][-4.2256713 -4.2473559 -4.2625351 -4.2681408 -4.2571964 -4.2396512 -4.2350626 -4.2308521 -4.2201791 -4.2059765 -4.1878271 -4.1757865 -4.181119 -4.2087288 -4.2424636][-4.240263 -4.2510672 -4.2508836 -4.2388206 -4.2133417 -4.1881809 -4.1860628 -4.1907139 -4.1865997 -4.1793575 -4.1703429 -4.1687679 -4.1843457 -4.2210445 -4.2599974][-4.2476463 -4.2415724 -4.2216868 -4.1914568 -4.1548371 -4.1257486 -4.126718 -4.1395321 -4.1458268 -4.1479793 -4.1488962 -4.1598258 -4.1844945 -4.2233176 -4.2577667][-4.2337651 -4.2133212 -4.18174 -4.1459179 -4.1115627 -4.0882688 -4.0921974 -4.1074686 -4.1193643 -4.1272755 -4.133368 -4.1524992 -4.1845388 -4.2230387 -4.250773][-4.215879 -4.19061 -4.1589246 -4.1266432 -4.1011415 -4.0877161 -4.0956831 -4.1090574 -4.1190538 -4.1286912 -4.1371164 -4.1592741 -4.1964097 -4.232173 -4.2500949][-4.2094741 -4.1843133 -4.156 -4.1304693 -4.1147151 -4.1100368 -4.1178308 -4.1253452 -4.1308074 -4.1408792 -4.1530428 -4.1776071 -4.2163258 -4.2461948 -4.2515612][-4.2111192 -4.1879578 -4.1638489 -4.1449256 -4.1368527 -4.1377964 -4.1429873 -4.1449604 -4.1458397 -4.1568804 -4.1715684 -4.1947923 -4.2305541 -4.2532978 -4.2489252][-4.2095709 -4.189959 -4.1682429 -4.1536827 -4.150744 -4.1512685 -4.15245 -4.1497879 -4.1500278 -4.1634412 -4.1800337 -4.2015519 -4.2298841 -4.2457032 -4.2353458][-4.2095437 -4.1903529 -4.1681519 -4.1531973 -4.1512957 -4.1514297 -4.150773 -4.1493163 -4.1532807 -4.1677203 -4.1815023 -4.1949773 -4.2120161 -4.2203207 -4.2081451][-4.2116032 -4.1897626 -4.1660047 -4.1497221 -4.1480093 -4.1500068 -4.1519871 -4.1557493 -4.1637564 -4.1759286 -4.1839457 -4.1870265 -4.1918015 -4.1917524 -4.1808009][-4.2178822 -4.192739 -4.171021 -4.1578546 -4.1583834 -4.1661739 -4.1747012 -4.1829529 -4.1899481 -4.1961856 -4.1959672 -4.1897173 -4.1849966 -4.180799 -4.1751065]]...]
INFO - root - 2017-12-07 21:27:04.417184: step 45210, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 74h:29m:11s remains)
INFO - root - 2017-12-07 21:27:14.096180: step 45220, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.031 sec/batch; 82h:16m:33s remains)
INFO - root - 2017-12-07 21:27:23.743999: step 45230, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.948 sec/batch; 75h:40m:54s remains)
INFO - root - 2017-12-07 21:27:33.351193: step 45240, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 77h:42m:13s remains)
INFO - root - 2017-12-07 21:27:43.026858: step 45250, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 76h:50m:56s remains)
INFO - root - 2017-12-07 21:27:52.636150: step 45260, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 74h:58m:59s remains)
INFO - root - 2017-12-07 21:28:02.339836: step 45270, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.025 sec/batch; 81h:48m:52s remains)
INFO - root - 2017-12-07 21:28:12.032319: step 45280, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 78h:31m:35s remains)
INFO - root - 2017-12-07 21:28:21.368640: step 45290, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 78h:24m:47s remains)
INFO - root - 2017-12-07 21:28:30.993902: step 45300, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 77h:46m:10s remains)
2017-12-07 21:28:32.010111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2363334 -4.2206006 -4.2043438 -4.1845884 -4.1810923 -4.19565 -4.21171 -4.22549 -4.2334452 -4.2320476 -4.2310386 -4.2362556 -4.2436571 -4.2573624 -4.2766333][-4.2343287 -4.214088 -4.1918521 -4.1682591 -4.167232 -4.186471 -4.20438 -4.2153568 -4.2207651 -4.222712 -4.2273908 -4.2372565 -4.2478971 -4.2633734 -4.2824917][-4.2226038 -4.2052989 -4.1849408 -4.1649404 -4.1668606 -4.1812458 -4.1913004 -4.195138 -4.1976366 -4.2047944 -4.2182856 -4.2333879 -4.2475719 -4.2640715 -4.282836][-4.1946654 -4.1818557 -4.1659112 -4.1527123 -4.1569972 -4.1640239 -4.1630511 -4.1595025 -4.1607094 -4.1738439 -4.1971846 -4.2189717 -4.2369995 -4.2561088 -4.2775207][-4.1712689 -4.159059 -4.1437621 -4.1339 -4.1370468 -4.1365294 -4.1294503 -4.1264114 -4.1305194 -4.1486392 -4.1793747 -4.2065234 -4.2274928 -4.2481971 -4.2718811][-4.1567421 -4.1432853 -4.12752 -4.1171031 -4.1137676 -4.1060219 -4.0975628 -4.0999131 -4.109211 -4.1317453 -4.1678514 -4.1983404 -4.2196894 -4.2403345 -4.265666][-4.1312065 -4.1199937 -4.1069026 -4.0970397 -4.0878782 -4.0751004 -4.0716529 -4.0842724 -4.1005483 -4.1261 -4.1631284 -4.1940475 -4.2142577 -4.2346931 -4.2607059][-4.0930657 -4.09027 -4.08619 -4.0827541 -4.0728378 -4.0593967 -4.0628228 -4.0850592 -4.1086388 -4.1373372 -4.1731606 -4.2010522 -4.2174363 -4.2359576 -4.2615919][-4.0636296 -4.0653996 -4.07271 -4.0807824 -4.0767045 -4.0667381 -4.0765228 -4.1050992 -4.1335707 -4.1637945 -4.1965265 -4.2188897 -4.2307158 -4.24599 -4.2691178][-4.0707545 -4.0692568 -4.0796628 -4.0938687 -4.095767 -4.09347 -4.1106515 -4.1436605 -4.1739726 -4.2022872 -4.2294364 -4.2466192 -4.2543354 -4.264658 -4.2820854][-4.1095247 -4.1018186 -4.1078029 -4.1180377 -4.1183481 -4.1186571 -4.1389375 -4.1720304 -4.2008796 -4.226336 -4.2506547 -4.2661242 -4.273488 -4.2816105 -4.2938685][-4.1399288 -4.1306529 -4.1315517 -4.1338367 -4.1296797 -4.1272264 -4.1426325 -4.1686339 -4.1899891 -4.210434 -4.2351408 -4.2562428 -4.2706552 -4.28274 -4.2956419][-4.1466684 -4.1410456 -4.1383228 -4.1340737 -4.1277919 -4.1245513 -4.1333165 -4.1488552 -4.1593289 -4.17257 -4.1981492 -4.2259464 -4.2495837 -4.2696619 -4.2876821][-4.1500111 -4.147541 -4.1409836 -4.1316271 -4.1246452 -4.1224294 -4.1278467 -4.1374035 -4.1421938 -4.1498632 -4.1717334 -4.1989903 -4.2253222 -4.2506042 -4.274333][-4.1735563 -4.1726084 -4.1652703 -4.1560822 -4.1514688 -4.1508942 -4.1538339 -4.1608586 -4.1650882 -4.1690254 -4.1817894 -4.19794 -4.2147617 -4.23591 -4.2608123]]...]
INFO - root - 2017-12-07 21:28:41.735491: step 45310, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 76h:02m:18s remains)
INFO - root - 2017-12-07 21:28:51.461742: step 45320, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 75h:17m:56s remains)
INFO - root - 2017-12-07 21:29:01.174648: step 45330, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 75h:48m:32s remains)
INFO - root - 2017-12-07 21:29:10.893698: step 45340, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 75h:11m:46s remains)
INFO - root - 2017-12-07 21:29:20.716952: step 45350, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 79h:27m:37s remains)
INFO - root - 2017-12-07 21:29:30.407176: step 45360, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 76h:01m:21s remains)
INFO - root - 2017-12-07 21:29:39.963339: step 45370, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 76h:02m:45s remains)
INFO - root - 2017-12-07 21:29:49.664777: step 45380, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.954 sec/batch; 76h:03m:05s remains)
INFO - root - 2017-12-07 21:29:59.222491: step 45390, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 75h:40m:20s remains)
INFO - root - 2017-12-07 21:30:09.060049: step 45400, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 78h:13m:12s remains)
2017-12-07 21:30:10.016927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3435283 -4.3448658 -4.3427277 -4.3400068 -4.3373885 -4.3367181 -4.3379154 -4.3386846 -4.3385172 -4.338047 -4.3393431 -4.3409843 -4.3391752 -4.3246484 -4.2976894][-4.32875 -4.3305092 -4.3270311 -4.3219905 -4.3172455 -4.317225 -4.3209114 -4.32285 -4.3229804 -4.3228245 -4.3255696 -4.3291774 -4.3293262 -4.3219423 -4.3042583][-4.307857 -4.3086896 -4.3012705 -4.2922411 -4.2863841 -4.2893405 -4.2961874 -4.2991238 -4.300077 -4.3017988 -4.3072772 -4.3139706 -4.3173203 -4.315918 -4.3106136][-4.2831659 -4.2791295 -4.2643557 -4.2494888 -4.2431464 -4.2523475 -4.2657461 -4.2700138 -4.271636 -4.2770786 -4.2867985 -4.2971115 -4.3015656 -4.3010015 -4.3019485][-4.2341905 -4.2195535 -4.1963739 -4.1756763 -4.1693015 -4.1859117 -4.2073455 -4.2136126 -4.2154803 -4.2240729 -4.2383618 -4.2535787 -4.2614813 -4.2618575 -4.2648864][-4.1897016 -4.165596 -4.1353178 -4.1078215 -4.1002808 -4.1214767 -4.1474619 -4.153439 -4.1540337 -4.1646156 -4.1854196 -4.20684 -4.2194161 -4.21968 -4.2201338][-4.1594715 -4.1338429 -4.1005816 -4.0656929 -4.05252 -4.0734448 -4.0982156 -4.1028566 -4.1023989 -4.1136904 -4.1401081 -4.1688352 -4.1882119 -4.1879888 -4.182056][-4.14013 -4.1249676 -4.1029677 -4.0715642 -4.0559416 -4.070652 -4.0893846 -4.0935178 -4.0940361 -4.10384 -4.1306276 -4.1628795 -4.1851273 -4.181356 -4.1650152][-4.1457648 -4.1452465 -4.140882 -4.1214252 -4.1093512 -4.11725 -4.1294112 -4.1333261 -4.1358881 -4.1446104 -4.1668081 -4.19311 -4.2100258 -4.2019906 -4.177454][-4.1800709 -4.1880026 -4.1942387 -4.1860433 -4.1788578 -4.1827135 -4.1894512 -4.1957436 -4.2044196 -4.2134032 -4.2280641 -4.241838 -4.2498059 -4.2396445 -4.211257][-4.2304688 -4.2419672 -4.2510729 -4.2474732 -4.2432466 -4.2432194 -4.2465358 -4.2566833 -4.2693076 -4.2789421 -4.2863655 -4.2869716 -4.2841258 -4.2698064 -4.2400784][-4.2720156 -4.28183 -4.2896266 -4.2883844 -4.2853189 -4.2837582 -4.2866921 -4.2988205 -4.3111935 -4.32012 -4.3227119 -4.3151822 -4.3027897 -4.283711 -4.2546272][-4.2915182 -4.3013854 -4.306798 -4.3057942 -4.3047156 -4.3038106 -4.3062105 -4.3156605 -4.3242269 -4.329021 -4.327477 -4.3177218 -4.3026028 -4.282115 -4.2548509][-4.2963095 -4.3026981 -4.3037233 -4.3022609 -4.302341 -4.302043 -4.3042388 -4.3106766 -4.3161969 -4.316915 -4.3129478 -4.3040462 -4.2923536 -4.2737331 -4.2502403][-4.2998848 -4.3045335 -4.3040023 -4.3042603 -4.3062668 -4.3072176 -4.3086553 -4.311584 -4.314189 -4.3132057 -4.3092828 -4.3040524 -4.2951527 -4.2758079 -4.2530708]]...]
INFO - root - 2017-12-07 21:30:19.601339: step 45410, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 76h:33m:59s remains)
INFO - root - 2017-12-07 21:30:29.160578: step 45420, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 77h:59m:55s remains)
INFO - root - 2017-12-07 21:30:38.869162: step 45430, loss = 2.05, batch loss = 2.00 (7.8 examples/sec; 1.021 sec/batch; 81h:23m:09s remains)
INFO - root - 2017-12-07 21:30:48.633048: step 45440, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 75h:39m:47s remains)
INFO - root - 2017-12-07 21:30:58.284935: step 45450, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.014 sec/batch; 80h:50m:38s remains)
INFO - root - 2017-12-07 21:31:07.826306: step 45460, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 78h:23m:53s remains)
INFO - root - 2017-12-07 21:31:17.472567: step 45470, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 77h:20m:41s remains)
INFO - root - 2017-12-07 21:31:27.069629: step 45480, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 75h:57m:58s remains)
INFO - root - 2017-12-07 21:31:36.695572: step 45490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 76h:47m:42s remains)
INFO - root - 2017-12-07 21:31:46.361810: step 45500, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 79h:04m:11s remains)
2017-12-07 21:31:47.326480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.327795 -4.3222113 -4.3104153 -4.3011131 -4.3002849 -4.3024907 -4.3008704 -4.2964225 -4.2848577 -4.2742281 -4.2688394 -4.2717762 -4.27792 -4.2808986 -4.2752509][-4.3334603 -4.3221498 -4.3034081 -4.2900062 -4.2872634 -4.2940197 -4.3009748 -4.3002095 -4.2860141 -4.2707987 -4.2621841 -4.2612414 -4.2660551 -4.2714911 -4.2694173][-4.324604 -4.3012185 -4.2712121 -4.2471991 -4.2391315 -4.2533011 -4.2762394 -4.2899013 -4.2863374 -4.2774611 -4.2718019 -4.2697153 -4.2734165 -4.2781754 -4.2738404][-4.3028517 -4.2612677 -4.2112627 -4.167419 -4.1484461 -4.1710615 -4.2173324 -4.2584405 -4.2796268 -4.2861404 -4.2879448 -4.2889829 -4.2924409 -4.2948637 -4.2859583][-4.2751193 -4.21309 -4.1386766 -4.0693493 -4.0336509 -4.0600057 -4.131124 -4.2048926 -4.2545705 -4.280221 -4.2938824 -4.3015218 -4.305655 -4.3061318 -4.29373][-4.2564526 -4.1829305 -4.0943832 -4.00344 -3.9436686 -3.9584577 -4.0438085 -4.1432943 -4.2166572 -4.260046 -4.2834778 -4.2961593 -4.3027396 -4.3040771 -4.2933331][-4.2514596 -4.1790104 -4.0899405 -3.9923315 -3.91623 -3.9104877 -3.9898577 -4.0943413 -4.1774249 -4.2329378 -4.263587 -4.2792296 -4.2886472 -4.2936697 -4.289691][-4.2528811 -4.195271 -4.1226044 -4.0377665 -3.9671106 -3.9412639 -3.9895267 -4.0719285 -4.1475725 -4.2033176 -4.2355661 -4.2530651 -4.2655382 -4.2749677 -4.2795959][-4.2664561 -4.2279458 -4.1794281 -4.1180253 -4.0645704 -4.0311446 -4.0453882 -4.0936637 -4.1478481 -4.190064 -4.2137594 -4.2278013 -4.2411413 -4.2547536 -4.2685871][-4.2893696 -4.265069 -4.2378597 -4.1998329 -4.1635675 -4.1344028 -4.1323094 -4.1508288 -4.1765833 -4.1971941 -4.2090039 -4.2175403 -4.2310157 -4.2494326 -4.2704754][-4.3104944 -4.2935014 -4.2784519 -4.2567015 -4.2355242 -4.2161889 -4.209938 -4.2127776 -4.2189388 -4.2200351 -4.2203274 -4.2233915 -4.2360411 -4.2551947 -4.2762094][-4.3245034 -4.3089871 -4.2978029 -4.2865257 -4.2790403 -4.2703695 -4.26537 -4.2609076 -4.2563682 -4.2472844 -4.2401109 -4.2381563 -4.2468696 -4.2614903 -4.2784309][-4.3294649 -4.3113689 -4.2964272 -4.2879272 -4.290792 -4.2946424 -4.2983942 -4.2942224 -4.2845769 -4.2699203 -4.2589006 -4.2532706 -4.2561011 -4.2645216 -4.276783][-4.3234634 -4.3031235 -4.2819972 -4.2709374 -4.2774224 -4.2909136 -4.3047543 -4.3073115 -4.3013635 -4.28826 -4.2757721 -4.2656794 -4.2620511 -4.2651415 -4.2717428][-4.309072 -4.290102 -4.2670579 -4.25421 -4.2599182 -4.2769074 -4.2963161 -4.3051252 -4.3039379 -4.2943845 -4.2827716 -4.2714944 -4.2651672 -4.2659664 -4.269464]]...]
INFO - root - 2017-12-07 21:31:56.990596: step 45510, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 77h:19m:50s remains)
INFO - root - 2017-12-07 21:32:06.600964: step 45520, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 79h:22m:36s remains)
INFO - root - 2017-12-07 21:32:16.325182: step 45530, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 78h:45m:52s remains)
INFO - root - 2017-12-07 21:32:26.024170: step 45540, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 77h:05m:25s remains)
INFO - root - 2017-12-07 21:32:35.656394: step 45550, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 75h:47m:44s remains)
INFO - root - 2017-12-07 21:32:45.334273: step 45560, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 75h:06m:50s remains)
INFO - root - 2017-12-07 21:32:55.094824: step 45570, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.021 sec/batch; 81h:22m:57s remains)
INFO - root - 2017-12-07 21:33:04.716796: step 45580, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 72h:06m:38s remains)
INFO - root - 2017-12-07 21:33:14.435344: step 45590, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 77h:52m:13s remains)
INFO - root - 2017-12-07 21:33:24.203174: step 45600, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.011 sec/batch; 80h:31m:58s remains)
2017-12-07 21:33:25.123980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.303153 -4.3128595 -4.3171749 -4.3207216 -4.3237424 -4.3231974 -4.3212104 -4.3193693 -4.3185487 -4.3202639 -4.3237448 -4.3264346 -4.3277311 -4.3282695 -4.3253622][-4.2756467 -4.2833605 -4.2925906 -4.30274 -4.3115387 -4.3145709 -4.3125129 -4.3091292 -4.3073249 -4.3093705 -4.3149409 -4.31953 -4.3212576 -4.3216195 -4.3162074][-4.230165 -4.2294455 -4.2432995 -4.2644811 -4.2865019 -4.2976375 -4.2985749 -4.2976627 -4.2951913 -4.297152 -4.3041797 -4.3100343 -4.3115311 -4.3094015 -4.3005843][-4.1935949 -4.1757979 -4.1835036 -4.2093811 -4.239943 -4.2601705 -4.268219 -4.27405 -4.2777209 -4.285656 -4.2964482 -4.3044057 -4.3065729 -4.3038692 -4.29506][-4.1856775 -4.1511569 -4.1439233 -4.1567364 -4.1788993 -4.1970558 -4.2085881 -4.2255187 -4.2468548 -4.2685652 -4.28546 -4.2974224 -4.3022051 -4.3031273 -4.3007665][-4.1985497 -4.159801 -4.1350327 -4.1187892 -4.1057429 -4.0933919 -4.0871511 -4.1162539 -4.1692753 -4.2187762 -4.2515121 -4.2712064 -4.2826314 -4.2899265 -4.2988539][-4.2051649 -4.176126 -4.1429791 -4.0922823 -4.02674 -3.9542804 -3.89659 -3.9283161 -4.0254207 -4.1157346 -4.1729484 -4.2045751 -4.2215176 -4.2372346 -4.2580895][-4.2226224 -4.216465 -4.1899252 -4.1225057 -4.0229368 -3.8955498 -3.7642877 -3.7791505 -3.9086497 -4.0289702 -4.0995641 -4.1312747 -4.1435852 -4.16048 -4.1902905][-4.2397237 -4.2579126 -4.2489629 -4.19702 -4.1107359 -3.9946055 -3.8658836 -3.8519678 -3.9419203 -4.0331903 -4.080905 -4.0906534 -4.0856738 -4.0889583 -4.1176529][-4.2232776 -4.2591105 -4.2744513 -4.2565989 -4.2068748 -4.13473 -4.0512605 -4.0213943 -4.0496831 -4.086689 -4.0950274 -4.0795035 -4.0593438 -4.0489268 -4.0649853][-4.1726737 -4.2214327 -4.2620397 -4.2783551 -4.2606277 -4.2231183 -4.1770139 -4.1444793 -4.1355138 -4.1320076 -4.11584 -4.0940552 -4.0732541 -4.0572858 -4.0541677][-4.1132784 -4.164453 -4.2197661 -4.2571511 -4.2619181 -4.253253 -4.2353311 -4.2071123 -4.1782823 -4.1569314 -4.1423 -4.1307106 -4.121099 -4.1107941 -4.0950065][-4.0932488 -4.1293235 -4.1727777 -4.2098322 -4.227108 -4.2409992 -4.2499104 -4.2330804 -4.1991119 -4.1719437 -4.1653218 -4.1651244 -4.1708031 -4.1769485 -4.1669922][-4.1290507 -4.1457748 -4.1627536 -4.1842647 -4.2002292 -4.2285147 -4.2542229 -4.2476225 -4.2141294 -4.187501 -4.1846328 -4.1908784 -4.2069349 -4.2263327 -4.23162][-4.2052922 -4.2093797 -4.2086153 -4.2156849 -4.2217097 -4.2457361 -4.2688279 -4.2656727 -4.2373266 -4.2143 -4.2144341 -4.2224731 -4.239346 -4.2625661 -4.2796068]]...]
INFO - root - 2017-12-07 21:33:34.745527: step 45610, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 76h:06m:20s remains)
INFO - root - 2017-12-07 21:33:44.397983: step 45620, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 79h:45m:45s remains)
INFO - root - 2017-12-07 21:33:53.985304: step 45630, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 79h:35m:07s remains)
INFO - root - 2017-12-07 21:34:03.759652: step 45640, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 76h:00m:24s remains)
INFO - root - 2017-12-07 21:34:13.309211: step 45650, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.960 sec/batch; 76h:30m:17s remains)
INFO - root - 2017-12-07 21:34:22.940833: step 45660, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 76h:12m:31s remains)
INFO - root - 2017-12-07 21:34:32.721333: step 45670, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 77h:54m:18s remains)
INFO - root - 2017-12-07 21:34:42.344464: step 45680, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 76h:12m:10s remains)
INFO - root - 2017-12-07 21:34:51.920029: step 45690, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 77h:25m:04s remains)
INFO - root - 2017-12-07 21:35:01.748394: step 45700, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 80h:38m:59s remains)
2017-12-07 21:35:02.759169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3212619 -4.31719 -4.3127451 -4.3108139 -4.3113942 -4.3150396 -4.3201041 -4.3205471 -4.3153024 -4.3022532 -4.2853837 -4.2648644 -4.2419271 -4.2248564 -4.2221584][-4.3242373 -4.3166957 -4.308156 -4.3030615 -4.3022 -4.3073163 -4.3164325 -4.3238664 -4.3262038 -4.3182631 -4.3003254 -4.2739835 -4.2432528 -4.2177477 -4.2076039][-4.3198795 -4.3074555 -4.2921095 -4.28125 -4.2777004 -4.2817726 -4.2898493 -4.3014688 -4.3124156 -4.3139362 -4.2986269 -4.2688861 -4.2310762 -4.1973457 -4.180902][-4.310883 -4.2932372 -4.2676063 -4.24462 -4.23234 -4.2328315 -4.238832 -4.2528315 -4.2738156 -4.2854414 -4.2758846 -4.2473431 -4.2103868 -4.1775718 -4.1620522][-4.299304 -4.2749324 -4.2361059 -4.1976609 -4.1725373 -4.1631818 -4.159029 -4.17042 -4.2022843 -4.231729 -4.2369132 -4.2166314 -4.1877069 -4.1618862 -4.14843][-4.2963428 -4.2703328 -4.2237659 -4.1716328 -4.1287122 -4.0966721 -4.0677629 -4.0688858 -4.1121721 -4.1652236 -4.1955204 -4.1884089 -4.1692853 -4.1486831 -4.1349735][-4.3012056 -4.2815928 -4.2384462 -4.1815739 -4.1254034 -4.0719986 -4.01166 -3.9898965 -4.0358028 -4.1105871 -4.1693311 -4.1785355 -4.1673021 -4.1460676 -4.1266289][-4.3143735 -4.3027244 -4.2678242 -4.2135348 -4.1506686 -4.0849142 -4.0083637 -3.9704492 -4.0098372 -4.0919867 -4.168467 -4.1900616 -4.1840224 -4.1595964 -4.1304464][-4.3313379 -4.322648 -4.2936244 -4.2444777 -4.1817117 -4.1160283 -4.044837 -4.0087147 -4.0374775 -4.1089106 -4.1832662 -4.2101164 -4.2107315 -4.1877332 -4.1541295][-4.3441162 -4.3346705 -4.3101315 -4.26951 -4.2169266 -4.1603055 -4.1015773 -4.0685635 -4.084187 -4.1397667 -4.2026887 -4.2327833 -4.241384 -4.2261734 -4.1968808][-4.3527517 -4.342989 -4.3219442 -4.2880535 -4.2434578 -4.1980662 -4.153182 -4.1276689 -4.1376677 -4.1809816 -4.234056 -4.2658048 -4.2806749 -4.2729173 -4.2503409][-4.3544779 -4.3462396 -4.3294363 -4.3027916 -4.2687011 -4.2348828 -4.2025452 -4.1840124 -4.1918259 -4.2259789 -4.2688088 -4.2997909 -4.3163576 -4.3121061 -4.2943845][-4.3520217 -4.3462272 -4.3352537 -4.3179765 -4.2935281 -4.2672958 -4.2434559 -4.2306423 -4.2374759 -4.2639985 -4.2977543 -4.3242974 -4.3387251 -4.3354774 -4.3221421][-4.3468685 -4.3454742 -4.3417344 -4.3335695 -4.3168788 -4.29592 -4.2779264 -4.2692232 -4.2751255 -4.2949653 -4.3196349 -4.3389983 -4.3494368 -4.3458514 -4.336041][-4.3384304 -4.3426056 -4.345067 -4.3431892 -4.3324475 -4.3158455 -4.3019018 -4.296411 -4.3020926 -4.3168416 -4.3332458 -4.344481 -4.3487439 -4.3447385 -4.3393159]]...]
INFO - root - 2017-12-07 21:35:12.499344: step 45710, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 73h:21m:54s remains)
INFO - root - 2017-12-07 21:35:22.328669: step 45720, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 75h:01m:03s remains)
INFO - root - 2017-12-07 21:35:31.983070: step 45730, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 77h:26m:47s remains)
INFO - root - 2017-12-07 21:35:41.834948: step 45740, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 79h:13m:38s remains)
INFO - root - 2017-12-07 21:35:51.598967: step 45750, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 79h:12m:47s remains)
INFO - root - 2017-12-07 21:36:00.958089: step 45760, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 76h:02m:45s remains)
INFO - root - 2017-12-07 21:36:10.546184: step 45770, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 74h:44m:20s remains)
INFO - root - 2017-12-07 21:36:20.087728: step 45780, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 75h:22m:38s remains)
INFO - root - 2017-12-07 21:36:29.848782: step 45790, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 80h:35m:12s remains)
INFO - root - 2017-12-07 21:36:39.349844: step 45800, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 78h:45m:49s remains)
2017-12-07 21:36:40.301525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2493539 -4.2310343 -4.1989923 -4.176476 -4.1632442 -4.1622176 -4.1678352 -4.1717119 -4.1784763 -4.19149 -4.2068634 -4.2223315 -4.2311087 -4.2430439 -4.2463984][-4.2264681 -4.2138605 -4.1867385 -4.1597347 -4.1402059 -4.141871 -4.1527719 -4.1607695 -4.16901 -4.1859088 -4.2018514 -4.2182059 -4.2333813 -4.2467232 -4.2488804][-4.2102718 -4.2052932 -4.1865883 -4.1619139 -4.1426735 -4.1443529 -4.1549478 -4.1611195 -4.1665635 -4.1809263 -4.198895 -4.2162213 -4.2320423 -4.2475295 -4.2528129][-4.2161684 -4.2173724 -4.2036872 -4.18249 -4.1660151 -4.1606412 -4.1656218 -4.1698728 -4.1756396 -4.1873984 -4.2000074 -4.2103829 -4.2217078 -4.237051 -4.24619][-4.2239757 -4.2252522 -4.2082062 -4.1845675 -4.1600766 -4.1363583 -4.1302333 -4.136889 -4.1534891 -4.1741214 -4.1878762 -4.1989889 -4.2101731 -4.2209554 -4.2279367][-4.2271252 -4.2269344 -4.2067389 -4.1766853 -4.1339912 -4.08078 -4.054965 -4.0613327 -4.093215 -4.1347051 -4.1641855 -4.1884437 -4.2028995 -4.2062793 -4.2109342][-4.2211142 -4.2197628 -4.1972914 -4.1580558 -4.0929432 -4.0152326 -3.9665692 -3.9696054 -4.0219173 -4.0897856 -4.1397018 -4.1783047 -4.1979876 -4.2012668 -4.20655][-4.1982865 -4.1930065 -4.1642485 -4.1134019 -4.0330348 -3.9415817 -3.8817308 -3.8913469 -3.9736626 -4.0667086 -4.1303983 -4.1776052 -4.2015605 -4.2072062 -4.212709][-4.1742458 -4.1671844 -4.1391726 -4.0922093 -4.0312719 -3.9710312 -3.9326956 -3.9387004 -4.0126123 -4.090323 -4.1371856 -4.1768131 -4.20026 -4.2071528 -4.2129889][-4.1613917 -4.1573467 -4.139019 -4.108264 -4.0828481 -4.0681415 -4.0563183 -4.0571136 -4.1074395 -4.157208 -4.178153 -4.1992936 -4.2118535 -4.2111874 -4.2125664][-4.1630793 -4.1644006 -4.1562953 -4.1417365 -4.1392665 -4.1452374 -4.1428514 -4.1390557 -4.1758556 -4.2105551 -4.2151165 -4.2238746 -4.2299571 -4.2261672 -4.2210107][-4.1760473 -4.1837392 -4.1844363 -4.1741676 -4.1768379 -4.1867013 -4.1847863 -4.1733465 -4.19932 -4.2271891 -4.2258396 -4.2288508 -4.2338181 -4.2285113 -4.2216926][-4.1898737 -4.2017593 -4.2010541 -4.1904731 -4.1915269 -4.2023439 -4.19894 -4.1827655 -4.1959453 -4.2128458 -4.213233 -4.2195182 -4.2296777 -4.2246847 -4.2174525][-4.1982861 -4.2031431 -4.1992507 -4.1896949 -4.1876707 -4.1963773 -4.1947312 -4.1811247 -4.1892319 -4.2004828 -4.1991363 -4.2085104 -4.222075 -4.2212176 -4.21552][-4.1999321 -4.1983228 -4.1915541 -4.1829252 -4.1759825 -4.1771827 -4.174695 -4.1731095 -4.1859264 -4.1955385 -4.198751 -4.2149787 -4.2254972 -4.221344 -4.2159786]]...]
INFO - root - 2017-12-07 21:36:50.110972: step 45810, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 77h:51m:52s remains)
INFO - root - 2017-12-07 21:36:59.962073: step 45820, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.984 sec/batch; 78h:21m:04s remains)
INFO - root - 2017-12-07 21:37:09.609952: step 45830, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 76h:56m:04s remains)
INFO - root - 2017-12-07 21:37:19.352751: step 45840, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 77h:06m:58s remains)
INFO - root - 2017-12-07 21:37:29.124932: step 45850, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 75h:38m:32s remains)
INFO - root - 2017-12-07 21:37:38.693527: step 45860, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.984 sec/batch; 78h:22m:49s remains)
INFO - root - 2017-12-07 21:37:48.505081: step 45870, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 74h:29m:02s remains)
INFO - root - 2017-12-07 21:37:58.228365: step 45880, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 79h:29m:28s remains)
INFO - root - 2017-12-07 21:38:07.898647: step 45890, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 75h:29m:53s remains)
INFO - root - 2017-12-07 21:38:17.628100: step 45900, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 76h:35m:27s remains)
2017-12-07 21:38:18.662692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3470292 -4.3301148 -4.291636 -4.2368064 -4.1770449 -4.1338234 -4.1238508 -4.1423993 -4.167222 -4.1930594 -4.2092156 -4.2092919 -4.1986117 -4.18952 -4.1795168][-4.3494854 -4.3368526 -4.3088732 -4.269207 -4.2217731 -4.18749 -4.1732235 -4.1774006 -4.1918912 -4.2083983 -4.2136364 -4.2055011 -4.191195 -4.179368 -4.1665831][-4.3535104 -4.3438659 -4.3211279 -4.2864146 -4.2421436 -4.2079906 -4.184937 -4.1754303 -4.1842394 -4.1990924 -4.2040305 -4.2029567 -4.1995668 -4.1895905 -4.1715961][-4.3550358 -4.3427706 -4.31594 -4.2785382 -4.2322121 -4.1941605 -4.1591868 -4.1398253 -4.152576 -4.1792235 -4.1970034 -4.2073436 -4.2106137 -4.2006845 -4.1824608][-4.3507214 -4.332489 -4.2968292 -4.2522559 -4.2009382 -4.1551614 -4.1046371 -4.0803862 -4.107626 -4.1567736 -4.194963 -4.2177095 -4.2219329 -4.2092943 -4.1921406][-4.3423953 -4.3169212 -4.26915 -4.2109652 -4.1470151 -4.0833087 -4.0132966 -3.9868307 -4.038671 -4.1178274 -4.1777964 -4.2138915 -4.2194386 -4.205904 -4.1866021][-4.3330941 -4.3010116 -4.2409582 -4.1653943 -4.0867424 -4.0079684 -3.9300263 -3.9131393 -3.9865634 -4.08154 -4.1478238 -4.1848483 -4.1910114 -4.1792731 -4.1605725][-4.3284888 -4.2949657 -4.2296219 -4.1462584 -4.0642524 -3.9897552 -3.9295213 -3.9285798 -3.9999189 -4.081336 -4.1342239 -4.1632357 -4.1640472 -4.1526275 -4.1417584][-4.331872 -4.3021197 -4.2410603 -4.1626105 -4.0872836 -4.0246825 -3.9751785 -3.9659836 -4.016448 -4.0801015 -4.1286306 -4.1619849 -4.1709075 -4.1676626 -4.1666846][-4.3409176 -4.3194284 -4.2705407 -4.2038546 -4.1368465 -4.0771303 -4.0198684 -3.9861095 -4.0051422 -4.055222 -4.1116605 -4.16276 -4.1899524 -4.20164 -4.2101688][-4.3509192 -4.338593 -4.3048744 -4.2540536 -4.1966085 -4.1395206 -4.0801954 -4.0309143 -4.0242543 -4.0575733 -4.1108189 -4.1666985 -4.2044592 -4.2263002 -4.2412605][-4.3572993 -4.3497248 -4.3255043 -4.2848706 -4.2389345 -4.1938229 -4.1479158 -4.1057138 -4.0945554 -4.1141391 -4.152143 -4.1946449 -4.2288232 -4.2517238 -4.2677007][-4.3579245 -4.3487811 -4.323379 -4.2843971 -4.2445345 -4.207633 -4.1750278 -4.14765 -4.1447744 -4.163065 -4.1928229 -4.2229834 -4.2515225 -4.272068 -4.2863116][-4.3555927 -4.3413029 -4.3089962 -4.2650323 -4.2235203 -4.1872382 -4.1612844 -4.1459432 -4.1509938 -4.173389 -4.2011023 -4.22881 -4.2548661 -4.2746983 -4.2880216][-4.3514957 -4.3324575 -4.2922854 -4.2394457 -4.1912632 -4.1517959 -4.131032 -4.1272087 -4.1394076 -4.1628551 -4.1873074 -4.2147245 -4.2395253 -4.260149 -4.2745042]]...]
INFO - root - 2017-12-07 21:38:28.410973: step 45910, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 78h:34m:51s remains)
INFO - root - 2017-12-07 21:38:38.066214: step 45920, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 78h:56m:04s remains)
INFO - root - 2017-12-07 21:38:47.584552: step 45930, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 74h:18m:19s remains)
INFO - root - 2017-12-07 21:38:57.318831: step 45940, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 79h:16m:52s remains)
INFO - root - 2017-12-07 21:39:06.824561: step 45950, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 69h:46m:41s remains)
INFO - root - 2017-12-07 21:39:16.339748: step 45960, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.816 sec/batch; 64h:57m:13s remains)
INFO - root - 2017-12-07 21:39:25.930915: step 45970, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 75h:58m:33s remains)
INFO - root - 2017-12-07 21:39:35.616810: step 45980, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 76h:47m:39s remains)
INFO - root - 2017-12-07 21:39:45.339898: step 45990, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 76h:47m:21s remains)
INFO - root - 2017-12-07 21:39:55.013493: step 46000, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 76h:01m:48s remains)
2017-12-07 21:39:56.028826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0835352 -4.0829306 -4.0716782 -4.0712414 -4.0939612 -4.1120949 -4.13521 -4.1667376 -4.15541 -4.1196547 -4.0896111 -4.0750012 -4.0682068 -4.0526829 -4.0349984][-4.1082635 -4.1162419 -4.1077881 -4.1061373 -4.1199956 -4.1324019 -4.1457691 -4.171989 -4.1716151 -4.1445289 -4.119308 -4.1059151 -4.0992961 -4.0883956 -4.0734262][-4.1147814 -4.1274781 -4.123343 -4.1184168 -4.1217833 -4.1274781 -4.13531 -4.1596856 -4.1763387 -4.1657553 -4.1508031 -4.1403427 -4.1351547 -4.1313281 -4.1281962][-4.09719 -4.112114 -4.1095762 -4.1049418 -4.1050024 -4.1101537 -4.1188765 -4.1465 -4.1773834 -4.1855874 -4.1836681 -4.1827989 -4.18438 -4.1899962 -4.1996465][-4.075901 -4.08612 -4.0772171 -4.0689926 -4.0669856 -4.073956 -4.0865922 -4.120223 -4.1637297 -4.1915011 -4.2059255 -4.2191925 -4.2311926 -4.244349 -4.259963][-4.0737767 -4.079483 -4.0667996 -4.0506949 -4.0401874 -4.045465 -4.0602541 -4.09285 -4.136683 -4.1767259 -4.2089596 -4.2374239 -4.2617736 -4.2806206 -4.2977533][-4.097836 -4.1011825 -4.0855994 -4.0596204 -4.0309772 -4.0224428 -4.0308409 -4.0549145 -4.0945926 -4.1399341 -4.1852031 -4.2281723 -4.2645473 -4.2917914 -4.3103013][-4.1404076 -4.1341343 -4.1097083 -4.0718231 -4.0239005 -3.9922733 -3.9895782 -4.0128341 -4.055027 -4.10583 -4.1583524 -4.2096081 -4.2539787 -4.2866492 -4.3059096][-4.1818871 -4.1632 -4.1261759 -4.0750928 -4.0112667 -3.9596941 -3.9484711 -3.9775782 -4.0294409 -4.0865068 -4.1417885 -4.1944962 -4.2405515 -4.2749929 -4.2942243][-4.2131929 -4.178216 -4.1278687 -4.0687881 -4.0015965 -3.9440753 -3.9298923 -3.9613292 -4.0150247 -4.0734577 -4.1286716 -4.1816268 -4.2300968 -4.2674031 -4.2862177][-4.2375884 -4.187449 -4.1265316 -4.065712 -4.0061059 -3.9578655 -3.9462807 -3.972626 -4.0177174 -4.0736051 -4.1304493 -4.1866512 -4.2382793 -4.2741542 -4.28963][-4.2619624 -4.2054543 -4.1390595 -4.0773945 -4.0231633 -3.9832048 -3.9744477 -3.9988811 -4.0420456 -4.0999589 -4.1616817 -4.2198358 -4.2674551 -4.2965584 -4.3037748][-4.2897172 -4.240375 -4.1787281 -4.1185207 -4.0651579 -4.0265684 -4.0182805 -4.0426593 -4.0857968 -4.1448774 -4.2055964 -4.2596631 -4.2990589 -4.3184495 -4.3156328][-4.3131003 -4.280128 -4.2349544 -4.1880445 -4.1456776 -4.1149359 -4.1070042 -4.125844 -4.1605616 -4.2066727 -4.2540193 -4.2955713 -4.3242693 -4.333477 -4.3211446][-4.326467 -4.3088965 -4.2817726 -4.2508006 -4.2225804 -4.204102 -4.2000928 -4.21136 -4.2332425 -4.263639 -4.2960334 -4.3238425 -4.3407593 -4.3387618 -4.3159108]]...]
INFO - root - 2017-12-07 21:40:05.626384: step 46010, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 75h:47m:14s remains)
INFO - root - 2017-12-07 21:40:15.242676: step 46020, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 80h:48m:36s remains)
INFO - root - 2017-12-07 21:40:25.037277: step 46030, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 80h:32m:54s remains)
INFO - root - 2017-12-07 21:40:34.625392: step 46040, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 74h:49m:45s remains)
INFO - root - 2017-12-07 21:40:44.154411: step 46050, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 78h:24m:23s remains)
INFO - root - 2017-12-07 21:40:53.864107: step 46060, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 78h:34m:09s remains)
INFO - root - 2017-12-07 21:41:03.353996: step 46070, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 75h:24m:45s remains)
INFO - root - 2017-12-07 21:41:12.922382: step 46080, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 73h:01m:56s remains)
INFO - root - 2017-12-07 21:41:22.502782: step 46090, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 74h:38m:28s remains)
INFO - root - 2017-12-07 21:41:32.016193: step 46100, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 78h:31m:06s remains)
2017-12-07 21:41:33.041405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2853217 -4.292275 -4.2948465 -4.295239 -4.2899361 -4.2749925 -4.2727404 -4.2825713 -4.297513 -4.3062873 -4.3104944 -4.3102226 -4.3099675 -4.3110919 -4.3082685][-4.2835307 -4.2915373 -4.2940965 -4.2909994 -4.283731 -4.2661319 -4.2605047 -4.2684813 -4.2871838 -4.3025975 -4.3116784 -4.3157663 -4.3169889 -4.3200521 -4.3199387][-4.2781854 -4.2856131 -4.2842674 -4.2732425 -4.2558713 -4.2306323 -4.2213578 -4.2314496 -4.2571492 -4.2817693 -4.2972555 -4.3059888 -4.3079295 -4.3091435 -4.3106666][-4.2736049 -4.2782083 -4.2702451 -4.2498775 -4.2191725 -4.1748247 -4.1501608 -4.1622033 -4.2022085 -4.2377615 -4.2612462 -4.2766051 -4.283195 -4.2823305 -4.2833948][-4.2788415 -4.2805328 -4.2697949 -4.2458739 -4.2040434 -4.1326814 -4.0765839 -4.0830054 -4.1349978 -4.1803613 -4.2114716 -4.2339778 -4.2484479 -4.2515407 -4.2539773][-4.2892685 -4.2894654 -4.2759714 -4.2524848 -4.2044353 -4.1053095 -4.0041428 -3.9983163 -4.0661292 -4.1219444 -4.1561222 -4.1812449 -4.2048697 -4.2248669 -4.2374358][-4.2885137 -4.287684 -4.2684746 -4.2371407 -4.1811171 -4.0514045 -3.8983037 -3.8862996 -3.9879806 -4.0627589 -4.1031332 -4.1366425 -4.1770959 -4.2150292 -4.2383003][-4.2816768 -4.2719922 -4.2399855 -4.1999106 -4.1390157 -3.9969616 -3.8305259 -3.8271427 -3.957583 -4.0513144 -4.1013556 -4.14781 -4.1976724 -4.2390103 -4.2612777][-4.2709904 -4.2502465 -4.2111578 -4.1718311 -4.11728 -4.0064974 -3.8966522 -3.9129815 -4.0223603 -4.1061978 -4.1559796 -4.2041297 -4.2486658 -4.2804074 -4.29443][-4.2610922 -4.2388325 -4.2074 -4.1799254 -4.1424932 -4.0774174 -4.0353751 -4.0747266 -4.151403 -4.2103481 -4.24869 -4.2833567 -4.3125339 -4.3313079 -4.33755][-4.24803 -4.2248168 -4.2009869 -4.1899295 -4.1718802 -4.1377482 -4.1361537 -4.1932936 -4.2535515 -4.2898321 -4.3140221 -4.3347411 -4.3495617 -4.3587022 -4.3587489][-4.2311668 -4.2056828 -4.1848578 -4.1836905 -4.1764469 -4.1561875 -4.1684737 -4.2289286 -4.2785921 -4.3013029 -4.3135076 -4.3209786 -4.3265872 -4.3299975 -4.327589][-4.2171011 -4.1917815 -4.1745076 -4.1750331 -4.1712193 -4.1553731 -4.1657729 -4.2126365 -4.2481952 -4.2584348 -4.2609167 -4.2616777 -4.2639132 -4.2664146 -4.2663541][-4.2063851 -4.1866741 -4.176476 -4.1774759 -4.1749468 -4.1624 -4.1721196 -4.2043524 -4.2251043 -4.2292094 -4.2292304 -4.229053 -4.2302423 -4.2316389 -4.2323656][-4.2177196 -4.2065678 -4.2050257 -4.208456 -4.2052131 -4.1969309 -4.2044854 -4.2263832 -4.2375646 -4.2376032 -4.2378464 -4.2377958 -4.238544 -4.2391033 -4.2392578]]...]
INFO - root - 2017-12-07 21:41:42.757153: step 46110, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 78h:49m:14s remains)
INFO - root - 2017-12-07 21:41:52.383261: step 46120, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 74h:58m:12s remains)
INFO - root - 2017-12-07 21:42:01.987136: step 46130, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 79h:56m:00s remains)
INFO - root - 2017-12-07 21:42:11.761238: step 46140, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 76h:36m:35s remains)
INFO - root - 2017-12-07 21:42:21.277661: step 46150, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 79h:23m:16s remains)
INFO - root - 2017-12-07 21:42:30.952487: step 46160, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.978 sec/batch; 77h:46m:30s remains)
INFO - root - 2017-12-07 21:42:40.720976: step 46170, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.016 sec/batch; 80h:48m:32s remains)
INFO - root - 2017-12-07 21:42:50.323516: step 46180, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.960 sec/batch; 76h:22m:33s remains)
INFO - root - 2017-12-07 21:43:00.163326: step 46190, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 75h:48m:47s remains)
INFO - root - 2017-12-07 21:43:09.691368: step 46200, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 77h:14m:02s remains)
2017-12-07 21:43:10.691102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.260036 -4.2602525 -4.2515278 -4.2398028 -4.2351308 -4.23053 -4.2426186 -4.2667661 -4.2818475 -4.275465 -4.2581673 -4.2271357 -4.2038054 -4.201314 -4.2042089][-4.257216 -4.2547264 -4.239893 -4.2300363 -4.230485 -4.2269735 -4.2364397 -4.2568145 -4.2654848 -4.2522383 -4.2326202 -4.1993656 -4.1798539 -4.1884227 -4.2013626][-4.2553873 -4.2457 -4.2288079 -4.22 -4.2234373 -4.2202892 -4.2255592 -4.2411842 -4.2406578 -4.2207527 -4.1907482 -4.150105 -4.1314135 -4.1501327 -4.1761789][-4.2556314 -4.2538943 -4.24072 -4.2318463 -4.2317615 -4.2213149 -4.2163568 -4.2218971 -4.2113233 -4.1887164 -4.1562972 -4.1167297 -4.1025324 -4.124135 -4.1540551][-4.2626691 -4.2807784 -4.2749147 -4.2607579 -4.2472267 -4.2204781 -4.20017 -4.188261 -4.1663361 -4.1459303 -4.1185188 -4.09037 -4.0893192 -4.1174946 -4.1470633][-4.2653623 -4.2943006 -4.2904482 -4.2658 -4.2350779 -4.1892223 -4.1441822 -4.1100454 -4.0784979 -4.07609 -4.0733533 -4.0667586 -4.0816517 -4.1178923 -4.1503625][-4.2814503 -4.3060932 -4.2956691 -4.253746 -4.1983676 -4.117537 -4.0294046 -3.9683971 -3.9431179 -3.9866047 -4.02998 -4.05541 -4.08376 -4.1252346 -4.1653514][-4.3203034 -4.3361745 -4.316576 -4.2500916 -4.1577435 -4.0226541 -3.8742251 -3.7840621 -3.7839615 -3.8997378 -4.0063772 -4.0705428 -4.1113596 -4.1510611 -4.1887512][-4.3533597 -4.3597631 -4.3326964 -4.2504344 -4.1356859 -3.9770708 -3.8102899 -3.7141323 -3.7271771 -3.8766689 -4.0163927 -4.1022587 -4.1536989 -4.1914191 -4.22228][-4.3624678 -4.3605418 -4.3305497 -4.2522135 -4.1492438 -4.0144587 -3.8885381 -3.8265805 -3.8474641 -3.9734638 -4.0899625 -4.1657262 -4.209651 -4.2349944 -4.254283][-4.3466792 -4.339592 -4.3130608 -4.2502041 -4.172235 -4.0791278 -4.0069351 -3.9841187 -4.0133123 -4.1049843 -4.1886511 -4.2410994 -4.2661114 -4.272953 -4.2776175][-4.3282108 -4.3212471 -4.3011637 -4.2588644 -4.2136536 -4.1605477 -4.1240883 -4.1179218 -4.1426854 -4.2053623 -4.2645149 -4.2955928 -4.3028831 -4.2937107 -4.2867265][-4.3152676 -4.3125515 -4.3018656 -4.28173 -4.2626705 -4.234149 -4.2134709 -4.2083797 -4.223948 -4.2637529 -4.3045082 -4.3212905 -4.3141475 -4.2923155 -4.2770834][-4.2994928 -4.3032565 -4.3058157 -4.3072677 -4.3088579 -4.2948031 -4.2759595 -4.2641859 -4.2716942 -4.2952008 -4.32055 -4.3260493 -4.307724 -4.2776623 -4.2570329][-4.2799044 -4.2879772 -4.3022909 -4.3189144 -4.3317518 -4.3260727 -4.3105907 -4.2971768 -4.2986064 -4.3092608 -4.3225503 -4.31744 -4.2889328 -4.249331 -4.2236466]]...]
INFO - root - 2017-12-07 21:43:20.371436: step 46210, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.031 sec/batch; 82h:01m:30s remains)
INFO - root - 2017-12-07 21:43:30.218608: step 46220, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 79h:45m:12s remains)
INFO - root - 2017-12-07 21:43:39.689954: step 46230, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.996 sec/batch; 79h:11m:10s remains)
INFO - root - 2017-12-07 21:43:49.275991: step 46240, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 69h:59m:23s remains)
INFO - root - 2017-12-07 21:43:58.802240: step 46250, loss = 2.11, batch loss = 2.06 (8.3 examples/sec; 0.961 sec/batch; 76h:24m:50s remains)
INFO - root - 2017-12-07 21:44:08.694149: step 46260, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.001 sec/batch; 79h:36m:56s remains)
INFO - root - 2017-12-07 21:44:18.144025: step 46270, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 77h:04m:33s remains)
INFO - root - 2017-12-07 21:44:27.677879: step 46280, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 75h:53m:12s remains)
INFO - root - 2017-12-07 21:44:37.568762: step 46290, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 79h:31m:24s remains)
INFO - root - 2017-12-07 21:44:47.023153: step 46300, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 76h:45m:46s remains)
2017-12-07 21:44:47.990738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2273493 -4.2307906 -4.2333956 -4.2265139 -4.216382 -4.2138467 -4.21569 -4.2259612 -4.2390919 -4.2451496 -4.2461038 -4.247848 -4.24875 -4.2462974 -4.2404037][-4.2297344 -4.2284222 -4.2309666 -4.2280455 -4.2240572 -4.2287025 -4.2364297 -4.2457843 -4.2517595 -4.2471433 -4.2397156 -4.2317972 -4.2209744 -4.2097464 -4.2003412][-4.2267718 -4.2271261 -4.2304029 -4.2277141 -4.2265348 -4.2267017 -4.2276411 -4.2316918 -4.2338643 -4.2266188 -4.2127619 -4.1943965 -4.172401 -4.1570668 -4.1490431][-4.2026863 -4.2118216 -4.223032 -4.2237778 -4.2160816 -4.2011704 -4.1853271 -4.1779366 -4.1758156 -4.167305 -4.1476841 -4.1226192 -4.10245 -4.0940642 -4.0949049][-4.1548748 -4.17459 -4.1948438 -4.1936283 -4.1746569 -4.1399565 -4.0993676 -4.0687437 -4.0588274 -4.0568852 -4.0502429 -4.042696 -4.047895 -4.0605793 -4.0743866][-4.1249723 -4.1483021 -4.1634092 -4.148345 -4.1076384 -4.0486355 -3.9731741 -3.9084158 -3.9063087 -3.9444809 -3.9801731 -4.009418 -4.04457 -4.07487 -4.0999732][-4.1363115 -4.148881 -4.1386437 -4.0976543 -4.0380263 -3.9523096 -3.8365769 -3.7383733 -3.7766986 -3.8837805 -3.967623 -4.0282412 -4.0790529 -4.1132388 -4.1408525][-4.1500759 -4.1403069 -4.1069679 -4.0552721 -3.9968376 -3.9150686 -3.8072481 -3.7241592 -3.7920294 -3.9170375 -4.0056829 -4.0708466 -4.1175838 -4.1436381 -4.1669693][-4.1378789 -4.1205792 -4.0931611 -4.0603843 -4.0288877 -3.9835739 -3.9317889 -3.8945553 -3.92766 -3.9946473 -4.0492563 -4.0969286 -4.1352654 -4.1605849 -4.1878047][-4.1106248 -4.1098924 -4.1121063 -4.1097078 -4.1059504 -4.0893426 -4.073041 -4.0546846 -4.0473485 -4.0570717 -4.0743141 -4.1014996 -4.1410885 -4.1795564 -4.2132788][-4.0942597 -4.1194091 -4.1455951 -4.1590104 -4.1651168 -4.1642866 -4.1587944 -4.1393905 -4.1120114 -4.09273 -4.0867023 -4.0987062 -4.1403189 -4.18859 -4.2301769][-4.081624 -4.1113925 -4.1376905 -4.1533027 -4.1645126 -4.1757889 -4.1763067 -4.158299 -4.1338391 -4.1111755 -4.1013641 -4.110301 -4.1514874 -4.1987157 -4.2355752][-4.0771108 -4.0932336 -4.1148171 -4.1297288 -4.1390443 -4.1522512 -4.156877 -4.1487193 -4.1377964 -4.1260052 -4.1244631 -4.1392856 -4.1775837 -4.2131205 -4.2352157][-4.0955048 -4.0963759 -4.111196 -4.1247625 -4.12977 -4.1403213 -4.147861 -4.150166 -4.1517973 -4.15378 -4.1605539 -4.1754332 -4.2012596 -4.2200375 -4.2250443][-4.1219983 -4.1164575 -4.12894 -4.1413078 -4.1467485 -4.1563554 -4.1629715 -4.1670365 -4.1685505 -4.1744418 -4.182548 -4.1876755 -4.1971011 -4.2032895 -4.1971717]]...]
INFO - root - 2017-12-07 21:44:57.643282: step 46310, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 73h:08m:41s remains)
INFO - root - 2017-12-07 21:45:07.179164: step 46320, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 76h:47m:13s remains)
INFO - root - 2017-12-07 21:45:16.985215: step 46330, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 79h:51m:40s remains)
INFO - root - 2017-12-07 21:45:26.618316: step 46340, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 70h:48m:57s remains)
INFO - root - 2017-12-07 21:45:36.147453: step 46350, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 72h:56m:20s remains)
INFO - root - 2017-12-07 21:45:45.825265: step 46360, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 75h:22m:28s remains)
INFO - root - 2017-12-07 21:45:55.668649: step 46370, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 75h:28m:08s remains)
INFO - root - 2017-12-07 21:46:05.348211: step 46380, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 76h:25m:18s remains)
INFO - root - 2017-12-07 21:46:14.956711: step 46390, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.970 sec/batch; 77h:06m:49s remains)
INFO - root - 2017-12-07 21:46:24.643573: step 46400, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 76h:50m:38s remains)
2017-12-07 21:46:25.593077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3439183 -4.3241177 -4.3204284 -4.3224111 -4.3163338 -4.3011184 -4.2820935 -4.2636857 -4.258739 -4.26433 -4.2730889 -4.2824039 -4.294301 -4.3060436 -4.3178344][-4.3459835 -4.3264079 -4.3223219 -4.3182321 -4.3009911 -4.2759676 -4.25599 -4.2396851 -4.2332807 -4.2392917 -4.2478437 -4.2576089 -4.2745314 -4.2911186 -4.3085308][-4.3496137 -4.331749 -4.3272181 -4.31647 -4.2905035 -4.2575397 -4.23362 -4.2191882 -4.2120624 -4.2171049 -4.2262573 -4.23843 -4.259603 -4.2795191 -4.2999439][-4.3528705 -4.3377919 -4.3342257 -4.3205132 -4.2913108 -4.2532158 -4.2252479 -4.2108727 -4.2025132 -4.2030168 -4.2127976 -4.2292075 -4.2506409 -4.2715631 -4.2929263][-4.3541088 -4.3415613 -4.3385119 -4.3258862 -4.2972465 -4.2548394 -4.2195358 -4.201623 -4.1911654 -4.190063 -4.2026854 -4.2257872 -4.2469072 -4.2647409 -4.2857184][-4.3564987 -4.3434076 -4.3389559 -4.3250489 -4.2949448 -4.2473359 -4.2015009 -4.1755323 -4.1671114 -4.1753831 -4.1987467 -4.2273178 -4.2446008 -4.2570353 -4.2766442][-4.360867 -4.3473182 -4.3375111 -4.3191395 -4.2863345 -4.2317605 -4.1703963 -4.1253185 -4.116395 -4.1422291 -4.183249 -4.2211132 -4.2394791 -4.2524605 -4.2686791][-4.368485 -4.3562145 -4.3417139 -4.320416 -4.2859521 -4.2258639 -4.1452065 -4.0709829 -4.0566034 -4.0969334 -4.15344 -4.2066703 -4.2355185 -4.2557573 -4.2705832][-4.3776655 -4.369566 -4.3533597 -4.3321333 -4.3005309 -4.2448554 -4.1593208 -4.065268 -4.0388603 -4.0762911 -4.13254 -4.1888957 -4.2260327 -4.253684 -4.2724843][-4.3811736 -4.3772674 -4.3603582 -4.3409543 -4.3152537 -4.272182 -4.2027831 -4.1147685 -4.0859842 -4.1087117 -4.1435766 -4.1810412 -4.2116446 -4.2419453 -4.2654915][-4.3765783 -4.3736672 -4.3600597 -4.3444176 -4.322237 -4.285213 -4.2269115 -4.157197 -4.1390414 -4.1540771 -4.1714559 -4.1872468 -4.2058678 -4.2337303 -4.2579408][-4.368525 -4.3655114 -4.35791 -4.3486209 -4.3290606 -4.2947578 -4.2453642 -4.1927876 -4.1820774 -4.1915226 -4.1984181 -4.2038603 -4.216248 -4.2410474 -4.2637596][-4.36191 -4.3587222 -4.3568044 -4.3540154 -4.3372111 -4.3061304 -4.2679458 -4.2310119 -4.2223253 -4.2264295 -4.2289548 -4.2336926 -4.2436757 -4.2637358 -4.2834578][-4.3581982 -4.3533149 -4.3522806 -4.3511291 -4.33675 -4.3127332 -4.2873325 -4.2633533 -4.2570038 -4.2586823 -4.2601442 -4.2652059 -4.2719193 -4.2856178 -4.3016896][-4.358696 -4.3506684 -4.3464417 -4.3413072 -4.3273616 -4.3118472 -4.2973919 -4.2845778 -4.2831354 -4.2870808 -4.2886267 -4.29039 -4.2945638 -4.3031249 -4.31512]]...]
INFO - root - 2017-12-07 21:46:35.399809: step 46410, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 76h:23m:54s remains)
INFO - root - 2017-12-07 21:46:45.178188: step 46420, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 77h:13m:45s remains)
INFO - root - 2017-12-07 21:46:54.857266: step 46430, loss = 2.12, batch loss = 2.06 (8.1 examples/sec; 0.993 sec/batch; 78h:53m:37s remains)
INFO - root - 2017-12-07 21:47:04.397024: step 46440, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 73h:34m:54s remains)
INFO - root - 2017-12-07 21:47:14.115187: step 46450, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 77h:48m:15s remains)
INFO - root - 2017-12-07 21:47:23.938068: step 46460, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 76h:44m:34s remains)
INFO - root - 2017-12-07 21:47:33.449450: step 46470, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 78h:36m:28s remains)
INFO - root - 2017-12-07 21:47:43.244399: step 46480, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 80h:02m:21s remains)
INFO - root - 2017-12-07 21:47:52.810390: step 46490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 77h:00m:45s remains)
INFO - root - 2017-12-07 21:48:02.404228: step 46500, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 75h:05m:06s remains)
2017-12-07 21:48:03.435407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3287621 -4.3291674 -4.3263121 -4.3210669 -4.3147211 -4.3028417 -4.289526 -4.279254 -4.2735925 -4.2760434 -4.2855744 -4.2939696 -4.2965093 -4.2965932 -4.299952][-4.3233051 -4.3225145 -4.3185391 -4.3103423 -4.2990546 -4.2779627 -4.2532153 -4.236659 -4.2335348 -4.2416816 -4.2586756 -4.2704124 -4.270752 -4.2673588 -4.2711878][-4.3155403 -4.3093491 -4.2984161 -4.283946 -4.2631903 -4.2287021 -4.1912646 -4.1703978 -4.1745915 -4.1926832 -4.22215 -4.2416244 -4.2427516 -4.2372279 -4.2416096][-4.3066616 -4.2918034 -4.2715111 -4.2467003 -4.2090588 -4.1568217 -4.103272 -4.0784364 -4.097692 -4.138401 -4.191535 -4.2276969 -4.2354631 -4.2304344 -4.2319965][-4.300714 -4.2763147 -4.2434297 -4.2007818 -4.1352849 -4.0546408 -3.9798899 -3.953476 -3.9978774 -4.0736594 -4.1615591 -4.225348 -4.2458606 -4.244462 -4.2411222][-4.2930412 -4.2579155 -4.2080564 -4.1412778 -4.0433216 -3.9357741 -3.8442397 -3.8202095 -3.8940523 -4.0075994 -4.1300907 -4.2193522 -4.2515483 -4.2541165 -4.2497573][-4.2885532 -4.2458196 -4.1817718 -4.0948577 -3.9760096 -3.8530517 -3.7561255 -3.7309928 -3.8159127 -3.9526606 -4.097466 -4.2014952 -4.2437482 -4.2524476 -4.2502251][-4.2914586 -4.2460361 -4.1775923 -4.0850887 -3.9656675 -3.846168 -3.7570846 -3.7252393 -3.796797 -3.9350669 -4.0789523 -4.1822076 -4.2294273 -4.2424293 -4.2439804][-4.3009343 -4.2576237 -4.194006 -4.1108408 -4.009707 -3.9106612 -3.8340826 -3.8052752 -3.8628926 -3.9821 -4.1032362 -4.1920466 -4.235384 -4.2437892 -4.2435489][-4.3072567 -4.2706442 -4.2188768 -4.1548095 -4.0821018 -4.0100355 -3.9495616 -3.9282236 -3.9758124 -4.0694885 -4.1624188 -4.2272482 -4.2553854 -4.2514777 -4.243094][-4.3119116 -4.2833872 -4.2449679 -4.2010889 -4.1556792 -4.110075 -4.0657792 -4.050734 -4.0889854 -4.1562281 -4.2210612 -4.2621255 -4.27497 -4.2621746 -4.245769][-4.3177509 -4.2964034 -4.2678514 -4.2375355 -4.2127571 -4.1897559 -4.163249 -4.1514177 -4.1760378 -4.2181888 -4.2589412 -4.2836714 -4.2884469 -4.2724781 -4.2541828][-4.3252335 -4.3108821 -4.2902184 -4.270411 -4.2590709 -4.2500181 -4.2372651 -4.2277589 -4.2404308 -4.2620835 -4.2836227 -4.2956266 -4.2963228 -4.2830815 -4.2676411][-4.3302608 -4.3212118 -4.3070951 -4.2932663 -4.2877622 -4.2858853 -4.2823544 -4.2795472 -4.287581 -4.29902 -4.3094649 -4.3137445 -4.3131728 -4.303441 -4.2939072][-4.3356 -4.3317776 -4.3239603 -4.3161507 -4.3137922 -4.3151879 -4.3171172 -4.3196917 -4.3254709 -4.3298311 -4.3331676 -4.3331237 -4.3306656 -4.3245749 -4.3204103]]...]
INFO - root - 2017-12-07 21:48:13.193403: step 46510, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 77h:45m:05s remains)
INFO - root - 2017-12-07 21:48:22.774301: step 46520, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 75h:40m:22s remains)
INFO - root - 2017-12-07 21:48:32.445843: step 46530, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 74h:19m:57s remains)
INFO - root - 2017-12-07 21:48:41.960389: step 46540, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 78h:35m:43s remains)
INFO - root - 2017-12-07 21:48:51.617298: step 46550, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.970 sec/batch; 77h:00m:43s remains)
INFO - root - 2017-12-07 21:49:01.203076: step 46560, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 77h:15m:32s remains)
INFO - root - 2017-12-07 21:49:11.043672: step 46570, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 78h:26m:14s remains)
INFO - root - 2017-12-07 21:49:20.677070: step 46580, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 77h:52m:02s remains)
INFO - root - 2017-12-07 21:49:30.329679: step 46590, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 72h:18m:41s remains)
INFO - root - 2017-12-07 21:49:39.972265: step 46600, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 79h:11m:59s remains)
2017-12-07 21:49:40.866840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3048673 -4.2942924 -4.2758822 -4.2486897 -4.2287273 -4.2140126 -4.20003 -4.2070212 -4.2370119 -4.2646027 -4.2802997 -4.2905126 -4.2897305 -4.283854 -4.2724175][-4.3218131 -4.3152223 -4.3009028 -4.2759132 -4.2580066 -4.2485504 -4.2341924 -4.2330008 -4.2554488 -4.28043 -4.2911386 -4.2934613 -4.2886314 -4.2807231 -4.2702341][-4.3353949 -4.3318095 -4.3230934 -4.2987266 -4.2771373 -4.2662086 -4.2529993 -4.2464423 -4.2614164 -4.2836885 -4.29023 -4.2844939 -4.2752442 -4.2679672 -4.2630806][-4.3342061 -4.3335133 -4.3321729 -4.3104143 -4.2852039 -4.2678 -4.2486973 -4.2410913 -4.2558875 -4.2766681 -4.2773705 -4.2643619 -4.2526555 -4.2459307 -4.247807][-4.31184 -4.3063688 -4.3087029 -4.2909851 -4.2634759 -4.2379856 -4.2078166 -4.1968732 -4.2184324 -4.2443666 -4.2472463 -4.2336493 -4.2133555 -4.204905 -4.2118058][-4.2701 -4.2489824 -4.2434039 -4.2255421 -4.1991472 -4.1620188 -4.108458 -4.0848012 -4.1260872 -4.1751671 -4.1926579 -4.184289 -4.150239 -4.1351042 -4.1420846][-4.2288265 -4.1874752 -4.1660585 -4.1422095 -4.1115217 -4.0551577 -3.9548926 -3.8956277 -3.9687757 -4.067977 -4.114254 -4.1142306 -4.0643482 -4.040997 -4.0440907][-4.1961842 -4.1466923 -4.1159058 -4.0831785 -4.0430269 -3.9685349 -3.8180661 -3.7040863 -3.8008561 -3.9563308 -4.0428281 -4.064352 -4.0107541 -3.9751167 -3.9735758][-4.1889267 -4.1460876 -4.1181006 -4.0826411 -4.0412126 -3.9739666 -3.829752 -3.7020719 -3.7671113 -3.9225545 -4.0303707 -4.0766025 -4.03936 -4.0030656 -4.0021667][-4.209815 -4.1804843 -4.1657066 -4.1382475 -4.1090326 -4.0657978 -3.9669878 -3.8729258 -3.8963361 -4.004302 -4.09053 -4.1369991 -4.1247816 -4.1022129 -4.1023521][-4.2470231 -4.2317357 -4.2270918 -4.2110596 -4.1969223 -4.1735883 -4.1080832 -4.045723 -4.0497727 -4.118032 -4.1788239 -4.2150493 -4.2164478 -4.2031465 -4.2006149][-4.2840328 -4.28122 -4.284215 -4.2770648 -4.2720914 -4.2627692 -4.2190084 -4.1735897 -4.1681595 -4.211113 -4.2584529 -4.2910342 -4.2917757 -4.2816858 -4.2801766][-4.31393 -4.3151636 -4.3176031 -4.313633 -4.3124294 -4.3102803 -4.2865047 -4.2568583 -4.2468863 -4.2705789 -4.3062673 -4.33339 -4.33085 -4.3221378 -4.3232036][-4.335413 -4.3350816 -4.3332839 -4.3243947 -4.3213296 -4.3214016 -4.311007 -4.2973313 -4.2912045 -4.3060646 -4.3300376 -4.3481665 -4.3437352 -4.3346596 -4.3338828][-4.3346953 -4.3298736 -4.3220067 -4.3099055 -4.3040609 -4.3047194 -4.3003187 -4.2959256 -4.2959938 -4.3068705 -4.3225689 -4.3334079 -4.3300657 -4.323421 -4.3213348]]...]
INFO - root - 2017-12-07 21:49:50.653452: step 46610, loss = 2.11, batch loss = 2.06 (8.3 examples/sec; 0.968 sec/batch; 76h:53m:38s remains)
INFO - root - 2017-12-07 21:50:00.349928: step 46620, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 78h:35m:02s remains)
INFO - root - 2017-12-07 21:50:09.963905: step 46630, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.930 sec/batch; 73h:52m:49s remains)
INFO - root - 2017-12-07 21:50:19.291497: step 46640, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.978 sec/batch; 77h:41m:27s remains)
INFO - root - 2017-12-07 21:50:28.977735: step 46650, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 77h:40m:47s remains)
INFO - root - 2017-12-07 21:50:38.666986: step 46660, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 73h:44m:43s remains)
INFO - root - 2017-12-07 21:50:48.323510: step 46670, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 75h:31m:05s remains)
INFO - root - 2017-12-07 21:50:57.916085: step 46680, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 77h:08m:15s remains)
INFO - root - 2017-12-07 21:51:07.395255: step 46690, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 72h:38m:24s remains)
INFO - root - 2017-12-07 21:51:16.849487: step 46700, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 75h:45m:30s remains)
2017-12-07 21:51:17.861883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2736654 -4.2468266 -4.204401 -4.1566224 -4.1233473 -4.1276445 -4.1526 -4.1718116 -4.170619 -4.1657925 -4.1788592 -4.1945457 -4.1932874 -4.1760325 -4.1621914][-4.2922573 -4.2707705 -4.2267489 -4.1700125 -4.1217508 -4.1062422 -4.1207848 -4.1425047 -4.15589 -4.1674809 -4.187489 -4.2069597 -4.2137241 -4.2059474 -4.1979613][-4.2828612 -4.2725787 -4.2366638 -4.179894 -4.1242218 -4.0887966 -4.08624 -4.1090226 -4.1438808 -4.1812005 -4.215548 -4.2395558 -4.2516289 -4.2492905 -4.2412028][-4.2692475 -4.2736688 -4.2499471 -4.199748 -4.1385155 -4.0811105 -4.0509953 -4.0654578 -4.12234 -4.1901207 -4.24383 -4.2743921 -4.2907543 -4.2889733 -4.27774][-4.2676468 -4.285419 -4.27578 -4.2361622 -4.1752429 -4.1011429 -4.0380979 -4.0241413 -4.0829964 -4.1726294 -4.2426152 -4.2814393 -4.3035073 -4.304955 -4.2932787][-4.2746286 -4.2959642 -4.2931733 -4.2613683 -4.2036715 -4.1225529 -4.035532 -3.9874995 -4.0333371 -4.1313987 -4.2169003 -4.2676191 -4.2984395 -4.3047218 -4.2952404][-4.2936325 -4.3140349 -4.3070369 -4.2741117 -4.2153878 -4.1329222 -4.0348873 -3.9665151 -3.9991674 -4.1002321 -4.1993752 -4.2610369 -4.2969642 -4.30351 -4.2920337][-4.3034334 -4.3225546 -4.3090205 -4.2713757 -4.2125478 -4.136003 -4.0500221 -3.9947128 -4.0258231 -4.1222634 -4.220592 -4.278389 -4.3058739 -4.3022652 -4.2838821][-4.2970357 -4.312438 -4.2932773 -4.2502618 -4.190012 -4.1244173 -4.0673771 -4.049263 -4.0917926 -4.1788144 -4.2624145 -4.305336 -4.3133254 -4.2949052 -4.2685466][-4.2877 -4.2939997 -4.272368 -4.2261362 -4.1602993 -4.1004434 -4.069459 -4.0871716 -4.1444807 -4.2230453 -4.2890787 -4.3165693 -4.3077531 -4.2751274 -4.2391672][-4.2832518 -4.2825279 -4.2570958 -4.2083392 -4.1365104 -4.0741429 -4.0565372 -4.0996509 -4.172698 -4.2473803 -4.3021212 -4.3186297 -4.2991753 -4.2548833 -4.2087231][-4.2764835 -4.2755013 -4.2528434 -4.2074 -4.1362295 -4.0744481 -4.062242 -4.1184845 -4.2006407 -4.2711563 -4.3189669 -4.3296051 -4.3063936 -4.2568469 -4.2055731][-4.2623692 -4.2687154 -4.2591796 -4.2260442 -4.1646028 -4.1094265 -4.103569 -4.1652217 -4.2429352 -4.3043809 -4.3451352 -4.3531108 -4.3294463 -4.2817116 -4.2322249][-4.2423887 -4.2604432 -4.2686653 -4.250186 -4.2032881 -4.1604753 -4.160635 -4.2165155 -4.2808623 -4.3297696 -4.3588777 -4.3635821 -4.34317 -4.306004 -4.2698684][-4.214673 -4.2447448 -4.2646542 -4.2549663 -4.2182426 -4.1845751 -4.1931319 -4.241684 -4.2927961 -4.3277979 -4.3441448 -4.3398557 -4.3210669 -4.2962604 -4.2798572]]...]
INFO - root - 2017-12-07 21:51:27.557975: step 46710, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 77h:54m:38s remains)
INFO - root - 2017-12-07 21:51:37.283780: step 46720, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 75h:54m:04s remains)
INFO - root - 2017-12-07 21:51:46.761734: step 46730, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 68h:32m:24s remains)
INFO - root - 2017-12-07 21:51:56.607472: step 46740, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.036 sec/batch; 82h:15m:21s remains)
INFO - root - 2017-12-07 21:52:06.356045: step 46750, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.019 sec/batch; 80h:52m:28s remains)
INFO - root - 2017-12-07 21:52:16.034635: step 46760, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 76h:04m:52s remains)
INFO - root - 2017-12-07 21:52:25.838716: step 46770, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 79h:21m:06s remains)
INFO - root - 2017-12-07 21:52:35.568794: step 46780, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 76h:38m:59s remains)
INFO - root - 2017-12-07 21:52:45.338462: step 46790, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 76h:52m:53s remains)
INFO - root - 2017-12-07 21:52:54.905418: step 46800, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 76h:12m:24s remains)
2017-12-07 21:52:55.789451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2752719 -4.2809954 -4.2927461 -4.2988262 -4.2963066 -4.2925854 -4.2813635 -4.2625723 -4.2537651 -4.2517357 -4.2510762 -4.2560687 -4.255012 -4.2530675 -4.2519841][-4.2716179 -4.2800441 -4.2904878 -4.2906094 -4.2808957 -4.2711573 -4.25885 -4.2393546 -4.2302465 -4.2271333 -4.2268634 -4.2356968 -4.2365155 -4.2352643 -4.2349706][-4.2717977 -4.2812562 -4.2876668 -4.2775474 -4.255609 -4.2371488 -4.2243037 -4.2048283 -4.1968508 -4.1963882 -4.1971045 -4.2086229 -4.2135 -4.2134132 -4.2121782][-4.2651849 -4.2679577 -4.2637973 -4.2428322 -4.2104497 -4.1846156 -4.1707983 -4.149334 -4.1481733 -4.1601949 -4.1640553 -4.1698747 -4.1696067 -4.1693482 -4.166728][-4.2545743 -4.2459784 -4.2286668 -4.2032423 -4.168345 -4.1344771 -4.1122952 -4.0858769 -4.1024194 -4.1387105 -4.1548281 -4.1514335 -4.1341228 -4.129786 -4.1280456][-4.2401433 -4.2178092 -4.1882486 -4.1605544 -4.1199493 -4.0697193 -4.017705 -3.968724 -4.0135603 -4.0878754 -4.125422 -4.1232114 -4.0963168 -4.0842938 -4.0846667][-4.2194366 -4.1835251 -4.145124 -4.1162462 -4.0722365 -4.0059481 -3.9127524 -3.820544 -3.8915725 -4.0038404 -4.0616894 -4.0639119 -4.0444326 -4.0361395 -4.0447626][-4.2161236 -4.185606 -4.1565938 -4.1439385 -4.1161156 -4.0600863 -3.9649377 -3.8636684 -3.9106731 -4.0026455 -4.0544014 -4.0639086 -4.0616126 -4.06122 -4.0747747][-4.2263832 -4.2042861 -4.1827145 -4.1785107 -4.1663227 -4.1334248 -4.0739222 -4.0175729 -4.0381222 -4.0779457 -4.0949459 -4.0970426 -4.1077318 -4.1245985 -4.1433597][-4.2423925 -4.2312889 -4.215939 -4.2098942 -4.2000623 -4.1821661 -4.1495867 -4.1263142 -4.1357942 -4.1407533 -4.1321445 -4.1257095 -4.1413426 -4.1738367 -4.194191][-4.2680578 -4.2706032 -4.2651348 -4.2604475 -4.2538891 -4.2436814 -4.22418 -4.2109742 -4.2137842 -4.2026033 -4.1838932 -4.1751156 -4.1875424 -4.2202458 -4.2365532][-4.2886748 -4.2987347 -4.3009686 -4.3001823 -4.2971082 -4.2890882 -4.2719975 -4.2627635 -4.2665677 -4.2562652 -4.244451 -4.2390637 -4.2455492 -4.269392 -4.2790523][-4.2879524 -4.2977519 -4.3005376 -4.3040805 -4.30612 -4.3000751 -4.2868404 -4.2795033 -4.2817292 -4.2748256 -4.2702408 -4.2715263 -4.2795444 -4.2968779 -4.2989807][-4.2870083 -4.2950077 -4.2981739 -4.3046083 -4.3089528 -4.3044143 -4.2939711 -4.2887635 -4.2875342 -4.2812972 -4.2788835 -4.2845902 -4.2915034 -4.3019958 -4.2958937][-4.2875366 -4.2928152 -4.2958765 -4.2990618 -4.300437 -4.2953291 -4.2882342 -4.2871509 -4.2884164 -4.2833047 -4.2805309 -4.2844205 -4.2885294 -4.2946 -4.2886982]]...]
INFO - root - 2017-12-07 21:53:05.473796: step 46810, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.959 sec/batch; 76h:06m:48s remains)
INFO - root - 2017-12-07 21:53:15.188092: step 46820, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 77h:09m:06s remains)
INFO - root - 2017-12-07 21:53:24.774614: step 46830, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 75h:26m:49s remains)
INFO - root - 2017-12-07 21:53:34.533830: step 46840, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 79h:56m:57s remains)
INFO - root - 2017-12-07 21:53:44.225436: step 46850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 72h:18m:31s remains)
INFO - root - 2017-12-07 21:53:53.828941: step 46860, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 76h:26m:22s remains)
INFO - root - 2017-12-07 21:54:03.447056: step 46870, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 79h:25m:58s remains)
INFO - root - 2017-12-07 21:54:13.285230: step 46880, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 76h:04m:29s remains)
INFO - root - 2017-12-07 21:54:23.021794: step 46890, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 80h:35m:35s remains)
INFO - root - 2017-12-07 21:54:32.668501: step 46900, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 77h:30m:05s remains)
2017-12-07 21:54:33.607132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3266277 -4.30344 -4.2812567 -4.271204 -4.2654634 -4.2591133 -4.2700872 -4.2950921 -4.3070517 -4.306294 -4.306509 -4.2956867 -4.2811065 -4.2764912 -4.2774835][-4.31315 -4.2870231 -4.2627549 -4.2532129 -4.2452078 -4.23457 -4.2472639 -4.2780962 -4.2897272 -4.285881 -4.2864165 -4.2783155 -4.2628741 -4.2589335 -4.2620492][-4.2839136 -4.2543707 -4.229176 -4.2224922 -4.2193985 -4.2125473 -4.2270885 -4.2555671 -4.2619138 -4.2554941 -4.25856 -4.2525043 -4.2341576 -4.2307363 -4.2377429][-4.2418866 -4.2129831 -4.191483 -4.1892567 -4.1927338 -4.1908484 -4.2044334 -4.2309866 -4.2358556 -4.2292852 -4.2323446 -4.2258635 -4.2062292 -4.2025237 -4.2079506][-4.1965151 -4.1752849 -4.1645021 -4.1645131 -4.1679363 -4.163095 -4.1725779 -4.20016 -4.2077241 -4.2044783 -4.2072272 -4.1962767 -4.1729469 -4.1702962 -4.1795607][-4.1572838 -4.1455622 -4.1417551 -4.1436067 -4.1429539 -4.1324282 -4.1375804 -4.1636419 -4.1743641 -4.1748967 -4.1766725 -4.1607342 -4.1334395 -4.12934 -4.1415815][-4.1405272 -4.1352973 -4.1374693 -4.1363831 -4.1294384 -4.1167288 -4.1182 -4.1388369 -4.149652 -4.1514196 -4.149158 -4.1272845 -4.0982704 -4.0897341 -4.0922422][-4.1178794 -4.1208949 -4.1380453 -4.1441679 -4.13721 -4.1218281 -4.1174827 -4.12792 -4.1352019 -4.1361685 -4.1285462 -4.1049232 -4.0786314 -4.0667486 -4.0587893][-4.1057563 -4.1130486 -4.1414189 -4.1570826 -4.154985 -4.1405897 -4.1346235 -4.1389718 -4.1405973 -4.1366887 -4.12632 -4.1051288 -4.0829391 -4.0661006 -4.0503507][-4.1219373 -4.1219149 -4.14851 -4.1711183 -4.1794982 -4.1754589 -4.1723752 -4.1713915 -4.1654143 -4.1604095 -4.1532283 -4.1377239 -4.1183596 -4.0957832 -4.0748358][-4.1655831 -4.1548219 -4.1676192 -4.1833243 -4.1895819 -4.1912317 -4.189652 -4.1839943 -4.1733146 -4.1706705 -4.1722689 -4.1670437 -4.1567111 -4.137466 -4.1159072][-4.2095056 -4.1984758 -4.2014689 -4.204402 -4.2014685 -4.196866 -4.187511 -4.1714392 -4.1576595 -4.1574059 -4.1638608 -4.1694746 -4.1707892 -4.16274 -4.1464343][-4.236053 -4.2390127 -4.2459283 -4.2396765 -4.2291284 -4.215241 -4.1926303 -4.1671848 -4.15057 -4.150465 -4.1599374 -4.1719985 -4.1819358 -4.1813583 -4.1676607][-4.2384729 -4.257329 -4.2739258 -4.2668281 -4.2499704 -4.2298169 -4.1975441 -4.1683035 -4.1551895 -4.1581149 -4.1728892 -4.1905775 -4.2037807 -4.2017083 -4.1821389][-4.2323465 -4.2575588 -4.2799459 -4.2723684 -4.2579727 -4.2398567 -4.2053022 -4.1749725 -4.1651773 -4.1701736 -4.1867642 -4.208807 -4.2241507 -4.2231731 -4.2002826]]...]
INFO - root - 2017-12-07 21:54:43.344647: step 46910, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.034 sec/batch; 81h:59m:48s remains)
INFO - root - 2017-12-07 21:54:53.124818: step 46920, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 80h:38m:32s remains)
INFO - root - 2017-12-07 21:55:02.635735: step 46930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 76h:56m:13s remains)
INFO - root - 2017-12-07 21:55:12.263351: step 46940, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 72h:36m:25s remains)
INFO - root - 2017-12-07 21:55:21.889499: step 46950, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 76h:32m:50s remains)
INFO - root - 2017-12-07 21:55:31.615554: step 46960, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.023 sec/batch; 81h:06m:38s remains)
INFO - root - 2017-12-07 21:55:41.230847: step 46970, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 75h:58m:43s remains)
INFO - root - 2017-12-07 21:55:50.842568: step 46980, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 73h:52m:13s remains)
INFO - root - 2017-12-07 21:56:00.458307: step 46990, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 76h:32m:06s remains)
INFO - root - 2017-12-07 21:56:10.066274: step 47000, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 79h:15m:54s remains)
2017-12-07 21:56:11.047038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2136407 -4.2247295 -4.2284989 -4.2243509 -4.2150226 -4.2109451 -4.2184997 -4.2333622 -4.2454605 -4.2547245 -4.2657957 -4.2656426 -4.2485304 -4.242146 -4.2460074][-4.222188 -4.2277226 -4.2274837 -4.2221913 -4.2153349 -4.2178936 -4.2312312 -4.2498031 -4.262589 -4.2661519 -4.269536 -4.2690439 -4.2550144 -4.2468448 -4.24845][-4.2404504 -4.2381248 -4.2308292 -4.2230868 -4.2191329 -4.2244468 -4.2390943 -4.2570577 -4.2684011 -4.2682571 -4.2645936 -4.2618384 -4.251687 -4.2456665 -4.2484965][-4.2535672 -4.2449541 -4.2323818 -4.2214084 -4.219079 -4.2209687 -4.23006 -4.242106 -4.2510867 -4.2483397 -4.2410297 -4.2372904 -4.2287803 -4.2226691 -4.225945][-4.2545218 -4.2374711 -4.2180219 -4.2026391 -4.1945662 -4.1859436 -4.1817689 -4.1882982 -4.1958761 -4.1958737 -4.1937056 -4.1996603 -4.1985216 -4.1939549 -4.1973233][-4.2309346 -4.2067213 -4.1822772 -4.1627975 -4.1442628 -4.1164021 -4.0896316 -4.0874672 -4.0997138 -4.110508 -4.1249809 -4.1540251 -4.170136 -4.1754527 -4.1828728][-4.1825857 -4.1555333 -4.131988 -4.1108813 -4.0836668 -4.032795 -3.9781036 -3.9691288 -4.0049839 -4.0456381 -4.0827565 -4.1268005 -4.1566739 -4.1726766 -4.1870914][-4.1180234 -4.0951128 -4.0820637 -4.0667162 -4.0421038 -3.9883082 -3.9240751 -3.9222622 -3.9878592 -4.051414 -4.0953379 -4.135263 -4.1661754 -4.1859417 -4.1970768][-4.0638719 -4.053885 -4.05617 -4.05492 -4.0459094 -4.0118022 -3.9713755 -3.9844918 -4.0539937 -4.1110578 -4.14225 -4.1678638 -4.18847 -4.1983504 -4.1971288][-4.0587759 -4.0657654 -4.0787964 -4.0863781 -4.0900021 -4.07798 -4.0658832 -4.0875645 -4.1431265 -4.1830616 -4.196485 -4.20634 -4.2144489 -4.2134581 -4.2041235][-4.0851865 -4.1015468 -4.1196008 -4.1297722 -4.1377721 -4.1399627 -4.14193 -4.1629896 -4.200726 -4.2244482 -4.2277007 -4.2257414 -4.2260838 -4.2221246 -4.2161746][-4.1060066 -4.1247292 -4.1487608 -4.1633921 -4.1751857 -4.1848793 -4.1892929 -4.2009869 -4.2210207 -4.2339649 -4.2349772 -4.2287254 -4.2239046 -4.2186809 -4.2185173][-4.1039181 -4.1302052 -4.1622324 -4.1830282 -4.1971335 -4.2074523 -4.2099471 -4.2136388 -4.2213979 -4.2306633 -4.2387676 -4.2372155 -4.2280545 -4.2186785 -4.2219296][-4.1054511 -4.1413884 -4.1783972 -4.2016916 -4.2164249 -4.2251453 -4.2249284 -4.2265 -4.2310805 -4.2392907 -4.2469416 -4.2461562 -4.235507 -4.2268686 -4.2329826][-4.1303606 -4.1662621 -4.1999831 -4.2195754 -4.2295728 -4.2364597 -4.2360907 -4.2390542 -4.2446051 -4.2503061 -4.2481146 -4.2411175 -4.2328939 -4.2317352 -4.2410789]]...]
INFO - root - 2017-12-07 21:56:20.683314: step 47010, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 74h:12m:30s remains)
INFO - root - 2017-12-07 21:56:30.328610: step 47020, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 76h:24m:12s remains)
INFO - root - 2017-12-07 21:56:40.070461: step 47030, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 77h:02m:59s remains)
INFO - root - 2017-12-07 21:56:49.715164: step 47040, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 77h:48m:10s remains)
INFO - root - 2017-12-07 21:56:59.219851: step 47050, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.935 sec/batch; 74h:10m:10s remains)
INFO - root - 2017-12-07 21:57:08.760916: step 47060, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.982 sec/batch; 77h:53m:39s remains)
INFO - root - 2017-12-07 21:57:18.443837: step 47070, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 76h:59m:03s remains)
INFO - root - 2017-12-07 21:57:28.269347: step 47080, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.981 sec/batch; 77h:46m:57s remains)
INFO - root - 2017-12-07 21:57:37.788143: step 47090, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.931 sec/batch; 73h:47m:12s remains)
INFO - root - 2017-12-07 21:57:47.472117: step 47100, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 75h:49m:43s remains)
2017-12-07 21:57:48.464872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3183203 -4.3146429 -4.3115525 -4.3095756 -4.309936 -4.3124743 -4.31579 -4.3184352 -4.3195648 -4.31931 -4.3181472 -4.3170581 -4.3169818 -4.3174853 -4.3177094][-4.3240838 -4.3183708 -4.311645 -4.3046 -4.3006148 -4.3015966 -4.3063254 -4.3123527 -4.3176246 -4.3214855 -4.3244824 -4.3268642 -4.3295197 -4.3316388 -4.3316813][-4.3265138 -4.3175669 -4.3029609 -4.2849064 -4.2711058 -4.2672586 -4.2724953 -4.2819386 -4.2921648 -4.3023291 -4.3144412 -4.3255954 -4.3349714 -4.3419652 -4.3440962][-4.3253255 -4.3096838 -4.2814994 -4.2451625 -4.2150984 -4.2012162 -4.2037878 -4.2147551 -4.2308621 -4.2513 -4.2774925 -4.302537 -4.3243451 -4.3409357 -4.3480897][-4.3132372 -4.2888474 -4.2443929 -4.1871648 -4.1389132 -4.1126351 -4.1085668 -4.1157336 -4.1348562 -4.16774 -4.2120986 -4.2557268 -4.2955585 -4.3262525 -4.3413939][-4.2843704 -4.2499685 -4.190618 -4.1167278 -4.0545306 -4.0160561 -3.9996912 -3.9939513 -4.0109973 -4.058116 -4.123508 -4.1884379 -4.2500186 -4.298686 -4.3249254][-4.2359838 -4.198482 -4.1355906 -4.0564761 -3.9872653 -3.9367504 -3.9014244 -3.8756039 -3.8886247 -3.9518859 -4.0408978 -4.1276283 -4.2082143 -4.272202 -4.3085222][-4.1781006 -4.1499305 -4.0993433 -4.0302243 -3.9661002 -3.9105291 -3.8593483 -3.814702 -3.8222675 -3.896277 -3.9998045 -4.100091 -4.1917734 -4.2629881 -4.3008332][-4.1324472 -4.127521 -4.1036711 -4.0603805 -4.0155878 -3.9698474 -3.9188354 -3.868926 -3.8685699 -3.9320848 -4.0241437 -4.1181755 -4.2047524 -4.2701669 -4.3006415][-4.1355762 -4.1505847 -4.1506243 -4.134068 -4.1126218 -4.084497 -4.0473328 -4.0067868 -4.0010581 -4.0445385 -4.1113486 -4.1827826 -4.2481923 -4.2944641 -4.3109813][-4.1881371 -4.2066607 -4.215 -4.2124043 -4.2064505 -4.19556 -4.1772985 -4.1535954 -4.1474504 -4.1721263 -4.2140779 -4.2589073 -4.2966838 -4.3202839 -4.3225641][-4.2540774 -4.2652807 -4.26932 -4.267117 -4.2655964 -4.2646222 -4.2614422 -4.2534962 -4.2524924 -4.2667184 -4.2890916 -4.3117962 -4.3267975 -4.3322315 -4.3239646][-4.2999854 -4.3022108 -4.2988734 -4.2925711 -4.2912388 -4.2951012 -4.29991 -4.3015509 -4.3052526 -4.314774 -4.3240333 -4.3298945 -4.3291759 -4.3230782 -4.3095822][-4.3186889 -4.31468 -4.3054042 -4.2950726 -4.2919626 -4.2958779 -4.3034616 -4.3097696 -4.315793 -4.3218856 -4.3232222 -4.3182225 -4.3078337 -4.2967081 -4.2841268][-4.3173842 -4.3081889 -4.294806 -4.2820983 -4.2774076 -4.2806363 -4.2882237 -4.2946424 -4.2984071 -4.2993288 -4.294888 -4.284986 -4.2727003 -4.262598 -4.2555194]]...]
INFO - root - 2017-12-07 21:57:58.100140: step 47110, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 73h:07m:24s remains)
INFO - root - 2017-12-07 21:58:07.803464: step 47120, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 78h:39m:59s remains)
INFO - root - 2017-12-07 21:58:17.518496: step 47130, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 78h:55m:01s remains)
INFO - root - 2017-12-07 21:58:27.046839: step 47140, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.970 sec/batch; 76h:54m:38s remains)
INFO - root - 2017-12-07 21:58:36.517567: step 47150, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 72h:05m:18s remains)
INFO - root - 2017-12-07 21:58:46.203995: step 47160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 73h:37m:22s remains)
INFO - root - 2017-12-07 21:58:55.592837: step 47170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 72h:12m:00s remains)
INFO - root - 2017-12-07 21:59:05.302428: step 47180, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.016 sec/batch; 80h:29m:09s remains)
INFO - root - 2017-12-07 21:59:15.104816: step 47190, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 78h:49m:11s remains)
INFO - root - 2017-12-07 21:59:24.839182: step 47200, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 77h:56m:04s remains)
2017-12-07 21:59:25.795506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3293042 -4.3288364 -4.3272457 -4.3259034 -4.3252859 -4.3258142 -4.3261371 -4.3273735 -4.3283329 -4.329371 -4.3298678 -4.33173 -4.3340945 -4.3364725 -4.3393583][-4.325037 -4.32267 -4.320282 -4.3167171 -4.312973 -4.3115339 -4.3108292 -4.3135161 -4.3162565 -4.3182096 -4.3179913 -4.3193092 -4.3226337 -4.3257456 -4.3286619][-4.3119535 -4.3059921 -4.2995248 -4.2906103 -4.2814088 -4.2768412 -4.2773643 -4.2832732 -4.2904463 -4.2936425 -4.292666 -4.2937326 -4.299284 -4.3058209 -4.310276][-4.2919164 -4.2833877 -4.2738972 -4.2601647 -4.2465339 -4.2393322 -4.2412457 -4.2513514 -4.2627048 -4.2634535 -4.2589321 -4.259531 -4.2676806 -4.278532 -4.2890329][-4.2724447 -4.26313 -4.2510171 -4.2334456 -4.2176428 -4.2094107 -4.2106748 -4.2221985 -4.2348461 -4.2325344 -4.2231531 -4.2229147 -4.2319417 -4.2470336 -4.2642279][-4.2503729 -4.2443538 -4.2308412 -4.2090187 -4.1898456 -4.1795392 -4.1813207 -4.1944432 -4.2067409 -4.2028375 -4.1916986 -4.1904635 -4.2000508 -4.2175984 -4.240603][-4.23364 -4.2333961 -4.22345 -4.2003875 -4.1753144 -4.1584468 -4.1565638 -4.17066 -4.1843224 -4.1842632 -4.1764956 -4.1784277 -4.1901555 -4.2047753 -4.2248993][-4.2312784 -4.2381063 -4.2314458 -4.2114749 -4.1829772 -4.1543994 -4.1432314 -4.1536064 -4.1708484 -4.1787233 -4.179709 -4.1897516 -4.2052011 -4.2167974 -4.2309251][-4.2403207 -4.2499933 -4.2421865 -4.2202148 -4.1830416 -4.1384807 -4.1110649 -4.11333 -4.133049 -4.1546826 -4.1713061 -4.1944857 -4.2238526 -4.2431078 -4.2584949][-4.2300005 -4.2366052 -4.228241 -4.207974 -4.1684747 -4.1130972 -4.0707655 -4.0624561 -4.0832653 -4.1163387 -4.1441164 -4.1749139 -4.2139397 -4.2432404 -4.2651172][-4.2136512 -4.2232304 -4.2231746 -4.2146297 -4.1850333 -4.13088 -4.0811691 -4.0654221 -4.0825982 -4.1146274 -4.1426125 -4.1699605 -4.2052283 -4.2325811 -4.254725][-4.1963363 -4.2125793 -4.2245884 -4.2290907 -4.2149615 -4.1741557 -4.1318097 -4.1153674 -4.1290622 -4.1513195 -4.1702361 -4.189065 -4.2117467 -4.2298112 -4.24581][-4.1797342 -4.2010579 -4.2207146 -4.2323775 -4.2261043 -4.1967754 -4.1651559 -4.1532903 -4.1647472 -4.1795511 -4.1885753 -4.1989789 -4.2124248 -4.2256446 -4.23706][-4.1793513 -4.1984572 -4.2142477 -4.2247477 -4.2192435 -4.1957741 -4.1724863 -4.1655917 -4.1772561 -4.1907454 -4.196413 -4.2022772 -4.2128158 -4.2268476 -4.2384124][-4.2102122 -4.2251773 -4.2351751 -4.2424483 -4.2375875 -4.2190828 -4.2018919 -4.1972718 -4.2055912 -4.216033 -4.2181644 -4.2205567 -4.23026 -4.2444124 -4.2560668]]...]
INFO - root - 2017-12-07 21:59:35.571535: step 47210, loss = 2.07, batch loss = 2.02 (7.8 examples/sec; 1.023 sec/batch; 81h:05m:34s remains)
INFO - root - 2017-12-07 21:59:45.171501: step 47220, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 79h:46m:16s remains)
INFO - root - 2017-12-07 21:59:54.863274: step 47230, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 73h:32m:15s remains)
INFO - root - 2017-12-07 22:00:04.641985: step 47240, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 76h:31m:17s remains)
INFO - root - 2017-12-07 22:00:14.339670: step 47250, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 76h:13m:55s remains)
INFO - root - 2017-12-07 22:00:23.923878: step 47260, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.968 sec/batch; 76h:40m:54s remains)
INFO - root - 2017-12-07 22:00:33.615170: step 47270, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 76h:33m:59s remains)
INFO - root - 2017-12-07 22:00:43.416444: step 47280, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 73h:42m:44s remains)
INFO - root - 2017-12-07 22:00:53.061894: step 47290, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 79h:13m:20s remains)
INFO - root - 2017-12-07 22:01:02.762089: step 47300, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 80h:01m:31s remains)
2017-12-07 22:01:03.835908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2044067 -4.2145996 -4.2402534 -4.2661614 -4.284337 -4.2969451 -4.3039455 -4.3114662 -4.316514 -4.317925 -4.3177018 -4.3100615 -4.295414 -4.2801948 -4.2683406][-4.2151108 -4.2205963 -4.2407622 -4.2598786 -4.2749434 -4.2869234 -4.294136 -4.301208 -4.3055143 -4.3051858 -4.3032618 -4.2948694 -4.279521 -4.26263 -4.2502213][-4.2228456 -4.2237058 -4.2371397 -4.2481089 -4.2572603 -4.2655339 -4.2715874 -4.2800441 -4.2870994 -4.2881255 -4.2863884 -4.2779889 -4.2632689 -4.2470937 -4.2358165][-4.2242637 -4.22037 -4.2262287 -4.228744 -4.2284737 -4.2279639 -4.2286553 -4.237751 -4.2508807 -4.2580318 -4.2603178 -4.2535119 -4.238482 -4.2228932 -4.2124577][-4.220696 -4.2122946 -4.2114882 -4.2077923 -4.1980886 -4.18438 -4.17271 -4.1786847 -4.19715 -4.2128582 -4.2229648 -4.2215862 -4.2091365 -4.191309 -4.1791844][-4.20849 -4.1959653 -4.1904635 -4.1847405 -4.1693311 -4.1421351 -4.1156368 -4.1133671 -4.1325622 -4.1549616 -4.1732721 -4.1784744 -4.1696258 -4.1509266 -4.135582][-4.1871843 -4.1665416 -4.1557274 -4.148695 -4.1317496 -4.0968289 -4.0590324 -4.04562 -4.0589223 -4.0839014 -4.1102414 -4.12404 -4.1198072 -4.1002216 -4.0832248][-4.1709185 -4.1399994 -4.1204519 -4.10943 -4.0963526 -4.0643311 -4.0244761 -4.0024557 -4.0029387 -4.021997 -4.0485916 -4.069479 -4.0728936 -4.0532966 -4.0372233][-4.1728797 -4.1389742 -4.1159096 -4.1026955 -4.0940404 -4.072156 -4.0428286 -4.02533 -4.0213084 -4.0348911 -4.0544939 -4.0710325 -4.0742617 -4.0544195 -4.0361519][-4.1866622 -4.1602454 -4.1429896 -4.1327634 -4.1279273 -4.1156807 -4.0981264 -4.0908747 -4.0927944 -4.1059694 -4.1199927 -4.1324763 -4.1342664 -4.1158109 -4.0961742][-4.1971011 -4.1800623 -4.1714878 -4.1668267 -4.1672034 -4.163743 -4.1566238 -4.1577625 -4.1637869 -4.1754432 -4.1851683 -4.1980162 -4.2034073 -4.1914473 -4.1767869][-4.2003536 -4.189424 -4.1877279 -4.1871991 -4.1879692 -4.187088 -4.1851363 -4.1907549 -4.1995449 -4.2103968 -4.2180347 -4.2305036 -4.2406092 -4.2379026 -4.2326632][-4.199285 -4.1887622 -4.1886411 -4.1875873 -4.1834979 -4.1782646 -4.1771235 -4.1854639 -4.197629 -4.2105331 -4.2210665 -4.2348051 -4.2479768 -4.2528214 -4.2559619][-4.195837 -4.1799421 -4.1765604 -4.1721888 -4.1631441 -4.1530442 -4.1514049 -4.1632156 -4.1805391 -4.1991525 -4.2142887 -4.2267318 -4.2383542 -4.2450109 -4.2499013][-4.1970744 -4.176229 -4.1667314 -4.1561966 -4.1442304 -4.131227 -4.1278582 -4.1408863 -4.1608381 -4.1827145 -4.2005935 -4.211987 -4.2200236 -4.2245164 -4.226368]]...]
INFO - root - 2017-12-07 22:01:13.347238: step 47310, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 78h:40m:09s remains)
INFO - root - 2017-12-07 22:01:22.768476: step 47320, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 73h:51m:44s remains)
INFO - root - 2017-12-07 22:01:32.540812: step 47330, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.016 sec/batch; 80h:27m:02s remains)
INFO - root - 2017-12-07 22:01:42.198729: step 47340, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 74h:09m:19s remains)
INFO - root - 2017-12-07 22:01:51.890869: step 47350, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 74h:22m:06s remains)
INFO - root - 2017-12-07 22:02:01.652066: step 47360, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.038 sec/batch; 82h:11m:57s remains)
INFO - root - 2017-12-07 22:02:11.264835: step 47370, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 79h:59m:11s remains)
INFO - root - 2017-12-07 22:02:20.864458: step 47380, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 73h:56m:18s remains)
INFO - root - 2017-12-07 22:02:30.445881: step 47390, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 73h:17m:29s remains)
INFO - root - 2017-12-07 22:02:39.986916: step 47400, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 75h:47m:21s remains)
2017-12-07 22:02:40.948834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2335415 -4.2330308 -4.2401252 -4.2481136 -4.2412882 -4.2274861 -4.2253423 -4.2316575 -4.2280083 -4.21669 -4.211565 -4.218452 -4.2205968 -4.2168922 -4.2062774][-4.212234 -4.200726 -4.1989732 -4.2063026 -4.2000284 -4.1885791 -4.1911964 -4.20066 -4.1937585 -4.1721091 -4.1636252 -4.1812863 -4.188477 -4.1824117 -4.1627359][-4.1746516 -4.1585231 -4.1526093 -4.1562338 -4.1457386 -4.1326375 -4.1400428 -4.1620531 -4.1612306 -4.1384544 -4.1389346 -4.16587 -4.1724958 -4.1631441 -4.1377993][-4.1388741 -4.1332684 -4.1355553 -4.1376348 -4.1178675 -4.0997243 -4.104507 -4.1319661 -4.1344533 -4.1143293 -4.1198626 -4.1513562 -4.1645184 -4.1643829 -4.1455784][-4.1408458 -4.1437106 -4.1529098 -4.1575136 -4.1381993 -4.1138811 -4.1101027 -4.1274781 -4.1211944 -4.0954328 -4.0989671 -4.1316409 -4.1574016 -4.1764669 -4.1768804][-4.1840258 -4.19163 -4.1990709 -4.196135 -4.1669154 -4.1203942 -4.0947008 -4.08769 -4.0737414 -4.0596042 -4.0742278 -4.1087012 -4.143867 -4.1820278 -4.2028236][-4.2534208 -4.2519474 -4.2437143 -4.2229862 -4.1764641 -4.1113925 -4.0618038 -4.0348754 -4.0273209 -4.0388384 -4.0674624 -4.1049838 -4.1461434 -4.1932096 -4.2265434][-4.2987809 -4.2827682 -4.2610416 -4.2316623 -4.1810632 -4.114027 -4.0580273 -4.0233073 -4.0326104 -4.0755959 -4.1160808 -4.1544628 -4.1883736 -4.2245784 -4.2530179][-4.3149185 -4.290844 -4.26814 -4.2404628 -4.1975121 -4.1467052 -4.1046686 -4.07934 -4.1022067 -4.1595469 -4.2004066 -4.229547 -4.2482319 -4.2649732 -4.2774825][-4.3111811 -4.2885137 -4.2707162 -4.2508636 -4.2252569 -4.20178 -4.1824579 -4.1713715 -4.1967149 -4.2433772 -4.271492 -4.287365 -4.29254 -4.294332 -4.2942543][-4.2931666 -4.2792649 -4.2704926 -4.2617173 -4.2550297 -4.2549047 -4.250792 -4.2461224 -4.2603984 -4.2853508 -4.3018861 -4.3106365 -4.3113003 -4.3080115 -4.3038507][-4.27768 -4.27268 -4.2700396 -4.2705555 -4.275598 -4.2823415 -4.2814603 -4.2733359 -4.2709141 -4.2790289 -4.2908249 -4.3035836 -4.3108778 -4.3111782 -4.3080707][-4.2728043 -4.2733355 -4.2725945 -4.2750845 -4.2776847 -4.2771835 -4.2719345 -4.2578988 -4.2449327 -4.2474871 -4.2614303 -4.2808714 -4.2956977 -4.3015232 -4.3022833][-4.277318 -4.2785878 -4.2751231 -4.2710509 -4.26273 -4.2508779 -4.2419891 -4.2284193 -4.2169619 -4.2250519 -4.2448997 -4.2682 -4.2840915 -4.2921062 -4.2947493][-4.2781935 -4.2804818 -4.2740073 -4.2634106 -4.2454476 -4.2291479 -4.2250552 -4.2217293 -4.223619 -4.24188 -4.2646689 -4.28532 -4.2962146 -4.2993455 -4.296649]]...]
INFO - root - 2017-12-07 22:02:50.608709: step 47410, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 74h:04m:27s remains)
INFO - root - 2017-12-07 22:03:00.092633: step 47420, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 73h:28m:40s remains)
INFO - root - 2017-12-07 22:03:09.619693: step 47430, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 75h:38m:11s remains)
INFO - root - 2017-12-07 22:03:19.228768: step 47440, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 80h:12m:08s remains)
INFO - root - 2017-12-07 22:03:28.879586: step 47450, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 80h:10m:25s remains)
INFO - root - 2017-12-07 22:03:38.372999: step 47460, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 76h:12m:27s remains)
INFO - root - 2017-12-07 22:03:47.851008: step 47470, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 72h:36m:50s remains)
INFO - root - 2017-12-07 22:03:57.451181: step 47480, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.946 sec/batch; 74h:53m:24s remains)
INFO - root - 2017-12-07 22:04:07.130669: step 47490, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 77h:30m:34s remains)
INFO - root - 2017-12-07 22:04:16.651735: step 47500, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:20m:50s remains)
2017-12-07 22:04:17.610867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34524 -4.3372746 -4.3248448 -4.3142648 -4.3090343 -4.3078475 -4.3080215 -4.3104057 -4.314187 -4.3180671 -4.3198137 -4.3160415 -4.3076143 -4.3038259 -4.3073][-4.3407364 -4.3319454 -4.3168044 -4.3026462 -4.293818 -4.2875204 -4.2819748 -4.2796717 -4.2806439 -4.2860422 -4.2905684 -4.2882147 -4.2777162 -4.2719431 -4.2752867][-4.338913 -4.3293495 -4.3144312 -4.2992024 -4.2857738 -4.2729034 -4.2594271 -4.2501421 -4.2470436 -4.2515211 -4.2561975 -4.2564096 -4.2484932 -4.2446871 -4.2493834][-4.3434958 -4.3327522 -4.3188634 -4.3030577 -4.282814 -4.2615423 -4.2398906 -4.225306 -4.2193928 -4.2222786 -4.225184 -4.2264309 -4.2243271 -4.2232285 -4.2291193][-4.3448129 -4.3317971 -4.3177514 -4.3024096 -4.2760506 -4.2443681 -4.2133069 -4.1952248 -4.1897178 -4.1920919 -4.1909642 -4.1893053 -4.1920896 -4.192884 -4.1999106][-4.3319445 -4.3134661 -4.294271 -4.2731051 -4.2408943 -4.199729 -4.1593394 -4.1370997 -4.1412969 -4.1526694 -4.1505742 -4.1446958 -4.1496592 -4.1554151 -4.1643224][-4.3076429 -4.2830172 -4.2548795 -4.2212033 -4.1764278 -4.1203728 -4.0634413 -4.0302362 -4.0513844 -4.0881529 -4.101274 -4.1002665 -4.1082172 -4.1157084 -4.1247349][-4.278563 -4.246913 -4.20876 -4.164155 -4.1078248 -4.0343366 -3.9555535 -3.9068909 -3.9467854 -4.0098324 -4.0418663 -4.0583773 -4.0777197 -4.0881696 -4.0951257][-4.2607284 -4.22506 -4.1834941 -4.1375985 -4.0877995 -4.0208826 -3.9500537 -3.9104722 -3.9436958 -3.9948752 -4.0260844 -4.0536604 -4.0818934 -4.0958891 -4.1036949][-4.2669477 -4.2316551 -4.1909389 -4.1465373 -4.1058683 -4.0600634 -4.0216537 -4.0124693 -4.0393586 -4.0666738 -4.0865736 -4.1123762 -4.1368074 -4.1479869 -4.1562657][-4.2953105 -4.2712088 -4.2407742 -4.2047296 -4.1697731 -4.1404533 -4.12552 -4.1318169 -4.156332 -4.1741428 -4.1896238 -4.2105322 -4.2269425 -4.2327037 -4.2402334][-4.3229957 -4.3114729 -4.2981973 -4.282064 -4.2630253 -4.2459307 -4.2376928 -4.242075 -4.2560306 -4.268043 -4.2818375 -4.2986307 -4.3107324 -4.3155913 -4.32092][-4.3420534 -4.3370657 -4.3342919 -4.3312821 -4.326057 -4.3186126 -4.3159761 -4.3175569 -4.3228021 -4.3305349 -4.3398628 -4.350687 -4.3586664 -4.3629808 -4.3655434][-4.3511081 -4.3492827 -4.3492947 -4.3507838 -4.3522587 -4.3516793 -4.3528128 -4.353807 -4.3553009 -4.3586287 -4.3628573 -4.3668938 -4.3704877 -4.3730717 -4.3738775][-4.3536491 -4.3534484 -4.3540359 -4.3558526 -4.3578382 -4.3591547 -4.3600225 -4.3593416 -4.3591204 -4.3606343 -4.3626285 -4.3639636 -4.3652349 -4.3667593 -4.3670635]]...]
INFO - root - 2017-12-07 22:04:27.045268: step 47510, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 76h:14m:33s remains)
INFO - root - 2017-12-07 22:04:36.598899: step 47520, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.020 sec/batch; 80h:44m:08s remains)
INFO - root - 2017-12-07 22:04:46.341314: step 47530, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 75h:31m:59s remains)
INFO - root - 2017-12-07 22:04:55.914461: step 47540, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 73h:47m:59s remains)
INFO - root - 2017-12-07 22:05:05.701696: step 47550, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.015 sec/batch; 80h:19m:32s remains)
INFO - root - 2017-12-07 22:05:15.380597: step 47560, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.949 sec/batch; 75h:08m:38s remains)
INFO - root - 2017-12-07 22:05:25.153139: step 47570, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 79h:02m:19s remains)
INFO - root - 2017-12-07 22:05:34.787874: step 47580, loss = 2.11, batch loss = 2.06 (8.6 examples/sec; 0.926 sec/batch; 73h:16m:50s remains)
INFO - root - 2017-12-07 22:05:44.379085: step 47590, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 73h:49m:22s remains)
INFO - root - 2017-12-07 22:05:54.039950: step 47600, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 77h:46m:24s remains)
2017-12-07 22:05:55.003371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2155628 -4.195488 -4.1634655 -4.12453 -4.1014404 -4.1148281 -4.1470704 -4.1648602 -4.1570616 -4.1370997 -4.1378851 -4.1591063 -4.1933579 -4.2297873 -4.2626195][-4.2289052 -4.2182021 -4.1901407 -4.1526184 -4.1283751 -4.1411982 -4.1693745 -4.1823111 -4.1701255 -4.1463532 -4.1490378 -4.1712351 -4.2023807 -4.2339525 -4.2616129][-4.2441721 -4.2460485 -4.2269278 -4.1965094 -4.1724834 -4.1800342 -4.19993 -4.2029786 -4.1782308 -4.1448941 -4.1502957 -4.1759295 -4.2052417 -4.2332697 -4.2564511][-4.25148 -4.2588325 -4.2441115 -4.2175121 -4.19329 -4.1931982 -4.2000756 -4.18988 -4.1488543 -4.1073408 -4.1204824 -4.1552186 -4.1904454 -4.2213078 -4.24587][-4.240253 -4.2507963 -4.2386169 -4.2119975 -4.1847534 -4.1724157 -4.1644926 -4.1391106 -4.0856442 -4.0409994 -4.0670929 -4.1177769 -4.1640878 -4.2027135 -4.2320695][-4.2223635 -4.2349572 -4.224082 -4.1961918 -4.1631575 -4.1420555 -4.129324 -4.0992694 -4.0411644 -3.9992182 -4.0376363 -4.0992069 -4.1521306 -4.1964989 -4.2285876][-4.2094774 -4.2189865 -4.2017508 -4.1723018 -4.13662 -4.1112452 -4.0982509 -4.0653543 -4.0058117 -3.9707453 -4.0215907 -4.0916462 -4.1478572 -4.196311 -4.2274652][-4.2186522 -4.2191834 -4.1962996 -4.1660371 -4.1321592 -4.10795 -4.0993996 -4.0689545 -4.0168214 -3.9939976 -4.0517063 -4.1175809 -4.1652374 -4.2077317 -4.2332454][-4.238049 -4.2322807 -4.2055116 -4.175981 -4.1519575 -4.1364326 -4.1339812 -4.1068149 -4.06052 -4.0463991 -4.1039634 -4.1601658 -4.1980228 -4.2293005 -4.2477627][-4.2553263 -4.2472425 -4.2214932 -4.1999145 -4.1883082 -4.1848526 -4.1890635 -4.1645136 -4.1171522 -4.1021514 -4.1491017 -4.1945243 -4.222908 -4.2466345 -4.2602859][-4.2709823 -4.2622247 -4.2402811 -4.2233858 -4.216742 -4.2193713 -4.2282748 -4.2086916 -4.1676378 -4.1562223 -4.1927705 -4.2272487 -4.2493081 -4.2687798 -4.278019][-4.2788663 -4.2707176 -4.2541423 -4.2399578 -4.2330995 -4.2365704 -4.24499 -4.2308269 -4.1997476 -4.1930189 -4.2199407 -4.2463546 -4.2666969 -4.28566 -4.2929673][-4.2792749 -4.2699561 -4.2582054 -4.2504334 -4.2468867 -4.250752 -4.2587566 -4.2520323 -4.2314987 -4.2253304 -4.2420068 -4.2611041 -4.2780914 -4.2945561 -4.30187][-4.2848892 -4.2763848 -4.2674088 -4.2633581 -4.264833 -4.2714653 -4.2781868 -4.2746191 -4.26055 -4.2525034 -4.2603736 -4.2745333 -4.2897987 -4.3029652 -4.3094568][-4.2962284 -4.2902346 -4.2842793 -4.2817254 -4.28328 -4.28749 -4.2908378 -4.28878 -4.2802472 -4.2734733 -4.2765174 -4.2859197 -4.297893 -4.3085327 -4.3151431]]...]
INFO - root - 2017-12-07 22:06:04.597604: step 47610, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 75h:24m:22s remains)
INFO - root - 2017-12-07 22:06:14.275897: step 47620, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.026 sec/batch; 81h:10m:07s remains)
INFO - root - 2017-12-07 22:06:23.734928: step 47630, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 73h:09m:08s remains)
INFO - root - 2017-12-07 22:06:33.294381: step 47640, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.029 sec/batch; 81h:25m:36s remains)
INFO - root - 2017-12-07 22:06:42.930713: step 47650, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 75h:56m:20s remains)
INFO - root - 2017-12-07 22:06:52.479069: step 47660, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 72h:36m:18s remains)
INFO - root - 2017-12-07 22:07:02.023139: step 47670, loss = 2.12, batch loss = 2.06 (8.4 examples/sec; 0.955 sec/batch; 75h:34m:57s remains)
INFO - root - 2017-12-07 22:07:11.673077: step 47680, loss = 2.11, batch loss = 2.05 (8.1 examples/sec; 0.987 sec/batch; 78h:07m:26s remains)
INFO - root - 2017-12-07 22:07:21.436044: step 47690, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.976 sec/batch; 77h:14m:48s remains)
INFO - root - 2017-12-07 22:07:31.092038: step 47700, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.971 sec/batch; 76h:47m:21s remains)
2017-12-07 22:07:31.993633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1595578 -4.1381731 -4.1172628 -4.1088338 -4.1107221 -4.1204033 -4.1414833 -4.1595893 -4.1563859 -4.1406579 -4.1158042 -4.0881028 -4.075027 -4.0798554 -4.096858][-4.1606374 -4.1432667 -4.1258039 -4.1245575 -4.1401424 -4.1594324 -4.1835003 -4.2045045 -4.2095904 -4.2010717 -4.1807289 -4.158771 -4.1453934 -4.1433816 -4.1500335][-4.1564474 -4.150785 -4.1428022 -4.1489868 -4.1673088 -4.1850395 -4.2062683 -4.2275305 -4.2369375 -4.2373405 -4.2290115 -4.21596 -4.2026067 -4.1943693 -4.1940608][-4.14563 -4.1553159 -4.1631689 -4.1737738 -4.1799173 -4.1791372 -4.1863127 -4.2000031 -4.2085872 -4.2176561 -4.2214847 -4.2166939 -4.2060213 -4.2009726 -4.20794][-4.1326275 -4.157042 -4.1765218 -4.184082 -4.1705284 -4.1428437 -4.1238437 -4.1214218 -4.1297135 -4.1512332 -4.1667018 -4.1699204 -4.1680312 -4.1748104 -4.196681][-4.1309466 -4.1628227 -4.1842012 -4.1806526 -4.1443048 -4.0895071 -4.0407457 -4.0175896 -4.02899 -4.0690665 -4.1010804 -4.1115861 -4.1194682 -4.1396718 -4.1722627][-4.141139 -4.1682615 -4.1822696 -4.1679645 -4.1195478 -4.0575056 -3.9952018 -3.9525194 -3.9557769 -4.0053916 -4.0506492 -4.065546 -4.0778193 -4.1030293 -4.14069][-4.1416836 -4.1611843 -4.1701465 -4.1564837 -4.1110382 -4.0600553 -4.0061355 -3.9572804 -3.946286 -3.9861879 -4.0304689 -4.04677 -4.0553608 -4.0743904 -4.1096888][-4.1549082 -4.1689186 -4.1770144 -4.1696539 -4.1347427 -4.0987296 -4.0618973 -4.0206318 -4.00235 -4.0261545 -4.0615716 -4.0750432 -4.0754333 -4.0810981 -4.1050062][-4.1971459 -4.204885 -4.2108088 -4.2076359 -4.1861868 -4.1637158 -4.1424623 -4.1154075 -4.0990949 -4.108943 -4.1289887 -4.134151 -4.127286 -4.1213031 -4.1323442][-4.24932 -4.2482653 -4.2484422 -4.2473378 -4.2398782 -4.2316604 -4.2217293 -4.208775 -4.1997414 -4.2000985 -4.2045169 -4.2014542 -4.1906314 -4.1793871 -4.1836691][-4.2904654 -4.2851667 -4.2828012 -4.2847619 -4.286633 -4.2880969 -4.2863264 -4.2836909 -4.2798481 -4.2733703 -4.2664843 -4.2591004 -4.2493348 -4.2381473 -4.240664][-4.313571 -4.3079948 -4.3054175 -4.3084135 -4.3140578 -4.3209834 -4.3257422 -4.3293328 -4.329123 -4.3215671 -4.31109 -4.3008294 -4.2915187 -4.2839355 -4.2873073][-4.3273754 -4.3248706 -4.3236036 -4.3254995 -4.3298965 -4.3365412 -4.3433843 -4.3495889 -4.3529739 -4.3489833 -4.3403168 -4.3316355 -4.3245511 -4.3207936 -4.3242774][-4.3408504 -4.3419609 -4.3426318 -4.343297 -4.3447871 -4.3476706 -4.3514047 -4.3555059 -4.3590245 -4.3586383 -4.3548741 -4.3501854 -4.3461332 -4.3446212 -4.3469973]]...]
INFO - root - 2017-12-07 22:07:41.548757: step 47710, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 75h:59m:49s remains)
INFO - root - 2017-12-07 22:07:51.220462: step 47720, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 77h:47m:43s remains)
INFO - root - 2017-12-07 22:08:00.794015: step 47730, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.948 sec/batch; 75h:01m:13s remains)
INFO - root - 2017-12-07 22:08:10.375159: step 47740, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 78h:38m:06s remains)
INFO - root - 2017-12-07 22:08:20.110763: step 47750, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.950 sec/batch; 75h:10m:40s remains)
INFO - root - 2017-12-07 22:08:29.847401: step 47760, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 75h:15m:10s remains)
INFO - root - 2017-12-07 22:08:39.465945: step 47770, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 79h:21m:44s remains)
INFO - root - 2017-12-07 22:08:49.292733: step 47780, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.936 sec/batch; 73h:59m:48s remains)
INFO - root - 2017-12-07 22:08:58.859404: step 47790, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 75h:12m:48s remains)
INFO - root - 2017-12-07 22:09:08.497128: step 47800, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 68h:42m:52s remains)
2017-12-07 22:09:09.443707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2889047 -4.2930593 -4.29093 -4.2849827 -4.2698355 -4.2469516 -4.2335763 -4.2487564 -4.2643948 -4.2708139 -4.2688131 -4.2618794 -4.2605004 -4.2582526 -4.2527943][-4.2820539 -4.2858205 -4.2842984 -4.2803736 -4.2689342 -4.2455 -4.2293835 -4.2453623 -4.2649589 -4.2720819 -4.2688384 -4.2606921 -4.2574344 -4.2532969 -4.2481871][-4.2779312 -4.2823629 -4.281846 -4.2809043 -4.2742653 -4.2518907 -4.2336903 -4.2497082 -4.2727566 -4.2826939 -4.2807097 -4.2724667 -4.2652092 -4.2566142 -4.2492723][-4.27473 -4.2783194 -4.2774563 -4.2773461 -4.2727475 -4.2497849 -4.2286839 -4.2429743 -4.268703 -4.2825685 -4.2845378 -4.2806292 -4.2733474 -4.261044 -4.2482266][-4.2718043 -4.2716012 -4.26736 -4.2644849 -4.2608581 -4.2376952 -4.2156811 -4.2323942 -4.2624598 -4.2810426 -4.2894993 -4.2934661 -4.2858057 -4.2667451 -4.24619][-4.2690854 -4.2644362 -4.2558045 -4.2491703 -4.2464581 -4.22517 -4.2061105 -4.2259274 -4.2559614 -4.2763891 -4.2901831 -4.2997413 -4.2894764 -4.2668653 -4.2447424][-4.2660174 -4.2580094 -4.2473035 -4.2393422 -4.2388988 -4.2225 -4.2095423 -4.2300787 -4.2555504 -4.2733278 -4.2857933 -4.2927356 -4.2774568 -4.2527723 -4.2326169][-4.2634249 -4.2523894 -4.2393432 -4.2307973 -4.2314205 -4.2189755 -4.2107348 -4.2319064 -4.2521305 -4.2633014 -4.2698164 -4.2708235 -4.2508593 -4.2258329 -4.2106781][-4.259872 -4.2452745 -4.2282438 -4.2165723 -4.215414 -4.2035465 -4.1962957 -4.2194448 -4.239471 -4.2448921 -4.2446694 -4.2393022 -4.2168832 -4.194675 -4.1876888][-4.255424 -4.2379756 -4.2177587 -4.203054 -4.1989336 -4.1859217 -4.17863 -4.2055516 -4.2287712 -4.2298341 -4.2229915 -4.2148352 -4.1946964 -4.1779714 -4.1766338][-4.2522154 -4.2333627 -4.2130461 -4.1970263 -4.1892791 -4.1741648 -4.1650877 -4.1946135 -4.2200727 -4.2186289 -4.207087 -4.1979461 -4.1823363 -4.1697068 -4.170598][-4.2506013 -4.23105 -4.2114925 -4.1952748 -4.1845083 -4.1668472 -4.1542144 -4.1799564 -4.2041888 -4.2022791 -4.1891088 -4.1804657 -4.1695886 -4.1600327 -4.1630311][-4.251143 -4.2300324 -4.2089624 -4.1909409 -4.17694 -4.1570616 -4.1421609 -4.1633625 -4.1858444 -4.1853743 -4.1740417 -4.1684074 -4.1644835 -4.1589608 -4.1647091][-4.2537217 -4.229372 -4.205121 -4.1841035 -4.1686616 -4.1494632 -4.137043 -4.1576228 -4.1787682 -4.18026 -4.1722565 -4.1698656 -4.1697245 -4.1672306 -4.1753235][-4.2595253 -4.2321019 -4.2046719 -4.1821861 -4.1675949 -4.1521125 -4.1454878 -4.1667485 -4.1871047 -4.1907358 -4.1848645 -4.1837826 -4.1850567 -4.184031 -4.1917582]]...]
INFO - root - 2017-12-07 22:09:18.969402: step 47810, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 78h:36m:05s remains)
INFO - root - 2017-12-07 22:09:28.577230: step 47820, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 73h:09m:50s remains)
INFO - root - 2017-12-07 22:09:38.300082: step 47830, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.004 sec/batch; 79h:24m:28s remains)
INFO - root - 2017-12-07 22:09:48.043465: step 47840, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 73h:12m:00s remains)
INFO - root - 2017-12-07 22:09:57.674655: step 47850, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 73h:58m:31s remains)
INFO - root - 2017-12-07 22:10:07.447146: step 47860, loss = 2.04, batch loss = 1.98 (7.8 examples/sec; 1.020 sec/batch; 80h:39m:48s remains)
INFO - root - 2017-12-07 22:10:17.190417: step 47870, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 76h:55m:01s remains)
INFO - root - 2017-12-07 22:10:26.928801: step 47880, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 75h:21m:07s remains)
INFO - root - 2017-12-07 22:10:36.508459: step 47890, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.003 sec/batch; 79h:17m:11s remains)
INFO - root - 2017-12-07 22:10:46.189686: step 47900, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 76h:18m:41s remains)
2017-12-07 22:10:47.153796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2578211 -4.2727761 -4.2793164 -4.2845654 -4.2876291 -4.2876568 -4.287241 -4.2844 -4.2772 -4.2713847 -4.2694793 -4.2685766 -4.2704411 -4.2721982 -4.2688537][-4.2172546 -4.24052 -4.25553 -4.2655067 -4.2714219 -4.2755961 -4.2822323 -4.2908897 -4.2953391 -4.2966604 -4.295125 -4.2903309 -4.2832093 -4.2740068 -4.2613149][-4.1850619 -4.2131934 -4.235033 -4.2466211 -4.2508316 -4.2549357 -4.2649221 -4.2814631 -4.2948093 -4.3029613 -4.3031411 -4.2973018 -4.2838016 -4.2660193 -4.246942][-4.1856518 -4.2092528 -4.2293882 -4.2368841 -4.2322116 -4.2279081 -4.2365227 -4.258297 -4.282021 -4.2977242 -4.2994828 -4.2917695 -4.2738214 -4.2540269 -4.2372022][-4.2108393 -4.2212834 -4.2292714 -4.223958 -4.2040906 -4.184236 -4.1805162 -4.2046371 -4.2413931 -4.26738 -4.2741337 -4.2670717 -4.2475834 -4.2284985 -4.2185016][-4.2439656 -4.2391467 -4.2305784 -4.2074156 -4.1669512 -4.1252632 -4.101707 -4.1212277 -4.1711941 -4.2110882 -4.2300162 -4.233305 -4.2202229 -4.204339 -4.2009821][-4.2731533 -4.2550488 -4.2302485 -4.1869974 -4.123816 -4.0563 -4.0056868 -4.0144205 -4.0790725 -4.1406302 -4.1828394 -4.2084312 -4.213953 -4.2088189 -4.2097225][-4.2924109 -4.2603469 -4.2159882 -4.1478467 -4.0584383 -3.9636092 -3.88109 -3.8718441 -3.9541457 -4.0483294 -4.1242657 -4.1813178 -4.2149892 -4.2296333 -4.23768][-4.29293 -4.2522955 -4.1953449 -4.1145849 -4.018713 -3.9197221 -3.8298233 -3.8103216 -3.8907175 -3.9943891 -4.0899639 -4.1699219 -4.2250252 -4.2576737 -4.2745242][-4.2886577 -4.2493806 -4.1950545 -4.1240463 -4.0474806 -3.97655 -3.9158239 -3.9047234 -3.9615858 -4.041934 -4.1269326 -4.2038608 -4.2608004 -4.2968159 -4.3151135][-4.2882929 -4.2582312 -4.2160153 -4.1630936 -4.1110439 -4.0726056 -4.0468645 -4.0503221 -4.08858 -4.1412492 -4.2016735 -4.2573524 -4.3003817 -4.3286934 -4.3444147][-4.3015985 -4.2847171 -4.2602434 -4.2288656 -4.2002759 -4.1846142 -4.1791167 -4.1887789 -4.2122936 -4.2408919 -4.2749038 -4.3054938 -4.3299532 -4.3465705 -4.3565068][-4.3219442 -4.3151488 -4.3056064 -4.2938452 -4.2847452 -4.281991 -4.2845521 -4.292778 -4.3040762 -4.3135767 -4.3261008 -4.3378897 -4.3479786 -4.3558292 -4.3601012][-4.3366294 -4.3341904 -4.3329296 -4.3329749 -4.3332815 -4.3348923 -4.3390903 -4.3444905 -4.3502388 -4.3517408 -4.3538675 -4.3560414 -4.3582654 -4.3608928 -4.3622427][-4.3435545 -4.343451 -4.3451948 -4.3495665 -4.353301 -4.35538 -4.3576794 -4.3592644 -4.3612375 -4.3622446 -4.3634686 -4.3633919 -4.36302 -4.3633995 -4.3631063]]...]
INFO - root - 2017-12-07 22:10:56.891222: step 47910, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 74h:14m:32s remains)
INFO - root - 2017-12-07 22:11:06.415147: step 47920, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.982 sec/batch; 77h:35m:23s remains)
INFO - root - 2017-12-07 22:11:16.135792: step 47930, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.018 sec/batch; 80h:26m:26s remains)
INFO - root - 2017-12-07 22:11:25.732795: step 47940, loss = 2.12, batch loss = 2.06 (9.1 examples/sec; 0.882 sec/batch; 69h:44m:47s remains)
INFO - root - 2017-12-07 22:11:35.451895: step 47950, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 74h:08m:37s remains)
INFO - root - 2017-12-07 22:11:44.918580: step 47960, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.007 sec/batch; 79h:37m:27s remains)
INFO - root - 2017-12-07 22:11:54.609101: step 47970, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 73h:31m:26s remains)
INFO - root - 2017-12-07 22:12:04.180709: step 47980, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 73h:19m:33s remains)
INFO - root - 2017-12-07 22:12:13.862192: step 47990, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 74h:14m:38s remains)
INFO - root - 2017-12-07 22:12:23.377651: step 48000, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 72h:35m:24s remains)
2017-12-07 22:12:24.364327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2306776 -4.2198062 -4.2147832 -4.2198634 -4.2214379 -4.2226281 -4.2111683 -4.1922374 -4.1829829 -4.1840029 -4.1806173 -4.167192 -4.1558132 -4.1480451 -4.1407418][-4.2182703 -4.2130451 -4.21028 -4.2161441 -4.2136679 -4.2168121 -4.209199 -4.1927314 -4.1793933 -4.1734352 -4.17208 -4.1657763 -4.1514583 -4.1390624 -4.1276035][-4.2100835 -4.2113752 -4.2073383 -4.2116933 -4.2084355 -4.2137632 -4.2105331 -4.1979394 -4.1840158 -4.1768484 -4.1784096 -4.1713462 -4.1519918 -4.1406488 -4.1325293][-4.1994452 -4.2021313 -4.1972079 -4.2000284 -4.1989985 -4.2037868 -4.2032151 -4.1995974 -4.1975579 -4.1987138 -4.202322 -4.1891017 -4.1677327 -4.1588278 -4.1506019][-4.1809978 -4.1815763 -4.1754851 -4.1765313 -4.1752992 -4.1741247 -4.1657586 -4.1656551 -4.1771355 -4.192347 -4.1975389 -4.1802917 -4.1604934 -4.1536469 -4.1470494][-4.1611938 -4.1606669 -4.15656 -4.1546426 -4.1454573 -4.125515 -4.1027918 -4.0918417 -4.1030712 -4.1268578 -4.1374717 -4.1254578 -4.1111193 -4.1092072 -4.1095691][-4.1469607 -4.1440873 -4.14443 -4.1383824 -4.1180263 -4.0839434 -4.0481081 -4.0262079 -4.0310779 -4.05607 -4.0703163 -4.0672927 -4.0654979 -4.0717063 -4.0708604][-4.1411819 -4.1361566 -4.1417942 -4.1358056 -4.1142974 -4.081068 -4.0475392 -4.0273933 -4.0289712 -4.0431805 -4.0488739 -4.0541773 -4.0641594 -4.0738807 -4.0645671][-4.1424994 -4.1394529 -4.1483216 -4.1469836 -4.1326714 -4.1183233 -4.1025262 -4.0935631 -4.0938363 -4.09569 -4.0916734 -4.0950842 -4.1057515 -4.1146894 -4.1042871][-4.1459 -4.1413145 -4.1443281 -4.1466446 -4.1481743 -4.1550183 -4.1535559 -4.1549373 -4.160131 -4.1624012 -4.1573267 -4.157855 -4.1655221 -4.1748877 -4.175519][-4.1522446 -4.1408358 -4.1350269 -4.1421337 -4.1562128 -4.1763673 -4.1851149 -4.1934648 -4.2027521 -4.207756 -4.2049694 -4.20591 -4.21562 -4.2254429 -4.233583][-4.1758347 -4.1596341 -4.1457767 -4.1510706 -4.1684122 -4.1837859 -4.1930218 -4.2040963 -4.2121019 -4.2119212 -4.2112823 -4.2146082 -4.2250676 -4.237164 -4.2487688][-4.2001557 -4.1827364 -4.1661873 -4.1666231 -4.1770816 -4.18013 -4.1839609 -4.1904068 -4.189827 -4.1859984 -4.1875544 -4.19379 -4.2022347 -4.2126846 -4.2258887][-4.2128797 -4.1980834 -4.1858954 -4.1850333 -4.184927 -4.1736393 -4.1685295 -4.1630516 -4.1517153 -4.1463175 -4.1498332 -4.1601729 -4.163897 -4.1719379 -4.1878977][-4.21542 -4.2028413 -4.1937261 -4.1945271 -4.1876874 -4.1688943 -4.15552 -4.1376863 -4.1163874 -4.1108661 -4.12304 -4.138423 -4.1383948 -4.14545 -4.1574926]]...]
INFO - root - 2017-12-07 22:12:33.890204: step 48010, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.936 sec/batch; 74h:00m:18s remains)
INFO - root - 2017-12-07 22:12:43.523242: step 48020, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 76h:18m:15s remains)
INFO - root - 2017-12-07 22:12:53.180185: step 48030, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 74h:54m:17s remains)
INFO - root - 2017-12-07 22:13:02.794845: step 48040, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 78h:09m:17s remains)
INFO - root - 2017-12-07 22:13:12.448125: step 48050, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 77h:55m:45s remains)
INFO - root - 2017-12-07 22:13:22.266778: step 48060, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 74h:55m:45s remains)
INFO - root - 2017-12-07 22:13:31.980078: step 48070, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 72h:29m:45s remains)
INFO - root - 2017-12-07 22:13:41.778246: step 48080, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.046 sec/batch; 82h:36m:27s remains)
INFO - root - 2017-12-07 22:13:51.546550: step 48090, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 73h:28m:07s remains)
INFO - root - 2017-12-07 22:14:01.169765: step 48100, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 76h:53m:03s remains)
2017-12-07 22:14:02.124499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.259409 -4.2702894 -4.2843194 -4.2901187 -4.2861862 -4.2788777 -4.2709737 -4.2768087 -4.2924762 -4.3041968 -4.310019 -4.309011 -4.3061485 -4.3083324 -4.3139939][-4.23911 -4.2338181 -4.2397323 -4.2474232 -4.2446666 -4.2373757 -4.2369046 -4.2543039 -4.278738 -4.2962389 -4.3060741 -4.3080969 -4.3072577 -4.3119493 -4.3204513][-4.2022924 -4.1854477 -4.1823645 -4.192749 -4.1971135 -4.2007356 -4.21487 -4.2428188 -4.2657347 -4.2750521 -4.2799625 -4.2802815 -4.2815738 -4.289454 -4.303031][-4.1718841 -4.156929 -4.1442008 -4.1490326 -4.1565132 -4.1678209 -4.193903 -4.2256789 -4.2444425 -4.2487164 -4.2464857 -4.2445011 -4.2477264 -4.2599277 -4.2765126][-4.1610875 -4.1609583 -4.1426215 -4.1315575 -4.129096 -4.1376896 -4.1608748 -4.1858449 -4.2007351 -4.2106123 -4.2126575 -4.2118177 -4.2167511 -4.2291365 -4.2454066][-4.1583676 -4.1799312 -4.1616378 -4.1340179 -4.115315 -4.1064448 -4.1110473 -4.1197457 -4.1299973 -4.1497521 -4.1639862 -4.1747527 -4.1905847 -4.2096782 -4.2283483][-4.1607738 -4.1896043 -4.1665883 -4.1219878 -4.0854206 -4.0572643 -4.0446467 -4.048974 -4.0651169 -4.0989914 -4.1309977 -4.1562014 -4.1841693 -4.2135243 -4.2357][-4.1703248 -4.1870079 -4.1549373 -4.0974298 -4.05486 -4.0255346 -4.0085416 -4.0174761 -4.0434208 -4.0904956 -4.1374979 -4.1667228 -4.1993475 -4.2318578 -4.2570958][-4.1843553 -4.179697 -4.1419339 -4.0900211 -4.0609818 -4.0460653 -4.0382724 -4.0515966 -4.0769415 -4.1256285 -4.1783791 -4.207449 -4.2374229 -4.2642946 -4.2883811][-4.2144356 -4.1929374 -4.1513524 -4.1139469 -4.0996404 -4.0974331 -4.1013832 -4.1145377 -4.1342769 -4.1686969 -4.2146564 -4.2412648 -4.2637496 -4.2845106 -4.3089561][-4.2279863 -4.2038016 -4.1685338 -4.1443319 -4.1402187 -4.14949 -4.1617527 -4.1720638 -4.1839643 -4.200655 -4.2366877 -4.2572675 -4.2723017 -4.2897716 -4.3120928][-4.2256737 -4.2048006 -4.18013 -4.1673927 -4.1709275 -4.1869512 -4.2017174 -4.2070732 -4.2141953 -4.2214146 -4.247345 -4.2608714 -4.2691894 -4.2840543 -4.3053579][-4.2143416 -4.1947851 -4.1800289 -4.1763167 -4.1818686 -4.1965938 -4.2131691 -4.2118793 -4.2125425 -4.2136025 -4.230855 -4.2406964 -4.2469115 -4.2641406 -4.2865334][-4.21004 -4.1854887 -4.1671157 -4.1599541 -4.1605344 -4.1710415 -4.1895719 -4.1907268 -4.1923409 -4.1919088 -4.2042375 -4.2118468 -4.2129107 -4.2314095 -4.2527633][-4.2086196 -4.1799188 -4.1530342 -4.1382504 -4.1329303 -4.1416044 -4.1618061 -4.1704478 -4.1763716 -4.1798797 -4.1950064 -4.2016654 -4.192328 -4.2028093 -4.2159095]]...]
INFO - root - 2017-12-07 22:14:11.708409: step 48110, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 74h:55m:57s remains)
INFO - root - 2017-12-07 22:14:21.361930: step 48120, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 75h:31m:58s remains)
INFO - root - 2017-12-07 22:14:30.930038: step 48130, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 76h:58m:42s remains)
INFO - root - 2017-12-07 22:14:40.769783: step 48140, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 78h:04m:40s remains)
INFO - root - 2017-12-07 22:14:50.235110: step 48150, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 74h:26m:11s remains)
INFO - root - 2017-12-07 22:14:59.979412: step 48160, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 79h:23m:25s remains)
INFO - root - 2017-12-07 22:15:09.647391: step 48170, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 76h:58m:00s remains)
INFO - root - 2017-12-07 22:15:19.259735: step 48180, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 76h:30m:21s remains)
INFO - root - 2017-12-07 22:15:28.766129: step 48190, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 74h:15m:13s remains)
INFO - root - 2017-12-07 22:15:38.402677: step 48200, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.997 sec/batch; 78h:45m:44s remains)
2017-12-07 22:15:39.435239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3425932 -4.3486714 -4.34428 -4.3353529 -4.3306704 -4.3236518 -4.3172379 -4.3080473 -4.3004827 -4.2949777 -4.3017573 -4.3119254 -4.3212075 -4.328547 -4.3274064][-4.333169 -4.3306293 -4.3218188 -4.3117795 -4.3031635 -4.2886214 -4.2764783 -4.261416 -4.2491465 -4.2434006 -4.2586718 -4.2785811 -4.2990932 -4.3139238 -4.3145576][-4.3114328 -4.3002872 -4.2877321 -4.2786422 -4.26801 -4.25216 -4.23705 -4.2178864 -4.202445 -4.1960149 -4.2176895 -4.2501359 -4.282403 -4.3036423 -4.3047256][-4.2823968 -4.2633576 -4.2468519 -4.2375793 -4.229279 -4.2164989 -4.1991587 -4.1783752 -4.164084 -4.1609254 -4.1878624 -4.2278271 -4.2676182 -4.29419 -4.29757][-4.2563543 -4.2305088 -4.2117796 -4.1997294 -4.1906676 -4.1790376 -4.1624026 -4.1413908 -4.1309166 -4.132658 -4.1625719 -4.2089109 -4.2589655 -4.2945013 -4.3033814][-4.2281089 -4.1994138 -4.175591 -4.1520691 -4.1295023 -4.109971 -4.09038 -4.0700827 -4.073873 -4.0944996 -4.1392 -4.2018747 -4.2627721 -4.3052421 -4.317544][-4.2066355 -4.174603 -4.1436915 -4.1015415 -4.0580006 -4.0187917 -3.9822898 -3.959317 -3.9821935 -4.0357041 -4.1123414 -4.1950092 -4.2622 -4.3074269 -4.3225913][-4.2085075 -4.1776543 -4.1413946 -4.0910287 -4.0471768 -4.0103087 -3.9728162 -3.9442124 -3.9632063 -4.0188727 -4.0976605 -4.1787415 -4.2444096 -4.2927365 -4.311924][-4.22906 -4.203126 -4.1735764 -4.136651 -4.1134148 -4.0983052 -4.0793023 -4.0486512 -4.0386562 -4.0568571 -4.1039629 -4.1671648 -4.2299309 -4.2815 -4.3023891][-4.2400751 -4.2185025 -4.1985283 -4.178473 -4.1724973 -4.1719503 -4.1651387 -4.1375165 -4.1197939 -4.1209126 -4.1416969 -4.1806931 -4.2340727 -4.2813687 -4.3020592][-4.2447405 -4.2254725 -4.2103453 -4.1981354 -4.2019091 -4.211596 -4.2134442 -4.1945343 -4.1815524 -4.1815915 -4.1915236 -4.2132282 -4.2497835 -4.2842431 -4.29962][-4.2485614 -4.234755 -4.2208972 -4.2087412 -4.2104688 -4.2220278 -4.23322 -4.2307186 -4.2301707 -4.2338171 -4.2422462 -4.2570634 -4.2811627 -4.2996984 -4.3041759][-4.25072 -4.2423019 -4.2323313 -4.220696 -4.2187333 -4.2263556 -4.2362247 -4.2363338 -4.2381582 -4.2460961 -4.2593842 -4.2763534 -4.2954669 -4.307229 -4.3066726][-4.237772 -4.2320065 -4.2270641 -4.2215781 -4.2205353 -4.2238317 -4.2275877 -4.2246628 -4.2231288 -4.2293181 -4.2434373 -4.2631903 -4.2829738 -4.2941151 -4.2973256][-4.2322564 -4.2286577 -4.2251425 -4.2218318 -4.221446 -4.2228808 -4.2237496 -4.2227116 -4.2224016 -4.2266579 -4.2384758 -4.2556334 -4.2732034 -4.2852306 -4.2919817]]...]
INFO - root - 2017-12-07 22:15:49.007027: step 48210, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 77h:35m:09s remains)
INFO - root - 2017-12-07 22:15:58.668576: step 48220, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 78h:05m:14s remains)
INFO - root - 2017-12-07 22:16:08.174680: step 48230, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 75h:09m:15s remains)
INFO - root - 2017-12-07 22:16:17.855412: step 48240, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 78h:36m:47s remains)
INFO - root - 2017-12-07 22:16:27.434436: step 48250, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 73h:40m:55s remains)
INFO - root - 2017-12-07 22:16:37.186985: step 48260, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.010 sec/batch; 79h:43m:22s remains)
INFO - root - 2017-12-07 22:16:46.870027: step 48270, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.018 sec/batch; 80h:21m:49s remains)
INFO - root - 2017-12-07 22:16:56.744981: step 48280, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 77h:04m:38s remains)
INFO - root - 2017-12-07 22:17:06.367826: step 48290, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 74h:24m:54s remains)
INFO - root - 2017-12-07 22:17:16.073969: step 48300, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 75h:30m:57s remains)
2017-12-07 22:17:17.038431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2581697 -4.2665262 -4.2616725 -4.2388654 -4.2107282 -4.1926136 -4.1935868 -4.2024813 -4.20953 -4.221324 -4.2400618 -4.2472811 -4.2450924 -4.2465467 -4.2520242][-4.2690811 -4.2696958 -4.2643676 -4.250001 -4.2334185 -4.2215505 -4.2207241 -4.222816 -4.226181 -4.2375126 -4.24861 -4.244792 -4.2382431 -4.2391062 -4.2442288][-4.2799921 -4.2768207 -4.2753649 -4.2659869 -4.2564893 -4.247077 -4.2407146 -4.2366948 -4.237884 -4.2492266 -4.2521038 -4.2395387 -4.2334375 -4.2368512 -4.2413197][-4.2721133 -4.2674403 -4.2698812 -4.2699304 -4.2667603 -4.2554412 -4.2479987 -4.2398334 -4.2409592 -4.25234 -4.2466593 -4.2299848 -4.2266021 -4.2329383 -4.237186][-4.2404065 -4.2298503 -4.2327785 -4.239346 -4.2421479 -4.2310772 -4.2216816 -4.2082977 -4.208847 -4.2250018 -4.2229958 -4.2055488 -4.2081981 -4.2263446 -4.236022][-4.2006845 -4.1875038 -4.1888423 -4.1949668 -4.1997242 -4.1880093 -4.1697598 -4.1475291 -4.146008 -4.1762934 -4.1904569 -4.1796365 -4.1897855 -4.2195506 -4.235909][-4.1684995 -4.1473284 -4.1356711 -4.1344476 -4.1358733 -4.1246386 -4.0998631 -4.0666556 -4.0657263 -4.1197734 -4.1547685 -4.1544638 -4.1716847 -4.2075768 -4.2313881][-4.1530643 -4.117703 -4.0870504 -4.0733223 -4.0648646 -4.0456772 -4.0080991 -3.9571702 -3.9559557 -4.04201 -4.1049166 -4.1240487 -4.1475067 -4.1879015 -4.2225566][-4.1642675 -4.1241179 -4.0824866 -4.05459 -4.0313396 -4.0023103 -3.9551492 -3.8908615 -3.8817723 -3.9824579 -4.0686145 -4.1053329 -4.1310577 -4.1686816 -4.2109618][-4.1852193 -4.15463 -4.1209559 -4.0929656 -4.0735583 -4.0571651 -4.0302124 -3.9856069 -3.9756341 -4.0444789 -4.1126876 -4.1409569 -4.1555419 -4.178791 -4.2132459][-4.1987615 -4.1819649 -4.1653514 -4.1461344 -4.13765 -4.1394877 -4.1355071 -4.1125951 -4.1056328 -4.1477342 -4.1905484 -4.2021933 -4.20094 -4.2069345 -4.2265959][-4.1981444 -4.1928792 -4.1883254 -4.1789355 -4.1796579 -4.1918368 -4.2010517 -4.1914759 -4.1869292 -4.2127991 -4.239202 -4.2392097 -4.2291508 -4.2285256 -4.2432442][-4.1727839 -4.1735129 -4.1763678 -4.1799631 -4.1951618 -4.2191315 -4.2403474 -4.2383151 -4.23062 -4.2429876 -4.2538047 -4.2443309 -4.2345591 -4.2398314 -4.2582712][-4.1430974 -4.1400123 -4.1398292 -4.1498165 -4.1752162 -4.2071557 -4.236402 -4.2379212 -4.2287717 -4.2358704 -4.2404442 -4.2299914 -4.2282696 -4.2443128 -4.267386][-4.1219864 -4.1117592 -4.1025434 -4.1102223 -4.1353741 -4.1636963 -4.1885247 -4.1878004 -4.1791615 -4.1892152 -4.2015362 -4.204833 -4.2167912 -4.2409573 -4.2676382]]...]
INFO - root - 2017-12-07 22:17:26.717771: step 48310, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 71h:51m:01s remains)
INFO - root - 2017-12-07 22:17:36.430999: step 48320, loss = 2.10, batch loss = 2.04 (7.7 examples/sec; 1.034 sec/batch; 81h:38m:08s remains)
INFO - root - 2017-12-07 22:17:46.090592: step 48330, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 77h:08m:57s remains)
INFO - root - 2017-12-07 22:17:55.619705: step 48340, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 75h:23m:55s remains)
INFO - root - 2017-12-07 22:18:05.245000: step 48350, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 76h:41m:15s remains)
INFO - root - 2017-12-07 22:18:15.131910: step 48360, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 76h:59m:31s remains)
INFO - root - 2017-12-07 22:18:24.683274: step 48370, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 79h:14m:10s remains)
INFO - root - 2017-12-07 22:18:34.428267: step 48380, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 79h:32m:41s remains)
INFO - root - 2017-12-07 22:18:43.861057: step 48390, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.984 sec/batch; 77h:38m:55s remains)
INFO - root - 2017-12-07 22:18:53.731188: step 48400, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 72h:03m:20s remains)
2017-12-07 22:18:54.720597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3675346 -4.3585176 -4.34089 -4.3067894 -4.2396407 -4.1523747 -4.0672164 -4.0270286 -4.088294 -4.1831851 -4.2213073 -4.2164793 -4.195991 -4.19079 -4.1868725][-4.3698587 -4.3618965 -4.3458319 -4.3125968 -4.2446108 -4.1528406 -4.0575404 -4.0106492 -4.0763726 -4.1751757 -4.2173567 -4.2151446 -4.201447 -4.2019749 -4.1981483][-4.3707504 -4.3630686 -4.34852 -4.3163695 -4.2471981 -4.1501026 -4.0446825 -3.9912753 -4.0600524 -4.1625972 -4.2102189 -4.2142954 -4.2098193 -4.2165856 -4.2143273][-4.3716774 -4.3637176 -4.3499517 -4.31903 -4.25011 -4.1511135 -4.0384707 -3.9772525 -4.0486836 -4.1548595 -4.2051373 -4.2138643 -4.2163768 -4.2273626 -4.2255416][-4.3722434 -4.3640809 -4.3499942 -4.31974 -4.2515459 -4.1525412 -4.0343084 -3.9636235 -4.0375996 -4.1470857 -4.1999354 -4.2125068 -4.2175345 -4.2283897 -4.2272429][-4.3723431 -4.3642654 -4.3495359 -4.3191891 -4.2512431 -4.1515765 -4.02738 -3.9460869 -4.0219431 -4.1343722 -4.1883655 -4.2047563 -4.2130733 -4.2263393 -4.2287188][-4.3725429 -4.3643074 -4.3491287 -4.3183408 -4.2496171 -4.1479139 -4.0187087 -3.9307542 -4.0084825 -4.1228805 -4.1785059 -4.1994572 -4.2142806 -4.2326093 -4.2397227][-4.3726087 -4.3641062 -4.3484573 -4.3167815 -4.2474709 -4.1458015 -4.0199871 -3.9349856 -4.014245 -4.1275516 -4.1830206 -4.2061319 -4.2246652 -4.244513 -4.2519093][-4.3723269 -4.3637257 -4.3476233 -4.3160834 -4.24808 -4.150413 -4.0350003 -3.9612803 -4.0391707 -4.1478686 -4.2011652 -4.2224851 -4.2398281 -4.2573066 -4.2636614][-4.3714437 -4.3628712 -4.3462572 -4.3156619 -4.2500539 -4.1570544 -4.0530396 -3.9918523 -4.0663581 -4.171175 -4.2210584 -4.2382679 -4.2517767 -4.2653928 -4.2716703][-4.3702641 -4.3608551 -4.3437996 -4.3141046 -4.2516742 -4.1637344 -4.0704589 -4.0206881 -4.0928531 -4.1939421 -4.2391915 -4.2524166 -4.264071 -4.27647 -4.2828922][-4.3683648 -4.3580127 -4.3402457 -4.3111248 -4.2518935 -4.1700039 -4.0869493 -4.0481086 -4.11828 -4.2136207 -4.2534089 -4.2645111 -4.2762966 -4.2878475 -4.2939463][-4.3661594 -4.3546634 -4.3365397 -4.3074594 -4.251328 -4.1771412 -4.1039758 -4.0739856 -4.140233 -4.2258348 -4.2596993 -4.269671 -4.2816958 -4.29215 -4.2981339][-4.3644276 -4.3519 -4.3341141 -4.3053827 -4.2518492 -4.185616 -4.1237378 -4.1005573 -4.1569877 -4.228826 -4.2579765 -4.2692113 -4.2827744 -4.2926989 -4.2985606][-4.3629251 -4.3492904 -4.3320169 -4.3047948 -4.2558908 -4.198729 -4.1490359 -4.1310763 -4.1763291 -4.2339711 -4.2611074 -4.2763777 -4.2910695 -4.3002043 -4.3041286]]...]
INFO - root - 2017-12-07 22:19:04.407770: step 48410, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 76h:44m:41s remains)
INFO - root - 2017-12-07 22:19:13.968843: step 48420, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 76h:34m:04s remains)
INFO - root - 2017-12-07 22:19:23.714185: step 48430, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 75h:56m:31s remains)
INFO - root - 2017-12-07 22:19:33.442910: step 48440, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 75h:08m:42s remains)
INFO - root - 2017-12-07 22:19:43.088147: step 48450, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 77h:27m:05s remains)
INFO - root - 2017-12-07 22:19:52.621308: step 48460, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 73h:11m:14s remains)
INFO - root - 2017-12-07 22:20:02.203098: step 48470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 75h:24m:12s remains)
INFO - root - 2017-12-07 22:20:11.986402: step 48480, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 77h:40m:19s remains)
INFO - root - 2017-12-07 22:20:21.366194: step 48490, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 77h:24m:19s remains)
INFO - root - 2017-12-07 22:20:31.034685: step 48500, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 73h:56m:35s remains)
2017-12-07 22:20:32.053673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14653 -4.1697512 -4.1850533 -4.1794896 -4.1705394 -4.1602874 -4.1492624 -4.1472297 -4.1635866 -4.2027521 -4.2415824 -4.2473183 -4.2233515 -4.1830406 -4.1412449][-4.184514 -4.19465 -4.2016344 -4.1973691 -4.1879673 -4.1767368 -4.15854 -4.1423283 -4.150342 -4.1879506 -4.2288451 -4.2336035 -4.1998105 -4.1478291 -4.103981][-4.2151532 -4.22132 -4.2275476 -4.2229462 -4.2051854 -4.1833043 -4.1540737 -4.1223464 -4.1179447 -4.150485 -4.1921997 -4.2052789 -4.1712952 -4.1139569 -4.0740676][-4.2516017 -4.2562571 -4.2561731 -4.235764 -4.1976476 -4.148962 -4.0975895 -4.0640693 -4.0659313 -4.10658 -4.1568456 -4.1840649 -4.1644764 -4.1134815 -4.0836964][-4.2946348 -4.2996049 -4.2830696 -4.2279749 -4.1492844 -4.0644236 -3.9921029 -3.9670155 -3.9978504 -4.072154 -4.1417894 -4.1831646 -4.1790829 -4.1391382 -4.1153774][-4.3198214 -4.320385 -4.2739553 -4.1720104 -4.04249 -3.9225044 -3.8432996 -3.842504 -3.931535 -4.0598407 -4.1546855 -4.206183 -4.2129135 -4.1841164 -4.1593032][-4.3302989 -4.3208623 -4.2437253 -4.1077313 -3.9437528 -3.8038223 -3.7402091 -3.78549 -3.9177642 -4.0743384 -4.1801615 -4.2363186 -4.2496152 -4.2298269 -4.2058849][-4.3263359 -4.3129168 -4.233233 -4.0976682 -3.9339981 -3.7912121 -3.7428679 -3.8283565 -3.9748311 -4.1148887 -4.210319 -4.2648268 -4.2800183 -4.2634988 -4.2409329][-4.3086476 -4.2990842 -4.2435904 -4.1411481 -4.0103822 -3.88313 -3.8365011 -3.9306047 -4.0649195 -4.171864 -4.2442336 -4.2893624 -4.3026729 -4.28353 -4.2623138][-4.2945218 -4.2980418 -4.2778611 -4.2212167 -4.133481 -4.0384092 -3.9955823 -4.0669875 -4.1602993 -4.2246232 -4.2663317 -4.2965865 -4.3065929 -4.289422 -4.2743511][-4.2906084 -4.3071094 -4.308311 -4.2781873 -4.2162867 -4.1462507 -4.1161819 -4.1662192 -4.2204576 -4.24819 -4.2733536 -4.2934394 -4.3011642 -4.2907109 -4.2824988][-4.289803 -4.3153014 -4.321588 -4.2964935 -4.2443905 -4.1863527 -4.1658888 -4.2021518 -4.2376842 -4.2583566 -4.2834234 -4.2951674 -4.2923908 -4.2838864 -4.2780232][-4.2844443 -4.3171082 -4.3232532 -4.298717 -4.25559 -4.2117896 -4.1979179 -4.215436 -4.2440372 -4.2720065 -4.2995996 -4.2995834 -4.2818022 -4.2688761 -4.263206][-4.2660308 -4.3000817 -4.3068476 -4.2868319 -4.2563362 -4.2294679 -4.2224264 -4.2321711 -4.2538972 -4.2861881 -4.3094692 -4.2992578 -4.2747688 -4.2559772 -4.2366338][-4.2329574 -4.2639785 -4.276186 -4.2649217 -4.2456346 -4.2372217 -4.2366433 -4.2479949 -4.2676506 -4.2921114 -4.3043375 -4.288497 -4.2611694 -4.2330542 -4.20666]]...]
INFO - root - 2017-12-07 22:20:41.703431: step 48510, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 76h:33m:48s remains)
INFO - root - 2017-12-07 22:20:51.447459: step 48520, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.016 sec/batch; 80h:09m:20s remains)
INFO - root - 2017-12-07 22:21:01.024504: step 48530, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 72h:09m:56s remains)
INFO - root - 2017-12-07 22:21:10.503360: step 48540, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 72h:59m:07s remains)
INFO - root - 2017-12-07 22:21:20.306567: step 48550, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 78h:50m:56s remains)
INFO - root - 2017-12-07 22:21:29.989817: step 48560, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 76h:18m:00s remains)
INFO - root - 2017-12-07 22:21:39.579128: step 48570, loss = 2.12, batch loss = 2.06 (8.1 examples/sec; 0.986 sec/batch; 77h:44m:32s remains)
INFO - root - 2017-12-07 22:21:48.937069: step 48580, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 75h:10m:51s remains)
INFO - root - 2017-12-07 22:21:58.596227: step 48590, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 71h:54m:31s remains)
INFO - root - 2017-12-07 22:22:08.095434: step 48600, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 73h:44m:56s remains)
2017-12-07 22:22:09.110832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3554521 -4.3558993 -4.3524666 -4.3468385 -4.3421144 -4.3378167 -4.3318205 -4.3260627 -4.322103 -4.3270741 -4.3293309 -4.3308005 -4.3290238 -4.3170967 -4.3122907][-4.3466539 -4.3477163 -4.3454609 -4.3413448 -4.3388324 -4.3365321 -4.3322234 -4.3282809 -4.3240438 -4.3266039 -4.32868 -4.3284879 -4.324543 -4.3115096 -4.3046894][-4.337307 -4.3395281 -4.3395133 -4.3373661 -4.3362622 -4.3329444 -4.32612 -4.318665 -4.3077917 -4.3038044 -4.3038392 -4.3027878 -4.2997365 -4.2919388 -4.2866979][-4.330863 -4.3331742 -4.3327136 -4.3282175 -4.3227005 -4.3107567 -4.2925987 -4.2742071 -4.2508516 -4.2381248 -4.2388711 -4.2402625 -4.2440085 -4.25005 -4.2541742][-4.3205576 -4.3177686 -4.3090134 -4.2926359 -4.2723365 -4.2405462 -4.2030168 -4.1734996 -4.1409163 -4.1227779 -4.1335855 -4.1473031 -4.167202 -4.1933489 -4.2123322][-4.2862096 -4.2711153 -4.2444434 -4.2038341 -4.1605735 -4.1070666 -4.0525017 -4.0215764 -3.9905872 -3.9794397 -4.0146365 -4.05504 -4.0989552 -4.1468058 -4.1802564][-4.2203736 -4.183135 -4.1331925 -4.069396 -4.0133295 -3.9548442 -3.9048133 -3.8952267 -3.8862994 -3.8985224 -3.9600589 -4.023006 -4.0830269 -4.1383829 -4.1754904][-4.1478386 -4.09441 -4.0366492 -3.9782407 -3.9432709 -3.9097631 -3.8835185 -3.8964195 -3.9075618 -3.9368517 -4.004148 -4.06723 -4.1257915 -4.1739221 -4.2017293][-4.1212182 -4.0759497 -4.0381637 -4.0106959 -4.0129251 -4.0116076 -4.0042067 -4.0173569 -4.0271239 -4.0560856 -4.1106625 -4.1570082 -4.2013917 -4.2345333 -4.2488179][-4.1579838 -4.1343455 -4.1226029 -4.1211996 -4.1417365 -4.1516356 -4.149025 -4.155457 -4.1590672 -4.1810617 -4.2215686 -4.2534533 -4.2823105 -4.3002234 -4.3021097][-4.2233267 -4.2166796 -4.2190423 -4.2256947 -4.2437587 -4.2508216 -4.2460704 -4.2468672 -4.2479897 -4.264586 -4.2935796 -4.3162112 -4.3355365 -4.3443317 -4.3382769][-4.2661867 -4.2643175 -4.268959 -4.2729325 -4.28424 -4.2893553 -4.2848086 -4.285193 -4.2892275 -4.3039675 -4.3256316 -4.3430943 -4.3569722 -4.360745 -4.3522778][-4.2666779 -4.2637568 -4.2697363 -4.2681751 -4.2739706 -4.2810006 -4.2779489 -4.2803135 -4.2904263 -4.307055 -4.3271036 -4.3431072 -4.3551397 -4.3578386 -4.3502226][-4.2192721 -4.2144866 -4.2239528 -4.2231126 -4.2322474 -4.247479 -4.25003 -4.2575159 -4.2757483 -4.29887 -4.3225641 -4.3401065 -4.3515983 -4.35235 -4.3449855][-4.1502376 -4.1388488 -4.1495476 -4.1539245 -4.1737504 -4.2030964 -4.2177854 -4.2343125 -4.2595487 -4.2878151 -4.3143291 -4.3336673 -4.3449769 -4.344243 -4.3368769]]...]
INFO - root - 2017-12-07 22:22:18.816455: step 48610, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 74h:45m:59s remains)
INFO - root - 2017-12-07 22:22:28.352479: step 48620, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 76h:01m:35s remains)
INFO - root - 2017-12-07 22:22:37.937307: step 48630, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 76h:21m:53s remains)
INFO - root - 2017-12-07 22:22:47.627325: step 48640, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.919 sec/batch; 72h:27m:16s remains)
INFO - root - 2017-12-07 22:22:57.084146: step 48650, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 67h:05m:33s remains)
INFO - root - 2017-12-07 22:23:06.762565: step 48660, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 72h:07m:07s remains)
INFO - root - 2017-12-07 22:23:16.563230: step 48670, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 75h:20m:04s remains)
INFO - root - 2017-12-07 22:23:26.099534: step 48680, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 77h:49m:54s remains)
INFO - root - 2017-12-07 22:23:35.858379: step 48690, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 75h:12m:14s remains)
INFO - root - 2017-12-07 22:23:45.515124: step 48700, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.919 sec/batch; 72h:26m:29s remains)
2017-12-07 22:23:46.498786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2205896 -4.2249932 -4.2335324 -4.2429075 -4.2468076 -4.2444005 -4.2389874 -4.2335396 -4.2315893 -4.2297597 -4.2245307 -4.2084513 -4.1864734 -4.1733108 -4.1668477][-4.2029309 -4.2071533 -4.2196536 -4.2344322 -4.2376833 -4.231226 -4.225935 -4.2214136 -4.2222872 -4.2234421 -4.2232475 -4.2089429 -4.1820617 -4.1601362 -4.1443076][-4.1879082 -4.1920009 -4.20631 -4.2252574 -4.2284079 -4.2193274 -4.2134442 -4.2130284 -4.2188349 -4.2234993 -4.2318668 -4.2223959 -4.1955433 -4.1657524 -4.137301][-4.1618147 -4.1674566 -4.1810822 -4.197865 -4.1986957 -4.1877408 -4.1792641 -4.1825271 -4.1969042 -4.211072 -4.2297678 -4.2285833 -4.2036285 -4.1677241 -4.1317163][-4.145102 -4.15356 -4.1623454 -4.1682882 -4.1588922 -4.1403918 -4.1239676 -4.1320777 -4.1587291 -4.1900654 -4.2206106 -4.2267265 -4.204987 -4.1668463 -4.1300926][-4.1273265 -4.1381726 -4.1410213 -4.1348448 -4.1139255 -4.088244 -4.0652947 -4.0734577 -4.106009 -4.1529851 -4.1991992 -4.2162156 -4.2029743 -4.1716928 -4.139483][-4.1138496 -4.1248093 -4.1224685 -4.105772 -4.0773911 -4.0417504 -4.0016842 -3.9895427 -4.00868 -4.0662889 -4.1384778 -4.1803932 -4.1839628 -4.1682181 -4.1473536][-4.1122508 -4.1273413 -4.1244035 -4.1051946 -4.0718565 -4.0250669 -3.9583883 -3.9039531 -3.8801589 -3.9233887 -4.0233006 -4.1007929 -4.1296062 -4.1338997 -4.1311812][-4.1010647 -4.1267381 -4.1370711 -4.1286979 -4.0980544 -4.048377 -3.9753482 -3.8983536 -3.8377442 -3.8369336 -3.9321 -4.0286393 -4.0711603 -4.0849085 -4.0970969][-4.0841093 -4.1145458 -4.14066 -4.1537671 -4.14423 -4.1133657 -4.0630689 -4.0069642 -3.9477856 -3.909955 -3.9529347 -4.0239 -4.055975 -4.062438 -4.0747118][-4.0637703 -4.0960178 -4.1342158 -4.17136 -4.1938572 -4.1912551 -4.1690989 -4.141614 -4.1018124 -4.0557175 -4.0493522 -4.073319 -4.0789471 -4.0664721 -4.068223][-4.0565557 -4.0866446 -4.1266403 -4.1708126 -4.2094893 -4.2284765 -4.2270756 -4.2188334 -4.1970763 -4.1616397 -4.1369972 -4.1303906 -4.1152058 -4.0846281 -4.0738831][-4.08065 -4.1006041 -4.126461 -4.1537051 -4.1890154 -4.2210422 -4.2366633 -4.2444615 -4.2408214 -4.2229586 -4.1973786 -4.1768069 -4.1493654 -4.1072922 -4.08407][-4.1099653 -4.1126075 -4.1159048 -4.1189818 -4.1430373 -4.1862483 -4.2214174 -4.2428188 -4.2534256 -4.2504773 -4.2315321 -4.2088356 -4.1764116 -4.1319523 -4.0997944][-4.1374049 -4.12493 -4.1064849 -4.0870762 -4.0996547 -4.1483436 -4.1969938 -4.2262874 -4.2463174 -4.2542806 -4.243278 -4.2243557 -4.19103 -4.1447096 -4.10459]]...]
INFO - root - 2017-12-07 22:23:56.012731: step 48710, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 75h:52m:23s remains)
INFO - root - 2017-12-07 22:24:05.815393: step 48720, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 78h:30m:12s remains)
INFO - root - 2017-12-07 22:24:15.363712: step 48730, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.959 sec/batch; 75h:34m:04s remains)
INFO - root - 2017-12-07 22:24:25.040084: step 48740, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 76h:07m:45s remains)
INFO - root - 2017-12-07 22:24:34.736510: step 48750, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 78h:56m:58s remains)
INFO - root - 2017-12-07 22:24:44.576041: step 48760, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 77h:05m:19s remains)
INFO - root - 2017-12-07 22:24:54.194598: step 48770, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 75h:10m:08s remains)
INFO - root - 2017-12-07 22:25:03.756443: step 48780, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 76h:01m:33s remains)
INFO - root - 2017-12-07 22:25:13.359818: step 48790, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 79h:49m:00s remains)
INFO - root - 2017-12-07 22:25:23.089117: step 48800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 74h:58m:26s remains)
2017-12-07 22:25:24.040192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2991896 -4.2963333 -4.2960758 -4.2964516 -4.3018909 -4.3092122 -4.3152361 -4.3166714 -4.3204408 -4.3237672 -4.3258491 -4.3252563 -4.3238173 -4.3182425 -4.3104792][-4.2898993 -4.2863269 -4.2865729 -4.2862725 -4.2916241 -4.2985387 -4.3069148 -4.3110719 -4.3205357 -4.3263688 -4.3303018 -4.3312221 -4.3291874 -4.3195882 -4.3065991][-4.2825112 -4.282691 -4.2858629 -4.2866778 -4.2875843 -4.2879014 -4.2931695 -4.2939291 -4.30248 -4.3104777 -4.3177433 -4.3245916 -4.3242011 -4.3114481 -4.2947922][-4.2755075 -4.2787175 -4.28494 -4.2844009 -4.2748623 -4.264914 -4.2604208 -4.2498107 -4.2546868 -4.2678432 -4.28119 -4.2950659 -4.2991219 -4.2868004 -4.2657676][-4.2720547 -4.271945 -4.2744122 -4.2665834 -4.2445726 -4.2209792 -4.1950068 -4.1691051 -4.1782274 -4.2092752 -4.2410216 -4.2666459 -4.2783413 -4.2689738 -4.2398129][-4.2784324 -4.2700768 -4.2585273 -4.2349634 -4.1957216 -4.1506033 -4.0913539 -4.0501952 -4.0851245 -4.1551523 -4.2169228 -4.2503076 -4.2629561 -4.2537689 -4.2175336][-4.2886291 -4.2684913 -4.238277 -4.1939449 -4.1314163 -4.0505819 -3.9415612 -3.8875117 -3.9774175 -4.0995936 -4.1908436 -4.2240505 -4.2307744 -4.2180104 -4.1786995][-4.2950826 -4.2633109 -4.213161 -4.1495585 -4.0663319 -3.9548833 -3.7986326 -3.7396574 -3.8925116 -4.0562854 -4.1601276 -4.1891966 -4.1893744 -4.1749783 -4.1398816][-4.2903829 -4.248354 -4.1887302 -4.1258774 -4.0521984 -3.9546897 -3.8284392 -3.8002913 -3.9399643 -4.0823011 -4.1674633 -4.1822209 -4.1644073 -4.1408544 -4.1097622][-4.2772641 -4.2319841 -4.1756005 -4.1324825 -4.0901918 -4.031477 -3.97063 -3.971832 -4.0545053 -4.1363573 -4.1816783 -4.1731591 -4.1401863 -4.1159716 -4.0928206][-4.2604275 -4.2216816 -4.1777387 -4.1557465 -4.1453962 -4.1226311 -4.103775 -4.1132154 -4.1495385 -4.177959 -4.1858754 -4.1616635 -4.1278057 -4.1117272 -4.0978789][-4.2537522 -4.2302103 -4.206037 -4.2034421 -4.2099438 -4.2030792 -4.2003288 -4.2132215 -4.2244925 -4.2235603 -4.2119284 -4.1859503 -4.1634178 -4.1543007 -4.1388865][-4.2660594 -4.2572093 -4.2497306 -4.2549758 -4.2645578 -4.264019 -4.2676158 -4.2774482 -4.2775664 -4.2699037 -4.2591934 -4.2397828 -4.2282505 -4.2185574 -4.1997271][-4.2852154 -4.2831988 -4.2862797 -4.2964849 -4.305057 -4.3073282 -4.3126941 -4.3173881 -4.3148823 -4.3116221 -4.3077183 -4.2923603 -4.2818265 -4.2716236 -4.257473][-4.2997322 -4.2993665 -4.3064866 -4.318387 -4.3274412 -4.3321762 -4.3365021 -4.337831 -4.3360615 -4.3369141 -4.3363404 -4.3241444 -4.3139424 -4.3065224 -4.2974448]]...]
INFO - root - 2017-12-07 22:25:33.732969: step 48810, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 77h:12m:17s remains)
INFO - root - 2017-12-07 22:25:43.262463: step 48820, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 75h:21m:43s remains)
INFO - root - 2017-12-07 22:25:52.851206: step 48830, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 77h:15m:21s remains)
INFO - root - 2017-12-07 22:26:02.538040: step 48840, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 75h:11m:20s remains)
INFO - root - 2017-12-07 22:26:12.366334: step 48850, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 76h:34m:28s remains)
INFO - root - 2017-12-07 22:26:21.968151: step 48860, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 74h:57m:34s remains)
INFO - root - 2017-12-07 22:26:31.496949: step 48870, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 72h:37m:56s remains)
INFO - root - 2017-12-07 22:26:41.019893: step 48880, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 73h:02m:43s remains)
INFO - root - 2017-12-07 22:26:50.521824: step 48890, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 75h:28m:38s remains)
INFO - root - 2017-12-07 22:27:00.308470: step 48900, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 73h:06m:58s remains)
2017-12-07 22:27:01.373118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2669053 -4.2681804 -4.2702017 -4.2721548 -4.2680345 -4.2485881 -4.21042 -4.1956911 -4.2116919 -4.2181768 -4.227344 -4.2438817 -4.257699 -4.2748327 -4.2976003][-4.268889 -4.269506 -4.2689967 -4.2664642 -4.2578421 -4.2350211 -4.1945567 -4.178968 -4.1983533 -4.2105331 -4.2254434 -4.2450762 -4.2600274 -4.2771964 -4.2994118][-4.2674961 -4.2640977 -4.2560625 -4.2438974 -4.2276912 -4.2020712 -4.1614881 -4.1478529 -4.1740832 -4.1975641 -4.2232819 -4.2484169 -4.2655096 -4.2818556 -4.3016753][-4.2568808 -4.24415 -4.2224364 -4.1966996 -4.1731167 -4.1484489 -4.1127949 -4.1042318 -4.1396723 -4.1779103 -4.2181315 -4.2518015 -4.2722559 -4.2881541 -4.3052831][-4.2356596 -4.2096758 -4.1717777 -4.1333189 -4.1061187 -4.0883784 -4.062881 -4.06002 -4.10182 -4.1525865 -4.207356 -4.2512784 -4.2763133 -4.2936177 -4.3093367][-4.208334 -4.169488 -4.1196456 -4.075007 -4.0488 -4.0372534 -4.018734 -4.0174289 -4.0620046 -4.1224108 -4.1900854 -4.2443056 -4.274663 -4.2951365 -4.3118567][-4.1840091 -4.1395235 -4.0882726 -4.044724 -4.0149589 -3.9953744 -3.9719489 -3.9702981 -4.0214038 -4.0939779 -4.1726069 -4.2337484 -4.2672305 -4.2915497 -4.3114858][-4.1701241 -4.1279593 -4.0831614 -4.0413222 -3.99941 -3.9590168 -3.9244285 -3.9280529 -3.9958589 -4.0845418 -4.1679921 -4.2276154 -4.2595272 -4.2854247 -4.3086123][-4.1675239 -4.1302948 -4.0928187 -4.050931 -3.9963512 -3.9400218 -3.9000721 -3.9114625 -3.9942369 -4.09214 -4.1730714 -4.2259536 -4.2544494 -4.2798071 -4.3049226][-4.1768 -4.1455784 -4.1151938 -4.0757275 -4.01782 -3.9582005 -3.9178925 -3.9317064 -4.0140204 -4.1070623 -4.180933 -4.2267337 -4.2515821 -4.2758455 -4.3018823][-4.1944733 -4.1691246 -4.1453853 -4.1109662 -4.0581732 -4.0055656 -3.9689071 -3.9780867 -4.046896 -4.1258268 -4.1896181 -4.2297912 -4.2514458 -4.2741432 -4.3002944][-4.2119904 -4.1904092 -4.1714864 -4.1449161 -4.104784 -4.0657191 -4.0337119 -4.0364623 -4.0878406 -4.1478782 -4.1991692 -4.2342091 -4.2531939 -4.274178 -4.30001][-4.2260537 -4.2070961 -4.1916447 -4.1739764 -4.1501188 -4.125504 -4.0969849 -4.0935173 -4.13037 -4.17184 -4.2101169 -4.2386494 -4.2552953 -4.2748852 -4.3001084][-4.2368774 -4.2215147 -4.2102151 -4.2017193 -4.1911116 -4.17583 -4.148035 -4.1403656 -4.1670933 -4.1930461 -4.2189794 -4.2417183 -4.25649 -4.2751608 -4.2998242][-4.2466173 -4.2356248 -4.2282882 -4.2259159 -4.2220883 -4.2094684 -4.1807232 -4.1709843 -4.1917849 -4.2075319 -4.2241874 -4.2429018 -4.2567024 -4.2749219 -4.2992115]]...]
INFO - root - 2017-12-07 22:27:10.820617: step 48910, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.957 sec/batch; 75h:22m:42s remains)
INFO - root - 2017-12-07 22:27:20.314856: step 48920, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.945 sec/batch; 74h:26m:41s remains)
INFO - root - 2017-12-07 22:27:29.893968: step 48930, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 74h:13m:06s remains)
INFO - root - 2017-12-07 22:27:39.455355: step 48940, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.002 sec/batch; 78h:53m:43s remains)
INFO - root - 2017-12-07 22:27:49.095050: step 48950, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 77h:19m:50s remains)
INFO - root - 2017-12-07 22:27:58.667667: step 48960, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 73h:47m:42s remains)
INFO - root - 2017-12-07 22:28:08.081606: step 48970, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 79h:03m:17s remains)
INFO - root - 2017-12-07 22:28:17.727472: step 48980, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 71h:03m:51s remains)
INFO - root - 2017-12-07 22:28:26.972425: step 48990, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 72h:31m:50s remains)
INFO - root - 2017-12-07 22:28:36.636685: step 49000, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.982 sec/batch; 77h:19m:38s remains)
2017-12-07 22:28:37.581629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.306932 -4.2945657 -4.2753177 -4.2660384 -4.2512584 -4.2114844 -4.169312 -4.1466417 -4.1427026 -4.1681895 -4.1928883 -4.2087584 -4.2162404 -4.2135191 -4.2115784][-4.3046851 -4.2930937 -4.2743707 -4.2661052 -4.2506771 -4.213532 -4.1829014 -4.1670713 -4.1541033 -4.1684313 -4.1918855 -4.2078209 -4.2151575 -4.2103143 -4.2017341][-4.3005767 -4.2909117 -4.2775326 -4.2723541 -4.2588768 -4.2252231 -4.197506 -4.1857524 -4.16664 -4.1719828 -4.188571 -4.1993017 -4.2056751 -4.1972551 -4.180728][-4.2936811 -4.2838674 -4.2720113 -4.2628703 -4.2468967 -4.2127662 -4.1886649 -4.1823273 -4.1603222 -4.1627727 -4.1780462 -4.1903806 -4.2014933 -4.1864309 -4.1534667][-4.2829447 -4.2676954 -4.249475 -4.2312651 -4.2085505 -4.1755576 -4.1656694 -4.1712394 -4.1485038 -4.1483707 -4.1696582 -4.1927438 -4.2114019 -4.1910763 -4.13674][-4.2738023 -4.2525725 -4.2277975 -4.1999788 -4.1651812 -4.1272869 -4.1317897 -4.1493869 -4.1247926 -4.12465 -4.1608167 -4.1985207 -4.2237792 -4.2038159 -4.1394444][-4.2687297 -4.2453208 -4.2204628 -4.1863065 -4.1345639 -4.0802445 -4.0858188 -4.102715 -4.0720954 -4.0814748 -4.1390615 -4.1911707 -4.2246723 -4.2156663 -4.1587272][-4.268764 -4.2499189 -4.2306228 -4.1964326 -4.1346135 -4.0682044 -4.0597172 -4.0654063 -4.0311675 -4.0487738 -4.1175342 -4.1733561 -4.2124481 -4.217452 -4.1782575][-4.2740216 -4.2628756 -4.2556973 -4.2338328 -4.1854811 -4.1298718 -4.1072888 -4.0969043 -4.0640664 -4.0787668 -4.1312389 -4.1712241 -4.2056551 -4.2181916 -4.1965303][-4.2784734 -4.274435 -4.2811747 -4.2757072 -4.24526 -4.2037377 -4.1775727 -4.1629176 -4.1378794 -4.1471491 -4.17959 -4.2032151 -4.2286124 -4.2415709 -4.2293291][-4.2756739 -4.2747936 -4.2877312 -4.2923918 -4.27688 -4.2520013 -4.2317915 -4.2185488 -4.1993546 -4.2025857 -4.221899 -4.2346392 -4.2516575 -4.2608528 -4.2581577][-4.2648745 -4.2664051 -4.28044 -4.2905478 -4.2861905 -4.2741718 -4.2588663 -4.2456155 -4.2273631 -4.2260184 -4.2366076 -4.2398338 -4.2449064 -4.2534928 -4.2598572][-4.2582722 -4.2611508 -4.2705073 -4.278224 -4.2760267 -4.2668128 -4.2555451 -4.2485938 -4.2364092 -4.2317557 -4.2348962 -4.2297497 -4.2243962 -4.2321968 -4.2448816][-4.2576957 -4.2598162 -4.2656345 -4.2711339 -4.2672834 -4.2553215 -4.2461877 -4.245492 -4.2388306 -4.2341 -4.2359204 -4.2277794 -4.21615 -4.2216673 -4.2358747][-4.2640114 -4.2621412 -4.2643886 -4.269423 -4.2677031 -4.2564726 -4.2498732 -4.2529783 -4.2484117 -4.2445717 -4.2481771 -4.2432785 -4.2324762 -4.2332473 -4.2390089]]...]
INFO - root - 2017-12-07 22:28:47.313941: step 49010, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 74h:46m:39s remains)
INFO - root - 2017-12-07 22:28:56.936542: step 49020, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 78h:34m:52s remains)
INFO - root - 2017-12-07 22:29:06.689026: step 49030, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.018 sec/batch; 80h:11m:47s remains)
INFO - root - 2017-12-07 22:29:16.403005: step 49040, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 77h:51m:26s remains)
INFO - root - 2017-12-07 22:29:25.926512: step 49050, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.796 sec/batch; 62h:42m:37s remains)
INFO - root - 2017-12-07 22:29:35.424244: step 49060, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 72h:13m:53s remains)
INFO - root - 2017-12-07 22:29:44.992368: step 49070, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 77h:54m:41s remains)
INFO - root - 2017-12-07 22:29:54.692675: step 49080, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 72h:17m:25s remains)
INFO - root - 2017-12-07 22:30:04.442561: step 49090, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 74h:01m:50s remains)
INFO - root - 2017-12-07 22:30:14.099624: step 49100, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 74h:36m:18s remains)
2017-12-07 22:30:15.134181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2910967 -4.2977934 -4.3062434 -4.3125067 -4.3178015 -4.31376 -4.2958717 -4.273169 -4.2516904 -4.249764 -4.2589722 -4.2525635 -4.230557 -4.207612 -4.2022719][-4.2832012 -4.2895 -4.2978129 -4.30383 -4.3077235 -4.3002806 -4.2762537 -4.2525659 -4.2342129 -4.23777 -4.2517672 -4.2495303 -4.2267075 -4.1997128 -4.1942258][-4.2789979 -4.2844672 -4.293561 -4.2997918 -4.2993131 -4.2841048 -4.2518888 -4.22513 -4.21099 -4.2214389 -4.2431393 -4.2503471 -4.2322149 -4.2030163 -4.1980152][-4.2714906 -4.2721443 -4.2832904 -4.2955122 -4.2947059 -4.2759032 -4.236681 -4.2018847 -4.1823921 -4.1924658 -4.220789 -4.2380672 -4.2290053 -4.2064915 -4.2052937][-4.2557373 -4.2450342 -4.2523727 -4.2718573 -4.2761784 -4.2589593 -4.2177687 -4.1722279 -4.1361794 -4.1384678 -4.1721458 -4.2067757 -4.2189445 -4.2128644 -4.2172389][-4.2179661 -4.1994686 -4.2016077 -4.2208738 -4.2305737 -4.22391 -4.1855435 -4.1253176 -4.0611987 -4.0518775 -4.0949731 -4.1530895 -4.1923838 -4.2103429 -4.2239723][-4.1655407 -4.1421552 -4.1370654 -4.150887 -4.1657085 -4.1721458 -4.1388016 -4.0616021 -3.9671121 -3.9472823 -4.0097804 -4.0966148 -4.1638651 -4.1963587 -4.2130771][-4.1301074 -4.107234 -4.1033592 -4.1195569 -4.1413941 -4.1588445 -4.1313443 -4.04599 -3.9344268 -3.9088094 -3.9854228 -4.0846882 -4.1537209 -4.1815095 -4.1955876][-4.1378117 -4.1200495 -4.1133981 -4.12494 -4.1496663 -4.1711154 -4.15267 -4.0758443 -3.9717655 -3.9530275 -4.0311069 -4.1169496 -4.1636753 -4.1745167 -4.1831694][-4.174664 -4.1630797 -4.1517029 -4.1491694 -4.1684146 -4.185966 -4.1726279 -4.1158261 -4.0381846 -4.0335493 -4.1006784 -4.1621933 -4.1848836 -4.1792479 -4.1817217][-4.2155175 -4.2148457 -4.2047014 -4.1932435 -4.2018838 -4.2102218 -4.202095 -4.1640515 -4.1105695 -4.1117325 -4.1623549 -4.2010665 -4.2068057 -4.1930413 -4.1943359][-4.2567639 -4.2613726 -4.256258 -4.2439151 -4.2406325 -4.2399449 -4.2357244 -4.2113972 -4.1727443 -4.1716475 -4.2048311 -4.2293835 -4.2288132 -4.215168 -4.2195907][-4.2870164 -4.2942972 -4.2991915 -4.2916031 -4.2798562 -4.2724509 -4.2708049 -4.2577009 -4.2316213 -4.2269735 -4.2446675 -4.2578325 -4.2560859 -4.2480083 -4.2550316][-4.3105445 -4.3162947 -4.321775 -4.3186283 -4.3085074 -4.3027005 -4.303782 -4.2982287 -4.2828612 -4.2769222 -4.2826371 -4.2875557 -4.2860684 -4.2833204 -4.2896752][-4.3179741 -4.3216934 -4.3270788 -4.3265028 -4.3217144 -4.3196106 -4.3223863 -4.3205285 -4.3125696 -4.3077741 -4.3083477 -4.3111949 -4.3108912 -4.3120227 -4.3167586]]...]
INFO - root - 2017-12-07 22:30:24.822633: step 49110, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 73h:55m:39s remains)
INFO - root - 2017-12-07 22:30:34.376806: step 49120, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 73h:17m:26s remains)
INFO - root - 2017-12-07 22:30:44.037794: step 49130, loss = 2.11, batch loss = 2.05 (7.7 examples/sec; 1.034 sec/batch; 81h:23m:24s remains)
INFO - root - 2017-12-07 22:30:53.602807: step 49140, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 73h:12m:45s remains)
INFO - root - 2017-12-07 22:31:03.249843: step 49150, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 76h:01m:39s remains)
INFO - root - 2017-12-07 22:31:12.650585: step 49160, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.952 sec/batch; 74h:54m:23s remains)
INFO - root - 2017-12-07 22:31:22.163814: step 49170, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 74h:47m:43s remains)
INFO - root - 2017-12-07 22:31:31.918032: step 49180, loss = 2.08, batch loss = 2.03 (7.7 examples/sec; 1.038 sec/batch; 81h:42m:13s remains)
INFO - root - 2017-12-07 22:31:41.661236: step 49190, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.961 sec/batch; 75h:39m:02s remains)
INFO - root - 2017-12-07 22:31:51.158758: step 49200, loss = 2.12, batch loss = 2.06 (8.2 examples/sec; 0.980 sec/batch; 77h:05m:34s remains)
2017-12-07 22:31:52.134496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2247128 -4.2338676 -4.2488384 -4.2642212 -4.2776575 -4.2848539 -4.2856 -4.2827511 -4.2733221 -4.2582517 -4.2471762 -4.2473817 -4.2635393 -4.2834721 -4.3006592][-4.2161908 -4.2168584 -4.2251248 -4.2373447 -4.25495 -4.2670631 -4.2703567 -4.2680154 -4.2591658 -4.2448139 -4.23364 -4.23395 -4.2506256 -4.2695322 -4.2857056][-4.2002635 -4.189744 -4.1869206 -4.1966195 -4.2183237 -4.2374353 -4.2417488 -4.2395072 -4.2322607 -4.2214618 -4.2138081 -4.2180119 -4.2368069 -4.2547011 -4.270411][-4.1945505 -4.1759558 -4.1657619 -4.1707516 -4.1937308 -4.2153487 -4.2158284 -4.2077746 -4.199851 -4.1934738 -4.1902208 -4.197979 -4.2203245 -4.2419834 -4.2611327][-4.1953797 -4.1747379 -4.1642909 -4.171349 -4.1936154 -4.2069836 -4.1929827 -4.1758361 -4.1709948 -4.1731138 -4.1754684 -4.18378 -4.2064195 -4.2332091 -4.2578106][-4.1683364 -4.1492276 -4.1459179 -4.1593933 -4.1779137 -4.1766624 -4.1497793 -4.130084 -4.1368337 -4.1569309 -4.168016 -4.17805 -4.2000175 -4.2290082 -4.2571073][-4.1247168 -4.1137242 -4.1143775 -4.1233277 -4.1259117 -4.1020169 -4.0642066 -4.0528159 -4.0822806 -4.1275964 -4.1511474 -4.1661944 -4.1922145 -4.2257051 -4.2579021][-4.1128106 -4.10777 -4.1049032 -4.0957818 -4.0698981 -4.0142536 -3.9627044 -3.9647725 -4.0213461 -4.0862007 -4.1218004 -4.1469483 -4.1847463 -4.2267194 -4.2611241][-4.1463408 -4.1424828 -4.1311345 -4.1004314 -4.0492806 -3.9731359 -3.9202662 -3.939369 -4.0103626 -4.0752482 -4.1115203 -4.1438918 -4.1904106 -4.2378607 -4.2730503][-4.1851997 -4.1778135 -4.1566396 -4.1137385 -4.0570617 -3.9895694 -3.958323 -3.9929245 -4.0565262 -4.1013618 -4.1252103 -4.1545229 -4.2023239 -4.2500243 -4.2822685][-4.1927428 -4.1757751 -4.145318 -4.1047053 -4.0642571 -4.0269146 -4.0233645 -4.06186 -4.1075006 -4.1307664 -4.1398973 -4.1597219 -4.2030697 -4.247036 -4.2761164][-4.1799307 -4.1528683 -4.1223722 -4.0965786 -4.0793486 -4.0715775 -4.085259 -4.1168828 -4.1446524 -4.1515937 -4.1521029 -4.1644926 -4.2011976 -4.2400055 -4.266778][-4.179904 -4.1519923 -4.1248174 -4.1091738 -4.1080484 -4.119555 -4.1403542 -4.1625686 -4.1761131 -4.1760206 -4.1770649 -4.1887007 -4.2204375 -4.2540054 -4.2760944][-4.2117572 -4.1924276 -4.171092 -4.1612029 -4.1693006 -4.1895714 -4.2108521 -4.2222085 -4.2242455 -4.2195096 -4.2214 -4.2349253 -4.260962 -4.2864456 -4.3004327][-4.2663379 -4.2592926 -4.2439613 -4.2380772 -4.2484031 -4.2666249 -4.2807608 -4.2823367 -4.27559 -4.2668934 -4.2694817 -4.2839503 -4.3019943 -4.3160062 -4.3227935]]...]
INFO - root - 2017-12-07 22:32:01.991888: step 49210, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.036 sec/batch; 81h:31m:38s remains)
INFO - root - 2017-12-07 22:32:11.550519: step 49220, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.969 sec/batch; 76h:16m:20s remains)
INFO - root - 2017-12-07 22:32:21.124912: step 49230, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 74h:41m:41s remains)
INFO - root - 2017-12-07 22:32:30.708117: step 49240, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.947 sec/batch; 74h:28m:59s remains)
INFO - root - 2017-12-07 22:32:40.325790: step 49250, loss = 2.11, batch loss = 2.06 (8.3 examples/sec; 0.967 sec/batch; 76h:04m:42s remains)
INFO - root - 2017-12-07 22:32:50.018298: step 49260, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 69h:20m:53s remains)
INFO - root - 2017-12-07 22:32:59.572336: step 49270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 74h:25m:46s remains)
INFO - root - 2017-12-07 22:33:09.223263: step 49280, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 76h:53m:37s remains)
INFO - root - 2017-12-07 22:33:18.703038: step 49290, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 77h:26m:52s remains)
INFO - root - 2017-12-07 22:33:28.390290: step 49300, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 77h:50m:31s remains)
2017-12-07 22:33:29.236645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1795864 -4.1550426 -4.161634 -4.2112694 -4.2641888 -4.2895441 -4.2891684 -4.2822909 -4.2804065 -4.2807107 -4.2714267 -4.2467427 -4.2015595 -4.1349659 -4.0446453][-4.1755209 -4.1600347 -4.17993 -4.2305851 -4.2710795 -4.2829113 -4.2728281 -4.2551141 -4.24975 -4.2603483 -4.260603 -4.2389851 -4.191617 -4.1093383 -3.9931307][-4.1669216 -4.1587124 -4.1872172 -4.2310896 -4.2551928 -4.252192 -4.2287769 -4.199615 -4.19442 -4.227242 -4.2493582 -4.2402892 -4.1993465 -4.1206765 -4.0067039][-4.1761465 -4.1681509 -4.1917005 -4.216126 -4.2170134 -4.1946878 -4.1554623 -4.1196032 -4.1220937 -4.1847944 -4.2346773 -4.2444372 -4.2185755 -4.1625676 -4.0813756][-4.2087274 -4.1967497 -4.2029357 -4.1971989 -4.1675115 -4.1176157 -4.0593109 -4.018415 -4.0402188 -4.1397071 -4.2181396 -4.2507625 -4.2441864 -4.2158613 -4.1727653][-4.2538457 -4.2372966 -4.2239442 -4.1901584 -4.1313252 -4.0537605 -3.9716878 -3.9171247 -3.9529405 -4.0808587 -4.1820488 -4.2401237 -4.2567649 -4.2547984 -4.2422442][-4.2945213 -4.28047 -4.2609258 -4.2185431 -4.1482344 -4.0567613 -3.955965 -3.8789029 -3.9006605 -4.0193787 -4.1183538 -4.1905851 -4.2328825 -4.2577109 -4.2724748][-4.3183784 -4.3154826 -4.3036137 -4.2702374 -4.207674 -4.1223679 -4.0202723 -3.9317343 -3.9198802 -3.992913 -4.0668945 -4.1375093 -4.1964259 -4.2417307 -4.2714763][-4.3229742 -4.3295579 -4.3286128 -4.3110933 -4.2652879 -4.1956415 -4.1035757 -4.0191145 -3.9819949 -4.0032649 -4.0418234 -4.1040263 -4.1705079 -4.2267246 -4.2618251][-4.3041115 -4.3130746 -4.3206306 -4.3173275 -4.2888122 -4.236239 -4.1578803 -4.0827236 -4.0391908 -4.0355134 -4.0542245 -4.1103272 -4.1752229 -4.2281361 -4.2583003][-4.2803211 -4.2849364 -4.2921133 -4.2939458 -4.27449 -4.2327394 -4.1693048 -4.1129203 -4.0843849 -4.0848274 -4.1033688 -4.1541944 -4.2088246 -4.2481027 -4.26344][-4.2661657 -4.265595 -4.2666492 -4.2654886 -4.2500319 -4.2173991 -4.1730866 -4.1415114 -4.1363049 -4.1475058 -4.1689529 -4.2124219 -4.2542343 -4.2792573 -4.2811646][-4.2668328 -4.2607155 -4.2540503 -4.2483611 -4.23443 -4.2113528 -4.1860714 -4.1749125 -4.1837735 -4.1990862 -4.2222424 -4.2596965 -4.291379 -4.3048291 -4.29867][-4.2791839 -4.2701445 -4.2612362 -4.2566233 -4.2469006 -4.2334833 -4.221036 -4.2176962 -4.2259893 -4.2375588 -4.2576966 -4.288425 -4.31307 -4.31933 -4.3095355][-4.2894139 -4.2820449 -4.2765574 -4.2786078 -4.2760119 -4.2699962 -4.2651224 -4.2634969 -4.2686481 -4.2770562 -4.2918887 -4.3141975 -4.3308225 -4.3315034 -4.3202453]]...]
INFO - root - 2017-12-07 22:33:38.852493: step 49310, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 73h:52m:42s remains)
INFO - root - 2017-12-07 22:33:48.416756: step 49320, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 73h:24m:59s remains)
INFO - root - 2017-12-07 22:33:58.111865: step 49330, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 77h:17m:47s remains)
INFO - root - 2017-12-07 22:34:07.723057: step 49340, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 74h:14m:34s remains)
INFO - root - 2017-12-07 22:34:17.513114: step 49350, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 78h:44m:46s remains)
INFO - root - 2017-12-07 22:34:27.181640: step 49360, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 77h:28m:39s remains)
INFO - root - 2017-12-07 22:34:36.758843: step 49370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:58m:05s remains)
INFO - root - 2017-12-07 22:34:46.404140: step 49380, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 73h:41m:17s remains)
INFO - root - 2017-12-07 22:34:56.171211: step 49390, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 75h:48m:24s remains)
INFO - root - 2017-12-07 22:35:05.810673: step 49400, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 76h:06m:59s remains)
2017-12-07 22:35:06.792005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2910333 -4.2910023 -4.2884688 -4.2828612 -4.2786188 -4.2776871 -4.2780252 -4.2760849 -4.2680922 -4.2460432 -4.2166657 -4.1987782 -4.1994491 -4.2077942 -4.2276149][-4.2931738 -4.2910328 -4.286139 -4.2807174 -4.2778354 -4.276854 -4.2730255 -4.263772 -4.24963 -4.2211766 -4.1867218 -4.1727996 -4.18057 -4.1930389 -4.2176275][-4.28589 -4.2795 -4.27153 -4.2670026 -4.2656817 -4.263989 -4.2562857 -4.2384539 -4.2183518 -4.1924262 -4.1594486 -4.1489773 -4.163765 -4.1882277 -4.2211218][-4.268837 -4.2553043 -4.2441764 -4.2378268 -4.2340407 -4.2264271 -4.2072015 -4.1750178 -4.1521635 -4.1409993 -4.1272817 -4.1271472 -4.1527424 -4.1897902 -4.2289276][-4.2355967 -4.20808 -4.1904421 -4.1851177 -4.1847343 -4.1793833 -4.1512351 -4.0962543 -4.061873 -4.0717912 -4.09033 -4.1056423 -4.1384172 -4.1849856 -4.2280235][-4.2057519 -4.1544743 -4.1204653 -4.1140604 -4.1191783 -4.1263728 -4.0949378 -4.0070243 -3.9409266 -3.9712648 -4.0385351 -4.0848966 -4.1238937 -4.1728387 -4.216187][-4.1739416 -4.1003656 -4.043952 -4.0292563 -4.0411472 -4.0610204 -4.0265245 -3.8986993 -3.7744589 -3.8206224 -3.9527586 -4.0411186 -4.0863304 -4.132669 -4.1797729][-4.125165 -4.0428019 -3.9686148 -3.9372609 -3.9464738 -3.9704387 -3.9322834 -3.7807393 -3.6081436 -3.6624081 -3.8491392 -3.9780953 -4.0363154 -4.0866475 -4.1448421][-4.1197834 -4.0522647 -3.990191 -3.9634352 -3.9785967 -4.0135679 -3.9924412 -3.8703721 -3.7206922 -3.7419252 -3.8896298 -4.005547 -4.0552192 -4.0935707 -4.1515884][-4.1524129 -4.1016192 -4.0545325 -4.0377865 -4.0587468 -4.0963407 -4.0908332 -4.0159473 -3.9160786 -3.9129984 -3.9985418 -4.0799336 -4.1181974 -4.1449389 -4.1951566][-4.1899257 -4.1444798 -4.1067257 -4.099535 -4.1257348 -4.1620297 -4.1659837 -4.1259274 -4.0653925 -4.0508051 -4.0944505 -4.147419 -4.1794519 -4.2036486 -4.2437844][-4.2224755 -4.1771946 -4.1416111 -4.1397982 -4.1679206 -4.206728 -4.2215366 -4.2028451 -4.1614828 -4.139194 -4.1610084 -4.1965003 -4.2212329 -4.2424374 -4.2758679][-4.2401719 -4.2026196 -4.1746669 -4.1750331 -4.1985407 -4.2326741 -4.2502146 -4.2327127 -4.1910682 -4.161397 -4.1720338 -4.1989694 -4.2208343 -4.2420897 -4.2729597][-4.2399845 -4.211874 -4.1949134 -4.1955609 -4.2144394 -4.2448711 -4.2606759 -4.2357955 -4.1870575 -4.1538029 -4.1559782 -4.1783195 -4.1984015 -4.2181234 -4.2460775][-4.2338276 -4.2104816 -4.2008691 -4.2018657 -4.2163782 -4.2437944 -4.2588186 -4.2296076 -4.1748767 -4.134428 -4.1263962 -4.1468172 -4.171113 -4.1938629 -4.2236943]]...]
INFO - root - 2017-12-07 22:35:16.466012: step 49410, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 73h:44m:43s remains)
INFO - root - 2017-12-07 22:35:26.113521: step 49420, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 77h:11m:39s remains)
INFO - root - 2017-12-07 22:35:35.678406: step 49430, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 77h:23m:18s remains)
INFO - root - 2017-12-07 22:35:45.300481: step 49440, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 72h:25m:47s remains)
INFO - root - 2017-12-07 22:35:55.103005: step 49450, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 74h:00m:53s remains)
INFO - root - 2017-12-07 22:36:04.685156: step 49460, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 75h:17m:51s remains)
INFO - root - 2017-12-07 22:36:14.359917: step 49470, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 73h:05m:39s remains)
INFO - root - 2017-12-07 22:36:23.847976: step 49480, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 76h:59m:06s remains)
INFO - root - 2017-12-07 22:36:33.460045: step 49490, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 77h:27m:23s remains)
INFO - root - 2017-12-07 22:36:43.088617: step 49500, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.021 sec/batch; 80h:16m:01s remains)
2017-12-07 22:36:44.040765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1703396 -4.1669745 -4.1675906 -4.1624908 -4.1445408 -4.1359458 -4.1399469 -4.1452765 -4.146039 -4.1500568 -4.1707039 -4.1938105 -4.2179217 -4.2519155 -4.2847381][-4.1348863 -4.1378279 -4.1520605 -4.1661725 -4.1619749 -4.1616158 -4.1744757 -4.1852651 -4.1819625 -4.1795688 -4.1941562 -4.209836 -4.2267504 -4.2553439 -4.2835755][-4.1197338 -4.1286669 -4.1484509 -4.17229 -4.1812272 -4.1893229 -4.2077932 -4.2254686 -4.2280636 -4.2251606 -4.2324295 -4.2379866 -4.2453609 -4.2660122 -4.2868958][-4.1321979 -4.1389418 -4.1508727 -4.1661997 -4.1740437 -4.1830258 -4.200768 -4.2233057 -4.2364721 -4.2430973 -4.2544947 -4.258646 -4.2641439 -4.2819519 -4.2971077][-4.1453853 -4.1411343 -4.14042 -4.1371136 -4.1288738 -4.1280355 -4.1351638 -4.1567574 -4.1785254 -4.1994376 -4.2251258 -4.2411442 -4.2553678 -4.2802873 -4.2971582][-4.1554327 -4.1342649 -4.118649 -4.0963912 -4.0693555 -4.0483084 -4.0363464 -4.0498633 -4.0727916 -4.1077652 -4.1545162 -4.1893258 -4.2195935 -4.2559252 -4.2803431][-4.1677513 -4.1341319 -4.1104064 -4.0820966 -4.0463662 -4.0095611 -3.9782557 -3.9709156 -3.9742215 -4.0107226 -4.0744839 -4.1292534 -4.1766825 -4.2244239 -4.2582746][-4.1780748 -4.1428132 -4.1196017 -4.098527 -4.0709734 -4.0395455 -4.0079894 -3.9921422 -3.9844925 -4.0129685 -4.0676451 -4.1201115 -4.1678319 -4.214747 -4.2502813][-4.1965375 -4.1714535 -4.1568937 -4.1477103 -4.1338854 -4.1140423 -4.0941105 -4.0873137 -4.0860443 -4.1072235 -4.1407142 -4.1724696 -4.2035937 -4.2365913 -4.2621288][-4.2120943 -4.2017345 -4.2012753 -4.2023525 -4.1969275 -4.1843357 -4.1732039 -4.1733432 -4.1743279 -4.1853404 -4.2038617 -4.2221727 -4.2403154 -4.2615442 -4.2777491][-4.2285786 -4.2278595 -4.23587 -4.242631 -4.240839 -4.2303367 -4.2201242 -4.2171192 -4.2108703 -4.2108893 -4.2195053 -4.2303362 -4.2441306 -4.2642145 -4.2802339][-4.2472334 -4.2474914 -4.2542529 -4.260582 -4.2609034 -4.252398 -4.2398911 -4.227211 -4.2100286 -4.2036982 -4.2074661 -4.213634 -4.2258563 -4.2488103 -4.2707543][-4.24635 -4.2429767 -4.2439032 -4.24423 -4.241941 -4.2333207 -4.2196145 -4.1995072 -4.1761827 -4.1699195 -4.1765676 -4.1853237 -4.1985269 -4.2248211 -4.253737][-4.2326107 -4.2262025 -4.2239246 -4.2208495 -4.2162361 -4.208003 -4.1959558 -4.1753197 -4.1545916 -4.1533394 -4.165679 -4.1768203 -4.1880517 -4.214715 -4.2447157][-4.2302618 -4.2248197 -4.2228208 -4.2203684 -4.2173257 -4.2117033 -4.2034326 -4.1886649 -4.1740994 -4.1759152 -4.1882911 -4.1972075 -4.205996 -4.2306533 -4.2559633]]...]
INFO - root - 2017-12-07 22:36:53.606918: step 49510, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 75h:37m:02s remains)
INFO - root - 2017-12-07 22:37:03.144203: step 49520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:32m:54s remains)
INFO - root - 2017-12-07 22:37:12.841855: step 49530, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 75h:45m:25s remains)
INFO - root - 2017-12-07 22:37:22.469419: step 49540, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 73h:53m:32s remains)
INFO - root - 2017-12-07 22:37:32.110945: step 49550, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.947 sec/batch; 74h:24m:39s remains)
INFO - root - 2017-12-07 22:37:41.795630: step 49560, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.958 sec/batch; 75h:17m:48s remains)
INFO - root - 2017-12-07 22:37:51.351683: step 49570, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 72h:59m:36s remains)
INFO - root - 2017-12-07 22:38:00.996359: step 49580, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 74h:06m:16s remains)
INFO - root - 2017-12-07 22:38:10.533894: step 49590, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 77h:43m:55s remains)
INFO - root - 2017-12-07 22:38:20.172963: step 49600, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 71h:55m:01s remains)
2017-12-07 22:38:21.165658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2858362 -4.2863789 -4.2781129 -4.2648253 -4.2525043 -4.2481928 -4.2470217 -4.2388 -4.2255645 -4.2126417 -4.1952372 -4.1733294 -4.1560683 -4.1623898 -4.1927056][-4.2630262 -4.26132 -4.2551665 -4.2438989 -4.2325392 -4.22896 -4.22242 -4.2139459 -4.204257 -4.1919408 -4.1718488 -4.1505756 -4.13472 -4.1483197 -4.1847229][-4.2178802 -4.21636 -4.2131357 -4.210072 -4.2050595 -4.1993065 -4.1896114 -4.1844668 -4.1801376 -4.1737924 -4.1563811 -4.1362638 -4.1260862 -4.1469131 -4.1828613][-4.182508 -4.1872754 -4.1862974 -4.1864119 -4.1854377 -4.1702352 -4.1465607 -4.1360974 -4.1347737 -4.1401505 -4.1389656 -4.1313171 -4.1332273 -4.1571913 -4.185812][-4.1641145 -4.1792021 -4.1786208 -4.1756315 -4.1704421 -4.1350961 -4.0818663 -4.0536065 -4.054481 -4.07952 -4.1068935 -4.12393 -4.1424937 -4.168745 -4.1886916][-4.1757731 -4.1973429 -4.1959014 -4.1850047 -4.1614466 -4.09698 -4.0084958 -3.9495156 -3.9556327 -4.013371 -4.0769577 -4.1216755 -4.1514077 -4.1799278 -4.19407][-4.2071376 -4.2300129 -4.2288733 -4.2118969 -4.1658854 -4.0780354 -3.9643376 -3.8833208 -3.8997991 -3.9848988 -4.0734768 -4.1344681 -4.1684251 -4.1945357 -4.2028985][-4.2304711 -4.25175 -4.2499957 -4.2267137 -4.1671066 -4.0814624 -3.9753823 -3.90305 -3.9296587 -4.017549 -4.102108 -4.1566572 -4.186852 -4.2076964 -4.2113295][-4.2370315 -4.2577577 -4.2628751 -4.2357082 -4.1675868 -4.0892272 -4.0152526 -3.9776084 -4.0121007 -4.0850143 -4.15036 -4.1863103 -4.2063141 -4.2231579 -4.2290807][-4.2414536 -4.2613912 -4.2694845 -4.2358632 -4.1563625 -4.0826211 -4.0411677 -4.039197 -4.0830612 -4.1420422 -4.1886706 -4.2117081 -4.2252612 -4.2408667 -4.2523952][-4.2464075 -4.2588139 -4.2610049 -4.2175627 -4.127059 -4.057982 -4.0412326 -4.0688944 -4.1250877 -4.1777573 -4.2144427 -4.2317986 -4.2430744 -4.2593923 -4.2752204][-4.2439432 -4.2419348 -4.23223 -4.1787338 -4.0868912 -4.0258007 -4.0253835 -4.0756187 -4.1443157 -4.195765 -4.2290845 -4.2469368 -4.258637 -4.2743349 -4.2895145][-4.2458754 -4.2284789 -4.2049403 -4.1431251 -4.0558057 -4.0038757 -4.0130367 -4.0768633 -4.1506462 -4.2018094 -4.2334137 -4.2521882 -4.2636232 -4.2787189 -4.2922087][-4.2470746 -4.2257915 -4.1909204 -4.1240025 -4.0485792 -4.0117 -4.0322065 -4.0969095 -4.1639795 -4.2127781 -4.241116 -4.2580976 -4.2699323 -4.28439 -4.2949591][-4.2431149 -4.2267962 -4.1923537 -4.1303983 -4.0694852 -4.0474076 -4.079021 -4.1365724 -4.1901765 -4.2283773 -4.2511859 -4.2655034 -4.2771668 -4.2904582 -4.2973452]]...]
INFO - root - 2017-12-07 22:38:30.611796: step 49610, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 74h:38m:30s remains)
INFO - root - 2017-12-07 22:38:40.197473: step 49620, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:56m:45s remains)
INFO - root - 2017-12-07 22:38:49.795936: step 49630, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.972 sec/batch; 76h:24m:39s remains)
INFO - root - 2017-12-07 22:38:59.532200: step 49640, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.969 sec/batch; 76h:06m:48s remains)
INFO - root - 2017-12-07 22:39:09.145978: step 49650, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 67h:31m:04s remains)
INFO - root - 2017-12-07 22:39:18.693598: step 49660, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 67h:52m:32s remains)
INFO - root - 2017-12-07 22:39:28.334000: step 49670, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.010 sec/batch; 79h:22m:25s remains)
INFO - root - 2017-12-07 22:39:38.054185: step 49680, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 72h:16m:33s remains)
INFO - root - 2017-12-07 22:39:47.713010: step 49690, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 74h:07m:18s remains)
INFO - root - 2017-12-07 22:39:57.375213: step 49700, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 78h:35m:35s remains)
2017-12-07 22:39:58.361496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2039437 -4.1723042 -4.1558728 -4.1585646 -4.1572003 -4.14602 -4.118989 -4.1075478 -4.1201148 -4.1516747 -4.1970592 -4.24921 -4.2900758 -4.3173079 -4.3327761][-4.1831098 -4.1374073 -4.1177692 -4.1305108 -4.1382189 -4.1293268 -4.1011105 -4.0966363 -4.1170845 -4.1519094 -4.19404 -4.2438111 -4.2838874 -4.3108878 -4.3261018][-4.1683664 -4.107523 -4.0867577 -4.10999 -4.1271453 -4.1241813 -4.0975485 -4.1009083 -4.1332588 -4.1756549 -4.2181177 -4.2614946 -4.2942886 -4.3151584 -4.325469][-4.1655774 -4.10072 -4.0838089 -4.112751 -4.1307721 -4.1277275 -4.0983353 -4.1042747 -4.14512 -4.1947055 -4.2414308 -4.2833085 -4.3105869 -4.3252625 -4.3290005][-4.1772118 -4.1204605 -4.1108828 -4.1385641 -4.1526136 -4.143641 -4.107635 -4.1079688 -4.1477962 -4.2030263 -4.2588291 -4.3019352 -4.3253489 -4.3352232 -4.335547][-4.1963124 -4.1537232 -4.1503124 -4.1726427 -4.1806917 -4.1674738 -4.1269369 -4.1136394 -4.1451135 -4.2009072 -4.2657571 -4.3127804 -4.3364162 -4.343668 -4.3422489][-4.2184658 -4.1866732 -4.1824303 -4.1958809 -4.1978579 -4.1826982 -4.1396227 -4.1133518 -4.1372714 -4.1948481 -4.2635994 -4.3139768 -4.34073 -4.3488288 -4.3470087][-4.239748 -4.2114005 -4.2038355 -4.2079949 -4.2000656 -4.1814766 -4.1353612 -4.1018038 -4.1226239 -4.1842937 -4.2531562 -4.3047547 -4.3349996 -4.3467121 -4.3480206][-4.258348 -4.2349439 -4.2256989 -4.2212386 -4.199223 -4.1673856 -4.1190839 -4.0906668 -4.116199 -4.1790695 -4.2406807 -4.2907157 -4.3235874 -4.340486 -4.3460503][-4.2726359 -4.2540278 -4.2406292 -4.2289081 -4.1980028 -4.1582694 -4.117558 -4.0964522 -4.1228752 -4.183342 -4.2358961 -4.2790723 -4.311965 -4.332634 -4.3426247][-4.2783189 -4.2611089 -4.2469821 -4.2338333 -4.2023349 -4.1595535 -4.1267881 -4.1139932 -4.1393294 -4.1926918 -4.2400727 -4.2767854 -4.3076591 -4.3284497 -4.3401356][-4.28115 -4.2635326 -4.249301 -4.2404609 -4.2177649 -4.1777105 -4.1485019 -4.1394811 -4.1646552 -4.2119026 -4.2530174 -4.2845383 -4.3130269 -4.3293939 -4.3400121][-4.2819963 -4.2660489 -4.2514181 -4.2429528 -4.2286091 -4.1982493 -4.173882 -4.1691194 -4.1968222 -4.2426271 -4.2783608 -4.3025379 -4.3236904 -4.3338947 -4.3421855][-4.2761464 -4.2644138 -4.2534437 -4.2440624 -4.2323046 -4.2116933 -4.1963992 -4.2007003 -4.2307858 -4.2733216 -4.301415 -4.3196115 -4.3353152 -4.3413138 -4.3464947][-4.2845497 -4.27805 -4.2717834 -4.2610469 -4.2468419 -4.2328105 -4.2225132 -4.2278767 -4.2531142 -4.2888651 -4.3130636 -4.3310461 -4.3466463 -4.3513856 -4.353055]]...]
INFO - root - 2017-12-07 22:40:07.952766: step 49710, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 75h:07m:26s remains)
INFO - root - 2017-12-07 22:40:17.639522: step 49720, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 74h:06m:33s remains)
INFO - root - 2017-12-07 22:40:27.210184: step 49730, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 78h:27m:14s remains)
INFO - root - 2017-12-07 22:40:36.731814: step 49740, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 77h:35m:28s remains)
INFO - root - 2017-12-07 22:40:46.166457: step 49750, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 70h:11m:48s remains)
INFO - root - 2017-12-07 22:40:55.739194: step 49760, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 74h:34m:12s remains)
INFO - root - 2017-12-07 22:41:05.326097: step 49770, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 78h:07m:07s remains)
INFO - root - 2017-12-07 22:41:15.106858: step 49780, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.992 sec/batch; 77h:53m:25s remains)
INFO - root - 2017-12-07 22:41:24.613125: step 49790, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 73h:11m:28s remains)
INFO - root - 2017-12-07 22:41:34.335318: step 49800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 74h:54m:41s remains)
2017-12-07 22:41:35.299183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2997146 -4.3084497 -4.3189015 -4.3287253 -4.3360658 -4.3394437 -4.3355718 -4.3245416 -4.3124275 -4.3033147 -4.3021894 -4.3066988 -4.3134351 -4.3187952 -4.3196421][-4.31345 -4.3227205 -4.3323011 -4.338686 -4.3404317 -4.3366094 -4.3252325 -4.3087935 -4.293797 -4.2836509 -4.2855787 -4.2954555 -4.3080864 -4.3193741 -4.324913][-4.314712 -4.323771 -4.3306608 -4.3317409 -4.3250675 -4.3096118 -4.2857223 -4.2603936 -4.24053 -4.227818 -4.2335539 -4.2529664 -4.2779694 -4.3003392 -4.3145733][-4.2973566 -4.3057151 -4.3103056 -4.3064251 -4.2911887 -4.2648497 -4.2304115 -4.1950526 -4.1673975 -4.15006 -4.1587081 -4.18825 -4.2262454 -4.2602515 -4.2832036][-4.2701182 -4.2780738 -4.2804403 -4.2729163 -4.25095 -4.2164316 -4.173728 -4.1309986 -4.0972295 -4.07631 -4.087338 -4.1237426 -4.16765 -4.206481 -4.2340732][-4.2377777 -4.246325 -4.2490878 -4.2401295 -4.2163486 -4.1791148 -4.1342163 -4.0902066 -4.055975 -4.0376019 -4.0510917 -4.0846767 -4.1213684 -4.1524682 -4.1753435][-4.2051215 -4.2164068 -4.2222247 -4.2155166 -4.1953387 -4.1629877 -4.1241903 -4.0869122 -4.0590148 -4.0491028 -4.0630064 -4.0845313 -4.1048703 -4.1196494 -4.1295419][-4.1948309 -4.20754 -4.21531 -4.2123036 -4.1995268 -4.1781163 -4.1523495 -4.1276994 -4.1099496 -4.1078434 -4.1170974 -4.1254411 -4.1306767 -4.1303449 -4.1259623][-4.2113209 -4.2211266 -4.2276497 -4.22782 -4.22337 -4.2145572 -4.2039857 -4.1925278 -4.18198 -4.180644 -4.1826463 -4.1809897 -4.176641 -4.1675959 -4.1548266][-4.2342534 -4.2383251 -4.2411823 -4.2414207 -4.2418928 -4.2423863 -4.2413774 -4.237359 -4.2313137 -4.2304807 -4.2289271 -4.2239823 -4.2161107 -4.2039003 -4.1881084][-4.2513404 -4.2497106 -4.2482858 -4.2472091 -4.2494321 -4.253531 -4.2555208 -4.2530036 -4.2494268 -4.2507248 -4.2516775 -4.2491479 -4.2421284 -4.2307277 -4.2164903][-4.2631183 -4.2578034 -4.2537656 -4.2505035 -4.2516203 -4.2556925 -4.2569513 -4.2530093 -4.2492595 -4.2521071 -4.2572212 -4.2593923 -4.2555938 -4.2474647 -4.2374611][-4.2779059 -4.2721381 -4.2677264 -4.2632141 -4.262176 -4.2644782 -4.2640967 -4.2585044 -4.2536106 -4.2564478 -4.2639594 -4.2696605 -4.2691941 -4.2647576 -4.2589931][-4.2914476 -4.2867765 -4.2826309 -4.2785068 -4.2763786 -4.2760634 -4.2724071 -4.2653031 -4.2599921 -4.262991 -4.2708697 -4.2771511 -4.2780247 -4.2757211 -4.2734518][-4.3000126 -4.2963586 -4.2932105 -4.2910118 -4.2903428 -4.2889462 -4.2836995 -4.2757239 -4.2700877 -4.2711887 -4.2761464 -4.2798843 -4.2796035 -4.2781029 -4.27877]]...]
INFO - root - 2017-12-07 22:41:44.951120: step 49810, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.985 sec/batch; 77h:18m:29s remains)
INFO - root - 2017-12-07 22:41:54.594511: step 49820, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 76h:46m:22s remains)
INFO - root - 2017-12-07 22:42:04.294048: step 49830, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 75h:40m:19s remains)
INFO - root - 2017-12-07 22:42:14.068826: step 49840, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 78h:11m:15s remains)
INFO - root - 2017-12-07 22:42:23.530656: step 49850, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.949 sec/batch; 74h:29m:42s remains)
INFO - root - 2017-12-07 22:42:33.151674: step 49860, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 76h:07m:59s remains)
INFO - root - 2017-12-07 22:42:42.771375: step 49870, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 75h:49m:50s remains)
INFO - root - 2017-12-07 22:42:52.626014: step 49880, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 71h:39m:38s remains)
INFO - root - 2017-12-07 22:43:02.359853: step 49890, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.998 sec/batch; 78h:21m:55s remains)
INFO - root - 2017-12-07 22:43:11.982776: step 49900, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 76h:09m:45s remains)
2017-12-07 22:43:12.899557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1929455 -4.1915603 -4.2063866 -4.23801 -4.2676167 -4.2846169 -4.2910604 -4.2840815 -4.26021 -4.2112017 -4.1466751 -4.0958595 -4.0708447 -4.0742674 -4.1004243][-4.1916132 -4.1935534 -4.2241869 -4.2713532 -4.3075924 -4.3178105 -4.3058672 -4.27683 -4.2326684 -4.1725545 -4.1175642 -4.0835986 -4.0745311 -4.0861897 -4.1207223][-4.1918015 -4.2018962 -4.2439933 -4.29844 -4.3340168 -4.3326359 -4.2996769 -4.2450414 -4.18505 -4.1322045 -4.1001763 -4.0861611 -4.08664 -4.0997248 -4.1386981][-4.19677 -4.2245927 -4.2770481 -4.3265557 -4.3434429 -4.3156986 -4.2520094 -4.1686025 -4.10563 -4.0859923 -4.0910072 -4.0974641 -4.1090479 -4.1253285 -4.1598082][-4.2201252 -4.2667561 -4.318871 -4.3482223 -4.3278017 -4.25589 -4.146091 -4.0333428 -3.9874642 -4.026988 -4.08157 -4.1177797 -4.1447911 -4.1653528 -4.185277][-4.2510576 -4.3045654 -4.3463211 -4.3468289 -4.2843904 -4.162931 -3.99515 -3.8481183 -3.838552 -3.9545341 -4.0686264 -4.1417775 -4.186316 -4.206943 -4.2097321][-4.2793765 -4.3286934 -4.3518214 -4.3217645 -4.224566 -4.0601811 -3.8524411 -3.6982784 -3.748951 -3.9280086 -4.0829272 -4.1764793 -4.2252393 -4.2384706 -4.2283635][-4.2988353 -4.3365555 -4.3399024 -4.290441 -4.1754422 -4.0058551 -3.8225112 -3.7271404 -3.8219271 -3.9953673 -4.140727 -4.2281241 -4.2652359 -4.2685461 -4.2518311][-4.3023825 -4.3262024 -4.3185916 -4.2653933 -4.1606126 -4.0300903 -3.9216738 -3.8997784 -3.9873214 -4.1083026 -4.2124777 -4.2788448 -4.3013334 -4.2972674 -4.2783][-4.2954736 -4.3111806 -4.3023634 -4.2571812 -4.1787038 -4.0977459 -4.0522394 -4.0694923 -4.1351871 -4.2066007 -4.2700129 -4.3121338 -4.3192315 -4.3066926 -4.288147][-4.2922211 -4.3013058 -4.2942686 -4.2620406 -4.2121615 -4.1698627 -4.1602745 -4.1905341 -4.2394629 -4.2829237 -4.3163128 -4.3329916 -4.3236856 -4.3061638 -4.2888942][-4.2970986 -4.3000159 -4.2931652 -4.2734728 -4.2465935 -4.2314582 -4.2416744 -4.2722278 -4.3045826 -4.3301992 -4.3432732 -4.3389025 -4.31826 -4.2999139 -4.285728][-4.3049822 -4.3022718 -4.2941756 -4.2816606 -4.2695041 -4.2711978 -4.2909474 -4.3173089 -4.3383307 -4.3480082 -4.3419237 -4.3213615 -4.3002405 -4.2895832 -4.2833872][-4.3035674 -4.2964993 -4.2848721 -4.2748656 -4.2722578 -4.2853374 -4.3100543 -4.33345 -4.3434734 -4.3363781 -4.3125834 -4.2812152 -4.2655773 -4.2674084 -4.2713475][-4.2737961 -4.2626758 -4.246758 -4.23637 -4.2397938 -4.2627225 -4.2941723 -4.3173456 -4.3198366 -4.2995396 -4.262989 -4.2274833 -4.2193942 -4.2321296 -4.2432551]]...]
INFO - root - 2017-12-07 22:43:22.464626: step 49910, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 77h:24m:06s remains)
INFO - root - 2017-12-07 22:43:32.185649: step 49920, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 77h:23m:49s remains)
INFO - root - 2017-12-07 22:43:41.917352: step 49930, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 75h:22m:05s remains)
INFO - root - 2017-12-07 22:43:51.546887: step 49940, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 74h:46m:57s remains)
INFO - root - 2017-12-07 22:44:01.062463: step 49950, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 71h:23m:05s remains)
INFO - root - 2017-12-07 22:44:10.755244: step 49960, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 75h:21m:19s remains)
INFO - root - 2017-12-07 22:44:20.448978: step 49970, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 74h:24m:22s remains)
INFO - root - 2017-12-07 22:44:30.095351: step 49980, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 74h:52m:39s remains)
INFO - root - 2017-12-07 22:44:39.728518: step 49990, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 76h:52m:13s remains)
INFO - root - 2017-12-07 22:44:49.031263: step 50000, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 74h:20m:33s remains)
2017-12-07 22:44:49.936898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3305211 -4.3227959 -4.3121805 -4.2965755 -4.2709002 -4.2444186 -4.2315073 -4.2347016 -4.2416754 -4.2478876 -4.2556987 -4.2621479 -4.2672415 -4.2718558 -4.2785854][-4.3280249 -4.3154306 -4.3015094 -4.28223 -4.250495 -4.2129488 -4.1907029 -4.197268 -4.2054024 -4.2044649 -4.2098818 -4.2170339 -4.2253466 -4.2325416 -4.2398767][-4.3226509 -4.3071132 -4.2888818 -4.26284 -4.22504 -4.1746297 -4.1371202 -4.1394544 -4.149148 -4.1456566 -4.1561575 -4.1731324 -4.1906152 -4.2054205 -4.2126627][-4.3171053 -4.2970195 -4.2685003 -4.2315612 -4.1872582 -4.1257229 -4.0763268 -4.0688834 -4.0779495 -4.0851026 -4.1094675 -4.1418052 -4.17314 -4.195766 -4.1994796][-4.3112416 -4.2814178 -4.2386942 -4.1879735 -4.1299944 -4.0539937 -3.9843125 -3.9651041 -3.9887948 -4.0279465 -4.0812716 -4.1352429 -4.1736755 -4.1939249 -4.1942906][-4.306489 -4.2643814 -4.2070837 -4.1400275 -4.0566096 -3.9503 -3.8436894 -3.8149135 -3.8845525 -3.9815328 -4.0748072 -4.141942 -4.1764436 -4.1851478 -4.1841021][-4.3018456 -4.2449846 -4.1671677 -4.0727987 -3.94829 -3.7993004 -3.6396911 -3.6101053 -3.766366 -3.9356155 -4.0594363 -4.1279669 -4.1530218 -4.1564083 -4.1649432][-4.2966857 -4.2273784 -4.1275053 -4.0019641 -3.8455067 -3.6745989 -3.5034955 -3.5023713 -3.7155421 -3.9062023 -4.029563 -4.0869374 -4.1052537 -4.1208434 -4.1444011][-4.2908216 -4.2203474 -4.1185722 -3.9979515 -3.8700583 -3.7586112 -3.6707838 -3.6934445 -3.8353307 -3.9562395 -4.0395327 -4.0756955 -4.0919871 -4.1182528 -4.1511774][-4.287694 -4.224587 -4.1416936 -4.0564551 -3.9830315 -3.9338763 -3.9018319 -3.9187236 -3.9873307 -4.0424905 -4.0880604 -4.11234 -4.1269813 -4.1580749 -4.1887217][-4.2985754 -4.2545433 -4.2020903 -4.1548681 -4.1200104 -4.0959129 -4.0744996 -4.0764189 -4.1098757 -4.1322184 -4.1498065 -4.1633782 -4.1747255 -4.2019 -4.2241979][-4.314405 -4.287199 -4.2566924 -4.2355566 -4.2229505 -4.2091479 -4.191474 -4.1879368 -4.2080812 -4.2190528 -4.2252364 -4.2321534 -4.2393351 -4.2553072 -4.2626081][-4.3253183 -4.3097529 -4.2939034 -4.288074 -4.2869072 -4.2836609 -4.2773018 -4.2710881 -4.2757006 -4.2790713 -4.2832894 -4.2871804 -4.2914381 -4.3004961 -4.2999063][-4.332962 -4.3252687 -4.3171897 -4.3162594 -4.319243 -4.3241081 -4.3277683 -4.3264022 -4.3265114 -4.3261752 -4.3258142 -4.3262768 -4.3268723 -4.333396 -4.3364377][-4.3370314 -4.3314891 -4.32698 -4.3275137 -4.3319616 -4.340179 -4.3460073 -4.3478875 -4.3476081 -4.3455119 -4.3419814 -4.3424692 -4.3449035 -4.3519068 -4.3587961]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 22:45:00.210451: step 50010, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 74h:50m:29s remains)
INFO - root - 2017-12-07 22:45:09.885780: step 50020, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.005 sec/batch; 78h:51m:53s remains)
INFO - root - 2017-12-07 22:45:19.526133: step 50030, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 77h:10m:32s remains)
INFO - root - 2017-12-07 22:45:29.076439: step 50040, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 75h:02m:56s remains)
INFO - root - 2017-12-07 22:45:38.850946: step 50050, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 74h:50m:48s remains)
INFO - root - 2017-12-07 22:45:48.540145: step 50060, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 76h:01m:14s remains)
INFO - root - 2017-12-07 22:45:58.247826: step 50070, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 76h:45m:59s remains)
INFO - root - 2017-12-07 22:46:07.816965: step 50080, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 75h:36m:27s remains)
INFO - root - 2017-12-07 22:46:17.488133: step 50090, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.935 sec/batch; 73h:22m:00s remains)
INFO - root - 2017-12-07 22:46:27.169547: step 50100, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.977 sec/batch; 76h:36m:58s remains)
2017-12-07 22:46:28.197698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1958547 -4.20543 -4.233171 -4.2596893 -4.2546129 -4.2058849 -4.1387553 -4.0964975 -4.113091 -4.1550016 -4.1876597 -4.2137046 -4.2210259 -4.2145648 -4.2108116][-4.21127 -4.2305341 -4.2541161 -4.2666187 -4.2508206 -4.1979885 -4.1347013 -4.0983868 -4.1163721 -4.151444 -4.1777616 -4.2004972 -4.2108393 -4.2096128 -4.2110071][-4.2163286 -4.2430549 -4.2656732 -4.2667532 -4.236999 -4.1797476 -4.1177368 -4.0849338 -4.1036239 -4.13861 -4.1674905 -4.1919737 -4.2045074 -4.2109947 -4.2180133][-4.2088017 -4.2414074 -4.26942 -4.2645283 -4.2280641 -4.1720834 -4.1144137 -4.0863938 -4.1076913 -4.1456637 -4.1754141 -4.1997714 -4.2151966 -4.2268448 -4.2348523][-4.1897678 -4.2246413 -4.2593331 -4.2565207 -4.2238255 -4.1754136 -4.1282382 -4.1128321 -4.1417441 -4.1777058 -4.1996751 -4.216464 -4.2296486 -4.2419577 -4.2489333][-4.157979 -4.1963091 -4.2404189 -4.2455788 -4.2190332 -4.1766753 -4.1319013 -4.1261764 -4.1627016 -4.2004728 -4.2209005 -4.2375326 -4.2500978 -4.2560811 -4.2590232][-4.1119771 -4.1570134 -4.2143564 -4.2306776 -4.2069874 -4.1615591 -4.1116571 -4.1162505 -4.166482 -4.2122474 -4.2346563 -4.251987 -4.2667928 -4.2683544 -4.2672281][-4.0755486 -4.1280117 -4.197134 -4.224194 -4.2001243 -4.1464481 -4.0942626 -4.1088066 -4.1687274 -4.2188144 -4.2403245 -4.2562294 -4.2763839 -4.2801161 -4.2734103][-4.0961027 -4.1410394 -4.2041354 -4.2319088 -4.2071686 -4.1570497 -4.1099505 -4.1269073 -4.1836538 -4.2317467 -4.2502055 -4.2606559 -4.2811241 -4.2872891 -4.2815509][-4.1448355 -4.1752777 -4.2200155 -4.2415066 -4.2223334 -4.1864333 -4.152164 -4.1658106 -4.2116022 -4.2502327 -4.2645626 -4.2710581 -4.2878032 -4.29353 -4.2928262][-4.1779938 -4.2014403 -4.2327685 -4.2535419 -4.2452073 -4.223639 -4.1992507 -4.2064333 -4.2400837 -4.2696719 -4.280056 -4.2866206 -4.3023515 -4.308517 -4.3123779][-4.2054243 -4.2264175 -4.2434454 -4.25871 -4.2591362 -4.2455349 -4.226831 -4.2300353 -4.2567892 -4.2773366 -4.2817974 -4.2909789 -4.3083968 -4.3191605 -4.3254876][-4.2284389 -4.2467794 -4.2481928 -4.2507639 -4.257277 -4.2546058 -4.24142 -4.24145 -4.2633519 -4.2782111 -4.2816644 -4.2960176 -4.3146625 -4.3285384 -4.3368087][-4.2402349 -4.2529421 -4.2447104 -4.2366128 -4.2512517 -4.2613139 -4.258213 -4.2577004 -4.2731838 -4.2827411 -4.2877603 -4.3042626 -4.3209691 -4.3338637 -4.3422546][-4.2563248 -4.2633977 -4.2508979 -4.240572 -4.2569237 -4.2751446 -4.2831488 -4.2838368 -4.2930722 -4.296196 -4.2991204 -4.3115058 -4.3234262 -4.3324065 -4.3392344]]...]
INFO - root - 2017-12-07 22:46:37.785234: step 50110, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 76h:50m:24s remains)
INFO - root - 2017-12-07 22:46:47.453900: step 50120, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 70h:29m:46s remains)
INFO - root - 2017-12-07 22:46:57.067168: step 50130, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 76h:42m:27s remains)
INFO - root - 2017-12-07 22:47:06.704092: step 50140, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 76h:19m:40s remains)
INFO - root - 2017-12-07 22:47:16.297642: step 50150, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 78h:24m:26s remains)
INFO - root - 2017-12-07 22:47:26.120090: step 50160, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 73h:30m:38s remains)
INFO - root - 2017-12-07 22:47:35.838839: step 50170, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 78h:53m:31s remains)
INFO - root - 2017-12-07 22:47:45.477558: step 50180, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 73h:33m:03s remains)
INFO - root - 2017-12-07 22:47:55.251392: step 50190, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 75h:58m:53s remains)
INFO - root - 2017-12-07 22:48:04.875248: step 50200, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 72h:28m:05s remains)
2017-12-07 22:48:05.847789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3357496 -4.3384118 -4.3404202 -4.3423538 -4.3381205 -4.3244591 -4.3071451 -4.2916579 -4.2802844 -4.28201 -4.2897277 -4.295948 -4.30119 -4.310164 -4.3225965][-4.3366547 -4.3491569 -4.3583279 -4.3623662 -4.3538218 -4.3297739 -4.2990961 -4.2686081 -4.2484431 -4.2504659 -4.2617435 -4.2683029 -4.2716403 -4.2833977 -4.3017187][-4.3300691 -4.3555207 -4.372787 -4.376617 -4.3611879 -4.323853 -4.276289 -4.2287169 -4.2004476 -4.2052784 -4.2235727 -4.2313786 -4.2345986 -4.2513781 -4.2766447][-4.3152237 -4.3534989 -4.3782835 -4.3825312 -4.362987 -4.3164792 -4.250854 -4.1835842 -4.1454649 -4.1548505 -4.1825428 -4.1919351 -4.1982703 -4.2241507 -4.260242][-4.2907214 -4.3384976 -4.3651352 -4.3645782 -4.337255 -4.2770133 -4.1917219 -4.1010389 -4.0566282 -4.0835338 -4.1291609 -4.1503315 -4.1692677 -4.2106938 -4.2590036][-4.2642732 -4.3144612 -4.3353543 -4.3245106 -4.2858262 -4.2098055 -4.103076 -3.9885406 -3.9410105 -4.0005107 -4.0748916 -4.1194825 -4.1597505 -4.2147551 -4.2686381][-4.2456422 -4.2924852 -4.306675 -4.286037 -4.2355137 -4.1415496 -4.0089011 -3.864043 -3.8139486 -3.9162948 -4.025835 -4.0939264 -4.1519661 -4.21815 -4.2771621][-4.2421489 -4.281405 -4.2898006 -4.2633076 -4.2061687 -4.1003819 -3.9499147 -3.789788 -3.7569675 -3.8975377 -4.0280681 -4.1054668 -4.1660619 -4.2315435 -4.2879667][-4.2565908 -4.2892594 -4.2937407 -4.2636213 -4.2100725 -4.1157718 -3.9807794 -3.8509514 -3.85078 -3.9806886 -4.0951834 -4.160346 -4.2081113 -4.2589712 -4.30259][-4.2741513 -4.2983041 -4.295135 -4.2605963 -4.211916 -4.1344752 -4.0302858 -3.9476638 -3.9741802 -4.0778737 -4.1636834 -4.2098627 -4.2433181 -4.2806973 -4.3140168][-4.2888665 -4.3023143 -4.29046 -4.2556109 -4.2151356 -4.1539044 -4.0834737 -4.0447855 -4.0838761 -4.1607976 -4.217731 -4.2465949 -4.2680316 -4.2974129 -4.3246818][-4.3031664 -4.3106985 -4.29872 -4.2733874 -4.2450104 -4.20423 -4.1658888 -4.1552668 -4.1880689 -4.2350841 -4.2647958 -4.2794662 -4.2926769 -4.3153362 -4.33587][-4.3131337 -4.3176594 -4.3105965 -4.2966652 -4.2803807 -4.2592874 -4.2444077 -4.2450914 -4.2638669 -4.2859135 -4.297092 -4.3031721 -4.3129172 -4.3301373 -4.3445659][-4.3243718 -4.3261986 -4.3222055 -4.3155541 -4.3098421 -4.3038607 -4.300292 -4.3011541 -4.3078742 -4.3145471 -4.3152952 -4.315671 -4.323204 -4.3377619 -4.3488307][-4.3389754 -4.3396626 -4.3373275 -4.3337188 -4.33194 -4.3313622 -4.330389 -4.32958 -4.3298974 -4.3295565 -4.32605 -4.3246655 -4.3307867 -4.3420396 -4.3503308]]...]
INFO - root - 2017-12-07 22:48:15.481969: step 50210, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 73h:57m:40s remains)
INFO - root - 2017-12-07 22:48:25.242434: step 50220, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 77h:44m:15s remains)
INFO - root - 2017-12-07 22:48:34.951658: step 50230, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 76h:08m:23s remains)
INFO - root - 2017-12-07 22:48:44.540000: step 50240, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 78h:27m:46s remains)
INFO - root - 2017-12-07 22:48:54.192587: step 50250, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 77h:19m:22s remains)
INFO - root - 2017-12-07 22:49:03.898007: step 50260, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 77h:15m:23s remains)
INFO - root - 2017-12-07 22:49:13.648648: step 50270, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 69h:31m:18s remains)
INFO - root - 2017-12-07 22:49:23.276022: step 50280, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.962 sec/batch; 75h:22m:40s remains)
INFO - root - 2017-12-07 22:49:32.963545: step 50290, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 74h:51m:16s remains)
INFO - root - 2017-12-07 22:49:42.793915: step 50300, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 76h:47m:54s remains)
2017-12-07 22:49:43.708659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2360792 -4.248569 -4.2604938 -4.2691507 -4.2618651 -4.2491546 -4.2417836 -4.2427535 -4.2521253 -4.2634139 -4.2668982 -4.2654667 -4.2668014 -4.2660871 -4.2577925][-4.2007947 -4.215776 -4.2307119 -4.2351007 -4.2196589 -4.2011271 -4.1914954 -4.19212 -4.2065206 -4.2285624 -4.241529 -4.24832 -4.2537313 -4.252398 -4.237947][-4.1671619 -4.1852407 -4.2023988 -4.2016549 -4.17645 -4.1496525 -4.1342478 -4.1336484 -4.1487694 -4.1808758 -4.2093368 -4.229569 -4.2397428 -4.2353659 -4.2180495][-4.1402135 -4.1625934 -4.1815057 -4.1787786 -4.1491289 -4.1138182 -4.0854368 -4.0711203 -4.0792208 -4.1188755 -4.165956 -4.1979055 -4.2141018 -4.2118797 -4.1979141][-4.1258035 -4.1478658 -4.159986 -4.1552649 -4.1279411 -4.0828552 -4.0299573 -3.9877987 -3.9873881 -4.0409694 -4.1120453 -4.1590104 -4.1827917 -4.187901 -4.185606][-4.1026926 -4.1157541 -4.1212139 -4.1206269 -4.0965986 -4.0388074 -3.9508524 -3.8667245 -3.8612418 -3.9541411 -4.0627956 -4.1287222 -4.15699 -4.1675878 -4.1754942][-4.0742331 -4.0735393 -4.070971 -4.070045 -4.0471425 -3.9749341 -3.842412 -3.7068095 -3.708065 -3.8700075 -4.025516 -4.1066642 -4.1407061 -4.1571488 -4.167326][-4.0719552 -4.0639067 -4.0514307 -4.0417557 -4.0130072 -3.9272254 -3.7707028 -3.6066771 -3.6247482 -3.8418088 -4.0234804 -4.1085658 -4.1404076 -4.1532955 -4.1585855][-4.1110315 -4.1076837 -4.0953217 -4.0788641 -4.0466356 -3.9688914 -3.8413081 -3.7216053 -3.7500942 -3.9269941 -4.073133 -4.1401997 -4.1594357 -4.1610909 -4.157259][-4.1479535 -4.1617241 -4.1554036 -4.1362882 -4.1071458 -4.0501609 -3.9685724 -3.9021726 -3.9255929 -4.0379572 -4.1359282 -4.1822791 -4.1928134 -4.1850696 -4.1715794][-4.1698933 -4.1959472 -4.1984034 -4.1815948 -4.1604843 -4.1245875 -4.0773792 -4.0397191 -4.0499859 -4.1142406 -4.1834364 -4.2156134 -4.219821 -4.2059135 -4.1855521][-4.1753645 -4.2001057 -4.2076149 -4.19697 -4.1848679 -4.1659951 -4.140089 -4.11467 -4.11242 -4.1505933 -4.2038126 -4.227633 -4.2269616 -4.2141 -4.1985526][-4.206079 -4.2214279 -4.2230916 -4.212759 -4.2055235 -4.1941328 -4.1788769 -4.1613264 -4.1559086 -4.1804709 -4.2208796 -4.2383528 -4.2359705 -4.22578 -4.2181249][-4.25018 -4.253427 -4.2461038 -4.2327065 -4.2271185 -4.2190542 -4.2103596 -4.2008357 -4.1977539 -4.2152934 -4.2415094 -4.2549253 -4.2587085 -4.2559204 -4.25294][-4.2938786 -4.290134 -4.2777572 -4.2647214 -4.2605009 -4.2551765 -4.2480369 -4.2418623 -4.2424307 -4.2546573 -4.271143 -4.284451 -4.2946672 -4.2962246 -4.2925477]]...]
INFO - root - 2017-12-07 22:49:53.380713: step 50310, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 75h:03m:37s remains)
INFO - root - 2017-12-07 22:50:03.103901: step 50320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 75h:21m:10s remains)
INFO - root - 2017-12-07 22:50:12.663424: step 50330, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 76h:45m:55s remains)
INFO - root - 2017-12-07 22:50:22.253779: step 50340, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 73h:55m:11s remains)
INFO - root - 2017-12-07 22:50:31.919179: step 50350, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 77h:39m:43s remains)
INFO - root - 2017-12-07 22:50:41.657453: step 50360, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 76h:12m:44s remains)
INFO - root - 2017-12-07 22:50:51.370871: step 50370, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 73h:45m:54s remains)
INFO - root - 2017-12-07 22:51:01.084201: step 50380, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.991 sec/batch; 77h:38m:24s remains)
INFO - root - 2017-12-07 22:51:10.706270: step 50390, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.994 sec/batch; 77h:52m:05s remains)
INFO - root - 2017-12-07 22:51:20.385041: step 50400, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 76h:49m:02s remains)
2017-12-07 22:51:21.331975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1867127 -4.1969995 -4.2043447 -4.2067013 -4.2096438 -4.21619 -4.2241697 -4.2285542 -4.2299333 -4.2316794 -4.2331719 -4.2261839 -4.2085543 -4.1910281 -4.1798272][-4.2008958 -4.2155967 -4.2279272 -4.2373128 -4.2446318 -4.2516232 -4.2557645 -4.2569132 -4.2559395 -4.2548194 -4.2530122 -4.2428088 -4.2215304 -4.1985407 -4.1833453][-4.2018595 -4.2203517 -4.23835 -4.2572732 -4.2711439 -4.2767425 -4.275701 -4.2736025 -4.2748556 -4.278028 -4.2759805 -4.2630382 -4.239747 -4.2126656 -4.191783][-4.1883078 -4.2071872 -4.2264528 -4.2489319 -4.2632875 -4.2637362 -4.2584496 -4.2571449 -4.2652321 -4.2779236 -4.2842641 -4.2777262 -4.2579451 -4.23361 -4.2091241][-4.1829195 -4.2015314 -4.213273 -4.2230711 -4.2211308 -4.2032671 -4.1885271 -4.1910572 -4.2146492 -4.2474113 -4.2721634 -4.2794652 -4.2705431 -4.2531137 -4.2284842][-4.1920371 -4.2049251 -4.1990867 -4.1809735 -4.1498313 -4.1021147 -4.0626564 -4.0638881 -4.1159792 -4.18753 -4.2434425 -4.2742257 -4.2838712 -4.2770925 -4.2552242][-4.2176971 -4.2187309 -4.1896148 -4.1374907 -4.0721979 -3.986557 -3.9036922 -3.8869352 -3.9754572 -4.1012287 -4.197298 -4.2543144 -4.2816429 -4.2853956 -4.267087][-4.239934 -4.2322206 -4.1920824 -4.1205587 -4.0330191 -3.9250157 -3.8088768 -3.768054 -3.8822141 -4.0407677 -4.155128 -4.2233047 -4.2603011 -4.2742953 -4.2663856][-4.2411342 -4.2337594 -4.2024508 -4.1407681 -4.0639749 -3.9768946 -3.8929706 -3.8597407 -3.9364071 -4.0570841 -4.1479735 -4.2046561 -4.2421627 -4.2638803 -4.2640972][-4.2296004 -4.2248554 -4.212553 -4.1798792 -4.1349087 -4.0838351 -4.0421462 -4.0188532 -4.0479951 -4.1152821 -4.1737437 -4.2107639 -4.2376084 -4.2554703 -4.2537174][-4.2232695 -4.2229609 -4.2265491 -4.221487 -4.2081609 -4.1860986 -4.1644964 -4.1409349 -4.1356297 -4.1612539 -4.1970916 -4.2246704 -4.2421389 -4.25018 -4.2444968][-4.214325 -4.2179651 -4.2314816 -4.2442126 -4.2497568 -4.2468948 -4.2411575 -4.2222 -4.2037544 -4.2034278 -4.21569 -4.2294974 -4.2374554 -4.236238 -4.2279272][-4.2071643 -4.2136011 -4.2298312 -4.2505307 -4.262362 -4.2669592 -4.2709737 -4.2603431 -4.2390118 -4.2198491 -4.2092052 -4.211091 -4.2132373 -4.2097259 -4.207849][-4.2050657 -4.2165871 -4.2351036 -4.2551794 -4.2633018 -4.2651691 -4.2697549 -4.2637672 -4.2414079 -4.2118025 -4.1868577 -4.182301 -4.1912885 -4.1976509 -4.2046475][-4.2016168 -4.2162929 -4.2331119 -4.2465777 -4.2487245 -4.2476368 -4.2495089 -4.2448397 -4.2256517 -4.1984963 -4.1764364 -4.1717467 -4.1877837 -4.2031 -4.2168264]]...]
INFO - root - 2017-12-07 22:51:31.091582: step 50410, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.027 sec/batch; 80h:26m:18s remains)
INFO - root - 2017-12-07 22:51:40.728277: step 50420, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.999 sec/batch; 78h:17m:08s remains)
INFO - root - 2017-12-07 22:51:50.302045: step 50430, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 74h:40m:03s remains)
INFO - root - 2017-12-07 22:52:00.125106: step 50440, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 74h:37m:25s remains)
INFO - root - 2017-12-07 22:52:09.641919: step 50450, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 75h:28m:29s remains)
INFO - root - 2017-12-07 22:52:19.287911: step 50460, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 76h:31m:43s remains)
INFO - root - 2017-12-07 22:52:28.954744: step 50470, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 77h:29m:49s remains)
INFO - root - 2017-12-07 22:52:38.690403: step 50480, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.982 sec/batch; 76h:57m:42s remains)
INFO - root - 2017-12-07 22:52:48.634806: step 50490, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 78h:13m:54s remains)
INFO - root - 2017-12-07 22:52:58.140726: step 50500, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 69h:17m:38s remains)
2017-12-07 22:52:59.013892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2742448 -4.2786918 -4.2866898 -4.2940149 -4.29448 -4.2872353 -4.27017 -4.2485504 -4.2362518 -4.2421622 -4.2556267 -4.2628679 -4.2605028 -4.2528644 -4.2486987][-4.2719493 -4.2761512 -4.2861071 -4.2957611 -4.2989783 -4.2923927 -4.2691193 -4.2362337 -4.2142172 -4.2183561 -4.2316394 -4.2396722 -4.2408061 -4.2342329 -4.2321553][-4.2819891 -4.2830696 -4.29029 -4.3004565 -4.3023634 -4.2917032 -4.2592726 -4.2156792 -4.1884317 -4.1964111 -4.2113657 -4.2199326 -4.2271194 -4.2291102 -4.2323942][-4.3011513 -4.3006983 -4.3030567 -4.3102837 -4.30694 -4.2889376 -4.2470016 -4.1944442 -4.1624227 -4.1749763 -4.1952038 -4.2075367 -4.2195787 -4.2316151 -4.2445464][-4.3171635 -4.3151431 -4.3159018 -4.3189554 -4.3110614 -4.2881656 -4.2388186 -4.1790452 -4.1424446 -4.1570673 -4.1828842 -4.2025251 -4.2182493 -4.2362285 -4.2570114][-4.3272491 -4.3254042 -4.3234777 -4.3201857 -4.3061242 -4.2791944 -4.2226658 -4.15733 -4.1214757 -4.1414719 -4.1760483 -4.2080369 -4.2306767 -4.2493405 -4.267242][-4.3319035 -4.3339033 -4.3293176 -4.3164091 -4.2902017 -4.2529883 -4.1867728 -4.1181288 -4.0893316 -4.1204844 -4.1679888 -4.2152982 -4.2504458 -4.2716942 -4.2822123][-4.3323622 -4.3389988 -4.3340344 -4.3136544 -4.2747917 -4.2230735 -4.1481104 -4.0794587 -4.0594 -4.0981178 -4.155623 -4.21564 -4.2650552 -4.2923431 -4.3002257][-4.33028 -4.3389707 -4.3351378 -4.3112612 -4.2676983 -4.2127895 -4.1402321 -4.0779705 -4.0589104 -4.0891719 -4.1404262 -4.2015872 -4.2601566 -4.29792 -4.3139434][-4.3274593 -4.3355279 -4.3342824 -4.3125052 -4.2717338 -4.2257619 -4.1683259 -4.1210542 -4.1018181 -4.1102676 -4.137877 -4.1845555 -4.239594 -4.2841458 -4.3125372][-4.3266773 -4.3335047 -4.3364825 -4.3224616 -4.2906103 -4.2563596 -4.2174921 -4.1843948 -4.1667681 -4.1580372 -4.160078 -4.1808128 -4.217495 -4.2571692 -4.2918015][-4.328321 -4.3329535 -4.3373685 -4.3311253 -4.3088274 -4.2819195 -4.25447 -4.228941 -4.210228 -4.1952949 -4.1854849 -4.1868649 -4.2032247 -4.2284756 -4.2588634][-4.3305645 -4.3331833 -4.3366094 -4.3337727 -4.3171206 -4.2959914 -4.275311 -4.25464 -4.2363186 -4.219728 -4.2051892 -4.196836 -4.2008109 -4.2116661 -4.2294407][-4.3307409 -4.33221 -4.3351183 -4.3328109 -4.3200231 -4.3027048 -4.2876511 -4.2740874 -4.2580137 -4.241519 -4.2260623 -4.211123 -4.2067676 -4.2093163 -4.213274][-4.3284297 -4.3284926 -4.3307972 -4.3281379 -4.3181648 -4.3054867 -4.2950277 -4.2882633 -4.2796583 -4.2675786 -4.2516794 -4.233243 -4.2187877 -4.2103214 -4.2037883]]...]
INFO - root - 2017-12-07 22:53:08.691647: step 50510, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 71h:37m:05s remains)
INFO - root - 2017-12-07 22:53:18.320849: step 50520, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 73h:47m:39s remains)
INFO - root - 2017-12-07 22:53:27.840101: step 50530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 75h:42m:40s remains)
INFO - root - 2017-12-07 22:53:37.544049: step 50540, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 76h:45m:45s remains)
INFO - root - 2017-12-07 22:53:47.238322: step 50550, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 76h:12m:33s remains)
INFO - root - 2017-12-07 22:53:56.959859: step 50560, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 78h:14m:56s remains)
INFO - root - 2017-12-07 22:54:06.569158: step 50570, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.981 sec/batch; 76h:48m:40s remains)
INFO - root - 2017-12-07 22:54:16.312158: step 50580, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 77h:08m:46s remains)
INFO - root - 2017-12-07 22:54:25.897199: step 50590, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 75h:52m:29s remains)
INFO - root - 2017-12-07 22:54:35.608843: step 50600, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 74h:36m:03s remains)
2017-12-07 22:54:36.591540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2829041 -4.2985597 -4.3065562 -4.3124828 -4.3093476 -4.2953248 -4.2771611 -4.2656302 -4.2694545 -4.2860379 -4.3016667 -4.3048205 -4.3022971 -4.3040962 -4.3071814][-4.3052859 -4.3243327 -4.3395433 -4.3489656 -4.339859 -4.3152332 -4.2839389 -4.255331 -4.2525716 -4.2718325 -4.2924595 -4.3072524 -4.3136377 -4.3181849 -4.3218675][-4.3199253 -4.3431616 -4.3615651 -4.3695607 -4.3543282 -4.3136435 -4.2618618 -4.2217774 -4.2213583 -4.2465215 -4.2739549 -4.3005977 -4.3173747 -4.3266511 -4.3327928][-4.3284259 -4.3508391 -4.3663049 -4.3682709 -4.3453569 -4.2845173 -4.210052 -4.1634851 -4.1693654 -4.2082357 -4.24649 -4.2811384 -4.3046279 -4.3173714 -4.3281822][-4.3293214 -4.3475909 -4.35791 -4.3515272 -4.3144403 -4.2378244 -4.1451983 -4.0891047 -4.1054854 -4.1648827 -4.2219119 -4.2640276 -4.2881255 -4.3018122 -4.3157644][-4.3313818 -4.3432579 -4.3438454 -4.3242426 -4.2697248 -4.1785436 -4.0606475 -3.9829183 -4.0155258 -4.111351 -4.1947846 -4.2448792 -4.2710605 -4.2889519 -4.3039551][-4.3310227 -4.3342719 -4.3218889 -4.2863183 -4.2154517 -4.1043792 -3.9466467 -3.8352156 -3.8990335 -4.0512438 -4.167747 -4.228651 -4.258678 -4.2828417 -4.2988548][-4.3279762 -4.3256931 -4.3034639 -4.2522216 -4.1658254 -4.0321407 -3.8272953 -3.6816063 -3.8001077 -4.0109344 -4.147903 -4.2145772 -4.2560282 -4.2881656 -4.3027][-4.3249683 -4.3192463 -4.2915664 -4.2305903 -4.1376162 -3.9976392 -3.79156 -3.6693733 -3.8189454 -4.0280018 -4.1504397 -4.2142711 -4.2654958 -4.3028107 -4.3144059][-4.3257008 -4.3165345 -4.285656 -4.2225022 -4.1410651 -4.0249772 -3.8827078 -3.8344378 -3.9529209 -4.0918975 -4.1740789 -4.226337 -4.2789021 -4.3135319 -4.321341][-4.3300447 -4.3162417 -4.2831206 -4.2299485 -4.1719451 -4.0962467 -4.0221653 -4.0179787 -4.0880566 -4.1550345 -4.20216 -4.2454596 -4.2914677 -4.3183241 -4.3241177][-4.3323402 -4.3137469 -4.2812362 -4.2438431 -4.2139707 -4.178967 -4.1520467 -4.1599407 -4.1870904 -4.2090144 -4.2370591 -4.2697949 -4.3014908 -4.3212967 -4.331418][-4.3328314 -4.3113017 -4.2798 -4.2552352 -4.2461977 -4.2409592 -4.2423959 -4.2509503 -4.2510448 -4.2469611 -4.2600541 -4.2847266 -4.3062868 -4.3258572 -4.3401608][-4.3360138 -4.3169818 -4.2886686 -4.2714834 -4.2714195 -4.2785444 -4.2895436 -4.296515 -4.2913828 -4.2809725 -4.2829432 -4.2964029 -4.3120079 -4.3281856 -4.3396578][-4.3385339 -4.326726 -4.3023753 -4.2864389 -4.288219 -4.2992167 -4.3144183 -4.3169761 -4.3090239 -4.2984381 -4.2938948 -4.2980275 -4.3076468 -4.3178563 -4.3210063]]...]
INFO - root - 2017-12-07 22:54:46.336762: step 50610, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 74h:08m:04s remains)
INFO - root - 2017-12-07 22:54:56.049593: step 50620, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 76h:00m:11s remains)
INFO - root - 2017-12-07 22:55:05.543708: step 50630, loss = 2.04, batch loss = 1.98 (7.8 examples/sec; 1.028 sec/batch; 80h:30m:17s remains)
INFO - root - 2017-12-07 22:55:15.171918: step 50640, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 75h:42m:15s remains)
INFO - root - 2017-12-07 22:55:24.755833: step 50650, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 75h:56m:11s remains)
INFO - root - 2017-12-07 22:55:34.539363: step 50660, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 76h:13m:35s remains)
INFO - root - 2017-12-07 22:55:44.160426: step 50670, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 75h:06m:43s remains)
INFO - root - 2017-12-07 22:55:53.731115: step 50680, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 76h:26m:23s remains)
INFO - root - 2017-12-07 22:56:03.321552: step 50690, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 74h:48m:51s remains)
INFO - root - 2017-12-07 22:56:12.932051: step 50700, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 73h:22m:23s remains)
2017-12-07 22:56:13.887239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2077751 -4.2317629 -4.2431946 -4.2408924 -4.2287183 -4.216042 -4.2103915 -4.2115769 -4.2251515 -4.2407503 -4.2398925 -4.2073836 -4.1533074 -4.1174264 -4.1253052][-4.2026577 -4.2258668 -4.2315326 -4.221096 -4.204896 -4.1944332 -4.196341 -4.2038803 -4.2189956 -4.2327833 -4.229897 -4.1974874 -4.1459236 -4.1089253 -4.1142192][-4.2043815 -4.2283969 -4.22858 -4.2069454 -4.1853309 -4.1816058 -4.1956449 -4.2113161 -4.2270522 -4.2380304 -4.2325549 -4.2013173 -4.1565866 -4.1218281 -4.1216006][-4.2219305 -4.2400188 -4.2308836 -4.194891 -4.1623473 -4.1603088 -4.1848011 -4.2122092 -4.2335887 -4.2446966 -4.2408357 -4.2145271 -4.1792588 -4.1496325 -4.1452818][-4.2387996 -4.2494183 -4.2303295 -4.180809 -4.1324339 -4.123477 -4.1520238 -4.1920586 -4.22696 -4.2456493 -4.2473688 -4.2264686 -4.19406 -4.1647458 -4.1594467][-4.2378712 -4.2449422 -4.22359 -4.16864 -4.1078839 -4.0841246 -4.10767 -4.1572537 -4.2085252 -4.2388835 -4.2450175 -4.2243118 -4.188354 -4.1579194 -4.1556969][-4.2134042 -4.221993 -4.2076416 -4.1570358 -4.0934014 -4.057591 -4.072906 -4.1252093 -4.18662 -4.225759 -4.2339306 -4.2115254 -4.1740532 -4.1460176 -4.1484604][-4.1685791 -4.1781616 -4.17444 -4.1383538 -4.0840774 -4.0470676 -4.0580854 -4.1081486 -4.1699948 -4.2116594 -4.2200727 -4.1978908 -4.1625271 -4.1389389 -4.1443577][-4.1334524 -4.13526 -4.1353359 -4.1106987 -4.0672665 -4.0358911 -4.0495486 -4.1002269 -4.1594152 -4.1999822 -4.20857 -4.1895814 -4.1603513 -4.1425648 -4.1479259][-4.1425238 -4.1279311 -4.1168723 -4.0907235 -4.0544848 -4.0341091 -4.0521359 -4.0992684 -4.1504397 -4.1850233 -4.1927967 -4.17818 -4.1566296 -4.1462522 -4.1527104][-4.1856441 -4.163547 -4.1396728 -4.1073589 -4.0775766 -4.068717 -4.085043 -4.1169081 -4.1481767 -4.1680474 -4.1708527 -4.1606197 -4.146666 -4.1433311 -4.1539483][-4.225081 -4.2120581 -4.1907864 -4.1620836 -4.1391811 -4.134563 -4.1420159 -4.1528206 -4.1602378 -4.1612911 -4.1557875 -4.1479077 -4.1374145 -4.1345782 -4.1461668][-4.2343965 -4.2378373 -4.2332516 -4.2197127 -4.2091541 -4.2069869 -4.2051077 -4.1989574 -4.1901941 -4.1773086 -4.1646681 -4.1514249 -4.1356268 -4.123827 -4.1270218][-4.2175622 -4.2327175 -4.2447195 -4.2489133 -4.2511239 -4.2528 -4.2478437 -4.2354918 -4.2220569 -4.2086082 -4.1943736 -4.1737976 -4.1446214 -4.1171985 -4.105773][-4.1904535 -4.2058134 -4.2266994 -4.245873 -4.2611461 -4.2692924 -4.2667861 -4.2555633 -4.2444963 -4.2364855 -4.2255111 -4.201736 -4.1617818 -4.11807 -4.0916533]]...]
INFO - root - 2017-12-07 22:56:23.517585: step 50710, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 77h:59m:21s remains)
INFO - root - 2017-12-07 22:56:32.953463: step 50720, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 74h:32m:16s remains)
INFO - root - 2017-12-07 22:56:42.510060: step 50730, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 75h:59m:07s remains)
INFO - root - 2017-12-07 22:56:52.239172: step 50740, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 74h:07m:10s remains)
INFO - root - 2017-12-07 22:57:01.915312: step 50750, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.007 sec/batch; 78h:47m:46s remains)
INFO - root - 2017-12-07 22:57:11.521252: step 50760, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 77h:29m:26s remains)
INFO - root - 2017-12-07 22:57:21.176570: step 50770, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 76h:11m:08s remains)
INFO - root - 2017-12-07 22:57:30.882633: step 50780, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 77h:54m:15s remains)
INFO - root - 2017-12-07 22:57:40.491093: step 50790, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 76h:50m:39s remains)
INFO - root - 2017-12-07 22:57:50.155408: step 50800, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 75h:17m:44s remains)
2017-12-07 22:57:51.090821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1820178 -4.1938248 -4.2119408 -4.2239122 -4.2222466 -4.2149796 -4.20519 -4.2078872 -4.219861 -4.2280231 -4.2312317 -4.2382412 -4.2429671 -4.24121 -4.2393923][-4.1338596 -4.138669 -4.151711 -4.1595168 -4.1550059 -4.1478038 -4.1402974 -4.1490507 -4.1722436 -4.1904311 -4.2002344 -4.2140732 -4.2247977 -4.223825 -4.2251444][-4.1006813 -4.0964861 -4.1058397 -4.1123619 -4.1082644 -4.10141 -4.0973821 -4.1093054 -4.1389933 -4.1658735 -4.1817908 -4.2010436 -4.2175078 -4.2141113 -4.2181535][-4.0854692 -4.0717859 -4.080719 -4.0913119 -4.0906825 -4.0854058 -4.0820541 -4.0924139 -4.1175556 -4.1454129 -4.1683645 -4.191524 -4.2082987 -4.2065449 -4.2141695][-4.0896449 -4.0783906 -4.093112 -4.107492 -4.1101627 -4.1020904 -4.087461 -4.0844812 -4.0962892 -4.1203127 -4.1485643 -4.1759391 -4.1977234 -4.2011719 -4.2130795][-4.1028976 -4.1021104 -4.1243868 -4.1400728 -4.14339 -4.1314478 -4.1027837 -4.0840259 -4.0809422 -4.0986338 -4.1275234 -4.159503 -4.1868238 -4.1985 -4.2130451][-4.0951548 -4.1021371 -4.1309924 -4.1474938 -4.1477623 -4.1277118 -4.0835433 -4.0535808 -4.0403595 -4.0565963 -4.0937161 -4.1375303 -4.176703 -4.2010756 -4.2174697][-4.0664439 -4.0656295 -4.0922031 -4.1046109 -4.0979195 -4.06498 -4.0067058 -3.9725056 -3.9666212 -3.9932756 -4.0477886 -4.1089134 -4.1613908 -4.1982589 -4.219461][-4.0315647 -4.0118737 -4.0259156 -4.03083 -4.0128527 -3.9657815 -3.9032803 -3.8859589 -3.9063168 -3.957428 -4.0309052 -4.1053038 -4.1642365 -4.2050848 -4.2264223][-4.018961 -3.9887877 -3.9931722 -3.9935269 -3.9710236 -3.9241946 -3.8746018 -3.8759012 -3.9168162 -3.9786048 -4.0489159 -4.1179938 -4.1697316 -4.2082691 -4.2315779][-4.0268011 -4.00029 -4.001967 -4.0019569 -3.9836164 -3.9500144 -3.923636 -3.9382672 -3.9813175 -4.0317326 -4.0868864 -4.1411443 -4.181839 -4.2169337 -4.2415509][-4.0697441 -4.0542192 -4.0547681 -4.0527625 -4.0405149 -4.0198832 -4.0058413 -4.0205064 -4.05586 -4.0905614 -4.1280608 -4.1660371 -4.1980753 -4.2307024 -4.2551608][-4.1400876 -4.1335926 -4.1338263 -4.1326542 -4.1286116 -4.1191373 -4.109724 -4.1160269 -4.138196 -4.1594596 -4.182991 -4.2076769 -4.2315817 -4.2570119 -4.2751889][-4.2177596 -4.2114391 -4.2078409 -4.2050586 -4.2038784 -4.2011971 -4.1975341 -4.2001324 -4.2115407 -4.223732 -4.2394977 -4.257165 -4.272862 -4.287446 -4.2969675][-4.2738743 -4.2670259 -4.2614136 -4.2574487 -4.2553844 -4.2529359 -4.2501068 -4.2506566 -4.2563457 -4.2634463 -4.2749205 -4.2885666 -4.2992697 -4.3069105 -4.3109713]]...]
INFO - root - 2017-12-07 22:58:00.716479: step 50810, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 74h:06m:14s remains)
INFO - root - 2017-12-07 22:58:10.264581: step 50820, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 77h:53m:54s remains)
INFO - root - 2017-12-07 22:58:19.924010: step 50830, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 75h:57m:07s remains)
INFO - root - 2017-12-07 22:58:29.370227: step 50840, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 72h:59m:28s remains)
INFO - root - 2017-12-07 22:58:39.126532: step 50850, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 75h:38m:57s remains)
INFO - root - 2017-12-07 22:58:48.698538: step 50860, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 76h:21m:44s remains)
INFO - root - 2017-12-07 22:58:58.351233: step 50870, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 77h:34m:04s remains)
INFO - root - 2017-12-07 22:59:07.987196: step 50880, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 74h:51m:48s remains)
INFO - root - 2017-12-07 22:59:17.653094: step 50890, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 72h:25m:34s remains)
INFO - root - 2017-12-07 22:59:27.331277: step 50900, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 75h:27m:35s remains)
2017-12-07 22:59:28.208788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2605505 -4.2298908 -4.2181487 -4.212091 -4.2074876 -4.2130589 -4.2236228 -4.2395811 -4.2450986 -4.2361188 -4.2224832 -4.2067547 -4.193367 -4.1915131 -4.1872115][-4.2500119 -4.2144518 -4.1995578 -4.1921568 -4.1912546 -4.2000542 -4.2156897 -4.2369637 -4.2405233 -4.2301884 -4.2188559 -4.2004662 -4.1804757 -4.1762629 -4.1749153][-4.240077 -4.2061486 -4.1940112 -4.1909003 -4.1916065 -4.1966658 -4.2123337 -4.233357 -4.2355556 -4.2299123 -4.2216511 -4.2018147 -4.1786346 -4.1715646 -4.1757479][-4.2395191 -4.2106752 -4.2028427 -4.20367 -4.2005582 -4.1974669 -4.2062845 -4.222384 -4.2257833 -4.22732 -4.224371 -4.2063289 -4.1822839 -4.1732931 -4.1827145][-4.2523236 -4.2278852 -4.2216983 -4.2201161 -4.2049074 -4.1829157 -4.1745272 -4.1835408 -4.1931176 -4.2057953 -4.2105784 -4.2010927 -4.1823316 -4.1719217 -4.1826944][-4.2562947 -4.2309809 -4.2212739 -4.2141461 -4.183619 -4.1401792 -4.1095448 -4.1083531 -4.1270132 -4.1596184 -4.1787696 -4.183301 -4.1767592 -4.16705 -4.17932][-4.2337956 -4.19904 -4.1820059 -4.1717267 -4.1344662 -4.0724564 -4.0183477 -4.0053816 -4.0341444 -4.0909696 -4.1279454 -4.1478028 -4.1602592 -4.1638837 -4.1830468][-4.2013187 -4.1556721 -4.133007 -4.1236043 -4.0878644 -4.0227566 -3.9557421 -3.9336388 -3.9712417 -4.0458193 -4.0987306 -4.1297083 -4.1570578 -4.1749458 -4.1961155][-4.195034 -4.1483355 -4.1300526 -4.131794 -4.1180062 -4.0757108 -4.0189691 -3.9949074 -4.0233479 -4.0848427 -4.1313972 -4.1568375 -4.1816468 -4.2015896 -4.2177105][-4.2110934 -4.1726403 -4.1624608 -4.1750364 -4.1847749 -4.16633 -4.12417 -4.0986967 -4.1066918 -4.1405888 -4.1726785 -4.1926279 -4.2132835 -4.2302613 -4.2376437][-4.2188015 -4.186727 -4.1818705 -4.2006006 -4.2242184 -4.2174459 -4.1874185 -4.1657863 -4.1619277 -4.1766849 -4.20255 -4.2243648 -4.2424817 -4.2529588 -4.2512507][-4.2143803 -4.1829004 -4.1769576 -4.19914 -4.2303333 -4.2313266 -4.2121129 -4.1986904 -4.1936774 -4.202981 -4.2273993 -4.2523222 -4.2667642 -4.2658572 -4.2538443][-4.2150717 -4.1828094 -4.1718788 -4.1908326 -4.2186389 -4.2243748 -4.2178226 -4.2148695 -4.2165575 -4.2272763 -4.249774 -4.2719193 -4.2761588 -4.2633681 -4.2464771][-4.2281876 -4.1955252 -4.1811156 -4.1920762 -4.2114654 -4.2182302 -4.2226238 -4.2299213 -4.2387152 -4.2494168 -4.2667632 -4.2792482 -4.2748165 -4.2577267 -4.2416711][-4.246747 -4.2135987 -4.1945248 -4.1952577 -4.2080975 -4.2179117 -4.2326927 -4.2478371 -4.2591004 -4.2684159 -4.277081 -4.2778311 -4.269629 -4.2557988 -4.2444177]]...]
INFO - root - 2017-12-07 22:59:37.840281: step 50910, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 75h:29m:08s remains)
INFO - root - 2017-12-07 22:59:47.416772: step 50920, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.010 sec/batch; 79h:00m:18s remains)
INFO - root - 2017-12-07 22:59:57.130556: step 50930, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 71h:13m:54s remains)
INFO - root - 2017-12-07 23:00:06.607699: step 50940, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 73h:51m:05s remains)
INFO - root - 2017-12-07 23:00:16.275574: step 50950, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.008 sec/batch; 78h:49m:38s remains)
INFO - root - 2017-12-07 23:00:25.933768: step 50960, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.949 sec/batch; 74h:13m:22s remains)
INFO - root - 2017-12-07 23:00:35.602350: step 50970, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 77h:03m:55s remains)
INFO - root - 2017-12-07 23:00:45.332836: step 50980, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 74h:28m:25s remains)
INFO - root - 2017-12-07 23:00:55.193611: step 50990, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 72h:52m:58s remains)
INFO - root - 2017-12-07 23:01:04.786240: step 51000, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.974 sec/batch; 76h:07m:40s remains)
2017-12-07 23:01:05.855973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2306075 -4.2061071 -4.1945753 -4.1970668 -4.2102995 -4.2262764 -4.2232742 -4.2058992 -4.191596 -4.1913152 -4.204823 -4.2203984 -4.2258396 -4.218545 -4.2008142][-4.2263675 -4.2065616 -4.2020717 -4.2121062 -4.2259626 -4.2344 -4.2214994 -4.1976681 -4.1807747 -4.1814089 -4.1962976 -4.2131653 -4.2159119 -4.2039561 -4.1808128][-4.2078934 -4.1916432 -4.1913533 -4.210103 -4.2294321 -4.2354517 -4.2180567 -4.1907787 -4.1707516 -4.1677289 -4.1792159 -4.1969156 -4.2006226 -4.1903415 -4.1666889][-4.1726818 -4.1606035 -4.1657305 -4.1910524 -4.2187543 -4.2272325 -4.2082577 -4.1807604 -4.1571193 -4.1456671 -4.1512146 -4.1685114 -4.1766253 -4.1727829 -4.1562853][-4.1181383 -4.1140027 -4.1280475 -4.1624317 -4.1947603 -4.1971288 -4.1696315 -4.1446886 -4.1260481 -4.1126676 -4.1157465 -4.1311493 -4.1391063 -4.1392927 -4.1327133][-4.0613184 -4.0662155 -4.0893164 -4.1253271 -4.1510177 -4.1351695 -4.0942607 -4.0800657 -4.0766706 -4.0689392 -4.0695133 -4.0829005 -4.0865498 -4.08466 -4.08744][-4.0062566 -4.014667 -4.0441065 -4.0792418 -4.0901289 -4.054544 -4.0023503 -4.0031109 -4.0274687 -4.0331192 -4.0292444 -4.03269 -4.0318718 -4.0291433 -4.0418911][-3.9792919 -3.981075 -4.0068932 -4.0399017 -4.0409923 -3.9993124 -3.9468393 -3.9615049 -4.0066352 -4.0197492 -4.0074525 -4.0014076 -4.0009065 -4.0020471 -4.0250416][-4.0173306 -4.0123053 -4.02885 -4.0564079 -4.0520897 -4.0139751 -3.9696009 -3.9909978 -4.0398817 -4.05136 -4.0347905 -4.0266104 -4.0236092 -4.0259871 -4.0533757][-4.0814643 -4.077 -4.0927677 -4.1159735 -4.1117973 -4.080564 -4.0438848 -4.0591278 -4.1006327 -4.1112218 -4.0984483 -4.0885487 -4.0814075 -4.0831928 -4.1081872][-4.1547523 -4.1514544 -4.1653786 -4.1821041 -4.1786013 -4.1538057 -4.1221223 -4.1313429 -4.1670284 -4.1811438 -4.1710806 -4.1615567 -4.1521425 -4.1509948 -4.1706476][-4.2271633 -4.2238054 -4.232729 -4.2454286 -4.2414842 -4.2222414 -4.2002687 -4.2084742 -4.2377114 -4.2494216 -4.2421436 -4.2307959 -4.2206745 -4.2188926 -4.2318792][-4.277173 -4.2723246 -4.2764144 -4.2849393 -4.2826967 -4.2702241 -4.2584591 -4.2656078 -4.28667 -4.2933683 -4.2876039 -4.2775445 -4.2702012 -4.2696085 -4.2757249][-4.2910109 -4.2879348 -4.2901573 -4.2949 -4.29501 -4.28911 -4.2853436 -4.2935462 -4.3092341 -4.3132787 -4.3087435 -4.3011231 -4.2968864 -4.2955914 -4.298377][-4.2905278 -4.2903008 -4.2918181 -4.2946715 -4.2973123 -4.2966852 -4.296875 -4.3053236 -4.3160033 -4.3170762 -4.3142748 -4.3107476 -4.3081331 -4.30666 -4.3077497]]...]
INFO - root - 2017-12-07 23:01:15.595063: step 51010, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 78h:42m:10s remains)
INFO - root - 2017-12-07 23:01:25.208740: step 51020, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 73h:11m:33s remains)
INFO - root - 2017-12-07 23:01:34.766061: step 51030, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 75h:02m:03s remains)
INFO - root - 2017-12-07 23:01:44.304025: step 51040, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 76h:49m:57s remains)
INFO - root - 2017-12-07 23:01:53.914631: step 51050, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.016 sec/batch; 79h:23m:55s remains)
INFO - root - 2017-12-07 23:02:03.759747: step 51060, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 76h:45m:22s remains)
INFO - root - 2017-12-07 23:02:13.507508: step 51070, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.942 sec/batch; 73h:36m:14s remains)
INFO - root - 2017-12-07 23:02:23.371807: step 51080, loss = 2.05, batch loss = 2.00 (8.0 examples/sec; 1.006 sec/batch; 78h:38m:15s remains)
INFO - root - 2017-12-07 23:02:33.149641: step 51090, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 78h:18m:17s remains)
INFO - root - 2017-12-07 23:02:42.671418: step 51100, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 76h:08m:19s remains)
2017-12-07 23:02:43.662211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.230248 -4.2636275 -4.2977715 -4.3167658 -4.3040252 -4.2602625 -4.19882 -4.135509 -4.0848465 -4.0431013 -4.0032034 -3.9827197 -4.0191231 -4.0882835 -4.1534538][-4.265492 -4.2912731 -4.3137155 -4.3196964 -4.2972765 -4.2470136 -4.1830654 -4.1294847 -4.1046462 -4.0983057 -4.0990772 -4.1074796 -4.1411185 -4.1890593 -4.2270207][-4.2946348 -4.306499 -4.3152232 -4.3113389 -4.288722 -4.2458887 -4.1928654 -4.1575294 -4.1577849 -4.1779661 -4.2001595 -4.2187476 -4.2433033 -4.2690597 -4.2863231][-4.3102684 -4.3080373 -4.3038611 -4.2944946 -4.2758179 -4.2434316 -4.2014403 -4.1778054 -4.1881084 -4.2104516 -4.2315636 -4.2531319 -4.2772274 -4.3012443 -4.319613][-4.3127232 -4.3005204 -4.2843127 -4.2673473 -4.2466526 -4.2179356 -4.1803823 -4.1572781 -4.1636395 -4.1793709 -4.1988425 -4.2289066 -4.2604451 -4.29538 -4.3249264][-4.305522 -4.2864556 -4.2606974 -4.2354579 -4.2070689 -4.1717873 -4.1265106 -4.0902176 -4.0866346 -4.1056643 -4.1392956 -4.1848717 -4.2267361 -4.2741656 -4.3166952][-4.2890668 -4.2671146 -4.23792 -4.2042928 -4.163023 -4.1110296 -4.0450029 -3.9865658 -3.9759073 -4.0185871 -4.0842633 -4.1510606 -4.2044992 -4.2598681 -4.3072104][-4.2701869 -4.2491021 -4.2170467 -4.1724429 -4.11583 -4.0476007 -3.9700732 -3.9028211 -3.898535 -3.9713547 -4.06566 -4.147285 -4.2078066 -4.2637424 -4.3071828][-4.2518816 -4.2316523 -4.196279 -4.1441655 -4.0814209 -4.0175605 -3.9596329 -3.9183764 -3.9315767 -4.01309 -4.1086531 -4.1836157 -4.236207 -4.2814412 -4.3139625][-4.2439666 -4.2235231 -4.1849742 -4.1322985 -4.0807285 -4.0504732 -4.0408511 -4.0399289 -4.0608459 -4.1206093 -4.18823 -4.236237 -4.2681503 -4.297513 -4.3197169][-4.2537346 -4.234539 -4.1980977 -4.1551061 -4.1264915 -4.1330137 -4.1597042 -4.1789737 -4.1949325 -4.2255173 -4.2566776 -4.2749357 -4.2853761 -4.2999625 -4.3148174][-4.2738132 -4.25797 -4.2293329 -4.2025614 -4.1942472 -4.2151623 -4.2473865 -4.2660584 -4.2759032 -4.2879343 -4.2934756 -4.288466 -4.28082 -4.2812891 -4.2912164][-4.2882094 -4.2740269 -4.2544985 -4.2428417 -4.246583 -4.2682395 -4.2915807 -4.303503 -4.30854 -4.3111706 -4.3003888 -4.2762451 -4.248188 -4.2357321 -4.2472038][-4.2818084 -4.2687573 -4.259686 -4.262979 -4.2755609 -4.2945743 -4.3081579 -4.3146415 -4.3150115 -4.3047767 -4.2737432 -4.2250824 -4.176929 -4.1597385 -4.1827488][-4.2655344 -4.2538729 -4.2541327 -4.267921 -4.2840991 -4.2983379 -4.3041286 -4.3012452 -4.2905722 -4.2635694 -4.2112465 -4.1423965 -4.0849977 -4.0745006 -4.115005]]...]
INFO - root - 2017-12-07 23:02:53.253329: step 51110, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 73h:10m:18s remains)
INFO - root - 2017-12-07 23:03:03.025227: step 51120, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.979 sec/batch; 76h:32m:39s remains)
INFO - root - 2017-12-07 23:03:12.627738: step 51130, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 75h:02m:41s remains)
INFO - root - 2017-12-07 23:03:22.308167: step 51140, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 69h:15m:37s remains)
INFO - root - 2017-12-07 23:03:32.073588: step 51150, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 76h:37m:40s remains)
INFO - root - 2017-12-07 23:03:41.691489: step 51160, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.974 sec/batch; 76h:07m:29s remains)
INFO - root - 2017-12-07 23:03:51.187628: step 51170, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 73h:54m:49s remains)
INFO - root - 2017-12-07 23:04:00.751042: step 51180, loss = 2.13, batch loss = 2.07 (8.7 examples/sec; 0.924 sec/batch; 72h:13m:12s remains)
INFO - root - 2017-12-07 23:04:10.555394: step 51190, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 77h:10m:51s remains)
INFO - root - 2017-12-07 23:04:20.223281: step 51200, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.974 sec/batch; 76h:04m:16s remains)
2017-12-07 23:04:21.179402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3324938 -4.3305993 -4.32934 -4.3277397 -4.3261886 -4.3240194 -4.3221526 -4.32216 -4.3239074 -4.3262982 -4.3288565 -4.3307152 -4.3306332 -4.3287048 -4.3267612][-4.3265395 -4.3245182 -4.3241034 -4.3239527 -4.3235912 -4.3211446 -4.3170786 -4.3149004 -4.3161974 -4.319912 -4.3246059 -4.3284578 -4.3304415 -4.3305655 -4.3306208][-4.3113351 -4.3105536 -4.3125415 -4.3149643 -4.3151364 -4.3104787 -4.3017492 -4.2953296 -4.2960091 -4.3021488 -4.3087091 -4.312923 -4.3152552 -4.3170867 -4.3203607][-4.3030005 -4.3017507 -4.3043594 -4.306644 -4.3050761 -4.2970009 -4.283093 -4.2721667 -4.2732549 -4.2823415 -4.2906551 -4.2938185 -4.2952 -4.297986 -4.3036108][-4.2940278 -4.29416 -4.2946877 -4.293354 -4.286911 -4.274406 -4.2547188 -4.2388825 -4.2421374 -4.2565169 -4.2699318 -4.2755327 -4.27608 -4.2763419 -4.2795596][-4.2685761 -4.270823 -4.2686129 -4.2601719 -4.24498 -4.2248206 -4.1959481 -4.1720457 -4.1780162 -4.20557 -4.2332873 -4.2488875 -4.2518687 -4.2484341 -4.2458696][-4.2337527 -4.2361746 -4.2319341 -4.2164383 -4.1912942 -4.1604209 -4.1171775 -4.077239 -4.0844097 -4.1319995 -4.1806812 -4.2097716 -4.2165179 -4.2107406 -4.2035365][-4.2048855 -4.2066092 -4.2022271 -4.1833124 -4.15207 -4.1120844 -4.0553713 -3.997869 -4.0030589 -4.06814 -4.1356654 -4.1763892 -4.187057 -4.1816325 -4.1713672][-4.205472 -4.2081518 -4.2068586 -4.1913924 -4.160069 -4.1192112 -4.0640969 -4.0113583 -4.0157561 -4.075922 -4.1407814 -4.1815171 -4.1931348 -4.1865835 -4.1701231][-4.218329 -4.222784 -4.2255111 -4.2171712 -4.1911387 -4.1584878 -4.1196451 -4.0862937 -4.0944185 -4.1369772 -4.1830864 -4.2128353 -4.2197447 -4.2106853 -4.1912141][-4.2176991 -4.2201023 -4.2280421 -4.22854 -4.2106123 -4.1872897 -4.1604619 -4.1399827 -4.1500945 -4.1822619 -4.2147512 -4.236485 -4.243557 -4.2366033 -4.2213793][-4.1971612 -4.1925464 -4.2047467 -4.2168093 -4.211164 -4.1946115 -4.170043 -4.1498613 -4.1561365 -4.18756 -4.2219129 -4.24916 -4.2646689 -4.2643142 -4.2538915][-4.1770911 -4.1621318 -4.1727109 -4.19234 -4.20202 -4.1957645 -4.1738062 -4.1470871 -4.1419597 -4.172359 -4.2117319 -4.2469416 -4.2695904 -4.27734 -4.2749209][-4.1589231 -4.1384287 -4.146162 -4.1699686 -4.1931119 -4.2027435 -4.1921787 -4.1664548 -4.1499162 -4.1713209 -4.2049422 -4.2374773 -4.2618957 -4.2747579 -4.2788262][-4.1608915 -4.1414418 -4.1470075 -4.1686578 -4.1942587 -4.2168913 -4.2231417 -4.2090321 -4.1904573 -4.1968246 -4.2125106 -4.2305679 -4.251462 -4.2651777 -4.2720647]]...]
INFO - root - 2017-12-07 23:04:30.867969: step 51210, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 71h:54m:50s remains)
INFO - root - 2017-12-07 23:04:40.519788: step 51220, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 75h:37m:38s remains)
INFO - root - 2017-12-07 23:04:50.106417: step 51230, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 76h:22m:03s remains)
INFO - root - 2017-12-07 23:04:59.651414: step 51240, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.021 sec/batch; 79h:46m:23s remains)
INFO - root - 2017-12-07 23:05:09.150457: step 51250, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.934 sec/batch; 72h:57m:59s remains)
INFO - root - 2017-12-07 23:05:18.870675: step 51260, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 75h:36m:06s remains)
INFO - root - 2017-12-07 23:05:28.670424: step 51270, loss = 2.05, batch loss = 2.00 (7.8 examples/sec; 1.028 sec/batch; 80h:17m:04s remains)
INFO - root - 2017-12-07 23:05:38.270756: step 51280, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 74h:11m:58s remains)
INFO - root - 2017-12-07 23:05:47.863291: step 51290, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 74h:13m:53s remains)
INFO - root - 2017-12-07 23:05:57.592704: step 51300, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 77h:26m:06s remains)
2017-12-07 23:05:58.563110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2931724 -4.2786322 -4.2618914 -4.2408957 -4.2142215 -4.1892262 -4.1671219 -4.1508856 -4.1457253 -4.1358085 -4.1203527 -4.1135097 -4.1131935 -4.1313276 -4.1578684][-4.2696681 -4.2476435 -4.2325807 -4.2189517 -4.2033105 -4.1917048 -4.1819096 -4.1790628 -4.1834683 -4.1749721 -4.159946 -4.1422353 -4.1251597 -4.1287751 -4.1485929][-4.2409258 -4.2189469 -4.2065411 -4.1970873 -4.1875839 -4.1863613 -4.1830568 -4.18389 -4.1948686 -4.1941962 -4.1906118 -4.1726723 -4.1472507 -4.1401472 -4.1515727][-4.2247148 -4.2100143 -4.2018862 -4.1919146 -4.1803684 -4.1754184 -4.1629815 -4.1564927 -4.175693 -4.1954222 -4.2079177 -4.1954045 -4.1690917 -4.1504879 -4.1498275][-4.227747 -4.223094 -4.2173767 -4.2017274 -4.1792121 -4.1566863 -4.1194487 -4.0928082 -4.1199765 -4.1703444 -4.2057333 -4.2065678 -4.1836119 -4.15432 -4.13814][-4.2352376 -4.2407093 -4.2347827 -4.2084641 -4.1710634 -4.1263962 -4.0500712 -3.9892721 -4.0265312 -4.1164894 -4.1767616 -4.1938152 -4.1871562 -4.1661448 -4.1402955][-4.2432208 -4.2525215 -4.2400188 -4.2007737 -4.1479216 -4.0804477 -3.9693921 -3.8723588 -3.9198165 -4.0466418 -4.1329412 -4.1688967 -4.1827617 -4.1823974 -4.158421][-4.250587 -4.2529235 -4.2303672 -4.1835136 -4.127162 -4.0568519 -3.944936 -3.8474538 -3.8949575 -4.0210538 -4.1091857 -4.1536479 -4.1783214 -4.1858697 -4.1648769][-4.2545009 -4.2423835 -4.2106256 -4.1664548 -4.1242876 -4.0783782 -4.006207 -3.94461 -3.9725688 -4.0594625 -4.1212354 -4.1507363 -4.162703 -4.1628675 -4.1446795][-4.2371426 -4.2070479 -4.1705046 -4.1408319 -4.1299295 -4.1203055 -4.093451 -4.0661392 -4.0789313 -4.1234655 -4.1480584 -4.1495137 -4.1400108 -4.1271486 -4.1113286][-4.2115021 -4.1708522 -4.1377025 -4.1266594 -4.1437349 -4.165328 -4.16984 -4.1642361 -4.1701312 -4.1849027 -4.1764655 -4.1510444 -4.1244459 -4.0992379 -4.0857711][-4.2072968 -4.1704369 -4.1466393 -4.1475239 -4.1751847 -4.2052064 -4.2187433 -4.2198067 -4.218205 -4.2162809 -4.1934252 -4.1623116 -4.1292858 -4.0981331 -4.0892973][-4.2272973 -4.2024155 -4.1862984 -4.1889577 -4.2069535 -4.2276268 -4.2380862 -4.2379003 -4.2294459 -4.2166505 -4.1961341 -4.1745796 -4.1455932 -4.1163168 -4.1087112][-4.2515006 -4.2395639 -4.2326603 -4.2290478 -4.2283664 -4.2298946 -4.2289815 -4.2246423 -4.2093363 -4.1921868 -4.1811218 -4.175168 -4.1606073 -4.1392441 -4.1358881][-4.2668524 -4.2611523 -4.2554946 -4.24603 -4.2336054 -4.2203164 -4.2102675 -4.2014227 -4.1866994 -4.168685 -4.1627331 -4.1716943 -4.1770597 -4.1699204 -4.173718]]...]
INFO - root - 2017-12-07 23:06:08.277343: step 51310, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.994 sec/batch; 77h:36m:48s remains)
INFO - root - 2017-12-07 23:06:17.775751: step 51320, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 73h:39m:20s remains)
INFO - root - 2017-12-07 23:06:27.533691: step 51330, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 75h:10m:57s remains)
INFO - root - 2017-12-07 23:06:37.267786: step 51340, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.986 sec/batch; 76h:59m:13s remains)
INFO - root - 2017-12-07 23:06:46.767424: step 51350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 72h:39m:30s remains)
INFO - root - 2017-12-07 23:06:56.463253: step 51360, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 74h:09m:06s remains)
INFO - root - 2017-12-07 23:07:06.229738: step 51370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:28m:10s remains)
INFO - root - 2017-12-07 23:07:15.946093: step 51380, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 73h:35m:47s remains)
INFO - root - 2017-12-07 23:07:25.640328: step 51390, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.014 sec/batch; 79h:08m:30s remains)
INFO - root - 2017-12-07 23:07:35.325901: step 51400, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 74h:16m:39s remains)
2017-12-07 23:07:36.298079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3057318 -4.3001695 -4.2966003 -4.2931342 -4.2883906 -4.2840786 -4.2824054 -4.2842536 -4.2902656 -4.2983384 -4.3074803 -4.3145337 -4.3164921 -4.3150315 -4.3146815][-4.2874818 -4.2798619 -4.2766209 -4.2739344 -4.2674637 -4.2589297 -4.2525544 -4.2513423 -4.2587433 -4.2724485 -4.2902946 -4.3035412 -4.3079548 -4.3075905 -4.3086195][-4.2563992 -4.245409 -4.2415266 -4.2390051 -4.2297349 -4.2145114 -4.1989503 -4.1917477 -4.2011557 -4.2237363 -4.253603 -4.2757616 -4.2842464 -4.2867274 -4.2926455][-4.2152882 -4.1999249 -4.1950779 -4.1914768 -4.1783805 -4.1575966 -4.1341205 -4.1207876 -4.1316032 -4.1623259 -4.2033567 -4.2335787 -4.2462931 -4.2538767 -4.26716][-4.1773281 -4.1595531 -4.1543579 -4.1489816 -4.1339655 -4.1099834 -4.0791931 -4.0604215 -4.0726728 -4.1120386 -4.1616783 -4.1976171 -4.2125549 -4.2228017 -4.241138][-4.1609697 -4.1444445 -4.1403813 -4.1338358 -4.1165538 -4.0872707 -4.0491123 -4.0273571 -4.0451961 -4.0917125 -4.1460357 -4.1848526 -4.2007527 -4.2097263 -4.2264886][-4.1685462 -4.1561136 -4.1571274 -4.1534657 -4.1379833 -4.1073022 -4.0658941 -4.0456853 -4.0694122 -4.1186533 -4.1720834 -4.2088075 -4.2201958 -4.2208595 -4.2286077][-4.1826043 -4.17541 -4.1853371 -4.1907363 -4.1840334 -4.158515 -4.1148572 -4.0944653 -4.1191392 -4.1670394 -4.2151504 -4.2455969 -4.2509589 -4.2434235 -4.2423668][-4.18794 -4.184978 -4.2021055 -4.215385 -4.2163959 -4.1972585 -4.1571174 -4.1389351 -4.1633511 -4.206645 -4.2463727 -4.271945 -4.2755628 -4.2660532 -4.2611084][-4.1943221 -4.1962953 -4.2149177 -4.2290568 -4.2296634 -4.2138786 -4.1827273 -4.1738596 -4.19837 -4.2347379 -4.2676382 -4.2884927 -4.2913647 -4.2826071 -4.2776542][-4.2096729 -4.216063 -4.2325563 -4.2426105 -4.2405982 -4.2284894 -4.2084217 -4.20819 -4.2319713 -4.2628 -4.2898297 -4.3062649 -4.3082886 -4.3004594 -4.2950869][-4.2401738 -4.2473235 -4.2602062 -4.2673411 -4.2647796 -4.2561955 -4.2453113 -4.2497463 -4.2705469 -4.2954879 -4.3155875 -4.3267889 -4.3273458 -4.319891 -4.3135839][-4.2807879 -4.2866178 -4.2953882 -4.3002329 -4.2992096 -4.293467 -4.2871661 -4.2908173 -4.3060842 -4.3234243 -4.3364248 -4.3422904 -4.340178 -4.3331003 -4.3262639][-4.3086271 -4.312139 -4.317162 -4.320323 -4.32108 -4.3188224 -4.3158646 -4.3177896 -4.3261595 -4.3353171 -4.3413558 -4.3425608 -4.3398528 -4.3352327 -4.3306804][-4.3215308 -4.3227773 -4.3248658 -4.3267789 -4.3282185 -4.3285837 -4.3273735 -4.3275409 -4.3305011 -4.3335347 -4.3354869 -4.3354053 -4.33411 -4.3320789 -4.33016]]...]
INFO - root - 2017-12-07 23:07:45.737163: step 51410, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 74h:41m:03s remains)
INFO - root - 2017-12-07 23:07:55.412778: step 51420, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 77h:23m:37s remains)
INFO - root - 2017-12-07 23:08:05.320827: step 51430, loss = 2.06, batch loss = 2.01 (7.7 examples/sec; 1.034 sec/batch; 80h:44m:04s remains)
INFO - root - 2017-12-07 23:08:15.008907: step 51440, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 74h:52m:02s remains)
INFO - root - 2017-12-07 23:08:24.620496: step 51450, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 73h:48m:46s remains)
INFO - root - 2017-12-07 23:08:34.185244: step 51460, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 76h:31m:24s remains)
INFO - root - 2017-12-07 23:08:43.823012: step 51470, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.960 sec/batch; 74h:58m:19s remains)
INFO - root - 2017-12-07 23:08:53.441270: step 51480, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 72h:26m:25s remains)
INFO - root - 2017-12-07 23:09:03.110736: step 51490, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 72h:40m:22s remains)
INFO - root - 2017-12-07 23:09:12.652234: step 51500, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 73h:14m:54s remains)
2017-12-07 23:09:13.570888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3084564 -4.3097525 -4.3138127 -4.3134933 -4.3095078 -4.3060441 -4.3026643 -4.3008084 -4.3022723 -4.3062348 -4.3101096 -4.3125334 -4.3129086 -4.3098164 -4.3046193][-4.3002291 -4.2989254 -4.30163 -4.3016248 -4.2995138 -4.2962742 -4.2920303 -4.2897487 -4.2916059 -4.2982559 -4.3042107 -4.3047175 -4.3031878 -4.295557 -4.2811422][-4.2838979 -4.2773085 -4.274075 -4.2725191 -4.27631 -4.27913 -4.2798371 -4.2788143 -4.2830791 -4.29318 -4.2985196 -4.2950897 -4.2876425 -4.2710786 -4.24062][-4.280149 -4.2712111 -4.2623105 -4.2580504 -4.2634077 -4.26935 -4.2751846 -4.2741766 -4.27935 -4.2961826 -4.3049655 -4.3017135 -4.2893481 -4.2612171 -4.2120275][-4.2736878 -4.2669415 -4.2567649 -4.2500691 -4.2464032 -4.2388105 -4.23469 -4.2235065 -4.2294655 -4.2619147 -4.2861371 -4.2935815 -4.286036 -4.2568612 -4.1964908][-4.2468796 -4.2490511 -4.2435584 -4.2326632 -4.2101555 -4.1667609 -4.1197972 -4.0670681 -4.0734162 -4.1449137 -4.2064195 -4.243659 -4.2604132 -4.24807 -4.1940312][-4.2083111 -4.2255096 -4.2260351 -4.2094545 -4.1652 -4.0808167 -3.9724729 -3.8496904 -3.8505654 -3.9793258 -4.0931115 -4.1689677 -4.2190151 -4.2335625 -4.2030034][-4.2012196 -4.2247519 -4.2318716 -4.21754 -4.1721625 -4.0837874 -3.9649615 -3.8239639 -3.8184924 -3.9542882 -4.0742359 -4.1509604 -4.2117262 -4.2446318 -4.2407379][-4.2144594 -4.2343445 -4.2497053 -4.2514591 -4.2282815 -4.1707177 -4.0930495 -4.0033932 -3.999644 -4.0785708 -4.1431522 -4.1878948 -4.236937 -4.2741356 -4.2843318][-4.2128005 -4.2264404 -4.2534671 -4.2766728 -4.2729588 -4.24033 -4.19642 -4.1483507 -4.1466947 -4.1787896 -4.1985617 -4.2194839 -4.2598095 -4.2922349 -4.3020539][-4.2122464 -4.2198873 -4.2535172 -4.2842474 -4.2887597 -4.2725506 -4.25274 -4.2291536 -4.2252345 -4.2318215 -4.232688 -4.2437916 -4.2753434 -4.2987614 -4.3017197][-4.2295628 -4.2339768 -4.2644792 -4.28975 -4.2940574 -4.288136 -4.2838483 -4.2740483 -4.2711229 -4.2717571 -4.2694755 -4.2721934 -4.28915 -4.297286 -4.2898326][-4.2502618 -4.2495089 -4.27193 -4.2868338 -4.28692 -4.2875209 -4.2968369 -4.3012104 -4.3039379 -4.306818 -4.304965 -4.3021026 -4.3036561 -4.2961903 -4.2793994][-4.2659469 -4.2603817 -4.267221 -4.2671967 -4.2612953 -4.2661724 -4.28277 -4.2942061 -4.30519 -4.3141065 -4.3172307 -4.3171091 -4.3142524 -4.302763 -4.281405][-4.2858133 -4.2779 -4.2708116 -4.2571549 -4.2402692 -4.2377777 -4.2473016 -4.2564778 -4.2735639 -4.2887616 -4.2999773 -4.3080831 -4.3095717 -4.3022437 -4.2838855]]...]
INFO - root - 2017-12-07 23:09:23.056582: step 51510, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 73h:22m:28s remains)
INFO - root - 2017-12-07 23:09:32.723929: step 51520, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 71h:33m:49s remains)
INFO - root - 2017-12-07 23:09:42.333637: step 51530, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 76h:33m:44s remains)
INFO - root - 2017-12-07 23:09:52.034360: step 51540, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 77h:38m:57s remains)
INFO - root - 2017-12-07 23:10:01.675094: step 51550, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 76h:15m:26s remains)
INFO - root - 2017-12-07 23:10:11.361583: step 51560, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.017 sec/batch; 79h:23m:22s remains)
INFO - root - 2017-12-07 23:10:21.180855: step 51570, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 73h:15m:09s remains)
INFO - root - 2017-12-07 23:10:30.861075: step 51580, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.018 sec/batch; 79h:28m:33s remains)
INFO - root - 2017-12-07 23:10:40.502054: step 51590, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 77h:09m:56s remains)
INFO - root - 2017-12-07 23:10:50.105298: step 51600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 73h:06m:23s remains)
2017-12-07 23:10:51.149586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3057375 -4.2973123 -4.2772675 -4.2512689 -4.2262783 -4.2100739 -4.2059932 -4.2152452 -4.2284689 -4.2365947 -4.2434196 -4.2476621 -4.2498121 -4.25048 -4.2555947][-4.307435 -4.30516 -4.2909923 -4.2700167 -4.2487264 -4.2336063 -4.2316942 -4.2436271 -4.2568827 -4.2593613 -4.2567396 -4.2512946 -4.2450972 -4.2390666 -4.240654][-4.2930045 -4.2968354 -4.2880645 -4.2736578 -4.2567143 -4.2430964 -4.2421503 -4.2545924 -4.2675376 -4.2691255 -4.2625041 -4.2527347 -4.2430949 -4.235261 -4.2342687][-4.2678652 -4.2750597 -4.26887 -4.2562847 -4.2426496 -4.2329373 -4.2358494 -4.2497587 -4.2654805 -4.2706766 -4.26422 -4.2536054 -4.2434039 -4.2377377 -4.2377124][-4.2320571 -4.2440987 -4.2397532 -4.2247853 -4.2117138 -4.2070336 -4.2168326 -4.2346492 -4.2547283 -4.2651463 -4.2626882 -4.2527866 -4.2429037 -4.2389717 -4.2399011][-4.19832 -4.2125835 -4.2064958 -4.1835251 -4.1633444 -4.1600523 -4.177331 -4.2023759 -4.2279668 -4.2465019 -4.2530742 -4.2468705 -4.2384887 -4.2383757 -4.2415237][-4.18157 -4.1917596 -4.1790681 -4.1451645 -4.1134076 -4.1045847 -4.1214375 -4.1515822 -4.1857567 -4.2172322 -4.23321 -4.2312522 -4.2264881 -4.2322493 -4.2386889][-4.1743445 -4.1808033 -4.1655369 -4.1263075 -4.0882525 -4.0666447 -4.0686426 -4.0947037 -4.1391497 -4.1854563 -4.2117605 -4.2152534 -4.21272 -4.2218227 -4.2334051][-4.180963 -4.187356 -4.1745381 -4.1383443 -4.0960383 -4.0546174 -4.025959 -4.034102 -4.0879622 -4.1531453 -4.1883612 -4.1935658 -4.1898217 -4.2002769 -4.2170181][-4.19107 -4.2001061 -4.1923571 -4.1628327 -4.1184931 -4.0599532 -3.996103 -3.9767566 -4.0375671 -4.1167331 -4.1591206 -4.1662745 -4.1627932 -4.1694646 -4.1828122][-4.182662 -4.1951079 -4.1935415 -4.1735444 -4.1376619 -4.0798707 -4.0043092 -3.9635818 -4.0097208 -4.084774 -4.1321473 -4.1458149 -4.1457243 -4.144599 -4.1475196][-4.153583 -4.1737642 -4.1824727 -4.1756749 -4.1536703 -4.1127138 -4.0535665 -4.0083637 -4.0201769 -4.0698013 -4.1143308 -4.1385927 -4.1431823 -4.1342988 -4.1224756][-4.1244903 -4.1517625 -4.16844 -4.1755352 -4.1695409 -4.1460948 -4.1018271 -4.0585642 -4.0436382 -4.0674415 -4.105175 -4.1362834 -4.1477323 -4.1381445 -4.1192961][-4.1101131 -4.1344786 -4.15315 -4.1698217 -4.1750293 -4.1601367 -4.1256056 -4.0885363 -4.0635171 -4.0672989 -4.0958829 -4.1293902 -4.1512089 -4.1509137 -4.1345158][-4.1043825 -4.11761 -4.1353974 -4.157259 -4.17016 -4.1622639 -4.1411071 -4.1134219 -4.0883989 -4.0793815 -4.09301 -4.1202588 -4.147223 -4.1558461 -4.1418552]]...]
INFO - root - 2017-12-07 23:11:00.982464: step 51610, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.992 sec/batch; 77h:22m:27s remains)
INFO - root - 2017-12-07 23:11:10.547945: step 51620, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 78h:49m:19s remains)
INFO - root - 2017-12-07 23:11:20.154092: step 51630, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.935 sec/batch; 72h:55m:37s remains)
INFO - root - 2017-12-07 23:11:30.136769: step 51640, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.010 sec/batch; 78h:47m:46s remains)
INFO - root - 2017-12-07 23:11:39.789952: step 51650, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 73h:15m:51s remains)
INFO - root - 2017-12-07 23:11:49.613124: step 51660, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 77h:23m:31s remains)
INFO - root - 2017-12-07 23:11:59.352445: step 51670, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 73h:41m:59s remains)
INFO - root - 2017-12-07 23:12:08.893880: step 51680, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.000 sec/batch; 78h:01m:56s remains)
INFO - root - 2017-12-07 23:12:18.512525: step 51690, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 76h:28m:48s remains)
INFO - root - 2017-12-07 23:12:28.159911: step 51700, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 75h:48m:31s remains)
2017-12-07 23:12:29.107462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2268386 -4.2072949 -4.1745639 -4.1569071 -4.15386 -4.1546712 -4.1635642 -4.1814604 -4.1792974 -4.1340108 -4.082037 -4.0625854 -4.0917993 -4.1519566 -4.2089157][-4.2422228 -4.2126126 -4.1763048 -4.1560793 -4.1443224 -4.1305842 -4.1272588 -4.141057 -4.1442232 -4.1068544 -4.070097 -4.0638113 -4.0974483 -4.1533647 -4.2046623][-4.2552223 -4.2259569 -4.1917195 -4.1710062 -4.1460519 -4.1136603 -4.0967946 -4.10547 -4.11786 -4.09124 -4.06842 -4.0749855 -4.1078362 -4.15857 -4.20568][-4.2613149 -4.2382789 -4.2109218 -4.1893 -4.1535649 -4.1037965 -4.0678263 -4.0672493 -4.0913544 -4.081677 -4.0702639 -4.0854254 -4.1194916 -4.168725 -4.2136846][-4.2596354 -4.2425141 -4.2202411 -4.1980562 -4.1521964 -4.0835991 -4.0199766 -4.0062323 -4.0475225 -4.0626259 -4.0658832 -4.092073 -4.1292167 -4.1783905 -4.2218738][-4.2534814 -4.2392855 -4.2193389 -4.1938252 -4.135407 -4.0431914 -3.9463658 -3.9090912 -3.9707792 -4.0178456 -4.0421548 -4.0831556 -4.1247826 -4.1742597 -4.2174354][-4.2499018 -4.2415729 -4.224771 -4.1978183 -4.1279321 -4.0070791 -3.8733244 -3.8042963 -3.883029 -3.9626818 -4.0116482 -4.0696564 -4.1162534 -4.1646509 -4.2080536][-4.2653327 -4.2625675 -4.2510524 -4.2264814 -4.1574774 -4.0293517 -3.8904643 -3.8120675 -3.8844109 -3.970679 -4.0256305 -4.0877643 -4.1307931 -4.1741381 -4.21401][-4.2944322 -4.2945924 -4.2867603 -4.2686315 -4.2122316 -4.1045413 -3.9972689 -3.9353518 -3.9837215 -4.0458975 -4.0844669 -4.1358352 -4.168468 -4.2039785 -4.2358017][-4.31843 -4.318223 -4.3088956 -4.2938819 -4.2527027 -4.1762033 -4.1078076 -4.0627851 -4.0908318 -4.1267667 -4.1426497 -4.176496 -4.2003059 -4.2301164 -4.2542987][-4.3293934 -4.3281808 -4.315167 -4.3023205 -4.2727704 -4.2212486 -4.1787925 -4.1445541 -4.1582203 -4.1784105 -4.1784835 -4.1958766 -4.2126994 -4.2379012 -4.2567167][-4.3282533 -4.3287849 -4.3161573 -4.3053117 -4.2822118 -4.2430506 -4.2143016 -4.190177 -4.1983213 -4.2091756 -4.1991138 -4.2060056 -4.2179127 -4.2385969 -4.2536063][-4.3303981 -4.3324695 -4.3227921 -4.314765 -4.2966323 -4.2669072 -4.2466545 -4.2335262 -4.2399807 -4.2422314 -4.2265229 -4.2260408 -4.2336473 -4.2475924 -4.2567954][-4.3414297 -4.3456903 -4.3390651 -4.3314867 -4.3160305 -4.2926784 -4.2782645 -4.2695246 -4.2751775 -4.2731709 -4.255466 -4.2501416 -4.2539954 -4.262949 -4.2670431][-4.3547921 -4.3605504 -4.3543715 -4.3453994 -4.3305454 -4.3109231 -4.2989874 -4.2925916 -4.2979178 -4.294426 -4.279964 -4.2739658 -4.2753949 -4.2816882 -4.2835321]]...]
INFO - root - 2017-12-07 23:12:38.893057: step 51710, loss = 2.09, batch loss = 2.03 (7.6 examples/sec; 1.057 sec/batch; 82h:28m:00s remains)
INFO - root - 2017-12-07 23:12:48.519965: step 51720, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 76h:21m:02s remains)
INFO - root - 2017-12-07 23:12:58.089528: step 51730, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 72h:53m:43s remains)
INFO - root - 2017-12-07 23:13:07.767789: step 51740, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 77h:21m:25s remains)
INFO - root - 2017-12-07 23:13:17.516025: step 51750, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 76h:16m:29s remains)
INFO - root - 2017-12-07 23:13:27.217577: step 51760, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.998 sec/batch; 77h:48m:13s remains)
INFO - root - 2017-12-07 23:13:36.979576: step 51770, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 77h:08m:50s remains)
INFO - root - 2017-12-07 23:13:46.674261: step 51780, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 74h:54m:59s remains)
INFO - root - 2017-12-07 23:13:56.388611: step 51790, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 76h:22m:09s remains)
INFO - root - 2017-12-07 23:14:05.870808: step 51800, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.952 sec/batch; 74h:12m:32s remains)
2017-12-07 23:14:06.801861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3283038 -4.3232884 -4.3132672 -4.2961154 -4.2764926 -4.2587204 -4.2442956 -4.2483535 -4.2650895 -4.2820692 -4.2916017 -4.2883816 -4.2806973 -4.290133 -4.2984867][-4.318778 -4.3143859 -4.3042879 -4.2830129 -4.2518744 -4.2200232 -4.1924047 -4.1922178 -4.2111769 -4.2312613 -4.2450733 -4.2486444 -4.2499418 -4.2738023 -4.2961359][-4.2943077 -4.2911787 -4.2855573 -4.266036 -4.2302103 -4.1885962 -4.1491265 -4.1431384 -4.1611905 -4.1814313 -4.1959395 -4.2039952 -4.2138925 -4.2454329 -4.2766886][-4.2482648 -4.2393303 -4.2394834 -4.2299342 -4.2024341 -4.1648722 -4.1282959 -4.1206837 -4.1360712 -4.1491928 -4.1537156 -4.1579003 -4.1671157 -4.1973114 -4.2338715][-4.2181754 -4.1982484 -4.1912284 -4.1850882 -4.1694 -4.1477718 -4.1266651 -4.1228647 -4.1366086 -4.1456084 -4.1359639 -4.1242666 -4.1196308 -4.1404729 -4.1780005][-4.2177782 -4.1929278 -4.1711593 -4.1555614 -4.143785 -4.138988 -4.1379061 -4.1428318 -4.1560125 -4.1621733 -4.1466861 -4.1265607 -4.1115785 -4.1242652 -4.1553888][-4.236671 -4.2208934 -4.19313 -4.1633425 -4.141645 -4.1388841 -4.151083 -4.1650834 -4.1812944 -4.1863928 -4.170155 -4.14921 -4.136095 -4.1518621 -4.1793942][-4.2509961 -4.25219 -4.2365217 -4.2071486 -4.1756697 -4.1642346 -4.1746397 -4.191658 -4.2089982 -4.2148004 -4.1965261 -4.1697845 -4.155055 -4.1745119 -4.2033653][-4.2365065 -4.25605 -4.2600851 -4.25037 -4.227046 -4.2131677 -4.2178349 -4.2268138 -4.2425122 -4.2495513 -4.2317834 -4.1989412 -4.1781969 -4.1924539 -4.2168546][-4.2127471 -4.2368464 -4.248198 -4.2498288 -4.2409587 -4.2346215 -4.2398005 -4.2466159 -4.2630453 -4.2760425 -4.2673 -4.2365527 -4.2108665 -4.2138219 -4.2252741][-4.2007627 -4.2182631 -4.2289391 -4.2348213 -4.2354445 -4.2355781 -4.2409077 -4.2463112 -4.2652917 -4.285924 -4.2874765 -4.2709284 -4.2520247 -4.2470946 -4.245718][-4.2078462 -4.2192473 -4.2244582 -4.2322841 -4.2404466 -4.2419753 -4.2405853 -4.2400336 -4.2540746 -4.2775688 -4.2877007 -4.2845964 -4.27664 -4.2792563 -4.279562][-4.2077246 -4.21265 -4.2170477 -4.2234983 -4.2304349 -4.2324467 -4.232563 -4.2349648 -4.2455688 -4.2658954 -4.2778792 -4.2810178 -4.2806606 -4.2910852 -4.2982321][-4.219954 -4.2203317 -4.2203069 -4.2186427 -4.214942 -4.2092047 -4.2107081 -4.2212119 -4.236239 -4.2549977 -4.2667255 -4.2720771 -4.2754784 -4.2910838 -4.3008938][-4.2371025 -4.2346683 -4.2314348 -4.2217169 -4.2075753 -4.1930337 -4.1920843 -4.2054048 -4.22158 -4.237421 -4.2452369 -4.245007 -4.24592 -4.2672048 -4.2875156]]...]
INFO - root - 2017-12-07 23:14:16.478933: step 51810, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.981 sec/batch; 76h:27m:34s remains)
INFO - root - 2017-12-07 23:14:26.209907: step 51820, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 77h:19m:32s remains)
INFO - root - 2017-12-07 23:14:35.840029: step 51830, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 75h:08m:38s remains)
INFO - root - 2017-12-07 23:14:45.386970: step 51840, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 70h:13m:31s remains)
INFO - root - 2017-12-07 23:14:54.876387: step 51850, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 74h:46m:31s remains)
INFO - root - 2017-12-07 23:15:04.366982: step 51860, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 75h:41m:48s remains)
INFO - root - 2017-12-07 23:15:14.137293: step 51870, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.009 sec/batch; 78h:38m:35s remains)
INFO - root - 2017-12-07 23:15:23.657740: step 51880, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 74h:28m:16s remains)
INFO - root - 2017-12-07 23:15:33.314293: step 51890, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 76h:24m:06s remains)
INFO - root - 2017-12-07 23:15:42.999677: step 51900, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.927 sec/batch; 72h:14m:54s remains)
2017-12-07 23:15:44.006351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2323632 -4.2462969 -4.2644668 -4.2784891 -4.2937341 -4.3006291 -4.2949739 -4.2808843 -4.2745342 -4.2655449 -4.2532 -4.2456856 -4.2415495 -4.24592 -4.2568059][-4.1938133 -4.2047157 -4.2196522 -4.235312 -4.2540336 -4.2662749 -4.2649813 -4.2542644 -4.2556915 -4.25251 -4.2429 -4.2379451 -4.2368655 -4.2458582 -4.2573752][-4.1800566 -4.1849322 -4.1944923 -4.2090793 -4.2288041 -4.2424779 -4.2417765 -4.2339931 -4.2418404 -4.2453561 -4.2398276 -4.2375145 -4.2390251 -4.2500529 -4.2628245][-4.1714659 -4.1721916 -4.1817465 -4.1932507 -4.210794 -4.2220082 -4.2167368 -4.2104225 -4.2288542 -4.2423377 -4.2382221 -4.2381778 -4.2421641 -4.2539487 -4.2672491][-4.1487241 -4.1387548 -4.1417346 -4.1433563 -4.1506405 -4.1551881 -4.1378632 -4.1270666 -4.1583214 -4.1886415 -4.1961842 -4.2062464 -4.2207069 -4.2391019 -4.2561007][-4.0638242 -4.0511413 -4.0520811 -4.0465412 -4.0423779 -4.0315113 -3.9922335 -3.9726782 -4.0198445 -4.0767841 -4.1096134 -4.1440792 -4.1804509 -4.2158728 -4.2423015][-3.957139 -3.9385843 -3.9330897 -3.9246097 -3.9082146 -3.8779187 -3.8169379 -3.7914708 -3.861284 -3.9514856 -4.0205283 -4.0855293 -4.1445074 -4.197525 -4.2337542][-3.9777 -3.9592755 -3.9376855 -3.9104981 -3.8733168 -3.8211732 -3.7485335 -3.7177548 -3.7904322 -3.8913474 -3.9761386 -4.0564981 -4.1279035 -4.1895604 -4.2305107][-4.0895247 -4.0785117 -4.0548587 -4.0248837 -3.9928107 -3.9520679 -3.8966217 -3.870101 -3.9195294 -3.9888198 -4.0414104 -4.0956016 -4.150485 -4.1991849 -4.2345786][-4.1893754 -4.1825824 -4.1650105 -4.140583 -4.1186647 -4.0911379 -4.0529318 -4.0311923 -4.05987 -4.0998116 -4.1221709 -4.1483703 -4.1826906 -4.2154841 -4.2429714][-4.258656 -4.2549777 -4.2438717 -4.2250133 -4.2073765 -4.1837234 -4.1533241 -4.1341209 -4.1474972 -4.1701903 -4.1806283 -4.1927738 -4.2136245 -4.2364855 -4.2550077][-4.3169394 -4.3177905 -4.3124366 -4.3023491 -4.2935395 -4.2793236 -4.2561774 -4.2414727 -4.2435422 -4.2509828 -4.2528257 -4.2562013 -4.2661471 -4.2765422 -4.2824435][-4.3281813 -4.3314424 -4.3324108 -4.3306813 -4.3293147 -4.3239503 -4.3113694 -4.3044848 -4.3066607 -4.3090453 -4.3057036 -4.3045716 -4.306757 -4.3089228 -4.3076105][-4.3218389 -4.3226318 -4.3237281 -4.3238688 -4.3246012 -4.3231816 -4.3156357 -4.3131552 -4.3213992 -4.32998 -4.3320675 -4.3345909 -4.3354778 -4.3330469 -4.3272305][-4.3181133 -4.3174872 -4.316884 -4.3155661 -4.3149538 -4.312603 -4.3033671 -4.2990136 -4.3079886 -4.3193517 -4.327023 -4.3351769 -4.3387933 -4.3364768 -4.3317862]]...]
INFO - root - 2017-12-07 23:15:53.843459: step 51910, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 76h:05m:57s remains)
INFO - root - 2017-12-07 23:16:03.690750: step 51920, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 75h:06m:53s remains)
INFO - root - 2017-12-07 23:16:13.331723: step 51930, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 76h:51m:06s remains)
INFO - root - 2017-12-07 23:16:23.083169: step 51940, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 76h:13m:48s remains)
INFO - root - 2017-12-07 23:16:32.800123: step 51950, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 77h:16m:01s remains)
INFO - root - 2017-12-07 23:16:42.512954: step 51960, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 74h:15m:12s remains)
INFO - root - 2017-12-07 23:16:52.185688: step 51970, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 68h:47m:26s remains)
INFO - root - 2017-12-07 23:17:01.971316: step 51980, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 75h:57m:51s remains)
INFO - root - 2017-12-07 23:17:11.537348: step 51990, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 73h:57m:33s remains)
INFO - root - 2017-12-07 23:17:21.127136: step 52000, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.991 sec/batch; 77h:13m:10s remains)
2017-12-07 23:17:22.076536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3286667 -4.3338141 -4.337822 -4.3381739 -4.3341851 -4.3251166 -4.3143678 -4.3041825 -4.2976 -4.2983441 -4.306417 -4.3189607 -4.3297052 -4.33365 -4.3300667][-4.3193 -4.3244905 -4.3268919 -4.3229775 -4.3121662 -4.29694 -4.281446 -4.2675424 -4.2574315 -4.2590532 -4.2717443 -4.2923 -4.3118896 -4.3218021 -4.3197489][-4.3148179 -4.3182631 -4.3169441 -4.3049426 -4.2855816 -4.2631488 -4.2443218 -4.2308054 -4.219203 -4.2229486 -4.2423449 -4.271163 -4.2983737 -4.3128829 -4.31239][-4.3128242 -4.312943 -4.3063931 -4.2853823 -4.2552304 -4.2246761 -4.2021184 -4.1894336 -4.1784053 -4.1839032 -4.2103972 -4.2489595 -4.2841935 -4.30394 -4.3054304][-4.30797 -4.3054881 -4.2930832 -4.2593217 -4.209693 -4.157918 -4.1223969 -4.104435 -4.0880861 -4.0977154 -4.144423 -4.2031193 -4.2529845 -4.2834558 -4.2928658][-4.3009357 -4.2952285 -4.2773366 -4.2318635 -4.161016 -4.0804963 -4.0161371 -3.9759281 -3.9422934 -3.956985 -4.0404592 -4.1374326 -4.2117748 -4.2595811 -4.2812061][-4.2932944 -4.2858205 -4.2692471 -4.2270732 -4.1552372 -4.0610652 -3.9642329 -3.8862431 -3.8258948 -3.8412855 -3.9556522 -4.0881596 -4.1853328 -4.2482939 -4.2810893][-4.2770319 -4.2676635 -4.2597475 -4.2389388 -4.1959853 -4.1236973 -4.0255718 -3.9268224 -3.8493712 -3.8523269 -3.9590173 -4.092803 -4.1912761 -4.2520862 -4.2858582][-4.2538457 -4.2439213 -4.2482014 -4.2542796 -4.2494769 -4.2159624 -4.1450405 -4.0561085 -3.9808667 -3.9697092 -4.0399384 -4.1419659 -4.2173548 -4.26031 -4.2842088][-4.2194858 -4.2122006 -4.2279487 -4.2558212 -4.2767596 -4.2723083 -4.2324619 -4.1699834 -4.1128206 -4.0945415 -4.13077 -4.1939483 -4.2390442 -4.2588878 -4.2693753][-4.1789584 -4.1712179 -4.1970558 -4.2407737 -4.2763915 -4.28993 -4.2784352 -4.244729 -4.2091031 -4.1902223 -4.2006269 -4.2309766 -4.2504268 -4.2533469 -4.2520785][-4.0943589 -4.0751362 -4.1153288 -4.1856894 -4.2406459 -4.2711205 -4.2828941 -4.2766 -4.2665496 -4.2581582 -4.2560263 -4.2624125 -4.2632923 -4.2535625 -4.2439208][-3.9826336 -3.9405167 -3.9927804 -4.0975342 -4.1803279 -4.2310696 -4.2643156 -4.2817259 -4.2974172 -4.3073945 -4.3050246 -4.2957234 -4.2792044 -4.2573428 -4.2419248][-3.8903682 -3.829423 -3.8924942 -4.0308537 -4.144258 -4.2124791 -4.2563195 -4.2816696 -4.3049927 -4.3278847 -4.3333921 -4.3217015 -4.2977304 -4.2696605 -4.251071][-3.905304 -3.8536103 -3.9139624 -4.0473194 -4.160008 -4.2250853 -4.2622452 -4.282023 -4.3030477 -4.3279366 -4.339992 -4.3371234 -4.319891 -4.2931318 -4.2709208]]...]
INFO - root - 2017-12-07 23:17:31.740263: step 52010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 73h:40m:17s remains)
INFO - root - 2017-12-07 23:17:41.335043: step 52020, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 75h:54m:04s remains)
INFO - root - 2017-12-07 23:17:51.040884: step 52030, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:19m:49s remains)
INFO - root - 2017-12-07 23:18:00.750920: step 52040, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 74h:11m:57s remains)
INFO - root - 2017-12-07 23:18:10.499749: step 52050, loss = 2.11, batch loss = 2.05 (7.7 examples/sec; 1.035 sec/batch; 80h:35m:36s remains)
INFO - root - 2017-12-07 23:18:20.271773: step 52060, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.958 sec/batch; 74h:39m:28s remains)
INFO - root - 2017-12-07 23:18:29.930288: step 52070, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 73h:38m:24s remains)
INFO - root - 2017-12-07 23:18:39.510099: step 52080, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.999 sec/batch; 77h:46m:56s remains)
INFO - root - 2017-12-07 23:18:49.071287: step 52090, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.965 sec/batch; 75h:10m:04s remains)
INFO - root - 2017-12-07 23:18:58.848960: step 52100, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 75h:51m:07s remains)
2017-12-07 23:18:59.779644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2048869 -4.2109165 -4.2171426 -4.217751 -4.216011 -4.2185116 -4.218503 -4.2043543 -4.1821685 -4.1701183 -4.1738081 -4.188798 -4.2104888 -4.2284904 -4.2354541][-4.1912432 -4.1995783 -4.2089095 -4.2115312 -4.2118568 -4.2141066 -4.2142019 -4.200458 -4.1816869 -4.1753111 -4.177258 -4.1897225 -4.212008 -4.2327452 -4.2401567][-4.1976161 -4.201889 -4.2099166 -4.2137637 -4.2156897 -4.22149 -4.2255759 -4.2156577 -4.20617 -4.2092075 -4.2132616 -4.2188907 -4.234499 -4.2524595 -4.2566309][-4.2231259 -4.2240553 -4.2317548 -4.2355618 -4.2344408 -4.2346954 -4.2308674 -4.2207265 -4.222743 -4.2408471 -4.2519112 -4.2530265 -4.2585745 -4.2671056 -4.265749][-4.2608223 -4.256155 -4.2587423 -4.256053 -4.2447529 -4.2293086 -4.2031727 -4.1876893 -4.2072134 -4.2440748 -4.2665248 -4.2715445 -4.2761693 -4.27432 -4.26663][-4.2787971 -4.2663956 -4.2634706 -4.252183 -4.2264128 -4.18207 -4.1176572 -4.0918946 -4.1422119 -4.2087145 -4.2496219 -4.26451 -4.27269 -4.2698503 -4.2596078][-4.269567 -4.2505903 -4.2419839 -4.2208605 -4.1779175 -4.0981064 -3.9829729 -3.9396527 -4.0377264 -4.1489811 -4.2115374 -4.2422915 -4.26106 -4.2633152 -4.2534051][-4.2307329 -4.2087655 -4.197576 -4.1750011 -4.1293163 -4.0317788 -3.8867083 -3.840265 -3.9824705 -4.1185374 -4.1892853 -4.2272491 -4.2512283 -4.2515516 -4.238472][-4.1773839 -4.1631813 -4.163404 -4.1565003 -4.1294489 -4.0518761 -3.9425669 -3.9236002 -4.0337415 -4.1308322 -4.1781173 -4.2084703 -4.2275715 -4.2207584 -4.2066298][-4.159946 -4.1561117 -4.1682048 -4.179584 -4.1728606 -4.12536 -4.0619326 -4.0567937 -4.1135039 -4.1577082 -4.1816311 -4.2058868 -4.2192841 -4.2110481 -4.2021537][-4.1820993 -4.1825757 -4.1923165 -4.2094 -4.2109022 -4.1810684 -4.1501989 -4.1561074 -4.1831717 -4.20106 -4.2136016 -4.2314377 -4.2380567 -4.233264 -4.23168][-4.2188692 -4.2173328 -4.2241006 -4.2413912 -4.2416997 -4.2208872 -4.2100244 -4.2245054 -4.2406106 -4.2502189 -4.2564869 -4.2687511 -4.2710166 -4.2738862 -4.2784328][-4.2559762 -4.2555361 -4.2631569 -4.2773962 -4.2724843 -4.2570968 -4.2599964 -4.27749 -4.2888227 -4.2934766 -4.2980647 -4.3032975 -4.3028145 -4.312314 -4.3183465][-4.2872605 -4.2860823 -4.2937651 -4.3027267 -4.2957997 -4.2868919 -4.2967887 -4.313231 -4.3192577 -4.3184395 -4.3174434 -4.3109031 -4.3064756 -4.316184 -4.3231945][-4.3017054 -4.2981887 -4.3029652 -4.308145 -4.3036342 -4.3008103 -4.3117647 -4.3227377 -4.3253961 -4.3227291 -4.3164177 -4.3001747 -4.2918677 -4.3022618 -4.3144298]]...]
INFO - root - 2017-12-07 23:19:09.463841: step 52110, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.003 sec/batch; 78h:06m:02s remains)
INFO - root - 2017-12-07 23:19:19.274886: step 52120, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 77h:54m:42s remains)
INFO - root - 2017-12-07 23:19:28.918497: step 52130, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 74h:42m:03s remains)
INFO - root - 2017-12-07 23:19:38.638712: step 52140, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.977 sec/batch; 76h:02m:52s remains)
INFO - root - 2017-12-07 23:19:48.263844: step 52150, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.968 sec/batch; 75h:21m:00s remains)
INFO - root - 2017-12-07 23:19:58.138292: step 52160, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 76h:59m:39s remains)
INFO - root - 2017-12-07 23:20:07.733435: step 52170, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 74h:34m:35s remains)
INFO - root - 2017-12-07 23:20:17.143944: step 52180, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 52h:54m:32s remains)
INFO - root - 2017-12-07 23:20:26.816466: step 52190, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 78h:41m:24s remains)
INFO - root - 2017-12-07 23:20:36.443875: step 52200, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 77h:38m:48s remains)
2017-12-07 23:20:37.469316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2711554 -4.2695737 -4.2694726 -4.2724061 -4.2646341 -4.2502003 -4.241096 -4.239913 -4.2501807 -4.2655792 -4.2766786 -4.2829614 -4.2907095 -4.29489 -4.2884765][-4.24877 -4.2482276 -4.2455921 -4.247592 -4.2447057 -4.2251992 -4.2086573 -4.2005768 -4.2155485 -4.239171 -4.2590847 -4.2678418 -4.2784805 -4.2856174 -4.28168][-4.2329926 -4.23256 -4.2285094 -4.228909 -4.2305427 -4.2108512 -4.1851478 -4.1677628 -4.1856041 -4.2186465 -4.247242 -4.2637725 -4.2722077 -4.2771997 -4.2739244][-4.2417741 -4.2393794 -4.2316294 -4.2298107 -4.2305865 -4.2059884 -4.1654243 -4.1355762 -4.1594453 -4.207891 -4.2454605 -4.2687507 -4.2788815 -4.2802606 -4.2738][-4.2597795 -4.256988 -4.247241 -4.2456412 -4.2396708 -4.200736 -4.1338673 -4.0798306 -4.1158128 -4.1885786 -4.2403374 -4.2688837 -4.2836013 -4.2849307 -4.2779059][-4.2725663 -4.2771597 -4.2703223 -4.2647438 -4.2462921 -4.1884861 -4.080996 -3.9867077 -4.0430574 -4.1506071 -4.2192492 -4.2522955 -4.2747259 -4.2834334 -4.2814689][-4.2833056 -4.2943425 -4.2914433 -4.2800069 -4.2505527 -4.1781921 -4.0465212 -3.9163747 -3.985395 -4.1215868 -4.2008452 -4.2318726 -4.2537875 -4.2699051 -4.273469][-4.3010736 -4.31144 -4.3096967 -4.29431 -4.2617154 -4.1923747 -4.0781975 -3.9690673 -4.0205231 -4.1433845 -4.2165074 -4.2374187 -4.2476549 -4.2592168 -4.2642326][-4.3205595 -4.3224621 -4.314671 -4.2978415 -4.2718658 -4.2194896 -4.1440272 -4.0781841 -4.1084027 -4.1900086 -4.2437539 -4.256114 -4.2592931 -4.2644944 -4.2678609][-4.3335528 -4.3260846 -4.3136454 -4.2998457 -4.2818227 -4.2482452 -4.2056637 -4.1726274 -4.18728 -4.2334552 -4.2692909 -4.2803659 -4.2820234 -4.286356 -4.2848644][-4.3376288 -4.3266954 -4.3151894 -4.3057756 -4.2940731 -4.2769814 -4.2540479 -4.2401733 -4.2447286 -4.2688928 -4.28838 -4.2958465 -4.2964444 -4.3004427 -4.2993298][-4.335084 -4.3227954 -4.3158646 -4.3127127 -4.3098583 -4.3040843 -4.2903886 -4.2821608 -4.2809973 -4.2939949 -4.3035903 -4.30522 -4.303154 -4.3057995 -4.30672][-4.3328848 -4.3221674 -4.3184223 -4.3200006 -4.3203211 -4.316824 -4.3094954 -4.305037 -4.3014503 -4.3064137 -4.31187 -4.3105621 -4.3077664 -4.3104911 -4.3127556][-4.3360763 -4.329433 -4.3280182 -4.3319817 -4.3336253 -4.3314209 -4.3289318 -4.3260827 -4.3224359 -4.3225546 -4.3241067 -4.322835 -4.3220229 -4.3251328 -4.32814][-4.3429241 -4.3396616 -4.3398438 -4.343739 -4.3470993 -4.3475966 -4.3476005 -4.3460188 -4.3428068 -4.3401 -4.3392081 -4.3379955 -4.3378825 -4.3398557 -4.3422809]]...]
INFO - root - 2017-12-07 23:20:47.170679: step 52210, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 75h:31m:18s remains)
INFO - root - 2017-12-07 23:20:56.873067: step 52220, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.936 sec/batch; 72h:50m:22s remains)
INFO - root - 2017-12-07 23:21:06.526009: step 52230, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 76h:26m:39s remains)
INFO - root - 2017-12-07 23:21:16.129866: step 52240, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 73h:57m:47s remains)
INFO - root - 2017-12-07 23:21:25.941295: step 52250, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 71h:58m:46s remains)
INFO - root - 2017-12-07 23:21:35.648060: step 52260, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 77h:01m:50s remains)
INFO - root - 2017-12-07 23:21:45.189981: step 52270, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 74h:40m:45s remains)
INFO - root - 2017-12-07 23:21:54.701737: step 52280, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 70h:27m:41s remains)
INFO - root - 2017-12-07 23:22:04.406632: step 52290, loss = 2.08, batch loss = 2.03 (7.8 examples/sec; 1.022 sec/batch; 79h:33m:38s remains)
INFO - root - 2017-12-07 23:22:14.052431: step 52300, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 75h:21m:35s remains)
2017-12-07 23:22:15.037255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3615193 -4.350009 -4.3393288 -4.332356 -4.329248 -4.3285446 -4.3276362 -4.3264141 -4.3291125 -4.3363543 -4.3417826 -4.3414669 -4.3436913 -4.3498511 -4.35595][-4.3549891 -4.3355889 -4.3174181 -4.3051867 -4.3040814 -4.3089457 -4.3096652 -4.306201 -4.310554 -4.3232656 -4.3331122 -4.3342791 -4.337224 -4.3442283 -4.3498635][-4.3468142 -4.3182788 -4.290185 -4.2699456 -4.2682414 -4.2746534 -4.2744527 -4.2675061 -4.2738938 -4.2930326 -4.3130889 -4.3219738 -4.3270173 -4.3343415 -4.3382797][-4.3294392 -4.2898359 -4.249445 -4.21795 -4.2101359 -4.2158742 -4.215611 -4.2039237 -4.210701 -4.2381053 -4.2722263 -4.2953653 -4.3080282 -4.3182392 -4.3218184][-4.2944589 -4.2357187 -4.1762352 -4.1281228 -4.1099992 -4.1178217 -4.118751 -4.1014667 -4.1060743 -4.1438656 -4.1933188 -4.2362218 -4.2648864 -4.2858057 -4.2946153][-4.2398281 -4.1566958 -4.07789 -4.0137081 -3.9907994 -4.0053458 -4.0111508 -3.9847727 -3.980474 -4.0277672 -4.096889 -4.1618948 -4.2084031 -4.2442107 -4.2628555][-4.1753426 -4.06547 -3.9738626 -3.9048581 -3.8921275 -3.9139709 -3.9187315 -3.8745785 -3.8554313 -3.9128613 -3.998704 -4.0828428 -4.1455889 -4.1961961 -4.2287278][-4.1406903 -4.0249038 -3.9399638 -3.8837621 -3.8846097 -3.9001303 -3.8911135 -3.8290145 -3.8029041 -3.8646641 -3.9496057 -4.0356207 -4.1071992 -4.1626925 -4.2013674][-4.1605353 -4.0702629 -4.0105991 -3.9750857 -3.9747047 -3.9729071 -3.9525363 -3.8920743 -3.8712945 -3.9233029 -3.9899492 -4.059485 -4.1203222 -4.1703258 -4.2058592][-4.2186151 -4.160758 -4.1249981 -4.1072493 -4.1026592 -4.0882196 -4.067318 -4.0254707 -4.0138636 -4.0466919 -4.0902743 -4.1362147 -4.1806622 -4.2181177 -4.2400837][-4.280086 -4.2486987 -4.2296996 -4.224041 -4.2193756 -4.2066307 -4.1956167 -4.1703978 -4.1615438 -4.1743512 -4.19711 -4.2261086 -4.2560816 -4.2768221 -4.2869592][-4.31633 -4.2988558 -4.2938304 -4.2985544 -4.2989507 -4.29272 -4.2895365 -4.2745037 -4.266222 -4.2687941 -4.2806063 -4.300457 -4.3179474 -4.3235993 -4.3254251][-4.3313923 -4.320652 -4.3195124 -4.325294 -4.3260145 -4.3233004 -4.3231812 -4.3163781 -4.3119087 -4.3147736 -4.3264608 -4.3407173 -4.3488293 -4.3502054 -4.351738][-4.3454366 -4.3385587 -4.3372955 -4.3375454 -4.3370571 -4.3354411 -4.3345957 -4.3319993 -4.3294559 -4.3340096 -4.3445139 -4.3554053 -4.3616261 -4.3634782 -4.3671112][-4.3603215 -4.3569431 -4.354804 -4.3523278 -4.3504682 -4.3484063 -4.3464246 -4.3453174 -4.3448067 -4.3477888 -4.3538966 -4.3597889 -4.3644905 -4.3680716 -4.3725781]]...]
INFO - root - 2017-12-07 23:22:24.616923: step 52310, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 71h:50m:26s remains)
INFO - root - 2017-12-07 23:22:34.440518: step 52320, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 76h:41m:42s remains)
INFO - root - 2017-12-07 23:22:44.158736: step 52330, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.005 sec/batch; 78h:15m:06s remains)
INFO - root - 2017-12-07 23:22:53.792496: step 52340, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 72h:40m:55s remains)
INFO - root - 2017-12-07 23:23:03.286517: step 52350, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 76h:45m:35s remains)
INFO - root - 2017-12-07 23:23:12.998393: step 52360, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 73h:54m:54s remains)
INFO - root - 2017-12-07 23:23:22.733107: step 52370, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 78h:39m:42s remains)
INFO - root - 2017-12-07 23:23:32.274034: step 52380, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 76h:55m:48s remains)
INFO - root - 2017-12-07 23:23:42.154749: step 52390, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.006 sec/batch; 78h:14m:49s remains)
INFO - root - 2017-12-07 23:23:51.726897: step 52400, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 72h:59m:14s remains)
2017-12-07 23:23:52.702229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2410078 -4.2141328 -4.198812 -4.1828117 -4.1721988 -4.1680846 -4.1685667 -4.1660094 -4.1601048 -4.1714406 -4.1964612 -4.2132359 -4.2156487 -4.2116981 -4.1919336][-4.221066 -4.1875572 -4.1692481 -4.1558256 -4.1441407 -4.1388097 -4.140132 -4.1394343 -4.1345525 -4.1495342 -4.1798038 -4.1958227 -4.19639 -4.1930971 -4.1671124][-4.2044506 -4.1652913 -4.143497 -4.1342192 -4.1242833 -4.1141253 -4.1127176 -4.1132612 -4.113771 -4.1313329 -4.1613431 -4.1756396 -4.1737108 -4.1710019 -4.148407][-4.1938977 -4.1530566 -4.1284761 -4.1231446 -4.1205077 -4.10853 -4.1039944 -4.1063652 -4.1140609 -4.130939 -4.1541753 -4.1639047 -4.1615343 -4.1600809 -4.1433372][-4.1915131 -4.1557407 -4.1332421 -4.1304 -4.1353149 -4.1289954 -4.1177549 -4.1112757 -4.1165986 -4.1300383 -4.1491256 -4.1605515 -4.1618052 -4.1599107 -4.1467991][-4.1992993 -4.168541 -4.1484509 -4.1434379 -4.15014 -4.1451693 -4.1296406 -4.1145778 -4.1145544 -4.1230507 -4.142633 -4.1626453 -4.1680965 -4.1614857 -4.1540751][-4.21392 -4.1876664 -4.169075 -4.1606975 -4.1610537 -4.1535048 -4.129621 -4.1118689 -4.1052732 -4.1057844 -4.13209 -4.1706686 -4.1848197 -4.1743884 -4.1620607][-4.2214479 -4.1994457 -4.182507 -4.1741343 -4.1684475 -4.158814 -4.1364098 -4.1200638 -4.1067572 -4.0943432 -4.127111 -4.1858315 -4.211905 -4.2017508 -4.1786609][-4.222548 -4.206696 -4.1961184 -4.1905208 -4.1863017 -4.1857085 -4.1711822 -4.1547351 -4.1375771 -4.1197505 -4.1484137 -4.2065058 -4.2387123 -4.2343836 -4.2077107][-4.2258043 -4.2164345 -4.2159572 -4.2162457 -4.2163405 -4.2255259 -4.2189126 -4.202035 -4.1889882 -4.1785874 -4.1984634 -4.2405291 -4.2676063 -4.2689319 -4.2444038][-4.231113 -4.2262206 -4.2316585 -4.2369018 -4.2385721 -4.2471857 -4.2418175 -4.2282934 -4.2270603 -4.2286563 -4.2427092 -4.2681332 -4.2862945 -4.2865586 -4.2695909][-4.2347507 -4.2292972 -4.2347445 -4.2394066 -4.2402763 -4.2481837 -4.24476 -4.2379055 -4.2454157 -4.2538843 -4.263773 -4.2793746 -4.2932587 -4.29306 -4.2826042][-4.2400889 -4.231761 -4.2358565 -4.23986 -4.2424397 -4.2507081 -4.2521358 -4.2479386 -4.2550817 -4.2651973 -4.274518 -4.286912 -4.2973423 -4.298655 -4.2919283][-4.2625012 -4.25418 -4.2561631 -4.2588511 -4.261076 -4.2675414 -4.27109 -4.2687197 -4.2725768 -4.2805576 -4.2883883 -4.2969341 -4.3046708 -4.3060055 -4.3004379][-4.2876306 -4.2812996 -4.2827439 -4.2847018 -4.2862263 -4.2896476 -4.2919235 -4.2907372 -4.2914028 -4.2950439 -4.2996197 -4.3044753 -4.3092465 -4.3095903 -4.3046]]...]
INFO - root - 2017-12-07 23:24:02.307892: step 52410, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 76h:26m:33s remains)
INFO - root - 2017-12-07 23:24:12.054609: step 52420, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 77h:44m:19s remains)
INFO - root - 2017-12-07 23:24:21.775743: step 52430, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 74h:35m:59s remains)
INFO - root - 2017-12-07 23:24:31.301464: step 52440, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 73h:03m:40s remains)
INFO - root - 2017-12-07 23:24:40.898095: step 52450, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.027 sec/batch; 79h:51m:24s remains)
INFO - root - 2017-12-07 23:24:50.528462: step 52460, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 77h:16m:42s remains)
INFO - root - 2017-12-07 23:25:00.272160: step 52470, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 76h:22m:36s remains)
INFO - root - 2017-12-07 23:25:09.891089: step 52480, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 75h:38m:07s remains)
INFO - root - 2017-12-07 23:25:19.464540: step 52490, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 74h:10m:31s remains)
INFO - root - 2017-12-07 23:25:29.049686: step 52500, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 76h:00m:04s remains)
2017-12-07 23:25:30.062504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.327095 -4.3244677 -4.3251266 -4.3293867 -4.3301749 -4.331286 -4.3286119 -4.3192105 -4.3087106 -4.2960448 -4.2842989 -4.2792039 -4.2769489 -4.2775269 -4.2840648][-4.3318024 -4.3300581 -4.3288083 -4.3298874 -4.3270259 -4.326602 -4.3233995 -4.3119159 -4.2967749 -4.2800984 -4.2646394 -4.2562661 -4.2539892 -4.2560487 -4.2640262][-4.3380136 -4.3386817 -4.3364415 -4.3352842 -4.3293943 -4.3241925 -4.316576 -4.3010798 -4.2842631 -4.2672367 -4.251461 -4.242588 -4.2422771 -4.2467136 -4.2542763][-4.3371682 -4.341013 -4.3405085 -4.3382869 -4.3296 -4.319211 -4.3060646 -4.286984 -4.2694545 -4.2526007 -4.2366896 -4.2278142 -4.2322254 -4.2430158 -4.24958][-4.3319168 -4.3387294 -4.34062 -4.33736 -4.326055 -4.3116345 -4.2941723 -4.2729759 -4.2544794 -4.237792 -4.2237825 -4.2187653 -4.2309618 -4.2450757 -4.2484369][-4.3204994 -4.3306837 -4.3350797 -4.3308163 -4.316577 -4.2985449 -4.2767091 -4.253655 -4.2348323 -4.2207665 -4.2117796 -4.2141032 -4.2351022 -4.2506633 -4.2532759][-4.3030195 -4.3144822 -4.3196068 -4.3150978 -4.2992964 -4.2787027 -4.25352 -4.2317734 -4.2176423 -4.210259 -4.2088208 -4.2191586 -4.2461433 -4.2596235 -4.2590857][-4.284215 -4.2935328 -4.2960844 -4.28833 -4.2707205 -4.24717 -4.2210774 -4.2027802 -4.1959023 -4.1947904 -4.1995463 -4.2155647 -4.2443333 -4.2537441 -4.2505608][-4.2705569 -4.272902 -4.26854 -4.253336 -4.2307563 -4.2024865 -4.1739159 -4.1567812 -4.15241 -4.1502614 -4.1544266 -4.1732874 -4.2041121 -4.2119 -4.207613][-4.2731891 -4.2650394 -4.2519097 -4.2280107 -4.1980639 -4.1642447 -4.13234 -4.1139188 -4.1066113 -4.0982914 -4.0949154 -4.1123719 -4.1439996 -4.152483 -4.14898][-4.2906752 -4.276062 -4.2603235 -4.2325921 -4.1995783 -4.1641169 -4.1329217 -4.1142645 -4.1044703 -4.0925274 -4.0840693 -4.0994925 -4.1266479 -4.13441 -4.133534][-4.3090119 -4.2941871 -4.2813993 -4.2577605 -4.2285132 -4.1974692 -4.168088 -4.1504097 -4.141396 -4.1312833 -4.1246929 -4.1400867 -4.1620584 -4.1695457 -4.1725187][-4.3179603 -4.3055873 -4.299345 -4.2841287 -4.2624121 -4.2380514 -4.2141142 -4.2004237 -4.1973634 -4.1965218 -4.1968365 -4.2113547 -4.2260103 -4.2283816 -4.2298822][-4.3121939 -4.3025231 -4.3011203 -4.2946682 -4.2833128 -4.2680531 -4.2535415 -4.2484131 -4.2538357 -4.2634125 -4.2704644 -4.2819724 -4.2885242 -4.2849808 -4.2790256][-4.2920175 -4.282104 -4.2830029 -4.2876644 -4.288125 -4.2836866 -4.2788911 -4.2838488 -4.295352 -4.307045 -4.3157835 -4.3218126 -4.3222203 -4.3150511 -4.3033028]]...]
INFO - root - 2017-12-07 23:25:39.813966: step 52510, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 77h:54m:29s remains)
INFO - root - 2017-12-07 23:25:49.328783: step 52520, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 75h:00m:34s remains)
INFO - root - 2017-12-07 23:25:59.207158: step 52530, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 78h:57m:02s remains)
INFO - root - 2017-12-07 23:26:08.983836: step 52540, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.985 sec/batch; 76h:37m:55s remains)
INFO - root - 2017-12-07 23:26:18.685606: step 52550, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.007 sec/batch; 78h:20m:30s remains)
INFO - root - 2017-12-07 23:26:28.283806: step 52560, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 72h:14m:24s remains)
INFO - root - 2017-12-07 23:26:37.978404: step 52570, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 69h:43m:13s remains)
INFO - root - 2017-12-07 23:26:47.709217: step 52580, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.926 sec/batch; 72h:01m:51s remains)
INFO - root - 2017-12-07 23:26:57.442509: step 52590, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.976 sec/batch; 75h:54m:40s remains)
INFO - root - 2017-12-07 23:27:07.155315: step 52600, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.033 sec/batch; 80h:19m:16s remains)
2017-12-07 23:27:08.133499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3022003 -4.3049397 -4.3022547 -4.2897143 -4.2737303 -4.2657728 -4.2679563 -4.2727437 -4.2755432 -4.2793431 -4.2781053 -4.2778487 -4.2914591 -4.3111539 -4.3234925][-4.299962 -4.3017454 -4.2987275 -4.2870951 -4.2737551 -4.2686744 -4.2709279 -4.2749763 -4.2783804 -4.2835617 -4.2823615 -4.2808204 -4.2913938 -4.3062825 -4.314167][-4.2971926 -4.3000536 -4.2979889 -4.2886815 -4.27985 -4.2766461 -4.2761874 -4.2761841 -4.2787452 -4.2842193 -4.28308 -4.2821565 -4.2898588 -4.2999363 -4.304482][-4.2930913 -4.3000331 -4.3017073 -4.2961936 -4.2899227 -4.2845073 -4.2771058 -4.2683463 -4.2674603 -4.2741246 -4.2784314 -4.281538 -4.2883186 -4.2957687 -4.2981305][-4.290472 -4.30225 -4.3079195 -4.3045535 -4.2964435 -4.283937 -4.2647028 -4.2443523 -4.2397108 -4.2511206 -4.2657537 -4.2766595 -4.2867694 -4.2946815 -4.2966561][-4.288517 -4.3050675 -4.3142557 -4.3113976 -4.2975092 -4.2731123 -4.2392149 -4.210731 -4.2077761 -4.2278705 -4.2515259 -4.2692828 -4.2831988 -4.29302 -4.2968793][-4.2864037 -4.305655 -4.3180671 -4.3157425 -4.2953963 -4.260262 -4.2168694 -4.1864877 -4.1871791 -4.2131948 -4.2415571 -4.2618113 -4.2769141 -4.2877126 -4.2947564][-4.2842097 -4.303237 -4.3163476 -4.3149686 -4.2932539 -4.2556849 -4.2129207 -4.1857719 -4.1848063 -4.2062407 -4.2295876 -4.249217 -4.266582 -4.2808928 -4.2905374][-4.2865944 -4.3034153 -4.3138185 -4.3124628 -4.2928195 -4.26083 -4.2260461 -4.2049932 -4.199717 -4.2091112 -4.2192512 -4.2341413 -4.2537818 -4.273407 -4.2860475][-4.2949028 -4.3101597 -4.31677 -4.3151565 -4.2998729 -4.2751889 -4.2490087 -4.2337265 -4.2263656 -4.2243056 -4.2198577 -4.2247977 -4.2413864 -4.2650704 -4.2820992][-4.3038077 -4.319046 -4.32152 -4.3169556 -4.30271 -4.28243 -4.263165 -4.2536812 -4.2466483 -4.2373505 -4.2212658 -4.2179914 -4.23204 -4.2590632 -4.2799568][-4.3100271 -4.3236294 -4.3211188 -4.310863 -4.2935925 -4.2753081 -4.2620425 -4.2591634 -4.2540426 -4.2412705 -4.2196121 -4.2132192 -4.228673 -4.2594419 -4.2818727][-4.3156881 -4.3245726 -4.3157582 -4.299171 -4.2799711 -4.2646804 -4.2572122 -4.2575407 -4.2538843 -4.241663 -4.2224488 -4.2188711 -4.2370396 -4.2682371 -4.2876029][-4.318747 -4.3243046 -4.3109417 -4.2892575 -4.2689052 -4.25687 -4.2538877 -4.2554545 -4.2520928 -4.2430873 -4.2293558 -4.22857 -4.2457142 -4.2742004 -4.2902312][-4.3163981 -4.320612 -4.3071074 -4.2835431 -4.2620354 -4.2503257 -4.2503157 -4.2543569 -4.2530022 -4.24672 -4.2367468 -4.2358222 -4.2480431 -4.2703824 -4.2836709]]...]
INFO - root - 2017-12-07 23:27:17.764069: step 52610, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.959 sec/batch; 74h:33m:19s remains)
INFO - root - 2017-12-07 23:27:27.293437: step 52620, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.005 sec/batch; 78h:09m:53s remains)
INFO - root - 2017-12-07 23:27:37.053855: step 52630, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.980 sec/batch; 76h:10m:40s remains)
INFO - root - 2017-12-07 23:27:46.716782: step 52640, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 76h:09m:48s remains)
INFO - root - 2017-12-07 23:27:56.319185: step 52650, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.893 sec/batch; 69h:25m:01s remains)
INFO - root - 2017-12-07 23:28:05.885826: step 52660, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 73h:48m:58s remains)
INFO - root - 2017-12-07 23:28:15.489259: step 52670, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 78h:21m:38s remains)
INFO - root - 2017-12-07 23:28:25.166252: step 52680, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 74h:28m:07s remains)
INFO - root - 2017-12-07 23:28:34.736322: step 52690, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 72h:21m:31s remains)
INFO - root - 2017-12-07 23:28:44.513672: step 52700, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 76h:23m:08s remains)
2017-12-07 23:28:45.552202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2593369 -4.266499 -4.2842503 -4.2896438 -4.2821131 -4.2761726 -4.2724476 -4.2681489 -4.2727447 -4.275084 -4.2573371 -4.2420316 -4.2467251 -4.26575 -4.2850032][-4.1985426 -4.2106586 -4.2387419 -4.2498341 -4.2435269 -4.2405491 -4.2390733 -4.2331944 -4.2339454 -4.2337732 -4.210506 -4.1937571 -4.2040496 -4.2330632 -4.2587419][-4.153635 -4.1652908 -4.2002487 -4.2185445 -4.2165775 -4.216548 -4.2169318 -4.2054834 -4.2011189 -4.2010546 -4.1799974 -4.16762 -4.1807113 -4.2135406 -4.240623][-4.1437731 -4.1452293 -4.1692796 -4.185689 -4.1883883 -4.1915164 -4.1918907 -4.1759076 -4.170908 -4.1767216 -4.1714911 -4.1714587 -4.1851473 -4.2074804 -4.2260094][-4.1561127 -4.1523752 -4.1589565 -4.1620221 -4.1642289 -4.1653733 -4.1591539 -4.1346416 -4.1334682 -4.1577039 -4.1786685 -4.1972752 -4.21236 -4.2194457 -4.2235861][-4.1785574 -4.1810617 -4.1773591 -4.1631365 -4.1557822 -4.143949 -4.1136942 -4.071053 -4.0758944 -4.1280022 -4.1805038 -4.2123795 -4.2285485 -4.2286892 -4.2241783][-4.1907268 -4.2016134 -4.1974282 -4.171442 -4.1493316 -4.1192727 -4.0621395 -3.9963286 -4.0099149 -4.0920544 -4.1683211 -4.2078238 -4.2256756 -4.2287755 -4.2236252][-4.1927686 -4.2103343 -4.2134228 -4.18628 -4.1504917 -4.1005349 -4.0209627 -3.9330063 -3.9512982 -4.0581746 -4.152729 -4.2009296 -4.2229075 -4.2335405 -4.2311115][-4.2032833 -4.2225151 -4.2335696 -4.2173667 -4.181047 -4.1261721 -4.0441065 -3.95033 -3.95501 -4.0537314 -4.14637 -4.1963353 -4.2239528 -4.2418289 -4.2422071][-4.2196918 -4.239604 -4.2568774 -4.2517748 -4.2238526 -4.1786318 -4.1194539 -4.0473733 -4.0381474 -4.098628 -4.1642141 -4.20125 -4.2239809 -4.2390313 -4.2394361][-4.2305503 -4.2497425 -4.2707181 -4.2728148 -4.2537379 -4.2231946 -4.1882858 -4.1458578 -4.1359987 -4.1616249 -4.1966047 -4.2164855 -4.2292438 -4.2353444 -4.2311745][-4.2436075 -4.2607121 -4.2791023 -4.2769222 -4.260498 -4.2442245 -4.2304111 -4.2106662 -4.2030296 -4.2165632 -4.2349257 -4.2485723 -4.2568455 -4.2578325 -4.2457108][-4.2598433 -4.271872 -4.2812047 -4.27462 -4.2612123 -4.2565618 -4.2572093 -4.2495751 -4.24503 -4.2569566 -4.2710528 -4.2815714 -4.2904477 -4.2903047 -4.27343][-4.2784972 -4.2838821 -4.2838264 -4.2732406 -4.2613826 -4.2638755 -4.2724552 -4.272747 -4.2753491 -4.2889581 -4.3007298 -4.3096013 -4.3182244 -4.317853 -4.30111][-4.2845688 -4.2851973 -4.2812352 -4.269371 -4.2578788 -4.2606926 -4.2737308 -4.2823095 -4.2888961 -4.3015704 -4.3115306 -4.3182912 -4.3246284 -4.3233609 -4.3099151]]...]
INFO - root - 2017-12-07 23:28:55.295019: step 52710, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 75h:20m:19s remains)
INFO - root - 2017-12-07 23:29:04.807736: step 52720, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 71h:36m:27s remains)
INFO - root - 2017-12-07 23:29:14.465004: step 52730, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 74h:26m:06s remains)
INFO - root - 2017-12-07 23:29:24.230471: step 52740, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 78h:43m:44s remains)
INFO - root - 2017-12-07 23:29:33.906555: step 52750, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 71h:49m:56s remains)
INFO - root - 2017-12-07 23:29:43.522787: step 52760, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 74h:22m:50s remains)
INFO - root - 2017-12-07 23:29:53.231165: step 52770, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.020 sec/batch; 79h:14m:03s remains)
INFO - root - 2017-12-07 23:30:02.823419: step 52780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 72h:06m:42s remains)
INFO - root - 2017-12-07 23:30:12.608917: step 52790, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 77h:43m:44s remains)
INFO - root - 2017-12-07 23:30:22.457777: step 52800, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 72h:51m:42s remains)
2017-12-07 23:30:23.425860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1500969 -4.1663036 -4.1674027 -4.1482344 -4.1274037 -4.1128964 -4.1207366 -4.1431985 -4.1592569 -4.1741304 -4.1821542 -4.1893015 -4.198771 -4.2155504 -4.2299628][-4.1651411 -4.1808529 -4.1743107 -4.1435571 -4.1128774 -4.0945125 -4.1034403 -4.1345186 -4.1624012 -4.1699653 -4.164351 -4.1598206 -4.1679111 -4.1913495 -4.2191997][-4.1861753 -4.2094231 -4.2033124 -4.165544 -4.1205282 -4.089098 -4.0876436 -4.1069794 -4.1369791 -4.1438408 -4.1301203 -4.1163516 -4.119947 -4.1456771 -4.18328][-4.1992922 -4.225204 -4.2193871 -4.1796246 -4.124372 -4.0797296 -4.063941 -4.0744672 -4.0973272 -4.1035314 -4.0892854 -4.0729475 -4.0759072 -4.1021605 -4.1436224][-4.2126083 -4.2329774 -4.2216296 -4.1681 -4.0925817 -4.023459 -3.9903331 -4.0102291 -4.0541282 -4.0718212 -4.0627651 -4.0451055 -4.0515018 -4.0836811 -4.12782][-4.2321019 -4.2418909 -4.2229338 -4.1566219 -4.0521388 -3.9338186 -3.8542347 -3.8914485 -3.9897854 -4.0478039 -4.0507178 -4.0284247 -4.0372691 -4.0762105 -4.1280503][-4.2514281 -4.2490292 -4.2211914 -4.1527939 -4.0410051 -3.8872967 -3.7470665 -3.7800486 -3.9287043 -4.0282078 -4.0507035 -4.0320187 -4.0344586 -4.0718293 -4.1345053][-4.2601366 -4.2511 -4.2243462 -4.1651387 -4.0748477 -3.9464107 -3.8205349 -3.8219185 -3.9376385 -4.0361528 -4.0693021 -4.0525947 -4.0447879 -4.0751014 -4.1424823][-4.2413445 -4.2253804 -4.2019415 -4.161613 -4.1079307 -4.03692 -3.9679172 -3.9599962 -4.0181594 -4.0789113 -4.0958228 -4.0734658 -4.0555921 -4.0765567 -4.141861][-4.2269588 -4.2130604 -4.1908689 -4.1571751 -4.1254935 -4.0950937 -4.0668664 -4.0624061 -4.0968785 -4.1337819 -4.134408 -4.1047511 -4.0785675 -4.0826693 -4.1315665][-4.2034764 -4.2037053 -4.1921849 -4.1667881 -4.1452894 -4.1309247 -4.1204 -4.1193671 -4.1463952 -4.1731944 -4.1610932 -4.1302485 -4.1018596 -4.0921941 -4.1202374][-4.1717238 -4.1796494 -4.1759238 -4.1598907 -4.150732 -4.1461816 -4.1459169 -4.1500492 -4.1741824 -4.1938214 -4.1738853 -4.1416707 -4.1097426 -4.09532 -4.1095591][-4.1616144 -4.1710258 -4.17067 -4.1554937 -4.1450377 -4.1456318 -4.15057 -4.1525273 -4.1628666 -4.174036 -4.1544237 -4.1218452 -4.096982 -4.09425 -4.1075168][-4.1846676 -4.1890874 -4.1851554 -4.1681256 -4.1548085 -4.1505818 -4.1532464 -4.1489539 -4.1430721 -4.1397562 -4.1132445 -4.0755763 -4.0599523 -4.0804205 -4.0970182][-4.2118468 -4.2144651 -4.2115436 -4.2009463 -4.19 -4.1850119 -4.181325 -4.1684127 -4.1498451 -4.1332445 -4.0969582 -4.0557513 -4.044837 -4.069725 -4.0727367]]...]
INFO - root - 2017-12-07 23:30:33.032483: step 52810, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 78h:01m:41s remains)
INFO - root - 2017-12-07 23:30:42.580317: step 52820, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.970 sec/batch; 75h:19m:50s remains)
INFO - root - 2017-12-07 23:30:52.395508: step 52830, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 74h:24m:36s remains)
INFO - root - 2017-12-07 23:31:02.161813: step 52840, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.950 sec/batch; 73h:48m:36s remains)
INFO - root - 2017-12-07 23:31:12.125837: step 52850, loss = 2.10, batch loss = 2.04 (7.4 examples/sec; 1.085 sec/batch; 84h:17m:09s remains)
INFO - root - 2017-12-07 23:31:21.638175: step 52860, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 74h:58m:39s remains)
INFO - root - 2017-12-07 23:31:31.214414: step 52870, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.994 sec/batch; 77h:12m:21s remains)
INFO - root - 2017-12-07 23:31:40.956415: step 52880, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.037 sec/batch; 80h:32m:14s remains)
INFO - root - 2017-12-07 23:31:50.758835: step 52890, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 75h:32m:48s remains)
INFO - root - 2017-12-07 23:32:00.357481: step 52900, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 75h:08m:17s remains)
2017-12-07 23:32:01.346614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3644018 -4.35424 -4.3217382 -4.2769628 -4.2372751 -4.2088017 -4.1926427 -4.1960926 -4.2105389 -4.2187295 -4.2113094 -4.2056289 -4.2032361 -4.2046127 -4.2097797][-4.3678374 -4.3547473 -4.3140364 -4.2608166 -4.2168117 -4.1867409 -4.1685495 -4.172533 -4.1886597 -4.2002025 -4.1996965 -4.1969686 -4.1930547 -4.1900697 -4.1917095][-4.3661537 -4.3498015 -4.3031983 -4.2479582 -4.2067475 -4.1813431 -4.1654692 -4.1702657 -4.1856647 -4.1960983 -4.2028613 -4.2087579 -4.2082167 -4.2020736 -4.202323][-4.3615618 -4.3401189 -4.289732 -4.2337751 -4.1948867 -4.1731191 -4.1606421 -4.1652036 -4.177381 -4.1859636 -4.2030392 -4.221282 -4.22786 -4.2229538 -4.2253327][-4.35456 -4.32653 -4.2732997 -4.2161613 -4.1772027 -4.1559987 -4.1431561 -4.1462793 -4.1542931 -4.1607141 -4.1880407 -4.220912 -4.23741 -4.2379932 -4.242806][-4.3466673 -4.3129048 -4.25749 -4.1993675 -4.159399 -4.1366005 -4.1198821 -4.1182113 -4.1204615 -4.1237655 -4.1583891 -4.2034044 -4.2295995 -4.2369342 -4.2447381][-4.3388324 -4.2987428 -4.2404485 -4.1818247 -4.1403494 -4.1156092 -4.0993948 -4.1002755 -4.0998054 -4.0992723 -4.1345186 -4.1865249 -4.21949 -4.2312779 -4.2377338][-4.3335037 -4.2906036 -4.2327909 -4.1787567 -4.1419578 -4.1197319 -4.11043 -4.1183653 -4.1184578 -4.1134467 -4.1407771 -4.1910758 -4.2252188 -4.2364607 -4.2381792][-4.3333583 -4.2930026 -4.2431378 -4.1992507 -4.1730909 -4.1584048 -4.1570392 -4.167223 -4.1645446 -4.1561427 -4.1727152 -4.2143989 -4.2431793 -4.2503858 -4.2455931][-4.334342 -4.2989621 -4.2581286 -4.2211666 -4.2013617 -4.1908007 -4.1924124 -4.1996017 -4.1972623 -4.1880884 -4.1991739 -4.2351475 -4.2599568 -4.2665477 -4.2602148][-4.334249 -4.3027844 -4.2683916 -4.2365694 -4.2218475 -4.214447 -4.2151237 -4.2172766 -4.2137461 -4.2052393 -4.2143493 -4.2485113 -4.2733011 -4.2829547 -4.2780962][-4.3340397 -4.304471 -4.2719765 -4.2427707 -4.2332325 -4.2304583 -4.2291374 -4.2264104 -4.2223964 -4.2176595 -4.2278819 -4.2591252 -4.2841368 -4.2972908 -4.2962418][-4.3345275 -4.30569 -4.2729621 -4.2430649 -4.234767 -4.2346416 -4.2339435 -4.2335844 -4.234426 -4.2360382 -4.2476296 -4.2723789 -4.296586 -4.3114161 -4.3136482][-4.335175 -4.3065615 -4.2741008 -4.2432957 -4.2330041 -4.2340488 -4.2388606 -4.2453489 -4.2502904 -4.2546492 -4.2649078 -4.2846479 -4.3072004 -4.32036 -4.3233767][-4.3371034 -4.3092089 -4.2779737 -4.2458153 -4.2317424 -4.2322993 -4.242517 -4.2533655 -4.2593913 -4.2646837 -4.2745728 -4.2910447 -4.3092079 -4.3200216 -4.3232131]]...]
INFO - root - 2017-12-07 23:32:10.971192: step 52910, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 77h:19m:38s remains)
INFO - root - 2017-12-07 23:32:20.544312: step 52920, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.019 sec/batch; 79h:09m:07s remains)
INFO - root - 2017-12-07 23:32:30.224698: step 52930, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.921 sec/batch; 71h:32m:06s remains)
INFO - root - 2017-12-07 23:32:39.795514: step 52940, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 74h:49m:56s remains)
INFO - root - 2017-12-07 23:32:49.326504: step 52950, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 75h:02m:55s remains)
INFO - root - 2017-12-07 23:32:58.797955: step 52960, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 72h:15m:59s remains)
INFO - root - 2017-12-07 23:33:08.662853: step 52970, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.011 sec/batch; 78h:28m:18s remains)
INFO - root - 2017-12-07 23:33:18.272726: step 52980, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.991 sec/batch; 76h:55m:23s remains)
INFO - root - 2017-12-07 23:33:27.809654: step 52990, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 72h:51m:18s remains)
INFO - root - 2017-12-07 23:33:37.429654: step 53000, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 74h:44m:58s remains)
2017-12-07 23:33:38.352419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.308455 -4.3088074 -4.3129973 -4.3169575 -4.3201442 -4.3208613 -4.3187289 -4.312952 -4.3022251 -4.2889605 -4.2788939 -4.2746658 -4.2806716 -4.300384 -4.3258862][-4.2890644 -4.2881989 -4.2939219 -4.3023434 -4.3114266 -4.3158541 -4.3123879 -4.3040733 -4.2875195 -4.2676215 -4.2522311 -4.2437692 -4.2471576 -4.2710843 -4.3084636][-4.2756548 -4.2717757 -4.2762141 -4.2874951 -4.300961 -4.3089819 -4.3074007 -4.2993007 -4.2800131 -4.2557039 -4.2361913 -4.2238913 -4.2217031 -4.2446785 -4.2893705][-4.2645469 -4.2578235 -4.2568269 -4.2664013 -4.2843471 -4.2978411 -4.3016644 -4.2949023 -4.27391 -4.2449346 -4.2207336 -4.2075825 -4.2033467 -4.2229748 -4.270987][-4.2460127 -4.2330394 -4.2235889 -4.2306867 -4.2521839 -4.2701368 -4.2761979 -4.266993 -4.2393246 -4.2030134 -4.1746764 -4.168159 -4.1757207 -4.2014966 -4.2534513][-4.2251287 -4.2040644 -4.1864119 -4.1913395 -4.2157249 -4.2385979 -4.2477608 -4.2376566 -4.2062283 -4.1648088 -4.1325974 -4.1331086 -4.1523681 -4.1849027 -4.2402887][-4.2113085 -4.18548 -4.1624565 -4.1637254 -4.1857018 -4.2104692 -4.22614 -4.2220936 -4.1954312 -4.1545115 -4.1185765 -4.1184015 -4.1407051 -4.1763053 -4.2317448][-4.2114611 -4.1885157 -4.1652055 -4.1625762 -4.1792254 -4.2010231 -4.2182865 -4.2212834 -4.2037249 -4.1673779 -4.1304054 -4.1242266 -4.1412921 -4.1758084 -4.22916][-4.2273259 -4.2141972 -4.1977038 -4.1930122 -4.2045107 -4.2246532 -4.2410526 -4.2472324 -4.2364306 -4.2042193 -4.167305 -4.1518703 -4.1603112 -4.1889853 -4.2357049][-4.2541862 -4.2495151 -4.237196 -4.2276068 -4.2334309 -4.25286 -4.26929 -4.2769952 -4.2701392 -4.2433352 -4.2093468 -4.1862593 -4.1868782 -4.209013 -4.2484646][-4.2725196 -4.2708516 -4.2605624 -4.2499585 -4.2531576 -4.2709169 -4.2855062 -4.2925649 -4.2887697 -4.2684007 -4.2413993 -4.2165794 -4.213748 -4.2303457 -4.26364][-4.2926536 -4.2921653 -4.2867517 -4.2813497 -4.2842059 -4.2980318 -4.3067579 -4.3112092 -4.3085251 -4.2954311 -4.2759023 -4.2536373 -4.2487311 -4.2582903 -4.2827344][-4.3025541 -4.3017321 -4.3018794 -4.3012948 -4.30275 -4.3110447 -4.3167629 -4.3203216 -4.3195281 -4.3135924 -4.3012381 -4.285398 -4.2808547 -4.2871923 -4.30488][-4.2981129 -4.2986717 -4.3007746 -4.3030849 -4.30308 -4.30694 -4.3115506 -4.3151655 -4.3167186 -4.31762 -4.3144784 -4.3083873 -4.3077321 -4.31494 -4.3290515][-4.303246 -4.3037615 -4.3048272 -4.3069987 -4.3077574 -4.3094769 -4.3127127 -4.3161249 -4.3198333 -4.3241539 -4.3271551 -4.3279696 -4.3311553 -4.3387003 -4.3488951]]...]
INFO - root - 2017-12-07 23:33:48.182727: step 53010, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 73h:33m:48s remains)
INFO - root - 2017-12-07 23:33:57.780013: step 53020, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.022 sec/batch; 79h:22m:26s remains)
INFO - root - 2017-12-07 23:34:07.434841: step 53030, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 72h:08m:49s remains)
INFO - root - 2017-12-07 23:34:17.238931: step 53040, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 74h:15m:55s remains)
INFO - root - 2017-12-07 23:34:26.972325: step 53050, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 73h:49m:32s remains)
INFO - root - 2017-12-07 23:34:36.655764: step 53060, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 76h:16m:34s remains)
INFO - root - 2017-12-07 23:34:46.426778: step 53070, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 72h:02m:32s remains)
INFO - root - 2017-12-07 23:34:56.135282: step 53080, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 75h:11m:17s remains)
INFO - root - 2017-12-07 23:35:05.934384: step 53090, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 75h:47m:48s remains)
INFO - root - 2017-12-07 23:35:15.666077: step 53100, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 74h:03m:29s remains)
2017-12-07 23:35:16.622862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3177047 -4.3167858 -4.3158259 -4.3167062 -4.3180089 -4.319088 -4.3193064 -4.3187218 -4.3192596 -4.3213153 -4.3222876 -4.3210526 -4.3186173 -4.3151269 -4.3112268][-4.3019085 -4.2987967 -4.2960062 -4.2980661 -4.301713 -4.3049459 -4.3067732 -4.307209 -4.3078694 -4.3096471 -4.3100576 -4.3074694 -4.3055539 -4.3007607 -4.2937818][-4.2830682 -4.2765889 -4.2717147 -4.2774158 -4.2872248 -4.2934141 -4.2982993 -4.3014531 -4.3001456 -4.2988157 -4.2982316 -4.295866 -4.2958016 -4.2882786 -4.2758427][-4.2665319 -4.2550797 -4.2458596 -4.2530632 -4.267683 -4.2755003 -4.283618 -4.28981 -4.2868114 -4.2818933 -4.28242 -4.2851229 -4.2899728 -4.2805777 -4.2632341][-4.2340808 -4.2154851 -4.1983943 -4.2038021 -4.2241859 -4.2339125 -4.2419767 -4.25192 -4.2461233 -4.2359171 -4.2401414 -4.2531548 -4.2698989 -4.2625875 -4.2406287][-4.1922736 -4.1642771 -4.1346679 -4.1328435 -4.1609454 -4.1757774 -4.1814919 -4.1926036 -4.1840158 -4.1672959 -4.1751804 -4.2009039 -4.2313337 -4.22825 -4.2068229][-4.1484523 -4.1095371 -4.065412 -4.0591063 -4.0961761 -4.1152658 -4.1156883 -4.1261868 -4.1156497 -4.0959005 -4.1113787 -4.1518412 -4.1917143 -4.1912632 -4.1745038][-4.1336 -4.0826416 -4.0214682 -4.0085616 -4.0506411 -4.07125 -4.0673013 -4.0790257 -4.0672359 -4.0434847 -4.0662069 -4.1205707 -4.1689177 -4.1745496 -4.1606493][-4.1591806 -4.1017103 -4.0306392 -4.0103812 -4.0519128 -4.0732689 -4.0658445 -4.0768714 -4.066081 -4.0435152 -4.067184 -4.1248312 -4.1768045 -4.1857657 -4.1701441][-4.1956611 -4.1457958 -4.0840993 -4.0663252 -4.1077523 -4.1319952 -4.1275949 -4.1372647 -4.1297836 -4.1141052 -4.1318808 -4.1754136 -4.2176576 -4.2220778 -4.2019238][-4.2201204 -4.1859765 -4.1442351 -4.13382 -4.1698442 -4.1938243 -4.1953783 -4.2043128 -4.1994357 -4.1906204 -4.2024055 -4.2280703 -4.2528739 -4.2521224 -4.22961][-4.222508 -4.1987586 -4.1728878 -4.1682086 -4.1965027 -4.2205143 -4.2283554 -4.2375708 -4.2364593 -4.2333546 -4.2413154 -4.256526 -4.2689023 -4.26386 -4.2431216][-4.2311292 -4.2138228 -4.1948462 -4.191361 -4.2110767 -4.231637 -4.2416892 -4.2525215 -4.2529078 -4.2500563 -4.2540994 -4.2652025 -4.2737403 -4.2687631 -4.2543859][-4.2511578 -4.2390518 -4.2268276 -4.2265024 -4.2412105 -4.2565536 -4.2658687 -4.2745142 -4.2692804 -4.2592983 -4.25934 -4.2670546 -4.2726216 -4.2702684 -4.2630911][-4.26614 -4.2576418 -4.2516565 -4.2536521 -4.2672467 -4.2812715 -4.28817 -4.2934232 -4.2835741 -4.2670264 -4.2640448 -4.2707624 -4.2741122 -4.273191 -4.2705312]]...]
INFO - root - 2017-12-07 23:35:26.487983: step 53110, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 76h:38m:05s remains)
INFO - root - 2017-12-07 23:35:36.189234: step 53120, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 72h:40m:20s remains)
INFO - root - 2017-12-07 23:35:45.838799: step 53130, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 74h:40m:13s remains)
INFO - root - 2017-12-07 23:35:55.340360: step 53140, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 75h:47m:48s remains)
INFO - root - 2017-12-07 23:36:05.135361: step 53150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 73h:51m:52s remains)
INFO - root - 2017-12-07 23:36:14.791983: step 53160, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 75h:14m:45s remains)
INFO - root - 2017-12-07 23:36:24.505586: step 53170, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 74h:57m:56s remains)
INFO - root - 2017-12-07 23:36:34.242566: step 53180, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.992 sec/batch; 76h:56m:09s remains)
INFO - root - 2017-12-07 23:36:43.871860: step 53190, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 77h:39m:46s remains)
INFO - root - 2017-12-07 23:36:53.468455: step 53200, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 72h:42m:35s remains)
2017-12-07 23:36:54.484701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2503109 -4.2169781 -4.2090073 -4.2256169 -4.2468891 -4.2661896 -4.2777371 -4.2716656 -4.2600155 -4.2467947 -4.2527828 -4.2650762 -4.2780385 -4.2857676 -4.2822022][-4.1876683 -4.1452661 -4.1455364 -4.1852694 -4.2320781 -4.2666326 -4.2865286 -4.2869158 -4.2820063 -4.2763338 -4.2889452 -4.3005795 -4.3088608 -4.3092694 -4.2973061][-4.1185136 -4.0719986 -4.0838919 -4.1456556 -4.2145786 -4.262136 -4.2908587 -4.302247 -4.305366 -4.30953 -4.3280473 -4.3350868 -4.3352203 -4.3283124 -4.3066645][-4.0911403 -4.0472755 -4.0690222 -4.1400895 -4.2147417 -4.2611136 -4.2825751 -4.2928448 -4.3036075 -4.3200445 -4.343379 -4.3483882 -4.3423448 -4.3301768 -4.3038692][-4.1289482 -4.1028633 -4.1259785 -4.1840405 -4.2342424 -4.2506061 -4.2426138 -4.2387147 -4.2589679 -4.2939167 -4.3296962 -4.3395214 -4.3357096 -4.3232565 -4.2989454][-4.1998534 -4.1887336 -4.2062125 -4.2342181 -4.2399397 -4.2071123 -4.1577363 -4.1378093 -4.16905 -4.2309375 -4.2892532 -4.3171172 -4.3230391 -4.3128581 -4.2929988][-4.2470717 -4.2398305 -4.2487292 -4.2449069 -4.205317 -4.1166716 -4.0158339 -3.9873881 -4.0508194 -4.1559258 -4.2428637 -4.2889619 -4.3057842 -4.2985516 -4.2825766][-4.2605834 -4.2484078 -4.2469172 -4.224844 -4.1577249 -4.0327487 -3.8993905 -3.8832116 -3.9905505 -4.1289225 -4.230052 -4.2838335 -4.2998319 -4.2901349 -4.2745709][-4.2547164 -4.2398992 -4.2338986 -4.2128682 -4.1558323 -4.05422 -3.958312 -3.9659479 -4.0715613 -4.1905293 -4.2681847 -4.3042808 -4.3056474 -4.2869153 -4.2677922][-4.223556 -4.2115183 -4.2145233 -4.2157989 -4.1945415 -4.1463933 -4.1061649 -4.1215839 -4.1947069 -4.2715082 -4.3135924 -4.32272 -4.3087234 -4.283668 -4.2624125][-4.195477 -4.1917906 -4.2101078 -4.2307148 -4.2333097 -4.2190409 -4.2089705 -4.2222981 -4.2619643 -4.3033271 -4.3230457 -4.3192368 -4.2992115 -4.2779531 -4.2622862][-4.1990771 -4.2030139 -4.2326517 -4.25632 -4.2633977 -4.2599649 -4.250536 -4.2485948 -4.2598033 -4.2793307 -4.2901683 -4.28599 -4.2745061 -4.2682023 -4.2669654][-4.2240167 -4.2312622 -4.2572579 -4.2693796 -4.2707028 -4.2674384 -4.2500224 -4.22738 -4.2153335 -4.2220106 -4.2340975 -4.2429452 -4.2506294 -4.2631822 -4.2746677][-4.24138 -4.2492819 -4.2631769 -4.2590928 -4.25357 -4.2480865 -4.2255936 -4.1873312 -4.1571317 -4.1554995 -4.176722 -4.2080007 -4.2364058 -4.2639894 -4.2805924][-4.246201 -4.24586 -4.2461557 -4.2305222 -4.2223721 -4.2223744 -4.2035203 -4.1565347 -4.113822 -4.1076937 -4.1396732 -4.1921082 -4.2356997 -4.2703023 -4.2871828]]...]
INFO - root - 2017-12-07 23:37:04.424007: step 53210, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.019 sec/batch; 79h:03m:29s remains)
INFO - root - 2017-12-07 23:37:14.065119: step 53220, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 73h:43m:52s remains)
INFO - root - 2017-12-07 23:37:23.712317: step 53230, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 76h:29m:37s remains)
INFO - root - 2017-12-07 23:37:33.579822: step 53240, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 75h:16m:45s remains)
INFO - root - 2017-12-07 23:37:43.315824: step 53250, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.967 sec/batch; 74h:59m:09s remains)
INFO - root - 2017-12-07 23:37:53.058103: step 53260, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 75h:30m:38s remains)
INFO - root - 2017-12-07 23:38:02.837317: step 53270, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.990 sec/batch; 76h:45m:34s remains)
INFO - root - 2017-12-07 23:38:12.587203: step 53280, loss = 2.12, batch loss = 2.06 (8.0 examples/sec; 0.998 sec/batch; 77h:23m:29s remains)
INFO - root - 2017-12-07 23:38:22.075262: step 53290, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 72h:08m:45s remains)
INFO - root - 2017-12-07 23:38:31.863025: step 53300, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.991 sec/batch; 76h:52m:41s remains)
2017-12-07 23:38:32.887436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3386078 -4.3419676 -4.3436971 -4.344882 -4.345305 -4.3449321 -4.3439856 -4.3430939 -4.3423257 -4.34158 -4.3403978 -4.3388009 -4.3374128 -4.3362265 -4.3349342][-4.3234897 -4.3304567 -4.3345046 -4.3366661 -4.3370786 -4.3364978 -4.3348255 -4.3329015 -4.3318729 -4.3309817 -4.328948 -4.3261428 -4.3235364 -4.3204055 -4.3160496][-4.3067966 -4.3164692 -4.3221912 -4.3240581 -4.323154 -4.3212857 -4.3189406 -4.3170586 -4.3158941 -4.3139529 -4.3106537 -4.3073835 -4.3046255 -4.3003736 -4.2923489][-4.2864027 -4.2956948 -4.3000717 -4.2992392 -4.2951241 -4.2915154 -4.289722 -4.2889109 -4.2873759 -4.2832069 -4.2789688 -4.2756619 -4.2727995 -4.2676883 -4.2580333][-4.2627816 -4.2672219 -4.2680368 -4.2650037 -4.259654 -4.2555122 -4.2550731 -4.256568 -4.2550125 -4.2477732 -4.2414689 -4.2390947 -4.2387938 -4.23617 -4.228713][-4.2455621 -4.24588 -4.2457075 -4.24255 -4.2370615 -4.2330794 -4.234158 -4.237361 -4.2352018 -4.2238445 -4.2139425 -4.2131495 -4.2182803 -4.22251 -4.2211995][-4.2336111 -4.2367096 -4.2396297 -4.2358303 -4.2293282 -4.2252169 -4.2265873 -4.2310872 -4.2293062 -4.2151275 -4.1981378 -4.1930385 -4.20143 -4.2144032 -4.2187624][-4.2178092 -4.2296166 -4.237792 -4.2332668 -4.2260351 -4.2220368 -4.222898 -4.2264142 -4.2244992 -4.2098327 -4.1883225 -4.1773758 -4.1836338 -4.2008095 -4.2115445][-4.1929846 -4.2147121 -4.2278423 -4.2227254 -4.2137575 -4.2080512 -4.206387 -4.2067742 -4.2030706 -4.1890874 -4.1691122 -4.1596737 -4.16847 -4.1894507 -4.2075047][-4.1724296 -4.1994829 -4.2144079 -4.2094669 -4.1976738 -4.1885304 -4.185442 -4.1854453 -4.1820083 -4.1710382 -4.1585078 -4.1586404 -4.1733551 -4.1963696 -4.2172446][-4.1641388 -4.1918221 -4.2068648 -4.2033868 -4.1905527 -4.1774111 -4.173121 -4.1760392 -4.1779065 -4.1764336 -4.1756148 -4.1838732 -4.1997666 -4.219141 -4.2341247][-4.1584563 -4.1841226 -4.2010875 -4.201508 -4.1906471 -4.1783581 -4.1773224 -4.1849833 -4.1937618 -4.2012882 -4.2098284 -4.2208066 -4.2314138 -4.2413445 -4.2456732][-4.1531496 -4.1775346 -4.1984706 -4.2042208 -4.1976438 -4.189559 -4.1929512 -4.2040992 -4.2175388 -4.2289686 -4.23962 -4.2488365 -4.2530689 -4.2538295 -4.249474][-4.1620421 -4.1860003 -4.2063293 -4.2125964 -4.2082577 -4.2031164 -4.208683 -4.2206488 -4.2336287 -4.2435479 -4.2524309 -4.2612867 -4.2618518 -4.2556105 -4.2438273][-4.1761518 -4.2003961 -4.218503 -4.2225533 -4.2184706 -4.2177291 -4.2276106 -4.2399554 -4.2484293 -4.2520208 -4.256021 -4.261837 -4.2595668 -4.2507358 -4.2382274]]...]
INFO - root - 2017-12-07 23:38:42.562809: step 53310, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 74h:56m:15s remains)
INFO - root - 2017-12-07 23:38:52.140222: step 53320, loss = 2.12, batch loss = 2.06 (8.1 examples/sec; 0.982 sec/batch; 76h:09m:42s remains)
INFO - root - 2017-12-07 23:39:01.887864: step 53330, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 74h:32m:39s remains)
INFO - root - 2017-12-07 23:39:11.661480: step 53340, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 77h:46m:04s remains)
INFO - root - 2017-12-07 23:39:21.213428: step 53350, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 75h:06m:31s remains)
INFO - root - 2017-12-07 23:39:30.710979: step 53360, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.902 sec/batch; 69h:54m:59s remains)
INFO - root - 2017-12-07 23:39:40.347010: step 53370, loss = 2.11, batch loss = 2.06 (8.3 examples/sec; 0.963 sec/batch; 74h:40m:31s remains)
INFO - root - 2017-12-07 23:39:49.918033: step 53380, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 70h:03m:15s remains)
INFO - root - 2017-12-07 23:39:59.607454: step 53390, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.004 sec/batch; 77h:49m:29s remains)
INFO - root - 2017-12-07 23:40:09.421774: step 53400, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 78h:41m:17s remains)
2017-12-07 23:40:10.340322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3225069 -4.3195567 -4.3165145 -4.3153534 -4.3129992 -4.3118153 -4.3131561 -4.3184919 -4.3252797 -4.327992 -4.3237171 -4.3119812 -4.2951255 -4.2786851 -4.2657504][-4.3248396 -4.3206072 -4.3148193 -4.308064 -4.2969003 -4.2863274 -4.2801738 -4.2797709 -4.2839408 -4.2875443 -4.2862358 -4.2778153 -4.262506 -4.249588 -4.2419653][-4.3367844 -4.3331294 -4.3247013 -4.3102179 -4.2841687 -4.2588081 -4.242012 -4.2318258 -4.2321348 -4.23887 -4.2453909 -4.2445879 -4.2337537 -4.2246075 -4.2217278][-4.34944 -4.3442149 -4.3300548 -4.3044558 -4.2612109 -4.2163525 -4.1844344 -4.1636467 -4.1635356 -4.1794996 -4.1995621 -4.2103257 -4.2068582 -4.2004218 -4.1984925][-4.3586826 -4.3480082 -4.3227129 -4.2799115 -4.2168913 -4.1519055 -4.100913 -4.0662055 -4.0668254 -4.0984578 -4.1383591 -4.1646242 -4.1680403 -4.161725 -4.157084][-4.35804 -4.337254 -4.296525 -4.2364821 -4.1553092 -4.0726047 -4.0013766 -3.9462934 -3.9407277 -3.9894316 -4.053812 -4.1002464 -4.1115685 -4.1026592 -4.0934892][-4.3449788 -4.3129539 -4.2585087 -4.1850071 -4.0932808 -4.00263 -3.920943 -3.8465776 -3.8258288 -3.8852737 -3.973321 -4.0387478 -4.0575323 -4.0479493 -4.0366087][-4.3267465 -4.2884045 -4.2267485 -4.148562 -4.0600696 -3.979429 -3.9112332 -3.8486159 -3.8305249 -3.8847084 -3.9716098 -4.0383143 -4.0563245 -4.0436988 -4.028543][-4.3224163 -4.28747 -4.2305856 -4.163002 -4.0937347 -4.0396409 -4.0027485 -3.971972 -3.9684591 -4.0037041 -4.059895 -4.1022224 -4.1096907 -4.0932741 -4.0779638][-4.3347321 -4.3137541 -4.2736669 -4.2247005 -4.1773987 -4.1470904 -4.1335745 -4.1242628 -4.1279888 -4.1485062 -4.1779485 -4.193532 -4.1873426 -4.1715679 -4.161479][-4.3505349 -4.3448758 -4.3235893 -4.2919431 -4.2612395 -4.2459764 -4.241765 -4.2407289 -4.2469 -4.259757 -4.2741661 -4.2761583 -4.2653036 -4.2505374 -4.2420211][-4.3538628 -4.3550019 -4.3471932 -4.3300285 -4.3148956 -4.3112869 -4.3122478 -4.3128982 -4.3176 -4.3250494 -4.329679 -4.3266535 -4.3181996 -4.3085365 -4.3010893][-4.3515954 -4.3521557 -4.3489728 -4.3428159 -4.3415756 -4.3474288 -4.3521166 -4.3532672 -4.3560519 -4.35814 -4.3575616 -4.3539252 -4.3483052 -4.3414607 -4.3346705][-4.352582 -4.3518867 -4.3502822 -4.3502026 -4.3545241 -4.36304 -4.3686771 -4.3697906 -4.3711047 -4.3710122 -4.3693137 -4.3663535 -4.3623867 -4.3558345 -4.3488832][-4.3549633 -4.3542862 -4.3526473 -4.3527226 -4.3553567 -4.360898 -4.3650622 -4.3671041 -4.3686266 -4.3694715 -4.3694744 -4.3678646 -4.3644838 -4.3584619 -4.3521395]]...]
INFO - root - 2017-12-07 23:40:20.036515: step 53410, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 74h:23m:50s remains)
INFO - root - 2017-12-07 23:40:29.575177: step 53420, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 76h:27m:53s remains)
INFO - root - 2017-12-07 23:40:39.265017: step 53430, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 73h:12m:30s remains)
INFO - root - 2017-12-07 23:40:48.845758: step 53440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 74h:50m:20s remains)
INFO - root - 2017-12-07 23:40:58.456259: step 53450, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 73h:02m:45s remains)
INFO - root - 2017-12-07 23:41:08.226142: step 53460, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.024 sec/batch; 79h:22m:38s remains)
INFO - root - 2017-12-07 23:41:17.965787: step 53470, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 75h:26m:53s remains)
INFO - root - 2017-12-07 23:41:27.681968: step 53480, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.926 sec/batch; 71h:45m:22s remains)
INFO - root - 2017-12-07 23:41:37.472163: step 53490, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 76h:15m:36s remains)
INFO - root - 2017-12-07 23:41:47.221763: step 53500, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 74h:27m:58s remains)
2017-12-07 23:41:48.177431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2384152 -4.2244821 -4.2205658 -4.2266011 -4.2413344 -4.2571583 -4.2673793 -4.2624826 -4.2462535 -4.2306771 -4.224175 -4.219429 -4.2167692 -4.2208767 -4.2344289][-4.2182574 -4.2014461 -4.2000766 -4.2076244 -4.2222834 -4.2394252 -4.2517509 -4.2462096 -4.2280407 -4.2132173 -4.2110229 -4.2113004 -4.2112651 -4.2100945 -4.21715][-4.202179 -4.1861086 -4.1865482 -4.1945834 -4.2083354 -4.2230716 -4.2347507 -4.2321677 -4.2171063 -4.2083158 -4.2144222 -4.2228789 -4.22523 -4.22005 -4.2181497][-4.1889935 -4.1756344 -4.1762738 -4.1836209 -4.1971016 -4.2105231 -4.2217517 -4.2220912 -4.2134185 -4.2132349 -4.2275853 -4.2411346 -4.2433553 -4.2368264 -4.229136][-4.1733513 -4.1601143 -4.1583519 -4.1658921 -4.1814537 -4.1960821 -4.2078528 -4.2093534 -4.2045174 -4.210166 -4.2285681 -4.2445903 -4.2485676 -4.244072 -4.2338758][-4.1653314 -4.1517854 -4.1470985 -4.156146 -4.1759763 -4.1922369 -4.1998057 -4.1938133 -4.1845975 -4.1906643 -4.2106552 -4.2305889 -4.2409015 -4.2415748 -4.2297921][-4.1749325 -4.1626267 -4.1572104 -4.1675215 -4.1873035 -4.1969872 -4.1885891 -4.1637006 -4.1416926 -4.1467276 -4.1730137 -4.2031875 -4.225244 -4.234354 -4.2234931][-4.1834903 -4.1736183 -4.1666856 -4.17067 -4.1803074 -4.1746693 -4.1461091 -4.0991268 -4.061583 -4.0733705 -4.1175866 -4.1649032 -4.2009425 -4.2201834 -4.2156506][-4.1719179 -4.1617417 -4.150372 -4.1426783 -4.1357393 -4.1125031 -4.0661345 -3.9991114 -3.9502525 -3.9799318 -4.05463 -4.1268086 -4.1785607 -4.2075577 -4.2096767][-4.1634111 -4.1473217 -4.1298828 -4.1135063 -4.0977082 -4.0684228 -4.0192928 -3.9497936 -3.9030619 -3.9440048 -4.0334005 -4.1123405 -4.1632509 -4.1904888 -4.1945887][-4.1811161 -4.160409 -4.1397381 -4.1242566 -4.1137 -4.0974107 -4.0692935 -4.0280848 -4.0039358 -4.0362177 -4.1032529 -4.1570106 -4.1819282 -4.1903481 -4.189817][-4.2187471 -4.199338 -4.1794276 -4.1671524 -4.1612792 -4.1533003 -4.1399341 -4.120935 -4.1092081 -4.1260462 -4.1670837 -4.1968083 -4.1997476 -4.1906357 -4.1863813][-4.2491765 -4.23458 -4.2204452 -4.2145314 -4.2127285 -4.2073178 -4.1993084 -4.1899414 -4.1808205 -4.1830587 -4.2007651 -4.2121081 -4.200346 -4.1799803 -4.173862][-4.2454863 -4.2339334 -4.2249713 -4.2235169 -4.2240372 -4.2203994 -4.2154117 -4.2107973 -4.2035241 -4.1999454 -4.2050157 -4.2073107 -4.1931291 -4.1728253 -4.1673193][-4.2160783 -4.2056394 -4.2018914 -4.204452 -4.2081432 -4.2074332 -4.2037206 -4.1980195 -4.1904106 -4.1865764 -4.1880555 -4.188159 -4.1793361 -4.1681638 -4.1663194]]...]
INFO - root - 2017-12-07 23:41:57.758253: step 53510, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 71h:26m:42s remains)
INFO - root - 2017-12-07 23:42:07.389410: step 53520, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 73h:28m:37s remains)
INFO - root - 2017-12-07 23:42:17.040682: step 53530, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 76h:16m:46s remains)
INFO - root - 2017-12-07 23:42:26.643717: step 53540, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 71h:18m:47s remains)
INFO - root - 2017-12-07 23:42:36.323345: step 53550, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.960 sec/batch; 74h:23m:13s remains)
INFO - root - 2017-12-07 23:42:46.107686: step 53560, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 78h:19m:39s remains)
INFO - root - 2017-12-07 23:42:55.902834: step 53570, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 78h:17m:50s remains)
INFO - root - 2017-12-07 23:43:05.647071: step 53580, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 73h:02m:22s remains)
INFO - root - 2017-12-07 23:43:15.340117: step 53590, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 69h:34m:58s remains)
INFO - root - 2017-12-07 23:43:25.241949: step 53600, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.997 sec/batch; 77h:12m:50s remains)
2017-12-07 23:43:26.266082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2166939 -4.2173667 -4.2147422 -4.2217236 -4.2446637 -4.2722993 -4.2863126 -4.2920175 -4.3013444 -4.3072925 -4.3067389 -4.2941136 -4.2707858 -4.2510624 -4.2414694][-4.21025 -4.2065234 -4.2028918 -4.2104959 -4.230999 -4.2545319 -4.2677298 -4.2786393 -4.2988434 -4.3162284 -4.3257861 -4.3208365 -4.3008909 -4.2821555 -4.2715874][-4.2043533 -4.1998029 -4.1989274 -4.2038569 -4.2159462 -4.2245669 -4.2259536 -4.2400951 -4.2715549 -4.30293 -4.3254266 -4.3301725 -4.3150339 -4.2986193 -4.2900486][-4.2051182 -4.2004366 -4.1983862 -4.1932206 -4.18639 -4.1715722 -4.1506715 -4.1704221 -4.2178435 -4.2680907 -4.3064084 -4.3242006 -4.314559 -4.2977858 -4.29186][-4.2187371 -4.2113838 -4.204627 -4.1815085 -4.146172 -4.0956922 -4.0419741 -4.0712376 -4.1424513 -4.21469 -4.2716789 -4.3024707 -4.2949624 -4.2750325 -4.2682929][-4.2306905 -4.2244325 -4.2118363 -4.1658239 -4.09764 -4.0059867 -3.9116504 -3.9597239 -4.0651627 -4.1601219 -4.2312098 -4.2731347 -4.2686405 -4.2444172 -4.2325039][-4.2341089 -4.2286267 -4.2097726 -4.1468024 -4.0533648 -3.9263039 -3.7918985 -3.8616419 -4.0009842 -4.1135111 -4.1940823 -4.2429657 -4.2447138 -4.219532 -4.2016268][-4.2424011 -4.2346005 -4.2125497 -4.1537504 -4.064455 -3.9402175 -3.8049667 -3.8713055 -4.0063167 -4.1112247 -4.1894689 -4.2370915 -4.2404079 -4.2180166 -4.1943684][-4.2575178 -4.2467327 -4.2262936 -4.1879025 -4.1312828 -4.0496426 -3.9573545 -4.0012732 -4.08942 -4.15989 -4.2165728 -4.2561231 -4.261385 -4.2448926 -4.2226849][-4.274096 -4.2622395 -4.243959 -4.2223735 -4.1948614 -4.1524954 -4.1031957 -4.1315527 -4.1784582 -4.214664 -4.2475204 -4.2759933 -4.2856832 -4.2811222 -4.267715][-4.2894282 -4.2804823 -4.2679615 -4.2551775 -4.2422504 -4.2204723 -4.1992769 -4.220674 -4.2452655 -4.2578044 -4.273006 -4.2914858 -4.3036714 -4.3068366 -4.3016372][-4.3009858 -4.2986126 -4.29232 -4.2835116 -4.2770638 -4.2667522 -4.2582579 -4.2724409 -4.2837157 -4.2837019 -4.2891984 -4.3036218 -4.3145838 -4.3177085 -4.3135028][-4.3048396 -4.3074937 -4.3059611 -4.3000441 -4.2987828 -4.2964239 -4.2917604 -4.2944546 -4.2942977 -4.2895894 -4.2912436 -4.3044896 -4.313736 -4.3144989 -4.3104868][-4.3070602 -4.3113871 -4.3107905 -4.305203 -4.3057718 -4.3084579 -4.3044295 -4.2994313 -4.2930708 -4.2884593 -4.2888751 -4.2994347 -4.3073683 -4.3065367 -4.301456][-4.3042283 -4.3116946 -4.3127112 -4.3075256 -4.3083549 -4.3103805 -4.3036494 -4.2926526 -4.2831373 -4.2809 -4.2844748 -4.294487 -4.3005428 -4.2974858 -4.2905121]]...]
INFO - root - 2017-12-07 23:43:36.029232: step 53610, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 72h:45m:32s remains)
INFO - root - 2017-12-07 23:43:45.669279: step 53620, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 75h:14m:18s remains)
INFO - root - 2017-12-07 23:43:55.360847: step 53630, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 75h:10m:59s remains)
INFO - root - 2017-12-07 23:44:04.928306: step 53640, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 78h:20m:49s remains)
INFO - root - 2017-12-07 23:44:14.636985: step 53650, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.043 sec/batch; 80h:45m:38s remains)
INFO - root - 2017-12-07 23:44:24.250597: step 53660, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 72h:14m:26s remains)
INFO - root - 2017-12-07 23:44:33.935981: step 53670, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.970 sec/batch; 75h:05m:27s remains)
INFO - root - 2017-12-07 23:44:43.776971: step 53680, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 78h:24m:39s remains)
INFO - root - 2017-12-07 23:44:53.308204: step 53690, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 68h:10m:42s remains)
INFO - root - 2017-12-07 23:45:02.888429: step 53700, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 73h:58m:58s remains)
2017-12-07 23:45:03.870614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3030138 -4.2770276 -4.2437158 -4.2320757 -4.229084 -4.2195077 -4.2173028 -4.2255149 -4.2338696 -4.2283058 -4.2128696 -4.2009144 -4.197824 -4.2001572 -4.2031198][-4.2671976 -4.2472696 -4.2234507 -4.2175627 -4.213913 -4.1929708 -4.1772542 -4.1850934 -4.2020741 -4.1928582 -4.1766825 -4.1724353 -4.178185 -4.1895881 -4.2003565][-4.2243209 -4.2136655 -4.1999 -4.1967168 -4.184783 -4.1442084 -4.107038 -4.1176548 -4.1575441 -4.1611452 -4.1472011 -4.1431322 -4.1461043 -4.1579103 -4.1803603][-4.19065 -4.1864166 -4.1796775 -4.1764107 -4.1520729 -4.0912728 -4.0276175 -4.0335388 -4.1053033 -4.1379209 -4.1351085 -4.1307425 -4.1254368 -4.1301017 -4.1590519][-4.1727743 -4.1684833 -4.1625376 -4.1575241 -4.1287947 -4.0521955 -3.9587872 -3.9495585 -4.0459919 -4.1170168 -4.13229 -4.1313477 -4.1234212 -4.1203012 -4.1473026][-4.1674 -4.1591587 -4.1487083 -4.136167 -4.1040173 -4.028811 -3.9318485 -3.91736 -4.0242887 -4.1160717 -4.1394868 -4.135251 -4.1244588 -4.1218767 -4.1455846][-4.1721687 -4.1591382 -4.1435103 -4.12342 -4.0907092 -4.0249906 -3.9492409 -3.9432678 -4.0410967 -4.1371212 -4.1655312 -4.1553478 -4.1328058 -4.1201053 -4.1396246][-4.1923556 -4.1786532 -4.1587076 -4.1319766 -4.1024237 -4.0560322 -4.0092587 -4.0107751 -4.0793552 -4.1588049 -4.1917968 -4.184701 -4.1599264 -4.1409483 -4.1537485][-4.2154784 -4.203845 -4.1846375 -4.15627 -4.1314983 -4.1042795 -4.0833941 -4.0918307 -4.1343985 -4.1873131 -4.2151976 -4.2107162 -4.1948743 -4.1838326 -4.1906953][-4.2441268 -4.2321224 -4.2165079 -4.1939011 -4.1743894 -4.1594915 -4.1540761 -4.1666365 -4.1938357 -4.2260013 -4.2485471 -4.2493434 -4.2423506 -4.2368531 -4.2376914][-4.2703981 -4.2600384 -4.246964 -4.2296042 -4.21514 -4.2072325 -4.2084122 -4.220768 -4.2360458 -4.2548003 -4.2710366 -4.2797303 -4.28142 -4.2807856 -4.2774572][-4.2769237 -4.2692432 -4.262804 -4.2511024 -4.2391038 -4.2326412 -4.238656 -4.2516932 -4.2606339 -4.2684464 -4.2756343 -4.2855754 -4.2933593 -4.2963114 -4.2929997][-4.2584047 -4.2521482 -4.2538857 -4.2505875 -4.242404 -4.232182 -4.23715 -4.2491097 -4.2586255 -4.2613935 -4.2608638 -4.2651916 -4.2745695 -4.280355 -4.2817163][-4.2405596 -4.2311678 -4.233542 -4.233942 -4.224741 -4.2083011 -4.2065749 -4.2150688 -4.2258134 -4.2301307 -4.234262 -4.2408051 -4.2499213 -4.2574263 -4.2629228][-4.2375617 -4.2297349 -4.22941 -4.223321 -4.2070279 -4.1825342 -4.1724896 -4.175807 -4.1854782 -4.1923642 -4.2009649 -4.2091513 -4.2194023 -4.2320981 -4.2432189]]...]
INFO - root - 2017-12-07 23:45:13.669821: step 53710, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.996 sec/batch; 77h:10m:05s remains)
INFO - root - 2017-12-07 23:45:23.199141: step 53720, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 74h:52m:20s remains)
INFO - root - 2017-12-07 23:45:32.790968: step 53730, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 72h:46m:57s remains)
INFO - root - 2017-12-07 23:45:42.191604: step 53740, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 70h:40m:53s remains)
INFO - root - 2017-12-07 23:45:51.736327: step 53750, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.975 sec/batch; 75h:27m:49s remains)
INFO - root - 2017-12-07 23:46:01.156008: step 53760, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 75h:45m:23s remains)
INFO - root - 2017-12-07 23:46:10.764572: step 53770, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 75h:55m:10s remains)
INFO - root - 2017-12-07 23:46:20.375898: step 53780, loss = 2.11, batch loss = 2.06 (8.6 examples/sec; 0.933 sec/batch; 72h:13m:33s remains)
INFO - root - 2017-12-07 23:46:30.106215: step 53790, loss = 2.10, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 74h:33m:09s remains)
INFO - root - 2017-12-07 23:46:39.891674: step 53800, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 71h:31m:39s remains)
2017-12-07 23:46:40.925453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2967248 -4.3122387 -4.3245325 -4.3284225 -4.3282375 -4.3290858 -4.3313375 -4.3334832 -4.335681 -4.3378687 -4.337934 -4.3362222 -4.335784 -4.3360848 -4.3368368][-4.2931213 -4.30738 -4.3192968 -4.3207755 -4.3156819 -4.311811 -4.3102136 -4.3104692 -4.3151789 -4.3211312 -4.3245373 -4.3254075 -4.3263292 -4.3284855 -4.3322964][-4.2854009 -4.2982116 -4.3081555 -4.3035626 -4.2896185 -4.2777953 -4.2702503 -4.268508 -4.2772255 -4.2882891 -4.2966528 -4.3001609 -4.3013067 -4.3048692 -4.3118591][-4.274672 -4.2855868 -4.2904439 -4.2775731 -4.2546897 -4.2348852 -4.2167277 -4.2086077 -4.223989 -4.2449217 -4.2606082 -4.2686086 -4.2702174 -4.2739344 -4.2823639][-4.2588248 -4.2693758 -4.2685966 -4.2474928 -4.2200122 -4.1928062 -4.1591783 -4.1407242 -4.1670704 -4.2010736 -4.2236805 -4.2355065 -4.2372432 -4.2392383 -4.2464981][-4.2375979 -4.244 -4.2360682 -4.2022223 -4.15857 -4.1076374 -4.044065 -4.021338 -4.080843 -4.145462 -4.1825728 -4.20221 -4.2061791 -4.20724 -4.2109947][-4.2141285 -4.2180634 -4.2031884 -4.1503797 -4.0739231 -3.973968 -3.8608098 -3.8436663 -3.9523551 -4.0574617 -4.1153092 -4.1453223 -4.153913 -4.156384 -4.1597362][-4.1842089 -4.1949239 -4.1801133 -4.1166525 -4.0128646 -3.8677583 -3.7199559 -3.7257013 -3.8799448 -4.0079126 -4.0734606 -4.1059084 -4.11642 -4.1193223 -4.1225567][-4.1745443 -4.1920257 -4.1856327 -4.1356487 -4.0505624 -3.9324749 -3.8344703 -3.8703592 -3.9946342 -4.0842543 -4.1231055 -4.1367626 -4.1404157 -4.1408439 -4.1434188][-4.21083 -4.2247119 -4.22115 -4.1877341 -4.1343369 -4.0689716 -4.031198 -4.07049 -4.1436696 -4.1886182 -4.2010784 -4.1994028 -4.1991782 -4.2019968 -4.205245][-4.2539396 -4.2617993 -4.25718 -4.2327557 -4.1968422 -4.1626949 -4.1511374 -4.1768079 -4.2107134 -4.2278442 -4.22678 -4.2239394 -4.2263808 -4.2307844 -4.2348046][-4.2635727 -4.2694263 -4.2677221 -4.2512245 -4.2246537 -4.2019749 -4.1937842 -4.2019792 -4.2131577 -4.2173848 -4.2128572 -4.2104745 -4.2156129 -4.2223 -4.2272463][-4.2410617 -4.2491159 -4.2541828 -4.2508497 -4.2357869 -4.2192464 -4.2101092 -4.2047415 -4.1977644 -4.1880288 -4.178812 -4.1765985 -4.1848216 -4.1976819 -4.20765][-4.2088203 -4.2221751 -4.2361989 -4.2428226 -4.2386732 -4.2288365 -4.2173057 -4.2028346 -4.1854286 -4.1688657 -4.1563268 -4.153347 -4.1647468 -4.1831622 -4.1989441][-4.1902361 -4.2066317 -4.222661 -4.2315993 -4.2343564 -4.2303948 -4.2190318 -4.2011442 -4.1779747 -4.156 -4.1396484 -4.1336432 -4.1478858 -4.1735077 -4.1991405]]...]
INFO - root - 2017-12-07 23:46:50.714180: step 53810, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 77h:17m:43s remains)
INFO - root - 2017-12-07 23:47:00.403191: step 53820, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 72h:06m:43s remains)
INFO - root - 2017-12-07 23:47:10.071843: step 53830, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.945 sec/batch; 73h:09m:28s remains)
INFO - root - 2017-12-07 23:47:19.489978: step 53840, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 74h:08m:03s remains)
INFO - root - 2017-12-07 23:47:29.151143: step 53850, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.006 sec/batch; 77h:50m:56s remains)
INFO - root - 2017-12-07 23:47:38.840601: step 53860, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 76h:05m:38s remains)
INFO - root - 2017-12-07 23:47:48.522755: step 53870, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 74h:56m:29s remains)
INFO - root - 2017-12-07 23:47:58.311515: step 53880, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 78h:33m:37s remains)
INFO - root - 2017-12-07 23:48:08.108597: step 53890, loss = 2.09, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 77h:12m:51s remains)
INFO - root - 2017-12-07 23:48:17.803689: step 53900, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 72h:32m:11s remains)
2017-12-07 23:48:18.901523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2941418 -4.2912912 -4.2898917 -4.2892523 -4.2903156 -4.2928038 -4.2958045 -4.2976809 -4.2982407 -4.2977209 -4.2968469 -4.2955208 -4.2941718 -4.2938118 -4.2937393][-4.266861 -4.263401 -4.2620573 -4.2620769 -4.26408 -4.26708 -4.2719154 -4.2759962 -4.2783489 -4.2782526 -4.2765188 -4.2734356 -4.2703371 -4.2688432 -4.26652][-4.2485738 -4.2450886 -4.2428145 -4.2419286 -4.2434268 -4.2454605 -4.2517033 -4.2599106 -4.265914 -4.2679887 -4.2662377 -4.2619424 -4.2570033 -4.2540069 -4.2486815][-4.2314162 -4.2249188 -4.2191682 -4.2140808 -4.2123337 -4.2128272 -4.2224503 -4.2384171 -4.2519307 -4.2595792 -4.2604284 -4.2560244 -4.2481694 -4.24067 -4.2299886][-4.2175169 -4.2078567 -4.19911 -4.1896691 -4.1828995 -4.1792316 -4.1871972 -4.2038693 -4.2195745 -4.2323503 -4.2401333 -4.2425051 -4.2380338 -4.2306867 -4.2167797][-4.2047811 -4.1943398 -4.1831722 -4.1685929 -4.152482 -4.1390309 -4.1382875 -4.1481838 -4.1619825 -4.1800418 -4.1971092 -4.2093673 -4.2137704 -4.2144675 -4.2061834][-4.1827807 -4.171586 -4.1568556 -4.1353612 -4.1107664 -4.0864429 -4.0742168 -4.0749717 -4.0850291 -4.1049743 -4.1285467 -4.1508365 -4.1664495 -4.180337 -4.1859708][-4.159513 -4.1494164 -4.1373391 -4.1184082 -4.0927339 -4.06251 -4.0417781 -4.0342903 -4.0377035 -4.0516386 -4.0716481 -4.0938077 -4.1135826 -4.1369648 -4.156002][-4.16432 -4.1571393 -4.1509228 -4.1402454 -4.1208549 -4.0936508 -4.0724769 -4.0632963 -4.0624785 -4.0670624 -4.0759907 -4.0866175 -4.0975432 -4.1158767 -4.1370945][-4.1865449 -4.1846967 -4.1873846 -4.1890221 -4.1799674 -4.1617231 -4.1455307 -4.1373539 -4.132237 -4.1271691 -4.1259308 -4.1247253 -4.1220174 -4.1266975 -4.1400976][-4.2046103 -4.2068472 -4.2179632 -4.231338 -4.231215 -4.2218442 -4.21254 -4.2080283 -4.2018237 -4.1905041 -4.1825027 -4.1729088 -4.1611395 -4.1537304 -4.1557508][-4.2059064 -4.206356 -4.2190986 -4.2388415 -4.2455454 -4.2437935 -4.2438927 -4.2468257 -4.2438736 -4.2315197 -4.220583 -4.2074447 -4.19253 -4.17949 -4.1748233][-4.1996579 -4.1961894 -4.2045383 -4.2233968 -4.2332482 -4.2362928 -4.2427759 -4.2505465 -4.2502947 -4.2401195 -4.2323709 -4.221921 -4.2101064 -4.1976032 -4.1913929][-4.2032208 -4.196538 -4.1985254 -4.2134447 -4.2230873 -4.2262654 -4.2346897 -4.2450085 -4.2469759 -4.2386708 -4.2338119 -4.2282076 -4.222271 -4.2144976 -4.2094574][-4.2098384 -4.2012129 -4.200386 -4.2112312 -4.2191849 -4.2184405 -4.2234893 -4.2334561 -4.2394452 -4.2349362 -4.2322683 -4.2299857 -4.2296319 -4.228816 -4.2245708]]...]
INFO - root - 2017-12-07 23:48:28.672541: step 53910, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 74h:01m:33s remains)
INFO - root - 2017-12-07 23:48:38.343173: step 53920, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 72h:25m:22s remains)
INFO - root - 2017-12-07 23:48:47.923630: step 53930, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 67h:29m:07s remains)
INFO - root - 2017-12-07 23:48:57.493584: step 53940, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 74h:08m:08s remains)
INFO - root - 2017-12-07 23:49:07.150223: step 53950, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 77h:40m:05s remains)
INFO - root - 2017-12-07 23:49:16.914052: step 53960, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 75h:12m:19s remains)
INFO - root - 2017-12-07 23:49:26.581693: step 53970, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 72h:11m:25s remains)
INFO - root - 2017-12-07 23:49:36.223659: step 53980, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 77h:04m:53s remains)
INFO - root - 2017-12-07 23:49:45.936168: step 53990, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 76h:16m:52s remains)
INFO - root - 2017-12-07 23:49:55.620004: step 54000, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 73h:42m:37s remains)
2017-12-07 23:49:56.561092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2856989 -4.287991 -4.2889638 -4.2888823 -4.2879 -4.2861538 -4.2838664 -4.2820263 -4.2820635 -4.283659 -4.2859898 -4.2888279 -4.2915258 -4.2944779 -4.2973576][-4.2853026 -4.2873425 -4.2881584 -4.2879009 -4.2865143 -4.283927 -4.2802625 -4.2771039 -4.2766762 -4.2784991 -4.2812824 -4.28496 -4.2888403 -4.2929583 -4.2964921][-4.2828274 -4.2836123 -4.2832627 -4.2818556 -4.2792521 -4.2753534 -4.2705617 -4.2669973 -4.266655 -4.2685733 -4.2721186 -4.27713 -4.2827439 -4.2886472 -4.293529][-4.2780094 -4.2762322 -4.2735233 -4.2705812 -4.2674551 -4.2628088 -4.2570453 -4.2532158 -4.252429 -4.2542925 -4.2583556 -4.2647867 -4.2728434 -4.2816515 -4.2892547][-4.2672739 -4.263567 -4.2592163 -4.2556167 -4.2523909 -4.2467985 -4.2399836 -4.2350779 -4.2315497 -4.2308784 -4.2338414 -4.2414904 -4.25283 -4.2661591 -4.2786403][-4.2479954 -4.2438922 -4.2398224 -4.2371736 -4.2340364 -4.2267671 -4.2185917 -4.212122 -4.2054238 -4.2009864 -4.2012029 -4.2084041 -4.22192 -4.2389383 -4.25584][-4.2271333 -4.2251463 -4.2246976 -4.2262797 -4.2251778 -4.2170129 -4.2070704 -4.1974268 -4.1862464 -4.1762776 -4.1713791 -4.1745548 -4.1855536 -4.2015896 -4.2197337][-4.2159209 -4.2165475 -4.2194605 -4.2237277 -4.2230892 -4.2145605 -4.2040749 -4.1924758 -4.1783433 -4.1646938 -4.1551213 -4.1525359 -4.157485 -4.1681924 -4.1833115][-4.2265325 -4.2271848 -4.2302365 -4.2339115 -4.2325311 -4.2255726 -4.2170277 -4.2066979 -4.1941757 -4.1812243 -4.1699834 -4.1631217 -4.1618767 -4.16451 -4.1722245][-4.23858 -4.2388706 -4.2425575 -4.248301 -4.2509913 -4.2498422 -4.2464128 -4.2408829 -4.23347 -4.2231531 -4.2112966 -4.2018757 -4.1964774 -4.1934214 -4.1954226][-4.2426677 -4.2427197 -4.2477832 -4.257422 -4.2652793 -4.2699285 -4.2715659 -4.272049 -4.27125 -4.2659216 -4.2561378 -4.2465496 -4.2396121 -4.2340956 -4.232851][-4.2481003 -4.2477608 -4.2537365 -4.2654357 -4.2761765 -4.283823 -4.2880735 -4.29117 -4.2932463 -4.2916985 -4.2852292 -4.2780819 -4.2731309 -4.2693 -4.2683845][-4.250463 -4.2501678 -4.2560015 -4.2665997 -4.2770252 -4.2853837 -4.2900333 -4.2925448 -4.2931957 -4.2926192 -4.2889495 -4.2852912 -4.2838087 -4.2835951 -4.2850218][-4.2575283 -4.2557993 -4.2588935 -4.26532 -4.2721133 -4.27792 -4.2804728 -4.2803125 -4.2787795 -4.2779865 -4.2769103 -4.2772536 -4.278914 -4.2814946 -4.28479][-4.2678442 -4.2651334 -4.2653079 -4.267004 -4.270144 -4.2724943 -4.27246 -4.2698483 -4.2665009 -4.2646246 -4.2637544 -4.2659903 -4.2692642 -4.2731771 -4.27731]]...]
INFO - root - 2017-12-07 23:50:06.424263: step 54010, loss = 2.06, batch loss = 2.01 (7.8 examples/sec; 1.020 sec/batch; 78h:53m:54s remains)
INFO - root - 2017-12-07 23:50:16.190906: step 54020, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 76h:36m:41s remains)
INFO - root - 2017-12-07 23:50:25.577084: step 54030, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 72h:33m:49s remains)
INFO - root - 2017-12-07 23:50:35.243469: step 54040, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.942 sec/batch; 72h:49m:45s remains)
INFO - root - 2017-12-07 23:50:45.029245: step 54050, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.978 sec/batch; 75h:39m:50s remains)
INFO - root - 2017-12-07 23:50:54.814391: step 54060, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 73h:40m:22s remains)
INFO - root - 2017-12-07 23:51:04.352436: step 54070, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 73h:36m:12s remains)
INFO - root - 2017-12-07 23:51:13.941320: step 54080, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 71h:40m:56s remains)
INFO - root - 2017-12-07 23:51:23.577178: step 54090, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 72h:50m:05s remains)
INFO - root - 2017-12-07 23:51:33.363298: step 54100, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.030 sec/batch; 79h:39m:03s remains)
2017-12-07 23:51:34.272895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1884155 -4.1662879 -4.1503181 -4.1527438 -4.1684966 -4.1856427 -4.201252 -4.2132587 -4.2256722 -4.2396069 -4.2514625 -4.248899 -4.2314882 -4.21221 -4.2027192][-4.2262106 -4.20358 -4.1871219 -4.1888342 -4.2126923 -4.2409635 -4.2625384 -4.2759814 -4.2848411 -4.2901926 -4.2904649 -4.2793832 -4.2572122 -4.2361488 -4.2266693][-4.2448049 -4.221384 -4.2069268 -4.2123222 -4.2425027 -4.2740035 -4.2942839 -4.3032422 -4.3065758 -4.3053646 -4.2985268 -4.2835927 -4.2628131 -4.2450404 -4.2379756][-4.2409143 -4.2156835 -4.2064776 -4.2174139 -4.2481303 -4.2728176 -4.2864556 -4.2923956 -4.2924504 -4.286272 -4.2738867 -4.2588367 -4.2449665 -4.2339993 -4.2302227][-4.2353125 -4.2105589 -4.2037077 -4.2131238 -4.2352934 -4.2474322 -4.2530956 -4.2585745 -4.2591972 -4.2484574 -4.228497 -4.2136364 -4.2084942 -4.20531 -4.2044396][-4.2394977 -4.2154679 -4.2043676 -4.2046757 -4.2096291 -4.2034111 -4.200943 -4.2081842 -4.2126245 -4.2010612 -4.176568 -4.1642847 -4.1668916 -4.1693583 -4.1713247][-4.246593 -4.2211332 -4.20002 -4.1865807 -4.1702847 -4.1450582 -4.1380076 -4.1531324 -4.1679816 -4.1613116 -4.1388307 -4.1322842 -4.1395583 -4.1417766 -4.14171][-4.247745 -4.2193933 -4.1900163 -4.1620913 -4.1264033 -4.0910621 -4.0909643 -4.1221957 -4.150785 -4.150702 -4.1330233 -4.1305623 -4.1365466 -4.131526 -4.1226029][-4.2359071 -4.2077689 -4.1753912 -4.1426067 -4.104073 -4.0798664 -4.0957575 -4.1345277 -4.1630273 -4.1615105 -4.1468291 -4.145051 -4.1458659 -4.1335497 -4.1161923][-4.2144895 -4.1923189 -4.1657825 -4.1397653 -4.1155648 -4.1126089 -4.135128 -4.1657476 -4.1848664 -4.1807451 -4.1675467 -4.1652813 -4.1639209 -4.1487632 -4.1268597][-4.1965303 -4.18099 -4.1605577 -4.1425405 -4.1340342 -4.1482415 -4.1699457 -4.1899681 -4.203618 -4.2019334 -4.1938806 -4.1917248 -4.1876569 -4.1714315 -4.1519156][-4.1810417 -4.1699672 -4.1558895 -4.1437168 -4.1458435 -4.1688809 -4.1894574 -4.2048297 -4.2178278 -4.2206693 -4.2176552 -4.2147 -4.2106438 -4.199224 -4.1863379][-4.1731358 -4.1659832 -4.1575975 -4.1483483 -4.150979 -4.1720738 -4.1878881 -4.1979346 -4.2069545 -4.21187 -4.2154026 -4.2163367 -4.2161546 -4.211637 -4.2054987][-4.1833463 -4.178103 -4.1725893 -4.1634889 -4.1607814 -4.1728745 -4.1805072 -4.1848617 -4.1919823 -4.201077 -4.2109952 -4.2168651 -4.2184529 -4.2164297 -4.2116094][-4.2113724 -4.2049642 -4.2002511 -4.1907234 -4.1840014 -4.1877103 -4.1863632 -4.1867318 -4.1952109 -4.2067304 -4.218586 -4.2256637 -4.2271113 -4.2243166 -4.2169342]]...]
INFO - root - 2017-12-07 23:51:43.960460: step 54110, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 75h:41m:02s remains)
INFO - root - 2017-12-07 23:51:53.663089: step 54120, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 1.002 sec/batch; 77h:31m:15s remains)
INFO - root - 2017-12-07 23:52:03.156485: step 54130, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 72h:47m:38s remains)
INFO - root - 2017-12-07 23:52:12.894155: step 54140, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 74h:41m:26s remains)
INFO - root - 2017-12-07 23:52:22.753680: step 54150, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 73h:49m:30s remains)
INFO - root - 2017-12-07 23:52:32.535316: step 54160, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.950 sec/batch; 73h:27m:31s remains)
INFO - root - 2017-12-07 23:52:42.255486: step 54170, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 72h:59m:53s remains)
INFO - root - 2017-12-07 23:52:51.930652: step 54180, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 75h:26m:47s remains)
INFO - root - 2017-12-07 23:53:01.594910: step 54190, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 72h:22m:24s remains)
INFO - root - 2017-12-07 23:53:11.095087: step 54200, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 76h:09m:58s remains)
2017-12-07 23:53:12.096858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1962123 -4.1527591 -4.1193666 -4.0939174 -4.0664144 -4.0576458 -4.048687 -4.0279493 -4.0314589 -4.0681834 -4.098516 -4.1078429 -4.1224909 -4.1376719 -4.1453915][-4.187696 -4.1415415 -4.1066427 -4.0738373 -4.046876 -4.0559154 -4.0681705 -4.0537472 -4.0606108 -4.10766 -4.143115 -4.1452827 -4.1551962 -4.1636357 -4.1574073][-4.1884255 -4.1438923 -4.1114264 -4.0805111 -4.0637841 -4.0855236 -4.1082811 -4.08334 -4.0793238 -4.1349592 -4.1756926 -4.1781435 -4.1850743 -4.1852913 -4.1694012][-4.1942596 -4.1552057 -4.1280723 -4.1075215 -4.0981464 -4.1145296 -4.1270514 -4.0777869 -4.0565262 -4.1154423 -4.1738396 -4.1940613 -4.2047567 -4.2026229 -4.1855917][-4.1977382 -4.1617179 -4.1387415 -4.1242533 -4.1162877 -4.1188965 -4.1014051 -4.0244565 -3.9927423 -4.0675468 -4.14727 -4.1822495 -4.1998343 -4.2076511 -4.201086][-4.2013659 -4.1661377 -4.1478267 -4.1407585 -4.1336279 -4.1118212 -4.04603 -3.9274409 -3.885159 -3.9928415 -4.1033487 -4.1570334 -4.1898074 -4.2187681 -4.2280512][-4.2065196 -4.1749134 -4.1616483 -4.1593413 -4.1488433 -4.1083813 -3.9993274 -3.8430405 -3.8071806 -3.947669 -4.0769429 -4.1399994 -4.1854963 -4.2292066 -4.2497821][-4.2098951 -4.1817422 -4.1723928 -4.1762514 -4.1709404 -4.13643 -4.0358062 -3.9079552 -3.8945963 -4.0137343 -4.1121454 -4.1581864 -4.1951561 -4.2360139 -4.2562652][-4.2132163 -4.1896086 -4.1873236 -4.199677 -4.2065911 -4.1896868 -4.1219335 -4.0416746 -4.0415974 -4.1250176 -4.1829491 -4.2007794 -4.2138958 -4.2396293 -4.2522049][-4.2209792 -4.2000709 -4.2006683 -4.2194085 -4.2339668 -4.2301154 -4.1887679 -4.1405935 -4.1471004 -4.2030296 -4.2317958 -4.2315125 -4.2250543 -4.2303548 -4.2309155][-4.22868 -4.2081327 -4.2093439 -4.2326727 -4.2520542 -4.2557092 -4.2311373 -4.1968017 -4.1944361 -4.227582 -4.2377477 -4.2255554 -4.2106042 -4.2053537 -4.1996231][-4.2371221 -4.2174606 -4.217339 -4.2391362 -4.2571135 -4.2622843 -4.2465744 -4.2163444 -4.2023878 -4.2206068 -4.2213488 -4.2035804 -4.1875873 -4.1841078 -4.1824813][-4.2468138 -4.2279367 -4.2246246 -4.2407122 -4.2553263 -4.2593966 -4.2477818 -4.219336 -4.2013516 -4.2189245 -4.2212887 -4.2069197 -4.1971226 -4.1964884 -4.1978073][-4.25498 -4.2364964 -4.2314048 -4.2418251 -4.2528181 -4.25549 -4.2443538 -4.2188272 -4.2072592 -4.2314258 -4.2406864 -4.2369423 -4.2344861 -4.2351675 -4.2358327][-4.2608109 -4.24372 -4.2396851 -4.2486181 -4.2589893 -4.261415 -4.24823 -4.2224979 -4.2158012 -4.2472353 -4.2655897 -4.2688432 -4.2704229 -4.2728276 -4.2752914]]...]
INFO - root - 2017-12-07 23:53:21.910400: step 54210, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:31m:03s remains)
INFO - root - 2017-12-07 23:53:31.518158: step 54220, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 74h:09m:23s remains)
INFO - root - 2017-12-07 23:53:41.082357: step 54230, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.013 sec/batch; 78h:15m:59s remains)
INFO - root - 2017-12-07 23:53:50.858790: step 54240, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 74h:01m:42s remains)
INFO - root - 2017-12-07 23:54:00.514680: step 54250, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 74h:11m:48s remains)
INFO - root - 2017-12-07 23:54:10.229600: step 54260, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 72h:05m:46s remains)
INFO - root - 2017-12-07 23:54:19.894461: step 54270, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 72h:41m:06s remains)
INFO - root - 2017-12-07 23:54:29.701913: step 54280, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.024 sec/batch; 79h:10m:35s remains)
INFO - root - 2017-12-07 23:54:39.381239: step 54290, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.023 sec/batch; 79h:01m:34s remains)
INFO - root - 2017-12-07 23:54:48.833615: step 54300, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 74h:18m:45s remains)
2017-12-07 23:54:49.768628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3397126 -4.3562007 -4.3436885 -4.2873163 -4.2088661 -4.1337419 -4.0886011 -4.0896583 -4.134841 -4.1967616 -4.2388043 -4.2492986 -4.2493877 -4.2501535 -4.2442102][-4.3418703 -4.3604913 -4.3506169 -4.297101 -4.222734 -4.1402922 -4.077137 -4.0707474 -4.1246481 -4.1971555 -4.2488136 -4.2738514 -4.2881732 -4.2876916 -4.2699785][-4.3456535 -4.3640695 -4.3527641 -4.2943797 -4.216332 -4.1267824 -4.0484066 -4.031342 -4.0877752 -4.168529 -4.2300215 -4.2684517 -4.2955794 -4.29881 -4.2798586][-4.3478613 -4.3665366 -4.3518114 -4.2864361 -4.1985235 -4.09961 -4.0057726 -3.9710577 -4.0250998 -4.1141891 -4.1907392 -4.2455082 -4.2876663 -4.300045 -4.2818918][-4.3482332 -4.3648839 -4.3456488 -4.2734337 -4.17851 -4.071022 -3.9609418 -3.9066691 -3.9576221 -4.0587344 -4.1518908 -4.2278132 -4.2821131 -4.2975383 -4.27644][-4.3456774 -4.3612361 -4.33845 -4.2612677 -4.1602468 -4.0445943 -3.9215033 -3.8467002 -3.9012971 -4.0189447 -4.1328235 -4.2279944 -4.2900457 -4.303978 -4.2779408][-4.3425455 -4.3571811 -4.334166 -4.2564063 -4.1529961 -4.030642 -3.8999329 -3.823226 -3.8887618 -4.0149531 -4.1415815 -4.2463641 -4.3073063 -4.3177052 -4.2826557][-4.3393497 -4.3544078 -4.3343849 -4.2611728 -4.1670237 -4.0546231 -3.9346957 -3.8776646 -3.948385 -4.0621486 -4.1789069 -4.273448 -4.3236566 -4.325911 -4.2794123][-4.3367677 -4.3539104 -4.3389335 -4.2758307 -4.1980076 -4.10847 -4.0150065 -3.9832633 -4.0489931 -4.1372685 -4.2287593 -4.3035231 -4.3387785 -4.3309536 -4.2744813][-4.334455 -4.3523312 -4.3426189 -4.291214 -4.2310596 -4.1658921 -4.1042318 -4.095984 -4.149807 -4.2101455 -4.2748513 -4.3267212 -4.3491397 -4.3363638 -4.2806339][-4.3299747 -4.3471966 -4.3431396 -4.3027067 -4.25938 -4.2185488 -4.1848946 -4.190876 -4.2307911 -4.269628 -4.3140779 -4.3486872 -4.3602152 -4.3422132 -4.2903643][-4.3254528 -4.341176 -4.3409958 -4.3106451 -4.2818904 -4.2605596 -4.2480664 -4.2597971 -4.2858639 -4.3122139 -4.3434777 -4.3674984 -4.3722835 -4.350719 -4.3049664][-4.3201447 -4.3321557 -4.3345041 -4.31477 -4.2985525 -4.2895041 -4.2882209 -4.2995019 -4.3145528 -4.3347144 -4.3584843 -4.3767285 -4.3784823 -4.355885 -4.3168974][-4.3132668 -4.3206882 -4.3254156 -4.31526 -4.3065991 -4.3040566 -4.3082838 -4.3191004 -4.3293557 -4.3433638 -4.3587022 -4.3706012 -4.3704414 -4.3520517 -4.3235216][-4.3055038 -4.308445 -4.3137407 -4.3085361 -4.3038912 -4.3063803 -4.3133473 -4.3236489 -4.3323741 -4.3391843 -4.3449812 -4.35155 -4.3518171 -4.3408675 -4.3242636]]...]
INFO - root - 2017-12-07 23:54:59.506792: step 54310, loss = 2.08, batch loss = 2.02 (7.6 examples/sec; 1.048 sec/batch; 80h:56m:44s remains)
INFO - root - 2017-12-07 23:55:09.209060: step 54320, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 69h:53m:41s remains)
INFO - root - 2017-12-07 23:55:18.907781: step 54330, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 73h:27m:50s remains)
INFO - root - 2017-12-07 23:55:28.502486: step 54340, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 72h:08m:31s remains)
INFO - root - 2017-12-07 23:55:38.114726: step 54350, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 76h:28m:12s remains)
INFO - root - 2017-12-07 23:55:47.923100: step 54360, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.015 sec/batch; 78h:26m:03s remains)
INFO - root - 2017-12-07 23:55:57.403956: step 54370, loss = 2.06, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 76h:39m:33s remains)
INFO - root - 2017-12-07 23:56:07.031071: step 54380, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 74h:09m:26s remains)
INFO - root - 2017-12-07 23:56:16.907302: step 54390, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 77h:26m:50s remains)
INFO - root - 2017-12-07 23:56:26.560010: step 54400, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 74h:08m:12s remains)
2017-12-07 23:56:27.506331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3361931 -4.3296194 -4.3143945 -4.2916307 -4.258306 -4.2131367 -4.1675777 -4.15908 -4.1874948 -4.2299771 -4.2656388 -4.2941694 -4.3077655 -4.2950392 -4.2554483][-4.3358011 -4.3291011 -4.313623 -4.2887235 -4.2486935 -4.1934209 -4.1413875 -4.1357388 -4.1686106 -4.2125111 -4.2495971 -4.2771411 -4.2907367 -4.2852993 -4.2565775][-4.3359256 -4.3305879 -4.3168435 -4.2900023 -4.2442226 -4.1815562 -4.1241612 -4.1262441 -4.1646967 -4.2078223 -4.2403755 -4.2574053 -4.2670727 -4.2681336 -4.2518735][-4.33568 -4.3313923 -4.3211708 -4.2923355 -4.245409 -4.1849656 -4.1303911 -4.1384439 -4.1742883 -4.2105412 -4.2327895 -4.2368231 -4.2407484 -4.246788 -4.242794][-4.3359294 -4.332725 -4.3227878 -4.2901053 -4.2384343 -4.1773787 -4.1227431 -4.13559 -4.1772618 -4.2104664 -4.2276039 -4.2252731 -4.2244473 -4.2313886 -4.2330651][-4.3364291 -4.332098 -4.3165669 -4.2735152 -4.2062387 -4.1328139 -4.0752163 -4.0971236 -4.1617765 -4.2050438 -4.2205014 -4.2134967 -4.2095861 -4.2164426 -4.2243462][-4.3369837 -4.3291521 -4.3070555 -4.2555456 -4.1689644 -4.0694113 -3.9970267 -4.0195804 -4.1126018 -4.1778512 -4.2009225 -4.2011676 -4.20323 -4.2118773 -4.22171][-4.3387022 -4.3283372 -4.3028383 -4.2473207 -4.1445856 -4.0163093 -3.912688 -3.9166183 -4.0305758 -4.1240668 -4.1648717 -4.1818972 -4.1944709 -4.2047806 -4.2146883][-4.3396735 -4.3288131 -4.304121 -4.2483249 -4.1363606 -3.994463 -3.8770783 -3.8663361 -3.9866123 -4.0921216 -4.1427383 -4.1689892 -4.1877561 -4.1996112 -4.2062769][-4.3378191 -4.3277836 -4.306128 -4.2560177 -4.1517487 -4.0261049 -3.9297585 -3.9277515 -4.0329762 -4.1230159 -4.16425 -4.1840353 -4.1973433 -4.2029896 -4.2021761][-4.3341813 -4.3236642 -4.3041654 -4.2600784 -4.1728992 -4.0771685 -4.01114 -4.0183764 -4.1029716 -4.1705194 -4.1977034 -4.2065525 -4.2109404 -4.2118354 -4.20591][-4.3299122 -4.3166785 -4.2962756 -4.2570109 -4.1864414 -4.116004 -4.0767503 -4.096735 -4.1685624 -4.2211118 -4.2396431 -4.2394891 -4.2346883 -4.2336974 -4.2254472][-4.3269129 -4.3110547 -4.2903171 -4.2567592 -4.2016144 -4.1525989 -4.1347194 -4.1640787 -4.2241468 -4.2641163 -4.2773738 -4.2722015 -4.2654443 -4.2640958 -4.2548561][-4.3262119 -4.3106828 -4.292387 -4.2663145 -4.2257042 -4.1929827 -4.1875849 -4.2164536 -4.2613597 -4.2906513 -4.3023753 -4.2992296 -4.2949319 -4.2952414 -4.2874365][-4.3288164 -4.3152242 -4.2996707 -4.2794828 -4.2502708 -4.2269478 -4.2257519 -4.250761 -4.2865472 -4.3097525 -4.3200645 -4.320642 -4.3195777 -4.3206496 -4.3159003]]...]
INFO - root - 2017-12-07 23:56:37.304954: step 54410, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 73h:20m:30s remains)
INFO - root - 2017-12-07 23:56:46.907378: step 54420, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 75h:53m:04s remains)
INFO - root - 2017-12-07 23:56:56.727846: step 54430, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 76h:53m:39s remains)
INFO - root - 2017-12-07 23:57:06.287376: step 54440, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 75h:52m:36s remains)
INFO - root - 2017-12-07 23:57:16.216061: step 54450, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 74h:58m:54s remains)
INFO - root - 2017-12-07 23:57:25.905490: step 54460, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.942 sec/batch; 72h:46m:52s remains)
INFO - root - 2017-12-07 23:57:35.473322: step 54470, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 73h:13m:26s remains)
INFO - root - 2017-12-07 23:57:45.330611: step 54480, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.026 sec/batch; 79h:14m:00s remains)
INFO - root - 2017-12-07 23:57:55.030588: step 54490, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.922 sec/batch; 71h:10m:02s remains)
INFO - root - 2017-12-07 23:58:04.670500: step 54500, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 74h:45m:25s remains)
2017-12-07 23:58:05.622393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1310349 -4.1283364 -4.1236396 -4.1320314 -4.1495214 -4.1685128 -4.1813016 -4.1932983 -4.2115278 -4.2279172 -4.2344027 -4.2226062 -4.2089572 -4.1867294 -4.1698852][-4.1251931 -4.125895 -4.1213531 -4.1266761 -4.1442051 -4.1662779 -4.1809874 -4.1920495 -4.2046022 -4.2196751 -4.2261043 -4.214365 -4.1988964 -4.1738486 -4.1554184][-4.1358538 -4.1380129 -4.1366596 -4.142437 -4.1616759 -4.181982 -4.1903205 -4.1930175 -4.1987405 -4.2142158 -4.2251716 -4.2181973 -4.2019653 -4.1778283 -4.1582971][-4.1565852 -4.1610332 -4.1581097 -4.1597834 -4.1731858 -4.183393 -4.1814318 -4.1726494 -4.1737719 -4.1928792 -4.2114859 -4.2144289 -4.2015738 -4.1783752 -4.1563225][-4.1819291 -4.1835804 -4.1741896 -4.1633153 -4.1589627 -4.153244 -4.1410556 -4.1264863 -4.1322532 -4.1589265 -4.1849475 -4.1952605 -4.191071 -4.1748323 -4.15865][-4.203342 -4.1944795 -4.1753078 -4.1484861 -4.1208773 -4.0920773 -4.0621605 -4.0493097 -4.0711102 -4.1120892 -4.1528058 -4.1770697 -4.1885166 -4.1859031 -4.1796112][-4.227272 -4.2060189 -4.1780982 -4.1345749 -4.0843959 -4.0238876 -3.9649105 -3.9576132 -4.0125303 -4.0787821 -4.1381631 -4.179534 -4.2061582 -4.219367 -4.2204309][-4.2532339 -4.2237558 -4.189702 -4.1392174 -4.074842 -3.9904821 -3.9086881 -3.9129002 -3.993228 -4.0785036 -4.1501641 -4.2026105 -4.2353454 -4.2583084 -4.2664833][-4.2680111 -4.2374692 -4.20432 -4.1585693 -4.0970793 -4.0218906 -3.958174 -3.9687526 -4.0362024 -4.1120853 -4.1772828 -4.2260628 -4.255394 -4.2813587 -4.296277][-4.262681 -4.2387486 -4.2113733 -4.1743093 -4.1225867 -4.0700841 -4.0369253 -4.05089 -4.09825 -4.1544294 -4.204751 -4.241694 -4.2630553 -4.2831321 -4.3009295][-4.2445564 -4.229352 -4.2057481 -4.1746769 -4.1311789 -4.095645 -4.0876741 -4.1111584 -4.149281 -4.1904082 -4.2269287 -4.253653 -4.2690682 -4.283246 -4.3002682][-4.2261496 -4.2162228 -4.1942282 -4.1615448 -4.1259975 -4.1025791 -4.1135769 -4.1481175 -4.1868773 -4.21954 -4.2445745 -4.2627621 -4.2691383 -4.2743635 -4.2839017][-4.2198267 -4.2111192 -4.1880383 -4.1550918 -4.1331172 -4.1259952 -4.14298 -4.1767669 -4.2130165 -4.2395787 -4.2576766 -4.2664037 -4.2647629 -4.2598991 -4.2605672][-4.2137423 -4.208395 -4.1898952 -4.1656961 -4.1576686 -4.1629949 -4.1795378 -4.2064667 -4.2350225 -4.25672 -4.2694092 -4.2691941 -4.2597337 -4.2466183 -4.2407646][-4.2079577 -4.207437 -4.2001514 -4.1896424 -4.1867614 -4.1927977 -4.2054605 -4.2258639 -4.2462263 -4.2608428 -4.2681379 -4.2646456 -4.2542968 -4.2415423 -4.2336912]]...]
INFO - root - 2017-12-07 23:58:15.444263: step 54510, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 73h:06m:52s remains)
INFO - root - 2017-12-07 23:58:25.037920: step 54520, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.044 sec/batch; 80h:36m:37s remains)
INFO - root - 2017-12-07 23:58:34.779552: step 54530, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:23m:17s remains)
INFO - root - 2017-12-07 23:58:44.267676: step 54540, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 72h:33m:10s remains)
INFO - root - 2017-12-07 23:58:53.905165: step 54550, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 73h:26m:01s remains)
INFO - root - 2017-12-07 23:59:03.548237: step 54560, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 71h:55m:28s remains)
INFO - root - 2017-12-07 23:59:13.379169: step 54570, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 76h:18m:48s remains)
INFO - root - 2017-12-07 23:59:23.108008: step 54580, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 76h:35m:15s remains)
INFO - root - 2017-12-07 23:59:32.732086: step 54590, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 73h:06m:34s remains)
INFO - root - 2017-12-07 23:59:42.420660: step 54600, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 70h:42m:10s remains)
2017-12-07 23:59:43.351439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2996225 -4.2831736 -4.2664266 -4.2434797 -4.2205062 -4.2024608 -4.1971908 -4.2048211 -4.2255607 -4.2579126 -4.2906766 -4.3175287 -4.3365498 -4.3424087 -4.342526][-4.273931 -4.2544103 -4.2293673 -4.1983109 -4.1718669 -4.151824 -4.1491194 -4.16557 -4.202239 -4.2470183 -4.2851186 -4.3154273 -4.3344283 -4.33805 -4.3372269][-4.2456956 -4.2244363 -4.197619 -4.1691613 -4.147171 -4.1301894 -4.1312122 -4.1569891 -4.20296 -4.2506914 -4.2886214 -4.3181381 -4.3333321 -4.3347406 -4.3334408][-4.2230964 -4.1975336 -4.171504 -4.1502571 -4.1325741 -4.1183391 -4.1271596 -4.1640425 -4.2111583 -4.2558293 -4.2917161 -4.3188124 -4.3318243 -4.33329 -4.3311129][-4.2109761 -4.1773248 -4.1454115 -4.1195459 -4.1017933 -4.0946469 -4.1183362 -4.168571 -4.2153697 -4.2568541 -4.2910323 -4.3158913 -4.3299642 -4.3318529 -4.3286042][-4.1928954 -4.1468811 -4.0972557 -4.0584464 -4.0375648 -4.0357747 -4.0724974 -4.1373591 -4.1957927 -4.2445474 -4.2818036 -4.3105474 -4.3266807 -4.3288307 -4.3251963][-4.1760559 -4.1248865 -4.0621738 -4.0095549 -3.9710519 -3.958406 -3.9969749 -4.0809584 -4.1601439 -4.2222204 -4.2673297 -4.3002629 -4.3186893 -4.322947 -4.3212361][-4.1645436 -4.1147747 -4.0505972 -3.9848027 -3.928489 -3.9074697 -3.94815 -4.0446963 -4.1413608 -4.2118192 -4.2596135 -4.2929749 -4.3135238 -4.3199916 -4.3194337][-4.1490788 -4.0998607 -4.0364475 -3.9686313 -3.9093864 -3.8950121 -3.9417894 -4.0402741 -4.1376929 -4.2068119 -4.2540073 -4.2885003 -4.3114295 -4.31963 -4.3202376][-4.1562066 -4.1061783 -4.0491538 -3.9875107 -3.9354618 -3.9313951 -3.9848466 -4.0747976 -4.1608629 -4.2227 -4.2642961 -4.2953792 -4.3151178 -4.3208561 -4.3218908][-4.1940231 -4.1472378 -4.0998144 -4.0489225 -4.0073819 -4.0089412 -4.0600309 -4.1355934 -4.2050686 -4.2560029 -4.2907162 -4.3169913 -4.3288913 -4.3296213 -4.3280172][-4.2377639 -4.1974339 -4.16139 -4.1248565 -4.0961466 -4.0982656 -4.1400185 -4.200386 -4.2552781 -4.2918062 -4.3178287 -4.3391976 -4.3446045 -4.3393731 -4.335206][-4.2734208 -4.2410955 -4.2167635 -4.1927767 -4.1763005 -4.1792874 -4.2100906 -4.2559795 -4.2978115 -4.3249636 -4.3448567 -4.3613529 -4.3603973 -4.3497586 -4.3435225][-4.2977552 -4.2753134 -4.2616229 -4.2493238 -4.2413182 -4.2438765 -4.2637658 -4.2956095 -4.3262463 -4.3472948 -4.3631477 -4.3730435 -4.3676896 -4.3552804 -4.3491282][-4.3077836 -4.2961841 -4.2910438 -4.28752 -4.2835364 -4.2838774 -4.2945309 -4.3148646 -4.3367224 -4.3521123 -4.3631239 -4.3687606 -4.3635311 -4.3546734 -4.3514585]]...]
INFO - root - 2017-12-07 23:59:53.098662: step 54610, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 73h:52m:22s remains)
INFO - root - 2017-12-08 00:00:02.727475: step 54620, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.890 sec/batch; 68h:41m:26s remains)
INFO - root - 2017-12-08 00:00:12.312805: step 54630, loss = 2.10, batch loss = 2.05 (8.9 examples/sec; 0.901 sec/batch; 69h:33m:50s remains)
INFO - root - 2017-12-08 00:00:21.976761: step 54640, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.980 sec/batch; 75h:39m:55s remains)
INFO - root - 2017-12-08 00:00:31.590210: step 54650, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 73h:10m:17s remains)
INFO - root - 2017-12-08 00:00:41.190818: step 54660, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 70h:16m:43s remains)
INFO - root - 2017-12-08 00:00:50.959141: step 54670, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 74h:35m:27s remains)
INFO - root - 2017-12-08 00:01:00.783164: step 54680, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.013 sec/batch; 78h:08m:24s remains)
INFO - root - 2017-12-08 00:01:10.524991: step 54690, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 74h:54m:24s remains)
INFO - root - 2017-12-08 00:01:19.894474: step 54700, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 77h:26m:35s remains)
2017-12-08 00:01:20.964262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2798266 -4.2574167 -4.2328153 -4.209 -4.1911736 -4.1794252 -4.1842313 -4.1991019 -4.2150221 -4.2216988 -4.2285419 -4.2350583 -4.2337332 -4.2345853 -4.2434111][-4.2741871 -4.2499051 -4.2230935 -4.1932068 -4.1708975 -4.1558928 -4.1582308 -4.1734805 -4.1957884 -4.2112389 -4.2219162 -4.2270446 -4.22307 -4.2206016 -4.2281737][-4.2717161 -4.2492409 -4.2234554 -4.1908937 -4.1644588 -4.143796 -4.1403556 -4.1530218 -4.1782775 -4.1991754 -4.2118182 -4.2190309 -4.2195954 -4.22159 -4.229301][-4.2710242 -4.2535086 -4.2357593 -4.2099261 -4.1848154 -4.1586027 -4.1410985 -4.1410384 -4.1605225 -4.1787329 -4.1895857 -4.2021418 -4.2130184 -4.2257419 -4.2376585][-4.2686424 -4.2561054 -4.2492461 -4.2353487 -4.2172241 -4.1872091 -4.1525574 -4.1283474 -4.1317039 -4.1426492 -4.1534204 -4.1712079 -4.1929827 -4.2174544 -4.2383289][-4.2624707 -4.250185 -4.2478161 -4.241787 -4.22784 -4.1991749 -4.1621966 -4.1252909 -4.1134343 -4.1135216 -4.1203165 -4.1385441 -4.1656566 -4.20006 -4.2314525][-4.2481513 -4.22848 -4.2228165 -4.21672 -4.2006941 -4.1736417 -4.144187 -4.1132426 -4.0993681 -4.0926452 -4.0910373 -4.1058693 -4.1366034 -4.1786008 -4.2170177][-4.227704 -4.1996422 -4.1886363 -4.1768694 -4.1563807 -4.1325483 -4.11837 -4.1040459 -4.1003628 -4.0946226 -4.0893383 -4.0997028 -4.1272364 -4.1694217 -4.2068849][-4.2203298 -4.1917548 -4.1762152 -4.1551151 -4.1293283 -4.111867 -4.1130104 -4.1145506 -4.1221395 -4.1248775 -4.1244278 -4.1336083 -4.1511679 -4.1817155 -4.2097936][-4.2326365 -4.2099819 -4.1931858 -4.1655383 -4.13752 -4.1234503 -4.1305785 -4.141603 -4.1570477 -4.1684861 -4.1740947 -4.1815939 -4.1893644 -4.2067738 -4.2233143][-4.2616234 -4.2466512 -4.2310967 -4.2064948 -4.1860995 -4.1758766 -4.1760969 -4.1827836 -4.1960359 -4.2093406 -4.2182169 -4.2243195 -4.2276874 -4.2380033 -4.246964][-4.28856 -4.2772579 -4.2631559 -4.2458258 -4.2360654 -4.2276759 -4.219698 -4.2205982 -4.2300982 -4.2405696 -4.2488031 -4.2551928 -4.2586584 -4.2645025 -4.2697711][-4.3066192 -4.2967916 -4.2847748 -4.2737012 -4.2698288 -4.263165 -4.252687 -4.2517352 -4.2590961 -4.2670183 -4.2734194 -4.2783804 -4.2816625 -4.2830572 -4.2865171][-4.3167052 -4.3084817 -4.2986627 -4.2906017 -4.2872705 -4.2823596 -4.2758389 -4.2759905 -4.2811742 -4.2861056 -4.290401 -4.2940702 -4.2962432 -4.2959852 -4.298202][-4.3248811 -4.32036 -4.3152 -4.3103724 -4.307013 -4.3037424 -4.300714 -4.3009958 -4.3031039 -4.3053989 -4.3077779 -4.3098083 -4.3095927 -4.3086104 -4.3105083]]...]
INFO - root - 2017-12-08 00:01:30.475660: step 54710, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 75h:08m:47s remains)
INFO - root - 2017-12-08 00:01:40.296917: step 54720, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 74h:36m:58s remains)
INFO - root - 2017-12-08 00:01:49.994432: step 54730, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 75h:32m:06s remains)
INFO - root - 2017-12-08 00:01:59.586625: step 54740, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 71h:40m:54s remains)
INFO - root - 2017-12-08 00:02:09.380426: step 54750, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.008 sec/batch; 77h:45m:52s remains)
INFO - root - 2017-12-08 00:02:18.883612: step 54760, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 75h:33m:10s remains)
INFO - root - 2017-12-08 00:02:28.442432: step 54770, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 71h:25m:50s remains)
INFO - root - 2017-12-08 00:02:38.232704: step 54780, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 76h:35m:29s remains)
INFO - root - 2017-12-08 00:02:47.892432: step 54790, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.004 sec/batch; 77h:26m:26s remains)
INFO - root - 2017-12-08 00:02:57.617596: step 54800, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 73h:35m:30s remains)
2017-12-08 00:02:58.645345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2260423 -4.2197218 -4.2259459 -4.2395449 -4.2429628 -4.2349062 -4.2271481 -4.2304263 -4.2418537 -4.2462258 -4.24904 -4.2492547 -4.242383 -4.2310586 -4.2233772][-4.2186718 -4.2151375 -4.2248096 -4.2371264 -4.2303777 -4.2134719 -4.20093 -4.2031312 -4.2161622 -4.2237334 -4.226366 -4.2242723 -4.21253 -4.1948848 -4.1810727][-4.2149892 -4.2207985 -4.2357635 -4.2430344 -4.224771 -4.2007809 -4.1881685 -4.192852 -4.2103343 -4.2191768 -4.2197704 -4.2141342 -4.1973028 -4.1752119 -4.1563487][-4.2108741 -4.2268786 -4.2492523 -4.2551241 -4.2316918 -4.2043953 -4.194088 -4.1982274 -4.2157841 -4.2271495 -4.2262115 -4.2143021 -4.1966767 -4.1801152 -4.1689129][-4.1763892 -4.2041173 -4.2337952 -4.243361 -4.2221003 -4.1977806 -4.191751 -4.1958303 -4.212225 -4.2274151 -4.2335768 -4.2259183 -4.2167358 -4.2133765 -4.2140279][-4.1078749 -4.1480789 -4.1863627 -4.1998711 -4.1846719 -4.170126 -4.1712785 -4.178134 -4.194942 -4.2179213 -4.2361288 -4.2396851 -4.2398829 -4.2461886 -4.2549715][-4.0449357 -4.0891018 -4.13004 -4.1428704 -4.1323333 -4.1304731 -4.1447549 -4.1611142 -4.1803665 -4.2067256 -4.2332339 -4.245811 -4.253233 -4.2631803 -4.2730951][-4.0573964 -4.0832143 -4.110189 -4.1112413 -4.0967107 -4.1023426 -4.1262465 -4.1499863 -4.1738453 -4.2024 -4.2282624 -4.2400465 -4.2487326 -4.2621942 -4.2712421][-4.1171169 -4.1271524 -4.1384997 -4.131391 -4.11595 -4.1194959 -4.1408796 -4.1599917 -4.1792049 -4.1967793 -4.2073388 -4.2074928 -4.2153082 -4.2361321 -4.2481952][-4.1688237 -4.1734681 -4.1784363 -4.1718059 -4.1642547 -4.165544 -4.1778378 -4.1847486 -4.192349 -4.1941795 -4.1804261 -4.1622787 -4.1642046 -4.1882644 -4.1999478][-4.2130003 -4.2166352 -4.2187738 -4.2117767 -4.20653 -4.2046022 -4.2095232 -4.2090611 -4.2079883 -4.1966014 -4.1623707 -4.1249113 -4.1167631 -4.1351132 -4.142375][-4.254909 -4.2585011 -4.257525 -4.2468834 -4.2374659 -4.2304997 -4.2297988 -4.2269416 -4.2201309 -4.2003684 -4.1550741 -4.1050968 -4.0860219 -4.091784 -4.0931635][-4.2816315 -4.284914 -4.2823009 -4.269876 -4.2587776 -4.2496028 -4.2452884 -4.2414131 -4.232461 -4.2118425 -4.1691942 -4.1208086 -4.0992079 -4.097683 -4.0968108][-4.2930088 -4.2948136 -4.2890148 -4.2779226 -4.2680397 -4.2571893 -4.25159 -4.2497554 -4.2455621 -4.234303 -4.2043772 -4.1697826 -4.1555986 -4.1546125 -4.1570826][-4.302701 -4.3035107 -4.2962847 -4.2871761 -4.27691 -4.2619424 -4.2520676 -4.2535348 -4.2563829 -4.2561159 -4.240757 -4.2231984 -4.2160845 -4.2139635 -4.2172875]]...]
INFO - root - 2017-12-08 00:03:08.064706: step 54810, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 73h:38m:14s remains)
INFO - root - 2017-12-08 00:03:17.633111: step 54820, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 76h:08m:55s remains)
INFO - root - 2017-12-08 00:03:27.262723: step 54830, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 77h:30m:37s remains)
INFO - root - 2017-12-08 00:03:37.027716: step 54840, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 73h:06m:32s remains)
INFO - root - 2017-12-08 00:03:46.610394: step 54850, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.965 sec/batch; 74h:23m:33s remains)
INFO - root - 2017-12-08 00:03:56.210746: step 54860, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.039 sec/batch; 80h:08m:58s remains)
INFO - root - 2017-12-08 00:04:05.789616: step 54870, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 77h:43m:59s remains)
INFO - root - 2017-12-08 00:04:15.558830: step 54880, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 75h:57m:04s remains)
INFO - root - 2017-12-08 00:04:25.095328: step 54890, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 68h:43m:15s remains)
INFO - root - 2017-12-08 00:04:34.822597: step 54900, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 76h:20m:55s remains)
2017-12-08 00:04:35.743033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2028604 -4.2066083 -4.2074661 -4.2190537 -4.2430806 -4.2580318 -4.251657 -4.2222395 -4.1958628 -4.1773515 -4.1579323 -4.1520867 -4.1443815 -4.1311865 -4.1313295][-4.21123 -4.2151527 -4.209898 -4.215054 -4.2327332 -4.23507 -4.2185054 -4.1837988 -4.1534467 -4.13399 -4.1164722 -4.1221871 -4.1324553 -4.1280627 -4.1286197][-4.224546 -4.2225747 -4.206809 -4.2021718 -4.2115765 -4.2046213 -4.1861377 -4.1578407 -4.1341653 -4.121892 -4.1137042 -4.1281281 -4.1496763 -4.153368 -4.148344][-4.2182584 -4.2150764 -4.1973271 -4.1857042 -4.1883774 -4.1776032 -4.1564603 -4.1386995 -4.1265693 -4.1189632 -4.1140609 -4.1266513 -4.1497602 -4.1564689 -4.1469297][-4.1931176 -4.1956973 -4.1821413 -4.165278 -4.155087 -4.1297908 -4.0993228 -4.0919557 -4.0964942 -4.0935678 -4.0879393 -4.0958004 -4.1128345 -4.116334 -4.1054177][-4.1592331 -4.1661177 -4.1544352 -4.1288614 -4.0965557 -4.0392761 -3.982022 -3.9872611 -4.0247536 -4.0421906 -4.0460644 -4.0600038 -4.0734782 -4.0695705 -4.0554743][-4.1424603 -4.1437693 -4.1287408 -4.0957594 -4.0425811 -3.9522314 -3.8696463 -3.8991895 -3.97976 -4.0282693 -4.0511408 -4.0784616 -4.093112 -4.0832067 -4.0637054][-4.1437054 -4.1448421 -4.1339736 -4.10847 -4.0648456 -3.9892864 -3.9293079 -3.9708085 -4.0477457 -4.0964231 -4.1207309 -4.1448145 -4.1528645 -4.1388035 -4.1186771][-4.1536283 -4.1577983 -4.1594281 -4.1554856 -4.1403761 -4.1023822 -4.0765471 -4.1102848 -4.15646 -4.1827497 -4.1959043 -4.2065434 -4.2067742 -4.1908917 -4.1757307][-4.1595731 -4.1667132 -4.1765709 -4.1931109 -4.2022014 -4.1937385 -4.1880512 -4.2121854 -4.2360606 -4.2466269 -4.250864 -4.251255 -4.2461748 -4.2342334 -4.2259364][-4.1450367 -4.1558657 -4.1721039 -4.2013912 -4.2303095 -4.2439251 -4.2499156 -4.2695727 -4.2853088 -4.2897482 -4.2884045 -4.2832122 -4.2763605 -4.2692442 -4.2649527][-4.1355391 -4.1540465 -4.1792789 -4.2170539 -4.2514157 -4.271554 -4.2817507 -4.2984462 -4.3101797 -4.312211 -4.3103065 -4.3047442 -4.2980304 -4.2933197 -4.2916694][-4.1575689 -4.18475 -4.2171831 -4.2524505 -4.2778583 -4.2915549 -4.2987313 -4.3116307 -4.3224974 -4.326129 -4.32597 -4.3216467 -4.315227 -4.310091 -4.3069372][-4.213562 -4.2395377 -4.2689605 -4.2940764 -4.3040571 -4.3054605 -4.306601 -4.3156042 -4.3260369 -4.3317075 -4.3326244 -4.3280482 -4.3219867 -4.3173742 -4.3124671][-4.2705979 -4.2930527 -4.3135991 -4.3256106 -4.3207636 -4.3117976 -4.3098249 -4.3159585 -4.3251514 -4.3315291 -4.3344359 -4.332418 -4.3283563 -4.3252015 -4.3200374]]...]
INFO - root - 2017-12-08 00:04:45.280312: step 54910, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 76h:12m:33s remains)
INFO - root - 2017-12-08 00:04:54.870548: step 54920, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 74h:28m:59s remains)
INFO - root - 2017-12-08 00:05:04.476398: step 54930, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.981 sec/batch; 75h:39m:33s remains)
INFO - root - 2017-12-08 00:05:14.189987: step 54940, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.001 sec/batch; 77h:09m:03s remains)
INFO - root - 2017-12-08 00:05:23.821353: step 54950, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 72h:00m:54s remains)
INFO - root - 2017-12-08 00:05:33.365692: step 54960, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 72h:27m:46s remains)
INFO - root - 2017-12-08 00:05:43.204238: step 54970, loss = 2.06, batch loss = 2.01 (7.9 examples/sec; 1.019 sec/batch; 78h:31m:43s remains)
INFO - root - 2017-12-08 00:05:52.890055: step 54980, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 76h:52m:50s remains)
INFO - root - 2017-12-08 00:06:02.455293: step 54990, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 69h:55m:19s remains)
INFO - root - 2017-12-08 00:06:12.075729: step 55000, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 68h:33m:04s remains)
2017-12-08 00:06:13.037822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3146124 -4.3225846 -4.3201647 -4.2943406 -4.2547297 -4.2175174 -4.1910892 -4.185967 -4.192349 -4.2103491 -4.2310824 -4.2379918 -4.2399035 -4.2536006 -4.2728353][-4.3134241 -4.3202662 -4.3169203 -4.2898374 -4.2445874 -4.1983695 -4.1630154 -4.1554275 -4.1657181 -4.1885495 -4.2128787 -4.2179642 -4.2122245 -4.2187257 -4.24121][-4.3091574 -4.31328 -4.3091812 -4.2834139 -4.2384157 -4.1893563 -4.1497 -4.1389508 -4.1488819 -4.1760235 -4.2035975 -4.2086 -4.198071 -4.1983972 -4.2183943][-4.3057103 -4.30659 -4.3003712 -4.2772455 -4.2380366 -4.1918874 -4.1534948 -4.1366925 -4.1398687 -4.1653862 -4.1913781 -4.1948128 -4.1812296 -4.1788378 -4.193357][-4.301343 -4.3041563 -4.2980905 -4.27685 -4.2426705 -4.202353 -4.1650314 -4.1400237 -4.1309133 -4.1503716 -4.1744981 -4.179657 -4.1678371 -4.1640806 -4.1704144][-4.2954359 -4.3043118 -4.3019238 -4.2821074 -4.2504158 -4.211894 -4.1712031 -4.1364794 -4.113318 -4.1246529 -4.1541667 -4.1672177 -4.1617684 -4.1599469 -4.1640048][-4.2902546 -4.3056784 -4.30552 -4.2863197 -4.2534294 -4.2127495 -4.16343 -4.1108103 -4.0661511 -4.0677576 -4.1168284 -4.1544676 -4.1646819 -4.1671705 -4.1692247][-4.28266 -4.3012819 -4.3010616 -4.2832584 -4.252614 -4.2121592 -4.1552057 -4.0820284 -4.0091166 -3.9939163 -4.0648623 -4.1384344 -4.1740155 -4.1859751 -4.1880517][-4.2767377 -4.2960372 -4.2950277 -4.2771382 -4.2483816 -4.211875 -4.1581545 -4.0813289 -3.9974754 -3.9642816 -4.0354109 -4.1295452 -4.1881151 -4.2134218 -4.218358][-4.2770267 -4.293613 -4.2904406 -4.2727919 -4.24673 -4.2176466 -4.1785345 -4.119998 -4.0545969 -4.0195932 -4.0655408 -4.1435165 -4.2039208 -4.2348518 -4.2423377][-4.281949 -4.2926736 -4.2863755 -4.2673364 -4.241178 -4.216207 -4.191679 -4.154923 -4.1120944 -4.0808511 -4.1010747 -4.1531463 -4.2067704 -4.24241 -4.2575755][-4.2825465 -4.2889466 -4.2812629 -4.2614045 -4.2318969 -4.206727 -4.1919174 -4.1719866 -4.145834 -4.1188269 -4.1233053 -4.15687 -4.2031579 -4.24434 -4.2669067][-4.2742586 -4.2780356 -4.2743225 -4.2609167 -4.2346163 -4.2057447 -4.1884165 -4.1736989 -4.1577988 -4.1398816 -4.1422963 -4.1695557 -4.2105989 -4.2515354 -4.2772164][-4.2560883 -4.2574911 -4.2568693 -4.2521024 -4.2349257 -4.2094851 -4.1916308 -4.180325 -4.1747193 -4.1687012 -4.17467 -4.2008786 -4.237473 -4.272861 -4.2966514][-4.2385969 -4.23417 -4.2331338 -4.23497 -4.2296681 -4.2143097 -4.2021794 -4.19667 -4.2011166 -4.2071524 -4.2181029 -4.242434 -4.2715931 -4.2990375 -4.3187151]]...]
INFO - root - 2017-12-08 00:06:22.718736: step 55010, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 76h:15m:46s remains)
INFO - root - 2017-12-08 00:06:32.347804: step 55020, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 75h:33m:15s remains)
INFO - root - 2017-12-08 00:06:41.957870: step 55030, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 72h:17m:59s remains)
INFO - root - 2017-12-08 00:06:51.647860: step 55040, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.018 sec/batch; 78h:25m:49s remains)
INFO - root - 2017-12-08 00:07:01.367758: step 55050, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 74h:26m:21s remains)
INFO - root - 2017-12-08 00:07:10.987568: step 55060, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 75h:52m:32s remains)
INFO - root - 2017-12-08 00:07:20.716221: step 55070, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 74h:49m:00s remains)
INFO - root - 2017-12-08 00:07:30.432247: step 55080, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 75h:30m:54s remains)
INFO - root - 2017-12-08 00:07:40.106580: step 55090, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 71h:54m:07s remains)
INFO - root - 2017-12-08 00:07:49.824370: step 55100, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.986 sec/batch; 75h:57m:30s remains)
2017-12-08 00:07:50.839976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3640261 -4.3578477 -4.3504267 -4.343008 -4.3293929 -4.3107581 -4.2946858 -4.27643 -4.263423 -4.2570887 -4.2533741 -4.26136 -4.2821884 -4.3066745 -4.3278346][-4.3639669 -4.3635893 -4.3639269 -4.3641248 -4.3614902 -4.3548212 -4.3469248 -4.3352509 -4.3240328 -4.313652 -4.3046532 -4.30368 -4.3110571 -4.3231359 -4.3347268][-4.3511796 -4.3544574 -4.3561773 -4.35641 -4.3560834 -4.3552389 -4.3525882 -4.3467493 -4.3396621 -4.3294382 -4.3192291 -4.3139863 -4.3145027 -4.3219509 -4.3309822][-4.33536 -4.3379006 -4.3361311 -4.3297882 -4.322526 -4.3181105 -4.316082 -4.3155704 -4.317183 -4.3146467 -4.3102069 -4.3060894 -4.3050213 -4.3116212 -4.3211079][-4.3248577 -4.3237624 -4.3165359 -4.3014975 -4.2838783 -4.2708006 -4.2646775 -4.2664213 -4.2738352 -4.2795916 -4.2841997 -4.2854314 -4.2865119 -4.2941556 -4.3047175][-4.3187842 -4.3113775 -4.2982178 -4.2747149 -4.2466903 -4.2241745 -4.2101264 -4.2089586 -4.2170348 -4.2295218 -4.2446337 -4.2547674 -4.2612481 -4.2716851 -4.283535][-4.314487 -4.3043079 -4.2864141 -4.2546329 -4.2132621 -4.17417 -4.1422195 -4.127295 -4.1328092 -4.1563692 -4.1891646 -4.2159171 -4.2332683 -4.2499776 -4.26408][-4.3049426 -4.2943249 -4.2734437 -4.235909 -4.1838984 -4.1282225 -4.0723863 -4.0337067 -4.0345831 -4.0728025 -4.1267476 -4.1724367 -4.2040739 -4.2282114 -4.2446289][-4.28634 -4.2717695 -4.2473474 -4.209435 -4.15828 -4.1000862 -4.0308208 -3.9724758 -3.9673679 -4.0120053 -4.0770922 -4.1355991 -4.1782522 -4.2072868 -4.2237358][-4.2676368 -4.2480841 -4.2233539 -4.190609 -4.14977 -4.1033864 -4.0456142 -3.9942818 -3.9871588 -4.0206523 -4.072947 -4.12322 -4.1619077 -4.1891775 -4.2025743][-4.263938 -4.2419581 -4.219862 -4.1938324 -4.1650314 -4.1349173 -4.10012 -4.0698109 -4.0666652 -4.085597 -4.1152949 -4.1456113 -4.1706381 -4.1899447 -4.1965709][-4.2830262 -4.2623682 -4.244647 -4.22212 -4.1992397 -4.1814537 -4.1653919 -4.15326 -4.1544771 -4.1639996 -4.1789889 -4.1938629 -4.2053967 -4.2149611 -4.2156296][-4.3113942 -4.2960339 -4.2814631 -4.25958 -4.2410026 -4.2331867 -4.2302 -4.2294602 -4.2332034 -4.2386293 -4.2469969 -4.2549782 -4.2605743 -4.2631731 -4.2569504][-4.3337584 -4.3250818 -4.3129606 -4.2937312 -4.27975 -4.2764668 -4.2799215 -4.2857213 -4.2917604 -4.2963438 -4.3033504 -4.3096824 -4.3120027 -4.3078623 -4.294837][-4.3419127 -4.3396788 -4.3308759 -4.3154306 -4.3044591 -4.3027167 -4.308094 -4.3155375 -4.3208179 -4.3242092 -4.3303461 -4.3359141 -4.337091 -4.3332257 -4.3211117]]...]
INFO - root - 2017-12-08 00:08:00.604101: step 55110, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.015 sec/batch; 78h:14m:34s remains)
INFO - root - 2017-12-08 00:08:10.333850: step 55120, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.947 sec/batch; 72h:55m:55s remains)
INFO - root - 2017-12-08 00:08:19.985830: step 55130, loss = 2.11, batch loss = 2.05 (8.8 examples/sec; 0.913 sec/batch; 70h:21m:49s remains)
INFO - root - 2017-12-08 00:08:29.694263: step 55140, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.030 sec/batch; 79h:19m:17s remains)
INFO - root - 2017-12-08 00:08:39.446512: step 55150, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 74h:02m:28s remains)
INFO - root - 2017-12-08 00:08:49.031053: step 55160, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 75h:02m:30s remains)
INFO - root - 2017-12-08 00:08:58.573158: step 55170, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 75h:30m:30s remains)
INFO - root - 2017-12-08 00:09:08.237972: step 55180, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 74h:12m:06s remains)
INFO - root - 2017-12-08 00:09:17.874595: step 55190, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 73h:22m:59s remains)
INFO - root - 2017-12-08 00:09:27.451765: step 55200, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 74h:42m:01s remains)
2017-12-08 00:09:28.532106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.234046 -4.2350264 -4.2367711 -4.2363615 -4.2361145 -4.2364926 -4.2358408 -4.2370334 -4.2394915 -4.241837 -4.2442913 -4.2490234 -4.2568169 -4.27009 -4.2873631][-4.2091074 -4.2157989 -4.2243929 -4.2315259 -4.2389126 -4.2443442 -4.2456236 -4.2464323 -4.2484784 -4.2507868 -4.25145 -4.2515254 -4.2524576 -4.2586141 -4.2705588][-4.1860003 -4.19698 -4.2110529 -4.2254672 -4.2396073 -4.2497697 -4.2532558 -4.2540746 -4.2570109 -4.2609463 -4.2620549 -4.2604027 -4.2558627 -4.2523932 -4.2525377][-4.1839571 -4.1946917 -4.2074666 -4.2211437 -4.2339616 -4.2423873 -4.2454629 -4.2473378 -4.2553434 -4.2667475 -4.2749519 -4.2771096 -4.27169 -4.2605076 -4.2467818][-4.1967182 -4.1995811 -4.2056918 -4.2125778 -4.2178073 -4.2180924 -4.2148418 -4.215095 -4.228622 -4.2488666 -4.2664018 -4.2760725 -4.2753139 -4.2629766 -4.2419105][-4.1940002 -4.1870031 -4.1822085 -4.1765227 -4.1679831 -4.1531515 -4.1355972 -4.1297617 -4.1499057 -4.186132 -4.22036 -4.2430792 -4.251688 -4.2423029 -4.2180238][-4.1725626 -4.1571813 -4.1405745 -4.121078 -4.0956717 -4.0580573 -4.0161362 -3.9951031 -4.0193567 -4.0758872 -4.1346507 -4.1778326 -4.2010088 -4.1978164 -4.1730161][-4.1606469 -4.1433139 -4.1253314 -4.1053576 -4.078372 -4.0324512 -3.9748552 -3.9345765 -3.9441369 -3.9992321 -4.0662594 -4.1225019 -4.1593843 -4.1680841 -4.1515708][-4.1786089 -4.1680818 -4.1584487 -4.1493721 -4.1353683 -4.1053772 -4.0638967 -4.0317345 -4.0312371 -4.0626621 -4.1060934 -4.1470532 -4.1783304 -4.1913056 -4.1862669][-4.2101617 -4.203125 -4.1996031 -4.1984606 -4.1951723 -4.18281 -4.16399 -4.1498394 -4.1515541 -4.1699905 -4.1937661 -4.2155533 -4.2317867 -4.2379646 -4.2341232][-4.2401767 -4.233459 -4.2305975 -4.231451 -4.2329535 -4.2305059 -4.2250075 -4.221662 -4.225728 -4.23763 -4.2498589 -4.2590866 -4.2636251 -4.2618465 -4.2544718][-4.2564392 -4.2507992 -4.2485147 -4.2503867 -4.25411 -4.2562923 -4.257863 -4.259933 -4.2638359 -4.2701054 -4.2750063 -4.2759123 -4.2711129 -4.2620783 -4.2515125][-4.2471809 -4.243721 -4.2415242 -4.24299 -4.24665 -4.2505035 -4.2559586 -4.2623549 -4.2679739 -4.2712984 -4.2703395 -4.2644715 -4.2530522 -4.2390213 -4.2281704][-4.2271476 -4.2253332 -4.2237382 -4.2240076 -4.2256846 -4.2278042 -4.2329774 -4.239604 -4.2453074 -4.2476687 -4.2445207 -4.2352862 -4.2203417 -4.2029066 -4.1924314][-4.2258124 -4.2226405 -4.21928 -4.2170553 -4.2155366 -4.2148194 -4.2175908 -4.2224016 -4.2269573 -4.2284946 -4.2246404 -4.2148352 -4.1997104 -4.1809373 -4.1704845]]...]
INFO - root - 2017-12-08 00:09:37.982973: step 55210, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 72h:17m:41s remains)
INFO - root - 2017-12-08 00:09:47.633241: step 55220, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 71h:38m:47s remains)
INFO - root - 2017-12-08 00:09:57.541671: step 55230, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 76h:36m:59s remains)
INFO - root - 2017-12-08 00:10:07.254366: step 55240, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 73h:34m:30s remains)
INFO - root - 2017-12-08 00:10:16.945011: step 55250, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 72h:53m:37s remains)
INFO - root - 2017-12-08 00:10:26.586720: step 55260, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 73h:08m:30s remains)
INFO - root - 2017-12-08 00:10:36.254975: step 55270, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 76h:55m:57s remains)
INFO - root - 2017-12-08 00:10:45.981501: step 55280, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 75h:31m:55s remains)
INFO - root - 2017-12-08 00:10:55.792319: step 55290, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.029 sec/batch; 79h:15m:01s remains)
INFO - root - 2017-12-08 00:11:05.362625: step 55300, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 71h:51m:10s remains)
2017-12-08 00:11:06.486149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462349 -4.2571697 -4.2701874 -4.2718048 -4.2567377 -4.2380071 -4.2268662 -4.2297583 -4.2407146 -4.2494955 -4.244204 -4.2275543 -4.2189307 -4.2319884 -4.2608709][-4.2354608 -4.2476983 -4.2602644 -4.25967 -4.2387533 -4.2133789 -4.1996384 -4.2025032 -4.2146964 -4.2254462 -4.2196655 -4.2017717 -4.1932492 -4.2092915 -4.2457623][-4.2180905 -4.2325974 -4.2447195 -4.2419367 -4.2168374 -4.1880112 -4.1770735 -4.1829238 -4.1971679 -4.208149 -4.1993766 -4.1789041 -4.1688261 -4.1848526 -4.2264681][-4.1950812 -4.2143383 -4.2279081 -4.2225771 -4.194191 -4.1639657 -4.15472 -4.164403 -4.1834617 -4.1952791 -4.1841011 -4.1602454 -4.1488585 -4.1645317 -4.2091832][-4.1703053 -4.1939974 -4.2096767 -4.2021627 -4.1722059 -4.1387439 -4.1282721 -4.1422086 -4.1708946 -4.1865783 -4.1749849 -4.1497307 -4.13604 -4.1508341 -4.1974783][-4.1423926 -4.1679912 -4.1849751 -4.1788092 -4.1493368 -4.1131158 -4.1019049 -4.1197524 -4.1577783 -4.1791606 -4.1694784 -4.1436191 -4.1268659 -4.140326 -4.1880889][-4.1192875 -4.1445708 -4.1636057 -4.1613164 -4.1315775 -4.0905228 -4.0753713 -4.097105 -4.1437511 -4.1710567 -4.1660089 -4.1405153 -4.1218681 -4.1356554 -4.1838508][-4.1131649 -4.1380167 -4.1573629 -4.1561217 -4.1212606 -4.0677185 -4.0407224 -4.0640397 -4.119832 -4.1559463 -4.1600175 -4.1387105 -4.1197062 -4.135283 -4.1852608][-4.1181645 -4.145287 -4.16308 -4.1584787 -4.1162896 -4.0510511 -4.0124054 -4.03383 -4.09603 -4.1410565 -4.1533856 -4.1383762 -4.1211052 -4.1387653 -4.1906271][-4.1178727 -4.1469107 -4.161736 -4.15308 -4.1106892 -4.0470042 -4.0089226 -4.0283084 -4.0883169 -4.1360545 -4.1518908 -4.1402788 -4.1250248 -4.1442876 -4.1964326][-4.111928 -4.13878 -4.1509466 -4.1416554 -4.1088624 -4.0619411 -4.0349593 -4.0505495 -4.1005545 -4.1423993 -4.1532545 -4.1406779 -4.1266069 -4.1479287 -4.199862][-4.1088886 -4.1287227 -4.138782 -4.1328721 -4.113914 -4.086575 -4.0720248 -4.0855484 -4.1242003 -4.1555467 -4.1583247 -4.1424413 -4.1303554 -4.1531062 -4.2031317][-4.1069517 -4.1206279 -4.1294308 -4.1274862 -4.1199784 -4.1064854 -4.1014509 -4.1139832 -4.1428103 -4.1657562 -4.16168 -4.1438308 -4.1348953 -4.1578312 -4.2054553][-4.1030912 -4.1153836 -4.1246057 -4.1269031 -4.1274471 -4.1245685 -4.1255 -4.1376271 -4.1592665 -4.1738787 -4.1639843 -4.1441822 -4.1370454 -4.1592851 -4.2044811][-4.1000547 -4.1110873 -4.1211734 -4.1261525 -4.1332183 -4.1388831 -4.1439672 -4.1560106 -4.1725755 -4.1798487 -4.164691 -4.1422477 -4.1362591 -4.1590571 -4.2018518]]...]
INFO - root - 2017-12-08 00:11:16.288310: step 55310, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 75h:10m:38s remains)
INFO - root - 2017-12-08 00:11:26.014526: step 55320, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.951 sec/batch; 73h:12m:49s remains)
INFO - root - 2017-12-08 00:11:35.625580: step 55330, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 74h:54m:51s remains)
INFO - root - 2017-12-08 00:11:45.300250: step 55340, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 73h:16m:10s remains)
INFO - root - 2017-12-08 00:11:55.127657: step 55350, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.981 sec/batch; 75h:33m:02s remains)
INFO - root - 2017-12-08 00:12:04.675487: step 55360, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.962 sec/batch; 74h:03m:32s remains)
INFO - root - 2017-12-08 00:12:14.108163: step 55370, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 0.774 sec/batch; 59h:36m:13s remains)
INFO - root - 2017-12-08 00:12:23.882184: step 55380, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 76h:30m:32s remains)
INFO - root - 2017-12-08 00:12:33.365872: step 55390, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 64h:49m:18s remains)
INFO - root - 2017-12-08 00:12:43.112072: step 55400, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 72h:55m:03s remains)
2017-12-08 00:12:44.153960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1757903 -4.1854558 -4.2116055 -4.2431355 -4.2774725 -4.304615 -4.3106 -4.2900648 -4.2493639 -4.2012329 -4.1544814 -4.1153011 -4.1296396 -4.1905327 -4.23435][-4.17911 -4.200685 -4.2354021 -4.2709618 -4.3032379 -4.32393 -4.3247628 -4.3039231 -4.2689524 -4.2300997 -4.1918936 -4.1601253 -4.1714463 -4.2195687 -4.2554526][-4.2035613 -4.2322297 -4.2668324 -4.2988138 -4.3203759 -4.326282 -4.317987 -4.2975526 -4.2715454 -4.248445 -4.2287297 -4.2170258 -4.2270837 -4.2568474 -4.2827258][-4.2234426 -4.2547731 -4.2876182 -4.310657 -4.312501 -4.2962108 -4.2716947 -4.25252 -4.2468181 -4.2507334 -4.2502947 -4.252872 -4.2595496 -4.2724319 -4.2883081][-4.2243505 -4.255599 -4.2846127 -4.2929935 -4.2705474 -4.2235613 -4.1717739 -4.1534605 -4.1873341 -4.2345014 -4.2603607 -4.2724 -4.2705793 -4.2685237 -4.2750354][-4.211719 -4.2430744 -4.2685175 -4.2603574 -4.2055535 -4.1164756 -4.0200663 -3.9956288 -4.087285 -4.1900878 -4.2502522 -4.2766395 -4.2690711 -4.2506738 -4.2477713][-4.209012 -4.2397809 -4.2597446 -4.2361035 -4.1550856 -4.0252538 -3.871902 -3.8334405 -3.98486 -4.1345963 -4.2220435 -4.2649674 -4.2602272 -4.2311993 -4.2184649][-4.2353144 -4.2613859 -4.2744665 -4.244247 -4.1636438 -4.0343008 -3.8811927 -3.8486869 -3.9908278 -4.1292496 -4.2103143 -4.2503257 -4.2445679 -4.2078304 -4.18683][-4.2779179 -4.2973442 -4.3045034 -4.27824 -4.2164674 -4.1249542 -4.0293045 -4.0125375 -4.0931029 -4.1730595 -4.21793 -4.2385273 -4.2195568 -4.1668119 -4.1369524][-4.3184261 -4.33191 -4.3356171 -4.3182187 -4.2792363 -4.2263451 -4.1777654 -4.1702971 -4.203043 -4.2276707 -4.2311444 -4.2246575 -4.1819844 -4.1063318 -4.069418][-4.33775 -4.3469286 -4.3479424 -4.3380008 -4.31962 -4.2963781 -4.2748342 -4.272378 -4.2819829 -4.2791734 -4.2594509 -4.2313824 -4.1700759 -4.075141 -4.0384436][-4.3361959 -4.3422031 -4.3417978 -4.3400249 -4.3375959 -4.3331223 -4.3270726 -4.328908 -4.330163 -4.315681 -4.2864103 -4.2498283 -4.1881528 -4.1039252 -4.0793138][-4.3265853 -4.3282681 -4.3279881 -4.3333073 -4.3402238 -4.345109 -4.3473649 -4.3547225 -4.3545122 -4.337904 -4.30977 -4.279489 -4.2374434 -4.1836696 -4.1725869][-4.3188281 -4.3189206 -4.3191028 -4.324419 -4.33175 -4.3382549 -4.3445106 -4.3549833 -4.3554149 -4.3447332 -4.3278875 -4.3104639 -4.28917 -4.265213 -4.2629147][-4.3143597 -4.3146067 -4.3146553 -4.3170261 -4.3216133 -4.3272967 -4.3331518 -4.3425045 -4.3425245 -4.3367696 -4.33159 -4.3269439 -4.3197732 -4.3107805 -4.3109379]]...]
INFO - root - 2017-12-08 00:12:53.835083: step 55410, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.018 sec/batch; 78h:21m:11s remains)
INFO - root - 2017-12-08 00:13:03.523991: step 55420, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 74h:41m:46s remains)
INFO - root - 2017-12-08 00:13:13.094506: step 55430, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 71h:35m:47s remains)
INFO - root - 2017-12-08 00:13:22.764638: step 55440, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.020 sec/batch; 78h:31m:43s remains)
INFO - root - 2017-12-08 00:13:32.542048: step 55450, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 76h:34m:35s remains)
INFO - root - 2017-12-08 00:13:42.096616: step 55460, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 74h:38m:43s remains)
INFO - root - 2017-12-08 00:13:51.676182: step 55470, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 73h:01m:21s remains)
INFO - root - 2017-12-08 00:14:01.391820: step 55480, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 71h:07m:20s remains)
INFO - root - 2017-12-08 00:14:10.972035: step 55490, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.978 sec/batch; 75h:16m:53s remains)
INFO - root - 2017-12-08 00:14:20.707542: step 55500, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 70h:07m:00s remains)
2017-12-08 00:14:21.742677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2651443 -4.2528496 -4.250102 -4.2628355 -4.281354 -4.2963943 -4.297617 -4.2854233 -4.2779903 -4.2798691 -4.2770243 -4.2716079 -4.2677536 -4.2673979 -4.2688437][-4.2464514 -4.2379017 -4.2429686 -4.2628608 -4.2830453 -4.296207 -4.2961464 -4.28343 -4.2761145 -4.2796888 -4.2769365 -4.2697515 -4.2642603 -4.2595458 -4.2576785][-4.2215352 -4.2189856 -4.2340231 -4.2583194 -4.2806687 -4.2889953 -4.2822218 -4.270411 -4.2696214 -4.27783 -4.2740903 -4.2630529 -4.2522044 -4.2394495 -4.232522][-4.2031264 -4.1986003 -4.2146378 -4.2373319 -4.2578874 -4.25931 -4.2463136 -4.2333975 -4.2396927 -4.25694 -4.2581263 -4.2452097 -4.2293148 -4.207828 -4.1985583][-4.1969571 -4.1921115 -4.2053266 -4.2244167 -4.2355232 -4.2232332 -4.1977706 -4.1718316 -4.1822982 -4.2203088 -4.2367396 -4.2272329 -4.2083044 -4.1790338 -4.1654305][-4.1907692 -4.1957831 -4.21268 -4.2232094 -4.2054081 -4.1632895 -4.1091428 -4.0522227 -4.0659466 -4.1386795 -4.1839027 -4.1894507 -4.1793385 -4.1501746 -4.1327586][-4.1829448 -4.19654 -4.20948 -4.2030063 -4.1484509 -4.0728016 -3.9763193 -3.8712754 -3.8890629 -4.0142345 -4.0949492 -4.1233025 -4.1370726 -4.1265984 -4.1158171][-4.18428 -4.2021551 -4.2088938 -4.1841908 -4.1111808 -4.0268412 -3.9115198 -3.7737579 -3.7921145 -3.9470596 -4.0399971 -4.0754213 -4.1052122 -4.1156764 -4.1224566][-4.2207327 -4.24158 -4.2499332 -4.2285333 -4.1703563 -4.1069613 -4.0195394 -3.9150906 -3.9311476 -4.0461011 -4.10389 -4.119556 -4.1421103 -4.1584897 -4.1725979][-4.2622466 -4.2830634 -4.2979732 -4.290349 -4.2548218 -4.212954 -4.1582718 -4.0974879 -4.1131196 -4.1820807 -4.20738 -4.2080951 -4.2213192 -4.2363062 -4.2510767][-4.2868309 -4.305244 -4.3240881 -4.3282471 -4.3122964 -4.2905827 -4.2602296 -4.2291474 -4.23992 -4.2766957 -4.28286 -4.2795553 -4.2866645 -4.297359 -4.3107953][-4.2963738 -4.3094378 -4.3249869 -4.3340268 -4.3315787 -4.3255672 -4.3119235 -4.296658 -4.3026338 -4.3187609 -4.318367 -4.3154078 -4.317451 -4.3218904 -4.3303461][-4.2967238 -4.3019133 -4.3096523 -4.3152819 -4.3167839 -4.3183236 -4.313777 -4.3024769 -4.3028994 -4.3116946 -4.3130426 -4.3131547 -4.3149953 -4.3171849 -4.3215256][-4.2947278 -4.2976351 -4.3012924 -4.303319 -4.3037171 -4.3069491 -4.3064547 -4.2974696 -4.2926927 -4.2964787 -4.3004112 -4.3029976 -4.3036766 -4.3048253 -4.3066583][-4.2769828 -4.2799644 -4.2812123 -4.2788348 -4.275866 -4.278451 -4.2782688 -4.2705312 -4.2649837 -4.2657809 -4.2688985 -4.2720909 -4.273931 -4.27812 -4.2820568]]...]
INFO - root - 2017-12-08 00:14:31.573017: step 55510, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 77h:12m:32s remains)
INFO - root - 2017-12-08 00:14:41.229036: step 55520, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 72h:40m:35s remains)
INFO - root - 2017-12-08 00:14:50.806545: step 55530, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.952 sec/batch; 73h:15m:15s remains)
INFO - root - 2017-12-08 00:15:00.447920: step 55540, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 76h:50m:33s remains)
INFO - root - 2017-12-08 00:15:10.137330: step 55550, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 73h:00m:56s remains)
INFO - root - 2017-12-08 00:15:19.750781: step 55560, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 75h:08m:14s remains)
INFO - root - 2017-12-08 00:15:29.354319: step 55570, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 75h:07m:46s remains)
INFO - root - 2017-12-08 00:15:39.005035: step 55580, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 69h:40m:02s remains)
INFO - root - 2017-12-08 00:15:48.581096: step 55590, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.012 sec/batch; 77h:50m:09s remains)
INFO - root - 2017-12-08 00:15:58.219382: step 55600, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.040 sec/batch; 80h:00m:12s remains)
2017-12-08 00:15:59.205800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2333527 -4.2433629 -4.2650733 -4.2804742 -4.2819114 -4.273818 -4.2639766 -4.2590847 -4.2588181 -4.2535324 -4.2524652 -4.2593303 -4.2615523 -4.2592015 -4.2624755][-4.2371354 -4.2492523 -4.2720742 -4.2881384 -4.2879124 -4.2760386 -4.2662706 -4.2611632 -4.263978 -4.2635241 -4.2676291 -4.2804565 -4.2834206 -4.2757821 -4.2693243][-4.2257047 -4.2327247 -4.2538133 -4.2770853 -4.2817283 -4.2700968 -4.2602849 -4.2581964 -4.2683039 -4.2749724 -4.2805262 -4.2909727 -4.2877908 -4.27156 -4.2547016][-4.214498 -4.2130547 -4.2292767 -4.2593951 -4.2712674 -4.2633243 -4.253305 -4.2490745 -4.2605252 -4.267777 -4.2666178 -4.2678175 -4.2593622 -4.2372751 -4.2148952][-4.2047648 -4.1966996 -4.2130289 -4.2477374 -4.2636528 -4.2550764 -4.2359776 -4.2187128 -4.2260914 -4.2338486 -4.2271481 -4.2194877 -4.2079844 -4.1811028 -4.154952][-4.1896067 -4.1807528 -4.1978889 -4.2318974 -4.2491755 -4.2350273 -4.1963191 -4.1557827 -4.1520953 -4.1623912 -4.1615577 -4.1565852 -4.149353 -4.1243382 -4.0996037][-4.1582704 -4.1520834 -4.1761193 -4.2115345 -4.2261481 -4.2060819 -4.1511345 -4.0886049 -4.0729814 -4.0881195 -4.0970492 -4.1009927 -4.1076927 -4.0986133 -4.0867405][-4.128881 -4.1286364 -4.1634383 -4.2046032 -4.2156715 -4.1898737 -4.130547 -4.0615292 -4.041472 -4.0574937 -4.0722561 -4.0858955 -4.1062512 -4.1156507 -4.1137509][-4.13393 -4.1418543 -4.1817632 -4.2215333 -4.2284369 -4.1996326 -4.1464462 -4.0851512 -4.067729 -4.0799294 -4.0973263 -4.115468 -4.138938 -4.1562471 -4.1582918][-4.1620984 -4.1764622 -4.2138186 -4.2438488 -4.2428055 -4.2130203 -4.1690979 -4.121994 -4.1138434 -4.1274848 -4.1473641 -4.1638207 -4.1836858 -4.2010918 -4.206008][-4.1838379 -4.200417 -4.2334437 -4.252924 -4.2391171 -4.2101221 -4.1791763 -4.1483045 -4.1493764 -4.1679959 -4.1895332 -4.2070847 -4.2279596 -4.2462039 -4.2523675][-4.178628 -4.1907058 -4.2166834 -4.2279091 -4.2072735 -4.1798224 -4.1604514 -4.1451225 -4.1588683 -4.1851659 -4.2101049 -4.2325668 -4.257225 -4.276504 -4.285635][-4.1621671 -4.1646738 -4.17894 -4.1836586 -4.1613693 -4.1380482 -4.1347132 -4.1398954 -4.1642809 -4.1936159 -4.2192073 -4.2430282 -4.2698522 -4.2929835 -4.30651][-4.1582508 -4.1543636 -4.1584511 -4.1563125 -4.1329794 -4.1186728 -4.1315031 -4.1569452 -4.190042 -4.2188568 -4.24211 -4.2633381 -4.2884188 -4.3103104 -4.3246965][-4.1948123 -4.1909771 -4.1912212 -4.1882448 -4.1694531 -4.1632671 -4.1836486 -4.2138963 -4.2448311 -4.2651496 -4.2787066 -4.2919316 -4.3086267 -4.3241844 -4.3356214]]...]
INFO - root - 2017-12-08 00:16:08.650272: step 55610, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 75h:16m:44s remains)
INFO - root - 2017-12-08 00:16:18.433282: step 55620, loss = 2.10, batch loss = 2.05 (8.1 examples/sec; 0.988 sec/batch; 76h:00m:24s remains)
INFO - root - 2017-12-08 00:16:27.977012: step 55630, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 70h:06m:13s remains)
INFO - root - 2017-12-08 00:16:37.308992: step 55640, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.983 sec/batch; 75h:34m:36s remains)
INFO - root - 2017-12-08 00:16:46.815642: step 55650, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 73h:43m:39s remains)
INFO - root - 2017-12-08 00:16:56.418943: step 55660, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.992 sec/batch; 76h:18m:39s remains)
INFO - root - 2017-12-08 00:17:06.099142: step 55670, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.980 sec/batch; 75h:23m:18s remains)
INFO - root - 2017-12-08 00:17:15.687472: step 55680, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 75h:48m:57s remains)
INFO - root - 2017-12-08 00:17:25.325493: step 55690, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 76h:17m:29s remains)
INFO - root - 2017-12-08 00:17:34.903753: step 55700, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 74h:05m:40s remains)
2017-12-08 00:17:35.784099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3350029 -4.3391428 -4.3430004 -4.3463817 -4.3485804 -4.3500094 -4.3514533 -4.3527131 -4.3521256 -4.349247 -4.3454056 -4.3429847 -4.3421617 -4.3418436 -4.3414326][-4.33295 -4.3367457 -4.3400822 -4.3420715 -4.3432469 -4.3447866 -4.3467774 -4.3478756 -4.3444505 -4.337183 -4.3296547 -4.325603 -4.3260531 -4.3280897 -4.3291669][-4.3232388 -4.3239927 -4.3243494 -4.3231506 -4.32245 -4.3225274 -4.3229284 -4.3232112 -4.3172379 -4.3057237 -4.295897 -4.2909341 -4.2935557 -4.2991328 -4.3031793][-4.2945809 -4.2896571 -4.2845874 -4.278398 -4.2744393 -4.2728982 -4.2718997 -4.2746539 -4.2720909 -4.2614994 -4.2510886 -4.2447076 -4.2480879 -4.2563 -4.2630715][-4.2528558 -4.2401357 -4.2263837 -4.2110777 -4.1979785 -4.19306 -4.1913648 -4.202426 -4.2113538 -4.2086811 -4.2014928 -4.1950269 -4.2003217 -4.2116175 -4.2231035][-4.2171578 -4.1951051 -4.1702237 -4.1389246 -4.109024 -4.0954919 -4.0944185 -4.121017 -4.1510768 -4.1661429 -4.16921 -4.1665134 -4.1735363 -4.1867256 -4.2016363][-4.19235 -4.16551 -4.1315336 -4.0829182 -4.0318513 -4.003758 -4.0040259 -4.0508513 -4.109 -4.1476879 -4.1644063 -4.165144 -4.1699758 -4.1804924 -4.1939631][-4.1786966 -4.1538486 -4.1146173 -4.0497427 -3.97503 -3.9232767 -3.9204946 -3.9855132 -4.0697627 -4.1319923 -4.1641955 -4.1710625 -4.1731653 -4.1794195 -4.1894403][-4.1717386 -4.1576943 -4.124949 -4.0607772 -3.9824109 -3.924391 -3.92063 -3.9843767 -4.0671258 -4.1302962 -4.1663294 -4.1739306 -4.1730165 -4.177763 -4.1867247][-4.1753249 -4.1731992 -4.1558418 -4.1112642 -4.0554681 -4.0159097 -4.0174727 -4.0631952 -4.1199441 -4.1584249 -4.178678 -4.1793571 -4.1773872 -4.1845627 -4.1946716][-4.1871095 -4.1967793 -4.1954908 -4.1710691 -4.1359434 -4.1090417 -4.1114669 -4.1421075 -4.1758165 -4.1925559 -4.1960545 -4.1907692 -4.1906638 -4.1990142 -4.205864][-4.2039361 -4.2229996 -4.2335792 -4.2246838 -4.2023535 -4.1789908 -4.1786642 -4.2012739 -4.2225418 -4.2266808 -4.2206345 -4.2134686 -4.2161283 -4.2228556 -4.221014][-4.2300758 -4.250701 -4.26239 -4.2586274 -4.2425857 -4.2234426 -4.2227812 -4.2417426 -4.2587132 -4.2582903 -4.2495074 -4.2416329 -4.2426438 -4.2436743 -4.2320647][-4.252502 -4.2647972 -4.2691407 -4.2622495 -4.2480965 -4.2334046 -4.2329369 -4.2481809 -4.2640615 -4.2658906 -4.2612238 -4.2573366 -4.2579732 -4.2550259 -4.2361784][-4.2603483 -4.2653294 -4.2660608 -4.2590995 -4.2481136 -4.2381792 -4.2369008 -4.2466164 -4.2576895 -4.25918 -4.2585969 -4.2608461 -4.2659121 -4.2651181 -4.245223]]...]
INFO - root - 2017-12-08 00:17:45.230731: step 55710, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.982 sec/batch; 75h:30m:41s remains)
INFO - root - 2017-12-08 00:17:54.836554: step 55720, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 71h:19m:03s remains)
INFO - root - 2017-12-08 00:18:04.568082: step 55730, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 73h:59m:37s remains)
INFO - root - 2017-12-08 00:18:14.247466: step 55740, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.000 sec/batch; 76h:50m:30s remains)
INFO - root - 2017-12-08 00:18:23.744520: step 55750, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.901 sec/batch; 69h:17m:45s remains)
INFO - root - 2017-12-08 00:18:33.376114: step 55760, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.989 sec/batch; 76h:03m:03s remains)
INFO - root - 2017-12-08 00:18:43.106861: step 55770, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 74h:16m:56s remains)
INFO - root - 2017-12-08 00:18:52.603255: step 55780, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 72h:39m:04s remains)
INFO - root - 2017-12-08 00:19:02.246587: step 55790, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.966 sec/batch; 74h:15m:49s remains)
INFO - root - 2017-12-08 00:19:11.807371: step 55800, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.026 sec/batch; 78h:49m:53s remains)
2017-12-08 00:19:12.707174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1909657 -4.1741939 -4.1609864 -4.1614289 -4.1729579 -4.18653 -4.1961942 -4.1975284 -4.1958051 -4.196507 -4.1982851 -4.1996827 -4.2004905 -4.2040515 -4.2082334][-4.1820264 -4.1679716 -4.1576052 -4.1579204 -4.1667194 -4.1756735 -4.1831231 -4.1850462 -4.1842341 -4.1842527 -4.1850467 -4.1840472 -4.182405 -4.1833448 -4.1827][-4.2038636 -4.19951 -4.1969461 -4.1994977 -4.2037258 -4.2064495 -4.2087364 -4.2076726 -4.2052855 -4.2051268 -4.2074924 -4.2071915 -4.2032 -4.1975145 -4.1865959][-4.2226434 -4.229177 -4.2355065 -4.2385449 -4.2366881 -4.2342548 -4.2317739 -4.2279506 -4.227015 -4.2312436 -4.23962 -4.2439952 -4.2425451 -4.2375507 -4.2243686][-4.2057414 -4.2215538 -4.2360473 -4.2391934 -4.2319603 -4.2239594 -4.2139139 -4.2005892 -4.1989017 -4.2110124 -4.228188 -4.2399955 -4.2447953 -4.2475643 -4.2426281][-4.1617403 -4.1807928 -4.1972413 -4.1977677 -4.1841416 -4.1691322 -4.1460857 -4.1162605 -4.1112733 -4.1348042 -4.1648655 -4.1878572 -4.2008882 -4.2166767 -4.2280483][-4.1108007 -4.1262531 -4.1356568 -4.1256418 -4.1004753 -4.0737948 -4.0346828 -3.9834249 -3.9775288 -4.0166063 -4.0630631 -4.0999537 -4.1200657 -4.1478138 -4.1748343][-4.10067 -4.1131563 -4.1149526 -4.0934253 -4.0609474 -4.0300584 -3.9861202 -3.9235091 -3.9143844 -3.9586928 -4.0050273 -4.0427556 -4.0578747 -4.0831532 -4.11336][-4.1658564 -4.1762247 -4.1747284 -4.1534734 -4.1285739 -4.1092296 -4.0805578 -4.035675 -4.0252705 -4.0522528 -4.0775251 -4.0971403 -4.0975485 -4.1081338 -4.1265259][-4.252811 -4.2644382 -4.263546 -4.2455025 -4.2268147 -4.2125273 -4.1941814 -4.1674623 -4.1596003 -4.1733875 -4.1832895 -4.1890121 -4.1821365 -4.181447 -4.1896086][-4.3003178 -4.31067 -4.3113952 -4.2975626 -4.2831078 -4.2742186 -4.2669215 -4.2554331 -4.2525711 -4.2621675 -4.2688537 -4.2704329 -4.2618518 -4.2537975 -4.250515][-4.3145752 -4.3201513 -4.321876 -4.3133788 -4.3041644 -4.2984118 -4.2959189 -4.2924204 -4.2948728 -4.3053203 -4.3123555 -4.3121228 -4.3028231 -4.2878933 -4.2762189][-4.3219633 -4.320241 -4.3186917 -4.3138542 -4.30779 -4.3017468 -4.2970042 -4.2928624 -4.2960281 -4.3054996 -4.3104563 -4.3046103 -4.2919426 -4.2752867 -4.2614751][-4.3229895 -4.314714 -4.3104196 -4.3081665 -4.30395 -4.2952638 -4.2849007 -4.2763591 -4.2779274 -4.2874 -4.28966 -4.2777648 -4.2591639 -4.2393632 -4.2226214][-4.3138967 -4.3009238 -4.292747 -4.2885995 -4.280695 -4.2646818 -4.2470493 -4.2375674 -4.2441516 -4.2591543 -4.260076 -4.2398772 -4.2123394 -4.1861415 -4.1661963]]...]
INFO - root - 2017-12-08 00:19:22.290418: step 55810, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 71h:46m:40s remains)
INFO - root - 2017-12-08 00:19:31.833440: step 55820, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.972 sec/batch; 74h:42m:05s remains)
INFO - root - 2017-12-08 00:19:41.409010: step 55830, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.972 sec/batch; 74h:41m:25s remains)
INFO - root - 2017-12-08 00:19:51.167748: step 55840, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 73h:52m:10s remains)
INFO - root - 2017-12-08 00:20:00.755002: step 55850, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 72h:13m:07s remains)
INFO - root - 2017-12-08 00:20:10.263916: step 55860, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 73h:10m:40s remains)
INFO - root - 2017-12-08 00:20:19.763282: step 55870, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.994 sec/batch; 76h:20m:53s remains)
INFO - root - 2017-12-08 00:20:29.125427: step 55880, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 77h:41m:15s remains)
INFO - root - 2017-12-08 00:20:38.876295: step 55890, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 76h:34m:15s remains)
INFO - root - 2017-12-08 00:20:48.456018: step 55900, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 71h:09m:16s remains)
2017-12-08 00:20:49.528714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.033679 -4.1183577 -4.1980147 -4.2590537 -4.2972631 -4.3152051 -4.3210196 -4.3181882 -4.3103976 -4.3039827 -4.2978029 -4.2879944 -4.2744284 -4.2577715 -4.2400913][-4.1556854 -4.2066393 -4.257277 -4.2929544 -4.3126416 -4.3198128 -4.3217721 -4.3204064 -4.3173313 -4.3147912 -4.3119259 -4.3062167 -4.2968588 -4.2837396 -4.2689404][-4.2573938 -4.28342 -4.3062487 -4.3169808 -4.3169556 -4.31138 -4.30688 -4.3068657 -4.3106647 -4.3151731 -4.3175836 -4.3158941 -4.3100457 -4.3001671 -4.2883377][-4.3144364 -4.3248587 -4.3289661 -4.3201141 -4.30252 -4.2847037 -4.2745624 -4.2778053 -4.2924972 -4.3083754 -4.3191481 -4.3212066 -4.3160095 -4.3054624 -4.2931933][-4.3360872 -4.3361182 -4.3281226 -4.3025923 -4.2676892 -4.2380033 -4.2263474 -4.2393045 -4.2673173 -4.2974248 -4.3196573 -4.3283267 -4.3240809 -4.3108935 -4.2960773][-4.3358512 -4.3265233 -4.306109 -4.26485 -4.2139955 -4.172267 -4.1597018 -4.1812849 -4.2230225 -4.2688241 -4.3050184 -4.323575 -4.3257775 -4.3172541 -4.3044553][-4.3276315 -4.3115854 -4.2800884 -4.2229419 -4.1537118 -4.0924859 -4.066432 -4.0888667 -4.1482987 -4.2167664 -4.2709279 -4.3021894 -4.314127 -4.3129444 -4.3059049][-4.3253646 -4.3063555 -4.2681069 -4.198184 -4.1077561 -4.0153227 -3.9562111 -3.9668722 -4.0490341 -4.1471682 -4.221736 -4.266037 -4.2871265 -4.2923479 -4.2913589][-4.3310251 -4.3169231 -4.2821932 -4.2131047 -4.1112661 -3.9915669 -3.888485 -3.8672423 -3.9584868 -4.0782466 -4.167058 -4.2196746 -4.2468867 -4.2564816 -4.259244][-4.3389225 -4.3336563 -4.313632 -4.2636919 -4.1778708 -4.064961 -3.9549291 -3.9050887 -3.9626217 -4.0651817 -4.1471105 -4.1950765 -4.2175674 -4.2243509 -4.225965][-4.3435 -4.3441138 -4.3383937 -4.3129139 -4.2582994 -4.1759481 -4.0914311 -4.0446067 -4.0701838 -4.1350842 -4.19247 -4.2236071 -4.230237 -4.2211637 -4.2106414][-4.3451223 -4.3484492 -4.3511868 -4.3448606 -4.3192763 -4.2712145 -4.217361 -4.1840134 -4.1941342 -4.2315512 -4.2655315 -4.2788882 -4.2708344 -4.2478833 -4.2244759][-4.3442655 -4.3476787 -4.3527131 -4.3560452 -4.3500586 -4.3296309 -4.301651 -4.2818842 -4.2848859 -4.3031907 -4.3177848 -4.3159957 -4.2981253 -4.27166 -4.2475853][-4.3426547 -4.3452806 -4.3489428 -4.3529949 -4.3535743 -4.3473043 -4.337503 -4.3295746 -4.3320227 -4.3396778 -4.3412485 -4.326972 -4.3006024 -4.2734928 -4.2558074][-4.340755 -4.3423944 -4.3445635 -4.3468628 -4.3483353 -4.3468347 -4.3446908 -4.3439207 -4.346642 -4.34792 -4.3400064 -4.3135405 -4.2781029 -4.2495751 -4.2398777]]...]
INFO - root - 2017-12-08 00:20:59.179257: step 55910, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 72h:02m:50s remains)
INFO - root - 2017-12-08 00:21:08.780367: step 55920, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 72h:52m:02s remains)
INFO - root - 2017-12-08 00:21:18.372495: step 55930, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.958 sec/batch; 73h:34m:20s remains)
INFO - root - 2017-12-08 00:21:28.124216: step 55940, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.998 sec/batch; 76h:42m:20s remains)
INFO - root - 2017-12-08 00:21:37.850023: step 55950, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 74h:24m:15s remains)
INFO - root - 2017-12-08 00:21:47.542992: step 55960, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 73h:28m:51s remains)
INFO - root - 2017-12-08 00:21:57.259654: step 55970, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 74h:21m:51s remains)
INFO - root - 2017-12-08 00:22:06.867790: step 55980, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 70h:26m:02s remains)
INFO - root - 2017-12-08 00:22:16.608185: step 55990, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 74h:02m:00s remains)
INFO - root - 2017-12-08 00:22:26.378551: step 56000, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 71h:45m:16s remains)
2017-12-08 00:22:27.373770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3218174 -4.3128123 -4.3029966 -4.2937708 -4.2877703 -4.2851605 -4.285368 -4.2862659 -4.2791643 -4.2681122 -4.2670169 -4.2707362 -4.2652626 -4.2464328 -4.2253718][-4.3107872 -4.3014069 -4.2915535 -4.2814031 -4.2744513 -4.272419 -4.275773 -4.2814741 -4.2778091 -4.2668633 -4.2642026 -4.2723279 -4.2723823 -4.2605739 -4.2417336][-4.30286 -4.294837 -4.2846894 -4.2720156 -4.2614326 -4.2563281 -4.2588592 -4.2626138 -4.2586 -4.2478361 -4.2483139 -4.2635732 -4.272058 -4.2686419 -4.2490196][-4.3039079 -4.2955284 -4.2818117 -4.2643876 -4.2462173 -4.2351809 -4.2337923 -4.2324152 -4.2261653 -4.2183056 -4.2256927 -4.2499781 -4.2672524 -4.269762 -4.2510386][-4.3122544 -4.3021593 -4.283123 -4.2598691 -4.233768 -4.2149839 -4.2076988 -4.201158 -4.1960988 -4.1983538 -4.2148952 -4.2435441 -4.2636671 -4.2689748 -4.2539978][-4.3231516 -4.3143907 -4.2907381 -4.2603722 -4.22514 -4.1964293 -4.1812868 -4.172811 -4.1747332 -4.1889877 -4.2118478 -4.2422481 -4.2643752 -4.2733369 -4.2637057][-4.3335671 -4.3275514 -4.3005233 -4.2597609 -4.2117267 -4.1718802 -4.154851 -4.1548767 -4.172317 -4.197762 -4.2215767 -4.2488942 -4.2674527 -4.2737436 -4.266654][-4.3398633 -4.3355594 -4.30408 -4.2513547 -4.1905189 -4.1427655 -4.1293187 -4.14466 -4.1797147 -4.2135005 -4.2346044 -4.2547116 -4.2667561 -4.2678504 -4.2574053][-4.3396397 -4.3341231 -4.2974477 -4.235405 -4.1673574 -4.116 -4.1088166 -4.1404948 -4.1891913 -4.2253213 -4.241796 -4.2529197 -4.2588964 -4.2578921 -4.2434492][-4.3382254 -4.3321953 -4.2932458 -4.2278662 -4.1593285 -4.1120987 -4.1113153 -4.1487641 -4.1986713 -4.2331619 -4.2465582 -4.2500896 -4.2508478 -4.2500005 -4.234673][-4.3401456 -4.3349295 -4.2996845 -4.2396321 -4.1791153 -4.1402488 -4.1409607 -4.1712723 -4.2141414 -4.2465563 -4.2572532 -4.2581568 -4.2575383 -4.2561116 -4.239912][-4.3448734 -4.3401756 -4.3120465 -4.2645755 -4.2169628 -4.1877575 -4.1871052 -4.2072721 -4.2409024 -4.2705851 -4.2796507 -4.2791209 -4.2774439 -4.2756763 -4.2619624][-4.351531 -4.3479729 -4.3293085 -4.297214 -4.2644854 -4.2439222 -4.2391992 -4.2512131 -4.2773237 -4.3024063 -4.3092523 -4.3088179 -4.3092532 -4.3082476 -4.2971516][-4.3567967 -4.3553724 -4.3455539 -4.3282485 -4.310185 -4.2976875 -4.2926059 -4.2989211 -4.3173532 -4.3344693 -4.3374109 -4.3371158 -4.3404436 -4.3416557 -4.3333468][-4.3622189 -4.3618188 -4.35747 -4.3497849 -4.3408275 -4.3344908 -4.3306456 -4.3325944 -4.3436751 -4.3517389 -4.3508477 -4.3506227 -4.3569207 -4.3607168 -4.3563948]]...]
INFO - root - 2017-12-08 00:22:37.115111: step 56010, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.958 sec/batch; 73h:35m:14s remains)
INFO - root - 2017-12-08 00:22:46.835482: step 56020, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 71h:35m:16s remains)
INFO - root - 2017-12-08 00:22:56.528593: step 56030, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.976 sec/batch; 74h:56m:36s remains)
INFO - root - 2017-12-08 00:23:06.119882: step 56040, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 72h:28m:16s remains)
INFO - root - 2017-12-08 00:23:15.726009: step 56050, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 73h:23m:21s remains)
INFO - root - 2017-12-08 00:23:25.379099: step 56060, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 71h:05m:55s remains)
INFO - root - 2017-12-08 00:23:34.866916: step 56070, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 66h:08m:22s remains)
INFO - root - 2017-12-08 00:23:44.378587: step 56080, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 71h:40m:07s remains)
INFO - root - 2017-12-08 00:23:53.961113: step 56090, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.973 sec/batch; 74h:43m:38s remains)
INFO - root - 2017-12-08 00:24:03.645203: step 56100, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 71h:21m:51s remains)
2017-12-08 00:24:04.755519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2020235 -4.2005577 -4.2143626 -4.2246776 -4.2235956 -4.2076068 -4.1960258 -4.1974416 -4.198071 -4.1926718 -4.1826229 -4.1787057 -4.1866741 -4.194838 -4.19336][-4.2409921 -4.2524762 -4.2745762 -4.2865591 -4.2810569 -4.2588515 -4.2396631 -4.2390771 -4.2437639 -4.2421103 -4.2371135 -4.23502 -4.2434368 -4.2538948 -4.2540641][-4.2729115 -4.2864103 -4.3044276 -4.3094788 -4.2988725 -4.2758408 -4.2545767 -4.2561579 -4.2682738 -4.2740741 -4.2760687 -4.2790179 -4.2871861 -4.2967048 -4.2970481][-4.305851 -4.3120418 -4.3159552 -4.3081861 -4.2919474 -4.2684665 -4.2463923 -4.2493234 -4.2700367 -4.2862568 -4.2970266 -4.3076677 -4.319191 -4.3281293 -4.3278475][-4.3311 -4.3237958 -4.3142571 -4.2958946 -4.271399 -4.2390761 -4.2104583 -4.2104759 -4.235877 -4.2596092 -4.2779694 -4.2966609 -4.3175297 -4.3296824 -4.3269978][-4.3312669 -4.3103023 -4.2867966 -4.2578955 -4.2241764 -4.18001 -4.1378117 -4.1299319 -4.1605864 -4.192173 -4.2183223 -4.2464767 -4.2787566 -4.2967448 -4.2938094][-4.30245 -4.2731028 -4.2393565 -4.1994419 -4.1515012 -4.0891056 -4.027596 -4.0075116 -4.0451956 -4.0921912 -4.12891 -4.1658783 -4.2079854 -4.2351251 -4.2406621][-4.2482767 -4.2192488 -4.1836452 -4.1348076 -4.0728612 -3.993155 -3.9060049 -3.8658028 -3.907917 -3.9725463 -4.0240617 -4.0770984 -4.136116 -4.1802869 -4.203516][-4.2033806 -4.185822 -4.159327 -4.1115174 -4.0479236 -3.9651754 -3.8721547 -3.8194416 -3.8535836 -3.9218707 -3.9826095 -4.0476179 -4.1158109 -4.1689768 -4.2059484][-4.21399 -4.2115498 -4.2002831 -4.1662674 -4.1192508 -4.05945 -3.990864 -3.942028 -3.9543574 -4.0028038 -4.051918 -4.1055126 -4.1592436 -4.2016449 -4.2351861][-4.2719312 -4.2763891 -4.2725921 -4.2516794 -4.2200766 -4.1791954 -4.134922 -4.099154 -4.0943007 -4.12249 -4.152751 -4.1860647 -4.2202463 -4.2468481 -4.2692108][-4.3214126 -4.3240762 -4.3210526 -4.308598 -4.2852678 -4.2546573 -4.2267342 -4.2016478 -4.1905761 -4.2065992 -4.2265477 -4.2465467 -4.2674489 -4.286931 -4.3007526][-4.3310924 -4.328763 -4.3260531 -4.3221507 -4.3098145 -4.2902484 -4.2723536 -4.2500811 -4.2336349 -4.24116 -4.2580204 -4.2755079 -4.294137 -4.310823 -4.3174725][-4.317102 -4.310966 -4.3072233 -4.305058 -4.2980518 -4.2839065 -4.2708678 -4.2509937 -4.2355375 -4.2423134 -4.2633481 -4.2846708 -4.3046131 -4.3203082 -4.3224373][-4.2980366 -4.285574 -4.2778149 -4.2740145 -4.2700996 -4.2634621 -4.2562127 -4.2436142 -4.2348466 -4.2449565 -4.26779 -4.2897878 -4.3082585 -4.32235 -4.3241272]]...]
INFO - root - 2017-12-08 00:24:14.178804: step 56110, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 73h:01m:10s remains)
INFO - root - 2017-12-08 00:24:23.882299: step 56120, loss = 2.06, batch loss = 2.01 (7.7 examples/sec; 1.038 sec/batch; 79h:43m:15s remains)
INFO - root - 2017-12-08 00:24:33.613781: step 56130, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 74h:28m:28s remains)
INFO - root - 2017-12-08 00:24:43.201968: step 56140, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 71h:25m:09s remains)
INFO - root - 2017-12-08 00:24:52.885969: step 56150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 72h:44m:56s remains)
INFO - root - 2017-12-08 00:25:02.666221: step 56160, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.018 sec/batch; 78h:08m:40s remains)
INFO - root - 2017-12-08 00:25:12.269395: step 56170, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 76h:32m:58s remains)
INFO - root - 2017-12-08 00:25:21.836397: step 56180, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 74h:41m:47s remains)
INFO - root - 2017-12-08 00:25:31.370837: step 56190, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 70h:41m:26s remains)
INFO - root - 2017-12-08 00:25:41.174741: step 56200, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.979 sec/batch; 75h:06m:47s remains)
2017-12-08 00:25:42.062116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2613993 -4.2617688 -4.2607942 -4.2510257 -4.2347622 -4.218998 -4.215188 -4.2157059 -4.2108126 -4.19908 -4.1881046 -4.1943903 -4.2148089 -4.2371073 -4.2436438][-4.2580605 -4.258462 -4.2591052 -4.250526 -4.2335811 -4.2191353 -4.2174864 -4.2187819 -4.2110987 -4.1937227 -4.1788611 -4.1851678 -4.2049994 -4.2257857 -4.236012][-4.2351685 -4.2332449 -4.2337937 -4.228179 -4.21411 -4.2019825 -4.2010903 -4.2026377 -4.1969752 -4.1835818 -4.1722307 -4.176712 -4.1918035 -4.2096782 -4.222774][-4.2031164 -4.1996508 -4.2009273 -4.1999507 -4.1878004 -4.1728473 -4.1670904 -4.1678138 -4.1725726 -4.1768866 -4.1778412 -4.1828766 -4.1925445 -4.2073574 -4.2220821][-4.1862111 -4.1832981 -4.1857409 -4.1854243 -4.1689062 -4.1413374 -4.11772 -4.1100192 -4.1309052 -4.1646395 -4.1870275 -4.1976261 -4.2022486 -4.2107944 -4.2201362][-4.1871228 -4.1819248 -4.1788254 -4.1712446 -4.1436505 -4.0922904 -4.0339808 -4.0050673 -4.047492 -4.12183 -4.1743746 -4.1925879 -4.1959915 -4.2003736 -4.2046146][-4.1883569 -4.1793346 -4.1674738 -4.151155 -4.1104813 -4.0360012 -3.9403026 -3.8902662 -3.9602733 -4.0725694 -4.1464219 -4.1699128 -4.1728144 -4.1741624 -4.1805296][-4.1978769 -4.1873746 -4.17252 -4.1548915 -4.1161089 -4.0431867 -3.9501958 -3.9068384 -3.9760354 -4.0861583 -4.1535707 -4.1673613 -4.1610107 -4.1585336 -4.1696587][-4.2244244 -4.2176328 -4.2068248 -4.194984 -4.1689219 -4.1178913 -4.0554314 -4.0294223 -4.0768232 -4.1514926 -4.1929722 -4.1918135 -4.1760941 -4.1716366 -4.1858983][-4.2451239 -4.2454185 -4.2424083 -4.2402134 -4.2308106 -4.2048869 -4.170558 -4.1551342 -4.17997 -4.2185926 -4.2369823 -4.228169 -4.2109771 -4.2075019 -4.222455][-4.2359467 -4.2444878 -4.2541676 -4.2653852 -4.2702975 -4.26044 -4.2449541 -4.2384763 -4.2498174 -4.2640557 -4.2676945 -4.2607217 -4.2519593 -4.252048 -4.26268][-4.2028189 -4.218698 -4.2417717 -4.2652025 -4.2815742 -4.2824769 -4.2793097 -4.2777481 -4.2791376 -4.2799492 -4.2788076 -4.2775316 -4.2786241 -4.2826586 -4.2871566][-4.1644764 -4.1856623 -4.2179561 -4.2488003 -4.2709293 -4.2791591 -4.281045 -4.2807627 -4.2785234 -4.2753143 -4.2756529 -4.2795033 -4.2843766 -4.2869678 -4.283823][-4.1399155 -4.1605439 -4.196475 -4.2310658 -4.2570186 -4.2700257 -4.2732735 -4.271605 -4.2681308 -4.2658219 -4.2684765 -4.2747445 -4.279213 -4.2796011 -4.2738838][-4.1510177 -4.1643548 -4.1953578 -4.2279267 -4.2545366 -4.2681761 -4.2718062 -4.2696805 -4.2663231 -4.2645149 -4.2676716 -4.2741203 -4.277956 -4.2793031 -4.2753286]]...]
INFO - root - 2017-12-08 00:25:51.706593: step 56210, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 74h:34m:47s remains)
INFO - root - 2017-12-08 00:26:01.393558: step 56220, loss = 2.07, batch loss = 2.02 (7.8 examples/sec; 1.022 sec/batch; 78h:27m:50s remains)
INFO - root - 2017-12-08 00:26:11.051893: step 56230, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 72h:58m:56s remains)
INFO - root - 2017-12-08 00:26:20.735281: step 56240, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.008 sec/batch; 77h:19m:16s remains)
INFO - root - 2017-12-08 00:26:30.371655: step 56250, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 72h:53m:58s remains)
INFO - root - 2017-12-08 00:26:39.992884: step 56260, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 75h:09m:20s remains)
INFO - root - 2017-12-08 00:26:49.592489: step 56270, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 76h:47m:10s remains)
INFO - root - 2017-12-08 00:26:59.281368: step 56280, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 74h:45m:55s remains)
INFO - root - 2017-12-08 00:27:08.937633: step 56290, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 73h:42m:23s remains)
INFO - root - 2017-12-08 00:27:18.626238: step 56300, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 74h:06m:31s remains)
2017-12-08 00:27:19.677140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3020654 -4.2984619 -4.2902036 -4.2861118 -4.2908149 -4.2998214 -4.3049936 -4.3036442 -4.299016 -4.2959204 -4.2924051 -4.2927294 -4.296629 -4.3002934 -4.2993197][-4.2798572 -4.2679367 -4.2510185 -4.2443924 -4.2577338 -4.27348 -4.2778735 -4.2722692 -4.2643585 -4.2605567 -4.2583518 -4.259851 -4.2644944 -4.2680268 -4.2651811][-4.2656946 -4.2434268 -4.2158875 -4.2043424 -4.2234492 -4.24021 -4.2373786 -4.2221732 -4.2100244 -4.2127466 -4.2205009 -4.2271833 -4.2344618 -4.2382741 -4.2320275][-4.2459974 -4.215971 -4.1815076 -4.1674914 -4.1870952 -4.195775 -4.1789947 -4.1490512 -4.1317611 -4.1486878 -4.1785707 -4.204247 -4.2205043 -4.2272892 -4.2199483][-4.2196617 -4.1866975 -4.1489434 -4.1336164 -4.1495285 -4.1390176 -4.0971656 -4.0446644 -4.0232687 -4.0615277 -4.1207376 -4.1688623 -4.1984577 -4.214047 -4.2080741][-4.2069035 -4.1741867 -4.1327124 -4.1082468 -4.1068745 -4.0698485 -4.0009336 -3.9227002 -3.9032686 -3.9760804 -4.0697689 -4.1459236 -4.1943407 -4.2205443 -4.2184849][-4.2176027 -4.1872544 -4.142303 -4.1030478 -4.0730491 -4.0075226 -3.9132121 -3.8152161 -3.8118429 -3.927825 -4.0501833 -4.1411986 -4.19949 -4.2314739 -4.2308226][-4.2253804 -4.1955295 -4.1519423 -4.1154852 -4.0814962 -4.0158968 -3.9317207 -3.8554456 -3.8725686 -3.9861295 -4.094192 -4.166431 -4.2113237 -4.2336111 -4.2282724][-4.2177491 -4.1905513 -4.1541581 -4.1338978 -4.1202693 -4.0809751 -4.0297132 -3.9949648 -4.0193734 -4.0965514 -4.1626191 -4.2012687 -4.2220607 -4.2277374 -4.2127328][-4.2079587 -4.1880155 -4.1635127 -4.1608281 -4.1660075 -4.1489878 -4.1201487 -4.1015015 -4.1171088 -4.1647868 -4.2018657 -4.2143021 -4.2133718 -4.2079992 -4.1865935][-4.2110577 -4.1920753 -4.1749878 -4.1812291 -4.1983557 -4.1972446 -4.1865382 -4.1753435 -4.1820946 -4.2124476 -4.2342253 -4.2335019 -4.2192712 -4.2039065 -4.1808934][-4.2298694 -4.2076783 -4.1914496 -4.200974 -4.2260838 -4.234508 -4.2346678 -4.2296877 -4.2311687 -4.2449641 -4.2565584 -4.2552137 -4.241981 -4.2255549 -4.2041392][-4.2528906 -4.2332525 -4.219131 -4.2293415 -4.2522264 -4.259161 -4.2630153 -4.2649755 -4.2669764 -4.2738428 -4.2814593 -4.2811966 -4.270751 -4.2569809 -4.2392015][-4.27907 -4.2653427 -4.2520871 -4.2569509 -4.269186 -4.2686009 -4.2720852 -4.2756257 -4.27862 -4.2839665 -4.2911768 -4.2926044 -4.2881513 -4.2807255 -4.2700572][-4.2937865 -4.2841287 -4.27572 -4.2794571 -4.2840309 -4.279376 -4.2792535 -4.2826591 -4.2868009 -4.2903562 -4.2964268 -4.2990623 -4.2967815 -4.2907577 -4.2866335]]...]
INFO - root - 2017-12-08 00:27:29.413751: step 56310, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 74h:21m:03s remains)
INFO - root - 2017-12-08 00:27:38.887950: step 56320, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 73h:47m:55s remains)
INFO - root - 2017-12-08 00:27:48.519036: step 56330, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 75h:02m:01s remains)
INFO - root - 2017-12-08 00:27:58.271993: step 56340, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.994 sec/batch; 76h:16m:17s remains)
INFO - root - 2017-12-08 00:28:07.966821: step 56350, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 75h:46m:43s remains)
INFO - root - 2017-12-08 00:28:17.606857: step 56360, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 72h:54m:11s remains)
INFO - root - 2017-12-08 00:28:27.148510: step 56370, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 73h:33m:11s remains)
INFO - root - 2017-12-08 00:28:36.730869: step 56380, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 73h:06m:48s remains)
INFO - root - 2017-12-08 00:28:46.432481: step 56390, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 75h:54m:39s remains)
INFO - root - 2017-12-08 00:28:56.072200: step 56400, loss = 2.11, batch loss = 2.06 (8.1 examples/sec; 0.992 sec/batch; 76h:04m:50s remains)
2017-12-08 00:28:57.021581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2580776 -4.2559085 -4.2598643 -4.2600288 -4.2564831 -4.2558279 -4.2595253 -4.2727275 -4.2862105 -4.2891364 -4.2838058 -4.2808061 -4.2801709 -4.2811804 -4.2855029][-4.2427864 -4.2388153 -4.2411318 -4.2365913 -4.2301688 -4.2287345 -4.2348838 -4.2559428 -4.2791061 -4.2892771 -4.2884512 -4.2865615 -4.2825546 -4.2778373 -4.2789989][-4.2345586 -4.2284794 -4.2242332 -4.2065439 -4.1908655 -4.1884842 -4.1983542 -4.2270122 -4.2604704 -4.2795558 -4.2886357 -4.2912116 -4.2849483 -4.2737012 -4.2697725][-4.2302155 -4.2228618 -4.2108068 -4.1808362 -4.1579385 -4.1525493 -4.1609421 -4.1975689 -4.2404857 -4.2679439 -4.2859812 -4.2959275 -4.2880883 -4.2714248 -4.2620268][-4.2299643 -4.224813 -4.2038379 -4.1603065 -4.1232872 -4.1026297 -4.1021461 -4.1514597 -4.2126 -4.2525949 -4.2785869 -4.2924356 -4.284493 -4.2654176 -4.2524757][-4.2363677 -4.2329216 -4.2051082 -4.1511393 -4.0947509 -4.0420313 -4.0192351 -4.0796151 -4.1683426 -4.2308745 -4.2679458 -4.2848916 -4.2781706 -4.2610354 -4.2476521][-4.2406459 -4.2336369 -4.2007427 -4.1402249 -4.0675039 -3.9806395 -3.9245484 -3.9834394 -4.096602 -4.1839972 -4.2354789 -4.2599263 -4.2620521 -4.2539911 -4.2432652][-4.24479 -4.2361603 -4.205215 -4.1499009 -4.0732121 -3.969981 -3.889925 -3.9332471 -4.0471644 -4.1438503 -4.2033591 -4.2332888 -4.2459774 -4.2488017 -4.2428331][-4.2434344 -4.2421904 -4.2273226 -4.1917491 -4.1324811 -4.0496039 -3.9807322 -4.0024552 -4.0789843 -4.1471591 -4.193738 -4.2223997 -4.2418208 -4.2530565 -4.2557645][-4.2271771 -4.2362905 -4.2411332 -4.2300344 -4.1955285 -4.1477065 -4.1069217 -4.1149292 -4.1520858 -4.18747 -4.2146716 -4.2344584 -4.251987 -4.2636065 -4.2711143][-4.2067351 -4.2225304 -4.2350612 -4.2349658 -4.2191229 -4.1993389 -4.1867867 -4.1939778 -4.212831 -4.2337818 -4.2493095 -4.2574134 -4.2647667 -4.2687359 -4.2727928][-4.1880627 -4.1991148 -4.2070861 -4.207531 -4.203753 -4.206255 -4.2110376 -4.2181282 -4.2353764 -4.255712 -4.2658286 -4.2687597 -4.2690244 -4.2618093 -4.25656][-4.1750522 -4.1756787 -4.1726751 -4.1652269 -4.1626053 -4.1730113 -4.181036 -4.1876521 -4.2086153 -4.2360296 -4.2512522 -4.2582097 -4.2618093 -4.2494416 -4.2345576][-4.1832409 -4.1740847 -4.1580381 -4.1363893 -4.1253524 -4.1277633 -4.1237059 -4.1254787 -4.158412 -4.2018385 -4.2272072 -4.2381387 -4.245327 -4.2375479 -4.2158203][-4.1992269 -4.1864271 -4.161891 -4.1252937 -4.1000357 -4.0836291 -4.0560226 -4.0498319 -4.0960722 -4.1585183 -4.1988997 -4.2223272 -4.2371535 -4.2408252 -4.2228169]]...]
INFO - root - 2017-12-08 00:29:06.700695: step 56410, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.985 sec/batch; 75h:33m:56s remains)
INFO - root - 2017-12-08 00:29:16.356502: step 56420, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.925 sec/batch; 70h:58m:05s remains)
INFO - root - 2017-12-08 00:29:26.007317: step 56430, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.936 sec/batch; 71h:46m:52s remains)
INFO - root - 2017-12-08 00:29:35.608804: step 56440, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 73h:51m:04s remains)
INFO - root - 2017-12-08 00:29:45.326986: step 56450, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.011 sec/batch; 77h:33m:30s remains)
INFO - root - 2017-12-08 00:29:54.906674: step 56460, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 76h:58m:56s remains)
INFO - root - 2017-12-08 00:30:04.649144: step 56470, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.997 sec/batch; 76h:28m:17s remains)
INFO - root - 2017-12-08 00:30:14.273471: step 56480, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 72h:30m:11s remains)
INFO - root - 2017-12-08 00:30:24.011750: step 56490, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.994 sec/batch; 76h:13m:36s remains)
INFO - root - 2017-12-08 00:30:33.673974: step 56500, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 73h:00m:55s remains)
2017-12-08 00:30:34.661890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1817245 -4.1617064 -4.1498637 -4.1486263 -4.1618176 -4.1898417 -4.2213025 -4.2474141 -4.2633877 -4.2631116 -4.2471542 -4.2258883 -4.208858 -4.1979237 -4.1895504][-4.1934371 -4.1767764 -4.1698332 -4.1698742 -4.1727934 -4.187531 -4.2142406 -4.2454143 -4.276226 -4.2925668 -4.2908711 -4.2742782 -4.251955 -4.2341256 -4.2136545][-4.2197189 -4.2067204 -4.20742 -4.2115602 -4.2056947 -4.1989889 -4.2087731 -4.2381587 -4.2764039 -4.3017697 -4.3071752 -4.2920728 -4.2705493 -4.2532549 -4.2296004][-4.2413249 -4.2317047 -4.23721 -4.2439642 -4.2307987 -4.20221 -4.1883583 -4.2093263 -4.2552619 -4.2915254 -4.3033996 -4.2902102 -4.2721429 -4.2602367 -4.2389479][-4.2359447 -4.22944 -4.2407393 -4.2533746 -4.2418981 -4.2008071 -4.1638103 -4.1704044 -4.2183294 -4.2638569 -4.2803569 -4.2705393 -4.2581272 -4.2546082 -4.2392807][-4.2000694 -4.19529 -4.216475 -4.243413 -4.2434187 -4.2011719 -4.1473441 -4.1370687 -4.1778822 -4.2265387 -4.2495346 -4.2453547 -4.2359209 -4.2381921 -4.2304912][-4.1414661 -4.1418 -4.1770554 -4.2225304 -4.2384629 -4.2033129 -4.1402969 -4.10988 -4.1342063 -4.1781526 -4.20596 -4.2107015 -4.209054 -4.2189808 -4.221158][-4.0994358 -4.1059232 -4.1478848 -4.2014017 -4.226965 -4.2011142 -4.1391687 -4.0956326 -4.1018171 -4.131855 -4.1600413 -4.1724868 -4.1786127 -4.1974425 -4.2101207][-4.101325 -4.11215 -4.1464891 -4.1912694 -4.2137709 -4.1944413 -4.1444798 -4.1049418 -4.1018462 -4.1173887 -4.1345105 -4.1450338 -4.1538982 -4.1750431 -4.1938009][-4.1387453 -4.1486993 -4.1676159 -4.1935697 -4.20513 -4.1866093 -4.1493225 -4.12283 -4.1237459 -4.1329694 -4.1364536 -4.1377125 -4.142045 -4.1608438 -4.1830425][-4.1952977 -4.2006683 -4.2056627 -4.2147741 -4.2145805 -4.1936646 -4.1653228 -4.153038 -4.16355 -4.1730065 -4.1674871 -4.1566448 -4.1486406 -4.1577349 -4.1810889][-4.2517791 -4.254838 -4.2526426 -4.2522111 -4.2461987 -4.2233067 -4.1959896 -4.1914635 -4.2098846 -4.2222152 -4.2133374 -4.1934233 -4.1714492 -4.1670251 -4.1885352][-4.2827768 -4.2859597 -4.2812066 -4.277915 -4.272212 -4.2518582 -4.2279024 -4.2263517 -4.2463632 -4.2606912 -4.2522945 -4.2281871 -4.1943393 -4.1748877 -4.1900792][-4.2942352 -4.2970405 -4.2905259 -4.2869921 -4.2847977 -4.2688928 -4.2480531 -4.2467971 -4.2651839 -4.2793808 -4.2746439 -4.2522469 -4.2116036 -4.1789255 -4.1856031][-4.2941685 -4.2972093 -4.2917585 -4.2906113 -4.2911162 -4.2814074 -4.2658167 -4.2646494 -4.2799239 -4.2922344 -4.2896318 -4.2695236 -4.2274222 -4.1858659 -4.1854692]]...]
INFO - root - 2017-12-08 00:30:44.339180: step 56510, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.023 sec/batch; 78h:24m:20s remains)
INFO - root - 2017-12-08 00:30:54.043519: step 56520, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 73h:53m:19s remains)
INFO - root - 2017-12-08 00:31:03.635151: step 56530, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 72h:20m:23s remains)
INFO - root - 2017-12-08 00:31:13.152680: step 56540, loss = 2.10, batch loss = 2.05 (8.4 examples/sec; 0.954 sec/batch; 73h:07m:05s remains)
INFO - root - 2017-12-08 00:31:22.768349: step 56550, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 74h:59m:54s remains)
INFO - root - 2017-12-08 00:31:32.502583: step 56560, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 76h:53m:25s remains)
INFO - root - 2017-12-08 00:31:42.214171: step 56570, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 68h:50m:02s remains)
INFO - root - 2017-12-08 00:31:51.640266: step 56580, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 73h:42m:00s remains)
INFO - root - 2017-12-08 00:32:01.323287: step 56590, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 73h:45m:07s remains)
INFO - root - 2017-12-08 00:32:11.016846: step 56600, loss = 2.10, batch loss = 2.05 (8.2 examples/sec; 0.970 sec/batch; 74h:21m:07s remains)
2017-12-08 00:32:12.045985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2916784 -4.2916307 -4.285275 -4.2732182 -4.2610188 -4.2550621 -4.2515044 -4.2487116 -4.2547917 -4.2634277 -4.2714295 -4.2795072 -4.2812386 -4.2748852 -4.26415][-4.2714715 -4.2748971 -4.2755771 -4.2676821 -4.2536173 -4.244729 -4.2345562 -4.225852 -4.2334805 -4.2463822 -4.2603326 -4.2718449 -4.2741189 -4.2680511 -4.2580757][-4.2525511 -4.2604 -4.2673254 -4.2617197 -4.2422872 -4.2228227 -4.1954751 -4.1749492 -4.1858473 -4.2089219 -4.2349129 -4.2549944 -4.2631068 -4.264616 -4.2612214][-4.2480197 -4.2561193 -4.2658191 -4.2604914 -4.2324667 -4.1994948 -4.1487546 -4.11089 -4.1299095 -4.1661644 -4.2062221 -4.236855 -4.2508268 -4.2600546 -4.2645698][-4.25012 -4.2517519 -4.2613769 -4.2570081 -4.2221274 -4.1771288 -4.1003642 -4.0396533 -4.0701761 -4.1253853 -4.1758828 -4.215519 -4.2355919 -4.2531271 -4.2663336][-4.2547331 -4.246388 -4.2523594 -4.2429395 -4.199574 -4.1423774 -4.0364528 -3.9408646 -3.9918635 -4.0774589 -4.14109 -4.1959152 -4.2268319 -4.2540579 -4.2760644][-4.268579 -4.2506795 -4.2481294 -4.2282429 -4.1735039 -4.0989695 -3.9533517 -3.8035674 -3.8794968 -4.0107961 -4.1011977 -4.1764855 -4.2236195 -4.2606378 -4.2903652][-4.2866631 -4.2614861 -4.2501388 -4.2227654 -4.1566191 -4.0606585 -3.875998 -3.6726291 -3.7663641 -3.9440627 -4.061799 -4.1567845 -4.2159557 -4.2601032 -4.2971697][-4.301919 -4.2740712 -4.2573085 -4.2323751 -4.1711521 -4.0779114 -3.9045246 -3.7134545 -3.7963409 -3.9677944 -4.0752683 -4.1643634 -4.2193856 -4.2627478 -4.3032269][-4.3184681 -4.2911577 -4.2750416 -4.2584095 -4.2164149 -4.1480708 -4.0286589 -3.9020677 -3.9478168 -4.0628395 -4.1328773 -4.1994572 -4.2392979 -4.2752442 -4.3085856][-4.32713 -4.3033953 -4.2903857 -4.2801156 -4.2509532 -4.2005715 -4.1286173 -4.0572476 -4.0798068 -4.1479487 -4.1903415 -4.2342291 -4.257513 -4.2839651 -4.309412][-4.3326936 -4.3144422 -4.3019495 -4.2964463 -4.2769156 -4.240643 -4.201056 -4.1682968 -4.1829524 -4.2223287 -4.2461996 -4.2679715 -4.276103 -4.2920966 -4.311193][-4.3329268 -4.3175545 -4.3011184 -4.2969003 -4.2852392 -4.2606997 -4.2436314 -4.2384567 -4.2520204 -4.2744613 -4.2893796 -4.2972512 -4.2966795 -4.3044362 -4.3176818][-4.3362255 -4.3222065 -4.3050051 -4.30181 -4.3005228 -4.289412 -4.2839208 -4.2918415 -4.3043723 -4.3146596 -4.3195252 -4.3193517 -4.315989 -4.3196487 -4.3266783][-4.3376126 -4.3266292 -4.312222 -4.310257 -4.3149185 -4.31648 -4.3198342 -4.3299603 -4.3379197 -4.3407 -4.3398156 -4.3343968 -4.3287992 -4.3316507 -4.3352866]]...]
INFO - root - 2017-12-08 00:32:21.642894: step 56610, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 71h:06m:56s remains)
INFO - root - 2017-12-08 00:32:31.358076: step 56620, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 74h:51m:19s remains)
INFO - root - 2017-12-08 00:32:40.984900: step 56630, loss = 2.05, batch loss = 2.00 (8.2 examples/sec; 0.971 sec/batch; 74h:25m:47s remains)
INFO - root - 2017-12-08 00:32:50.700382: step 56640, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.989 sec/batch; 75h:48m:03s remains)
INFO - root - 2017-12-08 00:33:00.278499: step 56650, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.981 sec/batch; 75h:10m:47s remains)
INFO - root - 2017-12-08 00:33:09.743039: step 56660, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 74h:03m:19s remains)
INFO - root - 2017-12-08 00:33:19.235114: step 56670, loss = 2.10, batch loss = 2.05 (9.2 examples/sec; 0.867 sec/batch; 66h:25m:51s remains)
INFO - root - 2017-12-08 00:33:28.783886: step 56680, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.958 sec/batch; 73h:23m:29s remains)
INFO - root - 2017-12-08 00:33:38.588289: step 56690, loss = 2.07, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 77h:45m:02s remains)
INFO - root - 2017-12-08 00:33:48.289446: step 56700, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 75h:41m:29s remains)
2017-12-08 00:33:49.255330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1640925 -4.1525803 -4.1561995 -4.1724539 -4.1869559 -4.1927662 -4.1909857 -4.1849818 -4.1790943 -4.184134 -4.194201 -4.2020483 -4.2012668 -4.2017331 -4.2042866][-4.1519055 -4.1348238 -4.1296005 -4.1461167 -4.1596665 -4.1646342 -4.1625667 -4.158535 -4.1536279 -4.1608467 -4.1707821 -4.1732879 -4.1705418 -4.1727781 -4.1767597][-4.184032 -4.1658778 -4.151732 -4.1577506 -4.1614327 -4.1591291 -4.1513124 -4.1447816 -4.141871 -4.1519451 -4.16119 -4.1551213 -4.1461692 -4.1446071 -4.1434207][-4.2075009 -4.1980171 -4.1874719 -4.1844635 -4.1775026 -4.1655674 -4.1491132 -4.1409426 -4.1453018 -4.164319 -4.1778769 -4.1698151 -4.1599622 -4.1538234 -4.1423597][-4.1802111 -4.1830111 -4.1806364 -4.173912 -4.1579113 -4.1323962 -4.1018715 -4.092382 -4.1119318 -4.1461983 -4.1719561 -4.1719337 -4.1699052 -4.1646447 -4.1492696][-4.1032319 -4.1139379 -4.1135888 -4.10257 -4.075943 -4.0278416 -3.9719186 -3.9543335 -3.9983261 -4.0624018 -4.1111493 -4.1285534 -4.1399016 -4.1399274 -4.1291308][-4.0075426 -4.0248446 -4.0258236 -4.016016 -3.9866192 -3.9234836 -3.8415861 -3.807426 -3.8635259 -3.9501772 -4.0172982 -4.0561776 -4.0829992 -4.0885649 -4.0887356][-3.9779763 -4.0009189 -4.0067739 -4.005568 -3.9871218 -3.9378762 -3.8664784 -3.8267019 -3.8594313 -3.9211521 -3.9737515 -4.0143847 -4.0463052 -4.054287 -4.0579691][-4.0530167 -4.0698848 -4.0753722 -4.08173 -4.0809193 -4.0593042 -4.0208855 -3.9906273 -3.9915447 -4.0103436 -4.0328426 -4.0600476 -4.082716 -4.0838447 -4.0858178][-4.1639547 -4.1735106 -4.1756964 -4.1815858 -4.1875463 -4.18219 -4.1662741 -4.1450157 -4.1308222 -4.1275558 -4.1327391 -4.1474948 -4.1600523 -4.1565094 -4.1541643][-4.249218 -4.2574005 -4.2574005 -4.2607722 -4.2639971 -4.2605848 -4.2507272 -4.2354503 -4.220643 -4.2110033 -4.2099738 -4.2194443 -4.2247796 -4.2170792 -4.2098165][-4.294271 -4.3011718 -4.3008747 -4.3023639 -4.301754 -4.298274 -4.2931871 -4.2823939 -4.2684307 -4.2548752 -4.2481084 -4.2501836 -4.2468872 -4.2332826 -4.2238][-4.3074412 -4.3126884 -4.3147988 -4.31641 -4.3144369 -4.3107843 -4.3069863 -4.2973094 -4.2814312 -4.2620378 -4.2486506 -4.2430749 -4.2310691 -4.21159 -4.2047729][-4.3048997 -4.309814 -4.3143067 -4.3183122 -4.3181739 -4.3158007 -4.3111129 -4.2978649 -4.2757821 -4.2484837 -4.2270055 -4.2155991 -4.1995058 -4.1822376 -4.1833882][-4.29537 -4.3012896 -4.3073878 -4.3144264 -4.317759 -4.318676 -4.3153028 -4.3017092 -4.2798281 -4.2538381 -4.2314115 -4.2171345 -4.2006259 -4.1869125 -4.1896367]]...]
INFO - root - 2017-12-08 00:33:59.008706: step 56710, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 70h:43m:57s remains)
INFO - root - 2017-12-08 00:34:08.589619: step 56720, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 71h:21m:50s remains)
INFO - root - 2017-12-08 00:34:18.217415: step 56730, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.960 sec/batch; 73h:33m:28s remains)
INFO - root - 2017-12-08 00:34:27.699619: step 56740, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 68h:42m:47s remains)
INFO - root - 2017-12-08 00:34:37.307927: step 56750, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 73h:38m:39s remains)
INFO - root - 2017-12-08 00:34:46.850368: step 56760, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 72h:30m:01s remains)
INFO - root - 2017-12-08 00:34:56.512062: step 56770, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 73h:37m:11s remains)
INFO - root - 2017-12-08 00:35:06.207705: step 56780, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 72h:43m:53s remains)
INFO - root - 2017-12-08 00:35:16.004957: step 56790, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.012 sec/batch; 77h:30m:57s remains)
INFO - root - 2017-12-08 00:35:25.721961: step 56800, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 72h:01m:16s remains)
2017-12-08 00:35:26.724314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2887917 -4.2833767 -4.2843814 -4.2831984 -4.274292 -4.258419 -4.2509332 -4.253159 -4.26386 -4.273757 -4.2729349 -4.2629776 -4.2526822 -4.2497582 -4.2553253][-4.2723184 -4.2666035 -4.2671957 -4.2627907 -4.2410545 -4.2115612 -4.2018833 -4.2118649 -4.2325759 -4.2476273 -4.2458291 -4.2334352 -4.2198758 -4.2170153 -4.2246938][-4.2622552 -4.2511497 -4.2509189 -4.2437282 -4.2128491 -4.1708074 -4.1587343 -4.1800609 -4.2148743 -4.2357159 -4.2317214 -4.21574 -4.198523 -4.193428 -4.2035189][-4.2598963 -4.2397871 -4.2404203 -4.2312932 -4.193748 -4.1427145 -4.1277041 -4.1583457 -4.2054968 -4.2280107 -4.2176976 -4.1986976 -4.1788568 -4.1728435 -4.1808009][-4.2539639 -4.2271609 -4.2292776 -4.2226782 -4.1797261 -4.1239991 -4.1057019 -4.1372805 -4.1907277 -4.2117529 -4.1993408 -4.1828966 -4.1615686 -4.1552758 -4.1592965][-4.2426515 -4.2092257 -4.2092714 -4.2027626 -4.1576548 -4.10236 -4.0773511 -4.1012182 -4.15166 -4.1684403 -4.15878 -4.1487336 -4.1313758 -4.1263266 -4.1301651][-4.223033 -4.185266 -4.1858225 -4.1814566 -4.1323514 -4.0743618 -4.0435462 -4.0581255 -4.1015873 -4.1162438 -4.1124129 -4.1076322 -4.0987225 -4.0966234 -4.1007996][-4.2117271 -4.1722603 -4.1753173 -4.178031 -4.1278076 -4.0648603 -4.027431 -4.028595 -4.0598917 -4.0748024 -4.0736494 -4.0723205 -4.0696917 -4.0687122 -4.0713048][-4.2207508 -4.1827979 -4.1840158 -4.1882782 -4.1436987 -4.0841708 -4.0479331 -4.035347 -4.0473409 -4.0558 -4.0535 -4.0548611 -4.0604177 -4.0648279 -4.0690384][-4.2297831 -4.19489 -4.1909461 -4.1925645 -4.15553 -4.1128192 -4.090169 -4.07463 -4.0715261 -4.0695443 -4.0656304 -4.0680771 -4.0791874 -4.093545 -4.1043634][-4.2325678 -4.2019477 -4.1958642 -4.1946764 -4.1662245 -4.1466289 -4.143054 -4.1324453 -4.118609 -4.1054425 -4.0940695 -4.0932636 -4.1057134 -4.1288681 -4.1513777][-4.2310786 -4.2071385 -4.2024283 -4.2027678 -4.1836715 -4.1809616 -4.189672 -4.1846871 -4.1650114 -4.1446824 -4.129601 -4.1268563 -4.1433611 -4.1718855 -4.1986971][-4.236681 -4.2207136 -4.2240772 -4.2271957 -4.212616 -4.2148027 -4.2282462 -4.2316394 -4.2174258 -4.1970735 -4.1830339 -4.1807179 -4.197278 -4.2231445 -4.2455306][-4.2517095 -4.23893 -4.2486115 -4.2557764 -4.2458358 -4.2465925 -4.2580886 -4.2664013 -4.2602286 -4.2429314 -4.2311482 -4.2298098 -4.24241 -4.2623167 -4.278522][-4.2760921 -4.263999 -4.2741828 -4.2824593 -4.2774363 -4.2776566 -4.2830987 -4.290009 -4.2862554 -4.2718787 -4.26283 -4.2634459 -4.273797 -4.2925711 -4.3068285]]...]
INFO - root - 2017-12-08 00:35:36.442551: step 56810, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 76h:43m:20s remains)
INFO - root - 2017-12-08 00:35:46.057704: step 56820, loss = 2.04, batch loss = 1.98 (7.8 examples/sec; 1.032 sec/batch; 79h:02m:05s remains)
INFO - root - 2017-12-08 00:35:55.689275: step 56830, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 72h:16m:53s remains)
INFO - root - 2017-12-08 00:36:05.317217: step 56840, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.954 sec/batch; 73h:03m:05s remains)
INFO - root - 2017-12-08 00:36:14.949615: step 56850, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 73h:33m:11s remains)
INFO - root - 2017-12-08 00:36:24.632290: step 56860, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 71h:48m:15s remains)
INFO - root - 2017-12-08 00:36:34.414036: step 56870, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 75h:24m:52s remains)
INFO - root - 2017-12-08 00:36:43.813857: step 56880, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 70h:37m:09s remains)
INFO - root - 2017-12-08 00:36:53.358988: step 56890, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 70h:38m:19s remains)
INFO - root - 2017-12-08 00:37:03.002867: step 56900, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 71h:05m:21s remains)
2017-12-08 00:37:03.997822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3278284 -4.307025 -4.2942915 -4.29973 -4.3134327 -4.3181882 -4.3098373 -4.2908568 -4.2810049 -4.2851911 -4.3009624 -4.3205738 -4.3375678 -4.3450522 -4.348434][-4.3263364 -4.307404 -4.2975459 -4.3015065 -4.3091512 -4.3027649 -4.2816186 -4.2540393 -4.24372 -4.2539315 -4.2772045 -4.3035436 -4.3251767 -4.3359737 -4.3426185][-4.32552 -4.3099623 -4.301158 -4.2997103 -4.2949538 -4.2733741 -4.238483 -4.2029014 -4.1932611 -4.212472 -4.2451887 -4.2788382 -4.3066125 -4.3231325 -4.3339005][-4.3142948 -4.3057756 -4.2975082 -4.2907968 -4.2739568 -4.2352991 -4.1845083 -4.1415915 -4.137259 -4.16933 -4.2139745 -4.256958 -4.2921128 -4.3132997 -4.3266969][-4.2805166 -4.2798586 -4.2748952 -4.2681012 -4.2439828 -4.1941075 -4.1301589 -4.0794754 -4.0805535 -4.1260371 -4.1824183 -4.2359881 -4.2800345 -4.3058581 -4.3205905][-4.2281752 -4.2354164 -4.2378607 -4.2324715 -4.2035575 -4.1433334 -4.066721 -4.0112739 -4.0222292 -4.0864739 -4.1566105 -4.2204404 -4.2709646 -4.2998505 -4.3140421][-4.1717567 -4.1893992 -4.202373 -4.1989388 -4.1624436 -4.0888882 -3.9967628 -3.9383366 -3.9663727 -4.0578618 -4.1472859 -4.2211361 -4.2738242 -4.3011432 -4.3117232][-4.13316 -4.1661916 -4.1884584 -4.18658 -4.1418114 -4.0558205 -3.9496391 -3.8918595 -3.9379659 -4.0497909 -4.1542563 -4.2372413 -4.2919364 -4.3150268 -4.3193569][-4.1399255 -4.1773906 -4.2004371 -4.1964655 -4.1521635 -4.0714154 -3.9772778 -3.932759 -3.979619 -4.081892 -4.178483 -4.2587996 -4.31201 -4.3293719 -4.3288522][-4.1834598 -4.2104197 -4.2244315 -4.216382 -4.1796851 -4.1214976 -4.055613 -4.0217276 -4.0539403 -4.1302457 -4.2058549 -4.275311 -4.3208437 -4.3347411 -4.3337073][-4.2177806 -4.2283096 -4.2278457 -4.2170782 -4.1933403 -4.1554289 -4.1068506 -4.0736847 -4.08786 -4.1414275 -4.2009969 -4.2618461 -4.3049107 -4.3239207 -4.3296428][-4.2207375 -4.2183828 -4.2101083 -4.1993217 -4.1876559 -4.1659927 -4.1297574 -4.0963979 -4.1001663 -4.1404634 -4.1928391 -4.2460217 -4.2861133 -4.308814 -4.321651][-4.2090178 -4.1988268 -4.1877146 -4.1801224 -4.1784029 -4.1693234 -4.1406307 -4.1092119 -4.1113257 -4.1484203 -4.1972227 -4.2441306 -4.2807732 -4.3042712 -4.3188024][-4.2180305 -4.2060032 -4.197145 -4.1941533 -4.196876 -4.1933846 -4.1695313 -4.14167 -4.146296 -4.1825843 -4.2271905 -4.2680497 -4.2995033 -4.3183188 -4.3279572][-4.2474208 -4.2385416 -4.2350454 -4.2351928 -4.2382455 -4.237124 -4.2215762 -4.2037854 -4.2104926 -4.2406864 -4.2753987 -4.3061538 -4.3279839 -4.3390126 -4.3417635]]...]
INFO - root - 2017-12-08 00:37:13.709275: step 56910, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.941 sec/batch; 72h:02m:54s remains)
INFO - root - 2017-12-08 00:37:23.442119: step 56920, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.993 sec/batch; 76h:01m:55s remains)
INFO - root - 2017-12-08 00:37:33.024007: step 56930, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.969 sec/batch; 74h:09m:58s remains)
INFO - root - 2017-12-08 00:37:42.928853: step 56940, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 75h:34m:15s remains)
INFO - root - 2017-12-08 00:37:52.372988: step 56950, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 72h:04m:04s remains)
INFO - root - 2017-12-08 00:38:01.985876: step 56960, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 69h:20m:02s remains)
INFO - root - 2017-12-08 00:38:11.669016: step 56970, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 73h:55m:39s remains)
INFO - root - 2017-12-08 00:38:21.235224: step 56980, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 73h:34m:39s remains)
INFO - root - 2017-12-08 00:38:31.071381: step 56990, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 72h:55m:24s remains)
INFO - root - 2017-12-08 00:38:40.767368: step 57000, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 72h:32m:59s remains)
2017-12-08 00:38:41.667829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2889748 -4.302258 -4.2956929 -4.2836432 -4.2759328 -4.2657309 -4.2560272 -4.258028 -4.2710261 -4.2885318 -4.3009357 -4.3087306 -4.3139482 -4.3243713 -4.3436418][-4.2666917 -4.2996082 -4.3070579 -4.2974162 -4.2732987 -4.2326837 -4.1994686 -4.1991057 -4.2233653 -4.2574968 -4.2832623 -4.301795 -4.3151741 -4.3289375 -4.349596][-4.25447 -4.297688 -4.3096404 -4.2950296 -4.2491159 -4.1742768 -4.1183119 -4.1219497 -4.165122 -4.2205405 -4.2626281 -4.2920895 -4.3124037 -4.326859 -4.3475928][-4.2525082 -4.299602 -4.3112206 -4.2860165 -4.2123632 -4.101553 -4.0258756 -4.0388203 -4.1052108 -4.1830435 -4.2427387 -4.2822194 -4.3075457 -4.321878 -4.3417583][-4.2570238 -4.3044415 -4.3145761 -4.2779765 -4.1768126 -4.0304193 -3.9342976 -3.9613194 -4.0571504 -4.156456 -4.2301626 -4.2750258 -4.3013787 -4.3158693 -4.3341889][-4.2625556 -4.3079062 -4.3164272 -4.27101 -4.1507897 -3.9756742 -3.8567004 -3.8973885 -4.0226092 -4.140624 -4.2237997 -4.27004 -4.2942185 -4.3093858 -4.327817][-4.2602444 -4.30417 -4.3131914 -4.2637076 -4.1313109 -3.9367635 -3.7939939 -3.8420439 -3.9934382 -4.1278629 -4.2178397 -4.26457 -4.2856646 -4.2999821 -4.3187289][-4.2545509 -4.2960348 -4.3032537 -4.2531519 -4.1211524 -3.9243922 -3.7689128 -3.8193371 -3.9856262 -4.1272922 -4.2158523 -4.26152 -4.2810025 -4.2938609 -4.3121619][-4.2506356 -4.28797 -4.2930436 -4.249577 -4.1327047 -3.9593141 -3.8217456 -3.8712893 -4.0281878 -4.1521425 -4.223628 -4.2634721 -4.2816114 -4.2925363 -4.3105145][-4.2530584 -4.2818465 -4.2861023 -4.2524123 -4.1625443 -4.0299835 -3.93046 -3.975543 -4.1018229 -4.1927075 -4.2388391 -4.2689071 -4.2851362 -4.2947965 -4.3124495][-4.2509942 -4.2695117 -4.2760367 -4.2568851 -4.1984415 -4.1059203 -4.0421591 -4.0796738 -4.1737375 -4.2332625 -4.256031 -4.2725191 -4.2812877 -4.2886553 -4.3061666][-4.2389374 -4.2479987 -4.2567172 -4.2559152 -4.2324185 -4.1785884 -4.1414733 -4.1657181 -4.2275181 -4.2598176 -4.2646112 -4.2664332 -4.2667985 -4.274559 -4.2947283][-4.2267146 -4.2187524 -4.222609 -4.2375717 -4.2444849 -4.2271109 -4.2115412 -4.2261782 -4.2620211 -4.2729292 -4.2653723 -4.2545967 -4.2500286 -4.2610612 -4.286283][-4.227026 -4.1975741 -4.1869535 -4.2088323 -4.2352772 -4.2423739 -4.2430186 -4.2531934 -4.2709951 -4.2710838 -4.258585 -4.2439537 -4.2394366 -4.2565789 -4.2875113][-4.2415533 -4.19813 -4.1721859 -4.1896415 -4.2206907 -4.2377324 -4.244 -4.2491374 -4.255991 -4.2528219 -4.242487 -4.2321725 -4.2330232 -4.2570672 -4.2939048]]...]
INFO - root - 2017-12-08 00:38:51.269330: step 57010, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.003 sec/batch; 76h:45m:38s remains)
INFO - root - 2017-12-08 00:39:00.918262: step 57020, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 70h:38m:20s remains)
INFO - root - 2017-12-08 00:39:10.622448: step 57030, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 69h:43m:23s remains)
INFO - root - 2017-12-08 00:39:20.189088: step 57040, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 74h:57m:23s remains)
INFO - root - 2017-12-08 00:39:29.477078: step 57050, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 65h:25m:44s remains)
INFO - root - 2017-12-08 00:39:39.284071: step 57060, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.000 sec/batch; 76h:29m:51s remains)
INFO - root - 2017-12-08 00:39:48.840544: step 57070, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 69h:16m:00s remains)
INFO - root - 2017-12-08 00:39:58.398566: step 57080, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.900 sec/batch; 68h:50m:05s remains)
INFO - root - 2017-12-08 00:40:08.202052: step 57090, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.994 sec/batch; 76h:01m:20s remains)
INFO - root - 2017-12-08 00:40:17.745141: step 57100, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 74h:01m:09s remains)
2017-12-08 00:40:18.871301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1090426 -4.1076946 -4.0972795 -4.0913134 -4.1101017 -4.1434851 -4.1788397 -4.2152114 -4.2377915 -4.2370529 -4.2247462 -4.2109966 -4.2018118 -4.201581 -4.2154374][-4.1146388 -4.1028466 -4.07759 -4.0541916 -4.0639172 -4.0968261 -4.1370783 -4.1823759 -4.2184095 -4.2325482 -4.2341213 -4.2329831 -4.2302489 -4.2301126 -4.2392554][-4.1375861 -4.1222796 -4.0832262 -4.0382686 -4.0234365 -4.0363092 -4.066977 -4.108933 -4.1497679 -4.1727214 -4.1892228 -4.2057042 -4.22027 -4.2334423 -4.2489924][-4.1482978 -4.144402 -4.1104012 -4.0607114 -4.0271206 -4.0145068 -4.0187364 -4.0316291 -4.0536804 -4.0721641 -4.0991459 -4.1326628 -4.1697187 -4.2036829 -4.2357616][-4.1342368 -4.13807 -4.1167326 -4.0812116 -4.0534587 -4.0360875 -4.0185895 -3.9937868 -3.9822862 -3.9842632 -4.0080185 -4.04803 -4.10041 -4.1531849 -4.2033634][-4.1136069 -4.1112137 -4.0964417 -4.0796227 -4.0713658 -4.0668745 -4.047648 -4.0033269 -3.9637554 -3.94438 -3.9535718 -3.9844153 -4.0360804 -4.097414 -4.1601648][-4.1149893 -4.1043963 -4.0892606 -4.0824728 -4.0867815 -4.0926018 -4.0779767 -4.0352259 -3.9898832 -3.9595239 -3.9535627 -3.9670398 -4.003746 -4.0589013 -4.1243439][-4.1467628 -4.1300521 -4.1126242 -4.106504 -4.114264 -4.1221528 -4.1126609 -4.0841632 -4.048244 -4.0218 -4.0114965 -4.0122375 -4.0306087 -4.0688992 -4.1234593][-4.1980195 -4.1778684 -4.1577911 -4.1490049 -4.1553106 -4.1631384 -4.1587119 -4.1448073 -4.12065 -4.1053429 -4.1009579 -4.1002526 -4.1066551 -4.12779 -4.1637673][-4.2609992 -4.2414904 -4.221262 -4.2110066 -4.2145038 -4.2202716 -4.2192626 -4.2166276 -4.2038765 -4.1969147 -4.1962876 -4.1953945 -4.1960082 -4.2042789 -4.2218103][-4.317565 -4.3036494 -4.2868781 -4.2760787 -4.2761993 -4.278748 -4.2783504 -4.2804141 -4.277338 -4.2762489 -4.2774653 -4.2762809 -4.2731047 -4.2721105 -4.275578][-4.3451443 -4.3375015 -4.32744 -4.3183808 -4.316009 -4.3158402 -4.3148971 -4.3177991 -4.3209047 -4.3244305 -4.3262539 -4.3238873 -4.3193245 -4.3154364 -4.31382][-4.3478656 -4.343606 -4.338182 -4.334084 -4.3336 -4.3341503 -4.3342609 -4.3357916 -4.3381548 -4.3409281 -4.3422561 -4.3408074 -4.3373537 -4.3340316 -4.3323092][-4.3410249 -4.3387513 -4.3362584 -4.33418 -4.3343043 -4.33512 -4.3359489 -4.3366847 -4.3368239 -4.3372545 -4.3379178 -4.3386788 -4.339169 -4.3395414 -4.3410158][-4.3405948 -4.3393397 -4.3373494 -4.3354797 -4.3343878 -4.33417 -4.3349161 -4.3363295 -4.3377457 -4.339016 -4.3407359 -4.3431377 -4.3458533 -4.348732 -4.3521509]]...]
INFO - root - 2017-12-08 00:40:28.531962: step 57110, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 1.006 sec/batch; 76h:57m:26s remains)
INFO - root - 2017-12-08 00:40:38.328795: step 57120, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.018 sec/batch; 77h:52m:41s remains)
INFO - root - 2017-12-08 00:40:47.962907: step 57130, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.979 sec/batch; 74h:51m:10s remains)
INFO - root - 2017-12-08 00:40:57.512457: step 57140, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 66h:45m:08s remains)
INFO - root - 2017-12-08 00:41:06.977112: step 57150, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 70h:00m:07s remains)
INFO - root - 2017-12-08 00:41:16.510377: step 57160, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 72h:00m:00s remains)
INFO - root - 2017-12-08 00:41:26.079621: step 57170, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 74h:35m:57s remains)
INFO - root - 2017-12-08 00:41:35.832421: step 57180, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 74h:05m:20s remains)
INFO - root - 2017-12-08 00:41:45.476691: step 57190, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 72h:08m:51s remains)
INFO - root - 2017-12-08 00:41:55.144375: step 57200, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 72h:59m:11s remains)
2017-12-08 00:41:56.097651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1485472 -4.1800485 -4.2144365 -4.22468 -4.2159991 -4.2063866 -4.2113452 -4.2164822 -4.2118163 -4.2050323 -4.1994 -4.2066832 -4.2221074 -4.2350168 -4.2401237][-4.1617866 -4.2015433 -4.2374334 -4.2480106 -4.2373352 -4.2270727 -4.2317104 -4.2416635 -4.2393608 -4.2285776 -4.2183704 -4.2200961 -4.2281671 -4.2339244 -4.2325544][-4.179903 -4.2245579 -4.2631412 -4.27979 -4.2703209 -4.2530222 -4.2516379 -4.2619033 -4.2675886 -4.2579775 -4.2430286 -4.2393713 -4.2409492 -4.2418571 -4.2387085][-4.1696987 -4.2171273 -4.2549324 -4.2771182 -4.2688313 -4.2464767 -4.2410965 -4.2536087 -4.2731414 -4.2729616 -4.2576852 -4.2465105 -4.2407975 -4.2397141 -4.2358494][-4.1491241 -4.1903844 -4.217051 -4.2292466 -4.2129993 -4.1813455 -4.1763697 -4.1987467 -4.2378621 -4.2612247 -4.2547288 -4.2402968 -4.2289495 -4.2278 -4.2248368][-4.1660376 -4.192543 -4.1958027 -4.1759858 -4.1281629 -4.068934 -4.0538216 -4.0879383 -4.1513586 -4.2048211 -4.2215538 -4.220396 -4.2164931 -4.2193646 -4.2173533][-4.2140465 -4.2252607 -4.1999569 -4.1451988 -4.0606503 -3.9628234 -3.9211597 -3.9527504 -4.0333786 -4.1209483 -4.1647964 -4.1827226 -4.1968818 -4.213501 -4.2189159][-4.2515125 -4.2538519 -4.2128339 -4.140049 -4.0419488 -3.9211478 -3.8494036 -3.8635559 -3.9473932 -4.0540624 -4.1130676 -4.1474943 -4.178823 -4.2061338 -4.2218218][-4.2637134 -4.2596135 -4.2185354 -4.1572652 -4.0860291 -3.9916854 -3.9225454 -3.9187262 -3.9844913 -4.0752478 -4.126977 -4.1620283 -4.1942635 -4.2185225 -4.2333775][-4.2802162 -4.2712789 -4.2376995 -4.1997423 -4.1646171 -4.1148672 -4.0696139 -4.060369 -4.1020107 -4.1603985 -4.1923122 -4.2127347 -4.2329655 -4.2505269 -4.2603927][-4.2967405 -4.2825341 -4.2578759 -4.2378993 -4.2284331 -4.2134824 -4.1943393 -4.1879387 -4.2106209 -4.2390327 -4.2512336 -4.2586532 -4.2696037 -4.2824178 -4.2905445][-4.3102417 -4.2970695 -4.2826796 -4.2729545 -4.2738795 -4.2752886 -4.270884 -4.2704444 -4.2815757 -4.292942 -4.2958088 -4.2953491 -4.2983022 -4.3045321 -4.308476][-4.3207045 -4.309082 -4.3010049 -4.297133 -4.2994518 -4.3037186 -4.30415 -4.3065343 -4.312428 -4.3167915 -4.316494 -4.3115015 -4.3087821 -4.3098416 -4.3114347][-4.3307629 -4.3193336 -4.3088269 -4.3020473 -4.2991109 -4.2995872 -4.3006749 -4.3050523 -4.3104534 -4.3142276 -4.3163157 -4.3137355 -4.3105111 -4.3104787 -4.31227][-4.3378735 -4.3310871 -4.3208208 -4.3099623 -4.302197 -4.299098 -4.2995443 -4.3042359 -4.3090668 -4.3126469 -4.3157711 -4.3167939 -4.3157735 -4.3161106 -4.317915]]...]
INFO - root - 2017-12-08 00:42:05.762059: step 57210, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.961 sec/batch; 73h:27m:28s remains)
INFO - root - 2017-12-08 00:42:15.355952: step 57220, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 72h:20m:47s remains)
INFO - root - 2017-12-08 00:42:24.994047: step 57230, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 76h:31m:43s remains)
INFO - root - 2017-12-08 00:42:34.481416: step 57240, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.987 sec/batch; 75h:29m:41s remains)
INFO - root - 2017-12-08 00:42:44.037923: step 57250, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 73h:44m:19s remains)
INFO - root - 2017-12-08 00:42:53.838599: step 57260, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 74h:55m:12s remains)
INFO - root - 2017-12-08 00:43:03.548574: step 57270, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 75h:15m:33s remains)
INFO - root - 2017-12-08 00:43:13.175286: step 57280, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 73h:55m:13s remains)
INFO - root - 2017-12-08 00:43:22.941027: step 57290, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.987 sec/batch; 75h:28m:31s remains)
INFO - root - 2017-12-08 00:43:32.596852: step 57300, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 72h:32m:23s remains)
2017-12-08 00:43:33.567322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2908826 -4.2731104 -4.2644224 -4.2604542 -4.2554917 -4.2497497 -4.2494097 -4.2515454 -4.2564526 -4.2586937 -4.2598004 -4.2561817 -4.2531953 -4.2492375 -4.2424583][-4.2826409 -4.261292 -4.2445 -4.23275 -4.2219973 -4.2151508 -4.2148 -4.2175808 -4.223906 -4.2296939 -4.2344055 -4.2258682 -4.2146487 -4.2079372 -4.2035885][-4.2590351 -4.2397146 -4.2205553 -4.2015429 -4.181005 -4.1677451 -4.1669297 -4.1730032 -4.1849589 -4.1966085 -4.2037826 -4.1935334 -4.17354 -4.1600003 -4.1607533][-4.2293777 -4.2133603 -4.19691 -4.17597 -4.14415 -4.1173973 -4.1109281 -4.1246672 -4.1483297 -4.1677909 -4.1788898 -4.1685882 -4.140882 -4.1187477 -4.1211982][-4.2183208 -4.1997466 -4.1854348 -4.1648974 -4.1266813 -4.0865612 -4.0709457 -4.0846691 -4.1185737 -4.1463141 -4.1684713 -4.1720443 -4.14708 -4.1151867 -4.1088209][-4.2139935 -4.1914134 -4.1762118 -4.1512136 -4.1152983 -4.0628638 -4.0191197 -4.0187173 -4.0714183 -4.1190128 -4.1584387 -4.1745968 -4.1569338 -4.1237984 -4.1122389][-4.2061615 -4.1827688 -4.1671119 -4.1355963 -4.0914826 -4.0175118 -3.9264607 -3.897187 -3.9789348 -4.0672293 -4.129611 -4.1611347 -4.151721 -4.1190906 -4.1092629][-4.22506 -4.2009706 -4.1797204 -4.1362176 -4.0781221 -3.9860907 -3.8546362 -3.7963562 -3.9011052 -4.0268011 -4.1081367 -4.1433129 -4.1337519 -4.0974979 -4.084034][-4.2437792 -4.2252975 -4.2004862 -4.1529446 -4.1005564 -4.0371823 -3.9506629 -3.8984492 -3.9519565 -4.0482168 -4.1195025 -4.1462255 -4.1287 -4.0879416 -4.0656776][-4.2407241 -4.2317452 -4.211226 -4.1682863 -4.1288252 -4.1025143 -4.0722566 -4.0428953 -4.0545163 -4.1115232 -4.1664143 -4.1830373 -4.1632891 -4.1315351 -4.1138563][-4.2419004 -4.2385645 -4.2248931 -4.1907411 -4.159111 -4.1487012 -4.1468577 -4.1396542 -4.1439514 -4.1759911 -4.2155695 -4.2254915 -4.2045369 -4.1818256 -4.1747012][-4.2386932 -4.2419395 -4.2402291 -4.221509 -4.200017 -4.1923914 -4.1993985 -4.2050929 -4.2107344 -4.2269659 -4.2498512 -4.2545786 -4.2301083 -4.2059021 -4.2045074][-4.2445436 -4.2517285 -4.2627134 -4.2553792 -4.2376542 -4.2242084 -4.2288432 -4.2404132 -4.2445674 -4.2520728 -4.2659984 -4.2696586 -4.2483144 -4.2301955 -4.2338][-4.2608562 -4.2678151 -4.2805381 -4.2763743 -4.2638516 -4.25114 -4.2511091 -4.2599878 -4.2625847 -4.2680168 -4.2791414 -4.2798429 -4.2660694 -4.2570543 -4.26429][-4.2778096 -4.2807913 -4.2873697 -4.2866673 -4.2807984 -4.2726474 -4.2715869 -4.276854 -4.2778268 -4.2819285 -4.2877541 -4.2853875 -4.275353 -4.271461 -4.275609]]...]
INFO - root - 2017-12-08 00:43:43.242546: step 57310, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 71h:41m:52s remains)
INFO - root - 2017-12-08 00:43:52.776957: step 57320, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 70h:42m:59s remains)
INFO - root - 2017-12-08 00:44:02.413981: step 57330, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 75h:23m:22s remains)
INFO - root - 2017-12-08 00:44:12.096640: step 57340, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 76h:35m:52s remains)
INFO - root - 2017-12-08 00:44:21.750882: step 57350, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.996 sec/batch; 76h:07m:33s remains)
INFO - root - 2017-12-08 00:44:31.373180: step 57360, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 75h:30m:56s remains)
INFO - root - 2017-12-08 00:44:40.930156: step 57370, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 71h:09m:07s remains)
INFO - root - 2017-12-08 00:44:50.691347: step 57380, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 71h:07m:04s remains)
INFO - root - 2017-12-08 00:45:00.265169: step 57390, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.007 sec/batch; 76h:59m:08s remains)
INFO - root - 2017-12-08 00:45:09.921680: step 57400, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 76h:28m:01s remains)
2017-12-08 00:45:10.882234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3341022 -4.3129468 -4.2919211 -4.2635555 -4.2090435 -4.12712 -4.0697489 -4.0690708 -4.0988522 -4.1261749 -4.1438951 -4.1555972 -4.1813297 -4.2244611 -4.2605619][-4.3322926 -4.3075809 -4.2850237 -4.2537093 -4.19664 -4.1116304 -4.0559468 -4.0557184 -4.0880628 -4.1220045 -4.1373329 -4.14305 -4.1683397 -4.2179689 -4.2607751][-4.3323121 -4.3073258 -4.2843628 -4.2531753 -4.19443 -4.1099505 -4.05394 -4.0542717 -4.0974493 -4.1458368 -4.162034 -4.1586471 -4.18028 -4.22816 -4.2631607][-4.3330417 -4.3096151 -4.2872467 -4.2576609 -4.1994987 -4.1153355 -4.0550737 -4.0559878 -4.1203442 -4.1920519 -4.2133336 -4.2026305 -4.2168994 -4.2545347 -4.276906][-4.3337059 -4.3118358 -4.2899795 -4.2615452 -4.2019191 -4.1105862 -4.0368109 -4.03445 -4.1178975 -4.2109241 -4.2440495 -4.2299781 -4.2373018 -4.2651391 -4.2791071][-4.3341432 -4.3134623 -4.2922306 -4.2638235 -4.1994376 -4.0943069 -3.9934173 -3.9734335 -4.0719228 -4.1834116 -4.2306366 -4.2258015 -4.2327981 -4.25696 -4.2690868][-4.3346972 -4.3145108 -4.2945337 -4.2671146 -4.1999469 -4.0811057 -3.9508853 -3.9003632 -4.0061088 -4.1348362 -4.2012572 -4.2124972 -4.2260489 -4.2488179 -4.2568026][-4.3350344 -4.3151131 -4.296813 -4.2725625 -4.208313 -4.08787 -3.9533002 -3.8926854 -3.9904232 -4.1133146 -4.1842389 -4.2067542 -4.2250438 -4.2487049 -4.2535582][-4.3355656 -4.3158803 -4.3005815 -4.2805524 -4.2236638 -4.1191044 -4.0141349 -3.9799609 -4.052681 -4.1385608 -4.1878304 -4.2082777 -4.23029 -4.2551732 -4.25856][-4.3360271 -4.3160586 -4.3036342 -4.28791 -4.238905 -4.1535373 -4.0821009 -4.0730238 -4.1275549 -4.1763825 -4.201107 -4.2122941 -4.2369781 -4.2636824 -4.2661138][-4.3365607 -4.3151622 -4.3027549 -4.2883773 -4.2440586 -4.1745892 -4.1259818 -4.1316762 -4.1764822 -4.2045541 -4.2124467 -4.2151332 -4.2379303 -4.2662187 -4.2686052][-4.3373089 -4.3146591 -4.2995687 -4.2822623 -4.2389245 -4.1801085 -4.1478138 -4.1613212 -4.1997333 -4.2151837 -4.2147355 -4.2122684 -4.232264 -4.2592726 -4.2615657][-4.3382568 -4.3169456 -4.3008685 -4.28023 -4.235672 -4.181314 -4.1554766 -4.1711078 -4.204669 -4.213707 -4.2073932 -4.2016931 -4.216135 -4.2400103 -4.2422509][-4.3395886 -4.3214254 -4.3076491 -4.2873597 -4.2427125 -4.1896744 -4.163413 -4.1731458 -4.1988993 -4.2037773 -4.1968136 -4.1926775 -4.2038054 -4.222527 -4.2224989][-4.340147 -4.3250151 -4.3143635 -4.2980251 -4.2589889 -4.2104411 -4.1828332 -4.1802926 -4.1963387 -4.1970191 -4.1885862 -4.186749 -4.1972785 -4.2125344 -4.2102261]]...]
INFO - root - 2017-12-08 00:45:20.634261: step 57410, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 75h:22m:22s remains)
INFO - root - 2017-12-08 00:45:30.352231: step 57420, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 73h:33m:46s remains)
INFO - root - 2017-12-08 00:45:40.024766: step 57430, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.000 sec/batch; 76h:23m:53s remains)
INFO - root - 2017-12-08 00:45:49.525916: step 57440, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 68h:38m:02s remains)
INFO - root - 2017-12-08 00:45:59.117736: step 57450, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.925 sec/batch; 70h:40m:49s remains)
INFO - root - 2017-12-08 00:46:08.778708: step 57460, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.992 sec/batch; 75h:47m:07s remains)
INFO - root - 2017-12-08 00:46:18.604380: step 57470, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.005 sec/batch; 76h:45m:32s remains)
INFO - root - 2017-12-08 00:46:28.203088: step 57480, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 73h:30m:29s remains)
INFO - root - 2017-12-08 00:46:37.775708: step 57490, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 72h:11m:02s remains)
INFO - root - 2017-12-08 00:46:47.370623: step 57500, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.992 sec/batch; 75h:48m:38s remains)
2017-12-08 00:46:48.357599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2611341 -4.2460079 -4.2384577 -4.2371411 -4.2372985 -4.2317119 -4.2161179 -4.2064404 -4.2133212 -4.2269516 -4.2352157 -4.2402439 -4.2486029 -4.2556868 -4.2579427][-4.2606573 -4.2494407 -4.242857 -4.2403197 -4.2383637 -4.2330761 -4.2194276 -4.2071948 -4.2101092 -4.2184353 -4.2210135 -4.2279396 -4.2405152 -4.2499495 -4.25193][-4.2634749 -4.2547731 -4.2487855 -4.242445 -4.2377238 -4.2339034 -4.2244344 -4.2142277 -4.2129807 -4.2155328 -4.2128687 -4.2191815 -4.2338538 -4.2428193 -4.239027][-4.2665396 -4.2588639 -4.2506418 -4.2384338 -4.232285 -4.2304258 -4.2254996 -4.2183032 -4.2151427 -4.2148824 -4.2122312 -4.2166109 -4.2293324 -4.2337675 -4.2213454][-4.27293 -4.2630463 -4.2497578 -4.2284222 -4.2158661 -4.215611 -4.2178011 -4.2143116 -4.2121034 -4.2149267 -4.2175069 -4.2230926 -4.2308722 -4.2266269 -4.20608][-4.2848167 -4.273376 -4.2561703 -4.2278337 -4.2034507 -4.1950574 -4.1967044 -4.1957183 -4.195303 -4.202642 -4.2111697 -4.219986 -4.2232656 -4.2101293 -4.1828279][-4.2896137 -4.2784939 -4.2599044 -4.2274594 -4.1936407 -4.1712084 -4.1599731 -4.1538897 -4.1531925 -4.1632395 -4.1784215 -4.1931858 -4.1969051 -4.1796517 -4.1497173][-4.278069 -4.2682877 -4.2509513 -4.21996 -4.181334 -4.1491733 -4.1248317 -4.1063342 -4.0970984 -4.1029115 -4.1224813 -4.1450753 -4.153089 -4.1355352 -4.1039367][-4.2480125 -4.2382216 -4.2231159 -4.1971374 -4.1597586 -4.120616 -4.0824075 -4.0468965 -4.0267477 -4.0272827 -4.0463338 -4.0783105 -4.0953431 -4.08016 -4.0442019][-4.2118163 -4.1991811 -4.1862378 -4.1648555 -4.1312003 -4.0876679 -4.0350184 -3.9843934 -3.9549439 -3.9505348 -3.9676039 -4.0048242 -4.0321236 -4.0242691 -3.9888663][-4.1890669 -4.1731095 -4.16013 -4.1421552 -4.1185932 -4.08391 -4.0352039 -3.9893267 -3.9674616 -3.9653366 -3.9781997 -4.0073795 -4.0342517 -4.0349665 -4.0112348][-4.2069154 -4.1892743 -4.1722584 -4.1533604 -4.1394157 -4.1226211 -4.092196 -4.0613294 -4.0497322 -4.0503726 -4.0568042 -4.0709858 -4.088098 -4.0935063 -4.0841312][-4.2560954 -4.2385759 -4.215477 -4.1922727 -4.1796012 -4.17568 -4.1653724 -4.1511226 -4.1452551 -4.1468024 -4.1503868 -4.1557841 -4.1637926 -4.1706438 -4.171329][-4.30104 -4.2870903 -4.2633996 -4.2384491 -4.2251654 -4.2271461 -4.2327003 -4.2318964 -4.2312841 -4.234334 -4.2379584 -4.2395978 -4.2416167 -4.2462292 -4.2494874][-4.3204684 -4.3112869 -4.2920742 -4.2705126 -4.2570529 -4.2598772 -4.2712932 -4.2784338 -4.2822332 -4.2855129 -4.2881932 -4.2887917 -4.2886367 -4.291059 -4.2938724]]...]
INFO - root - 2017-12-08 00:46:58.113805: step 57510, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 70h:46m:00s remains)
INFO - root - 2017-12-08 00:47:07.573863: step 57520, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 0.811 sec/batch; 61h:57m:18s remains)
INFO - root - 2017-12-08 00:47:17.170971: step 57530, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 71h:45m:32s remains)
INFO - root - 2017-12-08 00:47:26.751851: step 57540, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 71h:26m:18s remains)
INFO - root - 2017-12-08 00:47:36.506406: step 57550, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 74h:10m:24s remains)
INFO - root - 2017-12-08 00:47:45.930016: step 57560, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 74h:23m:40s remains)
INFO - root - 2017-12-08 00:47:55.572112: step 57570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 71h:18m:52s remains)
INFO - root - 2017-12-08 00:48:05.269012: step 57580, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 74h:52m:11s remains)
INFO - root - 2017-12-08 00:48:14.838111: step 57590, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.988 sec/batch; 75h:28m:50s remains)
INFO - root - 2017-12-08 00:48:24.548804: step 57600, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 70h:33m:29s remains)
2017-12-08 00:48:25.578568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.230782 -4.2309585 -4.22702 -4.2070875 -4.1910439 -4.1922035 -4.1857424 -4.1716523 -4.1623559 -4.1693206 -4.1986437 -4.2330503 -4.2514954 -4.2498426 -4.2293754][-4.2540321 -4.2530251 -4.2419271 -4.2144885 -4.1943083 -4.1913443 -4.1848674 -4.1702824 -4.1650877 -4.17803 -4.2076893 -4.237824 -4.2539015 -4.249979 -4.2256875][-4.274076 -4.26829 -4.2486086 -4.2152219 -4.1890574 -4.1763172 -4.1641841 -4.1495185 -4.1504045 -4.171782 -4.205584 -4.2357984 -4.2535233 -4.2485394 -4.2201543][-4.2796116 -4.2696757 -4.2452369 -4.2067838 -4.1724572 -4.1480694 -4.1284623 -4.1147041 -4.120913 -4.1525168 -4.1928844 -4.2252479 -4.2441764 -4.2381363 -4.2087522][-4.2766223 -4.2645926 -4.237844 -4.1962814 -4.1577106 -4.1255584 -4.0993781 -4.0858345 -4.0952215 -4.1355653 -4.1839495 -4.2159433 -4.2333555 -4.2270718 -4.1992083][-4.2671056 -4.253653 -4.2270255 -4.1887541 -4.153861 -4.119338 -4.0878315 -4.0710163 -4.0822058 -4.1304355 -4.1845422 -4.2147455 -4.2314115 -4.222918 -4.1958647][-4.258944 -4.2449036 -4.2192144 -4.1856232 -4.154501 -4.1158848 -4.0798578 -4.0593514 -4.0756779 -4.1359372 -4.1966419 -4.2279673 -4.2442875 -4.2324681 -4.2029405][-4.25786 -4.2417526 -4.2149644 -4.1851377 -4.1559782 -4.11308 -4.0709043 -4.0450807 -4.0670705 -4.1401277 -4.2072725 -4.2427554 -4.2590375 -4.2465544 -4.2170539][-4.2541723 -4.2349916 -4.2059026 -4.1798806 -4.1569767 -4.1141896 -4.0664811 -4.0322795 -4.0560369 -4.1371808 -4.2078552 -4.2481403 -4.2694383 -4.2617559 -4.2351756][-4.247273 -4.2254663 -4.195045 -4.1729679 -4.1571822 -4.1183329 -4.0634861 -4.0205097 -4.0474248 -4.1310711 -4.2000718 -4.2423205 -4.2688947 -4.2663784 -4.2423172][-4.2434344 -4.2199125 -4.1880932 -4.1678319 -4.1552491 -4.1177907 -4.0600338 -4.0182161 -4.0530882 -4.1321816 -4.191709 -4.229126 -4.2569036 -4.2592521 -4.2379851][-4.2430282 -4.2204022 -4.189682 -4.1702032 -4.1574655 -4.1213837 -4.0677261 -4.0376673 -4.0777349 -4.1429038 -4.1855426 -4.2155418 -4.241488 -4.2497668 -4.2330556][-4.244597 -4.227592 -4.20253 -4.1839967 -4.1686993 -4.1345506 -4.0880103 -4.07178 -4.1109381 -4.1569195 -4.179606 -4.2018652 -4.2285705 -4.24424 -4.2339888][-4.2463675 -4.2383828 -4.2220111 -4.2044425 -4.1862512 -4.1540256 -4.1169624 -4.1119857 -4.1415176 -4.1652718 -4.1699243 -4.1872411 -4.2181435 -4.2405248 -4.2362885][-4.2375131 -4.239069 -4.2309279 -4.2146211 -4.1935215 -4.1651592 -4.1417074 -4.1428361 -4.1575022 -4.1606541 -4.1524682 -4.1677113 -4.2041335 -4.2319674 -4.2311172]]...]
INFO - root - 2017-12-08 00:48:35.160630: step 57610, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 69h:11m:02s remains)
INFO - root - 2017-12-08 00:48:44.866504: step 57620, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.009 sec/batch; 77h:03m:14s remains)
INFO - root - 2017-12-08 00:48:54.491496: step 57630, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 72h:04m:32s remains)
INFO - root - 2017-12-08 00:49:04.113917: step 57640, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 72h:37m:51s remains)
INFO - root - 2017-12-08 00:49:13.777885: step 57650, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 74h:47m:48s remains)
INFO - root - 2017-12-08 00:49:23.706787: step 57660, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 77h:36m:53s remains)
INFO - root - 2017-12-08 00:49:33.511587: step 57670, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 73h:49m:13s remains)
INFO - root - 2017-12-08 00:49:43.184456: step 57680, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 76h:34m:50s remains)
INFO - root - 2017-12-08 00:49:52.955484: step 57690, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 73h:46m:26s remains)
INFO - root - 2017-12-08 00:50:02.720067: step 57700, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 73h:35m:09s remains)
2017-12-08 00:50:03.687629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2943811 -4.3004479 -4.3039074 -4.3059597 -4.3081856 -4.302485 -4.2840428 -4.2556219 -4.2281275 -4.2107911 -4.2089996 -4.2248807 -4.2441974 -4.2514319 -4.239151][-4.3240285 -4.325881 -4.3253551 -4.3226585 -4.3167009 -4.3016205 -4.272634 -4.2308183 -4.1927495 -4.1742721 -4.1779966 -4.1984677 -4.2238708 -4.2344441 -4.2194042][-4.32951 -4.3277287 -4.323822 -4.3163743 -4.2977233 -4.2675056 -4.2231164 -4.1643348 -4.1190085 -4.102962 -4.116816 -4.1506629 -4.1886125 -4.2017751 -4.1814742][-4.320466 -4.3152103 -4.3097353 -4.2979546 -4.2700953 -4.2258167 -4.16604 -4.09502 -4.0494323 -4.0465221 -4.0803676 -4.1301284 -4.1732516 -4.1785836 -4.1482081][-4.3079433 -4.2969937 -4.2926564 -4.2827926 -4.2559838 -4.2038703 -4.1333456 -4.0633273 -4.0327263 -4.051806 -4.0998855 -4.1519451 -4.1846805 -4.1760254 -4.135097][-4.2877994 -4.2730851 -4.2726183 -4.2690477 -4.2468762 -4.1944966 -4.121799 -4.064333 -4.0586071 -4.0939288 -4.1387095 -4.1791143 -4.19456 -4.1714282 -4.1234074][-4.2662129 -4.24811 -4.2514377 -4.2578931 -4.2431893 -4.1913881 -4.1197562 -4.0715456 -4.0798368 -4.1227169 -4.1636529 -4.1946416 -4.1944962 -4.1603723 -4.1112838][-4.2490039 -4.2321696 -4.2398381 -4.2557712 -4.242589 -4.1850581 -4.11106 -4.0642252 -4.0787854 -4.1303525 -4.1769013 -4.2024074 -4.1879292 -4.142375 -4.0973849][-4.2271433 -4.2217665 -4.241045 -4.2643018 -4.2487345 -4.1828332 -4.0987082 -4.0501943 -4.0715761 -4.135931 -4.1876545 -4.2090225 -4.1857567 -4.1365852 -4.098866][-4.21072 -4.2242193 -4.2554088 -4.2838593 -4.2656174 -4.1914515 -4.1010442 -4.0565343 -4.0864425 -4.1534882 -4.2028651 -4.2194777 -4.1933522 -4.1495752 -4.1177707][-4.2087889 -4.2359915 -4.2722154 -4.3009834 -4.2833819 -4.2095652 -4.1255884 -4.0921721 -4.1244106 -4.1821318 -4.2236819 -4.2351408 -4.209589 -4.1699762 -4.1387024][-4.2231669 -4.2510514 -4.28409 -4.3101015 -4.2989726 -4.2396755 -4.173708 -4.1489697 -4.1722436 -4.2159033 -4.2511077 -4.2584066 -4.2345543 -4.1991563 -4.1685452][-4.2521477 -4.2699676 -4.292634 -4.3163323 -4.3144779 -4.2770271 -4.2305737 -4.2067623 -4.2185478 -4.2505403 -4.2818503 -4.2898126 -4.2716208 -4.2434864 -4.2155485][-4.2910748 -4.2969608 -4.3078318 -4.3267179 -4.3314009 -4.3117228 -4.2791324 -4.25522 -4.2603579 -4.2843404 -4.31221 -4.3244467 -4.3150911 -4.2948737 -4.2699771][-4.3207521 -4.3212833 -4.3255529 -4.3387604 -4.3463855 -4.3379574 -4.3149085 -4.2930107 -4.2930017 -4.3093557 -4.3316607 -4.3461366 -4.3465748 -4.3334637 -4.3141484]]...]
INFO - root - 2017-12-08 00:50:13.298809: step 57710, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 77h:20m:52s remains)
INFO - root - 2017-12-08 00:50:22.959320: step 57720, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 72h:24m:43s remains)
INFO - root - 2017-12-08 00:50:32.319218: step 57730, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 69h:16m:05s remains)
INFO - root - 2017-12-08 00:50:42.072413: step 57740, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 76h:38m:45s remains)
INFO - root - 2017-12-08 00:50:51.680431: step 57750, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 71h:23m:57s remains)
INFO - root - 2017-12-08 00:51:01.514646: step 57760, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 73h:45m:49s remains)
INFO - root - 2017-12-08 00:51:11.130825: step 57770, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 68h:31m:08s remains)
INFO - root - 2017-12-08 00:51:20.819218: step 57780, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 70h:55m:50s remains)
INFO - root - 2017-12-08 00:51:30.536148: step 57790, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.008 sec/batch; 76h:57m:16s remains)
INFO - root - 2017-12-08 00:51:40.115300: step 57800, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.974 sec/batch; 74h:20m:55s remains)
2017-12-08 00:51:41.118526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1942577 -4.2088523 -4.2315016 -4.2471752 -4.2576389 -4.2625279 -4.2675939 -4.2685013 -4.2643929 -4.2524438 -4.2387252 -4.23034 -4.224299 -4.2219253 -4.2262149][-4.1957908 -4.2127709 -4.2365751 -4.2496815 -4.2557025 -4.2551126 -4.2611194 -4.27139 -4.2812028 -4.2793026 -4.26875 -4.2582512 -4.2474613 -4.24207 -4.2455912][-4.2036829 -4.2198291 -4.2430086 -4.2517576 -4.2473526 -4.2357006 -4.2367516 -4.2542343 -4.2781863 -4.2870245 -4.2801967 -4.2697296 -4.255188 -4.24833 -4.2507639][-4.2111111 -4.2241831 -4.2429724 -4.2449236 -4.22405 -4.1932154 -4.1846786 -4.2107725 -4.2522044 -4.273479 -4.2723675 -4.2640409 -4.2496281 -4.2423573 -4.2466335][-4.2155213 -4.2256813 -4.2357326 -4.2238941 -4.1813498 -4.1235991 -4.099194 -4.1326036 -4.1988168 -4.2433848 -4.2556887 -4.253293 -4.2416296 -4.235158 -4.2420168][-4.2160673 -4.2223096 -4.2206354 -4.1954594 -4.1329889 -4.0419273 -3.9842908 -4.0189462 -4.1153774 -4.1929345 -4.2267642 -4.2363052 -4.2303381 -4.2276793 -4.241395][-4.2116828 -4.2142529 -4.2003989 -4.1625156 -4.0829926 -3.9564126 -3.8503885 -3.8747587 -4.010911 -4.1329975 -4.1983 -4.2246952 -4.2288122 -4.2307925 -4.2491083][-4.1992016 -4.1984859 -4.1854715 -4.1479769 -4.06636 -3.9293616 -3.7993336 -3.8076932 -3.9548967 -4.102787 -4.1918597 -4.2301555 -4.2364511 -4.2378383 -4.2553535][-4.2079158 -4.2098823 -4.2010074 -4.1774254 -4.1205554 -4.017344 -3.9150417 -3.9074306 -4.013093 -4.1374741 -4.2210417 -4.2551236 -4.2534175 -4.2444186 -4.2496905][-4.2366276 -4.2427988 -4.2394276 -4.2260685 -4.1899858 -4.1258717 -4.0597682 -4.0452838 -4.1065216 -4.1921411 -4.2546811 -4.2789531 -4.274024 -4.2599983 -4.2531662][-4.26641 -4.27773 -4.279685 -4.2717776 -4.2480869 -4.21258 -4.1738834 -4.1595016 -4.1918035 -4.2468214 -4.2887769 -4.3044629 -4.2990761 -4.2862191 -4.2726388][-4.2861691 -4.2983947 -4.3038611 -4.3018317 -4.2902966 -4.2761087 -4.257947 -4.2489281 -4.2641206 -4.2931857 -4.3167572 -4.3236527 -4.3170938 -4.3052197 -4.2893124][-4.3134375 -4.3232365 -4.3282609 -4.3268695 -4.321569 -4.3170662 -4.3106732 -4.3081312 -4.315299 -4.3266907 -4.3362913 -4.3359089 -4.32666 -4.3146572 -4.2994566][-4.3301225 -4.3359714 -4.3398957 -4.3385253 -4.3339953 -4.330286 -4.3271317 -4.3280764 -4.3329487 -4.33887 -4.3426185 -4.3405623 -4.3324513 -4.3231893 -4.3100338][-4.3236866 -4.3281574 -4.3326025 -4.3328238 -4.3292413 -4.3259683 -4.3242908 -4.3256178 -4.328567 -4.33286 -4.3353195 -4.3352585 -4.3332181 -4.328568 -4.3206835]]...]
INFO - root - 2017-12-08 00:51:50.733659: step 57810, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 72h:14m:44s remains)
INFO - root - 2017-12-08 00:52:00.389613: step 57820, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.967 sec/batch; 73h:48m:28s remains)
INFO - root - 2017-12-08 00:52:10.072328: step 57830, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.993 sec/batch; 75h:45m:59s remains)
INFO - root - 2017-12-08 00:52:19.725991: step 57840, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.944 sec/batch; 72h:02m:02s remains)
INFO - root - 2017-12-08 00:52:29.311126: step 57850, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 70h:58m:40s remains)
INFO - root - 2017-12-08 00:52:39.006567: step 57860, loss = 2.05, batch loss = 1.99 (7.7 examples/sec; 1.039 sec/batch; 79h:15m:28s remains)
INFO - root - 2017-12-08 00:52:48.764933: step 57870, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.962 sec/batch; 73h:22m:57s remains)
INFO - root - 2017-12-08 00:52:58.387265: step 57880, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 72h:09m:24s remains)
INFO - root - 2017-12-08 00:53:07.820515: step 57890, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 72h:22m:36s remains)
INFO - root - 2017-12-08 00:53:17.336561: step 57900, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 75h:57m:38s remains)
2017-12-08 00:53:18.327469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.225492 -4.2309151 -4.2403 -4.2559657 -4.2663255 -4.262219 -4.2516036 -4.2406912 -4.2264972 -4.2207189 -4.2252522 -4.2292018 -4.2394228 -4.2520194 -4.2589369][-4.2042847 -4.2122416 -4.2292752 -4.243186 -4.2405853 -4.217474 -4.1819463 -4.1519079 -4.13738 -4.1483593 -4.1750655 -4.1965117 -4.2202873 -4.2373972 -4.2420945][-4.1988635 -4.2095227 -4.2252522 -4.2305546 -4.2124624 -4.1610379 -4.0824261 -4.0221319 -4.0184216 -4.064415 -4.1232038 -4.1655989 -4.2027035 -4.2241945 -4.225678][-4.2105913 -4.2206655 -4.2277756 -4.2211 -4.1902051 -4.1091223 -3.9917579 -3.9107165 -3.9271624 -4.0112066 -4.0999203 -4.161489 -4.2003942 -4.2161951 -4.2088146][-4.2228618 -4.2322054 -4.2344513 -4.2233796 -4.182951 -4.0887547 -3.9612803 -3.8886487 -3.9247031 -4.0227385 -4.1172075 -4.1755013 -4.1983538 -4.1982069 -4.1757183][-4.2358804 -4.2463832 -4.2468433 -4.233295 -4.1910133 -4.103137 -3.9915326 -3.9436021 -3.9903407 -4.0785422 -4.1616821 -4.2044516 -4.20368 -4.1864209 -4.1500888][-4.2353292 -4.2453327 -4.2472105 -4.239399 -4.2066121 -4.1374955 -4.0521936 -4.0246143 -4.068665 -4.1420593 -4.2046151 -4.2318835 -4.2166915 -4.18715 -4.144124][-4.2049737 -4.2087069 -4.213552 -4.2127676 -4.19439 -4.1576943 -4.1104889 -4.1066122 -4.1414518 -4.1915722 -4.2274117 -4.2374167 -4.2162781 -4.1855512 -4.1468668][-4.1635213 -4.1645312 -4.1735525 -4.1797705 -4.1724095 -4.158545 -4.1476827 -4.1622114 -4.1892786 -4.2201824 -4.2333846 -4.22534 -4.1996522 -4.177176 -4.1472268][-4.1573086 -4.1564908 -4.1671567 -4.1785231 -4.1722822 -4.162468 -4.1690907 -4.1879172 -4.2097 -4.2289705 -4.2301912 -4.2133541 -4.1919975 -4.1783261 -4.1599436][-4.1926489 -4.19116 -4.1935797 -4.1981263 -4.1885276 -4.18374 -4.193903 -4.2079897 -4.2213755 -4.2342606 -4.2347093 -4.2194853 -4.2097554 -4.2002573 -4.1896372][-4.2417049 -4.2363496 -4.2267876 -4.2215042 -4.212204 -4.2139373 -4.2253966 -4.2344489 -4.2435093 -4.2512503 -4.248888 -4.23529 -4.2323976 -4.2278347 -4.2242537][-4.2895565 -4.2839456 -4.2693081 -4.2605109 -4.2543807 -4.2583551 -4.268558 -4.2736015 -4.278296 -4.2800288 -4.2722082 -4.2583404 -4.2552514 -4.2534442 -4.252738][-4.3224444 -4.3182459 -4.30735 -4.2995787 -4.297565 -4.3042812 -4.3132286 -4.3159366 -4.3165689 -4.3130646 -4.3026333 -4.2919226 -4.2898731 -4.2881727 -4.2850537][-4.3462014 -4.34712 -4.3403959 -4.3337078 -4.3330393 -4.3383174 -4.3431845 -4.3441272 -4.3440437 -4.3407469 -4.3328085 -4.3264637 -4.325583 -4.3228788 -4.3186126]]...]
INFO - root - 2017-12-08 00:53:28.037941: step 57910, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 71h:58m:19s remains)
INFO - root - 2017-12-08 00:53:37.631286: step 57920, loss = 2.11, batch loss = 2.06 (9.1 examples/sec; 0.882 sec/batch; 67h:16m:59s remains)
INFO - root - 2017-12-08 00:53:47.143954: step 57930, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 73h:09m:47s remains)
INFO - root - 2017-12-08 00:53:56.830319: step 57940, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.998 sec/batch; 76h:06m:51s remains)
INFO - root - 2017-12-08 00:54:06.566589: step 57950, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 70h:46m:10s remains)
INFO - root - 2017-12-08 00:54:16.239827: step 57960, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 74h:02m:14s remains)
INFO - root - 2017-12-08 00:54:25.953984: step 57970, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 73h:50m:49s remains)
INFO - root - 2017-12-08 00:54:35.676180: step 57980, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 72h:53m:55s remains)
INFO - root - 2017-12-08 00:54:45.271482: step 57990, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 73h:46m:19s remains)
INFO - root - 2017-12-08 00:54:54.755855: step 58000, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 72h:53m:10s remains)
2017-12-08 00:54:55.800821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3433671 -4.3379717 -4.3331389 -4.3308039 -4.3318024 -4.3321929 -4.3325586 -4.3402 -4.3500004 -4.3547473 -4.3574743 -4.35952 -4.3626785 -4.365581 -4.3686581][-4.3277121 -4.3177309 -4.3108406 -4.30902 -4.3119206 -4.3104916 -4.3094139 -4.3208714 -4.335176 -4.3425145 -4.3458271 -4.3478422 -4.3529005 -4.3580356 -4.3635969][-4.3038292 -4.2878408 -4.278028 -4.2811975 -4.2919054 -4.2931919 -4.2933254 -4.3066397 -4.3204255 -4.3256321 -4.3276668 -4.3310103 -4.3399072 -4.34781 -4.3557372][-4.26652 -4.2465529 -4.2383351 -4.2480011 -4.2641559 -4.2700868 -4.2737322 -4.2889156 -4.3023372 -4.3062072 -4.3081474 -4.3127966 -4.3274426 -4.3374252 -4.3462205][-4.2314806 -4.2126575 -4.2063527 -4.2148461 -4.2298169 -4.2398415 -4.2510042 -4.2720141 -4.2840166 -4.2854381 -4.2835045 -4.2843337 -4.3030577 -4.31755 -4.3293576][-4.19228 -4.1788831 -4.1779413 -4.1832886 -4.1881838 -4.1928763 -4.2068853 -4.2371144 -4.255558 -4.2577639 -4.2507639 -4.2456236 -4.2643895 -4.2832289 -4.3019238][-4.1466832 -4.139266 -4.1351509 -4.1284795 -4.1224165 -4.1236768 -4.1395092 -4.1778445 -4.206799 -4.2144103 -4.2064843 -4.1997542 -4.2187181 -4.2403431 -4.26753][-4.1168628 -4.1087136 -4.0945997 -4.0732894 -4.0590968 -4.0585775 -4.0777688 -4.1209989 -4.1561356 -4.1702104 -4.1657891 -4.1643181 -4.1875782 -4.2124853 -4.2463346][-4.1156268 -4.1092825 -4.0944366 -4.0680923 -4.0479369 -4.0461097 -4.0616264 -4.1044784 -4.138505 -4.1539159 -4.1539145 -4.1587977 -4.1880379 -4.2151766 -4.2484593][-4.1275568 -4.1370864 -4.133997 -4.11085 -4.0802913 -4.0723724 -4.0787878 -4.1108246 -4.1379156 -4.1529751 -4.160306 -4.1703668 -4.2030168 -4.2312355 -4.2629552][-4.1205611 -4.1389112 -4.15006 -4.1377153 -4.1074538 -4.0932546 -4.0926714 -4.1145449 -4.1342959 -4.1506519 -4.1662965 -4.1830945 -4.2134666 -4.2427645 -4.2727289][-4.0816336 -4.0941567 -4.1145749 -4.1171312 -4.0975261 -4.0862551 -4.087986 -4.1100621 -4.1258655 -4.1457891 -4.1687279 -4.1892543 -4.2199125 -4.248415 -4.277348][-4.0334787 -4.0319314 -4.0574493 -4.0728579 -4.0707078 -4.0686326 -4.07693 -4.1084356 -4.1324444 -4.1526809 -4.1716981 -4.1907177 -4.2207747 -4.2502561 -4.2832761][-4.0325356 -4.0164957 -4.0371418 -4.0556722 -4.0618362 -4.0655842 -4.0796337 -4.1227837 -4.1588488 -4.1750712 -4.1782069 -4.1884184 -4.2137208 -4.2433448 -4.282032][-4.0735188 -4.0560861 -4.0657182 -4.0742192 -4.0788946 -4.0832973 -4.100255 -4.1467123 -4.1867175 -4.2018867 -4.1966796 -4.1962528 -4.2112856 -4.2369428 -4.2786489]]...]
INFO - root - 2017-12-08 00:55:05.528688: step 58010, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 75h:23m:44s remains)
INFO - root - 2017-12-08 00:55:15.114207: step 58020, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.946 sec/batch; 72h:06m:07s remains)
INFO - root - 2017-12-08 00:55:24.762667: step 58030, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 70h:51m:09s remains)
INFO - root - 2017-12-08 00:55:34.508572: step 58040, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.987 sec/batch; 75h:15m:14s remains)
INFO - root - 2017-12-08 00:55:44.198353: step 58050, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 75h:14m:26s remains)
INFO - root - 2017-12-08 00:55:53.639581: step 58060, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 73h:47m:13s remains)
INFO - root - 2017-12-08 00:56:03.104136: step 58070, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 71h:54m:43s remains)
INFO - root - 2017-12-08 00:56:12.650891: step 58080, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.934 sec/batch; 71h:12m:11s remains)
INFO - root - 2017-12-08 00:56:22.322645: step 58090, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.949 sec/batch; 72h:22m:27s remains)
INFO - root - 2017-12-08 00:56:31.932418: step 58100, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 70h:44m:20s remains)
2017-12-08 00:56:33.009667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2059832 -4.2202487 -4.248436 -4.2740426 -4.2824492 -4.2700515 -4.2515359 -4.2385969 -4.2341313 -4.2402759 -4.2474146 -4.2601662 -4.2706714 -4.2856722 -4.2948251][-4.2072792 -4.21807 -4.2412715 -4.2597995 -4.2626181 -4.2486272 -4.2328396 -4.2219429 -4.2216892 -4.23425 -4.2482905 -4.2652841 -4.279285 -4.2927928 -4.3009739][-4.215601 -4.220088 -4.2314978 -4.2371449 -4.232244 -4.2192655 -4.210741 -4.2079554 -4.2149696 -4.2310948 -4.2488995 -4.2642407 -4.2755771 -4.2868695 -4.2966423][-4.2244415 -4.2183046 -4.21291 -4.2009182 -4.1832347 -4.169147 -4.1654711 -4.1745205 -4.1927304 -4.2140784 -4.2353582 -4.2507076 -4.2613311 -4.2737708 -4.2886286][-4.24309 -4.2238855 -4.2012134 -4.1710691 -4.1390104 -4.1162715 -4.1079817 -4.1274486 -4.1634254 -4.1959181 -4.2252631 -4.2428007 -4.2546105 -4.2694545 -4.2888145][-4.2506733 -4.2222409 -4.1893644 -4.1489062 -4.1063824 -4.0682697 -4.0456696 -4.0762787 -4.1384063 -4.1892858 -4.227962 -4.2477632 -4.2603068 -4.2752929 -4.2947006][-4.2471542 -4.2194157 -4.1828146 -4.1355233 -4.0844131 -4.0259614 -3.9809837 -4.0228281 -4.1146674 -4.1857696 -4.2331152 -4.2556114 -4.2679949 -4.2816663 -4.2994661][-4.2349405 -4.21501 -4.1868353 -4.1455112 -4.0975838 -4.0311341 -3.97016 -4.0115509 -4.1118779 -4.1890192 -4.2381854 -4.2634 -4.2746997 -4.2864389 -4.3034086][-4.2163086 -4.2047906 -4.1882348 -4.1620216 -4.1313014 -4.0834107 -4.0343018 -4.0595522 -4.1340027 -4.1957326 -4.23764 -4.2616906 -4.27292 -4.2860107 -4.3048773][-4.198245 -4.1912079 -4.1825342 -4.1689024 -4.1577649 -4.1338167 -4.1035233 -4.1156025 -4.1601806 -4.1996164 -4.22879 -4.2491074 -4.2603812 -4.27789 -4.3015213][-4.18354 -4.1794157 -4.1752925 -4.1679196 -4.1673737 -4.1596093 -4.1449246 -4.1500716 -4.1750607 -4.1970496 -4.2162757 -4.2333608 -4.2465749 -4.2689829 -4.2964354][-4.1812768 -4.1782918 -4.1766357 -4.172554 -4.1774778 -4.1813583 -4.177412 -4.1784191 -4.1899605 -4.1977496 -4.2097244 -4.2271667 -4.24397 -4.2677722 -4.29436][-4.1815653 -4.1803975 -4.1820726 -4.1836963 -4.193284 -4.2032361 -4.2048635 -4.2027674 -4.202219 -4.1997528 -4.208539 -4.2290149 -4.2492638 -4.2742186 -4.2992096][-4.1967511 -4.198194 -4.2042847 -4.2088284 -4.2170119 -4.2258916 -4.2267542 -4.2204494 -4.2120566 -4.206347 -4.2151117 -4.2372708 -4.261198 -4.2877297 -4.3112178][-4.2227869 -4.2289195 -4.2388258 -4.2433891 -4.245512 -4.2467313 -4.242219 -4.2325315 -4.2232227 -4.2211351 -4.2331305 -4.257165 -4.2832618 -4.3091359 -4.3285046]]...]
INFO - root - 2017-12-08 00:56:42.748078: step 58110, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.003 sec/batch; 76h:26m:51s remains)
INFO - root - 2017-12-08 00:56:52.309477: step 58120, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 74h:31m:55s remains)
INFO - root - 2017-12-08 00:57:01.905797: step 58130, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 72h:34m:17s remains)
INFO - root - 2017-12-08 00:57:11.579373: step 58140, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.999 sec/batch; 76h:10m:08s remains)
INFO - root - 2017-12-08 00:57:21.230414: step 58150, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 73h:27m:08s remains)
INFO - root - 2017-12-08 00:57:30.901910: step 58160, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 74h:10m:17s remains)
INFO - root - 2017-12-08 00:57:40.563360: step 58170, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.988 sec/batch; 75h:17m:28s remains)
INFO - root - 2017-12-08 00:57:50.186994: step 58180, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 71h:28m:10s remains)
INFO - root - 2017-12-08 00:57:59.809038: step 58190, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 71h:05m:41s remains)
INFO - root - 2017-12-08 00:58:09.470108: step 58200, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 73h:58m:59s remains)
2017-12-08 00:58:10.418500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3395329 -4.3366804 -4.3297739 -4.3193073 -4.30546 -4.2950792 -4.2906547 -4.2903738 -4.289185 -4.2894611 -4.296133 -4.3127723 -4.3266249 -4.3288665 -4.3201213][-4.3421559 -4.3368874 -4.324646 -4.3075457 -4.2879515 -4.2745919 -4.2683048 -4.2653136 -4.2625842 -4.2626181 -4.2719183 -4.296689 -4.3188844 -4.3228378 -4.3115048][-4.33956 -4.3326626 -4.3187609 -4.295939 -4.268537 -4.2502337 -4.2413983 -4.236856 -4.2328205 -4.2301621 -4.2426519 -4.2753735 -4.3043804 -4.3111749 -4.2971058][-4.3336754 -4.3243465 -4.3096824 -4.2851796 -4.2541113 -4.234355 -4.2250385 -4.2221217 -4.2163768 -4.2043262 -4.2115669 -4.2479935 -4.2817788 -4.2952824 -4.278482][-4.318933 -4.3047037 -4.2851505 -4.2622824 -4.2367167 -4.2170267 -4.2093496 -4.2107644 -4.207233 -4.1894579 -4.1884437 -4.2214475 -4.2560639 -4.2713466 -4.2566209][-4.2969933 -4.2762585 -4.249763 -4.2252455 -4.207788 -4.19206 -4.1801381 -4.1808286 -4.1824651 -4.1703897 -4.1692038 -4.1940975 -4.2254653 -4.2441306 -4.2373986][-4.2865772 -4.2563043 -4.2183022 -4.1871653 -4.1715345 -4.1567478 -4.1448689 -4.1420188 -4.1493864 -4.1444111 -4.1496654 -4.1702137 -4.1981897 -4.2220674 -4.2260056][-4.2854629 -4.2531362 -4.2062778 -4.1676078 -4.1521559 -4.1461248 -4.14464 -4.1449866 -4.1521297 -4.1480174 -4.1522627 -4.1663408 -4.1903214 -4.2182245 -4.2344556][-4.2825861 -4.2577724 -4.2108049 -4.1596069 -4.1334324 -4.133738 -4.14937 -4.1580777 -4.1633921 -4.1572137 -4.1564655 -4.1626186 -4.1838617 -4.2163267 -4.2464151][-4.2772684 -4.2621422 -4.2251978 -4.174727 -4.13653 -4.12634 -4.142148 -4.1546679 -4.1603708 -4.1567297 -4.156384 -4.1612339 -4.1792259 -4.2093544 -4.2460265][-4.2603922 -4.2560458 -4.2348795 -4.1994538 -4.1668406 -4.151917 -4.1567922 -4.1663485 -4.1763144 -4.1764164 -4.1736526 -4.1748362 -4.1851716 -4.2083211 -4.2435803][-4.2533331 -4.2584724 -4.2512584 -4.2333875 -4.214653 -4.2027192 -4.1975904 -4.2021713 -4.2083941 -4.2072163 -4.1997952 -4.1954789 -4.20094 -4.2171063 -4.2478209][-4.2612567 -4.2728877 -4.2765779 -4.2708988 -4.2627616 -4.2574224 -4.2524581 -4.25228 -4.254034 -4.2488785 -4.2367749 -4.2276077 -4.226069 -4.234417 -4.258554][-4.2720737 -4.2857828 -4.2980084 -4.300158 -4.2971792 -4.2981915 -4.297224 -4.2958093 -4.2970042 -4.2938843 -4.2788477 -4.2639503 -4.2541976 -4.2548432 -4.2711911][-4.2802253 -4.2943454 -4.3106232 -4.3161135 -4.31361 -4.3166809 -4.3190689 -4.3200383 -4.3231936 -4.3243384 -4.3148527 -4.3004942 -4.287241 -4.2819576 -4.2887154]]...]
INFO - root - 2017-12-08 00:58:20.055952: step 58210, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.982 sec/batch; 74h:47m:18s remains)
INFO - root - 2017-12-08 00:58:29.595891: step 58220, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.977 sec/batch; 74h:25m:59s remains)
INFO - root - 2017-12-08 00:58:39.253878: step 58230, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.016 sec/batch; 77h:26m:32s remains)
INFO - root - 2017-12-08 00:58:48.892791: step 58240, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 74h:01m:11s remains)
INFO - root - 2017-12-08 00:58:58.608869: step 58250, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 71h:54m:11s remains)
INFO - root - 2017-12-08 00:59:08.392321: step 58260, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.022 sec/batch; 77h:49m:19s remains)
INFO - root - 2017-12-08 00:59:18.022656: step 58270, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 72h:50m:59s remains)
INFO - root - 2017-12-08 00:59:27.490130: step 58280, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 71h:04m:07s remains)
INFO - root - 2017-12-08 00:59:37.286369: step 58290, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 73h:49m:28s remains)
INFO - root - 2017-12-08 00:59:47.040604: step 58300, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.005 sec/batch; 76h:31m:05s remains)
2017-12-08 00:59:47.953906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2678919 -4.2792835 -4.28595 -4.2863789 -4.2833476 -4.2809167 -4.2811451 -4.290091 -4.301796 -4.3024096 -4.289659 -4.2708359 -4.2583389 -4.2607279 -4.2721243][-4.2320414 -4.2474189 -4.2650566 -4.2740335 -4.2734609 -4.2658105 -4.2614489 -4.2714739 -4.28792 -4.2906375 -4.2766681 -4.2547269 -4.2385173 -4.238071 -4.249763][-4.1929088 -4.2063069 -4.2356734 -4.258019 -4.2644453 -4.2553697 -4.2426367 -4.2456746 -4.2637897 -4.2718921 -4.2610617 -4.241673 -4.226481 -4.2222962 -4.2309389][-4.1612678 -4.1685915 -4.2041063 -4.2382059 -4.2522225 -4.2461743 -4.2218342 -4.2116237 -4.2296486 -4.2438307 -4.2385 -4.2242236 -4.2138672 -4.207902 -4.2148767][-4.1384 -4.1373119 -4.1675186 -4.2022133 -4.2159858 -4.2060647 -4.1648583 -4.1348786 -4.15864 -4.1942921 -4.2053952 -4.2050357 -4.2009144 -4.1927681 -4.2005496][-4.127461 -4.1204538 -4.1421814 -4.1677938 -4.173007 -4.1465516 -4.0780134 -4.0196176 -4.0555997 -4.1283045 -4.171494 -4.1916995 -4.1957564 -4.1877909 -4.1940637][-4.1262169 -4.1148462 -4.1288939 -4.1449475 -4.1382875 -4.0860419 -3.9827113 -3.8788128 -3.9204721 -4.0425115 -4.12171 -4.166666 -4.1834822 -4.1830873 -4.1891036][-4.1341324 -4.1140943 -4.1204605 -4.1347923 -4.1284723 -4.0687628 -3.9461803 -3.7962794 -3.8170156 -3.9696805 -4.0738239 -4.1353583 -4.1666546 -4.1748972 -4.1818871][-4.1651869 -4.1290431 -4.1224709 -4.1408639 -4.1519179 -4.1200027 -4.0269136 -3.8938184 -3.8782058 -3.9894421 -4.0769839 -4.1374364 -4.1748552 -4.1882143 -4.1939068][-4.19928 -4.1556015 -4.1396866 -4.160881 -4.186708 -4.1853485 -4.1358771 -4.0501308 -4.0175114 -4.0658374 -4.112257 -4.1545763 -4.1909404 -4.2044044 -4.2110686][-4.228519 -4.18361 -4.1618481 -4.1821494 -4.2149024 -4.2299557 -4.2082925 -4.1552305 -4.1189065 -4.1274524 -4.1406803 -4.161449 -4.1860275 -4.1924281 -4.1950774][-4.2528739 -4.2139378 -4.1880517 -4.2018237 -4.233099 -4.2574077 -4.2510319 -4.2185555 -4.1833072 -4.1699734 -4.163188 -4.1683278 -4.1752915 -4.1682873 -4.1622782][-4.2634006 -4.2363014 -4.2121625 -4.217145 -4.2422872 -4.2689428 -4.2741075 -4.2595973 -4.2332368 -4.2095222 -4.1930141 -4.1862178 -4.17618 -4.1586013 -4.1492648][-4.2574763 -4.2392077 -4.2226715 -4.22581 -4.2490716 -4.2761149 -4.2899241 -4.2898755 -4.2781668 -4.2570314 -4.2393637 -4.222374 -4.1992917 -4.1764016 -4.1680241][-4.2445474 -4.2354426 -4.2292433 -4.2331653 -4.2551661 -4.2808475 -4.3007469 -4.3126068 -4.3114576 -4.2933273 -4.2740169 -4.2572489 -4.2344689 -4.2167234 -4.2121477]]...]
INFO - root - 2017-12-08 00:59:57.518822: step 58310, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 73h:30m:21s remains)
INFO - root - 2017-12-08 01:00:07.022413: step 58320, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 73h:25m:50s remains)
INFO - root - 2017-12-08 01:00:16.754744: step 58330, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.043 sec/batch; 79h:27m:07s remains)
INFO - root - 2017-12-08 01:00:26.352002: step 58340, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 73h:34m:35s remains)
INFO - root - 2017-12-08 01:00:36.034083: step 58350, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 74h:29m:46s remains)
INFO - root - 2017-12-08 01:00:45.736197: step 58360, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 71h:07m:03s remains)
INFO - root - 2017-12-08 01:00:55.451333: step 58370, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.968 sec/batch; 73h:42m:05s remains)
INFO - root - 2017-12-08 01:01:05.082493: step 58380, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 69h:52m:43s remains)
INFO - root - 2017-12-08 01:01:14.769678: step 58390, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 71h:28m:13s remains)
INFO - root - 2017-12-08 01:01:24.313856: step 58400, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.920 sec/batch; 70h:04m:58s remains)
2017-12-08 01:01:25.315168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.181807 -4.1870818 -4.2002215 -4.2050309 -4.195106 -4.197083 -4.2145896 -4.2391739 -4.2449093 -4.2314687 -4.2206879 -4.2040634 -4.1895685 -4.1844344 -4.1914463][-4.1640387 -4.174849 -4.1875672 -4.1866775 -4.1714106 -4.1686149 -4.1853514 -4.2104764 -4.215416 -4.2030687 -4.1927238 -4.1744304 -4.1616411 -4.1543908 -4.165689][-4.151732 -4.1625648 -4.1730313 -4.1680326 -4.1497765 -4.14011 -4.1499877 -4.1745043 -4.1830244 -4.1778412 -4.1675544 -4.1496916 -4.1422405 -4.1376657 -4.1511059][-4.1469784 -4.15183 -4.1558065 -4.1470675 -4.1261191 -4.1142631 -4.1236682 -4.1484075 -4.163178 -4.164609 -4.1551447 -4.1430979 -4.147944 -4.1527452 -4.1654205][-4.1437049 -4.1430941 -4.1405892 -4.1213613 -4.0930882 -4.0823884 -4.1000624 -4.1308904 -4.1491613 -4.1536894 -4.1431451 -4.1357489 -4.1494007 -4.164639 -4.1814222][-4.1165586 -4.1205654 -4.1200695 -4.0880556 -4.0470819 -4.0397158 -4.0719414 -4.1108761 -4.123271 -4.1218667 -4.1131182 -4.1095495 -4.1255116 -4.1447153 -4.1654716][-4.0907245 -4.1061025 -4.1094375 -4.0707083 -4.01948 -4.0094776 -4.0455194 -4.0844083 -4.0913029 -4.08071 -4.0716214 -4.0771575 -4.09719 -4.1163921 -4.1371241][-4.0769997 -4.1034503 -4.1143265 -4.085494 -4.040699 -4.0249505 -4.0482616 -4.0744996 -4.0751729 -4.0543575 -4.0405679 -4.0531392 -4.0763383 -4.0935121 -4.109345][-4.0747519 -4.1025591 -4.115592 -4.1011419 -4.07241 -4.0577092 -4.06971 -4.0833373 -4.0771742 -4.0511956 -4.0321784 -4.0450125 -4.0710068 -4.0880809 -4.0969315][-4.0871568 -4.1087551 -4.1181817 -4.11179 -4.0985928 -4.0907626 -4.09761 -4.1020594 -4.0923414 -4.0703335 -4.0526867 -4.0603223 -4.0814242 -4.0958719 -4.0993853][-4.1197281 -4.131309 -4.1352372 -4.1334014 -4.1321464 -4.132092 -4.137279 -4.1386414 -4.1282015 -4.1124153 -4.0990834 -4.1019964 -4.1148167 -4.1256618 -4.1263008][-4.1724224 -4.1740422 -4.1727681 -4.1714458 -4.1732254 -4.1748381 -4.1775951 -4.1783819 -4.173172 -4.1670265 -4.1613975 -4.162487 -4.1685276 -4.173069 -4.1720409][-4.2317019 -4.228549 -4.2253122 -4.2220197 -4.2213864 -4.220674 -4.2199841 -4.2180066 -4.2149563 -4.2146373 -4.2168846 -4.2206216 -4.2247934 -4.2269478 -4.2262545][-4.2721505 -4.2690563 -4.2668138 -4.2651453 -4.2645884 -4.2636433 -4.2610292 -4.2565336 -4.2525082 -4.2514749 -4.2551079 -4.2609696 -4.2648487 -4.2662969 -4.2672229][-4.2979865 -4.2961578 -4.2952852 -4.2940559 -4.2924881 -4.2907329 -4.2882318 -4.2850733 -4.2824268 -4.2825093 -4.2868404 -4.2922225 -4.2946949 -4.2938972 -4.2921638]]...]
INFO - root - 2017-12-08 01:01:34.782390: step 58410, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.947 sec/batch; 72h:04m:03s remains)
INFO - root - 2017-12-08 01:01:44.489005: step 58420, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 73h:21m:18s remains)
INFO - root - 2017-12-08 01:01:54.190670: step 58430, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 74h:05m:15s remains)
INFO - root - 2017-12-08 01:02:03.899277: step 58440, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.999 sec/batch; 76h:02m:44s remains)
INFO - root - 2017-12-08 01:02:13.596708: step 58450, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 74h:11m:41s remains)
INFO - root - 2017-12-08 01:02:23.222451: step 58460, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.881 sec/batch; 67h:03m:55s remains)
INFO - root - 2017-12-08 01:02:32.820333: step 58470, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 73h:23m:12s remains)
INFO - root - 2017-12-08 01:02:42.563317: step 58480, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 74h:50m:21s remains)
INFO - root - 2017-12-08 01:02:52.259981: step 58490, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.963 sec/batch; 73h:16m:29s remains)
INFO - root - 2017-12-08 01:03:01.792558: step 58500, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 72h:43m:30s remains)
2017-12-08 01:03:02.751456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.213943 -4.2556171 -4.2890687 -4.2886982 -4.2631369 -4.227282 -4.1808543 -4.1511812 -4.1407623 -4.1677828 -4.2218871 -4.2753553 -4.3164177 -4.3390031 -4.3400254][-4.2133923 -4.25415 -4.2905693 -4.2965546 -4.2674141 -4.2233787 -4.1734543 -4.1491675 -4.1435113 -4.1723118 -4.22739 -4.2801361 -4.3259521 -4.3531294 -4.3516917][-4.2162695 -4.2562046 -4.2934694 -4.2986946 -4.2636976 -4.2103052 -4.1567774 -4.134841 -4.1339059 -4.1663413 -4.2228923 -4.2756071 -4.3242927 -4.3571033 -4.3581452][-4.2276263 -4.2638674 -4.2971992 -4.2940569 -4.2469392 -4.1823421 -4.1290617 -4.1134024 -4.12232 -4.1609178 -4.2217417 -4.2745233 -4.3226776 -4.357738 -4.362216][-4.2471294 -4.2732568 -4.2986197 -4.2831817 -4.2189803 -4.1372056 -4.0816379 -4.0742335 -4.1011944 -4.1552567 -4.223978 -4.2769284 -4.3248224 -4.3593616 -4.3640227][-4.2744975 -4.2885776 -4.3026471 -4.2732563 -4.1923432 -4.0958266 -4.0300093 -4.020165 -4.0655227 -4.1407394 -4.2192097 -4.276227 -4.3268571 -4.3596087 -4.3627028][-4.3019137 -4.3078084 -4.3105206 -4.2709813 -4.1821632 -4.07918 -3.9992583 -3.9802399 -4.0362396 -4.12645 -4.2098055 -4.271955 -4.3253307 -4.3564053 -4.3581796][-4.3172278 -4.3167381 -4.31132 -4.2666173 -4.1770964 -4.0733056 -3.986568 -3.963645 -4.0236058 -4.1171751 -4.2032108 -4.271616 -4.3256836 -4.3542781 -4.3554873][-4.322741 -4.3182096 -4.3072405 -4.2604289 -4.1720381 -4.0710578 -3.9904361 -3.9728532 -4.026608 -4.11301 -4.2016034 -4.2738853 -4.3270197 -4.354351 -4.3557291][-4.3249817 -4.3179722 -4.3046918 -4.2594643 -4.1771817 -4.0851178 -4.0199094 -4.0063367 -4.039875 -4.108839 -4.1954656 -4.2689791 -4.3230591 -4.3511896 -4.3548985][-4.3261905 -4.3206434 -4.3098035 -4.2691174 -4.1940036 -4.1115303 -4.0583706 -4.0392046 -4.0490823 -4.0997019 -4.1786313 -4.251997 -4.3099051 -4.3423047 -4.35042][-4.3287625 -4.3244805 -4.31679 -4.2830811 -4.2171707 -4.1445913 -4.0945 -4.0621004 -4.0531588 -4.0908222 -4.1613522 -4.2324524 -4.2939844 -4.3329163 -4.3467093][-4.3307333 -4.327157 -4.3213086 -4.2956328 -4.2418532 -4.1790504 -4.1256094 -4.0776277 -4.0557489 -4.084528 -4.1478906 -4.21755 -4.2828074 -4.3274703 -4.3457355][-4.3307028 -4.3264647 -4.3205948 -4.3006296 -4.2590537 -4.2065263 -4.1520462 -4.092577 -4.0604568 -4.0812464 -4.1398244 -4.2099695 -4.2778273 -4.3261662 -4.3466611][-4.3280997 -4.3234367 -4.318511 -4.3026323 -4.2709961 -4.2302294 -4.1777306 -4.1087294 -4.0649428 -4.0758724 -4.1281753 -4.2011209 -4.2744083 -4.326293 -4.3467622]]...]
INFO - root - 2017-12-08 01:03:12.357762: step 58510, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.990 sec/batch; 75h:19m:04s remains)
INFO - root - 2017-12-08 01:03:22.079882: step 58520, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 71h:53m:31s remains)
INFO - root - 2017-12-08 01:03:31.707433: step 58530, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.986 sec/batch; 75h:04m:21s remains)
INFO - root - 2017-12-08 01:03:41.310008: step 58540, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 74h:34m:18s remains)
INFO - root - 2017-12-08 01:03:50.875820: step 58550, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 72h:19m:58s remains)
INFO - root - 2017-12-08 01:04:00.484422: step 58560, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.960 sec/batch; 73h:05m:13s remains)
INFO - root - 2017-12-08 01:04:09.993564: step 58570, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 72h:14m:48s remains)
INFO - root - 2017-12-08 01:04:19.590500: step 58580, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 69h:53m:43s remains)
INFO - root - 2017-12-08 01:04:29.258028: step 58590, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:55m:46s remains)
INFO - root - 2017-12-08 01:04:38.947184: step 58600, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 74h:52m:21s remains)
2017-12-08 01:04:40.097291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.158618 -4.191072 -4.2229137 -4.2436843 -4.2564306 -4.265255 -4.27321 -4.2801108 -4.2856212 -4.2904053 -4.2923346 -4.292376 -4.2919822 -4.2916365 -4.2900419][-4.1396117 -4.1700439 -4.201467 -4.2216225 -4.2339363 -4.2435055 -4.2543726 -4.2655973 -4.2759972 -4.2848649 -4.2898874 -4.2922797 -4.2932439 -4.2932158 -4.291141][-4.1447439 -4.1696472 -4.1943855 -4.206181 -4.2108455 -4.2155013 -4.2255912 -4.2396417 -4.2554989 -4.2703943 -4.2810864 -4.2882853 -4.2924228 -4.293426 -4.2910008][-4.1685772 -4.184063 -4.1950397 -4.19128 -4.1826921 -4.178885 -4.1866241 -4.2029386 -4.2244043 -4.246078 -4.2637672 -4.278173 -4.288578 -4.293098 -4.2916808][-4.1935396 -4.1987209 -4.1941381 -4.1725116 -4.1481075 -4.133832 -4.1372895 -4.1536613 -4.1782832 -4.2053928 -4.230207 -4.25335 -4.273025 -4.2846546 -4.2871308][-4.2113748 -4.2080584 -4.1901045 -4.1542029 -4.1179705 -4.0959053 -4.0943813 -4.1066604 -4.1274633 -4.1531653 -4.1812773 -4.2115569 -4.2419882 -4.263927 -4.2735038][-4.2250009 -4.2161288 -4.1893892 -4.1463375 -4.1056271 -4.0806842 -4.076581 -4.0835385 -4.0937138 -4.1099319 -4.133872 -4.1649613 -4.2014937 -4.2330275 -4.2516327][-4.2332487 -4.2229671 -4.1952271 -4.1549711 -4.11871 -4.0967851 -4.0938706 -4.096746 -4.094862 -4.097775 -4.1114483 -4.1359372 -4.1711726 -4.2071462 -4.2326636][-4.2437744 -4.2360182 -4.2138805 -4.183641 -4.1569223 -4.141582 -4.1410623 -4.1403103 -4.1293316 -4.1204295 -4.1225333 -4.1363597 -4.1642041 -4.1985583 -4.2266726][-4.2625985 -4.2595806 -4.2455621 -4.2263212 -4.2089148 -4.1996088 -4.2004824 -4.19651 -4.1809878 -4.1648483 -4.1571326 -4.1604209 -4.1792049 -4.2084894 -4.2355647][-4.280704 -4.2818074 -4.2746429 -4.2633104 -4.2522631 -4.2478924 -4.2504907 -4.2471704 -4.233573 -4.2167125 -4.2043128 -4.2007771 -4.210597 -4.2320294 -4.253664][-4.2932787 -4.2960935 -4.2929044 -4.2862625 -4.2788882 -4.2772732 -4.2811871 -4.2812057 -4.2736731 -4.2605944 -4.2477303 -4.2406478 -4.2433662 -4.2565103 -4.2709217][-4.3003588 -4.3027682 -4.3010607 -4.2966919 -4.2918382 -4.2912273 -4.2952337 -4.2977886 -4.2959342 -4.2885556 -4.279336 -4.2725477 -4.2713795 -4.2779574 -4.2860456][-4.3040066 -4.3059297 -4.3050728 -4.3025289 -4.2996559 -4.2992706 -4.3021197 -4.3052521 -4.30606 -4.3026829 -4.2970195 -4.2922006 -4.2903986 -4.2934337 -4.2975168][-4.3056965 -4.3070393 -4.3063283 -4.3047085 -4.3030152 -4.3024173 -4.3036971 -4.3058376 -4.307301 -4.3066249 -4.3044672 -4.3021636 -4.3011613 -4.3026628 -4.3043671]]...]
INFO - root - 2017-12-08 01:04:49.712699: step 58610, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 74h:42m:40s remains)
INFO - root - 2017-12-08 01:04:59.475443: step 58620, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.966 sec/batch; 73h:29m:28s remains)
INFO - root - 2017-12-08 01:05:09.239377: step 58630, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 69h:06m:12s remains)
INFO - root - 2017-12-08 01:05:18.829987: step 58640, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 71h:47m:26s remains)
INFO - root - 2017-12-08 01:05:28.450867: step 58650, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.980 sec/batch; 74h:33m:57s remains)
INFO - root - 2017-12-08 01:05:38.272605: step 58660, loss = 2.08, batch loss = 2.03 (7.9 examples/sec; 1.011 sec/batch; 76h:55m:21s remains)
INFO - root - 2017-12-08 01:05:47.969772: step 58670, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.954 sec/batch; 72h:34m:16s remains)
INFO - root - 2017-12-08 01:05:57.688440: step 58680, loss = 2.11, batch loss = 2.06 (8.2 examples/sec; 0.974 sec/batch; 74h:03m:40s remains)
INFO - root - 2017-12-08 01:06:07.183931: step 58690, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.997 sec/batch; 75h:50m:27s remains)
INFO - root - 2017-12-08 01:06:16.719888: step 58700, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 71h:06m:25s remains)
2017-12-08 01:06:17.631887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3261051 -4.3240714 -4.3278351 -4.3311472 -4.3294015 -4.3205314 -4.3086429 -4.2954435 -4.2849722 -4.2827969 -4.2909164 -4.3018284 -4.3142166 -4.3315458 -4.3448114][-4.303647 -4.3004394 -4.3031168 -4.3042445 -4.2987618 -4.2859683 -4.2709417 -4.2573113 -4.245369 -4.2460737 -4.2620158 -4.2826233 -4.2992287 -4.3190012 -4.3357496][-4.277101 -4.2727203 -4.2757149 -4.2755527 -4.2667332 -4.2485909 -4.2264128 -4.2092924 -4.1972933 -4.2015915 -4.2235789 -4.2534032 -4.2789931 -4.3025312 -4.3227606][-4.247376 -4.2440581 -4.2519956 -4.2565756 -4.245039 -4.2174234 -4.1811118 -4.1559572 -4.1442738 -4.1537595 -4.178812 -4.2146482 -4.2509766 -4.2823405 -4.3078012][-4.2137942 -4.2119246 -4.2278442 -4.2388306 -4.2264266 -4.18805 -4.1301517 -4.0838118 -4.0747795 -4.0989184 -4.1313853 -4.1702375 -4.2136016 -4.256207 -4.2899938][-4.1822338 -4.1827846 -4.201766 -4.2110615 -4.1951747 -4.1494927 -4.0683446 -3.9920409 -3.9900656 -4.0466037 -4.0963731 -4.1359911 -4.1816483 -4.2314415 -4.27302][-4.1609559 -4.1631317 -4.1764784 -4.1718388 -4.1442409 -4.0882578 -3.9820883 -3.8636022 -3.8632362 -3.9636869 -4.0428987 -4.0904255 -4.1424904 -4.2029195 -4.2559657][-4.1670213 -4.1649137 -4.1603565 -4.1373434 -4.0983825 -4.0366411 -3.9153295 -3.7649226 -3.7615924 -3.8971198 -4.0040774 -4.0623579 -4.1242137 -4.1907344 -4.2476768][-4.1974134 -4.1897283 -4.1690907 -4.1332822 -4.0964966 -4.0507669 -3.9543307 -3.8345516 -3.8312268 -3.9423718 -4.0395136 -4.0967984 -4.1549644 -4.2143264 -4.2627206][-4.2263618 -4.2183666 -4.1919165 -4.1545243 -4.1309686 -4.1098347 -4.0562935 -3.9897583 -3.9906757 -4.0572577 -4.1225615 -4.1667404 -4.213563 -4.2581873 -4.2920985][-4.2484026 -4.2377639 -4.2139707 -4.1887093 -4.1848583 -4.1856337 -4.1628637 -4.1337814 -4.1397939 -4.1751051 -4.2120614 -4.2406211 -4.2741613 -4.3038568 -4.3237367][-4.2615337 -4.2490335 -4.2327633 -4.2257757 -4.2413139 -4.2576661 -4.2519083 -4.2393646 -4.2454181 -4.2637339 -4.2820616 -4.2962079 -4.3135881 -4.3308563 -4.3423958][-4.2533841 -4.2462997 -4.2436824 -4.2542591 -4.2814026 -4.3052392 -4.3106642 -4.3070116 -4.3085313 -4.3159275 -4.32425 -4.3309021 -4.33811 -4.3472652 -4.3540273][-4.2315445 -4.2345543 -4.2492309 -4.2738953 -4.3028121 -4.3253036 -4.3353543 -4.3349762 -4.332736 -4.3324208 -4.3351517 -4.3391957 -4.345274 -4.3537474 -4.3590689][-4.2036939 -4.2171826 -4.2450862 -4.2763662 -4.3017406 -4.3192153 -4.3284936 -4.3302631 -4.3285756 -4.3267031 -4.3288388 -4.3344388 -4.3430533 -4.3530145 -4.3592777]]...]
INFO - root - 2017-12-08 01:06:27.421503: step 58710, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 73h:12m:17s remains)
INFO - root - 2017-12-08 01:06:37.182646: step 58720, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 75h:50m:52s remains)
INFO - root - 2017-12-08 01:06:46.619482: step 58730, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.973 sec/batch; 73h:58m:51s remains)
INFO - root - 2017-12-08 01:06:56.260774: step 58740, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.987 sec/batch; 75h:05m:28s remains)
INFO - root - 2017-12-08 01:07:06.039016: step 58750, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.004 sec/batch; 76h:20m:16s remains)
INFO - root - 2017-12-08 01:07:15.659454: step 58760, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.943 sec/batch; 71h:44m:04s remains)
INFO - root - 2017-12-08 01:07:25.225173: step 58770, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 72h:12m:57s remains)
INFO - root - 2017-12-08 01:07:34.853088: step 58780, loss = 2.03, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 72h:05m:26s remains)
INFO - root - 2017-12-08 01:07:44.358104: step 58790, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 71h:27m:52s remains)
INFO - root - 2017-12-08 01:07:53.924072: step 58800, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 72h:27m:37s remains)
2017-12-08 01:07:54.853807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2670574 -4.2664495 -4.2662973 -4.2695317 -4.2751055 -4.2768869 -4.2782378 -4.2810545 -4.2777977 -4.2609491 -4.2355981 -4.2197495 -4.2122016 -4.2091632 -4.2162051][-4.2679715 -4.267765 -4.2666664 -4.2662225 -4.2662182 -4.2632475 -4.2604814 -4.2615261 -4.2588773 -4.2442636 -4.2197814 -4.2080264 -4.20781 -4.2097225 -4.2214713][-4.2687521 -4.2680645 -4.2645226 -4.2605629 -4.2541175 -4.2443056 -4.2360649 -4.2345753 -4.2361808 -4.2275472 -4.2063017 -4.201252 -4.2081447 -4.2164769 -4.2333169][-4.2467175 -4.2402978 -4.2329211 -4.2268825 -4.2193565 -4.2062335 -4.1885223 -4.1782193 -4.1865883 -4.1917009 -4.1795144 -4.1799793 -4.1906395 -4.2062364 -4.2308512][-4.2043662 -4.1906605 -4.17753 -4.1685419 -4.1589155 -4.1352768 -4.0955811 -4.0657334 -4.0834541 -4.1171837 -4.1267734 -4.1360269 -4.1476421 -4.1711659 -4.2063265][-4.1685061 -4.1583333 -4.1446533 -4.13335 -4.1189094 -4.0779047 -4.002048 -3.9398582 -3.9739995 -4.053268 -4.09699 -4.113924 -4.12085 -4.145287 -4.1846247][-4.1085687 -4.1047993 -4.0955725 -4.0815372 -4.0583825 -3.9953725 -3.8797958 -3.7735009 -3.8271716 -3.9602373 -4.0416822 -4.0746303 -4.0853968 -4.1160212 -4.1635094][-4.0614157 -4.0652552 -4.0610037 -4.0517755 -4.0323291 -3.9662216 -3.8404381 -3.7166247 -3.7688334 -3.9127386 -4.0023108 -4.0384693 -4.0547338 -4.0906792 -4.1441135][-4.0742726 -4.0847917 -4.0852971 -4.0847392 -4.0745273 -4.0304713 -3.9496665 -3.867399 -3.8941309 -3.9908831 -4.0578527 -4.08732 -4.0993633 -4.1261148 -4.1685619][-4.11235 -4.1237354 -4.1247854 -4.1269069 -4.1269784 -4.1054087 -4.06708 -4.01917 -4.0237575 -4.0746617 -4.1199121 -4.1475229 -4.1578712 -4.1715918 -4.1971631][-4.1573596 -4.1654086 -4.167 -4.1707592 -4.1792812 -4.1751547 -4.164444 -4.1366205 -4.1238856 -4.1392288 -4.1642118 -4.1904135 -4.2008977 -4.2026005 -4.2116237][-4.1897969 -4.192152 -4.1935325 -4.2021518 -4.216362 -4.2227306 -4.2245908 -4.2072735 -4.187314 -4.1805434 -4.1866112 -4.2064166 -4.2165513 -4.2143965 -4.2178745][-4.2229624 -4.22119 -4.2233229 -4.2353945 -4.2496672 -4.2577429 -4.2623858 -4.2530866 -4.2342186 -4.2162123 -4.20892 -4.2192345 -4.228898 -4.2304034 -4.2354522][-4.2586679 -4.25628 -4.2564888 -4.26587 -4.275713 -4.2798495 -4.2843704 -4.2830882 -4.2718992 -4.2543125 -4.2407708 -4.2422624 -4.2504144 -4.2585607 -4.2702293][-4.3031745 -4.2999849 -4.2947273 -4.2949414 -4.2966027 -4.2960548 -4.2982311 -4.3009777 -4.2956619 -4.2817359 -4.26672 -4.2638831 -4.272162 -4.2859693 -4.3027544]]...]
INFO - root - 2017-12-08 01:08:04.326700: step 58810, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 70h:42m:16s remains)
INFO - root - 2017-12-08 01:08:13.913458: step 58820, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 71h:55m:04s remains)
INFO - root - 2017-12-08 01:08:23.676827: step 58830, loss = 2.07, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 75h:01m:15s remains)
INFO - root - 2017-12-08 01:08:33.307645: step 58840, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 73h:01m:30s remains)
INFO - root - 2017-12-08 01:08:42.793172: step 58850, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 74h:48m:37s remains)
INFO - root - 2017-12-08 01:08:52.473965: step 58860, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 72h:34m:50s remains)
INFO - root - 2017-12-08 01:09:02.230541: step 58870, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 73h:01m:47s remains)
INFO - root - 2017-12-08 01:09:11.939253: step 58880, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 73h:46m:07s remains)
INFO - root - 2017-12-08 01:09:21.407264: step 58890, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 74h:05m:08s remains)
INFO - root - 2017-12-08 01:09:30.807715: step 58900, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 71h:59m:07s remains)
2017-12-08 01:09:31.820865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3419628 -4.3385973 -4.3373523 -4.3335128 -4.3230453 -4.3046846 -4.282753 -4.2655935 -4.2616076 -4.2652235 -4.2758431 -4.2912951 -4.3071308 -4.319901 -4.3301129][-4.3422523 -4.3369265 -4.3308005 -4.3163309 -4.2878351 -4.2490005 -4.2080426 -4.1798739 -4.1752281 -4.1844497 -4.2043509 -4.2345181 -4.2631578 -4.2865 -4.3057218][-4.3365312 -4.3251863 -4.3096642 -4.2754478 -4.2203617 -4.1555319 -4.0921278 -4.0548897 -4.0560102 -4.078536 -4.1150684 -4.1617484 -4.1998606 -4.2321692 -4.2628136][-4.3264189 -4.3062115 -4.275238 -4.2167826 -4.1323557 -4.0390215 -3.9581466 -3.9231336 -3.9438787 -3.9950743 -4.054925 -4.1100783 -4.1452465 -4.1764693 -4.2138705][-4.3103848 -4.2792172 -4.2289705 -4.1448221 -4.0330362 -3.9170253 -3.8382826 -3.831758 -3.8890316 -3.9704275 -4.0413847 -4.0884132 -4.1132154 -4.1383705 -4.1789088][-4.2922635 -4.2484837 -4.181253 -4.0755658 -3.9477973 -3.8274682 -3.7731948 -3.8087747 -3.9024677 -4.0005727 -4.06809 -4.0961652 -4.1082945 -4.1298289 -4.1724062][-4.2864575 -4.23351 -4.1542521 -4.0370431 -3.9092951 -3.8020802 -3.7782452 -3.8493896 -3.9620254 -4.0578084 -4.1139059 -4.1272421 -4.1296382 -4.1511731 -4.1930785][-4.2865925 -4.2282839 -4.1447158 -4.03219 -3.9180241 -3.826822 -3.8193462 -3.9039373 -4.0210605 -4.1133194 -4.1625252 -4.1686492 -4.1663733 -4.1857848 -4.2212858][-4.2893872 -4.2336287 -4.156641 -4.058363 -3.9602392 -3.8761337 -3.8670833 -3.9480975 -4.062861 -4.1527529 -4.1972132 -4.1986265 -4.1952014 -4.2109661 -4.2388544][-4.2906752 -4.2434192 -4.178318 -4.0910611 -4.0010786 -3.9131055 -3.8965702 -3.976207 -4.0892715 -4.1751504 -4.2127542 -4.21169 -4.2084932 -4.2203975 -4.2417688][-4.2862973 -4.2501855 -4.1989727 -4.1232195 -4.0400834 -3.9541259 -3.9358721 -4.0126915 -4.1171579 -4.1925459 -4.2237015 -4.2186847 -4.2113733 -4.2155027 -4.2310333][-4.2829547 -4.2569466 -4.2194147 -4.1573782 -4.0867977 -4.013566 -3.9982655 -4.0663304 -4.1554432 -4.2185149 -4.2425761 -4.23261 -4.2157068 -4.2077527 -4.2186122][-4.2813473 -4.2609696 -4.2317872 -4.1831889 -4.1282606 -4.0802732 -4.0757437 -4.1342292 -4.2066026 -4.2549109 -4.2695312 -4.25373 -4.2268109 -4.2094731 -4.2164154][-4.2885766 -4.2717466 -4.2468476 -4.2093143 -4.1716805 -4.1485648 -4.1575828 -4.2064042 -4.2621627 -4.2960205 -4.298368 -4.27461 -4.2437429 -4.2252955 -4.2325182][-4.3004489 -4.2852025 -4.2657104 -4.2401786 -4.2204285 -4.2182331 -4.2375665 -4.274724 -4.3096385 -4.3268933 -4.3183422 -4.2921562 -4.2654338 -4.2542391 -4.2648864]]...]
INFO - root - 2017-12-08 01:09:41.400679: step 58910, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 73h:20m:29s remains)
INFO - root - 2017-12-08 01:09:50.984739: step 58920, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 73h:33m:22s remains)
INFO - root - 2017-12-08 01:10:00.565190: step 58930, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 64h:05m:58s remains)
INFO - root - 2017-12-08 01:10:09.985692: step 58940, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 71h:15m:58s remains)
INFO - root - 2017-12-08 01:10:19.640806: step 58950, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.976 sec/batch; 74h:10m:39s remains)
INFO - root - 2017-12-08 01:10:29.364035: step 58960, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 75h:37m:46s remains)
INFO - root - 2017-12-08 01:10:38.896446: step 58970, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 72h:56m:28s remains)
INFO - root - 2017-12-08 01:10:48.525245: step 58980, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 74h:02m:36s remains)
INFO - root - 2017-12-08 01:10:58.057805: step 58990, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 65h:45m:36s remains)
INFO - root - 2017-12-08 01:11:07.716624: step 59000, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 69h:11m:49s remains)
2017-12-08 01:11:08.724098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1767445 -4.2092757 -4.2156925 -4.1947641 -4.169754 -4.16289 -4.1640596 -4.1630936 -4.1716409 -4.1881418 -4.1939263 -4.180769 -4.1522918 -4.1330013 -4.1391912][-4.2044282 -4.2257328 -4.223732 -4.2010012 -4.1761155 -4.1665154 -4.1637492 -4.1577177 -4.1619062 -4.1790371 -4.1932573 -4.1949553 -4.1748796 -4.1573639 -4.1619368][-4.2382717 -4.2473679 -4.243125 -4.2211509 -4.1950197 -4.1825418 -4.1783662 -4.1706629 -4.16773 -4.1724625 -4.183476 -4.1966958 -4.1892014 -4.17885 -4.1823187][-4.2661176 -4.2666039 -4.261445 -4.2455869 -4.2265491 -4.220068 -4.2202516 -4.2135315 -4.1997147 -4.1819992 -4.1718874 -4.1752815 -4.1752372 -4.1760921 -4.1838503][-4.28237 -4.2764535 -4.2668643 -4.2564263 -4.248343 -4.2497253 -4.256597 -4.2591553 -4.2418857 -4.2083611 -4.17726 -4.1636753 -4.1626697 -4.1684089 -4.1709652][-4.2697663 -4.2552605 -4.2403507 -4.2293086 -4.2230296 -4.2276111 -4.2475 -4.2672515 -4.2599311 -4.2256618 -4.1912026 -4.1713481 -4.1651187 -4.1676226 -4.1644373][-4.2334294 -4.2163305 -4.1978121 -4.1831818 -4.1637888 -4.1555452 -4.1885571 -4.2312312 -4.2420316 -4.2261095 -4.2043977 -4.1810546 -4.1666393 -4.1619291 -4.1599288][-4.1949763 -4.1841536 -4.1611743 -4.1322641 -4.0794244 -4.0392032 -4.0886679 -4.1622796 -4.2008233 -4.2094297 -4.20275 -4.1785183 -4.1589847 -4.1584187 -4.1682744][-4.1846905 -4.1797123 -4.1506886 -4.1059871 -4.0189209 -3.9421499 -4.0043368 -4.0984745 -4.1502428 -4.1740947 -4.1806555 -4.1637144 -4.1505971 -4.1591058 -4.1775031][-4.1996531 -4.203301 -4.1814466 -4.1438284 -4.0704041 -4.0004468 -4.0351176 -4.0954728 -4.1258054 -4.146594 -4.16013 -4.1546793 -4.1489367 -4.1568546 -4.1678472][-4.2235518 -4.2298112 -4.2154527 -4.1952605 -4.1546254 -4.1118627 -4.1126156 -4.12012 -4.120872 -4.1340594 -4.1537051 -4.1579919 -4.1522708 -4.1456318 -4.1390891][-4.239501 -4.2292933 -4.2142019 -4.2096858 -4.1946 -4.1734424 -4.168066 -4.1558061 -4.1459594 -4.1587806 -4.182446 -4.1894236 -4.1728511 -4.1477761 -4.1282978][-4.2454691 -4.2175589 -4.2026644 -4.2110996 -4.2134209 -4.206327 -4.2113376 -4.2017283 -4.1880879 -4.1955318 -4.2119331 -4.2109342 -4.1875181 -4.1616917 -4.1463556][-4.2552538 -4.2172227 -4.1957626 -4.205821 -4.2162127 -4.2213278 -4.2359796 -4.2284923 -4.2051811 -4.2007995 -4.2057152 -4.1993451 -4.1831717 -4.173378 -4.1707635][-4.2585125 -4.2195768 -4.1903286 -4.1945882 -4.2082562 -4.2203021 -4.2372561 -4.2298331 -4.2010708 -4.1908007 -4.1924286 -4.1841812 -4.1746826 -4.1737685 -4.1711407]]...]
INFO - root - 2017-12-08 01:11:18.353112: step 59010, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 69h:29m:02s remains)
INFO - root - 2017-12-08 01:11:28.074975: step 59020, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 72h:55m:59s remains)
INFO - root - 2017-12-08 01:11:37.679312: step 59030, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 73h:49m:30s remains)
INFO - root - 2017-12-08 01:11:47.157605: step 59040, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 72h:13m:34s remains)
INFO - root - 2017-12-08 01:11:56.786512: step 59050, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 75h:38m:41s remains)
INFO - root - 2017-12-08 01:12:06.465268: step 59060, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 69h:48m:12s remains)
INFO - root - 2017-12-08 01:12:16.151102: step 59070, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.000 sec/batch; 75h:55m:03s remains)
INFO - root - 2017-12-08 01:12:25.728314: step 59080, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.025 sec/batch; 77h:51m:40s remains)
INFO - root - 2017-12-08 01:12:35.238191: step 59090, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 72h:33m:11s remains)
INFO - root - 2017-12-08 01:12:45.035212: step 59100, loss = 2.10, batch loss = 2.04 (7.9 examples/sec; 1.015 sec/batch; 77h:04m:50s remains)
2017-12-08 01:12:45.905564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3364253 -4.3368049 -4.334918 -4.3303027 -4.324367 -4.3180432 -4.3135281 -4.3122563 -4.3151488 -4.3213634 -4.3286567 -4.3345709 -4.3379683 -4.3381424 -4.3368778][-4.34257 -4.3414221 -4.3384175 -4.3320279 -4.3214827 -4.3069587 -4.2924848 -4.2836308 -4.2853141 -4.2954865 -4.310473 -4.3262277 -4.3372178 -4.3416185 -4.3410172][-4.3456845 -4.3435335 -4.337862 -4.3249416 -4.3035369 -4.2778683 -4.2515168 -4.2327132 -4.2310791 -4.2467661 -4.2723465 -4.301229 -4.3246117 -4.3381619 -4.3426962][-4.3382077 -4.3342619 -4.3230557 -4.2990856 -4.2621193 -4.2192841 -4.1731544 -4.1368723 -4.1307254 -4.1609769 -4.2101612 -4.2606487 -4.3006959 -4.325726 -4.3382211][-4.3172731 -4.3140531 -4.3000741 -4.2672796 -4.2157445 -4.1468868 -4.064219 -3.9936037 -3.9792998 -4.0340481 -4.1194038 -4.1996264 -4.261889 -4.3032336 -4.3268542][-4.288518 -4.2901797 -4.2781577 -4.2437396 -4.1839666 -4.0935321 -3.9706702 -3.8520267 -3.8276463 -3.9148393 -4.0354242 -4.1394243 -4.2210917 -4.277463 -4.3118396][-4.2581272 -4.2714825 -4.2691312 -4.2401533 -4.1852736 -4.092845 -3.9596884 -3.8225467 -3.796154 -3.8891213 -4.0118027 -4.1194468 -4.2069736 -4.2671108 -4.3036208][-4.2394657 -4.2623076 -4.2732058 -4.258811 -4.2217135 -4.1537967 -4.0558991 -3.95575 -3.9325361 -3.9866502 -4.0699687 -4.1542792 -4.2268047 -4.2751131 -4.3031006][-4.24172 -4.2656088 -4.2853527 -4.28729 -4.2712917 -4.2326193 -4.1746483 -4.1149821 -4.0948796 -4.1138659 -4.1579165 -4.2127209 -4.2628846 -4.2939444 -4.306591][-4.2647595 -4.2828307 -4.3035893 -4.3146811 -4.3119216 -4.2913666 -4.2564049 -4.2182879 -4.1979671 -4.2008877 -4.2227378 -4.2571249 -4.2898254 -4.3080411 -4.3095388][-4.2908978 -4.3047743 -4.3224177 -4.334918 -4.3361416 -4.32484 -4.3016839 -4.2724919 -4.2504492 -4.2441421 -4.2537975 -4.2738738 -4.294313 -4.3047109 -4.3015604][-4.3030152 -4.3169084 -4.3330936 -4.3432407 -4.3443613 -4.3362494 -4.3186607 -4.2943015 -4.2715921 -4.2575922 -4.2576942 -4.2687049 -4.2824469 -4.2887673 -4.2853279][-4.2970457 -4.3130445 -4.3286123 -4.3360825 -4.3361063 -4.3294277 -4.3156781 -4.2940822 -4.2703123 -4.2528358 -4.24785 -4.2536736 -4.2626357 -4.2678947 -4.2685075][-4.2825823 -4.3018994 -4.3181028 -4.322011 -4.3171668 -4.3092284 -4.2981768 -4.2796869 -4.2588258 -4.2442784 -4.2394648 -4.2422142 -4.24735 -4.2520571 -4.2558722][-4.2726092 -4.2935982 -4.3088403 -4.309783 -4.3017817 -4.2926483 -4.2842927 -4.2728539 -4.26052 -4.2523384 -4.2475266 -4.244853 -4.244195 -4.2461638 -4.251009]]...]
INFO - root - 2017-12-08 01:12:55.545179: step 59110, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 70h:01m:27s remains)
INFO - root - 2017-12-08 01:13:05.099473: step 59120, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.979 sec/batch; 74h:18m:40s remains)
INFO - root - 2017-12-08 01:13:14.843071: step 59130, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 69h:21m:52s remains)
INFO - root - 2017-12-08 01:13:24.562252: step 59140, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.970 sec/batch; 73h:37m:27s remains)
INFO - root - 2017-12-08 01:13:34.194398: step 59150, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.015 sec/batch; 77h:06m:23s remains)
INFO - root - 2017-12-08 01:13:43.855566: step 59160, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.931 sec/batch; 70h:41m:28s remains)
INFO - root - 2017-12-08 01:13:53.502722: step 59170, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.984 sec/batch; 74h:43m:11s remains)
INFO - root - 2017-12-08 01:14:03.146751: step 59180, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.028 sec/batch; 78h:04m:37s remains)
INFO - root - 2017-12-08 01:14:12.874948: step 59190, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.040 sec/batch; 78h:56m:55s remains)
INFO - root - 2017-12-08 01:14:22.530930: step 59200, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 75h:48m:26s remains)
2017-12-08 01:14:23.488576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.255815 -4.2840614 -4.2974086 -4.2902737 -4.2636156 -4.2347889 -4.2207646 -4.2198596 -4.2273932 -4.2358055 -4.2464728 -4.2596712 -4.2791018 -4.3071423 -4.326611][-4.2461572 -4.2742124 -4.2802749 -4.2591538 -4.2168889 -4.1769943 -4.1609745 -4.16799 -4.18653 -4.207561 -4.22896 -4.2507515 -4.2771025 -4.3086586 -4.3294072][-4.2461405 -4.2670369 -4.2618561 -4.2235103 -4.1645303 -4.1123943 -4.0921226 -4.1067681 -4.1384926 -4.1743827 -4.2072845 -4.2359233 -4.2659316 -4.2964206 -4.31278][-4.2650919 -4.2731462 -4.2551765 -4.1994772 -4.1196184 -4.0461507 -4.0163727 -4.042511 -4.0959535 -4.150238 -4.1948457 -4.230001 -4.2587271 -4.2782016 -4.2781286][-4.2837005 -4.2838888 -4.261137 -4.1960874 -4.094418 -3.988061 -3.9372559 -3.9724798 -4.0522785 -4.1285057 -4.1867838 -4.2273903 -4.254405 -4.2599235 -4.2402277][-4.3012433 -4.3037977 -4.2840161 -4.2157941 -4.0989828 -3.9659848 -3.8913 -3.9268653 -4.025568 -4.1196241 -4.188077 -4.2308631 -4.2530427 -4.2472539 -4.2147145][-4.3015485 -4.3104196 -4.2947345 -4.2251878 -4.1014686 -3.96493 -3.8961623 -3.9404411 -4.0482249 -4.1500716 -4.2188039 -4.2557769 -4.2694068 -4.2558808 -4.2134438][-4.279984 -4.29401 -4.2827897 -4.2173219 -4.0993814 -3.9766769 -3.9246759 -3.9754376 -4.0824313 -4.1794615 -4.2418923 -4.2735562 -4.2865143 -4.2777863 -4.241621][-4.2581005 -4.274498 -4.2671914 -4.210484 -4.1120052 -4.0173755 -3.9873016 -4.0362725 -4.1298347 -4.211359 -4.2611318 -4.2848983 -4.2969122 -4.2965765 -4.2738576][-4.2504053 -4.2647738 -4.2591162 -4.2138934 -4.1389284 -4.0762124 -4.0675611 -4.1170306 -4.1963067 -4.2584529 -4.2928929 -4.3053594 -4.3106842 -4.3084764 -4.2910261][-4.2521949 -4.2655067 -4.2631192 -4.2308421 -4.1780047 -4.1393328 -4.1407018 -4.1846523 -4.2483978 -4.2936764 -4.3155022 -4.32363 -4.326776 -4.3200178 -4.3009396][-4.2570815 -4.2706203 -4.27427 -4.2572389 -4.2232471 -4.1959729 -4.1935754 -4.2220564 -4.2700148 -4.3057303 -4.3225694 -4.3319459 -4.3387794 -4.3333673 -4.31542][-4.2751102 -4.2869334 -4.2924714 -4.2842784 -4.2607446 -4.2371287 -4.22875 -4.2431746 -4.2783971 -4.3092785 -4.3261247 -4.3370161 -4.3461604 -4.3426838 -4.3270626][-4.2952766 -4.3026705 -4.305418 -4.2998953 -4.2826724 -4.2613974 -4.2489762 -4.256372 -4.2811146 -4.306953 -4.3237896 -4.3365874 -4.347631 -4.3467588 -4.3343573][-4.3057041 -4.3081708 -4.3078108 -4.3027949 -4.2897062 -4.27159 -4.2584519 -4.2630873 -4.2816882 -4.3017383 -4.3145819 -4.3267913 -4.3386207 -4.3416624 -4.33468]]...]
INFO - root - 2017-12-08 01:14:33.112826: step 59210, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 75h:29m:03s remains)
INFO - root - 2017-12-08 01:14:42.646769: step 59220, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.976 sec/batch; 74h:03m:14s remains)
INFO - root - 2017-12-08 01:14:52.243760: step 59230, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.994 sec/batch; 75h:27m:20s remains)
INFO - root - 2017-12-08 01:15:01.879456: step 59240, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.972 sec/batch; 73h:45m:04s remains)
INFO - root - 2017-12-08 01:15:11.404474: step 59250, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.988 sec/batch; 74h:59m:50s remains)
INFO - root - 2017-12-08 01:15:21.113537: step 59260, loss = 2.07, batch loss = 2.01 (7.8 examples/sec; 1.027 sec/batch; 77h:56m:50s remains)
INFO - root - 2017-12-08 01:15:30.798721: step 59270, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.959 sec/batch; 72h:47m:07s remains)
INFO - root - 2017-12-08 01:15:40.388141: step 59280, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 68h:02m:51s remains)
INFO - root - 2017-12-08 01:15:49.934940: step 59290, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 72h:50m:26s remains)
INFO - root - 2017-12-08 01:15:59.688517: step 59300, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.993 sec/batch; 75h:20m:22s remains)
2017-12-08 01:16:00.634918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2243075 -4.2080421 -4.1872206 -4.1736979 -4.1831341 -4.213728 -4.2514296 -4.2799797 -4.2978163 -4.3017607 -4.2927442 -4.2838054 -4.2825522 -4.2836533 -4.2830377][-4.2059121 -4.18747 -4.1734171 -4.1710129 -4.1854472 -4.21589 -4.2464681 -4.2654104 -4.2724876 -4.2633343 -4.2443013 -4.2306719 -4.2270923 -4.2285271 -4.2331681][-4.2031384 -4.1918359 -4.1896968 -4.1951013 -4.2079134 -4.2274513 -4.2393274 -4.238812 -4.2233238 -4.1953063 -4.1683817 -4.152514 -4.153214 -4.1625471 -4.1826196][-4.2152548 -4.2141366 -4.2200856 -4.2238507 -4.22421 -4.2224493 -4.2074847 -4.1796541 -4.1380267 -4.094552 -4.0678205 -4.0609732 -4.0783515 -4.1099844 -4.1527815][-4.229363 -4.2337513 -4.2418594 -4.2379403 -4.2222133 -4.1953607 -4.1460004 -4.0835152 -4.0146551 -3.9692998 -3.9634769 -3.9831061 -4.026546 -4.0856848 -4.1506891][-4.2356858 -4.237875 -4.2409945 -4.2292581 -4.1965694 -4.1406121 -4.0501719 -3.947145 -3.85825 -3.8433356 -3.8902376 -3.9549313 -4.0272017 -4.1031566 -4.1751881][-4.2276597 -4.2177095 -4.2095423 -4.18643 -4.1365809 -4.05392 -3.9289727 -3.7914295 -3.7023919 -3.7548156 -3.8773234 -3.9918785 -4.0817904 -4.1539235 -4.2113757][-4.20337 -4.1767478 -4.150774 -4.1142397 -4.056932 -3.9741352 -3.8482344 -3.7071929 -3.6476557 -3.7687831 -3.9340007 -4.0657153 -4.1570663 -4.2157397 -4.2465253][-4.1628466 -4.1233582 -4.0827894 -4.043489 -4.0046487 -3.959079 -3.8798659 -3.7912552 -3.7773745 -3.88927 -4.0265617 -4.1361752 -4.2151003 -4.2608528 -4.269558][-4.1107697 -4.0723648 -4.0346289 -4.0121322 -4.0108137 -4.0104876 -3.9827881 -3.94461 -3.9526415 -4.0258269 -4.114552 -4.1933484 -4.2549081 -4.2867713 -4.2801671][-4.0809321 -4.05573 -4.033483 -4.0352106 -4.0624795 -4.0944366 -4.1009736 -4.0946836 -4.1080837 -4.1502647 -4.2031016 -4.2542834 -4.2959838 -4.3103971 -4.2912459][-4.1018348 -4.0891523 -4.0805807 -4.0971155 -4.1381121 -4.1824956 -4.2061734 -4.2130771 -4.224206 -4.2483063 -4.277873 -4.3087664 -4.329103 -4.3252234 -4.2964029][-4.1623206 -4.158195 -4.1572509 -4.1755414 -4.2132411 -4.254241 -4.28228 -4.2948914 -4.3027263 -4.3140478 -4.3262291 -4.3377848 -4.3366108 -4.31648 -4.2815933][-4.22984 -4.2281666 -4.2273974 -4.2381511 -4.2634578 -4.294488 -4.3204451 -4.3342805 -4.3387542 -4.341116 -4.3387079 -4.3307924 -4.3087773 -4.2724152 -4.2323871][-4.270236 -4.2681923 -4.2668748 -4.2716737 -4.2867823 -4.3064537 -4.3236561 -4.3323231 -4.3319521 -4.32781 -4.3152127 -4.2954521 -4.2611523 -4.214632 -4.1713681]]...]
INFO - root - 2017-12-08 01:16:10.318993: step 59310, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 72h:46m:00s remains)
INFO - root - 2017-12-08 01:16:19.934785: step 59320, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 72h:03m:10s remains)
INFO - root - 2017-12-08 01:16:29.667287: step 59330, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 74h:35m:41s remains)
INFO - root - 2017-12-08 01:16:39.398180: step 59340, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 71h:56m:03s remains)
INFO - root - 2017-12-08 01:16:48.934568: step 59350, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.971 sec/batch; 73h:42m:01s remains)
INFO - root - 2017-12-08 01:16:58.603067: step 59360, loss = 2.11, batch loss = 2.06 (7.9 examples/sec; 1.011 sec/batch; 76h:42m:53s remains)
INFO - root - 2017-12-08 01:17:08.221542: step 59370, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 74h:10m:34s remains)
INFO - root - 2017-12-08 01:17:17.800030: step 59380, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 71h:20m:10s remains)
INFO - root - 2017-12-08 01:17:27.462593: step 59390, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.947 sec/batch; 71h:49m:04s remains)
INFO - root - 2017-12-08 01:17:37.006412: step 59400, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.995 sec/batch; 75h:28m:09s remains)
2017-12-08 01:17:38.017019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2633433 -4.2668743 -4.284833 -4.293653 -4.2866807 -4.2683296 -4.2487063 -4.2342539 -4.2259283 -4.2258153 -4.23353 -4.234108 -4.2284164 -4.2265458 -4.235683][-4.2559829 -4.2628675 -4.2843709 -4.2908196 -4.2822585 -4.2631717 -4.2406549 -4.22001 -4.2056789 -4.2106533 -4.2268915 -4.2292261 -4.2152858 -4.2005982 -4.20323][-4.251523 -4.2625527 -4.2860746 -4.2885141 -4.2789049 -4.2594404 -4.2283578 -4.1945434 -4.1726604 -4.1903949 -4.2251863 -4.2359958 -4.2162428 -4.1896119 -4.1851397][-4.2511492 -4.2689915 -4.2964191 -4.2981687 -4.285068 -4.2598505 -4.2131138 -4.161242 -4.1381636 -4.1755672 -4.22753 -4.2400966 -4.2153087 -4.1831217 -4.1758876][-4.2521625 -4.2803345 -4.3104005 -4.3108625 -4.2892513 -4.2461395 -4.1726093 -4.1045251 -4.099762 -4.1675978 -4.2277641 -4.2369843 -4.2083497 -4.1762156 -4.1716714][-4.2569637 -4.2915449 -4.3198757 -4.3150272 -4.2826705 -4.213706 -4.1069312 -4.032413 -4.0736475 -4.1765943 -4.2321267 -4.2329731 -4.1999278 -4.1694517 -4.1709175][-4.2552762 -4.284143 -4.3040977 -4.2988443 -4.2579746 -4.1606145 -4.0196643 -3.9513383 -4.0506363 -4.1784191 -4.2281961 -4.2207527 -4.186132 -4.1572642 -4.1670156][-4.238349 -4.2569952 -4.2706017 -4.2666073 -4.216414 -4.0928469 -3.9305556 -3.8892045 -4.0376711 -4.1695223 -4.2096262 -4.1950412 -4.1616759 -4.1377325 -4.1593342][-4.2239041 -4.233614 -4.2431483 -4.2401276 -4.1846862 -4.0554295 -3.9189322 -3.9278326 -4.0746708 -4.1708159 -4.1839218 -4.1607232 -4.1327572 -4.1230764 -4.1596155][-4.2113991 -4.2142391 -4.2204695 -4.2166033 -4.1586742 -4.04082 -3.9546509 -4.0085468 -4.125978 -4.1769271 -4.1641316 -4.1357474 -4.1175656 -4.1243477 -4.1696391][-4.1995387 -4.198092 -4.2021379 -4.1945052 -4.1368093 -4.0357141 -3.9906769 -4.0641155 -4.1540728 -4.1759496 -4.1525025 -4.12747 -4.1190886 -4.1374745 -4.1830406][-4.2109604 -4.2061071 -4.2078938 -4.1961765 -4.1418161 -4.0619636 -4.0448437 -4.1151114 -4.1776285 -4.1849856 -4.1654253 -4.1507578 -4.1505451 -4.1715679 -4.2112679][-4.2509508 -4.2441645 -4.2437778 -4.2300649 -4.185318 -4.13163 -4.12814 -4.1774292 -4.2138662 -4.215766 -4.2079892 -4.2081947 -4.2138143 -4.2300658 -4.260489][-4.3003535 -4.2934036 -4.2898693 -4.2772732 -4.2455592 -4.212616 -4.2131004 -4.24227 -4.2630486 -4.2636557 -4.2670121 -4.2777023 -4.2855048 -4.2952189 -4.3140578][-4.343461 -4.3390064 -4.3329077 -4.3222895 -4.3032069 -4.285058 -4.2871418 -4.3038168 -4.3144131 -4.3133297 -4.3201728 -4.332767 -4.339859 -4.3430395 -4.3518419]]...]
INFO - root - 2017-12-08 01:17:47.374076: step 59410, loss = 2.11, batch loss = 2.06 (8.7 examples/sec; 0.924 sec/batch; 70h:05m:14s remains)
INFO - root - 2017-12-08 01:17:56.952776: step 59420, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 71h:47m:41s remains)
INFO - root - 2017-12-08 01:18:06.487432: step 59430, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.978 sec/batch; 74h:09m:06s remains)
INFO - root - 2017-12-08 01:18:16.054815: step 59440, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 72h:08m:38s remains)
INFO - root - 2017-12-08 01:18:25.701016: step 59450, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 73h:15m:25s remains)
INFO - root - 2017-12-08 01:18:35.356136: step 59460, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 72h:45m:46s remains)
INFO - root - 2017-12-08 01:18:44.886994: step 59470, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 72h:28m:53s remains)
INFO - root - 2017-12-08 01:18:54.541942: step 59480, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.954 sec/batch; 72h:21m:55s remains)
INFO - root - 2017-12-08 01:19:04.313357: step 59490, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.989 sec/batch; 75h:02m:17s remains)
INFO - root - 2017-12-08 01:19:13.913059: step 59500, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 71h:22m:38s remains)
2017-12-08 01:19:14.917970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3370862 -4.3265152 -4.3043046 -4.2761116 -4.2616162 -4.2564082 -4.2528477 -4.2491341 -4.2543073 -4.271379 -4.2878156 -4.2890286 -4.2789288 -4.2582154 -4.2438517][-4.3323035 -4.3177867 -4.2886591 -4.2503271 -4.226213 -4.2186117 -4.2184658 -4.220757 -4.2312465 -4.2571354 -4.2788091 -4.2807984 -4.2711153 -4.2541203 -4.2326136][-4.3265491 -4.3088059 -4.2737522 -4.22307 -4.1846037 -4.1655421 -4.1619463 -4.1723542 -4.1978474 -4.2377481 -4.2695084 -4.2744222 -4.2677016 -4.259892 -4.2400541][-4.3274069 -4.3090396 -4.2730966 -4.2186384 -4.16838 -4.1330733 -4.1242809 -4.1411419 -4.1757135 -4.2233548 -4.2644815 -4.2764211 -4.2778788 -4.2786593 -4.2679744][-4.3311286 -4.3139706 -4.2812314 -4.2328758 -4.1817107 -4.1386008 -4.1268787 -4.1448216 -4.1773148 -4.2158742 -4.2516193 -4.2664313 -4.2723522 -4.2790918 -4.2809305][-4.3340135 -4.3170133 -4.28832 -4.2508283 -4.2089672 -4.1678314 -4.1519642 -4.166244 -4.1924329 -4.2189031 -4.2452812 -4.2582932 -4.260766 -4.264986 -4.2698412][-4.3341947 -4.3152475 -4.2878213 -4.2557368 -4.2214317 -4.1865525 -4.1680274 -4.1737375 -4.193006 -4.2111683 -4.23092 -4.2437391 -4.2462416 -4.2480526 -4.2535396][-4.331151 -4.3083229 -4.278204 -4.2438078 -4.210217 -4.1800475 -4.1604691 -4.1569963 -4.17224 -4.1887774 -4.2107658 -4.2242794 -4.2258506 -4.2275171 -4.2318816][-4.3248563 -4.2991982 -4.2666297 -4.2301521 -4.1973004 -4.1692152 -4.1473436 -4.1375527 -4.1522689 -4.1729684 -4.199151 -4.2143598 -4.2179685 -4.2215047 -4.223917][-4.3174071 -4.2896743 -4.2551336 -4.2157831 -4.1834297 -4.155302 -4.1314607 -4.1227937 -4.1421938 -4.169435 -4.19846 -4.216918 -4.2266273 -4.2351589 -4.238502][-4.3061128 -4.2760181 -4.2403016 -4.1983652 -4.1663995 -4.1413403 -4.122015 -4.118412 -4.1415281 -4.17203 -4.20093 -4.2180061 -4.2284384 -4.2411938 -4.24864][-4.2966981 -4.2666211 -4.2328539 -4.1915965 -4.1646628 -4.14875 -4.1379786 -4.1381512 -4.1568465 -4.1821904 -4.2060595 -4.219768 -4.2279725 -4.2430534 -4.2540541][-4.2960167 -4.2693763 -4.2419472 -4.2082887 -4.1893306 -4.1822395 -4.1790142 -4.1803794 -4.1910019 -4.2063007 -4.2245936 -4.2351809 -4.2406883 -4.2519913 -4.2621455][-4.3060617 -4.2842412 -4.2632422 -4.2390985 -4.2273073 -4.225328 -4.2251353 -4.2276511 -4.2357106 -4.2447753 -4.2562237 -4.2624779 -4.2632489 -4.2656579 -4.2680545][-4.3228097 -4.306119 -4.2920752 -4.2780128 -4.2699437 -4.2692657 -4.2697182 -4.2726588 -4.2788248 -4.2847824 -4.2889781 -4.2889242 -4.2854171 -4.2818856 -4.2797146]]...]
INFO - root - 2017-12-08 01:19:24.560570: step 59510, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.961 sec/batch; 72h:51m:24s remains)
INFO - root - 2017-12-08 01:19:34.227911: step 59520, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.960 sec/batch; 72h:46m:32s remains)
INFO - root - 2017-12-08 01:19:43.807542: step 59530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 73h:05m:54s remains)
INFO - root - 2017-12-08 01:19:53.535325: step 59540, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 72h:41m:04s remains)
INFO - root - 2017-12-08 01:20:03.203168: step 59550, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 73h:23m:31s remains)
INFO - root - 2017-12-08 01:20:12.764361: step 59560, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 74h:28m:46s remains)
INFO - root - 2017-12-08 01:20:22.177335: step 59570, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 0.806 sec/batch; 61h:05m:36s remains)
INFO - root - 2017-12-08 01:20:31.629790: step 59580, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 69h:48m:12s remains)
INFO - root - 2017-12-08 01:20:41.403523: step 59590, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 0.996 sec/batch; 75h:30m:38s remains)
INFO - root - 2017-12-08 01:20:51.148632: step 59600, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.959 sec/batch; 72h:43m:44s remains)
2017-12-08 01:20:52.104273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2234635 -4.2152252 -4.2166562 -4.2204814 -4.2232547 -4.2198834 -4.2109952 -4.1974258 -4.1860762 -4.1802888 -4.1825395 -4.1863685 -4.1736097 -4.1492071 -4.1412711][-4.244596 -4.2442942 -4.2438703 -4.2424479 -4.2433276 -4.2397346 -4.2314062 -4.2244825 -4.2282524 -4.2354393 -4.2355242 -4.2300434 -4.2113705 -4.1798143 -4.1589665][-4.2468381 -4.2528486 -4.2493472 -4.2403488 -4.2360144 -4.2296066 -4.2178617 -4.213572 -4.2300706 -4.2513618 -4.2543154 -4.2485161 -4.2366953 -4.2061877 -4.1741776][-4.2407074 -4.2509651 -4.24703 -4.2340627 -4.2216163 -4.20684 -4.1860261 -4.1797538 -4.20839 -4.2440486 -4.2545061 -4.2533813 -4.2502537 -4.22417 -4.1856833][-4.2281289 -4.2355952 -4.2309723 -4.2146816 -4.190877 -4.1571083 -4.1149249 -4.1038671 -4.1509781 -4.20763 -4.23453 -4.2432575 -4.2489891 -4.2303715 -4.1939349][-4.2108393 -4.2086053 -4.2006974 -4.1836333 -4.1428494 -4.0765381 -3.9982219 -3.979176 -4.0556312 -4.1454058 -4.1970468 -4.2208285 -4.23571 -4.226665 -4.1979609][-4.195797 -4.1874652 -4.1801066 -4.1655412 -4.1119466 -4.0167351 -3.9067402 -3.8829634 -3.9888248 -4.1034608 -4.1732068 -4.20569 -4.2229195 -4.2185907 -4.1966991][-4.1833725 -4.1718316 -4.1716843 -4.1672754 -4.1223512 -4.0324426 -3.9333293 -3.9137707 -4.0107951 -4.1127262 -4.1738191 -4.2021928 -4.212388 -4.2069449 -4.1894846][-4.1770587 -4.1644988 -4.1699395 -4.1786008 -4.1594486 -4.1048656 -4.0412874 -4.0271349 -4.0881848 -4.15261 -4.1874952 -4.2054887 -4.2063961 -4.1946611 -4.17664][-4.1823096 -4.1660867 -4.1674128 -4.1837268 -4.1890273 -4.1656394 -4.1299257 -4.1180806 -4.1515474 -4.1858664 -4.2012596 -4.2087812 -4.2003927 -4.1800938 -4.1607914][-4.1836195 -4.1628547 -4.1559987 -4.173914 -4.1938295 -4.1870513 -4.1665564 -4.1561508 -4.1752405 -4.195888 -4.2026472 -4.2009587 -4.1843567 -4.1591272 -4.1415539][-4.171103 -4.1489635 -4.1390657 -4.1542239 -4.1775022 -4.1797609 -4.1673293 -4.1578913 -4.170476 -4.1882997 -4.1936259 -4.1867785 -4.1674404 -4.1434727 -4.1299596][-4.1535897 -4.1329169 -4.1230717 -4.1343093 -4.1589174 -4.16865 -4.1652951 -4.1578884 -4.1651525 -4.1812515 -4.1892 -4.1833014 -4.1621408 -4.1384435 -4.1295357][-4.1598191 -4.1410422 -4.1273236 -4.1277895 -4.1494031 -4.1629367 -4.1609526 -4.1500812 -4.1520953 -4.1695929 -4.1847582 -4.182405 -4.1579871 -4.13295 -4.1291814][-4.1864009 -4.17022 -4.15046 -4.1366692 -4.1485929 -4.1603103 -4.1562753 -4.140069 -4.1370568 -4.1569123 -4.1791811 -4.1827803 -4.1599789 -4.1375127 -4.1384449]]...]
INFO - root - 2017-12-08 01:21:01.841006: step 59610, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 75h:57m:15s remains)
INFO - root - 2017-12-08 01:21:11.563463: step 59620, loss = 2.09, batch loss = 2.03 (7.5 examples/sec; 1.060 sec/batch; 80h:21m:47s remains)
INFO - root - 2017-12-08 01:21:21.183843: step 59630, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.980 sec/batch; 74h:17m:08s remains)
INFO - root - 2017-12-08 01:21:30.994513: step 59640, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.990 sec/batch; 75h:03m:38s remains)
INFO - root - 2017-12-08 01:21:40.623589: step 59650, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 72h:52m:31s remains)
INFO - root - 2017-12-08 01:21:50.188094: step 59660, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 72h:34m:00s remains)
INFO - root - 2017-12-08 01:21:59.801580: step 59670, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 69h:15m:07s remains)
INFO - root - 2017-12-08 01:22:09.407779: step 59680, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 74h:15m:03s remains)
INFO - root - 2017-12-08 01:22:19.240709: step 59690, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 69h:36m:21s remains)
INFO - root - 2017-12-08 01:22:28.993704: step 59700, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 72h:57m:22s remains)
2017-12-08 01:22:29.994716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2679811 -4.25228 -4.2303252 -4.2093754 -4.1934595 -4.1702089 -4.14734 -4.1515203 -4.1826992 -4.2119212 -4.2222314 -4.2246227 -4.2162104 -4.2021265 -4.1877575][-4.2757053 -4.2677045 -4.2521 -4.2286692 -4.2043905 -4.1710906 -4.141016 -4.1445875 -4.178174 -4.2111034 -4.2217612 -4.2168837 -4.2006769 -4.1864219 -4.1798439][-4.2757974 -4.2716866 -4.2613325 -4.2428589 -4.221508 -4.1881375 -4.1570697 -4.1588688 -4.1896381 -4.220304 -4.2298155 -4.2204885 -4.198555 -4.182107 -4.17453][-4.2766123 -4.2719545 -4.2655029 -4.25539 -4.2423916 -4.2173934 -4.1934795 -4.1914859 -4.2108712 -4.2327104 -4.2401958 -4.2296376 -4.2077188 -4.1909928 -4.1781678][-4.2743354 -4.2650537 -4.2559996 -4.2482162 -4.2414889 -4.2244644 -4.2084513 -4.2060947 -4.2166986 -4.2346683 -4.2446017 -4.2390704 -4.22266 -4.2050643 -4.1837921][-4.2621393 -4.2419419 -4.2202573 -4.203311 -4.1881757 -4.1632266 -4.1461868 -4.1491466 -4.1707692 -4.205215 -4.2315936 -4.2381892 -4.2295079 -4.2125039 -4.1882782][-4.2415681 -4.20958 -4.1720543 -4.1368089 -4.098165 -4.050179 -4.0215259 -4.033515 -4.085206 -4.1504273 -4.2006717 -4.2216668 -4.2225242 -4.2151933 -4.200491][-4.2255344 -4.1946526 -4.1539235 -4.1064911 -4.0459323 -3.9744611 -3.9371231 -3.9638479 -4.0433788 -4.1252089 -4.1815414 -4.2076154 -4.2155347 -4.2238727 -4.228693][-4.2264953 -4.2097855 -4.1816764 -4.1409135 -4.0837855 -4.0170951 -3.9890831 -4.0206742 -4.0930958 -4.1575065 -4.1932755 -4.2065606 -4.216661 -4.2375493 -4.2569685][-4.2361484 -4.2325277 -4.2170963 -4.1879625 -4.145021 -4.0962224 -4.0861969 -4.1190443 -4.169663 -4.2043433 -4.2112389 -4.2068553 -4.2147331 -4.2392445 -4.2614231][-4.2429123 -4.24647 -4.2384715 -4.2160625 -4.1816969 -4.1470203 -4.1488533 -4.1792488 -4.2119246 -4.22604 -4.2152271 -4.2027311 -4.2100158 -4.2312026 -4.244503][-4.2405224 -4.2449117 -4.2398705 -4.2226396 -4.1942077 -4.16897 -4.1724544 -4.1952496 -4.2144361 -4.2177043 -4.2027993 -4.1940036 -4.2034492 -4.2187643 -4.2219114][-4.2290592 -4.228128 -4.2223048 -4.2104063 -4.1889539 -4.172214 -4.1744452 -4.1883039 -4.1987028 -4.1975141 -4.1859064 -4.1830525 -4.1949482 -4.2070317 -4.2044806][-4.2274108 -4.2171926 -4.2070127 -4.1965818 -4.1811833 -4.1679726 -4.1674986 -4.1759481 -4.1822934 -4.1822495 -4.1782584 -4.1821637 -4.1959333 -4.2057686 -4.20389][-4.2518969 -4.2379117 -4.2255521 -4.2149873 -4.2018847 -4.1874027 -4.1820297 -4.1854057 -4.1888475 -4.1900063 -4.1929593 -4.202189 -4.2151494 -4.2232943 -4.2236748]]...]
INFO - root - 2017-12-08 01:22:39.748102: step 59710, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.982 sec/batch; 74h:25m:12s remains)
INFO - root - 2017-12-08 01:22:49.265463: step 59720, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 75h:29m:40s remains)
INFO - root - 2017-12-08 01:22:58.949214: step 59730, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 70h:18m:32s remains)
INFO - root - 2017-12-08 01:23:08.630556: step 59740, loss = 2.10, batch loss = 2.05 (8.5 examples/sec; 0.941 sec/batch; 71h:19m:48s remains)
INFO - root - 2017-12-08 01:23:18.323614: step 59750, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 72h:46m:36s remains)
INFO - root - 2017-12-08 01:23:28.042278: step 59760, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 72h:43m:04s remains)
INFO - root - 2017-12-08 01:23:37.636707: step 59770, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 74h:35m:36s remains)
INFO - root - 2017-12-08 01:23:47.218176: step 59780, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 73h:24m:41s remains)
INFO - root - 2017-12-08 01:23:56.736284: step 59790, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.941 sec/batch; 71h:18m:05s remains)
INFO - root - 2017-12-08 01:24:06.415499: step 59800, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.959 sec/batch; 72h:38m:19s remains)
2017-12-08 01:24:07.507784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3263197 -4.3232346 -4.3227291 -4.32411 -4.327261 -4.3303237 -4.3264356 -4.3190384 -4.3157606 -4.319066 -4.3277812 -4.3390169 -4.3478508 -4.3510671 -4.3519611][-4.3006439 -4.294107 -4.2938023 -4.2997231 -4.30822 -4.3141613 -4.3091655 -4.3005705 -4.3013492 -4.30873 -4.31969 -4.332962 -4.3424368 -4.3436742 -4.3438311][-4.2700424 -4.2604833 -4.2622914 -4.2730503 -4.2858768 -4.2946892 -4.2868662 -4.2778358 -4.2845678 -4.2962332 -4.3084345 -4.3220129 -4.3315372 -4.3302994 -4.3304963][-4.2402854 -4.2280045 -4.231348 -4.243535 -4.2591443 -4.2676473 -4.2543793 -4.2461672 -4.2610073 -4.2818265 -4.2988052 -4.3129973 -4.3188519 -4.3151879 -4.3163877][-4.2153296 -4.1989951 -4.1992545 -4.2089548 -4.2260995 -4.2296152 -4.2100816 -4.1997991 -4.224411 -4.260293 -4.2845974 -4.2996626 -4.3032761 -4.2990856 -4.3012042][-4.1911869 -4.1684513 -4.1645722 -4.1736131 -4.1833453 -4.1758323 -4.1401753 -4.1201549 -4.1599088 -4.2164936 -4.2517915 -4.2737412 -4.2834296 -4.2829361 -4.2854481][-4.177444 -4.1529212 -4.1443686 -4.1460266 -4.1388659 -4.1047406 -4.0344048 -4.0011697 -4.0717945 -4.1547852 -4.20374 -4.2339821 -4.255383 -4.2625446 -4.2673345][-4.1751814 -4.1442671 -4.1274729 -4.1187553 -4.0919652 -4.0189209 -3.8899488 -3.8450143 -3.9707243 -4.0883737 -4.1527696 -4.1934357 -4.2183752 -4.234509 -4.2461267][-4.18621 -4.15176 -4.13085 -4.1244736 -4.0998359 -4.0144658 -3.8829875 -3.8547618 -3.9839518 -4.0925279 -4.1480255 -4.1809049 -4.2011528 -4.2192111 -4.2357154][-4.2190528 -4.1932859 -4.1838994 -4.1879649 -4.171855 -4.10425 -4.0243931 -4.0181761 -4.0943341 -4.1567283 -4.1851444 -4.1997733 -4.21549 -4.2289033 -4.2418585][-4.2551084 -4.2390585 -4.2399273 -4.2507114 -4.240303 -4.1921477 -4.14426 -4.143106 -4.1813159 -4.2145309 -4.2251577 -4.2279181 -4.2431192 -4.2561965 -4.2636547][-4.2880898 -4.2789769 -4.2840681 -4.2946639 -4.2894354 -4.259594 -4.2283955 -4.2238808 -4.2431917 -4.2616839 -4.2635579 -4.2648721 -4.2764735 -4.2876949 -4.2924147][-4.3125706 -4.3068762 -4.3118281 -4.318141 -4.3160014 -4.3010411 -4.28506 -4.2814379 -4.2925196 -4.3024549 -4.3007746 -4.3010907 -4.31035 -4.3183041 -4.3188691][-4.3286538 -4.3237486 -4.325582 -4.3283043 -4.3273473 -4.3202305 -4.3122325 -4.3107228 -4.3189006 -4.3234744 -4.3213921 -4.3213496 -4.3287048 -4.3353591 -4.3357768][-4.3364949 -4.3316531 -4.3292203 -4.3299541 -4.3296022 -4.3253312 -4.3203025 -4.3198128 -4.3257389 -4.3305578 -4.3315921 -4.3328438 -4.3405623 -4.348134 -4.3502283]]...]
INFO - root - 2017-12-08 01:24:17.143516: step 59810, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.990 sec/batch; 75h:01m:28s remains)
INFO - root - 2017-12-08 01:24:26.684373: step 59820, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 73h:17m:31s remains)
INFO - root - 2017-12-08 01:24:36.187243: step 59830, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 71h:10m:40s remains)
INFO - root - 2017-12-08 01:24:45.823889: step 59840, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.983 sec/batch; 74h:28m:27s remains)
INFO - root - 2017-12-08 01:24:55.326781: step 59850, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 73h:33m:08s remains)
INFO - root - 2017-12-08 01:25:04.934293: step 59860, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 71h:00m:25s remains)
INFO - root - 2017-12-08 01:25:14.489387: step 59870, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 72h:09m:53s remains)
INFO - root - 2017-12-08 01:25:23.982904: step 59880, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 73h:05m:11s remains)
INFO - root - 2017-12-08 01:25:33.640575: step 59890, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 73h:03m:27s remains)
INFO - root - 2017-12-08 01:25:43.358199: step 59900, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.008 sec/batch; 76h:19m:04s remains)
2017-12-08 01:25:44.336708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.347353 -4.3437953 -4.3402271 -4.3399334 -4.3439155 -4.3466845 -4.3440218 -4.3369374 -4.3321886 -4.33236 -4.3365602 -4.341476 -4.34113 -4.3384686 -4.3348732][-4.3274732 -4.3172722 -4.3096652 -4.3096294 -4.3157811 -4.3184495 -4.3138523 -4.3050365 -4.3001561 -4.3022985 -4.3081107 -4.313251 -4.3118453 -4.3084545 -4.3052893][-4.297555 -4.2764626 -4.2605 -4.2578807 -4.2656264 -4.2686934 -4.2627764 -4.2527232 -4.2481036 -4.25139 -4.2574615 -4.2631717 -4.2610364 -4.2594147 -4.2587771][-4.2598562 -4.2244492 -4.19761 -4.189146 -4.2003422 -4.2062545 -4.1993432 -4.1880889 -4.1823549 -4.1857848 -4.1928716 -4.2017407 -4.2032728 -4.2060995 -4.20851][-4.2309985 -4.1866527 -4.148561 -4.1306391 -4.1394405 -4.1458945 -4.1368785 -4.1262422 -4.1231308 -4.1281853 -4.1336474 -4.1429672 -4.1534648 -4.1669903 -4.1766739][-4.2206883 -4.1778359 -4.1366286 -4.1079106 -4.1070361 -4.104929 -4.0871277 -4.06738 -4.0577126 -4.0640798 -4.0684447 -4.0765729 -4.0955839 -4.1198835 -4.1368856][-4.2180042 -4.178988 -4.1378894 -4.1034069 -4.09127 -4.0787134 -4.0497985 -4.0230241 -4.0076046 -4.0088716 -4.0058064 -4.0142379 -4.0411558 -4.0693893 -4.0816879][-4.2196913 -4.1840558 -4.1487064 -4.1189437 -4.1022635 -4.0816088 -4.0503407 -4.0284333 -4.0194135 -4.0264077 -4.0280323 -4.0383191 -4.0593719 -4.0736742 -4.0715652][-4.226974 -4.197547 -4.1704903 -4.1492639 -4.13049 -4.1025343 -4.0672851 -4.0508242 -4.0511622 -4.0694041 -4.0858283 -4.1066384 -4.1240978 -4.126193 -4.1120443][-4.2311082 -4.210144 -4.1942267 -4.1807079 -4.1636672 -4.1345906 -4.0975633 -4.0772214 -4.0691385 -4.0842133 -4.1073327 -4.1343346 -4.1556563 -4.1570139 -4.1398106][-4.23816 -4.2201767 -4.2116351 -4.2066379 -4.19734 -4.1765723 -4.1468954 -4.1236286 -4.1053386 -4.10615 -4.1209226 -4.14218 -4.1604919 -4.1618924 -4.1480408][-4.2573352 -4.2401791 -4.2321825 -4.2297392 -4.227035 -4.2147307 -4.1951847 -4.1712346 -4.1452723 -4.1333795 -4.1369805 -4.1503544 -4.1635318 -4.1648645 -4.1571393][-4.2862935 -4.273181 -4.265451 -4.2633715 -4.2614288 -4.250669 -4.2330914 -4.2084517 -4.1818438 -4.1658044 -4.1624603 -4.169507 -4.1772823 -4.1773114 -4.1726623][-4.3160915 -4.3051772 -4.2981639 -4.2947888 -4.2900128 -4.2785969 -4.2619157 -4.2400489 -4.2183647 -4.2045112 -4.2002182 -4.2019906 -4.2043695 -4.2014637 -4.1931319][-4.3411274 -4.3330588 -4.3275728 -4.3233542 -4.316781 -4.3080988 -4.295454 -4.2786584 -4.2619042 -4.2518153 -4.24846 -4.2460876 -4.2402554 -4.2303429 -4.217762]]...]
INFO - root - 2017-12-08 01:25:53.785129: step 59910, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 72h:20m:10s remains)
INFO - root - 2017-12-08 01:26:03.384312: step 59920, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.000 sec/batch; 75h:42m:25s remains)
INFO - root - 2017-12-08 01:26:13.089491: step 59930, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.958 sec/batch; 72h:33m:05s remains)
INFO - root - 2017-12-08 01:26:22.795858: step 59940, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.983 sec/batch; 74h:23m:37s remains)
INFO - root - 2017-12-08 01:26:32.374286: step 59950, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 75h:28m:54s remains)
INFO - root - 2017-12-08 01:26:41.988599: step 59960, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 70h:48m:01s remains)
INFO - root - 2017-12-08 01:26:51.644835: step 59970, loss = 2.08, batch loss = 2.02 (7.7 examples/sec; 1.037 sec/batch; 78h:28m:03s remains)
INFO - root - 2017-12-08 01:27:01.308699: step 59980, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.996 sec/batch; 75h:24m:40s remains)
INFO - root - 2017-12-08 01:27:11.023796: step 59990, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 74h:07m:58s remains)
INFO - root - 2017-12-08 01:27:20.746289: step 60000, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 70h:29m:13s remains)
2017-12-08 01:27:21.700646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3210115 -4.3204141 -4.3199553 -4.3202863 -4.3208151 -4.319778 -4.3175077 -4.3163214 -4.31512 -4.3132448 -4.3109365 -4.3102961 -4.3112574 -4.3109503 -4.3116012][-4.316741 -4.3163352 -4.3155112 -4.3146605 -4.3133864 -4.3090038 -4.3027868 -4.2999067 -4.298512 -4.2967453 -4.2939863 -4.2956982 -4.3009562 -4.3031273 -4.3039379][-4.32216 -4.3207407 -4.3161006 -4.3138037 -4.3099279 -4.3002934 -4.2886186 -4.2839384 -4.28178 -4.2804594 -4.2793112 -4.2835093 -4.2933297 -4.2993217 -4.3020034][-4.3279948 -4.3210573 -4.3105979 -4.3074942 -4.302536 -4.288085 -4.2742133 -4.2687917 -4.2647653 -4.2650089 -4.2662797 -4.2722373 -4.2838469 -4.2955189 -4.3041644][-4.3247275 -4.3108048 -4.2973881 -4.29595 -4.2875619 -4.2676616 -4.2552371 -4.2506061 -4.2442107 -4.2481165 -4.2535515 -4.2607536 -4.27198 -4.2874022 -4.302815][-4.3121185 -4.2968183 -4.2827768 -4.2775655 -4.2567821 -4.2301531 -4.2236753 -4.2194924 -4.2089496 -4.2192435 -4.2306471 -4.2398181 -4.2531686 -4.2710376 -4.2905712][-4.2949739 -4.27559 -4.2572165 -4.2414107 -4.205977 -4.1773281 -4.179862 -4.1719704 -4.1565409 -4.1756077 -4.1971879 -4.20888 -4.2263002 -4.2485862 -4.2716494][-4.2767363 -4.2499366 -4.22316 -4.1928606 -4.1480207 -4.1251245 -4.1357183 -4.1197515 -4.0920587 -4.1133938 -4.1443281 -4.1635237 -4.1905951 -4.2210426 -4.2499666][-4.2682981 -4.2307639 -4.1947036 -4.155766 -4.10922 -4.0940766 -4.1046042 -4.0793524 -4.0390282 -4.0559936 -4.095819 -4.1277666 -4.1646242 -4.2030597 -4.2342134][-4.2700109 -4.2315712 -4.1971879 -4.1647253 -4.1270556 -4.117166 -4.1159468 -4.0828853 -4.0411525 -4.0517273 -4.0959692 -4.1344967 -4.1718688 -4.2100563 -4.2378974][-4.2758226 -4.2508292 -4.2291718 -4.2119679 -4.1891637 -4.1778049 -4.1635518 -4.1270981 -4.0877419 -4.089695 -4.1304531 -4.1691074 -4.2013659 -4.2336698 -4.2539797][-4.2790203 -4.2663856 -4.2538 -4.2432737 -4.227407 -4.213551 -4.1935406 -4.1582136 -4.1213069 -4.1162152 -4.1524425 -4.1905985 -4.2194452 -4.24799 -4.2634687][-4.2845783 -4.2752318 -4.2641354 -4.2527623 -4.238914 -4.2254162 -4.2091556 -4.181077 -4.1498551 -4.1435723 -4.1758161 -4.2113004 -4.2372808 -4.2605233 -4.2707291][-4.2936187 -4.2831244 -4.2727375 -4.2590322 -4.2458873 -4.2373557 -4.2309036 -4.2121453 -4.1842322 -4.180244 -4.2078261 -4.2380247 -4.2609897 -4.2783842 -4.2847652][-4.2971387 -4.2879424 -4.2796359 -4.2682772 -4.257287 -4.2527337 -4.2548623 -4.2437735 -4.2204871 -4.2204671 -4.2426858 -4.2654705 -4.2847843 -4.2967434 -4.2980084]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.001/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 01:27:32.349032: step 60010, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 69h:42m:05s remains)
INFO - root - 2017-12-08 01:27:42.020851: step 60020, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.998 sec/batch; 75h:33m:26s remains)
INFO - root - 2017-12-08 01:27:51.701176: step 60030, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.009 sec/batch; 76h:23m:33s remains)
INFO - root - 2017-12-08 01:28:01.305571: step 60040, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 71h:37m:51s remains)
INFO - root - 2017-12-08 01:28:11.053332: step 60050, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 69h:42m:46s remains)
INFO - root - 2017-12-08 01:28:20.610260: step 60060, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 73h:09m:09s remains)
INFO - root - 2017-12-08 01:28:30.313811: step 60070, loss = 2.10, batch loss = 2.04 (8.2 examples/sec; 0.977 sec/batch; 73h:57m:56s remains)
INFO - root - 2017-12-08 01:28:39.862594: step 60080, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 73h:11m:39s remains)
INFO - root - 2017-12-08 01:28:49.764157: step 60090, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 69h:09m:18s remains)
INFO - root - 2017-12-08 01:28:59.436827: step 60100, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 71h:28m:13s remains)
2017-12-08 01:29:00.429576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3489351 -4.350131 -4.3496923 -4.348177 -4.3460383 -4.3433647 -4.3381329 -4.3302879 -4.3204136 -4.3124428 -4.3107653 -4.3183641 -4.3309131 -4.3450236 -4.3545046][-4.3546057 -4.3567696 -4.3549228 -4.3502226 -4.3442149 -4.3361106 -4.3223629 -4.30644 -4.290678 -4.2797318 -4.27978 -4.2941823 -4.315 -4.3362894 -4.3505344][-4.3593059 -4.3605494 -4.3551264 -4.3448005 -4.333199 -4.3162122 -4.290514 -4.2614384 -4.2371073 -4.2243123 -4.2316546 -4.2576289 -4.2906127 -4.3211379 -4.3414655][-4.3617158 -4.3603268 -4.3488 -4.3292589 -4.3069129 -4.2764497 -4.2357869 -4.191658 -4.1581082 -4.1511517 -4.1746421 -4.2197866 -4.2662148 -4.3052855 -4.3310976][-4.3621488 -4.356585 -4.3365512 -4.3041077 -4.2655811 -4.2188621 -4.1595659 -4.098588 -4.06561 -4.0820613 -4.1361065 -4.2031302 -4.2578926 -4.3003225 -4.3260164][-4.3611584 -4.3489313 -4.3181071 -4.2722583 -4.21534 -4.1426759 -4.0524058 -3.978406 -3.9740937 -4.0405874 -4.13248 -4.2139511 -4.2707257 -4.3087258 -4.3288183][-4.3544073 -4.3341322 -4.29244 -4.2319331 -4.1550894 -4.0503874 -3.9241614 -3.8567204 -3.912688 -4.0400825 -4.1595778 -4.2429314 -4.2945476 -4.323524 -4.3346167][-4.3412032 -4.3133173 -4.2604103 -4.1876168 -4.1014981 -3.9846594 -3.8582139 -3.836391 -3.9442768 -4.0937643 -4.2096705 -4.2784157 -4.3169522 -4.3352671 -4.3367271][-4.3253393 -4.2925725 -4.2340083 -4.1616049 -4.0856023 -4.0007267 -3.9329209 -3.9553676 -4.0596066 -4.1820703 -4.2683635 -4.3127346 -4.3347836 -4.3414197 -4.3338313][-4.3139257 -4.2851195 -4.2369475 -4.1788487 -4.1234188 -4.0822163 -4.06625 -4.102263 -4.1785994 -4.2611432 -4.3129263 -4.3354568 -4.343801 -4.3408537 -4.3275766][-4.309021 -4.28892 -4.2594652 -4.2203774 -4.1859207 -4.1726232 -4.1782603 -4.2089162 -4.2575784 -4.3079534 -4.3380213 -4.3481765 -4.3480797 -4.3384018 -4.3214874][-4.3100877 -4.2979608 -4.2833219 -4.2621145 -4.2450829 -4.2438769 -4.2537613 -4.2752824 -4.304966 -4.3353562 -4.3509512 -4.3530455 -4.3471117 -4.3336725 -4.3158336][-4.3175788 -4.311923 -4.3073535 -4.2999921 -4.2956557 -4.2989488 -4.3062024 -4.3175206 -4.3321853 -4.3463573 -4.3521104 -4.3492146 -4.341135 -4.3287535 -4.3145118][-4.3249741 -4.3206105 -4.3189392 -4.3180623 -4.3206553 -4.3247662 -4.3285947 -4.3331628 -4.3386889 -4.3425927 -4.3438931 -4.3414216 -4.3335 -4.321465 -4.3087749][-4.3264523 -4.3209839 -4.3194313 -4.3216972 -4.3284321 -4.3340845 -4.3366137 -4.3384128 -4.3392081 -4.3374658 -4.3365564 -4.3341508 -4.3245811 -4.3096528 -4.294651]]...]
INFO - root - 2017-12-08 01:29:10.143553: step 60110, loss = 2.06, batch loss = 2.01 (8.0 examples/sec; 0.997 sec/batch; 75h:26m:13s remains)
INFO - root - 2017-12-08 01:29:19.948539: step 60120, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.987 sec/batch; 74h:41m:44s remains)
INFO - root - 2017-12-08 01:29:29.580080: step 60130, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 68h:18m:32s remains)
INFO - root - 2017-12-08 01:29:39.124112: step 60140, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 71h:42m:59s remains)
INFO - root - 2017-12-08 01:29:48.718044: step 60150, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.974 sec/batch; 73h:42m:43s remains)
INFO - root - 2017-12-08 01:29:58.165256: step 60160, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 72h:38m:05s remains)
INFO - root - 2017-12-08 01:30:07.963615: step 60170, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.986 sec/batch; 74h:33m:02s remains)
INFO - root - 2017-12-08 01:30:17.693001: step 60180, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 75h:16m:07s remains)
INFO - root - 2017-12-08 01:30:27.302221: step 60190, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 72h:09m:53s remains)
INFO - root - 2017-12-08 01:30:37.040081: step 60200, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 73h:00m:46s remains)
2017-12-08 01:30:38.064968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1769891 -4.1689878 -4.175149 -4.1948442 -4.2222505 -4.2508273 -4.2662978 -4.26629 -4.2590795 -4.25324 -4.2556267 -4.2674627 -4.2826934 -4.2939558 -4.3012476][-4.1341972 -4.1347322 -4.1543894 -4.1796336 -4.2070017 -4.2331061 -4.2451992 -4.2419205 -4.2328453 -4.22539 -4.2269721 -4.2367554 -4.2518358 -4.2680807 -4.2854118][-4.0890489 -4.0912895 -4.1227903 -4.1535621 -4.1783214 -4.1996322 -4.20531 -4.1970797 -4.1862097 -4.1805487 -4.1856627 -4.1973529 -4.2151418 -4.2371888 -4.26657][-4.0548611 -4.0516481 -4.0867214 -4.1178422 -4.1410823 -4.1612196 -4.1586957 -4.1442308 -4.1347566 -4.1342497 -4.1442866 -4.160881 -4.1811991 -4.2064223 -4.2450442][-4.0547256 -4.0430641 -4.0657558 -4.0857735 -4.1025519 -4.1213779 -4.1077881 -4.0868392 -4.0851893 -4.0941267 -4.1139545 -4.1387472 -4.1621723 -4.1867056 -4.2275128][-4.0714278 -4.0535808 -4.0542822 -4.0533824 -4.0648255 -4.084373 -4.0639772 -4.0410461 -4.05155 -4.0684557 -4.0991488 -4.1301808 -4.1513309 -4.1700783 -4.2077856][-4.0961828 -4.0727963 -4.05234 -4.0284109 -4.035109 -4.0564857 -4.0387731 -4.0261989 -4.047564 -4.0700054 -4.1054139 -4.1330123 -4.1466508 -4.1565843 -4.18625][-4.1353912 -4.110816 -4.0772691 -4.0365472 -4.0367737 -4.0567889 -4.0505977 -4.0576868 -4.0865154 -4.1085243 -4.1350145 -4.1471553 -4.1486278 -4.1485739 -4.1653333][-4.16718 -4.1492643 -4.115109 -4.0724034 -4.0670204 -4.08645 -4.0971189 -4.1207566 -4.14845 -4.1615648 -4.1692128 -4.1603942 -4.1501551 -4.1433969 -4.148778][-4.17433 -4.1692991 -4.14523 -4.1105108 -4.1036806 -4.1248069 -4.1486216 -4.1791639 -4.2003393 -4.1990237 -4.1849127 -4.157115 -4.1394973 -4.1317687 -4.1338429][-4.163137 -4.1688533 -4.1599884 -4.1423469 -4.1437869 -4.1685805 -4.1981754 -4.2273374 -4.2401552 -4.2263613 -4.194633 -4.1556468 -4.1350718 -4.128818 -4.13174][-4.1601295 -4.1749496 -4.1802621 -4.1763816 -4.1838284 -4.2062798 -4.2334166 -4.2566471 -4.26487 -4.2491751 -4.2170849 -4.1860223 -4.1675272 -4.1535378 -4.1481557][-4.1783943 -4.1977396 -4.20958 -4.2079959 -4.2137895 -4.2324581 -4.2534313 -4.2688808 -4.275908 -4.2634268 -4.2400103 -4.2267046 -4.2112737 -4.1846581 -4.1657891][-4.2022271 -4.2223725 -4.2357197 -4.2316208 -4.2315264 -4.2452145 -4.2598014 -4.2680531 -4.2738762 -4.2642541 -4.2498169 -4.249157 -4.2338 -4.1972055 -4.1692152][-4.2198815 -4.2350292 -4.2452092 -4.23653 -4.2302742 -4.2391896 -4.2484822 -4.2511883 -4.2572627 -4.2531834 -4.2477717 -4.2500782 -4.2320738 -4.1942434 -4.1634388]]...]
INFO - root - 2017-12-08 01:30:47.595154: step 60210, loss = 2.09, batch loss = 2.03 (7.8 examples/sec; 1.020 sec/batch; 77h:08m:46s remains)
INFO - root - 2017-12-08 01:30:57.173030: step 60220, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.966 sec/batch; 73h:01m:27s remains)
INFO - root - 2017-12-08 01:31:06.655344: step 60230, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 70h:12m:18s remains)
INFO - root - 2017-12-08 01:31:16.199171: step 60240, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 68h:55m:40s remains)
INFO - root - 2017-12-08 01:31:25.804191: step 60250, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 70h:44m:40s remains)
INFO - root - 2017-12-08 01:31:35.357677: step 60260, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.966 sec/batch; 73h:03m:46s remains)
INFO - root - 2017-12-08 01:31:44.870336: step 60270, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 70h:07m:41s remains)
INFO - root - 2017-12-08 01:31:54.560544: step 60280, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.975 sec/batch; 73h:45m:04s remains)
INFO - root - 2017-12-08 01:32:04.470356: step 60290, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.022 sec/batch; 77h:14m:22s remains)
INFO - root - 2017-12-08 01:32:14.197606: step 60300, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 1.001 sec/batch; 75h:41m:42s remains)
2017-12-08 01:32:15.144361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2242374 -4.2316074 -4.2376976 -4.2400618 -4.244596 -4.2398486 -4.2072463 -4.1580176 -4.1519966 -4.187572 -4.2076015 -4.1979914 -4.2073617 -4.2212777 -4.2141781][-4.237638 -4.246069 -4.2554 -4.2611604 -4.2603111 -4.2507033 -4.2161779 -4.1627722 -4.1573348 -4.1922131 -4.2135096 -4.2020578 -4.2076688 -4.2210708 -4.21689][-4.2426391 -4.2575159 -4.2716055 -4.279829 -4.2748485 -4.2625008 -4.2236366 -4.1636519 -4.1549134 -4.1895323 -4.21096 -4.2002015 -4.2034926 -4.2159352 -4.214963][-4.2397351 -4.2551832 -4.272254 -4.2809076 -4.2753196 -4.260829 -4.2144475 -4.1473727 -4.1408358 -4.1804018 -4.2046804 -4.2027626 -4.2090931 -4.2200327 -4.2221322][-4.2131462 -4.2301822 -4.2513671 -4.2628536 -4.257266 -4.2411075 -4.1863894 -4.1053 -4.1089597 -4.1638641 -4.195406 -4.1994615 -4.2088151 -4.2193031 -4.2257657][-4.1668644 -4.1823854 -4.2146177 -4.2381048 -4.2327504 -4.2149024 -4.1450071 -4.0388279 -4.0569897 -4.1406856 -4.1822944 -4.1923661 -4.2030392 -4.21267 -4.2146072][-4.1132884 -4.1259456 -4.1717439 -4.209094 -4.2018895 -4.1773272 -4.0798383 -3.9292169 -3.9696431 -4.0963545 -4.1532307 -4.172214 -4.1868329 -4.196208 -4.198791][-4.0845885 -4.0897713 -4.1446915 -4.1904006 -4.1834612 -4.1479244 -4.025712 -3.8340678 -3.898639 -4.0589566 -4.1208534 -4.1472492 -4.1733756 -4.1895 -4.1983323][-4.09821 -4.1043444 -4.1603513 -4.2082996 -4.2066851 -4.1761293 -4.0636778 -3.9009953 -3.9594147 -4.0944614 -4.1389928 -4.1578536 -4.186738 -4.2020283 -4.2093754][-4.1282134 -4.1365156 -4.1875625 -4.2339611 -4.241272 -4.223629 -4.1359825 -4.01637 -4.0624461 -4.1576486 -4.1846871 -4.1938438 -4.2152214 -4.2199111 -4.215569][-4.14912 -4.1547341 -4.2000308 -4.2426553 -4.2580109 -4.2485285 -4.1807847 -4.0929618 -4.1250248 -4.1892548 -4.20635 -4.21682 -4.2338281 -4.2302947 -4.2219725][-4.1793303 -4.179543 -4.2117114 -4.2457705 -4.2615619 -4.2543721 -4.2015462 -4.1381741 -4.1627564 -4.2057953 -4.2140784 -4.2261281 -4.2431631 -4.2377763 -4.2306962][-4.2146716 -4.2082081 -4.2278309 -4.2527161 -4.2642236 -4.2553735 -4.20851 -4.1620226 -4.1786222 -4.206037 -4.20782 -4.2175312 -4.2327838 -4.231987 -4.2285767][-4.2196407 -4.2093472 -4.2259665 -4.2450824 -4.2598968 -4.2520695 -4.2053719 -4.1690474 -4.1831355 -4.2048154 -4.2022653 -4.2056351 -4.2145205 -4.2187381 -4.2156677][-4.1956162 -4.1800475 -4.1959786 -4.2086639 -4.2258005 -4.223403 -4.1759176 -4.1500688 -4.1678762 -4.1902614 -4.1871963 -4.1840243 -4.1901517 -4.2010489 -4.1972909]]...]
INFO - root - 2017-12-08 01:32:24.877123: step 60310, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 74h:25m:42s remains)
INFO - root - 2017-12-08 01:32:34.691524: step 60320, loss = 2.11, batch loss = 2.05 (7.6 examples/sec; 1.053 sec/batch; 79h:36m:53s remains)
INFO - root - 2017-12-08 01:32:44.232296: step 60330, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 73h:11m:17s remains)
INFO - root - 2017-12-08 01:32:53.961990: step 60340, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 71h:46m:34s remains)
INFO - root - 2017-12-08 01:33:03.751913: step 60350, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.933 sec/batch; 70h:32m:11s remains)
INFO - root - 2017-12-08 01:33:13.315387: step 60360, loss = 2.10, batch loss = 2.05 (8.6 examples/sec; 0.928 sec/batch; 70h:08m:11s remains)
INFO - root - 2017-12-08 01:33:22.895032: step 60370, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 73h:31m:58s remains)
INFO - root - 2017-12-08 01:33:32.515835: step 60380, loss = 2.09, batch loss = 2.03 (7.9 examples/sec; 1.017 sec/batch; 76h:50m:45s remains)
INFO - root - 2017-12-08 01:33:42.349009: step 60390, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.988 sec/batch; 74h:39m:27s remains)
INFO - root - 2017-12-08 01:33:52.075759: step 60400, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.985 sec/batch; 74h:28m:04s remains)
2017-12-08 01:33:53.049193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1872883 -4.1653814 -4.1655278 -4.1924586 -4.2249036 -4.2311664 -4.2237315 -4.2076192 -4.1781569 -4.1583495 -4.1619334 -4.1816969 -4.1898146 -4.196033 -4.2152729][-4.2265325 -4.2061543 -4.2031488 -4.2250276 -4.2494783 -4.2498617 -4.2347164 -4.2078128 -4.1794395 -4.1635346 -4.1673393 -4.1822157 -4.1863632 -4.194582 -4.221005][-4.2421417 -4.2270265 -4.2266517 -4.2446332 -4.2601714 -4.2555623 -4.2349491 -4.204401 -4.18063 -4.1774368 -4.1932926 -4.2068248 -4.205884 -4.2148194 -4.2460313][-4.2285285 -4.2161703 -4.2172866 -4.2288017 -4.2355876 -4.2232213 -4.1987939 -4.1757016 -4.1652355 -4.1800923 -4.2090168 -4.2247906 -4.2234936 -4.2312422 -4.2626662][-4.1943536 -4.1765571 -4.1759567 -4.1799784 -4.1760635 -4.1544452 -4.1266327 -4.1174631 -4.1340251 -4.1726713 -4.2133865 -4.2324109 -4.2313709 -4.2354732 -4.2635031][-4.1662292 -4.142487 -4.1295619 -4.1204844 -4.1028447 -4.0698433 -4.031086 -4.0285411 -4.0836449 -4.1544333 -4.2092247 -4.234292 -4.2340717 -4.23305 -4.25321][-4.1600785 -4.1316986 -4.1022806 -4.0736222 -4.0385671 -3.9912119 -3.9270976 -3.9148502 -4.0131445 -4.1228042 -4.1951666 -4.2298679 -4.2318506 -4.2274194 -4.2387228][-4.1601725 -4.1288524 -4.0934181 -4.0543633 -4.0140553 -3.9670622 -3.895299 -3.8747797 -3.9897387 -4.1129317 -4.1891842 -4.2240644 -4.2253489 -4.218266 -4.2230225][-4.1425848 -4.1040072 -4.0758767 -4.0499105 -4.0236611 -3.9980326 -3.9570582 -3.9580388 -4.0446935 -4.1403408 -4.2019138 -4.2278709 -4.2240596 -4.2116652 -4.2113576][-4.1261063 -4.0852466 -4.0704746 -4.0726428 -4.0672035 -4.0569363 -4.0445385 -4.061677 -4.1181011 -4.1786594 -4.220077 -4.235425 -4.2260113 -4.2102518 -4.2068181][-4.123477 -4.091948 -4.096056 -4.1192756 -4.1247969 -4.1199808 -4.12113 -4.1426368 -4.1758103 -4.208364 -4.2331991 -4.2414861 -4.2332096 -4.21672 -4.2108684][-4.1354165 -4.1152134 -4.1347909 -4.1654315 -4.1785765 -4.17869 -4.1824622 -4.2001 -4.2160769 -4.2305965 -4.2426763 -4.2448831 -4.2373977 -4.2223477 -4.2133207][-4.1593223 -4.1393156 -4.1630826 -4.1923027 -4.2094507 -4.214191 -4.2213426 -4.2359014 -4.2398744 -4.2392578 -4.2346554 -4.2304487 -4.228435 -4.2213483 -4.2126837][-4.1887317 -4.1598477 -4.1781497 -4.2043891 -4.2236838 -4.2334266 -4.2431121 -4.2527714 -4.2440109 -4.2269149 -4.2090554 -4.2028871 -4.20959 -4.2155628 -4.2182746][-4.2183008 -4.1791711 -4.1836662 -4.2000456 -4.2204847 -4.2388477 -4.250185 -4.2526507 -4.2304287 -4.2014189 -4.1811991 -4.1821914 -4.1988225 -4.2192216 -4.2360468]]...]
INFO - root - 2017-12-08 01:34:02.471140: step 60410, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 66h:01m:06s remains)
INFO - root - 2017-12-08 01:34:12.112377: step 60420, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.980 sec/batch; 74h:03m:34s remains)
INFO - root - 2017-12-08 01:34:21.798015: step 60430, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 69h:44m:24s remains)
INFO - root - 2017-12-08 01:34:31.375436: step 60440, loss = 2.12, batch loss = 2.06 (8.6 examples/sec; 0.929 sec/batch; 70h:11m:39s remains)
INFO - root - 2017-12-08 01:34:40.959951: step 60450, loss = 2.11, batch loss = 2.05 (8.0 examples/sec; 0.995 sec/batch; 75h:11m:02s remains)
INFO - root - 2017-12-08 01:34:50.595475: step 60460, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 72h:32m:25s remains)
INFO - root - 2017-12-08 01:35:00.399537: step 60470, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 70h:37m:44s remains)
INFO - root - 2017-12-08 01:35:10.006516: step 60480, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 71h:11m:14s remains)
INFO - root - 2017-12-08 01:35:19.520698: step 60490, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 73h:27m:25s remains)
INFO - root - 2017-12-08 01:35:29.212428: step 60500, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 71h:41m:46s remains)
2017-12-08 01:35:30.233353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2031007 -4.2279243 -4.2376747 -4.2269068 -4.2056532 -4.1752658 -4.1565847 -4.1579676 -4.1681571 -4.1916652 -4.2255306 -4.2540245 -4.2713165 -4.2759929 -4.2733622][-4.1882453 -4.2146535 -4.2190566 -4.19499 -4.1559124 -4.1068454 -4.0745134 -4.0742426 -4.0878248 -4.1217632 -4.1704693 -4.2073288 -4.2261949 -4.2260809 -4.213912][-4.199492 -4.2278204 -4.2335763 -4.2056994 -4.1561308 -4.0950189 -4.0488954 -4.0409465 -4.0522704 -4.0882545 -4.1419296 -4.1806517 -4.196352 -4.1860695 -4.1592588][-4.2025905 -4.2308731 -4.23798 -4.2124186 -4.1628132 -4.102581 -4.0554991 -4.0483551 -4.0604758 -4.0935755 -4.1415544 -4.1728463 -4.1785388 -4.1529045 -4.1131482][-4.1923027 -4.2214994 -4.229012 -4.2036433 -4.1515512 -4.0933056 -4.0532246 -4.0587983 -4.0807009 -4.1133938 -4.1532116 -4.1751947 -4.1696715 -4.1314034 -4.0849471][-4.1707439 -4.2009392 -4.2075424 -4.1757812 -4.1092362 -4.0435371 -4.0124769 -4.0412464 -4.0799789 -4.1189628 -4.1543074 -4.16956 -4.1571312 -4.1144214 -4.0687418][-4.1521959 -4.178453 -4.1755118 -4.1261687 -4.0371928 -3.9563708 -3.9367979 -3.9978349 -4.0610595 -4.1131544 -4.1487479 -4.1594005 -4.1415272 -4.0993714 -4.0562167][-4.1435051 -4.1649809 -4.1521544 -4.0879207 -3.9842756 -3.8964324 -3.8894539 -3.9737358 -4.0506864 -4.1063704 -4.1408463 -4.1492586 -4.1311622 -4.0939355 -4.0549936][-4.15075 -4.1667967 -4.1546688 -4.0968494 -4.0046706 -3.9275842 -3.9212887 -3.9985654 -4.0669818 -4.118403 -4.1534448 -4.1640511 -4.1490016 -4.1171002 -4.0841389][-4.1846085 -4.1985922 -4.1906524 -4.148746 -4.081429 -4.0239968 -4.0146971 -4.0691695 -4.1204162 -4.1642513 -4.1983066 -4.2108421 -4.1983547 -4.170064 -4.1402788][-4.2317681 -4.2466311 -4.2416883 -4.2122188 -4.1690354 -4.1330652 -4.1251407 -4.1579375 -4.1922407 -4.2266316 -4.2573385 -4.2697892 -4.2585392 -4.2328682 -4.2041349][-4.2684808 -4.2818007 -4.2784 -4.2594633 -4.2348104 -4.2153449 -4.2098107 -4.227767 -4.2492385 -4.2737589 -4.2968206 -4.3075643 -4.2998905 -4.2807531 -4.2593203][-4.2922983 -4.30094 -4.2970762 -4.285429 -4.2729831 -4.2646685 -4.2617865 -4.2708435 -4.2831626 -4.2978263 -4.3122311 -4.3211813 -4.3190451 -4.3099785 -4.2985439][-4.3120875 -4.3158383 -4.3118329 -4.3047667 -4.2992935 -4.297101 -4.2971392 -4.3015766 -4.3075719 -4.3149357 -4.3230805 -4.3295302 -4.3307729 -4.3280764 -4.3236332][-4.3270707 -4.3276572 -4.3250127 -4.321876 -4.3199215 -4.320065 -4.3212528 -4.3235607 -4.3260617 -4.3286448 -4.3313622 -4.3339186 -4.3347383 -4.3345571 -4.3342538]]...]
INFO - root - 2017-12-08 01:35:40.022659: step 60510, loss = 2.11, batch loss = 2.05 (8.2 examples/sec; 0.978 sec/batch; 73h:52m:57s remains)
INFO - root - 2017-12-08 01:35:49.562733: step 60520, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.963 sec/batch; 72h:47m:27s remains)
INFO - root - 2017-12-08 01:35:59.249940: step 60530, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.965 sec/batch; 72h:55m:11s remains)
INFO - root - 2017-12-08 01:36:08.960453: step 60540, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.956 sec/batch; 72h:13m:50s remains)
INFO - root - 2017-12-08 01:36:18.473518: step 60550, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 73h:27m:19s remains)
INFO - root - 2017-12-08 01:36:28.151504: step 60560, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 71h:03m:48s remains)
INFO - root - 2017-12-08 01:36:37.834001: step 60570, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.995 sec/batch; 75h:10m:54s remains)
INFO - root - 2017-12-08 01:36:47.299394: step 60580, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 71h:16m:44s remains)
INFO - root - 2017-12-08 01:36:56.940808: step 60590, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.957 sec/batch; 72h:18m:22s remains)
INFO - root - 2017-12-08 01:37:06.609548: step 60600, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.979 sec/batch; 73h:56m:44s remains)
2017-12-08 01:37:07.575001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2797952 -4.2835197 -4.2814345 -4.27119 -4.2626853 -4.2618694 -4.2675905 -4.2731028 -4.2792649 -4.279181 -4.2694554 -4.2607355 -4.2521696 -4.2487216 -4.2569766][-4.2874894 -4.2875776 -4.2826762 -4.2685676 -4.254878 -4.2501435 -4.2531452 -4.2572446 -4.2650471 -4.271131 -4.2708492 -4.26892 -4.2611356 -4.253674 -4.2578888][-4.290627 -4.28904 -4.2817507 -4.264822 -4.2441015 -4.2298317 -4.2226639 -4.2207818 -4.2288218 -4.24601 -4.2613115 -4.2707953 -4.2710724 -4.2694368 -4.2744927][-4.2824273 -4.2803054 -4.2713933 -4.2547817 -4.2318282 -4.2097011 -4.1917815 -4.1811209 -4.1866479 -4.2128062 -4.2401996 -4.2595906 -4.2688355 -4.2758412 -4.2841396][-4.2667341 -4.2581139 -4.2463946 -4.23345 -4.2111144 -4.1805162 -4.1482425 -4.1221714 -4.1192656 -4.1496768 -4.1862912 -4.2195182 -4.2409196 -4.2588477 -4.2716327][-4.2619381 -4.2434545 -4.2260218 -4.2135053 -4.1902614 -4.1467285 -4.0909605 -4.0346375 -4.0076966 -4.0368204 -4.0910316 -4.1514463 -4.19366 -4.22444 -4.2412519][-4.2750821 -4.2504148 -4.225729 -4.2097926 -4.1837635 -4.1263332 -4.04122 -3.9407122 -3.8610706 -3.8646369 -3.9378479 -4.0410476 -4.115706 -4.1660171 -4.193543][-4.2943215 -4.2700677 -4.2422042 -4.221725 -4.194922 -4.1357288 -4.037921 -3.9063265 -3.7747631 -3.7371762 -3.8137589 -3.9431603 -4.0396295 -4.1055689 -4.1454][-4.3103042 -4.2917213 -4.2672582 -4.2473197 -4.2283807 -4.1897688 -4.1222625 -4.0186076 -3.9084568 -3.8637912 -3.9041505 -3.9895031 -4.0563879 -4.102808 -4.1361427][-4.325305 -4.3129191 -4.2960744 -4.2809339 -4.2741213 -4.2599115 -4.2304907 -4.1722884 -4.1068192 -4.0738711 -4.0794492 -4.1065297 -4.1270661 -4.1413317 -4.1573081][-4.3376079 -4.33041 -4.3201728 -4.3105779 -4.3106208 -4.3076768 -4.299099 -4.2710791 -4.2362709 -4.2165904 -4.2096553 -4.2079659 -4.2028747 -4.19537 -4.1956773][-4.3442874 -4.3413644 -4.335866 -4.3271465 -4.3231521 -4.3205 -4.3158751 -4.300077 -4.2796335 -4.2668037 -4.26098 -4.2572918 -4.2518606 -4.2433286 -4.2412558][-4.3466859 -4.3458 -4.3432283 -4.3360252 -4.3269691 -4.3173909 -4.3073635 -4.2927165 -4.272862 -4.2604589 -4.2569585 -4.2624474 -4.2693911 -4.2714882 -4.2772512][-4.3460374 -4.346498 -4.3457522 -4.3403196 -4.3287449 -4.3147521 -4.2994657 -4.280108 -4.2565365 -4.2410197 -4.2402596 -4.2533445 -4.2727227 -4.2860713 -4.2990155][-4.338954 -4.3428116 -4.3444443 -4.34184 -4.3326125 -4.3159504 -4.2953286 -4.2705941 -4.243494 -4.2247467 -4.2256727 -4.2440677 -4.2694182 -4.287951 -4.3038859]]...]
INFO - root - 2017-12-08 01:37:17.351996: step 60610, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.968 sec/batch; 73h:06m:22s remains)
INFO - root - 2017-12-08 01:37:27.044950: step 60620, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 72h:14m:07s remains)
INFO - root - 2017-12-08 01:37:36.689840: step 60630, loss = 2.09, batch loss = 2.04 (8.2 examples/sec; 0.974 sec/batch; 73h:34m:42s remains)
INFO - root - 2017-12-08 01:37:46.275639: step 60640, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 69h:16m:44s remains)
INFO - root - 2017-12-08 01:37:55.751141: step 60650, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 71h:12m:53s remains)
INFO - root - 2017-12-08 01:38:05.515793: step 60660, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 70h:20m:24s remains)
INFO - root - 2017-12-08 01:38:15.255766: step 60670, loss = 2.08, batch loss = 2.02 (7.8 examples/sec; 1.025 sec/batch; 77h:22m:00s remains)
INFO - root - 2017-12-08 01:38:24.755275: step 60680, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 70h:05m:42s remains)
INFO - root - 2017-12-08 01:38:34.362193: step 60690, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 72h:09m:48s remains)
INFO - root - 2017-12-08 01:38:44.089100: step 60700, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 70h:10m:21s remains)
2017-12-08 01:38:45.068578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2564487 -4.2176013 -4.1614275 -4.1030989 -4.09097 -4.1340585 -4.1770592 -4.1932573 -4.1756487 -4.1492076 -4.1467237 -4.1796246 -4.2124162 -4.22612 -4.2251153][-4.243968 -4.2094817 -4.1612267 -4.1071177 -4.0956883 -4.1396565 -4.1817102 -4.1976604 -4.1826491 -4.156899 -4.1519995 -4.1755247 -4.2039356 -4.2199078 -4.2223091][-4.2267008 -4.2066236 -4.1662903 -4.1054025 -4.0845942 -4.1274276 -4.1741533 -4.1898713 -4.1699543 -4.1357169 -4.1250181 -4.1432061 -4.1702533 -4.1901765 -4.1950865][-4.2090516 -4.2018123 -4.1711636 -4.1011043 -4.0614252 -4.0974407 -4.1513152 -4.1700435 -4.1473279 -4.1060572 -4.0926294 -4.1085877 -4.1377344 -4.1638246 -4.1682267][-4.1863756 -4.1861734 -4.1606255 -4.0879316 -4.0324597 -4.0573788 -4.1140647 -4.1370363 -4.1190782 -4.0793228 -4.0674047 -4.0864496 -4.123137 -4.1607232 -4.1650405][-4.1815958 -4.1795397 -4.1527476 -4.0811305 -4.0165758 -4.0323782 -4.0902934 -4.1160288 -4.1056571 -4.0748506 -4.0646181 -4.0828848 -4.1269221 -4.174561 -4.1822309][-4.2011914 -4.1934514 -4.1632123 -4.0916753 -4.0205636 -4.0307193 -4.0888453 -4.1155272 -4.1110945 -4.0886922 -4.0759869 -4.0882554 -4.1294379 -4.1798692 -4.191165][-4.2207832 -4.2081728 -4.1704412 -4.0932145 -4.0155544 -4.0236735 -4.0873537 -4.1202087 -4.121541 -4.1050758 -4.0893197 -4.0926542 -4.122673 -4.1711597 -4.1839771][-4.2243505 -4.208086 -4.16024 -4.0735869 -3.9880552 -3.997242 -4.072917 -4.1181927 -4.1245184 -4.1152463 -4.1035647 -4.1025629 -4.1204505 -4.1627994 -4.1774178][-4.2322569 -4.2111959 -4.15425 -4.0629826 -3.9763362 -3.9875121 -4.069468 -4.1250315 -4.1371031 -4.1379924 -4.1322942 -4.1310329 -4.1390209 -4.1722808 -4.18567][-4.2441258 -4.2208061 -4.1632857 -4.0738072 -3.9894571 -3.9944954 -4.0727148 -4.1329713 -4.1492157 -4.159059 -4.1601062 -4.1573997 -4.1559191 -4.1834617 -4.1978912][-4.2502861 -4.228138 -4.1753097 -4.0875793 -4.0004277 -3.9958503 -4.0710678 -4.1348505 -4.1553059 -4.171638 -4.1778984 -4.1702652 -4.1601753 -4.1862473 -4.2062817][-4.2503119 -4.2287326 -4.1799078 -4.09276 -4.0013313 -3.9885781 -4.0646095 -4.1344824 -4.1591125 -4.1728325 -4.1810279 -4.1701941 -4.151257 -4.1755409 -4.2032194][-4.2565536 -4.2329988 -4.1842465 -4.0984073 -4.0073261 -3.9924407 -4.0699339 -4.1448627 -4.1731834 -4.1787734 -4.17925 -4.1677675 -4.14626 -4.1677256 -4.200264][-4.2719879 -4.2465606 -4.1968613 -4.1163945 -4.028914 -4.017395 -4.0944686 -4.1707492 -4.2002325 -4.1994958 -4.1864343 -4.1711335 -4.1513066 -4.1645517 -4.1942186]]...]
INFO - root - 2017-12-08 01:38:54.767720: step 60710, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.964 sec/batch; 72h:47m:05s remains)
INFO - root - 2017-12-08 01:39:04.378298: step 60720, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 73h:27m:47s remains)
INFO - root - 2017-12-08 01:39:14.070705: step 60730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 70h:35m:54s remains)
INFO - root - 2017-12-08 01:39:23.843770: step 60740, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.994 sec/batch; 75h:00m:44s remains)
INFO - root - 2017-12-08 01:39:33.282352: step 60750, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 72h:10m:23s remains)
INFO - root - 2017-12-08 01:39:42.828431: step 60760, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:20m:55s remains)
INFO - root - 2017-12-08 01:39:52.539269: step 60770, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 71h:41m:34s remains)
INFO - root - 2017-12-08 01:40:02.219921: step 60780, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.973 sec/batch; 73h:25m:50s remains)
INFO - root - 2017-12-08 01:40:11.953973: step 60790, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.011 sec/batch; 76h:16m:39s remains)
INFO - root - 2017-12-08 01:40:21.756253: step 60800, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.972 sec/batch; 73h:21m:33s remains)
2017-12-08 01:40:22.738485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3300071 -4.3301897 -4.3265643 -4.3230214 -4.3215327 -4.3245368 -4.3284678 -4.3318033 -4.33543 -4.3360395 -4.3339138 -4.331789 -4.3324523 -4.3334365 -4.3355341][-4.3205986 -4.3152533 -4.3058796 -4.2963777 -4.2901716 -4.2908483 -4.29662 -4.3057113 -4.3170891 -4.3244357 -4.3228612 -4.3184657 -4.3164606 -4.3168607 -4.3187313][-4.3032074 -4.2871709 -4.2667642 -4.2502756 -4.2401171 -4.239861 -4.2496643 -4.2671704 -4.2857213 -4.30024 -4.30144 -4.2959442 -4.2898631 -4.2888103 -4.289196][-4.2711897 -4.2416754 -4.2114086 -4.1917205 -4.1845045 -4.1865726 -4.1995893 -4.2248034 -4.2475777 -4.2658978 -4.2725158 -4.2701387 -4.2627368 -4.2594914 -4.2585382][-4.2325616 -4.1906924 -4.1532726 -4.1344566 -4.1303215 -4.13344 -4.1479959 -4.1780877 -4.20385 -4.2250333 -4.2391081 -4.2427111 -4.2402134 -4.2383771 -4.2384553][-4.1972628 -4.1451483 -4.1022205 -4.0810204 -4.07331 -4.0735269 -4.0884132 -4.1168857 -4.1463795 -4.1791997 -4.2073169 -4.2191286 -4.224412 -4.2241735 -4.2255573][-4.1820159 -4.1256218 -4.0783391 -4.0449185 -4.0204511 -4.0108409 -4.0156369 -4.0246086 -4.0463476 -4.0994163 -4.1506772 -4.18177 -4.200316 -4.2050166 -4.21339][-4.1986389 -4.1487145 -4.1041174 -4.0605264 -4.0187755 -3.989219 -3.9601605 -3.9208765 -3.917532 -3.993432 -4.0735321 -4.1236167 -4.157464 -4.1769476 -4.201479][-4.2296543 -4.1946445 -4.1620951 -4.1218143 -4.0785251 -4.0352993 -3.9654088 -3.8705571 -3.8377535 -3.922822 -4.0166979 -4.0767083 -4.1246066 -4.1606421 -4.2017994][-4.24825 -4.2230334 -4.202384 -4.1760936 -4.1488161 -4.113019 -4.0394154 -3.9401517 -3.9048436 -3.9640424 -4.0355387 -4.08762 -4.13365 -4.1764975 -4.2229428][-4.2458119 -4.2223072 -4.2083492 -4.199626 -4.197536 -4.1853805 -4.1362333 -4.067328 -4.0432763 -4.0722642 -4.1125636 -4.1464691 -4.1778226 -4.2103524 -4.2448444][-4.2397203 -4.2155676 -4.2042079 -4.2053394 -4.2219448 -4.2313843 -4.210341 -4.1720352 -4.1568031 -4.1661644 -4.1856527 -4.2061048 -4.2209659 -4.23666 -4.2560482][-4.2468295 -4.2235723 -4.2116923 -4.2154584 -4.2354045 -4.2504559 -4.2452207 -4.2250814 -4.212646 -4.2126484 -4.2251973 -4.2416124 -4.2483544 -4.2550445 -4.2682004][-4.2672772 -4.2462325 -4.2347484 -4.2398925 -4.2572589 -4.269094 -4.270452 -4.2611356 -4.2525535 -4.2525306 -4.26431 -4.2765727 -4.2793813 -4.2825093 -4.2923288][-4.2904677 -4.2759395 -4.2678266 -4.2721758 -4.283947 -4.2910767 -4.2946811 -4.2924576 -4.2875223 -4.289598 -4.2997704 -4.308496 -4.3108811 -4.3127084 -4.3198519]]...]
INFO - root - 2017-12-08 01:40:32.458332: step 60810, loss = 2.09, batch loss = 2.04 (7.9 examples/sec; 1.010 sec/batch; 76h:13m:47s remains)
INFO - root - 2017-12-08 01:40:41.965291: step 60820, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 73h:53m:47s remains)
INFO - root - 2017-12-08 01:40:51.468913: step 60830, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 70h:43m:41s remains)
INFO - root - 2017-12-08 01:41:01.278365: step 60840, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 74h:19m:13s remains)
INFO - root - 2017-12-08 01:41:10.962833: step 60850, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 72h:45m:43s remains)
INFO - root - 2017-12-08 01:41:20.589899: step 60860, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 71h:34m:44s remains)
INFO - root - 2017-12-08 01:41:30.162655: step 60870, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 70h:37m:23s remains)
INFO - root - 2017-12-08 01:41:40.031284: step 60880, loss = 2.10, batch loss = 2.04 (8.0 examples/sec; 1.005 sec/batch; 75h:48m:08s remains)
INFO - root - 2017-12-08 01:41:49.695399: step 60890, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 72h:43m:19s remains)
INFO - root - 2017-12-08 01:41:59.296020: step 60900, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.952 sec/batch; 71h:47m:38s remains)
2017-12-08 01:42:00.395469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2495284 -4.2517428 -4.2631264 -4.2658663 -4.2619762 -4.252667 -4.24597 -4.2265387 -4.2040124 -4.1974368 -4.1927862 -4.1776686 -4.1555963 -4.1378632 -4.1394329][-4.2658782 -4.2646465 -4.2685795 -4.2617393 -4.2567024 -4.2522097 -4.2555618 -4.250896 -4.2409067 -4.2325759 -4.2238159 -4.2061143 -4.1780052 -4.1541009 -4.1453524][-4.2668777 -4.2711864 -4.2729077 -4.2534671 -4.2380161 -4.2359495 -4.2472286 -4.2554398 -4.253921 -4.2493477 -4.2432985 -4.2289433 -4.1999412 -4.1734772 -4.1553049][-4.2740703 -4.2823348 -4.2772889 -4.2479172 -4.2223053 -4.2161064 -4.2292352 -4.2490344 -4.263504 -4.2699947 -4.2687354 -4.2565441 -4.22984 -4.20541 -4.1816258][-4.2613168 -4.2697444 -4.25749 -4.22469 -4.1947279 -4.1872993 -4.2006526 -4.2326126 -4.2613344 -4.2780871 -4.2813573 -4.2713346 -4.2494807 -4.2278886 -4.2021794][-4.2260227 -4.2305851 -4.2089572 -4.1700039 -4.1362362 -4.1270037 -4.1356244 -4.1726046 -4.2163496 -4.242847 -4.252502 -4.2522168 -4.2403426 -4.2239037 -4.1988516][-4.1859446 -4.1839995 -4.1557965 -4.1102629 -4.0674162 -4.0464358 -4.0433488 -4.0756369 -4.1246772 -4.16738 -4.1954761 -4.2114263 -4.21445 -4.2044144 -4.1837158][-4.1568327 -4.1518803 -4.1277137 -4.0798569 -4.020236 -3.980382 -3.9628744 -3.9789302 -4.01813 -4.0754189 -4.1282544 -4.1656833 -4.1872482 -4.1899877 -4.1769013][-4.1580243 -4.1576166 -4.1421356 -4.1048942 -4.041069 -3.9918382 -3.9594815 -3.9497828 -3.9679859 -4.0209413 -4.0833097 -4.1298132 -4.1675143 -4.1903682 -4.1871834][-4.1810446 -4.1865821 -4.1806755 -4.1606274 -4.1128192 -4.067543 -4.0336351 -4.0133109 -4.0155249 -4.0523477 -4.1015258 -4.136375 -4.1698389 -4.1967392 -4.1974058][-4.2050323 -4.2134361 -4.2138968 -4.2111316 -4.185482 -4.1491466 -4.1169596 -4.0989466 -4.1011662 -4.1208563 -4.1475983 -4.1699653 -4.1943669 -4.2088261 -4.2025251][-4.2270536 -4.2358208 -4.2420187 -4.2473431 -4.2359867 -4.2088718 -4.1781526 -4.1605573 -4.1683011 -4.18529 -4.200294 -4.2174563 -4.2360744 -4.2403588 -4.225894][-4.24098 -4.2492809 -4.2575445 -4.2638063 -4.2571073 -4.23782 -4.2162776 -4.201417 -4.2086778 -4.2264977 -4.238225 -4.2512293 -4.2686162 -4.271214 -4.2565117][-4.2518954 -4.2563224 -4.2599354 -4.2623019 -4.2563028 -4.2449684 -4.2323337 -4.2204127 -4.2213359 -4.2346797 -4.246079 -4.2600965 -4.2778196 -4.2839537 -4.2776179][-4.253757 -4.2580571 -4.2595882 -4.2593904 -4.2535634 -4.2482333 -4.2428284 -4.232059 -4.2216964 -4.2255325 -4.2346168 -4.2492142 -4.2675614 -4.2795405 -4.2835221]]...]
INFO - root - 2017-12-08 01:42:10.236465: step 60910, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 74h:15m:11s remains)
INFO - root - 2017-12-08 01:42:19.625072: step 60920, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 69h:44m:10s remains)
INFO - root - 2017-12-08 01:42:29.490204: step 60930, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.971 sec/batch; 73h:13m:54s remains)
INFO - root - 2017-12-08 01:42:39.128610: step 60940, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.013 sec/batch; 76h:23m:50s remains)
INFO - root - 2017-12-08 01:42:48.764651: step 60950, loss = 2.12, batch loss = 2.06 (8.3 examples/sec; 0.962 sec/batch; 72h:32m:00s remains)
INFO - root - 2017-12-08 01:42:58.363964: step 60960, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 69h:41m:55s remains)
INFO - root - 2017-12-08 01:43:08.046256: step 60970, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 71h:51m:27s remains)
INFO - root - 2017-12-08 01:43:17.720150: step 60980, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 74h:07m:13s remains)
INFO - root - 2017-12-08 01:43:27.349292: step 60990, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 72h:52m:13s remains)
INFO - root - 2017-12-08 01:43:37.135326: step 61000, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.977 sec/batch; 73h:42m:27s remains)
2017-12-08 01:43:38.114826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31467 -4.29564 -4.27317 -4.2520003 -4.2367778 -4.2360635 -4.247982 -4.2571411 -4.2678146 -4.2871456 -4.3019309 -4.3080707 -4.3098083 -4.3114252 -4.3137608][-4.3262243 -4.3131704 -4.2935967 -4.2727776 -4.2557631 -4.250227 -4.2568607 -4.263658 -4.272572 -4.2887487 -4.3009276 -4.3053923 -4.3059769 -4.3078384 -4.3116889][-4.3339324 -4.3267851 -4.3115239 -4.2917962 -4.2728119 -4.2637053 -4.2665815 -4.2723713 -4.2800851 -4.2923322 -4.3011966 -4.3045387 -4.3048468 -4.306684 -4.311132][-4.3334832 -4.32961 -4.316267 -4.2957587 -4.2741604 -4.2619882 -4.263145 -4.2708898 -4.2801752 -4.2910295 -4.2987428 -4.3027411 -4.3041449 -4.3065786 -4.3115244][-4.3254409 -4.3213248 -4.3068376 -4.283793 -4.2575917 -4.2379313 -4.2361135 -4.2492518 -4.2648282 -4.2781115 -4.2874 -4.2945881 -4.2989993 -4.3028808 -4.3084235][-4.3177118 -4.3108273 -4.2937541 -4.2678661 -4.2359915 -4.2072043 -4.200254 -4.2180266 -4.240922 -4.2577295 -4.2694082 -4.2809362 -4.2889271 -4.2947454 -4.301518][-4.3128209 -4.3038163 -4.2857165 -4.2602458 -4.228075 -4.1962333 -4.1834145 -4.1989274 -4.2238655 -4.2409921 -4.2528114 -4.2663636 -4.2770543 -4.2844553 -4.2917295][-4.30979 -4.3005581 -4.2839556 -4.2616243 -4.2338924 -4.2047043 -4.1878223 -4.1955371 -4.2153211 -4.2292213 -4.2395363 -4.253787 -4.26656 -4.2747903 -4.2819629][-4.3090081 -4.3005352 -4.2864804 -4.2680721 -4.2457509 -4.2207422 -4.2021251 -4.2013011 -4.2127237 -4.2227683 -4.2324538 -4.2473054 -4.2612281 -4.2699428 -4.2766171][-4.3110671 -4.3043852 -4.2936735 -4.2797394 -4.2632275 -4.2440481 -4.2268467 -4.2199078 -4.2233424 -4.22933 -4.2376504 -4.2510924 -4.2646341 -4.273385 -4.2796702][-4.3148861 -4.3107 -4.3031936 -4.2936921 -4.2824459 -4.2696514 -4.2567353 -4.2481847 -4.2463832 -4.2488952 -4.25532 -4.2653856 -4.2758422 -4.283309 -4.2892532][-4.3181653 -4.3162808 -4.3118234 -4.305953 -4.2986636 -4.2908235 -4.2825418 -4.2755852 -4.2725129 -4.2734942 -4.2777891 -4.2842808 -4.290957 -4.2962275 -4.3008718][-4.3206267 -4.3200226 -4.3176856 -4.3141427 -4.309474 -4.3046174 -4.3000126 -4.2961607 -4.2946234 -4.2957563 -4.2985983 -4.30212 -4.305757 -4.3086414 -4.3109179][-4.3219256 -4.3219132 -4.3207197 -4.3188295 -4.3161879 -4.3133616 -4.3111529 -4.3094482 -4.30925 -4.3104277 -4.3122649 -4.3140306 -4.3156662 -4.3168297 -4.3173494][-4.3226385 -4.3231163 -4.3225164 -4.3214869 -4.3199782 -4.3182893 -4.3169813 -4.3160958 -4.3162513 -4.3170447 -4.3179049 -4.3185825 -4.3192124 -4.3196387 -4.3197212]]...]
INFO - root - 2017-12-08 01:43:47.858353: step 61010, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.013 sec/batch; 76h:22m:29s remains)
INFO - root - 2017-12-08 01:43:57.419300: step 61020, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 72h:42m:52s remains)
INFO - root - 2017-12-08 01:44:07.044495: step 61030, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 72h:34m:15s remains)
INFO - root - 2017-12-08 01:44:16.555794: step 61040, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 72h:07m:23s remains)
INFO - root - 2017-12-08 01:44:26.411816: step 61050, loss = 2.10, batch loss = 2.04 (7.8 examples/sec; 1.025 sec/batch; 77h:15m:24s remains)
INFO - root - 2017-12-08 01:44:36.152989: step 61060, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 71h:23m:40s remains)
INFO - root - 2017-12-08 01:44:45.669846: step 61070, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.967 sec/batch; 72h:52m:49s remains)
INFO - root - 2017-12-08 01:44:55.406983: step 61080, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 71h:33m:43s remains)
INFO - root - 2017-12-08 01:45:04.945717: step 61090, loss = 2.08, batch loss = 2.02 (8.0 examples/sec; 0.995 sec/batch; 74h:59m:09s remains)
INFO - root - 2017-12-08 01:45:14.783866: step 61100, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.001 sec/batch; 75h:28m:26s remains)
2017-12-08 01:45:15.756084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3322568 -4.3306541 -4.3264046 -4.3200922 -4.315125 -4.3132429 -4.314559 -4.3168921 -4.3182135 -4.3175931 -4.3160453 -4.312624 -4.3072362 -4.2984576 -4.2843719][-4.3325915 -4.3296 -4.3217006 -4.3119216 -4.3041677 -4.302289 -4.3072124 -4.3143044 -4.3199687 -4.3222489 -4.3216009 -4.3181243 -4.3126845 -4.3044167 -4.2912145][-4.3256426 -4.3194327 -4.3056278 -4.2888465 -4.2752552 -4.2719264 -4.2817035 -4.2975111 -4.3129225 -4.3229594 -4.3264623 -4.324821 -4.3204966 -4.3136988 -4.3029127][-4.3111081 -4.2992887 -4.2749157 -4.2449064 -4.2209282 -4.2141995 -4.228663 -4.2557611 -4.2862387 -4.3097382 -4.3218751 -4.3252625 -4.3249965 -4.3218541 -4.3144279][-4.28138 -4.2628212 -4.2256083 -4.1816325 -4.14829 -4.1376185 -4.1535559 -4.18798 -4.2313175 -4.2693181 -4.2922983 -4.3038726 -4.3101916 -4.3114567 -4.3076334][-4.2351093 -4.2141671 -4.1678219 -4.1137667 -4.0722604 -4.0573888 -4.0705242 -4.1055689 -4.1544385 -4.2015133 -4.2340174 -4.25642 -4.2714052 -4.2760091 -4.2736735][-4.1808448 -4.1656084 -4.118794 -4.0596418 -4.0119281 -3.9902911 -3.993856 -4.0247369 -4.0745592 -4.1241016 -4.1621008 -4.19087 -4.2081809 -4.2093782 -4.203465][-4.143199 -4.1399713 -4.1029372 -4.0493264 -3.9996784 -3.9669261 -3.9539323 -3.9740248 -4.0169826 -4.06507 -4.1067948 -4.136848 -4.1457334 -4.131608 -4.1153893][-4.1447215 -4.157258 -4.1386127 -4.099133 -4.0530744 -4.0129366 -3.9893546 -3.9994106 -4.0317159 -4.0701852 -4.1075358 -4.1319065 -4.1278424 -4.0968022 -4.0730977][-4.1942267 -4.2123275 -4.2082033 -4.1848288 -4.1516047 -4.11993 -4.1025577 -4.1100845 -4.1302972 -4.1542349 -4.1788888 -4.1924963 -4.1780133 -4.1411557 -4.1123948][-4.2625031 -4.2776346 -4.2792163 -4.2666698 -4.2463422 -4.2268825 -4.2149162 -4.2183161 -4.2290606 -4.2431211 -4.2589436 -4.2673159 -4.2553067 -4.2265043 -4.2030277][-4.3087993 -4.3176565 -4.3194337 -4.3116331 -4.2986088 -4.284771 -4.2728143 -4.2676373 -4.2690616 -4.2778821 -4.292326 -4.303647 -4.3015079 -4.2866755 -4.2741714][-4.3315492 -4.3354192 -4.3352604 -4.3247881 -4.3077183 -4.2877121 -4.2649112 -4.2456317 -4.2402363 -4.253201 -4.2796683 -4.3046641 -4.3159404 -4.3136606 -4.3075552][-4.3377137 -4.3383923 -4.3342714 -4.3147259 -4.2815647 -4.2403479 -4.195982 -4.159802 -4.1495228 -4.1740971 -4.2227707 -4.268033 -4.2954421 -4.3031106 -4.2976465][-4.3228254 -4.3192949 -4.3087211 -4.2761512 -4.2218652 -4.15431 -4.0876794 -4.0372219 -4.0230675 -4.06068 -4.1333137 -4.2022891 -4.2475462 -4.2636213 -4.25488]]...]
INFO - root - 2017-12-08 01:45:25.371825: step 61110, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.968 sec/batch; 72h:57m:00s remains)
INFO - root - 2017-12-08 01:45:34.899639: step 61120, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 73h:49m:06s remains)
INFO - root - 2017-12-08 01:45:44.512172: step 61130, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 0.996 sec/batch; 75h:05m:01s remains)
INFO - root - 2017-12-08 01:45:54.293449: step 61140, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 69h:01m:25s remains)
INFO - root - 2017-12-08 01:46:03.945679: step 61150, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 69h:58m:51s remains)
INFO - root - 2017-12-08 01:46:13.479826: step 61160, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.982 sec/batch; 73h:59m:05s remains)
INFO - root - 2017-12-08 01:46:23.099232: step 61170, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.983 sec/batch; 74h:06m:42s remains)
INFO - root - 2017-12-08 01:46:32.773866: step 61180, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 72h:21m:37s remains)
INFO - root - 2017-12-08 01:46:42.492654: step 61190, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 72h:04m:08s remains)
INFO - root - 2017-12-08 01:46:52.107682: step 61200, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.017 sec/batch; 76h:39m:34s remains)
2017-12-08 01:46:53.118937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2390194 -4.2616034 -4.2777314 -4.266818 -4.212749 -4.1465435 -4.1043644 -4.1097097 -4.1520362 -4.2023263 -4.24943 -4.2884479 -4.3054957 -4.3069897 -4.3113446][-4.2258649 -4.2484989 -4.2633443 -4.2440286 -4.1794524 -4.0986743 -4.050703 -4.0645742 -4.1138849 -4.1693373 -4.2322426 -4.2841196 -4.3058476 -4.3062558 -4.3093338][-4.2007661 -4.2181363 -4.2290387 -4.2026181 -4.1299896 -4.0347548 -3.9818392 -4.0071688 -4.0692105 -4.1345763 -4.2134485 -4.2765017 -4.3029003 -4.3046346 -4.30782][-4.1821022 -4.1888223 -4.1951303 -4.1683273 -4.0926309 -3.9921498 -3.9401486 -3.9736621 -4.0436106 -4.1165037 -4.2007065 -4.2685308 -4.29862 -4.3022442 -4.3054061][-4.1713762 -4.1717091 -4.1712937 -4.1466684 -4.0791464 -3.9878759 -3.9459472 -3.9855049 -4.0472212 -4.1154275 -4.1942992 -4.26037 -4.2925067 -4.2982759 -4.3023229][-4.1664524 -4.1673312 -4.1627655 -4.1407337 -4.0848708 -4.0010352 -3.9680831 -4.0130868 -4.0619736 -4.1212382 -4.1930556 -4.2557664 -4.2890887 -4.2956753 -4.3006873][-4.163619 -4.1665707 -4.1608486 -4.13902 -4.0864449 -4.0004191 -3.969373 -4.0146103 -4.0557137 -4.1165872 -4.1896019 -4.2530847 -4.2881041 -4.2955661 -4.3005266][-4.1367278 -4.1449852 -4.1429205 -4.1197448 -4.0640388 -3.9683516 -3.9298518 -3.9663155 -4.0129628 -4.0939236 -4.1790004 -4.2469168 -4.2844644 -4.2935324 -4.3003178][-4.1121774 -4.1207371 -4.1261659 -4.1114845 -4.057198 -3.9543092 -3.9101558 -3.9369445 -3.9861825 -4.0823293 -4.1744628 -4.2414269 -4.2790837 -4.289854 -4.2996378][-4.1083231 -4.1135445 -4.1241941 -4.1194954 -4.0717673 -3.9761441 -3.9428589 -3.9703867 -4.0115833 -4.093864 -4.176743 -4.239594 -4.2764034 -4.2876916 -4.2989559][-4.1288333 -4.1330767 -4.1427474 -4.1404338 -4.0948057 -4.01026 -3.9922671 -4.0216727 -4.0524139 -4.1146712 -4.1834316 -4.2407274 -4.2766666 -4.2872963 -4.2987571][-4.1544008 -4.1576157 -4.1648674 -4.159924 -4.1081 -4.0275264 -4.0233488 -4.0586472 -4.084806 -4.1350703 -4.191062 -4.24455 -4.2798305 -4.2889915 -4.2996984][-4.18289 -4.1899347 -4.1973042 -4.183351 -4.1153326 -4.0242329 -4.0244727 -4.0669813 -4.1011462 -4.1528792 -4.2041407 -4.2524576 -4.2845445 -4.29173 -4.3008113][-4.20192 -4.2087064 -4.2173514 -4.1966934 -4.1227527 -4.0288358 -4.0245748 -4.0659943 -4.1093788 -4.1675482 -4.2197022 -4.2627058 -4.2896266 -4.2949753 -4.3020635][-4.209518 -4.2159853 -4.2226748 -4.2014127 -4.1360712 -4.0558319 -4.0496917 -4.081357 -4.1216569 -4.1770368 -4.2282763 -4.2684717 -4.2933693 -4.2982631 -4.303647]]...]
INFO - root - 2017-12-08 01:47:02.839173: step 61210, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.969 sec/batch; 73h:02m:14s remains)
INFO - root - 2017-12-08 01:47:12.558326: step 61220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 71h:17m:54s remains)
INFO - root - 2017-12-08 01:47:22.108022: step 61230, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 75h:00m:23s remains)
INFO - root - 2017-12-08 01:47:31.634596: step 61240, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 72h:24m:31s remains)
INFO - root - 2017-12-08 01:47:41.411336: step 61250, loss = 2.07, batch loss = 2.02 (8.0 examples/sec; 0.998 sec/batch; 75h:10m:19s remains)
INFO - root - 2017-12-08 01:47:51.028065: step 61260, loss = 2.09, batch loss = 2.03 (8.2 examples/sec; 0.979 sec/batch; 73h:47m:02s remains)
INFO - root - 2017-12-08 01:48:00.600673: step 61270, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 72h:33m:38s remains)
INFO - root - 2017-12-08 01:48:10.478878: step 61280, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.018 sec/batch; 76h:42m:21s remains)
INFO - root - 2017-12-08 01:48:19.951416: step 61290, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 73h:34m:17s remains)
INFO - root - 2017-12-08 01:48:29.585420: step 61300, loss = 2.08, batch loss = 2.03 (8.1 examples/sec; 0.991 sec/batch; 74h:39m:02s remains)
2017-12-08 01:48:30.557958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2849894 -4.2682433 -4.2568493 -4.2616425 -4.283299 -4.3054481 -4.3088021 -4.2922339 -4.2835908 -4.2822657 -4.2659283 -4.2356005 -4.2126064 -4.2061486 -4.2091765][-4.2816944 -4.2650871 -4.2575135 -4.2668118 -4.2866158 -4.3008981 -4.293716 -4.2740903 -4.26977 -4.2759581 -4.2722492 -4.2513561 -4.2319784 -4.22061 -4.2148352][-4.27916 -4.2656136 -4.2621841 -4.2720785 -4.2860951 -4.2886949 -4.27159 -4.2504249 -4.2492161 -4.2653885 -4.2769494 -4.2711129 -4.25905 -4.2442665 -4.2306352][-4.2803044 -4.2708054 -4.2704206 -4.2788277 -4.2838902 -4.273138 -4.2439489 -4.2185397 -4.2197008 -4.2457705 -4.271668 -4.2856417 -4.2827234 -4.2654953 -4.2416568][-4.283649 -4.2774611 -4.27748 -4.2802768 -4.2724028 -4.2435889 -4.1977005 -4.1630235 -4.1690993 -4.2117872 -4.2552905 -4.2835693 -4.2878437 -4.26839 -4.2367887][-4.2887168 -4.2866387 -4.2855124 -4.2780204 -4.2536564 -4.2019973 -4.1331749 -4.0810804 -4.0980134 -4.1687484 -4.2322311 -4.2665896 -4.2735682 -4.2518139 -4.2173939][-4.2960367 -4.3016405 -4.3015394 -4.28778 -4.24934 -4.17401 -4.0767965 -4.0027308 -4.0379496 -4.1356974 -4.2064557 -4.2386475 -4.2459607 -4.224206 -4.1969194][-4.3040037 -4.3158131 -4.3173089 -4.3034315 -4.26254 -4.1826425 -4.0787568 -4.0063581 -4.04789 -4.136394 -4.1901579 -4.2096257 -4.2162995 -4.2059479 -4.1979847][-4.3087535 -4.3197222 -4.3173523 -4.3039923 -4.2732782 -4.2111125 -4.1283636 -4.0829268 -4.1156931 -4.1659331 -4.187861 -4.1900711 -4.197072 -4.20627 -4.2204537][-4.3016133 -4.3033319 -4.2946377 -4.2831631 -4.2689791 -4.2348905 -4.180769 -4.1520615 -4.1722326 -4.1949854 -4.1976185 -4.1909914 -4.2012892 -4.2211604 -4.2452407][-4.2905154 -4.2809548 -4.2635112 -4.2518888 -4.2532067 -4.24239 -4.2116265 -4.1930213 -4.2069273 -4.2198467 -4.2169418 -4.2068248 -4.216723 -4.2373137 -4.2610192][-4.2768393 -4.2552819 -4.2298179 -4.2156768 -4.2225146 -4.2230978 -4.2106214 -4.2071257 -4.2254095 -4.2408514 -4.2364244 -4.2231479 -4.2267108 -4.2390304 -4.2562928][-4.2602434 -4.238224 -4.2161713 -4.2014561 -4.2044497 -4.2055359 -4.204299 -4.2105956 -4.231884 -4.2499676 -4.2484918 -4.2326479 -4.2283554 -4.2295122 -4.2363153][-4.2461262 -4.2347355 -4.227128 -4.2161283 -4.2088518 -4.2034416 -4.2046485 -4.2123823 -4.231945 -4.251009 -4.2521563 -4.2366018 -4.2289405 -4.2254786 -4.2238107][-4.2261462 -4.2262373 -4.2353544 -4.2328739 -4.2216372 -4.2124896 -4.2121768 -4.2165003 -4.2293658 -4.2441549 -4.2454853 -4.2338405 -4.23015 -4.2303166 -4.2289791]]...]
INFO - root - 2017-12-08 01:48:40.227493: step 61310, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.979 sec/batch; 73h:45m:57s remains)
INFO - root - 2017-12-08 01:48:49.910201: step 61320, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.994 sec/batch; 74h:50m:55s remains)
INFO - root - 2017-12-08 01:48:59.519626: step 61330, loss = 2.10, batch loss = 2.04 (8.3 examples/sec; 0.964 sec/batch; 72h:38m:49s remains)
INFO - root - 2017-12-08 01:49:09.212403: step 61340, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.993 sec/batch; 74h:47m:53s remains)
INFO - root - 2017-12-08 01:49:19.022574: step 61350, loss = 2.06, batch loss = 2.00 (8.1 examples/sec; 0.988 sec/batch; 74h:24m:49s remains)
INFO - root - 2017-12-08 01:49:28.558980: step 61360, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.983 sec/batch; 74h:00m:31s remains)
INFO - root - 2017-12-08 01:49:38.154493: step 61370, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 71h:48m:33s remains)
INFO - root - 2017-12-08 01:49:47.940486: step 61380, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 72h:52m:44s remains)
INFO - root - 2017-12-08 01:49:57.589663: step 61390, loss = 2.13, batch loss = 2.07 (8.2 examples/sec; 0.974 sec/batch; 73h:19m:52s remains)
INFO - root - 2017-12-08 01:50:07.187277: step 61400, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.955 sec/batch; 71h:55m:15s remains)
2017-12-08 01:50:08.109209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2499437 -4.2419157 -4.2375379 -4.2395449 -4.2434063 -4.2437053 -4.24689 -4.2573357 -4.26398 -4.2671628 -4.2697191 -4.2711687 -4.2679243 -4.2651038 -4.2687287][-4.230072 -4.2219458 -4.2173982 -4.2192645 -4.2236986 -4.2261291 -4.2338481 -4.2482429 -4.2577372 -4.2636471 -4.2677684 -4.2703853 -4.2687216 -4.265583 -4.266871][-4.1964955 -4.1875067 -4.1819038 -4.18236 -4.1874232 -4.1953969 -4.2128296 -4.2345471 -4.2485156 -4.2568445 -4.2622585 -4.2663531 -4.2679577 -4.2667117 -4.2665973][-4.1531458 -4.1447725 -4.1390533 -4.136929 -4.1410685 -4.1535692 -4.1812811 -4.2122741 -4.2301822 -4.2369604 -4.238965 -4.2406855 -4.2444534 -4.2461939 -4.24706][-4.11755 -4.1121349 -4.1076212 -4.1022363 -4.0998526 -4.1090379 -4.1389441 -4.1733289 -4.192986 -4.19756 -4.1937127 -4.1894159 -4.1926551 -4.1970959 -4.2005324][-4.1031308 -4.0986643 -4.0936456 -4.0824184 -4.0656514 -4.0608048 -4.0804796 -4.1095638 -4.1318393 -4.1392007 -4.1347933 -4.1271038 -4.1269031 -4.13225 -4.1375008][-4.1184192 -4.1086912 -4.0988832 -4.0797391 -4.0467849 -4.0227051 -4.0194597 -4.0323124 -4.0600395 -4.0770888 -4.0791144 -4.0729489 -4.0706315 -4.0722613 -4.0749645][-4.1633987 -4.1477489 -4.1315742 -4.1072974 -4.0679817 -4.0315361 -4.0035748 -3.9910574 -4.0143652 -4.0359144 -4.043664 -4.0435715 -4.0414524 -4.0383439 -4.0330267][-4.2089477 -4.1935 -4.1784306 -4.1577826 -4.1251516 -4.0914326 -4.05753 -4.0320444 -4.0387139 -4.0482297 -4.0487132 -4.0483623 -4.0473819 -4.0395451 -4.0259948][-4.2329063 -4.2232275 -4.2143307 -4.2029657 -4.184113 -4.1624265 -4.1377482 -4.1127715 -4.104301 -4.1004205 -4.0927362 -4.088665 -4.0869117 -4.0777693 -4.0611019][-4.2292309 -4.2255425 -4.2218986 -4.2183728 -4.2122908 -4.2040873 -4.1907635 -4.1712241 -4.1566992 -4.1501455 -4.1433 -4.1384063 -4.1367188 -4.1293139 -4.1147447][-4.2134938 -4.2116756 -4.2098341 -4.2097807 -4.2097931 -4.20948 -4.20552 -4.1921396 -4.1778769 -4.1736078 -4.1733432 -4.1717358 -4.172 -4.1691656 -4.1601377][-4.2042141 -4.2008247 -4.19919 -4.2006316 -4.2026334 -4.2045193 -4.2066069 -4.1993637 -4.1871786 -4.1850357 -4.1894155 -4.1901422 -4.1915007 -4.1919789 -4.1882181][-4.2083945 -4.2029715 -4.200346 -4.2027025 -4.2051835 -4.2068691 -4.2119241 -4.2098012 -4.2008781 -4.1998296 -4.2047973 -4.2042041 -4.2037659 -4.2043595 -4.2031393][-4.2273369 -4.2224016 -4.2208867 -4.2239394 -4.2265325 -4.2280073 -4.2336521 -4.2348723 -4.22869 -4.2270641 -4.2296271 -4.2264996 -4.2230582 -4.2218075 -4.2204809]]...]
INFO - root - 2017-12-08 01:50:17.746597: step 61410, loss = 2.09, batch loss = 2.04 (8.3 examples/sec; 0.965 sec/batch; 72h:39m:27s remains)
INFO - root - 2017-12-08 01:50:27.289358: step 61420, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 72h:31m:38s remains)
INFO - root - 2017-12-08 01:50:36.818295: step 61430, loss = 2.10, batch loss = 2.04 (8.1 examples/sec; 0.989 sec/batch; 74h:27m:57s remains)
INFO - root - 2017-12-08 01:50:46.491864: step 61440, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 69h:56m:21s remains)
INFO - root - 2017-12-08 01:50:56.205653: step 61450, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.009 sec/batch; 75h:56m:16s remains)
INFO - root - 2017-12-08 01:51:05.916733: step 61460, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.983 sec/batch; 74h:01m:53s remains)
INFO - root - 2017-12-08 01:51:15.497898: step 61470, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 69h:15m:27s remains)
INFO - root - 2017-12-08 01:51:25.193065: step 61480, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 1.002 sec/batch; 75h:28m:02s remains)
INFO - root - 2017-12-08 01:51:35.029894: step 61490, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.953 sec/batch; 71h:43m:10s remains)
INFO - root - 2017-12-08 01:51:44.689049: step 61500, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.010 sec/batch; 76h:02m:04s remains)
2017-12-08 01:51:45.650850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3191223 -4.3100266 -4.3041344 -4.3024597 -4.3017864 -4.3051267 -4.3067007 -4.3075995 -4.3142934 -4.3222375 -4.3253775 -4.3215661 -4.3140583 -4.3086448 -4.3075471][-4.3068871 -4.2983794 -4.2936511 -4.2914991 -4.2900276 -4.2928119 -4.29296 -4.2924495 -4.3024383 -4.3160925 -4.320498 -4.3159823 -4.30566 -4.2987103 -4.2968516][-4.2937989 -4.2825351 -4.2694392 -4.2594433 -4.2538013 -4.25162 -4.2498403 -4.2533031 -4.2724657 -4.293612 -4.2971654 -4.2914391 -4.2785196 -4.2734938 -4.2728372][-4.28096 -4.2609377 -4.2329135 -4.2096491 -4.1985264 -4.1913781 -4.1832237 -4.1861563 -4.2136445 -4.2412996 -4.246984 -4.2392831 -4.2261257 -4.2277837 -4.2304516][-4.264451 -4.233685 -4.1923294 -4.1582985 -4.1423426 -4.1317124 -4.1091466 -4.0948696 -4.1188707 -4.156671 -4.1678939 -4.1625671 -4.1552896 -4.1640339 -4.1696725][-4.2450104 -4.2050691 -4.1534705 -4.1107678 -4.0880919 -4.0682483 -4.021533 -3.978076 -3.9902465 -4.0457249 -4.0772467 -4.0818386 -4.0813289 -4.0973644 -4.1049275][-4.2362137 -4.1909242 -4.1322365 -4.074965 -4.0339293 -3.9979777 -3.920701 -3.8291605 -3.8231237 -3.9174871 -3.9903636 -4.0212579 -4.0320449 -4.0507522 -4.0585113][-4.2436867 -4.2015 -4.1487517 -4.0913196 -4.0430202 -3.9931877 -3.8991029 -3.7660382 -3.7472382 -3.8785162 -3.9798086 -4.0257969 -4.0434456 -4.0528159 -4.0553417][-4.2585034 -4.2278337 -4.193594 -4.1541338 -4.1185741 -4.0745726 -3.9989867 -3.894475 -3.8898153 -3.9856541 -4.0580935 -4.0936117 -4.1073151 -4.1051702 -4.0941777][-4.2709041 -4.2527032 -4.2363706 -4.2112274 -4.1848478 -4.1523824 -4.1082573 -4.05554 -4.0619264 -4.1163073 -4.15425 -4.1752977 -4.1837492 -4.1740632 -4.1516089][-4.2814722 -4.2686968 -4.2639503 -4.2499433 -4.2314472 -4.2123203 -4.1953206 -4.1755195 -4.1817918 -4.2112594 -4.2295818 -4.2426257 -4.2449074 -4.2335863 -4.2130218][-4.2933578 -4.2813969 -4.2761893 -4.2709589 -4.2638412 -4.2579923 -4.2577486 -4.2536507 -4.255569 -4.2661114 -4.2726922 -4.2796235 -4.2800293 -4.2731309 -4.2610049][-4.2994523 -4.2869382 -4.278604 -4.2766733 -4.2814631 -4.2858229 -4.2923436 -4.2934384 -4.2914481 -4.290082 -4.2888131 -4.29117 -4.2909527 -4.28807 -4.2833605][-4.3061833 -4.2959185 -4.2891083 -4.2897496 -4.2962151 -4.3011408 -4.3062482 -4.3077111 -4.3039813 -4.2992435 -4.2954793 -4.2953053 -4.2949762 -4.294137 -4.2930069][-4.3186021 -4.3112297 -4.30707 -4.3083196 -4.3103266 -4.3106556 -4.3119416 -4.3124051 -4.3108273 -4.3078079 -4.30656 -4.3077841 -4.3081641 -4.3067188 -4.3049927]]...]
INFO - root - 2017-12-08 01:51:55.380603: step 61510, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.970 sec/batch; 73h:01m:37s remains)
INFO - root - 2017-12-08 01:52:05.007887: step 61520, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 74h:07m:12s remains)
INFO - root - 2017-12-08 01:52:14.658559: step 61530, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.921 sec/batch; 69h:20m:11s remains)
INFO - root - 2017-12-08 01:52:24.391492: step 61540, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 71h:33m:42s remains)
INFO - root - 2017-12-08 01:52:33.945253: step 61550, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.969 sec/batch; 72h:55m:57s remains)
INFO - root - 2017-12-08 01:52:43.640632: step 61560, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 70h:30m:38s remains)
INFO - root - 2017-12-08 01:52:53.291095: step 61570, loss = 2.09, batch loss = 2.03 (8.0 examples/sec; 1.002 sec/batch; 75h:24m:00s remains)
INFO - root - 2017-12-08 01:53:03.039619: step 61580, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.045 sec/batch; 78h:36m:16s remains)
INFO - root - 2017-12-08 01:53:12.568004: step 61590, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.972 sec/batch; 73h:07m:38s remains)
INFO - root - 2017-12-08 01:53:22.228667: step 61600, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 69h:39m:58s remains)
2017-12-08 01:53:23.317222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3071876 -4.3040471 -4.303812 -4.3022251 -4.2962055 -4.28277 -4.2654643 -4.2409391 -4.2132635 -4.2018604 -4.2096496 -4.2139888 -4.22237 -4.2297335 -4.2273722][-4.2979274 -4.2958446 -4.2949953 -4.2917056 -4.2835608 -4.26945 -4.2570891 -4.24724 -4.2374096 -4.2344346 -4.2427959 -4.2453723 -4.244494 -4.2405143 -4.2295747][-4.26707 -4.2623825 -4.2588558 -4.2533965 -4.2418256 -4.225184 -4.2170892 -4.2250891 -4.2401528 -4.2495637 -4.2594948 -4.2618761 -4.2569723 -4.24683 -4.2327948][-4.2121282 -4.2040887 -4.19919 -4.1936908 -4.1793818 -4.154582 -4.1409249 -4.1623545 -4.2019587 -4.2241282 -4.2392941 -4.2489085 -4.2525668 -4.2474246 -4.2391181][-4.1474552 -4.1412106 -4.1407814 -4.1338382 -4.1082249 -4.0621586 -4.0310369 -4.06267 -4.1271439 -4.1694603 -4.197731 -4.2206764 -4.2394333 -4.2493744 -4.2533722][-4.082716 -4.0818272 -4.0880804 -4.0791068 -4.0377803 -3.9612703 -3.9003782 -3.9355755 -4.0272927 -4.0987678 -4.1485505 -4.1870775 -4.2233229 -4.25077 -4.2688937][-4.0387573 -4.041399 -4.0472178 -4.0290952 -3.9718835 -3.8729374 -3.78354 -3.8129234 -3.9300165 -4.0334959 -4.108139 -4.1599193 -4.2059555 -4.2435703 -4.2711568][-4.0538344 -4.059094 -4.0574336 -4.0299764 -3.9696114 -3.8706784 -3.7740803 -3.7802155 -3.8825772 -3.9908776 -4.0770478 -4.1374626 -4.1846728 -4.2194338 -4.2449222][-4.1249208 -4.1259093 -4.1140475 -4.085525 -4.0387583 -3.9646127 -3.8826053 -3.8612251 -3.9142089 -3.9889467 -4.0626769 -4.1241984 -4.1706138 -4.1952672 -4.2091608][-4.2135544 -4.2102389 -4.1909094 -4.1632357 -4.1294637 -4.0808663 -4.0216432 -3.9938169 -4.008791 -4.0456734 -4.0930462 -4.1422944 -4.183032 -4.1960278 -4.1929603][-4.2826495 -4.2799211 -4.2624383 -4.2379823 -4.2121024 -4.1831245 -4.1461005 -4.1244664 -4.1253972 -4.1420579 -4.1684613 -4.2006016 -4.2284107 -4.2324033 -4.2171454][-4.313448 -4.3128848 -4.3046951 -4.2918096 -4.27616 -4.2604265 -4.2396603 -4.2274485 -4.227365 -4.2364731 -4.2497854 -4.267231 -4.2801886 -4.2774677 -4.2593117][-4.3163447 -4.3158269 -4.3162994 -4.3161359 -4.3134212 -4.3086009 -4.2998476 -4.2956333 -4.2955008 -4.3002963 -4.3064022 -4.3143334 -4.3183265 -4.3132639 -4.2966285][-4.3066735 -4.30261 -4.3046908 -4.3096313 -4.3145676 -4.31956 -4.3208122 -4.3229761 -4.32568 -4.3300214 -4.3340406 -4.3377361 -4.3382668 -4.3345737 -4.3217549][-4.2975073 -4.2911139 -4.290833 -4.2942424 -4.2995 -4.3061428 -4.3116851 -4.3169842 -4.3227148 -4.3271689 -4.3303018 -4.332849 -4.33498 -4.335165 -4.3259239]]...]
INFO - root - 2017-12-08 01:53:33.156817: step 61610, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 70h:17m:30s remains)
INFO - root - 2017-12-08 01:53:42.700899: step 61620, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 69h:57m:55s remains)
INFO - root - 2017-12-08 01:53:52.293375: step 61630, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 72h:23m:32s remains)
INFO - root - 2017-12-08 01:54:01.911678: step 61640, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 69h:59m:38s remains)
INFO - root - 2017-12-08 01:54:11.607975: step 61650, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 70h:38m:42s remains)
INFO - root - 2017-12-08 01:54:21.379995: step 61660, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 72h:21m:11s remains)
INFO - root - 2017-12-08 01:54:31.137763: step 61670, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.982 sec/batch; 73h:50m:36s remains)
INFO - root - 2017-12-08 01:54:40.770810: step 61680, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 72h:30m:20s remains)
INFO - root - 2017-12-08 01:54:50.684841: step 61690, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 73h:32m:40s remains)
INFO - root - 2017-12-08 01:55:00.295754: step 61700, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 68h:39m:21s remains)
2017-12-08 01:55:01.241871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2779403 -4.3007183 -4.31103 -4.3090615 -4.2974982 -4.2857051 -4.2738075 -4.2592683 -4.2484288 -4.2388215 -4.2235546 -4.1999168 -4.1719022 -4.1586361 -4.170877][-4.2539558 -4.2797456 -4.2945132 -4.2984467 -4.2917151 -4.2863193 -4.2784376 -4.2728662 -4.2701359 -4.2639842 -4.2478585 -4.2222 -4.1892819 -4.1683178 -4.1691208][-4.2479787 -4.2687721 -4.2819362 -4.2874222 -4.2798195 -4.2714272 -4.2631927 -4.2706561 -4.2862964 -4.2890325 -4.2737384 -4.2506123 -4.2188258 -4.195375 -4.1914506][-4.2447748 -4.2573628 -4.2644043 -4.26764 -4.2539706 -4.2379704 -4.2320709 -4.2556982 -4.2936311 -4.3107357 -4.3021874 -4.2845631 -4.2582183 -4.2359848 -4.2265573][-4.2400222 -4.2429323 -4.2368298 -4.2270317 -4.1995592 -4.1738434 -4.1732736 -4.21747 -4.2750573 -4.3047342 -4.306725 -4.2978816 -4.2815204 -4.2629251 -4.2516351][-4.2331033 -4.2269354 -4.2061796 -4.1804776 -4.1395197 -4.1060429 -4.1118469 -4.1745386 -4.2444787 -4.2792492 -4.2866135 -4.2821374 -4.27277 -4.2607164 -4.2540727][-4.2255416 -4.2138343 -4.1826448 -4.1440725 -4.0916133 -4.0480838 -4.0578885 -4.1343536 -4.21198 -4.2477565 -4.2527852 -4.2496939 -4.2468839 -4.247108 -4.2500129][-4.2197948 -4.2015619 -4.1631861 -4.117394 -4.0575404 -4.0042577 -4.0113115 -4.0902886 -4.1674871 -4.2011971 -4.2040968 -4.201108 -4.205596 -4.2217851 -4.2378654][-4.2344031 -4.2152157 -4.17811 -4.1355071 -4.0810266 -4.0294681 -4.03225 -4.0926518 -4.1515388 -4.1773462 -4.1809726 -4.1811738 -4.1909428 -4.2119322 -4.2323689][-4.2641587 -4.2502432 -4.22177 -4.1904287 -4.1518617 -4.1127467 -4.1113214 -4.1468234 -4.1799045 -4.1948414 -4.2019858 -4.2096648 -4.2224865 -4.2374692 -4.2487693][-4.2674251 -4.258008 -4.2377467 -4.219408 -4.1995935 -4.1781573 -4.1762366 -4.1898651 -4.198844 -4.204195 -4.2134504 -4.2248216 -4.2373857 -4.2428889 -4.2424707][-4.2528925 -4.2495565 -4.2410688 -4.2368608 -4.2298374 -4.2177181 -4.2129602 -4.2081652 -4.1968036 -4.1948786 -4.2071991 -4.2210569 -4.2333517 -4.2341394 -4.2279344][-4.226368 -4.2267489 -4.2247267 -4.2263565 -4.225666 -4.2192116 -4.2144494 -4.2020993 -4.1813941 -4.1762886 -4.189065 -4.2014308 -4.2121282 -4.2140517 -4.2111239][-4.2009974 -4.2013435 -4.1997318 -4.1999631 -4.2005625 -4.1984682 -4.1966352 -4.1849246 -4.1662493 -4.1611319 -4.1704988 -4.179204 -4.1881032 -4.1941605 -4.1999435][-4.2074404 -4.2053685 -4.2018867 -4.1987443 -4.1979575 -4.1983557 -4.1989036 -4.1914158 -4.1782732 -4.1749883 -4.1790175 -4.1815968 -4.18804 -4.1958847 -4.206748]]...]
INFO - root - 2017-12-08 01:55:10.778499: step 61710, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 71h:16m:45s remains)
INFO - root - 2017-12-08 01:55:20.314299: step 61720, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 69h:46m:22s remains)
INFO - root - 2017-12-08 01:55:30.177666: step 61730, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.982 sec/batch; 73h:53m:04s remains)
INFO - root - 2017-12-08 01:55:39.848150: step 61740, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 71h:08m:30s remains)
INFO - root - 2017-12-08 01:55:49.551170: step 61750, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.999 sec/batch; 75h:07m:45s remains)
INFO - root - 2017-12-08 01:55:58.918143: step 61760, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.984 sec/batch; 73h:59m:56s remains)
INFO - root - 2017-12-08 01:56:08.517064: step 61770, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 69h:51m:41s remains)
INFO - root - 2017-12-08 01:56:18.225845: step 61780, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 72h:53m:32s remains)
