INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "39"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
sdfah Tensor("siamese_fc/conv5/def/offset2/BiasAdd:0", shape=(8, 8, 8, 72), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
sdfah Tensor("siamese_fc_1/conv5/def/offset2/BiasAdd:0", shape=(8, 22, 22, 72), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-03 06:37:09.556175: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:37:09.556216: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:37:09.556223: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:37:09.556227: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:37:09.556258: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 06:37:10.219103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-03 06:37:10.219143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-03 06:37:10.219150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-03 06:37:10.219159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-03 06:37:13.132427: step 0, loss = 1.12, batch loss = 1.03 (3.6 examples/sec; 2.213 sec/batch; 204h:21m:51s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-03 06:37:15.558029: step 10, loss = 1.00, batch loss = 0.91 (45.6 examples/sec; 0.175 sec/batch; 16h:11m:15s remains)
INFO - root - 2017-12-03 06:37:17.329109: step 20, loss = 1.18, batch loss = 1.08 (44.5 examples/sec; 0.180 sec/batch; 16h:36m:50s remains)
INFO - root - 2017-12-03 06:37:19.116893: step 30, loss = 1.08, batch loss = 0.98 (45.8 examples/sec; 0.175 sec/batch; 16h:07m:28s remains)
INFO - root - 2017-12-03 06:37:20.880050: step 40, loss = 1.09, batch loss = 0.99 (45.8 examples/sec; 0.175 sec/batch; 16h:07m:12s remains)
INFO - root - 2017-12-03 06:37:22.680150: step 50, loss = 1.06, batch loss = 0.97 (42.6 examples/sec; 0.188 sec/batch; 17h:20m:17s remains)
INFO - root - 2017-12-03 06:37:24.470939: step 60, loss = 1.06, batch loss = 0.96 (43.8 examples/sec; 0.183 sec/batch; 16h:51m:30s remains)
INFO - root - 2017-12-03 06:37:26.265495: step 70, loss = 1.06, batch loss = 0.96 (46.2 examples/sec; 0.173 sec/batch; 15h:59m:49s remains)
INFO - root - 2017-12-03 06:37:28.034879: step 80, loss = 1.17, batch loss = 1.08 (43.7 examples/sec; 0.183 sec/batch; 16h:54m:05s remains)
INFO - root - 2017-12-03 06:37:29.838308: step 90, loss = 1.18, batch loss = 1.09 (45.1 examples/sec; 0.177 sec/batch; 16h:23m:11s remains)
INFO - root - 2017-12-03 06:37:31.621917: step 100, loss = 1.09, batch loss = 0.99 (44.2 examples/sec; 0.181 sec/batch; 16h:42m:00s remains)
INFO - root - 2017-12-03 06:37:33.484662: step 110, loss = 1.08, batch loss = 0.98 (44.7 examples/sec; 0.179 sec/batch; 16h:31m:10s remains)
INFO - root - 2017-12-03 06:37:35.285112: step 120, loss = 1.19, batch loss = 1.10 (43.6 examples/sec; 0.184 sec/batch; 16h:57m:07s remains)
INFO - root - 2017-12-03 06:37:37.077987: step 130, loss = 1.15, batch loss = 1.05 (43.6 examples/sec; 0.183 sec/batch; 16h:55m:24s remains)
INFO - root - 2017-12-03 06:37:38.869966: step 140, loss = 1.08, batch loss = 0.98 (45.3 examples/sec; 0.177 sec/batch; 16h:18m:07s remains)
INFO - root - 2017-12-03 06:37:40.736960: step 150, loss = 1.12, batch loss = 1.03 (45.7 examples/sec; 0.175 sec/batch; 16h:09m:55s remains)
INFO - root - 2017-12-03 06:37:42.554223: step 160, loss = 1.23, batch loss = 1.13 (42.4 examples/sec; 0.188 sec/batch; 17h:23m:58s remains)
INFO - root - 2017-12-03 06:37:44.352530: step 170, loss = 1.06, batch loss = 0.96 (43.7 examples/sec; 0.183 sec/batch; 16h:53m:55s remains)
INFO - root - 2017-12-03 06:37:46.181911: step 180, loss = 1.10, batch loss = 1.00 (41.5 examples/sec; 0.193 sec/batch; 17h:48m:21s remains)
INFO - root - 2017-12-03 06:37:47.971137: step 190, loss = 0.99, batch loss = 0.90 (43.8 examples/sec; 0.183 sec/batch; 16h:52m:09s remains)
INFO - root - 2017-12-03 06:37:49.775707: step 200, loss = 0.99, batch loss = 0.89 (45.2 examples/sec; 0.177 sec/batch; 16h:21m:07s remains)
INFO - root - 2017-12-03 06:37:51.646882: step 210, loss = 1.16, batch loss = 1.06 (44.1 examples/sec; 0.182 sec/batch; 16h:45m:28s remains)
INFO - root - 2017-12-03 06:37:53.472557: step 220, loss = 1.16, batch loss = 1.06 (44.9 examples/sec; 0.178 sec/batch; 16h:27m:35s remains)
INFO - root - 2017-12-03 06:37:55.267976: step 230, loss = 1.16, batch loss = 1.07 (44.5 examples/sec; 0.180 sec/batch; 16h:36m:06s remains)
INFO - root - 2017-12-03 06:37:57.083713: step 240, loss = 1.29, batch loss = 1.19 (44.5 examples/sec; 0.180 sec/batch; 16h:34m:36s remains)
INFO - root - 2017-12-03 06:37:58.890684: step 250, loss = 1.15, batch loss = 1.06 (44.2 examples/sec; 0.181 sec/batch; 16h:41m:17s remains)
INFO - root - 2017-12-03 06:38:00.690603: step 260, loss = 1.30, batch loss = 1.20 (43.7 examples/sec; 0.183 sec/batch; 16h:54m:04s remains)
INFO - root - 2017-12-03 06:38:02.506446: step 270, loss = 1.19, batch loss = 1.09 (42.9 examples/sec; 0.186 sec/batch; 17h:12m:06s remains)
INFO - root - 2017-12-03 06:38:04.319572: step 280, loss = 1.08, batch loss = 0.98 (45.7 examples/sec; 0.175 sec/batch; 16h:09m:26s remains)
INFO - root - 2017-12-03 06:38:06.112288: step 290, loss = 1.06, batch loss = 0.96 (44.5 examples/sec; 0.180 sec/batch; 16h:35m:25s remains)
INFO - root - 2017-12-03 06:38:07.922502: step 300, loss = 1.07, batch loss = 0.98 (45.1 examples/sec; 0.177 sec/batch; 16h:22m:26s remains)
INFO - root - 2017-12-03 06:38:09.795717: step 310, loss = 1.11, batch loss = 1.01 (45.4 examples/sec; 0.176 sec/batch; 16h:14m:32s remains)
INFO - root - 2017-12-03 06:38:11.636757: step 320, loss = 1.18, batch loss = 1.08 (42.8 examples/sec; 0.187 sec/batch; 17h:13m:37s remains)
INFO - root - 2017-12-03 06:38:13.426316: step 330, loss = 1.11, batch loss = 1.02 (45.0 examples/sec; 0.178 sec/batch; 16h:25m:17s remains)
INFO - root - 2017-12-03 06:38:15.245068: step 340, loss = 1.13, batch loss = 1.04 (43.2 examples/sec; 0.185 sec/batch; 17h:05m:07s remains)
INFO - root - 2017-12-03 06:38:17.081410: step 350, loss = 1.22, batch loss = 1.12 (45.7 examples/sec; 0.175 sec/batch; 16h:08m:30s remains)
INFO - root - 2017-12-03 06:38:18.885153: step 360, loss = 1.20, batch loss = 1.10 (44.1 examples/sec; 0.181 sec/batch; 16h:44m:42s remains)
INFO - root - 2017-12-03 06:38:20.721030: step 370, loss = 1.29, batch loss = 1.20 (43.5 examples/sec; 0.184 sec/batch; 16h:58m:37s remains)
INFO - root - 2017-12-03 06:38:22.558588: step 380, loss = 1.26, batch loss = 1.16 (41.8 examples/sec; 0.191 sec/batch; 17h:38m:26s remains)
INFO - root - 2017-12-03 06:38:24.417908: step 390, loss = 1.10, batch loss = 1.00 (40.6 examples/sec; 0.197 sec/batch; 18h:11m:48s remains)
INFO - root - 2017-12-03 06:38:26.257314: step 400, loss = 1.07, batch loss = 0.98 (44.8 examples/sec; 0.179 sec/batch; 16h:29m:02s remains)
INFO - root - 2017-12-03 06:38:28.151311: step 410, loss = 1.15, batch loss = 1.05 (43.7 examples/sec; 0.183 sec/batch; 16h:53m:01s remains)
INFO - root - 2017-12-03 06:38:29.997857: step 420, loss = 1.22, batch loss = 1.12 (43.2 examples/sec; 0.185 sec/batch; 17h:03m:49s remains)
INFO - root - 2017-12-03 06:38:31.819658: step 430, loss = 1.09, batch loss = 0.99 (43.0 examples/sec; 0.186 sec/batch; 17h:10m:42s remains)
INFO - root - 2017-12-03 06:38:33.633746: step 440, loss = 1.17, batch loss = 1.08 (43.9 examples/sec; 0.182 sec/batch; 16h:47m:54s remains)
INFO - root - 2017-12-03 06:38:35.464748: step 450, loss = 1.16, batch loss = 1.06 (44.0 examples/sec; 0.182 sec/batch; 16h:45m:41s remains)
INFO - root - 2017-12-03 06:38:37.320774: step 460, loss = 1.03, batch loss = 0.93 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:25s remains)
INFO - root - 2017-12-03 06:38:39.162057: step 470, loss = 1.06, batch loss = 0.96 (42.1 examples/sec; 0.190 sec/batch; 17h:32m:08s remains)
INFO - root - 2017-12-03 06:38:41.009474: step 480, loss = 1.21, batch loss = 1.11 (42.5 examples/sec; 0.188 sec/batch; 17h:21m:50s remains)
INFO - root - 2017-12-03 06:38:42.832885: step 490, loss = 1.09, batch loss = 0.99 (45.2 examples/sec; 0.177 sec/batch; 16h:20m:06s remains)
INFO - root - 2017-12-03 06:38:44.671689: step 500, loss = 1.14, batch loss = 1.04 (43.1 examples/sec; 0.186 sec/batch; 17h:07m:12s remains)
INFO - root - 2017-12-03 06:38:46.567958: step 510, loss = 1.09, batch loss = 0.99 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:19s remains)
INFO - root - 2017-12-03 06:38:48.399079: step 520, loss = 1.31, batch loss = 1.22 (43.5 examples/sec; 0.184 sec/batch; 16h:56m:59s remains)
INFO - root - 2017-12-03 06:38:50.223889: step 530, loss = 1.04, batch loss = 0.94 (44.0 examples/sec; 0.182 sec/batch; 16h:46m:00s remains)
INFO - root - 2017-12-03 06:38:52.065619: step 540, loss = 1.08, batch loss = 0.99 (41.8 examples/sec; 0.192 sec/batch; 17h:39m:53s remains)
INFO - root - 2017-12-03 06:38:53.895248: step 550, loss = 0.99, batch loss = 0.89 (43.8 examples/sec; 0.183 sec/batch; 16h:50m:38s remains)
INFO - root - 2017-12-03 06:38:55.734588: step 560, loss = 1.14, batch loss = 1.05 (42.0 examples/sec; 0.191 sec/batch; 17h:34m:53s remains)
INFO - root - 2017-12-03 06:38:57.554995: step 570, loss = 1.08, batch loss = 0.98 (44.3 examples/sec; 0.181 sec/batch; 16h:39m:30s remains)
INFO - root - 2017-12-03 06:38:59.383060: step 580, loss = 1.13, batch loss = 1.03 (40.6 examples/sec; 0.197 sec/batch; 18h:09m:17s remains)
INFO - root - 2017-12-03 06:39:01.214704: step 590, loss = 1.15, batch loss = 1.05 (42.5 examples/sec; 0.188 sec/batch; 17h:21m:26s remains)
INFO - root - 2017-12-03 06:39:03.040811: step 600, loss = 1.28, batch loss = 1.18 (44.0 examples/sec; 0.182 sec/batch; 16h:46m:31s remains)
INFO - root - 2017-12-03 06:39:04.979733: step 610, loss = 1.24, batch loss = 1.14 (43.1 examples/sec; 0.186 sec/batch; 17h:06m:50s remains)
INFO - root - 2017-12-03 06:39:06.807764: step 620, loss = 1.08, batch loss = 0.98 (44.2 examples/sec; 0.181 sec/batch; 16h:41m:54s remains)
INFO - root - 2017-12-03 06:39:08.624754: step 630, loss = 1.16, batch loss = 1.06 (43.6 examples/sec; 0.183 sec/batch; 16h:54m:12s remains)
INFO - root - 2017-12-03 06:39:10.463835: step 640, loss = 1.03, batch loss = 0.94 (44.9 examples/sec; 0.178 sec/batch; 16h:26m:28s remains)
INFO - root - 2017-12-03 06:39:12.304877: step 650, loss = 1.16, batch loss = 1.06 (40.1 examples/sec; 0.199 sec/batch; 18h:22m:11s remains)
INFO - root - 2017-12-03 06:39:14.109434: step 660, loss = 1.00, batch loss = 0.90 (44.3 examples/sec; 0.180 sec/batch; 16h:38m:15s remains)
INFO - root - 2017-12-03 06:39:15.948531: step 670, loss = 1.06, batch loss = 0.96 (43.9 examples/sec; 0.182 sec/batch; 16h:48m:02s remains)
INFO - root - 2017-12-03 06:39:17.769174: step 680, loss = 1.13, batch loss = 1.03 (43.2 examples/sec; 0.185 sec/batch; 17h:03m:55s remains)
INFO - root - 2017-12-03 06:39:19.591341: step 690, loss = 0.99, batch loss = 0.90 (42.5 examples/sec; 0.188 sec/batch; 17h:22m:00s remains)
INFO - root - 2017-12-03 06:39:21.439598: step 700, loss = 1.16, batch loss = 1.07 (42.2 examples/sec; 0.189 sec/batch; 17h:27m:32s remains)
INFO - root - 2017-12-03 06:39:23.360981: step 710, loss = 1.20, batch loss = 1.11 (43.0 examples/sec; 0.186 sec/batch; 17h:09m:22s remains)
INFO - root - 2017-12-03 06:39:25.228595: step 720, loss = 1.10, batch loss = 1.00 (42.7 examples/sec; 0.187 sec/batch; 17h:16m:24s remains)
INFO - root - 2017-12-03 06:39:27.075898: step 730, loss = 1.13, batch loss = 1.03 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:20s remains)
INFO - root - 2017-12-03 06:39:28.906019: step 740, loss = 1.15, batch loss = 1.05 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:41s remains)
INFO - root - 2017-12-03 06:39:30.767246: step 750, loss = 1.13, batch loss = 1.03 (44.3 examples/sec; 0.181 sec/batch; 16h:39m:02s remains)
INFO - root - 2017-12-03 06:39:32.614799: step 760, loss = 1.07, batch loss = 0.97 (42.8 examples/sec; 0.187 sec/batch; 17h:14m:25s remains)
INFO - root - 2017-12-03 06:39:34.434746: step 770, loss = 1.21, batch loss = 1.11 (44.7 examples/sec; 0.179 sec/batch; 16h:30m:09s remains)
INFO - root - 2017-12-03 06:39:36.268548: step 780, loss = 1.13, batch loss = 1.04 (44.6 examples/sec; 0.179 sec/batch; 16h:31m:35s remains)
INFO - root - 2017-12-03 06:39:38.099015: step 790, loss = 1.25, batch loss = 1.16 (42.9 examples/sec; 0.186 sec/batch; 17h:10m:14s remains)
INFO - root - 2017-12-03 06:39:39.929715: step 800, loss = 1.14, batch loss = 1.04 (42.7 examples/sec; 0.188 sec/batch; 17h:16m:53s remains)
INFO - root - 2017-12-03 06:39:41.834713: step 810, loss = 1.13, batch loss = 1.03 (42.7 examples/sec; 0.187 sec/batch; 17h:16m:04s remains)
INFO - root - 2017-12-03 06:39:43.671899: step 820, loss = 1.10, batch loss = 1.00 (44.7 examples/sec; 0.179 sec/batch; 16h:30m:12s remains)
INFO - root - 2017-12-03 06:39:45.507887: step 830, loss = 1.13, batch loss = 1.03 (44.3 examples/sec; 0.181 sec/batch; 16h:38m:23s remains)
INFO - root - 2017-12-03 06:39:47.348161: step 840, loss = 1.18, batch loss = 1.09 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:38s remains)
INFO - root - 2017-12-03 06:39:49.176891: step 850, loss = 1.04, batch loss = 0.95 (44.4 examples/sec; 0.180 sec/batch; 16h:35m:08s remains)
INFO - root - 2017-12-03 06:39:51.009602: step 860, loss = 1.24, batch loss = 1.14 (42.8 examples/sec; 0.187 sec/batch; 17h:13m:23s remains)
INFO - root - 2017-12-03 06:39:52.866248: step 870, loss = 1.19, batch loss = 1.09 (42.0 examples/sec; 0.190 sec/batch; 17h:31m:40s remains)
INFO - root - 2017-12-03 06:39:54.692345: step 880, loss = 1.19, batch loss = 1.10 (44.2 examples/sec; 0.181 sec/batch; 16h:40m:39s remains)
INFO - root - 2017-12-03 06:39:56.522830: step 890, loss = 0.95, batch loss = 0.85 (43.0 examples/sec; 0.186 sec/batch; 17h:08m:20s remains)
INFO - root - 2017-12-03 06:39:58.346997: step 900, loss = 1.20, batch loss = 1.10 (43.4 examples/sec; 0.185 sec/batch; 16h:59m:54s remains)
INFO - root - 2017-12-03 06:40:00.249204: step 910, loss = 1.19, batch loss = 1.09 (44.5 examples/sec; 0.180 sec/batch; 16h:34m:29s remains)
INFO - root - 2017-12-03 06:40:02.098657: step 920, loss = 1.22, batch loss = 1.12 (43.2 examples/sec; 0.185 sec/batch; 17h:02m:35s remains)
INFO - root - 2017-12-03 06:40:03.915910: step 930, loss = 1.26, batch loss = 1.17 (43.2 examples/sec; 0.185 sec/batch; 17h:02m:56s remains)
INFO - root - 2017-12-03 06:40:05.754374: step 940, loss = 1.33, batch loss = 1.24 (43.3 examples/sec; 0.185 sec/batch; 17h:01m:46s remains)
INFO - root - 2017-12-03 06:40:07.586671: step 950, loss = 1.10, batch loss = 1.01 (44.0 examples/sec; 0.182 sec/batch; 16h:45m:10s remains)
INFO - root - 2017-12-03 06:40:09.458570: step 960, loss = 1.24, batch loss = 1.14 (42.0 examples/sec; 0.190 sec/batch; 17h:31m:48s remains)
INFO - root - 2017-12-03 06:40:11.333529: step 970, loss = 1.12, batch loss = 1.02 (43.9 examples/sec; 0.182 sec/batch; 16h:46m:15s remains)
INFO - root - 2017-12-03 06:40:13.152918: step 980, loss = 1.09, batch loss = 0.99 (45.3 examples/sec; 0.177 sec/batch; 16h:16m:50s remains)
INFO - root - 2017-12-03 06:40:15.008675: step 990, loss = 1.14, batch loss = 1.04 (42.5 examples/sec; 0.188 sec/batch; 17h:18m:53s remains)
INFO - root - 2017-12-03 06:40:16.849520: step 1000, loss = 1.09, batch loss = 1.00 (43.8 examples/sec; 0.183 sec/batch; 16h:48m:45s remains)
INFO - root - 2017-12-03 06:40:18.777049: step 1010, loss = 1.15, batch loss = 1.05 (44.0 examples/sec; 0.182 sec/batch; 16h:45m:29s remains)
INFO - root - 2017-12-03 06:40:20.625316: step 1020, loss = 1.08, batch loss = 0.98 (42.8 examples/sec; 0.187 sec/batch; 17h:11m:52s remains)
INFO - root - 2017-12-03 06:40:22.488194: step 1030, loss = 1.06, batch loss = 0.97 (43.5 examples/sec; 0.184 sec/batch; 16h:56m:32s remains)
INFO - root - 2017-12-03 06:40:24.349885: step 1040, loss = 1.34, batch loss = 1.24 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:05s remains)
INFO - root - 2017-12-03 06:40:26.199759: step 1050, loss = 1.11, batch loss = 1.02 (43.1 examples/sec; 0.186 sec/batch; 17h:05m:28s remains)
INFO - root - 2017-12-03 06:40:28.056807: step 1060, loss = 1.15, batch loss = 1.06 (42.9 examples/sec; 0.187 sec/batch; 17h:10m:57s remains)
INFO - root - 2017-12-03 06:40:29.894631: step 1070, loss = 1.10, batch loss = 1.00 (44.3 examples/sec; 0.180 sec/batch; 16h:36m:58s remains)
INFO - root - 2017-12-03 06:40:31.733825: step 1080, loss = 1.24, batch loss = 1.14 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:43s remains)
INFO - root - 2017-12-03 06:40:33.575899: step 1090, loss = 1.03, batch loss = 0.93 (41.0 examples/sec; 0.195 sec/batch; 17h:57m:10s remains)
INFO - root - 2017-12-03 06:40:35.403036: step 1100, loss = 1.15, batch loss = 1.05 (44.4 examples/sec; 0.180 sec/batch; 16h:34m:34s remains)
INFO - root - 2017-12-03 06:40:37.320350: step 1110, loss = 1.10, batch loss = 1.00 (43.9 examples/sec; 0.182 sec/batch; 16h:47m:06s remains)
INFO - root - 2017-12-03 06:40:39.158359: step 1120, loss = 1.05, batch loss = 0.96 (44.4 examples/sec; 0.180 sec/batch; 16h:34m:01s remains)
INFO - root - 2017-12-03 06:40:40.992591: step 1130, loss = 1.17, batch loss = 1.07 (43.0 examples/sec; 0.186 sec/batch; 17h:08m:23s remains)
INFO - root - 2017-12-03 06:40:42.825223: step 1140, loss = 1.22, batch loss = 1.12 (43.3 examples/sec; 0.185 sec/batch; 17h:00m:13s remains)
INFO - root - 2017-12-03 06:40:44.687755: step 1150, loss = 1.09, batch loss = 0.99 (43.9 examples/sec; 0.182 sec/batch; 16h:45m:49s remains)
INFO - root - 2017-12-03 06:40:46.530334: step 1160, loss = 1.20, batch loss = 1.10 (44.1 examples/sec; 0.181 sec/batch; 16h:41m:12s remains)
INFO - root - 2017-12-03 06:40:48.362441: step 1170, loss = 1.14, batch loss = 1.05 (42.4 examples/sec; 0.189 sec/batch; 17h:21m:50s remains)
INFO - root - 2017-12-03 06:40:50.208233: step 1180, loss = 1.30, batch loss = 1.20 (43.4 examples/sec; 0.184 sec/batch; 16h:58m:13s remains)
INFO - root - 2017-12-03 06:40:52.068034: step 1190, loss = 1.16, batch loss = 1.07 (43.9 examples/sec; 0.182 sec/batch; 16h:46m:46s remains)
INFO - root - 2017-12-03 06:40:53.906195: step 1200, loss = 1.22, batch loss = 1.13 (42.6 examples/sec; 0.188 sec/batch; 17h:16m:24s remains)
INFO - root - 2017-12-03 06:40:55.832696: step 1210, loss = 1.17, batch loss = 1.07 (44.7 examples/sec; 0.179 sec/batch; 16h:29m:00s remains)
INFO - root - 2017-12-03 06:40:57.709391: step 1220, loss = 1.10, batch loss = 1.00 (42.7 examples/sec; 0.187 sec/batch; 17h:13m:50s remains)
INFO - root - 2017-12-03 06:40:59.537448: step 1230, loss = 1.19, batch loss = 1.09 (44.3 examples/sec; 0.181 sec/batch; 16h:37m:07s remains)
INFO - root - 2017-12-03 06:41:01.380700: step 1240, loss = 1.21, batch loss = 1.12 (42.5 examples/sec; 0.188 sec/batch; 17h:18m:35s remains)
INFO - root - 2017-12-03 06:41:03.213961: step 1250, loss = 1.25, batch loss = 1.16 (43.6 examples/sec; 0.183 sec/batch; 16h:53m:03s remains)
INFO - root - 2017-12-03 06:41:05.046307: step 1260, loss = 1.11, batch loss = 1.01 (44.1 examples/sec; 0.181 sec/batch; 16h:41m:23s remains)
INFO - root - 2017-12-03 06:41:06.874210: step 1270, loss = 0.94, batch loss = 0.84 (43.1 examples/sec; 0.185 sec/batch; 17h:03m:48s remains)
INFO - root - 2017-12-03 06:41:08.717417: step 1280, loss = 1.17, batch loss = 1.07 (44.1 examples/sec; 0.181 sec/batch; 16h:41m:15s remains)
INFO - root - 2017-12-03 06:41:10.561478: step 1290, loss = 1.15, batch loss = 1.05 (41.3 examples/sec; 0.194 sec/batch; 17h:50m:31s remains)
INFO - root - 2017-12-03 06:41:12.381040: step 1300, loss = 1.28, batch loss = 1.18 (45.1 examples/sec; 0.177 sec/batch; 16h:18m:54s remains)
INFO - root - 2017-12-03 06:41:14.303097: step 1310, loss = 1.13, batch loss = 1.04 (44.1 examples/sec; 0.182 sec/batch; 16h:42m:20s remains)
INFO - root - 2017-12-03 06:41:16.130273: step 1320, loss = 1.26, batch loss = 1.17 (43.6 examples/sec; 0.183 sec/batch; 16h:52m:01s remains)
INFO - root - 2017-12-03 06:41:17.966587: step 1330, loss = 1.27, batch loss = 1.17 (42.9 examples/sec; 0.186 sec/batch; 17h:08m:59s remains)
INFO - root - 2017-12-03 06:41:19.796073: step 1340, loss = 1.09, batch loss = 1.00 (44.0 examples/sec; 0.182 sec/batch; 16h:43m:02s remains)
INFO - root - 2017-12-03 06:41:21.640089: step 1350, loss = 1.14, batch loss = 1.05 (43.6 examples/sec; 0.184 sec/batch; 16h:52m:57s remains)
INFO - root - 2017-12-03 06:41:23.481470: step 1360, loss = 1.13, batch loss = 1.04 (45.0 examples/sec; 0.178 sec/batch; 16h:21m:51s remains)
INFO - root - 2017-12-03 06:41:25.329744: step 1370, loss = 1.19, batch loss = 1.09 (42.9 examples/sec; 0.186 sec/batch; 17h:09m:10s remains)
INFO - root - 2017-12-03 06:41:27.169082: step 1380, loss = 1.21, batch loss = 1.11 (44.0 examples/sec; 0.182 sec/batch; 16h:43m:53s remains)
INFO - root - 2017-12-03 06:41:29.000055: step 1390, loss = 1.03, batch loss = 0.93 (43.6 examples/sec; 0.183 sec/batch; 16h:52m:03s remains)
INFO - root - 2017-12-03 06:41:30.849304: step 1400, loss = 1.12, batch loss = 1.02 (43.9 examples/sec; 0.182 sec/batch; 16h:45m:04s remains)
INFO - root - 2017-12-03 06:41:32.777311: step 1410, loss = 1.18, batch loss = 1.09 (43.3 examples/sec; 0.185 sec/batch; 17h:00m:23s remains)
INFO - root - 2017-12-03 06:41:34.609831: step 1420, loss = 1.19, batch loss = 1.09 (44.5 examples/sec; 0.180 sec/batch; 16h:32m:13s remains)
INFO - root - 2017-12-03 06:41:36.448336: step 1430, loss = 1.18, batch loss = 1.09 (42.5 examples/sec; 0.188 sec/batch; 17h:17m:30s remains)
INFO - root - 2017-12-03 06:41:38.282314: step 1440, loss = 1.18, batch loss = 1.08 (43.6 examples/sec; 0.184 sec/batch; 16h:53m:29s remains)
INFO - root - 2017-12-03 06:41:40.120661: step 1450, loss = 1.07, batch loss = 0.98 (42.4 examples/sec; 0.189 sec/batch; 17h:21m:42s remains)
INFO - root - 2017-12-03 06:41:41.953786: step 1460, loss = 1.04, batch loss = 0.94 (44.0 examples/sec; 0.182 sec/batch; 16h:43m:03s remains)
INFO - root - 2017-12-03 06:41:43.793011: step 1470, loss = 1.14, batch loss = 1.04 (43.2 examples/sec; 0.185 sec/batch; 17h:02m:49s remains)
INFO - root - 2017-12-03 06:41:45.623783: step 1480, loss = 1.15, batch loss = 1.05 (43.1 examples/sec; 0.185 sec/batch; 17h:03m:22s remains)
INFO - root - 2017-12-03 06:41:47.472568: step 1490, loss = 1.14, batch loss = 1.04 (43.8 examples/sec; 0.183 sec/batch; 16h:48m:35s remains)
INFO - root - 2017-12-03 06:41:49.327759: step 1500, loss = 1.15, batch loss = 1.06 (44.6 examples/sec; 0.179 sec/batch; 16h:30m:06s remains)
INFO - root - 2017-12-03 06:41:51.233467: step 1510, loss = 1.19, batch loss = 1.10 (44.5 examples/sec; 0.180 sec/batch; 16h:30m:55s remains)
INFO - root - 2017-12-03 06:41:53.104467: step 1520, loss = 1.16, batch loss = 1.06 (43.4 examples/sec; 0.184 sec/batch; 16h:57m:20s remains)
INFO - root - 2017-12-03 06:41:54.931088: step 1530, loss = 1.17, batch loss = 1.08 (44.6 examples/sec; 0.179 sec/batch; 16h:29m:37s remains)
INFO - root - 2017-12-03 06:41:56.778183: step 1540, loss = 1.16, batch loss = 1.07 (42.7 examples/sec; 0.187 sec/batch; 17h:12m:39s remains)
INFO - root - 2017-12-03 06:41:58.597591: step 1550, loss = 1.19, batch loss = 1.09 (44.4 examples/sec; 0.180 sec/batch; 16h:33m:10s remains)
INFO - root - 2017-12-03 06:42:00.444997: step 1560, loss = 1.21, batch loss = 1.12 (41.2 examples/sec; 0.194 sec/batch; 17h:52m:02s remains)
INFO - root - 2017-12-03 06:42:02.271498: step 1570, loss = 1.19, batch loss = 1.09 (42.6 examples/sec; 0.188 sec/batch; 17h:14m:53s remains)
INFO - root - 2017-12-03 06:42:04.121873: step 1580, loss = 1.05, batch loss = 0.95 (44.1 examples/sec; 0.181 sec/batch; 16h:39m:44s remains)
INFO - root - 2017-12-03 06:42:05.967143: step 1590, loss = 1.19, batch loss = 1.09 (44.3 examples/sec; 0.180 sec/batch; 16h:35m:01s remains)
INFO - root - 2017-12-03 06:42:07.834924: step 1600, loss = 1.16, batch loss = 1.06 (44.5 examples/sec; 0.180 sec/batch; 16h:30m:39s remains)
INFO - root - 2017-12-03 06:42:09.782942: step 1610, loss = 1.16, batch loss = 1.07 (44.0 examples/sec; 0.182 sec/batch; 16h:43m:43s remains)
INFO - root - 2017-12-03 06:42:11.630326: step 1620, loss = 1.09, batch loss = 0.99 (41.5 examples/sec; 0.193 sec/batch; 17h:42m:59s remains)
INFO - root - 2017-12-03 06:42:13.579421: step 1630, loss = 1.22, batch loss = 1.12 (42.4 examples/sec; 0.189 sec/batch; 17h:20m:11s remains)
INFO - root - 2017-12-03 06:42:15.450902: step 1640, loss = 1.18, batch loss = 1.09 (42.2 examples/sec; 0.190 sec/batch; 17h:26m:24s remains)
INFO - root - 2017-12-03 06:42:17.289142: step 1650, loss = 1.16, batch loss = 1.07 (43.4 examples/sec; 0.184 sec/batch; 16h:55m:19s remains)
INFO - root - 2017-12-03 06:42:19.182435: step 1660, loss = 1.12, batch loss = 1.03 (41.7 examples/sec; 0.192 sec/batch; 17h:36m:40s remains)
INFO - root - 2017-12-03 06:42:21.059777: step 1670, loss = 1.14, batch loss = 1.05 (41.4 examples/sec; 0.193 sec/batch; 17h:44m:26s remains)
INFO - root - 2017-12-03 06:42:22.934324: step 1680, loss = 1.15, batch loss = 1.05 (43.4 examples/sec; 0.185 sec/batch; 16h:57m:29s remains)
INFO - root - 2017-12-03 06:42:24.794873: step 1690, loss = 1.19, batch loss = 1.09 (42.6 examples/sec; 0.188 sec/batch; 17h:14m:55s remains)
INFO - root - 2017-12-03 06:42:26.711019: step 1700, loss = 1.12, batch loss = 1.03 (36.3 examples/sec; 0.220 sec/batch; 20h:14m:12s remains)
INFO - root - 2017-12-03 06:42:28.645950: step 1710, loss = 1.19, batch loss = 1.09 (43.4 examples/sec; 0.184 sec/batch; 16h:56m:17s remains)
INFO - root - 2017-12-03 06:42:30.474630: step 1720, loss = 1.04, batch loss = 0.94 (43.7 examples/sec; 0.183 sec/batch; 16h:49m:07s remains)
INFO - root - 2017-12-03 06:42:32.312673: step 1730, loss = 1.04, batch loss = 0.94 (43.7 examples/sec; 0.183 sec/batch; 16h:49m:08s remains)
INFO - root - 2017-12-03 06:42:34.145483: step 1740, loss = 1.21, batch loss = 1.11 (42.2 examples/sec; 0.190 sec/batch; 17h:25m:50s remains)
INFO - root - 2017-12-03 06:42:35.994661: step 1750, loss = 1.23, batch loss = 1.14 (42.8 examples/sec; 0.187 sec/batch; 17h:11m:18s remains)
INFO - root - 2017-12-03 06:42:37.832090: step 1760, loss = 1.26, batch loss = 1.16 (43.6 examples/sec; 0.183 sec/batch; 16h:50m:59s remains)
INFO - root - 2017-12-03 06:42:39.685585: step 1770, loss = 1.24, batch loss = 1.15 (44.1 examples/sec; 0.181 sec/batch; 16h:39m:20s remains)
INFO - root - 2017-12-03 06:42:41.514644: step 1780, loss = 1.16, batch loss = 1.07 (45.0 examples/sec; 0.178 sec/batch; 16h:19m:44s remains)
INFO - root - 2017-12-03 06:42:43.368166: step 1790, loss = 1.08, batch loss = 0.98 (44.4 examples/sec; 0.180 sec/batch; 16h:33m:18s remains)
INFO - root - 2017-12-03 06:42:45.228244: step 1800, loss = 1.25, batch loss = 1.15 (41.4 examples/sec; 0.193 sec/batch; 17h:45m:48s remains)
INFO - root - 2017-12-03 06:42:47.162525: step 1810, loss = 1.10, batch loss = 1.01 (43.4 examples/sec; 0.184 sec/batch; 16h:56m:45s remains)
INFO - root - 2017-12-03 06:42:49.000954: step 1820, loss = 1.21, batch loss = 1.11 (43.7 examples/sec; 0.183 sec/batch; 16h:49m:02s remains)
INFO - root - 2017-12-03 06:42:50.830782: step 1830, loss = 1.10, batch loss = 1.00 (44.7 examples/sec; 0.179 sec/batch; 16h:26m:58s remains)
INFO - root - 2017-12-03 06:42:52.658859: step 1840, loss = 1.10, batch loss = 1.01 (42.3 examples/sec; 0.189 sec/batch; 17h:21m:55s remains)
INFO - root - 2017-12-03 06:42:54.489772: step 1850, loss = 1.19, batch loss = 1.10 (44.8 examples/sec; 0.179 sec/batch; 16h:24m:06s remains)
INFO - root - 2017-12-03 06:42:56.346722: step 1860, loss = 1.20, batch loss = 1.10 (42.3 examples/sec; 0.189 sec/batch; 17h:22m:02s remains)
INFO - root - 2017-12-03 06:42:58.174507: step 1870, loss = 1.04, batch loss = 0.94 (43.9 examples/sec; 0.182 sec/batch; 16h:45m:12s remains)
INFO - root - 2017-12-03 06:43:00.018372: step 1880, loss = 1.13, batch loss = 1.03 (43.6 examples/sec; 0.183 sec/batch; 16h:50m:36s remains)
INFO - root - 2017-12-03 06:43:01.847471: step 1890, loss = 1.27, batch loss = 1.18 (42.6 examples/sec; 0.188 sec/batch; 17h:15m:12s remains)
INFO - root - 2017-12-03 06:43:03.676537: step 1900, loss = 1.01, batch loss = 0.91 (44.3 examples/sec; 0.180 sec/batch; 16h:33m:59s remains)
INFO - root - 2017-12-03 06:43:05.607476: step 1910, loss = 1.15, batch loss = 1.05 (44.1 examples/sec; 0.181 sec/batch; 16h:38m:53s remains)
INFO - root - 2017-12-03 06:43:07.445613: step 1920, loss = 1.24, batch loss = 1.14 (42.3 examples/sec; 0.189 sec/batch; 17h:21m:40s remains)
INFO - root - 2017-12-03 06:43:09.282050: step 1930, loss = 1.14, batch loss = 1.04 (41.3 examples/sec; 0.194 sec/batch; 17h:46m:45s remains)
INFO - root - 2017-12-03 06:43:11.156785: step 1940, loss = 1.16, batch loss = 1.06 (44.3 examples/sec; 0.181 sec/batch; 16h:34m:56s remains)
INFO - root - 2017-12-03 06:43:13.054398: step 1950, loss = 1.23, batch loss = 1.13 (40.7 examples/sec; 0.196 sec/batch; 18h:01m:48s remains)
INFO - root - 2017-12-03 06:43:14.893890: step 1960, loss = 1.14, batch loss = 1.05 (43.4 examples/sec; 0.184 sec/batch; 16h:55m:24s remains)
INFO - root - 2017-12-03 06:43:16.743919: step 1970, loss = 1.21, batch loss = 1.12 (43.2 examples/sec; 0.185 sec/batch; 16h:59m:31s remains)
INFO - root - 2017-12-03 06:43:18.594684: step 1980, loss = 1.12, batch loss = 1.03 (44.0 examples/sec; 0.182 sec/batch; 16h:42m:00s remains)
INFO - root - 2017-12-03 06:43:20.421898: step 1990, loss = 1.24, batch loss = 1.14 (44.2 examples/sec; 0.181 sec/batch; 16h:37m:09s remains)
INFO - root - 2017-12-03 06:43:22.272502: step 2000, loss = 1.06, batch loss = 0.96 (43.2 examples/sec; 0.185 sec/batch; 16h:59m:56s remains)
INFO - root - 2017-12-03 06:43:24.189575: step 2010, loss = 1.23, batch loss = 1.14 (44.0 examples/sec; 0.182 sec/batch; 16h:42m:05s remains)
INFO - root - 2017-12-03 06:43:26.048955: step 2020, loss = 1.24, batch loss = 1.15 (44.6 examples/sec; 0.179 sec/batch; 16h:28m:13s remains)
INFO - root - 2017-12-03 06:43:27.885907: step 2030, loss = 1.07, batch loss = 0.98 (42.9 examples/sec; 0.187 sec/batch; 17h:07m:18s remains)
INFO - root - 2017-12-03 06:43:29.729919: step 2040, loss = 1.19, batch loss = 1.10 (43.6 examples/sec; 0.183 sec/batch; 16h:49m:42s remains)
INFO - root - 2017-12-03 06:43:31.596030: step 2050, loss = 1.11, batch loss = 1.01 (43.3 examples/sec; 0.185 sec/batch; 16h:57m:26s remains)
INFO - root - 2017-12-03 06:43:33.426057: step 2060, loss = 1.16, batch loss = 1.07 (42.7 examples/sec; 0.187 sec/batch; 17h:11m:13s remains)
INFO - root - 2017-12-03 06:43:35.257842: step 2070, loss = 1.03, batch loss = 0.93 (43.7 examples/sec; 0.183 sec/batch; 16h:47m:04s remains)
INFO - root - 2017-12-03 06:43:37.094560: step 2080, loss = 1.14, batch loss = 1.04 (42.4 examples/sec; 0.188 sec/batch; 17h:17m:54s remains)
INFO - root - 2017-12-03 06:43:38.919469: step 2090, loss = 1.22, batch loss = 1.12 (44.2 examples/sec; 0.181 sec/batch; 16h:37m:01s remains)
INFO - root - 2017-12-03 06:43:40.784193: step 2100, loss = 1.25, batch loss = 1.16 (43.0 examples/sec; 0.186 sec/batch; 17h:03m:32s remains)
INFO - root - 2017-12-03 06:43:42.704721: step 2110, loss = 1.12, batch loss = 1.02 (43.8 examples/sec; 0.183 sec/batch; 16h:46m:13s remains)
INFO - root - 2017-12-03 06:43:44.591032: step 2120, loss = 1.15, batch loss = 1.05 (41.2 examples/sec; 0.194 sec/batch; 17h:50m:26s remains)
INFO - root - 2017-12-03 06:43:46.420077: step 2130, loss = 1.11, batch loss = 1.02 (41.6 examples/sec; 0.192 sec/batch; 17h:39m:29s remains)
INFO - root - 2017-12-03 06:43:48.253957: step 2140, loss = 1.26, batch loss = 1.17 (44.3 examples/sec; 0.180 sec/batch; 16h:33m:39s remains)
INFO - root - 2017-12-03 06:43:50.106472: step 2150, loss = 1.27, batch loss = 1.17 (41.3 examples/sec; 0.194 sec/batch; 17h:45m:29s remains)
INFO - root - 2017-12-03 06:43:51.945798: step 2160, loss = 1.15, batch loss = 1.06 (42.5 examples/sec; 0.188 sec/batch; 17h:16m:23s remains)
INFO - root - 2017-12-03 06:43:53.773532: step 2170, loss = 1.12, batch loss = 1.02 (43.6 examples/sec; 0.184 sec/batch; 16h:50m:37s remains)
INFO - root - 2017-12-03 06:43:55.613125: step 2180, loss = 1.17, batch loss = 1.07 (42.6 examples/sec; 0.188 sec/batch; 17h:14m:32s remains)
INFO - root - 2017-12-03 06:43:57.445372: step 2190, loss = 1.20, batch loss = 1.10 (44.1 examples/sec; 0.181 sec/batch; 16h:38m:53s remains)
INFO - root - 2017-12-03 06:43:59.302873: step 2200, loss = 1.15, batch loss = 1.05 (45.0 examples/sec; 0.178 sec/batch; 16h:18m:56s remains)
INFO - root - 2017-12-03 06:44:01.236267: step 2210, loss = 1.09, batch loss = 0.99 (44.2 examples/sec; 0.181 sec/batch; 16h:35m:32s remains)
INFO - root - 2017-12-03 06:44:03.089800: step 2220, loss = 1.11, batch loss = 1.01 (43.1 examples/sec; 0.186 sec/batch; 17h:01m:37s remains)
INFO - root - 2017-12-03 06:44:04.947047: step 2230, loss = 1.11, batch loss = 1.01 (43.3 examples/sec; 0.185 sec/batch; 16h:57m:23s remains)
INFO - root - 2017-12-03 06:44:06.791644: step 2240, loss = 1.09, batch loss = 0.99 (44.3 examples/sec; 0.181 sec/batch; 16h:33m:59s remains)
INFO - root - 2017-12-03 06:44:08.641257: step 2250, loss = 1.10, batch loss = 1.01 (43.3 examples/sec; 0.185 sec/batch; 16h:57m:23s remains)
INFO - root - 2017-12-03 06:44:10.468280: step 2260, loss = 0.96, batch loss = 0.87 (44.6 examples/sec; 0.179 sec/batch; 16h:26m:54s remains)
INFO - root - 2017-12-03 06:44:12.301844: step 2270, loss = 1.19, batch loss = 1.10 (42.5 examples/sec; 0.188 sec/batch; 17h:14m:55s remains)
INFO - root - 2017-12-03 06:44:14.128204: step 2280, loss = 1.18, batch loss = 1.08 (43.8 examples/sec; 0.183 sec/batch; 16h:45m:17s remains)
INFO - root - 2017-12-03 06:44:15.983318: step 2290, loss = 1.22, batch loss = 1.12 (44.5 examples/sec; 0.180 sec/batch; 16h:29m:53s remains)
INFO - root - 2017-12-03 06:44:17.824926: step 2300, loss = 1.20, batch loss = 1.10 (44.8 examples/sec; 0.179 sec/batch; 16h:23m:09s remains)
INFO - root - 2017-12-03 06:44:19.761205: step 2310, loss = 1.24, batch loss = 1.15 (43.5 examples/sec; 0.184 sec/batch; 16h:53m:07s remains)
INFO - root - 2017-12-03 06:44:21.617336: step 2320, loss = 1.24, batch loss = 1.14 (42.1 examples/sec; 0.190 sec/batch; 17h:26m:16s remains)
INFO - root - 2017-12-03 06:44:23.448873: step 2330, loss = 1.17, batch loss = 1.07 (44.2 examples/sec; 0.181 sec/batch; 16h:36m:07s remains)
INFO - root - 2017-12-03 06:44:25.305625: step 2340, loss = 1.30, batch loss = 1.20 (41.6 examples/sec; 0.192 sec/batch; 17h:38m:22s remains)
INFO - root - 2017-12-03 06:44:27.161833: step 2350, loss = 1.06, batch loss = 0.97 (40.4 examples/sec; 0.198 sec/batch; 18h:10m:19s remains)
INFO - root - 2017-12-03 06:44:28.991125: step 2360, loss = 1.25, batch loss = 1.15 (42.8 examples/sec; 0.187 sec/batch; 17h:08m:35s remains)
INFO - root - 2017-12-03 06:44:30.822670: step 2370, loss = 1.19, batch loss = 1.10 (42.9 examples/sec; 0.186 sec/batch; 17h:06m:02s remains)
INFO - root - 2017-12-03 06:44:32.658837: step 2380, loss = 1.20, batch loss = 1.10 (43.5 examples/sec; 0.184 sec/batch; 16h:52m:28s remains)
INFO - root - 2017-12-03 06:44:34.503459: step 2390, loss = 1.18, batch loss = 1.08 (43.1 examples/sec; 0.186 sec/batch; 17h:00m:51s remains)
INFO - root - 2017-12-03 06:44:36.364279: step 2400, loss = 1.02, batch loss = 0.92 (44.4 examples/sec; 0.180 sec/batch; 16h:30m:47s remains)
INFO - root - 2017-12-03 06:44:38.296809: step 2410, loss = 1.03, batch loss = 0.93 (42.3 examples/sec; 0.189 sec/batch; 17h:20m:21s remains)
INFO - root - 2017-12-03 06:44:40.132579: step 2420, loss = 1.17, batch loss = 1.07 (41.7 examples/sec; 0.192 sec/batch; 17h:35m:58s remains)
INFO - root - 2017-12-03 06:44:41.954536: step 2430, loss = 1.20, batch loss = 1.10 (44.5 examples/sec; 0.180 sec/batch; 16h:29m:13s remains)
INFO - root - 2017-12-03 06:44:43.793852: step 2440, loss = 1.27, batch loss = 1.17 (42.8 examples/sec; 0.187 sec/batch; 17h:07m:31s remains)
INFO - root - 2017-12-03 06:44:45.635694: step 2450, loss = 1.14, batch loss = 1.04 (43.9 examples/sec; 0.182 sec/batch; 16h:41m:38s remains)
INFO - root - 2017-12-03 06:44:47.497086: step 2460, loss = 1.22, batch loss = 1.12 (43.5 examples/sec; 0.184 sec/batch; 16h:50m:39s remains)
INFO - root - 2017-12-03 06:44:49.339963: step 2470, loss = 1.19, batch loss = 1.09 (43.6 examples/sec; 0.183 sec/batch; 16h:49m:05s remains)
INFO - root - 2017-12-03 06:44:51.187057: step 2480, loss = 1.14, batch loss = 1.04 (42.3 examples/sec; 0.189 sec/batch; 17h:20m:17s remains)
INFO - root - 2017-12-03 06:44:53.032226: step 2490, loss = 1.01, batch loss = 0.91 (44.8 examples/sec; 0.179 sec/batch; 16h:22m:02s remains)
INFO - root - 2017-12-03 06:44:54.886368: step 2500, loss = 1.15, batch loss = 1.05 (43.2 examples/sec; 0.185 sec/batch; 16h:58m:53s remains)
INFO - root - 2017-12-03 06:44:56.800407: step 2510, loss = 1.18, batch loss = 1.08 (43.7 examples/sec; 0.183 sec/batch; 16h:46m:39s remains)
INFO - root - 2017-12-03 06:44:58.651053: step 2520, loss = 1.01, batch loss = 0.92 (44.1 examples/sec; 0.181 sec/batch; 16h:37m:34s remains)
INFO - root - 2017-12-03 06:45:00.495295: step 2530, loss = 1.26, batch loss = 1.16 (41.7 examples/sec; 0.192 sec/batch; 17h:34m:17s remains)
INFO - root - 2017-12-03 06:45:02.376068: step 2540, loss = 1.08, batch loss = 0.99 (42.8 examples/sec; 0.187 sec/batch; 17h:07m:19s remains)
INFO - root - 2017-12-03 06:45:04.213781: step 2550, loss = 1.19, batch loss = 1.09 (43.8 examples/sec; 0.183 sec/batch; 16h:43m:56s remains)
INFO - root - 2017-12-03 06:45:06.067917: step 2560, loss = 1.03, batch loss = 0.93 (42.1 examples/sec; 0.190 sec/batch; 17h:23m:44s remains)
INFO - root - 2017-12-03 06:45:07.913008: step 2570, loss = 1.21, batch loss = 1.12 (41.9 examples/sec; 0.191 sec/batch; 17h:29m:36s remains)
INFO - root - 2017-12-03 06:45:09.771201: step 2580, loss = 1.09, batch loss = 0.99 (41.4 examples/sec; 0.193 sec/batch; 17h:42m:44s remains)
INFO - root - 2017-12-03 06:45:11.631692: step 2590, loss = 1.12, batch loss = 1.02 (43.1 examples/sec; 0.186 sec/batch; 17h:01m:35s remains)
INFO - root - 2017-12-03 06:45:13.463644: step 2600, loss = 1.15, batch loss = 1.05 (44.1 examples/sec; 0.181 sec/batch; 16h:36m:24s remains)
INFO - root - 2017-12-03 06:45:15.361608: step 2610, loss = 1.04, batch loss = 0.94 (44.2 examples/sec; 0.181 sec/batch; 16h:34m:37s remains)
INFO - root - 2017-12-03 06:45:17.235502: step 2620, loss = 1.05, batch loss = 0.95 (43.6 examples/sec; 0.183 sec/batch; 16h:48m:29s remains)
INFO - root - 2017-12-03 06:45:19.064705: step 2630, loss = 1.19, batch loss = 1.09 (44.3 examples/sec; 0.181 sec/batch; 16h:33m:23s remains)
INFO - root - 2017-12-03 06:45:20.941402: step 2640, loss = 1.19, batch loss = 1.09 (43.1 examples/sec; 0.186 sec/batch; 17h:01m:16s remains)
INFO - root - 2017-12-03 06:45:22.796246: step 2650, loss = 1.14, batch loss = 1.05 (44.0 examples/sec; 0.182 sec/batch; 16h:39m:17s remains)
INFO - root - 2017-12-03 06:45:24.662898: step 2660, loss = 1.03, batch loss = 0.93 (43.2 examples/sec; 0.185 sec/batch; 16h:59m:01s remains)
INFO - root - 2017-12-03 06:45:26.502303: step 2670, loss = 1.21, batch loss = 1.12 (43.9 examples/sec; 0.182 sec/batch; 16h:41m:06s remains)
INFO - root - 2017-12-03 06:45:28.343804: step 2680, loss = 1.16, batch loss = 1.06 (43.1 examples/sec; 0.186 sec/batch; 17h:00m:26s remains)
INFO - root - 2017-12-03 06:45:30.177574: step 2690, loss = 1.21, batch loss = 1.11 (42.2 examples/sec; 0.190 sec/batch; 17h:22m:08s remains)
INFO - root - 2017-12-03 06:45:32.010099: step 2700, loss = 1.13, batch loss = 1.03 (43.0 examples/sec; 0.186 sec/batch; 17h:02m:54s remains)
INFO - root - 2017-12-03 06:45:33.911052: step 2710, loss = 1.17, batch loss = 1.07 (42.4 examples/sec; 0.189 sec/batch; 17h:16m:09s remains)
INFO - root - 2017-12-03 06:45:35.752849: step 2720, loss = 1.06, batch loss = 0.96 (43.2 examples/sec; 0.185 sec/batch; 16h:58m:09s remains)
INFO - root - 2017-12-03 06:45:37.589460: step 2730, loss = 1.31, batch loss = 1.22 (42.9 examples/sec; 0.187 sec/batch; 17h:05m:26s remains)
INFO - root - 2017-12-03 06:45:39.454427: step 2740, loss = 1.20, batch loss = 1.10 (41.2 examples/sec; 0.194 sec/batch; 17h:46m:29s remains)
INFO - root - 2017-12-03 06:45:41.310020: step 2750, loss = 1.22, batch loss = 1.12 (43.4 examples/sec; 0.184 sec/batch; 16h:52m:33s remains)
INFO - root - 2017-12-03 06:45:43.163887: step 2760, loss = 1.14, batch loss = 1.05 (43.7 examples/sec; 0.183 sec/batch; 16h:46m:15s remains)
INFO - root - 2017-12-03 06:45:45.008011: step 2770, loss = 1.11, batch loss = 1.01 (44.0 examples/sec; 0.182 sec/batch; 16h:40m:14s remains)
INFO - root - 2017-12-03 06:45:46.837426: step 2780, loss = 1.16, batch loss = 1.06 (43.5 examples/sec; 0.184 sec/batch; 16h:50m:54s remains)
INFO - root - 2017-12-03 06:45:48.667381: step 2790, loss = 1.00, batch loss = 0.90 (43.7 examples/sec; 0.183 sec/batch; 16h:44m:51s remains)
INFO - root - 2017-12-03 06:45:50.520086: step 2800, loss = 1.18, batch loss = 1.08 (43.2 examples/sec; 0.185 sec/batch; 16h:56m:25s remains)
INFO - root - 2017-12-03 06:45:52.421006: step 2810, loss = 1.12, batch loss = 1.02 (44.8 examples/sec; 0.179 sec/batch; 16h:21m:01s remains)
INFO - root - 2017-12-03 06:45:54.279090: step 2820, loss = 1.11, batch loss = 1.02 (42.6 examples/sec; 0.188 sec/batch; 17h:12m:54s remains)
INFO - root - 2017-12-03 06:45:56.149745: step 2830, loss = 1.16, batch loss = 1.06 (43.9 examples/sec; 0.182 sec/batch; 16h:40m:54s remains)
INFO - root - 2017-12-03 06:45:57.984503: step 2840, loss = 1.13, batch loss = 1.03 (44.0 examples/sec; 0.182 sec/batch; 16h:38m:25s remains)
INFO - root - 2017-12-03 06:45:59.814405: step 2850, loss = 1.22, batch loss = 1.13 (43.4 examples/sec; 0.184 sec/batch; 16h:53m:30s remains)
INFO - root - 2017-12-03 06:46:01.660969: step 2860, loss = 1.01, batch loss = 0.92 (42.8 examples/sec; 0.187 sec/batch; 17h:07m:57s remains)
INFO - root - 2017-12-03 06:46:03.513118: step 2870, loss = 1.17, batch loss = 1.07 (41.3 examples/sec; 0.194 sec/batch; 17h:44m:40s remains)
INFO - root - 2017-12-03 06:46:05.357734: step 2880, loss = 1.12, batch loss = 1.02 (43.6 examples/sec; 0.183 sec/batch; 16h:47m:15s remains)
INFO - root - 2017-12-03 06:46:07.182208: step 2890, loss = 1.31, batch loss = 1.21 (43.3 examples/sec; 0.185 sec/batch; 16h:55m:29s remains)
INFO - root - 2017-12-03 06:46:09.022521: step 2900, loss = 1.12, batch loss = 1.02 (44.5 examples/sec; 0.180 sec/batch; 16h:28m:01s remains)
INFO - root - 2017-12-03 06:46:10.956270: step 2910, loss = 1.24, batch loss = 1.14 (43.5 examples/sec; 0.184 sec/batch; 16h:49m:49s remains)
INFO - root - 2017-12-03 06:46:12.810896: step 2920, loss = 1.12, batch loss = 1.02 (44.4 examples/sec; 0.180 sec/batch; 16h:30m:34s remains)
INFO - root - 2017-12-03 06:46:14.636622: step 2930, loss = 1.23, batch loss = 1.13 (43.2 examples/sec; 0.185 sec/batch; 16h:57m:07s remains)
INFO - root - 2017-12-03 06:46:16.463436: step 2940, loss = 1.13, batch loss = 1.03 (43.3 examples/sec; 0.185 sec/batch; 16h:54m:48s remains)
INFO - root - 2017-12-03 06:46:18.310603: step 2950, loss = 1.18, batch loss = 1.08 (44.3 examples/sec; 0.181 sec/batch; 16h:32m:04s remains)
INFO - root - 2017-12-03 06:46:20.165565: step 2960, loss = 1.30, batch loss = 1.20 (43.6 examples/sec; 0.184 sec/batch; 16h:48m:18s remains)
INFO - root - 2017-12-03 06:46:22.024069: step 2970, loss = 1.07, batch loss = 0.98 (42.7 examples/sec; 0.187 sec/batch; 17h:08m:35s remains)
INFO - root - 2017-12-03 06:46:23.873782: step 2980, loss = 1.28, batch loss = 1.18 (42.8 examples/sec; 0.187 sec/batch; 17h:07m:39s remains)
INFO - root - 2017-12-03 06:46:25.746428: step 2990, loss = 1.26, batch loss = 1.16 (42.0 examples/sec; 0.190 sec/batch; 17h:25m:08s remains)
INFO - root - 2017-12-03 06:46:27.588329: step 3000, loss = 1.14, batch loss = 1.04 (43.9 examples/sec; 0.182 sec/batch; 16h:40m:27s remains)
INFO - root - 2017-12-03 06:46:29.518023: step 3010, loss = 1.16, batch loss = 1.07 (43.0 examples/sec; 0.186 sec/batch; 17h:01m:24s remains)
INFO - root - 2017-12-03 06:46:31.356596: step 3020, loss = 1.14, batch loss = 1.05 (43.5 examples/sec; 0.184 sec/batch; 16h:50m:28s remains)
INFO - root - 2017-12-03 06:46:33.184860: step 3030, loss = 1.15, batch loss = 1.05 (44.9 examples/sec; 0.178 sec/batch; 16h:18m:42s remains)
INFO - root - 2017-12-03 06:46:35.019083: step 3040, loss = 1.09, batch loss = 0.99 (44.1 examples/sec; 0.181 sec/batch; 16h:35m:16s remains)
INFO - root - 2017-12-03 06:46:36.850468: step 3050, loss = 1.29, batch loss = 1.19 (42.9 examples/sec; 0.186 sec/batch; 17h:03m:55s remains)
INFO - root - 2017-12-03 06:46:38.687563: step 3060, loss = 1.07, batch loss = 0.98 (43.3 examples/sec; 0.185 sec/batch; 16h:53m:38s remains)
INFO - root - 2017-12-03 06:46:40.519890: step 3070, loss = 1.12, batch loss = 1.02 (43.6 examples/sec; 0.184 sec/batch; 16h:48m:03s remains)
INFO - root - 2017-12-03 06:46:42.354675: step 3080, loss = 0.95, batch loss = 0.85 (43.7 examples/sec; 0.183 sec/batch; 16h:43m:59s remains)
INFO - root - 2017-12-03 06:46:44.190421: step 3090, loss = 1.22, batch loss = 1.13 (44.2 examples/sec; 0.181 sec/batch; 16h:34m:15s remains)
INFO - root - 2017-12-03 06:46:46.016579: step 3100, loss = 1.11, batch loss = 1.01 (44.6 examples/sec; 0.179 sec/batch; 16h:23m:50s remains)
INFO - root - 2017-12-03 06:46:47.935311: step 3110, loss = 1.20, batch loss = 1.11 (44.1 examples/sec; 0.181 sec/batch; 16h:35m:30s remains)
INFO - root - 2017-12-03 06:46:49.762804: step 3120, loss = 1.24, batch loss = 1.14 (44.2 examples/sec; 0.181 sec/batch; 16h:32m:32s remains)
INFO - root - 2017-12-03 06:46:51.605655: step 3130, loss = 1.13, batch loss = 1.04 (44.0 examples/sec; 0.182 sec/batch; 16h:37m:13s remains)
INFO - root - 2017-12-03 06:46:53.434858: step 3140, loss = 1.28, batch loss = 1.19 (42.4 examples/sec; 0.189 sec/batch; 17h:15m:50s remains)
INFO - root - 2017-12-03 06:46:55.293951: step 3150, loss = 1.12, batch loss = 1.03 (42.2 examples/sec; 0.190 sec/batch; 17h:20m:21s remains)
INFO - root - 2017-12-03 06:46:57.138702: step 3160, loss = 1.08, batch loss = 0.98 (42.6 examples/sec; 0.188 sec/batch; 17h:11m:19s remains)
INFO - root - 2017-12-03 06:46:58.994121: step 3170, loss = 1.04, batch loss = 0.94 (43.8 examples/sec; 0.183 sec/batch; 16h:43m:17s remains)
INFO - root - 2017-12-03 06:47:00.846287: step 3180, loss = 1.17, batch loss = 1.08 (43.7 examples/sec; 0.183 sec/batch; 16h:45m:26s remains)
INFO - root - 2017-12-03 06:47:02.712266: step 3190, loss = 1.16, batch loss = 1.06 (41.9 examples/sec; 0.191 sec/batch; 17h:27m:00s remains)
INFO - root - 2017-12-03 06:47:04.551575: step 3200, loss = 1.18, batch loss = 1.09 (43.2 examples/sec; 0.185 sec/batch; 16h:56m:55s remains)
INFO - root - 2017-12-03 06:47:06.489593: step 3210, loss = 1.16, batch loss = 1.06 (42.7 examples/sec; 0.188 sec/batch; 17h:09m:10s remains)
INFO - root - 2017-12-03 06:47:08.337816: step 3220, loss = 1.21, batch loss = 1.11 (43.8 examples/sec; 0.183 sec/batch; 16h:42m:50s remains)
INFO - root - 2017-12-03 06:47:10.237472: step 3230, loss = 1.15, batch loss = 1.06 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:11s remains)
INFO - root - 2017-12-03 06:47:12.088120: step 3240, loss = 1.24, batch loss = 1.14 (42.7 examples/sec; 0.188 sec/batch; 17h:08m:57s remains)
INFO - root - 2017-12-03 06:47:13.963964: step 3250, loss = 1.00, batch loss = 0.90 (42.1 examples/sec; 0.190 sec/batch; 17h:23m:03s remains)
INFO - root - 2017-12-03 06:47:15.812131: step 3260, loss = 0.95, batch loss = 0.85 (43.1 examples/sec; 0.186 sec/batch; 16h:59m:08s remains)
INFO - root - 2017-12-03 06:47:17.643392: step 3270, loss = 1.16, batch loss = 1.07 (43.5 examples/sec; 0.184 sec/batch; 16h:48m:59s remains)
INFO - root - 2017-12-03 06:47:19.473022: step 3280, loss = 1.14, batch loss = 1.04 (45.0 examples/sec; 0.178 sec/batch; 16h:14m:57s remains)
INFO - root - 2017-12-03 06:47:21.317759: step 3290, loss = 1.06, batch loss = 0.96 (42.7 examples/sec; 0.188 sec/batch; 17h:08m:47s remains)
INFO - root - 2017-12-03 06:47:23.173445: step 3300, loss = 1.01, batch loss = 0.91 (42.3 examples/sec; 0.189 sec/batch; 17h:16m:48s remains)
INFO - root - 2017-12-03 06:47:25.129034: step 3310, loss = 1.02, batch loss = 0.92 (41.7 examples/sec; 0.192 sec/batch; 17h:31m:40s remains)
INFO - root - 2017-12-03 06:47:26.983277: step 3320, loss = 1.21, batch loss = 1.11 (44.3 examples/sec; 0.181 sec/batch; 16h:31m:39s remains)
INFO - root - 2017-12-03 06:47:28.845072: step 3330, loss = 1.24, batch loss = 1.15 (41.7 examples/sec; 0.192 sec/batch; 17h:32m:03s remains)
INFO - root - 2017-12-03 06:47:30.720423: step 3340, loss = 0.99, batch loss = 0.89 (44.5 examples/sec; 0.180 sec/batch; 16h:26m:08s remains)
INFO - root - 2017-12-03 06:47:32.545547: step 3350, loss = 1.17, batch loss = 1.08 (44.7 examples/sec; 0.179 sec/batch; 16h:21m:18s remains)
INFO - root - 2017-12-03 06:47:34.386886: step 3360, loss = 1.07, batch loss = 0.98 (44.1 examples/sec; 0.182 sec/batch; 16h:36m:14s remains)
INFO - root - 2017-12-03 06:47:36.220635: step 3370, loss = 1.20, batch loss = 1.10 (44.4 examples/sec; 0.180 sec/batch; 16h:29m:18s remains)
INFO - root - 2017-12-03 06:47:38.053798: step 3380, loss = 1.13, batch loss = 1.03 (43.4 examples/sec; 0.184 sec/batch; 16h:50m:46s remains)
INFO - root - 2017-12-03 06:47:39.929021: step 3390, loss = 1.24, batch loss = 1.15 (43.2 examples/sec; 0.185 sec/batch; 16h:55m:12s remains)
INFO - root - 2017-12-03 06:47:41.758125: step 3400, loss = 1.15, batch loss = 1.05 (44.5 examples/sec; 0.180 sec/batch; 16h:25m:33s remains)
INFO - root - 2017-12-03 06:47:43.701327: step 3410, loss = 1.19, batch loss = 1.09 (42.7 examples/sec; 0.187 sec/batch; 17h:06m:26s remains)
INFO - root - 2017-12-03 06:47:45.532115: step 3420, loss = 1.11, batch loss = 1.01 (45.0 examples/sec; 0.178 sec/batch; 16h:14m:09s remains)
INFO - root - 2017-12-03 06:47:47.405435: step 3430, loss = 1.10, batch loss = 1.01 (42.4 examples/sec; 0.189 sec/batch; 17h:15m:08s remains)
INFO - root - 2017-12-03 06:47:49.263223: step 3440, loss = 1.05, batch loss = 0.96 (42.4 examples/sec; 0.189 sec/batch; 17h:14m:06s remains)
INFO - root - 2017-12-03 06:47:51.114727: step 3450, loss = 1.10, batch loss = 1.00 (43.5 examples/sec; 0.184 sec/batch; 16h:48m:50s remains)
INFO - root - 2017-12-03 06:47:52.951244: step 3460, loss = 1.20, batch loss = 1.10 (43.8 examples/sec; 0.183 sec/batch; 16h:42m:39s remains)
INFO - root - 2017-12-03 06:47:54.799719: step 3470, loss = 1.15, batch loss = 1.05 (41.4 examples/sec; 0.193 sec/batch; 17h:40m:26s remains)
INFO - root - 2017-12-03 06:47:56.661637: step 3480, loss = 1.28, batch loss = 1.18 (44.0 examples/sec; 0.182 sec/batch; 16h:37m:44s remains)
INFO - root - 2017-12-03 06:47:58.497948: step 3490, loss = 1.33, batch loss = 1.24 (44.0 examples/sec; 0.182 sec/batch; 16h:36m:01s remains)
INFO - root - 2017-12-03 06:48:00.350725: step 3500, loss = 1.15, batch loss = 1.05 (44.0 examples/sec; 0.182 sec/batch; 16h:37m:58s remains)
INFO - root - 2017-12-03 06:48:02.305737: step 3510, loss = 1.14, batch loss = 1.04 (42.2 examples/sec; 0.190 sec/batch; 17h:19m:23s remains)
INFO - root - 2017-12-03 06:48:04.161094: step 3520, loss = 1.19, batch loss = 1.10 (43.1 examples/sec; 0.186 sec/batch; 16h:57m:50s remains)
INFO - root - 2017-12-03 06:48:06.000322: step 3530, loss = 1.15, batch loss = 1.05 (43.4 examples/sec; 0.184 sec/batch; 16h:50m:50s remains)
INFO - root - 2017-12-03 06:48:07.840226: step 3540, loss = 1.10, batch loss = 1.01 (43.5 examples/sec; 0.184 sec/batch; 16h:48m:34s remains)
INFO - root - 2017-12-03 06:48:09.684911: step 3550, loss = 1.21, batch loss = 1.11 (44.8 examples/sec; 0.179 sec/batch; 16h:19m:53s remains)
INFO - root - 2017-12-03 06:48:11.542319: step 3560, loss = 1.18, batch loss = 1.09 (43.4 examples/sec; 0.184 sec/batch; 16h:50m:31s remains)
INFO - root - 2017-12-03 06:48:13.402557: step 3570, loss = 1.19, batch loss = 1.10 (42.5 examples/sec; 0.188 sec/batch; 17h:12m:07s remains)
INFO - root - 2017-12-03 06:48:15.225895: step 3580, loss = 1.14, batch loss = 1.04 (45.0 examples/sec; 0.178 sec/batch; 16h:15m:30s remains)
INFO - root - 2017-12-03 06:48:17.062349: step 3590, loss = 1.07, batch loss = 0.98 (43.4 examples/sec; 0.184 sec/batch; 16h:49m:48s remains)
INFO - root - 2017-12-03 06:48:18.915169: step 3600, loss = 1.15, batch loss = 1.05 (43.0 examples/sec; 0.186 sec/batch; 16h:59m:34s remains)
INFO - root - 2017-12-03 06:48:20.826088: step 3610, loss = 1.13, batch loss = 1.03 (43.7 examples/sec; 0.183 sec/batch; 16h:42m:25s remains)
INFO - root - 2017-12-03 06:48:22.683693: step 3620, loss = 1.14, batch loss = 1.05 (42.0 examples/sec; 0.190 sec/batch; 17h:23m:44s remains)
INFO - root - 2017-12-03 06:48:24.523055: step 3630, loss = 1.21, batch loss = 1.11 (43.7 examples/sec; 0.183 sec/batch; 16h:42m:48s remains)
INFO - root - 2017-12-03 06:48:26.368713: step 3640, loss = 1.13, batch loss = 1.03 (44.4 examples/sec; 0.180 sec/batch; 16h:27m:54s remains)
INFO - root - 2017-12-03 06:48:28.216923: step 3650, loss = 1.22, batch loss = 1.12 (41.2 examples/sec; 0.194 sec/batch; 17h:43m:44s remains)
INFO - root - 2017-12-03 06:48:30.103399: step 3660, loss = 1.09, batch loss = 0.99 (42.5 examples/sec; 0.188 sec/batch; 17h:12m:25s remains)
INFO - root - 2017-12-03 06:48:31.945375: step 3670, loss = 1.10, batch loss = 1.00 (44.2 examples/sec; 0.181 sec/batch; 16h:33m:01s remains)
INFO - root - 2017-12-03 06:48:33.819608: step 3680, loss = 1.19, batch loss = 1.10 (43.9 examples/sec; 0.182 sec/batch; 16h:37m:55s remains)
INFO - root - 2017-12-03 06:48:35.673455: step 3690, loss = 1.32, batch loss = 1.22 (44.2 examples/sec; 0.181 sec/batch; 16h:32m:57s remains)
INFO - root - 2017-12-03 06:48:37.504904: step 3700, loss = 1.10, batch loss = 1.00 (43.1 examples/sec; 0.186 sec/batch; 16h:57m:37s remains)
INFO - root - 2017-12-03 06:48:39.411324: step 3710, loss = 1.14, batch loss = 1.04 (41.2 examples/sec; 0.194 sec/batch; 17h:44m:47s remains)
INFO - root - 2017-12-03 06:48:41.251733: step 3720, loss = 1.19, batch loss = 1.09 (44.2 examples/sec; 0.181 sec/batch; 16h:31m:19s remains)
INFO - root - 2017-12-03 06:48:43.123648: step 3730, loss = 1.17, batch loss = 1.07 (41.7 examples/sec; 0.192 sec/batch; 17h:31m:06s remains)
INFO - root - 2017-12-03 06:48:44.983054: step 3740, loss = 1.24, batch loss = 1.14 (44.2 examples/sec; 0.181 sec/batch; 16h:32m:30s remains)
INFO - root - 2017-12-03 06:48:46.810487: step 3750, loss = 1.17, batch loss = 1.07 (42.6 examples/sec; 0.188 sec/batch; 17h:08m:01s remains)
INFO - root - 2017-12-03 06:48:48.635067: step 3760, loss = 1.08, batch loss = 0.98 (43.9 examples/sec; 0.182 sec/batch; 16h:38m:47s remains)
INFO - root - 2017-12-03 06:48:50.471483: step 3770, loss = 1.21, batch loss = 1.12 (43.7 examples/sec; 0.183 sec/batch; 16h:42m:58s remains)
INFO - root - 2017-12-03 06:48:52.307267: step 3780, loss = 1.08, batch loss = 0.98 (42.9 examples/sec; 0.187 sec/batch; 17h:01m:52s remains)
INFO - root - 2017-12-03 06:48:54.148134: step 3790, loss = 1.12, batch loss = 1.02 (44.7 examples/sec; 0.179 sec/batch; 16h:21m:15s remains)
INFO - root - 2017-12-03 06:48:55.976886: step 3800, loss = 1.06, batch loss = 0.96 (43.8 examples/sec; 0.183 sec/batch; 16h:41m:18s remains)
INFO - root - 2017-12-03 06:48:57.868228: step 3810, loss = 1.16, batch loss = 1.06 (44.9 examples/sec; 0.178 sec/batch; 16h:15m:22s remains)
INFO - root - 2017-12-03 06:48:59.703916: step 3820, loss = 1.13, batch loss = 1.03 (43.2 examples/sec; 0.185 sec/batch; 16h:53m:23s remains)
INFO - root - 2017-12-03 06:49:01.539862: step 3830, loss = 1.28, batch loss = 1.18 (43.1 examples/sec; 0.186 sec/batch; 16h:57m:33s remains)
INFO - root - 2017-12-03 06:49:03.393289: step 3840, loss = 1.27, batch loss = 1.17 (44.1 examples/sec; 0.181 sec/batch; 16h:33m:43s remains)
INFO - root - 2017-12-03 06:49:05.245412: step 3850, loss = 1.01, batch loss = 0.91 (43.7 examples/sec; 0.183 sec/batch; 16h:42m:50s remains)
INFO - root - 2017-12-03 06:49:07.106265: step 3860, loss = 1.27, batch loss = 1.17 (43.3 examples/sec; 0.185 sec/batch; 16h:51m:27s remains)
INFO - root - 2017-12-03 06:49:08.938368: step 3870, loss = 1.13, batch loss = 1.03 (44.0 examples/sec; 0.182 sec/batch; 16h:35m:07s remains)
INFO - root - 2017-12-03 06:49:10.774206: step 3880, loss = 1.13, batch loss = 1.04 (43.3 examples/sec; 0.185 sec/batch; 16h:52m:25s remains)
INFO - root - 2017-12-03 06:49:12.608767: step 3890, loss = 1.13, batch loss = 1.03 (44.8 examples/sec; 0.179 sec/batch; 16h:18m:01s remains)
INFO - root - 2017-12-03 06:49:14.434661: step 3900, loss = 1.11, batch loss = 1.02 (43.4 examples/sec; 0.184 sec/batch; 16h:48m:28s remains)
INFO - root - 2017-12-03 06:49:16.324762: step 3910, loss = 1.28, batch loss = 1.19 (44.4 examples/sec; 0.180 sec/batch; 16h:26m:58s remains)
INFO - root - 2017-12-03 06:49:18.157018: step 3920, loss = 0.97, batch loss = 0.87 (43.7 examples/sec; 0.183 sec/batch; 16h:41m:52s remains)
INFO - root - 2017-12-03 06:49:20.014173: step 3930, loss = 1.07, batch loss = 0.97 (43.9 examples/sec; 0.182 sec/batch; 16h:37m:21s remains)
INFO - root - 2017-12-03 06:49:21.862430: step 3940, loss = 1.24, batch loss = 1.15 (43.8 examples/sec; 0.183 sec/batch; 16h:40m:10s remains)
INFO - root - 2017-12-03 06:49:23.724776: step 3950, loss = 1.39, batch loss = 1.29 (42.5 examples/sec; 0.188 sec/batch; 17h:10m:20s remains)
INFO - root - 2017-12-03 06:49:25.614253: step 3960, loss = 1.18, batch loss = 1.08 (43.4 examples/sec; 0.184 sec/batch; 16h:49m:55s remains)
INFO - root - 2017-12-03 06:49:27.442711: step 3970, loss = 1.28, batch loss = 1.18 (45.1 examples/sec; 0.177 sec/batch; 16h:10m:36s remains)
INFO - root - 2017-12-03 06:49:29.284052: step 3980, loss = 1.09, batch loss = 0.99 (42.6 examples/sec; 0.188 sec/batch; 17h:08m:11s remains)
INFO - root - 2017-12-03 06:49:31.109465: step 3990, loss = 1.26, batch loss = 1.17 (44.0 examples/sec; 0.182 sec/batch; 16h:34m:44s remains)
INFO - root - 2017-12-03 06:49:32.945021: step 4000, loss = 1.08, batch loss = 0.98 (43.9 examples/sec; 0.182 sec/batch; 16h:37m:29s remains)
INFO - root - 2017-12-03 06:49:34.865441: step 4010, loss = 1.16, batch loss = 1.06 (42.8 examples/sec; 0.187 sec/batch; 17h:03m:48s remains)
INFO - root - 2017-12-03 06:49:36.708901: step 4020, loss = 1.19, batch loss = 1.09 (44.4 examples/sec; 0.180 sec/batch; 16h:26m:19s remains)
INFO - root - 2017-12-03 06:49:38.546414: step 4030, loss = 1.19, batch loss = 1.09 (44.0 examples/sec; 0.182 sec/batch; 16h:34m:21s remains)
