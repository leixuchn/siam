INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "76"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 10:43:31.003646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:43:31.003686: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:43:31.003693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:43:31.003697: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:43:31.003792: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:43:32.081263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 4.03GiB
2017-12-05 10:43:32.081303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 10:43:32.081311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 10:43:32.081320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 10:43:41.858014: step 0, loss = 2.03, batch loss = 1.97 (1.1 examples/sec; 7.089 sec/batch; 654h:43m:26s remains)
2017-12-05 10:43:42.714500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3164864 -4.3127971 -4.3074846 -4.3024726 -4.2971616 -4.2883477 -4.2730789 -4.249011 -4.224597 -4.2192802 -4.2278719 -4.2483864 -4.276762 -4.3046613 -4.3257356][-4.308598 -4.3011303 -4.2894282 -4.278677 -4.2664337 -4.2488003 -4.2207837 -4.1826506 -4.1526136 -4.1545696 -4.17843 -4.2156711 -4.2592216 -4.2975059 -4.3243065][-4.2969584 -4.287262 -4.272357 -4.2578812 -4.2380142 -4.2064185 -4.1574874 -4.0997238 -4.0593266 -4.0676589 -4.1107674 -4.1702285 -4.2343845 -4.2855225 -4.3194008][-4.2829738 -4.274775 -4.2617455 -4.2446351 -4.2137308 -4.1638713 -4.0886993 -4.0056596 -3.9530849 -3.9721267 -4.0372753 -4.1213408 -4.2078109 -4.273469 -4.3149648][-4.2670279 -4.2603559 -4.2509317 -4.2294655 -4.1824436 -4.1092544 -3.999279 -3.8823152 -3.8189056 -3.8595791 -3.9550581 -4.0663829 -4.1774349 -4.2602425 -4.3106279][-4.2505908 -4.2447257 -4.2365746 -4.2064738 -4.1425037 -4.0444012 -3.8971007 -3.7389824 -3.6714213 -3.7540939 -3.8867922 -4.0256662 -4.1554909 -4.2488961 -4.3061833][-4.2378044 -4.2285972 -4.2137427 -4.1721416 -4.0966878 -3.9783454 -3.8030322 -3.6161628 -3.5761859 -3.7183492 -3.8828847 -4.0325546 -4.1592245 -4.2499108 -4.3065553][-4.2261019 -4.2081151 -4.182869 -4.13589 -4.0618129 -3.9490087 -3.7887206 -3.6342831 -3.6481822 -3.8112111 -3.96738 -4.0924892 -4.1932216 -4.2669992 -4.3150783][-4.2081966 -4.1849933 -4.1575532 -4.1154666 -4.0572529 -3.9748514 -3.8663754 -3.7777891 -3.8195283 -3.952369 -4.0719857 -4.15882 -4.2308421 -4.2868872 -4.3261952][-4.1912403 -4.1693168 -4.1475115 -4.1143641 -4.0724735 -4.0196757 -3.9575295 -3.9168072 -3.9616938 -4.0573592 -4.1412354 -4.1978135 -4.2497182 -4.296649 -4.3321495][-4.1778178 -4.1609898 -4.1439328 -4.1178894 -4.0896616 -4.0601673 -4.028955 -4.0151577 -4.0548167 -4.119874 -4.1759067 -4.21388 -4.2566495 -4.2993894 -4.33361][-4.1678286 -4.1589937 -4.14836 -4.1319137 -4.1152725 -4.103056 -4.0919523 -4.0905323 -4.1183805 -4.15773 -4.19485 -4.2217956 -4.25965 -4.3006234 -4.3349247][-4.178678 -4.1794481 -4.17662 -4.1673131 -4.1589384 -4.1591029 -4.1577063 -4.1558161 -4.1671228 -4.1834879 -4.2033763 -4.22178 -4.2569809 -4.2990627 -4.3352761][-4.2095866 -4.2173228 -4.2199292 -4.2151747 -4.211987 -4.21501 -4.2082129 -4.1954603 -4.190217 -4.189527 -4.1992636 -4.2158732 -4.2534719 -4.2986364 -4.3355012][-4.2383857 -4.2490096 -4.2546816 -4.2535076 -4.2525434 -4.2513785 -4.2321596 -4.2054124 -4.1868739 -4.1783242 -4.1884346 -4.2095242 -4.2520585 -4.2991843 -4.3360705]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 10:43:52.679193: step 10, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 82h:50m:05s remains)
INFO - root - 2017-12-05 10:44:01.718082: step 20, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 83h:40m:43s remains)
INFO - root - 2017-12-05 10:44:10.649575: step 30, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 84h:15m:33s remains)
INFO - root - 2017-12-05 10:44:19.462752: step 40, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 79h:36m:18s remains)
INFO - root - 2017-12-05 10:44:28.311708: step 50, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 82h:55m:03s remains)
INFO - root - 2017-12-05 10:44:37.236309: step 60, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.884 sec/batch; 81h:35m:32s remains)
INFO - root - 2017-12-05 10:44:46.101824: step 70, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 80h:49m:05s remains)
INFO - root - 2017-12-05 10:44:54.918317: step 80, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 82h:16m:45s remains)
INFO - root - 2017-12-05 10:45:03.701778: step 90, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.805 sec/batch; 74h:18m:15s remains)
INFO - root - 2017-12-05 10:45:12.442213: step 100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 78h:43m:25s remains)
2017-12-05 10:45:13.156899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2457089 -4.2098708 -4.1612096 -4.1167016 -4.0994487 -4.11319 -4.1371 -4.1617684 -4.2006173 -4.2308812 -4.2377934 -4.2380724 -4.2327385 -4.2095046 -4.1787987][-4.2481761 -4.206171 -4.1479931 -4.0924859 -4.0667319 -4.0773339 -4.1034088 -4.1288657 -4.1653152 -4.1980596 -4.21134 -4.220119 -4.2216425 -4.20576 -4.181107][-4.2522087 -4.2063527 -4.1444488 -4.0864534 -4.0588813 -4.065413 -4.0862789 -4.1086941 -4.1436553 -4.1806126 -4.197947 -4.2081242 -4.2115622 -4.2024293 -4.1829762][-4.2550354 -4.2089696 -4.1485305 -4.0925236 -4.0639143 -4.0626802 -4.0774837 -4.0988412 -4.1337876 -4.1753716 -4.1973948 -4.2085505 -4.2138691 -4.2105169 -4.1943007][-4.2560196 -4.2106929 -4.1534009 -4.099628 -4.0670662 -4.0555425 -4.062849 -4.0813231 -4.1141667 -4.1574249 -4.1853285 -4.1997008 -4.2094707 -4.2135978 -4.2038846][-4.2542677 -4.2107325 -4.1560779 -4.103406 -4.0660777 -4.0467091 -4.0480022 -4.0570183 -4.0774541 -4.1154785 -4.1522026 -4.1754918 -4.1915665 -4.2029896 -4.2008862][-4.2523708 -4.2107935 -4.15835 -4.1054854 -4.0642958 -4.0391593 -4.0349069 -4.0306559 -4.0334382 -4.0652862 -4.1103721 -4.1422405 -4.1630087 -4.1798978 -4.1847935][-4.2498441 -4.20768 -4.1542559 -4.10042 -4.0586767 -4.0332274 -4.0277238 -4.0193958 -4.0125213 -4.0390024 -4.086144 -4.1180458 -4.1374745 -4.1547952 -4.162046][-4.2508411 -4.2091308 -4.1565251 -4.1049728 -4.0672383 -4.0468059 -4.0436125 -4.0378804 -4.0316753 -4.0537653 -4.0928211 -4.1145329 -4.1265683 -4.1398296 -4.1446753][-4.2535524 -4.2113433 -4.1594558 -4.1084228 -4.0722361 -4.0535979 -4.0507431 -4.0513597 -4.0540662 -4.077651 -4.1074524 -4.1188364 -4.1218562 -4.1272683 -4.1255126][-4.2608194 -4.218111 -4.1673007 -4.1177354 -4.0825872 -4.0621762 -4.0573816 -4.0631356 -4.0747461 -4.0989203 -4.1210485 -4.1258879 -4.1211061 -4.1198506 -4.1155081][-4.27349 -4.2328486 -4.1853786 -4.1401086 -4.1097922 -4.0927095 -4.0894022 -4.0995092 -4.1133518 -4.1328216 -4.1471953 -4.1481705 -4.140564 -4.1369023 -4.132184][-4.2882581 -4.2525096 -4.2110815 -4.1718512 -4.1458807 -4.1333137 -4.1357789 -4.1519518 -4.1671667 -4.1813254 -4.1902618 -4.1907177 -4.1828537 -4.1775045 -4.1715927][-4.3057795 -4.2789116 -4.2477288 -4.2168031 -4.1936393 -4.183763 -4.1917338 -4.2118278 -4.2273626 -4.2384114 -4.2442627 -4.2436261 -4.2345333 -4.22665 -4.2204137][-4.3224654 -4.3056831 -4.2864923 -4.2670846 -4.2493434 -4.2410445 -4.2491479 -4.2665224 -4.2786446 -4.2851081 -4.2888017 -4.2877946 -4.2792916 -4.2724147 -4.2685127]]...]
INFO - root - 2017-12-05 10:45:22.034162: step 110, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 80h:28m:21s remains)
INFO - root - 2017-12-05 10:45:30.829305: step 120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 80h:29m:26s remains)
INFO - root - 2017-12-05 10:45:39.597104: step 130, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 81h:33m:26s remains)
INFO - root - 2017-12-05 10:45:48.486456: step 140, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 80h:53m:42s remains)
INFO - root - 2017-12-05 10:45:57.424162: step 150, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.874 sec/batch; 80h:38m:47s remains)
INFO - root - 2017-12-05 10:46:06.091469: step 160, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 81h:55m:46s remains)
INFO - root - 2017-12-05 10:46:14.926075: step 170, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 81h:28m:34s remains)
INFO - root - 2017-12-05 10:46:23.644435: step 180, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 79h:25m:05s remains)
INFO - root - 2017-12-05 10:46:32.560292: step 190, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 82h:07m:42s remains)
INFO - root - 2017-12-05 10:46:41.415642: step 200, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 82h:33m:05s remains)
2017-12-05 10:46:42.107260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2403269 -4.2389069 -4.23694 -4.227572 -4.2156491 -4.2182937 -4.228004 -4.2300878 -4.2294426 -4.2307405 -4.240324 -4.2506957 -4.255197 -4.2530231 -4.2469339][-4.2434821 -4.230618 -4.2173786 -4.2005329 -4.1873493 -4.1916542 -4.20434 -4.2094946 -4.2053113 -4.2049336 -4.2200742 -4.23689 -4.244885 -4.2416582 -4.2326751][-4.23376 -4.2192512 -4.2089415 -4.1957145 -4.1843767 -4.1879859 -4.1990948 -4.2055135 -4.2021756 -4.2043877 -4.2168236 -4.22988 -4.2398295 -4.2368488 -4.22614][-4.2153864 -4.1999111 -4.2013555 -4.1982603 -4.1905355 -4.189466 -4.1939831 -4.1997137 -4.2069144 -4.2229619 -4.2310195 -4.2311072 -4.2355127 -4.2315021 -4.2206855][-4.2116828 -4.1905856 -4.1963825 -4.198185 -4.1932707 -4.184895 -4.1799049 -4.1819139 -4.1996675 -4.232296 -4.2398791 -4.230442 -4.2253051 -4.2188306 -4.2137618][-4.2010193 -4.1794515 -4.1834254 -4.186625 -4.178153 -4.1599741 -4.1380057 -4.1279459 -4.1578941 -4.21096 -4.2292604 -4.223433 -4.2134895 -4.2025762 -4.2020488][-4.1915932 -4.1656961 -4.16326 -4.1660385 -4.15072 -4.1115985 -4.0504513 -4.0038571 -4.0554023 -4.1465516 -4.1896257 -4.1946549 -4.1850572 -4.1736937 -4.1820116][-4.1720505 -4.1415873 -4.134377 -4.1392746 -4.1227331 -4.0664744 -3.9571261 -3.8514285 -3.9244952 -4.0670066 -4.1455169 -4.1632962 -4.1513119 -4.1446738 -4.1599073][-4.1548934 -4.1310582 -4.129003 -4.1406994 -4.1335983 -4.0891747 -3.990236 -3.8842912 -3.9414449 -4.0747724 -4.1632028 -4.1946306 -4.1846328 -4.1678858 -4.1647935][-4.164104 -4.1496048 -4.1512909 -4.1665983 -4.1683064 -4.1487761 -4.0944271 -4.028451 -4.0570741 -4.142365 -4.2141581 -4.2498546 -4.2408538 -4.2165165 -4.19638][-4.1895084 -4.1793737 -4.1776395 -4.1843271 -4.1875782 -4.183104 -4.1574335 -4.1205144 -4.1356511 -4.18741 -4.2399158 -4.2683506 -4.25646 -4.2285523 -4.2050672][-4.1978879 -4.1885829 -4.1839976 -4.1819987 -4.1853886 -4.1904421 -4.1803226 -4.1589642 -4.1666594 -4.1977744 -4.2330279 -4.2519484 -4.2463193 -4.2265286 -4.2098351][-4.2120261 -4.201292 -4.196733 -4.1927195 -4.1927257 -4.1985879 -4.1946259 -4.1825 -4.1883416 -4.2068629 -4.2279572 -4.2409892 -4.2446275 -4.23982 -4.2355294][-4.2363024 -4.2318769 -4.2348466 -4.2336349 -4.2311268 -4.2335563 -4.2269883 -4.2158928 -4.2175517 -4.2289891 -4.2444983 -4.2558279 -4.2622395 -4.2638569 -4.2640615][-4.2488732 -4.2517366 -4.2588468 -4.2602611 -4.2580838 -4.2602358 -4.2542148 -4.2451324 -4.2418447 -4.2468753 -4.2586932 -4.2697453 -4.2736034 -4.2728167 -4.2729621]]...]
INFO - root - 2017-12-05 10:46:50.990665: step 210, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 81h:26m:19s remains)
INFO - root - 2017-12-05 10:46:59.862902: step 220, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 84h:04m:36s remains)
INFO - root - 2017-12-05 10:47:08.775111: step 230, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 81h:42m:23s remains)
INFO - root - 2017-12-05 10:47:17.795410: step 240, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 83h:58m:17s remains)
INFO - root - 2017-12-05 10:47:26.749087: step 250, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 80h:43m:29s remains)
INFO - root - 2017-12-05 10:47:35.482714: step 260, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 78h:12m:53s remains)
INFO - root - 2017-12-05 10:47:49.856199: step 270, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 81h:14m:12s remains)
INFO - root - 2017-12-05 10:47:58.673550: step 280, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 86h:00m:00s remains)
INFO - root - 2017-12-05 10:48:07.540516: step 290, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 81h:00m:12s remains)
INFO - root - 2017-12-05 10:48:16.441498: step 300, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 82h:50m:41s remains)
2017-12-05 10:48:17.255483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0787525 -4.0788569 -4.1035266 -4.1389275 -4.1712852 -4.1960049 -4.2058086 -4.2006083 -4.1788516 -4.1548152 -4.1419659 -4.1394849 -4.1458387 -4.1500449 -4.1543403][-4.139678 -4.1334085 -4.1384764 -4.1545205 -4.1791568 -4.204504 -4.21687 -4.21533 -4.2042 -4.195116 -4.1930518 -4.1905365 -4.1874232 -4.1792269 -4.1718154][-4.1955867 -4.1871428 -4.1745892 -4.1702781 -4.1808629 -4.2047095 -4.221158 -4.228518 -4.2326579 -4.2374649 -4.2415347 -4.2367387 -4.2241845 -4.2046609 -4.1843495][-4.22261 -4.212235 -4.1886888 -4.1681333 -4.1664524 -4.1890912 -4.2115388 -4.2284455 -4.2443542 -4.2568703 -4.2618055 -4.2511721 -4.2310033 -4.2053466 -4.1829381][-4.2163997 -4.2050052 -4.1804843 -4.154089 -4.143641 -4.1607895 -4.1840248 -4.2078409 -4.2306619 -4.2453785 -4.2481394 -4.2307882 -4.2046485 -4.1776462 -4.1602392][-4.1867533 -4.1755776 -4.154923 -4.133183 -4.1248736 -4.1316004 -4.1494389 -4.1756687 -4.1964273 -4.2056127 -4.2021427 -4.182025 -4.1552372 -4.1319275 -4.1246762][-4.1645751 -4.1500478 -4.1324978 -4.1200919 -4.1176 -4.1159682 -4.125474 -4.1471415 -4.1619654 -4.1612163 -4.1504521 -4.1278839 -4.0993686 -4.0805864 -4.0833731][-4.1568532 -4.1393404 -4.12367 -4.1181726 -4.1212955 -4.1144304 -4.1180887 -4.1334376 -4.14113 -4.1320515 -4.1167712 -4.0929937 -4.0625739 -4.0480046 -4.0598655][-4.1651559 -4.1510816 -4.141952 -4.1432915 -4.1482854 -4.1358595 -4.1322565 -4.1423631 -4.1467438 -4.1351452 -4.1218371 -4.1047873 -4.0781837 -4.0683103 -4.0826421][-4.1843944 -4.1778274 -4.178813 -4.1885638 -4.1953664 -4.1823997 -4.1712918 -4.1760697 -4.1793389 -4.1675415 -4.1563663 -4.1480722 -4.130003 -4.1253114 -4.1370497][-4.1943188 -4.196002 -4.205667 -4.2199469 -4.2295403 -4.2194057 -4.2058754 -4.2044196 -4.205811 -4.1964855 -4.1902418 -4.1914072 -4.1842256 -4.182519 -4.1910291][-4.192874 -4.2036037 -4.2212143 -4.2392416 -4.2511711 -4.2451577 -4.2310729 -4.2247725 -4.2224064 -4.2103462 -4.2042775 -4.2129173 -4.2168312 -4.2183933 -4.2270079][-4.1782932 -4.1949167 -4.216682 -4.2382326 -4.2545424 -4.2521849 -4.240726 -4.2321939 -4.2240129 -4.2093124 -4.2046742 -4.2180882 -4.2263532 -4.2319117 -4.2437487][-4.1765141 -4.191134 -4.2112093 -4.2315645 -4.2483993 -4.2494903 -4.2403569 -4.2294092 -4.2185779 -4.2055154 -4.2033429 -4.218143 -4.2282662 -4.236526 -4.247056][-4.1913571 -4.1980729 -4.2117572 -4.2264795 -4.2406435 -4.2464585 -4.2436428 -4.2341204 -4.220571 -4.2073941 -4.206521 -4.2178569 -4.2234058 -4.22865 -4.2339716]]...]
INFO - root - 2017-12-05 10:48:26.054140: step 310, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 81h:32m:39s remains)
INFO - root - 2017-12-05 10:48:34.942645: step 320, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 83h:59m:25s remains)
INFO - root - 2017-12-05 10:48:43.980634: step 330, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 84h:23m:50s remains)
INFO - root - 2017-12-05 10:48:53.055454: step 340, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 84h:24m:01s remains)
INFO - root - 2017-12-05 10:49:02.006237: step 350, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 83h:13m:46s remains)
INFO - root - 2017-12-05 10:49:11.218538: step 360, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 82h:59m:24s remains)
INFO - root - 2017-12-05 10:49:20.350831: step 370, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 81h:52m:18s remains)
INFO - root - 2017-12-05 10:49:29.251531: step 380, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 81h:48m:40s remains)
INFO - root - 2017-12-05 10:49:38.247417: step 390, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 76h:53m:00s remains)
INFO - root - 2017-12-05 10:49:47.403170: step 400, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 85h:58m:04s remains)
2017-12-05 10:49:48.173727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2730584 -4.26476 -4.24787 -4.2323103 -4.2252607 -4.2321091 -4.24946 -4.2697277 -4.2832551 -4.2835441 -4.2733788 -4.2541928 -4.2334218 -4.2148981 -4.1986742][-4.2868552 -4.2836881 -4.2672896 -4.2485919 -4.2384543 -4.2444086 -4.2635136 -4.2904906 -4.31329 -4.3227758 -4.3169641 -4.2934656 -4.2604485 -4.225059 -4.192512][-4.2855468 -4.28375 -4.2647891 -4.2420616 -4.2303171 -4.2355633 -4.2520852 -4.2794905 -4.3103924 -4.3325963 -4.3380876 -4.31897 -4.2826147 -4.234786 -4.1829653][-4.2795262 -4.2687883 -4.2403975 -4.2094588 -4.1950874 -4.1996865 -4.2113452 -4.2341909 -4.2692032 -4.3053555 -4.326714 -4.3195691 -4.290596 -4.2402182 -4.17574][-4.2753263 -4.2497773 -4.205132 -4.1611238 -4.140974 -4.138103 -4.136426 -4.1483836 -4.1875343 -4.2440519 -4.2884865 -4.3004236 -4.2868633 -4.243269 -4.1742768][-4.2696152 -4.2345948 -4.1780567 -4.1199679 -4.0881586 -4.0684295 -4.0406313 -4.0330844 -4.079432 -4.1653605 -4.2385969 -4.271626 -4.2754593 -4.2461414 -4.181963][-4.2725463 -4.2357154 -4.1754608 -4.1059961 -4.05356 -3.9998386 -3.9301832 -3.8887796 -3.9373446 -4.0580082 -4.1651893 -4.2257323 -4.2526407 -4.248044 -4.2063][-4.2823582 -4.2502303 -4.1981964 -4.1312995 -4.0657649 -3.9852276 -3.8768082 -3.7904174 -3.820127 -3.9582317 -4.0863876 -4.1700039 -4.2218685 -4.246201 -4.2358961][-4.2919855 -4.2720656 -4.2402763 -4.19575 -4.1417775 -4.0641117 -3.9555478 -3.8598039 -3.8577044 -3.9527071 -4.0524211 -4.1302834 -4.1929932 -4.2370067 -4.2540298][-4.3007226 -4.2934694 -4.2813139 -4.260962 -4.229794 -4.1762753 -4.0962353 -4.020216 -4.0029874 -4.046453 -4.100245 -4.1530619 -4.203527 -4.2466068 -4.2725868][-4.3090849 -4.310389 -4.309494 -4.3045068 -4.2925282 -4.2676811 -4.2247348 -4.1799865 -4.1639118 -4.1761541 -4.1955061 -4.2227578 -4.2543378 -4.2824616 -4.300704][-4.3174086 -4.3239694 -4.3273325 -4.327692 -4.3244491 -4.3190246 -4.30658 -4.2895622 -4.2799854 -4.2795887 -4.2828269 -4.2914968 -4.3050117 -4.31865 -4.3249121][-4.3261075 -4.3338246 -4.3375506 -4.3364935 -4.333869 -4.3348074 -4.3359704 -4.3346081 -4.3330469 -4.3323455 -4.3310556 -4.3298178 -4.3315811 -4.3360262 -4.3345833][-4.3317528 -4.3367791 -4.3391318 -4.3368487 -4.3335261 -4.3340421 -4.3365264 -4.338295 -4.3398027 -4.3406873 -4.3403153 -4.33778 -4.3371568 -4.3387923 -4.3353662][-4.3317308 -4.333952 -4.3347149 -4.3324347 -4.3300848 -4.3306422 -4.3316083 -4.3310218 -4.3308759 -4.3313375 -4.3324933 -4.3325844 -4.3337483 -4.335506 -4.3335142]]...]
INFO - root - 2017-12-05 10:49:57.210988: step 410, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 84h:44m:25s remains)
INFO - root - 2017-12-05 10:50:06.349200: step 420, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 82h:31m:21s remains)
INFO - root - 2017-12-05 10:50:15.372550: step 430, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 84h:30m:37s remains)
INFO - root - 2017-12-05 10:50:24.466991: step 440, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 82h:10m:04s remains)
INFO - root - 2017-12-05 10:50:33.766277: step 450, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 87h:00m:38s remains)
INFO - root - 2017-12-05 10:50:43.081207: step 460, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 85h:03m:48s remains)
INFO - root - 2017-12-05 10:50:52.024148: step 470, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 84h:53m:29s remains)
INFO - root - 2017-12-05 10:51:00.959415: step 480, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 82h:11m:29s remains)
INFO - root - 2017-12-05 10:51:10.041313: step 490, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 84h:57m:15s remains)
INFO - root - 2017-12-05 10:51:19.248418: step 500, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 84h:26m:52s remains)
2017-12-05 10:51:19.956656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1796727 -4.2181778 -4.2532082 -4.2804427 -4.2838936 -4.2579966 -4.2238269 -4.2060165 -4.2006383 -4.2020593 -4.2145739 -4.2318296 -4.2507334 -4.2594314 -4.2345276][-4.1583247 -4.2091417 -4.2539124 -4.2851868 -4.2916865 -4.2695446 -4.2310266 -4.2058253 -4.1956949 -4.1958494 -4.2062016 -4.2207131 -4.23781 -4.2504768 -4.2325935][-4.1554 -4.2188153 -4.2688875 -4.2946863 -4.2987461 -4.2766223 -4.2335167 -4.2028127 -4.1880541 -4.1863761 -4.1973162 -4.2128229 -4.2317357 -4.2483826 -4.239974][-4.174593 -4.2413445 -4.2843056 -4.2982907 -4.2957811 -4.2682271 -4.2186413 -4.1787124 -4.1598864 -4.1623807 -4.1856356 -4.2143521 -4.2386446 -4.2562685 -4.2572536][-4.2015514 -4.2596941 -4.290484 -4.2920656 -4.2804289 -4.2443061 -4.184042 -4.1307969 -4.1165342 -4.1375928 -4.1812568 -4.2235661 -4.2525883 -4.2714567 -4.27709][-4.2237444 -4.2642512 -4.2848721 -4.2766647 -4.256124 -4.207942 -4.127027 -4.0596523 -4.0677452 -4.1169477 -4.1804132 -4.2291012 -4.2572942 -4.2746925 -4.2811875][-4.2263207 -4.2473984 -4.2594957 -4.2446556 -4.2166343 -4.1479349 -4.032783 -3.957824 -4.0096655 -4.0938215 -4.1735673 -4.2266035 -4.2541714 -4.2708082 -4.2752314][-4.2064185 -4.2145486 -4.2208066 -4.2022662 -4.1635313 -4.0735793 -3.9321797 -3.8717031 -3.9729819 -4.0853519 -4.1734762 -4.2264223 -4.2534432 -4.2695169 -4.2709789][-4.1752162 -4.1770363 -4.1832156 -4.1660519 -4.1248279 -4.0372729 -3.9207151 -3.9043999 -4.0123143 -4.1175189 -4.1944141 -4.2379918 -4.2603536 -4.2752976 -4.273653][-4.1376719 -4.1395879 -4.149478 -4.1417446 -4.108562 -4.0444493 -3.9797468 -3.9976254 -4.0851789 -4.1652279 -4.2235074 -4.2553306 -4.2702971 -4.2818561 -4.2757988][-4.1160536 -4.1163788 -4.1255078 -4.127614 -4.1088014 -4.0736418 -4.0487919 -4.0801835 -4.1462789 -4.2031708 -4.2468925 -4.2729073 -4.2859249 -4.294075 -4.2852941][-4.109488 -4.1070457 -4.1148558 -4.1238708 -4.120646 -4.1098886 -4.106513 -4.1351752 -4.181231 -4.2226663 -4.2584629 -4.2819247 -4.296627 -4.3039889 -4.297132][-4.121944 -4.1243029 -4.1335793 -4.148272 -4.1559515 -4.1583133 -4.1620526 -4.18083 -4.2099791 -4.2410431 -4.2706084 -4.2915363 -4.3049784 -4.3088832 -4.3044782][-4.1691546 -4.1727018 -4.1820607 -4.1981659 -4.2091665 -4.2154927 -4.2188745 -4.2293158 -4.2477031 -4.2697535 -4.2915926 -4.3068261 -4.3150692 -4.3140111 -4.3099222][-4.2361259 -4.2391791 -4.2474046 -4.2597575 -4.2648854 -4.2679162 -4.2672615 -4.273035 -4.283896 -4.2976027 -4.3127809 -4.3223329 -4.3254485 -4.3208833 -4.316587]]...]
INFO - root - 2017-12-05 10:51:29.103473: step 510, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 83h:00m:34s remains)
INFO - root - 2017-12-05 10:51:38.201056: step 520, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 85h:39m:08s remains)
INFO - root - 2017-12-05 10:51:47.192512: step 530, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 84h:00m:09s remains)
INFO - root - 2017-12-05 10:51:56.334677: step 540, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 84h:01m:52s remains)
INFO - root - 2017-12-05 10:52:05.363081: step 550, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 84h:14m:52s remains)
INFO - root - 2017-12-05 10:52:14.474857: step 560, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 83h:00m:20s remains)
INFO - root - 2017-12-05 10:52:23.497250: step 570, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 82h:36m:53s remains)
INFO - root - 2017-12-05 10:52:32.435026: step 580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 77h:52m:34s remains)
INFO - root - 2017-12-05 10:52:41.553328: step 590, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 84h:19m:49s remains)
INFO - root - 2017-12-05 10:52:50.658583: step 600, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 85h:15m:14s remains)
2017-12-05 10:52:51.435698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2159762 -4.207243 -4.2122865 -4.2178812 -4.2213297 -4.2205281 -4.2285085 -4.2552032 -4.2918563 -4.3160787 -4.3248782 -4.3203015 -4.299026 -4.2788954 -4.2765179][-4.2215033 -4.2174387 -4.2213731 -4.2275066 -4.2289457 -4.22383 -4.2256694 -4.2496824 -4.2863731 -4.3092232 -4.3151293 -4.3101006 -4.2979712 -4.2911582 -4.2995019][-4.2205215 -4.2130537 -4.2153358 -4.2279363 -4.2303944 -4.2188206 -4.2135792 -4.2347808 -4.2685342 -4.2873816 -4.293252 -4.2925029 -4.2883167 -4.2891545 -4.3039927][-4.2094569 -4.1994338 -4.2032752 -4.2196627 -4.2201204 -4.1983256 -4.1807175 -4.1940746 -4.2234097 -4.2450714 -4.2607312 -4.269424 -4.2691236 -4.2714319 -4.2862358][-4.1896758 -4.185348 -4.1931605 -4.2065449 -4.1989141 -4.1634989 -4.1328893 -4.1411223 -4.1741595 -4.2056718 -4.231729 -4.2451086 -4.2434607 -4.2385454 -4.2444248][-4.1596279 -4.158792 -4.158288 -4.1533895 -4.1285934 -4.0829735 -4.0492325 -4.0629191 -4.1122971 -4.1579137 -4.1898093 -4.2012105 -4.1890182 -4.1708937 -4.1676545][-4.1443267 -4.1394444 -4.1263871 -4.0989914 -4.0533929 -3.9966242 -3.9642191 -3.9874749 -4.0495238 -4.0988584 -4.126513 -4.12925 -4.1038527 -4.0782928 -4.0780411][-4.1520376 -4.138154 -4.1166129 -4.0798655 -4.0279484 -3.9707549 -3.9424529 -3.9646606 -4.0144653 -4.0446477 -4.0508509 -4.0395827 -4.0099754 -3.9929314 -4.0128965][-4.1815639 -4.1626368 -4.1416149 -4.1116595 -4.0689754 -4.0218592 -3.9986773 -4.0107131 -4.034934 -4.0364 -4.0200372 -3.9940331 -3.9634557 -3.95593 -3.9906814][-4.2162457 -4.2001557 -4.1838956 -4.1662092 -4.1370282 -4.1025133 -4.0859876 -4.0912089 -4.0981817 -4.0869346 -4.0638976 -4.0355229 -4.007751 -4.0046139 -4.0390878][-4.2525854 -4.2401161 -4.2246366 -4.2099357 -4.1928687 -4.1737471 -4.1672211 -4.17075 -4.1721969 -4.1620636 -4.1453986 -4.1243958 -4.1042647 -4.1050529 -4.1311688][-4.272541 -4.2611604 -4.2433505 -4.2289491 -4.2191582 -4.2131414 -4.219038 -4.22627 -4.2301135 -4.2265797 -4.2173028 -4.2034526 -4.1909485 -4.19383 -4.2112007][-4.27508 -4.2604575 -4.2397523 -4.225481 -4.2229853 -4.2274814 -4.240345 -4.2495775 -4.2549834 -4.2552052 -4.250546 -4.2429652 -4.2355494 -4.2390132 -4.2510853][-4.2714605 -4.2602487 -4.2401743 -4.2257128 -4.2279239 -4.2363548 -4.2490296 -4.2549481 -4.2561178 -4.2535391 -4.2486353 -4.2431393 -4.2394748 -4.24564 -4.2569318][-4.2721448 -4.2691 -4.2543983 -4.241487 -4.2424927 -4.2471895 -4.2526054 -4.2515903 -4.2466249 -4.2373967 -4.2265525 -4.2182288 -4.2160931 -4.2253923 -4.2405171]]...]
INFO - root - 2017-12-05 10:53:00.344300: step 610, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 81h:23m:17s remains)
INFO - root - 2017-12-05 10:53:09.465463: step 620, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 85h:12m:57s remains)
INFO - root - 2017-12-05 10:53:18.718854: step 630, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 83h:34m:34s remains)
INFO - root - 2017-12-05 10:53:27.884491: step 640, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 84h:15m:27s remains)
INFO - root - 2017-12-05 10:53:36.892409: step 650, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 82h:16m:41s remains)
INFO - root - 2017-12-05 10:53:45.956255: step 660, loss = 2.05, batch loss = 1.99 (8.1 examples/sec; 0.987 sec/batch; 90h:58m:14s remains)
INFO - root - 2017-12-05 10:53:55.094262: step 670, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 85h:08m:26s remains)
INFO - root - 2017-12-05 10:54:04.191798: step 680, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 80h:59m:27s remains)
INFO - root - 2017-12-05 10:54:13.408502: step 690, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 82h:00m:08s remains)
INFO - root - 2017-12-05 10:54:22.491762: step 700, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 84h:51m:55s remains)
2017-12-05 10:54:23.242560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1821489 -4.1838355 -4.1870008 -4.1915035 -4.1954861 -4.197331 -4.196661 -4.1957922 -4.1959658 -4.1967406 -4.1944284 -4.1866217 -4.1747413 -4.1641874 -4.1633263][-4.1815248 -4.1805243 -4.1810579 -4.1828427 -4.1841512 -4.1828318 -4.1779604 -4.173388 -4.1724954 -4.1734862 -4.170948 -4.16235 -4.1503515 -4.1419845 -4.1444993][-4.20385 -4.2002153 -4.1973548 -4.1957517 -4.1947012 -4.1902418 -4.1808977 -4.1712308 -4.1661263 -4.1643353 -4.1613536 -4.1544857 -4.1451783 -4.1394224 -4.1445503][-4.2251954 -4.2215738 -4.2187967 -4.2187266 -4.2201538 -4.2166443 -4.2061868 -4.194478 -4.1862054 -4.1809812 -4.177031 -4.1723995 -4.1662149 -4.1618695 -4.1660533][-4.2168531 -4.2126503 -4.21189 -4.2168941 -4.2239628 -4.2246041 -4.2174077 -4.2090111 -4.2039814 -4.2007766 -4.1985426 -4.196579 -4.1918626 -4.1856618 -4.1860194][-4.1863823 -4.1776857 -4.1771913 -4.18767 -4.1996045 -4.2023711 -4.1967578 -4.1940742 -4.1996965 -4.2056918 -4.2108283 -4.2162061 -4.2147408 -4.2056184 -4.1990547][-4.1569071 -4.1409316 -4.1364632 -4.1474042 -4.1582685 -4.1548281 -4.1442571 -4.1462035 -4.16516 -4.1862388 -4.2053113 -4.2228785 -4.228611 -4.2199063 -4.2099481][-4.1453395 -4.1269064 -4.1191187 -4.1255026 -4.1283755 -4.1118279 -4.0887222 -4.08822 -4.1131659 -4.1469464 -4.180747 -4.21175 -4.2275505 -4.2234497 -4.2144403][-4.1571589 -4.1445074 -4.1376023 -4.1390128 -4.1335096 -4.1073713 -4.07522 -4.0679159 -4.0889268 -4.1235676 -4.1615791 -4.1976438 -4.2184238 -4.2189331 -4.2120872][-4.18607 -4.1800022 -4.1752934 -4.1744766 -4.1672826 -4.1434579 -4.1142454 -4.1050143 -4.1165881 -4.1411524 -4.1698489 -4.1974683 -4.2130804 -4.2124176 -4.20646][-4.2211514 -4.2168055 -4.2101593 -4.2054467 -4.1970057 -4.1794934 -4.1597877 -4.1551661 -4.1649575 -4.182651 -4.2012949 -4.2158661 -4.2194672 -4.2107062 -4.2019067][-4.2465835 -4.2393069 -4.2286572 -4.2190228 -4.2080288 -4.1943817 -4.1836905 -4.1854792 -4.1962967 -4.2112069 -4.2251625 -4.2315483 -4.225481 -4.2095351 -4.1981564][-4.2543125 -4.2461643 -4.2361226 -4.2276149 -4.2173924 -4.2058177 -4.1988244 -4.2010059 -4.2089081 -4.2206426 -4.231596 -4.2341261 -4.2250733 -4.20817 -4.1987329][-4.2442203 -4.23806 -4.2307525 -4.2264404 -4.2206469 -4.2135577 -4.2087278 -4.2095146 -4.2131906 -4.2205977 -4.2282748 -4.2281671 -4.2181768 -4.2025743 -4.1981196][-4.2280717 -4.2234392 -4.2182369 -4.2171354 -4.2152495 -4.2128744 -4.2114577 -4.2124515 -4.2131667 -4.2157526 -4.2191825 -4.2179108 -4.2093549 -4.1969571 -4.1973128]]...]
INFO - root - 2017-12-05 10:54:32.340323: step 710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 82h:33m:03s remains)
INFO - root - 2017-12-05 10:54:41.508628: step 720, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.913 sec/batch; 84h:07m:45s remains)
INFO - root - 2017-12-05 10:54:50.612933: step 730, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 84h:08m:29s remains)
INFO - root - 2017-12-05 10:54:59.599831: step 740, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 82h:46m:39s remains)
INFO - root - 2017-12-05 10:55:08.670353: step 750, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 86h:04m:34s remains)
INFO - root - 2017-12-05 10:55:17.750081: step 760, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 83h:46m:05s remains)
INFO - root - 2017-12-05 10:55:26.748815: step 770, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 83h:32m:12s remains)
INFO - root - 2017-12-05 10:55:35.812180: step 780, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 82h:51m:02s remains)
INFO - root - 2017-12-05 10:55:44.824783: step 790, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 87h:29m:21s remains)
INFO - root - 2017-12-05 10:55:53.837764: step 800, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 86h:56m:40s remains)
2017-12-05 10:55:54.678272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1096625 -4.1117177 -4.1087384 -4.09811 -4.0944414 -4.1215062 -4.1547332 -4.1705637 -4.1677885 -4.1527052 -4.1402397 -4.133615 -4.128602 -4.1341238 -4.1463466][-4.1502872 -4.1573253 -4.1607122 -4.1552997 -4.15554 -4.1724634 -4.18293 -4.1706958 -4.1438255 -4.1177673 -4.1096959 -4.1093545 -4.1071506 -4.1063595 -4.1118112][-4.1935234 -4.2018147 -4.209331 -4.2130365 -4.215127 -4.2167583 -4.2047877 -4.1687908 -4.1208363 -4.086998 -4.0818553 -4.0878596 -4.0881958 -4.0845485 -4.0875225][-4.2004108 -4.2099009 -4.2146297 -4.218739 -4.2204828 -4.2134118 -4.1867046 -4.14069 -4.0984988 -4.0740175 -4.0765777 -4.0870051 -4.0900192 -4.0838952 -4.0834074][-4.1823587 -4.1976628 -4.1994843 -4.1995492 -4.1974392 -4.1814871 -4.1495056 -4.1113296 -4.0859737 -4.0718803 -4.0754929 -4.0825973 -4.0860653 -4.0796704 -4.0775323][-4.1664524 -4.1920395 -4.1931257 -4.1912804 -4.1851974 -4.1626058 -4.12719 -4.09307 -4.0695376 -4.06844 -4.0844374 -4.0962534 -4.104126 -4.100307 -4.0971413][-4.1556282 -4.1879015 -4.1951861 -4.1977134 -4.1893544 -4.1602106 -4.113656 -4.0639138 -4.0360837 -4.0639358 -4.1071587 -4.1290731 -4.1384454 -4.1296086 -4.119988][-4.1575966 -4.190474 -4.2033653 -4.20286 -4.1844168 -4.1373382 -4.0606117 -3.9674394 -3.9280972 -3.9958463 -4.0818677 -4.12963 -4.1459546 -4.1301541 -4.1169386][-4.1608753 -4.1910915 -4.2013836 -4.19325 -4.1606951 -4.0957532 -3.9822783 -3.8457949 -3.8052077 -3.9152808 -4.0400257 -4.1079087 -4.1324415 -4.1151452 -4.10185][-4.168304 -4.1923456 -4.1991539 -4.1873875 -4.1573396 -4.1051254 -4.0111403 -3.901479 -3.8847365 -3.9861491 -4.0854783 -4.1288681 -4.1418986 -4.1258378 -4.1096315][-4.1858563 -4.2029834 -4.2051635 -4.1966023 -4.1771851 -4.1547575 -4.1093793 -4.0547857 -4.0550475 -4.1157036 -4.1676927 -4.1820822 -4.184186 -4.1695108 -4.1537189][-4.2059665 -4.217545 -4.2145028 -4.2060146 -4.1962357 -4.1930895 -4.1843305 -4.1659431 -4.1698928 -4.2018042 -4.2272081 -4.2278738 -4.2239523 -4.2127142 -4.2006283][-4.1984091 -4.2099319 -4.2084694 -4.2077188 -4.209321 -4.2173915 -4.2222714 -4.220088 -4.2258377 -4.2442431 -4.2571888 -4.256392 -4.2533994 -4.2441821 -4.2330894][-4.1612906 -4.1787038 -4.1868157 -4.1991553 -4.2133818 -4.2312675 -4.2433529 -4.24572 -4.2526908 -4.2657743 -4.2741222 -4.2752481 -4.2726502 -4.2591562 -4.2421722][-4.11119 -4.1308136 -4.148201 -4.17069 -4.1963615 -4.2273583 -4.2487116 -4.252039 -4.2540588 -4.2628789 -4.2701578 -4.2742515 -4.2730389 -4.2556829 -4.2349138]]...]
INFO - root - 2017-12-05 10:56:03.838871: step 810, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 82h:16m:26s remains)
INFO - root - 2017-12-05 10:56:12.934700: step 820, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.915 sec/batch; 84h:20m:10s remains)
INFO - root - 2017-12-05 10:56:22.070838: step 830, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 82h:49m:33s remains)
INFO - root - 2017-12-05 10:56:31.101740: step 840, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 78h:45m:37s remains)
INFO - root - 2017-12-05 10:56:40.139963: step 850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 83h:35m:02s remains)
INFO - root - 2017-12-05 10:56:49.158140: step 860, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 83h:04m:04s remains)
INFO - root - 2017-12-05 10:56:58.308634: step 870, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 83h:26m:34s remains)
INFO - root - 2017-12-05 10:57:07.384980: step 880, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 84h:15m:12s remains)
INFO - root - 2017-12-05 10:57:16.422768: step 890, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 85h:08m:09s remains)
INFO - root - 2017-12-05 10:57:25.458399: step 900, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.936 sec/batch; 86h:15m:24s remains)
2017-12-05 10:57:26.197099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1178365 -4.092659 -4.0760841 -4.0677824 -4.0607438 -4.0806608 -4.1103983 -4.1268754 -4.1342978 -4.1284442 -4.1228366 -4.1232243 -4.0933046 -4.0741 -4.0863085][-4.0937243 -4.0828247 -4.0759282 -4.0682397 -4.0561881 -4.0725775 -4.1019263 -4.1251364 -4.1431155 -4.1436429 -4.1339417 -4.1273751 -4.0934029 -4.0715814 -4.0814185][-4.0511394 -4.0701318 -4.0863228 -4.0801549 -4.0648456 -4.0785928 -4.1048546 -4.1239452 -4.1452584 -4.1507392 -4.139369 -4.1272573 -4.0931044 -4.0706143 -4.0806665][-3.9969854 -4.0599494 -4.1042032 -4.1060667 -4.0884542 -4.0941668 -4.1085854 -4.1173654 -4.1335716 -4.1466765 -4.1464853 -4.1385455 -4.1094894 -4.0909271 -4.0981607][-4.0114465 -4.0943322 -4.1485806 -4.1507897 -4.1205587 -4.1020026 -4.0909386 -4.0846038 -4.1021528 -4.1252804 -4.1405725 -4.146596 -4.1220584 -4.1063995 -4.1170917][-4.0868788 -4.1439171 -4.1810217 -4.16911 -4.1201935 -4.0775156 -4.041801 -4.0233126 -4.0522504 -4.0893316 -4.1195712 -4.1387315 -4.1189666 -4.1038704 -4.1114364][-4.1511822 -4.168448 -4.1741524 -4.1402802 -4.0695505 -4.0094504 -3.9551523 -3.9359016 -3.9947126 -4.052691 -4.0864077 -4.1072521 -4.086813 -4.0723472 -4.0783978][-4.1839848 -4.1699929 -4.1430092 -4.087688 -3.9988959 -3.921437 -3.8515506 -3.8479264 -3.955786 -4.0376992 -4.072937 -4.0842791 -4.0557547 -4.0380449 -4.0430336][-4.18842 -4.152565 -4.1022153 -4.0416951 -3.9621294 -3.8992541 -3.8461535 -3.864454 -3.9893939 -4.0709782 -4.0949268 -4.0937939 -4.05594 -4.0248456 -4.0227437][-4.178669 -4.1419406 -4.0949945 -4.0510721 -4.0036206 -3.980644 -3.9672689 -3.9933467 -4.0780954 -4.1268139 -4.129755 -4.1214385 -4.0887308 -4.0582514 -4.0542965][-4.17479 -4.1564851 -4.1318808 -4.1050391 -4.0817947 -4.08121 -4.08652 -4.1069641 -4.1495147 -4.1672082 -4.153286 -4.1411695 -4.1174974 -4.0974145 -4.0956798][-4.1822305 -4.1774611 -4.1698742 -4.1569853 -4.1467776 -4.1521931 -4.1606007 -4.171813 -4.1862788 -4.1862855 -4.1661253 -4.1539812 -4.1345143 -4.1167 -4.1167793][-4.1911678 -4.1881795 -4.1868963 -4.1844487 -4.1818223 -4.18912 -4.1984258 -4.2067366 -4.21015 -4.1991839 -4.1779904 -4.1673632 -4.1505761 -4.1265221 -4.1204205][-4.2022443 -4.19958 -4.1998878 -4.2030706 -4.206552 -4.2134304 -4.2207117 -4.2257152 -4.223177 -4.2102928 -4.1947074 -4.1914468 -4.1777415 -4.1477365 -4.1347694][-4.2207928 -4.2146454 -4.2109375 -4.2134371 -4.2183976 -4.2244663 -4.2315769 -4.2362404 -4.2298646 -4.2185059 -4.2086186 -4.2080154 -4.1968584 -4.1704569 -4.1586146]]...]
INFO - root - 2017-12-05 10:57:35.303810: step 910, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 85h:20m:32s remains)
INFO - root - 2017-12-05 10:57:44.363383: step 920, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 82h:50m:09s remains)
INFO - root - 2017-12-05 10:57:53.423436: step 930, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 82h:24m:58s remains)
INFO - root - 2017-12-05 10:58:02.527191: step 940, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 84h:57m:42s remains)
INFO - root - 2017-12-05 10:58:11.487545: step 950, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 84h:21m:50s remains)
INFO - root - 2017-12-05 10:58:20.574441: step 960, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 83h:09m:41s remains)
INFO - root - 2017-12-05 10:58:29.683605: step 970, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 84h:47m:18s remains)
INFO - root - 2017-12-05 10:58:38.779374: step 980, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 82h:06m:00s remains)
INFO - root - 2017-12-05 10:58:47.763002: step 990, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 81h:20m:06s remains)
INFO - root - 2017-12-05 10:58:56.679686: step 1000, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.920 sec/batch; 84h:45m:27s remains)
2017-12-05 10:58:57.438456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1695433 -4.1713753 -4.1713285 -4.1784244 -4.1891518 -4.190784 -4.1715531 -4.1506772 -4.1488748 -4.1602712 -4.1828332 -4.2041979 -4.2338877 -4.2588177 -4.28826][-4.144084 -4.1466932 -4.1469331 -4.1582594 -4.176034 -4.1812239 -4.1576972 -4.1293168 -4.125463 -4.140378 -4.170723 -4.2013106 -4.23735 -4.2656589 -4.295948][-4.1270156 -4.1328 -4.1382523 -4.1542263 -4.1755109 -4.1826029 -4.1574631 -4.1253781 -4.1198416 -4.1367307 -4.1696463 -4.2052226 -4.2453189 -4.2755127 -4.3044262][-4.1231079 -4.1353936 -4.1496005 -4.1694188 -4.1877518 -4.1909266 -4.1629696 -4.1254296 -4.1188369 -4.1383719 -4.1722469 -4.2058139 -4.2440438 -4.2758541 -4.3062515][-4.1321464 -4.1519837 -4.1720095 -4.1887841 -4.198132 -4.1914382 -4.15628 -4.1156187 -4.112174 -4.1383123 -4.1741 -4.2046714 -4.2404118 -4.2723122 -4.3031521][-4.152678 -4.173564 -4.1888232 -4.1951036 -4.1893554 -4.1667089 -4.1216946 -4.0791588 -4.0850143 -4.12733 -4.1723366 -4.2040095 -4.2372022 -4.2694225 -4.301116][-4.1825223 -4.1964078 -4.1976795 -4.1882811 -4.1650681 -4.1261163 -4.0706549 -4.0222416 -4.0394 -4.1058173 -4.1668377 -4.2050476 -4.2394428 -4.2725573 -4.3041863][-4.1983762 -4.2039 -4.1938787 -4.1744466 -4.1415234 -4.0917563 -4.0229087 -3.9634359 -3.9900074 -4.0807471 -4.159791 -4.209918 -4.246489 -4.2781191 -4.3081837][-4.2055159 -4.2024512 -4.185503 -4.1629977 -4.1295733 -4.0794783 -4.0068626 -3.9427557 -3.9754984 -4.0776591 -4.1646132 -4.2196 -4.2546835 -4.282855 -4.3107538][-4.205864 -4.1989951 -4.1800184 -4.1598349 -4.1355619 -4.10037 -4.045855 -3.9961135 -4.0241776 -4.107748 -4.1814909 -4.2288237 -4.2582803 -4.2843914 -4.3116169][-4.2008405 -4.192369 -4.1738338 -4.1581178 -4.1459026 -4.1297259 -4.0978436 -4.0637317 -4.0793886 -4.1346693 -4.189456 -4.226778 -4.2529287 -4.2801566 -4.3101344][-4.1936183 -4.18503 -4.1684871 -4.1574621 -4.1539392 -4.1517081 -4.1367083 -4.1124444 -4.1187439 -4.1548138 -4.1953735 -4.2258472 -4.2508359 -4.2801166 -4.3116865][-4.1906152 -4.1839418 -4.1705995 -4.1644788 -4.1667833 -4.1710663 -4.1649513 -4.1470637 -4.1491985 -4.1750321 -4.2060914 -4.2331071 -4.2589993 -4.2889571 -4.3188691][-4.1979737 -4.1947455 -4.1853752 -4.1831245 -4.1877656 -4.1936297 -4.18992 -4.1755548 -4.1773858 -4.1992955 -4.2257781 -4.2509708 -4.2770205 -4.3046141 -4.3293262][-4.2189379 -4.2196336 -4.2155695 -4.2159934 -4.2204618 -4.2244606 -4.218626 -4.2055507 -4.2083235 -4.2285872 -4.2530985 -4.2765927 -4.2996902 -4.3217688 -4.3401523]]...]
INFO - root - 2017-12-05 10:59:06.529906: step 1010, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 81h:38m:21s remains)
INFO - root - 2017-12-05 10:59:15.614211: step 1020, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 86h:59m:30s remains)
INFO - root - 2017-12-05 10:59:24.727176: step 1030, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 86h:05m:21s remains)
INFO - root - 2017-12-05 10:59:33.734989: step 1040, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 82h:38m:16s remains)
INFO - root - 2017-12-05 10:59:42.768370: step 1050, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 79h:45m:45s remains)
INFO - root - 2017-12-05 10:59:51.906036: step 1060, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 85h:07m:19s remains)
INFO - root - 2017-12-05 11:00:00.897876: step 1070, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 81h:47m:44s remains)
INFO - root - 2017-12-05 11:00:09.769737: step 1080, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 84h:17m:45s remains)
INFO - root - 2017-12-05 11:00:18.907874: step 1090, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 86h:05m:17s remains)
INFO - root - 2017-12-05 11:00:27.973225: step 1100, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 85h:48m:47s remains)
2017-12-05 11:00:28.776786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0837708 -4.041172 -4.0325 -4.0532146 -4.0816011 -4.1188917 -4.1745372 -4.225903 -4.2571073 -4.2664909 -4.275414 -4.290431 -4.3032784 -4.2985682 -4.2872825][-4.1063828 -4.0583425 -4.0204725 -4.0028133 -4.003839 -4.0412021 -4.1160431 -4.191154 -4.2371855 -4.2503362 -4.2593603 -4.2796974 -4.2999234 -4.3045158 -4.2968884][-4.1287942 -4.0733113 -4.014565 -3.9647388 -3.9419014 -3.9828315 -4.0658522 -4.1541491 -4.2122216 -4.2363596 -4.2493639 -4.2713356 -4.29493 -4.3051629 -4.3019876][-4.150001 -4.0981388 -4.0396862 -3.983954 -3.9503243 -3.9794717 -4.0492444 -4.13542 -4.1991234 -4.2313266 -4.2519116 -4.2748251 -4.2946043 -4.3041558 -4.3042936][-4.149497 -4.1098056 -4.0636764 -4.0197825 -3.9869688 -4.0001049 -4.0473137 -4.1184497 -4.1798482 -4.2190089 -4.248764 -4.2759895 -4.2937903 -4.3026128 -4.3061466][-4.1191297 -4.0871353 -4.0499768 -4.0191669 -3.99148 -3.9854712 -4.0065241 -4.0605 -4.1231661 -4.1751637 -4.2181377 -4.2565074 -4.2822561 -4.2974024 -4.3057933][-4.0993109 -4.0685282 -4.0351849 -4.0116444 -3.9827366 -3.9568024 -3.9501395 -3.9878459 -4.0502458 -4.112442 -4.1714282 -4.2243371 -4.2652221 -4.2903576 -4.3052068][-4.1234708 -4.0904965 -4.0575171 -4.0342164 -4.0040331 -3.9719844 -3.9501395 -3.9714625 -4.0247111 -4.0839214 -4.1467648 -4.2060246 -4.2568526 -4.2907948 -4.3099842][-4.1714139 -4.1382279 -4.1039977 -4.0744228 -4.0433035 -4.0146723 -3.9947906 -4.0058279 -4.0455036 -4.0964489 -4.1549292 -4.2105203 -4.2614994 -4.2995558 -4.3199558][-4.1965227 -4.1691756 -4.1375313 -4.1008 -4.0639977 -4.0375452 -4.0239782 -4.0299959 -4.059947 -4.1071677 -4.1660371 -4.2193041 -4.2681832 -4.3069172 -4.3275509][-4.1927752 -4.1766953 -4.1570754 -4.1245461 -4.0861416 -4.0574384 -4.044095 -4.0428839 -4.0591874 -4.102891 -4.1619697 -4.2156482 -4.2676744 -4.3072462 -4.3288217][-4.1970553 -4.1929374 -4.1871715 -4.1666236 -4.1343775 -4.1077766 -4.0901461 -4.07588 -4.0757194 -4.1092052 -4.1613379 -4.2137256 -4.2665315 -4.3054061 -4.3264613][-4.2190909 -4.2182069 -4.21815 -4.2088723 -4.1888018 -4.1699896 -4.1523657 -4.1293149 -4.114913 -4.1339293 -4.1732645 -4.220139 -4.2673411 -4.3012919 -4.3198056][-4.2376728 -4.23377 -4.23441 -4.2323732 -4.2248812 -4.2155237 -4.1998982 -4.1720982 -4.147264 -4.1523237 -4.1784782 -4.2181563 -4.2586217 -4.2874417 -4.3056393][-4.2439752 -4.2349777 -4.2314677 -4.2317581 -4.2313256 -4.2289233 -4.2171073 -4.189517 -4.1621504 -4.156508 -4.1725154 -4.2058392 -4.2405 -4.2663331 -4.28604]]...]
INFO - root - 2017-12-05 11:00:37.964317: step 1110, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 83h:08m:46s remains)
INFO - root - 2017-12-05 11:00:46.910968: step 1120, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 81h:06m:05s remains)
INFO - root - 2017-12-05 11:00:55.773395: step 1130, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.790 sec/batch; 72h:42m:47s remains)
INFO - root - 2017-12-05 11:01:04.755939: step 1140, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 83h:35m:07s remains)
INFO - root - 2017-12-05 11:01:13.765068: step 1150, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 82h:09m:32s remains)
INFO - root - 2017-12-05 11:01:22.718620: step 1160, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 83h:50m:26s remains)
INFO - root - 2017-12-05 11:01:31.781904: step 1170, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:51m:17s remains)
INFO - root - 2017-12-05 11:01:40.838314: step 1180, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 85h:11m:46s remains)
INFO - root - 2017-12-05 11:01:50.079666: step 1190, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 85h:13m:01s remains)
INFO - root - 2017-12-05 11:01:59.167109: step 1200, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 87h:02m:54s remains)
2017-12-05 11:01:59.894434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2223115 -4.2204447 -4.2269449 -4.2363276 -4.2470956 -4.2525516 -4.2534895 -4.25937 -4.2610269 -4.2580314 -4.2623768 -4.2675905 -4.2807093 -4.2886572 -4.2945824][-4.1988053 -4.2022271 -4.2133603 -4.2249923 -4.2366104 -4.2441483 -4.2470059 -4.2578826 -4.2632871 -4.2589588 -4.2596216 -4.258009 -4.2684779 -4.2749906 -4.2778144][-4.1680374 -4.1670322 -4.1823897 -4.2030425 -4.2206492 -4.2303371 -4.2339039 -4.2466912 -4.2575707 -4.2563491 -4.25338 -4.2498093 -4.25846 -4.2651119 -4.2646055][-4.1469665 -4.1443868 -4.1691966 -4.1967187 -4.2123442 -4.219429 -4.2219667 -4.23105 -4.24078 -4.2468739 -4.2496643 -4.2518458 -4.2600312 -4.2649531 -4.262754][-4.1550789 -4.150579 -4.174984 -4.203362 -4.2103276 -4.2049875 -4.1988149 -4.2053208 -4.2164888 -4.2328124 -4.247488 -4.2577214 -4.2668648 -4.2708726 -4.2726254][-4.1691985 -4.1669655 -4.1811395 -4.1978912 -4.1914692 -4.1686339 -4.1420021 -4.143065 -4.1645761 -4.1941533 -4.2213354 -4.2414937 -4.2571349 -4.2689753 -4.2766237][-4.1774497 -4.1814075 -4.1868911 -4.1813579 -4.1530857 -4.0991364 -4.0340114 -4.0178957 -4.0677772 -4.1289492 -4.1759434 -4.2083397 -4.2371244 -4.26032 -4.2715764][-4.1831374 -4.1911583 -4.1896582 -4.1612253 -4.10585 -4.0175037 -3.9085202 -3.8731024 -3.9719565 -4.0751152 -4.1420379 -4.1806374 -4.2138348 -4.2442837 -4.2582583][-4.1825829 -4.1846046 -4.1765318 -4.1425877 -4.0810709 -3.9945154 -3.8922887 -3.8626814 -3.9707751 -4.0760727 -4.1406484 -4.1747627 -4.2001429 -4.22783 -4.2438412][-4.1786342 -4.1747594 -4.1650028 -4.1409388 -4.09914 -4.0500827 -4.0040064 -4.0010824 -4.064713 -4.1298981 -4.1754889 -4.1978946 -4.2101765 -4.2269354 -4.2418771][-4.1846046 -4.183856 -4.1766658 -4.1637778 -4.1464343 -4.1341863 -4.1299353 -4.1446033 -4.1709828 -4.2006807 -4.2259145 -4.2334933 -4.2319412 -4.2379427 -4.2505107][-4.2088013 -4.2111745 -4.2097182 -4.2041912 -4.201889 -4.2108765 -4.2254124 -4.2438917 -4.2520332 -4.2590523 -4.2662969 -4.2612743 -4.2525129 -4.2529778 -4.2618136][-4.2454839 -4.2460165 -4.2441387 -4.2393522 -4.2435627 -4.2581038 -4.274693 -4.2871766 -4.2827988 -4.2754083 -4.2713203 -4.2643266 -4.2560654 -4.2551813 -4.2598267][-4.2664413 -4.2631073 -4.2597718 -4.2561612 -4.2580061 -4.2645187 -4.274231 -4.2797418 -4.2696609 -4.2554708 -4.2486234 -4.2450142 -4.2430158 -4.2450948 -4.2466345][-4.2775149 -4.2705789 -4.2675319 -4.2658215 -4.2644439 -4.2653737 -4.2688732 -4.2699728 -4.2586527 -4.2437668 -4.2360234 -4.2336464 -4.2341194 -4.2400913 -4.2441182]]...]
INFO - root - 2017-12-05 11:02:09.048457: step 1210, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 83h:20m:22s remains)
INFO - root - 2017-12-05 11:02:18.092438: step 1220, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 84h:09m:18s remains)
INFO - root - 2017-12-05 11:02:27.112404: step 1230, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 84h:29m:55s remains)
INFO - root - 2017-12-05 11:02:36.215327: step 1240, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 83h:28m:31s remains)
INFO - root - 2017-12-05 11:02:45.259770: step 1250, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 84h:43m:28s remains)
INFO - root - 2017-12-05 11:02:54.383030: step 1260, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 82h:52m:31s remains)
INFO - root - 2017-12-05 11:03:03.488506: step 1270, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 84h:26m:02s remains)
INFO - root - 2017-12-05 11:03:12.723364: step 1280, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 83h:13m:08s remains)
INFO - root - 2017-12-05 11:03:21.836588: step 1290, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 83h:18m:17s remains)
INFO - root - 2017-12-05 11:03:30.771340: step 1300, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 84h:33m:46s remains)
2017-12-05 11:03:31.646467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2334895 -4.2578921 -4.2549944 -4.2327685 -4.2062902 -4.1872239 -4.1889715 -4.2106185 -4.2367926 -4.2467422 -4.2434382 -4.2367158 -4.2316642 -4.2284875 -4.2290463][-4.2441192 -4.2587132 -4.2486634 -4.2185025 -4.1795516 -4.1487141 -4.1462741 -4.1717992 -4.2064567 -4.2271142 -4.23429 -4.234221 -4.2297816 -4.2204571 -4.2155094][-4.2284508 -4.2317619 -4.2167253 -4.1831508 -4.1372232 -4.0996976 -4.0966997 -4.127492 -4.1681333 -4.198245 -4.2145681 -4.218987 -4.2131357 -4.1988544 -4.1875267][-4.1903811 -4.18325 -4.1663117 -4.1344309 -4.0921326 -4.0569487 -4.0560279 -4.0897417 -4.1325345 -4.1679649 -4.1893864 -4.1952553 -4.185926 -4.17031 -4.1561451][-4.1477532 -4.1391468 -4.1271873 -4.102766 -4.0669336 -4.0293465 -4.0175624 -4.0483742 -4.0960255 -4.1362362 -4.1642804 -4.1756706 -4.1677074 -4.1564589 -4.1443319][-4.1265612 -4.1206193 -4.1122637 -4.0915952 -4.052577 -3.999372 -3.9613743 -3.9859912 -4.0499649 -4.1051989 -4.1503024 -4.1744757 -4.1723976 -4.1689806 -4.1586628][-4.1258488 -4.118516 -4.1078062 -4.0876632 -4.04123 -3.9733882 -3.9125817 -3.9333787 -4.0155687 -4.08917 -4.15064 -4.1880732 -4.1930552 -4.1950526 -4.1867929][-4.1257105 -4.117959 -4.1059046 -4.0901055 -4.0491285 -3.9935038 -3.9446433 -3.9629557 -4.0343413 -4.1037307 -4.1648812 -4.2061796 -4.2150292 -4.2209544 -4.2151022][-4.110404 -4.1025896 -4.095192 -4.0959177 -4.07541 -4.0498781 -4.0312552 -4.0420589 -4.0834389 -4.1305137 -4.1771116 -4.2103319 -4.2213593 -4.2305307 -4.223938][-4.1006012 -4.0965347 -4.1003094 -4.1191497 -4.114697 -4.1051431 -4.099803 -4.0997314 -4.1141338 -4.1408653 -4.1731048 -4.1962709 -4.2076349 -4.2153616 -4.2061849][-4.118969 -4.1257038 -4.1387515 -4.1606569 -4.1567311 -4.1434865 -4.1358533 -4.1261687 -4.1260343 -4.1423745 -4.1640258 -4.1788769 -4.1883335 -4.1900134 -4.1743941][-4.1514149 -4.1624708 -4.1756239 -4.1925836 -4.188787 -4.1726236 -4.16206 -4.1485996 -4.1416821 -4.1459856 -4.1569042 -4.1664925 -4.1769042 -4.1769528 -4.1576486][-4.1690865 -4.1814537 -4.1952167 -4.2110338 -4.2104154 -4.1986752 -4.1883659 -4.1706033 -4.1555829 -4.1451149 -4.1416607 -4.1455765 -4.1604533 -4.1673436 -4.1590137][-4.1803455 -4.1927505 -4.2105484 -4.2290049 -4.2318277 -4.2232618 -4.2105141 -4.186461 -4.1622095 -4.1407614 -4.12812 -4.1265564 -4.1442528 -4.1610456 -4.1692781][-4.1916103 -4.2010784 -4.221334 -4.2428021 -4.249743 -4.2443161 -4.2325139 -4.2042837 -4.1751909 -4.1531191 -4.1386752 -4.1348858 -4.1518373 -4.1717029 -4.1840358]]...]
INFO - root - 2017-12-05 11:03:40.737322: step 1310, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 83h:42m:12s remains)
INFO - root - 2017-12-05 11:03:49.752498: step 1320, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 84h:09m:12s remains)
INFO - root - 2017-12-05 11:03:58.655668: step 1330, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 81h:40m:34s remains)
INFO - root - 2017-12-05 11:04:07.673955: step 1340, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 80h:37m:09s remains)
INFO - root - 2017-12-05 11:04:16.710003: step 1350, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 85h:36m:56s remains)
INFO - root - 2017-12-05 11:04:25.753413: step 1360, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 83h:09m:44s remains)
INFO - root - 2017-12-05 11:04:34.819564: step 1370, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 84h:30m:20s remains)
INFO - root - 2017-12-05 11:04:44.027699: step 1380, loss = 2.03, batch loss = 1.98 (8.4 examples/sec; 0.958 sec/batch; 88h:04m:58s remains)
INFO - root - 2017-12-05 11:04:53.285174: step 1390, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.930 sec/batch; 85h:29m:37s remains)
INFO - root - 2017-12-05 11:05:02.430263: step 1400, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 77h:34m:19s remains)
2017-12-05 11:05:03.175404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1062379 -4.131711 -4.1313682 -4.120245 -4.1236033 -4.1450543 -4.1503677 -4.1412892 -4.1336017 -4.1458354 -4.147397 -4.1319408 -4.1099472 -4.0971766 -4.096426][-4.092371 -4.11501 -4.1150918 -4.1101685 -4.1177835 -4.1376567 -4.1392813 -4.1288824 -4.1323233 -4.1583447 -4.1645112 -4.1541872 -4.1283178 -4.1082463 -4.0994482][-4.0871091 -4.1112757 -4.113606 -4.1113405 -4.1211362 -4.128262 -4.1179175 -4.1069288 -4.128304 -4.1665225 -4.1774073 -4.1764026 -4.1607275 -4.1381426 -4.1167588][-4.0995846 -4.1253276 -4.1175041 -4.1111679 -4.1179433 -4.1086216 -4.0797448 -4.0760641 -4.1202984 -4.1696773 -4.1886792 -4.2006173 -4.2011981 -4.1800976 -4.145782][-4.1097836 -4.1354542 -4.1181693 -4.1076646 -4.1048956 -4.0715504 -4.0183496 -4.0315905 -4.1064029 -4.1688762 -4.1994486 -4.2254086 -4.2359757 -4.2153687 -4.1727333][-4.1049132 -4.132513 -4.1171379 -4.1033421 -4.0842071 -4.0247335 -3.9442894 -3.9766278 -4.0851674 -4.16127 -4.2031198 -4.2388926 -4.2564721 -4.2365336 -4.191195][-4.107316 -4.1376843 -4.1266522 -4.100872 -4.0614214 -3.9766688 -3.8727398 -3.9171872 -4.0526891 -4.1432371 -4.1965489 -4.2401891 -4.2616873 -4.2417383 -4.1924691][-4.1237903 -4.1578031 -4.1465683 -4.1062031 -4.049737 -3.9561925 -3.8466897 -3.8850648 -4.02203 -4.119565 -4.182961 -4.2342391 -4.2580137 -4.2340684 -4.1759963][-4.1506758 -4.1787653 -4.1658049 -4.1182733 -4.0612 -3.9804497 -3.8855774 -3.9050672 -4.0165391 -4.1092248 -4.1770682 -4.2320509 -4.2530689 -4.2229867 -4.1544][-4.1807709 -4.2009668 -4.1889186 -4.1467667 -4.1034784 -4.0459809 -3.9705334 -3.9740524 -4.0569186 -4.1349645 -4.1937327 -4.2387581 -4.2492728 -4.2170148 -4.1475639][-4.2009163 -4.2163472 -4.2070251 -4.1769109 -4.1515312 -4.112287 -4.0514517 -4.0466213 -4.1058655 -4.1727581 -4.2196951 -4.248157 -4.2502708 -4.2204604 -4.1562562][-4.2193151 -4.2316289 -4.2231569 -4.1991768 -4.1833067 -4.1594987 -4.1151443 -4.1059022 -4.1484833 -4.2054334 -4.2434282 -4.2607183 -4.2600412 -4.234026 -4.1771879][-4.2456632 -4.2519512 -4.2416782 -4.2183418 -4.2057981 -4.1921468 -4.1646209 -4.1585288 -4.1912107 -4.2388935 -4.2676139 -4.275454 -4.2731323 -4.2516065 -4.204453][-4.2704625 -4.269423 -4.2575312 -4.23528 -4.223762 -4.2170191 -4.203877 -4.2059407 -4.2343435 -4.2721815 -4.2911849 -4.2893949 -4.2842288 -4.2650681 -4.2290883][-4.2859631 -4.2808933 -4.2704067 -4.2531605 -4.2444105 -4.2407885 -4.2370319 -4.2456284 -4.2693443 -4.2957125 -4.3051772 -4.2976227 -4.2907791 -4.2743883 -4.2509317]]...]
INFO - root - 2017-12-05 11:05:12.243023: step 1410, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 76h:42m:02s remains)
INFO - root - 2017-12-05 11:05:21.219604: step 1420, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 82h:04m:14s remains)
INFO - root - 2017-12-05 11:05:30.445885: step 1430, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 83h:16m:13s remains)
INFO - root - 2017-12-05 11:05:39.596067: step 1440, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 85h:05m:59s remains)
INFO - root - 2017-12-05 11:05:48.634215: step 1450, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 81h:41m:13s remains)
INFO - root - 2017-12-05 11:05:57.763420: step 1460, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 85h:56m:31s remains)
INFO - root - 2017-12-05 11:06:06.843568: step 1470, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 83h:43m:01s remains)
INFO - root - 2017-12-05 11:06:15.869947: step 1480, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 82h:42m:06s remains)
INFO - root - 2017-12-05 11:06:24.968803: step 1490, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 85h:30m:16s remains)
INFO - root - 2017-12-05 11:06:33.966964: step 1500, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.940 sec/batch; 86h:25m:11s remains)
2017-12-05 11:06:34.772927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.288084 -4.2833643 -4.2779894 -4.2739029 -4.2732177 -4.2728658 -4.2635212 -4.2524252 -4.2478333 -4.2481008 -4.2514124 -4.2483606 -4.2232733 -4.1886263 -4.1739116][-4.2769313 -4.2643538 -4.2533684 -4.2503133 -4.2559443 -4.2629848 -4.258709 -4.2451525 -4.2343388 -4.2346096 -4.2446475 -4.2440996 -4.2148476 -4.171649 -4.152545][-4.2619014 -4.2456794 -4.2353058 -4.2356405 -4.2441015 -4.2560663 -4.2513046 -4.2320442 -4.21865 -4.2257495 -4.2428246 -4.2391572 -4.2000637 -4.1510096 -4.132813][-4.255712 -4.2416544 -4.2314072 -4.2306023 -4.2389088 -4.2515554 -4.2444835 -4.2253237 -4.21874 -4.2332487 -4.2485285 -4.2320924 -4.1832752 -4.1395493 -4.1330347][-4.2503467 -4.2375569 -4.2248397 -4.2231789 -4.2307825 -4.2359662 -4.2252789 -4.2145982 -4.2208176 -4.2381563 -4.2430491 -4.2170897 -4.1730523 -4.1468663 -4.1542659][-4.2452855 -4.2309275 -4.2146788 -4.2081494 -4.2057834 -4.1923723 -4.1787505 -4.1874337 -4.2100549 -4.2273369 -4.2196889 -4.1966414 -4.1709642 -4.1647396 -4.1812878][-4.2458663 -4.2302928 -4.2107606 -4.1935749 -4.1683941 -4.1352029 -4.1269016 -4.1598225 -4.1962862 -4.2118869 -4.1999469 -4.1841092 -4.1750941 -4.1830778 -4.2001495][-4.255023 -4.2409124 -4.221242 -4.1949205 -4.1524696 -4.1144195 -4.1204071 -4.1592226 -4.1887631 -4.1957154 -4.188797 -4.1849813 -4.1877532 -4.1991096 -4.2103677][-4.261179 -4.2572827 -4.24663 -4.21985 -4.1760693 -4.1455331 -4.1523924 -4.1692333 -4.178072 -4.1809659 -4.1858044 -4.1929421 -4.1987491 -4.2089777 -4.2145839][-4.2634959 -4.2717342 -4.2702723 -4.2447624 -4.203198 -4.1731281 -4.1666455 -4.1606622 -4.1610551 -4.1704888 -4.1844363 -4.1955023 -4.198751 -4.2056212 -4.2089543][-4.2524071 -4.2655091 -4.2651491 -4.2386746 -4.2003098 -4.1688795 -4.1495128 -4.1354876 -4.1424232 -4.1627326 -4.1821404 -4.1913071 -4.1903734 -4.1923356 -4.1939669][-4.2297335 -4.2413673 -4.2420239 -4.2220969 -4.194241 -4.1644154 -4.1381598 -4.1245823 -4.1378708 -4.16244 -4.1813664 -4.1854682 -4.1820717 -4.1832495 -4.187252][-4.2096949 -4.2183475 -4.2187371 -4.206656 -4.1896138 -4.1658978 -4.1419172 -4.1309209 -4.1430111 -4.1679068 -4.187851 -4.1883821 -4.1824107 -4.18677 -4.1968751][-4.1982436 -4.2056026 -4.2061191 -4.1997166 -4.1907129 -4.1740437 -4.15565 -4.1458473 -4.1533804 -4.1758389 -4.1938353 -4.1947827 -4.1913276 -4.1994786 -4.2120028][-4.196969 -4.2030292 -4.2008061 -4.1972618 -4.1960168 -4.1860118 -4.1736035 -4.1641631 -4.1634932 -4.1753654 -4.187283 -4.1900783 -4.1923494 -4.201581 -4.2116008]]...]
INFO - root - 2017-12-05 11:06:43.699739: step 1510, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 80h:54m:20s remains)
INFO - root - 2017-12-05 11:06:52.731298: step 1520, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 80h:49m:00s remains)
INFO - root - 2017-12-05 11:07:02.023324: step 1530, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 85h:18m:30s remains)
INFO - root - 2017-12-05 11:07:11.219173: step 1540, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 83h:17m:02s remains)
INFO - root - 2017-12-05 11:07:20.225918: step 1550, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 82h:16m:46s remains)
INFO - root - 2017-12-05 11:07:29.257486: step 1560, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 88h:28m:32s remains)
INFO - root - 2017-12-05 11:07:38.536426: step 1570, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 84h:17m:20s remains)
INFO - root - 2017-12-05 11:07:47.525310: step 1580, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 80h:26m:11s remains)
INFO - root - 2017-12-05 11:07:56.511193: step 1590, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 0.791 sec/batch; 72h:40m:08s remains)
INFO - root - 2017-12-05 11:08:05.662058: step 1600, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 87h:58m:06s remains)
2017-12-05 11:08:06.393747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1690936 -4.1667943 -4.1606884 -4.1540842 -4.147862 -4.1416383 -4.1369524 -4.132998 -4.1391678 -4.1503453 -4.15889 -4.154964 -4.1490574 -4.1427183 -4.1335144][-4.155901 -4.1534333 -4.1464806 -4.13678 -4.1258874 -4.1154742 -4.104146 -4.0991426 -4.1093183 -4.1254182 -4.137363 -4.1391068 -4.1357741 -4.1271262 -4.111455][-4.1378288 -4.1401019 -4.1350622 -4.1257772 -4.1134768 -4.1024365 -4.0905633 -4.0850258 -4.0937266 -4.1076703 -4.1175361 -4.1257153 -4.1285062 -4.1215868 -4.1080132][-4.1000524 -4.1105671 -4.1119323 -4.1116872 -4.107636 -4.1012974 -4.0878053 -4.0761051 -4.075892 -4.0805211 -4.0858407 -4.1016722 -4.1170554 -4.1207371 -4.1133585][-4.0629835 -4.0771394 -4.0855217 -4.095964 -4.1040406 -4.1078262 -4.0982189 -4.084424 -4.0724711 -4.0686259 -4.0735445 -4.0999718 -4.1242142 -4.1307216 -4.126852][-4.0529528 -4.0647373 -4.0692105 -4.0752692 -4.0833282 -4.0911789 -4.0889907 -4.0779486 -4.0639524 -4.0614595 -4.0722442 -4.1070743 -4.1310215 -4.1324148 -4.1264243][-4.0546885 -4.065311 -4.0669146 -4.0655003 -4.0663404 -4.06944 -4.064806 -4.0438485 -4.0149837 -4.0129628 -4.042129 -4.0860577 -4.108911 -4.1088533 -4.10498][-4.0841913 -4.0935478 -4.0897484 -4.079649 -4.0693126 -4.0607681 -4.0392008 -3.99197 -3.9287891 -3.9184992 -3.9744656 -4.0395823 -4.071476 -4.0848527 -4.0951624][-4.1215539 -4.1296611 -4.1163731 -4.0982189 -4.0829287 -4.0724726 -4.0501704 -3.9966168 -3.9200711 -3.8998752 -3.9566307 -4.0249438 -4.0655951 -4.0944157 -4.1169996][-4.1479578 -4.1600347 -4.1481013 -4.1291809 -4.1212759 -4.1205564 -4.11465 -4.0846505 -4.0309873 -4.0139904 -4.0450587 -4.088562 -4.11348 -4.1360674 -4.1590557][-4.1782112 -4.1912689 -4.18223 -4.1666365 -4.1620388 -4.1661787 -4.1682639 -4.1529436 -4.1226559 -4.1129527 -4.1291671 -4.1529007 -4.163888 -4.1757455 -4.1922054][-4.2111006 -4.2243805 -4.2211323 -4.2097344 -4.2030115 -4.2014484 -4.1980276 -4.1857214 -4.17123 -4.17151 -4.1878686 -4.2051883 -4.2096109 -4.2133327 -4.2234364][-4.2381697 -4.2501664 -4.2511344 -4.24532 -4.2390165 -4.2321148 -4.222652 -4.2122436 -4.2056994 -4.21185 -4.2276354 -4.2458086 -4.2533941 -4.2571921 -4.2624397][-4.2609959 -4.269001 -4.2722359 -4.2727656 -4.2691259 -4.2622142 -4.2540679 -4.24686 -4.2423129 -4.2465081 -4.2583194 -4.272666 -4.2814031 -4.2851276 -4.2868323][-4.2715445 -4.2739806 -4.2761297 -4.27927 -4.2808075 -4.27935 -4.2783208 -4.277204 -4.2758355 -4.2754364 -4.2784758 -4.2817483 -4.284122 -4.2851667 -4.2854686]]...]
INFO - root - 2017-12-05 11:08:15.430988: step 1610, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:44m:57s remains)
INFO - root - 2017-12-05 11:08:24.699905: step 1620, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 84h:14m:12s remains)
INFO - root - 2017-12-05 11:08:33.916175: step 1630, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 81h:22m:39s remains)
INFO - root - 2017-12-05 11:08:42.956996: step 1640, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 84h:38m:30s remains)
INFO - root - 2017-12-05 11:08:52.158720: step 1650, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 86h:18m:48s remains)
INFO - root - 2017-12-05 11:09:01.437676: step 1660, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 88h:14m:23s remains)
INFO - root - 2017-12-05 11:09:10.623514: step 1670, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 81h:28m:31s remains)
INFO - root - 2017-12-05 11:09:19.796865: step 1680, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 88h:00m:19s remains)
INFO - root - 2017-12-05 11:09:28.886486: step 1690, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 79h:04m:23s remains)
INFO - root - 2017-12-05 11:09:38.087836: step 1700, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 82h:30m:02s remains)
2017-12-05 11:09:38.849977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.11171 -4.066287 -4.0838609 -4.1458158 -4.2127018 -4.2636156 -4.2848158 -4.2854824 -4.2832351 -4.2851624 -4.2898107 -4.2920132 -4.2798381 -4.2537756 -4.2308311][-4.0673165 -4.0138531 -4.0445547 -4.122448 -4.2001419 -4.2494078 -4.267766 -4.2669258 -4.2683349 -4.2730794 -4.2814741 -4.2878838 -4.2795863 -4.25088 -4.2256274][-4.0226827 -3.9678633 -4.0138326 -4.1062942 -4.1918159 -4.2341776 -4.2417359 -4.2350249 -4.2364368 -4.250916 -4.2700977 -4.2865615 -4.2828879 -4.2529211 -4.2247109][-4.0335727 -3.9950318 -4.0468516 -4.1353641 -4.2094007 -4.229948 -4.2107491 -4.1871552 -4.1901464 -4.2245226 -4.2595015 -4.28574 -4.2837448 -4.2533073 -4.2243414][-4.097415 -4.0902596 -4.1369209 -4.1990204 -4.23962 -4.2220616 -4.1631265 -4.115016 -4.1288948 -4.1883211 -4.2410078 -4.2736034 -4.2697816 -4.2406225 -4.2157955][-4.1675062 -4.1860604 -4.2160335 -4.2457628 -4.2417426 -4.1751652 -4.062912 -3.9920652 -4.0337467 -4.1297116 -4.2035689 -4.246027 -4.2432289 -4.2192473 -4.2035813][-4.2130895 -4.2362304 -4.246398 -4.2431641 -4.1953926 -4.0726628 -3.898716 -3.8157883 -3.9092312 -4.0542169 -4.1567841 -4.2130685 -4.2174959 -4.203002 -4.1972394][-4.2496705 -4.265553 -4.255146 -4.2240338 -4.1431608 -3.9816928 -3.7792578 -3.7183948 -3.8596923 -4.02781 -4.13915 -4.198997 -4.2105446 -4.2017813 -4.2015586][-4.275744 -4.2827649 -4.2596779 -4.21665 -4.1354623 -4.0002546 -3.8584993 -3.8476973 -3.9677353 -4.0946574 -4.1795712 -4.2234621 -4.2285562 -4.2125363 -4.2101622][-4.2823868 -4.283618 -4.2599525 -4.2231607 -4.1643109 -4.07931 -4.0098166 -4.0268154 -4.1082749 -4.1875134 -4.2422309 -4.262641 -4.2506814 -4.2240934 -4.2189541][-4.2792854 -4.2799463 -4.2596283 -4.2330217 -4.1952062 -4.148417 -4.1175985 -4.1380491 -4.1950417 -4.2501545 -4.2844806 -4.29032 -4.2720947 -4.2463326 -4.2375836][-4.2716365 -4.2731495 -4.2594151 -4.241838 -4.220119 -4.1968727 -4.1852274 -4.2028785 -4.243835 -4.2839756 -4.3071303 -4.309597 -4.2942586 -4.2721219 -4.2599645][-4.2769451 -4.2763186 -4.2660751 -4.2567058 -4.2473865 -4.2384887 -4.2364335 -4.2530932 -4.2834973 -4.3096275 -4.3222728 -4.3222094 -4.3119078 -4.2946367 -4.2827077][-4.2990561 -4.2944479 -4.2852445 -4.2799067 -4.2761855 -4.2740173 -4.2762122 -4.2886257 -4.3082356 -4.3230076 -4.327601 -4.3264694 -4.3194065 -4.3080368 -4.3013115][-4.3154426 -4.3107219 -4.3048153 -4.3024368 -4.3003297 -4.2988691 -4.3002939 -4.30823 -4.318109 -4.3237119 -4.3245749 -4.3240819 -4.3207893 -4.3158746 -4.3137369]]...]
INFO - root - 2017-12-05 11:09:47.945910: step 1710, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 82h:21m:11s remains)
INFO - root - 2017-12-05 11:09:57.041490: step 1720, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 82h:07m:09s remains)
INFO - root - 2017-12-05 11:10:06.287599: step 1730, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 80h:07m:46s remains)
INFO - root - 2017-12-05 11:10:15.360487: step 1740, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 82h:33m:24s remains)
INFO - root - 2017-12-05 11:10:24.567854: step 1750, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 84h:37m:15s remains)
INFO - root - 2017-12-05 11:10:33.764043: step 1760, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 82h:45m:42s remains)
INFO - root - 2017-12-05 11:10:43.010592: step 1770, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 86h:24m:06s remains)
INFO - root - 2017-12-05 11:10:52.086458: step 1780, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 83h:34m:13s remains)
INFO - root - 2017-12-05 11:11:01.109296: step 1790, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 83h:34m:44s remains)
INFO - root - 2017-12-05 11:11:10.291609: step 1800, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 84h:09m:38s remains)
2017-12-05 11:11:11.021415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1503029 -4.1525965 -4.1570339 -4.1517148 -4.14574 -4.1537237 -4.1742573 -4.1975861 -4.2132273 -4.2192812 -4.2179294 -4.217072 -4.2131038 -4.2003427 -4.1905413][-4.1270032 -4.128468 -4.1361332 -4.1305914 -4.1243267 -4.1373229 -4.164011 -4.1890378 -4.2030406 -4.20531 -4.1983223 -4.1972117 -4.1928215 -4.1784368 -4.1702914][-4.1149287 -4.1078258 -4.1106148 -4.1022325 -4.09461 -4.1146336 -4.1513262 -4.1842027 -4.1997018 -4.1991134 -4.1887488 -4.1886997 -4.1828566 -4.1687765 -4.1638117][-4.1106782 -4.0918083 -4.08981 -4.0797768 -4.072401 -4.0906992 -4.1321993 -4.177319 -4.1992469 -4.1961203 -4.1823187 -4.1836677 -4.1747551 -4.16021 -4.1602259][-4.1307783 -4.1114674 -4.1084013 -4.0892563 -4.0715337 -4.0734329 -4.1117759 -4.1685638 -4.2043681 -4.2004557 -4.1833382 -4.1788273 -4.1630745 -4.1440873 -4.1492152][-4.1601353 -4.1469207 -4.144506 -4.1141543 -4.072547 -4.0473413 -4.0746975 -4.138505 -4.1938114 -4.1959052 -4.1763544 -4.1646523 -4.1351776 -4.1046872 -4.1125126][-4.1769085 -4.1741815 -4.1736488 -4.1364436 -4.0709829 -4.017776 -4.0260186 -4.0874839 -4.1549139 -4.168231 -4.1508403 -4.1318097 -4.0909429 -4.0402479 -4.0436721][-4.1810884 -4.1883674 -4.1849089 -4.1427927 -4.0587187 -3.973686 -3.9528146 -4.0100102 -4.0888686 -4.1237631 -4.11666 -4.0924811 -4.0389729 -3.9660947 -3.9599488][-4.1616869 -4.1798253 -4.1761513 -4.1315956 -4.0399837 -3.9215038 -3.8617477 -3.9089894 -4.0129886 -4.0838771 -4.0924873 -4.0666857 -4.0073833 -3.9206448 -3.9015815][-4.1161718 -4.142272 -4.1452403 -4.1063981 -4.0278282 -3.9071798 -3.8167496 -3.842958 -3.9601324 -4.0551834 -4.0807118 -4.0677252 -4.0214849 -3.9453402 -3.9241154][-4.0735035 -4.1025224 -4.115932 -4.0956068 -4.0454011 -3.9670422 -3.8905423 -3.8941729 -3.9783876 -4.0630322 -4.0917211 -4.0948424 -4.0768466 -4.03026 -4.0177846][-4.0728931 -4.1002827 -4.1184473 -4.1114345 -4.0915866 -4.0710588 -4.0429621 -4.0326123 -4.0632915 -4.1092596 -4.1304626 -4.1400323 -4.1421638 -4.1206079 -4.1142745][-4.1188483 -4.1345277 -4.1443162 -4.1415873 -4.1390948 -4.1508641 -4.1604309 -4.1597304 -4.159987 -4.1697392 -4.1828332 -4.1954141 -4.2068796 -4.1983628 -4.19204][-4.1908226 -4.1895509 -4.1880827 -4.1854196 -4.1904268 -4.2091813 -4.2290368 -4.2388225 -4.2372775 -4.2343507 -4.2424669 -4.2569466 -4.2702909 -4.2679243 -4.2599096][-4.2446237 -4.2406025 -4.2354469 -4.2329583 -4.2393928 -4.2542343 -4.268383 -4.2783837 -4.2833242 -4.285903 -4.2905369 -4.2985291 -4.3074708 -4.306128 -4.2956119]]...]
INFO - root - 2017-12-05 11:11:20.158105: step 1810, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 82h:35m:34s remains)
INFO - root - 2017-12-05 11:11:29.264435: step 1820, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 87h:07m:02s remains)
INFO - root - 2017-12-05 11:11:38.489482: step 1830, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 83h:38m:17s remains)
INFO - root - 2017-12-05 11:11:47.608162: step 1840, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 84h:09m:27s remains)
INFO - root - 2017-12-05 11:11:56.683612: step 1850, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 81h:41m:59s remains)
INFO - root - 2017-12-05 11:12:05.926718: step 1860, loss = 2.01, batch loss = 1.95 (8.8 examples/sec; 0.910 sec/batch; 83h:34m:18s remains)
INFO - root - 2017-12-05 11:12:15.271854: step 1870, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 85h:15m:29s remains)
INFO - root - 2017-12-05 11:12:24.240060: step 1880, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 83h:45m:44s remains)
INFO - root - 2017-12-05 11:12:33.401172: step 1890, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 84h:48m:30s remains)
INFO - root - 2017-12-05 11:12:42.500861: step 1900, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 82h:56m:53s remains)
2017-12-05 11:12:43.262568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1436439 -4.1467924 -4.1603007 -4.1792436 -4.1914763 -4.1905956 -4.1929016 -4.2133384 -4.2283273 -4.2203197 -4.2086034 -4.1905522 -4.1849113 -4.1993685 -4.2141142][-4.099503 -4.0875349 -4.1066103 -4.1368361 -4.152874 -4.1477194 -4.1496019 -4.1756792 -4.1991644 -4.2023139 -4.2089934 -4.2029724 -4.1961837 -4.2009926 -4.2077503][-4.0753484 -4.0412149 -4.0507746 -4.0832462 -4.1030784 -4.09892 -4.1051741 -4.1345687 -4.1630177 -4.1744895 -4.1981754 -4.2063475 -4.2028189 -4.2005215 -4.2005157][-4.0670614 -4.0164256 -4.0059166 -4.0295558 -4.0536723 -4.0590687 -4.0708027 -4.0999718 -4.1287923 -4.1430192 -4.1781783 -4.1968703 -4.2012906 -4.200604 -4.1974859][-4.0563173 -4.0113025 -3.9904971 -3.9999008 -4.023335 -4.0374508 -4.0575504 -4.0866747 -4.11259 -4.123466 -4.1593194 -4.1805272 -4.1907039 -4.1921673 -4.192246][-4.0635333 -4.0320263 -4.0093226 -4.0045586 -4.014513 -4.0258784 -4.04882 -4.0774622 -4.0977044 -4.1002145 -4.1245747 -4.1445503 -4.1610227 -4.1714826 -4.1819277][-4.0796924 -4.0570483 -4.0367627 -4.0228534 -4.016037 -4.0150223 -4.0332985 -4.0596304 -4.0783472 -4.07432 -4.0878186 -4.1066246 -4.1336451 -4.1613441 -4.1798744][-4.0856233 -4.0774775 -4.06315 -4.0456934 -4.0235972 -4.0065918 -4.010057 -4.0321259 -4.0542173 -4.0509725 -4.06 -4.0831881 -4.1201239 -4.1634288 -4.189405][-4.0743561 -4.074945 -4.0655293 -4.0480776 -4.0205774 -3.9959459 -3.9896812 -4.0037003 -4.0304904 -4.0365443 -4.0518284 -4.0813441 -4.120903 -4.1658878 -4.1905832][-4.0625558 -4.0596342 -4.051877 -4.03863 -4.0149765 -3.9898434 -3.977428 -3.9861722 -4.0161214 -4.0361323 -4.0674691 -4.1055408 -4.1410456 -4.1738954 -4.1859374][-4.07864 -4.0712967 -4.0590878 -4.0467134 -4.0295191 -4.0103149 -3.9991324 -4.0003052 -4.0217223 -4.0408549 -4.0770984 -4.1199565 -4.1513948 -4.1709733 -4.1702666][-4.1242371 -4.1169806 -4.1032381 -4.0911632 -4.0795541 -4.0680342 -4.0606279 -4.0575943 -4.0664682 -4.07521 -4.1011829 -4.1366491 -4.1604466 -4.17169 -4.1654105][-4.1841145 -4.1801791 -4.1693921 -4.1599388 -4.1530023 -4.146914 -4.1420231 -4.1374989 -4.1369109 -4.1337423 -4.1447697 -4.1659756 -4.1799388 -4.1860719 -4.1826324][-4.2430925 -4.2403975 -4.2323136 -4.2254391 -4.2210736 -4.217485 -4.2146482 -4.2102222 -4.2058511 -4.1987767 -4.2010093 -4.2105246 -4.2174811 -4.2224331 -4.2226028][-4.2822533 -4.2793736 -4.2743068 -4.270103 -4.2676592 -4.2658339 -4.2640953 -4.260869 -4.2566075 -4.2499413 -4.2487082 -4.2519722 -4.2544045 -4.2586236 -4.2605057]]...]
INFO - root - 2017-12-05 11:12:52.384345: step 1910, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 84h:26m:17s remains)
INFO - root - 2017-12-05 11:13:01.439229: step 1920, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 81h:35m:56s remains)
INFO - root - 2017-12-05 11:13:10.498670: step 1930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 84h:58m:33s remains)
INFO - root - 2017-12-05 11:13:19.590249: step 1940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 85h:14m:12s remains)
INFO - root - 2017-12-05 11:13:28.748664: step 1950, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 83h:09m:51s remains)
INFO - root - 2017-12-05 11:13:37.836386: step 1960, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 84h:36m:51s remains)
INFO - root - 2017-12-05 11:13:47.048211: step 1970, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 82h:44m:20s remains)
INFO - root - 2017-12-05 11:13:56.290987: step 1980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 85h:13m:15s remains)
INFO - root - 2017-12-05 11:14:05.573650: step 1990, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 83h:46m:11s remains)
INFO - root - 2017-12-05 11:14:14.752519: step 2000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 83h:48m:42s remains)
2017-12-05 11:14:15.532171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3358107 -4.3344011 -4.3326836 -4.3327508 -4.3334765 -4.3336382 -4.3338284 -4.3319459 -4.3279562 -4.3244996 -4.3241787 -4.3277788 -4.3353295 -4.3430643 -4.3464413][-4.3297067 -4.3248019 -4.3187628 -4.3152885 -4.3135586 -4.3127437 -4.3123636 -4.306757 -4.2990937 -4.2940316 -4.2938471 -4.2997217 -4.312335 -4.32787 -4.3385968][-4.3177 -4.3066306 -4.2939138 -4.2856331 -4.2817945 -4.2804904 -4.2788672 -4.2693772 -4.2568717 -4.2461 -4.2393851 -4.2450109 -4.2646308 -4.2919855 -4.3152823][-4.3035059 -4.2861419 -4.267838 -4.2547507 -4.2502894 -4.2472215 -4.2427859 -4.2249112 -4.2008209 -4.1736064 -4.1525526 -4.1549172 -4.1842165 -4.2293372 -4.271132][-4.2838287 -4.2613549 -4.2368054 -4.2168617 -4.2113228 -4.208581 -4.2041483 -4.1760745 -4.1355467 -4.0888205 -4.051259 -4.0524864 -4.0950541 -4.1587644 -4.2210693][-4.2608476 -4.2319489 -4.1973767 -4.1688647 -4.1613731 -4.1604366 -4.1576247 -4.1202145 -4.0678449 -4.0116768 -3.9706991 -3.9810643 -4.0388608 -4.1200967 -4.1957884][-4.2310505 -4.1893477 -4.1420212 -4.0980964 -4.079329 -4.0783567 -4.0839925 -4.0468822 -3.9988356 -3.9640567 -3.9438069 -3.9680693 -4.0366387 -4.1255803 -4.2021565][-4.1932 -4.130609 -4.0601034 -3.9870677 -3.95662 -3.9676895 -3.9971442 -3.9755094 -3.9562953 -3.9609351 -3.9678535 -4.001503 -4.0708318 -4.1574855 -4.226851][-4.1620955 -4.078999 -3.9876451 -3.8863823 -3.8569853 -3.8942358 -3.9557252 -3.9649892 -3.9791861 -4.0088196 -4.0318508 -4.0697408 -4.1308408 -4.2020717 -4.2569814][-4.152091 -4.0624757 -3.9731662 -3.8783143 -3.8622341 -3.9171455 -3.993413 -4.0274205 -4.0613241 -4.0929718 -4.1159911 -4.1485195 -4.1925306 -4.2436614 -4.281086][-4.18211 -4.1069407 -4.0453091 -3.9883449 -3.9824908 -4.0287786 -4.0916414 -4.1304393 -4.1667571 -4.19074 -4.2050815 -4.2256303 -4.2530084 -4.284441 -4.3055325][-4.241713 -4.1895881 -4.1531811 -4.1271596 -4.1275754 -4.1575031 -4.19841 -4.22917 -4.2562122 -4.2685437 -4.2730646 -4.2824588 -4.2969236 -4.31429 -4.3252425][-4.2986622 -4.2706366 -4.2521811 -4.240365 -4.23992 -4.2532239 -4.2746692 -4.2922454 -4.3082767 -4.3139791 -4.3153558 -4.3194118 -4.3261256 -4.3348026 -4.339222][-4.3343639 -4.3220019 -4.3150835 -4.30885 -4.3047323 -4.3079557 -4.3171492 -4.3251629 -4.3322821 -4.334126 -4.3357315 -4.3378935 -4.3404026 -4.3439851 -4.3451977][-4.3465843 -4.34223 -4.3394694 -4.3369846 -4.3333449 -4.3335171 -4.3363633 -4.338244 -4.339386 -4.3385658 -4.338654 -4.3392735 -4.3405848 -4.3423834 -4.3430481]]...]
INFO - root - 2017-12-05 11:14:24.683653: step 2010, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 82h:40m:34s remains)
INFO - root - 2017-12-05 11:14:33.637989: step 2020, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 82h:43m:16s remains)
INFO - root - 2017-12-05 11:14:42.666018: step 2030, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.904 sec/batch; 82h:56m:58s remains)
INFO - root - 2017-12-05 11:14:51.825952: step 2040, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.963 sec/batch; 88h:25m:49s remains)
INFO - root - 2017-12-05 11:15:01.099221: step 2050, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 86h:56m:49s remains)
INFO - root - 2017-12-05 11:15:10.162210: step 2060, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.874 sec/batch; 80h:15m:35s remains)
INFO - root - 2017-12-05 11:15:19.218079: step 2070, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 85h:56m:07s remains)
INFO - root - 2017-12-05 11:15:28.382705: step 2080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 82h:24m:00s remains)
INFO - root - 2017-12-05 11:15:37.553497: step 2090, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 82h:55m:36s remains)
INFO - root - 2017-12-05 11:15:46.877434: step 2100, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 84h:21m:06s remains)
2017-12-05 11:15:47.626751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1258922 -4.1198931 -4.1205182 -4.1301556 -4.1552916 -4.1736612 -4.1779733 -4.1853995 -4.20689 -4.2322488 -4.2587538 -4.2738738 -4.2691422 -4.2689776 -4.273613][-4.149755 -4.1384935 -4.1387129 -4.1467743 -4.166163 -4.1820984 -4.1829348 -4.1881938 -4.2107024 -4.238903 -4.2659907 -4.2804704 -4.2823439 -4.2872915 -4.2882328][-4.1754346 -4.1529045 -4.144721 -4.1495614 -4.1618533 -4.1668758 -4.1590877 -4.1600103 -4.1809936 -4.2091641 -4.2411308 -4.260602 -4.2719054 -4.2832284 -4.2876954][-4.1873913 -4.1584029 -4.1458936 -4.1468544 -4.14886 -4.1319122 -4.1017694 -4.0906115 -4.1096931 -4.1486135 -4.1974277 -4.2294817 -4.2508073 -4.2725248 -4.2874718][-4.1924405 -4.1709795 -4.1582866 -4.1513629 -4.1396885 -4.0987988 -4.038681 -4.000175 -4.0141716 -4.0726323 -4.1483555 -4.2004437 -4.23444 -4.2654166 -4.289732][-4.2034292 -4.1967845 -4.1836634 -4.1725526 -4.1508875 -4.09593 -4.0147834 -3.94542 -3.9467559 -4.0162249 -4.1071773 -4.1756463 -4.2237263 -4.263793 -4.29364][-4.2110763 -4.2181759 -4.2077971 -4.1956367 -4.1710911 -4.1141944 -4.0296788 -3.9533391 -3.9421911 -4.0027351 -4.0879388 -4.16038 -4.2153974 -4.2572045 -4.2844048][-4.21348 -4.2293563 -4.2188663 -4.2002606 -4.1715302 -4.1168308 -4.0386257 -3.9684336 -3.9584408 -4.0136876 -4.0909543 -4.1591582 -4.2109632 -4.2476625 -4.2693782][-4.2142859 -4.2303414 -4.2179418 -4.1922059 -4.1575503 -4.108779 -4.0429387 -3.9837747 -3.9771051 -4.0291252 -4.1009068 -4.1637731 -4.2094059 -4.2368059 -4.2510753][-4.2136164 -4.2244458 -4.2081404 -4.1813707 -4.1493835 -4.1122465 -4.0647039 -4.015451 -4.006393 -4.0513759 -4.1152687 -4.1647024 -4.1962318 -4.2162995 -4.2292428][-4.211658 -4.2173586 -4.2005057 -4.1776505 -4.1539121 -4.1315622 -4.10477 -4.0704365 -4.05709 -4.0908327 -4.1445212 -4.1765957 -4.1898522 -4.198113 -4.2067623][-4.2191453 -4.2217264 -4.2061195 -4.1868086 -4.1711044 -4.1589007 -4.1437039 -4.1201539 -4.1112752 -4.1365709 -4.178896 -4.195169 -4.1894531 -4.1760263 -4.1723638][-4.2300463 -4.2385936 -4.2299829 -4.2153859 -4.2012229 -4.1905932 -4.1804256 -4.166543 -4.1626039 -4.1804323 -4.2100391 -4.2113323 -4.1860104 -4.1460519 -4.1210127][-4.2259579 -4.2359929 -4.235929 -4.2306666 -4.2246056 -4.2183022 -4.2129407 -4.2066493 -4.2079182 -4.2201228 -4.2365227 -4.2251635 -4.1887727 -4.1317663 -4.0834866][-4.2124367 -4.2207284 -4.2254572 -4.2282329 -4.2276406 -4.2269778 -4.225101 -4.2251587 -4.2321672 -4.2411327 -4.248342 -4.23569 -4.2010565 -4.1441054 -4.0906096]]...]
INFO - root - 2017-12-05 11:15:56.682118: step 2110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 83h:52m:03s remains)
INFO - root - 2017-12-05 11:16:05.821156: step 2120, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 85h:40m:41s remains)
INFO - root - 2017-12-05 11:16:15.275691: step 2130, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 84h:45m:37s remains)
INFO - root - 2017-12-05 11:16:24.326796: step 2140, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 81h:23m:55s remains)
INFO - root - 2017-12-05 11:16:33.493244: step 2150, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.959 sec/batch; 88h:00m:55s remains)
INFO - root - 2017-12-05 11:16:42.613935: step 2160, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 83h:37m:14s remains)
INFO - root - 2017-12-05 11:16:51.929419: step 2170, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 84h:09m:56s remains)
INFO - root - 2017-12-05 11:17:00.921483: step 2180, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:39m:53s remains)
INFO - root - 2017-12-05 11:17:10.111255: step 2190, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 86h:23m:32s remains)
INFO - root - 2017-12-05 11:17:19.355486: step 2200, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 84h:13m:53s remains)
2017-12-05 11:17:20.136904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2362866 -4.2519755 -4.2788758 -4.2855787 -4.2665105 -4.2380447 -4.1850948 -4.1258054 -4.1009007 -4.1243587 -4.166852 -4.1992145 -4.2250619 -4.2493949 -4.2600031][-4.2323022 -4.2507796 -4.2766767 -4.2791753 -4.2518978 -4.2161536 -4.156415 -4.0954046 -4.0794888 -4.1133351 -4.1668296 -4.2095418 -4.2430038 -4.2697148 -4.2808666][-4.2324176 -4.2536135 -4.2773757 -4.2728739 -4.2348604 -4.1851478 -4.1126432 -4.0472226 -4.0399542 -4.0895162 -4.1599855 -4.2178917 -4.2586336 -4.2849321 -4.2926855][-4.2417793 -4.2616806 -4.2802906 -4.2679181 -4.2192855 -4.1518135 -4.0630374 -3.99203 -3.9935586 -4.0630565 -4.1550541 -4.2295995 -4.2749538 -4.2967534 -4.2963147][-4.2557855 -4.2748837 -4.2871618 -4.2662878 -4.206708 -4.1221194 -4.0187731 -3.9436414 -3.9544218 -4.0475373 -4.1599631 -4.243969 -4.2885895 -4.3052616 -4.2981987][-4.267621 -4.2879524 -4.2935405 -4.265132 -4.1977048 -4.0984797 -3.9790835 -3.891134 -3.9053698 -4.0216064 -4.1537051 -4.24686 -4.292635 -4.3065991 -4.2962918][-4.2762442 -4.29603 -4.2957168 -4.2621274 -4.1930609 -4.0864177 -3.9554858 -3.8487206 -3.8516672 -3.9795587 -4.1283484 -4.2354708 -4.2891946 -4.307312 -4.2957497][-4.2898655 -4.3060889 -4.2996793 -4.2619019 -4.196435 -4.0937667 -3.9620304 -3.8443031 -3.8287432 -3.9485207 -4.1006861 -4.2181449 -4.2813077 -4.3069091 -4.2972121][-4.3064408 -4.3174133 -4.3076348 -4.2693853 -4.2111492 -4.1236391 -4.0069356 -3.893913 -3.8650756 -3.9592929 -4.0959849 -4.2078018 -4.2741494 -4.3024874 -4.2960715][-4.3174243 -4.323957 -4.3132868 -4.2805233 -4.2338233 -4.1652026 -4.0746713 -3.9810526 -3.9480805 -4.0099959 -4.1177239 -4.2095675 -4.2665424 -4.2916269 -4.287921][-4.3211145 -4.3207216 -4.3106222 -4.28756 -4.2550535 -4.2070875 -4.1428318 -4.0746727 -4.0428829 -4.0761933 -4.1507649 -4.218719 -4.2620511 -4.28195 -4.2787361][-4.3199677 -4.312901 -4.3041263 -4.2894568 -4.2668204 -4.234344 -4.1922154 -4.1496654 -4.1283631 -4.1483817 -4.198194 -4.2447076 -4.2742052 -4.2884722 -4.285356][-4.3151674 -4.3024497 -4.294786 -4.2865496 -4.2700615 -4.2454877 -4.216712 -4.1940064 -4.1878271 -4.2064919 -4.2434359 -4.274013 -4.2903085 -4.2981791 -4.2948413][-4.3130078 -4.2969131 -4.2880735 -4.2811966 -4.2684636 -4.2508221 -4.2315011 -4.2219524 -4.225049 -4.2409992 -4.2685018 -4.2867951 -4.2927146 -4.294785 -4.2929196][-4.3139644 -4.2963848 -4.2849345 -4.277061 -4.267746 -4.2558665 -4.244348 -4.2419972 -4.2471375 -4.2563682 -4.2731867 -4.2852683 -4.2868433 -4.2861309 -4.2843666]]...]
INFO - root - 2017-12-05 11:17:29.166977: step 2210, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 86h:05m:38s remains)
INFO - root - 2017-12-05 11:17:38.252948: step 2220, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 81h:02m:51s remains)
INFO - root - 2017-12-05 11:17:47.316047: step 2230, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 85h:23m:33s remains)
INFO - root - 2017-12-05 11:17:56.540676: step 2240, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 82h:01m:10s remains)
INFO - root - 2017-12-05 11:18:05.696824: step 2250, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 83h:03m:44s remains)
INFO - root - 2017-12-05 11:18:14.834677: step 2260, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 84h:13m:15s remains)
INFO - root - 2017-12-05 11:18:24.041289: step 2270, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 84h:52m:59s remains)
INFO - root - 2017-12-05 11:18:33.218242: step 2280, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 82h:02m:41s remains)
INFO - root - 2017-12-05 11:18:42.398443: step 2290, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 81h:21m:55s remains)
INFO - root - 2017-12-05 11:18:51.496293: step 2300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 83h:27m:43s remains)
2017-12-05 11:18:52.342824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1264243 -4.047729 -4.0176377 -4.0389357 -4.0831566 -4.12081 -4.1302366 -4.124053 -4.1201363 -4.123445 -4.122942 -4.1250672 -4.1288915 -4.139555 -4.1536][-4.1357942 -4.0661135 -4.0375051 -4.0565252 -4.0952053 -4.1234 -4.129427 -4.1344471 -4.1398883 -4.1377392 -4.1278729 -4.1168957 -4.1075373 -4.1088562 -4.1182804][-4.1474023 -4.0861592 -4.0605659 -4.0741348 -4.1063933 -4.1290021 -4.1373086 -4.1466117 -4.1542125 -4.1411982 -4.1201582 -4.1055717 -4.0961065 -4.0961356 -4.0990205][-4.1569757 -4.1006713 -4.074738 -4.0817132 -4.1034603 -4.1227 -4.1384912 -4.1550803 -4.16511 -4.1489229 -4.1251698 -4.1128583 -4.1044679 -4.1017389 -4.0980015][-4.1691356 -4.1147814 -4.0868034 -4.0848422 -4.0906162 -4.1066437 -4.1295614 -4.1512575 -4.1614251 -4.1487064 -4.1345654 -4.1343136 -4.1324506 -4.1258173 -4.1171761][-4.1797791 -4.1308942 -4.105896 -4.0981836 -4.0912275 -4.1004734 -4.1243443 -4.145813 -4.1574478 -4.1520576 -4.1484718 -4.1572819 -4.1616464 -4.1578493 -4.1476493][-4.1867595 -4.1436834 -4.1198039 -4.1072526 -4.0946927 -4.097465 -4.1154776 -4.1288624 -4.13907 -4.1415963 -4.1466107 -4.1615491 -4.1706738 -4.1734076 -4.1670933][-4.1898251 -4.151278 -4.130971 -4.1168013 -4.100893 -4.0905771 -4.093648 -4.0965586 -4.1034079 -4.1167455 -4.1315279 -4.1500344 -4.1609674 -4.1642694 -4.16301][-4.1906672 -4.1561723 -4.1436639 -4.1350851 -4.1161342 -4.0888224 -4.0764155 -4.0737777 -4.0823317 -4.1065893 -4.1317744 -4.1500335 -4.1570411 -4.15578 -4.1544566][-4.1892118 -4.1556082 -4.150322 -4.15293 -4.1415439 -4.1117387 -4.0879307 -4.0792704 -4.0878429 -4.1151643 -4.1469975 -4.1654577 -4.1690984 -4.1630526 -4.1552281][-4.1901674 -4.1503134 -4.1426935 -4.1504788 -4.1516314 -4.1360526 -4.113565 -4.1005545 -4.1063881 -4.1287251 -4.1573453 -4.1751142 -4.1793561 -4.168241 -4.1512928][-4.1904526 -4.1419382 -4.12557 -4.1285195 -4.1366291 -4.1377449 -4.1259828 -4.1203761 -4.1253543 -4.1365128 -4.152287 -4.1612968 -4.1663055 -4.1556916 -4.1307282][-4.1917481 -4.1385136 -4.1170077 -4.1164756 -4.1242156 -4.1346092 -4.1323404 -4.1298952 -4.1308722 -4.1248379 -4.1223655 -4.1235752 -4.1340742 -4.1358604 -4.1161628][-4.1968436 -4.1446857 -4.1223841 -4.1202345 -4.1291895 -4.14381 -4.1443009 -4.1406188 -4.1362691 -4.1175122 -4.1047306 -4.1062279 -4.1228495 -4.133585 -4.1204481][-4.2051167 -4.1621122 -4.1417894 -4.1389017 -4.1455674 -4.1597009 -4.1648421 -4.1637549 -4.1566229 -4.1353045 -4.1180267 -4.1137314 -4.1199093 -4.1213059 -4.1068926]]...]
INFO - root - 2017-12-05 11:19:01.441198: step 2310, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 85h:11m:32s remains)
INFO - root - 2017-12-05 11:19:10.691414: step 2320, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 85h:09m:43s remains)
INFO - root - 2017-12-05 11:19:19.889758: step 2330, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 85h:07m:34s remains)
INFO - root - 2017-12-05 11:19:28.901394: step 2340, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 83h:38m:15s remains)
INFO - root - 2017-12-05 11:19:37.957323: step 2350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 84h:54m:00s remains)
INFO - root - 2017-12-05 11:19:47.047728: step 2360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:29m:35s remains)
INFO - root - 2017-12-05 11:19:56.153082: step 2370, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 83h:18m:54s remains)
INFO - root - 2017-12-05 11:20:05.289617: step 2380, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 86h:49m:23s remains)
INFO - root - 2017-12-05 11:20:14.604367: step 2390, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 81h:56m:21s remains)
INFO - root - 2017-12-05 11:20:23.709559: step 2400, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 82h:24m:35s remains)
2017-12-05 11:20:24.531861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0880117 -4.1153526 -4.1454024 -4.1753778 -4.1964 -4.2077122 -4.2126379 -4.2152228 -4.2224097 -4.2341785 -4.24233 -4.2455654 -4.2374368 -4.2251892 -4.2218137][-4.0859337 -4.110208 -4.13933 -4.1720295 -4.1983919 -4.2201748 -4.2323174 -4.2346778 -4.2340488 -4.238832 -4.2408214 -4.2364364 -4.2271342 -4.2177868 -4.2212558][-4.1122217 -4.128108 -4.1521344 -4.1822987 -4.207222 -4.228662 -4.2389507 -4.237524 -4.2300315 -4.2297568 -4.226892 -4.2186084 -4.2086577 -4.1982641 -4.2031379][-4.1355958 -4.1494451 -4.1708903 -4.1959963 -4.2145967 -4.2271729 -4.22935 -4.2194757 -4.2061343 -4.2057147 -4.2041392 -4.1993704 -4.1915936 -4.17958 -4.1807728][-4.1457934 -4.1611762 -4.1787219 -4.1939459 -4.2011447 -4.1987324 -4.1879959 -4.1680689 -4.154911 -4.1623387 -4.172801 -4.1812787 -4.1822205 -4.1743569 -4.1692085][-4.1354442 -4.1480622 -4.1576333 -4.1603484 -4.1534033 -4.1301 -4.0986462 -4.0672741 -4.06605 -4.0959864 -4.13224 -4.1635208 -4.1798682 -4.1802669 -4.1681805][-4.1168485 -4.1216378 -4.1230145 -4.116652 -4.0945883 -4.0472941 -3.9881034 -3.9434328 -3.9626634 -4.0260754 -4.0918655 -4.1474147 -4.1810474 -4.1899071 -4.1755128][-4.1087174 -4.109313 -4.1098766 -4.1026278 -4.0694742 -4.0074105 -3.9308777 -3.8822262 -3.9177189 -3.9951596 -4.0705395 -4.1351018 -4.1782069 -4.1968074 -4.187736][-4.125854 -4.1241755 -4.1304374 -4.1295757 -4.1020174 -4.0479355 -3.9887729 -3.9578507 -3.9840722 -4.0374169 -4.0938745 -4.1452489 -4.1870928 -4.2099824 -4.2057738][-4.1626306 -4.1547585 -4.1606846 -4.1668491 -4.1559415 -4.1246696 -4.0944128 -4.0804358 -4.0888762 -4.1082454 -4.1368475 -4.1696587 -4.2042084 -4.2248707 -4.2226872][-4.1991282 -4.1843247 -4.1835876 -4.190753 -4.1929092 -4.1842489 -4.17763 -4.1721797 -4.1658068 -4.1626534 -4.1712613 -4.189136 -4.2139544 -4.2296553 -4.22878][-4.2177911 -4.2046733 -4.1999598 -4.2088985 -4.2193012 -4.2246995 -4.2286735 -4.2215743 -4.2055712 -4.1901135 -4.1845517 -4.1911569 -4.2075095 -4.2170181 -4.2159224][-4.221314 -4.2151179 -4.2120361 -4.2227678 -4.2360792 -4.2443519 -4.24813 -4.2366166 -4.2149372 -4.1884403 -4.1659455 -4.1582069 -4.1652732 -4.1699982 -4.17398][-4.2079377 -4.2090569 -4.2110949 -4.2240467 -4.2395372 -4.2491522 -4.2501721 -4.2342591 -4.207871 -4.1728563 -4.1356134 -4.1128416 -4.1095452 -4.1120996 -4.1268482][-4.1841364 -4.1899366 -4.198514 -4.2119279 -4.2266197 -4.2365513 -4.23639 -4.21927 -4.1922288 -4.1588631 -4.1233506 -4.0984888 -4.0903597 -4.0934577 -4.1184964]]...]
INFO - root - 2017-12-05 11:20:33.651253: step 2410, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 80h:34m:54s remains)
INFO - root - 2017-12-05 11:20:42.897798: step 2420, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 84h:44m:22s remains)
INFO - root - 2017-12-05 11:20:51.981973: step 2430, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 83h:01m:38s remains)
INFO - root - 2017-12-05 11:21:00.995668: step 2440, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 83h:09m:42s remains)
INFO - root - 2017-12-05 11:21:10.130760: step 2450, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.874 sec/batch; 80h:07m:24s remains)
INFO - root - 2017-12-05 11:21:19.241321: step 2460, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 81h:56m:59s remains)
INFO - root - 2017-12-05 11:21:28.508766: step 2470, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 83h:02m:44s remains)
INFO - root - 2017-12-05 11:21:37.781057: step 2480, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 86h:17m:55s remains)
INFO - root - 2017-12-05 11:21:46.971254: step 2490, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 87h:34m:58s remains)
INFO - root - 2017-12-05 11:21:56.088537: step 2500, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 83h:08m:53s remains)
2017-12-05 11:21:56.880982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2784848 -4.2768059 -4.280334 -4.2867975 -4.2910094 -4.2939577 -4.2974315 -4.3058753 -4.3160625 -4.3170366 -4.3065476 -4.2949786 -4.2913227 -4.2944713 -4.2990189][-4.2562733 -4.2478671 -4.25023 -4.2554278 -4.256834 -4.2576637 -4.2626624 -4.2792807 -4.2982388 -4.3057127 -4.2963443 -4.2799907 -4.2725472 -4.2740769 -4.2778206][-4.2248716 -4.211411 -4.20782 -4.211132 -4.2154021 -4.2159462 -4.2198763 -4.2453265 -4.2750497 -4.288919 -4.28073 -4.2609158 -4.2490544 -4.24675 -4.2493224][-4.179379 -4.1617541 -4.1563492 -4.1615357 -4.1693544 -4.1642609 -4.159894 -4.1910295 -4.2328262 -4.2555609 -4.2542691 -4.2385259 -4.2251468 -4.2190361 -4.2192955][-4.1591458 -4.1371779 -4.1241145 -4.1266141 -4.1299009 -4.1108928 -4.0922623 -4.123065 -4.1764607 -4.2112164 -4.2189832 -4.2108855 -4.2035589 -4.1992917 -4.1963267][-4.1404715 -4.1097527 -4.0846996 -4.0742517 -4.0694013 -4.0364513 -3.9999475 -4.0278711 -4.0996981 -4.1526351 -4.1722536 -4.1757641 -4.18234 -4.1892276 -4.1940069][-4.1334009 -4.0948067 -4.0610824 -4.0345173 -4.014329 -3.9608133 -3.8953922 -3.9060068 -4.0007391 -4.0815573 -4.125958 -4.1458669 -4.1632218 -4.1762795 -4.1921616][-4.148272 -4.1075525 -4.0687733 -4.0285816 -3.9958844 -3.932266 -3.8436193 -3.8273125 -3.9248743 -4.0247731 -4.0847344 -4.1155386 -4.1414137 -4.1642647 -4.1889553][-4.1616778 -4.1212158 -4.0791416 -4.037612 -4.0035477 -3.9538493 -3.8849454 -3.8702743 -3.947551 -4.0271535 -4.0695176 -4.0928283 -4.1187897 -4.1487145 -4.1803288][-4.1832128 -4.1486373 -4.1117568 -4.0752358 -4.0469317 -4.01017 -3.9577675 -3.9417429 -3.9864969 -4.03259 -4.0563641 -4.0727925 -4.0976863 -4.1287665 -4.1647072][-4.1699123 -4.140275 -4.1132026 -4.094583 -4.0785933 -4.0561976 -4.0200882 -4.0024672 -4.01541 -4.0337381 -4.0418291 -4.0580192 -4.0884304 -4.1209965 -4.1541052][-4.1625619 -4.13612 -4.1139607 -4.1060638 -4.1032939 -4.0952935 -4.075305 -4.0648727 -4.0696583 -4.073204 -4.0697846 -4.0849962 -4.1135335 -4.1384373 -4.1655688][-4.1897182 -4.1661906 -4.1460819 -4.1375437 -4.1442909 -4.1455927 -4.1360521 -4.1331329 -4.1397595 -4.1374378 -4.1267967 -4.1395011 -4.1657224 -4.1845703 -4.20535][-4.2261004 -4.2086015 -4.1956143 -4.1887846 -4.1978717 -4.2057071 -4.2018671 -4.2042427 -4.2150893 -4.2118287 -4.2012749 -4.211627 -4.2313929 -4.24454 -4.259716][-4.2666817 -4.2557092 -4.2496924 -4.2457709 -4.2548451 -4.2667828 -4.2673073 -4.2694139 -4.2801437 -4.279109 -4.2719693 -4.2794075 -4.2913222 -4.2985654 -4.3062944]]...]
INFO - root - 2017-12-05 11:22:06.069763: step 2510, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 83h:06m:31s remains)
INFO - root - 2017-12-05 11:22:15.096106: step 2520, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 86h:17m:03s remains)
INFO - root - 2017-12-05 11:22:24.077980: step 2530, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 80h:33m:04s remains)
INFO - root - 2017-12-05 11:22:33.400594: step 2540, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 84h:19m:33s remains)
INFO - root - 2017-12-05 11:22:42.593971: step 2550, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 82h:24m:40s remains)
INFO - root - 2017-12-05 11:22:51.691952: step 2560, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 82h:07m:01s remains)
INFO - root - 2017-12-05 11:23:00.759941: step 2570, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 84h:49m:41s remains)
INFO - root - 2017-12-05 11:23:09.987293: step 2580, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 83h:07m:02s remains)
INFO - root - 2017-12-05 11:23:19.066235: step 2590, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 81h:50m:15s remains)
INFO - root - 2017-12-05 11:23:28.178510: step 2600, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 79h:23m:35s remains)
2017-12-05 11:23:28.961951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1953931 -4.1992717 -4.1970963 -4.191865 -4.1929264 -4.1942539 -4.194726 -4.1944008 -4.1858959 -4.1736097 -4.1586823 -4.1403961 -4.1383386 -4.1532784 -4.1670079][-4.1749969 -4.1705103 -4.1669192 -4.1689873 -4.1826749 -4.1987743 -4.2098317 -4.2113547 -4.1970468 -4.1745496 -4.1474562 -4.1232224 -4.1227875 -4.14075 -4.158618][-4.1550612 -4.152338 -4.15698 -4.1697807 -4.1890507 -4.2091479 -4.2198181 -4.215189 -4.19228 -4.1625228 -4.1295738 -4.1051121 -4.1044955 -4.1171708 -4.1342983][-4.1492391 -4.1538396 -4.1705217 -4.1936731 -4.2125239 -4.2228084 -4.2215238 -4.2067604 -4.1811957 -4.15058 -4.1184077 -4.0922585 -4.0832872 -4.0855122 -4.1000724][-4.157495 -4.1705203 -4.1936817 -4.2185564 -4.2297721 -4.2275367 -4.21173 -4.1886005 -4.1675177 -4.1486583 -4.1254516 -4.09735 -4.0763392 -4.0658774 -4.0760531][-4.1734738 -4.1922688 -4.2141881 -4.2327447 -4.23574 -4.2230911 -4.1943469 -4.1602407 -4.1401887 -4.13606 -4.1253734 -4.0988903 -4.0738583 -4.0596995 -4.069221][-4.176805 -4.1984005 -4.2159953 -4.2260652 -4.2207365 -4.1983662 -4.1589427 -4.1138659 -4.0928631 -4.100265 -4.1046782 -4.0918856 -4.0832238 -4.0827169 -4.0983739][-4.1673603 -4.1949663 -4.2103939 -4.2137947 -4.2018428 -4.1728616 -4.1292195 -4.0842071 -4.0657854 -4.07751 -4.09237 -4.0970449 -4.1057863 -4.1154027 -4.1304851][-4.1610222 -4.1981893 -4.2135344 -4.2108011 -4.1960435 -4.1670771 -4.1277714 -4.0923405 -4.0813985 -4.0920081 -4.1085739 -4.1246347 -4.1419377 -4.1529884 -4.1655927][-4.159924 -4.197639 -4.2139659 -4.2103238 -4.1958537 -4.1706529 -4.1380372 -4.1127987 -4.1129985 -4.127038 -4.1432533 -4.1578755 -4.172328 -4.1804671 -4.1912827][-4.1648817 -4.1950603 -4.2113614 -4.2135339 -4.2029185 -4.18069 -4.1540751 -4.1386333 -4.1468387 -4.16312 -4.1759825 -4.182354 -4.1854787 -4.1879396 -4.1992931][-4.1733694 -4.19472 -4.2078915 -4.2151203 -4.2102818 -4.1906605 -4.1694374 -4.1625376 -4.1749964 -4.1928744 -4.202878 -4.202177 -4.1977129 -4.1966815 -4.2093906][-4.1846676 -4.2003827 -4.2114363 -4.22045 -4.2222385 -4.2108345 -4.1967192 -4.193923 -4.2046523 -4.2194858 -4.2258177 -4.219749 -4.2119961 -4.2097707 -4.2184949][-4.2143779 -4.223362 -4.2315431 -4.2396312 -4.2451653 -4.2408023 -4.2332878 -4.2317395 -4.2383537 -4.247613 -4.25018 -4.242321 -4.2359076 -4.2355428 -4.2403307][-4.25174 -4.2536707 -4.2574291 -4.263093 -4.2694049 -4.2691169 -4.2657771 -4.265852 -4.2701283 -4.2750874 -4.2760592 -4.2695265 -4.2657619 -4.2670302 -4.2697959]]...]
INFO - root - 2017-12-05 11:23:38.092261: step 2610, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 81h:05m:48s remains)
INFO - root - 2017-12-05 11:23:47.194724: step 2620, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.940 sec/batch; 86h:10m:04s remains)
INFO - root - 2017-12-05 11:23:56.189032: step 2630, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 80h:51m:58s remains)
INFO - root - 2017-12-05 11:24:05.319005: step 2640, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 83h:02m:02s remains)
INFO - root - 2017-12-05 11:24:14.340711: step 2650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 80h:40m:29s remains)
INFO - root - 2017-12-05 11:24:23.477227: step 2660, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 84h:22m:07s remains)
INFO - root - 2017-12-05 11:24:32.699188: step 2670, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.898 sec/batch; 82h:17m:01s remains)
INFO - root - 2017-12-05 11:24:41.839419: step 2680, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 83h:12m:57s remains)
INFO - root - 2017-12-05 11:24:50.841742: step 2690, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:59m:35s remains)
INFO - root - 2017-12-05 11:24:59.985643: step 2700, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 85h:14m:05s remains)
2017-12-05 11:25:00.826857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2052016 -4.2208881 -4.2302489 -4.232162 -4.2302127 -4.2276955 -4.2266779 -4.2302933 -4.2327733 -4.2360778 -4.2388625 -4.2414789 -4.2422805 -4.2401614 -4.2363839][-4.2503734 -4.2593555 -4.2577734 -4.2498512 -4.2405019 -4.2315226 -4.2275181 -4.2321572 -4.238554 -4.2458444 -4.2512336 -4.2554984 -4.2570415 -4.25432 -4.249043][-4.2860675 -4.2891374 -4.2801218 -4.2659388 -4.2486386 -4.2329812 -4.2282434 -4.2348609 -4.2460933 -4.2590165 -4.2700524 -4.2787552 -4.2844453 -4.2836227 -4.2787681][-4.2874641 -4.2851796 -4.269453 -4.2467003 -4.2216196 -4.2032557 -4.1991558 -4.2079463 -4.22383 -4.241828 -4.257741 -4.2711978 -4.283287 -4.2838154 -4.2757206][-4.2578959 -4.2551718 -4.2354584 -4.2080584 -4.1785727 -4.1589513 -4.1553426 -4.1671844 -4.1865315 -4.2064233 -4.2238951 -4.2413554 -4.2578664 -4.256249 -4.2422018][-4.1923404 -4.1906586 -4.1703482 -4.1411352 -4.110302 -4.0905261 -4.0831227 -4.0959849 -4.1230354 -4.1486053 -4.169805 -4.1921253 -4.21332 -4.2104516 -4.1935558][-4.11392 -4.1061654 -4.0802684 -4.0478582 -4.015882 -3.9920585 -3.9761667 -3.9873688 -4.0230751 -4.0581045 -4.0881052 -4.119051 -4.1454706 -4.1442451 -4.1313281][-4.086122 -4.0782814 -4.0583577 -4.0346313 -4.00866 -3.9832897 -3.9615054 -3.9659429 -3.9976029 -4.0310631 -4.0596123 -4.0878677 -4.1053252 -4.0971556 -4.0816779][-4.1256342 -4.1275864 -4.1248088 -4.1203122 -4.107451 -4.0918021 -4.0769281 -4.0787048 -4.0992413 -4.1206651 -4.137382 -4.1509795 -4.1513963 -4.1338286 -4.1117644][-4.1865149 -4.19045 -4.1911139 -4.1928859 -4.1855488 -4.1771903 -4.1738281 -4.1787758 -4.1941667 -4.2098207 -4.221632 -4.2278981 -4.2226448 -4.2053461 -4.1843667][-4.2204876 -4.22276 -4.2213516 -4.2193942 -4.2111468 -4.205308 -4.208199 -4.2149954 -4.2281084 -4.240911 -4.2463355 -4.2478738 -4.2403755 -4.227438 -4.21115][-4.2069273 -4.2047486 -4.1966214 -4.186377 -4.1756029 -4.1720304 -4.1756721 -4.1790318 -4.187645 -4.1994843 -4.2070103 -4.2154202 -4.2178173 -4.2164116 -4.2104111][-4.17963 -4.1715827 -4.1574297 -4.1433563 -4.1315784 -4.12877 -4.1318436 -4.1316848 -4.1373982 -4.1477666 -4.1583695 -4.1761231 -4.1914644 -4.2030306 -4.2095547][-4.16913 -4.1582618 -4.1425648 -4.1299295 -4.1214795 -4.1183014 -4.119998 -4.1191559 -4.1238022 -4.1324558 -4.1424456 -4.1634259 -4.1853909 -4.2035322 -4.2165647][-4.1860924 -4.1742744 -4.1592445 -4.1502647 -4.1473641 -4.1459551 -4.1463914 -4.1440048 -4.1463995 -4.1531262 -4.1598449 -4.1760492 -4.1960993 -4.2134614 -4.22433]]...]
INFO - root - 2017-12-05 11:25:09.963751: step 2710, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.874 sec/batch; 80h:04m:22s remains)
INFO - root - 2017-12-05 11:25:18.923792: step 2720, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 85h:30m:27s remains)
INFO - root - 2017-12-05 11:25:28.096896: step 2730, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 84h:41m:20s remains)
INFO - root - 2017-12-05 11:25:37.194328: step 2740, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 83h:19m:38s remains)
INFO - root - 2017-12-05 11:25:46.407432: step 2750, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 86h:07m:02s remains)
INFO - root - 2017-12-05 11:25:55.372193: step 2760, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.894 sec/batch; 81h:50m:55s remains)
INFO - root - 2017-12-05 11:26:04.549544: step 2770, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 85h:42m:20s remains)
INFO - root - 2017-12-05 11:26:13.623533: step 2780, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 82h:43m:00s remains)
INFO - root - 2017-12-05 11:26:22.720312: step 2790, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 82h:59m:30s remains)
INFO - root - 2017-12-05 11:26:31.830979: step 2800, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 84h:26m:54s remains)
2017-12-05 11:26:32.649939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1781883 -4.1876483 -4.1822019 -4.1887121 -4.1986384 -4.1890125 -4.1470265 -4.1017704 -4.1074181 -4.146543 -4.2033148 -4.2554326 -4.2978778 -4.32815 -4.3386345][-4.1511912 -4.1504326 -4.1414361 -4.1531167 -4.172102 -4.1675262 -4.1264477 -4.082396 -4.0909352 -4.135644 -4.1934156 -4.246284 -4.2898874 -4.3229685 -4.337162][-4.1529641 -4.1327224 -4.1124334 -4.1196542 -4.1458511 -4.1523767 -4.1186366 -4.0740194 -4.0808506 -4.1330338 -4.1933112 -4.2440143 -4.28671 -4.31813 -4.3342195][-4.17987 -4.1490245 -4.1181316 -4.1095676 -4.1252761 -4.136271 -4.1146183 -4.0692186 -4.0693665 -4.1239147 -4.1892633 -4.2407737 -4.2858028 -4.3175883 -4.3335171][-4.21381 -4.1797256 -4.1389618 -4.1131849 -4.1078854 -4.1056881 -4.0870271 -4.0457072 -4.0466943 -4.1101446 -4.181931 -4.2337823 -4.2819505 -4.3152232 -4.3295107][-4.233768 -4.2012429 -4.15645 -4.1087432 -4.0740604 -4.0551457 -4.0337029 -3.9968114 -4.0084982 -4.0875587 -4.1731119 -4.2295418 -4.2776155 -4.3110356 -4.3245792][-4.229382 -4.203702 -4.1591454 -4.094696 -4.0309286 -4.000288 -3.9800608 -3.9482164 -3.9706154 -4.0631194 -4.1604714 -4.2265177 -4.2756014 -4.3078294 -4.319335][-4.2102404 -4.1925063 -4.1512733 -4.0796018 -4.002233 -3.9650309 -3.9444954 -3.9144208 -3.9466023 -4.0458422 -4.1497159 -4.2243404 -4.2766538 -4.3077421 -4.3171129][-4.1938152 -4.1777759 -4.1409321 -4.0925579 -4.0413904 -4.0186806 -3.9897189 -3.9449153 -3.9668107 -4.0534191 -4.1502481 -4.2253895 -4.2806063 -4.3131723 -4.3216581][-4.1916213 -4.1704173 -4.1425061 -4.1334658 -4.1325655 -4.1316109 -4.0949473 -4.0367289 -4.0353127 -4.0949039 -4.1667452 -4.2266841 -4.2803321 -4.3139753 -4.3257217][-4.19117 -4.1666346 -4.1447368 -4.1534448 -4.1808481 -4.1922932 -4.1597843 -4.1083221 -4.0976667 -4.1389794 -4.1903987 -4.2322607 -4.2786841 -4.3101339 -4.3270717][-4.1929159 -4.1630559 -4.1396961 -4.1523929 -4.1814756 -4.1929269 -4.1652904 -4.1283622 -4.1219149 -4.1554976 -4.1976676 -4.2337127 -4.2751503 -4.30719 -4.3282671][-4.1906214 -4.1684494 -4.1495757 -4.1581559 -4.1774178 -4.1786036 -4.1486969 -4.1284008 -4.1303186 -4.1569428 -4.1954575 -4.23121 -4.2701092 -4.3027825 -4.3270307][-4.1777697 -4.1625381 -4.1548257 -4.16664 -4.1773825 -4.1680984 -4.1374221 -4.1297836 -4.139801 -4.161675 -4.1954389 -4.2296953 -4.2670684 -4.29879 -4.3236036][-4.1580625 -4.1378951 -4.1309447 -4.1450224 -4.1599 -4.1549058 -4.1347508 -4.1394153 -4.1545129 -4.1717877 -4.200995 -4.2323341 -4.2630243 -4.2899723 -4.3134766]]...]
INFO - root - 2017-12-05 11:26:41.662728: step 2810, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 78h:31m:09s remains)
INFO - root - 2017-12-05 11:26:50.722286: step 2820, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 82h:34m:14s remains)
INFO - root - 2017-12-05 11:26:59.731596: step 2830, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:29m:15s remains)
INFO - root - 2017-12-05 11:27:08.849682: step 2840, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 85h:00m:50s remains)
INFO - root - 2017-12-05 11:27:18.061270: step 2850, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 83h:34m:57s remains)
INFO - root - 2017-12-05 11:27:27.074803: step 2860, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:44m:48s remains)
INFO - root - 2017-12-05 11:27:36.139315: step 2870, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 83h:15m:16s remains)
INFO - root - 2017-12-05 11:27:45.252105: step 2880, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 81h:32m:19s remains)
INFO - root - 2017-12-05 11:27:54.264358: step 2890, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 82h:15m:24s remains)
INFO - root - 2017-12-05 11:28:03.197194: step 2900, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 81h:45m:20s remains)
2017-12-05 11:28:03.957976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2701464 -4.2829304 -4.2883916 -4.2891178 -4.2873235 -4.275773 -4.2474995 -4.2139969 -4.1885476 -4.1929827 -4.2297673 -4.2679234 -4.2863274 -4.2821951 -4.2657237][-4.2678752 -4.2732282 -4.2700963 -4.2639351 -4.2618766 -4.2574358 -4.2377839 -4.2121763 -4.1915784 -4.1990495 -4.2401967 -4.2815228 -4.3004675 -4.2964454 -4.281899][-4.2685204 -4.2661281 -4.2514505 -4.2390437 -4.2428188 -4.2526565 -4.2471132 -4.22923 -4.2100177 -4.21283 -4.2473922 -4.2828984 -4.2972713 -4.2915053 -4.2798147][-4.2734776 -4.2669935 -4.2461152 -4.2328405 -4.242116 -4.2614374 -4.2629342 -4.2495952 -4.2326117 -4.2304368 -4.252172 -4.2737765 -4.2806768 -4.2742515 -4.2642269][-4.2734861 -4.2700872 -4.2534332 -4.2447538 -4.2558813 -4.2729506 -4.2743478 -4.2637262 -4.2494698 -4.2435045 -4.2509584 -4.2566175 -4.2540717 -4.2453895 -4.237782][-4.2735438 -4.2797527 -4.2740979 -4.2722445 -4.2810946 -4.2894111 -4.2860332 -4.2757778 -4.2638531 -4.2554483 -4.2531338 -4.2474179 -4.2381516 -4.2275114 -4.2212057][-4.2751732 -4.2904115 -4.2924051 -4.2932181 -4.2974348 -4.2966518 -4.2887278 -4.280025 -4.2721896 -4.2675743 -4.2669697 -4.2622886 -4.252902 -4.2423048 -4.2346253][-4.27363 -4.2963734 -4.3033428 -4.3021231 -4.2981105 -4.2893572 -4.2787123 -4.2712722 -4.26667 -4.2675948 -4.2733178 -4.2757812 -4.2724857 -4.2679887 -4.2643743][-4.2653933 -4.2973628 -4.3089375 -4.3049636 -4.2928042 -4.2779388 -4.2653642 -4.2589488 -4.2566595 -4.2610598 -4.2707558 -4.2782221 -4.27981 -4.2823944 -4.2859516][-4.2511096 -4.2919731 -4.30851 -4.3046107 -4.2875957 -4.2664113 -4.2515912 -4.246531 -4.2458467 -4.2507172 -4.2585235 -4.2648225 -4.267786 -4.2759 -4.2883606][-4.2415209 -4.2879963 -4.3079414 -4.3063216 -4.2865191 -4.2597084 -4.2439957 -4.2403169 -4.2374992 -4.2365007 -4.2371626 -4.2353239 -4.23378 -4.2440786 -4.263773][-4.25149 -4.2969866 -4.3154521 -4.3126063 -4.2900548 -4.2594333 -4.2429838 -4.2393665 -4.2335076 -4.2276692 -4.2195611 -4.2060747 -4.1955638 -4.2024817 -4.22467][-4.2726693 -4.304563 -4.3146009 -4.3081188 -4.2870235 -4.2585878 -4.2428007 -4.2389374 -4.2345624 -4.2304015 -4.2183213 -4.1977577 -4.1803231 -4.18201 -4.1964397][-4.286943 -4.301837 -4.3023939 -4.2953339 -4.2828517 -4.2630811 -4.247519 -4.2413273 -4.2405624 -4.24218 -4.2319565 -4.2109313 -4.1923013 -4.1891775 -4.1889381][-4.2876086 -4.2915206 -4.2862654 -4.2819781 -4.2796574 -4.2701449 -4.2575502 -4.2510409 -4.2536321 -4.259645 -4.2540493 -4.2370129 -4.2217875 -4.215528 -4.2027655]]...]
INFO - root - 2017-12-05 11:28:13.033350: step 2910, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 86h:36m:36s remains)
INFO - root - 2017-12-05 11:28:22.136997: step 2920, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 83h:43m:39s remains)
INFO - root - 2017-12-05 11:28:31.231692: step 2930, loss = 2.10, batch loss = 2.05 (8.9 examples/sec; 0.898 sec/batch; 82h:13m:26s remains)
INFO - root - 2017-12-05 11:28:40.552391: step 2940, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 84h:22m:57s remains)
INFO - root - 2017-12-05 11:28:49.762283: step 2950, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 84h:38m:37s remains)
INFO - root - 2017-12-05 11:28:58.885439: step 2960, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 85h:11m:26s remains)
INFO - root - 2017-12-05 11:29:08.111283: step 2970, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 86h:35m:21s remains)
INFO - root - 2017-12-05 11:29:17.115972: step 2980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 85h:20m:10s remains)
INFO - root - 2017-12-05 11:29:26.285120: step 2990, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 84h:40m:04s remains)
INFO - root - 2017-12-05 11:29:35.313637: step 3000, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 77h:11m:12s remains)
2017-12-05 11:29:36.072924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3171988 -4.3165951 -4.31691 -4.3153629 -4.3054519 -4.28135 -4.2414241 -4.1939826 -4.1479425 -4.0989017 -4.0399303 -3.9692748 -3.9062989 -3.8892007 -3.9353964][-4.3156328 -4.3150263 -4.3156309 -4.3156343 -4.3080626 -4.288404 -4.2544551 -4.2154775 -4.1796107 -4.1411734 -4.0944304 -4.0391455 -3.9915361 -3.9773135 -4.00804][-4.3151507 -4.3139405 -4.3140626 -4.3141923 -4.3087482 -4.2940278 -4.2675242 -4.2381034 -4.2135129 -4.1908121 -4.1647916 -4.131731 -4.1043329 -4.0974312 -4.118566][-4.3158121 -4.3134046 -4.311727 -4.3099337 -4.3043785 -4.2923408 -4.2707567 -4.24749 -4.2311516 -4.2222171 -4.2190633 -4.213964 -4.2111578 -4.218812 -4.23691][-4.3167262 -4.3134022 -4.30849 -4.3011379 -4.2887378 -4.2696214 -4.2442021 -4.217557 -4.2031112 -4.2065644 -4.2285314 -4.2553382 -4.2786717 -4.301146 -4.3181167][-4.317718 -4.3139782 -4.305717 -4.2893815 -4.2607803 -4.2198677 -4.1734185 -4.1289506 -4.103786 -4.1177425 -4.1729736 -4.2383418 -4.2927065 -4.3309693 -4.3494139][-4.3186936 -4.3155432 -4.3058543 -4.2825866 -4.2383022 -4.1731505 -4.0943804 -4.0152841 -3.9621816 -3.9781034 -4.0666332 -4.173068 -4.2575879 -4.3112335 -4.3348536][-4.3203564 -4.318676 -4.3099771 -4.2856221 -4.2356782 -4.1600409 -4.0632253 -3.9631209 -3.8864856 -3.8935087 -3.9956989 -4.123579 -4.2243381 -4.2879424 -4.3163347][-4.3224039 -4.3219662 -4.3163924 -4.29716 -4.2524171 -4.185431 -4.1028843 -4.0254693 -3.9683013 -3.9709864 -4.0488191 -4.1544304 -4.2398295 -4.293993 -4.3171315][-4.319634 -4.318912 -4.3166075 -4.3057795 -4.2747221 -4.2279119 -4.1723337 -4.1277723 -4.09618 -4.0966983 -4.1451635 -4.2168384 -4.275414 -4.3118863 -4.3265367][-4.304728 -4.3001189 -4.2960453 -4.2900786 -4.2729788 -4.2485442 -4.2199712 -4.1995869 -4.1844854 -4.18224 -4.2084303 -4.253737 -4.2916956 -4.3167977 -4.3290133][-4.2739625 -4.2627611 -4.25259 -4.248065 -4.2440109 -4.2422247 -4.23941 -4.2366147 -4.2294869 -4.2256308 -4.2380557 -4.2651587 -4.2889981 -4.3081183 -4.3232512][-4.229116 -4.2125659 -4.1962981 -4.1901722 -4.1937413 -4.2084861 -4.2254405 -4.2334566 -4.2266359 -4.2171407 -4.2157764 -4.229116 -4.2466664 -4.2690754 -4.2937088][-4.1906009 -4.1754127 -4.1590285 -4.1496677 -4.151288 -4.169013 -4.1920815 -4.2013988 -4.1890469 -4.168921 -4.1583729 -4.1670594 -4.1866169 -4.2158084 -4.2485504][-4.1856537 -4.1791015 -4.169066 -4.1596279 -4.1536446 -4.1602707 -4.1736007 -4.1762829 -4.1566696 -4.1272993 -4.111866 -4.1205521 -4.1432133 -4.1744008 -4.2062073]]...]
INFO - root - 2017-12-05 11:29:45.213627: step 3010, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 82h:01m:26s remains)
INFO - root - 2017-12-05 11:29:54.357212: step 3020, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 85h:54m:07s remains)
INFO - root - 2017-12-05 11:30:03.526053: step 3030, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 0.998 sec/batch; 91h:22m:26s remains)
INFO - root - 2017-12-05 11:30:12.680979: step 3040, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 81h:20m:13s remains)
INFO - root - 2017-12-05 11:30:21.588080: step 3050, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 84h:27m:13s remains)
INFO - root - 2017-12-05 11:30:30.705601: step 3060, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 84h:14m:38s remains)
INFO - root - 2017-12-05 11:30:39.948352: step 3070, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 85h:11m:53s remains)
INFO - root - 2017-12-05 11:30:48.984043: step 3080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 82h:00m:12s remains)
INFO - root - 2017-12-05 11:30:57.972411: step 3090, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 81h:26m:27s remains)
INFO - root - 2017-12-05 11:31:07.039770: step 3100, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 82h:38m:53s remains)
2017-12-05 11:31:07.783193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3289933 -4.3341537 -4.3386664 -4.3376255 -4.3309588 -4.3213482 -4.31394 -4.3138723 -4.3215165 -4.3318715 -4.3409386 -4.3452296 -4.3395681 -4.3278661 -4.3177466][-4.3269811 -4.3275371 -4.3273 -4.3226581 -4.3141656 -4.301127 -4.2890491 -4.2854943 -4.2936 -4.3068948 -4.3199935 -4.3281674 -4.3204155 -4.2992454 -4.273777][-4.3288159 -4.3248663 -4.3198218 -4.3121061 -4.3002529 -4.2812872 -4.2613978 -4.2524366 -4.2612405 -4.2792807 -4.2965784 -4.3069263 -4.2985764 -4.2677836 -4.2264514][-4.3268819 -4.31822 -4.3069267 -4.2915683 -4.2721334 -4.2441654 -4.2134638 -4.1999989 -4.2129169 -4.2390323 -4.2650833 -4.2865281 -4.288238 -4.2592092 -4.2125216][-4.3209987 -4.3057256 -4.2839885 -4.2556453 -4.2251935 -4.1864581 -4.1465344 -4.1328821 -4.1544857 -4.1889296 -4.2275538 -4.2659125 -4.2847247 -4.2720013 -4.2393951][-4.3132753 -4.2896323 -4.2542806 -4.2098145 -4.1662073 -4.1144676 -4.0644541 -4.0528374 -4.0859289 -4.1342878 -4.1895237 -4.2424235 -4.2696376 -4.2657285 -4.2464576][-4.3092551 -4.2800894 -4.2338214 -4.1706147 -4.1036892 -4.03003 -3.9669683 -3.9567037 -4.0038261 -4.0748377 -4.1515636 -4.2169046 -4.2473745 -4.2420859 -4.2234359][-4.3133612 -4.2848558 -4.2344289 -4.1571736 -4.0681887 -3.9754159 -3.904474 -3.8926377 -3.9506416 -4.0401349 -4.1295786 -4.1961479 -4.2184353 -4.2017226 -4.1723547][-4.3206072 -4.2940235 -4.24547 -4.1653647 -4.0674319 -3.9776464 -3.9163 -3.9088669 -3.968215 -4.059391 -4.1436944 -4.1985068 -4.203537 -4.1639175 -4.1114135][-4.3303337 -4.3059268 -4.2610784 -4.1874442 -4.0974908 -4.0231457 -3.9814341 -3.9840183 -4.0357585 -4.1124215 -4.1825795 -4.2261744 -4.2209654 -4.1713724 -4.1059947][-4.3412104 -4.3204865 -4.2816248 -4.2226238 -4.1509485 -4.0927715 -4.0684657 -4.0796008 -4.1246305 -4.18616 -4.2408447 -4.2728462 -4.26575 -4.224761 -4.1687045][-4.3488507 -4.333777 -4.3054667 -4.2665277 -4.2191362 -4.1825132 -4.1719809 -4.1852312 -4.2202187 -4.2655878 -4.3045025 -4.3248072 -4.3181014 -4.2892475 -4.2498174][-4.3533111 -4.3456216 -4.3303866 -4.3106446 -4.2870641 -4.2716913 -4.271821 -4.2850714 -4.3088441 -4.3363681 -4.35768 -4.3660903 -4.3601136 -4.3441343 -4.3229542][-4.3548517 -4.3534417 -4.3490052 -4.342679 -4.3346987 -4.332356 -4.338027 -4.3503804 -4.3643169 -4.3769531 -4.3834443 -4.3822594 -4.3767018 -4.3705087 -4.3652191][-4.3539915 -4.3570333 -4.3599505 -4.3616095 -4.3609047 -4.3627667 -4.3684959 -4.3772454 -4.3841782 -4.3885579 -4.3866992 -4.3791108 -4.3715582 -4.3695846 -4.373085]]...]
INFO - root - 2017-12-05 11:31:16.813989: step 3110, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 81h:33m:58s remains)
INFO - root - 2017-12-05 11:31:26.033491: step 3120, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 87h:57m:25s remains)
INFO - root - 2017-12-05 11:31:35.124163: step 3130, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 82h:23m:12s remains)
INFO - root - 2017-12-05 11:31:44.152809: step 3140, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 82h:13m:32s remains)
INFO - root - 2017-12-05 11:31:53.244231: step 3150, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.914 sec/batch; 83h:34m:41s remains)
INFO - root - 2017-12-05 11:32:02.427589: step 3160, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 84h:19m:38s remains)
INFO - root - 2017-12-05 11:32:11.508096: step 3170, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 82h:24m:22s remains)
INFO - root - 2017-12-05 11:32:20.589894: step 3180, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 85h:03m:57s remains)
INFO - root - 2017-12-05 11:32:29.628180: step 3190, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 82h:38m:01s remains)
INFO - root - 2017-12-05 11:32:38.674445: step 3200, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 82h:06m:59s remains)
2017-12-05 11:32:39.618772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054986 -4.3004704 -4.2932787 -4.2880468 -4.2836709 -4.2797947 -4.2780552 -4.2794986 -4.2790251 -4.2801137 -4.2840328 -4.2860231 -4.2890129 -4.293191 -4.2952838][-4.2837491 -4.2729039 -4.2576065 -4.2421846 -4.2256742 -4.215517 -4.2115173 -4.2152438 -4.2184839 -4.222724 -4.2298274 -4.2322674 -4.2374229 -4.2409964 -4.2391858][-4.2525482 -4.232955 -4.207634 -4.1764369 -4.1429753 -4.1249118 -4.1213069 -4.1312423 -4.1385617 -4.1440735 -4.1543212 -4.162 -4.1787057 -4.190012 -4.1859856][-4.220295 -4.1885509 -4.15099 -4.1037836 -4.0507674 -4.0215659 -4.0180774 -4.0329933 -4.0429368 -4.0515909 -4.0712705 -4.0933695 -4.1273279 -4.1548624 -4.1576142][-4.1980362 -4.1561637 -4.1061897 -4.0391121 -3.959764 -3.9034767 -3.8872256 -3.9043751 -3.9275062 -3.9528346 -3.9924333 -4.0348845 -4.0864425 -4.1309915 -4.144526][-4.1958494 -4.1502247 -4.0937767 -4.0085745 -3.8987875 -3.8073928 -3.7714262 -3.7896605 -3.8315327 -3.87959 -3.9403586 -3.9999044 -4.0622573 -4.1140637 -4.1333189][-4.2069793 -4.1671453 -4.116241 -4.03151 -3.9107604 -3.8051929 -3.7682989 -3.8012443 -3.862201 -3.9165149 -3.9757121 -4.0306516 -4.0776739 -4.11333 -4.1233506][-4.2166753 -4.1825528 -4.1440983 -4.073627 -3.962429 -3.8664944 -3.8395452 -3.8874681 -3.9618773 -4.0159874 -4.0617146 -4.0998044 -4.120676 -4.1293187 -4.1268363][-4.220459 -4.1871314 -4.1566038 -4.103507 -4.0172577 -3.9449472 -3.925559 -3.9786038 -4.0522666 -4.0964742 -4.1303577 -4.1558032 -4.1577973 -4.1432371 -4.127995][-4.222301 -4.1877232 -4.1620107 -4.1267958 -4.0686746 -4.019608 -4.0060692 -4.0551395 -4.1173606 -4.1497378 -4.1736345 -4.1885734 -4.1782374 -4.152709 -4.1287069][-4.2245855 -4.1943069 -4.1745872 -4.1506648 -4.1084986 -4.0722237 -4.0599089 -4.1003995 -4.14582 -4.1698608 -4.1931 -4.2068043 -4.1911869 -4.1610222 -4.1355848][-4.2275729 -4.2044396 -4.1883774 -4.1708908 -4.1409559 -4.1182637 -4.1091614 -4.1375093 -4.1665783 -4.1798105 -4.1981506 -4.2092962 -4.1926723 -4.1643157 -4.1453094][-4.244699 -4.2254767 -4.2109375 -4.1981416 -4.178791 -4.16824 -4.1649876 -4.1846414 -4.2009807 -4.2041755 -4.2120171 -4.2184086 -4.202354 -4.1796012 -4.169889][-4.2729273 -4.2570262 -4.2436419 -4.2322111 -4.21835 -4.21407 -4.2150106 -4.2282367 -4.2366252 -4.2337279 -4.2377148 -4.243228 -4.2311788 -4.2175851 -4.2183623][-4.3005109 -4.2871618 -4.2747521 -4.26314 -4.2514782 -4.2485237 -4.2514849 -4.2608523 -4.2661538 -4.2629547 -4.267211 -4.2732553 -4.2678261 -4.2622247 -4.2679677]]...]
INFO - root - 2017-12-05 11:32:48.827150: step 3210, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 82h:53m:43s remains)
INFO - root - 2017-12-05 11:32:57.762770: step 3220, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 84h:14m:39s remains)
INFO - root - 2017-12-05 11:33:06.894601: step 3230, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 84h:19m:53s remains)
INFO - root - 2017-12-05 11:33:15.993992: step 3240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 84h:20m:26s remains)
INFO - root - 2017-12-05 11:33:24.942450: step 3250, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.797 sec/batch; 72h:55m:53s remains)
INFO - root - 2017-12-05 11:33:34.068750: step 3260, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 82h:59m:52s remains)
INFO - root - 2017-12-05 11:33:43.135744: step 3270, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 81h:40m:42s remains)
INFO - root - 2017-12-05 11:33:52.018404: step 3280, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 75h:20m:17s remains)
INFO - root - 2017-12-05 11:34:01.174456: step 3290, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.936 sec/batch; 85h:38m:18s remains)
INFO - root - 2017-12-05 11:34:10.252071: step 3300, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 86h:35m:14s remains)
2017-12-05 11:34:11.063239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.217958 -4.2266564 -4.2557244 -4.2866917 -4.30517 -4.3127756 -4.3125725 -4.3048711 -4.2897191 -4.2701797 -4.2495174 -4.2315245 -4.2201214 -4.230906 -4.2488437][-4.2069259 -4.2216077 -4.2555275 -4.2890782 -4.30971 -4.3186164 -4.32247 -4.321568 -4.3182678 -4.3136358 -4.3062129 -4.2991757 -4.2917571 -4.2908096 -4.2871327][-4.2004724 -4.2168241 -4.2478385 -4.2746987 -4.2911186 -4.2972956 -4.2997494 -4.2988019 -4.3012948 -4.3087912 -4.3145194 -4.3190293 -4.3178563 -4.3144231 -4.3020935][-4.2278013 -4.2372088 -4.2522106 -4.2619004 -4.2636595 -4.2570529 -4.2467489 -4.2336173 -4.2314205 -4.2474689 -4.2693915 -4.2916813 -4.3068976 -4.3146091 -4.308403][-4.2602286 -4.2604656 -4.2583084 -4.2509246 -4.2351456 -4.2074351 -4.169714 -4.1298776 -4.1150827 -4.1425352 -4.1872063 -4.2347951 -4.2750111 -4.301127 -4.3059635][-4.2782955 -4.2742157 -4.2602677 -4.237546 -4.2032208 -4.1492186 -4.0763245 -4.0038829 -3.9765265 -4.0232363 -4.0981908 -4.1724896 -4.2359619 -4.2763128 -4.2893429][-4.2894583 -4.2893386 -4.2710381 -4.2363367 -4.1848974 -4.1060014 -3.9992588 -3.8938363 -3.8552985 -3.9222608 -4.0267119 -4.1240349 -4.2046685 -4.2535515 -4.2711749][-4.3018503 -4.3081441 -4.2920218 -4.2527037 -4.1924391 -4.1010437 -3.9788418 -3.859376 -3.8178596 -3.8941958 -4.0100222 -4.1144042 -4.1989279 -4.2463961 -4.2630281][-4.3224564 -4.3267956 -4.3102946 -4.27275 -4.2176228 -4.1354375 -4.0291052 -3.9321303 -3.9056628 -3.9727697 -4.0712647 -4.1586757 -4.2276921 -4.2636447 -4.275569][-4.3420725 -4.3402972 -4.3226757 -4.2908421 -4.2495456 -4.1914449 -4.1195717 -4.0623755 -4.0556383 -4.1059976 -4.1724844 -4.2299981 -4.2738032 -4.29617 -4.3044419][-4.354135 -4.347095 -4.3282948 -4.3018217 -4.2749476 -4.24201 -4.2045932 -4.1829166 -4.1907187 -4.2236409 -4.2579889 -4.2839856 -4.3029923 -4.3136353 -4.3216348][-4.3497038 -4.3382144 -4.3161907 -4.2933364 -4.2809753 -4.2713189 -4.2608175 -4.2625942 -4.277298 -4.294591 -4.3023596 -4.3020411 -4.3013272 -4.3050814 -4.31652][-4.3318253 -4.3178821 -4.2947149 -4.2757039 -4.2750077 -4.2819529 -4.2895494 -4.3039017 -4.3201561 -4.3268452 -4.3162451 -4.2951756 -4.2778716 -4.2767158 -4.2914648][-4.3078976 -4.2975039 -4.2785664 -4.2646041 -4.2702537 -4.2856064 -4.3016 -4.3198185 -4.3340473 -4.3347063 -4.3158145 -4.2836089 -4.2558837 -4.2500467 -4.2657552][-4.2897491 -4.2884083 -4.2792964 -4.2741232 -4.2841554 -4.300662 -4.3159151 -4.3296933 -4.3375444 -4.3333774 -4.3134804 -4.2818451 -4.2547894 -4.2478142 -4.2616014]]...]
INFO - root - 2017-12-05 11:34:20.103488: step 3310, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 85h:39m:25s remains)
INFO - root - 2017-12-05 11:34:29.307772: step 3320, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.901 sec/batch; 82h:24m:02s remains)
INFO - root - 2017-12-05 11:34:38.352663: step 3330, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 85h:14m:29s remains)
INFO - root - 2017-12-05 11:34:47.451796: step 3340, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 85h:28m:44s remains)
INFO - root - 2017-12-05 11:34:56.541287: step 3350, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 83h:47m:32s remains)
INFO - root - 2017-12-05 11:35:05.715888: step 3360, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 85h:21m:32s remains)
INFO - root - 2017-12-05 11:35:14.780828: step 3370, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.813 sec/batch; 74h:21m:25s remains)
INFO - root - 2017-12-05 11:35:23.641172: step 3380, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 83h:07m:22s remains)
INFO - root - 2017-12-05 11:35:32.801307: step 3390, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 82h:21m:17s remains)
INFO - root - 2017-12-05 11:35:41.870256: step 3400, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 81h:30m:01s remains)
2017-12-05 11:35:42.633680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2227516 -4.2510223 -4.2758293 -4.2927494 -4.2953391 -4.2881713 -4.2735152 -4.2603717 -4.2572575 -4.2644281 -4.2805595 -4.2872829 -4.2619276 -4.2070775 -4.1674352][-4.18519 -4.215353 -4.2454696 -4.26919 -4.2714639 -4.2595658 -4.2396889 -4.2197886 -4.2171564 -4.2292194 -4.2533298 -4.2621441 -4.2247143 -4.1456747 -4.0915833][-4.1372123 -4.1710854 -4.2092733 -4.2404509 -4.24429 -4.2262506 -4.1954947 -4.1684585 -4.1641884 -4.1838517 -4.2178164 -4.235086 -4.1984282 -4.1134677 -4.0581236][-4.0782695 -4.1216383 -4.1703615 -4.2066655 -4.2097621 -4.1828589 -4.1393323 -4.1070166 -4.1085348 -4.1442957 -4.1908212 -4.2161307 -4.1896496 -4.1197395 -4.0760508][-4.0123839 -4.0684481 -4.1312294 -4.1693392 -4.1637416 -4.1201668 -4.06579 -4.040688 -4.0645394 -4.1247249 -4.1746268 -4.2019205 -4.1888242 -4.1445179 -4.1191769][-3.9824076 -4.0485134 -4.1140451 -4.1409941 -4.1160645 -4.0448885 -3.9756923 -3.9697061 -4.0295668 -4.1099744 -4.1577435 -4.1867332 -4.1912832 -4.1746764 -4.1664448][-4.0242281 -4.0823417 -4.1258821 -4.1227684 -4.0644026 -3.9566071 -3.8715396 -3.8939157 -3.9920514 -4.0864649 -4.1343856 -4.1676984 -4.1886511 -4.1915197 -4.1942611][-4.0886579 -4.1284394 -4.1404591 -4.103332 -4.0136089 -3.88447 -3.7976506 -3.8498931 -3.9734113 -4.0696416 -4.1166778 -4.151732 -4.1818714 -4.1919532 -4.1947527][-4.1354008 -4.1612535 -4.1559153 -4.1045909 -4.0100322 -3.8977866 -3.839421 -3.8985934 -4.0059295 -4.0834 -4.1259351 -4.1618738 -4.1926045 -4.1984043 -4.19581][-4.1705594 -4.1857071 -4.178287 -4.1390195 -4.0650768 -3.9940975 -3.9681985 -4.011652 -4.0769587 -4.1267672 -4.1629071 -4.1973672 -4.2188249 -4.2139096 -4.2068324][-4.2055697 -4.2096162 -4.2018309 -4.17984 -4.1352191 -4.0988212 -4.0907178 -4.1150002 -4.1493707 -4.1795864 -4.207984 -4.2324281 -4.2386317 -4.2251029 -4.2189412][-4.2460446 -4.2406907 -4.2287726 -4.2155008 -4.1926646 -4.1774626 -4.1795192 -4.19364 -4.2141824 -4.2371664 -4.2586231 -4.2721434 -4.2678423 -4.2529321 -4.2472734][-4.2711129 -4.2639556 -4.2525392 -4.2439704 -4.2339187 -4.2312903 -4.2373657 -4.2469873 -4.2613859 -4.2814012 -4.2983122 -4.3036895 -4.2949085 -4.2788815 -4.2708163][-4.2821727 -4.2768316 -4.2694035 -4.2645526 -4.2609477 -4.2636042 -4.2708173 -4.2800589 -4.2909522 -4.3046403 -4.3149662 -4.3158431 -4.3053474 -4.289989 -4.2825642][-4.2936454 -4.2919869 -4.2893372 -4.2858553 -4.2815619 -4.28227 -4.2892547 -4.2988815 -4.3076005 -4.3151684 -4.3198342 -4.3192844 -4.3101006 -4.297965 -4.29282]]...]
INFO - root - 2017-12-05 11:35:51.668003: step 3410, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.915 sec/batch; 83h:40m:28s remains)
INFO - root - 2017-12-05 11:36:00.870053: step 3420, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 85h:46m:11s remains)
INFO - root - 2017-12-05 11:36:10.049693: step 3430, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 84h:47m:46s remains)
INFO - root - 2017-12-05 11:36:18.984380: step 3440, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 83h:10m:12s remains)
INFO - root - 2017-12-05 11:36:27.979623: step 3450, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 79h:53m:32s remains)
INFO - root - 2017-12-05 11:36:37.148538: step 3460, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.903 sec/batch; 82h:32m:38s remains)
INFO - root - 2017-12-05 11:36:46.196991: step 3470, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 83h:17m:49s remains)
INFO - root - 2017-12-05 11:36:55.268630: step 3480, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:15m:19s remains)
INFO - root - 2017-12-05 11:37:04.274303: step 3490, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 82h:11m:57s remains)
INFO - root - 2017-12-05 11:37:13.632048: step 3500, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 84h:49m:54s remains)
2017-12-05 11:37:14.485725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1350164 -4.1543417 -4.1573725 -4.1593504 -4.1689286 -4.1785207 -4.1860332 -4.1935697 -4.1918635 -4.1908388 -4.2021327 -4.2004833 -4.1857724 -4.1746454 -4.160677][-4.1187267 -4.1314859 -4.13359 -4.1380429 -4.154181 -4.1642981 -4.1674595 -4.1733069 -4.1709652 -4.1697779 -4.1856923 -4.1862106 -4.1688552 -4.1558766 -4.1408405][-4.1020441 -4.1125569 -4.1185985 -4.1269469 -4.1495104 -4.1581383 -4.1530356 -4.1541452 -4.15155 -4.1506739 -4.1693769 -4.1738539 -4.15893 -4.1460929 -4.1277714][-4.0995831 -4.1137509 -4.1267452 -4.1382985 -4.1603532 -4.1623745 -4.1469455 -4.1399059 -4.137105 -4.1348934 -4.1527929 -4.1609015 -4.1486335 -4.1301675 -4.10742][-4.1244078 -4.1412387 -4.1568861 -4.1643481 -4.176476 -4.1712751 -4.1457672 -4.1315422 -4.129673 -4.1296711 -4.1480269 -4.1563506 -4.1438551 -4.11967 -4.0947852][-4.1556883 -4.1649628 -4.1735306 -4.1750116 -4.1746745 -4.162303 -4.1292267 -4.11856 -4.1272469 -4.1352453 -4.1573 -4.168108 -4.1594582 -4.1338773 -4.1083379][-4.1534162 -4.1563816 -4.1586065 -4.1561131 -4.14656 -4.1275344 -4.0933428 -4.0941415 -4.11807 -4.1354337 -4.1624584 -4.1804929 -4.1805649 -4.1549954 -4.1229086][-4.139441 -4.1414785 -4.1407886 -4.1381917 -4.12489 -4.1027694 -4.0734158 -4.0800266 -4.1075583 -4.1238046 -4.147543 -4.1678672 -4.1734667 -4.1483841 -4.1153903][-4.1305575 -4.1372995 -4.1423264 -4.1472573 -4.1431479 -4.1300912 -4.1100345 -4.1108308 -4.1234365 -4.1268291 -4.1349216 -4.1469603 -4.1512408 -4.1317348 -4.1091714][-4.1310577 -4.1455669 -4.16194 -4.1824188 -4.1938982 -4.1911798 -4.1795783 -4.1748209 -4.1739626 -4.1676784 -4.1622462 -4.1628013 -4.1623654 -4.1483808 -4.1371403][-4.1402082 -4.1631236 -4.1906037 -4.2199411 -4.2374115 -4.2410331 -4.2365489 -4.2310147 -4.2265615 -4.221622 -4.2152572 -4.2113237 -4.2076678 -4.1966286 -4.1886253][-4.1469159 -4.1750693 -4.2039165 -4.2326365 -4.2519169 -4.26132 -4.2654238 -4.2645149 -4.2624469 -4.2621956 -4.25774 -4.2534719 -4.2467694 -4.2370043 -4.230577][-4.1628466 -4.1952343 -4.2234292 -4.2490559 -4.2675009 -4.279182 -4.2871227 -4.289341 -4.2910147 -4.2953162 -4.2935834 -4.2897711 -4.2824512 -4.2740116 -4.266223][-4.18604 -4.2188044 -4.2428231 -4.26337 -4.2793856 -4.2915974 -4.2993383 -4.2990336 -4.2990065 -4.3037786 -4.30502 -4.3029752 -4.298595 -4.2944503 -4.2899804][-4.1877394 -4.2162151 -4.2364149 -4.2555456 -4.2725549 -4.285295 -4.289753 -4.2838016 -4.2798643 -4.2847085 -4.2894735 -4.2918248 -4.2919245 -4.2920513 -4.2910223]]...]
INFO - root - 2017-12-05 11:37:23.640197: step 3510, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 83h:49m:10s remains)
INFO - root - 2017-12-05 11:37:32.710229: step 3520, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 81h:25m:54s remains)
INFO - root - 2017-12-05 11:37:41.798757: step 3530, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 84h:55m:56s remains)
INFO - root - 2017-12-05 11:37:51.028563: step 3540, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 83h:30m:40s remains)
INFO - root - 2017-12-05 11:38:00.216519: step 3550, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 84h:49m:35s remains)
INFO - root - 2017-12-05 11:38:09.241371: step 3560, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 83h:47m:59s remains)
INFO - root - 2017-12-05 11:38:18.182785: step 3570, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 82h:42m:57s remains)
INFO - root - 2017-12-05 11:38:27.118497: step 3580, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 81h:34m:06s remains)
INFO - root - 2017-12-05 11:38:36.195773: step 3590, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 83h:04m:03s remains)
INFO - root - 2017-12-05 11:38:45.368490: step 3600, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 83h:44m:00s remains)
2017-12-05 11:38:46.196586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2618818 -4.3052793 -4.3222256 -4.3178873 -4.3053756 -4.2938447 -4.2948956 -4.2979541 -4.295959 -4.2839355 -4.2573619 -4.2258205 -4.2060418 -4.2019815 -4.2251959][-4.2128849 -4.2726 -4.3063097 -4.3115621 -4.3007226 -4.2856669 -4.2865248 -4.2914948 -4.2909865 -4.2850375 -4.2658257 -4.244452 -4.2268009 -4.2131596 -4.2250504][-4.1735234 -4.239243 -4.2834654 -4.2987914 -4.291585 -4.27438 -4.271934 -4.2780967 -4.2817483 -4.2829151 -4.2711344 -4.2640319 -4.2547951 -4.2386913 -4.237833][-4.1747613 -4.2314095 -4.271378 -4.2903337 -4.2850842 -4.2647166 -4.2549543 -4.2560372 -4.2639861 -4.2765231 -4.2777123 -4.2840238 -4.2831974 -4.2698369 -4.2616725][-4.2007957 -4.2400966 -4.2677603 -4.2827477 -4.2766929 -4.2519674 -4.2304 -4.2252 -4.2384195 -4.2625408 -4.278193 -4.2921476 -4.2954445 -4.285048 -4.2727027][-4.2299318 -4.2543659 -4.2677784 -4.2724023 -4.2564297 -4.2215738 -4.1844387 -4.1729431 -4.200973 -4.2457457 -4.2781005 -4.29437 -4.295989 -4.2845974 -4.2692223][-4.2566762 -4.27333 -4.2782803 -4.2695894 -4.2362828 -4.1822791 -4.1234593 -4.09845 -4.147254 -4.2234864 -4.2727494 -4.2906694 -4.2873926 -4.2726593 -4.2578654][-4.2724891 -4.2885857 -4.2915745 -4.2731214 -4.2244864 -4.1494684 -4.0668097 -4.0244064 -4.083292 -4.1821661 -4.2434859 -4.2677379 -4.2684255 -4.2598052 -4.2471485][-4.290246 -4.3019519 -4.3042636 -4.2830338 -4.2298317 -4.1488647 -4.0603409 -4.0065069 -4.044734 -4.1364393 -4.2024722 -4.2328768 -4.2403607 -4.2348766 -4.2203364][-4.3097973 -4.3155532 -4.3140635 -4.2949734 -4.2496185 -4.1837416 -4.1129289 -4.0653734 -4.0793638 -4.1413808 -4.1949563 -4.2180595 -4.21912 -4.204771 -4.1804371][-4.31694 -4.31771 -4.3148928 -4.3023958 -4.2719584 -4.2298956 -4.1863832 -4.154058 -4.1495152 -4.1781983 -4.2115488 -4.2228508 -4.2137513 -4.1925206 -4.1668386][-4.308589 -4.3057032 -4.3015184 -4.295702 -4.2827406 -4.2669053 -4.2500172 -4.2332377 -4.220037 -4.2229605 -4.2391257 -4.2395587 -4.2246351 -4.2039351 -4.1869936][-4.28853 -4.279418 -4.2712603 -4.2673678 -4.2649083 -4.2662826 -4.2686963 -4.2669587 -4.2560239 -4.2490606 -4.2564607 -4.251596 -4.2380967 -4.2220035 -4.2089772][-4.2744021 -4.2594428 -4.2474027 -4.2421083 -4.2429481 -4.2484112 -4.2574449 -4.2654557 -4.2612433 -4.2531915 -4.2554426 -4.2484756 -4.2369089 -4.2267532 -4.2174664][-4.2786274 -4.264081 -4.2521057 -4.2457662 -4.246233 -4.2492394 -4.2557397 -4.26204 -4.2582884 -4.2491918 -4.2459559 -4.2373018 -4.2280111 -4.2243032 -4.2212605]]...]
INFO - root - 2017-12-05 11:38:55.133297: step 3610, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 79h:08m:24s remains)
INFO - root - 2017-12-05 11:39:04.307252: step 3620, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 86h:33m:44s remains)
INFO - root - 2017-12-05 11:39:13.520824: step 3630, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 84h:14m:36s remains)
INFO - root - 2017-12-05 11:39:22.544775: step 3640, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 82h:44m:09s remains)
INFO - root - 2017-12-05 11:39:31.575456: step 3650, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 81h:34m:22s remains)
INFO - root - 2017-12-05 11:39:40.511204: step 3660, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 81h:18m:55s remains)
INFO - root - 2017-12-05 11:39:49.570811: step 3670, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 83h:28m:54s remains)
INFO - root - 2017-12-05 11:39:58.884842: step 3680, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 81h:57m:41s remains)
INFO - root - 2017-12-05 11:40:07.999656: step 3690, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 84h:28m:32s remains)
INFO - root - 2017-12-05 11:40:17.021625: step 3700, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 82h:56m:05s remains)
2017-12-05 11:40:17.761203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516227 -4.252676 -4.2442865 -4.2215967 -4.1925254 -4.1698651 -4.1636233 -4.167388 -4.1716194 -4.1684742 -4.1743975 -4.1971726 -4.2078686 -4.1985421 -4.1943049][-4.2318482 -4.2399621 -4.2348866 -4.2125173 -4.1788893 -4.1576929 -4.1542592 -4.1564884 -4.1565208 -4.1452241 -4.1412 -4.1608887 -4.1733837 -4.1686354 -4.1686854][-4.2016182 -4.2172604 -4.2190108 -4.19737 -4.161809 -4.1437869 -4.1447053 -4.1471581 -4.139102 -4.1145668 -4.0982704 -4.1212068 -4.143177 -4.1480441 -4.1538424][-4.1635408 -4.1798067 -4.1860046 -4.1673508 -4.1374207 -4.1302857 -4.1405196 -4.1464095 -4.1394196 -4.1146936 -4.1003542 -4.1274843 -4.1521792 -4.1621847 -4.1647706][-4.1203322 -4.1296473 -4.1358891 -4.1220117 -4.1023498 -4.1063209 -4.1242466 -4.1388249 -4.1470904 -4.1418777 -4.1437163 -4.1710014 -4.1892824 -4.1914353 -4.1793489][-4.0882812 -4.0946264 -4.099853 -4.0920916 -4.0788689 -4.0833755 -4.0981774 -4.113091 -4.1305017 -4.1401825 -4.1549788 -4.1836524 -4.1964607 -4.1923513 -4.1706119][-4.0669665 -4.0693426 -4.0621467 -4.04702 -4.0310278 -4.0254498 -4.023365 -4.026494 -4.0454588 -4.0691171 -4.0996213 -4.1370506 -4.1541772 -4.1532211 -4.1348405][-4.0139022 -3.9954689 -3.9577577 -3.9160342 -3.8851857 -3.8710573 -3.8540974 -3.8434398 -3.8669477 -3.9135473 -3.9724171 -4.0299444 -4.0624619 -4.0748835 -4.0763388][-3.9861267 -3.9401817 -3.8718398 -3.8002036 -3.7502053 -3.7298315 -3.7038465 -3.6876612 -3.7222521 -3.7883952 -3.8638997 -3.9321117 -3.9735441 -3.9977241 -4.0231209][-4.0560451 -4.0111923 -3.9519074 -3.8939703 -3.8563476 -3.8426132 -3.825774 -3.816824 -3.8373094 -3.8753548 -3.9252539 -3.9731941 -4.002295 -4.0198402 -4.0455837][-4.1616921 -4.1330137 -4.097775 -4.0642667 -4.0453262 -4.039536 -4.0342331 -4.0309796 -4.0358129 -4.045506 -4.0672889 -4.0925994 -4.1065378 -4.1129503 -4.1262188][-4.2412791 -4.222641 -4.2012873 -4.1815987 -4.170526 -4.1668558 -4.1680245 -4.1715264 -4.173625 -4.1735196 -4.1806464 -4.1914062 -4.1974192 -4.1980796 -4.201273][-4.2860055 -4.2712846 -4.2568164 -4.2440329 -4.2370343 -4.2360897 -4.2397118 -4.2454576 -4.2460608 -4.2435851 -4.2437458 -4.2464271 -4.248363 -4.24824 -4.2502179][-4.30254 -4.2907004 -4.2792196 -4.2688937 -4.2619061 -4.260355 -4.2632542 -4.2678189 -4.269783 -4.2700229 -4.2704215 -4.2706041 -4.2714596 -4.2727513 -4.2758174][-4.3079925 -4.2994823 -4.289608 -4.2805953 -4.2736425 -4.2717438 -4.2741728 -4.2768803 -4.278017 -4.2779183 -4.2774649 -4.2768931 -4.2778664 -4.28032 -4.283874]]...]
INFO - root - 2017-12-05 11:40:26.801601: step 3710, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 82h:22m:27s remains)
INFO - root - 2017-12-05 11:40:35.935891: step 3720, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 80h:56m:27s remains)
INFO - root - 2017-12-05 11:40:45.037483: step 3730, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 80h:50m:16s remains)
INFO - root - 2017-12-05 11:40:54.100574: step 3740, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 83h:40m:55s remains)
INFO - root - 2017-12-05 11:41:03.043858: step 3750, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 80h:18m:14s remains)
INFO - root - 2017-12-05 11:41:12.143927: step 3760, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 82h:23m:17s remains)
INFO - root - 2017-12-05 11:41:21.161687: step 3770, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 81h:53m:27s remains)
INFO - root - 2017-12-05 11:41:30.108414: step 3780, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 79h:13m:00s remains)
INFO - root - 2017-12-05 11:41:39.306237: step 3790, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 82h:36m:12s remains)
INFO - root - 2017-12-05 11:41:48.417248: step 3800, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 83h:17m:52s remains)
2017-12-05 11:41:49.281233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2909122 -4.2720122 -4.2653584 -4.2676477 -4.2712212 -4.2731895 -4.2772813 -4.2820125 -4.280014 -4.2745333 -4.2728672 -4.2741065 -4.2787919 -4.2862968 -4.29411][-4.2665043 -4.242084 -4.2315078 -4.2306671 -4.2366052 -4.2432671 -4.251164 -4.2571692 -4.25474 -4.2483253 -4.24871 -4.2525492 -4.2574997 -4.2653441 -4.2756414][-4.2363186 -4.1999044 -4.1789489 -4.1737347 -4.1860995 -4.2024546 -4.2179713 -4.2288857 -4.2291994 -4.2256269 -4.2280083 -4.2339439 -4.2408638 -4.251246 -4.2643442][-4.2168536 -4.1667066 -4.1319189 -4.1161094 -4.1293039 -4.1548724 -4.1765547 -4.1964984 -4.2112112 -4.2190328 -4.2258921 -4.23512 -4.2454929 -4.2550793 -4.2671514][-4.211278 -4.1485186 -4.0990443 -4.0639033 -4.061161 -4.0800676 -4.1009483 -4.1314244 -4.1664953 -4.1939969 -4.213634 -4.2308893 -4.2471843 -4.258812 -4.2712603][-4.2140169 -4.1436467 -4.0822706 -4.0250249 -3.9918168 -3.9838367 -3.9819236 -4.0077047 -4.0710077 -4.1299481 -4.167892 -4.197258 -4.2233276 -4.2429428 -4.26212][-4.21764 -4.146956 -4.0799069 -4.0079865 -3.945715 -3.8911867 -3.8277402 -3.8136368 -3.9045997 -4.015347 -4.0906463 -4.1409802 -4.18037 -4.2113247 -4.2432895][-4.2294784 -4.1683664 -4.1074414 -4.039711 -3.9769418 -3.8982449 -3.775882 -3.6863532 -3.7582676 -3.8900695 -3.9964185 -4.0739427 -4.1320205 -4.1792359 -4.2246556][-4.2461267 -4.1980944 -4.1544781 -4.1127429 -4.0804338 -4.0266066 -3.9273257 -3.8406854 -3.8527315 -3.931808 -4.0188613 -4.0913577 -4.1457367 -4.1900587 -4.231184][-4.2568936 -4.21684 -4.184896 -4.1626821 -4.1554604 -4.1358218 -4.08809 -4.0457096 -4.0373821 -4.0694652 -4.1192269 -4.1661844 -4.2014465 -4.2319064 -4.2563796][-4.2677832 -4.2366133 -4.2104125 -4.1981611 -4.2040467 -4.2030392 -4.1882544 -4.1828723 -4.1798 -4.192173 -4.2193727 -4.2467842 -4.2691593 -4.2871175 -4.2947855][-4.2734571 -4.2516866 -4.2298107 -4.2188005 -4.226975 -4.2341375 -4.2359195 -4.247354 -4.2538457 -4.2667952 -4.2881131 -4.3074074 -4.3201418 -4.3293777 -4.3276353][-4.2922773 -4.282063 -4.2685051 -4.257957 -4.2620306 -4.267344 -4.2737875 -4.28687 -4.2945242 -4.3044028 -4.3217335 -4.3377442 -4.3456359 -4.3482962 -4.34203][-4.3165359 -4.315793 -4.31337 -4.3089976 -4.3084264 -4.3104253 -4.3154287 -4.3228717 -4.326654 -4.3305783 -4.3400187 -4.349009 -4.3523445 -4.3513145 -4.3457122][-4.330009 -4.3297725 -4.3316731 -4.3305907 -4.3293257 -4.3284173 -4.3294225 -4.3314362 -4.3332405 -4.33558 -4.3400226 -4.3440695 -4.3450427 -4.3446226 -4.3422346]]...]
INFO - root - 2017-12-05 11:41:58.276950: step 3810, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 86h:06m:13s remains)
INFO - root - 2017-12-05 11:42:07.312248: step 3820, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 83h:55m:15s remains)
INFO - root - 2017-12-05 11:42:16.431428: step 3830, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 81h:58m:13s remains)
INFO - root - 2017-12-05 11:42:25.406787: step 3840, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 77h:41m:45s remains)
INFO - root - 2017-12-05 11:42:34.466299: step 3850, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 84h:31m:56s remains)
INFO - root - 2017-12-05 11:42:43.535078: step 3860, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 81h:37m:19s remains)
INFO - root - 2017-12-05 11:42:52.654150: step 3870, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 84h:48m:31s remains)
INFO - root - 2017-12-05 11:43:01.765338: step 3880, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 84h:17m:08s remains)
INFO - root - 2017-12-05 11:43:10.907027: step 3890, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 82h:44m:32s remains)
INFO - root - 2017-12-05 11:43:20.088378: step 3900, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 86h:02m:10s remains)
2017-12-05 11:43:20.863491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1901183 -4.1375542 -4.0983224 -4.0775447 -4.0912676 -4.13616 -4.1854019 -4.21746 -4.2120495 -4.1891322 -4.1729774 -4.1656342 -4.1600895 -4.1544738 -4.1599264][-4.1850667 -4.1240788 -4.0740604 -4.0453076 -4.0702786 -4.1347127 -4.18866 -4.2143545 -4.2053695 -4.1789017 -4.1610255 -4.1544862 -4.1580243 -4.1618905 -4.1694679][-4.1789665 -4.1101995 -4.0487614 -4.0146956 -4.05086 -4.1344061 -4.1905513 -4.2127671 -4.204041 -4.1792936 -4.1631694 -4.1582742 -4.1624312 -4.1668105 -4.1734447][-4.1767125 -4.1073303 -4.0382681 -3.9984951 -4.0404053 -4.1364393 -4.1960196 -4.2186537 -4.2110763 -4.187552 -4.1725817 -4.1673107 -4.167151 -4.16725 -4.1754813][-4.1818171 -4.1164107 -4.0460839 -3.9961622 -4.0341158 -4.1354795 -4.2015791 -4.22874 -4.2231808 -4.1990795 -4.1833253 -4.1756215 -4.173039 -4.1719327 -4.1829967][-4.1962919 -4.13763 -4.0704026 -4.0126314 -4.0391679 -4.1330562 -4.2003584 -4.2300448 -4.2275648 -4.2060804 -4.1910248 -4.1821823 -4.1815767 -4.1830397 -4.1952457][-4.2116523 -4.1636629 -4.1045961 -4.0472703 -4.0598445 -4.135777 -4.1962662 -4.2241478 -4.2261415 -4.20767 -4.1879992 -4.1764669 -4.1761637 -4.1809883 -4.1933208][-4.2204461 -4.18048 -4.1324897 -4.083765 -4.0909295 -4.149828 -4.2003183 -4.2233238 -4.2249842 -4.2062435 -4.1805844 -4.1633377 -4.1585875 -4.1634755 -4.1763287][-4.2233691 -4.1896329 -4.1523747 -4.1195784 -4.1294222 -4.1766782 -4.2165904 -4.2322369 -4.2316532 -4.2136507 -4.188344 -4.1714416 -4.1615047 -4.1618876 -4.1718264][-4.2220068 -4.1918163 -4.1623921 -4.1429396 -4.1579304 -4.1996241 -4.2338786 -4.2444549 -4.240705 -4.22436 -4.2048588 -4.1927295 -4.1822739 -4.1776519 -4.1843162][-4.2121158 -4.1846089 -4.1594539 -4.1473327 -4.1644378 -4.2044969 -4.2397938 -4.2511778 -4.2481732 -4.2348337 -4.2213516 -4.2142558 -4.2056932 -4.195405 -4.1992269][-4.2052937 -4.1809468 -4.1584044 -4.1498427 -4.1668978 -4.2027678 -4.2377791 -4.2525487 -4.2519336 -4.2427564 -4.2351832 -4.2329273 -4.2266045 -4.217557 -4.2207971][-4.2057209 -4.1855645 -4.1663933 -4.1594448 -4.1735225 -4.2018642 -4.2343054 -4.2488704 -4.2503028 -4.2446036 -4.2420011 -4.2435403 -4.2391911 -4.2334833 -4.2421031][-4.2103982 -4.1966372 -4.1824346 -4.1772237 -4.1887403 -4.2117987 -4.239255 -4.250627 -4.2497091 -4.2434506 -4.2429838 -4.2481771 -4.249249 -4.2482505 -4.2583313][-4.2206788 -4.2128048 -4.2031603 -4.1995044 -4.20955 -4.2290988 -4.2493434 -4.2553968 -4.2495503 -4.2397285 -4.2358561 -4.2416325 -4.24921 -4.2552238 -4.2667928]]...]
INFO - root - 2017-12-05 11:43:30.054743: step 3910, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 82h:28m:37s remains)
INFO - root - 2017-12-05 11:43:39.174405: step 3920, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 81h:34m:59s remains)
INFO - root - 2017-12-05 11:43:48.212208: step 3930, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 82h:52m:42s remains)
INFO - root - 2017-12-05 11:43:57.182313: step 3940, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 80h:43m:47s remains)
INFO - root - 2017-12-05 11:44:06.249611: step 3950, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 84h:15m:58s remains)
INFO - root - 2017-12-05 11:44:15.268042: step 3960, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 84h:33m:30s remains)
INFO - root - 2017-12-05 11:44:24.254282: step 3970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 83h:16m:27s remains)
INFO - root - 2017-12-05 11:44:33.450110: step 3980, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.888 sec/batch; 81h:03m:52s remains)
INFO - root - 2017-12-05 11:44:42.382894: step 3990, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 80h:34m:30s remains)
INFO - root - 2017-12-05 11:44:51.463669: step 4000, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 81h:25m:51s remains)
2017-12-05 11:44:52.269751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1935697 -4.2042789 -4.2179322 -4.2355361 -4.2556024 -4.280127 -4.298224 -4.3036671 -4.3050008 -4.3008537 -4.2877078 -4.2778006 -4.2720652 -4.2684379 -4.2733493][-4.1857467 -4.20523 -4.2228365 -4.2350874 -4.2451181 -4.25966 -4.2738724 -4.2840972 -4.2930412 -4.2931156 -4.2796707 -4.27107 -4.264595 -4.2585549 -4.2611051][-4.1751404 -4.1945395 -4.21099 -4.2172518 -4.2152748 -4.2180567 -4.2299337 -4.2490683 -4.2701416 -4.2789497 -4.270565 -4.2643256 -4.2568283 -4.2450819 -4.2441111][-4.1593065 -4.1741581 -4.1904068 -4.1951385 -4.1885319 -4.1817493 -4.1897855 -4.2154312 -4.2455764 -4.2616963 -4.2589455 -4.2541275 -4.2464223 -4.2305341 -4.2230339][-4.1460919 -4.1564379 -4.17529 -4.1836443 -4.175148 -4.1596127 -4.1624441 -4.1900406 -4.223011 -4.2428656 -4.2442884 -4.2392945 -4.2337184 -4.2186718 -4.2052531][-4.1482611 -4.1548305 -4.173389 -4.1818 -4.1692863 -4.143312 -4.1358032 -4.1607165 -4.19551 -4.2187066 -4.2268558 -4.2272434 -4.2285185 -4.2196851 -4.2028079][-4.1640053 -4.1682692 -4.1827192 -4.1845155 -4.1630578 -4.1195307 -4.0912719 -4.1106868 -4.1536918 -4.186131 -4.2070913 -4.2215829 -4.2329712 -4.2325444 -4.2204304][-4.185102 -4.1891308 -4.1961617 -4.1903739 -4.1582074 -4.0944786 -4.0396667 -4.0551691 -4.1157813 -4.1670785 -4.2020841 -4.2262235 -4.2423506 -4.2474561 -4.2390137][-4.2033482 -4.2083282 -4.2089872 -4.1985326 -4.1635151 -4.0924397 -4.0217547 -4.0303621 -4.1008444 -4.1647768 -4.2073402 -4.234426 -4.2491446 -4.2543607 -4.2479076][-4.2212048 -4.2246037 -4.2189522 -4.2049613 -4.175961 -4.1162281 -4.0525346 -4.0500197 -4.1070886 -4.1690068 -4.211915 -4.2375097 -4.2496648 -4.2542419 -4.2512641][-4.2306728 -4.2331977 -4.226953 -4.2120256 -4.1868792 -4.1381454 -4.0859423 -4.0766959 -4.1187553 -4.1740456 -4.2176151 -4.2422266 -4.2533574 -4.2593536 -4.259171][-4.237278 -4.2392483 -4.2350721 -4.2248483 -4.2048521 -4.1653 -4.1206608 -4.1062121 -4.1373358 -4.1868453 -4.2283273 -4.2519312 -4.2633042 -4.2686667 -4.2679267][-4.2422123 -4.24389 -4.2385182 -4.2309194 -4.2167563 -4.1873031 -4.1535044 -4.1405568 -4.1660247 -4.2095737 -4.2443352 -4.2625656 -4.2711535 -4.2734532 -4.2706985][-4.2492433 -4.2492852 -4.2391329 -4.2280331 -4.2157879 -4.1959867 -4.1747074 -4.1677141 -4.1895537 -4.2269373 -4.2549381 -4.2662253 -4.270421 -4.269495 -4.2649431][-4.2588682 -4.2554374 -4.2386017 -4.2177138 -4.2019572 -4.189301 -4.1791844 -4.1776476 -4.1942363 -4.2278137 -4.2542667 -4.2635689 -4.2666826 -4.2663012 -4.2605743]]...]
INFO - root - 2017-12-05 11:45:01.321232: step 4010, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 80h:25m:18s remains)
INFO - root - 2017-12-05 11:45:10.283514: step 4020, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 85h:13m:00s remains)
INFO - root - 2017-12-05 11:45:19.260457: step 4030, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 79h:41m:29s remains)
INFO - root - 2017-12-05 11:45:28.374064: step 4040, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 78h:20m:36s remains)
INFO - root - 2017-12-05 11:45:37.468593: step 4050, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 83h:05m:35s remains)
INFO - root - 2017-12-05 11:45:46.720805: step 4060, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 84h:49m:35s remains)
INFO - root - 2017-12-05 11:45:55.871778: step 4070, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 80h:16m:08s remains)
INFO - root - 2017-12-05 11:46:04.901929: step 4080, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 84h:12m:14s remains)
INFO - root - 2017-12-05 11:46:13.977664: step 4090, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 82h:45m:26s remains)
INFO - root - 2017-12-05 11:46:23.056142: step 4100, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 84h:46m:53s remains)
2017-12-05 11:46:23.873253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3127165 -4.3111959 -4.3108888 -4.3119845 -4.3155761 -4.3222871 -4.3241882 -4.3229 -4.3241086 -4.3230081 -4.3158078 -4.3084655 -4.3059978 -4.3092427 -4.3107109][-4.2887058 -4.2832227 -4.2824855 -4.2856722 -4.2936916 -4.3051047 -4.3069987 -4.3054762 -4.3063254 -4.30236 -4.2909865 -4.2813377 -4.2781806 -4.287065 -4.2944083][-4.25192 -4.2457862 -4.2479868 -4.2553778 -4.2689815 -4.2842078 -4.2848516 -4.2819309 -4.2805004 -4.2698193 -4.2535048 -4.2473278 -4.250814 -4.2656765 -4.2766852][-4.2073374 -4.205523 -4.2123775 -4.2242861 -4.2400951 -4.2542391 -4.2505074 -4.2411366 -4.2321863 -4.2140579 -4.2016239 -4.207984 -4.2219396 -4.2418756 -4.2565837][-4.1708436 -4.1736107 -4.1847849 -4.1986961 -4.2123036 -4.2183876 -4.1999574 -4.1724935 -4.156518 -4.1470709 -4.1577835 -4.1851888 -4.2084517 -4.2271948 -4.2365828][-4.1582952 -4.1632824 -4.1733513 -4.1799817 -4.1808467 -4.1675072 -4.1226177 -4.071743 -4.0659738 -4.0954914 -4.1446781 -4.1886358 -4.2114267 -4.222147 -4.2222471][-4.1616154 -4.1638217 -4.1657176 -4.1601434 -4.1432977 -4.1037354 -4.0208707 -3.9445724 -3.9777584 -4.06516 -4.1468177 -4.1957173 -4.20987 -4.210413 -4.2057405][-4.1665874 -4.157248 -4.1506968 -4.1429548 -4.118897 -4.0680337 -3.9756742 -3.9102721 -3.9764972 -4.0861835 -4.1672759 -4.2038589 -4.2041693 -4.1933551 -4.1812372][-4.1752052 -4.1613684 -4.1579943 -4.1565275 -4.1399083 -4.1077809 -4.054348 -4.0313425 -4.0863562 -4.1593084 -4.2073426 -4.2209945 -4.2032681 -4.1837931 -4.17333][-4.1727529 -4.1658888 -4.1730113 -4.1797762 -4.1772423 -4.1668706 -4.1442537 -4.1379085 -4.1657548 -4.1974344 -4.2146912 -4.2079463 -4.1780457 -4.1611924 -4.1664057][-4.170269 -4.1758103 -4.1890054 -4.1992111 -4.2054648 -4.2071395 -4.1999226 -4.1957483 -4.2017417 -4.2053418 -4.2022791 -4.1810846 -4.1511488 -4.1486845 -4.1719236][-4.1612921 -4.1756463 -4.1912785 -4.1972837 -4.2031651 -4.2089462 -4.2144022 -4.2157393 -4.211772 -4.20187 -4.1826882 -4.1565046 -4.1377039 -4.1498904 -4.1844931][-4.1278539 -4.14516 -4.1561403 -4.1597795 -4.1667652 -4.1775064 -4.1929579 -4.1981544 -4.1884208 -4.1736588 -4.1534524 -4.1360941 -4.1316228 -4.1529627 -4.1894407][-4.0845881 -4.0937905 -4.0957432 -4.0982251 -4.1086183 -4.124321 -4.1463685 -4.1535087 -4.1444216 -4.1354761 -4.127964 -4.1201768 -4.121347 -4.1453481 -4.1772819][-4.0673504 -4.0681376 -4.0642223 -4.0631971 -4.0711961 -4.08664 -4.1098204 -4.1201372 -4.1199884 -4.1212969 -4.1228747 -4.1172218 -4.1156578 -4.1356931 -4.1644344]]...]
INFO - root - 2017-12-05 11:46:32.996005: step 4110, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 82h:28m:39s remains)
INFO - root - 2017-12-05 11:46:42.046533: step 4120, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 78h:24m:05s remains)
INFO - root - 2017-12-05 11:46:51.115970: step 4130, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 83h:46m:11s remains)
INFO - root - 2017-12-05 11:47:00.236694: step 4140, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.873 sec/batch; 79h:39m:57s remains)
INFO - root - 2017-12-05 11:47:09.170030: step 4150, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 83h:07m:04s remains)
INFO - root - 2017-12-05 11:47:18.316019: step 4160, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 79h:05m:02s remains)
INFO - root - 2017-12-05 11:47:27.545656: step 4170, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 84h:40m:23s remains)
INFO - root - 2017-12-05 11:47:36.454950: step 4180, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 82h:19m:20s remains)
INFO - root - 2017-12-05 11:47:45.461909: step 4190, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 82h:36m:57s remains)
INFO - root - 2017-12-05 11:47:54.733565: step 4200, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 86h:56m:17s remains)
2017-12-05 11:47:55.487983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1962833 -4.2088304 -4.2164159 -4.2136974 -4.1892662 -4.1481423 -4.110239 -4.0894718 -4.1049976 -4.1295605 -4.1352797 -4.1444373 -4.1726952 -4.2146015 -4.2516284][-4.2064996 -4.2256746 -4.2379012 -4.2315512 -4.1975436 -4.1528716 -4.1165743 -4.0958843 -4.101984 -4.1139488 -4.1148667 -4.1210847 -4.1419172 -4.1809683 -4.2249174][-4.2260184 -4.2493896 -4.2602487 -4.2490525 -4.2087812 -4.1606145 -4.1236672 -4.1061606 -4.1076665 -4.1101737 -4.1073318 -4.1115551 -4.1286287 -4.1571479 -4.1940851][-4.2465992 -4.2679691 -4.271945 -4.2540393 -4.210618 -4.1516843 -4.1046162 -4.0883188 -4.1001153 -4.1127043 -4.1104536 -4.1141362 -4.1347919 -4.1572628 -4.1843109][-4.258853 -4.2718329 -4.2672548 -4.2426624 -4.1933718 -4.1206422 -4.0582595 -4.0370035 -4.060039 -4.0945549 -4.1100855 -4.1225762 -4.150538 -4.1747656 -4.19768][-4.26649 -4.2699623 -4.261445 -4.23131 -4.1696377 -4.0824342 -4.0027666 -3.9719446 -4.0022364 -4.0614805 -4.1063986 -4.1384916 -4.1709266 -4.1983175 -4.2204018][-4.2719254 -4.2703781 -4.2619419 -4.2306762 -4.16482 -4.0728655 -3.9847577 -3.9410348 -3.9617584 -4.0293741 -4.0987854 -4.150475 -4.1861954 -4.2148252 -4.2403607][-4.276721 -4.2755442 -4.2692657 -4.2393618 -4.1792173 -4.0963688 -4.016521 -3.9679041 -3.9702733 -4.0228539 -4.0924172 -4.1515365 -4.1931615 -4.2254634 -4.2537045][-4.2807856 -4.2810211 -4.2746944 -4.2496834 -4.201839 -4.1394644 -4.0761833 -4.0313311 -4.0202618 -4.0473394 -4.1007414 -4.1565347 -4.2031908 -4.2385144 -4.2655993][-4.2788882 -4.278584 -4.2737508 -4.2587986 -4.2287836 -4.1862559 -4.1358876 -4.0928016 -4.0750985 -4.0884538 -4.1257682 -4.1749415 -4.2217422 -4.2556825 -4.2778792][-4.279458 -4.2781353 -4.2760358 -4.2706089 -4.256876 -4.2303147 -4.1912236 -4.1525211 -4.1314363 -4.1376219 -4.1648793 -4.2077227 -4.2516317 -4.2819715 -4.2972851][-4.2921195 -4.2894616 -4.288053 -4.28639 -4.2811766 -4.2656393 -4.2390265 -4.2097292 -4.187758 -4.1867342 -4.2081404 -4.2432742 -4.2781487 -4.3016624 -4.3120737][-4.3016706 -4.2977018 -4.2976213 -4.2985287 -4.2986827 -4.2925262 -4.2758188 -4.25559 -4.2371831 -4.232482 -4.2453232 -4.270916 -4.2950654 -4.3112335 -4.319684][-4.3036976 -4.2989731 -4.2999463 -4.3033714 -4.3078065 -4.3089128 -4.3009295 -4.2864723 -4.2708926 -4.2658825 -4.2738004 -4.2901859 -4.3040557 -4.3131533 -4.3192706][-4.3092761 -4.3047791 -4.304265 -4.3069773 -4.3106127 -4.3133607 -4.3118443 -4.3024716 -4.2924838 -4.2896471 -4.2948594 -4.3059855 -4.3130312 -4.316339 -4.3193741]]...]
INFO - root - 2017-12-05 11:48:04.675114: step 4210, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 82h:44m:48s remains)
INFO - root - 2017-12-05 11:48:13.620610: step 4220, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.922 sec/batch; 84h:03m:59s remains)
INFO - root - 2017-12-05 11:48:22.634477: step 4230, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.905 sec/batch; 82h:29m:06s remains)
INFO - root - 2017-12-05 11:48:31.877730: step 4240, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 84h:07m:32s remains)
INFO - root - 2017-12-05 11:48:40.907739: step 4250, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 79h:49m:30s remains)
INFO - root - 2017-12-05 11:48:49.845519: step 4260, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 81h:16m:50s remains)
INFO - root - 2017-12-05 11:48:58.909366: step 4270, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 81h:50m:40s remains)
INFO - root - 2017-12-05 11:49:07.868196: step 4280, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.889 sec/batch; 81h:01m:24s remains)
INFO - root - 2017-12-05 11:49:16.970840: step 4290, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 82h:49m:40s remains)
INFO - root - 2017-12-05 11:49:26.066928: step 4300, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 84h:53m:25s remains)
2017-12-05 11:49:26.835834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2059512 -4.2088027 -4.2049322 -4.1931248 -4.1748848 -4.1550012 -4.1450977 -4.1516671 -4.1695895 -4.1898017 -4.2062573 -4.214643 -4.21361 -4.205658 -4.1961107][-4.1918488 -4.1987743 -4.1951952 -4.1780839 -4.1484475 -4.1165261 -4.1016874 -4.1130786 -4.1397948 -4.1689487 -4.1951256 -4.2112427 -4.2134967 -4.2046885 -4.1927404][-4.1740427 -4.1821833 -4.1749353 -4.1499457 -4.1086926 -4.0663929 -4.0477915 -4.0645003 -4.0996203 -4.1375008 -4.1741772 -4.1987453 -4.2049375 -4.1967893 -4.1830945][-4.1615696 -4.1664214 -4.1519208 -4.1183395 -4.0684714 -4.0224657 -4.004066 -4.0253305 -4.0655537 -4.109077 -4.1542907 -4.1857786 -4.1948118 -4.187047 -4.1705141][-4.1609573 -4.1562386 -4.1317463 -4.0895944 -4.0342383 -3.9892306 -3.9755151 -4.0037766 -4.0512967 -4.1014357 -4.1519394 -4.185977 -4.1938844 -4.1823359 -4.1580563][-4.172668 -4.1548319 -4.1183996 -4.0671463 -4.0052681 -3.9587321 -3.9479904 -3.9860172 -4.045928 -4.106916 -4.1629062 -4.1978707 -4.200696 -4.1800914 -4.1425409][-4.1848841 -4.1590633 -4.1154356 -4.0550041 -3.9824359 -3.9262576 -3.9130874 -3.961463 -4.0370059 -4.1113024 -4.1737213 -4.2104545 -4.2084656 -4.17726 -4.1255331][-4.1864848 -4.1611233 -4.1211114 -4.0618534 -3.9838939 -3.9151778 -3.8937182 -3.9469109 -4.0321412 -4.1153297 -4.1828122 -4.2212033 -4.2170448 -4.1802163 -4.1218829][-4.1764483 -4.1563678 -4.1295786 -4.0863185 -4.020998 -3.9538245 -3.9295623 -3.9768667 -4.0575376 -4.1375403 -4.2009277 -4.2353392 -4.2306237 -4.1957445 -4.1417251][-4.1625776 -4.1499581 -4.1387181 -4.1167088 -4.0726652 -4.0181503 -4.0004263 -4.0422621 -4.111732 -4.1797228 -4.2300224 -4.2546911 -4.2495766 -4.2204523 -4.1744037][-4.1462779 -4.1415052 -4.1440067 -4.140666 -4.1168451 -4.0787511 -4.0700822 -4.1078472 -4.1657729 -4.2204361 -4.2571254 -4.2727113 -4.2674251 -4.2415023 -4.1983652][-4.1286244 -4.1311431 -4.1461349 -4.1586552 -4.1532669 -4.1310606 -4.128881 -4.1596832 -4.2052855 -4.2478418 -4.2744627 -4.2839413 -4.2784386 -4.2514358 -4.205337][-4.1148663 -4.1242924 -4.149085 -4.1715956 -4.1800213 -4.1728296 -4.17681 -4.1999536 -4.2339206 -4.266253 -4.2859879 -4.2911878 -4.2841249 -4.2551937 -4.206614][-4.1113906 -4.12831 -4.1588054 -4.1855965 -4.2007656 -4.202539 -4.2088594 -4.2244735 -4.2495875 -4.2758541 -4.2925406 -4.2960892 -4.2872834 -4.2580967 -4.2121048][-4.1175261 -4.1427817 -4.1755648 -4.1987844 -4.2099385 -4.2106161 -4.2115083 -4.2178392 -4.2367964 -4.2606068 -4.2780619 -4.2853112 -4.2787228 -4.2537408 -4.2174368]]...]
INFO - root - 2017-12-05 11:49:35.837236: step 4310, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 81h:12m:00s remains)
INFO - root - 2017-12-05 11:49:44.680735: step 4320, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 81h:46m:36s remains)
INFO - root - 2017-12-05 11:49:53.792894: step 4330, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 86h:21m:46s remains)
INFO - root - 2017-12-05 11:50:02.936637: step 4340, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 78h:56m:22s remains)
INFO - root - 2017-12-05 11:50:12.044909: step 4350, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 82h:34m:57s remains)
INFO - root - 2017-12-05 11:50:20.998566: step 4360, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 82h:39m:09s remains)
INFO - root - 2017-12-05 11:50:30.245379: step 4370, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 81h:23m:20s remains)
INFO - root - 2017-12-05 11:50:39.260811: step 4380, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 80h:37m:43s remains)
INFO - root - 2017-12-05 11:50:48.251284: step 4390, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 82h:36m:38s remains)
INFO - root - 2017-12-05 11:50:57.257132: step 4400, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:55m:55s remains)
2017-12-05 11:50:58.053244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2541041 -4.2478647 -4.2491069 -4.2515111 -4.2537141 -4.260293 -4.2583723 -4.2550993 -4.2524776 -4.2521563 -4.2484617 -4.2355876 -4.2229037 -4.2181845 -4.2220125][-4.2390976 -4.2357879 -4.2373171 -4.2342544 -4.2281265 -4.227459 -4.2236352 -4.2238727 -4.2254786 -4.2286034 -4.2336731 -4.2271409 -4.2135115 -4.2024274 -4.20432][-4.2196054 -4.2192407 -4.2220497 -4.2181683 -4.2055826 -4.1945677 -4.1837969 -4.1826129 -4.1874084 -4.1956019 -4.2122154 -4.2180696 -4.2092929 -4.1948996 -4.19371][-4.1987915 -4.2049465 -4.2107773 -4.2078824 -4.1892314 -4.1647182 -4.1427946 -4.1347585 -4.1409984 -4.154006 -4.1809092 -4.2017875 -4.2038021 -4.1947637 -4.1936874][-4.1823921 -4.1948547 -4.199564 -4.1951208 -4.170083 -4.1317725 -4.0936007 -4.0779781 -4.0880141 -4.1085782 -4.1431932 -4.1778584 -4.1930161 -4.19461 -4.1990094][-4.1599097 -4.1799221 -4.183589 -4.1765866 -4.14644 -4.0990295 -4.0466094 -4.0247941 -4.0428586 -4.0744891 -4.1145635 -4.1579332 -4.1861968 -4.1972132 -4.2065325][-4.1380663 -4.1558123 -4.15642 -4.1507421 -4.1207895 -4.0712895 -4.0140367 -3.9867427 -4.0141211 -4.0595713 -4.1055565 -4.1505709 -4.1868658 -4.2021809 -4.2135839][-4.1301403 -4.1435847 -4.1388345 -4.1347432 -4.1046653 -4.0547194 -3.9951448 -3.9633732 -3.9978528 -4.0567665 -4.1086082 -4.1498938 -4.1827664 -4.1984706 -4.2103][-4.1211309 -4.13599 -4.1371913 -4.1423674 -4.1138058 -4.0605354 -3.9946814 -3.9524286 -3.9864008 -4.0555315 -4.1109128 -4.1479335 -4.1741767 -4.1876836 -4.1998653][-4.1102934 -4.1313124 -4.1468654 -4.1640811 -4.1416535 -4.0870252 -4.0179696 -3.9639802 -3.9872091 -4.0540276 -4.1077824 -4.143064 -4.1654277 -4.1775484 -4.1895409][-4.1022449 -4.1297069 -4.1535244 -4.1775103 -4.1642213 -4.1149573 -4.0455656 -3.9843173 -3.9943845 -4.0559268 -4.1077819 -4.142653 -4.1604528 -4.16955 -4.1803565][-4.0868006 -4.1140881 -4.13704 -4.1622448 -4.1598353 -4.1212444 -4.0539331 -3.9907463 -3.9917963 -4.04764 -4.0995827 -4.1347542 -4.1500988 -4.1574321 -4.1708674][-4.0689888 -4.08586 -4.1046839 -4.1262259 -4.1304855 -4.1078444 -4.0507951 -3.9910612 -3.9828131 -4.0265346 -4.0775023 -4.1156693 -4.132844 -4.1419406 -4.1622181][-4.0645289 -4.0643826 -4.0756779 -4.0960832 -4.10923 -4.1038728 -4.0627723 -4.0117903 -3.9978783 -4.0272317 -4.0713844 -4.1115742 -4.1333814 -4.1468749 -4.1710734][-4.077435 -4.061492 -4.063169 -4.0815468 -4.1019821 -4.112668 -4.0853543 -4.0418315 -4.0244117 -4.0416813 -4.0790796 -4.1200953 -4.1506467 -4.1676369 -4.1897912]]...]
INFO - root - 2017-12-05 11:51:06.976454: step 4410, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 84h:06m:31s remains)
INFO - root - 2017-12-05 11:51:16.039917: step 4420, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:00m:18s remains)
INFO - root - 2017-12-05 11:51:25.210665: step 4430, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 82h:26m:33s remains)
INFO - root - 2017-12-05 11:51:34.299502: step 4440, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 86h:25m:29s remains)
INFO - root - 2017-12-05 11:51:43.308074: step 4450, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 80h:31m:32s remains)
INFO - root - 2017-12-05 11:51:52.350249: step 4460, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 79h:53m:16s remains)
INFO - root - 2017-12-05 11:52:01.401499: step 4470, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 81h:44m:18s remains)
INFO - root - 2017-12-05 11:52:10.502377: step 4480, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 80h:53m:14s remains)
INFO - root - 2017-12-05 11:52:19.567306: step 4490, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 81h:48m:39s remains)
INFO - root - 2017-12-05 11:52:28.381772: step 4500, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 76h:09m:07s remains)
2017-12-05 11:52:29.122690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2866044 -4.2777362 -4.2656927 -4.2574606 -4.25469 -4.2538929 -4.2418551 -4.2306485 -4.2354717 -4.2434673 -4.2575855 -4.2783008 -4.2962856 -4.3078709 -4.311883][-4.2771988 -4.2674255 -4.2519612 -4.2423639 -4.2369041 -4.2315316 -4.2054968 -4.1823411 -4.192029 -4.2113008 -4.2369976 -4.2709632 -4.2961736 -4.3117557 -4.3157492][-4.2683792 -4.2564521 -4.2369704 -4.2284007 -4.2216935 -4.2075243 -4.1628494 -4.125145 -4.1461153 -4.1861362 -4.2259398 -4.2687688 -4.3002687 -4.3166213 -4.3198805][-4.2603 -4.24464 -4.2223969 -4.2147207 -4.2069855 -4.1788955 -4.1085973 -4.0577183 -4.0914717 -4.1529388 -4.20898 -4.2608395 -4.2968025 -4.3156662 -4.3199272][-4.2502775 -4.2294221 -4.2071424 -4.1978021 -4.1830335 -4.1357942 -4.04263 -3.9871979 -4.0376668 -4.1165595 -4.1858797 -4.2461796 -4.2855191 -4.309236 -4.3168545][-4.2462873 -4.2221484 -4.200387 -4.1855807 -4.15681 -4.0845118 -3.9663107 -3.9100893 -3.988637 -4.0867047 -4.1659751 -4.2316427 -4.2724457 -4.3023858 -4.313838][-4.2468743 -4.2269044 -4.20527 -4.1846471 -4.1403594 -4.04108 -3.8898199 -3.8334355 -3.9511726 -4.0735016 -4.160027 -4.2268286 -4.2681694 -4.3023224 -4.3138137][-4.2501407 -4.2411728 -4.2273626 -4.2064986 -4.1544847 -4.0409918 -3.8753841 -3.8168094 -3.9512658 -4.0843267 -4.17163 -4.236722 -4.2772 -4.3070769 -4.3153744][-4.2477427 -4.2483649 -4.2441683 -4.2313147 -4.1896954 -4.0953636 -3.9636736 -3.9105122 -4.01048 -4.1232204 -4.2004638 -4.2590032 -4.292335 -4.3130431 -4.3164759][-4.2454629 -4.2437229 -4.243299 -4.2384353 -4.2084255 -4.1411195 -4.0563874 -4.0258346 -4.0924525 -4.1742568 -4.2320333 -4.2757149 -4.2986164 -4.3128438 -4.3154578][-4.2498293 -4.2413068 -4.2350345 -4.2306786 -4.206532 -4.1568007 -4.1040173 -4.0942473 -4.1490154 -4.2116513 -4.2517 -4.2770061 -4.289278 -4.3029137 -4.3079853][-4.2544541 -4.2478647 -4.238656 -4.2265944 -4.2001476 -4.1594238 -4.1211591 -4.1216726 -4.16965 -4.2193995 -4.248486 -4.2603207 -4.2655854 -4.2793522 -4.2853813][-4.252933 -4.25372 -4.2516079 -4.2370071 -4.2088451 -4.1768594 -4.1497912 -4.1500287 -4.1814146 -4.2127743 -4.2293944 -4.2298818 -4.231153 -4.2446303 -4.2497654][-4.2511196 -4.2585196 -4.2613168 -4.2477484 -4.2227869 -4.1999059 -4.1846986 -4.1865997 -4.2016764 -4.2161417 -4.2218118 -4.2135558 -4.2115293 -4.2200809 -4.2202234][-4.2557435 -4.2695942 -4.2726884 -4.2600546 -4.2401872 -4.2231989 -4.2141023 -4.2146759 -4.2201886 -4.2272773 -4.2282028 -4.2192755 -4.2149882 -4.21647 -4.208642]]...]
INFO - root - 2017-12-05 11:52:38.246877: step 4510, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.879 sec/batch; 80h:03m:37s remains)
INFO - root - 2017-12-05 11:52:47.363031: step 4520, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 81h:41m:56s remains)
INFO - root - 2017-12-05 11:52:56.373266: step 4530, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 84h:57m:50s remains)
INFO - root - 2017-12-05 11:53:05.519686: step 4540, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 82h:56m:16s remains)
INFO - root - 2017-12-05 11:53:14.544344: step 4550, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 82h:20m:57s remains)
INFO - root - 2017-12-05 11:53:23.542352: step 4560, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 82h:54m:25s remains)
INFO - root - 2017-12-05 11:53:32.782812: step 4570, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 87h:43m:40s remains)
INFO - root - 2017-12-05 11:53:41.959076: step 4580, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 84h:55m:40s remains)
INFO - root - 2017-12-05 11:53:51.050528: step 4590, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 80h:50m:53s remains)
INFO - root - 2017-12-05 11:54:00.141857: step 4600, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 82h:23m:20s remains)
2017-12-05 11:54:00.902982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3295994 -4.3307338 -4.3322735 -4.3330569 -4.3335547 -4.3311276 -4.3202057 -4.2953439 -4.2779851 -4.2774372 -4.2834563 -4.2883086 -4.2967153 -4.3085012 -4.3200469][-4.3266349 -4.3288069 -4.3306484 -4.3305812 -4.3301454 -4.32657 -4.3130536 -4.2860656 -4.2689543 -4.2703519 -4.276793 -4.2809625 -4.2899027 -4.3048878 -4.3179054][-4.3186011 -4.3200035 -4.3208637 -4.3188868 -4.3157229 -4.3106132 -4.2964678 -4.27175 -4.2595677 -4.2654724 -4.2726126 -4.2752819 -4.2843752 -4.3024058 -4.3162575][-4.3038073 -4.3025489 -4.3005958 -4.2948666 -4.2880316 -4.2795377 -4.263577 -4.2416973 -4.2369976 -4.2507348 -4.2619486 -4.2658529 -4.276813 -4.2994103 -4.3148828][-4.280354 -4.2772245 -4.2707314 -4.259706 -4.2465014 -4.2309365 -4.20827 -4.187531 -4.1930418 -4.2188025 -4.2386928 -4.2467647 -4.2614326 -4.2898803 -4.3095098][-4.2439241 -4.2381787 -4.2261391 -4.206718 -4.1833525 -4.1563187 -4.1229162 -4.1043978 -4.125813 -4.1710882 -4.202374 -4.2168164 -4.2388854 -4.2746592 -4.3019156][-4.1845608 -4.1786375 -4.1644092 -4.1406689 -4.1116505 -4.0787206 -4.0381808 -4.0234141 -4.0580878 -4.1161151 -4.157032 -4.1802716 -4.2126865 -4.2563248 -4.2923369][-4.1475139 -4.1467075 -4.140367 -4.1263618 -4.1088591 -4.0889964 -4.0582895 -4.0450511 -4.0709071 -4.1204681 -4.15829 -4.1816373 -4.2134418 -4.255218 -4.2915454][-4.1491265 -4.1540446 -4.1583662 -4.157351 -4.1550975 -4.1499004 -4.1303306 -4.1151156 -4.1264162 -4.1626205 -4.1928806 -4.21319 -4.2391291 -4.272995 -4.3022428][-4.181314 -4.1871591 -4.193738 -4.1967635 -4.1979632 -4.1964216 -4.1818309 -4.1636744 -4.1672812 -4.1958365 -4.2225647 -4.2426677 -4.2644725 -4.2910128 -4.3113794][-4.2178607 -4.2214108 -4.2250443 -4.2259469 -4.2253628 -4.2225986 -4.2099247 -4.1937819 -4.1963148 -4.21954 -4.2437944 -4.2635427 -4.2811036 -4.3003168 -4.3143573][-4.2330809 -4.2343121 -4.2349606 -4.2341604 -4.2334785 -4.2310419 -4.2218146 -4.2093782 -4.2104864 -4.2273583 -4.2480421 -4.265583 -4.2810421 -4.298739 -4.3136024][-4.2530308 -4.2535262 -4.2530975 -4.2523694 -4.25195 -4.2507305 -4.2444239 -4.2357674 -4.2356434 -4.2462931 -4.2614441 -4.2755966 -4.2895851 -4.3073297 -4.3221965][-4.2808242 -4.2816167 -4.2812123 -4.280899 -4.280838 -4.2808633 -4.2774305 -4.2718396 -4.2701006 -4.2768083 -4.2888117 -4.3011413 -4.3128366 -4.3268008 -4.3369422][-4.3090286 -4.3092289 -4.3088064 -4.3085628 -4.3086395 -4.3092456 -4.3084574 -4.3056612 -4.3034539 -4.3067083 -4.3157911 -4.3252778 -4.3331528 -4.34098 -4.3454971]]...]
INFO - root - 2017-12-05 11:54:10.019346: step 4610, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 83h:34m:27s remains)
INFO - root - 2017-12-05 11:54:19.230134: step 4620, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 84h:13m:18s remains)
INFO - root - 2017-12-05 11:54:28.371360: step 4630, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 79h:51m:34s remains)
INFO - root - 2017-12-05 11:54:37.333572: step 4640, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 83h:09m:54s remains)
INFO - root - 2017-12-05 11:54:46.436349: step 4650, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.931 sec/batch; 84h:45m:35s remains)
INFO - root - 2017-12-05 11:54:55.480052: step 4660, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 82h:28m:59s remains)
INFO - root - 2017-12-05 11:55:04.553765: step 4670, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 81h:04m:36s remains)
INFO - root - 2017-12-05 11:55:13.829029: step 4680, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 83h:28m:06s remains)
INFO - root - 2017-12-05 11:55:22.837939: step 4690, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 77h:12m:37s remains)
INFO - root - 2017-12-05 11:55:31.933423: step 4700, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 81h:39m:06s remains)
2017-12-05 11:55:32.687549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2735581 -4.2787971 -4.2806454 -4.2824984 -4.2866869 -4.2897296 -4.2892079 -4.2874103 -4.2827053 -4.2746496 -4.2692313 -4.267374 -4.2658281 -4.2554116 -4.2391558][-4.3050284 -4.3118386 -4.314198 -4.315187 -4.3158054 -4.3123488 -4.3045506 -4.298358 -4.2916136 -4.2801218 -4.2702122 -4.2614889 -4.2526331 -4.2376413 -4.2228212][-4.3146105 -4.3241243 -4.3293695 -4.3314691 -4.3285737 -4.3159647 -4.299 -4.2874317 -4.2796392 -4.2679911 -4.2544475 -4.2393479 -4.2221146 -4.2037721 -4.1932936][-4.3022342 -4.3143106 -4.3227596 -4.325139 -4.3180513 -4.29691 -4.27359 -4.2595496 -4.25372 -4.2463875 -4.2338495 -4.2143645 -4.19058 -4.1690588 -4.1593466][-4.27423 -4.2864385 -4.2963519 -4.2975903 -4.2865734 -4.2587481 -4.2311072 -4.218143 -4.2187843 -4.2176161 -4.20852 -4.1882539 -4.1637306 -4.1445832 -4.1339307][-4.237114 -4.2445612 -4.2507582 -4.2477412 -4.2287383 -4.190124 -4.1597538 -4.1566057 -4.171248 -4.1828728 -4.18225 -4.166019 -4.1449051 -4.1324129 -4.1235023][-4.20017 -4.1961932 -4.1919365 -4.1795707 -4.1482964 -4.0950651 -4.0628819 -4.0792794 -4.1198354 -4.1493959 -4.155221 -4.1449761 -4.1327658 -4.1329389 -4.1358309][-4.1873536 -4.1706038 -4.1520295 -4.1255026 -4.0782328 -4.007483 -3.9693055 -4.0038285 -4.0667357 -4.1096816 -4.1225758 -4.1178217 -4.1130714 -4.1272812 -4.148479][-4.1939888 -4.1720548 -4.1479487 -4.1186423 -4.0707226 -3.9997625 -3.9603093 -3.9925866 -4.0532789 -4.0967169 -4.1131086 -4.1089773 -4.1026163 -4.1177063 -4.1483655][-4.1920204 -4.1744657 -4.1570134 -4.142354 -4.1147137 -4.0682836 -4.04121 -4.0603094 -4.0990472 -4.1294055 -4.1432104 -4.1399145 -4.128458 -4.1314683 -4.1519265][-4.1905489 -4.181354 -4.1742749 -4.174593 -4.166677 -4.1452985 -4.1325459 -4.1437116 -4.1627116 -4.1773753 -4.1847806 -4.1793756 -4.1653318 -4.1582055 -4.1623335][-4.1846647 -4.1845341 -4.1895556 -4.2027583 -4.2105274 -4.2064996 -4.2049465 -4.212461 -4.2186108 -4.2226892 -4.2250242 -4.2175269 -4.2015438 -4.1856618 -4.1745443][-4.1894226 -4.1925526 -4.2068415 -4.2291913 -4.2437258 -4.2440391 -4.2422895 -4.2452869 -4.2473588 -4.2493439 -4.2501163 -4.2402577 -4.2249107 -4.207149 -4.1887622][-4.2260222 -4.227283 -4.2410359 -4.2592249 -4.2655287 -4.25568 -4.2401829 -4.236167 -4.2393022 -4.2455473 -4.2513089 -4.2476907 -4.2395263 -4.2262378 -4.2084365][-4.2749677 -4.2732334 -4.279346 -4.2859936 -4.2795539 -4.2562838 -4.2263637 -4.2157393 -4.218215 -4.2274189 -4.2412033 -4.2454844 -4.2437491 -4.2353158 -4.2231178]]...]
INFO - root - 2017-12-05 11:55:41.713983: step 4710, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 86h:44m:32s remains)
INFO - root - 2017-12-05 11:55:51.057982: step 4720, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.929 sec/batch; 84h:32m:30s remains)
INFO - root - 2017-12-05 11:56:00.121626: step 4730, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 81h:21m:52s remains)
INFO - root - 2017-12-05 11:56:09.235195: step 4740, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 85h:22m:53s remains)
INFO - root - 2017-12-05 11:56:18.399710: step 4750, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 83h:56m:51s remains)
INFO - root - 2017-12-05 11:56:27.479785: step 4760, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 81h:07m:40s remains)
INFO - root - 2017-12-05 11:56:36.512837: step 4770, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 85h:03m:52s remains)
INFO - root - 2017-12-05 11:56:45.682581: step 4780, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 80h:50m:03s remains)
INFO - root - 2017-12-05 11:56:54.659414: step 4790, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 82h:41m:15s remains)
INFO - root - 2017-12-05 11:57:03.996902: step 4800, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.943 sec/batch; 85h:49m:23s remains)
2017-12-05 11:57:04.778395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2192969 -4.2306242 -4.2368279 -4.2414513 -4.2494035 -4.2608728 -4.2732058 -4.290288 -4.3098536 -4.3220043 -4.3262568 -4.321013 -4.315064 -4.3148775 -4.3179641][-4.1575446 -4.1740818 -4.1844664 -4.1914725 -4.2002463 -4.2112832 -4.2225938 -4.2435174 -4.2734227 -4.2966757 -4.3104835 -4.3128924 -4.3108425 -4.3108444 -4.3135619][-4.1120429 -4.1304512 -4.1387625 -4.1435871 -4.1477089 -4.15203 -4.1583352 -4.1801744 -4.2180219 -4.2519326 -4.2763281 -4.2896895 -4.2961183 -4.3007817 -4.3052092][-4.0883932 -4.10454 -4.1075625 -4.1064839 -4.1001205 -4.090836 -4.087008 -4.1077662 -4.1546087 -4.1989784 -4.232408 -4.254499 -4.2689013 -4.2801509 -4.2886252][-4.097095 -4.1029363 -4.0939703 -4.077599 -4.0523214 -4.0224519 -4.0024595 -4.0228996 -4.0859113 -4.1467977 -4.1866241 -4.2102737 -4.2272444 -4.2430525 -4.2570925][-4.131515 -4.1269994 -4.1020017 -4.0638132 -4.0136352 -3.954062 -3.9058275 -3.9203985 -4.0058985 -4.0910048 -4.1401825 -4.163476 -4.17777 -4.1939569 -4.2122054][-4.1773462 -4.1710119 -4.1362271 -4.0797358 -4.0084825 -3.926028 -3.8521483 -3.8512826 -3.9448068 -4.0454912 -4.1013808 -4.1256042 -4.1377096 -4.1527715 -4.1717234][-4.21991 -4.2181177 -4.1824408 -4.1199231 -4.043735 -3.9567025 -3.8777394 -3.8627243 -3.9362047 -4.02809 -4.08339 -4.1055436 -4.1166778 -4.1306386 -4.14811][-4.2506742 -4.2550573 -4.2266159 -4.1733818 -4.1083055 -4.0327535 -3.9608777 -3.9348495 -3.9781487 -4.0466828 -4.0952034 -4.1161709 -4.1261129 -4.1350126 -4.1450644][-4.2681522 -4.2799149 -4.2653866 -4.2304106 -4.1849856 -4.1271634 -4.0695362 -4.0399933 -4.0588379 -4.1024 -4.1401858 -4.160347 -4.1662173 -4.1635518 -4.1552987][-4.2715034 -4.29105 -4.2923346 -4.2758307 -4.2478471 -4.2085538 -4.169991 -4.1501331 -4.15761 -4.1798296 -4.20523 -4.2217531 -4.2231307 -4.2093344 -4.1821132][-4.2627196 -4.284987 -4.2958388 -4.2932677 -4.2788572 -4.2572322 -4.2379761 -4.2335477 -4.2404165 -4.249876 -4.2635455 -4.2738304 -4.2728772 -4.2572737 -4.225131][-4.2426238 -4.2604747 -4.2731018 -4.2810011 -4.2799721 -4.27417 -4.2716041 -4.2798843 -4.2890449 -4.292232 -4.2971115 -4.3020024 -4.3020763 -4.291441 -4.2668066][-4.2215385 -4.2325439 -4.245542 -4.2600904 -4.2677226 -4.2714758 -4.2771688 -4.290226 -4.3001666 -4.3004346 -4.3001075 -4.3036056 -4.309577 -4.3083944 -4.2961993][-4.211555 -4.2159238 -4.2257462 -4.2409263 -4.2513733 -4.2587438 -4.2661695 -4.2763748 -4.2838097 -4.2810388 -4.2767682 -4.2782207 -4.2896886 -4.3017426 -4.306654]]...]
INFO - root - 2017-12-05 11:57:13.913878: step 4810, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 81h:42m:09s remains)
INFO - root - 2017-12-05 11:57:23.039788: step 4820, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 82h:17m:09s remains)
INFO - root - 2017-12-05 11:57:32.072220: step 4830, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 83h:12m:29s remains)
INFO - root - 2017-12-05 11:57:41.118015: step 4840, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 80h:29m:34s remains)
INFO - root - 2017-12-05 11:57:50.151901: step 4850, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 84h:44m:13s remains)
INFO - root - 2017-12-05 11:57:59.413009: step 4860, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 83h:16m:11s remains)
INFO - root - 2017-12-05 11:58:08.682055: step 4870, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 81h:50m:03s remains)
INFO - root - 2017-12-05 11:58:17.697402: step 4880, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 81h:41m:25s remains)
INFO - root - 2017-12-05 11:58:26.747608: step 4890, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 80h:49m:16s remains)
INFO - root - 2017-12-05 11:58:35.934044: step 4900, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 82h:56m:10s remains)
2017-12-05 11:58:36.687248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1726365 -4.2309761 -4.2577081 -4.2556429 -4.2189355 -4.1944804 -4.1904564 -4.1914515 -4.2088361 -4.2393117 -4.2625809 -4.2772236 -4.2781973 -4.2540417 -4.2292309][-4.1793847 -4.2403417 -4.2709227 -4.2737241 -4.2390933 -4.211658 -4.2033482 -4.2045836 -4.2252073 -4.2566876 -4.2737212 -4.2812481 -4.2820334 -4.2585716 -4.2307277][-4.1751862 -4.2410927 -4.2760019 -4.2804623 -4.2430377 -4.2061162 -4.1894631 -4.1937728 -4.224462 -4.2634811 -4.2811394 -4.2851872 -4.2840576 -4.2613139 -4.232265][-4.156848 -4.2261243 -4.2656193 -4.2707982 -4.2289729 -4.1806149 -4.1528029 -4.159183 -4.2028604 -4.2534847 -4.2788477 -4.2870255 -4.2845588 -4.2612839 -4.2317162][-4.1412115 -4.2085609 -4.2504945 -4.2558722 -4.2096338 -4.1470804 -4.1003866 -4.09993 -4.1594796 -4.22793 -4.2674155 -4.2840228 -4.2810974 -4.2554383 -4.2219982][-4.1428919 -4.2044129 -4.2455192 -4.2519536 -4.204257 -4.1294646 -4.057559 -4.0403953 -4.1129222 -4.1990047 -4.254117 -4.2790604 -4.2755537 -4.24574 -4.2088437][-4.1659679 -4.22028 -4.2569003 -4.2645092 -4.2209926 -4.1454926 -4.0629406 -4.0348973 -4.10688 -4.1965227 -4.2579803 -4.2870874 -4.2839818 -4.250607 -4.2109447][-4.1925941 -4.2362061 -4.26107 -4.2644086 -4.2261896 -4.1627569 -4.0943093 -4.075346 -4.1370821 -4.2171221 -4.2758689 -4.3070741 -4.3054166 -4.2751551 -4.2357283][-4.2044048 -4.2334132 -4.2439733 -4.2371788 -4.2006555 -4.15557 -4.1166039 -4.116148 -4.1712422 -4.2393551 -4.292408 -4.3224249 -4.3250604 -4.3014736 -4.2670088][-4.1996083 -4.2204051 -4.2219276 -4.2052703 -4.173759 -4.1501131 -4.1428175 -4.1571703 -4.204258 -4.2595892 -4.3038616 -4.3317823 -4.3370929 -4.3211584 -4.2946486][-4.1818419 -4.2011375 -4.201726 -4.183147 -4.1624422 -4.1591883 -4.1719341 -4.1909423 -4.228621 -4.27121 -4.3053255 -4.3295379 -4.3313975 -4.3171306 -4.295795][-4.1858873 -4.2033572 -4.2019668 -4.1838813 -4.1732411 -4.1837888 -4.2055969 -4.2259526 -4.256423 -4.285399 -4.3068085 -4.3224263 -4.3195214 -4.3056135 -4.2891264][-4.2163787 -4.2277503 -4.2219439 -4.2049932 -4.1976051 -4.2110229 -4.2349849 -4.2573757 -4.2824521 -4.301796 -4.3132663 -4.3209534 -4.3145323 -4.3003793 -4.2884159][-4.2457032 -4.2530212 -4.2481136 -4.234477 -4.2266989 -4.2359014 -4.2579784 -4.277739 -4.2959118 -4.3102922 -4.3169341 -4.3195767 -4.3130646 -4.30009 -4.292901][-4.2564578 -4.2625127 -4.2615576 -4.2544212 -4.2479239 -4.2517767 -4.2643423 -4.2761908 -4.2873926 -4.2992344 -4.3038135 -4.3041162 -4.2972879 -4.2879119 -4.285789]]...]
INFO - root - 2017-12-05 11:58:45.986625: step 4910, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 82h:36m:27s remains)
INFO - root - 2017-12-05 11:58:55.042584: step 4920, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 78h:16m:46s remains)
INFO - root - 2017-12-05 11:59:04.152775: step 4930, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 81h:21m:33s remains)
INFO - root - 2017-12-05 11:59:13.361503: step 4940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 84h:23m:59s remains)
INFO - root - 2017-12-05 11:59:22.520426: step 4950, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 81h:49m:49s remains)
INFO - root - 2017-12-05 11:59:31.558215: step 4960, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 83h:17m:37s remains)
INFO - root - 2017-12-05 11:59:40.561711: step 4970, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 83h:23m:53s remains)
INFO - root - 2017-12-05 11:59:49.736114: step 4980, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 85h:58m:59s remains)
INFO - root - 2017-12-05 11:59:58.946276: step 4990, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 81h:50m:34s remains)
INFO - root - 2017-12-05 12:00:08.078033: step 5000, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.814 sec/batch; 74h:04m:05s remains)
2017-12-05 12:00:08.849623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1724091 -4.1995749 -4.2042561 -4.1651773 -4.1096358 -4.0805387 -4.0863209 -4.1125522 -4.1422653 -4.1770878 -4.2187538 -4.2431602 -4.252533 -4.2656403 -4.2679482][-4.1808467 -4.2102647 -4.2145061 -4.1746378 -4.1153593 -4.0806551 -4.0853181 -4.1078219 -4.13148 -4.15426 -4.1899996 -4.2191925 -4.2391248 -4.2559652 -4.2598009][-4.1842842 -4.2161365 -4.2199259 -4.1826072 -4.1259437 -4.0943828 -4.0953326 -4.1137638 -4.1337724 -4.1498752 -4.1813674 -4.210032 -4.2336159 -4.2509704 -4.2583051][-4.169786 -4.2087951 -4.2181 -4.1902318 -4.1402736 -4.1087751 -4.0979881 -4.1022573 -4.1208515 -4.14232 -4.1764412 -4.204596 -4.2279387 -4.2458196 -4.2582331][-4.1440191 -4.1961255 -4.2168479 -4.1988339 -4.1600108 -4.1251774 -4.0925674 -4.0736465 -4.0925217 -4.1237674 -4.1583524 -4.1812334 -4.2049823 -4.2279658 -4.24769][-4.1343403 -4.195013 -4.2196417 -4.2017856 -4.1691132 -4.1296554 -4.0661106 -4.0160046 -4.0434179 -4.09536 -4.13697 -4.1586452 -4.1828942 -4.2120862 -4.2349782][-4.1530151 -4.2054486 -4.2181358 -4.1885991 -4.1501741 -4.0898404 -3.9824305 -3.9037294 -3.9657445 -4.0564923 -4.1143661 -4.1416 -4.1667671 -4.2024879 -4.2271719][-4.1600461 -4.1966143 -4.1910815 -4.1474013 -4.0960703 -4.0098782 -3.849236 -3.7455242 -3.8685532 -4.0168676 -4.0960703 -4.1280904 -4.1449871 -4.174685 -4.2019715][-4.16363 -4.183363 -4.1597261 -4.1067786 -4.0497303 -3.9586735 -3.78984 -3.6924689 -3.8528774 -4.0204048 -4.0971894 -4.1135707 -4.106945 -4.1245484 -4.1584058][-4.1852818 -4.1942511 -4.1693959 -4.1185756 -4.0746059 -4.0154309 -3.9051652 -3.8493984 -3.97061 -4.0912595 -4.1341372 -4.1258645 -4.0901141 -4.0890107 -4.1164274][-4.2208138 -4.2204242 -4.1966896 -4.1592631 -4.1330867 -4.1018124 -4.0410566 -4.0077724 -4.0802116 -4.152956 -4.1703811 -4.1484547 -4.1031561 -4.0974035 -4.1167164][-4.2389131 -4.2348366 -4.2177386 -4.1935754 -4.1771522 -4.1603675 -4.1270585 -4.1059942 -4.1424265 -4.1791253 -4.1803455 -4.1504521 -4.1084337 -4.1067324 -4.1260495][-4.2475548 -4.24588 -4.2392216 -4.2273741 -4.2160125 -4.2027383 -4.1846876 -4.16788 -4.1794095 -4.19863 -4.1951728 -4.1657381 -4.1282597 -4.1222229 -4.1316876][-4.2640338 -4.2633047 -4.2609024 -4.2555189 -4.2480297 -4.2379708 -4.2240672 -4.2101727 -4.2111068 -4.2208238 -4.2166185 -4.1935329 -4.16611 -4.1546583 -4.1506572][-4.2838507 -4.2817693 -4.2787838 -4.2711525 -4.2627687 -4.2528968 -4.2420874 -4.2309775 -4.2288284 -4.2341385 -4.2335024 -4.225203 -4.2124963 -4.201529 -4.1926632]]...]
INFO - root - 2017-12-05 12:00:18.157237: step 5010, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 85h:17m:16s remains)
INFO - root - 2017-12-05 12:00:27.258387: step 5020, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 82h:52m:20s remains)
INFO - root - 2017-12-05 12:00:36.346437: step 5030, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 83h:42m:52s remains)
INFO - root - 2017-12-05 12:00:45.455483: step 5040, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 81h:01m:34s remains)
INFO - root - 2017-12-05 12:00:54.577375: step 5050, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 82h:18m:59s remains)
INFO - root - 2017-12-05 12:01:03.498720: step 5060, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.809 sec/batch; 73h:34m:01s remains)
INFO - root - 2017-12-05 12:01:12.620331: step 5070, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 87h:00m:50s remains)
INFO - root - 2017-12-05 12:01:21.640928: step 5080, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 81h:04m:15s remains)
INFO - root - 2017-12-05 12:01:30.853900: step 5090, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 85h:18m:11s remains)
INFO - root - 2017-12-05 12:01:39.906443: step 5100, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 81h:13m:54s remains)
2017-12-05 12:01:40.677058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0957518 -4.1194472 -4.1298914 -4.1302671 -4.1438518 -4.1676216 -4.1774163 -4.1847095 -4.1842213 -4.1870503 -4.2062931 -4.2186389 -4.2119703 -4.1825314 -4.1485453][-4.0680957 -4.0982442 -4.12111 -4.1311793 -4.1392016 -4.1562147 -4.164289 -4.1686611 -4.1637235 -4.1666503 -4.1896081 -4.2099628 -4.2107954 -4.1843419 -4.1481314][-4.1164784 -4.1444044 -4.1663151 -4.1759734 -4.1711259 -4.1672254 -4.1627054 -4.1604018 -4.15218 -4.1569533 -4.1846437 -4.2129269 -4.2232327 -4.2052422 -4.1775603][-4.1881728 -4.203948 -4.2121558 -4.2105107 -4.1927233 -4.1725769 -4.1592131 -4.154923 -4.1491985 -4.1599116 -4.1900506 -4.2175188 -4.2292185 -4.2185097 -4.2025046][-4.2382164 -4.2401733 -4.2294607 -4.211834 -4.1832714 -4.1561737 -4.1346622 -4.1299348 -4.1316433 -4.1493287 -4.1827283 -4.2129445 -4.2264194 -4.2272825 -4.2266235][-4.2541785 -4.2508326 -4.2206211 -4.1797214 -4.1288486 -4.0826368 -4.0466805 -4.0412464 -4.0584908 -4.0919952 -4.1345453 -4.1765027 -4.2031193 -4.223556 -4.2393193][-4.2220263 -4.2175984 -4.1717525 -4.10538 -4.0254021 -3.9568379 -3.910656 -3.9125538 -3.9589639 -4.0175843 -4.0723677 -4.1250119 -4.1640711 -4.1993375 -4.2269688][-4.1879387 -4.1780357 -4.1236525 -4.0463161 -3.9520893 -3.8700898 -3.8200052 -3.8330019 -3.9074967 -3.9907749 -4.0577674 -4.1158662 -4.1576653 -4.1903048 -4.2155724][-4.1728058 -4.16508 -4.1182413 -4.0556469 -3.9781103 -3.9126372 -3.882901 -3.9035869 -3.9717934 -4.044219 -4.0992231 -4.1463871 -4.1752429 -4.1912775 -4.2046504][-4.1766253 -4.1772432 -4.1485357 -4.1073108 -4.057353 -4.0198741 -4.0094728 -4.0258169 -4.0676832 -4.111104 -4.1425791 -4.1678581 -4.1742148 -4.1720386 -4.1784825][-4.1954827 -4.2041554 -4.193974 -4.1676893 -4.1373577 -4.1169186 -4.1150713 -4.125721 -4.147306 -4.1677103 -4.1786604 -4.1847372 -4.1766896 -4.1663566 -4.1679883][-4.2204065 -4.2344079 -4.2353711 -4.2197461 -4.202085 -4.1913667 -4.1921105 -4.1985922 -4.2066717 -4.2130933 -4.2093787 -4.2013063 -4.1866331 -4.1765294 -4.173615][-4.2526121 -4.2673459 -4.2724538 -4.2652922 -4.255908 -4.2508154 -4.2495694 -4.2479835 -4.2471871 -4.2465367 -4.2380328 -4.2250118 -4.2111473 -4.2013197 -4.1926436][-4.2811089 -4.288022 -4.2893758 -4.2852626 -4.2810411 -4.2790623 -4.2769632 -4.2734871 -4.2706466 -4.2678003 -4.2603827 -4.2507024 -4.242022 -4.2328582 -4.218338][-4.2903237 -4.2886434 -4.2857494 -4.2823191 -4.2805033 -4.2809377 -4.2809038 -4.280055 -4.2802849 -4.2802329 -4.2793326 -4.27735 -4.2752285 -4.2706842 -4.2589393]]...]
INFO - root - 2017-12-05 12:01:49.707174: step 5110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 82h:59m:02s remains)
INFO - root - 2017-12-05 12:01:59.044336: step 5120, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 86h:02m:03s remains)
INFO - root - 2017-12-05 12:02:08.117292: step 5130, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 82h:45m:11s remains)
INFO - root - 2017-12-05 12:02:17.163849: step 5140, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 87h:00m:48s remains)
INFO - root - 2017-12-05 12:02:26.319843: step 5150, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 85h:54m:19s remains)
INFO - root - 2017-12-05 12:02:35.346447: step 5160, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 82h:36m:11s remains)
INFO - root - 2017-12-05 12:02:44.508322: step 5170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:43m:24s remains)
INFO - root - 2017-12-05 12:02:53.616785: step 5180, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 82h:31m:17s remains)
INFO - root - 2017-12-05 12:03:02.832836: step 5190, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 87h:42m:21s remains)
INFO - root - 2017-12-05 12:03:12.086920: step 5200, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 80h:47m:29s remains)
2017-12-05 12:03:12.865479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2983356 -4.2952013 -4.2898569 -4.2856159 -4.284905 -4.2871385 -4.2893739 -4.2911549 -4.292428 -4.2938538 -4.2953453 -4.2963481 -4.2953253 -4.291832 -4.2881947][-4.2995667 -4.2995324 -4.2919211 -4.2818494 -4.2746592 -4.274775 -4.2801204 -4.2865553 -4.2913055 -4.2939448 -4.2948246 -4.2945685 -4.2925835 -4.2888565 -4.2861018][-4.2923203 -4.2997003 -4.2927361 -4.2778769 -4.262557 -4.2582345 -4.2674294 -4.2823672 -4.2941775 -4.300571 -4.3016806 -4.2992282 -4.2946749 -4.2901559 -4.2874789][-4.2582126 -4.2812505 -4.2780056 -4.2576666 -4.2318773 -4.2208891 -4.2369127 -4.2674928 -4.2939053 -4.3090253 -4.3136263 -4.3096294 -4.3019428 -4.2943444 -4.2901435][-4.1955929 -4.2381659 -4.2417397 -4.2135777 -4.1735191 -4.1517744 -4.1714096 -4.2211747 -4.27224 -4.3077021 -4.3221493 -4.32045 -4.3112731 -4.3007717 -4.2934256][-4.1211638 -4.1870384 -4.1981955 -4.1587486 -4.09433 -4.0525069 -4.0712872 -4.1407132 -4.2199354 -4.2834296 -4.3177686 -4.3245916 -4.3171334 -4.3055744 -4.2968][-4.0584655 -4.142818 -4.1628714 -4.113801 -4.0191488 -3.9417205 -3.9489136 -4.0366535 -4.1439915 -4.2356157 -4.2949252 -4.3164892 -4.3142328 -4.30417 -4.2968678][-4.0534267 -4.1344314 -4.155757 -4.10553 -3.9984176 -3.8860376 -3.8571923 -3.9446435 -4.0690212 -4.1814275 -4.2610359 -4.2995577 -4.3065062 -4.2998552 -4.2940397][-4.1227522 -4.1726418 -4.1844053 -4.1380577 -4.0481396 -3.9540887 -3.908967 -3.951895 -4.0511856 -4.1579204 -4.2402554 -4.2869139 -4.30282 -4.2992253 -4.292285][-4.2136827 -4.2312078 -4.2216668 -4.1801648 -4.1215987 -4.0710688 -4.0458417 -4.0608387 -4.1149569 -4.1889234 -4.2547064 -4.295455 -4.3121023 -4.3086114 -4.2985644][-4.2720113 -4.2672386 -4.2393236 -4.2050505 -4.1780252 -4.1642866 -4.1605611 -4.1670847 -4.1920772 -4.2347312 -4.2813382 -4.3154192 -4.3310838 -4.3267641 -4.3116612][-4.2869921 -4.2683887 -4.2315927 -4.2027903 -4.1948915 -4.1992598 -4.2027178 -4.204042 -4.2130241 -4.23476 -4.2707577 -4.3109741 -4.3372278 -4.3388634 -4.3233137][-4.2708812 -4.2437968 -4.2066779 -4.1847095 -4.184226 -4.1867833 -4.176362 -4.1593075 -4.153111 -4.1679969 -4.2082415 -4.2618508 -4.3097692 -4.3310671 -4.3258281][-4.2479773 -4.2186589 -4.1907539 -4.1810226 -4.1848292 -4.1750236 -4.1361227 -4.0790777 -4.0366693 -4.0390258 -4.0954843 -4.1743984 -4.2497864 -4.2990918 -4.3135805][-4.2290382 -4.2033195 -4.1898217 -4.194819 -4.2061963 -4.1919923 -4.1364737 -4.0512743 -3.9667773 -3.9275281 -3.9789257 -4.0773463 -4.1764207 -4.2508307 -4.2878666]]...]
INFO - root - 2017-12-05 12:03:21.790738: step 5210, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 80h:37m:45s remains)
INFO - root - 2017-12-05 12:03:30.836727: step 5220, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 84h:06m:55s remains)
INFO - root - 2017-12-05 12:03:40.096904: step 5230, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 85h:56m:19s remains)
INFO - root - 2017-12-05 12:03:49.076271: step 5240, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 80h:22m:32s remains)
INFO - root - 2017-12-05 12:03:57.990749: step 5250, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.810 sec/batch; 73h:39m:39s remains)
INFO - root - 2017-12-05 12:04:07.230754: step 5260, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 87h:39m:24s remains)
INFO - root - 2017-12-05 12:04:16.373361: step 5270, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 82h:20m:43s remains)
INFO - root - 2017-12-05 12:04:25.622180: step 5280, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 82h:48m:51s remains)
INFO - root - 2017-12-05 12:04:34.772993: step 5290, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 78h:16m:05s remains)
INFO - root - 2017-12-05 12:04:43.746506: step 5300, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 80h:26m:51s remains)
2017-12-05 12:04:44.497458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2635622 -4.2552967 -4.2347612 -4.2187891 -4.2139988 -4.2131662 -4.2143769 -4.2233195 -4.2355504 -4.2587466 -4.2788076 -4.2955046 -4.3108392 -4.3246861 -4.3303285][-4.249433 -4.2327785 -4.2102714 -4.2008634 -4.2038269 -4.2008667 -4.1901588 -4.1904325 -4.2016664 -4.2324862 -4.257257 -4.2779884 -4.3006368 -4.3211474 -4.3338308][-4.2324142 -4.2127891 -4.1973214 -4.1956558 -4.2012653 -4.1877513 -4.1564722 -4.1440449 -4.1578364 -4.2007766 -4.2316036 -4.2583728 -4.2901349 -4.3162107 -4.3348656][-4.2163181 -4.1985559 -4.1895986 -4.1913433 -4.1946278 -4.169281 -4.11808 -4.0955014 -4.1158934 -4.1664047 -4.2004929 -4.2331781 -4.2755804 -4.3109088 -4.335587][-4.1980753 -4.1825328 -4.168951 -4.1647186 -4.1615896 -4.121345 -4.0563326 -4.032517 -4.0707145 -4.1313577 -4.1726804 -4.2118115 -4.2674122 -4.308219 -4.3358274][-4.1913052 -4.1756229 -4.1483569 -4.128284 -4.1130543 -4.0540667 -3.9678342 -3.9341178 -3.9956498 -4.0826726 -4.1429849 -4.1962 -4.2624369 -4.3044252 -4.3344784][-4.2019262 -4.1880403 -4.1498232 -4.1051483 -4.0743747 -3.9979448 -3.874136 -3.8075857 -3.9044135 -4.0376568 -4.1242661 -4.1879244 -4.2558522 -4.2976537 -4.3294897][-4.2194457 -4.2050042 -4.1542411 -4.0829329 -4.028563 -3.9220366 -3.7381566 -3.6286292 -3.794564 -4.0032849 -4.1201186 -4.1777048 -4.2370129 -4.2809081 -4.3161793][-4.2417836 -4.2215652 -4.1617908 -4.0712495 -3.9946029 -3.8649585 -3.6475568 -3.5252051 -3.7539756 -4.0023532 -4.1221213 -4.1587844 -4.2080851 -4.2583637 -4.3006635][-4.2661262 -4.2418704 -4.1801705 -4.0885706 -4.0125241 -3.8997545 -3.7397425 -3.6641278 -3.8423698 -4.0327568 -4.1183167 -4.1326885 -4.174058 -4.2367229 -4.2901521][-4.279439 -4.2553511 -4.198122 -4.119194 -4.0587988 -3.984807 -3.8977206 -3.8517261 -3.9371681 -4.0455532 -4.0963311 -4.096478 -4.1349006 -4.2162547 -4.2830224][-4.2837729 -4.263145 -4.2172585 -4.1576109 -4.1108913 -4.0598855 -4.0076952 -3.9691167 -3.9962497 -4.04818 -4.0759673 -4.0761876 -4.12159 -4.2168827 -4.2870631][-4.2850738 -4.2706532 -4.2381172 -4.1972766 -4.1622934 -4.1251078 -4.087214 -4.0510817 -4.0481319 -4.0689759 -4.0852494 -4.0995288 -4.1547308 -4.2438111 -4.3040957][-4.2832665 -4.2735591 -4.2519073 -4.2239695 -4.1961846 -4.1685743 -4.1372628 -4.0980587 -4.0800562 -4.0877957 -4.1014056 -4.1303911 -4.1925406 -4.2718053 -4.3203607][-4.2813959 -4.275363 -4.2638741 -4.2447066 -4.221715 -4.1970873 -4.1652389 -4.1239905 -4.0986261 -4.0972104 -4.1079969 -4.1431327 -4.2075405 -4.2789578 -4.3200274]]...]
INFO - root - 2017-12-05 12:04:53.599074: step 5310, loss = 2.11, batch loss = 2.06 (8.4 examples/sec; 0.955 sec/batch; 86h:48m:07s remains)
INFO - root - 2017-12-05 12:05:02.732246: step 5320, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 82h:48m:33s remains)
INFO - root - 2017-12-05 12:05:11.800223: step 5330, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 83h:30m:52s remains)
INFO - root - 2017-12-05 12:05:20.805416: step 5340, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 79h:57m:29s remains)
INFO - root - 2017-12-05 12:05:29.782007: step 5350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 85h:35m:44s remains)
INFO - root - 2017-12-05 12:05:38.981853: step 5360, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 82h:54m:47s remains)
INFO - root - 2017-12-05 12:05:48.170256: step 5370, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 81h:38m:47s remains)
INFO - root - 2017-12-05 12:05:57.190000: step 5380, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 84h:15m:01s remains)
INFO - root - 2017-12-05 12:06:06.472717: step 5390, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 83h:10m:58s remains)
INFO - root - 2017-12-05 12:06:15.651086: step 5400, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 82h:16m:38s remains)
2017-12-05 12:06:16.415662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2911229 -4.2880898 -4.2893891 -4.2883129 -4.2867131 -4.2887855 -4.2934551 -4.30317 -4.3077035 -4.2996888 -4.29257 -4.2903266 -4.2886038 -4.2908707 -4.29853][-4.2755566 -4.2744865 -4.2768254 -4.2741323 -4.2702069 -4.2709808 -4.2758923 -4.2885814 -4.2948656 -4.2851229 -4.2744946 -4.2678661 -4.2645741 -4.2680988 -4.275363][-4.2575579 -4.2552114 -4.2538915 -4.2466941 -4.2383394 -4.2320681 -4.23335 -4.2503161 -4.2645321 -4.2611389 -4.2526231 -4.2455053 -4.2433138 -4.2498832 -4.2578897][-4.236331 -4.2234697 -4.2092175 -4.1920714 -4.1758308 -4.1568732 -4.1513309 -4.17848 -4.212327 -4.22875 -4.2322497 -4.2337079 -4.2390428 -4.2502317 -4.2594295][-4.2125645 -4.1856236 -4.1533971 -4.1234317 -4.0934205 -4.0561705 -4.0317354 -4.0684214 -4.1341286 -4.1805639 -4.2059727 -4.2208705 -4.2380328 -4.2547469 -4.266542][-4.1994004 -4.1623545 -4.1137538 -4.0635653 -4.0075235 -3.9362159 -3.8758597 -3.9063501 -4.0009141 -4.0873694 -4.1488767 -4.1906958 -4.2257991 -4.2551994 -4.2742333][-4.1995039 -4.1589828 -4.1011019 -4.0350676 -3.9564335 -3.8553843 -3.7617123 -3.7691007 -3.8682086 -3.9825873 -4.0770297 -4.1442509 -4.1986308 -4.2425771 -4.2731771][-4.2207689 -4.1945539 -4.1539989 -4.0989761 -4.0284395 -3.9371634 -3.8470802 -3.8325386 -3.8955536 -3.9857235 -4.0693321 -4.1337929 -4.1871705 -4.2314425 -4.2636752][-4.2452803 -4.2370906 -4.2236409 -4.1959209 -4.1531606 -4.0948191 -4.0289545 -4.0027146 -4.0198283 -4.0580897 -4.1023159 -4.14002 -4.1745563 -4.2094965 -4.2369537][-4.250061 -4.2513828 -4.2536874 -4.243906 -4.2222457 -4.1945105 -4.1542597 -4.1315055 -4.1231427 -4.1204553 -4.1288919 -4.1397243 -4.1514616 -4.1779332 -4.2017269][-4.2414656 -4.2447276 -4.2530184 -4.2533708 -4.2474308 -4.2458539 -4.2319508 -4.2228918 -4.2114382 -4.1868334 -4.1681948 -4.15848 -4.1551404 -4.1730051 -4.1913986][-4.2313242 -4.2252455 -4.2287159 -4.2291555 -4.2324624 -4.2462921 -4.2521992 -4.2619276 -4.2593775 -4.2329345 -4.204998 -4.1872096 -4.1807775 -4.1951861 -4.209619][-4.2313428 -4.2170835 -4.2127609 -4.208549 -4.2148547 -4.2357821 -4.2498693 -4.27002 -4.2770071 -4.2572861 -4.233994 -4.220222 -4.2167559 -4.2284846 -4.2390161][-4.2457519 -4.230444 -4.2236438 -4.2195139 -4.2252936 -4.240777 -4.2512913 -4.2677093 -4.277256 -4.2682734 -4.2570152 -4.2498126 -4.2482605 -4.2573013 -4.265749][-4.2573414 -4.2415528 -4.2325397 -4.2286191 -4.2308288 -4.238163 -4.241303 -4.2493067 -4.2567525 -4.256166 -4.2535696 -4.2514157 -4.2550316 -4.2683973 -4.2829866]]...]
INFO - root - 2017-12-05 12:06:25.330297: step 5410, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 79h:56m:02s remains)
INFO - root - 2017-12-05 12:06:34.378379: step 5420, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:27m:51s remains)
INFO - root - 2017-12-05 12:06:43.440499: step 5430, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:38m:46s remains)
INFO - root - 2017-12-05 12:06:52.271201: step 5440, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 83h:55m:56s remains)
INFO - root - 2017-12-05 12:07:01.591633: step 5450, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 85h:39m:19s remains)
INFO - root - 2017-12-05 12:07:10.827823: step 5460, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 86h:47m:03s remains)
INFO - root - 2017-12-05 12:07:19.826734: step 5470, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 84h:21m:01s remains)
INFO - root - 2017-12-05 12:07:28.969555: step 5480, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.961 sec/batch; 87h:20m:00s remains)
INFO - root - 2017-12-05 12:07:38.206003: step 5490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 84h:11m:46s remains)
INFO - root - 2017-12-05 12:07:47.312443: step 5500, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 84h:22m:24s remains)
2017-12-05 12:07:48.051658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1609092 -4.157217 -4.1506891 -4.1522975 -4.1546993 -4.1640682 -4.1783648 -4.1965709 -4.2152195 -4.2136688 -4.2080684 -4.2145991 -4.202024 -4.1824946 -4.1909833][-4.1462755 -4.1426959 -4.1396632 -4.1439004 -4.1457167 -4.1536207 -4.1670666 -4.1827631 -4.2009745 -4.1984887 -4.1883736 -4.1940012 -4.1845641 -4.1723757 -4.1878757][-4.1339831 -4.1315093 -4.1348042 -4.1413903 -4.1418405 -4.1522665 -4.1722474 -4.1891971 -4.2050371 -4.1984415 -4.1798129 -4.1810141 -4.1754775 -4.1696625 -4.1904941][-4.1251826 -4.1250138 -4.1313848 -4.1346197 -4.1378288 -4.1508942 -4.1730452 -4.1868548 -4.1984735 -4.1928554 -4.1712804 -4.1663284 -4.1641374 -4.1642 -4.190721][-4.1259475 -4.1316924 -4.13539 -4.1318617 -4.12763 -4.13015 -4.1460915 -4.1616507 -4.1730576 -4.1741147 -4.156755 -4.1476221 -4.1419458 -4.14195 -4.1730609][-4.1081576 -4.1138692 -4.1173205 -4.1102381 -4.0871181 -4.0717449 -4.0923643 -4.1259751 -4.146831 -4.1557612 -4.1445584 -4.1308718 -4.1190634 -4.1130137 -4.1426578][-4.053194 -4.0470276 -4.0495977 -4.0418177 -3.9832177 -3.9205894 -3.9532812 -4.0376921 -4.0888638 -4.1121206 -4.1121593 -4.1101584 -4.1062403 -4.0996685 -4.1238775][-3.9907124 -3.9585814 -3.9549122 -3.9423978 -3.8525517 -3.7169309 -3.739434 -3.8863893 -3.9907327 -4.0408788 -4.0505342 -4.0671306 -4.0910387 -4.0966063 -4.1155086][-3.9948494 -3.9630766 -3.9601614 -3.9628415 -3.8921964 -3.7510114 -3.7344871 -3.8657751 -3.9753256 -4.0291133 -4.0445342 -4.0677691 -4.1063957 -4.1202693 -4.1315231][-4.0433245 -4.0329328 -4.0442619 -4.0668807 -4.0371189 -3.9494965 -3.9285312 -3.9942865 -4.0608783 -4.0932074 -4.1039352 -4.1228142 -4.1530833 -4.1613541 -4.164958][-4.0883436 -4.091918 -4.1094332 -4.1372705 -4.1343136 -4.0928016 -4.0791698 -4.1027646 -4.135251 -4.1456728 -4.1411085 -4.1541219 -4.1840892 -4.1949587 -4.1979814][-4.1201043 -4.1307282 -4.146286 -4.1686296 -4.1753073 -4.1589904 -4.1486874 -4.1482654 -4.1568832 -4.1549072 -4.1357527 -4.1423831 -4.1723204 -4.1874523 -4.1956968][-4.1325068 -4.155458 -4.1729245 -4.1911712 -4.1955385 -4.1903424 -4.1832719 -4.1721182 -4.1635237 -4.1462512 -4.1117983 -4.1092839 -4.1365738 -4.1556864 -4.1699061][-4.1284285 -4.1607347 -4.17968 -4.1948118 -4.1996717 -4.2040162 -4.2032108 -4.1919584 -4.1748414 -4.14858 -4.1141038 -4.1139927 -4.1403394 -4.1597252 -4.1720376][-4.1438355 -4.1709247 -4.1809492 -4.1896753 -4.194593 -4.2001181 -4.2004147 -4.1939764 -4.1833553 -4.1659241 -4.1460824 -4.1549897 -4.1806006 -4.1994896 -4.2093992]]...]
INFO - root - 2017-12-05 12:07:57.223720: step 5510, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 83h:45m:25s remains)
INFO - root - 2017-12-05 12:08:06.416528: step 5520, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 82h:21m:35s remains)
INFO - root - 2017-12-05 12:08:15.472501: step 5530, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 80h:11m:32s remains)
INFO - root - 2017-12-05 12:08:24.409054: step 5540, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 84h:25m:09s remains)
INFO - root - 2017-12-05 12:08:33.628572: step 5550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 83h:44m:56s remains)
INFO - root - 2017-12-05 12:08:42.818142: step 5560, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 81h:52m:01s remains)
INFO - root - 2017-12-05 12:08:51.755003: step 5570, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 82h:51m:39s remains)
INFO - root - 2017-12-05 12:09:00.812048: step 5580, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 81h:05m:27s remains)
INFO - root - 2017-12-05 12:09:09.908095: step 5590, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 83h:23m:10s remains)
INFO - root - 2017-12-05 12:09:19.122250: step 5600, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 84h:06m:17s remains)
2017-12-05 12:09:19.960616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2447186 -4.2290387 -4.2165761 -4.2038393 -4.191164 -4.1851034 -4.1896787 -4.2009358 -4.2158875 -4.2351708 -4.250577 -4.2618837 -4.2656374 -4.2645421 -4.2650533][-4.2245746 -4.2012773 -4.1830211 -4.1664095 -4.1507974 -4.1420169 -4.1409979 -4.1446314 -4.1573262 -4.1824574 -4.2074213 -4.2306781 -4.2435513 -4.2468681 -4.2465348][-4.2130256 -4.181026 -4.1582861 -4.1423841 -4.1254172 -4.1092086 -4.0952821 -4.0825095 -4.0890474 -4.1196208 -4.1595521 -4.2003393 -4.2251415 -4.2340479 -4.2321262][-4.2080607 -4.1689725 -4.1464958 -4.1348805 -4.1173949 -4.087822 -4.0508714 -4.0177245 -4.0156894 -4.0488791 -4.1047125 -4.1655431 -4.20526 -4.22031 -4.2199774][-4.2036486 -4.1642609 -4.1472254 -4.1437016 -4.125268 -4.0825524 -4.0199938 -3.963387 -3.9448833 -3.9715395 -4.0415812 -4.1292257 -4.1877985 -4.2107849 -4.2139935][-4.1950669 -4.1584563 -4.1476388 -4.1489 -4.1317115 -4.0814567 -4.000042 -3.9274762 -3.8880749 -3.8946109 -3.9738221 -4.0919423 -4.1719165 -4.2033672 -4.2107148][-4.1736393 -4.1391883 -4.1355867 -4.1472707 -4.1427774 -4.1046133 -4.0257163 -3.9389462 -3.8593626 -3.8243852 -3.9040494 -4.0447149 -4.1405935 -4.1774373 -4.1906075][-4.147676 -4.1123905 -4.1126604 -4.1326976 -4.1413512 -4.118052 -4.0496716 -3.9540234 -3.8387303 -3.7725878 -3.8566113 -4.0108967 -4.1136227 -4.14961 -4.1649785][-4.1531043 -4.1161289 -4.1134772 -4.1369147 -4.1545639 -4.1422825 -4.0897675 -4.0053539 -3.9020762 -3.8462586 -3.9175274 -4.0453687 -4.12869 -4.151649 -4.1585045][-4.19553 -4.1584253 -4.1462173 -4.1646581 -4.1835909 -4.1787953 -4.1420937 -4.0780244 -4.0045843 -3.9658716 -4.0083475 -4.0954218 -4.1536727 -4.1656251 -4.1647635][-4.2499466 -4.2162747 -4.1966429 -4.202436 -4.2151442 -4.2142067 -4.1923842 -4.1488438 -4.0990839 -4.0711079 -4.0868068 -4.1379519 -4.1806192 -4.1921749 -4.1935787][-4.2954459 -4.272274 -4.2547736 -4.2535396 -4.2578249 -4.2556219 -4.2401304 -4.2116356 -4.1783772 -4.1571321 -4.1578884 -4.1847887 -4.2186179 -4.2365308 -4.2438526][-4.3197594 -4.3080077 -4.2972035 -4.2937489 -4.2904024 -4.285553 -4.2729607 -4.2532496 -4.2317934 -4.2181959 -4.2163076 -4.2304339 -4.2566452 -4.277585 -4.2876635][-4.3265753 -4.3221993 -4.3178024 -4.3141866 -4.3104334 -4.3069825 -4.2997227 -4.2874637 -4.2751908 -4.2673469 -4.2626033 -4.2664232 -4.2819366 -4.300539 -4.3122892][-4.3231235 -4.32197 -4.3212771 -4.3197017 -4.3170629 -4.3136029 -4.3099847 -4.3033915 -4.2991848 -4.2952 -4.2876267 -4.2855825 -4.2957544 -4.3127131 -4.3246779]]...]
INFO - root - 2017-12-05 12:09:29.094227: step 5610, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 82h:45m:40s remains)
INFO - root - 2017-12-05 12:09:38.020641: step 5620, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 79h:54m:27s remains)
INFO - root - 2017-12-05 12:09:47.009350: step 5630, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 83h:25m:29s remains)
INFO - root - 2017-12-05 12:09:56.322440: step 5640, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 85h:30m:42s remains)
INFO - root - 2017-12-05 12:10:05.372053: step 5650, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 81h:40m:27s remains)
INFO - root - 2017-12-05 12:10:14.388169: step 5660, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 80h:10m:29s remains)
INFO - root - 2017-12-05 12:10:23.491705: step 5670, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 82h:49m:06s remains)
INFO - root - 2017-12-05 12:10:32.447038: step 5680, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 79h:40m:41s remains)
INFO - root - 2017-12-05 12:10:41.488586: step 5690, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 86h:34m:44s remains)
INFO - root - 2017-12-05 12:10:50.556695: step 5700, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 81h:12m:00s remains)
2017-12-05 12:10:51.300854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2164073 -4.2555971 -4.267426 -4.2658119 -4.2545996 -4.2433386 -4.2435832 -4.2453175 -4.238687 -4.2467213 -4.2651129 -4.269403 -4.2354965 -4.1794677 -4.1210546][-4.2346983 -4.262671 -4.2704563 -4.2703347 -4.254293 -4.23578 -4.235497 -4.2328811 -4.216774 -4.2203708 -4.2432137 -4.2554121 -4.2262597 -4.1745076 -4.1159697][-4.2420974 -4.2615671 -4.2683311 -4.2737589 -4.2602587 -4.2398262 -4.2304873 -4.2145028 -4.1861463 -4.1888208 -4.2212582 -4.2388144 -4.2074318 -4.16005 -4.1088276][-4.2410617 -4.25447 -4.2619605 -4.2706232 -4.2593441 -4.2358913 -4.2150054 -4.1907716 -4.1576815 -4.1658921 -4.2092395 -4.2334075 -4.2033477 -4.15773 -4.1122122][-4.2441678 -4.2531781 -4.2613087 -4.2689486 -4.2527704 -4.2233143 -4.187984 -4.1556368 -4.1273026 -4.1439886 -4.1909242 -4.2196789 -4.1959829 -4.1483607 -4.1056738][-4.2415471 -4.2528706 -4.2676363 -4.2744365 -4.2490606 -4.207675 -4.1498456 -4.0969634 -4.0748181 -4.1056743 -4.1641073 -4.2011185 -4.1811309 -4.1335578 -4.092473][-4.2323079 -4.2553492 -4.2746844 -4.2752185 -4.2405429 -4.1821036 -4.0991135 -4.0232091 -4.0049987 -4.0526891 -4.132462 -4.180728 -4.1636763 -4.1222014 -4.0821667][-4.2285781 -4.2535758 -4.2666087 -4.2598648 -4.2225275 -4.1618633 -4.0749373 -3.9946134 -3.97758 -4.0230994 -4.1057658 -4.1576657 -4.1460233 -4.1129055 -4.0734315][-4.2345281 -4.2529354 -4.254458 -4.2443228 -4.2147093 -4.1718225 -4.10825 -4.0429559 -4.0206919 -4.0433474 -4.1035433 -4.1510525 -4.1462951 -4.1219435 -4.0848851][-4.2421293 -4.256897 -4.252594 -4.2414889 -4.2229705 -4.194488 -4.1529708 -4.1064315 -4.0795178 -4.0842867 -4.119288 -4.1559043 -4.1502776 -4.1267734 -4.0965261][-4.2534347 -4.2632947 -4.2565041 -4.2451224 -4.2292423 -4.2059569 -4.1812315 -4.1520004 -4.1300406 -4.1305728 -4.1525764 -4.1778107 -4.1698532 -4.1511521 -4.132503][-4.2473769 -4.2588224 -4.2566586 -4.2496614 -4.2380824 -4.2195115 -4.206306 -4.1917887 -4.1746664 -4.1780953 -4.1957107 -4.2119861 -4.2030683 -4.1908073 -4.1792741][-4.23621 -4.2507563 -4.254478 -4.2553105 -4.2497964 -4.236588 -4.2293391 -4.2205544 -4.2087612 -4.2153258 -4.2306013 -4.239336 -4.229301 -4.219646 -4.213058][-4.2243476 -4.2415996 -4.249815 -4.2558532 -4.254653 -4.2475576 -4.244288 -4.2378893 -4.2288475 -4.2358618 -4.2479935 -4.2498364 -4.2350273 -4.224113 -4.220149][-4.2197404 -4.2381878 -4.2468286 -4.2552552 -4.2580891 -4.2543015 -4.254827 -4.250145 -4.2412071 -4.2402873 -4.2437391 -4.2369633 -4.2150216 -4.2019839 -4.2015719]]...]
INFO - root - 2017-12-05 12:11:00.445932: step 5710, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 81h:55m:21s remains)
INFO - root - 2017-12-05 12:11:09.337174: step 5720, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 80h:30m:56s remains)
INFO - root - 2017-12-05 12:11:18.403003: step 5730, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 85h:16m:20s remains)
INFO - root - 2017-12-05 12:11:27.614590: step 5740, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 83h:18m:53s remains)
INFO - root - 2017-12-05 12:11:36.864455: step 5750, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 82h:31m:35s remains)
INFO - root - 2017-12-05 12:11:45.944332: step 5760, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 84h:18m:53s remains)
INFO - root - 2017-12-05 12:11:55.050374: step 5770, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 88h:42m:28s remains)
INFO - root - 2017-12-05 12:12:04.245766: step 5780, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.946 sec/batch; 85h:50m:18s remains)
INFO - root - 2017-12-05 12:12:13.443160: step 5790, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.894 sec/batch; 81h:05m:28s remains)
INFO - root - 2017-12-05 12:12:22.468652: step 5800, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 84h:40m:51s remains)
2017-12-05 12:12:23.217179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1834087 -4.16448 -4.1489491 -4.1499047 -4.1644263 -4.183713 -4.2092137 -4.2307587 -4.2356696 -4.2199578 -4.1846676 -4.1527448 -4.1437941 -4.1406841 -4.1316385][-4.159585 -4.1563511 -4.1505322 -4.153194 -4.1665692 -4.1839948 -4.2068863 -4.2236362 -4.2269821 -4.2179294 -4.19633 -4.1721659 -4.1616516 -4.1613965 -4.1614246][-4.1546731 -4.1671453 -4.1734571 -4.1783776 -4.1814723 -4.1849332 -4.1987453 -4.2082405 -4.2072406 -4.205554 -4.201745 -4.1929188 -4.1875753 -4.1893578 -4.192986][-4.1466813 -4.163444 -4.1738625 -4.1769309 -4.16775 -4.156189 -4.1652613 -4.1763611 -4.1808295 -4.1902409 -4.1995625 -4.2016435 -4.199234 -4.2009935 -4.2031479][-4.1180067 -4.1281934 -4.1371861 -4.13567 -4.1138897 -4.0925694 -4.1026878 -4.1243639 -4.1385603 -4.1577864 -4.1762938 -4.1840672 -4.1825061 -4.1796484 -4.1734552][-4.0705748 -4.070888 -4.0761156 -4.0644069 -4.029202 -3.99563 -4.0060091 -4.0402594 -4.068377 -4.0972538 -4.1266222 -4.1448379 -4.1477709 -4.1434469 -4.1336517][-4.0357265 -4.0146561 -3.9965947 -3.9667346 -3.9144957 -3.8569214 -3.8492956 -3.905458 -3.9691436 -4.0195847 -4.0650487 -4.1025534 -4.1235709 -4.1302524 -4.1257954][-4.0431294 -4.0024891 -3.9548521 -3.8925703 -3.8102248 -3.712297 -3.6580772 -3.7226675 -3.8378282 -3.926259 -3.9929941 -4.0562835 -4.1050782 -4.1276784 -4.1378946][-4.0878844 -4.04009 -3.9795046 -3.9037962 -3.8162246 -3.7132003 -3.6375914 -3.6807649 -3.7996235 -3.8946209 -3.9662645 -4.0340466 -4.0941806 -4.1248107 -4.1458368][-4.1677346 -4.1275692 -4.0723739 -4.0060482 -3.9389281 -3.8692014 -3.816591 -3.8323181 -3.899827 -3.9597661 -4.0075231 -4.0537143 -4.1024413 -4.13025 -4.1548615][-4.2422352 -4.2132854 -4.1690922 -4.1188383 -4.0697722 -4.0229273 -3.9924364 -3.9982331 -4.0313778 -4.0644679 -4.0930591 -4.1179447 -4.1526871 -4.1735067 -4.1927977][-4.28494 -4.2669125 -4.2401152 -4.2109466 -4.180789 -4.1547923 -4.1406369 -4.1423774 -4.1557813 -4.173851 -4.1904387 -4.2032695 -4.22384 -4.2386818 -4.2500629][-4.3034997 -4.2929926 -4.2808166 -4.2708745 -4.261415 -4.2558436 -4.2546878 -4.25554 -4.2574358 -4.2647152 -4.2727642 -4.2792525 -4.2891784 -4.2960258 -4.2968721][-4.3065662 -4.3007979 -4.29675 -4.2960315 -4.2971163 -4.301374 -4.3067665 -4.3121424 -4.3143878 -4.3182988 -4.3238378 -4.3268976 -4.3304944 -4.3314638 -4.3268309][-4.3079314 -4.3031511 -4.3021493 -4.30432 -4.3078279 -4.3130016 -4.3192477 -4.3260188 -4.33144 -4.3371038 -4.3415661 -4.34393 -4.3444057 -4.3412662 -4.3344455]]...]
INFO - root - 2017-12-05 12:12:32.100604: step 5810, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 75h:30m:05s remains)
INFO - root - 2017-12-05 12:12:40.973637: step 5820, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 82h:00m:57s remains)
INFO - root - 2017-12-05 12:12:50.027858: step 5830, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.947 sec/batch; 85h:53m:51s remains)
INFO - root - 2017-12-05 12:12:59.170172: step 5840, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 81h:00m:56s remains)
INFO - root - 2017-12-05 12:13:08.191017: step 5850, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 83h:41m:23s remains)
INFO - root - 2017-12-05 12:13:17.338237: step 5860, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 83h:59m:00s remains)
INFO - root - 2017-12-05 12:13:26.349340: step 5870, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 80h:52m:03s remains)
INFO - root - 2017-12-05 12:13:35.369146: step 5880, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 82h:46m:09s remains)
INFO - root - 2017-12-05 12:13:44.519428: step 5890, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 83h:01m:29s remains)
INFO - root - 2017-12-05 12:13:53.687282: step 5900, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 85h:07m:10s remains)
2017-12-05 12:13:54.523803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2207794 -4.2274246 -4.2382627 -4.2489305 -4.2649984 -4.2713032 -4.2576432 -4.2415576 -4.2385125 -4.2463303 -4.2495127 -4.2494059 -4.2485671 -4.2393131 -4.2176108][-4.1702895 -4.1767774 -4.1934114 -4.2111411 -4.2274556 -4.2333045 -4.2182288 -4.1947842 -4.1902981 -4.2017136 -4.209774 -4.2089014 -4.2071018 -4.1935039 -4.1659064][-4.1287937 -4.1341629 -4.1511831 -4.1733475 -4.1950674 -4.203136 -4.1840281 -4.1505742 -4.1410437 -4.1573553 -4.1715426 -4.1730056 -4.1731629 -4.1584592 -4.1270676][-4.0889449 -4.098278 -4.1159306 -4.1451054 -4.1710286 -4.1747379 -4.1484981 -4.1055679 -4.0875216 -4.1094894 -4.1334133 -4.1369982 -4.1411724 -4.1311574 -4.1010995][-4.0642891 -4.0764284 -4.0956612 -4.1295819 -4.15202 -4.1436167 -4.1040778 -4.0484529 -4.0194407 -4.0460734 -4.086422 -4.1021228 -4.1163692 -4.1159506 -4.0945516][-4.063427 -4.0711241 -4.0884194 -4.1189356 -4.1340156 -4.1096849 -4.0497184 -3.9810746 -3.9452405 -3.9766819 -4.0367227 -4.0746374 -4.1075716 -4.118341 -4.1055665][-4.092854 -4.0899477 -4.0976954 -4.1172934 -4.1186352 -4.0708671 -3.979635 -3.8928676 -3.8661005 -3.9046474 -3.9817393 -4.0485454 -4.1062241 -4.1304927 -4.1262569][-4.1439466 -4.1331549 -4.123858 -4.1267219 -4.1109905 -4.0408335 -3.9240184 -3.8311341 -3.8278034 -3.871171 -3.9480062 -4.0311728 -4.1099148 -4.1459761 -4.1485662][-4.1724348 -4.1630659 -4.1509113 -4.1519566 -4.1334195 -4.066812 -3.96179 -3.8882737 -3.8956308 -3.930423 -3.9763479 -4.0446115 -4.1231232 -4.1600595 -4.1551962][-4.1730108 -4.1687932 -4.1638737 -4.1739573 -4.16823 -4.1244426 -4.0514722 -3.9996262 -3.9998631 -4.017065 -4.0342493 -4.0797167 -4.1456418 -4.1749725 -4.158143][-4.1735668 -4.1704636 -4.17096 -4.1916246 -4.1976976 -4.1771979 -4.1364732 -4.1024828 -4.092998 -4.09488 -4.0975623 -4.1284933 -4.1775866 -4.1979113 -4.1730561][-4.1821265 -4.1779304 -4.1804695 -4.2045875 -4.218255 -4.2156396 -4.2018456 -4.1850109 -4.1743569 -4.1700416 -4.1667924 -4.1839347 -4.2129526 -4.2221928 -4.1964059][-4.2002549 -4.2000041 -4.2082644 -4.2324877 -4.2461753 -4.2542906 -4.2541265 -4.2508278 -4.2449961 -4.2401519 -4.2378044 -4.2442203 -4.2546296 -4.25233 -4.226675][-4.2303195 -4.2402706 -4.2564077 -4.2796106 -4.286685 -4.2954879 -4.297308 -4.296433 -4.2943583 -4.29269 -4.2941933 -4.2960539 -4.2971239 -4.2845187 -4.25691][-4.2702804 -4.285861 -4.3032112 -4.3214903 -4.3236294 -4.3268423 -4.3268328 -4.3249431 -4.3242078 -4.3242226 -4.3273296 -4.328681 -4.32676 -4.3125257 -4.2881303]]...]
INFO - root - 2017-12-05 12:14:03.393143: step 5910, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 82h:01m:20s remains)
INFO - root - 2017-12-05 12:14:12.633111: step 5920, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 82h:09m:09s remains)
INFO - root - 2017-12-05 12:14:21.577944: step 5930, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 83h:21m:26s remains)
INFO - root - 2017-12-05 12:14:30.678758: step 5940, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 83h:29m:51s remains)
INFO - root - 2017-12-05 12:14:39.656976: step 5950, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 79h:21m:16s remains)
INFO - root - 2017-12-05 12:14:48.745639: step 5960, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 83h:22m:57s remains)
INFO - root - 2017-12-05 12:14:57.689851: step 5970, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 80h:16m:48s remains)
INFO - root - 2017-12-05 12:15:06.711395: step 5980, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 83h:48m:30s remains)
INFO - root - 2017-12-05 12:15:15.690770: step 5990, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 84h:09m:37s remains)
INFO - root - 2017-12-05 12:15:24.747492: step 6000, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 75h:29m:40s remains)
2017-12-05 12:15:25.541627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2251468 -4.2156835 -4.19184 -4.1629162 -4.1362391 -4.1226892 -4.1253633 -4.1314344 -4.1293569 -4.1354046 -4.14251 -4.1380558 -4.1222634 -4.1124954 -4.1350212][-4.2001061 -4.204402 -4.1879854 -4.1632166 -4.1342382 -4.1139221 -4.1051106 -4.1003466 -4.0918465 -4.100781 -4.1195273 -4.1300292 -4.1248479 -4.1169829 -4.1373343][-4.172246 -4.1856136 -4.1795034 -4.1634903 -4.1385694 -4.1143403 -4.0954628 -4.0809979 -4.0684533 -4.0798244 -4.10756 -4.1326432 -4.1354909 -4.134922 -4.1543579][-4.1487942 -4.168323 -4.1755505 -4.1723981 -4.1564021 -4.1323409 -4.1047144 -4.0742865 -4.0499048 -4.0531015 -4.0874982 -4.1251154 -4.1418896 -4.1535482 -4.1774015][-4.1361923 -4.1551747 -4.1713505 -4.1814518 -4.1767149 -4.156559 -4.11995 -4.0721731 -4.0266109 -4.0124755 -4.0446944 -4.0934706 -4.1301575 -4.1605744 -4.1946511][-4.1358981 -4.1449842 -4.1604571 -4.1767526 -4.1833525 -4.1684828 -4.1251354 -4.0657835 -4.0018291 -3.9686368 -3.9900837 -4.0437365 -4.1006441 -4.1509056 -4.2001996][-4.1369715 -4.1329985 -4.136292 -4.147954 -4.1595592 -4.1508918 -4.1129937 -4.0595589 -4.003511 -3.9619253 -3.9695697 -4.0188417 -4.0803204 -4.138207 -4.1976848][-4.1534061 -4.1401243 -4.129921 -4.1285558 -4.1364708 -4.1331062 -4.1081138 -4.0752521 -4.0419703 -4.0055194 -4.0051394 -4.0473771 -4.1020465 -4.1530762 -4.2104025][-4.1834288 -4.171124 -4.1611247 -4.1535239 -4.1520219 -4.1483173 -4.130548 -4.1137414 -4.0994897 -4.077364 -4.0793061 -4.117147 -4.1605744 -4.1969891 -4.2423196][-4.2135577 -4.2090354 -4.2043791 -4.1961994 -4.1883073 -4.1823359 -4.1673217 -4.1577258 -4.1528172 -4.1440315 -4.1505032 -4.1829967 -4.2134213 -4.2371774 -4.270164][-4.2387753 -4.2431774 -4.245677 -4.2414966 -4.2326756 -4.2237482 -4.2097106 -4.2007432 -4.1980891 -4.1973691 -4.2070847 -4.2335591 -4.2541585 -4.27123 -4.2949519][-4.2552624 -4.2627883 -4.2696691 -4.270113 -4.2626729 -4.2534218 -4.2437778 -4.2387972 -4.2404137 -4.2438974 -4.2539124 -4.2718005 -4.2825165 -4.2935152 -4.3086033][-4.2606516 -4.2656083 -4.2718849 -4.2754011 -4.272862 -4.2677178 -4.2633715 -4.2625537 -4.266243 -4.2709365 -4.2795577 -4.2905674 -4.2953653 -4.3010864 -4.3094115][-4.2602673 -4.2609711 -4.2639389 -4.2679849 -4.27051 -4.2707005 -4.2700925 -4.2701564 -4.272789 -4.27691 -4.2836413 -4.2918272 -4.2958875 -4.2991209 -4.3036757][-4.2631125 -4.26175 -4.2626562 -4.2650228 -4.267797 -4.2690158 -4.2692261 -4.26973 -4.2718849 -4.2753549 -4.2805023 -4.2868838 -4.2916451 -4.2962537 -4.3010631]]...]
INFO - root - 2017-12-05 12:15:34.820058: step 6010, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 84h:48m:40s remains)
INFO - root - 2017-12-05 12:15:44.170786: step 6020, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 85h:09m:12s remains)
INFO - root - 2017-12-05 12:15:53.271153: step 6030, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.925 sec/batch; 83h:52m:08s remains)
INFO - root - 2017-12-05 12:16:02.262421: step 6040, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 81h:00m:04s remains)
INFO - root - 2017-12-05 12:16:11.337241: step 6050, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 81h:01m:08s remains)
INFO - root - 2017-12-05 12:16:20.375292: step 6060, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 83h:24m:07s remains)
INFO - root - 2017-12-05 12:16:29.516488: step 6070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 83h:56m:34s remains)
INFO - root - 2017-12-05 12:16:38.643666: step 6080, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 81h:47m:16s remains)
INFO - root - 2017-12-05 12:16:47.688462: step 6090, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 81h:48m:53s remains)
INFO - root - 2017-12-05 12:16:56.643437: step 6100, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 81h:30m:22s remains)
2017-12-05 12:16:57.437491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25751 -4.2610407 -4.2635722 -4.2663331 -4.268446 -4.2710762 -4.2758493 -4.282876 -4.2918835 -4.3010521 -4.3075309 -4.3066916 -4.3087659 -4.3119278 -4.3188648][-4.2991529 -4.3012595 -4.3029222 -4.3050461 -4.3088212 -4.31469 -4.3216572 -4.3284135 -4.3360062 -4.3434548 -4.3495927 -4.3485837 -4.3456421 -4.341887 -4.3402219][-4.3297205 -4.32883 -4.3271666 -4.324779 -4.3249516 -4.3281374 -4.3320026 -4.3350749 -4.3399253 -4.3470683 -4.3557453 -4.35995 -4.35875 -4.3552933 -4.3513513][-4.3225317 -4.3184409 -4.3113203 -4.3027716 -4.297401 -4.2962866 -4.2964635 -4.2981715 -4.30422 -4.3134909 -4.3256421 -4.3374529 -4.3439875 -4.3470073 -4.3468919][-4.2836237 -4.2759519 -4.2609124 -4.2430086 -4.2310958 -4.225986 -4.2217045 -4.2203741 -4.2297564 -4.2456007 -4.2657318 -4.2870111 -4.3026094 -4.3137212 -4.3213291][-4.2388563 -4.2331452 -4.2159128 -4.1921005 -4.1733413 -4.1614332 -4.1504378 -4.1449413 -4.1577511 -4.1790924 -4.2052975 -4.2333703 -4.2539616 -4.2687392 -4.2814665][-4.2006431 -4.2013216 -4.1882944 -4.16269 -4.1357741 -4.1132073 -4.0922565 -4.0818253 -4.0953593 -4.1220231 -4.1553507 -4.1879878 -4.2073989 -4.2205725 -4.2351232][-4.1884608 -4.1929417 -4.1810927 -4.1548376 -4.123683 -4.0956783 -4.0702353 -4.0598435 -4.074913 -4.1037459 -4.1369629 -4.1653919 -4.1775541 -4.1844563 -4.1959267][-4.2039676 -4.2083974 -4.1966267 -4.1741242 -4.1471467 -4.124567 -4.1078153 -4.1077595 -4.1262422 -4.1520648 -4.1776156 -4.193079 -4.1927176 -4.1879907 -4.1898408][-4.2346964 -4.2393131 -4.2315717 -4.2189565 -4.2014909 -4.186945 -4.1788864 -4.1828027 -4.1993604 -4.2201519 -4.2389035 -4.247098 -4.239593 -4.2276659 -4.2212567][-4.2748966 -4.2768321 -4.271348 -4.2668929 -4.2588086 -4.2520452 -4.2492371 -4.2518616 -4.2625313 -4.2757115 -4.2882857 -4.2931695 -4.2858486 -4.27359 -4.2643814][-4.3158245 -4.3163481 -4.3125696 -4.3099432 -4.30376 -4.2985415 -4.295115 -4.2938623 -4.2988477 -4.3066649 -4.3136115 -4.315556 -4.311564 -4.30359 -4.295753][-4.3323178 -4.3331442 -4.3289866 -4.3228126 -4.3138204 -4.3061419 -4.2997174 -4.295383 -4.2988653 -4.3049045 -4.3082304 -4.3077083 -4.307476 -4.3053226 -4.3019013][-4.3209124 -4.3240609 -4.3233342 -4.317306 -4.3095145 -4.3017874 -4.2921481 -4.284234 -4.2841043 -4.2872305 -4.288353 -4.2878151 -4.2909493 -4.2942433 -4.2970738][-4.3006158 -4.3057804 -4.3094115 -4.3072371 -4.3053832 -4.3031034 -4.2961082 -4.2897811 -4.2882113 -4.2883921 -4.2868619 -4.2844787 -4.2878146 -4.2935343 -4.2997837]]...]
INFO - root - 2017-12-05 12:17:06.601396: step 6110, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 82h:56m:19s remains)
INFO - root - 2017-12-05 12:17:15.688895: step 6120, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.925 sec/batch; 83h:49m:31s remains)
INFO - root - 2017-12-05 12:17:24.915569: step 6130, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 82h:47m:54s remains)
INFO - root - 2017-12-05 12:17:33.888841: step 6140, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 80h:45m:43s remains)
INFO - root - 2017-12-05 12:17:43.003424: step 6150, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 85h:28m:18s remains)
INFO - root - 2017-12-05 12:17:52.095253: step 6160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 84h:08m:32s remains)
INFO - root - 2017-12-05 12:18:01.228195: step 6170, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:08m:23s remains)
INFO - root - 2017-12-05 12:18:10.291335: step 6180, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 83h:49m:12s remains)
INFO - root - 2017-12-05 12:18:19.299175: step 6190, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 83h:38m:10s remains)
INFO - root - 2017-12-05 12:18:28.357975: step 6200, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.900 sec/batch; 81h:32m:40s remains)
2017-12-05 12:18:29.170411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2111206 -4.1992588 -4.1939654 -4.2026496 -4.207787 -4.2041736 -4.1882386 -4.1439466 -4.0893307 -4.0711303 -4.1081181 -4.1542721 -4.1802216 -4.2001863 -4.2361331][-4.2232223 -4.2145967 -4.2139215 -4.221446 -4.2228947 -4.2148809 -4.1933413 -4.1474881 -4.0974736 -4.0845537 -4.1234074 -4.1741629 -4.2076921 -4.238162 -4.2734442][-4.2350616 -4.2332892 -4.2349052 -4.23872 -4.236167 -4.2218208 -4.1909418 -4.1400719 -4.0966949 -4.0879617 -4.1273952 -4.1865926 -4.2311797 -4.2715187 -4.3024797][-4.2478585 -4.2515368 -4.2565851 -4.2585411 -4.2491746 -4.2214875 -4.17406 -4.11535 -4.0782814 -4.0782704 -4.1232243 -4.1921039 -4.2476683 -4.2917886 -4.3128233][-4.2604904 -4.2680488 -4.27407 -4.2715988 -4.2488956 -4.2019868 -4.1329856 -4.057117 -4.0227714 -4.0442948 -4.1080408 -4.1906128 -4.2565031 -4.3017316 -4.3110037][-4.2671385 -4.2749405 -4.2789931 -4.2697506 -4.2330694 -4.1645522 -4.0696216 -3.9753435 -3.9599018 -4.0212274 -4.1096983 -4.1993771 -4.2682981 -4.305778 -4.2988162][-4.2650385 -4.2686076 -4.2688289 -4.25494 -4.206574 -4.1162591 -3.9983296 -3.9039843 -3.927861 -4.0290151 -4.1322474 -4.2191105 -4.2788792 -4.3017707 -4.2803745][-4.2467966 -4.24664 -4.2451491 -4.2273717 -4.1691818 -4.0686545 -3.955148 -3.8971167 -3.956166 -4.0651746 -4.1604009 -4.2320843 -4.2751861 -4.2826724 -4.2513475][-4.21656 -4.2192016 -4.220161 -4.1988621 -4.132998 -4.0379114 -3.9569685 -3.9474523 -4.020268 -4.113337 -4.1878324 -4.2385492 -4.2600913 -4.2518768 -4.2133107][-4.1877947 -4.1989603 -4.2022486 -4.1747007 -4.1032529 -4.0212073 -3.9782209 -4.0042744 -4.0806494 -4.1564188 -4.212667 -4.2446709 -4.2474723 -4.2252541 -4.183238][-4.1732707 -4.18618 -4.1819968 -4.1471791 -4.0761113 -4.0153646 -4.008121 -4.0558643 -4.1292911 -4.190105 -4.2334142 -4.2524581 -4.2436051 -4.2125735 -4.1675358][-4.1749167 -4.1802011 -4.1667056 -4.1300559 -4.0692759 -4.033205 -4.0504346 -4.1039457 -4.1693635 -4.2217531 -4.2581825 -4.2694607 -4.2516947 -4.2116575 -4.1619725][-4.1829863 -4.1798453 -4.1599488 -4.1250157 -4.0806665 -4.0679321 -4.0951042 -4.1436119 -4.1994758 -4.246594 -4.2773943 -4.2841082 -4.2584867 -4.2105041 -4.1550717][-4.1921587 -4.1854305 -4.1628375 -4.1297693 -4.1021705 -4.1036077 -4.1327343 -4.1734982 -4.2183743 -4.2547212 -4.2763224 -4.2775979 -4.2474384 -4.197845 -4.1413908][-4.2005281 -4.1980429 -4.1797686 -4.1550145 -4.1376214 -4.1388159 -4.1603575 -4.1916442 -4.2233934 -4.2444839 -4.2552476 -4.250875 -4.2200766 -4.1749258 -4.12235]]...]
INFO - root - 2017-12-05 12:18:38.248663: step 6210, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 83h:58m:59s remains)
INFO - root - 2017-12-05 12:18:47.375161: step 6220, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 84h:09m:08s remains)
INFO - root - 2017-12-05 12:18:56.358588: step 6230, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 80h:17m:36s remains)
INFO - root - 2017-12-05 12:19:05.335483: step 6240, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 81h:33m:39s remains)
INFO - root - 2017-12-05 12:19:14.405278: step 6250, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.929 sec/batch; 84h:13m:44s remains)
INFO - root - 2017-12-05 12:19:23.541566: step 6260, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 83h:44m:20s remains)
INFO - root - 2017-12-05 12:19:32.724601: step 6270, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 86h:23m:12s remains)
INFO - root - 2017-12-05 12:19:41.816940: step 6280, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 82h:27m:47s remains)
INFO - root - 2017-12-05 12:19:50.844947: step 6290, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 79h:06m:33s remains)
INFO - root - 2017-12-05 12:20:00.180639: step 6300, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 82h:57m:21s remains)
2017-12-05 12:20:00.975461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2590084 -4.2574768 -4.2485723 -4.2377687 -4.2238951 -4.2169738 -4.2083359 -4.1996894 -4.186522 -4.1629605 -4.1390553 -4.122283 -4.1176534 -4.1197586 -4.1345673][-4.2916074 -4.2990265 -4.2955742 -4.2782531 -4.25726 -4.2438526 -4.2280216 -4.2133117 -4.2004504 -4.182848 -4.161387 -4.1488328 -4.1467047 -4.1524534 -4.175456][-4.2967792 -4.3057475 -4.3058114 -4.2914782 -4.2760987 -4.2664061 -4.2514305 -4.2362237 -4.2207479 -4.2024 -4.184042 -4.1800113 -4.183691 -4.1927624 -4.2162976][-4.2846422 -4.2911654 -4.2919183 -4.2826233 -4.2739635 -4.2645645 -4.250113 -4.2369809 -4.2218456 -4.2002587 -4.1861629 -4.1924844 -4.20319 -4.2151203 -4.2367711][-4.2691336 -4.2749443 -4.2727227 -4.2617378 -4.2506385 -4.2339234 -4.2145653 -4.2001314 -4.1882696 -4.166986 -4.1632719 -4.1864977 -4.2049747 -4.2216473 -4.2398467][-4.2381144 -4.2386723 -4.2291417 -4.212945 -4.1942225 -4.170125 -4.1400771 -4.1181927 -4.1071391 -4.0894227 -4.0978284 -4.1382918 -4.1660848 -4.1896276 -4.2093682][-4.1829009 -4.182663 -4.1672029 -4.142096 -4.1134081 -4.0788245 -4.0325589 -4.0001564 -3.9888265 -3.9672227 -3.9793015 -4.0345907 -4.0713015 -4.1070833 -4.1447992][-4.1071072 -4.099946 -4.0750771 -4.0413475 -3.9995267 -3.9474938 -3.8850207 -3.8541579 -3.8473239 -3.8204741 -3.841136 -3.9167311 -3.9634151 -4.0113292 -4.0711541][-4.0415025 -4.025702 -3.999542 -3.9672775 -3.9227748 -3.8655 -3.8019824 -3.7803819 -3.7773564 -3.7487936 -3.779772 -3.8685811 -3.9218922 -3.9707344 -4.0311017][-4.0288577 -4.0142684 -4.0011568 -3.9842725 -3.9564519 -3.9208598 -3.883837 -3.8794031 -3.8820524 -3.8612146 -3.8902345 -3.965349 -4.0079775 -4.0361218 -4.0659208][-4.0666409 -4.0597 -4.0600142 -4.0588427 -4.0506606 -4.0385838 -4.0242143 -4.0296454 -4.0387135 -4.0293403 -4.0522294 -4.1070552 -4.1380391 -4.13945 -4.128562][-4.138319 -4.13873 -4.145977 -4.15227 -4.1524644 -4.1500483 -4.1468291 -4.1520653 -4.1572781 -4.1523657 -4.1712837 -4.2119884 -4.2323222 -4.2180414 -4.1811237][-4.210968 -4.2182751 -4.2299557 -4.2376833 -4.2361283 -4.2332706 -4.2310843 -4.2320228 -4.2312918 -4.2259192 -4.2389655 -4.2654123 -4.2751069 -4.2558036 -4.2179132][-4.2624707 -4.2719135 -4.2841964 -4.2912884 -4.2879133 -4.2816472 -4.2769408 -4.2732544 -4.2660766 -4.2570195 -4.2592177 -4.2654428 -4.2598267 -4.2382493 -4.2168651][-4.2869759 -4.293838 -4.3014412 -4.3048663 -4.3007345 -4.2945118 -4.2889457 -4.2807608 -4.2685089 -4.2541361 -4.2425175 -4.2300081 -4.2110529 -4.1899014 -4.18951]]...]
INFO - root - 2017-12-05 12:20:10.094309: step 6310, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 86h:20m:44s remains)
INFO - root - 2017-12-05 12:20:19.205202: step 6320, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 85h:55m:06s remains)
INFO - root - 2017-12-05 12:20:28.264704: step 6330, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 84h:32m:57s remains)
INFO - root - 2017-12-05 12:20:37.388106: step 6340, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 83h:49m:41s remains)
INFO - root - 2017-12-05 12:20:46.580633: step 6350, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 83h:42m:09s remains)
INFO - root - 2017-12-05 12:20:55.648102: step 6360, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 81h:16m:08s remains)
INFO - root - 2017-12-05 12:21:04.679011: step 6370, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 81h:07m:29s remains)
INFO - root - 2017-12-05 12:21:13.569960: step 6380, loss = 2.05, batch loss = 2.00 (8.1 examples/sec; 0.989 sec/batch; 89h:37m:29s remains)
INFO - root - 2017-12-05 12:21:22.747421: step 6390, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 85h:24m:35s remains)
INFO - root - 2017-12-05 12:21:31.925382: step 6400, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 81h:23m:29s remains)
2017-12-05 12:21:32.741369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2586794 -4.2619147 -4.2517805 -4.2371006 -4.2219791 -4.2061563 -4.1878972 -4.1745343 -4.1735005 -4.1744542 -4.1839056 -4.1954241 -4.1883712 -4.1638427 -4.1460981][-4.2567782 -4.2563324 -4.2420411 -4.223249 -4.2029395 -4.1831508 -4.1678572 -4.1623869 -4.1699786 -4.172338 -4.1793141 -4.1915488 -4.18668 -4.1645508 -4.1494212][-4.2425132 -4.2436404 -4.2284336 -4.2057648 -4.1767359 -4.146666 -4.13343 -4.1409397 -4.1615815 -4.1699882 -4.1741452 -4.1817708 -4.1807756 -4.164784 -4.1523433][-4.2308192 -4.235023 -4.2207212 -4.191505 -4.1497207 -4.1073141 -4.0951118 -4.1181545 -4.1493831 -4.1614351 -4.1606913 -4.1628242 -4.1649461 -4.1574121 -4.1498108][-4.2250538 -4.2307048 -4.2152257 -4.1790304 -4.1261239 -4.0776958 -4.0709643 -4.1067476 -4.1443253 -4.1554394 -4.1447215 -4.1367793 -4.1367893 -4.1329703 -4.1329579][-4.2208166 -4.2299032 -4.2133026 -4.1731048 -4.1158981 -4.068018 -4.0633564 -4.10096 -4.1363025 -4.1419835 -4.123065 -4.1107864 -4.11074 -4.1135545 -4.1254592][-4.2101107 -4.2213535 -4.2094884 -4.1703992 -4.1174655 -4.0726123 -4.0650878 -4.0974374 -4.129303 -4.1331205 -4.1188803 -4.1169419 -4.12329 -4.1372976 -4.1578259][-4.1965775 -4.2007275 -4.1915541 -4.1587434 -4.1106825 -4.0678978 -4.0570827 -4.0840726 -4.1159754 -4.1240358 -4.1209936 -4.1364903 -4.1557188 -4.180913 -4.2055926][-4.191885 -4.1851907 -4.1750951 -4.1504507 -4.1110806 -4.0738664 -4.06364 -4.0833392 -4.1101732 -4.1214466 -4.1261868 -4.156013 -4.189085 -4.2233219 -4.249166][-4.1974764 -4.1859179 -4.1752925 -4.1594486 -4.133049 -4.1053414 -4.0946259 -4.10344 -4.11865 -4.132956 -4.1458559 -4.180191 -4.2135758 -4.2416081 -4.2576408][-4.2050796 -4.1913114 -4.1828346 -4.1746349 -4.1592855 -4.1411157 -4.13026 -4.1295767 -4.1333365 -4.14665 -4.1633968 -4.1944761 -4.2192054 -4.2310424 -4.2306614][-4.2191648 -4.2066464 -4.1985731 -4.1952252 -4.1868739 -4.1743217 -4.1625781 -4.1568389 -4.1586041 -4.1707735 -4.1864381 -4.2049236 -4.2146149 -4.2103977 -4.1990085][-4.23049 -4.2202406 -4.2129192 -4.2126126 -4.2106166 -4.2033539 -4.1948195 -4.1946259 -4.2025065 -4.2119751 -4.2169085 -4.2153463 -4.2071118 -4.1908751 -4.1740193][-4.2340684 -4.22529 -4.2186646 -4.2174883 -4.2167964 -4.2141991 -4.2110043 -4.2176046 -4.2287211 -4.2334981 -4.2276554 -4.2141666 -4.2002039 -4.1824427 -4.163445][-4.2414875 -4.2359662 -4.2297997 -4.2259231 -4.2227736 -4.2202916 -4.2184262 -4.2258072 -4.2334213 -4.2329407 -4.2227855 -4.2095275 -4.1991324 -4.1840248 -4.1652474]]...]
INFO - root - 2017-12-05 12:21:41.787263: step 6410, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 83h:13m:00s remains)
INFO - root - 2017-12-05 12:21:50.988477: step 6420, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 84h:05m:38s remains)
INFO - root - 2017-12-05 12:22:00.005129: step 6430, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 82h:40m:51s remains)
INFO - root - 2017-12-05 12:22:08.959915: step 6440, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 81h:51m:47s remains)
INFO - root - 2017-12-05 12:22:18.174584: step 6450, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 83h:52m:53s remains)
INFO - root - 2017-12-05 12:22:27.350848: step 6460, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 82h:25m:31s remains)
INFO - root - 2017-12-05 12:22:36.147853: step 6470, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 74h:46m:31s remains)
INFO - root - 2017-12-05 12:22:45.196129: step 6480, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 82h:35m:12s remains)
INFO - root - 2017-12-05 12:22:54.366166: step 6490, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 82h:46m:31s remains)
INFO - root - 2017-12-05 12:23:03.637171: step 6500, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.888 sec/batch; 80h:23m:31s remains)
2017-12-05 12:23:04.379537: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2923565 -4.2815118 -4.2665534 -4.2570577 -4.2475605 -4.2385345 -4.23164 -4.2139764 -4.1900978 -4.1672916 -4.1486864 -4.1548243 -4.1825628 -4.2327566 -4.2753282][-4.2893538 -4.278388 -4.2647352 -4.2584858 -4.2540884 -4.2447119 -4.2299967 -4.2005033 -4.1667156 -4.1459966 -4.1363468 -4.145206 -4.1802578 -4.2354188 -4.2822771][-4.2806559 -4.2685938 -4.2523475 -4.2436476 -4.2391744 -4.2269249 -4.2072845 -4.1719017 -4.1314039 -4.1187973 -4.1208191 -4.1349154 -4.1740522 -4.2355914 -4.2861891][-4.2726727 -4.259264 -4.2395906 -4.224566 -4.2149396 -4.1995649 -4.174634 -4.1385169 -4.0895853 -4.0761991 -4.0853252 -4.1044211 -4.1550083 -4.2271566 -4.2847848][-4.2687397 -4.2549405 -4.2326574 -4.2084727 -4.1910357 -4.16612 -4.1407871 -4.1041212 -4.0455737 -4.0224676 -4.0364876 -4.064589 -4.1312385 -4.2164536 -4.2811022][-4.2686434 -4.2551589 -4.2333322 -4.201457 -4.1703196 -4.1345091 -4.1000457 -4.0551405 -3.9946115 -3.9740255 -3.9998674 -4.0478916 -4.1255293 -4.2140789 -4.2765121][-4.2721057 -4.258153 -4.2351952 -4.1983395 -4.1524215 -4.0970173 -4.0400314 -3.9847836 -3.9435782 -3.9476838 -3.9930863 -4.0625582 -4.1455665 -4.2261996 -4.2718554][-4.2731938 -4.256887 -4.2319117 -4.1914577 -4.1252751 -4.0400295 -3.9558539 -3.9048991 -3.9035854 -3.9483378 -4.0152736 -4.0997047 -4.1814451 -4.2460361 -4.2718339][-4.2676587 -4.2439432 -4.2117872 -4.159008 -4.0651264 -3.9435663 -3.8442686 -3.8185856 -3.870306 -3.9533203 -4.0420957 -4.1346192 -4.2116504 -4.2600784 -4.2724471][-4.2544236 -4.2233191 -4.1825657 -4.116127 -4.0027237 -3.8568165 -3.7534773 -3.7543664 -3.8411412 -3.9456146 -4.05121 -4.1479697 -4.2200732 -4.2557707 -4.2598467][-4.24093 -4.2013435 -4.1528397 -4.0844407 -3.9809983 -3.8517835 -3.7642932 -3.7736778 -3.8604026 -3.9653621 -4.0694103 -4.1586533 -4.2228608 -4.2513037 -4.2531552][-4.2306528 -4.1839881 -4.130177 -4.0655823 -3.9933724 -3.9085302 -3.8557804 -3.8749325 -3.945827 -4.0294895 -4.1125903 -4.1821938 -4.232728 -4.2542958 -4.2560358][-4.2219429 -4.1730418 -4.1160927 -4.0587425 -4.0115466 -3.9651158 -3.9494925 -3.9804039 -4.0358353 -4.0942698 -4.1510119 -4.2011967 -4.2367311 -4.2552795 -4.259099][-4.2210259 -4.1759272 -4.1236029 -4.0753593 -4.0410018 -4.014833 -4.0196133 -4.0577054 -4.1034927 -4.1448421 -4.1860271 -4.2204003 -4.2430573 -4.2575288 -4.2637615][-4.2361641 -4.2007556 -4.1601405 -4.1221995 -4.0940938 -4.0770421 -4.0863142 -4.1203814 -4.1579533 -4.1889524 -4.2192154 -4.2432518 -4.2581611 -4.2690387 -4.2746844]]...]
INFO - root - 2017-12-05 12:23:13.334792: step 6510, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 79h:39m:03s remains)
INFO - root - 2017-12-05 12:23:22.360793: step 6520, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 78h:00m:12s remains)
INFO - root - 2017-12-05 12:23:31.480065: step 6530, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 81h:18m:09s remains)
INFO - root - 2017-12-05 12:23:40.630166: step 6540, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.889 sec/batch; 80h:29m:07s remains)
INFO - root - 2017-12-05 12:23:49.682266: step 6550, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 82h:42m:58s remains)
INFO - root - 2017-12-05 12:23:58.746583: step 6560, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 81h:13m:39s remains)
INFO - root - 2017-12-05 12:24:07.601521: step 6570, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 82h:07m:23s remains)
INFO - root - 2017-12-05 12:24:16.596063: step 6580, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 83h:51m:44s remains)
INFO - root - 2017-12-05 12:24:25.806330: step 6590, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 82h:26m:34s remains)
INFO - root - 2017-12-05 12:24:34.890399: step 6600, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 81h:08m:55s remains)
2017-12-05 12:24:35.631746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2810531 -4.2357831 -4.183917 -4.1387386 -4.1031232 -4.0864244 -4.0835118 -4.0764055 -4.07157 -4.0876427 -4.106904 -4.1036463 -4.0908713 -4.0749159 -4.0670533][-4.2766881 -4.2235909 -4.1635814 -4.1154985 -4.0809817 -4.0679083 -4.0640936 -4.0540113 -4.0450621 -4.0553284 -4.07541 -4.0791483 -4.0795469 -4.07254 -4.0617695][-4.2729 -4.2160273 -4.1532955 -4.1028733 -4.068099 -4.0571074 -4.0544748 -4.0456991 -4.0340476 -4.0381756 -4.0566969 -4.0712252 -4.0874739 -4.0903621 -4.0721669][-4.2698336 -4.2119637 -4.1494484 -4.097466 -4.0613036 -4.0497665 -4.0519772 -4.0538745 -4.0506258 -4.0525179 -4.0657516 -4.0796013 -4.095561 -4.0989614 -4.0785308][-4.2649188 -4.2036991 -4.139647 -4.089179 -4.055964 -4.0430632 -4.0476689 -4.0582042 -4.0699863 -4.0821657 -4.0910397 -4.0957384 -4.1040139 -4.1079121 -4.092659][-4.2615232 -4.197844 -4.1314759 -4.0817323 -4.0524673 -4.0356121 -4.0326338 -4.0484982 -4.0742612 -4.0974808 -4.10289 -4.1020126 -4.1086464 -4.1169877 -4.1108584][-4.2560859 -4.190186 -4.1209326 -4.0713034 -4.0399814 -4.0120473 -3.9954274 -4.0127549 -4.0527787 -4.083468 -4.08964 -4.0856624 -4.0920753 -4.1039667 -4.1078339][-4.2498555 -4.1828761 -4.1115112 -4.0583391 -4.018292 -3.9710724 -3.9366295 -3.9556186 -4.0042782 -4.043993 -4.0602865 -4.0616727 -4.0677524 -4.0824971 -4.0926123][-4.2471657 -4.1851368 -4.1211491 -4.0689411 -4.0237045 -3.9636109 -3.9155843 -3.9228723 -3.96452 -4.0101585 -4.0394478 -4.0484271 -4.0512314 -4.060647 -4.0701079][-4.25132 -4.1970911 -4.1460366 -4.1049776 -4.0667067 -4.0102773 -3.9595213 -3.952632 -3.9819341 -4.0259848 -4.0539656 -4.0542464 -4.0420952 -4.0377254 -4.0412817][-4.2598596 -4.2125769 -4.1689358 -4.1364503 -4.1076355 -4.0649662 -4.0248146 -4.0133314 -4.0296712 -4.0671444 -4.087718 -4.0727482 -4.0421767 -4.0249066 -4.0237389][-4.2664056 -4.2224674 -4.18067 -4.1493134 -4.1228328 -4.0958781 -4.0749836 -4.0680604 -4.0738473 -4.0972085 -4.1117878 -4.092999 -4.0543423 -4.0292764 -4.0289149][-4.2736597 -4.2313967 -4.1872396 -4.1512403 -4.1288376 -4.1145949 -4.1090198 -4.1109319 -4.1105027 -4.1196089 -4.1242967 -4.1055336 -4.0725822 -4.0498352 -4.0506158][-4.2890358 -4.2483315 -4.2017484 -4.1624932 -4.1425481 -4.140305 -4.1458974 -4.1525674 -4.1516738 -4.1516824 -4.1485944 -4.132885 -4.1065454 -4.0880427 -4.0890532][-4.3114142 -4.2771649 -4.2330742 -4.1984677 -4.1835823 -4.1894884 -4.2008519 -4.2088294 -4.20667 -4.2014675 -4.1971507 -4.1882172 -4.1681962 -4.1473851 -4.1400046]]...]
INFO - root - 2017-12-05 12:24:44.759467: step 6610, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 82h:10m:49s remains)
INFO - root - 2017-12-05 12:24:54.011781: step 6620, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 78h:53m:06s remains)
INFO - root - 2017-12-05 12:25:03.091551: step 6630, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 81h:49m:40s remains)
INFO - root - 2017-12-05 12:25:12.125511: step 6640, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 81h:57m:21s remains)
INFO - root - 2017-12-05 12:25:21.248601: step 6650, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 84h:10m:44s remains)
INFO - root - 2017-12-05 12:25:29.993074: step 6660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 82h:48m:45s remains)
INFO - root - 2017-12-05 12:25:39.032756: step 6670, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 83h:07m:34s remains)
INFO - root - 2017-12-05 12:25:48.311749: step 6680, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 86h:32m:01s remains)
INFO - root - 2017-12-05 12:25:57.315770: step 6690, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 83h:14m:55s remains)
INFO - root - 2017-12-05 12:26:06.472089: step 6700, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 84h:24m:47s remains)
2017-12-05 12:26:07.257069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2883005 -4.2620955 -4.2379355 -4.2202234 -4.2076826 -4.209486 -4.2296171 -4.2482309 -4.2470112 -4.2337813 -4.2192955 -4.2157655 -4.2224689 -4.2376089 -4.2576747][-4.2851911 -4.260972 -4.2361917 -4.2148318 -4.2031155 -4.2103691 -4.2274656 -4.2361774 -4.2277565 -4.2124968 -4.194077 -4.1885109 -4.1977777 -4.2158909 -4.2371931][-4.2815895 -4.257566 -4.230299 -4.2069569 -4.196208 -4.2055559 -4.2109246 -4.201438 -4.1860924 -4.1748934 -4.1607919 -4.1590385 -4.1742969 -4.1978426 -4.22246][-4.2773695 -4.2506022 -4.2209358 -4.1967587 -4.184216 -4.1858592 -4.1678934 -4.1337538 -4.1138015 -4.1100206 -4.1059732 -4.1160603 -4.1478586 -4.181603 -4.2123241][-4.26976 -4.240037 -4.2080441 -4.18154 -4.1624966 -4.1448917 -4.0938349 -4.0344987 -4.0165429 -4.0285797 -4.0419993 -4.0705104 -4.1187663 -4.1631026 -4.2012396][-4.2608037 -4.2280183 -4.1908469 -4.159379 -4.1290789 -4.0812521 -3.9910898 -3.9125457 -3.9081182 -3.9477487 -3.9834461 -4.030992 -4.0916204 -4.1444764 -4.1856494][-4.2549543 -4.218823 -4.1768875 -4.1383924 -4.0884414 -3.9981992 -3.869247 -3.7843163 -3.8070362 -3.8724468 -3.9367294 -4.0074635 -4.0764356 -4.1311707 -4.1777363][-4.2473474 -4.2065954 -4.1586852 -4.1100054 -4.0344605 -3.9076071 -3.76584 -3.7065876 -3.7667718 -3.8540244 -3.9367151 -4.0196786 -4.0864825 -4.1361647 -4.1849418][-4.2405744 -4.1961665 -4.1445055 -4.0904918 -4.00587 -3.8799067 -3.7736411 -3.7520065 -3.8243551 -3.9098914 -3.9871325 -4.0585313 -4.1118526 -4.1545033 -4.1997404][-4.2403316 -4.2002635 -4.1560788 -4.1122866 -4.0413518 -3.9449298 -3.8824174 -3.8803084 -3.9336898 -3.9978542 -4.0526271 -4.1055651 -4.1490135 -4.1856232 -4.2245502][-4.2435136 -4.2119384 -4.1795764 -4.1494741 -4.1004791 -4.0337048 -4.0009956 -4.0064311 -4.0403662 -4.0838504 -4.1190958 -4.1548033 -4.1905246 -4.2212877 -4.2529254][-4.2468009 -4.2212133 -4.1989961 -4.183187 -4.1576762 -4.1166029 -4.096468 -4.101213 -4.122777 -4.1548595 -4.1787653 -4.2033281 -4.2306008 -4.2570124 -4.2832842][-4.2506838 -4.2261744 -4.2085042 -4.2014465 -4.1913834 -4.1725378 -4.1638284 -4.1704187 -4.1881604 -4.21559 -4.2337942 -4.2539129 -4.2732029 -4.29372 -4.3142762][-4.2571869 -4.230618 -4.2142258 -4.2121134 -4.2134328 -4.2114258 -4.2161613 -4.2299547 -4.2504191 -4.273653 -4.2865505 -4.3008056 -4.3131714 -4.3251405 -4.3390312][-4.2681322 -4.2411003 -4.2234497 -4.2224631 -4.2275915 -4.2345619 -4.2490096 -4.2700486 -4.2954578 -4.3154373 -4.3246818 -4.334013 -4.33973 -4.3430657 -4.3484144]]...]
INFO - root - 2017-12-05 12:26:16.462183: step 6710, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 81h:00m:23s remains)
INFO - root - 2017-12-05 12:26:25.616559: step 6720, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 84h:12m:50s remains)
INFO - root - 2017-12-05 12:26:34.678501: step 6730, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 81h:47m:32s remains)
INFO - root - 2017-12-05 12:26:43.765077: step 6740, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:18m:47s remains)
INFO - root - 2017-12-05 12:26:52.922111: step 6750, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 84h:09m:11s remains)
INFO - root - 2017-12-05 12:27:01.866340: step 6760, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 83h:35m:13s remains)
INFO - root - 2017-12-05 12:27:10.973381: step 6770, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 82h:37m:28s remains)
INFO - root - 2017-12-05 12:27:19.979304: step 6780, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 82h:54m:27s remains)
INFO - root - 2017-12-05 12:27:29.084580: step 6790, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 81h:01m:29s remains)
INFO - root - 2017-12-05 12:27:38.097973: step 6800, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 80h:22m:07s remains)
2017-12-05 12:27:38.906384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21745 -4.1920962 -4.1797915 -4.1764054 -4.1919575 -4.2281189 -4.25422 -4.2534819 -4.2418423 -4.2453241 -4.2479029 -4.2430573 -4.2325816 -4.2223387 -4.1996627][-4.2180839 -4.18898 -4.1688871 -4.1600962 -4.1724787 -4.2062778 -4.230679 -4.2326646 -4.2241154 -4.2304983 -4.2402143 -4.2334175 -4.2195969 -4.2081847 -4.1846876][-4.2152219 -4.1846681 -4.1600003 -4.1457715 -4.1515789 -4.17642 -4.1917877 -4.1944337 -4.1908073 -4.2038631 -4.2212276 -4.2148252 -4.198575 -4.18443 -4.154635][-4.2154694 -4.1840544 -4.1581268 -4.1400895 -4.137382 -4.1488385 -4.1510816 -4.1501036 -4.1475859 -4.1634917 -4.186492 -4.1789603 -4.1583114 -4.1383643 -4.1016216][-4.2155848 -4.1842775 -4.1577778 -4.1395826 -4.1284347 -4.1287541 -4.1232285 -4.1207781 -4.117753 -4.1357021 -4.16109 -4.1545129 -4.1350379 -4.1132007 -4.0734][-4.2181373 -4.1892834 -4.1608696 -4.141726 -4.127882 -4.12776 -4.1195483 -4.116219 -4.1128011 -4.1323986 -4.1592684 -4.1572008 -4.1464911 -4.1290612 -4.0931144][-4.2189527 -4.1904244 -4.1578307 -4.1318321 -4.1171727 -4.1228576 -4.1165123 -4.1094084 -4.1018753 -4.1219788 -4.1513915 -4.15832 -4.160399 -4.1490693 -4.1179404][-4.2099676 -4.1718292 -4.1302257 -4.0992694 -4.0874677 -4.0936184 -4.0819263 -4.0693884 -4.0559969 -4.0784512 -4.1149812 -4.1367049 -4.1485281 -4.1437459 -4.1244221][-4.1937518 -4.1403828 -4.0867829 -4.0495582 -4.0405126 -4.0416307 -4.0195785 -3.9942994 -3.9733567 -4.0012808 -4.0481768 -4.0826297 -4.1064324 -4.1167021 -4.1165137][-4.1820178 -4.1146107 -4.0502968 -4.00116 -3.986165 -3.9848685 -3.9630463 -3.9358268 -3.9127333 -3.9415393 -3.9939551 -4.0357633 -4.0696955 -4.0950809 -4.1133046][-4.1903696 -4.124135 -4.0602574 -4.008234 -3.9881392 -3.9884381 -3.9809244 -3.9657764 -3.9509625 -3.9747584 -4.01763 -4.0541992 -4.0901952 -4.1232338 -4.1511283][-4.2205181 -4.1664433 -4.1140962 -4.0707555 -4.0539141 -4.0573654 -4.0606446 -4.0579224 -4.0514112 -4.0666466 -4.0937591 -4.1229048 -4.1570926 -4.1908388 -4.2166796][-4.2617588 -4.2243338 -4.1871824 -4.1561852 -4.1433315 -4.1446996 -4.1482263 -4.1508093 -4.1497893 -4.15849 -4.1743932 -4.1963215 -4.2223954 -4.2474556 -4.2656279][-4.296186 -4.27386 -4.2486763 -4.2292962 -4.2201676 -4.2198663 -4.2226696 -4.2253132 -4.2247324 -4.2270083 -4.2346473 -4.2483249 -4.2644153 -4.2799959 -4.29147][-4.3142738 -4.3038483 -4.2898626 -4.2788877 -4.2745566 -4.2738466 -4.2756748 -4.2774696 -4.27698 -4.277451 -4.2799821 -4.2853775 -4.2925439 -4.3000035 -4.3055997]]...]
INFO - root - 2017-12-05 12:27:47.927261: step 6810, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 83h:06m:19s remains)
INFO - root - 2017-12-05 12:27:56.927058: step 6820, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 76h:00m:31s remains)
INFO - root - 2017-12-05 12:28:05.857499: step 6830, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 81h:31m:52s remains)
INFO - root - 2017-12-05 12:28:15.066409: step 6840, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 87h:46m:43s remains)
INFO - root - 2017-12-05 12:28:24.035186: step 6850, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 82h:57m:24s remains)
INFO - root - 2017-12-05 12:28:33.073079: step 6860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 78h:50m:36s remains)
INFO - root - 2017-12-05 12:28:42.072352: step 6870, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 81h:10m:30s remains)
INFO - root - 2017-12-05 12:28:51.115967: step 6880, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 81h:29m:21s remains)
INFO - root - 2017-12-05 12:29:00.216418: step 6890, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 84h:11m:58s remains)
INFO - root - 2017-12-05 12:29:09.222682: step 6900, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 80h:24m:51s remains)
2017-12-05 12:29:10.003026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2126088 -4.1861286 -4.1785369 -4.1743221 -4.1586018 -4.1357021 -4.119698 -4.1320219 -4.1711612 -4.2054672 -4.2287755 -4.245347 -4.2540388 -4.23667 -4.1986141][-4.2000127 -4.1695714 -4.1603851 -4.1569748 -4.14159 -4.1113148 -4.0870075 -4.1026764 -4.1579494 -4.2031054 -4.2278466 -4.2400222 -4.2400422 -4.213347 -4.1664281][-4.1836872 -4.1497459 -4.1400061 -4.1394143 -4.1326513 -4.1094971 -4.0842514 -4.10387 -4.1689467 -4.2137308 -4.2338495 -4.2391858 -4.2317519 -4.1994276 -4.1433845][-4.1756377 -4.136025 -4.1236725 -4.1247072 -4.1267385 -4.1108441 -4.0822349 -4.0926652 -4.1500134 -4.1926789 -4.2192473 -4.2296038 -4.2250862 -4.1959839 -4.1431537][-4.1785383 -4.1389503 -4.1228743 -4.1215644 -4.1225138 -4.1019015 -4.0543561 -4.0429649 -4.0947967 -4.1516285 -4.1924663 -4.214323 -4.21799 -4.2023015 -4.157588][-4.1861868 -4.1460977 -4.12511 -4.1161132 -4.106142 -4.0636277 -3.9792542 -3.9334993 -3.9925823 -4.0864911 -4.1541328 -4.1897769 -4.2042212 -4.2052627 -4.1753278][-4.1989975 -4.1626625 -4.1340714 -4.1058331 -4.0697246 -3.9858732 -3.84405 -3.7639709 -3.8657789 -4.0222349 -4.1267214 -4.1788645 -4.2092905 -4.2276368 -4.2125869][-4.209301 -4.1734166 -4.1395874 -4.1041903 -4.0494986 -3.9245663 -3.7288666 -3.6369433 -3.7937231 -3.9901032 -4.1103892 -4.1729369 -4.217772 -4.2471104 -4.2397122][-4.2108784 -4.1715937 -4.1415806 -4.1162858 -4.0680065 -3.9522583 -3.7940741 -3.7375093 -3.8646049 -4.0218439 -4.1204195 -4.175952 -4.2257266 -4.2548513 -4.2478986][-4.2086496 -4.1662488 -4.1425962 -4.1270523 -4.0940561 -4.0185928 -3.9391613 -3.9231832 -3.9999449 -4.0936813 -4.1552553 -4.1870613 -4.2237015 -4.2418237 -4.2313051][-4.2152972 -4.1752105 -4.1581492 -4.1456952 -4.1188974 -4.0669141 -4.0256639 -4.0242133 -4.0711179 -4.1269474 -4.16626 -4.1888423 -4.2101016 -4.2156353 -4.1990685][-4.2323456 -4.199048 -4.1829815 -4.1711659 -4.1456776 -4.1057892 -4.074666 -4.0717521 -4.0957451 -4.1294165 -4.1577206 -4.1770272 -4.1908641 -4.1974258 -4.1867018][-4.2444916 -4.2174721 -4.1984577 -4.1830869 -4.16305 -4.1375585 -4.1124454 -4.1114163 -4.1283293 -4.1470447 -4.1677732 -4.1810122 -4.1907635 -4.1991544 -4.196022][-4.2503119 -4.2265987 -4.2071972 -4.1906447 -4.17598 -4.1589112 -4.1378059 -4.1404943 -4.1563153 -4.1684475 -4.1863012 -4.1988139 -4.2128706 -4.223516 -4.218998][-4.25319 -4.2324176 -4.2174182 -4.2031169 -4.1912894 -4.1776261 -4.159368 -4.162066 -4.1759448 -4.18952 -4.2097321 -4.226912 -4.2476993 -4.2588482 -4.2520905]]...]
INFO - root - 2017-12-05 12:29:19.046070: step 6910, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 81h:45m:51s remains)
INFO - root - 2017-12-05 12:29:28.280792: step 6920, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 85h:03m:43s remains)
INFO - root - 2017-12-05 12:29:37.272265: step 6930, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 80h:42m:32s remains)
INFO - root - 2017-12-05 12:29:46.062011: step 6940, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 81h:18m:20s remains)
INFO - root - 2017-12-05 12:29:55.172199: step 6950, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 82h:41m:26s remains)
INFO - root - 2017-12-05 12:30:04.256699: step 6960, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 76h:03m:53s remains)
INFO - root - 2017-12-05 12:30:13.289987: step 6970, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 81h:40m:44s remains)
INFO - root - 2017-12-05 12:30:22.319036: step 6980, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 82h:40m:37s remains)
INFO - root - 2017-12-05 12:30:31.342024: step 6990, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 79h:58m:33s remains)
INFO - root - 2017-12-05 12:30:40.508124: step 7000, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 83h:41m:11s remains)
2017-12-05 12:30:41.255097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0917454 -4.127758 -4.1295738 -4.1214972 -4.1016407 -4.0795836 -4.0723619 -4.0701046 -4.0642118 -4.056551 -4.0489769 -4.0531859 -4.0658569 -4.0805683 -4.0897369][-4.0800514 -4.1093483 -4.1177988 -4.115449 -4.1002727 -4.0850048 -4.0810413 -4.0775533 -4.0710979 -4.0643511 -4.0540066 -4.0557032 -4.0633759 -4.069303 -4.067749][-4.0692663 -4.0854063 -4.0937719 -4.0919113 -4.0780878 -4.0702572 -4.0714808 -4.0687938 -4.0676775 -4.0702362 -4.0681663 -4.0719566 -4.064321 -4.0539012 -4.0440726][-4.0715566 -4.0774908 -4.0844975 -4.08565 -4.0740666 -4.0627437 -4.0576391 -4.0462565 -4.0394111 -4.0555668 -4.0722132 -4.0807319 -4.0570855 -4.0381975 -4.0337596][-4.0805883 -4.0876789 -4.0940046 -4.0942893 -4.0859671 -4.0686164 -4.0481424 -4.0201459 -4.0020123 -4.032269 -4.0711231 -4.0890975 -4.0622115 -4.0427132 -4.0477309][-4.0718875 -4.0823689 -4.092391 -4.0940175 -4.0940557 -4.0764289 -4.038209 -3.9833391 -3.9550648 -4.0105824 -4.07742 -4.1117439 -4.0995345 -4.0883884 -4.0926843][-4.0365129 -4.0472736 -4.0690894 -4.0849257 -4.0907445 -4.06885 -4.0057907 -3.9133735 -3.8881385 -3.9892619 -4.084589 -4.1318779 -4.1367745 -4.12875 -4.1244912][-3.9794114 -3.9881852 -4.0246663 -4.0553432 -4.0662856 -4.0351171 -3.9474142 -3.8334084 -3.830209 -3.971966 -4.0774808 -4.1275496 -4.1446342 -4.1389618 -4.1264982][-3.9432271 -3.9521623 -4.0010095 -4.0406766 -4.0503507 -4.0154109 -3.9305089 -3.8433044 -3.8638043 -3.9891717 -4.0720668 -4.110857 -4.1300879 -4.1267052 -4.1141138][-3.9472551 -3.9677508 -4.0226431 -4.0600543 -4.0658674 -4.038588 -3.9794183 -3.9314711 -3.9530602 -4.030736 -4.0748267 -4.0935836 -4.1087718 -4.1034079 -4.0892181][-3.9740052 -4.0094023 -4.0555592 -4.07995 -4.0831051 -4.069303 -4.028388 -3.9991779 -4.0139995 -4.0602226 -4.0779691 -4.0860958 -4.0987587 -4.0865111 -4.0642357][-4.0083456 -4.0505457 -4.0816965 -4.0932717 -4.0945773 -4.0844069 -4.049552 -4.0215487 -4.0298581 -4.0628109 -4.0750189 -4.0867295 -4.0992527 -4.0797133 -4.0482154][-4.05765 -4.0915117 -4.1063023 -4.1095514 -4.1033616 -4.0851474 -4.0469055 -4.0247445 -4.0357251 -4.066896 -4.0863891 -4.1048551 -4.1138167 -4.0883369 -4.05361][-4.0764332 -4.1044393 -4.1099067 -4.1116471 -4.1039104 -4.0838556 -4.0547719 -4.0458117 -4.0639887 -4.0904818 -4.1088419 -4.126102 -4.1308875 -4.1022935 -4.0692987][-4.0716076 -4.0909905 -4.09092 -4.0977125 -4.0984573 -4.0895596 -4.0742803 -4.0693703 -4.0808287 -4.0984349 -4.1131287 -4.1255631 -4.1282926 -4.1043577 -4.0761189]]...]
INFO - root - 2017-12-05 12:30:50.390752: step 7010, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 80h:46m:50s remains)
INFO - root - 2017-12-05 12:30:59.572777: step 7020, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 83h:07m:46s remains)
INFO - root - 2017-12-05 12:31:08.590465: step 7030, loss = 2.04, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 73h:58m:29s remains)
INFO - root - 2017-12-05 12:31:17.698878: step 7040, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 81h:57m:46s remains)
INFO - root - 2017-12-05 12:31:26.835073: step 7050, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 86h:14m:49s remains)
INFO - root - 2017-12-05 12:31:36.037353: step 7060, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 82h:38m:13s remains)
INFO - root - 2017-12-05 12:31:45.148408: step 7070, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 80h:30m:11s remains)
INFO - root - 2017-12-05 12:31:54.086648: step 7080, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 82h:33m:14s remains)
INFO - root - 2017-12-05 12:32:03.110688: step 7090, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 86h:03m:02s remains)
INFO - root - 2017-12-05 12:32:12.287045: step 7100, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 81h:44m:02s remains)
2017-12-05 12:32:13.078298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1798334 -4.1671085 -4.1526275 -4.1428185 -4.1467509 -4.1468086 -4.1429219 -4.1380324 -4.1352262 -4.1482649 -4.1717939 -4.1901546 -4.1863432 -4.1753912 -4.1827512][-4.17331 -4.167182 -4.1579995 -4.1441054 -4.1343942 -4.129086 -4.1352091 -4.1390171 -4.1334181 -4.1425385 -4.1664147 -4.1856823 -4.1788054 -4.1661878 -4.1758623][-4.1570272 -4.1609917 -4.1623406 -4.1518536 -4.1318774 -4.1200943 -4.1371579 -4.1585364 -4.1590004 -4.160965 -4.1750345 -4.1898379 -4.1807265 -4.1677351 -4.1783557][-4.1278825 -4.1430225 -4.1580839 -4.1542273 -4.1295042 -4.11365 -4.1403537 -4.1757879 -4.184473 -4.1846709 -4.1905222 -4.2018085 -4.1937842 -4.1826224 -4.1902533][-4.1000571 -4.1201921 -4.1430807 -4.1458416 -4.1199393 -4.0976257 -4.117301 -4.1490579 -4.167089 -4.1797628 -4.1929874 -4.2076173 -4.205184 -4.1968765 -4.2007537][-4.0872378 -4.1061068 -4.1328659 -4.1402192 -4.1162138 -4.079031 -4.0668416 -4.072238 -4.0901093 -4.1281972 -4.1651821 -4.1908135 -4.1991782 -4.1982579 -4.2022567][-4.1078033 -4.1183095 -4.1427746 -4.1494131 -4.1193833 -4.0598092 -4.0027432 -3.960351 -3.9616034 -4.0290966 -4.1016874 -4.1471434 -4.1691494 -4.1846223 -4.2029786][-4.1542521 -4.1541281 -4.1673474 -4.1696019 -4.1310029 -4.0511818 -3.9537508 -3.8591213 -3.8324804 -3.9199584 -4.0269337 -4.0942712 -4.1312332 -4.1658459 -4.202095][-4.1918626 -4.1893172 -4.1907821 -4.1878128 -4.1533594 -4.0752926 -3.97264 -3.866343 -3.8237648 -3.8915241 -3.9856575 -4.0531206 -4.0984879 -4.14727 -4.1964221][-4.1895971 -4.1962757 -4.1957965 -4.1918354 -4.1738973 -4.1253543 -4.0594854 -3.9897494 -3.9513037 -3.9702051 -4.005816 -4.036273 -4.0692091 -4.1165066 -4.1691289][-4.1444321 -4.1624055 -4.1728745 -4.1803074 -4.1796823 -4.162992 -4.1353865 -4.1032834 -4.0776787 -4.0665455 -4.0542827 -4.0429211 -4.0495772 -4.079073 -4.1217804][-4.1002417 -4.1216011 -4.14161 -4.1563139 -4.1616096 -4.1606765 -4.15758 -4.1526856 -4.1472721 -4.1351709 -4.1086907 -4.076755 -4.0603228 -4.0679555 -4.0942073][-4.0773697 -4.0903149 -4.1144419 -4.1301961 -4.128006 -4.1235943 -4.129261 -4.1407003 -4.1574187 -4.1627746 -4.1485753 -4.1207709 -4.095777 -4.0892825 -4.1029739][-4.0824528 -4.0851469 -4.1093273 -4.1270218 -4.1163979 -4.1017075 -4.1000376 -4.1152806 -4.144824 -4.1666727 -4.1701927 -4.1566176 -4.1372986 -4.1305237 -4.1392159][-4.1115956 -4.111783 -4.1350484 -4.1563611 -4.1518412 -4.1340175 -4.116354 -4.1139765 -4.1298084 -4.1477137 -4.1598911 -4.161942 -4.1590257 -4.1609383 -4.1694169]]...]
INFO - root - 2017-12-05 12:32:22.075440: step 7110, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 82h:17m:17s remains)
INFO - root - 2017-12-05 12:32:31.118707: step 7120, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 81h:36m:29s remains)
INFO - root - 2017-12-05 12:32:39.997921: step 7130, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 83h:26m:14s remains)
INFO - root - 2017-12-05 12:32:49.137754: step 7140, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:14m:55s remains)
INFO - root - 2017-12-05 12:32:58.169576: step 7150, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 80h:28m:40s remains)
INFO - root - 2017-12-05 12:33:07.150804: step 7160, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 78h:24m:59s remains)
INFO - root - 2017-12-05 12:33:16.272019: step 7170, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 83h:10m:23s remains)
INFO - root - 2017-12-05 12:33:25.347283: step 7180, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 82h:20m:45s remains)
INFO - root - 2017-12-05 12:33:34.337368: step 7190, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 83h:37m:35s remains)
INFO - root - 2017-12-05 12:33:43.577436: step 7200, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 82h:36m:26s remains)
2017-12-05 12:33:44.348204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2268414 -4.2235847 -4.2162242 -4.2111335 -4.2137666 -4.2207289 -4.2154112 -4.2077584 -4.2095556 -4.2094584 -4.1948628 -4.1803846 -4.1790981 -4.1902871 -4.1992273][-4.2412229 -4.2326765 -4.2183919 -4.2070527 -4.2035971 -4.2068429 -4.2025657 -4.1975317 -4.2005019 -4.2066312 -4.2007675 -4.1897054 -4.1837692 -4.1834741 -4.1810689][-4.252584 -4.2425008 -4.2203922 -4.198411 -4.1838326 -4.1758771 -4.1667995 -4.1624866 -4.1717649 -4.192884 -4.2101007 -4.2126336 -4.2059917 -4.1932149 -4.177424][-4.2543235 -4.2453175 -4.218792 -4.1865916 -4.1594176 -4.1367955 -4.1187792 -4.1117997 -4.1264238 -4.1666689 -4.20979 -4.2321329 -4.2349334 -4.2178741 -4.1931448][-4.2488089 -4.2467623 -4.2204447 -4.1733065 -4.1247492 -4.0780277 -4.0359421 -4.0101757 -4.0251527 -4.0932026 -4.171525 -4.2231789 -4.2509956 -4.2463017 -4.2229276][-4.2509947 -4.2533994 -4.2273707 -4.1667962 -4.0901709 -4.0118303 -3.9267173 -3.8576469 -3.8662581 -3.9748244 -4.101872 -4.1900468 -4.25146 -4.2708368 -4.2580013][-4.258172 -4.2641611 -4.2377481 -4.173038 -4.079443 -3.9761956 -3.8532231 -3.7307978 -3.7186897 -3.8629494 -4.0298233 -4.1439261 -4.2308059 -4.2759395 -4.2760954][-4.2560215 -4.2769632 -4.2586346 -4.2030792 -4.1193943 -4.0253592 -3.9050422 -3.769731 -3.7326396 -3.8553185 -4.0144153 -4.1272988 -4.217133 -4.2697606 -4.2778621][-4.2605472 -4.288352 -4.2778735 -4.2345109 -4.1740146 -4.1072669 -4.0238547 -3.9246871 -3.8854036 -3.9581168 -4.0712938 -4.1581769 -4.227222 -4.2652278 -4.2648726][-4.2846479 -4.3101249 -4.3005819 -4.2668047 -4.2213755 -4.1752734 -4.1253123 -4.0644455 -4.0365939 -4.0763059 -4.1526022 -4.21325 -4.2512779 -4.2619858 -4.2448497][-4.3130074 -4.3319449 -4.323256 -4.2988119 -4.2619271 -4.2250423 -4.1949468 -4.1610312 -4.1475163 -4.1744881 -4.2291551 -4.2678671 -4.2787004 -4.2671723 -4.2353115][-4.3253803 -4.3390145 -4.3333755 -4.3180695 -4.289 -4.2557425 -4.2312951 -4.215364 -4.2161217 -4.240272 -4.2820983 -4.30612 -4.3011889 -4.2766762 -4.2395706][-4.3193913 -4.331605 -4.3268871 -4.3208203 -4.3053145 -4.2835255 -4.2663226 -4.2599206 -4.2641721 -4.2827606 -4.3114805 -4.3261857 -4.3184257 -4.2905989 -4.24937][-4.3144479 -4.3256011 -4.321476 -4.3171983 -4.3121104 -4.3022342 -4.2909722 -4.2868986 -4.2893591 -4.3001223 -4.317822 -4.3312893 -4.3317852 -4.3093343 -4.2707281][-4.3107724 -4.3176188 -4.3145666 -4.3122325 -4.3114438 -4.3067951 -4.300941 -4.29948 -4.3011804 -4.3044114 -4.3140278 -4.3285117 -4.3364744 -4.3281908 -4.3046632]]...]
INFO - root - 2017-12-05 12:33:53.535753: step 7210, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 83h:55m:02s remains)
INFO - root - 2017-12-05 12:34:02.543309: step 7220, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 75h:59m:37s remains)
INFO - root - 2017-12-05 12:34:11.555009: step 7230, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 80h:54m:56s remains)
INFO - root - 2017-12-05 12:34:20.781729: step 7240, loss = 2.02, batch loss = 1.96 (8.7 examples/sec; 0.917 sec/batch; 82h:48m:22s remains)
INFO - root - 2017-12-05 12:34:29.889583: step 7250, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 82h:54m:05s remains)
INFO - root - 2017-12-05 12:34:38.993067: step 7260, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 79h:15m:41s remains)
INFO - root - 2017-12-05 12:34:48.074147: step 7270, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 84h:40m:48s remains)
INFO - root - 2017-12-05 12:34:57.228996: step 7280, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 80h:04m:14s remains)
INFO - root - 2017-12-05 12:35:06.450647: step 7290, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 82h:38m:50s remains)
INFO - root - 2017-12-05 12:35:15.517399: step 7300, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 81h:22m:47s remains)
2017-12-05 12:35:16.307083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1920638 -4.2102246 -4.2172904 -4.2101555 -4.1972632 -4.1789403 -4.1545439 -4.1318078 -4.1221433 -4.1329947 -4.1650429 -4.187798 -4.1785011 -4.1653662 -4.173243][-4.17527 -4.1952376 -4.2049432 -4.2041645 -4.188796 -4.1647034 -4.1381326 -4.1160049 -4.1079226 -4.1234803 -4.1598988 -4.1836343 -4.1713557 -4.1498523 -4.1474891][-4.1544824 -4.1754789 -4.1878619 -4.1894851 -4.1732254 -4.141892 -4.1117568 -4.0925512 -4.0896463 -4.1074982 -4.1401172 -4.1612444 -4.1481557 -4.1249475 -4.119173][-4.1400366 -4.1587753 -4.1686959 -4.1707993 -4.1550803 -4.1142306 -4.0783854 -4.0659528 -4.079227 -4.1033125 -4.1302376 -4.1422157 -4.1241241 -4.1039219 -4.099577][-4.138473 -4.1529965 -4.1614828 -4.1612468 -4.1426163 -4.0917039 -4.0555935 -4.05266 -4.0813 -4.1157742 -4.1338749 -4.1232519 -4.0961027 -4.0840468 -4.0886459][-4.1488428 -4.1670837 -4.1709838 -4.1615644 -4.1358547 -4.0817175 -4.0435686 -4.0412064 -4.0738125 -4.1160469 -4.1300859 -4.10695 -4.0790277 -4.0752735 -4.0880432][-4.1600852 -4.1798267 -4.1767969 -4.1554494 -4.1279716 -4.0780325 -4.0349965 -4.020659 -4.0511603 -4.1029396 -4.124146 -4.1096067 -4.092659 -4.0951128 -4.1051311][-4.1563411 -4.169755 -4.1656523 -4.14298 -4.1146531 -4.0732059 -4.0311923 -4.0088758 -4.0376587 -4.0944939 -4.1230822 -4.1202121 -4.1178865 -4.1236796 -4.1297808][-4.1580687 -4.1595936 -4.152101 -4.1285496 -4.0983562 -4.0646315 -4.0346284 -4.0204382 -4.0434294 -4.0917292 -4.1155996 -4.1192665 -4.1277447 -4.1326451 -4.1335554][-4.1663523 -4.1610079 -4.1563916 -4.139133 -4.1113529 -4.0800695 -4.0631809 -4.0553408 -4.0650353 -4.0945706 -4.1078362 -4.1122203 -4.1215787 -4.1251197 -4.1239042][-4.1658745 -4.1622095 -4.1618576 -4.1545343 -4.1331964 -4.103992 -4.090467 -4.0828338 -4.0867743 -4.1084123 -4.114202 -4.1127262 -4.1163983 -4.1211843 -4.1208792][-4.1609783 -4.15918 -4.1567855 -4.1545944 -4.1370673 -4.11127 -4.101769 -4.10045 -4.1068573 -4.1268816 -4.1275806 -4.1204605 -4.1214914 -4.1238141 -4.1244988][-4.1475158 -4.153214 -4.1520815 -4.1509633 -4.1351438 -4.1173325 -4.113646 -4.120441 -4.1298943 -4.146162 -4.1471839 -4.1387057 -4.1386461 -4.1363759 -4.1360126][-4.1201506 -4.1459522 -4.1556058 -4.1513619 -4.1340203 -4.1172433 -4.1105957 -4.1198058 -4.1337991 -4.1502609 -4.153677 -4.1489725 -4.1492424 -4.1459465 -4.1503787][-4.1210771 -4.1575651 -4.1689615 -4.1607623 -4.1391869 -4.1124158 -4.0921597 -4.0909 -4.1037145 -4.122714 -4.1311073 -4.1353078 -4.1407766 -4.1439304 -4.1596317]]...]
INFO - root - 2017-12-05 12:35:25.475967: step 7310, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 82h:54m:40s remains)
INFO - root - 2017-12-05 12:35:34.350062: step 7320, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 78h:58m:08s remains)
INFO - root - 2017-12-05 12:35:43.458410: step 7330, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 83h:05m:49s remains)
INFO - root - 2017-12-05 12:35:52.731768: step 7340, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 82h:19m:24s remains)
INFO - root - 2017-12-05 12:36:01.939351: step 7350, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 81h:50m:54s remains)
INFO - root - 2017-12-05 12:36:11.151251: step 7360, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 86h:38m:35s remains)
INFO - root - 2017-12-05 12:36:20.351815: step 7370, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 82h:57m:55s remains)
INFO - root - 2017-12-05 12:36:29.514257: step 7380, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.927 sec/batch; 83h:45m:19s remains)
INFO - root - 2017-12-05 12:36:38.577941: step 7390, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:12m:18s remains)
INFO - root - 2017-12-05 12:36:47.675553: step 7400, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 84h:37m:17s remains)
2017-12-05 12:36:48.497091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2409539 -4.2499971 -4.2396913 -4.2151847 -4.1941476 -4.1703453 -4.1281333 -4.0801334 -4.0677371 -4.0988455 -4.1428938 -4.1780977 -4.1994987 -4.2202706 -4.2394991][-4.2618246 -4.2630515 -4.249362 -4.2301955 -4.2134972 -4.185751 -4.1362247 -4.0809617 -4.0659742 -4.0988541 -4.1392584 -4.1751037 -4.2019973 -4.2250071 -4.2437234][-4.2528014 -4.2537594 -4.243741 -4.2309632 -4.2160182 -4.1866341 -4.1365542 -4.0848217 -4.0732803 -4.1082592 -4.1538343 -4.1946139 -4.2274714 -4.2513661 -4.2659492][-4.2472363 -4.2437143 -4.2352386 -4.229434 -4.2197595 -4.1959686 -4.1492968 -4.099792 -4.0853324 -4.1168275 -4.1671953 -4.21165 -4.2462955 -4.271143 -4.2871242][-4.2584043 -4.2457895 -4.2315993 -4.2239337 -4.2105112 -4.18749 -4.1423287 -4.0897 -4.0706291 -4.1055241 -4.1591249 -4.207118 -4.2468224 -4.2775254 -4.3005328][-4.2796845 -4.2625604 -4.2404642 -4.21748 -4.1833467 -4.1391063 -4.0767155 -4.0107226 -3.9990792 -4.0597854 -4.1285758 -4.188745 -4.2366543 -4.2753406 -4.3055987][-4.2986317 -4.2805591 -4.2526746 -4.2109108 -4.1441908 -4.0608082 -3.9573057 -3.8631215 -3.8729613 -3.9865937 -4.0892453 -4.1691384 -4.2273564 -4.2711849 -4.3056827][-4.307445 -4.2878666 -4.2566895 -4.200182 -4.106904 -3.9841704 -3.8318162 -3.6973646 -3.7286532 -3.893857 -4.0386896 -4.1421146 -4.2133269 -4.2670665 -4.3088746][-4.3059831 -4.2846889 -4.250586 -4.1907492 -4.0914345 -3.9581823 -3.7959042 -3.6494913 -3.6696472 -3.8390052 -4.0032558 -4.1241131 -4.205636 -4.265852 -4.3113832][-4.3003707 -4.2804 -4.2498026 -4.20018 -4.1194744 -4.015717 -3.8902791 -3.770901 -3.7572455 -3.8684616 -4.0072536 -4.1264858 -4.2104506 -4.2698593 -4.3146973][-4.2985115 -4.2819118 -4.2578959 -4.220643 -4.1627893 -4.0936756 -4.0094624 -3.9222062 -3.893064 -3.9536979 -4.0553603 -4.158875 -4.2342339 -4.2842503 -4.3216658][-4.3032718 -4.2895036 -4.2680826 -4.239922 -4.2008195 -4.1548553 -4.1013789 -4.044148 -4.0212364 -4.0579696 -4.1281028 -4.2062159 -4.2653618 -4.3033295 -4.3302627][-4.310854 -4.2986774 -4.2769675 -4.2533584 -4.225811 -4.1960535 -4.1618719 -4.1284056 -4.1170354 -4.1432209 -4.1915522 -4.2471619 -4.2911658 -4.3192067 -4.3376255][-4.3131537 -4.302649 -4.2839108 -4.262774 -4.2403297 -4.2199888 -4.1980577 -4.1805482 -4.176373 -4.1970077 -4.2343669 -4.2752914 -4.3075304 -4.3278551 -4.3403082][-4.3046813 -4.2973223 -4.2860322 -4.2713776 -4.2538943 -4.2397623 -4.227674 -4.2182937 -4.2172322 -4.2329698 -4.2613659 -4.2909818 -4.3146625 -4.3290558 -4.3364549]]...]
INFO - root - 2017-12-05 12:36:57.607677: step 7410, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 84h:58m:41s remains)
INFO - root - 2017-12-05 12:37:06.740287: step 7420, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 80h:28m:27s remains)
INFO - root - 2017-12-05 12:37:15.930543: step 7430, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 83h:41m:08s remains)
INFO - root - 2017-12-05 12:37:25.084640: step 7440, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 83h:25m:03s remains)
INFO - root - 2017-12-05 12:37:34.160760: step 7450, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.886 sec/batch; 80h:02m:24s remains)
INFO - root - 2017-12-05 12:37:43.256325: step 7460, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 81h:26m:34s remains)
INFO - root - 2017-12-05 12:37:52.478072: step 7470, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 83h:13m:36s remains)
INFO - root - 2017-12-05 12:38:01.628998: step 7480, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 81h:52m:25s remains)
INFO - root - 2017-12-05 12:38:10.689621: step 7490, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 81h:43m:22s remains)
INFO - root - 2017-12-05 12:38:19.697692: step 7500, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 80h:06m:54s remains)
2017-12-05 12:38:20.446706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2783837 -4.2689819 -4.2594919 -4.2603292 -4.2563024 -4.2396841 -4.2262082 -4.23122 -4.2405267 -4.2421947 -4.243957 -4.2563863 -4.26223 -4.2609119 -4.2544537][-4.275279 -4.2641544 -4.2511997 -4.2468753 -4.2325764 -4.2040949 -4.1821942 -4.1897774 -4.208159 -4.2151475 -4.2197385 -4.23648 -4.2433543 -4.2385435 -4.2289391][-4.2708879 -4.2612906 -4.248312 -4.2380714 -4.2144861 -4.1766157 -4.1461506 -4.1574168 -4.1873441 -4.2052045 -4.2197714 -4.2387948 -4.2391052 -4.2211418 -4.2043834][-4.268681 -4.2616644 -4.2479749 -4.2343068 -4.2030993 -4.1561751 -4.1146579 -4.1264615 -4.1656122 -4.1974974 -4.22468 -4.2456017 -4.237843 -4.210525 -4.1871271][-4.2655149 -4.2586918 -4.2404385 -4.2184682 -4.1789865 -4.1237564 -4.0754404 -4.0887117 -4.1418705 -4.18855 -4.2200246 -4.2387128 -4.2258329 -4.1891155 -4.164144][-4.263422 -4.2541647 -4.2268033 -4.1939287 -4.1463723 -4.080121 -4.0235858 -4.0331392 -4.1026435 -4.1646237 -4.2020178 -4.2207894 -4.2037892 -4.1643519 -4.1491504][-4.2579947 -4.2394118 -4.203804 -4.1643672 -4.10848 -4.0246239 -3.9428048 -3.9401584 -4.0327115 -4.1173015 -4.1667261 -4.1878271 -4.1742997 -4.1417232 -4.1381707][-4.2351995 -4.2070565 -4.1662745 -4.1255331 -4.0668912 -3.9645648 -3.85107 -3.8315072 -3.9410622 -4.043478 -4.1028719 -4.1295485 -4.1212163 -4.0973182 -4.1043005][-4.2019625 -4.1723733 -4.1318536 -4.0964437 -4.05433 -3.9693525 -3.8753915 -3.85593 -3.9353497 -4.01853 -4.0701103 -4.0901632 -4.0824437 -4.0659933 -4.0820494][-4.1779833 -4.1509624 -4.1191325 -4.1026955 -4.0879135 -4.0431519 -3.9926398 -3.9801905 -4.0167613 -4.0584097 -4.086834 -4.096777 -4.0842819 -4.065423 -4.0817738][-4.1671877 -4.1450167 -4.1275387 -4.1250281 -4.1309471 -4.1157575 -4.0948424 -4.0859075 -4.1005826 -4.1168475 -4.1270905 -4.1273212 -4.1078615 -4.0836744 -4.0936022][-4.1777282 -4.1640205 -4.15597 -4.157824 -4.1694536 -4.1696634 -4.1650424 -4.16377 -4.1730518 -4.182724 -4.1896358 -4.1867051 -4.1640048 -4.1391735 -4.1419044][-4.2062926 -4.2036462 -4.202795 -4.2052059 -4.2160254 -4.2230506 -4.2233338 -4.2239852 -4.2335434 -4.2461462 -4.2561231 -4.2569861 -4.2418904 -4.2210789 -4.2169247][-4.2404275 -4.2447233 -4.2494864 -4.2516117 -4.2589688 -4.2644062 -4.2644653 -4.2674665 -4.2788386 -4.2900333 -4.3008971 -4.3026829 -4.2941566 -4.2801018 -4.273231][-4.275754 -4.2798796 -4.2845106 -4.2850318 -4.285872 -4.288878 -4.2931046 -4.2988238 -4.3063774 -4.3140264 -4.3231082 -4.3271012 -4.3238268 -4.31534 -4.309195]]...]
INFO - root - 2017-12-05 12:38:29.453444: step 7510, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 80h:43m:35s remains)
INFO - root - 2017-12-05 12:38:38.609739: step 7520, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 84h:17m:32s remains)
INFO - root - 2017-12-05 12:38:47.497318: step 7530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 78h:46m:10s remains)
INFO - root - 2017-12-05 12:38:56.507296: step 7540, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 81h:43m:03s remains)
INFO - root - 2017-12-05 12:39:05.737850: step 7550, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 83h:19m:40s remains)
INFO - root - 2017-12-05 12:39:14.811038: step 7560, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 83h:00m:35s remains)
INFO - root - 2017-12-05 12:39:23.902541: step 7570, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 79h:32m:13s remains)
INFO - root - 2017-12-05 12:39:32.978647: step 7580, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 83h:12m:34s remains)
INFO - root - 2017-12-05 12:39:42.090191: step 7590, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:08m:15s remains)
INFO - root - 2017-12-05 12:39:50.974849: step 7600, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 83h:13m:59s remains)
2017-12-05 12:39:51.763657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3125553 -4.2929144 -4.2728677 -4.2671828 -4.2666454 -4.2627544 -4.2579112 -4.2612271 -4.2735786 -4.284627 -4.289403 -4.2955375 -4.3113194 -4.3226533 -4.3260822][-4.2918787 -4.2624516 -4.2321038 -4.21922 -4.21498 -4.2031188 -4.188098 -4.185719 -4.200181 -4.2190561 -4.2335291 -4.2449446 -4.2664852 -4.28257 -4.2909431][-4.2735171 -4.2369909 -4.1980667 -4.176578 -4.1663547 -4.1464758 -4.1208172 -4.1133728 -4.1317306 -4.1568174 -4.1751809 -4.1874866 -4.2116213 -4.2311893 -4.2473245][-4.2647538 -4.2260909 -4.1826448 -4.1495657 -4.1270204 -4.0968428 -4.0642042 -4.0578136 -4.0841794 -4.1109781 -4.1238308 -4.1280122 -4.1463804 -4.1684823 -4.1957026][-4.2615075 -4.2237687 -4.1795344 -4.1334414 -4.0909057 -4.0400729 -4.000586 -4.0048151 -4.0477953 -4.07733 -4.08033 -4.068964 -4.0740871 -4.097322 -4.1368394][-4.26226 -4.2272329 -4.1834178 -4.1259379 -4.0582633 -3.9751685 -3.9197714 -3.9441204 -4.0192204 -4.063663 -4.0603795 -4.0303826 -4.0210619 -4.0499449 -4.1017056][-4.2632713 -4.2319193 -4.1905985 -4.1296306 -4.0467386 -3.934083 -3.8537915 -3.8954592 -4.0049305 -4.06544 -4.0615673 -4.02465 -4.0091491 -4.0390468 -4.0930061][-4.2664309 -4.2418237 -4.2087727 -4.1561174 -4.0830164 -3.9776745 -3.8896854 -3.9131434 -4.0108776 -4.0698967 -4.0738778 -4.049386 -4.0389695 -4.057013 -4.0963464][-4.2713385 -4.2538896 -4.2306561 -4.1903853 -4.1390638 -4.067018 -3.9999566 -3.9992008 -4.0553012 -4.0928578 -4.1025362 -4.1014948 -4.1067348 -4.107862 -4.1220837][-4.2729354 -4.2573142 -4.2390695 -4.2078161 -4.1741657 -4.1287594 -4.08501 -4.0801072 -4.1104665 -4.1292362 -4.1400418 -4.1536517 -4.1697073 -4.1632729 -4.158906][-4.272521 -4.2558675 -4.2390547 -4.2121425 -4.1840167 -4.1527896 -4.12925 -4.1323209 -4.1536961 -4.16627 -4.1797724 -4.1981697 -4.2131071 -4.2080441 -4.1985817][-4.2724476 -4.2532649 -4.2350416 -4.2082658 -4.1766906 -4.1512957 -4.1461716 -4.162416 -4.1860852 -4.2000031 -4.2162418 -4.2299705 -4.2389145 -4.2358732 -4.2289906][-4.2756133 -4.2554636 -4.2326646 -4.1978693 -4.1582355 -4.1320114 -4.1351848 -4.159513 -4.1928806 -4.21376 -4.2327342 -4.2411404 -4.245358 -4.2417245 -4.2359443][-4.2798157 -4.2591181 -4.2314863 -4.1866207 -4.135972 -4.104085 -4.1069026 -4.1343784 -4.1754508 -4.2037239 -4.2247486 -4.2309151 -4.2330537 -4.2298341 -4.2252421][-4.2826905 -4.2625384 -4.2340093 -4.1863794 -4.1347451 -4.1013122 -4.0994549 -4.1246624 -4.1667476 -4.1968479 -4.2190838 -4.2261968 -4.2290874 -4.2276978 -4.2231021]]...]
INFO - root - 2017-12-05 12:40:00.802520: step 7610, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 82h:36m:01s remains)
INFO - root - 2017-12-05 12:40:09.874608: step 7620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 79h:39m:57s remains)
INFO - root - 2017-12-05 12:40:18.937159: step 7630, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 81h:58m:47s remains)
INFO - root - 2017-12-05 12:40:28.002027: step 7640, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 85h:58m:05s remains)
INFO - root - 2017-12-05 12:40:37.027813: step 7650, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 79h:39m:31s remains)
INFO - root - 2017-12-05 12:40:46.036601: step 7660, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 79h:33m:54s remains)
INFO - root - 2017-12-05 12:40:55.327232: step 7670, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 81h:53m:22s remains)
INFO - root - 2017-12-05 12:41:04.458386: step 7680, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 81h:17m:01s remains)
INFO - root - 2017-12-05 12:41:13.269342: step 7690, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 79h:34m:15s remains)
INFO - root - 2017-12-05 12:41:22.253121: step 7700, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 81h:31m:06s remains)
2017-12-05 12:41:23.093725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2701578 -4.2675796 -4.268466 -4.2717047 -4.2776847 -4.2817173 -4.2825751 -4.2736015 -4.2514992 -4.2282314 -4.2180662 -4.2241664 -4.2417111 -4.2603288 -4.2710209][-4.2566552 -4.25653 -4.2565942 -4.2568045 -4.2607388 -4.2643151 -4.2672071 -4.2590857 -4.234642 -4.206594 -4.1898603 -4.1929646 -4.2139759 -4.2395887 -4.2577138][-4.233809 -4.2348657 -4.2328868 -4.22926 -4.2314548 -4.2344408 -4.2398887 -4.2389817 -4.2207279 -4.1954169 -4.1766119 -4.1758509 -4.1947289 -4.2207713 -4.2408748][-4.20547 -4.2090664 -4.2060862 -4.1969566 -4.1913023 -4.1900997 -4.1936936 -4.2009292 -4.1946807 -4.1801486 -4.165134 -4.1609049 -4.1777272 -4.2037172 -4.221993][-4.1826615 -4.1932993 -4.1914 -4.1743464 -4.1553068 -4.14222 -4.138546 -4.14969 -4.1555 -4.1565161 -4.1541843 -4.1532421 -4.170043 -4.1955161 -4.2098575][-4.1683168 -4.1852784 -4.185813 -4.160068 -4.121613 -4.0836048 -4.0638804 -4.0713725 -4.0913968 -4.1169691 -4.1404905 -4.1563993 -4.1794915 -4.2055831 -4.2165561][-4.1551547 -4.1779146 -4.1756492 -4.1380534 -4.0737257 -3.9968078 -3.9471917 -3.953311 -3.9989345 -4.0617146 -4.1233778 -4.1693654 -4.2046008 -4.2293782 -4.2361116][-4.1520638 -4.1762228 -4.1740608 -4.1314659 -4.0492716 -3.9403195 -3.8731043 -3.8936167 -3.9654279 -4.048882 -4.1291924 -4.1909332 -4.231894 -4.2520938 -4.2535563][-4.1527114 -4.1768317 -4.1840463 -4.1540313 -4.084178 -3.9900439 -3.9436104 -3.9761198 -4.03672 -4.0934386 -4.1499481 -4.2039962 -4.2419324 -4.2591472 -4.2589231][-4.1460977 -4.17139 -4.195787 -4.1903834 -4.14641 -4.0845432 -4.0590649 -4.085835 -4.1176891 -4.1368346 -4.1648231 -4.20807 -4.2438993 -4.260303 -4.2600007][-4.1395316 -4.1617508 -4.1948018 -4.2064567 -4.1810088 -4.140132 -4.1262813 -4.1458035 -4.1619511 -4.1619625 -4.1762328 -4.2144895 -4.2491789 -4.2653656 -4.266459][-4.1447515 -4.1628814 -4.1946592 -4.2115097 -4.1919689 -4.1581855 -4.1510835 -4.1710567 -4.1863127 -4.1863918 -4.1985631 -4.2320104 -4.2626023 -4.2768784 -4.2774568][-4.1525292 -4.1715779 -4.2017894 -4.2142096 -4.1910472 -4.1599183 -4.1591091 -4.186707 -4.2117715 -4.2206917 -4.2311435 -4.2561164 -4.2796292 -4.2880459 -4.2825632][-4.1573248 -4.1813169 -4.2107487 -4.2189288 -4.1967311 -4.1719675 -4.1773386 -4.2084432 -4.238059 -4.2514987 -4.2584271 -4.2732015 -4.2861581 -4.2873926 -4.2775559][-4.1437149 -4.1725993 -4.2124462 -4.2254653 -4.2117848 -4.1975689 -4.2087297 -4.2350435 -4.2576432 -4.2665048 -4.26864 -4.2744484 -4.279994 -4.2803283 -4.2742362]]...]
INFO - root - 2017-12-05 12:41:32.259233: step 7710, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 82h:11m:51s remains)
INFO - root - 2017-12-05 12:41:41.245695: step 7720, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 81h:38m:18s remains)
INFO - root - 2017-12-05 12:41:50.299295: step 7730, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 82h:17m:23s remains)
INFO - root - 2017-12-05 12:41:59.497974: step 7740, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 82h:52m:14s remains)
INFO - root - 2017-12-05 12:42:08.454184: step 7750, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 80h:59m:19s remains)
INFO - root - 2017-12-05 12:42:17.578315: step 7760, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 82h:10m:38s remains)
INFO - root - 2017-12-05 12:42:26.734864: step 7770, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 79h:49m:25s remains)
INFO - root - 2017-12-05 12:42:35.707650: step 7780, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 80h:05m:17s remains)
INFO - root - 2017-12-05 12:42:44.589890: step 7790, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 81h:53m:17s remains)
INFO - root - 2017-12-05 12:42:53.816656: step 7800, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 85h:19m:41s remains)
2017-12-05 12:42:54.629199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.221828 -4.2173409 -4.2164969 -4.2132134 -4.2103953 -4.2110662 -4.2167258 -4.2200432 -4.22328 -4.2253561 -4.2281623 -4.2325587 -4.244297 -4.2504749 -4.2450743][-4.1954532 -4.18819 -4.1931691 -4.1941457 -4.1894922 -4.1858325 -4.1895494 -4.1943216 -4.1995783 -4.2023978 -4.2044148 -4.204752 -4.2159634 -4.2250886 -4.2247682][-4.1668882 -4.16041 -4.171526 -4.1739736 -4.1658368 -4.1555495 -4.1499438 -4.152061 -4.1621671 -4.1720529 -4.1771722 -4.179193 -4.190114 -4.2011886 -4.2069616][-4.1645894 -4.1565132 -4.1611643 -4.1577072 -4.147234 -4.1288877 -4.108881 -4.1020155 -4.1203928 -4.1452413 -4.1595325 -4.167233 -4.1811318 -4.1961622 -4.2055554][-4.1716409 -4.1589923 -4.1541619 -4.149929 -4.1392136 -4.1146436 -4.0775671 -4.0526147 -4.0760541 -4.1182604 -4.1466165 -4.1623125 -4.1798997 -4.1991529 -4.2122326][-4.1596546 -4.1427426 -4.1323509 -4.1265206 -4.1102915 -4.0768185 -4.0236669 -3.976223 -4.008575 -4.0775723 -4.1218243 -4.1487293 -4.1695309 -4.1912045 -4.21101][-4.1366262 -4.1208544 -4.1036267 -4.0891609 -4.060668 -4.0078616 -3.9195223 -3.8279526 -3.8716109 -3.9856932 -4.0619626 -4.1114 -4.1462584 -4.1762223 -4.2054524][-4.1444273 -4.1355863 -4.1104755 -4.0772004 -4.0307989 -3.9591126 -3.8358736 -3.6960669 -3.7433324 -3.9003463 -4.0079927 -4.0714312 -4.1148806 -4.151525 -4.1860938][-4.186182 -4.1865845 -4.1636128 -4.1297874 -4.086102 -4.0277224 -3.9357243 -3.8297513 -3.852778 -3.9688478 -4.0567293 -4.1059241 -4.1369309 -4.1643724 -4.1892619][-4.2202783 -4.22452 -4.2053285 -4.1782484 -4.1476769 -4.1068134 -4.047555 -3.9818497 -3.993753 -4.06922 -4.1305323 -4.1633983 -4.1833124 -4.1991396 -4.2140994][-4.2419047 -4.249198 -4.2374086 -4.2159786 -4.1919565 -4.1578069 -4.110837 -4.063705 -4.06981 -4.1214066 -4.1676559 -4.1950755 -4.2146673 -4.2284608 -4.2383947][-4.2533584 -4.2680421 -4.2650838 -4.2514081 -4.2303357 -4.2011948 -4.1634288 -4.1291552 -4.13144 -4.1649637 -4.1967673 -4.2172594 -4.2332168 -4.244513 -4.2505107][-4.2591267 -4.2807746 -4.2879124 -4.2792091 -4.2565989 -4.2324324 -4.2096419 -4.1946468 -4.2016964 -4.2246423 -4.2440128 -4.2542243 -4.2628794 -4.2692561 -4.2699809][-4.256566 -4.2807384 -4.2944894 -4.2918377 -4.2746434 -4.2574692 -4.2460914 -4.2455192 -4.2544036 -4.2686677 -4.2799349 -4.285388 -4.2904325 -4.2943172 -4.2943416][-4.2569842 -4.2843695 -4.2989492 -4.3003392 -4.290545 -4.280304 -4.2763252 -4.2796812 -4.2845387 -4.2901077 -4.2959604 -4.3001304 -4.3034735 -4.30589 -4.3068604]]...]
INFO - root - 2017-12-05 12:43:03.645528: step 7810, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 82h:39m:53s remains)
INFO - root - 2017-12-05 12:43:12.715078: step 7820, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 82h:50m:07s remains)
INFO - root - 2017-12-05 12:43:21.749296: step 7830, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 81h:56m:15s remains)
INFO - root - 2017-12-05 12:43:30.803816: step 7840, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.916 sec/batch; 82h:34m:50s remains)
INFO - root - 2017-12-05 12:43:40.027794: step 7850, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 81h:11m:32s remains)
INFO - root - 2017-12-05 12:43:49.118166: step 7860, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 77h:20m:38s remains)
INFO - root - 2017-12-05 12:43:58.241448: step 7870, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 80h:19m:40s remains)
INFO - root - 2017-12-05 12:44:07.340040: step 7880, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 84h:12m:50s remains)
INFO - root - 2017-12-05 12:44:16.605542: step 7890, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 84h:57m:45s remains)
INFO - root - 2017-12-05 12:44:25.772011: step 7900, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 82h:31m:11s remains)
2017-12-05 12:44:26.577364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2678485 -4.2622232 -4.2579761 -4.2628713 -4.2777319 -4.290904 -4.2950478 -4.2928782 -4.2893052 -4.2882338 -4.293232 -4.3017621 -4.3079729 -4.3086944 -4.3067822][-4.2171211 -4.2042451 -4.2007432 -4.2127442 -4.2353706 -4.2518992 -4.2542896 -4.2491198 -4.2437611 -4.2436476 -4.2532973 -4.2677603 -4.2782249 -4.2819977 -4.2809939][-4.1751471 -4.1585684 -4.1597691 -4.1795182 -4.208365 -4.2244883 -4.2201991 -4.2104025 -4.2029924 -4.2059994 -4.2219777 -4.2425613 -4.2551646 -4.2579184 -4.2550197][-4.1513925 -4.1358209 -4.144381 -4.1705003 -4.2026095 -4.2145505 -4.2018867 -4.1859341 -4.1772923 -4.1823478 -4.2028365 -4.230052 -4.2465353 -4.2502537 -4.2478466][-4.1400766 -4.1292324 -4.1448274 -4.1712885 -4.1980019 -4.2045455 -4.1867256 -4.1692786 -4.1601467 -4.1649165 -4.1874428 -4.2194762 -4.2401624 -4.2479272 -4.251153][-4.1486187 -4.1430626 -4.1639566 -4.18663 -4.2013521 -4.1996617 -4.1772814 -4.1575203 -4.1454439 -4.1464348 -4.1681957 -4.2019029 -4.2269754 -4.241477 -4.2530346][-4.1719894 -4.1718311 -4.1951389 -4.2113261 -4.2136788 -4.2037134 -4.1803484 -4.160152 -4.1467667 -4.1462116 -4.1686506 -4.2015433 -4.227509 -4.2452335 -4.2602358][-4.18593 -4.18641 -4.206913 -4.2163711 -4.2126369 -4.2002192 -4.1799192 -4.1673722 -4.1577358 -4.1576543 -4.1806026 -4.2126942 -4.2358084 -4.2507792 -4.262713][-4.1889582 -4.1863256 -4.204145 -4.2121511 -4.20971 -4.2017193 -4.18855 -4.1845527 -4.1775742 -4.1759505 -4.1930265 -4.2185254 -4.2331691 -4.2399344 -4.2491512][-4.1800561 -4.1747231 -4.1915631 -4.203145 -4.2082291 -4.2099214 -4.2045178 -4.2042804 -4.1994452 -4.1947269 -4.1980104 -4.2076449 -4.2084122 -4.2051826 -4.213553][-4.1697936 -4.1635222 -4.1809545 -4.1975884 -4.2103453 -4.2174182 -4.2118764 -4.2101645 -4.2064128 -4.1981006 -4.1900487 -4.1866212 -4.1748466 -4.1627808 -4.1713781][-4.1657243 -4.159277 -4.1772852 -4.197546 -4.2139015 -4.2217135 -4.2150769 -4.2096295 -4.2050595 -4.1933804 -4.179316 -4.1715546 -4.1556067 -4.1415458 -4.1513257][-4.161109 -4.1560483 -4.1752973 -4.1983385 -4.216711 -4.225605 -4.2174082 -4.2064695 -4.1954565 -4.1817546 -4.1684575 -4.1672263 -4.1585345 -4.1511073 -4.1627235][-4.1640897 -4.1643405 -4.1846166 -4.2067432 -4.2239642 -4.2320285 -4.2236323 -4.2091837 -4.1921639 -4.1777663 -4.170743 -4.177237 -4.179081 -4.1793604 -4.19143][-4.1835632 -4.1908994 -4.210825 -4.2287569 -4.2426314 -4.2486153 -4.2425256 -4.2296028 -4.2126894 -4.1997948 -4.1979933 -4.2068996 -4.2097468 -4.2117872 -4.2227554]]...]
INFO - root - 2017-12-05 12:44:35.789614: step 7910, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 81h:47m:04s remains)
INFO - root - 2017-12-05 12:44:44.776480: step 7920, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 83h:36m:36s remains)
INFO - root - 2017-12-05 12:44:54.286794: step 7930, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 85h:23m:08s remains)
INFO - root - 2017-12-05 12:45:03.464231: step 7940, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 81h:39m:00s remains)
INFO - root - 2017-12-05 12:45:12.518135: step 7950, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.894 sec/batch; 80h:35m:28s remains)
INFO - root - 2017-12-05 12:45:21.718309: step 7960, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 83h:39m:43s remains)
INFO - root - 2017-12-05 12:45:30.716985: step 7970, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 73h:59m:53s remains)
INFO - root - 2017-12-05 12:45:39.727233: step 7980, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:04m:22s remains)
INFO - root - 2017-12-05 12:45:48.891019: step 7990, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 81h:41m:50s remains)
INFO - root - 2017-12-05 12:45:57.857828: step 8000, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 80h:25m:33s remains)
2017-12-05 12:45:58.588775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1540051 -4.2056417 -4.2363853 -4.2422323 -4.2384605 -4.233552 -4.2262845 -4.216258 -4.1937766 -4.154644 -4.1035848 -4.0433688 -3.9918036 -3.9626777 -3.9556789][-4.1015692 -4.1693497 -4.2126656 -4.2253103 -4.2282209 -4.2325921 -4.2302465 -4.2177606 -4.1889896 -4.1456122 -4.0927868 -4.0290647 -3.9819767 -3.9652023 -3.9733169][-4.05655 -4.1328955 -4.1836333 -4.20185 -4.2116175 -4.221879 -4.2231894 -4.2077103 -4.1760774 -4.136723 -4.0906739 -4.0358768 -4.0012093 -4.0016742 -4.02275][-4.0186892 -4.0977368 -4.15379 -4.1784582 -4.19292 -4.2064734 -4.2108078 -4.1910458 -4.1584153 -4.1300445 -4.1017475 -4.0669765 -4.0449772 -4.0509763 -4.07089][-3.984432 -4.065887 -4.1304145 -4.163568 -4.1824303 -4.1949172 -4.1977906 -4.168232 -4.1308432 -4.1140785 -4.1076651 -4.0969019 -4.0859547 -4.0869837 -4.098248][-3.9742086 -4.0561233 -4.122232 -4.1601167 -4.1779194 -4.1821041 -4.1775217 -4.1379156 -4.0972772 -4.0902648 -4.1040812 -4.118958 -4.1197529 -4.1119609 -4.1140561][-3.9977262 -4.0769882 -4.135663 -4.1698494 -4.181417 -4.1776066 -4.1648903 -4.1232705 -4.0853324 -4.0858989 -4.1136332 -4.1448226 -4.1539836 -4.1427093 -4.1367111][-4.0405626 -4.1115246 -4.1612172 -4.1923289 -4.2013016 -4.194797 -4.1839561 -4.1548371 -4.1275134 -4.1263304 -4.1500831 -4.1800857 -4.1897736 -4.1776972 -4.165997][-4.0907826 -4.1446066 -4.1851044 -4.2167249 -4.229444 -4.224319 -4.2186713 -4.2043982 -4.1886983 -4.1840444 -4.1937647 -4.2080574 -4.2097812 -4.1973534 -4.1813884][-4.1413326 -4.1744027 -4.2059865 -4.2345443 -4.2504821 -4.2515106 -4.2528434 -4.2518787 -4.2425284 -4.2353721 -4.2347431 -4.2350144 -4.2258015 -4.2096982 -4.1875815][-4.2013903 -4.2163429 -4.2353096 -4.2556376 -4.2703276 -4.2750192 -4.2806807 -4.2838554 -4.2775669 -4.2724533 -4.2701263 -4.2638974 -4.2482972 -4.2303338 -4.2094579][-4.2555695 -4.2610025 -4.2710919 -4.2844791 -4.2975583 -4.3024836 -4.3067064 -4.3071408 -4.3004346 -4.2977557 -4.294929 -4.2867336 -4.2699451 -4.2531929 -4.2395396][-4.2934537 -4.2968588 -4.3015904 -4.308547 -4.3155088 -4.3174095 -4.3194084 -4.31829 -4.3116879 -4.3094358 -4.3060145 -4.2990742 -4.2869315 -4.2754521 -4.2713389][-4.313355 -4.317347 -4.3204393 -4.3224487 -4.3213611 -4.3155527 -4.3123665 -4.3097372 -4.3054581 -4.30462 -4.3031416 -4.3002872 -4.2934632 -4.2887979 -4.2921419][-4.3110881 -4.3147855 -4.31886 -4.32057 -4.3171644 -4.3081489 -4.3010607 -4.2973733 -4.2948475 -4.2969141 -4.2996659 -4.301724 -4.3010869 -4.3013749 -4.3077278]]...]
INFO - root - 2017-12-05 12:46:07.636326: step 8010, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.951 sec/batch; 85h:45m:15s remains)
INFO - root - 2017-12-05 12:46:16.767151: step 8020, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 80h:58m:19s remains)
INFO - root - 2017-12-05 12:46:25.903984: step 8030, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 81h:28m:54s remains)
INFO - root - 2017-12-05 12:46:35.068882: step 8040, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 81h:58m:55s remains)
INFO - root - 2017-12-05 12:46:44.201217: step 8050, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 83h:01m:37s remains)
INFO - root - 2017-12-05 12:46:53.318284: step 8060, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 82h:12m:31s remains)
INFO - root - 2017-12-05 12:47:02.414868: step 8070, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 78h:05m:28s remains)
INFO - root - 2017-12-05 12:47:11.470281: step 8080, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 81h:04m:37s remains)
INFO - root - 2017-12-05 12:47:20.708591: step 8090, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 83h:54m:37s remains)
INFO - root - 2017-12-05 12:47:29.801239: step 8100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 81h:47m:51s remains)
2017-12-05 12:47:30.567667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2831526 -4.2712078 -4.2667165 -4.2591915 -4.2463479 -4.2416363 -4.2422566 -4.2469587 -4.251627 -4.2637877 -4.2776585 -4.2909536 -4.3029861 -4.3094044 -4.3117652][-4.2701392 -4.2544384 -4.2430067 -4.22519 -4.2021079 -4.1926279 -4.1902971 -4.1981058 -4.2037392 -4.218574 -4.2405109 -4.2640133 -4.2839904 -4.2986736 -4.3081632][-4.2573423 -4.2353678 -4.2115417 -4.1789327 -4.1466527 -4.1316223 -4.1232471 -4.1337457 -4.1385627 -4.1566811 -4.1900177 -4.2257295 -4.25113 -4.2748404 -4.296165][-4.2436624 -4.212914 -4.1781416 -4.1287551 -4.0828605 -4.0607643 -4.0454092 -4.060339 -4.0693502 -4.0901418 -4.1294227 -4.1762075 -4.2093582 -4.2396131 -4.2729168][-4.2325625 -4.1908097 -4.1486478 -4.0857248 -4.0218191 -3.9841242 -3.9624579 -3.99042 -4.0126991 -4.0362406 -4.0760651 -4.1292377 -4.1668587 -4.2020168 -4.2465219][-4.218647 -4.1618881 -4.1109638 -4.0397573 -3.9621673 -3.9110618 -3.8883846 -3.9258978 -3.9496753 -3.9655519 -4.0018849 -4.06383 -4.1149383 -4.1635032 -4.2215681][-4.2084861 -4.1391306 -4.0816059 -4.0115981 -3.9350991 -3.8803318 -3.8538225 -3.8850644 -3.8967192 -3.8922145 -3.9210634 -3.9982975 -4.0704737 -4.1348395 -4.2044215][-4.2115521 -4.1412487 -4.0844908 -4.0211353 -3.9524674 -3.8959391 -3.865802 -3.8864455 -3.8853745 -3.8678031 -3.8879676 -3.9729452 -4.0587478 -4.1321416 -4.20486][-4.2229605 -4.1634455 -4.1188579 -4.0647507 -4.005826 -3.9468522 -3.9091685 -3.9177752 -3.9197912 -3.9095075 -3.9256058 -4.0060349 -4.0915909 -4.162652 -4.226841][-4.2500138 -4.2017803 -4.16599 -4.1209297 -4.0671935 -4.0052834 -3.9628971 -3.9652994 -3.9775479 -3.9830766 -4.0018439 -4.0701823 -4.1447206 -4.2058549 -4.2557683][-4.2815895 -4.2466536 -4.2190895 -4.1828413 -4.1383424 -4.0833054 -4.0433311 -4.0451341 -4.0643706 -4.0812821 -4.0974588 -4.1479888 -4.2064857 -4.2528472 -4.2860556][-4.3085947 -4.2899575 -4.2726078 -4.24641 -4.2149692 -4.1767015 -4.1469917 -4.1495543 -4.1681547 -4.1854453 -4.1954546 -4.2269549 -4.2652359 -4.2920451 -4.3095517][-4.3179846 -4.3087959 -4.3013215 -4.2868805 -4.2696056 -4.2469125 -4.2311544 -4.2346334 -4.2470188 -4.2595611 -4.2656441 -4.2829518 -4.3027816 -4.3145323 -4.3214469][-4.3176541 -4.3129363 -4.3110347 -4.3068972 -4.3009424 -4.2898245 -4.2822075 -4.2846122 -4.2917528 -4.299871 -4.3024249 -4.3086553 -4.3177552 -4.3232536 -4.3241854][-4.3173046 -4.3138838 -4.3130794 -4.3118687 -4.311852 -4.3089147 -4.3060327 -4.3074985 -4.3123345 -4.317512 -4.3171711 -4.3162684 -4.3195982 -4.32228 -4.3209972]]...]
INFO - root - 2017-12-05 12:47:39.706199: step 8110, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.928 sec/batch; 83h:39m:15s remains)
INFO - root - 2017-12-05 12:47:48.868368: step 8120, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 82h:56m:02s remains)
INFO - root - 2017-12-05 12:47:58.078456: step 8130, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 83h:12m:46s remains)
INFO - root - 2017-12-05 12:48:07.388430: step 8140, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.935 sec/batch; 84h:13m:57s remains)
INFO - root - 2017-12-05 12:48:16.459144: step 8150, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 82h:54m:27s remains)
INFO - root - 2017-12-05 12:48:25.286173: step 8160, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 82h:45m:33s remains)
INFO - root - 2017-12-05 12:48:34.521287: step 8170, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 81h:17m:47s remains)
INFO - root - 2017-12-05 12:48:43.660012: step 8180, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 83h:59m:07s remains)
INFO - root - 2017-12-05 12:48:52.785743: step 8190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 81h:29m:15s remains)
INFO - root - 2017-12-05 12:49:01.786689: step 8200, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 80h:44m:10s remains)
2017-12-05 12:49:02.605670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1455445 -4.1397853 -4.120779 -4.0995054 -4.0695391 -4.0609741 -4.0928 -4.1426253 -4.1787648 -4.1923208 -4.1629543 -4.1176596 -4.1079426 -4.1378322 -4.1792793][-4.15161 -4.1569457 -4.1460052 -4.1221328 -4.0831251 -4.0695806 -4.0977154 -4.1412816 -4.1716137 -4.1764627 -4.1354995 -4.08715 -4.0818858 -4.1190434 -4.16759][-4.1713724 -4.1880555 -4.1851616 -4.1543651 -4.1025281 -4.0779786 -4.0980573 -4.13937 -4.1668587 -4.1579351 -4.1016684 -4.0516472 -4.0543747 -4.1033921 -4.1600375][-4.1898084 -4.2160468 -4.2199211 -4.182209 -4.115943 -4.0758171 -4.0853806 -4.1219854 -4.15224 -4.1375542 -4.0711169 -4.0208745 -4.0336685 -4.0941577 -4.1549535][-4.1985846 -4.2268271 -4.2330194 -4.18567 -4.1007109 -4.0450935 -4.0486112 -4.0877862 -4.1234527 -4.106029 -4.0408268 -3.9975903 -4.0185156 -4.0865221 -4.1505175][-4.1994567 -4.22165 -4.2197618 -4.1585703 -4.05404 -3.9794211 -3.9835286 -4.0377455 -4.0796194 -4.06849 -4.0209618 -3.9905369 -4.0134716 -4.0832195 -4.1516047][-4.2119994 -4.2267661 -4.210856 -4.1337328 -4.0124817 -3.923717 -3.9389396 -4.0075612 -4.0513659 -4.0499239 -4.0266895 -4.0040879 -4.0141249 -4.0783815 -4.1539688][-4.2293224 -4.2439957 -4.2219877 -4.14189 -4.0204825 -3.936718 -3.9621089 -4.0298171 -4.0649338 -4.0621991 -4.05111 -4.0301743 -4.0243626 -4.08042 -4.1616054][-4.2335424 -4.2576866 -4.2421083 -4.1730337 -4.0731297 -4.00783 -4.0252948 -4.0805793 -4.1081734 -4.1054087 -4.0943079 -4.0760202 -4.0639443 -4.1083112 -4.1840491][-4.2272086 -4.2636533 -4.2640843 -4.2169933 -4.1449 -4.0905385 -4.0942068 -4.1407261 -4.1645947 -4.1574235 -4.1395822 -4.1224923 -4.1121554 -4.1466303 -4.2102442][-4.2045279 -4.2497196 -4.2640138 -4.2448778 -4.200655 -4.1577635 -4.1565433 -4.1990209 -4.2187629 -4.203269 -4.1724195 -4.1451921 -4.1318235 -4.15998 -4.2146864][-4.1766119 -4.22118 -4.2459464 -4.2463551 -4.2267027 -4.1990337 -4.1968484 -4.2299275 -4.2419176 -4.2207794 -4.1784749 -4.1476083 -4.13596 -4.1647768 -4.2139921][-4.1579852 -4.1970692 -4.2218084 -4.2328181 -4.2300739 -4.2136884 -4.2120013 -4.2370644 -4.2440095 -4.2187233 -4.1691246 -4.1379418 -4.1296191 -4.162868 -4.2124772][-4.1722622 -4.1945324 -4.20893 -4.2211294 -4.22665 -4.2192268 -4.2171483 -4.2324438 -4.2323523 -4.2023654 -4.1527581 -4.1225753 -4.1214275 -4.1599212 -4.2097688][-4.2036257 -4.21662 -4.223876 -4.2334466 -4.2377372 -4.2327418 -4.2303543 -4.2327614 -4.2210193 -4.1880207 -4.141624 -4.1116037 -4.1151381 -4.1572475 -4.2073617]]...]
INFO - root - 2017-12-05 12:49:11.876813: step 8210, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 81h:01m:03s remains)
INFO - root - 2017-12-05 12:49:21.009625: step 8220, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 82h:16m:02s remains)
INFO - root - 2017-12-05 12:49:30.067588: step 8230, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 84h:07m:28s remains)
INFO - root - 2017-12-05 12:49:39.360955: step 8240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 82h:26m:56s remains)
INFO - root - 2017-12-05 12:49:48.549964: step 8250, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 75h:11m:00s remains)
INFO - root - 2017-12-05 12:49:57.365716: step 8260, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 80h:37m:26s remains)
INFO - root - 2017-12-05 12:50:06.463340: step 8270, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 85h:03m:41s remains)
INFO - root - 2017-12-05 12:50:15.722898: step 8280, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 82h:31m:44s remains)
INFO - root - 2017-12-05 12:50:25.034337: step 8290, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 82h:14m:38s remains)
INFO - root - 2017-12-05 12:50:34.130280: step 8300, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 81h:30m:36s remains)
2017-12-05 12:50:34.844765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1875916 -4.1769519 -4.181704 -4.1886153 -4.1888728 -4.1989207 -4.2074757 -4.2199287 -4.2394462 -4.2438588 -4.2413588 -4.2411447 -4.2416868 -4.2382064 -4.2380996][-4.1906066 -4.1776 -4.1826434 -4.19114 -4.1877222 -4.1967797 -4.2055087 -4.2172413 -4.2334337 -4.2375741 -4.2356663 -4.2355685 -4.227519 -4.2061071 -4.1942472][-4.1892767 -4.1717825 -4.1755428 -4.1830492 -4.1775036 -4.1857395 -4.1954932 -4.2052217 -4.2204514 -4.2290754 -4.2352085 -4.2388043 -4.2231116 -4.182385 -4.1559682][-4.1897864 -4.1643271 -4.1622143 -4.1635032 -4.1523113 -4.1591039 -4.1687379 -4.1799717 -4.1973863 -4.2153096 -4.23271 -4.2414465 -4.2194877 -4.1641135 -4.1254945][-4.1989932 -4.1598959 -4.1404734 -4.1263528 -4.1070156 -4.1084609 -4.1181755 -4.1289177 -4.1475244 -4.1776147 -4.2092624 -4.2278886 -4.2081637 -4.1515932 -4.1117034][-4.2059588 -4.1534519 -4.1100969 -4.0749288 -4.0459242 -4.040555 -4.0423384 -4.0472531 -4.06481 -4.1060843 -4.1545439 -4.1862335 -4.1771054 -4.1331787 -4.1049743][-4.2097025 -4.1571507 -4.0982141 -4.0446343 -4.0067492 -3.9936285 -3.9795551 -3.9651616 -3.9788978 -4.0296817 -4.0894561 -4.1294651 -4.1354108 -4.1142163 -4.1084895][-4.2128515 -4.1752553 -4.1191487 -4.0587549 -4.0165787 -3.9973412 -3.9694204 -3.9353845 -3.9380612 -3.9927454 -4.056325 -4.0984726 -4.1120777 -4.1098485 -4.1236229][-4.2147589 -4.1987538 -4.1635985 -4.1105494 -4.0716271 -4.0494051 -4.0148191 -3.97008 -3.9588463 -4.0043664 -4.0610538 -4.0973172 -4.1090717 -4.1126547 -4.1343136][-4.2117352 -4.2100649 -4.1926522 -4.1509404 -4.1206851 -4.1057625 -4.0746212 -4.0330939 -4.0124555 -4.0424824 -4.0850682 -4.1101103 -4.1182971 -4.1173573 -4.1336279][-4.2090855 -4.2103071 -4.1996756 -4.1655345 -4.1432981 -4.1383581 -4.1143727 -4.0826654 -4.0646272 -4.0798707 -4.103754 -4.1187096 -4.1234064 -4.112855 -4.115458][-4.2142029 -4.2116232 -4.203825 -4.1766324 -4.1582627 -4.1514931 -4.1315432 -4.109045 -4.1009936 -4.1099381 -4.116878 -4.1212997 -4.1215119 -4.1016054 -4.0920572][-4.2170696 -4.2116838 -4.2073231 -4.1914449 -4.1764064 -4.16356 -4.1411462 -4.1247015 -4.1279573 -4.1398039 -4.1379571 -4.1359944 -4.1301403 -4.1055689 -4.0880136][-4.2134891 -4.2063055 -4.2052808 -4.1992946 -4.1873188 -4.1670413 -4.1391315 -4.1262436 -4.1399412 -4.1580696 -4.159277 -4.1583757 -4.1516423 -4.1288753 -4.1081805][-4.2125187 -4.1991539 -4.1970677 -4.1984386 -4.1900821 -4.1638536 -4.1332903 -4.1236939 -4.1449895 -4.1702046 -4.1836352 -4.19173 -4.1885796 -4.1680584 -4.1459193]]...]
INFO - root - 2017-12-05 12:50:43.844851: step 8310, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 81h:41m:45s remains)
INFO - root - 2017-12-05 12:50:53.145529: step 8320, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 84h:50m:26s remains)
INFO - root - 2017-12-05 12:51:02.391616: step 8330, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 81h:32m:51s remains)
INFO - root - 2017-12-05 12:51:11.407819: step 8340, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 80h:40m:58s remains)
INFO - root - 2017-12-05 12:51:20.333372: step 8350, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 84h:25m:19s remains)
INFO - root - 2017-12-05 12:51:29.587319: step 8360, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 82h:57m:23s remains)
INFO - root - 2017-12-05 12:51:38.644623: step 8370, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 79h:48m:52s remains)
INFO - root - 2017-12-05 12:51:47.708016: step 8380, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 81h:43m:58s remains)
INFO - root - 2017-12-05 12:51:57.036625: step 8390, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 82h:22m:03s remains)
INFO - root - 2017-12-05 12:52:06.061113: step 8400, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 81h:53m:33s remains)
2017-12-05 12:52:06.842609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3326612 -4.3259978 -4.3131509 -4.282856 -4.2325468 -4.1719422 -4.1302805 -4.1219831 -4.14364 -4.1944265 -4.2537918 -4.2935343 -4.3062568 -4.298305 -4.284451][-4.3396955 -4.3345537 -4.3309197 -4.3144288 -4.2754326 -4.2230177 -4.181622 -4.1591206 -4.1582451 -4.1878185 -4.2389517 -4.28335 -4.3075452 -4.3081274 -4.298203][-4.3426175 -4.3382707 -4.3400831 -4.333046 -4.3039727 -4.2622709 -4.2246156 -4.1896744 -4.1643596 -4.1705008 -4.2131367 -4.2650814 -4.3045192 -4.3200984 -4.31955][-4.3429508 -4.3381157 -4.3405027 -4.3377223 -4.3144784 -4.2793684 -4.2412524 -4.1941662 -4.149673 -4.1360016 -4.1740556 -4.2362933 -4.2925954 -4.3271151 -4.3400683][-4.344625 -4.3400006 -4.3402576 -4.3374786 -4.3163581 -4.2827392 -4.2391658 -4.1800461 -4.1191511 -4.0867791 -4.1211672 -4.194221 -4.2655206 -4.3172913 -4.3454952][-4.3452339 -4.3407049 -4.3376904 -4.3297396 -4.3049541 -4.26776 -4.2175717 -4.1462626 -4.0672135 -4.0157309 -4.052978 -4.1417108 -4.227798 -4.2941265 -4.336668][-4.3448792 -4.3389406 -4.3304558 -4.3125672 -4.279871 -4.2351789 -4.1768808 -4.0913849 -3.9898779 -3.9192071 -3.9682736 -4.0791588 -4.181241 -4.2603989 -4.3165951][-4.3438396 -4.3353748 -4.3203316 -4.2898903 -4.2476711 -4.195066 -4.128849 -4.0321126 -3.9134231 -3.8333886 -3.8976595 -4.0270543 -4.1405363 -4.2285018 -4.2945404][-4.3434567 -4.3312798 -4.3088636 -4.2682953 -4.2202682 -4.1661825 -4.1024842 -4.0128932 -3.9040465 -3.8384047 -3.9037609 -4.0243678 -4.130209 -4.212894 -4.2776041][-4.3446536 -4.3292651 -4.3012924 -4.2562981 -4.2067237 -4.1550908 -4.1009283 -4.0326209 -3.9552503 -3.9206691 -3.9762774 -4.0682235 -4.15225 -4.2188849 -4.2728786][-4.3458118 -4.3298268 -4.3024712 -4.2624669 -4.218236 -4.1731119 -4.1288033 -4.0808549 -4.0340919 -4.0238075 -4.0661035 -4.1301246 -4.1937971 -4.2454181 -4.28529][-4.3453155 -4.33088 -4.3100038 -4.2819581 -4.2499733 -4.2146826 -4.1810389 -4.1491828 -4.1237073 -4.1266508 -4.1556525 -4.1987057 -4.2449651 -4.282403 -4.3064108][-4.3456635 -4.3349781 -4.3222833 -4.3065839 -4.2876058 -4.2646494 -4.2418842 -4.2223721 -4.2118363 -4.2208557 -4.2419028 -4.2699447 -4.2988663 -4.3194184 -4.3262968][-4.3472567 -4.3405609 -4.3342714 -4.326838 -4.3179665 -4.3059692 -4.2930098 -4.2829146 -4.2819357 -4.291801 -4.3056688 -4.3203716 -4.3324785 -4.3369756 -4.3315845][-4.3463273 -4.3421245 -4.3391342 -4.3360353 -4.3328419 -4.3282633 -4.3233185 -4.320107 -4.3223782 -4.3286548 -4.3350258 -4.3383365 -4.3369894 -4.33204 -4.3229022]]...]
INFO - root - 2017-12-05 12:52:16.033936: step 8410, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.929 sec/batch; 83h:35m:20s remains)
INFO - root - 2017-12-05 12:52:25.228653: step 8420, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.904 sec/batch; 81h:22m:44s remains)
INFO - root - 2017-12-05 12:52:34.476228: step 8430, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 84h:16m:11s remains)
INFO - root - 2017-12-05 12:52:43.252304: step 8440, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 79h:42m:25s remains)
INFO - root - 2017-12-05 12:52:52.415678: step 8450, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.928 sec/batch; 83h:34m:10s remains)
INFO - root - 2017-12-05 12:53:01.738584: step 8460, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 83h:37m:53s remains)
INFO - root - 2017-12-05 12:53:10.940028: step 8470, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 83h:18m:41s remains)
INFO - root - 2017-12-05 12:53:20.125773: step 8480, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 80h:56m:03s remains)
INFO - root - 2017-12-05 12:53:29.185534: step 8490, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 86h:46m:20s remains)
INFO - root - 2017-12-05 12:53:38.371591: step 8500, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 83h:26m:06s remains)
2017-12-05 12:53:39.131009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2010212 -4.1903625 -4.1845655 -4.1861887 -4.17373 -4.1618929 -4.1595831 -4.1632862 -4.1790047 -4.1996965 -4.2149587 -4.2127795 -4.2010875 -4.1883826 -4.175581][-4.1783123 -4.1719317 -4.1755252 -4.1834869 -4.1763115 -4.1660442 -4.1664338 -4.1720929 -4.1812992 -4.188756 -4.1930213 -4.1842632 -4.1720805 -4.1641865 -4.1559925][-4.1683269 -4.1647949 -4.1702003 -4.1743145 -4.1653104 -4.1546373 -4.1564283 -4.1655297 -4.1749468 -4.182137 -4.18205 -4.168251 -4.1533408 -4.143383 -4.1358938][-4.1727648 -4.1649418 -4.1606927 -4.155261 -4.14068 -4.1258283 -4.1242366 -4.1351709 -4.1522574 -4.165535 -4.1631866 -4.1442795 -4.1237774 -4.109972 -4.1031055][-4.1743622 -4.1598806 -4.1412425 -4.1213784 -4.0935979 -4.0687852 -4.0629377 -4.0778794 -4.1077495 -4.1337886 -4.1400146 -4.1282449 -4.1105 -4.0914755 -4.0758004][-4.1723986 -4.1534257 -4.12156 -4.0868607 -4.0408525 -3.9992945 -3.9854455 -4.0015516 -4.0429659 -4.08568 -4.1126814 -4.1216125 -4.1131859 -4.0913596 -4.066309][-4.1796951 -4.1563749 -4.1123424 -4.0634007 -4.0043535 -3.9519017 -3.9309962 -3.940964 -3.9873664 -4.04553 -4.0913177 -4.1172652 -4.1198435 -4.1046906 -4.0804858][-4.1892595 -4.1623168 -4.1125393 -4.0575271 -3.9972148 -3.9502141 -3.9296775 -3.9319973 -3.9773474 -4.0455546 -4.0967255 -4.1214786 -4.126565 -4.1156154 -4.0956092][-4.1963906 -4.1704044 -4.1259241 -4.076592 -4.024888 -3.9941378 -3.9840229 -3.9847512 -4.0265589 -4.0908442 -4.1308012 -4.140614 -4.1372509 -4.1243382 -4.1056924][-4.2044582 -4.189014 -4.1595497 -4.1246524 -4.0881486 -4.0726457 -4.0669923 -4.0638804 -4.0943542 -4.1421452 -4.163033 -4.1522226 -4.134737 -4.1200171 -4.1041031][-4.2119179 -4.2106628 -4.1941013 -4.1684341 -4.1421537 -4.13099 -4.1231337 -4.1164956 -4.1408734 -4.1769776 -4.1802831 -4.1518116 -4.1190305 -4.0998354 -4.0927882][-4.2148919 -4.2231879 -4.215745 -4.1920943 -4.16571 -4.1505036 -4.1375775 -4.13122 -4.1521659 -4.17873 -4.1713076 -4.1375866 -4.1031976 -4.0877 -4.09506][-4.2186165 -4.2301836 -4.2269263 -4.2004476 -4.167099 -4.1411839 -4.1221328 -4.1161594 -4.1347589 -4.1546311 -4.142838 -4.1125855 -4.0874662 -4.0864987 -4.1101189][-4.21905 -4.2307754 -4.2293 -4.2017322 -4.15962 -4.1221671 -4.0954432 -4.088748 -4.1067848 -4.1273756 -4.1192822 -4.0966635 -4.0809245 -4.0923371 -4.1237712][-4.2146759 -4.2263145 -4.2287307 -4.2050157 -4.1583209 -4.1100888 -4.0763445 -4.0707045 -4.09219 -4.1191487 -4.1235962 -4.1109848 -4.1018839 -4.1157637 -4.14652]]...]
INFO - root - 2017-12-05 12:53:48.182134: step 8510, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.903 sec/batch; 81h:13m:46s remains)
INFO - root - 2017-12-05 12:53:57.267872: step 8520, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 82h:23m:25s remains)
INFO - root - 2017-12-05 12:54:06.520119: step 8530, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 84h:35m:03s remains)
INFO - root - 2017-12-05 12:54:15.484753: step 8540, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 79h:52m:31s remains)
INFO - root - 2017-12-05 12:54:24.595111: step 8550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 82h:34m:24s remains)
INFO - root - 2017-12-05 12:54:33.637610: step 8560, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 81h:38m:49s remains)
INFO - root - 2017-12-05 12:54:42.878950: step 8570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:58m:06s remains)
INFO - root - 2017-12-05 12:54:51.889338: step 8580, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 79h:44m:14s remains)
INFO - root - 2017-12-05 12:55:01.141014: step 8590, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 83h:33m:12s remains)
INFO - root - 2017-12-05 12:55:10.330897: step 8600, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 84h:07m:25s remains)
2017-12-05 12:55:11.126986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2541881 -4.2537842 -4.2523417 -4.2518907 -4.25183 -4.25103 -4.2488246 -4.2462916 -4.2444654 -4.2442813 -4.245172 -4.2462683 -4.2460341 -4.2443309 -4.2394028][-4.2548089 -4.2538776 -4.2527618 -4.2537122 -4.2553325 -4.2553129 -4.2522645 -4.2474422 -4.2439346 -4.2436633 -4.246089 -4.2490993 -4.2506838 -4.2490268 -4.2425203][-4.2656546 -4.265872 -4.2663336 -4.2709408 -4.275528 -4.2752018 -4.2704148 -4.2636156 -4.2595835 -4.258419 -4.2611427 -4.2653723 -4.2682738 -4.2671371 -4.2604713][-4.2710958 -4.2711787 -4.2731981 -4.2807178 -4.2861967 -4.2835441 -4.2763791 -4.27135 -4.2726812 -4.2745371 -4.2777638 -4.2816849 -4.2834616 -4.281847 -4.2763171][-4.2682881 -4.2643981 -4.264801 -4.2705488 -4.2702894 -4.2589064 -4.2424917 -4.2377796 -4.2515726 -4.2678056 -4.2787328 -4.2830648 -4.2829585 -4.2821288 -4.2797256][-4.2381816 -4.233191 -4.2351856 -4.2402821 -4.2304778 -4.200913 -4.1619868 -4.145637 -4.1755962 -4.2176466 -4.2451091 -4.2541342 -4.254796 -4.2582335 -4.26494][-4.1811895 -4.1804981 -4.1906447 -4.2029872 -4.1913919 -4.1396518 -4.062417 -4.014576 -4.0542893 -4.127686 -4.177743 -4.196599 -4.2005754 -4.209239 -4.2253733][-4.1329322 -4.1301408 -4.1441007 -4.1643515 -4.1564288 -4.0902376 -3.9789741 -3.8949656 -3.9337261 -4.032042 -4.1019297 -4.1347179 -4.145977 -4.1585016 -4.1782432][-4.1199713 -4.1125569 -4.1234565 -4.1454229 -4.1443624 -4.092061 -3.9972644 -3.922385 -3.948797 -4.03284 -4.0971842 -4.1314454 -4.1437836 -4.1543231 -4.1706228][-4.1214304 -4.1114244 -4.116641 -4.1326447 -4.1367159 -4.1146669 -4.0672874 -4.0266266 -4.0374765 -4.0866895 -4.128695 -4.15444 -4.1621957 -4.1653605 -4.175838][-4.1245623 -4.1147509 -4.1178389 -4.1285162 -4.1297011 -4.1254759 -4.1131582 -4.0974011 -4.0969515 -4.1172624 -4.1424131 -4.16367 -4.1706014 -4.1693444 -4.1739368][-4.1196771 -4.1142983 -4.1187015 -4.1275349 -4.1286116 -4.1320477 -4.137814 -4.1342549 -4.1292219 -4.1386337 -4.158298 -4.1786036 -4.1873684 -4.1859231 -4.1869149][-4.1253619 -4.1229491 -4.1273971 -4.1373544 -4.1392479 -4.1465235 -4.157649 -4.15749 -4.1508579 -4.1589961 -4.1812873 -4.20072 -4.2101126 -4.2106466 -4.2110519][-4.1632171 -4.159925 -4.1597176 -4.1638212 -4.1605659 -4.1643128 -4.1747336 -4.1770411 -4.1712375 -4.1793809 -4.1986842 -4.2124314 -4.2195778 -4.2243695 -4.2293887][-4.2150636 -4.213501 -4.2074485 -4.2048364 -4.1995134 -4.1981149 -4.2003708 -4.2003584 -4.19539 -4.2014422 -4.2141085 -4.2208447 -4.2257366 -4.2334628 -4.2424088]]...]
INFO - root - 2017-12-05 12:55:20.343334: step 8610, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 82h:27m:26s remains)
INFO - root - 2017-12-05 12:55:29.247705: step 8620, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 78h:06m:38s remains)
INFO - root - 2017-12-05 12:55:38.271116: step 8630, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 83h:59m:45s remains)
INFO - root - 2017-12-05 12:55:47.420250: step 8640, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 81h:47m:26s remains)
INFO - root - 2017-12-05 12:55:56.473637: step 8650, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 79h:48m:39s remains)
INFO - root - 2017-12-05 12:56:05.567097: step 8660, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 83h:21m:28s remains)
INFO - root - 2017-12-05 12:56:14.824728: step 8670, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 83h:14m:59s remains)
INFO - root - 2017-12-05 12:56:24.030528: step 8680, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 86h:05m:47s remains)
INFO - root - 2017-12-05 12:56:33.102231: step 8690, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 80h:32m:08s remains)
INFO - root - 2017-12-05 12:56:42.276932: step 8700, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.927 sec/batch; 83h:20m:45s remains)
2017-12-05 12:56:43.017707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2834883 -4.2762828 -4.2708015 -4.2747312 -4.286427 -4.2917881 -4.2865868 -4.277566 -4.2677608 -4.2591238 -4.2557058 -4.2638645 -4.271318 -4.2757635 -4.2870836][-4.2643056 -4.2537155 -4.2420855 -4.2407069 -4.2539654 -4.2612476 -4.2540555 -4.2429233 -4.2331462 -4.2273107 -4.2277446 -4.2388282 -4.246923 -4.253633 -4.2663617][-4.238987 -4.2236319 -4.20442 -4.1940188 -4.2042356 -4.2162771 -4.2092452 -4.1972795 -4.186245 -4.1853342 -4.1913514 -4.2042961 -4.2144408 -4.2269778 -4.2465034][-4.2157049 -4.1976333 -4.1715503 -4.1590528 -4.1636977 -4.1697512 -4.1529722 -4.1319351 -4.1201544 -4.127089 -4.1489282 -4.1704054 -4.1864486 -4.2082362 -4.2376142][-4.2004871 -4.1767812 -4.1459041 -4.1356058 -4.1334004 -4.118927 -4.079277 -4.0414219 -4.042799 -4.0712371 -4.115716 -4.1531162 -4.1794243 -4.2100019 -4.2434106][-4.1970181 -4.1629362 -4.1191072 -4.1089087 -4.1001949 -4.0559063 -3.9832938 -3.928762 -3.9544945 -4.0086522 -4.0730414 -4.1291561 -4.1709876 -4.2103872 -4.2464213][-4.1885324 -4.1425061 -4.0829172 -4.0626483 -4.038002 -3.9564133 -3.841413 -3.7821312 -3.8498826 -3.9427428 -4.0266342 -4.1011176 -4.1609159 -4.2057076 -4.2423649][-4.1775279 -4.1254134 -4.0573463 -4.0212522 -3.9661279 -3.8458085 -3.701005 -3.6584013 -3.7840796 -3.9158387 -4.0103297 -4.0909505 -4.1545224 -4.199563 -4.2361164][-4.1730189 -4.1214161 -4.0561423 -4.0098953 -3.9423449 -3.8286371 -3.714081 -3.7134962 -3.8592448 -3.9912448 -4.0741158 -4.1327982 -4.1768885 -4.2120552 -4.2426658][-4.17016 -4.1212339 -4.0613604 -4.0162249 -3.9581518 -3.8766041 -3.8153334 -3.841733 -3.9659026 -4.0729237 -4.1319304 -4.1669316 -4.1933618 -4.2203135 -4.2486773][-4.177084 -4.1344986 -4.0872245 -4.0499792 -4.0123348 -3.9674764 -3.941813 -3.9696765 -4.0579371 -4.1343079 -4.171248 -4.1929207 -4.2119508 -4.2349977 -4.2612224][-4.1899109 -4.15536 -4.1216812 -4.0951858 -4.0765615 -4.060904 -4.0589137 -4.0852828 -4.1414552 -4.1899772 -4.21308 -4.2292547 -4.2449179 -4.2653351 -4.2882533][-4.2157373 -4.1887188 -4.1654487 -4.1501794 -4.1430736 -4.1428409 -4.1520839 -4.1743765 -4.2083073 -4.2398791 -4.256669 -4.2696142 -4.2806163 -4.296761 -4.314353][-4.2453375 -4.2221484 -4.2042871 -4.1964889 -4.1987329 -4.2073617 -4.2222257 -4.2381725 -4.2565765 -4.2759314 -4.2889457 -4.2989874 -4.305305 -4.3155751 -4.3273044][-4.2637749 -4.2436409 -4.2299838 -4.2277017 -4.2367835 -4.24701 -4.2597389 -4.2692342 -4.2778931 -4.2898293 -4.3020716 -4.3103805 -4.3138347 -4.3192744 -4.3251419]]...]
INFO - root - 2017-12-05 12:56:52.027079: step 8710, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 79h:10m:06s remains)
INFO - root - 2017-12-05 12:57:00.922859: step 8720, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.829 sec/batch; 74h:33m:05s remains)
INFO - root - 2017-12-05 12:57:10.032071: step 8730, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.958 sec/batch; 86h:07m:37s remains)
INFO - root - 2017-12-05 12:57:19.167113: step 8740, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 81h:14m:26s remains)
INFO - root - 2017-12-05 12:57:28.294481: step 8750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:53m:43s remains)
INFO - root - 2017-12-05 12:57:37.404712: step 8760, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 82h:23m:04s remains)
INFO - root - 2017-12-05 12:57:46.530367: step 8770, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 80h:42m:07s remains)
INFO - root - 2017-12-05 12:57:55.566165: step 8780, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 79h:32m:16s remains)
INFO - root - 2017-12-05 12:58:04.664949: step 8790, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.871 sec/batch; 78h:17m:46s remains)
INFO - root - 2017-12-05 12:58:13.840080: step 8800, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 79h:43m:52s remains)
2017-12-05 12:58:14.627077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1897035 -4.2016182 -4.1945677 -4.1764126 -4.1495552 -4.1248803 -4.1223183 -4.1425896 -4.163084 -4.1797624 -4.2048492 -4.223165 -4.2336359 -4.2373347 -4.2290311][-4.1772957 -4.1981959 -4.1992488 -4.1885929 -4.1679096 -4.14534 -4.1363635 -4.1485047 -4.1660519 -4.1800871 -4.1977544 -4.2117667 -4.222445 -4.2282586 -4.2195487][-4.1727719 -4.1971192 -4.2006826 -4.1895661 -4.1704721 -4.1496778 -4.1338382 -4.1361289 -4.1521258 -4.1682143 -4.1832809 -4.1986141 -4.2133832 -4.2177029 -4.2049174][-4.1829195 -4.2029104 -4.2006531 -4.1815767 -4.1546745 -4.1279416 -4.1047711 -4.1008997 -4.1178555 -4.1401381 -4.1581225 -4.1802325 -4.1992493 -4.1995931 -4.1848063][-4.2018104 -4.214993 -4.200985 -4.1672573 -4.1236725 -4.0784564 -4.0381308 -4.0305219 -4.0603609 -4.1018949 -4.1317477 -4.159348 -4.1798153 -4.1808286 -4.1671953][-4.2278905 -4.2345204 -4.206636 -4.1563058 -4.0886383 -4.0114417 -3.9389298 -3.9292514 -3.9918118 -4.0680432 -4.1174927 -4.1489272 -4.1673532 -4.1694636 -4.159296][-4.248632 -4.2468915 -4.2082663 -4.1486778 -4.0621872 -3.9471936 -3.8292158 -3.8187585 -3.9289308 -4.0518003 -4.1221642 -4.154109 -4.1665692 -4.1651831 -4.1552296][-4.2559476 -4.2478876 -4.2070794 -4.1479831 -4.0525589 -3.9139557 -3.769989 -3.7676737 -3.9157786 -4.0628972 -4.1389866 -4.1669803 -4.1755943 -4.1708474 -4.1611066][-4.2461333 -4.242475 -4.2117667 -4.1640768 -4.0768518 -3.9459589 -3.8237817 -3.8356457 -3.9699717 -4.0951853 -4.1567931 -4.1778936 -4.1857309 -4.1809015 -4.177712][-4.2202926 -4.2285995 -4.2199678 -4.1926394 -4.1257005 -4.0261536 -3.945838 -3.9627321 -4.0511956 -4.1318588 -4.1728415 -4.1893134 -4.1973472 -4.1961584 -4.2007751][-4.1932621 -4.2111783 -4.2221651 -4.2164574 -4.1720505 -4.1067338 -4.0625858 -4.0753651 -4.1240454 -4.1697512 -4.1951213 -4.2057037 -4.2102103 -4.2119479 -4.2212424][-4.1750154 -4.1984086 -4.2227154 -4.2339277 -4.2099609 -4.1700745 -4.1450257 -4.148705 -4.175199 -4.2042346 -4.2218671 -4.2278004 -4.2285128 -4.2321615 -4.2371416][-4.1757183 -4.2023826 -4.229414 -4.2472644 -4.2395911 -4.2144217 -4.1953092 -4.1893773 -4.2046385 -4.2261248 -4.2397075 -4.2446775 -4.2457323 -4.2467637 -4.244822][-4.1925511 -4.2163053 -4.2351789 -4.2472482 -4.2464776 -4.2324886 -4.2158413 -4.2025762 -4.2099771 -4.2287421 -4.2417707 -4.2486558 -4.2474651 -4.2408366 -4.2327361][-4.2176204 -4.2332745 -4.2403965 -4.244205 -4.2446275 -4.2353759 -4.216918 -4.1953883 -4.193572 -4.2118793 -4.2278471 -4.2349329 -4.227859 -4.2129 -4.2018347]]...]
INFO - root - 2017-12-05 12:58:23.696865: step 8810, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 81h:24m:15s remains)
INFO - root - 2017-12-05 12:58:31.868093: step 8820, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 82h:42m:30s remains)
INFO - root - 2017-12-05 12:58:41.064871: step 8830, loss = 2.06, batch loss = 2.00 (7.6 examples/sec; 1.049 sec/batch; 94h:20m:53s remains)
INFO - root - 2017-12-05 12:58:50.319008: step 8840, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 81h:14m:46s remains)
INFO - root - 2017-12-05 12:58:59.341782: step 8850, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 81h:50m:16s remains)
INFO - root - 2017-12-05 12:59:08.493514: step 8860, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 82h:38m:39s remains)
INFO - root - 2017-12-05 12:59:17.717858: step 8870, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 81h:39m:15s remains)
INFO - root - 2017-12-05 12:59:26.963285: step 8880, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 83h:29m:14s remains)
INFO - root - 2017-12-05 12:59:35.950721: step 8890, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 81h:30m:09s remains)
INFO - root - 2017-12-05 12:59:45.235454: step 8900, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.009 sec/batch; 90h:41m:17s remains)
2017-12-05 12:59:46.041038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2986088 -4.2947335 -4.2869515 -4.2790065 -4.2751017 -4.2783728 -4.2895832 -4.306685 -4.3204432 -4.3263874 -4.3271384 -4.3252063 -4.3224106 -4.3153677 -4.3010917][-4.2884793 -4.2784071 -4.2658358 -4.2554259 -4.2501636 -4.2554269 -4.270268 -4.2950044 -4.3161554 -4.3282647 -4.3308716 -4.3293366 -4.3238764 -4.3136673 -4.2973228][-4.2715855 -4.2531314 -4.2304621 -4.2137413 -4.2088556 -4.2152209 -4.234158 -4.2676582 -4.2985878 -4.317492 -4.3243985 -4.3243446 -4.316649 -4.3024158 -4.2851543][-4.2588325 -4.2354326 -4.2022386 -4.1762915 -4.1683612 -4.1689262 -4.1844392 -4.2242918 -4.2671566 -4.2973065 -4.3132458 -4.3201957 -4.3144531 -4.2989106 -4.280582][-4.2519741 -4.2255697 -4.1864362 -4.148982 -4.1279187 -4.1101422 -4.1118903 -4.154211 -4.2158813 -4.2654719 -4.2960076 -4.3152237 -4.3175411 -4.3023667 -4.2803526][-4.2474031 -4.2181344 -4.1736641 -4.1231937 -4.0767994 -4.0255942 -4.0026188 -4.0492191 -4.1413507 -4.2216716 -4.2730875 -4.3057532 -4.3166595 -4.3045216 -4.278769][-4.2558894 -4.2264748 -4.1778197 -4.11253 -4.0350022 -3.9335527 -3.8636892 -3.9218359 -4.0601969 -4.1760006 -4.2487688 -4.2967973 -4.314713 -4.306675 -4.2803717][-4.2715597 -4.2465577 -4.2006507 -4.12748 -4.0266328 -3.8749042 -3.7351093 -3.7899103 -3.9705942 -4.1118879 -4.2040462 -4.2705016 -4.3020225 -4.3038073 -4.2838054][-4.2832122 -4.2640467 -4.2297077 -4.1656284 -4.0655346 -3.9089229 -3.7506051 -3.7698481 -3.932112 -4.0619154 -4.1548948 -4.2340717 -4.2851276 -4.2997 -4.2883472][-4.2908297 -4.2773504 -4.2570167 -4.2157869 -4.1408114 -4.0217028 -3.8984601 -3.8798437 -3.97236 -4.0605817 -4.1300292 -4.2032208 -4.2677193 -4.2968497 -4.2932944][-4.3007441 -4.292479 -4.2812786 -4.2588248 -4.2132049 -4.1372738 -4.051187 -4.0154219 -4.0497403 -4.0989218 -4.1464972 -4.2039342 -4.2653928 -4.3005366 -4.2996964][-4.2983727 -4.3019476 -4.2994547 -4.2905059 -4.2662129 -4.2200136 -4.1642408 -4.131114 -4.1381674 -4.1621356 -4.1944022 -4.2393732 -4.2856975 -4.3120632 -4.3068876][-4.2887359 -4.3046894 -4.3116136 -4.3136182 -4.305789 -4.2811284 -4.2463388 -4.2223654 -4.2182493 -4.2288132 -4.2481637 -4.2802739 -4.3092074 -4.3225117 -4.3114271][-4.2720909 -4.3004713 -4.3175616 -4.3261633 -4.3269787 -4.3160343 -4.2974477 -4.2866774 -4.285111 -4.2875347 -4.2945385 -4.3125467 -4.3276453 -4.3309717 -4.3156133][-4.2600846 -4.29779 -4.32164 -4.3323345 -4.3326621 -4.3269172 -4.3201094 -4.3194141 -4.3241849 -4.3272524 -4.3295197 -4.3366594 -4.3412423 -4.3368063 -4.3192754]]...]
INFO - root - 2017-12-05 12:59:54.791545: step 8910, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 79h:15m:45s remains)
INFO - root - 2017-12-05 13:00:03.867995: step 8920, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 82h:54m:58s remains)
INFO - root - 2017-12-05 13:00:12.966973: step 8930, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 82h:16m:02s remains)
INFO - root - 2017-12-05 13:00:22.124937: step 8940, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 81h:30m:16s remains)
INFO - root - 2017-12-05 13:00:31.241502: step 8950, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 79h:20m:26s remains)
INFO - root - 2017-12-05 13:00:40.324637: step 8960, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 84h:24m:07s remains)
INFO - root - 2017-12-05 13:00:49.430240: step 8970, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 82h:16m:47s remains)
INFO - root - 2017-12-05 13:00:58.614407: step 8980, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 84h:21m:57s remains)
INFO - root - 2017-12-05 13:01:07.644055: step 8990, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 80h:23m:10s remains)
INFO - root - 2017-12-05 13:01:16.858296: step 9000, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 83h:03m:51s remains)
2017-12-05 13:01:17.646753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1615634 -4.1502409 -4.1433291 -4.1428971 -4.1425672 -4.1417565 -4.1417022 -4.1393514 -4.1447587 -4.1560955 -4.1646833 -4.1736894 -4.1739168 -4.1602168 -4.1380458][-4.1370969 -4.1259384 -4.1215234 -4.123642 -4.1213045 -4.1128726 -4.1016226 -4.0920949 -4.0989418 -4.1188297 -4.1393509 -4.1586857 -4.1672568 -4.1591911 -4.1384077][-4.1170125 -4.1079535 -4.1064725 -4.1114378 -4.1098003 -4.0993252 -4.0841141 -4.0736403 -4.0835538 -4.1084223 -4.13671 -4.1629415 -4.1754088 -4.1708331 -4.1511893][-4.0918608 -4.0880971 -4.0897117 -4.0943766 -4.0921226 -4.0796103 -4.0612321 -4.0521612 -4.0673652 -4.10221 -4.1408467 -4.1741056 -4.1892409 -4.1869922 -4.1710806][-4.0618548 -4.0677609 -4.071826 -4.0691881 -4.0560031 -4.0315576 -4.0012255 -3.9875259 -4.0075784 -4.0585742 -4.1155581 -4.1605487 -4.1839948 -4.1882353 -4.1787958][-4.0322022 -4.0402961 -4.0392051 -4.0216055 -3.9898806 -3.9497859 -3.9062147 -3.8857882 -3.9108593 -3.9794784 -4.0594993 -4.12588 -4.1656709 -4.1780639 -4.1743989][-4.0394249 -4.0390596 -4.0296626 -4.003458 -3.9639003 -3.9158306 -3.8595002 -3.8249018 -3.8426538 -3.9116464 -4.00271 -4.0851574 -4.1376715 -4.1562438 -4.1538334][-4.081038 -4.0791979 -4.0731144 -4.0533695 -4.02235 -3.9850678 -3.9370308 -3.9007237 -3.9015381 -3.9440925 -4.0151558 -4.0854659 -4.1307216 -4.1433883 -4.1323562][-4.12846 -4.1283407 -4.124043 -4.1103168 -4.0920115 -4.0732346 -4.0469327 -4.0269423 -4.0235472 -4.0429993 -4.084774 -4.1288571 -4.1558104 -4.1587634 -4.1397452][-4.1546087 -4.1562972 -4.1551509 -4.1479344 -4.1383047 -4.1306758 -4.1229563 -4.1209841 -4.1248236 -4.1331325 -4.1549392 -4.17876 -4.1924534 -4.1929097 -4.1770663][-4.179522 -4.1828537 -4.1823759 -4.177155 -4.1694727 -4.1656823 -4.1699924 -4.1799841 -4.1880722 -4.1905317 -4.2006068 -4.2118864 -4.2173381 -4.2162476 -4.2056756][-4.20689 -4.2123041 -4.2114053 -4.2038255 -4.1936493 -4.190073 -4.1979542 -4.2094307 -4.21498 -4.213922 -4.2170205 -4.22076 -4.2204795 -4.2162204 -4.208148][-4.2389455 -4.2434931 -4.2399573 -4.2281208 -4.2145333 -4.2082329 -4.2127652 -4.2202082 -4.2231455 -4.2203555 -4.2199521 -4.2189455 -4.2153249 -4.2087064 -4.1999135][-4.2606258 -4.2648454 -4.2641573 -4.2540703 -4.2402616 -4.2327781 -4.232964 -4.2350264 -4.2350111 -4.2305346 -4.2272596 -4.2232285 -4.2179432 -4.2104788 -4.2018876][-4.2577391 -4.2620692 -4.265007 -4.26012 -4.250679 -4.2458744 -4.246675 -4.2491212 -4.2505345 -4.24873 -4.2454653 -4.2411451 -4.236815 -4.2322078 -4.2280173]]...]
INFO - root - 2017-12-05 13:01:26.454199: step 9010, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 82h:22m:49s remains)
INFO - root - 2017-12-05 13:01:35.421946: step 9020, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 84h:09m:49s remains)
INFO - root - 2017-12-05 13:01:44.419297: step 9030, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 82h:09m:09s remains)
INFO - root - 2017-12-05 13:01:53.617358: step 9040, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 82h:55m:11s remains)
INFO - root - 2017-12-05 13:02:02.593926: step 9050, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 80h:12m:05s remains)
INFO - root - 2017-12-05 13:02:11.718089: step 9060, loss = 2.11, batch loss = 2.05 (8.3 examples/sec; 0.963 sec/batch; 86h:31m:45s remains)
INFO - root - 2017-12-05 13:02:20.859183: step 9070, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 81h:44m:13s remains)
INFO - root - 2017-12-05 13:02:29.798223: step 9080, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 81h:29m:36s remains)
INFO - root - 2017-12-05 13:02:38.872656: step 9090, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 83h:15m:34s remains)
INFO - root - 2017-12-05 13:02:47.763070: step 9100, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 84h:23m:49s remains)
2017-12-05 13:02:48.550183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1915965 -4.2034788 -4.2266464 -4.2382627 -4.2564731 -4.2682934 -4.2552991 -4.2443371 -4.2521534 -4.2600045 -4.2541385 -4.24009 -4.22491 -4.20881 -4.2013707][-4.1814351 -4.1981416 -4.2202697 -4.2299542 -4.2467489 -4.2586021 -4.2436056 -4.2263002 -4.2302828 -4.2352219 -4.2218204 -4.20095 -4.1862297 -4.177269 -4.1771083][-4.1704583 -4.1910596 -4.2095833 -4.2134533 -4.2219453 -4.2294126 -4.210988 -4.1821651 -4.1830735 -4.1964808 -4.1853175 -4.1582632 -4.1396661 -4.1376071 -4.1501589][-4.1676292 -4.1864915 -4.1984367 -4.1974049 -4.1987877 -4.1960711 -4.1658468 -4.1141415 -4.1120553 -4.1500559 -4.1583748 -4.1369014 -4.1202388 -4.1263027 -4.1470475][-4.166409 -4.1761618 -4.18311 -4.1818109 -4.1760368 -4.1596365 -4.1087174 -4.0242143 -4.0190787 -4.1003575 -4.1503592 -4.151309 -4.1447954 -4.1580148 -4.1824551][-4.1584353 -4.1622014 -4.1624341 -4.1603065 -4.1461153 -4.1155186 -4.0384107 -3.9127038 -3.9059319 -4.0372086 -4.13517 -4.1763639 -4.1932049 -4.2100058 -4.2294941][-4.1385608 -4.1373005 -4.1348314 -4.1325922 -4.1148505 -4.0711575 -3.9647286 -3.7925148 -3.7882836 -3.9678702 -4.1066041 -4.1826749 -4.22541 -4.2495856 -4.2694521][-4.1107616 -4.1034288 -4.106482 -4.1183386 -4.1118488 -4.0661883 -3.9520638 -3.7678413 -3.7609479 -3.9507823 -4.0931416 -4.1792059 -4.2326775 -4.2603426 -4.2797933][-4.1127224 -4.0987315 -4.1066561 -4.1307044 -4.1352916 -4.1034226 -4.019587 -3.8881073 -3.8868957 -4.0263119 -4.1302629 -4.1977754 -4.2435832 -4.2660408 -4.2806439][-4.1344905 -4.1180372 -4.1214485 -4.14136 -4.1468172 -4.1291046 -4.0801215 -4.0038791 -4.01536 -4.1094427 -4.1776843 -4.2185774 -4.2493863 -4.2647786 -4.2722383][-4.1610661 -4.145669 -4.1428461 -4.1540532 -4.1573524 -4.1491685 -4.1251564 -4.0842519 -4.1014724 -4.1690121 -4.216126 -4.2375035 -4.2544956 -4.2603159 -4.2592888][-4.1826448 -4.1717997 -4.1727281 -4.1813974 -4.1861672 -4.1839948 -4.1728315 -4.1513987 -4.167222 -4.2171326 -4.2508283 -4.2593246 -4.2629967 -4.25901 -4.2503333][-4.195879 -4.1881495 -4.1984444 -4.2120695 -4.2203622 -4.2220106 -4.2157569 -4.2034826 -4.2140083 -4.2475929 -4.2697515 -4.2697096 -4.2643151 -4.2560215 -4.2474051][-4.2067122 -4.2003732 -4.215035 -4.2351427 -4.2481265 -4.2523828 -4.2498989 -4.2438464 -4.2503281 -4.2708683 -4.2826757 -4.2788429 -4.2702546 -4.2609806 -4.2546291][-4.2217493 -4.212635 -4.2246418 -4.2452221 -4.2606459 -4.2673378 -4.2686357 -4.2668643 -4.2693043 -4.2806406 -4.2863812 -4.2815189 -4.2730327 -4.2645521 -4.2596445]]...]
INFO - root - 2017-12-05 13:02:57.688506: step 9110, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.943 sec/batch; 84h:40m:30s remains)
INFO - root - 2017-12-05 13:03:06.835532: step 9120, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 81h:27m:59s remains)
INFO - root - 2017-12-05 13:03:16.009113: step 9130, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 81h:42m:31s remains)
INFO - root - 2017-12-05 13:03:25.098338: step 9140, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 81h:24m:06s remains)
INFO - root - 2017-12-05 13:03:34.129173: step 9150, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 82h:31m:38s remains)
INFO - root - 2017-12-05 13:03:43.389853: step 9160, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 82h:59m:01s remains)
INFO - root - 2017-12-05 13:03:52.481233: step 9170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 81h:29m:40s remains)
INFO - root - 2017-12-05 13:04:01.438562: step 9180, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 81h:45m:09s remains)
INFO - root - 2017-12-05 13:04:10.620216: step 9190, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 83h:24m:54s remains)
INFO - root - 2017-12-05 13:04:19.511327: step 9200, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 80h:56m:56s remains)
2017-12-05 13:04:20.300737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2495222 -4.2614274 -4.2586346 -4.2602677 -4.2617111 -4.2611856 -4.24764 -4.2243977 -4.2106428 -4.2011652 -4.19532 -4.2111216 -4.2432337 -4.2819996 -4.3110514][-4.2585325 -4.2686472 -4.2660069 -4.269362 -4.2703528 -4.2746267 -4.2700205 -4.25413 -4.2434783 -4.2304173 -4.2193141 -4.2280498 -4.2525058 -4.2883968 -4.3199396][-4.249403 -4.258945 -4.2545152 -4.2556009 -4.2514162 -4.2516651 -4.2566137 -4.2569127 -4.2567 -4.2494464 -4.2389989 -4.2426696 -4.26052 -4.2945089 -4.3278403][-4.2253408 -4.2309132 -4.2175245 -4.2085233 -4.1918659 -4.1818943 -4.1929021 -4.2151036 -4.2413497 -4.2547555 -4.2565017 -4.2639914 -4.2781196 -4.3054261 -4.3356295][-4.1866722 -4.1888976 -4.1641631 -4.1382327 -4.1036882 -4.0798073 -4.0876746 -4.1225877 -4.1801124 -4.2286019 -4.2544937 -4.2723856 -4.2862568 -4.3084593 -4.3331976][-4.14791 -4.1462073 -4.1124439 -4.0670061 -4.0090694 -3.9625537 -3.9477487 -3.9797111 -4.0654631 -4.151763 -4.2083259 -4.2450418 -4.2664447 -4.2882266 -4.310277][-4.130722 -4.12518 -4.0834126 -4.02153 -3.9435823 -3.8668046 -3.8032589 -3.8002007 -3.9026666 -4.031714 -4.1224484 -4.1872606 -4.2267332 -4.2573409 -4.2810979][-4.1295123 -4.1250668 -4.0854845 -4.0254569 -3.9438715 -3.8496017 -3.7422209 -3.6839273 -3.7636967 -3.9023476 -4.0163174 -4.1119328 -4.1771779 -4.2248 -4.25619][-4.1436524 -4.1424141 -4.1116052 -4.0666742 -4.0052938 -3.9277356 -3.8235185 -3.7533131 -3.7993274 -3.8997087 -3.9951129 -4.085886 -4.1530352 -4.2091088 -4.2461176][-4.1898131 -4.1871843 -4.1647635 -4.1377339 -4.1019077 -4.0538187 -3.981174 -3.9330828 -3.9624274 -4.0213828 -4.0849757 -4.1448259 -4.1921897 -4.2420311 -4.2729058][-4.2537432 -4.2489185 -4.2324281 -4.2173352 -4.2003784 -4.1762681 -4.1348057 -4.1116052 -4.1316233 -4.1632824 -4.20199 -4.2340045 -4.2616467 -4.2987633 -4.3197284][-4.2995911 -4.2963119 -4.2889781 -4.2843685 -4.2791581 -4.2700348 -4.2516608 -4.2423949 -4.2540684 -4.269186 -4.2916512 -4.3072548 -4.3197751 -4.3425403 -4.355248][-4.3252115 -4.3237367 -4.3236589 -4.3258028 -4.3283138 -4.3266654 -4.3221073 -4.3191638 -4.3246341 -4.3305626 -4.3423915 -4.3482704 -4.3499861 -4.3608546 -4.36791][-4.3390794 -4.3375158 -4.3390341 -4.34278 -4.3473854 -4.3500948 -4.3520136 -4.3522348 -4.3541837 -4.3564506 -4.3618441 -4.3630252 -4.3616037 -4.3660436 -4.3703451][-4.3450761 -4.3440619 -4.344624 -4.3462496 -4.3486257 -4.3506188 -4.3528624 -4.35436 -4.3555312 -4.356926 -4.3602109 -4.3620172 -4.36196 -4.3633842 -4.3659067]]...]
INFO - root - 2017-12-05 13:04:29.310362: step 9210, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 80h:32m:10s remains)
INFO - root - 2017-12-05 13:04:38.459093: step 9220, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 82h:31m:17s remains)
INFO - root - 2017-12-05 13:04:47.559732: step 9230, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 83h:44m:54s remains)
INFO - root - 2017-12-05 13:04:56.769012: step 9240, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.922 sec/batch; 82h:49m:42s remains)
INFO - root - 2017-12-05 13:05:05.897541: step 9250, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 81h:43m:13s remains)
INFO - root - 2017-12-05 13:05:15.000167: step 9260, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 83h:11m:05s remains)
INFO - root - 2017-12-05 13:05:24.138154: step 9270, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 82h:39m:04s remains)
INFO - root - 2017-12-05 13:05:33.142453: step 9280, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 81h:40m:26s remains)
INFO - root - 2017-12-05 13:05:42.104405: step 9290, loss = 2.05, batch loss = 1.99 (8.0 examples/sec; 1.006 sec/batch; 90h:18m:05s remains)
INFO - root - 2017-12-05 13:05:51.264180: step 9300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 81h:18m:29s remains)
2017-12-05 13:05:52.097770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2157021 -4.213819 -4.2032371 -4.1874228 -4.1772637 -4.1746917 -4.1708479 -4.1756 -4.1953897 -4.2153683 -4.2232914 -4.2178254 -4.202755 -4.1925383 -4.2023134][-4.1914792 -4.1855359 -4.1708937 -4.1522832 -4.1418347 -4.1423035 -4.14256 -4.1489649 -4.1719956 -4.1959896 -4.2078509 -4.207582 -4.19853 -4.19407 -4.2060723][-4.1552243 -4.1463442 -4.1291976 -4.1083956 -4.0995264 -4.1068125 -4.1142597 -4.1236353 -4.1505256 -4.1782985 -4.1921186 -4.1931143 -4.1883354 -4.189085 -4.2038622][-4.1243625 -4.1143093 -4.0975142 -4.0758567 -4.0679283 -4.0793653 -4.0916982 -4.10162 -4.1290812 -4.1593995 -4.1764293 -4.1795192 -4.1775808 -4.1812949 -4.1984363][-4.1166778 -4.108099 -4.0949855 -4.0799732 -4.0770712 -4.0882373 -4.0976262 -4.1017413 -4.1240139 -4.1536484 -4.1744556 -4.1815376 -4.1837473 -4.1873131 -4.2007847][-4.1178756 -4.1128979 -4.1024008 -4.0935807 -4.0916133 -4.0969343 -4.0973811 -4.0932288 -4.1117339 -4.1423569 -4.1666627 -4.179143 -4.1863904 -4.1899514 -4.2009516][-4.1093636 -4.1032715 -4.0888405 -4.0787888 -4.0755711 -4.0771976 -4.075129 -4.0737696 -4.0961971 -4.1302052 -4.1575165 -4.1723704 -4.1808205 -4.1842194 -4.1968293][-4.0910506 -4.0847335 -4.0634241 -4.0462823 -4.0393667 -4.0415883 -4.0468068 -4.0580983 -4.087667 -4.1233883 -4.1511197 -4.1654181 -4.1733236 -4.1794987 -4.19617][-4.0763912 -4.0720592 -4.0488567 -4.028111 -4.0181289 -4.0178895 -4.0303903 -4.053967 -4.0879359 -4.1234136 -4.1504421 -4.1638274 -4.1705432 -4.1789122 -4.1991434][-4.0872512 -4.0902495 -4.0733509 -4.0533223 -4.0417776 -4.0398054 -4.0541859 -4.0835314 -4.1161175 -4.1457019 -4.1699891 -4.18157 -4.1845856 -4.1901565 -4.2080193][-4.0989785 -4.1043987 -4.0918508 -4.0724092 -4.0600529 -4.0586104 -4.0735531 -4.1040754 -4.1355524 -4.1638308 -4.1899524 -4.2038856 -4.2045364 -4.2052693 -4.2189789][-4.0948672 -4.0986958 -4.0886259 -4.0730629 -4.0657115 -4.0671053 -4.0813041 -4.1086845 -4.1374559 -4.1669946 -4.1969962 -4.2133193 -4.2129221 -4.2114439 -4.2237525][-4.08616 -4.0890107 -4.0829744 -4.0762434 -4.0758252 -4.0792294 -4.0876203 -4.1064172 -4.1309004 -4.1593833 -4.1900229 -4.2066774 -4.2060924 -4.2047195 -4.217947][-4.0793567 -4.078948 -4.0735173 -4.0703297 -4.0719285 -4.072607 -4.071876 -4.0814967 -4.1026945 -4.1306667 -4.1628618 -4.1825786 -4.1855655 -4.1875596 -4.2037377][-4.0807133 -4.0748625 -4.0664258 -4.0610065 -4.0603404 -4.0602903 -4.0565438 -4.0609784 -4.0790567 -4.105741 -4.1394186 -4.1623049 -4.1689754 -4.1744618 -4.192555]]...]
INFO - root - 2017-12-05 13:06:01.097190: step 9310, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 77h:05m:23s remains)
INFO - root - 2017-12-05 13:06:10.298953: step 9320, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.963 sec/batch; 86h:24m:28s remains)
INFO - root - 2017-12-05 13:06:19.324414: step 9330, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 80h:55m:21s remains)
INFO - root - 2017-12-05 13:06:28.531023: step 9340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 82h:07m:20s remains)
INFO - root - 2017-12-05 13:06:37.520740: step 9350, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 81h:02m:58s remains)
INFO - root - 2017-12-05 13:06:46.665078: step 9360, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.927 sec/batch; 83h:13m:00s remains)
INFO - root - 2017-12-05 13:06:55.815782: step 9370, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.892 sec/batch; 80h:01m:52s remains)
INFO - root - 2017-12-05 13:07:04.739544: step 9380, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 81h:26m:31s remains)
INFO - root - 2017-12-05 13:07:13.897790: step 9390, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 79h:52m:18s remains)
INFO - root - 2017-12-05 13:07:22.999625: step 9400, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 83h:33m:17s remains)
2017-12-05 13:07:23.812632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2293386 -4.2068181 -4.199007 -4.2028971 -4.2094021 -4.2177896 -4.2208409 -4.2116585 -4.2033572 -4.2039652 -4.2090983 -4.2177076 -4.2267962 -4.2361965 -4.2530355][-4.2014933 -4.172195 -4.1618619 -4.1669421 -4.175365 -4.1838784 -4.1853905 -4.1747413 -4.1651087 -4.1679063 -4.173902 -4.1805992 -4.1846828 -4.1898208 -4.2099228][-4.1682682 -4.1344676 -4.1267147 -4.134953 -4.1445765 -4.1508675 -4.145246 -4.1277566 -4.1190677 -4.1281438 -4.1378708 -4.1435108 -4.1429749 -4.143959 -4.1671753][-4.1369534 -4.1057577 -4.1039734 -4.116992 -4.1235666 -4.1161919 -4.0885239 -4.0560379 -4.0506425 -4.0730281 -4.0961971 -4.1087575 -4.1062717 -4.1042743 -4.1318235][-4.1228247 -4.0941153 -4.0972338 -4.1127725 -4.1102018 -4.0793805 -4.0214868 -3.9690776 -3.9686503 -4.0125747 -4.05742 -4.0811806 -4.0753593 -4.070724 -4.1008344][-4.1381454 -4.114686 -4.1186628 -4.125351 -4.1037707 -4.0422735 -3.951581 -3.8779864 -3.8893871 -3.9590664 -4.02513 -4.0601544 -4.0550957 -4.0535297 -4.0896049][-4.1745572 -4.1589551 -4.1565866 -4.1436186 -4.0976467 -4.0096436 -3.8905931 -3.7980363 -3.832098 -3.9296072 -4.0108209 -4.0529079 -4.0534129 -4.0589342 -4.0993304][-4.212852 -4.1997457 -4.1870737 -4.1541734 -4.0937438 -3.9969463 -3.8698649 -3.7770433 -3.8325169 -3.9465609 -4.02962 -4.0683575 -4.0722923 -4.08342 -4.1248035][-4.23204 -4.2150221 -4.1941128 -4.1518784 -4.0936723 -4.0111184 -3.9109745 -3.8505952 -3.9065387 -4.0080609 -4.0755997 -4.1015697 -4.1032066 -4.11403 -4.151125][-4.2380114 -4.2166948 -4.191896 -4.1512976 -4.105125 -4.0462489 -3.9798062 -3.9469194 -3.994205 -4.0726647 -4.1219745 -4.1363692 -4.1344905 -4.1389985 -4.1645918][-4.2400975 -4.2146373 -4.190248 -4.1578937 -4.1254339 -4.0904655 -4.0461111 -4.024673 -4.0579772 -4.1163507 -4.1586928 -4.1725149 -4.1697969 -4.16238 -4.17133][-4.2384233 -4.2105241 -4.1904783 -4.1722193 -4.1554365 -4.1399469 -4.1082778 -4.083427 -4.0994329 -4.1406374 -4.17926 -4.1944466 -4.1914439 -4.1771755 -4.1745963][-4.2313504 -4.2020526 -4.1858535 -4.1824231 -4.1832118 -4.1824818 -4.1599007 -4.1292834 -4.1297746 -4.1565614 -4.1895185 -4.2070251 -4.20792 -4.1946864 -4.18783][-4.212532 -4.179791 -4.165576 -4.1741209 -4.1895933 -4.2013879 -4.1910114 -4.1641183 -4.1598697 -4.1765652 -4.2032619 -4.2246833 -4.231586 -4.2236257 -4.2185159][-4.19534 -4.1609788 -4.1493468 -4.1648955 -4.1880407 -4.20689 -4.2082129 -4.1926928 -4.1932693 -4.2055812 -4.2252607 -4.2441049 -4.2548966 -4.2568016 -4.260581]]...]
INFO - root - 2017-12-05 13:07:32.996372: step 9410, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 83h:25m:08s remains)
INFO - root - 2017-12-05 13:07:42.110800: step 9420, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 82h:55m:45s remains)
INFO - root - 2017-12-05 13:07:51.222399: step 9430, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 80h:37m:53s remains)
INFO - root - 2017-12-05 13:08:00.186824: step 9440, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 82h:28m:09s remains)
INFO - root - 2017-12-05 13:08:09.390719: step 9450, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 80h:52m:13s remains)
INFO - root - 2017-12-05 13:08:18.489691: step 9460, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 78h:19m:20s remains)
INFO - root - 2017-12-05 13:08:27.455881: step 9470, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 79h:20m:00s remains)
INFO - root - 2017-12-05 13:08:36.410537: step 9480, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 83h:41m:32s remains)
INFO - root - 2017-12-05 13:08:45.614329: step 9490, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 81h:37m:51s remains)
INFO - root - 2017-12-05 13:08:54.607735: step 9500, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 82h:42m:05s remains)
2017-12-05 13:08:55.368788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1581588 -4.1802688 -4.2057123 -4.2128615 -4.1821365 -4.1120605 -4.0396814 -4.01598 -4.055409 -4.1315389 -4.18893 -4.2227058 -4.2393651 -4.2383647 -4.2282577][-4.1826692 -4.1976271 -4.2159348 -4.217598 -4.184639 -4.1083889 -4.0316529 -4.0063844 -4.0459256 -4.1256609 -4.1925797 -4.2373672 -4.2564335 -4.2558417 -4.2425961][-4.2007384 -4.209094 -4.2166009 -4.2106175 -4.1724477 -4.0967164 -4.0213408 -3.9878175 -4.020853 -4.0990696 -4.1753368 -4.2313709 -4.2610331 -4.2659526 -4.25636][-4.2074084 -4.2083039 -4.2083931 -4.1946192 -4.15415 -4.0886779 -4.0188842 -3.9793797 -4.00525 -4.0800776 -4.1591029 -4.2196636 -4.2563972 -4.2665076 -4.263567][-4.1908941 -4.1850429 -4.1794262 -4.1600513 -4.1193948 -4.0651231 -4.0068717 -3.9716189 -3.988019 -4.054914 -4.1318088 -4.1934557 -4.2367454 -4.2535625 -4.2564139][-4.1573024 -4.1459422 -4.1376863 -4.1104259 -4.064187 -4.020144 -3.9762807 -3.9482234 -3.9563966 -4.0157828 -4.0909858 -4.154779 -4.201725 -4.2258224 -4.23303][-4.1286592 -4.1188259 -4.107707 -4.0707955 -4.0198345 -3.9812679 -3.951431 -3.933912 -3.938664 -3.9863768 -4.05985 -4.1279974 -4.174767 -4.1999269 -4.2117538][-4.1117306 -4.1059585 -4.0974445 -4.0631618 -4.0147562 -3.9842582 -3.969243 -3.9613967 -3.9629631 -3.9987433 -4.0663276 -4.1307745 -4.1707468 -4.1923094 -4.2116871][-4.1281495 -4.122931 -4.1217079 -4.094274 -4.0456553 -4.010891 -4.0010161 -4.005579 -4.0117183 -4.0377507 -4.0980115 -4.1515489 -4.1813164 -4.1956682 -4.2161632][-4.1682396 -4.1619473 -4.1640244 -4.1456313 -4.1060863 -4.0719624 -4.0656385 -4.075789 -4.0822935 -4.0964727 -4.1397529 -4.1826277 -4.20804 -4.218605 -4.2308073][-4.20431 -4.1987834 -4.2035446 -4.1935883 -4.1647444 -4.1407118 -4.1448946 -4.1589203 -4.1631074 -4.1670871 -4.1907916 -4.2228327 -4.24204 -4.2452207 -4.2484946][-4.2245746 -4.2193069 -4.2239251 -4.2200308 -4.2027426 -4.18995 -4.1997957 -4.2165022 -4.2205195 -4.2172379 -4.2315717 -4.2516184 -4.2616906 -4.2597876 -4.2601466][-4.2331829 -4.2249541 -4.2295728 -4.2285333 -4.2181406 -4.21278 -4.22261 -4.2377319 -4.2393112 -4.2373991 -4.248951 -4.2596641 -4.262393 -4.2624264 -4.2626023][-4.2472653 -4.2373881 -4.2383065 -4.2364411 -4.2259946 -4.2208862 -4.2264328 -4.2373719 -4.2404966 -4.2410693 -4.25043 -4.2570248 -4.2602239 -4.2627716 -4.26559][-4.2617736 -4.255085 -4.2534223 -4.2507992 -4.2438736 -4.2393603 -4.239819 -4.244225 -4.2494192 -4.2531848 -4.2584157 -4.2623882 -4.2654247 -4.2690325 -4.27233]]...]
INFO - root - 2017-12-05 13:09:04.476923: step 9510, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.937 sec/batch; 84h:04m:43s remains)
INFO - root - 2017-12-05 13:09:13.728467: step 9520, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 81h:48m:18s remains)
INFO - root - 2017-12-05 13:09:22.820944: step 9530, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 80h:16m:35s remains)
INFO - root - 2017-12-05 13:09:31.930307: step 9540, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 83h:28m:18s remains)
INFO - root - 2017-12-05 13:09:41.131577: step 9550, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 83h:33m:53s remains)
INFO - root - 2017-12-05 13:09:50.539751: step 9560, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.936 sec/batch; 83h:55m:43s remains)
INFO - root - 2017-12-05 13:09:59.402128: step 9570, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 80h:56m:05s remains)
INFO - root - 2017-12-05 13:10:08.421460: step 9580, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 81h:30m:35s remains)
INFO - root - 2017-12-05 13:10:17.503774: step 9590, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 79h:36m:43s remains)
INFO - root - 2017-12-05 13:10:26.588242: step 9600, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 79h:25m:25s remains)
2017-12-05 13:10:27.379864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.289866 -4.2964435 -4.3009419 -4.30423 -4.3046513 -4.3036551 -4.3030643 -4.301918 -4.3009577 -4.3019624 -4.3050466 -4.3084788 -4.3113661 -4.3142176 -4.315877][-4.2649412 -4.2758141 -4.2851377 -4.2916155 -4.2928352 -4.2915897 -4.2907133 -4.2879009 -4.2844429 -4.2835131 -4.2862587 -4.2903314 -4.2943974 -4.2986441 -4.3014045][-4.2396684 -4.2540026 -4.2669244 -4.2750468 -4.2749257 -4.2716575 -4.2690043 -4.2622743 -4.254777 -4.2526207 -4.25606 -4.2612505 -4.2665896 -4.2732077 -4.2787142][-4.2335811 -4.2491679 -4.2637649 -4.2710805 -4.2684355 -4.2634382 -4.2601695 -4.2519875 -4.242197 -4.2398481 -4.24255 -4.249064 -4.2562413 -4.2652416 -4.2725992][-4.2465639 -4.2605109 -4.2722855 -4.2737627 -4.2655883 -4.2596912 -4.2594714 -4.2554817 -4.2485271 -4.2476273 -4.2499261 -4.2583184 -4.2687378 -4.2818222 -4.2909021][-4.2564769 -4.2637391 -4.2649021 -4.2564769 -4.2439079 -4.2401948 -4.2471509 -4.2523117 -4.2511148 -4.2527919 -4.2562304 -4.2655935 -4.277596 -4.2923369 -4.3009276][-4.2597804 -4.2579789 -4.2480764 -4.2317891 -4.2170033 -4.2156234 -4.2275696 -4.2392883 -4.2454343 -4.2523332 -4.2578411 -4.265811 -4.2741036 -4.2871008 -4.2937489][-4.2576303 -4.2500911 -4.2339787 -4.2143989 -4.2000084 -4.1994328 -4.21111 -4.2232294 -4.2351747 -4.2470112 -4.2534719 -4.258234 -4.2590818 -4.265079 -4.2676625][-4.2552176 -4.2426143 -4.2227407 -4.2028837 -4.1890993 -4.1902862 -4.2012992 -4.2119622 -4.2249312 -4.2379723 -4.2421842 -4.241106 -4.2337441 -4.2345529 -4.2356215][-4.2469511 -4.2308288 -4.2104411 -4.1948 -4.1853967 -4.189517 -4.1993451 -4.2090311 -4.2239771 -4.2396278 -4.2398067 -4.2294884 -4.2124567 -4.2090034 -4.2127123][-4.2364874 -4.2204719 -4.2052007 -4.1974497 -4.1991272 -4.2095819 -4.2175288 -4.2214813 -4.2348504 -4.2513475 -4.2495322 -4.2307358 -4.204021 -4.1942954 -4.1977506][-4.2168179 -4.2050486 -4.1986847 -4.2019968 -4.2165565 -4.2366867 -4.2466888 -4.24823 -4.2578354 -4.2712455 -4.2695422 -4.2467275 -4.213326 -4.1936641 -4.1920314][-4.2084761 -4.200655 -4.2004666 -4.2118406 -4.2369394 -4.2619386 -4.2747746 -4.2771454 -4.2802544 -4.2851839 -4.2796726 -4.2543554 -4.2173519 -4.1899223 -4.1809111][-4.2187743 -4.2120457 -4.2122111 -4.2285924 -4.256597 -4.27945 -4.2901216 -4.2913232 -4.2872658 -4.2796693 -4.2674627 -4.2402506 -4.2031717 -4.1744375 -4.1618748][-4.2348723 -4.2251863 -4.224762 -4.2435522 -4.2669153 -4.2802315 -4.28835 -4.2888031 -4.280941 -4.2634344 -4.2419014 -4.2131915 -4.1795864 -4.1551094 -4.1455836]]...]
INFO - root - 2017-12-05 13:10:36.320572: step 9610, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 80h:30m:45s remains)
INFO - root - 2017-12-05 13:10:45.490217: step 9620, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 81h:53m:25s remains)
INFO - root - 2017-12-05 13:10:54.630880: step 9630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 76h:32m:58s remains)
INFO - root - 2017-12-05 13:11:03.735717: step 9640, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 78h:30m:45s remains)
INFO - root - 2017-12-05 13:11:12.847638: step 9650, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 80h:42m:02s remains)
INFO - root - 2017-12-05 13:11:21.751932: step 9660, loss = 2.07, batch loss = 2.02 (10.6 examples/sec; 0.756 sec/batch; 67h:45m:45s remains)
INFO - root - 2017-12-05 13:11:30.791332: step 9670, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 82h:40m:20s remains)
INFO - root - 2017-12-05 13:11:39.965933: step 9680, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 80h:22m:27s remains)
INFO - root - 2017-12-05 13:11:49.016136: step 9690, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 83h:50m:08s remains)
INFO - root - 2017-12-05 13:11:58.245185: step 9700, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 82h:47m:34s remains)
2017-12-05 13:11:59.132132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2227964 -4.2270412 -4.2342935 -4.2424064 -4.2458072 -4.2443171 -4.2402868 -4.234055 -4.2234759 -4.2150168 -4.2144504 -4.2222939 -4.2246604 -4.2163143 -4.2021246][-4.2099791 -4.2120519 -4.2224603 -4.234098 -4.238565 -4.2343359 -4.2260809 -4.2162104 -4.2027879 -4.1921148 -4.1920328 -4.2045364 -4.2105284 -4.2042618 -4.1938987][-4.2134748 -4.22067 -4.2344475 -4.244524 -4.24601 -4.2385645 -4.2272811 -4.2127681 -4.1960111 -4.1849251 -4.18745 -4.2039976 -4.2123876 -4.2040415 -4.190537][-4.2136159 -4.2303591 -4.2452025 -4.2474394 -4.2392025 -4.2245255 -4.20757 -4.1882234 -4.1737738 -4.1718965 -4.1851768 -4.2064762 -4.2145348 -4.2019844 -4.1848154][-4.2158251 -4.240952 -4.2534733 -4.2419243 -4.2178965 -4.1885576 -4.1622348 -4.1397219 -4.1349297 -4.149262 -4.1755929 -4.1996069 -4.2071829 -4.1924076 -4.1738749][-4.2138791 -4.2364087 -4.2379651 -4.2098956 -4.1654997 -4.118803 -4.0834227 -4.0649567 -4.0796175 -4.1194568 -4.1647377 -4.1982222 -4.2107339 -4.19717 -4.1798477][-4.2013564 -4.2073908 -4.1915884 -4.1485209 -4.0862083 -4.0221376 -3.9911239 -3.9983644 -4.041492 -4.1014004 -4.1595621 -4.1997833 -4.217917 -4.2124434 -4.2023854][-4.2066298 -4.1999865 -4.17835 -4.1366496 -4.0770006 -4.0211186 -4.0109262 -4.0342903 -4.0769176 -4.1305809 -4.1804385 -4.2147708 -4.2346168 -4.2378192 -4.2315321][-4.2334828 -4.2271409 -4.2132831 -4.1878366 -4.1471386 -4.1119847 -4.1125674 -4.1313477 -4.1578212 -4.1945462 -4.2277384 -4.2516985 -4.2686629 -4.2735863 -4.2630506][-4.26337 -4.2629118 -4.2576542 -4.2456379 -4.2241526 -4.206984 -4.2076044 -4.2164311 -4.2299986 -4.2549615 -4.2769532 -4.2926893 -4.3039374 -4.3020973 -4.2820086][-4.2771873 -4.2825575 -4.2833104 -4.2804518 -4.2724905 -4.2667651 -4.2672386 -4.2710562 -4.2796979 -4.2974505 -4.3101091 -4.3156705 -4.3143263 -4.3021207 -4.2761912][-4.2765884 -4.2849545 -4.2871928 -4.2868323 -4.2865295 -4.2882366 -4.2928352 -4.2977619 -4.304131 -4.3133535 -4.3168316 -4.3126831 -4.301517 -4.2841558 -4.2585154][-4.273982 -4.2803884 -4.2815194 -4.2805357 -4.282578 -4.2861037 -4.2922692 -4.2971864 -4.2998962 -4.3015118 -4.2983193 -4.2909007 -4.2791514 -4.2636447 -4.2429657][-4.2687569 -4.2694526 -4.2680836 -4.2652698 -4.2643895 -4.266922 -4.2718859 -4.2753072 -4.2760077 -4.2750177 -4.2727442 -4.2698212 -4.2659149 -4.2584028 -4.2451267][-4.2718325 -4.2674456 -4.2634735 -4.2590337 -4.2557516 -4.2561154 -4.2592759 -4.2617526 -4.2630887 -4.2645826 -4.2664652 -4.268362 -4.2710161 -4.27063 -4.2646036]]...]
INFO - root - 2017-12-05 13:12:08.146150: step 9710, loss = 2.03, batch loss = 1.98 (8.2 examples/sec; 0.975 sec/batch; 87h:26m:47s remains)
INFO - root - 2017-12-05 13:12:17.193928: step 9720, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 77h:48m:08s remains)
INFO - root - 2017-12-05 13:12:26.217586: step 9730, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 80h:02m:59s remains)
INFO - root - 2017-12-05 13:12:35.246835: step 9740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:35m:41s remains)
INFO - root - 2017-12-05 13:12:44.354229: step 9750, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 81h:26m:36s remains)
INFO - root - 2017-12-05 13:12:53.356615: step 9760, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 80h:15m:45s remains)
INFO - root - 2017-12-05 13:13:02.314849: step 9770, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 79h:31m:03s remains)
INFO - root - 2017-12-05 13:13:11.379117: step 9780, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.889 sec/batch; 79h:39m:53s remains)
INFO - root - 2017-12-05 13:13:20.587049: step 9790, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 82h:40m:53s remains)
INFO - root - 2017-12-05 13:13:29.687664: step 9800, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 82h:05m:11s remains)
2017-12-05 13:13:30.444679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3090177 -4.3121228 -4.311718 -4.3108153 -4.3097749 -4.3095579 -4.3108163 -4.3130622 -4.314136 -4.3115907 -4.3059931 -4.298111 -4.292882 -4.2877784 -4.2797408][-4.3029332 -4.3058424 -4.3037262 -4.3005557 -4.2981105 -4.297967 -4.3014088 -4.3075824 -4.3125739 -4.311491 -4.3056359 -4.2975326 -4.2912784 -4.2809339 -4.2694769][-4.2920747 -4.2921252 -4.2875271 -4.2803383 -4.2735128 -4.2707882 -4.2756872 -4.2864838 -4.2961426 -4.29815 -4.2959838 -4.2913437 -4.285758 -4.2736235 -4.2601452][-4.2649069 -4.2649651 -4.2585392 -4.2464 -4.2328329 -4.2246065 -4.2282019 -4.2401266 -4.2504835 -4.2535806 -4.2584252 -4.2622633 -4.2611833 -4.2520852 -4.2424345][-4.2383146 -4.2394543 -4.2310247 -4.2121887 -4.1891847 -4.1717997 -4.1713467 -4.1830568 -4.1938643 -4.19796 -4.21247 -4.2273378 -4.2335424 -4.2306023 -4.226295][-4.1902623 -4.1934257 -4.1839423 -4.1596785 -4.1289444 -4.1023645 -4.0934839 -4.0965986 -4.1017237 -4.1053553 -4.1318979 -4.1604614 -4.1790752 -4.1880927 -4.1934047][-4.1650882 -4.171741 -4.1642237 -4.1404519 -4.1092725 -4.0802803 -4.0620012 -4.0503397 -4.0436492 -4.0433278 -4.0789533 -4.1190276 -4.1470809 -4.1622329 -4.17436][-4.2296414 -4.2324934 -4.2249541 -4.2066054 -4.1869259 -4.1716056 -4.15949 -4.1443591 -4.1307325 -4.1223569 -4.1459627 -4.176034 -4.1957636 -4.2025137 -4.2078466][-4.2650228 -4.2621565 -4.25456 -4.243238 -4.2358389 -4.2356157 -4.2336469 -4.2229838 -4.2087212 -4.1936059 -4.199759 -4.2150679 -4.2259893 -4.225853 -4.2254887][-4.2629552 -4.2594938 -4.2549281 -4.2486105 -4.246944 -4.2545247 -4.2598162 -4.2533717 -4.2423668 -4.225873 -4.22195 -4.2264218 -4.2316394 -4.2294283 -4.2290421][-4.2719564 -4.2698512 -4.2662706 -4.2586608 -4.2531366 -4.2563424 -4.2608228 -4.255136 -4.247541 -4.2354021 -4.2327275 -4.2335334 -4.2364173 -4.2341185 -4.2332048][-4.286427 -4.2857914 -4.2823253 -4.27306 -4.2607203 -4.2531528 -4.2502966 -4.2438507 -4.2373495 -4.2273154 -4.2278905 -4.2303476 -4.23209 -4.2295785 -4.2305603][-4.2998457 -4.2995181 -4.2948995 -4.2843165 -4.2681317 -4.2516336 -4.2399945 -4.23088 -4.2241979 -4.2150369 -4.2177324 -4.2206545 -4.2197275 -4.2161083 -4.2202721][-4.3093524 -4.3111453 -4.3072329 -4.2970023 -4.280901 -4.2606206 -4.2430048 -4.2295742 -4.2194915 -4.2093658 -4.2117105 -4.2117677 -4.2058759 -4.1988139 -4.2052841][-4.3118854 -4.3161836 -4.3158617 -4.3097739 -4.2981749 -4.2815561 -4.2660284 -4.2514577 -4.2383337 -4.2248268 -4.22202 -4.2169166 -4.2083344 -4.2009392 -4.2069263]]...]
INFO - root - 2017-12-05 13:13:39.568559: step 9810, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 85h:05m:40s remains)
INFO - root - 2017-12-05 13:13:48.744279: step 9820, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.917 sec/batch; 82h:11m:09s remains)
INFO - root - 2017-12-05 13:13:57.794209: step 9830, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 81h:34m:48s remains)
INFO - root - 2017-12-05 13:14:06.772027: step 9840, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 78h:56m:29s remains)
INFO - root - 2017-12-05 13:14:15.686928: step 9850, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 84h:45m:57s remains)
INFO - root - 2017-12-05 13:14:24.912485: step 9860, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 83h:05m:54s remains)
INFO - root - 2017-12-05 13:14:33.962893: step 9870, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.894 sec/batch; 80h:08m:22s remains)
INFO - root - 2017-12-05 13:14:43.072467: step 9880, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 80h:43m:45s remains)
INFO - root - 2017-12-05 13:14:52.174699: step 9890, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.936 sec/batch; 83h:50m:20s remains)
INFO - root - 2017-12-05 13:15:01.291827: step 9900, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 82h:48m:37s remains)
2017-12-05 13:15:02.079976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2396145 -4.255621 -4.267993 -4.2699666 -4.2673903 -4.2676439 -4.2679491 -4.2664733 -4.2676353 -4.2715421 -4.2685442 -4.2596874 -4.2549295 -4.2459917 -4.22819][-4.2276788 -4.2489729 -4.25996 -4.2573261 -4.2523623 -4.2474694 -4.2437639 -4.2411661 -4.2462273 -4.2535229 -4.2528958 -4.2432365 -4.2328281 -4.2142529 -4.189373][-4.2084427 -4.234726 -4.2447491 -4.2404962 -4.2352762 -4.2265744 -4.2155828 -4.2074738 -4.2122307 -4.222764 -4.2288256 -4.2220421 -4.2062159 -4.1761284 -4.1410284][-4.1899996 -4.2094636 -4.2169256 -4.2145839 -4.2125177 -4.200808 -4.1802764 -4.16328 -4.1670046 -4.1825848 -4.2004132 -4.2065887 -4.19234 -4.1534295 -4.1061897][-4.1904063 -4.2043934 -4.2026057 -4.19527 -4.1895223 -4.1712446 -4.1395373 -4.1141148 -4.1211228 -4.1452413 -4.1745181 -4.1958737 -4.1876607 -4.14519 -4.0890684][-4.1813121 -4.1933475 -4.1821284 -4.16528 -4.1515651 -4.129252 -4.09517 -4.068574 -4.0836554 -4.1167603 -4.1555929 -4.1833353 -4.1772952 -4.1339726 -4.07697][-4.1532025 -4.1674027 -4.1527205 -4.1289411 -4.1070733 -4.0793104 -4.0450912 -4.0270867 -4.0498452 -4.08999 -4.1360335 -4.1670856 -4.1641665 -4.1273942 -4.0798974][-4.1320467 -4.1455083 -4.1291656 -4.1060548 -4.0792 -4.0446863 -4.0127525 -4.0016646 -4.0283608 -4.072546 -4.1203642 -4.1506939 -4.1497188 -4.1231341 -4.085103][-4.1194606 -4.1243987 -4.1017246 -4.0762787 -4.052896 -4.0236344 -3.9973934 -3.9916122 -4.0242796 -4.0756593 -4.120059 -4.145113 -4.1426854 -4.1205239 -4.08839][-4.1096869 -4.1037087 -4.0751367 -4.0473204 -4.0333233 -4.0181789 -4.0025492 -4.0031781 -4.0402923 -4.0928545 -4.1317081 -4.1490736 -4.14571 -4.1269526 -4.0953789][-4.110497 -4.0954309 -4.0599656 -4.0297761 -4.0256443 -4.0329132 -4.0321865 -4.038702 -4.0711889 -4.1138711 -4.1446118 -4.1561337 -4.152843 -4.1353273 -4.1041136][-4.105701 -4.0881195 -4.0480189 -4.0187116 -4.021246 -4.04562 -4.0614686 -4.0735435 -4.0962915 -4.1244044 -4.1483212 -4.1574879 -4.1523652 -4.1366286 -4.1071253][-4.1000705 -4.0815458 -4.0498252 -4.0305247 -4.0397277 -4.0734339 -4.1015921 -4.1193509 -4.1382647 -4.1561213 -4.167141 -4.1679587 -4.1582737 -4.1431594 -4.1167083][-4.1144385 -4.099823 -4.0831242 -4.0786657 -4.0938358 -4.1252584 -4.1531749 -4.1739507 -4.1946378 -4.209384 -4.21109 -4.2043471 -4.1918883 -4.1762056 -4.1556473][-4.1589112 -4.1516666 -4.1474414 -4.1531262 -4.170023 -4.1929655 -4.2132721 -4.2306175 -4.2478967 -4.2593622 -4.2584147 -4.2475805 -4.23682 -4.22486 -4.210866]]...]
INFO - root - 2017-12-05 13:15:11.368154: step 9910, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 79h:43m:21s remains)
INFO - root - 2017-12-05 13:15:20.492256: step 9920, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 79h:27m:17s remains)
INFO - root - 2017-12-05 13:15:29.602004: step 9930, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 80h:17m:45s remains)
INFO - root - 2017-12-05 13:15:38.671096: step 9940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 76h:41m:00s remains)
INFO - root - 2017-12-05 13:15:47.798407: step 9950, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 80h:59m:48s remains)
INFO - root - 2017-12-05 13:15:56.825470: step 9960, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 80h:31m:16s remains)
INFO - root - 2017-12-05 13:16:05.900543: step 9970, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 82h:10m:03s remains)
INFO - root - 2017-12-05 13:16:14.926022: step 9980, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 80h:26m:25s remains)
INFO - root - 2017-12-05 13:16:23.861097: step 9990, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 81h:15m:51s remains)
INFO - root - 2017-12-05 13:16:32.866965: step 10000, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 78h:34m:48s remains)
2017-12-05 13:16:33.635659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1427956 -4.1531968 -4.1606979 -4.1668653 -4.1715717 -4.1691051 -4.1663666 -4.1596832 -4.1510334 -4.1423063 -4.1287251 -4.1191974 -4.1209164 -4.1375089 -4.1551547][-4.1419697 -4.1583986 -4.1725259 -4.1827106 -4.1909084 -4.1913047 -4.1926203 -4.1914129 -4.1846881 -4.1767178 -4.1639991 -4.1522241 -4.1456251 -4.1556544 -4.1710963][-4.1472507 -4.1686268 -4.1848812 -4.1928763 -4.197679 -4.19655 -4.1996622 -4.2038245 -4.2016072 -4.19872 -4.1927209 -4.1818628 -4.17353 -4.1805463 -4.1931429][-4.1559281 -4.1786308 -4.1932278 -4.1962523 -4.1922803 -4.1850123 -4.1832237 -4.1843314 -4.182323 -4.1849947 -4.1906133 -4.18662 -4.1817522 -4.1919613 -4.206109][-4.1556368 -4.1709118 -4.1750183 -4.1659122 -4.1481137 -4.1305041 -4.1227512 -4.1157551 -4.1136031 -4.1263323 -4.1490293 -4.1621962 -4.1694136 -4.1894126 -4.21248][-4.1406236 -4.1423807 -4.1313014 -4.1050625 -4.0740767 -4.0485258 -4.0267291 -4.0025935 -4.0051417 -4.0393491 -4.0829487 -4.113255 -4.1392565 -4.1770325 -4.2159266][-4.1295686 -4.1124454 -4.079587 -4.0316114 -3.9807551 -3.9409995 -3.8903415 -3.8280897 -3.8357024 -3.9183033 -3.9954982 -4.04979 -4.0966368 -4.1564922 -4.2131543][-4.1338315 -4.1022787 -4.0483918 -3.9732676 -3.8955891 -3.8323672 -3.7472014 -3.6375372 -3.6586854 -3.7991238 -3.9080658 -3.9765975 -4.0368576 -4.1135345 -4.1854157][-4.1478686 -4.1223187 -4.0697637 -3.9959307 -3.9182405 -3.8547044 -3.7779727 -3.6851177 -3.7120357 -3.8252952 -3.8988478 -3.9392371 -3.9889898 -4.067626 -4.1491342][-4.1471858 -4.1339531 -4.09859 -4.0512791 -4.0017405 -3.9614983 -3.9186757 -3.8670602 -3.8821545 -3.9350119 -3.9500384 -3.9527388 -3.9829822 -4.0520792 -4.1313224][-4.1361203 -4.1331468 -4.1170955 -4.0961094 -4.0719872 -4.0512309 -4.0318036 -4.0074568 -4.008996 -4.023191 -4.0113497 -4.002346 -4.027884 -4.087141 -4.1531472][-4.1115375 -4.128777 -4.1362576 -4.1359744 -4.1305537 -4.1229258 -4.1149125 -4.1024308 -4.0902452 -4.0753832 -4.0516448 -4.0442495 -4.0729542 -4.129396 -4.18374][-4.1000023 -4.1321611 -4.1556182 -4.1655493 -4.1658764 -4.1565146 -4.1418285 -4.1329827 -4.1135154 -4.0875249 -4.06587 -4.0676227 -4.1017275 -4.1524906 -4.1979227][-4.1026049 -4.1335711 -4.1564732 -4.1641479 -4.1604619 -4.1439118 -4.11977 -4.1106048 -4.0971255 -4.080934 -4.0736032 -4.0856442 -4.1183839 -4.1577768 -4.196918][-4.0982118 -4.1244822 -4.145587 -4.1506467 -4.1480107 -4.1328678 -4.1067204 -4.0980539 -4.0913205 -4.0787406 -4.0715523 -4.0809107 -4.1065321 -4.1430674 -4.1845303]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 13:16:43.286589: step 10010, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 83h:00m:31s remains)
INFO - root - 2017-12-05 13:16:52.363279: step 10020, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 82h:39m:03s remains)
INFO - root - 2017-12-05 13:17:01.363999: step 10030, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 84h:07m:04s remains)
INFO - root - 2017-12-05 13:17:10.214889: step 10040, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 82h:52m:15s remains)
INFO - root - 2017-12-05 13:17:19.458914: step 10050, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 84h:01m:17s remains)
INFO - root - 2017-12-05 13:17:28.433652: step 10060, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 81h:01m:18s remains)
INFO - root - 2017-12-05 13:17:37.501508: step 10070, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 81h:02m:47s remains)
INFO - root - 2017-12-05 13:17:46.570964: step 10080, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 83h:29m:18s remains)
INFO - root - 2017-12-05 13:17:55.792930: step 10090, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 84h:51m:37s remains)
INFO - root - 2017-12-05 13:18:04.785299: step 10100, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 80h:15m:26s remains)
2017-12-05 13:18:05.530913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2240672 -4.2253942 -4.2476287 -4.2855754 -4.3156962 -4.3335586 -4.3383946 -4.3310747 -4.3188739 -4.3043666 -4.2948875 -4.2957931 -4.3010025 -4.3052173 -4.3082314][-4.2139926 -4.2070804 -4.2187843 -4.2524328 -4.286118 -4.3124757 -4.3274121 -4.3288584 -4.3200021 -4.3027129 -4.2876916 -4.2839136 -4.284483 -4.286849 -4.2933078][-4.2049503 -4.1890826 -4.1884003 -4.2127509 -4.2442541 -4.274694 -4.29557 -4.3045278 -4.3030324 -4.2898583 -4.2723165 -4.2614174 -4.2557912 -4.258955 -4.2714925][-4.1951528 -4.1692553 -4.1563907 -4.1692276 -4.1917238 -4.21756 -4.2380176 -4.2547493 -4.2655878 -4.2634754 -4.250361 -4.2372966 -4.22956 -4.23611 -4.25322][-4.18347 -4.1470423 -4.1228523 -4.1190715 -4.1208987 -4.1278491 -4.1401734 -4.166513 -4.1983747 -4.2194982 -4.2217751 -4.217721 -4.2183814 -4.2296319 -4.2449203][-4.1794024 -4.132791 -4.0943494 -4.0664606 -4.0347776 -4.00827 -4.0018225 -4.041194 -4.1069922 -4.1638412 -4.1935625 -4.2087088 -4.2238979 -4.2391682 -4.2476144][-4.1830773 -4.1313605 -4.0792418 -4.0254049 -3.953546 -3.8793917 -3.8424463 -3.8925946 -3.9990995 -4.0981135 -4.16184 -4.1996651 -4.2288132 -4.2485032 -4.2556882][-4.1892347 -4.144526 -4.0905209 -4.0257468 -3.9309945 -3.8212085 -3.7522428 -3.7932611 -3.9107194 -4.02813 -4.1164026 -4.1775002 -4.2207642 -4.2471523 -4.2591591][-4.1827536 -4.1520686 -4.1133518 -4.0662169 -3.9911852 -3.898706 -3.8302386 -3.8399103 -3.9170952 -4.0094061 -4.0904188 -4.1551814 -4.2034211 -4.2339115 -4.2500882][-4.1550097 -4.1378264 -4.1226745 -4.112514 -4.0786185 -4.0282183 -3.9816489 -3.9730732 -4.0100527 -4.0630836 -4.1121216 -4.1548557 -4.1892877 -4.2109776 -4.2225938][-4.1159019 -4.1078949 -4.1178327 -4.1430092 -4.1460128 -4.1322966 -4.1123028 -4.1039772 -4.11963 -4.1420851 -4.1581311 -4.1699958 -4.1806178 -4.1821771 -4.1779995][-4.0985923 -4.0955992 -4.1213026 -4.1674256 -4.1899719 -4.1957092 -4.1909275 -4.1863894 -4.1920619 -4.1965551 -4.1912336 -4.1809106 -4.1696968 -4.1520123 -4.1315222][-4.1265659 -4.1271882 -4.157023 -4.2069526 -4.2310557 -4.2341394 -4.2259269 -4.2160296 -4.2098556 -4.2009611 -4.1855392 -4.1673284 -4.1500168 -4.1293173 -4.1083512][-4.1843767 -4.1874146 -4.2149243 -4.2583847 -4.2737141 -4.2644181 -4.2436023 -4.2180862 -4.1916194 -4.1651793 -4.1418929 -4.1244206 -4.1153 -4.1104641 -4.1073155][-4.2448812 -4.2503958 -4.2726612 -4.3052049 -4.31001 -4.2950859 -4.2688537 -4.2310414 -4.1864934 -4.143889 -4.1128888 -4.0954986 -4.09489 -4.1070571 -4.1250386]]...]
INFO - root - 2017-12-05 13:18:14.764112: step 10110, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 85h:52m:15s remains)
INFO - root - 2017-12-05 13:18:23.815331: step 10120, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 84h:04m:39s remains)
INFO - root - 2017-12-05 13:18:32.584513: step 10130, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 79h:44m:46s remains)
INFO - root - 2017-12-05 13:18:41.539700: step 10140, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 81h:33m:17s remains)
INFO - root - 2017-12-05 13:18:50.739523: step 10150, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 83h:03m:13s remains)
INFO - root - 2017-12-05 13:18:59.785533: step 10160, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 82h:44m:33s remains)
INFO - root - 2017-12-05 13:19:08.895545: step 10170, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 82h:18m:20s remains)
INFO - root - 2017-12-05 13:19:17.930581: step 10180, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 79h:16m:32s remains)
INFO - root - 2017-12-05 13:19:26.993351: step 10190, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 80h:48m:11s remains)
INFO - root - 2017-12-05 13:19:36.363552: step 10200, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 84h:27m:10s remains)
2017-12-05 13:19:37.170754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2373796 -4.2326818 -4.2220025 -4.2167335 -4.2198672 -4.2306447 -4.240674 -4.2442131 -4.2432866 -4.2413588 -4.2399931 -4.2358007 -4.2310243 -4.2299604 -4.2264066][-4.2403526 -4.2318029 -4.2175508 -4.2128568 -4.2175412 -4.2285461 -4.2373257 -4.2398629 -4.2377748 -4.2321706 -4.2255206 -4.2122555 -4.1996894 -4.1966658 -4.1910119][-4.2362318 -4.2203364 -4.2039404 -4.2011 -4.2023253 -4.2067485 -4.2149119 -4.2227902 -4.2227077 -4.2145495 -4.2045197 -4.1881237 -4.1733365 -4.1711426 -4.1656504][-4.2257128 -4.2039495 -4.1853824 -4.1856732 -4.1823421 -4.1772509 -4.187057 -4.2010441 -4.2002997 -4.1946445 -4.1901093 -4.1750069 -4.1576133 -4.149581 -4.1440229][-4.2177939 -4.1959305 -4.17544 -4.1772256 -4.1717157 -4.1622391 -4.1755133 -4.188066 -4.1785712 -4.169786 -4.1722255 -4.162703 -4.1448064 -4.1334438 -4.1290936][-4.2156777 -4.1956339 -4.1766825 -4.1773324 -4.1708989 -4.1577992 -4.1677227 -4.1676278 -4.1423354 -4.1317396 -4.1411424 -4.137588 -4.1223664 -4.1152034 -4.1159387][-4.2059159 -4.1874285 -4.1678238 -4.1690245 -4.1597018 -4.1411824 -4.1392136 -4.1158881 -4.0693583 -4.0630794 -4.0920734 -4.0986223 -4.0890965 -4.0888309 -4.0979323][-4.18365 -4.1629052 -4.1408949 -4.1385169 -4.1195421 -4.0906281 -4.0713868 -4.026844 -3.9710405 -3.9853063 -4.0416441 -4.0675035 -4.0746155 -4.0917993 -4.114193][-4.13966 -4.1146264 -4.0856624 -4.0758047 -4.0497432 -4.0182772 -3.9994612 -3.9654388 -3.9349859 -3.9764366 -4.042418 -4.0746861 -4.092186 -4.1180444 -4.1431918][-4.0927896 -4.0674343 -4.0342541 -4.0191751 -3.9997239 -3.9847245 -3.987237 -3.9821196 -3.9831021 -4.0268426 -4.0737448 -4.0944257 -4.1074796 -4.1278639 -4.1481643][-4.0817838 -4.0600467 -4.0307178 -4.0177274 -4.0072947 -4.0091686 -4.025691 -4.0355816 -4.0475216 -4.0795722 -4.1050386 -4.1128221 -4.1197724 -4.1338882 -4.1495819][-4.1206822 -4.104403 -4.0826197 -4.0712957 -4.0628552 -4.0684175 -4.0845494 -4.0936184 -4.1017179 -4.1188393 -4.1310835 -4.1347189 -4.1413155 -4.1533446 -4.1671352][-4.1774664 -4.1674161 -4.155232 -4.1472893 -4.1391807 -4.140605 -4.1474991 -4.1480656 -4.1493664 -4.1568055 -4.1653 -4.169477 -4.1755924 -4.1855493 -4.1970692][-4.2335582 -4.2300115 -4.226408 -4.2201996 -4.2129569 -4.2102442 -4.2095575 -4.2065039 -4.2050824 -4.2087259 -4.2145934 -4.2177343 -4.2212973 -4.2272625 -4.2335153][-4.2858357 -4.2872677 -4.2872872 -4.2831545 -4.2797737 -4.2789674 -4.2780724 -4.27512 -4.2726674 -4.2727704 -4.2737103 -4.2745814 -4.2767568 -4.2799892 -4.2824755]]...]
INFO - root - 2017-12-05 13:19:46.315930: step 10210, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 81h:58m:39s remains)
INFO - root - 2017-12-05 13:19:55.232215: step 10220, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 77h:27m:38s remains)
INFO - root - 2017-12-05 13:20:04.271753: step 10230, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.919 sec/batch; 82h:14m:52s remains)
INFO - root - 2017-12-05 13:20:13.400899: step 10240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 82h:06m:27s remains)
INFO - root - 2017-12-05 13:20:22.477775: step 10250, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 82h:35m:48s remains)
INFO - root - 2017-12-05 13:20:31.651678: step 10260, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 80h:15m:56s remains)
INFO - root - 2017-12-05 13:20:40.682149: step 10270, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 81h:41m:19s remains)
INFO - root - 2017-12-05 13:20:49.782952: step 10280, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.912 sec/batch; 81h:38m:05s remains)
INFO - root - 2017-12-05 13:20:59.029684: step 10290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 80h:47m:11s remains)
INFO - root - 2017-12-05 13:21:08.043233: step 10300, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 81h:10m:25s remains)
2017-12-05 13:21:08.801695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3299246 -4.3251743 -4.3168421 -4.3038435 -4.2891631 -4.2812366 -4.2825055 -4.2927995 -4.3064413 -4.3176389 -4.3264003 -4.3314738 -4.332108 -4.3306322 -4.3303437][-4.3273721 -4.3180604 -4.3044176 -4.2859516 -4.2642412 -4.251349 -4.2515478 -4.2652678 -4.2867193 -4.30519 -4.3191504 -4.3274803 -4.330677 -4.330833 -4.331069][-4.3227658 -4.3114667 -4.2948456 -4.272541 -4.2435536 -4.2186852 -4.2087255 -4.2208443 -4.2503967 -4.2786183 -4.3001046 -4.3158607 -4.3237224 -4.3276548 -4.3296537][-4.3182564 -4.3069243 -4.2901816 -4.2642279 -4.2237172 -4.1783648 -4.1483641 -4.152627 -4.1893539 -4.2331285 -4.2695608 -4.2996564 -4.3155112 -4.3242745 -4.3287787][-4.3163271 -4.3029246 -4.2824359 -4.2462878 -4.1884127 -4.1188869 -4.0671082 -4.0633082 -4.1064281 -4.1689982 -4.2277403 -4.2798762 -4.3071566 -4.3220577 -4.3291025][-4.3103991 -4.294477 -4.2688432 -4.2220206 -4.1456885 -4.0517721 -3.9775429 -3.9646077 -4.0146942 -4.0964775 -4.1815667 -4.2555275 -4.2938323 -4.314857 -4.3258471][-4.2940464 -4.2791567 -4.2511778 -4.194941 -4.0996389 -3.9788623 -3.87687 -3.8522096 -3.9139917 -4.0247369 -4.1387811 -4.2306542 -4.2801504 -4.308249 -4.3229012][-4.2690349 -4.2580352 -4.2308745 -4.17199 -4.0678973 -3.9331996 -3.8135309 -3.7757652 -3.844624 -3.9800911 -4.1164975 -4.2215419 -4.2795053 -4.310791 -4.3251963][-4.233335 -4.2291422 -4.2113242 -4.1669059 -4.0764723 -3.957597 -3.8526125 -3.8139179 -3.87473 -4.0046124 -4.1383376 -4.2394271 -4.2968278 -4.3231792 -4.3320012][-4.1955166 -4.2066469 -4.209681 -4.1896758 -4.124794 -4.0370474 -3.9602075 -3.9310043 -3.977509 -4.0786433 -4.1873889 -4.26807 -4.3149834 -4.3336225 -4.335906][-4.1858115 -4.2123771 -4.2342029 -4.2333107 -4.1903539 -4.1279368 -4.0734687 -4.0528579 -4.0859385 -4.1585717 -4.2357769 -4.2910571 -4.3240905 -4.3357916 -4.3345795][-4.223896 -4.2545314 -4.2797303 -4.2845759 -4.2567434 -4.2170014 -4.1831965 -4.1696067 -4.1906929 -4.2351589 -4.2799344 -4.3105874 -4.3300872 -4.33533 -4.3322606][-4.2821393 -4.3052831 -4.3226848 -4.3260555 -4.3094831 -4.2896671 -4.2723546 -4.2641497 -4.2734103 -4.2940488 -4.31339 -4.3258815 -4.3346267 -4.3341379 -4.3301811][-4.3291006 -4.3368349 -4.3425813 -4.3435359 -4.3356071 -4.3286409 -4.3216767 -4.3172927 -4.3198509 -4.3256807 -4.3305564 -4.3338203 -4.3356419 -4.3329353 -4.3299069][-4.3478456 -4.3451219 -4.3444724 -4.34403 -4.3425913 -4.342958 -4.3420877 -4.3398638 -4.3389611 -4.3382816 -4.3371868 -4.3361144 -4.3346272 -4.3326168 -4.3311276]]...]
INFO - root - 2017-12-05 13:21:18.009119: step 10310, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 80h:32m:25s remains)
INFO - root - 2017-12-05 13:21:26.892707: step 10320, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 84h:04m:58s remains)
INFO - root - 2017-12-05 13:21:35.886073: step 10330, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 80h:04m:48s remains)
INFO - root - 2017-12-05 13:21:45.066010: step 10340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 82h:05m:22s remains)
INFO - root - 2017-12-05 13:21:54.336248: step 10350, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 79h:24m:29s remains)
INFO - root - 2017-12-05 13:22:03.487021: step 10360, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 82h:40m:19s remains)
INFO - root - 2017-12-05 13:22:12.673778: step 10370, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 81h:32m:10s remains)
INFO - root - 2017-12-05 13:22:21.814085: step 10380, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 84h:10m:11s remains)
INFO - root - 2017-12-05 13:22:30.983906: step 10390, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 83h:50m:04s remains)
INFO - root - 2017-12-05 13:22:40.350668: step 10400, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 80h:23m:51s remains)
2017-12-05 13:22:41.143273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2492442 -4.2421041 -4.2324948 -4.22576 -4.2214003 -4.2145348 -4.2025557 -4.2005515 -4.21654 -4.2341323 -4.2396765 -4.2288437 -4.2118592 -4.2021933 -4.2070251][-4.2338195 -4.2233443 -4.2085481 -4.1954575 -4.1841154 -4.1693926 -4.1531968 -4.1537828 -4.1803489 -4.2115264 -4.2226772 -4.2111597 -4.18998 -4.1785164 -4.1846623][-4.2158022 -4.2027397 -4.1837454 -4.1629872 -4.1432128 -4.12114 -4.1006413 -4.1042132 -4.1436224 -4.18915 -4.2078094 -4.2039366 -4.1869526 -4.1768255 -4.1795306][-4.2024722 -4.1887212 -4.16694 -4.1412 -4.1126456 -4.0780168 -4.0489793 -4.0527744 -4.1053557 -4.1637349 -4.1936159 -4.2078161 -4.2075768 -4.2051249 -4.2051153][-4.1959052 -4.18494 -4.1655245 -4.1353526 -4.093576 -4.0391493 -3.994065 -3.996851 -4.0628467 -4.1342177 -4.1769714 -4.2080412 -4.2259793 -4.235631 -4.2372603][-4.1891608 -4.186451 -4.1740975 -4.1396666 -4.0792928 -3.9973555 -3.92577 -3.9281595 -4.01756 -4.1101093 -4.1662636 -4.2069387 -4.2327557 -4.2486877 -4.25228][-4.1772442 -4.1866579 -4.1841145 -4.1444025 -4.0647049 -3.9499879 -3.841723 -3.845439 -3.9605036 -4.0715585 -4.1447258 -4.1942172 -4.2270246 -4.2462692 -4.2502031][-4.1603951 -4.1848097 -4.194715 -4.157393 -4.0742178 -3.9482582 -3.8249722 -3.8245666 -3.932735 -4.0357022 -4.1118703 -4.1654053 -4.2030592 -4.2250247 -4.227222][-4.1454659 -4.1811361 -4.1987858 -4.1731591 -4.1088805 -4.0079226 -3.9133346 -3.9001584 -3.9558969 -4.0193806 -4.0775247 -4.1268444 -4.1650119 -4.1875725 -4.1859384][-4.1427279 -4.1774268 -4.1953974 -4.1829419 -4.1431079 -4.0786896 -4.0191646 -3.9931726 -3.9963002 -4.0147042 -4.0430903 -4.0779281 -4.1116743 -4.1352015 -4.1334743][-4.1550574 -4.1828685 -4.1993427 -4.1968656 -4.1758652 -4.138658 -4.101727 -4.0713124 -4.0464993 -4.0339489 -4.0295372 -4.0419707 -4.0647521 -4.0877671 -4.0897603][-4.180439 -4.2010565 -4.2117724 -4.2105694 -4.1987634 -4.1768208 -4.1501904 -4.1225982 -4.0961623 -4.0733647 -4.046598 -4.0363188 -4.040792 -4.0546384 -4.063776][-4.2026477 -4.2180357 -4.2196765 -4.211247 -4.2030458 -4.1918526 -4.1750989 -4.1560645 -4.1385784 -4.1138458 -4.0730238 -4.0439353 -4.0324721 -4.039094 -4.0564427][-4.21627 -4.228519 -4.2199626 -4.2023063 -4.1950006 -4.1939378 -4.1899619 -4.1819911 -4.1693864 -4.1405711 -4.0916424 -4.0488324 -4.0281577 -4.0371251 -4.0673594][-4.2215209 -4.2365818 -4.2260423 -4.2035832 -4.1966577 -4.2020383 -4.2080173 -4.2080336 -4.1951995 -4.1595173 -4.1057768 -4.0585332 -4.0361896 -4.0513616 -4.0901642]]...]
INFO - root - 2017-12-05 13:22:50.087007: step 10410, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 80h:20m:46s remains)
INFO - root - 2017-12-05 13:22:59.340387: step 10420, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 78h:30m:32s remains)
INFO - root - 2017-12-05 13:23:08.709258: step 10430, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 82h:38m:26s remains)
INFO - root - 2017-12-05 13:23:17.792821: step 10440, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 79h:19m:24s remains)
INFO - root - 2017-12-05 13:23:26.820193: step 10450, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.892 sec/batch; 79h:46m:33s remains)
INFO - root - 2017-12-05 13:23:35.757790: step 10460, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.820 sec/batch; 73h:22m:44s remains)
INFO - root - 2017-12-05 13:23:44.966499: step 10470, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 82h:10m:50s remains)
INFO - root - 2017-12-05 13:23:54.355045: step 10480, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 81h:51m:24s remains)
INFO - root - 2017-12-05 13:24:03.648689: step 10490, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 80h:41m:38s remains)
INFO - root - 2017-12-05 13:24:12.706285: step 10500, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 74h:22m:30s remains)
2017-12-05 13:24:13.464510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846441 -4.2847414 -4.2789726 -4.27357 -4.2751989 -4.2845111 -4.2982159 -4.310504 -4.3166151 -4.3177981 -4.3191056 -4.3222756 -4.3204727 -4.3092561 -4.281765][-4.2650309 -4.2676005 -4.2659631 -4.2675686 -4.2771063 -4.2902226 -4.3054833 -4.3177271 -4.3219237 -4.3202138 -4.3221688 -4.3258038 -4.3202243 -4.2994432 -4.2629952][-4.2426004 -4.250268 -4.2533035 -4.2609925 -4.2775073 -4.2934093 -4.3068647 -4.3161669 -4.3196726 -4.3193083 -4.3240423 -4.3250556 -4.3121767 -4.2818937 -4.240778][-4.2149496 -4.2309084 -4.240303 -4.2560282 -4.2789249 -4.2960005 -4.3061519 -4.3113894 -4.3112493 -4.3101892 -4.31239 -4.3064404 -4.2846251 -4.2488875 -4.2126789][-4.1871395 -4.2148094 -4.2342072 -4.2572532 -4.2827125 -4.2976928 -4.3007941 -4.2950177 -4.2839408 -4.2761903 -4.2711434 -4.2592778 -4.2355433 -4.2054543 -4.1852221][-4.1727047 -4.2097783 -4.2336764 -4.2573094 -4.2785444 -4.2856045 -4.276505 -4.251533 -4.2259674 -4.21451 -4.2091746 -4.2010813 -4.1882334 -4.1764207 -4.1787214][-4.1797304 -4.2128997 -4.2310562 -4.24654 -4.2597218 -4.2569923 -4.2310915 -4.1860447 -4.1509538 -4.1454477 -4.1516538 -4.1586342 -4.1660986 -4.1764584 -4.1961241][-4.2048554 -4.2224503 -4.230176 -4.237062 -4.2405291 -4.2271967 -4.1858034 -4.1267281 -4.0927792 -4.1056075 -4.1312575 -4.1575246 -4.1820331 -4.2043343 -4.22929][-4.2309394 -4.2339416 -4.2342863 -4.2331429 -4.229898 -4.2110868 -4.162622 -4.1013374 -4.0843511 -4.1232691 -4.1624141 -4.1947269 -4.2209735 -4.24072 -4.2608004][-4.2388706 -4.2360654 -4.2359648 -4.2317657 -4.2270727 -4.2109075 -4.1699753 -4.1238828 -4.1289768 -4.1779661 -4.2150478 -4.2402425 -4.2571549 -4.2688246 -4.2825484][-4.2344894 -4.233602 -4.2367449 -4.2354879 -4.2336769 -4.2256613 -4.2004495 -4.1723013 -4.1837029 -4.2211308 -4.2475767 -4.2640772 -4.2737651 -4.2819996 -4.292625][-4.2297978 -4.231668 -4.2359428 -4.2386031 -4.240768 -4.239502 -4.2255125 -4.2075591 -4.2148175 -4.23534 -4.2523117 -4.2645283 -4.2707028 -4.2781367 -4.2862196][-4.2355084 -4.2372174 -4.2390585 -4.2455759 -4.2487092 -4.2462287 -4.23343 -4.2196078 -4.222445 -4.2330294 -4.2464719 -4.255002 -4.2578073 -4.2632637 -4.2683434][-4.24006 -4.2383876 -4.2359824 -4.2425513 -4.243979 -4.2393446 -4.2294154 -4.2214775 -4.2244787 -4.2309093 -4.2400484 -4.2424879 -4.2410235 -4.2434416 -4.2443347][-4.241416 -4.236836 -4.2322478 -4.2382483 -4.2375555 -4.2327876 -4.2248259 -4.222208 -4.225585 -4.230689 -4.2367072 -4.2364087 -4.2326794 -4.2299647 -4.225945]]...]
INFO - root - 2017-12-05 13:24:22.610244: step 10510, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 80h:11m:27s remains)
INFO - root - 2017-12-05 13:24:31.847948: step 10520, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 83h:28m:41s remains)
INFO - root - 2017-12-05 13:24:40.857543: step 10530, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 82h:37m:52s remains)
INFO - root - 2017-12-05 13:24:50.039228: step 10540, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 78h:30m:56s remains)
INFO - root - 2017-12-05 13:24:59.167278: step 10550, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 80h:10m:06s remains)
INFO - root - 2017-12-05 13:25:08.420596: step 10560, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 85h:50m:25s remains)
INFO - root - 2017-12-05 13:25:17.489413: step 10570, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 80h:10m:17s remains)
INFO - root - 2017-12-05 13:25:26.728107: step 10580, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 80h:47m:31s remains)
INFO - root - 2017-12-05 13:25:35.776820: step 10590, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 79h:08m:25s remains)
INFO - root - 2017-12-05 13:25:44.857492: step 10600, loss = 2.09, batch loss = 2.03 (8.1 examples/sec; 0.985 sec/batch; 88h:04m:49s remains)
2017-12-05 13:25:45.646306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.119154 -4.0692539 -4.0108409 -3.9642766 -4.0074162 -4.070118 -4.1020365 -4.1250873 -4.1368079 -4.1421514 -4.1414547 -4.1340961 -4.1312103 -4.1367426 -4.1362424][-4.0964108 -4.0465488 -3.9937723 -3.9573812 -4.0013595 -4.05091 -4.0688143 -4.0832782 -4.0948534 -4.1064715 -4.1170359 -4.1185303 -4.1234579 -4.1365552 -4.1436439][-4.0835838 -4.0380788 -3.9996078 -3.9835463 -4.0231576 -4.05708 -4.05933 -4.06502 -4.0785489 -4.1037111 -4.128881 -4.1367121 -4.1424079 -4.1492276 -4.1506777][-4.0986309 -4.05852 -4.0356245 -4.0402513 -4.0758228 -4.0966859 -4.0866451 -4.0824027 -4.0975513 -4.128531 -4.1580305 -4.1681495 -4.1728673 -4.1725883 -4.16595][-4.130456 -4.0926 -4.0752716 -4.09063 -4.1230478 -4.1375508 -4.1324878 -4.1375318 -4.156199 -4.1767473 -4.1879482 -4.1852188 -4.1838655 -4.1816378 -4.1753683][-4.15734 -4.1182718 -4.09674 -4.1064339 -4.1294007 -4.1400394 -4.147337 -4.1732278 -4.207962 -4.2215738 -4.211647 -4.1893125 -4.1755548 -4.1748047 -4.1782508][-4.1689548 -4.1213837 -4.0871568 -4.0812359 -4.0895619 -4.089838 -4.100069 -4.1395431 -4.1897845 -4.2066603 -4.1912289 -4.168714 -4.1575646 -4.1705308 -4.1881804][-4.1539044 -4.0950904 -4.0491419 -4.0292549 -4.0271955 -4.0181437 -4.0183725 -4.051281 -4.1055784 -4.1301551 -4.1230764 -4.1164103 -4.1272435 -4.167016 -4.2036858][-4.1208711 -4.0534067 -4.0017228 -3.9758134 -3.970118 -3.9578581 -3.9435503 -3.9591696 -4.0082521 -4.036582 -4.0405569 -4.0625682 -4.1063161 -4.1658211 -4.2062869][-4.0919919 -4.0159993 -3.9587898 -3.9326267 -3.928875 -3.9175084 -3.8946316 -3.8985691 -3.9420121 -3.9690158 -3.9836805 -4.0375338 -4.1088414 -4.170948 -4.2001133][-4.0864496 -4.0073214 -3.9481895 -3.925241 -3.9293606 -3.9258502 -3.9123421 -3.9151478 -3.941452 -3.9559495 -3.97434 -4.0394211 -4.1176333 -4.1683221 -4.1804843][-4.1030049 -4.0332155 -3.9842312 -3.9689076 -3.976732 -3.9822507 -3.9848735 -3.9921954 -4.0076847 -4.018024 -4.0326519 -4.0798426 -4.1335793 -4.1590796 -4.1546364][-4.1262312 -4.0673113 -4.0360789 -4.0346007 -4.0479603 -4.0618591 -4.0687151 -4.073678 -4.088903 -4.1062632 -4.1178613 -4.137917 -4.1590843 -4.1599078 -4.140276][-4.1503038 -4.0997133 -4.0806403 -4.0916414 -4.116292 -4.1370606 -4.1435213 -4.1482043 -4.1618323 -4.180057 -4.189558 -4.1935916 -4.1926441 -4.1774864 -4.1474223][-4.1653576 -4.1222305 -4.1111994 -4.1299391 -4.1623006 -4.187717 -4.1979289 -4.206337 -4.2196016 -4.2347913 -4.2414231 -4.2409353 -4.2320004 -4.2089982 -4.1737971]]...]
INFO - root - 2017-12-05 13:25:54.853066: step 10610, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 82h:30m:22s remains)
INFO - root - 2017-12-05 13:26:03.960988: step 10620, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 82h:31m:38s remains)
INFO - root - 2017-12-05 13:26:13.071642: step 10630, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 84h:49m:39s remains)
INFO - root - 2017-12-05 13:26:22.337748: step 10640, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 81h:30m:07s remains)
INFO - root - 2017-12-05 13:26:31.353267: step 10650, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 81h:25m:16s remains)
INFO - root - 2017-12-05 13:26:40.533817: step 10660, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 80h:36m:59s remains)
INFO - root - 2017-12-05 13:26:49.647365: step 10670, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 79h:47m:45s remains)
INFO - root - 2017-12-05 13:26:58.644895: step 10680, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 80h:15m:05s remains)
INFO - root - 2017-12-05 13:27:07.699642: step 10690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 75h:23m:28s remains)
INFO - root - 2017-12-05 13:27:16.808793: step 10700, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 81h:09m:05s remains)
2017-12-05 13:27:17.576854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1702423 -4.2005439 -4.2279415 -4.2488337 -4.2614155 -4.2651296 -4.2625275 -4.2566237 -4.24998 -4.2437134 -4.2364717 -4.2292233 -4.2228336 -4.2167211 -4.2107139][-4.2020473 -4.2284613 -4.2483687 -4.2620573 -4.2676315 -4.26564 -4.2595868 -4.2531819 -4.247086 -4.2413149 -4.2339911 -4.2264996 -4.2207851 -4.2167516 -4.2134743][-4.2363296 -4.2575006 -4.2675295 -4.2699981 -4.2654471 -4.2568669 -4.2481761 -4.2420721 -4.2371597 -4.23225 -4.2252192 -4.2175417 -4.2124939 -4.2107153 -4.2103577][-4.2558823 -4.2685785 -4.2672405 -4.2579074 -4.2443643 -4.2313952 -4.2220473 -4.2176018 -4.2155671 -4.2140803 -4.2102046 -4.2051811 -4.202652 -4.204061 -4.2070322][-4.258637 -4.2601652 -4.2468843 -4.2282062 -4.2085295 -4.1933041 -4.1845803 -4.1834435 -4.1871815 -4.1924057 -4.1950035 -4.1955914 -4.197248 -4.2013454 -4.2070107][-4.2520247 -4.243331 -4.2231731 -4.19979 -4.1774306 -4.1615458 -4.1549773 -4.1587486 -4.1694174 -4.1820879 -4.1914835 -4.1974649 -4.2022729 -4.2072444 -4.2125406][-4.2427673 -4.2310615 -4.2115273 -4.1898522 -4.1702008 -4.1575565 -4.1547771 -4.162147 -4.1757703 -4.1903319 -4.2010884 -4.208097 -4.2132325 -4.2174668 -4.2215681][-4.2389746 -4.229249 -4.2139177 -4.1966944 -4.18165 -4.1728325 -4.1726365 -4.1805453 -4.1923437 -4.20338 -4.2108254 -4.2156086 -4.2196655 -4.2234631 -4.2271438][-4.2385488 -4.2316213 -4.2208261 -4.2086329 -4.1983776 -4.1931081 -4.1943612 -4.2008181 -4.2084913 -4.2141809 -4.2170844 -4.2189713 -4.2216845 -4.2252092 -4.2288313][-4.2410975 -4.2364511 -4.2293963 -4.2216015 -4.21511 -4.2124543 -4.2143965 -4.2188807 -4.2225609 -4.2235494 -4.222578 -4.2218127 -4.222949 -4.2256422 -4.2290215][-4.2450175 -4.2423081 -4.2385564 -4.23423 -4.2302427 -4.2286568 -4.2299008 -4.2319679 -4.2326069 -4.2310667 -4.2285461 -4.2269158 -4.2273531 -4.2294188 -4.2323375][-4.2472754 -4.2463479 -4.2449555 -4.24296 -4.2407627 -4.2398734 -4.2405539 -4.2412915 -4.2409992 -4.239213 -4.23687 -4.2353249 -4.2352819 -4.2365994 -4.2389374][-4.2459679 -4.245945 -4.245687 -4.24511 -4.244173 -4.2439051 -4.2446508 -4.24517 -4.2451906 -4.2442122 -4.2429481 -4.2422566 -4.2422428 -4.2429066 -4.2445493][-4.24139 -4.2415762 -4.241498 -4.2413716 -4.2409592 -4.2408404 -4.2414346 -4.2419577 -4.2424555 -4.2422905 -4.2420154 -4.2421484 -4.24242 -4.242991 -4.2441907][-4.2351155 -4.2349815 -4.2346535 -4.2344079 -4.2340765 -4.2338762 -4.2342486 -4.2346139 -4.2350054 -4.2347951 -4.2345009 -4.2347422 -4.2351422 -4.2356367 -4.2365117]]...]
INFO - root - 2017-12-05 13:27:26.689008: step 10710, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:52m:37s remains)
INFO - root - 2017-12-05 13:27:35.850647: step 10720, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 81h:45m:20s remains)
INFO - root - 2017-12-05 13:27:45.052929: step 10730, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 79h:41m:39s remains)
INFO - root - 2017-12-05 13:27:54.252994: step 10740, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.967 sec/batch; 86h:24m:51s remains)
INFO - root - 2017-12-05 13:28:03.327466: step 10750, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 77h:25m:43s remains)
INFO - root - 2017-12-05 13:28:12.571439: step 10760, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 83h:57m:25s remains)
INFO - root - 2017-12-05 13:28:21.537327: step 10770, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 79h:38m:45s remains)
INFO - root - 2017-12-05 13:28:30.569651: step 10780, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 79h:25m:34s remains)
INFO - root - 2017-12-05 13:28:39.510496: step 10790, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 82h:00m:12s remains)
INFO - root - 2017-12-05 13:28:48.574244: step 10800, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 79h:57m:25s remains)
2017-12-05 13:28:49.313747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32506 -4.3249245 -4.3206 -4.3151708 -4.3080564 -4.2998991 -4.2923384 -4.2888656 -4.2863903 -4.2804828 -4.2744656 -4.2624254 -4.2457004 -4.2284694 -4.2079458][-4.3154011 -4.3110123 -4.3042274 -4.2949262 -4.2826014 -4.2693963 -4.2583532 -4.253067 -4.250226 -4.2456589 -4.2434144 -4.2333736 -4.2164931 -4.1935673 -4.1629329][-4.2962213 -4.289032 -4.2799177 -4.2630491 -4.2426815 -4.2281203 -4.2205868 -4.2184262 -4.217936 -4.2168741 -4.2186246 -4.2121572 -4.1947484 -4.1648388 -4.1256886][-4.2732296 -4.2642307 -4.2514033 -4.2300897 -4.2085114 -4.1960769 -4.1917686 -4.1940222 -4.1968832 -4.1962171 -4.2042747 -4.205452 -4.1906409 -4.1617069 -4.1282606][-4.2492175 -4.2397575 -4.2252679 -4.2035823 -4.1820974 -4.1682706 -4.1594253 -4.1576257 -4.1600776 -4.1636481 -4.1827688 -4.2018151 -4.2034254 -4.1902614 -4.1721306][-4.2298117 -4.2217875 -4.207613 -4.1873703 -4.1665053 -4.1436248 -4.1142554 -4.0912447 -4.0940094 -4.1112309 -4.1427031 -4.17612 -4.1936126 -4.1998119 -4.19813][-4.229846 -4.225256 -4.2075067 -4.1783495 -4.1467242 -4.1010528 -4.032733 -3.9795327 -3.9936788 -4.039979 -4.0839744 -4.1239691 -4.1569991 -4.1851344 -4.2011738][-4.2388363 -4.2364011 -4.2136312 -4.1678667 -4.1165533 -4.050302 -3.9579678 -3.8863444 -3.9219811 -3.9985256 -4.0490823 -4.0884891 -4.1327887 -4.1796675 -4.2130251][-4.2407961 -4.2399969 -4.2134681 -4.1611872 -4.103766 -4.042491 -3.9729404 -3.9259787 -3.9612432 -4.0257626 -4.0627656 -4.0966415 -4.145854 -4.1993275 -4.2390909][-4.216392 -4.2137828 -4.1978555 -4.1667619 -4.1284461 -4.0929966 -4.0613885 -4.0401096 -4.0524631 -4.0797191 -4.1019807 -4.1332316 -4.1807241 -4.2284765 -4.2619696][-4.1646614 -4.1684418 -4.1784472 -4.1806893 -4.1692305 -4.1544609 -4.1460133 -4.1370616 -4.1366844 -4.1423159 -4.1520257 -4.1737828 -4.2077689 -4.2411819 -4.2640986][-4.116817 -4.1443477 -4.1802974 -4.1973987 -4.1984415 -4.1947947 -4.1967978 -4.1950593 -4.1900592 -4.1856403 -4.1839967 -4.1921983 -4.211359 -4.2346616 -4.2519727][-4.1024919 -4.1513677 -4.1885462 -4.2020092 -4.204597 -4.2010937 -4.2029743 -4.2052574 -4.2042537 -4.2005253 -4.1987114 -4.2024126 -4.2148685 -4.2330437 -4.2500739][-4.1232181 -4.1650529 -4.186728 -4.1912761 -4.188695 -4.1804214 -4.1805716 -4.1885304 -4.1966062 -4.2034583 -4.2108059 -4.2189474 -4.2344832 -4.25564 -4.2745991][-4.1454835 -4.1661625 -4.1736994 -4.1720657 -4.1704011 -4.1674085 -4.1728206 -4.1900139 -4.2105827 -4.2297292 -4.2449646 -4.2560353 -4.2718635 -4.289196 -4.3026872]]...]
INFO - root - 2017-12-05 13:28:58.341039: step 10810, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 80h:44m:44s remains)
INFO - root - 2017-12-05 13:29:07.648990: step 10820, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 83h:07m:39s remains)
INFO - root - 2017-12-05 13:29:16.711817: step 10830, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 79h:13m:32s remains)
INFO - root - 2017-12-05 13:29:25.791438: step 10840, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 80h:46m:05s remains)
INFO - root - 2017-12-05 13:29:35.044441: step 10850, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.901 sec/batch; 80h:31m:48s remains)
INFO - root - 2017-12-05 13:29:44.272668: step 10860, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 84h:43m:12s remains)
INFO - root - 2017-12-05 13:29:53.361326: step 10870, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 79h:11m:13s remains)
INFO - root - 2017-12-05 13:30:02.230351: step 10880, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 81h:19m:30s remains)
INFO - root - 2017-12-05 13:30:11.499634: step 10890, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 81h:34m:19s remains)
INFO - root - 2017-12-05 13:30:20.637847: step 10900, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 81h:25m:57s remains)
2017-12-05 13:30:21.407047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2563252 -4.2481413 -4.236392 -4.2222681 -4.2112894 -4.21049 -4.2140269 -4.214046 -4.2081585 -4.1976357 -4.1793528 -4.160553 -4.1564512 -4.1717243 -4.1931596][-4.2495794 -4.2405138 -4.220614 -4.1993289 -4.1881571 -4.1914263 -4.197063 -4.1968822 -4.1914072 -4.1794477 -4.1597481 -4.1436067 -4.1475115 -4.1708183 -4.1966691][-4.2134404 -4.2120371 -4.1890888 -4.1626482 -4.1533222 -4.1608381 -4.1691122 -4.1700544 -4.1648827 -4.1507936 -4.13056 -4.1163239 -4.1248231 -4.1520138 -4.1788464][-4.1753554 -4.1821489 -4.1593275 -4.1279316 -4.1167469 -4.1248732 -4.136373 -4.1417351 -4.1366348 -4.1211424 -4.10401 -4.093658 -4.1063218 -4.1365886 -4.1662087][-4.1124554 -4.1229649 -4.1003962 -4.0697465 -4.0623865 -4.0775337 -4.0995975 -4.1160688 -4.1211319 -4.11334 -4.1078968 -4.10918 -4.1313128 -4.1642494 -4.1894603][-3.995357 -4.0108633 -4.0006833 -3.9854491 -3.9961185 -4.0256295 -4.0661445 -4.1045995 -4.128242 -4.1349378 -4.1382666 -4.1452308 -4.1691179 -4.1999249 -4.2173829][-3.8322818 -3.8656592 -3.8721361 -3.8740654 -3.9001846 -3.9442348 -4.0054116 -4.0660176 -4.1060176 -4.1214275 -4.1273375 -4.1356778 -4.1604886 -4.1892371 -4.2040515][-3.7054298 -3.7603755 -3.7813873 -3.7948263 -3.8259659 -3.8769138 -3.9491875 -4.0204034 -4.0658722 -4.0875597 -4.0960746 -4.1055946 -4.1273255 -4.1520853 -4.1651974][-3.7584727 -3.8174314 -3.8435729 -3.8608224 -3.88792 -3.9323711 -3.9924426 -4.0465913 -4.0810952 -4.0949922 -4.0958343 -4.0946097 -4.1028442 -4.1168175 -4.1283379][-3.9132891 -3.9528875 -3.9692779 -3.9820404 -3.9987676 -4.0285892 -4.0675898 -4.1025949 -4.125617 -4.1284113 -4.119205 -4.1088586 -4.1086793 -4.1150894 -4.1292381][-4.0568509 -4.0790558 -4.088058 -4.0966668 -4.1091571 -4.1283941 -4.1512985 -4.1726 -4.1881633 -4.1857204 -4.1727138 -4.159903 -4.1555748 -4.1566162 -4.1672168][-4.1679354 -4.179594 -4.1828513 -4.188992 -4.1984887 -4.2112694 -4.2240028 -4.2348671 -4.2431531 -4.2412233 -4.2277389 -4.2137194 -4.2052631 -4.2002182 -4.2043724][-4.2495022 -4.2559047 -4.2557092 -4.2584772 -4.2626405 -4.2698336 -4.2771 -4.2834048 -4.2883849 -4.2872858 -4.2755833 -4.2609878 -4.2472539 -4.2350273 -4.2303762][-4.2942381 -4.2985344 -4.2974868 -4.2969751 -4.2978754 -4.3019867 -4.3058038 -4.3096175 -4.3132448 -4.3137031 -4.3035913 -4.2864361 -4.2678771 -4.2502556 -4.2381215][-4.2983794 -4.302268 -4.3025751 -4.3011971 -4.3005009 -4.3021398 -4.3042793 -4.3070154 -4.3100824 -4.3105369 -4.3008609 -4.2828197 -4.263341 -4.2451639 -4.2297835]]...]
INFO - root - 2017-12-05 13:30:30.514709: step 10910, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 81h:10m:05s remains)
INFO - root - 2017-12-05 13:30:39.606947: step 10920, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 80h:22m:09s remains)
INFO - root - 2017-12-05 13:30:48.652909: step 10930, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 80h:58m:32s remains)
INFO - root - 2017-12-05 13:30:57.846400: step 10940, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 80h:37m:18s remains)
INFO - root - 2017-12-05 13:31:06.919411: step 10950, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 81h:22m:33s remains)
INFO - root - 2017-12-05 13:31:15.956965: step 10960, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 79h:14m:56s remains)
INFO - root - 2017-12-05 13:31:25.102471: step 10970, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 84h:35m:12s remains)
INFO - root - 2017-12-05 13:31:33.931357: step 10980, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 81h:13m:07s remains)
INFO - root - 2017-12-05 13:31:43.022285: step 10990, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 83h:29m:08s remains)
INFO - root - 2017-12-05 13:31:52.162575: step 11000, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 84h:49m:24s remains)
2017-12-05 13:31:52.997394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.339745 -4.3330593 -4.3281283 -4.3282914 -4.3331137 -4.3382835 -4.3409028 -4.341116 -4.343286 -4.3471503 -4.3492556 -4.3477921 -4.3466783 -4.3497791 -4.3524561][-4.3233294 -4.3118753 -4.3023057 -4.3006144 -4.306983 -4.3146052 -4.3179445 -4.3187275 -4.3222322 -4.330359 -4.3351855 -4.3332319 -4.3331356 -4.339282 -4.3459864][-4.3000231 -4.2840567 -4.2706556 -4.2663484 -4.2734346 -4.2826133 -4.2868314 -4.2857552 -4.288465 -4.29855 -4.3068833 -4.3036542 -4.3042727 -4.3170323 -4.3316894][-4.2749243 -4.25294 -4.2350788 -4.2270026 -4.2303677 -4.2349663 -4.2357235 -4.2296648 -4.2344313 -4.2497773 -4.2616639 -4.252872 -4.2521229 -4.2752175 -4.3032832][-4.2486424 -4.2156291 -4.1890407 -4.174964 -4.1751394 -4.1748228 -4.1695795 -4.1542311 -4.1647778 -4.1963425 -4.2161512 -4.2053995 -4.2032924 -4.2341037 -4.2720728][-4.2207222 -4.1714506 -4.129775 -4.1089187 -4.106019 -4.0962105 -4.0726061 -4.0413337 -4.0663137 -4.1253562 -4.1590028 -4.1497774 -4.1517386 -4.1931157 -4.2378769][-4.2019506 -4.1393881 -4.0847883 -4.0494685 -4.0299664 -3.9933388 -3.9251413 -3.8676484 -3.9252532 -4.026804 -4.0818305 -4.0825548 -4.094604 -4.1488028 -4.2000756][-4.1966939 -4.1303911 -4.0704603 -4.0197992 -3.9766979 -3.9089851 -3.7964008 -3.7128518 -3.8036425 -3.9364007 -4.0143766 -4.0347986 -4.0603504 -4.1253524 -4.1809816][-4.1996756 -4.1347933 -4.079277 -4.0254216 -3.9736044 -3.9022179 -3.7861197 -3.7067308 -3.7937233 -3.9062135 -3.9790332 -4.01254 -4.0498872 -4.1214814 -4.1790905][-4.21122 -4.1538696 -4.1085663 -4.0639081 -4.0206685 -3.971622 -3.8924515 -3.8429735 -3.9035566 -3.9680967 -4.00845 -4.033484 -4.06705 -4.1300778 -4.18309][-4.2308469 -4.1873722 -4.15373 -4.1231933 -4.0972748 -4.0747304 -4.0362091 -4.0139508 -4.050612 -4.079916 -4.0951109 -4.1030173 -4.1194139 -4.160346 -4.198401][-4.2576337 -4.2291307 -4.2076449 -4.1906729 -4.1801434 -4.1768436 -4.1665692 -4.1642 -4.1858478 -4.1971803 -4.2002692 -4.1954155 -4.1963129 -4.2177849 -4.2396684][-4.2838049 -4.2666011 -4.2546649 -4.2472782 -4.2471504 -4.2547297 -4.25809 -4.2657104 -4.2818813 -4.2895074 -4.2907314 -4.2829022 -4.2759485 -4.2816849 -4.2895746][-4.3007793 -4.288168 -4.2814665 -4.28065 -4.2868733 -4.2985735 -4.3071065 -4.3174953 -4.3297887 -4.3354745 -4.3359003 -4.3295345 -4.3226042 -4.3234515 -4.3251843][-4.3168397 -4.305614 -4.3009562 -4.3033628 -4.3126321 -4.3247929 -4.3340678 -4.3432059 -4.3522778 -4.3568583 -4.3566422 -4.3516335 -4.3476152 -4.3470631 -4.3458595]]...]
INFO - root - 2017-12-05 13:32:02.099969: step 11010, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 78h:55m:52s remains)
INFO - root - 2017-12-05 13:32:11.135306: step 11020, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 81h:46m:13s remains)
INFO - root - 2017-12-05 13:32:20.458318: step 11030, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.972 sec/batch; 86h:48m:45s remains)
INFO - root - 2017-12-05 13:32:29.659106: step 11040, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 81h:26m:00s remains)
INFO - root - 2017-12-05 13:32:38.813286: step 11050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 78h:03m:14s remains)
INFO - root - 2017-12-05 13:32:47.912830: step 11060, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 82h:08m:32s remains)
INFO - root - 2017-12-05 13:32:56.733691: step 11070, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 79h:48m:00s remains)
INFO - root - 2017-12-05 13:33:05.728026: step 11080, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 76h:00m:18s remains)
INFO - root - 2017-12-05 13:33:14.814076: step 11090, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.937 sec/batch; 83h:40m:52s remains)
INFO - root - 2017-12-05 13:33:23.973866: step 11100, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 82h:46m:54s remains)
2017-12-05 13:33:24.773289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1731496 -4.1899009 -4.1869965 -4.1871367 -4.2056336 -4.2297444 -4.2473793 -4.258296 -4.2620883 -4.2676754 -4.2654581 -4.2503662 -4.2281337 -4.2128382 -4.2062178][-4.1604662 -4.1852016 -4.1923618 -4.1938281 -4.2073259 -4.2282367 -4.2461491 -4.26102 -4.2684374 -4.2724743 -4.2698269 -4.2554431 -4.231339 -4.2129855 -4.2069473][-4.1583982 -4.1886568 -4.2034173 -4.20433 -4.208672 -4.2182765 -4.2325549 -4.2482672 -4.2574234 -4.2611012 -4.2592411 -4.2487035 -4.2269635 -4.2082992 -4.2049732][-4.1700211 -4.1985679 -4.2106819 -4.2073741 -4.1989055 -4.1949444 -4.2038546 -4.2207408 -4.2308331 -4.2322969 -4.2313647 -4.22715 -4.2118659 -4.1969447 -4.1967688][-4.1899152 -4.2154274 -4.2209034 -4.2100415 -4.1863642 -4.1661987 -4.16915 -4.190443 -4.2056584 -4.2063231 -4.2038121 -4.2009544 -4.1904621 -4.1775274 -4.1789508][-4.2010174 -4.2233906 -4.2226124 -4.20354 -4.1674943 -4.131773 -4.1286783 -4.1590395 -4.1877704 -4.1917362 -4.1853695 -4.1798735 -4.1682634 -4.1574578 -4.1650658][-4.197093 -4.21533 -4.2103419 -4.1874695 -4.1454973 -4.1021318 -4.0936618 -4.1312361 -4.1731586 -4.18069 -4.170229 -4.1632357 -4.1532326 -4.1431522 -4.1527133][-4.1926003 -4.2064977 -4.1978016 -4.17739 -4.1400795 -4.0984297 -4.08223 -4.1145124 -4.1582785 -4.1667972 -4.1575108 -4.1533771 -4.1476178 -4.1371231 -4.1450739][-4.1865697 -4.193398 -4.1826358 -4.168766 -4.1445312 -4.1179147 -4.1013212 -4.1215248 -4.1546793 -4.1599298 -4.1516166 -4.1531391 -4.1567354 -4.1538591 -4.1633954][-4.175261 -4.1770716 -4.1678452 -4.1610842 -4.1508274 -4.1446686 -4.1382918 -4.1507206 -4.16733 -4.1641097 -4.1548181 -4.1608281 -4.1746459 -4.1797442 -4.1905308][-4.165813 -4.1634841 -4.1570683 -4.1558771 -4.1562953 -4.1613679 -4.163002 -4.1689215 -4.1682577 -4.1533685 -4.1410294 -4.151402 -4.1715527 -4.18072 -4.1910448][-4.1697779 -4.1584477 -4.1475325 -4.1459012 -4.1501813 -4.1586938 -4.1643715 -4.16636 -4.1544881 -4.1324987 -4.1213026 -4.135035 -4.1604986 -4.1707578 -4.1841335][-4.1820965 -4.1588235 -4.1326256 -4.1204834 -4.120564 -4.1266489 -4.1376319 -4.146153 -4.1388316 -4.1226544 -4.1192327 -4.1360164 -4.1590171 -4.1676235 -4.1801138][-4.1969085 -4.1681943 -4.1290379 -4.1043367 -4.0941315 -4.0931516 -4.1081853 -4.1295767 -4.1369758 -4.13168 -4.1330404 -4.1474338 -4.1626596 -4.166666 -4.1765089][-4.2010341 -4.1768289 -4.1400809 -4.1158867 -4.0972238 -4.0842905 -4.0949593 -4.1236711 -4.1441264 -4.1505289 -4.1542034 -4.1629653 -4.1691513 -4.168056 -4.1741395]]...]
INFO - root - 2017-12-05 13:33:33.836511: step 11110, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.892 sec/batch; 79h:38m:19s remains)
INFO - root - 2017-12-05 13:33:42.984027: step 11120, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 83h:03m:46s remains)
INFO - root - 2017-12-05 13:33:52.229571: step 11130, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 82h:01m:15s remains)
INFO - root - 2017-12-05 13:34:01.173468: step 11140, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 80h:49m:06s remains)
INFO - root - 2017-12-05 13:34:10.297637: step 11150, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 82h:48m:19s remains)
INFO - root - 2017-12-05 13:34:19.294777: step 11160, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:53m:08s remains)
INFO - root - 2017-12-05 13:34:28.294253: step 11170, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 81h:53m:12s remains)
INFO - root - 2017-12-05 13:34:37.417638: step 11180, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 81h:35m:19s remains)
INFO - root - 2017-12-05 13:34:46.500871: step 11190, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 82h:16m:04s remains)
INFO - root - 2017-12-05 13:34:55.581402: step 11200, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 81h:49m:36s remains)
2017-12-05 13:34:56.363654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0415335 -4.0498924 -4.0723338 -4.0984073 -4.1118126 -4.116159 -4.1200509 -4.1267643 -4.1324706 -4.1396732 -4.1508579 -4.1603851 -4.1604528 -4.1491585 -4.1431947][-4.0394545 -4.0586886 -4.0856194 -4.1164026 -4.1308603 -4.133348 -4.1337595 -4.1340833 -4.1316266 -4.126071 -4.1327195 -4.1457934 -4.1445589 -4.1317306 -4.1215243][-4.0691385 -4.083436 -4.1056056 -4.1324282 -4.1391068 -4.1347013 -4.1346192 -4.1350775 -4.1257391 -4.1070333 -4.1060343 -4.1160049 -4.1146436 -4.1066694 -4.0930772][-4.09244 -4.1048293 -4.1256123 -4.1466632 -4.147306 -4.138545 -4.1362004 -4.1271634 -4.106061 -4.0832968 -4.0775814 -4.0883989 -4.094625 -4.09265 -4.0814695][-4.1137738 -4.126421 -4.1414714 -4.1525908 -4.15002 -4.1355205 -4.1210489 -4.0995307 -4.0769005 -4.0608382 -4.0612931 -4.0792894 -4.0985441 -4.1093378 -4.11306][-4.12581 -4.1306744 -4.1325293 -4.1352234 -4.1292753 -4.1091094 -4.0884295 -4.0747457 -4.0633574 -4.0585728 -4.0681124 -4.0914059 -4.1199613 -4.1358404 -4.1450582][-4.1156373 -4.1065626 -4.0985069 -4.1004105 -4.0921469 -4.0707073 -4.0492911 -4.047863 -4.0526409 -4.0570407 -4.0720735 -4.0973525 -4.1243572 -4.1402621 -4.1536331][-4.0913281 -4.0758963 -4.059495 -4.0510497 -4.0344524 -4.0115981 -3.9867458 -3.9959812 -4.0147781 -4.0319443 -4.05774 -4.0893559 -4.1175036 -4.1356926 -4.15328][-4.0702834 -4.0413766 -3.9931517 -3.9529963 -3.9431684 -3.93696 -3.9228377 -3.9508703 -3.993454 -4.0303721 -4.0617771 -4.0950904 -4.1194882 -4.1376014 -4.1580539][-4.0385637 -3.9990318 -3.9325602 -3.8877912 -3.9104168 -3.9438646 -3.9507723 -3.981143 -4.0225649 -4.0551305 -4.0760427 -4.1013389 -4.1152611 -4.1280804 -4.15164][-4.0097132 -3.9947739 -3.9667876 -3.9534519 -3.984453 -4.0198803 -4.0238404 -4.0356946 -4.0634904 -4.0873418 -4.0992312 -4.1161938 -4.1215715 -4.125998 -4.1450815][-4.0273085 -4.0335803 -4.0364165 -4.0356207 -4.0509772 -4.0705361 -4.0724192 -4.07172 -4.0891132 -4.1109614 -4.1259623 -4.1436124 -4.1481929 -4.1446028 -4.155292][-4.0446248 -4.0601387 -4.0751076 -4.0740275 -4.0736322 -4.0822144 -4.0798006 -4.0783148 -4.0989094 -4.1317759 -4.1604233 -4.1820993 -4.1830611 -4.1715612 -4.17247][-4.0496378 -4.0685492 -4.0904851 -4.0909643 -4.0861921 -4.08717 -4.0821652 -4.0873666 -4.1144824 -4.1500773 -4.1823139 -4.2051373 -4.2042408 -4.1897082 -4.1828718][-4.0406427 -4.0618353 -4.0875931 -4.093369 -4.0882497 -4.0916066 -4.0922008 -4.1032524 -4.1276083 -4.1577921 -4.1919036 -4.21644 -4.2141528 -4.1975346 -4.1891909]]...]
INFO - root - 2017-12-05 13:35:05.333303: step 11210, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 80h:50m:38s remains)
INFO - root - 2017-12-05 13:35:14.521559: step 11220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 83h:50m:57s remains)
INFO - root - 2017-12-05 13:35:23.630547: step 11230, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 79h:48m:28s remains)
INFO - root - 2017-12-05 13:35:32.747761: step 11240, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 80h:55m:02s remains)
INFO - root - 2017-12-05 13:35:41.830313: step 11250, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 77h:46m:32s remains)
INFO - root - 2017-12-05 13:35:50.919199: step 11260, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 79h:10m:29s remains)
INFO - root - 2017-12-05 13:35:59.900584: step 11270, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 82h:22m:58s remains)
INFO - root - 2017-12-05 13:36:09.026883: step 11280, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 80h:54m:39s remains)
INFO - root - 2017-12-05 13:36:18.135002: step 11290, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 79h:36m:07s remains)
INFO - root - 2017-12-05 13:36:27.334932: step 11300, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 84h:42m:05s remains)
2017-12-05 13:36:28.116539: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1847534 -4.1909294 -4.17436 -4.1544409 -4.1451917 -4.1528139 -4.1670561 -4.1835294 -4.1997895 -4.2043238 -4.1902566 -4.1669641 -4.157443 -4.1678562 -4.1945734][-4.172225 -4.181705 -4.1712255 -4.1467829 -4.124578 -4.1279769 -4.1490393 -4.1768413 -4.2046862 -4.2112193 -4.1927824 -4.1687512 -4.1648622 -4.1809545 -4.2068486][-4.1597767 -4.1754913 -4.1697359 -4.1392865 -4.1057887 -4.1041036 -4.126781 -4.1585464 -4.1915088 -4.1957459 -4.1688128 -4.1459432 -4.1512108 -4.1785541 -4.206985][-4.1459894 -4.1669116 -4.1688733 -4.1413431 -4.0979214 -4.0815907 -4.0943031 -4.1296606 -4.1727428 -4.1792903 -4.1480174 -4.1263127 -4.13958 -4.1738353 -4.202601][-4.1341114 -4.1621804 -4.1713881 -4.1498733 -4.1016135 -4.0636396 -4.0602589 -4.1019192 -4.1617694 -4.1760559 -4.1419325 -4.1221433 -4.1396418 -4.1719451 -4.1953616][-4.1334286 -4.1620789 -4.1742573 -4.1531587 -4.0948281 -4.0364227 -4.0204506 -4.0739794 -4.1539354 -4.1791439 -4.1448517 -4.1234784 -4.1405497 -4.1666307 -4.185329][-4.1502066 -4.1694045 -4.172297 -4.1403055 -4.0592847 -3.9691696 -3.9403582 -4.0157542 -4.1218204 -4.165875 -4.1447563 -4.1312704 -4.1474323 -4.1638017 -4.1781893][-4.1669388 -4.1732888 -4.1637945 -4.117403 -4.0142241 -3.8884258 -3.8392901 -3.9396534 -4.0749464 -4.14137 -4.1404076 -4.1420422 -4.15849 -4.1676736 -4.1790237][-4.1724911 -4.174768 -4.1647 -4.119379 -4.0222197 -3.8976226 -3.84234 -3.9355464 -4.0613775 -4.1232748 -4.1326084 -4.14356 -4.1623697 -4.1704626 -4.1807971][-4.1772332 -4.1832709 -4.1833076 -4.153554 -4.0872893 -4.0010152 -3.9616814 -4.0125375 -4.0890346 -4.1251245 -4.1327014 -4.1423936 -4.160604 -4.1708012 -4.1824107][-4.174427 -4.1824269 -4.192481 -4.1855526 -4.1504011 -4.0953646 -4.06294 -4.08116 -4.1183062 -4.1361742 -4.1422305 -4.1508174 -4.167336 -4.1793423 -4.190208][-4.1584234 -4.1626129 -4.175426 -4.1821361 -4.169261 -4.1362858 -4.1076832 -4.1099143 -4.1297612 -4.143786 -4.1527915 -4.15979 -4.17062 -4.1798792 -4.1843481][-4.13937 -4.1387935 -4.1492324 -4.15889 -4.1555667 -4.1366191 -4.1134443 -4.1114011 -4.1282864 -4.144649 -4.1546254 -4.1575952 -4.159452 -4.1592774 -4.155983][-4.1347747 -4.1300521 -4.1343145 -4.1424265 -4.14099 -4.1280742 -4.111824 -4.1140556 -4.1342692 -4.15393 -4.1639037 -4.1624093 -4.1571317 -4.1495948 -4.1429229][-4.16124 -4.1597171 -4.1576881 -4.1562462 -4.1504989 -4.1371684 -4.1232381 -4.1295552 -4.1541491 -4.1760454 -4.1860566 -4.1822009 -4.1723523 -4.1611209 -4.1502457]]...]
INFO - root - 2017-12-05 13:36:37.207583: step 11310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 82h:47m:11s remains)
INFO - root - 2017-12-05 13:36:46.336532: step 11320, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 80h:15m:21s remains)
INFO - root - 2017-12-05 13:36:55.333972: step 11330, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 81h:51m:31s remains)
INFO - root - 2017-12-05 13:37:04.485169: step 11340, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 81h:44m:21s remains)
INFO - root - 2017-12-05 13:37:13.565772: step 11350, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 79h:14m:07s remains)
INFO - root - 2017-12-05 13:37:22.700736: step 11360, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 80h:45m:13s remains)
INFO - root - 2017-12-05 13:37:31.824519: step 11370, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 80h:55m:23s remains)
INFO - root - 2017-12-05 13:37:40.920172: step 11380, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 82h:05m:56s remains)
INFO - root - 2017-12-05 13:37:50.019335: step 11390, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 82h:18m:16s remains)
INFO - root - 2017-12-05 13:37:59.048616: step 11400, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 80h:27m:45s remains)
2017-12-05 13:37:59.873367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2849441 -4.2786508 -4.2793264 -4.2811189 -4.284368 -4.2939177 -4.3017855 -4.2995992 -4.2897873 -4.2832994 -4.2829056 -4.2871814 -4.3007889 -4.31842 -4.3326797][-4.2471495 -4.2355733 -4.2329626 -4.2322679 -4.2353477 -4.2493343 -4.2628617 -4.2640147 -4.2538781 -4.2469969 -4.2487669 -4.257514 -4.274231 -4.2987118 -4.3215103][-4.2080493 -4.1904707 -4.1811671 -4.1766286 -4.1757827 -4.1903973 -4.21054 -4.2164478 -4.2076421 -4.2023525 -4.207263 -4.2197661 -4.2404871 -4.2704339 -4.3026905][-4.1764746 -4.15598 -4.1398787 -4.1247063 -4.1127305 -4.1176825 -4.1377797 -4.144701 -4.1396537 -4.1474848 -4.1665277 -4.1906857 -4.2196603 -4.2491746 -4.2824955][-4.1525221 -4.1303239 -4.1094642 -4.08154 -4.0559855 -4.0421476 -4.0453529 -4.0386457 -4.0352736 -4.07097 -4.1204162 -4.1653037 -4.199265 -4.2225194 -4.253365][-4.1440711 -4.1192479 -4.0914173 -4.0492887 -4.0049143 -3.9635749 -3.9357526 -3.8969445 -3.8855147 -3.9564962 -4.0481658 -4.1192088 -4.1612768 -4.1861277 -4.2239037][-4.1453819 -4.1175618 -4.0865736 -4.0437884 -3.9879539 -3.9141715 -3.832845 -3.7291842 -3.6909802 -3.8098371 -3.9566588 -4.0648432 -4.1275439 -4.1669617 -4.2189245][-4.1331763 -4.1025662 -4.0795441 -4.0514965 -4.0036526 -3.9147863 -3.790976 -3.6304152 -3.5669138 -3.7207236 -3.9077392 -4.0435972 -4.1227202 -4.1751766 -4.2341094][-4.1172681 -4.0851083 -4.076086 -4.0715914 -4.04738 -3.9803843 -3.8757205 -3.738312 -3.6813419 -3.7957952 -3.9517164 -4.0753889 -4.1516757 -4.2032514 -4.2566128][-4.1090097 -4.0785909 -4.0838737 -4.1023717 -4.1071987 -4.077271 -4.0102577 -3.9155304 -3.8710005 -3.9390135 -4.0389605 -4.1301723 -4.19051 -4.2300816 -4.2721939][-4.1142793 -4.0846677 -4.093811 -4.1216717 -4.1491451 -4.149291 -4.1080909 -4.0478687 -4.0203986 -4.0590167 -4.1208978 -4.1860518 -4.230206 -4.2588882 -4.2869825][-4.1277432 -4.1010218 -4.1089654 -4.1357517 -4.1763659 -4.1985073 -4.1810184 -4.1473451 -4.1328859 -4.1518955 -4.1909385 -4.2359338 -4.2659688 -4.28856 -4.3084922][-4.1705408 -4.1538453 -4.1627612 -4.181777 -4.2182508 -4.2428083 -4.240171 -4.2277727 -4.2215281 -4.2296386 -4.2530389 -4.28111 -4.2988276 -4.3148727 -4.3287334][-4.244904 -4.2351441 -4.2408419 -4.2519875 -4.2772856 -4.2949758 -4.2956867 -4.2926946 -4.2890916 -4.2917585 -4.3051434 -4.3206863 -4.3290219 -4.3378525 -4.34643][-4.3058534 -4.3001132 -4.3028893 -4.3094244 -4.3229456 -4.3316 -4.3323326 -4.3319921 -4.3301768 -4.3302007 -4.3379121 -4.3466306 -4.3496547 -4.3534212 -4.3571911]]...]
INFO - root - 2017-12-05 13:38:08.914046: step 11410, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 80h:33m:44s remains)
INFO - root - 2017-12-05 13:38:17.992998: step 11420, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 82h:30m:18s remains)
INFO - root - 2017-12-05 13:38:27.077289: step 11430, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:13m:32s remains)
INFO - root - 2017-12-05 13:38:36.025030: step 11440, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:55m:15s remains)
INFO - root - 2017-12-05 13:38:45.073406: step 11450, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.932 sec/batch; 83h:05m:37s remains)
INFO - root - 2017-12-05 13:38:54.234311: step 11460, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 82h:58m:51s remains)
INFO - root - 2017-12-05 13:39:03.369392: step 11470, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:49m:57s remains)
INFO - root - 2017-12-05 13:39:12.369347: step 11480, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 80h:20m:48s remains)
INFO - root - 2017-12-05 13:39:21.387650: step 11490, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 84h:20m:22s remains)
INFO - root - 2017-12-05 13:39:30.417676: step 11500, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 83h:06m:29s remains)
2017-12-05 13:39:31.211760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3001819 -4.2976217 -4.2942309 -4.2889977 -4.2807631 -4.2739925 -4.273675 -4.277009 -4.28603 -4.2903047 -4.2879944 -4.283587 -4.2850404 -4.289886 -4.293581][-4.2917943 -4.2871952 -4.2821107 -4.2768645 -4.2691517 -4.2602596 -4.2560945 -4.2569361 -4.2675467 -4.2760286 -4.281939 -4.2899094 -4.300859 -4.3044295 -4.2989225][-4.2838621 -4.2764678 -4.2702355 -4.264339 -4.2538857 -4.238687 -4.2264652 -4.2230139 -4.2355242 -4.2494335 -4.2684617 -4.2919359 -4.3123837 -4.3154545 -4.3017087][-4.2755208 -4.2662549 -4.2605071 -4.2551565 -4.2413597 -4.2134862 -4.1884403 -4.1787443 -4.1942816 -4.2184625 -4.2554088 -4.2949719 -4.3214903 -4.3238049 -4.3057632][-4.2654071 -4.2542119 -4.2464991 -4.2397246 -4.2183266 -4.1751661 -4.1306472 -4.1122952 -4.137784 -4.1815085 -4.2371011 -4.2899547 -4.3203316 -4.3229504 -4.3040094][-4.2531033 -4.235085 -4.2162037 -4.1993065 -4.1629753 -4.1001406 -4.0310411 -3.9933124 -4.037714 -4.1174078 -4.1979504 -4.2633109 -4.3016438 -4.3091416 -4.2916579][-4.2346635 -4.2055492 -4.1695743 -4.1346788 -4.0779643 -3.9919434 -3.8924894 -3.8259623 -3.8935077 -4.0212049 -4.1305475 -4.2082033 -4.2593694 -4.2800961 -4.2707157][-4.2120738 -4.17219 -4.1215649 -4.072072 -4.0075941 -3.9195285 -3.817524 -3.7499094 -3.8319883 -3.9727838 -4.0849724 -4.1616979 -4.2189736 -4.2539477 -4.2593536][-4.205792 -4.163209 -4.1102428 -4.0607481 -4.0075417 -3.9436309 -3.8809483 -3.8538043 -3.9159546 -4.0108094 -4.0936618 -4.1566749 -4.2101159 -4.2505336 -4.2685413][-4.2212853 -4.1799846 -4.1304936 -4.0860934 -4.0495715 -4.0126624 -3.9884546 -3.988569 -4.0294728 -4.0821071 -4.1331062 -4.1817813 -4.229465 -4.2698369 -4.2912097][-4.2481184 -4.2099924 -4.1634221 -4.1220136 -4.0975146 -4.0798545 -4.0762415 -4.0898862 -4.1205363 -4.1488361 -4.1783323 -4.2166457 -4.2590752 -4.2960987 -4.3148952][-4.268836 -4.2367439 -4.1985693 -4.1648254 -4.1516471 -4.1491976 -4.1563039 -4.175478 -4.2005367 -4.2174106 -4.2332678 -4.2596707 -4.2917585 -4.31927 -4.331924][-4.2838583 -4.2611885 -4.236948 -4.2182903 -4.2182422 -4.2275252 -4.2411451 -4.259594 -4.2762394 -4.2846384 -4.28996 -4.3034606 -4.3204651 -4.3336287 -4.3382726][-4.2980485 -4.287209 -4.2773685 -4.2698231 -4.2732182 -4.2827077 -4.2943897 -4.3073769 -4.316535 -4.3198318 -4.3206797 -4.3248763 -4.331264 -4.3361897 -4.33604][-4.3095493 -4.3063173 -4.3043165 -4.302175 -4.3051171 -4.3098812 -4.3161173 -4.3229837 -4.3265162 -4.3270698 -4.3262596 -4.3276896 -4.3313847 -4.3342314 -4.3338914]]...]
INFO - root - 2017-12-05 13:39:40.304626: step 11510, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 82h:37m:10s remains)
INFO - root - 2017-12-05 13:39:49.375895: step 11520, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 79h:05m:17s remains)
INFO - root - 2017-12-05 13:39:58.452780: step 11530, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 83h:31m:58s remains)
INFO - root - 2017-12-05 13:40:07.496670: step 11540, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 82h:02m:21s remains)
INFO - root - 2017-12-05 13:40:16.649605: step 11550, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 81h:38m:57s remains)
INFO - root - 2017-12-05 13:40:25.690532: step 11560, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 81h:06m:08s remains)
INFO - root - 2017-12-05 13:40:34.869580: step 11570, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.930 sec/batch; 82h:56m:50s remains)
INFO - root - 2017-12-05 13:40:43.965667: step 11580, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 81h:28m:56s remains)
INFO - root - 2017-12-05 13:40:52.992683: step 11590, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:55m:29s remains)
INFO - root - 2017-12-05 13:41:01.972552: step 11600, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 82h:14m:41s remains)
2017-12-05 13:41:02.730993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30559 -4.3133211 -4.322855 -4.3297262 -4.3342409 -4.3360877 -4.3362927 -4.3382559 -4.3389878 -4.3384433 -4.3383484 -4.3393722 -4.3402529 -4.3428316 -4.347497][-4.2717657 -4.2827034 -4.2948279 -4.2998028 -4.3052244 -4.3086853 -4.3111515 -4.3140182 -4.3141623 -4.31328 -4.3152986 -4.3181176 -4.3215218 -4.328608 -4.3352933][-4.2275362 -4.2414861 -4.2563281 -4.26119 -4.2662868 -4.2700491 -4.2729077 -4.2767353 -4.2787848 -4.2798119 -4.2863455 -4.2927508 -4.3002143 -4.3127971 -4.3225102][-4.1621146 -4.1770759 -4.1987391 -4.2083869 -4.2133694 -4.2164474 -4.2161918 -4.21943 -4.2270327 -4.2351885 -4.250875 -4.2638049 -4.2742748 -4.2904139 -4.3040533][-4.1024957 -4.112772 -4.1329927 -4.1435843 -4.1461077 -4.144433 -4.1411295 -4.14815 -4.168746 -4.1916871 -4.2202988 -4.2390094 -4.2532969 -4.27315 -4.289257][-4.0527844 -4.0512328 -4.0605888 -4.0617213 -4.0560818 -4.0454264 -4.036839 -4.0518785 -4.0879416 -4.1313415 -4.1742191 -4.2040191 -4.22697 -4.2503834 -4.2689357][-4.0259356 -4.0121293 -4.0041332 -3.9899819 -3.9725826 -3.9454067 -3.9227591 -3.941009 -3.9908624 -4.0516381 -4.1132646 -4.1621928 -4.1964469 -4.2250752 -4.25002][-4.0402031 -4.0215611 -4.0030985 -3.9759288 -3.9422483 -3.8914587 -3.8481097 -3.8663597 -3.9235835 -3.9938302 -4.0685453 -4.1285672 -4.1706815 -4.20668 -4.2379646][-4.0687509 -4.0562768 -4.039372 -4.0118904 -3.9743445 -3.9178424 -3.8703511 -3.8844175 -3.9324191 -3.9938967 -4.0644565 -4.1226783 -4.1668167 -4.2062845 -4.2361884][-4.08378 -4.0823741 -4.0756612 -4.0546069 -4.023241 -3.9741638 -3.9347086 -3.9392867 -3.9663882 -4.0103559 -4.0718222 -4.1280441 -4.1737156 -4.2111874 -4.2378454][-4.0931282 -4.0974107 -4.0965805 -4.0809922 -4.0584841 -4.0195942 -3.9875357 -3.9829018 -3.9930763 -4.0218935 -4.0746841 -4.1300888 -4.1756959 -4.2103343 -4.2356582][-4.1231461 -4.1274943 -4.1254282 -4.1125965 -4.0952463 -4.0625467 -4.0343308 -4.0255442 -4.028357 -4.0498776 -4.0925093 -4.1392708 -4.1782088 -4.2111397 -4.2379909][-4.1816773 -4.1851954 -4.1823773 -4.1695886 -4.1538434 -4.1236596 -4.0960155 -4.0837979 -4.0831985 -4.1004534 -4.1315942 -4.1665888 -4.1982775 -4.2282662 -4.2553244][-4.2500815 -4.2528181 -4.2507076 -4.24043 -4.2281861 -4.2059393 -4.1836386 -4.17128 -4.1686425 -4.1788235 -4.1986237 -4.2210321 -4.243578 -4.2651787 -4.2864809][-4.2989817 -4.300736 -4.3003798 -4.2951884 -4.2881088 -4.2768512 -4.2648854 -4.2588449 -4.2563372 -4.2615566 -4.2709122 -4.2816439 -4.2945466 -4.3069906 -4.3186336]]...]
INFO - root - 2017-12-05 13:41:11.866744: step 11610, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 81h:23m:15s remains)
INFO - root - 2017-12-05 13:41:20.950848: step 11620, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 81h:40m:11s remains)
INFO - root - 2017-12-05 13:41:29.762649: step 11630, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 78h:55m:52s remains)
INFO - root - 2017-12-05 13:41:38.850851: step 11640, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 83h:44m:00s remains)
INFO - root - 2017-12-05 13:41:47.885365: step 11650, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.884 sec/batch; 78h:48m:03s remains)
INFO - root - 2017-12-05 13:41:56.983292: step 11660, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 79h:48m:13s remains)
INFO - root - 2017-12-05 13:42:06.058930: step 11670, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 78h:36m:36s remains)
INFO - root - 2017-12-05 13:42:15.215237: step 11680, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 83h:45m:31s remains)
INFO - root - 2017-12-05 13:42:24.447052: step 11690, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 80h:11m:31s remains)
INFO - root - 2017-12-05 13:42:33.422227: step 11700, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 83h:21m:02s remains)
2017-12-05 13:42:34.298674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2874255 -4.2777405 -4.2696118 -4.2573376 -4.2470026 -4.2405066 -4.2333412 -4.2257128 -4.22549 -4.2155252 -4.2052794 -4.2039986 -4.2104812 -4.2177191 -4.2233768][-4.2882419 -4.2779536 -4.2651944 -4.2499161 -4.2372866 -4.2270079 -4.2165318 -4.210937 -4.2182956 -4.2113757 -4.2001696 -4.1966481 -4.2020035 -4.2056756 -4.2063837][-4.2885342 -4.2712421 -4.2473993 -4.2272439 -4.2155871 -4.2055144 -4.1951809 -4.1955075 -4.2122269 -4.2118411 -4.2003336 -4.1934447 -4.197176 -4.1973629 -4.1932573][-4.2848263 -4.2584572 -4.2243795 -4.2004757 -4.1888018 -4.1782379 -4.1675606 -4.1693745 -4.1914134 -4.1958613 -4.1849852 -4.1776752 -4.1825123 -4.18359 -4.1794429][-4.2793865 -4.2506442 -4.2133436 -4.1852474 -4.1676345 -4.14724 -4.124742 -4.1208348 -4.1464906 -4.15893 -4.154377 -4.1518307 -4.1586151 -4.1646233 -4.168169][-4.2714581 -4.2410789 -4.2049747 -4.1729712 -4.1411171 -4.0996342 -4.0507112 -4.0328126 -4.0634866 -4.0927958 -4.1026268 -4.1146526 -4.1307831 -4.1464887 -4.1607547][-4.2623591 -4.227778 -4.18899 -4.150229 -4.1014977 -4.0324535 -3.9487236 -3.9133582 -3.9615893 -4.018259 -4.049304 -4.0802646 -4.106811 -4.1317186 -4.1563015][-4.2560005 -4.2169223 -4.1743855 -4.131958 -4.076633 -3.9994755 -3.904645 -3.8666878 -3.9280095 -3.9999855 -4.0418129 -4.07841 -4.1015844 -4.12576 -4.1554017][-4.2579784 -4.2173743 -4.1743188 -4.1357474 -4.0923586 -4.0413079 -3.9787455 -3.9537363 -3.9982815 -4.0515528 -4.0820246 -4.1068845 -4.1177964 -4.1309609 -4.1536155][-4.2666454 -4.2281618 -4.1892805 -4.1565518 -4.1283159 -4.1052485 -4.0767212 -4.0606027 -4.0798182 -4.1076288 -4.1229386 -4.1377997 -4.1389136 -4.138113 -4.1481705][-4.2780719 -4.2436123 -4.210217 -4.1833286 -4.164464 -4.1555724 -4.1447153 -4.130609 -4.1306505 -4.141664 -4.1482663 -4.1563177 -4.1551261 -4.1480274 -4.1508813][-4.2842431 -4.254775 -4.2289033 -4.2106929 -4.1999211 -4.1947231 -4.1870303 -4.170186 -4.1550617 -4.1533151 -4.1538229 -4.1607032 -4.1619387 -4.1568832 -4.1614857][-4.28345 -4.2590742 -4.2424054 -4.2332225 -4.2259417 -4.2177606 -4.2060332 -4.1841183 -4.1608238 -4.1519055 -4.15326 -4.1616993 -4.168303 -4.1707525 -4.178504][-4.2848616 -4.2651968 -4.2531219 -4.2466021 -4.2366362 -4.2226272 -4.2056828 -4.1835089 -4.1628213 -4.1580319 -4.1657729 -4.17648 -4.1864381 -4.19482 -4.1997538][-4.2910104 -4.2751021 -4.2638645 -4.2555127 -4.2436118 -4.2289038 -4.2134123 -4.1980543 -4.1845512 -4.1831775 -4.192328 -4.2011337 -4.2097673 -4.2185206 -4.2175021]]...]
INFO - root - 2017-12-05 13:42:43.328888: step 11710, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 83h:05m:04s remains)
INFO - root - 2017-12-05 13:42:52.399650: step 11720, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 75h:44m:40s remains)
INFO - root - 2017-12-05 13:43:01.445589: step 11730, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 80h:01m:33s remains)
INFO - root - 2017-12-05 13:43:10.396327: step 11740, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 79h:52m:18s remains)
INFO - root - 2017-12-05 13:43:19.490597: step 11750, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 80h:51m:28s remains)
INFO - root - 2017-12-05 13:43:28.621443: step 11760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 80h:47m:19s remains)
INFO - root - 2017-12-05 13:43:37.593278: step 11770, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 80h:09m:31s remains)
INFO - root - 2017-12-05 13:43:46.703021: step 11780, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 80h:40m:38s remains)
INFO - root - 2017-12-05 13:43:55.850726: step 11790, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 80h:08m:25s remains)
INFO - root - 2017-12-05 13:44:04.952542: step 11800, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 76h:26m:11s remains)
2017-12-05 13:44:05.731787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.194694 -4.2102895 -4.2096243 -4.2102461 -4.2171888 -4.2063313 -4.1908751 -4.1740346 -4.1676908 -4.1699138 -4.1683125 -4.1639996 -4.1739364 -4.2041178 -4.2328043][-4.1770034 -4.1854239 -4.1802955 -4.1774821 -4.1831636 -4.1826839 -4.1780915 -4.1708789 -4.164382 -4.1659865 -4.1718707 -4.1705527 -4.1836047 -4.2170105 -4.2451167][-4.1486478 -4.1516771 -4.1442714 -4.1380548 -4.1404443 -4.1465683 -4.152122 -4.1586223 -4.158783 -4.1630259 -4.1738167 -4.1776547 -4.1922607 -4.2215519 -4.2443557][-4.1241779 -4.1285849 -4.1233053 -4.1184287 -4.11889 -4.1278348 -4.1387806 -4.1531458 -4.1562595 -4.1605544 -4.1753945 -4.1867013 -4.1997633 -4.2187328 -4.2324758][-4.1010294 -4.1088142 -4.1115346 -4.1112309 -4.1155705 -4.1244111 -4.1338806 -4.1484275 -4.1492724 -4.1531549 -4.16617 -4.1829948 -4.1968503 -4.2132988 -4.2241287][-4.0870428 -4.0909734 -4.1004987 -4.1086526 -4.1164594 -4.1175213 -4.118331 -4.1272187 -4.1348748 -4.1454091 -4.15743 -4.1751628 -4.1929889 -4.2143579 -4.2275877][-4.0775142 -4.0797725 -4.0981064 -4.1164851 -4.1231232 -4.1088572 -4.0905032 -4.0878868 -4.1079259 -4.1336083 -4.1508613 -4.1704173 -4.1935272 -4.2190962 -4.2351561][-4.0492749 -4.050108 -4.0754457 -4.0993567 -4.0980282 -4.0630379 -4.0190239 -4.0047231 -4.0458984 -4.09749 -4.1292577 -4.1544781 -4.1863294 -4.2190948 -4.24058][-4.0244856 -4.0268135 -4.0489111 -4.0696225 -4.0587034 -4.0066185 -3.9460275 -3.928901 -3.9912155 -4.0733972 -4.1247883 -4.1562176 -4.1912694 -4.2234817 -4.2462997][-4.0542884 -4.0564156 -4.0683222 -4.0738544 -4.0565333 -4.0054274 -3.9543085 -3.9460418 -4.0107436 -4.096046 -4.1511703 -4.1794109 -4.20813 -4.2336893 -4.2557831][-4.1138763 -4.1143727 -4.1258068 -4.1251507 -4.10847 -4.0700574 -4.0343642 -4.0299611 -4.0744667 -4.1386218 -4.1834178 -4.2065053 -4.2298117 -4.2498269 -4.2664862][-4.1646314 -4.1659861 -4.1825461 -4.1845965 -4.1717229 -4.1462541 -4.11932 -4.1162677 -4.1433649 -4.1858196 -4.2179914 -4.2320981 -4.2501636 -4.2685962 -4.27799][-4.1961875 -4.2016406 -4.2219572 -4.2244358 -4.2150779 -4.19842 -4.1768813 -4.173439 -4.1886516 -4.2158933 -4.2396646 -4.2473497 -4.2620177 -4.2803288 -4.2823448][-4.2197547 -4.2287393 -4.2475305 -4.2459369 -4.2362075 -4.2255483 -4.2075424 -4.2011709 -4.2094522 -4.2304759 -4.2494736 -4.2527976 -4.2636852 -4.2816448 -4.2795053][-4.2432804 -4.2519436 -4.2631707 -4.257597 -4.24801 -4.2421908 -4.2263174 -4.2193875 -4.2269111 -4.2437835 -4.2567406 -4.2540441 -4.2589211 -4.2746649 -4.2744851]]...]
INFO - root - 2017-12-05 13:44:14.791805: step 11810, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 79h:21m:04s remains)
INFO - root - 2017-12-05 13:44:23.618685: step 11820, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 82h:53m:08s remains)
INFO - root - 2017-12-05 13:44:32.748301: step 11830, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:40m:42s remains)
INFO - root - 2017-12-05 13:44:41.808376: step 11840, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 78h:24m:41s remains)
INFO - root - 2017-12-05 13:44:50.679980: step 11850, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 78h:38m:04s remains)
INFO - root - 2017-12-05 13:44:59.864508: step 11860, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.946 sec/batch; 84h:16m:46s remains)
INFO - root - 2017-12-05 13:45:09.125721: step 11870, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 81h:02m:23s remains)
INFO - root - 2017-12-05 13:45:18.045585: step 11880, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 79h:13m:58s remains)
INFO - root - 2017-12-05 13:45:27.196869: step 11890, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 81h:00m:59s remains)
INFO - root - 2017-12-05 13:45:36.330983: step 11900, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.936 sec/batch; 83h:19m:07s remains)
2017-12-05 13:45:37.119920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.273169 -4.2587976 -4.2510195 -4.251781 -4.2525229 -4.2444668 -4.2264137 -4.1954265 -4.1657772 -4.1459317 -4.1423984 -4.1660681 -4.2144985 -4.2597375 -4.290596][-4.2764139 -4.2637796 -4.2530866 -4.2474704 -4.24503 -4.2353168 -4.2150578 -4.1856132 -4.158329 -4.1375489 -4.1335397 -4.1577263 -4.2115922 -4.2630281 -4.2955494][-4.2766209 -4.2651844 -4.2484288 -4.2338214 -4.2267618 -4.211338 -4.1855974 -4.1586819 -4.1363811 -4.1233106 -4.1270695 -4.1550589 -4.2110038 -4.264823 -4.2948079][-4.2637539 -4.24792 -4.2267566 -4.2110534 -4.2040262 -4.18485 -4.1577148 -4.1282077 -4.1086535 -4.1079135 -4.1227789 -4.1565633 -4.2131858 -4.2620616 -4.2853856][-4.2414465 -4.219636 -4.1973124 -4.1836185 -4.1773639 -4.1532068 -4.1199036 -4.0852804 -4.0711274 -4.0860233 -4.1153083 -4.1563511 -4.2100854 -4.2506266 -4.2664814][-4.2192168 -4.1932192 -4.1678486 -4.1465869 -4.1287727 -4.0891433 -4.0395761 -4.0013027 -4.0049663 -4.0509286 -4.0983634 -4.1496325 -4.2000608 -4.2337523 -4.245193][-4.2031336 -4.1753912 -4.1462526 -4.1109333 -4.0715666 -4.0087142 -3.9287546 -3.8768842 -3.9115863 -4.0020533 -4.0717897 -4.1361084 -4.1902232 -4.2228827 -4.2360711][-4.1963682 -4.1731 -4.1423841 -4.0998535 -4.0470791 -3.9674263 -3.8618753 -3.7924402 -3.8508546 -3.9698095 -4.0525446 -4.125164 -4.1825352 -4.2172308 -4.23636][-4.2025762 -4.1845555 -4.1544256 -4.1127462 -4.0610237 -3.9857349 -3.8798549 -3.8055592 -3.8705275 -3.9826932 -4.0546665 -4.124999 -4.1802063 -4.2156825 -4.2436652][-4.214427 -4.2024956 -4.1792064 -4.1465306 -4.1032605 -4.0471711 -3.9643791 -3.8919952 -3.9384344 -4.0191517 -4.0720305 -4.1327643 -4.1831751 -4.2205124 -4.2561426][-4.2306581 -4.2196264 -4.2033548 -4.181036 -4.1503353 -4.1176968 -4.0598636 -3.9969783 -4.023212 -4.0758672 -4.1151848 -4.1629138 -4.2050653 -4.2396593 -4.274374][-4.2506504 -4.2379293 -4.2222595 -4.20595 -4.1851649 -4.1662374 -4.129405 -4.0865607 -4.1072159 -4.1467752 -4.1782417 -4.2117162 -4.2411146 -4.2653432 -4.2902822][-4.2725697 -4.2549291 -4.2368164 -4.2227845 -4.2120543 -4.2003803 -4.1763496 -4.1502728 -4.1683731 -4.1989717 -4.2231531 -4.24628 -4.2669892 -4.2815857 -4.2983737][-4.2861915 -4.2648511 -4.2479072 -4.2385454 -4.2349634 -4.2293043 -4.2132406 -4.2002425 -4.2153583 -4.2359872 -4.2505717 -4.2664967 -4.2815366 -4.289793 -4.301662][-4.2968278 -4.2773266 -4.2657695 -4.260252 -4.2603617 -4.2575912 -4.2469821 -4.2417674 -4.2511663 -4.2585382 -4.2624245 -4.2708945 -4.2831283 -4.2901011 -4.3008919]]...]
INFO - root - 2017-12-05 13:45:45.960315: step 11910, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 81h:45m:29s remains)
INFO - root - 2017-12-05 13:45:54.896246: step 11920, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 79h:20m:29s remains)
INFO - root - 2017-12-05 13:46:03.989738: step 11930, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 80h:15m:14s remains)
INFO - root - 2017-12-05 13:46:13.136419: step 11940, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 81h:10m:41s remains)
INFO - root - 2017-12-05 13:46:22.074655: step 11950, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 78h:52m:55s remains)
INFO - root - 2017-12-05 13:46:31.214085: step 11960, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 81h:25m:50s remains)
INFO - root - 2017-12-05 13:46:40.338123: step 11970, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 81h:33m:47s remains)
INFO - root - 2017-12-05 13:46:49.382610: step 11980, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.889 sec/batch; 79h:11m:14s remains)
INFO - root - 2017-12-05 13:46:58.472551: step 11990, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 84h:29m:50s remains)
INFO - root - 2017-12-05 13:47:07.627288: step 12000, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 85h:36m:58s remains)
2017-12-05 13:47:08.493210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2402539 -4.2313938 -4.2252979 -4.2243857 -4.2293978 -4.2302341 -4.2268457 -4.2216206 -4.2275314 -4.237854 -4.2427273 -4.2426033 -4.2376027 -4.2328911 -4.2320986][-4.2102652 -4.2019949 -4.1974421 -4.19745 -4.2034574 -4.2007003 -4.1887479 -4.174612 -4.1789002 -4.1945357 -4.2079725 -4.2133508 -4.2067108 -4.1941462 -4.1929789][-4.1816554 -4.1758351 -4.1740589 -4.1746135 -4.1761746 -4.1679926 -4.1476707 -4.12408 -4.1260095 -4.1447062 -4.1608782 -4.1722465 -4.1683297 -4.1531706 -4.155654][-4.1605678 -4.1562824 -4.1564121 -4.1585541 -4.1551523 -4.1425014 -4.1179996 -4.091291 -4.092638 -4.1061392 -4.1141024 -4.1262646 -4.1300044 -4.1199918 -4.131186][-4.1597285 -4.1543584 -4.1514096 -4.1525164 -4.1450334 -4.128377 -4.1041079 -4.0823503 -4.0817866 -4.0805736 -4.0678115 -4.0793 -4.0989833 -4.1049223 -4.12447][-4.1677752 -4.1561036 -4.1417031 -4.1394963 -4.131094 -4.1150494 -4.0951672 -4.0806184 -4.0726995 -4.0489755 -4.0127244 -4.0257583 -4.0642915 -4.0935955 -4.1215048][-4.1760211 -4.1612897 -4.1401339 -4.1323957 -4.1235933 -4.1052451 -4.0839291 -4.0742493 -4.0559688 -4.0089869 -3.9530163 -3.9752884 -4.0390706 -4.0903592 -4.1265426][-4.1814313 -4.1716137 -4.1533155 -4.1443009 -4.1343021 -4.113718 -4.0881829 -4.0793428 -4.0541711 -3.9925935 -3.9358165 -3.9672616 -4.038898 -4.0948462 -4.1363163][-4.1835136 -4.1793666 -4.1672416 -4.1573672 -4.1491041 -4.1302581 -4.107018 -4.098958 -4.07358 -4.0189 -3.9868104 -4.0220423 -4.0747495 -4.1124306 -4.1497345][-4.177597 -4.1700058 -4.1552811 -4.1447949 -4.1474428 -4.1429114 -4.1338506 -4.1306429 -4.1109943 -4.0773206 -4.0663204 -4.0943136 -4.1180596 -4.1266813 -4.1548514][-4.17604 -4.1564684 -4.1314182 -4.1163826 -4.1287007 -4.1463671 -4.1554375 -4.1584687 -4.1467805 -4.1309977 -4.1295314 -4.1425238 -4.1415429 -4.1308084 -4.1474676][-4.1823435 -4.1555738 -4.1215272 -4.0980406 -4.1114893 -4.1434193 -4.1623521 -4.1725817 -4.1684475 -4.1616359 -4.1600065 -4.1556888 -4.1345096 -4.1199288 -4.1329021][-4.2081561 -4.1854153 -4.1476769 -4.1136336 -4.1188064 -4.1490374 -4.1641221 -4.17327 -4.1754913 -4.1729212 -4.1684365 -4.1535254 -4.1239729 -4.1154456 -4.1263618][-4.236547 -4.2220831 -4.1877494 -4.1522994 -4.1528349 -4.1767378 -4.1805143 -4.18156 -4.1851335 -4.1850114 -4.1827364 -4.1700869 -4.1459174 -4.1449695 -4.1521363][-4.2590604 -4.2482576 -4.2222495 -4.1947 -4.1914873 -4.2023883 -4.1950054 -4.193902 -4.2003584 -4.2060871 -4.2122507 -4.20936 -4.1923723 -4.1915021 -4.1941309]]...]
INFO - root - 2017-12-05 13:47:17.411171: step 12010, loss = 2.02, batch loss = 1.97 (8.7 examples/sec; 0.915 sec/batch; 81h:29m:15s remains)
INFO - root - 2017-12-05 13:47:26.510848: step 12020, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 79h:29m:48s remains)
INFO - root - 2017-12-05 13:47:35.508950: step 12030, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 78h:35m:41s remains)
INFO - root - 2017-12-05 13:47:44.541341: step 12040, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 76h:55m:45s remains)
INFO - root - 2017-12-05 13:47:53.567457: step 12050, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 78h:48m:32s remains)
INFO - root - 2017-12-05 13:48:02.416354: step 12060, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 78h:30m:41s remains)
INFO - root - 2017-12-05 13:48:11.446326: step 12070, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 80h:00m:10s remains)
INFO - root - 2017-12-05 13:48:20.346298: step 12080, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 81h:10m:24s remains)
INFO - root - 2017-12-05 13:48:29.422783: step 12090, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.909 sec/batch; 80h:54m:22s remains)
INFO - root - 2017-12-05 13:48:38.509276: step 12100, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 80h:08m:58s remains)
2017-12-05 13:48:39.303797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1798644 -4.1933193 -4.1892095 -4.17665 -4.1625562 -4.14895 -4.1554956 -4.1892462 -4.2309 -4.2512321 -4.2551546 -4.25671 -4.2523694 -4.2509747 -4.2571712][-4.1728821 -4.1879616 -4.1842232 -4.1715951 -4.1576467 -4.147109 -4.1577215 -4.1939707 -4.2382865 -4.2603717 -4.2656183 -4.2660336 -4.2594576 -4.25335 -4.2520142][-4.1760426 -4.1892991 -4.1891246 -4.1831708 -4.1684532 -4.1535063 -4.1618381 -4.1941175 -4.2348061 -4.2576466 -4.2655096 -4.2676606 -4.2644353 -4.2608309 -4.2583222][-4.1965384 -4.2091289 -4.210175 -4.2097831 -4.1936979 -4.16879 -4.1610494 -4.1770058 -4.2093534 -4.23679 -4.2512493 -4.2566175 -4.25778 -4.2587094 -4.2610278][-4.2206297 -4.2310209 -4.232801 -4.2299237 -4.2055507 -4.1649151 -4.1293406 -4.1190443 -4.1467171 -4.1908603 -4.2235441 -4.2393394 -4.2458186 -4.2471852 -4.2513814][-4.2405319 -4.2489557 -4.2502217 -4.2418756 -4.2043762 -4.1416073 -4.07142 -4.0227742 -4.0416422 -4.1136856 -4.1794491 -4.2140107 -4.2308393 -4.237751 -4.2432919][-4.2520733 -4.2604389 -4.2628784 -4.25441 -4.2138996 -4.1439772 -4.0514221 -3.9584429 -3.9417698 -4.0237751 -4.1174741 -4.1781092 -4.2124987 -4.2278647 -4.2354565][-4.2605562 -4.2688704 -4.2722774 -4.2672386 -4.2348561 -4.1786742 -4.0969729 -3.9983022 -3.9432807 -3.9823468 -4.0624728 -4.1314497 -4.182425 -4.2104239 -4.2236371][-4.2684464 -4.2747436 -4.2797632 -4.2786894 -4.2599063 -4.22347 -4.1670351 -4.0915952 -4.0355883 -4.0313611 -4.0627503 -4.1033087 -4.1503429 -4.1842017 -4.2044926][-4.2707596 -4.2761917 -4.282413 -4.2867002 -4.2825561 -4.2639885 -4.2289414 -4.1761723 -4.1319046 -4.1152811 -4.1199355 -4.1334739 -4.1576467 -4.1780596 -4.1938028][-4.2699151 -4.2768645 -4.2827497 -4.2886615 -4.2915473 -4.2850857 -4.2629085 -4.2255187 -4.1927071 -4.1789255 -4.1772838 -4.1837859 -4.195003 -4.2025013 -4.2063904][-4.2757697 -4.2868137 -4.2903962 -4.2906513 -4.2904606 -4.2873654 -4.2707696 -4.2397771 -4.2103434 -4.198482 -4.1993804 -4.2112732 -4.2257166 -4.2320004 -4.2306194][-4.2865758 -4.2991958 -4.2992516 -4.2919822 -4.2849512 -4.2783966 -4.2604766 -4.2283483 -4.1970892 -4.1831288 -4.1884661 -4.2100925 -4.23263 -4.2431741 -4.2440548][-4.29467 -4.30265 -4.2995977 -4.2887006 -4.2794333 -4.2707896 -4.2515974 -4.21612 -4.1805615 -4.1660981 -4.1758165 -4.201479 -4.2256942 -4.2360225 -4.239676][-4.2951741 -4.2959967 -4.2902203 -4.2779326 -4.2697744 -4.2660174 -4.2516861 -4.2196012 -4.1873164 -4.1763425 -4.1871934 -4.20824 -4.2250595 -4.2279067 -4.2278409]]...]
INFO - root - 2017-12-05 13:48:48.407385: step 12110, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 81h:22m:45s remains)
INFO - root - 2017-12-05 13:48:57.641171: step 12120, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 82h:15m:39s remains)
INFO - root - 2017-12-05 13:49:06.705004: step 12130, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.914 sec/batch; 81h:21m:53s remains)
INFO - root - 2017-12-05 13:49:15.913219: step 12140, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 82h:30m:00s remains)
INFO - root - 2017-12-05 13:49:24.994275: step 12150, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 82h:11m:05s remains)
INFO - root - 2017-12-05 13:49:34.121221: step 12160, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 83h:23m:22s remains)
INFO - root - 2017-12-05 13:49:43.365035: step 12170, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 82h:14m:58s remains)
INFO - root - 2017-12-05 13:49:52.482433: step 12180, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 79h:48m:51s remains)
INFO - root - 2017-12-05 13:50:01.444291: step 12190, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 77h:52m:37s remains)
INFO - root - 2017-12-05 13:50:10.346694: step 12200, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 83h:05m:55s remains)
2017-12-05 13:50:11.128517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2851887 -4.2920666 -4.2975483 -4.3011146 -4.3022923 -4.3040428 -4.3086233 -4.3140607 -4.3167124 -4.3162146 -4.3131986 -4.3098011 -4.3083773 -4.307858 -4.30612][-4.2727661 -4.2814274 -4.29001 -4.2962332 -4.2988248 -4.3013139 -4.3062463 -4.3122053 -4.3167491 -4.3180203 -4.3146782 -4.3093672 -4.3080425 -4.3096218 -4.3083229][-4.2475886 -4.2563138 -4.267693 -4.277185 -4.2805004 -4.2820673 -4.2843046 -4.2893653 -4.2985845 -4.3068967 -4.3086772 -4.3043966 -4.304348 -4.3078303 -4.3065729][-4.2094855 -4.2146573 -4.2269258 -4.2374983 -4.2389927 -4.2361431 -4.2336593 -4.2383947 -4.2552881 -4.2752204 -4.2887855 -4.2925272 -4.2991238 -4.3066 -4.3060131][-4.157167 -4.1544123 -4.1644616 -4.1747303 -4.1721191 -4.1607256 -4.1510282 -4.1553569 -4.1798253 -4.2141414 -4.2434196 -4.2615843 -4.2793145 -4.2917156 -4.2924161][-4.1017432 -4.088583 -4.0939884 -4.1016226 -4.0938663 -4.0726933 -4.0510163 -4.0456724 -4.0695038 -4.1194067 -4.1690211 -4.2070479 -4.2394085 -4.2585106 -4.2610092][-4.0566478 -4.0337338 -4.0319166 -4.0348086 -4.0196552 -3.985678 -3.9430344 -3.9111001 -3.9286289 -4.0012507 -4.0798535 -4.1420422 -4.1903276 -4.216867 -4.222609][-4.0288053 -3.9962311 -3.9822078 -3.974638 -3.9524875 -3.9090955 -3.8438392 -3.7762568 -3.781564 -3.8806181 -3.9913363 -4.0780048 -4.1371932 -4.1711378 -4.1853781][-4.0247016 -3.9822891 -3.9553318 -3.9376731 -3.9149697 -3.8790965 -3.8167591 -3.7417674 -3.7360706 -3.8316233 -3.9418874 -4.0314937 -4.0898895 -4.1244287 -4.1489048][-4.0447316 -3.9967206 -3.9645257 -3.9441838 -3.9299688 -3.9175847 -3.8877835 -3.8445981 -3.8350949 -3.8894057 -3.9614232 -4.0264354 -4.0705657 -4.1006675 -4.1287184][-4.0814052 -4.0361061 -4.0072255 -3.9939287 -3.9919932 -3.9972115 -3.992069 -3.9741321 -3.9659255 -3.9869056 -4.01941 -4.0544648 -4.0842953 -4.109375 -4.1357913][-4.1194458 -4.0812941 -4.05994 -4.0561614 -4.062645 -4.0745525 -4.0806127 -4.0752215 -4.0692878 -4.0739393 -4.085444 -4.1039262 -4.1249833 -4.1457353 -4.1664915][-4.153739 -4.1230068 -4.1071634 -4.1078777 -4.1167116 -4.1293797 -4.1383362 -4.1384091 -4.1349196 -4.1358418 -4.1426697 -4.1571026 -4.1747355 -4.1912904 -4.2062621][-4.1848216 -4.1615577 -4.15043 -4.1528172 -4.1609178 -4.1705351 -4.1771965 -4.1786804 -4.1783152 -4.1801376 -4.186759 -4.2001872 -4.2168517 -4.23115 -4.2425871][-4.2194757 -4.2049327 -4.1989241 -4.2018294 -4.2079573 -4.2138929 -4.217031 -4.217598 -4.2175059 -4.2192049 -4.22389 -4.2330852 -4.245934 -4.25814 -4.2676282]]...]
INFO - root - 2017-12-05 13:50:20.010229: step 12210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:10m:31s remains)
INFO - root - 2017-12-05 13:50:28.948719: step 12220, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 78h:39m:41s remains)
INFO - root - 2017-12-05 13:50:37.992479: step 12230, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 79h:14m:08s remains)
INFO - root - 2017-12-05 13:50:47.061594: step 12240, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 79h:48m:59s remains)
INFO - root - 2017-12-05 13:50:56.023486: step 12250, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 80h:08m:45s remains)
INFO - root - 2017-12-05 13:51:05.041458: step 12260, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 77h:09m:17s remains)
INFO - root - 2017-12-05 13:51:14.012622: step 12270, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 77h:35m:20s remains)
INFO - root - 2017-12-05 13:51:22.951365: step 12280, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 78h:41m:29s remains)
INFO - root - 2017-12-05 13:51:31.739769: step 12290, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 80h:21m:04s remains)
INFO - root - 2017-12-05 13:51:40.874820: step 12300, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 81h:39m:32s remains)
2017-12-05 13:51:41.625397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1979361 -4.2024064 -4.1990595 -4.1898928 -4.1851311 -4.1743083 -4.1611309 -4.1632285 -4.1834812 -4.2123432 -4.2359176 -4.2516079 -4.262176 -4.2699771 -4.2719588][-4.1824603 -4.1925392 -4.1954441 -4.1952548 -4.1946321 -4.1809654 -4.1622772 -4.1575093 -4.176744 -4.2063718 -4.2283683 -4.2404022 -4.2437644 -4.2458768 -4.2477255][-4.1709857 -4.1850357 -4.1931548 -4.1948862 -4.1907969 -4.1700859 -4.1455703 -4.1410818 -4.1625929 -4.1920567 -4.2122374 -4.2224531 -4.2221837 -4.2229066 -4.2258787][-4.155715 -4.1729584 -4.1823487 -4.1792536 -4.1655784 -4.1355715 -4.11036 -4.1147227 -4.1430511 -4.172338 -4.1911783 -4.2026596 -4.2037363 -4.2019844 -4.2019877][-4.1320891 -4.1563888 -4.1684833 -4.1603265 -4.1356091 -4.0929365 -4.0634546 -4.0741343 -4.108098 -4.1416097 -4.1629715 -4.1771517 -4.1818662 -4.1777267 -4.1708083][-4.1138959 -4.1447315 -4.1594105 -4.1459689 -4.1056752 -4.0446873 -3.9949007 -3.9995475 -4.0472713 -4.1020064 -4.1325903 -4.1432433 -4.1493731 -4.1444654 -4.1332121][-4.111043 -4.1453104 -4.1554923 -4.1342454 -4.08188 -4.0019546 -3.9204 -3.9113839 -3.9814868 -4.0633492 -4.0986838 -4.0970778 -4.1041055 -4.1074233 -4.100924][-4.1101551 -4.1410532 -4.145771 -4.1262569 -4.0744767 -3.9900434 -3.8910193 -3.8724689 -3.9604685 -4.0536551 -4.0872703 -4.0783663 -4.083189 -4.0947027 -4.0922251][-4.116385 -4.1379142 -4.1402922 -4.1338243 -4.1031036 -4.0412383 -3.9608183 -3.9467695 -4.0208354 -4.094533 -4.1189928 -4.1115937 -4.1084175 -4.1106577 -4.1048241][-4.1469326 -4.1565084 -4.1577554 -4.1674604 -4.1603813 -4.1267533 -4.07394 -4.062098 -4.1044812 -4.1426311 -4.1500235 -4.1418924 -4.1301856 -4.1249442 -4.1204281][-4.1911097 -4.1883454 -4.184411 -4.1973 -4.2009225 -4.1877589 -4.157383 -4.1512032 -4.1701169 -4.18139 -4.17229 -4.1539974 -4.1355696 -4.1329484 -4.1384521][-4.2269711 -4.2204847 -4.2074242 -4.2094364 -4.2101054 -4.2042904 -4.1916876 -4.19458 -4.20691 -4.2095861 -4.1915731 -4.1666493 -4.1502819 -4.1586137 -4.1699457][-4.2495179 -4.2439775 -4.2268157 -4.2192945 -4.2162285 -4.2121177 -4.2106791 -4.2168617 -4.2263627 -4.22588 -4.2033567 -4.1788688 -4.1723328 -4.186245 -4.1988816][-4.2555132 -4.2522082 -4.2402444 -4.2324786 -4.2279744 -4.221519 -4.2220182 -4.2256103 -4.22922 -4.2241549 -4.2001615 -4.1805058 -4.1834922 -4.1975284 -4.2120018][-4.2415218 -4.2394481 -4.236062 -4.2332559 -4.2297182 -4.2227778 -4.2239008 -4.2268343 -4.2283187 -4.2243075 -4.2043805 -4.1891761 -4.1937704 -4.2056489 -4.2207456]]...]
INFO - root - 2017-12-05 13:51:50.489104: step 12310, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 79h:39m:18s remains)
INFO - root - 2017-12-05 13:51:59.434015: step 12320, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 80h:52m:23s remains)
INFO - root - 2017-12-05 13:52:08.521133: step 12330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 81h:11m:55s remains)
INFO - root - 2017-12-05 13:52:17.539778: step 12340, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 82h:13m:26s remains)
INFO - root - 2017-12-05 13:52:26.551552: step 12350, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:32m:54s remains)
INFO - root - 2017-12-05 13:52:35.643074: step 12360, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.957 sec/batch; 85h:07m:40s remains)
INFO - root - 2017-12-05 13:52:44.734790: step 12370, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 77h:56m:52s remains)
INFO - root - 2017-12-05 13:52:53.849918: step 12380, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 84h:03m:27s remains)
INFO - root - 2017-12-05 13:53:02.818173: step 12390, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 82h:45m:10s remains)
INFO - root - 2017-12-05 13:53:11.818787: step 12400, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 79h:52m:50s remains)
2017-12-05 13:53:12.646170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2392974 -4.2359338 -4.2414451 -4.2475038 -4.2522407 -4.2629228 -4.2657747 -4.2559452 -4.248754 -4.2488236 -4.2530837 -4.2572031 -4.2533131 -4.2548404 -4.2667737][-4.1953712 -4.19799 -4.209199 -4.2209969 -4.2319303 -4.2472157 -4.2474961 -4.2266626 -4.2095847 -4.2049809 -4.2078958 -4.2141891 -4.2149572 -4.2276082 -4.2515655][-4.1980009 -4.2013125 -4.2093029 -4.2181153 -4.229578 -4.2462335 -4.2430348 -4.2158504 -4.1945252 -4.1890745 -4.1940022 -4.201642 -4.2049646 -4.222815 -4.2506709][-4.2389636 -4.2363043 -4.2342763 -4.2334242 -4.237936 -4.2478418 -4.2392836 -4.2116795 -4.1942863 -4.1949768 -4.2079692 -4.2206864 -4.2273769 -4.2437949 -4.2653351][-4.2581859 -4.2495 -4.2449288 -4.2413616 -4.2379427 -4.2345052 -4.2146964 -4.1835833 -4.1722894 -4.184814 -4.2093539 -4.2320747 -4.2461224 -4.2598152 -4.2720928][-4.2247596 -4.2153168 -4.2177744 -4.2178264 -4.2070889 -4.189342 -4.1534309 -4.11382 -4.1056991 -4.1290026 -4.1670365 -4.2036157 -4.2276273 -4.2424908 -4.2479076][-4.1560135 -4.1408973 -4.1495152 -4.1557755 -4.1421165 -4.1096888 -4.0487909 -3.9928749 -3.9870195 -4.0188236 -4.0670018 -4.116519 -4.151381 -4.1720977 -4.1738214][-4.1021166 -4.0864143 -4.0992026 -4.1048012 -4.0843925 -4.0374465 -3.9564891 -3.8867149 -3.8813972 -3.9124486 -3.95135 -3.9911335 -4.0204663 -4.0427928 -4.0413547][-4.1201267 -4.1123562 -4.1230817 -4.1205816 -4.094677 -4.0477309 -3.9761932 -3.9150889 -3.9100919 -3.930918 -3.9494905 -3.9582865 -3.95504 -3.9536662 -3.9422605][-4.1617713 -4.1604538 -4.1622691 -4.1525321 -4.1303911 -4.1051474 -4.0701008 -4.0401311 -4.0404768 -4.0525355 -4.0586867 -4.0506916 -4.0269327 -3.9971347 -3.9681122][-4.1667094 -4.1658978 -4.162044 -4.1522708 -4.1401167 -4.1382785 -4.13484 -4.1312194 -4.1394963 -4.1500745 -4.1524348 -4.1437221 -4.1261497 -4.0964389 -4.0627961][-4.1517992 -4.1482453 -4.1418028 -4.1341138 -4.1269164 -4.1348882 -4.1464624 -4.1567969 -4.1727896 -4.1856384 -4.1877713 -4.1829314 -4.1745725 -4.1577473 -4.1341009][-4.1524987 -4.1477036 -4.1425962 -4.1347146 -4.1259913 -4.1310487 -4.1422362 -4.1532722 -4.1681166 -4.1802588 -4.1822791 -4.179039 -4.175982 -4.1698275 -4.159656][-4.1656089 -4.1642332 -4.1610856 -4.1516886 -4.1380033 -4.1360588 -4.1384068 -4.1422768 -4.1495657 -4.1586452 -4.162096 -4.1621819 -4.1656537 -4.1655836 -4.1622219][-4.1646605 -4.1727257 -4.1739454 -4.1623926 -4.1444273 -4.1370125 -4.1298413 -4.1260481 -4.1257286 -4.1323986 -4.1414413 -4.149581 -4.1596408 -4.1624622 -4.158267]]...]
INFO - root - 2017-12-05 13:53:21.853365: step 12410, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 82h:50m:54s remains)
INFO - root - 2017-12-05 13:53:30.816863: step 12420, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 78h:16m:04s remains)
INFO - root - 2017-12-05 13:53:39.917397: step 12430, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 79h:52m:37s remains)
INFO - root - 2017-12-05 13:53:49.208419: step 12440, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 82h:46m:25s remains)
INFO - root - 2017-12-05 13:53:58.312926: step 12450, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 81h:05m:59s remains)
INFO - root - 2017-12-05 13:54:07.394448: step 12460, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 82h:44m:24s remains)
INFO - root - 2017-12-05 13:54:16.577305: step 12470, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 84h:30m:38s remains)
INFO - root - 2017-12-05 13:54:25.429070: step 12480, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 82h:48m:55s remains)
INFO - root - 2017-12-05 13:54:34.693377: step 12490, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 83h:49m:44s remains)
INFO - root - 2017-12-05 13:54:43.977365: step 12500, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 82h:47m:08s remains)
2017-12-05 13:54:44.768328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.104167 -4.1126242 -4.13541 -4.154211 -4.1713142 -4.1719923 -4.1513171 -4.1344032 -4.14818 -4.1746888 -4.1997194 -4.2214632 -4.2331395 -4.2301736 -4.2099147][-4.0904942 -4.0961571 -4.1166744 -4.1324997 -4.1530929 -4.1604691 -4.144 -4.1287746 -4.1405082 -4.1649184 -4.18785 -4.2044964 -4.2132893 -4.2137671 -4.2007232][-4.106967 -4.1027608 -4.1155062 -4.1291795 -4.1517367 -4.1662693 -4.158915 -4.142993 -4.1483264 -4.1727524 -4.1930919 -4.2025614 -4.204308 -4.2051673 -4.1971755][-4.1424522 -4.1279449 -4.1270447 -4.133132 -4.1521811 -4.1704245 -4.1683545 -4.15054 -4.1534848 -4.1799541 -4.2001605 -4.2071905 -4.2054172 -4.2050452 -4.1955304][-4.1836743 -4.1633077 -4.1511779 -4.1447625 -4.1499925 -4.1516991 -4.1376786 -4.1123185 -4.1143918 -4.1503677 -4.1830354 -4.2016273 -4.2085085 -4.2114906 -4.2030191][-4.2057734 -4.186543 -4.1712804 -4.1534672 -4.1353164 -4.1100845 -4.0733137 -4.0257092 -4.0162773 -4.07573 -4.1409893 -4.1836638 -4.2052727 -4.21686 -4.2129226][-4.1940017 -4.1728706 -4.158752 -4.1339226 -4.0984182 -4.0519705 -3.9880705 -3.9016948 -3.8748147 -3.9728222 -4.0834732 -4.1590123 -4.2007942 -4.2201529 -4.219295][-4.1717572 -4.1516595 -4.1392837 -4.1122923 -4.0699072 -4.01964 -3.9483604 -3.8364472 -3.7941141 -3.9128685 -4.0473752 -4.1385159 -4.1916571 -4.2124505 -4.2082419][-4.1627984 -4.1482558 -4.1413488 -4.1200371 -4.086834 -4.0508728 -3.9975286 -3.9046896 -3.8639302 -3.9565456 -4.0677466 -4.1453943 -4.1943979 -4.2075319 -4.1933374][-4.1789317 -4.173234 -4.1746011 -4.1652327 -4.1461449 -4.1267548 -4.097055 -4.0448885 -4.0190859 -4.0661116 -4.1289806 -4.1763458 -4.2077212 -4.2049747 -4.1805282][-4.2120519 -4.2145195 -4.2219887 -4.217555 -4.204484 -4.1921005 -4.173315 -4.146327 -4.1301188 -4.1544738 -4.1863532 -4.20911 -4.2228646 -4.2051744 -4.1724286][-4.2461252 -4.2510328 -4.2584352 -4.2538881 -4.2416792 -4.23061 -4.2126656 -4.1952963 -4.188139 -4.20764 -4.2301373 -4.2419276 -4.2425895 -4.214273 -4.1777453][-4.2665267 -4.2710614 -4.2733903 -4.2669339 -4.2561235 -4.2491031 -4.2340765 -4.2196689 -4.2202992 -4.2423577 -4.264544 -4.2727494 -4.2660289 -4.2356586 -4.1998653][-4.2797279 -4.2833266 -4.2801089 -4.2701783 -4.2607479 -4.2567592 -4.2454243 -4.2334175 -4.2394657 -4.2594662 -4.279192 -4.2868991 -4.2806282 -4.25387 -4.2228694][-4.2816749 -4.2813787 -4.2768965 -4.26804 -4.2632508 -4.2653027 -4.2616215 -4.2504654 -4.25382 -4.2654533 -4.2790923 -4.2888374 -4.2884026 -4.2691622 -4.2440777]]...]
INFO - root - 2017-12-05 13:54:53.815489: step 12510, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 78h:38m:23s remains)
INFO - root - 2017-12-05 13:55:02.944982: step 12520, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 80h:55m:58s remains)
INFO - root - 2017-12-05 13:55:12.046811: step 12530, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 81h:32m:21s remains)
INFO - root - 2017-12-05 13:55:21.210694: step 12540, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 80h:55m:42s remains)
INFO - root - 2017-12-05 13:55:30.324564: step 12550, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 79h:04m:08s remains)
INFO - root - 2017-12-05 13:55:39.414103: step 12560, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 81h:32m:48s remains)
INFO - root - 2017-12-05 13:55:48.439704: step 12570, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.913 sec/batch; 81h:06m:15s remains)
INFO - root - 2017-12-05 13:55:57.615158: step 12580, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 78h:16m:31s remains)
INFO - root - 2017-12-05 13:56:06.610298: step 12590, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 79h:42m:12s remains)
INFO - root - 2017-12-05 13:56:15.793843: step 12600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 83h:58m:58s remains)
2017-12-05 13:56:16.522088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0744605 -4.0785789 -4.068644 -4.0477285 -4.0411282 -4.0488167 -4.0567365 -4.0741253 -4.0842266 -4.0780973 -4.0765314 -4.1040373 -4.1406631 -4.1712117 -4.2068057][-4.0406556 -4.0400829 -4.0256324 -3.9989705 -3.9889483 -3.9948509 -4.0011368 -4.0182014 -4.0318489 -4.0303168 -4.0344744 -4.0689559 -4.1131029 -4.1526747 -4.1983919][-4.0355868 -4.0313811 -4.0099454 -3.9790266 -3.9639432 -3.9635463 -3.9671493 -3.9891262 -4.0075941 -4.0105419 -4.0174532 -4.0505056 -4.0926843 -4.1354752 -4.1855068][-4.0429358 -4.034369 -4.0063453 -3.9690905 -3.9456315 -3.9383383 -3.9407866 -3.9664621 -3.9945364 -4.0088325 -4.0257154 -4.0566988 -4.0907273 -4.1283937 -4.1738186][-4.05286 -4.0453777 -4.0174832 -3.9797339 -3.9432588 -3.9199619 -3.9176223 -3.9436636 -3.9823041 -4.01131 -4.0424852 -4.07423 -4.1012754 -4.1323013 -4.1721249][-4.061862 -4.0637946 -4.0444202 -4.0066514 -3.954035 -3.9053476 -3.88783 -3.9105539 -3.9591751 -4.0051651 -4.0561571 -4.10004 -4.1304579 -4.1568189 -4.1882329][-4.0756359 -4.0851307 -4.0713882 -4.0309396 -3.9659576 -3.9003618 -3.8718967 -3.8894563 -3.9355142 -3.9871621 -4.0519185 -4.1167293 -4.1621623 -4.1885366 -4.212966][-4.0909419 -4.104917 -4.0960245 -4.05733 -3.992909 -3.9289026 -3.8992324 -3.9088495 -3.9395986 -3.9830084 -4.0489745 -4.1269026 -4.1861458 -4.2138252 -4.2341604][-4.1200671 -4.1372304 -4.133244 -4.100172 -4.0435872 -3.9888632 -3.9611297 -3.9599755 -3.9719548 -4.0029058 -4.0626478 -4.1397004 -4.1987505 -4.2228971 -4.242147][-4.1394277 -4.1596684 -4.1631494 -4.1424322 -4.1008606 -4.0575428 -4.0340075 -4.0256634 -4.0295811 -4.0459552 -4.0893507 -4.1513371 -4.2003517 -4.2201462 -4.2399759][-4.1490741 -4.1768155 -4.1874723 -4.1777105 -4.1517425 -4.1212969 -4.1011434 -4.0926709 -4.092855 -4.0979943 -4.1236219 -4.1657224 -4.2031217 -4.2194362 -4.2405782][-4.1596251 -4.1840005 -4.1966228 -4.1955647 -4.182529 -4.1641684 -4.1476288 -4.1405711 -4.1383028 -4.1399274 -4.1554065 -4.1847425 -4.2137566 -4.2265625 -4.2465372][-4.1790228 -4.1940675 -4.2038131 -4.2061243 -4.2010622 -4.1921477 -4.1816721 -4.1777372 -4.17562 -4.1753836 -4.1861019 -4.2075524 -4.2302122 -4.2420444 -4.2587261][-4.2094545 -4.2154078 -4.2187757 -4.2200756 -4.2183805 -4.2146535 -4.2093759 -4.2078981 -4.2073789 -4.2059965 -4.2124977 -4.229075 -4.2480016 -4.2588892 -4.2721529][-4.2362609 -4.2354603 -4.233706 -4.2333856 -4.2326741 -4.2314186 -4.2294469 -4.2294188 -4.2309895 -4.231595 -4.2367768 -4.2493753 -4.2636318 -4.2739625 -4.2851596]]...]
INFO - root - 2017-12-05 13:56:25.676583: step 12610, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 79h:02m:19s remains)
INFO - root - 2017-12-05 13:56:34.755614: step 12620, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 79h:05m:21s remains)
INFO - root - 2017-12-05 13:56:43.725750: step 12630, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 80h:07m:45s remains)
INFO - root - 2017-12-05 13:56:52.911283: step 12640, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.930 sec/batch; 82h:38m:05s remains)
INFO - root - 2017-12-05 13:57:02.088001: step 12650, loss = 2.05, batch loss = 1.99 (8.2 examples/sec; 0.971 sec/batch; 86h:18m:09s remains)
INFO - root - 2017-12-05 13:57:11.203724: step 12660, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.876 sec/batch; 77h:47m:11s remains)
INFO - root - 2017-12-05 13:57:20.130140: step 12670, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 79h:24m:55s remains)
INFO - root - 2017-12-05 13:57:29.094694: step 12680, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 81h:58m:00s remains)
INFO - root - 2017-12-05 13:57:38.256524: step 12690, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 81h:51m:37s remains)
INFO - root - 2017-12-05 13:57:47.584058: step 12700, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 81h:02m:37s remains)
2017-12-05 13:57:48.344914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1711106 -4.1747923 -4.1776428 -4.1806216 -4.1990929 -4.2353778 -4.2749233 -4.2991767 -4.2978354 -4.2707739 -4.2349925 -4.2033982 -4.2109494 -4.2470732 -4.2688828][-4.1869535 -4.1942577 -4.1920524 -4.1789908 -4.1840582 -4.2208972 -4.2671127 -4.3027563 -4.3130627 -4.2945824 -4.2648473 -4.2296882 -4.22987 -4.2604327 -4.282721][-4.2032452 -4.2141223 -4.2054129 -4.1761627 -4.1636682 -4.1949949 -4.24312 -4.286602 -4.3101935 -4.3054028 -4.2861872 -4.2560296 -4.2515454 -4.2731338 -4.2893777][-4.2133064 -4.2255411 -4.2146349 -4.1753783 -4.1457014 -4.1648135 -4.2084332 -4.2560139 -4.2919602 -4.3042493 -4.298306 -4.2794223 -4.2752233 -4.2903991 -4.3016186][-4.2216911 -4.2340288 -4.2255712 -4.1849337 -4.144258 -4.1445971 -4.1738663 -4.218359 -4.2628269 -4.290863 -4.3013582 -4.2996278 -4.3030763 -4.31479 -4.31935][-4.2314143 -4.2484679 -4.2457447 -4.2115335 -4.165453 -4.140449 -4.1405559 -4.1698065 -4.2190962 -4.2653375 -4.2947869 -4.30975 -4.32231 -4.3302956 -4.32911][-4.2314911 -4.2573895 -4.2660408 -4.2406883 -4.1908755 -4.1386976 -4.0964756 -4.0971775 -4.1532359 -4.2226353 -4.2751651 -4.3081832 -4.3266559 -4.3334041 -4.3284259][-4.2164583 -4.2560668 -4.2773924 -4.2603574 -4.2100711 -4.1320262 -4.0431042 -4.0057316 -4.0728989 -4.1697865 -4.2449241 -4.2924452 -4.3139048 -4.3174038 -4.30874][-4.1948681 -4.2486606 -4.2812405 -4.2700543 -4.2192 -4.1236196 -3.9996729 -3.9283581 -4.0066051 -4.1261277 -4.2135496 -4.2683153 -4.2879586 -4.281826 -4.2692075][-4.1769052 -4.2375336 -4.2766914 -4.2688274 -4.2175813 -4.1212621 -3.9962578 -3.9192286 -3.9967082 -4.1177077 -4.204432 -4.2564569 -4.2662134 -4.24377 -4.2219129][-4.158658 -4.2239017 -4.2683086 -4.2630553 -4.2147508 -4.1289692 -4.0282412 -3.9723251 -4.0344467 -4.1365843 -4.2115521 -4.2514482 -4.2432165 -4.1968627 -4.164185][-4.143961 -4.2112103 -4.2589889 -4.2566371 -4.2107964 -4.1357474 -4.0596056 -4.0249686 -4.0748186 -4.1574445 -4.21669 -4.2357264 -4.2048788 -4.1393013 -4.1003494][-4.1428871 -4.2092214 -4.2549944 -4.2535434 -4.207777 -4.141418 -4.0838976 -4.0636106 -4.1054678 -4.1730433 -4.2173281 -4.2173023 -4.1738958 -4.1063581 -4.0639715][-4.1574583 -4.2201858 -4.2619309 -4.2586217 -4.2136712 -4.1538906 -4.1083746 -4.0940957 -4.1274881 -4.1832528 -4.2205367 -4.21522 -4.1748152 -4.1194062 -4.0827847][-4.1806035 -4.2339554 -4.271759 -4.2674818 -4.2267685 -4.1704664 -4.1276212 -4.1125817 -4.1374588 -4.1852942 -4.2215915 -4.2253547 -4.1963439 -4.1544976 -4.1285038]]...]
INFO - root - 2017-12-05 13:57:57.313023: step 12710, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 78h:22m:28s remains)
INFO - root - 2017-12-05 13:58:06.442226: step 12720, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 82h:07m:15s remains)
INFO - root - 2017-12-05 13:58:15.651021: step 12730, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 81h:59m:33s remains)
INFO - root - 2017-12-05 13:58:24.695599: step 12740, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 78h:26m:00s remains)
INFO - root - 2017-12-05 13:58:33.651413: step 12750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 77h:05m:49s remains)
INFO - root - 2017-12-05 13:58:42.843506: step 12760, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 83h:18m:27s remains)
INFO - root - 2017-12-05 13:58:52.045765: step 12770, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 79h:29m:33s remains)
INFO - root - 2017-12-05 13:59:01.066626: step 12780, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 80h:56m:44s remains)
INFO - root - 2017-12-05 13:59:10.058240: step 12790, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 80h:25m:43s remains)
INFO - root - 2017-12-05 13:59:19.249401: step 12800, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 80h:44m:37s remains)
2017-12-05 13:59:20.082241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1639924 -4.1533518 -4.1522884 -4.1574926 -4.1774206 -4.2103276 -4.2392464 -4.2545242 -4.253799 -4.23313 -4.2074566 -4.18768 -4.1748648 -4.1642241 -4.1527386][-4.1427593 -4.1364841 -4.136416 -4.1386929 -4.1521211 -4.1814456 -4.207098 -4.2184229 -4.2204423 -4.2090096 -4.1964059 -4.1873522 -4.1767988 -4.1560254 -4.1301069][-4.138813 -4.1367974 -4.135541 -4.1327429 -4.1360021 -4.1554546 -4.1740007 -4.1788707 -4.1798344 -4.1804667 -4.1865506 -4.1917715 -4.1892529 -4.16311 -4.1260557][-4.1539655 -4.153461 -4.1470637 -4.1380758 -4.1300821 -4.1357894 -4.1403079 -4.1323538 -4.1259823 -4.1426773 -4.1762938 -4.2002039 -4.2074566 -4.1811004 -4.1359277][-4.1799994 -4.1783957 -4.1650791 -4.1483335 -4.127562 -4.1176424 -4.102025 -4.0737877 -4.0540915 -4.0917583 -4.1618223 -4.2094946 -4.2284255 -4.2048144 -4.1544056][-4.2020683 -4.1983433 -4.1821585 -4.1586218 -4.1229572 -4.0924387 -4.0547991 -4.0021338 -3.965924 -4.0262809 -4.1332979 -4.2041593 -4.23638 -4.2166553 -4.16119][-4.2141895 -4.2115722 -4.1968141 -4.170217 -4.1222882 -4.07518 -4.0176463 -3.9419446 -3.8861458 -3.964623 -4.10012 -4.18666 -4.2287517 -4.209837 -4.1519532][-4.2284632 -4.2274194 -4.2164726 -4.1921244 -4.1441035 -4.0912838 -4.0254951 -3.9415326 -3.8794916 -3.9592247 -4.0954704 -4.1818604 -4.2242165 -4.2049088 -4.1501188][-4.2456694 -4.2445617 -4.2350578 -4.2174335 -4.1816688 -4.1381755 -4.0770669 -4.0024819 -3.9489682 -4.0119491 -4.1191549 -4.1912632 -4.2268925 -4.2076683 -4.1599245][-4.2591825 -4.2565837 -4.2459826 -4.2349687 -4.21298 -4.1836143 -4.13429 -4.0717983 -4.0257707 -4.0677328 -4.1424627 -4.1997046 -4.2307382 -4.214386 -4.1736422][-4.2645884 -4.2575836 -4.2409382 -4.2292185 -4.2150764 -4.1955571 -4.1604681 -4.1136746 -4.07505 -4.100966 -4.1518254 -4.1992316 -4.2282133 -4.2164869 -4.1840935][-4.2636385 -4.2541428 -4.2314968 -4.2165775 -4.2073402 -4.1947241 -4.17189 -4.1401548 -4.1136141 -4.131659 -4.165226 -4.2014852 -4.2272696 -4.2191591 -4.1936078][-4.2638311 -4.2549028 -4.2322512 -4.2176256 -4.2133627 -4.206449 -4.1922979 -4.1726933 -4.1572785 -4.1713042 -4.1920171 -4.2159944 -4.2351427 -4.2287278 -4.2072][-4.2676039 -4.2601032 -4.2407722 -4.2283826 -4.2282152 -4.2252836 -4.2174807 -4.2068882 -4.2003307 -4.2106142 -4.222 -4.23504 -4.2479758 -4.2426643 -4.2251205][-4.2778358 -4.2722487 -4.2564607 -4.2449903 -4.2455559 -4.2447224 -4.2411575 -4.2371407 -4.2360511 -4.2440524 -4.2505083 -4.2566581 -4.2652164 -4.2616839 -4.2481971]]...]
INFO - root - 2017-12-05 13:59:29.298010: step 12810, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 80h:23m:50s remains)
INFO - root - 2017-12-05 13:59:38.282036: step 12820, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 77h:31m:48s remains)
INFO - root - 2017-12-05 13:59:47.222386: step 12830, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 73h:56m:38s remains)
INFO - root - 2017-12-05 13:59:56.437438: step 12840, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 82h:14m:32s remains)
INFO - root - 2017-12-05 14:00:05.454179: step 12850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 75h:11m:15s remains)
INFO - root - 2017-12-05 14:00:14.545612: step 12860, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 81h:01m:47s remains)
INFO - root - 2017-12-05 14:00:23.531886: step 12870, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 79h:26m:34s remains)
INFO - root - 2017-12-05 14:00:32.772457: step 12880, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 82h:33m:44s remains)
INFO - root - 2017-12-05 14:00:41.893091: step 12890, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 78h:18m:02s remains)
INFO - root - 2017-12-05 14:00:50.906756: step 12900, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 81h:52m:19s remains)
2017-12-05 14:00:51.779122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2700109 -4.2809443 -4.2834873 -4.2824888 -4.280376 -4.2812457 -4.2831039 -4.2866974 -4.2892294 -4.2866263 -4.2783642 -4.2644186 -4.2554154 -4.258841 -4.2652678][-4.2512951 -4.2675762 -4.2705321 -4.2656031 -4.2606878 -4.2586184 -4.2542782 -4.2563515 -4.261806 -4.2605023 -4.2475271 -4.2243114 -4.2094326 -4.2128925 -4.224247][-4.2419124 -4.2641649 -4.26536 -4.2538471 -4.2444606 -4.2376575 -4.2268763 -4.230443 -4.2434044 -4.2467794 -4.2319136 -4.2011118 -4.1821456 -4.1862173 -4.1997118][-4.2319007 -4.260334 -4.2617621 -4.2435217 -4.223835 -4.2055635 -4.1866627 -4.1950111 -4.2200532 -4.2324309 -4.2226005 -4.1909409 -4.1706486 -4.1739249 -4.1863861][-4.2256265 -4.2559476 -4.2543721 -4.2272692 -4.1929922 -4.1580091 -4.1287656 -4.1465349 -4.184793 -4.205533 -4.204493 -4.180634 -4.1643815 -4.1651316 -4.1720133][-4.2213521 -4.24977 -4.2396832 -4.2017736 -4.1525488 -4.0946412 -4.0529637 -4.0867 -4.1453614 -4.1743512 -4.1799955 -4.1621122 -4.14699 -4.1451616 -4.1439571][-4.2212615 -4.2460489 -4.2287273 -4.1825666 -4.1172976 -4.0302024 -3.9658623 -4.0174489 -4.10151 -4.1443615 -4.1564975 -4.1452041 -4.1353731 -4.1305156 -4.118813][-4.2235842 -4.2488279 -4.2313914 -4.1829062 -4.1079721 -3.9966359 -3.9096947 -3.9683504 -4.0680609 -4.1236253 -4.1426549 -4.1386 -4.1362419 -4.1353121 -4.1205473][-4.2231565 -4.2478285 -4.2346525 -4.1936707 -4.1250668 -4.0196862 -3.9400737 -3.9870863 -4.0781407 -4.1363564 -4.1596622 -4.1589746 -4.1575809 -4.1570368 -4.1452842][-4.2244682 -4.2421126 -4.2334862 -4.2083545 -4.161921 -4.087388 -4.0348125 -4.0656004 -4.131567 -4.1788774 -4.1987271 -4.1992521 -4.1924038 -4.1825624 -4.1696825][-4.2259803 -4.2339931 -4.2244487 -4.2106776 -4.1875119 -4.1458006 -4.1169481 -4.1360145 -4.1776586 -4.2104959 -4.2271132 -4.2298293 -4.2205291 -4.2038479 -4.1893482][-4.2153292 -4.21595 -4.2080059 -4.2031994 -4.1973529 -4.1827893 -4.1706076 -4.180542 -4.2035847 -4.2231193 -4.2344012 -4.237751 -4.2288132 -4.2116256 -4.1973381][-4.1920276 -4.19462 -4.198163 -4.2054281 -4.2138205 -4.2169871 -4.2160344 -4.22095 -4.2268238 -4.2298489 -4.2301154 -4.2318106 -4.2270312 -4.2147903 -4.2036238][-4.1610107 -4.17609 -4.2025347 -4.2269144 -4.2416892 -4.2504892 -4.2566004 -4.2612867 -4.2564516 -4.2442122 -4.2341046 -4.2338405 -4.233614 -4.2263784 -4.2183094][-4.1336217 -4.1596456 -4.2089558 -4.2478213 -4.2621646 -4.2678952 -4.2759662 -4.2817822 -4.2733369 -4.2562528 -4.2438517 -4.2442961 -4.2474647 -4.2442746 -4.2389903]]...]
INFO - root - 2017-12-05 14:01:00.976494: step 12910, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 80h:32m:38s remains)
INFO - root - 2017-12-05 14:01:10.230840: step 12920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 81h:14m:25s remains)
INFO - root - 2017-12-05 14:01:19.277294: step 12930, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 79h:34m:57s remains)
INFO - root - 2017-12-05 14:01:28.183598: step 12940, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 78h:32m:45s remains)
INFO - root - 2017-12-05 14:01:37.275106: step 12950, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 82h:44m:59s remains)
INFO - root - 2017-12-05 14:01:46.230267: step 12960, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 78h:38m:48s remains)
INFO - root - 2017-12-05 14:01:55.421058: step 12970, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 78h:44m:55s remains)
INFO - root - 2017-12-05 14:02:04.447112: step 12980, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 84h:43m:27s remains)
INFO - root - 2017-12-05 14:02:13.616575: step 12990, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 78h:35m:16s remains)
INFO - root - 2017-12-05 14:02:22.674061: step 13000, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 79h:54m:27s remains)
2017-12-05 14:02:23.499384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31231 -4.311296 -4.3054795 -4.2986326 -4.294261 -4.291729 -4.2888074 -4.2865205 -4.2850609 -4.2867827 -4.2901249 -4.294065 -4.2985191 -4.3031731 -4.3067632][-4.3105674 -4.3075275 -4.3006082 -4.2933803 -4.2876782 -4.2826757 -4.2772479 -4.2723045 -4.268342 -4.2699761 -4.2764406 -4.2845321 -4.2926612 -4.3002753 -4.3064766][-4.3082728 -4.3028922 -4.2942352 -4.2855625 -4.2766161 -4.26582 -4.2542362 -4.2431855 -4.2342334 -4.2346339 -4.2457457 -4.2605476 -4.2751632 -4.2880259 -4.2993746][-4.304883 -4.2983851 -4.28823 -4.2766004 -4.2599235 -4.2354941 -4.208797 -4.1844211 -4.1669726 -4.1666293 -4.1856384 -4.2136512 -4.2414913 -4.2647557 -4.2849412][-4.2927685 -4.2872248 -4.2759533 -4.25922 -4.229763 -4.1841378 -4.1341767 -4.0917296 -4.0661659 -4.0675955 -4.1002021 -4.1490993 -4.196631 -4.2350931 -4.266192][-4.26477 -4.2620173 -4.2520161 -4.2306323 -4.1887083 -4.1236739 -4.0504179 -3.9909413 -3.9622157 -3.9717355 -4.0254745 -4.1001749 -4.1667295 -4.2165637 -4.2541604][-4.2230906 -4.22705 -4.2253547 -4.2098641 -4.169467 -4.1028223 -4.0245075 -3.9622545 -3.9369655 -3.9537432 -4.0193172 -4.1042767 -4.175561 -4.225225 -4.2588539][-4.1736183 -4.185946 -4.201386 -4.2075453 -4.1889143 -4.144052 -4.0851769 -4.0369606 -4.0178757 -4.0309644 -4.0849757 -4.15508 -4.2136049 -4.2526474 -4.27456][-4.1181602 -4.1373391 -4.1712093 -4.2032928 -4.2159829 -4.2044568 -4.1753931 -4.14617 -4.1332827 -4.1393967 -4.1738286 -4.2207112 -4.25947 -4.2836003 -4.2933569][-4.0815983 -4.10286 -4.1462035 -4.194819 -4.2322764 -4.2490888 -4.2477956 -4.2375288 -4.2298403 -4.2309365 -4.2486911 -4.2746258 -4.2951708 -4.3052564 -4.3050122][-4.0709271 -4.0880165 -4.1319165 -4.1855574 -4.2362218 -4.2711477 -4.2911906 -4.297183 -4.2946782 -4.2913742 -4.2950449 -4.3041334 -4.3099318 -4.3092661 -4.3029227][-4.0578227 -4.0673385 -4.1087556 -4.1647449 -4.2228274 -4.2699523 -4.3032079 -4.3193712 -4.3166666 -4.3044443 -4.296689 -4.2947917 -4.2931557 -4.2892475 -4.2836204][-4.040072 -4.0430422 -4.0797429 -4.1365743 -4.201776 -4.2578206 -4.2948403 -4.3087087 -4.2948451 -4.2690563 -4.2523522 -4.2474732 -4.2474971 -4.2483454 -4.2492661][-4.0425673 -4.0419846 -4.0725036 -4.1254468 -4.19135 -4.2506666 -4.2831721 -4.2835574 -4.2506156 -4.2065015 -4.181488 -4.1788244 -4.186307 -4.1964869 -4.2049131][-4.0703077 -4.070127 -4.0955367 -4.1409373 -4.1993957 -4.2520046 -4.2737236 -4.2559128 -4.1997252 -4.1362228 -4.102993 -4.1042261 -4.1224518 -4.1460443 -4.1638422]]...]
INFO - root - 2017-12-05 14:02:32.757748: step 13010, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 82h:29m:44s remains)
INFO - root - 2017-12-05 14:02:41.773688: step 13020, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 81h:37m:11s remains)
INFO - root - 2017-12-05 14:02:50.824359: step 13030, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 79h:38m:21s remains)
INFO - root - 2017-12-05 14:02:59.757218: step 13040, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 81h:38m:13s remains)
INFO - root - 2017-12-05 14:03:08.750725: step 13050, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 79h:43m:06s remains)
INFO - root - 2017-12-05 14:03:17.804100: step 13060, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 84h:41m:52s remains)
INFO - root - 2017-12-05 14:03:26.968672: step 13070, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 79h:01m:59s remains)
INFO - root - 2017-12-05 14:03:36.064088: step 13080, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 81h:03m:47s remains)
INFO - root - 2017-12-05 14:03:45.109579: step 13090, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 82h:37m:44s remains)
INFO - root - 2017-12-05 14:03:54.279159: step 13100, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 83h:13m:11s remains)
2017-12-05 14:03:55.089081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3227482 -4.3056793 -4.2932377 -4.2848425 -4.283154 -4.2806454 -4.27703 -4.2767158 -4.2808681 -4.2843571 -4.2870789 -4.2921066 -4.3000412 -4.3101192 -4.3198614][-4.3022132 -4.2765017 -4.2573996 -4.2431521 -4.2388878 -4.2317476 -4.2240686 -4.2225456 -4.2316213 -4.2408338 -4.2447243 -4.2535844 -4.2613969 -4.2686744 -4.2756724][-4.2776055 -4.2431602 -4.2144423 -4.1903167 -4.180253 -4.164011 -4.1484628 -4.1437311 -4.1604295 -4.1780515 -4.18405 -4.1966157 -4.2044187 -4.2085795 -4.2131405][-4.2528372 -4.2069788 -4.162725 -4.1276226 -4.1059504 -4.0770931 -4.0581417 -4.0518537 -4.0734544 -4.100831 -4.108798 -4.1216497 -4.1255507 -4.1278057 -4.1371922][-4.234901 -4.1809416 -4.1212826 -4.0738487 -4.0381317 -3.9946651 -3.9701147 -3.962954 -3.988421 -4.017293 -4.0195966 -4.0320048 -4.03812 -4.0455132 -4.0653758][-4.2244191 -4.1649837 -4.0935211 -4.0338831 -3.9822135 -3.9203973 -3.8821187 -3.8640995 -3.8898933 -3.9161458 -3.9180019 -3.9489522 -3.9756646 -3.9965978 -4.0259533][-4.2205453 -4.1506348 -4.0643811 -3.9897418 -3.9209123 -3.8322084 -3.7656708 -3.7409682 -3.7888508 -3.8393021 -3.8540671 -3.8980265 -3.9469156 -3.9862895 -4.0195041][-4.2131653 -4.1287994 -4.0252233 -3.9401853 -3.8640738 -3.7678819 -3.6926348 -3.6812086 -3.7703786 -3.8533609 -3.8785329 -3.9193962 -3.9723153 -4.014379 -4.0432148][-4.2067575 -4.1186419 -4.0162573 -3.9405196 -3.886775 -3.8241823 -3.7768581 -3.7796621 -3.8654311 -3.951555 -3.977375 -4.0071764 -4.0488911 -4.0811849 -4.1032953][-4.2104235 -4.1333752 -4.0507956 -3.99385 -3.9636421 -3.9339614 -3.9109342 -3.9094265 -3.972043 -4.0427165 -4.066081 -4.0918555 -4.1284842 -4.1565962 -4.1749768][-4.229641 -4.1710172 -4.1097374 -4.0644755 -4.0439744 -4.03117 -4.0222664 -4.0153003 -4.0554824 -4.1082587 -4.1282554 -4.1510282 -4.1809435 -4.2036185 -4.2191038][-4.2580662 -4.2175856 -4.175355 -4.1417475 -4.123776 -4.1171551 -4.1148896 -4.10933 -4.1381359 -4.1799364 -4.1992188 -4.2176876 -4.2349553 -4.2454038 -4.2523923][-4.2807794 -4.2526894 -4.2234669 -4.199501 -4.1847987 -4.1817441 -4.1846209 -4.1878123 -4.2149415 -4.2503223 -4.2677965 -4.2777176 -4.2830372 -4.2833633 -4.2812357][-4.2982373 -4.2784581 -4.2606091 -4.2448525 -4.2345142 -4.2359033 -4.2461 -4.257266 -4.2810545 -4.3051205 -4.3141375 -4.3148427 -4.3131733 -4.3105283 -4.3064785][-4.3137445 -4.2982225 -4.2854214 -4.2740545 -4.2670417 -4.2700739 -4.2813587 -4.293962 -4.3121576 -4.3269696 -4.3311338 -4.330071 -4.327713 -4.3269296 -4.3244896]]...]
INFO - root - 2017-12-05 14:04:04.216197: step 13110, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 78h:42m:06s remains)
INFO - root - 2017-12-05 14:04:13.326276: step 13120, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.914 sec/batch; 81h:07m:10s remains)
INFO - root - 2017-12-05 14:04:22.312557: step 13130, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 80h:37m:20s remains)
INFO - root - 2017-12-05 14:04:31.260385: step 13140, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 77h:39m:17s remains)
INFO - root - 2017-12-05 14:04:40.275860: step 13150, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 82h:06m:04s remains)
INFO - root - 2017-12-05 14:04:49.522285: step 13160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 82h:49m:17s remains)
INFO - root - 2017-12-05 14:04:58.760639: step 13170, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 84h:54m:00s remains)
INFO - root - 2017-12-05 14:05:08.012078: step 13180, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 80h:33m:42s remains)
INFO - root - 2017-12-05 14:05:16.980082: step 13190, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 82h:41m:40s remains)
INFO - root - 2017-12-05 14:05:26.187554: step 13200, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 81h:43m:03s remains)
2017-12-05 14:05:26.951215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2230277 -4.2202916 -4.2159529 -4.2086396 -4.2125711 -4.2246704 -4.243279 -4.251092 -4.2463317 -4.2370019 -4.2380152 -4.2402968 -4.2426844 -4.2410412 -4.2387748][-4.2059393 -4.2019005 -4.1926842 -4.178906 -4.1822734 -4.1960182 -4.2205968 -4.2291179 -4.2205286 -4.2099938 -4.2153406 -4.2208648 -4.2239842 -4.2152967 -4.2063956][-4.1901069 -4.1849575 -4.1708879 -4.1503024 -4.147717 -4.1602297 -4.1916628 -4.2050319 -4.1923194 -4.1857429 -4.1963925 -4.2043238 -4.20504 -4.1879525 -4.1749825][-4.1775041 -4.1699619 -4.1520047 -4.1254044 -4.1143794 -4.1194792 -4.1533942 -4.1698236 -4.1574426 -4.1613231 -4.1853466 -4.200912 -4.1960425 -4.1631513 -4.1437254][-4.1657333 -4.1527643 -4.1336021 -4.1030197 -4.0819449 -4.0751948 -4.1059942 -4.1254497 -4.1159019 -4.124908 -4.1609225 -4.1871042 -4.1819377 -4.1382737 -4.1117797][-4.1602044 -4.1404834 -4.1146641 -4.0734816 -4.0333138 -4.0084333 -4.0357423 -4.0639191 -4.0621061 -4.0730767 -4.1159983 -4.1536112 -4.1481218 -4.0932312 -4.0636992][-4.1558919 -4.1306677 -4.0948725 -4.046474 -3.9920731 -3.9472299 -3.9622924 -3.9923894 -4.0039787 -4.0230927 -4.06091 -4.1039333 -4.0966873 -4.0293021 -4.0039139][-4.1557922 -4.129065 -4.09099 -4.04286 -3.9863961 -3.9386871 -3.9436285 -3.9659679 -3.9879959 -4.013124 -4.0348206 -4.0675368 -4.0499558 -3.9659271 -3.9539671][-4.149322 -4.1244097 -4.0941133 -4.0577874 -4.0195665 -3.9886749 -3.9868476 -3.9952328 -4.0099888 -4.0267448 -4.0354629 -4.0528879 -4.0275211 -3.9443402 -3.9508486][-4.1551781 -4.1352921 -4.119525 -4.098022 -4.0795894 -4.06037 -4.0480371 -4.0418668 -4.0431194 -4.0519166 -4.0555983 -4.0675678 -4.0476885 -3.986372 -4.0045748][-4.1782 -4.1639938 -4.1544 -4.13565 -4.1243205 -4.1060028 -4.0892515 -4.0707536 -4.0618267 -4.0699468 -4.0786533 -4.0975847 -4.0904503 -4.0534482 -4.0693364][-4.1966419 -4.1852193 -4.1739826 -4.1548281 -4.1453233 -4.1250606 -4.1025319 -4.0770836 -4.0625992 -4.0746984 -4.0907946 -4.1190476 -4.120728 -4.0920444 -4.0972509][-4.2031169 -4.1888437 -4.1778603 -4.162879 -4.1540761 -4.13067 -4.1088085 -4.0919185 -4.0863118 -4.1007323 -4.1169634 -4.1454492 -4.1487708 -4.1231089 -4.1141849][-4.2073307 -4.1937456 -4.1856737 -4.1762409 -4.1706934 -4.1527929 -4.1399527 -4.1368127 -4.1391029 -4.1524911 -4.1680813 -4.1902304 -4.190681 -4.1674271 -4.1559992][-4.2223749 -4.2153778 -4.2127247 -4.2097559 -4.20621 -4.1946106 -4.1912537 -4.1976233 -4.2046337 -4.2166214 -4.2294927 -4.2442265 -4.2425346 -4.2250681 -4.2120233]]...]
INFO - root - 2017-12-05 14:05:36.022692: step 13210, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 79h:46m:24s remains)
INFO - root - 2017-12-05 14:05:45.126843: step 13220, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.895 sec/batch; 79h:23m:33s remains)
INFO - root - 2017-12-05 14:05:54.137355: step 13230, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 82h:32m:57s remains)
INFO - root - 2017-12-05 14:06:03.374066: step 13240, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 81h:48m:24s remains)
INFO - root - 2017-12-05 14:06:12.428584: step 13250, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 83h:28m:17s remains)
INFO - root - 2017-12-05 14:06:21.655405: step 13260, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 80h:32m:34s remains)
INFO - root - 2017-12-05 14:06:30.828969: step 13270, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 76h:47m:11s remains)
INFO - root - 2017-12-05 14:06:39.988150: step 13280, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 83h:34m:04s remains)
INFO - root - 2017-12-05 14:06:49.206951: step 13290, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 80h:44m:04s remains)
INFO - root - 2017-12-05 14:06:58.394637: step 13300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 80h:52m:45s remains)
2017-12-05 14:06:59.328543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1691117 -4.1732893 -4.1758113 -4.1667643 -4.1459837 -4.1183176 -4.0940471 -4.0693679 -4.0243511 -3.9741569 -3.9702907 -3.9995229 -4.0249586 -4.0373797 -4.0245318][-4.1899195 -4.1998506 -4.2037864 -4.1945381 -4.1700768 -4.1448193 -4.125309 -4.1065063 -4.0691929 -4.0285854 -4.0227356 -4.0367417 -4.0481272 -4.0538836 -4.0462694][-4.2156515 -4.230135 -4.2328196 -4.2162795 -4.183042 -4.1566076 -4.1449084 -4.1271644 -4.0894976 -4.0546083 -4.0453687 -4.0484447 -4.0499911 -4.0483613 -4.0474415][-4.2254219 -4.2372761 -4.2327814 -4.2071819 -4.164731 -4.1343284 -4.1269631 -4.1084871 -4.0690475 -4.0358667 -4.0254803 -4.0317583 -4.042789 -4.0471749 -4.0526543][-4.2309208 -4.2353172 -4.2223625 -4.1866288 -4.133707 -4.0953913 -4.0886621 -4.0650282 -4.0198879 -3.9904771 -3.9807682 -4.0017896 -4.0336342 -4.0515385 -4.0632348][-4.2053075 -4.19627 -4.1769714 -4.1375446 -4.0793223 -4.0436754 -4.0472851 -4.0295372 -3.982883 -3.9530785 -3.9427493 -3.97218 -4.0201077 -4.04037 -4.0563021][-4.1520905 -4.1320357 -4.1137581 -4.076745 -4.0201554 -3.9985266 -4.02192 -4.0164933 -3.9746642 -3.9464738 -3.9356296 -3.9618747 -4.007318 -4.0217495 -4.0375042][-4.1038265 -4.0832357 -4.0692825 -4.0371084 -3.9878216 -3.9834945 -4.0188465 -4.0209494 -3.9900942 -3.9717805 -3.9629383 -3.9810205 -4.0098515 -4.0150747 -4.0312586][-4.0757923 -4.0618324 -4.0533314 -4.02697 -3.9869375 -4.0000224 -4.0421524 -4.0528073 -4.035696 -4.0300479 -4.0257726 -4.0351005 -4.0408454 -4.029943 -4.0350676][-4.0762253 -4.0678473 -4.062274 -4.0389013 -4.0104184 -4.0340757 -4.0760827 -4.08672 -4.0816631 -4.0926847 -4.0953956 -4.0932078 -4.0817819 -4.0543575 -4.0464578][-4.0874052 -4.0833569 -4.0772896 -4.0533571 -4.0351076 -4.0599566 -4.0892982 -4.0918789 -4.0881844 -4.1096549 -4.1243815 -4.1216574 -4.1040297 -4.0739894 -4.0588179][-4.1145272 -4.1104765 -4.0987859 -4.0755773 -4.0602703 -4.0764718 -4.0893393 -4.0802875 -4.0752821 -4.1007619 -4.1201253 -4.1181149 -4.1012621 -4.0824075 -4.0727119][-4.1490526 -4.1448522 -4.1316404 -4.1117129 -4.0948753 -4.0936036 -4.0910869 -4.0786686 -4.0781379 -4.1041889 -4.1176095 -4.110858 -4.0956869 -4.0881438 -4.0844884][-4.178195 -4.170393 -4.1551933 -4.139492 -4.1237273 -4.1135406 -4.1032243 -4.092576 -4.0951028 -4.1161022 -4.12307 -4.1137486 -4.1015997 -4.0988383 -4.0995088][-4.19851 -4.1863751 -4.170156 -4.1588211 -4.1471705 -4.1331334 -4.1194186 -4.1130877 -4.1192579 -4.134563 -4.1382308 -4.1320438 -4.1240005 -4.1209154 -4.11864]]...]
INFO - root - 2017-12-05 14:07:08.575347: step 13310, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 81h:07m:49s remains)
INFO - root - 2017-12-05 14:07:17.478799: step 13320, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 81h:07m:55s remains)
INFO - root - 2017-12-05 14:07:26.450848: step 13330, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 82h:07m:49s remains)
INFO - root - 2017-12-05 14:07:35.666488: step 13340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 81h:49m:52s remains)
INFO - root - 2017-12-05 14:07:44.600116: step 13350, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 77h:58m:02s remains)
INFO - root - 2017-12-05 14:07:53.678235: step 13360, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 78h:48m:12s remains)
INFO - root - 2017-12-05 14:08:02.872211: step 13370, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 78h:41m:44s remains)
INFO - root - 2017-12-05 14:08:11.900672: step 13380, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 81h:17m:21s remains)
INFO - root - 2017-12-05 14:08:21.193626: step 13390, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 83h:16m:26s remains)
INFO - root - 2017-12-05 14:08:30.384639: step 13400, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 79h:33m:39s remains)
2017-12-05 14:08:31.099144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3066049 -4.2849512 -4.2506547 -4.2153463 -4.1893573 -4.1749687 -4.1522465 -4.1211109 -4.0985332 -4.0980687 -4.102798 -4.13887 -4.1985755 -4.2503285 -4.2862163][-4.3023405 -4.2769289 -4.2406945 -4.20633 -4.1823549 -4.1683388 -4.1477132 -4.1103969 -4.0868058 -4.0958381 -4.1147318 -4.1626716 -4.2257938 -4.2799983 -4.3099136][-4.2953224 -4.2681274 -4.232976 -4.1998167 -4.1767659 -4.1575751 -4.1359706 -4.0950055 -4.0739045 -4.092103 -4.1227441 -4.174026 -4.2386489 -4.2950554 -4.3205838][-4.2895532 -4.2605443 -4.2245612 -4.1889238 -4.1584897 -4.1306863 -4.1020856 -4.0601959 -4.044528 -4.072197 -4.1149549 -4.1699767 -4.2372327 -4.2954488 -4.3227205][-4.2879262 -4.2578216 -4.2176833 -4.1726794 -4.1257057 -4.0838661 -4.042963 -3.9991798 -3.9946973 -4.0366488 -4.0945697 -4.1590452 -4.22953 -4.2885995 -4.3214431][-4.2850194 -4.2520652 -4.2060285 -4.1458569 -4.0756183 -4.0120111 -3.9600806 -3.9296055 -3.9582758 -4.0196633 -4.0887456 -4.1603937 -4.229198 -4.2852535 -4.3223662][-4.2844081 -4.2470322 -4.1924667 -4.1142969 -4.0208268 -3.9333205 -3.8718548 -3.8695312 -3.9395237 -4.021091 -4.0983348 -4.175385 -4.2399931 -4.2916365 -4.3271756][-4.2913074 -4.2535367 -4.1948738 -4.1081533 -4.0036678 -3.902353 -3.8369288 -3.8564465 -3.9532089 -4.0439267 -4.1215143 -4.1959238 -4.257164 -4.302937 -4.3348923][-4.2986164 -4.2658176 -4.2139878 -4.1333861 -4.0317631 -3.9322078 -3.8708599 -3.9010634 -3.9999082 -4.0870523 -4.1607971 -4.2301316 -4.2828846 -4.3173442 -4.3428826][-4.3040724 -4.2771893 -4.2348013 -4.166492 -4.0772896 -3.9950533 -3.9482844 -3.9780092 -4.0645051 -4.1448703 -4.2143731 -4.2739038 -4.3137569 -4.3341146 -4.3508658][-4.3138261 -4.2910132 -4.2573142 -4.2030668 -4.1295338 -4.0696788 -4.0426021 -4.0682197 -4.133338 -4.1997271 -4.2569113 -4.3030438 -4.3337235 -4.345365 -4.3562069][-4.3252444 -4.3043127 -4.2764835 -4.2322612 -4.1704216 -4.1272411 -4.1177692 -4.1441536 -4.194459 -4.2453208 -4.2871633 -4.3192787 -4.3411546 -4.3499365 -4.3591461][-4.3314314 -4.3111734 -4.2855744 -4.2468214 -4.196311 -4.1654429 -4.1684628 -4.1984496 -4.2388291 -4.2783737 -4.3094578 -4.3312173 -4.3464541 -4.3532262 -4.3608265][-4.3347578 -4.3175735 -4.2952437 -4.2623234 -4.2204847 -4.1970954 -4.2057323 -4.2350888 -4.2684684 -4.299777 -4.3236208 -4.3401318 -4.3505759 -4.3540163 -4.3585615][-4.3362446 -4.3226094 -4.3062444 -4.2822795 -4.2524447 -4.2354827 -4.2429872 -4.2663255 -4.2929025 -4.3170385 -4.333591 -4.3446059 -4.3513842 -4.352716 -4.354353]]...]
INFO - root - 2017-12-05 14:08:39.954815: step 13410, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 80h:04m:53s remains)
INFO - root - 2017-12-05 14:08:49.103491: step 13420, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 81h:11m:17s remains)
INFO - root - 2017-12-05 14:08:58.392418: step 13430, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 79h:25m:52s remains)
INFO - root - 2017-12-05 14:09:07.635964: step 13440, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 82h:27m:39s remains)
INFO - root - 2017-12-05 14:09:16.679014: step 13450, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 78h:36m:19s remains)
INFO - root - 2017-12-05 14:09:25.880969: step 13460, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.964 sec/batch; 85h:26m:05s remains)
INFO - root - 2017-12-05 14:09:35.031861: step 13470, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 79h:28m:37s remains)
INFO - root - 2017-12-05 14:09:44.131010: step 13480, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 79h:41m:13s remains)
INFO - root - 2017-12-05 14:09:53.191603: step 13490, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 82h:32m:25s remains)
INFO - root - 2017-12-05 14:10:02.437194: step 13500, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.938 sec/batch; 83h:09m:35s remains)
2017-12-05 14:10:03.190769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2708654 -4.2557082 -4.2464819 -4.2389836 -4.2346964 -4.2339621 -4.2332025 -4.2438993 -4.2634778 -4.2799335 -4.2905397 -4.3034377 -4.3052964 -4.3013287 -4.290556][-4.2754631 -4.2620273 -4.2562537 -4.2476697 -4.2385511 -4.2312779 -4.2209663 -4.2244473 -4.2459683 -4.2687125 -4.2811413 -4.292686 -4.2937393 -4.2922597 -4.2789178][-4.277554 -4.2713032 -4.2677269 -4.25422 -4.2398267 -4.2273669 -4.2085495 -4.2026606 -4.2229881 -4.2502084 -4.2675476 -4.2807345 -4.2830396 -4.2802505 -4.264874][-4.279119 -4.2782006 -4.2717662 -4.2530823 -4.2374148 -4.2227588 -4.1969442 -4.1781516 -4.1925426 -4.2235413 -4.251935 -4.2711854 -4.2776937 -4.2748318 -4.2583065][-4.2765493 -4.2756004 -4.2644181 -4.2383842 -4.2183194 -4.1994467 -4.1688023 -4.1422892 -4.1485271 -4.1839461 -4.2280517 -4.2613368 -4.2784295 -4.2786779 -4.2600393][-4.2676454 -4.2672153 -4.2546625 -4.2233253 -4.1909943 -4.1603723 -4.1263452 -4.0996342 -4.0989213 -4.1322079 -4.1887994 -4.2401123 -4.2736578 -4.2818074 -4.263876][-4.2593369 -4.2596626 -4.247169 -4.208919 -4.162674 -4.1161003 -4.0740342 -4.041048 -4.0325089 -4.0621762 -4.1287279 -4.1996322 -4.2536368 -4.2734661 -4.2598462][-4.2645893 -4.2640786 -4.2493172 -4.2068586 -4.1490893 -4.0866442 -4.0325136 -3.9896274 -3.972532 -3.9973679 -4.0735235 -4.1604452 -4.227766 -4.2568569 -4.2516708][-4.2728071 -4.2699041 -4.2508559 -4.2052407 -4.1423016 -4.0738955 -4.0154123 -3.9753428 -3.9598389 -3.9810357 -4.0574126 -4.1447639 -4.2132258 -4.24666 -4.2508526][-4.2807279 -4.2760468 -4.2557797 -4.2117305 -4.1497631 -4.0802555 -4.0263996 -3.9983349 -3.9954925 -4.0199504 -4.0868177 -4.1573763 -4.212779 -4.2418075 -4.2502346][-4.2775927 -4.2730527 -4.257328 -4.2205215 -4.1686306 -4.1091948 -4.0659232 -4.0504594 -4.0610104 -4.0910726 -4.1406493 -4.1840725 -4.2141466 -4.2269506 -4.2288256][-4.2688808 -4.2662044 -4.2567434 -4.2318664 -4.1977324 -4.154387 -4.1276832 -4.1252871 -4.1425886 -4.169332 -4.1997333 -4.2140455 -4.2110033 -4.1982994 -4.1872358][-4.2444739 -4.2451191 -4.242197 -4.2298732 -4.2146006 -4.1921577 -4.1848345 -4.1973648 -4.218483 -4.2366319 -4.2466607 -4.2356539 -4.20602 -4.17098 -4.1399517][-4.2097235 -4.2128291 -4.2149439 -4.2112241 -4.2078629 -4.1979222 -4.2055807 -4.230782 -4.2577572 -4.27048 -4.2668018 -4.2401032 -4.1974735 -4.1497712 -4.1028376][-4.177659 -4.1803727 -4.1774826 -4.1710138 -4.1679196 -4.1631765 -4.1815543 -4.2206712 -4.2591677 -4.2709866 -4.26067 -4.2276006 -4.1787333 -4.1244617 -4.07049]]...]
INFO - root - 2017-12-05 14:10:12.332026: step 13510, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 80h:56m:56s remains)
INFO - root - 2017-12-05 14:10:21.417559: step 13520, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 80h:58m:30s remains)
INFO - root - 2017-12-05 14:10:30.520039: step 13530, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 80h:49m:07s remains)
INFO - root - 2017-12-05 14:10:39.520620: step 13540, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 79h:11m:55s remains)
INFO - root - 2017-12-05 14:10:48.750100: step 13550, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.943 sec/batch; 83h:32m:05s remains)
INFO - root - 2017-12-05 14:10:57.875108: step 13560, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 82h:27m:43s remains)
INFO - root - 2017-12-05 14:11:06.940706: step 13570, loss = 2.02, batch loss = 1.97 (8.9 examples/sec; 0.896 sec/batch; 79h:21m:43s remains)
INFO - root - 2017-12-05 14:11:16.000088: step 13580, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 81h:53m:31s remains)
INFO - root - 2017-12-05 14:11:25.322232: step 13590, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.960 sec/batch; 84h:59m:56s remains)
INFO - root - 2017-12-05 14:11:34.451405: step 13600, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 80h:14m:29s remains)
2017-12-05 14:11:35.315121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9787619 -4.05034 -4.1299105 -4.2081008 -4.2709317 -4.2974582 -4.2879353 -4.2614188 -4.2057762 -4.1217928 -4.0297375 -3.9510772 -3.8963313 -3.8793225 -3.9167063][-3.9360037 -4.0042319 -4.0881329 -4.1708708 -4.2391129 -4.2728257 -4.2733417 -4.2620687 -4.223804 -4.1587305 -4.0790539 -3.9975142 -3.9270267 -3.8880694 -3.9064269][-3.9746418 -4.0389271 -4.1078668 -4.1693869 -4.215518 -4.2390566 -4.2468657 -4.2560282 -4.2476578 -4.2196331 -4.1681771 -4.0945625 -4.0163732 -3.9681673 -3.9766273][-4.0580678 -4.1141658 -4.1560559 -4.1804447 -4.1891851 -4.1890273 -4.197475 -4.2259612 -4.2485476 -4.25439 -4.2340727 -4.1807261 -4.1169133 -4.0787287 -4.0845222][-4.138864 -4.1703758 -4.1809254 -4.1690054 -4.1419635 -4.1162081 -4.122179 -4.1664076 -4.2180963 -4.2521973 -4.261066 -4.238492 -4.2041516 -4.1786175 -4.17463][-4.1936803 -4.1870241 -4.1576781 -4.1128011 -4.0641804 -4.0263515 -4.032506 -4.0849867 -4.1530471 -4.2093034 -4.2486992 -4.2664752 -4.262537 -4.24565 -4.2260766][-4.2224417 -4.1796288 -4.1100268 -4.0379205 -3.9818311 -3.9452443 -3.9451342 -3.9903526 -4.0598226 -4.1319933 -4.1988897 -4.2581739 -4.2842455 -4.2768207 -4.2480712][-4.238863 -4.1748657 -4.0829926 -3.9931552 -3.9319491 -3.8964529 -3.8847516 -3.9118457 -3.9730432 -4.052784 -4.1436348 -4.2353439 -4.2902288 -4.29827 -4.2686687][-4.2547235 -4.1949525 -4.111598 -4.0265851 -3.9598012 -3.9127381 -3.8887558 -3.9043725 -3.9563026 -4.0303578 -4.1221814 -4.2175336 -4.2833042 -4.304904 -4.2896724][-4.2707324 -4.2303267 -4.1719341 -4.1064692 -4.0406532 -3.9835875 -3.9567161 -3.9758148 -4.0235567 -4.08379 -4.150701 -4.2189879 -4.2726493 -4.3028488 -4.3072023][-4.2774405 -4.2581758 -4.2236948 -4.1786356 -4.1227188 -4.0721188 -4.0556407 -4.0791955 -4.1206436 -4.1619854 -4.1953588 -4.2224236 -4.2508225 -4.280467 -4.30303][-4.2656612 -4.2654047 -4.2511258 -4.225769 -4.1887832 -4.15852 -4.1544614 -4.1692538 -4.1870627 -4.1978245 -4.1976337 -4.1857729 -4.1891885 -4.2199888 -4.2590427][-4.2430463 -4.2539587 -4.2548475 -4.2486916 -4.2334461 -4.2183743 -4.2147279 -4.2100148 -4.1977406 -4.1772552 -4.1510186 -4.11493 -4.1025977 -4.1387873 -4.1928258][-4.2306232 -4.2443438 -4.2532792 -4.2607636 -4.2599459 -4.2503767 -4.2341251 -4.2028332 -4.1564388 -4.1050706 -4.0636148 -4.0209856 -4.0047731 -4.042655 -4.1058822][-4.2373161 -4.2489324 -4.26102 -4.2750368 -4.2812133 -4.2702236 -4.23914 -4.1827683 -4.1031237 -4.021534 -3.9707065 -3.9352088 -3.9179673 -3.9461403 -4.0052986]]...]
INFO - root - 2017-12-05 14:11:44.522414: step 13610, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 81h:54m:58s remains)
INFO - root - 2017-12-05 14:11:53.674336: step 13620, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 79h:55m:51s remains)
INFO - root - 2017-12-05 14:12:02.780468: step 13630, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 84h:22m:32s remains)
INFO - root - 2017-12-05 14:12:11.946680: step 13640, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 78h:30m:18s remains)
INFO - root - 2017-12-05 14:12:21.014488: step 13650, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 78h:10m:16s remains)
INFO - root - 2017-12-05 14:12:30.014351: step 13660, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 80h:28m:27s remains)
INFO - root - 2017-12-05 14:12:39.287001: step 13670, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 81h:22m:21s remains)
INFO - root - 2017-12-05 14:12:48.434629: step 13680, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 80h:18m:47s remains)
INFO - root - 2017-12-05 14:12:57.296862: step 13690, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 71h:29m:35s remains)
INFO - root - 2017-12-05 14:13:06.614727: step 13700, loss = 2.09, batch loss = 2.04 (8.4 examples/sec; 0.949 sec/batch; 84h:04m:21s remains)
2017-12-05 14:13:07.395115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1387897 -4.141407 -4.1535435 -4.176137 -4.1907868 -4.1977706 -4.2088346 -4.2146983 -4.2134395 -4.2109194 -4.2097921 -4.2071123 -4.2063971 -4.2023644 -4.2053514][-4.1550255 -4.1669922 -4.1823988 -4.2024279 -4.20641 -4.2005444 -4.201889 -4.2051373 -4.2062068 -4.2017903 -4.196701 -4.1893845 -4.1870747 -4.186275 -4.1927223][-4.1717544 -4.1915374 -4.2062292 -4.2215018 -4.2220273 -4.20864 -4.196609 -4.1897092 -4.19018 -4.1877317 -4.1823521 -4.1746736 -4.1724315 -4.1723394 -4.1810951][-4.1749825 -4.2002263 -4.2140427 -4.2248812 -4.2239676 -4.2090178 -4.1856084 -4.1646152 -4.1611285 -4.1581011 -4.1533494 -4.1532826 -4.1558237 -4.1576552 -4.1691456][-4.1647129 -4.1877055 -4.2017832 -4.2059441 -4.200078 -4.1759458 -4.1320872 -4.1054473 -4.1142454 -4.1160946 -4.1133533 -4.1208425 -4.1327477 -4.1457729 -4.1622143][-4.1510653 -4.170083 -4.1826339 -4.1776857 -4.1572595 -4.106298 -4.0275078 -3.9965489 -4.0410991 -4.0785108 -4.0929155 -4.1067047 -4.1185346 -4.1333776 -4.1522322][-4.1306715 -4.1446786 -4.1551619 -4.1423454 -4.1112566 -4.0388331 -3.9259689 -3.8961086 -3.9874697 -4.0671959 -4.099514 -4.1109405 -4.1126142 -4.1205683 -4.1367264][-4.1228995 -4.130363 -4.1369386 -4.12086 -4.09264 -4.038919 -3.95769 -3.9476697 -4.0361691 -4.1077871 -4.13257 -4.1339126 -4.1263256 -4.1254525 -4.1329608][-4.1376076 -4.1432023 -4.1481094 -4.135675 -4.1184392 -4.096837 -4.0674853 -4.073823 -4.1253772 -4.1634197 -4.1701965 -4.1591291 -4.1427279 -4.136579 -4.1389937][-4.1675916 -4.175323 -4.1821008 -4.1749582 -4.1636338 -4.1525226 -4.1440973 -4.1531014 -4.1805472 -4.197722 -4.1948376 -4.178092 -4.1546421 -4.1422939 -4.1441717][-4.2034588 -4.2030783 -4.2039056 -4.1997962 -4.1910591 -4.1790938 -4.1741228 -4.1780224 -4.1898155 -4.1977959 -4.1951513 -4.183773 -4.1628175 -4.1454096 -4.142364][-4.22155 -4.2084489 -4.1985435 -4.1957994 -4.1893067 -4.177515 -4.1727386 -4.1725259 -4.1762404 -4.1859455 -4.19473 -4.1942091 -4.1798892 -4.1616817 -4.1500072][-4.2183146 -4.1976852 -4.1824379 -4.1798787 -4.1755228 -4.1655607 -4.1647539 -4.1631718 -4.1621652 -4.1758881 -4.193428 -4.2019315 -4.1964607 -4.1846414 -4.1689615][-4.2053156 -4.1854715 -4.1703849 -4.1664419 -4.167212 -4.1635032 -4.1654406 -4.1629648 -4.15913 -4.1715384 -4.1895127 -4.2002811 -4.2042952 -4.2021255 -4.1915078][-4.1958408 -4.1832323 -4.17444 -4.1722493 -4.1778111 -4.1728983 -4.1683211 -4.1636443 -4.160738 -4.1669235 -4.183485 -4.1992455 -4.2127342 -4.2200847 -4.2140937]]...]
INFO - root - 2017-12-05 14:13:16.599669: step 13710, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 82h:51m:53s remains)
INFO - root - 2017-12-05 14:13:25.667627: step 13720, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 79h:37m:26s remains)
INFO - root - 2017-12-05 14:13:34.791925: step 13730, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 81h:11m:08s remains)
INFO - root - 2017-12-05 14:13:44.032445: step 13740, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 78h:43m:49s remains)
INFO - root - 2017-12-05 14:13:53.120971: step 13750, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 80h:49m:32s remains)
INFO - root - 2017-12-05 14:14:02.118713: step 13760, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 78h:35m:31s remains)
INFO - root - 2017-12-05 14:14:11.139678: step 13770, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 80h:25m:47s remains)
INFO - root - 2017-12-05 14:14:20.243559: step 13780, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 80h:53m:20s remains)
INFO - root - 2017-12-05 14:14:29.264077: step 13790, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 83h:04m:53s remains)
INFO - root - 2017-12-05 14:14:38.415997: step 13800, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 79h:54m:16s remains)
2017-12-05 14:14:39.306342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2774415 -4.2765937 -4.2706542 -4.2624111 -4.2598834 -4.2559037 -4.2382264 -4.22595 -4.2287741 -4.2346468 -4.2438912 -4.262908 -4.284122 -4.3038716 -4.3231363][-4.2433596 -4.2405629 -4.2351813 -4.2226911 -4.2218494 -4.2209926 -4.1958723 -4.1703281 -4.1720886 -4.1854439 -4.2002492 -4.2258945 -4.2553296 -4.2820311 -4.3074913][-4.1986442 -4.1911178 -4.1872082 -4.1725883 -4.1726303 -4.1747847 -4.1412539 -4.0976887 -4.0982475 -4.1227579 -4.1502376 -4.1881361 -4.2315969 -4.2674036 -4.2969737][-4.1462808 -4.1294785 -4.1260858 -4.1142774 -4.1172915 -4.1266031 -4.0841756 -4.0238528 -4.0235968 -4.0637832 -4.1045938 -4.1550932 -4.2137265 -4.26046 -4.2939944][-4.0910153 -4.0627055 -4.0653019 -4.0577354 -4.0608096 -4.0735769 -4.0219569 -3.9473376 -3.9542465 -4.0197096 -4.0738282 -4.1314888 -4.1991715 -4.2555223 -4.2948937][-4.0495214 -4.0105739 -4.015532 -4.009582 -4.0057125 -4.0074658 -3.9366765 -3.8470187 -3.8805311 -3.9896014 -4.0617118 -4.1209397 -4.1921797 -4.255271 -4.2988319][-4.0426612 -4.0038157 -3.9983604 -3.9805984 -3.9595249 -3.9421086 -3.8535984 -3.7501047 -3.8110762 -3.9625483 -4.0530066 -4.1161146 -4.1921234 -4.2587037 -4.30161][-4.0603213 -4.0385504 -4.0285878 -3.9995973 -3.9648588 -3.93555 -3.8590803 -3.7697959 -3.8204002 -3.9640336 -4.046979 -4.1055222 -4.1881571 -4.2595019 -4.3010912][-4.09766 -4.0883679 -4.0781755 -4.052731 -4.0185 -3.9954951 -3.9468856 -3.8774626 -3.8978553 -3.9986558 -4.0577865 -4.1077776 -4.1913552 -4.2620173 -4.3007636][-4.1364608 -4.1347704 -4.1346083 -4.1225786 -4.0929031 -4.0671234 -4.0284033 -3.9645603 -3.962626 -4.0315542 -4.0775552 -4.1234994 -4.2019386 -4.2693954 -4.304266][-4.1644979 -4.1612825 -4.1653457 -4.1661716 -4.1454482 -4.1155448 -4.0777483 -4.0248346 -4.0169368 -4.0686579 -4.1091037 -4.1507959 -4.2192039 -4.2796259 -4.3097529][-4.1706362 -4.1626711 -4.1685872 -4.1755419 -4.1651845 -4.1403446 -4.1068273 -4.0663457 -4.0604296 -4.1022763 -4.1361122 -4.17187 -4.2323461 -4.2873306 -4.3141656][-4.1760516 -4.1667504 -4.1705446 -4.1794696 -4.1712518 -4.1493654 -4.119895 -4.0887055 -4.0882192 -4.1266875 -4.1589551 -4.1910148 -4.2454925 -4.29587 -4.3186836][-4.1894326 -4.1831918 -4.1863508 -4.19418 -4.1877337 -4.170434 -4.1474113 -4.1235824 -4.12371 -4.1568069 -4.1913419 -4.2229729 -4.2675204 -4.3075452 -4.3248367][-4.2181678 -4.2183909 -4.2232242 -4.2310748 -4.2308526 -4.2205434 -4.2022333 -4.1834803 -4.1839004 -4.2094193 -4.2409167 -4.2685752 -4.29843 -4.3223071 -4.3324394]]...]
INFO - root - 2017-12-05 14:14:48.446275: step 13810, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 83h:08m:26s remains)
INFO - root - 2017-12-05 14:14:57.669930: step 13820, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:19m:53s remains)
INFO - root - 2017-12-05 14:15:06.816052: step 13830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 80h:43m:17s remains)
INFO - root - 2017-12-05 14:15:16.067859: step 13840, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 81h:04m:51s remains)
INFO - root - 2017-12-05 14:15:25.203425: step 13850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 80h:04m:38s remains)
INFO - root - 2017-12-05 14:15:34.199593: step 13860, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 79h:30m:37s remains)
INFO - root - 2017-12-05 14:15:43.307879: step 13870, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 78h:18m:32s remains)
INFO - root - 2017-12-05 14:15:52.356786: step 13880, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 82h:14m:12s remains)
INFO - root - 2017-12-05 14:16:01.391526: step 13890, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 77h:03m:40s remains)
INFO - root - 2017-12-05 14:16:10.738282: step 13900, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 81h:48m:53s remains)
2017-12-05 14:16:11.522417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2348003 -4.2300987 -4.2296529 -4.2326107 -4.2408681 -4.2569489 -4.2693329 -4.2718821 -4.2703338 -4.2668061 -4.2596951 -4.2568808 -4.2670288 -4.2802539 -4.2841597][-4.2318792 -4.2249527 -4.2218862 -4.2189941 -4.2213216 -4.237958 -4.2543731 -4.2620397 -4.2655272 -4.2634397 -4.2584395 -4.2573576 -4.2660871 -4.2789774 -4.2826271][-4.2384448 -4.2302828 -4.2237129 -4.209167 -4.1998444 -4.2128568 -4.2332377 -4.2487206 -4.2611122 -4.2667756 -4.2697148 -4.2721572 -4.278347 -4.2866869 -4.2883806][-4.2418909 -4.2331228 -4.2197886 -4.1890764 -4.1610413 -4.1601624 -4.1752768 -4.1929111 -4.2156644 -4.238771 -4.2607036 -4.2753787 -4.2840047 -4.2897072 -4.2897544][-4.2464857 -4.2365179 -4.2122078 -4.1615181 -4.1093841 -4.0826697 -4.0785675 -4.0854526 -4.1133747 -4.1586614 -4.2116284 -4.2504272 -4.2709022 -4.2807369 -4.2844291][-4.2555203 -4.2456775 -4.2142682 -4.1514173 -4.0821662 -4.0283632 -3.9936037 -3.9761317 -3.9968407 -4.0574226 -4.1405153 -4.20747 -4.2458496 -4.265069 -4.2763047][-4.2635241 -4.2576585 -4.23073 -4.1752524 -4.1095777 -4.0467129 -3.9915669 -3.9491942 -3.9504068 -4.0053735 -4.09149 -4.1684227 -4.2155628 -4.2413144 -4.2586594][-4.2669115 -4.267046 -4.254396 -4.2220969 -4.18169 -4.1386824 -4.090291 -4.0369444 -4.0161357 -4.0439816 -4.1034365 -4.1658616 -4.204947 -4.2276969 -4.2473326][-4.2639956 -4.2708712 -4.274797 -4.2656813 -4.2499971 -4.2321033 -4.2038479 -4.1581917 -4.1288304 -4.1305509 -4.1580858 -4.1956697 -4.219007 -4.2335238 -4.2485733][-4.2499747 -4.260901 -4.2719951 -4.2747025 -4.2681684 -4.2669139 -4.2625771 -4.2368226 -4.2159953 -4.2105641 -4.2209935 -4.2361765 -4.243207 -4.2479348 -4.2536335][-4.2356439 -4.23992 -4.2435813 -4.2440796 -4.2351418 -4.24553 -4.2621183 -4.2564917 -4.2484808 -4.2468948 -4.251688 -4.2551479 -4.2518306 -4.2486849 -4.2459464][-4.2339506 -4.2270622 -4.2161188 -4.2091861 -4.1972246 -4.21378 -4.2393894 -4.2441854 -4.2444963 -4.2462921 -4.2502518 -4.2515655 -4.2458043 -4.2377806 -4.2332563][-4.2432461 -4.2331305 -4.2155237 -4.2039213 -4.1929483 -4.2095184 -4.2322192 -4.237843 -4.2394066 -4.2418242 -4.2456217 -4.2466073 -4.2414026 -4.2333961 -4.2290215][-4.2669134 -4.2663083 -4.2534394 -4.2442141 -4.2345276 -4.2440562 -4.2561893 -4.2583241 -4.2578783 -4.2591033 -4.2626538 -4.2648935 -4.2622805 -4.2571983 -4.254178][-4.2829947 -4.2936163 -4.2907472 -4.2888618 -4.2835803 -4.2878547 -4.292522 -4.29236 -4.2907515 -4.2898321 -4.2907434 -4.2922864 -4.2924542 -4.291224 -4.2911639]]...]
INFO - root - 2017-12-05 14:16:20.567384: step 13910, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 83h:04m:58s remains)
INFO - root - 2017-12-05 14:16:29.598245: step 13920, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 81h:42m:33s remains)
INFO - root - 2017-12-05 14:16:38.806446: step 13930, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 80h:44m:30s remains)
INFO - root - 2017-12-05 14:16:48.009905: step 13940, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 82h:14m:00s remains)
INFO - root - 2017-12-05 14:16:57.027966: step 13950, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 79h:59m:21s remains)
INFO - root - 2017-12-05 14:17:06.108657: step 13960, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 76h:33m:29s remains)
INFO - root - 2017-12-05 14:17:15.118001: step 13970, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 82h:24m:05s remains)
INFO - root - 2017-12-05 14:17:24.198042: step 13980, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 81h:34m:55s remains)
INFO - root - 2017-12-05 14:17:33.183707: step 13990, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 83h:25m:03s remains)
INFO - root - 2017-12-05 14:17:42.336364: step 14000, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 80h:59m:42s remains)
2017-12-05 14:17:43.102495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2770848 -4.2737083 -4.2662988 -4.2653208 -4.2667265 -4.2631311 -4.2597966 -4.2616539 -4.2648716 -4.2620053 -4.2550974 -4.2508268 -4.2526164 -4.2562571 -4.2602873][-4.2561469 -4.2510433 -4.2418671 -4.2454195 -4.25133 -4.2486725 -4.2455649 -4.24439 -4.2445908 -4.2424579 -4.23626 -4.2327404 -4.2353139 -4.2380829 -4.2414742][-4.2164307 -4.2106338 -4.2026649 -4.2108064 -4.2211065 -4.218256 -4.2155838 -4.2146912 -4.2197685 -4.2231922 -4.2179413 -4.2113652 -4.2096033 -4.2129765 -4.2226844][-4.1677933 -4.15707 -4.1473551 -4.1562991 -4.1673784 -4.1603189 -4.1532359 -4.1576734 -4.1770287 -4.195488 -4.1993003 -4.1938019 -4.1901822 -4.1928248 -4.203464][-4.1350832 -4.1145487 -4.0929604 -4.0896025 -4.0916781 -4.0773244 -4.0633025 -4.0760336 -4.118607 -4.1625214 -4.1852007 -4.1890464 -4.1887422 -4.1923389 -4.1997085][-4.1432004 -4.1114268 -4.0726247 -4.0478277 -4.0287542 -3.9924531 -3.9582059 -3.9782517 -4.0541773 -4.1331 -4.177424 -4.1891189 -4.1933722 -4.2000604 -4.2028279][-4.1626992 -4.1321912 -4.0906968 -4.0493774 -4.0036054 -3.9338751 -3.86666 -3.8856742 -3.9939423 -4.1047082 -4.1666574 -4.1868644 -4.1966729 -4.2072105 -4.2073479][-4.1669669 -4.1474361 -4.1148033 -4.0793242 -4.0299926 -3.95876 -3.885726 -3.8914523 -3.9898052 -4.0937281 -4.1541295 -4.1795917 -4.1988893 -4.2158313 -4.2250962][-4.1586776 -4.1500149 -4.134635 -4.1166639 -4.0879569 -4.0532284 -4.0133333 -4.011241 -4.0589228 -4.1155639 -4.1464543 -4.1662722 -4.1919923 -4.2208605 -4.2419457][-4.1463928 -4.1381712 -4.1348419 -4.1341381 -4.1350846 -4.1401377 -4.1308575 -4.1277723 -4.1444588 -4.1656089 -4.1692114 -4.1742592 -4.1932931 -4.2225456 -4.2475777][-4.13612 -4.1220336 -4.1206703 -4.135716 -4.1639309 -4.1932955 -4.2007227 -4.2015743 -4.2087293 -4.2122974 -4.2077818 -4.2054362 -4.2130885 -4.2297583 -4.2477746][-4.1444874 -4.1226854 -4.1230845 -4.1489658 -4.1863093 -4.2166371 -4.2254448 -4.22879 -4.2344527 -4.2386422 -4.2415004 -4.243681 -4.24964 -4.2571878 -4.2636585][-4.1805806 -4.1655993 -4.1679153 -4.192071 -4.2214932 -4.2412605 -4.2415118 -4.2396379 -4.2431908 -4.2541032 -4.2677283 -4.2787495 -4.2858682 -4.2893987 -4.2881579][-4.2281928 -4.224061 -4.2267447 -4.2397246 -4.2566242 -4.2662644 -4.26044 -4.2506232 -4.250412 -4.2647066 -4.2844977 -4.2992148 -4.3072333 -4.3095226 -4.3067656][-4.2664051 -4.268178 -4.271668 -4.2798991 -4.2908154 -4.2971597 -4.2915144 -4.2787962 -4.2762184 -4.2856727 -4.3014946 -4.3144279 -4.3214831 -4.3203716 -4.3164787]]...]
INFO - root - 2017-12-05 14:17:52.268020: step 14010, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 81h:11m:32s remains)
INFO - root - 2017-12-05 14:18:01.438916: step 14020, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 81h:09m:35s remains)
INFO - root - 2017-12-05 14:18:10.551514: step 14030, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 78h:46m:56s remains)
INFO - root - 2017-12-05 14:18:19.752343: step 14040, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.954 sec/batch; 84h:25m:49s remains)
INFO - root - 2017-12-05 14:18:29.025625: step 14050, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 81h:03m:07s remains)
INFO - root - 2017-12-05 14:18:38.147859: step 14060, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 81h:17m:07s remains)
INFO - root - 2017-12-05 14:18:47.197879: step 14070, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 83h:10m:28s remains)
INFO - root - 2017-12-05 14:18:56.560069: step 14080, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 81h:26m:18s remains)
INFO - root - 2017-12-05 14:19:05.711844: step 14090, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 82h:04m:37s remains)
INFO - root - 2017-12-05 14:19:14.756027: step 14100, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 78h:30m:58s remains)
2017-12-05 14:19:15.621652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2504449 -4.25363 -4.2548857 -4.2555418 -4.2564812 -4.257772 -4.2630014 -4.2719603 -4.2801938 -4.2867885 -4.2902145 -4.2893481 -4.2852407 -4.2805977 -4.276567][-4.2512083 -4.2531328 -4.2522812 -4.2509131 -4.2516537 -4.2546968 -4.2616062 -4.2706003 -4.2769527 -4.2798896 -4.2795553 -4.2762251 -4.27125 -4.267622 -4.2665968][-4.2447205 -4.2460008 -4.2449284 -4.2437043 -4.2456889 -4.2496896 -4.2548409 -4.2590327 -4.2605286 -4.2600527 -4.2576046 -4.2530727 -4.2476773 -4.2453804 -4.24672][-4.245934 -4.2453771 -4.2436557 -4.2437754 -4.2480106 -4.2522655 -4.2538457 -4.2516751 -4.2477489 -4.2446256 -4.2418957 -4.2387433 -4.2352829 -4.2349348 -4.2372561][-4.2516236 -4.2485805 -4.2453361 -4.2472992 -4.2539754 -4.2588873 -4.2581911 -4.25132 -4.2435365 -4.2377992 -4.234755 -4.2329764 -4.2324595 -4.2343378 -4.2374063][-4.2602453 -4.2540379 -4.2491684 -4.2520642 -4.26018 -4.265698 -4.2640967 -4.2553039 -4.24594 -4.2385912 -4.2347016 -4.2332935 -4.2353024 -4.2396984 -4.2442207][-4.2641158 -4.2557716 -4.2503605 -4.2546997 -4.2651663 -4.2727804 -4.2723022 -4.26379 -4.2538962 -4.2447324 -4.2389364 -4.2362528 -4.239737 -4.2465024 -4.2524157][-4.2606015 -4.251492 -4.2464013 -4.2515626 -4.2631373 -4.2731338 -4.2751927 -4.2684903 -4.2586894 -4.2476726 -4.2391605 -4.2344594 -4.2377319 -4.2448492 -4.2503681][-4.2569928 -4.24811 -4.243618 -4.249011 -4.2604542 -4.2713881 -4.2749014 -4.26893 -4.2584944 -4.2445788 -4.2319903 -4.224082 -4.2252493 -4.2323318 -4.2381253][-4.2480178 -4.2397 -4.2361431 -4.2419305 -4.2536135 -4.2654042 -4.2709565 -4.2660909 -4.2545342 -4.2385588 -4.2232389 -4.2134404 -4.2127771 -4.2195415 -4.2256079][-4.236434 -4.2280722 -4.2245297 -4.2305045 -4.2426195 -4.2548742 -4.262579 -4.2601032 -4.2497182 -4.2353992 -4.2224822 -4.2141027 -4.212872 -4.2184868 -4.223927][-4.2226396 -4.2144966 -4.2105107 -4.2161469 -4.2285395 -4.2416058 -4.2514319 -4.2524276 -4.2469616 -4.2379322 -4.2286682 -4.2218151 -4.2205524 -4.2249084 -4.2294254][-4.2058868 -4.1969428 -4.1919208 -4.1966476 -4.2086825 -4.2227125 -4.23516 -4.240314 -4.2402058 -4.2360544 -4.2300177 -4.2248988 -4.2241635 -4.2281494 -4.2331357][-4.2048497 -4.1965394 -4.191195 -4.1940346 -4.2030373 -4.214406 -4.2255082 -4.232007 -4.23499 -4.2347379 -4.2324576 -4.2304921 -4.2304478 -4.2329316 -4.2363319][-4.2183619 -4.21077 -4.204927 -4.2043033 -4.2080054 -4.21493 -4.2230382 -4.2298207 -4.2362771 -4.2407594 -4.2424765 -4.24308 -4.2433991 -4.2430091 -4.2420521]]...]
INFO - root - 2017-12-05 14:19:24.709154: step 14110, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 81h:44m:55s remains)
INFO - root - 2017-12-05 14:19:33.995958: step 14120, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 81h:22m:45s remains)
INFO - root - 2017-12-05 14:19:43.211284: step 14130, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.914 sec/batch; 80h:51m:34s remains)
INFO - root - 2017-12-05 14:19:52.272062: step 14140, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 79h:55m:20s remains)
INFO - root - 2017-12-05 14:20:01.329447: step 14150, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 80h:21m:18s remains)
INFO - root - 2017-12-05 14:20:10.294201: step 14160, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 83h:27m:54s remains)
INFO - root - 2017-12-05 14:20:19.322965: step 14170, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 81h:36m:21s remains)
INFO - root - 2017-12-05 14:20:28.431230: step 14180, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 78h:11m:03s remains)
INFO - root - 2017-12-05 14:20:37.516573: step 14190, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 80h:36m:36s remains)
INFO - root - 2017-12-05 14:20:46.762952: step 14200, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 82h:00m:44s remains)
2017-12-05 14:20:47.519680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3313713 -4.3282681 -4.3287067 -4.3303232 -4.329793 -4.327086 -4.326241 -4.32664 -4.328599 -4.3289471 -4.329834 -4.3299861 -4.3304524 -4.3315291 -4.3340983][-4.3193026 -4.3143034 -4.3138194 -4.3133383 -4.3107371 -4.3088717 -4.3107247 -4.3127813 -4.3161292 -4.3188376 -4.3222361 -4.321177 -4.3213644 -4.3228779 -4.3269506][-4.2986989 -4.2926531 -4.2903476 -4.2873383 -4.284132 -4.2820029 -4.2843981 -4.285737 -4.2862878 -4.2879138 -4.2931914 -4.2949729 -4.2985396 -4.3035817 -4.3106122][-4.2702403 -4.2630997 -4.26083 -4.2575397 -4.2537928 -4.2501078 -4.2506676 -4.2502689 -4.2454867 -4.2417397 -4.2453904 -4.25069 -4.2616191 -4.2717891 -4.2809][-4.232717 -4.2232065 -4.2233114 -4.2217064 -4.2163486 -4.2092204 -4.2055216 -4.1997905 -4.1895776 -4.184639 -4.1928759 -4.2051449 -4.222178 -4.2364659 -4.24563][-4.2028875 -4.1909647 -4.1928782 -4.1892557 -4.1748724 -4.1590614 -4.1423678 -4.1209035 -4.0964375 -4.0970225 -4.123806 -4.1505895 -4.1744332 -4.192255 -4.2005739][-4.2007113 -4.1894765 -4.1917028 -4.1806641 -4.1513886 -4.1161022 -4.0754318 -4.0313396 -3.9882879 -3.9880714 -4.0337691 -4.0798087 -4.1094451 -4.1340976 -4.1478286][-4.216557 -4.2072434 -4.2097964 -4.1955185 -4.1590133 -4.1071439 -4.0410166 -3.9699397 -3.9056191 -3.8968954 -3.9521735 -4.0137477 -4.050355 -4.0843043 -4.1082454][-4.2258358 -4.2208214 -4.2248521 -4.2161961 -4.187675 -4.1372843 -4.0646377 -3.9824874 -3.9104912 -3.8918204 -3.9381931 -3.9981706 -4.0366278 -4.0748515 -4.1028156][-4.2197857 -4.2198834 -4.22689 -4.2268543 -4.2132573 -4.1806607 -4.1258717 -4.0607939 -4.00467 -3.9836304 -4.0076442 -4.0455551 -4.0736437 -4.1056371 -4.1255803][-4.2038221 -4.2065396 -4.2167983 -4.2239294 -4.2228065 -4.2116389 -4.1828365 -4.144268 -4.10995 -4.09521 -4.1021 -4.119061 -4.1340103 -4.1530066 -4.1568427][-4.193635 -4.1933303 -4.2016916 -4.21237 -4.2207336 -4.2252207 -4.2168765 -4.1991735 -4.1798811 -4.1743135 -4.1784515 -4.1873693 -4.1920195 -4.1958418 -4.1862774][-4.1971903 -4.1897922 -4.1916676 -4.2017918 -4.2129903 -4.2243357 -4.22344 -4.2144527 -4.2060366 -4.2094946 -4.217042 -4.2257876 -4.2256374 -4.219656 -4.2014532][-4.2113037 -4.1955018 -4.189651 -4.1954184 -4.2044029 -4.2148237 -4.2114539 -4.2007818 -4.1943536 -4.202714 -4.2126627 -4.2223368 -4.2254567 -4.2200313 -4.1982441][-4.2288671 -4.2081861 -4.1984315 -4.1990776 -4.2021585 -4.2059593 -4.1961975 -4.1811228 -4.17328 -4.1805716 -4.1920223 -4.2035861 -4.2128377 -4.2122421 -4.1886215]]...]
INFO - root - 2017-12-05 14:20:56.680339: step 14210, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 81h:30m:40s remains)
INFO - root - 2017-12-05 14:21:05.829201: step 14220, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 82h:21m:20s remains)
INFO - root - 2017-12-05 14:21:14.849691: step 14230, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.905 sec/batch; 80h:01m:39s remains)
INFO - root - 2017-12-05 14:21:23.841694: step 14240, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 78h:40m:45s remains)
INFO - root - 2017-12-05 14:21:32.872554: step 14250, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:13m:20s remains)
INFO - root - 2017-12-05 14:21:41.974858: step 14260, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 81h:58m:14s remains)
INFO - root - 2017-12-05 14:21:50.949532: step 14270, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 78h:07m:34s remains)
INFO - root - 2017-12-05 14:21:59.884765: step 14280, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 79h:29m:50s remains)
INFO - root - 2017-12-05 14:22:09.026048: step 14290, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 81h:42m:27s remains)
INFO - root - 2017-12-05 14:22:18.493117: step 14300, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 82h:31m:05s remains)
2017-12-05 14:22:19.279394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2851748 -4.2796288 -4.286612 -4.2881708 -4.2825255 -4.2764974 -4.282033 -4.2872019 -4.281249 -4.2725391 -4.2584505 -4.2337446 -4.2136092 -4.2069235 -4.2119861][-4.2922187 -4.288044 -4.2913294 -4.2881384 -4.2789836 -4.26844 -4.2711887 -4.277122 -4.2703872 -4.259624 -4.2436891 -4.2168159 -4.1933784 -4.1864872 -4.1943521][-4.2997284 -4.2960758 -4.2935915 -4.2842727 -4.2718978 -4.2527862 -4.2487655 -4.2536592 -4.2444115 -4.2332854 -4.2186689 -4.1937394 -4.1722474 -4.1745248 -4.1938457][-4.3034592 -4.2989879 -4.2904115 -4.2757359 -4.2593293 -4.2321081 -4.21788 -4.2192359 -4.2106748 -4.2013087 -4.1940465 -4.18062 -4.1719394 -4.1871037 -4.2169185][-4.3050194 -4.297 -4.2817516 -4.2614808 -4.238328 -4.201179 -4.1693168 -4.1602387 -4.1593294 -4.1657295 -4.178288 -4.186542 -4.1957855 -4.2200212 -4.2513089][-4.3065443 -4.2971373 -4.2784271 -4.25023 -4.2152357 -4.1579814 -4.0927057 -4.0641313 -4.0834713 -4.1243734 -4.166007 -4.1964531 -4.2218289 -4.2526374 -4.2844181][-4.30895 -4.3006067 -4.2805181 -4.2455225 -4.1947446 -4.1079569 -3.992579 -3.935406 -3.9921815 -4.0838671 -4.1535454 -4.1993747 -4.2374911 -4.2747526 -4.3059187][-4.3097572 -4.3014989 -4.2828288 -4.2461715 -4.1881766 -4.0815573 -3.9267859 -3.8408351 -3.9302132 -4.061852 -4.1506257 -4.2042761 -4.2477984 -4.2868652 -4.3154154][-4.3075728 -4.2991681 -4.2823052 -4.2502437 -4.1999273 -4.10842 -3.9716597 -3.8864179 -3.9618635 -4.0858831 -4.1695385 -4.2185755 -4.2577705 -4.2909274 -4.3121524][-4.303915 -4.2936134 -4.2771249 -4.2508163 -4.21245 -4.1507578 -4.0614834 -3.9987741 -4.0419192 -4.1307306 -4.1950316 -4.2348685 -4.2689109 -4.2960248 -4.3104706][-4.3007927 -4.2876806 -4.2698288 -4.2456 -4.2146363 -4.1738658 -4.11866 -4.0761747 -4.102139 -4.1659994 -4.2144923 -4.2473397 -4.2791214 -4.30527 -4.3143916][-4.3003755 -4.2847123 -4.2643952 -4.2374196 -4.206049 -4.171113 -4.130846 -4.1029768 -4.1276555 -4.1832337 -4.2264047 -4.2557049 -4.2862434 -4.3116331 -4.3172946][-4.3027573 -4.2863946 -4.2635112 -4.2323461 -4.1959167 -4.1564288 -4.1163864 -4.0974021 -4.1265965 -4.1827855 -4.2248945 -4.253037 -4.2809577 -4.3021336 -4.3048363][-4.3081908 -4.2954307 -4.272903 -4.2388549 -4.1989274 -4.1541204 -4.107903 -4.0908027 -4.1212678 -4.1788392 -4.2190638 -4.2433205 -4.2678962 -4.2841377 -4.28771][-4.313148 -4.3035741 -4.281198 -4.244792 -4.2040758 -4.158484 -4.1128635 -4.0996456 -4.1299691 -4.1846013 -4.2196875 -4.2372713 -4.2558985 -4.2680788 -4.27463]]...]
INFO - root - 2017-12-05 14:22:28.490436: step 14310, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 79h:17m:47s remains)
INFO - root - 2017-12-05 14:22:37.693890: step 14320, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 80h:13m:20s remains)
INFO - root - 2017-12-05 14:22:46.720696: step 14330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 80h:47m:03s remains)
INFO - root - 2017-12-05 14:22:55.894440: step 14340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 81h:32m:06s remains)
INFO - root - 2017-12-05 14:23:04.960607: step 14350, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 79h:56m:57s remains)
INFO - root - 2017-12-05 14:23:14.052088: step 14360, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 82h:23m:05s remains)
INFO - root - 2017-12-05 14:23:23.090601: step 14370, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 77h:19m:22s remains)
INFO - root - 2017-12-05 14:23:32.224115: step 14380, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 79h:28m:31s remains)
INFO - root - 2017-12-05 14:23:41.453794: step 14390, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 78h:01m:36s remains)
INFO - root - 2017-12-05 14:23:50.343179: step 14400, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 79h:13m:31s remains)
2017-12-05 14:23:51.155780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3072181 -4.2782445 -4.2559581 -4.2466 -4.2344036 -4.2218366 -4.2318516 -4.2550106 -4.2690597 -4.2732944 -4.2734289 -4.278616 -4.2803707 -4.2768464 -4.2663779][-4.3111 -4.2818351 -4.257 -4.2463322 -4.227499 -4.2015343 -4.1996064 -4.2217436 -4.2449083 -4.2593436 -4.267004 -4.2770791 -4.28117 -4.2774987 -4.263998][-4.3064632 -4.2748241 -4.2445974 -4.2277527 -4.20039 -4.161325 -4.1478062 -4.1723313 -4.21039 -4.2414985 -4.2569885 -4.2672825 -4.2689829 -4.2617307 -4.2442341][-4.2896152 -4.2538443 -4.2187052 -4.1969104 -4.1650367 -4.1141381 -4.0855832 -4.1087232 -4.1644783 -4.2132578 -4.2337942 -4.24058 -4.2362051 -4.2245741 -4.2014275][-4.2764835 -4.2384171 -4.2026134 -4.1821418 -4.1530218 -4.0953069 -4.0458164 -4.0561824 -4.123817 -4.1869349 -4.2095346 -4.2097926 -4.1971922 -4.1802449 -4.1519909][-4.2665582 -4.2263188 -4.1868925 -4.1631203 -4.12995 -4.0567489 -3.9738545 -3.9627357 -4.0477157 -4.1343884 -4.16997 -4.1702147 -4.1549582 -4.1353936 -4.1007547][-4.2509332 -4.2058334 -4.1580086 -4.120419 -4.0685344 -3.9697015 -3.8404818 -3.798785 -3.9064651 -4.030139 -4.0919981 -4.1063828 -4.102788 -4.0914941 -4.0599284][-4.2432456 -4.19852 -4.1522074 -4.1138735 -4.0609446 -3.9595182 -3.8103659 -3.7488976 -3.8523548 -3.9843206 -4.0595307 -4.0843024 -4.0875793 -4.0789752 -4.0489][-4.2465425 -4.2077742 -4.1713886 -4.1463714 -4.1131887 -4.0421853 -3.9275236 -3.8779454 -3.9545605 -4.0575509 -4.1214223 -4.1427751 -4.1413732 -4.1215806 -4.0820565][-4.2533522 -4.2184939 -4.1853309 -4.1641221 -4.1407628 -4.0934658 -4.0168438 -3.9856863 -4.0422072 -4.1153779 -4.1638174 -4.1847668 -4.1853285 -4.1659431 -4.1252513][-4.2593994 -4.2249107 -4.1916695 -4.1685848 -4.148334 -4.113719 -4.0617938 -4.0442405 -4.0851331 -4.135818 -4.1704144 -4.1885786 -4.1926203 -4.1795244 -4.1487789][-4.2597818 -4.2223697 -4.1842971 -4.1569543 -4.1368756 -4.1115732 -4.079906 -4.0749693 -4.10851 -4.1470809 -4.1735778 -4.1897178 -4.196373 -4.1874228 -4.1638665][-4.2605944 -4.2209506 -4.1782389 -4.1480036 -4.1302423 -4.1178784 -4.1105042 -4.11902 -4.1469417 -4.174108 -4.1893606 -4.1969023 -4.1994143 -4.1920414 -4.1755962][-4.2681813 -4.230763 -4.1899056 -4.1591415 -4.1444812 -4.144444 -4.15649 -4.1722379 -4.1913266 -4.20681 -4.2129211 -4.213964 -4.2133842 -4.2073874 -4.1949768][-4.2834849 -4.2522192 -4.2165785 -4.1873207 -4.175149 -4.1823421 -4.205441 -4.2244763 -4.2360449 -4.24206 -4.2428083 -4.24152 -4.2389402 -4.2325015 -4.2208962]]...]
INFO - root - 2017-12-05 14:24:00.418526: step 14410, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.952 sec/batch; 84h:09m:33s remains)
INFO - root - 2017-12-05 14:24:09.389433: step 14420, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 79h:42m:43s remains)
INFO - root - 2017-12-05 14:24:18.473866: step 14430, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 80h:07m:45s remains)
INFO - root - 2017-12-05 14:24:27.607029: step 14440, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 84h:23m:50s remains)
INFO - root - 2017-12-05 14:24:36.707115: step 14450, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 79h:15m:01s remains)
INFO - root - 2017-12-05 14:24:45.753180: step 14460, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 81h:10m:35s remains)
INFO - root - 2017-12-05 14:24:54.847855: step 14470, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 80h:26m:52s remains)
INFO - root - 2017-12-05 14:25:03.965654: step 14480, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 82h:54m:41s remains)
INFO - root - 2017-12-05 14:25:12.935702: step 14490, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 80h:16m:54s remains)
INFO - root - 2017-12-05 14:25:22.044311: step 14500, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 80h:26m:36s remains)
2017-12-05 14:25:22.848604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2049065 -4.2017765 -4.1868763 -4.1512575 -4.101665 -4.0604095 -4.0603027 -4.0950403 -4.1372733 -4.1647272 -4.1682134 -4.1760964 -4.1856527 -4.1708822 -4.1370716][-4.2352176 -4.2252922 -4.2052684 -4.168695 -4.1153975 -4.0619826 -4.0465584 -4.0731225 -4.1096983 -4.1311836 -4.1315947 -4.1414642 -4.1590514 -4.1514812 -4.1215811][-4.25095 -4.2355585 -4.2145648 -4.1850352 -4.1328444 -4.0692606 -4.0342722 -4.0440583 -4.0739031 -4.0907483 -4.0917311 -4.1074014 -4.1375656 -4.1392865 -4.1153445][-4.2507486 -4.2321205 -4.2129035 -4.1893587 -4.140008 -4.0728607 -4.0185866 -4.0119405 -4.0382891 -4.0571547 -4.0625706 -4.0850739 -4.1279025 -4.1395555 -4.1236563][-4.2442164 -4.2260971 -4.2067909 -4.186532 -4.1454463 -4.0752635 -4.00226 -3.9811559 -4.0113988 -4.036603 -4.0453415 -4.0695477 -4.1186047 -4.1412249 -4.1381011][-4.2331047 -4.217422 -4.1981459 -4.1776824 -4.14131 -4.0691109 -3.9775889 -3.9432626 -3.9914479 -4.03322 -4.0414443 -4.0566421 -4.1050205 -4.1360621 -4.1460738][-4.2130651 -4.2003016 -4.1797876 -4.154336 -4.1186061 -4.0482755 -3.9433372 -3.8991184 -3.9765708 -4.0451984 -4.0488 -4.0421667 -4.0787439 -4.1148748 -4.1308084][-4.1867304 -4.1778169 -4.1567411 -4.1274939 -4.0972338 -4.0348725 -3.9235024 -3.8657517 -3.9612765 -4.0535717 -4.0565457 -4.0301075 -4.0488858 -4.0849009 -4.1011868][-4.1729527 -4.1724896 -4.1585321 -4.132019 -4.1053987 -4.0515971 -3.9377646 -3.8602462 -3.9469693 -4.0457864 -4.0541158 -4.0186176 -4.0251031 -4.0567641 -4.0746894][-4.1791844 -4.1835184 -4.1768847 -4.1560879 -4.1322818 -4.0884447 -3.9838934 -3.8922918 -3.9476979 -4.0347767 -4.04583 -4.0138297 -4.019815 -4.0495305 -4.0713139][-4.1920133 -4.19488 -4.1890116 -4.1743374 -4.1553779 -4.1216435 -4.0369134 -3.9453254 -3.965802 -4.0317774 -4.0392613 -4.0183916 -4.0347075 -4.0649281 -4.0881634][-4.2073326 -4.2040758 -4.1943 -4.1819186 -4.1680274 -4.1419644 -4.0770292 -3.9962249 -3.9925668 -4.0371394 -4.0407572 -4.0327058 -4.0578036 -4.0848827 -4.1072063][-4.2254233 -4.2131934 -4.1934338 -4.1780877 -4.1683578 -4.1525393 -4.1065645 -4.0434914 -4.0287724 -4.0544605 -4.05493 -4.0562034 -4.0842991 -4.10705 -4.1270289][-4.243969 -4.2245603 -4.1940484 -4.1716866 -4.1642952 -4.1607037 -4.1361322 -4.092783 -4.0742116 -4.0827932 -4.0761571 -4.0772514 -4.1035256 -4.1264563 -4.1477222][-4.2594318 -4.2392092 -4.2051616 -4.1757 -4.1650853 -4.1660833 -4.1585145 -4.1332464 -4.115448 -4.1090212 -4.0917196 -4.0832624 -4.1049089 -4.1352658 -4.1629276]]...]
INFO - root - 2017-12-05 14:25:31.997380: step 14510, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 80h:57m:46s remains)
INFO - root - 2017-12-05 14:25:41.113405: step 14520, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 77h:58m:07s remains)
INFO - root - 2017-12-05 14:25:50.016713: step 14530, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 78h:13m:57s remains)
INFO - root - 2017-12-05 14:25:59.099508: step 14540, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 82h:30m:31s remains)
INFO - root - 2017-12-05 14:26:08.181045: step 14550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 82h:08m:10s remains)
INFO - root - 2017-12-05 14:26:17.192104: step 14560, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 79h:50m:28s remains)
INFO - root - 2017-12-05 14:26:26.523705: step 14570, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 84h:12m:33s remains)
INFO - root - 2017-12-05 14:26:35.637104: step 14580, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 83h:49m:10s remains)
INFO - root - 2017-12-05 14:26:44.728301: step 14590, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 80h:29m:35s remains)
INFO - root - 2017-12-05 14:26:53.723447: step 14600, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 77h:16m:55s remains)
2017-12-05 14:26:54.504247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2313728 -4.1768489 -4.1410151 -4.1295223 -4.1595774 -4.2143617 -4.2666707 -4.3048029 -4.3192835 -4.3266692 -4.3376508 -4.347703 -4.3504376 -4.3446279 -4.3255916][-4.2706704 -4.21523 -4.1682978 -4.1424475 -4.1580181 -4.2013474 -4.2477536 -4.2823663 -4.2991853 -4.3116035 -4.3267226 -4.3401351 -4.3445697 -4.3413744 -4.3324623][-4.2915492 -4.2391663 -4.1849632 -4.1475487 -4.1462207 -4.1730967 -4.2123475 -4.2487888 -4.2744937 -4.2947178 -4.3132815 -4.3280778 -4.3347592 -4.33337 -4.3294969][-4.3003545 -4.2514153 -4.191442 -4.1413345 -4.1189222 -4.1231976 -4.1543574 -4.1997948 -4.2412143 -4.272131 -4.2940774 -4.3098369 -4.3209643 -4.3252792 -4.3276262][-4.3004231 -4.24947 -4.1783037 -4.1103096 -4.0672808 -4.0493441 -4.0685983 -4.12796 -4.1939192 -4.2410703 -4.2674241 -4.2831211 -4.2988677 -4.3127031 -4.3245697][-4.293097 -4.2349062 -4.1511736 -4.071413 -4.0166554 -3.980046 -3.978467 -4.0457835 -4.1367197 -4.2013736 -4.2287984 -4.2415261 -4.2622347 -4.2883072 -4.3131347][-4.2821436 -4.2157159 -4.1251473 -4.0442228 -3.984822 -3.929369 -3.8981054 -3.959933 -4.0683293 -4.1467543 -4.1774683 -4.1929169 -4.2221913 -4.2618861 -4.2981329][-4.2731881 -4.1970663 -4.1027927 -4.0258965 -3.9684823 -3.9062188 -3.8562095 -3.8958554 -4.000505 -4.0817232 -4.1177726 -4.1439347 -4.1855936 -4.2391844 -4.2858667][-4.2741275 -4.1937747 -4.1004438 -4.0244946 -3.9682729 -3.9115512 -3.8637068 -3.8761292 -3.9477131 -4.01389 -4.0538058 -4.0937514 -4.1500754 -4.2159777 -4.2735667][-4.2838688 -4.2064114 -4.1153526 -4.0372229 -3.9759121 -3.9220459 -3.883868 -3.8815377 -3.9177921 -3.9571776 -3.9938982 -4.0446982 -4.113452 -4.1904774 -4.2576909][-4.3055058 -4.2429562 -4.163281 -4.089623 -4.0270729 -3.97417 -3.9392476 -3.9272664 -3.9334505 -3.938725 -3.9569876 -4.0028067 -4.0753579 -4.1580906 -4.2319736][-4.3287807 -4.2869892 -4.2287569 -4.1720853 -4.1189466 -4.070684 -4.0358844 -4.0149474 -3.9984915 -3.9743721 -3.9644084 -3.9889042 -4.0524578 -4.1356382 -4.2124567][-4.345562 -4.3221354 -4.2865319 -4.2503777 -4.2142119 -4.1805134 -4.1532288 -4.1318412 -4.1073413 -4.073113 -4.0461249 -4.0477509 -4.0903645 -4.1595817 -4.2252169][-4.3567705 -4.3445764 -4.3253751 -4.3067675 -4.28806 -4.2703185 -4.2560658 -4.2434826 -4.224577 -4.1958241 -4.1691914 -4.1610985 -4.1832232 -4.2290025 -4.2727022][-4.3642483 -4.3579884 -4.3477826 -4.339673 -4.3330097 -4.3257818 -4.321826 -4.317615 -4.3083224 -4.2920442 -4.275147 -4.2671895 -4.2767444 -4.3011231 -4.3237333]]...]
INFO - root - 2017-12-05 14:27:03.841900: step 14610, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 81h:18m:06s remains)
INFO - root - 2017-12-05 14:27:12.960378: step 14620, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 80h:30m:14s remains)
INFO - root - 2017-12-05 14:27:21.893248: step 14630, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 0.783 sec/batch; 69h:05m:52s remains)
INFO - root - 2017-12-05 14:27:30.968029: step 14640, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 84h:42m:29s remains)
INFO - root - 2017-12-05 14:27:40.126366: step 14650, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 82h:56m:54s remains)
INFO - root - 2017-12-05 14:27:49.199304: step 14660, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.900 sec/batch; 79h:29m:06s remains)
INFO - root - 2017-12-05 14:27:58.231214: step 14670, loss = 2.06, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 85h:28m:15s remains)
INFO - root - 2017-12-05 14:28:07.317757: step 14680, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 76h:54m:46s remains)
INFO - root - 2017-12-05 14:28:16.549468: step 14690, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 80h:04m:28s remains)
INFO - root - 2017-12-05 14:28:25.729918: step 14700, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 82h:42m:47s remains)
2017-12-05 14:28:26.513272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3190808 -4.3137217 -4.305171 -4.299356 -4.2914863 -4.2864361 -4.2888541 -4.291697 -4.2867379 -4.2758474 -4.269486 -4.27328 -4.275105 -4.2789445 -4.2847281][-4.2878442 -4.2787948 -4.2717524 -4.2708 -4.27061 -4.2697949 -4.2712731 -4.2703757 -4.2629089 -4.24932 -4.2415481 -4.2429047 -4.2443857 -4.2480464 -4.251833][-4.2386589 -4.2290316 -4.2215018 -4.2233691 -4.23154 -4.2363648 -4.2358413 -4.2288613 -4.2178364 -4.2021694 -4.1913724 -4.1880851 -4.1865993 -4.1878552 -4.1896181][-4.1784086 -4.1607466 -4.1463475 -4.144733 -4.155313 -4.1613936 -4.1607742 -4.1529355 -4.1393452 -4.1204925 -4.1061945 -4.09705 -4.0903158 -4.0894442 -4.0889893][-4.106967 -4.0713429 -4.0440416 -4.0358019 -4.0476651 -4.0559654 -4.0552683 -4.049438 -4.0353904 -4.0161433 -4.000206 -3.9831247 -3.9680543 -3.9627178 -3.9591823][-4.0512309 -4.0009522 -3.9609933 -3.9459951 -3.9562995 -3.962775 -3.9598546 -3.955348 -3.9450848 -3.933429 -3.9218435 -3.9029288 -3.8867013 -3.8811686 -3.8776245][-4.0711532 -4.0220871 -3.9792807 -3.9591014 -3.9639983 -3.9644816 -3.9568176 -3.9539502 -3.9541695 -3.9595447 -3.9628198 -3.9538887 -3.9444354 -3.9410677 -3.9377139][-4.1474462 -4.11045 -4.0744271 -4.0510349 -4.0476689 -4.0375705 -4.019558 -4.0140839 -4.0247617 -4.0460672 -4.0671277 -4.0752974 -4.0770969 -4.0785136 -4.0757232][-4.196599 -4.1686912 -4.1389208 -4.1158195 -4.1083035 -4.093504 -4.0712657 -4.0619 -4.0738778 -4.0994744 -4.1286292 -4.1485028 -4.159184 -4.1635675 -4.161459][-4.2086968 -4.1840086 -4.1566658 -4.1382465 -4.1337185 -4.1228 -4.1062689 -4.0966611 -4.1042933 -4.1263485 -4.1549444 -4.1766658 -4.18875 -4.193687 -4.1920137][-4.2087374 -4.184783 -4.1577473 -4.1437616 -4.145112 -4.1414371 -4.1344051 -4.1274 -4.1274338 -4.1407623 -4.1629062 -4.1800709 -4.1880593 -4.1889086 -4.1847482][-4.216526 -4.1928749 -4.1642432 -4.1510048 -4.1555452 -4.156415 -4.1562243 -4.1515484 -4.1452007 -4.147666 -4.1585112 -4.1675544 -4.1701884 -4.169291 -4.163692][-4.2293148 -4.20782 -4.1783304 -4.1630335 -4.1660156 -4.1644869 -4.162611 -4.15547 -4.14256 -4.1361947 -4.137382 -4.1402955 -4.1421466 -4.1449771 -4.1422486][-4.2410321 -4.2208352 -4.1909556 -4.174675 -4.1756797 -4.170094 -4.1641121 -4.1552825 -4.141449 -4.1327543 -4.1311407 -4.13208 -4.1352115 -4.1408744 -4.1394796][-4.2496109 -4.2268553 -4.1974993 -4.1807213 -4.1816292 -4.1763945 -4.1695318 -4.1625657 -4.153089 -4.146069 -4.1435118 -4.143889 -4.1482348 -4.1559157 -4.1580024]]...]
INFO - root - 2017-12-05 14:28:35.507081: step 14710, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 81h:05m:06s remains)
INFO - root - 2017-12-05 14:28:44.634024: step 14720, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 82h:42m:19s remains)
INFO - root - 2017-12-05 14:28:53.546090: step 14730, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 78h:59m:35s remains)
INFO - root - 2017-12-05 14:29:02.695518: step 14740, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 83h:13m:48s remains)
INFO - root - 2017-12-05 14:29:11.937144: step 14750, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 80h:47m:15s remains)
INFO - root - 2017-12-05 14:29:20.795621: step 14760, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 77h:45m:55s remains)
INFO - root - 2017-12-05 14:29:29.855531: step 14770, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 81h:59m:03s remains)
INFO - root - 2017-12-05 14:29:38.890796: step 14780, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 79h:16m:39s remains)
INFO - root - 2017-12-05 14:29:47.835452: step 14790, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 80h:36m:53s remains)
INFO - root - 2017-12-05 14:29:56.933309: step 14800, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 78h:40m:17s remains)
2017-12-05 14:29:57.750243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0772076 -4.1624179 -4.2347417 -4.27529 -4.29679 -4.3023729 -4.3020039 -4.2998085 -4.2928147 -4.2858143 -4.2781529 -4.2754531 -4.2801661 -4.2853374 -4.2910652][-4.0689373 -4.1547732 -4.2307706 -4.2755718 -4.2962289 -4.3006692 -4.3024955 -4.3075027 -4.3142591 -4.3229594 -4.3287344 -4.3338752 -4.3394327 -4.3421464 -4.3411369][-4.0735164 -4.1521182 -4.2243252 -4.2699413 -4.2900457 -4.2951107 -4.2988405 -4.3086147 -4.3221736 -4.3379126 -4.3488283 -4.3551693 -4.358922 -4.3604236 -4.3557444][-4.1100469 -4.1755872 -4.2344608 -4.2708282 -4.2820773 -4.2799206 -4.2823358 -4.2938256 -4.3108149 -4.3276992 -4.3399582 -4.3486123 -4.3550429 -4.3587265 -4.3521881][-4.1435642 -4.1951427 -4.2308788 -4.247448 -4.2421346 -4.2283068 -4.2284718 -4.2393932 -4.2566633 -4.2783165 -4.2972403 -4.3178477 -4.3374577 -4.3499551 -4.34558][-4.1405821 -4.1774163 -4.1919112 -4.18509 -4.1616325 -4.135118 -4.13127 -4.1372952 -4.1505303 -4.1832552 -4.2182889 -4.2603421 -4.3008037 -4.3284426 -4.3345056][-4.1061811 -4.138607 -4.1424279 -4.1197834 -4.0819678 -4.0387135 -4.0158453 -4.0042758 -4.0062265 -4.0538898 -4.1152778 -4.1835113 -4.2489114 -4.2972016 -4.3239264][-4.0775995 -4.1070967 -4.1030025 -4.0713921 -4.0219922 -3.9662194 -3.9117332 -3.8540664 -3.8281631 -3.8996787 -3.9962556 -4.0929656 -4.181149 -4.2488179 -4.2989769][-4.0844088 -4.1122074 -4.109375 -4.0765057 -4.020946 -3.9572437 -3.8766229 -3.7768526 -3.7241135 -3.8116755 -3.9241772 -4.0293775 -4.1216545 -4.1942515 -4.2570033][-4.1252627 -4.1538029 -4.1612639 -4.1416073 -4.0951056 -4.0360518 -3.9585876 -3.8695836 -3.8252277 -3.881232 -3.9534605 -4.0238433 -4.090764 -4.1526756 -4.2162824][-4.2122297 -4.2317834 -4.24014 -4.225738 -4.184166 -4.131669 -4.0655279 -3.9995556 -3.9696574 -3.9921377 -4.0176921 -4.0442672 -4.0768104 -4.1171136 -4.1732454][-4.2847986 -4.3034239 -4.3109045 -4.293828 -4.2519259 -4.2032661 -4.1499534 -4.1066446 -4.0941591 -4.1031575 -4.1068125 -4.1088753 -4.1102324 -4.1232681 -4.1553245][-4.3242617 -4.3446603 -4.3533063 -4.3381977 -4.3028393 -4.2636318 -4.228333 -4.2034321 -4.2030244 -4.2096572 -4.2116513 -4.2093196 -4.1979203 -4.190557 -4.1910057][-4.3416781 -4.3598738 -4.3683615 -4.3569036 -4.3273678 -4.2967272 -4.2770638 -4.2670994 -4.2762728 -4.28892 -4.2988029 -4.3005691 -4.2890286 -4.2735305 -4.2526884][-4.3407373 -4.35521 -4.3648443 -4.3601904 -4.3378987 -4.312407 -4.2956662 -4.2890658 -4.3027954 -4.3250217 -4.3465853 -4.3591838 -4.357728 -4.3442059 -4.3170071]]...]
INFO - root - 2017-12-05 14:30:06.765581: step 14810, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 72h:05m:49s remains)
INFO - root - 2017-12-05 14:30:15.761677: step 14820, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.949 sec/batch; 83h:46m:23s remains)
INFO - root - 2017-12-05 14:30:24.875437: step 14830, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 80h:32m:52s remains)
INFO - root - 2017-12-05 14:30:34.082479: step 14840, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 81h:23m:53s remains)
INFO - root - 2017-12-05 14:30:43.196065: step 14850, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 80h:46m:10s remains)
INFO - root - 2017-12-05 14:30:52.212916: step 14860, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 78h:10m:45s remains)
INFO - root - 2017-12-05 14:31:01.257141: step 14870, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 81h:04m:52s remains)
INFO - root - 2017-12-05 14:31:10.364506: step 14880, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.916 sec/batch; 80h:51m:07s remains)
INFO - root - 2017-12-05 14:31:19.309513: step 14890, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 80h:01m:14s remains)
INFO - root - 2017-12-05 14:31:28.258792: step 14900, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 78h:36m:05s remains)
2017-12-05 14:31:29.072082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3400178 -4.3411436 -4.3401423 -4.3407545 -4.3416953 -4.3429136 -4.343606 -4.3435659 -4.3430257 -4.3423238 -4.3413205 -4.3407845 -4.3406925 -4.3408656 -4.3422003][-4.3351011 -4.3345661 -4.3308582 -4.3311434 -4.3337164 -4.3363538 -4.3380961 -4.3384237 -4.3382463 -4.3385687 -4.3385735 -4.338933 -4.338748 -4.3379331 -4.3380775][-4.3212566 -4.3200808 -4.3131676 -4.3134942 -4.3188467 -4.3222575 -4.3247142 -4.3248014 -4.3243861 -4.32727 -4.3297448 -4.3315344 -4.330327 -4.3266344 -4.3244033][-4.2991853 -4.2989426 -4.2869687 -4.2857447 -4.29245 -4.2966075 -4.3010826 -4.30163 -4.3023372 -4.3086557 -4.3137064 -4.3183274 -4.3154964 -4.3055673 -4.2974954][-4.2652006 -4.2660689 -4.25238 -4.2511225 -4.2595863 -4.2663617 -4.2728496 -4.2733517 -4.2749686 -4.2856135 -4.2947693 -4.3000689 -4.2907557 -4.2713475 -4.2568707][-4.2238688 -4.2249465 -4.2109928 -4.2103944 -4.222816 -4.2319193 -4.2389321 -4.2392707 -4.2423644 -4.2573032 -4.2710786 -4.2762403 -4.260004 -4.2309952 -4.2095962][-4.18069 -4.1777515 -4.1633091 -4.1684275 -4.189992 -4.2036805 -4.2111492 -4.2104788 -4.2119718 -4.2275019 -4.2441473 -4.2484331 -4.2280235 -4.1921058 -4.1646442][-4.1509676 -4.1447058 -4.1307135 -4.144001 -4.1744108 -4.1905994 -4.1968665 -4.1953735 -4.1959925 -4.2113028 -4.2292347 -4.2320876 -4.2093363 -4.1719022 -4.1430154][-4.1533866 -4.144187 -4.1284533 -4.1454253 -4.17839 -4.1930223 -4.1957541 -4.1916671 -4.1903028 -4.2021809 -4.2185135 -4.2196922 -4.1984382 -4.1680255 -4.1470966][-4.1669216 -4.1555424 -4.1389475 -4.1561747 -4.1888642 -4.2022848 -4.20366 -4.1985011 -4.1937642 -4.1990309 -4.2110929 -4.2104788 -4.1956873 -4.177464 -4.1678643][-4.1895328 -4.1736231 -4.150887 -4.1652207 -4.1983805 -4.214057 -4.2173567 -4.2153215 -4.2088842 -4.206533 -4.2099481 -4.2061467 -4.1990294 -4.1924982 -4.1909251][-4.2165542 -4.19516 -4.1615143 -4.1687341 -4.2015967 -4.2230563 -4.2331309 -4.2368011 -4.2301688 -4.221015 -4.2148881 -4.2067595 -4.2043409 -4.2061024 -4.2092576][-4.2229514 -4.200242 -4.1611776 -4.1615281 -4.1945219 -4.2241678 -4.2431507 -4.2528486 -4.2483015 -4.2364178 -4.2234163 -4.2111359 -4.2101221 -4.2175379 -4.2271194][-4.2259068 -4.2039943 -4.16513 -4.1630492 -4.1969538 -4.2331114 -4.2597761 -4.2744336 -4.2713857 -4.2572832 -4.2397504 -4.2251511 -4.2268071 -4.2399697 -4.253387][-4.237709 -4.218224 -4.1827025 -4.178369 -4.2109761 -4.2510147 -4.2820754 -4.300168 -4.2986455 -4.2829938 -4.262722 -4.2474685 -4.2521682 -4.2698088 -4.2864628]]...]
INFO - root - 2017-12-05 14:31:38.061907: step 14910, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 78h:59m:38s remains)
INFO - root - 2017-12-05 14:31:47.197505: step 14920, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.877 sec/batch; 77h:23m:16s remains)
INFO - root - 2017-12-05 14:31:56.331168: step 14930, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 80h:33m:52s remains)
INFO - root - 2017-12-05 14:32:05.461323: step 14940, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 80h:12m:40s remains)
INFO - root - 2017-12-05 14:32:14.532853: step 14950, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 80h:35m:58s remains)
INFO - root - 2017-12-05 14:32:23.666091: step 14960, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 80h:10m:29s remains)
INFO - root - 2017-12-05 14:32:32.788562: step 14970, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 79h:22m:49s remains)
INFO - root - 2017-12-05 14:32:41.860268: step 14980, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 79h:23m:48s remains)
INFO - root - 2017-12-05 14:32:50.845182: step 14990, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 79h:11m:05s remains)
INFO - root - 2017-12-05 14:32:59.852794: step 15000, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.876 sec/batch; 77h:13m:14s remains)
2017-12-05 14:33:00.660786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2093334 -4.2301788 -4.2397079 -4.2455921 -4.2415652 -4.2107286 -4.1591563 -4.0902872 -4.0299978 -4.0286355 -4.0876322 -4.1658373 -4.245276 -4.2990956 -4.3195028][-4.1988783 -4.2229891 -4.23571 -4.2441821 -4.2430439 -4.2148466 -4.1728878 -4.1272187 -4.0906763 -4.0895786 -4.1380758 -4.2068887 -4.2770071 -4.3224044 -4.3355718][-4.179565 -4.2104559 -4.233707 -4.244957 -4.2407317 -4.2092776 -4.1671991 -4.1330137 -4.1111503 -4.115314 -4.160624 -4.2243853 -4.2893643 -4.3312025 -4.3429613][-4.1527123 -4.1921186 -4.22555 -4.24085 -4.23011 -4.1833181 -4.123364 -4.0860124 -4.0793676 -4.1005917 -4.1522808 -4.2179246 -4.2824116 -4.3249359 -4.3398232][-4.1252637 -4.16655 -4.2022419 -4.2158241 -4.19697 -4.12571 -4.0267186 -3.9709737 -3.9920788 -4.0520558 -4.1219597 -4.19576 -4.2664528 -4.3130908 -4.3302021][-4.1046729 -4.132504 -4.1563249 -4.1616311 -4.13701 -4.0444355 -3.8943343 -3.8038414 -3.863781 -3.977854 -4.0732994 -4.161242 -4.2447782 -4.3008909 -4.3214273][-4.1285658 -4.1363597 -4.1332712 -4.1215644 -4.0962114 -4.0033383 -3.8202522 -3.685257 -3.7678864 -3.9249263 -4.0403256 -4.1348724 -4.2285571 -4.2923408 -4.3166876][-4.1931391 -4.1847186 -4.1560946 -4.1297212 -4.1145945 -4.05401 -3.9037378 -3.7707033 -3.8192453 -3.9558954 -4.0643153 -4.1510329 -4.2383533 -4.297266 -4.3175669][-4.2481713 -4.2309394 -4.1910543 -4.159884 -4.1556106 -4.1293974 -4.0379152 -3.9413552 -3.9519825 -4.0340033 -4.1176252 -4.194582 -4.2685275 -4.3128419 -4.3224406][-4.2903047 -4.2693872 -4.2293472 -4.1976762 -4.1913834 -4.1814551 -4.1346655 -4.0782857 -4.0711951 -4.1081743 -4.1685748 -4.2380219 -4.3008375 -4.3290629 -4.3282018][-4.3167286 -4.3002796 -4.268229 -4.2398386 -4.2307944 -4.2238975 -4.200901 -4.1715822 -4.1592951 -4.1707587 -4.2125468 -4.2730279 -4.3253336 -4.3407617 -4.3334341][-4.3251247 -4.32035 -4.3012304 -4.2779956 -4.2677217 -4.2611747 -4.2499971 -4.2351122 -4.22454 -4.229022 -4.2578387 -4.3028932 -4.3406978 -4.3467393 -4.3353062][-4.3176408 -4.323236 -4.316741 -4.3009653 -4.2925515 -4.2887735 -4.2846956 -4.2753243 -4.2680259 -4.2738047 -4.2969651 -4.3279929 -4.3503103 -4.3484473 -4.3343563][-4.3149271 -4.3222156 -4.3222661 -4.3145185 -4.3089104 -4.3073497 -4.3062587 -4.2992191 -4.2940788 -4.3029995 -4.3202991 -4.3400688 -4.3506265 -4.343574 -4.3299165][-4.3123269 -4.3174644 -4.3203893 -4.3180566 -4.313406 -4.3114886 -4.3099408 -4.3037248 -4.3003092 -4.3100181 -4.3245034 -4.3374686 -4.3404241 -4.3326607 -4.3247461]]...]
INFO - root - 2017-12-05 14:33:09.649867: step 15010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 80h:12m:03s remains)
INFO - root - 2017-12-05 14:33:18.824123: step 15020, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 83h:02m:53s remains)
INFO - root - 2017-12-05 14:33:28.115655: step 15030, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 81h:23m:37s remains)
INFO - root - 2017-12-05 14:33:37.189583: step 15040, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 77h:58m:07s remains)
INFO - root - 2017-12-05 14:33:46.143285: step 15050, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 77h:22m:01s remains)
INFO - root - 2017-12-05 14:33:55.186192: step 15060, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 80h:10m:28s remains)
INFO - root - 2017-12-05 14:34:04.486578: step 15070, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 80h:21m:25s remains)
INFO - root - 2017-12-05 14:34:13.592802: step 15080, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 79h:47m:13s remains)
INFO - root - 2017-12-05 14:34:22.540966: step 15090, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.808 sec/batch; 71h:15m:25s remains)
INFO - root - 2017-12-05 14:34:31.554753: step 15100, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 71h:44m:21s remains)
2017-12-05 14:34:32.372618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2995596 -4.2811284 -4.2608957 -4.2428765 -4.2212043 -4.1957 -4.1749969 -4.1737227 -4.2012014 -4.2363782 -4.2580776 -4.265573 -4.2651982 -4.259532 -4.2479382][-4.2790623 -4.2584176 -4.2372122 -4.2215896 -4.1992936 -4.1583424 -4.1191211 -4.1157775 -4.1568985 -4.2073574 -4.241076 -4.254601 -4.2525859 -4.2434635 -4.2290525][-4.2535405 -4.2323265 -4.2115836 -4.1940374 -4.1698489 -4.1206765 -4.0615368 -4.0511894 -4.1032195 -4.1701984 -4.2145433 -4.2392349 -4.2451105 -4.2379179 -4.2230439][-4.2237148 -4.2000008 -4.1754556 -4.1516252 -4.1227927 -4.0674214 -3.9894414 -3.9595594 -4.0209589 -4.1099267 -4.17262 -4.2136602 -4.2342477 -4.2375817 -4.2276454][-4.1995354 -4.1728539 -4.1442213 -4.1151347 -4.08003 -4.0143476 -3.918925 -3.8643792 -3.9253287 -4.0378051 -4.126524 -4.1866388 -4.2187719 -4.2344742 -4.2327347][-4.1868467 -4.1597738 -4.1318269 -4.10212 -4.0599623 -3.9839017 -3.8755412 -3.8008099 -3.8501029 -3.9754286 -4.083498 -4.1575012 -4.1975775 -4.2210207 -4.2286806][-4.1828108 -4.1607027 -4.1386337 -4.1135678 -4.0680747 -3.984756 -3.8725004 -3.7920852 -3.8233628 -3.9343579 -4.0448904 -4.1236677 -4.1689382 -4.2007332 -4.2181053][-4.1854281 -4.1758862 -4.1654816 -4.1532984 -4.1127315 -4.0357556 -3.9382181 -3.8737829 -3.889533 -3.9610894 -4.0472736 -4.1183782 -4.1620603 -4.1962 -4.2182007][-4.1937466 -4.195972 -4.1938295 -4.1905704 -4.1572967 -4.0936346 -4.0205154 -3.981601 -3.9914045 -4.0302219 -4.0936294 -4.1499653 -4.1867657 -4.2155266 -4.23384][-4.2053 -4.2154551 -4.2149692 -4.21122 -4.184185 -4.1300187 -4.0762796 -4.0589886 -4.072135 -4.0981069 -4.1475768 -4.1923189 -4.2221708 -4.2455554 -4.2598515][-4.2171812 -4.22849 -4.2268705 -4.2264333 -4.2093287 -4.1603794 -4.1137109 -4.1058111 -4.1239591 -4.147563 -4.1877804 -4.2259893 -4.2541642 -4.2753129 -4.2847972][-4.225935 -4.23486 -4.234076 -4.2390714 -4.2305293 -4.1839585 -4.144053 -4.1442647 -4.167613 -4.1867151 -4.21508 -4.2498875 -4.2806206 -4.3010445 -4.306057][-4.2265725 -4.23463 -4.2352562 -4.2397647 -4.2337408 -4.1970048 -4.1712642 -4.1780844 -4.1996155 -4.2137985 -4.2350264 -4.2672319 -4.2977419 -4.3148341 -4.3155737][-4.2298021 -4.235496 -4.235858 -4.2372332 -4.2323794 -4.2072639 -4.1945252 -4.2033277 -4.2198749 -4.2349949 -4.2559743 -4.2807508 -4.3014526 -4.3119488 -4.3105812][-4.2511449 -4.250926 -4.2480116 -4.2447653 -4.2380271 -4.2235446 -4.216743 -4.2225289 -4.2367 -4.2545981 -4.275351 -4.2922783 -4.3026819 -4.3069 -4.3059883]]...]
INFO - root - 2017-12-05 14:34:41.373178: step 15110, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 80h:37m:18s remains)
INFO - root - 2017-12-05 14:34:50.352863: step 15120, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 78h:35m:55s remains)
INFO - root - 2017-12-05 14:34:59.592361: step 15130, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 80h:34m:23s remains)
INFO - root - 2017-12-05 14:35:08.567653: step 15140, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 78h:08m:26s remains)
INFO - root - 2017-12-05 14:35:17.747918: step 15150, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 82h:13m:15s remains)
INFO - root - 2017-12-05 14:35:26.809547: step 15160, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 80h:01m:32s remains)
INFO - root - 2017-12-05 14:35:35.824967: step 15170, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 81h:52m:49s remains)
INFO - root - 2017-12-05 14:35:45.098332: step 15180, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.885 sec/batch; 77h:57m:53s remains)
INFO - root - 2017-12-05 14:35:54.025097: step 15190, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 77h:01m:16s remains)
INFO - root - 2017-12-05 14:36:02.997366: step 15200, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.900 sec/batch; 79h:21m:43s remains)
2017-12-05 14:36:03.778274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1846175 -4.187789 -4.1855264 -4.1809368 -4.1781073 -4.1844735 -4.2027941 -4.2277331 -4.251123 -4.2575836 -4.2464018 -4.2324638 -4.2338619 -4.2500348 -4.26363][-4.1797228 -4.1837769 -4.1779089 -4.1730785 -4.1737881 -4.18367 -4.2078567 -4.23916 -4.2647152 -4.2697906 -4.2562118 -4.2401161 -4.2370329 -4.2426863 -4.2434568][-4.1765141 -4.1750546 -4.1632504 -4.1585007 -4.1643171 -4.1799159 -4.2079964 -4.2402053 -4.2631717 -4.2687407 -4.262475 -4.2546792 -4.2483177 -4.2381392 -4.2224126][-4.1697764 -4.1645026 -4.1529918 -4.1515994 -4.1639934 -4.1822677 -4.2054033 -4.2294097 -4.2433672 -4.2490888 -4.2563787 -4.2627387 -4.2545819 -4.2320514 -4.2070036][-4.1665049 -4.1631784 -4.16122 -4.1686034 -4.1832023 -4.19373 -4.1996641 -4.2049532 -4.2065611 -4.2143173 -4.2363443 -4.2532887 -4.2448468 -4.2195659 -4.1947803][-4.1702113 -4.1710954 -4.1805043 -4.1943326 -4.2029691 -4.1956058 -4.1745758 -4.1540442 -4.1439786 -4.1574354 -4.1932182 -4.221396 -4.2217188 -4.2066641 -4.1914597][-4.1724696 -4.1710577 -4.1851525 -4.2020416 -4.2055583 -4.180366 -4.1296544 -4.0838876 -4.0700765 -4.0953908 -4.144733 -4.1875205 -4.2050576 -4.2068748 -4.2011366][-4.16515 -4.1566544 -4.1715975 -4.1941652 -4.2004275 -4.169404 -4.1037211 -4.0470395 -4.0380073 -4.0708189 -4.1227684 -4.171411 -4.202692 -4.2164965 -4.2141614][-4.1565089 -4.1386733 -4.152523 -4.1828771 -4.2021775 -4.1838379 -4.1310306 -4.0861835 -4.0791173 -4.0989823 -4.1318893 -4.168499 -4.1995759 -4.2183247 -4.2195759][-4.1511407 -4.1274185 -4.1397133 -4.1769233 -4.208528 -4.2090669 -4.1804562 -4.1519561 -4.1404066 -4.1415563 -4.1533136 -4.172545 -4.1947994 -4.2113047 -4.2155371][-4.1575527 -4.1319952 -4.1399689 -4.1749568 -4.2085419 -4.2191029 -4.2077227 -4.1920943 -4.1781759 -4.17076 -4.1727519 -4.1824145 -4.1959372 -4.2065859 -4.2086329][-4.17704 -4.153285 -4.1530352 -4.1772118 -4.20298 -4.2127428 -4.2088366 -4.202281 -4.1926031 -4.1873126 -4.1891956 -4.1971016 -4.2040858 -4.2067628 -4.2034922][-4.2049055 -4.188302 -4.182559 -4.1943645 -4.2085056 -4.211462 -4.2071691 -4.2028704 -4.1981096 -4.198276 -4.2050872 -4.2155714 -4.2201514 -4.2174344 -4.2104445][-4.2327704 -4.2229433 -4.215661 -4.219398 -4.2265887 -4.2250471 -4.2183361 -4.2127409 -4.2093644 -4.2122321 -4.2208 -4.2314939 -4.2345681 -4.2307148 -4.2266779][-4.2492881 -4.2425561 -4.2355914 -4.2371383 -4.24252 -4.2406116 -4.2352576 -4.2304387 -4.2271638 -4.2272773 -4.2314792 -4.2367468 -4.2384968 -4.2401433 -4.2462292]]...]
INFO - root - 2017-12-05 14:36:12.800728: step 15210, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 81h:23m:43s remains)
INFO - root - 2017-12-05 14:36:21.981623: step 15220, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 82h:03m:16s remains)
INFO - root - 2017-12-05 14:36:31.123277: step 15230, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 81h:26m:10s remains)
INFO - root - 2017-12-05 14:36:39.961395: step 15240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 75h:12m:59s remains)
INFO - root - 2017-12-05 14:36:49.044200: step 15250, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 77h:35m:34s remains)
INFO - root - 2017-12-05 14:36:58.069793: step 15260, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 80h:39m:54s remains)
INFO - root - 2017-12-05 14:37:07.076880: step 15270, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 81h:31m:42s remains)
INFO - root - 2017-12-05 14:37:16.119286: step 15280, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.965 sec/batch; 85h:02m:29s remains)
INFO - root - 2017-12-05 14:37:24.922334: step 15290, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 78h:38m:13s remains)
INFO - root - 2017-12-05 14:37:33.914088: step 15300, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 78h:39m:37s remains)
2017-12-05 14:37:34.667347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2059526 -4.2140484 -4.2103014 -4.1888494 -4.1621161 -4.1379223 -4.1278353 -4.12868 -4.1400018 -4.1634474 -4.1856103 -4.1990671 -4.1994452 -4.1909738 -4.1917558][-4.1860557 -4.196558 -4.1944847 -4.1725225 -4.1443877 -4.122354 -4.1178246 -4.1245418 -4.1397223 -4.1629219 -4.1834497 -4.1960225 -4.1975269 -4.18946 -4.1897807][-4.17119 -4.183218 -4.182322 -4.1623335 -4.1366386 -4.1157093 -4.1146741 -4.127244 -4.1459703 -4.1668811 -4.1830168 -4.19304 -4.1970954 -4.192493 -4.1937323][-4.1576214 -4.1718087 -4.172801 -4.1559558 -4.1347127 -4.1152472 -4.1152668 -4.1287923 -4.1468134 -4.1624212 -4.173224 -4.1833549 -4.1934905 -4.1965532 -4.2011027][-4.1524825 -4.1680751 -4.1698246 -4.1555853 -4.1386123 -4.1194434 -4.1132145 -4.1198215 -4.1314373 -4.1414456 -4.1471262 -4.1592188 -4.1788788 -4.1922016 -4.2032175][-4.1625729 -4.1761227 -4.1759448 -4.1637335 -4.1517968 -4.132803 -4.1179352 -4.11249 -4.1147408 -4.1176257 -4.1178231 -4.13243 -4.1612577 -4.18268 -4.1994257][-4.1779327 -4.1877723 -4.1851311 -4.174109 -4.1670389 -4.1501579 -4.1287212 -4.1144195 -4.112606 -4.1138797 -4.1127152 -4.1274052 -4.1575127 -4.1787467 -4.1952314][-4.1903811 -4.1954288 -4.1919847 -4.1829386 -4.1804075 -4.1691046 -4.1499877 -4.1389017 -4.1434135 -4.1503487 -4.1488614 -4.1558242 -4.1760616 -4.1888623 -4.1993604][-4.1908779 -4.1919508 -4.1871572 -4.1783266 -4.1781607 -4.1740093 -4.1657805 -4.1669121 -4.1823277 -4.1966944 -4.1971512 -4.1985292 -4.2080989 -4.2107987 -4.2128596][-4.1849861 -4.1842504 -4.1804237 -4.173317 -4.1745019 -4.1747479 -4.1772079 -4.1914792 -4.2160735 -4.2342315 -4.237082 -4.234724 -4.2345848 -4.2300353 -4.2261381][-4.1888523 -4.1872659 -4.18614 -4.183053 -4.1844869 -4.1854863 -4.1928115 -4.2142262 -4.242106 -4.2600932 -4.2627735 -4.2568407 -4.2495766 -4.24125 -4.2364492][-4.1973596 -4.1962295 -4.1972895 -4.1990042 -4.2018752 -4.2026739 -4.2102971 -4.2307291 -4.2543225 -4.2669897 -4.2684441 -4.2616105 -4.2527366 -4.2460885 -4.2451191][-4.2075524 -4.2089667 -4.2119908 -4.2157478 -4.2181392 -4.2189941 -4.2251768 -4.2409177 -4.2567916 -4.2630882 -4.2637658 -4.2590213 -4.2519608 -4.2484231 -4.25016][-4.2161279 -4.2167239 -4.2182817 -4.222374 -4.225615 -4.2288337 -4.2360115 -4.2485676 -4.258059 -4.2599983 -4.2596359 -4.2557807 -4.25071 -4.2490306 -4.250412][-4.214787 -4.2133617 -4.2143507 -4.2194586 -4.22562 -4.2343011 -4.2436662 -4.2533603 -4.2589579 -4.2584643 -4.2554522 -4.2504854 -4.2463064 -4.2453485 -4.2460303]]...]
INFO - root - 2017-12-05 14:37:43.907562: step 15310, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 80h:55m:55s remains)
INFO - root - 2017-12-05 14:37:52.939923: step 15320, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 80h:32m:23s remains)
INFO - root - 2017-12-05 14:38:01.980915: step 15330, loss = 2.01, batch loss = 1.95 (8.6 examples/sec; 0.929 sec/batch; 81h:49m:10s remains)
INFO - root - 2017-12-05 14:38:11.065186: step 15340, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 81h:44m:15s remains)
INFO - root - 2017-12-05 14:38:20.237058: step 15350, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 80h:32m:58s remains)
INFO - root - 2017-12-05 14:38:29.349382: step 15360, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 79h:01m:34s remains)
INFO - root - 2017-12-05 14:38:38.421431: step 15370, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 79h:37m:49s remains)
INFO - root - 2017-12-05 14:38:47.383285: step 15380, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 78h:14m:22s remains)
INFO - root - 2017-12-05 14:38:56.454009: step 15390, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 80h:00m:59s remains)
INFO - root - 2017-12-05 14:39:05.624825: step 15400, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 81h:14m:24s remains)
2017-12-05 14:39:06.367395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3332915 -4.3336439 -4.3310947 -4.3277416 -4.3226237 -4.3155079 -4.3080239 -4.3033614 -4.3042111 -4.3123918 -4.3234496 -4.3302932 -4.3301916 -4.32187 -4.3015385][-4.3321781 -4.3311205 -4.3278213 -4.3236208 -4.3173079 -4.3089385 -4.2991443 -4.292098 -4.2911515 -4.2988 -4.3112292 -4.3208561 -4.3241525 -4.31482 -4.2828641][-4.3321061 -4.3309803 -4.3281364 -4.3235393 -4.3161006 -4.30609 -4.2917709 -4.2806907 -4.2769818 -4.2834473 -4.2949381 -4.3044853 -4.3105865 -4.3010721 -4.254189][-4.3333459 -4.3326721 -4.3293777 -4.3216634 -4.3118 -4.2956223 -4.2719254 -4.254096 -4.249434 -4.2593145 -4.2744732 -4.2848988 -4.2875614 -4.2716541 -4.2058625][-4.3335176 -4.3312893 -4.3250017 -4.3139672 -4.3010225 -4.274981 -4.2351761 -4.2027736 -4.1968174 -4.2178378 -4.2449617 -4.2646084 -4.2658625 -4.2368679 -4.1483212][-4.331881 -4.3260465 -4.3153515 -4.2998533 -4.2811623 -4.2423754 -4.1799574 -4.1195 -4.1085606 -4.1521335 -4.2068706 -4.2426987 -4.2489309 -4.2130814 -4.118392][-4.327394 -4.3165588 -4.3002892 -4.275423 -4.2463231 -4.1888661 -4.0942125 -3.9929452 -3.9798374 -4.065074 -4.1625156 -4.2238064 -4.2433176 -4.2158294 -4.142199][-4.3207483 -4.3048363 -4.2827826 -4.2485924 -4.2051473 -4.1264315 -4.0015178 -3.8628268 -3.8553092 -3.9889116 -4.1295657 -4.215168 -4.243814 -4.2285995 -4.1836972][-4.3169327 -4.2968068 -4.2693257 -4.22885 -4.174859 -4.0921412 -3.9778624 -3.8636522 -3.8718171 -4.005815 -4.1451139 -4.2274489 -4.256259 -4.2511735 -4.226861][-4.3140469 -4.2954211 -4.2717671 -4.2365551 -4.1877313 -4.1208849 -4.0499759 -3.9977756 -4.0187688 -4.1058707 -4.1981831 -4.2570634 -4.2860737 -4.2871261 -4.2658796][-4.3169417 -4.3075418 -4.2915015 -4.2668972 -4.2281523 -4.179328 -4.1424317 -4.1280036 -4.1498604 -4.1985078 -4.2496424 -4.2882824 -4.3106527 -4.3046236 -4.2695274][-4.3236303 -4.3233047 -4.3141122 -4.2971382 -4.2697592 -4.2372475 -4.2224703 -4.225297 -4.2414961 -4.2648921 -4.2915792 -4.3153181 -4.32206 -4.2901344 -4.2273088][-4.3256927 -4.32923 -4.3260708 -4.3168921 -4.296979 -4.2726641 -4.2657633 -4.2758608 -4.2887073 -4.3012505 -4.3129096 -4.316833 -4.2975116 -4.2293048 -4.1287503][-4.3154821 -4.3225517 -4.3250246 -4.3135686 -4.2844567 -4.2523532 -4.2397547 -4.2515912 -4.2704859 -4.29051 -4.3020954 -4.2933736 -4.2452312 -4.1395135 -3.9981873][-4.2990675 -4.3069673 -4.3098383 -4.2909522 -4.2425685 -4.1867852 -4.1589236 -4.1771727 -4.2186794 -4.2565918 -4.2754831 -4.26402 -4.2021451 -4.0847559 -3.9397333]]...]
INFO - root - 2017-12-05 14:39:15.449275: step 15410, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 79h:26m:04s remains)
INFO - root - 2017-12-05 14:39:24.450767: step 15420, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 80h:53m:31s remains)
INFO - root - 2017-12-05 14:39:33.511660: step 15430, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 80h:10m:56s remains)
INFO - root - 2017-12-05 14:39:42.714572: step 15440, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 81h:39m:09s remains)
INFO - root - 2017-12-05 14:39:51.682455: step 15450, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 78h:30m:16s remains)
INFO - root - 2017-12-05 14:40:00.775565: step 15460, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 80h:47m:47s remains)
INFO - root - 2017-12-05 14:40:09.907253: step 15470, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 80h:29m:01s remains)
INFO - root - 2017-12-05 14:40:18.931170: step 15480, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 81h:20m:58s remains)
INFO - root - 2017-12-05 14:40:27.992795: step 15490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 82h:16m:54s remains)
INFO - root - 2017-12-05 14:40:37.105599: step 15500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 78h:40m:25s remains)
2017-12-05 14:40:37.883684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2100544 -4.2096806 -4.1972156 -4.1898937 -4.1784306 -4.1730909 -4.182054 -4.1924229 -4.2091985 -4.2245212 -4.2384996 -4.2423325 -4.2385058 -4.2350779 -4.2278628][-4.1592751 -4.1568465 -4.1400385 -4.1364288 -4.1359434 -4.1455817 -4.1705284 -4.1873546 -4.2050338 -4.2224979 -4.2374015 -4.2352386 -4.2233214 -4.2166567 -4.204711][-4.1070175 -4.10551 -4.1007395 -4.1169319 -4.13594 -4.1597419 -4.1948247 -4.2089391 -4.2188544 -4.2291937 -4.2399955 -4.2333117 -4.2171221 -4.20783 -4.1927218][-4.079381 -4.0901732 -4.1124811 -4.1513791 -4.1822171 -4.2067828 -4.2333288 -4.2325573 -4.2285848 -4.2272511 -4.2321558 -4.2268958 -4.2145176 -4.2044282 -4.1883597][-4.1084046 -4.1310015 -4.1649866 -4.2027802 -4.2255287 -4.2345228 -4.2403269 -4.2224178 -4.2072325 -4.2021241 -4.2062435 -4.2078247 -4.2010093 -4.1908965 -4.1752915][-4.1664715 -4.1912313 -4.2154145 -4.2322559 -4.2315779 -4.2120843 -4.1932478 -4.1673436 -4.153079 -4.1599193 -4.1760588 -4.1830831 -4.1799526 -4.176178 -4.1703615][-4.210535 -4.2213511 -4.2258468 -4.2157125 -4.1823435 -4.1249733 -4.0821166 -4.0633345 -4.0628138 -4.0875845 -4.1215568 -4.1427059 -4.1568255 -4.1782084 -4.1959791][-4.2329497 -4.2212224 -4.2025819 -4.1657009 -4.0974402 -3.9902236 -3.9139342 -3.9206791 -3.9588883 -4.0060205 -4.0604491 -4.111938 -4.1546597 -4.2034149 -4.2417636][-4.2385464 -4.2068448 -4.1612949 -4.0971503 -4.004889 -3.8658676 -3.7723866 -3.8141832 -3.9062457 -3.9846563 -4.0571365 -4.1286187 -4.1895785 -4.2476292 -4.2883739][-4.2348194 -4.1931562 -4.13342 -4.0594554 -3.9774883 -3.8755236 -3.8232918 -3.884285 -3.9848256 -4.0681286 -4.1448369 -4.21799 -4.2709041 -4.3101435 -4.3304968][-4.2420106 -4.1999779 -4.1435766 -4.0772462 -4.0232286 -3.9786267 -3.979382 -4.0419354 -4.1222897 -4.192759 -4.2555146 -4.3050704 -4.3329363 -4.3434873 -4.3418155][-4.2357187 -4.1897988 -4.1402049 -4.0856323 -4.0510564 -4.044333 -4.085041 -4.1540246 -4.2154169 -4.2657361 -4.30561 -4.3272977 -4.3289628 -4.3207855 -4.3115807][-4.1951604 -4.1406746 -4.0942578 -4.0567031 -4.0442853 -4.0625811 -4.128521 -4.2052827 -4.2536058 -4.2808504 -4.2972856 -4.2991714 -4.2827806 -4.2655468 -4.2599511][-4.13698 -4.0744061 -4.0292115 -4.0169096 -4.0359659 -4.0769229 -4.148015 -4.2180443 -4.2527714 -4.2609286 -4.2568111 -4.2456803 -4.2216787 -4.2014556 -4.2008548][-4.1098542 -4.0526853 -4.017447 -4.0284433 -4.0671196 -4.1130805 -4.1665044 -4.21303 -4.2282414 -4.21808 -4.197031 -4.1789708 -4.1615996 -4.1489973 -4.1576295]]...]
INFO - root - 2017-12-05 14:40:46.712436: step 15510, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 80h:49m:31s remains)
INFO - root - 2017-12-05 14:40:55.858587: step 15520, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 80h:49m:27s remains)
INFO - root - 2017-12-05 14:41:05.001271: step 15530, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 77h:35m:10s remains)
INFO - root - 2017-12-05 14:41:14.010835: step 15540, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 80h:39m:08s remains)
INFO - root - 2017-12-05 14:41:23.159464: step 15550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 81h:45m:59s remains)
INFO - root - 2017-12-05 14:41:32.146348: step 15560, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 76h:12m:15s remains)
INFO - root - 2017-12-05 14:41:41.004279: step 15570, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 76h:57m:26s remains)
INFO - root - 2017-12-05 14:41:50.141223: step 15580, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.902 sec/batch; 79h:25m:58s remains)
INFO - root - 2017-12-05 14:41:59.254428: step 15590, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.884 sec/batch; 77h:46m:32s remains)
INFO - root - 2017-12-05 14:42:08.252734: step 15600, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 79h:10m:34s remains)
2017-12-05 14:42:09.081064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.208344 -4.2156105 -4.2180729 -4.2246466 -4.2260108 -4.22072 -4.2138166 -4.2188835 -4.2342682 -4.2437077 -4.247695 -4.2465386 -4.2474179 -4.2556071 -4.2648945][-4.1861529 -4.1986527 -4.20705 -4.2206974 -4.2265458 -4.22435 -4.2204323 -4.2304692 -4.2465892 -4.2498136 -4.2434464 -4.2355919 -4.2338767 -4.2426805 -4.2534113][-4.175385 -4.1870365 -4.2005072 -4.2208953 -4.23043 -4.2288628 -4.2247214 -4.2328691 -4.2471008 -4.250051 -4.2444663 -4.23819 -4.2362242 -4.2416253 -4.2474666][-4.1757326 -4.1829138 -4.1984377 -4.2191978 -4.2251396 -4.220418 -4.2163949 -4.2214613 -4.2391868 -4.2500229 -4.2502108 -4.2481618 -4.2496638 -4.25726 -4.2589087][-4.1872439 -4.1933932 -4.2039957 -4.2144566 -4.20993 -4.197361 -4.1899652 -4.1945763 -4.2188826 -4.2415948 -4.2511306 -4.2528038 -4.2576857 -4.2683787 -4.2713342][-4.19829 -4.2072253 -4.2119312 -4.2072129 -4.1908164 -4.1684589 -4.1555338 -4.1618662 -4.1931739 -4.2270222 -4.2469263 -4.2532988 -4.2598133 -4.26984 -4.2704644][-4.1942568 -4.2037363 -4.2039852 -4.1895089 -4.1652226 -4.1358223 -4.1192565 -4.1318808 -4.1720982 -4.2148376 -4.2432017 -4.2524137 -4.2534456 -4.254364 -4.2494221][-4.1803961 -4.1837258 -4.177916 -4.1567287 -4.1275706 -4.0956564 -4.0816832 -4.1051006 -4.15435 -4.2031164 -4.23942 -4.2488117 -4.2391367 -4.2275023 -4.2155371][-4.1605172 -4.1542878 -4.1400676 -4.1135874 -4.0847731 -4.0557137 -4.0481982 -4.0770836 -4.1288533 -4.1855793 -4.2300434 -4.2353759 -4.2114892 -4.1920691 -4.178987][-4.1425524 -4.1228924 -4.1037636 -4.0827122 -4.059916 -4.0389209 -4.0376124 -4.063334 -4.1124949 -4.172997 -4.212389 -4.2037206 -4.167335 -4.1450768 -4.1392975][-4.1491175 -4.1180735 -4.096983 -4.0829744 -4.0668125 -4.0543656 -4.0580592 -4.0771618 -4.1172872 -4.1707778 -4.1930323 -4.1648755 -4.120513 -4.0971951 -4.0978041][-4.1845469 -4.1539388 -4.1350007 -4.1247549 -4.1149812 -4.1100855 -4.1146297 -4.1224904 -4.1473827 -4.184978 -4.1897783 -4.1531868 -4.1104679 -4.0880146 -4.0866642][-4.2178984 -4.1980691 -4.1863785 -4.181869 -4.1817155 -4.18356 -4.1857228 -4.1837826 -4.1909595 -4.2099228 -4.2005863 -4.1664076 -4.136282 -4.1185102 -4.1104784][-4.2445021 -4.2378888 -4.234333 -4.236135 -4.2405429 -4.2450242 -4.2452 -4.2381663 -4.2354069 -4.2406735 -4.227119 -4.19949 -4.1803083 -4.1674781 -4.1568022][-4.2641273 -4.2672253 -4.2662663 -4.2685909 -4.2720752 -4.2758708 -4.2765965 -4.2714124 -4.2669721 -4.2671533 -4.2534628 -4.2308064 -4.2163138 -4.2089071 -4.200316]]...]
INFO - root - 2017-12-05 14:42:18.133120: step 15610, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.889 sec/batch; 78h:13m:33s remains)
INFO - root - 2017-12-05 14:42:27.299750: step 15620, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 81h:25m:16s remains)
INFO - root - 2017-12-05 14:42:36.517217: step 15630, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 81h:27m:05s remains)
INFO - root - 2017-12-05 14:42:45.505003: step 15640, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 79h:23m:25s remains)
INFO - root - 2017-12-05 14:42:54.439025: step 15650, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.825 sec/batch; 72h:35m:19s remains)
INFO - root - 2017-12-05 14:43:03.492931: step 15660, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 79h:48m:57s remains)
INFO - root - 2017-12-05 14:43:12.379342: step 15670, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 77h:45m:55s remains)
INFO - root - 2017-12-05 14:43:21.532828: step 15680, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 79h:06m:46s remains)
INFO - root - 2017-12-05 14:43:30.738217: step 15690, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 79h:47m:01s remains)
INFO - root - 2017-12-05 14:43:39.704310: step 15700, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 79h:07m:58s remains)
2017-12-05 14:43:40.477816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1749182 -4.1796861 -4.1988888 -4.2189569 -4.2376909 -4.2531924 -4.2662745 -4.2724113 -4.26618 -4.2542644 -4.242527 -4.2396107 -4.24606 -4.2615557 -4.282352][-4.1568556 -4.1534443 -4.1652622 -4.1809683 -4.2000785 -4.2189722 -4.2304363 -4.2385936 -4.2370958 -4.2267175 -4.21524 -4.2126665 -4.2249832 -4.2467947 -4.2737656][-4.1526904 -4.1414456 -4.1458392 -4.1543021 -4.1648941 -4.1786375 -4.1877146 -4.197753 -4.2022595 -4.2004547 -4.1969213 -4.2013884 -4.2198381 -4.2464428 -4.2728586][-4.1390662 -4.1224384 -4.1247725 -4.1265965 -4.1231208 -4.12337 -4.1286106 -4.1425729 -4.15634 -4.1681995 -4.1782012 -4.1929021 -4.2159247 -4.244946 -4.2703118][-4.112803 -4.0983982 -4.1029482 -4.1004086 -4.0836482 -4.0626125 -4.0497508 -4.059381 -4.0883946 -4.1187706 -4.1457543 -4.1697321 -4.1982641 -4.2299361 -4.2569637][-4.0914178 -4.0884738 -4.094368 -4.0829148 -4.0472579 -3.9909987 -3.9373894 -3.9341416 -3.9935246 -4.0559354 -4.0952616 -4.1234961 -4.158504 -4.1924238 -4.2188268][-4.08169 -4.09152 -4.0981503 -4.0741825 -4.0099096 -3.9047098 -3.7948964 -3.7804813 -3.8897717 -3.9866118 -4.0332766 -4.0652347 -4.1055293 -4.1370144 -4.1657763][-4.1022596 -4.1141672 -4.1161885 -4.0811429 -3.9949214 -3.8628044 -3.7315485 -3.724082 -3.8540773 -3.9452028 -3.9781547 -4.010828 -4.0567884 -4.0903468 -4.1241593][-4.1335363 -4.1422505 -4.142931 -4.110393 -4.0360456 -3.9305847 -3.8404629 -3.8525414 -3.93991 -3.9736462 -3.9751406 -4.0047631 -4.0578957 -4.0963821 -4.1281333][-4.1524282 -4.1640835 -4.1725636 -4.156177 -4.115097 -4.0582662 -4.0128093 -4.023581 -4.0567389 -4.0423746 -4.0238028 -4.0458474 -4.0960732 -4.1351023 -4.1618676][-4.1607704 -4.1755676 -4.1916909 -4.191287 -4.1777687 -4.1550674 -4.1324253 -4.1340432 -4.1348577 -4.1043696 -4.0834012 -4.096415 -4.1357603 -4.173111 -4.1962628][-4.151473 -4.1717429 -4.1958346 -4.2072244 -4.2064896 -4.1985855 -4.1904893 -4.1967163 -4.1958332 -4.1743207 -4.1641688 -4.1716561 -4.1936135 -4.2180295 -4.23269][-4.15851 -4.1832328 -4.2078609 -4.2183247 -4.217104 -4.2125573 -4.2144423 -4.2332497 -4.2458372 -4.2389011 -4.2368307 -4.2397804 -4.2430744 -4.2469177 -4.2493539][-4.1911902 -4.2130466 -4.2315431 -4.2326412 -4.2259135 -4.2230468 -4.23341 -4.2593365 -4.2809033 -4.2843909 -4.2852116 -4.2869 -4.2821121 -4.2704811 -4.2598405][-4.2307715 -4.2441878 -4.2488985 -4.2381415 -4.2241793 -4.2249055 -4.2447176 -4.2752995 -4.2992625 -4.3090029 -4.3129663 -4.3153148 -4.3084888 -4.2903681 -4.2750115]]...]
INFO - root - 2017-12-05 14:43:49.641828: step 15710, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 79h:51m:22s remains)
INFO - root - 2017-12-05 14:43:58.910403: step 15720, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 80h:06m:55s remains)
INFO - root - 2017-12-05 14:44:07.948371: step 15730, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 80h:02m:07s remains)
INFO - root - 2017-12-05 14:44:16.936248: step 15740, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 78h:13m:29s remains)
INFO - root - 2017-12-05 14:44:25.965270: step 15750, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 79h:54m:52s remains)
INFO - root - 2017-12-05 14:44:35.069055: step 15760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 80h:09m:23s remains)
INFO - root - 2017-12-05 14:44:44.206970: step 15770, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 81h:32m:41s remains)
INFO - root - 2017-12-05 14:44:53.413014: step 15780, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 81h:03m:07s remains)
INFO - root - 2017-12-05 14:45:02.565667: step 15790, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 77h:51m:46s remains)
INFO - root - 2017-12-05 14:45:11.659352: step 15800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 78h:44m:40s remains)
2017-12-05 14:45:12.448302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1250796 -4.1406555 -4.160686 -4.18525 -4.2084451 -4.2259932 -4.23097 -4.2325397 -4.2307591 -4.2234 -4.2025995 -4.17828 -4.1744938 -4.1824541 -4.1903796][-4.1352315 -4.1366806 -4.1478786 -4.1682987 -4.1935716 -4.2125196 -4.2139626 -4.2101855 -4.2079515 -4.2038312 -4.1907272 -4.179379 -4.1874604 -4.1976881 -4.2048512][-4.156384 -4.152257 -4.1574812 -4.1707854 -4.1909914 -4.2012167 -4.194128 -4.1856375 -4.1855636 -4.1951628 -4.2004519 -4.2011905 -4.2102804 -4.2173319 -4.2221389][-4.18559 -4.1883173 -4.1890278 -4.1873708 -4.1888175 -4.1796303 -4.1588454 -4.1487513 -4.1571732 -4.1863327 -4.2152634 -4.2297707 -4.2424703 -4.2453394 -4.24002][-4.2059331 -4.2145863 -4.2092853 -4.1924591 -4.1720128 -4.1404982 -4.1027761 -4.0888724 -4.1125188 -4.1664176 -4.2184386 -4.2482834 -4.2675776 -4.266871 -4.2505541][-4.1923881 -4.201622 -4.1930175 -4.1678972 -4.132266 -4.084208 -4.0355124 -4.0283356 -4.0777802 -4.1593475 -4.2288237 -4.2666817 -4.2872896 -4.2789464 -4.2526364][-4.1632891 -4.1657505 -4.1533613 -4.124743 -4.0830154 -4.0349116 -3.9978311 -4.0156569 -4.0890265 -4.1802659 -4.2478395 -4.2826695 -4.2964525 -4.2797036 -4.2451949][-4.1330352 -4.1230717 -4.1038733 -4.07388 -4.0393558 -4.0140066 -4.0097632 -4.0481877 -4.1215091 -4.1997309 -4.2547054 -4.2830343 -4.2908945 -4.2714806 -4.2356515][-4.1184554 -4.0893369 -4.058414 -4.0288157 -4.0100985 -4.0111303 -4.0331464 -4.0771751 -4.1357732 -4.1935654 -4.232451 -4.2518334 -4.2547321 -4.237915 -4.2108512][-4.1199026 -4.0836711 -4.051321 -4.0266767 -4.0200629 -4.0309353 -4.0524426 -4.083703 -4.1247487 -4.1657581 -4.1946292 -4.2071681 -4.2042904 -4.1899042 -4.1717348][-4.1335378 -4.108427 -4.0870919 -4.0717139 -4.0712676 -4.0812845 -4.0924983 -4.1072135 -4.1310539 -4.1546917 -4.1672325 -4.1677842 -4.1556511 -4.1382155 -4.123312][-4.1537437 -4.1455569 -4.13515 -4.126514 -4.1265359 -4.1336184 -4.1412449 -4.1508436 -4.1665173 -4.1790209 -4.1802325 -4.1700149 -4.1475821 -4.1219721 -4.0982413][-4.1793413 -4.1822815 -4.1751704 -4.1686716 -4.1684132 -4.1733403 -4.1815434 -4.1927395 -4.2079034 -4.216588 -4.2128005 -4.1959929 -4.1689305 -4.1407523 -4.1135111][-4.2032843 -4.2031288 -4.1928725 -4.185657 -4.1863933 -4.1932573 -4.2049074 -4.2191997 -4.2342191 -4.2417006 -4.235023 -4.2144127 -4.1878724 -4.1653442 -4.1435165][-4.2244534 -4.2149215 -4.1996136 -4.1913629 -4.1934285 -4.201715 -4.2142391 -4.2277894 -4.2387347 -4.2411933 -4.2296009 -4.2070203 -4.1871943 -4.1761265 -4.165843]]...]
INFO - root - 2017-12-05 14:45:21.513166: step 15810, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 78h:26m:31s remains)
INFO - root - 2017-12-05 14:45:30.579882: step 15820, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 79h:47m:49s remains)
INFO - root - 2017-12-05 14:45:39.491093: step 15830, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.953 sec/batch; 83h:48m:31s remains)
INFO - root - 2017-12-05 14:45:48.485958: step 15840, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 79h:38m:19s remains)
INFO - root - 2017-12-05 14:45:57.680175: step 15850, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 78h:41m:22s remains)
INFO - root - 2017-12-05 14:46:06.681923: step 15860, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 81h:42m:10s remains)
INFO - root - 2017-12-05 14:46:15.848274: step 15870, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 77h:50m:36s remains)
INFO - root - 2017-12-05 14:46:24.935838: step 15880, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 80h:09m:40s remains)
INFO - root - 2017-12-05 14:46:33.949489: step 15890, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 80h:36m:26s remains)
INFO - root - 2017-12-05 14:46:43.098547: step 15900, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 80h:28m:43s remains)
2017-12-05 14:46:43.876106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3039165 -4.3125143 -4.3087826 -4.30402 -4.2995338 -4.3040318 -4.3085604 -4.31035 -4.309432 -4.3048244 -4.2980137 -4.2937274 -4.2947025 -4.2989287 -4.3034954][-4.2744541 -4.2872043 -4.2814112 -4.2747526 -4.2722473 -4.283145 -4.2940269 -4.300169 -4.2988076 -4.2889595 -4.2755108 -4.2673841 -4.2683334 -4.2764812 -4.286274][-4.2371817 -4.2512436 -4.2407651 -4.2284422 -4.22637 -4.2419882 -4.2601433 -4.2721753 -4.2740836 -4.2626848 -4.244535 -4.2343774 -4.2376881 -4.2513 -4.2674408][-4.1936536 -4.2048254 -4.1845078 -4.1644826 -4.1603546 -4.1782026 -4.2038579 -4.2261429 -4.2408185 -4.2366962 -4.2209854 -4.2145686 -4.22393 -4.243834 -4.261992][-4.1643395 -4.1668139 -4.13248 -4.0998812 -4.0864048 -4.0972424 -4.1257615 -4.1589575 -4.1940312 -4.2104983 -4.2082672 -4.2115178 -4.2261472 -4.2507339 -4.2676582][-4.1599617 -4.1523438 -4.1046171 -4.0560408 -4.0228372 -4.0083408 -4.0187697 -4.0565896 -4.1194124 -4.167851 -4.1913147 -4.2107863 -4.2298098 -4.256588 -4.2742057][-4.1737795 -4.161705 -4.1098728 -4.0476542 -3.9861131 -3.9266405 -3.8930228 -3.9241166 -4.016 -4.0975828 -4.1461525 -4.1856108 -4.213232 -4.2426691 -4.2677426][-4.1846933 -4.1818457 -4.1419082 -4.0778885 -3.9948187 -3.8935921 -3.8091025 -3.8218641 -3.9288139 -4.0247078 -4.0788689 -4.1238213 -4.1610303 -4.2013135 -4.2421665][-4.1768374 -4.1935096 -4.1762619 -4.1265903 -4.0474672 -3.9467859 -3.8660572 -3.8654912 -3.9374511 -4.0014768 -4.0304637 -4.0576215 -4.09429 -4.1451612 -4.1996074][-4.1612988 -4.1984577 -4.2052536 -4.1788387 -4.1224728 -4.0488873 -3.992631 -3.9808621 -4.004096 -4.0247397 -4.0255055 -4.0314336 -4.0590982 -4.1105275 -4.1657929][-4.15756 -4.2060843 -4.2311897 -4.2250257 -4.1935997 -4.1485014 -4.1126714 -4.0961065 -4.0918145 -4.0857763 -4.0738993 -4.0681834 -4.08272 -4.119535 -4.1609039][-4.1747127 -4.2202129 -4.2525067 -4.258625 -4.24343 -4.21702 -4.1948342 -4.1825757 -4.1744404 -4.1639152 -4.1496234 -4.1398153 -4.1437082 -4.1653719 -4.1910958][-4.214973 -4.2485657 -4.2791209 -4.2898054 -4.2849574 -4.2703896 -4.2574906 -4.2500625 -4.2473207 -4.2409306 -4.229888 -4.221673 -4.2208495 -4.2312288 -4.2439084][-4.2700787 -4.2925577 -4.3140707 -4.3222432 -4.3213563 -4.315331 -4.3091407 -4.3055692 -4.3076167 -4.306047 -4.3002439 -4.2966127 -4.2960095 -4.2998209 -4.3024831][-4.3268185 -4.34025 -4.351789 -4.353724 -4.3511186 -4.3476253 -4.3452168 -4.344316 -4.3477354 -4.3481326 -4.3451109 -4.3436804 -4.3449731 -4.3473692 -4.3467517]]...]
INFO - root - 2017-12-05 14:46:52.771328: step 15910, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 78h:50m:45s remains)
INFO - root - 2017-12-05 14:47:01.916388: step 15920, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 80h:59m:53s remains)
INFO - root - 2017-12-05 14:47:11.155301: step 15930, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 81h:56m:23s remains)
INFO - root - 2017-12-05 14:47:20.107889: step 15940, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 82h:57m:41s remains)
INFO - root - 2017-12-05 14:47:29.086028: step 15950, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 80h:18m:36s remains)
INFO - root - 2017-12-05 14:47:38.207449: step 15960, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 81h:12m:25s remains)
INFO - root - 2017-12-05 14:47:47.281484: step 15970, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 79h:59m:53s remains)
INFO - root - 2017-12-05 14:47:56.357417: step 15980, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 81h:05m:32s remains)
INFO - root - 2017-12-05 14:48:05.441005: step 15990, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 81h:24m:27s remains)
INFO - root - 2017-12-05 14:48:14.639475: step 16000, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 80h:26m:49s remains)
2017-12-05 14:48:15.389713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1662507 -4.1478119 -4.1560278 -4.1773505 -4.1969261 -4.2144046 -4.216785 -4.2144403 -4.2236638 -4.2429242 -4.2437797 -4.2146568 -4.1954384 -4.1928148 -4.1853743][-4.1674247 -4.1519904 -4.1668439 -4.1935544 -4.2159882 -4.2327366 -4.2327724 -4.2294588 -4.2292914 -4.236249 -4.2305264 -4.2035003 -4.1841326 -4.1832485 -4.1812816][-4.1653185 -4.1555085 -4.1771865 -4.209281 -4.2340822 -4.2490482 -4.2482152 -4.243731 -4.2364569 -4.2346382 -4.2236371 -4.1953168 -4.1691632 -4.1652684 -4.165287][-4.1784654 -4.1709833 -4.1885214 -4.2184844 -4.2421155 -4.2548156 -4.2492061 -4.23667 -4.2221837 -4.2194514 -4.2091722 -4.1794529 -4.147944 -4.1387973 -4.140348][-4.1975584 -4.1876993 -4.1961846 -4.2211695 -4.2342606 -4.2334332 -4.2122808 -4.185802 -4.1650486 -4.1685724 -4.1678796 -4.1527619 -4.1301813 -4.1138587 -4.1079283][-4.1968102 -4.1828947 -4.1912327 -4.2114935 -4.2085805 -4.1858649 -4.133853 -4.0825763 -4.0449023 -4.0605693 -4.0912595 -4.1084456 -4.1037045 -4.0871215 -4.0704813][-4.151639 -4.13131 -4.1456561 -4.1726174 -4.1682758 -4.1262217 -4.0444531 -3.9515171 -3.8853452 -3.9233651 -4.0005369 -4.0528946 -4.0678067 -4.0577335 -4.0359049][-4.0636311 -4.0297437 -4.0523834 -4.1041336 -4.11714 -4.0721226 -3.9792469 -3.8589666 -3.7735515 -3.8442354 -3.9580851 -4.0227232 -4.0472136 -4.0397091 -4.0169144][-3.9699767 -3.9210885 -3.9472857 -4.0234785 -4.0682173 -4.0505457 -3.9851983 -3.8950381 -3.8432417 -3.9184055 -4.0123386 -4.0594621 -4.0794425 -4.0709267 -4.0540767][-3.9633274 -3.9133322 -3.9311528 -4.0043459 -4.06312 -4.076406 -4.0504241 -4.0077734 -3.9872046 -4.0429869 -4.1004796 -4.1271267 -4.135963 -4.12594 -4.1152673][-4.049727 -4.0213346 -4.027236 -4.0668578 -4.1053319 -4.1215811 -4.1183238 -4.10329 -4.1016469 -4.1406813 -4.1735625 -4.1857443 -4.1824603 -4.168293 -4.1594782][-4.127162 -4.1240716 -4.1275849 -4.1355925 -4.1426654 -4.1444912 -4.146471 -4.1452551 -4.1566958 -4.1917982 -4.21458 -4.2171059 -4.2030473 -4.1846457 -4.1762085][-4.1653538 -4.1738677 -4.1776195 -4.170701 -4.1613741 -4.1521688 -4.1551166 -4.1600866 -4.1768107 -4.21092 -4.2325311 -4.2327609 -4.2181063 -4.2031994 -4.1975155][-4.1821876 -4.1871796 -4.1887026 -4.1815834 -4.1727476 -4.1687717 -4.1784177 -4.1834135 -4.1935911 -4.2159219 -4.2324977 -4.233191 -4.2241335 -4.2170191 -4.2131405][-4.189209 -4.1915717 -4.1930327 -4.19325 -4.1941047 -4.2030945 -4.2159128 -4.2159896 -4.213037 -4.21644 -4.2221437 -4.2204213 -4.2149744 -4.2128124 -4.2129717]]...]
INFO - root - 2017-12-05 14:48:24.462294: step 16010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 80h:08m:45s remains)
INFO - root - 2017-12-05 14:48:33.684643: step 16020, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 79h:38m:05s remains)
INFO - root - 2017-12-05 14:48:42.760362: step 16030, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 79h:42m:40s remains)
INFO - root - 2017-12-05 14:48:51.855670: step 16040, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 76h:34m:07s remains)
INFO - root - 2017-12-05 14:49:00.966057: step 16050, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 81h:28m:10s remains)
INFO - root - 2017-12-05 14:49:10.150841: step 16060, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 79h:32m:51s remains)
INFO - root - 2017-12-05 14:49:19.473372: step 16070, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 82h:38m:53s remains)
INFO - root - 2017-12-05 14:49:28.927707: step 16080, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 82h:55m:13s remains)
INFO - root - 2017-12-05 14:49:38.027230: step 16090, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 79h:00m:34s remains)
INFO - root - 2017-12-05 14:49:47.111981: step 16100, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.864 sec/batch; 75h:58m:41s remains)
2017-12-05 14:49:47.865141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3220224 -4.3231912 -4.3220344 -4.3206415 -4.3188524 -4.3179016 -4.3181458 -4.3176346 -4.316328 -4.3146291 -4.313448 -4.3144264 -4.31867 -4.3264208 -4.3347368][-4.3047791 -4.3102512 -4.3119831 -4.3131933 -4.314096 -4.3156209 -4.3181391 -4.3182178 -4.3156023 -4.3118057 -4.3079834 -4.3065271 -4.3094597 -4.3182626 -4.3289604][-4.2866035 -4.296772 -4.3015532 -4.3059559 -4.3113523 -4.3173208 -4.3233156 -4.3254681 -4.3231807 -4.3176012 -4.3105588 -4.3045621 -4.3038039 -4.312314 -4.3242579][-4.2685347 -4.2773952 -4.2812281 -4.2865553 -4.2958059 -4.3064137 -4.3157516 -4.3207564 -4.3226042 -4.3178353 -4.308331 -4.2987032 -4.2944608 -4.3016033 -4.3152981][-4.2483053 -4.247745 -4.2445006 -4.2457738 -4.2529368 -4.2616115 -4.271544 -4.2825828 -4.2938962 -4.2973557 -4.2910323 -4.27984 -4.2734756 -4.2815256 -4.2987838][-4.2138138 -4.1995687 -4.185699 -4.1829534 -4.1849146 -4.183506 -4.1863842 -4.2037115 -4.2283773 -4.2447963 -4.2475309 -4.241128 -4.2367048 -4.2491016 -4.2724085][-4.1642523 -4.1341047 -4.1075535 -4.1001272 -4.0938735 -4.0749941 -4.0647721 -4.0918961 -4.135407 -4.1694894 -4.1863151 -4.1904736 -4.1938324 -4.2122264 -4.2402935][-4.1285892 -4.0795135 -4.0334096 -4.01159 -3.9872403 -3.9368865 -3.9044926 -3.94914 -4.0234928 -4.0810823 -4.1178036 -4.1391025 -4.15493 -4.178874 -4.2105708][-4.1383266 -4.0817385 -4.0212121 -3.9770341 -3.9250569 -3.8386803 -3.7742596 -3.8269148 -3.9230332 -3.9974067 -4.0508771 -4.097301 -4.135345 -4.1706653 -4.2056575][-4.191658 -4.1499143 -4.0984435 -4.0489964 -3.9864459 -3.8955157 -3.8230758 -3.8488431 -3.9196844 -3.9793878 -4.0325017 -4.0916877 -4.1466408 -4.1925778 -4.2295904][-4.2429557 -4.2267213 -4.2001586 -4.1685219 -4.1210122 -4.0553026 -3.9994307 -3.9987531 -4.025825 -4.0502014 -4.0812969 -4.1280675 -4.1798191 -4.2254252 -4.2603254][-4.2506261 -4.2597303 -4.2604756 -4.2559767 -4.2375736 -4.2065678 -4.1744318 -4.1622448 -4.1597605 -4.1558161 -4.1604519 -4.1819878 -4.2152953 -4.2488151 -4.2760968][-4.2177753 -4.2429218 -4.2626719 -4.281251 -4.2891774 -4.2860065 -4.2759447 -4.2658339 -4.2542472 -4.2368679 -4.2222986 -4.2216954 -4.2345457 -4.2555575 -4.276958][-4.1783876 -4.2062378 -4.2333293 -4.2679172 -4.2944126 -4.3072343 -4.3111615 -4.3073683 -4.2961941 -4.2751641 -4.2514791 -4.2366772 -4.2349973 -4.2468333 -4.2655759][-4.1850615 -4.2024441 -4.2239542 -4.2613125 -4.2944164 -4.3122296 -4.3204088 -4.3195691 -4.3120213 -4.2951117 -4.2729239 -4.2553 -4.2471786 -4.251761 -4.2668595]]...]
INFO - root - 2017-12-05 14:49:56.928974: step 16110, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 78h:31m:43s remains)
INFO - root - 2017-12-05 14:50:05.950316: step 16120, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 76h:55m:06s remains)
INFO - root - 2017-12-05 14:50:15.035900: step 16130, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 83h:08m:09s remains)
INFO - root - 2017-12-05 14:50:24.187442: step 16140, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 81h:05m:11s remains)
INFO - root - 2017-12-05 14:50:33.232523: step 16150, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 79h:01m:43s remains)
INFO - root - 2017-12-05 14:50:42.247438: step 16160, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 80h:13m:40s remains)
INFO - root - 2017-12-05 14:50:51.307382: step 16170, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 78h:31m:57s remains)
INFO - root - 2017-12-05 14:51:00.403096: step 16180, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 78h:24m:09s remains)
INFO - root - 2017-12-05 14:51:09.370790: step 16190, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 79h:22m:09s remains)
INFO - root - 2017-12-05 14:51:18.704117: step 16200, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 80h:21m:25s remains)
2017-12-05 14:51:19.481246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.278584 -4.2692 -4.2571836 -4.2525024 -4.2463017 -4.2304349 -4.2078533 -4.18975 -4.190176 -4.2076077 -4.2175827 -4.2204332 -4.2183452 -4.2198892 -4.2289047][-4.2736969 -4.2624412 -4.2474866 -4.2346559 -4.2160363 -4.1822834 -4.1336989 -4.1007295 -4.1110067 -4.1476502 -4.1784077 -4.2063346 -4.2287588 -4.2366567 -4.2433233][-4.2689795 -4.2545395 -4.2320194 -4.2072749 -4.1730971 -4.1182585 -4.0451474 -3.9963303 -4.0129128 -4.067801 -4.117939 -4.1740279 -4.2239709 -4.24217 -4.2494407][-4.2598233 -4.242166 -4.2169194 -4.1867509 -4.1463213 -4.0819488 -3.9957712 -3.9341998 -3.9428668 -3.997803 -4.0576763 -4.1307983 -4.2004132 -4.22859 -4.240932][-4.2502246 -4.230031 -4.2019506 -4.1658368 -4.1230669 -4.0670919 -3.9867039 -3.9196475 -3.9065151 -3.9472103 -4.0145411 -4.0974522 -4.1702471 -4.2020717 -4.2196531][-4.2420239 -4.2233338 -4.1952982 -4.1543083 -4.1099963 -4.0602 -3.9948549 -3.9279935 -3.8956404 -3.9273579 -4.0028405 -4.0849848 -4.146172 -4.17337 -4.1918254][-4.2449679 -4.2313509 -4.20623 -4.1685758 -4.1233473 -4.0727916 -4.0150895 -3.9577491 -3.9297571 -3.9668336 -4.0374832 -4.0947638 -4.125927 -4.1394377 -4.1546354][-4.2485852 -4.2391362 -4.2219191 -4.1953707 -4.1589756 -4.1122088 -4.0624094 -4.0288677 -4.0237241 -4.0629258 -4.1079931 -4.1259789 -4.1230159 -4.121839 -4.1283078][-4.2507758 -4.2462459 -4.2362032 -4.2214961 -4.1968951 -4.1607227 -4.1241803 -4.1100187 -4.120616 -4.15531 -4.1759825 -4.165216 -4.1406546 -4.1274047 -4.1257944][-4.2607794 -4.2593079 -4.252274 -4.2437267 -4.229435 -4.2050171 -4.1815176 -4.17784 -4.1939187 -4.2183852 -4.227109 -4.208869 -4.1772695 -4.1560211 -4.1442323][-4.2675004 -4.2693467 -4.2621036 -4.2549062 -4.2515826 -4.2420354 -4.2309375 -4.2319937 -4.2437739 -4.2569933 -4.2618217 -4.2455316 -4.214067 -4.1881266 -4.1697907][-4.2745533 -4.2803974 -4.2734427 -4.2697196 -4.2713985 -4.2692757 -4.2654672 -4.2671685 -4.2728238 -4.2812147 -4.284348 -4.2687154 -4.2387614 -4.2127309 -4.1943269][-4.2784696 -4.2899103 -4.2835941 -4.2806587 -4.2838078 -4.2850761 -4.2843561 -4.2862215 -4.289433 -4.2963128 -4.2981462 -4.2841716 -4.2576032 -4.2348781 -4.2219281][-4.2656689 -4.28102 -4.2788939 -4.2798538 -4.2857242 -4.2891512 -4.2900853 -4.2900386 -4.2913141 -4.2962604 -4.2949171 -4.2789478 -4.2528229 -4.234776 -4.2327623][-4.2292819 -4.2468195 -4.2484441 -4.2521839 -4.2593608 -4.2635369 -4.2657051 -4.2645464 -4.2645717 -4.2689061 -4.2666507 -4.2482505 -4.2195644 -4.2040124 -4.2114058]]...]
INFO - root - 2017-12-05 14:51:28.506016: step 16210, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 80h:47m:20s remains)
INFO - root - 2017-12-05 14:51:37.611445: step 16220, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 82h:05m:13s remains)
INFO - root - 2017-12-05 14:51:46.211675: step 16230, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 78h:30m:20s remains)
INFO - root - 2017-12-05 14:51:55.384148: step 16240, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 80h:39m:51s remains)
INFO - root - 2017-12-05 14:52:04.559445: step 16250, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 80h:33m:24s remains)
INFO - root - 2017-12-05 14:52:13.553311: step 16260, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.904 sec/batch; 79h:24m:37s remains)
INFO - root - 2017-12-05 14:52:22.773701: step 16270, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 78h:15m:30s remains)
INFO - root - 2017-12-05 14:52:31.886652: step 16280, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 78h:58m:30s remains)
INFO - root - 2017-12-05 14:52:40.943789: step 16290, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 77h:43m:40s remains)
INFO - root - 2017-12-05 14:52:50.138284: step 16300, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 80h:09m:31s remains)
2017-12-05 14:52:50.917120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2998977 -4.2942848 -4.2866793 -4.2821536 -4.2801876 -4.2827854 -4.2876058 -4.2966928 -4.3074627 -4.3151226 -4.3153782 -4.3061051 -4.2912521 -4.2825871 -4.2877164][-4.2631059 -4.2558575 -4.2439537 -4.2354269 -4.2296724 -4.2295427 -4.2333269 -4.2466135 -4.2644949 -4.278513 -4.2798171 -4.2646704 -4.24199 -4.2298284 -4.2403064][-4.2175684 -4.2116213 -4.2011662 -4.1927032 -4.1814017 -4.1710939 -4.1668034 -4.1823239 -4.2100382 -4.2349968 -4.2377234 -4.2153587 -4.1816936 -4.1632304 -4.1770477][-4.179492 -4.1784873 -4.1739769 -4.166625 -4.1483073 -4.1225204 -4.1025367 -4.1169257 -4.1610003 -4.2043366 -4.2162566 -4.1932068 -4.1508026 -4.119287 -4.1263471][-4.1543283 -4.1575871 -4.1574836 -4.150681 -4.1266003 -4.0828018 -4.0381665 -4.0431256 -4.1070189 -4.176177 -4.206408 -4.1913695 -4.147552 -4.1031227 -4.0984874][-4.1456633 -4.1541705 -4.1571774 -4.1480684 -4.1145835 -4.0488195 -3.9747686 -3.9649475 -4.0473533 -4.146277 -4.2008243 -4.2034674 -4.1702933 -4.1186748 -4.0995722][-4.1410403 -4.15488 -4.1617346 -4.1478858 -4.1025062 -4.0189309 -3.9216115 -3.8957102 -3.9868314 -4.1078496 -4.1860018 -4.213758 -4.1986818 -4.1492867 -4.1168423][-4.1298361 -4.1509929 -4.166563 -4.1575437 -4.1117525 -4.0232387 -3.9206018 -3.8814931 -3.9580052 -4.0792723 -4.1723347 -4.2231274 -4.2256384 -4.1879349 -4.147325][-4.1232071 -4.1467314 -4.1744 -4.1822329 -4.1539254 -4.085597 -4.0046492 -3.9646344 -4.005506 -4.0964203 -4.1805534 -4.2385507 -4.2510805 -4.2233415 -4.178431][-4.1238961 -4.1422629 -4.1766758 -4.2015362 -4.1993318 -4.1629763 -4.1129093 -4.0798459 -4.088707 -4.1413975 -4.20278 -4.2511191 -4.2621503 -4.2380943 -4.1947207][-4.1392636 -4.1483045 -4.1799173 -4.2142534 -4.2316561 -4.2227049 -4.1964173 -4.1694627 -4.1602783 -4.183722 -4.2225947 -4.2594981 -4.2705445 -4.2533455 -4.2180653][-4.1695023 -4.1686678 -4.1926904 -4.225594 -4.2498112 -4.2579961 -4.2498279 -4.2317767 -4.2186012 -4.2235518 -4.242362 -4.2675552 -4.2800689 -4.2723064 -4.2470765][-4.2134891 -4.206872 -4.22048 -4.2444654 -4.2670379 -4.2804937 -4.2846637 -4.2792797 -4.2710128 -4.26666 -4.2704897 -4.2840919 -4.2942638 -4.2903848 -4.2729683][-4.2558413 -4.2491584 -4.2546096 -4.2681084 -4.2832079 -4.2937846 -4.3029966 -4.3079829 -4.3080778 -4.3027158 -4.3008161 -4.306839 -4.3130951 -4.3106117 -4.2981763][-4.2907829 -4.2869377 -4.2890072 -4.2946806 -4.3013115 -4.3078408 -4.3158994 -4.323216 -4.3263426 -4.3240228 -4.3214436 -4.32385 -4.3280768 -4.3279748 -4.32027]]...]
INFO - root - 2017-12-05 14:53:00.072831: step 16310, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 78h:11m:25s remains)
INFO - root - 2017-12-05 14:53:09.152685: step 16320, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 79h:01m:09s remains)
INFO - root - 2017-12-05 14:53:18.224458: step 16330, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 80h:36m:33s remains)
INFO - root - 2017-12-05 14:53:27.354325: step 16340, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 81h:08m:29s remains)
INFO - root - 2017-12-05 14:53:36.531437: step 16350, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 78h:47m:13s remains)
INFO - root - 2017-12-05 14:53:45.746176: step 16360, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.936 sec/batch; 82h:11m:43s remains)
INFO - root - 2017-12-05 14:53:54.873827: step 16370, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 78h:17m:18s remains)
INFO - root - 2017-12-05 14:54:03.968002: step 16380, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 80h:53m:17s remains)
INFO - root - 2017-12-05 14:54:13.253256: step 16390, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 82h:25m:51s remains)
INFO - root - 2017-12-05 14:54:22.341808: step 16400, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 73h:32m:02s remains)
2017-12-05 14:54:23.197593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18742 -4.1868172 -4.1713209 -4.153512 -4.1269751 -4.080441 -4.0337763 -4.0150437 -4.0406766 -4.0910063 -4.1506376 -4.2039518 -4.2440205 -4.2678127 -4.2908182][-4.1850595 -4.1872087 -4.1755676 -4.1594725 -4.1323285 -4.0775266 -4.0232091 -4.0029922 -4.0343919 -4.0858564 -4.1437073 -4.1989555 -4.2411761 -4.2649822 -4.2881932][-4.1808448 -4.1798739 -4.1716437 -4.1625204 -4.145432 -4.0988731 -4.042635 -4.0147734 -4.03622 -4.076592 -4.1268134 -4.1812067 -4.2278104 -4.2566209 -4.2851596][-4.1480541 -4.14672 -4.1436028 -4.1484647 -4.1544309 -4.1270533 -4.0674081 -4.020565 -4.021884 -4.0537395 -4.1029067 -4.158669 -4.2131515 -4.2515011 -4.2863564][-4.1052904 -4.1019521 -4.1020746 -4.1137857 -4.133512 -4.12118 -4.0515575 -3.9745722 -3.9613194 -4.0082684 -4.0756288 -4.1424804 -4.203578 -4.2500005 -4.28692][-4.0671325 -4.061657 -4.0640759 -4.0811186 -4.0997357 -4.0798955 -3.982255 -3.8538594 -3.8292389 -3.9197011 -4.0343375 -4.124815 -4.1940656 -4.2511468 -4.2900052][-4.0464158 -4.0412555 -4.0493073 -4.06701 -4.0716376 -4.02569 -3.8816102 -3.6922545 -3.6634603 -3.8222413 -3.9942813 -4.1120677 -4.1932311 -4.2570443 -4.2958536][-4.0559831 -4.0536304 -4.0647783 -4.0734606 -4.0620027 -3.9914823 -3.8211639 -3.620379 -3.6200109 -3.8250995 -4.0146308 -4.1327682 -4.2096448 -4.2663822 -4.2987309][-4.0959568 -4.0973825 -4.1066504 -4.1045089 -4.0874639 -4.0221567 -3.8929 -3.7716148 -3.7946467 -3.9467218 -4.0848103 -4.1719313 -4.2302904 -4.2752876 -4.3010859][-4.1440911 -4.1530046 -4.1580424 -4.1503105 -4.1309423 -4.0772762 -3.9966831 -3.9369364 -3.9559839 -4.0426016 -4.1297431 -4.1938076 -4.2450609 -4.2828865 -4.3045177][-4.1766496 -4.1905456 -4.1939564 -4.1838284 -4.1628671 -4.1186137 -4.0694618 -4.0395622 -4.0514874 -4.0979962 -4.1572161 -4.212481 -4.2615 -4.2931724 -4.3091578][-4.1721783 -4.1899838 -4.1986575 -4.1933985 -4.1761651 -4.1413612 -4.1056695 -4.0882282 -4.0918603 -4.1144133 -4.1610389 -4.2161493 -4.2673564 -4.2955418 -4.3083243][-4.1564193 -4.1725373 -4.1821237 -4.179338 -4.1683946 -4.1394997 -4.1056328 -4.084496 -4.0850067 -4.1021142 -4.1480169 -4.2101297 -4.2689409 -4.2984385 -4.3094435][-4.1518617 -4.159276 -4.1642675 -4.1622624 -4.1605206 -4.1447396 -4.1185265 -4.0955696 -4.0955405 -4.1162696 -4.1629891 -4.2277632 -4.2850771 -4.3090091 -4.3159943][-4.1593876 -4.1576943 -4.1564188 -4.1580133 -4.1668978 -4.1666079 -4.1521854 -4.1345797 -4.1353488 -4.1595387 -4.2040286 -4.2619143 -4.3059344 -4.3204107 -4.3241167]]...]
INFO - root - 2017-12-05 14:54:32.393086: step 16410, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 81h:45m:43s remains)
INFO - root - 2017-12-05 14:54:41.548433: step 16420, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 80h:42m:34s remains)
INFO - root - 2017-12-05 14:54:50.643036: step 16430, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 78h:55m:09s remains)
INFO - root - 2017-12-05 14:54:59.735703: step 16440, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 79h:46m:25s remains)
INFO - root - 2017-12-05 14:55:08.810372: step 16450, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 80h:35m:45s remains)
INFO - root - 2017-12-05 14:55:18.070265: step 16460, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 82h:09m:53s remains)
INFO - root - 2017-12-05 14:55:27.278314: step 16470, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 81h:33m:12s remains)
INFO - root - 2017-12-05 14:55:36.186289: step 16480, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 78h:34m:58s remains)
INFO - root - 2017-12-05 14:55:45.295211: step 16490, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 79h:53m:59s remains)
INFO - root - 2017-12-05 14:55:54.505407: step 16500, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.897 sec/batch; 78h:46m:29s remains)
2017-12-05 14:55:55.264936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2498879 -4.2462573 -4.2372594 -4.2297053 -4.2293229 -4.2360206 -4.2481484 -4.2612228 -4.2681236 -4.2698107 -4.2712483 -4.2717376 -4.267808 -4.2612357 -4.2518506][-4.2438951 -4.243392 -4.2311735 -4.2173204 -4.2088304 -4.2081 -4.2157779 -4.2284274 -4.23881 -4.2466497 -4.2569909 -4.2683225 -4.2716622 -4.269846 -4.2621374][-4.2277827 -4.2308722 -4.2153964 -4.1947937 -4.1773148 -4.1653781 -4.1615038 -4.168179 -4.1788721 -4.1929922 -4.2146773 -4.2392807 -4.2542839 -4.2610736 -4.2544289][-4.1922007 -4.2003975 -4.1863308 -4.1659727 -4.1461463 -4.1275363 -4.1131687 -4.1092882 -4.109992 -4.11796 -4.1416068 -4.1737866 -4.2015891 -4.2190609 -4.2172523][-4.139276 -4.1535397 -4.1481309 -4.1353621 -4.1221671 -4.105299 -4.0863223 -4.0703316 -4.0522504 -4.042706 -4.0564418 -4.0848036 -4.1164579 -4.1434813 -4.1545744][-4.0959425 -4.1103296 -4.1102676 -4.1058884 -4.0994687 -4.0866184 -4.0682364 -4.0447545 -4.0088973 -3.9821875 -3.9835203 -3.9995985 -4.0257607 -4.0560861 -4.08132][-4.0702324 -4.0812511 -4.0852723 -4.0872979 -4.0872831 -4.079206 -4.0629592 -4.0339932 -3.9865172 -3.9491153 -3.9379966 -3.9357843 -3.949563 -3.9804094 -4.0169148][-4.0569792 -4.0634255 -4.0689926 -4.0754757 -4.0800757 -4.0796208 -4.0735364 -4.0514579 -4.0096421 -3.978189 -3.9641256 -3.950711 -3.9518161 -3.9717777 -4.0027709][-4.0610251 -4.0610881 -4.0626879 -4.0679574 -4.0685873 -4.07343 -4.0817389 -4.0773706 -4.0521216 -4.0359974 -4.0290523 -4.0201311 -4.0192642 -4.0191789 -4.0215797][-4.0825286 -4.0788412 -4.07704 -4.0771308 -4.0669889 -4.0677681 -4.0802493 -4.089994 -4.0814095 -4.0785575 -4.0822029 -4.0803671 -4.0800414 -4.066875 -4.0461755][-4.1072993 -4.0993938 -4.0955839 -4.0928788 -4.0760512 -4.0683851 -4.0746241 -4.0871811 -4.08654 -4.088388 -4.098372 -4.1051331 -4.1084795 -4.0917706 -4.0685587][-4.132987 -4.1271896 -4.1241417 -4.1209478 -4.0987992 -4.0841079 -4.0800376 -4.0898652 -4.0927858 -4.0942297 -4.1043234 -4.1175289 -4.1256804 -4.1126304 -4.1023116][-4.1526651 -4.1558771 -4.1577177 -4.1553245 -4.1303263 -4.1073837 -4.0977793 -4.1042991 -4.1056972 -4.1034274 -4.1110373 -4.1254916 -4.1379905 -4.1389332 -4.1479154][-4.1635852 -4.1716943 -4.1735668 -4.17083 -4.1470232 -4.1222439 -4.1115503 -4.1150608 -4.1148677 -4.111218 -4.1187873 -4.1326914 -4.1451273 -4.1592603 -4.1874123][-4.1585922 -4.1629767 -4.1598096 -4.1596961 -4.1491132 -4.1322107 -4.1239614 -4.1279874 -4.1294093 -4.1278782 -4.1316633 -4.1403508 -4.1466365 -4.1655693 -4.2021451]]...]
INFO - root - 2017-12-05 14:56:04.276339: step 16510, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 81h:54m:21s remains)
INFO - root - 2017-12-05 14:56:13.533710: step 16520, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 82h:10m:28s remains)
INFO - root - 2017-12-05 14:56:22.741416: step 16530, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 80h:20m:48s remains)
INFO - root - 2017-12-05 14:56:31.778485: step 16540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 75h:49m:23s remains)
INFO - root - 2017-12-05 14:56:41.100274: step 16550, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 82h:57m:27s remains)
INFO - root - 2017-12-05 14:56:50.451189: step 16560, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 82h:31m:48s remains)
INFO - root - 2017-12-05 14:56:59.613367: step 16570, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 79h:32m:39s remains)
INFO - root - 2017-12-05 14:57:08.681297: step 16580, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 80h:07m:57s remains)
INFO - root - 2017-12-05 14:57:17.679643: step 16590, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 77h:37m:52s remains)
INFO - root - 2017-12-05 14:57:26.907839: step 16600, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 77h:58m:40s remains)
2017-12-05 14:57:27.661150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0478697 -4.0344911 -4.037847 -4.0208645 -4.0188308 -4.0656872 -4.10688 -4.1252627 -4.1304131 -4.138711 -4.1377249 -4.1303477 -4.1524405 -4.1836658 -4.1848836][-4.0636859 -4.0509048 -4.0498557 -4.0260587 -4.0185986 -4.06232 -4.103672 -4.1259184 -4.1379132 -4.1534448 -4.1547523 -4.145968 -4.1692972 -4.2004409 -4.1910686][-4.0843558 -4.0712538 -4.07261 -4.0485945 -4.0343666 -4.07333 -4.1041059 -4.1159077 -4.1262989 -4.14284 -4.1503644 -4.1489272 -4.1741323 -4.203167 -4.1881919][-4.0938 -4.0790219 -4.0855441 -4.063621 -4.0464678 -4.0765915 -4.09013 -4.0874844 -4.0938015 -4.1132717 -4.1301823 -4.141345 -4.1683097 -4.1919947 -4.1725025][-4.0916281 -4.0708675 -4.0765328 -4.0549917 -4.0361567 -4.0623484 -4.0641732 -4.0497475 -4.0571947 -4.0841055 -4.1094222 -4.1282868 -4.158885 -4.174346 -4.1493058][-4.0849895 -4.055923 -4.0499568 -4.0190549 -3.9996538 -4.0241203 -4.0086322 -3.9697261 -3.9865012 -4.0413547 -4.0853744 -4.1127291 -4.1511087 -4.1672697 -4.1455927][-4.0803676 -4.0395513 -4.0171976 -3.9709964 -3.9436433 -3.9553986 -3.9026694 -3.815762 -3.846801 -3.9562933 -4.0374312 -4.0853086 -4.139082 -4.1643968 -4.1638484][-4.0803037 -4.031745 -3.9961045 -3.9457424 -3.9102895 -3.8952773 -3.7900553 -3.6248651 -3.6687996 -3.8518243 -3.9744155 -4.0440588 -4.1209588 -4.1628523 -4.1784339][-4.0817013 -4.0339313 -3.997627 -3.9564559 -3.9227297 -3.8992286 -3.7836108 -3.5976212 -3.6386185 -3.8360677 -3.9585066 -4.0321069 -4.1212234 -4.1725612 -4.19715][-4.0732803 -4.0300107 -4.0082183 -3.9824204 -3.9640152 -3.9583364 -3.8871427 -3.769959 -3.7983141 -3.9349198 -4.0108523 -4.0616145 -4.1372781 -4.1885858 -4.2172351][-4.0486255 -4.0117912 -4.0109706 -3.9986343 -3.9987311 -4.0149531 -3.9826591 -3.9206352 -3.9464004 -4.03323 -4.0727191 -4.0976963 -4.1533422 -4.1964192 -4.2206945][-4.0136204 -3.979558 -3.9920061 -3.9939868 -4.0122094 -4.0539045 -4.0556941 -4.0268431 -4.0529456 -4.1134467 -4.1276784 -4.1357312 -4.1766953 -4.2019968 -4.2108421][-4.0011497 -3.9664843 -3.9824963 -3.9913712 -4.0237355 -4.0882483 -4.1174159 -4.1079144 -4.125773 -4.1630292 -4.1587729 -4.1536307 -4.183538 -4.1933608 -4.186532][-4.0226774 -3.9892201 -4.0008421 -4.0124145 -4.0486274 -4.1155934 -4.1549368 -4.1548257 -4.1656885 -4.1823153 -4.1657562 -4.150722 -4.1682353 -4.1686649 -4.1585131][-4.0693493 -4.0422306 -4.0502777 -4.0593395 -4.0890803 -4.14402 -4.1803956 -4.1829286 -4.1879468 -4.1952715 -4.1777115 -4.1614509 -4.1669731 -4.1607728 -4.149478]]...]
INFO - root - 2017-12-05 14:57:36.936486: step 16610, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 81h:33m:21s remains)
INFO - root - 2017-12-05 14:57:46.241619: step 16620, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 80h:44m:41s remains)
INFO - root - 2017-12-05 14:57:55.334288: step 16630, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 80h:44m:14s remains)
INFO - root - 2017-12-05 14:58:04.339391: step 16640, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:21m:46s remains)
INFO - root - 2017-12-05 14:58:13.564779: step 16650, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 82h:38m:21s remains)
INFO - root - 2017-12-05 14:58:22.766013: step 16660, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 80h:26m:39s remains)
INFO - root - 2017-12-05 14:58:32.046502: step 16670, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 82h:38m:20s remains)
INFO - root - 2017-12-05 14:58:41.151697: step 16680, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 77h:43m:17s remains)
INFO - root - 2017-12-05 14:58:50.274652: step 16690, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 78h:40m:21s remains)
INFO - root - 2017-12-05 14:58:59.276870: step 16700, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 77h:34m:59s remains)
2017-12-05 14:59:00.052167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3441873 -4.3367381 -4.3276772 -4.3209295 -4.3192415 -4.3214436 -4.32641 -4.3281808 -4.325057 -4.3206067 -4.3142719 -4.3025107 -4.2863712 -4.2741857 -4.2677007][-4.3322282 -4.3237095 -4.3165212 -4.3131771 -4.3137794 -4.3162794 -4.320641 -4.322401 -4.3196731 -4.3161106 -4.3110547 -4.3009977 -4.2874212 -4.276567 -4.2690287][-4.3226428 -4.3133883 -4.3087616 -4.3076229 -4.3091044 -4.3116136 -4.3167481 -4.3197012 -4.3183541 -4.3169332 -4.3153348 -4.3095431 -4.30048 -4.2923136 -4.2843218][-4.3190937 -4.3086104 -4.3030853 -4.3014836 -4.303659 -4.3070068 -4.3155141 -4.3196173 -4.316143 -4.3146996 -4.3158121 -4.3147664 -4.3130851 -4.3083954 -4.3001885][-4.3148918 -4.3007145 -4.2905564 -4.2854972 -4.285852 -4.287097 -4.300241 -4.3067079 -4.3026538 -4.2998734 -4.3028283 -4.30676 -4.31312 -4.3134241 -4.3068423][-4.2958937 -4.2732105 -4.25368 -4.2388697 -4.2267623 -4.2146759 -4.2244329 -4.2318592 -4.2330332 -4.2348843 -4.2448182 -4.2590504 -4.2769184 -4.2833056 -4.2817693][-4.2594085 -4.2244358 -4.1902204 -4.1537976 -4.1125913 -4.0698991 -4.0643454 -4.0811586 -4.1047583 -4.1235867 -4.1490774 -4.1797409 -4.210742 -4.223115 -4.2283325][-4.2167106 -4.1722732 -4.1233878 -4.06047 -3.9788032 -3.8849466 -3.8529441 -3.8996396 -3.9689338 -4.0172195 -4.0631185 -4.1061363 -4.1435285 -4.1590414 -4.1696424][-4.1922278 -4.1451836 -4.0919542 -4.0179105 -3.9156113 -3.7892063 -3.7400503 -3.8185136 -3.9193859 -3.9834397 -4.0333142 -4.0702567 -4.0975142 -4.1072979 -4.1212783][-4.1852884 -4.1411343 -4.0967507 -4.03619 -3.9524426 -3.8502364 -3.8207223 -3.8889081 -3.9661002 -4.012085 -4.0428753 -4.0624089 -4.0717964 -4.0743394 -4.0897141][-4.1920338 -4.1523089 -4.1221581 -4.0863094 -4.0336003 -3.9687979 -3.9618258 -4.0056772 -4.0449948 -4.0673876 -4.0838165 -4.095017 -4.0976119 -4.0993319 -4.1158667][-4.2153139 -4.1841059 -4.1675367 -4.150712 -4.1221867 -4.0885725 -4.0926137 -4.1154389 -4.1318135 -4.1435533 -4.1578903 -4.168396 -4.1721926 -4.1764226 -4.1899009][-4.2520881 -4.2318759 -4.2258644 -4.2206941 -4.2096133 -4.199707 -4.2080059 -4.2191362 -4.2267776 -4.2351933 -4.2459416 -4.2538238 -4.2585125 -4.2615218 -4.2682533][-4.2945709 -4.2849984 -4.2857122 -4.2877979 -4.2864966 -4.2865119 -4.2942367 -4.3006072 -4.3058262 -4.3120275 -4.3189898 -4.3245077 -4.3291516 -4.3317947 -4.3345084][-4.33116 -4.3285623 -4.3333406 -4.3403912 -4.34388 -4.3454337 -4.3487005 -4.3503985 -4.3514438 -4.3537459 -4.3579388 -4.3627396 -4.3675637 -4.3717737 -4.374496]]...]
INFO - root - 2017-12-05 14:59:09.117899: step 16710, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 77h:30m:24s remains)
INFO - root - 2017-12-05 14:59:18.211358: step 16720, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 80h:08m:39s remains)
INFO - root - 2017-12-05 14:59:27.467726: step 16730, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 80h:44m:43s remains)
INFO - root - 2017-12-05 14:59:36.613373: step 16740, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 74h:36m:03s remains)
INFO - root - 2017-12-05 14:59:45.665850: step 16750, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 80h:26m:51s remains)
INFO - root - 2017-12-05 14:59:54.626273: step 16760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 75h:15m:15s remains)
INFO - root - 2017-12-05 15:00:03.857217: step 16770, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 80h:05m:12s remains)
INFO - root - 2017-12-05 15:00:12.982490: step 16780, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 79h:34m:27s remains)
INFO - root - 2017-12-05 15:00:22.059263: step 16790, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.873 sec/batch; 76h:31m:49s remains)
INFO - root - 2017-12-05 15:00:31.400177: step 16800, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 80h:25m:38s remains)
2017-12-05 15:00:32.170596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2734952 -4.2574129 -4.2447791 -4.2321415 -4.2165689 -4.2057414 -4.2047772 -4.2114062 -4.2283049 -4.2395225 -4.2429876 -4.2410178 -4.2419672 -4.2403774 -4.2400684][-4.2601523 -4.2413192 -4.2257409 -4.2092614 -4.185832 -4.1658559 -4.1578465 -4.1659231 -4.1875553 -4.2055416 -4.2161679 -4.2195449 -4.2228494 -4.2204022 -4.2208714][-4.2508664 -4.2310405 -4.214427 -4.1971922 -4.1668735 -4.1342163 -4.118258 -4.1287622 -4.1558237 -4.18308 -4.2022591 -4.2151346 -4.2244554 -4.220953 -4.2207675][-4.2388315 -4.2196484 -4.2077174 -4.1974249 -4.16863 -4.130425 -4.1121149 -4.1254506 -4.1553431 -4.1828232 -4.2018161 -4.2178617 -4.2332497 -4.2287049 -4.228816][-4.2186627 -4.198348 -4.1927271 -4.1927028 -4.1733165 -4.1362634 -4.1170144 -4.1367493 -4.1720605 -4.1963291 -4.2067423 -4.2185836 -4.236095 -4.23399 -4.2356811][-4.1932454 -4.1715012 -4.1675825 -4.1690788 -4.1510525 -4.1043725 -4.0790315 -4.1100154 -4.163765 -4.1907206 -4.1970177 -4.2058272 -4.2243037 -4.2238846 -4.2294745][-4.1607218 -4.1355586 -4.1259165 -4.1185055 -4.0850019 -4.0107551 -3.9631248 -4.0090423 -4.0981779 -4.1417556 -4.1520753 -4.1686563 -4.1922135 -4.1940732 -4.2049403][-4.133162 -4.1020627 -4.0857854 -4.0614429 -4.000638 -3.8905125 -3.8080542 -3.8680787 -3.9981172 -4.073122 -4.0949073 -4.121222 -4.1531777 -4.163271 -4.182261][-4.1385694 -4.1027684 -4.0819 -4.0423822 -3.964782 -3.8437891 -3.7564242 -3.8346846 -3.9794428 -4.0661697 -4.0948792 -4.1222324 -4.1538534 -4.1696606 -4.1927619][-4.1770635 -4.1422005 -4.1186762 -4.0749702 -4.0081863 -3.9229429 -3.8779898 -3.9455972 -4.0528326 -4.120213 -4.1443844 -4.1677728 -4.196238 -4.21609 -4.2369289][-4.2163639 -4.1902542 -4.1732287 -4.139966 -4.1001387 -4.0533156 -4.0379515 -4.0834184 -4.151648 -4.1982188 -4.21835 -4.2375097 -4.25985 -4.2741594 -4.2865272][-4.2449632 -4.22963 -4.2196465 -4.198832 -4.1803427 -4.1566896 -4.1521254 -4.1825628 -4.2272739 -4.2606335 -4.2768145 -4.289474 -4.3047194 -4.3098125 -4.3145161][-4.2631063 -4.254786 -4.2504878 -4.2386684 -4.2282071 -4.2168179 -4.218112 -4.2418294 -4.2715631 -4.2951832 -4.3089938 -4.3169422 -4.3232536 -4.3222594 -4.3220611][-4.2706623 -4.26626 -4.2677636 -4.2654686 -4.2616758 -4.2545147 -4.2560964 -4.2720947 -4.2928095 -4.3103232 -4.3220577 -4.3285961 -4.328711 -4.32319 -4.3181953][-4.280601 -4.2774944 -4.2807631 -4.283083 -4.2830071 -4.279048 -4.2781105 -4.2851758 -4.298213 -4.310874 -4.31939 -4.3252497 -4.3251028 -4.3195024 -4.3124905]]...]
INFO - root - 2017-12-05 15:00:41.232922: step 16810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 81h:20m:34s remains)
INFO - root - 2017-12-05 15:00:50.308875: step 16820, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 77h:09m:19s remains)
INFO - root - 2017-12-05 15:00:59.485570: step 16830, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 79h:45m:28s remains)
INFO - root - 2017-12-05 15:01:08.540608: step 16840, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 78h:51m:13s remains)
INFO - root - 2017-12-05 15:01:17.769102: step 16850, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 78h:51m:48s remains)
INFO - root - 2017-12-05 15:01:26.903699: step 16860, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 77h:51m:48s remains)
INFO - root - 2017-12-05 15:01:35.964512: step 16870, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 80h:55m:35s remains)
INFO - root - 2017-12-05 15:01:45.059810: step 16880, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 81h:54m:55s remains)
INFO - root - 2017-12-05 15:01:54.339764: step 16890, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 82h:02m:27s remains)
INFO - root - 2017-12-05 15:02:03.495593: step 16900, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 81h:10m:38s remains)
2017-12-05 15:02:04.246191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3143158 -4.3106351 -4.3115158 -4.3137321 -4.311728 -4.3111753 -4.3155384 -4.3166313 -4.3156948 -4.3188887 -4.3212395 -4.3198256 -4.3228512 -4.3274903 -4.3302488][-4.2931972 -4.2858057 -4.2886276 -4.2923436 -4.2904491 -4.2906919 -4.2947288 -4.2901444 -4.2835274 -4.28484 -4.2887154 -4.2913704 -4.2996726 -4.3099332 -4.3149238][-4.2725873 -4.262166 -4.2639375 -4.2652855 -4.2637777 -4.2661619 -4.271 -4.2601647 -4.2469673 -4.2445669 -4.2456145 -4.2476435 -4.2554979 -4.27073 -4.2826486][-4.2363472 -4.2133021 -4.2054234 -4.205822 -4.213068 -4.2295184 -4.2477522 -4.244348 -4.22879 -4.2213135 -4.2184367 -4.2164211 -4.2173352 -4.2299604 -4.2440906][-4.1951852 -4.1467438 -4.113946 -4.1140208 -4.1360846 -4.1691704 -4.2003675 -4.2076325 -4.1941237 -4.1808691 -4.1770329 -4.1755352 -4.1697865 -4.1797166 -4.1994848][-4.1899443 -4.1308346 -4.0764236 -4.0678058 -4.0938473 -4.1296024 -4.1574292 -4.1618624 -4.1438055 -4.1217203 -4.1142945 -4.1175423 -4.1156826 -4.1287065 -4.154634][-4.203959 -4.1549878 -4.0957661 -4.0735583 -4.0874 -4.1140862 -4.1293836 -4.1296091 -4.1085491 -4.0819521 -4.0683522 -4.0655375 -4.062746 -4.0750322 -4.0999417][-4.2079072 -4.168633 -4.1133804 -4.0854454 -4.0915637 -4.1073904 -4.1147356 -4.1140056 -4.1017866 -4.0849218 -4.0679274 -4.0545387 -4.0503182 -4.0582223 -4.0749559][-4.2005477 -4.1661572 -4.1140246 -4.0836143 -4.0858645 -4.098042 -4.1001568 -4.09733 -4.0920029 -4.0912857 -4.0789704 -4.06068 -4.05958 -4.0698895 -4.0788302][-4.2047873 -4.1753016 -4.1257296 -4.0916176 -4.0906997 -4.1040821 -4.0995393 -4.0865912 -4.07791 -4.080905 -4.07448 -4.06011 -4.0625114 -4.07717 -4.080328][-4.2159557 -4.1968093 -4.1618805 -4.1332445 -4.13448 -4.1494107 -4.1393971 -4.1171889 -4.1009994 -4.0998316 -4.0862646 -4.0722904 -4.0776443 -4.0955815 -4.09371][-4.2059426 -4.191267 -4.1731019 -4.1569595 -4.168642 -4.1897326 -4.1824589 -4.1618557 -4.1475935 -4.1422529 -4.1257777 -4.1084294 -4.1143227 -4.1330104 -4.1259422][-4.1946392 -4.1785707 -4.1656141 -4.1550179 -4.1755157 -4.2016935 -4.2023859 -4.1879992 -4.1786051 -4.1730065 -4.1593413 -4.1432705 -4.1485772 -4.1673079 -4.1602364][-4.2004571 -4.1832004 -4.168499 -4.1569896 -4.1735311 -4.1955366 -4.1976948 -4.1869931 -4.1817818 -4.1805053 -4.1695566 -4.1588469 -4.1646938 -4.1809258 -4.1749878][-4.2061653 -4.1883192 -4.1745124 -4.1662 -4.18176 -4.1955805 -4.1951194 -4.1850476 -4.1795092 -4.1778607 -4.1681972 -4.1608791 -4.1635652 -4.1682587 -4.1570344]]...]
INFO - root - 2017-12-05 15:02:13.349973: step 16910, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.935 sec/batch; 81h:55m:55s remains)
INFO - root - 2017-12-05 15:02:22.651095: step 16920, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 80h:06m:43s remains)
INFO - root - 2017-12-05 15:02:32.004519: step 16930, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 79h:16m:05s remains)
INFO - root - 2017-12-05 15:02:41.396977: step 16940, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 82h:13m:38s remains)
INFO - root - 2017-12-05 15:02:50.359283: step 16950, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 80h:16m:15s remains)
INFO - root - 2017-12-05 15:02:59.442515: step 16960, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 74h:44m:19s remains)
INFO - root - 2017-12-05 15:03:08.654514: step 16970, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 82h:15m:27s remains)
INFO - root - 2017-12-05 15:03:17.555346: step 16980, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 79h:17m:38s remains)
INFO - root - 2017-12-05 15:03:26.679467: step 16990, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 81h:01m:42s remains)
INFO - root - 2017-12-05 15:03:36.033187: step 17000, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 84h:02m:42s remains)
2017-12-05 15:03:36.794985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1468983 -4.1475821 -4.1607018 -4.17967 -4.1845856 -4.1714692 -4.1602912 -4.1629963 -4.1722121 -4.1685915 -4.1575694 -4.1671543 -4.191978 -4.209919 -4.2163529][-4.1578736 -4.166204 -4.1885452 -4.2093239 -4.2094369 -4.1894393 -4.1736407 -4.17199 -4.173275 -4.1695123 -4.167984 -4.1852403 -4.211761 -4.2264161 -4.2274528][-4.1831841 -4.1972227 -4.2197332 -4.2355733 -4.2298942 -4.2075062 -4.190403 -4.1860023 -4.1828232 -4.180161 -4.1850467 -4.2044353 -4.2290111 -4.2428331 -4.2402382][-4.1875587 -4.202002 -4.2190566 -4.229918 -4.220437 -4.2003689 -4.1873651 -4.1881528 -4.1903677 -4.1922951 -4.2015972 -4.2201104 -4.2416854 -4.2541308 -4.2511683][-4.1469555 -4.1571112 -4.1740985 -4.1884346 -4.1811762 -4.1655221 -4.1594882 -4.1711884 -4.1823263 -4.1857233 -4.1964326 -4.2139344 -4.232594 -4.2444339 -4.2467308][-4.0817871 -4.0910912 -4.1095486 -4.1279016 -4.12339 -4.1132379 -4.1126537 -4.1292129 -4.1443925 -4.1476603 -4.1606159 -4.1801739 -4.1992831 -4.2131476 -4.2247715][-4.0260186 -4.0382957 -4.0582337 -4.0748425 -4.06876 -4.0605979 -4.0596914 -4.0701833 -4.0795016 -4.0821323 -4.1020017 -4.1315608 -4.1579642 -4.17747 -4.1979485][-3.9959271 -4.0118952 -4.0310874 -4.0380688 -4.0249152 -4.0155787 -4.0112486 -4.0074673 -4.0022693 -4.0006838 -4.0291324 -4.076376 -4.1171408 -4.1446657 -4.1747851][-4.0099225 -4.0256104 -4.038734 -4.0312085 -4.0062389 -3.9884791 -3.9785511 -3.9644828 -3.9472723 -3.9435236 -3.977977 -4.0392332 -4.0893755 -4.1224713 -4.1612306][-4.0568495 -4.069437 -4.0714655 -4.0540824 -4.0255995 -4.0049281 -3.9933729 -3.9797919 -3.9635737 -3.9587252 -3.9907017 -4.0512209 -4.0997477 -4.1322889 -4.173255][-4.1172776 -4.1205649 -4.1107512 -4.0887027 -4.0643487 -4.0463152 -4.0345097 -4.0269723 -4.0213614 -4.0200586 -4.0442891 -4.0945005 -4.1370897 -4.1663589 -4.203342][-4.1816044 -4.1789551 -4.1639719 -4.1443119 -4.1266537 -4.1125841 -4.1021824 -4.0996337 -4.1029239 -4.1047406 -4.1204643 -4.1552382 -4.1876488 -4.2108641 -4.2376308][-4.2266259 -4.22354 -4.2122655 -4.2016253 -4.1944938 -4.1880407 -4.1808944 -4.1824188 -4.1890125 -4.1901774 -4.1963625 -4.2135358 -4.2314191 -4.2465024 -4.2625928][-4.2491922 -4.2464213 -4.2397032 -4.2370396 -4.2389288 -4.2390723 -4.2361307 -4.2410383 -4.2467632 -4.2451873 -4.2433872 -4.2462239 -4.2531757 -4.2637954 -4.2745962][-4.2564836 -4.2547579 -4.251 -4.2512126 -4.2559662 -4.2595716 -4.2602167 -4.2657194 -4.2680669 -4.26266 -4.2559867 -4.2516766 -4.2548294 -4.26576 -4.2767172]]...]
INFO - root - 2017-12-05 15:03:46.009037: step 17010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 80h:37m:37s remains)
INFO - root - 2017-12-05 15:03:55.140181: step 17020, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 80h:20m:19s remains)
INFO - root - 2017-12-05 15:04:04.223777: step 17030, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 79h:46m:09s remains)
INFO - root - 2017-12-05 15:04:13.356250: step 17040, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.973 sec/batch; 85h:15m:17s remains)
INFO - root - 2017-12-05 15:04:22.544722: step 17050, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 79h:05m:15s remains)
INFO - root - 2017-12-05 15:04:31.670373: step 17060, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 81h:14m:12s remains)
INFO - root - 2017-12-05 15:04:40.760140: step 17070, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 80h:26m:33s remains)
INFO - root - 2017-12-05 15:04:49.909035: step 17080, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 81h:07m:59s remains)
INFO - root - 2017-12-05 15:04:59.136821: step 17090, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 78h:43m:10s remains)
INFO - root - 2017-12-05 15:05:08.302445: step 17100, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.941 sec/batch; 82h:26m:53s remains)
2017-12-05 15:05:09.078211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2551632 -4.2416821 -4.2351851 -4.232563 -4.2320333 -4.2348976 -4.2439718 -4.2528572 -4.260705 -4.2672324 -4.2697654 -4.268137 -4.26886 -4.2707725 -4.2712765][-4.2277012 -4.2050271 -4.1878819 -4.1744184 -4.1666126 -4.1677852 -4.1812611 -4.2023249 -4.2254333 -4.2455406 -4.258637 -4.2606964 -4.2592759 -4.25979 -4.2601018][-4.2043624 -4.1817203 -4.1555505 -4.1261663 -4.1051416 -4.0978417 -4.1107907 -4.1435137 -4.1845841 -4.2189665 -4.2425604 -4.2502441 -4.246016 -4.239006 -4.2371736][-4.1949387 -4.1782556 -4.1453905 -4.1015429 -4.0606737 -4.0285316 -4.0221405 -4.0593076 -4.1219687 -4.1742907 -4.2087331 -4.2215338 -4.2143331 -4.2004747 -4.1970205][-4.1888986 -4.1793375 -4.1416788 -4.0847044 -4.0195379 -3.953846 -3.9107611 -3.9445314 -4.0318356 -4.1077108 -4.1532664 -4.1666427 -4.1543636 -4.1377616 -4.1380649][-4.174201 -4.1727843 -4.136538 -4.0682344 -3.9817669 -3.8901298 -3.81105 -3.8336022 -3.9464107 -4.0443554 -4.0994458 -4.1157355 -4.1060681 -4.0973706 -4.1078086][-4.1593246 -4.1622658 -4.1365461 -4.0738888 -3.9903266 -3.8918469 -3.7855265 -3.7776194 -3.8986411 -4.0195174 -4.08166 -4.0973911 -4.0852404 -4.0788803 -4.0970092][-4.1659842 -4.1682138 -4.1554227 -4.1149726 -4.052578 -3.9707499 -3.8610933 -3.8119991 -3.9088247 -4.0349178 -4.0991774 -4.1119742 -4.0945759 -4.0806994 -4.0906239][-4.18348 -4.1873074 -4.1842442 -4.1686268 -4.1323538 -4.0784369 -3.9937131 -3.9305165 -3.9863455 -4.0839181 -4.1341105 -4.1390157 -4.1119137 -4.0863419 -4.0857997][-4.1813297 -4.1877818 -4.1907692 -4.1895423 -4.1777196 -4.1543694 -4.1055665 -4.0601544 -4.0840592 -4.1418343 -4.1700621 -4.1596818 -4.1205173 -4.08246 -4.0746489][-4.1686158 -4.1764493 -4.1821809 -4.1861119 -4.1884694 -4.1862354 -4.1643076 -4.1421437 -4.152627 -4.1816373 -4.1912308 -4.1702633 -4.1256871 -4.0885477 -4.0758824][-4.1603107 -4.1710272 -4.1785502 -4.1859827 -4.196754 -4.2041183 -4.199872 -4.1938543 -4.1981525 -4.2093291 -4.2099762 -4.1916165 -4.1583285 -4.13257 -4.1231833][-4.1688771 -4.1820841 -4.1952467 -4.2076693 -4.220048 -4.2294111 -4.2352538 -4.2367549 -4.2392435 -4.2449956 -4.2433119 -4.2285743 -4.2061677 -4.1923666 -4.1862259][-4.1863389 -4.1983151 -4.2154388 -4.2302737 -4.2427969 -4.2505107 -4.256587 -4.2586269 -4.2622867 -4.2682042 -4.2669687 -4.2572713 -4.2446423 -4.239223 -4.2375083][-4.2142997 -4.2226186 -4.2368107 -4.24931 -4.2596087 -4.265029 -4.2707477 -4.2731156 -4.2783828 -4.2861309 -4.285574 -4.2812753 -4.2767186 -4.2776976 -4.2767153]]...]
INFO - root - 2017-12-05 15:05:18.150237: step 17110, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 79h:26m:23s remains)
INFO - root - 2017-12-05 15:05:27.269574: step 17120, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 79h:08m:06s remains)
INFO - root - 2017-12-05 15:05:36.264026: step 17130, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 80h:59m:45s remains)
INFO - root - 2017-12-05 15:05:45.412409: step 17140, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 80h:27m:44s remains)
INFO - root - 2017-12-05 15:05:54.402619: step 17150, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 79h:27m:14s remains)
INFO - root - 2017-12-05 15:06:03.539746: step 17160, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 80h:03m:06s remains)
INFO - root - 2017-12-05 15:06:12.584293: step 17170, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 80h:06m:05s remains)
INFO - root - 2017-12-05 15:06:21.600360: step 17180, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 75h:25m:47s remains)
INFO - root - 2017-12-05 15:06:30.819539: step 17190, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 82h:50m:18s remains)
INFO - root - 2017-12-05 15:06:39.875716: step 17200, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 79h:56m:35s remains)
2017-12-05 15:06:40.662929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2549372 -4.2498059 -4.2434788 -4.2412095 -4.2445016 -4.2495003 -4.2505927 -4.2493749 -4.2488914 -4.254117 -4.26284 -4.2672405 -4.2612524 -4.2480235 -4.2317452][-4.2515268 -4.2484908 -4.2402983 -4.2331305 -4.2309079 -4.2333918 -4.2336421 -4.2333565 -4.2364888 -4.2441211 -4.2539444 -4.2578726 -4.2528758 -4.2413507 -4.2278275][-4.2316842 -4.2279778 -4.2158523 -4.2028394 -4.1965089 -4.1985269 -4.2012372 -4.2057405 -4.2150373 -4.2271667 -4.2395873 -4.2443595 -4.240067 -4.2325521 -4.223763][-4.1961966 -4.1886268 -4.1676526 -4.1439476 -4.1295142 -4.1293688 -4.1369052 -4.1489348 -4.1661873 -4.1857228 -4.20522 -4.21776 -4.2202044 -4.2186022 -4.2166648][-4.1465955 -4.1412964 -4.1144533 -4.0805464 -4.058322 -4.0549335 -4.0632854 -4.07461 -4.0919819 -4.114459 -4.1392846 -4.162426 -4.1789603 -4.1908574 -4.1999726][-4.1067042 -4.1123686 -4.0866818 -4.048027 -4.0221982 -4.0157218 -4.0168881 -4.0206928 -4.0330086 -4.0529084 -4.07725 -4.1036096 -4.1297994 -4.1526308 -4.1727724][-4.0946956 -4.1085663 -4.0869951 -4.0523491 -4.0292811 -4.0178595 -4.0074186 -3.9985723 -4.0016165 -4.0186353 -4.04206 -4.0682826 -4.1001759 -4.12976 -4.1549892][-4.1071873 -4.1279979 -4.1170278 -4.0935583 -4.0774078 -4.0640626 -4.0445776 -4.0210876 -4.0105572 -4.0189166 -4.0357733 -4.0580893 -4.0902352 -4.125103 -4.1546693][-4.1545544 -4.1767559 -4.1741195 -4.1616225 -4.1522722 -4.1430597 -4.1254764 -4.0997205 -4.0819697 -4.0792942 -4.0849876 -4.0987487 -4.1234927 -4.15421 -4.1812358][-4.2249951 -4.2389269 -4.2360611 -4.2275305 -4.2201896 -4.2125182 -4.1999092 -4.1832352 -4.1684828 -4.1628361 -4.1619282 -4.1679296 -4.1836429 -4.2043481 -4.2232375][-4.2858033 -4.2910743 -4.2838125 -4.2761378 -4.272377 -4.26851 -4.2628479 -4.256547 -4.2479739 -4.241868 -4.237155 -4.2377253 -4.2437587 -4.254559 -4.264987][-4.3171439 -4.318274 -4.3077493 -4.3009109 -4.2998643 -4.300209 -4.3015394 -4.3025761 -4.2990327 -4.29365 -4.2895451 -4.2884107 -4.2882457 -4.2911882 -4.2951097][-4.3183007 -4.3234415 -4.31405 -4.307508 -4.3062205 -4.3094058 -4.3145905 -4.3176808 -4.3144193 -4.3082967 -4.3057594 -4.3055477 -4.303175 -4.3020329 -4.3024578][-4.2997093 -4.3109202 -4.3065953 -4.3012352 -4.3002605 -4.3058376 -4.3144689 -4.3194027 -4.3148818 -4.3056431 -4.302011 -4.3015337 -4.2987046 -4.2958846 -4.2957897][-4.28268 -4.3003235 -4.3015532 -4.2973995 -4.2956491 -4.3004236 -4.3102379 -4.3169937 -4.312808 -4.3012748 -4.2944617 -4.2927852 -4.2908397 -4.2885485 -4.288476]]...]
INFO - root - 2017-12-05 15:06:49.776040: step 17210, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 80h:51m:15s remains)
INFO - root - 2017-12-05 15:06:58.845701: step 17220, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 79h:32m:56s remains)
INFO - root - 2017-12-05 15:07:08.080880: step 17230, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 82h:10m:29s remains)
INFO - root - 2017-12-05 15:07:17.200930: step 17240, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 76h:30m:06s remains)
INFO - root - 2017-12-05 15:07:26.418052: step 17250, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 80h:23m:17s remains)
INFO - root - 2017-12-05 15:07:35.489944: step 17260, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 79h:27m:57s remains)
INFO - root - 2017-12-05 15:07:44.594962: step 17270, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 79h:44m:40s remains)
INFO - root - 2017-12-05 15:07:53.576368: step 17280, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 80h:56m:46s remains)
INFO - root - 2017-12-05 15:08:02.816951: step 17290, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 81h:14m:16s remains)
INFO - root - 2017-12-05 15:08:12.148856: step 17300, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.936 sec/batch; 81h:55m:11s remains)
2017-12-05 15:08:13.012609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2277946 -4.2473178 -4.2637711 -4.2677794 -4.2586231 -4.26149 -4.2769871 -4.2871513 -4.2868352 -4.2878003 -4.2903271 -4.28584 -4.2858934 -4.2915492 -4.289331][-4.2472053 -4.2598963 -4.2724905 -4.277401 -4.2714171 -4.274981 -4.2899013 -4.2970529 -4.2874365 -4.278007 -4.2725673 -4.2629871 -4.2589736 -4.2599273 -4.2541466][-4.2621689 -4.2680187 -4.2751441 -4.2787957 -4.2737846 -4.2739139 -4.2879696 -4.2959461 -4.2861977 -4.273469 -4.2620487 -4.246254 -4.2368369 -4.2319164 -4.2207861][-4.2626867 -4.2591119 -4.2622609 -4.2651081 -4.2591105 -4.254364 -4.2643247 -4.2767754 -4.2743869 -4.2676077 -4.2564135 -4.2387161 -4.226985 -4.2207808 -4.2103381][-4.2512279 -4.2419848 -4.2421856 -4.2448711 -4.2399764 -4.2307529 -4.2336311 -4.2469954 -4.2537961 -4.2552204 -4.2496152 -4.2344713 -4.225872 -4.2226839 -4.2142792][-4.2273536 -4.2117972 -4.2050943 -4.2053418 -4.2014685 -4.1934628 -4.194119 -4.2090368 -4.2253304 -4.237155 -4.2424726 -4.236805 -4.2333431 -4.2313304 -4.2232223][-4.1908579 -4.1648383 -4.1453238 -4.1350269 -4.1261683 -4.1240072 -4.1292005 -4.1455984 -4.1683617 -4.191112 -4.2071896 -4.2126937 -4.2180638 -4.2219167 -4.2194715][-4.1562967 -4.1203032 -4.0855918 -4.0617032 -4.0422716 -4.0409708 -4.0472631 -4.0605264 -4.0856833 -4.1192141 -4.1522679 -4.1738739 -4.1922474 -4.2075438 -4.21693][-4.1324024 -4.0909123 -4.0457149 -4.0089583 -3.9810963 -3.9775517 -3.98613 -3.9965219 -4.0201626 -4.0632553 -4.1177292 -4.1604505 -4.19336 -4.2204561 -4.2396269][-4.1319995 -4.09635 -4.054523 -4.0172706 -3.988719 -3.9816897 -3.988785 -3.9988155 -4.0150442 -4.051888 -4.11144 -4.1640925 -4.2043033 -4.2359748 -4.2571287][-4.1611958 -4.1371713 -4.1070609 -4.0833287 -4.0654297 -4.0624542 -4.0695128 -4.0795197 -4.0874834 -4.10834 -4.1514082 -4.1940103 -4.2280583 -4.2515035 -4.2667532][-4.1915226 -4.1787634 -4.1603332 -4.1467519 -4.1408606 -4.1467571 -4.1607995 -4.1746883 -4.1783252 -4.1862092 -4.206285 -4.22937 -4.2461352 -4.2535505 -4.2592497][-4.1937108 -4.1892772 -4.1812468 -4.1766224 -4.1834168 -4.2000194 -4.2227945 -4.24131 -4.245369 -4.2431469 -4.2433462 -4.2472415 -4.2471676 -4.2407274 -4.238451][-4.1653795 -4.1666875 -4.1704731 -4.1762075 -4.1961746 -4.2209778 -4.2479758 -4.2673936 -4.2737412 -4.2664614 -4.254467 -4.246933 -4.237205 -4.2231212 -4.215354][-4.1286693 -4.1359291 -4.1444626 -4.1539717 -4.17417 -4.1980166 -4.2246132 -4.2416477 -4.2497926 -4.2433462 -4.2286153 -4.2178397 -4.2066216 -4.1941066 -4.1861391]]...]
INFO - root - 2017-12-05 15:08:22.202016: step 17310, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 81h:03m:36s remains)
INFO - root - 2017-12-05 15:08:31.228496: step 17320, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 80h:19m:01s remains)
INFO - root - 2017-12-05 15:08:40.399291: step 17330, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.925 sec/batch; 80h:59m:45s remains)
INFO - root - 2017-12-05 15:08:49.428652: step 17340, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 78h:47m:56s remains)
INFO - root - 2017-12-05 15:08:58.394062: step 17350, loss = 2.05, batch loss = 2.00 (9.9 examples/sec; 0.812 sec/batch; 71h:03m:25s remains)
INFO - root - 2017-12-05 15:09:07.556905: step 17360, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 82h:18m:23s remains)
INFO - root - 2017-12-05 15:09:16.805075: step 17370, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 77h:58m:48s remains)
INFO - root - 2017-12-05 15:09:25.754577: step 17380, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 76h:51m:34s remains)
INFO - root - 2017-12-05 15:09:34.772818: step 17390, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 74h:47m:35s remains)
INFO - root - 2017-12-05 15:09:43.936671: step 17400, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 77h:00m:35s remains)
2017-12-05 15:09:44.737673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2302661 -4.168777 -4.10642 -4.0547128 -4.0555391 -4.1065426 -4.1692858 -4.2125678 -4.237637 -4.2498741 -4.2512822 -4.2481418 -4.23559 -4.21572 -4.2026706][-4.2201529 -4.1545539 -4.0810609 -4.0194592 -4.0098467 -4.0590134 -4.1350327 -4.1979356 -4.2351108 -4.2530007 -4.2552447 -4.2489491 -4.230165 -4.2057 -4.1866312][-4.2144737 -4.146184 -4.0614753 -3.98813 -3.9631548 -4.0070829 -4.0921516 -4.1704655 -4.2224488 -4.2513728 -4.2615709 -4.2536459 -4.2275391 -4.1956906 -4.1698551][-4.2109051 -4.1376953 -4.0453949 -3.9638832 -3.9255018 -3.964884 -4.0593882 -4.1462727 -4.2058616 -4.2438884 -4.2555056 -4.2434974 -4.21101 -4.1774817 -4.151154][-4.2090225 -4.1314259 -4.0315933 -3.9380236 -3.8841538 -3.9181395 -4.0182018 -4.106916 -4.166069 -4.2094407 -4.2209063 -4.2037959 -4.1735368 -4.1486635 -4.1340795][-4.20608 -4.1232386 -4.0138316 -3.9020963 -3.8282523 -3.8550711 -3.9556198 -4.0390792 -4.0915 -4.132143 -4.1406593 -4.118783 -4.0979438 -4.0965376 -4.1051197][-4.2002759 -4.1078205 -3.9878621 -3.860806 -3.7716296 -3.7925935 -3.891068 -3.9654176 -4.0095763 -4.0463347 -4.0561104 -4.0389347 -4.0359983 -4.0587869 -4.0875454][-4.199564 -4.1007671 -3.9795067 -3.8519504 -3.7640326 -3.7821441 -3.8701463 -3.9313486 -3.9673252 -4.0055981 -4.0286975 -4.0306554 -4.0404649 -4.0654721 -4.0912919][-4.2119389 -4.1183267 -4.0076451 -3.8962991 -3.8239527 -3.8382375 -3.9066625 -3.9511662 -3.9771874 -4.0120029 -4.0429783 -4.0616441 -4.0785584 -4.0919204 -4.1014156][-4.2354636 -4.158432 -4.0695763 -3.9847512 -3.9317024 -3.9397416 -3.9861455 -4.0178895 -4.0372367 -4.0623941 -4.0897779 -4.1092262 -4.1227717 -4.1247663 -4.1187668][-4.2626138 -4.2076688 -4.146369 -4.0912094 -4.05747 -4.0607171 -4.0894694 -4.1106482 -4.1243443 -4.1377125 -4.1521926 -4.1632032 -4.171102 -4.166625 -4.1531377][-4.2843442 -4.2479062 -4.2088251 -4.1748619 -4.1531277 -4.1528721 -4.1710019 -4.1858091 -4.1946383 -4.2013636 -4.2080393 -4.2135549 -4.21763 -4.2130413 -4.1997862][-4.2998266 -4.2751236 -4.2475615 -4.2245946 -4.2092476 -4.2074695 -4.2183318 -4.2282896 -4.2345171 -4.239428 -4.2441216 -4.248857 -4.2529793 -4.2509413 -4.2425609][-4.3086033 -4.2921081 -4.2715092 -4.2531991 -4.2405467 -4.2377911 -4.2429047 -4.2499242 -4.2565055 -4.2619867 -4.2657194 -4.2697506 -4.2743979 -4.274159 -4.2699652][-4.3139658 -4.3041105 -4.2907333 -4.2781448 -4.2694907 -4.26705 -4.269628 -4.2743249 -4.2792749 -4.2836552 -4.2865548 -4.2898636 -4.2932334 -4.293148 -4.2902555]]...]
INFO - root - 2017-12-05 15:09:53.700909: step 17410, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.859 sec/batch; 75h:11m:06s remains)
INFO - root - 2017-12-05 15:10:02.789354: step 17420, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 80h:41m:10s remains)
INFO - root - 2017-12-05 15:10:11.883940: step 17430, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 81h:36m:50s remains)
INFO - root - 2017-12-05 15:10:21.053839: step 17440, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 81h:09m:31s remains)
INFO - root - 2017-12-05 15:10:30.018996: step 17450, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 77h:04m:06s remains)
INFO - root - 2017-12-05 15:10:39.048137: step 17460, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.814 sec/batch; 71h:13m:49s remains)
INFO - root - 2017-12-05 15:10:48.207216: step 17470, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.915 sec/batch; 80h:02m:49s remains)
INFO - root - 2017-12-05 15:10:57.417425: step 17480, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 79h:40m:03s remains)
INFO - root - 2017-12-05 15:11:06.488185: step 17490, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 80h:11m:05s remains)
INFO - root - 2017-12-05 15:11:15.669774: step 17500, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 81h:04m:05s remains)
2017-12-05 15:11:16.413054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2784038 -4.2709222 -4.2629318 -4.2654862 -4.2773066 -4.2838697 -4.291944 -4.3037734 -4.3112621 -4.3166904 -4.3189607 -4.3178844 -4.3156905 -4.3136644 -4.3114433][-4.2388792 -4.2280416 -4.2184978 -4.2218871 -4.2372203 -4.2446237 -4.2524419 -4.2699184 -4.285038 -4.2990251 -4.3057952 -4.3043203 -4.2999711 -4.297471 -4.29618][-4.181025 -4.1745424 -4.1682343 -4.1710687 -4.1827636 -4.185482 -4.1891675 -4.2071147 -4.2294097 -4.256917 -4.2770576 -4.283668 -4.2813144 -4.2794075 -4.2774391][-4.115416 -4.1287055 -4.1316919 -4.1314173 -4.1359687 -4.1300454 -4.1173286 -4.1198988 -4.1405392 -4.1864429 -4.2283478 -4.2496176 -4.2535534 -4.2538977 -4.2504649][-4.0455856 -4.0878806 -4.1069193 -4.1071215 -4.1049018 -4.0869231 -4.0476074 -4.0134535 -4.0225887 -4.093473 -4.1666179 -4.2054954 -4.2161865 -4.2167263 -4.2081351][-3.9646735 -4.0366812 -4.0790086 -4.0844793 -4.0766277 -4.0445614 -3.9685094 -3.8776689 -3.87006 -3.9799061 -4.0925813 -4.153347 -4.1745505 -4.174788 -4.1544762][-3.910918 -3.9996505 -4.0549955 -4.0628037 -4.0507751 -4.0041161 -3.8824356 -3.717783 -3.6795957 -3.8412952 -4.0049114 -4.09247 -4.1265841 -4.1264825 -4.0957785][-3.9223697 -4.0039654 -4.0539293 -4.0581512 -4.0476775 -3.9992495 -3.8553441 -3.6433017 -3.5595617 -3.7391307 -3.9335771 -4.0365038 -4.080843 -4.0878115 -4.0526671][-3.9956622 -4.0544977 -4.0900311 -4.0937409 -4.0926647 -4.0566292 -3.9360533 -3.7563736 -3.6592708 -3.7819917 -3.9486549 -4.0381308 -4.0800195 -4.0948009 -4.0681591][-4.0825524 -4.1177812 -4.1388392 -4.1437573 -4.1517229 -4.129436 -4.0475335 -3.9312949 -3.8524079 -3.9124761 -4.0226955 -4.0845132 -4.1127577 -4.1289549 -4.1166906][-4.1482673 -4.1616068 -4.1692367 -4.174891 -4.1888113 -4.1814008 -4.1347437 -4.0713463 -4.0159917 -4.0361176 -4.0985484 -4.1349258 -4.1529284 -4.1664591 -4.1615748][-4.1879549 -4.1861715 -4.1826706 -4.187335 -4.2061539 -4.2118449 -4.1908765 -4.1590004 -4.1226692 -4.1247396 -4.1572895 -4.1739874 -4.1856627 -4.196085 -4.1962724][-4.2106729 -4.1998153 -4.1911755 -4.1959634 -4.2145848 -4.22449 -4.21803 -4.2040644 -4.183423 -4.1832929 -4.200069 -4.2054863 -4.20995 -4.217031 -4.2207][-4.2525163 -4.2389975 -4.2295671 -4.2319636 -4.2440953 -4.2526579 -4.2539783 -4.2517614 -4.245883 -4.2490134 -4.2569218 -4.2563128 -4.2562633 -4.2595263 -4.2619448][-4.2984266 -4.289588 -4.2831359 -4.2828426 -4.2882733 -4.2932477 -4.2968674 -4.3002458 -4.3018775 -4.3063211 -4.3097215 -4.3086066 -4.3067088 -4.3063793 -4.3065057]]...]
INFO - root - 2017-12-05 15:11:25.424934: step 17510, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 79h:09m:14s remains)
INFO - root - 2017-12-05 15:11:34.595855: step 17520, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 80h:49m:22s remains)
INFO - root - 2017-12-05 15:11:43.737869: step 17530, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 78h:18m:11s remains)
INFO - root - 2017-12-05 15:11:52.733988: step 17540, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.955 sec/batch; 83h:32m:00s remains)
INFO - root - 2017-12-05 15:12:01.879369: step 17550, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 83h:21m:17s remains)
INFO - root - 2017-12-05 15:12:11.132624: step 17560, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 81h:03m:11s remains)
INFO - root - 2017-12-05 15:12:20.138410: step 17570, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 78h:06m:22s remains)
INFO - root - 2017-12-05 15:12:29.384288: step 17580, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 78h:20m:04s remains)
INFO - root - 2017-12-05 15:12:38.514150: step 17590, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 81h:57m:11s remains)
INFO - root - 2017-12-05 15:12:47.453021: step 17600, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 77h:32m:54s remains)
2017-12-05 15:12:48.232634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2778096 -4.2811193 -4.2778473 -4.2674041 -4.2560658 -4.2561097 -4.25761 -4.2604556 -4.2707167 -4.2815752 -4.2815504 -4.2709746 -4.2592368 -4.2553644 -4.2664375][-4.2591777 -4.2653294 -4.2591209 -4.2414608 -4.2213135 -4.2160273 -4.2196169 -4.2292385 -4.2482138 -4.2623515 -4.2587981 -4.241868 -4.2220349 -4.2141943 -4.2303967][-4.2578177 -4.2659254 -4.2563572 -4.2305388 -4.1990886 -4.1814113 -4.1859274 -4.2060466 -4.2358694 -4.2552118 -4.2501345 -4.2277064 -4.1979809 -4.1852989 -4.2037406][-4.2645435 -4.2710133 -4.2580404 -4.2247272 -4.1783261 -4.1423192 -4.1452594 -4.1815391 -4.2265553 -4.2532649 -4.2503695 -4.2248306 -4.1869488 -4.1697717 -4.1885791][-4.2595382 -4.2631369 -4.246664 -4.20771 -4.1489582 -4.0961981 -4.0937481 -4.1468067 -4.2081771 -4.2429729 -4.2458181 -4.2216544 -4.1805215 -4.1597314 -4.1792388][-4.2465439 -4.2486258 -4.2292223 -4.1859159 -4.119771 -4.0567427 -4.0553412 -4.1236744 -4.1970658 -4.2378883 -4.2465982 -4.2234573 -4.1824408 -4.1595168 -4.1784534][-4.2359281 -4.2374034 -4.21614 -4.1699996 -4.0949025 -4.0232458 -4.0307231 -4.1115513 -4.1913433 -4.235424 -4.2454066 -4.2218728 -4.1849556 -4.1646776 -4.1832438][-4.2282085 -4.2293921 -4.2069135 -4.1546478 -4.0680408 -3.985121 -4.0012083 -4.0916719 -4.17705 -4.2239828 -4.2369537 -4.2166042 -4.1852803 -4.1678624 -4.1864471][-4.2165866 -4.2187357 -4.1988368 -4.1447353 -4.0541277 -3.9663618 -3.9814694 -4.0696516 -4.1554236 -4.2050352 -4.2207928 -4.204072 -4.1771722 -4.1616826 -4.18285][-4.2161098 -4.220232 -4.2032185 -4.1538568 -4.0703874 -3.9903145 -4.0032325 -4.0785422 -4.1551847 -4.2010107 -4.2154713 -4.2016106 -4.1783967 -4.1635756 -4.1856246][-4.2274756 -4.2327042 -4.2195468 -4.1793957 -4.1054978 -4.0366406 -4.0484452 -4.1092606 -4.1717138 -4.2119155 -4.2223029 -4.2075639 -4.1861043 -4.1735587 -4.1967406][-4.241364 -4.2470875 -4.2381473 -4.2059574 -4.1444464 -4.0902281 -4.09986 -4.143445 -4.1899858 -4.2235632 -4.2298236 -4.2148223 -4.193429 -4.1848145 -4.210784][-4.2573094 -4.2613173 -4.2557087 -4.2316794 -4.1878643 -4.1506457 -4.158545 -4.1862082 -4.2166471 -4.2412953 -4.2435164 -4.2293067 -4.2103271 -4.2068372 -4.2336793][-4.2718167 -4.2742438 -4.2702966 -4.253767 -4.2255187 -4.2024689 -4.2083931 -4.2233462 -4.2415447 -4.2581663 -4.259172 -4.2487407 -4.2363038 -4.2397852 -4.2644577][-4.281 -4.2854085 -4.28267 -4.2677379 -4.2462997 -4.2308645 -4.2330408 -4.2416368 -4.2542081 -4.2683897 -4.2734551 -4.2691679 -4.2635369 -4.2714996 -4.2921271]]...]
INFO - root - 2017-12-05 15:12:57.493932: step 17610, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 82h:18m:56s remains)
INFO - root - 2017-12-05 15:13:06.518146: step 17620, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 81h:07m:41s remains)
INFO - root - 2017-12-05 15:13:15.723697: step 17630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 79h:28m:59s remains)
INFO - root - 2017-12-05 15:13:24.729939: step 17640, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 81h:46m:49s remains)
INFO - root - 2017-12-05 15:13:33.933213: step 17650, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 81h:08m:45s remains)
INFO - root - 2017-12-05 15:13:43.183630: step 17660, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 78h:48m:16s remains)
INFO - root - 2017-12-05 15:13:52.275822: step 17670, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 78h:35m:45s remains)
INFO - root - 2017-12-05 15:14:01.314194: step 17680, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 77h:34m:01s remains)
INFO - root - 2017-12-05 15:14:10.588182: step 17690, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 78h:10m:18s remains)
INFO - root - 2017-12-05 15:14:19.657909: step 17700, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 78h:26m:21s remains)
2017-12-05 15:14:20.435863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1817975 -4.1809063 -4.1687317 -4.14926 -4.1502037 -4.1675577 -4.1908245 -4.2067461 -4.2129369 -4.20592 -4.1957574 -4.18895 -4.1967587 -4.217205 -4.2241678][-4.1564765 -4.1509047 -4.135901 -4.1203527 -4.1278286 -4.1493382 -4.1710072 -4.179112 -4.18036 -4.1765437 -4.1753488 -4.1765084 -4.1886473 -4.212904 -4.2216945][-4.13141 -4.1247759 -4.1105423 -4.096293 -4.1071835 -4.1299663 -4.1452422 -4.1476121 -4.1445127 -4.1450534 -4.1563497 -4.1678305 -4.1858897 -4.211123 -4.2188029][-4.1058612 -4.1067472 -4.097271 -4.0827174 -4.0912447 -4.1117268 -4.11889 -4.1126966 -4.1072125 -4.1122251 -4.1362877 -4.15973 -4.184083 -4.206389 -4.2115393][-4.1011539 -4.1100149 -4.1021562 -4.0809631 -4.080493 -4.0980949 -4.1046529 -4.0979967 -4.0965834 -4.1089821 -4.1397715 -4.1645341 -4.1864548 -4.200078 -4.1984844][-4.1169767 -4.1232057 -4.1123595 -4.085124 -4.0754719 -4.0888233 -4.0962782 -4.0957637 -4.1060386 -4.1262183 -4.1504178 -4.1642523 -4.1831212 -4.2005968 -4.2019262][-4.1196795 -4.1236792 -4.110909 -4.07609 -4.0559807 -4.0588803 -4.063942 -4.074296 -4.0983505 -4.1218405 -4.1388841 -4.147193 -4.1708941 -4.2013474 -4.2056823][-4.1083441 -4.1103048 -4.1003761 -4.0654073 -4.0404553 -4.0365767 -4.0409665 -4.0596728 -4.0921006 -4.1185436 -4.1350236 -4.1466088 -4.1705 -4.1982651 -4.1977463][-4.0968895 -4.0935097 -4.0901833 -4.0712929 -4.0562696 -4.051753 -4.0515833 -4.0694242 -4.1028233 -4.1282444 -4.1417494 -4.15242 -4.1667924 -4.1847 -4.1804981][-4.1054406 -4.0964613 -4.0922618 -4.0832934 -4.0798769 -4.0779991 -4.0739832 -4.0837488 -4.1131186 -4.1354489 -4.1454515 -4.1499538 -4.1515913 -4.1624908 -4.1608205][-4.1266627 -4.1174293 -4.1084189 -4.0999184 -4.0974679 -4.095664 -4.092082 -4.0975223 -4.1218848 -4.1363063 -4.1428146 -4.1428728 -4.1409769 -4.1490135 -4.1479478][-4.1372619 -4.1357737 -4.1253233 -4.1171379 -4.1141634 -4.1120582 -4.1111784 -4.1166449 -4.1330919 -4.1398578 -4.1431141 -4.1391134 -4.1315646 -4.1291447 -4.120276][-4.1275406 -4.1377087 -4.1309795 -4.1263733 -4.1271257 -4.1292472 -4.1327891 -4.135323 -4.1372118 -4.1322002 -4.1276355 -4.1192803 -4.1048408 -4.0908813 -4.0737243][-4.1098127 -4.1339488 -4.1343775 -4.1292381 -4.1317458 -4.1400471 -4.1477919 -4.1436195 -4.1303463 -4.1135421 -4.1050448 -4.1019855 -4.0887494 -4.0640993 -4.037313][-4.0973916 -4.122375 -4.1256661 -4.1171169 -4.1203928 -4.1345634 -4.1454329 -4.1354885 -4.1121397 -4.0862579 -4.0835109 -4.0966158 -4.0936527 -4.0672569 -4.0308223]]...]
INFO - root - 2017-12-05 15:14:29.415443: step 17710, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 78h:54m:56s remains)
INFO - root - 2017-12-05 15:14:38.745217: step 17720, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 81h:23m:49s remains)
INFO - root - 2017-12-05 15:14:47.883446: step 17730, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 78h:03m:43s remains)
INFO - root - 2017-12-05 15:14:56.842645: step 17740, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 76h:48m:06s remains)
INFO - root - 2017-12-05 15:15:06.004549: step 17750, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 78h:05m:55s remains)
INFO - root - 2017-12-05 15:15:14.991017: step 17760, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 77h:44m:04s remains)
INFO - root - 2017-12-05 15:15:24.047152: step 17770, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 79h:58m:42s remains)
INFO - root - 2017-12-05 15:15:33.238129: step 17780, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 75h:48m:25s remains)
INFO - root - 2017-12-05 15:15:42.456021: step 17790, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 80h:10m:59s remains)
INFO - root - 2017-12-05 15:15:51.766736: step 17800, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 80h:39m:52s remains)
2017-12-05 15:15:52.532912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3067584 -4.2794886 -4.2528048 -4.236908 -4.2408347 -4.2564096 -4.2714357 -4.2813287 -4.2864342 -4.2917256 -4.2985992 -4.3040142 -4.3062229 -4.3029118 -4.2955832][-4.2947016 -4.2621155 -4.2360868 -4.22668 -4.2363687 -4.2522168 -4.2631993 -4.2699747 -4.2740536 -4.2799282 -4.2864032 -4.2885647 -4.2861605 -4.2791128 -4.2712545][-4.2850704 -4.2551951 -4.2367449 -4.2343583 -4.2434387 -4.2497978 -4.2489219 -4.2473192 -4.2483954 -4.254806 -4.2622223 -4.2630167 -4.2593031 -4.2527857 -4.2501488][-4.2854838 -4.2647295 -4.253366 -4.2500663 -4.2492862 -4.2395763 -4.2210536 -4.2060428 -4.2032633 -4.2164783 -4.231719 -4.2394409 -4.2409825 -4.2408218 -4.2469516][-4.2942328 -4.2823944 -4.2739997 -4.2642665 -4.2506924 -4.2231407 -4.1834211 -4.1493998 -4.1469531 -4.1763892 -4.2068048 -4.2259431 -4.2361636 -4.2453561 -4.2580104][-4.3016438 -4.292614 -4.2834826 -4.2663279 -4.2388549 -4.193408 -4.1310811 -4.083868 -4.0984464 -4.1533275 -4.1976404 -4.2235384 -4.2391253 -4.2551312 -4.2709308][-4.292222 -4.27934 -4.2663922 -4.2417383 -4.2011037 -4.14312 -4.0737 -4.0373211 -4.0821533 -4.1529694 -4.1989961 -4.2223587 -4.2363157 -4.2518544 -4.2694874][-4.27709 -4.2612219 -4.2457309 -4.2163725 -4.1699924 -4.1148477 -4.0673308 -4.0654964 -4.1188455 -4.1768146 -4.2097664 -4.2228446 -4.2278628 -4.2388673 -4.2586102][-4.2729192 -4.2538586 -4.2346334 -4.2020307 -4.1586304 -4.121201 -4.1095405 -4.13036 -4.1685247 -4.2010388 -4.2175879 -4.2180581 -4.2110357 -4.2173915 -4.2412491][-4.2725439 -4.250627 -4.2268991 -4.1929727 -4.1593194 -4.1445532 -4.1593 -4.183567 -4.2020049 -4.2128134 -4.2178206 -4.2121716 -4.1986418 -4.2006087 -4.2274189][-4.2724113 -4.2452369 -4.2158847 -4.1840396 -4.1629057 -4.16798 -4.1947966 -4.2160311 -4.2221117 -4.2199469 -4.2164903 -4.2084308 -4.1941109 -4.1938252 -4.2196875][-4.2591052 -4.2281547 -4.1952863 -4.1674619 -4.1557593 -4.1712089 -4.2018504 -4.2212524 -4.224905 -4.2186975 -4.2117896 -4.2049427 -4.1951585 -4.1959758 -4.2175121][-4.2335272 -4.2040868 -4.1727724 -4.1520872 -4.1465149 -4.1642756 -4.1915064 -4.208437 -4.2143421 -4.2103953 -4.2043819 -4.2001514 -4.1947327 -4.1969633 -4.213243][-4.2081251 -4.1836872 -4.1571541 -4.1438103 -4.1450539 -4.1618805 -4.1824474 -4.1952686 -4.2024837 -4.2001171 -4.1959386 -4.1945748 -4.1922317 -4.19665 -4.2102275][-4.1942611 -4.1758389 -4.1575718 -4.1510811 -4.1581111 -4.1738768 -4.1891465 -4.1983771 -4.2044573 -4.2019768 -4.1988049 -4.1987524 -4.19769 -4.2024274 -4.2127414]]...]
INFO - root - 2017-12-05 15:16:01.528830: step 17810, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 76h:33m:02s remains)
INFO - root - 2017-12-05 15:16:10.521328: step 17820, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 78h:25m:46s remains)
INFO - root - 2017-12-05 15:16:19.572570: step 17830, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 82h:10m:55s remains)
INFO - root - 2017-12-05 15:16:28.699452: step 17840, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 78h:22m:39s remains)
INFO - root - 2017-12-05 15:16:37.800243: step 17850, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 80h:42m:53s remains)
INFO - root - 2017-12-05 15:16:46.831934: step 17860, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 83h:00m:22s remains)
INFO - root - 2017-12-05 15:16:55.870062: step 17870, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 77h:08m:17s remains)
INFO - root - 2017-12-05 15:17:05.073758: step 17880, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 78h:37m:08s remains)
INFO - root - 2017-12-05 15:17:14.325843: step 17890, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 82h:27m:46s remains)
INFO - root - 2017-12-05 15:17:23.196261: step 17900, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 79h:24m:26s remains)
2017-12-05 15:17:23.963338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1662183 -4.18223 -4.2065644 -4.2144403 -4.2047215 -4.1964073 -4.1912756 -4.1828403 -4.1667061 -4.1470842 -4.1289096 -4.1253366 -4.1473303 -4.1833596 -4.2106409][-4.2007194 -4.20711 -4.2221065 -4.2241616 -4.2150979 -4.2109017 -4.2066922 -4.2010365 -4.189065 -4.1741862 -4.1564951 -4.148695 -4.1652317 -4.1906004 -4.206109][-4.2443347 -4.2433991 -4.245224 -4.2420692 -4.2364864 -4.2343969 -4.2326145 -4.2316341 -4.2277026 -4.2219305 -4.2056532 -4.1891785 -4.196147 -4.2113447 -4.2174811][-4.26028 -4.2622948 -4.2612205 -4.2559624 -4.2528739 -4.2508769 -4.2534633 -4.2604194 -4.2662597 -4.2699375 -4.2573962 -4.2387409 -4.2377119 -4.2441211 -4.2424755][-4.238637 -4.2531304 -4.2578578 -4.251461 -4.2470117 -4.2457075 -4.2485785 -4.2584467 -4.2736592 -4.2867584 -4.287704 -4.2799964 -4.2764082 -4.2773757 -4.2744722][-4.1903009 -4.2152925 -4.2226763 -4.21391 -4.2106376 -4.211937 -4.2156744 -4.2280765 -4.2511764 -4.2724338 -4.290164 -4.299489 -4.2969441 -4.2952356 -4.2958584][-4.1346979 -4.1667075 -4.1730061 -4.1616015 -4.1598272 -4.1634779 -4.1713815 -4.1908603 -4.2192931 -4.2408257 -4.2674975 -4.2848668 -4.2818756 -4.2812366 -4.29068][-4.1045804 -4.1291404 -4.1251516 -4.1084681 -4.1094065 -4.1145811 -4.1248069 -4.1477809 -4.1743975 -4.1951046 -4.2243457 -4.2450318 -4.2462997 -4.2509909 -4.2664046][-4.1318588 -4.1371803 -4.119257 -4.0989189 -4.0985041 -4.0965371 -4.0981731 -4.11444 -4.1306553 -4.1475725 -4.1775122 -4.1981072 -4.2040372 -4.2142739 -4.2350435][-4.1798453 -4.1715436 -4.1494775 -4.1313667 -4.1294007 -4.122529 -4.1181054 -4.126297 -4.1309519 -4.1350932 -4.1488538 -4.1555414 -4.1607475 -4.1770091 -4.2055907][-4.22034 -4.2139449 -4.1989422 -4.1907444 -4.1900706 -4.1830573 -4.1751819 -4.1716766 -4.1606941 -4.1458664 -4.1338711 -4.1231751 -4.1286497 -4.1540465 -4.1911712][-4.2176075 -4.2204418 -4.2163067 -4.2173729 -4.2196264 -4.2147908 -4.2049751 -4.1921277 -4.1724544 -4.1454859 -4.1141829 -4.0895739 -4.0922 -4.1259055 -4.1705637][-4.2154317 -4.2210913 -4.2213411 -4.2254105 -4.2283888 -4.22455 -4.2134323 -4.1993179 -4.1812949 -4.153317 -4.1148176 -4.08212 -4.0811019 -4.1165934 -4.1613121][-4.227623 -4.23058 -4.2321649 -4.2364693 -4.2384977 -4.235333 -4.2275157 -4.2175484 -4.2041683 -4.1790833 -4.1432266 -4.1092825 -4.1046581 -4.133307 -4.1721406][-4.2414994 -4.2416921 -4.2426257 -4.2447224 -4.2452459 -4.2420769 -4.2353249 -4.2286534 -4.22138 -4.2034345 -4.1733809 -4.1440372 -4.1369038 -4.1569414 -4.1848764]]...]
INFO - root - 2017-12-05 15:17:33.183735: step 17910, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 82h:14m:52s remains)
INFO - root - 2017-12-05 15:17:42.272540: step 17920, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 80h:03m:26s remains)
INFO - root - 2017-12-05 15:17:51.352952: step 17930, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 79h:05m:16s remains)
INFO - root - 2017-12-05 15:18:00.395132: step 17940, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 79h:10m:19s remains)
INFO - root - 2017-12-05 15:18:09.604984: step 17950, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 80h:08m:16s remains)
INFO - root - 2017-12-05 15:18:18.585620: step 17960, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 76h:05m:38s remains)
INFO - root - 2017-12-05 15:18:27.698716: step 17970, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 77h:44m:23s remains)
INFO - root - 2017-12-05 15:18:36.838788: step 17980, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 81h:49m:37s remains)
INFO - root - 2017-12-05 15:18:45.868200: step 17990, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 73h:51m:28s remains)
INFO - root - 2017-12-05 15:18:55.011964: step 18000, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 78h:28m:33s remains)
2017-12-05 15:18:55.740923: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.328228 -4.3127379 -4.2951055 -4.2735305 -4.2513437 -4.230217 -4.2196245 -4.222919 -4.2369576 -4.2550759 -4.2737727 -4.2862315 -4.291141 -4.2870359 -4.2701979][-4.3244228 -4.3052912 -4.2819896 -4.2524018 -4.2250977 -4.2008648 -4.190887 -4.1939912 -4.2109423 -4.2367277 -4.2598495 -4.2739754 -4.2763796 -4.2690749 -4.2483735][-4.3200593 -4.2992377 -4.2718596 -4.2350278 -4.2027621 -4.1761022 -4.1660051 -4.1690884 -4.1886053 -4.2193837 -4.2441998 -4.261476 -4.2636833 -4.2540007 -4.2310629][-4.3151345 -4.294189 -4.2646718 -4.2243943 -4.1892881 -4.1626639 -4.1541533 -4.1568246 -4.1756663 -4.2047014 -4.2286954 -4.2511091 -4.2563477 -4.2468877 -4.2253203][-4.3098454 -4.289957 -4.2617097 -4.2250333 -4.19107 -4.1663394 -4.1590633 -4.158998 -4.1680946 -4.1885633 -4.2137418 -4.2433977 -4.2537055 -4.2464252 -4.2303495][-4.3052092 -4.2860065 -4.2600293 -4.2300324 -4.201498 -4.1762137 -4.1654329 -4.1562991 -4.1503482 -4.1595936 -4.1841407 -4.2229128 -4.2457643 -4.2462053 -4.2410541][-4.3019252 -4.2816896 -4.2567592 -4.2292042 -4.2003169 -4.1746535 -4.1575456 -4.1375651 -4.1153889 -4.118937 -4.148428 -4.2001472 -4.2409306 -4.2515712 -4.2519684][-4.300487 -4.2782831 -4.2514334 -4.2204442 -4.1866488 -4.1549811 -4.12817 -4.097569 -4.0650315 -4.0689549 -4.1093483 -4.1740746 -4.2302461 -4.25048 -4.2517834][-4.3006973 -4.2786536 -4.2502966 -4.21614 -4.1763015 -4.1357737 -4.0977859 -4.0543156 -4.0157709 -4.0243926 -4.0779924 -4.1541681 -4.2157364 -4.2405128 -4.2412233][-4.3022237 -4.2824111 -4.2570271 -4.2226305 -4.1789126 -4.13264 -4.0877762 -4.0353432 -3.9933691 -4.004375 -4.0687475 -4.151825 -4.2101283 -4.2312236 -4.2291236][-4.3041315 -4.2859254 -4.2624011 -4.2285218 -4.1883197 -4.1469569 -4.1096873 -4.0614376 -4.0218353 -4.0330658 -4.0967584 -4.1739435 -4.2242503 -4.2415786 -4.2378154][-4.30609 -4.287075 -4.2613912 -4.2280989 -4.19309 -4.1607237 -4.1383152 -4.1050324 -4.0790906 -4.0904756 -4.1399341 -4.1987877 -4.2390423 -4.2539587 -4.2488966][-4.3080611 -4.2869163 -4.2568312 -4.2212353 -4.189086 -4.1630106 -4.1488218 -4.1329565 -4.12969 -4.1490021 -4.1848292 -4.2255254 -4.2546735 -4.2664609 -4.2597246][-4.3111777 -4.2873755 -4.2532382 -4.2146859 -4.1823716 -4.1606112 -4.1560235 -4.1565971 -4.1719208 -4.2005129 -4.231658 -4.2599325 -4.2777815 -4.2809033 -4.2682524][-4.3170843 -4.292448 -4.2569013 -4.2178068 -4.1869392 -4.1705222 -4.1753192 -4.1903706 -4.2130756 -4.2425151 -4.2694006 -4.2881918 -4.2957864 -4.2896743 -4.2732368]]...]
INFO - root - 2017-12-05 15:19:04.766144: step 18010, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 78h:23m:54s remains)
INFO - root - 2017-12-05 15:19:13.942207: step 18020, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 81h:01m:37s remains)
INFO - root - 2017-12-05 15:19:23.067884: step 18030, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 80h:49m:42s remains)
INFO - root - 2017-12-05 15:19:32.150479: step 18040, loss = 2.11, batch loss = 2.05 (8.8 examples/sec; 0.908 sec/batch; 79h:20m:10s remains)
INFO - root - 2017-12-05 15:19:41.263160: step 18050, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 79h:35m:48s remains)
INFO - root - 2017-12-05 15:19:50.506166: step 18060, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 81h:19m:22s remains)
INFO - root - 2017-12-05 15:19:59.622106: step 18070, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 78h:49m:49s remains)
INFO - root - 2017-12-05 15:20:08.622593: step 18080, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 77h:07m:17s remains)
INFO - root - 2017-12-05 15:20:17.758232: step 18090, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 78h:34m:29s remains)
INFO - root - 2017-12-05 15:20:26.959435: step 18100, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 78h:16m:03s remains)
2017-12-05 15:20:27.807451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3153477 -4.306438 -4.301774 -4.3014092 -4.3005075 -4.2992811 -4.2986894 -4.2975135 -4.2933722 -4.2863536 -4.2831116 -4.2902393 -4.3049178 -4.321775 -4.3332148][-4.3049135 -4.2919116 -4.2859759 -4.2847967 -4.2811632 -4.28015 -4.282269 -4.2829733 -4.2770114 -4.2650084 -4.2578182 -4.2658563 -4.2872057 -4.3148661 -4.3325949][-4.2849779 -4.2716007 -4.2697411 -4.2662206 -4.2525034 -4.2474871 -4.2534552 -4.2573423 -4.2510133 -4.2348948 -4.2236629 -4.2295117 -4.2563777 -4.2979712 -4.32583][-4.2610044 -4.2458715 -4.2424445 -4.2287755 -4.1987925 -4.1879835 -4.2007174 -4.2087054 -4.1971393 -4.1721025 -4.1580954 -4.1691966 -4.2040915 -4.2604709 -4.3043613][-4.2476377 -4.228168 -4.2159262 -4.1839585 -4.1318641 -4.109024 -4.1249552 -4.1375504 -4.1164532 -4.081583 -4.0736752 -4.09593 -4.1418881 -4.2114005 -4.2720432][-4.2464461 -4.2213964 -4.1927185 -4.1342487 -4.0501995 -4.0011358 -4.0134993 -4.0325065 -4.0130391 -3.9848142 -3.9986451 -4.044898 -4.1060867 -4.184257 -4.25245][-4.244081 -4.2109327 -4.1650195 -4.0777941 -3.9585466 -3.8808725 -3.8860762 -3.9158587 -3.9198022 -3.9217138 -3.9675393 -4.0389309 -4.1119537 -4.1870437 -4.2483435][-4.2463508 -4.2076235 -4.1583133 -4.0716305 -3.955729 -3.8750238 -3.8687315 -3.8879485 -3.8995776 -3.9177713 -3.9775853 -4.0578737 -4.1336246 -4.1994643 -4.2475257][-4.2555008 -4.2202425 -4.1777153 -4.1123552 -4.0302153 -3.9714785 -3.9601219 -3.9570727 -3.9550116 -3.9713092 -4.0266795 -4.1001816 -4.1663041 -4.2206297 -4.2584453][-4.2638836 -4.2367516 -4.2047257 -4.160656 -4.1084852 -4.0698686 -4.0612893 -4.0472732 -4.0334263 -4.0435433 -4.0876217 -4.147644 -4.2035584 -4.2498808 -4.2801824][-4.2830949 -4.262579 -4.2370396 -4.2051592 -4.170835 -4.1458783 -4.1448727 -4.1366043 -4.1249771 -4.1278768 -4.1577458 -4.2010355 -4.245656 -4.2843332 -4.3077517][-4.3059244 -4.2929068 -4.2744503 -4.2511783 -4.22774 -4.2121654 -4.21573 -4.2142372 -4.2069516 -4.2047763 -4.2213826 -4.2482491 -4.2781734 -4.3059807 -4.3215909][-4.3171825 -4.3077784 -4.2945933 -4.2786956 -4.2655191 -4.2584825 -4.2636929 -4.2642083 -4.2582879 -4.2539306 -4.2595611 -4.2721024 -4.2893362 -4.3091469 -4.3202658][-4.3201447 -4.3119931 -4.3023887 -4.2917032 -4.2839389 -4.282474 -4.2898631 -4.2931938 -4.2897477 -4.2859759 -4.2860355 -4.2899418 -4.2986045 -4.3112907 -4.319664][-4.3174973 -4.30832 -4.2995806 -4.291296 -4.28625 -4.2869153 -4.2939067 -4.2997241 -4.2998853 -4.2977428 -4.2972484 -4.2999845 -4.3062434 -4.3149781 -4.3212042]]...]
INFO - root - 2017-12-05 15:20:37.067010: step 18110, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 81h:48m:02s remains)
INFO - root - 2017-12-05 15:20:46.155257: step 18120, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 78h:46m:01s remains)
INFO - root - 2017-12-05 15:20:55.311051: step 18130, loss = 2.02, batch loss = 1.96 (8.9 examples/sec; 0.896 sec/batch; 78h:16m:48s remains)
INFO - root - 2017-12-05 15:21:04.658044: step 18140, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 81h:47m:29s remains)
INFO - root - 2017-12-05 15:21:13.876243: step 18150, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 82h:02m:36s remains)
INFO - root - 2017-12-05 15:21:22.899266: step 18160, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 79h:58m:52s remains)
INFO - root - 2017-12-05 15:21:32.246146: step 18170, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 82h:52m:31s remains)
INFO - root - 2017-12-05 15:21:41.113376: step 18180, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 75h:26m:05s remains)
INFO - root - 2017-12-05 15:21:50.140519: step 18190, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 81h:06m:29s remains)
INFO - root - 2017-12-05 15:21:58.917924: step 18200, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 74h:02m:07s remains)
2017-12-05 15:21:59.704114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3001695 -4.2994409 -4.29814 -4.2972503 -4.2969327 -4.2978387 -4.30002 -4.3023787 -4.3034854 -4.3034768 -4.3034687 -4.3033624 -4.3025765 -4.3016052 -4.3007946][-4.2967558 -4.2942438 -4.2892423 -4.2837157 -4.2805104 -4.2824392 -4.2891068 -4.2971716 -4.3025279 -4.3041787 -4.3039594 -4.3028078 -4.3014503 -4.3003731 -4.2993331][-4.2904158 -4.2825308 -4.2695637 -4.2555971 -4.2472625 -4.250422 -4.2638993 -4.2816043 -4.2964468 -4.3052979 -4.3080635 -4.306335 -4.3030162 -4.3004704 -4.2985549][-4.2757454 -4.2560978 -4.2287927 -4.201149 -4.1857934 -4.1916366 -4.214674 -4.2457633 -4.275847 -4.2986422 -4.3108988 -4.3125658 -4.3079605 -4.3026624 -4.2985387][-4.2555051 -4.2167768 -4.1649203 -4.1138916 -4.0855122 -4.0925736 -4.1283607 -4.1789546 -4.2314014 -4.2752409 -4.304038 -4.3152075 -4.3127007 -4.305378 -4.2979922][-4.240561 -4.1835504 -4.1031761 -4.0214419 -3.9687335 -3.967227 -4.0150185 -4.0896568 -4.1690993 -4.2368298 -4.2844753 -4.3079796 -4.3114777 -4.3053474 -4.2960277][-4.2418466 -4.180202 -4.0868707 -3.9833806 -3.9013813 -3.8761783 -3.9220452 -4.0097432 -4.1075621 -4.1959376 -4.2596917 -4.294127 -4.3040471 -4.3012857 -4.29231][-4.2574606 -4.2071352 -4.123477 -4.0217476 -3.9299405 -3.8864586 -3.9129198 -3.9876673 -4.0827847 -4.1762037 -4.2465296 -4.2854128 -4.2977743 -4.2966132 -4.2881403][-4.2783046 -4.2481914 -4.18893 -4.1105351 -4.0365405 -3.9980993 -4.0102963 -4.0594935 -4.1320863 -4.2070484 -4.2648253 -4.2956481 -4.302341 -4.2974916 -4.2872376][-4.2940359 -4.282413 -4.251214 -4.2050772 -4.159029 -4.1327825 -4.1370206 -4.1669245 -4.2165465 -4.2671318 -4.30462 -4.3204188 -4.3174491 -4.3060493 -4.292634][-4.2987165 -4.2983813 -4.2895546 -4.2726054 -4.2535262 -4.2421584 -4.2455792 -4.2631259 -4.2927504 -4.3220649 -4.3428488 -4.3470397 -4.3368926 -4.3201776 -4.3031688][-4.2959895 -4.3002791 -4.3024354 -4.301959 -4.2991543 -4.2976141 -4.3014131 -4.3099837 -4.3257937 -4.3438425 -4.3583026 -4.3605332 -4.3494682 -4.3317037 -4.3130732][-4.2891731 -4.2903962 -4.2906079 -4.2896285 -4.2852135 -4.2808809 -4.2811728 -4.2873359 -4.3019471 -4.3217158 -4.341455 -4.3511467 -4.3474512 -4.3346648 -4.3188543][-4.2780528 -4.2688174 -4.2548885 -4.2368817 -4.21369 -4.1945252 -4.1865048 -4.195127 -4.220448 -4.2554536 -4.2918258 -4.3187656 -4.3309155 -4.3289433 -4.3192058][-4.261878 -4.235662 -4.1983809 -4.152864 -4.1048117 -4.0703335 -4.0601282 -4.0800295 -4.1245694 -4.181098 -4.2390008 -4.28565 -4.3126426 -4.3195329 -4.314712]]...]
INFO - root - 2017-12-05 15:22:08.914479: step 18210, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 77h:41m:39s remains)
INFO - root - 2017-12-05 15:22:18.045680: step 18220, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 78h:32m:13s remains)
INFO - root - 2017-12-05 15:22:27.197822: step 18230, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.931 sec/batch; 81h:16m:34s remains)
INFO - root - 2017-12-05 15:22:36.238980: step 18240, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 79h:08m:30s remains)
INFO - root - 2017-12-05 15:22:45.313723: step 18250, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 79h:38m:55s remains)
INFO - root - 2017-12-05 15:22:54.626373: step 18260, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 82h:35m:59s remains)
INFO - root - 2017-12-05 15:23:03.648030: step 18270, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.794 sec/batch; 69h:16m:40s remains)
INFO - root - 2017-12-05 15:23:12.856989: step 18280, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 80h:49m:50s remains)
INFO - root - 2017-12-05 15:23:21.812350: step 18290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 78h:27m:27s remains)
INFO - root - 2017-12-05 15:23:31.021858: step 18300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 79h:42m:32s remains)
2017-12-05 15:23:31.836868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1993232 -4.2335896 -4.2457218 -4.2475863 -4.2531281 -4.2517424 -4.2455764 -4.242094 -4.2457523 -4.2570186 -4.2657628 -4.2690578 -4.2709227 -4.2665653 -4.2517953][-4.239727 -4.2675819 -4.2647552 -4.2470059 -4.2324572 -4.2131438 -4.1963978 -4.1939578 -4.2103333 -4.2371831 -4.2588916 -4.2702918 -4.2743764 -4.269629 -4.253027][-4.2598581 -4.281621 -4.2649841 -4.2276549 -4.1901 -4.1472936 -4.1143155 -4.1118431 -4.1435232 -4.1917086 -4.2344608 -4.2636781 -4.2772121 -4.2764254 -4.260869][-4.2597494 -4.2751365 -4.2473893 -4.1944652 -4.1371031 -4.0708928 -4.0178304 -4.0125904 -4.0598946 -4.1302509 -4.1956406 -4.2458682 -4.2728429 -4.2791142 -4.2664957][-4.2550197 -4.2652392 -4.2295237 -4.165894 -4.0932817 -4.0060105 -3.9298933 -3.918407 -3.9795783 -4.0707116 -4.1545854 -4.2229619 -4.2622824 -4.2760968 -4.2675753][-4.2448545 -4.250536 -4.2093186 -4.1375871 -4.0507193 -3.9437497 -3.8442025 -3.8242924 -3.9016988 -4.0182066 -4.1214147 -4.2038026 -4.2524304 -4.2716208 -4.2667894][-4.2288857 -4.23021 -4.1850533 -4.1077738 -4.0099072 -3.8862414 -3.7611723 -3.7255583 -3.8233328 -3.970674 -4.0945921 -4.1884565 -4.24563 -4.2710853 -4.2698588][-4.2138581 -4.2127557 -4.167202 -4.0908141 -3.9928529 -3.8655372 -3.7252719 -3.6701674 -3.7762942 -3.9436762 -4.0794845 -4.1781478 -4.2402024 -4.2694473 -4.2715387][-4.2093787 -4.2115479 -4.1733832 -4.1100788 -4.0300374 -3.9236686 -3.8032475 -3.7487063 -3.8276362 -3.9729507 -4.0926261 -4.1787887 -4.2353544 -4.2624793 -4.2648211][-4.2081962 -4.2176509 -4.1949549 -4.1545362 -4.1038837 -4.0368862 -3.9587703 -3.9214711 -3.9655712 -4.0610743 -4.1367111 -4.1904716 -4.2310467 -4.2552867 -4.2592125][-4.1951461 -4.2110729 -4.2043223 -4.1880422 -4.1671762 -4.1409273 -4.1043024 -4.0879674 -4.1090994 -4.1557751 -4.1847916 -4.2038941 -4.2243032 -4.243721 -4.2487445][-4.1613693 -4.1829515 -4.1919765 -4.20064 -4.2083135 -4.2127805 -4.2040963 -4.1981907 -4.202765 -4.2093582 -4.1984577 -4.1899834 -4.1959753 -4.212328 -4.2195125][-4.1183844 -4.1422958 -4.1631365 -4.1923575 -4.223053 -4.2476807 -4.2574844 -4.256207 -4.2486224 -4.2253876 -4.1836653 -4.1540656 -4.1506462 -4.1659074 -4.1778903][-4.0955133 -4.1201806 -4.1471863 -4.1863832 -4.229876 -4.2651639 -4.2842517 -4.28337 -4.2674341 -4.2261858 -4.1682734 -4.1298909 -4.1232157 -4.1376781 -4.1517062][-4.1132216 -4.1356869 -4.1602526 -4.1975102 -4.2428546 -4.2806239 -4.3015323 -4.3012886 -4.2835054 -4.2395582 -4.1802578 -4.1427507 -4.137743 -4.1473665 -4.1528249]]...]
INFO - root - 2017-12-05 15:23:41.048500: step 18310, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 83h:03m:57s remains)
INFO - root - 2017-12-05 15:23:50.135278: step 18320, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 77h:26m:25s remains)
INFO - root - 2017-12-05 15:23:59.545989: step 18330, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 80h:21m:46s remains)
INFO - root - 2017-12-05 15:24:08.681077: step 18340, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 78h:13m:17s remains)
INFO - root - 2017-12-05 15:24:17.765143: step 18350, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 77h:51m:31s remains)
INFO - root - 2017-12-05 15:24:26.735420: step 18360, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 78h:16m:40s remains)
INFO - root - 2017-12-05 15:24:35.852478: step 18370, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 76h:29m:26s remains)
INFO - root - 2017-12-05 15:24:45.073440: step 18380, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 80h:32m:15s remains)
INFO - root - 2017-12-05 15:24:54.139657: step 18390, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 78h:51m:32s remains)
INFO - root - 2017-12-05 15:25:03.316149: step 18400, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 80h:14m:27s remains)
2017-12-05 15:25:04.156891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1772275 -4.1800065 -4.1763539 -4.171237 -4.1751647 -4.1804824 -4.185638 -4.1812253 -4.18938 -4.2160945 -4.23802 -4.251421 -4.2612967 -4.274343 -4.2869444][-4.2048268 -4.2007589 -4.1904216 -4.1833658 -4.1883059 -4.1984072 -4.2130632 -4.2175875 -4.2306862 -4.25333 -4.2679663 -4.2678313 -4.2653313 -4.2683182 -4.2807508][-4.2239594 -4.2187362 -4.2073832 -4.2024403 -4.2089777 -4.2206087 -4.2363853 -4.2463031 -4.2578073 -4.2707887 -4.2742972 -4.2649016 -4.2583628 -4.2614613 -4.27613][-4.2289166 -4.227365 -4.2201924 -4.21892 -4.2257662 -4.2337971 -4.2468834 -4.25438 -4.2594976 -4.2591252 -4.2535691 -4.24206 -4.2402935 -4.2516332 -4.2734523][-4.2104015 -4.2166972 -4.217382 -4.2224 -4.2267127 -4.2329621 -4.2446094 -4.2461658 -4.2373462 -4.2250786 -4.214757 -4.2103572 -4.2179933 -4.23903 -4.2686248][-4.1850638 -4.1959858 -4.1970758 -4.1992726 -4.2021132 -4.2089992 -4.2215781 -4.2202682 -4.2041512 -4.1917672 -4.1900744 -4.1980553 -4.2140946 -4.2369871 -4.266221][-4.1725087 -4.1838346 -4.1736317 -4.1598763 -4.1533847 -4.1532106 -4.158855 -4.1531315 -4.1429152 -4.1517005 -4.1735587 -4.202023 -4.22656 -4.2456269 -4.2686787][-4.1692877 -4.1688824 -4.1401529 -4.1076331 -4.083559 -4.0686283 -4.0651259 -4.0567183 -4.0570407 -4.0910816 -4.1457968 -4.2043705 -4.2405128 -4.258224 -4.2749758][-4.1610217 -4.1493559 -4.109971 -4.0662284 -4.0284505 -4.0060458 -4.001687 -3.9924586 -3.9974105 -4.0466132 -4.1208868 -4.1996279 -4.2460113 -4.2677579 -4.2830687][-4.1533971 -4.1448684 -4.1066337 -4.0597091 -4.0153561 -3.99577 -4.0028353 -4.0037889 -4.0151472 -4.0655551 -4.1350012 -4.2081523 -4.2528143 -4.2778411 -4.2931767][-4.170383 -4.1641583 -4.1297727 -4.0819964 -4.0410357 -4.0350189 -4.057682 -4.0729828 -4.0884457 -4.1321054 -4.1816535 -4.23016 -4.2634392 -4.2873621 -4.30126][-4.1904187 -4.1899595 -4.1650019 -4.1290073 -4.1028681 -4.1118121 -4.1411886 -4.1570745 -4.1669626 -4.1964331 -4.2250924 -4.2490964 -4.2721624 -4.2950335 -4.3080392][-4.2179136 -4.2214146 -4.2081833 -4.1890512 -4.1760778 -4.189528 -4.2151117 -4.2278571 -4.2321968 -4.2462707 -4.2571287 -4.2631392 -4.277914 -4.2980232 -4.3118043][-4.2598362 -4.2643209 -4.2587285 -4.2497993 -4.2456665 -4.2580228 -4.27573 -4.2823052 -4.2775426 -4.2798209 -4.2774076 -4.2728634 -4.2823682 -4.300324 -4.3143721][-4.30076 -4.2998643 -4.2948971 -4.2907953 -4.290678 -4.30008 -4.3099084 -4.3092737 -4.2987757 -4.2954168 -4.2886515 -4.2800622 -4.2857246 -4.3022208 -4.3162918]]...]
INFO - root - 2017-12-05 15:25:13.163459: step 18410, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 80h:19m:56s remains)
INFO - root - 2017-12-05 15:25:22.287971: step 18420, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 78h:46m:05s remains)
INFO - root - 2017-12-05 15:25:31.355184: step 18430, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 83h:41m:19s remains)
INFO - root - 2017-12-05 15:25:40.417856: step 18440, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 77h:36m:07s remains)
INFO - root - 2017-12-05 15:25:49.484461: step 18450, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 80h:51m:22s remains)
INFO - root - 2017-12-05 15:25:58.577733: step 18460, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 80h:43m:35s remains)
INFO - root - 2017-12-05 15:26:07.884287: step 18470, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 78h:33m:31s remains)
INFO - root - 2017-12-05 15:26:17.013627: step 18480, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 77h:50m:50s remains)
INFO - root - 2017-12-05 15:26:25.989653: step 18490, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 77h:43m:38s remains)
INFO - root - 2017-12-05 15:26:35.209370: step 18500, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 80h:08m:39s remains)
2017-12-05 15:26:35.973576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1522136 -4.1505551 -4.1483226 -4.1503105 -4.1578345 -4.1594381 -4.1537814 -4.1503739 -4.1419582 -4.1277232 -4.118361 -4.1174912 -4.1333156 -4.1663547 -4.1970925][-4.1179838 -4.1211739 -4.1240582 -4.1303158 -4.1442423 -4.15395 -4.1495519 -4.1458759 -4.1423764 -4.1258888 -4.1101451 -4.1039362 -4.1193519 -4.1474643 -4.1685433][-4.1289735 -4.1401024 -4.1512079 -4.1651487 -4.1852732 -4.1980419 -4.1903644 -4.1787252 -4.1712427 -4.1527348 -4.1319237 -4.1160393 -4.1161313 -4.1269679 -4.1380095][-4.1622081 -4.176754 -4.1895533 -4.2047043 -4.2209187 -4.2266793 -4.2121243 -4.1942849 -4.1857557 -4.1675453 -4.1420021 -4.1172271 -4.10188 -4.0997496 -4.1122584][-4.18107 -4.1969995 -4.2050309 -4.21216 -4.214097 -4.2065606 -4.1828504 -4.1555429 -4.1492786 -4.1400719 -4.1190228 -4.0973215 -4.0829778 -4.0793061 -4.0972509][-4.1777053 -4.1901345 -4.1917853 -4.1896386 -4.1762943 -4.1513171 -4.1125731 -4.0669842 -4.0620494 -4.07194 -4.0691905 -4.0660162 -4.064888 -4.0632615 -4.0779066][-4.1652074 -4.1735749 -4.1729097 -4.1667356 -4.1440921 -4.1092916 -4.0627327 -4.0040579 -3.9934194 -4.0195827 -4.0356216 -4.054863 -4.0678983 -4.0696096 -4.0765772][-4.1632881 -4.1709952 -4.1702762 -4.1672869 -4.1505566 -4.1218109 -4.0890594 -4.0412588 -4.0261765 -4.0462651 -4.0617642 -4.085506 -4.1039028 -4.1090288 -4.1091824][-4.1759081 -4.1850467 -4.1860046 -4.1878505 -4.1797004 -4.1627622 -4.1516676 -4.1256905 -4.1148109 -4.1237955 -4.1265116 -4.1354241 -4.1446924 -4.147429 -4.1454582][-4.18715 -4.2009983 -4.2029552 -4.2047167 -4.1993885 -4.1882768 -4.1908236 -4.180717 -4.1757879 -4.1783285 -4.1714978 -4.1655169 -4.1621442 -4.1569963 -4.1532731][-4.1906886 -4.2020369 -4.2024679 -4.2038331 -4.2009745 -4.194242 -4.2015495 -4.2000122 -4.1945109 -4.1937222 -4.1896234 -4.1827407 -4.1788449 -4.1698313 -4.1609898][-4.2033577 -4.2070436 -4.2019825 -4.1992331 -4.1959348 -4.1895823 -4.1935854 -4.19252 -4.1853023 -4.1838632 -4.1888504 -4.1907759 -4.1910834 -4.1819997 -4.1706834][-4.22071 -4.2184343 -4.2118273 -4.2054119 -4.2019324 -4.198091 -4.1978655 -4.1968455 -4.1873617 -4.1817441 -4.1869373 -4.1917892 -4.1932044 -4.1870928 -4.178267][-4.2344604 -4.2268729 -4.2219348 -4.2145276 -4.2107019 -4.2137709 -4.2150931 -4.2152348 -4.2044973 -4.1916432 -4.1899605 -4.1917753 -4.1923738 -4.1902728 -4.187758][-4.2300954 -4.2171326 -4.2124925 -4.2080216 -4.2066236 -4.2163539 -4.22384 -4.225738 -4.21468 -4.199821 -4.1940041 -4.1937203 -4.1950135 -4.1976237 -4.2029934]]...]
INFO - root - 2017-12-05 15:26:45.045096: step 18510, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 78h:20m:37s remains)
INFO - root - 2017-12-05 15:26:54.221036: step 18520, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 80h:03m:51s remains)
INFO - root - 2017-12-05 15:27:03.370424: step 18530, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 77h:55m:04s remains)
INFO - root - 2017-12-05 15:27:12.439762: step 18540, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 79h:20m:53s remains)
INFO - root - 2017-12-05 15:27:21.517494: step 18550, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 73h:24m:55s remains)
INFO - root - 2017-12-05 15:27:30.767652: step 18560, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 78h:25m:51s remains)
INFO - root - 2017-12-05 15:27:39.597541: step 18570, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 70h:23m:37s remains)
INFO - root - 2017-12-05 15:27:48.711618: step 18580, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 80h:49m:08s remains)
INFO - root - 2017-12-05 15:27:57.939232: step 18590, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 79h:50m:39s remains)
INFO - root - 2017-12-05 15:28:06.924377: step 18600, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 79h:12m:02s remains)
2017-12-05 15:28:07.732665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1231494 -4.1308966 -4.1324081 -4.1339607 -4.1333847 -4.1349564 -4.1339583 -4.13036 -4.1246915 -4.1217651 -4.131146 -4.1352034 -4.1367297 -4.1268625 -4.1172738][-4.1220503 -4.136282 -4.141572 -4.14679 -4.1474428 -4.143508 -4.1406679 -4.1380992 -4.1342516 -4.1280484 -4.1327662 -4.1340084 -4.132678 -4.1261444 -4.117537][-4.1289897 -4.1449203 -4.1548705 -4.1656632 -4.1679921 -4.1613493 -4.15791 -4.1550069 -4.1521907 -4.1448727 -4.1457443 -4.1436062 -4.1395764 -4.1396537 -4.1362572][-4.1498322 -4.1622677 -4.1725969 -4.1844487 -4.1852417 -4.1758895 -4.1698141 -4.1633725 -4.1617589 -4.157712 -4.1610622 -4.1601224 -4.1554742 -4.1608629 -4.1648684][-4.1748776 -4.1795077 -4.1863909 -4.1936917 -4.1881728 -4.1738238 -4.1622844 -4.1520476 -4.1523442 -4.1557407 -4.170527 -4.1818943 -4.1821408 -4.1910219 -4.1999669][-4.1876497 -4.182219 -4.1836724 -4.1860557 -4.1748757 -4.1539621 -4.1349144 -4.1223779 -4.1277933 -4.1411104 -4.1708922 -4.2011466 -4.2118092 -4.22164 -4.2295942][-4.1756039 -4.162209 -4.1606927 -4.1641026 -4.1563759 -4.137475 -4.1165533 -4.1041684 -4.1161804 -4.138267 -4.1800056 -4.2242012 -4.2446303 -4.2551355 -4.2572751][-4.1470222 -4.1321735 -4.13469 -4.1504006 -4.1619134 -4.1600204 -4.147521 -4.1401215 -4.1575036 -4.1823478 -4.2226372 -4.2626152 -4.2820544 -4.2888818 -4.2843084][-4.1290956 -4.1182728 -4.1264811 -4.1561828 -4.1886764 -4.2059908 -4.2083559 -4.2077212 -4.226954 -4.2497106 -4.2779412 -4.300427 -4.3101692 -4.311749 -4.3041077][-4.1333394 -4.1277175 -4.1379943 -4.1714759 -4.212872 -4.2413807 -4.2548923 -4.259306 -4.276258 -4.2938018 -4.3070412 -4.3119907 -4.3128271 -4.311501 -4.3030519][-4.1555505 -4.1542864 -4.1619782 -4.1869283 -4.2211075 -4.2493706 -4.2657485 -4.2708616 -4.2830067 -4.2932863 -4.2952733 -4.2904119 -4.2873797 -4.2872138 -4.2815018][-4.1891332 -4.1894436 -4.1884031 -4.1952848 -4.213973 -4.2343316 -4.24833 -4.2546577 -4.2657561 -4.2728491 -4.2694812 -4.2624111 -4.2597027 -4.2608848 -4.2575989][-4.2157707 -4.2153549 -4.2051654 -4.1996903 -4.2076688 -4.2186842 -4.2290125 -4.2392526 -4.2518725 -4.257688 -4.250628 -4.2423229 -4.2390394 -4.2402511 -4.238862][-4.2290049 -4.2264895 -4.210752 -4.2005939 -4.2028785 -4.2068505 -4.2153125 -4.229301 -4.2435184 -4.2483468 -4.2392807 -4.2285976 -4.2231364 -4.22316 -4.2239151][-4.2220125 -4.2177691 -4.2030506 -4.1940179 -4.1946974 -4.1974788 -4.2066388 -4.2230291 -4.2375088 -4.24204 -4.23469 -4.2247858 -4.2199135 -4.2207136 -4.2247887]]...]
INFO - root - 2017-12-05 15:28:16.634052: step 18610, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 76h:58m:42s remains)
INFO - root - 2017-12-05 15:28:25.778387: step 18620, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 80h:03m:59s remains)
INFO - root - 2017-12-05 15:28:35.005653: step 18630, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 78h:48m:47s remains)
INFO - root - 2017-12-05 15:28:44.109742: step 18640, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 79h:00m:57s remains)
INFO - root - 2017-12-05 15:28:53.105384: step 18650, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 78h:36m:44s remains)
INFO - root - 2017-12-05 15:29:02.154738: step 18660, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 78h:28m:38s remains)
INFO - root - 2017-12-05 15:29:11.144724: step 18670, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 80h:26m:43s remains)
INFO - root - 2017-12-05 15:29:20.299501: step 18680, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 78h:16m:26s remains)
INFO - root - 2017-12-05 15:29:29.532839: step 18690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 76h:17m:50s remains)
INFO - root - 2017-12-05 15:29:38.450572: step 18700, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 80h:42m:17s remains)
2017-12-05 15:29:39.212329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2561769 -4.2272496 -4.1885157 -4.1571431 -4.1353717 -4.1252656 -4.150948 -4.1684256 -4.16897 -4.183115 -4.1812358 -4.161243 -4.1395011 -4.1459193 -4.1647243][-4.236938 -4.2112241 -4.1730638 -4.1385312 -4.1121988 -4.1018124 -4.134078 -4.1617732 -4.1769385 -4.1989155 -4.2021775 -4.1940193 -4.1781545 -4.17781 -4.1897478][-4.2228627 -4.1972685 -4.1614246 -4.127142 -4.1008768 -4.0966182 -4.1324644 -4.1630297 -4.1866703 -4.2129507 -4.2195964 -4.2188625 -4.2070918 -4.2012777 -4.2069073][-4.22283 -4.2019382 -4.1762905 -4.1517229 -4.1316781 -4.1353812 -4.1664677 -4.181922 -4.1929622 -4.2129426 -4.2223878 -4.22365 -4.2165532 -4.2154932 -4.221559][-4.2319927 -4.2230463 -4.213634 -4.2036057 -4.189178 -4.1904979 -4.204443 -4.2000756 -4.1953835 -4.2080307 -4.2191157 -4.2209511 -4.2166452 -4.2224989 -4.2326956][-4.2363024 -4.2369776 -4.238472 -4.2352223 -4.2224731 -4.2174559 -4.2164321 -4.1921539 -4.1726265 -4.1864371 -4.2075076 -4.2132874 -4.2109408 -4.2166262 -4.2277][-4.2349849 -4.2365556 -4.239212 -4.2356725 -4.2195992 -4.2085104 -4.1890869 -4.1380897 -4.101831 -4.1308975 -4.1746259 -4.19211 -4.1928039 -4.1945972 -4.2078605][-4.2327342 -4.2290382 -4.2288742 -4.2231541 -4.2002993 -4.1751847 -4.128757 -4.0451741 -3.9932821 -4.0522594 -4.1297894 -4.169816 -4.1821861 -4.1840172 -4.1972909][-4.2342114 -4.2246695 -4.2186832 -4.2109342 -4.1837687 -4.1408229 -4.0689349 -3.9658327 -3.9109187 -3.9951773 -4.0995965 -4.1626534 -4.1930118 -4.2008944 -4.2092795][-4.2463961 -4.2315974 -4.221076 -4.2149372 -4.1944928 -4.1525569 -4.0813184 -3.9942648 -3.9550476 -4.0268621 -4.1238875 -4.1849194 -4.2137194 -4.2213955 -4.2246437][-4.2606297 -4.2473669 -4.2363868 -4.2340155 -4.2249193 -4.1963983 -4.1467919 -4.0926118 -4.0740976 -4.122395 -4.1844053 -4.221097 -4.2361078 -4.2375975 -4.2332888][-4.2727113 -4.2615047 -4.2475061 -4.2451434 -4.240016 -4.2232895 -4.1965427 -4.1652222 -4.1566396 -4.1899276 -4.2284236 -4.2466626 -4.2523284 -4.2504106 -4.2446041][-4.2754855 -4.2665834 -4.2498894 -4.2447333 -4.2419829 -4.2349772 -4.2209749 -4.202282 -4.1991143 -4.2214189 -4.2457476 -4.2550845 -4.2580462 -4.2610431 -4.2622471][-4.2678967 -4.2590933 -4.242753 -4.2355037 -4.2342744 -4.2331486 -4.2282181 -4.2205467 -4.223402 -4.2406149 -4.2579045 -4.264782 -4.2671542 -4.2734904 -4.2802839][-4.2641068 -4.2572069 -4.2427454 -4.2340007 -4.2303395 -4.2306204 -4.2302957 -4.2306237 -4.2397823 -4.2566338 -4.2713628 -4.2785997 -4.2833548 -4.2914381 -4.2989368]]...]
INFO - root - 2017-12-05 15:29:48.239753: step 18710, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 76h:59m:38s remains)
INFO - root - 2017-12-05 15:29:57.299603: step 18720, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:33m:38s remains)
INFO - root - 2017-12-05 15:30:06.310754: step 18730, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 79h:10m:43s remains)
INFO - root - 2017-12-05 15:30:15.293100: step 18740, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 80h:12m:19s remains)
INFO - root - 2017-12-05 15:30:24.386157: step 18750, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 78h:43m:20s remains)
INFO - root - 2017-12-05 15:30:33.340216: step 18760, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 79h:49m:34s remains)
INFO - root - 2017-12-05 15:30:42.527439: step 18770, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 79h:12m:39s remains)
INFO - root - 2017-12-05 15:30:51.510306: step 18780, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 79h:00m:17s remains)
INFO - root - 2017-12-05 15:31:00.644048: step 18790, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 80h:16m:31s remains)
INFO - root - 2017-12-05 15:31:09.651673: step 18800, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 76h:39m:58s remains)
2017-12-05 15:31:10.447259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2433538 -4.2666173 -4.2683353 -4.251596 -4.2265821 -4.2024217 -4.183558 -4.1651287 -4.1446805 -4.1172361 -4.0957856 -4.08518 -4.0857759 -4.0998521 -4.122468][-4.2203 -4.2432389 -4.2438946 -4.2284174 -4.210217 -4.1925321 -4.1753883 -4.1549144 -4.1347237 -4.1157904 -4.1004229 -4.0891461 -4.0822954 -4.0830626 -4.0967436][-4.1953621 -4.2143106 -4.2107782 -4.193491 -4.1808887 -4.1707568 -4.154994 -4.135427 -4.1194329 -4.1129408 -4.1052723 -4.0933557 -4.08026 -4.066401 -4.0685835][-4.1675506 -4.1820774 -4.1751914 -4.1544957 -4.141561 -4.1318488 -4.1114044 -4.090384 -4.0868034 -4.0964413 -4.0996904 -4.0911479 -4.07883 -4.0605087 -4.0571284][-4.1464787 -4.1569281 -4.1465025 -4.1205373 -4.0986233 -4.0779247 -4.0458403 -4.0220957 -4.0337577 -4.0632339 -4.0827017 -4.0881162 -4.0895162 -4.0784454 -4.0745645][-4.1460934 -4.1548557 -4.1377707 -4.1016264 -4.0623469 -4.0212183 -3.96934 -3.9351237 -3.9624717 -4.0160275 -4.0554886 -4.0801291 -4.098125 -4.1006846 -4.101934][-4.1665392 -4.1748104 -4.1521535 -4.1017056 -4.0411277 -3.9716299 -3.8861914 -3.828459 -3.8740695 -3.9593234 -4.022295 -4.0663118 -4.09854 -4.1147237 -4.1255751][-4.1817269 -4.18408 -4.1584883 -4.101254 -4.025866 -3.9312315 -3.8162894 -3.7339509 -3.8003135 -3.9184244 -4.0017586 -4.0575681 -4.1033 -4.1351614 -4.1586475][-4.1851139 -4.1780939 -4.151876 -4.1041164 -4.0391092 -3.9518361 -3.8503296 -3.7775657 -3.8338671 -3.9401121 -4.0172057 -4.0680437 -4.1148429 -4.1560664 -4.1878843][-4.1941981 -4.1813984 -4.152566 -4.1158524 -4.0733566 -4.0193267 -3.9613175 -3.9218235 -3.9514477 -4.0162296 -4.0653992 -4.0962 -4.1272111 -4.1633306 -4.1944623][-4.1852751 -4.1752434 -4.1497831 -4.1206279 -4.0966024 -4.070333 -4.0458903 -4.0305552 -4.0451169 -4.079052 -4.1074419 -4.1223111 -4.1397724 -4.1715546 -4.1984997][-4.1787863 -4.1785536 -4.1607914 -4.1386976 -4.1219845 -4.1068325 -4.0950918 -4.0867386 -4.0903568 -4.1046205 -4.122118 -4.1316104 -4.1484208 -4.1833553 -4.2044888][-4.18928 -4.1998534 -4.1935444 -4.1782866 -4.1625981 -4.1472139 -4.1354523 -4.12525 -4.1206512 -4.1226149 -4.1337628 -4.1439581 -4.1647487 -4.1975274 -4.2112632][-4.2112818 -4.2284083 -4.2288256 -4.2194767 -4.2059059 -4.1895261 -4.1731758 -4.1601472 -4.1499343 -4.1448541 -4.1539621 -4.1703763 -4.1972594 -4.2280526 -4.2382078][-4.2361426 -4.2552671 -4.2579656 -4.253036 -4.2432804 -4.2292638 -4.2140727 -4.1991448 -4.18245 -4.1732936 -4.1832051 -4.207603 -4.2376361 -4.2646179 -4.270936]]...]
INFO - root - 2017-12-05 15:31:19.584286: step 18810, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 79h:20m:36s remains)
INFO - root - 2017-12-05 15:31:28.719612: step 18820, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 75h:57m:37s remains)
INFO - root - 2017-12-05 15:31:37.720023: step 18830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 76h:45m:29s remains)
INFO - root - 2017-12-05 15:31:46.684022: step 18840, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 77h:40m:44s remains)
INFO - root - 2017-12-05 15:31:55.851072: step 18850, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 78h:38m:34s remains)
INFO - root - 2017-12-05 15:32:04.935373: step 18860, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 79h:44m:22s remains)
INFO - root - 2017-12-05 15:32:13.908156: step 18870, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 79h:20m:23s remains)
INFO - root - 2017-12-05 15:32:22.999416: step 18880, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 77h:14m:38s remains)
INFO - root - 2017-12-05 15:32:32.170050: step 18890, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 81h:12m:50s remains)
INFO - root - 2017-12-05 15:32:41.128537: step 18900, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 78h:20m:37s remains)
2017-12-05 15:32:41.828584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2711134 -4.2771297 -4.2775874 -4.2714796 -4.2601032 -4.2522244 -4.2453685 -4.2371349 -4.2398148 -4.2470808 -4.2552395 -4.2628288 -4.266078 -4.2663822 -4.2657313][-4.2445221 -4.2511644 -4.2532129 -4.2467866 -4.2330532 -4.2251949 -4.2159395 -4.2036119 -4.2083731 -4.2209954 -4.2343163 -4.2420006 -4.2432618 -4.246181 -4.2503824][-4.2036662 -4.2129006 -4.2217636 -4.2172132 -4.2047186 -4.1991243 -4.1874523 -4.1704 -4.1766925 -4.1948986 -4.2123609 -4.2157779 -4.2152376 -4.22281 -4.2356067][-4.1527734 -4.1618056 -4.1783576 -4.1793151 -4.1709137 -4.1652665 -4.1438613 -4.1202583 -4.1325836 -4.1568775 -4.1783624 -4.1845431 -4.1879454 -4.2054977 -4.2309752][-4.11467 -4.12367 -4.1434956 -4.1460366 -4.1321292 -4.1148782 -4.0832219 -4.0683432 -4.0943613 -4.1301818 -4.1610661 -4.1756773 -4.1828642 -4.2075639 -4.2373581][-4.0826077 -4.0898094 -4.1129193 -4.1139073 -4.0885053 -4.0504217 -4.0095949 -4.0179396 -4.0661988 -4.1121788 -4.1503458 -4.1744418 -4.1888089 -4.2177577 -4.2423835][-4.0784087 -4.0681143 -4.0783339 -4.067502 -4.0224137 -3.9483881 -3.9062762 -3.9609647 -4.0443311 -4.1044884 -4.1485233 -4.1756434 -4.193923 -4.2203646 -4.2381892][-4.1024728 -4.0883341 -4.0843639 -4.0488873 -3.9664798 -3.8380964 -3.7834432 -3.8953457 -4.026926 -4.1045527 -4.14991 -4.1750278 -4.1952209 -4.2152538 -4.2271719][-4.1184559 -4.1154022 -4.1184316 -4.0890579 -4.0060868 -3.8781593 -3.8175993 -3.9154789 -4.0377531 -4.1109691 -4.1486073 -4.1694021 -4.1896806 -4.2081327 -4.21758][-4.1076727 -4.1117506 -4.1217613 -4.1104469 -4.065474 -3.9952633 -3.9539163 -3.9959037 -4.0693073 -4.1192975 -4.1489515 -4.1700678 -4.1893368 -4.2067509 -4.21533][-4.0810933 -4.079895 -4.09152 -4.0981941 -4.0859013 -4.0596118 -4.042604 -4.0597644 -4.0963697 -4.121182 -4.1427364 -4.1669106 -4.189847 -4.2089324 -4.2204008][-4.0612769 -4.06018 -4.0724411 -4.0836115 -4.0859027 -4.0832071 -4.0860019 -4.1026278 -4.120986 -4.1273341 -4.1416016 -4.1675591 -4.190877 -4.2126603 -4.2295108][-4.0675621 -4.071754 -4.086916 -4.0993876 -4.1082077 -4.111033 -4.1164846 -4.1308093 -4.1379128 -4.1368513 -4.1494875 -4.1752625 -4.1955242 -4.2172227 -4.2385015][-4.0903549 -4.0965481 -4.1160164 -4.13459 -4.1468911 -4.1463637 -4.1463537 -4.1536369 -4.1548944 -4.1517406 -4.161706 -4.1796207 -4.1962972 -4.2209091 -4.2469091][-4.125864 -4.1323352 -4.1512833 -4.1720228 -4.1845527 -4.18139 -4.1784105 -4.1831617 -4.1843615 -4.1816068 -4.1857805 -4.1950994 -4.2075534 -4.2303386 -4.2531886]]...]
INFO - root - 2017-12-05 15:32:50.896180: step 18910, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.894 sec/batch; 77h:51m:08s remains)
INFO - root - 2017-12-05 15:33:00.018790: step 18920, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 81h:10m:25s remains)
INFO - root - 2017-12-05 15:33:08.895188: step 18930, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 78h:00m:13s remains)
INFO - root - 2017-12-05 15:33:18.043740: step 18940, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 82h:59m:40s remains)
INFO - root - 2017-12-05 15:33:27.077961: step 18950, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 79h:17m:45s remains)
INFO - root - 2017-12-05 15:33:36.185711: step 18960, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 73h:27m:37s remains)
INFO - root - 2017-12-05 15:33:45.290535: step 18970, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 78h:39m:26s remains)
INFO - root - 2017-12-05 15:33:54.545756: step 18980, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 82h:58m:00s remains)
INFO - root - 2017-12-05 15:34:03.668127: step 18990, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 78h:31m:07s remains)
INFO - root - 2017-12-05 15:34:12.627211: step 19000, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 77h:17m:21s remains)
2017-12-05 15:34:13.417455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2681389 -4.2322311 -4.1840253 -4.1355104 -4.0978227 -4.0716438 -4.0677338 -4.0930648 -4.1349683 -4.1634946 -4.1818643 -4.2020068 -4.2262368 -4.25057 -4.27679][-4.2594137 -4.2138834 -4.15869 -4.1053357 -4.0699081 -4.0514631 -4.057478 -4.08363 -4.1195269 -4.1431932 -4.1622262 -4.1873922 -4.217186 -4.2458 -4.2755594][-4.2761126 -4.2307906 -4.1742368 -4.1217041 -4.0886631 -4.0752439 -4.078609 -4.0912185 -4.1127558 -4.1314154 -4.1548095 -4.1886888 -4.2223954 -4.2528973 -4.2830286][-4.2987318 -4.2577634 -4.2038407 -4.1570568 -4.1259685 -4.1100955 -4.0998144 -4.0933447 -4.100132 -4.1139951 -4.1439166 -4.1891212 -4.2267156 -4.2583303 -4.2889748][-4.3074303 -4.271606 -4.2220945 -4.1798329 -4.1466727 -4.12019 -4.090332 -4.0672617 -4.0701051 -4.0845714 -4.1265011 -4.183507 -4.2238941 -4.2558408 -4.2890682][-4.3025861 -4.2689285 -4.2180567 -4.1705995 -4.1308761 -4.0824537 -4.0280304 -3.9922686 -4.00602 -4.0447178 -4.1078482 -4.1738806 -4.2165771 -4.2498674 -4.2867131][-4.2828889 -4.2449031 -4.1880445 -4.1307545 -4.0741568 -4.0001121 -3.9086008 -3.8639555 -3.9228404 -4.0140066 -4.1013217 -4.1702466 -4.2120824 -4.2463517 -4.2839079][-4.2619944 -4.2177653 -4.1535721 -4.0871248 -4.0118384 -3.9063125 -3.7735369 -3.7389445 -3.8709943 -4.0073047 -4.1043754 -4.1674457 -4.2040248 -4.2392182 -4.2788463][-4.2489548 -4.1979456 -4.1304016 -4.0601344 -3.977998 -3.8712296 -3.7598698 -3.7682226 -3.9179213 -4.0443091 -4.1219034 -4.1679697 -4.1997337 -4.2365732 -4.2794089][-4.2432003 -4.1903238 -4.1257915 -4.0617032 -3.9997313 -3.9414816 -3.9008505 -3.9325554 -4.0365796 -4.1139336 -4.1601853 -4.1883807 -4.2145905 -4.2511735 -4.2915492][-4.2415452 -4.1932945 -4.1400948 -4.0958762 -4.0673466 -4.0522256 -4.0444932 -4.067554 -4.1298242 -4.1759262 -4.2018714 -4.2190776 -4.2428207 -4.2757783 -4.3075809][-4.2488337 -4.21041 -4.1717134 -4.1440506 -4.1305289 -4.1228962 -4.111587 -4.1219139 -4.1686006 -4.2087374 -4.2297869 -4.2443042 -4.2657719 -4.2933421 -4.3168473][-4.2605696 -4.2282753 -4.1950893 -4.1711826 -4.1565013 -4.1427994 -4.1273036 -4.1362352 -4.1822009 -4.2237935 -4.2456622 -4.260479 -4.2796884 -4.302876 -4.3221531][-4.2690845 -4.2356629 -4.2005792 -4.1727643 -4.1554289 -4.1430936 -4.1342273 -4.1493487 -4.1946669 -4.2346807 -4.2560153 -4.2705493 -4.2877564 -4.3090148 -4.3268957][-4.275001 -4.2386637 -4.2017007 -4.1722775 -4.156786 -4.1527381 -4.1566081 -4.1786752 -4.2203245 -4.2549295 -4.2728958 -4.2847977 -4.2990294 -4.3180947 -4.3344707]]...]
INFO - root - 2017-12-05 15:34:22.391305: step 19010, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 81h:50m:49s remains)
INFO - root - 2017-12-05 15:34:31.336948: step 19020, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 80h:12m:40s remains)
INFO - root - 2017-12-05 15:34:40.302247: step 19030, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 76h:04m:41s remains)
INFO - root - 2017-12-05 15:34:49.291526: step 19040, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 79h:38m:07s remains)
INFO - root - 2017-12-05 15:34:58.359675: step 19050, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 78h:13m:56s remains)
INFO - root - 2017-12-05 15:35:07.293068: step 19060, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 78h:51m:08s remains)
INFO - root - 2017-12-05 15:35:16.378715: step 19070, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 78h:05m:40s remains)
INFO - root - 2017-12-05 15:35:25.549599: step 19080, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 77h:41m:03s remains)
INFO - root - 2017-12-05 15:35:34.479285: step 19090, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.874 sec/batch; 76h:07m:21s remains)
INFO - root - 2017-12-05 15:35:43.551743: step 19100, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 80h:20m:05s remains)
2017-12-05 15:35:44.409373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3012242 -4.3016887 -4.3043 -4.3086214 -4.3146338 -4.3196464 -4.3222766 -4.3242741 -4.3264809 -4.3301678 -4.3321939 -4.3330607 -4.3345218 -4.3386078 -4.3423753][-4.2802167 -4.2793827 -4.2820506 -4.2868404 -4.2959185 -4.3042941 -4.3083768 -4.3120594 -4.3163719 -4.3230886 -4.3279238 -4.3296075 -4.331181 -4.3361726 -4.3416624][-4.2501583 -4.2466397 -4.2474484 -4.252852 -4.2646241 -4.2754965 -4.2798834 -4.28445 -4.292717 -4.3050437 -4.3162727 -4.3213162 -4.3242049 -4.3298059 -4.3381658][-4.2142997 -4.2056394 -4.20326 -4.2092171 -4.221097 -4.2308583 -4.2341561 -4.2415562 -4.2538671 -4.271152 -4.2894168 -4.3031416 -4.3111491 -4.3188944 -4.3311439][-4.184382 -4.1712661 -4.1635404 -4.165554 -4.1732645 -4.173646 -4.1709042 -4.1814642 -4.1998763 -4.2258396 -4.255271 -4.2817574 -4.2980914 -4.3096089 -4.3240795][-4.1609635 -4.1440616 -4.1302624 -4.1283865 -4.1282878 -4.1152463 -4.1027107 -4.1180305 -4.1469231 -4.1851969 -4.2275887 -4.2654872 -4.2896242 -4.3052635 -4.3191075][-4.124608 -4.0974522 -4.07593 -4.0732741 -4.0700417 -4.04637 -4.0246081 -4.0514588 -4.0988107 -4.1534615 -4.2045336 -4.2469921 -4.2751803 -4.2961011 -4.3116016][-4.0925384 -4.0484638 -4.0121613 -4.0035353 -3.9946237 -3.9637132 -3.9424739 -3.988276 -4.0563493 -4.1206565 -4.1789603 -4.2267 -4.2581153 -4.284369 -4.3040442][-4.0837665 -4.0250921 -3.974838 -3.9537292 -3.9336543 -3.8993037 -3.8955822 -3.958642 -4.0358043 -4.1019044 -4.1605272 -4.2118754 -4.2491279 -4.2823749 -4.3041315][-4.1044927 -4.0426931 -3.9869916 -3.9627781 -3.9407184 -3.9136524 -3.9231548 -3.9868152 -4.0580106 -4.1186905 -4.171834 -4.2224183 -4.2636209 -4.2972984 -4.315084][-4.1372643 -4.0842266 -4.0379357 -4.0205946 -4.0039935 -3.9857202 -4.0007219 -4.0549364 -4.1145744 -4.1618729 -4.2052922 -4.2521234 -4.2899537 -4.3166981 -4.3279281][-4.180222 -4.1414828 -4.1126585 -4.1033068 -4.0946417 -4.0871143 -4.1037059 -4.1435385 -4.1864667 -4.2183123 -4.2473435 -4.281569 -4.3096256 -4.3280282 -4.3339648][-4.2303987 -4.2062769 -4.1921911 -4.1911364 -4.1926107 -4.1981592 -4.2130075 -4.235002 -4.2575736 -4.2735748 -4.2869039 -4.3050523 -4.3204331 -4.3313246 -4.3360438][-4.2769442 -4.2638369 -4.2577882 -4.2615032 -4.2696633 -4.2814784 -4.2929983 -4.3017135 -4.3094521 -4.3144436 -4.3183465 -4.3252974 -4.3316154 -4.3373637 -4.3399234][-4.3068037 -4.3002033 -4.2973876 -4.3012724 -4.3092246 -4.3199482 -4.3293672 -4.3343935 -4.3362103 -4.3369956 -4.3372664 -4.3382854 -4.3393779 -4.3409948 -4.3419523]]...]
INFO - root - 2017-12-05 15:35:53.438045: step 19110, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 79h:14m:27s remains)
INFO - root - 2017-12-05 15:36:02.481873: step 19120, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 80h:31m:51s remains)
INFO - root - 2017-12-05 15:36:11.741893: step 19130, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.930 sec/batch; 80h:56m:13s remains)
INFO - root - 2017-12-05 15:36:20.837420: step 19140, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 80h:38m:05s remains)
INFO - root - 2017-12-05 15:36:29.991153: step 19150, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 80h:18m:50s remains)
INFO - root - 2017-12-05 15:36:39.175847: step 19160, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 80h:04m:02s remains)
INFO - root - 2017-12-05 15:36:48.321303: step 19170, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 79h:01m:39s remains)
INFO - root - 2017-12-05 15:36:57.442689: step 19180, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 80h:06m:53s remains)
INFO - root - 2017-12-05 15:37:06.559173: step 19190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 78h:50m:59s remains)
INFO - root - 2017-12-05 15:37:15.652852: step 19200, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 77h:15m:01s remains)
2017-12-05 15:37:16.470683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533555 -4.2462654 -4.2341309 -4.2240529 -4.2173967 -4.2049131 -4.1815348 -4.1751261 -4.1917 -4.2080059 -4.2301054 -4.261776 -4.2904854 -4.3103552 -4.3206706][-4.2390728 -4.2262058 -4.208549 -4.19522 -4.1863751 -4.1707273 -4.143887 -4.1373558 -4.1621284 -4.1909442 -4.22222 -4.2612495 -4.2915759 -4.3112354 -4.3217735][-4.2265959 -4.2097235 -4.1895289 -4.1771088 -4.16768 -4.1506305 -4.122191 -4.1151147 -4.147893 -4.1858935 -4.2223034 -4.2685084 -4.3021927 -4.3186536 -4.3265805][-4.2106161 -4.1955237 -4.18126 -4.1711955 -4.1604514 -4.1401768 -4.1118846 -4.1057205 -4.1425138 -4.1858563 -4.225677 -4.2756038 -4.3099394 -4.3238697 -4.3300676][-4.1896029 -4.1834984 -4.1835327 -4.1831374 -4.1732683 -4.1502352 -4.1188831 -4.1070805 -4.140399 -4.1867394 -4.2309756 -4.2818947 -4.3134332 -4.3256469 -4.3309369][-4.1750026 -4.1825719 -4.198544 -4.2085695 -4.1993904 -4.1714725 -4.1322241 -4.1066895 -4.1293912 -4.1765532 -4.2279911 -4.2803698 -4.311615 -4.3248248 -4.3305421][-4.159462 -4.1800308 -4.210372 -4.2281585 -4.2185564 -4.1835375 -4.1320205 -4.090169 -4.10783 -4.1595478 -4.22013 -4.2755895 -4.3100142 -4.3253126 -4.3315053][-4.1325364 -4.1666718 -4.2098207 -4.2353067 -4.2257152 -4.1824365 -4.1190939 -4.0659852 -4.0832081 -4.139873 -4.2110209 -4.271234 -4.3093538 -4.3270836 -4.3346438][-4.090982 -4.1407657 -4.1976581 -4.229856 -4.2198224 -4.1727729 -4.1019769 -4.0412045 -4.057004 -4.1189156 -4.1983676 -4.2645435 -4.3084464 -4.3296194 -4.3386636][-4.0541325 -4.1095924 -4.1691551 -4.20409 -4.1943841 -4.1495008 -4.0806479 -4.0208635 -4.0409346 -4.1087217 -4.19271 -4.2629976 -4.3108482 -4.3346381 -4.3432684][-4.0366306 -4.0877047 -4.1387548 -4.170156 -4.1635585 -4.1279097 -4.0698929 -4.02693 -4.0568457 -4.1239386 -4.2028317 -4.268796 -4.3158121 -4.3398738 -4.3472009][-4.0404739 -4.0766892 -4.1178989 -4.1480403 -4.1515026 -4.1311741 -4.0896478 -4.0700684 -4.1068096 -4.16364 -4.2257667 -4.2822986 -4.3242679 -4.3449621 -4.3497005][-4.0706339 -4.0914083 -4.1209083 -4.1458983 -4.155077 -4.1471438 -4.1220684 -4.1209207 -4.1578951 -4.2015028 -4.2492175 -4.2967615 -4.3327842 -4.3483758 -4.3506312][-4.106822 -4.1181812 -4.1391177 -4.1527872 -4.1580982 -4.156774 -4.1441984 -4.1548805 -4.1886468 -4.2217693 -4.2605095 -4.3039646 -4.3366942 -4.3501644 -4.3517313][-4.1388912 -4.1499043 -4.1639514 -4.1657314 -4.1629996 -4.1628785 -4.1558123 -4.1709962 -4.2004652 -4.2282891 -4.264852 -4.3069234 -4.3379183 -4.3513 -4.3526063]]...]
INFO - root - 2017-12-05 15:37:25.398909: step 19210, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 81h:25m:55s remains)
INFO - root - 2017-12-05 15:37:34.481982: step 19220, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 77h:14m:54s remains)
INFO - root - 2017-12-05 15:37:43.516291: step 19230, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.896 sec/batch; 77h:56m:31s remains)
INFO - root - 2017-12-05 15:37:52.826415: step 19240, loss = 2.08, batch loss = 2.02 (7.6 examples/sec; 1.054 sec/batch; 91h:40m:46s remains)
INFO - root - 2017-12-05 15:38:02.005590: step 19250, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.946 sec/batch; 82h:18m:21s remains)
INFO - root - 2017-12-05 15:38:11.201857: step 19260, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 78h:27m:10s remains)
INFO - root - 2017-12-05 15:38:20.294376: step 19270, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 77h:28m:33s remains)
INFO - root - 2017-12-05 15:38:29.547182: step 19280, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 79h:09m:06s remains)
INFO - root - 2017-12-05 15:38:38.755061: step 19290, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 80h:00m:46s remains)
INFO - root - 2017-12-05 15:38:47.945276: step 19300, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 78h:07m:11s remains)
2017-12-05 15:38:48.709544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2033648 -4.2236624 -4.22749 -4.2228918 -4.2144508 -4.2071824 -4.2039924 -4.2065425 -4.2110968 -4.2162046 -4.2190251 -4.2174754 -4.2125711 -4.208004 -4.2053967][-4.1844072 -4.2169461 -4.225379 -4.2196088 -4.2062211 -4.1925254 -4.1835761 -4.1849885 -4.193574 -4.20566 -4.2149754 -4.216711 -4.211576 -4.205265 -4.1994419][-4.1568131 -4.2014718 -4.2177577 -4.2117062 -4.1907792 -4.1672277 -4.1485252 -4.1466155 -4.15987 -4.1816912 -4.2012234 -4.2101421 -4.2080059 -4.2010918 -4.1916437][-4.1373868 -4.1872621 -4.2095547 -4.2030659 -4.1735711 -4.136025 -4.1022983 -4.0917678 -4.1090903 -4.1441169 -4.1771116 -4.1959538 -4.2001562 -4.19614 -4.1866689][-4.14067 -4.1874166 -4.2081728 -4.1970506 -4.1573958 -4.1008906 -4.0450478 -4.0206761 -4.0430155 -4.094655 -4.1434932 -4.1767855 -4.192081 -4.194406 -4.1879516][-4.1580577 -4.1984625 -4.2123179 -4.1921911 -4.1398897 -4.0626922 -3.9831119 -3.9438484 -3.9740329 -4.0414515 -4.1039824 -4.1519651 -4.1801295 -4.1912642 -4.1891131][-4.1686172 -4.208487 -4.2175817 -4.1906476 -4.128449 -4.0383434 -3.9466453 -3.9012861 -3.93431 -4.0050139 -4.0685377 -4.1215611 -4.1560183 -4.1727252 -4.1742749][-4.1639261 -4.2074847 -4.2202191 -4.1944089 -4.132555 -4.0445032 -3.9601049 -3.9212983 -3.9499865 -4.00827 -4.05601 -4.0999804 -4.1314936 -4.1497197 -4.1543508][-4.1557751 -4.2027121 -4.2239861 -4.207922 -4.1580629 -4.0840058 -4.0175471 -3.988482 -4.0084171 -4.0459595 -4.071208 -4.0993004 -4.1212826 -4.1349583 -4.1366992][-4.1695375 -4.213903 -4.2392526 -4.2329831 -4.1990042 -4.1443033 -4.0981193 -4.07772 -4.0885267 -4.10413 -4.1091113 -4.1224403 -4.1374578 -4.1455555 -4.140027][-4.1912236 -4.2295003 -4.2550974 -4.2569675 -4.236136 -4.1984615 -4.1661506 -4.1499033 -4.1528049 -4.1531835 -4.1449037 -4.1486578 -4.157711 -4.1615191 -4.1519723][-4.2202005 -4.2502842 -4.2726135 -4.2768803 -4.2635355 -4.236064 -4.2121534 -4.1978593 -4.197331 -4.1919551 -4.1800113 -4.1808581 -4.1886854 -4.1935534 -4.186018][-4.2439256 -4.2676549 -4.2869596 -4.2942333 -4.2868 -4.2655034 -4.2441859 -4.2299876 -4.2272906 -4.2212906 -4.2105689 -4.2114067 -4.222611 -4.2337046 -4.2345386][-4.2610993 -4.2819953 -4.2995772 -4.3108454 -4.3088903 -4.2915282 -4.2701349 -4.253459 -4.2479372 -4.2418532 -4.2321863 -4.2313442 -4.2437606 -4.2591715 -4.2669988][-4.280149 -4.2979875 -4.314899 -4.3283806 -4.3291535 -4.3138885 -4.2929273 -4.2753944 -4.2676563 -4.2612238 -4.2513905 -4.2453036 -4.2528772 -4.2655492 -4.2750244]]...]
INFO - root - 2017-12-05 15:38:57.826145: step 19310, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 79h:03m:44s remains)
INFO - root - 2017-12-05 15:39:07.100273: step 19320, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 74h:22m:15s remains)
INFO - root - 2017-12-05 15:39:16.258417: step 19330, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 80h:09m:17s remains)
INFO - root - 2017-12-05 15:39:25.525248: step 19340, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 78h:38m:32s remains)
INFO - root - 2017-12-05 15:39:34.649145: step 19350, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 80h:00m:57s remains)
INFO - root - 2017-12-05 15:39:43.858549: step 19360, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 80h:18m:26s remains)
INFO - root - 2017-12-05 15:39:53.062493: step 19370, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 82h:17m:10s remains)
INFO - root - 2017-12-05 15:40:02.224541: step 19380, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 77h:52m:35s remains)
INFO - root - 2017-12-05 15:40:11.393031: step 19390, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 80h:31m:53s remains)
INFO - root - 2017-12-05 15:40:20.436841: step 19400, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 79h:12m:17s remains)
2017-12-05 15:40:21.255113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20138 -4.2164021 -4.2368731 -4.2447991 -4.2390213 -4.2324882 -4.2196236 -4.2005744 -4.1916323 -4.196197 -4.1987314 -4.1985512 -4.2153215 -4.2340255 -4.2487516][-4.1871285 -4.20535 -4.2264724 -4.2386308 -4.2331972 -4.2253795 -4.2157216 -4.2051525 -4.2033472 -4.2053881 -4.2045093 -4.2045035 -4.2171926 -4.2303562 -4.2399449][-4.1687107 -4.19396 -4.220325 -4.2360106 -4.2275882 -4.2126656 -4.1991043 -4.19289 -4.1993265 -4.2063651 -4.2092385 -4.2147522 -4.2224245 -4.2250595 -4.2205777][-4.1496329 -4.1828938 -4.2160592 -4.2366476 -4.2306652 -4.212369 -4.1903372 -4.1805782 -4.1874886 -4.19901 -4.2107396 -4.2300768 -4.2424326 -4.2393169 -4.2214823][-4.1588454 -4.1881862 -4.2165809 -4.2337451 -4.2286062 -4.2083564 -4.1836019 -4.1726618 -4.1753249 -4.1841726 -4.2046838 -4.2385163 -4.2597909 -4.2612429 -4.244163][-4.1912951 -4.2081084 -4.2176137 -4.2215762 -4.2079034 -4.1809206 -4.1557775 -4.1498632 -4.152844 -4.1601954 -4.185761 -4.2299032 -4.2584062 -4.270175 -4.2626729][-4.2130322 -4.2206111 -4.2135096 -4.2016215 -4.1723933 -4.1329436 -4.106215 -4.1078916 -4.1170416 -4.1300216 -4.1605191 -4.2063742 -4.2390323 -4.2608809 -4.2670856][-4.2246919 -4.2262735 -4.2085257 -4.1852684 -4.1401381 -4.0810304 -4.0423717 -4.0448093 -4.0663438 -4.0939641 -4.13264 -4.1784463 -4.2121716 -4.2387762 -4.2541604][-4.2217326 -4.2192078 -4.2012472 -4.1752806 -4.1228156 -4.04711 -3.9872782 -3.9786332 -4.007226 -4.0539494 -4.1059589 -4.1563735 -4.1923647 -4.2194939 -4.2350707][-4.222271 -4.217988 -4.2068868 -4.18985 -4.144742 -4.0740466 -4.0127277 -3.9957683 -4.017642 -4.06705 -4.1215944 -4.17245 -4.2106228 -4.2378592 -4.2503123][-4.2447915 -4.2363868 -4.2332635 -4.2284312 -4.1994934 -4.1500869 -4.1076422 -4.0946097 -4.1072416 -4.1435065 -4.1860762 -4.2305532 -4.2677984 -4.2926908 -4.3018918][-4.2706456 -4.2589812 -4.2599964 -4.2637267 -4.2495441 -4.2220941 -4.198576 -4.1930523 -4.2020054 -4.2252631 -4.2554035 -4.2898865 -4.3198981 -4.3390708 -4.344955][-4.3003716 -4.2888365 -4.2888317 -4.2941718 -4.2877054 -4.2738791 -4.263422 -4.2612448 -4.2664814 -4.2806621 -4.3021455 -4.327374 -4.3495917 -4.3628159 -4.3674426][-4.3254514 -4.3152118 -4.3127408 -4.3144956 -4.3114882 -4.30516 -4.3015847 -4.3025289 -4.3068371 -4.3166714 -4.3313265 -4.3486643 -4.3638391 -4.3726039 -4.3756948][-4.3366652 -4.3288894 -4.3253279 -4.3238764 -4.3208327 -4.317687 -4.3172693 -4.3197165 -4.3244305 -4.3332129 -4.3447876 -4.3564777 -4.3651142 -4.3690209 -4.3696952]]...]
INFO - root - 2017-12-05 15:40:30.289608: step 19410, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 79h:42m:04s remains)
INFO - root - 2017-12-05 15:40:39.251216: step 19420, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 79h:57m:42s remains)
INFO - root - 2017-12-05 15:40:48.272619: step 19430, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 77h:06m:03s remains)
INFO - root - 2017-12-05 15:40:57.398805: step 19440, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 81h:47m:00s remains)
INFO - root - 2017-12-05 15:41:06.799255: step 19450, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 78h:37m:49s remains)
INFO - root - 2017-12-05 15:41:15.937754: step 19460, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 80h:03m:54s remains)
INFO - root - 2017-12-05 15:41:24.847721: step 19470, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 77h:19m:28s remains)
INFO - root - 2017-12-05 15:41:33.958183: step 19480, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 79h:01m:04s remains)
INFO - root - 2017-12-05 15:41:42.978382: step 19490, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 77h:58m:56s remains)
INFO - root - 2017-12-05 15:41:51.985951: step 19500, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 78h:31m:53s remains)
2017-12-05 15:41:52.798322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3327103 -4.3324022 -4.33151 -4.3292308 -4.3266582 -4.3247218 -4.32431 -4.3251467 -4.3266153 -4.3282528 -4.3292327 -4.3297973 -4.3300791 -4.3304229 -4.3310208][-4.3269215 -4.3245387 -4.3212132 -4.3164492 -4.3118873 -4.3096313 -4.3110752 -4.315011 -4.3197551 -4.3232055 -4.3246455 -4.3246584 -4.3241572 -4.3244019 -4.325984][-4.3175583 -4.3118873 -4.30448 -4.2955537 -4.287569 -4.2847705 -4.2885447 -4.2979255 -4.3083148 -4.3161268 -4.320055 -4.32125 -4.3213716 -4.3222904 -4.3245716][-4.2982488 -4.2885756 -4.2771077 -4.2630248 -4.249258 -4.2417727 -4.2436476 -4.256144 -4.273777 -4.29159 -4.3049474 -4.3127933 -4.3167596 -4.3198895 -4.3235164][-4.2678533 -4.2538323 -4.2378263 -4.217773 -4.1948466 -4.1773734 -4.1730113 -4.1847029 -4.2080526 -4.23721 -4.2641292 -4.2849021 -4.2993832 -4.3101292 -4.3178554][-4.2220206 -4.2060614 -4.1876206 -4.1609211 -4.1255121 -4.0936384 -4.0820012 -4.0933466 -4.1202731 -4.1574669 -4.1960464 -4.23066 -4.2585063 -4.2812557 -4.2984476][-4.1636424 -4.15394 -4.1390872 -4.1086259 -4.0592785 -4.0073128 -3.9855967 -3.9944255 -4.0219574 -4.0628653 -4.1068907 -4.1505075 -4.19085 -4.2283616 -4.2598143][-4.1062951 -4.1068068 -4.1026092 -4.0775323 -4.0255027 -3.9617944 -3.9261284 -3.9209449 -3.9350696 -3.9659638 -4.0081806 -4.0582457 -4.1097403 -4.1607361 -4.2072949][-4.0624509 -4.0692725 -4.0781517 -4.069458 -4.0377035 -3.9903388 -3.9540803 -3.9327202 -3.9242263 -3.9320729 -3.958756 -4.0026288 -4.0553093 -4.1116681 -4.1675072][-4.0857716 -4.0879169 -4.0948143 -4.0926285 -4.07773 -4.0556746 -4.0401773 -4.030993 -4.0244794 -4.0219455 -4.0294018 -4.0524187 -4.0895648 -4.1349978 -4.1834517][-4.1734648 -4.1661835 -4.160985 -4.1539855 -4.1467075 -4.1443648 -4.1511984 -4.1613379 -4.1675191 -4.1691871 -4.1704965 -4.1777453 -4.1936207 -4.21648 -4.2435908][-4.2669415 -4.25631 -4.2471433 -4.2401118 -4.23767 -4.2449884 -4.2617011 -4.2808886 -4.2934465 -4.297924 -4.2973413 -4.2965174 -4.2980084 -4.3021646 -4.308846][-4.3316936 -4.3239326 -4.3185396 -4.3159828 -4.317831 -4.3274722 -4.344346 -4.3619685 -4.3719225 -4.3735375 -4.3703661 -4.3652177 -4.3601804 -4.3558021 -4.3518963][-4.3678355 -4.3632107 -4.3598995 -4.3589034 -4.3620048 -4.3701034 -4.3806262 -4.3900981 -4.3950353 -4.3951359 -4.3917575 -4.385941 -4.3795295 -4.372839 -4.3657179][-4.3759 -4.3724766 -4.3707652 -4.3704677 -4.372076 -4.3755417 -4.3782182 -4.3800554 -4.3800373 -4.3787975 -4.3760777 -4.3718371 -4.3669548 -4.3617883 -4.355895]]...]
INFO - root - 2017-12-05 15:42:01.871099: step 19510, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 79h:21m:42s remains)
INFO - root - 2017-12-05 15:42:10.837828: step 19520, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 80h:01m:35s remains)
INFO - root - 2017-12-05 15:42:19.885310: step 19530, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 78h:07m:15s remains)
INFO - root - 2017-12-05 15:42:29.006547: step 19540, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 76h:17m:29s remains)
INFO - root - 2017-12-05 15:42:38.180826: step 19550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 80h:08m:16s remains)
INFO - root - 2017-12-05 15:42:47.225564: step 19560, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.947 sec/batch; 82h:16m:56s remains)
INFO - root - 2017-12-05 15:42:56.335577: step 19570, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 80h:19m:25s remains)
INFO - root - 2017-12-05 15:43:05.446809: step 19580, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 80h:13m:16s remains)
INFO - root - 2017-12-05 15:43:14.371021: step 19590, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 82h:48m:52s remains)
INFO - root - 2017-12-05 15:43:23.497790: step 19600, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 78h:40m:05s remains)
2017-12-05 15:43:24.287081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2734022 -4.2456183 -4.2245064 -4.2122459 -4.2087555 -4.2148132 -4.2333236 -4.2517767 -4.2609096 -4.2652292 -4.2643881 -4.2639394 -4.2636805 -4.2649369 -4.2674222][-4.2547164 -4.2200351 -4.192256 -4.1748195 -4.1675086 -4.1732607 -4.1932611 -4.2186294 -4.2363949 -4.243134 -4.2416716 -4.2412429 -4.24305 -4.2469454 -4.2475729][-4.2343369 -4.1895871 -4.1524091 -4.1281948 -4.1152859 -4.1186643 -4.1394687 -4.1718512 -4.2019854 -4.2131238 -4.2115335 -4.2105694 -4.21228 -4.2153425 -4.2106047][-4.2129288 -4.1556339 -4.106977 -4.0744381 -4.0536427 -4.0520124 -4.0711327 -4.1055765 -4.1449871 -4.1629395 -4.1644011 -4.1625137 -4.1646137 -4.1662111 -4.1562691][-4.1975675 -4.1361823 -4.0861282 -4.0524979 -4.0277829 -4.0212622 -4.0266428 -4.0525923 -4.095314 -4.1194811 -4.1232166 -4.1199493 -4.1231489 -4.1289434 -4.1183743][-4.1916127 -4.1318135 -4.0873127 -4.0587225 -4.0332308 -4.0209403 -4.0057473 -4.0132289 -4.0576162 -4.0914807 -4.1012621 -4.1014729 -4.1059203 -4.1144214 -4.1070704][-4.1885018 -4.1298122 -4.0883021 -4.0571961 -4.0239587 -4.000813 -3.9582512 -3.9406092 -3.9944959 -4.0485349 -4.0711131 -4.0818405 -4.0918231 -4.1039224 -4.103291][-4.1997976 -4.1448879 -4.1078029 -4.072926 -4.0262156 -3.977711 -3.8901553 -3.839148 -3.9134808 -3.9901202 -4.0251727 -4.0461841 -4.0662494 -4.0868263 -4.0920753][-4.2243195 -4.1771135 -4.1490688 -4.117991 -4.0669603 -4.0024991 -3.8972621 -3.8372889 -3.91402 -3.9916153 -4.026844 -4.0494165 -4.067039 -4.0828347 -4.0850253][-4.2481194 -4.2096157 -4.1920142 -4.169394 -4.1260457 -4.0674567 -3.9895732 -3.9553115 -4.0112362 -4.0642424 -4.0890107 -4.1053276 -4.1141372 -4.1171231 -4.1087871][-4.2619 -4.2313566 -4.2213864 -4.2051806 -4.1686831 -4.1197114 -4.0731831 -4.060246 -4.0947542 -4.1228094 -4.1393957 -4.1507287 -4.1547217 -4.1477842 -4.13267][-4.2718482 -4.2500048 -4.2415023 -4.225491 -4.1879625 -4.1456475 -4.1244011 -4.1256981 -4.1483736 -4.1595173 -4.1681657 -4.175148 -4.1751723 -4.1624146 -4.1438484][-4.2836075 -4.2694864 -4.2586975 -4.2389235 -4.2006631 -4.1658258 -4.1633329 -4.1748643 -4.1882205 -4.18687 -4.1873646 -4.1886592 -4.1877222 -4.176898 -4.1612225][-4.29286 -4.28135 -4.2703786 -4.255444 -4.2248726 -4.202004 -4.2070975 -4.2193551 -4.2253823 -4.218442 -4.2135339 -4.2110581 -4.2145476 -4.2125559 -4.2014751][-4.299047 -4.2853708 -4.2748418 -4.266974 -4.2484021 -4.2360845 -4.2427263 -4.2488313 -4.2510157 -4.246089 -4.2413535 -4.2373009 -4.2429748 -4.2476192 -4.2447596]]...]
INFO - root - 2017-12-05 15:43:33.362017: step 19610, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 80h:20m:34s remains)
INFO - root - 2017-12-05 15:43:42.315328: step 19620, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 78h:15m:34s remains)
INFO - root - 2017-12-05 15:43:51.325684: step 19630, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 81h:31m:31s remains)
INFO - root - 2017-12-05 15:44:00.416363: step 19640, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.904 sec/batch; 78h:32m:27s remains)
INFO - root - 2017-12-05 15:44:09.528633: step 19650, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 79h:51m:06s remains)
INFO - root - 2017-12-05 15:44:18.608646: step 19660, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 79h:55m:58s remains)
INFO - root - 2017-12-05 15:44:27.906610: step 19670, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 78h:35m:13s remains)
INFO - root - 2017-12-05 15:44:36.724747: step 19680, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 79h:16m:33s remains)
INFO - root - 2017-12-05 15:44:45.951164: step 19690, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 79h:10m:48s remains)
INFO - root - 2017-12-05 15:44:54.915624: step 19700, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 79h:57m:28s remains)
2017-12-05 15:44:55.683836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2398391 -4.2084088 -4.1684446 -4.1320119 -4.1192775 -4.1433425 -4.1783123 -4.1932106 -4.1823192 -4.1582336 -4.1562309 -4.1682777 -4.1757174 -4.1890883 -4.2102365][-4.25015 -4.2230439 -4.1884365 -4.1560931 -4.1372905 -4.1488929 -4.1722231 -4.1752491 -4.1562314 -4.1383481 -4.1517205 -4.1715441 -4.1793504 -4.1962366 -4.2254996][-4.2559891 -4.2375832 -4.2105207 -4.1818023 -4.1545167 -4.1491046 -4.154407 -4.1416812 -4.1137176 -4.1010127 -4.1261954 -4.1552434 -4.1712031 -4.197444 -4.2341228][-4.2563028 -4.2490582 -4.2286706 -4.1993866 -4.1648946 -4.1452408 -4.1324844 -4.1073227 -4.072618 -4.0611115 -4.0925984 -4.1305666 -4.1631117 -4.2033029 -4.2447][-4.2537208 -4.2587757 -4.2452321 -4.2146363 -4.1727638 -4.1393538 -4.114912 -4.0881209 -4.0563884 -4.0472808 -4.0767932 -4.1188827 -4.1643782 -4.2141962 -4.2556953][-4.2471933 -4.2612104 -4.2513433 -4.2152576 -4.1632981 -4.1183066 -4.0909395 -4.0762572 -4.0618649 -4.0641894 -4.0885863 -4.1274776 -4.1779132 -4.2296324 -4.2654266][-4.230298 -4.242475 -4.2301774 -4.1861811 -4.1261806 -4.0765877 -4.0560875 -4.0606451 -4.0689387 -4.0884047 -4.1120405 -4.1473031 -4.19798 -4.2475061 -4.2733669][-4.2035003 -4.20092 -4.1761904 -4.1233134 -4.05912 -4.012784 -4.0074868 -4.0325708 -4.065074 -4.1065416 -4.14081 -4.177588 -4.2217355 -4.2611647 -4.277144][-4.1756744 -4.1545768 -4.112318 -4.0523443 -3.9911351 -3.9602666 -3.9746313 -4.018908 -4.0708709 -4.1265874 -4.1719136 -4.2097211 -4.2417293 -4.2663016 -4.2749124][-4.1514368 -4.1200452 -4.0710382 -4.0117888 -3.9610071 -3.9478607 -3.9766486 -4.0299139 -4.0894442 -4.1509695 -4.2011414 -4.233417 -4.2522559 -4.2622986 -4.2655315][-4.14364 -4.1094475 -4.0628047 -4.0103297 -3.968606 -3.9642348 -3.9932268 -4.0447421 -4.1069694 -4.1693268 -4.2162757 -4.2405291 -4.249475 -4.2506504 -4.2509069][-4.1515007 -4.1217875 -4.0805683 -4.0367785 -4.00129 -3.9957018 -4.01774 -4.0631824 -4.1240683 -4.1818795 -4.2219887 -4.2393494 -4.2417059 -4.2387595 -4.23866][-4.166132 -4.1467004 -4.1176915 -4.0848732 -4.0555673 -4.0455866 -4.0582957 -4.0931063 -4.1435313 -4.1894073 -4.2201853 -4.2326522 -4.2337132 -4.2308397 -4.2302189][-4.18057 -4.1727042 -4.1576824 -4.1388073 -4.1185746 -4.1067643 -4.1101937 -4.1315951 -4.1626821 -4.1906838 -4.2135863 -4.2240686 -4.2253613 -4.2229853 -4.2230039][-4.1911435 -4.1872706 -4.18232 -4.1765471 -4.1669779 -4.1595397 -4.1565013 -4.1624007 -4.1747 -4.1874981 -4.201653 -4.2123489 -4.2158089 -4.2145133 -4.21658]]...]
INFO - root - 2017-12-05 15:45:04.758393: step 19710, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 79h:40m:39s remains)
INFO - root - 2017-12-05 15:45:13.883267: step 19720, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 78h:09m:51s remains)
INFO - root - 2017-12-05 15:45:23.026342: step 19730, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 79h:06m:17s remains)
INFO - root - 2017-12-05 15:45:32.054616: step 19740, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 77h:36m:11s remains)
INFO - root - 2017-12-05 15:45:41.190080: step 19750, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 81h:25m:27s remains)
INFO - root - 2017-12-05 15:45:50.241481: step 19760, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 77h:58m:51s remains)
INFO - root - 2017-12-05 15:45:59.120968: step 19770, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 77h:01m:41s remains)
INFO - root - 2017-12-05 15:46:08.376054: step 19780, loss = 2.11, batch loss = 2.06 (8.9 examples/sec; 0.902 sec/batch; 78h:19m:56s remains)
INFO - root - 2017-12-05 15:46:17.453497: step 19790, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 74h:38m:04s remains)
INFO - root - 2017-12-05 15:46:26.523458: step 19800, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.899 sec/batch; 78h:04m:56s remains)
2017-12-05 15:46:27.365295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1604495 -4.1615334 -4.177928 -4.1863127 -4.1930385 -4.2004838 -4.2060347 -4.2158928 -4.232492 -4.2461872 -4.2427483 -4.216063 -4.1697888 -4.1172624 -4.0919595][-4.1917772 -4.1848235 -4.1916347 -4.1948633 -4.1959057 -4.1959844 -4.1929131 -4.1948457 -4.2059984 -4.2230988 -4.2271857 -4.2036829 -4.1502409 -4.0796852 -4.0384784][-4.2358708 -4.2241387 -4.2194796 -4.2129664 -4.2038741 -4.1931448 -4.1806602 -4.1701136 -4.1702142 -4.1853156 -4.2040849 -4.1955662 -4.1433253 -4.0628829 -4.0114923][-4.26504 -4.2466826 -4.2264514 -4.2109571 -4.1959858 -4.1818395 -4.1674595 -4.146224 -4.1315718 -4.1416569 -4.175478 -4.1896391 -4.14893 -4.0666752 -4.0037379][-4.2763443 -4.2504988 -4.2148018 -4.1890125 -4.1730628 -4.1618619 -4.1472192 -4.1173506 -4.0849881 -4.0858407 -4.1316009 -4.169486 -4.150949 -4.0833054 -4.0162826][-4.282011 -4.2488589 -4.19757 -4.1592112 -4.1408973 -4.1284375 -4.1074476 -4.0700526 -4.0302653 -4.0293651 -4.0878682 -4.1486363 -4.1513796 -4.1045103 -4.0411272][-4.2796922 -4.2472887 -4.1935415 -4.1494889 -4.1244531 -4.1029382 -4.074964 -4.0379872 -4.0080862 -4.0139365 -4.074842 -4.1471415 -4.1649814 -4.1290879 -4.0653262][-4.271729 -4.245172 -4.1989594 -4.1572471 -4.1242747 -4.0886703 -4.0488634 -4.0121994 -3.9974098 -4.0134783 -4.0692759 -4.1422863 -4.1771154 -4.1509733 -4.0872531][-4.270854 -4.2490497 -4.2107825 -4.1717148 -4.1300011 -4.0815043 -4.0341172 -4.0011387 -3.996716 -4.0187864 -4.0706615 -4.1408563 -4.190156 -4.1764975 -4.1138306][-4.2691007 -4.2533655 -4.2240038 -4.1911335 -4.14532 -4.0901585 -4.0399351 -4.0144072 -4.0182662 -4.0449753 -4.0943723 -4.1600704 -4.2064347 -4.1925383 -4.1307421][-4.2615123 -4.2564144 -4.2372665 -4.2130127 -4.1717772 -4.1161723 -4.0670438 -4.0449691 -4.0516033 -4.0815454 -4.1297512 -4.1870246 -4.2231703 -4.2029181 -4.1389546][-4.2402248 -4.2455926 -4.2364788 -4.2226124 -4.1930642 -4.148562 -4.111227 -4.0969634 -4.1032248 -4.1282363 -4.1678228 -4.2140117 -4.2394829 -4.2107611 -4.1434875][-4.2221818 -4.2317958 -4.2315984 -4.2280416 -4.2098212 -4.1787076 -4.1568789 -4.1488466 -4.1527176 -4.1683383 -4.1933694 -4.2269845 -4.2421942 -4.2068176 -4.1386342][-4.2083573 -4.2205234 -4.227191 -4.2316942 -4.2229724 -4.2049947 -4.1955905 -4.1912818 -4.1900926 -4.1953945 -4.2036467 -4.2204514 -4.2279158 -4.1932235 -4.1263752][-4.1958308 -4.2056985 -4.213367 -4.2220812 -4.2222857 -4.2165236 -4.2183528 -4.2189808 -4.2141452 -4.2119484 -4.2073889 -4.2105289 -4.2113595 -4.1809511 -4.1182523]]...]
INFO - root - 2017-12-05 15:46:36.742816: step 19810, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 80h:16m:26s remains)
INFO - root - 2017-12-05 15:46:45.706942: step 19820, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 76h:27m:25s remains)
INFO - root - 2017-12-05 15:46:54.658058: step 19830, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 79h:21m:40s remains)
INFO - root - 2017-12-05 15:47:03.736941: step 19840, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 78h:27m:24s remains)
INFO - root - 2017-12-05 15:47:13.037553: step 19850, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 81h:20m:17s remains)
INFO - root - 2017-12-05 15:47:22.429148: step 19860, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 80h:23m:38s remains)
INFO - root - 2017-12-05 15:47:31.495605: step 19870, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 79h:08m:14s remains)
INFO - root - 2017-12-05 15:47:40.656103: step 19880, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 78h:39m:30s remains)
INFO - root - 2017-12-05 15:47:49.701759: step 19890, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.930 sec/batch; 80h:42m:54s remains)
INFO - root - 2017-12-05 15:47:58.876245: step 19900, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 80h:21m:26s remains)
2017-12-05 15:47:59.656106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0542603 -3.9916482 -3.9614749 -3.9821489 -4.0324235 -4.0755391 -4.1046567 -4.1252627 -4.1308937 -4.1416588 -4.1502404 -4.1489148 -4.1472979 -4.1391764 -4.1322961][-4.0546608 -4.00316 -3.9901938 -4.0166516 -4.066134 -4.1018586 -4.1188369 -4.1333976 -4.1288524 -4.1306267 -4.1365867 -4.1341429 -4.1345282 -4.1323047 -4.1330328][-4.0561371 -4.0228662 -4.0248804 -4.0495734 -4.0922046 -4.1226754 -4.1304188 -4.1352568 -4.1237092 -4.1210179 -4.1204915 -4.1085882 -4.1031 -4.1039004 -4.1089954][-4.0651817 -4.0467548 -4.0591307 -4.0815225 -4.1190467 -4.1448665 -4.1430345 -4.1335135 -4.1153488 -4.1056075 -4.0935235 -4.0657849 -4.04934 -4.0528755 -4.0613403][-4.0850964 -4.0743828 -4.0922108 -4.1175551 -4.1467829 -4.1612978 -4.1479216 -4.1266103 -4.1017909 -4.0835395 -4.0584097 -4.0173965 -3.9894493 -3.9953454 -4.0128217][-4.097682 -4.0897641 -4.1062412 -4.1313629 -4.1495838 -4.1487041 -4.1240048 -4.1001935 -4.07516 -4.050271 -4.0184183 -3.9756353 -3.944591 -3.9508686 -3.9725349][-4.0943208 -4.0853796 -4.0979209 -4.1207914 -4.1276512 -4.1132188 -4.0772085 -4.0502982 -4.0299215 -4.0061388 -3.9776585 -3.9504554 -3.9323673 -3.9397936 -3.9545803][-4.0875597 -4.0698857 -4.0705395 -4.0838633 -4.0831213 -4.0618567 -4.0145311 -3.9848254 -3.975873 -3.9686944 -3.9561238 -3.9508717 -3.9495065 -3.9585557 -3.9681041][-4.0744734 -4.0405135 -4.0250359 -4.0307684 -4.0295162 -4.009141 -3.9648483 -3.9424841 -3.950407 -3.9643526 -3.9706128 -3.9765973 -3.9807706 -3.9899371 -3.9976315][-4.0462451 -4.001945 -3.9841354 -3.9945343 -3.9992287 -3.9916468 -3.9666686 -3.9583061 -3.9728031 -3.9909256 -4.0008087 -4.0065155 -4.0083194 -4.0145946 -4.0178385][-4.0179915 -3.9773917 -3.9720378 -3.9902546 -3.9977446 -4.0009685 -3.9915435 -3.9916611 -4.0024848 -4.011342 -4.0146379 -4.0147343 -4.0174069 -4.0235438 -4.0237932][-4.0084934 -3.9819028 -3.9887147 -4.0098591 -4.018271 -4.025598 -4.0240469 -4.0243707 -4.0291 -4.0300875 -4.0251627 -4.0216751 -4.0260091 -4.0330105 -4.0339355][-4.0415254 -4.0302396 -4.0455089 -4.0696468 -4.0812197 -4.0860014 -4.0819359 -4.079936 -4.079978 -4.076097 -4.0688906 -4.0639882 -4.0671124 -4.0757809 -4.0794411][-4.1209421 -4.1228218 -4.1409173 -4.1635332 -4.1740284 -4.1747932 -4.1660337 -4.1597338 -4.1579585 -4.15362 -4.1471663 -4.1410761 -4.1414585 -4.1485176 -4.1536989][-4.2119036 -4.2202139 -4.2372303 -4.2570186 -4.2636676 -4.260581 -4.2514129 -4.2438841 -4.2396131 -4.2346911 -4.22916 -4.2252278 -4.2248149 -4.2297778 -4.2332263]]...]
INFO - root - 2017-12-05 15:48:08.776264: step 19910, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 81h:10m:07s remains)
INFO - root - 2017-12-05 15:48:17.741525: step 19920, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 76h:35m:26s remains)
INFO - root - 2017-12-05 15:48:26.846901: step 19930, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.805 sec/batch; 69h:52m:25s remains)
INFO - root - 2017-12-05 15:48:36.013118: step 19940, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 78h:53m:33s remains)
INFO - root - 2017-12-05 15:48:45.112295: step 19950, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 78h:43m:05s remains)
INFO - root - 2017-12-05 15:48:54.079345: step 19960, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 78h:58m:30s remains)
INFO - root - 2017-12-05 15:49:03.235685: step 19970, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 79h:16m:51s remains)
INFO - root - 2017-12-05 15:49:12.290938: step 19980, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.884 sec/batch; 76h:42m:36s remains)
INFO - root - 2017-12-05 15:49:21.336301: step 19990, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 77h:43m:20s remains)
INFO - root - 2017-12-05 15:49:30.451878: step 20000, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 76h:58m:01s remains)
2017-12-05 15:49:31.244953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2738791 -4.2753744 -4.277246 -4.2868452 -4.2958918 -4.297379 -4.2937737 -4.2912097 -4.2916675 -4.2887554 -4.279295 -4.2710238 -4.2751145 -4.2841535 -4.2947626][-4.233253 -4.2345085 -4.2347107 -4.2486825 -4.2620249 -4.2620068 -4.2540669 -4.2481647 -4.247385 -4.2408094 -4.2272358 -4.2166166 -4.2234616 -4.237958 -4.2557955][-4.1909132 -4.1915617 -4.1884565 -4.2023869 -4.2155409 -4.2120557 -4.1983905 -4.1880589 -4.1871204 -4.1805558 -4.1678395 -4.1572371 -4.1661534 -4.1880069 -4.2146025][-4.1588874 -4.1571465 -4.1520362 -4.1627188 -4.1673079 -4.1556463 -4.1320567 -4.1116681 -4.113452 -4.1143804 -4.1081605 -4.1025028 -4.1202049 -4.1512928 -4.1855454][-4.1429305 -4.1346369 -4.1270881 -4.1299858 -4.1214433 -4.0979309 -4.0550137 -4.0134172 -4.0209832 -4.0421233 -4.0475197 -4.0523109 -4.087595 -4.1335497 -4.1727238][-4.137259 -4.1211739 -4.1111636 -4.0989532 -4.0666361 -4.0214348 -3.9408326 -3.8557274 -3.8755186 -3.9324262 -3.9567254 -3.9853153 -4.0517416 -4.1165209 -4.1641455][-4.14808 -4.1299863 -4.1134357 -4.0737848 -4.004725 -3.9283693 -3.7888823 -3.6371241 -3.6913304 -3.8091078 -3.8583989 -3.9094503 -4.01037 -4.0942883 -4.1519895][-4.1802549 -4.1657534 -4.1378832 -4.0730972 -3.9783106 -3.8900666 -3.7249155 -3.5520482 -3.6424241 -3.7925143 -3.8420305 -3.8905506 -3.9994311 -4.085228 -4.1459827][-4.2202644 -4.2124009 -4.1804929 -4.1187048 -4.0396829 -3.9773409 -3.8575354 -3.7351184 -3.8006325 -3.9055705 -3.9275067 -3.9564018 -4.0420589 -4.1111083 -4.1627026][-4.2601151 -4.2567058 -4.2245207 -4.180985 -4.1292176 -4.0914068 -4.0185962 -3.9371884 -3.9647369 -4.0204439 -4.0261703 -4.0415277 -4.1032419 -4.1578259 -4.1994038][-4.2807946 -4.2800193 -4.2558265 -4.2328529 -4.2061024 -4.1870503 -4.1489558 -4.0966687 -4.1028018 -4.1270819 -4.123776 -4.1278872 -4.1680369 -4.209578 -4.2394366][-4.3002391 -4.3040142 -4.290616 -4.2822323 -4.273602 -4.2641354 -4.24264 -4.20985 -4.2026544 -4.2064371 -4.2008338 -4.2009454 -4.2243576 -4.2509308 -4.267993][-4.3058009 -4.310133 -4.3047 -4.3048425 -4.3042078 -4.2996182 -4.2889972 -4.2716551 -4.2608118 -4.2552361 -4.2505012 -4.2489314 -4.2603626 -4.2759523 -4.2865677][-4.3041873 -4.3053927 -4.3045197 -4.3067565 -4.3075609 -4.30462 -4.2999754 -4.2941771 -4.285934 -4.2789855 -4.2760563 -4.2758503 -4.2824411 -4.2929344 -4.301724][-4.3106356 -4.309504 -4.3078051 -4.307148 -4.3076839 -4.3078794 -4.3068967 -4.3064179 -4.3035808 -4.3005505 -4.298687 -4.2981853 -4.3033185 -4.3118043 -4.3185549]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 15:49:41.088836: step 20010, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 79h:28m:17s remains)
INFO - root - 2017-12-05 15:49:50.124412: step 20020, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.938 sec/batch; 81h:27m:28s remains)
INFO - root - 2017-12-05 15:49:59.175653: step 20030, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 75h:08m:22s remains)
INFO - root - 2017-12-05 15:50:08.210013: step 20040, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 78h:36m:08s remains)
INFO - root - 2017-12-05 15:50:17.212350: step 20050, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 77h:51m:57s remains)
INFO - root - 2017-12-05 15:50:26.441965: step 20060, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 79h:46m:55s remains)
INFO - root - 2017-12-05 15:50:35.635440: step 20070, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 71h:15m:22s remains)
INFO - root - 2017-12-05 15:50:44.558813: step 20080, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 80h:37m:13s remains)
INFO - root - 2017-12-05 15:50:53.722039: step 20090, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 81h:14m:59s remains)
INFO - root - 2017-12-05 15:51:02.781004: step 20100, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 79h:10m:40s remains)
2017-12-05 15:51:03.560710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1952839 -4.2088628 -4.2136269 -4.2065878 -4.1918125 -4.1648583 -4.1247377 -4.0903049 -4.0789084 -4.0835371 -4.111064 -4.1578383 -4.1991134 -4.2247243 -4.2360687][-4.2070851 -4.222436 -4.2250524 -4.2162528 -4.1975746 -4.1659622 -4.1156135 -4.0682468 -4.0487428 -4.0529857 -4.0802717 -4.1265721 -4.1673303 -4.1909518 -4.2019272][-4.2201009 -4.2338257 -4.2343779 -4.2262688 -4.2068539 -4.1733356 -4.115129 -4.05583 -4.0281143 -4.0276809 -4.0518179 -4.0971565 -4.1365647 -4.1587281 -4.1697645][-4.2363238 -4.2474303 -4.2412548 -4.2273035 -4.2031093 -4.1658769 -4.103724 -4.0384169 -4.0105805 -4.0168042 -4.043179 -4.0835791 -4.1162648 -4.1331959 -4.1428027][-4.2499685 -4.2611346 -4.246654 -4.2219725 -4.1902609 -4.149425 -4.0905204 -4.0296817 -4.0105433 -4.0334916 -4.0705333 -4.1046667 -4.1222167 -4.1271505 -4.1315789][-4.2597041 -4.2688727 -4.249907 -4.2206488 -4.189208 -4.1497121 -4.0963492 -4.0407877 -4.0279617 -4.0652118 -4.1160827 -4.1525006 -4.1621089 -4.1574993 -4.1508908][-4.2515426 -4.259202 -4.2418456 -4.215147 -4.1914372 -4.1635389 -4.121635 -4.073277 -4.0594487 -4.1000309 -4.1589112 -4.20355 -4.2206235 -4.2162833 -4.1987329][-4.2387652 -4.2430425 -4.2298756 -4.2102213 -4.1927514 -4.1794524 -4.1513991 -4.1100612 -4.0894151 -4.1216855 -4.178946 -4.2327318 -4.2601609 -4.2608094 -4.2391329][-4.2413044 -4.2418838 -4.2293687 -4.2126656 -4.1967149 -4.1895227 -4.1743984 -4.1412539 -4.1174135 -4.1374059 -4.1838903 -4.2385774 -4.2709203 -4.2736416 -4.2524357][-4.255271 -4.2535896 -4.2405043 -4.2217288 -4.2027593 -4.1955285 -4.1856575 -4.16291 -4.1470704 -4.1600828 -4.1932163 -4.235878 -4.2619171 -4.2603188 -4.2384548][-4.2685242 -4.2630572 -4.249166 -4.2321968 -4.21459 -4.20732 -4.2001624 -4.1877003 -4.1816769 -4.1913652 -4.2110639 -4.2379355 -4.2528825 -4.2439628 -4.2199287][-4.2793446 -4.2717381 -4.2587986 -4.2448416 -4.2289133 -4.2235241 -4.2237568 -4.2231679 -4.2243557 -4.2270746 -4.2364883 -4.252964 -4.26119 -4.2472496 -4.2202463][-4.2852254 -4.2759962 -4.2639108 -4.2540956 -4.2418184 -4.2418213 -4.2507648 -4.2568555 -4.25955 -4.2601571 -4.2651162 -4.2771487 -4.282311 -4.2676849 -4.2409463][-4.2889881 -4.2780361 -4.266551 -4.257823 -4.2484231 -4.2519288 -4.2654243 -4.2761173 -4.2810473 -4.2838316 -4.2879972 -4.2949562 -4.2985339 -4.2873588 -4.263936][-4.2937584 -4.2826228 -4.2704773 -4.2593718 -4.2517605 -4.2578573 -4.2726398 -4.2855129 -4.2921462 -4.2949233 -4.2964272 -4.2974896 -4.2987123 -4.2921224 -4.276154]]...]
INFO - root - 2017-12-05 15:51:12.554475: step 20110, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 82h:27m:17s remains)
INFO - root - 2017-12-05 15:51:21.801814: step 20120, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 79h:06m:31s remains)
INFO - root - 2017-12-05 15:51:30.924188: step 20130, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 79h:40m:16s remains)
INFO - root - 2017-12-05 15:51:39.941304: step 20140, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 82h:30m:41s remains)
INFO - root - 2017-12-05 15:51:48.913909: step 20150, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 79h:35m:12s remains)
INFO - root - 2017-12-05 15:51:58.108674: step 20160, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 77h:12m:10s remains)
INFO - root - 2017-12-05 15:52:07.134500: step 20170, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 80h:29m:52s remains)
INFO - root - 2017-12-05 15:52:16.218538: step 20180, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 76h:21m:52s remains)
INFO - root - 2017-12-05 15:52:25.388256: step 20190, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 77h:38m:53s remains)
INFO - root - 2017-12-05 15:52:34.598338: step 20200, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 81h:16m:12s remains)
2017-12-05 15:52:35.332277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2360864 -4.2546368 -4.2632527 -4.2576389 -4.2433167 -4.2288203 -4.2152486 -4.2043409 -4.2088857 -4.2256031 -4.24096 -4.2504916 -4.254055 -4.2452989 -4.2297931][-4.2601209 -4.271174 -4.2711906 -4.2564454 -4.2341185 -4.2113395 -4.1920104 -4.1811209 -4.1903009 -4.2134213 -4.2319851 -4.2405748 -4.240746 -4.2311468 -4.2167821][-4.2795415 -4.2839994 -4.2779484 -4.257534 -4.2312675 -4.2043424 -4.1827188 -4.1743736 -4.1887412 -4.2122612 -4.2269797 -4.2276516 -4.2216454 -4.211122 -4.2004952][-4.2794881 -4.2787232 -4.2714195 -4.2532234 -4.230772 -4.2070541 -4.1885896 -4.1852469 -4.2025123 -4.2231221 -4.2325854 -4.2247505 -4.2134337 -4.2027941 -4.1940303][-4.2466607 -4.2443819 -4.2397995 -4.2305303 -4.217679 -4.202631 -4.1893473 -4.1901979 -4.21064 -4.2293582 -4.2357388 -4.2268376 -4.2167225 -4.2103062 -4.2051172][-4.1857467 -4.1807923 -4.1781249 -4.1752524 -4.1688695 -4.16116 -4.1555958 -4.1646175 -4.1924715 -4.2162013 -4.2262387 -4.2211485 -4.2155962 -4.2168803 -4.21872][-4.1347661 -4.1260905 -4.1193275 -4.1088834 -4.0938454 -4.0810184 -4.0734806 -4.0883627 -4.1283436 -4.166625 -4.1897526 -4.1936512 -4.1959085 -4.2073441 -4.217309][-4.1115808 -4.0978022 -4.07866 -4.0454431 -4.0045247 -3.9694278 -3.9485562 -3.9741116 -4.04209 -4.1041594 -4.1402874 -4.1556125 -4.1687613 -4.1911283 -4.2094541][-4.127501 -4.1059828 -4.0703759 -4.0071511 -3.9318354 -3.8678238 -3.8279524 -3.8622584 -3.9560442 -4.0368476 -4.08022 -4.10251 -4.1233544 -4.151967 -4.1759915][-4.1666417 -4.1485133 -4.1108141 -4.0402412 -3.9578905 -3.8928661 -3.8448749 -3.8527184 -3.9166796 -3.9797735 -4.0165415 -4.0416303 -4.0703912 -4.1014829 -4.1276937][-4.2089539 -4.2013745 -4.1742272 -4.1170573 -4.0519934 -3.9999692 -3.951515 -3.9296389 -3.944279 -3.9699998 -3.9853859 -4.0007353 -4.0283933 -4.0577474 -4.0847454][-4.2547517 -4.254178 -4.2371454 -4.19686 -4.1531096 -4.1145244 -4.0694842 -4.0321536 -4.0151167 -4.0121555 -4.0084467 -4.0105886 -4.0268321 -4.0468249 -4.0690336][-4.2900658 -4.2908506 -4.2801 -4.2551312 -4.2305059 -4.2085853 -4.1778932 -4.1425991 -4.1145139 -4.0972424 -4.0808005 -4.0706868 -4.0741091 -4.0835772 -4.0975385][-4.3117318 -4.3101649 -4.3015952 -4.28704 -4.2745404 -4.2660761 -4.252522 -4.2315116 -4.2081528 -4.1890774 -4.1716366 -4.1583838 -4.1534009 -4.1524539 -4.154942][-4.3241491 -4.3214736 -4.314743 -4.3052297 -4.2974114 -4.2933922 -4.2885938 -4.2791543 -4.2661648 -4.254518 -4.2428412 -4.23198 -4.2235274 -4.2156754 -4.2104392]]...]
INFO - root - 2017-12-05 15:52:44.514812: step 20210, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 79h:21m:59s remains)
INFO - root - 2017-12-05 15:52:53.758533: step 20220, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 79h:09m:11s remains)
INFO - root - 2017-12-05 15:53:02.926671: step 20230, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 79h:59m:33s remains)
INFO - root - 2017-12-05 15:53:11.929218: step 20240, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.898 sec/batch; 77h:51m:59s remains)
INFO - root - 2017-12-05 15:53:20.898345: step 20250, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 79h:53m:11s remains)
INFO - root - 2017-12-05 15:53:29.942546: step 20260, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 81h:34m:51s remains)
INFO - root - 2017-12-05 15:53:39.171157: step 20270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 81h:24m:40s remains)
INFO - root - 2017-12-05 15:53:48.234857: step 20280, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 78h:13m:22s remains)
INFO - root - 2017-12-05 15:53:57.287900: step 20290, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 76h:45m:26s remains)
INFO - root - 2017-12-05 15:54:06.379115: step 20300, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 79h:23m:30s remains)
2017-12-05 15:54:07.165038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.231504 -4.2107763 -4.19544 -4.1958156 -4.2131329 -4.2327571 -4.24599 -4.2553449 -4.2563896 -4.2556205 -4.2530127 -4.2393808 -4.2114425 -4.1903434 -4.1982222][-4.2169323 -4.18569 -4.1589484 -4.15112 -4.1684842 -4.1938944 -4.2154155 -4.2382064 -4.247128 -4.2430444 -4.235939 -4.2217622 -4.1965308 -4.1770477 -4.1883807][-4.2079835 -4.1710563 -4.1327486 -4.1116667 -4.1206965 -4.14822 -4.1778932 -4.2126474 -4.2367229 -4.2383261 -4.22622 -4.2099209 -4.1858211 -4.1651282 -4.1763139][-4.1990633 -4.162045 -4.1133528 -4.0741172 -4.0672154 -4.08678 -4.1212649 -4.1743307 -4.2221584 -4.2412744 -4.2339597 -4.2144203 -4.1887302 -4.1635671 -4.1672406][-4.1910796 -4.1547289 -4.1018314 -4.0484848 -4.0226393 -4.0248547 -4.0559249 -4.1201 -4.1860166 -4.2245841 -4.2349386 -4.2272787 -4.2062449 -4.1787176 -4.172822][-4.1931825 -4.1567893 -4.1026835 -4.0388417 -3.9873736 -3.9603734 -3.9736331 -4.0400949 -4.1275229 -4.1932263 -4.2258153 -4.2358279 -4.2274585 -4.207284 -4.1977139][-4.2015114 -4.172121 -4.1232896 -4.0579052 -3.9851813 -3.9172442 -3.882622 -3.9282966 -4.0379887 -4.139472 -4.2044587 -4.2408986 -4.2510967 -4.2416143 -4.2324228][-4.2055187 -4.1890397 -4.15642 -4.1082878 -4.0415716 -3.9488914 -3.8629506 -3.8606153 -3.9553328 -4.0676603 -4.1595626 -4.2247028 -4.258019 -4.263412 -4.2595692][-4.1989784 -4.194 -4.1808224 -4.1562853 -4.1179986 -4.051343 -3.9684341 -3.929842 -3.9569066 -4.0211763 -4.1046405 -4.1830506 -4.234026 -4.2594538 -4.2714314][-4.1884627 -4.1901255 -4.1851783 -4.1734905 -4.160017 -4.1364331 -4.095747 -4.0604477 -4.0409803 -4.042727 -4.07955 -4.1355124 -4.1869578 -4.229074 -4.2635846][-4.1770029 -4.1778569 -4.1747417 -4.1678462 -4.1710372 -4.1806684 -4.1782312 -4.1659355 -4.14801 -4.1288218 -4.1207547 -4.1269975 -4.14909 -4.18516 -4.2323217][-4.1722703 -4.1677394 -4.156055 -4.1445174 -4.149251 -4.1723995 -4.1992574 -4.2107859 -4.2090983 -4.1958652 -4.1788387 -4.1581788 -4.1446056 -4.1561351 -4.1986804][-4.1648631 -4.1610503 -4.1420932 -4.1200705 -4.1119385 -4.1294293 -4.1698523 -4.2035346 -4.2182827 -4.214829 -4.2064013 -4.1895075 -4.1638775 -4.1523051 -4.1770272][-4.1456418 -4.1515822 -4.1376681 -4.1152816 -4.0959783 -4.0958343 -4.1258531 -4.1625276 -4.1840982 -4.1934218 -4.2006335 -4.2009416 -4.1871414 -4.1711454 -4.1783772][-4.1191921 -4.1337438 -4.1276283 -4.1154985 -4.10178 -4.093111 -4.1039672 -4.1244621 -4.1385283 -4.14949 -4.1697869 -4.1919804 -4.2018781 -4.2011604 -4.20655]]...]
INFO - root - 2017-12-05 15:54:16.233142: step 20310, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 78h:31m:07s remains)
INFO - root - 2017-12-05 15:54:25.349173: step 20320, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 78h:18m:39s remains)
INFO - root - 2017-12-05 15:54:34.460139: step 20330, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 72h:02m:09s remains)
INFO - root - 2017-12-05 15:54:43.502531: step 20340, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 80h:15m:03s remains)
INFO - root - 2017-12-05 15:54:52.610363: step 20350, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 79h:16m:03s remains)
INFO - root - 2017-12-05 15:55:01.681702: step 20360, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.958 sec/batch; 83h:01m:58s remains)
INFO - root - 2017-12-05 15:55:10.845467: step 20370, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 78h:32m:10s remains)
INFO - root - 2017-12-05 15:55:19.964951: step 20380, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 78h:17m:01s remains)
INFO - root - 2017-12-05 15:55:29.109311: step 20390, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 78h:05m:13s remains)
INFO - root - 2017-12-05 15:55:38.241374: step 20400, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 80h:13m:37s remains)
2017-12-05 15:55:39.003782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2125487 -4.2043271 -4.2041578 -4.2073088 -4.2169952 -4.2302852 -4.2337866 -4.2209725 -4.1878834 -4.1441431 -4.1226916 -4.1484542 -4.196804 -4.2362351 -4.25559][-4.1972551 -4.1839585 -4.1808438 -4.18226 -4.1935868 -4.2127938 -4.2220569 -4.2109847 -4.1761761 -4.1223221 -4.0842357 -4.0992894 -4.1460838 -4.1920104 -4.2215972][-4.1988325 -4.1771035 -4.1654859 -4.16119 -4.1711097 -4.1952305 -4.2109971 -4.2054696 -4.1794944 -4.1292715 -4.0775604 -4.0660419 -4.0941267 -4.13841 -4.175477][-4.2102013 -4.1833248 -4.1618118 -4.147429 -4.1467671 -4.1627469 -4.1788397 -4.1861868 -4.1796293 -4.1445489 -4.0903869 -4.0510497 -4.0463367 -4.0722113 -4.1100497][-4.223927 -4.1995821 -4.1702061 -4.13817 -4.1114459 -4.0974913 -4.1035748 -4.1300931 -4.1553149 -4.1531663 -4.1170821 -4.068253 -4.0288687 -4.0226374 -4.0502033][-4.2372537 -4.2220039 -4.1890779 -4.1378479 -4.0769634 -4.0155606 -3.9923911 -4.0365524 -4.1061525 -4.1513042 -4.1479874 -4.1073527 -4.0508986 -4.0187588 -4.0327487][-4.230124 -4.2289186 -4.1996651 -4.1372552 -4.0473342 -3.9376674 -3.8646386 -3.917294 -4.0352669 -4.1291251 -4.1587949 -4.1345634 -4.0790448 -4.0374689 -4.043561][-4.2005811 -4.2135491 -4.1963506 -4.1384668 -4.0433064 -3.91282 -3.7982168 -3.8402562 -3.986891 -4.1085072 -4.1587543 -4.1501908 -4.1033125 -4.0593147 -4.0585985][-4.162837 -4.1837664 -4.1824708 -4.1461868 -4.0753689 -3.9690149 -3.8625996 -3.8764277 -3.9982576 -4.108676 -4.1606717 -4.1619763 -4.1265483 -4.0837822 -4.0768061][-4.1380305 -4.16021 -4.1713395 -4.1622987 -4.1255174 -4.0595069 -3.9850485 -3.9770789 -4.0423794 -4.1154008 -4.1559305 -4.1627989 -4.1440167 -4.1143193 -4.1036425][-4.1212521 -4.1396565 -4.1581955 -4.171608 -4.1636624 -4.1289568 -4.0827613 -4.0615149 -4.0786242 -4.109858 -4.1351523 -4.1508536 -4.1518703 -4.1414557 -4.1315565][-4.1100364 -4.1248975 -4.1481547 -4.1738844 -4.1850648 -4.1705413 -4.1412044 -4.1137581 -4.0991216 -4.0964661 -4.1049519 -4.1298103 -4.1527286 -4.1623373 -4.1583109][-4.115716 -4.1247683 -4.1483455 -4.1789694 -4.199461 -4.1955023 -4.1755509 -4.1470132 -4.1166229 -4.0958905 -4.0938606 -4.1230164 -4.1608739 -4.1835027 -4.1850986][-4.1289396 -4.13558 -4.15916 -4.19019 -4.2131977 -4.214252 -4.20046 -4.176105 -4.1452403 -4.1216879 -4.1151409 -4.1397371 -4.1767068 -4.2011514 -4.20551][-4.1404185 -4.1529231 -4.1784039 -4.2063179 -4.2285709 -4.2327957 -4.2259049 -4.2120132 -4.1903796 -4.171979 -4.1623535 -4.1733894 -4.1970954 -4.2149754 -4.2195749]]...]
INFO - root - 2017-12-05 15:55:48.102800: step 20410, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 77h:26m:35s remains)
INFO - root - 2017-12-05 15:55:57.162106: step 20420, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.884 sec/batch; 76h:39m:01s remains)
INFO - root - 2017-12-05 15:56:06.256831: step 20430, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 76h:54m:16s remains)
INFO - root - 2017-12-05 15:56:15.296077: step 20440, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.889 sec/batch; 77h:04m:40s remains)
INFO - root - 2017-12-05 15:56:24.160394: step 20450, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 78h:36m:50s remains)
INFO - root - 2017-12-05 15:56:33.262373: step 20460, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 78h:38m:06s remains)
INFO - root - 2017-12-05 15:56:42.413654: step 20470, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 79h:03m:07s remains)
INFO - root - 2017-12-05 15:56:51.468190: step 20480, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 78h:54m:01s remains)
INFO - root - 2017-12-05 15:57:00.452629: step 20490, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 75h:11m:59s remains)
INFO - root - 2017-12-05 15:57:09.387383: step 20500, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 76h:29m:10s remains)
2017-12-05 15:57:10.170098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1656556 -4.1585221 -4.1595321 -4.1636806 -4.1649055 -4.1534386 -4.1296778 -4.1106057 -4.0985618 -4.0863166 -4.0785675 -4.0900445 -4.09743 -4.0888929 -4.0848732][-4.1663518 -4.1621218 -4.1673093 -4.174027 -4.1763544 -4.1585951 -4.1294837 -4.1079531 -4.097712 -4.0889525 -4.0880389 -4.1038342 -4.1019325 -4.0861387 -4.0792747][-4.1615739 -4.1607146 -4.171052 -4.1793032 -4.1798449 -4.1589584 -4.1318097 -4.1128082 -4.1004753 -4.0897894 -4.0964794 -4.1235104 -4.1195207 -4.0967674 -4.0818286][-4.1648555 -4.1670818 -4.1803746 -4.1916952 -4.1908917 -4.1692772 -4.1444821 -4.1230364 -4.1026745 -4.087832 -4.0983887 -4.1334753 -4.1357346 -4.1098723 -4.0886388][-4.1847811 -4.1898031 -4.2006903 -4.208611 -4.2054353 -4.1844783 -4.1554031 -4.1271143 -4.1012521 -4.0793719 -4.0883169 -4.118958 -4.1245866 -4.1016216 -4.0826459][-4.201479 -4.2080169 -4.2095671 -4.2049341 -4.1936126 -4.1706848 -4.1372938 -4.1011276 -4.0688648 -4.0362773 -4.0397334 -4.0710797 -4.0832725 -4.0734773 -4.0642819][-4.2192211 -4.2205625 -4.2072787 -4.1823206 -4.1494341 -4.1083636 -4.0674376 -4.0279536 -3.9954579 -3.9672167 -3.9796081 -4.0291295 -4.0551095 -4.0584836 -4.0567617][-4.2302876 -4.2254262 -4.1981077 -4.1500573 -4.092144 -4.0308475 -3.9862368 -3.9539888 -3.9363813 -3.9298921 -3.9597006 -4.0248938 -4.0601387 -4.0687509 -4.0691519][-4.2172422 -4.2133694 -4.1889062 -4.14203 -4.09022 -4.0423417 -4.0120859 -3.995435 -3.9842978 -3.9855764 -4.0179944 -4.0704212 -4.0958176 -4.0979881 -4.0923567][-4.196712 -4.1994061 -4.1884265 -4.1581149 -4.1279378 -4.1025534 -4.0885668 -4.0824223 -4.07157 -4.0708642 -4.092772 -4.1188016 -4.1269603 -4.1234694 -4.1167555][-4.1807089 -4.1842322 -4.1768379 -4.1574082 -4.1370206 -4.1242237 -4.1193795 -4.1187096 -4.1066613 -4.1038909 -4.121161 -4.1373792 -4.1421766 -4.1462193 -4.1479688][-4.1720815 -4.1721964 -4.1644449 -4.149066 -4.128365 -4.1191339 -4.1203856 -4.1219654 -4.1040039 -4.1022544 -4.1266751 -4.1462154 -4.1531954 -4.1610942 -4.1667428][-4.1661553 -4.1607313 -4.1486664 -4.1324511 -4.1119947 -4.1063881 -4.1132402 -4.1154642 -4.1000233 -4.1072474 -4.1390367 -4.1601791 -4.1651492 -4.1718583 -4.17814][-4.1619544 -4.1502786 -4.1316271 -4.1128573 -4.0933051 -4.0888591 -4.0999622 -4.1091108 -4.1074524 -4.1241426 -4.1603937 -4.1805634 -4.181231 -4.1861949 -4.1944056][-4.1567421 -4.1416078 -4.1243057 -4.1079121 -4.0953274 -4.0908308 -4.1027966 -4.1156569 -4.1253643 -4.1455321 -4.1765418 -4.1913905 -4.1888132 -4.1978631 -4.2094531]]...]
INFO - root - 2017-12-05 15:57:19.416963: step 20510, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 81h:26m:47s remains)
INFO - root - 2017-12-05 15:57:28.527343: step 20520, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 79h:35m:33s remains)
INFO - root - 2017-12-05 15:57:37.576513: step 20530, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.894 sec/batch; 77h:30m:44s remains)
INFO - root - 2017-12-05 15:57:46.552452: step 20540, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 74h:14m:02s remains)
INFO - root - 2017-12-05 15:57:55.655006: step 20550, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 81h:22m:16s remains)
INFO - root - 2017-12-05 15:58:04.799795: step 20560, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 81h:03m:35s remains)
INFO - root - 2017-12-05 15:58:13.763618: step 20570, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 77h:25m:01s remains)
INFO - root - 2017-12-05 15:58:22.953297: step 20580, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 81h:19m:20s remains)
INFO - root - 2017-12-05 15:58:32.106313: step 20590, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 81h:44m:05s remains)
INFO - root - 2017-12-05 15:58:41.317972: step 20600, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 78h:43m:28s remains)
2017-12-05 15:58:42.153375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054104 -4.2995553 -4.294733 -4.2918544 -4.2908225 -4.2913337 -4.2935386 -4.2922816 -4.288053 -4.2853131 -4.2864518 -4.2900968 -4.2947383 -4.2998877 -4.3034477][-4.2923021 -4.2834306 -4.2753448 -4.2699637 -4.2692909 -4.27309 -4.2780008 -4.275012 -4.2671123 -4.2627821 -4.2634645 -4.266058 -4.2724628 -4.2818241 -4.2885861][-4.2699771 -4.2553978 -4.2406092 -4.2317896 -4.2333727 -4.2424192 -4.2503557 -4.244154 -4.2324867 -4.2293849 -4.2317014 -4.2356558 -4.2434421 -4.2563953 -4.2646766][-4.2428722 -4.2198892 -4.1969776 -4.1845379 -4.1907692 -4.206028 -4.218164 -4.2111292 -4.2008629 -4.203589 -4.2080569 -4.2105489 -4.2160239 -4.2286649 -4.2354407][-4.2235208 -4.1896353 -4.1577 -4.1387548 -4.1412115 -4.1582518 -4.1753387 -4.1714735 -4.171864 -4.1854248 -4.1914797 -4.1914 -4.1942458 -4.206099 -4.2129445][-4.2180257 -4.175561 -4.1305304 -4.098949 -4.0911283 -4.1042147 -4.1168523 -4.11657 -4.1310148 -4.1541843 -4.161355 -4.1607041 -4.1674647 -4.1841655 -4.1938696][-4.2227521 -4.1782718 -4.1270833 -4.0888486 -4.0715489 -4.0724044 -4.0725608 -4.0704732 -4.0923977 -4.1200848 -4.1330619 -4.1391354 -4.1557851 -4.1788292 -4.1915212][-4.2294416 -4.18674 -4.1385112 -4.103085 -4.0868449 -4.0810809 -4.0715032 -4.0690131 -4.0922675 -4.1204934 -4.1385045 -4.1523209 -4.1707644 -4.1914325 -4.2034035][-4.2392635 -4.1964812 -4.1524453 -4.1289687 -4.1223907 -4.1205444 -4.109344 -4.1078639 -4.1230073 -4.1418285 -4.1576185 -4.1689491 -4.1803741 -4.1963754 -4.21252][-4.2506351 -4.209527 -4.1715903 -4.1590214 -4.1588182 -4.1603475 -4.155941 -4.1583247 -4.1641779 -4.1692224 -4.1741457 -4.1719933 -4.1722226 -4.1853771 -4.2102637][-4.2615595 -4.2180328 -4.1808577 -4.1707988 -4.1741571 -4.1807384 -4.1823635 -4.1827908 -4.1803422 -4.1746783 -4.1716242 -4.1639366 -4.1580954 -4.1657219 -4.1930718][-4.2657976 -4.2223291 -4.1838336 -4.1716557 -4.1769295 -4.185997 -4.1866288 -4.1788568 -4.1664367 -4.1582494 -4.1594243 -4.1569228 -4.1453109 -4.1410761 -4.1613321][-4.2668781 -4.2252941 -4.186718 -4.170846 -4.1734309 -4.1826487 -4.1821313 -4.1690407 -4.1541772 -4.1493149 -4.1579456 -4.162468 -4.1469779 -4.1325717 -4.1456][-4.2693338 -4.2296009 -4.1903281 -4.170609 -4.1714559 -4.1849723 -4.1872487 -4.1710835 -4.1561742 -4.15654 -4.1742568 -4.1856556 -4.172009 -4.1575394 -4.1636415][-4.2757034 -4.2428503 -4.2094574 -4.1909204 -4.1939049 -4.2082019 -4.2095442 -4.1928926 -4.1806188 -4.1852508 -4.2083707 -4.2245708 -4.2167716 -4.2073121 -4.2042313]]...]
INFO - root - 2017-12-05 15:58:51.318525: step 20610, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 80h:01m:01s remains)
INFO - root - 2017-12-05 15:59:00.480923: step 20620, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 82h:05m:57s remains)
INFO - root - 2017-12-05 15:59:09.476576: step 20630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 79h:01m:20s remains)
INFO - root - 2017-12-05 15:59:18.498176: step 20640, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 76h:27m:01s remains)
INFO - root - 2017-12-05 15:59:27.440292: step 20650, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 71h:51m:15s remains)
INFO - root - 2017-12-05 15:59:36.616593: step 20660, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 80h:21m:01s remains)
INFO - root - 2017-12-05 15:59:45.609959: step 20670, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 79h:54m:25s remains)
INFO - root - 2017-12-05 15:59:54.741386: step 20680, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 78h:46m:33s remains)
INFO - root - 2017-12-05 16:00:03.925331: step 20690, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 77h:28m:19s remains)
INFO - root - 2017-12-05 16:00:13.071053: step 20700, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 77h:23m:02s remains)
2017-12-05 16:00:13.841791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3215117 -4.3117738 -4.2968869 -4.2870793 -4.2812228 -4.2727513 -4.2592363 -4.2583575 -4.2804747 -4.310792 -4.3314047 -4.3459225 -4.3560019 -4.3547187 -4.3485885][-4.3101177 -4.2954974 -4.2786117 -4.2694521 -4.2726288 -4.275527 -4.2714362 -4.2724485 -4.2844248 -4.3079329 -4.3272119 -4.3399816 -4.3524561 -4.3549533 -4.3504639][-4.2870045 -4.2669015 -4.2484994 -4.2357368 -4.2366047 -4.2392077 -4.24051 -4.2423167 -4.2478571 -4.2718759 -4.2954259 -4.3088746 -4.3273559 -4.34186 -4.3452616][-4.2653227 -4.2376008 -4.2134252 -4.1947126 -4.1888185 -4.1817122 -4.1750007 -4.1687379 -4.16806 -4.20099 -4.2363811 -4.258388 -4.2876735 -4.3159776 -4.3310041][-4.2522507 -4.2195034 -4.1874471 -4.1573777 -4.138617 -4.1153245 -4.0877409 -4.0590897 -4.0554104 -4.1107121 -4.16858 -4.2079511 -4.2498846 -4.2908149 -4.3152][-4.2472219 -4.21449 -4.1756048 -4.1275673 -4.0835233 -4.0300589 -3.9634683 -3.8973482 -3.89641 -3.999253 -4.0968242 -4.1624 -4.2221937 -4.2757583 -4.3073955][-4.2528911 -4.2266479 -4.1903162 -4.1332774 -4.0648642 -3.9752483 -3.8535957 -3.7257617 -3.7347038 -3.8919637 -4.0309215 -4.1281424 -4.2078552 -4.2708421 -4.304708][-4.2653136 -4.2446642 -4.2182708 -4.17217 -4.1073303 -4.0115948 -3.8742204 -3.72253 -3.7270901 -3.885674 -4.0244145 -4.1311393 -4.218143 -4.2801523 -4.3102379][-4.2794657 -4.2620783 -4.2447028 -4.2141681 -4.1672974 -4.0954437 -3.9917047 -3.8865705 -3.9007497 -4.0050483 -4.099906 -4.1847281 -4.2575979 -4.3043132 -4.3230219][-4.2930069 -4.2800627 -4.2683039 -4.2505031 -4.2186913 -4.1706862 -4.1049013 -4.0466428 -4.0700393 -4.1391778 -4.1952491 -4.2516356 -4.2985945 -4.3241844 -4.3306003][-4.3036108 -4.2952757 -4.2884188 -4.2803369 -4.262495 -4.2318525 -4.1930313 -4.1659532 -4.1921329 -4.2389455 -4.2750082 -4.3086686 -4.3316407 -4.338253 -4.334146][-4.310679 -4.3061876 -4.3023486 -4.300755 -4.2948818 -4.2790031 -4.2548466 -4.2417707 -4.2626071 -4.2948089 -4.3175983 -4.3366261 -4.3469925 -4.3442059 -4.3347011][-4.3137918 -4.312819 -4.3118596 -4.3127947 -4.3145633 -4.3123045 -4.3015995 -4.2950621 -4.3074017 -4.3251519 -4.3359251 -4.3433948 -4.34715 -4.3420677 -4.3314][-4.3136463 -4.3139777 -4.3152781 -4.3168535 -4.3196058 -4.3224449 -4.3227262 -4.3237677 -4.3319449 -4.3406138 -4.3417354 -4.340518 -4.339541 -4.3340206 -4.3248935][-4.3135281 -4.3129821 -4.314517 -4.3160543 -4.3179407 -4.3199663 -4.3219662 -4.3249426 -4.3303857 -4.3342519 -4.3324046 -4.329217 -4.3275771 -4.3235912 -4.3176131]]...]
INFO - root - 2017-12-05 16:00:22.801439: step 20710, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 77h:26m:36s remains)
INFO - root - 2017-12-05 16:00:32.035781: step 20720, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 76h:58m:10s remains)
INFO - root - 2017-12-05 16:00:41.098647: step 20730, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 76h:12m:00s remains)
INFO - root - 2017-12-05 16:00:50.098634: step 20740, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 80h:48m:23s remains)
INFO - root - 2017-12-05 16:00:59.155409: step 20750, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 77h:05m:31s remains)
INFO - root - 2017-12-05 16:01:08.152108: step 20760, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 79h:55m:59s remains)
INFO - root - 2017-12-05 16:01:17.170101: step 20770, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 79h:45m:28s remains)
INFO - root - 2017-12-05 16:01:26.337628: step 20780, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 79h:52m:06s remains)
INFO - root - 2017-12-05 16:01:35.562878: step 20790, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.907 sec/batch; 78h:32m:03s remains)
INFO - root - 2017-12-05 16:01:44.638565: step 20800, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 76h:25m:59s remains)
2017-12-05 16:01:45.417769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1540775 -4.1586246 -4.1633973 -4.1602206 -4.1564136 -4.1642094 -4.1855063 -4.2081866 -4.2287617 -4.2471094 -4.2664533 -4.2816777 -4.2980866 -4.3116775 -4.3190145][-4.1766524 -4.1787953 -4.1785154 -4.1682129 -4.1665173 -4.1780839 -4.2005382 -4.2212582 -4.2395115 -4.256187 -4.27326 -4.2846079 -4.2971392 -4.3075938 -4.3127341][-4.1971154 -4.1861191 -4.1721816 -4.1505809 -4.143168 -4.1512637 -4.1691766 -4.1952448 -4.2249093 -4.2490463 -4.272191 -4.2866292 -4.2976007 -4.3018918 -4.3020415][-4.2132697 -4.18559 -4.1576052 -4.1273465 -4.1074767 -4.1067367 -4.1141014 -4.1419153 -4.1796918 -4.2146587 -4.2518277 -4.27731 -4.2917404 -4.2936068 -4.2909341][-4.2075682 -4.169219 -4.1372995 -4.1049414 -4.0795703 -4.0691228 -4.0623665 -4.07536 -4.1089406 -4.1544123 -4.2155738 -4.2577338 -4.2781229 -4.2827444 -4.2795534][-4.1825628 -4.142457 -4.1165733 -4.0860896 -4.0581689 -4.0403137 -4.01829 -4.0072012 -4.0215416 -4.0737867 -4.1641426 -4.2286668 -4.2580442 -4.2674394 -4.2690415][-4.1600327 -4.1246319 -4.105783 -4.0766993 -4.0471911 -4.0235367 -3.9783545 -3.9253058 -3.9111841 -3.9763703 -4.0974722 -4.1900005 -4.2315807 -4.2428994 -4.2479482][-4.1540232 -4.1241431 -4.1139803 -4.0902343 -4.0626512 -4.0295968 -3.9599242 -3.8660522 -3.8250873 -3.8996482 -4.0450826 -4.1550088 -4.1994777 -4.2060289 -4.2075953][-4.1619606 -4.1347132 -4.1286411 -4.1148515 -4.1024809 -4.0840945 -4.0216751 -3.9327688 -3.8919477 -3.9499826 -4.0641031 -4.153614 -4.1855292 -4.17863 -4.1664653][-4.160037 -4.1252408 -4.1142592 -4.1078181 -4.1129618 -4.121922 -4.093173 -4.0381656 -4.0098934 -4.0411386 -4.1032724 -4.1606817 -4.1811976 -4.1662297 -4.1420188][-4.1678963 -4.1331725 -4.1135411 -4.0998974 -4.1079373 -4.1336818 -4.1295071 -4.1020412 -4.0814271 -4.091536 -4.1266546 -4.1639142 -4.1798558 -4.1612449 -4.1296597][-4.1720858 -4.1361523 -4.1060271 -4.0778 -4.07901 -4.1101804 -4.1206746 -4.1058536 -4.0877275 -4.0866504 -4.1126919 -4.1505489 -4.1725464 -4.1590462 -4.1299167][-4.1703868 -4.1414723 -4.1065736 -4.0722427 -4.0719543 -4.10509 -4.1194706 -4.1083822 -4.0865483 -4.0728321 -4.0957942 -4.14414 -4.1797118 -4.176908 -4.1561265][-4.174798 -4.1566286 -4.1277342 -4.0973468 -4.0954318 -4.1223073 -4.1394053 -4.1327133 -4.1114707 -4.0921445 -4.1036229 -4.1511941 -4.1942391 -4.1978364 -4.1815219][-4.1737857 -4.1623521 -4.1412563 -4.113245 -4.102994 -4.1171036 -4.1332927 -4.1267323 -4.1059346 -4.0869336 -4.0961504 -4.1460409 -4.1890316 -4.1954188 -4.1829219]]...]
INFO - root - 2017-12-05 16:01:54.584079: step 20810, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 79h:29m:35s remains)
INFO - root - 2017-12-05 16:02:03.712877: step 20820, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 79h:46m:46s remains)
INFO - root - 2017-12-05 16:02:12.785944: step 20830, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 77h:42m:46s remains)
INFO - root - 2017-12-05 16:02:21.827051: step 20840, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 78h:54m:47s remains)
INFO - root - 2017-12-05 16:02:30.974705: step 20850, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.902 sec/batch; 78h:03m:33s remains)
INFO - root - 2017-12-05 16:02:40.049035: step 20860, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 77h:16m:12s remains)
INFO - root - 2017-12-05 16:02:49.196628: step 20870, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 77h:29m:52s remains)
INFO - root - 2017-12-05 16:02:58.187380: step 20880, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 78h:37m:00s remains)
INFO - root - 2017-12-05 16:03:07.326763: step 20890, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.889 sec/batch; 76h:58m:38s remains)
INFO - root - 2017-12-05 16:03:16.548361: step 20900, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 78h:33m:27s remains)
2017-12-05 16:03:17.318350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2680168 -4.2692327 -4.26863 -4.2666116 -4.264061 -4.2617741 -4.2603564 -4.259234 -4.2580614 -4.2573857 -4.2556052 -4.2542028 -4.2546906 -4.2556353 -4.2570691][-4.2679749 -4.2688861 -4.2679548 -4.2659969 -4.2639074 -4.2629108 -4.2630625 -4.2623291 -4.2599211 -4.2570724 -4.2541142 -4.253612 -4.2557487 -4.2573457 -4.2585649][-4.2676158 -4.2682981 -4.2678938 -4.2674618 -4.2672987 -4.2683682 -4.2695923 -4.2676 -4.2611132 -4.2537565 -4.2493014 -4.2506962 -4.2555685 -4.258358 -4.2598171][-4.2661214 -4.2671766 -4.2685986 -4.2710633 -4.2736154 -4.2765656 -4.2773046 -4.2705541 -4.2564831 -4.2427564 -4.2374659 -4.2425895 -4.2519121 -4.2571878 -4.2598429][-4.2643161 -4.2671356 -4.2718349 -4.2782874 -4.2834668 -4.2870803 -4.2840829 -4.2683125 -4.2433982 -4.2227955 -4.2183576 -4.2294326 -4.244873 -4.254487 -4.2601237][-4.2646313 -4.2705984 -4.279882 -4.2904577 -4.2970772 -4.2984753 -4.2875361 -4.2598882 -4.223206 -4.1973228 -4.1964974 -4.2153096 -4.23768 -4.2524624 -4.2621474][-4.2687435 -4.2786293 -4.2926235 -4.305706 -4.3117285 -4.3080134 -4.28738 -4.2481127 -4.2021341 -4.1756344 -4.1815448 -4.20851 -4.2364111 -4.2551889 -4.267549][-4.2749524 -4.2886119 -4.3058853 -4.319623 -4.3232965 -4.31363 -4.2843823 -4.2362876 -4.1862493 -4.1652875 -4.1802063 -4.2136335 -4.2441282 -4.2639084 -4.2763395][-4.2797747 -4.2959089 -4.3150005 -4.3284636 -4.3300648 -4.3153858 -4.2801113 -4.2277622 -4.1814308 -4.17074 -4.1942797 -4.22966 -4.2580705 -4.2757626 -4.2860756][-4.28128 -4.2984595 -4.3177929 -4.3301654 -4.3299503 -4.3127022 -4.275466 -4.2260828 -4.1912 -4.1926618 -4.2207718 -4.2523203 -4.2743373 -4.2871075 -4.2936721][-4.2795186 -4.2963314 -4.3146706 -4.3256974 -4.3247375 -4.3078833 -4.2740726 -4.2347908 -4.2148991 -4.2253056 -4.2515931 -4.2747927 -4.2882566 -4.2948008 -4.2964311][-4.2759576 -4.2915349 -4.3078856 -4.3180304 -4.3180079 -4.3047438 -4.2794447 -4.2536654 -4.2459493 -4.25935 -4.2787638 -4.2920127 -4.2971983 -4.2976723 -4.2936792][-4.2721219 -4.2857914 -4.2994337 -4.3089013 -4.3111186 -4.3040872 -4.2890258 -4.2750959 -4.274313 -4.2855906 -4.2966747 -4.3018441 -4.3011885 -4.2966375 -4.2877693][-4.2694755 -4.2806334 -4.2915616 -4.3004189 -4.3048391 -4.3032684 -4.296267 -4.2903185 -4.2922096 -4.2993703 -4.3038054 -4.3037853 -4.2994313 -4.2915688 -4.2810912][-4.2686939 -4.2770405 -4.2848573 -4.2923865 -4.297255 -4.2985187 -4.2964878 -4.2952247 -4.2972736 -4.300437 -4.3005881 -4.2970824 -4.2903447 -4.2819433 -4.2740769]]...]
INFO - root - 2017-12-05 16:03:26.432140: step 20910, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 79h:11m:59s remains)
INFO - root - 2017-12-05 16:03:35.394450: step 20920, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 77h:11m:16s remains)
INFO - root - 2017-12-05 16:03:44.629820: step 20930, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 81h:57m:34s remains)
INFO - root - 2017-12-05 16:03:53.792370: step 20940, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 79h:39m:22s remains)
INFO - root - 2017-12-05 16:04:02.998955: step 20950, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 78h:26m:19s remains)
INFO - root - 2017-12-05 16:04:12.227086: step 20960, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 78h:36m:57s remains)
INFO - root - 2017-12-05 16:04:21.354739: step 20970, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 75h:50m:52s remains)
INFO - root - 2017-12-05 16:04:30.385773: step 20980, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 78h:18m:13s remains)
INFO - root - 2017-12-05 16:04:39.452678: step 20990, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 78h:18m:03s remains)
INFO - root - 2017-12-05 16:04:48.584222: step 21000, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 79h:08m:14s remains)
2017-12-05 16:04:49.350295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.270968 -4.2842979 -4.2942882 -4.3026934 -4.3077965 -4.3061047 -4.3050475 -4.3041859 -4.2981796 -4.2826662 -4.2775106 -4.2838798 -4.2951093 -4.3063645 -4.3133469][-4.2297482 -4.2423897 -4.2507882 -4.261601 -4.2680573 -4.2655382 -4.2665 -4.265852 -4.2582788 -4.2397151 -4.2343769 -4.2445726 -4.2616949 -4.2792459 -4.2902851][-4.1848989 -4.1967297 -4.201 -4.2091784 -4.2135544 -4.2095723 -4.2130275 -4.2152786 -4.21047 -4.1942554 -4.1918316 -4.2045708 -4.2279658 -4.2506943 -4.2661343][-4.1467457 -4.1586342 -4.1591964 -4.1618924 -4.1600356 -4.1505604 -4.1547132 -4.163177 -4.1625295 -4.1498928 -4.1531234 -4.1731586 -4.2023973 -4.2267461 -4.245007][-4.119823 -4.1365337 -4.1364665 -4.1310177 -4.1196675 -4.1018171 -4.1017184 -4.1157265 -4.1228428 -4.1185355 -4.1321149 -4.1636243 -4.1944985 -4.21568 -4.2306242][-4.0983052 -4.1213069 -4.1237826 -4.1135769 -4.0889645 -4.0512104 -4.0421286 -4.0642347 -4.0872936 -4.0994582 -4.1279941 -4.1701641 -4.1994781 -4.2159944 -4.2286129][-4.0801778 -4.11134 -4.1200027 -4.1053019 -4.0548177 -3.9829626 -3.9524055 -3.9850745 -4.0303903 -4.0654316 -4.1134605 -4.1668043 -4.2002568 -4.222065 -4.2394805][-4.0572467 -4.094986 -4.1066046 -4.0810776 -4.0031757 -3.8923101 -3.8343308 -3.8768077 -3.9441032 -3.999423 -4.0671229 -4.135242 -4.1802368 -4.2164454 -4.2448459][-4.0227385 -4.0710707 -4.0954552 -4.0777612 -4.0061388 -3.8958972 -3.8284643 -3.8575439 -3.915772 -3.9618366 -4.0278792 -4.0964265 -4.1490254 -4.1971607 -4.2359614][-3.9964545 -4.0484905 -4.0816226 -4.0842333 -4.0493126 -3.9842134 -3.9372947 -3.9449978 -3.967762 -3.9753714 -4.0141006 -4.0670977 -4.1180429 -4.1724877 -4.2186608][-4.0056438 -4.0503941 -4.0831566 -4.0961075 -4.0917063 -4.0672646 -4.044683 -4.04171 -4.0412354 -4.0220194 -4.0325341 -4.0631022 -4.1043043 -4.1567516 -4.2037044][-4.0461736 -4.0803323 -4.1082792 -4.1238885 -4.1315055 -4.1267457 -4.1171813 -4.114244 -4.1068287 -4.0791593 -4.0752807 -4.090632 -4.1211848 -4.1648421 -4.20673][-4.1187096 -4.1422539 -4.1632619 -4.1783037 -4.1871953 -4.1884823 -4.187077 -4.185451 -4.178309 -4.1552072 -4.1473017 -4.1544886 -4.176599 -4.2100267 -4.2431345][-4.1982961 -4.2134309 -4.22839 -4.2401986 -4.2479734 -4.2523923 -4.2555428 -4.255384 -4.2501464 -4.2346625 -4.2276926 -4.2298355 -4.2424979 -4.2658215 -4.288641][-4.26784 -4.2788544 -4.289948 -4.2968698 -4.3014889 -4.3022532 -4.3032179 -4.30391 -4.3021073 -4.2941704 -4.2877841 -4.2862048 -4.2908049 -4.3053107 -4.3219781]]...]
INFO - root - 2017-12-05 16:04:58.397791: step 21010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 78h:55m:01s remains)
INFO - root - 2017-12-05 16:05:07.580822: step 21020, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 79h:59m:35s remains)
INFO - root - 2017-12-05 16:05:16.815105: step 21030, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 78h:10m:28s remains)
INFO - root - 2017-12-05 16:05:25.967578: step 21040, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 78h:56m:08s remains)
INFO - root - 2017-12-05 16:05:35.142873: step 21050, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 78h:39m:33s remains)
INFO - root - 2017-12-05 16:05:44.301057: step 21060, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 72h:07m:16s remains)
INFO - root - 2017-12-05 16:05:53.442747: step 21070, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 77h:53m:27s remains)
INFO - root - 2017-12-05 16:06:02.626770: step 21080, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 79h:07m:22s remains)
INFO - root - 2017-12-05 16:06:11.804867: step 21090, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 77h:22m:18s remains)
INFO - root - 2017-12-05 16:06:20.824908: step 21100, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 77h:34m:19s remains)
2017-12-05 16:06:21.572571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3235173 -4.3229065 -4.3206406 -4.3189116 -4.3150792 -4.3046241 -4.2926631 -4.2842155 -4.2821012 -4.2853556 -4.2885509 -4.2921834 -4.293458 -4.2928047 -4.2896094][-4.3040757 -4.3038874 -4.3001804 -4.2940392 -4.2836804 -4.2669621 -4.2524948 -4.2423368 -4.2402153 -4.2479548 -4.2528782 -4.2562966 -4.2570133 -4.258543 -4.2557464][-4.2758474 -4.2765417 -4.2724848 -4.2604208 -4.241509 -4.2184634 -4.2045383 -4.1978874 -4.2011886 -4.2174711 -4.22469 -4.2269197 -4.2255673 -4.2262845 -4.2220116][-4.2425065 -4.246913 -4.24519 -4.2293105 -4.2038641 -4.1759233 -4.1619968 -4.1575832 -4.1678748 -4.1928105 -4.202929 -4.2021594 -4.198082 -4.1974359 -4.1936941][-4.2124519 -4.2242088 -4.2295489 -4.2141943 -4.18344 -4.1478086 -4.127634 -4.1183867 -4.1324253 -4.1670303 -4.182229 -4.1797771 -4.1747656 -4.1749749 -4.1736293][-4.1894965 -4.2121921 -4.2240977 -4.2060971 -4.1643019 -4.1129231 -4.0788622 -4.0563622 -4.0752993 -4.1269674 -4.1520729 -4.1524343 -4.1475496 -4.15156 -4.160017][-4.17185 -4.2003684 -4.2122455 -4.1866565 -4.1278825 -4.0553722 -3.9972222 -3.9483595 -3.9715841 -4.0495749 -4.09125 -4.0981064 -4.0959916 -4.1096888 -4.1320782][-4.1635513 -4.1922364 -4.1946449 -4.1532149 -4.0726933 -3.973496 -3.8781743 -3.7928088 -3.8236034 -3.9328973 -3.9952002 -4.0139914 -4.0209618 -4.0490918 -4.0849533][-4.1813049 -4.2055383 -4.1989074 -4.1476736 -4.0592103 -3.9493406 -3.8323388 -3.7309015 -3.7660229 -3.8817353 -3.9483442 -3.9730191 -3.9875724 -4.0253363 -4.0648241][-4.2077112 -4.2275805 -4.2197685 -4.1756549 -4.1052079 -4.0177164 -3.9225087 -3.8461065 -3.8779223 -3.963392 -4.0133581 -4.0303841 -4.0424156 -4.0783095 -4.1087785][-4.2295432 -4.2474647 -4.2444673 -4.2152362 -4.1709952 -4.1151624 -4.05333 -4.0085006 -4.031424 -4.08051 -4.1091533 -4.1181469 -4.1250391 -4.1529655 -4.1700826][-4.2438016 -4.2598958 -4.2616892 -4.2463942 -4.2235184 -4.1924648 -4.1577625 -4.1366677 -4.1542954 -4.1796288 -4.1924343 -4.1938171 -4.1942258 -4.2112341 -4.2174673][-4.2473764 -4.2598486 -4.2634597 -4.2580328 -4.2479939 -4.2325821 -4.2170205 -4.2130103 -4.2304196 -4.244647 -4.2495441 -4.2453961 -4.2400255 -4.2457108 -4.2431793][-4.2366633 -4.2452822 -4.2479076 -4.2470274 -4.2439251 -4.2382421 -4.2343364 -4.2396154 -4.2550335 -4.2634339 -4.2636871 -4.2551708 -4.2454076 -4.2418995 -4.2368522][-4.2386661 -4.2432771 -4.2430019 -4.2423906 -4.242599 -4.2421179 -4.2432261 -4.250525 -4.2622013 -4.2673178 -4.2642817 -4.2535 -4.242341 -4.2364492 -4.2334385]]...]
INFO - root - 2017-12-05 16:06:30.592820: step 21110, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 78h:05m:10s remains)
INFO - root - 2017-12-05 16:06:39.735793: step 21120, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 77h:02m:24s remains)
INFO - root - 2017-12-05 16:06:48.969938: step 21130, loss = 2.06, batch loss = 2.00 (7.9 examples/sec; 1.016 sec/batch; 87h:54m:22s remains)
INFO - root - 2017-12-05 16:06:58.159194: step 21140, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 78h:50m:56s remains)
INFO - root - 2017-12-05 16:07:07.195115: step 21150, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 76h:09m:03s remains)
INFO - root - 2017-12-05 16:07:16.211935: step 21160, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 80h:17m:50s remains)
INFO - root - 2017-12-05 16:07:25.303742: step 21170, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 77h:00m:17s remains)
INFO - root - 2017-12-05 16:07:34.519353: step 21180, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 81h:31m:34s remains)
INFO - root - 2017-12-05 16:07:43.601391: step 21190, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 77h:52m:49s remains)
INFO - root - 2017-12-05 16:07:52.636058: step 21200, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 81h:28m:29s remains)
2017-12-05 16:07:53.430545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1404343 -4.1207681 -4.1108146 -4.1218433 -4.1439376 -4.160676 -4.16593 -4.1707597 -4.1687479 -4.1689029 -4.1960373 -4.22764 -4.2384124 -4.2366176 -4.2227879][-4.13204 -4.1179862 -4.1098247 -4.1171451 -4.1440058 -4.1669912 -4.1771531 -4.1873412 -4.1876712 -4.1862507 -4.2081504 -4.2333646 -4.23882 -4.234272 -4.2212543][-4.1331005 -4.1332226 -4.1296563 -4.12938 -4.1496186 -4.1663055 -4.1706891 -4.1823392 -4.1907935 -4.1962829 -4.2147923 -4.2286034 -4.2245069 -4.2166505 -4.2109437][-4.1423459 -4.1582174 -4.1568284 -4.144845 -4.14788 -4.1429038 -4.1273966 -4.1330948 -4.1547623 -4.17994 -4.2021894 -4.2059994 -4.1933784 -4.1786447 -4.1775694][-4.1648607 -4.1860237 -4.1793475 -4.1499705 -4.1231771 -4.0833416 -4.0382061 -4.0366783 -4.0837049 -4.1447058 -4.182128 -4.1842203 -4.1701713 -4.1427188 -4.132884][-4.2094078 -4.2222853 -4.2035713 -4.1516566 -4.0875483 -4.0062318 -3.924994 -3.914115 -3.9872172 -4.0869431 -4.1469746 -4.153511 -4.1404848 -4.1052094 -4.0835819][-4.2524042 -4.2584486 -4.2325978 -4.1610379 -4.0597272 -3.9376695 -3.8268187 -3.8109655 -3.9031949 -4.0328994 -4.1119766 -4.1212492 -4.1059809 -4.0642991 -4.0360518][-4.2818294 -4.2856994 -4.2608743 -4.1882534 -4.0728397 -3.9405284 -3.8332934 -3.8205862 -3.9060621 -4.030539 -4.1116295 -4.11782 -4.0942411 -4.0457869 -4.0082626][-4.2962303 -4.2992606 -4.2799778 -4.2239876 -4.1294384 -4.0285964 -3.96173 -3.9609385 -4.0131373 -4.0950952 -4.1536078 -4.1522083 -4.1209469 -4.06821 -4.020298][-4.3011742 -4.3018103 -4.2874131 -4.2481756 -4.179379 -4.1145077 -4.0876508 -4.1005259 -4.1234941 -4.1608038 -4.1945262 -4.1855683 -4.1504469 -4.0962625 -4.043539][-4.2972341 -4.2937469 -4.2801557 -4.25106 -4.2007837 -4.15716 -4.1493797 -4.1705279 -4.1809783 -4.1930165 -4.2104535 -4.1981859 -4.160069 -4.1033759 -4.0475073][-4.2927775 -4.2880349 -4.2733092 -4.2471323 -4.2082973 -4.1759276 -4.1742573 -4.1956067 -4.2010121 -4.2030621 -4.2100749 -4.1976614 -4.1604323 -4.1053596 -4.0557656][-4.300004 -4.2955432 -4.2798548 -4.2561517 -4.2292528 -4.208374 -4.2135205 -4.2309504 -4.2279749 -4.2206788 -4.2168646 -4.1990767 -4.162446 -4.1130915 -4.0777378][-4.3089476 -4.30359 -4.2878404 -4.2687902 -4.2528358 -4.2438741 -4.2565608 -4.2721729 -4.2649488 -4.2522912 -4.23884 -4.2141237 -4.1753364 -4.1264296 -4.09868][-4.3105459 -4.3060269 -4.2922215 -4.2763662 -4.2656188 -4.2632174 -4.2793169 -4.29537 -4.2922387 -4.2797675 -4.2579122 -4.2239609 -4.1779075 -4.1239176 -4.0950375]]...]
INFO - root - 2017-12-05 16:08:02.382953: step 21210, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 75h:18m:08s remains)
INFO - root - 2017-12-05 16:08:11.500753: step 21220, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 77h:23m:30s remains)
INFO - root - 2017-12-05 16:08:20.514211: step 21230, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 76h:47m:56s remains)
INFO - root - 2017-12-05 16:08:29.577678: step 21240, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 75h:40m:08s remains)
INFO - root - 2017-12-05 16:08:39.010985: step 21250, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.969 sec/batch; 83h:47m:50s remains)
INFO - root - 2017-12-05 16:08:48.106931: step 21260, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 78h:34m:14s remains)
INFO - root - 2017-12-05 16:08:57.080623: step 21270, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 78h:30m:45s remains)
INFO - root - 2017-12-05 16:09:06.154229: step 21280, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 78h:30m:25s remains)
INFO - root - 2017-12-05 16:09:15.107500: step 21290, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 72h:11m:31s remains)
INFO - root - 2017-12-05 16:09:24.204751: step 21300, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 80h:47m:08s remains)
2017-12-05 16:09:24.964025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1745796 -4.1922054 -4.21681 -4.2228251 -4.19827 -4.1669645 -4.1390328 -4.1417985 -4.1692033 -4.2076263 -4.2499609 -4.2826591 -4.2945609 -4.3002515 -4.3142638][-4.150321 -4.175159 -4.2034035 -4.211853 -4.183022 -4.14762 -4.1116462 -4.1116996 -4.1490593 -4.198329 -4.2449017 -4.2779875 -4.2916121 -4.3014135 -4.3153491][-4.1291485 -4.1594133 -4.1873422 -4.1899791 -4.1558638 -4.1164074 -4.0782456 -4.0833755 -4.1333132 -4.1933746 -4.2452269 -4.2793455 -4.2923713 -4.3043165 -4.3168826][-4.118751 -4.150456 -4.1735158 -4.1688194 -4.1399994 -4.1048985 -4.0681911 -4.0723977 -4.1253386 -4.1893744 -4.2454534 -4.2817187 -4.2959971 -4.3066339 -4.317399][-4.1321335 -4.1605229 -4.1763253 -4.1682353 -4.1516 -4.1295705 -4.0930605 -4.0845056 -4.127553 -4.1900873 -4.2494559 -4.2855787 -4.3002057 -4.3085275 -4.3178644][-4.1592879 -4.1723018 -4.1759877 -4.1680946 -4.1630569 -4.15384 -4.12426 -4.1063118 -4.1382446 -4.1999164 -4.2569356 -4.289166 -4.3043017 -4.3109469 -4.3184838][-4.1512041 -4.1417084 -4.1298804 -4.1242967 -4.1381154 -4.1485033 -4.1360912 -4.120038 -4.1464958 -4.2079463 -4.2614336 -4.2903748 -4.3051581 -4.3118696 -4.3184118][-4.1134152 -4.0848136 -4.0603924 -4.0646296 -4.1013184 -4.1291018 -4.127511 -4.1149554 -4.1350694 -4.1960382 -4.252274 -4.2819819 -4.29894 -4.3094921 -4.3176751][-4.0885596 -4.0541396 -4.0250177 -4.0428119 -4.091269 -4.1249995 -4.1253681 -4.111392 -4.1239057 -4.1817532 -4.2428236 -4.2751856 -4.292459 -4.3061209 -4.3169746][-4.0862503 -4.0707359 -4.0554128 -4.0803375 -4.1236682 -4.153008 -4.1514111 -4.1343393 -4.137445 -4.1849575 -4.2450209 -4.2767558 -4.2911839 -4.3039551 -4.3160753][-4.1122923 -4.1224809 -4.1207414 -4.1405387 -4.1726742 -4.1925111 -4.1851821 -4.1685376 -4.1635785 -4.1967454 -4.25136 -4.2829418 -4.2942591 -4.3033128 -4.31511][-4.1623077 -4.1799288 -4.1850605 -4.1945229 -4.2147264 -4.2251167 -4.2120352 -4.1974959 -4.1953721 -4.2203608 -4.2665014 -4.2938805 -4.3001456 -4.3045921 -4.315][-4.2034597 -4.2189765 -4.2245903 -4.2268348 -4.2366304 -4.24075 -4.2303982 -4.2223454 -4.2198396 -4.2401624 -4.2795129 -4.3029 -4.304461 -4.3057466 -4.3157024][-4.2309246 -4.2423167 -4.2441521 -4.2385273 -4.2365909 -4.2372355 -4.2358394 -4.2354155 -4.2278476 -4.240375 -4.2758441 -4.297327 -4.3009257 -4.3042746 -4.3161783][-4.2402754 -4.2489872 -4.251049 -4.2406631 -4.2284627 -4.2222219 -4.2270408 -4.2331114 -4.2248721 -4.23174 -4.2629423 -4.28545 -4.2927632 -4.300745 -4.3161883]]...]
INFO - root - 2017-12-05 16:09:34.101383: step 21310, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 81h:23m:19s remains)
INFO - root - 2017-12-05 16:09:43.412432: step 21320, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 78h:29m:55s remains)
INFO - root - 2017-12-05 16:09:52.544906: step 21330, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 80h:14m:30s remains)
INFO - root - 2017-12-05 16:10:01.626840: step 21340, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 77h:02m:39s remains)
INFO - root - 2017-12-05 16:10:10.668303: step 21350, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 76h:03m:11s remains)
INFO - root - 2017-12-05 16:10:19.669362: step 21360, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 72h:26m:38s remains)
INFO - root - 2017-12-05 16:10:28.760716: step 21370, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 79h:02m:19s remains)
INFO - root - 2017-12-05 16:10:37.835963: step 21380, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.894 sec/batch; 77h:13m:29s remains)
INFO - root - 2017-12-05 16:10:46.801477: step 21390, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 80h:51m:18s remains)
INFO - root - 2017-12-05 16:10:56.042834: step 21400, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 79h:31m:34s remains)
2017-12-05 16:10:56.812295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3024244 -4.3003721 -4.3057232 -4.3110418 -4.3143539 -4.3127742 -4.3118486 -4.3120832 -4.3133631 -4.3201709 -4.3203568 -4.3111267 -4.303154 -4.301394 -4.3043556][-4.2791243 -4.2732115 -4.2792072 -4.2869105 -4.2910676 -4.2867374 -4.2819109 -4.2803378 -4.2847166 -4.2983794 -4.3051605 -4.2967186 -4.2814827 -4.2750611 -4.2792611][-4.2547917 -4.2451744 -4.2518945 -4.2638025 -4.2694268 -4.2618217 -4.2493277 -4.2429514 -4.2481365 -4.2674322 -4.2822804 -4.2797666 -4.26228 -4.2515821 -4.2544093][-4.2311254 -4.2209816 -4.2298856 -4.245482 -4.2525086 -4.2384 -4.2157664 -4.1996794 -4.2007108 -4.2270389 -4.2562542 -4.263957 -4.2490377 -4.235425 -4.2335305][-4.1998091 -4.1911559 -4.2033324 -4.2215014 -4.2306633 -4.20943 -4.1715174 -4.1374493 -4.1288443 -4.1649203 -4.2136512 -4.2339358 -4.2232575 -4.2106476 -4.2051792][-4.1737418 -4.1648784 -4.1790013 -4.1976476 -4.2064652 -4.1754913 -4.1137047 -4.0512938 -4.0276556 -4.0777144 -4.1522694 -4.1928554 -4.1947904 -4.1893282 -4.1841378][-4.1727834 -4.166615 -4.1784496 -4.1935267 -4.1960545 -4.1531472 -4.06741 -3.9701567 -3.9202271 -3.9804778 -4.0843291 -4.1493707 -4.172483 -4.1855626 -4.1882238][-4.1848717 -4.1830378 -4.1892304 -4.2008839 -4.2020178 -4.1604409 -4.0740366 -3.97075 -3.9097269 -3.9612877 -4.0640621 -4.1339388 -4.168128 -4.1961188 -4.2080712][-4.1926136 -4.1911912 -4.19455 -4.2029815 -4.2048006 -4.17754 -4.1196418 -4.048059 -4.0075274 -4.0352416 -4.0944176 -4.1404152 -4.1685362 -4.198854 -4.2168727][-4.1829429 -4.177453 -4.1780939 -4.1861987 -4.1908832 -4.1820893 -4.159019 -4.1262889 -4.1032009 -4.1069632 -4.1240749 -4.1431808 -4.1574712 -4.1814933 -4.2026582][-4.1678782 -4.1565719 -4.1521168 -4.1563239 -4.1612549 -4.1662965 -4.17131 -4.1680365 -4.1534338 -4.14042 -4.136394 -4.1396775 -4.1409631 -4.1553063 -4.1768885][-4.1558361 -4.1422544 -4.1344762 -4.1353788 -4.1395669 -4.1520638 -4.1733432 -4.1856055 -4.1760635 -4.1587982 -4.1484003 -4.1428208 -4.1336727 -4.1384144 -4.1558456][-4.1517739 -4.1434965 -4.1432619 -4.1521549 -4.1608076 -4.1751146 -4.1971269 -4.2113872 -4.2057114 -4.1938162 -4.18316 -4.1689038 -4.1487861 -4.1437531 -4.1541462][-4.1844668 -4.1865873 -4.1990356 -4.2159758 -4.2268004 -4.2360611 -4.248632 -4.2559891 -4.2506385 -4.2437949 -4.2349658 -4.2181635 -4.1948156 -4.1839337 -4.1883731][-4.2535806 -4.2601681 -4.2750077 -4.289012 -4.2946754 -4.2961035 -4.3003888 -4.3015881 -4.2964144 -4.29161 -4.2853403 -4.2740197 -4.2589469 -4.2513633 -4.2527881]]...]
INFO - root - 2017-12-05 16:11:06.042410: step 21410, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 77h:41m:37s remains)
INFO - root - 2017-12-05 16:11:15.227049: step 21420, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 78h:57m:38s remains)
INFO - root - 2017-12-05 16:11:24.248607: step 21430, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 78h:22m:16s remains)
INFO - root - 2017-12-05 16:11:33.411418: step 21440, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 76h:34m:10s remains)
INFO - root - 2017-12-05 16:11:42.489242: step 21450, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 79h:41m:43s remains)
INFO - root - 2017-12-05 16:11:51.359711: step 21460, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 73h:58m:49s remains)
INFO - root - 2017-12-05 16:12:00.537078: step 21470, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 72h:38m:47s remains)
INFO - root - 2017-12-05 16:12:09.541766: step 21480, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 79h:46m:08s remains)
INFO - root - 2017-12-05 16:12:18.622355: step 21490, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 76h:30m:03s remains)
INFO - root - 2017-12-05 16:12:27.616510: step 21500, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 80h:01m:29s remains)
2017-12-05 16:12:28.417886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.197094 -4.2116737 -4.2238393 -4.2369075 -4.2450156 -4.2398396 -4.2366347 -4.2304912 -4.2251849 -4.2221131 -4.2246618 -4.2277884 -4.2295218 -4.226491 -4.2201791][-4.2148905 -4.2301579 -4.2410116 -4.2521458 -4.259819 -4.2576094 -4.25371 -4.2472982 -4.2418337 -4.2349863 -4.2312202 -4.228652 -4.2263451 -4.2228808 -4.22151][-4.2122264 -4.2320719 -4.2480083 -4.2614827 -4.269135 -4.267107 -4.2582045 -4.2459178 -4.2335806 -4.2205715 -4.2122455 -4.2097025 -4.2103362 -4.2110648 -4.2107792][-4.2118654 -4.2300334 -4.2440305 -4.2549205 -4.2590647 -4.2539525 -4.2380843 -4.2147093 -4.1910295 -4.171504 -4.1637425 -4.1705384 -4.1834593 -4.1912045 -4.1917405][-4.22211 -4.2313347 -4.23351 -4.2318339 -4.2267327 -4.2108965 -4.1837473 -4.1498041 -4.1187677 -4.1033731 -4.1072059 -4.1294432 -4.1568952 -4.1732864 -4.1758733][-4.2197142 -4.216311 -4.2059488 -4.1896677 -4.1727753 -4.1463351 -4.1109481 -4.0715413 -4.0446391 -4.0449734 -4.0652781 -4.1010041 -4.138555 -4.1610389 -4.1678367][-4.186614 -4.1704826 -4.1518254 -4.1265149 -4.0997219 -4.0626521 -4.0220923 -3.9848325 -3.9767921 -4.0068679 -4.0456958 -4.0856137 -4.1253147 -4.1514235 -4.16361][-4.1509795 -4.1233768 -4.100852 -4.0725031 -4.0340533 -3.9804442 -3.9326885 -3.9082603 -3.9330993 -3.993295 -4.0435963 -4.0815406 -4.1181617 -4.1468978 -4.1612072][-4.1309848 -4.1079326 -4.09029 -4.05859 -4.0034671 -3.9345319 -3.8924179 -3.8967085 -3.9465618 -4.0133047 -4.0598364 -4.0943484 -4.1283832 -4.1554894 -4.1683393][-4.1328368 -4.12166 -4.1115971 -4.0766439 -4.0124521 -3.9516516 -3.9373584 -3.9609818 -4.0097928 -4.0644116 -4.1004343 -4.1266389 -4.1478109 -4.1667261 -4.176487][-4.1446857 -4.1455665 -4.1432095 -4.1114392 -4.0576591 -4.0187697 -4.0191064 -4.0393357 -4.0741892 -4.1166158 -4.1435547 -4.1552181 -4.159637 -4.1657796 -4.1682568][-4.16852 -4.1822186 -4.1903467 -4.167232 -4.1278 -4.1046028 -4.1084018 -4.1215715 -4.1466155 -4.1805458 -4.1976056 -4.1921315 -4.1789584 -4.1699409 -4.1648865][-4.2050929 -4.2258554 -4.2391291 -4.2236385 -4.1954241 -4.1807771 -4.1849618 -4.195497 -4.2148724 -4.2400789 -4.2468147 -4.229156 -4.2017636 -4.1832323 -4.1757679][-4.2320123 -4.2533379 -4.2666526 -4.2585278 -4.2394047 -4.22926 -4.2313581 -4.2408667 -4.256928 -4.2715864 -4.2688818 -4.2428889 -4.2099142 -4.1913753 -4.1877513][-4.2412963 -4.2638264 -4.276834 -4.2725568 -4.2594366 -4.2509007 -4.2525496 -4.25972 -4.27038 -4.2736459 -4.2618179 -4.2351227 -4.2082815 -4.1967225 -4.1951551]]...]
INFO - root - 2017-12-05 16:12:37.406873: step 21510, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 76h:37m:15s remains)
INFO - root - 2017-12-05 16:12:46.511093: step 21520, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 78h:15m:09s remains)
INFO - root - 2017-12-05 16:12:55.709200: step 21530, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 79h:54m:14s remains)
INFO - root - 2017-12-05 16:13:04.845955: step 21540, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.918 sec/batch; 79h:15m:30s remains)
INFO - root - 2017-12-05 16:13:13.878257: step 21550, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 75h:52m:22s remains)
INFO - root - 2017-12-05 16:13:22.797847: step 21560, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 77h:27m:27s remains)
INFO - root - 2017-12-05 16:13:31.932651: step 21570, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 80h:06m:03s remains)
INFO - root - 2017-12-05 16:13:40.965893: step 21580, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 78h:48m:53s remains)
INFO - root - 2017-12-05 16:13:49.902474: step 21590, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 74h:51m:40s remains)
INFO - root - 2017-12-05 16:13:58.927876: step 21600, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 77h:34m:04s remains)
2017-12-05 16:13:59.679301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2322478 -4.2399063 -4.2452955 -4.2472296 -4.2386208 -4.2231503 -4.2079673 -4.2017426 -4.2046094 -4.2139082 -4.2272649 -4.2442989 -4.2578149 -4.2715082 -4.2855082][-4.2288723 -4.2311783 -4.2326069 -4.2319503 -4.221272 -4.203548 -4.1870856 -4.1805086 -4.1851296 -4.199008 -4.2195578 -4.2427897 -4.2580972 -4.2704053 -4.28252][-4.20422 -4.2003202 -4.1962605 -4.1904163 -4.1768994 -4.1580544 -4.1436491 -4.1425881 -4.1538329 -4.1749635 -4.2036657 -4.2332945 -4.2529817 -4.2664442 -4.2779651][-4.1562819 -4.1515574 -4.1455259 -4.1352959 -4.1197968 -4.1016364 -4.0922074 -4.101748 -4.1244149 -4.1551867 -4.1901622 -4.2245879 -4.2490306 -4.2645855 -4.2760191][-4.0988951 -4.0983634 -4.096056 -4.0872135 -4.0745544 -4.0581264 -4.052237 -4.0697541 -4.1015382 -4.1394463 -4.1786642 -4.2159925 -4.2436972 -4.2628312 -4.2761564][-4.0601912 -4.0617356 -4.0606976 -4.0531669 -4.0433631 -4.0272 -4.0196595 -4.0386004 -4.0752478 -4.1175656 -4.162518 -4.2046423 -4.2359004 -4.2597837 -4.2766867][-4.04778 -4.0491328 -4.047514 -4.0399294 -4.0299153 -4.0116696 -4.00086 -4.0185323 -4.0546927 -4.0978885 -4.1461535 -4.1918116 -4.227344 -4.2559781 -4.2763209][-4.0596313 -4.0583305 -4.0546718 -4.0444365 -4.0312033 -4.0116582 -3.9977167 -4.0089641 -4.0394917 -4.0804973 -4.129169 -4.176404 -4.2152853 -4.2482667 -4.2725463][-4.0577054 -4.0546303 -4.0494394 -4.0384355 -4.0239196 -4.0054069 -3.9902759 -3.9949262 -4.0195532 -4.0593376 -4.108355 -4.1579852 -4.2014456 -4.238585 -4.2665949][-4.063581 -4.0627627 -4.058847 -4.0525188 -4.042027 -4.0264325 -4.0094218 -4.0060396 -4.0208139 -4.0523596 -4.0955791 -4.1437435 -4.1894937 -4.2297163 -4.26147][-4.086556 -4.0892448 -4.0888782 -4.0867524 -4.0814991 -4.0683846 -4.0491414 -4.0389781 -4.043962 -4.0646925 -4.0981793 -4.1413074 -4.1852679 -4.2262921 -4.2603712][-4.1319652 -4.1327281 -4.129735 -4.1269579 -4.1258326 -4.1184945 -4.1026726 -4.092906 -4.0943003 -4.1068273 -4.1300583 -4.1633372 -4.1999307 -4.2362332 -4.2670093][-4.188282 -4.1878777 -4.1839581 -4.1804357 -4.178669 -4.1729836 -4.1604795 -4.1525474 -4.15247 -4.1602163 -4.1756635 -4.1986713 -4.2248211 -4.25303 -4.2774606][-4.2184458 -4.2203908 -4.2188473 -4.2162404 -4.2125316 -4.2054014 -4.1940413 -4.1877308 -4.1892161 -4.1968074 -4.2097912 -4.2279425 -4.2477593 -4.2695637 -4.2881188][-4.207973 -4.2128048 -4.2128654 -4.2108278 -4.2062082 -4.1986971 -4.1901755 -4.189043 -4.19683 -4.2096577 -4.2261257 -4.2448068 -4.2635093 -4.2828608 -4.2975836]]...]
INFO - root - 2017-12-05 16:14:08.905991: step 21610, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 76h:26m:04s remains)
INFO - root - 2017-12-05 16:14:17.954287: step 21620, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 77h:37m:12s remains)
INFO - root - 2017-12-05 16:14:27.024966: step 21630, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 75h:47m:12s remains)
INFO - root - 2017-12-05 16:14:36.302579: step 21640, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.903 sec/batch; 77h:59m:01s remains)
INFO - root - 2017-12-05 16:14:45.331913: step 21650, loss = 2.02, batch loss = 1.96 (9.0 examples/sec; 0.886 sec/batch; 76h:32m:38s remains)
INFO - root - 2017-12-05 16:14:54.503827: step 21660, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 79h:41m:00s remains)
INFO - root - 2017-12-05 16:15:03.488632: step 21670, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 81h:27m:51s remains)
INFO - root - 2017-12-05 16:15:12.577264: step 21680, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 77h:58m:28s remains)
INFO - root - 2017-12-05 16:15:21.669849: step 21690, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 76h:39m:25s remains)
INFO - root - 2017-12-05 16:15:30.632210: step 21700, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.884 sec/batch; 76h:16m:39s remains)
2017-12-05 16:15:31.407522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1312008 -4.1669693 -4.202908 -4.2324715 -4.2527022 -4.261723 -4.2534943 -4.23788 -4.2080708 -4.1856227 -4.178894 -4.1720939 -4.1730108 -4.1816406 -4.1871619][-4.1206951 -4.1539211 -4.1865368 -4.2101789 -4.2253642 -4.232461 -4.2281685 -4.2161818 -4.19423 -4.1786566 -4.17515 -4.1703615 -4.1715293 -4.1799603 -4.1838036][-4.1267996 -4.15187 -4.1765633 -4.1945252 -4.2019186 -4.204545 -4.2046938 -4.1969404 -4.1849189 -4.1815224 -4.1850138 -4.1868482 -4.1859493 -4.1839046 -4.17636][-4.1371288 -4.156868 -4.1743436 -4.1839056 -4.1812792 -4.1759524 -4.1696925 -4.1579428 -4.159122 -4.17693 -4.1940079 -4.2033091 -4.2011085 -4.1920476 -4.1733813][-4.1347823 -4.1589241 -4.1779351 -4.1788197 -4.1657338 -4.146729 -4.1192551 -4.0910554 -4.1020288 -4.1491761 -4.1867137 -4.2020445 -4.1999555 -4.190196 -4.1609211][-4.1300073 -4.1595607 -4.1810694 -4.1786361 -4.1636858 -4.1321344 -4.0710864 -4.0061593 -4.014411 -4.0901246 -4.1548023 -4.1821609 -4.1891303 -4.1772032 -4.1360555][-4.1323185 -4.1695018 -4.1923962 -4.189724 -4.1695976 -4.1244712 -4.0376935 -3.9336109 -3.9343626 -4.0321817 -4.1143847 -4.1544714 -4.1686797 -4.1552029 -4.1070771][-4.1294117 -4.1696115 -4.1929207 -4.1956682 -4.1738491 -4.1227183 -4.0319657 -3.9177253 -3.9154046 -4.0170021 -4.1044135 -4.1486578 -4.1649818 -4.1539936 -4.110599][-4.1256557 -4.1654387 -4.1908984 -4.199718 -4.1845942 -4.1387415 -4.0649185 -3.9758959 -3.9728618 -4.0524426 -4.1274734 -4.1675181 -4.1838889 -4.1789927 -4.1486068][-4.1361723 -4.1757283 -4.2019191 -4.2134943 -4.2065291 -4.1726027 -4.1230412 -4.0606337 -4.0573459 -4.1115065 -4.1659484 -4.1935048 -4.2076368 -4.2114587 -4.19772][-4.1518097 -4.1895413 -4.2107434 -4.2205095 -4.2224 -4.205688 -4.17866 -4.1382608 -4.1351433 -4.1716676 -4.2075605 -4.2254572 -4.2361379 -4.2433696 -4.2406192][-4.1522532 -4.1878357 -4.2080746 -4.2215276 -4.2310858 -4.2263007 -4.2156568 -4.1984296 -4.1975441 -4.2172184 -4.2372971 -4.2492042 -4.2571664 -4.2643371 -4.264873][-4.141665 -4.1804843 -4.205843 -4.22572 -4.2409081 -4.23926 -4.2345009 -4.2303357 -4.2295375 -4.2372808 -4.2502804 -4.2597284 -4.2645626 -4.2696056 -4.2682858][-4.14264 -4.1817012 -4.2094293 -4.233459 -4.2480769 -4.2458854 -4.2413568 -4.2395163 -4.2371697 -4.2408819 -4.2525835 -4.2631764 -4.2661085 -4.2664871 -4.2611732][-4.162374 -4.19843 -4.2252903 -4.2480025 -4.25674 -4.2524381 -4.2463231 -4.2443361 -4.2450967 -4.2469196 -4.255085 -4.264195 -4.2675929 -4.2653432 -4.260107]]...]
INFO - root - 2017-12-05 16:15:40.635858: step 21710, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 78h:10m:20s remains)
INFO - root - 2017-12-05 16:15:49.966490: step 21720, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 82h:31m:47s remains)
INFO - root - 2017-12-05 16:15:59.204876: step 21730, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 81h:29m:56s remains)
INFO - root - 2017-12-05 16:16:08.094611: step 21740, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 75h:45m:58s remains)
INFO - root - 2017-12-05 16:16:17.215309: step 21750, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 78h:36m:41s remains)
INFO - root - 2017-12-05 16:16:26.330517: step 21760, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 75h:30m:36s remains)
INFO - root - 2017-12-05 16:16:35.354992: step 21770, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 77h:15m:51s remains)
INFO - root - 2017-12-05 16:16:44.383027: step 21780, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 76h:58m:22s remains)
INFO - root - 2017-12-05 16:16:53.414122: step 21790, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.914 sec/batch; 78h:50m:54s remains)
INFO - root - 2017-12-05 16:17:02.408035: step 21800, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 75h:23m:32s remains)
2017-12-05 16:17:03.142463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1297574 -4.1386948 -4.141149 -4.1231337 -4.1031427 -4.0975533 -4.1142015 -4.1362791 -4.16335 -4.1953216 -4.2307429 -4.2492676 -4.25774 -4.2675056 -4.2718229][-4.0982037 -4.11444 -4.1208196 -4.1048231 -4.0834074 -4.0775809 -4.0972867 -4.129549 -4.1630836 -4.1980867 -4.2379088 -4.2644467 -4.2797203 -4.2946262 -4.3003178][-4.0600266 -4.0839286 -4.0967007 -4.0867686 -4.0697608 -4.0676818 -4.0930495 -4.1397891 -4.1811147 -4.2164135 -4.2548742 -4.2834487 -4.2997713 -4.3093209 -4.3102946][-4.0390778 -4.0671291 -4.081944 -4.0747681 -4.0633616 -4.0706053 -4.107008 -4.1673822 -4.2166166 -4.2524686 -4.2841086 -4.3052359 -4.3138547 -4.3123679 -4.3044291][-4.0571494 -4.0856733 -4.1003838 -4.0902624 -4.0775652 -4.0873947 -4.1281252 -4.1945648 -4.2517643 -4.2894039 -4.3163967 -4.3276753 -4.3248048 -4.31068 -4.289125][-4.1012878 -4.1278267 -4.1392531 -4.1241565 -4.1040707 -4.1018629 -4.1315947 -4.1947007 -4.259932 -4.3055182 -4.3310671 -4.337018 -4.3287554 -4.306622 -4.2767749][-4.1455112 -4.1667805 -4.1743927 -4.1570764 -4.132823 -4.1169753 -4.1320186 -4.1858382 -4.250381 -4.2985864 -4.3234286 -4.3284097 -4.3197346 -4.2931709 -4.2615075][-4.1740365 -4.1802707 -4.1772432 -4.1597176 -4.1348453 -4.1149645 -4.1258678 -4.1743116 -4.2302508 -4.2721186 -4.293458 -4.2996821 -4.2910552 -4.2636814 -4.2367425][-4.1770282 -4.16459 -4.1476779 -4.1252775 -4.1001472 -4.0887809 -4.1093988 -4.1547866 -4.1986542 -4.231133 -4.249198 -4.2562485 -4.247869 -4.2245893 -4.2081952][-4.1598749 -4.1313057 -4.1003585 -4.0764885 -4.0597148 -4.0658512 -4.1016493 -4.1464996 -4.180707 -4.2068186 -4.2211018 -4.2254229 -4.2170353 -4.2006221 -4.19506][-4.1436267 -4.1105356 -4.0757036 -4.05671 -4.0540333 -4.0748873 -4.1176982 -4.1601105 -4.1891689 -4.210269 -4.2215223 -4.2211738 -4.2122579 -4.2025738 -4.2018023][-4.1383562 -4.1188779 -4.0995469 -4.09264 -4.0997386 -4.1250825 -4.1644964 -4.1983 -4.2246017 -4.2438412 -4.2525139 -4.2489796 -4.2387686 -4.230473 -4.2268558][-4.12873 -4.1323438 -4.1397448 -4.1538768 -4.17117 -4.1952934 -4.2262836 -4.2500081 -4.2716522 -4.2883749 -4.2954769 -4.29242 -4.2807522 -4.2677622 -4.2553277][-4.1218071 -4.1404238 -4.1679974 -4.2004657 -4.2282953 -4.2534571 -4.2779641 -4.2933917 -4.3069348 -4.317193 -4.3220382 -4.3213372 -4.3112392 -4.2961006 -4.2784019][-4.1317515 -4.1561685 -4.1908879 -4.2307382 -4.2625 -4.2875118 -4.3054671 -4.3117304 -4.3157983 -4.320395 -4.3250475 -4.3256683 -4.3190517 -4.3072772 -4.2915211]]...]
INFO - root - 2017-12-05 16:17:12.284169: step 21810, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 78h:02m:33s remains)
INFO - root - 2017-12-05 16:17:21.376547: step 21820, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 77h:09m:17s remains)
INFO - root - 2017-12-05 16:17:30.394599: step 21830, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.810 sec/batch; 69h:53m:44s remains)
INFO - root - 2017-12-05 16:17:39.430975: step 21840, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 81h:49m:17s remains)
INFO - root - 2017-12-05 16:17:48.564100: step 21850, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 77h:10m:42s remains)
INFO - root - 2017-12-05 16:17:57.636878: step 21860, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 80h:35m:07s remains)
INFO - root - 2017-12-05 16:18:06.804247: step 21870, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 79h:12m:47s remains)
INFO - root - 2017-12-05 16:18:15.834326: step 21880, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 70h:54m:51s remains)
INFO - root - 2017-12-05 16:18:24.976574: step 21890, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 80h:50m:49s remains)
INFO - root - 2017-12-05 16:18:34.091517: step 21900, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 78h:12m:22s remains)
2017-12-05 16:18:34.895927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.261847 -4.2658787 -4.2814789 -4.2899795 -4.2844734 -4.2722425 -4.2631793 -4.2661738 -4.2713108 -4.2697926 -4.2713785 -4.2771831 -4.2866774 -4.3014421 -4.3201318][-4.1945934 -4.2053919 -4.2404041 -4.2642188 -4.2597976 -4.2418294 -4.2236075 -4.2216039 -4.2240877 -4.2207565 -4.224649 -4.2396417 -4.2583795 -4.2795324 -4.30635][-4.1208243 -4.1350918 -4.192616 -4.2356758 -4.2357082 -4.2158861 -4.1909933 -4.1824741 -4.181468 -4.1746893 -4.1797438 -4.2028384 -4.2299366 -4.2594953 -4.294271][-4.0895042 -4.1001945 -4.1655278 -4.2141972 -4.2141433 -4.1944728 -4.1682444 -4.1565671 -4.1583285 -4.1534524 -4.1560483 -4.179832 -4.2081227 -4.2428503 -4.2844348][-4.1006651 -4.1035457 -4.1551218 -4.1857448 -4.1741858 -4.1504393 -4.1213236 -4.1166968 -4.1350446 -4.1447043 -4.1538405 -4.1769185 -4.2018094 -4.2390876 -4.28333][-4.1042838 -4.0995493 -4.1243005 -4.1207423 -4.0877504 -4.0471964 -4.0024366 -4.0067253 -4.06386 -4.1137552 -4.1472969 -4.1811943 -4.2120261 -4.2510128 -4.2912378][-4.0917635 -4.0685816 -4.0613809 -4.0262942 -3.9706705 -3.8941576 -3.8031893 -3.8052287 -3.9164495 -4.0216565 -4.0907922 -4.1481056 -4.1982388 -4.2488294 -4.2922263][-4.089005 -4.0595331 -4.0407844 -3.9994946 -3.9378495 -3.8387513 -3.7162044 -3.6999509 -3.8182411 -3.9372694 -4.0210376 -4.0978074 -4.171175 -4.2369208 -4.2876353][-4.128819 -4.1110864 -4.10896 -4.0894341 -4.0447197 -3.9704309 -3.8875008 -3.8707304 -3.9305861 -3.9955187 -4.046401 -4.1072006 -4.1776013 -4.2407908 -4.2893043][-4.1787996 -4.1754732 -4.1884551 -4.1870356 -4.1605639 -4.1199794 -4.0799322 -4.0689435 -4.0875473 -4.109405 -4.1326485 -4.1701813 -4.2195005 -4.266923 -4.3018994][-4.2061963 -4.2095985 -4.2246885 -4.2290707 -4.2145905 -4.199966 -4.1840868 -4.1736517 -4.1734366 -4.1804991 -4.1951256 -4.2226753 -4.2584 -4.2931943 -4.3175797][-4.2196107 -4.2291884 -4.2431765 -4.2484221 -4.2400575 -4.236093 -4.2317653 -4.2235985 -4.2164226 -4.2183771 -4.2293396 -4.2531981 -4.2840858 -4.3117943 -4.3296247][-4.2412367 -4.2510629 -4.26481 -4.2702851 -4.2670407 -4.2672434 -4.2654147 -4.2588854 -4.2524738 -4.2523241 -4.26066 -4.2806306 -4.30599 -4.326633 -4.3404984][-4.28824 -4.2977285 -4.3088126 -4.3128362 -4.3107567 -4.3097906 -4.3082156 -4.3045311 -4.3004203 -4.2997794 -4.3056245 -4.3185697 -4.333992 -4.3454194 -4.3539696][-4.3291254 -4.3355813 -4.3422256 -4.3444991 -4.3437166 -4.3430743 -4.3417692 -4.34071 -4.3405709 -4.3417444 -4.3452344 -4.3506856 -4.3565555 -4.36021 -4.3639]]...]
INFO - root - 2017-12-05 16:18:43.957827: step 21910, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 78h:25m:30s remains)
INFO - root - 2017-12-05 16:18:53.112536: step 21920, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 79h:03m:18s remains)
INFO - root - 2017-12-05 16:19:02.053368: step 21930, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.904 sec/batch; 77h:57m:43s remains)
INFO - root - 2017-12-05 16:19:11.153023: step 21940, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.916 sec/batch; 79h:03m:31s remains)
INFO - root - 2017-12-05 16:19:20.311938: step 21950, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 79h:08m:40s remains)
INFO - root - 2017-12-05 16:19:29.328967: step 21960, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 78h:28m:33s remains)
INFO - root - 2017-12-05 16:19:38.329390: step 21970, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 79h:22m:53s remains)
INFO - root - 2017-12-05 16:19:47.501134: step 21980, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 78h:04m:52s remains)
INFO - root - 2017-12-05 16:19:56.519442: step 21990, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 79h:03m:18s remains)
INFO - root - 2017-12-05 16:20:05.581499: step 22000, loss = 2.02, batch loss = 1.96 (8.5 examples/sec; 0.938 sec/batch; 80h:53m:39s remains)
2017-12-05 16:20:06.381552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2515907 -4.2523556 -4.2520976 -4.251739 -4.2518816 -4.2525721 -4.2531977 -4.2538061 -4.2548151 -4.2556596 -4.2560968 -4.2556419 -4.2536573 -4.2509861 -4.2488484][-4.2523303 -4.2536178 -4.2549791 -4.2574782 -4.2607517 -4.2637486 -4.2655172 -4.2653236 -4.2641816 -4.2622533 -4.2601089 -4.2575426 -4.2541227 -4.2512264 -4.2492557][-4.2572007 -4.2590656 -4.2619195 -4.26693 -4.273397 -4.2793279 -4.2832065 -4.2830982 -4.2803082 -4.2758226 -4.2706342 -4.2652111 -4.2592 -4.2545753 -4.2514443][-4.263957 -4.2656546 -4.2687025 -4.2745433 -4.282938 -4.2916465 -4.298615 -4.3000789 -4.2971225 -4.2915096 -4.284133 -4.2760143 -4.2669158 -4.2600312 -4.2551823][-4.2714152 -4.2719231 -4.2735562 -4.2781177 -4.2863364 -4.2963586 -4.3064151 -4.3107181 -4.3097267 -4.3049936 -4.2971039 -4.287715 -4.2758427 -4.2664084 -4.2595897][-4.2770667 -4.2760348 -4.2750483 -4.2777052 -4.2856665 -4.2978415 -4.311336 -4.3188748 -4.32136 -4.3187666 -4.311172 -4.3004856 -4.2857156 -4.2733397 -4.2641249][-4.2793918 -4.2766395 -4.2730112 -4.2731662 -4.2799177 -4.2938671 -4.3105841 -4.321322 -4.3274088 -4.3275151 -4.3211761 -4.3099241 -4.2933626 -4.2787685 -4.267869][-4.2761884 -4.2726789 -4.2671862 -4.2646103 -4.2688742 -4.2824912 -4.3013229 -4.3150258 -4.3243537 -4.3275084 -4.3234329 -4.313313 -4.2964621 -4.28107 -4.2697873][-4.27181 -4.2673836 -4.2596593 -4.2531123 -4.25243 -4.2621045 -4.280407 -4.2959952 -4.308795 -4.3159604 -4.3151336 -4.3071737 -4.2924609 -4.27883 -4.2685833][-4.2681212 -4.2640738 -4.2559366 -4.2474575 -4.2429729 -4.2481084 -4.2639136 -4.2799573 -4.294909 -4.3049011 -4.3061433 -4.2994938 -4.2870345 -4.2751656 -4.2656875][-4.2630472 -4.2606273 -4.2548785 -4.2485781 -4.244112 -4.2465191 -4.2582884 -4.2716293 -4.2850351 -4.2948508 -4.2965713 -4.29065 -4.2806821 -4.2708411 -4.2624965][-4.2567124 -4.2559052 -4.2533517 -4.250742 -4.2488379 -4.2507277 -4.258779 -4.2680545 -4.2769442 -4.283236 -4.2833467 -4.2780752 -4.2705245 -4.2636776 -4.2581615][-4.2532287 -4.2538033 -4.2534494 -4.253221 -4.2531013 -4.2547216 -4.2597566 -4.2652206 -4.2691936 -4.2714992 -4.270164 -4.2658668 -4.26085 -4.2568903 -4.2544188][-4.2536907 -4.2550855 -4.2557492 -4.2564225 -4.25693 -4.258173 -4.2614169 -4.2645111 -4.2660208 -4.2668757 -4.2656846 -4.2622056 -4.2581816 -4.2553062 -4.2539673][-4.2551479 -4.2573256 -4.2585759 -4.2597222 -4.2610121 -4.2632494 -4.2667437 -4.2697291 -4.271297 -4.2726626 -4.2722435 -4.2684612 -4.2636261 -4.2596469 -4.2570291]]...]
INFO - root - 2017-12-05 16:20:15.437014: step 22010, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 76h:43m:39s remains)
INFO - root - 2017-12-05 16:20:24.605525: step 22020, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 78h:43m:33s remains)
INFO - root - 2017-12-05 16:20:33.588855: step 22030, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 78h:35m:39s remains)
INFO - root - 2017-12-05 16:20:42.613214: step 22040, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 74h:45m:29s remains)
INFO - root - 2017-12-05 16:20:51.658920: step 22050, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 80h:09m:54s remains)
INFO - root - 2017-12-05 16:21:00.776993: step 22060, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 75h:38m:24s remains)
INFO - root - 2017-12-05 16:21:09.863221: step 22070, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 79h:43m:56s remains)
INFO - root - 2017-12-05 16:21:19.194729: step 22080, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 78h:44m:38s remains)
INFO - root - 2017-12-05 16:21:28.383441: step 22090, loss = 2.04, batch loss = 1.98 (8.1 examples/sec; 0.982 sec/batch; 84h:38m:21s remains)
INFO - root - 2017-12-05 16:21:37.590746: step 22100, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 77h:42m:17s remains)
2017-12-05 16:21:38.396328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2619224 -4.2523079 -4.2479725 -4.2501116 -4.2501097 -4.2507982 -4.2522225 -4.2467222 -4.2400036 -4.2338839 -4.2308908 -4.2432351 -4.2623911 -4.2726674 -4.2789493][-4.2636223 -4.2513666 -4.2458043 -4.2500672 -4.251514 -4.2501 -4.2450094 -4.2334232 -4.222013 -4.2127347 -4.2067614 -4.2224493 -4.2457266 -4.2561321 -4.2626138][-4.2558603 -4.2410054 -4.2362051 -4.2411089 -4.240593 -4.2348642 -4.2232523 -4.2068253 -4.1916285 -4.1776319 -4.1741533 -4.2001638 -4.2314916 -4.24399 -4.2519226][-4.2402635 -4.2203636 -4.2103572 -4.2089944 -4.2013741 -4.1923041 -4.1804628 -4.1682444 -4.1550307 -4.1399021 -4.1432886 -4.1832204 -4.2267537 -4.2485719 -4.2613611][-4.2132311 -4.1892252 -4.1716604 -4.1637964 -4.15214 -4.144599 -4.1402216 -4.1377735 -4.126822 -4.1070175 -4.1112232 -4.15845 -4.2129092 -4.2468548 -4.2673521][-4.1855583 -4.1599607 -4.1333337 -4.119978 -4.1074271 -4.1024151 -4.1053777 -4.1099639 -4.0985403 -4.074369 -4.0781112 -4.1268835 -4.1875887 -4.2303033 -4.2584691][-4.1485338 -4.1176085 -4.0852475 -4.0704288 -4.0616508 -4.0621014 -4.0665359 -4.0694704 -4.0548396 -4.0285029 -4.0400224 -4.0925922 -4.1586843 -4.20937 -4.2399282][-4.087923 -4.0491214 -4.01427 -4.0034561 -4.0100737 -4.0236998 -4.0354805 -4.0369062 -4.0152044 -3.9853776 -4.0061073 -4.0642862 -4.1331353 -4.1912174 -4.2247758][-4.0333776 -3.9960775 -3.9709063 -3.973825 -3.9964685 -4.0233774 -4.0412083 -4.0424409 -4.0168037 -3.9859705 -4.0121932 -4.072053 -4.134779 -4.190815 -4.2239285][-4.0466766 -4.0248966 -4.0131049 -4.0235076 -4.0507812 -4.080287 -4.0968814 -4.0983224 -4.0744843 -4.0452547 -4.0643368 -4.1133 -4.1622467 -4.2069077 -4.2344394][-4.11855 -4.1075273 -4.1000624 -4.1076632 -4.1291533 -4.1520219 -4.1654344 -4.1687531 -4.1514306 -4.1254816 -4.1311908 -4.1648264 -4.2006683 -4.233654 -4.254878][-4.1961322 -4.1898017 -4.1835279 -4.187232 -4.1999812 -4.2120872 -4.2185531 -4.220242 -4.2095337 -4.189589 -4.1877918 -4.2075887 -4.231473 -4.2546654 -4.2735071][-4.2561073 -4.2524052 -4.24763 -4.2495065 -4.2566433 -4.26294 -4.2652931 -4.2650986 -4.2581172 -4.2467065 -4.2437263 -4.2528367 -4.26418 -4.2763076 -4.2912378][-4.3002143 -4.2968097 -4.2944322 -4.2963266 -4.3008366 -4.3043113 -4.3043017 -4.3023543 -4.2973967 -4.2910962 -4.2885823 -4.292213 -4.2955823 -4.3004441 -4.309267][-4.3149896 -4.311564 -4.3113847 -4.3134737 -4.3162751 -4.3173866 -4.3162985 -4.3141313 -4.3113613 -4.3092036 -4.3089447 -4.3113093 -4.3121624 -4.3137674 -4.3169246]]...]
INFO - root - 2017-12-05 16:21:47.357451: step 22110, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.798 sec/batch; 68h:47m:14s remains)
INFO - root - 2017-12-05 16:21:56.475238: step 22120, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 80h:10m:44s remains)
INFO - root - 2017-12-05 16:22:05.548782: step 22130, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 76h:57m:33s remains)
INFO - root - 2017-12-05 16:22:14.638045: step 22140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 75h:32m:18s remains)
INFO - root - 2017-12-05 16:22:23.775558: step 22150, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 79h:50m:42s remains)
INFO - root - 2017-12-05 16:22:33.064839: step 22160, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 81h:08m:36s remains)
INFO - root - 2017-12-05 16:22:42.076726: step 22170, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 78h:55m:55s remains)
INFO - root - 2017-12-05 16:22:51.212515: step 22180, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 77h:38m:41s remains)
INFO - root - 2017-12-05 16:23:00.296993: step 22190, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 79h:50m:47s remains)
INFO - root - 2017-12-05 16:23:09.464457: step 22200, loss = 2.01, batch loss = 1.95 (8.5 examples/sec; 0.942 sec/batch; 81h:14m:02s remains)
2017-12-05 16:23:10.261934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1579437 -4.1417241 -4.12418 -4.1233215 -4.1529546 -4.2062645 -4.2538004 -4.286305 -4.3082838 -4.3178968 -4.3112283 -4.2938595 -4.2816873 -4.2773414 -4.279767][-4.1302705 -4.1224613 -4.1082306 -4.1111889 -4.1450205 -4.2016587 -4.2501378 -4.2805958 -4.3072677 -4.3245325 -4.3206425 -4.3022432 -4.289813 -4.287837 -4.2921038][-4.1008992 -4.1060181 -4.0977774 -4.1055632 -4.1444907 -4.2031889 -4.2479305 -4.27321 -4.3023195 -4.3271775 -4.3292146 -4.3136454 -4.3014684 -4.2976427 -4.2988219][-4.0882497 -4.1048737 -4.1032591 -4.1131263 -4.1515636 -4.205605 -4.2414794 -4.2584715 -4.2881989 -4.3176589 -4.3247461 -4.3144073 -4.3042321 -4.296205 -4.291585][-4.1019268 -4.1208544 -4.1226339 -4.1337576 -4.165298 -4.2059846 -4.2264013 -4.2313924 -4.2589197 -4.2904711 -4.3028836 -4.2985325 -4.2892036 -4.2765703 -4.2668562][-4.1376486 -4.1520829 -4.1523137 -4.1600375 -4.1805692 -4.2011013 -4.2010722 -4.1928277 -4.2172561 -4.2548261 -4.2755532 -4.2795486 -4.2719994 -4.255506 -4.2423592][-4.1839747 -4.1892419 -4.1831083 -4.1803737 -4.1836662 -4.1813664 -4.1606092 -4.139143 -4.1619563 -4.2098093 -4.2416081 -4.2554851 -4.2523332 -4.2354245 -4.2208328][-4.2261834 -4.2221971 -4.2088313 -4.1955166 -4.18173 -4.1565013 -4.1128287 -4.0730605 -4.0909872 -4.1506939 -4.1956987 -4.2210355 -4.225668 -4.2116332 -4.1982832][-4.2542696 -4.249392 -4.2333422 -4.2125611 -4.1850004 -4.1383982 -4.0724812 -4.0096741 -4.0176921 -4.0864468 -4.1447258 -4.1820617 -4.1980491 -4.1919088 -4.1829457][-4.258009 -4.2605038 -4.2499847 -4.2321544 -4.2013969 -4.1451283 -4.06855 -3.9929152 -3.9865777 -4.0509338 -4.115057 -4.1614876 -4.1892462 -4.1920838 -4.1864495][-4.2444425 -4.2576008 -4.2570405 -4.248981 -4.2276592 -4.1792169 -4.1095867 -4.0402122 -4.0233426 -4.0674462 -4.1209416 -4.168283 -4.2041073 -4.2133131 -4.2056675][-4.23314 -4.2532454 -4.2619677 -4.2672658 -4.2595916 -4.2278171 -4.1720762 -4.1148062 -4.0948381 -4.1204939 -4.1581774 -4.1982031 -4.2327566 -4.2410703 -4.2261591][-4.2480741 -4.2672205 -4.2810535 -4.2939658 -4.294971 -4.2769251 -4.2366433 -4.1918769 -4.1720638 -4.1844416 -4.2084312 -4.2395225 -4.2672324 -4.2708716 -4.2488737][-4.2816572 -4.2967682 -4.3084621 -4.3196049 -4.3231368 -4.3150029 -4.2888622 -4.2573643 -4.2411108 -4.2441397 -4.2575259 -4.2800975 -4.3022079 -4.30234 -4.2760973][-4.3119082 -4.3222136 -4.3299928 -4.3370295 -4.34155 -4.3412204 -4.3271837 -4.3078008 -4.2956924 -4.2935233 -4.2993546 -4.3143554 -4.3308897 -4.327816 -4.3023067]]...]
INFO - root - 2017-12-05 16:23:19.202697: step 22210, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 79h:15m:18s remains)
INFO - root - 2017-12-05 16:23:28.492600: step 22220, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 79h:52m:02s remains)
INFO - root - 2017-12-05 16:23:37.602837: step 22230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 79h:15m:47s remains)
INFO - root - 2017-12-05 16:23:46.644682: step 22240, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 76h:52m:53s remains)
INFO - root - 2017-12-05 16:23:55.817100: step 22250, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 78h:14m:05s remains)
INFO - root - 2017-12-05 16:24:04.940305: step 22260, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 79h:28m:39s remains)
INFO - root - 2017-12-05 16:24:14.113879: step 22270, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 79h:25m:09s remains)
INFO - root - 2017-12-05 16:24:23.291046: step 22280, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 78h:41m:41s remains)
INFO - root - 2017-12-05 16:24:32.329012: step 22290, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 78h:35m:58s remains)
INFO - root - 2017-12-05 16:24:41.252521: step 22300, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 76h:27m:24s remains)
2017-12-05 16:24:42.043542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2881389 -4.2739897 -4.2646165 -4.2604618 -4.2600274 -4.262639 -4.2668681 -4.2693653 -4.2735033 -4.2797666 -4.2825246 -4.2817893 -4.2761321 -4.2692208 -4.2660251][-4.2620997 -4.2426724 -4.2263751 -4.2141509 -4.2074142 -4.2050943 -4.2048397 -4.2073011 -4.2152972 -4.2299442 -4.2420106 -4.2460594 -4.2395463 -4.2296243 -4.2231007][-4.2486167 -4.224268 -4.1995187 -4.1767788 -4.1622677 -4.1535597 -4.1472912 -4.1474934 -4.155128 -4.1752453 -4.1974144 -4.2081995 -4.2017989 -4.1889243 -4.1799603][-4.2483444 -4.2236423 -4.1964531 -4.1666346 -4.146265 -4.1323957 -4.1206117 -4.1158657 -4.120657 -4.1436782 -4.1707764 -4.1819963 -4.1751137 -4.1615915 -4.1525702][-4.2505121 -4.2259264 -4.1961389 -4.158041 -4.1252341 -4.0966582 -4.0705404 -4.0612469 -4.074801 -4.1089044 -4.1404305 -4.1523571 -4.146472 -4.1346297 -4.128818][-4.2412353 -4.2120094 -4.1745987 -4.1225705 -4.068891 -4.0142961 -3.9636264 -3.9473951 -3.9846132 -4.0460911 -4.0934644 -4.11419 -4.1137943 -4.1071019 -4.1053739][-4.2210231 -4.1885486 -4.1467853 -4.0857511 -4.0169325 -3.9371328 -3.8580916 -3.8339009 -3.8973398 -3.9848886 -4.0495219 -4.083703 -4.0965295 -4.0984778 -4.0986762][-4.2130246 -4.18512 -4.1516557 -4.1023293 -4.0454397 -3.9748855 -3.9108582 -3.8945837 -3.9389434 -4.00438 -4.0545125 -4.0820184 -4.0964756 -4.104589 -4.1047931][-4.2228217 -4.2011776 -4.176167 -4.1378918 -4.0951324 -4.0469165 -4.0105958 -4.0054789 -4.0288954 -4.0674443 -4.0969157 -4.1103415 -4.1137648 -4.1199279 -4.1161952][-4.2399716 -4.2167516 -4.1899676 -4.1571026 -4.12319 -4.0903254 -4.069674 -4.0720596 -4.0876679 -4.1168461 -4.1403632 -4.1501951 -4.1486592 -4.1497874 -4.1382637][-4.2574835 -4.2335973 -4.2047029 -4.1720939 -4.1399803 -4.1108031 -4.0932279 -4.0941253 -4.1075268 -4.1362514 -4.160603 -4.1750154 -4.180974 -4.1827135 -4.1639462][-4.2697277 -4.2486329 -4.2233753 -4.1972308 -4.1738338 -4.1503372 -4.1356506 -4.1334963 -4.141911 -4.1627307 -4.179884 -4.1949258 -4.2060595 -4.209702 -4.1897774][-4.2753282 -4.2553372 -4.2360525 -4.2195888 -4.2074895 -4.1944103 -4.1882625 -4.1892414 -4.1945868 -4.203218 -4.2095237 -4.2215929 -4.2316475 -4.2336617 -4.2145314][-4.2838664 -4.2660642 -4.2525611 -4.2443209 -4.2395883 -4.2371812 -4.2384472 -4.241333 -4.2445393 -4.2454576 -4.2452388 -4.251492 -4.2550287 -4.2527981 -4.2367887][-4.3015223 -4.2880931 -4.2786016 -4.27436 -4.272706 -4.2749329 -4.2779446 -4.2806082 -4.2814684 -4.2791433 -4.2766805 -4.2793703 -4.2788405 -4.2755337 -4.2643666]]...]
INFO - root - 2017-12-05 16:24:51.054385: step 22310, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 76h:24m:04s remains)
INFO - root - 2017-12-05 16:25:00.203457: step 22320, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 77h:01m:12s remains)
INFO - root - 2017-12-05 16:25:09.229767: step 22330, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 79h:12m:24s remains)
INFO - root - 2017-12-05 16:25:18.463581: step 22340, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 78h:59m:36s remains)
INFO - root - 2017-12-05 16:25:27.505820: step 22350, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 78h:08m:57s remains)
INFO - root - 2017-12-05 16:25:36.483735: step 22360, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 77h:08m:49s remains)
INFO - root - 2017-12-05 16:25:45.571508: step 22370, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 77h:35m:41s remains)
INFO - root - 2017-12-05 16:25:54.738782: step 22380, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 78h:28m:35s remains)
INFO - root - 2017-12-05 16:26:03.785248: step 22390, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 80h:47m:09s remains)
INFO - root - 2017-12-05 16:26:12.771152: step 22400, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 78h:01m:38s remains)
2017-12-05 16:26:13.612821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3179359 -4.3177323 -4.3068914 -4.2937093 -4.2815485 -4.2714877 -4.26509 -4.2709007 -4.2820368 -4.2857771 -4.2828803 -4.2816367 -4.2852745 -4.2939229 -4.300703][-4.3120623 -4.30981 -4.2970548 -4.2844748 -4.2764316 -4.2696147 -4.2654204 -4.2717352 -4.2814746 -4.281765 -4.2754765 -4.273849 -4.279664 -4.2913523 -4.2992449][-4.30692 -4.3024144 -4.2876148 -4.2760034 -4.2710934 -4.2668433 -4.2631264 -4.2682862 -4.2768717 -4.2731137 -4.2650785 -4.2647963 -4.271699 -4.2826838 -4.2900739][-4.2983108 -4.2896442 -4.2698379 -4.2560015 -4.2493615 -4.245141 -4.244184 -4.2532396 -4.2649493 -4.2595706 -4.2501693 -4.2520518 -4.2556047 -4.2600489 -4.2652407][-4.2919407 -4.2777739 -4.2510018 -4.2341318 -4.2234159 -4.2171307 -4.2191854 -4.2307296 -4.2394004 -4.2282033 -4.2162638 -4.2219243 -4.2219143 -4.2183819 -4.2217455][-4.2887917 -4.2706447 -4.2407279 -4.2199 -4.2019596 -4.1825819 -4.175477 -4.1828527 -4.1896448 -4.1737633 -4.1591711 -4.1689591 -4.1726246 -4.1726155 -4.1807275][-4.2864356 -4.26656 -4.2330222 -4.2035241 -4.171123 -4.1318665 -4.1065478 -4.1106696 -4.1247129 -4.1142921 -4.1030807 -4.115891 -4.1256905 -4.1393642 -4.1541615][-4.2799764 -4.2555518 -4.2120638 -4.1656189 -4.1157141 -4.0611882 -4.0281482 -4.042594 -4.0763607 -4.0822568 -4.0757661 -4.0847783 -4.09595 -4.1174088 -4.1367264][-4.2713037 -4.241909 -4.1912413 -4.1341896 -4.071701 -4.0104952 -3.9759102 -4.001225 -4.0512605 -4.0701261 -4.064537 -4.0655408 -4.0747681 -4.0977678 -4.121788][-4.2609534 -4.2322154 -4.1862526 -4.1352386 -4.074892 -4.01175 -3.9694917 -3.9836125 -4.0297904 -4.0546908 -4.0518556 -4.0526204 -4.0656595 -4.0931511 -4.1184993][-4.247014 -4.2245727 -4.1906757 -4.1534476 -4.1092682 -4.0549326 -4.0071511 -4.005384 -4.038281 -4.0594683 -4.057086 -4.0597897 -4.072753 -4.1003962 -4.1234765][-4.2337632 -4.2164693 -4.1914568 -4.1655211 -4.1378565 -4.1007619 -4.0629268 -4.0539684 -4.0720968 -4.0842352 -4.0801477 -4.0822015 -4.0907931 -4.1134219 -4.1271982][-4.229054 -4.2117696 -4.1912766 -4.1732483 -4.1575003 -4.1363726 -4.1138053 -4.1087694 -4.1195245 -4.1224146 -4.1146994 -4.1133027 -4.1159725 -4.1289554 -4.1312432][-4.2358885 -4.2182846 -4.2007751 -4.1869435 -4.1787591 -4.167522 -4.1569762 -4.1592903 -4.16919 -4.1673746 -4.1579504 -4.155179 -4.1549582 -4.1607146 -4.1567922][-4.2508869 -4.236937 -4.22602 -4.2170806 -4.2129045 -4.2066412 -4.201951 -4.2054319 -4.2120123 -4.2075148 -4.1981564 -4.1974058 -4.2009215 -4.2079391 -4.2064886]]...]
INFO - root - 2017-12-05 16:26:22.747533: step 22410, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 77h:48m:04s remains)
INFO - root - 2017-12-05 16:26:31.544796: step 22420, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 77h:24m:04s remains)
INFO - root - 2017-12-05 16:26:40.648787: step 22430, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.930 sec/batch; 80h:05m:03s remains)
INFO - root - 2017-12-05 16:26:49.743904: step 22440, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 78h:01m:51s remains)
INFO - root - 2017-12-05 16:26:58.754162: step 22450, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.881 sec/batch; 75h:51m:33s remains)
INFO - root - 2017-12-05 16:27:07.998543: step 22460, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 79h:24m:17s remains)
INFO - root - 2017-12-05 16:27:17.328600: step 22470, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 80h:29m:28s remains)
INFO - root - 2017-12-05 16:27:26.296779: step 22480, loss = 2.11, batch loss = 2.06 (9.1 examples/sec; 0.881 sec/batch; 75h:52m:36s remains)
INFO - root - 2017-12-05 16:27:35.313123: step 22490, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 78h:24m:31s remains)
INFO - root - 2017-12-05 16:27:44.357211: step 22500, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 75h:22m:18s remains)
2017-12-05 16:27:45.129474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2995811 -4.2979369 -4.2947311 -4.2917261 -4.2901988 -4.2901034 -4.2900434 -4.2890253 -4.2868638 -4.2842951 -4.2786613 -4.2691278 -4.2596536 -4.2569957 -4.2583265][-4.3063669 -4.3037891 -4.2983913 -4.2919703 -4.2866464 -4.2836785 -4.2837143 -4.2845554 -4.2832284 -4.2785444 -4.2704906 -4.26124 -4.2544427 -4.2558537 -4.26267][-4.3089209 -4.3050537 -4.29732 -4.2872109 -4.276845 -4.2689576 -4.2663074 -4.2660031 -4.2635446 -4.2568712 -4.251049 -4.2492847 -4.2513528 -4.2573819 -4.2656708][-4.3121009 -4.3073964 -4.2978716 -4.2842288 -4.2682862 -4.2543817 -4.2463989 -4.2415619 -4.2350497 -4.2264638 -4.224194 -4.2321715 -4.2432752 -4.2530494 -4.2613583][-4.3151932 -4.3100562 -4.2992997 -4.2828469 -4.2619205 -4.2417569 -4.2275882 -4.2176561 -4.2067571 -4.1963153 -4.1957464 -4.2091503 -4.2253952 -4.2384415 -4.2472095][-4.31561 -4.3102064 -4.2997766 -4.2836089 -4.2610884 -4.2373314 -4.21832 -4.2041807 -4.1909966 -4.1775556 -4.1743016 -4.1864018 -4.2032881 -4.2197247 -4.2300668][-4.3146572 -4.3092103 -4.2999067 -4.286355 -4.2657981 -4.2427888 -4.2232814 -4.2081313 -4.1947803 -4.1760564 -4.1646342 -4.1697173 -4.1827226 -4.2013421 -4.2151513][-4.3120127 -4.3065658 -4.2983088 -4.2867661 -4.2674794 -4.2461424 -4.2276406 -4.2132192 -4.1990418 -4.1750894 -4.1554084 -4.1532159 -4.1654572 -4.1869383 -4.2042408][-4.3079448 -4.3030572 -4.2949376 -4.2829971 -4.2614193 -4.239356 -4.2208734 -4.2053928 -4.1897483 -4.1633019 -4.1381221 -4.1330805 -4.1508331 -4.1764712 -4.1944056][-4.3012576 -4.2973604 -4.2883534 -4.2728019 -4.247221 -4.2236481 -4.2048507 -4.1852994 -4.1675897 -4.1419592 -4.1152792 -4.1119618 -4.137866 -4.1683769 -4.186954][-4.2945042 -4.29195 -4.2809138 -4.2604814 -4.2313895 -4.2109923 -4.1972828 -4.1782527 -4.1588511 -4.1336279 -4.1081047 -4.1048703 -4.1342883 -4.1675491 -4.1854362][-4.2861295 -4.2864552 -4.2727385 -4.2472672 -4.2173042 -4.202559 -4.1966023 -4.1833463 -4.162735 -4.1347337 -4.1085057 -4.1035485 -4.1319 -4.164176 -4.1811819][-4.2779841 -4.2813454 -4.2669048 -4.2393374 -4.2123976 -4.2003679 -4.1995049 -4.1942773 -4.1774263 -4.1513548 -4.1265516 -4.1187625 -4.1379943 -4.1621351 -4.1757383][-4.2674041 -4.2706947 -4.257854 -4.2329092 -4.2110429 -4.20154 -4.2064285 -4.2119017 -4.2054591 -4.1871262 -4.1654477 -4.1508923 -4.155777 -4.1683383 -4.1732168][-4.2481813 -4.2509861 -4.2431784 -4.2266788 -4.2128229 -4.2102623 -4.2221045 -4.2364297 -4.2367382 -4.2221017 -4.1995039 -4.1740265 -4.1637907 -4.164032 -4.1634011]]...]
INFO - root - 2017-12-05 16:27:54.142553: step 22510, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 77h:09m:21s remains)
INFO - root - 2017-12-05 16:28:03.255681: step 22520, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 77h:53m:28s remains)
INFO - root - 2017-12-05 16:28:12.560397: step 22530, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 79h:17m:32s remains)
INFO - root - 2017-12-05 16:28:21.632428: step 22540, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 79h:39m:40s remains)
INFO - root - 2017-12-05 16:28:30.696556: step 22550, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 76h:37m:31s remains)
INFO - root - 2017-12-05 16:28:39.722062: step 22560, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 78h:30m:16s remains)
INFO - root - 2017-12-05 16:28:48.871684: step 22570, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 77h:00m:54s remains)
INFO - root - 2017-12-05 16:28:57.736809: step 22580, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 78h:54m:00s remains)
INFO - root - 2017-12-05 16:29:06.948223: step 22590, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 78h:02m:27s remains)
INFO - root - 2017-12-05 16:29:15.971802: step 22600, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 76h:04m:48s remains)
2017-12-05 16:29:16.743345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2704453 -4.2646246 -4.2641907 -4.2683082 -4.2769003 -4.2902966 -4.3008285 -4.3059053 -4.3107715 -4.314178 -4.3167424 -4.3197784 -4.3212085 -4.3180189 -4.3133349][-4.2505593 -4.235981 -4.2314968 -4.2311239 -4.2360368 -4.2486367 -4.2616062 -4.2688646 -4.277595 -4.281754 -4.2840323 -4.2889509 -4.2937603 -4.291122 -4.2861242][-4.2277775 -4.2052784 -4.19381 -4.1866832 -4.1868515 -4.1990027 -4.212203 -4.2212234 -4.234962 -4.2413421 -4.2391849 -4.244873 -4.2550726 -4.2579274 -4.253799][-4.2032485 -4.175631 -4.1548543 -4.140101 -4.1380777 -4.1483912 -4.1556411 -4.1567564 -4.1783719 -4.1951213 -4.1924925 -4.20043 -4.2154479 -4.2303004 -4.2282577][-4.1850481 -4.1456976 -4.11537 -4.0925231 -4.0873275 -4.0897045 -4.0820956 -4.0625539 -4.0939054 -4.1329536 -4.1421647 -4.154007 -4.1731138 -4.1989508 -4.2049971][-4.1706872 -4.1121154 -4.0639505 -4.0241208 -4.00528 -3.9955039 -3.9583485 -3.9006341 -3.9448643 -4.0222411 -4.049078 -4.0707226 -4.1033425 -4.1449924 -4.1660924][-4.1500111 -4.06463 -3.9843953 -3.9192648 -3.8844557 -3.8524506 -3.7663603 -3.6344285 -3.7073412 -3.8588946 -3.9258163 -3.963712 -4.0230484 -4.0865145 -4.1235552][-4.1396637 -4.0440693 -3.9434812 -3.8624811 -3.8257067 -3.7859297 -3.6730103 -3.4890182 -3.5986733 -3.8032973 -3.8936517 -3.9435256 -4.0186377 -4.0809813 -4.1171808][-4.1541653 -4.0701003 -3.9851129 -3.929563 -3.9131331 -3.8938475 -3.8209317 -3.6994495 -3.7824306 -3.9259953 -3.9916956 -4.0307446 -4.0872931 -4.1247811 -4.1484661][-4.1760721 -4.1102724 -4.0515108 -4.0210629 -4.0121131 -4.0002408 -3.962323 -3.90204 -3.94531 -4.0208268 -4.0599985 -4.0850048 -4.1189542 -4.1428981 -4.1672635][-4.2058744 -4.1566095 -4.1129985 -4.08816 -4.0692773 -4.0480256 -4.0261164 -4.0037265 -4.0275822 -4.0677519 -4.0962753 -4.1173587 -4.1381574 -4.1578851 -4.1847692][-4.2344856 -4.195703 -4.1624184 -4.1450987 -4.1273236 -4.10604 -4.0918808 -4.0924845 -4.1117973 -4.135179 -4.1561303 -4.173389 -4.1875024 -4.2037106 -4.2274861][-4.2703891 -4.2452946 -4.2270288 -4.2206 -4.2139692 -4.2007113 -4.1914835 -4.1973767 -4.2105875 -4.2229171 -4.2332997 -4.2459531 -4.2560935 -4.2672081 -4.2790346][-4.2978849 -4.2861762 -4.2789583 -4.2783079 -4.2765541 -4.2688513 -4.2639632 -4.2684355 -4.2756195 -4.2811661 -4.2831049 -4.2899895 -4.2955341 -4.3012466 -4.3054223][-4.3090458 -4.3034749 -4.3010764 -4.3003449 -4.2989273 -4.2955012 -4.2949171 -4.2985539 -4.3027182 -4.3048668 -4.304966 -4.3067274 -4.3094759 -4.3120365 -4.3136506]]...]
INFO - root - 2017-12-05 16:29:25.626791: step 22610, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 79h:21m:03s remains)
INFO - root - 2017-12-05 16:29:34.744400: step 22620, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 79h:52m:32s remains)
INFO - root - 2017-12-05 16:29:43.839193: step 22630, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 76h:37m:51s remains)
INFO - root - 2017-12-05 16:29:52.962844: step 22640, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 81h:17m:48s remains)
INFO - root - 2017-12-05 16:30:02.107891: step 22650, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 78h:51m:41s remains)
INFO - root - 2017-12-05 16:30:11.292885: step 22660, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 76h:23m:57s remains)
INFO - root - 2017-12-05 16:30:20.361590: step 22670, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 76h:57m:49s remains)
INFO - root - 2017-12-05 16:30:29.258630: step 22680, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 75h:36m:42s remains)
INFO - root - 2017-12-05 16:30:38.360485: step 22690, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 77h:40m:11s remains)
INFO - root - 2017-12-05 16:30:47.234164: step 22700, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 77h:29m:13s remains)
2017-12-05 16:30:47.997013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2823992 -4.2612944 -4.24135 -4.2254128 -4.2188845 -4.218224 -4.2159429 -4.2132983 -4.215013 -4.2228632 -4.23036 -4.2357693 -4.2395678 -4.2495794 -4.2616534][-4.2604852 -4.230895 -4.20231 -4.1791515 -4.1684022 -4.1657348 -4.1610756 -4.1533422 -4.1556454 -4.1654754 -4.1742 -4.1831632 -4.1948466 -4.2071934 -4.216105][-4.24492 -4.2104921 -4.1750312 -4.1449957 -4.1316342 -4.1285863 -4.12215 -4.1116409 -4.1179314 -4.1330595 -4.144578 -4.1554585 -4.1683764 -4.1728988 -4.1685886][-4.2382846 -4.2009087 -4.1588283 -4.120398 -4.1014986 -4.090878 -4.0777483 -4.0698404 -4.087585 -4.1134319 -4.1281285 -4.1395493 -4.1502295 -4.1453061 -4.126255][-4.2382712 -4.1969738 -4.1453161 -4.0941377 -4.058774 -4.0299506 -4.0066576 -4.0077648 -4.0469942 -4.0953808 -4.1179461 -4.131731 -4.1420979 -4.131556 -4.1032314][-4.2371674 -4.190064 -4.1287928 -4.0614381 -4.002739 -3.946636 -3.9042137 -3.9100296 -3.9784245 -4.0578127 -4.0954113 -4.116313 -4.1263976 -4.1134329 -4.0813255][-4.2356257 -4.1822519 -4.1103864 -4.0298214 -3.9507947 -3.8675268 -3.7882552 -3.7658834 -3.8540742 -3.9717274 -4.0330029 -4.064806 -4.0784926 -4.0686989 -4.0481644][-4.2313085 -4.1720877 -4.0904279 -3.9986796 -3.9092298 -3.8103006 -3.6910245 -3.6128964 -3.7050061 -3.8585279 -3.9436705 -3.9829996 -4.0011473 -4.0010352 -3.9960938][-4.2262869 -4.1664724 -4.0829725 -3.98888 -3.9064553 -3.8201108 -3.7077584 -3.6196389 -3.6864214 -3.8244112 -3.903836 -3.9377713 -3.9523129 -3.9554565 -3.9627116][-4.2303247 -4.17758 -4.102139 -4.0143557 -3.9457026 -3.8840268 -3.8142126 -3.7662404 -3.7989032 -3.876277 -3.9303813 -3.9523447 -3.9579711 -3.9555006 -3.9668944][-4.2433176 -4.1992431 -4.136919 -4.0635285 -4.0123816 -3.9752085 -3.9426968 -3.922936 -3.9327159 -3.9570379 -3.9886587 -4.0059505 -4.005722 -3.9994388 -4.0049486][-4.2690182 -4.2372236 -4.1900558 -4.134017 -4.0970812 -4.0785341 -4.0664959 -4.0570617 -4.0554671 -4.0570507 -4.0730639 -4.0852113 -4.0857472 -4.0790544 -4.0798349][-4.2969642 -4.2762113 -4.2438164 -4.206111 -4.1826706 -4.1763682 -4.1769204 -4.1769042 -4.1779003 -4.1750488 -4.1772127 -4.1796074 -4.1778889 -4.1715794 -4.1704144][-4.3159966 -4.3039536 -4.2849307 -4.264894 -4.2543845 -4.2538342 -4.2583003 -4.2623816 -4.2650585 -4.2612133 -4.2568512 -4.253756 -4.2551074 -4.2563 -4.2574677][-4.3272176 -4.3211708 -4.3109612 -4.3015003 -4.296957 -4.2960625 -4.2983541 -4.3018093 -4.302969 -4.3005924 -4.2971931 -4.2949262 -4.2975192 -4.3014703 -4.3051658]]...]
INFO - root - 2017-12-05 16:30:56.913488: step 22710, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 77h:04m:42s remains)
INFO - root - 2017-12-05 16:31:06.019710: step 22720, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 81h:12m:44s remains)
INFO - root - 2017-12-05 16:31:15.147874: step 22730, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 79h:34m:26s remains)
INFO - root - 2017-12-05 16:31:24.068329: step 22740, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.901 sec/batch; 77h:33m:04s remains)
INFO - root - 2017-12-05 16:31:33.201767: step 22750, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 80h:46m:05s remains)
INFO - root - 2017-12-05 16:31:42.365063: step 22760, loss = 2.01, batch loss = 1.95 (8.8 examples/sec; 0.905 sec/batch; 77h:50m:22s remains)
INFO - root - 2017-12-05 16:31:51.304642: step 22770, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 75h:34m:38s remains)
INFO - root - 2017-12-05 16:32:00.429896: step 22780, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 77h:29m:44s remains)
INFO - root - 2017-12-05 16:32:09.420742: step 22790, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 75h:39m:53s remains)
INFO - root - 2017-12-05 16:32:18.240808: step 22800, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.933 sec/batch; 80h:17m:04s remains)
2017-12-05 16:32:19.026978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2831197 -4.3017836 -4.3065577 -4.3027506 -4.2895184 -4.2732577 -4.265172 -4.2640762 -4.2676635 -4.2731175 -4.2814012 -4.2900872 -4.2973161 -4.3016114 -4.3003263][-4.2713871 -4.28613 -4.2900891 -4.2859683 -4.2727251 -4.2578044 -4.2558689 -4.2602434 -4.2676768 -4.2785587 -4.2892179 -4.2958336 -4.2984352 -4.2978125 -4.2913637][-4.2699151 -4.2743087 -4.2718081 -4.2606068 -4.243504 -4.2339554 -4.2395964 -4.2466531 -4.2552152 -4.2743049 -4.2880177 -4.2923474 -4.2918525 -4.2897797 -4.2799225][-4.272316 -4.2646513 -4.2518196 -4.2272587 -4.2014742 -4.1945925 -4.2028313 -4.2097292 -4.2224922 -4.2517624 -4.2728844 -4.2799668 -4.2845154 -4.2850943 -4.2757287][-4.2748542 -4.2578864 -4.2298031 -4.1857247 -4.1483674 -4.1366162 -4.1411967 -4.1442366 -4.165102 -4.2080216 -4.24073 -4.2554317 -4.2699976 -4.2766595 -4.2700152][-4.27046 -4.2437654 -4.1979675 -4.1346469 -4.08411 -4.0593486 -4.0560932 -4.0557489 -4.0870452 -4.146512 -4.1929259 -4.2175918 -4.2430573 -4.2598248 -4.2612758][-4.2522054 -4.2183881 -4.1607618 -4.087194 -4.0270844 -3.9877639 -3.97122 -3.9679785 -4.0104613 -4.0819788 -4.1390262 -4.1742043 -4.2090893 -4.233623 -4.2464442][-4.2279258 -4.1892638 -4.1273818 -4.0575414 -4.0042534 -3.9564724 -3.9231696 -3.9154189 -3.9576182 -4.0290446 -4.0926814 -4.1415181 -4.1859636 -4.2155089 -4.2362914][-4.2102256 -4.174552 -4.1198659 -4.0637388 -4.0231671 -3.9746263 -3.9366477 -3.9267111 -3.9553719 -4.013166 -4.0741191 -4.1293116 -4.1767473 -4.2119422 -4.2387781][-4.2066855 -4.1778851 -4.1399851 -4.101965 -4.075047 -4.0410957 -4.0158415 -4.0071549 -4.0192409 -4.0556946 -4.1020231 -4.1479573 -4.189198 -4.2234435 -4.2474151][-4.2160373 -4.1933818 -4.1707106 -4.1490145 -4.1370158 -4.123816 -4.1124249 -4.1044321 -4.1065922 -4.1296415 -4.15893 -4.1869607 -4.2151251 -4.2386613 -4.2493691][-4.2241211 -4.2086759 -4.1968441 -4.1868315 -4.1892591 -4.1929822 -4.1909676 -4.1830559 -4.1822739 -4.1968832 -4.2095475 -4.2211266 -4.2372952 -4.2465849 -4.2467871][-4.2290297 -4.2223244 -4.2190371 -4.218791 -4.2279582 -4.2387877 -4.2434006 -4.2385874 -4.2370687 -4.241869 -4.2406635 -4.2384272 -4.2421122 -4.2411122 -4.2368312][-4.2306781 -4.2288976 -4.2295694 -4.2345996 -4.2454891 -4.2616563 -4.2732186 -4.2724066 -4.2713118 -4.2705021 -4.2604237 -4.2462535 -4.2376308 -4.2312241 -4.2262225][-4.2419243 -4.2366648 -4.2328072 -4.2346344 -4.2426047 -4.2607589 -4.2762189 -4.2822332 -4.2871237 -4.2845058 -4.2692909 -4.2493792 -4.23689 -4.22702 -4.2206807]]...]
INFO - root - 2017-12-05 16:32:28.069419: step 22810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 79h:39m:11s remains)
INFO - root - 2017-12-05 16:32:37.164005: step 22820, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 78h:42m:39s remains)
INFO - root - 2017-12-05 16:32:46.152821: step 22830, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.916 sec/batch; 78h:45m:35s remains)
INFO - root - 2017-12-05 16:32:55.256674: step 22840, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 79h:25m:08s remains)
INFO - root - 2017-12-05 16:33:04.415538: step 22850, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 79h:04m:19s remains)
INFO - root - 2017-12-05 16:33:13.389014: step 22860, loss = 2.11, batch loss = 2.05 (9.8 examples/sec; 0.819 sec/batch; 70h:28m:43s remains)
INFO - root - 2017-12-05 16:33:22.430694: step 22870, loss = 2.02, batch loss = 1.96 (8.7 examples/sec; 0.922 sec/batch; 79h:16m:19s remains)
INFO - root - 2017-12-05 16:33:31.572321: step 22880, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 80h:57m:26s remains)
INFO - root - 2017-12-05 16:33:40.683443: step 22890, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.924 sec/batch; 79h:25m:53s remains)
INFO - root - 2017-12-05 16:33:49.823669: step 22900, loss = 2.02, batch loss = 1.96 (8.8 examples/sec; 0.907 sec/batch; 78h:00m:57s remains)
2017-12-05 16:33:50.650367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3176084 -4.3133597 -4.3086686 -4.3048272 -4.3038425 -4.305521 -4.3094578 -4.31522 -4.3218603 -4.3264842 -4.328073 -4.327446 -4.3262296 -4.3246646 -4.3233857][-4.3044658 -4.2996669 -4.2930555 -4.2854738 -4.2823992 -4.2832613 -4.2874851 -4.2958016 -4.305768 -4.3164692 -4.3244452 -4.3275585 -4.326757 -4.3238764 -4.320549][-4.2819266 -4.2744222 -4.2618389 -4.2481346 -4.2420697 -4.2422743 -4.2472038 -4.2591581 -4.2751112 -4.2910709 -4.3057137 -4.3170037 -4.3209085 -4.3194919 -4.3150191][-4.240644 -4.2290726 -4.2112393 -4.1907692 -4.1791515 -4.1771502 -4.1816216 -4.1983838 -4.2229815 -4.2443604 -4.2676325 -4.2888474 -4.2994604 -4.3018441 -4.298173][-4.1838789 -4.1718836 -4.1511412 -4.1216478 -4.0973063 -4.0844059 -4.08165 -4.1073203 -4.1448207 -4.1768765 -4.2140841 -4.2463632 -4.2633247 -4.2703266 -4.2683811][-4.1113729 -4.0990286 -4.0727825 -4.0298262 -3.9844224 -3.9420259 -3.9216013 -3.9662244 -4.031476 -4.0840659 -4.1435437 -4.1916175 -4.2142653 -4.2260838 -4.2265987][-4.0316329 -4.0125313 -3.9785185 -3.9250712 -3.8585019 -3.7786109 -3.7371709 -3.8062217 -3.9006252 -3.9751873 -4.0569453 -4.1207829 -4.1527047 -4.1716509 -4.1756363][-3.9883895 -3.9659557 -3.9314854 -3.8866334 -3.8287823 -3.7518883 -3.7106228 -3.7698627 -3.8520463 -3.9231856 -4.007421 -4.0753908 -4.1136141 -4.1374092 -4.1420593][-4.0204239 -4.0048275 -3.9840386 -3.966326 -3.9455843 -3.9088116 -3.8835135 -3.9069879 -3.9482858 -3.9945307 -4.0576825 -4.1133504 -4.1467957 -4.1681342 -4.1712956][-4.105608 -4.0987105 -4.0858879 -4.0785532 -4.0747848 -4.0628157 -4.0479507 -4.05481 -4.0763144 -4.1068611 -4.1517086 -4.1924496 -4.215631 -4.2300754 -4.2310128][-4.1882749 -4.1854463 -4.1775622 -4.173872 -4.1750932 -4.1734185 -4.1656322 -4.1674194 -4.1785393 -4.1946688 -4.2217674 -4.2492943 -4.2634168 -4.2715578 -4.2696066][-4.225944 -4.2263341 -4.2225847 -4.2215161 -4.2242169 -4.2256522 -4.2222476 -4.2231321 -4.2284074 -4.2363296 -4.2510366 -4.266932 -4.2744107 -4.2773623 -4.2721467][-4.2111692 -4.2135143 -4.2123117 -4.2128568 -4.2160449 -4.2178087 -4.2161493 -4.2169957 -4.2196221 -4.2228389 -4.2285757 -4.2358308 -4.23878 -4.2398472 -4.2369032][-4.1858149 -4.1876636 -4.1867309 -4.1867585 -4.1881008 -4.18863 -4.1869841 -4.1869054 -4.1868267 -4.1861019 -4.187017 -4.1896133 -4.1906428 -4.193543 -4.1953645][-4.1997328 -4.2005134 -4.199801 -4.1998949 -4.200078 -4.1999168 -4.1994228 -4.1987743 -4.1963067 -4.1933422 -4.1921344 -4.1921043 -4.1925397 -4.1964555 -4.2012453]]...]
INFO - root - 2017-12-05 16:33:59.645358: step 22910, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 74h:54m:44s remains)
INFO - root - 2017-12-05 16:34:08.698247: step 22920, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 77h:35m:46s remains)
INFO - root - 2017-12-05 16:34:17.647247: step 22930, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 76h:38m:00s remains)
INFO - root - 2017-12-05 16:34:26.687348: step 22940, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 80h:13m:08s remains)
INFO - root - 2017-12-05 16:34:35.817165: step 22950, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 80h:49m:56s remains)
INFO - root - 2017-12-05 16:34:44.918734: step 22960, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 78h:27m:52s remains)
INFO - root - 2017-12-05 16:34:54.045672: step 22970, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 76h:20m:00s remains)
INFO - root - 2017-12-05 16:35:02.928106: step 22980, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.807 sec/batch; 69h:21m:53s remains)
INFO - root - 2017-12-05 16:35:12.044427: step 22990, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 79h:15m:24s remains)
INFO - root - 2017-12-05 16:35:21.079772: step 23000, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 79h:07m:36s remains)
2017-12-05 16:35:21.846932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1525078 -4.174686 -4.182827 -4.1762114 -4.1676621 -4.1515288 -4.1321197 -4.1447763 -4.1873312 -4.2182922 -4.2305145 -4.238472 -4.2514234 -4.2665596 -4.285563][-4.1330523 -4.1510615 -4.164 -4.1687527 -4.1706142 -4.1569424 -4.1354342 -4.148077 -4.1927342 -4.2234383 -4.2346931 -4.2438397 -4.2586255 -4.2739735 -4.2909331][-4.1249852 -4.1384788 -4.1530681 -4.1678257 -4.1800232 -4.1684532 -4.1466947 -4.1616788 -4.2057962 -4.2409391 -4.2568951 -4.2698832 -4.2853208 -4.2949219 -4.3055372][-4.1206155 -4.1323733 -4.1460581 -4.1647282 -4.1821461 -4.1718278 -4.1580381 -4.179831 -4.2257 -4.2656283 -4.286087 -4.3031697 -4.3169956 -4.3189621 -4.3206043][-4.1079865 -4.1177468 -4.1282387 -4.1469455 -4.1663065 -4.15795 -4.1551 -4.1866255 -4.2376785 -4.281827 -4.3062344 -4.3222408 -4.33034 -4.3270173 -4.3250976][-4.0910482 -4.0970454 -4.1020989 -4.1155171 -4.1287107 -4.1112819 -4.1056142 -4.1407852 -4.1947308 -4.2458858 -4.2812309 -4.3041887 -4.313242 -4.3153505 -4.3182955][-4.0554752 -4.0567417 -4.0555077 -4.0638218 -4.0710387 -4.0413837 -4.0193629 -4.0452137 -4.0977607 -4.1566205 -4.2071791 -4.2420778 -4.2624245 -4.2818933 -4.3004093][-4.0285978 -4.0279994 -4.0246859 -4.0290389 -4.0324826 -3.9927723 -3.942688 -3.941 -3.9817879 -4.0461431 -4.1215529 -4.1764345 -4.2099733 -4.2442484 -4.2773194][-4.0279837 -4.0278749 -4.029283 -4.0322881 -4.0265894 -3.9807792 -3.9197118 -3.8932934 -3.9080887 -3.9634812 -4.0575004 -4.1309986 -4.1773839 -4.2224503 -4.2627053][-4.0374355 -4.0438423 -4.0545478 -4.0595832 -4.0486808 -4.0051055 -3.9534981 -3.9276972 -3.9291818 -3.9711683 -4.0610585 -4.1342916 -4.1835551 -4.22925 -4.2678728][-4.0731239 -4.0814133 -4.095799 -4.1045017 -4.0961943 -4.0651326 -4.0308013 -4.0198817 -4.0269384 -4.0603981 -4.1252084 -4.1820917 -4.2251768 -4.2633567 -4.2942123][-4.1382551 -4.1438193 -4.1584711 -4.1694584 -4.1654663 -4.1466923 -4.1271138 -4.1248755 -4.1352925 -4.1598077 -4.2006464 -4.2405806 -4.2740183 -4.3002315 -4.3209867][-4.2061129 -4.2107372 -4.2213926 -4.2307129 -4.2309475 -4.2215323 -4.210175 -4.2097564 -4.2176933 -4.2342067 -4.2585244 -4.2834392 -4.3044477 -4.3211274 -4.335072][-4.2646589 -4.2664638 -4.2711797 -4.2766356 -4.2793927 -4.2769766 -4.2704206 -4.2674427 -4.2697854 -4.2781458 -4.2916832 -4.3068357 -4.3191876 -4.3302212 -4.3407097][-4.3031144 -4.3004618 -4.3006244 -4.3031888 -4.3060074 -4.306633 -4.3037786 -4.3003016 -4.2994046 -4.3038411 -4.3117981 -4.3205953 -4.3287921 -4.3362703 -4.3429379]]...]
INFO - root - 2017-12-05 16:35:30.922949: step 23010, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 80h:27m:49s remains)
INFO - root - 2017-12-05 16:35:40.196083: step 23020, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 77h:53m:54s remains)
INFO - root - 2017-12-05 16:35:49.200963: step 23030, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 78h:53m:14s remains)
INFO - root - 2017-12-05 16:35:58.255649: step 23040, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 79h:03m:12s remains)
INFO - root - 2017-12-05 16:36:07.421896: step 23050, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 76h:55m:34s remains)
INFO - root - 2017-12-05 16:36:16.643065: step 23060, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 76h:51m:41s remains)
INFO - root - 2017-12-05 16:36:25.837864: step 23070, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 79h:36m:43s remains)
INFO - root - 2017-12-05 16:36:34.870186: step 23080, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 79h:52m:16s remains)
INFO - root - 2017-12-05 16:36:44.164070: step 23090, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 80h:11m:11s remains)
INFO - root - 2017-12-05 16:36:53.274245: step 23100, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 79h:42m:51s remains)
2017-12-05 16:36:54.054337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2099214 -4.2286658 -4.2454505 -4.25479 -4.2553668 -4.2500291 -4.2327552 -4.2090635 -4.2061472 -4.2256761 -4.2566261 -4.2885146 -4.3099108 -4.3213329 -4.3340859][-4.2101803 -4.2239757 -4.2382736 -4.2454467 -4.2480721 -4.2468853 -4.2301574 -4.2051649 -4.198276 -4.2115974 -4.2389255 -4.2696886 -4.2914453 -4.3057075 -4.323493][-4.2279692 -4.2342696 -4.2440743 -4.2507024 -4.2557898 -4.2572141 -4.2390828 -4.2109661 -4.1944342 -4.1974139 -4.2209759 -4.2516956 -4.2760515 -4.2949133 -4.3198209][-4.2594976 -4.2621551 -4.2680058 -4.2726803 -4.2755475 -4.2699142 -4.2444611 -4.2072 -4.1801558 -4.1761279 -4.1976128 -4.228261 -4.2575212 -4.2859254 -4.3199472][-4.2655849 -4.2678108 -4.2702994 -4.2717953 -4.269537 -4.2533016 -4.2151966 -4.1679235 -4.1390829 -4.1381555 -4.16325 -4.1976285 -4.2378445 -4.2810788 -4.3229856][-4.2260594 -4.2333088 -4.2332535 -4.2312536 -4.2211967 -4.1924353 -4.1409669 -4.0875092 -4.0680542 -4.0835223 -4.1233921 -4.1699848 -4.2259903 -4.2818837 -4.3281536][-4.1489577 -4.1738334 -4.1765828 -4.165175 -4.1418362 -4.0976577 -4.027822 -3.96689 -3.9694474 -4.0182614 -4.0892897 -4.1558819 -4.2232327 -4.2841196 -4.331388][-4.0681615 -4.1124825 -4.1159921 -4.0937662 -4.0543818 -3.9943681 -3.9051924 -3.8506544 -3.8955257 -3.9877295 -4.0881777 -4.1687894 -4.2372937 -4.2944136 -4.3374543][-4.0159984 -4.0723467 -4.0770988 -4.0488591 -4.0043697 -3.9434059 -3.8591466 -3.8306892 -3.9070578 -4.01624 -4.1239653 -4.2048068 -4.2678518 -4.316802 -4.3496189][-4.0230465 -4.074347 -4.0816169 -4.0589151 -4.0256095 -3.986587 -3.9336095 -3.929235 -3.9992492 -4.088901 -4.1773205 -4.2460985 -4.2994132 -4.3384848 -4.3603044][-4.1047354 -4.1383767 -4.1460972 -4.1330218 -4.115962 -4.0977674 -4.07055 -4.0736623 -4.118434 -4.1754904 -4.2375412 -4.2918649 -4.3335919 -4.3602409 -4.3703108][-4.2095122 -4.2245722 -4.2256079 -4.2177939 -4.2122545 -4.2068305 -4.1953979 -4.1997561 -4.2257075 -4.2575612 -4.29778 -4.3359952 -4.3643556 -4.3770304 -4.3771858][-4.2905283 -4.2941928 -4.2900529 -4.2849 -4.2863088 -4.2894077 -4.2870393 -4.2890968 -4.3020382 -4.3204865 -4.3461661 -4.3695259 -4.3853631 -4.3881168 -4.3819146][-4.3336682 -4.3336897 -4.3298788 -4.327529 -4.3303785 -4.3344121 -4.3349962 -4.33439 -4.3390994 -4.350225 -4.365963 -4.3787394 -4.3850794 -4.3842707 -4.378541][-4.3465195 -4.34388 -4.3412504 -4.3402505 -4.34171 -4.3435531 -4.3434329 -4.3428 -4.3442359 -4.3515 -4.3613377 -4.3690467 -4.3721938 -4.3723 -4.3704967]]...]
INFO - root - 2017-12-05 16:37:03.163673: step 23110, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 79h:17m:36s remains)
INFO - root - 2017-12-05 16:37:12.173116: step 23120, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 76h:54m:25s remains)
INFO - root - 2017-12-05 16:37:21.409818: step 23130, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.952 sec/batch; 81h:47m:48s remains)
INFO - root - 2017-12-05 16:37:30.662554: step 23140, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 82h:36m:44s remains)
INFO - root - 2017-12-05 16:37:39.952146: step 23150, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 81h:19m:58s remains)
INFO - root - 2017-12-05 16:37:49.094507: step 23160, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 80h:31m:22s remains)
INFO - root - 2017-12-05 16:37:58.082719: step 23170, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 80h:10m:17s remains)
INFO - root - 2017-12-05 16:38:07.158116: step 23180, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 78h:59m:38s remains)
INFO - root - 2017-12-05 16:38:16.448833: step 23190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 78h:30m:06s remains)
INFO - root - 2017-12-05 16:38:25.502476: step 23200, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 77h:06m:44s remains)
2017-12-05 16:38:26.306247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3479166 -4.342041 -4.3367829 -4.3318081 -4.3288517 -4.3288727 -4.3320336 -4.3348756 -4.3363352 -4.337781 -4.3373947 -4.3370862 -4.336638 -4.3360009 -4.3361645][-4.3475318 -4.3431783 -4.3392558 -4.3342113 -4.3296223 -4.3268828 -4.3271103 -4.3285284 -4.3309021 -4.3342733 -4.3360062 -4.3364615 -4.3359156 -4.3350749 -4.3357134][-4.3443565 -4.3411369 -4.3380146 -4.3328552 -4.3266478 -4.3204942 -4.3169374 -4.31661 -4.3205047 -4.3271718 -4.33279 -4.3370786 -4.3382726 -4.3371534 -4.3370929][-4.3405313 -4.338275 -4.3361678 -4.3308668 -4.3219643 -4.31063 -4.3013606 -4.2976441 -4.3032889 -4.314395 -4.3263154 -4.3370008 -4.3417616 -4.3412552 -4.3392944][-4.3342953 -4.3335447 -4.3324714 -4.3244209 -4.3096209 -4.2889633 -4.2674494 -4.252768 -4.2580466 -4.2766337 -4.3007088 -4.3237462 -4.3372335 -4.3406706 -4.3382158][-4.3220491 -4.3227129 -4.321126 -4.3070965 -4.2789178 -4.2359667 -4.1881304 -4.1536288 -4.1579928 -4.193594 -4.24378 -4.29027 -4.3199091 -4.3324738 -4.3328276][-4.3064904 -4.3080411 -4.3042359 -4.2810378 -4.2324862 -4.1529317 -4.06192 -3.9971972 -4.0077929 -4.0724788 -4.1618371 -4.2401004 -4.2912335 -4.3176885 -4.3252096][-4.2973065 -4.298182 -4.2903423 -4.2589087 -4.19236 -4.0798407 -3.9473319 -3.8514233 -3.8722694 -3.9686332 -4.0913849 -4.1955853 -4.2651591 -4.3047647 -4.3207154][-4.3000894 -4.3009644 -4.29166 -4.2588139 -4.188807 -4.0701804 -3.9318938 -3.8306932 -3.856154 -3.9586167 -4.0830808 -4.1868362 -4.2588296 -4.3027682 -4.3227587][-4.3093872 -4.3139935 -4.3077841 -4.2800674 -4.2245555 -4.1316338 -4.0276423 -3.9536138 -3.968667 -4.0420237 -4.1348562 -4.2146754 -4.271822 -4.308228 -4.325232][-4.3161182 -4.3261328 -4.3266683 -4.3097353 -4.2753348 -4.217515 -4.1548495 -4.1107492 -4.1173224 -4.1592774 -4.21408 -4.2627745 -4.2965007 -4.3169065 -4.3262992][-4.3165131 -4.3317227 -4.33937 -4.3347831 -4.3191209 -4.2898469 -4.2577872 -4.2364841 -4.23981 -4.2601528 -4.2849984 -4.3070989 -4.3203039 -4.326129 -4.3281727][-4.3128848 -4.331398 -4.3445067 -4.3492055 -4.3454089 -4.3325105 -4.3179717 -4.3087535 -4.3094692 -4.3182011 -4.3272147 -4.3335614 -4.3335257 -4.3317819 -4.3304253][-4.30721 -4.3259377 -4.3418422 -4.3520041 -4.3534269 -4.3465605 -4.3361669 -4.3278422 -4.3246117 -4.3273129 -4.3316131 -4.3354363 -4.334938 -4.3342214 -4.3342638][-4.3002114 -4.3154659 -4.3312378 -4.3441648 -4.3477635 -4.3406134 -4.3273616 -4.3145843 -4.3067565 -4.3080111 -4.315105 -4.3251853 -4.3319139 -4.3366971 -4.3399754]]...]
INFO - root - 2017-12-05 16:38:35.515194: step 23210, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 76h:51m:31s remains)
INFO - root - 2017-12-05 16:38:44.670269: step 23220, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 77h:45m:51s remains)
INFO - root - 2017-12-05 16:38:53.783875: step 23230, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 78h:38m:47s remains)
INFO - root - 2017-12-05 16:39:02.853004: step 23240, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 80h:12m:07s remains)
INFO - root - 2017-12-05 16:39:11.916041: step 23250, loss = 2.02, batch loss = 1.97 (8.7 examples/sec; 0.917 sec/batch; 78h:44m:56s remains)
INFO - root - 2017-12-05 16:39:21.042428: step 23260, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 76h:45m:50s remains)
INFO - root - 2017-12-05 16:39:30.150580: step 23270, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 75h:29m:05s remains)
INFO - root - 2017-12-05 16:39:39.376808: step 23280, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 80h:11m:31s remains)
INFO - root - 2017-12-05 16:39:48.553090: step 23290, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 78h:58m:13s remains)
INFO - root - 2017-12-05 16:39:57.733914: step 23300, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 75h:30m:35s remains)
2017-12-05 16:39:58.476429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0900435 -4.0711222 -4.0700312 -4.0722156 -4.0650368 -4.061285 -4.0755682 -4.0861015 -4.0893021 -4.09683 -4.1028252 -4.1004992 -4.096138 -4.1068416 -4.1219544][-4.100215 -4.078599 -4.0697017 -4.064836 -4.0563231 -4.0478783 -4.0566144 -4.0779076 -4.0927587 -4.1005898 -4.1020384 -4.0995588 -4.0944524 -4.1067438 -4.1254816][-4.1145258 -4.0950823 -4.0795031 -4.0693288 -4.0590081 -4.046958 -4.0508628 -4.07808 -4.1001081 -4.1035275 -4.1002092 -4.0951657 -4.0936537 -4.1115413 -4.1369352][-4.12327 -4.1040916 -4.0843148 -4.0645657 -4.0427232 -4.0280766 -4.0416842 -4.0757747 -4.0954585 -4.0917583 -4.08363 -4.078649 -4.0859051 -4.11479 -4.147913][-4.1212578 -4.1018529 -4.0773964 -4.0475674 -4.0160937 -4.0010042 -4.0228286 -4.0623245 -4.0784845 -4.067728 -4.0569754 -4.0510178 -4.0584903 -4.0926571 -4.1310387][-4.1310315 -4.1167936 -4.0939488 -4.0617423 -4.0281405 -4.0127826 -4.0291591 -4.0642614 -4.0815296 -4.0772142 -4.0658283 -4.0528851 -4.0457463 -4.0648155 -4.0999718][-4.1309395 -4.1266851 -4.11103 -4.0875311 -4.0691853 -4.0643091 -4.0746183 -4.1019154 -4.1182613 -4.1124182 -4.1021748 -4.0898128 -4.0689473 -4.059586 -4.0756569][-4.1232767 -4.1213961 -4.1079426 -4.0984521 -4.1004939 -4.108007 -4.1139278 -4.136127 -4.1547422 -4.1483221 -4.13736 -4.12775 -4.10647 -4.08589 -4.08404][-4.136178 -4.1268954 -4.109479 -4.1029077 -4.1115627 -4.1248894 -4.1305838 -4.1466069 -4.1707344 -4.1729503 -4.1653833 -4.1542683 -4.1317277 -4.1105523 -4.1064649][-4.1617284 -4.1490164 -4.1304231 -4.125567 -4.1315684 -4.139677 -4.1381817 -4.1449108 -4.1639404 -4.1724625 -4.1683283 -4.1566525 -4.1347 -4.1132946 -4.1086726][-4.1663589 -4.1633139 -4.1543941 -4.1505289 -4.1501126 -4.1490741 -4.1460733 -4.1461558 -4.1589994 -4.1681786 -4.1662 -4.1564565 -4.1380796 -4.1157107 -4.1048245][-4.1502528 -4.1548071 -4.1557393 -4.1555061 -4.1566167 -4.1591649 -4.1625075 -4.1643367 -4.1759048 -4.1851358 -4.1847339 -4.1802015 -4.1678009 -4.1447096 -4.1189232][-4.1395278 -4.1404324 -4.1396546 -4.137958 -4.1403737 -4.145998 -4.1528516 -4.1556244 -4.1652732 -4.1760902 -4.182703 -4.1866193 -4.1838222 -4.1692348 -4.142118][-4.1593161 -4.1507678 -4.1421452 -4.1330657 -4.129375 -4.1300573 -4.1328506 -4.132422 -4.1369762 -4.1470327 -4.1554551 -4.1605768 -4.1648211 -4.1610775 -4.142035][-4.1853843 -4.1770363 -4.1665635 -4.1537657 -4.146924 -4.1428609 -4.1393871 -4.1325445 -4.1250091 -4.1240754 -4.124989 -4.1281772 -4.1323018 -4.1332517 -4.1212034]]...]
INFO - root - 2017-12-05 16:40:07.645051: step 23310, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.964 sec/batch; 82h:47m:48s remains)
INFO - root - 2017-12-05 16:40:16.657576: step 23320, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 79h:03m:32s remains)
INFO - root - 2017-12-05 16:40:25.801854: step 23330, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 78h:19m:42s remains)
INFO - root - 2017-12-05 16:40:34.923534: step 23340, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 79h:30m:28s remains)
INFO - root - 2017-12-05 16:40:44.052199: step 23350, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 79h:50m:20s remains)
INFO - root - 2017-12-05 16:40:53.037777: step 23360, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 77h:44m:34s remains)
INFO - root - 2017-12-05 16:41:02.076507: step 23370, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.947 sec/batch; 81h:20m:19s remains)
INFO - root - 2017-12-05 16:41:11.177088: step 23380, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 77h:41m:32s remains)
INFO - root - 2017-12-05 16:41:20.302714: step 23390, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 76h:22m:27s remains)
INFO - root - 2017-12-05 16:41:29.393383: step 23400, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 79h:01m:58s remains)
2017-12-05 16:41:30.166681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.283215 -4.2756052 -4.2628279 -4.2494068 -4.2326078 -4.2023692 -4.1627607 -4.1307292 -4.124414 -4.1383743 -4.1656342 -4.2033582 -4.2510781 -4.2887669 -4.316206][-4.2618241 -4.2571445 -4.2468033 -4.2366953 -4.2198529 -4.1875925 -4.1402268 -4.1015868 -4.08884 -4.1006465 -4.1323423 -4.1776071 -4.2346716 -4.282342 -4.3209643][-4.2414045 -4.2364531 -4.2284608 -4.2193437 -4.1998682 -4.1583095 -4.1002884 -4.0557656 -4.0378165 -4.0448828 -4.0814347 -4.1355743 -4.2021112 -4.2644019 -4.3170433][-4.2217965 -4.2143235 -4.2077317 -4.2010851 -4.1791368 -4.1331992 -4.0678005 -4.0149803 -3.9949663 -4.0009613 -4.04257 -4.1035542 -4.1758013 -4.247611 -4.3110914][-4.1882114 -4.1825957 -4.1831741 -4.1857419 -4.1682734 -4.1206818 -4.0479279 -3.9843516 -3.9570029 -3.9568722 -3.9996071 -4.0716476 -4.1541524 -4.234724 -4.303853][-4.1610146 -4.1582417 -4.1645947 -4.1691632 -4.1525874 -4.1056361 -4.0336967 -3.9594088 -3.9172437 -3.9103384 -3.9538031 -4.03621 -4.125699 -4.2172194 -4.2930574][-4.15174 -4.1492906 -4.1545119 -4.1543641 -4.1365256 -4.0946126 -4.0324774 -3.9621518 -3.9226229 -3.9137003 -3.9497333 -4.02857 -4.1128392 -4.2033567 -4.2797561][-4.1515746 -4.1436734 -4.1460295 -4.144897 -4.1323724 -4.1057959 -4.0645375 -4.0088348 -3.9629838 -3.9312007 -3.9425509 -4.0092807 -4.0909705 -4.1805491 -4.2602544][-4.1629686 -4.1488261 -4.1464543 -4.1432953 -4.1328859 -4.122004 -4.10047 -4.0496864 -3.9846129 -3.9302189 -3.9196339 -3.9784117 -4.0626655 -4.1519985 -4.2366204][-4.1741409 -4.1700811 -4.1680126 -4.1594558 -4.1440687 -4.1392088 -4.1302104 -4.08429 -4.0152054 -3.9610441 -3.9432397 -3.986037 -4.0614514 -4.1440592 -4.2251115][-4.2094722 -4.2175708 -4.2130947 -4.2001996 -4.182694 -4.1771603 -4.16928 -4.1300554 -4.069644 -4.0224094 -4.0066595 -4.0354362 -4.0984335 -4.1690979 -4.2407575][-4.253737 -4.2656884 -4.2609606 -4.2494574 -4.2352252 -4.2299051 -4.2192588 -4.1819515 -4.1286688 -4.0900836 -4.0781374 -4.0980906 -4.1514034 -4.2086868 -4.2675624][-4.2951736 -4.3066773 -4.3044043 -4.2960839 -4.2885618 -4.2848315 -4.2740655 -4.2443047 -4.2010593 -4.1725035 -4.1674619 -4.1855049 -4.2251596 -4.2647719 -4.3044887][-4.3311639 -4.3376603 -4.3363681 -4.3312249 -4.3269186 -4.3237219 -4.3161192 -4.2993388 -4.2743044 -4.2572632 -4.2563848 -4.2696123 -4.2931266 -4.3161683 -4.3383107][-4.3540883 -4.3562827 -4.3550086 -4.3519635 -4.3492656 -4.3462248 -4.3423729 -4.3367462 -4.3258853 -4.3179274 -4.3171673 -4.3227339 -4.3324246 -4.3438053 -4.354002]]...]
INFO - root - 2017-12-05 16:41:39.300439: step 23410, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.932 sec/batch; 80h:02m:00s remains)
INFO - root - 2017-12-05 16:41:48.382976: step 23420, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 77h:08m:49s remains)
INFO - root - 2017-12-05 16:41:57.326861: step 23430, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 74h:16m:02s remains)
INFO - root - 2017-12-05 16:42:06.348137: step 23440, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 80h:02m:27s remains)
INFO - root - 2017-12-05 16:42:15.411174: step 23450, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 77h:28m:57s remains)
INFO - root - 2017-12-05 16:42:24.561905: step 23460, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 77h:21m:43s remains)
INFO - root - 2017-12-05 16:42:33.573901: step 23470, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 77h:42m:32s remains)
INFO - root - 2017-12-05 16:42:42.685574: step 23480, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 79h:01m:21s remains)
INFO - root - 2017-12-05 16:42:52.136362: step 23490, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 81h:12m:27s remains)
INFO - root - 2017-12-05 16:43:01.311942: step 23500, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 81h:49m:30s remains)
2017-12-05 16:43:02.071376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22286 -4.2207918 -4.2280574 -4.230175 -4.2155204 -4.214241 -4.2235012 -4.222404 -4.2162023 -4.2093625 -4.1965055 -4.19534 -4.2000289 -4.2001743 -4.2156754][-4.192698 -4.198617 -4.212069 -4.2155271 -4.2020712 -4.2044272 -4.2146149 -4.2129726 -4.2039418 -4.18745 -4.1652555 -4.1666903 -4.180398 -4.1847577 -4.2028317][-4.1701894 -4.1808414 -4.1953678 -4.1987228 -4.1888833 -4.199595 -4.2134233 -4.2148223 -4.2078013 -4.1861653 -4.1574225 -4.15739 -4.1734271 -4.1810684 -4.2005415][-4.1619568 -4.1680303 -4.1747551 -4.1765947 -4.1735644 -4.1921506 -4.2048879 -4.2077651 -4.2101526 -4.1931796 -4.1627 -4.15857 -4.172595 -4.182735 -4.2019691][-4.1767235 -4.1753559 -4.1705108 -4.1713 -4.1737418 -4.1893373 -4.1854639 -4.1774292 -4.1929555 -4.194212 -4.1724644 -4.1675215 -4.1780944 -4.1861572 -4.2023335][-4.2038794 -4.1998672 -4.1870761 -4.1876831 -4.1917787 -4.192389 -4.1573992 -4.117435 -4.1387496 -4.1733146 -4.1767106 -4.17962 -4.1907949 -4.1926417 -4.2055368][-4.2104158 -4.2079868 -4.1976066 -4.1995029 -4.2016506 -4.1823697 -4.1065679 -4.0142403 -4.0331674 -4.1166959 -4.1619844 -4.1835046 -4.2002769 -4.2000103 -4.2099333][-4.1932549 -4.1958861 -4.1977267 -4.2058463 -4.210227 -4.1814084 -4.0805478 -3.9420338 -3.9497249 -4.0764375 -4.1564989 -4.1946335 -4.2154837 -4.2153153 -4.2203565][-4.1785316 -4.1899228 -4.2034726 -4.2170992 -4.2269578 -4.2072539 -4.11763 -3.9895315 -3.9865763 -4.095242 -4.1721368 -4.2139525 -4.2357731 -4.2363706 -4.2395616][-4.17974 -4.1981297 -4.2171612 -4.2349768 -4.2495646 -4.2436543 -4.1826239 -4.0905271 -4.0841489 -4.1483011 -4.1989012 -4.23236 -4.2537723 -4.2588019 -4.2646742][-4.1971054 -4.2130842 -4.2284207 -4.2440834 -4.2580609 -4.2622652 -4.22736 -4.170608 -4.1678967 -4.1988764 -4.2232776 -4.2463131 -4.2678585 -4.278791 -4.2890286][-4.2164459 -4.22674 -4.2376103 -4.2493839 -4.2584429 -4.2654977 -4.2502418 -4.2201285 -4.2228384 -4.235527 -4.2426105 -4.2572837 -4.2796035 -4.29359 -4.306489][-4.2443976 -4.25001 -4.2571707 -4.2661495 -4.2696762 -4.2757239 -4.2720728 -4.256978 -4.2620845 -4.2653613 -4.2653913 -4.2753239 -4.2952166 -4.3061028 -4.3178072][-4.2726417 -4.2760844 -4.2811165 -4.2887158 -4.2894878 -4.2915134 -4.2911873 -4.2852693 -4.2887368 -4.2884083 -4.2872677 -4.294415 -4.3079367 -4.3154163 -4.3245678][-4.2874231 -4.2912989 -4.2965612 -4.3026695 -4.3017778 -4.3011889 -4.3023877 -4.3002973 -4.3015828 -4.3005877 -4.3004546 -4.3065081 -4.3149462 -4.3194551 -4.3261232]]...]
INFO - root - 2017-12-05 16:43:11.092422: step 23510, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 75h:14m:04s remains)
INFO - root - 2017-12-05 16:43:20.164713: step 23520, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 80h:57m:59s remains)
INFO - root - 2017-12-05 16:43:29.194019: step 23530, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 77h:07m:54s remains)
INFO - root - 2017-12-05 16:43:38.314494: step 23540, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 78h:51m:14s remains)
INFO - root - 2017-12-05 16:43:47.358743: step 23550, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 79h:23m:34s remains)
INFO - root - 2017-12-05 16:43:56.458627: step 23560, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 75h:46m:03s remains)
INFO - root - 2017-12-05 16:44:05.672403: step 23570, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 79h:52m:12s remains)
INFO - root - 2017-12-05 16:44:14.594360: step 23580, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 75h:43m:47s remains)
INFO - root - 2017-12-05 16:44:23.628870: step 23590, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 77h:03m:34s remains)
INFO - root - 2017-12-05 16:44:32.835204: step 23600, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 77h:35m:24s remains)
2017-12-05 16:44:33.666453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2232494 -4.2270823 -4.2329254 -4.2355475 -4.2386808 -4.2397494 -4.2380676 -4.2350392 -4.2312722 -4.2258325 -4.2153807 -4.2028508 -4.1993814 -4.2022047 -4.2019844][-4.235498 -4.2379236 -4.2434368 -4.2448287 -4.2478337 -4.2498231 -4.2462549 -4.239264 -4.2347441 -4.2291169 -4.2195177 -4.2112155 -4.2120824 -4.2168975 -4.2130284][-4.2468319 -4.2452493 -4.2475533 -4.2457042 -4.2453175 -4.2465816 -4.2408061 -4.23141 -4.2277789 -4.2275181 -4.2257681 -4.2266035 -4.232626 -4.2365913 -4.2283683][-4.2538776 -4.2478728 -4.2412481 -4.2301426 -4.2206564 -4.2158318 -4.2057233 -4.1945171 -4.1970558 -4.208951 -4.2192755 -4.2306657 -4.2428703 -4.2473717 -4.2376614][-4.2512803 -4.2405 -4.2243824 -4.2025981 -4.1817331 -4.1668386 -4.14846 -4.1331348 -4.1432428 -4.1696286 -4.1945963 -4.2180028 -4.2388377 -4.2464972 -4.2400036][-4.2374892 -4.2228012 -4.2012439 -4.1719556 -4.1412978 -4.1128063 -4.0794587 -4.0546141 -4.0713711 -4.1133413 -4.1544881 -4.190732 -4.2204814 -4.2345567 -4.2343559][-4.2169976 -4.2006941 -4.1794791 -4.1490088 -4.1098228 -4.065464 -4.0127273 -3.9724126 -3.9962754 -4.0559497 -4.1109281 -4.15694 -4.1949525 -4.21796 -4.2243776][-4.2120562 -4.1979995 -4.180572 -4.1542044 -4.1156325 -4.0685425 -4.0106659 -3.9641368 -3.9864354 -4.04815 -4.1026196 -4.1464877 -4.1839361 -4.209836 -4.21931][-4.2234731 -4.2137108 -4.20257 -4.1861281 -4.1610403 -4.1294441 -4.087728 -4.0499973 -4.0573664 -4.0969849 -4.1348419 -4.167408 -4.19763 -4.2209163 -4.2270975][-4.2380533 -4.2325635 -4.2283549 -4.2214856 -4.2086725 -4.1920314 -4.1677275 -4.1394706 -4.1357656 -4.1545067 -4.1759939 -4.1973557 -4.2197347 -4.2381763 -4.2409868][-4.2513704 -4.2486773 -4.2487831 -4.246181 -4.2395773 -4.2326822 -4.2202778 -4.1997366 -4.1926017 -4.2015862 -4.2113638 -4.2229815 -4.236589 -4.2491951 -4.2504292][-4.258255 -4.258142 -4.2590547 -4.2583847 -4.2552404 -4.2523928 -4.2444472 -4.2287774 -4.2216797 -4.2268825 -4.2315111 -4.2375455 -4.2442684 -4.2518487 -4.2528276][-4.254971 -4.2562232 -4.2557693 -4.2557936 -4.2551022 -4.2530732 -4.2475557 -4.2365756 -4.2313657 -4.2353077 -4.2396274 -4.2434039 -4.2463937 -4.2496529 -4.2500086][-4.2385659 -4.2380581 -4.236084 -4.2356992 -4.2367435 -4.2378616 -4.2365732 -4.2319212 -4.2301116 -4.2343812 -4.2392244 -4.2421947 -4.2426252 -4.2429276 -4.2430987][-4.2202735 -4.2183638 -4.2157903 -4.2145872 -4.2158217 -4.219368 -4.2223067 -4.2230368 -4.2248969 -4.2302418 -4.2358952 -4.2387567 -4.2387605 -4.2379451 -4.2393732]]...]
INFO - root - 2017-12-05 16:44:42.778978: step 23610, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 74h:22m:48s remains)
INFO - root - 2017-12-05 16:44:51.919369: step 23620, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 77h:52m:36s remains)
INFO - root - 2017-12-05 16:45:00.938217: step 23630, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 76h:47m:42s remains)
INFO - root - 2017-12-05 16:45:09.796661: step 23640, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 78h:36m:17s remains)
INFO - root - 2017-12-05 16:45:18.877149: step 23650, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 78h:27m:38s remains)
INFO - root - 2017-12-05 16:45:27.967740: step 23660, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 77h:17m:11s remains)
INFO - root - 2017-12-05 16:45:37.036139: step 23670, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 78h:26m:07s remains)
INFO - root - 2017-12-05 16:45:46.138533: step 23680, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 76h:10m:14s remains)
INFO - root - 2017-12-05 16:45:55.413914: step 23690, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 77h:56m:18s remains)
INFO - root - 2017-12-05 16:46:04.305537: step 23700, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 75h:31m:05s remains)
2017-12-05 16:46:05.157565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2832351 -4.2895565 -4.2958083 -4.299767 -4.3001146 -4.2977076 -4.2952371 -4.2933664 -4.2911415 -4.2897873 -4.291234 -4.2935381 -4.2909956 -4.2836862 -4.2785316][-4.2554116 -4.2643309 -4.2731624 -4.2790108 -4.2809672 -4.2799015 -4.27748 -4.2747927 -4.2718816 -4.2715945 -4.2749186 -4.2785149 -4.2753253 -4.2657986 -4.2598815][-4.2284136 -4.2384176 -4.2479844 -4.2540097 -4.2558212 -4.2533445 -4.248486 -4.244647 -4.2430463 -4.2451344 -4.2508688 -4.2571182 -4.2564335 -4.2485414 -4.2440052][-4.2061849 -4.2149429 -4.2237315 -4.2283635 -4.2282777 -4.2225509 -4.2135019 -4.2078209 -4.2079287 -4.213768 -4.2220359 -4.2314019 -4.2361746 -4.2329054 -4.2299018][-4.1987548 -4.2047553 -4.2100968 -4.2099972 -4.2054696 -4.1955771 -4.1818547 -4.1720591 -4.1715255 -4.182601 -4.1973758 -4.2121577 -4.2226133 -4.2213464 -4.2144003][-4.2060595 -4.2088509 -4.207346 -4.198873 -4.185482 -4.16704 -4.1421795 -4.1183529 -4.1090069 -4.1258612 -4.1556187 -4.1824675 -4.2017879 -4.2051482 -4.1976194][-4.2158332 -4.217021 -4.2127013 -4.2005343 -4.1824083 -4.1567545 -4.119298 -4.0753126 -4.0466647 -4.061008 -4.103446 -4.1444035 -4.1762314 -4.1907015 -4.188622][-4.2290378 -4.2303042 -4.2288957 -4.2222424 -4.2098737 -4.1884694 -4.15493 -4.1104212 -4.0765882 -4.0801044 -4.1136527 -4.1505961 -4.182528 -4.2001987 -4.20047][-4.231761 -4.2313304 -4.2325244 -4.2329617 -4.2294803 -4.2183084 -4.1971531 -4.1665215 -4.140748 -4.1383233 -4.1582904 -4.1845107 -4.20858 -4.2247143 -4.2276883][-4.2295952 -4.2285633 -4.230092 -4.2334952 -4.2355704 -4.2354565 -4.2285981 -4.2151971 -4.2010202 -4.1946216 -4.2025509 -4.2180805 -4.2322454 -4.2437963 -4.2505379][-4.2269044 -4.2252111 -4.2248921 -4.2259426 -4.2288547 -4.2352133 -4.2391286 -4.2394452 -4.2381883 -4.2366667 -4.2421818 -4.25184 -4.2577248 -4.2620025 -4.2661915][-4.223928 -4.2236652 -4.221426 -4.2175293 -4.2164326 -4.2194147 -4.2227945 -4.2270055 -4.2319551 -4.2362285 -4.2466245 -4.2589869 -4.2659307 -4.2686582 -4.2714219][-4.2122674 -4.2105665 -4.2075973 -4.2027988 -4.1994133 -4.1999679 -4.2022591 -4.206109 -4.2118559 -4.2174754 -4.2277513 -4.2385244 -4.2441559 -4.2477427 -4.2521811][-4.2131686 -4.2059112 -4.1993527 -4.1915507 -4.1848311 -4.18225 -4.183485 -4.1877294 -4.1939106 -4.1998029 -4.2084713 -4.2164803 -4.2204027 -4.223062 -4.2267628][-4.2288594 -4.2200823 -4.2126131 -4.2046137 -4.1955523 -4.1873684 -4.1809406 -4.1776772 -4.1784711 -4.1819139 -4.1873808 -4.1917481 -4.1944818 -4.1983681 -4.2038693]]...]
INFO - root - 2017-12-05 16:46:14.134354: step 23710, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 78h:47m:07s remains)
INFO - root - 2017-12-05 16:46:23.331662: step 23720, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 77h:26m:54s remains)
INFO - root - 2017-12-05 16:46:32.309633: step 23730, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 72h:18m:34s remains)
INFO - root - 2017-12-05 16:46:41.364898: step 23740, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 76h:46m:50s remains)
INFO - root - 2017-12-05 16:46:50.685251: step 23750, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 80h:04m:57s remains)
INFO - root - 2017-12-05 16:46:59.847904: step 23760, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 77h:13m:39s remains)
INFO - root - 2017-12-05 16:47:08.835984: step 23770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 75h:02m:19s remains)
INFO - root - 2017-12-05 16:47:17.784936: step 23780, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 75h:21m:24s remains)
INFO - root - 2017-12-05 16:47:26.887395: step 23790, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 75h:19m:08s remains)
INFO - root - 2017-12-05 16:47:36.079480: step 23800, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 78h:46m:52s remains)
2017-12-05 16:47:36.862842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2412248 -4.2437496 -4.2458167 -4.248354 -4.2543669 -4.2549133 -4.2487559 -4.2412076 -4.2377448 -4.2379446 -4.2356982 -4.2302232 -4.229156 -4.2313018 -4.2342587][-4.2305098 -4.2352843 -4.2391286 -4.240036 -4.2408829 -4.2387733 -4.2350564 -4.2307715 -4.2273498 -4.2243547 -4.2186103 -4.2122765 -4.209733 -4.2102876 -4.2137184][-4.2085452 -4.2126489 -4.2169924 -4.2187843 -4.21676 -4.2116518 -4.2087932 -4.2082357 -4.2079978 -4.2034054 -4.1945324 -4.1873293 -4.1835127 -4.1818895 -4.18387][-4.1859779 -4.1838779 -4.1837726 -4.1834455 -4.1791673 -4.1710434 -4.1678252 -4.1728029 -4.1819839 -4.1800227 -4.1691332 -4.1584172 -4.1512456 -4.1460891 -4.1452947][-4.1708794 -4.1589828 -4.1491117 -4.1409397 -4.1302767 -4.1167603 -4.1100225 -4.1207337 -4.1431742 -4.1510925 -4.1393881 -4.1235633 -4.1129851 -4.0995078 -4.0867643][-4.1680493 -4.1460638 -4.1247253 -4.1058621 -4.0842919 -4.0539656 -4.0260768 -4.0333552 -4.0747333 -4.1034646 -4.1019764 -4.0890961 -4.0785904 -4.0517159 -4.0134025][-4.1722717 -4.1462226 -4.1166382 -4.0863881 -4.04551 -3.9832685 -3.911607 -3.9012015 -3.9722166 -4.0396647 -4.0633855 -4.0647221 -4.0559578 -4.0189276 -3.96073][-4.1875525 -4.1690207 -4.1415486 -4.1083193 -4.0584683 -3.97797 -3.876075 -3.8416471 -3.9231431 -4.0096354 -4.0492744 -4.0647964 -4.0663619 -4.0377808 -3.9873674][-4.2075872 -4.2002306 -4.1835175 -4.1610451 -4.1230135 -4.0575156 -3.9785426 -3.9454169 -3.9911382 -4.0462441 -4.0737181 -4.0927172 -4.1040459 -4.0895824 -4.0585928][-4.2075949 -4.2009368 -4.190083 -4.1792564 -4.1585155 -4.1187649 -4.0738525 -4.053381 -4.072896 -4.0971427 -4.10596 -4.1179547 -4.1292257 -4.1191492 -4.1009607][-4.1863742 -4.1768889 -4.170815 -4.1697459 -4.1625371 -4.1405735 -4.1167336 -4.1066623 -4.111083 -4.1178894 -4.1151323 -4.1227722 -4.1295795 -4.1210561 -4.1131935][-4.157547 -4.150455 -4.1457391 -4.1522851 -4.1558504 -4.1421762 -4.1266842 -4.1194377 -4.112062 -4.1058359 -4.1005831 -4.1092806 -4.1206822 -4.1184855 -4.1191382][-4.1273856 -4.11953 -4.1120844 -4.1223526 -4.1324716 -4.1230922 -4.1086254 -4.1039577 -4.0924935 -4.0774264 -4.0746632 -4.0901666 -4.1067052 -4.1101203 -4.1184788][-4.1170034 -4.1049161 -4.0952344 -4.1076665 -4.1240954 -4.118753 -4.1046195 -4.1045218 -4.0970297 -4.0810037 -4.0826941 -4.104125 -4.1179709 -4.1213813 -4.1328015][-4.1150966 -4.1010051 -4.0978336 -4.1176772 -4.140923 -4.1435137 -4.1345539 -4.1321163 -4.1268392 -4.1165714 -4.1220846 -4.1444783 -4.1537604 -4.1508651 -4.1570516]]...]
INFO - root - 2017-12-05 16:47:46.035203: step 23810, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 78h:13m:17s remains)
INFO - root - 2017-12-05 16:47:55.013070: step 23820, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 77h:34m:48s remains)
INFO - root - 2017-12-05 16:48:03.878650: step 23830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 75h:25m:30s remains)
INFO - root - 2017-12-05 16:48:12.963897: step 23840, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 79h:37m:51s remains)
INFO - root - 2017-12-05 16:48:22.172954: step 23850, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 77h:47m:03s remains)
INFO - root - 2017-12-05 16:48:31.273633: step 23860, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 75h:33m:02s remains)
INFO - root - 2017-12-05 16:48:40.371651: step 23870, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 78h:43m:35s remains)
INFO - root - 2017-12-05 16:48:49.501633: step 23880, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.927 sec/batch; 79h:27m:52s remains)
INFO - root - 2017-12-05 16:48:58.633163: step 23890, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 75h:34m:08s remains)
INFO - root - 2017-12-05 16:49:07.542176: step 23900, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:32m:08s remains)
2017-12-05 16:49:08.258318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2948351 -4.2549772 -4.2066784 -4.1614976 -4.12443 -4.0978408 -4.0922251 -4.1210394 -4.1727934 -4.2246757 -4.2538767 -4.2581348 -4.2552643 -4.2417355 -4.2168608][-4.2918591 -4.2490115 -4.1966982 -4.1422768 -4.0937471 -4.0552654 -4.0408416 -4.076376 -4.1426764 -4.2070093 -4.2425132 -4.2413726 -4.232336 -4.2162595 -4.18917][-4.288372 -4.2461219 -4.1904049 -4.1277356 -4.0639172 -4.0093126 -3.9859381 -4.02459 -4.0992002 -4.1716633 -4.2074308 -4.2003093 -4.1874919 -4.1701932 -4.1412191][-4.28511 -4.242702 -4.1837173 -4.1101017 -4.0287 -3.958744 -3.9314735 -3.9729218 -4.0471792 -4.1186814 -4.1516342 -4.1426783 -4.1317563 -4.115725 -4.0854888][-4.2830338 -4.2407331 -4.1765885 -4.0900321 -3.9929032 -3.9059153 -3.8731842 -3.9155483 -3.994092 -4.0582881 -4.0893264 -4.0839448 -4.0809526 -4.0697975 -4.0449352][-4.2826581 -4.2405553 -4.1729312 -4.078167 -3.9710529 -3.8683212 -3.8235166 -3.8678498 -3.9518404 -4.0111485 -4.0405445 -4.0394158 -4.042974 -4.0374351 -4.0224066][-4.283711 -4.2416348 -4.1735229 -4.0798688 -3.975781 -3.8739414 -3.8326094 -3.8884926 -3.9740012 -4.0255494 -4.0482788 -4.04729 -4.0472693 -4.0394769 -4.0266433][-4.2868924 -4.2480192 -4.1860547 -4.1063752 -4.0193014 -3.9370966 -3.9128852 -3.9709656 -4.0409 -4.0738845 -4.0851822 -4.0796227 -4.0739908 -4.0601726 -4.0407028][-4.291183 -4.2585006 -4.2068386 -4.1451721 -4.0810108 -4.0168653 -3.9966245 -4.0457954 -4.0981178 -4.1121678 -4.1098523 -4.102684 -4.0986171 -4.0791388 -4.0499344][-4.2965527 -4.2679696 -4.22425 -4.1745977 -4.124774 -4.0704279 -4.0512691 -4.0932055 -4.1377797 -4.1431437 -4.1324887 -4.1283922 -4.1310925 -4.1153197 -4.0865154][-4.3049974 -4.2790504 -4.2365766 -4.1870427 -4.138999 -4.0930309 -4.0798697 -4.1185803 -4.1607647 -4.1663008 -4.154428 -4.1537814 -4.1645145 -4.1569419 -4.1394253][-4.31187 -4.2854886 -4.244987 -4.1937537 -4.144516 -4.108747 -4.1069365 -4.1430917 -4.1774564 -4.1794152 -4.1646175 -4.15971 -4.1712189 -4.1736755 -4.1694188][-4.31297 -4.2872334 -4.2472515 -4.1976657 -4.1489644 -4.1219049 -4.128201 -4.1573048 -4.178844 -4.176 -4.1574073 -4.1485162 -4.1599 -4.17041 -4.17525][-4.3085442 -4.2839756 -4.2458935 -4.1987138 -4.1527219 -4.13089 -4.1375279 -4.1584015 -4.1735582 -4.1709614 -4.1565228 -4.1451306 -4.1499186 -4.1576548 -4.1605716][-4.3083067 -4.2848983 -4.2513151 -4.209692 -4.1702161 -4.1526065 -4.1576729 -4.1740179 -4.1895509 -4.1911917 -4.1803403 -4.1665053 -4.1621327 -4.16296 -4.1632538]]...]
INFO - root - 2017-12-05 16:49:17.443364: step 23910, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 78h:15m:17s remains)
INFO - root - 2017-12-05 16:49:26.569432: step 23920, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 79h:26m:40s remains)
INFO - root - 2017-12-05 16:49:35.576848: step 23930, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 76h:26m:07s remains)
INFO - root - 2017-12-05 16:49:44.537615: step 23940, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 77h:43m:40s remains)
INFO - root - 2017-12-05 16:49:53.539000: step 23950, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 77h:29m:48s remains)
INFO - root - 2017-12-05 16:50:02.548490: step 23960, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 79h:20m:15s remains)
INFO - root - 2017-12-05 16:50:11.539999: step 23970, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 78h:53m:24s remains)
INFO - root - 2017-12-05 16:50:20.692958: step 23980, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 76h:57m:39s remains)
INFO - root - 2017-12-05 16:50:29.641998: step 23990, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 75h:41m:12s remains)
INFO - root - 2017-12-05 16:50:38.853841: step 24000, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 80h:20m:51s remains)
2017-12-05 16:50:39.627787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1672797 -4.1586347 -4.158618 -4.1657104 -4.1757741 -4.1774445 -4.1666808 -4.1340442 -4.1108718 -4.1194639 -4.1478772 -4.1750522 -4.2050967 -4.2238154 -4.19809][-4.161839 -4.1504803 -4.1493464 -4.1534786 -4.1644859 -4.1673546 -4.1561618 -4.1240578 -4.1006732 -4.1113853 -4.1414618 -4.1716304 -4.2038751 -4.2197962 -4.1888809][-4.162128 -4.1514835 -4.1485906 -4.1480236 -4.1547017 -4.1586189 -4.1512065 -4.1253691 -4.1091266 -4.1235895 -4.1560392 -4.1863632 -4.2136025 -4.2211304 -4.1893644][-4.1442165 -4.1349206 -4.1369886 -4.1396303 -4.1430459 -4.143754 -4.1338115 -4.1100249 -4.1091151 -4.1397867 -4.1810675 -4.2133675 -4.22931 -4.2281213 -4.1971116][-4.1194806 -4.113883 -4.1267962 -4.13947 -4.1419554 -4.1322551 -4.1053348 -4.0740466 -4.090116 -4.1441813 -4.1983662 -4.2319117 -4.2398796 -4.2313547 -4.1970973][-4.0930142 -4.0891166 -4.1150274 -4.1449938 -4.151906 -4.1246791 -4.0673351 -4.0216861 -4.0547881 -4.13484 -4.2052922 -4.2397432 -4.242702 -4.2318387 -4.1968656][-4.0778408 -4.0750332 -4.1123891 -4.1526632 -4.15428 -4.101222 -4.0075316 -3.9434297 -4.0051503 -4.1181235 -4.2038832 -4.2406826 -4.2430024 -4.2344823 -4.2016063][-4.0954819 -4.1005177 -4.14105 -4.1755033 -4.1569734 -4.0748839 -3.954283 -3.883008 -3.9765275 -4.1108375 -4.2016087 -4.2363248 -4.2387986 -4.2308884 -4.1992188][-4.1327553 -4.1445179 -4.1812377 -4.2042389 -4.1745667 -4.0934596 -3.9906368 -3.9445496 -4.0281024 -4.1386533 -4.2058239 -4.22626 -4.2279863 -4.2268791 -4.2029533][-4.1658239 -4.181756 -4.2106328 -4.2254887 -4.2022848 -4.1429892 -4.0775886 -4.0552068 -4.1058478 -4.1743588 -4.2136483 -4.2206254 -4.223206 -4.2264156 -4.2056623][-4.1815906 -4.1987786 -4.2220478 -4.235136 -4.2209187 -4.1776495 -4.1355019 -4.1209373 -4.1465073 -4.1861439 -4.210155 -4.2136359 -4.2206306 -4.2248306 -4.2054133][-4.1929579 -4.2072172 -4.226665 -4.2376237 -4.2292595 -4.1937461 -4.1629806 -4.1529837 -4.1671357 -4.1916947 -4.20688 -4.2085581 -4.2170095 -4.2215838 -4.2028522][-4.2090564 -4.2228556 -4.237237 -4.2434549 -4.2350941 -4.2009931 -4.1758046 -4.1711164 -4.1844258 -4.2041874 -4.2136207 -4.2126503 -4.21638 -4.2165642 -4.2006392][-4.22411 -4.2342892 -4.2401438 -4.240747 -4.2307587 -4.2024817 -4.1836667 -4.1863866 -4.2045374 -4.2243552 -4.2306333 -4.2287827 -4.2264066 -4.2227764 -4.211699][-4.2419195 -4.2424369 -4.2416153 -4.2392607 -4.2310514 -4.21418 -4.2045741 -4.2130542 -4.2331357 -4.2509632 -4.2565494 -4.2553544 -4.2541614 -4.2516055 -4.2414665]]...]
INFO - root - 2017-12-05 16:50:48.825070: step 24010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 79h:11m:10s remains)
INFO - root - 2017-12-05 16:50:57.927372: step 24020, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.920 sec/batch; 78h:47m:47s remains)
INFO - root - 2017-12-05 16:51:07.109209: step 24030, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 76h:27m:44s remains)
INFO - root - 2017-12-05 16:51:16.292909: step 24040, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 79h:32m:02s remains)
INFO - root - 2017-12-05 16:51:25.495438: step 24050, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 78h:22m:04s remains)
INFO - root - 2017-12-05 16:51:34.701801: step 24060, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 78h:23m:41s remains)
INFO - root - 2017-12-05 16:51:43.574148: step 24070, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 75h:46m:40s remains)
INFO - root - 2017-12-05 16:51:52.614637: step 24080, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 70h:04m:30s remains)
INFO - root - 2017-12-05 16:52:01.779964: step 24090, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.971 sec/batch; 83h:13m:25s remains)
INFO - root - 2017-12-05 16:52:10.931126: step 24100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 77h:52m:43s remains)
2017-12-05 16:52:11.823904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2967219 -4.293891 -4.294837 -4.2955313 -4.2998638 -4.3028421 -4.2913551 -4.2669754 -4.24866 -4.2488751 -4.26211 -4.2829938 -4.3099742 -4.3260517 -4.3228579][-4.301301 -4.3064275 -4.3095465 -4.3107185 -4.3131571 -4.30674 -4.277802 -4.2328343 -4.198863 -4.1949391 -4.2118697 -4.243906 -4.2865171 -4.3147273 -4.3191872][-4.294764 -4.3097739 -4.3172388 -4.3193307 -4.3185539 -4.3014908 -4.2547131 -4.1899557 -4.1426907 -4.13559 -4.1561952 -4.19979 -4.260293 -4.3026409 -4.31387][-4.2724233 -4.3011794 -4.3154216 -4.3188434 -4.3153176 -4.2897916 -4.229557 -4.151525 -4.0959969 -4.0851378 -4.1049643 -4.1592255 -4.2365079 -4.2919493 -4.3092856][-4.2386274 -4.2804136 -4.303638 -4.310277 -4.304975 -4.27522 -4.2103419 -4.1302485 -4.0739942 -4.0576444 -4.0724316 -4.1300397 -4.2124891 -4.2734571 -4.2980585][-4.2127881 -4.2592483 -4.2849226 -4.2910151 -4.2852936 -4.2589564 -4.2026157 -4.1335921 -4.0852723 -4.0645323 -4.070631 -4.117239 -4.1848617 -4.2382936 -4.2680531][-4.2111497 -4.2555709 -4.2776365 -4.28073 -4.2756634 -4.2571664 -4.2156215 -4.1620736 -4.1256666 -4.1049552 -4.1040969 -4.1304636 -4.1695566 -4.2047958 -4.232285][-4.2355766 -4.2717304 -4.28705 -4.2850661 -4.2778668 -4.2657571 -4.2370844 -4.1971049 -4.1687484 -4.1504922 -4.1468883 -4.1563091 -4.1712375 -4.1898966 -4.2137761][-4.2636518 -4.2913995 -4.3008647 -4.2954783 -4.2870131 -4.2774844 -4.2571583 -4.2289929 -4.2096634 -4.1984959 -4.1975183 -4.1997123 -4.2021818 -4.2105541 -4.2287016][-4.2845821 -4.3062539 -4.3119149 -4.3058319 -4.2976594 -4.2898211 -4.2756934 -4.2568059 -4.245348 -4.2412829 -4.2433677 -4.2442703 -4.2455964 -4.251482 -4.2644215][-4.3003616 -4.317565 -4.321579 -4.31586 -4.3097019 -4.3046856 -4.29606 -4.2834921 -4.2758579 -4.2727222 -4.2726951 -4.2734065 -4.2774038 -4.2856226 -4.2969975][-4.3128376 -4.3255544 -4.3285828 -4.3245625 -4.3210883 -4.3189588 -4.3144431 -4.3067179 -4.3005753 -4.2950315 -4.2910738 -4.2909713 -4.2973018 -4.3075514 -4.3189917][-4.3194666 -4.3275728 -4.3297129 -4.3277617 -4.3270535 -4.3272152 -4.3251534 -4.3193312 -4.3121085 -4.3040342 -4.2983108 -4.2978663 -4.3039742 -4.3143282 -4.3266544][-4.3176026 -4.3230209 -4.3251786 -4.3256016 -4.326756 -4.3276415 -4.3256121 -4.318841 -4.3100014 -4.3018427 -4.2967319 -4.2959485 -4.30058 -4.3100591 -4.3231449][-4.31146 -4.3159409 -4.3189874 -4.3212934 -4.3236327 -4.3240132 -4.3201871 -4.3116469 -4.3029737 -4.29773 -4.2950487 -4.2937827 -4.2969961 -4.3054686 -4.3176484]]...]
INFO - root - 2017-12-05 16:52:20.845768: step 24110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 77h:43m:49s remains)
INFO - root - 2017-12-05 16:52:30.131323: step 24120, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 80h:28m:17s remains)
INFO - root - 2017-12-05 16:52:39.334396: step 24130, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 78h:27m:45s remains)
INFO - root - 2017-12-05 16:52:48.424489: step 24140, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 72h:13m:30s remains)
INFO - root - 2017-12-05 16:52:57.589194: step 24150, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 76h:22m:21s remains)
INFO - root - 2017-12-05 16:53:06.691892: step 24160, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 80h:01m:52s remains)
INFO - root - 2017-12-05 16:53:15.913163: step 24170, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 78h:36m:20s remains)
INFO - root - 2017-12-05 16:53:24.908651: step 24180, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 75h:44m:30s remains)
INFO - root - 2017-12-05 16:53:34.105882: step 24190, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 81h:00m:15s remains)
INFO - root - 2017-12-05 16:53:43.348620: step 24200, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 74h:51m:44s remains)
2017-12-05 16:53:44.186360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2282267 -4.216928 -4.2246985 -4.248538 -4.26518 -4.2716284 -4.27375 -4.2768526 -4.2854953 -4.2974024 -4.3026834 -4.2958364 -4.2871404 -4.2859025 -4.2769337][-4.1732864 -4.1711116 -4.1885977 -4.218945 -4.2432766 -4.2578368 -4.2623672 -4.2648287 -4.2710762 -4.2789121 -4.2791939 -4.2681112 -4.2600508 -4.2615514 -4.2525668][-4.1343822 -4.1447897 -4.1728749 -4.2050481 -4.2326145 -4.2528095 -4.2596011 -4.2602472 -4.26301 -4.2669435 -4.2611113 -4.2458677 -4.2387881 -4.2416472 -4.232976][-4.1559939 -4.170064 -4.1950984 -4.2175221 -4.2337418 -4.2472634 -4.2510815 -4.2496243 -4.2511086 -4.2526259 -4.2411232 -4.2260962 -4.2253742 -4.2309875 -4.2227159][-4.2049026 -4.2143507 -4.2269158 -4.22908 -4.2223926 -4.222065 -4.2227097 -4.2215228 -4.2255116 -4.2297206 -4.21802 -4.2096148 -4.2196131 -4.2311139 -4.2281342][-4.2292366 -4.2283244 -4.2263508 -4.2048841 -4.1739807 -4.1585054 -4.1591454 -4.1637025 -4.1741724 -4.1833959 -4.177072 -4.18178 -4.2067389 -4.2303343 -4.2377939][-4.2245412 -4.2117062 -4.1930447 -4.1438818 -4.086144 -4.0557828 -4.0598593 -4.0774484 -4.1000547 -4.1218653 -4.1329365 -4.1552334 -4.19232 -4.2286482 -4.2464309][-4.20151 -4.18111 -4.1515555 -4.0852494 -4.0169568 -3.9835167 -3.9934661 -4.0198374 -4.0478959 -4.0820379 -4.114254 -4.1525812 -4.1956177 -4.2369037 -4.258966][-4.1659966 -4.1481609 -4.1238689 -4.0690846 -4.02224 -4.003499 -4.0144029 -4.0358725 -4.0577507 -4.0945191 -4.1372337 -4.1800442 -4.2205234 -4.2569709 -4.2746034][-4.1385536 -4.1358242 -4.1338744 -4.11166 -4.0993023 -4.0943813 -4.0988293 -4.108274 -4.1167011 -4.1435766 -4.1814537 -4.21562 -4.2440805 -4.27063 -4.2819309][-4.14816 -4.1588964 -4.1752462 -4.17906 -4.1852584 -4.1875954 -4.1857872 -4.1861815 -4.1857677 -4.2015133 -4.2283325 -4.24922 -4.2636161 -4.27862 -4.283144][-4.1740823 -4.1876335 -4.2104497 -4.225224 -4.2355752 -4.2403979 -4.2359662 -4.2335644 -4.2313056 -4.239471 -4.2557282 -4.2659793 -4.2709146 -4.2768559 -4.276577][-4.1777287 -4.1887255 -4.21228 -4.2299962 -4.2374053 -4.2418032 -4.2409215 -4.2441325 -4.24595 -4.2491517 -4.2546816 -4.2558131 -4.2568526 -4.2615795 -4.2599306][-4.1617975 -4.1677127 -4.1866446 -4.2012196 -4.2055874 -4.2111454 -4.2177095 -4.2280731 -4.2340636 -4.2349687 -4.2308645 -4.2241426 -4.2247806 -4.2329712 -4.233902][-4.145443 -4.1458225 -4.1597624 -4.1740241 -4.1832705 -4.1919727 -4.2006388 -4.2111888 -4.2178307 -4.217268 -4.2094488 -4.197886 -4.1983881 -4.2095962 -4.2158747]]...]
INFO - root - 2017-12-05 16:53:53.209753: step 24210, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 79h:41m:12s remains)
INFO - root - 2017-12-05 16:54:02.494509: step 24220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 80h:56m:51s remains)
INFO - root - 2017-12-05 16:54:11.739397: step 24230, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 80h:00m:46s remains)
INFO - root - 2017-12-05 16:54:20.727142: step 24240, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 79h:23m:15s remains)
INFO - root - 2017-12-05 16:54:29.784875: step 24250, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 76h:38m:29s remains)
INFO - root - 2017-12-05 16:54:38.861956: step 24260, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 78h:38m:36s remains)
INFO - root - 2017-12-05 16:54:48.054919: step 24270, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 78h:56m:43s remains)
INFO - root - 2017-12-05 16:54:57.044569: step 24280, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 76h:39m:33s remains)
INFO - root - 2017-12-05 16:55:06.261285: step 24290, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.905 sec/batch; 77h:28m:52s remains)
INFO - root - 2017-12-05 16:55:15.432082: step 24300, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 76h:10m:17s remains)
2017-12-05 16:55:16.246005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3082066 -4.284174 -4.2678585 -4.264226 -4.2639184 -4.2525973 -4.2254248 -4.2012095 -4.1947346 -4.2071285 -4.2302809 -4.2575154 -4.2921262 -4.3218803 -4.33435][-4.2984376 -4.2768 -4.2642245 -4.2603207 -4.2566981 -4.2363248 -4.1892972 -4.1453867 -4.1305285 -4.1508331 -4.1886687 -4.2296987 -4.2763133 -4.3182254 -4.33655][-4.2892766 -4.2750235 -4.2637053 -4.2541022 -4.2417088 -4.2066412 -4.1391578 -4.0732145 -4.05192 -4.0850229 -4.1429815 -4.2019138 -4.2630372 -4.3170643 -4.3390241][-4.278542 -4.2735424 -4.2613239 -4.2449961 -4.2246838 -4.1770773 -4.0941825 -4.0096979 -3.9820812 -4.0246067 -4.0994973 -4.1775184 -4.2507644 -4.3118095 -4.33797][-4.2696772 -4.2716603 -4.2610469 -4.2400517 -4.2132492 -4.156445 -4.05817 -3.9575682 -3.9213314 -3.9691706 -4.0595236 -4.1553612 -4.2373204 -4.3022461 -4.3327718][-4.2687912 -4.2736478 -4.2615395 -4.2334027 -4.1983671 -4.1310444 -4.0190487 -3.9090767 -3.8736882 -3.9238036 -4.0244107 -4.1333961 -4.2248077 -4.2941532 -4.3278508][-4.2773533 -4.2826133 -4.26624 -4.2297449 -4.1877313 -4.1134534 -3.9945602 -3.8888555 -3.8631313 -3.9132879 -4.012536 -4.1217394 -4.216692 -4.2881546 -4.3250327][-4.2765617 -4.2828112 -4.2651882 -4.2229195 -4.1744533 -4.1017656 -3.9887469 -3.9009624 -3.8899162 -3.9399159 -4.0311117 -4.1311398 -4.2217574 -4.2893214 -4.325716][-4.2563381 -4.2635455 -4.2470922 -4.2003608 -4.143651 -4.0751657 -3.9732988 -3.8976417 -3.8932271 -3.9483435 -4.0429564 -4.1431303 -4.2315617 -4.2949967 -4.3289413][-4.2206655 -4.2270622 -4.2120624 -4.1631713 -4.102138 -4.0395274 -3.9482124 -3.8729792 -3.8677731 -3.9355109 -4.0488076 -4.1573319 -4.2453437 -4.3042412 -4.3345661][-4.2058539 -4.2053 -4.1883345 -4.1443281 -4.0899425 -4.0369406 -3.9592171 -3.8859296 -3.8794837 -3.9488769 -4.0646162 -4.1711121 -4.2563705 -4.3125296 -4.3397441][-4.1964688 -4.1857758 -4.1702418 -4.1419129 -4.1069984 -4.0679479 -4.0068536 -3.9433758 -3.9327421 -3.9883323 -4.0843558 -4.17751 -4.2567153 -4.3118057 -4.3395724][-4.1747742 -4.1523438 -4.1381164 -4.1296258 -4.12205 -4.1049814 -4.0645437 -4.0148726 -4.0038943 -4.0466604 -4.1216989 -4.1957283 -4.2618232 -4.3103805 -4.3371038][-4.1637106 -4.1354647 -4.1217556 -4.1255541 -4.133687 -4.129446 -4.1000886 -4.0631757 -4.0610905 -4.1048937 -4.1716967 -4.2325583 -4.282352 -4.3186612 -4.3378396][-4.1524925 -4.1316223 -4.1200447 -4.1228118 -4.1311932 -4.1287127 -4.1049719 -4.0777712 -4.0898314 -4.1425033 -4.2102637 -4.264595 -4.3041387 -4.3297544 -4.3405461]]...]
INFO - root - 2017-12-05 16:55:25.284460: step 24310, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 80h:26m:06s remains)
INFO - root - 2017-12-05 16:55:34.392744: step 24320, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 77h:31m:17s remains)
INFO - root - 2017-12-05 16:55:43.450630: step 24330, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 77h:17m:56s remains)
INFO - root - 2017-12-05 16:55:52.580107: step 24340, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 79h:34m:35s remains)
INFO - root - 2017-12-05 16:56:01.627372: step 24350, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 76h:22m:00s remains)
INFO - root - 2017-12-05 16:56:10.726449: step 24360, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.807 sec/batch; 69h:04m:20s remains)
INFO - root - 2017-12-05 16:56:19.927409: step 24370, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 74h:04m:00s remains)
INFO - root - 2017-12-05 16:56:28.946423: step 24380, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 75h:57m:58s remains)
INFO - root - 2017-12-05 16:56:37.919202: step 24390, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.921 sec/batch; 78h:49m:32s remains)
INFO - root - 2017-12-05 16:56:47.181583: step 24400, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 80h:48m:29s remains)
2017-12-05 16:56:47.989430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.305872 -4.3044581 -4.301506 -4.30089 -4.3041339 -4.3065076 -4.3054147 -4.3063188 -4.307508 -4.3081412 -4.3067784 -4.3062949 -4.3064404 -4.3038673 -4.2986479][-4.3039417 -4.307641 -4.307385 -4.3072743 -4.3114858 -4.3145175 -4.3146033 -4.3172183 -4.318635 -4.3196154 -4.31841 -4.3169107 -4.315793 -4.3127046 -4.3072329][-4.2876883 -4.2914653 -4.290916 -4.2890835 -4.2902093 -4.2901397 -4.288228 -4.2937627 -4.2977629 -4.3028226 -4.3083363 -4.3114505 -4.31245 -4.3113761 -4.3068433][-4.2538524 -4.2541962 -4.2515259 -4.2450533 -4.2387085 -4.2305665 -4.2214561 -4.2284651 -4.2380929 -4.2529311 -4.272078 -4.2872062 -4.2978258 -4.304678 -4.3058062][-4.2031579 -4.2022147 -4.1964355 -4.1821537 -4.1622128 -4.1377444 -4.1146879 -4.1186957 -4.1396289 -4.1717062 -4.2088461 -4.235146 -4.2524672 -4.2678466 -4.2784724][-4.1436214 -4.1450496 -4.1385937 -4.11632 -4.0775766 -4.0303221 -3.9861798 -3.988992 -4.0293884 -4.0814795 -4.1282268 -4.1580181 -4.1782918 -4.1980081 -4.2156725][-4.0932565 -4.0990853 -4.0887437 -4.0534973 -3.9946151 -3.9188185 -3.847302 -3.8522496 -3.9228866 -4.0011635 -4.0572648 -4.0901957 -4.1109262 -4.1291223 -4.1444645][-4.0451531 -4.0667524 -4.0637341 -4.02795 -3.9601715 -3.8704114 -3.7851968 -3.7883844 -3.87316 -3.9617953 -4.02021 -4.0525837 -4.0726895 -4.0887003 -4.0997925][-3.9997573 -4.0450716 -4.0675898 -4.0511642 -3.9990478 -3.9335032 -3.8798585 -3.8804283 -3.9315543 -3.9888849 -4.0299878 -4.0525041 -4.0673156 -4.0809541 -4.0888486][-3.9804006 -4.0398436 -4.0790982 -4.0801668 -4.0502553 -4.0220675 -4.014801 -4.0254846 -4.0492764 -4.0734186 -4.0884762 -4.0952282 -4.1018672 -4.1084523 -4.1081519][-4.0043097 -4.0518355 -4.0887651 -4.0964689 -4.08183 -4.0762677 -4.0910869 -4.1081896 -4.1222992 -4.1384726 -4.1535673 -4.16088 -4.1657166 -4.1644273 -4.1522675][-4.0760565 -4.0960722 -4.1161056 -4.1198816 -4.1078715 -4.0993938 -4.1069803 -4.1167431 -4.1282406 -4.1532612 -4.1848636 -4.2062087 -4.2130079 -4.2092414 -4.1950541][-4.1752234 -4.1758556 -4.1766682 -4.1710925 -4.1551957 -4.13216 -4.1159883 -4.1073389 -4.1157093 -4.1477919 -4.1913509 -4.2207727 -4.2308435 -4.2347951 -4.2319059][-4.2595468 -4.2585759 -4.2504787 -4.2369452 -4.2144356 -4.1768479 -4.136972 -4.1109252 -4.115396 -4.1482162 -4.1881495 -4.2192922 -4.2377205 -4.2513947 -4.2589517][-4.2937293 -4.298315 -4.2890387 -4.2725081 -4.24862 -4.2139812 -4.17482 -4.1468449 -4.1445756 -4.1641803 -4.1918716 -4.2192531 -4.2412839 -4.2569389 -4.2656641]]...]
INFO - root - 2017-12-05 16:56:57.108437: step 24410, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 77h:48m:54s remains)
INFO - root - 2017-12-05 16:57:06.258483: step 24420, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 79h:39m:00s remains)
INFO - root - 2017-12-05 16:57:15.401531: step 24430, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 76h:52m:56s remains)
INFO - root - 2017-12-05 16:57:24.493425: step 24440, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 76h:56m:24s remains)
INFO - root - 2017-12-05 16:57:33.667980: step 24450, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 77h:58m:29s remains)
INFO - root - 2017-12-05 16:57:42.872806: step 24460, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 79h:25m:21s remains)
INFO - root - 2017-12-05 16:57:51.950999: step 24470, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 77h:04m:26s remains)
INFO - root - 2017-12-05 16:58:01.115435: step 24480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 74h:02m:30s remains)
INFO - root - 2017-12-05 16:58:10.188962: step 24490, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 78h:44m:25s remains)
INFO - root - 2017-12-05 16:58:19.283280: step 24500, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 76h:27m:45s remains)
2017-12-05 16:58:20.107322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14795 -4.1440611 -4.1364474 -4.1299791 -4.127099 -4.1375713 -4.1423769 -4.1365137 -4.1234617 -4.1016951 -4.0710306 -4.0593047 -4.0609951 -4.0630941 -4.0928373][-4.1113019 -4.096765 -4.0805516 -4.0698276 -4.0693445 -4.0878453 -4.0973034 -4.0952897 -4.0867519 -4.068851 -4.0388155 -4.0229235 -4.0184431 -4.0144849 -4.0482168][-4.1005421 -4.0801353 -4.0592623 -4.046608 -4.05473 -4.0829911 -4.1028342 -4.1102815 -4.111773 -4.10106 -4.0788531 -4.0662708 -4.0607429 -4.0543928 -4.081264][-4.1059933 -4.0862627 -4.0662346 -4.0528255 -4.0622473 -4.0921435 -4.1127172 -4.1283617 -4.1372523 -4.1406541 -4.1335626 -4.1345682 -4.1421475 -4.1482458 -4.1728888][-4.1228671 -4.1065993 -4.0873032 -4.0736141 -4.07735 -4.095746 -4.1067729 -4.1187811 -4.1262479 -4.1361041 -4.1444511 -4.1621771 -4.193614 -4.2204657 -4.2490916][-4.1336622 -4.1091671 -4.0794625 -4.0568228 -4.0504766 -4.0572357 -4.0574756 -4.0618811 -4.0700979 -4.0849094 -4.1044374 -4.1437154 -4.19684 -4.239068 -4.276988][-4.1272345 -4.0835142 -4.0385609 -4.0090961 -3.9911551 -3.9839106 -3.9706006 -3.9635882 -3.9806459 -4.0138469 -4.0497403 -4.1123853 -4.1829348 -4.2319908 -4.2750821][-4.1024961 -4.0469742 -3.9958587 -3.9640675 -3.9345312 -3.9117794 -3.8862736 -3.873913 -3.9020648 -3.949357 -3.9914777 -4.0629373 -4.139586 -4.1898112 -4.23425][-4.0796714 -4.0345316 -3.9947236 -3.9683466 -3.9359198 -3.9012501 -3.8713183 -3.8584204 -3.8896754 -3.9309368 -3.9583788 -4.0156994 -4.0835853 -4.1264858 -4.1714354][-4.0645895 -4.0360703 -4.01278 -4.0027518 -3.9847682 -3.9600546 -3.9369411 -3.9207978 -3.9401503 -3.95849 -3.9566963 -3.9822185 -4.0259156 -4.0553761 -4.1030326][-4.0689154 -4.0536389 -4.0407891 -4.0451689 -4.0456967 -4.0436282 -4.0402584 -4.0242538 -4.0252819 -4.0161767 -3.986999 -3.9801707 -3.99472 -4.0082927 -4.0560193][-4.0967178 -4.0882339 -4.0781608 -4.08378 -4.0959988 -4.11344 -4.125917 -4.1146502 -4.1031141 -4.0792079 -4.0409851 -4.0127482 -4.0029645 -4.002017 -4.04387][-4.1409245 -4.1327314 -4.1230273 -4.1290154 -4.1458621 -4.1731162 -4.1965289 -4.1938391 -4.1832047 -4.157948 -4.1221867 -4.08479 -4.0570183 -4.0462565 -4.0764742][-4.200223 -4.1962357 -4.1924195 -4.2002363 -4.2170796 -4.2414503 -4.2643733 -4.2686319 -4.2628818 -4.2433333 -4.2135105 -4.176785 -4.1457982 -4.1340265 -4.1553888][-4.25922 -4.2611294 -4.263711 -4.2740645 -4.2899771 -4.3092771 -4.3251572 -4.3311357 -4.3268967 -4.308507 -4.2845154 -4.2562041 -4.2343826 -4.2292686 -4.2441573]]...]
INFO - root - 2017-12-05 16:58:29.229051: step 24510, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 79h:02m:46s remains)
INFO - root - 2017-12-05 16:58:38.459757: step 24520, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 77h:38m:09s remains)
INFO - root - 2017-12-05 16:58:47.669784: step 24530, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 79h:40m:08s remains)
INFO - root - 2017-12-05 16:58:56.915688: step 24540, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 78h:25m:54s remains)
INFO - root - 2017-12-05 16:59:06.114366: step 24550, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 76h:52m:24s remains)
INFO - root - 2017-12-05 16:59:15.032668: step 24560, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 79h:11m:58s remains)
INFO - root - 2017-12-05 16:59:24.283871: step 24570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 79h:16m:03s remains)
INFO - root - 2017-12-05 16:59:33.328987: step 24580, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 75h:04m:41s remains)
INFO - root - 2017-12-05 16:59:42.371201: step 24590, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 77h:22m:50s remains)
INFO - root - 2017-12-05 16:59:51.539034: step 24600, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 79h:47m:43s remains)
2017-12-05 16:59:52.294769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3018775 -4.2918487 -4.28309 -4.277844 -4.2774425 -4.2824383 -4.2906704 -4.3012896 -4.3110452 -4.3177886 -4.3190861 -4.3140883 -4.3091841 -4.30819 -4.3113112][-4.2805967 -4.2615857 -4.2435436 -4.2290893 -4.2257442 -4.23279 -4.2450857 -4.2604189 -4.2776871 -4.2923131 -4.2981205 -4.2934022 -4.2859259 -4.2799134 -4.2789612][-4.2564859 -4.2262053 -4.19419 -4.1666193 -4.1601472 -4.1674948 -4.1791062 -4.19839 -4.2258911 -4.2542377 -4.2687883 -4.2680626 -4.2602134 -4.2496371 -4.241415][-4.2349086 -4.19435 -4.1478076 -4.1074615 -4.0958414 -4.0976825 -4.1056128 -4.1272249 -4.1644664 -4.2058568 -4.2294765 -4.23433 -4.2312369 -4.2209911 -4.2074094][-4.2187243 -4.1683683 -4.1088 -4.0579624 -4.037581 -4.0270872 -4.0232129 -4.0421453 -4.0900154 -4.1459265 -4.1775236 -4.1898246 -4.1979804 -4.1938272 -4.1829047][-4.2149296 -4.1615872 -4.0925 -4.031518 -3.9973733 -3.964978 -3.935143 -3.9433894 -4.0054979 -4.0762548 -4.1162305 -4.1362214 -4.1531858 -4.1613989 -4.1625681][-4.2192092 -4.1683588 -4.0954356 -4.0229445 -3.9678695 -3.9030168 -3.8359416 -3.8319907 -3.9102173 -3.9956121 -4.0366917 -4.0554209 -4.0808582 -4.1087346 -4.1280026][-4.2274208 -4.1829128 -4.1097593 -4.0290227 -3.9567616 -3.8642702 -3.7668114 -3.7579794 -3.8483679 -3.9351976 -3.961776 -3.9619653 -3.9920592 -4.0407028 -4.0815907][-4.2373042 -4.2013559 -4.1374755 -4.0640616 -3.994875 -3.9066403 -3.8211682 -3.811094 -3.8863518 -3.9474583 -3.9453044 -3.9144578 -3.9300444 -3.9804878 -4.029211][-4.2433562 -4.2152071 -4.1660357 -4.1144061 -4.0662947 -4.0058451 -3.9520915 -3.9419408 -3.9857786 -4.0151691 -3.9890959 -3.9355786 -3.927515 -3.9603832 -4.0004849][-4.2448516 -4.221632 -4.1864214 -4.1537633 -4.1249208 -4.0882173 -4.0564289 -4.0416393 -4.05727 -4.0737758 -4.0509295 -4.0040488 -3.989557 -4.0017657 -4.020298][-4.2437897 -4.2199588 -4.1917062 -4.1720233 -4.1592112 -4.1407418 -4.1228423 -4.1068211 -4.1099453 -4.1250319 -4.118525 -4.0972509 -4.0890627 -4.0856552 -4.0786071][-4.2443886 -4.2178516 -4.1918926 -4.1790051 -4.1787181 -4.173892 -4.1671238 -4.1565714 -4.157516 -4.1726842 -4.1785192 -4.1751122 -4.1681194 -4.1556549 -4.1398487][-4.2541924 -4.2262392 -4.2019148 -4.1923485 -4.196764 -4.1981034 -4.1968441 -4.1941981 -4.1991353 -4.2112761 -4.219604 -4.2244163 -4.2224059 -4.211832 -4.2002926][-4.2706418 -4.2450562 -4.2245092 -4.2181749 -4.2232013 -4.227087 -4.2289395 -4.2306705 -4.2402539 -4.2500043 -4.2552571 -4.2604156 -4.2620311 -4.25744 -4.2518587]]...]
INFO - root - 2017-12-05 17:00:01.264318: step 24610, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 76h:49m:59s remains)
INFO - root - 2017-12-05 17:00:10.346232: step 24620, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 77h:34m:06s remains)
INFO - root - 2017-12-05 17:00:19.600566: step 24630, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.935 sec/batch; 79h:56m:00s remains)
INFO - root - 2017-12-05 17:00:28.602159: step 24640, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 79h:00m:27s remains)
INFO - root - 2017-12-05 17:00:37.679327: step 24650, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 76h:49m:35s remains)
INFO - root - 2017-12-05 17:00:46.830999: step 24660, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 76h:42m:18s remains)
INFO - root - 2017-12-05 17:00:55.732571: step 24670, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.906 sec/batch; 77h:29m:41s remains)
INFO - root - 2017-12-05 17:01:04.865301: step 24680, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 78h:09m:33s remains)
INFO - root - 2017-12-05 17:01:13.963694: step 24690, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.887 sec/batch; 75h:47m:53s remains)
INFO - root - 2017-12-05 17:01:23.144375: step 24700, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 79h:39m:01s remains)
2017-12-05 17:01:23.950455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3204694 -4.3249116 -4.324255 -4.3234997 -4.3211756 -4.3173723 -4.31287 -4.3086839 -4.3062277 -4.3056345 -4.3081961 -4.3124967 -4.3172283 -4.3212867 -4.3232293][-4.3357921 -4.337635 -4.3356729 -4.3343334 -4.3320117 -4.3290896 -4.3257589 -4.3222351 -4.3209496 -4.3209763 -4.3231673 -4.3274136 -4.3322139 -4.336585 -4.3386889][-4.3286481 -4.3259063 -4.3224835 -4.3213444 -4.3211637 -4.3192453 -4.3150096 -4.3104949 -4.3100085 -4.3104525 -4.3116369 -4.3161383 -4.3204107 -4.3241687 -4.3277369][-4.310101 -4.3020377 -4.2949071 -4.2917929 -4.2918663 -4.2896051 -4.2814875 -4.2732024 -4.2750521 -4.279006 -4.281733 -4.288074 -4.2928362 -4.2966542 -4.2990828][-4.2880831 -4.2735262 -4.2589245 -4.2486105 -4.2453852 -4.240036 -4.2243476 -4.2110882 -4.2177863 -4.2293386 -4.2404 -4.2541327 -4.2598133 -4.262053 -4.2605133][-4.2624249 -4.2399979 -4.21347 -4.188942 -4.1775627 -4.1650429 -4.14151 -4.1266565 -4.1438131 -4.1686239 -4.1922388 -4.215836 -4.2214451 -4.2196307 -4.2121692][-4.2315168 -4.1947155 -4.1506324 -4.1072888 -4.0817904 -4.0588312 -4.0278516 -4.0149469 -4.0489984 -4.0883608 -4.1214528 -4.1514993 -4.1579881 -4.15255 -4.1390719][-4.2033725 -4.1465654 -4.0814886 -4.0202994 -3.9821963 -3.9473538 -3.9053056 -3.8960812 -3.9518263 -4.0081148 -4.0544844 -4.0979691 -4.1162753 -4.1155128 -4.0999312][-4.2003169 -4.1345887 -4.0635881 -4.0039668 -3.972157 -3.9452493 -3.9148831 -3.9180253 -3.9794879 -4.0364108 -4.083487 -4.1290679 -4.1519685 -4.1565247 -4.14661][-4.2292852 -4.1732712 -4.1149864 -4.0699391 -4.0518408 -4.0391731 -4.0255785 -4.036664 -4.0876188 -4.1325674 -4.1715693 -4.2086062 -4.22739 -4.2324452 -4.2256141][-4.2712078 -4.2337656 -4.1952224 -4.16716 -4.1592979 -4.1564569 -4.155 -4.1690669 -4.2068214 -4.2397552 -4.2682939 -4.29035 -4.30114 -4.3039784 -4.2975907][-4.3160243 -4.2952323 -4.2743 -4.2598157 -4.2583094 -4.2600408 -4.2621536 -4.2731228 -4.2968621 -4.3170271 -4.3326755 -4.3420625 -4.3469043 -4.3470478 -4.341167][-4.3488879 -4.339643 -4.3300195 -4.3236556 -4.323678 -4.3257174 -4.3285356 -4.3351088 -4.3476496 -4.3574 -4.3633285 -4.3657293 -4.3653517 -4.3619709 -4.3567052][-4.3607683 -4.3565931 -4.352478 -4.35038 -4.3502908 -4.3519988 -4.3544765 -4.3580947 -4.362772 -4.3648505 -4.3636413 -4.360517 -4.3558407 -4.3500342 -4.3458562][-4.3583031 -4.3534107 -4.3485842 -4.3452387 -4.344213 -4.3466349 -4.3497462 -4.3519812 -4.3527021 -4.3505969 -4.3463092 -4.3416166 -4.3377819 -4.3347225 -4.33427]]...]
INFO - root - 2017-12-05 17:01:33.193949: step 24710, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 75h:32m:18s remains)
INFO - root - 2017-12-05 17:01:42.216352: step 24720, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 75h:59m:41s remains)
INFO - root - 2017-12-05 17:01:51.464850: step 24730, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.928 sec/batch; 79h:22m:00s remains)
INFO - root - 2017-12-05 17:02:00.580516: step 24740, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 79h:18m:24s remains)
INFO - root - 2017-12-05 17:02:09.850347: step 24750, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 81h:41m:55s remains)
INFO - root - 2017-12-05 17:02:18.753235: step 24760, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 78h:17m:58s remains)
INFO - root - 2017-12-05 17:02:27.787447: step 24770, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 76h:41m:10s remains)
INFO - root - 2017-12-05 17:02:36.935606: step 24780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 79h:28m:24s remains)
INFO - root - 2017-12-05 17:02:46.027155: step 24790, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 78h:37m:21s remains)
INFO - root - 2017-12-05 17:02:55.016919: step 24800, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 78h:07m:54s remains)
2017-12-05 17:02:55.753093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1917176 -4.172544 -4.1689987 -4.160759 -4.1470456 -4.1392927 -4.133234 -4.1377125 -4.1550469 -4.17665 -4.1974554 -4.2157722 -4.2314363 -4.2426157 -4.2466559][-4.2021513 -4.1841016 -4.1819296 -4.1725335 -4.1556134 -4.1478629 -4.1410632 -4.144835 -4.1651373 -4.1871953 -4.2062378 -4.2211161 -4.2343383 -4.2433081 -4.2452478][-4.2142797 -4.1969261 -4.1945381 -4.1853576 -4.1682663 -4.1642933 -4.1611547 -4.1655211 -4.1867175 -4.2066159 -4.2222247 -4.2319121 -4.2406917 -4.2461157 -4.2449322][-4.2278528 -4.2132459 -4.2109656 -4.2023954 -4.18573 -4.1868258 -4.189435 -4.195302 -4.2150574 -4.2304149 -4.2402172 -4.2441506 -4.2478857 -4.248682 -4.2434411][-4.2388988 -4.2282023 -4.2272143 -4.2205343 -4.2047138 -4.2079325 -4.2137074 -4.2205467 -4.2374158 -4.2478962 -4.2521238 -4.2515879 -4.251184 -4.2469726 -4.2371974][-4.2475939 -4.2405138 -4.240613 -4.235342 -4.2192454 -4.2211981 -4.2274756 -4.2341752 -4.2487688 -4.2563934 -4.2575531 -4.2540622 -4.2496257 -4.2404137 -4.2263389][-4.256794 -4.252183 -4.2522445 -4.2474871 -4.2325039 -4.2333059 -4.2386436 -4.2446694 -4.2593312 -4.2665982 -4.2661552 -4.2612834 -4.2536068 -4.2402234 -4.2237229][-4.2625031 -4.2596231 -4.2597847 -4.2574787 -4.2476444 -4.2515478 -4.2585654 -4.2648025 -4.2796516 -4.2873268 -4.2856655 -4.2780433 -4.266367 -4.2493191 -4.2310638][-4.2638483 -4.2604523 -4.2610774 -4.2612319 -4.2563534 -4.2665706 -4.2784691 -4.2872481 -4.3023906 -4.3110957 -4.3110828 -4.3020887 -4.28753 -4.2681284 -4.2481961][-4.2638316 -4.260767 -4.2625251 -4.2646489 -4.2632771 -4.2781777 -4.2942376 -4.3058553 -4.3219256 -4.3322382 -4.3351417 -4.3270669 -4.3125391 -4.2939439 -4.2758551][-4.2671685 -4.2660151 -4.2690153 -4.27229 -4.2724414 -4.2892928 -4.3056808 -4.3173132 -4.3334746 -4.3442507 -4.3485188 -4.3423839 -4.3308468 -4.3167419 -4.3027644][-4.2731404 -4.2740717 -4.2785239 -4.2819428 -4.2807961 -4.2966933 -4.3107572 -4.3208003 -4.3356237 -4.3448339 -4.3488226 -4.3443027 -4.3364916 -4.3276715 -4.3186054][-4.2785373 -4.2806392 -4.2853694 -4.2884293 -4.2862024 -4.3011479 -4.3132405 -4.3225312 -4.3357906 -4.3431444 -4.3457003 -4.3422847 -4.3375998 -4.3323503 -4.3271408][-4.2845793 -4.2885246 -4.2934408 -4.2956271 -4.2936873 -4.3085423 -4.3189178 -4.3274336 -4.3384814 -4.3436933 -4.344378 -4.3407097 -4.3369441 -4.3331037 -4.3290715][-4.2881708 -4.2932367 -4.2982483 -4.3004417 -4.3003635 -4.3139243 -4.322084 -4.3282561 -4.3369031 -4.3396664 -4.3381772 -4.3334761 -4.3285437 -4.3236127 -4.3191981]]...]
INFO - root - 2017-12-05 17:03:04.692120: step 24810, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 76h:21m:07s remains)
INFO - root - 2017-12-05 17:03:13.774118: step 24820, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 78h:12m:31s remains)
INFO - root - 2017-12-05 17:03:22.762668: step 24830, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.916 sec/batch; 78h:16m:57s remains)
INFO - root - 2017-12-05 17:03:31.799169: step 24840, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.899 sec/batch; 76h:51m:31s remains)
INFO - root - 2017-12-05 17:03:40.917361: step 24850, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 75h:23m:59s remains)
INFO - root - 2017-12-05 17:03:49.969219: step 24860, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 79h:48m:52s remains)
INFO - root - 2017-12-05 17:03:58.975233: step 24870, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 78h:20m:34s remains)
INFO - root - 2017-12-05 17:04:08.080792: step 24880, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 78h:16m:10s remains)
INFO - root - 2017-12-05 17:04:17.192318: step 24890, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 75h:27m:50s remains)
INFO - root - 2017-12-05 17:04:26.136875: step 24900, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 77h:43m:05s remains)
2017-12-05 17:04:26.932840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2738342 -4.2681847 -4.2430654 -4.2059765 -4.1700811 -4.1461029 -4.1437259 -4.1558557 -4.1537328 -4.1488004 -4.1559582 -4.1755672 -4.1995778 -4.2228041 -4.2364459][-4.2639232 -4.25589 -4.2247562 -4.1869974 -4.151185 -4.1273403 -4.1283875 -4.1479321 -4.1525373 -4.149816 -4.1566672 -4.1787143 -4.2052722 -4.22263 -4.22588][-4.2646618 -4.2510653 -4.2149205 -4.1691928 -4.1245189 -4.0939069 -4.0974522 -4.1283321 -4.149385 -4.1592178 -4.1734123 -4.1965814 -4.2188177 -4.2267442 -4.2190218][-4.2778134 -4.25517 -4.2102323 -4.1529589 -4.0941167 -4.0479617 -4.0447283 -4.0832253 -4.125566 -4.153966 -4.1781878 -4.2035937 -4.2220774 -4.2266822 -4.2136178][-4.2954049 -4.2691903 -4.2223778 -4.1594138 -4.0899577 -4.0262642 -4.0075216 -4.0368671 -4.0839868 -4.1264958 -4.163775 -4.1965251 -4.2102365 -4.2107544 -4.2003803][-4.2998586 -4.2770319 -4.2364616 -4.1802707 -4.1106467 -4.0345607 -3.9918888 -3.9967976 -4.0362759 -4.0882964 -4.1415424 -4.1838531 -4.1971726 -4.1939664 -4.1881213][-4.2877288 -4.2693429 -4.2356262 -4.1934218 -4.1343088 -4.0560641 -3.9896402 -3.9695966 -3.9997048 -4.0572553 -4.1238003 -4.1743169 -4.1908879 -4.1884809 -4.1837878][-4.2730279 -4.2556276 -4.2269373 -4.1988997 -4.1569 -4.0904627 -4.0179167 -3.9853776 -4.0042896 -4.0549555 -4.1215215 -4.1776834 -4.1974354 -4.1948309 -4.1877565][-4.2586513 -4.2433467 -4.2186027 -4.2006936 -4.1800184 -4.139133 -4.0834055 -4.0552759 -4.064158 -4.0985365 -4.1546049 -4.2053742 -4.2261043 -4.2201352 -4.2067847][-4.2490196 -4.2403536 -4.224473 -4.2117715 -4.2035813 -4.1834373 -4.14608 -4.1305852 -4.1353617 -4.159379 -4.2028384 -4.2437434 -4.2590675 -4.2483869 -4.2268353][-4.2515655 -4.2469664 -4.2371812 -4.2257457 -4.2194166 -4.2089596 -4.1892262 -4.186419 -4.1958156 -4.2176514 -4.2523408 -4.2804193 -4.285532 -4.269033 -4.2422743][-4.2490749 -4.2499819 -4.2476921 -4.2412767 -4.237195 -4.2330709 -4.2251081 -4.2299714 -4.2431149 -4.264379 -4.2893877 -4.3054523 -4.3051877 -4.2869349 -4.2575512][-4.2532778 -4.25395 -4.2518086 -4.24758 -4.2458386 -4.2470312 -4.2469316 -4.2570066 -4.2729936 -4.2914453 -4.30871 -4.3161597 -4.3108058 -4.2949781 -4.2720304][-4.2681813 -4.2679663 -4.264174 -4.2595344 -4.2567277 -4.2576275 -4.2608728 -4.2712421 -4.2864995 -4.302289 -4.3127527 -4.313026 -4.3050203 -4.2921505 -4.2788339][-4.2778296 -4.2768464 -4.271029 -4.2643557 -4.26066 -4.2601147 -4.2622633 -4.2712007 -4.2851734 -4.2980704 -4.30457 -4.3025885 -4.2944059 -4.2839823 -4.2784643]]...]
INFO - root - 2017-12-05 17:04:35.998780: step 24910, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 79h:31m:14s remains)
INFO - root - 2017-12-05 17:04:45.150029: step 24920, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 76h:59m:37s remains)
INFO - root - 2017-12-05 17:04:54.056144: step 24930, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 75h:45m:38s remains)
INFO - root - 2017-12-05 17:05:03.232479: step 24940, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 78h:05m:45s remains)
INFO - root - 2017-12-05 17:05:12.170070: step 24950, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 72h:10m:07s remains)
INFO - root - 2017-12-05 17:05:21.410337: step 24960, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.952 sec/batch; 81h:17m:45s remains)
INFO - root - 2017-12-05 17:05:30.628821: step 24970, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 80h:17m:25s remains)
INFO - root - 2017-12-05 17:05:39.695978: step 24980, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 78h:36m:50s remains)
INFO - root - 2017-12-05 17:05:48.830041: step 24990, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 73h:41m:37s remains)
INFO - root - 2017-12-05 17:05:57.981243: step 25000, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 74h:25m:29s remains)
2017-12-05 17:05:58.782570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2982802 -4.3031454 -4.2963409 -4.2875409 -4.2765722 -4.2739692 -4.2687397 -4.2254286 -4.1872859 -4.2158804 -4.2601929 -4.2809415 -4.280261 -4.2944436 -4.3164883][-4.2866426 -4.2875848 -4.2810354 -4.2721143 -4.260818 -4.2616596 -4.2554555 -4.2099662 -4.1739492 -4.2098241 -4.2582006 -4.2793655 -4.28144 -4.2971444 -4.3197846][-4.2759466 -4.2689505 -4.2555261 -4.2434373 -4.2344403 -4.2439156 -4.2400246 -4.1940503 -4.1592479 -4.2062387 -4.2617917 -4.284286 -4.2906933 -4.3079553 -4.3292871][-4.2675123 -4.2504292 -4.2221994 -4.2014065 -4.1928034 -4.2058306 -4.2003245 -4.1549358 -4.1267042 -4.1923132 -4.259181 -4.2846313 -4.294467 -4.3120141 -4.3300381][-4.2603431 -4.2337265 -4.1934977 -4.1654434 -4.1530952 -4.1586475 -4.1419454 -4.0953565 -4.0788832 -4.1656189 -4.2438989 -4.2726731 -4.2862144 -4.3020468 -4.3134351][-4.2424016 -4.2207384 -4.17995 -4.1491208 -4.1298251 -4.1182156 -4.079268 -4.0177026 -4.0037384 -4.1112933 -4.2025957 -4.2372904 -4.2556949 -4.2740321 -4.2820864][-4.214283 -4.208282 -4.1770711 -4.1489329 -4.1185293 -4.0798569 -4.0013895 -3.8978696 -3.8716512 -4.0065804 -4.123487 -4.1736121 -4.2035294 -4.2323246 -4.2467275][-4.1963158 -4.200819 -4.1803942 -4.1518288 -4.1166229 -4.0676403 -3.960624 -3.8235843 -3.7879796 -3.9393809 -4.0716057 -4.1305571 -4.1649156 -4.1972256 -4.21567][-4.1868653 -4.1958771 -4.1841831 -4.1609859 -4.1378188 -4.10713 -4.0232649 -3.9148312 -3.892086 -3.9999545 -4.0968456 -4.1379004 -4.1622276 -4.1899638 -4.2081676][-4.1870022 -4.1959944 -4.1904025 -4.1813736 -4.1796484 -4.1745377 -4.1276588 -4.0612645 -4.0436869 -4.0925431 -4.1383533 -4.1545162 -4.1675353 -4.1954641 -4.2152739][-4.2078462 -4.2155876 -4.21416 -4.2164907 -4.2310019 -4.2382603 -4.2125978 -4.1728163 -4.1569014 -4.172678 -4.19056 -4.1926956 -4.1969347 -4.219285 -4.2339177][-4.2465439 -4.245851 -4.2421722 -4.248826 -4.2705722 -4.284822 -4.2732768 -4.2504597 -4.2411089 -4.2498379 -4.2591848 -4.2566128 -4.2513051 -4.2574148 -4.2601981][-4.2813449 -4.2729917 -4.2643585 -4.2698956 -4.2918744 -4.3077831 -4.3056931 -4.2952867 -4.2963347 -4.3079872 -4.3139992 -4.3097348 -4.2985134 -4.2921729 -4.2860489][-4.3063803 -4.2970304 -4.2863159 -4.28844 -4.302659 -4.3153334 -4.3167214 -4.31398 -4.319325 -4.3294673 -4.3338933 -4.3304491 -4.3210087 -4.3125539 -4.3043008][-4.3199286 -4.3160996 -4.3087988 -4.3081312 -4.3127465 -4.3168993 -4.3152461 -4.313076 -4.3151717 -4.3194661 -4.3217006 -4.3205333 -4.3167 -4.3110867 -4.3047361]]...]
INFO - root - 2017-12-05 17:06:07.679630: step 25010, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 74h:48m:46s remains)
INFO - root - 2017-12-05 17:06:16.719990: step 25020, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 77h:41m:22s remains)
INFO - root - 2017-12-05 17:06:25.819819: step 25030, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 80h:08m:01s remains)
INFO - root - 2017-12-05 17:06:34.845855: step 25040, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 77h:04m:38s remains)
INFO - root - 2017-12-05 17:06:43.819830: step 25050, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 76h:00m:48s remains)
INFO - root - 2017-12-05 17:06:52.917468: step 25060, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 75h:14m:02s remains)
INFO - root - 2017-12-05 17:07:01.997253: step 25070, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 80h:54m:14s remains)
INFO - root - 2017-12-05 17:07:11.082403: step 25080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 77h:11m:19s remains)
INFO - root - 2017-12-05 17:07:20.213563: step 25090, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 79h:38m:21s remains)
INFO - root - 2017-12-05 17:07:29.404032: step 25100, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 79h:31m:08s remains)
2017-12-05 17:07:30.163178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3557482 -4.3471193 -4.338769 -4.334867 -4.3324556 -4.3307595 -4.3296747 -4.3298345 -4.3308868 -4.3305488 -4.3282285 -4.3272128 -4.3324776 -4.3407555 -4.3532381][-4.3389831 -4.3269358 -4.3146014 -4.3063478 -4.2985168 -4.2867346 -4.2754416 -4.2719159 -4.2798281 -4.2881546 -4.2871022 -4.2845864 -4.2910318 -4.3025117 -4.3197803][-4.3182387 -4.3025122 -4.2849569 -4.2723565 -4.2607265 -4.2380857 -4.2169156 -4.2112069 -4.227356 -4.2441854 -4.2426815 -4.2357631 -4.2406797 -4.2511611 -4.2730856][-4.29829 -4.27362 -4.24671 -4.2294078 -4.2177339 -4.1904988 -4.1621914 -4.1538763 -4.1705527 -4.18687 -4.1874814 -4.1762757 -4.1781754 -4.189455 -4.2209206][-4.280426 -4.2488265 -4.2169037 -4.1962361 -4.1843719 -4.1546988 -4.1132603 -4.0966668 -4.1118531 -4.1233335 -4.1268058 -4.1190162 -4.1251016 -4.14099 -4.1805449][-4.2659669 -4.2348928 -4.2027168 -4.1774988 -4.1580286 -4.1116457 -4.042522 -4.0110054 -4.0378494 -4.0616546 -4.07571 -4.0793419 -4.0952048 -4.1177039 -4.1607971][-4.2455921 -4.2140131 -4.1779289 -4.1446228 -4.1063533 -4.0299296 -3.9179788 -3.8664539 -3.9246604 -3.9935343 -4.0371685 -4.0601134 -4.0849571 -4.1072531 -4.1485][-4.2239838 -4.1866097 -4.1438193 -4.1031442 -4.0419836 -3.9266746 -3.7640936 -3.6871226 -3.7966914 -3.9308333 -4.0066657 -4.0439568 -4.0729084 -4.0947423 -4.1338587][-4.211247 -4.1665449 -4.1205721 -4.07314 -4.0018082 -3.880765 -3.7245264 -3.6612611 -3.7925577 -3.9380064 -4.00956 -4.0481062 -4.0756907 -4.0989656 -4.1333208][-4.2135434 -4.1666636 -4.1191659 -4.0749192 -4.0241642 -3.9516132 -3.8737202 -3.8433273 -3.9266798 -4.0158772 -4.0567226 -4.0827641 -4.1027803 -4.1252656 -4.1562562][-4.2306943 -4.1872945 -4.1454868 -4.1122823 -4.0887942 -4.061049 -4.035295 -4.0217619 -4.0629873 -4.1124983 -4.1315203 -4.139667 -4.1478472 -4.1654677 -4.1930814][-4.2490792 -4.2141471 -4.1828842 -4.163218 -4.1564817 -4.1478333 -4.1386862 -4.1302195 -4.1539812 -4.1845827 -4.1939869 -4.1932917 -4.1969919 -4.2109218 -4.2322178][-4.2662692 -4.2360821 -4.2147465 -4.2070913 -4.2091322 -4.2046051 -4.1972804 -4.1904573 -4.2070818 -4.2277436 -4.23357 -4.2342615 -4.2414303 -4.2556911 -4.2698522][-4.2921925 -4.2673092 -4.2529078 -4.2506666 -4.2527251 -4.2495341 -4.2454748 -4.2430763 -4.2542896 -4.2660723 -4.26815 -4.2703834 -4.2787671 -4.290823 -4.3000813][-4.3236842 -4.3047519 -4.29389 -4.292623 -4.2937293 -4.2929974 -4.2930331 -4.2952275 -4.3012757 -4.30562 -4.3083286 -4.3121004 -4.3190131 -4.3261623 -4.3292718]]...]
INFO - root - 2017-12-05 17:07:39.024625: step 25110, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 75h:41m:54s remains)
INFO - root - 2017-12-05 17:07:48.155982: step 25120, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 78h:34m:08s remains)
INFO - root - 2017-12-05 17:07:57.256875: step 25130, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.884 sec/batch; 75h:29m:04s remains)
INFO - root - 2017-12-05 17:08:06.210146: step 25140, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.925 sec/batch; 78h:56m:34s remains)
INFO - root - 2017-12-05 17:08:15.434742: step 25150, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 76h:22m:03s remains)
INFO - root - 2017-12-05 17:08:24.509201: step 25160, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 76h:52m:53s remains)
INFO - root - 2017-12-05 17:08:33.690685: step 25170, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.904 sec/batch; 77h:09m:50s remains)
INFO - root - 2017-12-05 17:08:42.797976: step 25180, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 78h:09m:15s remains)
INFO - root - 2017-12-05 17:08:52.022074: step 25190, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 78h:23m:36s remains)
INFO - root - 2017-12-05 17:09:01.189878: step 25200, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 76h:04m:10s remains)
2017-12-05 17:09:01.982873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.182343 -4.1898971 -4.1772618 -4.1475582 -4.1255817 -4.1267133 -4.1438408 -4.1628203 -4.1707172 -4.1743646 -4.1778708 -4.1708903 -4.1588836 -4.1544843 -4.1505628][-4.1733403 -4.1764369 -4.1607552 -4.1290488 -4.1060147 -4.1062951 -4.1245389 -4.1430368 -4.1542225 -4.1605091 -4.1659827 -4.1589813 -4.1479855 -4.1435776 -4.13912][-4.1687975 -4.1676774 -4.1560893 -4.1283994 -4.1061859 -4.1071477 -4.1270247 -4.1448703 -4.1574278 -4.1663933 -4.1703 -4.162684 -4.154191 -4.1503606 -4.1455245][-4.1601853 -4.15606 -4.1489182 -4.1263881 -4.1097808 -4.1135879 -4.1322303 -4.1522574 -4.1677012 -4.1808543 -4.1812229 -4.1690607 -4.1583447 -4.15246 -4.1463451][-4.1450462 -4.1393681 -4.1316967 -4.1129723 -4.1062059 -4.1147289 -4.1335144 -4.1517358 -4.1638365 -4.1781006 -4.1748614 -4.1599007 -4.1446996 -4.1378608 -4.1366663][-4.132453 -4.1267166 -4.1177006 -4.0902743 -4.0736732 -4.0858264 -4.1168051 -4.1395226 -4.1469078 -4.1577096 -4.1567173 -4.13986 -4.1185279 -4.1056395 -4.1040335][-4.0858088 -4.0789542 -4.0635777 -4.0139465 -3.9558234 -3.9682853 -4.0328145 -4.0813494 -4.1029716 -4.1199708 -4.1277924 -4.1173763 -4.0940843 -4.0744491 -4.0636215][-4.0107436 -3.9994607 -3.9823458 -3.9088731 -3.7889619 -3.7810726 -3.8941195 -3.9819717 -4.0291605 -4.0653424 -4.0942936 -4.1093521 -4.1018667 -4.0791407 -4.0562692][-3.9778745 -3.9717302 -3.9714148 -3.9164097 -3.795037 -3.7565491 -3.85931 -3.9491808 -4.006423 -4.0565543 -4.09916 -4.1248055 -4.1287251 -4.1094871 -4.0773544][-4.0234919 -4.0374875 -4.0568032 -4.0328312 -3.9645696 -3.9341772 -3.9735241 -4.0208282 -4.0664744 -4.1096711 -4.1419468 -4.1531477 -4.152338 -4.133285 -4.0985851][-4.084106 -4.107595 -4.1239367 -4.1183004 -4.0961971 -4.088141 -4.0942326 -4.10041 -4.1227036 -4.1477194 -4.1600237 -4.1598845 -4.1619415 -4.1476626 -4.1144609][-4.1186652 -4.1465178 -4.1575742 -4.1570816 -4.1539583 -4.1590996 -4.1574392 -4.147758 -4.1515675 -4.1588044 -4.1564107 -4.1510935 -4.15816 -4.1503992 -4.1122732][-4.1343403 -4.168087 -4.177659 -4.1757827 -4.1786804 -4.1896281 -4.1859078 -4.1724067 -4.1619263 -4.1549559 -4.1468163 -4.1364617 -4.1376734 -4.1269016 -4.0823345][-4.148519 -4.1806617 -4.1925445 -4.195416 -4.2071371 -4.2207451 -4.2152114 -4.1950336 -4.1731095 -4.1559906 -4.1403461 -4.1249928 -4.1141558 -4.0978508 -4.0551991][-4.1635008 -4.1889062 -4.1996541 -4.2078714 -4.2238607 -4.2354736 -4.2274232 -4.2046432 -4.1788015 -4.1567774 -4.1349411 -4.118206 -4.1012678 -4.0818725 -4.0501332]]...]
INFO - root - 2017-12-05 17:09:10.875938: step 25210, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 75h:21m:25s remains)
INFO - root - 2017-12-05 17:09:20.047620: step 25220, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 77h:49m:30s remains)
INFO - root - 2017-12-05 17:09:29.087928: step 25230, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 79h:33m:06s remains)
INFO - root - 2017-12-05 17:09:38.007031: step 25240, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 78h:12m:07s remains)
INFO - root - 2017-12-05 17:09:47.251453: step 25250, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 79h:03m:18s remains)
INFO - root - 2017-12-05 17:09:56.337053: step 25260, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 77h:45m:41s remains)
INFO - root - 2017-12-05 17:10:05.387894: step 25270, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 79h:45m:20s remains)
INFO - root - 2017-12-05 17:10:14.622754: step 25280, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 76h:35m:07s remains)
INFO - root - 2017-12-05 17:10:23.756021: step 25290, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 81h:54m:56s remains)
INFO - root - 2017-12-05 17:10:32.852332: step 25300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 77h:32m:47s remains)
2017-12-05 17:10:33.624336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2736125 -4.2748132 -4.2735157 -4.2669992 -4.2626958 -4.2653065 -4.2693558 -4.2719169 -4.27281 -4.275732 -4.2788925 -4.2829719 -4.2906427 -4.3032079 -4.3192029][-4.2406192 -4.2394891 -4.2359877 -4.2281713 -4.2251406 -4.2305589 -4.2369642 -4.2391191 -4.2381907 -4.2401223 -4.242888 -4.2500949 -4.2657852 -4.2877746 -4.3127618][-4.206141 -4.2008467 -4.1915483 -4.1830454 -4.1849556 -4.1967263 -4.2095985 -4.2135425 -4.2122636 -4.2132039 -4.2152753 -4.224874 -4.2460356 -4.2744389 -4.3061709][-4.1748991 -4.1617951 -4.1446161 -4.1324258 -4.1385036 -4.1604176 -4.1833658 -4.1912484 -4.19295 -4.1968889 -4.2014132 -4.2121258 -4.2353191 -4.2666955 -4.3026361][-4.1479044 -4.12566 -4.1002078 -4.0833549 -4.0909328 -4.117991 -4.1465306 -4.1595039 -4.1689944 -4.17856 -4.186409 -4.1981959 -4.2223392 -4.2575488 -4.2975826][-4.12972 -4.0982862 -4.0642872 -4.0401292 -4.0416789 -4.064044 -4.0894251 -4.1079192 -4.1287589 -4.14765 -4.1621051 -4.1777339 -4.2046227 -4.2451992 -4.2902246][-4.1160583 -4.0750961 -4.027812 -3.9859178 -3.9695492 -3.9737902 -3.9846506 -4.00851 -4.0500135 -4.0893793 -4.121717 -4.151536 -4.1878638 -4.2343521 -4.2821193][-4.0887022 -4.0425963 -3.9896257 -3.9378293 -3.9107068 -3.8980246 -3.8943467 -3.9232271 -3.9821672 -4.0369449 -4.0860119 -4.1298208 -4.1758537 -4.2269287 -4.2761335][-4.073307 -4.0340705 -3.9902859 -3.9478655 -3.927562 -3.9157066 -3.9138615 -3.9486063 -4.0080786 -4.0569105 -4.1010709 -4.1398139 -4.1796031 -4.22651 -4.2739758][-4.0786152 -4.0485578 -4.0142622 -3.9827504 -3.9690571 -3.9627824 -3.9677205 -4.0062046 -4.0603743 -4.0998497 -4.1337757 -4.1623969 -4.1918068 -4.2315631 -4.275095][-4.1041231 -4.0834155 -4.0619121 -4.0452733 -4.0419888 -4.0422659 -4.0497632 -4.0819597 -4.12377 -4.1512403 -4.174161 -4.1932116 -4.2134581 -4.2448292 -4.2818279][-4.1523418 -4.1409583 -4.1319551 -4.1285491 -4.1328449 -4.1364326 -4.1432786 -4.1652246 -4.1922216 -4.2086592 -4.220808 -4.2305808 -4.2432175 -4.2664337 -4.2951446][-4.2060442 -4.201036 -4.1989346 -4.2016964 -4.2076693 -4.2110858 -4.2159433 -4.2280545 -4.2432122 -4.2521181 -4.2588592 -4.2645893 -4.2736616 -4.2910404 -4.3119988][-4.2454991 -4.24162 -4.2406497 -4.2433944 -4.2472496 -4.2496257 -4.2522006 -4.2574544 -4.2659845 -4.2728047 -4.2792597 -4.2855315 -4.2944794 -4.3087664 -4.32456][-4.2758269 -4.2714772 -4.2690377 -4.2701788 -4.2727056 -4.2753038 -4.2773328 -4.2802696 -4.285305 -4.2902164 -4.2958364 -4.3019733 -4.309999 -4.3207026 -4.3313274]]...]
INFO - root - 2017-12-05 17:10:42.743558: step 25310, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 78h:57m:29s remains)
INFO - root - 2017-12-05 17:10:51.678486: step 25320, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 76h:35m:39s remains)
INFO - root - 2017-12-05 17:11:00.780728: step 25330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 77h:51m:54s remains)
INFO - root - 2017-12-05 17:11:09.917715: step 25340, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 79h:31m:56s remains)
INFO - root - 2017-12-05 17:11:19.025101: step 25350, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 75h:17m:14s remains)
INFO - root - 2017-12-05 17:11:28.147584: step 25360, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 76h:59m:19s remains)
INFO - root - 2017-12-05 17:11:37.248133: step 25370, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 77h:38m:05s remains)
INFO - root - 2017-12-05 17:11:46.252406: step 25380, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 76h:44m:49s remains)
INFO - root - 2017-12-05 17:11:55.385998: step 25390, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 76h:51m:52s remains)
INFO - root - 2017-12-05 17:12:04.248886: step 25400, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 77h:06m:47s remains)
2017-12-05 17:12:04.998209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2568126 -4.238307 -4.2124262 -4.1957717 -4.1936893 -4.1967444 -4.1826348 -4.1545935 -4.1456695 -4.1643515 -4.1941786 -4.2320743 -4.2775588 -4.3132839 -4.3404517][-4.22082 -4.19705 -4.1602492 -4.1372776 -4.1333127 -4.1356616 -4.1174808 -4.0729475 -4.0517969 -4.0761676 -4.1220894 -4.1760669 -4.2379665 -4.289073 -4.3287458][-4.1852336 -4.1606951 -4.1182742 -4.093955 -4.0897989 -4.0905452 -4.0693026 -4.0084162 -3.9700837 -3.9975345 -4.0594916 -4.12829 -4.204278 -4.269155 -4.3199334][-4.1436791 -4.11871 -4.0758758 -4.0569515 -4.0519156 -4.0464716 -4.0193295 -3.944665 -3.8889871 -3.9214969 -4.0067568 -4.0935845 -4.1788797 -4.2525129 -4.3119149][-4.1067328 -4.0837417 -4.04048 -4.0272536 -4.0193281 -4.0032091 -3.9682329 -3.8832426 -3.8103154 -3.8461797 -3.9600029 -4.0701222 -4.1648021 -4.2431536 -4.3069553][-4.0858159 -4.0678773 -4.0276079 -4.0146704 -4.0033693 -3.9756136 -3.9262121 -3.8354828 -3.7442706 -3.7749789 -3.9189296 -4.0560246 -4.16078 -4.242033 -4.3059926][-4.0651841 -4.0568018 -4.0261865 -4.0144243 -4.0048475 -3.9786036 -3.9211788 -3.82572 -3.7164252 -3.7336984 -3.8966584 -4.0517292 -4.1624088 -4.2434306 -4.3055377][-4.0636263 -4.0668 -4.0504065 -4.0436945 -4.0395169 -4.0241132 -3.9720416 -3.8790905 -3.7708614 -3.7810261 -3.9299335 -4.0751128 -4.1791668 -4.25128 -4.3069129][-4.0875459 -4.1078367 -4.1142087 -4.1198535 -4.1245842 -4.1188545 -4.0743551 -3.9852512 -3.888181 -3.8974576 -4.0077357 -4.1215582 -4.2103353 -4.2677336 -4.312398][-4.1419516 -4.1683559 -4.1808162 -4.1884379 -4.1979828 -4.1920762 -4.1490889 -4.066402 -3.982209 -3.9914956 -4.0731397 -4.1635952 -4.2392564 -4.2837033 -4.316896][-4.2118363 -4.227375 -4.2289228 -4.2323513 -4.2410831 -4.2350297 -4.1964602 -4.1259313 -4.0525174 -4.0605 -4.1225824 -4.19756 -4.2618628 -4.2981911 -4.322217][-4.2687774 -4.273931 -4.2668066 -4.2687554 -4.2746115 -4.2672853 -4.2352529 -4.1811881 -4.12198 -4.1238074 -4.1689687 -4.2280569 -4.2812066 -4.3118057 -4.3296633][-4.3070793 -4.3056903 -4.2970729 -4.2989688 -4.3051934 -4.3016386 -4.2793751 -4.2397757 -4.1949811 -4.1904268 -4.2195549 -4.2601342 -4.302093 -4.3271775 -4.34045][-4.3282824 -4.32671 -4.3205309 -4.3208194 -4.3261456 -4.3259306 -4.3117375 -4.2837834 -4.2515216 -4.2449546 -4.2637672 -4.2915125 -4.3221807 -4.3406968 -4.3502216][-4.3490219 -4.3464508 -4.3401136 -4.3372731 -4.3372869 -4.3347921 -4.3256369 -4.3092933 -4.288692 -4.2814307 -4.2930932 -4.3132577 -4.3355279 -4.34935 -4.3569474]]...]
INFO - root - 2017-12-05 17:12:14.216931: step 25410, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 78h:50m:55s remains)
INFO - root - 2017-12-05 17:12:23.340403: step 25420, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 76h:41m:41s remains)
INFO - root - 2017-12-05 17:12:32.324387: step 25430, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 77h:00m:35s remains)
INFO - root - 2017-12-05 17:12:41.367104: step 25440, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 79h:19m:42s remains)
INFO - root - 2017-12-05 17:12:50.581434: step 25450, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.969 sec/batch; 82h:39m:55s remains)
INFO - root - 2017-12-05 17:12:59.605952: step 25460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 75h:46m:12s remains)
INFO - root - 2017-12-05 17:13:08.691982: step 25470, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 76h:02m:35s remains)
INFO - root - 2017-12-05 17:13:17.785183: step 25480, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 75h:44m:28s remains)
INFO - root - 2017-12-05 17:13:26.862679: step 25490, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 76h:20m:46s remains)
INFO - root - 2017-12-05 17:13:35.911356: step 25500, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 74h:18m:40s remains)
2017-12-05 17:13:36.774034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2283883 -4.2301536 -4.2401233 -4.2428927 -4.2415838 -4.2424273 -4.2463183 -4.248138 -4.2432294 -4.2403655 -4.2422061 -4.2436652 -4.2462382 -4.2527089 -4.256412][-4.2140594 -4.2172256 -4.2334847 -4.2432346 -4.2456493 -4.2437239 -4.242208 -4.2435417 -4.2394514 -4.2388482 -4.2462449 -4.2536583 -4.2617273 -4.2691193 -4.2672505][-4.1980219 -4.2014294 -4.2214575 -4.2382545 -4.2412243 -4.2355633 -4.2314911 -4.2332511 -4.22985 -4.2331047 -4.2428193 -4.2549453 -4.267251 -4.2749333 -4.2723632][-4.1632857 -4.1632648 -4.185914 -4.207242 -4.2092352 -4.2019591 -4.1999207 -4.2041216 -4.2041903 -4.2116194 -4.2216797 -4.2359505 -4.2516146 -4.2584562 -4.2535162][-4.1336646 -4.129261 -4.1478238 -4.1638613 -4.1599035 -4.1471481 -4.1448288 -4.1511469 -4.1583109 -4.175561 -4.1957116 -4.2194414 -4.2431054 -4.250598 -4.24262][-4.116878 -4.1050539 -4.1095128 -4.1082206 -4.0895958 -4.06321 -4.0548363 -4.0657473 -4.0818357 -4.1163626 -4.1524229 -4.1869793 -4.2206655 -4.2340522 -4.2260652][-4.0990834 -4.0700469 -4.0459909 -4.0177078 -3.9783931 -3.9271772 -3.9056344 -3.9196396 -3.9443829 -3.9975839 -4.0533614 -4.100039 -4.1485081 -4.1776047 -4.1843629][-4.1012411 -4.0592365 -4.0161982 -3.9789257 -3.9365754 -3.8764293 -3.8479383 -3.861598 -3.8890376 -3.9463651 -4.0112085 -4.06413 -4.1166925 -4.1514091 -4.1679468][-4.1330571 -4.0994611 -4.0632367 -4.0414257 -4.019289 -3.9795482 -3.9655483 -3.9787536 -3.9971862 -4.0379028 -4.0876293 -4.12688 -4.1616335 -4.1832318 -4.1927757][-4.16057 -4.13763 -4.1161928 -4.1052742 -4.090734 -4.0634336 -4.0576153 -4.068716 -4.08097 -4.1087551 -4.1459579 -4.1741242 -4.1991472 -4.2144775 -4.21545][-4.1771922 -4.1615744 -4.1548061 -4.1527848 -4.1427751 -4.1221819 -4.1171947 -4.1241212 -4.1302347 -4.1497974 -4.1814051 -4.2067432 -4.2270055 -4.2388783 -4.2331152][-4.206903 -4.1972008 -4.1983862 -4.2013273 -4.1958547 -4.182549 -4.1795053 -4.1847372 -4.1875319 -4.2007728 -4.2265339 -4.24899 -4.2658238 -4.2743144 -4.2664175][-4.2380152 -4.2320576 -4.2363882 -4.2398949 -4.23677 -4.2280293 -4.2259908 -4.2304296 -4.2333703 -4.241806 -4.2600307 -4.2781539 -4.2932682 -4.3005657 -4.2928548][-4.2649097 -4.2608972 -4.2650738 -4.2684727 -4.2667489 -4.260385 -4.2579927 -4.2606735 -4.2631173 -4.2683482 -4.279119 -4.2914109 -4.3032846 -4.3087754 -4.3025594][-4.2897863 -4.2853789 -4.2870131 -4.2902317 -4.2908177 -4.2880807 -4.287322 -4.2894168 -4.2920356 -4.2960854 -4.3023424 -4.3094831 -4.3160968 -4.3189731 -4.3140039]]...]
INFO - root - 2017-12-05 17:13:45.843921: step 25510, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.915 sec/batch; 78h:03m:59s remains)
INFO - root - 2017-12-05 17:13:54.950716: step 25520, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 79h:12m:28s remains)
INFO - root - 2017-12-05 17:14:04.155551: step 25530, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 75h:48m:28s remains)
INFO - root - 2017-12-05 17:14:13.311686: step 25540, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 75h:32m:44s remains)
INFO - root - 2017-12-05 17:14:22.535804: step 25550, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.959 sec/batch; 81h:44m:31s remains)
INFO - root - 2017-12-05 17:14:31.602295: step 25560, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:05m:38s remains)
INFO - root - 2017-12-05 17:14:40.626972: step 25570, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 77h:16m:42s remains)
INFO - root - 2017-12-05 17:14:49.641514: step 25580, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 79h:56m:50s remains)
INFO - root - 2017-12-05 17:14:58.683755: step 25590, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 75h:50m:27s remains)
INFO - root - 2017-12-05 17:15:07.831000: step 25600, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 77h:39m:32s remains)
2017-12-05 17:15:08.670616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2666125 -4.2530489 -4.2388811 -4.2299862 -4.230309 -4.2354388 -4.2330756 -4.2218976 -4.20267 -4.17963 -4.1645613 -4.16106 -4.1610603 -4.1653752 -4.1761551][-4.276329 -4.2636223 -4.2496467 -4.2435355 -4.2470417 -4.2520971 -4.24539 -4.2294421 -4.2085 -4.1850181 -4.16935 -4.1679063 -4.1731243 -4.1844172 -4.2036572][-4.2822733 -4.276382 -4.2651019 -4.2565112 -4.2545743 -4.2530665 -4.2415686 -4.22533 -4.2069831 -4.1864781 -4.1691604 -4.165401 -4.1741338 -4.1891212 -4.2110214][-4.2709265 -4.2700243 -4.2588758 -4.2433577 -4.2318015 -4.2241855 -4.2138028 -4.206327 -4.1959977 -4.178854 -4.1572351 -4.1473961 -4.1536565 -4.1704831 -4.1934414][-4.2500787 -4.2475243 -4.2330794 -4.2125239 -4.19259 -4.1795869 -4.1663313 -4.16495 -4.1659422 -4.1540642 -4.133637 -4.1247187 -4.1278758 -4.1458597 -4.1714454][-4.2287183 -4.2232466 -4.2126288 -4.1945305 -4.171875 -4.1522937 -4.1306405 -4.1235905 -4.129981 -4.1214719 -4.1047745 -4.1024208 -4.1125774 -4.13692 -4.1646161][-4.20432 -4.1955891 -4.1875796 -4.1778 -4.1616516 -4.14305 -4.1162138 -4.1014118 -4.1031189 -4.094481 -4.08521 -4.08761 -4.1056094 -4.1383758 -4.1716294][-4.1921244 -4.1766558 -4.1714168 -4.167346 -4.1556211 -4.137084 -4.1063857 -4.0880613 -4.08269 -4.0688224 -4.059371 -4.0650773 -4.0950155 -4.1421156 -4.181149][-4.1949224 -4.178206 -4.1691604 -4.1606517 -4.1490717 -4.1314316 -4.0980325 -4.0771279 -4.0671539 -4.0516071 -4.0451479 -4.0528684 -4.0911322 -4.1459885 -4.1833181][-4.2263937 -4.211894 -4.1987104 -4.18108 -4.1645432 -4.1414232 -4.105495 -4.0827537 -4.0712004 -4.059165 -4.0571032 -4.067656 -4.10291 -4.14802 -4.1739278][-4.2612891 -4.2465677 -4.2296057 -4.2068315 -4.1884365 -4.1638665 -4.1318417 -4.1123767 -4.1002321 -4.0926056 -4.0902305 -4.1019311 -4.1333928 -4.1617489 -4.168293][-4.2742004 -4.2599297 -4.2438774 -4.2236838 -4.2115626 -4.1943784 -4.171957 -4.1585746 -4.1504316 -4.143095 -4.1388 -4.147368 -4.1676054 -4.185895 -4.179862][-4.2667866 -4.2570567 -4.245276 -4.2320576 -4.2281108 -4.2179561 -4.2040882 -4.1974268 -4.1964645 -4.1929398 -4.190289 -4.1957188 -4.2008805 -4.2035069 -4.1881962][-4.26448 -4.2612777 -4.2500405 -4.238739 -4.23757 -4.2339292 -4.2244635 -4.2220221 -4.2237267 -4.225275 -4.2291012 -4.2330685 -4.2286592 -4.218297 -4.1980405][-4.2661786 -4.2643251 -4.2500358 -4.2384496 -4.2400603 -4.2446475 -4.2405081 -4.2370043 -4.2392039 -4.2456594 -4.2562022 -4.262414 -4.2552333 -4.2394857 -4.2205529]]...]
INFO - root - 2017-12-05 17:15:17.563619: step 25610, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 77h:31m:57s remains)
INFO - root - 2017-12-05 17:15:26.808291: step 25620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:07m:33s remains)
INFO - root - 2017-12-05 17:15:35.999033: step 25630, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 77h:57m:08s remains)
INFO - root - 2017-12-05 17:15:45.066976: step 25640, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 77h:53m:42s remains)
INFO - root - 2017-12-05 17:15:54.095301: step 25650, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 75h:46m:51s remains)
INFO - root - 2017-12-05 17:16:03.275742: step 25660, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 79h:05m:56s remains)
INFO - root - 2017-12-05 17:16:12.323470: step 25670, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 79h:37m:03s remains)
INFO - root - 2017-12-05 17:16:21.253279: step 25680, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 77h:37m:21s remains)
INFO - root - 2017-12-05 17:16:30.373873: step 25690, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.940 sec/batch; 80h:04m:20s remains)
INFO - root - 2017-12-05 17:16:39.625681: step 25700, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 78h:44m:44s remains)
2017-12-05 17:16:40.483335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3205357 -4.3161674 -4.3040438 -4.2885671 -4.2878733 -4.304255 -4.3212991 -4.3216023 -4.3088894 -4.2872324 -4.2520218 -4.2122812 -4.1739593 -4.1620183 -4.1768847][-4.3106833 -4.3039374 -4.2880445 -4.2730374 -4.2750864 -4.2922983 -4.3088312 -4.3142962 -4.3124838 -4.2998328 -4.2656593 -4.2179217 -4.1702366 -4.1530848 -4.1681023][-4.2820067 -4.2714787 -4.2475781 -4.2253571 -4.2249751 -4.2414393 -4.2572212 -4.27187 -4.2854848 -4.2909627 -4.2752514 -4.2397208 -4.1955128 -4.1718493 -4.1735959][-4.2484288 -4.2302284 -4.1954741 -4.1613173 -4.1527886 -4.1645741 -4.1782336 -4.2055793 -4.2408738 -4.2684412 -4.2759328 -4.2606883 -4.2297168 -4.2030873 -4.18715][-4.2149825 -4.1887178 -4.1451063 -4.1048541 -4.0864949 -4.086103 -4.0922971 -4.1288075 -4.1852369 -4.2347188 -4.2619662 -4.2657876 -4.2498946 -4.2245297 -4.1961846][-4.1778383 -4.1440363 -4.0973763 -4.0604539 -4.0383492 -4.0219541 -4.0130548 -4.051661 -4.1265106 -4.1977215 -4.24404 -4.2607384 -4.2527986 -4.2285624 -4.1936512][-4.15626 -4.1120086 -4.0627832 -4.0344615 -4.0182166 -3.9915504 -3.9612074 -3.9845095 -4.0648251 -4.1525745 -4.2162452 -4.2457271 -4.249217 -4.2334619 -4.2013578][-4.1619091 -4.1099262 -4.0613966 -4.0412211 -4.0348268 -4.0082979 -3.9629467 -3.9608734 -4.027411 -4.1132069 -4.1853781 -4.2286744 -4.2476358 -4.2459631 -4.22461][-4.19896 -4.149159 -4.1079144 -4.0935168 -4.0939 -4.0701194 -4.0222325 -4.0023108 -4.0442619 -4.1121588 -4.1790867 -4.2289057 -4.2595692 -4.2679672 -4.2548308][-4.2502418 -4.2129149 -4.1812758 -4.1705856 -4.1726155 -4.1525574 -4.1126018 -4.0873966 -4.1083302 -4.1533942 -4.2039447 -4.2479272 -4.279952 -4.2925339 -4.283884][-4.2825294 -4.25946 -4.2382755 -4.2316375 -4.2362103 -4.2249837 -4.19683 -4.1731753 -4.179234 -4.2057085 -4.2393365 -4.2717657 -4.2966013 -4.3082728 -4.3016782][-4.2844825 -4.2739954 -4.2635841 -4.2645178 -4.2748723 -4.2719197 -4.2539792 -4.2349577 -4.2340674 -4.2487054 -4.2676144 -4.2858219 -4.2990541 -4.3060813 -4.2991714][-4.2554846 -4.2514496 -4.2522197 -4.2640467 -4.2811327 -4.2846642 -4.2715507 -4.2563725 -4.2536626 -4.2619834 -4.2712011 -4.2766361 -4.2792926 -4.2812214 -4.2743325][-4.2157207 -4.2128353 -4.2208543 -4.2406898 -4.2599096 -4.2642198 -4.2503 -4.2349119 -4.2316561 -4.2384758 -4.2447677 -4.2440763 -4.241384 -4.242424 -4.2397861][-4.2042823 -4.2017159 -4.2100377 -4.2275953 -4.2399392 -4.2383718 -4.2184043 -4.1992831 -4.1951356 -4.2028975 -4.2110248 -4.211144 -4.2095742 -4.2146811 -4.2190704]]...]
INFO - root - 2017-12-05 17:16:49.557655: step 25710, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 77h:42m:06s remains)
INFO - root - 2017-12-05 17:16:58.846090: step 25720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 78h:12m:13s remains)
INFO - root - 2017-12-05 17:17:07.901913: step 25730, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 77h:33m:29s remains)
INFO - root - 2017-12-05 17:17:16.890793: step 25740, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 77h:09m:53s remains)
INFO - root - 2017-12-05 17:17:26.128124: step 25750, loss = 2.03, batch loss = 1.97 (8.3 examples/sec; 0.960 sec/batch; 81h:48m:13s remains)
INFO - root - 2017-12-05 17:17:35.422891: step 25760, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 77h:16m:41s remains)
INFO - root - 2017-12-05 17:17:44.537192: step 25770, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 77h:06m:46s remains)
INFO - root - 2017-12-05 17:17:53.603842: step 25780, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 78h:37m:57s remains)
INFO - root - 2017-12-05 17:18:02.576201: step 25790, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 74h:53m:35s remains)
INFO - root - 2017-12-05 17:18:11.601997: step 25800, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 81h:26m:24s remains)
2017-12-05 17:18:12.515404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1416178 -4.1268706 -4.122364 -4.1324949 -4.1590447 -4.1921759 -4.2157946 -4.2146835 -4.1957536 -4.180603 -4.172616 -4.159514 -4.1413312 -4.1303387 -4.1301131][-4.1389647 -4.1162782 -4.1050758 -4.1130319 -4.1429968 -4.1743011 -4.1915574 -4.18658 -4.1724391 -4.1662145 -4.1643386 -4.1509671 -4.1247659 -4.1078577 -4.1065688][-4.1342287 -4.1025109 -4.0866241 -4.0998588 -4.1332154 -4.1600156 -4.1675286 -4.160625 -4.1566477 -4.1626453 -4.1659245 -4.1487494 -4.1107893 -4.0833135 -4.0737739][-4.1055317 -4.073432 -4.0684 -4.0955329 -4.1303954 -4.1494241 -4.14312 -4.1289358 -4.1321564 -4.1536713 -4.1650672 -4.1393166 -4.0854454 -4.0462685 -4.0342379][-4.0728173 -4.0575142 -4.0714674 -4.1035557 -4.1281571 -4.127255 -4.0969138 -4.0661521 -4.0738754 -4.1131353 -4.1366448 -4.1076245 -4.0445113 -4.0063624 -4.008533][-4.0818148 -4.0860133 -4.1031413 -4.1226015 -4.124536 -4.092473 -4.025661 -3.9736161 -3.9939058 -4.0541949 -4.0941749 -4.0751748 -4.0275006 -4.0091639 -4.0240397][-4.1010962 -4.1121392 -4.1192441 -4.1180296 -4.0923028 -4.0260296 -3.9171336 -3.8558934 -3.9171169 -4.0114532 -4.0716848 -4.0795469 -4.059968 -4.0548825 -4.0681291][-4.1007004 -4.1144772 -4.1124167 -4.0957942 -4.0517483 -3.9696174 -3.8528066 -3.8193836 -3.9193144 -4.029871 -4.0986147 -4.1215353 -4.1127014 -4.0998454 -4.0992537][-4.1105204 -4.1248994 -4.1183419 -4.0954227 -4.0484424 -3.9809766 -3.9093723 -3.9154122 -3.9991655 -4.0834184 -4.1349697 -4.1522589 -4.1401157 -4.1156278 -4.101491][-4.1377072 -4.1456122 -4.13146 -4.1061349 -4.066083 -4.0193458 -3.9889865 -4.0174279 -4.0807686 -4.1330123 -4.1597629 -4.1652803 -4.146915 -4.1128836 -4.0876055][-4.1545148 -4.1445231 -4.1176929 -4.0933719 -4.0650263 -4.0412855 -4.0432339 -4.0851312 -4.13934 -4.1734185 -4.1847911 -4.1803145 -4.1554823 -4.1158547 -4.0861549][-4.157299 -4.1362367 -4.1057329 -4.0863819 -4.0700545 -4.069808 -4.0969696 -4.1432428 -4.189187 -4.2141819 -4.2179193 -4.20949 -4.183332 -4.145153 -4.1182151][-4.1748471 -4.1527476 -4.1292233 -4.1136293 -4.1059508 -4.1196022 -4.1590681 -4.1996522 -4.2332077 -4.24947 -4.2507806 -4.2438517 -4.2227354 -4.1913681 -4.1690865][-4.2147889 -4.195406 -4.1792688 -4.1712613 -4.1701484 -4.1839037 -4.2168913 -4.2454128 -4.2653937 -4.2742891 -4.272613 -4.2647128 -4.2472291 -4.2234936 -4.2034621][-4.2607274 -4.2459354 -4.2347584 -4.2330151 -4.2367158 -4.2481308 -4.2678819 -4.2833261 -4.2912469 -4.2929373 -4.2883911 -4.2786055 -4.2640915 -4.2467556 -4.2317262]]...]
INFO - root - 2017-12-05 17:18:21.837686: step 25810, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 77h:39m:27s remains)
INFO - root - 2017-12-05 17:18:31.064292: step 25820, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 81h:57m:41s remains)
INFO - root - 2017-12-05 17:18:40.148568: step 25830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 75h:07m:32s remains)
INFO - root - 2017-12-05 17:18:49.387448: step 25840, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 79h:16m:57s remains)
INFO - root - 2017-12-05 17:18:58.528115: step 25850, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.929 sec/batch; 79h:09m:18s remains)
INFO - root - 2017-12-05 17:19:07.467073: step 25860, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 76h:52m:53s remains)
INFO - root - 2017-12-05 17:19:16.640078: step 25870, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 78h:23m:04s remains)
INFO - root - 2017-12-05 17:19:25.842704: step 25880, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 80h:24m:39s remains)
INFO - root - 2017-12-05 17:19:34.834842: step 25890, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 74h:41m:05s remains)
INFO - root - 2017-12-05 17:19:43.899856: step 25900, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 77h:14m:55s remains)
2017-12-05 17:19:44.726117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3419905 -4.3365059 -4.3347235 -4.3330369 -4.3315506 -4.3257065 -4.3217306 -4.3217645 -4.3263454 -4.3275313 -4.3274465 -4.3306613 -4.3326955 -4.3362918 -4.3424692][-4.3176832 -4.305263 -4.2987628 -4.2942052 -4.2876568 -4.2772131 -4.2739019 -4.2792816 -4.2880645 -4.2919106 -4.2952652 -4.3000245 -4.3010192 -4.3047705 -4.3135643][-4.3036394 -4.2833419 -4.2688127 -4.2565126 -4.2421875 -4.2276711 -4.2258172 -4.2352424 -4.2481647 -4.2543397 -4.2610908 -4.2663975 -4.2656627 -4.2694712 -4.281867][-4.2977142 -4.2716842 -4.2518411 -4.2309036 -4.2088332 -4.192276 -4.1951418 -4.2096138 -4.2231774 -4.2289944 -4.2332263 -4.2354646 -4.2319803 -4.2352481 -4.2505074][-4.3029571 -4.2774568 -4.2572718 -4.2318425 -4.2050314 -4.1900868 -4.1928005 -4.2018552 -4.2049584 -4.2019415 -4.1959395 -4.1939592 -4.1892014 -4.192101 -4.2104564][-4.316258 -4.2976837 -4.2813148 -4.2564816 -4.2292485 -4.2094622 -4.198112 -4.1889839 -4.1713033 -4.1520762 -4.1359692 -4.1352081 -4.1342363 -4.1428823 -4.1678057][-4.319294 -4.3040404 -4.2873826 -4.2609415 -4.2313724 -4.2003717 -4.170681 -4.1412783 -4.1031585 -4.0688062 -4.0455589 -4.0487633 -4.0551147 -4.0762815 -4.1147842][-4.2931495 -4.2707257 -4.2455292 -4.210628 -4.1744676 -4.1305332 -4.0862455 -4.042758 -3.9902625 -3.9497287 -3.9281621 -3.9391181 -3.9596539 -4.0017557 -4.063684][-4.240025 -4.2026134 -4.1637597 -4.1139669 -4.06265 -4.0039697 -3.9493959 -3.9017944 -3.8500819 -3.8202024 -3.8194666 -3.85228 -3.8995054 -3.96881 -4.0522318][-4.185854 -4.1336222 -4.0839534 -4.0258455 -3.9683776 -3.9105883 -3.866075 -3.8375208 -3.8086655 -3.8052614 -3.8313098 -3.8806548 -3.9427965 -4.0203767 -4.1036596][-4.1884675 -4.1369243 -4.0919271 -4.0462751 -4.0054073 -3.9664862 -3.9431341 -3.9359019 -3.9272268 -3.9383163 -3.9716456 -4.0177088 -4.0726476 -4.1363358 -4.1981611][-4.252934 -4.2210836 -4.1959405 -4.1716032 -4.1505542 -4.1302471 -4.1195159 -4.1199288 -4.1196818 -4.1322093 -4.1576037 -4.18827 -4.2241583 -4.2618394 -4.2952557][-4.3176174 -4.3034782 -4.2946839 -4.2869759 -4.2807217 -4.2736526 -4.2700448 -4.2719269 -4.2745614 -4.2824197 -4.2949219 -4.3092828 -4.3255072 -4.3410664 -4.3538346][-4.3503561 -4.3434305 -4.3405595 -4.3384142 -4.3362818 -4.3340812 -4.333571 -4.3351364 -4.3384748 -4.3425832 -4.348258 -4.354105 -4.3604193 -4.3661742 -4.370295][-4.3630362 -4.3585038 -4.3566518 -4.3551092 -4.35435 -4.354579 -4.3557968 -4.3573194 -4.3599524 -4.3626213 -4.3656459 -4.3680449 -4.3702826 -4.37223 -4.3733892]]...]
INFO - root - 2017-12-05 17:19:53.937760: step 25910, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 77h:02m:08s remains)
INFO - root - 2017-12-05 17:20:02.964686: step 25920, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 75h:09m:13s remains)
INFO - root - 2017-12-05 17:20:12.073663: step 25930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 78h:54m:34s remains)
INFO - root - 2017-12-05 17:20:21.343574: step 25940, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 78h:22m:43s remains)
INFO - root - 2017-12-05 17:20:30.253573: step 25950, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 71h:40m:39s remains)
INFO - root - 2017-12-05 17:20:39.270870: step 25960, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 72h:14m:57s remains)
INFO - root - 2017-12-05 17:20:48.337415: step 25970, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 75h:19m:49s remains)
INFO - root - 2017-12-05 17:20:57.364650: step 25980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 72h:55m:06s remains)
INFO - root - 2017-12-05 17:21:06.241081: step 25990, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 78h:23m:48s remains)
INFO - root - 2017-12-05 17:21:15.354644: step 26000, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 78h:32m:08s remains)
2017-12-05 17:21:16.119349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1807017 -4.187036 -4.1943808 -4.1966786 -4.1996164 -4.2039313 -4.2023449 -4.1986704 -4.2041469 -4.2159438 -4.2330036 -4.2586341 -4.2832503 -4.2974162 -4.306489][-4.1223736 -4.1311955 -4.1459184 -4.1545529 -4.160954 -4.1660323 -4.1593847 -4.1492572 -4.1526446 -4.1666393 -4.1911368 -4.2263341 -4.2607079 -4.2823586 -4.2970119][-4.0899148 -4.1019 -4.1239257 -4.1367259 -4.142982 -4.1444912 -4.1286922 -4.1069055 -4.1043129 -4.1192293 -4.14776 -4.1903982 -4.2350745 -4.2656918 -4.28677][-4.0913606 -4.1065512 -4.1296873 -4.1409397 -4.1424351 -4.1365566 -4.1096077 -4.0689073 -4.0546412 -4.0715942 -4.1072135 -4.159811 -4.2152934 -4.2541838 -4.2800541][-4.1024981 -4.1165185 -4.13588 -4.1434693 -4.13614 -4.1166744 -4.0723934 -4.0058832 -3.9807162 -4.0113573 -4.0648384 -4.1275315 -4.1945009 -4.24271 -4.2741203][-4.1199884 -4.1287465 -4.1406279 -4.1410818 -4.1209412 -4.0807438 -4.0062137 -3.9056389 -3.8746634 -3.9360356 -4.0183964 -4.0931726 -4.17271 -4.2329721 -4.2700076][-4.1348133 -4.1355972 -4.135632 -4.1283321 -4.0952539 -4.0330868 -3.9217784 -3.7882848 -3.7698474 -3.8772974 -3.9909763 -4.076757 -4.1629319 -4.229681 -4.2688107][-4.1476803 -4.14099 -4.1340971 -4.116931 -4.0704618 -3.9936192 -3.8662393 -3.7408526 -3.7574873 -3.8855805 -4.0044737 -4.0901732 -4.17129 -4.2351046 -4.2729454][-4.1622252 -4.1568532 -4.1475644 -4.1196084 -4.0668087 -3.9925966 -3.8888531 -3.8093436 -3.8447323 -3.9530492 -4.0526781 -4.1270928 -4.1965837 -4.2501278 -4.2813497][-4.182847 -4.1840043 -4.1734118 -4.1451025 -4.0979838 -4.0371952 -3.9609735 -3.9094748 -3.9387205 -4.0157452 -4.0934339 -4.1558013 -4.2161503 -4.262053 -4.2871127][-4.2044873 -4.2064762 -4.1979446 -4.17831 -4.1462808 -4.098309 -4.032691 -3.9873755 -4.0079684 -4.0650091 -4.1234627 -4.1730957 -4.228519 -4.2697434 -4.2893791][-4.2183342 -4.2225909 -4.2219491 -4.2126307 -4.1923656 -4.1517982 -4.085124 -4.0333056 -4.0442019 -4.0918536 -4.1378741 -4.1803489 -4.2337627 -4.2726021 -4.2896891][-4.2305455 -4.2397213 -4.2459826 -4.2473645 -4.237318 -4.1963444 -4.1184187 -4.0470457 -4.0456576 -4.09438 -4.1406322 -4.1848869 -4.2373419 -4.2750235 -4.2897248][-4.2505279 -4.2579174 -4.2610669 -4.26598 -4.2567129 -4.2136459 -4.1248765 -4.0446653 -4.0427427 -4.0967245 -4.1517882 -4.20032 -4.2486725 -4.280479 -4.2912478][-4.2787442 -4.2784004 -4.2705078 -4.2670374 -4.2548046 -4.2131071 -4.1300588 -4.0634108 -4.068409 -4.1228518 -4.1802216 -4.2288985 -4.2701755 -4.2909584 -4.2954011]]...]
INFO - root - 2017-12-05 17:21:25.285711: step 26010, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 79h:51m:26s remains)
INFO - root - 2017-12-05 17:21:34.343310: step 26020, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 80h:30m:19s remains)
INFO - root - 2017-12-05 17:21:43.666988: step 26030, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 77h:47m:46s remains)
INFO - root - 2017-12-05 17:21:52.712653: step 26040, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 75h:24m:47s remains)
INFO - root - 2017-12-05 17:22:01.700976: step 26050, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 76h:49m:59s remains)
INFO - root - 2017-12-05 17:22:10.740880: step 26060, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 76h:26m:51s remains)
INFO - root - 2017-12-05 17:22:19.878661: step 26070, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 77h:15m:21s remains)
INFO - root - 2017-12-05 17:22:28.912139: step 26080, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 75h:53m:17s remains)
INFO - root - 2017-12-05 17:22:37.848376: step 26090, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 74h:35m:00s remains)
INFO - root - 2017-12-05 17:22:46.856647: step 26100, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 77h:38m:06s remains)
2017-12-05 17:22:47.676814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26865 -4.2657785 -4.2718124 -4.2818761 -4.2772427 -4.25452 -4.2289505 -4.2107987 -4.2017097 -4.2102351 -4.2243562 -4.2351179 -4.2300215 -4.2055392 -4.1852026][-4.2679648 -4.2624259 -4.2679057 -4.27674 -4.2651453 -4.2360377 -4.2081795 -4.1928091 -4.1926022 -4.2126942 -4.2340403 -4.246067 -4.237288 -4.2103238 -4.1928039][-4.2610016 -4.2541251 -4.2606683 -4.266212 -4.2463727 -4.2145133 -4.1886044 -4.1807008 -4.1949782 -4.2250972 -4.2498541 -4.2612867 -4.2472315 -4.218791 -4.2037477][-4.2545 -4.2427192 -4.24792 -4.2453785 -4.2138915 -4.1757603 -4.1529021 -4.1624393 -4.1986003 -4.2371941 -4.2616086 -4.2722969 -4.2592592 -4.2291446 -4.2102237][-4.2369347 -4.22247 -4.2280173 -4.2176814 -4.1757193 -4.1281977 -4.1019535 -4.129077 -4.1864529 -4.2321987 -4.2562647 -4.2701364 -4.2632465 -4.234561 -4.2115769][-4.2089553 -4.1948757 -4.2023978 -4.18618 -4.1348891 -4.0747361 -4.0412135 -4.0879607 -4.164443 -4.2153444 -4.2382298 -4.2530909 -4.24978 -4.2220931 -4.1943359][-4.1875081 -4.1762304 -4.1779852 -4.1519818 -4.09235 -4.0185561 -3.9801681 -4.0494766 -4.142117 -4.1987715 -4.2253618 -4.2393069 -4.2356043 -4.2067 -4.1718454][-4.1923137 -4.17782 -4.1672158 -4.1293206 -4.0641079 -3.9809189 -3.9429991 -4.0266871 -4.1243291 -4.1868935 -4.2186766 -4.2347517 -4.2270026 -4.19326 -4.1524305][-4.2075109 -4.1884885 -4.1660981 -4.1208081 -4.0564075 -3.9778311 -3.956058 -4.0430255 -4.1301446 -4.19164 -4.2255611 -4.2426624 -4.2304611 -4.19609 -4.1511579][-4.2273922 -4.2101703 -4.1855564 -4.1428914 -4.087389 -4.0236783 -4.0203452 -4.0943308 -4.1639843 -4.215188 -4.2445455 -4.2582064 -4.2444448 -4.2093382 -4.1592484][-4.255435 -4.2399607 -4.2153654 -4.179626 -4.133842 -4.0852618 -4.0887723 -4.1476717 -4.2020173 -4.2422462 -4.265255 -4.2718439 -4.253437 -4.2147007 -4.1614304][-4.2757421 -4.2610312 -4.2383265 -4.2054248 -4.161025 -4.1209025 -4.131897 -4.1873345 -4.2360034 -4.2653346 -4.27793 -4.2775855 -4.2581935 -4.21916 -4.1681705][-4.2812915 -4.2667079 -4.2473378 -4.217412 -4.1736913 -4.1413136 -4.1582408 -4.21376 -4.260663 -4.2798696 -4.2828922 -4.2790055 -4.2600737 -4.2209182 -4.1744838][-4.28074 -4.2641644 -4.2464342 -4.2219219 -4.1842971 -4.1588411 -4.177084 -4.2289562 -4.2711024 -4.2822857 -4.2805481 -4.2742724 -4.2548304 -4.2144213 -4.1720133][-4.2822018 -4.266078 -4.2498889 -4.230175 -4.2002721 -4.1811442 -4.1987014 -4.243125 -4.2753973 -4.2786536 -4.2718229 -4.2628965 -4.2449136 -4.20495 -4.1642842]]...]
INFO - root - 2017-12-05 17:22:56.725956: step 26110, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 77h:46m:02s remains)
INFO - root - 2017-12-05 17:23:05.771054: step 26120, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 75h:49m:14s remains)
INFO - root - 2017-12-05 17:23:14.815711: step 26130, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 77h:01m:55s remains)
INFO - root - 2017-12-05 17:23:23.713463: step 26140, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 76h:17m:49s remains)
INFO - root - 2017-12-05 17:23:32.833444: step 26150, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 75h:35m:42s remains)
INFO - root - 2017-12-05 17:23:42.010811: step 26160, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 78h:17m:01s remains)
INFO - root - 2017-12-05 17:23:50.732226: step 26170, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 75h:04m:30s remains)
INFO - root - 2017-12-05 17:23:59.861185: step 26180, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 76h:54m:53s remains)
INFO - root - 2017-12-05 17:24:08.847206: step 26190, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 76h:47m:57s remains)
INFO - root - 2017-12-05 17:24:17.915559: step 26200, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 77h:03m:38s remains)
2017-12-05 17:24:18.807702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2485094 -4.2681465 -4.2789621 -4.2791066 -4.2738032 -4.27167 -4.2749596 -4.2816367 -4.2861633 -4.2874565 -4.2848525 -4.278163 -4.2706552 -4.2674379 -4.2667565][-4.2323771 -4.2551465 -4.2665358 -4.2618113 -4.250977 -4.2453084 -4.2498384 -4.2595868 -4.26501 -4.2680049 -4.268229 -4.2618155 -4.253602 -4.2475266 -4.2445316][-4.2226748 -4.2454891 -4.2556276 -4.2464943 -4.22973 -4.2171092 -4.2171812 -4.2277126 -4.2352347 -4.2406335 -4.2459722 -4.2425246 -4.2379427 -4.2297153 -4.2246938][-4.2048359 -4.2280536 -4.2391548 -4.2275515 -4.2067652 -4.1860061 -4.1769404 -4.1831307 -4.1952868 -4.2098379 -4.224329 -4.2260842 -4.2245116 -4.2138877 -4.2018461][-4.1758909 -4.1936235 -4.2070107 -4.2000937 -4.1775708 -4.1449885 -4.1202374 -4.1196427 -4.1429148 -4.1759086 -4.2024961 -4.2106895 -4.2114925 -4.1976147 -4.1750827][-4.1421285 -4.1504354 -4.16049 -4.1556067 -4.1276107 -4.0714588 -4.0176144 -4.0180154 -4.070857 -4.1298881 -4.1709518 -4.1875987 -4.1941743 -4.1773896 -4.1401086][-4.1283154 -4.1246996 -4.1226473 -4.1064363 -4.061933 -3.9724159 -3.8761625 -3.881511 -3.9783936 -4.0664239 -4.1173277 -4.139544 -4.1551652 -4.1367526 -4.0789866][-4.1447854 -4.1324387 -4.1212435 -4.0968995 -4.041688 -3.9365573 -3.8246026 -3.8359509 -3.9476926 -4.03737 -4.0843716 -4.1043878 -4.121489 -4.0975227 -4.022511][-4.1594806 -4.1484985 -4.1382833 -4.1181426 -4.0769839 -4.005271 -3.9364004 -3.9454341 -4.0153527 -4.0733504 -4.10366 -4.1152143 -4.1250696 -4.0930066 -4.017004][-4.1518078 -4.1439042 -4.1380138 -4.1285281 -4.1140418 -4.0830064 -4.0592246 -4.0672712 -4.1008849 -4.1311026 -4.1435456 -4.1459551 -4.1508236 -4.1220536 -4.0661526][-4.1383905 -4.1265836 -4.1206059 -4.1232228 -4.1324391 -4.1303144 -4.1313729 -4.1404037 -4.1621747 -4.1830454 -4.189599 -4.1872625 -4.1898313 -4.16968 -4.1347122][-4.1302266 -4.1182308 -4.110827 -4.1203575 -4.1426883 -4.1567745 -4.1687961 -4.1787581 -4.1996369 -4.2218366 -4.2299647 -4.2285333 -4.2267847 -4.2141328 -4.1936817][-4.131618 -4.1204185 -4.1078844 -4.1172071 -4.1419988 -4.1653204 -4.18411 -4.1992731 -4.2237453 -4.2468133 -4.2575326 -4.2570596 -4.2530475 -4.2435389 -4.2326522][-4.1463089 -4.1332588 -4.11733 -4.1244264 -4.1443653 -4.1687098 -4.1927547 -4.2166 -4.2478342 -4.2740531 -4.2854509 -4.2850146 -4.2805462 -4.2711549 -4.2644553][-4.1569028 -4.144784 -4.135149 -4.1452909 -4.1627488 -4.1864586 -4.2157569 -4.2460818 -4.2807026 -4.3082566 -4.3190212 -4.3160987 -4.3090653 -4.3021 -4.2971158]]...]
INFO - root - 2017-12-05 17:24:28.016672: step 26210, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.916 sec/batch; 77h:56m:21s remains)
INFO - root - 2017-12-05 17:24:36.970162: step 26220, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 78h:14m:29s remains)
INFO - root - 2017-12-05 17:24:46.094085: step 26230, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 78h:28m:14s remains)
INFO - root - 2017-12-05 17:24:55.002831: step 26240, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 80h:08m:29s remains)
INFO - root - 2017-12-05 17:25:03.998591: step 26250, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 75h:58m:04s remains)
INFO - root - 2017-12-05 17:25:13.047345: step 26260, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 78h:10m:22s remains)
INFO - root - 2017-12-05 17:25:22.161041: step 26270, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 76h:26m:52s remains)
INFO - root - 2017-12-05 17:25:31.298287: step 26280, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 77h:30m:04s remains)
INFO - root - 2017-12-05 17:25:40.296248: step 26290, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 76h:28m:30s remains)
INFO - root - 2017-12-05 17:25:49.270071: step 26300, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 77h:48m:45s remains)
2017-12-05 17:25:50.039230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0745339 -4.0509534 -4.0302396 -4.0272994 -4.0637264 -4.1103435 -4.108151 -4.0672746 -4.0328493 -4.0144839 -3.9946225 -3.999105 -4.0047474 -4.0137053 -4.0528][-4.0571308 -4.04353 -4.0272636 -4.0308967 -4.0764923 -4.1291065 -4.1355934 -4.0966086 -4.0518351 -4.0145535 -3.9746227 -3.9522996 -3.9379604 -3.9447834 -3.9918039][-4.0941753 -4.0954671 -4.0835733 -4.0852637 -4.1204491 -4.1586413 -4.16299 -4.1311736 -4.0940981 -4.05945 -4.0116191 -3.9621017 -3.92312 -3.9206095 -3.9696279][-4.11848 -4.1264162 -4.1172991 -4.1124411 -4.1351275 -4.1569781 -4.16027 -4.1474371 -4.131712 -4.1173668 -4.0807619 -4.0235677 -3.9768965 -3.9743919 -4.0176816][-4.1326966 -4.132257 -4.1161494 -4.1062446 -4.1164536 -4.1243749 -4.1235142 -4.1192875 -4.1313686 -4.1415172 -4.1257019 -4.0873041 -4.0524588 -4.0522113 -4.0878849][-4.1688452 -4.1539044 -4.1233978 -4.0892005 -4.0658436 -4.04268 -4.0190482 -4.004962 -4.0459442 -4.1041684 -4.1214809 -4.1142759 -4.10526 -4.1149607 -4.1437469][-4.2230487 -4.198092 -4.148788 -4.0841441 -4.0152717 -3.9359112 -3.8511355 -3.7884831 -3.8580058 -3.9919934 -4.0616903 -4.0882907 -4.1083603 -4.1334152 -4.1579614][-4.274981 -4.2421141 -4.1776528 -4.0948529 -3.9918983 -3.862114 -3.7062626 -3.5554321 -3.6257048 -3.844063 -3.9772553 -4.0396419 -4.0838165 -4.1188145 -4.134439][-4.3137455 -4.2793131 -4.2133904 -4.1275339 -4.01808 -3.8894315 -3.7404859 -3.5834575 -3.6116319 -3.8119383 -3.9509902 -4.0145373 -4.0609703 -4.0963731 -4.1096225][-4.3239355 -4.2940011 -4.2414918 -4.1710744 -4.0817003 -3.9928536 -3.9039457 -3.8164923 -3.8175735 -3.9230633 -3.9992959 -4.02467 -4.052074 -4.0885777 -4.1014905][-4.3059597 -4.2798357 -4.2430825 -4.1996779 -4.1433582 -4.0919504 -4.0532928 -4.0148439 -4.0122676 -4.0496464 -4.064611 -4.0548587 -4.0636492 -4.0976386 -4.1094279][-4.2636776 -4.245966 -4.225728 -4.2083879 -4.1845827 -4.1662369 -4.1539059 -4.1405044 -4.1311979 -4.132884 -4.1195354 -4.0990591 -4.1027369 -4.1303864 -4.1437335][-4.222652 -4.218298 -4.2138119 -4.214479 -4.2147708 -4.21725 -4.2173719 -4.2131586 -4.2024503 -4.1877303 -4.16367 -4.1449895 -4.146657 -4.1674709 -4.18113][-4.2130032 -4.2169476 -4.2214274 -4.2291503 -4.2367029 -4.24473 -4.2484469 -4.2488074 -4.242219 -4.2243223 -4.1983833 -4.1797314 -4.1746292 -4.1853547 -4.2001739][-4.2265663 -4.2337523 -4.2431183 -4.2521944 -4.2547417 -4.2580495 -4.26127 -4.2639542 -4.2633758 -4.2504134 -4.2308903 -4.21571 -4.2064939 -4.2098503 -4.2203097]]...]
INFO - root - 2017-12-05 17:25:59.034933: step 26310, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.902 sec/batch; 76h:44m:28s remains)
INFO - root - 2017-12-05 17:26:08.100907: step 26320, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 77h:37m:09s remains)
INFO - root - 2017-12-05 17:26:17.233390: step 26330, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 80h:48m:44s remains)
INFO - root - 2017-12-05 17:26:26.472448: step 26340, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 80h:32m:05s remains)
INFO - root - 2017-12-05 17:26:35.600084: step 26350, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.875 sec/batch; 74h:24m:18s remains)
INFO - root - 2017-12-05 17:26:44.513852: step 26360, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 78h:32m:13s remains)
INFO - root - 2017-12-05 17:26:53.562104: step 26370, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 74h:48m:27s remains)
INFO - root - 2017-12-05 17:27:02.556418: step 26380, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 78h:03m:24s remains)
INFO - root - 2017-12-05 17:27:11.544509: step 26390, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 77h:57m:47s remains)
INFO - root - 2017-12-05 17:27:20.804681: step 26400, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 78h:30m:38s remains)
2017-12-05 17:27:21.564010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3179064 -4.309073 -4.3000565 -4.2953434 -4.295866 -4.3052692 -4.3216863 -4.3349724 -4.3422 -4.3436394 -4.3390384 -4.3310504 -4.3231831 -4.318006 -4.3175063][-4.3112226 -4.2920375 -4.2723708 -4.260263 -4.259541 -4.274075 -4.2993231 -4.3224411 -4.3381619 -4.3466115 -4.3464303 -4.3400922 -4.3330293 -4.3277106 -4.3266926][-4.3035812 -4.2704382 -4.2329636 -4.2050529 -4.1990857 -4.21903 -4.2534952 -4.2877474 -4.3165445 -4.3380728 -4.3474684 -4.3457847 -4.3406639 -4.3353891 -4.3331904][-4.2995195 -4.2541628 -4.195466 -4.1438513 -4.1239161 -4.1427217 -4.1818891 -4.2263808 -4.272429 -4.312912 -4.3379269 -4.347229 -4.3488097 -4.3461914 -4.3433285][-4.29982 -4.2503791 -4.1768 -4.1011419 -4.0570841 -4.0593696 -4.0897465 -4.1391354 -4.2050934 -4.2684641 -4.3123274 -4.3372455 -4.3503828 -4.3545513 -4.353519][-4.3019118 -4.2575359 -4.1814137 -4.0906115 -4.018218 -3.9852734 -3.9855475 -4.02891 -4.1157093 -4.2049255 -4.2697659 -4.3118482 -4.3384972 -4.3526878 -4.3568769][-4.3062468 -4.2735009 -4.2069259 -4.1150823 -4.0219259 -3.9464307 -3.8987842 -3.9216056 -4.0186181 -4.1281748 -4.2121029 -4.2723193 -4.31395 -4.3399873 -4.3518143][-4.3118439 -4.291913 -4.2429748 -4.1655917 -4.0710077 -3.9695761 -3.8815069 -3.8702581 -3.9510789 -4.0588822 -4.1510015 -4.2240963 -4.2798567 -4.3188877 -4.3397112][-4.3165417 -4.3076453 -4.2782636 -4.2252016 -4.1482339 -4.0491996 -3.9503188 -3.9094443 -3.9492483 -4.0281672 -4.10993 -4.1826806 -4.2450476 -4.2938104 -4.3230047][-4.3195767 -4.3184948 -4.3044753 -4.2749057 -4.22407 -4.1475677 -4.062171 -4.0076828 -4.0070295 -4.0453577 -4.1019578 -4.1609678 -4.2186551 -4.2703013 -4.3049107][-4.3201218 -4.3243051 -4.3206344 -4.3082356 -4.2813854 -4.2355351 -4.17671 -4.1264176 -4.10261 -4.1074328 -4.1337347 -4.1686163 -4.2108436 -4.2560191 -4.2897491][-4.3205218 -4.3276134 -4.3290529 -4.325717 -4.3152347 -4.29611 -4.2663236 -4.2333288 -4.2076573 -4.1962352 -4.1987948 -4.2082658 -4.2291536 -4.2600713 -4.2858791][-4.3213038 -4.3318157 -4.3352432 -4.3337512 -4.329896 -4.32504 -4.3149486 -4.3003254 -4.2858429 -4.2750783 -4.2686648 -4.2636132 -4.2674146 -4.2814636 -4.2955689][-4.3172107 -4.3310809 -4.3355894 -4.33361 -4.330512 -4.3294954 -4.3277979 -4.3247137 -4.3214946 -4.3176093 -4.3130536 -4.3067484 -4.3040323 -4.3064427 -4.3109756][-4.3075066 -4.3220544 -4.3261485 -4.3232784 -4.3199596 -4.3188276 -4.3191657 -4.3214531 -4.3243885 -4.3261557 -4.3265138 -4.3249874 -4.3234215 -4.3224158 -4.3229322]]...]
INFO - root - 2017-12-05 17:27:30.619684: step 26410, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 79h:53m:18s remains)
INFO - root - 2017-12-05 17:27:39.500693: step 26420, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 72h:15m:08s remains)
INFO - root - 2017-12-05 17:27:48.566810: step 26430, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 77h:33m:44s remains)
INFO - root - 2017-12-05 17:27:57.698736: step 26440, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 78h:05m:39s remains)
INFO - root - 2017-12-05 17:28:06.635498: step 26450, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.810 sec/batch; 68h:53m:53s remains)
INFO - root - 2017-12-05 17:28:15.745807: step 26460, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 75h:17m:44s remains)
INFO - root - 2017-12-05 17:28:24.866271: step 26470, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 77h:16m:08s remains)
INFO - root - 2017-12-05 17:28:34.078486: step 26480, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 79h:23m:40s remains)
INFO - root - 2017-12-05 17:28:43.158013: step 26490, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 74h:57m:38s remains)
INFO - root - 2017-12-05 17:28:52.138456: step 26500, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 76h:40m:00s remains)
2017-12-05 17:28:53.193411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.270206 -4.2670689 -4.2507138 -4.2244649 -4.2019496 -4.1953235 -4.1867595 -4.1702523 -4.1564407 -4.1716466 -4.2075276 -4.2490158 -4.2740984 -4.2762141 -4.2552128][-4.2864013 -4.2835021 -4.260231 -4.225596 -4.1970572 -4.1849561 -4.1738153 -4.1636219 -4.1650295 -4.1945896 -4.2360563 -4.2797389 -4.3025312 -4.3003478 -4.2704024][-4.2978368 -4.293004 -4.2597551 -4.2168484 -4.1833687 -4.1723995 -4.1699133 -4.1700425 -4.183527 -4.2129364 -4.2506094 -4.2922149 -4.3096657 -4.3017712 -4.263998][-4.2948952 -4.2827783 -4.2392545 -4.1896238 -4.1602912 -4.1603775 -4.16654 -4.1692533 -4.1762576 -4.1980071 -4.23612 -4.2780981 -4.2882824 -4.2717581 -4.22512][-4.280551 -4.2607708 -4.2115941 -4.1642194 -4.1445951 -4.1433525 -4.1378675 -4.1227441 -4.113524 -4.138638 -4.1914253 -4.2412033 -4.2481461 -4.2227979 -4.1727686][-4.2597823 -4.240675 -4.196331 -4.1559134 -4.1330295 -4.1084747 -4.0626688 -4.0030775 -3.9778178 -4.0265932 -4.1092825 -4.1638718 -4.16594 -4.1389222 -4.1011243][-4.2350483 -4.2209845 -4.1861877 -4.1494303 -4.1077132 -4.0411139 -3.9330626 -3.8154655 -3.8054857 -3.9130087 -4.0303955 -4.0973763 -4.1103826 -4.1039538 -4.0970879][-4.1999936 -4.1902943 -4.167294 -4.1390486 -4.090621 -4.00802 -3.885344 -3.7800572 -3.829422 -3.9681184 -4.0883861 -4.1532974 -4.1709909 -4.1715794 -4.1741657][-4.1797485 -4.1808944 -4.1778264 -4.1722507 -4.1417427 -4.0815659 -4.0038829 -3.9627891 -4.0160871 -4.1115265 -4.1894526 -4.2298942 -4.2426867 -4.2426395 -4.2392244][-4.19417 -4.2028966 -4.2109437 -4.2173939 -4.2046762 -4.1725278 -4.1331778 -4.1207376 -4.1515365 -4.2010837 -4.2404914 -4.2634773 -4.2774043 -4.2840691 -4.2780857][-4.2008457 -4.2121415 -4.2222333 -4.2286787 -4.2248421 -4.2072854 -4.1839571 -4.176785 -4.1954532 -4.2246079 -4.2443671 -4.2588663 -4.2758193 -4.2875075 -4.2842946][-4.1910419 -4.2032819 -4.2102203 -4.2140141 -4.2107453 -4.19549 -4.1765876 -4.1741896 -4.1947703 -4.2183862 -4.2314968 -4.2460351 -4.2683711 -4.2860851 -4.2859874][-4.1907196 -4.194572 -4.1961641 -4.1966696 -4.1928463 -4.1795077 -4.1685023 -4.1728277 -4.194129 -4.2139921 -4.226182 -4.2398157 -4.2616262 -4.2798419 -4.2819104][-4.2126112 -4.2076907 -4.2006211 -4.1923614 -4.1836867 -4.1744471 -4.1723585 -4.1835837 -4.2041836 -4.2194395 -4.2293591 -4.2399678 -4.2580957 -4.2733078 -4.2747803][-4.2352505 -4.2222252 -4.2081637 -4.1961823 -4.1902657 -4.1869731 -4.1888781 -4.2004271 -4.215404 -4.2193913 -4.2212563 -4.226615 -4.2399712 -4.2534122 -4.2545824]]...]
INFO - root - 2017-12-05 17:29:01.991168: step 26510, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 77h:43m:57s remains)
INFO - root - 2017-12-05 17:29:11.082533: step 26520, loss = 2.04, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 81h:35m:24s remains)
INFO - root - 2017-12-05 17:29:20.188266: step 26530, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 75h:39m:26s remains)
INFO - root - 2017-12-05 17:29:29.310308: step 26540, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 76h:11m:13s remains)
INFO - root - 2017-12-05 17:29:38.461340: step 26550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 77h:57m:08s remains)
INFO - root - 2017-12-05 17:29:47.496763: step 26560, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 75h:28m:23s remains)
INFO - root - 2017-12-05 17:29:56.457598: step 26570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 78h:49m:56s remains)
INFO - root - 2017-12-05 17:30:05.503291: step 26580, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 73h:01m:40s remains)
INFO - root - 2017-12-05 17:30:14.567438: step 26590, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 78h:38m:56s remains)
INFO - root - 2017-12-05 17:30:23.610391: step 26600, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 78h:02m:19s remains)
2017-12-05 17:30:24.421720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.289289 -4.2900243 -4.2881336 -4.2882681 -4.29049 -4.2945113 -4.2920227 -4.2839851 -4.2766871 -4.2731428 -4.2768683 -4.286 -4.2912974 -4.2883377 -4.2767334][-4.3102493 -4.3063126 -4.3001947 -4.3002357 -4.3017454 -4.3063779 -4.3087525 -4.3053145 -4.2979193 -4.2917538 -4.287591 -4.2919979 -4.29952 -4.3010926 -4.2929487][-4.3208456 -4.3101873 -4.2994471 -4.2989459 -4.2992425 -4.2994747 -4.3012352 -4.3041596 -4.3039489 -4.30001 -4.2904563 -4.2868652 -4.2957969 -4.3040395 -4.3027911][-4.309649 -4.2859254 -4.2664437 -4.2618208 -4.2559471 -4.2437692 -4.2385464 -4.2488909 -4.2613883 -4.2696652 -4.2634625 -4.2578435 -4.2715898 -4.2899675 -4.2955508][-4.2833128 -4.2433386 -4.213573 -4.2014551 -4.1820912 -4.1518016 -4.1373544 -4.1587315 -4.1877966 -4.2124057 -4.21307 -4.2040863 -4.2187443 -4.2470441 -4.263586][-4.2464237 -4.1889114 -4.1477761 -4.120501 -4.0769997 -4.0171218 -3.9875169 -4.0287123 -4.0870252 -4.1325421 -4.1425538 -4.125536 -4.1333051 -4.1679206 -4.2011046][-4.2070284 -4.134377 -4.0838003 -4.0422206 -3.9742706 -3.8813047 -3.8203661 -3.8725622 -3.9682391 -4.037396 -4.054637 -4.031271 -4.0269132 -4.0652351 -4.1153636][-4.1859012 -4.1119161 -4.0586529 -4.0181532 -3.9527135 -3.8619976 -3.7930455 -3.8262591 -3.9238904 -3.9997501 -4.0200038 -3.9967139 -3.9811916 -4.0031905 -4.0485611][-4.1955 -4.14848 -4.10783 -4.07636 -4.0335264 -3.9782751 -3.9350259 -3.9468951 -4.0015054 -4.0532351 -4.0723777 -4.0607362 -4.04915 -4.0555258 -4.081111][-4.20364 -4.1908207 -4.1751914 -4.1611366 -4.144527 -4.1226668 -4.1009135 -4.0995159 -4.1175194 -4.139545 -4.1505861 -4.1485472 -4.1453204 -4.1484847 -4.1623917][-4.1895685 -4.2004347 -4.2112527 -4.2184916 -4.2229013 -4.2228093 -4.21631 -4.2131414 -4.2163596 -4.2231054 -4.2292747 -4.2336082 -4.236012 -4.2384534 -4.2452688][-4.1902471 -4.2138443 -4.2377014 -4.2570295 -4.2707629 -4.2774253 -4.2772846 -4.274817 -4.2748752 -4.2758975 -4.2794795 -4.2856288 -4.2895269 -4.2927876 -4.2953272][-4.2224255 -4.2414441 -4.2602797 -4.2763758 -4.2873168 -4.2907166 -4.2905188 -4.2902107 -4.2915096 -4.2946815 -4.2999067 -4.3055196 -4.3094172 -4.3124 -4.3121729][-4.2662725 -4.2745147 -4.2804241 -4.2856917 -4.2889495 -4.28836 -4.2874827 -4.2898631 -4.293632 -4.2991695 -4.3061051 -4.3126278 -4.3165183 -4.3152814 -4.3103132][-4.3035107 -4.3049555 -4.3008571 -4.2960954 -4.2927604 -4.2891941 -4.2878232 -4.2917104 -4.2979736 -4.305182 -4.311841 -4.3168378 -4.3182144 -4.3136182 -4.304225]]...]
INFO - root - 2017-12-05 17:30:33.340771: step 26610, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 71h:40m:43s remains)
INFO - root - 2017-12-05 17:30:42.419497: step 26620, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.930 sec/batch; 78h:59m:46s remains)
INFO - root - 2017-12-05 17:30:51.536416: step 26630, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 76h:13m:05s remains)
INFO - root - 2017-12-05 17:31:00.469196: step 26640, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 77h:52m:30s remains)
INFO - root - 2017-12-05 17:31:09.482753: step 26650, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 76h:41m:44s remains)
INFO - root - 2017-12-05 17:31:18.546266: step 26660, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 77h:08m:09s remains)
INFO - root - 2017-12-05 17:31:27.862004: step 26670, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.958 sec/batch; 81h:21m:15s remains)
INFO - root - 2017-12-05 17:31:37.030970: step 26680, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 77h:19m:15s remains)
INFO - root - 2017-12-05 17:31:46.122827: step 26690, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 75h:18m:33s remains)
INFO - root - 2017-12-05 17:31:55.193901: step 26700, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 77h:41m:18s remains)
2017-12-05 17:31:56.042036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275528 -4.2766495 -4.2738366 -4.269732 -4.2731748 -4.2810493 -4.2828603 -4.2821255 -4.2837491 -4.2858682 -4.2850294 -4.2829809 -4.2765865 -4.2649684 -4.2582026][-4.2606187 -4.2651858 -4.2631764 -4.2541585 -4.2562819 -4.26609 -4.2657709 -4.2600727 -4.2584143 -4.2629137 -4.2663422 -4.2652464 -4.2567964 -4.2428579 -4.2352209][-4.2442136 -4.25268 -4.2526727 -4.2399421 -4.2380829 -4.2433977 -4.2352552 -4.2207818 -4.2135105 -4.2214422 -4.2345281 -4.2378511 -4.2302523 -4.2212148 -4.2191796][-4.2281604 -4.2385926 -4.2393765 -4.226234 -4.2188039 -4.2145767 -4.196198 -4.1708665 -4.1575079 -4.1720848 -4.1997824 -4.2107296 -4.2063246 -4.2072845 -4.2144289][-4.2110324 -4.2125072 -4.2096195 -4.1971946 -4.1872921 -4.1735911 -4.1456819 -4.1147237 -4.1028018 -4.1319456 -4.1734242 -4.1869216 -4.1856513 -4.1951828 -4.2096558][-4.1969204 -4.1860776 -4.1742649 -4.1579003 -4.1405063 -4.1102481 -4.0670595 -4.0372219 -4.0509114 -4.1065083 -4.1520782 -4.1615944 -4.1614642 -4.1720724 -4.1895866][-4.1765032 -4.1615205 -4.1407108 -4.1175 -4.0884867 -4.0346031 -3.9594626 -3.9290771 -3.9893289 -4.0833397 -4.1402068 -4.1518354 -4.1527224 -4.1604347 -4.1764431][-4.1433115 -4.1278992 -4.105618 -4.083107 -4.0513926 -3.9715436 -3.8527284 -3.8201656 -3.9331861 -4.0613861 -4.1317716 -4.1501074 -4.1491938 -4.14888 -4.1610074][-4.1097417 -4.0957 -4.0809913 -4.0723462 -4.0515971 -3.9694114 -3.843437 -3.8195121 -3.9471297 -4.0750022 -4.1392069 -4.154705 -4.1452093 -4.1360688 -4.1399465][-4.1072683 -4.0920076 -4.0835223 -4.0868092 -4.0790849 -4.0198865 -3.9395683 -3.9389641 -4.0387855 -4.1269984 -4.1690292 -4.1703558 -4.1523571 -4.1340575 -4.1286879][-4.1391692 -4.121789 -4.1136761 -4.1210918 -4.1164627 -4.077919 -4.033442 -4.0450115 -4.121244 -4.1818752 -4.209177 -4.20333 -4.181211 -4.152441 -4.13665][-4.1673265 -4.1522655 -4.1462607 -4.1545253 -4.1511235 -4.11918 -4.0863438 -4.1002436 -4.1623254 -4.211937 -4.236917 -4.2359118 -4.2168689 -4.1829553 -4.160655][-4.1762657 -4.1732025 -4.1733904 -4.1809258 -4.1782732 -4.1516519 -4.1260657 -4.1384292 -4.1845036 -4.2206225 -4.2441106 -4.25329 -4.2396216 -4.2088714 -4.1859145][-4.1935744 -4.2016315 -4.2064462 -4.2105656 -4.205214 -4.1861329 -4.172627 -4.1805649 -4.2024307 -4.2211695 -4.2404437 -4.2527728 -4.24654 -4.2250133 -4.2102518][-4.2149014 -4.2263179 -4.2333417 -4.2337346 -4.2253785 -4.2138958 -4.2092648 -4.2105536 -4.2154574 -4.2232327 -4.2388315 -4.2510715 -4.2493796 -4.237462 -4.2310796]]...]
INFO - root - 2017-12-05 17:32:05.092962: step 26710, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 76h:39m:20s remains)
INFO - root - 2017-12-05 17:32:14.145721: step 26720, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 78h:29m:10s remains)
INFO - root - 2017-12-05 17:32:23.391784: step 26730, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 76h:26m:21s remains)
INFO - root - 2017-12-05 17:32:32.389894: step 26740, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 78h:49m:55s remains)
INFO - root - 2017-12-05 17:32:41.577133: step 26750, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 75h:33m:35s remains)
INFO - root - 2017-12-05 17:32:50.709246: step 26760, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 77h:45m:26s remains)
INFO - root - 2017-12-05 17:32:59.683118: step 26770, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 76h:20m:40s remains)
INFO - root - 2017-12-05 17:33:08.902852: step 26780, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 78h:18m:16s remains)
INFO - root - 2017-12-05 17:33:18.029605: step 26790, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 79h:23m:17s remains)
INFO - root - 2017-12-05 17:33:26.997921: step 26800, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 78h:16m:26s remains)
2017-12-05 17:33:27.782751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2992344 -4.2655573 -4.2263813 -4.2016039 -4.1866846 -4.1638 -4.1326523 -4.1104712 -4.1242361 -4.1561551 -4.174592 -4.1821146 -4.171247 -4.1377926 -4.083611][-4.3005929 -4.2679648 -4.2279396 -4.1999512 -4.1824675 -4.1545677 -4.1159959 -4.0949359 -4.1113505 -4.1460795 -4.16568 -4.1693516 -4.1534424 -4.1159906 -4.0627503][-4.2976665 -4.2661 -4.2247477 -4.1930089 -4.1741829 -4.1422319 -4.0968256 -4.0723319 -4.0846252 -4.1156459 -4.1379018 -4.1431613 -4.1272645 -4.0958142 -4.0510736][-4.294178 -4.2640796 -4.2222133 -4.1867456 -4.1644936 -4.122901 -4.0644145 -4.0322652 -4.0396223 -4.0674162 -4.0965872 -4.1078253 -4.098341 -4.0804806 -4.0503092][-4.2895555 -4.259738 -4.2165895 -4.173665 -4.1426044 -4.0865235 -4.0103807 -3.9720328 -3.9798226 -4.0122538 -4.05597 -4.0793614 -4.0770583 -4.0681014 -4.0547123][-4.2845559 -4.2535896 -4.2076759 -4.1574507 -4.1178079 -4.0457859 -3.9415898 -3.8896816 -3.9098852 -3.9625013 -4.0275273 -4.0660357 -4.0626645 -4.0521603 -4.0509357][-4.2788615 -4.2455516 -4.1987534 -4.1434813 -4.0927153 -3.9947956 -3.8415165 -3.7660453 -3.8290412 -3.9363959 -4.0281682 -4.0752769 -4.0698667 -4.0515351 -4.0536313][-4.2756104 -4.2408476 -4.194418 -4.1348152 -4.0668621 -3.9372385 -3.7379918 -3.6499634 -3.7796333 -3.937815 -4.0432477 -4.0942259 -4.0959024 -4.08364 -4.0880151][-4.27586 -4.2392721 -4.1904688 -4.1251211 -4.0505338 -3.9300358 -3.7656469 -3.7083097 -3.8403654 -3.9870083 -4.0782747 -4.1175704 -4.1250625 -4.122642 -4.1269484][-4.27783 -4.2390418 -4.1857376 -4.1178856 -4.0498657 -3.9656596 -3.8645864 -3.8360238 -3.9344563 -4.0512 -4.1266613 -4.1586552 -4.1678944 -4.1670465 -4.1655288][-4.2796235 -4.2420506 -4.1912451 -4.1307545 -4.0768275 -4.0172119 -3.9491873 -3.9384265 -4.018641 -4.1161509 -4.1792555 -4.2042341 -4.2146354 -4.2108369 -4.2039633][-4.2778773 -4.2438178 -4.2004695 -4.1516781 -4.1060052 -4.0538397 -4.0036674 -4.0074139 -4.0802808 -4.1573319 -4.2081761 -4.2315245 -4.2443891 -4.239707 -4.2277451][-4.2748423 -4.24519 -4.2085185 -4.1677079 -4.1301041 -4.086864 -4.0495338 -4.0585728 -4.1213441 -4.1777229 -4.2189937 -4.2397938 -4.2485271 -4.2399983 -4.22231][-4.2729859 -4.2478566 -4.217103 -4.1796002 -4.1470714 -4.1130166 -4.0804749 -4.0844355 -4.1329579 -4.1740274 -4.2061677 -4.2229533 -4.2294931 -4.2189336 -4.1979885][-4.2748976 -4.2514687 -4.223629 -4.1864114 -4.1513944 -4.1154866 -4.0782437 -4.0768147 -4.1158729 -4.1505423 -4.1778979 -4.1904783 -4.1986728 -4.1903338 -4.1694765]]...]
INFO - root - 2017-12-05 17:33:36.932446: step 26810, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.920 sec/batch; 78h:07m:23s remains)
INFO - root - 2017-12-05 17:33:45.967878: step 26820, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 75h:58m:37s remains)
INFO - root - 2017-12-05 17:33:55.066599: step 26830, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 80h:21m:14s remains)
INFO - root - 2017-12-05 17:34:04.298540: step 26840, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 77h:03m:01s remains)
INFO - root - 2017-12-05 17:34:13.508035: step 26850, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 79h:05m:54s remains)
INFO - root - 2017-12-05 17:34:22.587051: step 26860, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 75h:21m:28s remains)
INFO - root - 2017-12-05 17:34:31.655249: step 26870, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 76h:24m:46s remains)
INFO - root - 2017-12-05 17:34:40.749329: step 26880, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 77h:30m:53s remains)
INFO - root - 2017-12-05 17:34:49.922187: step 26890, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 76h:40m:46s remains)
INFO - root - 2017-12-05 17:34:59.217272: step 26900, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 77h:52m:20s remains)
2017-12-05 17:35:00.071419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2690396 -4.2731066 -4.2812152 -4.2919788 -4.3012066 -4.3075271 -4.3103371 -4.3073845 -4.3001733 -4.29555 -4.2952547 -4.297473 -4.3008161 -4.3040476 -4.3039074][-4.2569318 -4.2592244 -4.2648535 -4.2738876 -4.2833705 -4.2936687 -4.3013549 -4.2974639 -4.2890339 -4.2837019 -4.2839432 -4.2878075 -4.2937145 -4.3001428 -4.2998476][-4.2312374 -4.2241893 -4.2227 -4.2267971 -4.2380943 -4.2567911 -4.2710509 -4.2658653 -4.2569489 -4.2570319 -4.2622995 -4.2698631 -4.280014 -4.2908244 -4.2910447][-4.1973848 -4.176456 -4.1655369 -4.1660619 -4.1784668 -4.2019758 -4.2204652 -4.2149348 -4.2105594 -4.220015 -4.2338424 -4.2452006 -4.2568116 -4.2713566 -4.2726378][-4.146739 -4.1105175 -4.0924244 -4.0940046 -4.1043577 -4.1210623 -4.1338849 -4.1316438 -4.1431613 -4.1699305 -4.1939683 -4.2079873 -4.2195015 -4.2348928 -4.2356186][-4.1028976 -4.0543246 -4.0239043 -4.0179787 -4.0160751 -4.0051775 -3.9904542 -3.9902575 -4.0372248 -4.0957546 -4.1359196 -4.1548762 -4.1689692 -4.1909294 -4.1968956][-4.0782828 -4.0211411 -3.9704764 -3.9446559 -3.9177141 -3.8553827 -3.7722163 -3.7507334 -3.8511977 -3.965898 -4.0339041 -4.0693569 -4.097466 -4.1352673 -4.1570411][-4.0874376 -4.0383158 -3.9844661 -3.949002 -3.9059348 -3.8053005 -3.6576986 -3.5944781 -3.7310162 -3.8903198 -3.9839792 -4.0316629 -4.0680923 -4.1164341 -4.1520634][-4.1388164 -4.1089168 -4.0750937 -4.0544338 -4.0233345 -3.943646 -3.8220911 -3.7510962 -3.8431787 -3.9751704 -4.0588036 -4.1022305 -4.1288457 -4.16516 -4.195612][-4.1967907 -4.1766157 -4.1568108 -4.1533833 -4.1415157 -4.094492 -4.0131049 -3.9567397 -4.0046086 -4.0897942 -4.1499128 -4.1808848 -4.1939287 -4.2117691 -4.2275224][-4.226861 -4.2137537 -4.2025642 -4.2068577 -4.2068863 -4.1799212 -4.1289396 -4.089169 -4.115715 -4.1715903 -4.2106638 -4.2245932 -4.2215486 -4.222537 -4.221415][-4.2342863 -4.2276597 -4.2215476 -4.2300892 -4.2349176 -4.2147107 -4.17646 -4.1464729 -4.158596 -4.1957169 -4.2210693 -4.2238488 -4.2149482 -4.2139468 -4.2108736][-4.2175784 -4.2206655 -4.2189674 -4.2267647 -4.2316875 -4.2151642 -4.1793933 -4.1492763 -4.1464386 -4.169857 -4.192862 -4.2018485 -4.2074752 -4.220777 -4.2297635][-4.1834641 -4.1980863 -4.2014861 -4.2085052 -4.21178 -4.1927104 -4.1521077 -4.1160941 -4.1060157 -4.1251707 -4.1526861 -4.1754627 -4.1983995 -4.2264724 -4.2440672][-4.1492505 -4.176333 -4.1846495 -4.190052 -4.188364 -4.1650128 -4.1250343 -4.0891871 -4.0800319 -4.1001911 -4.1313481 -4.1623344 -4.1937094 -4.2257123 -4.2468081]]...]
INFO - root - 2017-12-05 17:35:09.206638: step 26910, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 76h:51m:00s remains)
INFO - root - 2017-12-05 17:35:18.035613: step 26920, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 75h:44m:53s remains)
INFO - root - 2017-12-05 17:35:27.062990: step 26930, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 75h:58m:00s remains)
INFO - root - 2017-12-05 17:35:36.299042: step 26940, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 73h:59m:31s remains)
INFO - root - 2017-12-05 17:35:45.539054: step 26950, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 77h:02m:19s remains)
INFO - root - 2017-12-05 17:35:54.809072: step 26960, loss = 2.05, batch loss = 2.00 (7.9 examples/sec; 1.012 sec/batch; 85h:55m:53s remains)
INFO - root - 2017-12-05 17:36:04.094781: step 26970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 77h:25m:20s remains)
INFO - root - 2017-12-05 17:36:13.342136: step 26980, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 79h:52m:05s remains)
INFO - root - 2017-12-05 17:36:22.200090: step 26990, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 78h:37m:42s remains)
INFO - root - 2017-12-05 17:36:31.360267: step 27000, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 75h:01m:29s remains)
2017-12-05 17:36:32.259174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3221464 -4.3393149 -4.3498855 -4.3518863 -4.3484983 -4.3441043 -4.3407416 -4.3396964 -4.3407569 -4.342967 -4.3455768 -4.3467727 -4.34616 -4.3447094 -4.34285][-4.2710652 -4.3082914 -4.3366995 -4.3501496 -4.3506722 -4.3445797 -4.3371196 -4.3321328 -4.3329535 -4.3375587 -4.3447709 -4.3500652 -4.351449 -4.349421 -4.34591][-4.19262 -4.2584395 -4.3101416 -4.338388 -4.3447347 -4.3366594 -4.3219948 -4.3097162 -4.3093424 -4.3185477 -4.3339362 -4.3479466 -4.3557091 -4.3551774 -4.3506594][-4.1080737 -4.1994624 -4.2710037 -4.3103294 -4.3179717 -4.302465 -4.2739558 -4.2517562 -4.2504826 -4.2682962 -4.2993808 -4.3299513 -4.3513279 -4.3573356 -4.3544345][-4.0384197 -4.1451168 -4.2264795 -4.2670341 -4.2657084 -4.2302823 -4.178236 -4.1451902 -4.1493425 -4.1821489 -4.2360253 -4.2897663 -4.3299584 -4.3485761 -4.3519459][-4.045495 -4.1407924 -4.2091751 -4.2331815 -4.2082477 -4.1428137 -4.0608263 -4.015399 -4.0273843 -4.0783339 -4.1558447 -4.2333136 -4.2936988 -4.32756 -4.3401709][-4.1152468 -4.180057 -4.2188725 -4.2147689 -4.1608429 -4.0646925 -3.9566479 -3.8993845 -3.9173889 -3.9826546 -4.0792546 -4.1760993 -4.2528372 -4.300921 -4.3224339][-4.1892 -4.2248912 -4.2396708 -4.2179036 -4.1543584 -4.0546846 -3.9474711 -3.8904195 -3.9050546 -3.9685996 -4.0658731 -4.1639404 -4.2419639 -4.2930446 -4.317409][-4.2302294 -4.24441 -4.2519741 -4.2361083 -4.1923175 -4.1230574 -4.0463462 -4.0024128 -4.0082722 -4.054502 -4.1319714 -4.2098303 -4.2709236 -4.311543 -4.3315105][-4.2254081 -4.2244415 -4.2375174 -4.246397 -4.2406473 -4.2140107 -4.1746225 -4.1456113 -4.1397743 -4.1615968 -4.2103448 -4.2637043 -4.3050575 -4.3332276 -4.347302][-4.1964526 -4.1833458 -4.2026343 -4.2385216 -4.2713971 -4.28236 -4.2730594 -4.2552171 -4.2390709 -4.2373195 -4.2584033 -4.291326 -4.3208914 -4.3429976 -4.3560052][-4.176146 -4.1515527 -4.1690841 -4.2195587 -4.2756562 -4.3107591 -4.319521 -4.3069611 -4.27756 -4.2516546 -4.2492208 -4.2695575 -4.2996616 -4.3279557 -4.3484545][-4.1832275 -4.1462746 -4.14983 -4.1965442 -4.2588253 -4.304255 -4.3212934 -4.3087025 -4.2668095 -4.2198963 -4.1985607 -4.2115469 -4.248004 -4.2900333 -4.326014][-4.2103066 -4.1620412 -4.1444464 -4.1734 -4.2294078 -4.2776418 -4.2997365 -4.2876515 -4.2398291 -4.1785769 -4.1431246 -4.15147 -4.1938863 -4.2476654 -4.2990479][-4.2427778 -4.1880856 -4.1509705 -4.1571946 -4.2008243 -4.2492242 -4.2792006 -4.2755814 -4.2315784 -4.1660304 -4.1202865 -4.1204314 -4.1600804 -4.21673 -4.2773547]]...]
INFO - root - 2017-12-05 17:36:41.275714: step 27010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 76h:45m:51s remains)
INFO - root - 2017-12-05 17:36:50.352379: step 27020, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 70h:04m:40s remains)
INFO - root - 2017-12-05 17:36:59.477101: step 27030, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 75h:44m:15s remains)
INFO - root - 2017-12-05 17:37:08.603089: step 27040, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 77h:54m:39s remains)
INFO - root - 2017-12-05 17:37:17.725729: step 27050, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 76h:27m:05s remains)
INFO - root - 2017-12-05 17:37:26.928626: step 27060, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 75h:51m:59s remains)
INFO - root - 2017-12-05 17:37:36.196336: step 27070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 78h:45m:50s remains)
INFO - root - 2017-12-05 17:37:45.084752: step 27080, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 78h:55m:02s remains)
INFO - root - 2017-12-05 17:37:54.165373: step 27090, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 74h:56m:59s remains)
INFO - root - 2017-12-05 17:38:03.220049: step 27100, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 79h:37m:21s remains)
2017-12-05 17:38:04.020692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3204465 -4.32081 -4.3183508 -4.3151383 -4.3127065 -4.3119149 -4.3126249 -4.314374 -4.3158011 -4.3171239 -4.3195515 -4.320889 -4.3203058 -4.3187509 -4.3174744][-4.299562 -4.3009682 -4.2979345 -4.2912936 -4.2855005 -4.2843642 -4.2869935 -4.2896919 -4.2923388 -4.2955151 -4.3009562 -4.3051229 -4.3065863 -4.30527 -4.3026471][-4.2798262 -4.2818046 -4.2760415 -4.2612395 -4.2485423 -4.2470694 -4.2509356 -4.2533932 -4.2563248 -4.2621064 -4.273488 -4.28337 -4.2896366 -4.2922287 -4.2901969][-4.2677975 -4.2681956 -4.2571821 -4.2316833 -4.2090983 -4.2028322 -4.2054658 -4.20799 -4.2085657 -4.2149549 -4.2334166 -4.2523327 -4.2684789 -4.2800093 -4.2829456][-4.2627239 -4.2606206 -4.2453847 -4.2088733 -4.1770539 -4.1640449 -4.1614647 -4.1638727 -4.1648169 -4.1718035 -4.193625 -4.2197142 -4.2471395 -4.2696242 -4.28116][-4.2614751 -4.258462 -4.2398252 -4.1994219 -4.1643028 -4.146019 -4.1354508 -4.1358018 -4.1385779 -4.1446567 -4.1634178 -4.1914163 -4.2255373 -4.2565637 -4.2761326][-4.2554569 -4.25394 -4.2382908 -4.2015009 -4.1660891 -4.1424093 -4.1239529 -4.121408 -4.1268559 -4.1295872 -4.1414638 -4.1692286 -4.2073 -4.2426691 -4.2668571][-4.2387562 -4.2438221 -4.2370787 -4.2073035 -4.1728363 -4.1493082 -4.1309657 -4.1282773 -4.1360655 -4.1358228 -4.1381397 -4.16091 -4.19635 -4.2289395 -4.2544885][-4.2218189 -4.2305493 -4.2302518 -4.2054415 -4.1753182 -4.1597776 -4.1524811 -4.1557817 -4.1667166 -4.1689544 -4.1697717 -4.1828313 -4.2070966 -4.2309985 -4.2524896][-4.218461 -4.2228141 -4.2204871 -4.1962771 -4.1693883 -4.1683869 -4.1787868 -4.1916904 -4.2041326 -4.21079 -4.2157693 -4.2220736 -4.2308865 -4.2402182 -4.2527041][-4.2285347 -4.223011 -4.214838 -4.1890984 -4.1596808 -4.1601367 -4.1799388 -4.2010083 -4.2146416 -4.22551 -4.2372651 -4.2443118 -4.2440486 -4.2398944 -4.2443323][-4.2492018 -4.2395744 -4.2250676 -4.1989651 -4.1668353 -4.1565657 -4.164681 -4.1792884 -4.1914363 -4.2069526 -4.2281256 -4.2434554 -4.2459412 -4.2399921 -4.2391953][-4.2600541 -4.2534318 -4.2404242 -4.2171254 -4.1856804 -4.1646204 -4.1548052 -4.1530042 -4.1573653 -4.1728745 -4.2006731 -4.2268515 -4.238966 -4.2376156 -4.2317944][-4.2537465 -4.2545991 -4.2517848 -4.2390542 -4.2106338 -4.1820722 -4.1578188 -4.142499 -4.138051 -4.1482906 -4.1736903 -4.2032442 -4.221519 -4.2239661 -4.2140503][-4.2403445 -4.2501822 -4.2622752 -4.26622 -4.24651 -4.219305 -4.18546 -4.1612606 -4.1508722 -4.1473308 -4.1567588 -4.174314 -4.1926284 -4.1986895 -4.1879764]]...]
INFO - root - 2017-12-05 17:38:13.114511: step 27110, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.925 sec/batch; 78h:28m:06s remains)
INFO - root - 2017-12-05 17:38:22.294905: step 27120, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.896 sec/batch; 75h:59m:34s remains)
INFO - root - 2017-12-05 17:38:31.339001: step 27130, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 78h:32m:44s remains)
INFO - root - 2017-12-05 17:38:40.239456: step 27140, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 75h:44m:20s remains)
INFO - root - 2017-12-05 17:38:49.418000: step 27150, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 76h:32m:24s remains)
INFO - root - 2017-12-05 17:38:58.508683: step 27160, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 77h:16m:36s remains)
INFO - root - 2017-12-05 17:39:07.415000: step 27170, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 69h:44m:07s remains)
INFO - root - 2017-12-05 17:39:16.542156: step 27180, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.942 sec/batch; 79h:55m:52s remains)
INFO - root - 2017-12-05 17:39:25.665904: step 27190, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 75h:30m:13s remains)
INFO - root - 2017-12-05 17:39:34.678180: step 27200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:01m:18s remains)
2017-12-05 17:39:35.415845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2712078 -4.2776122 -4.2902417 -4.3009458 -4.306705 -4.3109255 -4.3159127 -4.3195648 -4.3217463 -4.3230376 -4.3252821 -4.3295784 -4.3320656 -4.3288264 -4.3237][-4.2310162 -4.2413735 -4.2586341 -4.2721138 -4.277257 -4.2804966 -4.2871928 -4.2946568 -4.3051529 -4.3157659 -4.323904 -4.3311257 -4.3347144 -4.329535 -4.3207355][-4.2016139 -4.2159367 -4.2313962 -4.2390628 -4.2365332 -4.2344255 -4.2398543 -4.2501245 -4.2726111 -4.2969885 -4.3145318 -4.3265896 -4.3308992 -4.323904 -4.3125219][-4.1891675 -4.2055106 -4.213439 -4.2105703 -4.1953187 -4.1797161 -4.1741047 -4.1789541 -4.2140856 -4.2578406 -4.2899332 -4.3086824 -4.3135452 -4.3055592 -4.2940693][-4.1936474 -4.2091184 -4.2070327 -4.1903319 -4.1592045 -4.123158 -4.0901432 -4.0767927 -4.1219258 -4.1886396 -4.2407169 -4.2712235 -4.2799859 -4.2745094 -4.2670708][-4.2059216 -4.2169504 -4.2034054 -4.17358 -4.1251807 -4.0626378 -3.9888573 -3.9462788 -4.0038862 -4.1040649 -4.1805167 -4.2228074 -4.2359366 -4.2345629 -4.232481][-4.2160716 -4.2203403 -4.1936126 -4.1536493 -4.0900626 -3.996273 -3.8768542 -3.7969193 -3.878931 -4.0282912 -4.131557 -4.1828222 -4.1974106 -4.1977091 -4.2008252][-4.2276092 -4.2252674 -4.19176 -4.1480894 -4.0765176 -3.9644666 -3.8133359 -3.7121682 -3.8226554 -4.0019126 -4.1084466 -4.1525288 -4.1612883 -4.156836 -4.1630211][-4.2391787 -4.2335691 -4.2001472 -4.1602645 -4.0979919 -4.0008092 -3.868737 -3.7932346 -3.8961318 -4.043993 -4.117692 -4.1359854 -4.1273918 -4.1088181 -4.1169472][-4.247025 -4.2423162 -4.2163281 -4.1872768 -4.1417856 -4.0721636 -3.9883893 -3.9504931 -4.0195284 -4.1045265 -4.1260843 -4.1098857 -4.0787115 -4.0470147 -4.0610418][-4.2470517 -4.24671 -4.2319541 -4.2118931 -4.1800194 -4.1309805 -4.0836287 -4.0644164 -4.0994596 -4.1289473 -4.107008 -4.0640235 -4.015842 -3.9780943 -3.9994497][-4.2406907 -4.2436738 -4.2376261 -4.2235565 -4.2007647 -4.1650205 -4.1369987 -4.1262007 -4.1364818 -4.1292372 -4.0799994 -4.0263357 -3.9656653 -3.921699 -3.9429212][-4.2361751 -4.2401934 -4.2392554 -4.2343779 -4.2209544 -4.1927867 -4.170485 -4.1626496 -4.1524582 -4.121973 -4.0650296 -4.0130453 -3.951288 -3.9041979 -3.9233146][-4.2419968 -4.2446909 -4.247695 -4.2508488 -4.2459416 -4.2255173 -4.2058778 -4.1937256 -4.1690149 -4.1266289 -4.0725555 -4.0308185 -3.9858773 -3.9556384 -3.9740272][-4.2460485 -4.2425427 -4.2507396 -4.2618289 -4.2641091 -4.2539587 -4.2403555 -4.2256041 -4.1980448 -4.1569128 -4.1079812 -4.073987 -4.0495477 -4.0411687 -4.0555162]]...]
INFO - root - 2017-12-05 17:39:44.554923: step 27210, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.925 sec/batch; 78h:29m:01s remains)
INFO - root - 2017-12-05 17:39:53.914623: step 27220, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.970 sec/batch; 82h:14m:59s remains)
INFO - root - 2017-12-05 17:40:03.043205: step 27230, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 78h:32m:48s remains)
INFO - root - 2017-12-05 17:40:12.108781: step 27240, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 77h:46m:00s remains)
INFO - root - 2017-12-05 17:40:21.196337: step 27250, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 76h:39m:38s remains)
INFO - root - 2017-12-05 17:40:30.291579: step 27260, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 76h:48m:53s remains)
INFO - root - 2017-12-05 17:40:39.528041: step 27270, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.929 sec/batch; 78h:46m:36s remains)
INFO - root - 2017-12-05 17:40:48.555518: step 27280, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 74h:59m:39s remains)
INFO - root - 2017-12-05 17:40:57.716019: step 27290, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 76h:56m:37s remains)
INFO - root - 2017-12-05 17:41:06.811299: step 27300, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 80h:28m:50s remains)
2017-12-05 17:41:07.580830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1919484 -4.2215123 -4.2383747 -4.2361155 -4.2403665 -4.2521148 -4.2594838 -4.2621489 -4.2604384 -4.2545905 -4.2502642 -4.2443066 -4.2314758 -4.2266512 -4.2265687][-4.2101626 -4.2323446 -4.2397695 -4.231914 -4.2292051 -4.2319622 -4.2294483 -4.2253432 -4.2226868 -4.222188 -4.2276297 -4.228826 -4.222424 -4.2249093 -4.2282805][-4.2185936 -4.2381277 -4.2449894 -4.2391648 -4.2304921 -4.2199454 -4.2027488 -4.1883698 -4.1823893 -4.1899629 -4.2078819 -4.2161732 -4.2142043 -4.2175031 -4.2193179][-4.2077117 -4.228065 -4.2377682 -4.2358069 -4.2264352 -4.2063985 -4.1777635 -4.1518312 -4.1423073 -4.1594968 -4.1891303 -4.2067327 -4.2093296 -4.206533 -4.2022872][-4.1959672 -4.2165046 -4.2283387 -4.2266603 -4.2094369 -4.1811314 -4.1481881 -4.113915 -4.0996685 -4.1250057 -4.168642 -4.1990075 -4.2080278 -4.2034941 -4.1946573][-4.191011 -4.2148976 -4.2296009 -4.226295 -4.1945162 -4.1475611 -4.0967884 -4.0471916 -4.0291672 -4.071559 -4.1390686 -4.190556 -4.2123642 -4.2150512 -4.2098479][-4.1926169 -4.2196555 -4.2371988 -4.2354937 -4.1960993 -4.1325889 -4.05444 -3.9729061 -3.9355915 -3.9952312 -4.096417 -4.1732039 -4.2156115 -4.2300835 -4.2272296][-4.2012491 -4.230525 -4.2495909 -4.2522154 -4.217042 -4.1563935 -4.0717845 -3.9777195 -3.917798 -3.9682736 -4.0749235 -4.1589384 -4.2094688 -4.2325063 -4.233757][-4.2068753 -4.2379379 -4.2579746 -4.2622333 -4.2378173 -4.1966577 -4.1332569 -4.0605221 -4.0084977 -4.0328426 -4.1047163 -4.1664615 -4.2096472 -4.232975 -4.2365065][-4.2069039 -4.2305117 -4.2487564 -4.2594581 -4.2518306 -4.2325177 -4.1992197 -4.1576033 -4.1272807 -4.1294007 -4.157289 -4.191577 -4.22223 -4.239028 -4.2424216][-4.2065821 -4.2210588 -4.2321439 -4.24069 -4.2429695 -4.2399139 -4.2296405 -4.2140727 -4.2057776 -4.2027607 -4.2079606 -4.2231555 -4.2410522 -4.2475309 -4.2486057][-4.21872 -4.2186556 -4.2153234 -4.2137752 -4.2165241 -4.2196417 -4.2236223 -4.2269897 -4.2334218 -4.2383032 -4.2406907 -4.2469149 -4.2542348 -4.2512264 -4.2451458][-4.2333713 -4.2246871 -4.213201 -4.2029629 -4.1978593 -4.1977496 -4.2051778 -4.221755 -4.2428555 -4.2562346 -4.2592659 -4.259943 -4.2560925 -4.243165 -4.2328405][-4.2481923 -4.2408996 -4.2311754 -4.2188883 -4.2072682 -4.2020993 -4.2073588 -4.2263179 -4.2499804 -4.264955 -4.2692785 -4.2672043 -4.2590814 -4.2447324 -4.232831][-4.2628012 -4.2611251 -4.2577991 -4.25009 -4.2400637 -4.2345047 -4.2387094 -4.2541933 -4.272058 -4.2800179 -4.2811484 -4.2791519 -4.2715473 -4.2571445 -4.2408495]]...]
INFO - root - 2017-12-05 17:41:16.750919: step 27310, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 77h:31m:26s remains)
INFO - root - 2017-12-05 17:41:25.876809: step 27320, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 80h:31m:50s remains)
INFO - root - 2017-12-05 17:41:35.050036: step 27330, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 79h:27m:52s remains)
INFO - root - 2017-12-05 17:41:44.237973: step 27340, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 77h:10m:25s remains)
INFO - root - 2017-12-05 17:41:53.276791: step 27350, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 74h:47m:37s remains)
INFO - root - 2017-12-05 17:42:02.243211: step 27360, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 75h:18m:15s remains)
INFO - root - 2017-12-05 17:42:11.438062: step 27370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 78h:37m:45s remains)
INFO - root - 2017-12-05 17:42:20.617988: step 27380, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 77h:21m:42s remains)
INFO - root - 2017-12-05 17:42:29.561283: step 27390, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 76h:24m:34s remains)
INFO - root - 2017-12-05 17:42:38.555231: step 27400, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 75h:34m:03s remains)
2017-12-05 17:42:39.344060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23219 -4.2007618 -4.1525865 -4.1067967 -4.0828528 -4.0793581 -4.1161952 -4.1870561 -4.2246418 -4.222436 -4.193748 -4.1391397 -4.1048617 -4.1074 -4.1329145][-4.2363949 -4.2035117 -4.1532326 -4.1019535 -4.0792232 -4.08172 -4.1273232 -4.1960187 -4.2288308 -4.2244048 -4.1974797 -4.1522088 -4.1267242 -4.1249132 -4.139667][-4.2268853 -4.1972027 -4.1527481 -4.1028194 -4.0849371 -4.0926614 -4.1377926 -4.196795 -4.2183466 -4.20873 -4.1809411 -4.1464267 -4.1334219 -4.1314564 -4.1390023][-4.2094207 -4.185421 -4.1445112 -4.0947986 -4.0832429 -4.093679 -4.1331191 -4.1770129 -4.1917934 -4.1808319 -4.15991 -4.1431661 -4.141644 -4.1425138 -4.145957][-4.1933112 -4.1721168 -4.133975 -4.088191 -4.0812421 -4.092628 -4.1151876 -4.140841 -4.1511045 -4.147397 -4.1460409 -4.1510696 -4.1615639 -4.1651354 -4.1651115][-4.181119 -4.1665535 -4.1370807 -4.10325 -4.0991783 -4.0990281 -4.0891085 -4.0920286 -4.1019917 -4.1136603 -4.13336 -4.1612029 -4.1821671 -4.1901875 -4.1908312][-4.1767254 -4.1693478 -4.1521411 -4.1348858 -4.1270509 -4.1034303 -4.059906 -4.0403705 -4.0575109 -4.0886092 -4.1246853 -4.1632314 -4.1875224 -4.1959147 -4.1987977][-4.1737642 -4.1708088 -4.1636686 -4.1553717 -4.1422863 -4.1025987 -4.0334458 -4.0012527 -4.0294361 -4.0793328 -4.124999 -4.1609621 -4.1806884 -4.1890712 -4.1949205][-4.1672287 -4.1694474 -4.1682229 -4.1644168 -4.1505828 -4.106771 -4.0316486 -3.9980538 -4.0336623 -4.0934362 -4.132751 -4.1513858 -4.1651354 -4.1779194 -4.186183][-4.16377 -4.1689811 -4.1716428 -4.1759558 -4.166678 -4.1293249 -4.065711 -4.0294075 -4.0511036 -4.0979371 -4.1248465 -4.1289172 -4.1382837 -4.1600046 -4.1804991][-4.1619496 -4.1713161 -4.1802111 -4.1922822 -4.1890287 -4.1620965 -4.1117053 -4.0692739 -4.0641394 -4.0832787 -4.0913391 -4.0817595 -4.0887141 -4.128191 -4.1689453][-4.1629457 -4.1791825 -4.1941562 -4.2085519 -4.2039843 -4.1805763 -4.1412272 -4.1045756 -4.0894818 -4.0849051 -4.0639095 -4.0316315 -4.0361271 -4.0927377 -4.1506777][-4.1730151 -4.1915174 -4.2072086 -4.2184176 -4.2088985 -4.18416 -4.15395 -4.1302037 -4.1162844 -4.1008987 -4.0633187 -4.0200348 -4.0214686 -4.0749245 -4.1304464][-4.18492 -4.2012825 -4.2104096 -4.2169538 -4.2077603 -4.185874 -4.1664524 -4.1526952 -4.143569 -4.1309743 -4.1024904 -4.072454 -4.0698791 -4.0954347 -4.1266518][-4.1888742 -4.1983776 -4.2034817 -4.2124357 -4.2076621 -4.1912866 -4.175868 -4.163588 -4.1554785 -4.1488695 -4.1382742 -4.1277161 -4.12617 -4.1326623 -4.1456995]]...]
INFO - root - 2017-12-05 17:42:48.428651: step 27410, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 79h:44m:33s remains)
INFO - root - 2017-12-05 17:42:57.702101: step 27420, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 77h:11m:45s remains)
INFO - root - 2017-12-05 17:43:06.787190: step 27430, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 77h:47m:58s remains)
INFO - root - 2017-12-05 17:43:15.773388: step 27440, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 77h:05m:39s remains)
INFO - root - 2017-12-05 17:43:24.966443: step 27450, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.896 sec/batch; 75h:53m:45s remains)
INFO - root - 2017-12-05 17:43:34.112156: step 27460, loss = 2.03, batch loss = 1.97 (7.7 examples/sec; 1.043 sec/batch; 88h:20m:28s remains)
INFO - root - 2017-12-05 17:43:43.146626: step 27470, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.904 sec/batch; 76h:34m:20s remains)
INFO - root - 2017-12-05 17:43:52.165600: step 27480, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 75h:08m:19s remains)
INFO - root - 2017-12-05 17:44:01.238189: step 27490, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 77h:23m:25s remains)
INFO - root - 2017-12-05 17:44:10.461853: step 27500, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 77h:11m:25s remains)
2017-12-05 17:44:11.213308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1594658 -4.1464448 -4.1489549 -4.1720982 -4.2070107 -4.2338109 -4.244803 -4.2494788 -4.251121 -4.249505 -4.2424216 -4.2269812 -4.2072544 -4.1949406 -4.1973243][-4.171947 -4.1499653 -4.1422687 -4.15847 -4.1909375 -4.2197108 -4.2348919 -4.2441392 -4.2498 -4.247694 -4.2398791 -4.2249308 -4.2075744 -4.1977978 -4.19727][-4.1897669 -4.1624765 -4.1468124 -4.1505966 -4.1697626 -4.1951671 -4.214427 -4.2266378 -4.2357216 -4.2337723 -4.2244015 -4.2125664 -4.2060723 -4.20297 -4.1988311][-4.2021632 -4.1745453 -4.1530991 -4.1462789 -4.1478043 -4.1636753 -4.185379 -4.2056031 -4.2187204 -4.2166591 -4.2065077 -4.199234 -4.2039361 -4.2066236 -4.199944][-4.2104774 -4.1861291 -4.1634235 -4.1468091 -4.1270151 -4.1252265 -4.1450548 -4.1733012 -4.1947231 -4.1994443 -4.1945262 -4.1949229 -4.2101703 -4.2185779 -4.2163544][-4.2293949 -4.210041 -4.1897354 -4.1635633 -4.125947 -4.0997882 -4.1072011 -4.1360588 -4.1707196 -4.19407 -4.2030897 -4.2115569 -4.230217 -4.2422757 -4.2440286][-4.2531343 -4.2389259 -4.2228961 -4.1945052 -4.1468854 -4.1002173 -4.077992 -4.0927863 -4.1354046 -4.1820126 -4.2130318 -4.230823 -4.2485075 -4.2595925 -4.2593451][-4.266757 -4.2535543 -4.2423449 -4.2181764 -4.1744223 -4.11804 -4.0648708 -4.053493 -4.1040692 -4.1765079 -4.229331 -4.2548079 -4.2671037 -4.2724972 -4.2662659][-4.2832355 -4.2731314 -4.2670722 -4.2525415 -4.2194681 -4.1681914 -4.1063504 -4.0786028 -4.1271124 -4.2067795 -4.2661467 -4.291007 -4.2927189 -4.2870154 -4.2770376][-4.3080063 -4.302959 -4.3035512 -4.30083 -4.2819152 -4.245708 -4.1963444 -4.1660628 -4.1937256 -4.2565041 -4.3099484 -4.3290353 -4.3164883 -4.298131 -4.2838855][-4.325583 -4.3258939 -4.3322358 -4.3372054 -4.3279629 -4.3060169 -4.2755141 -4.2480731 -4.2553258 -4.2957253 -4.3381991 -4.3493261 -4.3269048 -4.3012891 -4.2845807][-4.3276615 -4.3282909 -4.3363943 -4.3471193 -4.3444505 -4.3300452 -4.3087878 -4.2850132 -4.2814889 -4.3059216 -4.3383179 -4.3463244 -4.3254347 -4.3028712 -4.2891321][-4.3128257 -4.3100047 -4.3186359 -4.3342342 -4.3389692 -4.3318291 -4.3170824 -4.2984247 -4.2918649 -4.3045683 -4.32669 -4.3347697 -4.3225493 -4.3084793 -4.2999496][-4.3043933 -4.3004932 -4.3079433 -4.3218036 -4.3297462 -4.3272953 -4.3173013 -4.3022733 -4.2943635 -4.2995114 -4.313489 -4.3220615 -4.3180294 -4.3113675 -4.3071713][-4.301363 -4.2972775 -4.3020468 -4.3110237 -4.3165808 -4.3156614 -4.308013 -4.2957139 -4.2878942 -4.288239 -4.2960558 -4.3035016 -4.3048077 -4.3033376 -4.3013115]]...]
INFO - root - 2017-12-05 17:44:20.498936: step 27510, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 75h:06m:02s remains)
INFO - root - 2017-12-05 17:44:29.520158: step 27520, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 75h:23m:25s remains)
INFO - root - 2017-12-05 17:44:38.542492: step 27530, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 77h:22m:54s remains)
INFO - root - 2017-12-05 17:44:47.658018: step 27540, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 77h:23m:47s remains)
INFO - root - 2017-12-05 17:44:56.779770: step 27550, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.936 sec/batch; 79h:17m:56s remains)
INFO - root - 2017-12-05 17:45:06.190085: step 27560, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 79h:36m:26s remains)
INFO - root - 2017-12-05 17:45:15.392197: step 27570, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 76h:19m:51s remains)
INFO - root - 2017-12-05 17:45:24.166351: step 27580, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 77h:18m:13s remains)
INFO - root - 2017-12-05 17:45:33.342983: step 27590, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 79h:05m:09s remains)
INFO - root - 2017-12-05 17:45:42.402517: step 27600, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 76h:36m:04s remains)
2017-12-05 17:45:43.283400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20451 -4.2033339 -4.2105055 -4.2170439 -4.2082591 -4.1947341 -4.1872821 -4.193727 -4.1979909 -4.1835117 -4.164814 -4.146852 -4.1389971 -4.1494155 -4.1700487][-4.195991 -4.2053418 -4.2211771 -4.2369308 -4.2271886 -4.206655 -4.1908 -4.1915064 -4.198154 -4.1876264 -4.172554 -4.1567869 -4.1508012 -4.1619115 -4.1775742][-4.1790528 -4.2007422 -4.2262 -4.2473917 -4.234406 -4.2063932 -4.1800294 -4.1746016 -4.18542 -4.1862874 -4.1782632 -4.1658731 -4.16133 -4.1709075 -4.1807384][-4.1646442 -4.1866546 -4.2165852 -4.2397413 -4.22476 -4.1858091 -4.1438637 -4.1325631 -4.1560454 -4.1782961 -4.181994 -4.1706815 -4.1615644 -4.161953 -4.1641688][-4.1556983 -4.170413 -4.1967268 -4.2201142 -4.206893 -4.158761 -4.0980396 -4.0726452 -4.1105433 -4.1604738 -4.182755 -4.1728964 -4.1561351 -4.1454606 -4.143796][-4.1481042 -4.1604681 -4.1825762 -4.1989722 -4.178791 -4.11671 -4.0311837 -3.9829485 -4.031991 -4.1141667 -4.1650066 -4.1655021 -4.1483006 -4.13698 -4.1349487][-4.1277404 -4.1426449 -4.1684923 -4.18552 -4.1544061 -4.0674758 -3.9401431 -3.8517303 -3.9147797 -4.0428309 -4.1336575 -4.1558356 -4.1448612 -4.1375465 -4.1378632][-4.1105385 -4.1223264 -4.1493783 -4.1730461 -4.146688 -4.0564957 -3.9097159 -3.7828846 -3.8401675 -3.9945343 -4.115108 -4.1582565 -4.15817 -4.1556492 -4.1563392][-4.1276116 -4.1302719 -4.1499066 -4.1741529 -4.1625977 -4.1033373 -3.998291 -3.8957074 -3.9147525 -4.0261579 -4.1295733 -4.1713462 -4.1807537 -4.1823945 -4.1822648][-4.1712389 -4.1627812 -4.1686106 -4.1832037 -4.1839108 -4.1604056 -4.1085691 -4.0557046 -4.05193 -4.0986176 -4.1573186 -4.1852241 -4.1966529 -4.202363 -4.2054148][-4.2225885 -4.2082305 -4.1966748 -4.19798 -4.2013545 -4.2025318 -4.1878653 -4.16789 -4.1547894 -4.1586523 -4.1806278 -4.196136 -4.2061591 -4.2138081 -4.2194414][-4.2531161 -4.2405605 -4.2259674 -4.2186708 -4.2174973 -4.2254691 -4.2296815 -4.226233 -4.211699 -4.1973748 -4.1960855 -4.2027135 -4.2126832 -4.2232513 -4.2331119][-4.26728 -4.2636328 -4.2575893 -4.253449 -4.2513018 -4.2566729 -4.2635927 -4.2634449 -4.2481012 -4.2264128 -4.211133 -4.209619 -4.2165308 -4.2312741 -4.2484403][-4.2678838 -4.2726016 -4.2716393 -4.2693205 -4.2694631 -4.2744789 -4.2807159 -4.2797852 -4.2648935 -4.2404504 -4.2189503 -4.2113891 -4.2155414 -4.2354383 -4.26047][-4.2591896 -4.2672534 -4.2670875 -4.2629061 -4.2602868 -4.2655969 -4.2737365 -4.2747607 -4.2639861 -4.2427449 -4.2230153 -4.2169647 -4.2203703 -4.2406006 -4.2679276]]...]
INFO - root - 2017-12-05 17:45:52.560323: step 27610, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 75h:47m:51s remains)
INFO - root - 2017-12-05 17:46:01.623438: step 27620, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 77h:37m:54s remains)
INFO - root - 2017-12-05 17:46:10.639759: step 27630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 77h:07m:47s remains)
INFO - root - 2017-12-05 17:46:19.817535: step 27640, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 76h:29m:38s remains)
INFO - root - 2017-12-05 17:46:28.959476: step 27650, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.916 sec/batch; 77h:31m:54s remains)
INFO - root - 2017-12-05 17:46:37.960003: step 27660, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 76h:30m:58s remains)
INFO - root - 2017-12-05 17:46:46.882245: step 27670, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 0.791 sec/batch; 66h:56m:20s remains)
INFO - root - 2017-12-05 17:46:56.099768: step 27680, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 77h:58m:42s remains)
INFO - root - 2017-12-05 17:47:05.093293: step 27690, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.882 sec/batch; 74h:39m:59s remains)
INFO - root - 2017-12-05 17:47:14.127979: step 27700, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 78h:57m:37s remains)
2017-12-05 17:47:14.905995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2211404 -4.2156906 -4.2230158 -4.2152352 -4.1828165 -4.159493 -4.158257 -4.1820292 -4.2225814 -4.2487931 -4.2620587 -4.2658696 -4.2654071 -4.2539563 -4.2288737][-4.2181282 -4.2103457 -4.2144232 -4.2032142 -4.1770382 -4.1652579 -4.1717639 -4.1951046 -4.2367611 -4.2665296 -4.284163 -4.2916465 -4.2895555 -4.2716975 -4.2341976][-4.2203116 -4.208982 -4.2034545 -4.1846924 -4.163353 -4.1606588 -4.1666932 -4.1805534 -4.2170515 -4.2515554 -4.2780447 -4.2980528 -4.3053255 -4.2917213 -4.2532959][-4.2195215 -4.2039971 -4.1866603 -4.158783 -4.135417 -4.1313 -4.1272993 -4.131196 -4.1649265 -4.2082138 -4.2473192 -4.2846346 -4.305079 -4.2999935 -4.2673826][-4.2137737 -4.1956854 -4.1704035 -4.1341066 -4.1022615 -4.0804343 -4.0542064 -4.051218 -4.0921373 -4.150115 -4.205348 -4.2608333 -4.293695 -4.2963591 -4.2750773][-4.2021008 -4.1806955 -4.1512265 -4.1094041 -4.0636516 -4.0078006 -3.9424953 -3.9284916 -3.9872093 -4.0677943 -4.1455841 -4.220078 -4.269146 -4.2820716 -4.2679658][-4.187335 -4.1567473 -4.1190772 -4.0690427 -4.006042 -3.913888 -3.8022933 -3.7710581 -3.8588829 -3.9762354 -4.0779629 -4.1656475 -4.2265091 -4.2484188 -4.2410245][-4.170969 -4.1253238 -4.0750709 -4.0155334 -3.9464934 -3.84954 -3.7380438 -3.7120671 -3.8160651 -3.9494269 -4.0567284 -4.1422305 -4.1981044 -4.216917 -4.21144][-4.1628318 -4.1068869 -4.0515213 -3.994575 -3.9420128 -3.8825052 -3.8263237 -3.8313098 -3.9120498 -4.0134397 -4.0949287 -4.1574912 -4.19465 -4.205462 -4.202116][-4.1705575 -4.1151557 -4.0647283 -4.0200548 -3.9885354 -3.9635398 -3.9535413 -3.9845023 -4.0445409 -4.1122317 -4.1624012 -4.1985636 -4.218895 -4.2234311 -4.222044][-4.1904712 -4.1455479 -4.1077771 -4.0792694 -4.0644355 -4.0599289 -4.07262 -4.1120811 -4.1565433 -4.1995449 -4.2293296 -4.2492666 -4.2625885 -4.2664361 -4.2655816][-4.2149086 -4.1843243 -4.1617355 -4.1469188 -4.1436763 -4.1526594 -4.1774769 -4.2141223 -4.2435546 -4.2698612 -4.2872128 -4.2994471 -4.3070574 -4.3086529 -4.3060322][-4.2354732 -4.2126012 -4.20102 -4.199667 -4.2082319 -4.2261596 -4.2535467 -4.2826037 -4.3014507 -4.3170233 -4.3266559 -4.3324008 -4.3346777 -4.3347182 -4.3306227][-4.2487254 -4.2303843 -4.2248344 -4.233408 -4.2509179 -4.2738643 -4.2982697 -4.3169818 -4.3268309 -4.3345108 -4.3377395 -4.3381658 -4.3374696 -4.3362036 -4.3330293][-4.2544675 -4.24086 -4.2393789 -4.2503219 -4.270803 -4.2966332 -4.3159695 -4.3240905 -4.3252468 -4.3267508 -4.3254633 -4.3226657 -4.3197165 -4.3166828 -4.3132973]]...]
INFO - root - 2017-12-05 17:47:24.092109: step 27710, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 76h:32m:32s remains)
INFO - root - 2017-12-05 17:47:33.159992: step 27720, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 77h:53m:45s remains)
INFO - root - 2017-12-05 17:47:42.190426: step 27730, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 74h:52m:51s remains)
INFO - root - 2017-12-05 17:47:51.118836: step 27740, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 75h:49m:35s remains)
INFO - root - 2017-12-05 17:48:00.339912: step 27750, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 78h:03m:52s remains)
INFO - root - 2017-12-05 17:48:09.422669: step 27760, loss = 2.02, batch loss = 1.96 (8.9 examples/sec; 0.901 sec/batch; 76h:14m:11s remains)
INFO - root - 2017-12-05 17:48:18.418842: step 27770, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 75h:58m:47s remains)
INFO - root - 2017-12-05 17:48:27.463957: step 27780, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 77h:35m:14s remains)
INFO - root - 2017-12-05 17:48:36.716143: step 27790, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 78h:15m:32s remains)
INFO - root - 2017-12-05 17:48:45.835787: step 27800, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 77h:39m:58s remains)
2017-12-05 17:48:46.616538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17534 -4.1840067 -4.1953139 -4.2095242 -4.219152 -4.2141929 -4.2032948 -4.1994319 -4.1969619 -4.2040234 -4.2185016 -4.2333703 -4.2432532 -4.2502728 -4.2319984][-4.1547127 -4.1691451 -4.18135 -4.1916065 -4.2085252 -4.2080207 -4.199264 -4.1924787 -4.1867476 -4.1927876 -4.2059579 -4.2215543 -4.2373352 -4.2507014 -4.2405424][-4.1278944 -4.1477065 -4.1582208 -4.1697083 -4.1936083 -4.19836 -4.1891694 -4.1806321 -4.1756606 -4.1797285 -4.1875329 -4.2022209 -4.2234039 -4.2430658 -4.2431993][-4.0942616 -4.1215167 -4.138752 -4.1578393 -4.183742 -4.1858139 -4.169538 -4.1615028 -4.1625776 -4.1696992 -4.1773973 -4.1944604 -4.2194448 -4.2422433 -4.2489643][-4.0742841 -4.1049237 -4.1322522 -4.1547318 -4.1676426 -4.1464028 -4.1043029 -4.0943851 -4.1159706 -4.1433582 -4.1601543 -4.1840734 -4.2127357 -4.2403774 -4.2521148][-4.09253 -4.1152916 -4.1419878 -4.1540127 -4.1377044 -4.0807166 -3.9934707 -3.9757249 -4.0340948 -4.0959263 -4.1352105 -4.1671906 -4.1977463 -4.22948 -4.2454877][-4.1282988 -4.1409097 -4.1527653 -4.1435723 -4.0936441 -3.9922256 -3.8593922 -3.8372374 -3.945544 -4.0500851 -4.1144357 -4.1573944 -4.1816006 -4.2070241 -4.2233572][-4.167532 -4.169591 -4.1654482 -4.1367683 -4.0662394 -3.9499989 -3.8156514 -3.7990408 -3.9228218 -4.0419025 -4.1104612 -4.1513915 -4.1648703 -4.1768107 -4.187891][-4.19149 -4.1861248 -4.1726775 -4.1395693 -4.0772142 -3.99652 -3.91764 -3.9129262 -3.9963663 -4.0846086 -4.1346307 -4.1583371 -4.1597018 -4.1569524 -4.1582088][-4.1938958 -4.1834931 -4.1653533 -4.1339049 -4.0903921 -4.0516539 -4.0272312 -4.0381708 -4.0876851 -4.1416039 -4.1720738 -4.1786323 -4.167912 -4.1507792 -4.1382084][-4.1809297 -4.1703725 -4.1502123 -4.1232085 -4.098084 -4.094635 -4.108942 -4.134656 -4.1661196 -4.1954775 -4.2087479 -4.2030058 -4.1822047 -4.1535959 -4.1302652][-4.1886916 -4.1795373 -4.159133 -4.1324053 -4.1156735 -4.1316934 -4.1673374 -4.1982565 -4.2203479 -4.236877 -4.2379537 -4.2244835 -4.2036252 -4.1729 -4.1414275][-4.2160296 -4.20788 -4.1872706 -4.1621242 -4.1475086 -4.1670165 -4.2041941 -4.2341943 -4.2543607 -4.2670364 -4.2618022 -4.2460704 -4.2293105 -4.2005625 -4.165339][-4.2525034 -4.2472057 -4.2295423 -4.2112684 -4.1978226 -4.2118731 -4.2383223 -4.2620716 -4.2794394 -4.285872 -4.2776823 -4.2640553 -4.2541642 -4.2323861 -4.2002826][-4.2773786 -4.2766767 -4.2670436 -4.25479 -4.2436123 -4.2513208 -4.2659011 -4.2788782 -4.2902789 -4.2920623 -4.2832317 -4.2715783 -4.2633891 -4.2493019 -4.2273903]]...]
INFO - root - 2017-12-05 17:48:55.752187: step 27810, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 76h:14m:43s remains)
INFO - root - 2017-12-05 17:49:04.907579: step 27820, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 79h:34m:54s remains)
INFO - root - 2017-12-05 17:49:14.097588: step 27830, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 76h:15m:05s remains)
INFO - root - 2017-12-05 17:49:23.178302: step 27840, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 78h:09m:48s remains)
INFO - root - 2017-12-05 17:49:32.203332: step 27850, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 77h:33m:36s remains)
INFO - root - 2017-12-05 17:49:41.263488: step 27860, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 75h:29m:03s remains)
INFO - root - 2017-12-05 17:49:50.335323: step 27870, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 79h:54m:54s remains)
INFO - root - 2017-12-05 17:49:59.304240: step 27880, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 76h:32m:52s remains)
INFO - root - 2017-12-05 17:50:08.429566: step 27890, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 77h:08m:20s remains)
INFO - root - 2017-12-05 17:50:17.651072: step 27900, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 79h:40m:42s remains)
2017-12-05 17:50:18.514935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2885838 -4.2861238 -4.28948 -4.2933741 -4.29528 -4.2955356 -4.2955985 -4.2959175 -4.2977219 -4.3009696 -4.3047395 -4.3085732 -4.3157134 -4.3253536 -4.3312774][-4.2538528 -4.249156 -4.2545824 -4.25939 -4.2614079 -4.2602978 -4.2601905 -4.2600064 -4.2633014 -4.2699137 -4.2805552 -4.2884068 -4.29972 -4.314105 -4.3242116][-4.1971664 -4.1915646 -4.1961708 -4.2014489 -4.2029214 -4.2003713 -4.2010808 -4.1979709 -4.2022591 -4.2128553 -4.2296152 -4.2425451 -4.2618785 -4.2847028 -4.3030019][-4.1286931 -4.1222119 -4.126112 -4.1310215 -4.13087 -4.1253185 -4.1210957 -4.1115246 -4.1164389 -4.130229 -4.1518555 -4.1732736 -4.2032018 -4.2359142 -4.2663188][-4.0878115 -4.0757179 -4.0747514 -4.0742106 -4.0672879 -4.04758 -4.0271287 -4.0075397 -4.0126534 -4.0244126 -4.0483284 -4.0825586 -4.1262918 -4.1725788 -4.21786][-4.0612125 -4.0395412 -4.0324039 -4.029665 -4.0117245 -3.966053 -3.9203329 -3.8907094 -3.8973708 -3.9150968 -3.9504225 -4.0019684 -4.0579143 -4.1133609 -4.1733522][-4.037468 -4.0015345 -3.9868534 -3.983851 -3.9589362 -3.895705 -3.84055 -3.8275278 -3.8501792 -3.8858011 -3.9399726 -3.9977729 -4.0414386 -4.0845647 -4.1427727][-4.0259876 -3.9847324 -3.9703028 -3.9780917 -3.963516 -3.902925 -3.8640926 -3.8715839 -3.9001446 -3.941318 -3.9989271 -4.0472803 -4.078371 -4.1087255 -4.1528587][-4.0370317 -4.0035 -3.999438 -4.0248923 -4.0306525 -3.9906795 -3.9736474 -3.9855 -4.0012341 -4.0309291 -4.0766859 -4.1115632 -4.1304522 -4.1520057 -4.1868505][-4.0556741 -4.0396495 -4.049293 -4.086431 -4.1072884 -4.0917006 -4.0884562 -4.0963883 -4.1066074 -4.1270952 -4.1598411 -4.1867032 -4.2017503 -4.2156992 -4.2400885][-4.0954666 -4.091104 -4.104754 -4.1393361 -4.1628881 -4.1641884 -4.1700373 -4.18045 -4.1914158 -4.207253 -4.2315917 -4.2532959 -4.2652211 -4.2742786 -4.2910414][-4.15841 -4.1554294 -4.1614456 -4.1809988 -4.1977987 -4.2095165 -4.2233658 -4.2358918 -4.2486973 -4.2640405 -4.2846036 -4.3012557 -4.3084326 -4.3137813 -4.3247166][-4.2292995 -4.2245579 -4.2238259 -4.2343845 -4.2462983 -4.2573547 -4.2682552 -4.276402 -4.2851324 -4.2984848 -4.3168964 -4.3311729 -4.3367863 -4.3380227 -4.3430929][-4.2861891 -4.2815008 -4.278182 -4.2842593 -4.292367 -4.3001542 -4.30451 -4.3063664 -4.3084645 -4.3145013 -4.326014 -4.3367834 -4.3415732 -4.3420486 -4.345643][-4.3136835 -4.3103442 -4.3092747 -4.3127227 -4.3176422 -4.3215179 -4.3224926 -4.3227644 -4.3234558 -4.3262792 -4.3316765 -4.336823 -4.3387804 -4.3384833 -4.3403521]]...]
INFO - root - 2017-12-05 17:50:27.569486: step 27910, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 75h:45m:35s remains)
INFO - root - 2017-12-05 17:50:36.507055: step 27920, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 70h:00m:50s remains)
INFO - root - 2017-12-05 17:50:45.666380: step 27930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 78h:40m:31s remains)
INFO - root - 2017-12-05 17:50:54.791821: step 27940, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 76h:11m:15s remains)
INFO - root - 2017-12-05 17:51:03.729001: step 27950, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 69h:34m:07s remains)
INFO - root - 2017-12-05 17:51:12.693606: step 27960, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 77h:32m:46s remains)
INFO - root - 2017-12-05 17:51:21.624479: step 27970, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 73h:42m:37s remains)
INFO - root - 2017-12-05 17:51:30.534517: step 27980, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 74h:40m:21s remains)
INFO - root - 2017-12-05 17:51:39.661029: step 27990, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 78h:35m:56s remains)
INFO - root - 2017-12-05 17:51:48.804486: step 28000, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 75h:41m:55s remains)
2017-12-05 17:51:49.567814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9801426 -4.0217619 -4.0891237 -4.1483874 -4.2027836 -4.2473669 -4.2777815 -4.3031135 -4.3226357 -4.3295641 -4.3264256 -4.3180728 -4.3127556 -4.3110075 -4.31208][-4.0930004 -4.1156793 -4.1502957 -4.1738706 -4.1937375 -4.2158794 -4.2460051 -4.2775097 -4.30776 -4.3251214 -4.3290462 -4.3223367 -4.3157282 -4.3128853 -4.3124113][-4.1882453 -4.1967878 -4.204978 -4.1980639 -4.1843719 -4.1821508 -4.2062964 -4.2426052 -4.2809291 -4.3100114 -4.3247104 -4.3258228 -4.3215303 -4.3171854 -4.315208][-4.2530751 -4.2462225 -4.22869 -4.1943822 -4.154881 -4.1319976 -4.1448045 -4.1852427 -4.23334 -4.2712221 -4.2996693 -4.315083 -4.3214622 -4.3216543 -4.3197746][-4.2753444 -4.2577434 -4.2233243 -4.1695452 -4.1078687 -4.0630126 -4.0598812 -4.1019168 -4.161047 -4.2110372 -4.2558289 -4.2913222 -4.3137507 -4.32159 -4.3227725][-4.2643862 -4.24272 -4.1989655 -4.1268854 -4.0438709 -3.970072 -3.9410114 -3.9778175 -4.0530982 -4.1281714 -4.1990452 -4.2599754 -4.3004036 -4.3166771 -4.322052][-4.2607141 -4.2364907 -4.1937366 -4.1149106 -4.0158181 -3.9115319 -3.8377762 -3.8449981 -3.9308932 -4.0361891 -4.1349206 -4.2180872 -4.275805 -4.3008652 -4.3136749][-4.2743998 -4.2516289 -4.2182326 -4.1535878 -4.0589614 -3.9438634 -3.8387618 -3.8040934 -3.8729541 -3.9856617 -4.0990224 -4.1893163 -4.25333 -4.283608 -4.303504][-4.29192 -4.2741327 -4.2550468 -4.2167287 -4.1481981 -4.0524831 -3.9584231 -3.9117253 -3.9479845 -4.0242829 -4.10904 -4.178894 -4.2314596 -4.261538 -4.2886839][-4.2986345 -4.2810211 -4.2707996 -4.2572203 -4.2252054 -4.1657076 -4.1000843 -4.0623803 -4.0758958 -4.1100206 -4.14861 -4.1796741 -4.2073793 -4.2302608 -4.2625027][-4.2915554 -4.2709956 -4.2639666 -4.2685642 -4.2657509 -4.2411509 -4.2075763 -4.1882024 -4.1913466 -4.1973367 -4.1993527 -4.1891074 -4.183537 -4.1940413 -4.2274508][-4.27267 -4.2479711 -4.2407818 -4.2560368 -4.277247 -4.2814813 -4.2754712 -4.274878 -4.2719288 -4.2578859 -4.2311478 -4.1854367 -4.1468639 -4.1373572 -4.1679211][-4.2413411 -4.2087169 -4.194943 -4.2127814 -4.2533016 -4.28466 -4.3004127 -4.3127069 -4.3071136 -4.2845354 -4.24046 -4.1701236 -4.0977693 -4.0624843 -4.0859423][-4.2103 -4.1618037 -4.1234264 -4.1292219 -4.1810331 -4.2369452 -4.278255 -4.3048363 -4.3050704 -4.2827778 -4.2293406 -4.1418581 -4.0393033 -3.9720402 -3.9731295][-4.1916223 -4.1228223 -4.0477743 -4.0197077 -4.0580249 -4.1310987 -4.210115 -4.2663383 -4.2866 -4.2760124 -4.2265539 -4.1333694 -4.015317 -3.9153075 -3.8695104]]...]
INFO - root - 2017-12-05 17:51:58.567824: step 28010, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 75h:57m:23s remains)
INFO - root - 2017-12-05 17:52:07.584285: step 28020, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 74h:34m:30s remains)
INFO - root - 2017-12-05 17:52:16.606470: step 28030, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 78h:10m:59s remains)
INFO - root - 2017-12-05 17:52:25.804402: step 28040, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 79h:34m:22s remains)
INFO - root - 2017-12-05 17:52:34.692381: step 28050, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 77h:20m:02s remains)
INFO - root - 2017-12-05 17:52:43.683724: step 28060, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 74h:48m:31s remains)
INFO - root - 2017-12-05 17:52:52.721489: step 28070, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 78h:31m:33s remains)
INFO - root - 2017-12-05 17:53:01.897734: step 28080, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 78h:24m:32s remains)
INFO - root - 2017-12-05 17:53:11.075685: step 28090, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 75h:10m:03s remains)
INFO - root - 2017-12-05 17:53:20.075645: step 28100, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 77h:39m:57s remains)
2017-12-05 17:53:20.810106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2441587 -4.2353988 -4.2320614 -4.2373533 -4.2444096 -4.2475128 -4.2457685 -4.2423525 -4.2397919 -4.2379689 -4.2361627 -4.23446 -4.2327161 -4.2283769 -4.2262726][-4.247189 -4.2383847 -4.2332273 -4.2392273 -4.2484756 -4.2514944 -4.2468433 -4.2414079 -4.2380443 -4.2355747 -4.234097 -4.2337003 -4.2325077 -4.2270164 -4.2241855][-4.2542453 -4.2439389 -4.2346888 -4.2380834 -4.2467523 -4.248765 -4.2435966 -4.2362342 -4.2289629 -4.2237792 -4.2225823 -4.2272077 -4.2305655 -4.2270279 -4.2259908][-4.2546759 -4.2435894 -4.2298059 -4.2279005 -4.2323327 -4.2319293 -4.2253528 -4.2151217 -4.2020926 -4.1911116 -4.1883116 -4.1974239 -4.2074003 -4.2105126 -4.2167506][-4.2394962 -4.2289896 -4.21555 -4.2111487 -4.212081 -4.2090831 -4.1980224 -4.1805148 -4.1584945 -4.1380463 -4.1296515 -4.138875 -4.1551561 -4.1674833 -4.1847963][-4.208766 -4.2015128 -4.1921139 -4.1894016 -4.1906619 -4.1877446 -4.1727037 -4.1480155 -4.1156192 -4.085094 -4.0697117 -4.0735559 -4.0890923 -4.106256 -4.1314955][-4.1719184 -4.1635146 -4.1567116 -4.1562819 -4.161036 -4.1634912 -4.1518273 -4.1282682 -4.0951324 -4.0638223 -4.0473504 -4.0469408 -4.0577269 -4.0716033 -4.0943537][-4.1393223 -4.1243949 -4.117486 -4.1199579 -4.1285262 -4.1366487 -4.13201 -4.1163726 -4.0913491 -4.070096 -4.0604572 -4.0619507 -4.0721231 -4.0838242 -4.0986452][-4.1199937 -4.0996628 -4.0923977 -4.0963936 -4.1080713 -4.1225271 -4.1271987 -4.1198649 -4.1013637 -4.0858254 -4.0803928 -4.0834818 -4.0946789 -4.1080265 -4.12188][-4.1240826 -4.1006041 -4.0908217 -4.0934029 -4.1037183 -4.1195235 -4.1292939 -4.125668 -4.1091094 -4.0957913 -4.093358 -4.0975389 -4.108429 -4.1225214 -4.1381788][-4.137074 -4.11351 -4.1020918 -4.1035614 -4.1096416 -4.1231236 -4.1331553 -4.1309457 -4.1173873 -4.1079297 -4.1079159 -4.1119871 -4.1210647 -4.1321445 -4.1467056][-4.1507263 -4.1271577 -4.1143584 -4.1196465 -4.1292124 -4.142735 -4.1534595 -4.1525183 -4.1430216 -4.135129 -4.1350546 -4.1378822 -4.1427035 -4.1473255 -4.1581416][-4.1696596 -4.1466613 -4.1327333 -4.1391764 -4.1540575 -4.1734109 -4.1893582 -4.1921606 -4.1853943 -4.1755061 -4.172276 -4.1710334 -4.1687942 -4.1657672 -4.1731825][-4.193511 -4.1739397 -4.1579862 -4.1593657 -4.1733737 -4.1951652 -4.2163272 -4.2254386 -4.2236547 -4.2149034 -4.2092862 -4.2036042 -4.1943541 -4.18366 -4.1864142][-4.2141743 -4.2001491 -4.1854696 -4.1812167 -4.1890154 -4.2074585 -4.2274408 -4.2405462 -4.2454166 -4.2424755 -4.2384505 -4.2300782 -4.2161174 -4.2001982 -4.1984668]]...]
INFO - root - 2017-12-05 17:53:30.006689: step 28110, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 76h:05m:09s remains)
INFO - root - 2017-12-05 17:53:39.103046: step 28120, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 74h:22m:41s remains)
INFO - root - 2017-12-05 17:53:48.058679: step 28130, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 76h:37m:27s remains)
INFO - root - 2017-12-05 17:53:57.143022: step 28140, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 78h:51m:51s remains)
INFO - root - 2017-12-05 17:54:06.265949: step 28150, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 73h:53m:24s remains)
INFO - root - 2017-12-05 17:54:15.129984: step 28160, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 79h:11m:42s remains)
INFO - root - 2017-12-05 17:54:24.221944: step 28170, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 77h:58m:23s remains)
INFO - root - 2017-12-05 17:54:33.338393: step 28180, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 77h:12m:51s remains)
INFO - root - 2017-12-05 17:54:42.446000: step 28190, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 77h:14m:36s remains)
INFO - root - 2017-12-05 17:54:51.454203: step 28200, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 76h:03m:26s remains)
2017-12-05 17:54:52.196534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2936344 -4.2905288 -4.2908421 -4.2860389 -4.2762966 -4.26454 -4.2535548 -4.2453861 -4.2451115 -4.2526708 -4.2617035 -4.2683878 -4.2668328 -4.267262 -4.2681003][-4.2554135 -4.249887 -4.2544842 -4.2551661 -4.2451677 -4.230865 -4.2153544 -4.2009912 -4.2065682 -4.2269945 -4.2417216 -4.2513919 -4.2490091 -4.2526784 -4.2605524][-4.2044544 -4.1961479 -4.205 -4.2130737 -4.2097292 -4.2017584 -4.1807232 -4.1529689 -4.1580124 -4.1879349 -4.2055321 -4.2173944 -4.2183471 -4.2290368 -4.2483525][-4.1551619 -4.145822 -4.1551442 -4.1662784 -4.1668916 -4.1653056 -4.1435285 -4.10614 -4.1000509 -4.1284103 -4.15031 -4.1634398 -4.173954 -4.195436 -4.222703][-4.1251721 -4.1185212 -4.1297126 -4.1379662 -4.1304789 -4.1181459 -4.0881143 -4.05203 -4.0453768 -4.0674949 -4.0865459 -4.1029329 -4.1282716 -4.1639018 -4.1964617][-4.1079617 -4.1041112 -4.1179366 -4.1255293 -4.1093717 -4.0691829 -4.0115862 -3.974021 -3.9883995 -4.0262794 -4.0464911 -4.0642271 -4.0981636 -4.1419678 -4.1774707][-4.1010394 -4.1001334 -4.1174192 -4.1329827 -4.1128292 -4.0464106 -3.94907 -3.8939731 -3.9438679 -4.0230494 -4.06226 -4.0818081 -4.108047 -4.1451368 -4.180645][-4.0995207 -4.1037774 -4.126864 -4.1474767 -4.1317654 -4.064446 -3.9512739 -3.8701034 -3.9294319 -4.0310979 -4.0862155 -4.1097169 -4.1289344 -4.15388 -4.1885786][-4.0941176 -4.1010494 -4.1240158 -4.1413627 -4.1301475 -4.0820379 -4.0017867 -3.9441187 -3.9757192 -4.0427632 -4.0848217 -4.1084938 -4.1278744 -4.15069 -4.18765][-4.088275 -4.0972919 -4.118083 -4.1266694 -4.1124692 -4.0827227 -4.0473666 -4.0334506 -4.0554981 -4.0856924 -4.1039243 -4.1209145 -4.1388903 -4.1615782 -4.1931539][-4.0861874 -4.0988927 -4.1237597 -4.1340184 -4.1234059 -4.1015034 -4.0875635 -4.099205 -4.1249232 -4.1413507 -4.1493187 -4.1613588 -4.1753969 -4.1935124 -4.2164364][-4.1049204 -4.1165347 -4.145155 -4.1657128 -4.1682434 -4.1549292 -4.14243 -4.1548805 -4.1821485 -4.1996803 -4.2077837 -4.2155628 -4.2240548 -4.2359338 -4.2505293][-4.1582932 -4.1655054 -4.1901865 -4.2130332 -4.2234778 -4.2186656 -4.209022 -4.2144804 -4.2347932 -4.2526231 -4.2628355 -4.26928 -4.27415 -4.2824831 -4.293251][-4.2369637 -4.2406 -4.2560878 -4.27205 -4.2813373 -4.2799692 -4.2753615 -4.2788687 -4.2891765 -4.3000984 -4.3068452 -4.3119788 -4.3173881 -4.3255916 -4.3334236][-4.3058028 -4.3093872 -4.31756 -4.3257413 -4.3307452 -4.3290486 -4.3263125 -4.3285022 -4.3334851 -4.3386436 -4.34206 -4.3449016 -4.3480673 -4.3521895 -4.3557096]]...]
INFO - root - 2017-12-05 17:55:01.293650: step 28210, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 76h:25m:05s remains)
INFO - root - 2017-12-05 17:55:10.466794: step 28220, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 76h:02m:51s remains)
INFO - root - 2017-12-05 17:55:19.475283: step 28230, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 76h:00m:15s remains)
INFO - root - 2017-12-05 17:55:28.587357: step 28240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 77h:55m:56s remains)
INFO - root - 2017-12-05 17:55:37.750709: step 28250, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 75h:11m:53s remains)
INFO - root - 2017-12-05 17:55:46.838826: step 28260, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 78h:00m:26s remains)
INFO - root - 2017-12-05 17:55:55.858817: step 28270, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 76h:26m:37s remains)
INFO - root - 2017-12-05 17:56:05.035564: step 28280, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 77h:51m:42s remains)
INFO - root - 2017-12-05 17:56:14.262463: step 28290, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 78h:15m:13s remains)
INFO - root - 2017-12-05 17:56:23.177040: step 28300, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 75h:52m:36s remains)
2017-12-05 17:56:23.928025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2769923 -4.2777762 -4.273335 -4.2705731 -4.2640424 -4.2467675 -4.2212 -4.2000728 -4.1857338 -4.1863995 -4.2110515 -4.2276621 -4.2245688 -4.207459 -4.190217][-4.2738681 -4.272397 -4.2683153 -4.2656536 -4.2564316 -4.2327595 -4.2020731 -4.1773129 -4.1600285 -4.1648808 -4.2005806 -4.2248325 -4.2249336 -4.2103138 -4.1940079][-4.2727418 -4.2738018 -4.2732 -4.2659688 -4.2459788 -4.2130189 -4.177249 -4.1498652 -4.127646 -4.1389422 -4.1873336 -4.2182446 -4.2190132 -4.2073593 -4.1955481][-4.2770281 -4.2801557 -4.2781744 -4.2618051 -4.2293739 -4.186677 -4.1411767 -4.0995288 -4.0764871 -4.1059723 -4.1679115 -4.2049313 -4.2113767 -4.2052717 -4.1981444][-4.2842536 -4.2867732 -4.2802124 -4.2553945 -4.2151113 -4.164289 -4.1024718 -4.0432472 -4.0239873 -4.0760942 -4.1528735 -4.2026238 -4.2229772 -4.2246847 -4.216825][-4.2890792 -4.2882767 -4.2778325 -4.2500157 -4.20514 -4.1407571 -4.0559258 -3.9756143 -3.9698529 -4.0526271 -4.1494637 -4.2193322 -4.2550173 -4.2605386 -4.2467055][-4.288558 -4.2861986 -4.2754359 -4.2468038 -4.1906915 -4.104053 -3.9831841 -3.8703754 -3.8812611 -4.0070152 -4.1369128 -4.2328386 -4.2844248 -4.2949057 -4.2769384][-4.2838988 -4.2833052 -4.2716017 -4.2388082 -4.1649137 -4.0537786 -3.9046645 -3.7750509 -3.8164675 -3.9821303 -4.1324539 -4.2369804 -4.2975554 -4.3155403 -4.3024659][-4.2778959 -4.2786942 -4.2651858 -4.2261305 -4.1444826 -4.0301089 -3.8906717 -3.7957122 -3.8589768 -4.0123119 -4.1444721 -4.23624 -4.2937632 -4.3167257 -4.3134885][-4.2728953 -4.2734442 -4.2595825 -4.2237062 -4.1512547 -4.0496154 -3.9352562 -3.8781562 -3.9446373 -4.0630369 -4.1647511 -4.2405553 -4.2902288 -4.3119569 -4.3136587][-4.2722931 -4.2711749 -4.2565737 -4.2238364 -4.1599555 -4.0735693 -3.982043 -3.9540122 -4.0190368 -4.1102285 -4.1904936 -4.2564225 -4.2977781 -4.3124132 -4.3121362][-4.2759552 -4.272243 -4.2572241 -4.2275558 -4.1750216 -4.1079874 -4.0422678 -4.0387807 -4.0987096 -4.1682949 -4.2305236 -4.2818584 -4.3104234 -4.317574 -4.3128548][-4.2781734 -4.2692561 -4.2527452 -4.2270875 -4.1864338 -4.1396632 -4.1007791 -4.1150212 -4.1667747 -4.21946 -4.2641025 -4.2994504 -4.3166056 -4.31922 -4.31386][-4.2741508 -4.2593665 -4.2420125 -4.2211738 -4.1932473 -4.1674538 -4.1534691 -4.1744838 -4.2150865 -4.2536635 -4.2826848 -4.3043065 -4.3132148 -4.31462 -4.312078][-4.2780595 -4.2616644 -4.2449608 -4.2290616 -4.2125521 -4.2012925 -4.2003613 -4.2186236 -4.2457838 -4.2724733 -4.2928457 -4.3074236 -4.3131685 -4.3161559 -4.3171244]]...]
INFO - root - 2017-12-05 17:56:33.098837: step 28310, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 77h:22m:31s remains)
INFO - root - 2017-12-05 17:56:42.171858: step 28320, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 73h:31m:06s remains)
INFO - root - 2017-12-05 17:56:51.137979: step 28330, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 74h:13m:04s remains)
INFO - root - 2017-12-05 17:57:00.146845: step 28340, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 75h:00m:51s remains)
INFO - root - 2017-12-05 17:57:09.346469: step 28350, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 77h:42m:19s remains)
INFO - root - 2017-12-05 17:57:18.417609: step 28360, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 78h:57m:00s remains)
INFO - root - 2017-12-05 17:57:27.530196: step 28370, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 75h:37m:36s remains)
INFO - root - 2017-12-05 17:57:36.652544: step 28380, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 77h:54m:54s remains)
INFO - root - 2017-12-05 17:57:45.629879: step 28390, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 69h:28m:21s remains)
INFO - root - 2017-12-05 17:57:54.727674: step 28400, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 75h:51m:18s remains)
2017-12-05 17:57:55.562070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054118 -4.2967362 -4.2883792 -4.2848988 -4.2819424 -4.2783122 -4.2733445 -4.2754974 -4.2778306 -4.2750568 -4.2776704 -4.2855096 -4.2969437 -4.3141565 -4.3291926][-4.2945809 -4.2848697 -4.2767835 -4.2724237 -4.2678871 -4.2592149 -4.2483921 -4.2496433 -4.2544594 -4.2524905 -4.2580781 -4.2700653 -4.2843409 -4.3044438 -4.3237514][-4.2806878 -4.2681479 -4.2590604 -4.2541733 -4.2455792 -4.22995 -4.2149253 -4.2189579 -4.2310929 -4.2347956 -4.2457814 -4.2611227 -4.2763114 -4.2985706 -4.3210669][-4.266921 -4.2511048 -4.2401485 -4.2320251 -4.215281 -4.1880894 -4.1640821 -4.1697879 -4.1933036 -4.2106886 -4.2341127 -4.2588797 -4.2772903 -4.2989159 -4.3207455][-4.2481389 -4.2256522 -4.2070031 -4.1888151 -4.1545606 -4.1042156 -4.0641446 -4.0746632 -4.1199088 -4.1592746 -4.2013125 -4.2418032 -4.2698488 -4.2958784 -4.31892][-4.2371931 -4.2057781 -4.1716237 -4.13327 -4.07376 -3.9872103 -3.9159751 -3.9337034 -4.0142956 -4.0832024 -4.1456771 -4.2039289 -4.2450128 -4.2809381 -4.3106561][-4.2229528 -4.1755033 -4.1201138 -4.0560913 -3.9690099 -3.8480327 -3.7357898 -3.7578025 -3.8786561 -3.9881468 -4.0760365 -4.1548595 -4.2103553 -4.2576408 -4.2969913][-4.226862 -4.1767044 -4.1202564 -4.0591865 -3.9826388 -3.8821549 -3.7857327 -3.7949975 -3.8955343 -3.9969759 -4.0776925 -4.1526184 -4.2066126 -4.2522664 -4.2930489][-4.2464113 -4.2104354 -4.1703849 -4.125545 -4.0730376 -4.0102339 -3.952239 -3.9497292 -4.0066161 -4.07184 -4.125937 -4.1801925 -4.221909 -4.2585483 -4.2924185][-4.2571864 -4.2359376 -4.2124557 -4.1832414 -4.1470633 -4.1106482 -4.079464 -4.0756078 -4.10817 -4.1496615 -4.1854858 -4.2204347 -4.2455506 -4.2696853 -4.2932982][-4.2643123 -4.2538118 -4.2416825 -4.2236915 -4.1986227 -4.1749325 -4.158783 -4.1589274 -4.1794434 -4.2026629 -4.2279119 -4.2534428 -4.2688065 -4.2842627 -4.3020873][-4.2683053 -4.2569447 -4.2437015 -4.227951 -4.2075639 -4.1888089 -4.1787758 -4.186058 -4.2059274 -4.2193489 -4.2402139 -4.265533 -4.2794642 -4.2926359 -4.3100634][-4.2805114 -4.2676911 -4.25253 -4.2370133 -4.2203727 -4.2054515 -4.1977315 -4.2076025 -4.2272887 -4.237783 -4.2549863 -4.2785234 -4.29187 -4.3035045 -4.3182354][-4.2948484 -4.2827749 -4.2694225 -4.2570286 -4.2442894 -4.2337995 -4.2294397 -4.2384086 -4.2538238 -4.2618766 -4.274581 -4.292912 -4.3057113 -4.3161035 -4.3271508][-4.3124013 -4.3037624 -4.294765 -4.2873926 -4.2799883 -4.2754149 -4.2751818 -4.2812281 -4.2895145 -4.2936769 -4.3000097 -4.3104873 -4.3200884 -4.3281331 -4.3353806]]...]
INFO - root - 2017-12-05 17:58:04.595692: step 28410, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 78h:16m:30s remains)
INFO - root - 2017-12-05 17:58:13.590282: step 28420, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 70h:16m:41s remains)
INFO - root - 2017-12-05 17:58:22.659878: step 28430, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 77h:38m:56s remains)
INFO - root - 2017-12-05 17:58:31.885976: step 28440, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 76h:08m:16s remains)
INFO - root - 2017-12-05 17:58:41.014739: step 28450, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 76h:07m:03s remains)
INFO - root - 2017-12-05 17:58:49.930799: step 28460, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 69h:03m:49s remains)
INFO - root - 2017-12-05 17:58:58.960029: step 28470, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 77h:16m:04s remains)
INFO - root - 2017-12-05 17:59:08.020569: step 28480, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 75h:12m:07s remains)
INFO - root - 2017-12-05 17:59:16.874998: step 28490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 77h:25m:35s remains)
INFO - root - 2017-12-05 17:59:25.956890: step 28500, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 74h:07m:23s remains)
2017-12-05 17:59:26.762849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2520285 -4.2675142 -4.2753606 -4.2738328 -4.2740641 -4.2801709 -4.2839494 -4.2816505 -4.28278 -4.2838254 -4.2790122 -4.2699809 -4.2575693 -4.2460766 -4.2451353][-4.2210536 -4.2387066 -4.2519317 -4.2589474 -4.2648764 -4.2726569 -4.277246 -4.274673 -4.2814341 -4.2892842 -4.2849708 -4.2708554 -4.2498493 -4.2302904 -4.2257094][-4.1815805 -4.1972671 -4.2085476 -4.2221961 -4.2339087 -4.239399 -4.2421627 -4.2402287 -4.2530031 -4.2698507 -4.2723293 -4.2563939 -4.2273064 -4.1983085 -4.1878085][-4.1415372 -4.1523376 -4.1635175 -4.180017 -4.1933641 -4.1929779 -4.1879363 -4.1843514 -4.2026973 -4.2302523 -4.2426405 -4.2303405 -4.1966877 -4.1584568 -4.1424136][-4.0971351 -4.099112 -4.1069927 -4.117907 -4.1242886 -4.1142526 -4.0946379 -4.0879111 -4.1166778 -4.1590762 -4.1858087 -4.1816621 -4.1463037 -4.1017518 -4.0806541][-4.0663772 -4.0526595 -4.0496836 -4.0427961 -4.0331531 -4.008307 -3.9659221 -3.9504044 -3.9902182 -4.0487442 -4.0937448 -4.1022677 -4.0729728 -4.033834 -4.0149126][-4.0482273 -4.0189877 -3.9992926 -3.9751561 -3.9458036 -3.8998528 -3.8296549 -3.796756 -3.8445466 -3.9204502 -3.9884839 -4.0178742 -4.0066047 -3.982415 -3.9723902][-4.0652494 -4.0333748 -4.0035915 -3.9701355 -3.9305079 -3.8752623 -3.8007953 -3.7600496 -3.7968891 -3.8682244 -3.9412217 -3.9831374 -3.9871609 -3.9751742 -3.9674959][-4.1044693 -4.0805836 -4.0536566 -4.0189881 -3.9818664 -3.9370573 -3.886544 -3.8596539 -3.8772967 -3.9217894 -3.9757516 -4.0115361 -4.0207644 -4.0107179 -3.9939094][-4.126483 -4.114862 -4.0938826 -4.061367 -4.0294304 -3.9961412 -3.9661403 -3.9491708 -3.9497035 -3.9710824 -4.0084805 -4.0386043 -4.0518446 -4.0453711 -4.024157][-4.1243291 -4.1249013 -4.1106577 -4.0829062 -4.0581441 -4.03773 -4.02444 -4.0132637 -3.9988518 -4.000432 -4.0222769 -4.0456576 -4.0620842 -4.0629191 -4.048419][-4.0978413 -4.1112413 -4.1080928 -4.088378 -4.0680809 -4.0536108 -4.0493937 -4.0432563 -4.0248246 -4.0158682 -4.02777 -4.0460863 -4.0613046 -4.0666118 -4.0587835][-4.0769773 -4.0986013 -4.102809 -4.0914278 -4.0740118 -4.0628438 -4.0610166 -4.0558138 -4.0392904 -4.0283585 -4.0335793 -4.0463533 -4.0577192 -4.0650024 -4.0631051][-4.0740352 -4.0956669 -4.1028237 -4.0982847 -4.0857887 -4.0789156 -4.078495 -4.0728436 -4.0601916 -4.0496316 -4.0499244 -4.0575218 -4.0652852 -4.0717435 -4.0736508][-4.085279 -4.1014147 -4.10769 -4.1076307 -4.1015744 -4.098999 -4.1005688 -4.0962663 -4.0878696 -4.0810962 -4.0793333 -4.082736 -4.0870075 -4.0909777 -4.0941529]]...]
INFO - root - 2017-12-05 17:59:35.825222: step 28510, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.907 sec/batch; 76h:35m:02s remains)
INFO - root - 2017-12-05 17:59:44.856081: step 28520, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 78h:28m:41s remains)
INFO - root - 2017-12-05 17:59:53.981091: step 28530, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 76h:31m:52s remains)
INFO - root - 2017-12-05 18:00:02.894887: step 28540, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 75h:41m:49s remains)
INFO - root - 2017-12-05 18:00:11.878027: step 28550, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 77h:35m:22s remains)
INFO - root - 2017-12-05 18:00:20.985161: step 28560, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 77h:58m:05s remains)
INFO - root - 2017-12-05 18:00:29.807367: step 28570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 73h:20m:05s remains)
INFO - root - 2017-12-05 18:00:38.833374: step 28580, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 74h:58m:00s remains)
INFO - root - 2017-12-05 18:00:47.832172: step 28590, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 73h:18m:38s remains)
INFO - root - 2017-12-05 18:00:57.053735: step 28600, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 79h:59m:37s remains)
2017-12-05 18:00:57.833228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2056546 -4.1890173 -4.1559052 -4.100914 -4.0361347 -3.9923797 -4.0112381 -4.0768452 -4.1460619 -4.2046885 -4.2412705 -4.2515154 -4.2534475 -4.2618237 -4.271656][-4.2243381 -4.2058406 -4.1679535 -4.1163864 -4.0478916 -3.9907789 -3.9967608 -4.0544181 -4.1244688 -4.1889882 -4.228755 -4.2430325 -4.2460933 -4.2581949 -4.2704735][-4.2330213 -4.2144079 -4.1760592 -4.1236429 -4.0486531 -3.9769194 -3.9656672 -4.015213 -4.0901279 -4.1661005 -4.21526 -4.2382593 -4.2451792 -4.2586083 -4.2690568][-4.2367687 -4.2210441 -4.1849041 -4.1294127 -4.0450225 -3.9612827 -3.934854 -3.9768629 -4.0563426 -4.1423516 -4.1994486 -4.2335129 -4.2468147 -4.25823 -4.2630277][-4.23991 -4.2290177 -4.1951241 -4.1339955 -4.0406137 -3.9424891 -3.9010534 -3.9387298 -4.0244684 -4.1199589 -4.1876564 -4.2338152 -4.2556672 -4.2638116 -4.259707][-4.2463942 -4.2413964 -4.2109413 -4.1459055 -4.0448976 -3.9257202 -3.8610747 -3.892025 -3.98948 -4.0992246 -4.1783247 -4.2319732 -4.2570739 -4.259305 -4.24437][-4.2539635 -4.2547293 -4.2328792 -4.1700921 -4.0595989 -3.9151235 -3.8245349 -3.8538778 -3.967972 -4.0886884 -4.1738286 -4.2302461 -4.2552328 -4.25171 -4.2294431][-4.2641521 -4.2696562 -4.2569218 -4.1986609 -4.0832148 -3.9276791 -3.8239455 -3.8519397 -3.967948 -4.0812516 -4.1611018 -4.2140141 -4.2433767 -4.2412906 -4.2149744][-4.269908 -4.2763872 -4.2708588 -4.2205291 -4.1115174 -3.9623113 -3.8582137 -3.8714204 -3.9673955 -4.0622926 -4.1347075 -4.1872334 -4.2210894 -4.2218609 -4.1971517][-4.2666569 -4.2723365 -4.2734561 -4.2345428 -4.1429358 -4.0127296 -3.9134178 -3.9034262 -3.969672 -4.0476155 -4.11337 -4.1675029 -4.2019663 -4.204143 -4.18754][-4.2654324 -4.2737117 -4.2799087 -4.2524405 -4.1821127 -4.0706592 -3.9729209 -3.9410222 -3.981523 -4.0462294 -4.1036205 -4.15626 -4.1886663 -4.1951842 -4.1883836][-4.2649837 -4.2817755 -4.29346 -4.27382 -4.2208395 -4.1255589 -4.0319257 -3.9889281 -4.0115743 -4.0647192 -4.1183391 -4.1695952 -4.2014828 -4.2101645 -4.2071762][-4.26698 -4.2954388 -4.3103848 -4.2952971 -4.2554936 -4.1824045 -4.1050611 -4.0624666 -4.0714331 -4.1083469 -4.1521683 -4.1957078 -4.2209163 -4.2279563 -4.2277932][-4.2655182 -4.3053694 -4.3223686 -4.312583 -4.2811675 -4.22958 -4.1742539 -4.1365242 -4.1330228 -4.1529579 -4.1841111 -4.2193446 -4.2405858 -4.249177 -4.2551537][-4.2586465 -4.30847 -4.3281813 -4.3232894 -4.2973971 -4.2598257 -4.2222972 -4.1932468 -4.18409 -4.192112 -4.2133837 -4.2427721 -4.2638354 -4.2777071 -4.288444]]...]
INFO - root - 2017-12-05 18:01:06.884042: step 28610, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 75h:37m:13s remains)
INFO - root - 2017-12-05 18:01:16.081445: step 28620, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 77h:24m:07s remains)
INFO - root - 2017-12-05 18:01:25.065038: step 28630, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 78h:11m:40s remains)
INFO - root - 2017-12-05 18:01:34.120073: step 28640, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 75h:23m:54s remains)
INFO - root - 2017-12-05 18:01:43.192891: step 28650, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 76h:43m:39s remains)
INFO - root - 2017-12-05 18:01:52.233640: step 28660, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 77h:03m:26s remains)
INFO - root - 2017-12-05 18:02:01.339235: step 28670, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 75h:42m:43s remains)
INFO - root - 2017-12-05 18:02:10.402743: step 28680, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 76h:24m:39s remains)
INFO - root - 2017-12-05 18:02:19.543027: step 28690, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 77h:02m:45s remains)
INFO - root - 2017-12-05 18:02:28.587745: step 28700, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 74h:13m:09s remains)
2017-12-05 18:02:29.427911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3176827 -4.318305 -4.3154907 -4.3103719 -4.3035245 -4.2987108 -4.3016286 -4.3093081 -4.3152404 -4.3189659 -4.321239 -4.3219256 -4.3223667 -4.3207235 -4.3194227][-4.3115363 -4.3056774 -4.294558 -4.2800241 -4.2665329 -4.2614026 -4.2704468 -4.2868476 -4.3010883 -4.3106589 -4.3162141 -4.3183422 -4.3180571 -4.314342 -4.3119493][-4.3001204 -4.2846971 -4.2610469 -4.2344122 -4.2136784 -4.2064643 -4.2169347 -4.2367187 -4.2599831 -4.2786541 -4.29028 -4.2961483 -4.2986121 -4.296217 -4.295577][-4.2850647 -4.2593112 -4.2226839 -4.1860337 -4.1621361 -4.1511784 -4.1544557 -4.1693897 -4.1972051 -4.22498 -4.2408028 -4.2517147 -4.2628231 -4.2685137 -4.273757][-4.267591 -4.2330356 -4.1839466 -4.1396585 -4.1136537 -4.0941329 -4.0812311 -4.0872645 -4.1216941 -4.1620913 -4.1806068 -4.1939049 -4.21738 -4.2364292 -4.2517271][-4.25266 -4.2130241 -4.1530714 -4.0994306 -4.0657949 -4.0278311 -3.9859457 -3.9843795 -4.0365887 -4.0966439 -4.1213303 -4.1394176 -4.1787243 -4.2150192 -4.2423248][-4.2443681 -4.2040725 -4.1384811 -4.07322 -4.0242734 -3.957303 -3.8714948 -3.8585432 -3.9439223 -4.0296063 -4.0668516 -4.0981417 -4.1543765 -4.2083 -4.2471323][-4.242053 -4.2054205 -4.1407738 -4.0696497 -4.0086012 -3.9172266 -3.7878695 -3.7585504 -3.8766358 -3.9875269 -4.0413671 -4.0884991 -4.1552858 -4.2169528 -4.2602916][-4.2474375 -4.2147107 -4.1575351 -4.090981 -4.0311728 -3.9412129 -3.8127804 -3.7740059 -3.8892393 -4.0060539 -4.0713363 -4.1278257 -4.19061 -4.2429457 -4.2784176][-4.261642 -4.2298784 -4.1778584 -4.1206641 -4.069706 -4.0035591 -3.9185429 -3.8913274 -3.9734573 -4.0713348 -4.138999 -4.1955671 -4.2437162 -4.2761903 -4.2959657][-4.2781644 -4.2482996 -4.201169 -4.1513443 -4.1118703 -4.0686932 -4.0260024 -4.0187597 -4.0732231 -4.1472034 -4.208972 -4.257719 -4.2883892 -4.3031068 -4.30931][-4.2935381 -4.2704926 -4.2309804 -4.1899347 -4.1621594 -4.1356249 -4.1157 -4.1197524 -4.1573858 -4.2097163 -4.2578235 -4.2950792 -4.3116937 -4.3162961 -4.3155189][-4.3055706 -4.2929568 -4.2660341 -4.2380309 -4.2225947 -4.2090673 -4.1984859 -4.2032895 -4.2261009 -4.2569523 -4.286191 -4.3085527 -4.3156385 -4.3161058 -4.3148465][-4.313302 -4.3104539 -4.2977562 -4.2826204 -4.2777238 -4.2739267 -4.2693367 -4.270936 -4.2799139 -4.2922769 -4.3039508 -4.3131795 -4.3143911 -4.313602 -4.3136411][-4.3164511 -4.3188043 -4.3160048 -4.3097606 -4.3088064 -4.3103247 -4.3093734 -4.3090873 -4.3100729 -4.3126068 -4.3144217 -4.3151264 -4.3141489 -4.3138471 -4.315073]]...]
INFO - root - 2017-12-05 18:02:38.438711: step 28710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 76h:07m:59s remains)
INFO - root - 2017-12-05 18:02:47.665212: step 28720, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.936 sec/batch; 78h:57m:00s remains)
INFO - root - 2017-12-05 18:02:56.765695: step 28730, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 75h:58m:49s remains)
INFO - root - 2017-12-05 18:03:05.894885: step 28740, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 75h:00m:19s remains)
INFO - root - 2017-12-05 18:03:14.995915: step 28750, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 75h:53m:51s remains)
INFO - root - 2017-12-05 18:03:24.039877: step 28760, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 74h:43m:37s remains)
INFO - root - 2017-12-05 18:03:32.974656: step 28770, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 77h:55m:59s remains)
INFO - root - 2017-12-05 18:03:42.107755: step 28780, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 77h:14m:17s remains)
INFO - root - 2017-12-05 18:03:51.104274: step 28790, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 75h:22m:34s remains)
INFO - root - 2017-12-05 18:04:00.150587: step 28800, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 76h:17m:39s remains)
2017-12-05 18:04:00.921203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1952329 -4.2015753 -4.1983376 -4.196177 -4.2017736 -4.2072091 -4.2091508 -4.2027068 -4.1862216 -4.1626539 -4.1418023 -4.142952 -4.1622748 -4.1639204 -4.1543303][-4.2076421 -4.2115021 -4.2085814 -4.2110534 -4.2199812 -4.2220964 -4.2190561 -4.20744 -4.1867366 -4.1623373 -4.1446095 -4.1500168 -4.1701851 -4.1680183 -4.1505418][-4.2200756 -4.2215066 -4.2242 -4.2363715 -4.24671 -4.2447758 -4.2355976 -4.2201505 -4.2005529 -4.1811452 -4.1713953 -4.1793318 -4.1978703 -4.1911707 -4.16029][-4.2286677 -4.2293334 -4.2412858 -4.2623158 -4.2709923 -4.2623768 -4.24241 -4.2172289 -4.1972809 -4.1859808 -4.1871767 -4.2037292 -4.22715 -4.2209272 -4.1772485][-4.2227654 -4.2244482 -4.2405744 -4.2618957 -4.267951 -4.2525811 -4.2222772 -4.1889782 -4.1682172 -4.1613092 -4.1727877 -4.2017374 -4.2365427 -4.2428875 -4.1991496][-4.2163277 -4.218523 -4.2324739 -4.2439632 -4.2411823 -4.2186069 -4.1790123 -4.1366796 -4.1144714 -4.1091523 -4.1304374 -4.1753168 -4.2253051 -4.2473893 -4.2174397][-4.2078714 -4.2128997 -4.2261076 -4.2288885 -4.2191238 -4.1907525 -4.138875 -4.0839291 -4.056283 -4.0512118 -4.0780635 -4.1376443 -4.2037635 -4.2426324 -4.2281523][-4.1941171 -4.1987824 -4.2186918 -4.2263389 -4.2187972 -4.1918793 -4.1361275 -4.0735197 -4.0393338 -4.0283532 -4.0544658 -4.1179681 -4.1921706 -4.2388783 -4.2334924][-4.1784935 -4.1788521 -4.2042904 -4.2240162 -4.22893 -4.2142549 -4.170094 -4.1137671 -4.0796232 -4.0660186 -4.0848165 -4.13546 -4.1997323 -4.2393713 -4.2329669][-4.1696444 -4.1657114 -4.19357 -4.2222071 -4.2374067 -4.2353034 -4.2079186 -4.1701155 -4.1459908 -4.1362753 -4.1473885 -4.1772223 -4.2197704 -4.240046 -4.2226381][-4.1771765 -4.171875 -4.1952629 -4.2234378 -4.2398348 -4.244173 -4.2270031 -4.2037091 -4.1905055 -4.1865516 -4.1956859 -4.2147026 -4.2410479 -4.2435846 -4.2139945][-4.1886821 -4.1811461 -4.1936626 -4.2160082 -4.2296348 -4.2320738 -4.2180219 -4.2012439 -4.1933641 -4.1933708 -4.2021966 -4.2210894 -4.2436562 -4.2419338 -4.2125754][-4.2048492 -4.1978383 -4.2008939 -4.2123513 -4.2193007 -4.2163877 -4.202436 -4.1896091 -4.1887918 -4.19318 -4.2032413 -4.2240262 -4.2460265 -4.2441964 -4.2211533][-4.2391243 -4.2356358 -4.2354474 -4.2367964 -4.2352896 -4.2268977 -4.2163072 -4.2085137 -4.2130122 -4.2197042 -4.2281313 -4.24686 -4.2641253 -4.2577071 -4.237689][-4.2819362 -4.2832417 -4.2850347 -4.2820153 -4.2766762 -4.2651114 -4.2576509 -4.2540426 -4.2592354 -4.2651296 -4.2710309 -4.2863388 -4.2963028 -4.2838221 -4.2620921]]...]
INFO - root - 2017-12-05 18:04:10.007092: step 28810, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 77h:47m:05s remains)
INFO - root - 2017-12-05 18:04:19.017235: step 28820, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 78h:09m:16s remains)
INFO - root - 2017-12-05 18:04:28.262312: step 28830, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 77h:38m:39s remains)
INFO - root - 2017-12-05 18:04:37.244601: step 28840, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 77h:15m:48s remains)
INFO - root - 2017-12-05 18:04:46.272406: step 28850, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 76h:36m:47s remains)
INFO - root - 2017-12-05 18:04:55.154343: step 28860, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 77h:40m:18s remains)
INFO - root - 2017-12-05 18:05:04.234742: step 28870, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 75h:22m:51s remains)
INFO - root - 2017-12-05 18:05:13.235641: step 28880, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 78h:52m:00s remains)
INFO - root - 2017-12-05 18:05:22.170789: step 28890, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 70h:59m:44s remains)
INFO - root - 2017-12-05 18:05:31.406095: step 28900, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 76h:46m:29s remains)
2017-12-05 18:05:32.207254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1391344 -4.128942 -4.1172118 -4.1013765 -4.0868282 -4.0796318 -4.0942144 -4.1337252 -4.1737928 -4.1875072 -4.2001672 -4.2304378 -4.2570229 -4.2782426 -4.2928944][-4.1379528 -4.1286011 -4.1140924 -4.0964837 -4.087935 -4.0837822 -4.0891786 -4.1261163 -4.1691566 -4.1782894 -4.1879082 -4.2207026 -4.2560368 -4.2815895 -4.2951093][-4.1520371 -4.1433563 -4.1287 -4.1150918 -4.1058049 -4.0833626 -4.0585618 -4.0877209 -4.1412868 -4.1630177 -4.1774416 -4.2153711 -4.2572403 -4.2864575 -4.29786][-4.1670485 -4.1564856 -4.1449981 -4.137063 -4.1309867 -4.0866714 -4.0252333 -4.0393314 -4.110343 -4.1514149 -4.17307 -4.2166243 -4.257966 -4.2886367 -4.2992282][-4.1665764 -4.1551337 -4.1473575 -4.1398668 -4.1321106 -4.0743713 -3.9863839 -3.9928224 -4.0788078 -4.1358533 -4.1636949 -4.2102804 -4.2540336 -4.284657 -4.2961936][-4.1460586 -4.1351733 -4.1238217 -4.1068554 -4.09266 -4.0241256 -3.91057 -3.9199996 -4.0278983 -4.098062 -4.1320629 -4.1875715 -4.2413063 -4.2744 -4.2880335][-4.1024466 -4.0824161 -4.0579414 -4.0243316 -3.9977875 -3.9049308 -3.7495623 -3.7644868 -3.90949 -4.0039949 -4.056982 -4.1332769 -4.2064381 -4.2511373 -4.2724109][-4.0692468 -4.0379019 -4.0014591 -3.9587288 -3.9237456 -3.8164177 -3.6310835 -3.6392114 -3.8057492 -3.9119189 -3.9791293 -4.0761557 -4.16858 -4.2250962 -4.2570691][-4.0793886 -4.0418377 -4.0008225 -3.9637218 -3.9390345 -3.8552313 -3.7088277 -3.7151687 -3.8456678 -3.9222085 -3.9744141 -4.0663362 -4.159133 -4.2181416 -4.2563381][-4.1160574 -4.0758109 -4.032774 -4.0011826 -3.9840662 -3.9271858 -3.836338 -3.8537743 -3.9404941 -3.987572 -4.0248227 -4.1008797 -4.1799879 -4.2332664 -4.2690496][-4.1631174 -4.1270633 -4.0883 -4.0621114 -4.0480032 -4.0103755 -3.9547048 -3.97646 -4.0343051 -4.0666656 -4.0984674 -4.1573582 -4.219183 -4.2618265 -4.2886553][-4.2089005 -4.18203 -4.1526375 -4.1333795 -4.1239505 -4.1007514 -4.0649686 -4.0836477 -4.1261592 -4.1497469 -4.1754518 -4.217103 -4.2598848 -4.28909 -4.3059468][-4.2423182 -4.2247667 -4.2058744 -4.1944537 -4.1915112 -4.1805992 -4.1598496 -4.1716928 -4.1988711 -4.2147617 -4.2322741 -4.2597737 -4.2875357 -4.3076839 -4.3168983][-4.2627172 -4.2518158 -4.2406845 -4.2360282 -4.2368455 -4.2332253 -4.2236614 -4.2290845 -4.242013 -4.2510209 -4.2621779 -4.2791529 -4.2964768 -4.3097568 -4.3143096][-4.2746205 -4.2664294 -4.2598462 -4.2576575 -4.2591152 -4.2588706 -4.2569137 -4.259212 -4.2634053 -4.26742 -4.2740755 -4.2829428 -4.2929311 -4.3008981 -4.3039608]]...]
INFO - root - 2017-12-05 18:05:41.459438: step 28910, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 76h:53m:54s remains)
INFO - root - 2017-12-05 18:05:50.620664: step 28920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 77h:07m:33s remains)
INFO - root - 2017-12-05 18:05:59.565738: step 28930, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 74h:49m:46s remains)
INFO - root - 2017-12-05 18:06:08.627338: step 28940, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 75h:35m:42s remains)
INFO - root - 2017-12-05 18:06:17.881407: step 28950, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 77h:11m:46s remains)
INFO - root - 2017-12-05 18:06:26.814918: step 28960, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 75h:58m:28s remains)
INFO - root - 2017-12-05 18:06:35.685806: step 28970, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 75h:36m:58s remains)
INFO - root - 2017-12-05 18:06:44.676496: step 28980, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 78h:17m:03s remains)
INFO - root - 2017-12-05 18:06:53.895278: step 28990, loss = 2.02, batch loss = 1.97 (9.1 examples/sec; 0.878 sec/batch; 74h:00m:12s remains)
INFO - root - 2017-12-05 18:07:03.091834: step 29000, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 75h:10m:34s remains)
2017-12-05 18:07:03.935334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2525806 -4.2340846 -4.2136192 -4.1936283 -4.1811967 -4.195858 -4.2199912 -4.2281642 -4.2318749 -4.2342987 -4.2320142 -4.2343588 -4.240622 -4.2465568 -4.2562656][-4.2295227 -4.2167063 -4.1982112 -4.1758709 -4.1578641 -4.1717396 -4.2029109 -4.2201009 -4.2275743 -4.2276731 -4.2248235 -4.2297778 -4.23671 -4.2432528 -4.2537794][-4.2161503 -4.2128844 -4.1987996 -4.1703486 -4.1467462 -4.1580677 -4.188149 -4.2056694 -4.2107434 -4.2101974 -4.2140932 -4.2258515 -4.2350564 -4.24155 -4.2527561][-4.2169557 -4.21929 -4.2060223 -4.1781526 -4.15515 -4.1606131 -4.1800275 -4.1890006 -4.1883645 -4.1887636 -4.1963415 -4.2157345 -4.2330928 -4.2461286 -4.2608776][-4.2070785 -4.2097774 -4.1991959 -4.1781583 -4.15848 -4.157095 -4.1612406 -4.1593966 -4.1596842 -4.16473 -4.1743813 -4.1971226 -4.2208152 -4.2429771 -4.2613578][-4.1789975 -4.1821704 -4.180027 -4.1689806 -4.1573324 -4.1500177 -4.1407547 -4.13186 -4.1359925 -4.1453466 -4.160975 -4.1826072 -4.2034392 -4.2265997 -4.2499728][-4.1525207 -4.1497908 -4.1500664 -4.1439996 -4.1392813 -4.1340055 -4.1208262 -4.1126847 -4.1231465 -4.1365581 -4.1555309 -4.1765056 -4.1918283 -4.2098823 -4.23701][-4.1293941 -4.1154041 -4.1099458 -4.1083364 -4.1102648 -4.11309 -4.1074576 -4.1066518 -4.1186943 -4.1328897 -4.1585402 -4.1827717 -4.1909914 -4.1985173 -4.2207065][-4.1084914 -4.0935531 -4.0868573 -4.0935783 -4.1051431 -4.1151466 -4.1133757 -4.1089339 -4.1159954 -4.130445 -4.1601348 -4.1884785 -4.1966066 -4.1975079 -4.2110686][-4.09948 -4.101707 -4.1020427 -4.1118774 -4.1262536 -4.1388383 -4.1390514 -4.1285648 -4.1272469 -4.1409497 -4.1709237 -4.2044034 -4.2183695 -4.2179537 -4.221179][-4.1177373 -4.1345139 -4.1415181 -4.1486449 -4.159802 -4.167943 -4.164392 -4.1509752 -4.1474757 -4.1649733 -4.193892 -4.222012 -4.2374911 -4.2397871 -4.2377553][-4.1500182 -4.1591206 -4.1606941 -4.1641378 -4.17205 -4.1743784 -4.1673818 -4.1572795 -4.1598821 -4.1828365 -4.2129049 -4.2373419 -4.249928 -4.2542133 -4.2485223][-4.1725836 -4.1672168 -4.1589465 -4.1588712 -4.1645584 -4.1631713 -4.1567311 -4.1545315 -4.1666546 -4.1929607 -4.2244678 -4.243957 -4.2497487 -4.2531896 -4.24372][-4.1793737 -4.1654873 -4.1507139 -4.1483431 -4.1529722 -4.1550679 -4.1535821 -4.1568265 -4.1690378 -4.1928449 -4.2229185 -4.23798 -4.2371578 -4.2379637 -4.2223091][-4.1780663 -4.1620255 -4.1475072 -4.1462231 -4.1501417 -4.1561379 -4.1602421 -4.1642523 -4.1690779 -4.1845379 -4.21237 -4.2250271 -4.2224727 -4.2190852 -4.1995411]]...]
INFO - root - 2017-12-05 18:07:13.001034: step 29010, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 75h:35m:33s remains)
INFO - root - 2017-12-05 18:07:22.057313: step 29020, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 75h:59m:46s remains)
INFO - root - 2017-12-05 18:07:31.185161: step 29030, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 77h:37m:05s remains)
INFO - root - 2017-12-05 18:07:40.407870: step 29040, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.894 sec/batch; 75h:23m:05s remains)
INFO - root - 2017-12-05 18:07:49.324829: step 29050, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 74h:18m:32s remains)
INFO - root - 2017-12-05 18:07:58.393156: step 29060, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 74h:09m:40s remains)
INFO - root - 2017-12-05 18:08:07.438937: step 29070, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 77h:59m:39s remains)
INFO - root - 2017-12-05 18:08:16.318688: step 29080, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 78h:11m:11s remains)
INFO - root - 2017-12-05 18:08:25.359670: step 29090, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.887 sec/batch; 74h:47m:15s remains)
INFO - root - 2017-12-05 18:08:34.521165: step 29100, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.903 sec/batch; 76h:06m:43s remains)
2017-12-05 18:08:35.340462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438564 -4.2383866 -4.2359748 -4.2349968 -4.2355695 -4.2362857 -4.2376385 -4.2426963 -4.2529149 -4.2643743 -4.2705235 -4.2689018 -4.2664051 -4.2652812 -4.2647338][-4.22698 -4.2177048 -4.2124729 -4.2100549 -4.2102208 -4.2121229 -4.2160659 -4.2266531 -4.2439394 -4.2611852 -4.2717562 -4.2736635 -4.27418 -4.2723465 -4.2702394][-4.2061167 -4.1916413 -4.1823697 -4.1743689 -4.1690888 -4.1693382 -4.1767597 -4.1941905 -4.2209735 -4.2475414 -4.2669883 -4.2789717 -4.285954 -4.2845035 -4.2798824][-4.1794677 -4.15832 -4.1403189 -4.1215034 -4.1056833 -4.1018081 -4.1113038 -4.1337943 -4.17011 -4.2104883 -4.246 -4.2742333 -4.2924652 -4.2953782 -4.2913227][-4.1558571 -4.1285019 -4.0987797 -4.0643711 -4.0327487 -4.0181851 -4.0199394 -4.0377474 -4.0794168 -4.1360736 -4.1944456 -4.2458429 -4.2800827 -4.2941794 -4.2967515][-4.137989 -4.1074409 -4.0704136 -4.0244184 -3.9778438 -3.944133 -3.9220607 -3.9202676 -3.9636657 -4.041048 -4.1255269 -4.2000403 -4.2477727 -4.2721753 -4.2833819][-4.123558 -4.0905843 -4.0473461 -3.9960718 -3.9388223 -3.8811765 -3.8199909 -3.7910795 -3.83961 -3.9377618 -4.0442877 -4.1347003 -4.1931105 -4.2250347 -4.2431269][-4.1190557 -4.0874333 -4.0446148 -3.9953597 -3.9325461 -3.8514783 -3.7473807 -3.689085 -3.7418385 -3.8507035 -3.9682782 -4.0706625 -4.1373558 -4.171885 -4.1889958][-4.1248007 -4.103888 -4.0723696 -4.0341711 -3.9761181 -3.8934343 -3.7838755 -3.7153556 -3.7438216 -3.8275564 -3.9323294 -4.0323248 -4.1022582 -4.1409359 -4.153008][-4.1361451 -4.1245031 -4.109015 -4.0844183 -4.0421591 -3.9822509 -3.9043634 -3.8501015 -3.848834 -3.8861136 -3.9551175 -4.0352669 -4.1013269 -4.1398978 -4.1485076][-4.1444426 -4.1377349 -4.133584 -4.126646 -4.1032591 -4.0645733 -4.0143809 -3.9731412 -3.9535563 -3.9601812 -4.0010233 -4.0620785 -4.120286 -4.1574545 -4.1657081][-4.1415329 -4.1376743 -4.140182 -4.1467309 -4.1420207 -4.1203356 -4.0872526 -4.0515332 -4.0217328 -4.0151467 -4.0431361 -4.0924249 -4.1491437 -4.1912746 -4.203691][-4.1217923 -4.1188259 -4.1242671 -4.1387196 -4.1477838 -4.1429086 -4.1236272 -4.0958481 -4.0710363 -4.0676084 -4.0948114 -4.13653 -4.1910238 -4.2378016 -4.2548342][-4.0971761 -4.0922012 -4.0965505 -4.1148648 -4.1304212 -4.1336703 -4.1250868 -4.1128254 -4.1069622 -4.1191039 -4.1497769 -4.1878214 -4.2370682 -4.2799449 -4.297729][-4.0675306 -4.0594683 -4.0597477 -4.074369 -4.0852323 -4.087152 -4.0838366 -4.0882597 -4.1038208 -4.1368155 -4.1794348 -4.21779 -4.2579737 -4.2918839 -4.3076315]]...]
INFO - root - 2017-12-05 18:08:44.420785: step 29110, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 74h:58m:20s remains)
INFO - root - 2017-12-05 18:08:53.386160: step 29120, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 75h:57m:46s remains)
INFO - root - 2017-12-05 18:09:02.476202: step 29130, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 75h:15m:54s remains)
INFO - root - 2017-12-05 18:09:11.804824: step 29140, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.944 sec/batch; 79h:32m:57s remains)
INFO - root - 2017-12-05 18:09:20.935054: step 29150, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 77h:20m:46s remains)
INFO - root - 2017-12-05 18:09:29.963961: step 29160, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 76h:50m:27s remains)
INFO - root - 2017-12-05 18:09:39.045539: step 29170, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 73h:31m:21s remains)
INFO - root - 2017-12-05 18:09:47.843276: step 29180, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 72h:24m:05s remains)
INFO - root - 2017-12-05 18:09:56.855400: step 29190, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 76h:12m:48s remains)
INFO - root - 2017-12-05 18:10:05.850440: step 29200, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 77h:18m:09s remains)
2017-12-05 18:10:06.707257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2651677 -4.2342162 -4.1987171 -4.1628523 -4.1370826 -4.1042829 -4.067874 -4.0743093 -4.1226463 -4.1192613 -4.1078086 -4.1074333 -4.1102109 -4.1097379 -4.1115642][-4.2464209 -4.2023768 -4.1539884 -4.1103892 -4.0803118 -4.0435395 -4.0054073 -4.0135956 -4.0744905 -4.0677552 -4.0516138 -4.0557833 -4.0683365 -4.0683622 -4.0611453][-4.2270808 -4.1751113 -4.1237979 -4.0840778 -4.0590863 -4.0222006 -3.9769735 -3.9845309 -4.0485086 -4.034276 -4.0136795 -4.0191965 -4.0372453 -4.0361748 -4.0204487][-4.2066612 -4.150898 -4.1050372 -4.0793252 -4.062696 -4.0222445 -3.9669971 -3.9759555 -4.0436077 -4.03101 -4.0098295 -4.0124207 -4.0266361 -4.0233483 -4.0077991][-4.1904812 -4.1290345 -4.0807133 -4.0611944 -4.0480704 -4.0022011 -3.9380336 -3.9451714 -4.0212941 -4.0303173 -4.0228882 -4.0251117 -4.0341058 -4.0354824 -4.0352044][-4.1750364 -4.1037436 -4.0440197 -4.0181 -3.9970846 -3.9373996 -3.8523755 -3.8544984 -3.9441733 -3.9780734 -3.9854596 -3.9919915 -4.0123348 -4.0352893 -4.0594358][-4.1526814 -4.0656376 -3.9898958 -3.9469578 -3.8930151 -3.7875919 -3.6517148 -3.6533408 -3.7956138 -3.8827078 -3.9165256 -3.9387128 -3.9832733 -4.0345936 -4.0730252][-4.1292562 -4.03679 -3.9519448 -3.8874774 -3.7890837 -3.6154351 -3.4185188 -3.4437191 -3.6647952 -3.8098788 -3.8638964 -3.8955312 -3.9564943 -4.0233746 -4.0694289][-4.1164441 -4.03271 -3.9578509 -3.8957653 -3.7985926 -3.6424146 -3.487227 -3.5113337 -3.6976228 -3.8152585 -3.8574162 -3.8879614 -3.9499974 -4.0217443 -4.0680666][-4.1288123 -4.0545754 -3.9912329 -3.9446502 -3.8789439 -3.787694 -3.7076118 -3.7235811 -3.8343079 -3.8973694 -3.912745 -3.9324632 -3.9845726 -4.0426269 -4.081389][-4.1566019 -4.0882096 -4.0300789 -3.9950802 -3.9568172 -3.9104352 -3.8733525 -3.886344 -3.9552808 -3.9936206 -3.9962003 -4.0043902 -4.0402389 -4.0807104 -4.1106796][-4.1940231 -4.1288228 -4.0735536 -4.044064 -4.0273714 -4.0046959 -3.985817 -3.9953649 -4.0411725 -4.0668244 -4.0634737 -4.0635333 -4.0833063 -4.1109009 -4.1361403][-4.2382832 -4.1803713 -4.1315422 -4.1067162 -4.0963511 -4.0828691 -4.0737486 -4.0838442 -4.1156421 -4.1309161 -4.12585 -4.1247768 -4.1339936 -4.1496506 -4.1668167][-4.2780848 -4.2333064 -4.1961565 -4.1766748 -4.16813 -4.1601305 -4.1589131 -4.1662927 -4.181788 -4.1859074 -4.1832094 -4.1865325 -4.194562 -4.2041521 -4.2140145][-4.3072429 -4.2767973 -4.2516723 -4.2397351 -4.2372379 -4.2362547 -4.2382388 -4.2406535 -4.2435369 -4.2425208 -4.2410803 -4.2442408 -4.2504377 -4.25673 -4.2636642]]...]
INFO - root - 2017-12-05 18:10:15.859926: step 29210, loss = 2.01, batch loss = 1.95 (8.7 examples/sec; 0.920 sec/batch; 77h:31m:51s remains)
INFO - root - 2017-12-05 18:10:24.887821: step 29220, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 75h:05m:06s remains)
INFO - root - 2017-12-05 18:10:33.937441: step 29230, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 76h:18m:55s remains)
INFO - root - 2017-12-05 18:10:43.011461: step 29240, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 80h:22m:39s remains)
INFO - root - 2017-12-05 18:10:52.177105: step 29250, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 79h:36m:57s remains)
INFO - root - 2017-12-05 18:11:01.351008: step 29260, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 78h:37m:52s remains)
INFO - root - 2017-12-05 18:11:10.530936: step 29270, loss = 2.09, batch loss = 2.03 (7.2 examples/sec; 1.111 sec/batch; 93h:35m:13s remains)
INFO - root - 2017-12-05 18:11:19.543186: step 29280, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 76h:03m:58s remains)
INFO - root - 2017-12-05 18:11:28.551760: step 29290, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 75h:28m:54s remains)
INFO - root - 2017-12-05 18:11:37.556633: step 29300, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 73h:20m:20s remains)
2017-12-05 18:11:38.335006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3211751 -4.3181829 -4.3181329 -4.3183236 -4.3162031 -4.3123217 -4.3059988 -4.2984786 -4.2926235 -4.2910466 -4.29555 -4.30232 -4.3103275 -4.3168011 -4.3214421][-4.2894077 -4.286531 -4.2881942 -4.2892056 -4.2868109 -4.2810612 -4.272481 -4.2596221 -4.2493582 -4.2496738 -4.2599297 -4.2719388 -4.2849216 -4.2970977 -4.3049955][-4.248786 -4.2483592 -4.2537627 -4.2565069 -4.2548923 -4.2483468 -4.2362156 -4.2172275 -4.2018352 -4.2049518 -4.2222457 -4.2396674 -4.2584195 -4.2763219 -4.28813][-4.2040796 -4.206614 -4.2158008 -4.220005 -4.2174945 -4.2099619 -4.194459 -4.1689858 -4.1486712 -4.1586094 -4.1859832 -4.2106709 -4.2353249 -4.2577209 -4.2762108][-4.1762471 -4.1834884 -4.1968174 -4.2013125 -4.1941657 -4.1798053 -4.155364 -4.1163778 -4.0874281 -4.108202 -4.1490774 -4.1824965 -4.2121415 -4.2402935 -4.269134][-4.1662288 -4.1781635 -4.19434 -4.1966324 -4.1810651 -4.1529984 -4.1101189 -4.0490746 -4.0077295 -4.0402775 -4.0985169 -4.1464391 -4.1849189 -4.2218814 -4.2626529][-4.167254 -4.1830807 -4.2000747 -4.1973991 -4.1710157 -4.1253324 -4.0595436 -3.9743135 -3.9257421 -3.9750922 -4.0532742 -4.1167588 -4.1641617 -4.2075973 -4.2582664][-4.1785131 -4.1987286 -4.217124 -4.2084908 -4.169126 -4.1063995 -4.0222092 -3.9206061 -3.8732467 -3.9361503 -4.0282478 -4.1033173 -4.1577406 -4.2059746 -4.2628617][-4.1959257 -4.2182093 -4.2355175 -4.2202578 -4.1674571 -4.0924511 -4.0083208 -3.9243176 -3.898869 -3.9593148 -4.0460095 -4.1192207 -4.1741209 -4.2209148 -4.2751451][-4.2183604 -4.242764 -4.2594576 -4.2407389 -4.1816587 -4.10637 -4.038022 -3.9895434 -3.9907618 -4.04255 -4.1106811 -4.1711869 -4.216773 -4.2537475 -4.2959766][-4.2235079 -4.2502308 -4.2672782 -4.2490683 -4.1933465 -4.1294765 -4.0827875 -4.06229 -4.0792465 -4.1214175 -4.1725364 -4.21915 -4.2539525 -4.2804046 -4.3110633][-4.2209058 -4.2492614 -4.2663317 -4.2524924 -4.2068081 -4.1616449 -4.1364608 -4.1353951 -4.1578383 -4.1882629 -4.2211714 -4.2519627 -4.2744246 -4.2922177 -4.3158865][-4.2239189 -4.2531514 -4.2693157 -4.2593956 -4.225893 -4.1995368 -4.194 -4.2054172 -4.2251258 -4.24121 -4.25644 -4.2725978 -4.2847009 -4.2962589 -4.3167968][-4.2343116 -4.2603369 -4.2762179 -4.2707343 -4.247231 -4.2350459 -4.2435765 -4.2615523 -4.2768235 -4.282712 -4.28535 -4.2911577 -4.2950091 -4.3011217 -4.3187938][-4.2534785 -4.2757869 -4.290061 -4.2875037 -4.27247 -4.2687206 -4.2802134 -4.2968621 -4.3084922 -4.3098664 -4.307981 -4.3084588 -4.3073797 -4.3100848 -4.3237271]]...]
INFO - root - 2017-12-05 18:11:47.481022: step 29310, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 73h:47m:24s remains)
INFO - root - 2017-12-05 18:11:56.641750: step 29320, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 78h:10m:22s remains)
INFO - root - 2017-12-05 18:12:05.569549: step 29330, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 79h:45m:07s remains)
INFO - root - 2017-12-05 18:12:14.670060: step 29340, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 76h:41m:14s remains)
INFO - root - 2017-12-05 18:12:23.834907: step 29350, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 76h:37m:09s remains)
INFO - root - 2017-12-05 18:12:33.000996: step 29360, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 76h:30m:42s remains)
INFO - root - 2017-12-05 18:12:41.958570: step 29370, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 76h:42m:28s remains)
INFO - root - 2017-12-05 18:12:51.242179: step 29380, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 76h:16m:50s remains)
INFO - root - 2017-12-05 18:13:00.320411: step 29390, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 76h:38m:57s remains)
INFO - root - 2017-12-05 18:13:09.383665: step 29400, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 74h:53m:11s remains)
2017-12-05 18:13:10.307570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2646489 -4.26738 -4.270937 -4.2742109 -4.2706771 -4.2638969 -4.2566233 -4.247653 -4.2452254 -4.2424479 -4.2343559 -4.2165084 -4.1866188 -4.1716232 -4.178359][-4.2372007 -4.2461576 -4.2580152 -4.2683463 -4.2663083 -4.2535248 -4.241848 -4.2371216 -4.2412448 -4.2434487 -4.2418628 -4.2319565 -4.2085185 -4.194499 -4.19945][-4.2087507 -4.2250867 -4.2454195 -4.2599316 -4.2561359 -4.2321019 -4.2116141 -4.2118297 -4.2247272 -4.2312613 -4.2348361 -4.23364 -4.2205496 -4.210072 -4.2117915][-4.1736073 -4.2005734 -4.227026 -4.239697 -4.2246752 -4.1819129 -4.1499524 -4.1585374 -4.1853614 -4.2013493 -4.2112079 -4.217411 -4.2156506 -4.2133927 -4.2153978][-4.1543045 -4.1827478 -4.20438 -4.2031875 -4.1651411 -4.0966058 -4.0561094 -4.0825429 -4.1311569 -4.1667829 -4.1910148 -4.2078242 -4.2162657 -4.2204318 -4.2218747][-4.1581903 -4.1799059 -4.1872396 -4.16314 -4.0936403 -3.9943957 -3.9401839 -3.9901295 -4.0681095 -4.1344891 -4.1814818 -4.2128768 -4.2291455 -4.2341442 -4.2327003][-4.1819382 -4.1887908 -4.1739793 -4.1219344 -4.020997 -3.8926656 -3.825866 -3.9048581 -4.019258 -4.1209531 -4.1939096 -4.2377739 -4.2570643 -4.2588558 -4.2533259][-4.2154446 -4.2017322 -4.1627774 -4.0842485 -3.9603674 -3.8165123 -3.7503164 -3.8587019 -4.001296 -4.1284733 -4.2180276 -4.2678676 -4.2843485 -4.280293 -4.270155][-4.244175 -4.214582 -4.1601081 -4.0667167 -3.9322195 -3.7865562 -3.7269433 -3.8516576 -4.0098991 -4.1503587 -4.2457418 -4.2953053 -4.3083963 -4.2989111 -4.2848473][-4.2672734 -4.2307668 -4.1714678 -4.0781975 -3.9486706 -3.816642 -3.7660937 -3.8855996 -4.039886 -4.1780944 -4.2710538 -4.3164964 -4.3267622 -4.3142982 -4.2992344][-4.2837706 -4.2484407 -4.1924706 -4.1077418 -3.9900963 -3.8751605 -3.8334188 -3.937794 -4.0780096 -4.2036428 -4.2882886 -4.3280716 -4.3358932 -4.323988 -4.3113885][-4.2931356 -4.262671 -4.2143717 -4.1433935 -4.046576 -3.9521525 -3.9197242 -4.0047154 -4.1227584 -4.2287216 -4.3000817 -4.3329291 -4.3394804 -4.3303094 -4.3213272][-4.2977104 -4.2728267 -4.2352066 -4.182446 -4.1140556 -4.0493584 -4.0306973 -4.0946989 -4.1825824 -4.2599864 -4.3105741 -4.3323936 -4.336256 -4.3293295 -4.3229785][-4.2992568 -4.2815623 -4.2561646 -4.2213984 -4.1783924 -4.1421018 -4.1355228 -4.1793675 -4.2376208 -4.2877626 -4.3189597 -4.331336 -4.3321805 -4.3272843 -4.3229647][-4.3005462 -4.2893658 -4.2760181 -4.2588258 -4.2368765 -4.2192054 -4.218082 -4.2430248 -4.2763186 -4.3038092 -4.320538 -4.3271351 -4.32742 -4.3248696 -4.3224044]]...]
INFO - root - 2017-12-05 18:13:19.233487: step 29410, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.891 sec/batch; 74h:58m:44s remains)
INFO - root - 2017-12-05 18:13:28.440996: step 29420, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 76h:47m:03s remains)
INFO - root - 2017-12-05 18:13:37.656226: step 29430, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 75h:33m:53s remains)
INFO - root - 2017-12-05 18:13:46.751701: step 29440, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 76h:03m:40s remains)
INFO - root - 2017-12-05 18:13:55.883894: step 29450, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 79h:10m:48s remains)
INFO - root - 2017-12-05 18:14:04.901175: step 29460, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 75h:39m:12s remains)
INFO - root - 2017-12-05 18:14:14.032022: step 29470, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 76h:00m:38s remains)
INFO - root - 2017-12-05 18:14:23.041398: step 29480, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 76h:51m:11s remains)
INFO - root - 2017-12-05 18:14:31.954049: step 29490, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 76h:47m:16s remains)
INFO - root - 2017-12-05 18:14:41.094887: step 29500, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 78h:08m:07s remains)
2017-12-05 18:14:41.906900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1693192 -4.1735611 -4.1693053 -4.1650767 -4.1796722 -4.1896219 -4.177011 -4.1637659 -4.1548238 -4.1481419 -4.1549606 -4.1479807 -4.1318793 -4.1158342 -4.1060567][-4.1241126 -4.1278644 -4.1232467 -4.1202745 -4.1451254 -4.1634164 -4.1536484 -4.1419492 -4.1307945 -4.1295843 -4.1432433 -4.1359291 -4.1139674 -4.0945573 -4.0755987][-4.073246 -4.0731196 -4.0633082 -4.0556846 -4.0822091 -4.105092 -4.0991096 -4.0903082 -4.0831885 -4.0933857 -4.1130986 -4.1055632 -4.0830593 -4.0665517 -4.0447264][-4.0305991 -4.0226865 -4.0023675 -3.9810629 -4.0039449 -4.0337567 -4.0362377 -4.0373158 -4.0420308 -4.0656819 -4.0893006 -4.0882835 -4.0752997 -4.0705004 -4.0488205][-4.0034575 -3.9920571 -3.9591889 -3.925519 -3.9469042 -3.9816916 -4.0044522 -4.0147605 -4.025115 -4.0556355 -4.0831747 -4.0875344 -4.0864549 -4.0943871 -4.0750732][-3.9988759 -3.9873245 -3.9563472 -3.9192958 -3.9188161 -3.943038 -3.9723749 -3.982645 -3.9995241 -4.0405922 -4.0809188 -4.0901723 -4.0968447 -4.1072388 -4.0923462][-4.0207443 -4.0085225 -3.9828391 -3.9454889 -3.9170594 -3.907145 -3.9186211 -3.9209065 -3.9462111 -4.0075622 -4.0627561 -4.0744591 -4.0823374 -4.0950723 -4.0922089][-4.0473905 -4.0339117 -4.0096021 -3.9722958 -3.922178 -3.8712192 -3.8591461 -3.8550439 -3.9001064 -3.9780886 -4.0294819 -4.0351868 -4.040575 -4.0557566 -4.0670795][-4.060504 -4.0407305 -4.0159564 -3.979872 -3.9234803 -3.8579319 -3.8359594 -3.8427265 -3.8936729 -3.9625208 -3.9952345 -3.9899566 -3.991652 -4.0067554 -4.0298719][-4.076869 -4.0567718 -4.037262 -4.0074773 -3.9659727 -3.9200268 -3.9098263 -3.9243119 -3.957567 -3.9945509 -4.0057874 -3.9910796 -3.9902523 -4.0020008 -4.0282211][-4.09866 -4.0860085 -4.0754647 -4.0518374 -4.0275793 -4.0014544 -4.0054517 -4.020165 -4.0375051 -4.0442944 -4.0420651 -4.0296507 -4.0274615 -4.0406518 -4.0650563][-4.1334243 -4.129806 -4.1241255 -4.1044626 -4.0888844 -4.0744762 -4.0828552 -4.0975747 -4.1072206 -4.105535 -4.1010981 -4.0940704 -4.0954037 -4.1091161 -4.1242909][-4.1766157 -4.180439 -4.1800609 -4.1668777 -4.1548882 -4.1444244 -4.1531687 -4.1667228 -4.1742754 -4.1708074 -4.1693273 -4.1637888 -4.1653061 -4.1725292 -4.1807785][-4.2134414 -4.219707 -4.2224193 -4.216114 -4.207972 -4.2001405 -4.2057295 -4.2187672 -4.2259545 -4.22211 -4.2204356 -4.2158985 -4.2129059 -4.2096424 -4.212441][-4.2397122 -4.2454128 -4.2494197 -4.2491512 -4.245326 -4.2416954 -4.2464008 -4.25797 -4.2622561 -4.2572689 -4.2547455 -4.2512965 -4.2463722 -4.2389231 -4.2391386]]...]
INFO - root - 2017-12-05 18:14:50.927483: step 29510, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.889 sec/batch; 74h:47m:21s remains)
INFO - root - 2017-12-05 18:14:59.787248: step 29520, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 74h:02m:55s remains)
INFO - root - 2017-12-05 18:15:08.857261: step 29530, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 74h:09m:26s remains)
INFO - root - 2017-12-05 18:15:17.873071: step 29540, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 74h:38m:18s remains)
INFO - root - 2017-12-05 18:15:26.749722: step 29550, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 77h:01m:02s remains)
INFO - root - 2017-12-05 18:15:35.873167: step 29560, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 76h:59m:01s remains)
INFO - root - 2017-12-05 18:15:44.964922: step 29570, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.900 sec/batch; 75h:45m:07s remains)
INFO - root - 2017-12-05 18:15:53.906919: step 29580, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 74h:51m:02s remains)
INFO - root - 2017-12-05 18:16:03.021215: step 29590, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 79h:03m:01s remains)
INFO - root - 2017-12-05 18:16:12.044184: step 29600, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 75h:43m:19s remains)
2017-12-05 18:16:12.807542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1604753 -4.16875 -4.1811156 -4.1905184 -4.2042689 -4.2143149 -4.2155504 -4.2185307 -4.225997 -4.2330551 -4.2312441 -4.2179337 -4.1973829 -4.179769 -4.1739335][-4.1639943 -4.1745453 -4.19192 -4.2043762 -4.2150869 -4.2169137 -4.2130466 -4.2180982 -4.2291374 -4.2371006 -4.2341595 -4.2191167 -4.1937566 -4.17034 -4.1639891][-4.1899686 -4.1991596 -4.2156172 -4.2269478 -4.2311907 -4.2213383 -4.2072306 -4.2078567 -4.2212324 -4.2347074 -4.2366304 -4.2239876 -4.1985025 -4.1776233 -4.1765871][-4.2230191 -4.2278662 -4.2379045 -4.2405925 -4.2270083 -4.1976738 -4.172421 -4.1710949 -4.1932259 -4.2228608 -4.245111 -4.2481318 -4.2310877 -4.2137814 -4.2158461][-4.2406645 -4.2397761 -4.2407565 -4.2270231 -4.1868095 -4.1326003 -4.095171 -4.0958037 -4.1415911 -4.206224 -4.2613692 -4.2867613 -4.2838755 -4.2697573 -4.2648587][-4.2216358 -4.2191782 -4.2091269 -4.1741009 -4.1020184 -4.01341 -3.9457393 -3.9397724 -4.0200744 -4.1320238 -4.2294731 -4.2826557 -4.3003139 -4.2949371 -4.2879848][-4.1849685 -4.1730366 -4.1445203 -4.0810003 -3.972384 -3.8293624 -3.7006612 -3.6750994 -3.8005943 -3.9792392 -4.1285639 -4.218595 -4.2647376 -4.2784424 -4.2807012][-4.1713529 -4.1496277 -4.0958309 -3.9977534 -3.8432059 -3.6397946 -3.4355698 -3.3806686 -3.557327 -3.8041372 -4.0051451 -4.1338143 -4.2070355 -4.2396393 -4.252089][-4.1911759 -4.1709237 -4.1140819 -4.0170865 -3.8775182 -3.6955514 -3.5137954 -3.4664879 -3.6129584 -3.8214335 -3.9948854 -4.1081295 -4.1746378 -4.2039132 -4.2181845][-4.2448325 -4.2259417 -4.1765709 -4.10053 -3.9991388 -3.8783071 -3.7727056 -3.7637889 -3.8628218 -3.9945445 -4.1023498 -4.1699915 -4.2057233 -4.2140541 -4.2139087][-4.282444 -4.2635093 -4.2252231 -4.1727357 -4.1052556 -4.0268683 -3.9724641 -3.9870067 -4.0553122 -4.1376996 -4.1977787 -4.2303348 -4.2420359 -4.2326822 -4.2206922][-4.2837915 -4.2614574 -4.2305851 -4.1960268 -4.1529989 -4.1066184 -4.0844316 -4.1106725 -4.1628032 -4.21355 -4.2394543 -4.2489085 -4.25057 -4.238409 -4.2214336][-4.2645688 -4.2413816 -4.2147703 -4.1926789 -4.1675119 -4.1456661 -4.1476469 -4.1830115 -4.2264214 -4.2567263 -4.2644591 -4.265862 -4.2674708 -4.2551208 -4.2321572][-4.2655296 -4.2440009 -4.2246685 -4.2121754 -4.1967134 -4.186018 -4.1969275 -4.2308178 -4.26646 -4.2864718 -4.2862735 -4.2870011 -4.2910876 -4.2788119 -4.2545061][-4.267056 -4.2499051 -4.2366571 -4.2262344 -4.2129216 -4.2033997 -4.2107248 -4.2366958 -4.26679 -4.284358 -4.2859211 -4.288229 -4.29242 -4.2799125 -4.2570858]]...]
INFO - root - 2017-12-05 18:16:21.845733: step 29610, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 76h:38m:38s remains)
INFO - root - 2017-12-05 18:16:30.908229: step 29620, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.925 sec/batch; 77h:48m:02s remains)
INFO - root - 2017-12-05 18:16:39.866176: step 29630, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 74h:12m:06s remains)
INFO - root - 2017-12-05 18:16:48.739625: step 29640, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.861 sec/batch; 72h:24m:55s remains)
INFO - root - 2017-12-05 18:16:58.111425: step 29650, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 78h:31m:53s remains)
INFO - root - 2017-12-05 18:17:07.077777: step 29660, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 77h:32m:56s remains)
INFO - root - 2017-12-05 18:17:16.305150: step 29670, loss = 2.08, batch loss = 2.02 (7.6 examples/sec; 1.047 sec/batch; 88h:03m:54s remains)
INFO - root - 2017-12-05 18:17:25.549660: step 29680, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 78h:25m:46s remains)
INFO - root - 2017-12-05 18:17:34.621440: step 29690, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 80h:14m:18s remains)
INFO - root - 2017-12-05 18:17:43.599307: step 29700, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.804 sec/batch; 67h:36m:38s remains)
2017-12-05 18:17:44.414922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3359776 -4.3228297 -4.3082905 -4.2823949 -4.2306681 -4.1544166 -4.0913358 -4.0649657 -4.0679359 -4.085103 -4.09649 -4.0975575 -4.1166296 -4.1491351 -4.1875672][-4.3337965 -4.318604 -4.3057346 -4.283946 -4.2318935 -4.143404 -4.0684295 -4.0442014 -4.0608349 -4.0951834 -4.117456 -4.1169481 -4.1302872 -4.1621923 -4.2016339][-4.3312573 -4.3143063 -4.3040338 -4.2865615 -4.2346277 -4.1365247 -4.0470047 -4.0222659 -4.0587382 -4.1146688 -4.149087 -4.1486073 -4.1591873 -4.188776 -4.22093][-4.3289938 -4.30955 -4.3007751 -4.2874985 -4.2357626 -4.13079 -4.0301251 -4.0053096 -4.0567307 -4.1348429 -4.1828842 -4.1849413 -4.1943846 -4.2186828 -4.242403][-4.3276896 -4.3059816 -4.2969823 -4.2865124 -4.2353082 -4.1236458 -4.0140538 -3.9878292 -4.050065 -4.14803 -4.2126846 -4.2194266 -4.2250381 -4.2454414 -4.2622662][-4.3277016 -4.3044491 -4.2932396 -4.2837238 -4.2344031 -4.1193352 -4.0004091 -3.970726 -4.0435696 -4.158803 -4.2362452 -4.2494178 -4.2538004 -4.2692132 -4.2783976][-4.3286538 -4.3042269 -4.289927 -4.2791576 -4.2320957 -4.1191711 -3.9944584 -3.9562254 -4.0324149 -4.1577663 -4.2438731 -4.2654953 -4.271409 -4.2806053 -4.2800922][-4.32964 -4.3044591 -4.2875361 -4.2758961 -4.2326903 -4.1292009 -4.007369 -3.9551835 -4.0173783 -4.1424203 -4.23332 -4.2647734 -4.27369 -4.2793078 -4.2724419][-4.3297911 -4.3040667 -4.2849984 -4.2734051 -4.2365146 -4.1469603 -4.038527 -3.979331 -4.0158315 -4.1252208 -4.2149262 -4.2554173 -4.2684274 -4.2717328 -4.2628713][-4.3291154 -4.3022933 -4.2826581 -4.2719483 -4.242682 -4.1707721 -4.0851593 -4.0336051 -4.0470042 -4.1233907 -4.2007232 -4.2461033 -4.2631159 -4.263155 -4.2539387][-4.3273849 -4.2994113 -4.2800994 -4.2707453 -4.2487645 -4.1950083 -4.1358104 -4.1011491 -4.10358 -4.1445131 -4.1950049 -4.2376223 -4.2591233 -4.2585816 -4.2470922][-4.3251381 -4.2964239 -4.2766752 -4.2677989 -4.2516356 -4.2135277 -4.17705 -4.1609988 -4.1615081 -4.1794815 -4.202415 -4.233079 -4.2541733 -4.2547684 -4.243309][-4.3236651 -4.293817 -4.2730517 -4.2628241 -4.2485418 -4.2209291 -4.1999111 -4.199708 -4.2044044 -4.2129564 -4.2168012 -4.2308383 -4.2475019 -4.2491746 -4.2405386][-4.3239269 -4.2932024 -4.2701797 -4.2570262 -4.2409377 -4.2191658 -4.208221 -4.2174258 -4.2291508 -4.2375197 -4.2337976 -4.2333589 -4.2421532 -4.2415824 -4.2342343][-4.3267727 -4.2965517 -4.2700372 -4.2520537 -4.2334523 -4.215148 -4.21111 -4.223413 -4.2371063 -4.2463923 -4.2443509 -4.2385368 -4.2410297 -4.2377615 -4.2334847]]...]
INFO - root - 2017-12-05 18:17:53.671825: step 29710, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 76h:21m:30s remains)
INFO - root - 2017-12-05 18:18:02.864002: step 29720, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 77h:50m:46s remains)
INFO - root - 2017-12-05 18:18:11.904852: step 29730, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 75h:20m:33s remains)
INFO - root - 2017-12-05 18:18:21.001604: step 29740, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.937 sec/batch; 78h:49m:49s remains)
INFO - root - 2017-12-05 18:18:30.298972: step 29750, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 79h:10m:37s remains)
INFO - root - 2017-12-05 18:18:39.351053: step 29760, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 76h:05m:32s remains)
INFO - root - 2017-12-05 18:18:48.399487: step 29770, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 76h:53m:49s remains)
INFO - root - 2017-12-05 18:18:57.600246: step 29780, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 75h:08m:42s remains)
INFO - root - 2017-12-05 18:19:06.712298: step 29790, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 75h:05m:44s remains)
INFO - root - 2017-12-05 18:19:15.648014: step 29800, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 79h:18m:39s remains)
2017-12-05 18:19:16.443838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2870193 -4.2840481 -4.2834511 -4.2847614 -4.2906327 -4.2975321 -4.3031945 -4.3081055 -4.3081307 -4.3001833 -4.286727 -4.2732654 -4.2602777 -4.2488194 -4.2398958][-4.2571936 -4.2555943 -4.2568917 -4.2604074 -4.26722 -4.2742667 -4.2803521 -4.2856312 -4.2847357 -4.2776375 -4.267168 -4.2543492 -4.2407475 -4.22772 -4.2152815][-4.2302656 -4.2282872 -4.2295065 -4.2357249 -4.2401781 -4.2433972 -4.2460341 -4.248683 -4.2480297 -4.2478924 -4.2473106 -4.2410178 -4.2313175 -4.2242842 -4.2151194][-4.2205629 -4.2089744 -4.1987662 -4.1999326 -4.1985507 -4.1951132 -4.1897535 -4.1840324 -4.1885009 -4.2061124 -4.2240658 -4.233325 -4.2382112 -4.2422733 -4.2396531][-4.2205758 -4.186913 -4.1534967 -4.1380348 -4.1264586 -4.10825 -4.0883961 -4.0753031 -4.0954404 -4.143126 -4.1865907 -4.2161031 -4.2419004 -4.2620678 -4.2693529][-4.2196679 -4.1617112 -4.0947504 -4.044837 -4.0070181 -3.9661973 -3.9303379 -3.9218183 -3.9652691 -4.0418973 -4.1113958 -4.1587877 -4.2061906 -4.2448077 -4.2644453][-4.20907 -4.138823 -4.05052 -3.9657512 -3.8948812 -3.8289056 -3.789578 -3.8091779 -3.8821678 -3.9668512 -4.0389519 -4.0967112 -4.1631894 -4.2194285 -4.2513227][-4.2210236 -4.1629891 -4.078331 -3.9824059 -3.8944378 -3.8257537 -3.7979431 -3.8337021 -3.9042358 -3.9642038 -4.0172825 -4.0726767 -4.1464982 -4.2105727 -4.2482319][-4.2460718 -4.2176394 -4.1557527 -4.0781069 -4.0056639 -3.9542637 -3.9399908 -3.9697163 -4.0101018 -4.036231 -4.0693979 -4.1160378 -4.1778779 -4.2312965 -4.2651272][-4.2502432 -4.2531686 -4.2249746 -4.1804152 -4.1371908 -4.1078534 -4.0988379 -4.1165257 -4.1361413 -4.1459746 -4.1690035 -4.205215 -4.2508273 -4.2832475 -4.3047009][-4.2370892 -4.2588387 -4.2593474 -4.2460141 -4.2331643 -4.2240629 -4.2196455 -4.2271333 -4.2388926 -4.2498846 -4.2690234 -4.2943597 -4.3206954 -4.3316865 -4.3395333][-4.2449269 -4.2690592 -4.2826872 -4.2899103 -4.293355 -4.2955074 -4.2927356 -4.2922831 -4.3001909 -4.3140311 -4.3327522 -4.3477621 -4.3598366 -4.3610039 -4.3614573][-4.2851796 -4.3032007 -4.3166747 -4.3288379 -4.3355188 -4.3390417 -4.3341 -4.3294258 -4.3347321 -4.3469205 -4.3593154 -4.363543 -4.3642788 -4.363255 -4.3638883][-4.3280659 -4.3396955 -4.346643 -4.354311 -4.3574457 -4.3578763 -4.3535743 -4.350018 -4.3527522 -4.3584003 -4.3625164 -4.3604984 -4.35685 -4.355463 -4.3555989][-4.3563733 -4.3619475 -4.3627882 -4.3633323 -4.3619704 -4.3598547 -4.3564258 -4.3531528 -4.352695 -4.3528376 -4.3521013 -4.3494062 -4.3464894 -4.3451557 -4.344965]]...]
INFO - root - 2017-12-05 18:19:25.636121: step 29810, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 76h:09m:13s remains)
INFO - root - 2017-12-05 18:19:34.740599: step 29820, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 77h:22m:44s remains)
INFO - root - 2017-12-05 18:19:43.943762: step 29830, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 79h:36m:51s remains)
INFO - root - 2017-12-05 18:19:53.140755: step 29840, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 75h:58m:52s remains)
INFO - root - 2017-12-05 18:20:02.324601: step 29850, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 77h:56m:05s remains)
INFO - root - 2017-12-05 18:20:11.475397: step 29860, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 76h:23m:19s remains)
INFO - root - 2017-12-05 18:20:20.735448: step 29870, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.961 sec/batch; 80h:49m:02s remains)
INFO - root - 2017-12-05 18:20:30.014268: step 29880, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 75h:41m:33s remains)
INFO - root - 2017-12-05 18:20:38.936718: step 29890, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 77h:56m:55s remains)
INFO - root - 2017-12-05 18:20:48.092725: step 29900, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 70h:41m:58s remains)
2017-12-05 18:20:48.869253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1991167 -4.2054143 -4.2157631 -4.2226233 -4.2236323 -4.208962 -4.1829762 -4.1600418 -4.152226 -4.1685815 -4.1907735 -4.2136092 -4.2239776 -4.2348213 -4.2388659][-4.1922078 -4.2026186 -4.2147045 -4.2201915 -4.2249103 -4.2155881 -4.1931524 -4.166739 -4.1540041 -4.1663361 -4.1827626 -4.200664 -4.208611 -4.2187834 -4.221921][-4.168623 -4.1796656 -4.19314 -4.1974592 -4.2097969 -4.216126 -4.2023311 -4.1809807 -4.17462 -4.1834598 -4.1934118 -4.2030554 -4.2039714 -4.2029934 -4.2011132][-4.1255279 -4.1360974 -4.1491871 -4.1600657 -4.1790652 -4.1966863 -4.1912937 -4.1820388 -4.1910396 -4.2058854 -4.2134876 -4.2170653 -4.2072749 -4.1882305 -4.1794677][-4.09217 -4.1040454 -4.1201358 -4.1342764 -4.1583352 -4.1729016 -4.1580458 -4.155272 -4.190608 -4.2208977 -4.2306013 -4.2301693 -4.2072148 -4.1722717 -4.1528683][-4.0905108 -4.0883412 -4.1000934 -4.1136422 -4.1329851 -4.1234565 -4.075604 -4.068047 -4.1366 -4.195652 -4.2175426 -4.2178016 -4.1932836 -4.154192 -4.1329126][-4.0925603 -4.056787 -4.0536723 -4.0538983 -4.0535541 -4.0066061 -3.8974836 -3.8572752 -3.967804 -4.0749936 -4.1180859 -4.1361957 -4.1356668 -4.1122041 -4.1000261][-4.098259 -4.0326004 -4.0070977 -3.9910753 -3.972044 -3.8954508 -3.7346675 -3.6454523 -3.8013752 -3.9507318 -4.0109787 -4.0536628 -4.0912628 -4.0936918 -4.0922728][-4.1391721 -4.0724096 -4.0414371 -4.0272593 -4.0079813 -3.9486532 -3.8250427 -3.7577338 -3.8670111 -3.9833059 -4.0289912 -4.0685582 -4.1137028 -4.1273742 -4.1318436][-4.197113 -4.1529403 -4.1265469 -4.115859 -4.1051712 -4.0726957 -4.0057516 -3.9727058 -4.0280056 -4.0920196 -4.119102 -4.1437759 -4.1762385 -4.1849003 -4.1863937][-4.2291574 -4.2113538 -4.1963024 -4.1908607 -4.1856627 -4.1660242 -4.1339293 -4.1195712 -4.14683 -4.1834092 -4.2045975 -4.2250142 -4.2457871 -4.2457829 -4.2369704][-4.2246079 -4.2303181 -4.2301788 -4.2328916 -4.2341895 -4.2238693 -4.2083936 -4.207777 -4.2265282 -4.2465334 -4.2607903 -4.27632 -4.2872934 -4.2786412 -4.2653422][-4.2009349 -4.2175727 -4.2318254 -4.2422795 -4.2521582 -4.25029 -4.2434187 -4.2476487 -4.2608747 -4.272182 -4.2778635 -4.2840958 -4.2894859 -4.2770605 -4.2631121][-4.1886058 -4.2052269 -4.2258506 -4.2396 -4.2521214 -4.2551126 -4.2513304 -4.2547483 -4.2606144 -4.2649779 -4.2681165 -4.2702675 -4.2711525 -4.262928 -4.2533927][-4.2016997 -4.2117596 -4.2287178 -4.2386131 -4.2482271 -4.2497835 -4.2454948 -4.2454152 -4.2463827 -4.2467093 -4.2490716 -4.251123 -4.2519045 -4.2507796 -4.2477]]...]
INFO - root - 2017-12-05 18:20:57.982298: step 29910, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 77h:04m:01s remains)
INFO - root - 2017-12-05 18:21:07.001817: step 29920, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 71h:08m:16s remains)
INFO - root - 2017-12-05 18:21:16.309077: step 29930, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 79h:39m:04s remains)
INFO - root - 2017-12-05 18:21:25.423825: step 29940, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 75h:53m:47s remains)
INFO - root - 2017-12-05 18:21:34.597740: step 29950, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 79h:41m:35s remains)
INFO - root - 2017-12-05 18:21:43.728231: step 29960, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 79h:14m:02s remains)
INFO - root - 2017-12-05 18:21:53.033777: step 29970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 76h:35m:18s remains)
INFO - root - 2017-12-05 18:22:02.156052: step 29980, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 76h:12m:17s remains)
INFO - root - 2017-12-05 18:22:11.090031: step 29990, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 75h:33m:34s remains)
INFO - root - 2017-12-05 18:22:20.354767: step 30000, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 77h:59m:06s remains)
2017-12-05 18:22:21.191813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1668453 -4.1962318 -4.2176948 -4.2370811 -4.2531104 -4.2714734 -4.2900176 -4.2977552 -4.2875776 -4.2734065 -4.2751994 -4.2946877 -4.316144 -4.3253164 -4.3170633][-4.1652327 -4.2020144 -4.2331629 -4.2579184 -4.2739182 -4.2909021 -4.3082328 -4.3189888 -4.3194528 -4.317853 -4.3238544 -4.3374934 -4.34568 -4.3409638 -4.3223834][-4.1680589 -4.1983919 -4.227963 -4.2515254 -4.2614994 -4.266293 -4.2742324 -4.2858925 -4.2978039 -4.3128934 -4.3262992 -4.3348784 -4.3303256 -4.3118229 -4.2831068][-4.1909666 -4.2065911 -4.2222934 -4.2306466 -4.2239804 -4.2123194 -4.2083545 -4.2158122 -4.2304239 -4.2538691 -4.2749662 -4.2838354 -4.2794557 -4.2612519 -4.2319889][-4.2237797 -4.2206984 -4.2122712 -4.1970668 -4.1718326 -4.1482983 -4.1331325 -4.1335664 -4.1473789 -4.1744146 -4.2001939 -4.2162075 -4.2205133 -4.2116632 -4.190475][-4.2520227 -4.2357864 -4.2075 -4.171031 -4.1214561 -4.073431 -4.0382962 -4.029984 -4.050004 -4.0833106 -4.117836 -4.1455016 -4.1656685 -4.1704159 -4.1611757][-4.2637544 -4.2444992 -4.2099242 -4.1609378 -4.0874057 -4.0089731 -3.95367 -3.9430232 -3.9726176 -4.0091715 -4.0467787 -4.0839376 -4.1185422 -4.1402106 -4.1504364][-4.2590909 -4.2415361 -4.2095375 -4.1576023 -4.0734072 -3.9830863 -3.9288244 -3.9296105 -3.961112 -3.9940848 -4.0252781 -4.0600758 -4.0954003 -4.12209 -4.14202][-4.2452784 -4.2243228 -4.189065 -4.1320333 -4.0475931 -3.9680057 -3.9354107 -3.959815 -4.0032444 -4.0385914 -4.0594873 -4.0753412 -4.085218 -4.0930967 -4.1079249][-4.213316 -4.1834621 -4.1433916 -4.0886464 -4.0222454 -3.9709544 -3.9665346 -4.0088706 -4.0572052 -4.0865073 -4.0891218 -4.0737743 -4.0522313 -4.0475421 -4.0644584][-4.1630774 -4.1367111 -4.1089873 -4.0779309 -4.0457921 -4.0247474 -4.0340462 -4.0743055 -4.1166797 -4.1347132 -4.11553 -4.0740476 -4.0366373 -4.0383925 -4.0674839][-4.1401763 -4.1397905 -4.1366339 -4.1278057 -4.1143532 -4.1060276 -4.11776 -4.1500511 -4.1807065 -4.1885624 -4.1636262 -4.119575 -4.0885925 -4.0942559 -4.1197462][-4.1610475 -4.1840324 -4.1959915 -4.1973772 -4.1917176 -4.1886272 -4.1978955 -4.2205443 -4.2388067 -4.2398572 -4.2137871 -4.1797729 -4.1599326 -4.1621656 -4.1710281][-4.1886992 -4.2227244 -4.2363772 -4.2399278 -4.2370877 -4.2345223 -4.2389951 -4.2516236 -4.2609606 -4.2585835 -4.239377 -4.2177811 -4.2016788 -4.1956682 -4.1894393][-4.18005 -4.2191138 -4.2331972 -4.2391191 -4.2401052 -4.2397203 -4.244102 -4.2532492 -4.2580056 -4.2529073 -4.2364831 -4.2201996 -4.204493 -4.1929407 -4.1795373]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 18:22:30.625756: step 30010, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 77h:03m:11s remains)
INFO - root - 2017-12-05 18:22:39.721635: step 30020, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 76h:07m:36s remains)
INFO - root - 2017-12-05 18:22:48.824719: step 30030, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.904 sec/batch; 75h:57m:37s remains)
INFO - root - 2017-12-05 18:22:57.964233: step 30040, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 79h:00m:46s remains)
INFO - root - 2017-12-05 18:23:07.086577: step 30050, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 76h:19m:47s remains)
INFO - root - 2017-12-05 18:23:16.277162: step 30060, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 77h:36m:15s remains)
INFO - root - 2017-12-05 18:23:25.451510: step 30070, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 73h:14m:12s remains)
INFO - root - 2017-12-05 18:23:34.511103: step 30080, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 73h:07m:20s remains)
INFO - root - 2017-12-05 18:23:43.748182: step 30090, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.907 sec/batch; 76h:10m:28s remains)
INFO - root - 2017-12-05 18:23:52.871149: step 30100, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 77h:21m:53s remains)
2017-12-05 18:23:53.680413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3120532 -4.309433 -4.3125234 -4.3202 -4.3303862 -4.3376193 -4.3394852 -4.3388481 -4.3371325 -4.3336253 -4.3294854 -4.3266587 -4.3257809 -4.3265409 -4.3283753][-4.2943439 -4.293366 -4.2998986 -4.3122797 -4.3273787 -4.3372884 -4.3403964 -4.3399582 -4.3363976 -4.3293538 -4.3214068 -4.3157377 -4.3142476 -4.3155608 -4.319943][-4.2684846 -4.2730045 -4.2879295 -4.3078761 -4.3266263 -4.3329382 -4.3317008 -4.3296032 -4.3254366 -4.3167734 -4.3049664 -4.2941809 -4.2902894 -4.2920651 -4.2998285][-4.2370739 -4.2491221 -4.2760925 -4.3044424 -4.3232994 -4.3213425 -4.3110919 -4.3035331 -4.300766 -4.298161 -4.2902827 -4.2767959 -4.2697887 -4.2694974 -4.2761784][-4.1939635 -4.2153716 -4.2527809 -4.2870703 -4.3017607 -4.2865415 -4.2629786 -4.2487922 -4.2526088 -4.2660503 -4.2732697 -4.2648516 -4.2570925 -4.2540426 -4.2581744][-4.152833 -4.179975 -4.2194252 -4.250936 -4.2477717 -4.2072744 -4.1639738 -4.1476359 -4.1685224 -4.210835 -4.2421803 -4.2475491 -4.245276 -4.2432008 -4.2461119][-4.1371889 -4.1602607 -4.1864405 -4.2021847 -4.1738086 -4.0971737 -4.0303197 -4.0180354 -4.0617514 -4.1349645 -4.1916842 -4.2154026 -4.2250142 -4.2314725 -4.2388253][-4.1384406 -4.1579967 -4.1699519 -4.169486 -4.1280308 -4.0354028 -3.9623253 -3.9564667 -4.0067396 -4.0879641 -4.152739 -4.1868782 -4.20649 -4.2246332 -4.2405272][-4.1419086 -4.155591 -4.1613975 -4.1602159 -4.1304407 -4.0635853 -4.0139971 -4.0076447 -4.0366225 -4.0924063 -4.1473103 -4.1815081 -4.2075505 -4.2344818 -4.2521186][-4.1642318 -4.1698103 -4.1712122 -4.1755562 -4.1696115 -4.1414542 -4.1176214 -4.1115308 -4.1170163 -4.1408591 -4.1752119 -4.2047119 -4.2308893 -4.255291 -4.2664461][-4.1867661 -4.18837 -4.1892962 -4.1998587 -4.2107573 -4.2083673 -4.203279 -4.2001739 -4.1969938 -4.2020769 -4.2181988 -4.238905 -4.2593813 -4.2748127 -4.2785392][-4.1928635 -4.195941 -4.2047825 -4.2236829 -4.240912 -4.2496753 -4.2539458 -4.2546263 -4.2495441 -4.2482209 -4.2544293 -4.2683454 -4.2826204 -4.2908072 -4.2908683][-4.1994014 -4.2089405 -4.2270517 -4.250833 -4.2671852 -4.2746758 -4.2813935 -4.2841272 -4.2782354 -4.2729921 -4.2746429 -4.2842736 -4.293232 -4.2964091 -4.2951651][-4.2066073 -4.2226081 -4.2475934 -4.2752171 -4.2900267 -4.2927008 -4.2959414 -4.2963371 -4.2898612 -4.2836785 -4.2854471 -4.2920046 -4.2951794 -4.2923279 -4.28847][-4.2157035 -4.2365656 -4.2668033 -4.2967615 -4.3071594 -4.3055696 -4.304719 -4.3034992 -4.2994409 -4.2949653 -4.2975397 -4.3004894 -4.2973046 -4.2888875 -4.2826357]]...]
INFO - root - 2017-12-05 18:24:02.617393: step 30110, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 76h:02m:47s remains)
INFO - root - 2017-12-05 18:24:11.816395: step 30120, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 72h:08m:15s remains)
INFO - root - 2017-12-05 18:24:20.898185: step 30130, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.937 sec/batch; 78h:41m:41s remains)
INFO - root - 2017-12-05 18:24:30.011653: step 30140, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 77h:05m:45s remains)
INFO - root - 2017-12-05 18:24:39.288744: step 30150, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 76h:45m:23s remains)
INFO - root - 2017-12-05 18:24:48.365281: step 30160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 77h:52m:22s remains)
INFO - root - 2017-12-05 18:24:57.352938: step 30170, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 77h:52m:17s remains)
INFO - root - 2017-12-05 18:25:06.749642: step 30180, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 77h:05m:32s remains)
INFO - root - 2017-12-05 18:25:15.932295: step 30190, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 75h:51m:48s remains)
INFO - root - 2017-12-05 18:25:25.087834: step 30200, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 78h:11m:48s remains)
2017-12-05 18:25:25.817149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2785878 -4.2796211 -4.2733092 -4.2600007 -4.245326 -4.2332892 -4.2311277 -4.2388339 -4.2496686 -4.2481432 -4.225523 -4.1992903 -4.1781535 -4.1621 -4.1542807][-4.24356 -4.2392526 -4.2248816 -4.2003889 -4.1779718 -4.1627412 -4.1651874 -4.1854792 -4.2129021 -4.2223444 -4.2031913 -4.1760836 -4.1508732 -4.1288939 -4.1150007][-4.2121277 -4.197979 -4.1711268 -4.1335773 -4.1001511 -4.0809631 -4.0915174 -4.1308146 -4.1826029 -4.2092671 -4.1980639 -4.1725807 -4.1445475 -4.1148353 -4.0900869][-4.1959963 -4.1728988 -4.1324158 -4.0844307 -4.041111 -4.0161967 -4.0319033 -4.0882206 -4.1661043 -4.2155061 -4.2154403 -4.1915069 -4.1569371 -4.1173415 -4.0788665][-4.2025971 -4.1760969 -4.1248345 -4.0610256 -3.9961061 -3.9490714 -3.9576526 -4.0308719 -4.1411743 -4.2220631 -4.2406888 -4.2215729 -4.1822577 -4.1348543 -4.0864458][-4.2169018 -4.19319 -4.135869 -4.046875 -3.9415708 -3.8483088 -3.8336351 -3.9319692 -4.0867724 -4.2089453 -4.2553511 -4.2506328 -4.212501 -4.1619167 -4.1072903][-4.2292075 -4.2110763 -4.1475019 -4.0308123 -3.8765934 -3.7181442 -3.6687274 -3.8007274 -4.0088515 -4.1719937 -4.2490482 -4.2667093 -4.2376895 -4.1856179 -4.1261344][-4.2326541 -4.222465 -4.1595073 -4.0286646 -3.8415234 -3.6302338 -3.5424166 -3.6955533 -3.9379668 -4.1279879 -4.230135 -4.2710981 -4.253798 -4.2015676 -4.1373854][-4.2309942 -4.232914 -4.1842947 -4.0665755 -3.8922567 -3.6940427 -3.5961335 -3.7224092 -3.9440107 -4.1242518 -4.2285266 -4.2764306 -4.2635789 -4.2070155 -4.1371312][-4.2240543 -4.2417827 -4.2139444 -4.124464 -3.9945018 -3.8556135 -3.7826562 -3.8597951 -4.02008 -4.1585917 -4.2432981 -4.2811871 -4.2624197 -4.2011518 -4.1306448][-4.2043653 -4.2342834 -4.228126 -4.172297 -4.0886831 -4.004807 -3.9604359 -4.00575 -4.1079969 -4.1982827 -4.2547674 -4.2758293 -4.2487278 -4.1875772 -4.1272497][-4.1957984 -4.2261171 -4.232183 -4.2061234 -4.1585169 -4.1101775 -4.0864234 -4.1171365 -4.1822786 -4.236938 -4.2700863 -4.2756333 -4.2429409 -4.1879091 -4.1441913][-4.2027431 -4.2248316 -4.2315674 -4.2223654 -4.1982484 -4.1688061 -4.155386 -4.1787944 -4.2235727 -4.2608113 -4.2812724 -4.2794375 -4.2483082 -4.2034369 -4.177783][-4.2275743 -4.2419162 -4.246727 -4.2479124 -4.2408152 -4.2225375 -4.2126894 -4.2280259 -4.257246 -4.2817812 -4.2946715 -4.290545 -4.26541 -4.2322206 -4.219346][-4.2547026 -4.26513 -4.270237 -4.2768641 -4.2804527 -4.2728233 -4.2658234 -4.2731662 -4.2903652 -4.3048773 -4.3104353 -4.3047447 -4.2872462 -4.2664919 -4.2636232]]...]
INFO - root - 2017-12-05 18:25:35.147261: step 30210, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 76h:07m:37s remains)
INFO - root - 2017-12-05 18:25:44.394586: step 30220, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 76h:45m:55s remains)
INFO - root - 2017-12-05 18:25:53.398301: step 30230, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 74h:02m:57s remains)
INFO - root - 2017-12-05 18:26:02.624346: step 30240, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 76h:28m:04s remains)
INFO - root - 2017-12-05 18:26:11.670389: step 30250, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 76h:22m:53s remains)
INFO - root - 2017-12-05 18:26:20.718866: step 30260, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 73h:28m:46s remains)
INFO - root - 2017-12-05 18:26:29.884487: step 30270, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 78h:34m:09s remains)
INFO - root - 2017-12-05 18:26:38.979231: step 30280, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 74h:58m:16s remains)
INFO - root - 2017-12-05 18:26:48.084409: step 30290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 75h:10m:14s remains)
INFO - root - 2017-12-05 18:26:57.222378: step 30300, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 80h:45m:29s remains)
2017-12-05 18:26:58.007361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2326679 -4.2163982 -4.2180395 -4.2183819 -4.2153111 -4.2257204 -4.2314472 -4.2352748 -4.2251949 -4.2102861 -4.2041087 -4.1961474 -4.17581 -4.1477175 -4.1275625][-4.2124643 -4.1947427 -4.1945624 -4.1900821 -4.1865625 -4.2031465 -4.2146916 -4.2245655 -4.2190828 -4.2013988 -4.1914983 -4.183372 -4.1680632 -4.1465135 -4.1319747][-4.18925 -4.16975 -4.1683292 -4.160645 -4.155798 -4.1753969 -4.1909232 -4.2058225 -4.2106538 -4.1980829 -4.1895108 -4.1815252 -4.1709762 -4.1560845 -4.1426563][-4.1852345 -4.1635318 -4.1566515 -4.1422939 -4.1302881 -4.1449842 -4.1567 -4.1726308 -4.1871471 -4.1836395 -4.180346 -4.1766157 -4.1728468 -4.1658964 -4.1525636][-4.1806426 -4.1603575 -4.1521759 -4.13281 -4.1136589 -4.1190453 -4.1165257 -4.1215429 -4.142231 -4.1512628 -4.158639 -4.1659789 -4.1695933 -4.1672893 -4.152997][-4.1794047 -4.1578183 -4.14575 -4.1257596 -4.1092463 -4.1071305 -4.0819516 -4.0571256 -4.074955 -4.1022477 -4.13016 -4.1527748 -4.1639652 -4.1610079 -4.1462135][-4.1812239 -4.1606641 -4.1429043 -4.1179767 -4.0982361 -4.082212 -4.0247374 -3.9595985 -3.9656649 -4.0180168 -4.0798793 -4.128037 -4.1569281 -4.1643615 -4.153132][-4.1747632 -4.164782 -4.1512294 -4.1277452 -4.1032252 -4.0730181 -3.992619 -3.8948381 -3.8782225 -3.9352593 -4.0170336 -4.08635 -4.1370807 -4.1635842 -4.1635747][-4.1786246 -4.1788583 -4.1756783 -4.1637044 -4.1521354 -4.1347833 -4.074501 -3.9967878 -3.9701846 -3.9905388 -4.03526 -4.0791564 -4.123517 -4.1535273 -4.162303][-4.1995821 -4.202641 -4.2021112 -4.193131 -4.1868639 -4.181818 -4.1451626 -4.0959334 -4.0745387 -4.0775924 -4.0932665 -4.1083403 -4.1319385 -4.1545377 -4.1620164][-4.219902 -4.22112 -4.2200818 -4.2090559 -4.1974053 -4.1946225 -4.1735177 -4.1425314 -4.1271415 -4.1255488 -4.1322665 -4.1355391 -4.1420984 -4.1530266 -4.1549544][-4.2373171 -4.2368016 -4.2335658 -4.2215943 -4.2034559 -4.1969504 -4.1849403 -4.1649203 -4.1503887 -4.1488 -4.1564369 -4.1604543 -4.1599174 -4.16161 -4.1570897][-4.2510595 -4.2486587 -4.2438021 -4.2359633 -4.2186866 -4.2100706 -4.2031579 -4.18716 -4.169363 -4.1636677 -4.1703506 -4.1795669 -4.1819282 -4.1795321 -4.1732821][-4.2579465 -4.2578411 -4.2532864 -4.2495208 -4.2382269 -4.2317328 -4.2262163 -4.2079492 -4.184226 -4.1703839 -4.1708608 -4.17692 -4.1802964 -4.1850815 -4.1922297][-4.2441421 -4.2445655 -4.2424526 -4.2430944 -4.2408695 -4.2429209 -4.2435164 -4.2266355 -4.2019129 -4.179553 -4.1689658 -4.1643529 -4.159904 -4.1650691 -4.1804409]]...]
INFO - root - 2017-12-05 18:27:07.105912: step 30310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 72h:59m:10s remains)
INFO - root - 2017-12-05 18:27:16.154710: step 30320, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 76h:21m:00s remains)
INFO - root - 2017-12-05 18:27:25.248115: step 30330, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 79h:10m:55s remains)
INFO - root - 2017-12-05 18:27:34.469214: step 30340, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 78h:25m:39s remains)
INFO - root - 2017-12-05 18:27:43.667767: step 30350, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 76h:46m:36s remains)
INFO - root - 2017-12-05 18:27:52.685894: step 30360, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 75h:04m:22s remains)
INFO - root - 2017-12-05 18:28:01.717250: step 30370, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 75h:35m:52s remains)
INFO - root - 2017-12-05 18:28:10.856244: step 30380, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 76h:08m:38s remains)
INFO - root - 2017-12-05 18:28:19.824978: step 30390, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 77h:42m:53s remains)
INFO - root - 2017-12-05 18:28:28.970655: step 30400, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 78h:31m:09s remains)
2017-12-05 18:28:29.710808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2170095 -4.2283573 -4.236095 -4.2389083 -4.2391496 -4.24001 -4.240941 -4.2408156 -4.2397108 -4.2383804 -4.2373152 -4.2363081 -4.2340255 -4.2310624 -4.2254539][-4.2367973 -4.2432089 -4.2467942 -4.2468643 -4.2454796 -4.2450747 -4.2458673 -4.2462449 -4.2456985 -4.2442317 -4.2425718 -4.2410722 -4.2383471 -4.2348828 -4.2283497][-4.2636843 -4.2656775 -4.2648296 -4.2610826 -4.2573323 -4.2561111 -4.2581329 -4.2617178 -4.26472 -4.2652588 -4.2642565 -4.2626543 -4.2603526 -4.2573767 -4.2507138][-4.2762995 -4.2764716 -4.2719941 -4.2625728 -4.2537484 -4.2499752 -4.2531395 -4.2616725 -4.2721405 -4.2791643 -4.2823343 -4.2825913 -4.2815418 -4.2796278 -4.2739468][-4.2616844 -4.26182 -4.2543664 -4.2383709 -4.22168 -4.2125845 -4.2161632 -4.2307611 -4.2512817 -4.2683649 -4.27919 -4.2824335 -4.2823095 -4.2814431 -4.2773643][-4.2271361 -4.2277441 -4.2184463 -4.1972365 -4.1745362 -4.1594968 -4.15748 -4.171483 -4.2007246 -4.2302051 -4.25014 -4.257174 -4.2575159 -4.258491 -4.2576756][-4.2036824 -4.2021341 -4.1905184 -4.1671243 -4.1395979 -4.1140413 -4.0966926 -4.1009369 -4.1337528 -4.1728592 -4.1999006 -4.2107587 -4.2131915 -4.218142 -4.2233009][-4.2104721 -4.2060785 -4.1914926 -4.1666431 -4.1338859 -4.0940857 -4.0570354 -4.0469308 -4.0772276 -4.1181107 -4.1445913 -4.1548414 -4.1582246 -4.1680675 -4.1815562][-4.2359514 -4.2283983 -4.2119403 -4.1884575 -4.1566463 -4.1136627 -4.0690317 -4.0484438 -4.0671287 -4.096776 -4.113699 -4.1171 -4.1168613 -4.1273413 -4.146605][-4.2664537 -4.2592058 -4.2439914 -4.2248483 -4.2005315 -4.1655817 -4.1269088 -4.1055789 -4.1121364 -4.1259313 -4.130374 -4.1263523 -4.1225891 -4.1321421 -4.1521964][-4.2937655 -4.2897658 -4.2768183 -4.2609272 -4.2422915 -4.215107 -4.1849275 -4.1687193 -4.169723 -4.1740355 -4.1725965 -4.1663575 -4.1622992 -4.1715717 -4.1895738][-4.3051682 -4.3075585 -4.3001966 -4.2897158 -4.2769871 -4.2581081 -4.2368593 -4.2243814 -4.2214131 -4.220675 -4.2177906 -4.2113805 -4.2069287 -4.214107 -4.2281947][-4.2948251 -4.3037252 -4.3033719 -4.2994585 -4.2933621 -4.2819996 -4.2668467 -4.2562065 -4.2514052 -4.2498655 -4.2470031 -4.2397475 -4.2340264 -4.2380981 -4.2484608][-4.2747564 -4.2882929 -4.29258 -4.2927094 -4.2904634 -4.28245 -4.2685418 -4.2568588 -4.2515688 -4.2513785 -4.2489367 -4.24084 -4.2342744 -4.2366891 -4.2465987][-4.2590394 -4.2733078 -4.2789464 -4.27948 -4.2769718 -4.2672167 -4.2504797 -4.2368646 -4.2333765 -4.2363696 -4.2364817 -4.2312932 -4.2266254 -4.2292747 -4.239574]]...]
INFO - root - 2017-12-05 18:28:39.054359: step 30410, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 77h:23m:54s remains)
INFO - root - 2017-12-05 18:28:48.126188: step 30420, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 77h:06m:44s remains)
INFO - root - 2017-12-05 18:28:57.166280: step 30430, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 74h:55m:12s remains)
INFO - root - 2017-12-05 18:29:06.316492: step 30440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 78h:44m:19s remains)
INFO - root - 2017-12-05 18:29:15.351971: step 30450, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 73h:19m:05s remains)
INFO - root - 2017-12-05 18:29:24.601197: step 30460, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 76h:33m:39s remains)
INFO - root - 2017-12-05 18:29:33.589766: step 30470, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 78h:01m:19s remains)
INFO - root - 2017-12-05 18:29:42.773174: step 30480, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 76h:15m:45s remains)
INFO - root - 2017-12-05 18:29:51.639380: step 30490, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 75h:10m:28s remains)
INFO - root - 2017-12-05 18:30:00.796465: step 30500, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 76h:54m:56s remains)
2017-12-05 18:30:01.624094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.205687 -4.2078362 -4.2098222 -4.2119446 -4.2135277 -4.2150159 -4.2161226 -4.2161713 -4.21535 -4.2133822 -4.2112451 -4.2092361 -4.2048984 -4.1964893 -4.1863866][-4.2047844 -4.2058811 -4.207129 -4.2091155 -4.2106872 -4.2126875 -4.2149277 -4.2161393 -4.2157035 -4.2127805 -4.209341 -4.2059994 -4.2000842 -4.18837 -4.173285][-4.2139587 -4.2141018 -4.2137432 -4.2133594 -4.2118521 -4.2112961 -4.2137241 -4.2168026 -4.21779 -4.2160482 -4.2142029 -4.2123322 -4.2074661 -4.1959085 -4.1795492][-4.2264514 -4.226531 -4.2250948 -4.2203345 -4.2115192 -4.203877 -4.2030444 -4.2064061 -4.2098923 -4.2132177 -4.2173772 -4.2206535 -4.2204666 -4.2136836 -4.2008276][-4.2409682 -4.2401748 -4.2367148 -4.2269506 -4.2089291 -4.1886892 -4.1763387 -4.1768079 -4.1861935 -4.1987906 -4.212523 -4.2238011 -4.2311964 -4.2320213 -4.225832][-4.2429624 -4.2389197 -4.2326479 -4.2186842 -4.1909022 -4.151988 -4.1189051 -4.1124611 -4.131804 -4.16003 -4.18848 -4.2107825 -4.2265668 -4.2357616 -4.2381268][-4.2328324 -4.2212691 -4.2074661 -4.1867309 -4.1482248 -4.088408 -4.0303698 -4.0168037 -4.0519867 -4.102211 -4.1481113 -4.1813722 -4.2052045 -4.2228808 -4.2346215][-4.2088685 -4.1824536 -4.1546206 -4.1238165 -4.0757837 -4.00114 -3.9211662 -3.9054995 -3.9612229 -4.0323772 -4.0871811 -4.1259031 -4.1599097 -4.1913462 -4.2162457][-4.1770167 -4.1283073 -4.0819993 -4.0449786 -4.0003581 -3.9263875 -3.8365209 -3.820406 -3.8914924 -3.9701037 -4.023838 -4.0663862 -4.1139164 -4.1613431 -4.1985373][-4.1739211 -4.1181931 -4.07042 -4.0409117 -4.0117855 -3.9565754 -3.8841376 -3.8688655 -3.9254327 -3.9880669 -4.0326552 -4.0727668 -4.1200194 -4.1666927 -4.20035][-4.2021332 -4.1598759 -4.12619 -4.1088371 -4.0960212 -4.0681133 -4.0285535 -4.0174427 -4.0441861 -4.0775528 -4.1046109 -4.1333365 -4.1658254 -4.1956849 -4.2136416][-4.232585 -4.2080684 -4.1884718 -4.1811056 -4.1822248 -4.175364 -4.1591296 -4.1527333 -4.1617756 -4.1759424 -4.1869922 -4.2007318 -4.2156382 -4.2269034 -4.2287974][-4.2514825 -4.242178 -4.233861 -4.2336493 -4.2421594 -4.2475839 -4.2451429 -4.2426667 -4.243432 -4.2466812 -4.2482562 -4.2508507 -4.2525535 -4.2502074 -4.2411084][-4.2541223 -4.2542372 -4.2540174 -4.2572937 -4.2672276 -4.2773681 -4.2811084 -4.2806983 -4.2784886 -4.276382 -4.2729983 -4.2694974 -4.2640071 -4.2535577 -4.2384915][-4.2484508 -4.2529631 -4.25614 -4.2594056 -4.2656446 -4.2722936 -4.2755122 -4.27518 -4.2731705 -4.2708621 -4.2672009 -4.2621851 -4.2528358 -4.2376804 -4.2193594]]...]
INFO - root - 2017-12-05 18:30:10.765120: step 30510, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 76h:28m:16s remains)
INFO - root - 2017-12-05 18:30:19.705179: step 30520, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 75h:06m:27s remains)
INFO - root - 2017-12-05 18:30:28.648253: step 30530, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 78h:24m:22s remains)
INFO - root - 2017-12-05 18:30:37.744135: step 30540, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 79h:32m:14s remains)
INFO - root - 2017-12-05 18:30:46.791047: step 30550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 77h:51m:20s remains)
INFO - root - 2017-12-05 18:30:55.771873: step 30560, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 73h:32m:50s remains)
INFO - root - 2017-12-05 18:31:04.847474: step 30570, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 76h:34m:29s remains)
INFO - root - 2017-12-05 18:31:13.842003: step 30580, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 74h:31m:25s remains)
INFO - root - 2017-12-05 18:31:23.026278: step 30590, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.938 sec/batch; 78h:39m:18s remains)
INFO - root - 2017-12-05 18:31:32.062489: step 30600, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 73h:22m:13s remains)
2017-12-05 18:31:32.814047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26186 -4.2650352 -4.2668886 -4.2698345 -4.2735119 -4.2769589 -4.2810869 -4.2873135 -4.2929673 -4.295269 -4.2927771 -4.291059 -4.2924733 -4.2948875 -4.2959614][-4.2613769 -4.2612939 -4.2609086 -4.2605267 -4.2595129 -4.2599893 -4.2630358 -4.2711077 -4.281601 -4.2890854 -4.2892709 -4.2864275 -4.2848053 -4.2837439 -4.283401][-4.2668562 -4.2621522 -4.25469 -4.2456675 -4.236053 -4.2296615 -4.2279553 -4.2365155 -4.2542758 -4.2691011 -4.2737575 -4.2721572 -4.2688694 -4.2635789 -4.2611122][-4.2703862 -4.2568936 -4.2378507 -4.2170949 -4.1970091 -4.1819625 -4.1735377 -4.1829538 -4.211328 -4.2371082 -4.2477436 -4.2486982 -4.2449417 -4.2350535 -4.2286382][-4.2573953 -4.23155 -4.1961184 -4.1579423 -4.1227946 -4.0942435 -4.0767031 -4.0902615 -4.1384726 -4.1846442 -4.2076149 -4.2124376 -4.2077289 -4.1931839 -4.1809187][-4.2277184 -4.1906066 -4.1412764 -4.0855861 -4.0343142 -3.9906795 -3.9602292 -3.9748263 -4.0481324 -4.1214771 -4.1603541 -4.1710486 -4.169107 -4.1552272 -4.1410847][-4.2063618 -4.1653337 -4.1100507 -4.044487 -3.9798934 -3.9187713 -3.8619027 -3.8605683 -3.9499609 -4.0497832 -4.1110444 -4.1352072 -4.1417613 -4.1360836 -4.1267529][-4.2169585 -4.17804 -4.1194792 -4.0489984 -3.9799669 -3.9061227 -3.8192537 -3.7877595 -3.872961 -3.9870291 -4.0716581 -4.1190886 -4.1432667 -4.1531558 -4.1537151][-4.2575359 -4.2242093 -4.1713171 -4.1078939 -4.0475802 -3.9796686 -3.893 -3.847157 -3.8999791 -3.9951091 -4.0842419 -4.1467652 -4.1800361 -4.2020993 -4.2145238][-4.2982736 -4.2763205 -4.2377362 -4.1903625 -4.1440344 -4.0948687 -4.0318685 -3.9925835 -4.0139503 -4.075304 -4.14926 -4.2087364 -4.2389665 -4.2620249 -4.2788291][-4.3292003 -4.3186073 -4.2931623 -4.2586522 -4.224062 -4.1896477 -4.1474056 -4.1171088 -4.1211791 -4.1579409 -4.2116051 -4.2604828 -4.288126 -4.309072 -4.3221073][-4.3438568 -4.3426156 -4.3289304 -4.3051543 -4.2789974 -4.2550335 -4.2286525 -4.2078109 -4.2067971 -4.2265086 -4.2597404 -4.2931957 -4.3143396 -4.3280878 -4.3344555][-4.34196 -4.3468995 -4.3455262 -4.33397 -4.3169475 -4.3003545 -4.2839694 -4.2706356 -4.2692604 -4.2776871 -4.292552 -4.310257 -4.3228874 -4.3270097 -4.3241229][-4.3366838 -4.3443389 -4.3509097 -4.3486547 -4.3395023 -4.329319 -4.319695 -4.311471 -4.3081927 -4.3080692 -4.3103051 -4.3163114 -4.3213911 -4.3190579 -4.3103218][-4.3327413 -4.3397527 -4.3476925 -4.34935 -4.3456135 -4.3410311 -4.3369188 -4.3333988 -4.3284855 -4.3213682 -4.3159709 -4.31356 -4.3105054 -4.3045125 -4.2942491]]...]
INFO - root - 2017-12-05 18:31:41.851857: step 30610, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 77h:11m:45s remains)
INFO - root - 2017-12-05 18:31:50.917320: step 30620, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 76h:40m:04s remains)
INFO - root - 2017-12-05 18:32:00.080776: step 30630, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 74h:56m:08s remains)
INFO - root - 2017-12-05 18:32:09.145795: step 30640, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 77h:35m:32s remains)
INFO - root - 2017-12-05 18:32:18.415864: step 30650, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 79h:07m:40s remains)
INFO - root - 2017-12-05 18:32:27.708205: step 30660, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 80h:27m:21s remains)
INFO - root - 2017-12-05 18:32:36.856310: step 30670, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 69h:12m:49s remains)
INFO - root - 2017-12-05 18:32:45.850413: step 30680, loss = 2.02, batch loss = 1.96 (8.9 examples/sec; 0.898 sec/batch; 75h:16m:37s remains)
INFO - root - 2017-12-05 18:32:55.146276: step 30690, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 78h:07m:16s remains)
INFO - root - 2017-12-05 18:33:04.361152: step 30700, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 78h:34m:49s remains)
2017-12-05 18:33:05.158137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2975221 -4.2953234 -4.300652 -4.3025713 -4.2996554 -4.2979231 -4.2964015 -4.2897563 -4.283227 -4.2825379 -4.2869005 -4.2941089 -4.3009386 -4.3080463 -4.3130026][-4.2751064 -4.27161 -4.2768922 -4.2770319 -4.2695761 -4.2661033 -4.2664204 -4.2622929 -4.259521 -4.2630858 -4.2687669 -4.2749009 -4.2808046 -4.2877016 -4.2938552][-4.2537837 -4.2471371 -4.2492552 -4.2450252 -4.2294726 -4.2239513 -4.2296491 -4.2313981 -4.2339487 -4.2401571 -4.2440562 -4.2484727 -4.2545843 -4.2663331 -4.2785878][-4.2233529 -4.2152224 -4.21551 -4.2084103 -4.1885257 -4.1816254 -4.1915088 -4.19883 -4.2086697 -4.2204919 -4.2254171 -4.2326865 -4.2424259 -4.25858 -4.2765627][-4.1822805 -4.1778288 -4.1801939 -4.177721 -4.16232 -4.154469 -4.1545763 -4.1572075 -4.1743665 -4.1939311 -4.1993027 -4.21144 -4.2265224 -4.2478061 -4.2731061][-4.1530995 -4.1551838 -4.1619539 -4.1643009 -4.1477175 -4.1200686 -4.0874915 -4.07696 -4.1073513 -4.1380944 -4.1485138 -4.1633987 -4.1873231 -4.2235351 -4.2613831][-4.1578679 -4.1606812 -4.1622505 -4.1552129 -4.1224532 -4.0625157 -3.9906726 -3.9704652 -4.0284534 -4.0866489 -4.1124363 -4.1346693 -4.1689281 -4.2194095 -4.264493][-4.1658645 -4.1600676 -4.1508441 -4.1347508 -4.097352 -4.0306277 -3.9556012 -3.9496684 -4.0316844 -4.1019969 -4.1299806 -4.1454873 -4.1768394 -4.2263222 -4.2695537][-4.1597762 -4.1483741 -4.1412449 -4.1382794 -4.124598 -4.0931387 -4.0565982 -4.0611386 -4.1209097 -4.1665463 -4.175561 -4.171885 -4.1884589 -4.22805 -4.2677832][-4.1558237 -4.1523428 -4.1618328 -4.1719089 -4.1695642 -4.1559944 -4.1395159 -4.1444492 -4.1813464 -4.2046461 -4.2007856 -4.1881127 -4.1973877 -4.22864 -4.2634892][-4.1592174 -4.160964 -4.1763644 -4.1896219 -4.1877074 -4.1783662 -4.1681786 -4.1721358 -4.1977582 -4.2163029 -4.2162795 -4.2069292 -4.2134585 -4.2382774 -4.2665257][-4.1772914 -4.1763334 -4.1902914 -4.2039328 -4.2037067 -4.1978331 -4.1922817 -4.1982055 -4.219645 -4.2380853 -4.2441611 -4.2386422 -4.2390723 -4.2533236 -4.272562][-4.2054543 -4.1994309 -4.2083373 -4.2220316 -4.2263684 -4.2281923 -4.2275915 -4.2327409 -4.2476411 -4.2612023 -4.2687263 -4.265069 -4.2606816 -4.2658153 -4.2779412][-4.2428331 -4.2359381 -4.2428575 -4.255383 -4.2636237 -4.2697945 -4.2701106 -4.2708216 -4.2766209 -4.2825561 -4.2871213 -4.282938 -4.2777905 -4.2810497 -4.2916145][-4.2796283 -4.2753568 -4.2828417 -4.2929821 -4.2995453 -4.3038759 -4.3042908 -4.3045235 -4.3065724 -4.3091764 -4.311285 -4.3058109 -4.2997985 -4.3012023 -4.3107615]]...]
INFO - root - 2017-12-05 18:33:14.194661: step 30710, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 74h:45m:29s remains)
INFO - root - 2017-12-05 18:33:23.336319: step 30720, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 77h:51m:17s remains)
INFO - root - 2017-12-05 18:33:32.361574: step 30730, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 75h:44m:59s remains)
INFO - root - 2017-12-05 18:33:41.638201: step 30740, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.927 sec/batch; 77h:44m:40s remains)
INFO - root - 2017-12-05 18:33:50.679936: step 30750, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 75h:59m:54s remains)
INFO - root - 2017-12-05 18:33:59.932056: step 30760, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.931 sec/batch; 78h:01m:13s remains)
INFO - root - 2017-12-05 18:34:09.176304: step 30770, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 78h:32m:07s remains)
INFO - root - 2017-12-05 18:34:18.393859: step 30780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 77h:31m:14s remains)
INFO - root - 2017-12-05 18:34:27.408797: step 30790, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.910 sec/batch; 76h:14m:31s remains)
INFO - root - 2017-12-05 18:34:36.725106: step 30800, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 77h:23m:03s remains)
2017-12-05 18:34:37.584505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2874432 -4.2650962 -4.25099 -4.2474065 -4.2502241 -4.2482262 -4.2454205 -4.2396307 -4.2296786 -4.225626 -4.2290921 -4.2323766 -4.2273793 -4.2201533 -4.2262874][-4.2731938 -4.2452345 -4.2288651 -4.2246857 -4.2232528 -4.2126241 -4.2047739 -4.1934266 -4.1755414 -4.1672392 -4.1768913 -4.1938162 -4.1921988 -4.1776638 -4.1764994][-4.2602906 -4.2296052 -4.2112417 -4.2042975 -4.1967244 -4.1776867 -4.1645713 -4.1478128 -4.1236148 -4.1114345 -4.1273308 -4.1580219 -4.1614189 -4.1387224 -4.1274848][-4.2539573 -4.2233157 -4.2009387 -4.1870065 -4.1723251 -4.1454897 -4.1277504 -4.1065669 -4.0778179 -4.0667624 -4.0881424 -4.131206 -4.1421285 -4.1167006 -4.0953608][-4.2506766 -4.2185225 -4.191843 -4.1704884 -4.14881 -4.1159163 -4.0874286 -4.0485258 -4.0096445 -4.0044732 -4.0428333 -4.0982418 -4.11846 -4.0967927 -4.0713725][-4.2506781 -4.21799 -4.1896486 -4.1657829 -4.1394882 -4.0999084 -4.0520868 -3.9841285 -3.9188511 -3.9205034 -3.9850831 -4.0533185 -4.0842786 -4.0690823 -4.0459452][-4.2535715 -4.2239738 -4.1914873 -4.1632447 -4.1355262 -4.0935092 -4.0355992 -3.9478106 -3.8577523 -3.8667653 -3.9476836 -4.0238948 -4.0651336 -4.0617838 -4.0424967][-4.2444434 -4.2128754 -4.1737366 -4.1429553 -4.1171651 -4.081079 -4.0362997 -3.969815 -3.9030504 -3.9170656 -3.9823284 -4.0414681 -4.0781646 -4.0839968 -4.069521][-4.2368078 -4.2026162 -4.1608472 -4.126173 -4.1024957 -4.0769048 -4.0530033 -4.0214734 -3.9952703 -4.014637 -4.0500431 -4.0789862 -4.100369 -4.1084037 -4.0995479][-4.2439051 -4.2145534 -4.1782522 -4.1423268 -4.1154552 -4.0920105 -4.075141 -4.060204 -4.0502567 -4.06855 -4.0847836 -4.0969591 -4.10806 -4.1148214 -4.115468][-4.2609987 -4.2395597 -4.2139926 -4.18662 -4.1570654 -4.1320157 -4.1183753 -4.1084332 -4.0991383 -4.1101966 -4.1193495 -4.1220551 -4.1221275 -4.125948 -4.12842][-4.2821417 -4.2681575 -4.2536387 -4.2338576 -4.2058392 -4.1807957 -4.1687527 -4.1602063 -4.1499143 -4.1577191 -4.1662483 -4.1689739 -4.1706839 -4.1757932 -4.1778655][-4.2942371 -4.2854185 -4.2780018 -4.266706 -4.2479787 -4.2312274 -4.2241445 -4.2184386 -4.2128105 -4.2214608 -4.2302461 -4.2361846 -4.241641 -4.2474837 -4.2465591][-4.3039007 -4.2975831 -4.2947884 -4.289341 -4.2785635 -4.2710729 -4.2722831 -4.2709327 -4.2703295 -4.2763119 -4.2838445 -4.2911744 -4.2981796 -4.3020191 -4.2998304][-4.3173504 -4.3130374 -4.313067 -4.3092833 -4.3028927 -4.301652 -4.3068833 -4.3091626 -4.3084478 -4.3098273 -4.3131452 -4.3185387 -4.3244419 -4.3274155 -4.3249745]]...]
INFO - root - 2017-12-05 18:34:46.682067: step 30810, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 76h:10m:55s remains)
INFO - root - 2017-12-05 18:34:55.837416: step 30820, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 76h:51m:40s remains)
INFO - root - 2017-12-05 18:35:04.916757: step 30830, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.921 sec/batch; 77h:08m:15s remains)
INFO - root - 2017-12-05 18:35:14.155931: step 30840, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 76h:05m:51s remains)
INFO - root - 2017-12-05 18:35:23.194196: step 30850, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 73h:27m:00s remains)
INFO - root - 2017-12-05 18:35:32.206753: step 30860, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 76h:35m:41s remains)
INFO - root - 2017-12-05 18:35:41.451946: step 30870, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 76h:50m:59s remains)
INFO - root - 2017-12-05 18:35:50.493831: step 30880, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 75h:20m:15s remains)
INFO - root - 2017-12-05 18:35:59.714545: step 30890, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 78h:33m:41s remains)
INFO - root - 2017-12-05 18:36:08.924832: step 30900, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 78h:57m:05s remains)
2017-12-05 18:36:09.734287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2163906 -4.1950216 -4.1967521 -4.2099352 -4.2137303 -4.2152505 -4.2285976 -4.234087 -4.23828 -4.2456517 -4.2480979 -4.2606359 -4.2738128 -4.266274 -4.25623][-4.2200451 -4.192307 -4.1866364 -4.1930652 -4.1905084 -4.18822 -4.1973658 -4.2018394 -4.2096539 -4.2276969 -4.2451468 -4.2680168 -4.28098 -4.2676172 -4.2522178][-4.2205582 -4.1897383 -4.1774812 -4.1784554 -4.1708159 -4.1657948 -4.16913 -4.1653004 -4.1673307 -4.1946363 -4.2288918 -4.2613039 -4.2746105 -4.2584844 -4.2427206][-4.2095542 -4.1792564 -4.1670675 -4.1665788 -4.159431 -4.1532321 -4.1467662 -4.1307006 -4.1196637 -4.1494722 -4.1954222 -4.2357249 -4.25392 -4.2410984 -4.2302694][-4.1988964 -4.1699924 -4.1601276 -4.160728 -4.1568766 -4.1489916 -4.1334052 -4.1069465 -4.0834131 -4.1086206 -4.1582394 -4.203989 -4.2247448 -4.2161078 -4.2148018][-4.2074332 -4.1826134 -4.1796536 -4.1862769 -4.1853251 -4.1714487 -4.1428161 -4.096518 -4.0556183 -4.0717106 -4.1237793 -4.172646 -4.1932564 -4.1869864 -4.1928463][-4.22735 -4.2091522 -4.2118626 -4.2177176 -4.2118182 -4.1924515 -4.1522479 -4.0938005 -4.0454254 -4.0544972 -4.1079016 -4.1569839 -4.1690826 -4.1572356 -4.164289][-4.2541828 -4.2379031 -4.2386904 -4.2412729 -4.2306919 -4.2072229 -4.1630855 -4.1095371 -4.070106 -4.0777526 -4.1231833 -4.1659679 -4.1690717 -4.144907 -4.1424465][-4.2786593 -4.2594953 -4.2540035 -4.2544217 -4.2451143 -4.2249608 -4.1847363 -4.1377373 -4.1072536 -4.1107335 -4.143858 -4.1784096 -4.1810656 -4.14999 -4.1379604][-4.2914929 -4.2695823 -4.258605 -4.2564011 -4.2486081 -4.2348733 -4.2024279 -4.162158 -4.1339011 -4.1268744 -4.1506505 -4.1774731 -4.18317 -4.1590223 -4.1477556][-4.2998376 -4.2779212 -4.2640357 -4.2614098 -4.2556448 -4.2445188 -4.217176 -4.1799603 -4.1525221 -4.1350574 -4.15306 -4.1764784 -4.1880965 -4.1753669 -4.1678209][-4.3094749 -4.2910252 -4.2774372 -4.2732263 -4.2687497 -4.2577205 -4.2324743 -4.2001867 -4.1734328 -4.1504812 -4.1619205 -4.1845098 -4.2026749 -4.1989532 -4.1941557][-4.3177361 -4.3041339 -4.2920194 -4.2856059 -4.2805619 -4.2717566 -4.2524819 -4.2273703 -4.2041783 -4.1844735 -4.1900368 -4.20672 -4.2208529 -4.2174683 -4.2124186][-4.3216615 -4.3140664 -4.3057766 -4.2990365 -4.2939396 -4.2878041 -4.2767816 -4.2613654 -4.2471728 -4.2348981 -4.2366967 -4.2459254 -4.2472892 -4.2394786 -4.2327194][-4.3231091 -4.3201022 -4.3145876 -4.3091745 -4.3067427 -4.3042564 -4.2990217 -4.2915068 -4.2853866 -4.2800961 -4.2808175 -4.28418 -4.27809 -4.2668333 -4.2618527]]...]
INFO - root - 2017-12-05 18:36:18.796644: step 30910, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 75h:32m:25s remains)
INFO - root - 2017-12-05 18:36:27.897494: step 30920, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 79h:25m:31s remains)
INFO - root - 2017-12-05 18:36:37.069460: step 30930, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 75h:24m:44s remains)
INFO - root - 2017-12-05 18:36:46.234752: step 30940, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 74h:39m:45s remains)
INFO - root - 2017-12-05 18:36:55.477231: step 30950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:52m:47s remains)
INFO - root - 2017-12-05 18:37:04.609418: step 30960, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 76h:59m:40s remains)
INFO - root - 2017-12-05 18:37:13.678262: step 30970, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 79h:05m:56s remains)
INFO - root - 2017-12-05 18:37:22.726708: step 30980, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 77h:12m:59s remains)
INFO - root - 2017-12-05 18:37:31.925428: step 30990, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.931 sec/batch; 77h:57m:18s remains)
INFO - root - 2017-12-05 18:37:40.952065: step 31000, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 74h:27m:05s remains)
2017-12-05 18:37:41.697817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3243027 -4.327004 -4.326261 -4.3256497 -4.3249383 -4.3253064 -4.3272605 -4.3289323 -4.3288412 -4.3263283 -4.3232231 -4.3207321 -4.3191209 -4.3171716 -4.3146825][-4.3330307 -4.3335814 -4.3316045 -4.3290043 -4.3264742 -4.327107 -4.3307762 -4.3353286 -4.3353205 -4.3306055 -4.3244386 -4.3189363 -4.3166542 -4.314908 -4.3123713][-4.32282 -4.3206344 -4.3172908 -4.3111563 -4.30556 -4.3032064 -4.3063421 -4.3139091 -4.3149586 -4.3093719 -4.3008375 -4.29484 -4.2967386 -4.3012214 -4.3029013][-4.290318 -4.2894187 -4.2859626 -4.2753673 -4.2623277 -4.24931 -4.2439575 -4.2515364 -4.2588067 -4.2571359 -4.2501593 -4.2473335 -4.2580037 -4.2746706 -4.2858987][-4.2415051 -4.2512927 -4.24858 -4.2296619 -4.2004504 -4.1636562 -4.1369739 -4.1434627 -4.1669006 -4.1800365 -4.1821084 -4.1883664 -4.2127743 -4.243979 -4.2666674][-4.183075 -4.2118235 -4.2110362 -4.180934 -4.1264625 -4.0491934 -3.9851284 -3.9937434 -4.0511155 -4.0940371 -4.1154585 -4.1416936 -4.1840687 -4.2270393 -4.2566404][-4.1204572 -4.1719675 -4.1793904 -4.1409926 -4.0584702 -3.9326591 -3.8256874 -3.8441529 -3.9482536 -4.0317245 -4.0828156 -4.1358175 -4.1915278 -4.2341642 -4.2601161][-4.0864344 -4.1527796 -4.1708741 -4.1349497 -4.045692 -3.9078343 -3.7915497 -3.8154917 -3.9343948 -4.0386648 -4.1095209 -4.1764469 -4.2307377 -4.2624865 -4.2770839][-4.1151104 -4.1790276 -4.1995306 -4.172946 -4.098865 -3.993139 -3.912333 -3.9308143 -4.0199771 -4.1043572 -4.1686263 -4.2287416 -4.269599 -4.288064 -4.2919345][-4.1766019 -4.2268658 -4.2447138 -4.2271433 -4.1726656 -4.1065288 -4.0660572 -4.0796609 -4.1306944 -4.1826115 -4.227159 -4.2682881 -4.291872 -4.2987881 -4.2957592][-4.2354341 -4.2705832 -4.2834282 -4.2725606 -4.2359867 -4.1985788 -4.1839628 -4.1934743 -4.216341 -4.2407851 -4.2641463 -4.287344 -4.2984152 -4.2988768 -4.293879][-4.2745667 -4.29697 -4.3038011 -4.2950811 -4.2724028 -4.2547894 -4.2535162 -4.2593918 -4.2660894 -4.2749863 -4.2855372 -4.2975364 -4.3016462 -4.29886 -4.2934165][-4.2982025 -4.3090725 -4.3094463 -4.3019829 -4.289844 -4.2836261 -4.2867765 -4.2904234 -4.291852 -4.2945852 -4.29874 -4.3031912 -4.3024631 -4.2979383 -4.2925162][-4.3100324 -4.3128223 -4.3094139 -4.3032393 -4.2973065 -4.2959633 -4.298944 -4.3003645 -4.2993474 -4.2993441 -4.300138 -4.3005624 -4.2979708 -4.2937322 -4.2890382][-4.3093457 -4.3075719 -4.3027534 -4.2981296 -4.2951908 -4.2951889 -4.2968035 -4.2964582 -4.2946186 -4.2937231 -4.2932606 -4.292665 -4.2903433 -4.2876663 -4.2846222]]...]
INFO - root - 2017-12-05 18:37:50.723306: step 31010, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.788 sec/batch; 66h:01m:20s remains)
INFO - root - 2017-12-05 18:37:59.934777: step 31020, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 77h:17m:04s remains)
INFO - root - 2017-12-05 18:38:09.055470: step 31030, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 72h:48m:38s remains)
INFO - root - 2017-12-05 18:38:17.933982: step 31040, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.894 sec/batch; 74h:52m:09s remains)
INFO - root - 2017-12-05 18:38:26.840067: step 31050, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 74h:39m:13s remains)
INFO - root - 2017-12-05 18:38:36.041615: step 31060, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 78h:21m:20s remains)
INFO - root - 2017-12-05 18:38:45.225485: step 31070, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 76h:55m:27s remains)
INFO - root - 2017-12-05 18:38:54.250254: step 31080, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 77h:44m:03s remains)
INFO - root - 2017-12-05 18:39:03.260390: step 31090, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 75h:22m:12s remains)
INFO - root - 2017-12-05 18:39:12.499380: step 31100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 76h:29m:56s remains)
2017-12-05 18:39:13.297354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2068915 -4.1841006 -4.1875968 -4.2006059 -4.1951404 -4.1830168 -4.1874294 -4.209578 -4.2319994 -4.2404513 -4.2374015 -4.2340517 -4.2324281 -4.2240181 -4.205442][-4.1992745 -4.1711144 -4.1726546 -4.1936908 -4.20012 -4.1912465 -4.1970606 -4.2210236 -4.2422881 -4.2434192 -4.2316942 -4.2230692 -4.2221 -4.2145252 -4.1999364][-4.1993413 -4.1722097 -4.179523 -4.2049866 -4.2166605 -4.2086372 -4.207068 -4.2230334 -4.23845 -4.240202 -4.2303667 -4.2185731 -4.2152672 -4.2087808 -4.1988673][-4.1985784 -4.1737089 -4.1867142 -4.2108607 -4.2212696 -4.2064314 -4.1906567 -4.1941924 -4.2064991 -4.2172651 -4.2215576 -4.2203784 -4.221684 -4.2160225 -4.2112393][-4.2058473 -4.1782084 -4.1874542 -4.200932 -4.2044282 -4.1714187 -4.1305776 -4.1197357 -4.126368 -4.1482983 -4.186471 -4.2170568 -4.2335458 -4.23209 -4.2297807][-4.2142072 -4.182261 -4.1823239 -4.1840382 -4.1726441 -4.117322 -4.0516019 -4.0211015 -4.0205274 -4.0622873 -4.1462474 -4.2136984 -4.2493095 -4.256516 -4.2517428][-4.2198386 -4.1876669 -4.1806273 -4.1759086 -4.1541858 -4.0883355 -4.0071878 -3.9535625 -3.9419196 -4.0035381 -4.12247 -4.2155786 -4.265574 -4.2813292 -4.2730474][-4.2159958 -4.1825371 -4.1761184 -4.1756196 -4.1556172 -4.0914626 -4.0090871 -3.9471474 -3.9346962 -4.0065837 -4.1327438 -4.2308264 -4.2838116 -4.2986312 -4.2834234][-4.2010369 -4.167027 -4.1661272 -4.1755662 -4.1645141 -4.1142712 -4.0515313 -4.0038686 -4.0038013 -4.0729365 -4.1810389 -4.2648592 -4.3084254 -4.3139338 -4.287034][-4.1792188 -4.148499 -4.1491656 -4.1630054 -4.1638589 -4.1412039 -4.11322 -4.0962 -4.109261 -4.1701007 -4.2522225 -4.3062429 -4.322504 -4.3116508 -4.275631][-4.1681719 -4.1370921 -4.1365967 -4.1546893 -4.17011 -4.1715469 -4.1672277 -4.1739235 -4.1981215 -4.2517357 -4.3046265 -4.3247037 -4.3168173 -4.2967339 -4.2599864][-4.17992 -4.1462984 -4.1368179 -4.1511083 -4.1769066 -4.1982703 -4.2161088 -4.2386093 -4.2643957 -4.3032365 -4.3298907 -4.326467 -4.3079534 -4.2879562 -4.2564359][-4.199614 -4.16007 -4.1407576 -4.1536188 -4.188108 -4.2246785 -4.2564607 -4.2855854 -4.3067656 -4.3250384 -4.329154 -4.3150058 -4.30021 -4.2866797 -4.2615638][-4.1994185 -4.1600766 -4.145946 -4.1697311 -4.2123575 -4.2556353 -4.2876239 -4.3114038 -4.3224239 -4.3217993 -4.3106589 -4.2955613 -4.2878566 -4.2819571 -4.2631526][-4.2005424 -4.1722827 -4.1700435 -4.20373 -4.2465415 -4.286171 -4.3105979 -4.3226171 -4.3247342 -4.3153596 -4.3007784 -4.2873793 -4.2802558 -4.2738414 -4.2538576]]...]
INFO - root - 2017-12-05 18:39:22.328775: step 31110, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 77h:19m:12s remains)
INFO - root - 2017-12-05 18:39:31.268455: step 31120, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 76h:53m:22s remains)
INFO - root - 2017-12-05 18:39:40.327024: step 31130, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 77h:41m:15s remains)
INFO - root - 2017-12-05 18:39:49.228261: step 31140, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 75h:08m:53s remains)
INFO - root - 2017-12-05 18:39:58.305356: step 31150, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 74h:20m:13s remains)
INFO - root - 2017-12-05 18:40:07.293718: step 31160, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 76h:07m:19s remains)
INFO - root - 2017-12-05 18:40:16.312606: step 31170, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 75h:21m:46s remains)
INFO - root - 2017-12-05 18:40:25.549622: step 31180, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 76h:57m:09s remains)
INFO - root - 2017-12-05 18:40:34.684537: step 31190, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 73h:19m:10s remains)
INFO - root - 2017-12-05 18:40:43.797381: step 31200, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 78h:47m:29s remains)
2017-12-05 18:40:44.653595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2769818 -4.2824974 -4.2750773 -4.2651763 -4.2564931 -4.2502689 -4.2457671 -4.246314 -4.25772 -4.2747855 -4.2764478 -4.2551136 -4.22589 -4.20232 -4.181747][-4.2573233 -4.2609787 -4.2480793 -4.2306066 -4.211997 -4.1971111 -4.1887288 -4.1906877 -4.2138367 -4.2494521 -4.2640328 -4.2458711 -4.2154884 -4.1892538 -4.1631479][-4.242487 -4.2398143 -4.2202458 -4.1968961 -4.1696086 -4.1442537 -4.129499 -4.1336904 -4.1706047 -4.2275567 -4.2588334 -4.2465916 -4.217432 -4.1916466 -4.1604195][-4.2416868 -4.2358246 -4.2123632 -4.1842146 -4.1494808 -4.1119938 -4.0842767 -4.0820384 -4.1274061 -4.2075758 -4.2636385 -4.2637267 -4.2382984 -4.2087016 -4.1724887][-4.2506523 -4.2460461 -4.2232432 -4.1933966 -4.1481705 -4.0877886 -4.0310206 -4.0068483 -4.0590396 -4.1704178 -4.2601147 -4.2826395 -4.2663579 -4.2366438 -4.1965809][-4.2540045 -4.2543144 -4.2368946 -4.204267 -4.1413383 -4.0445433 -3.937506 -3.878252 -3.9463797 -4.1043038 -4.2328024 -4.2836518 -4.2850904 -4.2626586 -4.2222967][-4.2512813 -4.2604432 -4.2473006 -4.2083111 -4.1205974 -3.9828324 -3.8122153 -3.7037764 -3.7970212 -4.0136423 -4.1829691 -4.2639909 -4.2881241 -4.2767072 -4.2365422][-4.2428689 -4.2613125 -4.2520165 -4.2076273 -4.1043324 -3.9422395 -3.7233424 -3.5660505 -3.6798706 -3.9421616 -4.1419687 -4.2453961 -4.2871585 -4.2839036 -4.2412815][-4.2341886 -4.2639403 -4.2640738 -4.2240353 -4.1272588 -3.9821789 -3.7804925 -3.6272843 -3.7216072 -3.9617374 -4.1514893 -4.2527661 -4.2942734 -4.2877092 -4.2377257][-4.2190628 -4.2579613 -4.2685452 -4.2411132 -4.1706185 -4.0695243 -3.9265807 -3.8158696 -3.8748722 -4.04587 -4.189796 -4.2656302 -4.294004 -4.2778821 -4.2224188][-4.2021689 -4.2452273 -4.2590642 -4.2473569 -4.20615 -4.1454773 -4.059557 -3.9914961 -4.0277267 -4.1347623 -4.227272 -4.2721219 -4.2825856 -4.2564464 -4.1979184][-4.2055087 -4.24293 -4.254447 -4.2499609 -4.2311535 -4.1981463 -4.1500258 -4.1127653 -4.1390414 -4.2038689 -4.258862 -4.2840538 -4.2810373 -4.2485657 -4.1935287][-4.2207532 -4.247725 -4.2543545 -4.2507668 -4.2453947 -4.2337828 -4.2079134 -4.1882243 -4.2095881 -4.2505007 -4.2829766 -4.29727 -4.2890182 -4.2576041 -4.2103968][-4.2451491 -4.2645369 -4.2706933 -4.2678866 -4.2714343 -4.2737088 -4.2614145 -4.2501011 -4.2629042 -4.2874651 -4.304904 -4.3113775 -4.3025141 -4.2780962 -4.242516][-4.2723007 -4.283905 -4.2879672 -4.28813 -4.2958393 -4.3042693 -4.3023481 -4.2962551 -4.3013649 -4.3154006 -4.3228927 -4.3221931 -4.3136253 -4.2971673 -4.2758894]]...]
INFO - root - 2017-12-05 18:40:53.787223: step 31210, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 75h:45m:33s remains)
INFO - root - 2017-12-05 18:41:02.775812: step 31220, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 78h:43m:19s remains)
INFO - root - 2017-12-05 18:41:11.819281: step 31230, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 75h:36m:36s remains)
INFO - root - 2017-12-05 18:41:20.718636: step 31240, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.896 sec/batch; 74h:56m:52s remains)
INFO - root - 2017-12-05 18:41:29.792264: step 31250, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 75h:31m:23s remains)
INFO - root - 2017-12-05 18:41:38.797571: step 31260, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 76h:25m:28s remains)
INFO - root - 2017-12-05 18:41:47.920982: step 31270, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 77h:22m:29s remains)
INFO - root - 2017-12-05 18:41:57.083741: step 31280, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 76h:05m:15s remains)
INFO - root - 2017-12-05 18:42:06.269111: step 31290, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 69h:07m:19s remains)
INFO - root - 2017-12-05 18:42:15.430121: step 31300, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 75h:59m:23s remains)
2017-12-05 18:42:16.238980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0599961 -4.0770917 -4.0847683 -4.0867929 -4.0990777 -4.1164694 -4.1221981 -4.1193743 -4.1134706 -4.1047573 -4.102962 -4.1141844 -4.1222243 -4.1138372 -4.096117][-4.057982 -4.0790715 -4.0989523 -4.1112247 -4.1281905 -4.1453881 -4.1436958 -4.1310658 -4.1209812 -4.1136 -4.11404 -4.1270556 -4.1379991 -4.1330318 -4.1197953][-4.0658588 -4.0862956 -4.1097226 -4.1249208 -4.142334 -4.1586871 -4.1497245 -4.1259723 -4.1128798 -4.1106434 -4.1209235 -4.1433239 -4.1613145 -4.1636024 -4.1570454][-4.0787139 -4.09864 -4.1210971 -4.1378164 -4.1531878 -4.1639175 -4.1439333 -4.1111035 -4.0981779 -4.105453 -4.1294494 -4.1652622 -4.1910048 -4.195641 -4.1905422][-4.1118803 -4.1276641 -4.14114 -4.1548347 -4.1648555 -4.1609445 -4.1227274 -4.0807714 -4.0719733 -4.0923004 -4.1290469 -4.1754589 -4.2083926 -4.2119513 -4.1997724][-4.1494236 -4.1591468 -4.1645918 -4.172328 -4.1751142 -4.1486573 -4.0875092 -4.034318 -4.0282025 -4.0604877 -4.1110725 -4.1701345 -4.2104483 -4.2124906 -4.1917453][-4.1844864 -4.1876183 -4.1887984 -4.1909919 -4.1820793 -4.1384606 -4.064281 -4.0066104 -4.0020251 -4.0405169 -4.0959539 -4.1601653 -4.2041936 -4.2089181 -4.1890759][-4.2065558 -4.2067313 -4.206634 -4.2052402 -4.1909223 -4.1441479 -4.0781479 -4.03349 -4.0320387 -4.0647049 -4.1078806 -4.1593652 -4.1973138 -4.2049007 -4.1906581][-4.2037697 -4.2004094 -4.2014232 -4.2030869 -4.1963 -4.1635256 -4.1198325 -4.0961437 -4.0985823 -4.1190743 -4.1383562 -4.1656251 -4.1894088 -4.1904936 -4.1757789][-4.184649 -4.17517 -4.1771145 -4.1852684 -4.1886749 -4.1752992 -4.1557813 -4.1487494 -4.1514521 -4.1583238 -4.1575041 -4.1646161 -4.1746831 -4.1688762 -4.1520448][-4.1754861 -4.1591487 -4.1574931 -4.1649532 -4.1695924 -4.1658711 -4.1590891 -4.1622415 -4.16627 -4.166903 -4.158155 -4.1536531 -4.1557536 -4.1485558 -4.1329885][-4.1609955 -4.1416512 -4.1351209 -4.1404529 -4.1446671 -4.1432834 -4.1412292 -4.1476631 -4.1498556 -4.1468196 -4.1386547 -4.1319981 -4.1302609 -4.122643 -4.1096058][-4.1487556 -4.1324797 -4.1236758 -4.1280847 -4.134871 -4.137176 -4.1379271 -4.1428595 -4.1428165 -4.1407175 -4.1357903 -4.129292 -4.1211138 -4.1102118 -4.1015825][-4.1690464 -4.1583858 -4.1536961 -4.1601849 -4.1691413 -4.1722317 -4.1732183 -4.1761165 -4.1754823 -4.1744037 -4.1712461 -4.1665974 -4.1573763 -4.1460814 -4.1394153][-4.2202988 -4.2167687 -4.2169752 -4.2246432 -4.2329044 -4.2352118 -4.2338629 -4.2339182 -4.2337923 -4.2347 -4.235199 -4.2334666 -4.2261939 -4.21719 -4.2124228]]...]
INFO - root - 2017-12-05 18:42:25.296591: step 31310, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 72h:18m:35s remains)
INFO - root - 2017-12-05 18:42:34.388767: step 31320, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 75h:44m:46s remains)
INFO - root - 2017-12-05 18:42:43.568445: step 31330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 75h:43m:20s remains)
INFO - root - 2017-12-05 18:42:52.670490: step 31340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 76h:46m:01s remains)
INFO - root - 2017-12-05 18:43:01.742583: step 31350, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 72h:32m:55s remains)
INFO - root - 2017-12-05 18:43:10.789198: step 31360, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 74h:29m:18s remains)
INFO - root - 2017-12-05 18:43:19.820115: step 31370, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 73h:39m:06s remains)
INFO - root - 2017-12-05 18:43:28.927240: step 31380, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 74h:19m:09s remains)
INFO - root - 2017-12-05 18:43:37.891982: step 31390, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 76h:01m:48s remains)
INFO - root - 2017-12-05 18:43:46.943457: step 31400, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 73h:17m:15s remains)
2017-12-05 18:43:47.736651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3067927 -4.2879434 -4.2748122 -4.261395 -4.2480369 -4.2335348 -4.2221527 -4.2221689 -4.2215605 -4.22037 -4.2162414 -4.2118707 -4.2085638 -4.2043438 -4.1925468][-4.3007183 -4.2822967 -4.2699347 -4.2562838 -4.2432828 -4.2309618 -4.2218719 -4.2249293 -4.2274761 -4.229279 -4.2276459 -4.2246003 -4.2248688 -4.2224855 -4.2128468][-4.2988548 -4.2807083 -4.2673945 -4.2517648 -4.23877 -4.22659 -4.218874 -4.2217703 -4.2268867 -4.2300754 -4.2270064 -4.2232013 -4.2244706 -4.2209778 -4.2104864][-4.2995596 -4.2802768 -4.2638164 -4.2445593 -4.2298069 -4.2180648 -4.2117724 -4.2172132 -4.2276449 -4.2379003 -4.2382011 -4.2352839 -4.2303953 -4.2170167 -4.2004395][-4.2956 -4.2710176 -4.2488761 -4.2271175 -4.2114916 -4.1995935 -4.1897454 -4.1922135 -4.2063618 -4.225666 -4.2338967 -4.236289 -4.2278261 -4.2040787 -4.1797667][-4.28646 -4.25518 -4.2250395 -4.1981144 -4.1776953 -4.1619463 -4.1462865 -4.1414208 -4.1608119 -4.1977262 -4.2195506 -4.2304139 -4.2232957 -4.1980753 -4.174922][-4.2785916 -4.2422881 -4.2025118 -4.165988 -4.1338162 -4.1060739 -4.07999 -4.067584 -4.0932031 -4.1534433 -4.1963735 -4.2225432 -4.2220449 -4.1989927 -4.1791945][-4.26638 -4.226676 -4.1825404 -4.1388535 -4.0959654 -4.0570292 -4.0244675 -4.00848 -4.0413094 -4.1199579 -4.1764951 -4.2139673 -4.2195373 -4.1994534 -4.1771626][-4.2583003 -4.2186918 -4.17739 -4.1358566 -4.0942326 -4.0516372 -4.0239258 -4.0168729 -4.0546136 -4.1352506 -4.1855392 -4.216979 -4.2202234 -4.1941695 -4.1630621][-4.2505941 -4.2091684 -4.1745453 -4.1445146 -4.113708 -4.0781536 -4.0618439 -4.070065 -4.1121025 -4.1809068 -4.2112889 -4.222518 -4.2105122 -4.175981 -4.1379647][-4.2448158 -4.1995182 -4.1675653 -4.1456442 -4.1232557 -4.0957227 -4.0845046 -4.0972505 -4.1380839 -4.1926794 -4.2068787 -4.2018886 -4.1735363 -4.1293497 -4.089067][-4.2418838 -4.1943703 -4.1602511 -4.1383305 -4.1218762 -4.1051297 -4.0985355 -4.1065259 -4.1369781 -4.177382 -4.1836057 -4.1733251 -4.137939 -4.0929723 -4.0567503][-4.2433305 -4.1950021 -4.15992 -4.1393585 -4.1300054 -4.1229415 -4.1203032 -4.1220407 -4.1401949 -4.1665096 -4.16577 -4.1543159 -4.1214566 -4.0868073 -4.063344][-4.2520108 -4.2039509 -4.1703939 -4.1517324 -4.1459785 -4.1426506 -4.1408324 -4.137351 -4.1471744 -4.1638165 -4.1632562 -4.1555324 -4.1337757 -4.1154637 -4.1030674][-4.2654381 -4.219295 -4.1868825 -4.1682768 -4.1624246 -4.1594152 -4.158741 -4.1569834 -4.1632442 -4.1731091 -4.17086 -4.1672735 -4.1597176 -4.1577311 -4.1568623]]...]
INFO - root - 2017-12-05 18:43:56.834725: step 31410, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 77h:22m:45s remains)
INFO - root - 2017-12-05 18:44:05.877796: step 31420, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 75h:53m:04s remains)
INFO - root - 2017-12-05 18:44:14.929741: step 31430, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 74h:15m:08s remains)
INFO - root - 2017-12-05 18:44:23.975981: step 31440, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 75h:39m:51s remains)
INFO - root - 2017-12-05 18:44:32.931085: step 31450, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 77h:28m:25s remains)
INFO - root - 2017-12-05 18:44:42.043544: step 31460, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 73h:03m:19s remains)
INFO - root - 2017-12-05 18:44:51.362736: step 31470, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 76h:46m:10s remains)
INFO - root - 2017-12-05 18:45:00.460390: step 31480, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 75h:15m:06s remains)
INFO - root - 2017-12-05 18:45:09.532503: step 31490, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.973 sec/batch; 81h:20m:54s remains)
INFO - root - 2017-12-05 18:45:18.703385: step 31500, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 77h:29m:30s remains)
2017-12-05 18:45:19.566567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2010541 -4.2152267 -4.2187047 -4.2114391 -4.196075 -4.1719389 -4.1582985 -4.1766162 -4.2114983 -4.2534208 -4.2904034 -4.3076668 -4.3089857 -4.3013921 -4.2943897][-4.2433124 -4.2457957 -4.2352595 -4.2098274 -4.1770067 -4.1388574 -4.1171646 -4.1422834 -4.1946821 -4.2599015 -4.316606 -4.345386 -4.3522611 -4.3472366 -4.3404574][-4.2836127 -4.2729173 -4.2469039 -4.20596 -4.1559567 -4.0978904 -4.0594883 -4.0794249 -4.1414824 -4.227509 -4.3069263 -4.3511729 -4.3667464 -4.3673911 -4.3620582][-4.3109307 -4.2902446 -4.2556243 -4.2079597 -4.1482015 -4.07375 -4.009933 -4.0115342 -4.0735531 -4.1751018 -4.2729635 -4.3322964 -4.3576961 -4.3631773 -4.3588371][-4.3174458 -4.2913623 -4.2527127 -4.20042 -4.136888 -4.05241 -3.9676495 -3.9532142 -4.0133996 -4.1213098 -4.2290893 -4.3002872 -4.3364515 -4.346487 -4.3424215][-4.3125992 -4.2842813 -4.2437086 -4.1879153 -4.1199183 -4.0294805 -3.9364545 -3.9102139 -3.9686785 -4.0804229 -4.1927485 -4.2715125 -4.3169565 -4.3330865 -4.3309307][-4.3085833 -4.2793159 -4.2383976 -4.1804714 -4.1066046 -4.0105424 -3.9127111 -3.8757868 -3.9329188 -4.0538344 -4.1714058 -4.2561135 -4.3092432 -4.3305674 -4.3312049][-4.3027792 -4.2736206 -4.2337813 -4.1762433 -4.099771 -3.9981704 -3.8932478 -3.8448577 -3.8958063 -4.0218115 -4.1500039 -4.2478647 -4.3094954 -4.3342385 -4.3374181][-4.2933598 -4.2632384 -4.2229271 -4.167079 -4.0949416 -3.9988012 -3.8988914 -3.8480287 -3.8896451 -4.0088787 -4.139555 -4.245769 -4.3132834 -4.3392434 -4.3436604][-4.2847714 -4.25443 -4.21307 -4.1578059 -4.0895219 -4.0022836 -3.9160182 -3.8721831 -3.9074354 -4.0171051 -4.1409416 -4.2456584 -4.3141208 -4.340796 -4.3461418][-4.2847767 -4.2584891 -4.2172194 -4.1638484 -4.0991716 -4.0239305 -3.9554329 -3.9229329 -3.9536328 -4.0524931 -4.1622124 -4.2563672 -4.3182907 -4.3436084 -4.3487039][-4.2952971 -4.2740617 -4.23648 -4.1894045 -4.1346207 -4.0769253 -4.0323553 -4.0158911 -4.0409555 -4.1205978 -4.2078633 -4.2845187 -4.3328338 -4.3505445 -4.3528519][-4.3109183 -4.2969666 -4.2686558 -4.2331152 -4.1940145 -4.1557789 -4.1332717 -4.1296344 -4.149086 -4.2044449 -4.2642021 -4.3176575 -4.3484488 -4.3569183 -4.3555226][-4.3271527 -4.3207655 -4.3044653 -4.2843328 -4.2636695 -4.2443948 -4.2352586 -4.2375236 -4.2525039 -4.2848015 -4.3181729 -4.3470397 -4.3619471 -4.3622937 -4.35812][-4.3394589 -4.3375406 -4.3310537 -4.3232789 -4.3159866 -4.3111305 -4.3104653 -4.3151836 -4.3266153 -4.343996 -4.3586235 -4.3690996 -4.3711 -4.3655052 -4.3593802]]...]
INFO - root - 2017-12-05 18:45:28.746697: step 31510, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 75h:34m:45s remains)
INFO - root - 2017-12-05 18:45:37.807159: step 31520, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 77h:56m:15s remains)
INFO - root - 2017-12-05 18:45:47.069473: step 31530, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 78h:02m:09s remains)
INFO - root - 2017-12-05 18:45:56.277894: step 31540, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 76h:19m:54s remains)
INFO - root - 2017-12-05 18:46:05.437648: step 31550, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 75h:04m:16s remains)
INFO - root - 2017-12-05 18:46:14.598692: step 31560, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 76h:48m:11s remains)
INFO - root - 2017-12-05 18:46:23.663779: step 31570, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.917 sec/batch; 76h:38m:57s remains)
INFO - root - 2017-12-05 18:46:32.748417: step 31580, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 75h:43m:54s remains)
INFO - root - 2017-12-05 18:46:41.891250: step 31590, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 75h:24m:19s remains)
INFO - root - 2017-12-05 18:46:51.111215: step 31600, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 74h:43m:30s remains)
2017-12-05 18:46:51.901842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2774582 -4.2831192 -4.2873435 -4.2899885 -4.2906265 -4.2895374 -4.2911148 -4.2966857 -4.2930465 -4.2832589 -4.2666397 -4.2496943 -4.24396 -4.2427545 -4.2326078][-4.269114 -4.275341 -4.2778344 -4.2772665 -4.2773395 -4.2766638 -4.2800117 -4.2842331 -4.2720528 -4.2442503 -4.2136722 -4.1973228 -4.1983666 -4.2006149 -4.1923547][-4.2428017 -4.2466135 -4.244719 -4.2402215 -4.2362504 -4.2311339 -4.23269 -4.2320461 -4.2115498 -4.1712637 -4.1323791 -4.12389 -4.1401296 -4.1552792 -4.1548791][-4.2143016 -4.2130713 -4.2031131 -4.1902065 -4.1783528 -4.1647182 -4.1595106 -4.1532936 -4.13302 -4.097321 -4.0630903 -4.0647273 -4.0951939 -4.1229224 -4.1357384][-4.20448 -4.1961508 -4.1732931 -4.1446428 -4.1165919 -4.087472 -4.0672479 -4.0559092 -4.0525236 -4.0471725 -4.040525 -4.0595622 -4.1000533 -4.133142 -4.1508608][-4.206459 -4.1887321 -4.15621 -4.1166077 -4.0745139 -4.0251312 -3.9794567 -3.9619946 -3.9889853 -4.0292835 -4.0587626 -4.0979705 -4.1458411 -4.1785345 -4.1924391][-4.2171907 -4.1925764 -4.1570916 -4.1177759 -4.075222 -4.0179806 -3.9554265 -3.9343884 -3.9812317 -4.049067 -4.1012726 -4.1520734 -4.2010727 -4.2329373 -4.2424664][-4.2282343 -4.2015524 -4.170486 -4.1432786 -4.1147408 -4.0688624 -4.0136442 -3.9987149 -4.0427513 -4.106298 -4.1569781 -4.2016616 -4.2462568 -4.276679 -4.2808189][-4.2273092 -4.2039442 -4.1857686 -4.1770234 -4.1679983 -4.1426229 -4.108881 -4.1054678 -4.1380534 -4.1820164 -4.218533 -4.2494931 -4.2821908 -4.3030066 -4.2973547][-4.22169 -4.2085037 -4.2070904 -4.2136927 -4.2174492 -4.206943 -4.1912794 -4.1969233 -4.2215595 -4.2491279 -4.2738266 -4.293694 -4.3118782 -4.3159895 -4.2950869][-4.2254076 -4.2254982 -4.236465 -4.2505279 -4.2590375 -4.2566147 -4.2511282 -4.2597327 -4.2783108 -4.2982874 -4.3166938 -4.3288403 -4.3312039 -4.3151212 -4.2780323][-4.2280378 -4.2400966 -4.2572432 -4.2732749 -4.2835689 -4.2858267 -4.2869806 -4.2950659 -4.3073311 -4.321044 -4.3344021 -4.3397136 -4.3282819 -4.2952652 -4.2471309][-4.2309365 -4.2508163 -4.2697158 -4.2855539 -4.2950912 -4.2993689 -4.302875 -4.3077168 -4.3153763 -4.3216524 -4.3258371 -4.3218675 -4.298955 -4.2550364 -4.2034049][-4.2440224 -4.2651148 -4.282618 -4.296113 -4.3033948 -4.3071909 -4.3089762 -4.3107667 -4.3144274 -4.313169 -4.3064857 -4.2919993 -4.2612696 -4.2130837 -4.1652408][-4.2709804 -4.2852778 -4.2955294 -4.3029284 -4.3047347 -4.30455 -4.3016572 -4.2989922 -4.2998457 -4.2944984 -4.2805524 -4.2604837 -4.2290235 -4.1855321 -4.1459875]]...]
INFO - root - 2017-12-05 18:47:00.904137: step 31610, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 74h:55m:20s remains)
INFO - root - 2017-12-05 18:47:09.945860: step 31620, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 75h:58m:43s remains)
INFO - root - 2017-12-05 18:47:19.137619: step 31630, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 76h:11m:34s remains)
INFO - root - 2017-12-05 18:47:28.266777: step 31640, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.922 sec/batch; 77h:05m:24s remains)
INFO - root - 2017-12-05 18:47:37.322629: step 31650, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.927 sec/batch; 77h:29m:27s remains)
INFO - root - 2017-12-05 18:47:46.456769: step 31660, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 76h:16m:58s remains)
INFO - root - 2017-12-05 18:47:55.494317: step 31670, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 75h:31m:22s remains)
INFO - root - 2017-12-05 18:48:04.655302: step 31680, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 78h:28m:14s remains)
INFO - root - 2017-12-05 18:48:13.921560: step 31690, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.945 sec/batch; 78h:55m:17s remains)
INFO - root - 2017-12-05 18:48:23.130572: step 31700, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 74h:46m:26s remains)
2017-12-05 18:48:23.914094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1235623 -4.1242056 -4.1291022 -4.135376 -4.1348176 -4.1229959 -4.1199622 -4.1291075 -4.1370149 -4.1290946 -4.1164165 -4.1111975 -4.1122541 -4.1263232 -4.1506181][-4.1154165 -4.1186066 -4.1258683 -4.1342163 -4.13521 -4.124835 -4.118978 -4.125463 -4.1350183 -4.1224227 -4.1042147 -4.0990753 -4.1088014 -4.1291308 -4.1566353][-4.1166148 -4.1186948 -4.12386 -4.1296005 -4.1227822 -4.1075406 -4.0990634 -4.1066556 -4.1264491 -4.1287012 -4.1155186 -4.1078548 -4.1185193 -4.1399922 -4.1661172][-4.1203346 -4.1270275 -4.1328244 -4.1349955 -4.1125894 -4.07542 -4.0561657 -4.0720663 -4.1076531 -4.1326733 -4.1365237 -4.1327925 -4.1373739 -4.1527643 -4.1745133][-4.1020522 -4.1218243 -4.134243 -4.1367025 -4.1002622 -4.0359731 -3.9926605 -4.0110273 -4.0696926 -4.1194124 -4.1446991 -4.150044 -4.148531 -4.1579428 -4.1774397][-4.071826 -4.1050997 -4.1241817 -4.1292176 -4.0818791 -3.990459 -3.9141943 -3.9290621 -4.0164075 -4.0941954 -4.1402006 -4.1519547 -4.1501503 -4.1585503 -4.1799622][-4.0635471 -4.1004796 -4.1228848 -4.1199923 -4.0563455 -3.9433522 -3.8491857 -3.8662109 -3.9781971 -4.0757914 -4.1309929 -4.1447911 -4.14937 -4.1657271 -4.1899972][-4.082346 -4.11512 -4.136004 -4.1241741 -4.0587397 -3.962456 -3.8908191 -3.9139037 -4.0112405 -4.0928435 -4.1335382 -4.1457052 -4.1565781 -4.1801853 -4.2021065][-4.108983 -4.1340861 -4.153769 -4.1431117 -4.102551 -4.0465956 -4.007906 -4.027791 -4.0878773 -4.1297779 -4.1422424 -4.15047 -4.1693482 -4.1964417 -4.2183862][-4.1299419 -4.1534896 -4.1711249 -4.1689744 -4.1587834 -4.1364441 -4.1179342 -4.1279035 -4.1568551 -4.1659455 -4.1517749 -4.1524658 -4.175087 -4.2097139 -4.2329688][-4.1630521 -4.1834407 -4.2005606 -4.2013512 -4.2039189 -4.1982379 -4.1899834 -4.1911874 -4.2037954 -4.1932631 -4.1626754 -4.1551409 -4.1810908 -4.2244315 -4.249794][-4.2165685 -4.2311168 -4.2471232 -4.250546 -4.2527771 -4.248508 -4.243762 -4.2395 -4.2372217 -4.2149916 -4.1810608 -4.171524 -4.1981974 -4.244638 -4.26978][-4.2716384 -4.2820148 -4.2929282 -4.297863 -4.2995925 -4.2900434 -4.2838912 -4.2777243 -4.2660384 -4.2412062 -4.2162237 -4.2071829 -4.2279415 -4.2650356 -4.2845163][-4.2883372 -4.296134 -4.3028007 -4.30969 -4.3134613 -4.30654 -4.2998414 -4.2940879 -4.2839146 -4.2653751 -4.2490888 -4.2430058 -4.2538195 -4.279778 -4.2966886][-4.2762542 -4.2840009 -4.2901006 -4.2986865 -4.306654 -4.308064 -4.30588 -4.3020282 -4.2961926 -4.2863712 -4.2768307 -4.2735772 -4.2776418 -4.2933178 -4.3076758]]...]
INFO - root - 2017-12-05 18:48:33.123399: step 31710, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 76h:25m:01s remains)
INFO - root - 2017-12-05 18:48:42.330280: step 31720, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 74h:49m:25s remains)
INFO - root - 2017-12-05 18:48:51.339377: step 31730, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 76h:01m:02s remains)
INFO - root - 2017-12-05 18:49:00.522181: step 31740, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 74h:24m:37s remains)
INFO - root - 2017-12-05 18:49:09.558154: step 31750, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 75h:49m:05s remains)
INFO - root - 2017-12-05 18:49:18.670860: step 31760, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 75h:27m:53s remains)
INFO - root - 2017-12-05 18:49:27.853213: step 31770, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 76h:59m:12s remains)
INFO - root - 2017-12-05 18:49:36.897129: step 31780, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 77h:37m:42s remains)
INFO - root - 2017-12-05 18:49:46.093686: step 31790, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.925 sec/batch; 77h:16m:57s remains)
INFO - root - 2017-12-05 18:49:55.242191: step 31800, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 79h:56m:55s remains)
2017-12-05 18:49:56.016070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2586846 -4.2518015 -4.24432 -4.2326956 -4.2175193 -4.2088022 -4.2079506 -4.22112 -4.2350578 -4.2402663 -4.2434769 -4.2531681 -4.2481127 -4.2137127 -4.1846642][-4.2538428 -4.2528224 -4.2480149 -4.2364588 -4.2213945 -4.2090521 -4.2011957 -4.212131 -4.2329917 -4.2489614 -4.2606583 -4.26161 -4.2392783 -4.1896172 -4.1505365][-4.25197 -4.2538838 -4.2485933 -4.2382832 -4.2213626 -4.1963158 -4.167449 -4.1702447 -4.2060785 -4.2361703 -4.2582483 -4.2574735 -4.2261453 -4.166049 -4.1167369][-4.2487531 -4.2521086 -4.2481523 -4.2405806 -4.2161536 -4.1701169 -4.1130052 -4.1064334 -4.1616311 -4.2097464 -4.2471232 -4.2559853 -4.2314439 -4.1777744 -4.1286855][-4.24446 -4.2482562 -4.2432852 -4.2344027 -4.2033515 -4.1402435 -4.0616646 -4.0457945 -4.1172028 -4.18389 -4.2367315 -4.2600541 -4.2478018 -4.2063308 -4.1642418][-4.2487721 -4.2449551 -4.2324457 -4.21574 -4.1761727 -4.0961828 -3.9961922 -3.9718318 -4.0621634 -4.1487045 -4.2187977 -4.2570968 -4.2599869 -4.233912 -4.2004471][-4.2494755 -4.2383604 -4.2151508 -4.1864619 -4.136466 -4.0419016 -3.9242482 -3.8906102 -3.9914656 -4.0969791 -4.186317 -4.2441525 -4.2618752 -4.2474914 -4.2178531][-4.2292967 -4.2164073 -4.1897936 -4.1621418 -4.116992 -4.0235057 -3.9108682 -3.8787208 -3.9664929 -4.066802 -4.1576071 -4.2211576 -4.2473855 -4.2399588 -4.2109056][-4.2093315 -4.2071238 -4.1878424 -4.1710129 -4.1377621 -4.0602226 -3.9636514 -3.9410655 -4.0029607 -4.077322 -4.1474428 -4.2020149 -4.2288494 -4.2275496 -4.2011242][-4.1946011 -4.2088847 -4.2022452 -4.1934576 -4.1717873 -4.1140733 -4.0393014 -4.0311594 -4.0731564 -4.114912 -4.1551337 -4.1956911 -4.2242117 -4.230144 -4.2083325][-4.1782875 -4.2084718 -4.2177334 -4.2178354 -4.2029395 -4.1552105 -4.09584 -4.09153 -4.1174431 -4.1374688 -4.1567941 -4.1903949 -4.2235646 -4.237608 -4.2232852][-4.1790705 -4.2141509 -4.2315884 -4.235827 -4.2243228 -4.1842308 -4.1355171 -4.1283855 -4.1407394 -4.1487484 -4.1588812 -4.1895318 -4.2253675 -4.2459621 -4.2409372][-4.2029514 -4.2281456 -4.2432928 -4.2480059 -4.2396021 -4.2132058 -4.180891 -4.1764908 -4.1833854 -4.1851287 -4.1879196 -4.2088523 -4.2370963 -4.255774 -4.2590733][-4.2309728 -4.2415037 -4.246089 -4.2495351 -4.2444611 -4.2286439 -4.2141585 -4.2186885 -4.2252946 -4.224122 -4.2237368 -4.2337379 -4.2498031 -4.2608705 -4.2632079][-4.2432094 -4.2445135 -4.2414417 -4.2428036 -4.2407732 -4.2333207 -4.2321668 -4.2419381 -4.2485967 -4.2486014 -4.2479887 -4.2505217 -4.2560153 -4.2603993 -4.2603531]]...]
INFO - root - 2017-12-05 18:50:05.171216: step 31810, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 76h:15m:13s remains)
INFO - root - 2017-12-05 18:50:14.371401: step 31820, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 76h:38m:02s remains)
INFO - root - 2017-12-05 18:50:23.477397: step 31830, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 74h:03m:55s remains)
INFO - root - 2017-12-05 18:50:32.647819: step 31840, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 76h:13m:14s remains)
INFO - root - 2017-12-05 18:50:41.843206: step 31850, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 75h:35m:42s remains)
INFO - root - 2017-12-05 18:50:50.920735: step 31860, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 77h:51m:06s remains)
INFO - root - 2017-12-05 18:51:00.079089: step 31870, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.912 sec/batch; 76h:08m:08s remains)
INFO - root - 2017-12-05 18:51:09.146367: step 31880, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 75h:30m:48s remains)
INFO - root - 2017-12-05 18:51:18.279182: step 31890, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 76h:20m:15s remains)
INFO - root - 2017-12-05 18:51:27.438129: step 31900, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 74h:58m:04s remains)
2017-12-05 18:51:28.275850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2115259 -4.1542168 -4.1216817 -4.11789 -4.1378155 -4.1840572 -4.215239 -4.2148609 -4.2008963 -4.1816082 -4.1535892 -4.1307569 -4.1304889 -4.1554332 -4.1916122][-4.2277641 -4.1743016 -4.1323748 -4.1192188 -4.1381969 -4.1829324 -4.2082481 -4.2035203 -4.1895194 -4.1773081 -4.1593938 -4.1408181 -4.140018 -4.165658 -4.2036242][-4.2463326 -4.1976247 -4.1466613 -4.1228375 -4.1368966 -4.1730447 -4.187993 -4.1765661 -4.1659613 -4.1661067 -4.16405 -4.1560273 -4.1603522 -4.1893506 -4.2274342][-4.2642326 -4.2209063 -4.1630297 -4.1261206 -4.12872 -4.1487451 -4.1475916 -4.1272759 -4.1243758 -4.1446466 -4.1635323 -4.17095 -4.1863647 -4.2204967 -4.2569475][-4.2770567 -4.2391553 -4.17798 -4.1301312 -4.120255 -4.1201344 -4.0988727 -4.068224 -4.0765414 -4.1229115 -4.1642408 -4.1870427 -4.2148643 -4.254487 -4.2885237][-4.2776041 -4.2431812 -4.1776519 -4.1199145 -4.0968709 -4.0757546 -4.034483 -3.9918582 -4.0137296 -4.086688 -4.1501837 -4.1884274 -4.2284193 -4.2742038 -4.3044271][-4.2698412 -4.2332664 -4.1579232 -4.0856633 -4.0491772 -4.0075083 -3.9443533 -3.8868256 -3.9277458 -4.0275221 -4.1070938 -4.1586618 -4.2139955 -4.2705345 -4.3002071][-4.2461443 -4.210783 -4.1349015 -4.0601315 -4.0192938 -3.9630148 -3.8871 -3.8233438 -3.88406 -3.9983482 -4.0806775 -4.1351075 -4.1973958 -4.2584405 -4.2868075][-4.2038789 -4.1702466 -4.0968328 -4.0245266 -3.979841 -3.9131587 -3.8232608 -3.7520518 -3.8214943 -3.9399874 -4.0237913 -4.0825558 -4.1567359 -4.2288141 -4.2644715][-4.1821961 -4.1484184 -4.0820932 -4.0163164 -3.9699116 -3.9019213 -3.811733 -3.7413657 -3.7968616 -3.8994703 -3.9730358 -4.0331097 -4.11714 -4.1991425 -4.2443261][-4.1773582 -4.1533628 -4.1074524 -4.0630465 -4.0254693 -3.9689157 -3.9008803 -3.85324 -3.8862333 -3.9523618 -4.0052547 -4.0565052 -4.1303625 -4.2003064 -4.2426062][-4.188796 -4.1739788 -4.1457338 -4.1188169 -4.0910697 -4.04699 -4.0009904 -3.9705811 -3.9838309 -4.0188351 -4.05228 -4.0914211 -4.1479545 -4.2009206 -4.2339158][-4.1912203 -4.1853576 -4.1729293 -4.1612134 -4.1463413 -4.1178336 -4.0904374 -4.0709066 -4.074995 -4.0933805 -4.1141095 -4.1421628 -4.1812949 -4.2159214 -4.2356849][-4.1985426 -4.2005215 -4.1997538 -4.1999836 -4.1987176 -4.1879506 -4.1747837 -4.1634455 -4.1619978 -4.1708803 -4.1838722 -4.2006025 -4.221364 -4.2386 -4.2471223][-4.233201 -4.237905 -4.2410769 -4.2449045 -4.2510152 -4.2516651 -4.2473211 -4.2403984 -4.2350192 -4.2358723 -4.2398739 -4.2453728 -4.2518024 -4.2563076 -4.2556291]]...]
INFO - root - 2017-12-05 18:51:37.314076: step 31910, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 76h:22m:54s remains)
INFO - root - 2017-12-05 18:51:46.507479: step 31920, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 76h:42m:20s remains)
INFO - root - 2017-12-05 18:51:55.750729: step 31930, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 72h:54m:32s remains)
INFO - root - 2017-12-05 18:52:05.071073: step 31940, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 77h:18m:14s remains)
INFO - root - 2017-12-05 18:52:14.115651: step 31950, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 75h:57m:55s remains)
INFO - root - 2017-12-05 18:52:23.276956: step 31960, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 76h:40m:16s remains)
INFO - root - 2017-12-05 18:52:32.463683: step 31970, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 77h:20m:10s remains)
INFO - root - 2017-12-05 18:52:41.487724: step 31980, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 74h:46m:51s remains)
INFO - root - 2017-12-05 18:52:50.614168: step 31990, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 77h:04m:08s remains)
INFO - root - 2017-12-05 18:52:59.817919: step 32000, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.889 sec/batch; 74h:14m:00s remains)
2017-12-05 18:53:00.602873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3193059 -4.3027625 -4.2823863 -4.2615013 -4.2416449 -4.2183261 -4.1959686 -4.1837115 -4.1890473 -4.1992531 -4.21242 -4.2368755 -4.2582064 -4.2756109 -4.2934375][-4.3179026 -4.2987165 -4.2757053 -4.2557554 -4.2372856 -4.2094045 -4.17345 -4.14842 -4.1555791 -4.1757674 -4.1939545 -4.2220817 -4.2487483 -4.2716637 -4.2937531][-4.3126307 -4.2906289 -4.2667747 -4.2492995 -4.2323861 -4.1989965 -4.1500382 -4.1149764 -4.1289577 -4.1619849 -4.183218 -4.2124033 -4.2432919 -4.2724395 -4.2995057][-4.3057275 -4.2799125 -4.2553735 -4.2373056 -4.2173452 -4.1757164 -4.1139278 -4.0748034 -4.1011209 -4.1485424 -4.1749554 -4.2046018 -4.2381558 -4.2728615 -4.3046722][-4.2967582 -4.2692823 -4.2402849 -4.2126541 -4.1782255 -4.1192327 -4.0418863 -4.0048194 -4.0459528 -4.1136694 -4.1538157 -4.1873178 -4.2251005 -4.2694087 -4.3064375][-4.2839351 -4.2522869 -4.2101784 -4.1593575 -4.0961633 -4.0082726 -3.9153318 -3.88849 -3.9511633 -4.0490565 -4.1154881 -4.1625462 -4.2139363 -4.2695704 -4.3107638][-4.2617149 -4.2183056 -4.1494794 -4.0636325 -3.9699802 -3.864655 -3.7792218 -3.7803402 -3.8673697 -3.993937 -4.088655 -4.1541715 -4.2199407 -4.2812381 -4.3193913][-4.2356482 -4.1788392 -4.08604 -3.9737875 -3.870811 -3.7859836 -3.740921 -3.774838 -3.8748226 -4.0057693 -4.1053638 -4.1737385 -4.2397647 -4.2954383 -4.3274951][-4.2186136 -4.1617384 -4.0704284 -3.9627283 -3.8773046 -3.832149 -3.8255146 -3.8729625 -3.9658072 -4.0748606 -4.1531487 -4.2052588 -4.2573657 -4.3013792 -4.3267989][-4.2113714 -4.1660051 -4.0957794 -4.0173965 -3.9618709 -3.9454103 -3.9578137 -4.0064125 -4.0848756 -4.1631179 -4.2102346 -4.23325 -4.2605224 -4.29005 -4.3111215][-4.2148571 -4.1844058 -4.1402788 -4.0936728 -4.0626903 -4.0606942 -4.0786576 -4.1228862 -4.1817827 -4.2281785 -4.2430234 -4.2345223 -4.2379813 -4.2572436 -4.2783322][-4.2271948 -4.2075329 -4.1838846 -4.15893 -4.144568 -4.1548905 -4.1752334 -4.20916 -4.2410116 -4.2569928 -4.2426515 -4.2119761 -4.2013283 -4.2157631 -4.2408605][-4.2475719 -4.2368245 -4.2249074 -4.2106009 -4.2020478 -4.2121191 -4.2262607 -4.2443905 -4.2526178 -4.2438736 -4.2092543 -4.1674714 -4.1528835 -4.1735344 -4.2113791][-4.2765384 -4.2719975 -4.2634187 -4.249948 -4.2378516 -4.2357874 -4.235539 -4.2376056 -4.2313719 -4.2096195 -4.16617 -4.1242342 -4.1162095 -4.15061 -4.2015495][-4.3046479 -4.3021393 -4.2938056 -4.2782769 -4.2609372 -4.2478371 -4.2365651 -4.2280383 -4.2141237 -4.1883931 -4.1494174 -4.1178112 -4.1205182 -4.1616831 -4.2131481]]...]
INFO - root - 2017-12-05 18:53:09.733119: step 32010, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 76h:54m:55s remains)
INFO - root - 2017-12-05 18:53:18.843639: step 32020, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 76h:44m:46s remains)
INFO - root - 2017-12-05 18:53:28.074855: step 32030, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 73h:48m:20s remains)
INFO - root - 2017-12-05 18:53:37.043515: step 32040, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 76h:18m:52s remains)
INFO - root - 2017-12-05 18:53:46.280532: step 32050, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 77h:19m:14s remains)
INFO - root - 2017-12-05 18:53:55.384863: step 32060, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 77h:24m:42s remains)
INFO - root - 2017-12-05 18:54:04.644677: step 32070, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.953 sec/batch; 79h:32m:19s remains)
INFO - root - 2017-12-05 18:54:13.635500: step 32080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 75h:22m:03s remains)
INFO - root - 2017-12-05 18:54:22.723459: step 32090, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 78h:00m:13s remains)
INFO - root - 2017-12-05 18:54:31.846981: step 32100, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 76h:09m:35s remains)
2017-12-05 18:54:32.647342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2469192 -4.238431 -4.2228184 -4.2154083 -4.2203231 -4.2293673 -4.2366004 -4.2425661 -4.2465329 -4.2482977 -4.2517457 -4.2528968 -4.2526741 -4.2527537 -4.253583][-4.2422652 -4.2296596 -4.2104459 -4.2015405 -4.2064471 -4.2167225 -4.2305408 -4.2419815 -4.2468748 -4.2468467 -4.247848 -4.2451239 -4.2370706 -4.2296815 -4.2283144][-4.2355943 -4.2246032 -4.2046647 -4.1908207 -4.1903782 -4.1989083 -4.2175722 -4.2365088 -4.2448292 -4.2463446 -4.2477474 -4.243628 -4.2304955 -4.2195568 -4.2165737][-4.2215619 -4.2149343 -4.1961789 -4.1775174 -4.1701913 -4.1754365 -4.1936989 -4.212976 -4.2239661 -4.2314391 -4.2356534 -4.2332821 -4.222126 -4.212626 -4.2088151][-4.1906357 -4.191678 -4.1771464 -4.1575341 -4.1456232 -4.1496882 -4.1706333 -4.1895323 -4.2016835 -4.2118535 -4.2151146 -4.2106915 -4.2018795 -4.1949387 -4.1938519][-4.139133 -4.1467547 -4.1386061 -4.1229663 -4.1125445 -4.1197023 -4.1444125 -4.16375 -4.175014 -4.1870127 -4.1913571 -4.1859088 -4.17924 -4.1761594 -4.1797709][-4.0927382 -4.0994725 -4.0953016 -4.0863438 -4.0787783 -4.0863624 -4.1127534 -4.1357675 -4.1503253 -4.1643133 -4.1721067 -4.1692572 -4.165266 -4.1650496 -4.1735029][-4.0804424 -4.0811706 -4.0755072 -4.0683441 -4.0580053 -4.053946 -4.0681548 -4.0870295 -4.108099 -4.1339984 -4.1539226 -4.1607733 -4.1631188 -4.1671362 -4.17914][-4.1031394 -4.1010895 -4.093719 -4.0837331 -4.0658112 -4.0403376 -4.0263844 -4.026032 -4.0445862 -4.0804558 -4.1179194 -4.1431117 -4.1592174 -4.1705027 -4.1885848][-4.1366363 -4.1364403 -4.1303139 -4.1213341 -4.1040025 -4.068984 -4.0326219 -4.007585 -4.0076637 -4.03612 -4.0741539 -4.1053019 -4.1293025 -4.1522307 -4.1834965][-4.1555996 -4.1615095 -4.160284 -4.1570363 -4.1484189 -4.1216245 -4.0842724 -4.049212 -4.03017 -4.0325804 -4.0468707 -4.0627775 -4.0828543 -4.1118731 -4.1542206][-4.151659 -4.1629896 -4.1706495 -4.1772828 -4.1793251 -4.1671906 -4.1434817 -4.1154628 -4.0917373 -4.0744014 -4.0587435 -4.0473976 -4.0519471 -4.0742469 -4.1131024][-4.130898 -4.1429491 -4.15826 -4.1753907 -4.1863389 -4.1871138 -4.1796703 -4.16795 -4.1542006 -4.1366215 -4.110487 -4.0842023 -4.0737858 -4.0793033 -4.0962887][-4.1106124 -4.119504 -4.1359305 -4.1557488 -4.1688042 -4.1775646 -4.1844959 -4.1918983 -4.193397 -4.1830978 -4.1604004 -4.1348896 -4.1195354 -4.1138453 -4.1118727][-4.11116 -4.1150026 -4.1267009 -4.1415057 -4.1494908 -4.1555572 -4.1673956 -4.1844759 -4.1967034 -4.1940184 -4.1810184 -4.1647248 -4.1523061 -4.1450438 -4.1380196]]...]
INFO - root - 2017-12-05 18:54:41.848873: step 32110, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 76h:34m:36s remains)
INFO - root - 2017-12-05 18:54:50.900386: step 32120, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 77h:16m:31s remains)
INFO - root - 2017-12-05 18:55:00.124222: step 32130, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 78h:36m:06s remains)
INFO - root - 2017-12-05 18:55:09.145417: step 32140, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 77h:07m:24s remains)
INFO - root - 2017-12-05 18:55:18.248645: step 32150, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 76h:27m:57s remains)
INFO - root - 2017-12-05 18:55:27.262977: step 32160, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 69h:22m:16s remains)
INFO - root - 2017-12-05 18:55:36.354322: step 32170, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 78h:51m:38s remains)
INFO - root - 2017-12-05 18:55:45.553212: step 32180, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.918 sec/batch; 76h:32m:43s remains)
INFO - root - 2017-12-05 18:55:54.602579: step 32190, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 76h:00m:18s remains)
INFO - root - 2017-12-05 18:56:03.775103: step 32200, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 75h:56m:31s remains)
2017-12-05 18:56:04.579296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1167469 -4.1415477 -4.1607161 -4.1728654 -4.1652513 -4.1423254 -4.1257434 -4.1221547 -4.1373768 -4.1538873 -4.1599488 -4.1726179 -4.1845684 -4.1900754 -4.2140303][-4.0998259 -4.1197867 -4.1387348 -4.14644 -4.1375737 -4.1255841 -4.1177831 -4.1129746 -4.122714 -4.1325293 -4.1335034 -4.1383476 -4.1525397 -4.1691003 -4.2026877][-4.080018 -4.0951 -4.1133742 -4.1224594 -4.1223903 -4.1229172 -4.1205292 -4.1163578 -4.1206207 -4.12129 -4.1120043 -4.1065903 -4.1194854 -4.148942 -4.1931987][-4.0563655 -4.0744286 -4.0981255 -4.1115375 -4.11934 -4.1272779 -4.1239643 -4.1212783 -4.1274333 -4.1316638 -4.1203423 -4.1115713 -4.1222372 -4.154398 -4.2010565][-4.04856 -4.0703225 -4.0914173 -4.1014838 -4.1068006 -4.1112142 -4.1054082 -4.106184 -4.1168675 -4.1300354 -4.1281004 -4.1250472 -4.1368823 -4.1669288 -4.2116351][-4.0473762 -4.0650854 -4.0734468 -4.0740356 -4.072031 -4.0711656 -4.0667109 -4.0678892 -4.0755963 -4.0892959 -4.0966086 -4.10329 -4.1224203 -4.1575537 -4.2040229][-4.0582547 -4.0673561 -4.064362 -4.0558214 -4.0485592 -4.0430021 -4.0384269 -4.0320892 -4.0195556 -4.0157886 -4.0194292 -4.0405664 -4.077878 -4.1262612 -4.181674][-4.0775738 -4.0766063 -4.0649843 -4.053401 -4.0479412 -4.0423675 -4.0361519 -4.0168972 -3.9856899 -3.9611554 -3.9539328 -3.98128 -4.0326581 -4.0930924 -4.1595798][-4.0778146 -4.0671778 -4.0480556 -4.036643 -4.0374227 -4.0391922 -4.03473 -4.0090923 -3.9748189 -3.9489012 -3.9413815 -3.9683342 -4.019917 -4.0799127 -4.147316][-4.0674052 -4.0586076 -4.0383444 -4.0274067 -4.0346408 -4.0472035 -4.0486956 -4.0264792 -4.0008841 -3.9788373 -3.9731503 -3.9966905 -4.0392895 -4.0888109 -4.1457815][-4.0664611 -4.0653405 -4.0528746 -4.0456486 -4.0522985 -4.0652981 -4.0710421 -4.0595593 -4.0510368 -4.0366893 -4.0308647 -4.0446911 -4.07395 -4.1078968 -4.1532559][-4.0851979 -4.0938406 -4.0915575 -4.0904722 -4.0906153 -4.094924 -4.0950823 -4.0889053 -4.091517 -4.0820074 -4.0769529 -4.0841808 -4.102829 -4.1281695 -4.1680765][-4.1113138 -4.1306257 -4.1371183 -4.1462398 -4.1483231 -4.14275 -4.1289039 -4.1147585 -4.1161571 -4.108161 -4.1054506 -4.1132259 -4.1275573 -4.147325 -4.1822114][-4.1329 -4.1529784 -4.15669 -4.1651287 -4.1750412 -4.1695504 -4.151207 -4.1336274 -4.1323161 -4.1250648 -4.1232629 -4.1320076 -4.1490197 -4.1666307 -4.1967244][-4.1365938 -4.1471486 -4.1425796 -4.1462235 -4.164268 -4.1706119 -4.1578455 -4.1459842 -4.1407976 -4.1356778 -4.1392946 -4.152422 -4.1712666 -4.1868744 -4.214869]]...]
INFO - root - 2017-12-05 18:56:13.853482: step 32210, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 77h:10m:38s remains)
INFO - root - 2017-12-05 18:56:22.924703: step 32220, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 75h:56m:30s remains)
INFO - root - 2017-12-05 18:56:31.902408: step 32230, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 74h:41m:48s remains)
INFO - root - 2017-12-05 18:56:41.179273: step 32240, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 78h:43m:56s remains)
INFO - root - 2017-12-05 18:56:50.148897: step 32250, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 75h:02m:47s remains)
INFO - root - 2017-12-05 18:56:59.296805: step 32260, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 74h:51m:32s remains)
INFO - root - 2017-12-05 18:57:08.371164: step 32270, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 73h:52m:11s remains)
INFO - root - 2017-12-05 18:57:17.942050: step 32280, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 76h:35m:23s remains)
INFO - root - 2017-12-05 18:57:27.250281: step 32290, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 78h:42m:18s remains)
INFO - root - 2017-12-05 18:57:36.405437: step 32300, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 75h:17m:16s remains)
2017-12-05 18:57:37.233911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1914191 -4.1686697 -4.1607556 -4.1604834 -4.1580191 -4.147233 -4.1406608 -4.1492453 -4.1691175 -4.1947441 -4.2158341 -4.2096539 -4.1889129 -4.1708155 -4.1575389][-4.1957941 -4.1859865 -4.1872559 -4.1854081 -4.171546 -4.1526976 -4.1418419 -4.14572 -4.1630044 -4.1938739 -4.2275929 -4.2338357 -4.2086468 -4.1723738 -4.1434708][-4.2010574 -4.1997914 -4.2048492 -4.2049727 -4.189558 -4.1638312 -4.1406736 -4.1367092 -4.1551785 -4.1916595 -4.2333436 -4.2489181 -4.2269282 -4.1814666 -4.1407018][-4.2016463 -4.2023621 -4.2123604 -4.2202082 -4.2065396 -4.1699815 -4.1270733 -4.1157823 -4.1390152 -4.1779661 -4.2225981 -4.2470293 -4.2369771 -4.1991096 -4.1577148][-4.2008567 -4.1989837 -4.2106833 -4.2222786 -4.204977 -4.1489477 -4.0858412 -4.078845 -4.1152115 -4.1577411 -4.2030015 -4.23493 -4.2372522 -4.2144642 -4.1830454][-4.2000952 -4.1943064 -4.2033119 -4.2100854 -4.1749668 -4.0886188 -4.0050683 -4.0186138 -4.0861149 -4.1407337 -4.1857266 -4.2197652 -4.2315245 -4.2241073 -4.2056][-4.2025142 -4.1958041 -4.2008338 -4.195991 -4.1390562 -4.0173664 -3.9071226 -3.9469826 -4.0555296 -4.1283474 -4.1760659 -4.2076473 -4.221025 -4.2246404 -4.2162776][-4.1995068 -4.1974816 -4.2055721 -4.1949005 -4.1333117 -4.009973 -3.8885961 -3.9272113 -4.0494537 -4.1320176 -4.1782956 -4.2030525 -4.2098894 -4.2170067 -4.2152672][-4.1875992 -4.1936717 -4.206171 -4.2002449 -4.1569805 -4.069241 -3.9765346 -3.9877236 -4.078568 -4.1509433 -4.190712 -4.2077618 -4.205575 -4.2070904 -4.2072463][-4.1746197 -4.183219 -4.1915565 -4.1887517 -4.1678829 -4.1202717 -4.0648541 -4.0679092 -4.123137 -4.1763167 -4.208981 -4.2186985 -4.2060661 -4.1961646 -4.1924896][-4.1626115 -4.1682534 -4.1714292 -4.169385 -4.1640134 -4.1428752 -4.11342 -4.1203322 -4.1519589 -4.1887975 -4.217123 -4.2242961 -4.2036238 -4.1818757 -4.1697292][-4.1574512 -4.1546941 -4.1476579 -4.1446161 -4.1503878 -4.1458125 -4.1282568 -4.1383562 -4.1585751 -4.1852322 -4.2155523 -4.2255988 -4.2027817 -4.1726608 -4.1492662][-4.1640511 -4.1510806 -4.1327324 -4.1259217 -4.1367598 -4.1420221 -4.1276655 -4.1359787 -4.1509862 -4.1747961 -4.2065816 -4.2204685 -4.2030306 -4.1717291 -4.1439147][-4.1840568 -4.1651077 -4.1407242 -4.1263466 -4.1322742 -4.13554 -4.11754 -4.1230364 -4.1405907 -4.1649652 -4.1980844 -4.2148519 -4.2047758 -4.1796241 -4.1562595][-4.2052989 -4.1887279 -4.1680393 -4.1528535 -4.1477718 -4.1377583 -4.1134138 -4.1178946 -4.1391416 -4.1646419 -4.1957407 -4.2132473 -4.20787 -4.1885791 -4.172514]]...]
INFO - root - 2017-12-05 18:57:46.245629: step 32310, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 76h:17m:55s remains)
INFO - root - 2017-12-05 18:57:55.434514: step 32320, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 76h:40m:50s remains)
INFO - root - 2017-12-05 18:58:04.501428: step 32330, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 72h:21m:01s remains)
INFO - root - 2017-12-05 18:58:13.596809: step 32340, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 75h:04m:56s remains)
INFO - root - 2017-12-05 18:58:22.703400: step 32350, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 78h:24m:20s remains)
INFO - root - 2017-12-05 18:58:31.821648: step 32360, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 76h:33m:39s remains)
INFO - root - 2017-12-05 18:58:40.790495: step 32370, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 76h:42m:47s remains)
INFO - root - 2017-12-05 18:58:50.050794: step 32380, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 77h:09m:29s remains)
INFO - root - 2017-12-05 18:58:59.202285: step 32390, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.914 sec/batch; 76h:14m:04s remains)
INFO - root - 2017-12-05 18:59:08.281254: step 32400, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 77h:46m:27s remains)
2017-12-05 18:59:09.098783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2084036 -4.2348719 -4.2516251 -4.2604756 -4.2657461 -4.2619982 -4.2604766 -4.2592731 -4.2593541 -4.26071 -4.2587152 -4.2500772 -4.240654 -4.2334313 -4.2277894][-4.193285 -4.2195039 -4.2380524 -4.2479205 -4.2510991 -4.2452888 -4.2424111 -4.24257 -4.244626 -4.248415 -4.250587 -4.2460365 -4.238143 -4.2296696 -4.2225747][-4.1638288 -4.1920671 -4.2154322 -4.2272797 -4.2301006 -4.2234993 -4.2185783 -4.2182021 -4.2219114 -4.2284293 -4.2328734 -4.2324457 -4.2284088 -4.2221737 -4.2162852][-4.122961 -4.151226 -4.1796603 -4.2000985 -4.2095609 -4.2059684 -4.2003818 -4.1978111 -4.2006969 -4.2047348 -4.206615 -4.2062182 -4.2032647 -4.1997004 -4.1954088][-4.0859542 -4.1108551 -4.1406922 -4.1668539 -4.1793432 -4.1768503 -4.1699743 -4.1644249 -4.1631937 -4.1611876 -4.1599784 -4.1581941 -4.1519794 -4.14479 -4.1413627][-4.0593977 -4.0760736 -4.1015019 -4.1233907 -4.12871 -4.124095 -4.1166611 -4.1084638 -4.1057134 -4.10298 -4.1043506 -4.1006589 -4.0834336 -4.0639167 -4.0517159][-4.0528364 -4.057344 -4.0677524 -4.0743208 -4.0636306 -4.04823 -4.0329142 -4.0220141 -4.026423 -4.0381846 -4.0527205 -4.0529327 -4.0262432 -3.9948306 -3.9726338][-4.052124 -4.0447593 -4.0357718 -4.0187254 -3.983676 -3.9492803 -3.9290619 -3.928211 -3.9547255 -3.9958632 -4.0326462 -4.0411272 -4.013308 -3.9755814 -3.943449][-4.0681443 -4.05606 -4.0346627 -3.9977617 -3.9447081 -3.9049416 -3.8911862 -3.9061341 -3.9514914 -4.0074019 -4.05133 -4.061976 -4.0391016 -4.0000319 -3.960115][-4.0713511 -4.0631022 -4.0434322 -4.0036864 -3.9512568 -3.9199166 -3.9188509 -3.9453905 -3.9928553 -4.0428996 -4.0839772 -4.0985494 -4.0854697 -4.0546012 -4.0199156][-4.0721207 -4.0719771 -4.0596 -4.0262995 -3.9849236 -3.9644508 -3.9719203 -4.0035505 -4.0481582 -4.0912261 -4.1264324 -4.1409025 -4.1349978 -4.1139522 -4.0881624][-4.1039762 -4.1128321 -4.1096964 -4.0869374 -4.0578194 -4.0439043 -4.0489841 -4.0747662 -4.10857 -4.1392422 -4.1647267 -4.176125 -4.1730547 -4.1598887 -4.1429572][-4.1221852 -4.1363463 -4.1395693 -4.1281567 -4.1130934 -4.1059318 -4.1111226 -4.1312561 -4.1532025 -4.1707349 -4.1846833 -4.1900649 -4.1882296 -4.1803985 -4.170032][-4.1503525 -4.161293 -4.1654153 -4.1621809 -4.1599441 -4.1602039 -4.1667247 -4.1819849 -4.1945486 -4.2008991 -4.2049074 -4.2059765 -4.2042065 -4.2002983 -4.1962314][-4.1838026 -4.1902795 -4.193953 -4.1956058 -4.2001867 -4.2040691 -4.209311 -4.2189445 -4.2262664 -4.2286277 -4.2293687 -4.2273331 -4.2232552 -4.2195749 -4.2185817]]...]
INFO - root - 2017-12-05 18:59:18.325086: step 32410, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 76h:54m:40s remains)
INFO - root - 2017-12-05 18:59:27.457431: step 32420, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 77h:20m:29s remains)
INFO - root - 2017-12-05 18:59:36.462471: step 32430, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 74h:40m:14s remains)
INFO - root - 2017-12-05 18:59:45.588128: step 32440, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 75h:04m:57s remains)
INFO - root - 2017-12-05 18:59:54.705302: step 32450, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 77h:24m:40s remains)
INFO - root - 2017-12-05 19:00:03.945108: step 32460, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 74h:50m:52s remains)
INFO - root - 2017-12-05 19:00:13.000920: step 32470, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 75h:56m:25s remains)
INFO - root - 2017-12-05 19:00:22.261823: step 32480, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 74h:49m:33s remains)
INFO - root - 2017-12-05 19:00:31.453777: step 32490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 76h:29m:42s remains)
INFO - root - 2017-12-05 19:00:40.593673: step 32500, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 75h:06m:31s remains)
2017-12-05 19:00:41.396000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.266047 -4.2657118 -4.2605977 -4.2566915 -4.2549019 -4.2490396 -4.2424493 -4.234117 -4.2273684 -4.2266555 -4.2222705 -4.2099462 -4.1906505 -4.1754217 -4.1811733][-4.2218113 -4.2304792 -4.2330446 -4.2363963 -4.2418022 -4.2399597 -4.2312632 -4.2171283 -4.2036543 -4.1989527 -4.1952109 -4.1852956 -4.1675353 -4.1551995 -4.1677213][-4.1673574 -4.1924939 -4.2078543 -4.2194691 -4.2315187 -4.2338972 -4.2227926 -4.2002249 -4.1785407 -4.17081 -4.1691451 -4.1625371 -4.1473689 -4.1402369 -4.1598115][-4.1032386 -4.1461921 -4.17075 -4.1852341 -4.199039 -4.2039289 -4.1905909 -4.1619749 -4.1347542 -4.1267619 -4.1308031 -4.1323395 -4.1270728 -4.1291666 -4.1568789][-4.0385785 -4.0887976 -4.1140461 -4.1247711 -4.1354194 -4.1402135 -4.1254492 -4.0905428 -4.0564556 -4.0495348 -4.0603681 -4.07562 -4.0887957 -4.1087041 -4.1504655][-4.0073142 -4.0554008 -4.0764146 -4.0772014 -4.0729523 -4.0658884 -4.0441136 -4.0046306 -3.9642179 -3.9595387 -3.9828436 -4.0176916 -4.0530024 -4.0931797 -4.1477156][-4.0359297 -4.0705485 -4.0830393 -4.07606 -4.0597291 -4.0428534 -4.0201197 -3.9874227 -3.9527004 -3.9544559 -3.9912694 -4.0397544 -4.0825791 -4.1239066 -4.1734114][-4.0812922 -4.0993409 -4.1044993 -4.097064 -4.0856423 -4.0771108 -4.0681119 -4.05402 -4.0353374 -4.0435829 -4.0824914 -4.1267185 -4.1598153 -4.1887851 -4.2208352][-4.1168556 -4.1280613 -4.1337729 -4.1316638 -4.1318455 -4.1361074 -4.1408529 -4.1412711 -4.1378589 -4.1518469 -4.1834946 -4.2117076 -4.2292266 -4.2446256 -4.2596617][-4.1493292 -4.1636443 -4.1768022 -4.1834826 -4.1929588 -4.2036166 -4.2131062 -4.2193809 -4.2246017 -4.2395163 -4.2605491 -4.2718811 -4.2724705 -4.2737794 -4.2761569][-4.1867952 -4.2068176 -4.2273207 -4.241631 -4.2579308 -4.2723703 -4.2822504 -4.286623 -4.2901359 -4.299479 -4.3079171 -4.3041949 -4.2911992 -4.280509 -4.2768993][-4.2219443 -4.24456 -4.267642 -4.2847528 -4.3026419 -4.3172493 -4.3257818 -4.3262625 -4.3247752 -4.3262053 -4.3237681 -4.3103342 -4.2909613 -4.2761116 -4.2718592][-4.242115 -4.2638783 -4.2837129 -4.2983418 -4.3117757 -4.3215432 -4.3239307 -4.3175535 -4.310791 -4.3071227 -4.3017392 -4.2889566 -4.2728415 -4.2628574 -4.2638245][-4.2490215 -4.2659535 -4.2777247 -4.28326 -4.2850175 -4.2836957 -4.2757158 -4.2627172 -4.2558842 -4.2551942 -4.2561717 -4.2522488 -4.24593 -4.2468905 -4.2567959][-4.2467608 -4.2585878 -4.2625141 -4.2563944 -4.2433014 -4.2290249 -4.2128181 -4.1985984 -4.1977797 -4.2071681 -4.2190094 -4.225184 -4.22796 -4.2383194 -4.2556376]]...]
INFO - root - 2017-12-05 19:00:50.435632: step 32510, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 74h:25m:42s remains)
INFO - root - 2017-12-05 19:00:59.599880: step 32520, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 76h:22m:53s remains)
INFO - root - 2017-12-05 19:01:08.796555: step 32530, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 73h:24m:48s remains)
INFO - root - 2017-12-05 19:01:17.870209: step 32540, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.933 sec/batch; 77h:46m:39s remains)
INFO - root - 2017-12-05 19:01:26.829687: step 32550, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 75h:28m:43s remains)
INFO - root - 2017-12-05 19:01:36.010820: step 32560, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 74h:49m:57s remains)
INFO - root - 2017-12-05 19:01:45.136742: step 32570, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.968 sec/batch; 80h:39m:40s remains)
INFO - root - 2017-12-05 19:01:54.301908: step 32580, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 75h:59m:41s remains)
INFO - root - 2017-12-05 19:02:03.445178: step 32590, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 74h:46m:48s remains)
INFO - root - 2017-12-05 19:02:12.525259: step 32600, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 75h:42m:50s remains)
2017-12-05 19:02:13.330876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1821971 -4.2149343 -4.2376981 -4.245985 -4.23599 -4.2208915 -4.1979733 -4.1627679 -4.1367006 -4.1288218 -4.1508417 -4.2017455 -4.256434 -4.3042393 -4.3361611][-4.1784062 -4.212481 -4.2429638 -4.2571473 -4.2520962 -4.2422514 -4.2197094 -4.1713772 -4.129766 -4.1093593 -4.1215734 -4.171051 -4.2310538 -4.2909913 -4.3340926][-4.1682253 -4.1973157 -4.2325158 -4.2541132 -4.2547622 -4.2475142 -4.2179427 -4.1520061 -4.0928869 -4.0567088 -4.05954 -4.1091266 -4.1794453 -4.2573824 -4.3185649][-4.1492329 -4.1700168 -4.2026396 -4.2245493 -4.2292538 -4.221283 -4.1818871 -4.0997782 -4.0215869 -3.9635377 -3.9520588 -4.0093646 -4.1034193 -4.2063961 -4.2892742][-4.1189895 -4.1406455 -4.165709 -4.1842775 -4.1882677 -4.1661363 -4.10345 -3.995065 -3.8931339 -3.8205156 -3.8037069 -3.8847768 -4.0139561 -4.145452 -4.2511034][-4.0904813 -4.1166992 -4.1303091 -4.134964 -4.1317644 -4.0900455 -3.9879141 -3.8375959 -3.7099168 -3.6313467 -3.6221783 -3.7471559 -3.9218724 -4.0835562 -4.2108212][-4.0621324 -4.0825381 -4.0828738 -4.0700364 -4.06006 -4.0067267 -3.88906 -3.7163675 -3.5765913 -3.4965174 -3.5080302 -3.6700788 -3.8714125 -4.0515575 -4.1905279][-4.0293012 -4.036479 -4.0172248 -3.9948547 -3.9867573 -3.9565926 -3.8757486 -3.7458014 -3.6339793 -3.5714142 -3.5984793 -3.7490437 -3.925694 -4.0893955 -4.2122641][-3.9785113 -3.9781446 -3.9550953 -3.9395227 -3.9506788 -3.9624867 -3.9438851 -3.8868368 -3.8296926 -3.798224 -3.826232 -3.9303284 -4.05281 -4.174438 -4.2626629][-3.9363818 -3.9339201 -3.9202716 -3.9253387 -3.9624252 -4.01403 -4.0456076 -4.0395956 -4.0218086 -4.0101552 -4.0278106 -4.089632 -4.1679745 -4.2506537 -4.3089018][-3.9544067 -3.9541342 -3.9550877 -3.9782479 -4.0275841 -4.0955515 -4.1483951 -4.1569238 -4.1460338 -4.1319389 -4.1356025 -4.1755452 -4.2328916 -4.2941408 -4.3348527][-4.0038276 -4.0131712 -4.0319424 -4.0714631 -4.1214304 -4.1823931 -4.2257247 -4.2226825 -4.202168 -4.1789017 -4.1725445 -4.203712 -4.2525587 -4.3053288 -4.3391895][-4.0430484 -4.0685396 -4.1048532 -4.1546612 -4.2019329 -4.2437267 -4.2596407 -4.2339668 -4.19914 -4.1671181 -4.1581316 -4.1917114 -4.2422481 -4.2955818 -4.3304787][-4.062551 -4.1068044 -4.1581125 -4.2086234 -4.2511706 -4.2750006 -4.26336 -4.2173743 -4.1716981 -4.1337876 -4.1265225 -4.1643181 -4.2185712 -4.2775855 -4.3176832][-4.0723991 -4.138761 -4.2026081 -4.2546191 -4.2902493 -4.2976179 -4.2670293 -4.21377 -4.1666808 -4.1285481 -4.1224937 -4.1579666 -4.2104678 -4.2710114 -4.3139157]]...]
INFO - root - 2017-12-05 19:02:22.633748: step 32610, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 76h:19m:31s remains)
INFO - root - 2017-12-05 19:02:31.826776: step 32620, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 74h:36m:01s remains)
INFO - root - 2017-12-05 19:02:41.038397: step 32630, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 77h:22m:28s remains)
INFO - root - 2017-12-05 19:02:50.176736: step 32640, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 75h:46m:34s remains)
INFO - root - 2017-12-05 19:02:59.330084: step 32650, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 77h:46m:20s remains)
INFO - root - 2017-12-05 19:03:08.640007: step 32660, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 77h:23m:20s remains)
INFO - root - 2017-12-05 19:03:17.735523: step 32670, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 74h:48m:59s remains)
INFO - root - 2017-12-05 19:03:26.913009: step 32680, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 76h:36m:32s remains)
INFO - root - 2017-12-05 19:03:36.166389: step 32690, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 75h:49m:01s remains)
INFO - root - 2017-12-05 19:03:45.230631: step 32700, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 73h:25m:17s remains)
2017-12-05 19:03:46.014618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2311115 -4.2312264 -4.2369213 -4.2325587 -4.2119761 -4.1922374 -4.1915722 -4.2005763 -4.2080708 -4.2148647 -4.2197552 -4.2186909 -4.2075582 -4.2063832 -4.2100797][-4.2098236 -4.2079139 -4.2141509 -4.2101288 -4.1896734 -4.1664753 -4.1652536 -4.1697884 -4.1726823 -4.1730618 -4.1767645 -4.1780715 -4.165123 -4.1591744 -4.1655083][-4.2018919 -4.1982355 -4.2032785 -4.2000432 -4.1829605 -4.1577721 -4.1455355 -4.1369009 -4.1328321 -4.1326838 -4.1378784 -4.14517 -4.1343317 -4.1247826 -4.1275778][-4.2173815 -4.210609 -4.2083178 -4.197156 -4.1744466 -4.1410961 -4.1135912 -4.0932293 -4.0898523 -4.0953579 -4.1081729 -4.122416 -4.1166539 -4.1065426 -4.1055493][-4.243917 -4.2312722 -4.2173438 -4.1901116 -4.149281 -4.0972667 -4.0539708 -4.0362673 -4.0498829 -4.0721045 -4.0924325 -4.1067195 -4.1075287 -4.1050739 -4.1089482][-4.2651253 -4.2470918 -4.2214379 -4.1719403 -4.1009479 -4.0118465 -3.9442058 -3.94608 -3.9988291 -4.0522656 -4.0831609 -4.1032681 -4.1165438 -4.1264296 -4.1367755][-4.2763405 -4.2566743 -4.2242117 -4.1585312 -4.0605292 -3.9276817 -3.8231037 -3.8450942 -3.9462516 -4.0310984 -4.078372 -4.115603 -4.1466227 -4.1667008 -4.1789975][-4.2766247 -4.2583675 -4.2266436 -4.1573439 -4.0496163 -3.8964481 -3.7691612 -3.8024597 -3.9280469 -4.0299039 -4.0964513 -4.1537333 -4.1965914 -4.2143874 -4.2194624][-4.2737889 -4.2547708 -4.2224922 -4.155899 -4.0552821 -3.9234579 -3.828177 -3.8673713 -3.9770095 -4.0629282 -4.1256781 -4.1856422 -4.2286596 -4.2380772 -4.2336178][-4.2716365 -4.2485681 -4.2139883 -4.1522484 -4.0672245 -3.970535 -3.9189196 -3.9673607 -4.0539055 -4.1221275 -4.17313 -4.2240176 -4.2572603 -4.2594862 -4.2465277][-4.2716241 -4.2457662 -4.2088556 -4.1475339 -4.0755525 -4.0089879 -3.9909532 -4.0465364 -4.1183281 -4.17933 -4.221128 -4.2551179 -4.272131 -4.268 -4.2542229][-4.271534 -4.2446966 -4.2063303 -4.1452193 -4.0803452 -4.0350766 -4.0351181 -4.094511 -4.159935 -4.2189441 -4.2530704 -4.2717185 -4.2754269 -4.26828 -4.2581115][-4.2706113 -4.244288 -4.2080989 -4.1495008 -4.0906005 -4.0578814 -4.0697021 -4.1293015 -4.1862168 -4.2375193 -4.2620115 -4.2713065 -4.27288 -4.2693048 -4.2620935][-4.270514 -4.2459564 -4.2152352 -4.1628017 -4.1091528 -4.0845361 -4.1016126 -4.1578679 -4.2033591 -4.2398081 -4.2527652 -4.2570848 -4.2620564 -4.2667594 -4.2632437][-4.2725849 -4.2502975 -4.2243824 -4.1774158 -4.1303482 -4.1122456 -4.1299815 -4.17861 -4.2090034 -4.2264609 -4.2267857 -4.2289944 -4.2426782 -4.255939 -4.2580504]]...]
INFO - root - 2017-12-05 19:03:55.038069: step 32710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 72h:12m:59s remains)
INFO - root - 2017-12-05 19:04:04.106925: step 32720, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 72h:47m:18s remains)
INFO - root - 2017-12-05 19:04:13.155081: step 32730, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 68h:23m:56s remains)
INFO - root - 2017-12-05 19:04:22.487638: step 32740, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.948 sec/batch; 78h:57m:10s remains)
INFO - root - 2017-12-05 19:04:31.691883: step 32750, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 77h:09m:12s remains)
INFO - root - 2017-12-05 19:04:40.861780: step 32760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 75h:42m:45s remains)
INFO - root - 2017-12-05 19:04:50.035408: step 32770, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.924 sec/batch; 76h:53m:37s remains)
INFO - root - 2017-12-05 19:04:59.040508: step 32780, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 76h:00m:18s remains)
INFO - root - 2017-12-05 19:05:08.036771: step 32790, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 74h:58m:42s remains)
INFO - root - 2017-12-05 19:05:17.183805: step 32800, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 76h:51m:47s remains)
2017-12-05 19:05:18.069793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2497067 -4.2392716 -4.2328143 -4.2327023 -4.232059 -4.2389731 -4.2521591 -4.266799 -4.2839093 -4.2918272 -4.2941895 -4.2946415 -4.2992806 -4.3116846 -4.3181186][-4.2304606 -4.2178054 -4.2080755 -4.198709 -4.1888981 -4.1953435 -4.2149062 -4.23691 -4.2625642 -4.2746053 -4.2832918 -4.288291 -4.2954688 -4.31011 -4.3170042][-4.2198915 -4.2019835 -4.1831217 -4.1606145 -4.1417427 -4.1463251 -4.1670561 -4.1921 -4.2260656 -4.2435164 -4.2594166 -4.2753458 -4.286828 -4.304883 -4.3128519][-4.2095356 -4.1843863 -4.1565533 -4.122026 -4.095377 -4.0951362 -4.1101103 -4.1332793 -4.1725831 -4.2016315 -4.2270851 -4.2571678 -4.2764854 -4.298296 -4.3080153][-4.1889262 -4.1542969 -4.1173005 -4.0755019 -4.0431671 -4.033318 -4.0282474 -4.0379176 -4.0848112 -4.1367946 -4.1799455 -4.22996 -4.2613249 -4.2900653 -4.3028088][-4.1553154 -4.1060309 -4.0606408 -4.0185523 -3.9806478 -3.9560328 -3.9225802 -3.9088988 -3.970938 -4.0539412 -4.1177597 -4.1832752 -4.2226081 -4.2583275 -4.2827311][-4.1202927 -4.061245 -4.011941 -3.9739583 -3.9266124 -3.8805492 -3.8166718 -3.7757409 -3.8467441 -3.9450817 -4.0187654 -4.0924382 -4.1372986 -4.1859703 -4.2336712][-4.1105709 -4.0555472 -4.013052 -3.9842863 -3.9341698 -3.8735681 -3.7912655 -3.7342753 -3.7819507 -3.8547807 -3.9147804 -3.9842014 -4.0389757 -4.1037722 -4.1729064][-4.12488 -4.0832334 -4.0536594 -4.0349383 -3.9956489 -3.9396043 -3.864377 -3.8100488 -3.8209205 -3.8508067 -3.8834362 -3.9415014 -4.0037923 -4.0780239 -4.1523571][-4.1730294 -4.1479478 -4.1322904 -4.1223907 -4.0934453 -4.0465188 -3.9864707 -3.9409542 -3.9259746 -3.9257474 -3.9386735 -3.9843652 -4.0459528 -4.1163206 -4.1816497][-4.2407012 -4.2289724 -4.2223234 -4.2183294 -4.1966548 -4.1596837 -4.1131945 -4.0799541 -4.0587454 -4.0450859 -4.0463028 -4.0785856 -4.1270752 -4.1801867 -4.2302485][-4.3038321 -4.2969351 -4.2912445 -4.2861376 -4.2720342 -4.2506804 -4.2234888 -4.2017612 -4.1848655 -4.1708255 -4.1662049 -4.1812339 -4.2064452 -4.2379317 -4.2723203][-4.3365149 -4.3304491 -4.3255734 -4.3239913 -4.3202658 -4.3144493 -4.3051629 -4.2932396 -4.278831 -4.267345 -4.2629313 -4.2664418 -4.2752748 -4.2896452 -4.3093052][-4.3392205 -4.3353596 -4.3331103 -4.334465 -4.3371935 -4.3398976 -4.3386683 -4.3334274 -4.3249431 -4.3191071 -4.3169961 -4.3154445 -4.3176537 -4.3244386 -4.3346868][-4.3326159 -4.33127 -4.33111 -4.3320146 -4.3344107 -4.3372178 -4.3376927 -4.3363695 -4.3338189 -4.332541 -4.333034 -4.3322163 -4.3327866 -4.3362508 -4.3419757]]...]
INFO - root - 2017-12-05 19:05:27.175174: step 32810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 72h:30m:53s remains)
INFO - root - 2017-12-05 19:05:36.219280: step 32820, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 74h:51m:46s remains)
INFO - root - 2017-12-05 19:05:45.412770: step 32830, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.942 sec/batch; 78h:25m:24s remains)
INFO - root - 2017-12-05 19:05:54.354719: step 32840, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 73h:38m:19s remains)
INFO - root - 2017-12-05 19:06:03.422208: step 32850, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.962 sec/batch; 80h:06m:04s remains)
INFO - root - 2017-12-05 19:06:12.745642: step 32860, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 76h:44m:53s remains)
INFO - root - 2017-12-05 19:06:21.915350: step 32870, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 75h:47m:21s remains)
INFO - root - 2017-12-05 19:06:30.954306: step 32880, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 73h:04m:38s remains)
INFO - root - 2017-12-05 19:06:40.127570: step 32890, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 75h:49m:26s remains)
INFO - root - 2017-12-05 19:06:49.224033: step 32900, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 74h:33m:54s remains)
2017-12-05 19:06:50.055765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1761155 -4.1730714 -4.1765661 -4.1870809 -4.1997285 -4.2158546 -4.2362051 -4.2521248 -4.2567978 -4.2478724 -4.244988 -4.2590756 -4.2682581 -4.262835 -4.2539744][-4.150929 -4.1427135 -4.1448584 -4.1584578 -4.1733775 -4.1919136 -4.216001 -4.2370176 -4.2502747 -4.2489853 -4.2494636 -4.2638044 -4.2749095 -4.274672 -4.2649245][-4.1279612 -4.1224546 -4.1282663 -4.1495743 -4.1663213 -4.1813464 -4.20663 -4.2316604 -4.252459 -4.2600689 -4.2626042 -4.2736077 -4.2857132 -4.2914882 -4.2819448][-4.0925131 -4.0965085 -4.1130962 -4.1425757 -4.1584182 -4.1664081 -4.1876554 -4.2134337 -4.2404428 -4.2612963 -4.2728596 -4.2828016 -4.2932744 -4.3029389 -4.2932391][-4.0515156 -4.0640049 -4.0872316 -4.1181912 -4.1256447 -4.1223946 -4.1342993 -4.1553559 -4.188426 -4.2259283 -4.2527819 -4.2684355 -4.2809916 -4.2944961 -4.2865105][-4.0339684 -4.0445542 -4.0630174 -4.079648 -4.0723515 -4.0542736 -4.0430608 -4.0460482 -4.0861635 -4.1413436 -4.1847916 -4.2110524 -4.2291231 -4.252687 -4.2574716][-4.028142 -4.0293183 -4.0340738 -4.0294662 -4.0056639 -3.9694829 -3.9184389 -3.8892794 -3.9451985 -4.0331807 -4.0966549 -4.1333547 -4.1600237 -4.1975679 -4.224844][-4.0282617 -4.0183988 -4.0114341 -3.9922192 -3.9608231 -3.9074063 -3.809077 -3.7329271 -3.802983 -3.92422 -4.0045667 -4.0492249 -4.0894489 -4.145534 -4.1948485][-4.0651612 -4.0473413 -4.0366211 -4.0198641 -3.9925923 -3.9429774 -3.8442891 -3.755487 -3.7992792 -3.8988822 -3.9695833 -4.0150776 -4.0597448 -4.1181288 -4.17352][-4.1256456 -4.1056314 -4.0952663 -4.0863967 -4.0725865 -4.0433955 -3.9800658 -3.9188519 -3.928973 -3.9776328 -4.019568 -4.0567155 -4.0939918 -4.1333046 -4.1736965][-4.1700325 -4.1539516 -4.1450148 -4.14237 -4.1397858 -4.127615 -4.0961256 -4.06171 -4.0557523 -4.071629 -4.0974522 -4.1269217 -4.144587 -4.1608377 -4.1825452][-4.1903133 -4.1807 -4.174768 -4.1761389 -4.1799841 -4.1766329 -4.1639543 -4.1467195 -4.1378756 -4.1376867 -4.1516404 -4.1700263 -4.1732736 -4.1748104 -4.184628][-4.203486 -4.1980395 -4.1942372 -4.1974831 -4.2038927 -4.2061377 -4.2036119 -4.196528 -4.1880174 -4.1808033 -4.185122 -4.1942372 -4.1925707 -4.1913805 -4.1950636][-4.2222915 -4.2190032 -4.2171907 -4.2192979 -4.2241859 -4.2274923 -4.2289815 -4.2268453 -4.2226639 -4.2165012 -4.2185287 -4.2233744 -4.2217703 -4.2224841 -4.22582][-4.2561345 -4.2541256 -4.2532949 -4.254427 -4.2569666 -4.2592983 -4.2612853 -4.2622781 -4.2610755 -4.2567697 -4.25857 -4.2629533 -4.2625632 -4.2643061 -4.2671833]]...]
INFO - root - 2017-12-05 19:06:59.264293: step 32910, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 77h:31m:11s remains)
INFO - root - 2017-12-05 19:07:08.367708: step 32920, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 78h:02m:10s remains)
INFO - root - 2017-12-05 19:07:17.483970: step 32930, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 76h:18m:08s remains)
INFO - root - 2017-12-05 19:07:26.633437: step 32940, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 76h:55m:55s remains)
INFO - root - 2017-12-05 19:07:35.824701: step 32950, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 78h:36m:27s remains)
INFO - root - 2017-12-05 19:07:44.922973: step 32960, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 76h:49m:17s remains)
INFO - root - 2017-12-05 19:07:54.004875: step 32970, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 76h:54m:17s remains)
INFO - root - 2017-12-05 19:08:03.018198: step 32980, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 77h:32m:39s remains)
INFO - root - 2017-12-05 19:08:12.170514: step 32990, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 76h:15m:29s remains)
INFO - root - 2017-12-05 19:08:21.321610: step 33000, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 76h:36m:43s remains)
2017-12-05 19:08:22.121054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2321868 -4.1940303 -4.18252 -4.1907592 -4.2062349 -4.219317 -4.2189269 -4.2133284 -4.2079134 -4.2161131 -4.2340908 -4.2430611 -4.2423005 -4.2461209 -4.2573528][-4.2114453 -4.1688309 -4.1553168 -4.1572523 -4.1633263 -4.1677384 -4.1579714 -4.1406155 -4.1289725 -4.14396 -4.177794 -4.199944 -4.2084332 -4.221962 -4.2392488][-4.2036824 -4.1591969 -4.1411252 -4.1346245 -4.1280088 -4.1240134 -4.1070275 -4.0759025 -4.0571547 -4.0863161 -4.1404939 -4.1754556 -4.1925516 -4.2122664 -4.2310281][-4.1972218 -4.1542854 -4.1354256 -4.12374 -4.1077371 -4.0978184 -4.0710049 -4.0231791 -3.9995723 -4.0461421 -4.1196918 -4.1646981 -4.1887131 -4.2106633 -4.2257733][-4.1898303 -4.155642 -4.1434617 -4.1301088 -4.1113133 -4.0909538 -4.0336461 -3.9516494 -3.9302754 -4.0026617 -4.0924125 -4.1479545 -4.1797056 -4.2050557 -4.2191534][-4.1957207 -4.1711 -4.1627474 -4.1520405 -4.1313133 -4.0851183 -3.9715633 -3.8323088 -3.82447 -3.94134 -4.0555449 -4.1286678 -4.172883 -4.2015667 -4.2140207][-4.2130604 -4.1920433 -4.1761055 -4.1558585 -4.1223893 -4.0374174 -3.853816 -3.6443422 -3.6676023 -3.861177 -4.0188112 -4.1156516 -4.1713009 -4.2008281 -4.209796][-4.2364883 -4.21289 -4.1849484 -4.1458564 -4.0899496 -3.9776673 -3.7491822 -3.4955907 -3.5665276 -3.8252149 -4.0073729 -4.1117811 -4.1683369 -4.1952615 -4.2027173][-4.2616324 -4.2380896 -4.2058706 -4.1610346 -4.1007271 -3.9956875 -3.8009193 -3.6064758 -3.6850781 -3.9056141 -4.0502491 -4.1279693 -4.1682692 -4.18806 -4.1957269][-4.287261 -4.268115 -4.2437119 -4.2092071 -4.1590414 -4.0761948 -3.9385641 -3.8218131 -3.8835802 -4.0278854 -4.1102514 -4.1513028 -4.1714253 -4.1874819 -4.199605][-4.3040719 -4.2899365 -4.2704358 -4.2403679 -4.1971288 -4.1289511 -4.0326686 -3.9691391 -4.0196667 -4.1121287 -4.1557508 -4.1766648 -4.1875372 -4.2029686 -4.217906][-4.3105264 -4.2960582 -4.2716374 -4.23851 -4.1986709 -4.1448164 -4.0742283 -4.0449615 -4.0966077 -4.1637368 -4.1906867 -4.20472 -4.2102456 -4.2236624 -4.2389903][-4.3052068 -4.2882085 -4.2606549 -4.2266846 -4.1938396 -4.1565456 -4.1110072 -4.1024065 -4.1529427 -4.2040968 -4.220717 -4.227808 -4.2285714 -4.2387486 -4.2521014][-4.2944064 -4.2743139 -4.2495589 -4.2270865 -4.2092791 -4.1905665 -4.1687131 -4.1699481 -4.20242 -4.2336583 -4.2424078 -4.2426138 -4.2406559 -4.2494278 -4.2627082][-4.2980118 -4.2791352 -4.2593641 -4.2471623 -4.2408504 -4.2353029 -4.2271652 -4.2277484 -4.2420511 -4.2581835 -4.2621713 -4.2596326 -4.2589259 -4.2691731 -4.2830858]]...]
INFO - root - 2017-12-05 19:08:31.294868: step 33010, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 72h:49m:56s remains)
INFO - root - 2017-12-05 19:08:40.437442: step 33020, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 76h:34m:19s remains)
INFO - root - 2017-12-05 19:08:49.646357: step 33030, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 77h:48m:34s remains)
INFO - root - 2017-12-05 19:08:58.628695: step 33040, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 76h:06m:15s remains)
INFO - root - 2017-12-05 19:09:07.851182: step 33050, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 79h:11m:01s remains)
INFO - root - 2017-12-05 19:09:16.877410: step 33060, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 74h:12m:16s remains)
INFO - root - 2017-12-05 19:09:25.948648: step 33070, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 75h:05m:19s remains)
INFO - root - 2017-12-05 19:09:35.164906: step 33080, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 75h:28m:37s remains)
INFO - root - 2017-12-05 19:09:44.213947: step 33090, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 75h:57m:44s remains)
INFO - root - 2017-12-05 19:09:53.268418: step 33100, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 74h:26m:22s remains)
2017-12-05 19:09:54.035617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2744832 -4.2507319 -4.236867 -4.2389421 -4.2385573 -4.237884 -4.2334638 -4.22226 -4.2081637 -4.1971412 -4.20013 -4.2221708 -4.2464857 -4.2537761 -4.2413435][-4.2576728 -4.2244644 -4.2067208 -4.2093554 -4.2126689 -4.2162871 -4.2115903 -4.1955771 -4.1810079 -4.1738973 -4.1838293 -4.2126417 -4.240387 -4.2436495 -4.2249036][-4.2247162 -4.184341 -4.1636367 -4.1715422 -4.1846385 -4.1921864 -4.1835871 -4.1636815 -4.1458936 -4.144043 -4.1601787 -4.1896091 -4.2138295 -4.2118864 -4.1905212][-4.192811 -4.1501017 -4.1311336 -4.1435142 -4.1666737 -4.1722078 -4.1568475 -4.1275878 -4.1082129 -4.1210175 -4.1558409 -4.1865973 -4.2020564 -4.1919942 -4.1666784][-4.1785841 -4.1379747 -4.123631 -4.1401482 -4.1678185 -4.1657991 -4.1407619 -4.1 -4.0822091 -4.109921 -4.1599679 -4.1907172 -4.1997914 -4.1849017 -4.1661639][-4.1847258 -4.1425962 -4.1269217 -4.1417794 -4.1684055 -4.1610584 -4.1255283 -4.0780225 -4.0676451 -4.1062965 -4.1560273 -4.1796494 -4.18058 -4.1667495 -4.1591983][-4.1991825 -4.1523166 -4.1309023 -4.1413589 -4.1627035 -4.1520395 -4.1147952 -4.0700293 -4.0658388 -4.1078744 -4.152792 -4.1717706 -4.16869 -4.1546178 -4.1483049][-4.2031484 -4.1576948 -4.1362739 -4.1455674 -4.163825 -4.1542683 -4.119545 -4.08026 -4.0700078 -4.104167 -4.1455979 -4.1656847 -4.1657 -4.1595087 -4.1537652][-4.1993709 -4.1609 -4.1468163 -4.1556253 -4.1753759 -4.1664362 -4.1290917 -4.0875139 -4.0697107 -4.0944271 -4.1323819 -4.155478 -4.1657104 -4.1687551 -4.1648426][-4.1898808 -4.1566114 -4.1463947 -4.1620817 -4.1847868 -4.1797419 -4.147738 -4.105834 -4.085783 -4.1016517 -4.1285591 -4.1535463 -4.1708851 -4.1779637 -4.1699309][-4.1706848 -4.1287336 -4.11674 -4.1414804 -4.1732016 -4.1795688 -4.1580353 -4.1217413 -4.1050339 -4.1178155 -4.1335487 -4.1564932 -4.1736941 -4.1788011 -4.1632991][-4.1507931 -4.0953484 -4.07401 -4.0915475 -4.1244764 -4.144598 -4.1359315 -4.1144061 -4.1146307 -4.1357927 -4.1520662 -4.1695523 -4.1771069 -4.1742697 -4.1496248][-4.1527348 -4.0900426 -4.056231 -4.0503764 -4.0621591 -4.0820928 -4.09277 -4.0954862 -4.1187143 -4.1592689 -4.1799293 -4.1904445 -4.1896296 -4.1786871 -4.1480803][-4.1721029 -4.1208215 -4.0895543 -4.0663204 -4.0438704 -4.04173 -4.062438 -4.0890169 -4.1286221 -4.1756673 -4.1982379 -4.207129 -4.2126622 -4.2048745 -4.1767316][-4.1986022 -4.1676311 -4.1496658 -4.1232591 -4.0841436 -4.0687208 -4.0864444 -4.1201658 -4.16107 -4.1998911 -4.2160373 -4.2208414 -4.2280941 -4.2229939 -4.1966481]]...]
INFO - root - 2017-12-05 19:10:03.144721: step 33110, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 74h:11m:22s remains)
INFO - root - 2017-12-05 19:10:12.229123: step 33120, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 74h:02m:17s remains)
INFO - root - 2017-12-05 19:10:21.317174: step 33130, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 73h:56m:42s remains)
INFO - root - 2017-12-05 19:10:30.398272: step 33140, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 73h:31m:51s remains)
INFO - root - 2017-12-05 19:10:39.494196: step 33150, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 75h:37m:18s remains)
INFO - root - 2017-12-05 19:10:48.678425: step 33160, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 74h:33m:48s remains)
INFO - root - 2017-12-05 19:10:57.625670: step 33170, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 74h:47m:52s remains)
INFO - root - 2017-12-05 19:11:06.817254: step 33180, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 76h:49m:47s remains)
INFO - root - 2017-12-05 19:11:15.828417: step 33190, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.793 sec/batch; 65h:55m:00s remains)
INFO - root - 2017-12-05 19:11:24.800669: step 33200, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 75h:35m:58s remains)
2017-12-05 19:11:25.572905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2600331 -4.2607722 -4.2494822 -4.2224674 -4.1827722 -4.136354 -4.0922589 -4.0656414 -4.0584316 -4.0674496 -4.0904036 -4.1233234 -4.1599026 -4.1684213 -4.174603][-4.2510734 -4.2490158 -4.2381225 -4.2101259 -4.1618361 -4.1007423 -4.0475864 -4.0268559 -4.0337687 -4.0554008 -4.0763245 -4.1054869 -4.1505489 -4.1692038 -4.180295][-4.2363343 -4.2297873 -4.2145729 -4.1884518 -4.1346045 -4.0592422 -4.0018616 -3.9960346 -4.0221262 -4.0514059 -4.066906 -4.0899396 -4.13767 -4.1664786 -4.1849837][-4.2097731 -4.204381 -4.1866488 -4.1633606 -4.1116872 -4.0289016 -3.9658134 -3.9736834 -4.0184646 -4.0546021 -4.0679178 -4.0829029 -4.1255779 -4.1589613 -4.1835155][-4.1772084 -4.1782994 -4.1646595 -4.1427751 -4.0926428 -4.0024691 -3.9247029 -3.9332449 -3.997582 -4.0564222 -4.0750074 -4.0826125 -4.1155357 -4.1488404 -4.1815319][-4.1508012 -4.15796 -4.151597 -4.1270447 -4.0700297 -3.9656088 -3.8536844 -3.8393662 -3.9320602 -4.0296116 -4.0668259 -4.0786133 -4.1087928 -4.1454 -4.1884708][-4.1096935 -4.1269536 -4.1311173 -4.1046352 -4.037271 -3.9150803 -3.7603946 -3.7119532 -3.8378499 -3.9767263 -4.0415516 -4.0703526 -4.10713 -4.1473427 -4.1980219][-4.045002 -4.0716271 -4.0909791 -4.0709305 -4.0082068 -3.8940341 -3.7449272 -3.6929162 -3.8192358 -3.9649208 -4.0441852 -4.0840011 -4.1218081 -4.1560683 -4.2023249][-4.0278325 -4.059833 -4.0855017 -4.0779333 -4.0346837 -3.9613953 -3.8701208 -3.8437457 -3.9202337 -4.0169683 -4.0697331 -4.0972247 -4.1274872 -4.1534729 -4.191937][-4.0895367 -4.1173773 -4.129797 -4.1157937 -4.0847483 -4.0430579 -3.9976974 -3.9886935 -4.0260234 -4.07847 -4.1053839 -4.1200733 -4.1411452 -4.1597204 -4.1885848][-4.1454859 -4.165935 -4.1692653 -4.1540833 -4.1307817 -4.1058779 -4.0827093 -4.0818353 -4.103652 -4.1331968 -4.1458383 -4.1564469 -4.1742339 -4.190135 -4.2107825][-4.1850762 -4.1957393 -4.1967459 -4.1864858 -4.1702452 -4.1551 -4.1422248 -4.1465831 -4.1636786 -4.1831679 -4.1917105 -4.193356 -4.2030849 -4.2192736 -4.2387948][-4.2181292 -4.2235489 -4.2233181 -4.2154827 -4.1994257 -4.1867027 -4.1792574 -4.1843181 -4.1981196 -4.2115865 -4.2194724 -4.2138219 -4.2197666 -4.2382488 -4.2575741][-4.2407608 -4.2421708 -4.2408137 -4.2357411 -4.2255707 -4.2173944 -4.2119288 -4.2141175 -4.22278 -4.2296953 -4.2324595 -4.2278996 -4.2364349 -4.2567134 -4.2753119][-4.2569852 -4.2571311 -4.2582903 -4.25912 -4.2585669 -4.257215 -4.2568607 -4.2574306 -4.2575593 -4.2565174 -4.25321 -4.249855 -4.2576785 -4.2741256 -4.2901716]]...]
INFO - root - 2017-12-05 19:11:34.808437: step 33210, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 75h:41m:16s remains)
INFO - root - 2017-12-05 19:11:43.999398: step 33220, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 76h:05m:02s remains)
INFO - root - 2017-12-05 19:11:53.241479: step 33230, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 79h:13m:19s remains)
INFO - root - 2017-12-05 19:12:02.285859: step 33240, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 75h:34m:03s remains)
INFO - root - 2017-12-05 19:12:11.522652: step 33250, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 77h:17m:47s remains)
INFO - root - 2017-12-05 19:12:20.607988: step 33260, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 76h:07m:46s remains)
INFO - root - 2017-12-05 19:12:29.729823: step 33270, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 76h:38m:34s remains)
INFO - root - 2017-12-05 19:12:38.788062: step 33280, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:59m:42s remains)
INFO - root - 2017-12-05 19:12:48.079204: step 33290, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 79h:54m:57s remains)
INFO - root - 2017-12-05 19:12:57.144614: step 33300, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 76h:04m:19s remains)
2017-12-05 19:12:57.947645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2458606 -4.2568684 -4.2643352 -4.2593789 -4.23476 -4.1970482 -4.1500916 -4.0980096 -4.0725174 -4.1008391 -4.1692472 -4.2417593 -4.301569 -4.3357482 -4.348772][-4.2468548 -4.2605252 -4.2701669 -4.2640958 -4.2349358 -4.1891947 -4.1308188 -4.0632682 -4.0287442 -4.0663357 -4.1541071 -4.2402196 -4.3066964 -4.3415656 -4.3528166][-4.2439642 -4.2618604 -4.27491 -4.2693796 -4.2364697 -4.1805134 -4.1060619 -4.02464 -3.9868956 -4.035224 -4.1404424 -4.2396064 -4.3108549 -4.3459983 -4.35583][-4.2382932 -4.2613192 -4.2764573 -4.2682929 -4.2280931 -4.158308 -4.0670037 -3.9782195 -3.9461725 -4.0083733 -4.1299548 -4.2404866 -4.3145766 -4.3488474 -4.3577089][-4.2375245 -4.2626395 -4.2733774 -4.258132 -4.2087069 -4.1247578 -4.0204167 -3.9351285 -3.9193785 -3.9986389 -4.1338797 -4.2523255 -4.3247838 -4.3541989 -4.360086][-4.2405906 -4.2614412 -4.263628 -4.2400026 -4.1815195 -4.0840311 -3.9709983 -3.9018717 -3.9126933 -4.0082064 -4.1492457 -4.2690167 -4.3378072 -4.35995 -4.3612981][-4.2406964 -4.25366 -4.2453456 -4.2114568 -4.1405768 -4.0255117 -3.9069576 -3.8735292 -3.9220152 -4.0300865 -4.1677008 -4.2817345 -4.3444214 -4.360846 -4.3596468][-4.2390013 -4.2449589 -4.2290692 -4.18755 -4.1070037 -3.9787827 -3.8602586 -3.8620849 -3.9396219 -4.0514784 -4.1787553 -4.2819238 -4.3393521 -4.3563123 -4.3570867][-4.2476158 -4.2522907 -4.235486 -4.1939449 -4.1161537 -3.9938684 -3.8872993 -3.8986392 -3.9738264 -4.0727 -4.1835175 -4.275229 -4.3301873 -4.3508849 -4.3552942][-4.2539678 -4.2587695 -4.2452006 -4.208416 -4.1399131 -4.0336342 -3.9485683 -3.9604571 -4.0193825 -4.1003728 -4.1925745 -4.2717171 -4.3246589 -4.3473158 -4.3543005][-4.2570381 -4.2595854 -4.2501721 -4.2207136 -4.1634154 -4.078145 -4.0152698 -4.0250978 -4.0711331 -4.1367335 -4.2116604 -4.2777882 -4.3257484 -4.3469949 -4.3540039][-4.2605162 -4.2604704 -4.2545924 -4.2307076 -4.1828504 -4.1167126 -4.0738692 -4.0818448 -4.11749 -4.173562 -4.2355418 -4.2911277 -4.3333111 -4.3507934 -4.3551478][-4.2690382 -4.26401 -4.256362 -4.2354593 -4.1971173 -4.1453133 -4.1158714 -4.1225815 -4.1512856 -4.200613 -4.255497 -4.305438 -4.3410745 -4.3553395 -4.3566][-4.2724204 -4.265831 -4.2569227 -4.236752 -4.2040744 -4.1629405 -4.1411409 -4.146822 -4.1709108 -4.2137623 -4.2648735 -4.3123555 -4.3435612 -4.3566661 -4.3567557][-4.266902 -4.2593083 -4.2476277 -4.2274384 -4.1989861 -4.16685 -4.1481886 -4.1525216 -4.1765261 -4.2186608 -4.2696228 -4.3150167 -4.3436446 -4.3567157 -4.3564243]]...]
INFO - root - 2017-12-05 19:13:07.103678: step 33310, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 77h:19m:54s remains)
INFO - root - 2017-12-05 19:13:16.230814: step 33320, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 74h:05m:45s remains)
INFO - root - 2017-12-05 19:13:25.381825: step 33330, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 76h:28m:59s remains)
INFO - root - 2017-12-05 19:13:34.537612: step 33340, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 77h:14m:02s remains)
INFO - root - 2017-12-05 19:13:43.648721: step 33350, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 72h:51m:27s remains)
INFO - root - 2017-12-05 19:13:52.715022: step 33360, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 73h:10m:19s remains)
INFO - root - 2017-12-05 19:14:01.997143: step 33370, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 76h:27m:31s remains)
INFO - root - 2017-12-05 19:14:11.199895: step 33380, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 75h:32m:19s remains)
INFO - root - 2017-12-05 19:14:20.449306: step 33390, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 77h:22m:01s remains)
INFO - root - 2017-12-05 19:14:29.671682: step 33400, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 75h:11m:32s remains)
2017-12-05 19:14:30.447495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2292747 -4.2146339 -4.1946745 -4.1814933 -4.1807551 -4.193933 -4.2145581 -4.2258525 -4.2275386 -4.2262387 -4.2316375 -4.2448263 -4.25948 -4.2704511 -4.2757387][-4.2056069 -4.1942339 -4.1761651 -4.1612997 -4.15516 -4.1608539 -4.1748958 -4.1823778 -4.1838746 -4.1856217 -4.19471 -4.2103405 -4.2267494 -4.239223 -4.2468615][-4.1899509 -4.1814413 -4.1696811 -4.1590786 -4.1494894 -4.1441889 -4.1429491 -4.1398468 -4.1394038 -4.1445322 -4.1575274 -4.1745367 -4.1909628 -4.2047372 -4.2165952][-4.1814942 -4.1759171 -4.1714163 -4.1668224 -4.1574316 -4.1415391 -4.1231947 -4.1094093 -4.1113496 -4.12245 -4.1365409 -4.1501913 -4.1620069 -4.1736588 -4.1897559][-4.1637259 -4.159668 -4.1633735 -4.16625 -4.1596861 -4.1354351 -4.1012588 -4.0775313 -4.0850964 -4.1073327 -4.1284332 -4.1418009 -4.149056 -4.1588397 -4.1766253][-4.1448359 -4.139812 -4.1446018 -4.1479545 -4.1396389 -4.1063719 -4.0537152 -4.020422 -4.039135 -4.0804653 -4.1160688 -4.138464 -4.1495256 -4.162272 -4.1826606][-4.1250958 -4.1205993 -4.1211185 -4.1184025 -4.1018462 -4.0530429 -3.976305 -3.9282763 -3.9640746 -4.0362406 -4.0998678 -4.1390214 -4.1589017 -4.1755781 -4.1990762][-4.1078715 -4.10878 -4.1126041 -4.10695 -4.0832725 -4.0233574 -3.9301419 -3.8663831 -3.9096479 -4.0070825 -4.0948725 -4.147222 -4.1710653 -4.1840625 -4.204361][-4.0945115 -4.0976586 -4.107739 -4.1086679 -4.09456 -4.0513206 -3.9819074 -3.9295304 -3.9541965 -4.0309 -4.1098 -4.1598577 -4.1798129 -4.1836214 -4.1958981][-4.0848074 -4.0829291 -4.0962777 -4.1097922 -4.1126652 -4.0916414 -4.0532365 -4.0220647 -4.0321579 -4.0765371 -4.1289754 -4.1653113 -4.1801691 -4.1764855 -4.1790547][-4.0810485 -4.065361 -4.0782766 -4.10454 -4.12376 -4.1167746 -4.0946817 -4.0798645 -4.0860596 -4.1118169 -4.1446338 -4.1704988 -4.180491 -4.1723409 -4.1691051][-4.07707 -4.0508332 -4.0614796 -4.0975847 -4.1296549 -4.1300535 -4.111712 -4.1017327 -4.1096959 -4.1316242 -4.1582785 -4.180512 -4.1865864 -4.1760721 -4.1710339][-4.07998 -4.0561094 -4.0633574 -4.0990958 -4.134573 -4.1416383 -4.1261334 -4.1171966 -4.1280861 -4.1491513 -4.1733685 -4.1936 -4.1992369 -4.1910896 -4.1869092][-4.1009054 -4.0891266 -4.0937595 -4.120831 -4.1540728 -4.167275 -4.156714 -4.1492805 -4.1599679 -4.1801057 -4.2017674 -4.2186532 -4.22314 -4.2168717 -4.2138495][-4.1283054 -4.1297407 -4.1368971 -4.1570129 -4.185729 -4.2051549 -4.2038569 -4.200211 -4.2079735 -4.2244668 -4.24258 -4.2560587 -4.2582846 -4.2532749 -4.2493191]]...]
INFO - root - 2017-12-05 19:14:39.456162: step 33410, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 71h:12m:43s remains)
INFO - root - 2017-12-05 19:14:48.688183: step 33420, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 78h:15m:57s remains)
INFO - root - 2017-12-05 19:14:57.794656: step 33430, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 73h:40m:13s remains)
INFO - root - 2017-12-05 19:15:07.084217: step 33440, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.962 sec/batch; 79h:54m:56s remains)
INFO - root - 2017-12-05 19:15:16.132990: step 33450, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 76h:01m:13s remains)
INFO - root - 2017-12-05 19:15:25.251957: step 33460, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 74h:00m:23s remains)
INFO - root - 2017-12-05 19:15:34.447208: step 33470, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 76h:10m:23s remains)
INFO - root - 2017-12-05 19:15:43.448073: step 33480, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 69h:36m:49s remains)
INFO - root - 2017-12-05 19:15:52.568981: step 33490, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 73h:07m:52s remains)
INFO - root - 2017-12-05 19:16:01.729706: step 33500, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 74h:33m:59s remains)
2017-12-05 19:16:02.508502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3044953 -4.28784 -4.2675886 -4.2439704 -4.2228794 -4.2039714 -4.1877065 -4.1862192 -4.2093387 -4.2395682 -4.2552223 -4.25709 -4.2544861 -4.2492976 -4.2384796][-4.3075366 -4.2894573 -4.2634149 -4.2319622 -4.2023792 -4.1752596 -4.1496134 -4.1391358 -4.1604366 -4.1967106 -4.2252836 -4.2388029 -4.2419987 -4.2430148 -4.2379174][-4.3094797 -4.2914443 -4.2624187 -4.2267928 -4.1899166 -4.1523628 -4.1125159 -4.0887623 -4.1071 -4.1509342 -4.193676 -4.2173691 -4.2248535 -4.2306929 -4.2297473][-4.3100033 -4.2933569 -4.2639184 -4.2269478 -4.1853085 -4.1381903 -4.0869727 -4.0535145 -4.0747471 -4.1276932 -4.1778564 -4.2028866 -4.2064118 -4.2100158 -4.2110896][-4.3085661 -4.293087 -4.2647691 -4.2269154 -4.1803308 -4.12579 -4.0674624 -4.0290051 -4.0560789 -4.1186271 -4.1733446 -4.196981 -4.1947861 -4.194284 -4.1957483][-4.3051572 -4.2889671 -4.25913 -4.216115 -4.1628752 -4.1032143 -4.0429363 -4.0030918 -4.0329328 -4.1039295 -4.16655 -4.1952057 -4.1954308 -4.1941657 -4.1945424][-4.3004 -4.2817283 -4.24672 -4.1944251 -4.1338887 -4.0729194 -4.0171585 -3.9815705 -4.0136433 -4.0892811 -4.1621003 -4.2013664 -4.2093396 -4.2102652 -4.2122579][-4.2958894 -4.276051 -4.2377605 -4.174715 -4.1076503 -4.0514593 -4.0101519 -3.9918613 -4.032939 -4.1086712 -4.1820836 -4.2242551 -4.235374 -4.2382655 -4.241354][-4.291585 -4.2706056 -4.2303209 -4.1654367 -4.1008782 -4.0549536 -4.0329089 -4.0378451 -4.0895491 -4.1593947 -4.2210131 -4.257257 -4.2703514 -4.2729497 -4.272017][-4.2868285 -4.2637086 -4.223949 -4.1656723 -4.1098394 -4.0735521 -4.0641069 -4.0876694 -4.1498251 -4.2146335 -4.2618995 -4.2886205 -4.3010316 -4.3022747 -4.2957578][-4.2809811 -4.2554197 -4.2187285 -4.1707358 -4.1285195 -4.1050034 -4.1039495 -4.1352906 -4.1996 -4.2597995 -4.2957067 -4.3102012 -4.3161025 -4.3146324 -4.3057704][-4.2784634 -4.2525444 -4.2215576 -4.1849632 -4.1567125 -4.1465716 -4.1536021 -4.1863861 -4.245224 -4.295383 -4.3183055 -4.3202343 -4.3175 -4.3126307 -4.3034267][-4.2784777 -4.2559867 -4.2345958 -4.2105389 -4.1934261 -4.1921339 -4.2038293 -4.2329116 -4.2801571 -4.3168764 -4.3265481 -4.3196898 -4.3106427 -4.301785 -4.2911687][-4.2800908 -4.2616172 -4.2487569 -4.2364841 -4.2313485 -4.2360969 -4.2462053 -4.2667828 -4.3003221 -4.3216863 -4.3197279 -4.3079948 -4.2953243 -4.2828951 -4.2690144][-4.2837358 -4.266531 -4.2571568 -4.25396 -4.2582374 -4.2672081 -4.275641 -4.28779 -4.3079648 -4.3152027 -4.3054442 -4.2914157 -4.277709 -4.263186 -4.2446613]]...]
INFO - root - 2017-12-05 19:16:11.470548: step 33510, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 75h:04m:11s remains)
INFO - root - 2017-12-05 19:16:20.626138: step 33520, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.888 sec/batch; 73h:45m:49s remains)
INFO - root - 2017-12-05 19:16:29.750756: step 33530, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 76h:34m:54s remains)
INFO - root - 2017-12-05 19:16:38.761922: step 33540, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 75h:25m:12s remains)
INFO - root - 2017-12-05 19:16:47.928123: step 33550, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 75h:24m:46s remains)
INFO - root - 2017-12-05 19:16:56.940177: step 33560, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 76h:15m:47s remains)
INFO - root - 2017-12-05 19:17:06.029480: step 33570, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 74h:46m:35s remains)
INFO - root - 2017-12-05 19:17:14.931495: step 33580, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 74h:45m:06s remains)
INFO - root - 2017-12-05 19:17:24.067573: step 33590, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 78h:43m:26s remains)
INFO - root - 2017-12-05 19:17:33.255620: step 33600, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 77h:28m:02s remains)
2017-12-05 19:17:33.998678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.289434 -4.2628474 -4.2278118 -4.1961164 -4.1751375 -4.1633997 -4.1571259 -4.1576552 -4.1694489 -4.1805305 -4.1839466 -4.1686878 -4.1543975 -4.1503778 -4.158216][-4.2464037 -4.2134924 -4.1751504 -4.138566 -4.1078768 -4.0909462 -4.0860658 -4.0909085 -4.1077361 -4.1288147 -4.1492133 -4.1534343 -4.1487722 -4.140254 -4.1352892][-4.2102275 -4.1793914 -4.1480188 -4.1122851 -4.0783005 -4.0560136 -4.0428104 -4.0384588 -4.0502272 -4.0701218 -4.0935431 -4.1134973 -4.1255121 -4.1256933 -4.12218][-4.2066212 -4.1828737 -4.1622581 -4.135901 -4.1095433 -4.0923891 -4.0777736 -4.0650196 -4.0581565 -4.0531917 -4.05361 -4.0690928 -4.0892763 -4.1009889 -4.11211][-4.227169 -4.2039471 -4.1823397 -4.165071 -4.158112 -4.1613431 -4.1559153 -4.1410708 -4.1217337 -4.0907989 -4.0587759 -4.0560584 -4.07339 -4.0939636 -4.1202116][-4.2505364 -4.2229848 -4.187644 -4.1612191 -4.1596208 -4.1692443 -4.1724362 -4.16779 -4.1555338 -4.1305 -4.0946198 -4.0772104 -4.0814919 -4.0971961 -4.1257138][-4.2591171 -4.2279654 -4.1798058 -4.1376824 -4.1201582 -4.1157169 -4.1164756 -4.1219754 -4.1322651 -4.1317024 -4.1137524 -4.0982304 -4.0930467 -4.100738 -4.127995][-4.2593813 -4.2245779 -4.1732769 -4.1233091 -4.088963 -4.0582561 -4.0289955 -4.014451 -4.0329847 -4.0610156 -4.0830183 -4.0957265 -4.1008492 -4.1103096 -4.1336026][-4.2380161 -4.2091622 -4.1668057 -4.1225958 -4.0869293 -4.0401325 -3.9731493 -3.9097149 -3.9075189 -3.9557171 -4.019783 -4.0736375 -4.10177 -4.1171227 -4.1346979][-4.199492 -4.1830196 -4.1619263 -4.1337 -4.1128054 -4.0757408 -4.0092521 -3.9321384 -3.9120526 -3.9482405 -4.0079136 -4.0640326 -4.0922284 -4.1076379 -4.1213732][-4.1712337 -4.1666889 -4.1655135 -4.1562943 -4.1521349 -4.1413193 -4.110393 -4.0635943 -4.0476022 -4.0631256 -4.092483 -4.1203966 -4.1275811 -4.1273293 -4.1300383][-4.1548448 -4.15165 -4.1581 -4.1621609 -4.1711435 -4.1799483 -4.176445 -4.154295 -4.1487274 -4.1618514 -4.1837511 -4.2008419 -4.1991687 -4.1885362 -4.1780949][-4.1431923 -4.14001 -4.1504297 -4.1608934 -4.1725569 -4.1860442 -4.1900163 -4.1799278 -4.1864572 -4.2064018 -4.228241 -4.2427192 -4.2449517 -4.2386265 -4.2279286][-4.137538 -4.1362143 -4.146174 -4.1573539 -4.1680365 -4.17866 -4.1845093 -4.1866932 -4.2056665 -4.2311716 -4.2487164 -4.2612925 -4.2693524 -4.2719007 -4.2685819][-4.1507421 -4.146698 -4.1514192 -4.1594443 -4.1699467 -4.1822271 -4.1931829 -4.2038589 -4.2244911 -4.2476707 -4.2658443 -4.2808542 -4.2928243 -4.2992916 -4.2997956]]...]
INFO - root - 2017-12-05 19:17:43.097173: step 33610, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 75h:10m:29s remains)
INFO - root - 2017-12-05 19:17:52.201275: step 33620, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.931 sec/batch; 77h:17m:32s remains)
INFO - root - 2017-12-05 19:18:01.266842: step 33630, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 69h:40m:04s remains)
INFO - root - 2017-12-05 19:18:10.383118: step 33640, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 76h:53m:35s remains)
INFO - root - 2017-12-05 19:18:19.402352: step 33650, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 74h:51m:40s remains)
INFO - root - 2017-12-05 19:18:28.442178: step 33660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 73h:10m:51s remains)
INFO - root - 2017-12-05 19:18:37.421098: step 33670, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 76h:44m:36s remains)
INFO - root - 2017-12-05 19:18:46.400907: step 33680, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 75h:48m:44s remains)
INFO - root - 2017-12-05 19:18:55.437731: step 33690, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 77h:27m:23s remains)
INFO - root - 2017-12-05 19:19:04.565472: step 33700, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 74h:34m:52s remains)
2017-12-05 19:19:05.358851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2240729 -4.23981 -4.2638245 -4.2872834 -4.2897725 -4.2780595 -4.2706351 -4.2736182 -4.2669988 -4.2528257 -4.2512717 -4.2655773 -4.2872057 -4.30878 -4.32663][-4.1574984 -4.1796703 -4.21341 -4.2480226 -4.2540312 -4.2392 -4.2294769 -4.2311597 -4.2244868 -4.2088256 -4.2121425 -4.2326937 -4.2597804 -4.28855 -4.3151808][-4.0895238 -4.1221132 -4.169795 -4.2113914 -4.2140951 -4.1972947 -4.1855927 -4.1846685 -4.1769447 -4.1629429 -4.1722732 -4.1985593 -4.2288709 -4.2656155 -4.301106][-4.0432105 -4.0848427 -4.1459794 -4.1901188 -4.1881819 -4.1671419 -4.1513481 -4.1443892 -4.1330767 -4.1208057 -4.1373296 -4.1728325 -4.2107544 -4.2534308 -4.2943711][-4.0508189 -4.0885811 -4.1454577 -4.177588 -4.1588082 -4.1246319 -4.0970526 -4.0830145 -4.0726995 -4.0694418 -4.0976186 -4.1466088 -4.193161 -4.2432842 -4.2860389][-4.0920177 -4.1169338 -4.1545887 -4.1628547 -4.1205525 -4.063704 -4.0171175 -3.9997978 -4.0017481 -4.007864 -4.0432882 -4.1011457 -4.1570158 -4.2230873 -4.2735386][-4.1430712 -4.15415 -4.1683507 -4.1497574 -4.0845532 -4.0007639 -3.9344988 -3.9150283 -3.9247048 -3.936681 -3.9821925 -4.0489793 -4.1080389 -4.18501 -4.250309][-4.1936579 -4.1921716 -4.1874762 -4.1488371 -4.0725555 -3.9772484 -3.9032261 -3.8732736 -3.8646991 -3.8615236 -3.9177527 -3.9976332 -4.05697 -4.1360116 -4.2135892][-4.2381091 -4.2267509 -4.210938 -4.1748209 -4.1083441 -4.026516 -3.9665966 -3.9353628 -3.9104106 -3.8889828 -3.9326897 -4.002079 -4.051507 -4.117023 -4.1906815][-4.2684908 -4.255652 -4.24025 -4.2141604 -4.166635 -4.1101174 -4.0728793 -4.0480146 -4.0215683 -3.9917059 -4.0098529 -4.0541339 -4.0837822 -4.1335325 -4.1982131][-4.2899184 -4.2819376 -4.2741151 -4.2608547 -4.2326884 -4.1983356 -4.175334 -4.1548848 -4.1298985 -4.0989642 -4.0968976 -4.1171312 -4.1339374 -4.177588 -4.2341256][-4.3046474 -4.3032241 -4.3038025 -4.3006897 -4.2868185 -4.2701125 -4.2583146 -4.2437119 -4.2237659 -4.1938353 -4.1794071 -4.1849284 -4.196589 -4.2350459 -4.2818527][-4.3119354 -4.3131948 -4.3190489 -4.3228498 -4.3169422 -4.3099566 -4.3067741 -4.3011603 -4.2911348 -4.2688875 -4.2496243 -4.2442551 -4.2525949 -4.281898 -4.3160782][-4.3166714 -4.3171129 -4.321836 -4.3271155 -4.3271341 -4.3257666 -4.3289804 -4.3305631 -4.32634 -4.312459 -4.294857 -4.2864385 -4.291738 -4.3111506 -4.3345294][-4.3209696 -4.3199277 -4.3217854 -4.3244157 -4.3255086 -4.3268013 -4.332407 -4.3365383 -4.3356552 -4.328434 -4.3171034 -4.3114414 -4.315218 -4.3277097 -4.3416433]]...]
INFO - root - 2017-12-05 19:19:14.411908: step 33710, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 76h:19m:58s remains)
INFO - root - 2017-12-05 19:19:23.560673: step 33720, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 76h:38m:11s remains)
INFO - root - 2017-12-05 19:19:32.477433: step 33730, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 73h:48m:42s remains)
INFO - root - 2017-12-05 19:19:41.726979: step 33740, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 77h:35m:48s remains)
INFO - root - 2017-12-05 19:19:50.921305: step 33750, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 76h:07m:46s remains)
INFO - root - 2017-12-05 19:20:00.009661: step 33760, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 77h:06m:55s remains)
INFO - root - 2017-12-05 19:20:09.010605: step 33770, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 75h:08m:01s remains)
INFO - root - 2017-12-05 19:20:18.019670: step 33780, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:50m:40s remains)
INFO - root - 2017-12-05 19:20:27.217344: step 33790, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 76h:40m:38s remains)
INFO - root - 2017-12-05 19:20:36.447820: step 33800, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 76h:38m:26s remains)
2017-12-05 19:20:37.271470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2172103 -4.2298422 -4.2136788 -4.1770926 -4.1133814 -4.0266137 -3.9487612 -3.9586575 -4.0291152 -4.0981894 -4.1520967 -4.2030883 -4.2526274 -4.2673492 -4.2639337][-4.2369156 -4.2467022 -4.2270226 -4.1906204 -4.1354942 -4.052742 -3.9620471 -3.9559414 -4.0130215 -4.0809197 -4.1276426 -4.1778111 -4.2356925 -4.2630239 -4.2657385][-4.2534227 -4.2583165 -4.2351241 -4.1972985 -4.14703 -4.0690794 -3.9779975 -3.9578018 -4.0071778 -4.0763903 -4.1221743 -4.1692019 -4.2266788 -4.2597885 -4.2663875][-4.2591262 -4.2596378 -4.2360654 -4.1965694 -4.1440496 -4.059288 -3.9628339 -3.92534 -3.9791603 -4.0647836 -4.1192136 -4.1682291 -4.224124 -4.2579579 -4.266633][-4.2640786 -4.2554417 -4.2260895 -4.1797919 -4.11702 -4.0204778 -3.9056673 -3.8503592 -3.9172029 -4.0259066 -4.0998731 -4.1603971 -4.2220826 -4.2581015 -4.2682853][-4.2690835 -4.2515283 -4.2139535 -4.1577163 -4.083971 -3.9787776 -3.8562274 -3.801723 -3.8822577 -3.9980056 -4.0843244 -4.1554832 -4.2198238 -4.2588458 -4.2700162][-4.2731075 -4.251605 -4.2085543 -4.1437941 -4.0632768 -3.9578943 -3.8473766 -3.8139377 -3.8967981 -3.996819 -4.0809231 -4.1546926 -4.2186823 -4.2602992 -4.2734957][-4.2757473 -4.251853 -4.2109056 -4.1472516 -4.0638876 -3.9616389 -3.8593853 -3.8320332 -3.9017072 -3.9862509 -4.0688705 -4.1477036 -4.2160783 -4.2607665 -4.2767577][-4.2797341 -4.2569418 -4.2254992 -4.1726913 -4.0950708 -3.9944127 -3.8827343 -3.8454442 -3.9039834 -3.987179 -4.069438 -4.1507297 -4.2213988 -4.2652078 -4.2799811][-4.2851686 -4.2645802 -4.2395186 -4.19723 -4.1298442 -4.0309572 -3.9114437 -3.8665721 -3.920938 -4.0184608 -4.1055417 -4.1802583 -4.242559 -4.2766795 -4.2850037][-4.2923336 -4.2744241 -4.250361 -4.2149 -4.1594763 -4.0716558 -3.9639704 -3.9151187 -3.9506004 -4.0428386 -4.1306424 -4.2009387 -4.2559643 -4.279376 -4.2797346][-4.2984657 -4.2836194 -4.2609692 -4.2322869 -4.1895318 -4.1170459 -4.0292497 -3.9751151 -3.9861383 -4.0612316 -4.1451421 -4.2114162 -4.2572103 -4.2721825 -4.2669158][-4.3009953 -4.2848954 -4.2662678 -4.2478466 -4.2197623 -4.1615639 -4.0939827 -4.0443211 -4.0430851 -4.0998449 -4.17054 -4.2287173 -4.2633953 -4.2690587 -4.2614222][-4.2984967 -4.2800179 -4.2670517 -4.2546082 -4.234787 -4.1894593 -4.1393862 -4.1040435 -4.1028543 -4.1445217 -4.1982265 -4.2439346 -4.2685595 -4.2664347 -4.2564974][-4.2915173 -4.2688766 -4.2568254 -4.2427821 -4.2239928 -4.19194 -4.1625481 -4.147347 -4.1551256 -4.1877184 -4.2267518 -4.2605829 -4.2768774 -4.2721839 -4.2613053]]...]
INFO - root - 2017-12-05 19:20:46.356330: step 33810, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 75h:54m:47s remains)
INFO - root - 2017-12-05 19:20:55.317151: step 33820, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 73h:05m:27s remains)
INFO - root - 2017-12-05 19:21:04.399040: step 33830, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 74h:23m:00s remains)
INFO - root - 2017-12-05 19:21:13.366354: step 33840, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.925 sec/batch; 76h:42m:50s remains)
INFO - root - 2017-12-05 19:21:22.479714: step 33850, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 78h:24m:20s remains)
INFO - root - 2017-12-05 19:21:31.731812: step 33860, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 76h:42m:35s remains)
INFO - root - 2017-12-05 19:21:40.999970: step 33870, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 77h:11m:27s remains)
INFO - root - 2017-12-05 19:21:50.047339: step 33880, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 77h:04m:42s remains)
INFO - root - 2017-12-05 19:21:59.078733: step 33890, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 77h:30m:35s remains)
INFO - root - 2017-12-05 19:22:08.146267: step 33900, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 75h:26m:18s remains)
2017-12-05 19:22:08.948959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2885222 -4.2874231 -4.2955379 -4.3051763 -4.3111448 -4.3131208 -4.315444 -4.3174939 -4.3180518 -4.3150754 -4.3113232 -4.3114419 -4.3163581 -4.3214865 -4.3247652][-4.2654395 -4.2632341 -4.2764616 -4.2886457 -4.293335 -4.2928472 -4.29441 -4.2968497 -4.2973704 -4.2937551 -4.29093 -4.2941508 -4.304266 -4.3151326 -4.3229485][-4.2536783 -4.2502046 -4.2634153 -4.2722859 -4.2730074 -4.2673817 -4.2651682 -4.2645135 -4.2629833 -4.2589931 -4.2586565 -4.2666187 -4.2823324 -4.2992187 -4.311656][-4.2500763 -4.2426152 -4.2485662 -4.2502766 -4.2439365 -4.2335238 -4.2290187 -4.2257447 -4.2237763 -4.2214332 -4.2213926 -4.2316403 -4.2513061 -4.2727075 -4.2901978][-4.2384424 -4.2234058 -4.2215171 -4.2167244 -4.2015285 -4.1901479 -4.189744 -4.19187 -4.1952367 -4.1945763 -4.1901217 -4.1980486 -4.2206883 -4.24628 -4.2700386][-4.214426 -4.1855283 -4.1734357 -4.16658 -4.1485429 -4.1365895 -4.1427989 -4.1512671 -4.160346 -4.1650457 -4.162684 -4.1709304 -4.1950741 -4.2207685 -4.247879][-4.1893148 -4.1482544 -4.1294336 -4.1190033 -4.0979362 -4.0870495 -4.098618 -4.1107578 -4.1234803 -4.1350327 -4.1425467 -4.1559186 -4.1833258 -4.2101502 -4.24026][-4.1828794 -4.1430068 -4.1209931 -4.1057954 -4.084065 -4.073689 -4.0823741 -4.0914984 -4.1035137 -4.121244 -4.1325636 -4.1465821 -4.1737843 -4.2035112 -4.2370086][-4.1899042 -4.1588469 -4.1385975 -4.1246629 -4.10305 -4.086997 -4.0839553 -4.0869646 -4.0990953 -4.1219511 -4.1344366 -4.1444731 -4.1715355 -4.2060466 -4.2411542][-4.2024078 -4.1764717 -4.1549439 -4.1363311 -4.107626 -4.0835185 -4.0720515 -4.0716491 -4.0874267 -4.1153831 -4.1242261 -4.1259084 -4.156178 -4.1998243 -4.2395091][-4.2049508 -4.1826119 -4.1592407 -4.1354709 -4.0994778 -4.0692163 -4.050355 -4.042161 -4.0523825 -4.07702 -4.0824165 -4.0872583 -4.1258006 -4.1768413 -4.2215319][-4.2009516 -4.18191 -4.1618004 -4.1403604 -4.105535 -4.075304 -4.0519309 -4.0379043 -4.0410867 -4.0574474 -4.059113 -4.0663643 -4.1080246 -4.161263 -4.2039218][-4.1914015 -4.1801763 -4.1727257 -4.1673093 -4.1470394 -4.1239462 -4.0977054 -4.0811853 -4.0793204 -4.0880218 -4.0873408 -4.08964 -4.1217508 -4.1637225 -4.1983805][-4.1634893 -4.15734 -4.1681781 -4.1838841 -4.1795359 -4.1655922 -4.1421423 -4.1279926 -4.1223893 -4.1231918 -4.1188025 -4.1185246 -4.1417184 -4.1724825 -4.2013793][-4.1347618 -4.1292434 -4.154736 -4.1858892 -4.1982541 -4.1971231 -4.1847749 -4.177258 -4.1712346 -4.1667271 -4.1587973 -4.1558833 -4.1695638 -4.1914454 -4.215683]]...]
INFO - root - 2017-12-05 19:22:17.861555: step 33910, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.814 sec/batch; 67h:32m:17s remains)
INFO - root - 2017-12-05 19:22:26.878200: step 33920, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 72h:53m:44s remains)
INFO - root - 2017-12-05 19:22:35.944581: step 33930, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 73h:22m:19s remains)
INFO - root - 2017-12-05 19:22:45.010582: step 33940, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 75h:12m:23s remains)
INFO - root - 2017-12-05 19:22:53.995935: step 33950, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 69h:08m:46s remains)
INFO - root - 2017-12-05 19:23:03.185587: step 33960, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 75h:55m:42s remains)
INFO - root - 2017-12-05 19:23:12.209375: step 33970, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 73h:09m:11s remains)
INFO - root - 2017-12-05 19:23:21.206170: step 33980, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.884 sec/batch; 73h:20m:34s remains)
INFO - root - 2017-12-05 19:23:30.398786: step 33990, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.940 sec/batch; 77h:56m:51s remains)
INFO - root - 2017-12-05 19:23:39.493274: step 34000, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 78h:31m:23s remains)
2017-12-05 19:23:40.331846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2366028 -4.2254853 -4.2244554 -4.2259917 -4.2294517 -4.2308211 -4.2261534 -4.2224627 -4.2198181 -4.2180033 -4.2165551 -4.2083883 -4.1953197 -4.1885304 -4.1936159][-4.2112641 -4.1948 -4.1916642 -4.191947 -4.196734 -4.2002306 -4.1936655 -4.1909204 -4.1933489 -4.1967 -4.1974897 -4.1853476 -4.1670976 -4.1577091 -4.1623688][-4.1917543 -4.1689472 -4.1601982 -4.1542778 -4.1566 -4.1596 -4.1517234 -4.1529412 -4.1655588 -4.1801248 -4.1864123 -4.17358 -4.1544304 -4.1460686 -4.149724][-4.1809535 -4.152842 -4.1370807 -4.1222286 -4.1160278 -4.1104727 -4.0948086 -4.0989251 -4.1250114 -4.154067 -4.1701045 -4.1630139 -4.1495752 -4.1446705 -4.1462812][-4.179728 -4.1445413 -4.117547 -4.0875969 -4.0646958 -4.0414171 -4.0112863 -4.0213447 -4.0677347 -4.1157436 -4.1448879 -4.1481776 -4.1444974 -4.1425734 -4.1407666][-4.1832085 -4.1385875 -4.0978246 -4.0485663 -4.0025616 -3.9541821 -3.9034448 -3.9233973 -4.00009 -4.0715647 -4.1156359 -4.1282582 -4.1358991 -4.1409841 -4.1384745][-4.1823339 -4.1314454 -4.0820808 -4.0213261 -3.9532998 -3.8750546 -3.799037 -3.8293517 -3.9373217 -4.0283394 -4.0844569 -4.1038575 -4.1184082 -4.1350532 -4.139401][-4.1795621 -4.12923 -4.0838466 -4.02659 -3.9555044 -3.8691537 -3.7878687 -3.8248358 -3.9365208 -4.0225573 -4.0758724 -4.0915508 -4.1053038 -4.1257987 -4.134192][-4.1841664 -4.1399403 -4.1053224 -4.0610933 -4.0068941 -3.9453168 -3.8972294 -3.9381623 -4.0183873 -4.0716958 -4.103796 -4.1082664 -4.1167655 -4.1364989 -4.1462584][-4.2026086 -4.1662536 -4.14005 -4.1072049 -4.0721469 -4.040009 -4.0255537 -4.0672507 -4.1179252 -4.1421542 -4.1532273 -4.1450028 -4.1428361 -4.155612 -4.1637836][-4.2295327 -4.2004151 -4.180244 -4.1581626 -4.138639 -4.1296015 -4.1358447 -4.1725168 -4.2019529 -4.2069678 -4.2040381 -4.1892066 -4.1780977 -4.1832247 -4.1889877][-4.2615366 -4.2396927 -4.2246442 -4.2105761 -4.2006817 -4.2043953 -4.21868 -4.2449794 -4.2602673 -4.2577276 -4.2529106 -4.2380004 -4.2231727 -4.2225695 -4.2246342][-4.2924666 -4.2774763 -4.2668219 -4.2568817 -4.252955 -4.2620044 -4.2772493 -4.295434 -4.3043942 -4.3014631 -4.2991385 -4.2892418 -4.2771363 -4.2733612 -4.269289][-4.3199263 -4.3115263 -4.304183 -4.2964339 -4.2947369 -4.3036518 -4.3162003 -4.3280764 -4.3327374 -4.3303952 -4.3307714 -4.3283024 -4.324789 -4.3231578 -4.3162942][-4.3311925 -4.3283358 -4.3240066 -4.3188643 -4.3178129 -4.3232512 -4.3304911 -4.3361716 -4.3369007 -4.3348246 -4.3359551 -4.3378968 -4.3410912 -4.3430858 -4.3378997]]...]
INFO - root - 2017-12-05 19:23:49.330717: step 34010, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.940 sec/batch; 77h:56m:34s remains)
INFO - root - 2017-12-05 19:23:58.460965: step 34020, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 77h:41m:17s remains)
INFO - root - 2017-12-05 19:24:07.665080: step 34030, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 79h:17m:35s remains)
INFO - root - 2017-12-05 19:24:16.731615: step 34040, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 74h:08m:54s remains)
INFO - root - 2017-12-05 19:24:25.709922: step 34050, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.958 sec/batch; 79h:27m:37s remains)
INFO - root - 2017-12-05 19:24:34.852902: step 34060, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 74h:30m:33s remains)
INFO - root - 2017-12-05 19:24:43.927741: step 34070, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 74h:25m:50s remains)
INFO - root - 2017-12-05 19:24:52.909307: step 34080, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 74h:50m:24s remains)
INFO - root - 2017-12-05 19:25:02.006259: step 34090, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 77h:35m:24s remains)
INFO - root - 2017-12-05 19:25:11.056406: step 34100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 75h:41m:15s remains)
2017-12-05 19:25:11.765741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3286262 -4.3295722 -4.3282747 -4.3218765 -4.3106618 -4.2976303 -4.2902188 -4.2905097 -4.2966843 -4.3057632 -4.3121014 -4.3152237 -4.3154688 -4.3141794 -4.3140087][-4.3377738 -4.3386889 -4.3393488 -4.3370829 -4.3315978 -4.3250294 -4.3222575 -4.3235397 -4.32791 -4.3332734 -4.3359623 -4.3351884 -4.331924 -4.3284779 -4.3273182][-4.3413534 -4.3426666 -4.345274 -4.3465757 -4.3463988 -4.3458591 -4.3464308 -4.3477011 -4.3503227 -4.3528996 -4.35279 -4.3484826 -4.3412466 -4.3344817 -4.33054][-4.3391361 -4.3406429 -4.342998 -4.3447032 -4.3472934 -4.3486004 -4.3475032 -4.3460469 -4.3475814 -4.3507466 -4.3523779 -4.3491158 -4.3401694 -4.3299546 -4.3206644][-4.3298345 -4.3291411 -4.3262606 -4.322227 -4.3213224 -4.319531 -4.3127003 -4.30617 -4.3092332 -4.3189964 -4.3300533 -4.3345776 -4.3282757 -4.3163624 -4.2999082][-4.3136172 -4.3071461 -4.2938271 -4.2747979 -4.258451 -4.2442737 -4.2257309 -4.2141161 -4.2248716 -4.2514858 -4.281508 -4.3014565 -4.3058529 -4.2961106 -4.2713671][-4.2949147 -4.2796016 -4.2491293 -4.2049756 -4.1595058 -4.1178918 -4.0797091 -4.0689244 -4.1010747 -4.1579943 -4.2125468 -4.2494617 -4.2682762 -4.2636714 -4.2335453][-4.2792997 -4.2537847 -4.2042823 -4.1312647 -4.0479937 -3.9680369 -3.9016168 -3.8899994 -3.9471598 -4.0383215 -4.1192536 -4.1765857 -4.2141933 -4.2233238 -4.2011161][-4.2654352 -4.236392 -4.1807961 -4.0963788 -3.9940259 -3.8911004 -3.8081985 -3.7964766 -3.8638806 -3.9707963 -4.06542 -4.1372771 -4.1877332 -4.2093291 -4.2005172][-4.2519293 -4.23078 -4.1907415 -4.1268263 -4.0471258 -3.969399 -3.9134326 -3.9135249 -3.9669237 -4.0495119 -4.1233683 -4.1799769 -4.2187529 -4.2339854 -4.2246952][-4.250227 -4.2447248 -4.2277231 -4.1942544 -4.1493344 -4.1058331 -4.0806289 -4.0910096 -4.1282296 -4.1816611 -4.2275171 -4.2588139 -4.2739329 -4.2695446 -4.2468724][-4.2648711 -4.2686639 -4.2666831 -4.2551622 -4.2383766 -4.2218561 -4.2155619 -4.2274175 -4.2509537 -4.2809472 -4.3050704 -4.3184648 -4.3179607 -4.3017492 -4.270637][-4.2889938 -4.2947221 -4.2986479 -4.2981844 -4.2970657 -4.2956195 -4.2976289 -4.3057189 -4.3173966 -4.3306675 -4.3406339 -4.3451047 -4.3406115 -4.3257656 -4.3006692][-4.3142581 -4.31903 -4.3232908 -4.3260908 -4.3302979 -4.3321991 -4.3330965 -4.3349838 -4.33839 -4.3428655 -4.3455191 -4.3463917 -4.3437424 -4.3357091 -4.3226094][-4.3297176 -4.3334985 -4.3372808 -4.3401914 -4.3442588 -4.3448882 -4.3433042 -4.3408 -4.33808 -4.3364244 -4.3349118 -4.3346176 -4.3346186 -4.3330026 -4.3298783]]...]
INFO - root - 2017-12-05 19:25:20.800263: step 34110, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.951 sec/batch; 78h:48m:14s remains)
INFO - root - 2017-12-05 19:25:29.741146: step 34120, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 72h:35m:24s remains)
INFO - root - 2017-12-05 19:25:38.850390: step 34130, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 77h:26m:41s remains)
INFO - root - 2017-12-05 19:25:47.744246: step 34140, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 74h:16m:21s remains)
INFO - root - 2017-12-05 19:25:56.838985: step 34150, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 75h:03m:46s remains)
INFO - root - 2017-12-05 19:26:05.952725: step 34160, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 76h:06m:18s remains)
INFO - root - 2017-12-05 19:26:15.080129: step 34170, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 76h:41m:24s remains)
INFO - root - 2017-12-05 19:26:24.208066: step 34180, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 75h:51m:40s remains)
INFO - root - 2017-12-05 19:26:33.242217: step 34190, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 74h:50m:27s remains)
INFO - root - 2017-12-05 19:26:42.367785: step 34200, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 75h:44m:11s remains)
2017-12-05 19:26:43.254909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1719613 -4.197834 -4.2291732 -4.2420449 -4.2442079 -4.2409415 -4.227612 -4.2207766 -4.2222939 -4.2230473 -4.2142863 -4.2072005 -4.2052746 -4.1985927 -4.197371][-4.15976 -4.1875553 -4.2149086 -4.2240286 -4.2257233 -4.226572 -4.2177014 -4.2124491 -4.2152948 -4.2183905 -4.2135649 -4.2074537 -4.2043128 -4.1933684 -4.1902766][-4.1490784 -4.1744533 -4.194068 -4.1984944 -4.1962643 -4.1985178 -4.1941485 -4.190105 -4.1920929 -4.19212 -4.1918945 -4.1951871 -4.1988373 -4.1900921 -4.1880622][-4.1442184 -4.1686215 -4.1829157 -4.1806154 -4.1694674 -4.1653137 -4.15952 -4.1564693 -4.1547675 -4.1527457 -4.1593184 -4.174468 -4.1901608 -4.192533 -4.1923809][-4.1289773 -4.1531734 -4.1655183 -4.1579256 -4.1359582 -4.1134062 -4.0952792 -4.0923944 -4.1002197 -4.1096039 -4.1263089 -4.1528482 -4.1785469 -4.1875844 -4.1876221][-4.0914583 -4.1127672 -4.1238365 -4.1136227 -4.080987 -4.038002 -4.0066047 -4.0100765 -4.0362644 -4.069901 -4.1037493 -4.1382341 -4.1666994 -4.1756053 -4.1745343][-4.0574489 -4.0707407 -4.0777087 -4.0624018 -4.02106 -3.9638669 -3.9171999 -3.9214303 -3.9700599 -4.0321488 -4.0871115 -4.1317706 -4.1616144 -4.1707959 -4.1650991][-4.0634141 -4.0617428 -4.0606837 -4.0400209 -3.9974227 -3.9357853 -3.8748403 -3.8686037 -3.9241252 -4.0057883 -4.0766559 -4.1272125 -4.1552548 -4.1658354 -4.1625638][-4.1065555 -4.0914721 -4.0871072 -4.0698695 -4.0384307 -3.987335 -3.9277012 -3.9113803 -3.9504647 -4.02038 -4.08407 -4.1278543 -4.1480145 -4.1566563 -4.1568451][-4.153296 -4.1385403 -4.1402235 -4.1345749 -4.1187296 -4.0859561 -4.0417161 -4.0202761 -4.0368214 -4.0780253 -4.12008 -4.1477628 -4.1520653 -4.1504836 -4.1500382][-4.1749778 -4.1673489 -4.1790867 -4.1867924 -4.1843567 -4.1688271 -4.1402802 -4.1192541 -4.122838 -4.1441803 -4.1694193 -4.1800838 -4.1700482 -4.160161 -4.1595721][-4.1714735 -4.1688485 -4.1850419 -4.2002659 -4.2082429 -4.2075152 -4.1949673 -4.1811361 -4.181901 -4.1961403 -4.2116055 -4.21104 -4.1954393 -4.1827812 -4.1845155][-4.168148 -4.1635046 -4.175293 -4.18866 -4.1999388 -4.2108846 -4.2151141 -4.2098122 -4.2110615 -4.2212958 -4.2294073 -4.2206335 -4.1994553 -4.18897 -4.1948342][-4.1773787 -4.1699548 -4.1745734 -4.1832972 -4.1952367 -4.2116113 -4.2270288 -4.2312045 -4.2312717 -4.2306566 -4.2261739 -4.2070427 -4.1788297 -4.1681118 -4.1767063][-4.1920471 -4.1799974 -4.1796765 -4.1883192 -4.2008677 -4.21764 -4.2388129 -4.2493873 -4.250277 -4.2383957 -4.2201481 -4.1940856 -4.1619267 -4.1466308 -4.1528049]]...]
INFO - root - 2017-12-05 19:26:52.507199: step 34210, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 76h:14m:18s remains)
INFO - root - 2017-12-05 19:27:01.611556: step 34220, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 76h:43m:50s remains)
INFO - root - 2017-12-05 19:27:10.700600: step 34230, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 72h:29m:31s remains)
INFO - root - 2017-12-05 19:27:19.524239: step 34240, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 73h:28m:47s remains)
INFO - root - 2017-12-05 19:27:28.649744: step 34250, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 75h:15m:55s remains)
INFO - root - 2017-12-05 19:27:37.698245: step 34260, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 76h:20m:38s remains)
INFO - root - 2017-12-05 19:27:46.764946: step 34270, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 74h:48m:12s remains)
INFO - root - 2017-12-05 19:27:55.872871: step 34280, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 71h:54m:01s remains)
INFO - root - 2017-12-05 19:28:04.744872: step 34290, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 72h:20m:27s remains)
INFO - root - 2017-12-05 19:28:13.741948: step 34300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 75h:24m:32s remains)
2017-12-05 19:28:14.507454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2752781 -4.2764831 -4.27648 -4.2692657 -4.2585659 -4.2574167 -4.2686658 -4.28761 -4.3058095 -4.3153076 -4.30683 -4.2917428 -4.2858877 -4.2880387 -4.2940135][-4.275249 -4.271306 -4.2600513 -4.2369919 -4.2145491 -4.2100716 -4.2259421 -4.257133 -4.2875547 -4.3078718 -4.3044939 -4.290905 -4.2848721 -4.2876978 -4.2956119][-4.2652721 -4.2521095 -4.2246475 -4.1845593 -4.1522064 -4.144402 -4.1625276 -4.2019916 -4.2447352 -4.2798853 -4.2895041 -4.2829041 -4.2803063 -4.2841387 -4.2926083][-4.2397504 -4.2170081 -4.1760049 -4.1247606 -4.0889621 -4.0805969 -4.1000948 -4.1416044 -4.188746 -4.2347736 -4.2600985 -4.2648087 -4.2711577 -4.2793136 -4.2894959][-4.2121515 -4.1833119 -4.1386247 -4.0932913 -4.0695791 -4.0686045 -4.0855021 -4.115653 -4.1495867 -4.1913009 -4.222363 -4.2379293 -4.2557092 -4.2706852 -4.2834997][-4.1976333 -4.161881 -4.1219673 -4.0930433 -4.0839787 -4.0859375 -4.0918951 -4.0998096 -4.112009 -4.1427736 -4.1764092 -4.2020373 -4.2329216 -4.2581086 -4.27565][-4.2044473 -4.1608548 -4.1202207 -4.09591 -4.0908155 -4.0904489 -4.0834336 -4.0752034 -4.0773807 -4.1028318 -4.1338181 -4.1624179 -4.2005882 -4.2347445 -4.2556839][-4.2326007 -4.184557 -4.1382451 -4.1100488 -4.1042023 -4.1007533 -4.0821743 -4.0614138 -4.0604668 -4.0828838 -4.1039519 -4.1256652 -4.1617732 -4.1953082 -4.2155][-4.2679005 -4.2213049 -4.1703463 -4.1379914 -4.1290617 -4.1233416 -4.1012244 -4.0737524 -4.0674114 -4.0801768 -4.0851054 -4.0953016 -4.1201782 -4.1396589 -4.1510134][-4.2930527 -4.2508855 -4.2005768 -4.169445 -4.1588569 -4.1548891 -4.1396866 -4.1149621 -4.0981793 -4.0914359 -4.0758057 -4.0650244 -4.0687685 -4.0700665 -4.0747352][-4.30428 -4.266499 -4.2199583 -4.1929793 -4.184751 -4.1840668 -4.18168 -4.1656113 -4.14509 -4.1218476 -4.0857129 -4.0504675 -4.0306168 -4.0190969 -4.0293789][-4.3081679 -4.2750731 -4.2344422 -4.2122917 -4.2079062 -4.2105274 -4.2175579 -4.2071662 -4.184001 -4.1533246 -4.1100173 -4.0679483 -4.0421171 -4.0338473 -4.0558577][-4.3102531 -4.2836928 -4.2535005 -4.2382727 -4.2365866 -4.24019 -4.2471952 -4.2370362 -4.2155256 -4.1892567 -4.1545033 -4.1194453 -4.0965095 -4.0940542 -4.124022][-4.3097115 -4.2898932 -4.2691288 -4.2591848 -4.2579932 -4.2600579 -4.2603512 -4.2496414 -4.2347054 -4.2228322 -4.2058029 -4.1835403 -4.1699405 -4.1758227 -4.2073283][-4.3002753 -4.2848153 -4.268466 -4.2598124 -4.2581406 -4.2573071 -4.2519021 -4.2434759 -4.2396398 -4.2444916 -4.2486787 -4.2455211 -4.2484717 -4.2626762 -4.2901731]]...]
INFO - root - 2017-12-05 19:28:23.587977: step 34310, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.904 sec/batch; 74h:51m:25s remains)
INFO - root - 2017-12-05 19:28:32.657472: step 34320, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 69h:46m:25s remains)
INFO - root - 2017-12-05 19:28:41.498434: step 34330, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 73h:58m:19s remains)
INFO - root - 2017-12-05 19:28:50.609484: step 34340, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 74h:45m:14s remains)
INFO - root - 2017-12-05 19:28:59.686439: step 34350, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 75h:39m:34s remains)
INFO - root - 2017-12-05 19:29:08.760729: step 34360, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 76h:07m:42s remains)
INFO - root - 2017-12-05 19:29:17.915804: step 34370, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 77h:08m:55s remains)
INFO - root - 2017-12-05 19:29:26.967282: step 34380, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.822 sec/batch; 68h:02m:09s remains)
INFO - root - 2017-12-05 19:29:35.951083: step 34390, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 74h:59m:03s remains)
INFO - root - 2017-12-05 19:29:44.924119: step 34400, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 73h:53m:03s remains)
2017-12-05 19:29:45.728222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1890411 -4.2015781 -4.1944704 -4.1811194 -4.1766982 -4.1810136 -4.1844 -4.1872983 -4.1903467 -4.19393 -4.1974692 -4.1996903 -4.1989326 -4.1937389 -4.1818247][-4.1854048 -4.1960855 -4.18453 -4.1682005 -4.1629634 -4.1693358 -4.1729856 -4.1752424 -4.1773415 -4.1809645 -4.1852407 -4.18961 -4.1910958 -4.1878233 -4.1795578][-4.2126546 -4.2254925 -4.2135906 -4.1953053 -4.1885543 -4.1948957 -4.1967516 -4.1962037 -4.1957393 -4.1964498 -4.1984291 -4.2028279 -4.2059379 -4.2060761 -4.2031989][-4.2403393 -4.2585883 -4.2490807 -4.2306342 -4.2214622 -4.2248263 -4.2210298 -4.2150931 -4.209825 -4.2055082 -4.2047853 -4.2097597 -4.2147551 -4.216939 -4.2209473][-4.2666225 -4.28482 -4.27399 -4.2532077 -4.2396059 -4.2374773 -4.225615 -4.2099252 -4.1970325 -4.1865439 -4.1811538 -4.1877279 -4.1970272 -4.2043366 -4.2167745][-4.2808237 -4.2915735 -4.2706661 -4.2394938 -4.2150297 -4.20185 -4.17563 -4.1466675 -4.1328282 -4.128664 -4.1323719 -4.1475668 -4.1625919 -4.1760106 -4.1980419][-4.2760777 -4.2719159 -4.2314377 -4.1780634 -4.13407 -4.101903 -4.0561538 -4.0188727 -4.0224361 -4.0421886 -4.0644422 -4.09582 -4.1204815 -4.139936 -4.1659265][-4.2541833 -4.2278748 -4.1582232 -4.0756993 -4.0104709 -3.9627736 -3.9051545 -3.8743243 -3.9128151 -3.9660275 -4.015233 -4.0686083 -4.1063976 -4.1321683 -4.1590662][-4.23207 -4.1914749 -4.1057711 -4.0131993 -3.9450586 -3.9036481 -3.8645425 -3.8630633 -3.9240937 -3.9875433 -4.0431514 -4.1019855 -4.141499 -4.1671886 -4.1916852][-4.2337642 -4.2006388 -4.1274886 -4.0509338 -3.9979913 -3.976902 -3.9642937 -3.9794266 -4.0324731 -4.080174 -4.1240144 -4.1727896 -4.2052994 -4.2247872 -4.2409682][-4.26233 -4.2489147 -4.1993423 -4.1454253 -4.1100817 -4.1041512 -4.1059737 -4.1246972 -4.1636715 -4.1968484 -4.2264032 -4.259253 -4.2802076 -4.2884822 -4.2903404][-4.2919025 -4.29748 -4.2683649 -4.2357335 -4.2184896 -4.2214937 -4.2292223 -4.2458344 -4.2723031 -4.294847 -4.3144689 -4.33377 -4.3414025 -4.3374863 -4.3246469][-4.3066077 -4.3239875 -4.30915 -4.2903218 -4.2838445 -4.2896032 -4.2990665 -4.31493 -4.3347883 -4.3509984 -4.3632941 -4.3696847 -4.3634233 -4.3465657 -4.3227234][-4.2983069 -4.3213983 -4.3157773 -4.3042283 -4.3048396 -4.3146644 -4.3237686 -4.33685 -4.3493752 -4.3590212 -4.3641243 -4.3623676 -4.3489509 -4.32554 -4.2982154][-4.279098 -4.2985516 -4.2932525 -4.2800283 -4.2803593 -4.2911987 -4.2992482 -4.3095212 -4.3172359 -4.3221784 -4.3234749 -4.3196654 -4.3060675 -4.2840009 -4.2596951]]...]
INFO - root - 2017-12-05 19:29:54.865356: step 34410, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 77h:00m:01s remains)
INFO - root - 2017-12-05 19:30:03.888471: step 34420, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 75h:28m:42s remains)
INFO - root - 2017-12-05 19:30:12.922406: step 34430, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 75h:40m:42s remains)
INFO - root - 2017-12-05 19:30:21.902871: step 34440, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 77h:30m:31s remains)
INFO - root - 2017-12-05 19:30:30.939543: step 34450, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:37m:49s remains)
INFO - root - 2017-12-05 19:30:40.083566: step 34460, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 77h:00m:06s remains)
INFO - root - 2017-12-05 19:30:49.177492: step 34470, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 77h:05m:04s remains)
INFO - root - 2017-12-05 19:30:58.131273: step 34480, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 75h:06m:43s remains)
INFO - root - 2017-12-05 19:31:07.297169: step 34490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 77h:13m:35s remains)
INFO - root - 2017-12-05 19:31:16.294418: step 34500, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 76h:52m:47s remains)
2017-12-05 19:31:17.164571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1794605 -4.1810279 -4.1830435 -4.1869245 -4.1907444 -4.197782 -4.2108073 -4.2238946 -4.2179317 -4.1924257 -4.15972 -4.1359754 -4.1380172 -4.1634169 -4.1965413][-4.1972027 -4.1950231 -4.1949439 -4.1973329 -4.20076 -4.2072845 -4.2180948 -4.2255211 -4.2116232 -4.1791158 -4.1423206 -4.1213875 -4.1332254 -4.1708312 -4.213963][-4.2244625 -4.2201495 -4.2181888 -4.2189107 -4.2211413 -4.2256608 -4.2328315 -4.2357793 -4.2210956 -4.1912432 -4.1599693 -4.1463223 -4.1629143 -4.2022696 -4.2444458][-4.2491059 -4.2413378 -4.238122 -4.238884 -4.2405663 -4.243988 -4.248024 -4.2502685 -4.2430158 -4.2255468 -4.2077856 -4.2018952 -4.2154045 -4.244041 -4.2744188][-4.2577548 -4.2498527 -4.2483339 -4.2501307 -4.249855 -4.2477431 -4.2454815 -4.2473955 -4.25085 -4.2512951 -4.2518311 -4.2551708 -4.2646379 -4.278986 -4.2920475][-4.2590446 -4.2533679 -4.2510686 -4.2479367 -4.237946 -4.2217836 -4.2067523 -4.2077007 -4.224968 -4.2475953 -4.2687135 -4.2825222 -4.2901635 -4.29378 -4.29269][-4.2448273 -4.238091 -4.2291174 -4.2128878 -4.1845832 -4.14786 -4.118382 -4.122426 -4.1594768 -4.2093196 -4.2521052 -4.2766147 -4.2841673 -4.2812219 -4.2735462][-4.2184219 -4.2060785 -4.185895 -4.1527834 -4.10499 -4.051681 -4.0143027 -4.0289836 -4.0914803 -4.1668711 -4.2251062 -4.2561059 -4.2639284 -4.2588882 -4.2508197][-4.1855369 -4.1659775 -4.1349874 -4.0908952 -4.0345216 -3.9781218 -3.9448106 -3.974082 -4.0563669 -4.145575 -4.21007 -4.2422733 -4.2494421 -4.24411 -4.2366419][-4.1548576 -4.1354365 -4.1080928 -4.0725594 -4.0311642 -3.9923716 -3.97318 -4.004755 -4.0809841 -4.1607227 -4.2177706 -4.2444334 -4.2479544 -4.2408662 -4.2320714][-4.1472406 -4.1380525 -4.1265297 -4.1127834 -4.0952573 -4.0780611 -4.0700288 -4.0947461 -4.1501541 -4.20682 -4.24559 -4.2597561 -4.2563553 -4.2449884 -4.2337971][-4.15362 -4.1552935 -4.1577086 -4.1602125 -4.1593394 -4.1587119 -4.1628542 -4.1846681 -4.2226834 -4.2575965 -4.2762346 -4.275671 -4.2635126 -4.2485037 -4.2369747][-4.1799612 -4.185801 -4.1925893 -4.2000885 -4.2079673 -4.2198119 -4.234148 -4.2543483 -4.2781134 -4.2941351 -4.2943411 -4.2808228 -4.2632141 -4.2481084 -4.2396393][-4.2102656 -4.2140317 -4.2191725 -4.2266908 -4.2383666 -4.2569675 -4.2765656 -4.2935596 -4.30551 -4.3068819 -4.2949071 -4.2757721 -4.2580824 -4.2466235 -4.2433858][-4.236486 -4.2356706 -4.2369342 -4.2418413 -4.2518568 -4.2682476 -4.2853031 -4.2975173 -4.3015413 -4.2964511 -4.2828627 -4.2668815 -4.2548313 -4.2491789 -4.2508478]]...]
INFO - root - 2017-12-05 19:31:26.443017: step 34510, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 75h:16m:02s remains)
INFO - root - 2017-12-05 19:31:35.532124: step 34520, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 76h:01m:52s remains)
INFO - root - 2017-12-05 19:31:44.480355: step 34530, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 73h:47m:13s remains)
INFO - root - 2017-12-05 19:31:53.737256: step 34540, loss = 2.07, batch loss = 2.02 (8.2 examples/sec; 0.978 sec/batch; 80h:58m:52s remains)
INFO - root - 2017-12-05 19:32:03.021862: step 34550, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 77h:36m:35s remains)
INFO - root - 2017-12-05 19:32:12.146503: step 34560, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 75h:09m:51s remains)
INFO - root - 2017-12-05 19:32:21.173796: step 34570, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 75h:21m:19s remains)
INFO - root - 2017-12-05 19:32:30.449465: step 34580, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 76h:44m:08s remains)
INFO - root - 2017-12-05 19:32:39.601075: step 34590, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 76h:26m:15s remains)
INFO - root - 2017-12-05 19:32:48.705350: step 34600, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 74h:45m:38s remains)
2017-12-05 19:32:49.528565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2474856 -4.2520394 -4.2624869 -4.2638803 -4.261024 -4.2563972 -4.2538629 -4.2587 -4.2708545 -4.2861652 -4.2864566 -4.27474 -4.2575111 -4.2448611 -4.2412758][-4.2529063 -4.2591562 -4.2660966 -4.26391 -4.2565103 -4.2484341 -4.2434225 -4.2487006 -4.2623038 -4.2775011 -4.2803822 -4.2760715 -4.271822 -4.2627392 -4.2535152][-4.2677841 -4.2762375 -4.2778764 -4.2700667 -4.2590022 -4.2480259 -4.240025 -4.2435293 -4.2577391 -4.2733979 -4.2798371 -4.2827835 -4.2851429 -4.2728748 -4.2525182][-4.2876792 -4.2938814 -4.2884288 -4.27764 -4.2654109 -4.2460885 -4.228385 -4.2243347 -4.2373538 -4.2550817 -4.2634892 -4.27178 -4.273756 -4.2516937 -4.21377][-4.3030505 -4.3067985 -4.2984943 -4.2864332 -4.2684522 -4.2335153 -4.1970358 -4.1766038 -4.1876755 -4.2113447 -4.2237663 -4.240633 -4.244298 -4.2121663 -4.1524634][-4.3083014 -4.3092093 -4.2960277 -4.2728772 -4.2370477 -4.1796346 -4.1147518 -4.0729818 -4.0878687 -4.129704 -4.16031 -4.1972022 -4.2163506 -4.1881008 -4.1246843][-4.3089223 -4.3047552 -4.2809863 -4.2420406 -4.1830192 -4.0925803 -3.9922683 -3.9267597 -3.9542902 -4.0378137 -4.1101441 -4.1733403 -4.2097 -4.198257 -4.1565094][-4.3002267 -4.2920327 -4.2602677 -4.2111588 -4.1344447 -4.01612 -3.8895173 -3.8188508 -3.8766649 -4.0059414 -4.108182 -4.1791291 -4.2189279 -4.2190528 -4.1989188][-4.2745318 -4.26712 -4.2420506 -4.1966152 -4.1233625 -4.0104084 -3.9011195 -3.8542316 -3.922081 -4.0474381 -4.1395874 -4.1942477 -4.2240562 -4.2273908 -4.2212062][-4.2301145 -4.2269158 -4.22045 -4.1977167 -4.1530881 -4.0780621 -4.0090084 -3.9811995 -4.0265021 -4.1117673 -4.1720142 -4.2052855 -4.2233253 -4.2233992 -4.2225633][-4.1842551 -4.1818395 -4.1891251 -4.1897469 -4.177742 -4.143558 -4.1060429 -4.087852 -4.1105995 -4.1606793 -4.1989279 -4.2198606 -4.2279348 -4.2252107 -4.2277942][-4.1654778 -4.1559744 -4.1623163 -4.1745739 -4.1868954 -4.1857615 -4.1704297 -4.1612053 -4.1706805 -4.1991572 -4.2265553 -4.2428584 -4.2471318 -4.247344 -4.2524767][-4.1685033 -4.145031 -4.1420541 -4.1568608 -4.1837268 -4.2071381 -4.2160339 -4.2165546 -4.2237654 -4.2422285 -4.2606659 -4.2718449 -4.2742076 -4.2759795 -4.2801747][-4.1924486 -4.1648774 -4.1567116 -4.1689315 -4.1986656 -4.2328382 -4.2540927 -4.2628088 -4.2707849 -4.2815952 -4.2931485 -4.2997727 -4.2974911 -4.2943196 -4.2953014][-4.2326994 -4.2144175 -4.2085261 -4.2131596 -4.2322283 -4.2619886 -4.2859964 -4.2989826 -4.305223 -4.3080297 -4.3119421 -4.3144732 -4.3095403 -4.3018942 -4.2987924]]...]
INFO - root - 2017-12-05 19:32:58.532664: step 34610, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 76h:27m:42s remains)
INFO - root - 2017-12-05 19:33:07.719127: step 34620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 73h:07m:16s remains)
INFO - root - 2017-12-05 19:33:16.835896: step 34630, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 71h:51m:55s remains)
INFO - root - 2017-12-05 19:33:25.986235: step 34640, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 77h:37m:44s remains)
INFO - root - 2017-12-05 19:33:35.152235: step 34650, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 77h:36m:29s remains)
INFO - root - 2017-12-05 19:33:44.247422: step 34660, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 68h:46m:14s remains)
INFO - root - 2017-12-05 19:33:53.126474: step 34670, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 77h:31m:33s remains)
INFO - root - 2017-12-05 19:34:02.271917: step 34680, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 75h:55m:23s remains)
INFO - root - 2017-12-05 19:34:11.476045: step 34690, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 74h:12m:37s remains)
INFO - root - 2017-12-05 19:34:20.587680: step 34700, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 74h:38m:48s remains)
2017-12-05 19:34:21.300414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3161864 -4.3144646 -4.3131008 -4.3128467 -4.3133616 -4.3141317 -4.3156695 -4.317771 -4.31962 -4.3198371 -4.3188219 -4.3171787 -4.3167014 -4.3178711 -4.3201818][-4.3155632 -4.3122091 -4.3096819 -4.3094449 -4.3096981 -4.3094926 -4.310235 -4.3118687 -4.3134046 -4.3131771 -4.3116837 -4.3099294 -4.3097315 -4.3119774 -4.316093][-4.30955 -4.3027582 -4.2974434 -4.29694 -4.2992578 -4.3003931 -4.302022 -4.3040357 -4.3049006 -4.3028512 -4.2985554 -4.2955012 -4.295434 -4.2993712 -4.3057947][-4.2971349 -4.2857075 -4.2758036 -4.2730074 -4.2762542 -4.278203 -4.2799125 -4.2791338 -4.2774916 -4.2746243 -4.2696533 -4.2667809 -4.2684245 -4.2765222 -4.2880759][-4.2745256 -4.255022 -4.2382774 -4.2312355 -4.232965 -4.2315459 -4.2260556 -4.2149019 -4.2086687 -4.2108021 -4.2135472 -4.2171988 -4.2235484 -4.2375741 -4.2562284][-4.2437849 -4.2144046 -4.1921196 -4.1850572 -4.1857619 -4.17725 -4.1533036 -4.1179552 -4.0992942 -4.1093035 -4.1284423 -4.1464586 -4.16202 -4.1836281 -4.209475][-4.2135262 -4.1773443 -4.1520972 -4.1455107 -4.1431708 -4.1264772 -4.0827465 -4.0224309 -3.9894631 -4.00969 -4.0508056 -4.0889759 -4.1177936 -4.1462483 -4.17546][-4.2074203 -4.1742873 -4.147748 -4.1373758 -4.1293049 -4.1057415 -4.0555735 -3.9920485 -3.958375 -3.9824805 -4.0343657 -4.0849872 -4.1232963 -4.1548133 -4.1819][-4.2250266 -4.2045617 -4.18446 -4.1729054 -4.1623497 -4.1416736 -4.1056414 -4.0642595 -4.0437651 -4.061254 -4.1002235 -4.142386 -4.17332 -4.1953311 -4.2125506][-4.2419615 -4.2363482 -4.2280626 -4.2226882 -4.219141 -4.2101111 -4.1928439 -4.1737809 -4.1650281 -4.1731062 -4.1915889 -4.2142258 -4.2274089 -4.232121 -4.2351437][-4.24487 -4.2483883 -4.2480435 -4.2483182 -4.2514653 -4.2528419 -4.2522144 -4.2492008 -4.2473097 -4.2465825 -4.2474575 -4.2518582 -4.250926 -4.2424231 -4.2335134][-4.225112 -4.231462 -4.2343984 -4.2366157 -4.2405624 -4.2450829 -4.2524166 -4.2577276 -4.2571292 -4.249506 -4.2418227 -4.2394228 -4.2352881 -4.2253594 -4.2146335][-4.2028131 -4.2074037 -4.2091546 -4.20838 -4.207931 -4.2084374 -4.217957 -4.227541 -4.2273049 -4.2191043 -4.2123461 -4.2128139 -4.2128687 -4.2094455 -4.204525][-4.203784 -4.2051244 -4.203249 -4.1982965 -4.19322 -4.1907687 -4.1986427 -4.2078853 -4.2080517 -4.2030048 -4.2017474 -4.2071543 -4.2109671 -4.2134614 -4.2155528][-4.2266817 -4.2256007 -4.2215695 -4.2126617 -4.204483 -4.200923 -4.20614 -4.2124925 -4.2134023 -4.2124825 -4.2165837 -4.2261667 -4.2330542 -4.240582 -4.24805]]...]
INFO - root - 2017-12-05 19:34:30.371233: step 34710, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.923 sec/batch; 76h:21m:42s remains)
INFO - root - 2017-12-05 19:34:39.597524: step 34720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 75h:59m:45s remains)
INFO - root - 2017-12-05 19:34:48.733125: step 34730, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 71h:04m:30s remains)
INFO - root - 2017-12-05 19:34:57.769002: step 34740, loss = 2.01, batch loss = 1.96 (8.8 examples/sec; 0.913 sec/batch; 75h:29m:57s remains)
INFO - root - 2017-12-05 19:35:06.903918: step 34750, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 74h:50m:43s remains)
INFO - root - 2017-12-05 19:35:15.974743: step 34760, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 73h:26m:47s remains)
INFO - root - 2017-12-05 19:35:24.957767: step 34770, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 77h:46m:25s remains)
INFO - root - 2017-12-05 19:35:34.138012: step 34780, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 76h:06m:24s remains)
INFO - root - 2017-12-05 19:35:43.192853: step 34790, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 72h:33m:13s remains)
INFO - root - 2017-12-05 19:35:52.311024: step 34800, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 75h:22m:46s remains)
2017-12-05 19:35:53.205807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0936813 -4.098918 -4.0875168 -4.0718274 -4.0643988 -4.0670304 -4.0860558 -4.0960107 -4.0911231 -4.1038861 -4.1237683 -4.1461897 -4.1645846 -4.1716814 -4.1504188][-4.0916719 -4.0944781 -4.0851378 -4.0730724 -4.0695281 -4.0674143 -4.0834522 -4.0986829 -4.1035695 -4.1221714 -4.1401124 -4.1566038 -4.16742 -4.1649537 -4.1399646][-4.1305342 -4.1299496 -4.1221361 -4.1106634 -4.099566 -4.0821028 -4.0850058 -4.0932069 -4.1023831 -4.1320128 -4.1596246 -4.1754541 -4.1836042 -4.176702 -4.1546187][-4.16218 -4.1656961 -4.1608272 -4.14839 -4.1270046 -4.0964141 -4.0814939 -4.0809612 -4.0946646 -4.1327252 -4.1697493 -4.1863484 -4.1943088 -4.1894908 -4.1695709][-4.1708679 -4.1769223 -4.1716261 -4.155344 -4.1251721 -4.0817285 -4.045547 -4.0349603 -4.0640974 -4.1168065 -4.1618853 -4.1788783 -4.1915507 -4.1915812 -4.173605][-4.1627879 -4.1597152 -4.1463017 -4.1238294 -4.0806174 -4.0112042 -3.9440732 -3.9350638 -4.003346 -4.0830107 -4.1411071 -4.1685457 -4.1941371 -4.2038889 -4.190856][-4.1451473 -4.1241822 -4.1005621 -4.0746012 -4.0189939 -3.915453 -3.8131351 -3.8244002 -3.942908 -4.0456071 -4.1144614 -4.1578159 -4.1924744 -4.2120605 -4.2055135][-4.1389427 -4.1016955 -4.0716391 -4.0519972 -4.0039325 -3.9035654 -3.8151636 -3.8419249 -3.9555786 -4.0472374 -4.11117 -4.1583648 -4.1957507 -4.2196183 -4.2172341][-4.1374183 -4.0930276 -4.0703616 -4.0642767 -4.0347376 -3.9701114 -3.9242983 -3.9443302 -4.0147357 -4.0761795 -4.1291924 -4.1732244 -4.2090826 -4.2283759 -4.2231383][-4.1291409 -4.0846443 -4.0717487 -4.0792241 -4.067328 -4.0340023 -4.0182395 -4.0263906 -4.0586705 -4.09762 -4.1447539 -4.1863661 -4.2159438 -4.2284465 -4.2209287][-4.1372466 -4.0955191 -4.0864882 -4.0957041 -4.0916386 -4.0744886 -4.0745029 -4.0777264 -4.0932455 -4.121017 -4.1616569 -4.1967807 -4.2179589 -4.224719 -4.2203741][-4.1672912 -4.1299891 -4.1171479 -4.1187253 -4.117557 -4.111526 -4.1195736 -4.1242285 -4.13311 -4.1523151 -4.181922 -4.2068586 -4.2192473 -4.2228041 -4.2252545][-4.2020359 -4.1736364 -4.1566648 -4.1484065 -4.1463737 -4.1487756 -4.160481 -4.1671357 -4.1736388 -4.1858306 -4.2028537 -4.2174697 -4.2232842 -4.2265368 -4.2334771][-4.2309766 -4.2110152 -4.1963906 -4.1860476 -4.1831169 -4.1881018 -4.1967211 -4.1999965 -4.2031927 -4.2105355 -4.2199 -4.2273722 -4.2314043 -4.2360768 -4.2425985][-4.2514253 -4.2387285 -4.2285781 -4.2210221 -4.219686 -4.2238655 -4.2279572 -4.2280159 -4.2282739 -4.2312474 -4.2363491 -4.2417641 -4.2470331 -4.2519526 -4.2561135]]...]
INFO - root - 2017-12-05 19:36:02.328469: step 34810, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 72h:42m:46s remains)
INFO - root - 2017-12-05 19:36:11.282515: step 34820, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.878 sec/batch; 72h:36m:07s remains)
INFO - root - 2017-12-05 19:36:20.453597: step 34830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 75h:34m:42s remains)
INFO - root - 2017-12-05 19:36:29.466092: step 34840, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 72h:42m:22s remains)
INFO - root - 2017-12-05 19:36:38.529513: step 34850, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 72h:48m:27s remains)
INFO - root - 2017-12-05 19:36:47.584484: step 34860, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 73h:34m:45s remains)
INFO - root - 2017-12-05 19:36:56.744406: step 34870, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 78h:01m:55s remains)
INFO - root - 2017-12-05 19:37:05.909542: step 34880, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 77h:29m:08s remains)
INFO - root - 2017-12-05 19:37:15.000485: step 34890, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 77h:37m:18s remains)
INFO - root - 2017-12-05 19:37:24.161081: step 34900, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 73h:16m:34s remains)
2017-12-05 19:37:24.932045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2376785 -4.2436643 -4.2449722 -4.2417874 -4.2391028 -4.2359509 -4.2454 -4.2667217 -4.2787862 -4.270216 -4.2595334 -4.259912 -4.2704372 -4.2796221 -4.287674][-4.196507 -4.2104897 -4.2179246 -4.21404 -4.2079878 -4.2006674 -4.2103357 -4.2356019 -4.25204 -4.2462945 -4.2420697 -4.2506433 -4.2635307 -4.2712107 -4.278769][-4.1483297 -4.1689396 -4.1811581 -4.1739306 -4.1654658 -4.1583614 -4.1740475 -4.2024465 -4.2170353 -4.2175536 -4.225904 -4.2468491 -4.2618895 -4.2669077 -4.2717333][-4.1092906 -4.1265416 -4.1334424 -4.1228352 -4.1204853 -4.1224108 -4.1446104 -4.1686773 -4.1744246 -4.1802177 -4.2046294 -4.2394595 -4.2590809 -4.2641411 -4.2662621][-4.0834365 -4.0884919 -4.0829563 -4.0651603 -4.0664949 -4.0825706 -4.1098781 -4.1278257 -4.1277556 -4.1353965 -4.1727076 -4.2228231 -4.2525425 -4.2634563 -4.2656803][-4.0552554 -4.0499053 -4.0385232 -4.0238962 -4.0321903 -4.0549932 -4.0748153 -4.0755177 -4.0591874 -4.0687466 -4.1183953 -4.1825709 -4.230689 -4.2566929 -4.2662606][-4.0135827 -4.0030951 -3.99629 -3.9941218 -4.0031624 -4.0200844 -4.0251679 -4.0015736 -3.9719148 -3.9905183 -4.0583739 -4.1322703 -4.1932492 -4.236227 -4.2577152][-3.9620986 -3.9440246 -3.9412971 -3.94775 -3.9530096 -3.9624219 -3.95252 -3.9094782 -3.8705275 -3.8981268 -3.985496 -4.0645981 -4.1348724 -4.1933751 -4.2297654][-3.960099 -3.940268 -3.9376416 -3.9508326 -3.9583073 -3.9636493 -3.9449012 -3.8966591 -3.8534241 -3.8703396 -3.944227 -4.0174561 -4.0896854 -4.1557627 -4.2019267][-4.0238485 -4.0175023 -4.0156651 -4.0275846 -4.0363531 -4.036974 -4.0123787 -3.9679027 -3.9331021 -3.935143 -3.9782991 -4.0309205 -4.088984 -4.1485977 -4.1945972][-4.0918345 -4.0933 -4.0956044 -4.1067681 -4.1117105 -4.1061683 -4.0826139 -4.0493388 -4.0256696 -4.0177574 -4.033843 -4.0659237 -4.10925 -4.1586409 -4.2023225][-4.1553717 -4.1600361 -4.1627407 -4.1680303 -4.1681871 -4.16219 -4.1467838 -4.1279163 -4.1157317 -4.1060071 -4.1062036 -4.1215606 -4.1522317 -4.1898537 -4.2273326][-4.2176747 -4.2232609 -4.2238188 -4.2234573 -4.2208605 -4.2167854 -4.2077942 -4.1972704 -4.1912169 -4.1843662 -4.181231 -4.1898522 -4.2095633 -4.234612 -4.2592483][-4.2545719 -4.260488 -4.2621088 -4.2614098 -4.2587986 -4.2559667 -4.2504129 -4.2446623 -4.2425623 -4.23975 -4.2379327 -4.2431617 -4.255074 -4.2695637 -4.2832861][-4.2750421 -4.2788978 -4.2808547 -4.2809134 -4.2798567 -4.2786279 -4.2767873 -4.2745242 -4.2728548 -4.2710457 -4.269402 -4.2728286 -4.2806563 -4.2895212 -4.2976027]]...]
INFO - root - 2017-12-05 19:37:34.058988: step 34910, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.960 sec/batch; 79h:19m:11s remains)
INFO - root - 2017-12-05 19:37:43.373380: step 34920, loss = 2.07, batch loss = 2.01 (7.7 examples/sec; 1.041 sec/batch; 86h:05m:25s remains)
INFO - root - 2017-12-05 19:37:52.536825: step 34930, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.936 sec/batch; 77h:22m:36s remains)
INFO - root - 2017-12-05 19:38:01.479873: step 34940, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 75h:33m:54s remains)
INFO - root - 2017-12-05 19:38:10.570476: step 34950, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.894 sec/batch; 73h:52m:32s remains)
INFO - root - 2017-12-05 19:38:19.592859: step 34960, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 75h:58m:41s remains)
INFO - root - 2017-12-05 19:38:28.619979: step 34970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 75h:10m:12s remains)
INFO - root - 2017-12-05 19:38:37.908246: step 34980, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.946 sec/batch; 78h:08m:47s remains)
INFO - root - 2017-12-05 19:38:46.958951: step 34990, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 73h:54m:31s remains)
INFO - root - 2017-12-05 19:38:55.971402: step 35000, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 75h:25m:10s remains)
2017-12-05 19:38:56.847563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0770926 -4.075613 -4.084589 -4.0886173 -4.0848193 -4.0883536 -4.0975003 -4.0939217 -4.0962863 -4.1137276 -4.1180496 -4.099587 -4.0934844 -4.1153965 -4.1373444][-4.0855937 -4.08685 -4.0871339 -4.0771828 -4.0588331 -4.04797 -4.0549297 -4.0566273 -4.0703468 -4.1033273 -4.117003 -4.1059995 -4.1045175 -4.1252451 -4.1493616][-4.0641885 -4.085505 -4.0894337 -4.074019 -4.0420427 -4.0138621 -4.0120025 -4.02111 -4.0413651 -4.0829582 -4.104907 -4.1039214 -4.1099367 -4.1306353 -4.1568236][-4.0086074 -4.0502443 -4.0707374 -4.0571589 -4.0223184 -3.99193 -3.9850569 -3.995168 -4.0137253 -4.0531416 -4.077878 -4.0809388 -4.0967331 -4.1282558 -4.1581149][-3.9425325 -3.9764411 -3.9998591 -3.9906862 -3.9668498 -3.9450419 -3.9362423 -3.9489002 -3.9623208 -3.9904876 -4.0114031 -4.0179815 -4.0471025 -4.0962172 -4.1384258][-3.9196324 -3.9224966 -3.9241624 -3.904573 -3.8660088 -3.8133154 -3.7817116 -3.7985709 -3.8269548 -3.8643212 -3.8932662 -3.9076824 -3.9482398 -4.0236239 -4.0868959][-3.9400954 -3.9181736 -3.8893609 -3.847255 -3.773726 -3.6672363 -3.5864928 -3.6025853 -3.6675549 -3.7340789 -3.7813964 -3.8147135 -3.8716619 -3.967195 -4.0418615][-3.9677796 -3.9384606 -3.9038086 -3.870028 -3.7994838 -3.6885715 -3.5976188 -3.5917823 -3.6434865 -3.7070699 -3.759213 -3.8087819 -3.8797252 -3.9814575 -4.0504274][-4.006743 -3.9886167 -3.9665327 -3.9524734 -3.9097526 -3.836482 -3.7755418 -3.756824 -3.7725525 -3.8109317 -3.846396 -3.8864956 -3.9483101 -4.0397148 -4.094079][-4.0236616 -4.0192833 -4.0103879 -4.0050926 -3.9789691 -3.9371972 -3.9048481 -3.88885 -3.8917952 -3.9168751 -3.9249892 -3.9337556 -3.97294 -4.0485454 -4.0867925][-4.0280089 -4.0423493 -4.0506163 -4.0521393 -4.0330939 -4.0042272 -3.9859126 -3.9784579 -3.9810793 -3.9937468 -3.97731 -3.9588745 -3.9752488 -4.0257144 -4.0450115][-4.0114961 -4.0461535 -4.0728078 -4.0815496 -4.0724912 -4.0579791 -4.0501757 -4.0484781 -4.0521212 -4.0526571 -4.0230994 -3.9917936 -3.9930868 -4.0239644 -4.031168][-4.0353727 -4.0739274 -4.101656 -4.1122885 -4.1098223 -4.1045165 -4.1026893 -4.1026969 -4.1045918 -4.1049552 -4.0842252 -4.061677 -4.0603604 -4.0794272 -4.0793476][-4.1190972 -4.1508131 -4.1668463 -4.1689992 -4.16576 -4.1631818 -4.1627107 -4.1628852 -4.1634245 -4.1635251 -4.1531234 -4.1429062 -4.1457882 -4.1600642 -4.1578751][-4.2063355 -4.2233362 -4.2266417 -4.223279 -4.2202168 -4.2184958 -4.2177615 -4.2182055 -4.218926 -4.2186518 -4.2133579 -4.2091188 -4.2153172 -4.2306519 -4.23401]]...]
INFO - root - 2017-12-05 19:39:05.904238: step 35010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 76h:19m:57s remains)
INFO - root - 2017-12-05 19:39:15.138371: step 35020, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 74h:28m:43s remains)
INFO - root - 2017-12-05 19:39:24.480085: step 35030, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 76h:44m:37s remains)
INFO - root - 2017-12-05 19:39:33.542130: step 35040, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 75h:30m:53s remains)
INFO - root - 2017-12-05 19:39:42.612642: step 35050, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 76h:08m:50s remains)
INFO - root - 2017-12-05 19:39:51.667652: step 35060, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 76h:07m:37s remains)
INFO - root - 2017-12-05 19:40:00.738723: step 35070, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 75h:11m:45s remains)
INFO - root - 2017-12-05 19:40:09.883582: step 35080, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 74h:20m:15s remains)
INFO - root - 2017-12-05 19:40:19.028797: step 35090, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 73h:42m:28s remains)
INFO - root - 2017-12-05 19:40:28.043590: step 35100, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 74h:15m:07s remains)
2017-12-05 19:40:28.833288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2917562 -4.2766213 -4.2624459 -4.2516723 -4.244844 -4.2365503 -4.2323132 -4.2382851 -4.2415304 -4.2335067 -4.2276297 -4.2312918 -4.2429123 -4.2525215 -4.2633638][-4.2966118 -4.2857471 -4.2718244 -4.2597113 -4.2503576 -4.2440586 -4.244338 -4.2540059 -4.2607031 -4.2580218 -4.2564669 -4.2605491 -4.2685475 -4.2745757 -4.280489][-4.287014 -4.2798405 -4.26816 -4.2585111 -4.2520361 -4.2518811 -4.2583137 -4.2686 -4.2725477 -4.2691007 -4.2689648 -4.2753558 -4.2850542 -4.2883387 -4.2890716][-4.27575 -4.2694044 -4.2602854 -4.2554188 -4.2545781 -4.2613506 -4.2725325 -4.2821336 -4.2811971 -4.2742844 -4.27538 -4.2843738 -4.2944322 -4.2936072 -4.2902603][-4.2636209 -4.2536058 -4.2432604 -4.2416196 -4.2440567 -4.2551823 -4.269743 -4.28035 -4.2777128 -4.2699332 -4.2729993 -4.2842126 -4.29413 -4.2922249 -4.2902083][-4.2383928 -4.2207093 -4.208179 -4.2073479 -4.2102275 -4.2199697 -4.2366705 -4.2526937 -4.2549481 -4.2516141 -4.25613 -4.2709732 -4.2827773 -4.2831774 -4.28704][-4.2098017 -4.1838489 -4.1673031 -4.1619425 -4.1593757 -4.1639771 -4.1822262 -4.2038188 -4.2112684 -4.2096405 -4.2135959 -4.2349753 -4.2556047 -4.264317 -4.2782874][-4.195363 -4.16452 -4.1428719 -4.1332355 -4.1250625 -4.121676 -4.1342196 -4.1505876 -4.1549869 -4.15003 -4.1538348 -4.1853428 -4.2196245 -4.2411728 -4.2659321][-4.1939778 -4.16853 -4.1464634 -4.1364589 -4.1240063 -4.1114864 -4.1153717 -4.1243482 -4.1258197 -4.1219087 -4.1283221 -4.1651287 -4.2080054 -4.2387123 -4.2681637][-4.2054973 -4.1897526 -4.1726065 -4.164526 -4.1529655 -4.1392908 -4.1384544 -4.143846 -4.1490226 -4.1525631 -4.1625557 -4.1957688 -4.23192 -4.2579517 -4.28305][-4.2160106 -4.2106986 -4.1992216 -4.1939735 -4.1891112 -4.1803708 -4.1784396 -4.1831284 -4.1942334 -4.2047529 -4.2181091 -4.2464209 -4.2743926 -4.290153 -4.3067832][-4.2302289 -4.2332854 -4.2238259 -4.2160463 -4.2141476 -4.213057 -4.2150378 -4.2228484 -4.2358909 -4.2463951 -4.2605004 -4.2862835 -4.3091626 -4.3168335 -4.3262653][-4.2489381 -4.2563324 -4.2501054 -4.2408781 -4.2388926 -4.240366 -4.2433839 -4.2508326 -4.2612152 -4.2672219 -4.2795167 -4.3005662 -4.3183289 -4.3248262 -4.3333163][-4.2555614 -4.2635875 -4.2625718 -4.2555313 -4.2533836 -4.2541585 -4.2542229 -4.2575 -4.2644267 -4.2702327 -4.2826381 -4.3015246 -4.3158436 -4.3217621 -4.3313909][-4.2575245 -4.2630396 -4.260932 -4.253572 -4.2480841 -4.2439904 -4.2384648 -4.2377129 -4.2460341 -4.2574482 -4.2758207 -4.2974124 -4.3115563 -4.3182564 -4.3272214]]...]
INFO - root - 2017-12-05 19:40:38.137866: step 35110, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 76h:22m:44s remains)
INFO - root - 2017-12-05 19:40:47.257808: step 35120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 75h:17m:06s remains)
INFO - root - 2017-12-05 19:40:56.215241: step 35130, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 75h:15m:48s remains)
INFO - root - 2017-12-05 19:41:05.465236: step 35140, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 76h:04m:28s remains)
INFO - root - 2017-12-05 19:41:14.731421: step 35150, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 78h:58m:25s remains)
INFO - root - 2017-12-05 19:41:23.924600: step 35160, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 75h:47m:14s remains)
INFO - root - 2017-12-05 19:41:32.783171: step 35170, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 72h:03m:55s remains)
INFO - root - 2017-12-05 19:41:41.877147: step 35180, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 77h:20m:22s remains)
INFO - root - 2017-12-05 19:41:50.996409: step 35190, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 76h:44m:57s remains)
INFO - root - 2017-12-05 19:41:59.993470: step 35200, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 75h:42m:15s remains)
2017-12-05 19:42:00.759669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2341161 -4.2286873 -4.2292547 -4.214602 -4.178895 -4.1524835 -4.1316681 -4.1164384 -4.1171961 -4.1023979 -4.0805764 -4.0761986 -4.0826325 -4.09308 -4.1007571][-4.2319403 -4.233789 -4.232851 -4.2147703 -4.1731434 -4.1400728 -4.1222839 -4.1100225 -4.1075635 -4.0978346 -4.0830841 -4.0755596 -4.0705738 -4.0655236 -4.065937][-4.2285233 -4.233788 -4.2293568 -4.2039986 -4.1583619 -4.12231 -4.1034169 -4.0884767 -4.090251 -4.0965037 -4.0974689 -4.09252 -4.0792608 -4.0613322 -4.0562096][-4.2231169 -4.2319317 -4.2260962 -4.1960344 -4.1460085 -4.1068921 -4.0844431 -4.0611033 -4.0648026 -4.089467 -4.1126251 -4.1164918 -4.1011534 -4.0747261 -4.064992][-4.2192841 -4.2314515 -4.2287521 -4.1998048 -4.1484189 -4.1083045 -4.0762029 -4.0334153 -4.0303183 -4.0771441 -4.1245651 -4.1408682 -4.1318398 -4.1030483 -4.081903][-4.215055 -4.2277913 -4.227777 -4.2038679 -4.1564302 -4.1157336 -4.0667243 -3.9923289 -3.9748268 -4.0461969 -4.1221576 -4.1573858 -4.1598887 -4.1382113 -4.1175942][-4.2128425 -4.2223148 -4.2238169 -4.205616 -4.1623797 -4.1174445 -4.0479422 -3.9357038 -3.9008989 -4.0004849 -4.1048837 -4.1625152 -4.1791968 -4.1701961 -4.1554694][-4.21463 -4.2206869 -4.221529 -4.2066665 -4.1644182 -4.1153011 -4.0386395 -3.9136517 -3.8770814 -3.9887049 -4.1013417 -4.1691465 -4.194468 -4.1963725 -4.1870012][-4.2113404 -4.2138262 -4.2111974 -4.1979704 -4.1618943 -4.1206851 -4.0649667 -3.9769878 -3.9544973 -4.0316362 -4.1185026 -4.1767983 -4.2018123 -4.211041 -4.2083364][-4.2049446 -4.2041111 -4.1997042 -4.1868029 -4.1591 -4.1341085 -4.1067834 -4.0613856 -4.0458503 -4.0851126 -4.1392694 -4.1802607 -4.2004347 -4.213892 -4.2171397][-4.1955004 -4.1918087 -4.189208 -4.1777477 -4.157053 -4.1450572 -4.1398773 -4.1184206 -4.1035233 -4.1205225 -4.1514354 -4.1800156 -4.1954842 -4.2088447 -4.213685][-4.1775475 -4.1763873 -4.1794248 -4.1708484 -4.1543918 -4.1517487 -4.1565394 -4.1465626 -4.1308432 -4.1363273 -4.1543469 -4.1770878 -4.1876473 -4.1970859 -4.1984177][-4.1565509 -4.1602049 -4.1704431 -4.16335 -4.14928 -4.1534429 -4.1586852 -4.1513553 -4.1398377 -4.1410127 -4.1536536 -4.1741996 -4.1805687 -4.1836457 -4.1807456][-4.1349115 -4.144629 -4.1608353 -4.1552305 -4.1440983 -4.1506157 -4.15746 -4.1522751 -4.1435776 -4.140944 -4.1491094 -4.1664548 -4.1745906 -4.179564 -4.1735821][-4.1284018 -4.1447248 -4.1632843 -4.158957 -4.1456819 -4.1455994 -4.1502242 -4.1456532 -4.1371994 -4.1353426 -4.1435041 -4.1603918 -4.1766577 -4.190372 -4.1893759]]...]
INFO - root - 2017-12-05 19:42:09.899520: step 35210, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 77h:23m:46s remains)
INFO - root - 2017-12-05 19:42:19.041009: step 35220, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 76h:32m:47s remains)
INFO - root - 2017-12-05 19:42:28.150436: step 35230, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 75h:51m:14s remains)
INFO - root - 2017-12-05 19:42:37.332881: step 35240, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 73h:46m:11s remains)
INFO - root - 2017-12-05 19:42:46.478779: step 35250, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.963 sec/batch; 79h:30m:18s remains)
INFO - root - 2017-12-05 19:42:55.448228: step 35260, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 74h:24m:16s remains)
INFO - root - 2017-12-05 19:43:04.536086: step 35270, loss = 2.09, batch loss = 2.04 (8.5 examples/sec; 0.940 sec/batch; 77h:35m:41s remains)
INFO - root - 2017-12-05 19:43:13.661327: step 35280, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 74h:57m:58s remains)
INFO - root - 2017-12-05 19:43:22.592306: step 35290, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.865 sec/batch; 71h:26m:14s remains)
INFO - root - 2017-12-05 19:43:31.598414: step 35300, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 69h:59m:47s remains)
2017-12-05 19:43:32.366868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2357845 -4.2409449 -4.2549706 -4.27572 -4.28644 -4.2742486 -4.25605 -4.2390337 -4.2176709 -4.1972647 -4.1784554 -4.159399 -4.138731 -4.1024508 -4.0617447][-4.228869 -4.2354512 -4.2463064 -4.2607036 -4.2680254 -4.2550206 -4.2376933 -4.2297421 -4.2226286 -4.2120471 -4.1963673 -4.1720304 -4.1454992 -4.1116714 -4.0770483][-4.2283807 -4.2299733 -4.2310824 -4.2318492 -4.2338157 -4.220562 -4.2046518 -4.2069669 -4.21641 -4.2219296 -4.2203121 -4.2037435 -4.1847839 -4.1669145 -4.1528859][-4.2234612 -4.2159824 -4.2017155 -4.180357 -4.1640372 -4.1403565 -4.1159477 -4.11765 -4.1451941 -4.1781754 -4.206121 -4.215723 -4.2188349 -4.2227678 -4.2266626][-4.2266083 -4.207911 -4.1728411 -4.1204414 -4.0725908 -4.0218134 -3.969101 -3.9518502 -3.9963317 -4.0712037 -4.1340876 -4.1726918 -4.2035027 -4.2325506 -4.2534456][-4.2342982 -4.2025771 -4.1460304 -4.0628638 -3.9761944 -3.8803751 -3.7814593 -3.7357483 -3.8046565 -3.9280946 -4.0247169 -4.086247 -4.1434216 -4.1960053 -4.2285833][-4.2384653 -4.1986556 -4.12744 -4.030468 -3.9239097 -3.7984469 -3.6694002 -3.6065578 -3.6912704 -3.8395731 -3.9507205 -4.0282 -4.1001639 -4.16662 -4.205617][-4.2304325 -4.1973991 -4.1391177 -4.0618768 -3.9783981 -3.8797812 -3.7818365 -3.7367516 -3.795954 -3.9073715 -3.9949646 -4.0619278 -4.1254606 -4.1848984 -4.2194061][-4.227272 -4.2102418 -4.1778369 -4.1334085 -4.0869732 -4.0314951 -3.977083 -3.9505444 -3.9825456 -4.0478363 -4.1051188 -4.1525254 -4.1973419 -4.2391181 -4.2650614][-4.2214003 -4.2165079 -4.2052736 -4.1878209 -4.1734986 -4.1531696 -4.1313329 -4.1190162 -4.1348619 -4.17027 -4.2029581 -4.2351 -4.2610955 -4.2843127 -4.3024969][-4.2053 -4.2037616 -4.20097 -4.1995015 -4.2060385 -4.2128983 -4.2144327 -4.2153029 -4.2259059 -4.2428279 -4.2576294 -4.2772489 -4.2842817 -4.2865229 -4.294271][-4.1755404 -4.1719227 -4.171792 -4.1797538 -4.1983147 -4.2203841 -4.2352505 -4.2434158 -4.24939 -4.2539439 -4.2594242 -4.2679296 -4.2596354 -4.2435102 -4.239459][-4.1494861 -4.142437 -4.1445637 -4.1562123 -4.1779838 -4.2047067 -4.2285519 -4.2407684 -4.242497 -4.2406812 -4.2399964 -4.2425885 -4.2264605 -4.1974163 -4.1811709][-4.1555924 -4.1432028 -4.1429453 -4.1525145 -4.1716018 -4.2001424 -4.2276573 -4.2384424 -4.2393279 -4.2370996 -4.2358503 -4.2380266 -4.2231779 -4.191864 -4.1687531][-4.1985149 -4.1803107 -4.1710372 -4.1744452 -4.1908708 -4.2182164 -4.2457128 -4.256505 -4.2582569 -4.2579732 -4.2565756 -4.2588406 -4.2526264 -4.2292109 -4.2078347]]...]
INFO - root - 2017-12-05 19:43:41.447455: step 35310, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 73h:04m:55s remains)
INFO - root - 2017-12-05 19:43:50.478707: step 35320, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 75h:59m:40s remains)
INFO - root - 2017-12-05 19:43:59.611679: step 35330, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 73h:18m:17s remains)
INFO - root - 2017-12-05 19:44:08.858019: step 35340, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 74h:43m:59s remains)
INFO - root - 2017-12-05 19:44:18.037713: step 35350, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 74h:39m:25s remains)
INFO - root - 2017-12-05 19:44:26.967984: step 35360, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 75h:39m:17s remains)
INFO - root - 2017-12-05 19:44:36.114509: step 35370, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.945 sec/batch; 78h:01m:36s remains)
INFO - root - 2017-12-05 19:44:45.185849: step 35380, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 75h:09m:36s remains)
INFO - root - 2017-12-05 19:44:54.246127: step 35390, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 74h:24m:24s remains)
INFO - root - 2017-12-05 19:45:03.321041: step 35400, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 73h:03m:39s remains)
2017-12-05 19:45:04.107690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2978163 -4.2891588 -4.2644587 -4.2368169 -4.2154984 -4.1974287 -4.1893191 -4.1902556 -4.1971917 -4.2007666 -4.1986003 -4.1945376 -4.2002535 -4.2137647 -4.2232785][-4.3000693 -4.2855692 -4.2494912 -4.2105465 -4.1798291 -4.1542091 -4.1457047 -4.1514997 -4.16461 -4.172236 -4.1688337 -4.1622996 -4.1685324 -4.1820817 -4.1954212][-4.3002124 -4.2829957 -4.2413564 -4.1946759 -4.1525245 -4.1156259 -4.099968 -4.1062961 -4.12928 -4.1429086 -4.1402378 -4.1381121 -4.1486607 -4.1603808 -4.1740289][-4.3038497 -4.2883019 -4.24677 -4.1971421 -4.1461439 -4.094583 -4.0638952 -4.0714331 -4.1087017 -4.1325254 -4.1375051 -4.1433692 -4.1529555 -4.1585331 -4.1697807][-4.308567 -4.2951636 -4.2563305 -4.2061362 -4.1437626 -4.0687666 -4.0170507 -4.0314074 -4.092535 -4.136559 -4.1555896 -4.1661396 -4.1726809 -4.1731076 -4.1782732][-4.3103151 -4.294086 -4.250483 -4.19081 -4.1077909 -4.00218 -3.9225497 -3.9539909 -4.051878 -4.1225958 -4.1579528 -4.1776857 -4.1845741 -4.1818309 -4.1824274][-4.3077345 -4.2858114 -4.2347031 -4.1601396 -4.0537858 -3.9126382 -3.815814 -3.8876541 -4.01909 -4.1033573 -4.1492853 -4.1734705 -4.1802936 -4.180253 -4.1835189][-4.3002405 -4.2729611 -4.2173982 -4.1389861 -4.03392 -3.9118614 -3.8581285 -3.942414 -4.0490108 -4.1080737 -4.1401906 -4.1560483 -4.1610627 -4.1687241 -4.1828709][-4.2926741 -4.2633166 -4.2116189 -4.1498218 -4.0805736 -4.0145845 -3.99794 -4.0520868 -4.1080656 -4.134728 -4.1488385 -4.1529379 -4.1542077 -4.1720252 -4.1984272][-4.2873297 -4.2608385 -4.222888 -4.1842113 -4.1461716 -4.1136837 -4.1098723 -4.1376648 -4.1584454 -4.1684313 -4.1706557 -4.1660814 -4.1680784 -4.1943235 -4.2233133][-4.2906146 -4.2695751 -4.2441144 -4.2190943 -4.195365 -4.1770277 -4.1769705 -4.1906476 -4.1954269 -4.1982465 -4.1962953 -4.1895008 -4.195159 -4.2216291 -4.24364][-4.3000507 -4.2849255 -4.2642231 -4.2419214 -4.2227082 -4.2103391 -4.2141252 -4.2278724 -4.2338123 -4.2335553 -4.2274876 -4.2185955 -4.2236676 -4.2443347 -4.2567091][-4.3096275 -4.2983708 -4.2776375 -4.2532716 -4.2351475 -4.2268147 -4.234509 -4.2532 -4.2640758 -4.2613168 -4.2505322 -4.2383842 -4.2407455 -4.2537279 -4.2585816][-4.3147769 -4.3026953 -4.2791719 -4.2531261 -4.2351379 -4.227634 -4.2366714 -4.2564359 -4.2677383 -4.2630067 -4.2518625 -4.242795 -4.2468066 -4.2568851 -4.2576213][-4.3165846 -4.3026643 -4.2753258 -4.2464714 -4.22426 -4.2117505 -4.2190537 -4.2411985 -4.2561617 -4.2517123 -4.2425175 -4.2396193 -4.2476263 -4.2557869 -4.2523441]]...]
INFO - root - 2017-12-05 19:45:13.009254: step 35410, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 73h:11m:41s remains)
INFO - root - 2017-12-05 19:45:22.254493: step 35420, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:25m:45s remains)
INFO - root - 2017-12-05 19:45:31.274656: step 35430, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 75h:36m:42s remains)
INFO - root - 2017-12-05 19:45:40.387642: step 35440, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 74h:07m:20s remains)
INFO - root - 2017-12-05 19:45:49.328886: step 35450, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 70h:31m:31s remains)
INFO - root - 2017-12-05 19:45:58.447231: step 35460, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 76h:55m:37s remains)
INFO - root - 2017-12-05 19:46:07.546829: step 35470, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 75h:57m:38s remains)
INFO - root - 2017-12-05 19:46:16.664906: step 35480, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 78h:07m:58s remains)
INFO - root - 2017-12-05 19:46:25.821181: step 35490, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 75h:38m:37s remains)
INFO - root - 2017-12-05 19:46:34.986448: step 35500, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.915 sec/batch; 75h:31m:41s remains)
2017-12-05 19:46:35.767313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3161068 -4.2967792 -4.271143 -4.2489824 -4.2250147 -4.1982632 -4.1746855 -4.182663 -4.2130985 -4.2305655 -4.2420359 -4.2471876 -4.2508807 -4.2570806 -4.2658639][-4.300137 -4.2704606 -4.2342772 -4.1986561 -4.1618729 -4.1250567 -4.0909019 -4.097199 -4.1395731 -4.1661143 -4.1825795 -4.1863141 -4.1890006 -4.1997805 -4.2181082][-4.2900929 -4.2562046 -4.2136831 -4.1675024 -4.1203284 -4.0778675 -4.0367708 -4.0391026 -4.0895162 -4.1187973 -4.1301603 -4.125432 -4.1249232 -4.1410418 -4.1678696][-4.2858005 -4.2427869 -4.1883335 -4.1304922 -4.0729971 -4.0270758 -3.9860744 -3.9906876 -4.0398159 -4.0584579 -4.05819 -4.0504661 -4.0532484 -4.0754337 -4.1084304][-4.2774668 -4.222496 -4.1519747 -4.0828214 -4.0198455 -3.9775949 -3.9360466 -3.9323323 -3.9726074 -3.9781628 -3.9679391 -3.9653618 -3.9826541 -4.0191169 -4.0598569][-4.266613 -4.2050114 -4.12896 -4.0606308 -4.002563 -3.9612119 -3.9022048 -3.8678498 -3.89013 -3.8968375 -3.8909364 -3.8992536 -3.9337633 -3.980881 -4.0238652][-4.2577262 -4.193399 -4.1192579 -4.05671 -3.9978192 -3.9410467 -3.8539588 -3.7870266 -3.8013785 -3.8318086 -3.8465545 -3.866672 -3.9086509 -3.950937 -3.9913032][-4.2531695 -4.1880083 -4.1195807 -4.0607896 -3.9990501 -3.9331923 -3.84162 -3.7792063 -3.8021088 -3.8448329 -3.8603947 -3.8787851 -3.9127264 -3.9415016 -3.9822426][-4.2602825 -4.2018251 -4.1407657 -4.0900588 -4.0386848 -3.9891715 -3.9293728 -3.8973134 -3.9221058 -3.9531174 -3.950604 -3.9582238 -3.982537 -4.0001569 -4.0362744][-4.279603 -4.2321978 -4.1801558 -4.1375856 -4.0966377 -4.0660281 -4.0336776 -4.0161481 -4.0350294 -4.0543566 -4.0483012 -4.0571065 -4.0829244 -4.0958872 -4.1223159][-4.2958193 -4.260148 -4.2204962 -4.1897259 -4.1604819 -4.1392922 -4.1174388 -4.10468 -4.1149654 -4.127779 -4.126349 -4.1418624 -4.1703806 -4.181572 -4.195745][-4.3084264 -4.2858286 -4.2607288 -4.2427249 -4.2250705 -4.2104011 -4.1943312 -4.183774 -4.1877894 -4.1988883 -4.2037187 -4.22117 -4.2448087 -4.24886 -4.2546525][-4.3191161 -4.3059125 -4.2906628 -4.2798066 -4.2707248 -4.2629895 -4.25173 -4.244236 -4.2513041 -4.267293 -4.2785645 -4.2917366 -4.3048005 -4.30229 -4.3041773][-4.3292618 -4.322546 -4.3132625 -4.3050995 -4.2991848 -4.2949867 -4.2861362 -4.2817321 -4.290616 -4.3048115 -4.3149028 -4.3232489 -4.3301668 -4.3278484 -4.329309][-4.3366494 -4.332551 -4.3264332 -4.3195004 -4.3142104 -4.3101344 -4.3033371 -4.3009014 -4.3085594 -4.3185215 -4.3253155 -4.3299432 -4.3330703 -4.3327408 -4.3351569]]...]
INFO - root - 2017-12-05 19:46:44.654185: step 35510, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 75h:03m:28s remains)
INFO - root - 2017-12-05 19:46:53.692060: step 35520, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.924 sec/batch; 76h:12m:00s remains)
INFO - root - 2017-12-05 19:47:02.814088: step 35530, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 73h:20m:01s remains)
INFO - root - 2017-12-05 19:47:11.884444: step 35540, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 74h:14m:39s remains)
INFO - root - 2017-12-05 19:47:21.011888: step 35550, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 76h:12m:38s remains)
INFO - root - 2017-12-05 19:47:30.060590: step 35560, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 75h:33m:03s remains)
INFO - root - 2017-12-05 19:47:39.066834: step 35570, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 76h:18m:47s remains)
INFO - root - 2017-12-05 19:47:48.203608: step 35580, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 74h:19m:05s remains)
INFO - root - 2017-12-05 19:47:57.237620: step 35590, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 73h:38m:18s remains)
INFO - root - 2017-12-05 19:48:06.137438: step 35600, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 72h:35m:42s remains)
2017-12-05 19:48:06.916380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2784324 -4.2442904 -4.2050338 -4.1623149 -4.1239882 -4.1064005 -4.1317868 -4.1836205 -4.2204156 -4.2526388 -4.2886562 -4.3150334 -4.33233 -4.3356924 -4.3301249][-4.2748952 -4.2400479 -4.19777 -4.1491556 -4.1016436 -4.0733418 -4.09798 -4.1561332 -4.1994781 -4.2409687 -4.2858458 -4.3183656 -4.3366675 -4.3385053 -4.3307896][-4.2735729 -4.2351241 -4.1866198 -4.1302447 -4.0715709 -4.0319166 -4.0602007 -4.1299224 -4.1857896 -4.234674 -4.2874284 -4.3238721 -4.3414259 -4.341136 -4.3309817][-4.2732668 -4.2321391 -4.17783 -4.1134205 -4.0443821 -3.9947679 -4.0237117 -4.1057887 -4.1772704 -4.2345948 -4.2918553 -4.330174 -4.3457427 -4.342689 -4.3289089][-4.271121 -4.2313194 -4.1741881 -4.102078 -4.0261407 -3.9712062 -3.9917929 -4.0819449 -4.1670942 -4.2322121 -4.2924147 -4.3325529 -4.3475847 -4.3418393 -4.3238068][-4.2689853 -4.2342062 -4.1769238 -4.098701 -4.0173025 -3.953234 -3.9563415 -4.0494919 -4.1483812 -4.220037 -4.2849627 -4.3279095 -4.342823 -4.33602 -4.3166761][-4.2656393 -4.2361717 -4.1785831 -4.0934277 -3.9997311 -3.9198523 -3.9032922 -4.0017328 -4.1175823 -4.2000852 -4.2711387 -4.3161469 -4.3304777 -4.324048 -4.3050365][-4.2601047 -4.2343907 -4.1766233 -4.08553 -3.9735932 -3.8694506 -3.8334937 -3.9345603 -4.0688038 -4.1679463 -4.2484255 -4.2966638 -4.3135705 -4.309442 -4.2901158][-4.2564635 -4.2349987 -4.1821175 -4.0911369 -3.9680903 -3.8496532 -3.794641 -3.8833151 -4.0267177 -4.1402345 -4.2288065 -4.2811556 -4.3002381 -4.2973485 -4.2773046][-4.2568316 -4.2407303 -4.1981349 -4.1195073 -4.007627 -3.898756 -3.834048 -3.8898787 -4.0198679 -4.1326294 -4.2181129 -4.2688427 -4.2900391 -4.2881441 -4.2676682][-4.2586837 -4.2454448 -4.2138934 -4.1504059 -4.0584369 -3.9700372 -3.9112473 -3.9396021 -4.0425491 -4.1409097 -4.2153821 -4.2595406 -4.2806315 -4.2802048 -4.2615185][-4.262043 -4.2486849 -4.2251945 -4.1767907 -4.1046791 -4.038661 -3.9958742 -4.012105 -4.0882239 -4.1677127 -4.2274365 -4.2615409 -4.27786 -4.277194 -4.2605081][-4.2666173 -4.2547684 -4.2393236 -4.2071509 -4.1569443 -4.1101542 -4.083972 -4.0961242 -4.14958 -4.2090087 -4.2527018 -4.2750421 -4.2821112 -4.2775393 -4.2607322][-4.2704191 -4.2620373 -4.2543793 -4.2365208 -4.2056704 -4.1766853 -4.1635656 -4.1722326 -4.2069993 -4.2477179 -4.2776818 -4.2895627 -4.2883682 -4.2803073 -4.2644205][-4.2740021 -4.2680078 -4.2653227 -4.2567577 -4.24045 -4.2244754 -4.219192 -4.2252755 -4.2464266 -4.2716818 -4.2900929 -4.2960753 -4.2918291 -4.2837043 -4.2711854]]...]
INFO - root - 2017-12-05 19:48:16.227881: step 35610, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 76h:56m:44s remains)
INFO - root - 2017-12-05 19:48:25.340772: step 35620, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 74h:06m:19s remains)
INFO - root - 2017-12-05 19:48:34.213013: step 35630, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 71h:09m:04s remains)
INFO - root - 2017-12-05 19:48:43.356402: step 35640, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.966 sec/batch; 79h:41m:37s remains)
INFO - root - 2017-12-05 19:48:52.479131: step 35650, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.937 sec/batch; 77h:15m:58s remains)
INFO - root - 2017-12-05 19:49:01.531605: step 35660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 75h:32m:55s remains)
INFO - root - 2017-12-05 19:49:10.555259: step 35670, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 73h:52m:23s remains)
INFO - root - 2017-12-05 19:49:19.775699: step 35680, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 76h:26m:41s remains)
INFO - root - 2017-12-05 19:49:28.842945: step 35690, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 76h:16m:34s remains)
INFO - root - 2017-12-05 19:49:37.904325: step 35700, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 72h:26m:43s remains)
2017-12-05 19:49:38.792767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1467185 -4.113975 -4.1063733 -4.1348586 -4.1773729 -4.1866732 -4.1583667 -4.13007 -4.1339884 -4.1683426 -4.2141247 -4.2574277 -4.2787976 -4.2759871 -4.2551026][-4.168396 -4.1243072 -4.1059079 -4.1204391 -4.1509676 -4.1542497 -4.1274562 -4.1044612 -4.117012 -4.159935 -4.2103391 -4.2536321 -4.2726989 -4.2704573 -4.25311][-4.2054663 -4.1645 -4.1381388 -4.1331429 -4.1419435 -4.130394 -4.0985332 -4.073698 -4.0888577 -4.1404409 -4.1984572 -4.2421937 -4.2572732 -4.2562304 -4.2451749][-4.2262416 -4.200027 -4.1783476 -4.162878 -4.1515508 -4.1230068 -4.0764375 -4.0387621 -4.0462284 -4.1073232 -4.1780791 -4.2255549 -4.239943 -4.2409544 -4.2371154][-4.2208934 -4.2140117 -4.2075415 -4.1954279 -4.1705365 -4.1261573 -4.0586691 -4.0006 -4.0017629 -4.0737171 -4.1575813 -4.2097282 -4.2232838 -4.2264643 -4.2285576][-4.1947527 -4.2036324 -4.2125969 -4.2115469 -4.1880283 -4.1365285 -4.0537367 -3.979759 -3.9807827 -4.0602469 -4.1481795 -4.2046041 -4.21679 -4.220696 -4.22268][-4.1634784 -4.1851082 -4.205472 -4.2124066 -4.1931334 -4.139029 -4.0508552 -3.9759407 -3.985507 -4.0697789 -4.156271 -4.2147846 -4.2263522 -4.2277255 -4.2275023][-4.1543026 -4.1790032 -4.1986051 -4.2052503 -4.1822414 -4.1218119 -4.0390725 -3.9799981 -4.0059342 -4.0944638 -4.1798768 -4.2338295 -4.2442465 -4.2431488 -4.2434969][-4.1716094 -4.1923232 -4.2025681 -4.1990585 -4.1642718 -4.0971684 -4.0232115 -3.988029 -4.0308971 -4.1213865 -4.2030182 -4.2516985 -4.2616191 -4.2592955 -4.2604418][-4.2023993 -4.21753 -4.2154179 -4.1972122 -4.15286 -4.0857191 -4.0277619 -4.0158558 -4.068583 -4.1491003 -4.220561 -4.2627921 -4.2728267 -4.27227 -4.2719564][-4.2267046 -4.2355676 -4.2193551 -4.1894555 -4.1469874 -4.0947609 -4.0623612 -4.0687237 -4.1168642 -4.1755819 -4.2284904 -4.261755 -4.2730541 -4.2763524 -4.2726026][-4.2394209 -4.2421155 -4.2182755 -4.1818757 -4.1418681 -4.1083708 -4.1030736 -4.125493 -4.167809 -4.2064381 -4.2350245 -4.2529154 -4.2610164 -4.2647443 -4.2615418][-4.2512894 -4.2449255 -4.2155166 -4.1759281 -4.1368012 -4.1179523 -4.1327238 -4.167 -4.2061357 -4.2297659 -4.2381182 -4.2414889 -4.2438512 -4.2462525 -4.246767][-4.2682867 -4.2509179 -4.2140436 -4.1681948 -4.1273141 -4.1194792 -4.1471267 -4.1884074 -4.2261167 -4.2450285 -4.2461753 -4.2399158 -4.2347031 -4.2361269 -4.2406974][-4.2811279 -4.2546549 -4.2091937 -4.1560593 -4.1164131 -4.117157 -4.1500258 -4.1950779 -4.2352853 -4.2575173 -4.2624817 -4.25652 -4.2482791 -4.2468796 -4.2488918]]...]
INFO - root - 2017-12-05 19:49:47.808483: step 35710, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 74h:50m:04s remains)
INFO - root - 2017-12-05 19:49:56.951523: step 35720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 75h:50m:01s remains)
INFO - root - 2017-12-05 19:50:05.961030: step 35730, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 73h:41m:56s remains)
INFO - root - 2017-12-05 19:50:15.042810: step 35740, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 78h:36m:37s remains)
INFO - root - 2017-12-05 19:50:24.326161: step 35750, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 74h:46m:29s remains)
INFO - root - 2017-12-05 19:50:33.453869: step 35760, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 74h:13m:34s remains)
INFO - root - 2017-12-05 19:50:42.488046: step 35770, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.891 sec/batch; 73h:25m:51s remains)
INFO - root - 2017-12-05 19:50:51.547827: step 35780, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 73h:55m:05s remains)
INFO - root - 2017-12-05 19:51:00.476245: step 35790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 70h:23m:07s remains)
INFO - root - 2017-12-05 19:51:09.437730: step 35800, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 72h:42m:11s remains)
2017-12-05 19:51:10.231219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9810791 -4.0588031 -4.134871 -4.1981015 -4.2279248 -4.2330465 -4.2205319 -4.1646113 -4.0562015 -3.9527674 -3.9226761 -3.9847524 -4.0850782 -4.1898446 -4.25427][-4.0722618 -4.1393809 -4.1884675 -4.22863 -4.2487264 -4.2576213 -4.2543468 -4.2251768 -4.1653757 -4.1125207 -4.10352 -4.1351972 -4.1851773 -4.2481465 -4.2865829][-4.1411624 -4.2005091 -4.2330885 -4.2551622 -4.2643943 -4.2629085 -4.2524686 -4.2395215 -4.2183514 -4.2059951 -4.2192612 -4.2432041 -4.269628 -4.3030324 -4.3216419][-4.1886859 -4.2340651 -4.2542591 -4.2676854 -4.2655716 -4.2399521 -4.1996937 -4.1847277 -4.2004032 -4.2304678 -4.272902 -4.3072085 -4.32582 -4.341177 -4.3427477][-4.2318497 -4.2503953 -4.2518744 -4.2458987 -4.2183037 -4.1550765 -4.0713911 -4.0386214 -4.0857525 -4.1720166 -4.2587724 -4.32328 -4.3497043 -4.3573027 -4.3475552][-4.2645535 -4.2524447 -4.2289305 -4.1884146 -4.1108441 -3.9831853 -3.8373876 -3.7724509 -3.862685 -4.0295219 -4.1764 -4.2838054 -4.3352838 -4.3538151 -4.34731][-4.2881279 -4.2610331 -4.2205215 -4.1474128 -4.0200562 -3.8199854 -3.5836196 -3.4435914 -3.5691068 -3.8303924 -4.0486913 -4.2013531 -4.2841973 -4.3237839 -4.3304791][-4.2984529 -4.273653 -4.23455 -4.1648331 -4.0419116 -3.8296094 -3.5516315 -3.3305459 -3.41207 -3.6946344 -3.9421091 -4.116786 -4.221406 -4.2802753 -4.2995048][-4.29053 -4.2694798 -4.240459 -4.1958094 -4.1175303 -3.9630413 -3.7490461 -3.5590093 -3.5754571 -3.7607942 -3.9408796 -4.0863175 -4.1861887 -4.2494755 -4.2753124][-4.2796187 -4.2668076 -4.2455921 -4.2202458 -4.1771593 -4.0824895 -3.945766 -3.8229554 -3.8194871 -3.914289 -4.0210404 -4.11869 -4.1945586 -4.2451482 -4.2695637][-4.2703652 -4.268157 -4.2581139 -4.2452021 -4.2207379 -4.164001 -4.0840931 -4.0075784 -3.9888461 -4.0231252 -4.0861254 -4.1567373 -4.2188067 -4.2582669 -4.2774363][-4.2632875 -4.2733412 -4.2790623 -4.2764897 -4.26422 -4.2318277 -4.1823239 -4.1213756 -4.0763416 -4.0624571 -4.0911078 -4.1459775 -4.209218 -4.2555528 -4.2805963][-4.2611723 -4.2760968 -4.2919264 -4.2990766 -4.2970114 -4.2832346 -4.2511797 -4.1977077 -4.1337247 -4.0774384 -4.0670624 -4.098146 -4.1616611 -4.2218618 -4.2642832][-4.2668276 -4.27938 -4.2927608 -4.2992554 -4.3007646 -4.2978663 -4.2800622 -4.2417164 -4.1815038 -4.1083689 -4.0632877 -4.0638027 -4.114 -4.18249 -4.2439547][-4.2738724 -4.2814555 -4.2862535 -4.2848253 -4.28334 -4.2838492 -4.2761197 -4.2532287 -4.2100978 -4.1449041 -4.0911574 -4.0732937 -4.1071725 -4.1736312 -4.2431831]]...]
INFO - root - 2017-12-05 19:51:19.379532: step 35810, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 71h:09m:53s remains)
INFO - root - 2017-12-05 19:51:28.630954: step 35820, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 75h:03m:50s remains)
INFO - root - 2017-12-05 19:51:37.418746: step 35830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 75h:17m:50s remains)
INFO - root - 2017-12-05 19:51:46.649442: step 35840, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.928 sec/batch; 76h:30m:25s remains)
INFO - root - 2017-12-05 19:51:55.810735: step 35850, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 73h:11m:25s remains)
INFO - root - 2017-12-05 19:52:04.876912: step 35860, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 77h:10m:10s remains)
INFO - root - 2017-12-05 19:52:14.081117: step 35870, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.936 sec/batch; 77h:05m:37s remains)
INFO - root - 2017-12-05 19:52:23.259583: step 35880, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 76h:13m:29s remains)
INFO - root - 2017-12-05 19:52:32.312151: step 35890, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 75h:04m:09s remains)
INFO - root - 2017-12-05 19:52:41.352847: step 35900, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 72h:18m:43s remains)
2017-12-05 19:52:42.250523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1460428 -4.1651497 -4.1694741 -4.1665187 -4.1688519 -4.1741271 -4.1665788 -4.1486254 -4.1321793 -4.14546 -4.1825323 -4.2176447 -4.2385535 -4.2450523 -4.2308969][-4.1457415 -4.1587534 -4.1602211 -4.1509147 -4.145874 -4.1466002 -4.1414409 -4.1312451 -4.1226835 -4.1393518 -4.1826673 -4.2307787 -4.2613783 -4.2691045 -4.2475476][-4.1521664 -4.154232 -4.1531763 -4.1399789 -4.1275544 -4.1205392 -4.1214204 -4.12753 -4.1332011 -4.1520171 -4.1926093 -4.2436647 -4.2842155 -4.2970014 -4.2753267][-4.1782188 -4.1653075 -4.1517849 -4.1327367 -4.112258 -4.0948029 -4.0960212 -4.1159973 -4.13731 -4.1579628 -4.1920924 -4.2431874 -4.2891326 -4.3074064 -4.2914715][-4.1991854 -4.1718583 -4.1425061 -4.1193376 -4.0993748 -4.0780082 -4.0760312 -4.09263 -4.1197162 -4.1469722 -4.1818056 -4.2322326 -4.2771354 -4.2982845 -4.296092][-4.2127771 -4.1792831 -4.1415205 -4.1161871 -4.0997787 -4.0744257 -4.057261 -4.0505981 -4.0662608 -4.1020088 -4.1536045 -4.2117648 -4.2584987 -4.2867451 -4.29564][-4.2248583 -4.195828 -4.1577253 -4.1295342 -4.1069593 -4.0702305 -4.0248327 -3.9760406 -3.9683912 -4.0248156 -4.1085315 -4.1828561 -4.2401137 -4.2750864 -4.2866626][-4.2346954 -4.2160921 -4.1827059 -4.1471791 -4.1089454 -4.0586381 -3.9887917 -3.9045513 -3.8841782 -3.9729333 -4.0822167 -4.1579571 -4.216764 -4.2547617 -4.2640467][-4.231307 -4.2235875 -4.1960206 -4.1557 -4.1024756 -4.045011 -3.9826214 -3.9190784 -3.9117794 -3.9914389 -4.0763283 -4.1340923 -4.1921806 -4.231215 -4.2371235][-4.2089067 -4.213182 -4.1927004 -4.1556883 -4.1074352 -4.0640478 -4.0292354 -3.9948204 -3.9866431 -4.0208588 -4.06429 -4.10859 -4.1664991 -4.2017069 -4.2089858][-4.1795726 -4.1924109 -4.180326 -4.1569724 -4.1333308 -4.1155758 -4.0984917 -4.0690279 -4.0389123 -4.0279193 -4.0422211 -4.0838509 -4.1417618 -4.1760869 -4.1880474][-4.1564088 -4.1717305 -4.164433 -4.1489115 -4.1417742 -4.142786 -4.1379871 -4.111423 -4.0703206 -4.0307884 -4.0287652 -4.0756 -4.1336179 -4.1648245 -4.1738043][-4.1441321 -4.1546068 -4.1493163 -4.1367459 -4.131856 -4.1383629 -4.1455736 -4.1302695 -4.0978694 -4.0617733 -4.0674276 -4.1100483 -4.1456823 -4.15077 -4.1477289][-4.143785 -4.1486559 -4.1443958 -4.1296573 -4.1195 -4.1273875 -4.1453471 -4.1478839 -4.1368837 -4.1198716 -4.1309543 -4.147357 -4.144393 -4.1238594 -4.115356][-4.1559687 -4.1585884 -4.1544304 -4.1357584 -4.1221924 -4.1385155 -4.1690645 -4.1867676 -4.1917238 -4.18491 -4.1842723 -4.1734753 -4.1427684 -4.1128435 -4.1050644]]...]
INFO - root - 2017-12-05 19:52:51.355526: step 35910, loss = 2.10, batch loss = 2.05 (8.8 examples/sec; 0.906 sec/batch; 74h:38m:53s remains)
INFO - root - 2017-12-05 19:53:00.437230: step 35920, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.935 sec/batch; 76h:59m:18s remains)
INFO - root - 2017-12-05 19:53:09.488120: step 35930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 76h:53m:17s remains)
INFO - root - 2017-12-05 19:53:18.640216: step 35940, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 75h:25m:01s remains)
INFO - root - 2017-12-05 19:53:27.749066: step 35950, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:17m:41s remains)
INFO - root - 2017-12-05 19:53:36.788116: step 35960, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 75h:13m:12s remains)
INFO - root - 2017-12-05 19:53:45.746850: step 35970, loss = 2.04, batch loss = 1.98 (10.0 examples/sec; 0.798 sec/batch; 65h:42m:49s remains)
INFO - root - 2017-12-05 19:53:54.809210: step 35980, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 74h:54m:49s remains)
INFO - root - 2017-12-05 19:54:03.738013: step 35990, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:14m:48s remains)
INFO - root - 2017-12-05 19:54:12.922942: step 36000, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 75h:40m:03s remains)
2017-12-05 19:54:13.690996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3066316 -4.2992778 -4.2970152 -4.2985711 -4.3015456 -4.3054228 -4.3076615 -4.3071551 -4.3073006 -4.3070912 -4.3083053 -4.3108568 -4.31412 -4.3178625 -4.3212962][-4.2977877 -4.2889447 -4.2880135 -4.2916551 -4.2952876 -4.3000612 -4.3024812 -4.3010097 -4.2993612 -4.297019 -4.296977 -4.3004189 -4.3058853 -4.312736 -4.319838][-4.283536 -4.2705178 -4.2675352 -4.2718987 -4.2764111 -4.2797389 -4.2791781 -4.2745214 -4.2707596 -4.2674241 -4.2689295 -4.2769632 -4.2882934 -4.3025236 -4.3162251][-4.262506 -4.2431216 -4.2353907 -4.2359495 -4.2422371 -4.2459388 -4.2385263 -4.2260332 -4.21841 -4.2150478 -4.223815 -4.2413826 -4.2610264 -4.2824812 -4.30411][-4.2448354 -4.2202859 -4.2064266 -4.201355 -4.2062922 -4.2099581 -4.1918488 -4.1658611 -4.153883 -4.1515479 -4.1653333 -4.1874304 -4.2122293 -4.2437038 -4.2768416][-4.2291842 -4.1965427 -4.1723952 -4.1583748 -4.1564021 -4.1520495 -4.1172075 -4.0721831 -4.0624614 -4.0655136 -4.0885558 -4.1221948 -4.1549497 -4.1991768 -4.2476597][-4.2182765 -4.1766047 -4.1391549 -4.1080422 -4.0870762 -4.0636148 -4.0016327 -3.9294362 -3.9331269 -3.9560921 -3.9996436 -4.0561481 -4.10242 -4.1552658 -4.2152166][-4.2057977 -4.1543088 -4.1003046 -4.047936 -4.0010061 -3.95216 -3.8622272 -3.7574012 -3.7779324 -3.8343511 -3.9061704 -3.9919887 -4.0536976 -4.1140985 -4.1812325][-4.19546 -4.1418447 -4.081995 -4.0241137 -3.9705455 -3.9163749 -3.8292389 -3.7261202 -3.7538886 -3.8246446 -3.8994875 -3.9898381 -4.0529866 -4.1091204 -4.1736741][-4.2023358 -4.1611738 -4.1151304 -4.0725021 -4.031477 -3.9982657 -3.9439623 -3.8718567 -3.8831871 -3.9325509 -3.9885371 -4.0607638 -4.1104593 -4.152658 -4.2024736][-4.2254124 -4.1970568 -4.1634779 -4.1338768 -4.1095591 -4.096868 -4.0675879 -4.0222878 -4.0209866 -4.0496192 -4.0896959 -4.1458192 -4.18526 -4.2083974 -4.2402811][-4.262856 -4.2453165 -4.221067 -4.1988792 -4.1844897 -4.1822977 -4.1681471 -4.1433959 -4.137464 -4.1533222 -4.1821189 -4.22409 -4.2523007 -4.26014 -4.2762351][-4.2914381 -4.2777371 -4.2592525 -4.242476 -4.2335563 -4.2352648 -4.229465 -4.2175727 -4.2081842 -4.2153144 -4.2356668 -4.2654366 -4.2890544 -4.293972 -4.3031683][-4.3086987 -4.2974129 -4.2839208 -4.2727976 -4.2693257 -4.2736387 -4.2721181 -4.2645159 -4.254684 -4.2572794 -4.2723708 -4.2947598 -4.3127732 -4.3160095 -4.3189759][-4.3230853 -4.3165622 -4.3104262 -4.3065314 -4.3075433 -4.3119874 -4.3133922 -4.31058 -4.305007 -4.3069234 -4.3166833 -4.3286114 -4.3355064 -4.3324265 -4.3291416]]...]
INFO - root - 2017-12-05 19:54:22.742676: step 36010, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 75h:56m:16s remains)
INFO - root - 2017-12-05 19:54:31.712822: step 36020, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 73h:25m:00s remains)
INFO - root - 2017-12-05 19:54:40.779245: step 36030, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 75h:12m:07s remains)
INFO - root - 2017-12-05 19:54:49.811898: step 36040, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 74h:36m:54s remains)
INFO - root - 2017-12-05 19:54:59.055432: step 36050, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 76h:34m:31s remains)
INFO - root - 2017-12-05 19:55:08.137902: step 36060, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 73h:43m:20s remains)
INFO - root - 2017-12-05 19:55:17.109607: step 36070, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 74h:51m:13s remains)
INFO - root - 2017-12-05 19:55:26.187168: step 36080, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 74h:54m:31s remains)
INFO - root - 2017-12-05 19:55:35.314087: step 36090, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.921 sec/batch; 75h:50m:49s remains)
INFO - root - 2017-12-05 19:55:44.458136: step 36100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 74h:56m:56s remains)
2017-12-05 19:55:45.347239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1912494 -4.2087197 -4.2238441 -4.23138 -4.2263789 -4.2116795 -4.2059751 -4.2158065 -4.2279506 -4.2340107 -4.24542 -4.2624846 -4.2713332 -4.2644496 -4.2502933][-4.148181 -4.168241 -4.1888289 -4.1997666 -4.1920247 -4.1722689 -4.1597095 -4.1709747 -4.191205 -4.2042785 -4.220325 -4.2405977 -4.2514806 -4.2447329 -4.2292066][-4.1402569 -4.157114 -4.1741662 -4.1843848 -4.1731019 -4.1494789 -4.1297665 -4.1377354 -4.1634789 -4.1876664 -4.2089782 -4.228487 -4.2390289 -4.2326927 -4.2173347][-4.1415553 -4.1538854 -4.167891 -4.1751218 -4.1604652 -4.1354814 -4.111527 -4.1111164 -4.1351342 -4.1678662 -4.1959105 -4.2173729 -4.2293758 -4.2255812 -4.2128115][-4.1632457 -4.1696534 -4.1779585 -4.1790829 -4.1630125 -4.1373205 -4.1089168 -4.0986357 -4.1183076 -4.1567268 -4.1906896 -4.2143879 -4.2267184 -4.2256188 -4.2160287][-4.2055187 -4.203815 -4.2022457 -4.1952696 -4.1790376 -4.1500669 -4.11162 -4.0913124 -4.1080871 -4.151432 -4.1922879 -4.2172828 -4.2288404 -4.2282524 -4.2220516][-4.2446904 -4.23507 -4.2230663 -4.2079344 -4.1913438 -4.16059 -4.1094813 -4.0748625 -4.0867209 -4.1352496 -4.1869698 -4.2167253 -4.2267861 -4.2231264 -4.2191138][-4.2722588 -4.2567468 -4.2393141 -4.2206335 -4.2035728 -4.1754856 -4.1179647 -4.0683422 -4.0715194 -4.1209354 -4.1826086 -4.2207451 -4.2310057 -4.2251191 -4.2209191][-4.277102 -4.2582974 -4.2391233 -4.224689 -4.20881 -4.1821713 -4.124876 -4.0702953 -4.0658765 -4.1148171 -4.1822529 -4.2246828 -4.2346368 -4.2264142 -4.2216706][-4.2725592 -4.2527227 -4.23251 -4.2207823 -4.2084012 -4.1841087 -4.1384172 -4.090178 -4.0819454 -4.1235 -4.1835389 -4.2207346 -4.2278905 -4.2183132 -4.2130876][-4.2610564 -4.2412305 -4.2224283 -4.21398 -4.2093334 -4.1928639 -4.1653986 -4.1331949 -4.1219912 -4.1486659 -4.1897736 -4.21504 -4.216548 -4.2054515 -4.19846][-4.2405519 -4.222496 -4.2087107 -4.2056017 -4.20718 -4.1967664 -4.1818657 -4.1609859 -4.1485581 -4.1650667 -4.1912093 -4.2083244 -4.2052212 -4.1948586 -4.1882443][-4.2196703 -4.2046442 -4.1958928 -4.1952233 -4.1957068 -4.1843705 -4.1731992 -4.1542368 -4.1426725 -4.1597748 -4.1805148 -4.1968493 -4.1957359 -4.1896806 -4.1843143][-4.2057633 -4.1971197 -4.1968842 -4.1968718 -4.1900253 -4.1734343 -4.1632757 -4.1436348 -4.1342859 -4.1509762 -4.1717768 -4.189045 -4.1906962 -4.1909652 -4.1909285][-4.2033343 -4.2038445 -4.2131615 -4.2121086 -4.1952262 -4.1705871 -4.1572614 -4.1350913 -4.1267447 -4.1436634 -4.1686959 -4.1896806 -4.1955929 -4.2019105 -4.205832]]...]
INFO - root - 2017-12-05 19:55:54.289134: step 36110, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 74h:07m:03s remains)
INFO - root - 2017-12-05 19:56:03.533685: step 36120, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 76h:45m:11s remains)
INFO - root - 2017-12-05 19:56:12.872006: step 36130, loss = 2.05, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 79h:00m:51s remains)
INFO - root - 2017-12-05 19:56:21.877783: step 36140, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 74h:59m:14s remains)
INFO - root - 2017-12-05 19:56:30.902723: step 36150, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 75h:06m:13s remains)
INFO - root - 2017-12-05 19:56:39.905144: step 36160, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 74h:54m:32s remains)
INFO - root - 2017-12-05 19:56:49.019172: step 36170, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 72h:41m:18s remains)
INFO - root - 2017-12-05 19:56:57.872632: step 36180, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 74h:17m:13s remains)
INFO - root - 2017-12-05 19:57:07.039262: step 36190, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 75h:01m:44s remains)
INFO - root - 2017-12-05 19:57:16.237000: step 36200, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 73h:16m:21s remains)
2017-12-05 19:57:16.978336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2172356 -4.2419624 -4.2482109 -4.2337122 -4.1876369 -4.1011143 -3.9901571 -3.8925714 -3.9149997 -4.024992 -4.1510234 -4.2512569 -4.311646 -4.3345585 -4.3388872][-4.2380466 -4.2578044 -4.2569795 -4.2325797 -4.1737351 -4.0688291 -3.9325962 -3.8094 -3.829834 -3.9626882 -4.1170988 -4.2378902 -4.3082814 -4.3348918 -4.3406897][-4.2440448 -4.2566752 -4.2514896 -4.2243996 -4.1619849 -4.0544581 -3.9129486 -3.7768638 -3.7930143 -3.9332294 -4.0997739 -4.230947 -4.3054829 -4.3348169 -4.3424683][-4.2456393 -4.2487335 -4.2393675 -4.2137012 -4.1576538 -4.065268 -3.9379473 -3.8153958 -3.8313036 -3.9602029 -4.116817 -4.2403803 -4.3093963 -4.3369088 -4.344142][-4.2468834 -4.2415838 -4.2286777 -4.2059135 -4.1602955 -4.0852709 -3.9735541 -3.8723152 -3.8927822 -4.0093846 -4.1493855 -4.256813 -4.3166842 -4.3403497 -4.3452859][-4.2522941 -4.2457366 -4.2323875 -4.2106705 -4.1684675 -4.0996089 -3.9963706 -3.9069498 -3.9307294 -4.0416131 -4.1696491 -4.2665539 -4.3214521 -4.3426571 -4.3451214][-4.2582908 -4.2527566 -4.240202 -4.2191687 -4.1747313 -4.1016726 -3.9958744 -3.9052088 -3.93077 -4.0419183 -4.1687651 -4.2642436 -4.3211818 -4.3430667 -4.3443036][-4.2555165 -4.2497935 -4.2400818 -4.22326 -4.1772757 -4.0973005 -3.9816184 -3.8832161 -3.9100657 -4.02563 -4.1562605 -4.2556853 -4.3176603 -4.3417034 -4.3433728][-4.2415609 -4.2363391 -4.231492 -4.221314 -4.176733 -4.0931163 -3.9675329 -3.8588142 -3.8843851 -4.0051489 -4.1412144 -4.2462564 -4.31249 -4.3391242 -4.3423104][-4.228971 -4.2253342 -4.224144 -4.2193255 -4.1779251 -4.0965509 -3.9694369 -3.8588157 -3.88175 -4.0007195 -4.1364121 -4.2423792 -4.3095374 -4.3373804 -4.3416915][-4.2277279 -4.2246437 -4.2243791 -4.2204175 -4.1829209 -4.1103349 -3.9929316 -3.8930459 -3.9158852 -4.0239906 -4.1487341 -4.247602 -4.3111839 -4.3379741 -4.3419003][-4.2324462 -4.230938 -4.2299242 -4.2245417 -4.1927071 -4.1307912 -4.0270424 -3.9414194 -3.966104 -4.0619559 -4.1714311 -4.2589083 -4.3167567 -4.3404727 -4.3429041][-4.2352605 -4.234972 -4.233254 -4.2253447 -4.1970096 -4.1420989 -4.0486865 -3.9730668 -4.001451 -4.0908 -4.1896944 -4.2692323 -4.3222756 -4.3431311 -4.3440351][-4.2309513 -4.2320271 -4.2296829 -4.2192531 -4.1925335 -4.1383586 -4.0484676 -3.9771502 -4.0097637 -4.1004696 -4.1986275 -4.2759633 -4.3262353 -4.3448997 -4.344574][-4.2256846 -4.2294426 -4.2271881 -4.214098 -4.1868334 -4.1315207 -4.0424705 -3.9747636 -4.0119419 -4.1052713 -4.2037864 -4.2792645 -4.3269982 -4.34435 -4.3441114]]...]
INFO - root - 2017-12-05 19:57:26.065197: step 36210, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.942 sec/batch; 77h:31m:16s remains)
INFO - root - 2017-12-05 19:57:35.177417: step 36220, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 74h:15m:53s remains)
INFO - root - 2017-12-05 19:57:44.360890: step 36230, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 74h:43m:32s remains)
INFO - root - 2017-12-05 19:57:53.503215: step 36240, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 74h:45m:16s remains)
INFO - root - 2017-12-05 19:58:02.569189: step 36250, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 72h:49m:03s remains)
INFO - root - 2017-12-05 19:58:11.403541: step 36260, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 71h:55m:23s remains)
INFO - root - 2017-12-05 19:58:20.573298: step 36270, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 75h:58m:43s remains)
INFO - root - 2017-12-05 19:58:29.665130: step 36280, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 70h:23m:24s remains)
INFO - root - 2017-12-05 19:58:38.780563: step 36290, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 78h:07m:05s remains)
INFO - root - 2017-12-05 19:58:47.820912: step 36300, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 76h:43m:01s remains)
2017-12-05 19:58:48.620324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3133049 -4.3094239 -4.3075 -4.3068843 -4.3063049 -4.3047514 -4.3024092 -4.2985148 -4.289578 -4.2825842 -4.283988 -4.2922163 -4.3046441 -4.3205037 -4.336401][-4.2934532 -4.2831883 -4.2772751 -4.2735882 -4.2703 -4.2640834 -4.2539296 -4.2426023 -4.2273359 -4.2195244 -4.2258749 -4.2407327 -4.2607431 -4.2847924 -4.3115158][-4.2675605 -4.2494659 -4.2385058 -4.2314334 -4.2240238 -4.2124057 -4.1933527 -4.1765618 -4.1621656 -4.1624546 -4.1786075 -4.2002292 -4.2243505 -4.2521133 -4.2858081][-4.2443657 -4.2228227 -4.2117848 -4.2039938 -4.1936331 -4.1749763 -4.1503081 -4.1374149 -4.1342249 -4.1428103 -4.16066 -4.1792436 -4.2003903 -4.2285681 -4.2675629][-4.2284374 -4.2071543 -4.1983876 -4.190331 -4.1738067 -4.1423 -4.1107683 -4.1063366 -4.1174259 -4.1305618 -4.1462884 -4.1623116 -4.1835065 -4.2144575 -4.2566166][-4.2173343 -4.1961169 -4.185194 -4.1709542 -4.1435337 -4.0964642 -4.0518117 -4.0597196 -4.0933609 -4.1145525 -4.1274185 -4.1417584 -4.1649919 -4.2001686 -4.2459035][-4.2022724 -4.178237 -4.1630049 -4.1410565 -4.0974665 -4.0250621 -3.9564483 -3.9765697 -4.0438838 -4.0885897 -4.103601 -4.1132288 -4.1375403 -4.1796045 -4.2308636][-4.185401 -4.1604652 -4.14606 -4.1221771 -4.0707126 -3.9818738 -3.8966951 -3.9278967 -4.0206208 -4.0850892 -4.1038895 -4.1113267 -4.1373081 -4.1827278 -4.2316904][-4.1778617 -4.1544013 -4.1433415 -4.1240396 -4.083724 -4.0164213 -3.956671 -3.9826164 -4.0607376 -4.115243 -4.1337633 -4.1424165 -4.1684484 -4.2131844 -4.2557678][-4.1888566 -4.1691442 -4.15872 -4.1417584 -4.119525 -4.0887961 -4.0612473 -4.0780716 -4.1316671 -4.1697979 -4.1857033 -4.1985497 -4.224648 -4.2618804 -4.2903562][-4.2123051 -4.19451 -4.17848 -4.1587729 -4.1489606 -4.144053 -4.1379061 -4.1481 -4.1831937 -4.2107906 -4.2242126 -4.2420354 -4.2680149 -4.2975974 -4.3129554][-4.2395549 -4.2238564 -4.2048278 -4.1886649 -4.1892719 -4.1977391 -4.1997495 -4.204329 -4.2229586 -4.2398648 -4.2489853 -4.2667003 -4.2922769 -4.3183432 -4.3301263][-4.2727242 -4.2612243 -4.2465649 -4.2363605 -4.2418466 -4.2543082 -4.2584753 -4.2575068 -4.2626233 -4.2698526 -4.2774677 -4.2952991 -4.31858 -4.3371091 -4.3416958][-4.3031282 -4.2960887 -4.286953 -4.2792096 -4.2821903 -4.2921195 -4.295434 -4.2918763 -4.290235 -4.2891641 -4.2934518 -4.3071723 -4.3247976 -4.33712 -4.3396106][-4.32321 -4.3191757 -4.3125415 -4.3057857 -4.305522 -4.3094974 -4.3097305 -4.3054757 -4.3025703 -4.3013272 -4.3046665 -4.3136158 -4.3249264 -4.3321023 -4.3343239]]...]
INFO - root - 2017-12-05 19:58:57.619752: step 36310, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 73h:29m:32s remains)
INFO - root - 2017-12-05 19:59:06.650647: step 36320, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.884 sec/batch; 72h:44m:58s remains)
INFO - root - 2017-12-05 19:59:15.701851: step 36330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 74h:27m:12s remains)
INFO - root - 2017-12-05 19:59:24.879269: step 36340, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 73h:21m:19s remains)
INFO - root - 2017-12-05 19:59:33.827645: step 36350, loss = 2.10, batch loss = 2.05 (7.7 examples/sec; 1.040 sec/batch; 85h:31m:27s remains)
INFO - root - 2017-12-05 19:59:42.949285: step 36360, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 71h:58m:21s remains)
INFO - root - 2017-12-05 19:59:51.926790: step 36370, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 74h:23m:04s remains)
INFO - root - 2017-12-05 20:00:01.062678: step 36380, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 74h:19m:03s remains)
INFO - root - 2017-12-05 20:00:10.047702: step 36390, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 75h:20m:06s remains)
INFO - root - 2017-12-05 20:00:19.058079: step 36400, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 73h:56m:02s remains)
2017-12-05 20:00:19.877254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1634974 -4.1569877 -4.1339512 -4.1150937 -4.1112657 -4.1139307 -4.1047921 -4.090713 -4.1122413 -4.1542811 -4.1906624 -4.2226686 -4.255208 -4.2704487 -4.2742057][-4.153614 -4.1478767 -4.1182785 -4.0955811 -4.0939627 -4.0998511 -4.0905271 -4.0843725 -4.1108837 -4.1525664 -4.1850967 -4.2149906 -4.250339 -4.2681918 -4.2732172][-4.15375 -4.1486273 -4.120862 -4.1010833 -4.1059828 -4.1107326 -4.0930662 -4.0842328 -4.1057978 -4.1447906 -4.1812129 -4.217731 -4.2553883 -4.2756786 -4.2792621][-4.1457996 -4.14269 -4.1217308 -4.1086783 -4.1175995 -4.1183572 -4.0918808 -4.07615 -4.0950427 -4.1351471 -4.1754165 -4.2177734 -4.2591586 -4.283555 -4.2863765][-4.1251335 -4.1271424 -4.1168442 -4.1068988 -4.1130066 -4.1086588 -4.0723948 -4.0495138 -4.0778608 -4.1280146 -4.1708832 -4.2153273 -4.2584748 -4.2863364 -4.2929606][-4.1012144 -4.1035271 -4.0960679 -4.0880103 -4.0866318 -4.0738015 -4.0232091 -3.9915152 -4.0428724 -4.1163025 -4.1659679 -4.2138262 -4.2599993 -4.2886648 -4.296145][-4.0866842 -4.0792613 -4.0635991 -4.0478864 -4.0328016 -3.9987087 -3.9227648 -3.8886056 -3.9776595 -4.0847235 -4.1486721 -4.2039423 -4.2587781 -4.2878633 -4.29366][-4.1068411 -4.0908275 -4.0618563 -4.0291839 -3.9869235 -3.9109344 -3.7922437 -3.7442448 -3.8707366 -4.0164776 -4.1053905 -4.1780157 -4.2464991 -4.280004 -4.2871327][-4.1364551 -4.1261477 -4.1025839 -4.0704942 -4.0211654 -3.9232311 -3.7647967 -3.6676514 -3.7763488 -3.9282417 -4.0367479 -4.1323204 -4.2186451 -4.263988 -4.2776909][-4.1468315 -4.1499438 -4.1447239 -4.1321049 -4.1010933 -4.0285745 -3.8881562 -3.7733293 -3.8251636 -3.9266765 -4.0060205 -4.0930905 -4.1840863 -4.242486 -4.26637][-4.1356096 -4.1474628 -4.1572552 -4.1640334 -4.1570086 -4.1170983 -4.0161772 -3.9254436 -3.953634 -4.0150681 -4.0561123 -4.1102161 -4.1823053 -4.2379069 -4.2634044][-4.1287246 -4.1449709 -4.1645541 -4.1798034 -4.181304 -4.1570158 -4.0887537 -4.0305462 -4.05047 -4.0931559 -4.12316 -4.1636634 -4.2144628 -4.2554383 -4.272706][-4.1608434 -4.1710749 -4.186913 -4.2020712 -4.2045274 -4.1861138 -4.1380463 -4.1003642 -4.1141219 -4.1479373 -4.1761127 -4.2072759 -4.2427158 -4.2703276 -4.2816453][-4.2134819 -4.2191982 -4.2310038 -4.242012 -4.2436929 -4.2294359 -4.1969857 -4.1687765 -4.1724763 -4.1928463 -4.2156882 -4.2410955 -4.2678671 -4.2893624 -4.2983427][-4.2747178 -4.2777958 -4.2847939 -4.2910647 -4.2926579 -4.28398 -4.2649417 -4.24329 -4.2380075 -4.2439489 -4.2572803 -4.2765522 -4.2965255 -4.3137012 -4.3203998]]...]
INFO - root - 2017-12-05 20:00:28.886346: step 36410, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 73h:00m:00s remains)
INFO - root - 2017-12-05 20:00:37.868394: step 36420, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 75h:37m:04s remains)
INFO - root - 2017-12-05 20:00:46.991779: step 36430, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 73h:37m:02s remains)
INFO - root - 2017-12-05 20:00:56.080333: step 36440, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.825 sec/batch; 67h:52m:26s remains)
INFO - root - 2017-12-05 20:01:04.985482: step 36450, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 74h:04m:42s remains)
INFO - root - 2017-12-05 20:01:14.140743: step 36460, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 75h:28m:29s remains)
INFO - root - 2017-12-05 20:01:23.225707: step 36470, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 74h:33m:21s remains)
INFO - root - 2017-12-05 20:01:32.335484: step 36480, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 74h:21m:13s remains)
INFO - root - 2017-12-05 20:01:41.192569: step 36490, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 72h:44m:43s remains)
INFO - root - 2017-12-05 20:01:50.347827: step 36500, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 75h:03m:46s remains)
2017-12-05 20:01:51.147147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1913724 -4.2103734 -4.2335157 -4.256155 -4.2814331 -4.2864604 -4.2817383 -4.2799478 -4.2828369 -4.2853293 -4.2860804 -4.2833676 -4.2677197 -4.2400241 -4.2142682][-4.1566796 -4.1766706 -4.1971779 -4.2236376 -4.2559714 -4.2624035 -4.2538548 -4.2504258 -4.2563887 -4.264091 -4.2686133 -4.2679319 -4.25182 -4.224031 -4.2017245][-4.1355515 -4.1518021 -4.1634965 -4.18454 -4.2105913 -4.2086759 -4.194171 -4.1885128 -4.2000618 -4.2208776 -4.2362962 -4.2403097 -4.2279153 -4.2036152 -4.1879592][-4.1373119 -4.143425 -4.1412435 -4.1478791 -4.1575389 -4.1420879 -4.1163878 -4.1058049 -4.1296768 -4.1731005 -4.2029839 -4.2139983 -4.2082543 -4.191793 -4.1816506][-4.1395292 -4.13993 -4.1272392 -4.1162744 -4.1039109 -4.0646868 -4.0172234 -4.0010319 -4.049962 -4.1251349 -4.1735544 -4.193203 -4.1949921 -4.1846018 -4.1758461][-4.1293807 -4.1316495 -4.1164751 -4.0924635 -4.0529165 -3.9785135 -3.8938255 -3.8615828 -3.9427938 -4.0589066 -4.1346011 -4.1696329 -4.1798353 -4.1751537 -4.1665311][-4.1150246 -4.1271334 -4.1185942 -4.0880966 -4.0286942 -3.9228389 -3.7940366 -3.7330866 -3.8412395 -3.9911478 -4.0873013 -4.1381435 -4.1620116 -4.1670566 -4.1632061][-4.1286011 -4.1551905 -4.1592865 -4.1380987 -4.086616 -3.9848282 -3.8535895 -3.7849827 -3.8788416 -4.0086837 -4.0875931 -4.1349411 -4.1662011 -4.1797147 -4.179584][-4.1597505 -4.1919174 -4.2061324 -4.200737 -4.1758485 -4.1074657 -4.0108995 -3.9582634 -4.0139623 -4.0953178 -4.1396828 -4.1685605 -4.1955581 -4.2091327 -4.2081823][-4.1766181 -4.2050128 -4.2203631 -4.2280736 -4.226892 -4.1896276 -4.1271439 -4.0909019 -4.1205044 -4.1646104 -4.1815724 -4.1905017 -4.20583 -4.2143135 -4.21297][-4.1796846 -4.2029347 -4.2185864 -4.235 -4.2485385 -4.2308955 -4.1921959 -4.1687174 -4.1824923 -4.2039752 -4.2084637 -4.2067103 -4.2122717 -4.2137566 -4.2070851][-4.165278 -4.1869087 -4.20677 -4.2291088 -4.2493258 -4.2419758 -4.2158089 -4.1958871 -4.1995029 -4.2107553 -4.2131763 -4.2114086 -4.210731 -4.2056007 -4.1943817][-4.1380229 -4.1638937 -4.1921854 -4.2196145 -4.2408743 -4.2373919 -4.2198014 -4.2034349 -4.2019138 -4.2089586 -4.2122574 -4.213768 -4.2111316 -4.1989183 -4.1836948][-4.1020451 -4.1328611 -4.1714616 -4.2068615 -4.2320514 -4.2327795 -4.2217703 -4.2102394 -4.2079997 -4.212646 -4.2174182 -4.2208443 -4.2146072 -4.1974158 -4.18151][-4.09064 -4.125895 -4.1672878 -4.2044549 -4.2324443 -4.2369695 -4.2309465 -4.2230797 -4.2196517 -4.2221556 -4.2262692 -4.2282214 -4.2178693 -4.1986074 -4.1847372]]...]
INFO - root - 2017-12-05 20:02:00.152056: step 36510, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 76h:26m:04s remains)
INFO - root - 2017-12-05 20:02:09.243363: step 36520, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 74h:09m:36s remains)
INFO - root - 2017-12-05 20:02:18.398051: step 36530, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 74h:31m:09s remains)
INFO - root - 2017-12-05 20:02:27.412555: step 36540, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 75h:57m:16s remains)
INFO - root - 2017-12-05 20:02:36.436365: step 36550, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 76h:24m:25s remains)
INFO - root - 2017-12-05 20:02:45.675787: step 36560, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 75h:48m:24s remains)
INFO - root - 2017-12-05 20:02:54.923496: step 36570, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 75h:30m:57s remains)
INFO - root - 2017-12-05 20:03:03.799420: step 36580, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 73h:57m:39s remains)
INFO - root - 2017-12-05 20:03:12.815684: step 36590, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 75h:35m:13s remains)
INFO - root - 2017-12-05 20:03:21.872730: step 36600, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 75h:44m:22s remains)
2017-12-05 20:03:22.633218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2987041 -4.2806091 -4.268043 -4.2646451 -4.2707725 -4.2812529 -4.289887 -4.2943373 -4.2969542 -4.3001108 -4.3038259 -4.3062987 -4.307457 -4.307148 -4.3063035][-4.2759838 -4.2469077 -4.226975 -4.2228689 -4.2339087 -4.2497311 -4.262105 -4.2681427 -4.2722068 -4.2796774 -4.2885423 -4.2945819 -4.2973518 -4.2974591 -4.2963419][-4.2477994 -4.2062907 -4.1782541 -4.1745629 -4.1916208 -4.2156081 -4.2323318 -4.2376909 -4.2392864 -4.2501383 -4.2666421 -4.2792668 -4.287056 -4.2909184 -4.290998][-4.2255888 -4.1742654 -4.139679 -4.1347322 -4.1553197 -4.1864705 -4.2056861 -4.2039008 -4.1971498 -4.2092566 -4.2357397 -4.2576294 -4.2728038 -4.2828403 -4.2868142][-4.2178721 -4.16231 -4.1248126 -4.1141958 -4.12839 -4.1571941 -4.1702023 -4.1557903 -4.1390672 -4.1538563 -4.1915212 -4.2263846 -4.2531328 -4.2713642 -4.2782197][-4.229084 -4.1763868 -4.1392679 -4.1200309 -4.11804 -4.1285472 -4.1210704 -4.08351 -4.0488334 -4.067173 -4.121172 -4.1744709 -4.2185206 -4.25023 -4.2643723][-4.2434816 -4.1956477 -4.1624932 -4.1415434 -4.12746 -4.1144667 -4.0771317 -4.0013814 -3.93226 -3.9523165 -4.029 -4.1016321 -4.16206 -4.2097888 -4.2359252][-4.2534494 -4.2097554 -4.1815009 -4.1665349 -4.1525846 -4.1271267 -4.0650744 -3.9546232 -3.8556552 -3.8758812 -3.9627962 -4.0387139 -4.105001 -4.1647363 -4.2028918][-4.2567573 -4.2151885 -4.1927261 -4.1879945 -4.18218 -4.1593857 -4.0984497 -4.0013075 -3.9238114 -3.9414134 -4.0004354 -4.0459337 -4.0938859 -4.1523218 -4.193119][-4.2606869 -4.2233219 -4.2080178 -4.2102256 -4.2096014 -4.1945949 -4.1536493 -4.0942817 -4.0534897 -4.0682492 -4.0954614 -4.10774 -4.1251707 -4.1668377 -4.2003689][-4.2760978 -4.2475905 -4.2384648 -4.2422185 -4.2420654 -4.2317524 -4.2067571 -4.1730447 -4.1559763 -4.1697426 -4.181025 -4.1776991 -4.1758375 -4.198319 -4.2224374][-4.2962108 -4.2780514 -4.2742872 -4.2783608 -4.2761621 -4.2649207 -4.2468843 -4.2271547 -4.2219334 -4.2344851 -4.2381811 -4.2318358 -4.2253618 -4.2357821 -4.2513671][-4.3144603 -4.3055534 -4.3061204 -4.3106308 -4.3068495 -4.2955518 -4.2817221 -4.269474 -4.2650967 -4.2686396 -4.2666354 -4.26206 -4.259584 -4.265852 -4.2756734][-4.3279872 -4.3257017 -4.3280678 -4.3321252 -4.3300872 -4.3221097 -4.3119979 -4.3032632 -4.2969794 -4.2936277 -4.2894917 -4.2867646 -4.286746 -4.2901597 -4.2949052][-4.3339696 -4.3345976 -4.3370404 -4.3400049 -4.3397236 -4.3353529 -4.3289714 -4.3227987 -4.3168206 -4.3122382 -4.30904 -4.3075891 -4.3072519 -4.3085318 -4.3107133]]...]
INFO - root - 2017-12-05 20:03:31.709475: step 36610, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 73h:54m:53s remains)
INFO - root - 2017-12-05 20:03:40.807489: step 36620, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 73h:16m:51s remains)
INFO - root - 2017-12-05 20:03:49.822726: step 36630, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 74h:28m:46s remains)
INFO - root - 2017-12-05 20:03:58.786452: step 36640, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 72h:56m:36s remains)
INFO - root - 2017-12-05 20:04:07.877357: step 36650, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 77h:09m:26s remains)
INFO - root - 2017-12-05 20:04:17.079734: step 36660, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 81h:01m:18s remains)
INFO - root - 2017-12-05 20:04:26.025226: step 36670, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 74h:48m:40s remains)
INFO - root - 2017-12-05 20:04:35.037995: step 36680, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 74h:32m:25s remains)
INFO - root - 2017-12-05 20:04:44.191904: step 36690, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 76h:38m:11s remains)
INFO - root - 2017-12-05 20:04:53.104788: step 36700, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.888 sec/batch; 72h:59m:28s remains)
2017-12-05 20:04:53.961779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2906742 -4.2784419 -4.275846 -4.2805037 -4.2848577 -4.2859483 -4.2902737 -4.2960858 -4.2998471 -4.3005772 -4.2988687 -4.2987213 -4.2973647 -4.293705 -4.2914171][-4.2635608 -4.2438297 -4.23775 -4.2419052 -4.2448468 -4.2411075 -4.2441678 -4.255558 -4.2657576 -4.2713704 -4.2757797 -4.2795477 -4.2780476 -4.2716784 -4.2673812][-4.2307267 -4.1996455 -4.187746 -4.1905265 -4.1947556 -4.1947412 -4.1976943 -4.2092681 -4.2213464 -4.2318635 -4.244843 -4.2543216 -4.2545333 -4.249217 -4.24759][-4.1908 -4.1499219 -4.1369424 -4.1430378 -4.1516972 -4.1586461 -4.1577559 -4.1621003 -4.17334 -4.1897721 -4.2105551 -4.2258286 -4.231257 -4.2266135 -4.2252049][-4.1530328 -4.1082678 -4.094624 -4.1004148 -4.1106138 -4.1157126 -4.1025357 -4.0930352 -4.1033192 -4.1274858 -4.1588511 -4.1851788 -4.200726 -4.1965928 -4.1939616][-4.1273141 -4.0825605 -4.0726919 -4.078558 -4.0847583 -4.0743775 -4.0441613 -4.0181146 -4.0231729 -4.0556989 -4.0991807 -4.1342597 -4.1564827 -4.157958 -4.1585011][-4.1174836 -4.0765667 -4.0703468 -4.0729351 -4.0619869 -4.027729 -3.9796212 -3.9429078 -3.9442878 -3.985261 -4.0441074 -4.0878649 -4.1134839 -4.122839 -4.1309872][-4.1368918 -4.1023746 -4.0917206 -4.0720515 -4.0234718 -3.953752 -3.8904576 -3.8598492 -3.8722308 -3.9265437 -3.9996123 -4.0525465 -4.0817719 -4.0939293 -4.1068058][-4.17149 -4.1469421 -4.1254039 -4.0781054 -3.9923413 -3.8982375 -3.8342185 -3.8216236 -3.8532262 -3.9197586 -3.9947376 -4.0497789 -4.0801811 -4.0975947 -4.1146407][-4.2098346 -4.1930065 -4.1670556 -4.1100936 -4.0181174 -3.9298289 -3.8798938 -3.8886604 -3.9441421 -4.014142 -4.0726938 -4.109066 -4.1312184 -4.1480141 -4.1655211][-4.2423921 -4.2295885 -4.2061343 -4.1579304 -4.0833497 -4.0132356 -3.9800227 -3.9995172 -4.0598211 -4.1186628 -4.1590548 -4.179285 -4.1930904 -4.2058172 -4.218966][-4.2693863 -4.2557888 -4.2341146 -4.1941285 -4.1380377 -4.088716 -4.0697827 -4.0910854 -4.1372437 -4.1799865 -4.206789 -4.2167554 -4.2244782 -4.2341685 -4.243299][-4.2852526 -4.2730293 -4.2563205 -4.2266064 -4.1878219 -4.1555676 -4.1450787 -4.1623006 -4.1948686 -4.2207875 -4.234385 -4.2352138 -4.235076 -4.2382975 -4.2439537][-4.281599 -4.2706785 -4.2594991 -4.24188 -4.2197046 -4.2022715 -4.1985059 -4.21037 -4.2316661 -4.2448926 -4.2497015 -4.245553 -4.2393484 -4.2359776 -4.2387857][-4.270566 -4.26072 -4.2547855 -4.2469106 -4.2376938 -4.2326717 -4.2331977 -4.2394691 -4.2509403 -4.2578635 -4.2595143 -4.2551246 -4.2472353 -4.2403879 -4.2394061]]...]
INFO - root - 2017-12-05 20:05:02.971556: step 36710, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 71h:34m:05s remains)
INFO - root - 2017-12-05 20:05:12.121724: step 36720, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.957 sec/batch; 78h:37m:20s remains)
INFO - root - 2017-12-05 20:05:21.204720: step 36730, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 74h:49m:43s remains)
INFO - root - 2017-12-05 20:05:30.321869: step 36740, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 72h:52m:30s remains)
INFO - root - 2017-12-05 20:05:39.423238: step 36750, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 73h:43m:25s remains)
INFO - root - 2017-12-05 20:05:48.451204: step 36760, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 76h:29m:37s remains)
INFO - root - 2017-12-05 20:05:57.586913: step 36770, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 74h:02m:22s remains)
INFO - root - 2017-12-05 20:06:06.811167: step 36780, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 75h:55m:43s remains)
INFO - root - 2017-12-05 20:06:16.125504: step 36790, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 76h:08m:35s remains)
INFO - root - 2017-12-05 20:06:24.877877: step 36800, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 73h:15m:45s remains)
2017-12-05 20:06:25.644924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2212644 -4.195169 -4.1759663 -4.1787167 -4.1975017 -4.2204275 -4.2378922 -4.2444658 -4.2392163 -4.221067 -4.1970305 -4.17451 -4.1554737 -4.1392975 -4.1358509][-4.2038851 -4.1636939 -4.1362119 -4.1399636 -4.1656418 -4.1945224 -4.2160535 -4.2236648 -4.2195182 -4.203105 -4.1759176 -4.1472569 -4.1180053 -4.0900784 -4.0832558][-4.1796141 -4.130084 -4.1039305 -4.1146321 -4.1433783 -4.1681352 -4.1829038 -4.1876321 -4.1872535 -4.1778784 -4.1550517 -4.1270671 -4.0888152 -4.0457921 -4.0356941][-4.161777 -4.1151114 -4.0973439 -4.1128111 -4.1343584 -4.1438365 -4.1430035 -4.14086 -4.1480813 -4.1536222 -4.1475749 -4.1277952 -4.0873957 -4.0352974 -4.0221415][-4.1611433 -4.1278629 -4.1159415 -4.1241193 -4.1293349 -4.1123753 -4.0818329 -4.0695171 -4.0876956 -4.1168213 -4.1392093 -4.1423149 -4.118947 -4.0785675 -4.067111][-4.1736298 -4.1505237 -4.1318073 -4.1141772 -4.0888233 -4.0348854 -3.9575541 -3.9236321 -3.9690037 -4.0452437 -4.1107459 -4.1479058 -4.1546545 -4.1381035 -4.1344438][-4.1757011 -4.1617503 -4.1332512 -4.0881104 -4.0282974 -3.9273562 -3.7832971 -3.7080967 -3.7965579 -3.9466667 -4.0662088 -4.1330795 -4.1640978 -4.168849 -4.1745486][-4.1704693 -4.1638112 -4.1374431 -4.0861292 -4.0095167 -3.8757918 -3.67743 -3.5618918 -3.6875987 -3.8920591 -4.0446019 -4.1261187 -4.1675363 -4.1831737 -4.1937175][-4.171196 -4.1767139 -4.1634564 -4.1300969 -4.0743346 -3.9657164 -3.8003802 -3.7032113 -3.7986479 -3.9708333 -4.0979514 -4.1621275 -4.1937833 -4.2067709 -4.2149067][-4.154531 -4.1691184 -4.1748409 -4.1674185 -4.1412854 -4.075202 -3.9706044 -3.9101586 -3.9737978 -4.0913119 -4.1751652 -4.209815 -4.2190728 -4.2172542 -4.21296][-4.1391683 -4.1602349 -4.1771989 -4.1866736 -4.1805258 -4.1419806 -4.0813966 -4.0535522 -4.1011271 -4.1769528 -4.2258697 -4.2370205 -4.2243533 -4.2023873 -4.1853471][-4.1448197 -4.1664543 -4.1849532 -4.1976995 -4.200881 -4.1835566 -4.1527514 -4.1437297 -4.1807594 -4.2240591 -4.2433748 -4.2338657 -4.2038159 -4.168694 -4.1482692][-4.1726341 -4.1886988 -4.199358 -4.2044311 -4.2085943 -4.2089496 -4.2023339 -4.2047057 -4.2289553 -4.2466612 -4.2408724 -4.2131681 -4.173975 -4.1378312 -4.1221442][-4.2055736 -4.21225 -4.2149944 -4.2145786 -4.2206316 -4.2321153 -4.242394 -4.2487307 -4.2565832 -4.2546883 -4.2378778 -4.2060294 -4.1694584 -4.1407175 -4.1316733][-4.2397828 -4.2415605 -4.2412133 -4.2411423 -4.2503428 -4.26463 -4.2772751 -4.2813468 -4.2785974 -4.2705283 -4.2560234 -4.2290773 -4.201746 -4.181962 -4.1749434]]...]
INFO - root - 2017-12-05 20:06:34.754862: step 36810, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 74h:10m:40s remains)
INFO - root - 2017-12-05 20:06:43.701057: step 36820, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 75h:36m:50s remains)
INFO - root - 2017-12-05 20:06:52.830451: step 36830, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 77h:15m:58s remains)
INFO - root - 2017-12-05 20:07:01.933252: step 36840, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 76h:02m:26s remains)
INFO - root - 2017-12-05 20:07:11.076910: step 36850, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 73h:15m:38s remains)
INFO - root - 2017-12-05 20:07:19.977928: step 36860, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 75h:41m:58s remains)
INFO - root - 2017-12-05 20:07:29.082374: step 36870, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 75h:42m:08s remains)
INFO - root - 2017-12-05 20:07:37.957823: step 36880, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 74h:45m:44s remains)
INFO - root - 2017-12-05 20:07:47.085471: step 36890, loss = 2.02, batch loss = 1.96 (9.0 examples/sec; 0.886 sec/batch; 72h:46m:43s remains)
INFO - root - 2017-12-05 20:07:56.182007: step 36900, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 72h:43m:08s remains)
2017-12-05 20:07:56.955635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3248906 -4.3151484 -4.2969251 -4.2786546 -4.2662296 -4.2586336 -4.2548695 -4.244432 -4.2316823 -4.2266974 -4.2336574 -4.247798 -4.2637935 -4.27881 -4.2979641][-4.317028 -4.3088927 -4.2911696 -4.273273 -4.2583489 -4.2438774 -4.235456 -4.2211742 -4.2048721 -4.1993508 -4.212729 -4.23615 -4.2568293 -4.2744479 -4.2960014][-4.3089957 -4.3030105 -4.2870822 -4.2669029 -4.2480369 -4.2289491 -4.21734 -4.2023411 -4.1833143 -4.1760406 -4.1948147 -4.2250948 -4.2506642 -4.2710814 -4.29368][-4.2907619 -4.2835298 -4.2680545 -4.2465982 -4.2267261 -4.2094269 -4.2004552 -4.1876426 -4.1669712 -4.1563559 -4.1769638 -4.2118073 -4.24315 -4.2691741 -4.2924018][-4.2770219 -4.2662392 -4.245079 -4.2242684 -4.208312 -4.1944323 -4.1840429 -4.1702542 -4.1521249 -4.1443558 -4.1653018 -4.2013135 -4.2373652 -4.268136 -4.2919087][-4.2682366 -4.2494426 -4.2206221 -4.2025065 -4.1924911 -4.1789556 -4.1666331 -4.1593838 -4.1516461 -4.1485419 -4.1673646 -4.2008858 -4.2364554 -4.2691197 -4.29236][-4.2499132 -4.2239466 -4.1907492 -4.1734152 -4.16512 -4.1542282 -4.15114 -4.1579866 -4.1584396 -4.1599789 -4.1786213 -4.20877 -4.2402411 -4.2699642 -4.2923336][-4.2273183 -4.1996479 -4.1642137 -4.1468468 -4.1433334 -4.1418757 -4.150332 -4.1623564 -4.1635976 -4.1679454 -4.1897116 -4.2178736 -4.2452011 -4.2703943 -4.2915583][-4.2007837 -4.17728 -4.1466761 -4.1350789 -4.1412272 -4.1517816 -4.1634855 -4.1656017 -4.156209 -4.1590962 -4.1827531 -4.2146311 -4.2424326 -4.2672257 -4.2900887][-4.1782155 -4.1631989 -4.1410508 -4.1356392 -4.1474638 -4.1583395 -4.1602726 -4.1480451 -4.1291585 -4.1288834 -4.1561007 -4.1946507 -4.2298093 -4.2614956 -4.2892313][-4.1467957 -4.1443248 -4.136951 -4.1383109 -4.1483583 -4.1503654 -4.1400561 -4.1177578 -4.0936813 -4.0917068 -4.1224365 -4.17127 -4.21845 -4.2592564 -4.2906842][-4.1117096 -4.120595 -4.1264052 -4.1342359 -4.1434669 -4.1411829 -4.1229453 -4.0963135 -4.0681386 -4.0629516 -4.0978346 -4.1592407 -4.2182426 -4.2637086 -4.294311][-4.1029387 -4.1161814 -4.124537 -4.1310916 -4.1408896 -4.1388021 -4.120996 -4.0933347 -4.0700684 -4.0729971 -4.1123805 -4.1745844 -4.2316513 -4.2728696 -4.2988768][-4.1227489 -4.1336756 -4.1338234 -4.1295891 -4.1297135 -4.1225858 -4.1065063 -4.085011 -4.0786357 -4.0996661 -4.1422815 -4.1987133 -4.2489138 -4.2829576 -4.303926][-4.1228433 -4.1261568 -4.1159191 -4.1009321 -4.0909338 -4.0789585 -4.0660925 -4.0582118 -4.0741248 -4.1131716 -4.1606865 -4.2141013 -4.2604218 -4.2905498 -4.3084488]]...]
INFO - root - 2017-12-05 20:08:05.885981: step 36910, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 75h:06m:06s remains)
INFO - root - 2017-12-05 20:08:14.831501: step 36920, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 72h:49m:06s remains)
INFO - root - 2017-12-05 20:08:23.926625: step 36930, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 75h:28m:27s remains)
INFO - root - 2017-12-05 20:08:32.930797: step 36940, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.914 sec/batch; 75h:03m:29s remains)
INFO - root - 2017-12-05 20:08:41.933010: step 36950, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 75h:05m:42s remains)
INFO - root - 2017-12-05 20:08:50.826908: step 36960, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.939 sec/batch; 77h:06m:50s remains)
INFO - root - 2017-12-05 20:09:00.027525: step 36970, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 73h:53m:03s remains)
INFO - root - 2017-12-05 20:09:09.207420: step 36980, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 75h:16m:12s remains)
INFO - root - 2017-12-05 20:09:18.355048: step 36990, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 77h:39m:18s remains)
INFO - root - 2017-12-05 20:09:27.606568: step 37000, loss = 2.02, batch loss = 1.96 (8.7 examples/sec; 0.919 sec/batch; 75h:27m:35s remains)
2017-12-05 20:09:28.378433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.217886 -4.1620865 -4.103385 -4.0604835 -4.0353026 -4.03812 -4.0723376 -4.1113434 -4.1363187 -4.1588025 -4.1775341 -4.1937318 -4.2098193 -4.2284193 -4.2273049][-4.1952586 -4.1408734 -4.0926113 -4.0700622 -4.0714478 -4.0845742 -4.10775 -4.135035 -4.1447897 -4.1474981 -4.150537 -4.1630921 -4.1811886 -4.204195 -4.2039213][-4.1786642 -4.1264176 -4.0928469 -4.0880332 -4.1027408 -4.116878 -4.13313 -4.1495147 -4.1467104 -4.1301322 -4.1181092 -4.1215868 -4.1365943 -4.1650586 -4.1760983][-4.1722574 -4.126318 -4.1020546 -4.1043158 -4.1202435 -4.1370039 -4.1522064 -4.1590395 -4.144978 -4.1166205 -4.09466 -4.0849342 -4.0871253 -4.1205316 -4.1480341][-4.1803117 -4.1378636 -4.1119962 -4.1134582 -4.1285977 -4.1441736 -4.1582923 -4.1613455 -4.1439228 -4.1057892 -4.065475 -4.0318427 -4.0208712 -4.06548 -4.1177611][-4.2099419 -4.1667166 -4.1299057 -4.1152363 -4.1174107 -4.1219869 -4.1287565 -4.1276474 -4.110055 -4.0714388 -4.0230713 -3.9752069 -3.9550276 -4.0133209 -4.090467][-4.2499275 -4.203969 -4.14735 -4.1025138 -4.0785718 -4.063776 -4.0586667 -4.056211 -4.0510139 -4.0278292 -3.9879155 -3.9404755 -3.9187312 -3.9812732 -4.0725846][-4.2707171 -4.2194095 -4.1468129 -4.0776739 -4.0298185 -4.0003924 -3.9900448 -3.9983964 -4.0157256 -4.0199957 -4.0013127 -3.9651659 -3.943228 -3.9911604 -4.0751185][-4.2612004 -4.20248 -4.126647 -4.0608673 -4.0225306 -4.0063677 -4.0017142 -4.0261517 -4.0620713 -4.0828242 -4.0782952 -4.0510221 -4.030652 -4.0573373 -4.1133609][-4.2398157 -4.1839104 -4.1209931 -4.0785279 -4.0605931 -4.0582333 -4.063889 -4.0965681 -4.13621 -4.1600647 -4.161098 -4.1389675 -4.1168747 -4.1280074 -4.1593213][-4.2239718 -4.1846628 -4.1461515 -4.1256824 -4.1171103 -4.1184092 -4.1278028 -4.154655 -4.1823659 -4.2013803 -4.2063384 -4.1931138 -4.1724615 -4.1719227 -4.1876521][-4.2226329 -4.1951127 -4.17564 -4.1705427 -4.1723652 -4.1768222 -4.1819839 -4.196209 -4.2122459 -4.2270055 -4.2376428 -4.2347808 -4.2224216 -4.2198305 -4.2277312][-4.2335076 -4.212647 -4.2032056 -4.2095366 -4.221231 -4.2316494 -4.2368803 -4.2441216 -4.2520318 -4.2617359 -4.2739587 -4.27649 -4.2709785 -4.2690287 -4.273622][-4.2511897 -4.2387695 -4.2384992 -4.2497883 -4.2664428 -4.27914 -4.2834482 -4.2852159 -4.2869616 -4.2938414 -4.305583 -4.3106413 -4.3087215 -4.3074837 -4.3090835][-4.279912 -4.274868 -4.2823153 -4.2970467 -4.3109083 -4.3190956 -4.321228 -4.32119 -4.3211222 -4.3244696 -4.3311887 -4.3345881 -4.3342671 -4.3337417 -4.3323092]]...]
INFO - root - 2017-12-05 20:09:37.332491: step 37010, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 75h:55m:14s remains)
INFO - root - 2017-12-05 20:09:46.384088: step 37020, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 74h:06m:26s remains)
INFO - root - 2017-12-05 20:09:55.269194: step 37030, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 74h:29m:01s remains)
INFO - root - 2017-12-05 20:10:04.453613: step 37040, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 75h:57m:06s remains)
INFO - root - 2017-12-05 20:10:13.381385: step 37050, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 72h:27m:56s remains)
INFO - root - 2017-12-05 20:10:22.499114: step 37060, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 75h:13m:57s remains)
INFO - root - 2017-12-05 20:10:31.649360: step 37070, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 74h:18m:35s remains)
INFO - root - 2017-12-05 20:10:40.659498: step 37080, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 73h:58m:46s remains)
INFO - root - 2017-12-05 20:10:49.592785: step 37090, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 72h:49m:46s remains)
INFO - root - 2017-12-05 20:10:58.657568: step 37100, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 75h:14m:00s remains)
2017-12-05 20:10:59.456107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2620568 -4.2446232 -4.2359567 -4.2308593 -4.2234969 -4.2145629 -4.1912341 -4.1403966 -4.0866246 -4.067378 -4.0987954 -4.1473136 -4.1822543 -4.2062922 -4.2164025][-4.281651 -4.2672329 -4.2653646 -4.2652674 -4.2573934 -4.2392621 -4.2048597 -4.1504016 -4.0948853 -4.0677981 -4.093647 -4.1484704 -4.1994009 -4.2376118 -4.2561507][-4.2781963 -4.2709932 -4.2776213 -4.2878852 -4.292098 -4.28079 -4.2453804 -4.1946836 -4.1430492 -4.1100926 -4.1212888 -4.1680541 -4.2199345 -4.2588372 -4.2805381][-4.2565613 -4.2540092 -4.2628703 -4.2796574 -4.296762 -4.3021541 -4.2828531 -4.2460608 -4.2039723 -4.1721678 -4.1738567 -4.207202 -4.2486658 -4.2777662 -4.2936974][-4.2207079 -4.2048993 -4.2038693 -4.2211552 -4.253078 -4.287189 -4.2982278 -4.2830677 -4.2549324 -4.2321997 -4.2311373 -4.2540083 -4.2829967 -4.2989407 -4.3034234][-4.1751909 -4.127892 -4.100039 -4.1090994 -4.1557136 -4.2227759 -4.2733092 -4.2883015 -4.2783875 -4.2699027 -4.2725973 -4.2907715 -4.3104897 -4.3187618 -4.3167615][-4.110323 -4.0305943 -3.9664452 -3.955811 -4.0115118 -4.105721 -4.1845036 -4.2275257 -4.2470021 -4.2654538 -4.2819848 -4.3035326 -4.32077 -4.3283587 -4.3266048][-4.0534005 -3.9515498 -3.8553774 -3.8192861 -3.8735797 -3.9810283 -4.07176 -4.13057 -4.1759148 -4.222229 -4.2618885 -4.2964282 -4.3196039 -4.3289871 -4.330215][-4.0463476 -3.9298072 -3.8075395 -3.740845 -3.7817323 -3.8908887 -3.989634 -4.0550265 -4.112359 -4.1734867 -4.2286482 -4.2758093 -4.3048558 -4.3155727 -4.3185511][-4.0932279 -3.9844313 -3.8561926 -3.772764 -3.7909572 -3.878437 -3.9691422 -4.0302215 -4.0831857 -4.143364 -4.2009583 -4.2519031 -4.2831826 -4.2950025 -4.3009052][-4.1617832 -4.0813723 -3.9830115 -3.9218082 -3.929225 -3.9783587 -4.0335298 -4.0655608 -4.098238 -4.1473632 -4.1986737 -4.2425976 -4.2693424 -4.279459 -4.2856154][-4.2252846 -4.1764588 -4.1210341 -4.096406 -4.1041479 -4.12765 -4.146327 -4.1487565 -4.157208 -4.1847134 -4.2227254 -4.2545023 -4.2720962 -4.2740574 -4.2731729][-4.26897 -4.2451844 -4.2234159 -4.2197514 -4.2259545 -4.2373767 -4.2431965 -4.2376914 -4.2357473 -4.2472854 -4.2686563 -4.28589 -4.2926764 -4.2859454 -4.2760983][-4.2929349 -4.2843857 -4.2796893 -4.2832251 -4.2859468 -4.2895923 -4.2936511 -4.2926021 -4.2910767 -4.2946067 -4.3009963 -4.3046122 -4.3033323 -4.2932453 -4.2810583][-4.3030529 -4.3015237 -4.3029985 -4.3081803 -4.3100758 -4.3099074 -4.3118763 -4.3149343 -4.3168507 -4.3171206 -4.3145547 -4.3105679 -4.3052521 -4.2973676 -4.2900777]]...]
INFO - root - 2017-12-05 20:11:08.774325: step 37110, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 76h:08m:44s remains)
INFO - root - 2017-12-05 20:11:17.923794: step 37120, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 74h:42m:25s remains)
INFO - root - 2017-12-05 20:11:27.196126: step 37130, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 77h:30m:23s remains)
INFO - root - 2017-12-05 20:11:36.205318: step 37140, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 75h:25m:16s remains)
INFO - root - 2017-12-05 20:11:45.214588: step 37150, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 73h:39m:02s remains)
INFO - root - 2017-12-05 20:11:54.308394: step 37160, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 76h:22m:55s remains)
INFO - root - 2017-12-05 20:12:03.261231: step 37170, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 72h:50m:00s remains)
INFO - root - 2017-12-05 20:12:12.268538: step 37180, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 75h:39m:39s remains)
INFO - root - 2017-12-05 20:12:21.370171: step 37190, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 67h:20m:25s remains)
INFO - root - 2017-12-05 20:12:30.514613: step 37200, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 74h:34m:08s remains)
2017-12-05 20:12:31.282920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2601914 -4.2666492 -4.2653227 -4.2522535 -4.2317233 -4.2210183 -4.2120395 -4.1936774 -4.1873088 -4.189311 -4.1955791 -4.2113285 -4.2246366 -4.220551 -4.2056246][-4.30459 -4.3120518 -4.3082142 -4.2960334 -4.2742796 -4.2573862 -4.2391782 -4.2120123 -4.1989617 -4.2032366 -4.2149172 -4.2289171 -4.2373176 -4.2342138 -4.2209015][-4.3192434 -4.3229079 -4.3169937 -4.3073645 -4.2882538 -4.2694478 -4.2466035 -4.2186236 -4.2036495 -4.2103834 -4.2277184 -4.2413635 -4.2454677 -4.2389584 -4.221952][-4.31034 -4.309732 -4.3039412 -4.2956982 -4.2808962 -4.2621956 -4.2382526 -4.2111673 -4.1949735 -4.20044 -4.2192082 -4.2359009 -4.2426171 -4.2328005 -4.208858][-4.2923217 -4.2908888 -4.2869425 -4.2778153 -4.2632327 -4.24582 -4.2245097 -4.1994619 -4.1815557 -4.1837025 -4.203464 -4.2223787 -4.2290878 -4.2139106 -4.1826315][-4.2747951 -4.2712474 -4.2660789 -4.2515068 -4.2342558 -4.2214932 -4.2065239 -4.186573 -4.1725221 -4.1741681 -4.190567 -4.2073312 -4.2128429 -4.1932716 -4.1559582][-4.2657094 -4.2564564 -4.2452064 -4.2272978 -4.2112112 -4.2021174 -4.1895204 -4.173058 -4.1659532 -4.1733017 -4.1893291 -4.2044358 -4.20744 -4.1854677 -4.1469984][-4.2524734 -4.2372818 -4.2173395 -4.1955676 -4.1819668 -4.175004 -4.1643863 -4.1581388 -4.167336 -4.1864672 -4.2088385 -4.224328 -4.2245064 -4.2027555 -4.1652188][-4.2258534 -4.2093205 -4.1867623 -4.1631751 -4.1492176 -4.1463857 -4.1483631 -4.1622949 -4.1868248 -4.2149525 -4.2403584 -4.2536492 -4.2526612 -4.2332826 -4.1989565][-4.2034774 -4.1904688 -4.1756825 -4.155529 -4.1423922 -4.1459389 -4.1633878 -4.1936541 -4.2228222 -4.247386 -4.2696562 -4.2836146 -4.2857471 -4.2702389 -4.2392387][-4.1978049 -4.1909952 -4.1810904 -4.1637783 -4.1526146 -4.1613307 -4.1864057 -4.2203469 -4.2466984 -4.2672997 -4.2876234 -4.3038416 -4.3093581 -4.2967362 -4.2698574][-4.1948214 -4.1955585 -4.1905313 -4.1802545 -4.17478 -4.1874871 -4.2098041 -4.2360191 -4.2588768 -4.278141 -4.2984676 -4.3163772 -4.3213291 -4.30827 -4.2842693][-4.1990867 -4.2081842 -4.2110319 -4.2081094 -4.2077608 -4.2206912 -4.2372146 -4.2538843 -4.2711153 -4.2891607 -4.3085976 -4.3233161 -4.3221235 -4.3047853 -4.2807875][-4.209662 -4.2254753 -4.2371235 -4.2394605 -4.2404108 -4.2514 -4.2637215 -4.2744207 -4.2875242 -4.3034329 -4.3192282 -4.3261895 -4.3160005 -4.2918973 -4.2646871][-4.215673 -4.2392545 -4.257256 -4.2632265 -4.2646732 -4.2749372 -4.2850895 -4.2941704 -4.3056135 -4.3188944 -4.3277731 -4.3248353 -4.3042955 -4.27241 -4.2411556]]...]
INFO - root - 2017-12-05 20:12:40.272533: step 37210, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 75h:14m:47s remains)
INFO - root - 2017-12-05 20:12:49.308284: step 37220, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 73h:37m:49s remains)
INFO - root - 2017-12-05 20:12:58.448010: step 37230, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 72h:40m:29s remains)
INFO - root - 2017-12-05 20:13:07.359971: step 37240, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 71h:03m:57s remains)
INFO - root - 2017-12-05 20:13:16.451488: step 37250, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 72h:25m:43s remains)
INFO - root - 2017-12-05 20:13:25.506350: step 37260, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 74h:59m:42s remains)
INFO - root - 2017-12-05 20:13:34.680368: step 37270, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 77h:27m:16s remains)
INFO - root - 2017-12-05 20:13:43.733214: step 37280, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 72h:12m:49s remains)
INFO - root - 2017-12-05 20:13:52.722625: step 37290, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 75h:57m:24s remains)
INFO - root - 2017-12-05 20:14:01.834709: step 37300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 71h:38m:18s remains)
2017-12-05 20:14:02.635553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3004827 -4.2998347 -4.2931161 -4.2757344 -4.2570138 -4.2409749 -4.2356648 -4.2394667 -4.2468038 -4.26348 -4.2840018 -4.2969513 -4.3016496 -4.3053346 -4.3093915][-4.2865782 -4.290484 -4.2844863 -4.2613583 -4.2311912 -4.2057686 -4.1937761 -4.1900086 -4.1908121 -4.2071414 -4.2327838 -4.2520332 -4.2619271 -4.2695971 -4.2786655][-4.2714939 -4.2757673 -4.2681556 -4.2448926 -4.2139053 -4.1850719 -4.166142 -4.1498151 -4.1393695 -4.1561522 -4.1858087 -4.2092295 -4.2246947 -4.2349544 -4.2421565][-4.255022 -4.2506242 -4.23996 -4.2216988 -4.1972337 -4.1696057 -4.1464086 -4.1181688 -4.0943708 -4.10565 -4.1343522 -4.160471 -4.1871285 -4.2024841 -4.2064013][-4.2427359 -4.2255211 -4.2088923 -4.1931281 -4.1752944 -4.1557922 -4.1372886 -4.105309 -4.0692453 -4.0624223 -4.0766215 -4.1012607 -4.1387935 -4.1638193 -4.171494][-4.2314191 -4.2072291 -4.1813703 -4.1627855 -4.1548023 -4.154911 -4.1512437 -4.1228986 -4.0759792 -4.0459967 -4.0367241 -4.0521317 -4.0922351 -4.1266341 -4.1420703][-4.2243495 -4.19712 -4.160491 -4.1344013 -4.1341329 -4.153029 -4.1680603 -4.154295 -4.1100035 -4.0643911 -4.0339942 -4.0334797 -4.0621285 -4.0995708 -4.1252637][-4.21502 -4.1951313 -4.1550121 -4.1204586 -4.1143079 -4.136579 -4.1626048 -4.1697788 -4.1440372 -4.1017275 -4.0598478 -4.043972 -4.057519 -4.09314 -4.1299195][-4.2044039 -4.2036185 -4.1766887 -4.1452212 -4.1295843 -4.1420984 -4.167182 -4.1823287 -4.1746106 -4.145175 -4.1049948 -4.0778022 -4.0742378 -4.1004229 -4.1382084][-4.175993 -4.1963792 -4.1925287 -4.1807885 -4.1697731 -4.171339 -4.1846642 -4.1959782 -4.1967621 -4.1814656 -4.1508017 -4.11856 -4.0977364 -4.1084666 -4.1413593][-4.146472 -4.1799555 -4.1938334 -4.1977878 -4.1924114 -4.1928964 -4.2022605 -4.209991 -4.2132726 -4.2090158 -4.1879554 -4.1557479 -4.1263742 -4.1212106 -4.1409378][-4.135745 -4.1718879 -4.19154 -4.2011056 -4.1997972 -4.2034478 -4.2138133 -4.2224622 -4.2279224 -4.2297945 -4.213603 -4.1847024 -4.1581178 -4.1493254 -4.1569891][-4.1311631 -4.1574421 -4.1749711 -4.1829157 -4.1862888 -4.1959109 -4.2083573 -4.2214155 -4.2321119 -4.2408237 -4.2338243 -4.2124386 -4.18966 -4.1793642 -4.1790957][-4.1217303 -4.1403842 -4.1535153 -4.1553965 -4.1593156 -4.1718125 -4.186615 -4.204052 -4.2214794 -4.2375364 -4.2435508 -4.2355051 -4.2196097 -4.2072749 -4.1980929][-4.1044607 -4.1152472 -4.122128 -4.1184378 -4.1228313 -4.13602 -4.1535926 -4.1770644 -4.1979942 -4.2163024 -4.2304111 -4.2358589 -4.2305555 -4.2228236 -4.2120447]]...]
INFO - root - 2017-12-05 20:14:11.794361: step 37310, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 74h:07m:11s remains)
INFO - root - 2017-12-05 20:14:20.905842: step 37320, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 74h:04m:38s remains)
INFO - root - 2017-12-05 20:14:29.704375: step 37330, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 77h:05m:33s remains)
INFO - root - 2017-12-05 20:14:38.939326: step 37340, loss = 2.07, batch loss = 2.01 (7.9 examples/sec; 1.014 sec/batch; 83h:06m:26s remains)
INFO - root - 2017-12-05 20:14:47.991052: step 37350, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 74h:47m:22s remains)
INFO - root - 2017-12-05 20:14:56.905084: step 37360, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 72h:09m:50s remains)
INFO - root - 2017-12-05 20:15:06.120550: step 37370, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 76h:01m:53s remains)
INFO - root - 2017-12-05 20:15:15.338198: step 37380, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 75h:43m:30s remains)
INFO - root - 2017-12-05 20:15:24.268554: step 37390, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 76h:47m:33s remains)
INFO - root - 2017-12-05 20:15:33.384015: step 37400, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 75h:27m:41s remains)
2017-12-05 20:15:34.183460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.277606 -4.2784619 -4.2878761 -4.302433 -4.306644 -4.3069563 -4.3074365 -4.3041048 -4.2935262 -4.273365 -4.2547808 -4.2283406 -4.1919417 -4.1631813 -4.1454811][-4.3046975 -4.3060112 -4.3120179 -4.320662 -4.3214922 -4.3178434 -4.3159947 -4.31035 -4.2957368 -4.2713385 -4.2477655 -4.2192173 -4.1852412 -4.1544127 -4.1376424][-4.3105569 -4.3106303 -4.3142371 -4.31657 -4.3112111 -4.3024888 -4.2984962 -4.2938013 -4.2819166 -4.2606492 -4.2370167 -4.2103071 -4.1806345 -4.1514921 -4.1344056][-4.2984872 -4.2933683 -4.2920904 -4.288466 -4.2761259 -4.2584543 -4.2478886 -4.2481289 -4.2486668 -4.2375903 -4.2192016 -4.1979413 -4.1737618 -4.1514692 -4.1369233][-4.265244 -4.2495222 -4.2439818 -4.2346911 -4.2115817 -4.1784539 -4.1538267 -4.1652908 -4.1865573 -4.1905894 -4.1843367 -4.1699214 -4.1550865 -4.1511559 -4.1449575][-4.2107477 -4.177628 -4.1691766 -4.1564646 -4.1207161 -4.0568562 -3.998328 -4.0251117 -4.0838733 -4.1126819 -4.117125 -4.1003442 -4.0990725 -4.1237335 -4.1388168][-4.146451 -4.0921979 -4.086998 -4.0766644 -4.0289674 -3.9230547 -3.8078074 -3.8607798 -3.9756849 -4.0365658 -4.0429616 -4.0110197 -4.0211592 -4.0761223 -4.120697][-4.0915084 -4.0280967 -4.0366163 -4.0327787 -3.9775479 -3.8482018 -3.7041912 -3.7799923 -3.9240322 -4.001266 -4.0105138 -3.9707546 -3.983093 -4.047699 -4.1049042][-4.0683746 -4.026134 -4.0453577 -4.0388894 -3.9971244 -3.9090412 -3.8167019 -3.8693597 -3.9768672 -4.0488429 -4.0694304 -4.0422621 -4.0423007 -4.0793843 -4.1209307][-4.0805826 -4.0702171 -4.0936275 -4.0900846 -4.0690484 -4.03267 -3.9889817 -4.0118556 -4.0745807 -4.1321249 -4.1591463 -4.14441 -4.1341033 -4.1412735 -4.1596007][-4.13609 -4.1456246 -4.1671286 -4.1734991 -4.171464 -4.1595664 -4.1361709 -4.1416664 -4.1757083 -4.2170768 -4.2402115 -4.2313466 -4.2166948 -4.2132759 -4.2215056][-4.2135768 -4.2274165 -4.2421274 -4.2532549 -4.2605972 -4.2563219 -4.2430878 -4.2423754 -4.2583265 -4.2823625 -4.293829 -4.2879987 -4.2777038 -4.2773404 -4.2835932][-4.2768865 -4.2878504 -4.2963233 -4.3055415 -4.3144226 -4.3144112 -4.3062587 -4.3028312 -4.3067269 -4.3185906 -4.3233466 -4.3218489 -4.3167949 -4.3183241 -4.32354][-4.3118458 -4.3212709 -4.3280239 -4.3349991 -4.3408694 -4.3411546 -4.3363113 -4.3320546 -4.3288221 -4.3316464 -4.3324828 -4.3314967 -4.3292589 -4.3314629 -4.3361659][-4.3270845 -4.3364844 -4.3442354 -4.3489342 -4.3501382 -4.3465052 -4.3406277 -4.335362 -4.3294735 -4.3263407 -4.322638 -4.3210526 -4.3217888 -4.3255138 -4.3294511]]...]
INFO - root - 2017-12-05 20:15:43.469301: step 37410, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 75h:01m:42s remains)
INFO - root - 2017-12-05 20:15:52.232151: step 37420, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 0.818 sec/batch; 67h:01m:17s remains)
INFO - root - 2017-12-05 20:16:01.260521: step 37430, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 74h:40m:22s remains)
INFO - root - 2017-12-05 20:16:10.605861: step 37440, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 75h:22m:31s remains)
INFO - root - 2017-12-05 20:16:19.670705: step 37450, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 74h:39m:25s remains)
INFO - root - 2017-12-05 20:16:28.702242: step 37460, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 75h:20m:17s remains)
INFO - root - 2017-12-05 20:16:37.812405: step 37470, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 75h:23m:49s remains)
INFO - root - 2017-12-05 20:16:46.862444: step 37480, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.887 sec/batch; 72h:42m:26s remains)
INFO - root - 2017-12-05 20:16:56.040728: step 37490, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 74h:25m:39s remains)
INFO - root - 2017-12-05 20:17:05.145802: step 37500, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 74h:23m:48s remains)
2017-12-05 20:17:05.958432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1514959 -4.1477814 -4.1472645 -4.1643724 -4.1812429 -4.1899719 -4.1910367 -4.1931915 -4.2013006 -4.2008114 -4.199019 -4.2005944 -4.1973677 -4.191432 -4.1790638][-4.1527114 -4.1491108 -4.1555157 -4.1804214 -4.2059846 -4.2171612 -4.2127247 -4.2055612 -4.2038846 -4.19749 -4.1973834 -4.2030706 -4.2010736 -4.1921878 -4.1799479][-4.1706524 -4.1688538 -4.1729932 -4.1898379 -4.20803 -4.2203031 -4.2157855 -4.201333 -4.18986 -4.1806555 -4.1857276 -4.1951914 -4.1953125 -4.1908488 -4.1857152][-4.1779737 -4.1729488 -4.1748152 -4.1895714 -4.2078977 -4.2210793 -4.2112241 -4.1877713 -4.1697412 -4.1603217 -4.1722717 -4.1911974 -4.2026277 -4.208322 -4.2076168][-4.17185 -4.1651444 -4.1738272 -4.1944032 -4.2145209 -4.2184644 -4.1959519 -4.1653028 -4.1425161 -4.1360264 -4.1614161 -4.2023134 -4.2346115 -4.2490859 -4.2459478][-4.1660962 -4.1630449 -4.1781631 -4.1958332 -4.2036362 -4.1903009 -4.1549978 -4.1153307 -4.0917983 -4.0986028 -4.1484914 -4.2127848 -4.2592726 -4.2772694 -4.2676597][-4.1688237 -4.1682754 -4.1828961 -4.1911626 -4.1804438 -4.146369 -4.0925517 -4.0380964 -4.0164256 -4.0503054 -4.1282005 -4.2033973 -4.2498894 -4.2662992 -4.2568731][-4.1613035 -4.1657982 -4.1800852 -4.1822729 -4.161232 -4.1126752 -4.0498838 -3.9943881 -3.9844787 -4.0368872 -4.1184545 -4.1865144 -4.2246194 -4.2404466 -4.2356734][-4.1552043 -4.169035 -4.1846361 -4.1860504 -4.1661882 -4.1237044 -4.0743294 -4.0336967 -4.0306673 -4.071013 -4.1327815 -4.1844125 -4.2133355 -4.2278357 -4.226285][-4.1821146 -4.1999288 -4.2092052 -4.2049661 -4.1887045 -4.1609221 -4.1318736 -4.1072588 -4.1026483 -4.1262527 -4.1673131 -4.2011566 -4.2176795 -4.225606 -4.2230544][-4.2163205 -4.23138 -4.231977 -4.2209177 -4.2052298 -4.1894031 -4.1746488 -4.1632252 -4.1631751 -4.1819797 -4.2075562 -4.22115 -4.2217216 -4.2205071 -4.2145543][-4.226078 -4.234724 -4.2305603 -4.2202644 -4.20879 -4.2035003 -4.2004476 -4.2024765 -4.2104564 -4.2246408 -4.2335968 -4.2276645 -4.214437 -4.2088385 -4.2059026][-4.2231135 -4.2227945 -4.2198844 -4.2162704 -4.2124114 -4.2166023 -4.2246513 -4.2342114 -4.239511 -4.2421837 -4.2377629 -4.22334 -4.2096772 -4.2079878 -4.211669][-4.2064328 -4.1997476 -4.20182 -4.2101908 -4.2198563 -4.2349987 -4.2483554 -4.2546387 -4.2473059 -4.2376018 -4.2303095 -4.2235622 -4.2224107 -4.2292328 -4.2346091][-4.1772962 -4.1613703 -4.164248 -4.1876359 -4.2168612 -4.2477727 -4.2674885 -4.2702661 -4.2534966 -4.2368312 -4.2306237 -4.2322764 -4.2403531 -4.2500272 -4.25211]]...]
INFO - root - 2017-12-05 20:17:15.008974: step 37510, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 74h:01m:24s remains)
INFO - root - 2017-12-05 20:17:23.894621: step 37520, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.953 sec/batch; 78h:05m:05s remains)
INFO - root - 2017-12-05 20:17:33.102689: step 37530, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 71h:28m:45s remains)
INFO - root - 2017-12-05 20:17:42.153761: step 37540, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 73h:57m:18s remains)
INFO - root - 2017-12-05 20:17:51.324549: step 37550, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 77h:40m:43s remains)
INFO - root - 2017-12-05 20:18:00.553763: step 37560, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 74h:47m:04s remains)
INFO - root - 2017-12-05 20:18:09.557611: step 37570, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 75h:13m:34s remains)
INFO - root - 2017-12-05 20:18:18.759089: step 37580, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 73h:43m:43s remains)
INFO - root - 2017-12-05 20:18:27.848386: step 37590, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 69h:58m:37s remains)
INFO - root - 2017-12-05 20:18:36.798417: step 37600, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 72h:33m:58s remains)
2017-12-05 20:18:37.632751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2037697 -4.1871476 -4.1630173 -4.1510544 -4.1584768 -4.1716223 -4.1716089 -4.1600752 -4.1488309 -4.1493793 -4.1580677 -4.1690989 -4.1799479 -4.1847639 -4.18051][-4.1824884 -4.1604509 -4.1390915 -4.1275921 -4.1335006 -4.1466203 -4.1472039 -4.1376119 -4.1380796 -4.1523504 -4.1667862 -4.1750407 -4.1793828 -4.1799884 -4.17756][-4.1573887 -4.1412182 -4.1273904 -4.1183219 -4.1239076 -4.1379166 -4.1415997 -4.1364555 -4.1426396 -4.1581411 -4.1607494 -4.1562834 -4.1520314 -4.1513944 -4.153389][-4.1587753 -4.148746 -4.1376595 -4.1264758 -4.1289239 -4.140718 -4.1463256 -4.1495886 -4.1592665 -4.1683879 -4.1590877 -4.1423016 -4.1260562 -4.1194849 -4.1259041][-4.1772037 -4.1666393 -4.1494222 -4.13634 -4.13607 -4.140779 -4.145514 -4.1584473 -4.1736727 -4.1808381 -4.1654396 -4.1398072 -4.1143041 -4.1010013 -4.1114][-4.1969132 -4.1863518 -4.1665521 -4.1490812 -4.1389632 -4.1323504 -4.1364441 -4.1573544 -4.1806345 -4.1906977 -4.1757541 -4.1477923 -4.1208606 -4.1085005 -4.1223068][-4.2046251 -4.198442 -4.1767011 -4.146729 -4.1186934 -4.10038 -4.1074467 -4.1369786 -4.1708193 -4.1881576 -4.18286 -4.1651583 -4.1459765 -4.1409173 -4.1557083][-4.1961212 -4.1889243 -4.1596184 -4.1165609 -4.0767903 -4.0573897 -4.074595 -4.1128011 -4.1516819 -4.1740847 -4.1809063 -4.176826 -4.1706419 -4.1741309 -4.1865373][-4.1761112 -4.1673474 -4.1302309 -4.0801439 -4.0387454 -4.0243711 -4.0465264 -4.0864 -4.1242356 -4.1508412 -4.1651931 -4.1705 -4.1720991 -4.1782918 -4.1876121][-4.1681638 -4.1624727 -4.125433 -4.0787649 -4.0429578 -4.0309925 -4.0459313 -4.0782566 -4.1139193 -4.1376753 -4.1492405 -4.14965 -4.1450815 -4.1483183 -4.1548414][-4.1752677 -4.1734157 -4.1507988 -4.1213527 -4.0990391 -4.0868444 -4.0874815 -4.1077466 -4.134994 -4.1466637 -4.1468768 -4.1361341 -4.1210966 -4.1205277 -4.1233974][-4.1890121 -4.192544 -4.1836457 -4.164844 -4.1480861 -4.1358376 -4.1321611 -4.1470652 -4.1668072 -4.17123 -4.1643605 -4.1443844 -4.1211677 -4.1165404 -4.1171021][-4.1927142 -4.1945057 -4.1920581 -4.1785612 -4.1647749 -4.1562343 -4.1565976 -4.1699381 -4.1886373 -4.1937971 -4.185853 -4.16083 -4.135488 -4.1327209 -4.1383224][-4.1861186 -4.1814027 -4.1763806 -4.167562 -4.1630464 -4.162178 -4.1688733 -4.1813526 -4.1978984 -4.2054195 -4.1999397 -4.1780219 -4.154521 -4.1476283 -4.1517148][-4.1705232 -4.1649532 -4.16051 -4.1568394 -4.1587977 -4.1637859 -4.1724205 -4.1819887 -4.1977482 -4.2088532 -4.2088451 -4.1934519 -4.1698947 -4.1538515 -4.1457071]]...]
INFO - root - 2017-12-05 20:18:46.855969: step 37610, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 73h:18m:48s remains)
INFO - root - 2017-12-05 20:18:56.009526: step 37620, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 76h:56m:58s remains)
INFO - root - 2017-12-05 20:19:05.005350: step 37630, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 73h:14m:59s remains)
INFO - root - 2017-12-05 20:19:14.071350: step 37640, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 73h:56m:19s remains)
INFO - root - 2017-12-05 20:19:23.373678: step 37650, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 76h:44m:06s remains)
INFO - root - 2017-12-05 20:19:32.294114: step 37660, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 72h:33m:39s remains)
INFO - root - 2017-12-05 20:19:41.312415: step 37670, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 72h:07m:21s remains)
INFO - root - 2017-12-05 20:19:50.359157: step 37680, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 74h:57m:00s remains)
INFO - root - 2017-12-05 20:19:59.476411: step 37690, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 73h:01m:21s remains)
INFO - root - 2017-12-05 20:20:08.526867: step 37700, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 72h:18m:03s remains)
2017-12-05 20:20:09.366034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3098822 -4.2855945 -4.2600231 -4.2345352 -4.2155542 -4.2098627 -4.2260041 -4.2597656 -4.2915182 -4.3044953 -4.2933021 -4.2683425 -4.2437019 -4.218976 -4.1980214][-4.312727 -4.2929745 -4.2726183 -4.2512431 -4.2303677 -4.2197828 -4.2317004 -4.2646503 -4.2977624 -4.3127894 -4.3043375 -4.2819109 -4.2602544 -4.2381134 -4.21787][-4.3155408 -4.2986131 -4.2817254 -4.26228 -4.2395449 -4.2248554 -4.232913 -4.2612562 -4.2922592 -4.3086891 -4.3067484 -4.292871 -4.2806449 -4.2664533 -4.2516675][-4.3160539 -4.2978387 -4.28002 -4.2589774 -4.2329693 -4.2133923 -4.21774 -4.2420077 -4.2717009 -4.291975 -4.2998123 -4.297667 -4.2954822 -4.2892365 -4.280972][-4.3138156 -4.2916007 -4.2689672 -4.2426405 -4.209311 -4.182107 -4.1810336 -4.2044268 -4.2388554 -4.2663455 -4.2851686 -4.2950921 -4.3020606 -4.3030567 -4.2997632][-4.3112922 -4.2856669 -4.2575703 -4.2227354 -4.1808572 -4.1473589 -4.1431136 -4.1719775 -4.2169094 -4.2524228 -4.2767644 -4.2914 -4.3018 -4.3054643 -4.3027725][-4.3095989 -4.2835174 -4.2544041 -4.2156849 -4.1725321 -4.1412191 -4.1417241 -4.1775389 -4.2265873 -4.2619119 -4.2818584 -4.2917542 -4.2991467 -4.301106 -4.2965622][-4.3083014 -4.2846122 -4.2594309 -4.2234221 -4.1872692 -4.16648 -4.1733546 -4.2064104 -4.2468891 -4.2737932 -4.2863865 -4.2909007 -4.2945733 -4.2943797 -4.28822][-4.3063436 -4.2851672 -4.2647667 -4.2340708 -4.2054729 -4.1926217 -4.1990457 -4.2225423 -4.2524858 -4.2732244 -4.283433 -4.2868118 -4.2875123 -4.2862468 -4.2809796][-4.3023558 -4.2808657 -4.2626071 -4.2339787 -4.2059174 -4.1936464 -4.1972752 -4.2172379 -4.2441878 -4.2648816 -4.276248 -4.2805147 -4.2789464 -4.2777977 -4.2761769][-4.2959642 -4.2734175 -4.2558279 -4.22671 -4.1954317 -4.1792731 -4.1787815 -4.1982021 -4.2237406 -4.2458563 -4.2601085 -4.2661653 -4.2665234 -4.2689462 -4.2728276][-4.2893186 -4.2681651 -4.2514286 -4.2233076 -4.1916013 -4.1718011 -4.1640234 -4.1780739 -4.1990752 -4.2189574 -4.2326975 -4.2406135 -4.2479258 -4.2580333 -4.2693887][-4.2857361 -4.2680016 -4.25469 -4.2322259 -4.2073312 -4.1879745 -4.1730938 -4.174068 -4.183744 -4.1944757 -4.2021384 -4.2122841 -4.2291756 -4.2495923 -4.268837][-4.2835889 -4.2670913 -4.25384 -4.2337651 -4.2150235 -4.1998477 -4.1813846 -4.1727514 -4.172296 -4.174387 -4.1780772 -4.1921391 -4.2191095 -4.24857 -4.2720613][-4.2807913 -4.2620611 -4.2465644 -4.2254424 -4.2080812 -4.1964531 -4.179122 -4.1674495 -4.1625576 -4.1632252 -4.1696796 -4.1897821 -4.223875 -4.2561841 -4.2769675]]...]
INFO - root - 2017-12-05 20:20:18.325046: step 37710, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 75h:29m:28s remains)
INFO - root - 2017-12-05 20:20:27.533125: step 37720, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 75h:39m:07s remains)
INFO - root - 2017-12-05 20:20:36.643371: step 37730, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 75h:06m:06s remains)
INFO - root - 2017-12-05 20:20:45.757297: step 37740, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 76h:04m:56s remains)
INFO - root - 2017-12-05 20:20:54.729973: step 37750, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 72h:55m:53s remains)
INFO - root - 2017-12-05 20:21:03.874375: step 37760, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 76h:23m:23s remains)
INFO - root - 2017-12-05 20:21:13.111294: step 37770, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 74h:37m:50s remains)
INFO - root - 2017-12-05 20:21:22.206289: step 37780, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 74h:33m:08s remains)
INFO - root - 2017-12-05 20:21:31.302974: step 37790, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 76h:55m:22s remains)
INFO - root - 2017-12-05 20:21:40.340942: step 37800, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 74h:50m:28s remains)
2017-12-05 20:21:41.213133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2559729 -4.2637014 -4.2527814 -4.2146845 -4.1553144 -4.1019888 -4.0847626 -4.1051373 -4.1493363 -4.1869941 -4.2014213 -4.2043109 -4.1989584 -4.1918831 -4.1910772][-4.2497883 -4.2595005 -4.2521749 -4.2215319 -4.170898 -4.1229062 -4.1074047 -4.12725 -4.1710482 -4.206037 -4.2145791 -4.2135205 -4.2025719 -4.1925673 -4.1911783][-4.233973 -4.2457814 -4.2473836 -4.23084 -4.1962585 -4.1607819 -4.1466289 -4.1572852 -4.1882105 -4.213716 -4.2156725 -4.2109728 -4.1990056 -4.1870418 -4.1825871][-4.209054 -4.2267566 -4.241528 -4.2442307 -4.22723 -4.1999154 -4.1798277 -4.1747384 -4.1881785 -4.2006593 -4.1965089 -4.1910191 -4.1824069 -4.1755347 -4.1741366][-4.1915789 -4.2155528 -4.2398396 -4.2546248 -4.2451921 -4.2154655 -4.1836247 -4.1641407 -4.1680946 -4.17444 -4.1680317 -4.1631742 -4.1565313 -4.1538181 -4.1564436][-4.2012658 -4.2257423 -4.2448564 -4.2537832 -4.2372637 -4.1940956 -4.1452184 -4.1143508 -4.1212687 -4.1343536 -4.1348238 -4.1333871 -4.1288185 -4.1266618 -4.1330938][-4.2304029 -4.2465596 -4.2507634 -4.2429852 -4.2141428 -4.1562138 -4.084239 -4.0331454 -4.04943 -4.0870781 -4.1068168 -4.1126585 -4.1131186 -4.1123409 -4.1183882][-4.2439094 -4.2451081 -4.23297 -4.2112384 -4.1754146 -4.1100917 -4.0207334 -3.951545 -3.9832792 -4.0550861 -4.1016335 -4.11738 -4.1206789 -4.1177254 -4.1179385][-4.235465 -4.2203264 -4.1917362 -4.1599903 -4.128191 -4.0789132 -4.0043287 -3.9456921 -3.9891109 -4.0717621 -4.125185 -4.1448917 -4.1479635 -4.1419563 -4.1335921][-4.2142358 -4.1878037 -4.1509519 -4.1185646 -4.0995879 -4.0820165 -4.0481539 -4.022162 -4.0631557 -4.1238809 -4.1620674 -4.1753445 -4.1730452 -4.1597095 -4.1406164][-4.1922612 -4.1712227 -4.1423678 -4.1173611 -4.1076245 -4.1079068 -4.0998755 -4.0924139 -4.1211276 -4.1573887 -4.1830258 -4.1929245 -4.1889391 -4.1705685 -4.143712][-4.1832871 -4.1758819 -4.1617541 -4.1481133 -4.1427512 -4.1466374 -4.1439366 -4.1364794 -4.1483383 -4.1672821 -4.1891656 -4.201117 -4.1999249 -4.1826711 -4.1492028][-4.1835327 -4.1901908 -4.1911154 -4.1901655 -4.1875448 -4.1877027 -4.1797228 -4.162415 -4.1567822 -4.1651149 -4.1838431 -4.1971245 -4.1997666 -4.1872225 -4.1554079][-4.1918645 -4.2082815 -4.22009 -4.2266626 -4.2263994 -4.2227759 -4.210113 -4.18698 -4.167418 -4.1632061 -4.1701307 -4.1818218 -4.188158 -4.1835003 -4.1628189][-4.2026448 -4.225369 -4.2442546 -4.2560735 -4.255867 -4.2492733 -4.2349339 -4.2094941 -4.1802778 -4.1631942 -4.1563764 -4.1633968 -4.171813 -4.1759052 -4.1713209]]...]
INFO - root - 2017-12-05 20:21:50.297676: step 37810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 75h:50m:57s remains)
INFO - root - 2017-12-05 20:21:59.341092: step 37820, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 77h:08m:47s remains)
INFO - root - 2017-12-05 20:22:08.320935: step 37830, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 74h:55m:30s remains)
INFO - root - 2017-12-05 20:22:17.333993: step 37840, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 73h:41m:19s remains)
INFO - root - 2017-12-05 20:22:26.352914: step 37850, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 73h:49m:09s remains)
INFO - root - 2017-12-05 20:22:35.447141: step 37860, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.942 sec/batch; 77h:03m:45s remains)
INFO - root - 2017-12-05 20:22:44.474615: step 37870, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 70h:27m:16s remains)
INFO - root - 2017-12-05 20:22:53.619985: step 37880, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 72h:43m:50s remains)
INFO - root - 2017-12-05 20:23:02.714674: step 37890, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 74h:53m:30s remains)
INFO - root - 2017-12-05 20:23:11.731178: step 37900, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 75h:16m:43s remains)
2017-12-05 20:23:12.581865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2499027 -4.24564 -4.237752 -4.21648 -4.1933036 -4.1822672 -4.175631 -4.1680689 -4.174674 -4.1988668 -4.2190437 -4.2399263 -4.2665911 -4.2965837 -4.3220911][-4.258781 -4.252955 -4.2395592 -4.2108083 -4.184237 -4.1729016 -4.1649642 -4.1525126 -4.1579876 -4.1871223 -4.2159615 -4.2443581 -4.2763476 -4.3039346 -4.3258357][-4.2388453 -4.2330647 -4.2222719 -4.1971326 -4.1766839 -4.1707492 -4.1546474 -4.13009 -4.1304708 -4.1671767 -4.2084355 -4.2446523 -4.2816877 -4.3097134 -4.3266592][-4.1841779 -4.1784983 -4.1808023 -4.1722178 -4.1556144 -4.1395426 -4.1059623 -4.0669022 -4.0746584 -4.1281424 -4.18604 -4.2352509 -4.2819753 -4.3126431 -4.3264942][-4.1358867 -4.1329918 -4.1457996 -4.1408563 -4.1121058 -4.0675716 -3.9972475 -3.933109 -3.9647055 -4.0598769 -4.1475644 -4.2156873 -4.2758226 -4.31193 -4.3243775][-4.1340256 -4.1256108 -4.1294556 -4.1115494 -4.065351 -3.991365 -3.8688722 -3.7575386 -3.8204868 -3.9703004 -4.0949354 -4.1817551 -4.255002 -4.2996917 -4.317112][-4.1657658 -4.151319 -4.1424265 -4.1165833 -4.0658307 -3.984426 -3.8497484 -3.7239525 -3.784615 -3.9394238 -4.0684242 -4.1581693 -4.2343712 -4.2824955 -4.304852][-4.1813865 -4.1676345 -4.159842 -4.1432757 -4.1128225 -4.0544634 -3.9502294 -3.8508797 -3.8840404 -3.9919326 -4.088088 -4.1625824 -4.2303691 -4.2745023 -4.296762][-4.1937237 -4.1902156 -4.1930652 -4.1899004 -4.1769867 -4.137157 -4.056004 -3.9762056 -3.9913654 -4.0584836 -4.1262388 -4.1837029 -4.2435827 -4.2830629 -4.3018732][-4.2073116 -4.215066 -4.22204 -4.2246838 -4.2201452 -4.1874332 -4.117806 -4.0450788 -4.0466266 -4.092504 -4.1484375 -4.2002859 -4.258225 -4.2988605 -4.3170385][-4.2261839 -4.237339 -4.2405539 -4.2388067 -4.2350388 -4.2064495 -4.1457348 -4.0856309 -4.0814433 -4.1146021 -4.1642556 -4.2124333 -4.2666636 -4.3078523 -4.3282948][-4.2425351 -4.2495661 -4.2493544 -4.2449832 -4.2402611 -4.219852 -4.1710749 -4.126298 -4.1227674 -4.1477795 -4.1885805 -4.2294602 -4.2734656 -4.3100071 -4.3325925][-4.2463164 -4.2499938 -4.2477417 -4.2466059 -4.2466097 -4.2356825 -4.2040086 -4.17735 -4.1772771 -4.1951628 -4.2201967 -4.2471166 -4.2777119 -4.3079839 -4.3324676][-4.2451954 -4.2372265 -4.2305937 -4.2321239 -4.2369957 -4.234952 -4.2179346 -4.2069426 -4.2146449 -4.22988 -4.245913 -4.2634239 -4.2859221 -4.312386 -4.3366661][-4.2650795 -4.2448659 -4.2277865 -4.222702 -4.2282805 -4.2344694 -4.2307158 -4.2306833 -4.2412553 -4.2545776 -4.2686834 -4.2840838 -4.3025632 -4.3238292 -4.3432789]]...]
INFO - root - 2017-12-05 20:23:21.738825: step 37910, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 76h:06m:29s remains)
INFO - root - 2017-12-05 20:23:31.016972: step 37920, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 76h:24m:12s remains)
INFO - root - 2017-12-05 20:23:40.011799: step 37930, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 73h:40m:16s remains)
INFO - root - 2017-12-05 20:23:49.057292: step 37940, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 74h:04m:52s remains)
INFO - root - 2017-12-05 20:23:58.104277: step 37950, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 72h:11m:40s remains)
INFO - root - 2017-12-05 20:24:07.111988: step 37960, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 73h:41m:34s remains)
INFO - root - 2017-12-05 20:24:16.410559: step 37970, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 75h:20m:55s remains)
INFO - root - 2017-12-05 20:24:25.580079: step 37980, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 74h:36m:28s remains)
INFO - root - 2017-12-05 20:24:34.588991: step 37990, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.935 sec/batch; 76h:27m:36s remains)
INFO - root - 2017-12-05 20:24:43.501410: step 38000, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 72h:37m:55s remains)
2017-12-05 20:24:44.277079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.132163 -4.1267314 -4.1378059 -4.1556883 -4.1569991 -4.162641 -4.1702919 -4.1794219 -4.1918526 -4.1966877 -4.1916227 -4.1796937 -4.1596956 -4.1363888 -4.1179972][-4.141099 -4.1284657 -4.1265192 -4.1348667 -4.1262074 -4.1314292 -4.1418643 -4.1567864 -4.1755409 -4.1872325 -4.1845064 -4.1767387 -4.1582737 -4.1256614 -4.0987682][-4.1437225 -4.1288166 -4.1158681 -4.1120553 -4.0988512 -4.1053963 -4.1183004 -4.1322269 -4.1523418 -4.1708465 -4.1759844 -4.1769657 -4.1623559 -4.1202807 -4.0791636][-4.1362705 -4.1212258 -4.1061058 -4.0995016 -4.08909 -4.0932255 -4.100842 -4.1023216 -4.1140871 -4.138341 -4.1581187 -4.1713896 -4.1615391 -4.1170392 -4.0684657][-4.1334419 -4.1134315 -4.0934792 -4.0809827 -4.0679612 -4.0643725 -4.0669637 -4.0631423 -4.0674362 -4.0989966 -4.1352553 -4.1570649 -4.1531982 -4.1144466 -4.0630178][-4.1363993 -4.1034131 -4.0677671 -4.0345855 -4.0061975 -3.9929206 -4.0034585 -4.0148292 -4.032649 -4.0758853 -4.1223688 -4.143455 -4.1376734 -4.09994 -4.0461287][-4.1343722 -4.0894356 -4.0326109 -3.9690595 -3.9101098 -3.8794844 -3.9111681 -3.966857 -4.0218062 -4.0840788 -4.1298766 -4.1454096 -4.1360836 -4.0983253 -4.0534849][-4.1424303 -4.0904508 -4.0177822 -3.9301507 -3.8437314 -3.7975895 -3.8451838 -3.9397695 -4.0260224 -4.0982981 -4.129777 -4.1311316 -4.1164665 -4.0884671 -4.0635295][-4.1738 -4.1302228 -4.0682392 -3.9977083 -3.9244194 -3.8790658 -3.9126868 -3.9964428 -4.0785632 -4.1374288 -4.1456261 -4.1341205 -4.1186256 -4.0992017 -4.09092][-4.2070012 -4.1756911 -4.1337848 -4.0907545 -4.0454988 -4.0129776 -4.0268278 -4.0795612 -4.1396422 -4.1749105 -4.1650338 -4.1454096 -4.1275249 -4.1179996 -4.1224909][-4.2392583 -4.2185597 -4.1913576 -4.1679497 -4.140986 -4.1209755 -4.1251397 -4.1505375 -4.1814485 -4.1931491 -4.1754031 -4.151648 -4.134901 -4.1284871 -4.1365976][-4.2470093 -4.2369633 -4.2216496 -4.2120261 -4.1955051 -4.1876178 -4.1938 -4.2041368 -4.21029 -4.2034283 -4.1887188 -4.1734581 -4.1581025 -4.1456323 -4.1425524][-4.2423725 -4.2375426 -4.2276268 -4.2209315 -4.2121644 -4.2137475 -4.2259474 -4.2327352 -4.22685 -4.2112708 -4.1972728 -4.1853213 -4.172668 -4.15784 -4.1429086][-4.2358217 -4.2313857 -4.2245212 -4.2181993 -4.2141848 -4.223484 -4.2408738 -4.24997 -4.2442775 -4.2288866 -4.21645 -4.2058187 -4.1916647 -4.1746225 -4.15668][-4.2442937 -4.2379131 -4.2305193 -4.2219362 -4.2191749 -4.232265 -4.2531328 -4.2646842 -4.2614741 -4.2444563 -4.2309971 -4.2231312 -4.210536 -4.1916704 -4.1665125]]...]
INFO - root - 2017-12-05 20:24:53.477355: step 38010, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 72h:38m:15s remains)
INFO - root - 2017-12-05 20:25:02.608537: step 38020, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 74h:57m:32s remains)
INFO - root - 2017-12-05 20:25:11.554496: step 38030, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 70h:33m:07s remains)
INFO - root - 2017-12-05 20:25:20.631817: step 38040, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.896 sec/batch; 73h:19m:41s remains)
INFO - root - 2017-12-05 20:25:29.661722: step 38050, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.912 sec/batch; 74h:34m:35s remains)
INFO - root - 2017-12-05 20:25:38.893032: step 38060, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 74h:17m:03s remains)
INFO - root - 2017-12-05 20:25:47.924473: step 38070, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 74h:48m:25s remains)
INFO - root - 2017-12-05 20:25:56.854472: step 38080, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 74h:29m:38s remains)
INFO - root - 2017-12-05 20:26:05.975895: step 38090, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 71h:03m:02s remains)
INFO - root - 2017-12-05 20:26:15.114807: step 38100, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 75h:25m:11s remains)
2017-12-05 20:26:15.944842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3181849 -4.314157 -4.3136549 -4.31403 -4.3162494 -4.3194532 -4.3217077 -4.32018 -4.3147764 -4.3092084 -4.3068514 -4.3060207 -4.3053136 -4.3048358 -4.305625][-4.288537 -4.2806983 -4.2811117 -4.2853422 -4.2908988 -4.2939887 -4.2941527 -4.2886248 -4.2761393 -4.2652421 -4.2623825 -4.2655587 -4.2690811 -4.2702036 -4.2708526][-4.2556329 -4.2437906 -4.2443228 -4.2522063 -4.2597046 -4.260581 -4.2564874 -4.2482295 -4.2309604 -4.2146454 -4.2092671 -4.2128367 -4.2183509 -4.2231784 -4.2259021][-4.2250791 -4.2095237 -4.2081342 -4.2134275 -4.2167373 -4.2100635 -4.1977363 -4.187933 -4.1721792 -4.1565351 -4.1495256 -4.1517868 -4.1591539 -4.1664476 -4.171803][-4.1936269 -4.1704674 -4.1633711 -4.163065 -4.1594758 -4.1433334 -4.1194539 -4.107132 -4.097249 -4.0881958 -4.0885706 -4.0979667 -4.1109242 -4.118844 -4.1229048][-4.1610813 -4.1284261 -4.1136947 -4.1054959 -4.0881491 -4.0537696 -4.0105295 -3.9925747 -3.9964647 -4.0069971 -4.03073 -4.0553918 -4.07149 -4.0758481 -4.0793343][-4.1388626 -4.0988617 -4.0712671 -4.0478549 -4.0085244 -3.9408882 -3.8586922 -3.8280468 -3.8631835 -3.916244 -3.9725323 -4.0110135 -4.0226674 -4.0206008 -4.0239925][-4.1333618 -4.087873 -4.0502558 -4.012085 -3.9549479 -3.8586512 -3.7352777 -3.6882968 -3.7648375 -3.8600581 -3.9378815 -3.9860561 -3.9971898 -3.9926767 -3.9980059][-4.1525426 -4.1073937 -4.0704184 -4.0299721 -3.9777577 -3.8945494 -3.7921631 -3.7726798 -3.84579 -3.915617 -3.9694011 -4.0052614 -4.0125365 -4.0143075 -4.0250511][-4.1765604 -4.1316915 -4.1018505 -4.0685234 -4.0319715 -3.9751952 -3.9047136 -3.9081073 -3.9638443 -3.9999189 -4.0222607 -4.0367694 -4.0418463 -4.0529537 -4.06882][-4.17801 -4.1324391 -4.1070614 -4.0851378 -4.0634441 -4.0225358 -3.9700761 -3.9796796 -4.0213752 -4.0430784 -4.0539951 -4.0591884 -4.0587449 -4.0694838 -4.0889907][-4.1707888 -4.124505 -4.0958133 -4.076232 -4.0607386 -4.0291042 -3.9875908 -3.9967887 -4.033916 -4.0583134 -4.0734663 -4.0782022 -4.0724382 -4.0795078 -4.095921][-4.1714644 -4.1243196 -4.0916247 -4.0700164 -4.0515442 -4.0258441 -3.9988694 -4.0081868 -4.0413561 -4.0703759 -4.0880094 -4.0957036 -4.0943313 -4.100163 -4.1149187][-4.1787572 -4.1327567 -4.1031842 -4.0874128 -4.071497 -4.0489345 -4.0322356 -4.040525 -4.0694804 -4.098599 -4.1127191 -4.1236467 -4.1317348 -4.1372437 -4.1524496][-4.1937389 -4.1547818 -4.1306453 -4.1237888 -4.1137638 -4.0957732 -4.0841565 -4.0893817 -4.1147747 -4.1364207 -4.1411819 -4.1478972 -4.1603584 -4.169055 -4.186799]]...]
INFO - root - 2017-12-05 20:26:24.951623: step 38110, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 75h:07m:48s remains)
INFO - root - 2017-12-05 20:26:34.159574: step 38120, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 74h:58m:00s remains)
INFO - root - 2017-12-05 20:26:42.980075: step 38130, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 68h:35m:41s remains)
INFO - root - 2017-12-05 20:26:52.204702: step 38140, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 77h:31m:40s remains)
INFO - root - 2017-12-05 20:27:01.372711: step 38150, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.917 sec/batch; 74h:59m:27s remains)
INFO - root - 2017-12-05 20:27:10.558403: step 38160, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 74h:05m:25s remains)
INFO - root - 2017-12-05 20:27:19.650615: step 38170, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 70h:51m:36s remains)
INFO - root - 2017-12-05 20:27:28.841345: step 38180, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 79h:34m:44s remains)
INFO - root - 2017-12-05 20:27:38.020549: step 38190, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 75h:17m:16s remains)
INFO - root - 2017-12-05 20:27:47.162100: step 38200, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 73h:20m:57s remains)
2017-12-05 20:27:47.968152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3247309 -4.3181567 -4.3130116 -4.3114376 -4.3121738 -4.3137274 -4.3139191 -4.3143711 -4.3150334 -4.3150644 -4.3151622 -4.3140841 -4.3144937 -4.32286 -4.336163][-4.2965956 -4.2859564 -4.2784247 -4.2748656 -4.272121 -4.2709723 -4.2700248 -4.2715635 -4.2751327 -4.2763238 -4.2763319 -4.2749548 -4.276032 -4.2897234 -4.30994][-4.2618175 -4.2495036 -4.2433438 -4.2374935 -4.2266479 -4.2167835 -4.21155 -4.2152023 -4.2247806 -4.2303605 -4.234055 -4.2357841 -4.2397389 -4.2578931 -4.2846055][-4.2274618 -4.213006 -4.2089114 -4.2018137 -4.1847191 -4.1639233 -4.149456 -4.151032 -4.1663184 -4.1804485 -4.1912093 -4.197331 -4.2030535 -4.2234259 -4.2557278][-4.1980262 -4.1790309 -4.174633 -4.1663356 -4.1448164 -4.1101637 -4.0766792 -4.0671363 -4.0873423 -4.1150827 -4.1355095 -4.1469226 -4.1563497 -4.1793661 -4.2166386][-4.1676173 -4.1392884 -4.1278944 -4.1165404 -4.0905981 -4.0422945 -3.9822545 -3.9439535 -3.9629469 -4.017314 -4.0634561 -4.0870514 -4.1040535 -4.1334414 -4.1771603][-4.138474 -4.0969715 -4.0741043 -4.0551043 -4.0220366 -3.9617803 -3.8742156 -3.788857 -3.7852273 -3.8707843 -3.9557068 -3.99972 -4.0286021 -4.0727954 -4.1330333][-4.1281228 -4.0798221 -4.0482364 -4.0209603 -3.9827085 -3.9213421 -3.8236051 -3.7002323 -3.6528318 -3.7384605 -3.8436172 -3.9065089 -3.947758 -4.0054321 -4.0847507][-4.1510243 -4.1128006 -4.0881929 -4.0669718 -4.0388765 -3.995549 -3.9218931 -3.8183873 -3.7529323 -3.7828927 -3.8491011 -3.9021249 -3.9395168 -3.9952474 -4.0735459][-4.1905551 -4.1682272 -4.156158 -4.1463971 -4.1336579 -4.1103525 -4.0690479 -4.0052977 -3.9521677 -3.9430385 -3.966969 -3.9978192 -4.0264888 -4.0710778 -4.1303992][-4.2323084 -4.2220416 -4.2182584 -4.2163658 -4.2143531 -4.207027 -4.1911955 -4.1628361 -4.1326694 -4.1150112 -4.1134834 -4.1199579 -4.1349754 -4.1636896 -4.201673][-4.2769275 -4.2744341 -4.2753034 -4.2772121 -4.2793365 -4.2792506 -4.274085 -4.2643986 -4.2511258 -4.2399187 -4.2312503 -4.2262554 -4.2275062 -4.2430239 -4.2648754][-4.3122592 -4.3127694 -4.3158393 -4.3195 -4.3237023 -4.3259583 -4.3251057 -4.3211875 -4.3159146 -4.3115444 -4.3058763 -4.2995815 -4.2975316 -4.3049459 -4.3172121][-4.3352466 -4.3354135 -4.337543 -4.3400826 -4.3433456 -4.346559 -4.3487029 -4.348105 -4.3467026 -4.3454304 -4.3439555 -4.3422651 -4.341136 -4.3439069 -4.3487835][-4.3462605 -4.3444819 -4.343832 -4.3443074 -4.3456154 -4.347662 -4.3496928 -4.3510489 -4.3531933 -4.3546753 -4.3554869 -4.3553948 -4.3549662 -4.3551631 -4.3566604]]...]
INFO - root - 2017-12-05 20:27:57.099955: step 38210, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 75h:23m:04s remains)
INFO - root - 2017-12-05 20:28:06.388154: step 38220, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.956 sec/batch; 78h:08m:58s remains)
INFO - root - 2017-12-05 20:28:15.459602: step 38230, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 73h:54m:39s remains)
INFO - root - 2017-12-05 20:28:24.513799: step 38240, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 75h:56m:59s remains)
INFO - root - 2017-12-05 20:28:33.527011: step 38250, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 76h:22m:57s remains)
INFO - root - 2017-12-05 20:28:42.799597: step 38260, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.947 sec/batch; 77h:21m:59s remains)
INFO - root - 2017-12-05 20:28:51.961563: step 38270, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 73h:46m:31s remains)
INFO - root - 2017-12-05 20:29:01.145052: step 38280, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 76h:18m:43s remains)
INFO - root - 2017-12-05 20:29:10.303620: step 38290, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 74h:27m:17s remains)
INFO - root - 2017-12-05 20:29:19.522727: step 38300, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 72h:50m:10s remains)
2017-12-05 20:29:20.400583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3058748 -4.2951889 -4.2870083 -4.28194 -4.2751102 -4.2648835 -4.2522831 -4.2528186 -4.2773767 -4.2979584 -4.3011961 -4.3031993 -4.3052173 -4.3064137 -4.3087139][-4.3001928 -4.2907686 -4.2856579 -4.2848611 -4.2834945 -4.273735 -4.2581944 -4.2594528 -4.2893586 -4.313581 -4.3108311 -4.3028536 -4.2968607 -4.2954717 -4.2975883][-4.2930779 -4.2876248 -4.2865205 -4.2913814 -4.29397 -4.2834249 -4.2664194 -4.2676282 -4.2996826 -4.3203697 -4.3087697 -4.295692 -4.2864175 -4.2886586 -4.2957358][-4.280067 -4.2764063 -4.2763491 -4.2832613 -4.2871852 -4.2743673 -4.2553635 -4.2568722 -4.2946687 -4.3124118 -4.2958179 -4.2858248 -4.2814751 -4.29037 -4.3023992][-4.2609053 -4.2517629 -4.2446847 -4.2448177 -4.2432885 -4.227087 -4.2057528 -4.2101507 -4.2584753 -4.2831545 -4.2710547 -4.2681322 -4.2732625 -4.2894096 -4.3062148][-4.244616 -4.2260227 -4.2094235 -4.1972551 -4.1811175 -4.1501813 -4.1211443 -4.1275382 -4.1922426 -4.2329626 -4.2297115 -4.237452 -4.2566628 -4.2821207 -4.3020139][-4.2369556 -4.2077823 -4.1790609 -4.1485348 -4.1065474 -4.045002 -3.9975693 -4.0099196 -4.1001582 -4.1651692 -4.1767945 -4.2016726 -4.2360616 -4.2691402 -4.2926722][-4.2198825 -4.1745648 -4.1283822 -4.0779867 -4.0070195 -3.9074535 -3.8324008 -3.8480361 -3.9693911 -4.061121 -4.0910382 -4.1350532 -4.1834788 -4.22633 -4.2580485][-4.21818 -4.1652231 -4.10975 -4.0494962 -3.9647491 -3.8512616 -3.7637203 -3.7787137 -3.9081998 -4.0055614 -4.0348587 -4.0753503 -4.1248393 -4.1780295 -4.2224722][-4.2407284 -4.1989021 -4.1537752 -4.1028581 -4.0327077 -3.9387565 -3.8614845 -3.8618073 -3.9606745 -4.0307794 -4.0421047 -4.0665264 -4.1025133 -4.15514 -4.2026529][-4.2572155 -4.2310081 -4.2038574 -4.1738906 -4.1332321 -4.0711617 -4.0158353 -4.0065842 -4.062253 -4.0920992 -4.0807405 -4.0871539 -4.1110611 -4.1534452 -4.1921096][-4.2603736 -4.2422032 -4.2257266 -4.2139707 -4.1956291 -4.1591997 -4.1247544 -4.1175947 -4.1450872 -4.1470366 -4.1231022 -4.114203 -4.1292152 -4.1637621 -4.1975207][-4.2544165 -4.234962 -4.2220311 -4.2218261 -4.2153091 -4.1946521 -4.1748996 -4.172749 -4.1915956 -4.1886015 -4.1659341 -4.1509976 -4.1623044 -4.1902871 -4.2157903][-4.2577424 -4.2367668 -4.2255592 -4.226387 -4.2258372 -4.2155776 -4.2053804 -4.2061634 -4.2239528 -4.22425 -4.2082825 -4.1978407 -4.20997 -4.2293854 -4.2427168][-4.2660928 -4.2443743 -4.2307925 -4.2247281 -4.2209411 -4.2107234 -4.2010374 -4.2018862 -4.2187862 -4.2223268 -4.2148533 -4.2129045 -4.22837 -4.2446456 -4.2548747]]...]
INFO - root - 2017-12-05 20:29:29.557139: step 38310, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 72h:03m:00s remains)
INFO - root - 2017-12-05 20:29:38.482237: step 38320, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 72h:50m:56s remains)
INFO - root - 2017-12-05 20:29:47.756653: step 38330, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 73h:46m:06s remains)
INFO - root - 2017-12-05 20:29:56.926728: step 38340, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 77h:32m:33s remains)
INFO - root - 2017-12-05 20:30:06.101896: step 38350, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.949 sec/batch; 77h:31m:31s remains)
INFO - root - 2017-12-05 20:30:15.335255: step 38360, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 74h:41m:56s remains)
INFO - root - 2017-12-05 20:30:24.535324: step 38370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 70h:03m:10s remains)
INFO - root - 2017-12-05 20:30:33.700003: step 38380, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 75h:00m:08s remains)
INFO - root - 2017-12-05 20:30:42.710739: step 38390, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 73h:43m:01s remains)
INFO - root - 2017-12-05 20:30:51.772984: step 38400, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 75h:56m:37s remains)
2017-12-05 20:30:52.579121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1409845 -4.1180029 -4.1150575 -4.143827 -4.1822138 -4.2112565 -4.2292728 -4.2291417 -4.212337 -4.1925044 -4.1697373 -4.1524515 -4.15724 -4.1750164 -4.2125826][-4.1503577 -4.1333709 -4.1261673 -4.1460228 -4.17589 -4.2032065 -4.2250485 -4.2273555 -4.2178187 -4.2063255 -4.1861844 -4.170608 -4.1720147 -4.188436 -4.2265258][-4.1639881 -4.16017 -4.1534491 -4.16065 -4.1757913 -4.1932526 -4.2101541 -4.2134852 -4.2141 -4.2165418 -4.2038474 -4.1906128 -4.1885877 -4.204071 -4.2414041][-4.1849761 -4.1937413 -4.1899137 -4.182621 -4.1748595 -4.1711063 -4.1707497 -4.1755915 -4.1950154 -4.2195482 -4.2173405 -4.2036757 -4.2027688 -4.2182746 -4.2511053][-4.2097073 -4.2215462 -4.2158375 -4.1937232 -4.1601629 -4.1280479 -4.1039071 -4.1027074 -4.1424618 -4.1967778 -4.2156262 -4.2102594 -4.2117376 -4.2252216 -4.2534952][-4.2285151 -4.2344027 -4.2257123 -4.1955681 -4.1431313 -4.0789437 -4.0193791 -3.9938526 -4.0511532 -4.1423311 -4.1910148 -4.2010379 -4.2114263 -4.2270164 -4.2518234][-4.24181 -4.2397909 -4.2259884 -4.1914368 -4.12848 -4.0397587 -3.9317377 -3.8539622 -3.9233305 -4.0599895 -4.1444669 -4.1788287 -4.2049408 -4.2253261 -4.2493954][-4.248786 -4.2390947 -4.2205391 -4.1834226 -4.1219978 -4.0298629 -3.9003885 -3.7757494 -3.8338895 -3.9940343 -4.0989285 -4.1467361 -4.1870279 -4.2189884 -4.2460823][-4.2470956 -4.235661 -4.2158241 -4.1770945 -4.1262851 -4.0571451 -3.9598 -3.856385 -3.8767354 -3.9908531 -4.0765414 -4.1228523 -4.1677866 -4.2061663 -4.23846][-4.2341404 -4.2326274 -4.2185307 -4.1912594 -4.1561246 -4.1149421 -4.0564055 -3.9842956 -3.9762597 -4.0337286 -4.0866842 -4.1215677 -4.1602278 -4.1955104 -4.2274985][-4.2237263 -4.2383661 -4.2375722 -4.22763 -4.2114334 -4.191781 -4.1570206 -4.1070137 -4.0852418 -4.1016169 -4.1262646 -4.1441312 -4.1730905 -4.20258 -4.230885][-4.2229934 -4.246417 -4.2587514 -4.2628226 -4.261147 -4.254437 -4.2356052 -4.2029805 -4.17754 -4.16941 -4.1711864 -4.1714392 -4.1899409 -4.213933 -4.237721][-4.2339945 -4.2553926 -4.2735453 -4.2884665 -4.2961626 -4.2952709 -4.2880549 -4.2708707 -4.2464762 -4.2258797 -4.2092013 -4.1954427 -4.1989098 -4.2169209 -4.2372828][-4.2590079 -4.2734156 -4.2893243 -4.30687 -4.3196931 -4.325048 -4.3244562 -4.3162689 -4.2926083 -4.269228 -4.2431259 -4.2204895 -4.2124963 -4.2225537 -4.2408013][-4.2958293 -4.3014655 -4.3106194 -4.3243718 -4.3390064 -4.3486695 -4.3499408 -4.3453889 -4.3257709 -4.3051505 -4.2796259 -4.25409 -4.2397709 -4.239625 -4.2513056]]...]
INFO - root - 2017-12-05 20:31:01.508418: step 38410, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 75h:38m:15s remains)
INFO - root - 2017-12-05 20:31:10.607572: step 38420, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 72h:40m:36s remains)
INFO - root - 2017-12-05 20:31:19.917939: step 38430, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 75h:10m:01s remains)
INFO - root - 2017-12-05 20:31:29.153721: step 38440, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 76h:08m:40s remains)
INFO - root - 2017-12-05 20:31:38.365498: step 38450, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 72h:35m:56s remains)
INFO - root - 2017-12-05 20:31:47.295659: step 38460, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 76h:12m:38s remains)
INFO - root - 2017-12-05 20:31:56.368726: step 38470, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 77h:53m:42s remains)
INFO - root - 2017-12-05 20:32:05.570777: step 38480, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 75h:32m:08s remains)
INFO - root - 2017-12-05 20:32:14.718401: step 38490, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 75h:44m:27s remains)
INFO - root - 2017-12-05 20:32:23.657705: step 38500, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 74h:50m:52s remains)
2017-12-05 20:32:24.470324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2119322 -4.21047 -4.209177 -4.2055521 -4.206399 -4.2034931 -4.1995583 -4.2106705 -4.2290068 -4.24457 -4.2512412 -4.2494617 -4.2485981 -4.2545862 -4.2620759][-4.1930752 -4.1971521 -4.2045841 -4.2081957 -4.2140503 -4.2155323 -4.211802 -4.2168455 -4.2296429 -4.2429237 -4.2471027 -4.2409215 -4.2333088 -4.2341652 -4.241528][-4.1788206 -4.186605 -4.2003756 -4.2085962 -4.2181163 -4.2249289 -4.2272949 -4.2308369 -4.2383261 -4.2479215 -4.2473407 -4.2339625 -4.2157083 -4.2072077 -4.2122974][-4.1708269 -4.1813455 -4.1989694 -4.2075491 -4.215765 -4.224843 -4.2342715 -4.240591 -4.2482967 -4.2570443 -4.2530951 -4.2323065 -4.204278 -4.1876297 -4.1893458][-4.1732373 -4.18096 -4.1951246 -4.2005467 -4.20055 -4.2040329 -4.2149181 -4.2267938 -4.2418103 -4.2554803 -4.2536049 -4.2348619 -4.2065725 -4.187469 -4.1844578][-4.1741586 -4.1758022 -4.1824331 -4.1799593 -4.1684141 -4.1572742 -4.1594667 -4.1790671 -4.210278 -4.235827 -4.2434907 -4.2352962 -4.2183728 -4.203145 -4.1938138][-4.1737785 -4.1605582 -4.1517177 -4.1367235 -4.1108379 -4.0793657 -4.0661159 -4.0984216 -4.1566114 -4.2044187 -4.2249069 -4.2274852 -4.2235279 -4.2160482 -4.2030411][-4.2068481 -4.1809783 -4.1535997 -4.1179266 -4.0680776 -4.0025749 -3.9621522 -4.0078297 -4.0927963 -4.1612153 -4.1946507 -4.2109118 -4.2202654 -4.2197657 -4.2078714][-4.2484927 -4.2314959 -4.2054939 -4.1659951 -4.1115041 -4.0369153 -3.9838572 -4.0160418 -4.0918083 -4.1549582 -4.1870947 -4.2041874 -4.2155342 -4.21911 -4.2124567][-4.2651219 -4.2620215 -4.2496815 -4.2268548 -4.1945357 -4.144362 -4.103507 -4.1126471 -4.157536 -4.1966186 -4.21084 -4.2161055 -4.2163348 -4.2165136 -4.2140627][-4.2625837 -4.2651753 -4.2591386 -4.2516861 -4.2419958 -4.2184896 -4.1929817 -4.192996 -4.2174644 -4.2402697 -4.2416244 -4.23522 -4.2196355 -4.2085028 -4.2023292][-4.2581806 -4.2677493 -4.270153 -4.2709064 -4.2703786 -4.2590051 -4.2419548 -4.2388697 -4.2541537 -4.269485 -4.2649922 -4.2523413 -4.2277689 -4.2035689 -4.1893969][-4.2524848 -4.2633343 -4.2691889 -4.2735615 -4.2756548 -4.2675848 -4.2562079 -4.2554069 -4.2677407 -4.2780995 -4.2707534 -4.2552142 -4.231349 -4.204895 -4.1879816][-4.2485209 -4.2524562 -4.2514367 -4.2497687 -4.2491636 -4.2437868 -4.23958 -4.2451172 -4.2594047 -4.2720618 -4.2701554 -4.2564359 -4.2396307 -4.21935 -4.2020187][-4.2361627 -4.2295866 -4.2195525 -4.2121091 -4.2099752 -4.2092733 -4.21168 -4.2224612 -4.2379856 -4.2503853 -4.2509365 -4.2452745 -4.2365446 -4.2239022 -4.2107649]]...]
INFO - root - 2017-12-05 20:32:33.548376: step 38510, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 76h:07m:27s remains)
INFO - root - 2017-12-05 20:32:42.650178: step 38520, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 73h:41m:52s remains)
INFO - root - 2017-12-05 20:32:51.714506: step 38530, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 75h:04m:03s remains)
INFO - root - 2017-12-05 20:33:00.982699: step 38540, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 73h:18m:15s remains)
INFO - root - 2017-12-05 20:33:10.055913: step 38550, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 76h:50m:04s remains)
INFO - root - 2017-12-05 20:33:19.231359: step 38560, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 73h:28m:54s remains)
INFO - root - 2017-12-05 20:33:28.478797: step 38570, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 75h:30m:39s remains)
INFO - root - 2017-12-05 20:33:37.453461: step 38580, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.929 sec/batch; 75h:51m:37s remains)
INFO - root - 2017-12-05 20:33:46.637652: step 38590, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 76h:09m:07s remains)
INFO - root - 2017-12-05 20:33:55.591154: step 38600, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 72h:16m:37s remains)
2017-12-05 20:33:56.370109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2656331 -4.2511864 -4.2286243 -4.197835 -4.1589246 -4.1070294 -4.0525794 -4.0306511 -4.0720453 -4.1224666 -4.1610737 -4.18318 -4.1884856 -4.185039 -4.1819038][-4.2643261 -4.2561789 -4.2399626 -4.2158928 -4.1873584 -4.1567016 -4.1253533 -4.117063 -4.1459284 -4.1741257 -4.2031569 -4.2232838 -4.2268252 -4.2220693 -4.213716][-4.2632136 -4.26104 -4.2486238 -4.2268853 -4.2032604 -4.1925092 -4.1859493 -4.1890392 -4.2033691 -4.2145238 -4.2316532 -4.2497091 -4.2581973 -4.2585683 -4.2472916][-4.2602854 -4.2644963 -4.2536354 -4.2275505 -4.2013788 -4.1948638 -4.200737 -4.2146492 -4.2267103 -4.2313967 -4.2348361 -4.24149 -4.2519741 -4.2622113 -4.2574553][-4.2536221 -4.2639933 -4.2504597 -4.2173276 -4.184166 -4.1713986 -4.1752567 -4.1923175 -4.2068281 -4.2132616 -4.2085853 -4.2050948 -4.2124572 -4.226522 -4.2303491][-4.2460747 -4.2619853 -4.2449746 -4.2022362 -4.1568284 -4.1253004 -4.1087093 -4.1129236 -4.1304431 -4.1579442 -4.1713943 -4.1705465 -4.1704273 -4.1806512 -4.1840839][-4.2394037 -4.2574759 -4.2405152 -4.1950583 -4.1388025 -4.0807261 -4.024169 -3.9963994 -4.0256457 -4.0963359 -4.14259 -4.1545777 -4.1521382 -4.1538396 -4.1519642][-4.2320504 -4.2537556 -4.2395453 -4.1980281 -4.1389127 -4.06499 -3.9772658 -3.9165754 -3.9508224 -4.051764 -4.1179533 -4.1444397 -4.1487823 -4.1474729 -4.1434593][-4.2301745 -4.2531505 -4.2432857 -4.2097149 -4.1592793 -4.0947437 -4.0121055 -3.9502339 -3.9693165 -4.0481968 -4.1056929 -4.1398616 -4.155426 -4.1621461 -4.161962][-4.2440729 -4.2618341 -4.2532039 -4.2220469 -4.1783085 -4.1314006 -4.0713086 -4.0247574 -4.0289907 -4.08054 -4.1219878 -4.1516838 -4.170342 -4.1871934 -4.1943336][-4.2641349 -4.2736149 -4.2666874 -4.2369523 -4.1945457 -4.1545811 -4.1138396 -4.0852818 -4.0922513 -4.1332045 -4.1659455 -4.1886897 -4.2054362 -4.2239714 -4.2338614][-4.2775416 -4.2829018 -4.2792244 -4.2532139 -4.2133465 -4.1765347 -4.1499205 -4.138998 -4.1534238 -4.1943188 -4.2258329 -4.242291 -4.2564011 -4.2690811 -4.2781987][-4.2845683 -4.29008 -4.2907472 -4.27147 -4.2383618 -4.2075253 -4.1912251 -4.1913013 -4.2101078 -4.2488747 -4.2794218 -4.2954097 -4.306746 -4.3100362 -4.3143115][-4.2910514 -4.2960858 -4.299562 -4.2885003 -4.2659264 -4.2448673 -4.2347689 -4.2384372 -4.2576718 -4.2896423 -4.3157144 -4.3309784 -4.3385029 -4.3327942 -4.3266382][-4.2989855 -4.3016753 -4.3046517 -4.2988539 -4.28595 -4.2736773 -4.2680578 -4.2715459 -4.2861838 -4.3083811 -4.3278856 -4.33979 -4.3425164 -4.3291321 -4.314188]]...]
INFO - root - 2017-12-05 20:34:05.299054: step 38610, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 74h:00m:27s remains)
INFO - root - 2017-12-05 20:34:14.556032: step 38620, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.954 sec/batch; 77h:55m:00s remains)
INFO - root - 2017-12-05 20:34:23.834052: step 38630, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 76h:17m:32s remains)
INFO - root - 2017-12-05 20:34:32.980412: step 38640, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 76h:28m:11s remains)
INFO - root - 2017-12-05 20:34:42.001272: step 38650, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 75h:42m:18s remains)
INFO - root - 2017-12-05 20:34:51.024811: step 38660, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 73h:15m:14s remains)
INFO - root - 2017-12-05 20:35:00.006280: step 38670, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 74h:02m:54s remains)
INFO - root - 2017-12-05 20:35:09.268834: step 38680, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 75h:18m:52s remains)
INFO - root - 2017-12-05 20:35:18.219327: step 38690, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 73h:06m:46s remains)
INFO - root - 2017-12-05 20:35:27.395253: step 38700, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 77h:13m:40s remains)
2017-12-05 20:35:28.250282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1815081 -4.178586 -4.1727543 -4.1666007 -4.1691236 -4.1897335 -4.2129583 -4.2156181 -4.19868 -4.1714864 -4.1455674 -4.1391082 -4.1618543 -4.20138 -4.2381606][-4.1784964 -4.1804767 -4.171865 -4.1621404 -4.1667585 -4.1878524 -4.2069349 -4.2030053 -4.181529 -4.1582108 -4.1389742 -4.1385422 -4.1645732 -4.2084985 -4.2480865][-4.1866546 -4.1897421 -4.1788068 -4.1719694 -4.1807194 -4.1956778 -4.2014766 -4.1849275 -4.1560397 -4.1406765 -4.1364317 -4.1450143 -4.1764078 -4.2225261 -4.2610321][-4.1971197 -4.2007003 -4.1900711 -4.1853242 -4.1913848 -4.1970963 -4.1938515 -4.1700191 -4.1358318 -4.12136 -4.1287484 -4.1452932 -4.1845994 -4.2349277 -4.2751393][-4.214674 -4.2206068 -4.2118444 -4.2057104 -4.2035584 -4.1980748 -4.1853476 -4.1539979 -4.118597 -4.1043119 -4.1192117 -4.1476717 -4.1973166 -4.2508435 -4.29199][-4.2278914 -4.2365475 -4.2294397 -4.2212076 -4.2117214 -4.1946039 -4.1674709 -4.1259379 -4.089386 -4.0809679 -4.1072702 -4.1523933 -4.2121067 -4.2690344 -4.31137][-4.2333112 -4.2418222 -4.2329111 -4.2208395 -4.2072473 -4.1835155 -4.1415029 -4.0860143 -4.0480633 -4.0540829 -4.0970378 -4.1599278 -4.2263083 -4.2847533 -4.3280621][-4.2324886 -4.2329025 -4.2175789 -4.2012234 -4.1842241 -4.1594582 -4.1131687 -4.0532522 -4.0191622 -4.03955 -4.1020236 -4.17864 -4.2470479 -4.3022285 -4.3411593][-4.2281523 -4.2210035 -4.2015686 -4.1811051 -4.1571088 -4.1288047 -4.0852776 -4.0344906 -4.0105524 -4.0419044 -4.1151729 -4.1972418 -4.2637162 -4.3156757 -4.3494768][-4.229732 -4.2214794 -4.2028222 -4.1803079 -4.1493831 -4.1137595 -4.0710154 -4.0269666 -4.0131965 -4.050663 -4.1254387 -4.2054539 -4.2706971 -4.3212743 -4.3516707][-4.2444372 -4.2370658 -4.2196274 -4.1942706 -4.1585035 -4.1197729 -4.0792136 -4.0393414 -4.0289984 -4.0644231 -4.13236 -4.2053146 -4.2714725 -4.3215852 -4.34987][-4.2628241 -4.2558808 -4.2391615 -4.214088 -4.1793423 -4.1424279 -4.1054854 -4.0692306 -4.0598221 -4.0889263 -4.1462054 -4.2119246 -4.2750506 -4.3220968 -4.3458004][-4.2730417 -4.2699366 -4.2585483 -4.2398767 -4.2127061 -4.1802497 -4.1461635 -4.1146283 -4.1060677 -4.1304269 -4.1785021 -4.2342119 -4.287488 -4.3254046 -4.3432136][-4.2853041 -4.2851572 -4.2813258 -4.2718091 -4.2539673 -4.2270594 -4.1971049 -4.1710176 -4.1639905 -4.1837592 -4.2212324 -4.2637272 -4.3031731 -4.3303585 -4.3433933][-4.303751 -4.3029737 -4.3032746 -4.3006206 -4.291698 -4.2734122 -4.2509284 -4.2309265 -4.22485 -4.2377672 -4.2634616 -4.29285 -4.3188472 -4.3361392 -4.344542]]...]
INFO - root - 2017-12-05 20:35:37.317520: step 38710, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 75h:44m:31s remains)
INFO - root - 2017-12-05 20:35:46.419571: step 38720, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 74h:13m:14s remains)
INFO - root - 2017-12-05 20:35:55.420846: step 38730, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 73h:07m:53s remains)
INFO - root - 2017-12-05 20:36:04.588583: step 38740, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 76h:48m:01s remains)
INFO - root - 2017-12-05 20:36:13.670264: step 38750, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 73h:38m:44s remains)
INFO - root - 2017-12-05 20:36:22.837191: step 38760, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 76h:28m:30s remains)
INFO - root - 2017-12-05 20:36:31.891125: step 38770, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 75h:11m:45s remains)
INFO - root - 2017-12-05 20:36:41.051282: step 38780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 76h:14m:36s remains)
INFO - root - 2017-12-05 20:36:50.488295: step 38790, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 75h:18m:17s remains)
INFO - root - 2017-12-05 20:36:59.496115: step 38800, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 73h:45m:04s remains)
2017-12-05 20:37:00.366771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2253375 -4.2364488 -4.2420378 -4.2409177 -4.2374687 -4.2338352 -4.2303481 -4.23258 -4.2376132 -4.2383695 -4.2384548 -4.2375851 -4.2354541 -4.231431 -4.220849][-4.2318091 -4.2437491 -4.248055 -4.2440271 -4.2389941 -4.2367425 -4.2361054 -4.2403059 -4.2468 -4.25012 -4.2509937 -4.2490873 -4.2446342 -4.2369361 -4.2223406][-4.2383709 -4.2487221 -4.2502809 -4.2435288 -4.236969 -4.2363095 -4.238368 -4.2441254 -4.2516279 -4.2581449 -4.26147 -4.260253 -4.2556896 -4.247982 -4.2352557][-4.2433653 -4.2518353 -4.2501092 -4.2423463 -4.2344975 -4.2337704 -4.2362275 -4.2429848 -4.2526188 -4.2634959 -4.27131 -4.2732358 -4.2708349 -4.265377 -4.2568674][-4.2373161 -4.2446809 -4.2422919 -4.2356343 -4.2284145 -4.2269292 -4.2273507 -4.2338643 -4.245676 -4.2619457 -4.2762775 -4.2840295 -4.2852535 -4.2825451 -4.2764506][-4.2210975 -4.2263036 -4.2229986 -4.2168155 -4.210227 -4.2051673 -4.19957 -4.202054 -4.2144918 -4.2356591 -4.2561526 -4.2698312 -4.2764668 -4.2780066 -4.2739968][-4.1998053 -4.2033215 -4.1996756 -4.1911631 -4.1785994 -4.1604466 -4.139698 -4.1334419 -4.1449142 -4.1706958 -4.1966658 -4.2157936 -4.2283673 -4.2367225 -4.2390838][-4.1841416 -4.1891041 -4.1853862 -4.1729918 -4.1484323 -4.11022 -4.0693836 -4.0536203 -4.0654655 -4.0944576 -4.123517 -4.1457944 -4.1623445 -4.1762815 -4.1866345][-4.1837163 -4.1914411 -4.1896086 -4.1765876 -4.1440797 -4.0931597 -4.0412745 -4.0220423 -4.0358648 -4.0641661 -4.0912881 -4.1122723 -4.1286077 -4.1446829 -4.1581979][-4.1933646 -4.2034154 -4.2063241 -4.199585 -4.1733947 -4.12943 -4.0852351 -4.0703678 -4.0850391 -4.1089244 -4.1304951 -4.146327 -4.1583786 -4.1701379 -4.178328][-4.1996694 -4.2103329 -4.2168279 -4.2155356 -4.1981273 -4.1665096 -4.1352196 -4.1262312 -4.1404891 -4.1598229 -4.1751647 -4.1846242 -4.1901951 -4.1934195 -4.1913505][-4.2033877 -4.2128606 -4.2198668 -4.2203918 -4.2092757 -4.1888556 -4.1682582 -4.1616006 -4.1705971 -4.1821485 -4.1904058 -4.194519 -4.1944814 -4.1894007 -4.178071][-4.2137861 -4.2209978 -4.2259946 -4.2251635 -4.2168231 -4.2049928 -4.1927505 -4.1861038 -4.1882758 -4.192266 -4.1949377 -4.19607 -4.1942306 -4.1859574 -4.1693439][-4.2293243 -4.2351165 -4.237299 -4.2351265 -4.2292147 -4.2235327 -4.2173986 -4.2122669 -4.2112312 -4.2108345 -4.2102108 -4.2095852 -4.2073526 -4.2003579 -4.1847377][-4.2445955 -4.2498512 -4.2504654 -4.2487764 -4.2452035 -4.2428985 -4.2398853 -4.2374759 -4.2367268 -4.2355933 -4.2347355 -4.2342062 -4.2332973 -4.2299957 -4.2204332]]...]
INFO - root - 2017-12-05 20:37:09.451821: step 38810, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 75h:48m:28s remains)
INFO - root - 2017-12-05 20:37:18.829498: step 38820, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 74h:52m:08s remains)
INFO - root - 2017-12-05 20:37:27.814808: step 38830, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 75h:47m:52s remains)
INFO - root - 2017-12-05 20:37:36.816364: step 38840, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 74h:12m:56s remains)
INFO - root - 2017-12-05 20:37:45.845139: step 38850, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 74h:51m:14s remains)
INFO - root - 2017-12-05 20:37:54.857318: step 38860, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 73h:40m:06s remains)
INFO - root - 2017-12-05 20:38:04.092567: step 38870, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 75h:40m:01s remains)
INFO - root - 2017-12-05 20:38:13.292751: step 38880, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 75h:57m:22s remains)
INFO - root - 2017-12-05 20:38:22.309936: step 38890, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 71h:17m:00s remains)
INFO - root - 2017-12-05 20:38:31.300215: step 38900, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 74h:24m:38s remains)
2017-12-05 20:38:32.102368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.186595 -4.1757331 -4.1664157 -4.1854253 -4.2213831 -4.2409844 -4.228343 -4.2047782 -4.1929417 -4.2068214 -4.232 -4.2593107 -4.294518 -4.3085184 -4.3122411][-4.1816912 -4.1737757 -4.1705079 -4.1916451 -4.2255526 -4.237483 -4.2105021 -4.1699696 -4.1543169 -4.173244 -4.2058568 -4.2437935 -4.2864385 -4.3051395 -4.3108168][-4.1779466 -4.175859 -4.176723 -4.1927133 -4.2187853 -4.2189155 -4.1811647 -4.1300621 -4.1161852 -4.1409826 -4.1815529 -4.2292972 -4.2789197 -4.3042479 -4.3126354][-4.1820197 -4.1858354 -4.189105 -4.1972375 -4.2114463 -4.197618 -4.1524343 -4.0972953 -4.0856719 -4.1110411 -4.1538396 -4.2070861 -4.2653246 -4.3017654 -4.3128991][-4.1926947 -4.2021809 -4.2028341 -4.2014356 -4.2031932 -4.1768517 -4.1196094 -4.0539136 -4.038455 -4.0684137 -4.11758 -4.1796694 -4.2494555 -4.2985888 -4.3127232][-4.200655 -4.2109523 -4.2089739 -4.1994743 -4.1878157 -4.1480165 -4.0740418 -3.9899349 -3.9679673 -4.0151353 -4.0812812 -4.1554289 -4.2381511 -4.2987385 -4.3163548][-4.200572 -4.20878 -4.2033677 -4.1902704 -4.1694193 -4.119391 -4.031899 -3.9321837 -3.9101312 -3.9779205 -4.0659151 -4.15358 -4.2444592 -4.3077035 -4.324923][-4.1945219 -4.1937461 -4.1825294 -4.1682806 -4.1450648 -4.0928574 -4.0044503 -3.9091144 -3.8976769 -3.9805906 -4.083643 -4.1766281 -4.2648273 -4.3221841 -4.3360124][-4.1695018 -4.1566777 -4.1390572 -4.1269569 -4.1114292 -4.0717278 -4.0027294 -3.9341786 -3.9366777 -4.018075 -4.1175728 -4.2062917 -4.2861233 -4.3340197 -4.3445354][-4.1364417 -4.1124358 -4.0867314 -4.0758009 -4.0684104 -4.0474234 -4.0064092 -3.9714217 -3.9868698 -4.0621548 -4.1525073 -4.2334242 -4.3028779 -4.3414259 -4.3492794][-4.1076512 -4.0739408 -4.04346 -4.0316739 -4.0288768 -4.0201726 -4.0007534 -3.9917727 -4.0220904 -4.0955019 -4.1780171 -4.2499104 -4.30934 -4.341177 -4.3485432][-4.0983419 -4.0583267 -4.0274773 -4.0156713 -4.0116844 -4.0050688 -3.9977508 -4.0060067 -4.0486631 -4.1212034 -4.19711 -4.2599735 -4.3091488 -4.33535 -4.3433542][-4.1167321 -4.0827665 -4.0577517 -4.0478096 -4.040062 -4.0305276 -4.0261326 -4.04057 -4.0861888 -4.1539826 -4.2213798 -4.2741685 -4.3108535 -4.3305278 -4.3386579][-4.1546059 -4.1327734 -4.1126828 -4.1019087 -4.0941691 -4.0848742 -4.0815763 -4.0973773 -4.1417165 -4.1992736 -4.2537131 -4.2929358 -4.3168459 -4.330461 -4.3369575][-4.2019272 -4.1897769 -4.172936 -4.1641073 -4.1609278 -4.1546779 -4.1522126 -4.1661448 -4.2030764 -4.2488103 -4.2884908 -4.3126011 -4.3254485 -4.3337021 -4.3373694]]...]
INFO - root - 2017-12-05 20:38:41.267794: step 38910, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 76h:57m:23s remains)
INFO - root - 2017-12-05 20:38:50.210522: step 38920, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 74h:38m:44s remains)
INFO - root - 2017-12-05 20:38:59.270359: step 38930, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 72h:30m:33s remains)
INFO - root - 2017-12-05 20:39:08.482246: step 38940, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 77h:45m:39s remains)
INFO - root - 2017-12-05 20:39:17.657392: step 38950, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 74h:42m:11s remains)
INFO - root - 2017-12-05 20:39:26.617702: step 38960, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 71h:56m:52s remains)
INFO - root - 2017-12-05 20:39:35.520875: step 38970, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 75h:20m:05s remains)
INFO - root - 2017-12-05 20:39:44.675081: step 38980, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 74h:36m:22s remains)
INFO - root - 2017-12-05 20:39:53.891933: step 38990, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 75h:03m:05s remains)
INFO - root - 2017-12-05 20:40:03.072638: step 39000, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 75h:01m:06s remains)
2017-12-05 20:40:03.910090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3381085 -4.332972 -4.3251219 -4.3161597 -4.3098516 -4.3075686 -4.3101845 -4.3168616 -4.3256764 -4.33434 -4.3404365 -4.344275 -4.3472676 -4.3489852 -4.3490233][-4.3289833 -4.3260241 -4.3181071 -4.3079462 -4.2990489 -4.2919779 -4.2903266 -4.2956429 -4.3062167 -4.3176565 -4.3269782 -4.3335342 -4.3395066 -4.3441319 -4.3465452][-4.316679 -4.3159828 -4.3052344 -4.2899323 -4.2752171 -4.2602096 -4.2511988 -4.2556639 -4.2715449 -4.2885351 -4.3033781 -4.3155942 -4.327014 -4.3361254 -4.3417106][-4.2924113 -4.2947197 -4.2802491 -4.2588406 -4.2335944 -4.2059016 -4.1857085 -4.1889009 -4.2132335 -4.2394996 -4.2617331 -4.2821493 -4.3036013 -4.3227663 -4.3362522][-4.2522764 -4.2631364 -4.2501006 -4.2255745 -4.1919508 -4.1491928 -4.1118178 -4.1024833 -4.1243134 -4.1537371 -4.1843562 -4.2202268 -4.2604666 -4.2974 -4.32443][-4.1956024 -4.2245054 -4.2271771 -4.2148275 -4.1861439 -4.1390181 -4.0841064 -4.0431609 -4.0275068 -4.0310626 -4.0621715 -4.12106 -4.1916838 -4.2563972 -4.3038116][-4.1200814 -4.1666565 -4.190608 -4.1997166 -4.1925311 -4.1617241 -4.1094246 -4.0408854 -3.9735692 -3.9244187 -3.9337759 -4.0095444 -4.1114092 -4.2030392 -4.2717462][-4.0673137 -4.103601 -4.1360388 -4.1605506 -4.1738853 -4.1632681 -4.1242251 -4.0541215 -3.9711177 -3.8958237 -3.8823609 -3.9482734 -4.0511913 -4.1527877 -4.2358985][-4.0851989 -4.0930519 -4.1099372 -4.1326375 -4.1488357 -4.142221 -4.107955 -4.0466852 -3.9867473 -3.9413409 -3.9357133 -3.98353 -4.0571666 -4.1397424 -4.219327][-4.1312256 -4.1208053 -4.1224251 -4.1346478 -4.1410751 -4.1302452 -4.0961161 -4.0494161 -4.0188046 -4.0077648 -4.0185423 -4.0564876 -4.1078806 -4.1645336 -4.2245293][-4.1612468 -4.1373487 -4.125011 -4.1221495 -4.122993 -4.1208496 -4.1062593 -4.0866809 -4.0815878 -4.0876331 -4.1016731 -4.131156 -4.1666689 -4.2003727 -4.2392631][-4.1837192 -4.1463518 -4.1133437 -4.0895624 -4.0826354 -4.0990777 -4.1193848 -4.1324749 -4.1501513 -4.166832 -4.1813889 -4.2029729 -4.2245584 -4.2395644 -4.2603507][-4.2094193 -4.1714721 -4.1236496 -4.07564 -4.0550923 -4.0822 -4.1282845 -4.1625366 -4.1889 -4.2114916 -4.2312403 -4.2518106 -4.2649403 -4.2688079 -4.2773371][-4.2235913 -4.1972303 -4.15988 -4.1128287 -4.083168 -4.0960793 -4.134799 -4.1715269 -4.1988535 -4.2208428 -4.2414455 -4.2638359 -4.277977 -4.2804909 -4.2856979][-4.2205462 -4.2145228 -4.2014503 -4.1760216 -4.1489434 -4.1362176 -4.1409116 -4.1539059 -4.1675463 -4.1860609 -4.2108765 -4.2416453 -4.2654505 -4.275465 -4.2833991]]...]
INFO - root - 2017-12-05 20:40:12.878684: step 39010, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 71h:16m:30s remains)
INFO - root - 2017-12-05 20:40:22.160067: step 39020, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 73h:06m:50s remains)
INFO - root - 2017-12-05 20:40:31.177926: step 39030, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 73h:12m:29s remains)
INFO - root - 2017-12-05 20:40:40.111186: step 39040, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 74h:42m:58s remains)
INFO - root - 2017-12-05 20:40:49.167125: step 39050, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 74h:52m:06s remains)
INFO - root - 2017-12-05 20:40:58.084111: step 39060, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 73h:05m:47s remains)
INFO - root - 2017-12-05 20:41:07.234480: step 39070, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 73h:31m:22s remains)
INFO - root - 2017-12-05 20:41:16.451035: step 39080, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 77h:11m:43s remains)
INFO - root - 2017-12-05 20:41:25.483297: step 39090, loss = 2.02, batch loss = 1.96 (9.0 examples/sec; 0.887 sec/batch; 72h:19m:31s remains)
INFO - root - 2017-12-05 20:41:34.629421: step 39100, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 76h:29m:42s remains)
2017-12-05 20:41:35.511240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.245265 -4.2182193 -4.1983948 -4.1830587 -4.153533 -4.1041212 -4.0872316 -4.1556993 -4.2200837 -4.2205997 -4.2130976 -4.20316 -4.1893373 -4.2002454 -4.1968174][-4.2397809 -4.2044053 -4.1803446 -4.1655717 -4.1406317 -4.0925903 -4.0746307 -4.1411414 -4.2037992 -4.2073221 -4.2021313 -4.1954517 -4.1868305 -4.2006235 -4.2036028][-4.23566 -4.1994333 -4.1779976 -4.1671495 -4.1448636 -4.1017 -4.0821667 -4.140255 -4.2033124 -4.2158737 -4.2157607 -4.2094979 -4.2014956 -4.2126079 -4.2158618][-4.2447138 -4.2137113 -4.1988478 -4.1909204 -4.1710777 -4.1313577 -4.1042657 -4.1498938 -4.2154813 -4.2419395 -4.2481303 -4.2411404 -4.2302036 -4.2308064 -4.2294941][-4.2535491 -4.2222352 -4.2051969 -4.1933808 -4.1679144 -4.1233516 -4.0880108 -4.125711 -4.1962819 -4.2335572 -4.2442307 -4.2383447 -4.2248774 -4.2206 -4.2198749][-4.2475982 -4.2091079 -4.1771197 -4.1474428 -4.1065683 -4.0516567 -4.0138078 -4.0539484 -4.1319914 -4.1816745 -4.205483 -4.2074633 -4.1928949 -4.1883221 -4.1978][-4.2270622 -4.1848011 -4.1411228 -4.0978818 -4.0386214 -3.9574134 -3.9047332 -3.9353161 -4.0088482 -4.0726619 -4.1214843 -4.1381226 -4.1289539 -4.1357117 -4.16059][-4.2245564 -4.1889882 -4.1462 -4.0947 -4.0174141 -3.9145026 -3.841156 -3.848794 -3.89761 -3.9685812 -4.0437093 -4.0837612 -4.0942841 -4.1123753 -4.1418767][-4.251586 -4.2287021 -4.19787 -4.1574759 -4.087759 -3.9945395 -3.9286122 -3.9345541 -3.9667645 -4.0172629 -4.0836024 -4.1241164 -4.1425643 -4.1617918 -4.1837449][-4.2749753 -4.2611051 -4.2445779 -4.2202644 -4.1691747 -4.1008492 -4.0607314 -4.0802326 -4.1070461 -4.1310945 -4.1720009 -4.2014766 -4.2154818 -4.2249227 -4.2334924][-4.283226 -4.2717586 -4.2621026 -4.249608 -4.2141418 -4.1662483 -4.147615 -4.1767325 -4.2007241 -4.2096138 -4.2285795 -4.2424612 -4.2462435 -4.2456255 -4.2414484][-4.2820697 -4.2695136 -4.261724 -4.254879 -4.2316227 -4.2022038 -4.197619 -4.2298541 -4.2488375 -4.2461109 -4.2463913 -4.2453232 -4.2384973 -4.230197 -4.2196379][-4.2768517 -4.2616725 -4.2502608 -4.2456608 -4.2340302 -4.220005 -4.2248435 -4.2548027 -4.267508 -4.2594376 -4.2485943 -4.2348733 -4.216826 -4.1965556 -4.1836066][-4.266036 -4.245626 -4.2293024 -4.2255416 -4.2205663 -4.2135024 -4.2207541 -4.2448111 -4.2544732 -4.2471757 -4.235507 -4.2206674 -4.2022076 -4.181818 -4.171133][-4.26253 -4.2413907 -4.2239947 -4.2190833 -4.2138624 -4.2052402 -4.2090082 -4.227551 -4.2401719 -4.2375674 -4.2290287 -4.2173 -4.2028942 -4.1869273 -4.180191]]...]
INFO - root - 2017-12-05 20:41:44.621055: step 39110, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.954 sec/batch; 77h:46m:02s remains)
INFO - root - 2017-12-05 20:41:53.939201: step 39120, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 73h:14m:55s remains)
INFO - root - 2017-12-05 20:42:02.974171: step 39130, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.948 sec/batch; 77h:14m:26s remains)
INFO - root - 2017-12-05 20:42:12.234164: step 39140, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 73h:51m:31s remains)
INFO - root - 2017-12-05 20:42:21.628490: step 39150, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 74h:18m:22s remains)
INFO - root - 2017-12-05 20:42:30.581395: step 39160, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 76h:04m:36s remains)
INFO - root - 2017-12-05 20:42:39.560406: step 39170, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 73h:51m:28s remains)
INFO - root - 2017-12-05 20:42:48.733012: step 39180, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 76h:37m:22s remains)
INFO - root - 2017-12-05 20:42:57.831002: step 39190, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.916 sec/batch; 74h:38m:29s remains)
INFO - root - 2017-12-05 20:43:06.964680: step 39200, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 73h:20m:41s remains)
2017-12-05 20:43:07.681256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2388649 -4.2477088 -4.2533379 -4.2586451 -4.2849894 -4.2991977 -4.2925768 -4.2817059 -4.2727742 -4.2791266 -4.2917824 -4.3005929 -4.3033633 -4.3010135 -4.2907462][-4.2390723 -4.2368555 -4.2295008 -4.2231441 -4.2430258 -4.2597489 -4.2596064 -4.2555552 -4.2511139 -4.2624087 -4.2734184 -4.2757449 -4.2735167 -4.26875 -4.2512803][-4.224576 -4.2122016 -4.1934495 -4.1768284 -4.1893506 -4.2077065 -4.2142448 -4.2202992 -4.2245665 -4.2383094 -4.2412405 -4.2357717 -4.2264466 -4.2157116 -4.1905818][-4.2139511 -4.1989589 -4.1776824 -4.1563549 -4.1597314 -4.1737795 -4.1791377 -4.1902442 -4.2071242 -4.2284117 -4.2231369 -4.2080107 -4.1904221 -4.1717505 -4.1397247][-4.2124929 -4.1977634 -4.1775136 -4.1574655 -4.1573224 -4.158412 -4.1488962 -4.1515241 -4.1784616 -4.2158689 -4.2153649 -4.1971827 -4.175024 -4.148572 -4.1093445][-4.2154613 -4.2049294 -4.1818342 -4.1637931 -4.1606407 -4.1383791 -4.0952659 -4.07552 -4.114 -4.1753883 -4.19145 -4.1867814 -4.17823 -4.159327 -4.1200175][-4.213933 -4.2034945 -4.1738529 -4.148334 -4.1283126 -4.0649166 -3.9662483 -3.9254093 -3.9965367 -4.0975595 -4.1390142 -4.1534882 -4.164279 -4.1565461 -4.12343][-4.1937165 -4.1808281 -4.1490827 -4.1185818 -4.0787954 -3.974771 -3.8187032 -3.7514975 -3.8564785 -4.0012541 -4.0669436 -4.0975571 -4.1209083 -4.122407 -4.1004171][-4.1659379 -4.159894 -4.142313 -4.1258163 -4.0943909 -4.0021873 -3.8675148 -3.7958245 -3.8657966 -3.9802952 -4.0341444 -4.0660944 -4.0969472 -4.1108017 -4.1052589][-4.1560559 -4.1641073 -4.1657023 -4.1700921 -4.1607957 -4.1107769 -4.0400066 -3.9970479 -4.01851 -4.0618129 -4.080945 -4.1022291 -4.1311703 -4.1518435 -4.1591778][-4.1709104 -4.1855922 -4.1976175 -4.2120719 -4.2177119 -4.1995335 -4.1689482 -4.1391029 -4.1321836 -4.1338015 -4.135695 -4.1524715 -4.1819773 -4.2054725 -4.2183285][-4.1967559 -4.2164989 -4.2309089 -4.2458224 -4.2566452 -4.2557893 -4.2459664 -4.2292843 -4.2144151 -4.1991587 -4.1937504 -4.20673 -4.231142 -4.250947 -4.2624974][-4.2129841 -4.2375517 -4.257174 -4.2744656 -4.2863255 -4.2904258 -4.2894073 -4.282104 -4.2709284 -4.2583661 -4.2528596 -4.2574706 -4.2688203 -4.2789397 -4.2871709][-4.2398515 -4.2633653 -4.2853746 -4.3024473 -4.312098 -4.3147626 -4.3140383 -4.3097072 -4.302794 -4.2978525 -4.2966361 -4.2969866 -4.2983723 -4.3007112 -4.3041635][-4.2777424 -4.2920232 -4.3067069 -4.31754 -4.3235965 -4.3252225 -4.3244705 -4.3226128 -4.3195167 -4.3187156 -4.3184056 -4.3167725 -4.3153906 -4.3156562 -4.3170033]]...]
INFO - root - 2017-12-05 20:43:16.873855: step 39210, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 74h:47m:49s remains)
INFO - root - 2017-12-05 20:43:26.106421: step 39220, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 76h:52m:35s remains)
INFO - root - 2017-12-05 20:43:35.310536: step 39230, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 76h:52m:35s remains)
INFO - root - 2017-12-05 20:43:44.447531: step 39240, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 71h:36m:14s remains)
INFO - root - 2017-12-05 20:43:53.435527: step 39250, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 75h:26m:21s remains)
INFO - root - 2017-12-05 20:44:02.497632: step 39260, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 74h:34m:37s remains)
INFO - root - 2017-12-05 20:44:11.567496: step 39270, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:45m:04s remains)
INFO - root - 2017-12-05 20:44:20.537642: step 39280, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 72h:05m:17s remains)
INFO - root - 2017-12-05 20:44:29.690537: step 39290, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 73h:05m:32s remains)
INFO - root - 2017-12-05 20:44:38.667082: step 39300, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 71h:25m:14s remains)
2017-12-05 20:44:39.452657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281703 -4.3273082 -4.3230438 -4.3195014 -4.3200588 -4.3233533 -4.3254132 -4.3253412 -4.3246689 -4.3250318 -4.3263869 -4.3272696 -4.3307128 -4.3345242 -4.3353682][-4.3244305 -4.3250918 -4.32166 -4.319612 -4.3205862 -4.3240318 -4.3233619 -4.3180723 -4.314599 -4.3139076 -4.3135366 -4.3134131 -4.3184452 -4.3270655 -4.3302813][-4.3231263 -4.3222346 -4.3176947 -4.314136 -4.3129644 -4.31546 -4.3093052 -4.2957554 -4.2843285 -4.2815742 -4.2855515 -4.2920322 -4.3036933 -4.3190365 -4.3266687][-4.31847 -4.3160648 -4.3082447 -4.29882 -4.2925806 -4.2933598 -4.2844868 -4.2628107 -4.2435308 -4.2429209 -4.2569942 -4.2744493 -4.292335 -4.3093605 -4.3179007][-4.3038549 -4.300561 -4.2897015 -4.2766676 -4.2680454 -4.2686424 -4.2569251 -4.2264223 -4.2007728 -4.2044125 -4.2297425 -4.2584038 -4.2797174 -4.2944679 -4.2995081][-4.279748 -4.2772236 -4.2662091 -4.2503242 -4.2405024 -4.238132 -4.2216663 -4.1869168 -4.1587553 -4.1645179 -4.1994505 -4.2355533 -4.2608056 -4.2769532 -4.2802196][-4.2461748 -4.2456651 -4.2346773 -4.2129664 -4.1957164 -4.1891556 -4.1781077 -4.15306 -4.1272583 -4.1358972 -4.1806192 -4.2216744 -4.2456617 -4.2630377 -4.2678933][-4.1905704 -4.1862159 -4.1767411 -4.1554184 -4.1351781 -4.1319675 -4.1434379 -4.1401072 -4.1288562 -4.1449175 -4.1885386 -4.2225657 -4.2345281 -4.2469735 -4.2524209][-4.108892 -4.1034765 -4.1095438 -4.1079531 -4.099432 -4.11123 -4.1430321 -4.1558394 -4.1598759 -4.1792431 -4.2067118 -4.2256637 -4.2222733 -4.21856 -4.2125349][-4.0317559 -4.0399752 -4.0756974 -4.1071258 -4.1297822 -4.1545939 -4.1829948 -4.1906366 -4.18647 -4.1941223 -4.2023854 -4.2063046 -4.1856475 -4.1581612 -4.1353006][-4.0249143 -4.0566196 -4.1093082 -4.1506624 -4.1737776 -4.1834388 -4.1881018 -4.176755 -4.1538987 -4.1467452 -4.148169 -4.1488948 -4.1242456 -4.0837369 -4.052844][-4.0708141 -4.1107707 -4.1581874 -4.1810327 -4.17866 -4.1602364 -4.1452913 -4.1244307 -4.0844274 -4.0605936 -4.0664363 -4.0826774 -4.0763063 -4.057055 -4.0509844][-4.1254787 -4.1521711 -4.18086 -4.178915 -4.1482677 -4.1080604 -4.0899291 -4.06622 -4.0186563 -3.9958506 -4.0222898 -4.0607815 -4.0770984 -4.0851655 -4.1020284][-4.1776595 -4.1822052 -4.1857424 -4.1673965 -4.1235294 -4.0822387 -4.0695505 -4.0504313 -4.0165043 -4.0126486 -4.0532789 -4.0951662 -4.1152005 -4.1271458 -4.1475167][-4.2238846 -4.2158446 -4.2036743 -4.1756573 -4.1362071 -4.1083331 -4.1013255 -4.0909405 -4.0732732 -4.0866356 -4.1301517 -4.1639142 -4.1776381 -4.1858644 -4.2070127]]...]
INFO - root - 2017-12-05 20:44:48.516049: step 39310, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 74h:11m:16s remains)
INFO - root - 2017-12-05 20:44:57.532045: step 39320, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 72h:00m:09s remains)
INFO - root - 2017-12-05 20:45:06.652492: step 39330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 74h:04m:43s remains)
INFO - root - 2017-12-05 20:45:15.840105: step 39340, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 73h:59m:36s remains)
INFO - root - 2017-12-05 20:45:25.081926: step 39350, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 74h:54m:09s remains)
INFO - root - 2017-12-05 20:45:34.211627: step 39360, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 75h:01m:57s remains)
INFO - root - 2017-12-05 20:45:43.432539: step 39370, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 77h:11m:21s remains)
INFO - root - 2017-12-05 20:45:52.475990: step 39380, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 73h:13m:56s remains)
INFO - root - 2017-12-05 20:46:01.453751: step 39390, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 74h:12m:27s remains)
INFO - root - 2017-12-05 20:46:10.686767: step 39400, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 75h:18m:22s remains)
2017-12-05 20:46:11.517613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3043489 -4.2925816 -4.2874355 -4.2845941 -4.2836556 -4.2793832 -4.27307 -4.2672677 -4.2659736 -4.2760406 -4.2856331 -4.2885489 -4.28997 -4.28778 -4.2835336][-4.2868528 -4.2673793 -4.2589951 -4.255609 -4.2536211 -4.2451115 -4.23394 -4.2239285 -4.2225718 -4.239871 -4.25644 -4.26018 -4.26085 -4.2573805 -4.25258][-4.2697182 -4.2438431 -4.2301 -4.2238688 -4.2177033 -4.2022381 -4.1797976 -4.1587296 -4.1551781 -4.1830831 -4.2108297 -4.218504 -4.2206597 -4.2167153 -4.21057][-4.25687 -4.2255654 -4.20383 -4.1911569 -4.1765676 -4.1507277 -4.1103845 -4.0678768 -4.05704 -4.1003242 -4.1506228 -4.171515 -4.1808076 -4.1813164 -4.178803][-4.2470145 -4.2090092 -4.1761069 -4.1523757 -4.125937 -4.0855212 -4.0247488 -3.9463124 -3.9158773 -3.9819348 -4.0638661 -4.106092 -4.1292815 -4.1424041 -4.1510558][-4.2464986 -4.2027931 -4.1582155 -4.1206961 -4.0809383 -4.0299339 -3.9517372 -3.8403451 -3.7898967 -3.8825788 -3.9959867 -4.0608549 -4.0948243 -4.1138687 -4.1287532][-4.255065 -4.2089577 -4.1554742 -4.1055579 -4.0567565 -4.0050344 -3.9299147 -3.82402 -3.7787242 -3.8699059 -3.9857354 -4.0579381 -4.093616 -4.1135588 -4.128005][-4.2665796 -4.2244864 -4.1752787 -4.1297207 -4.0907493 -4.05409 -4.0025706 -3.9269433 -3.8923101 -3.9509985 -4.0342941 -4.0937405 -4.123457 -4.14039 -4.1527109][-4.2775078 -4.2430634 -4.2043514 -4.170784 -4.1469193 -4.1268325 -4.0981808 -4.05285 -4.0289254 -4.0621409 -4.1154747 -4.1577859 -4.1787715 -4.189383 -4.1948085][-4.2904315 -4.2650175 -4.2376256 -4.2169409 -4.2063069 -4.1983075 -4.1832466 -4.1604805 -4.1474476 -4.1653042 -4.1982117 -4.2243371 -4.2355466 -4.2415786 -4.2457013][-4.3017306 -4.2836809 -4.2650471 -4.2508874 -4.2455773 -4.24382 -4.2374916 -4.2311397 -4.2289791 -4.2402635 -4.2594471 -4.2736163 -4.2764864 -4.2800975 -4.2842078][-4.310389 -4.2982016 -4.2856956 -4.2756233 -4.272254 -4.2757163 -4.2784014 -4.2828112 -4.2860732 -4.2925572 -4.2999787 -4.3022056 -4.2995391 -4.3023329 -4.30537][-4.3185706 -4.3104711 -4.3028874 -4.2969275 -4.2955184 -4.3007841 -4.308908 -4.3176632 -4.3222504 -4.3259687 -4.3255544 -4.3212171 -4.3174047 -4.31862 -4.318809][-4.3268523 -4.3223891 -4.31885 -4.3162632 -4.3160825 -4.3207293 -4.3285589 -4.3364248 -4.3397784 -4.3415365 -4.3391623 -4.3347368 -4.3313365 -4.3300447 -4.3273649][-4.3339214 -4.3326764 -4.331408 -4.329916 -4.3293662 -4.3320408 -4.3370619 -4.3423448 -4.3445663 -4.3444657 -4.3420167 -4.338366 -4.3348579 -4.3326139 -4.3304839]]...]
INFO - root - 2017-12-05 20:46:20.751644: step 39410, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 74h:36m:32s remains)
INFO - root - 2017-12-05 20:46:29.691260: step 39420, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 71h:45m:56s remains)
INFO - root - 2017-12-05 20:46:38.698545: step 39430, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 72h:26m:08s remains)
INFO - root - 2017-12-05 20:46:47.754458: step 39440, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 74h:09m:07s remains)
INFO - root - 2017-12-05 20:46:56.879274: step 39450, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 74h:20m:27s remains)
INFO - root - 2017-12-05 20:47:05.953102: step 39460, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.937 sec/batch; 76h:15m:33s remains)
INFO - root - 2017-12-05 20:47:15.054666: step 39470, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.957 sec/batch; 77h:55m:37s remains)
INFO - root - 2017-12-05 20:47:24.188292: step 39480, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 73h:21m:52s remains)
INFO - root - 2017-12-05 20:47:33.143633: step 39490, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 71h:41m:02s remains)
INFO - root - 2017-12-05 20:47:42.131795: step 39500, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 74h:24m:01s remains)
2017-12-05 20:47:42.912225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1162753 -4.139533 -4.158884 -4.1660266 -4.1766224 -4.1934528 -4.19942 -4.1925373 -4.1964049 -4.2125349 -4.2223468 -4.2081213 -4.1772809 -4.1335831 -4.0982904][-4.0515332 -4.0977783 -4.1457858 -4.1733885 -4.1899738 -4.204751 -4.2021875 -4.1898727 -4.1904969 -4.2044511 -4.2091928 -4.188539 -4.1466808 -4.0961561 -4.0602903][-3.9959538 -4.0674419 -4.1420474 -4.1857004 -4.1994662 -4.1997218 -4.1798029 -4.1533761 -4.1438322 -4.1595492 -4.16853 -4.1548576 -4.12302 -4.0898848 -4.0735841][-4.0385776 -4.114151 -4.188386 -4.2197113 -4.2109179 -4.1849861 -4.1395445 -4.0903544 -4.0717025 -4.0953746 -4.1207104 -4.1298375 -4.1264691 -4.1221366 -4.1304412][-4.12546 -4.1843338 -4.23416 -4.2384286 -4.2033315 -4.150619 -4.0804305 -4.0137167 -4.0040612 -4.0499144 -4.1019478 -4.1395736 -4.1622443 -4.1768842 -4.1945682][-4.1765575 -4.2145028 -4.23742 -4.2144437 -4.1473823 -4.0643268 -3.9717417 -3.8942549 -3.9182041 -4.0062556 -4.0870714 -4.1489854 -4.1918874 -4.2124486 -4.2236285][-4.20161 -4.217804 -4.2145872 -4.1667724 -4.0704851 -3.9672933 -3.8661792 -3.7930162 -3.8589783 -3.981353 -4.0757833 -4.1480851 -4.2008772 -4.2192526 -4.2179141][-4.2280326 -4.2225046 -4.1963067 -4.1382952 -4.0444665 -3.9567606 -3.8866615 -3.8502886 -3.9221678 -4.034646 -4.1146803 -4.1727042 -4.2164865 -4.22736 -4.2162][-4.2301497 -4.21234 -4.1735168 -4.11671 -4.0472741 -3.9939921 -3.9663706 -3.9657717 -4.0232091 -4.1068358 -4.163888 -4.2013359 -4.2313843 -4.2343121 -4.2218013][-4.2098613 -4.185771 -4.144877 -4.0970716 -4.0528069 -4.0263839 -4.02691 -4.0500674 -4.0959625 -4.1543465 -4.1945009 -4.2161674 -4.2307549 -4.2277536 -4.2195139][-4.2030258 -4.1770487 -4.1431322 -4.1152325 -4.0942931 -4.0834122 -4.0976782 -4.1297832 -4.1642208 -4.1976609 -4.2199659 -4.2298603 -4.2328176 -4.2267165 -4.2229714][-4.2288795 -4.2064285 -4.1851234 -4.1777306 -4.1741681 -4.172545 -4.1885862 -4.217433 -4.2394261 -4.2543612 -4.26478 -4.2683549 -4.2633281 -4.2543497 -4.2506127][-4.276432 -4.259254 -4.2480845 -4.2515111 -4.2564478 -4.2581692 -4.2700677 -4.2888651 -4.2990427 -4.3030362 -4.309186 -4.3129559 -4.30757 -4.2994051 -4.2953639][-4.3127213 -4.3010511 -4.29785 -4.3054786 -4.3124952 -4.3147478 -4.32259 -4.3332295 -4.3354464 -4.3335977 -4.3361015 -4.3382697 -4.3330836 -4.327776 -4.3254151][-4.3267517 -4.3194213 -4.3196206 -4.3261647 -4.3318529 -4.3329506 -4.33565 -4.3395667 -4.3388381 -4.3366394 -4.338016 -4.3393927 -4.3359461 -4.3324909 -4.3315024]]...]
INFO - root - 2017-12-05 20:47:52.115306: step 39510, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 74h:42m:09s remains)
INFO - root - 2017-12-05 20:48:01.227723: step 39520, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 73h:19m:20s remains)
INFO - root - 2017-12-05 20:48:10.281418: step 39530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 70h:55m:25s remains)
INFO - root - 2017-12-05 20:48:19.416101: step 39540, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 73h:31m:02s remains)
INFO - root - 2017-12-05 20:48:28.617985: step 39550, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 77h:12m:26s remains)
INFO - root - 2017-12-05 20:48:37.718576: step 39560, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 69h:26m:43s remains)
INFO - root - 2017-12-05 20:48:46.790128: step 39570, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 73h:58m:46s remains)
INFO - root - 2017-12-05 20:48:55.767340: step 39580, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 74h:58m:27s remains)
INFO - root - 2017-12-05 20:49:04.920723: step 39590, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 76h:36m:45s remains)
INFO - root - 2017-12-05 20:49:14.069602: step 39600, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.946 sec/batch; 76h:56m:37s remains)
2017-12-05 20:49:14.910305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1835723 -4.2131982 -4.2362561 -4.2279563 -4.2076755 -4.1897087 -4.1900392 -4.2064824 -4.22811 -4.2417188 -4.2387071 -4.2281151 -4.2229185 -4.2238083 -4.2240143][-4.1736155 -4.2028003 -4.2222667 -4.2121515 -4.1917496 -4.170661 -4.1648121 -4.1790156 -4.2007957 -4.2149515 -4.2163777 -4.2111506 -4.2087903 -4.2075891 -4.2039967][-4.1630936 -4.1891956 -4.2055807 -4.1929245 -4.1684036 -4.1436872 -4.1360664 -4.150888 -4.1749511 -4.1901956 -4.1934686 -4.1897449 -4.1870041 -4.182538 -4.1782069][-4.1701803 -4.1913471 -4.2037015 -4.1833282 -4.1427402 -4.1018729 -4.0902019 -4.1132655 -4.145443 -4.1651683 -4.167788 -4.15899 -4.148891 -4.1373258 -4.1323233][-4.1849365 -4.202137 -4.2099338 -4.1802931 -4.1167951 -4.0486126 -4.0284123 -4.068871 -4.1226215 -4.1568613 -4.1602497 -4.1449308 -4.1275554 -4.11086 -4.1053925][-4.1911211 -4.2051783 -4.2092352 -4.1738963 -4.0912452 -3.9886644 -3.9514027 -4.0132623 -4.098043 -4.1514263 -4.1649494 -4.1537514 -4.1358466 -4.1176891 -4.1114707][-4.1856422 -4.1969547 -4.2011805 -4.1682854 -4.0766921 -3.946295 -3.8909349 -3.9700921 -4.07819 -4.146421 -4.1735954 -4.1742496 -4.1605535 -4.1444349 -4.1385856][-4.1704035 -4.1831203 -4.1945572 -4.1716976 -4.0922732 -3.9719534 -3.9158564 -3.983927 -4.081223 -4.1436749 -4.1743703 -4.1836157 -4.1761723 -4.1686664 -4.1679864][-4.1504498 -4.1668382 -4.1849804 -4.1758146 -4.1230664 -4.0413194 -3.997031 -4.0340834 -4.0988917 -4.1414204 -4.1668792 -4.1797032 -4.1802526 -4.1819649 -4.1862621][-4.1272163 -4.1480465 -4.1755633 -4.1824694 -4.1552892 -4.1032829 -4.0671706 -4.0802703 -4.1167989 -4.1407318 -4.1558456 -4.1636314 -4.1662369 -4.1742954 -4.186306][-4.1080775 -4.13119 -4.16822 -4.1907492 -4.1799407 -4.1437106 -4.1150546 -4.117342 -4.1400404 -4.153955 -4.1607528 -4.1617012 -4.1607547 -4.1705422 -4.1862273][-4.1136942 -4.1347208 -4.1711369 -4.1968007 -4.1937704 -4.1677289 -4.1467023 -4.1462512 -4.1611295 -4.1680841 -4.169456 -4.1682897 -4.1665053 -4.1762843 -4.1905494][-4.1460614 -4.1638508 -4.1920342 -4.2123351 -4.2104926 -4.191329 -4.17381 -4.1692753 -4.1771173 -4.179224 -4.1774778 -4.1772981 -4.178988 -4.1887603 -4.2000504][-4.1809196 -4.1973524 -4.2183881 -4.2304444 -4.223928 -4.2099357 -4.1943378 -4.1851616 -4.1891909 -4.1902628 -4.1890593 -4.1930585 -4.20079 -4.212554 -4.2221465][-4.2062273 -4.218914 -4.2354345 -4.2439089 -4.2370291 -4.2269311 -4.2124043 -4.2010093 -4.2028918 -4.2049837 -4.2066469 -4.2141905 -4.22497 -4.2361588 -4.2432184]]...]
INFO - root - 2017-12-05 20:49:23.955268: step 39610, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 75h:51m:36s remains)
INFO - root - 2017-12-05 20:49:33.226084: step 39620, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 73h:41m:19s remains)
INFO - root - 2017-12-05 20:49:42.375344: step 39630, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 73h:51m:43s remains)
INFO - root - 2017-12-05 20:49:51.310521: step 39640, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 74h:37m:09s remains)
INFO - root - 2017-12-05 20:50:00.423302: step 39650, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 73h:37m:57s remains)
INFO - root - 2017-12-05 20:50:09.595026: step 39660, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 72h:46m:21s remains)
INFO - root - 2017-12-05 20:50:18.457694: step 39670, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 67h:39m:50s remains)
INFO - root - 2017-12-05 20:50:27.496321: step 39680, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 74h:04m:28s remains)
INFO - root - 2017-12-05 20:50:36.688062: step 39690, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 75h:31m:59s remains)
INFO - root - 2017-12-05 20:50:45.933734: step 39700, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 74h:30m:47s remains)
2017-12-05 20:50:46.813999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2837954 -4.2849474 -4.2658691 -4.2402658 -4.2254715 -4.2308149 -4.2549057 -4.2870326 -4.3134546 -4.3287272 -4.3348475 -4.3368444 -4.3381023 -4.3368959 -4.32964][-4.3158774 -4.3143921 -4.2922726 -4.2614636 -4.2366719 -4.2305613 -4.2452364 -4.2721443 -4.2978354 -4.3157153 -4.3273764 -4.3349423 -4.339397 -4.3380322 -4.3273153][-4.3367343 -4.3362293 -4.31779 -4.2897286 -4.2623968 -4.2472315 -4.2485542 -4.2620854 -4.279798 -4.2962432 -4.3125329 -4.32658 -4.335114 -4.3345656 -4.3222647][-4.3458986 -4.3483419 -4.33816 -4.31907 -4.2960439 -4.2754126 -4.2623553 -4.2583137 -4.2632728 -4.2765651 -4.2982674 -4.3199635 -4.3329792 -4.3342052 -4.3231759][-4.3442955 -4.3503542 -4.3489404 -4.3398471 -4.3225603 -4.2982416 -4.272047 -4.2514105 -4.245235 -4.2570496 -4.2846708 -4.3139133 -4.3326468 -4.3376489 -4.3302131][-4.3278632 -4.3351817 -4.3380942 -4.334559 -4.3191881 -4.2894335 -4.25102 -4.2183313 -4.2079167 -4.2239261 -4.2590284 -4.2956862 -4.3207464 -4.3319888 -4.3305545][-4.3024912 -4.3087621 -4.3108 -4.3050752 -4.2832942 -4.2424164 -4.1899576 -4.1469431 -4.1366343 -4.1616454 -4.2077026 -4.2547121 -4.2888837 -4.3088045 -4.3161726][-4.2804217 -4.2830911 -4.2794685 -4.2655048 -4.2335963 -4.1794367 -4.1102095 -4.0520215 -4.0384922 -4.0742841 -4.13631 -4.196352 -4.2406993 -4.2694559 -4.2861419][-4.2682195 -4.2675557 -4.2568645 -4.2328825 -4.1891894 -4.1203828 -4.03424 -3.9604657 -3.9427681 -3.9882936 -4.0661941 -4.1393757 -4.1936364 -4.2295265 -4.2526741][-4.2647109 -4.2626123 -4.2477508 -4.218194 -4.1682944 -4.0944853 -4.0058603 -3.9331238 -3.9170704 -3.9638183 -4.0450873 -4.1216469 -4.1774311 -4.2143874 -4.2370338][-4.2606311 -4.2595177 -4.2455177 -4.218504 -4.1764622 -4.1164923 -4.0459275 -3.9910464 -3.9823914 -4.0235744 -4.0938683 -4.1597872 -4.2071934 -4.2371674 -4.2521362][-4.24769 -4.2498064 -4.2425218 -4.2282491 -4.2073059 -4.1765442 -4.1381822 -4.1074228 -4.1056561 -4.1360493 -4.1852927 -4.2303605 -4.2624 -4.2805829 -4.286973][-4.2317109 -4.238718 -4.2412362 -4.24342 -4.2456183 -4.2434883 -4.2350459 -4.2246633 -4.2267933 -4.24507 -4.2719164 -4.2949147 -4.3099208 -4.3162355 -4.3166442][-4.2283864 -4.2374492 -4.2449059 -4.2555776 -4.2690353 -4.2809691 -4.2884274 -4.289803 -4.2932768 -4.3019385 -4.3122106 -4.3191204 -4.3220706 -4.3217831 -4.3219881][-4.2399421 -4.2451706 -4.2510962 -4.2628708 -4.2797465 -4.2953959 -4.3058476 -4.3090262 -4.3104692 -4.3125725 -4.3138776 -4.3126793 -4.3103027 -4.3090978 -4.3122735]]...]
INFO - root - 2017-12-05 20:50:55.886612: step 39710, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 73h:40m:10s remains)
INFO - root - 2017-12-05 20:51:04.889614: step 39720, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 73h:39m:08s remains)
INFO - root - 2017-12-05 20:51:13.955634: step 39730, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 73h:26m:38s remains)
INFO - root - 2017-12-05 20:51:23.148102: step 39740, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 73h:42m:04s remains)
INFO - root - 2017-12-05 20:51:32.247865: step 39750, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 72h:38m:32s remains)
INFO - root - 2017-12-05 20:51:41.336009: step 39760, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 74h:29m:47s remains)
INFO - root - 2017-12-05 20:51:50.396422: step 39770, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.905 sec/batch; 73h:33m:31s remains)
INFO - root - 2017-12-05 20:51:59.488336: step 39780, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 74h:02m:00s remains)
INFO - root - 2017-12-05 20:52:08.591158: step 39790, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 75h:52m:17s remains)
INFO - root - 2017-12-05 20:52:17.725446: step 39800, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 73h:07m:05s remains)
2017-12-05 20:52:18.723812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2512507 -4.23595 -4.2335591 -4.2425995 -4.2582936 -4.274322 -4.2765865 -4.2633553 -4.2366304 -4.2072473 -4.1910744 -4.1848583 -4.1924314 -4.2115884 -4.234098][-4.2459064 -4.2328873 -4.2349257 -4.2491612 -4.2688513 -4.2836323 -4.2774134 -4.25195 -4.2108216 -4.1755519 -4.1616163 -4.1635 -4.1817932 -4.2078862 -4.23136][-4.2552524 -4.2500992 -4.2567515 -4.269289 -4.2827735 -4.2900758 -4.2741971 -4.2397966 -4.191339 -4.161006 -4.1574588 -4.1678724 -4.190217 -4.2191348 -4.2435851][-4.26974 -4.2741656 -4.2832246 -4.2896624 -4.2940235 -4.28869 -4.259336 -4.2159095 -4.1659961 -4.1454258 -4.1567116 -4.1739354 -4.1994243 -4.2310081 -4.2565584][-4.2862692 -4.2970748 -4.3054056 -4.3053527 -4.2967629 -4.2755232 -4.231266 -4.1745167 -4.1231971 -4.1188469 -4.1516705 -4.1808496 -4.2128277 -4.244051 -4.264842][-4.3014722 -4.3126636 -4.3158579 -4.3056164 -4.2823353 -4.2435241 -4.1849594 -4.1081843 -4.0461278 -4.0634766 -4.1305275 -4.1864023 -4.2306428 -4.2606087 -4.2733665][-4.3077116 -4.3147006 -4.3099809 -4.2903743 -4.256443 -4.2063293 -4.1290307 -4.0235028 -3.9449806 -3.9875922 -4.0973511 -4.1884427 -4.2468443 -4.2749743 -4.2802639][-4.3026094 -4.300756 -4.2879357 -4.2654696 -4.2310629 -4.1760988 -4.0787563 -3.9416838 -3.8560109 -3.9306955 -4.076695 -4.1931663 -4.2563562 -4.2803879 -4.28137][-4.2856846 -4.2802591 -4.2662272 -4.2481294 -4.217041 -4.1608987 -4.0513778 -3.8976917 -3.8277164 -3.9361424 -4.0980105 -4.2126961 -4.2665567 -4.2839732 -4.2819991][-4.27734 -4.2713747 -4.2581873 -4.2444315 -4.2175765 -4.1631274 -4.0591779 -3.925607 -3.8919554 -4.010006 -4.151638 -4.2402291 -4.2788725 -4.2903194 -4.2881155][-4.2692437 -4.2633762 -4.2490005 -4.2354188 -4.2110858 -4.1657319 -4.0885959 -4.0006852 -3.9997363 -4.1030092 -4.2068849 -4.2667027 -4.2908816 -4.2987313 -4.2969928][-4.2527838 -4.2468939 -4.2305131 -4.2185574 -4.2021151 -4.1694956 -4.1175008 -4.0659871 -4.0787497 -4.1586065 -4.2334995 -4.2741346 -4.2902222 -4.2945008 -4.2930231][-4.2323155 -4.2288976 -4.2149935 -4.2093935 -4.2053556 -4.1871262 -4.1555381 -4.1246748 -4.1335721 -4.1857872 -4.2387924 -4.27043 -4.2818141 -4.2823982 -4.2802787][-4.2215047 -4.2196193 -4.2103415 -4.2125368 -4.2215376 -4.2207561 -4.2060618 -4.1878767 -4.1879921 -4.2167649 -4.2495728 -4.2730789 -4.2814379 -4.2779179 -4.2733932][-4.2255697 -4.22119 -4.213037 -4.2191629 -4.2369032 -4.2498527 -4.2496676 -4.2429605 -4.2417579 -4.256279 -4.2732306 -4.28801 -4.2964125 -4.2929506 -4.2853718]]...]
INFO - root - 2017-12-05 20:52:27.699178: step 39810, loss = 2.02, batch loss = 1.96 (8.7 examples/sec; 0.916 sec/batch; 74h:26m:06s remains)
INFO - root - 2017-12-05 20:52:36.959535: step 39820, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 74h:39m:01s remains)
INFO - root - 2017-12-05 20:52:46.102884: step 39830, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 71h:21m:42s remains)
INFO - root - 2017-12-05 20:52:55.220092: step 39840, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 74h:51m:21s remains)
INFO - root - 2017-12-05 20:53:04.409271: step 39850, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 71h:45m:25s remains)
INFO - root - 2017-12-05 20:53:13.441504: step 39860, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 72h:31m:19s remains)
INFO - root - 2017-12-05 20:53:22.417510: step 39870, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 72h:34m:39s remains)
INFO - root - 2017-12-05 20:53:31.465226: step 39880, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 72h:01m:46s remains)
INFO - root - 2017-12-05 20:53:40.745541: step 39890, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 76h:15m:21s remains)
INFO - root - 2017-12-05 20:53:49.699452: step 39900, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 73h:46m:01s remains)
2017-12-05 20:53:50.512110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.224628 -4.248394 -4.2688093 -4.2779312 -4.2725086 -4.2613072 -4.2556396 -4.2460093 -4.2162485 -4.1653528 -4.1256623 -4.1295757 -4.1606307 -4.2035136 -4.2531357][-4.2051039 -4.231461 -4.25247 -4.2569122 -4.2449942 -4.2281842 -4.2179275 -4.2057042 -4.1788516 -4.1365981 -4.1036887 -4.1129761 -4.1466746 -4.1937261 -4.2493267][-4.175797 -4.2059326 -4.2260265 -4.2240157 -4.2044415 -4.1863685 -4.1753154 -4.1614494 -4.1427774 -4.1250653 -4.1098418 -4.1240835 -4.1537638 -4.2000985 -4.2565088][-4.1364684 -4.1642241 -4.1816711 -4.181397 -4.1663609 -4.1514297 -4.1379485 -4.1163082 -4.1038523 -4.1171994 -4.127481 -4.1457834 -4.1700015 -4.2155328 -4.2690287][-4.1055856 -4.129931 -4.146615 -4.151011 -4.140975 -4.1229486 -4.0934405 -4.0503516 -4.0361118 -4.0870609 -4.1352205 -4.1681166 -4.1934881 -4.2372079 -4.2857838][-4.1189342 -4.1419082 -4.1548767 -4.1541057 -4.1381273 -4.1015782 -4.0322795 -3.9381406 -3.9057016 -4.0047317 -4.109129 -4.1760635 -4.2145691 -4.2590508 -4.3005996][-4.1624222 -4.1829023 -4.1856136 -4.17215 -4.1397743 -4.0748382 -3.9505155 -3.7794838 -3.7096415 -3.8596976 -4.0318327 -4.1460404 -4.2128191 -4.2659683 -4.303834][-4.2090874 -4.22872 -4.2228985 -4.2000647 -4.1537404 -4.0654864 -3.9056847 -3.6875474 -3.587266 -3.7631736 -3.9727592 -4.1122684 -4.1969347 -4.2575507 -4.2962537][-4.2332468 -4.2564321 -4.2559781 -4.2351236 -4.1902752 -4.10091 -3.954061 -3.7681947 -3.6823292 -3.8159795 -3.9904873 -4.1123776 -4.1891861 -4.2461238 -4.2846475][-4.2290797 -4.2596712 -4.2740388 -4.2659793 -4.2319107 -4.1594634 -4.0549746 -3.9329269 -3.8747375 -3.9453697 -4.0499144 -4.1332641 -4.18821 -4.2335372 -4.2704358][-4.2174535 -4.2509317 -4.2776542 -4.2849169 -4.2638741 -4.2114115 -4.1471095 -4.0742083 -4.033452 -4.0544372 -4.1008844 -4.1473384 -4.184123 -4.2219071 -4.2591648][-4.2151542 -4.2485847 -4.2814631 -4.29632 -4.2836394 -4.2481728 -4.2129335 -4.1726084 -4.1401544 -4.1323652 -4.1429591 -4.1640463 -4.1880336 -4.2185435 -4.2554922][-4.2212324 -4.2520685 -4.2818608 -4.2975621 -4.2907825 -4.2714553 -4.25597 -4.23604 -4.2097411 -4.1839385 -4.1707397 -4.1771669 -4.1963382 -4.2232084 -4.2581692][-4.2358074 -4.2622895 -4.2861805 -4.29882 -4.2937775 -4.2845316 -4.2801294 -4.2692871 -4.247776 -4.217237 -4.1955142 -4.1990671 -4.2168436 -4.2388158 -4.2678423][-4.2561288 -4.277689 -4.2949371 -4.3033848 -4.2984695 -4.2924309 -4.2905622 -4.2836146 -4.2678075 -4.2440062 -4.2255311 -4.2270851 -4.2394605 -4.2558136 -4.2769637]]...]
INFO - root - 2017-12-05 20:53:59.639476: step 39910, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 75h:27m:29s remains)
INFO - root - 2017-12-05 20:54:08.712264: step 39920, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.874 sec/batch; 71h:03m:48s remains)
INFO - root - 2017-12-05 20:54:17.726804: step 39930, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 71h:10m:18s remains)
INFO - root - 2017-12-05 20:54:26.930575: step 39940, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 73h:18m:31s remains)
INFO - root - 2017-12-05 20:54:35.997964: step 39950, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 71h:05m:04s remains)
INFO - root - 2017-12-05 20:54:44.835815: step 39960, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 70h:48m:54s remains)
INFO - root - 2017-12-05 20:54:53.998966: step 39970, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 73h:51m:17s remains)
INFO - root - 2017-12-05 20:55:03.053582: step 39980, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 73h:08m:59s remains)
INFO - root - 2017-12-05 20:55:12.083808: step 39990, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 73h:12m:56s remains)
INFO - root - 2017-12-05 20:55:21.059907: step 40000, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 74h:39m:42s remains)
2017-12-05 20:55:21.919539: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2050037 -4.2121844 -4.1850047 -4.1210608 -4.0508194 -4.043047 -4.1032386 -4.1700668 -4.2022581 -4.2069726 -4.19802 -4.1917033 -4.1891885 -4.1850843 -4.1765194][-4.1814613 -4.2003508 -4.1945257 -4.1502109 -4.0786605 -4.0309491 -4.0692363 -4.1408658 -4.1872654 -4.2019153 -4.1933928 -4.1786971 -4.1720629 -4.1725025 -4.1752782][-4.1679425 -4.19215 -4.2062569 -4.1869216 -4.1269569 -4.0510068 -4.0472527 -4.1112442 -4.1692672 -4.1965275 -4.1934962 -4.175643 -4.1655331 -4.168942 -4.1818404][-4.1733842 -4.1944423 -4.2185478 -4.2173433 -4.1690793 -4.08243 -4.0274034 -4.0684357 -4.1396265 -4.1869607 -4.1981163 -4.18588 -4.1710482 -4.1731191 -4.1867747][-4.1903481 -4.2038174 -4.2252636 -4.2305155 -4.1914167 -4.1042457 -4.0082293 -4.0062847 -4.0886445 -4.1660748 -4.2022696 -4.2031612 -4.1894169 -4.1854086 -4.1920795][-4.2072906 -4.2152414 -4.2286921 -4.2297826 -4.1966982 -4.1145391 -3.9973249 -3.9387419 -4.0140648 -4.1252909 -4.1982026 -4.2206531 -4.2155561 -4.20515 -4.2036147][-4.2132893 -4.2161894 -4.2246628 -4.2224021 -4.1952376 -4.1273308 -4.0102844 -3.9069493 -3.9419451 -4.0698404 -4.1781454 -4.2291245 -4.240211 -4.2319441 -4.2245946][-4.2112632 -4.2090149 -4.212656 -4.2119241 -4.1960845 -4.1539721 -4.0648589 -3.9577112 -3.9298408 -4.0289536 -4.1467319 -4.2223864 -4.253376 -4.2543297 -4.2451191][-4.2107387 -4.2063489 -4.2019491 -4.2008419 -4.19908 -4.186193 -4.1392403 -4.0569425 -3.9878523 -4.018374 -4.1131845 -4.1976662 -4.2464175 -4.2621303 -4.25713][-4.2145886 -4.2105036 -4.1981363 -4.1915188 -4.1961532 -4.2043376 -4.195313 -4.1460047 -4.0718231 -4.0371675 -4.08537 -4.1621709 -4.2222137 -4.2537875 -4.2574997][-4.2221785 -4.2215252 -4.2063775 -4.1894817 -4.190537 -4.2091007 -4.225174 -4.204071 -4.1451216 -4.0788059 -4.0729303 -4.1287169 -4.1901984 -4.2314167 -4.245636][-4.2299776 -4.234025 -4.2212925 -4.2022591 -4.1963658 -4.2147446 -4.2394109 -4.2333665 -4.1921077 -4.1235337 -4.0756688 -4.096848 -4.1510496 -4.1978106 -4.221467][-4.2311192 -4.2392111 -4.23137 -4.2189803 -4.2099385 -4.2208366 -4.2441931 -4.243576 -4.2121286 -4.1517344 -4.0881662 -4.0736 -4.1107774 -4.1599536 -4.190619][-4.2194524 -4.2338152 -4.23116 -4.225615 -4.2170362 -4.2193694 -4.232676 -4.2314882 -4.2078609 -4.1631417 -4.1051855 -4.0689182 -4.0864162 -4.1345191 -4.1693091][-4.2030449 -4.2227073 -4.2255259 -4.2217531 -4.2148709 -4.2111268 -4.2117467 -4.2072926 -4.1894569 -4.1625504 -4.1241894 -4.0916753 -4.0926461 -4.1309047 -4.1668582]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 20:55:31.959296: step 40010, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 72h:17m:46s remains)
INFO - root - 2017-12-05 20:55:41.170863: step 40020, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 76h:31m:21s remains)
INFO - root - 2017-12-05 20:55:50.085536: step 40030, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.901 sec/batch; 73h:11m:42s remains)
INFO - root - 2017-12-05 20:55:59.195308: step 40040, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 75h:45m:00s remains)
INFO - root - 2017-12-05 20:56:08.329228: step 40050, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 76h:31m:22s remains)
INFO - root - 2017-12-05 20:56:17.367554: step 40060, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 75h:03m:04s remains)
INFO - root - 2017-12-05 20:56:26.386270: step 40070, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 71h:31m:59s remains)
INFO - root - 2017-12-05 20:56:35.416571: step 40080, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:35m:05s remains)
INFO - root - 2017-12-05 20:56:44.359838: step 40090, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 71h:58m:49s remains)
INFO - root - 2017-12-05 20:56:53.307661: step 40100, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 72h:53m:56s remains)
2017-12-05 20:56:54.253646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2863526 -4.264102 -4.2535572 -4.256752 -4.2531676 -4.2389283 -4.2244091 -4.2220464 -4.2152739 -4.2038 -4.1867709 -4.1690712 -4.1759343 -4.2034469 -4.2345691][-4.2854304 -4.2725296 -4.272089 -4.2861371 -4.2895041 -4.2762351 -4.2570205 -4.2455497 -4.229733 -4.2095752 -4.182272 -4.1507645 -4.1441851 -4.1640582 -4.1933479][-4.2664518 -4.2611179 -4.2678056 -4.2892442 -4.2991724 -4.2916102 -4.27707 -4.2675939 -4.2527494 -4.233263 -4.2059536 -4.1711369 -4.1544943 -4.1639771 -4.1864753][-4.2372856 -4.2333307 -4.2393851 -4.2591705 -4.2678924 -4.2648492 -4.25952 -4.2622004 -4.2595716 -4.2523613 -4.2361722 -4.2115254 -4.19533 -4.1977048 -4.210556][-4.198627 -4.192481 -4.1946836 -4.20774 -4.2112584 -4.2077579 -4.2097335 -4.225656 -4.2385159 -4.2465482 -4.2447505 -4.2336369 -4.2223959 -4.2184796 -4.2252364][-4.15744 -4.1453619 -4.1449475 -4.1517406 -4.1476665 -4.1383762 -4.1424251 -4.1696954 -4.1962433 -4.2162719 -4.2218871 -4.2182837 -4.2080488 -4.1971812 -4.2001004][-4.1247582 -4.1052833 -4.1012936 -4.0997143 -4.0855784 -4.0618687 -4.0554676 -4.0838985 -4.1201582 -4.1537671 -4.1680632 -4.1720138 -4.1645775 -4.1516833 -4.1544509][-4.1125374 -4.0864038 -4.0757966 -4.0638151 -4.0366874 -3.9910839 -3.9638748 -3.9834557 -4.0278597 -4.0778151 -4.1059947 -4.122746 -4.1209316 -4.1104884 -4.1139627][-4.1375122 -4.1131134 -4.1002526 -4.0821447 -4.0459595 -3.9804466 -3.9278998 -3.9308727 -3.9744267 -4.037065 -4.0810995 -4.1120391 -4.1168647 -4.1113944 -4.1145897][-4.1949449 -4.1809988 -4.1704683 -4.1526694 -4.1144595 -4.0453587 -3.9845083 -3.9763904 -4.0138211 -4.0750837 -4.1266241 -4.1668544 -4.177628 -4.1775351 -4.1803842][-4.2467561 -4.2407942 -4.2342429 -4.2236953 -4.1945934 -4.1386967 -4.0870223 -4.0727253 -4.1014547 -4.1523323 -4.2008781 -4.2433262 -4.2567787 -4.2594752 -4.2625413][-4.2581334 -4.256619 -4.256711 -4.2567329 -4.2415371 -4.2053561 -4.1709518 -4.1572394 -4.1778183 -4.2171516 -4.2581334 -4.29327 -4.303894 -4.3045297 -4.3064685][-4.2360272 -4.2357869 -4.2384324 -4.2460341 -4.2440157 -4.229341 -4.2147455 -4.2091269 -4.2270203 -4.2587256 -4.2891912 -4.3120151 -4.3157973 -4.3143845 -4.3171339][-4.2062926 -4.2034 -4.2026134 -4.2098322 -4.2173362 -4.2216849 -4.2251816 -4.2321529 -4.2537637 -4.2826757 -4.3068261 -4.3198171 -4.3187647 -4.31663 -4.319952][-4.2026834 -4.1949382 -4.1894665 -4.1943135 -4.2073178 -4.2221389 -4.2386003 -4.2553945 -4.2785239 -4.3034811 -4.3217278 -4.3285747 -4.324811 -4.3214073 -4.3234982]]...]
INFO - root - 2017-12-05 20:57:03.431065: step 40110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 73h:56m:13s remains)
INFO - root - 2017-12-05 20:57:12.580874: step 40120, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 74h:35m:12s remains)
INFO - root - 2017-12-05 20:57:21.654589: step 40130, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 70h:43m:07s remains)
INFO - root - 2017-12-05 20:57:30.650315: step 40140, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 76h:04m:03s remains)
INFO - root - 2017-12-05 20:57:39.789970: step 40150, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 73h:19m:11s remains)
INFO - root - 2017-12-05 20:57:48.838102: step 40160, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 72h:30m:59s remains)
INFO - root - 2017-12-05 20:57:57.834529: step 40170, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 72h:22m:18s remains)
INFO - root - 2017-12-05 20:58:06.780554: step 40180, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 72h:22m:26s remains)
INFO - root - 2017-12-05 20:58:15.825925: step 40190, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 73h:25m:56s remains)
INFO - root - 2017-12-05 20:58:24.701091: step 40200, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 71h:59m:23s remains)
2017-12-05 20:58:25.487917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3027558 -4.2881446 -4.2698169 -4.2537379 -4.2444119 -4.2471228 -4.247179 -4.2429638 -4.2486887 -4.2672925 -4.2806625 -4.2841144 -4.2859125 -4.283442 -4.2799163][-4.3022604 -4.2898221 -4.2737274 -4.2559977 -4.2426262 -4.2408361 -4.2369041 -4.2327704 -4.2426491 -4.2655239 -4.2779 -4.2762585 -4.2730188 -4.2679162 -4.263906][-4.302475 -4.2927055 -4.2783871 -4.2587342 -4.2397928 -4.2308397 -4.2236834 -4.2232471 -4.2391582 -4.265327 -4.2757716 -4.2694492 -4.2620778 -4.2557292 -4.2533569][-4.3027139 -4.295753 -4.2834921 -4.2632823 -4.2402158 -4.2243085 -4.2114339 -4.2112436 -4.2339344 -4.2647653 -4.2795849 -4.273747 -4.2636447 -4.2552319 -4.2528749][-4.3031812 -4.3004465 -4.2926273 -4.2729683 -4.2461004 -4.2205396 -4.1931376 -4.1833868 -4.2091389 -4.2493067 -4.2743154 -4.2754884 -4.2665453 -4.2559981 -4.2520895][-4.3050051 -4.30588 -4.3011422 -4.2794652 -4.2466745 -4.2091193 -4.1638737 -4.1379433 -4.1650772 -4.2177882 -4.2541761 -4.263341 -4.2585726 -4.2476182 -4.239532][-4.3071833 -4.3085465 -4.3030305 -4.2767835 -4.2391081 -4.1939154 -4.1352873 -4.0941296 -4.1206245 -4.18457 -4.2297397 -4.244422 -4.242825 -4.2343025 -4.2247486][-4.3096614 -4.309422 -4.3003445 -4.2696338 -4.2278142 -4.1782951 -4.1118889 -4.0616636 -4.0859303 -4.1551919 -4.2071857 -4.2286921 -4.231916 -4.227169 -4.2189784][-4.3117805 -4.30914 -4.2969928 -4.2662511 -4.225841 -4.1768527 -4.1094275 -4.0534444 -4.0723157 -4.1395855 -4.1952629 -4.2248325 -4.23255 -4.2245979 -4.2114367][-4.3136091 -4.3086729 -4.2956829 -4.267519 -4.2333107 -4.1922207 -4.1310644 -4.0740442 -4.0801082 -4.1359849 -4.1876941 -4.2186823 -4.2282767 -4.2185187 -4.1978903][-4.3143229 -4.3078494 -4.2952127 -4.2717228 -4.2436962 -4.2102523 -4.1600938 -4.1106634 -4.1044812 -4.1411076 -4.1796751 -4.205615 -4.2138672 -4.206099 -4.1843395][-4.3144732 -4.3071051 -4.2958283 -4.277235 -4.2527432 -4.223731 -4.1862521 -4.1493607 -4.1396637 -4.1594338 -4.184371 -4.2009315 -4.2046003 -4.1994519 -4.1800847][-4.3142352 -4.3074269 -4.2990165 -4.2846217 -4.2616243 -4.236383 -4.2106662 -4.1870155 -4.17583 -4.182229 -4.1965661 -4.2025042 -4.2001996 -4.1950788 -4.1792417][-4.3141241 -4.3099666 -4.3058128 -4.2963228 -4.2759085 -4.2547355 -4.2370157 -4.2189879 -4.2017751 -4.1969438 -4.2032475 -4.1993251 -4.1881318 -4.1803021 -4.1680331][-4.3146844 -4.3130741 -4.3122473 -4.3065505 -4.2875605 -4.2658553 -4.2507696 -4.2337689 -4.2121696 -4.2036362 -4.2040863 -4.1901641 -4.1675735 -4.1552448 -4.14094]]...]
INFO - root - 2017-12-05 20:58:34.583123: step 40210, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.916 sec/batch; 74h:22m:29s remains)
INFO - root - 2017-12-05 20:58:43.809803: step 40220, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 73h:07m:26s remains)
INFO - root - 2017-12-05 20:58:52.962926: step 40230, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 73h:03m:40s remains)
INFO - root - 2017-12-05 20:59:01.819979: step 40240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 74h:46m:27s remains)
INFO - root - 2017-12-05 20:59:10.941129: step 40250, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 75h:39m:10s remains)
INFO - root - 2017-12-05 20:59:20.250849: step 40260, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.945 sec/batch; 76h:41m:02s remains)
INFO - root - 2017-12-05 20:59:29.389016: step 40270, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 73h:34m:56s remains)
INFO - root - 2017-12-05 20:59:38.396765: step 40280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 74h:24m:41s remains)
INFO - root - 2017-12-05 20:59:47.328059: step 40290, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 73h:50m:49s remains)
INFO - root - 2017-12-05 20:59:56.338502: step 40300, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 72h:14m:55s remains)
2017-12-05 20:59:57.206430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2397203 -4.242733 -4.2332492 -4.2185907 -4.2104573 -4.2027264 -4.1899943 -4.1705389 -4.1577239 -4.1663308 -4.1904583 -4.2041311 -4.2006822 -4.1932106 -4.2043042][-4.2271614 -4.2312274 -4.2200747 -4.2035193 -4.1934767 -4.17835 -4.1519728 -4.118515 -4.0987105 -4.1154208 -4.1546946 -4.1782923 -4.1770058 -4.1667824 -4.1784463][-4.2200394 -4.2225871 -4.2082753 -4.189465 -4.1747031 -4.1493559 -4.1079464 -4.0600681 -4.03603 -4.0630608 -4.1206689 -4.1563673 -4.1613717 -4.1506438 -4.160471][-4.212954 -4.2096715 -4.1900005 -4.16664 -4.1408038 -4.0987496 -4.0383873 -3.9816818 -3.9615967 -4.0069957 -4.0851269 -4.1332083 -4.1457171 -4.1360774 -4.1441479][-4.2093005 -4.1996531 -4.1711617 -4.1358323 -4.0906091 -4.0259643 -3.943162 -3.8754129 -3.8695509 -3.94708 -4.0502739 -4.11349 -4.1325197 -4.1246004 -4.1323152][-4.2116108 -4.19598 -4.1576633 -4.1087437 -4.0456562 -3.9584296 -3.8487728 -3.7692978 -3.7767706 -3.8885617 -4.0238028 -4.1069083 -4.1340728 -4.1280751 -4.1360235][-4.2218866 -4.1999984 -4.1512866 -4.0916672 -4.0173635 -3.9149079 -3.7874465 -3.7112374 -3.7360878 -3.8730421 -4.0308433 -4.12428 -4.1539879 -4.1473641 -4.1558208][-4.2330461 -4.2028055 -4.148983 -4.0890274 -4.0152082 -3.9072483 -3.7738175 -3.7245736 -3.7843502 -3.9350214 -4.08631 -4.1614628 -4.1818104 -4.1724954 -4.1813507][-4.2359939 -4.2031417 -4.1527705 -4.105226 -4.0440655 -3.9451094 -3.8211021 -3.8060005 -3.9032197 -4.0404649 -4.1505971 -4.1898904 -4.1931429 -4.1856561 -4.1980519][-4.2350192 -4.2050886 -4.1651735 -4.1333361 -4.0898538 -4.0116959 -3.9172993 -3.9296958 -4.0292873 -4.1292825 -4.1890578 -4.1979141 -4.1905951 -4.188911 -4.2074375][-4.2434645 -4.218636 -4.1904774 -4.168324 -4.1366229 -4.0798488 -4.025032 -4.0516362 -4.1262822 -4.1820297 -4.2029853 -4.2016788 -4.1943045 -4.1985955 -4.222671][-4.260982 -4.2415676 -4.2222662 -4.2052097 -4.1768141 -4.136569 -4.1141834 -4.1480227 -4.195899 -4.2132754 -4.2110791 -4.2081971 -4.2067137 -4.216043 -4.2421861][-4.2762117 -4.2631135 -4.2500858 -4.2378778 -4.21581 -4.1910272 -4.1849828 -4.213985 -4.2356267 -4.2296014 -4.2174406 -4.2180114 -4.222899 -4.2379093 -4.2629652][-4.2879343 -4.281508 -4.2753825 -4.2696595 -4.2593131 -4.2461243 -4.2427931 -4.2551188 -4.2587423 -4.2441368 -4.2307539 -4.2366433 -4.24863 -4.2642388 -4.2845831][-4.3027172 -4.30167 -4.3011265 -4.3000035 -4.2947788 -4.2873034 -4.2831364 -4.2832136 -4.2767434 -4.2633629 -4.2571054 -4.2667065 -4.2796478 -4.2912621 -4.3048534]]...]
INFO - root - 2017-12-05 21:00:06.272765: step 40310, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 73h:30m:13s remains)
INFO - root - 2017-12-05 21:00:15.319734: step 40320, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 71h:43m:43s remains)
INFO - root - 2017-12-05 21:00:24.224535: step 40330, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 72h:59m:41s remains)
INFO - root - 2017-12-05 21:00:33.373698: step 40340, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 74h:01m:37s remains)
INFO - root - 2017-12-05 21:00:42.644384: step 40350, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 73h:23m:17s remains)
INFO - root - 2017-12-05 21:00:51.822041: step 40360, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 75h:34m:31s remains)
INFO - root - 2017-12-05 21:01:00.769891: step 40370, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 70h:47m:38s remains)
INFO - root - 2017-12-05 21:01:09.939596: step 40380, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 72h:49m:43s remains)
INFO - root - 2017-12-05 21:01:18.825842: step 40390, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 75h:25m:54s remains)
INFO - root - 2017-12-05 21:01:27.880686: step 40400, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 74h:55m:38s remains)
2017-12-05 21:01:28.733323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.280179 -4.2570243 -4.2430944 -4.2182479 -4.174336 -4.1576462 -4.1797228 -4.2093172 -4.2413554 -4.2577777 -4.261559 -4.2644639 -4.2669187 -4.268815 -4.2702231][-4.2764225 -4.2561173 -4.24516 -4.2325845 -4.2031841 -4.1863837 -4.1971989 -4.2171636 -4.245718 -4.2614408 -4.2680454 -4.2721415 -4.2753329 -4.2787075 -4.281559][-4.2687607 -4.247087 -4.2331548 -4.2262626 -4.2066212 -4.1887364 -4.1905918 -4.2057533 -4.2342062 -4.2532396 -4.2666936 -4.2740574 -4.2780023 -4.2819376 -4.2859478][-4.2707295 -4.2470407 -4.2264385 -4.2130051 -4.1897564 -4.1660781 -4.1602311 -4.1751666 -4.2062917 -4.2307734 -4.2503457 -4.26162 -4.2690916 -4.2770815 -4.2843509][-4.2815228 -4.2538223 -4.2220869 -4.1897917 -4.1520247 -4.121223 -4.1102767 -4.1291375 -4.1649261 -4.1925669 -4.216784 -4.2355709 -4.2520003 -4.2702169 -4.2863803][-4.2897048 -4.2555413 -4.2108731 -4.1580005 -4.1029067 -4.0719194 -4.0659881 -4.0862904 -4.1167555 -4.1359992 -4.1584344 -4.1864047 -4.2184086 -4.2528172 -4.2819209][-4.2926388 -4.251873 -4.1954651 -4.1269922 -4.0614014 -4.0368419 -4.0383439 -4.0552068 -4.0695682 -4.0679221 -4.0811191 -4.1222935 -4.1768317 -4.2314615 -4.2749281][-4.29308 -4.2494192 -4.1881986 -4.1119919 -4.0411773 -4.0228491 -4.0290909 -4.0378361 -4.0351715 -4.0082312 -4.004931 -4.0593023 -4.1375179 -4.2089171 -4.2642784][-4.290637 -4.2473731 -4.1866722 -4.1098461 -4.035984 -4.0208626 -4.0321817 -4.0356236 -4.023304 -3.9812126 -3.9651377 -4.0267668 -4.11764 -4.1963015 -4.2563486][-4.2875085 -4.2470269 -4.1928291 -4.1239643 -4.0540557 -4.0380158 -4.0463657 -4.0474577 -4.0350513 -3.9970171 -3.9836519 -4.0471411 -4.1369281 -4.2090855 -4.2615027][-4.2865543 -4.2525945 -4.2086477 -4.1537442 -4.0967526 -4.0785141 -4.0821652 -4.0889359 -4.0840425 -4.0612068 -4.0593686 -4.1125236 -4.1818933 -4.2352772 -4.2734985][-4.2854886 -4.2584977 -4.2246966 -4.1842146 -4.1429825 -4.1265092 -4.1321788 -4.1491127 -4.1576128 -4.1490889 -4.1516671 -4.1848927 -4.223825 -4.255487 -4.2802148][-4.2796178 -4.2563634 -4.2308311 -4.2055531 -4.1803379 -4.1709714 -4.1805139 -4.20152 -4.2192268 -4.221911 -4.225471 -4.2424045 -4.26004 -4.2766423 -4.2924848][-4.2783923 -4.2559137 -4.2350397 -4.2209978 -4.2100573 -4.2112327 -4.2243409 -4.2445126 -4.2640858 -4.2730708 -4.2768693 -4.2844253 -4.29016 -4.2982364 -4.3079481][-4.2854724 -4.26238 -4.2426639 -4.2317905 -4.2292118 -4.2411532 -4.2589622 -4.2770829 -4.2944183 -4.3041329 -4.3068886 -4.3092837 -4.3108191 -4.3133268 -4.3172059]]...]
INFO - root - 2017-12-05 21:01:37.861386: step 40410, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 73h:56m:58s remains)
INFO - root - 2017-12-05 21:01:46.641265: step 40420, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 74h:15m:15s remains)
INFO - root - 2017-12-05 21:01:55.803803: step 40430, loss = 2.08, batch loss = 2.03 (8.3 examples/sec; 0.966 sec/batch; 78h:21m:36s remains)
INFO - root - 2017-12-05 21:02:04.914900: step 40440, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 74h:26m:21s remains)
INFO - root - 2017-12-05 21:02:13.904290: step 40450, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 72h:15m:58s remains)
INFO - root - 2017-12-05 21:02:22.964559: step 40460, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 73h:40m:19s remains)
INFO - root - 2017-12-05 21:02:32.130913: step 40470, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 75h:45m:13s remains)
INFO - root - 2017-12-05 21:02:41.114654: step 40480, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 71h:30m:40s remains)
INFO - root - 2017-12-05 21:02:50.077352: step 40490, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 73h:39m:12s remains)
INFO - root - 2017-12-05 21:02:59.127158: step 40500, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 73h:10m:54s remains)
2017-12-05 21:02:59.908417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9154363 -3.9362953 -3.9916503 -4.0548725 -4.1223497 -4.199789 -4.2523928 -4.2902932 -4.3071027 -4.3162169 -4.3178329 -4.300221 -4.2786007 -4.273015 -4.2700424][-4.0180397 -4.0255971 -4.0653324 -4.1180243 -4.1784024 -4.2454357 -4.29025 -4.31451 -4.3184509 -4.3201113 -4.3227425 -4.3167877 -4.3109088 -4.3159509 -4.3164473][-4.1227403 -4.1172848 -4.1402683 -4.1797957 -4.228241 -4.2759967 -4.3045959 -4.3120017 -4.3080688 -4.3054233 -4.3080316 -4.311028 -4.320756 -4.333097 -4.33553][-4.2142248 -4.2043858 -4.21609 -4.2422752 -4.2726021 -4.2916379 -4.2961431 -4.2830491 -4.2707205 -4.2668929 -4.2711926 -4.2811522 -4.3024554 -4.3207827 -4.326014][-4.2733378 -4.2667971 -4.2727027 -4.2855473 -4.289535 -4.2745442 -4.2469573 -4.2106681 -4.1905727 -4.193953 -4.2056174 -4.2277465 -4.25982 -4.2877908 -4.3003283][-4.2959256 -4.2879114 -4.2838812 -4.2771587 -4.2501912 -4.1993761 -4.1396189 -4.0861745 -4.0694647 -4.0959535 -4.1279335 -4.168766 -4.2117305 -4.2494073 -4.2682409][-4.2882185 -4.2696862 -4.2482176 -4.2157717 -4.1599255 -4.0764918 -3.9855497 -3.9182103 -3.9201274 -3.9789793 -4.0342097 -4.0941973 -4.1484957 -4.19416 -4.2186875][-4.2610555 -4.2269459 -4.180717 -4.1195641 -4.0413651 -3.9400122 -3.8344691 -3.7773132 -3.8178482 -3.9019012 -3.9681494 -4.0359077 -4.0972533 -4.1471353 -4.1748595][-4.2387891 -4.1931667 -4.1265416 -4.0475054 -3.9631932 -3.8682582 -3.7834122 -3.768517 -3.8376358 -3.9203162 -3.9820616 -4.0471058 -4.1050434 -4.1516056 -4.1787825][-4.2403269 -4.1937919 -4.1289725 -4.0604959 -3.9987433 -3.9407182 -3.8997347 -3.9119408 -3.9691267 -4.022543 -4.064395 -4.1147728 -4.1582632 -4.1913872 -4.2128868][-4.2567873 -4.2182083 -4.1722016 -4.1321907 -4.1034641 -4.0824003 -4.0723524 -4.0875721 -4.120151 -4.1456161 -4.1716285 -4.2057471 -4.2316723 -4.2499909 -4.2629642][-4.2866921 -4.26143 -4.2340689 -4.2134061 -4.2021356 -4.1985354 -4.2018681 -4.21495 -4.2296824 -4.241509 -4.258637 -4.2799368 -4.293469 -4.3042474 -4.3126011][-4.3113446 -4.2992797 -4.2867055 -4.277998 -4.2753735 -4.2778049 -4.2832341 -4.2910991 -4.2972159 -4.3023968 -4.3128815 -4.32532 -4.3334937 -4.3402877 -4.34479][-4.3208427 -4.3159122 -4.3116503 -4.309442 -4.3104343 -4.3143916 -4.3198071 -4.32602 -4.328927 -4.3302426 -4.3338413 -4.3388872 -4.3418326 -4.3431988 -4.3430257][-4.3193989 -4.317018 -4.3148885 -4.3135214 -4.3139505 -4.3165922 -4.3202724 -4.3246684 -4.3271379 -4.3273072 -4.3267326 -4.326726 -4.3268666 -4.3268166 -4.3254147]]...]
INFO - root - 2017-12-05 21:03:09.103318: step 40510, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 72h:50m:46s remains)
INFO - root - 2017-12-05 21:03:18.140296: step 40520, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 72h:19m:13s remains)
INFO - root - 2017-12-05 21:03:27.136551: step 40530, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.930 sec/batch; 75h:24m:48s remains)
INFO - root - 2017-12-05 21:03:36.386010: step 40540, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 77h:29m:53s remains)
INFO - root - 2017-12-05 21:03:45.572162: step 40550, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 74h:56m:38s remains)
INFO - root - 2017-12-05 21:03:54.353394: step 40560, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 74h:02m:33s remains)
INFO - root - 2017-12-05 21:04:03.455547: step 40570, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 75h:20m:13s remains)
INFO - root - 2017-12-05 21:04:12.683808: step 40580, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 74h:35m:00s remains)
INFO - root - 2017-12-05 21:04:21.827367: step 40590, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 74h:55m:37s remains)
INFO - root - 2017-12-05 21:04:31.017431: step 40600, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 75h:18m:35s remains)
2017-12-05 21:04:31.840085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3103466 -4.3088527 -4.3017168 -4.294847 -4.2927547 -4.2939043 -4.2982068 -4.3073363 -4.3150716 -4.3119955 -4.2989607 -4.2813468 -4.2622929 -4.2408738 -4.2276545][-4.306942 -4.3012848 -4.2897792 -4.2801762 -4.2788939 -4.2823396 -4.2879305 -4.2983017 -4.3074908 -4.3066397 -4.2952356 -4.2767239 -4.2540145 -4.2318516 -4.224092][-4.3004346 -4.2901473 -4.2743154 -4.2607393 -4.2565589 -4.2575178 -4.2602534 -4.269115 -4.2804985 -4.2872157 -4.2867832 -4.2791243 -4.2646685 -4.2496338 -4.2454944][-4.2911882 -4.2777452 -4.259089 -4.2417965 -4.2301917 -4.2185092 -4.2085423 -4.210041 -4.224206 -4.244328 -4.2646184 -4.2791305 -4.2831345 -4.2789793 -4.27569][-4.2838006 -4.2711196 -4.2533441 -4.2333651 -4.2102985 -4.1769471 -4.143981 -4.1299853 -4.141417 -4.17633 -4.22129 -4.2622333 -4.28778 -4.295435 -4.29253][-4.2771955 -4.2671566 -4.2506108 -4.2279439 -4.1949763 -4.1401806 -4.0788097 -4.0424647 -4.0489569 -4.0978909 -4.1664181 -4.2304425 -4.2755246 -4.2928782 -4.2900248][-4.2707677 -4.2631683 -4.2457209 -4.2222667 -4.1865196 -4.1184492 -4.0329728 -3.9730117 -3.9718344 -4.0294223 -4.1119437 -4.1902061 -4.2500529 -4.2756195 -4.2734547][-4.2617564 -4.2515516 -4.2310634 -4.2100506 -4.1822205 -4.1147285 -4.0171051 -3.9400513 -3.9322679 -3.991003 -4.0739946 -4.1560922 -4.223732 -4.2546735 -4.2527008][-4.2621641 -4.2501783 -4.2297111 -4.2131419 -4.195035 -4.1370325 -4.0473862 -3.9726472 -3.9630048 -4.0129805 -4.081418 -4.151814 -4.2153692 -4.2451196 -4.2424469][-4.2731943 -4.2695022 -4.2579141 -4.2501688 -4.2396336 -4.1947951 -4.1238589 -4.0630445 -4.048255 -4.0811334 -4.128334 -4.1790395 -4.2306495 -4.256794 -4.256793][-4.2801547 -4.28838 -4.2881331 -4.2895794 -4.2845926 -4.2493811 -4.1921692 -4.1386456 -4.1177773 -4.1349492 -4.1667032 -4.2042694 -4.2483826 -4.2759171 -4.2833586][-4.2742357 -4.2883167 -4.2936416 -4.3004608 -4.299139 -4.26913 -4.2159786 -4.1620984 -4.1362114 -4.1460762 -4.1734948 -4.2088971 -4.2508378 -4.2816949 -4.2962079][-4.2562804 -4.2700105 -4.2751274 -4.2824159 -4.2839689 -4.2595778 -4.211153 -4.1612282 -4.1368542 -4.1490889 -4.1790423 -4.2139788 -4.2518358 -4.2784147 -4.2914491][-4.2521677 -4.2578616 -4.2564459 -4.2607603 -4.2663054 -4.2521915 -4.2222686 -4.1947093 -4.1855578 -4.1998382 -4.2225986 -4.2437687 -4.2623615 -4.2722692 -4.2741795][-4.2713637 -4.2660456 -4.256629 -4.2570782 -4.2650232 -4.262908 -4.2561641 -4.2550569 -4.2598858 -4.2693005 -4.276268 -4.2759309 -4.2700872 -4.2609119 -4.2516]]...]
INFO - root - 2017-12-05 21:04:40.857047: step 40610, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 73h:01m:21s remains)
INFO - root - 2017-12-05 21:04:49.943142: step 40620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 74h:12m:18s remains)
INFO - root - 2017-12-05 21:04:59.310352: step 40630, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.924 sec/batch; 74h:54m:59s remains)
INFO - root - 2017-12-05 21:05:08.572447: step 40640, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 74h:22m:28s remains)
INFO - root - 2017-12-05 21:05:17.773363: step 40650, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 68h:06m:17s remains)
INFO - root - 2017-12-05 21:05:26.721815: step 40660, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 72h:50m:05s remains)
INFO - root - 2017-12-05 21:05:35.907028: step 40670, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 71h:37m:54s remains)
INFO - root - 2017-12-05 21:05:45.001066: step 40680, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 76h:00m:03s remains)
INFO - root - 2017-12-05 21:05:54.050355: step 40690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 71h:25m:31s remains)
INFO - root - 2017-12-05 21:06:02.900057: step 40700, loss = 2.05, batch loss = 2.00 (10.0 examples/sec; 0.798 sec/batch; 64h:38m:49s remains)
2017-12-05 21:06:03.684053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1311526 -4.1369963 -4.1569772 -4.1782279 -4.1941047 -4.1931357 -4.1700087 -4.1377473 -4.1212611 -4.131731 -4.1699233 -4.2116423 -4.253468 -4.2839394 -4.2991056][-4.1309175 -4.1321597 -4.1484003 -4.1684933 -4.1841197 -4.1824718 -4.1585093 -4.1275349 -4.1154342 -4.1308594 -4.1736007 -4.216289 -4.2601161 -4.2945518 -4.3113723][-4.1491523 -4.1457767 -4.1572266 -4.1719594 -4.1785192 -4.1701193 -4.1450973 -4.1189833 -4.115078 -4.138968 -4.1839881 -4.22408 -4.2638116 -4.2972903 -4.3162832][-4.1782079 -4.1736679 -4.1760583 -4.1749406 -4.1626778 -4.1432948 -4.1174641 -4.0965843 -4.1015229 -4.136209 -4.1854038 -4.2260017 -4.2614126 -4.2931437 -4.31609][-4.21432 -4.2064428 -4.1932454 -4.1696014 -4.1368256 -4.1066737 -4.0784483 -4.0586667 -4.0730739 -4.1220255 -4.1795368 -4.2239823 -4.2579422 -4.2905278 -4.3173466][-4.2441435 -4.230979 -4.2044039 -4.1639652 -4.1168809 -4.0749569 -4.0370674 -4.012589 -4.0386992 -4.1065965 -4.1760273 -4.2251253 -4.2585549 -4.2923784 -4.3200045][-4.2506275 -4.2340822 -4.2006454 -4.1533179 -4.0994449 -4.047956 -3.9970326 -3.9679379 -4.0094218 -4.0972419 -4.1789804 -4.233099 -4.2662554 -4.2983184 -4.3231564][-4.2422194 -4.2265825 -4.1963134 -4.1527762 -4.0985947 -4.0398674 -3.9753356 -3.9419167 -3.9978228 -4.099504 -4.1883774 -4.2454181 -4.2804461 -4.3098311 -4.3293867][-4.2233963 -4.2108283 -4.1875744 -4.1555047 -4.111084 -4.0581846 -3.9946439 -3.9606829 -4.0151091 -4.1121559 -4.1994929 -4.2586179 -4.2956805 -4.3211994 -4.334816][-4.1986403 -4.1868362 -4.1700859 -4.151762 -4.1258883 -4.0885487 -4.0371141 -4.0065613 -4.0460887 -4.1259 -4.20678 -4.2652364 -4.3015647 -4.3242278 -4.3361568][-4.1806569 -4.1685019 -4.1573057 -4.1509271 -4.1421223 -4.1202378 -4.0809846 -4.0538335 -4.0786729 -4.1417422 -4.213048 -4.268105 -4.30287 -4.3247943 -4.3376284][-4.1673632 -4.1542535 -4.1441016 -4.1443191 -4.1473632 -4.1388216 -4.1122255 -4.0905838 -4.1069021 -4.1572461 -4.2180882 -4.2681646 -4.3022895 -4.325 -4.3395081][-4.156785 -4.1438489 -4.1350327 -4.1380582 -4.1464767 -4.1456218 -4.1272821 -4.1101871 -4.121459 -4.1643972 -4.2170744 -4.2626615 -4.2969985 -4.3209705 -4.3375711][-4.1455126 -4.1399369 -4.1354604 -4.1388497 -4.1471434 -4.1459146 -4.1286411 -4.1127882 -4.1219382 -4.1622763 -4.2112522 -4.2535915 -4.2872944 -4.3138695 -4.3339763][-4.1418719 -4.1460695 -4.147943 -4.1513162 -4.1543884 -4.14788 -4.1283178 -4.1124988 -4.1203547 -4.1591325 -4.2066946 -4.2469959 -4.2798061 -4.3084078 -4.332283]]...]
INFO - root - 2017-12-05 21:06:12.684082: step 40710, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 73h:46m:09s remains)
INFO - root - 2017-12-05 21:06:21.880022: step 40720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 74h:19m:34s remains)
INFO - root - 2017-12-05 21:06:31.008264: step 40730, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 72h:12m:17s remains)
INFO - root - 2017-12-05 21:06:40.034146: step 40740, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 75h:31m:33s remains)
INFO - root - 2017-12-05 21:06:49.135840: step 40750, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 73h:09m:37s remains)
INFO - root - 2017-12-05 21:06:58.455345: step 40760, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.956 sec/batch; 77h:28m:42s remains)
INFO - root - 2017-12-05 21:07:07.491562: step 40770, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 75h:24m:58s remains)
INFO - root - 2017-12-05 21:07:16.720237: step 40780, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 72h:01m:08s remains)
INFO - root - 2017-12-05 21:07:25.864691: step 40790, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 73h:48m:45s remains)
INFO - root - 2017-12-05 21:07:34.694009: step 40800, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 66h:28m:29s remains)
2017-12-05 21:07:35.492119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3197155 -4.3071527 -4.2903113 -4.2782316 -4.2713404 -4.2686515 -4.26868 -4.2729392 -4.2841682 -4.2996264 -4.3124809 -4.3116159 -4.3021293 -4.2961369 -4.2988114][-4.3229332 -4.3071308 -4.2917223 -4.2842369 -4.278296 -4.2747288 -4.2765875 -4.2817168 -4.2923956 -4.3080416 -4.32365 -4.329977 -4.3279853 -4.325273 -4.3244605][-4.3098922 -4.2865043 -4.2695274 -4.2629228 -4.2609606 -4.2610455 -4.2690892 -4.2813797 -4.2936673 -4.3065963 -4.3218107 -4.333952 -4.3411369 -4.3447318 -4.3448987][-4.2860785 -4.2524228 -4.2275996 -4.21714 -4.2204847 -4.2286711 -4.2440205 -4.266921 -4.2824163 -4.2918391 -4.306426 -4.3216109 -4.3340287 -4.3429537 -4.3462405][-4.2592468 -4.2123132 -4.172399 -4.1564016 -4.1648388 -4.1797886 -4.1985273 -4.2247257 -4.243381 -4.2546053 -4.2710433 -4.2888908 -4.3060627 -4.3210926 -4.3300481][-4.2391372 -4.1802874 -4.1247597 -4.0982094 -4.1016555 -4.1110263 -4.1237755 -4.139576 -4.1520605 -4.1645947 -4.191257 -4.2250419 -4.2568388 -4.2871141 -4.3088093][-4.2327943 -4.1678472 -4.1009235 -4.0585785 -4.0448194 -4.0370331 -4.0314016 -4.0288782 -4.0330076 -4.0451064 -4.0845866 -4.1452212 -4.201808 -4.25248 -4.2901564][-4.2416224 -4.181767 -4.1160073 -4.0642085 -4.029233 -3.9981291 -3.9660773 -3.9420786 -3.9397826 -3.9529362 -4.0001845 -4.08166 -4.1571569 -4.2222233 -4.274158][-4.2654924 -4.2177844 -4.1644416 -4.1193347 -4.0832543 -4.0442438 -3.9902694 -3.9405308 -3.9207883 -3.92123 -3.9617412 -4.0471325 -4.12977 -4.2005653 -4.2599835][-4.2953525 -4.2640562 -4.2284641 -4.1998005 -4.1743588 -4.141263 -4.0869956 -4.03075 -3.9983459 -3.9802365 -3.9971621 -4.0625005 -4.1369314 -4.2064214 -4.2641048][-4.3247733 -4.3092632 -4.2919807 -4.2752919 -4.2553058 -4.2290559 -4.1882176 -4.1418061 -4.1091204 -4.0888486 -4.0918412 -4.1278162 -4.1821117 -4.2406316 -4.28884][-4.3454213 -4.3397303 -4.333519 -4.3263278 -4.3136268 -4.2940516 -4.2667952 -4.2363706 -4.2127781 -4.2003512 -4.1994696 -4.2146549 -4.2472067 -4.2870922 -4.3181005][-4.35169 -4.3492422 -4.3465471 -4.3440013 -4.3378725 -4.3252568 -4.3079915 -4.2891908 -4.273459 -4.2682476 -4.2720814 -4.2818465 -4.2996325 -4.3227649 -4.3401475][-4.3541274 -4.3527889 -4.3505349 -4.3493843 -4.345798 -4.335618 -4.3234835 -4.3130212 -4.3047338 -4.3036952 -4.3098879 -4.3185005 -4.328495 -4.3419123 -4.3512383][-4.3599916 -4.3594985 -4.3565454 -4.3552504 -4.3518262 -4.342761 -4.3338194 -4.3281093 -4.3227954 -4.3225417 -4.3293319 -4.3379126 -4.3459706 -4.3530173 -4.3545265]]...]
INFO - root - 2017-12-05 21:07:44.627103: step 40810, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 73h:34m:42s remains)
INFO - root - 2017-12-05 21:07:53.701366: step 40820, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 75h:11m:50s remains)
INFO - root - 2017-12-05 21:08:02.692202: step 40830, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 72h:59m:24s remains)
INFO - root - 2017-12-05 21:08:11.671566: step 40840, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 72h:15m:18s remains)
INFO - root - 2017-12-05 21:08:20.969853: step 40850, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 75h:04m:31s remains)
INFO - root - 2017-12-05 21:08:30.219708: step 40860, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 73h:34m:46s remains)
INFO - root - 2017-12-05 21:08:39.282693: step 40870, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 69h:43m:01s remains)
INFO - root - 2017-12-05 21:08:48.382886: step 40880, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 74h:43m:06s remains)
INFO - root - 2017-12-05 21:08:57.594589: step 40890, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 74h:24m:08s remains)
INFO - root - 2017-12-05 21:09:06.683112: step 40900, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 71h:26m:58s remains)
2017-12-05 21:09:07.464509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3194261 -4.317883 -4.3158736 -4.3086939 -4.2991052 -4.2926445 -4.2922292 -4.2905788 -4.281661 -4.2618871 -4.2428017 -4.23216 -4.2345929 -4.2542343 -4.2855411][-4.3133674 -4.31167 -4.307682 -4.2977123 -4.286097 -4.2798347 -4.2818966 -4.2786951 -4.2641125 -4.2414932 -4.2179227 -4.2047019 -4.2125244 -4.2416539 -4.2813363][-4.3078694 -4.3021564 -4.2952914 -4.2852335 -4.276804 -4.2756042 -4.277967 -4.2677054 -4.2480907 -4.2235851 -4.196259 -4.1800542 -4.1891994 -4.2258673 -4.2720337][-4.3056126 -4.294271 -4.2842717 -4.276865 -4.2713351 -4.2717466 -4.2654171 -4.2463393 -4.2248869 -4.1986361 -4.1707397 -4.1535378 -4.1650023 -4.2106686 -4.2623544][-4.3068533 -4.2943411 -4.2798595 -4.2682161 -4.2624164 -4.2543888 -4.2284813 -4.2046385 -4.1904 -4.172667 -4.1546965 -4.143889 -4.1599207 -4.2075534 -4.2628183][-4.3030448 -4.2873788 -4.2648993 -4.243258 -4.2282147 -4.2002783 -4.1508512 -4.1263514 -4.135107 -4.1385717 -4.1354156 -4.140101 -4.16343 -4.2055225 -4.2535658][-4.2875476 -4.2662306 -4.2351618 -4.2051468 -4.1745319 -4.1153913 -4.0318456 -4.0014858 -4.0520082 -4.0988359 -4.1254535 -4.1471572 -4.1732841 -4.2029996 -4.2328377][-4.2533078 -4.2240705 -4.1877842 -4.151546 -4.1056256 -4.0178881 -3.8889108 -3.8352671 -3.9313741 -4.0330715 -4.0937195 -4.1295786 -4.1552138 -4.172514 -4.1916709][-4.2225933 -4.1869841 -4.1475182 -4.1119266 -4.0697083 -3.9875643 -3.8532419 -3.7919259 -3.9029324 -4.020503 -4.0850248 -4.1193538 -4.1376033 -4.1446857 -4.1549191][-4.2128887 -4.1798639 -4.1461229 -4.1200447 -4.0992861 -4.0482988 -3.9552679 -3.9088106 -3.9885888 -4.0739832 -4.1168461 -4.1406918 -4.1501107 -4.1464767 -4.1475897][-4.2047119 -4.1767426 -4.1504889 -4.1354752 -4.1297107 -4.1072936 -4.0533552 -4.0216975 -4.0711222 -4.1242976 -4.1467028 -4.1608853 -4.1656656 -4.1576614 -4.1551819][-4.1923785 -4.1700239 -4.1534848 -4.1457372 -4.1479912 -4.1440597 -4.1155944 -4.0967321 -4.1285863 -4.1641674 -4.1779232 -4.1866021 -4.1890225 -4.1801624 -4.1723995][-4.1919165 -4.1678529 -4.1498208 -4.1408577 -4.143055 -4.1483612 -4.136579 -4.1302953 -4.1519403 -4.1778016 -4.1915011 -4.197638 -4.1986318 -4.1906576 -4.1812425][-4.221077 -4.1919336 -4.1676016 -4.1560335 -4.1585593 -4.1676912 -4.1689248 -4.1703267 -4.180737 -4.1958604 -4.2071152 -4.2117829 -4.2148809 -4.2131252 -4.20681][-4.2734427 -4.2467265 -4.2229905 -4.2134018 -4.2164721 -4.2243981 -4.2315521 -4.236196 -4.2394471 -4.2436929 -4.2505054 -4.2540016 -4.2598977 -4.2644138 -4.2622867]]...]
INFO - root - 2017-12-05 21:09:16.568673: step 40910, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 73h:57m:58s remains)
INFO - root - 2017-12-05 21:09:25.726638: step 40920, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 78h:13m:07s remains)
INFO - root - 2017-12-05 21:09:34.820669: step 40930, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 72h:42m:23s remains)
INFO - root - 2017-12-05 21:09:43.800541: step 40940, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 75h:14m:48s remains)
INFO - root - 2017-12-05 21:09:52.982631: step 40950, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 72h:34m:27s remains)
INFO - root - 2017-12-05 21:10:02.348729: step 40960, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.904 sec/batch; 73h:14m:07s remains)
INFO - root - 2017-12-05 21:10:11.533387: step 40970, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 76h:09m:01s remains)
INFO - root - 2017-12-05 21:10:20.597710: step 40980, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 74h:35m:07s remains)
INFO - root - 2017-12-05 21:10:29.791987: step 40990, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.936 sec/batch; 75h:45m:43s remains)
INFO - root - 2017-12-05 21:10:38.903035: step 41000, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.943 sec/batch; 76h:22m:10s remains)
2017-12-05 21:10:39.676237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2876792 -4.2581649 -4.225596 -4.1983037 -4.1702924 -4.1283965 -4.09077 -4.1098294 -4.166029 -4.205296 -4.22539 -4.2386565 -4.2468328 -4.2544513 -4.2589135][-4.2948861 -4.2698388 -4.2380257 -4.2095895 -4.1778331 -4.1306224 -4.0873618 -4.1050706 -4.1648917 -4.200552 -4.2127042 -4.2178364 -4.217689 -4.2182031 -4.2176819][-4.3104682 -4.2949576 -4.2718439 -4.2487574 -4.2209597 -4.1801763 -4.1421804 -4.1544361 -4.2050653 -4.228713 -4.2278376 -4.2200384 -4.2076097 -4.196559 -4.1868949][-4.3163748 -4.3056684 -4.2893658 -4.2716475 -4.250524 -4.2197113 -4.191 -4.201654 -4.2445178 -4.2594948 -4.2512035 -4.2375617 -4.219101 -4.2016759 -4.1881466][-4.30461 -4.2868428 -4.2676582 -4.2511311 -4.2307057 -4.2019572 -4.1730037 -4.187923 -4.2361507 -4.2581606 -4.2569971 -4.2498932 -4.2364521 -4.2232418 -4.2163286][-4.286552 -4.2546511 -4.2224026 -4.19825 -4.1685634 -4.125176 -4.0838656 -4.1076694 -4.1726394 -4.2096133 -4.2208176 -4.2232757 -4.2172642 -4.2142696 -4.2256618][-4.2768164 -4.2352653 -4.1911225 -4.1527009 -4.1019988 -4.0310392 -3.9712329 -4.00352 -4.08819 -4.1404157 -4.16176 -4.1681108 -4.1624503 -4.1665025 -4.19704][-4.27322 -4.2295752 -4.1807866 -4.1329961 -4.0715437 -3.9867616 -3.9158692 -3.9472558 -4.0334773 -4.0852714 -4.1062756 -4.1080937 -4.0935545 -4.099246 -4.1457124][-4.2751813 -4.2352152 -4.1880097 -4.1414309 -4.086957 -4.0162911 -3.9571908 -3.977386 -4.0428848 -4.0762482 -4.0858583 -4.0756369 -4.0459313 -4.0449286 -4.0952764][-4.2697334 -4.2318888 -4.192214 -4.1542573 -4.1124535 -4.0621052 -4.0193071 -4.0288267 -4.0708365 -4.086616 -4.0837526 -4.0600719 -4.0209241 -4.0146675 -4.060195][-4.2552228 -4.2167726 -4.1834803 -4.1516752 -4.1140513 -4.0676165 -4.0281496 -4.0355759 -4.0728421 -4.0859294 -4.0760031 -4.0473666 -4.0072536 -3.9964173 -4.0368061][-4.2410946 -4.2037344 -4.1740279 -4.143919 -4.1028819 -4.0497861 -4.0052686 -4.0175152 -4.0633459 -4.0835071 -4.0737786 -4.0487885 -4.0159559 -4.0059237 -4.0438552][-4.2427526 -4.2109394 -4.1858768 -4.1574588 -4.1159725 -4.0661011 -4.0269504 -4.045692 -4.0956044 -4.1178074 -4.1096573 -4.0927658 -4.0743265 -4.0726051 -4.1068373][-4.2599473 -4.2335672 -4.214869 -4.1963172 -4.1698232 -4.1393661 -4.1160116 -4.1346498 -4.1747804 -4.1917863 -4.1875911 -4.1808968 -4.1763186 -4.1801376 -4.2053318][-4.287725 -4.2657032 -4.2531157 -4.2458797 -4.2364578 -4.2268286 -4.2194948 -4.2348804 -4.2607431 -4.2702947 -4.268074 -4.2667193 -4.2700691 -4.2751813 -4.2891145]]...]
INFO - root - 2017-12-05 21:10:48.881129: step 41010, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 76h:46m:29s remains)
INFO - root - 2017-12-05 21:10:58.092534: step 41020, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 71h:27m:16s remains)
INFO - root - 2017-12-05 21:11:06.867511: step 41030, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 73h:56m:27s remains)
INFO - root - 2017-12-05 21:11:16.004064: step 41040, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 74h:38m:52s remains)
INFO - root - 2017-12-05 21:11:25.230640: step 41050, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 72h:33m:19s remains)
INFO - root - 2017-12-05 21:11:34.336931: step 41060, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 73h:12m:24s remains)
INFO - root - 2017-12-05 21:11:43.500734: step 41070, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 75h:15m:40s remains)
INFO - root - 2017-12-05 21:11:52.595917: step 41080, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 73h:52m:56s remains)
INFO - root - 2017-12-05 21:12:01.940303: step 41090, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 74h:18m:18s remains)
INFO - root - 2017-12-05 21:12:10.923502: step 41100, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 73h:17m:09s remains)
2017-12-05 21:12:11.755791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1881986 -4.1833248 -4.2056155 -4.2303033 -4.242383 -4.2435889 -4.2338138 -4.220612 -4.2121272 -4.2083068 -4.2079449 -4.2133765 -4.2333097 -4.2604237 -4.2785344][-4.1665955 -4.1650953 -4.1895733 -4.2125587 -4.2181492 -4.2170334 -4.2108879 -4.2037888 -4.203012 -4.2023883 -4.1998372 -4.2033577 -4.2256727 -4.2539711 -4.2769556][-4.1656904 -4.1649656 -4.182394 -4.1945653 -4.1921134 -4.1837673 -4.1740537 -4.1705742 -4.1804938 -4.1914506 -4.1994085 -4.2095242 -4.2324629 -4.258604 -4.28131][-4.1919727 -4.188201 -4.1910172 -4.1836381 -4.1632733 -4.1374626 -4.1179924 -4.1152434 -4.1386247 -4.1730018 -4.2060022 -4.2302432 -4.2539167 -4.2776518 -4.298347][-4.2386456 -4.2281418 -4.2116876 -4.1769795 -4.1234255 -4.0631804 -4.026392 -4.0345817 -4.0828724 -4.1470242 -4.2077904 -4.2496634 -4.2762012 -4.2992864 -4.3163943][-4.2778835 -4.2610416 -4.2259593 -4.1600938 -4.0583229 -3.9499681 -3.9008169 -3.9437377 -4.0352416 -4.1296768 -4.2061939 -4.2594037 -4.2924123 -4.3168025 -4.32957][-4.2928767 -4.2747612 -4.2280545 -4.1370921 -3.9900942 -3.8353503 -3.7854395 -3.883971 -4.0255442 -4.1397357 -4.2174106 -4.2723117 -4.3084416 -4.3297319 -4.3340454][-4.2933812 -4.2759032 -4.2215056 -4.1178727 -3.9577427 -3.8037016 -3.7818635 -3.9139352 -4.0658484 -4.1761837 -4.2470784 -4.2953062 -4.3226924 -4.3300691 -4.3189273][-4.290122 -4.278162 -4.2233186 -4.1238718 -3.9894927 -3.8904338 -3.9076846 -4.0239477 -4.1425705 -4.2283864 -4.2825747 -4.3125081 -4.3212676 -4.3083081 -4.2809796][-4.2834272 -4.2839241 -4.2412806 -4.1653848 -4.0771708 -4.0328107 -4.0670681 -4.1450057 -4.2221837 -4.2799892 -4.3101463 -4.3148108 -4.3016706 -4.274703 -4.2432919][-4.284554 -4.297966 -4.2723122 -4.2258458 -4.1788316 -4.1629987 -4.1921268 -4.2358637 -4.2788072 -4.3079734 -4.3143206 -4.2999024 -4.2774873 -4.2536664 -4.2308812][-4.2980189 -4.3133307 -4.2990417 -4.272985 -4.2521296 -4.2476182 -4.267066 -4.2908196 -4.3079734 -4.3109083 -4.3005867 -4.2834034 -4.2663107 -4.2507739 -4.2358594][-4.3114114 -4.3196411 -4.3078084 -4.2905521 -4.2809453 -4.2788429 -4.2922378 -4.305532 -4.3080878 -4.2967763 -4.2825804 -4.2730532 -4.2659783 -4.2563391 -4.2423725][-4.3157506 -4.3125868 -4.2962165 -4.2801962 -4.2733564 -4.2703986 -4.2770419 -4.2815027 -4.2780728 -4.267601 -4.2605581 -4.2612948 -4.2603459 -4.2521133 -4.2382517][-4.3115649 -4.3006744 -4.2809339 -4.2637978 -4.2553906 -4.2462282 -4.2434354 -4.2399883 -4.2353029 -4.228828 -4.2287006 -4.2370272 -4.2416472 -4.2358351 -4.2229438]]...]
INFO - root - 2017-12-05 21:12:20.834620: step 41110, loss = 2.08, batch loss = 2.03 (8.5 examples/sec; 0.941 sec/batch; 76h:08m:52s remains)
INFO - root - 2017-12-05 21:12:29.882936: step 41120, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 76h:26m:04s remains)
INFO - root - 2017-12-05 21:12:38.968348: step 41130, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 75h:25m:28s remains)
INFO - root - 2017-12-05 21:12:48.167672: step 41140, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 72h:00m:41s remains)
INFO - root - 2017-12-05 21:12:57.333479: step 41150, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.913 sec/batch; 73h:55m:19s remains)
INFO - root - 2017-12-05 21:13:06.272301: step 41160, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.908 sec/batch; 73h:28m:13s remains)
INFO - root - 2017-12-05 21:13:15.330115: step 41170, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 73h:41m:28s remains)
INFO - root - 2017-12-05 21:13:24.312358: step 41180, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 74h:07m:27s remains)
INFO - root - 2017-12-05 21:13:33.525060: step 41190, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 73h:05m:05s remains)
INFO - root - 2017-12-05 21:13:42.777585: step 41200, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 77h:19m:18s remains)
2017-12-05 21:13:43.584128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2545109 -4.2444644 -4.2404404 -4.2421951 -4.2493787 -4.2601333 -4.2732673 -4.2874556 -4.300601 -4.3118114 -4.3216224 -4.3287067 -4.3326235 -4.3343916 -4.3348875][-4.2679677 -4.2538261 -4.245327 -4.2426705 -4.2464395 -4.2558918 -4.2690811 -4.2844229 -4.2986679 -4.3108726 -4.3217568 -4.3295689 -4.3340168 -4.3360047 -4.3364568][-4.3012214 -4.2883658 -4.2790365 -4.2737608 -4.27364 -4.2780657 -4.2857733 -4.296473 -4.3063555 -4.3153677 -4.3241658 -4.3309145 -4.3348885 -4.3366709 -4.3369808][-4.3342085 -4.32489 -4.3160214 -4.3077154 -4.3018084 -4.2980504 -4.2973709 -4.3013678 -4.3067303 -4.3134437 -4.3213816 -4.3285708 -4.3336387 -4.3363929 -4.3373003][-4.3437276 -4.3351617 -4.3244715 -4.3118176 -4.2987409 -4.2862015 -4.278163 -4.2776718 -4.2823849 -4.2922459 -4.3048038 -4.3163958 -4.325654 -4.3318248 -4.335187][-4.3183031 -4.3073692 -4.2926917 -4.2739854 -4.2523837 -4.2313762 -4.217998 -4.2162452 -4.223927 -4.2415152 -4.2652655 -4.2876215 -4.3053522 -4.3181348 -4.3263831][-4.2620358 -4.2501011 -4.2317572 -4.2070417 -4.1758957 -4.1446481 -4.1240435 -4.1209712 -4.1333804 -4.1611271 -4.1987443 -4.2359986 -4.2657752 -4.2884593 -4.3041854][-4.1949363 -4.1905031 -4.1768808 -4.1525478 -4.1161118 -4.0747056 -4.0432692 -4.0336246 -4.0445528 -4.0756478 -4.1212511 -4.1713471 -4.2139125 -4.2473869 -4.2710295][-4.1359982 -4.14527 -4.1471152 -4.136333 -4.1096549 -4.0729127 -4.0385251 -4.0201478 -4.0215168 -4.0426459 -4.0810184 -4.1305127 -4.1752191 -4.2116671 -4.2379723][-4.12067 -4.1389856 -4.1543217 -4.159132 -4.1499004 -4.1307268 -4.1074419 -4.0903125 -4.0852475 -4.0909839 -4.1078839 -4.1372709 -4.1677432 -4.1954732 -4.217587][-4.1623187 -4.1813059 -4.2006879 -4.21389 -4.2186756 -4.2166915 -4.2078781 -4.1978064 -4.1909633 -4.1853561 -4.1826282 -4.1878815 -4.1968946 -4.2078781 -4.219059][-4.22502 -4.2407479 -4.2594795 -4.2769918 -4.2907195 -4.3003635 -4.302258 -4.2985229 -4.2919564 -4.2810745 -4.26797 -4.2574606 -4.2502089 -4.2469187 -4.2469397][-4.2813478 -4.2917228 -4.3066673 -4.3230481 -4.3377271 -4.3494682 -4.3550072 -4.3548779 -4.3504553 -4.3410263 -4.3281679 -4.315299 -4.3038259 -4.2954865 -4.2903166][-4.3133879 -4.317379 -4.326272 -4.3372097 -4.3481259 -4.3575177 -4.3636179 -4.3661962 -4.3659029 -4.3622642 -4.3559566 -4.3484626 -4.3406773 -4.3338323 -4.3282237][-4.325973 -4.3243618 -4.3273091 -4.332459 -4.3389754 -4.3455439 -4.3515573 -4.3562756 -4.3596854 -4.3609872 -4.3602505 -4.3581038 -4.3548646 -4.3512192 -4.3477354]]...]
INFO - root - 2017-12-05 21:13:52.780610: step 41210, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 73h:55m:20s remains)
INFO - root - 2017-12-05 21:14:01.725599: step 41220, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 75h:58m:51s remains)
INFO - root - 2017-12-05 21:14:11.040963: step 41230, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 74h:17m:01s remains)
INFO - root - 2017-12-05 21:14:20.486729: step 41240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 73h:59m:11s remains)
INFO - root - 2017-12-05 21:14:29.625476: step 41250, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 73h:11m:34s remains)
INFO - root - 2017-12-05 21:14:38.700499: step 41260, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 76h:11m:54s remains)
INFO - root - 2017-12-05 21:14:47.793345: step 41270, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 76h:13m:25s remains)
INFO - root - 2017-12-05 21:14:56.970532: step 41280, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 73h:04m:48s remains)
INFO - root - 2017-12-05 21:15:06.066867: step 41290, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 71h:58m:20s remains)
INFO - root - 2017-12-05 21:15:15.370870: step 41300, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 75h:36m:24s remains)
2017-12-05 21:15:16.149162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1941409 -4.2131171 -4.2240863 -4.2245741 -4.2158608 -4.2220168 -4.2327414 -4.2307754 -4.2253709 -4.2237115 -4.2104406 -4.1885629 -4.1601729 -4.1125484 -4.0832243][-4.1902652 -4.208045 -4.21867 -4.2150378 -4.2039971 -4.2068944 -4.2119584 -4.2086039 -4.2069597 -4.21622 -4.2148776 -4.1977987 -4.1707158 -4.1270952 -4.1080217][-4.1810045 -4.1959925 -4.2038703 -4.1920819 -4.1770353 -4.1740441 -4.1715136 -4.1660557 -4.1732903 -4.1985879 -4.2128549 -4.2068205 -4.1899476 -4.1571264 -4.1505623][-4.1685877 -4.1830783 -4.191329 -4.1819463 -4.1654091 -4.1551223 -4.142695 -4.1345038 -4.1462879 -4.1787319 -4.2027006 -4.2060084 -4.1975894 -4.17269 -4.1715188][-4.1564403 -4.1774063 -4.1899886 -4.184885 -4.1664295 -4.1468143 -4.1215515 -4.1036057 -4.1133585 -4.1502109 -4.1815982 -4.1933174 -4.1919355 -4.1673245 -4.1616168][-4.1417732 -4.1687989 -4.1851625 -4.1826148 -4.1644783 -4.1408448 -4.1055393 -4.0748415 -4.07764 -4.1196957 -4.1583424 -4.1724524 -4.171432 -4.1393704 -4.122817][-4.131382 -4.1487069 -4.1521344 -4.1397591 -4.1199508 -4.0992942 -4.0676432 -4.0374813 -4.0420256 -4.0929928 -4.1397762 -4.1527386 -4.1442814 -4.0991621 -4.0646195][-4.135839 -4.13159 -4.1084557 -4.0722928 -4.0400009 -4.0269756 -4.020278 -4.0119772 -4.0257292 -4.076026 -4.1234207 -4.1346045 -4.1219764 -4.0729074 -4.0281615][-4.1646166 -4.1431046 -4.1024556 -4.0516424 -4.0162048 -4.0163813 -4.0353284 -4.0483537 -4.0670428 -4.1053276 -4.1407442 -4.1439309 -4.1310978 -4.0894914 -4.0552425][-4.196847 -4.1709657 -4.1331649 -4.0869961 -4.059732 -4.070488 -4.0999546 -4.1204052 -4.1379013 -4.1587906 -4.174356 -4.1671176 -4.1545076 -4.125082 -4.1093955][-4.2114325 -4.191648 -4.1663313 -4.1327782 -4.116353 -4.1326609 -4.1609187 -4.1816778 -4.1981797 -4.2069955 -4.2093844 -4.199492 -4.18988 -4.1691966 -4.1610327][-4.2157946 -4.2016106 -4.1878238 -4.1698289 -4.1631556 -4.1789341 -4.2005115 -4.2187662 -4.2307491 -4.2335992 -4.2310634 -4.225039 -4.2206593 -4.2081223 -4.200727][-4.210959 -4.2034726 -4.2002974 -4.1973476 -4.1985826 -4.2112637 -4.2247519 -4.2392106 -4.2459803 -4.2456908 -4.2445912 -4.2442508 -4.2436986 -4.2335868 -4.2227674][-4.1959958 -4.201961 -4.209466 -4.2115207 -4.2121258 -4.2197809 -4.2255135 -4.236289 -4.2404013 -4.2407084 -4.2427526 -4.246531 -4.2480884 -4.2372289 -4.2230163][-4.15596 -4.17629 -4.1958218 -4.1997509 -4.2012835 -4.2088833 -4.2127728 -4.2223072 -4.2279849 -4.2317052 -4.2349706 -4.2371092 -4.2355494 -4.2209849 -4.2049737]]...]
INFO - root - 2017-12-05 21:15:25.329892: step 41310, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.881 sec/batch; 71h:14m:14s remains)
INFO - root - 2017-12-05 21:15:34.471953: step 41320, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 74h:26m:03s remains)
INFO - root - 2017-12-05 21:15:43.439260: step 41330, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 74h:48m:11s remains)
INFO - root - 2017-12-05 21:15:52.812310: step 41340, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 74h:23m:12s remains)
INFO - root - 2017-12-05 21:16:01.980408: step 41350, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 72h:51m:16s remains)
INFO - root - 2017-12-05 21:16:10.984497: step 41360, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.939 sec/batch; 75h:58m:07s remains)
INFO - root - 2017-12-05 21:16:20.222738: step 41370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 75h:27m:35s remains)
INFO - root - 2017-12-05 21:16:29.420242: step 41380, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 73h:41m:54s remains)
INFO - root - 2017-12-05 21:16:38.557547: step 41390, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 74h:29m:41s remains)
INFO - root - 2017-12-05 21:16:47.490263: step 41400, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.794 sec/batch; 64h:13m:36s remains)
2017-12-05 21:16:48.237736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2513933 -4.2448692 -4.2396493 -4.2278705 -4.2166824 -4.2190337 -4.2290497 -4.2465458 -4.2644596 -4.2737617 -4.26713 -4.2443409 -4.2133794 -4.1927266 -4.1968288][-4.2379994 -4.2323222 -4.2222567 -4.2029495 -4.1829934 -4.1816406 -4.1963854 -4.2246237 -4.2513614 -4.2670069 -4.2638946 -4.2395062 -4.2066031 -4.1866713 -4.1960111][-4.2243247 -4.2164283 -4.2008777 -4.1719656 -4.1388111 -4.1271687 -4.1441808 -4.1853814 -4.2265549 -4.2525558 -4.2575111 -4.2390146 -4.2141442 -4.2015653 -4.2165327][-4.2088537 -4.1993823 -4.1807027 -4.1447725 -4.0980954 -4.0717707 -4.0873179 -4.1417212 -4.2004223 -4.2392092 -4.2550554 -4.2460523 -4.2340045 -4.2331309 -4.2488928][-4.20089 -4.1873693 -4.1583381 -4.1112342 -4.0464005 -4.0004559 -4.0121922 -4.0815229 -4.1605964 -4.21482 -4.2415195 -4.2430882 -4.2428021 -4.2516546 -4.2670879][-4.1988411 -4.18056 -4.139432 -4.0770741 -3.9913981 -3.9200482 -3.9283977 -4.0099382 -4.1032119 -4.1732116 -4.2112021 -4.218256 -4.2238603 -4.2372961 -4.2538571][-4.2032247 -4.1812625 -4.1276565 -4.0546284 -3.9564145 -3.8701069 -3.8769197 -3.9605687 -4.0559692 -4.134469 -4.1797576 -4.1874719 -4.1910844 -4.2048573 -4.2237391][-4.2283387 -4.2107539 -4.1566463 -4.0862303 -3.9910362 -3.9048419 -3.9059379 -3.9720032 -4.0536318 -4.128212 -4.1726575 -4.1786561 -4.1780648 -4.1884208 -4.20851][-4.2527065 -4.2443981 -4.1998796 -4.1424522 -4.0650563 -3.9931135 -3.9917681 -4.0378346 -4.0995383 -4.1600108 -4.1962323 -4.1985979 -4.1954932 -4.2044215 -4.22488][-4.2656088 -4.26549 -4.237905 -4.1970639 -4.13993 -4.0843444 -4.078835 -4.1091905 -4.1549821 -4.1974139 -4.2237663 -4.2274313 -4.2287359 -4.2393785 -4.2584558][-4.269712 -4.2699432 -4.2540073 -4.2245326 -4.1825833 -4.14463 -4.1423707 -4.16503 -4.2009969 -4.2318125 -4.2517109 -4.2604976 -4.269433 -4.2809124 -4.295681][-4.2626991 -4.2582607 -4.2469587 -4.2252574 -4.1954775 -4.1753359 -4.1799631 -4.1999731 -4.2296886 -4.25342 -4.2695708 -4.2800784 -4.2919688 -4.3051991 -4.3155642][-4.25887 -4.2503195 -4.2405305 -4.225256 -4.2056947 -4.1967697 -4.2069383 -4.22615 -4.2480369 -4.2631626 -4.2742448 -4.2845311 -4.2971063 -4.3090425 -4.3142939][-4.26321 -4.2499537 -4.2377234 -4.224781 -4.2113585 -4.2073946 -4.2161617 -4.23084 -4.2448087 -4.2531157 -4.2580023 -4.2633076 -4.2723513 -4.283298 -4.2897596][-4.2761889 -4.2619386 -4.2487249 -4.2368946 -4.2250624 -4.2192078 -4.2212987 -4.228694 -4.2346444 -4.2344747 -4.2314563 -4.231071 -4.2366381 -4.2447939 -4.2499423]]...]
INFO - root - 2017-12-05 21:16:57.399250: step 41410, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 72h:38m:41s remains)
INFO - root - 2017-12-05 21:17:06.470039: step 41420, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 74h:30m:37s remains)
INFO - root - 2017-12-05 21:17:15.635215: step 41430, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 73h:33m:24s remains)
INFO - root - 2017-12-05 21:17:24.823812: step 41440, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 75h:22m:11s remains)
INFO - root - 2017-12-05 21:17:33.920328: step 41450, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 68h:13m:45s remains)
INFO - root - 2017-12-05 21:17:43.102084: step 41460, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 74h:00m:44s remains)
INFO - root - 2017-12-05 21:17:52.153390: step 41470, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 74h:55m:01s remains)
INFO - root - 2017-12-05 21:18:01.447305: step 41480, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 72h:34m:10s remains)
INFO - root - 2017-12-05 21:18:10.598323: step 41490, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 73h:43m:27s remains)
INFO - root - 2017-12-05 21:18:19.584272: step 41500, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.887 sec/batch; 71h:39m:49s remains)
2017-12-05 21:18:20.452858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15461 -4.1712561 -4.1998291 -4.2377877 -4.267911 -4.2723007 -4.2446761 -4.2126465 -4.2037382 -4.2085228 -4.2119403 -4.2166557 -4.2179127 -4.2134671 -4.1933746][-4.1520162 -4.1809545 -4.2212019 -4.2621307 -4.2863646 -4.2867537 -4.2539454 -4.2182302 -4.2033882 -4.2034516 -4.2060556 -4.2129564 -4.2197776 -4.2226748 -4.2082138][-4.1594944 -4.2047358 -4.2478008 -4.2791243 -4.2924418 -4.2811642 -4.2419086 -4.2011642 -4.1828647 -4.1822829 -4.1889071 -4.2020531 -4.2176657 -4.2293935 -4.2268372][-4.1843123 -4.2354612 -4.2680354 -4.28041 -4.2774916 -4.2501745 -4.1982865 -4.1516652 -4.1398072 -4.1507487 -4.1761265 -4.2061477 -4.2328196 -4.2524762 -4.2582974][-4.2053189 -4.2495122 -4.2709551 -4.2684278 -4.2493372 -4.2049208 -4.1351843 -4.0853477 -4.0930309 -4.1317773 -4.1814628 -4.22392 -4.25464 -4.2757106 -4.285563][-4.2184434 -4.2456841 -4.2557397 -4.2409687 -4.2067943 -4.1390133 -4.0419693 -3.9968367 -4.0450559 -4.1183877 -4.1856284 -4.2312703 -4.2603478 -4.2778816 -4.28981][-4.2123852 -4.2226 -4.2248392 -4.2027707 -4.1538091 -4.057601 -3.9295082 -3.9033294 -4.0065603 -4.1129565 -4.1902366 -4.2362161 -4.2597618 -4.2721834 -4.2801394][-4.1837258 -4.1833625 -4.1838503 -4.1634746 -4.108942 -4.0086942 -3.8865302 -3.8949482 -4.021349 -4.1315689 -4.2048211 -4.2445946 -4.2592196 -4.2664509 -4.2705932][-4.1371021 -4.1361914 -4.142899 -4.1340942 -4.095427 -4.0273466 -3.9608431 -3.9953308 -4.09536 -4.1826434 -4.2379322 -4.264534 -4.26993 -4.2715678 -4.2706337][-4.0965853 -4.1009607 -4.115572 -4.1193943 -4.1027374 -4.0726366 -4.0550895 -4.0988908 -4.1715765 -4.233995 -4.2722535 -4.2911978 -4.2924576 -4.2911038 -4.2876959][-4.0933018 -4.0991154 -4.1164646 -4.1259012 -4.1277614 -4.1279168 -4.1333256 -4.1716466 -4.2245946 -4.2693782 -4.2980976 -4.313446 -4.31549 -4.3150859 -4.3143291][-4.1039262 -4.1119547 -4.1319809 -4.1463404 -4.1602912 -4.1728916 -4.1834316 -4.2122478 -4.2539945 -4.2917428 -4.3169608 -4.3283396 -4.3272791 -4.3270912 -4.3317165][-4.1372013 -4.147656 -4.1658955 -4.1799755 -4.1971116 -4.2115207 -4.2208958 -4.2420778 -4.2767606 -4.3061833 -4.3240747 -4.3287439 -4.3236318 -4.3219213 -4.32985][-4.2003345 -4.2069564 -4.2132711 -4.2166705 -4.2258906 -4.236156 -4.2424808 -4.2589602 -4.2872658 -4.30781 -4.3167419 -4.3133674 -4.3042417 -4.3012218 -4.3119359][-4.2561536 -4.260767 -4.2595706 -4.252933 -4.2483621 -4.2493749 -4.25124 -4.2638187 -4.2851496 -4.296649 -4.2966862 -4.28666 -4.277184 -4.275434 -4.2890582]]...]
INFO - root - 2017-12-05 21:18:29.564152: step 41510, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 71h:48m:12s remains)
INFO - root - 2017-12-05 21:18:38.756503: step 41520, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 72h:05m:29s remains)
INFO - root - 2017-12-05 21:18:47.899148: step 41530, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 72h:44m:06s remains)
INFO - root - 2017-12-05 21:18:56.841100: step 41540, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 70h:51m:33s remains)
INFO - root - 2017-12-05 21:19:05.831767: step 41550, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 73h:44m:15s remains)
INFO - root - 2017-12-05 21:19:14.791186: step 41560, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 72h:11m:57s remains)
INFO - root - 2017-12-05 21:19:23.875543: step 41570, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 74h:00m:15s remains)
INFO - root - 2017-12-05 21:19:33.053949: step 41580, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 76h:11m:59s remains)
INFO - root - 2017-12-05 21:19:41.904179: step 41590, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 71h:59m:23s remains)
INFO - root - 2017-12-05 21:19:50.900753: step 41600, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 73h:13m:14s remains)
2017-12-05 21:19:51.723788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3155007 -4.3286366 -4.3273196 -4.3168869 -4.3071938 -4.3046069 -4.3039975 -4.3061495 -4.3119111 -4.3183503 -4.3231668 -4.3275843 -4.3321552 -4.3374963 -4.3436852][-4.2925692 -4.3127193 -4.3113112 -4.2972579 -4.2820349 -4.2725549 -4.2661119 -4.2664943 -4.2758851 -4.2888379 -4.3007827 -4.3106842 -4.3173394 -4.3245482 -4.3334508][-4.2672591 -4.2864723 -4.280601 -4.261785 -4.2401433 -4.2196422 -4.203074 -4.2010937 -4.2165403 -4.240418 -4.26714 -4.2891159 -4.3009167 -4.3083773 -4.3170853][-4.2383013 -4.2504625 -4.2363343 -4.2121196 -4.1868033 -4.1608963 -4.1356783 -4.1263485 -4.1423 -4.1744795 -4.21638 -4.2526865 -4.2731419 -4.2842312 -4.2937117][-4.2141948 -4.2192254 -4.1973596 -4.1683664 -4.1408544 -4.1109943 -4.075563 -4.0515738 -4.0631318 -4.105969 -4.1606364 -4.206418 -4.2320514 -4.2476144 -4.2615976][-4.2120533 -4.2135525 -4.1882677 -4.1519051 -4.113533 -4.0693741 -4.0135179 -3.9619405 -3.9625623 -4.0231781 -4.097403 -4.1556935 -4.1881881 -4.2092457 -4.2297931][-4.2272968 -4.2264967 -4.2025619 -4.1618605 -4.1084304 -4.0403271 -3.955478 -3.8604555 -3.8369603 -3.9290881 -4.0367265 -4.11214 -4.1554227 -4.1846976 -4.2127485][-4.218102 -4.2176466 -4.2031364 -4.1751342 -4.1208076 -4.0438938 -3.9499257 -3.8370571 -3.7878942 -3.8880258 -4.0102649 -4.0925355 -4.1412678 -4.1762233 -4.2103109][-4.1744061 -4.1765842 -4.1763482 -4.1743269 -4.1451969 -4.0882177 -4.0225525 -3.9492946 -3.9080453 -3.9589233 -4.0369177 -4.0950341 -4.1370087 -4.1684651 -4.2033353][-4.1174812 -4.1292858 -4.1477556 -4.17181 -4.1679688 -4.1330457 -4.0961533 -4.0689077 -4.0491123 -4.0606732 -4.0889883 -4.1180911 -4.147614 -4.1711879 -4.2014065][-4.0867972 -4.1094089 -4.1440544 -4.181798 -4.1886969 -4.163466 -4.1434813 -4.1411366 -4.1381493 -4.1406589 -4.1446948 -4.1540141 -4.1750503 -4.1909428 -4.2134037][-4.1039505 -4.1241255 -4.1535635 -4.18741 -4.19788 -4.1810737 -4.1725917 -4.1837907 -4.191957 -4.1945977 -4.1883354 -4.1894531 -4.2046995 -4.2162871 -4.233995][-4.158174 -4.1717629 -4.1826658 -4.1985288 -4.2037125 -4.1921997 -4.1902556 -4.2051096 -4.2167249 -4.219821 -4.2187972 -4.2181807 -4.2309456 -4.24356 -4.2621059][-4.2205367 -4.2261462 -4.2194185 -4.2142735 -4.21182 -4.2059174 -4.2125187 -4.2280717 -4.2359638 -4.2431788 -4.249166 -4.252039 -4.2644362 -4.2791266 -4.2981887][-4.2670159 -4.2659597 -4.2486048 -4.2309351 -4.22315 -4.2266397 -4.2404041 -4.253099 -4.256681 -4.2638097 -4.2743011 -4.2821217 -4.2923875 -4.3054628 -4.3214746]]...]
INFO - root - 2017-12-05 21:20:00.759740: step 41610, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 72h:36m:47s remains)
INFO - root - 2017-12-05 21:20:09.852151: step 41620, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 72h:41m:41s remains)
INFO - root - 2017-12-05 21:20:19.047818: step 41630, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 74h:00m:01s remains)
INFO - root - 2017-12-05 21:20:27.975646: step 41640, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 73h:12m:55s remains)
INFO - root - 2017-12-05 21:20:37.251638: step 41650, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.925 sec/batch; 74h:45m:23s remains)
INFO - root - 2017-12-05 21:20:46.503755: step 41660, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 73h:00m:16s remains)
INFO - root - 2017-12-05 21:20:55.481987: step 41670, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 71h:47m:08s remains)
INFO - root - 2017-12-05 21:21:04.632538: step 41680, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.938 sec/batch; 75h:47m:52s remains)
INFO - root - 2017-12-05 21:21:13.672015: step 41690, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 73h:23m:14s remains)
INFO - root - 2017-12-05 21:21:22.630466: step 41700, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 71h:11m:35s remains)
2017-12-05 21:21:23.449510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3295312 -4.3226724 -4.3187957 -4.3162355 -4.3074193 -4.2887082 -4.2664723 -4.2549343 -4.253458 -4.2499771 -4.2489529 -4.2598448 -4.2648811 -4.27201 -4.273344][-4.3252692 -4.3144522 -4.30733 -4.3023357 -4.291091 -4.2714138 -4.2524858 -4.2430367 -4.2418284 -4.2394433 -4.2391834 -4.2519231 -4.2607908 -4.2669983 -4.2712836][-4.3245072 -4.3091784 -4.2971048 -4.289185 -4.2758789 -4.2522426 -4.2271132 -4.218565 -4.2223091 -4.2256637 -4.2285681 -4.2466626 -4.262743 -4.26878 -4.2726064][-4.3182511 -4.2977095 -4.2810416 -4.2706308 -4.2563367 -4.2231064 -4.1836739 -4.1747866 -4.1893277 -4.2041306 -4.2149963 -4.2390752 -4.2651224 -4.274404 -4.2765603][-4.3037181 -4.2761226 -4.2512732 -4.2359743 -4.218143 -4.1731424 -4.1155558 -4.1018133 -4.13038 -4.1646786 -4.1917095 -4.2242031 -4.2609172 -4.2776489 -4.2806354][-4.286181 -4.2529626 -4.2165365 -4.1880174 -4.1611018 -4.1036315 -4.0258121 -4.0011435 -4.0517716 -4.1156497 -4.1654406 -4.2079411 -4.2525835 -4.2774382 -4.2825947][-4.278863 -4.243525 -4.1991529 -4.1587405 -4.120266 -4.0466585 -3.9383755 -3.8870506 -3.9568002 -4.0577369 -4.1338487 -4.1895833 -4.2390771 -4.2719622 -4.2810373][-4.2775946 -4.2431703 -4.1957827 -4.1489096 -4.0999503 -4.0159211 -3.8942385 -3.8265977 -3.9087374 -4.03335 -4.1215625 -4.1831446 -4.2332768 -4.2679596 -4.2792377][-4.2804732 -4.2496881 -4.2044797 -4.1595416 -4.1139722 -4.0480456 -3.9610233 -3.917865 -3.9789979 -4.07126 -4.1368103 -4.1865568 -4.2331858 -4.2680554 -4.2790046][-4.2864289 -4.2626185 -4.2266221 -4.1898761 -4.157548 -4.1211977 -4.0729742 -4.0484018 -4.0807786 -4.1248331 -4.1583996 -4.1937432 -4.2369747 -4.2695227 -4.2795448][-4.2988486 -4.2875004 -4.2661576 -4.2420244 -4.22247 -4.2015195 -4.1683564 -4.1426935 -4.1519642 -4.1628823 -4.1767168 -4.2049122 -4.2446842 -4.2723856 -4.2802463][-4.3097034 -4.3093896 -4.3024263 -4.292387 -4.2861714 -4.2735744 -4.2421365 -4.2128115 -4.2052159 -4.1977777 -4.2001638 -4.2212377 -4.2554488 -4.2782903 -4.2813067][-4.3127637 -4.3186035 -4.3199921 -4.3190479 -4.3209443 -4.3149109 -4.2903972 -4.2657342 -4.2522779 -4.235652 -4.2283764 -4.2409821 -4.2674303 -4.2851253 -4.2829118][-4.3060889 -4.3137927 -4.318264 -4.3221111 -4.3282342 -4.3265862 -4.3095703 -4.2904096 -4.2782316 -4.26142 -4.2497654 -4.2584767 -4.276979 -4.2891703 -4.2829952][-4.2928882 -4.3019137 -4.3098192 -4.31803 -4.3263435 -4.3261719 -4.3107519 -4.2928872 -4.2824097 -4.2682796 -4.2581758 -4.2664003 -4.2802091 -4.2890286 -4.2814097]]...]
INFO - root - 2017-12-05 21:21:32.383111: step 41710, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 70h:47m:06s remains)
INFO - root - 2017-12-05 21:21:41.570248: step 41720, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 76h:56m:13s remains)
INFO - root - 2017-12-05 21:21:50.581254: step 41730, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 72h:33m:40s remains)
INFO - root - 2017-12-05 21:21:59.456870: step 41740, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 72h:18m:08s remains)
INFO - root - 2017-12-05 21:22:08.332475: step 41750, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 72h:00m:04s remains)
INFO - root - 2017-12-05 21:22:17.357866: step 41760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:52m:44s remains)
INFO - root - 2017-12-05 21:22:26.390169: step 41770, loss = 2.10, batch loss = 2.05 (9.0 examples/sec; 0.886 sec/batch; 71h:32m:28s remains)
INFO - root - 2017-12-05 21:22:35.420531: step 41780, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 73h:37m:42s remains)
INFO - root - 2017-12-05 21:22:44.620194: step 41790, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 70h:35m:30s remains)
INFO - root - 2017-12-05 21:22:53.653964: step 41800, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 74h:03m:43s remains)
2017-12-05 21:22:54.588414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1981711 -4.21805 -4.235508 -4.2524109 -4.26108 -4.2620511 -4.2555127 -4.244071 -4.240025 -4.2464314 -4.2473707 -4.2342386 -4.2075839 -4.1835337 -4.1807303][-4.176455 -4.1965041 -4.211978 -4.229557 -4.2344942 -4.2302308 -4.2169766 -4.2014933 -4.1983924 -4.2081437 -4.2157841 -4.211884 -4.1955185 -4.1780205 -4.1761351][-4.1506829 -4.1706767 -4.18679 -4.2061 -4.2063055 -4.1915145 -4.1678495 -4.1481209 -4.1482892 -4.1666465 -4.1879082 -4.192605 -4.1847014 -4.1771526 -4.179709][-4.1360469 -4.1562262 -4.1691604 -4.1815596 -4.1684675 -4.1386881 -4.100841 -4.0737987 -4.0772142 -4.1131153 -4.1570067 -4.1770711 -4.1831603 -4.19009 -4.2016435][-4.1390572 -4.1540504 -4.1569982 -4.1533122 -4.1228867 -4.0761652 -4.0254087 -3.9926255 -4.0032787 -4.0604515 -4.1262021 -4.1620989 -4.1825123 -4.1994619 -4.2193847][-4.1538849 -4.1565118 -4.14631 -4.1216736 -4.0654812 -3.9997342 -3.9334517 -3.8963981 -3.9215806 -4.00711 -4.0961647 -4.145658 -4.1768165 -4.1999612 -4.2292938][-4.1638417 -4.1539035 -4.1337585 -4.092876 -4.0230546 -3.9488671 -3.8669081 -3.8160467 -3.8499923 -3.9570446 -4.0611877 -4.1232896 -4.1654787 -4.1943274 -4.2281942][-4.163928 -4.1492891 -4.1313605 -4.094759 -4.040894 -3.9770303 -3.8922589 -3.8314154 -3.8557863 -3.9493706 -4.0418067 -4.1049056 -4.1521492 -4.1816463 -4.2128778][-4.1660438 -4.1556482 -4.1495824 -4.1343541 -4.1093645 -4.0630107 -3.9874892 -3.9308393 -3.9428473 -4.0047073 -4.0701022 -4.120605 -4.1592903 -4.1780167 -4.1991696][-4.1638007 -4.163497 -4.1708074 -4.1740012 -4.1686187 -4.1354218 -4.075129 -4.0344877 -4.0437803 -4.079422 -4.1199684 -4.1537428 -4.1729097 -4.1745629 -4.1850228][-4.1447263 -4.1548171 -4.1783977 -4.1939144 -4.1906452 -4.1615782 -4.1182423 -4.0941033 -4.1053925 -4.1313663 -4.1592913 -4.1771369 -4.176095 -4.1631613 -4.1680408][-4.1268568 -4.1466455 -4.1812658 -4.2028985 -4.1975269 -4.17113 -4.1437507 -4.1338291 -4.1474953 -4.1696196 -4.1896906 -4.1938257 -4.1780667 -4.158432 -4.1609812][-4.1365194 -4.16068 -4.194695 -4.2155113 -4.2111211 -4.1937132 -4.1813707 -4.1803479 -4.1928263 -4.2074451 -4.2173033 -4.2093253 -4.1869073 -4.1671634 -4.1679511][-4.1556921 -4.1827974 -4.2122421 -4.2307935 -4.2312016 -4.2241268 -4.221498 -4.22403 -4.2310424 -4.237915 -4.2363009 -4.2201924 -4.1965671 -4.1812019 -4.1802034][-4.1799445 -4.2052455 -4.2292862 -4.2465796 -4.2498937 -4.2485251 -4.2493944 -4.2515292 -4.25442 -4.2556124 -4.2480049 -4.2287464 -4.2079906 -4.1995873 -4.1992121]]...]
INFO - root - 2017-12-05 21:23:03.477216: step 41810, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 74h:46m:39s remains)
INFO - root - 2017-12-05 21:23:12.469263: step 41820, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 73h:28m:03s remains)
INFO - root - 2017-12-05 21:23:21.560327: step 41830, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 75h:04m:39s remains)
INFO - root - 2017-12-05 21:23:30.438240: step 41840, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 73h:38m:01s remains)
INFO - root - 2017-12-05 21:23:39.451680: step 41850, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 73h:00m:15s remains)
INFO - root - 2017-12-05 21:23:48.606237: step 41860, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 73h:11m:12s remains)
INFO - root - 2017-12-05 21:23:57.629115: step 41870, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 71h:56m:02s remains)
INFO - root - 2017-12-05 21:24:06.616042: step 41880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:24m:05s remains)
INFO - root - 2017-12-05 21:24:15.885248: step 41890, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 75h:19m:16s remains)
INFO - root - 2017-12-05 21:24:25.005512: step 41900, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 74h:47m:47s remains)
2017-12-05 21:24:25.771232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2277503 -4.1854544 -4.1418133 -4.107017 -4.0764241 -4.0601082 -4.060885 -4.0759821 -4.0876245 -4.093832 -4.0948491 -4.0861621 -4.0790715 -4.0742497 -4.0577559][-4.2106433 -4.1578794 -4.11095 -4.0881896 -4.0787539 -4.0769973 -4.0803952 -4.0826063 -4.088861 -4.0960026 -4.0933385 -4.0769362 -4.0646734 -4.0612307 -4.0478415][-4.1858058 -4.1264548 -4.0839896 -4.0738873 -4.0809884 -4.0871162 -4.0842872 -4.0684314 -4.057127 -4.0635757 -4.0646358 -4.0502086 -4.0407481 -4.0418191 -4.0330062][-4.1572413 -4.0926604 -4.0532913 -4.0494771 -4.0635657 -4.0679436 -4.0511432 -4.0076022 -3.97386 -3.9873328 -4.0168419 -4.0230632 -4.0199327 -4.0188041 -4.0050735][-4.1343322 -4.0671906 -4.0269337 -4.0224042 -4.0319796 -4.021544 -3.9706306 -3.8791533 -3.817627 -3.8668718 -3.9569039 -3.997427 -4.0046844 -3.9954011 -3.9739478][-4.1238508 -4.0597825 -4.0185924 -4.0083151 -4.0063195 -3.9710329 -3.8712811 -3.7235777 -3.6539173 -3.7663996 -3.9111423 -3.9880311 -4.0098181 -3.9991384 -3.9813592][-4.1274376 -4.0743814 -4.0394449 -4.0232434 -4.0057898 -3.9436693 -3.8093681 -3.6554623 -3.6385772 -3.7863383 -3.9334152 -4.0161438 -4.0454521 -4.0366011 -4.0246525][-4.1447535 -4.1094837 -4.0815158 -4.0560222 -4.0216708 -3.9393563 -3.8062866 -3.6992764 -3.7347713 -3.8716843 -3.9861457 -4.0544062 -4.0844879 -4.07403 -4.0616145][-4.1719632 -4.1515565 -4.1274981 -4.0930629 -4.0442896 -3.9570551 -3.8553135 -3.8090813 -3.8617332 -3.9591925 -4.0323591 -4.0808582 -4.1058164 -4.09762 -4.0864487][-4.19861 -4.1846871 -4.1558828 -4.1110353 -4.0502691 -3.9752388 -3.9210191 -3.9174261 -3.9600577 -4.0187173 -4.0633612 -4.0943918 -4.11275 -4.1084218 -4.0964708][-4.2145882 -4.1929903 -4.1491623 -4.093132 -4.0313463 -3.9899082 -3.9810174 -4.0008864 -4.02465 -4.0583587 -4.0835605 -4.103013 -4.1140652 -4.1063066 -4.0903745][-4.2129712 -4.1763053 -4.1216712 -4.0673804 -4.02793 -4.0218644 -4.0357165 -4.0519271 -4.0656528 -4.0873756 -4.0992751 -4.108511 -4.1070642 -4.0999885 -4.0830894][-4.1997342 -4.148603 -4.0897722 -4.0489306 -4.0432005 -4.0556269 -4.0685029 -4.0793428 -4.0912623 -4.11021 -4.112823 -4.1032481 -4.0890107 -4.0849533 -4.0700817][-4.1814957 -4.1204495 -4.065536 -4.044518 -4.0624218 -4.075201 -4.08252 -4.0946565 -4.1130438 -4.1312585 -4.1234746 -4.0972762 -4.0794134 -4.0773768 -4.0616889][-4.1682811 -4.1056414 -4.0673904 -4.0644112 -4.0869832 -4.0954547 -4.0996089 -4.1175289 -4.1420431 -4.1610851 -4.1498775 -4.1191874 -4.0968332 -4.0892339 -4.0698938]]...]
INFO - root - 2017-12-05 21:24:34.761699: step 41910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 69h:36m:00s remains)
INFO - root - 2017-12-05 21:24:43.872391: step 41920, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 73h:25m:53s remains)
INFO - root - 2017-12-05 21:24:53.008414: step 41930, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 72h:55m:52s remains)
INFO - root - 2017-12-05 21:25:01.946569: step 41940, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 72h:22m:51s remains)
INFO - root - 2017-12-05 21:25:10.997453: step 41950, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 73h:31m:15s remains)
INFO - root - 2017-12-05 21:25:19.928756: step 41960, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 71h:21m:06s remains)
INFO - root - 2017-12-05 21:25:28.953340: step 41970, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 74h:39m:40s remains)
INFO - root - 2017-12-05 21:25:38.071591: step 41980, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.930 sec/batch; 75h:02m:52s remains)
INFO - root - 2017-12-05 21:25:47.281355: step 41990, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 74h:08m:16s remains)
INFO - root - 2017-12-05 21:25:56.405896: step 42000, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 70h:35m:59s remains)
2017-12-05 21:25:57.199502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2417903 -4.2417989 -4.2369032 -4.2275114 -4.2185974 -4.2079539 -4.1861415 -4.1546159 -4.1279316 -4.1058068 -4.0973606 -4.11131 -4.1391869 -4.1569357 -4.1664796][-4.2282085 -4.2214069 -4.2110467 -4.2056246 -4.2037797 -4.1999006 -4.183042 -4.1562829 -4.1399455 -4.1274552 -4.1224527 -4.1254473 -4.1344662 -4.1341352 -4.1322861][-4.2038183 -4.1916575 -4.1825128 -4.1839352 -4.1858578 -4.183629 -4.1687155 -4.1478906 -4.144824 -4.1431532 -4.1399412 -4.1333933 -4.1200366 -4.0977058 -4.083375][-4.174993 -4.1562371 -4.1507344 -4.1576295 -4.1623836 -4.1611843 -4.1419563 -4.1197066 -4.1353164 -4.1520548 -4.1541877 -4.1382642 -4.1050234 -4.0718517 -4.0522656][-4.1518664 -4.1218896 -4.113184 -4.1192632 -4.1148138 -4.0983338 -4.053885 -4.019382 -4.0623441 -4.1129751 -4.1332183 -4.1189551 -4.0790896 -4.0544038 -4.0457592][-4.125659 -4.0800819 -4.0509262 -4.0378575 -4.0081978 -3.9538267 -3.8559139 -3.7983966 -3.8986747 -4.0175304 -4.0804377 -4.0896673 -4.0662365 -4.0650392 -4.0758543][-4.1018715 -4.0450292 -3.9934649 -3.9477777 -3.8801069 -3.7737207 -3.6124923 -3.5266001 -3.7096791 -3.9086339 -4.0151982 -4.0554738 -4.0541449 -4.0759244 -4.108398][-4.1092043 -4.0557065 -3.996768 -3.9381254 -3.8570647 -3.7442932 -3.5955386 -3.538115 -3.7173371 -3.9030931 -3.9965448 -4.0310278 -4.0377717 -4.0731516 -4.1195846][-4.1192279 -4.0835552 -4.0413761 -4.0016232 -3.9510963 -3.8828344 -3.8041005 -3.7847915 -3.8947761 -4.0006175 -4.0406685 -4.0481563 -4.0514326 -4.0865383 -4.1324053][-4.1370611 -4.1204362 -4.1001692 -4.0840673 -4.0647449 -4.0347118 -3.998229 -3.9916348 -4.0498033 -4.0974956 -4.1026111 -4.096859 -4.0997376 -4.1213107 -4.1497669][-4.1795139 -4.1714377 -4.1637511 -4.1597729 -4.1557865 -4.1426606 -4.1231775 -4.1135798 -4.1369228 -4.1553135 -4.1540303 -4.1485806 -4.1491103 -4.15243 -4.1611738][-4.2298255 -4.2236333 -4.2218213 -4.2247458 -4.2229781 -4.2142668 -4.2020545 -4.1917162 -4.1984763 -4.2078238 -4.2095165 -4.2061558 -4.1992025 -4.188498 -4.1841631][-4.2762136 -4.2730751 -4.2736087 -4.2766938 -4.2727032 -4.2641277 -4.2576241 -4.2532172 -4.2579966 -4.2657614 -4.2675571 -4.2621274 -4.2508273 -4.232357 -4.2223506][-4.3128386 -4.3134141 -4.3174148 -4.320724 -4.3151622 -4.3085732 -4.3079267 -4.3083615 -4.3131771 -4.3198657 -4.3212514 -4.3157244 -4.3058743 -4.2890434 -4.2774858][-4.3377171 -4.3394794 -4.3441644 -4.3476439 -4.3450837 -4.3421793 -4.3432226 -4.3439107 -4.3453512 -4.348496 -4.3496065 -4.3482065 -4.3441963 -4.3346639 -4.3250012]]...]
INFO - root - 2017-12-05 21:26:06.341209: step 42010, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 71h:16m:17s remains)
INFO - root - 2017-12-05 21:26:15.523544: step 42020, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 73h:33m:10s remains)
INFO - root - 2017-12-05 21:26:24.587950: step 42030, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 74h:24m:53s remains)
INFO - root - 2017-12-05 21:26:33.772596: step 42040, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 76h:37m:42s remains)
INFO - root - 2017-12-05 21:26:42.860806: step 42050, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 74h:08m:09s remains)
INFO - root - 2017-12-05 21:26:51.876383: step 42060, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 73h:45m:04s remains)
INFO - root - 2017-12-05 21:27:01.044990: step 42070, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 71h:21m:11s remains)
INFO - root - 2017-12-05 21:27:10.146059: step 42080, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.895 sec/batch; 72h:12m:07s remains)
INFO - root - 2017-12-05 21:27:19.268690: step 42090, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 76h:37m:53s remains)
INFO - root - 2017-12-05 21:27:28.725535: step 42100, loss = 2.07, batch loss = 2.01 (7.1 examples/sec; 1.131 sec/batch; 91h:12m:25s remains)
2017-12-05 21:27:29.502375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0859418 -4.1034307 -4.1428194 -4.1617689 -4.1562352 -4.1337662 -4.1104622 -4.0852647 -4.0844007 -4.0950532 -4.1166797 -4.1383681 -4.1518908 -4.1604443 -4.1465492][-4.0718479 -4.09093 -4.129128 -4.15102 -4.1476374 -4.1275749 -4.1115885 -4.0851393 -4.0797143 -4.0886726 -4.1142073 -4.1397424 -4.1523633 -4.1576915 -4.1372662][-4.0635548 -4.0834589 -4.1148734 -4.1351471 -4.1341534 -4.1164942 -4.1085553 -4.0869751 -4.0815825 -4.0951996 -4.1256552 -4.1532435 -4.1636815 -4.1637607 -4.1380348][-4.0517759 -4.0725121 -4.0913291 -4.1054192 -4.1030974 -4.0874925 -4.0820346 -4.0667439 -4.0693841 -4.0957131 -4.135375 -4.1665916 -4.1775064 -4.1717262 -4.1450782][-4.061183 -4.080214 -4.0790005 -4.0735221 -4.0567369 -4.0325537 -4.0201511 -4.0071406 -4.0213552 -4.0711007 -4.1287985 -4.1678991 -4.1807675 -4.1703544 -4.1461349][-4.0792117 -4.0966644 -4.0838075 -4.0557642 -4.0175447 -3.9760957 -3.9413533 -3.9115648 -3.9284556 -4.0132713 -4.0978742 -4.1467633 -4.1593828 -4.1427312 -4.1231389][-4.0977492 -4.1160083 -4.1079206 -4.066443 -4.0140729 -3.9568648 -3.8977284 -3.8441832 -3.8544776 -3.9579442 -4.0551739 -4.1109824 -4.1211834 -4.1007214 -4.0868716][-4.1099849 -4.1294823 -4.1258907 -4.0917416 -4.0488734 -4.0049224 -3.956944 -3.9202971 -3.9313543 -3.9954271 -4.0620975 -4.102541 -4.1028414 -4.0818372 -4.069344][-4.1197128 -4.1411538 -4.1439166 -4.120822 -4.094975 -4.0732365 -4.0482345 -4.0338259 -4.0447254 -4.0710826 -4.1035461 -4.1246195 -4.1226196 -4.1056571 -4.0892539][-4.1264496 -4.1519866 -4.1625543 -4.1480384 -4.1298985 -4.1201849 -4.1071224 -4.1023154 -4.108644 -4.1159539 -4.1302028 -4.1418066 -4.1469784 -4.1392946 -4.1215272][-4.1203642 -4.1445279 -4.1623974 -4.156116 -4.1465988 -4.1469316 -4.1437793 -4.1448569 -4.1512461 -4.1508074 -4.1519308 -4.1567559 -4.16141 -4.1578183 -4.1467619][-4.1014404 -4.1221595 -4.1413531 -4.1434927 -4.1445293 -4.1544447 -4.1599236 -4.1654811 -4.1750941 -4.1764231 -4.1726089 -4.1703682 -4.1738405 -4.173996 -4.1709533][-4.0962582 -4.1113515 -4.1315904 -4.1424809 -4.1512723 -4.1626644 -4.1686935 -4.1765628 -4.1861715 -4.1861925 -4.1804771 -4.1800518 -4.188962 -4.196835 -4.1964884][-4.105958 -4.1177559 -4.1371264 -4.1549592 -4.1687274 -4.1788244 -4.1818285 -4.1874123 -4.1951046 -4.1950846 -4.189971 -4.1919012 -4.2041826 -4.2131882 -4.2091322][-4.1322675 -4.1424427 -4.1581 -4.17446 -4.1892114 -4.1987996 -4.2014403 -4.2023778 -4.2043805 -4.2002707 -4.1923466 -4.1915388 -4.2007556 -4.2060814 -4.2016516]]...]
INFO - root - 2017-12-05 21:27:38.573340: step 42110, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 75h:06m:20s remains)
INFO - root - 2017-12-05 21:27:47.669243: step 42120, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 71h:30m:45s remains)
INFO - root - 2017-12-05 21:27:56.911525: step 42130, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 72h:22m:02s remains)
INFO - root - 2017-12-05 21:28:06.071428: step 42140, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.918 sec/batch; 74h:02m:04s remains)
INFO - root - 2017-12-05 21:28:15.062311: step 42150, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 72h:15m:27s remains)
INFO - root - 2017-12-05 21:28:23.997757: step 42160, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 73h:26m:58s remains)
INFO - root - 2017-12-05 21:28:33.264557: step 42170, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 74h:49m:05s remains)
INFO - root - 2017-12-05 21:28:42.514972: step 42180, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 73h:01m:45s remains)
INFO - root - 2017-12-05 21:28:51.605689: step 42190, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 72h:56m:57s remains)
INFO - root - 2017-12-05 21:29:00.649409: step 42200, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 72h:14m:28s remains)
2017-12-05 21:29:01.436585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.308084 -4.3095131 -4.3110232 -4.3134737 -4.3148961 -4.3139534 -4.3097167 -4.3073497 -4.3076286 -4.3093076 -4.3140116 -4.3187022 -4.3221107 -4.3256745 -4.328661][-4.2980189 -4.2995095 -4.3042164 -4.3118777 -4.316678 -4.3150125 -4.3051162 -4.3016844 -4.305078 -4.3082032 -4.3136544 -4.3194928 -4.32018 -4.3206687 -4.3214707][-4.2725697 -4.2717147 -4.2788696 -4.289175 -4.2944174 -4.2889142 -4.2725606 -4.2713876 -4.283392 -4.2923775 -4.3015733 -4.3097916 -4.3086967 -4.3041463 -4.3026419][-4.2398081 -4.2304287 -4.2347193 -4.2457561 -4.2501411 -4.2355814 -4.2090268 -4.2075143 -4.228889 -4.2472949 -4.2650595 -4.2812419 -4.2853847 -4.2806549 -4.2789254][-4.2056537 -4.1841183 -4.1816888 -4.18897 -4.1887164 -4.1642494 -4.1258736 -4.1176186 -4.1422205 -4.1767025 -4.2122211 -4.2413983 -4.2597556 -4.2595239 -4.2598672][-4.1727123 -4.138835 -4.1234322 -4.1175103 -4.1048446 -4.0678291 -4.0239429 -4.0131488 -4.0423379 -4.0922132 -4.1504612 -4.1965852 -4.2302742 -4.2346325 -4.2386451][-4.1359444 -4.0897341 -4.0587373 -4.031714 -3.9948189 -3.9408588 -3.8995461 -3.8964992 -3.9323118 -3.9982071 -4.0747037 -4.1331677 -4.176837 -4.187314 -4.2031159][-4.12645 -4.0723057 -4.0266695 -3.9727447 -3.8982751 -3.8222835 -3.7873502 -3.8021107 -3.8508914 -3.9327877 -4.0205493 -4.0875506 -4.1350479 -4.150722 -4.1755929][-4.147923 -4.102972 -4.065289 -4.0111885 -3.9241424 -3.8351855 -3.7938037 -3.8042696 -3.8468084 -3.9220328 -3.9974635 -4.0575342 -4.1014876 -4.1218762 -4.1575909][-4.161572 -4.13028 -4.1130629 -4.0838184 -4.0239129 -3.954776 -3.9123447 -3.9065449 -3.9220383 -3.9698207 -4.0220342 -4.0671334 -4.0979266 -4.1187186 -4.15755][-4.1583171 -4.1326756 -4.1237779 -4.1128631 -4.0804858 -4.034081 -4.0024276 -3.9939098 -3.9967589 -4.0264711 -4.0629597 -4.0964422 -4.1191106 -4.1398773 -4.1746283][-4.1613221 -4.1320205 -4.1195183 -4.1093268 -4.0895228 -4.0575342 -4.0356078 -4.03183 -4.0341454 -4.0570192 -4.086421 -4.1163855 -4.1407952 -4.1637816 -4.1945734][-4.1957335 -4.1698403 -4.1561537 -4.1444426 -4.1288967 -4.1036067 -4.084929 -4.0806351 -4.0824842 -4.0999031 -4.12498 -4.1534266 -4.1785507 -4.201067 -4.2261333][-4.2547469 -4.2406344 -4.2332568 -4.2259774 -4.21349 -4.1947813 -4.1821489 -4.1793575 -4.1806903 -4.191144 -4.2096138 -4.2319407 -4.2498612 -4.2624574 -4.2754388][-4.2995548 -4.2940087 -4.2928195 -4.2904363 -4.28451 -4.2771053 -4.2731848 -4.2727575 -4.2729988 -4.2778978 -4.2885742 -4.3020072 -4.3114114 -4.3156409 -4.3185773]]...]
INFO - root - 2017-12-05 21:29:10.549117: step 42210, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 73h:17m:25s remains)
INFO - root - 2017-12-05 21:29:19.560693: step 42220, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 72h:17m:09s remains)
INFO - root - 2017-12-05 21:29:28.732506: step 42230, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 74h:20m:31s remains)
INFO - root - 2017-12-05 21:29:37.912706: step 42240, loss = 2.02, batch loss = 1.96 (8.7 examples/sec; 0.917 sec/batch; 73h:57m:35s remains)
INFO - root - 2017-12-05 21:29:46.794502: step 42250, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 70h:14m:14s remains)
INFO - root - 2017-12-05 21:29:55.947660: step 42260, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 70h:23m:30s remains)
INFO - root - 2017-12-05 21:30:05.222165: step 42270, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 74h:04m:21s remains)
INFO - root - 2017-12-05 21:30:14.445416: step 42280, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 73h:06m:11s remains)
INFO - root - 2017-12-05 21:30:23.537181: step 42290, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 74h:03m:02s remains)
INFO - root - 2017-12-05 21:30:32.732260: step 42300, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.939 sec/batch; 75h:43m:44s remains)
2017-12-05 21:30:33.500754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2706361 -4.2738109 -4.2813869 -4.2897434 -4.2953887 -4.2951035 -4.2881122 -4.2821875 -4.2828479 -4.2844443 -4.2949572 -4.3098903 -4.3176823 -4.3250203 -4.3356333][-4.2452369 -4.2484956 -4.2563157 -4.2664351 -4.2723069 -4.2679882 -4.2550936 -4.2460995 -4.2479172 -4.250905 -4.2649574 -4.2873592 -4.2996473 -4.3090329 -4.3220077][-4.2350264 -4.24018 -4.2446685 -4.2510362 -4.2543716 -4.2461834 -4.2274489 -4.2190986 -4.2266531 -4.234561 -4.2504358 -4.2737207 -4.2879033 -4.29757 -4.3089581][-4.2210727 -4.2334023 -4.2372046 -4.2414732 -4.2446442 -4.2332921 -4.2116251 -4.2056923 -4.2219868 -4.2408242 -4.26029 -4.2800107 -4.2923455 -4.2991385 -4.3068018][-4.1873207 -4.2056136 -4.2147622 -4.2207851 -4.2245994 -4.2138109 -4.1942668 -4.1884189 -4.2046194 -4.2305746 -4.2548327 -4.2763867 -4.2918425 -4.3026657 -4.3118148][-4.1588283 -4.1743097 -4.1850357 -4.1940117 -4.1995516 -4.1929979 -4.1829963 -4.1757693 -4.1880159 -4.2147241 -4.2422209 -4.267766 -4.289371 -4.3080778 -4.3204732][-4.1538086 -4.1610537 -4.1672397 -4.1688833 -4.1664014 -4.1599584 -4.1619473 -4.1658592 -4.185185 -4.2157812 -4.2468095 -4.2773123 -4.3026633 -4.3223486 -4.33087][-4.1671114 -4.1615157 -4.1536193 -4.1354222 -4.1060653 -4.0850506 -4.089025 -4.1056066 -4.1421638 -4.1859188 -4.2276874 -4.2690659 -4.301167 -4.3200784 -4.3252583][-4.1879134 -4.1694584 -4.1491041 -4.1179285 -4.070713 -4.0362844 -4.0354457 -4.053277 -4.0977826 -4.1494412 -4.1980796 -4.2457204 -4.2820578 -4.3035145 -4.3106885][-4.2065744 -4.1856923 -4.1675243 -4.1481051 -4.1193633 -4.1013994 -4.1004252 -4.1064863 -4.1390781 -4.1773539 -4.2133751 -4.2516284 -4.2792568 -4.2954063 -4.2990761][-4.2012119 -4.1899962 -4.1830821 -4.183145 -4.1810918 -4.1829867 -4.1802731 -4.1733847 -4.1937609 -4.2199669 -4.2441435 -4.2731929 -4.2908025 -4.2994294 -4.2996869][-4.1888747 -4.1909189 -4.1956005 -4.2052407 -4.2173319 -4.2306557 -4.228776 -4.2190652 -4.2318478 -4.2495918 -4.2647047 -4.2866368 -4.2965274 -4.3005533 -4.3017716][-4.2132769 -4.2247763 -4.23321 -4.2418947 -4.2534518 -4.2671766 -4.2656107 -4.2567453 -4.2654738 -4.2773318 -4.2871079 -4.3017511 -4.3033743 -4.3014297 -4.3018756][-4.2657247 -4.2782106 -4.2842965 -4.2873635 -4.2889543 -4.2934041 -4.2874947 -4.2786889 -4.2825737 -4.2907748 -4.3028517 -4.3156857 -4.3154197 -4.3109345 -4.3077197][-4.3096323 -4.3152318 -4.3149796 -4.3112497 -4.3048568 -4.3001714 -4.28988 -4.2817287 -4.2832184 -4.2916193 -4.30476 -4.3166804 -4.32075 -4.3191051 -4.3138351]]...]
INFO - root - 2017-12-05 21:30:42.589987: step 42310, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 70h:37m:47s remains)
INFO - root - 2017-12-05 21:30:51.663934: step 42320, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 73h:47m:32s remains)
INFO - root - 2017-12-05 21:31:00.892664: step 42330, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 76h:39m:35s remains)
INFO - root - 2017-12-05 21:31:10.006819: step 42340, loss = 2.07, batch loss = 2.02 (8.3 examples/sec; 0.962 sec/batch; 77h:31m:30s remains)
INFO - root - 2017-12-05 21:31:19.041491: step 42350, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 74h:12m:12s remains)
INFO - root - 2017-12-05 21:31:28.198580: step 42360, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 73h:11m:44s remains)
INFO - root - 2017-12-05 21:31:37.292059: step 42370, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 73h:29m:24s remains)
INFO - root - 2017-12-05 21:31:46.455250: step 42380, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 73h:32m:07s remains)
INFO - root - 2017-12-05 21:31:55.608179: step 42390, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 74h:12m:33s remains)
INFO - root - 2017-12-05 21:32:04.655027: step 42400, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 73h:56m:23s remains)
2017-12-05 21:32:05.454083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3105578 -4.312078 -4.3165622 -4.3220167 -4.3262572 -4.3285589 -4.3316503 -4.3337827 -4.3340263 -4.334497 -4.335556 -4.3361254 -4.33669 -4.3376064 -4.3382053][-4.2808285 -4.2788277 -4.2824831 -4.2876062 -4.2924566 -4.2973166 -4.3033571 -4.3066764 -4.3055034 -4.3051434 -4.3067794 -4.309814 -4.3155375 -4.3218632 -4.3264475][-4.2398014 -4.2318654 -4.2311707 -4.2345376 -4.2394972 -4.2453055 -4.2528162 -4.2545319 -4.250041 -4.2518358 -4.2570539 -4.2653208 -4.2803278 -4.2947097 -4.3053517][-4.2106204 -4.1956773 -4.1886983 -4.1850324 -4.1826763 -4.1808295 -4.1815829 -4.1769872 -4.1712537 -4.1856408 -4.2019262 -4.2210493 -4.2467623 -4.2704 -4.2867837][-4.1981306 -4.1780663 -4.1598883 -4.1421189 -4.1261458 -4.1136312 -4.1100149 -4.1041059 -4.0982704 -4.1255689 -4.1560497 -4.1868811 -4.2207527 -4.2500529 -4.2693367][-4.1876841 -4.1584711 -4.1236343 -4.0887232 -4.0592442 -4.0415182 -4.0443568 -4.0418983 -4.03002 -4.06614 -4.1095471 -4.1513615 -4.1948962 -4.2307038 -4.2540298][-4.1693583 -4.1289711 -4.0791359 -4.0291667 -3.9899163 -3.9720449 -3.9793689 -3.9725606 -3.9546196 -3.9975522 -4.0563931 -4.1132727 -4.1702232 -4.2177362 -4.2471809][-4.1522856 -4.0983782 -4.0362115 -3.9799495 -3.938771 -3.9262905 -3.9412665 -3.9335923 -3.9137993 -3.9623618 -4.0270462 -4.0924215 -4.1553421 -4.2079105 -4.2415605][-4.1409349 -4.0793982 -4.0154958 -3.957581 -3.9136322 -3.9027336 -3.9175031 -3.9052086 -3.8810296 -3.9263418 -3.99117 -4.0654464 -4.1346493 -4.1914291 -4.230649][-4.1409945 -4.0882921 -4.0412412 -3.9977343 -3.9587746 -3.9428296 -3.9428544 -3.9233005 -3.8983421 -3.9289854 -3.9853842 -4.0599666 -4.1265407 -4.1794887 -4.2199211][-4.1672688 -4.1218119 -4.0872645 -4.0607924 -4.0323687 -4.0175805 -4.0119781 -3.9957459 -3.9815054 -4.0033054 -4.0455494 -4.1085553 -4.1608186 -4.2012877 -4.2361979][-4.2146711 -4.1694565 -4.1361165 -4.117497 -4.1005249 -4.0938463 -4.09356 -4.0874805 -4.0853386 -4.1043816 -4.136292 -4.1847348 -4.2207041 -4.2474356 -4.2719388][-4.2675204 -4.2265148 -4.1938629 -4.1816459 -4.1762576 -4.1772985 -4.1853662 -4.1899247 -4.197216 -4.2127776 -4.2340894 -4.263155 -4.2814164 -4.2947803 -4.3087873][-4.3132725 -4.2865515 -4.2642474 -4.2580876 -4.2582655 -4.26329 -4.2738667 -4.2823792 -4.2905688 -4.2999167 -4.3117309 -4.3252897 -4.3328261 -4.3371429 -4.3427634][-4.339591 -4.3291569 -4.3199439 -4.3182459 -4.3187079 -4.3223238 -4.3288522 -4.3333364 -4.3364544 -4.3399868 -4.3461423 -4.353209 -4.3565431 -4.3576021 -4.3591652]]...]
INFO - root - 2017-12-05 21:32:14.608757: step 42410, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 72h:46m:28s remains)
INFO - root - 2017-12-05 21:32:23.891938: step 42420, loss = 2.02, batch loss = 1.96 (8.5 examples/sec; 0.945 sec/batch; 76h:08m:15s remains)
INFO - root - 2017-12-05 21:32:33.161383: step 42430, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 72h:11m:26s remains)
INFO - root - 2017-12-05 21:32:42.097649: step 42440, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.894 sec/batch; 72h:03m:36s remains)
INFO - root - 2017-12-05 21:32:51.117468: step 42450, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 66h:03m:56s remains)
INFO - root - 2017-12-05 21:33:00.309549: step 42460, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 70h:44m:37s remains)
INFO - root - 2017-12-05 21:33:09.477201: step 42470, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 69h:57m:55s remains)
INFO - root - 2017-12-05 21:33:18.611758: step 42480, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 74h:40m:03s remains)
INFO - root - 2017-12-05 21:33:27.669269: step 42490, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 71h:27m:09s remains)
INFO - root - 2017-12-05 21:33:36.667235: step 42500, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 69h:16m:04s remains)
2017-12-05 21:33:37.496997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1613 -4.1611691 -4.1559339 -4.1423454 -4.1353021 -4.1341643 -4.1422505 -4.1533089 -4.1529932 -4.1363125 -4.1201491 -4.1115975 -4.1151414 -4.1338954 -4.1628809][-4.1613903 -4.1612215 -4.1537433 -4.1385489 -4.132844 -4.134932 -4.1480727 -4.1662879 -4.173203 -4.1571932 -4.1389008 -4.12277 -4.1139059 -4.1217217 -4.144773][-4.1775942 -4.1732073 -4.1642427 -4.1535821 -4.1505532 -4.1554952 -4.1704187 -4.1883607 -4.1948705 -4.1779184 -4.1631789 -4.1508675 -4.1355519 -4.1296935 -4.1378455][-4.1905251 -4.1829004 -4.1730952 -4.1671162 -4.1660905 -4.1739721 -4.1899104 -4.2102871 -4.2182026 -4.2002292 -4.1840644 -4.1728234 -4.1558065 -4.14116 -4.1362658][-4.188808 -4.1774797 -4.1649795 -4.1598487 -4.1575274 -4.1633015 -4.1803904 -4.2067413 -4.2184668 -4.2016439 -4.1804094 -4.1653447 -4.1503825 -4.1365132 -4.1283307][-4.1793284 -4.1606841 -4.1431427 -4.1355605 -4.1294074 -4.1300159 -4.1457963 -4.1712747 -4.1828632 -4.1650615 -4.1400905 -4.1239667 -4.1198015 -4.1213484 -4.1229458][-4.1825428 -4.1588349 -4.1366577 -4.1281424 -4.1240764 -4.1192064 -4.124999 -4.1413956 -4.1474805 -4.1272964 -4.1038847 -4.0951343 -4.1053247 -4.1252747 -4.1402059][-4.2105265 -4.1914072 -4.1711063 -4.1632991 -4.1592407 -4.1500244 -4.1439862 -4.1457958 -4.1419153 -4.1209183 -4.1046348 -4.1055007 -4.1237674 -4.1499772 -4.1666493][-4.2443218 -4.2345362 -4.2211676 -4.2122574 -4.2051625 -4.1955853 -4.1849623 -4.17731 -4.1670589 -4.1478214 -4.1380749 -4.1438 -4.1601133 -4.1825032 -4.19481][-4.275691 -4.2752652 -4.2671585 -4.258791 -4.250329 -4.2416062 -4.2312222 -4.2199903 -4.2106104 -4.1982288 -4.1954026 -4.2026415 -4.21121 -4.22181 -4.2252626][-4.2949915 -4.2998748 -4.2945971 -4.28641 -4.2777634 -4.268539 -4.258388 -4.2484155 -4.2442279 -4.2413754 -4.2450824 -4.251791 -4.254252 -4.2538462 -4.2477922][-4.2941394 -4.30006 -4.2982368 -4.2931952 -4.28699 -4.2799516 -4.2726021 -4.2663646 -4.2655263 -4.2662883 -4.271553 -4.2773652 -4.2750254 -4.2673554 -4.2570343][-4.2803655 -4.2833271 -4.2809081 -4.2795644 -4.2782359 -4.2755823 -4.2710609 -4.2668395 -4.2668796 -4.269577 -4.2767215 -4.2846546 -4.2824821 -4.27246 -4.2616391][-4.2661963 -4.266799 -4.2640586 -4.2643414 -4.2662239 -4.2660117 -4.2616272 -4.2566304 -4.2556987 -4.25889 -4.2658925 -4.2748284 -4.27654 -4.2705212 -4.2633905][-4.2588415 -4.25683 -4.25339 -4.2528186 -4.254477 -4.2549505 -4.2520552 -4.248148 -4.2471509 -4.2495995 -4.2550364 -4.2628293 -4.266891 -4.2663736 -4.2644043]]...]
INFO - root - 2017-12-05 21:33:46.550121: step 42510, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 73h:22m:08s remains)
INFO - root - 2017-12-05 21:33:55.689371: step 42520, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 70h:56m:17s remains)
INFO - root - 2017-12-05 21:34:04.600437: step 42530, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 73h:20m:10s remains)
INFO - root - 2017-12-05 21:34:13.734038: step 42540, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 73h:16m:13s remains)
INFO - root - 2017-12-05 21:34:22.767159: step 42550, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 73h:25m:58s remains)
INFO - root - 2017-12-05 21:34:31.749332: step 42560, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 73h:42m:00s remains)
INFO - root - 2017-12-05 21:34:41.018814: step 42570, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 73h:20m:32s remains)
INFO - root - 2017-12-05 21:34:50.169338: step 42580, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 73h:32m:25s remains)
INFO - root - 2017-12-05 21:34:59.074325: step 42590, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 70h:56m:08s remains)
INFO - root - 2017-12-05 21:35:08.330731: step 42600, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 73h:52m:08s remains)
2017-12-05 21:35:09.171638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3199139 -4.3077474 -4.299952 -4.2927351 -4.2871494 -4.2844892 -4.2871323 -4.2945433 -4.3041282 -4.3118567 -4.313386 -4.3116 -4.309093 -4.300498 -4.2966995][-4.3055849 -4.2912989 -4.2812185 -4.2683768 -4.2576828 -4.2520437 -4.2508841 -4.2557473 -4.2641068 -4.2713695 -4.2713647 -4.2679873 -4.2662411 -4.2584329 -4.2577882][-4.291707 -4.2744675 -4.2613716 -4.2442646 -4.228755 -4.2168427 -4.2076392 -4.2073727 -4.2152276 -4.2236366 -4.2249107 -4.2202148 -4.2201223 -4.2148352 -4.219583][-4.2816148 -4.2598729 -4.2452059 -4.2264576 -4.2096462 -4.1907544 -4.170536 -4.1644688 -4.17202 -4.1839223 -4.1943517 -4.1936936 -4.1959014 -4.1967831 -4.2054782][-4.2735586 -4.2462907 -4.23203 -4.21635 -4.1997786 -4.1769428 -4.148253 -4.1353345 -4.1390071 -4.1542006 -4.1771584 -4.1807184 -4.1829209 -4.1861224 -4.1945314][-4.2662358 -4.23548 -4.2225928 -4.2108545 -4.195828 -4.17161 -4.1370134 -4.1176167 -4.1179252 -4.1328368 -4.1623898 -4.1692 -4.1708274 -4.1730113 -4.1809468][-4.2647071 -4.2335925 -4.2196507 -4.2046595 -4.1855721 -4.1603703 -4.1279473 -4.11287 -4.1165252 -4.1367221 -4.1722145 -4.1816034 -4.1833282 -4.182096 -4.1884017][-4.2709942 -4.2404613 -4.2218094 -4.1995883 -4.1755953 -4.149579 -4.1217003 -4.1116257 -4.1229835 -4.1476855 -4.1887131 -4.2016878 -4.2048163 -4.2040224 -4.2112117][-4.2804365 -4.251606 -4.2273297 -4.1981049 -4.1701546 -4.1437135 -4.1166425 -4.1045976 -4.1147327 -4.1408997 -4.1876764 -4.2060757 -4.2129788 -4.2177477 -4.2296529][-4.2869534 -4.2584391 -4.2297707 -4.1958652 -4.1658034 -4.1424685 -4.1166644 -4.0992742 -4.1038351 -4.1297822 -4.1819448 -4.208292 -4.2180834 -4.2268791 -4.2448158][-4.2917047 -4.2628326 -4.2322922 -4.1975994 -4.1700444 -4.1539965 -4.1305766 -4.1084576 -4.1037188 -4.125309 -4.1757612 -4.2057047 -4.2185931 -4.2305546 -4.2525477][-4.2972183 -4.26891 -4.2391071 -4.208014 -4.1846685 -4.1720929 -4.1523147 -4.1270137 -4.1147876 -4.130127 -4.1734791 -4.2034655 -4.2183871 -4.2319918 -4.2536044][-4.3074489 -4.2822647 -4.2568355 -4.2317986 -4.2125196 -4.2007966 -4.1851263 -4.1635156 -4.1511111 -4.162365 -4.1973691 -4.2244649 -4.2381458 -4.2478771 -4.2621212][-4.3187122 -4.29728 -4.2765794 -4.25702 -4.2411566 -4.230144 -4.2183542 -4.2020383 -4.1909857 -4.196187 -4.2201872 -4.2417059 -4.2539949 -4.26011 -4.2681279][-4.3308959 -4.3140054 -4.2989779 -4.28691 -4.277782 -4.2724175 -4.2655082 -4.2533121 -4.2430353 -4.2434669 -4.2573123 -4.2724657 -4.281992 -4.2845874 -4.2856541]]...]
INFO - root - 2017-12-05 21:35:18.416302: step 42610, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 74h:59m:02s remains)
INFO - root - 2017-12-05 21:35:27.501513: step 42620, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.816 sec/batch; 65h:43m:43s remains)
INFO - root - 2017-12-05 21:35:36.526611: step 42630, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 72h:45m:16s remains)
INFO - root - 2017-12-05 21:35:45.688092: step 42640, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 72h:09m:18s remains)
INFO - root - 2017-12-05 21:35:54.909514: step 42650, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 74h:06m:46s remains)
INFO - root - 2017-12-05 21:36:03.927178: step 42660, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 71h:54m:11s remains)
INFO - root - 2017-12-05 21:36:12.945198: step 42670, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 70h:14m:12s remains)
INFO - root - 2017-12-05 21:36:21.936180: step 42680, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 71h:17m:37s remains)
INFO - root - 2017-12-05 21:36:31.223248: step 42690, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 73h:56m:38s remains)
INFO - root - 2017-12-05 21:36:40.419790: step 42700, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 74h:20m:19s remains)
2017-12-05 21:36:41.171646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2394414 -4.2499466 -4.255064 -4.2614379 -4.2654939 -4.267818 -4.2681842 -4.2676473 -4.2640653 -4.2534933 -4.2428093 -4.2361622 -4.2328873 -4.2223787 -4.2013583][-4.2165461 -4.2258368 -4.2323995 -4.2423272 -4.2503395 -4.2567582 -4.2592325 -4.2596512 -4.2560692 -4.2426519 -4.2298527 -4.2263389 -4.2249479 -4.2117748 -4.1816978][-4.2160645 -4.22257 -4.2256365 -4.23595 -4.2437692 -4.2535405 -4.2615728 -4.2666183 -4.2667537 -4.2541928 -4.2419829 -4.2421813 -4.2396178 -4.22081 -4.182631][-4.224607 -4.2274427 -4.2279744 -4.2386637 -4.2452574 -4.2550831 -4.2653031 -4.2754993 -4.2808895 -4.2718091 -4.2605958 -4.2625203 -4.2587838 -4.2389426 -4.1998239][-4.2348156 -4.2316914 -4.2289734 -4.2372923 -4.238843 -4.2446141 -4.2548294 -4.2675672 -4.2758079 -4.2680435 -4.2598581 -4.26342 -4.2611828 -4.2465096 -4.2138119][-4.2282615 -4.2160974 -4.2041941 -4.2058897 -4.2028041 -4.2030725 -4.2094417 -4.2238727 -4.2362051 -4.2332764 -4.232872 -4.2411351 -4.2445025 -4.23843 -4.2157674][-4.2108808 -4.1861863 -4.1592736 -4.1484871 -4.1383495 -4.1289444 -4.1252332 -4.1368313 -4.157618 -4.1667647 -4.1778026 -4.1945972 -4.2083478 -4.2127852 -4.2015462][-4.1945853 -4.1581364 -4.1159515 -4.0876822 -4.0577106 -4.0283566 -4.0080318 -4.0168858 -4.049686 -4.072804 -4.0962868 -4.1226277 -4.1494017 -4.1683927 -4.170167][-4.1990356 -4.1625261 -4.1178284 -4.082293 -4.032321 -3.9728591 -3.9244938 -3.9205441 -3.9507537 -3.9796188 -4.0111 -4.050488 -4.0934191 -4.1263185 -4.1412458][-4.2274413 -4.2011003 -4.1700377 -4.1447816 -4.0976372 -4.0379529 -3.9893758 -3.9747505 -3.982944 -3.99523 -4.0177431 -4.0558639 -4.1018581 -4.1364594 -4.1528554][-4.262413 -4.2530541 -4.2400017 -4.230073 -4.1987028 -4.1616936 -4.1333618 -4.1213045 -4.1169395 -4.1169152 -4.1263967 -4.1472263 -4.1747208 -4.1935792 -4.1973][-4.288414 -4.2905831 -4.287818 -4.287117 -4.2688107 -4.2473125 -4.2315378 -4.223268 -4.2201753 -4.2220578 -4.2279034 -4.2367883 -4.2480774 -4.2517266 -4.2403288][-4.3013725 -4.3100891 -4.3136778 -4.3178349 -4.3091922 -4.2974372 -4.2869329 -4.2819123 -4.2813845 -4.2828293 -4.2834935 -4.2845979 -4.28601 -4.2795329 -4.2576203][-4.2954006 -4.3083076 -4.3151894 -4.3206258 -4.315578 -4.3104839 -4.3045268 -4.3000827 -4.2994342 -4.298306 -4.293344 -4.2900805 -4.286314 -4.272387 -4.2414446][-4.2696252 -4.2842155 -4.2931943 -4.2989006 -4.29725 -4.3002596 -4.3019319 -4.3000331 -4.2983122 -4.295053 -4.2887106 -4.2817864 -4.2702208 -4.245626 -4.20566]]...]
INFO - root - 2017-12-05 21:36:50.273853: step 42710, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 75h:05m:25s remains)
INFO - root - 2017-12-05 21:36:59.349000: step 42720, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 73h:14m:47s remains)
INFO - root - 2017-12-05 21:37:08.418689: step 42730, loss = 2.02, batch loss = 1.96 (8.2 examples/sec; 0.980 sec/batch; 78h:51m:29s remains)
INFO - root - 2017-12-05 21:37:17.752957: step 42740, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 76h:03m:50s remains)
INFO - root - 2017-12-05 21:37:26.881844: step 42750, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.925 sec/batch; 74h:24m:52s remains)
INFO - root - 2017-12-05 21:37:35.916056: step 42760, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 71h:00m:20s remains)
INFO - root - 2017-12-05 21:37:44.926004: step 42770, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 73h:47m:14s remains)
INFO - root - 2017-12-05 21:37:54.072379: step 42780, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 71h:57m:28s remains)
INFO - root - 2017-12-05 21:38:03.191810: step 42790, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 73h:06m:15s remains)
INFO - root - 2017-12-05 21:38:12.202151: step 42800, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 72h:03m:56s remains)
2017-12-05 21:38:13.052433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2812562 -4.278028 -4.2803493 -4.2819543 -4.27972 -4.2809715 -4.2856188 -4.2908015 -4.292129 -4.2924018 -4.2924705 -4.2922611 -4.291492 -4.2929173 -4.2930079][-4.261569 -4.262691 -4.2703342 -4.2760472 -4.2748661 -4.2761726 -4.280849 -4.2865562 -4.2876654 -4.287972 -4.2881079 -4.2864933 -4.2829928 -4.2812138 -4.2776403][-4.24438 -4.2477927 -4.2589231 -4.2685375 -4.2668962 -4.2644625 -4.2645693 -4.2682977 -4.2684779 -4.2688103 -4.2701139 -4.2690868 -4.265552 -4.2639418 -4.26117][-4.2298555 -4.22997 -4.2392263 -4.2462864 -4.2395282 -4.229784 -4.2217221 -4.2231765 -4.2241244 -4.2267437 -4.2328072 -4.2356153 -4.2363272 -4.2386513 -4.2409067][-4.2163682 -4.2105317 -4.2163086 -4.2186518 -4.2052846 -4.1898913 -4.1760159 -4.1773434 -4.1808767 -4.184607 -4.1938806 -4.2010078 -4.20885 -4.2159324 -4.2234054][-4.2053337 -4.1940289 -4.1913414 -4.1838989 -4.161304 -4.1394567 -4.1239166 -4.1288815 -4.1371994 -4.1416745 -4.1511211 -4.1602569 -4.1716723 -4.1832137 -4.1958661][-4.1878204 -4.1660614 -4.1505818 -4.1316805 -4.0985489 -4.0684843 -4.0496154 -4.0555606 -4.0683393 -4.0768247 -4.0905452 -4.1039748 -4.1215339 -4.1379061 -4.1536465][-4.149415 -4.1110125 -4.0785942 -4.0441389 -3.9942369 -3.9506972 -3.9295893 -3.9447596 -3.9730995 -3.9907391 -4.01445 -4.0355849 -4.0616231 -4.08461 -4.1043692][-4.1261916 -4.0822973 -4.0438271 -4.0053444 -3.9540102 -3.9092591 -3.8934507 -3.9173927 -3.9542935 -3.9740157 -3.9990063 -4.0189209 -4.0427785 -4.0641184 -4.08337][-4.1470618 -4.1175365 -4.0934815 -4.0676231 -4.0309949 -3.9991834 -3.9918134 -4.0107012 -4.0387917 -4.0523624 -4.0693331 -4.0836658 -4.1001534 -4.1160975 -4.1302438][-4.1876073 -4.1723676 -4.1643558 -4.1524029 -4.128098 -4.1065116 -4.1021876 -4.1104617 -4.1289864 -4.1382332 -4.1505461 -4.1566687 -4.1640129 -4.1735039 -4.1837416][-4.2318683 -4.2255621 -4.2278309 -4.2233415 -4.2043276 -4.1866384 -4.1797676 -4.1792789 -4.19106 -4.200129 -4.2061481 -4.2053032 -4.2104964 -4.2227721 -4.2332006][-4.2732849 -4.2734094 -4.2797303 -4.277288 -4.2614017 -4.2490993 -4.2434416 -4.2399244 -4.2491841 -4.2593141 -4.2624331 -4.2601175 -4.2661829 -4.2797413 -4.2865696][-4.303967 -4.3076577 -4.315763 -4.3182454 -4.3102126 -4.3035831 -4.3006144 -4.2969637 -4.3031554 -4.3119092 -4.3155503 -4.3146229 -4.3198333 -4.3303318 -4.3320179][-4.3190403 -4.3225522 -4.329298 -4.3347287 -4.3324733 -4.3310146 -4.3305378 -4.3297191 -4.333497 -4.3385549 -4.3406444 -4.3405132 -4.3430796 -4.3495078 -4.3502688]]...]
INFO - root - 2017-12-05 21:38:22.029060: step 42810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 74h:45m:47s remains)
INFO - root - 2017-12-05 21:38:31.223856: step 42820, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 71h:09m:26s remains)
INFO - root - 2017-12-05 21:38:40.295611: step 42830, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 71h:53m:03s remains)
INFO - root - 2017-12-05 21:38:49.377142: step 42840, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 71h:14m:04s remains)
INFO - root - 2017-12-05 21:38:58.530057: step 42850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 73h:13m:25s remains)
INFO - root - 2017-12-05 21:39:07.532155: step 42860, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 73h:48m:10s remains)
INFO - root - 2017-12-05 21:39:16.643152: step 42870, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 73h:17m:09s remains)
INFO - root - 2017-12-05 21:39:25.839453: step 42880, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.978 sec/batch; 78h:41m:09s remains)
INFO - root - 2017-12-05 21:39:35.029685: step 42890, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 72h:09m:46s remains)
INFO - root - 2017-12-05 21:39:44.087339: step 42900, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 73h:18m:48s remains)
2017-12-05 21:39:44.866021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3126407 -4.30066 -4.2905574 -4.2952633 -4.3039503 -4.3124485 -4.3241262 -4.32868 -4.3250628 -4.3169889 -4.3048396 -4.2955379 -4.2971034 -4.3076558 -4.3172731][-4.29647 -4.2744179 -4.2583308 -4.2623596 -4.2800374 -4.30083 -4.319324 -4.3270679 -4.3214836 -4.3086729 -4.2937064 -4.2836485 -4.2856221 -4.2982483 -4.3100867][-4.2582984 -4.23407 -4.2163415 -4.2198009 -4.2416162 -4.2683764 -4.2857265 -4.2892504 -4.2823834 -4.2673192 -4.2493682 -4.2384248 -4.2431979 -4.2613311 -4.2808123][-4.2002673 -4.1758237 -4.1648808 -4.1774931 -4.2064018 -4.2307496 -4.237381 -4.2285972 -4.2213778 -4.2132978 -4.2012649 -4.1963105 -4.2086496 -4.2325959 -4.2560434][-4.1241641 -4.1057611 -4.1120534 -4.1421318 -4.1744509 -4.1837573 -4.1668396 -4.1462455 -4.1447906 -4.1523733 -4.1581283 -4.1701851 -4.1940193 -4.2206583 -4.2406754][-4.0448189 -4.035171 -4.0591893 -4.0992742 -4.1259074 -4.10834 -4.056807 -4.0267897 -4.0455918 -4.0826168 -4.1164174 -4.150919 -4.189538 -4.2201815 -4.2367373][-3.9995084 -3.9926143 -4.0197706 -4.0568275 -4.0662217 -4.0113168 -3.9203146 -3.888509 -3.9455867 -4.0207186 -4.0820551 -4.1327314 -4.1776609 -4.2084532 -4.2234654][-4.0220003 -4.0103288 -4.0235434 -4.0392847 -4.0260162 -3.9549112 -3.8604074 -3.8414037 -3.919045 -4.0066719 -4.0725069 -4.1229739 -4.1610594 -4.1861496 -4.2000155][-4.1034794 -4.087254 -4.0846963 -4.0788403 -4.0679932 -4.0296173 -3.9775505 -3.963223 -4.0062423 -4.0632081 -4.1080813 -4.1438766 -4.1724353 -4.1927361 -4.2074718][-4.1682329 -4.1536388 -4.1500921 -4.145638 -4.1518483 -4.152441 -4.133718 -4.1172948 -4.1221256 -4.1411171 -4.1627927 -4.187429 -4.2102585 -4.2283573 -4.2436266][-4.1854172 -4.1784849 -4.1848726 -4.1944857 -4.2133889 -4.2311649 -4.230196 -4.2174449 -4.2055063 -4.2021093 -4.2082825 -4.22773 -4.251152 -4.2702694 -4.2825308][-4.1829495 -4.1842203 -4.2032375 -4.2267427 -4.2509136 -4.2688127 -4.2714586 -4.2597189 -4.2435336 -4.2339549 -4.2355571 -4.2549663 -4.27599 -4.2926688 -4.3006592][-4.185267 -4.1937304 -4.2177792 -4.2443924 -4.2657108 -4.2792735 -4.2819815 -4.2712569 -4.2551193 -4.2463365 -4.2493114 -4.266242 -4.2811985 -4.2929573 -4.2973461][-4.2039409 -4.21557 -4.2357273 -4.2571726 -4.2738862 -4.2824016 -4.2830868 -4.2725248 -4.2569637 -4.2509623 -4.2559986 -4.2692652 -4.27817 -4.2834697 -4.28258][-4.2367024 -4.2450433 -4.2547426 -4.2669725 -4.279377 -4.2830729 -4.2799764 -4.2700911 -4.2592888 -4.2595072 -4.2659526 -4.2710371 -4.27006 -4.2687254 -4.2660241]]...]
INFO - root - 2017-12-05 21:39:53.973135: step 42910, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 73h:37m:46s remains)
INFO - root - 2017-12-05 21:40:02.969281: step 42920, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 72h:08m:11s remains)
INFO - root - 2017-12-05 21:40:12.017378: step 42930, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 71h:38m:12s remains)
INFO - root - 2017-12-05 21:40:21.057548: step 42940, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 72h:10m:30s remains)
INFO - root - 2017-12-05 21:40:30.072708: step 42950, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 72h:01m:35s remains)
INFO - root - 2017-12-05 21:40:39.112260: step 42960, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 74h:14m:03s remains)
INFO - root - 2017-12-05 21:40:48.239716: step 42970, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 74h:58m:45s remains)
INFO - root - 2017-12-05 21:40:57.266755: step 42980, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 73h:22m:59s remains)
INFO - root - 2017-12-05 21:41:06.478608: step 42990, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 75h:13m:15s remains)
INFO - root - 2017-12-05 21:41:15.591530: step 43000, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 75h:07m:22s remains)
2017-12-05 21:41:16.350319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3193073 -4.3104134 -4.2971826 -4.280436 -4.2649293 -4.25997 -4.26939 -4.2824373 -4.2815657 -4.2681675 -4.2469525 -4.2158222 -4.1810665 -4.1622429 -4.1846075][-4.3055372 -4.2953258 -4.2811446 -4.2620854 -4.2406349 -4.2268839 -4.23132 -4.2436943 -4.24572 -4.2318411 -4.2070985 -4.1669645 -4.1225162 -4.1030092 -4.1391544][-4.2766495 -4.2671947 -4.2577062 -4.2435508 -4.2196217 -4.1995511 -4.1954131 -4.2014136 -4.2045031 -4.1958041 -4.1771131 -4.1378603 -4.0939364 -4.0818195 -4.1305914][-4.2455711 -4.2413993 -4.2356334 -4.2219629 -4.193841 -4.1657581 -4.1454964 -4.1375542 -4.1466908 -4.1557593 -4.1547976 -4.1280551 -4.0945106 -4.0971231 -4.1560807][-4.2241044 -4.22172 -4.21126 -4.1909513 -4.1558294 -4.1136947 -4.0659695 -4.0310073 -4.0500093 -4.0930452 -4.1193962 -4.116467 -4.1026278 -4.1244111 -4.1905951][-4.2071762 -4.2063322 -4.19163 -4.1669264 -4.1260166 -4.0665579 -3.9791934 -3.8973293 -3.9162769 -3.9968457 -4.0586371 -4.0885677 -4.1059241 -4.1507525 -4.2203627][-4.1938634 -4.1912174 -4.17686 -4.1528726 -4.1113672 -4.041389 -3.9250045 -3.7987697 -3.8090596 -3.91774 -4.0084505 -4.0675931 -4.1140995 -4.1742849 -4.241734][-4.2023797 -4.19349 -4.1787882 -4.1606345 -4.1291561 -4.0712118 -3.9728284 -3.8658583 -3.8697166 -3.9556108 -4.0357237 -4.096909 -4.1501694 -4.2070093 -4.2613869][-4.2095957 -4.1940513 -4.177319 -4.1645284 -4.1455483 -4.111917 -4.0521188 -3.9915807 -4.0014162 -4.0530844 -4.1076283 -4.1551423 -4.1979604 -4.2409005 -4.2808824][-4.1965885 -4.1784267 -4.16213 -4.1561413 -4.1503253 -4.1353216 -4.1028171 -4.0784111 -4.0984354 -4.1352348 -4.1730504 -4.2085037 -4.2406058 -4.2712145 -4.3000479][-4.1684213 -4.1500592 -4.1381207 -4.1434879 -4.1507444 -4.1517262 -4.1411738 -4.1394506 -4.1659579 -4.1967516 -4.2267494 -4.2554164 -4.2792935 -4.2988858 -4.3169756][-4.1450191 -4.1316652 -4.1233516 -4.1359124 -4.1530814 -4.1691322 -4.1777563 -4.1890836 -4.2158422 -4.2428818 -4.2670903 -4.2888641 -4.3050404 -4.3169918 -4.328723][-4.1559186 -4.1500616 -4.1460776 -4.1608558 -4.1839886 -4.2072678 -4.2265372 -4.2433753 -4.2637386 -4.2819448 -4.2981086 -4.3119369 -4.3220558 -4.3295474 -4.3374348][-4.21026 -4.2101984 -4.2123966 -4.2252903 -4.2431669 -4.2612667 -4.2777348 -4.2909412 -4.3013287 -4.310678 -4.3198872 -4.3274288 -4.33338 -4.3386416 -4.3437033][-4.2750492 -4.2766328 -4.2791338 -4.2860284 -4.295548 -4.304709 -4.3131204 -4.3203859 -4.3257227 -4.3307576 -4.3355718 -4.3395805 -4.3429132 -4.3459873 -4.3485727]]...]
INFO - root - 2017-12-05 21:41:25.299026: step 43010, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.884 sec/batch; 71h:04m:42s remains)
INFO - root - 2017-12-05 21:41:34.448491: step 43020, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 71h:56m:54s remains)
INFO - root - 2017-12-05 21:41:43.407514: step 43030, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 70h:36m:56s remains)
INFO - root - 2017-12-05 21:41:52.537871: step 43040, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 74h:48m:40s remains)
INFO - root - 2017-12-05 21:42:01.524779: step 43050, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 71h:04m:22s remains)
INFO - root - 2017-12-05 21:42:10.713965: step 43060, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.912 sec/batch; 73h:18m:48s remains)
INFO - root - 2017-12-05 21:42:19.682016: step 43070, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 70h:50m:40s remains)
INFO - root - 2017-12-05 21:42:28.753708: step 43080, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 73h:58m:59s remains)
INFO - root - 2017-12-05 21:42:37.832014: step 43090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 69h:25m:46s remains)
INFO - root - 2017-12-05 21:42:46.969125: step 43100, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 73h:26m:02s remains)
2017-12-05 21:42:47.755777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0758777 -4.0795574 -4.0997729 -4.136373 -4.1622558 -4.1742549 -4.1749878 -4.1662536 -4.1650829 -4.1634164 -4.1664686 -4.1806474 -4.1905589 -4.1864781 -4.1788726][-4.087451 -4.0642085 -4.0589743 -4.0852408 -4.1162853 -4.1297417 -4.1322 -4.1381721 -4.1456733 -4.143641 -4.1448712 -4.1616325 -4.1782608 -4.1800423 -4.1754885][-4.1084805 -4.074595 -4.0422664 -4.0361061 -4.0502276 -4.0575838 -4.0649724 -4.0904179 -4.1120605 -4.1162643 -4.1228318 -4.1446676 -4.1620674 -4.1641507 -4.1644821][-4.1292224 -4.1041536 -4.0614572 -4.0236053 -3.9946012 -3.9749162 -3.9845657 -4.0348454 -4.0778155 -4.0936165 -4.110033 -4.1391454 -4.1612034 -4.1646361 -4.16242][-4.1396904 -4.1321321 -4.0969968 -4.0522704 -3.9956503 -3.9372902 -3.9311893 -3.9884932 -4.047524 -4.0759339 -4.100481 -4.1353512 -4.1615438 -4.17015 -4.1702375][-4.1414123 -4.1532707 -4.1392565 -4.1067076 -4.0479946 -3.9782057 -3.9484811 -3.9792254 -4.029336 -4.0598688 -4.0922632 -4.1342769 -4.1661439 -4.1835766 -4.1881118][-4.151063 -4.1700668 -4.1689849 -4.1513014 -4.1046524 -4.04695 -4.016027 -4.0191221 -4.0445709 -4.0666208 -4.1007757 -4.1447616 -4.1799359 -4.2020369 -4.2057233][-4.1757007 -4.19069 -4.1885433 -4.1713285 -4.1383138 -4.1006975 -4.075654 -4.0666094 -4.0755234 -4.0899515 -4.1242185 -4.1653214 -4.1965694 -4.2145839 -4.2139659][-4.1894069 -4.2014165 -4.1991463 -4.1771278 -4.1470013 -4.1228795 -4.10451 -4.0917873 -4.0901365 -4.0940652 -4.1288514 -4.1719308 -4.20111 -4.2149477 -4.2115097][-4.1869955 -4.2006512 -4.2013254 -4.1763768 -4.1455646 -4.1297383 -4.1166983 -4.1032577 -4.087647 -4.0720487 -4.0986352 -4.1477814 -4.1832957 -4.2000737 -4.1976142][-4.2019467 -4.2119308 -4.2091732 -4.1832108 -4.1548934 -4.1418033 -4.1302562 -4.1114526 -4.0783057 -4.0385933 -4.0451012 -4.0920949 -4.13699 -4.1657772 -4.1703234][-4.2311788 -4.2333503 -4.2234797 -4.1976643 -4.177011 -4.1635957 -4.1513572 -4.1287189 -4.0815377 -4.0278044 -4.0152445 -4.0497012 -4.0964785 -4.1329155 -4.1471567][-4.2529016 -4.25115 -4.2381949 -4.2131081 -4.19552 -4.1841817 -4.1760335 -4.1567163 -4.1089783 -4.0560474 -4.0388536 -4.0561304 -4.0896044 -4.1200871 -4.1361723][-4.2667818 -4.2615385 -4.2467985 -4.2229829 -4.2083068 -4.1968374 -4.1912923 -4.17719 -4.1387167 -4.0977902 -4.0842276 -4.092855 -4.1069465 -4.1250491 -4.1386585][-4.2651753 -4.2579341 -4.23808 -4.2130013 -4.201077 -4.1927009 -4.1895194 -4.180871 -4.1515169 -4.1246691 -4.1189518 -4.123045 -4.1263108 -4.1340733 -4.1461067]]...]
INFO - root - 2017-12-05 21:42:56.813988: step 43110, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 71h:11m:16s remains)
INFO - root - 2017-12-05 21:43:06.023697: step 43120, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 76h:12m:53s remains)
INFO - root - 2017-12-05 21:43:15.166092: step 43130, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 73h:09m:26s remains)
INFO - root - 2017-12-05 21:43:24.189872: step 43140, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.787 sec/batch; 63h:17m:50s remains)
INFO - root - 2017-12-05 21:43:33.226102: step 43150, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 73h:09m:39s remains)
INFO - root - 2017-12-05 21:43:42.300592: step 43160, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 71h:38m:27s remains)
INFO - root - 2017-12-05 21:43:51.408170: step 43170, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 73h:24m:34s remains)
INFO - root - 2017-12-05 21:44:00.518746: step 43180, loss = 2.08, batch loss = 2.03 (8.2 examples/sec; 0.970 sec/batch; 77h:57m:31s remains)
INFO - root - 2017-12-05 21:44:09.581580: step 43190, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 74h:09m:31s remains)
INFO - root - 2017-12-05 21:44:18.673965: step 43200, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 72h:22m:57s remains)
2017-12-05 21:44:19.507957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1873746 -4.174963 -4.1716614 -4.1802 -4.2030315 -4.2306771 -4.2544241 -4.270134 -4.2778654 -4.2768521 -4.2744126 -4.2732759 -4.2772064 -4.2781324 -4.259891][-4.175776 -4.1867089 -4.193697 -4.2014823 -4.2142262 -4.2311778 -4.2496834 -4.2710238 -4.2850604 -4.2832742 -4.2800751 -4.2813196 -4.2861123 -4.2902851 -4.2819605][-4.1822925 -4.2061996 -4.2171764 -4.2216835 -4.2274985 -4.2351918 -4.2484 -4.2721648 -4.2856636 -4.2819672 -4.283308 -4.2847862 -4.2854633 -4.2875462 -4.2838163][-4.2103353 -4.2276587 -4.2304444 -4.2251887 -4.2206693 -4.2210708 -4.2331934 -4.2576327 -4.2699437 -4.2722011 -4.2833977 -4.2876387 -4.2823629 -4.2793689 -4.2764812][-4.2387123 -4.2365513 -4.2238803 -4.2107787 -4.2017431 -4.198524 -4.20934 -4.2311077 -4.24464 -4.2581792 -4.2790465 -4.2879333 -4.2826495 -4.2782226 -4.2755575][-4.2545819 -4.238935 -4.2165651 -4.1998258 -4.1870618 -4.181344 -4.1889896 -4.2062688 -4.2189865 -4.2380953 -4.2645731 -4.2810678 -4.2831254 -4.2839003 -4.2845397][-4.2579308 -4.2447219 -4.2218652 -4.2028828 -4.184598 -4.1722536 -4.1745653 -4.1852589 -4.1960149 -4.2176933 -4.2480121 -4.2744517 -4.2873731 -4.2980909 -4.3009377][-4.2481127 -4.2477245 -4.2322717 -4.2126765 -4.1870046 -4.1648989 -4.1600313 -4.1636863 -4.177454 -4.2043457 -4.2368889 -4.2705245 -4.2955384 -4.3130584 -4.3139653][-4.2291565 -4.2431407 -4.2374363 -4.2169046 -4.1842089 -4.1525655 -4.1401291 -4.1426806 -4.1604104 -4.1892524 -4.22194 -4.2584591 -4.2924032 -4.312489 -4.3088021][-4.2112913 -4.2321672 -4.2342334 -4.2176 -4.1857677 -4.1504011 -4.1336761 -4.1344829 -4.1501532 -4.1735396 -4.2009687 -4.2373829 -4.2766447 -4.299108 -4.294044][-4.2110758 -4.2326431 -4.2399769 -4.2297225 -4.2012148 -4.1673737 -4.1479793 -4.1460853 -4.1551642 -4.172575 -4.1976614 -4.2343297 -4.271935 -4.2915034 -4.2871723][-4.2348847 -4.2542157 -4.2596927 -4.2510705 -4.2271762 -4.1969666 -4.1772289 -4.1729026 -4.1782889 -4.1952176 -4.2196865 -4.2496471 -4.2781029 -4.2889915 -4.2835011][-4.2589059 -4.2734847 -4.2774892 -4.2722597 -4.2570586 -4.234282 -4.2163725 -4.2081437 -4.2104187 -4.2214751 -4.23896 -4.2616634 -4.2831755 -4.2875109 -4.2801819][-4.2713037 -4.2889609 -4.2963157 -4.2946177 -4.2834487 -4.2653413 -4.2484379 -4.2365966 -4.2332935 -4.2369747 -4.2489653 -4.265214 -4.2799664 -4.2813063 -4.2722116][-4.263351 -4.2818303 -4.2906952 -4.2907181 -4.2808743 -4.2618809 -4.24525 -4.236557 -4.2358828 -4.2400923 -4.2475939 -4.2569203 -4.2655959 -4.2652106 -4.2607961]]...]
INFO - root - 2017-12-05 21:44:28.476061: step 43210, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 72h:33m:30s remains)
INFO - root - 2017-12-05 21:44:37.569839: step 43220, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.902 sec/batch; 72h:26m:45s remains)
INFO - root - 2017-12-05 21:44:46.719740: step 43230, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 71h:51m:04s remains)
INFO - root - 2017-12-05 21:44:55.860197: step 43240, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.943 sec/batch; 75h:47m:22s remains)
INFO - root - 2017-12-05 21:45:04.830950: step 43250, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 72h:09m:40s remains)
INFO - root - 2017-12-05 21:45:13.865778: step 43260, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 73h:16m:53s remains)
INFO - root - 2017-12-05 21:45:22.873889: step 43270, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 70h:36m:03s remains)
INFO - root - 2017-12-05 21:45:31.770038: step 43280, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 72h:42m:20s remains)
INFO - root - 2017-12-05 21:45:40.908499: step 43290, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 72h:31m:26s remains)
INFO - root - 2017-12-05 21:45:50.030952: step 43300, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 70h:55m:55s remains)
2017-12-05 21:45:50.808790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2099986 -4.2206492 -4.2279854 -4.2317286 -4.2307544 -4.2273588 -4.2223706 -4.2191653 -4.2163672 -4.213006 -4.211132 -4.21174 -4.2191939 -4.2293787 -4.2391043][-4.231328 -4.239903 -4.2490273 -4.2556577 -4.2586932 -4.2581587 -4.2549076 -4.2512221 -4.2459893 -4.2399187 -4.2352142 -4.2322917 -4.2358637 -4.244565 -4.2531333][-4.2520876 -4.2573948 -4.2646618 -4.2710238 -4.2742233 -4.2755117 -4.2749648 -4.2730217 -4.2687321 -4.2623696 -4.2572174 -4.2524652 -4.2537608 -4.2597494 -4.2630472][-4.2520409 -4.2544155 -4.2590227 -4.2656178 -4.2698555 -4.273067 -4.2717047 -4.2691669 -4.2661018 -4.2608366 -4.2576256 -4.2534337 -4.253747 -4.25672 -4.2524881][-4.2169175 -4.2203422 -4.2234383 -4.2297521 -4.2322793 -4.2328415 -4.2273211 -4.2214479 -4.2217293 -4.2238951 -4.2269878 -4.2259459 -4.2264571 -4.2260303 -4.2121811][-4.1358323 -4.1416426 -4.1450706 -4.1490097 -4.1442208 -4.1334281 -4.1161118 -4.1031961 -4.1105556 -4.130806 -4.151474 -4.1634545 -4.1699 -4.1677437 -4.1459293][-4.0206151 -4.0223875 -4.0263929 -4.032299 -4.0262189 -4.0035625 -3.9679432 -3.9440019 -3.9580395 -3.9966633 -4.0355763 -4.065414 -4.0875487 -4.0923266 -4.0703506][-3.9975374 -3.9937067 -3.9979358 -4.0086851 -4.0057182 -3.9808657 -3.9410191 -3.9169369 -3.9303644 -3.9658062 -4.0027528 -4.0360341 -4.06448 -4.0743375 -4.0593047][-4.0875049 -4.0882955 -4.0957031 -4.1088867 -4.1093464 -4.0915141 -4.0658298 -4.0551825 -4.0642171 -4.0783081 -4.0908909 -4.1037335 -4.1159196 -4.1160135 -4.1033616][-4.1805964 -4.1897531 -4.2007451 -4.2134886 -4.2131429 -4.1988969 -4.1827722 -4.17767 -4.1808114 -4.1811533 -4.1799293 -4.1789618 -4.17653 -4.1657057 -4.1542397][-4.2430534 -4.25378 -4.2632241 -4.2727461 -4.2720442 -4.2622113 -4.2523303 -4.2487211 -4.24834 -4.24411 -4.2388997 -4.2336922 -4.2272315 -4.2160292 -4.2087259][-4.2794337 -4.2861419 -4.2913756 -4.29865 -4.299726 -4.2961159 -4.2923722 -4.2914276 -4.289875 -4.2847748 -4.2792048 -4.2751832 -4.2704015 -4.2637424 -4.2610254][-4.2837548 -4.2830482 -4.28415 -4.2905149 -4.2946839 -4.2956252 -4.2958941 -4.2971768 -4.2977443 -4.2967958 -4.29619 -4.29714 -4.297328 -4.2957993 -4.2943192][-4.2685304 -4.2588787 -4.2537131 -4.2565393 -4.2630672 -4.2702956 -4.2783184 -4.2851582 -4.290534 -4.295958 -4.3016324 -4.307734 -4.312376 -4.3137393 -4.313446][-4.2598815 -4.2413321 -4.2298212 -4.2310939 -4.2421727 -4.2584033 -4.2762251 -4.2902093 -4.300488 -4.3102207 -4.31831 -4.3252044 -4.3302507 -4.33232 -4.3331819]]...]
INFO - root - 2017-12-05 21:45:59.770168: step 43310, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 71h:18m:47s remains)
INFO - root - 2017-12-05 21:46:09.107044: step 43320, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 76h:10m:07s remains)
INFO - root - 2017-12-05 21:46:18.045557: step 43330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 73h:14m:15s remains)
INFO - root - 2017-12-05 21:46:27.144535: step 43340, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 72h:50m:58s remains)
INFO - root - 2017-12-05 21:46:36.352955: step 43350, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 75h:58m:51s remains)
INFO - root - 2017-12-05 21:46:45.536280: step 43360, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 73h:20m:34s remains)
INFO - root - 2017-12-05 21:46:54.607489: step 43370, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.940 sec/batch; 75h:28m:44s remains)
INFO - root - 2017-12-05 21:47:03.500500: step 43380, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 72h:22m:11s remains)
INFO - root - 2017-12-05 21:47:12.566326: step 43390, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 73h:01m:42s remains)
INFO - root - 2017-12-05 21:47:21.963857: step 43400, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 71h:31m:18s remains)
2017-12-05 21:47:22.709404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0444856 -4.0263076 -4.0264091 -4.0701089 -4.1259613 -4.1288242 -4.0978169 -4.0779648 -4.1008334 -4.1158237 -4.1038885 -4.0762224 -4.0474319 -4.0396056 -4.0696111][-4.0431781 -4.015717 -4.0150247 -4.0647721 -4.1231718 -4.1228237 -4.0834937 -4.0645194 -4.1006417 -4.1213937 -4.099793 -4.055675 -4.0189362 -4.0176358 -4.0583706][-4.0542235 -4.0193343 -4.0187063 -4.0685387 -4.1202083 -4.1134114 -4.0684829 -4.0504408 -4.09423 -4.1177273 -4.090363 -4.04118 -4.0015078 -4.0076365 -4.0566363][-4.0667143 -4.0271149 -4.0264125 -4.074995 -4.1169691 -4.0992947 -4.041822 -4.0211034 -4.0753703 -4.1059227 -4.0846066 -4.0386405 -4.0025258 -4.0176644 -4.0660405][-4.0632682 -4.0206289 -4.0209513 -4.0722604 -4.1069741 -4.075748 -4.0014729 -3.9791915 -4.0507641 -4.0970407 -4.089726 -4.0515289 -4.0203919 -4.0400515 -4.0823541][-4.0432544 -3.994997 -3.9962039 -4.0524974 -4.0840573 -4.0372267 -3.9398983 -3.9155121 -4.0126748 -4.080636 -4.0917482 -4.067277 -4.0442071 -4.0598054 -4.0888505][-4.0306273 -3.9762979 -3.9772263 -4.0363479 -4.0642762 -4.0009713 -3.874886 -3.8474584 -3.967773 -4.0557718 -4.0802135 -4.0630994 -4.0427265 -4.0548506 -4.0710344][-4.0542021 -3.9939954 -3.9910717 -4.0457792 -4.0684481 -3.9926267 -3.8540361 -3.8278642 -3.9505615 -4.0382814 -4.0629816 -4.0379624 -4.0093184 -4.0196862 -4.0317435][-4.1057134 -4.042666 -4.034492 -4.0817828 -4.1025786 -4.0344334 -3.9198575 -3.9074268 -3.9994338 -4.0588279 -4.0680857 -4.0246897 -3.9851944 -3.9946723 -4.0088458][-4.1488047 -4.0915279 -4.080934 -4.1210165 -4.1375766 -4.0803108 -3.9949007 -4.00098 -4.0700727 -4.1052008 -4.100059 -4.0461926 -4.0079026 -4.0164442 -4.029973][-4.1738095 -4.1224532 -4.1083689 -4.1428156 -4.1592975 -4.1140466 -4.0496292 -4.0709357 -4.1288853 -4.15117 -4.1363688 -4.0846992 -4.0595808 -4.0670519 -4.0741634][-4.17427 -4.1269403 -4.1129942 -4.1464448 -4.1672907 -4.1404119 -4.0961 -4.1210647 -4.1691928 -4.1864586 -4.1705408 -4.122921 -4.1042085 -4.1081433 -4.1040416][-4.1489611 -4.1061912 -4.09867 -4.1379085 -4.1662703 -4.1560664 -4.1306558 -4.1559405 -4.1943684 -4.2080131 -4.1932979 -4.146894 -4.1226015 -4.11618 -4.1013207][-4.1308537 -4.096868 -4.0978522 -4.1371365 -4.1664448 -4.162097 -4.1503139 -4.1791158 -4.2133803 -4.221808 -4.2037969 -4.1577406 -4.1228623 -4.1031275 -4.0766][-4.1318412 -4.1097488 -4.1164989 -4.1535687 -4.1787996 -4.173264 -4.1638985 -4.1931357 -4.2238474 -4.2272248 -4.2024117 -4.1566429 -4.1145649 -4.0847707 -4.0525074]]...]
INFO - root - 2017-12-05 21:47:31.703926: step 43410, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 70h:49m:34s remains)
INFO - root - 2017-12-05 21:47:40.872551: step 43420, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 71h:56m:52s remains)
INFO - root - 2017-12-05 21:47:50.079544: step 43430, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 74h:08m:49s remains)
INFO - root - 2017-12-05 21:47:59.104203: step 43440, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 72h:20m:19s remains)
INFO - root - 2017-12-05 21:48:08.165049: step 43450, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 71h:59m:45s remains)
INFO - root - 2017-12-05 21:48:17.335605: step 43460, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 73h:49m:49s remains)
INFO - root - 2017-12-05 21:48:26.430052: step 43470, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 73h:51m:06s remains)
INFO - root - 2017-12-05 21:48:35.655465: step 43480, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 72h:32m:03s remains)
INFO - root - 2017-12-05 21:48:44.712742: step 43490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 69h:13m:44s remains)
INFO - root - 2017-12-05 21:48:53.833152: step 43500, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 72h:08m:48s remains)
2017-12-05 21:48:54.685996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2664719 -4.2424421 -4.2295313 -4.2195125 -4.2155404 -4.22592 -4.2310495 -4.2306552 -4.2366352 -4.2519159 -4.2721548 -4.2935114 -4.31452 -4.3306084 -4.3427653][-4.2273636 -4.2000356 -4.185966 -4.17045 -4.1633267 -4.1759586 -4.1854649 -4.1862564 -4.1959124 -4.2188711 -4.24749 -4.2805462 -4.3101497 -4.3303089 -4.3423786][-4.1896915 -4.1583872 -4.1440196 -4.1262097 -4.1172709 -4.1253581 -4.1274552 -4.126183 -4.1442151 -4.1823864 -4.2223191 -4.2670941 -4.3065934 -4.33173 -4.3429537][-4.156661 -4.1219993 -4.1068778 -4.0903511 -4.0799003 -4.0743532 -4.0552993 -4.0465417 -4.0753121 -4.1350136 -4.1950359 -4.2508826 -4.3008909 -4.332375 -4.343955][-4.1171627 -4.0796113 -4.0672178 -4.0639634 -4.0585723 -4.0344419 -3.9789195 -3.940577 -3.9742839 -4.0645885 -4.1576867 -4.2313561 -4.2900538 -4.3284535 -4.3428421][-4.0717959 -4.0312519 -4.0268869 -4.043014 -4.0456715 -3.9961329 -3.8850205 -3.7904143 -3.8356707 -3.978061 -4.1129084 -4.2050734 -4.27331 -4.3215618 -4.3414879][-4.0378056 -3.9987504 -3.9979913 -4.0185347 -4.0161886 -3.935087 -3.7485576 -3.5737176 -3.6569777 -3.8776202 -4.0578403 -4.1707263 -4.2501554 -4.3102012 -4.3364053][-4.027235 -3.9956722 -3.9934804 -4.0090356 -4.0013704 -3.9107134 -3.6945295 -3.4977748 -3.6193688 -3.8605304 -4.0457444 -4.1599913 -4.2380571 -4.2993736 -4.3272038][-4.0634389 -4.0425491 -4.0392542 -4.0487008 -4.0428634 -3.9785953 -3.8255506 -3.7068439 -3.7916498 -3.9556608 -4.0973487 -4.1930714 -4.2579246 -4.3038378 -4.3221855][-4.1357293 -4.1201611 -4.1122937 -4.115252 -4.1081352 -4.0705218 -3.9855154 -3.9257109 -3.9645064 -4.0531936 -4.1507692 -4.2291775 -4.2842727 -4.3161168 -4.3246436][-4.216568 -4.2026806 -4.1877165 -4.1813979 -4.169558 -4.1492214 -4.1041789 -4.0652757 -4.0742188 -4.1256585 -4.2001376 -4.2653503 -4.3110785 -4.3306327 -4.3317633][-4.2821321 -4.2705784 -4.2550645 -4.2439075 -4.2333937 -4.2233334 -4.1998935 -4.1711922 -4.1667514 -4.200398 -4.258707 -4.3099895 -4.3406639 -4.3470583 -4.3410659][-4.3189745 -4.3108888 -4.3014574 -4.2929368 -4.2849731 -4.2818122 -4.2716765 -4.2519231 -4.246911 -4.2684932 -4.307147 -4.3407121 -4.3589177 -4.3595362 -4.3512626][-4.3357286 -4.3284631 -4.3232989 -4.317605 -4.3125315 -4.310761 -4.306838 -4.2981215 -4.2957454 -4.3067956 -4.3288336 -4.3488274 -4.3606162 -4.3620791 -4.3568997][-4.3410358 -4.3344026 -4.3315377 -4.329216 -4.3254762 -4.3222432 -4.3192158 -4.3163419 -4.3158741 -4.32053 -4.331943 -4.343761 -4.3523316 -4.3557358 -4.3544497]]...]
INFO - root - 2017-12-05 21:49:03.712669: step 43510, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 72h:33m:03s remains)
INFO - root - 2017-12-05 21:49:12.833867: step 43520, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 74h:07m:54s remains)
INFO - root - 2017-12-05 21:49:21.914369: step 43530, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 71h:04m:21s remains)
INFO - root - 2017-12-05 21:49:30.895811: step 43540, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 71h:37m:53s remains)
INFO - root - 2017-12-05 21:49:40.047014: step 43550, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 72h:52m:23s remains)
INFO - root - 2017-12-05 21:49:49.160993: step 43560, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 72h:56m:39s remains)
INFO - root - 2017-12-05 21:49:58.138050: step 43570, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 75h:32m:08s remains)
INFO - root - 2017-12-05 21:50:07.274839: step 43580, loss = 2.02, batch loss = 1.96 (8.6 examples/sec; 0.932 sec/batch; 74h:45m:45s remains)
INFO - root - 2017-12-05 21:50:16.336042: step 43590, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 72h:59m:31s remains)
INFO - root - 2017-12-05 21:50:25.388234: step 43600, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 73h:30m:46s remains)
2017-12-05 21:50:26.284480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1757531 -4.1812072 -4.1845021 -4.1748261 -4.1630845 -4.1571989 -4.1674161 -4.1961393 -4.2308168 -4.2466412 -4.2380447 -4.2179952 -4.1929221 -4.1753306 -4.1419425][-4.1747665 -4.1859751 -4.1934972 -4.1844158 -4.1743841 -4.1705351 -4.1777253 -4.1996136 -4.2279558 -4.2454906 -4.2466421 -4.2354341 -4.2149897 -4.2025433 -4.1757107][-4.1774468 -4.1944056 -4.2068539 -4.1992874 -4.1892886 -4.1825972 -4.18243 -4.1929655 -4.2125425 -4.2311664 -4.2375116 -4.2312875 -4.2150764 -4.206686 -4.1874628][-4.1820946 -4.2028027 -4.2147346 -4.2022786 -4.1849813 -4.1711512 -4.1599083 -4.156601 -4.170218 -4.1941919 -4.2070889 -4.2098351 -4.2037954 -4.2044806 -4.1975856][-4.1843128 -4.2031064 -4.2085748 -4.1841073 -4.1511149 -4.1230917 -4.0968142 -4.0797024 -4.0968275 -4.1384964 -4.1666431 -4.1857963 -4.1967688 -4.2097058 -4.21709][-4.1845336 -4.1973991 -4.1941619 -4.1571259 -4.1052427 -4.0587316 -4.0188007 -3.9926052 -4.0186367 -4.0824537 -4.1305552 -4.1677918 -4.1948643 -4.2191119 -4.2366643][-4.1884222 -4.1967821 -4.18943 -4.1486444 -4.0895538 -4.0376635 -3.9980576 -3.9717119 -3.9990032 -4.0648394 -4.1163034 -4.1575356 -4.1886897 -4.2162027 -4.2372618][-4.1955805 -4.2039142 -4.197813 -4.16341 -4.1128306 -4.0690074 -4.0393229 -4.0181794 -4.0313168 -4.0741677 -4.1139469 -4.14602 -4.1696243 -4.1938782 -4.2164283][-4.2031183 -4.2127843 -4.2099366 -4.1868339 -4.1479435 -4.1115727 -4.0883989 -4.0722308 -4.071846 -4.09304 -4.1224437 -4.14593 -4.1618905 -4.1798949 -4.2003403][-4.2029514 -4.2116623 -4.2119846 -4.1999307 -4.1692996 -4.1367111 -4.1153851 -4.1022344 -4.0982103 -4.1125841 -4.1399279 -4.1616011 -4.1756649 -4.1881967 -4.2021761][-4.1809411 -4.1857867 -4.1923728 -4.1930518 -4.1740489 -4.1493979 -4.1303611 -4.1182542 -4.1143003 -4.1289349 -4.1561513 -4.178473 -4.1920748 -4.1988006 -4.2044725][-4.1463428 -4.1478262 -4.16138 -4.1747742 -4.17025 -4.1583061 -4.14567 -4.137444 -4.1371861 -4.1511159 -4.1728368 -4.1897759 -4.19743 -4.1980877 -4.1977296][-4.100441 -4.1019883 -4.1230526 -4.1492529 -4.1604857 -4.1610427 -4.1562881 -4.1519928 -4.1523886 -4.1597962 -4.1705389 -4.17801 -4.1794639 -4.1782503 -4.1763926][-4.0621753 -4.0675826 -4.0952897 -4.129849 -4.1499338 -4.1567631 -4.155549 -4.1525211 -4.1514573 -4.15323 -4.156446 -4.1587367 -4.1583142 -4.1583328 -4.1577835][-4.0673656 -4.0774331 -4.1077833 -4.1429391 -4.1621776 -4.1684837 -4.1681056 -4.1659083 -4.1646729 -4.163506 -4.1632471 -4.1644583 -4.16535 -4.1676369 -4.1681962]]...]
INFO - root - 2017-12-05 21:50:35.317212: step 43610, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 72h:46m:39s remains)
INFO - root - 2017-12-05 21:50:44.421793: step 43620, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 71h:46m:33s remains)
INFO - root - 2017-12-05 21:50:53.433605: step 43630, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 71h:09m:04s remains)
INFO - root - 2017-12-05 21:51:02.488572: step 43640, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 73h:27m:53s remains)
INFO - root - 2017-12-05 21:51:11.686451: step 43650, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 73h:10m:11s remains)
INFO - root - 2017-12-05 21:51:20.546976: step 43660, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 69h:41m:19s remains)
INFO - root - 2017-12-05 21:51:29.667193: step 43670, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 70h:25m:01s remains)
INFO - root - 2017-12-05 21:51:38.829075: step 43680, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 71h:26m:04s remains)
INFO - root - 2017-12-05 21:51:47.870415: step 43690, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 71h:40m:33s remains)
INFO - root - 2017-12-05 21:51:56.915255: step 43700, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 71h:07m:07s remains)
2017-12-05 21:51:57.765063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861671 -4.2609072 -4.2386184 -4.22981 -4.2214394 -4.1980982 -4.1853151 -4.1845126 -4.1819038 -4.1795964 -4.1721277 -4.1677656 -4.1707039 -4.1756978 -4.1826692][-4.2790518 -4.2515459 -4.2289581 -4.2218246 -4.2177196 -4.1973166 -4.1830149 -4.1800389 -4.1756968 -4.176733 -4.1748114 -4.1725163 -4.1795783 -4.1939158 -4.2043123][-4.2764277 -4.2458563 -4.2232909 -4.2208281 -4.2205796 -4.2002125 -4.1791296 -4.1684294 -4.163218 -4.1702943 -4.1734314 -4.1754417 -4.1874204 -4.2076735 -4.2177529][-4.27466 -4.2422423 -4.2222342 -4.2256455 -4.22729 -4.20502 -4.1764851 -4.1543331 -4.1486597 -4.1640043 -4.1727405 -4.181385 -4.1933408 -4.2098956 -4.218462][-4.2763348 -4.246151 -4.2301607 -4.2346153 -4.2338228 -4.2099962 -4.1757212 -4.1413503 -4.1342125 -4.1530972 -4.1627841 -4.173665 -4.1829 -4.1969938 -4.2123809][-4.2809358 -4.2551236 -4.2395558 -4.2380066 -4.2297735 -4.2006536 -4.1562872 -4.1033154 -4.0857124 -4.1034884 -4.1131611 -4.1331353 -4.1530113 -4.1805296 -4.2110248][-4.2855716 -4.2630496 -4.2428222 -4.2292213 -4.2061076 -4.1560297 -4.0871572 -4.0036907 -3.9784563 -4.0135546 -4.0441537 -4.0839386 -4.1185775 -4.1633978 -4.2082229][-4.2887745 -4.2654204 -4.2355475 -4.2050281 -4.1582222 -4.079988 -3.980037 -3.8738403 -3.8651643 -3.940541 -4.0066619 -4.0671563 -4.1085424 -4.1560907 -4.2021804][-4.2833881 -4.256865 -4.2161655 -4.1706367 -4.1071696 -4.0160737 -3.9141092 -3.8325386 -3.8621025 -3.9640303 -4.0327668 -4.0847049 -4.1186981 -4.15806 -4.2026672][-4.2732363 -4.2428083 -4.1971 -4.1483173 -4.0862246 -4.0139351 -3.9547327 -3.9282918 -3.9729655 -4.050292 -4.0822477 -4.1046042 -4.1265979 -4.1565013 -4.1991248][-4.2689834 -4.2401586 -4.1943588 -4.1507173 -4.1021309 -4.0541673 -4.0301094 -4.026042 -4.0547047 -4.0985351 -4.0985355 -4.0952291 -4.1068764 -4.1358 -4.1802316][-4.2713685 -4.2438407 -4.2016997 -4.1629448 -4.122376 -4.0844474 -4.0651941 -4.052207 -4.0557446 -4.0818987 -4.0748668 -4.0625739 -4.0702457 -4.1086226 -4.1586351][-4.2793145 -4.2523818 -4.21308 -4.1731648 -4.1288972 -4.0857487 -4.0597138 -4.0314255 -4.0204325 -4.0496325 -4.0560064 -4.0476151 -4.0569191 -4.0987258 -4.1497722][-4.2845683 -4.2570496 -4.2169995 -4.1732211 -4.1293769 -4.0832543 -4.0552197 -4.0189333 -4.0056534 -4.0467129 -4.0713406 -4.0743337 -4.0842896 -4.1147609 -4.157074][-4.289391 -4.2602291 -4.2219186 -4.1795115 -4.1385641 -4.0960951 -4.076088 -4.0507712 -4.041841 -4.0797925 -4.1069527 -4.1188278 -4.1240239 -4.13578 -4.1599817]]...]
INFO - root - 2017-12-05 21:52:06.777579: step 43710, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 73h:01m:16s remains)
INFO - root - 2017-12-05 21:52:15.858873: step 43720, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 74h:32m:32s remains)
INFO - root - 2017-12-05 21:52:25.025194: step 43730, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.908 sec/batch; 72h:49m:29s remains)
INFO - root - 2017-12-05 21:52:34.180091: step 43740, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 74h:08m:05s remains)
INFO - root - 2017-12-05 21:52:43.133548: step 43750, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 74h:43m:52s remains)
INFO - root - 2017-12-05 21:52:52.281442: step 43760, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.923 sec/batch; 74h:02m:30s remains)
INFO - root - 2017-12-05 21:53:01.474473: step 43770, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.949 sec/batch; 76h:07m:37s remains)
INFO - root - 2017-12-05 21:53:10.713505: step 43780, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 72h:17m:09s remains)
INFO - root - 2017-12-05 21:53:19.782064: step 43790, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.805 sec/batch; 64h:35m:07s remains)
INFO - root - 2017-12-05 21:53:28.920789: step 43800, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 73h:30m:32s remains)
2017-12-05 21:53:29.735611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.192955 -4.1944242 -4.2038026 -4.2224865 -4.2453318 -4.2622948 -4.2736144 -4.2745137 -4.2752833 -4.2706308 -4.263629 -4.252902 -4.2431059 -4.2348843 -4.2332096][-4.1530704 -4.1549983 -4.1690049 -4.19912 -4.2326064 -4.2576065 -4.2744823 -4.28084 -4.2831507 -4.278759 -4.2723274 -4.26002 -4.2482386 -4.2375646 -4.2330861][-4.1503363 -4.1532884 -4.1687 -4.2012782 -4.2347307 -4.2592754 -4.2753463 -4.2860489 -4.2947659 -4.29411 -4.2938356 -4.2856092 -4.2745767 -4.2618384 -4.25647][-4.1747537 -4.1780629 -4.1936927 -4.2200451 -4.2440572 -4.2598414 -4.269383 -4.2818389 -4.2949166 -4.2963853 -4.30094 -4.2988935 -4.2931361 -4.2827578 -4.2794905][-4.2071571 -4.2053514 -4.2157593 -4.2318692 -4.2457042 -4.2523689 -4.2582116 -4.2718611 -4.283093 -4.2801323 -4.2821136 -4.2809954 -4.2824736 -4.2793436 -4.2830296][-4.24295 -4.2320557 -4.2268267 -4.221725 -4.215539 -4.213644 -4.2180071 -4.2310371 -4.2353854 -4.2293758 -4.2327118 -4.2353811 -4.2452021 -4.2512226 -4.2675228][-4.2586608 -4.2378416 -4.2141352 -4.1838293 -4.1555591 -4.1426973 -4.1409597 -4.1484284 -4.1400695 -4.1290112 -4.1373076 -4.1553855 -4.179893 -4.2000737 -4.2334981][-4.2326908 -4.2039032 -4.1664228 -4.1168818 -4.0737848 -4.0484996 -4.0375285 -4.0338345 -4.0148268 -4.0034256 -4.0229979 -4.0603633 -4.0989418 -4.1348104 -4.1863022][-4.1952858 -4.1604438 -4.1187978 -4.0640669 -4.0143638 -3.9828875 -3.9674184 -3.9543602 -3.9307473 -3.9258108 -3.9602606 -4.0137596 -4.0636268 -4.111156 -4.1731491][-4.1862855 -4.1570354 -4.1324139 -4.0991445 -4.0609407 -4.033443 -4.0191174 -4.0059066 -3.9877195 -3.9903162 -4.0299869 -4.0826764 -4.1263456 -4.1656957 -4.2144661][-4.1816845 -4.1658 -4.16423 -4.1610689 -4.1480536 -4.1373243 -4.1314712 -4.1272225 -4.1223192 -4.1294765 -4.1605158 -4.1987405 -4.2258182 -4.2449512 -4.2661934][-4.1643295 -4.161283 -4.1755781 -4.1918278 -4.1980944 -4.2060375 -4.2121291 -4.2174921 -4.2232642 -4.2296596 -4.2479777 -4.2680168 -4.2773695 -4.275703 -4.2717171][-4.1372786 -4.1471968 -4.169054 -4.1908426 -4.2076364 -4.2278447 -4.2416196 -4.2515082 -4.2593045 -4.2613487 -4.2695971 -4.2756715 -4.272162 -4.2558126 -4.2368712][-4.1071625 -4.1295671 -4.15793 -4.1803865 -4.20053 -4.2243924 -4.239677 -4.2514429 -4.2611246 -4.2612414 -4.2644343 -4.2627087 -4.2508469 -4.2253346 -4.19713][-4.1285028 -4.1544976 -4.1816339 -4.1995025 -4.2131276 -4.2277679 -4.2343111 -4.2429767 -4.25065 -4.2478113 -4.2464404 -4.2414827 -4.2276287 -4.2012634 -4.1737003]]...]
INFO - root - 2017-12-05 21:53:38.730168: step 43810, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.931 sec/batch; 74h:37m:15s remains)
INFO - root - 2017-12-05 21:53:47.839130: step 43820, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 71h:27m:03s remains)
INFO - root - 2017-12-05 21:53:56.940479: step 43830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 72h:40m:25s remains)
INFO - root - 2017-12-05 21:54:05.925204: step 43840, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 72h:09m:05s remains)
INFO - root - 2017-12-05 21:54:15.093609: step 43850, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 70h:25m:13s remains)
INFO - root - 2017-12-05 21:54:24.345784: step 43860, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 75h:15m:54s remains)
INFO - root - 2017-12-05 21:54:33.350331: step 43870, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 70h:16m:02s remains)
INFO - root - 2017-12-05 21:54:42.519715: step 43880, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 73h:58m:26s remains)
INFO - root - 2017-12-05 21:54:51.622638: step 43890, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 70h:52m:06s remains)
INFO - root - 2017-12-05 21:55:00.530689: step 43900, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 75h:56m:34s remains)
2017-12-05 21:55:01.306669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2074008 -4.2292533 -4.2443233 -4.2489424 -4.24068 -4.223959 -4.2094369 -4.2094975 -4.230792 -4.2589478 -4.277885 -4.2869568 -4.2915859 -4.2886391 -4.2667756][-4.205976 -4.22956 -4.2437854 -4.2487845 -4.2415924 -4.2231479 -4.2047877 -4.2018886 -4.2244425 -4.2558179 -4.2772808 -4.2893987 -4.2963686 -4.2937121 -4.2698064][-4.1955175 -4.2202206 -4.2385306 -4.2481441 -4.2447205 -4.22549 -4.2023349 -4.195302 -4.2174449 -4.2504539 -4.2739654 -4.2901096 -4.2990904 -4.2953324 -4.2687764][-4.1834264 -4.2105441 -4.2346754 -4.2510228 -4.2520456 -4.2322159 -4.2036924 -4.1904125 -4.2096114 -4.242188 -4.2682629 -4.2893362 -4.301085 -4.2960186 -4.267107][-4.1679788 -4.1991506 -4.2305312 -4.2553964 -4.2627921 -4.2436581 -4.2093825 -4.1877313 -4.2010593 -4.2321887 -4.2614555 -4.2879963 -4.3028827 -4.297678 -4.2679396][-4.1526046 -4.1874971 -4.2261572 -4.259192 -4.2747941 -4.2578225 -4.2177553 -4.1864243 -4.1914659 -4.2207575 -4.2535429 -4.285306 -4.3033857 -4.2991366 -4.2705216][-4.1409378 -4.1788411 -4.2232161 -4.2627478 -4.2853355 -4.2713327 -4.227047 -4.187068 -4.1839037 -4.210197 -4.2452626 -4.2810555 -4.3018827 -4.2986455 -4.2716584][-4.1350827 -4.1740661 -4.221478 -4.2644739 -4.2909369 -4.2798815 -4.2344012 -4.1888175 -4.1787562 -4.201489 -4.2368894 -4.2749939 -4.2975507 -4.2950282 -4.2695832][-4.1368966 -4.1743693 -4.2210159 -4.2635365 -4.2902546 -4.2819624 -4.238308 -4.190587 -4.17538 -4.194994 -4.2295446 -4.2681193 -4.2917137 -4.2902026 -4.2669554][-4.1374159 -4.1706066 -4.2146153 -4.2553282 -4.2823257 -4.2777004 -4.2382689 -4.1915 -4.1738358 -4.1909156 -4.2239056 -4.2614865 -4.2857728 -4.2865529 -4.2668438][-4.1386533 -4.1652827 -4.2058082 -4.2451639 -4.2722321 -4.2708292 -4.236445 -4.1933885 -4.1755962 -4.190454 -4.2208281 -4.2563519 -4.2812018 -4.2849422 -4.2692833][-4.1437712 -4.1635494 -4.2001987 -4.2374554 -4.263618 -4.2644796 -4.2351904 -4.1979156 -4.1823835 -4.19546 -4.2225585 -4.25498 -4.2792792 -4.2849612 -4.2724557][-4.1543188 -4.1671987 -4.1977892 -4.2312927 -4.2563105 -4.2598381 -4.2363577 -4.2060504 -4.1941233 -4.2058258 -4.2293925 -4.2576642 -4.2798824 -4.2863741 -4.2766571][-4.16815 -4.17515 -4.1987987 -4.2272811 -4.2505388 -4.2561536 -4.238174 -4.2143908 -4.2058868 -4.2167616 -4.2376556 -4.2623234 -4.2821207 -4.2886591 -4.2813849][-4.1810484 -4.1833572 -4.2001467 -4.2236214 -4.24533 -4.2522879 -4.239141 -4.2211266 -4.2156763 -4.2259369 -4.2448344 -4.2668829 -4.2847128 -4.2911091 -4.2853484]]...]
INFO - root - 2017-12-05 21:55:10.532906: step 43910, loss = 2.06, batch loss = 2.01 (7.8 examples/sec; 1.025 sec/batch; 82h:07m:51s remains)
INFO - root - 2017-12-05 21:55:19.426452: step 43920, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 71h:11m:25s remains)
INFO - root - 2017-12-05 21:55:28.526064: step 43930, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 71h:51m:16s remains)
INFO - root - 2017-12-05 21:55:37.456691: step 43940, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 72h:37m:21s remains)
INFO - root - 2017-12-05 21:55:46.567524: step 43950, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 72h:23m:23s remains)
INFO - root - 2017-12-05 21:55:55.684149: step 43960, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 74h:19m:43s remains)
INFO - root - 2017-12-05 21:56:04.943786: step 43970, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 73h:54m:05s remains)
INFO - root - 2017-12-05 21:56:14.151550: step 43980, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 72h:13m:42s remains)
INFO - root - 2017-12-05 21:56:23.139428: step 43990, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 72h:23m:54s remains)
INFO - root - 2017-12-05 21:56:32.196243: step 44000, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.907 sec/batch; 72h:41m:12s remains)
2017-12-05 21:56:33.065608: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0797281 -4.0422907 -4.0094333 -3.9930549 -4.005765 -4.0369987 -4.0533228 -4.0732236 -4.1194019 -4.1900787 -4.2523894 -4.2957511 -4.3194427 -4.3364663 -4.3562417][-4.0806394 -4.0398073 -4.0045075 -3.987469 -4.003335 -4.0390058 -4.0566359 -4.0762506 -4.1186523 -4.1891322 -4.2511525 -4.2930703 -4.3138962 -4.3276658 -4.3476472][-4.0956969 -4.0625429 -4.035141 -4.0259266 -4.0468049 -4.0806942 -4.1004925 -4.1149707 -4.145834 -4.206552 -4.2628455 -4.297575 -4.3131371 -4.3208942 -4.3391156][-4.1077366 -4.0827293 -4.0691891 -4.0739074 -4.0970273 -4.1268353 -4.1444082 -4.1477246 -4.1613512 -4.2121387 -4.2652578 -4.2979665 -4.31309 -4.3179536 -4.3346972][-4.1200233 -4.0966487 -4.0933456 -4.1074877 -4.1247191 -4.1399641 -4.1459966 -4.1326313 -4.1335754 -4.180584 -4.23939 -4.2800531 -4.3028965 -4.3139248 -4.3348165][-4.1386709 -4.1128535 -4.1123748 -4.1245317 -4.1236644 -4.1166692 -4.1027918 -4.0719295 -4.0633693 -4.1091571 -4.1808128 -4.2400475 -4.2785993 -4.3014359 -4.3313966][-4.1511984 -4.1229963 -4.1185732 -4.1176262 -4.0962148 -4.0718679 -4.0422516 -3.9976037 -3.982846 -4.0315309 -4.1172962 -4.19448 -4.2488322 -4.2860332 -4.3246117][-4.1475472 -4.1182909 -4.1087489 -4.091042 -4.0577083 -4.0334339 -4.0038328 -3.9593427 -3.948549 -4.0041375 -4.0971885 -4.182744 -4.2410264 -4.2836604 -4.3223743][-4.1348085 -4.1079397 -4.0960789 -4.0696507 -4.0358891 -4.0191832 -4.0030179 -3.9788268 -3.9867384 -4.046042 -4.135325 -4.2159109 -4.2690568 -4.3049459 -4.3340092][-4.12287 -4.0998325 -4.0871263 -4.0605803 -4.0362267 -4.0290771 -4.0278554 -4.0296931 -4.055264 -4.1128464 -4.1900916 -4.260252 -4.3033266 -4.3278441 -4.3465362][-4.1423278 -4.1192431 -4.1020827 -4.0783868 -4.0674219 -4.0713243 -4.078196 -4.0945077 -4.1259947 -4.173357 -4.2343812 -4.2907982 -4.3234558 -4.3396077 -4.3530755][-4.1864452 -4.1621637 -4.1366639 -4.1098194 -4.1034293 -4.1126046 -4.1233964 -4.1455021 -4.1767216 -4.2133594 -4.2594862 -4.3035426 -4.3269649 -4.33867 -4.3539658][-4.2161613 -4.1946387 -4.1666703 -4.1380353 -4.1313448 -4.1396837 -4.1518149 -4.1765203 -4.207499 -4.2361569 -4.2688422 -4.3003197 -4.3176823 -4.3317246 -4.3517103][-4.2139978 -4.1988473 -4.1765232 -4.152585 -4.1454411 -4.1480889 -4.1581125 -4.1851754 -4.2164893 -4.2418776 -4.264091 -4.2861047 -4.3006234 -4.3186021 -4.3440986][-4.1872058 -4.1738644 -4.1573009 -4.1440668 -4.1407356 -4.1403623 -4.1476727 -4.1748147 -4.2076211 -4.231956 -4.2449837 -4.26107 -4.2764187 -4.299808 -4.3330226]]...]
INFO - root - 2017-12-05 21:56:42.119965: step 44010, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 72h:39m:50s remains)
INFO - root - 2017-12-05 21:56:51.249338: step 44020, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 72h:34m:42s remains)
INFO - root - 2017-12-05 21:57:00.412769: step 44030, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.950 sec/batch; 76h:07m:07s remains)
INFO - root - 2017-12-05 21:57:09.521110: step 44040, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 73h:36m:47s remains)
INFO - root - 2017-12-05 21:57:18.584300: step 44050, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 72h:55m:02s remains)
INFO - root - 2017-12-05 21:57:27.729927: step 44060, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 72h:28m:30s remains)
INFO - root - 2017-12-05 21:57:36.861063: step 44070, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.970 sec/batch; 77h:44m:07s remains)
INFO - root - 2017-12-05 21:57:45.886824: step 44080, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.946 sec/batch; 75h:45m:16s remains)
INFO - root - 2017-12-05 21:57:55.014652: step 44090, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 75h:08m:03s remains)
INFO - root - 2017-12-05 21:58:04.042357: step 44100, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 66h:01m:41s remains)
2017-12-05 21:58:04.793956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2628665 -4.2748089 -4.2598567 -4.2361531 -4.2063332 -4.1850343 -4.1873817 -4.1992455 -4.1998096 -4.1876593 -4.1814613 -4.1918221 -4.2251978 -4.2726865 -4.3170762][-4.2447677 -4.2612405 -4.2415662 -4.2112803 -4.1788688 -4.1547289 -4.14971 -4.1561689 -4.1563787 -4.1461806 -4.1411223 -4.1572452 -4.2028818 -4.2633505 -4.3164439][-4.2294588 -4.2523842 -4.2361822 -4.2052932 -4.1681523 -4.136313 -4.1186757 -4.1165996 -4.1180968 -4.1130719 -4.1110821 -4.1304297 -4.1853518 -4.2547283 -4.314436][-4.2205639 -4.2523303 -4.2385192 -4.2082677 -4.1681528 -4.1308784 -4.1017323 -4.0913157 -4.0930095 -4.0861983 -4.0816054 -4.105722 -4.1682396 -4.2429266 -4.3090248][-4.21499 -4.2501497 -4.23824 -4.2125072 -4.1771955 -4.1464586 -4.1114531 -4.0916638 -4.0933304 -4.0797553 -4.0668063 -4.0920906 -4.1578326 -4.2355514 -4.3043337][-4.21646 -4.2489409 -4.2334237 -4.2061443 -4.1779432 -4.1575117 -4.1236515 -4.09653 -4.0970411 -4.0816336 -4.0698152 -4.1000848 -4.164072 -4.2405553 -4.3067837][-4.223928 -4.2443471 -4.2227488 -4.1926389 -4.1677628 -4.1530867 -4.1224976 -4.0901923 -4.0855961 -4.072155 -4.0718393 -4.114058 -4.17928 -4.2526741 -4.3158045][-4.2370267 -4.2426996 -4.2140489 -4.1829152 -4.1557875 -4.1376958 -4.103447 -4.0706487 -4.0570059 -4.0412626 -4.0534153 -4.113328 -4.1890292 -4.2626696 -4.324759][-4.2492142 -4.2423697 -4.212153 -4.1846795 -4.1557112 -4.1306677 -4.0928984 -4.0573568 -4.0268707 -3.9977384 -4.0149589 -4.0952916 -4.1884475 -4.2687182 -4.3305807][-4.2348127 -4.21577 -4.1897464 -4.1690688 -4.145463 -4.1237292 -4.0983047 -4.0743394 -4.0320086 -3.9848363 -3.9946113 -4.0816669 -4.1843934 -4.2720838 -4.335278][-4.205894 -4.1798596 -4.1583624 -4.142581 -4.1224508 -4.1043677 -4.0947323 -4.088655 -4.0558996 -4.0097227 -4.0070753 -4.0830183 -4.1869011 -4.2779446 -4.3414345][-4.198307 -4.1733031 -4.1549444 -4.1413274 -4.1257229 -4.1085844 -4.0994816 -4.0991793 -4.0823178 -4.0556908 -4.0552554 -4.1151333 -4.2057681 -4.2909307 -4.3495708][-4.2198362 -4.2019887 -4.1886926 -4.1794262 -4.1649375 -4.1461816 -4.1345592 -4.1316376 -4.1257439 -4.1199441 -4.1287374 -4.1748104 -4.2434096 -4.3106194 -4.3577647][-4.2521505 -4.2437282 -4.2385211 -4.2347169 -4.222858 -4.2052221 -4.1903725 -4.1841054 -4.1855416 -4.192512 -4.2069149 -4.2420845 -4.2888079 -4.3341532 -4.3649244][-4.2847929 -4.2829828 -4.2827892 -4.2830443 -4.2775974 -4.2663164 -4.2547259 -4.2487745 -4.2509236 -4.2594018 -4.2724237 -4.2957335 -4.3241668 -4.3507152 -4.3675394]]...]
INFO - root - 2017-12-05 21:58:13.877265: step 44110, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 71h:46m:01s remains)
INFO - root - 2017-12-05 21:58:22.892922: step 44120, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 72h:50m:48s remains)
INFO - root - 2017-12-05 21:58:32.098144: step 44130, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 74h:44m:02s remains)
INFO - root - 2017-12-05 21:58:41.338710: step 44140, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 73h:28m:43s remains)
INFO - root - 2017-12-05 21:58:50.406470: step 44150, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 76h:10m:56s remains)
INFO - root - 2017-12-05 21:58:59.566134: step 44160, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 72h:22m:05s remains)
INFO - root - 2017-12-05 21:59:08.606847: step 44170, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 72h:27m:13s remains)
INFO - root - 2017-12-05 21:59:17.555369: step 44180, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 73h:45m:11s remains)
INFO - root - 2017-12-05 21:59:26.644411: step 44190, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.936 sec/batch; 74h:58m:39s remains)
INFO - root - 2017-12-05 21:59:35.905676: step 44200, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 73h:45m:13s remains)
2017-12-05 21:59:36.658675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2989135 -4.306396 -4.3116617 -4.3065634 -4.2979259 -4.2945843 -4.3004069 -4.30895 -4.3123121 -4.3079863 -4.2997971 -4.29729 -4.3013177 -4.3098454 -4.3213596][-4.2746119 -4.2855563 -4.2922454 -4.2799435 -4.2614541 -4.2525716 -4.2614903 -4.2781205 -4.2875872 -4.2861843 -4.2786274 -4.276125 -4.2818761 -4.2936091 -4.3090668][-4.2378697 -4.2566819 -4.266047 -4.2458243 -4.2148628 -4.1958752 -4.20352 -4.227684 -4.2467775 -4.2536678 -4.2508278 -4.2520132 -4.262609 -4.2765284 -4.294529][-4.1863003 -4.2186322 -4.2349052 -4.2085161 -4.1595325 -4.1245108 -4.1262841 -4.1577592 -4.1935749 -4.2158036 -4.2245493 -4.233285 -4.2489152 -4.2649422 -4.2837458][-4.1364784 -4.1816959 -4.2060351 -4.1760149 -4.1063857 -4.0419178 -4.0256696 -4.0635552 -4.1230316 -4.1706142 -4.1949334 -4.2099085 -4.2310486 -4.2522211 -4.2733221][-4.1066113 -4.1558681 -4.1857457 -4.1562204 -4.06429 -3.9565365 -3.9016516 -3.9387927 -4.0262737 -4.1085443 -4.1587663 -4.1859679 -4.21199 -4.2378945 -4.2624249][-4.0934343 -4.1459007 -4.1812472 -4.157279 -4.0530181 -3.9056025 -3.7999387 -3.8243337 -3.9350963 -4.04569 -4.1219268 -4.1633472 -4.1929984 -4.2216291 -4.249711][-4.0939293 -4.1476808 -4.1844382 -4.1687589 -4.0722957 -3.9201586 -3.7925293 -3.7935867 -3.8982472 -4.0134144 -4.0973954 -4.143631 -4.1718535 -4.2000914 -4.2325811][-4.0924721 -4.1435046 -4.1823273 -4.180131 -4.110075 -3.98727 -3.8768437 -3.8606994 -3.9297614 -4.0192771 -4.0888939 -4.1287689 -4.1529245 -4.1797886 -4.21676][-4.0629487 -4.1151886 -4.1601219 -4.1746 -4.1398172 -4.0615244 -3.9850535 -3.9658215 -3.9998722 -4.0550141 -4.1011887 -4.1298122 -4.149909 -4.1748171 -4.2125316][-4.0264225 -4.0842919 -4.135613 -4.1630659 -4.1568251 -4.1178989 -4.0760765 -4.0640364 -4.0779352 -4.1072135 -4.1332927 -4.1514544 -4.1688828 -4.19207 -4.2264805][-4.0348763 -4.0918941 -4.1434426 -4.1731272 -4.1803107 -4.1677389 -4.1519547 -4.1512866 -4.1598797 -4.1759734 -4.1895638 -4.1995792 -4.2133307 -4.2323785 -4.2587414][-4.1173038 -4.1596012 -4.1989112 -4.2210636 -4.22866 -4.2265058 -4.2250967 -4.2317557 -4.2415628 -4.2526493 -4.2587996 -4.2630548 -4.2718182 -4.28392 -4.2991281][-4.2294145 -4.2545738 -4.2772913 -4.2889657 -4.2902946 -4.2868319 -4.2880425 -4.2965631 -4.30785 -4.3177876 -4.3204284 -4.3200965 -4.3229971 -4.3275151 -4.3322372][-4.3125529 -4.3257418 -4.3352456 -4.3384223 -4.3355408 -4.3302569 -4.3296981 -4.3355432 -4.3452268 -4.3530993 -4.3543 -4.3526978 -4.3528805 -4.3532629 -4.3521681]]...]
INFO - root - 2017-12-05 21:59:45.685713: step 44210, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 75h:50m:38s remains)
INFO - root - 2017-12-05 21:59:54.715699: step 44220, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 73h:23m:02s remains)
INFO - root - 2017-12-05 22:00:03.778738: step 44230, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 71h:28m:05s remains)
INFO - root - 2017-12-05 22:00:12.892832: step 44240, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 74h:21m:04s remains)
INFO - root - 2017-12-05 22:00:22.195552: step 44250, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.946 sec/batch; 75h:43m:01s remains)
INFO - root - 2017-12-05 22:00:31.411802: step 44260, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 73h:14m:05s remains)
INFO - root - 2017-12-05 22:00:40.360680: step 44270, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 73h:26m:51s remains)
INFO - root - 2017-12-05 22:00:49.523208: step 44280, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.886 sec/batch; 70h:57m:27s remains)
INFO - root - 2017-12-05 22:00:58.630473: step 44290, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.872 sec/batch; 69h:48m:09s remains)
INFO - root - 2017-12-05 22:01:07.795996: step 44300, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 74h:50m:09s remains)
2017-12-05 22:01:08.634582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2085032 -4.2082887 -4.2159042 -4.2282162 -4.2449393 -4.251883 -4.2497339 -4.242785 -4.2240257 -4.1989179 -4.1857061 -4.1937675 -4.2102308 -4.2223186 -4.2298269][-4.1986923 -4.1931748 -4.2025857 -4.2193956 -4.2376184 -4.2444048 -4.2391615 -4.2295365 -4.2094793 -4.1794076 -4.1593218 -4.1688375 -4.1916761 -4.2089534 -4.2157607][-4.2105765 -4.2019806 -4.2061911 -4.2182088 -4.2303309 -4.2330971 -4.226028 -4.2154889 -4.1977911 -4.171267 -4.150269 -4.1579242 -4.181263 -4.2011666 -4.2082372][-4.2281933 -4.2212071 -4.2172537 -4.2189803 -4.2161703 -4.2049594 -4.1886344 -4.1791229 -4.1717916 -4.1607833 -4.1472425 -4.1482692 -4.1667089 -4.1868176 -4.1976652][-4.2285566 -4.2230425 -4.2083488 -4.19488 -4.1764655 -4.1515741 -4.1212883 -4.1071687 -4.1117668 -4.123312 -4.1240063 -4.12367 -4.134367 -4.1545787 -4.1735487][-4.2255077 -4.2195668 -4.1971335 -4.171535 -4.135551 -4.0873241 -4.024745 -3.9889772 -4.0074153 -4.052906 -4.0776787 -4.0852208 -4.0909185 -4.1110315 -4.1363735][-4.218926 -4.210681 -4.1833849 -4.1498113 -4.0973415 -4.0117011 -3.8955421 -3.8196359 -3.8626685 -3.966033 -4.0311956 -4.0534205 -4.0581131 -4.0796394 -4.1099534][-4.1982608 -4.1904583 -4.163023 -4.12673 -4.0670948 -3.9603107 -3.7992165 -3.6786377 -3.7504411 -3.9078093 -4.0052667 -4.037951 -4.0446057 -4.068675 -4.1014824][-4.1656065 -4.1667819 -4.1488891 -4.1207657 -4.0718222 -3.9852629 -3.8453572 -3.732321 -3.7838936 -3.9221022 -4.0114279 -4.0421352 -4.0516429 -4.0791888 -4.1109715][-4.140635 -4.1474876 -4.1378126 -4.1180058 -4.0874395 -4.0414114 -3.9646504 -3.8950603 -3.9148657 -3.9944766 -4.0495977 -4.0647659 -4.0645766 -4.0875149 -4.1180587][-4.1431909 -4.1469865 -4.1402116 -4.1245241 -4.1117878 -4.107676 -4.0873861 -4.0519681 -4.0487885 -4.0828037 -4.1042862 -4.1009941 -4.0889659 -4.101408 -4.12788][-4.1690245 -4.1683831 -4.164422 -4.1531038 -4.1551204 -4.1759291 -4.1843886 -4.1711507 -4.1598096 -4.16553 -4.1638169 -4.143702 -4.1199508 -4.121037 -4.142344][-4.2159114 -4.2138896 -4.2091832 -4.2001028 -4.205956 -4.2344365 -4.2540388 -4.2526026 -4.241982 -4.2351665 -4.2186828 -4.1904383 -4.1617112 -4.1509223 -4.1652508][-4.2619171 -4.2561955 -4.2458663 -4.2314343 -4.2352729 -4.2638721 -4.2877889 -4.2924619 -4.2851505 -4.2731071 -4.2495151 -4.2208104 -4.1933842 -4.1811571 -4.1936736][-4.2802248 -4.2684855 -4.253335 -4.2397823 -4.2444839 -4.2718568 -4.29515 -4.2992244 -4.2924118 -4.2792215 -4.2567859 -4.229104 -4.2058296 -4.1973453 -4.2103]]...]
INFO - root - 2017-12-05 22:01:17.596517: step 44310, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 73h:45m:08s remains)
INFO - root - 2017-12-05 22:01:26.676138: step 44320, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 75h:39m:24s remains)
INFO - root - 2017-12-05 22:01:35.703988: step 44330, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 72h:24m:12s remains)
INFO - root - 2017-12-05 22:01:44.774601: step 44340, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 73h:39m:53s remains)
INFO - root - 2017-12-05 22:01:53.995871: step 44350, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 73h:51m:44s remains)
INFO - root - 2017-12-05 22:02:03.017609: step 44360, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 65h:14m:25s remains)
INFO - root - 2017-12-05 22:02:12.069021: step 44370, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.949 sec/batch; 75h:57m:19s remains)
INFO - root - 2017-12-05 22:02:21.224289: step 44380, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 71h:14m:44s remains)
INFO - root - 2017-12-05 22:02:30.289074: step 44390, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 72h:12m:43s remains)
INFO - root - 2017-12-05 22:02:39.281553: step 44400, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 71h:31m:23s remains)
2017-12-05 22:02:40.086109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2547956 -4.2435179 -4.2327433 -4.2209682 -4.2112732 -4.2037129 -4.1995845 -4.1960015 -4.1956682 -4.2004356 -4.2098327 -4.2183409 -4.225832 -4.230916 -4.2302814][-4.2509437 -4.2376232 -4.2245064 -4.2124128 -4.2042336 -4.1978874 -4.19181 -4.1842823 -4.1803856 -4.1821833 -4.1906056 -4.2007723 -4.2113085 -4.220284 -4.2227588][-4.2471104 -4.231914 -4.2146311 -4.2015181 -4.196167 -4.1946659 -4.1918235 -4.1858845 -4.1813064 -4.1784697 -4.1801195 -4.1860991 -4.1964216 -4.2086997 -4.2161922][-4.2452273 -4.2289929 -4.2077451 -4.1921873 -4.1885715 -4.1921091 -4.19517 -4.1953673 -4.1941614 -4.1883655 -4.1804023 -4.1775084 -4.1842256 -4.1975274 -4.20941][-4.246398 -4.2310309 -4.208282 -4.1889348 -4.1830416 -4.1873374 -4.1943359 -4.2020097 -4.2076197 -4.2035437 -4.1903806 -4.1799445 -4.1806192 -4.1899147 -4.2007322][-4.250505 -4.2383385 -4.2172046 -4.1936812 -4.1806388 -4.1773329 -4.1802869 -4.1919308 -4.2060122 -4.2101488 -4.2004619 -4.1889386 -4.1844559 -4.1861229 -4.1914582][-4.2552209 -4.2477579 -4.2313609 -4.2065268 -4.183969 -4.16738 -4.1585374 -4.1669869 -4.1872687 -4.2019525 -4.202055 -4.1960635 -4.1897607 -4.1848888 -4.1838059][-4.2553368 -4.2512083 -4.2398438 -4.21713 -4.19054 -4.1650343 -4.1457796 -4.1482935 -4.1681242 -4.1855774 -4.191062 -4.190062 -4.1850524 -4.17965 -4.1771502][-4.2501469 -4.2458129 -4.2376857 -4.2206616 -4.1984649 -4.1773057 -4.1610117 -4.1604729 -4.171783 -4.1809344 -4.1816978 -4.1792197 -4.1746554 -4.1721153 -4.1738224][-4.2414875 -4.2329712 -4.2253151 -4.2158451 -4.2054086 -4.1988087 -4.1949687 -4.195972 -4.1975904 -4.1932478 -4.1828289 -4.1719952 -4.1628613 -4.1595793 -4.1639071][-4.234426 -4.2186775 -4.2078323 -4.2050781 -4.206161 -4.2120996 -4.2182178 -4.2220335 -4.2188234 -4.2071328 -4.1894054 -4.1717238 -4.1569057 -4.1499333 -4.1537213][-4.2327623 -4.2097063 -4.1919255 -4.1900606 -4.195662 -4.20696 -4.2172751 -4.2235365 -4.2206469 -4.2099848 -4.1954427 -4.1791286 -4.1629939 -4.1522107 -4.1514025][-4.2370977 -4.2087078 -4.1832376 -4.1762857 -4.1790242 -4.1892962 -4.20057 -4.2102518 -4.2107859 -4.2044516 -4.1965456 -4.1876206 -4.1768231 -4.1664224 -4.1605878][-4.2457933 -4.216855 -4.1885443 -4.1756339 -4.1728048 -4.1780238 -4.1869168 -4.1970959 -4.1974096 -4.1917219 -4.1881113 -4.188302 -4.18826 -4.1847687 -4.1781325][-4.2543917 -4.2292309 -4.2031274 -4.1885743 -4.1828918 -4.1839175 -4.189014 -4.1957 -4.1925678 -4.1850982 -4.1831779 -4.1895151 -4.2007093 -4.2059669 -4.2017236]]...]
INFO - root - 2017-12-05 22:02:49.046409: step 44410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 69h:03m:34s remains)
INFO - root - 2017-12-05 22:02:57.988859: step 44420, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 71h:14m:57s remains)
INFO - root - 2017-12-05 22:03:07.112627: step 44430, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 74h:19m:29s remains)
INFO - root - 2017-12-05 22:03:16.308245: step 44440, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 73h:24m:10s remains)
INFO - root - 2017-12-05 22:03:25.435965: step 44450, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 73h:36m:24s remains)
INFO - root - 2017-12-05 22:03:34.282736: step 44460, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 70h:05m:13s remains)
INFO - root - 2017-12-05 22:03:43.364728: step 44470, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 75h:25m:52s remains)
INFO - root - 2017-12-05 22:03:52.505107: step 44480, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 72h:36m:14s remains)
INFO - root - 2017-12-05 22:04:01.717769: step 44490, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.900 sec/batch; 71h:57m:47s remains)
INFO - root - 2017-12-05 22:04:10.749622: step 44500, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 72h:32m:21s remains)
2017-12-05 22:04:11.535208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3515215 -4.3506713 -4.3493881 -4.3479285 -4.3473792 -4.3485165 -4.3505478 -4.3522582 -4.3544431 -4.3568397 -4.3595071 -4.3615794 -4.3611164 -4.359098 -4.3561878][-4.34237 -4.3427062 -4.3422208 -4.3412929 -4.341023 -4.3405447 -4.3389058 -4.337657 -4.3377409 -4.340322 -4.3456559 -4.351512 -4.3533163 -4.3519917 -4.3486023][-4.3293257 -4.3307323 -4.3323145 -4.3341393 -4.3339982 -4.3277607 -4.3181624 -4.3085589 -4.3006086 -4.2982903 -4.3044491 -4.316246 -4.3260064 -4.3326468 -4.3350964][-4.309114 -4.3087268 -4.3109713 -4.3142896 -4.3120317 -4.2970705 -4.2742786 -4.2522221 -4.2321033 -4.2213511 -4.2290163 -4.2488308 -4.2708688 -4.293313 -4.3105044][-4.2798228 -4.2696385 -4.2656865 -4.2653203 -4.2548685 -4.226922 -4.1882219 -4.154098 -4.1270075 -4.1182165 -4.1324692 -4.1616888 -4.1974816 -4.2402172 -4.2809305][-4.2392206 -4.2107558 -4.1890945 -4.1742 -4.1491385 -4.1080546 -4.052669 -4.0118308 -3.9942465 -4.0036669 -4.0327916 -4.0712047 -4.1185813 -4.1752229 -4.2349048][-4.1828642 -4.1276231 -4.0798535 -4.0485716 -4.0119262 -3.9581075 -3.8839169 -3.8390288 -3.8502617 -3.8953426 -3.9400353 -3.9918036 -4.0506935 -4.1201086 -4.1923838][-4.1076841 -4.0209775 -3.9528089 -3.9142997 -3.8734531 -3.8125665 -3.7269528 -3.6846864 -3.7383513 -3.8229496 -3.8884356 -3.9605947 -4.0334725 -4.1081023 -4.1816583][-4.0350566 -3.9247706 -3.8516476 -3.8257904 -3.802011 -3.7593706 -3.6918368 -3.6670854 -3.7387805 -3.8329332 -3.9060717 -3.9857576 -4.0599689 -4.1308665 -4.2018094][-4.0204825 -3.9128091 -3.8570611 -3.8593163 -3.863652 -3.8559058 -3.8386579 -3.8395946 -3.8886814 -3.9515638 -4.0105691 -4.0777283 -4.1367478 -4.1912541 -4.2493277][-4.0967016 -4.02132 -3.9927144 -4.0119987 -4.0315909 -4.044136 -4.0586624 -4.0721145 -4.0933981 -4.1210108 -4.1563697 -4.2016468 -4.2422786 -4.27699 -4.3087921][-4.2096324 -4.1694 -4.1612668 -4.1826453 -4.2026086 -4.218286 -4.234879 -4.2497692 -4.2598553 -4.2670503 -4.2793221 -4.3004 -4.3224196 -4.3399706 -4.3510509][-4.2962141 -4.2804794 -4.2834616 -4.3002973 -4.3158031 -4.3265471 -4.33668 -4.3474631 -4.3534722 -4.3536968 -4.353117 -4.3579431 -4.3653545 -4.3699136 -4.3680072][-4.3409348 -4.3357034 -4.3395419 -4.3502941 -4.3616109 -4.3680677 -4.370718 -4.3741565 -4.3767304 -4.3747325 -4.3702536 -4.3681316 -4.3690705 -4.3691416 -4.3658166][-4.3569283 -4.3554387 -4.3567967 -4.360918 -4.3655372 -4.3673615 -4.3672714 -4.3680615 -4.3684196 -4.3670578 -4.3644218 -4.362371 -4.3615179 -4.3609409 -4.3602366]]...]
INFO - root - 2017-12-05 22:04:20.708029: step 44510, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 74h:15m:11s remains)
INFO - root - 2017-12-05 22:04:29.525599: step 44520, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 71h:45m:51s remains)
INFO - root - 2017-12-05 22:04:38.430908: step 44530, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 70h:50m:44s remains)
INFO - root - 2017-12-05 22:04:47.595954: step 44540, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 74h:05m:43s remains)
INFO - root - 2017-12-05 22:04:56.602134: step 44550, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 71h:16m:53s remains)
INFO - root - 2017-12-05 22:05:05.559577: step 44560, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 71h:26m:02s remains)
INFO - root - 2017-12-05 22:05:14.630990: step 44570, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 72h:30m:15s remains)
INFO - root - 2017-12-05 22:05:23.722814: step 44580, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 74h:01m:02s remains)
INFO - root - 2017-12-05 22:05:32.924840: step 44590, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.925 sec/batch; 73h:56m:41s remains)
INFO - root - 2017-12-05 22:05:41.865500: step 44600, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 72h:13m:25s remains)
2017-12-05 22:05:42.694034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3118939 -4.313015 -4.3145642 -4.3169894 -4.3205285 -4.3249764 -4.3285689 -4.3303952 -4.3301768 -4.3294592 -4.3295894 -4.3311238 -4.3339376 -4.3361707 -4.3365474][-4.2878304 -4.2881684 -4.2897477 -4.2932005 -4.2991 -4.3071189 -4.3135276 -4.3168697 -4.3163195 -4.3158941 -4.3173513 -4.3198977 -4.323545 -4.3269839 -4.3281274][-4.2536025 -4.2504663 -4.2506762 -4.2555408 -4.264554 -4.276576 -4.285316 -4.2893367 -4.2901278 -4.2925825 -4.2970862 -4.3007011 -4.3063846 -4.3122878 -4.3148642][-4.2168794 -4.2040777 -4.1998868 -4.206872 -4.2198477 -4.23276 -4.2395988 -4.2431788 -4.2486563 -4.2577391 -4.2661295 -4.2717462 -4.2809343 -4.2912416 -4.29653][-4.1896377 -4.1648345 -4.1536765 -4.1618109 -4.1761594 -4.1834364 -4.1824322 -4.1838865 -4.1984286 -4.2195044 -4.2327185 -4.2405624 -4.2545791 -4.2677956 -4.2765355][-4.16995 -4.1355991 -4.1178751 -4.123702 -4.1335182 -4.1307912 -4.1170635 -4.1145225 -4.1402874 -4.1767507 -4.195231 -4.2043676 -4.2245946 -4.2427597 -4.2577791][-4.1554556 -4.1203208 -4.0985904 -4.0976534 -4.0941539 -4.0743532 -4.0399613 -4.0322895 -4.0723767 -4.1293044 -4.1569991 -4.1677036 -4.1922917 -4.2175274 -4.2426405][-4.1426759 -4.1101804 -4.0869813 -4.0798922 -4.058589 -4.0130606 -3.9517336 -3.9434826 -4.0059 -4.0877347 -4.1267552 -4.1366949 -4.1581664 -4.1888566 -4.2239771][-4.1306605 -4.1007504 -4.0788364 -4.0708437 -4.0417929 -3.9786131 -3.9001503 -3.9026046 -3.9823532 -4.0716629 -4.10555 -4.1034961 -4.1141081 -4.147512 -4.1905632][-4.1253805 -4.094018 -4.0709786 -4.0641332 -4.0427475 -3.9896524 -3.9261959 -3.944988 -4.0203438 -4.0881047 -4.1000409 -4.0799913 -4.0782266 -4.108561 -4.1524386][-4.1295247 -4.093945 -4.0684285 -4.0601659 -4.053721 -4.0283775 -3.9989374 -4.025301 -4.0757895 -4.1117797 -4.1012349 -4.069603 -4.0591645 -4.0796828 -4.1148615][-4.1512375 -4.1138725 -4.0888448 -4.0796142 -4.0814009 -4.0720177 -4.0641618 -4.0866804 -4.1140919 -4.1279583 -4.1103044 -4.084341 -4.0762696 -4.0852242 -4.1030593][-4.1946554 -4.1575618 -4.1346545 -4.1285706 -4.1328812 -4.1298795 -4.131072 -4.1468673 -4.1613669 -4.1658511 -4.1533341 -4.1416368 -4.1392145 -4.1400557 -4.1422176][-4.247716 -4.2173944 -4.1989403 -4.1951814 -4.19952 -4.2014656 -4.2082844 -4.2198071 -4.2296104 -4.2322536 -4.2257781 -4.2233062 -4.2248306 -4.2227821 -4.2173586][-4.2952757 -4.2777081 -4.2669926 -4.266006 -4.2694287 -4.274291 -4.2826877 -4.290451 -4.29705 -4.2997308 -4.2971644 -4.2978535 -4.300312 -4.2982717 -4.2912869]]...]
INFO - root - 2017-12-05 22:05:51.802981: step 44610, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 75h:16m:07s remains)
INFO - root - 2017-12-05 22:06:00.885504: step 44620, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 72h:22m:52s remains)
INFO - root - 2017-12-05 22:06:09.743161: step 44630, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 71h:31m:47s remains)
INFO - root - 2017-12-05 22:06:18.831980: step 44640, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 75h:29m:38s remains)
INFO - root - 2017-12-05 22:06:27.927600: step 44650, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 72h:46m:47s remains)
INFO - root - 2017-12-05 22:06:36.861276: step 44660, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 73h:07m:24s remains)
INFO - root - 2017-12-05 22:06:45.918127: step 44670, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 75h:17m:17s remains)
INFO - root - 2017-12-05 22:06:55.103133: step 44680, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.943 sec/batch; 75h:25m:04s remains)
INFO - root - 2017-12-05 22:07:04.177616: step 44690, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 75h:11m:53s remains)
INFO - root - 2017-12-05 22:07:13.121547: step 44700, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 68h:57m:46s remains)
2017-12-05 22:07:13.965810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.282619 -4.2890339 -4.2867603 -4.284049 -4.2816858 -4.2762728 -4.2671356 -4.2553821 -4.2430849 -4.237565 -4.2401938 -4.2456269 -4.2529507 -4.2637458 -4.2808309][-4.2446814 -4.2511826 -4.2488084 -4.2465787 -4.2452407 -4.2412133 -4.227706 -4.2075171 -4.1861925 -4.1788468 -4.1865249 -4.1992917 -4.2092614 -4.2212162 -4.2443681][-4.2153282 -4.2211113 -4.2175179 -4.2141075 -4.2121954 -4.207952 -4.1869287 -4.1552405 -4.1278777 -4.1236019 -4.13891 -4.1586113 -4.1737204 -4.1920271 -4.2226834][-4.1852012 -4.1937647 -4.1897664 -4.1836185 -4.1792068 -4.1703033 -4.13752 -4.0951705 -4.0714149 -4.0773072 -4.0935936 -4.1106324 -4.13318 -4.1671753 -4.20912][-4.1340141 -4.1499219 -4.1484871 -4.1413732 -4.1321239 -4.11454 -4.0704327 -4.0269122 -4.0168142 -4.0385251 -4.052721 -4.05809 -4.082365 -4.1334653 -4.18919][-4.0784378 -4.1025624 -4.0983925 -4.0845718 -4.0663428 -4.034801 -3.9789023 -3.9329321 -3.9389689 -3.9763668 -3.9895499 -3.9831567 -4.0094504 -4.0770769 -4.1490965][-4.0494375 -4.0727868 -4.0583673 -4.0326238 -4.0051064 -3.963599 -3.8942876 -3.8409534 -3.8553939 -3.905091 -3.9188342 -3.90348 -3.9340394 -4.0175004 -4.102757][-4.0495863 -4.0670562 -4.0432019 -4.0157342 -3.9921207 -3.952668 -3.8815269 -3.8244781 -3.8340559 -3.8826118 -3.888062 -3.8604374 -3.8919919 -3.9817944 -4.0667725][-4.0566039 -4.0695529 -4.0507393 -4.0415812 -4.0436559 -4.0279779 -3.978718 -3.9348502 -3.9351232 -3.9653587 -3.9508548 -3.90513 -3.9241054 -3.9986966 -4.0643735][-4.0717397 -4.0818868 -4.0746069 -4.08803 -4.1137681 -4.1247787 -4.1069016 -4.0896106 -4.0948915 -4.1052976 -4.0678897 -4.0057192 -4.0068083 -4.0562172 -4.0977902][-4.0831723 -4.0914836 -4.0977287 -4.1308188 -4.1686683 -4.1901584 -4.1918254 -4.1967816 -4.2101836 -4.2083521 -4.1559381 -4.0847859 -4.0735345 -4.1054978 -4.1328321][-4.0900254 -4.1002688 -4.1190281 -4.1669755 -4.2093358 -4.2289448 -4.2363253 -4.24959 -4.2631335 -4.2526226 -4.1955791 -4.1237521 -4.1059761 -4.1299734 -4.1538792][-4.1010742 -4.114748 -4.1403942 -4.1885214 -4.2313161 -4.250762 -4.2568274 -4.2718129 -4.2844367 -4.2738442 -4.2220111 -4.1551127 -4.1338215 -4.1493216 -4.1685314][-4.1397033 -4.1549497 -4.1807652 -4.2173414 -4.2534084 -4.2715268 -4.278676 -4.2924981 -4.30546 -4.3006086 -4.2612906 -4.2071357 -4.1861868 -4.1932383 -4.2093773][-4.2036309 -4.2140584 -4.230217 -4.2522292 -4.2795682 -4.2950597 -4.30161 -4.3131275 -4.3250065 -4.3248219 -4.301446 -4.2666893 -4.2521887 -4.2559795 -4.2683535]]...]
INFO - root - 2017-12-05 22:07:23.046911: step 44710, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 71h:46m:25s remains)
INFO - root - 2017-12-05 22:07:32.108553: step 44720, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 74h:02m:48s remains)
INFO - root - 2017-12-05 22:07:41.081129: step 44730, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 72h:10m:15s remains)
INFO - root - 2017-12-05 22:07:50.084144: step 44740, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 74h:55m:26s remains)
INFO - root - 2017-12-05 22:07:59.182542: step 44750, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 71h:58m:57s remains)
INFO - root - 2017-12-05 22:08:08.237524: step 44760, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 73h:28m:43s remains)
INFO - root - 2017-12-05 22:08:17.275848: step 44770, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 72h:10m:33s remains)
INFO - root - 2017-12-05 22:08:26.578180: step 44780, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 72h:57m:55s remains)
INFO - root - 2017-12-05 22:08:35.734754: step 44790, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 71h:08m:39s remains)
INFO - root - 2017-12-05 22:08:44.728304: step 44800, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 71h:24m:14s remains)
2017-12-05 22:08:45.531231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2066565 -4.1968393 -4.1978626 -4.2049823 -4.2044196 -4.1984277 -4.1933489 -4.1859112 -4.1807609 -4.1792111 -4.1887283 -4.206655 -4.2238173 -4.2358503 -4.2548113][-4.2199531 -4.2086806 -4.20708 -4.2127581 -4.2102084 -4.2049108 -4.2071671 -4.2079349 -4.2058778 -4.2054114 -4.2161903 -4.2288923 -4.2351117 -4.2388039 -4.255259][-4.2234945 -4.215157 -4.2127109 -4.2154284 -4.21213 -4.207325 -4.2072387 -4.2057705 -4.2024946 -4.2057066 -4.2200541 -4.2310739 -4.2325144 -4.2343454 -4.2516103][-4.2294011 -4.223804 -4.2189169 -4.217793 -4.2128448 -4.2068043 -4.2000165 -4.1893854 -4.184752 -4.192296 -4.2094646 -4.2176089 -4.2207789 -4.2278943 -4.2488003][-4.2377882 -4.2300014 -4.2201614 -4.2139668 -4.2078781 -4.1997552 -4.187727 -4.1714144 -4.1623087 -4.1718011 -4.1946721 -4.2090416 -4.2173815 -4.23054 -4.253962][-4.236197 -4.2222147 -4.2066789 -4.1984529 -4.19308 -4.1816244 -4.1645522 -4.142487 -4.1305223 -4.1460419 -4.1790862 -4.2046456 -4.2212439 -4.2391596 -4.2641029][-4.2152877 -4.1905093 -4.1650257 -4.1522117 -4.1441464 -4.1262126 -4.1003532 -4.0691638 -4.0542841 -4.0852895 -4.1390338 -4.1837983 -4.216095 -4.24061 -4.2690239][-4.1969986 -4.1655149 -4.132483 -4.1098652 -4.0944891 -4.0674081 -4.02395 -3.967629 -3.9410839 -3.9911385 -4.0744176 -4.1442556 -4.1926165 -4.2265773 -4.2604284][-4.1890397 -4.1650629 -4.140903 -4.122467 -4.1135483 -4.0952377 -4.0545487 -3.9942305 -3.9611664 -4.00105 -4.0750484 -4.1375251 -4.1821685 -4.2155151 -4.2492318][-4.1942744 -4.1823521 -4.1721048 -4.1695452 -4.174654 -4.171381 -4.1473846 -4.10567 -4.0801587 -4.099328 -4.1419744 -4.1757421 -4.199605 -4.2204742 -4.2479758][-4.208591 -4.207149 -4.2032557 -4.2094846 -4.2237606 -4.22695 -4.2115593 -4.1844783 -4.1671848 -4.1729288 -4.1926746 -4.2087841 -4.2189159 -4.2286992 -4.248435][-4.2005987 -4.2089939 -4.2078414 -4.2158408 -4.2344885 -4.2390137 -4.2273207 -4.2131791 -4.2049475 -4.2048111 -4.2121172 -4.223412 -4.2312188 -4.2365236 -4.2526665][-4.1766324 -4.1889863 -4.194016 -4.2082858 -4.2307096 -4.234148 -4.2243037 -4.2198396 -4.2180176 -4.2153687 -4.2192268 -4.2333546 -4.2429967 -4.2490258 -4.2651167][-4.1506515 -4.1657805 -4.1807466 -4.2024918 -4.2271824 -4.2296028 -4.22145 -4.2183361 -4.2173948 -4.2195272 -4.2297821 -4.2496133 -4.2635827 -4.2721696 -4.2873788][-4.154078 -4.1665092 -4.1813345 -4.199923 -4.2218895 -4.2250428 -4.2168093 -4.2104888 -4.2077031 -4.2142963 -4.2326751 -4.2607164 -4.2808366 -4.2918992 -4.3048844]]...]
INFO - root - 2017-12-05 22:08:54.570295: step 44810, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.932 sec/batch; 74h:27m:40s remains)
INFO - root - 2017-12-05 22:09:03.769733: step 44820, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 73h:01m:56s remains)
INFO - root - 2017-12-05 22:09:12.718404: step 44830, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 70h:47m:20s remains)
INFO - root - 2017-12-05 22:09:21.878850: step 44840, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.902 sec/batch; 72h:04m:52s remains)
INFO - root - 2017-12-05 22:09:30.918546: step 44850, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 72h:50m:29s remains)
INFO - root - 2017-12-05 22:09:40.094506: step 44860, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 76h:00m:05s remains)
INFO - root - 2017-12-05 22:09:49.004385: step 44870, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 70h:48m:00s remains)
INFO - root - 2017-12-05 22:09:57.952388: step 44880, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 71h:20m:39s remains)
INFO - root - 2017-12-05 22:10:07.216515: step 44890, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 75h:14m:33s remains)
INFO - root - 2017-12-05 22:10:16.280127: step 44900, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 74h:02m:14s remains)
2017-12-05 22:10:17.124366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2465634 -4.2409053 -4.2269864 -4.2193322 -4.2143617 -4.2046366 -4.1866636 -4.1671381 -4.1666894 -4.18536 -4.2089081 -4.2293544 -4.2392473 -4.2379379 -4.2374148][-4.2250915 -4.21928 -4.2043886 -4.1968575 -4.1907554 -4.1703143 -4.1409073 -4.1165257 -4.114994 -4.1327381 -4.1601644 -4.1883488 -4.2055354 -4.2057958 -4.2078385][-4.1975112 -4.1961422 -4.1831512 -4.1757932 -4.1633358 -4.1313276 -4.0912695 -4.0705948 -4.0761652 -4.0940351 -4.1232185 -4.1559262 -4.1781983 -4.18066 -4.1835084][-4.1650252 -4.17402 -4.166223 -4.1509366 -4.1263385 -4.0786982 -4.026 -4.012423 -4.0377078 -4.0670223 -4.1055031 -4.1484036 -4.1777763 -4.1834188 -4.1858053][-4.1267643 -4.1408482 -4.1379576 -4.1167431 -4.075151 -3.9979856 -3.9085948 -3.8944595 -3.9550073 -4.0134931 -4.0663271 -4.1232495 -4.1659188 -4.1813445 -4.1876597][-4.0844364 -4.0971193 -4.1021638 -4.0769348 -4.0110259 -3.8895013 -3.7381687 -3.7253966 -3.844008 -3.9440267 -4.0136814 -4.0866065 -4.1471643 -4.1741805 -4.1809483][-4.0568705 -4.0687594 -4.0717363 -4.0346737 -3.9431329 -3.7765169 -3.5710497 -3.5848432 -3.7637055 -3.8968344 -3.9808927 -4.0647168 -4.138535 -4.1719556 -4.1755438][-4.0791669 -4.0867529 -4.0787969 -4.0349851 -3.94046 -3.7825484 -3.6055207 -3.6415946 -3.8129756 -3.9334469 -4.0107441 -4.0847635 -4.1526647 -4.1817284 -4.1767545][-4.1251249 -4.1284943 -4.1187286 -4.0819221 -4.0120821 -3.9099956 -3.8147373 -3.8445573 -3.9468923 -4.0194707 -4.0712953 -4.1239924 -4.1733294 -4.1929593 -4.1805825][-4.1655564 -4.1654763 -4.153513 -4.1217589 -4.0734816 -4.0117145 -3.971236 -3.9954133 -4.0500789 -4.092761 -4.1312103 -4.1711836 -4.2004256 -4.2098169 -4.1946583][-4.2096176 -4.2077489 -4.1844707 -4.1457214 -4.1060667 -4.0656223 -4.0513082 -4.0755849 -4.1122794 -4.1455736 -4.1806808 -4.2131958 -4.2292967 -4.2314405 -4.2191086][-4.2435904 -4.2389112 -4.2084455 -4.1651707 -4.1296582 -4.0953951 -4.0865107 -4.1096635 -4.1459856 -4.1818061 -4.2152715 -4.2421327 -4.2508097 -4.249445 -4.2445908][-4.2630625 -4.2586336 -4.2320418 -4.1918964 -4.159914 -4.1304045 -4.1214924 -4.1416893 -4.1783643 -4.2154989 -4.2470679 -4.2692451 -4.2742786 -4.2720933 -4.2703433][-4.2707729 -4.2688885 -4.249022 -4.216063 -4.1904564 -4.1682787 -4.1593513 -4.1746111 -4.2084851 -4.2402039 -4.2654467 -4.2834439 -4.2876086 -4.2857089 -4.284759][-4.2722688 -4.2718434 -4.2593136 -4.2374277 -4.2212362 -4.2053809 -4.1934218 -4.2010384 -4.2298656 -4.2548938 -4.2733135 -4.2891531 -4.2937841 -4.2918043 -4.2908978]]...]
INFO - root - 2017-12-05 22:10:26.148039: step 44910, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 70h:51m:13s remains)
INFO - root - 2017-12-05 22:10:35.218046: step 44920, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 71h:29m:51s remains)
INFO - root - 2017-12-05 22:10:44.221352: step 44930, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 74h:47m:28s remains)
INFO - root - 2017-12-05 22:10:53.323663: step 44940, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 72h:50m:20s remains)
INFO - root - 2017-12-05 22:11:02.414410: step 44950, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.888 sec/batch; 70h:54m:56s remains)
INFO - root - 2017-12-05 22:11:11.618793: step 44960, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 72h:09m:45s remains)
INFO - root - 2017-12-05 22:11:20.687106: step 44970, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 72h:12m:50s remains)
INFO - root - 2017-12-05 22:11:29.830948: step 44980, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 72h:22m:46s remains)
INFO - root - 2017-12-05 22:11:38.866938: step 44990, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 70h:24m:49s remains)
INFO - root - 2017-12-05 22:11:47.996965: step 45000, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.955 sec/batch; 76h:14m:40s remains)
2017-12-05 22:11:48.839468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1742129 -4.1825385 -4.1913347 -4.1875253 -4.1708245 -4.1588411 -4.158751 -4.166122 -4.1621814 -4.157968 -4.1712737 -4.1784291 -4.1721344 -4.1699591 -4.1688681][-4.1666565 -4.1730328 -4.1851835 -4.1861949 -4.1745906 -4.1696219 -4.1755247 -4.1778822 -4.1673188 -4.1632447 -4.1734862 -4.1696172 -4.154809 -4.1521888 -4.158886][-4.1641932 -4.1695566 -4.182034 -4.1852684 -4.1750045 -4.1712356 -4.1766095 -4.1769156 -4.16975 -4.1721611 -4.1808558 -4.1714907 -4.1501775 -4.1405654 -4.1478739][-4.1620455 -4.1681347 -4.1807303 -4.18641 -4.175561 -4.163713 -4.1616974 -4.1625113 -4.1619787 -4.1717811 -4.1859961 -4.1848712 -4.1662812 -4.1511264 -4.1501255][-4.1565585 -4.1603251 -4.1733403 -4.1841955 -4.1731882 -4.1493473 -4.1345797 -4.1360269 -4.1438241 -4.160111 -4.1827965 -4.1924977 -4.1815104 -4.1659222 -4.1571722][-4.1586156 -4.1582608 -4.1687479 -4.1799617 -4.1702557 -4.1429672 -4.1183925 -4.1184797 -4.1295834 -4.1483746 -4.172287 -4.181778 -4.1776633 -4.1711478 -4.1632767][-4.15772 -4.1554337 -4.1617923 -4.1693153 -4.1603303 -4.1364403 -4.1151028 -4.1182032 -4.1301103 -4.1468139 -4.1630573 -4.1648979 -4.1678596 -4.1743131 -4.1746917][-4.1499004 -4.1476679 -4.1481447 -4.1522622 -4.143878 -4.1239982 -4.1081343 -4.1173491 -4.1336555 -4.1492662 -4.159554 -4.157835 -4.1663003 -4.1817932 -4.1915565][-4.1369247 -4.135808 -4.1355391 -4.1416926 -4.1355424 -4.1143432 -4.0967355 -4.1087856 -4.129745 -4.14656 -4.1608143 -4.1669693 -4.1794243 -4.1957216 -4.209053][-4.1384706 -4.1383886 -4.1388206 -4.14853 -4.1449661 -4.1239452 -4.1018009 -4.1097665 -4.1293902 -4.14604 -4.1686587 -4.1868134 -4.2001314 -4.2078667 -4.2141261][-4.1602349 -4.1586866 -4.1554484 -4.16236 -4.1655736 -4.1548982 -4.136961 -4.1398606 -4.1509194 -4.1611118 -4.1820068 -4.2022138 -4.2125573 -4.2097178 -4.2050161][-4.1759968 -4.1759853 -4.1713428 -4.1745672 -4.1829448 -4.1845589 -4.1760864 -4.1776767 -4.1839981 -4.1873732 -4.197957 -4.2091694 -4.2119489 -4.2013593 -4.1904216][-4.1839628 -4.1814442 -4.1728392 -4.1714282 -4.1823444 -4.1926107 -4.1926827 -4.1980939 -4.2042508 -4.2033353 -4.2033024 -4.2035565 -4.1990981 -4.185606 -4.1740518][-4.1951551 -4.1842556 -4.1642489 -4.1524405 -4.161025 -4.1782093 -4.1912856 -4.2065816 -4.2149782 -4.210268 -4.1997385 -4.1898794 -4.1798086 -4.1657529 -4.1580949][-4.2080183 -4.191422 -4.1653342 -4.1441517 -4.1439753 -4.1644821 -4.1908927 -4.2162461 -4.2253728 -4.2160554 -4.1954503 -4.1735516 -4.1567645 -4.1427217 -4.1400418]]...]
INFO - root - 2017-12-05 22:11:57.817538: step 45010, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 70h:09m:20s remains)
INFO - root - 2017-12-05 22:12:06.827227: step 45020, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 72h:50m:24s remains)
INFO - root - 2017-12-05 22:12:15.825034: step 45030, loss = 2.02, batch loss = 1.97 (8.6 examples/sec; 0.926 sec/batch; 73h:57m:50s remains)
INFO - root - 2017-12-05 22:12:24.913974: step 45040, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 71h:59m:48s remains)
INFO - root - 2017-12-05 22:12:33.956151: step 45050, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 73h:00m:07s remains)
INFO - root - 2017-12-05 22:12:42.841889: step 45060, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 73h:07m:48s remains)
INFO - root - 2017-12-05 22:12:52.028247: step 45070, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 73h:40m:46s remains)
INFO - root - 2017-12-05 22:13:01.033494: step 45080, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 73h:15m:20s remains)
INFO - root - 2017-12-05 22:13:10.053718: step 45090, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.932 sec/batch; 74h:22m:54s remains)
INFO - root - 2017-12-05 22:13:19.294552: step 45100, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 70h:31m:28s remains)
2017-12-05 22:13:20.109130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1701775 -4.1570783 -4.1578026 -4.1618414 -4.1635389 -4.1698518 -4.16288 -4.1390548 -4.1137171 -4.0924444 -4.0840774 -4.0926337 -4.1352644 -4.1864028 -4.204226][-4.058229 -4.0287652 -4.0405769 -4.0586953 -4.0764804 -4.1199269 -4.1641326 -4.1826715 -4.1736283 -4.1515503 -4.1292214 -4.1113172 -4.1234145 -4.1533446 -4.1596794][-3.9491572 -3.909174 -3.9266677 -3.9593186 -4.0026474 -4.0791645 -4.1553254 -4.1896753 -4.18735 -4.16303 -4.1227493 -4.076448 -4.0600567 -4.0741196 -4.0749912][-3.9256337 -3.8918326 -3.9115379 -3.9565239 -4.0143027 -4.0920568 -4.162364 -4.1908617 -4.1852927 -4.152884 -4.0903406 -4.0212283 -3.9820666 -3.9777722 -3.9808002][-3.997843 -3.9821861 -3.9985392 -4.0389872 -4.0827 -4.13537 -4.1834288 -4.2015634 -4.1945691 -4.161932 -4.0990343 -4.0331755 -3.989289 -3.974143 -3.9800165][-4.0978069 -4.0923114 -4.0930457 -4.1082325 -4.120461 -4.1386833 -4.1698942 -4.1864171 -4.1840372 -4.1685791 -4.1362228 -4.1033273 -4.0765271 -4.060915 -4.0627489][-4.1820908 -4.1779671 -4.1557236 -4.1335435 -4.1018105 -4.0809493 -4.1003208 -4.1203632 -4.1293426 -4.1413035 -4.1527743 -4.1569681 -4.1450958 -4.1358724 -4.1443505][-4.2347531 -4.2248158 -4.1823683 -4.1182408 -4.0345688 -3.975127 -3.9813211 -4.0086541 -4.0317755 -4.0734386 -4.1256938 -4.1580086 -4.1619315 -4.1687098 -4.1944442][-4.2623677 -4.2442803 -4.1834297 -4.0843835 -3.9651985 -3.8836749 -3.88183 -3.9204998 -3.9650691 -4.0297146 -4.1017408 -4.1474881 -4.1690049 -4.1948934 -4.2340546][-4.2767563 -4.2561913 -4.1912174 -4.0865254 -3.9724467 -3.9007063 -3.903914 -3.9452472 -3.9928141 -4.057198 -4.12502 -4.1733184 -4.2048883 -4.2391477 -4.276474][-4.2803431 -4.2656679 -4.215868 -4.1414614 -4.0661774 -4.0203695 -4.0223813 -4.0538406 -4.0908322 -4.1420197 -4.1962848 -4.2367697 -4.2653394 -4.2917223 -4.31527][-4.2780819 -4.2716432 -4.2426696 -4.2028656 -4.1648703 -4.1402731 -4.1402111 -4.1600065 -4.1877427 -4.2254953 -4.26304 -4.2926855 -4.314105 -4.3290358 -4.337368][-4.2769036 -4.2787585 -4.2694941 -4.2557683 -4.243412 -4.2359047 -4.2379508 -4.2508345 -4.2699032 -4.2909946 -4.3097849 -4.3251085 -4.3355837 -4.340085 -4.3387971][-4.2811847 -4.2924142 -4.2970538 -4.2989507 -4.3011732 -4.303545 -4.3088393 -4.3161573 -4.3240304 -4.3306479 -4.3364472 -4.3401856 -4.3399382 -4.3360395 -4.3266554][-4.2956424 -4.3159628 -4.3292112 -4.3384361 -4.3456097 -4.3500371 -4.3542995 -4.3560019 -4.3535528 -4.3498816 -4.3468041 -4.343328 -4.3387671 -4.33116 -4.3185692]]...]
INFO - root - 2017-12-05 22:13:29.108986: step 45110, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.898 sec/batch; 71h:41m:56s remains)
INFO - root - 2017-12-05 22:13:38.019495: step 45120, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 73h:24m:29s remains)
INFO - root - 2017-12-05 22:13:47.062193: step 45130, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 65h:23m:39s remains)
INFO - root - 2017-12-05 22:13:56.180787: step 45140, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 72h:22m:26s remains)
INFO - root - 2017-12-05 22:14:05.341604: step 45150, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 74h:23m:39s remains)
INFO - root - 2017-12-05 22:14:14.441248: step 45160, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 71h:17m:43s remains)
INFO - root - 2017-12-05 22:14:23.375493: step 45170, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 71h:32m:21s remains)
INFO - root - 2017-12-05 22:14:32.599377: step 45180, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 72h:24m:05s remains)
INFO - root - 2017-12-05 22:14:41.684806: step 45190, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 72h:18m:44s remains)
INFO - root - 2017-12-05 22:14:50.615283: step 45200, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 72h:02m:18s remains)
2017-12-05 22:14:51.457506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1212969 -4.109478 -4.1208353 -4.1373687 -4.1234913 -4.0835047 -4.0492887 -4.0245352 -4.0200629 -4.0563254 -4.1055412 -4.1645446 -4.2374806 -4.2941952 -4.3215785][-4.1234612 -4.1097031 -4.1150374 -4.1277671 -4.1156669 -4.08213 -4.0486083 -4.0256085 -4.0255589 -4.0610108 -4.1178317 -4.1837015 -4.2539687 -4.3055468 -4.33146][-4.1135926 -4.0997787 -4.1047177 -4.1170182 -4.1094322 -4.0798879 -4.0449 -4.0199232 -4.0181174 -4.0500607 -4.114109 -4.18936 -4.2619925 -4.3104324 -4.3362656][-4.093205 -4.0905204 -4.0980554 -4.109436 -4.0972729 -4.0664186 -4.0354958 -4.0112715 -4.0138369 -4.0489779 -4.1126928 -4.191165 -4.2643342 -4.3095012 -4.3343592][-4.0694647 -4.0847583 -4.09718 -4.098948 -4.06979 -4.0386934 -4.0203133 -4.001605 -4.01217 -4.0592318 -4.12279 -4.1984029 -4.2688818 -4.3099728 -4.3333478][-4.0493951 -4.072082 -4.0794115 -4.0637064 -4.014503 -3.9902754 -3.9840343 -3.9604437 -3.9719996 -4.04267 -4.1219645 -4.2000313 -4.2715454 -4.3121777 -4.3351769][-4.0368714 -4.0542846 -4.048501 -4.0249724 -3.9652455 -3.9431834 -3.9256361 -3.8675575 -3.8648202 -3.9731522 -4.0846643 -4.1794891 -4.2614641 -4.3083568 -4.3333926][-4.0654 -4.0664644 -4.0536861 -4.0399551 -3.9844315 -3.9472721 -3.8877745 -3.771538 -3.7404232 -3.8849025 -4.0235648 -4.1360297 -4.2338324 -4.2956867 -4.3276348][-4.1311717 -4.1229792 -4.1183834 -4.1219268 -4.0817175 -4.0322895 -3.9534109 -3.8308163 -3.78645 -3.9019775 -4.0223885 -4.1219487 -4.2184024 -4.2870088 -4.3232417][-4.1813393 -4.1730671 -4.17657 -4.1919255 -4.1666808 -4.1251607 -4.0587711 -3.9742959 -3.9458668 -4.0041666 -4.0781603 -4.1482983 -4.2244973 -4.2856326 -4.3204269][-4.1922412 -4.1872087 -4.1991968 -4.2199759 -4.2044396 -4.1773329 -4.128768 -4.0696921 -4.0511632 -4.0806828 -4.1287813 -4.1835527 -4.2444487 -4.2917533 -4.3192453][-4.1863823 -4.1885576 -4.2063236 -4.2267008 -4.218658 -4.1987367 -4.16055 -4.1196113 -4.1085219 -4.1293192 -4.1679878 -4.2147274 -4.2670646 -4.3027754 -4.3225703][-4.2002115 -4.1998024 -4.211113 -4.2249875 -4.2214494 -4.2064247 -4.1781826 -4.1526237 -4.14854 -4.1671047 -4.201139 -4.243145 -4.2869291 -4.3139172 -4.3275008][-4.2346067 -4.2257419 -4.2296491 -4.2362838 -4.2329416 -4.2185459 -4.1973329 -4.1822429 -4.1820674 -4.1990685 -4.2298608 -4.2669148 -4.3030152 -4.3246589 -4.3350163][-4.2800965 -4.2698383 -4.2694354 -4.2694159 -4.2635584 -4.2497082 -4.2354469 -4.2270103 -4.2260981 -4.2365141 -4.2582879 -4.2865539 -4.3143635 -4.3325758 -4.3433537]]...]
INFO - root - 2017-12-05 22:15:00.540349: step 45210, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 73h:32m:14s remains)
INFO - root - 2017-12-05 22:15:09.612145: step 45220, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 73h:52m:05s remains)
INFO - root - 2017-12-05 22:15:18.773197: step 45230, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.901 sec/batch; 71h:55m:29s remains)
INFO - root - 2017-12-05 22:15:27.814197: step 45240, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 70h:17m:24s remains)
INFO - root - 2017-12-05 22:15:36.659200: step 45250, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 71h:48m:59s remains)
INFO - root - 2017-12-05 22:15:45.791543: step 45260, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 71h:41m:55s remains)
INFO - root - 2017-12-05 22:15:54.903373: step 45270, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 73h:10m:56s remains)
INFO - root - 2017-12-05 22:16:03.810446: step 45280, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 71h:46m:32s remains)
INFO - root - 2017-12-05 22:16:12.971360: step 45290, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.964 sec/batch; 76h:54m:59s remains)
INFO - root - 2017-12-05 22:16:22.009175: step 45300, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 74h:34m:00s remains)
2017-12-05 22:16:22.794134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1546216 -4.1461554 -4.1547942 -4.1789312 -4.195941 -4.1823921 -4.1415153 -4.1198454 -4.1278448 -4.1543756 -4.1966305 -4.2441521 -4.2869749 -4.3193922 -4.3391047][-4.1323214 -4.1195707 -4.1266851 -4.1523805 -4.1666965 -4.1459794 -4.0937719 -4.0624251 -4.0721836 -4.1103554 -4.1646867 -4.2245054 -4.2778292 -4.3146963 -4.3358903][-4.1371479 -4.1261392 -4.1321259 -4.1528406 -4.1550579 -4.1214366 -4.0618844 -4.02709 -4.0447674 -4.0950613 -4.162559 -4.2293887 -4.2848315 -4.3185611 -4.33762][-4.1602135 -4.1591682 -4.1673155 -4.1760516 -4.15419 -4.101748 -4.0312495 -3.9966156 -4.0259256 -4.0908279 -4.1683488 -4.24018 -4.2949181 -4.3240666 -4.3399715][-4.1714654 -4.1795464 -4.1935759 -4.1869388 -4.1410737 -4.0614314 -3.9717658 -3.9414244 -3.9880745 -4.0728521 -4.1593442 -4.2343793 -4.2907176 -4.3206563 -4.3370519][-4.1620035 -4.1716743 -4.1809173 -4.15706 -4.0878839 -3.9769142 -3.8598862 -3.8410761 -3.9162633 -4.0256252 -4.1284566 -4.2085915 -4.2690039 -4.3059173 -4.3291454][-4.10669 -4.1174822 -4.1229825 -4.084753 -3.9952848 -3.8523827 -3.7053423 -3.7018969 -3.8170047 -3.9563344 -4.077436 -4.1685877 -4.2402372 -4.2873921 -4.3201981][-4.0410495 -4.0616674 -4.07218 -4.0344577 -3.9410093 -3.7926726 -3.6402905 -3.6438487 -3.7693772 -3.9140322 -4.0412583 -4.142653 -4.225049 -4.2802258 -4.318234][-4.05149 -4.0824637 -4.0996194 -4.0741167 -4.0020123 -3.8912609 -3.7710292 -3.7609432 -3.8434815 -3.9526315 -4.0595942 -4.15471 -4.2365437 -4.2883134 -4.3227029][-4.1118708 -4.1407847 -4.1591578 -4.1424866 -4.0936785 -4.0203786 -3.9313776 -3.9142919 -3.9603238 -4.0332565 -4.1118546 -4.1926413 -4.2634153 -4.30623 -4.3318429][-4.1766729 -4.2026329 -4.217339 -4.2009158 -4.1643305 -4.1124525 -4.0472875 -4.0284662 -4.0505362 -4.1001792 -4.1580353 -4.2276511 -4.2887359 -4.3221455 -4.339529][-4.2236414 -4.2455773 -4.2555237 -4.2397413 -4.2133894 -4.1800981 -4.131566 -4.1100307 -4.116241 -4.153698 -4.2001476 -4.2585893 -4.3106871 -4.3351274 -4.345767][-4.2615123 -4.2796125 -4.2843652 -4.2688317 -4.25205 -4.2348175 -4.1973906 -4.1724548 -4.167974 -4.1978855 -4.2342687 -4.2799234 -4.3256488 -4.3442874 -4.3495717][-4.2875023 -4.2982607 -4.293746 -4.2779288 -4.2702513 -4.2618728 -4.2293396 -4.2031732 -4.1982617 -4.2225027 -4.2510047 -4.2890658 -4.3282843 -4.3447995 -4.347044][-4.2938328 -4.2947497 -4.2834997 -4.2675714 -4.2651377 -4.2616467 -4.2338467 -4.2111158 -4.208848 -4.2282767 -4.2530456 -4.2853036 -4.3178978 -4.3346648 -4.3396788]]...]
INFO - root - 2017-12-05 22:16:31.700950: step 45310, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 71h:37m:33s remains)
INFO - root - 2017-12-05 22:16:40.774309: step 45320, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 72h:27m:14s remains)
INFO - root - 2017-12-05 22:16:49.939049: step 45330, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 74h:07m:12s remains)
INFO - root - 2017-12-05 22:16:58.784533: step 45340, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 68h:43m:41s remains)
INFO - root - 2017-12-05 22:17:07.719674: step 45350, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 72h:35m:42s remains)
INFO - root - 2017-12-05 22:17:16.720628: step 45360, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 73h:43m:43s remains)
INFO - root - 2017-12-05 22:17:25.608767: step 45370, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 72h:58m:30s remains)
INFO - root - 2017-12-05 22:17:34.850476: step 45380, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.924 sec/batch; 73h:39m:42s remains)
INFO - root - 2017-12-05 22:17:44.258476: step 45390, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 71h:45m:32s remains)
INFO - root - 2017-12-05 22:17:53.244086: step 45400, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 70h:57m:25s remains)
2017-12-05 22:17:54.051244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1737819 -4.1837883 -4.1945376 -4.1977606 -4.1890516 -4.172729 -4.1571121 -4.157218 -4.1624413 -4.1640935 -4.174109 -4.1887851 -4.2019014 -4.2047281 -4.195313][-4.1639366 -4.1813908 -4.195806 -4.1975632 -4.1898308 -4.1793809 -4.1674972 -4.1630182 -4.1646972 -4.1692743 -4.1840391 -4.2029448 -4.2146978 -4.2114296 -4.1942139][-4.1728668 -4.1963038 -4.2116413 -4.20979 -4.2003183 -4.1914845 -4.1817179 -4.1735234 -4.17059 -4.1758037 -4.1914835 -4.2103896 -4.219048 -4.20878 -4.1830964][-4.1815238 -4.2134233 -4.227869 -4.2209873 -4.2055087 -4.1896782 -4.1787062 -4.1684403 -4.1617746 -4.1689434 -4.1871819 -4.20381 -4.2117729 -4.1988411 -4.1712313][-4.1711369 -4.213387 -4.2233586 -4.2045884 -4.1753473 -4.14282 -4.1262965 -4.1231856 -4.1259756 -4.1427 -4.1683135 -4.1846786 -4.197381 -4.1867423 -4.1635041][-4.1398654 -4.1885419 -4.1952615 -4.1648088 -4.1139717 -4.054225 -4.0246453 -4.040019 -4.0673871 -4.1085062 -4.146956 -4.1646 -4.1802764 -4.1707563 -4.1508074][-4.1057973 -4.1533575 -4.156569 -4.1120038 -4.03585 -3.9440222 -3.9015336 -3.9440122 -4.0101309 -4.0814314 -4.1327677 -4.1559319 -4.1705461 -4.1587858 -4.1384397][-4.0964 -4.1314678 -4.13177 -4.084722 -4.0013738 -3.8987679 -3.8458757 -3.8957944 -3.9850111 -4.0731912 -4.1306748 -4.1592078 -4.1704473 -4.1594787 -4.1388559][-4.1089191 -4.1269865 -4.1238241 -4.0901427 -4.0291319 -3.9474361 -3.9018764 -3.9339118 -4.0101094 -4.0876894 -4.1375875 -4.1649032 -4.1734953 -4.1649103 -4.1488733][-4.1214571 -4.110302 -4.097086 -4.086092 -4.0656657 -4.0241394 -4.0013943 -4.0166945 -4.0607219 -4.1106973 -4.1418481 -4.1641827 -4.1719642 -4.1656008 -4.1583877][-4.1085482 -4.0577574 -4.0214972 -4.0320916 -4.0565982 -4.0590878 -4.0637264 -4.0741405 -4.0931072 -4.1182404 -4.1384163 -4.1552849 -4.158071 -4.155015 -4.1574631][-4.0781054 -3.9965003 -3.9409947 -3.9638798 -4.0237827 -4.0573974 -4.0802307 -4.0944848 -4.10101 -4.11302 -4.1274838 -4.1414804 -4.1412678 -4.1406589 -4.1444364][-4.0672822 -3.9796219 -3.9258056 -3.9577487 -4.0245376 -4.0562682 -4.0751824 -4.0866742 -4.0886497 -4.0981331 -4.1141233 -4.1331034 -4.1383142 -4.1427603 -4.1423774][-4.0853105 -4.0086832 -3.9723907 -4.0030761 -4.0535779 -4.0656576 -4.0635824 -4.0634604 -4.0671129 -4.0812683 -4.1048493 -4.1327586 -4.1505423 -4.1621866 -4.1574459][-4.1100283 -4.0539451 -4.0328579 -4.0551825 -4.0800781 -4.0757842 -4.0596209 -4.0494242 -4.0561004 -4.0778756 -4.1061196 -4.1394553 -4.1653948 -4.1800728 -4.1743779]]...]
INFO - root - 2017-12-05 22:18:03.188141: step 45410, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 70h:02m:16s remains)
INFO - root - 2017-12-05 22:18:12.238985: step 45420, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 71h:39m:39s remains)
INFO - root - 2017-12-05 22:18:21.223181: step 45430, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 73h:19m:28s remains)
INFO - root - 2017-12-05 22:18:30.175113: step 45440, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 74h:14m:51s remains)
INFO - root - 2017-12-05 22:18:39.225605: step 45450, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 69h:57m:08s remains)
INFO - root - 2017-12-05 22:18:48.268799: step 45460, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 72h:33m:15s remains)
INFO - root - 2017-12-05 22:18:57.481633: step 45470, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 72h:51m:07s remains)
INFO - root - 2017-12-05 22:19:06.416949: step 45480, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 72h:12m:10s remains)
INFO - root - 2017-12-05 22:19:15.355045: step 45490, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 71h:49m:54s remains)
INFO - root - 2017-12-05 22:19:24.563992: step 45500, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 73h:41m:21s remains)
2017-12-05 22:19:25.358986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2361832 -4.2328181 -4.2427011 -4.2576213 -4.2697921 -4.280808 -4.2844949 -4.2792621 -4.2681808 -4.2539206 -4.2426457 -4.2361326 -4.2318873 -4.234818 -4.2382431][-4.2313113 -4.2331138 -4.2458482 -4.2574739 -4.260963 -4.262145 -4.2612233 -4.257215 -4.2543731 -4.2516122 -4.2485189 -4.2446494 -4.2394242 -4.2380815 -4.2359438][-4.2156463 -4.2193284 -4.2321663 -4.2397037 -4.235281 -4.2278328 -4.2227988 -4.2212453 -4.2306938 -4.2448187 -4.25357 -4.2540555 -4.2491426 -4.2427778 -4.2359724][-4.2048283 -4.2076635 -4.2200308 -4.2268019 -4.2185726 -4.2039614 -4.1941891 -4.1944628 -4.2159281 -4.244792 -4.2629347 -4.26782 -4.2641692 -4.2529354 -4.2426724][-4.2054443 -4.2023425 -4.211256 -4.2179022 -4.2096753 -4.1914992 -4.175621 -4.1753974 -4.2074995 -4.2473936 -4.2717433 -4.2808828 -4.2767353 -4.2622347 -4.2518744][-4.2147417 -4.203805 -4.2083135 -4.2135239 -4.2022648 -4.1775436 -4.1514044 -4.1464386 -4.1859417 -4.2372146 -4.2685323 -4.28144 -4.2753921 -4.2580667 -4.25289][-4.2333403 -4.2149372 -4.2123876 -4.2107654 -4.1884604 -4.1494303 -4.1046472 -4.0861988 -4.1328306 -4.20267 -4.2475533 -4.2676816 -4.2635 -4.2449841 -4.2437191][-4.2491326 -4.2287703 -4.2207127 -4.2114921 -4.1751957 -4.1148076 -4.0405 -3.9977455 -4.0523343 -4.1494813 -4.2176409 -4.2533383 -4.2595811 -4.24428 -4.2423415][-4.2547231 -4.23405 -4.2242913 -4.2153263 -4.1791062 -4.1140637 -4.0285339 -3.9714835 -4.0230832 -4.1282582 -4.2071438 -4.2522516 -4.268117 -4.2615404 -4.2594876][-4.247179 -4.2296729 -4.2261524 -4.2281713 -4.2098889 -4.1679654 -4.1070065 -4.0625534 -4.0913162 -4.1637564 -4.2233658 -4.2642183 -4.2851591 -4.2883344 -4.2888045][-4.23744 -4.22789 -4.2311835 -4.2428522 -4.2415371 -4.2257643 -4.1937637 -4.1652689 -4.1727772 -4.209044 -4.2428732 -4.2712855 -4.2922444 -4.3017778 -4.3048615][-4.2317104 -4.2297626 -4.2370558 -4.2519665 -4.2590218 -4.2578058 -4.2429271 -4.2227454 -4.2192411 -4.2345991 -4.2523046 -4.267982 -4.2831826 -4.294085 -4.301692][-4.2214723 -4.224864 -4.2363539 -4.2511349 -4.2582121 -4.26049 -4.2517848 -4.2370148 -4.2337723 -4.2457662 -4.2602472 -4.2673521 -4.2737164 -4.2841396 -4.2961545][-4.2083488 -4.2144237 -4.2318497 -4.246912 -4.2509613 -4.2533574 -4.2475162 -4.2380152 -4.2375054 -4.2508044 -4.2659488 -4.2678642 -4.265367 -4.2709956 -4.2839055][-4.1981993 -4.2032075 -4.2241826 -4.24148 -4.2455573 -4.2497 -4.2466769 -4.2388325 -4.2363462 -4.2470164 -4.2632742 -4.2673249 -4.2635436 -4.2657986 -4.2762117]]...]
INFO - root - 2017-12-05 22:19:34.356796: step 45510, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 71h:08m:37s remains)
INFO - root - 2017-12-05 22:19:43.449975: step 45520, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 74h:45m:28s remains)
INFO - root - 2017-12-05 22:19:52.363253: step 45530, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 71h:19m:47s remains)
INFO - root - 2017-12-05 22:20:01.396440: step 45540, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 75h:15m:30s remains)
INFO - root - 2017-12-05 22:20:10.448532: step 45550, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 73h:53m:32s remains)
INFO - root - 2017-12-05 22:20:19.619153: step 45560, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 73h:39m:39s remains)
INFO - root - 2017-12-05 22:20:28.873637: step 45570, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 70h:40m:02s remains)
INFO - root - 2017-12-05 22:20:37.655022: step 45580, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.802 sec/batch; 63h:53m:23s remains)
INFO - root - 2017-12-05 22:20:46.661760: step 45590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:09m:49s remains)
INFO - root - 2017-12-05 22:20:55.596859: step 45600, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.904 sec/batch; 72h:01m:29s remains)
2017-12-05 22:20:56.486897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.259666 -4.2545519 -4.2335544 -4.2032747 -4.2013288 -4.2220254 -4.231545 -4.2374244 -4.2510238 -4.26616 -4.2686081 -4.25398 -4.2339544 -4.2181726 -4.2128024][-4.2617569 -4.253593 -4.2240715 -4.1877179 -4.1917639 -4.2154622 -4.224956 -4.2329817 -4.252162 -4.2724881 -4.2758431 -4.2582769 -4.2323761 -4.2090569 -4.1993022][-4.2512784 -4.2433577 -4.2140489 -4.18155 -4.1934366 -4.2201319 -4.2298803 -4.2382588 -4.2596021 -4.2787633 -4.2783976 -4.2594562 -4.2345915 -4.2125707 -4.20435][-4.2476206 -4.2498279 -4.2327671 -4.2108488 -4.2236161 -4.2436905 -4.2447228 -4.2456255 -4.2635412 -4.2785664 -4.2779155 -4.2647791 -4.2485242 -4.235167 -4.2314606][-4.25614 -4.2709265 -4.2665882 -4.2520633 -4.2564344 -4.2621155 -4.2484522 -4.2396946 -4.2549639 -4.269619 -4.2748432 -4.2731175 -4.2673287 -4.2616057 -4.2610278][-4.2668743 -4.2858772 -4.2844038 -4.268786 -4.2592287 -4.2467332 -4.2155647 -4.2014203 -4.2236481 -4.2474709 -4.2642183 -4.2758622 -4.279088 -4.2779117 -4.2768726][-4.2647657 -4.2800021 -4.2747078 -4.2536426 -4.2318168 -4.2016425 -4.1523237 -4.1390138 -4.17886 -4.2201972 -4.2507114 -4.2742023 -4.2840376 -4.2834582 -4.2795453][-4.2463069 -4.2558327 -4.2467723 -4.2231593 -4.1989193 -4.15912 -4.0993032 -4.0900807 -4.14798 -4.2040448 -4.2446356 -4.2758665 -4.2888064 -4.2863231 -4.2780976][-4.2199326 -4.2259049 -4.2169704 -4.1997824 -4.187078 -4.1540923 -4.1025028 -4.1005049 -4.1590834 -4.2125344 -4.2513475 -4.28097 -4.2914267 -4.2870088 -4.2760167][-4.2022009 -4.2104726 -4.2073121 -4.2033987 -4.2057419 -4.1851583 -4.1480784 -4.1478586 -4.1884675 -4.2257667 -4.2533159 -4.2750773 -4.2834578 -4.2810016 -4.2714629][-4.1950307 -4.2088304 -4.2141132 -4.2233996 -4.2333379 -4.2184954 -4.1898761 -4.1841893 -4.2025661 -4.2221332 -4.2381535 -4.253705 -4.2647457 -4.2696757 -4.266005][-4.1993456 -4.2154131 -4.2242832 -4.2390108 -4.2475 -4.2313528 -4.2046652 -4.1915369 -4.1935215 -4.2036715 -4.216187 -4.2321424 -4.2484756 -4.2617269 -4.2640867][-4.2039814 -4.21614 -4.2247748 -4.2389154 -4.2443271 -4.2281055 -4.2041988 -4.1877341 -4.1833673 -4.1940546 -4.20997 -4.2283716 -4.2471838 -4.2632256 -4.2676659][-4.2002211 -4.2072439 -4.2159162 -4.2288189 -4.2341733 -4.2233415 -4.2050147 -4.1898012 -4.1864018 -4.1989846 -4.2174282 -4.2359238 -4.2544789 -4.2695484 -4.2736187][-4.1779881 -4.1873264 -4.2003 -4.2138481 -4.22283 -4.22058 -4.2096934 -4.198524 -4.196207 -4.2079439 -4.2258353 -4.2429175 -4.2603498 -4.2740779 -4.27767]]...]
INFO - root - 2017-12-05 22:21:05.490136: step 45610, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 74h:00m:28s remains)
INFO - root - 2017-12-05 22:21:14.572152: step 45620, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 72h:36m:50s remains)
INFO - root - 2017-12-05 22:21:23.641090: step 45630, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 74h:40m:57s remains)
INFO - root - 2017-12-05 22:21:32.540819: step 45640, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 72h:24m:46s remains)
INFO - root - 2017-12-05 22:21:41.419799: step 45650, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.945 sec/batch; 75h:15m:53s remains)
INFO - root - 2017-12-05 22:21:50.481341: step 45660, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 72h:36m:59s remains)
INFO - root - 2017-12-05 22:21:59.458727: step 45670, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 69h:46m:33s remains)
INFO - root - 2017-12-05 22:22:08.459443: step 45680, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.891 sec/batch; 70h:57m:31s remains)
INFO - root - 2017-12-05 22:22:17.478103: step 45690, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 71h:10m:59s remains)
INFO - root - 2017-12-05 22:22:26.633024: step 45700, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 71h:33m:47s remains)
2017-12-05 22:22:27.429258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3372173 -4.3334632 -4.3296895 -4.326509 -4.3234448 -4.3217473 -4.3215418 -4.3219266 -4.3242745 -4.3259783 -4.3254709 -4.325233 -4.3252616 -4.3249936 -4.3252249][-4.3228908 -4.316031 -4.3092737 -4.3030734 -4.298851 -4.2969322 -4.2957954 -4.294971 -4.2983317 -4.3012838 -4.30014 -4.299233 -4.299397 -4.2990894 -4.299468][-4.3002977 -4.2884793 -4.2764668 -4.264226 -4.2576561 -4.25712 -4.2573028 -4.2553329 -4.2590232 -4.2632246 -4.26121 -4.25951 -4.2596869 -4.2595234 -4.2612076][-4.2713346 -4.2513494 -4.2301497 -4.2098436 -4.2006612 -4.2039566 -4.2091775 -4.2097826 -4.2154803 -4.2197771 -4.2159715 -4.2138386 -4.2131829 -4.2120619 -4.2153258][-4.2445712 -4.2157726 -4.1848407 -4.1567655 -4.146482 -4.1534896 -4.1648083 -4.168632 -4.175859 -4.1821923 -4.1800823 -4.1806874 -4.1805115 -4.17971 -4.185369][-4.2256389 -4.1898017 -4.1487131 -4.109446 -4.0970235 -4.1072488 -4.1218934 -4.1267228 -4.1308236 -4.1365156 -4.1368594 -4.1415834 -4.1423559 -4.1406507 -4.1458488][-4.2166038 -4.1746659 -4.1196842 -4.06224 -4.040287 -4.0494361 -4.0649791 -4.0640945 -4.0638585 -4.0674076 -4.0677371 -4.0758677 -4.0747819 -4.0674367 -4.067461][-4.2186141 -4.175004 -4.1093621 -4.037035 -4.0035782 -4.0096774 -4.0222197 -4.0122657 -4.0062032 -4.0093875 -4.0079451 -4.0165577 -4.0131412 -4.0017152 -3.9966714][-4.2304082 -4.1932282 -4.1343226 -4.0744343 -4.0462322 -4.0452671 -4.047925 -4.0308328 -4.0187273 -4.0217876 -4.0191059 -4.0225682 -4.0153637 -3.9998753 -3.9923193][-4.244647 -4.2152176 -4.1702123 -4.1312308 -4.1134367 -4.1077337 -4.1018133 -4.082552 -4.0712023 -4.0790877 -4.0767422 -4.0736527 -4.0609207 -4.03606 -4.0162787][-4.254807 -4.2284017 -4.1930618 -4.1694579 -4.1581798 -4.149509 -4.1416826 -4.1263056 -4.1199183 -4.1379371 -4.1401076 -4.1337347 -4.115253 -4.0823236 -4.052496][-4.2653646 -4.2399936 -4.211061 -4.1944513 -4.185729 -4.1761727 -4.1703153 -4.15957 -4.1574111 -4.1806779 -4.1861081 -4.1786561 -4.1597986 -4.1295986 -4.1022973][-4.2775025 -4.254385 -4.23039 -4.2167144 -4.2086587 -4.1986051 -4.1910868 -4.181294 -4.1803951 -4.2037725 -4.2119722 -4.206934 -4.1951795 -4.1781344 -4.1638632][-4.2926044 -4.2735338 -4.2550607 -4.244669 -4.2405882 -4.2325587 -4.2214165 -4.2096305 -4.2111511 -4.2312613 -4.2381907 -4.2337503 -4.2298594 -4.2263532 -4.2251186][-4.309041 -4.2950263 -4.2816343 -4.2751579 -4.2748146 -4.2684803 -4.2549648 -4.2433953 -4.2478819 -4.2653451 -4.2701921 -4.2665596 -4.2665071 -4.2689667 -4.2726984]]...]
INFO - root - 2017-12-05 22:22:36.448411: step 45710, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 70h:02m:01s remains)
INFO - root - 2017-12-05 22:22:45.317052: step 45720, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 73h:20m:06s remains)
INFO - root - 2017-12-05 22:22:54.506624: step 45730, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 75h:03m:06s remains)
INFO - root - 2017-12-05 22:23:03.496772: step 45740, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 72h:16m:57s remains)
INFO - root - 2017-12-05 22:23:12.507379: step 45750, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 72h:52m:20s remains)
INFO - root - 2017-12-05 22:23:21.740147: step 45760, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 72h:57m:10s remains)
INFO - root - 2017-12-05 22:23:30.787617: step 45770, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 70h:35m:51s remains)
INFO - root - 2017-12-05 22:23:39.824379: step 45780, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 73h:13m:10s remains)
INFO - root - 2017-12-05 22:23:48.931058: step 45790, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 75h:09m:24s remains)
INFO - root - 2017-12-05 22:23:58.187436: step 45800, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 74h:28m:18s remains)
2017-12-05 22:23:58.960957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.338902 -4.3288178 -4.3056908 -4.2827792 -4.2670779 -4.2611032 -4.2477951 -4.2300167 -4.2198596 -4.2323027 -4.2547951 -4.2546082 -4.2298079 -4.1902456 -4.1571484][-4.336082 -4.3247728 -4.3014483 -4.2812409 -4.2689934 -4.2620931 -4.2452288 -4.2249861 -4.213985 -4.2255588 -4.2498159 -4.2497697 -4.2200427 -4.1788087 -4.148726][-4.3290024 -4.3124938 -4.2860522 -4.2661858 -4.2556458 -4.2459517 -4.2240934 -4.2049785 -4.1982722 -4.2167039 -4.2456722 -4.2454343 -4.2106895 -4.1669192 -4.1422763][-4.321619 -4.29856 -4.2670679 -4.2413583 -4.229404 -4.212574 -4.1815248 -4.158494 -4.162899 -4.1960959 -4.23652 -4.2457542 -4.2167134 -4.1737809 -4.1456089][-4.3140092 -4.285943 -4.2481527 -4.20955 -4.187099 -4.1536851 -4.1055093 -4.0705914 -4.0861511 -4.1456895 -4.2067895 -4.2354093 -4.2206 -4.1846571 -4.15292][-4.3077736 -4.2767105 -4.234385 -4.1851225 -4.1471634 -4.0947371 -4.0263247 -3.9687381 -3.9791479 -4.067944 -4.1559114 -4.205914 -4.2115588 -4.1882286 -4.1596665][-4.3047352 -4.2709389 -4.2259107 -4.1702709 -4.1195836 -4.0526118 -3.9667797 -3.8797078 -3.8647571 -3.973043 -4.0855956 -4.1583805 -4.1895504 -4.1851106 -4.162816][-4.3031483 -4.266099 -4.2177362 -4.1598411 -4.1039224 -4.0281668 -3.9333193 -3.8242419 -3.7732921 -3.8823192 -4.0125084 -4.10274 -4.1576276 -4.1753755 -4.1621089][-4.3042555 -4.2673454 -4.2189994 -4.1638069 -4.1126938 -4.0468168 -3.9682903 -3.8687112 -3.7997179 -3.875639 -3.9948654 -4.0843883 -4.1446948 -4.1724672 -4.1658182][-4.3085227 -4.2738795 -4.2281508 -4.1784344 -4.1342735 -4.0827522 -4.0263886 -3.9532235 -3.8947926 -3.9346566 -4.024447 -4.0990281 -4.1516495 -4.1774397 -4.17595][-4.3162928 -4.2859192 -4.2433043 -4.1988993 -4.162271 -4.1224928 -4.0829616 -4.0320544 -3.9888778 -4.0098133 -4.0686116 -4.1234393 -4.1662579 -4.1893172 -4.1962366][-4.3234477 -4.2982254 -4.2615209 -4.2248535 -4.2000208 -4.1739244 -4.1505361 -4.1177607 -4.0874586 -4.0972419 -4.1281261 -4.1609554 -4.1897669 -4.2103777 -4.225771][-4.3276048 -4.3062921 -4.2743754 -4.243093 -4.2257061 -4.2094464 -4.195962 -4.175456 -4.1550508 -4.1610966 -4.17672 -4.1918712 -4.2095194 -4.228425 -4.2481813][-4.3297086 -4.3102417 -4.2790027 -4.24792 -4.2306862 -4.2171979 -4.2077 -4.1947665 -4.1843886 -4.194344 -4.2041106 -4.208662 -4.2182608 -4.2365317 -4.2582321][-4.3305154 -4.3103828 -4.2773275 -4.2432122 -4.2211695 -4.2023129 -4.1874776 -4.1751785 -4.1727405 -4.1890426 -4.1985974 -4.1972852 -4.2025547 -4.2196655 -4.2413445]]...]
INFO - root - 2017-12-05 22:24:07.931831: step 45810, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.808 sec/batch; 64h:19m:26s remains)
INFO - root - 2017-12-05 22:24:17.086569: step 45820, loss = 2.11, batch loss = 2.05 (8.4 examples/sec; 0.954 sec/batch; 76h:00m:04s remains)
INFO - root - 2017-12-05 22:24:26.334076: step 45830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 72h:21m:51s remains)
INFO - root - 2017-12-05 22:24:35.412913: step 45840, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 71h:23m:56s remains)
INFO - root - 2017-12-05 22:24:44.506755: step 45850, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.961 sec/batch; 76h:31m:12s remains)
INFO - root - 2017-12-05 22:24:53.598991: step 45860, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 73h:14m:58s remains)
INFO - root - 2017-12-05 22:25:02.522697: step 45870, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 69h:57m:36s remains)
INFO - root - 2017-12-05 22:25:11.740291: step 45880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 68h:28m:53s remains)
INFO - root - 2017-12-05 22:25:20.817541: step 45890, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 73h:07m:16s remains)
INFO - root - 2017-12-05 22:25:29.998364: step 45900, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 70h:07m:22s remains)
2017-12-05 22:25:30.848608: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846079 -4.293026 -4.3052173 -4.3146253 -4.3178921 -4.3143239 -4.3054075 -4.29693 -4.294673 -4.2976937 -4.3073916 -4.3227296 -4.3414879 -4.3548326 -4.3561759][-4.2430434 -4.2564912 -4.271811 -4.287148 -4.2954431 -4.2935691 -4.2838545 -4.2732077 -4.2682028 -4.2704773 -4.2802687 -4.3001018 -4.3266435 -4.3493662 -4.3555841][-4.1760073 -4.1918445 -4.2058873 -4.2285357 -4.24494 -4.2448521 -4.235589 -4.2225542 -4.2116017 -4.217896 -4.2368436 -4.268436 -4.303339 -4.3344026 -4.3482685][-4.1074414 -4.1242337 -4.1330013 -4.1525421 -4.1718917 -4.171824 -4.162921 -4.1426716 -4.1226721 -4.1387239 -4.1725192 -4.2204523 -4.2647524 -4.30559 -4.3308892][-4.0502224 -4.0652204 -4.0616112 -4.0773368 -4.097486 -4.0949593 -4.0843649 -4.0508451 -4.0280585 -4.0646133 -4.1178579 -4.1797948 -4.2308488 -4.2784305 -4.3131223][-4.0040331 -4.0048447 -3.9812074 -3.994678 -4.0050745 -3.9902973 -3.9847353 -3.939141 -3.9200385 -3.9867177 -4.0693789 -4.1516929 -4.2053714 -4.253305 -4.2920561][-3.97283 -3.9480562 -3.9009268 -3.9062541 -3.8933887 -3.8420615 -3.8248105 -3.7607613 -3.7447078 -3.8546941 -3.9815273 -4.0947266 -4.1644754 -4.2223334 -4.269105][-3.9517159 -3.9053931 -3.8421812 -3.8429163 -3.8050153 -3.7134452 -3.6812205 -3.5922761 -3.5850773 -3.7494287 -3.912806 -4.0445681 -4.1280971 -4.196702 -4.2518625][-3.9587965 -3.9041138 -3.8400958 -3.8436539 -3.7895353 -3.6838834 -3.6445847 -3.5422363 -3.5463119 -3.7390909 -3.91181 -4.0429554 -4.1247091 -4.1926465 -4.2467484][-4.0190992 -3.973228 -3.9260743 -3.9335675 -3.8785658 -3.783051 -3.7358861 -3.6336017 -3.6436889 -3.8107653 -3.9619572 -4.0790849 -4.1510839 -4.209868 -4.2569361][-4.0696 -4.0379434 -4.0109158 -4.0242305 -3.9825478 -3.9140038 -3.8738513 -3.7914362 -3.8053179 -3.9324753 -4.0447049 -4.1413021 -4.1988173 -4.2417822 -4.2759933][-4.1106377 -4.0851398 -4.0704265 -4.088645 -4.0628619 -4.0200596 -3.99092 -3.9305034 -3.9462762 -4.0345449 -4.113688 -4.1915512 -4.2381644 -4.2697577 -4.2917023][-4.1690779 -4.1537266 -4.1490536 -4.1670885 -4.1569362 -4.1343856 -4.1123624 -4.0728254 -4.0888181 -4.1415749 -4.188201 -4.2425714 -4.2759113 -4.2982526 -4.3094492][-4.2186122 -4.2098813 -4.211627 -4.2271552 -4.2270575 -4.2211452 -4.2098742 -4.1907172 -4.2037158 -4.2307916 -4.256495 -4.2903118 -4.3085084 -4.3200755 -4.3242373][-4.2514362 -4.2428846 -4.2435427 -4.2508545 -4.2515035 -4.2517743 -4.2493038 -4.2459559 -4.2572393 -4.270659 -4.2869449 -4.3081632 -4.3199449 -4.3271394 -4.329864]]...]
INFO - root - 2017-12-05 22:25:39.842480: step 45910, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 69h:01m:21s remains)
INFO - root - 2017-12-05 22:25:48.847303: step 45920, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 70h:02m:50s remains)
INFO - root - 2017-12-05 22:25:58.112616: step 45930, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 70h:52m:03s remains)
INFO - root - 2017-12-05 22:26:07.392080: step 45940, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 72h:10m:35s remains)
INFO - root - 2017-12-05 22:26:16.554568: step 45950, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.921 sec/batch; 73h:20m:11s remains)
INFO - root - 2017-12-05 22:26:25.603002: step 45960, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 73h:03m:45s remains)
INFO - root - 2017-12-05 22:26:34.735055: step 45970, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 73h:55m:26s remains)
INFO - root - 2017-12-05 22:26:43.970502: step 45980, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 72h:46m:11s remains)
INFO - root - 2017-12-05 22:26:53.040650: step 45990, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 72h:50m:25s remains)
INFO - root - 2017-12-05 22:27:01.966320: step 46000, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 71h:36m:04s remains)
2017-12-05 22:27:02.771700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1867785 -4.1944752 -4.1852531 -4.1706481 -4.1559548 -4.1508431 -4.1548934 -4.1671839 -4.1827717 -4.19366 -4.2034774 -4.2114258 -4.2136445 -4.217268 -4.2215748][-4.1581264 -4.1617317 -4.1481524 -4.1319332 -4.117249 -4.1153445 -4.1241479 -4.1425476 -4.1646681 -4.1794105 -4.1935644 -4.2055831 -4.2080965 -4.2105904 -4.2166324][-4.1584339 -4.1544104 -4.13582 -4.1154418 -4.0972853 -4.0927157 -4.101963 -4.1221204 -4.1468573 -4.1622572 -4.1768203 -4.1909003 -4.1926227 -4.1930122 -4.2022667][-4.1770844 -4.1678638 -4.1454372 -4.1189628 -4.0904813 -4.0746593 -4.0773911 -4.0994411 -4.1233778 -4.1385818 -4.1543236 -4.1670527 -4.1655288 -4.1650214 -4.1780291][-4.2017794 -4.193717 -4.1717491 -4.1436777 -4.1059718 -4.0735207 -4.0611191 -4.0757585 -4.098433 -4.1167207 -4.14486 -4.1636319 -4.1632895 -4.1666141 -4.18122][-4.2207603 -4.2160649 -4.1965408 -4.1719713 -4.130518 -4.0805368 -4.0452628 -4.0418925 -4.054019 -4.0838432 -4.139308 -4.1681509 -4.1682811 -4.1747041 -4.1859684][-4.23078 -4.2249303 -4.2067962 -4.18617 -4.1429272 -4.0763683 -4.0101628 -3.9732103 -3.9662602 -4.0192633 -4.1087208 -4.1554327 -4.1625543 -4.1706495 -4.1752663][-4.2316804 -4.2243147 -4.2115936 -4.1963992 -4.1558709 -4.0819583 -3.9850974 -3.895447 -3.8536389 -3.9249337 -4.045794 -4.1124206 -4.1313553 -4.1406059 -4.1442819][-4.2215056 -4.2141633 -4.2122455 -4.2095766 -4.1871915 -4.1244311 -4.0207973 -3.9003463 -3.8281081 -3.892065 -4.0191803 -4.0949235 -4.1182113 -4.1241345 -4.1261878][-4.2047215 -4.1980677 -4.2062459 -4.2173548 -4.2163687 -4.181005 -4.1069045 -4.0115881 -3.948616 -3.9834473 -4.0679326 -4.1229315 -4.1424503 -4.1417694 -4.1403942][-4.1736636 -4.1658864 -4.1808319 -4.2025561 -4.2203555 -4.2138948 -4.1837263 -4.1366725 -4.0985074 -4.1090665 -4.1404619 -4.1635985 -4.1715841 -4.1609807 -4.1542196][-4.1247253 -4.1138806 -4.1325731 -4.1654005 -4.1969628 -4.2131028 -4.218328 -4.2077327 -4.1879869 -4.183136 -4.1818929 -4.1780314 -4.1683497 -4.1426725 -4.1290207][-4.0816851 -4.0627913 -4.0831385 -4.1247163 -4.1654382 -4.1942105 -4.2215509 -4.2329144 -4.2286186 -4.2157507 -4.1964016 -4.174212 -4.1468587 -4.1067243 -4.0880532][-4.0911994 -4.0692697 -4.090981 -4.1291838 -4.163228 -4.1904283 -4.2256484 -4.2487078 -4.2557859 -4.2433414 -4.2192011 -4.1916003 -4.1566696 -4.1121531 -4.0889387][-4.1602364 -4.1475449 -4.1650252 -4.1886659 -4.2033644 -4.2141137 -4.2400718 -4.2639475 -4.2754889 -4.2686863 -4.2507129 -4.2281251 -4.195775 -4.1582279 -4.1383834]]...]
INFO - root - 2017-12-05 22:27:12.028668: step 46010, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.904 sec/batch; 71h:57m:12s remains)
INFO - root - 2017-12-05 22:27:21.055603: step 46020, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 71h:27m:48s remains)
INFO - root - 2017-12-05 22:27:30.238601: step 46030, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 75h:14m:44s remains)
INFO - root - 2017-12-05 22:27:39.444233: step 46040, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 72h:47m:20s remains)
INFO - root - 2017-12-05 22:27:48.385100: step 46050, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 71h:47m:50s remains)
INFO - root - 2017-12-05 22:27:57.595584: step 46060, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:51m:58s remains)
INFO - root - 2017-12-05 22:28:06.769980: step 46070, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 72h:25m:38s remains)
INFO - root - 2017-12-05 22:28:15.780037: step 46080, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 72h:17m:30s remains)
INFO - root - 2017-12-05 22:28:24.936824: step 46090, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 72h:49m:42s remains)
INFO - root - 2017-12-05 22:28:33.993419: step 46100, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 70h:06m:47s remains)
2017-12-05 22:28:34.869367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2559295 -4.2406864 -4.2274733 -4.2142754 -4.2040806 -4.2032275 -4.213666 -4.2318535 -4.2518182 -4.2695351 -4.2787828 -4.2745109 -4.268127 -4.2635365 -4.2552304][-4.2557306 -4.2382817 -4.2133613 -4.1875134 -4.1751237 -4.1732273 -4.1824479 -4.2073774 -4.2397294 -4.2691936 -4.2834663 -4.2816644 -4.2777061 -4.276154 -4.2703872][-4.2476745 -4.2286839 -4.1987257 -4.1637497 -4.1453781 -4.1375785 -4.1444116 -4.176949 -4.22207 -4.261982 -4.2824297 -4.28639 -4.2849169 -4.2844687 -4.2821374][-4.2362642 -4.2156181 -4.1855154 -4.1458144 -4.11796 -4.0976458 -4.0941124 -4.1255474 -4.1800294 -4.2325954 -4.2601867 -4.2711711 -4.271112 -4.2703948 -4.2703657][-4.2364049 -4.2152257 -4.1850371 -4.1433887 -4.1060686 -4.0664754 -4.0414228 -4.0662127 -4.132009 -4.1960654 -4.2320323 -4.2479877 -4.2521524 -4.2542868 -4.2565022][-4.243834 -4.2227468 -4.1942267 -4.1560049 -4.1105933 -4.0412107 -3.9798083 -3.9958181 -4.0761571 -4.14826 -4.1853333 -4.2047749 -4.2186656 -4.2331924 -4.2425618][-4.242682 -4.2205787 -4.18927 -4.1546955 -4.0990648 -3.9921041 -3.8745584 -3.8871434 -4.0013866 -4.0854487 -4.1203275 -4.1418371 -4.1706014 -4.2052445 -4.232161][-4.2323384 -4.2015009 -4.16365 -4.1263833 -4.0579848 -3.9156904 -3.7498538 -3.774667 -3.9339731 -4.0365906 -4.08025 -4.1153865 -4.1589851 -4.2066364 -4.237895][-4.2182751 -4.1773772 -4.1350145 -4.0979452 -4.029326 -3.8985589 -3.7596292 -3.7976227 -3.9527662 -4.0461931 -4.0942216 -4.1405735 -4.1905208 -4.2375422 -4.2618928][-4.2069621 -4.1619139 -4.1212687 -4.0932589 -4.0408616 -3.9587433 -3.8949857 -3.9429483 -4.0502534 -4.115665 -4.1573434 -4.194396 -4.2333074 -4.2692242 -4.2861943][-4.1990376 -4.160419 -4.1319566 -4.1192861 -4.0904617 -4.0477896 -4.0340104 -4.0879364 -4.1578794 -4.1972361 -4.224194 -4.2464113 -4.266861 -4.287972 -4.2982769][-4.1891766 -4.1551523 -4.138195 -4.1409764 -4.132164 -4.1135678 -4.1192331 -4.171567 -4.2178936 -4.2382512 -4.2510676 -4.2603621 -4.2680569 -4.2767091 -4.2802343][-4.176187 -4.1472712 -4.1359272 -4.1447582 -4.1439967 -4.1312041 -4.1417494 -4.1891828 -4.2228255 -4.2300677 -4.2309632 -4.2307777 -4.23201 -4.2333484 -4.232738][-4.1783195 -4.1532173 -4.142035 -4.1465144 -4.1454568 -4.1380639 -4.1492109 -4.1840506 -4.2057228 -4.2054782 -4.2013025 -4.1984348 -4.19711 -4.1972017 -4.1975045][-4.1945443 -4.1783857 -4.1721754 -4.1711378 -4.1676612 -4.1637197 -4.1722255 -4.1946416 -4.2082739 -4.2074413 -4.2038813 -4.20302 -4.203052 -4.203136 -4.2036815]]...]
INFO - root - 2017-12-05 22:28:44.087255: step 46110, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 72h:41m:01s remains)
INFO - root - 2017-12-05 22:28:53.012864: step 46120, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 72h:13m:08s remains)
INFO - root - 2017-12-05 22:29:02.143651: step 46130, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 73h:06m:31s remains)
INFO - root - 2017-12-05 22:29:11.449989: step 46140, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 72h:12m:21s remains)
INFO - root - 2017-12-05 22:29:20.658628: step 46150, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 70h:28m:50s remains)
INFO - root - 2017-12-05 22:29:29.742284: step 46160, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 73h:52m:24s remains)
INFO - root - 2017-12-05 22:29:38.747648: step 46170, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 70h:39m:10s remains)
INFO - root - 2017-12-05 22:29:47.873156: step 46180, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.922 sec/batch; 73h:18m:24s remains)
INFO - root - 2017-12-05 22:29:56.928826: step 46190, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 70h:50m:27s remains)
INFO - root - 2017-12-05 22:30:06.059465: step 46200, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.904 sec/batch; 71h:51m:45s remains)
2017-12-05 22:30:06.844518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2373343 -4.2379336 -4.2462878 -4.2592435 -4.2589827 -4.2433844 -4.2298417 -4.2240763 -4.2285671 -4.2325044 -4.22191 -4.2086544 -4.1927452 -4.17644 -4.1645675][-4.2355666 -4.2399492 -4.2482691 -4.2584882 -4.25267 -4.234304 -4.2232919 -4.2209492 -4.2279563 -4.2349396 -4.2223763 -4.2026443 -4.1835375 -4.1700821 -4.1659212][-4.230114 -4.2386408 -4.2444921 -4.244782 -4.2319527 -4.2113376 -4.2000527 -4.2020283 -4.2165027 -4.233326 -4.2242093 -4.2020283 -4.1811223 -4.1717925 -4.1706238][-4.215508 -4.2256041 -4.2261643 -4.2152815 -4.1916881 -4.1664162 -4.1524148 -4.1631451 -4.1931562 -4.222188 -4.2202883 -4.2024531 -4.1840296 -4.1791935 -4.1792116][-4.1882467 -4.1989393 -4.1980295 -4.1794248 -4.1443167 -4.1077394 -4.0807571 -4.0999055 -4.1519423 -4.1991949 -4.2096372 -4.2001896 -4.1860867 -4.1841731 -4.1878748][-4.1598811 -4.1706915 -4.1685615 -4.1406765 -4.0897331 -4.0275903 -3.9741495 -4.0083871 -4.0948315 -4.1668797 -4.1950626 -4.1960368 -4.1883664 -4.1907082 -4.1998205][-4.1331682 -4.1418333 -4.1378393 -4.1044879 -4.0364723 -3.937139 -3.8496165 -3.9188535 -4.0517931 -4.1485133 -4.1913662 -4.1999707 -4.1979589 -4.2039065 -4.212934][-4.1269293 -4.136507 -4.1358161 -4.1046247 -4.0317802 -3.9153523 -3.8161833 -3.909616 -4.0534925 -4.1478796 -4.1902122 -4.2006378 -4.2008052 -4.2077174 -4.2143741][-4.1359615 -4.1506805 -4.1572556 -4.1368594 -4.0868731 -4.0081463 -3.9539483 -4.016912 -4.1096973 -4.1731567 -4.20134 -4.2081127 -4.20969 -4.21133 -4.2109222][-4.1570625 -4.1734591 -4.1842189 -4.1753945 -4.1500268 -4.1105361 -4.0900636 -4.123353 -4.166935 -4.2043161 -4.2211251 -4.2243986 -4.2237139 -4.2172217 -4.204524][-4.1743641 -4.1872268 -4.1997504 -4.2006812 -4.1871428 -4.167788 -4.1622624 -4.1758404 -4.1946363 -4.21564 -4.2286191 -4.2361765 -4.2340426 -4.2178912 -4.1878581][-4.1844683 -4.196363 -4.2086587 -4.2122321 -4.2039795 -4.1901984 -4.1901646 -4.1956935 -4.2060657 -4.218689 -4.2280421 -4.2357645 -4.2337503 -4.2133803 -4.1728148][-4.1936531 -4.2057133 -4.2180624 -4.2201581 -4.2114339 -4.1968122 -4.19523 -4.196157 -4.1980467 -4.2026048 -4.2127814 -4.2214818 -4.2226367 -4.2093763 -4.1783609][-4.208602 -4.2228141 -4.2335038 -4.2353811 -4.2308116 -4.212337 -4.1963205 -4.1846628 -4.1757021 -4.1740055 -4.1837807 -4.1951489 -4.2040815 -4.2074156 -4.1994629][-4.2227988 -4.2408271 -4.2486563 -4.2511048 -4.2514606 -4.2321711 -4.2062678 -4.1847472 -4.1666589 -4.1580281 -4.1605506 -4.1707516 -4.1922808 -4.2151923 -4.2253952]]...]
INFO - root - 2017-12-05 22:30:15.945527: step 46210, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:52m:42s remains)
INFO - root - 2017-12-05 22:30:25.039185: step 46220, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 71h:52m:51s remains)
INFO - root - 2017-12-05 22:30:34.171306: step 46230, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.916 sec/batch; 72h:50m:26s remains)
INFO - root - 2017-12-05 22:30:43.050493: step 46240, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 70h:46m:49s remains)
INFO - root - 2017-12-05 22:30:52.078672: step 46250, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.920 sec/batch; 73h:09m:27s remains)
INFO - root - 2017-12-05 22:31:01.311008: step 46260, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 74h:53m:06s remains)
INFO - root - 2017-12-05 22:31:10.370634: step 46270, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 72h:20m:22s remains)
INFO - root - 2017-12-05 22:31:19.400284: step 46280, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 0.811 sec/batch; 64h:29m:44s remains)
INFO - root - 2017-12-05 22:31:28.421693: step 46290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 71h:47m:56s remains)
INFO - root - 2017-12-05 22:31:37.598082: step 46300, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 72h:14m:44s remains)
2017-12-05 22:31:38.458609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2086987 -4.2104783 -4.2065058 -4.22424 -4.245409 -4.2467694 -4.236289 -4.2181897 -4.2105231 -4.2207465 -4.2460356 -4.26747 -4.2813911 -4.2829866 -4.2768049][-4.2167859 -4.2148857 -4.2123771 -4.2299333 -4.24833 -4.246798 -4.2376528 -4.2247353 -4.2194519 -4.2293992 -4.2543044 -4.2809033 -4.2973952 -4.2992125 -4.2905235][-4.2171783 -4.215415 -4.215095 -4.2279692 -4.2369971 -4.2354407 -4.2337193 -4.2336426 -4.2347426 -4.2436728 -4.2716856 -4.3035622 -4.3214765 -4.322156 -4.3137121][-4.2039413 -4.2041545 -4.2044411 -4.211956 -4.216291 -4.2177095 -4.2246284 -4.2330065 -4.2378225 -4.2461748 -4.2756386 -4.3083878 -4.3255844 -4.3269963 -4.3204055][-4.1962981 -4.1933851 -4.1909647 -4.1955714 -4.1967697 -4.2011571 -4.2120118 -4.2227278 -4.2299833 -4.2391744 -4.2682571 -4.2977781 -4.3123865 -4.3140521 -4.3108149][-4.1981139 -4.1922064 -4.1843228 -4.1837978 -4.1849685 -4.1910138 -4.2024932 -4.2127056 -4.2210259 -4.2325258 -4.2591047 -4.2856483 -4.2977338 -4.29851 -4.2966275][-4.1913118 -4.1850967 -4.1737051 -4.1678309 -4.1660094 -4.1727462 -4.1799855 -4.1863923 -4.19721 -4.2145658 -4.2409339 -4.2682047 -4.2818584 -4.2841353 -4.2856026][-4.1690559 -4.1605177 -4.1459122 -4.1338363 -4.1279206 -4.1320996 -4.1318045 -4.1288905 -4.1366577 -4.1563687 -4.1829038 -4.217514 -4.2434211 -4.2584739 -4.2692227][-4.1687822 -4.1499257 -4.1282921 -4.1074061 -4.0961761 -4.0969796 -4.0889835 -4.0782933 -4.07918 -4.0969548 -4.1225863 -4.1625743 -4.2006855 -4.2297058 -4.2519417][-4.197257 -4.1759553 -4.1535869 -4.1309152 -4.1183696 -4.1164575 -4.1064482 -4.0917826 -4.0879278 -4.1013393 -4.1218023 -4.1546197 -4.1916003 -4.2250543 -4.2523603][-4.2185311 -4.2109509 -4.1992383 -4.1812863 -4.1694293 -4.1641574 -4.1518264 -4.1367702 -4.1366324 -4.1505828 -4.1665096 -4.1902795 -4.2188215 -4.2444425 -4.2679424][-4.2169523 -4.22068 -4.2197723 -4.2083907 -4.1981893 -4.1923952 -4.1794567 -4.1661134 -4.1726704 -4.1907468 -4.2058611 -4.228302 -4.2540641 -4.2741852 -4.2912312][-4.2137127 -4.220531 -4.2198882 -4.2087836 -4.1994357 -4.195663 -4.1838775 -4.1750431 -4.1908364 -4.2166271 -4.2368336 -4.2639365 -4.2890878 -4.3030009 -4.3129478][-4.22171 -4.2289114 -4.2295632 -4.2241993 -4.2196765 -4.2188921 -4.2098417 -4.2024488 -4.2189012 -4.2449641 -4.2689118 -4.2977324 -4.3195553 -4.3270512 -4.3313832][-4.2240329 -4.2296834 -4.23549 -4.2419744 -4.2507343 -4.259131 -4.2558694 -4.249907 -4.2606821 -4.2798853 -4.2995315 -4.3229237 -4.3373027 -4.3398061 -4.3397574]]...]
INFO - root - 2017-12-05 22:31:47.634151: step 46310, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:47m:56s remains)
INFO - root - 2017-12-05 22:31:56.709918: step 46320, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 73h:06m:56s remains)
INFO - root - 2017-12-05 22:32:05.739739: step 46330, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.812 sec/batch; 64h:34m:43s remains)
INFO - root - 2017-12-05 22:32:14.509829: step 46340, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 70h:24m:11s remains)
INFO - root - 2017-12-05 22:32:23.623749: step 46350, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 73h:08m:50s remains)
INFO - root - 2017-12-05 22:32:32.649122: step 46360, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 69h:25m:44s remains)
INFO - root - 2017-12-05 22:32:41.688218: step 46370, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 70h:42m:23s remains)
INFO - root - 2017-12-05 22:32:50.790727: step 46380, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 71h:32m:29s remains)
INFO - root - 2017-12-05 22:32:59.890781: step 46390, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.910 sec/batch; 72h:17m:19s remains)
INFO - root - 2017-12-05 22:33:08.948398: step 46400, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 71h:53m:04s remains)
2017-12-05 22:33:09.746143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3048148 -4.2799053 -4.2649961 -4.2457943 -4.2237606 -4.197124 -4.1714416 -4.1614356 -4.1581206 -4.1603336 -4.1677547 -4.1876316 -4.2206688 -4.2578058 -4.2881136][-4.3047514 -4.2844048 -4.2710176 -4.2527008 -4.2327051 -4.2005186 -4.1648169 -4.1489263 -4.1463876 -4.1476073 -4.1600165 -4.187582 -4.226263 -4.2668476 -4.29681][-4.3059435 -4.2884626 -4.2751932 -4.2588696 -4.2409544 -4.2075315 -4.1652021 -4.1381536 -4.129889 -4.1272683 -4.143733 -4.1818547 -4.2295685 -4.2730613 -4.3019967][-4.3082504 -4.2897711 -4.2765174 -4.2631059 -4.2455435 -4.2070141 -4.1542344 -4.1140852 -4.0942426 -4.0882039 -4.1114936 -4.1606927 -4.2177997 -4.2636051 -4.2935205][-4.3117075 -4.2913494 -4.2770276 -4.2633309 -4.2441983 -4.2006717 -4.1395321 -4.0861425 -4.0496836 -4.0414023 -4.0779662 -4.13651 -4.1947255 -4.2436786 -4.2800007][-4.3172545 -4.2943373 -4.2763963 -4.2632027 -4.2425966 -4.1952391 -4.1275144 -4.0580378 -4.0029464 -3.9978342 -4.05055 -4.1145544 -4.1708107 -4.2247138 -4.2728815][-4.3267 -4.3043547 -4.2843127 -4.2727613 -4.2529063 -4.2065873 -4.1388354 -4.057426 -3.99019 -3.987844 -4.0415297 -4.1024094 -4.1571822 -4.2129765 -4.2705812][-4.3365541 -4.3176775 -4.2989645 -4.2874556 -4.2703514 -4.2307558 -4.1716251 -4.096559 -4.0354834 -4.031219 -4.064908 -4.1046553 -4.1478167 -4.2000656 -4.2615733][-4.3429646 -4.3278384 -4.3124695 -4.2988739 -4.2797637 -4.2457128 -4.1943831 -4.1349082 -4.091886 -4.0888643 -4.1060152 -4.12085 -4.14317 -4.1851463 -4.2443852][-4.3430538 -4.3315315 -4.3221021 -4.3105555 -4.2892666 -4.2574606 -4.2110353 -4.1631761 -4.1295128 -4.12661 -4.1355519 -4.141726 -4.1520438 -4.1834216 -4.2384167][-4.3346071 -4.3223338 -4.3174295 -4.3138356 -4.2973361 -4.2687049 -4.2281194 -4.190805 -4.1657295 -4.1631451 -4.1702108 -4.1772833 -4.1849294 -4.2088456 -4.255003][-4.32643 -4.309927 -4.303268 -4.2998109 -4.2867346 -4.2637253 -4.2313237 -4.20515 -4.1900425 -4.1908307 -4.2020555 -4.2176723 -4.2306266 -4.2499237 -4.28266][-4.324985 -4.3051724 -4.2920856 -4.2820077 -4.2663293 -4.2479215 -4.22608 -4.210063 -4.1998425 -4.2006054 -4.2148333 -4.2372665 -4.2569151 -4.2755475 -4.2989297][-4.3312163 -4.3108621 -4.2943368 -4.2795377 -4.261075 -4.2455668 -4.2296419 -4.2193227 -4.2119308 -4.2124214 -4.2251134 -4.2460947 -4.2660642 -4.2851119 -4.305706][-4.340301 -4.3234119 -4.30745 -4.292964 -4.2760987 -4.2629452 -4.2489109 -4.2395706 -4.2338572 -4.2336812 -4.2420673 -4.2574062 -4.2742348 -4.2927794 -4.3123536]]...]
INFO - root - 2017-12-05 22:33:18.740351: step 46410, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 71h:52m:32s remains)
INFO - root - 2017-12-05 22:33:27.898625: step 46420, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 71h:50m:22s remains)
INFO - root - 2017-12-05 22:33:36.961767: step 46430, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.920 sec/batch; 73h:04m:53s remains)
INFO - root - 2017-12-05 22:33:46.038062: step 46440, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 72h:48m:58s remains)
INFO - root - 2017-12-05 22:33:54.996835: step 46450, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 69h:09m:55s remains)
INFO - root - 2017-12-05 22:34:04.034273: step 46460, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 71h:49m:13s remains)
INFO - root - 2017-12-05 22:34:13.004521: step 46470, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 66h:37m:05s remains)
INFO - root - 2017-12-05 22:34:21.996550: step 46480, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 71h:37m:37s remains)
INFO - root - 2017-12-05 22:34:31.072225: step 46490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 72h:57m:20s remains)
INFO - root - 2017-12-05 22:34:40.170527: step 46500, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 70h:27m:34s remains)
2017-12-05 22:34:41.067835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2613683 -4.2578158 -4.2519403 -4.2459426 -4.2415466 -4.2388358 -4.235774 -4.2355971 -4.24031 -4.25092 -4.2576652 -4.2520838 -4.2463012 -4.2520156 -4.2618752][-4.2420182 -4.2443066 -4.2436695 -4.2370749 -4.2264404 -4.2144179 -4.2063737 -4.2015157 -4.2075548 -4.226984 -4.2393661 -4.2354431 -4.2313771 -4.2389688 -4.2480154][-4.2255044 -4.2360358 -4.2384734 -4.2305689 -4.2134914 -4.1938453 -4.1735024 -4.1558442 -4.1633086 -4.2007532 -4.2236657 -4.22321 -4.2196026 -4.22832 -4.234664][-4.2132692 -4.227756 -4.2313066 -4.2226896 -4.198288 -4.1704035 -4.1316528 -4.0953474 -4.1104221 -4.171452 -4.2067018 -4.2123809 -4.21015 -4.2164993 -4.2188783][-4.1962433 -4.2134428 -4.218339 -4.2089634 -4.1829014 -4.144948 -4.0813985 -4.0279455 -4.0637565 -4.1489124 -4.1958194 -4.2069259 -4.2032661 -4.20347 -4.1990409][-4.1753645 -4.1971922 -4.2055707 -4.1971765 -4.1679339 -4.1156678 -4.0233293 -3.9570823 -4.0201025 -4.1306329 -4.1891007 -4.2031355 -4.1943064 -4.187057 -4.173358][-4.1694345 -4.1938477 -4.2048988 -4.1937289 -4.1539655 -4.0792127 -3.9572589 -3.8807395 -3.9674358 -4.101 -4.1736808 -4.1942425 -4.1790195 -4.1616278 -4.1409874][-4.1841869 -4.2103457 -4.2209258 -4.2031994 -4.1510887 -4.0544925 -3.9122658 -3.8314183 -3.935714 -4.08783 -4.1685672 -4.1868529 -4.1621571 -4.1302876 -4.1026897][-4.2021637 -4.2298465 -4.2394719 -4.2164879 -4.1560931 -4.0510988 -3.9113512 -3.8391955 -3.9499881 -4.0984559 -4.1716661 -4.1783667 -4.1418719 -4.0953197 -4.0718722][-4.2090616 -4.2398448 -4.2504311 -4.2248116 -4.165102 -4.0689492 -3.9556026 -3.90636 -4.0019989 -4.1200542 -4.1756892 -4.1693878 -4.1189623 -4.0660853 -4.06157][-4.2117682 -4.2425685 -4.2536922 -4.2322321 -4.1817675 -4.1016073 -4.0173368 -3.9904633 -4.0660238 -4.1469593 -4.1790142 -4.1576409 -4.0951653 -4.0510912 -4.0715103][-4.2145863 -4.2425537 -4.2536817 -4.2398562 -4.2025027 -4.141531 -4.0825067 -4.0695848 -4.1232438 -4.1717525 -4.1813321 -4.14483 -4.0850229 -4.06194 -4.1013331][-4.2088542 -4.232091 -4.2433791 -4.2376437 -4.2128353 -4.1709948 -4.1342854 -4.1348281 -4.1688051 -4.1916957 -4.1862807 -4.1447959 -4.0950456 -4.0904961 -4.1369581][-4.2091985 -4.2259407 -4.2360883 -4.2345824 -4.2180886 -4.1929026 -4.1746187 -4.1808553 -4.1997423 -4.2075748 -4.1957 -4.1589394 -4.1240063 -4.13178 -4.1715965][-4.226799 -4.2364149 -4.2437506 -4.2435474 -4.2321568 -4.2212691 -4.2172017 -4.2238321 -4.2307677 -4.2279034 -4.2143731 -4.1887774 -4.1733079 -4.1867781 -4.2108469]]...]
INFO - root - 2017-12-05 22:34:50.111983: step 46510, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 70h:40m:31s remains)
INFO - root - 2017-12-05 22:34:59.094463: step 46520, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.902 sec/batch; 71h:38m:59s remains)
INFO - root - 2017-12-05 22:35:08.158323: step 46530, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 72h:48m:23s remains)
INFO - root - 2017-12-05 22:35:17.304512: step 46540, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 68h:56m:49s remains)
INFO - root - 2017-12-05 22:35:26.246926: step 46550, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 71h:38m:38s remains)
INFO - root - 2017-12-05 22:35:35.335620: step 46560, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 72h:21m:27s remains)
INFO - root - 2017-12-05 22:35:44.439694: step 46570, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 73h:40m:13s remains)
INFO - root - 2017-12-05 22:35:53.429240: step 46580, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 73h:08m:11s remains)
INFO - root - 2017-12-05 22:36:02.738929: step 46590, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 74h:33m:48s remains)
INFO - root - 2017-12-05 22:36:11.844131: step 46600, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 71h:49m:56s remains)
2017-12-05 22:36:12.600775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2822475 -4.2769775 -4.2707996 -4.2540827 -4.2314115 -4.2206888 -4.1972437 -4.1781034 -4.183269 -4.2152991 -4.2446938 -4.2633286 -4.2726912 -4.2703428 -4.2646956][-4.2615871 -4.2544107 -4.2499113 -4.2410369 -4.2321954 -4.2313118 -4.212687 -4.1919842 -4.191802 -4.2193651 -4.2459826 -4.2594528 -4.2608356 -4.2521276 -4.2416563][-4.2412405 -4.229393 -4.2250829 -4.2238636 -4.2271619 -4.2344494 -4.2183003 -4.1935263 -4.1886911 -4.2124186 -4.2393064 -4.2492619 -4.2401457 -4.2202897 -4.205584][-4.2225833 -4.2000251 -4.1920652 -4.19181 -4.2010527 -4.2096214 -4.1917038 -4.1622596 -4.1564584 -4.1823096 -4.216382 -4.2319226 -4.2140946 -4.1809506 -4.1632929][-4.2044415 -4.1715908 -4.1535487 -4.1456537 -4.1491675 -4.1530218 -4.1239781 -4.0813131 -4.0713844 -4.112288 -4.1645827 -4.1915979 -4.1674562 -4.1196108 -4.1023517][-4.1852522 -4.1436362 -4.1113648 -4.0902157 -4.081708 -4.0720453 -4.0206971 -3.9475694 -3.9286416 -4.0002856 -4.0897937 -4.1362062 -4.1091928 -4.0444331 -4.0243478][-4.1620579 -4.117352 -4.073637 -4.0405107 -4.0205536 -3.9925861 -3.9085689 -3.7871654 -3.7515495 -3.8670561 -4.0074911 -4.0869355 -4.0672336 -3.9888482 -3.9573734][-4.1504354 -4.108954 -4.0622048 -4.0226078 -3.9975166 -3.9656017 -3.8672311 -3.7160859 -3.6548607 -3.7830453 -3.9528384 -4.06391 -4.0667052 -3.9963725 -3.9607375][-4.1593866 -4.1260757 -4.0853415 -4.0510049 -4.033916 -4.0176373 -3.9385071 -3.8130684 -3.7476952 -3.8306775 -3.9741764 -4.08393 -4.1048231 -4.05786 -4.0287666][-4.1817307 -4.1569586 -4.12462 -4.1018538 -4.0973892 -4.1011958 -4.0500355 -3.9661512 -3.9125173 -3.9554155 -4.0540128 -4.1384978 -4.1640072 -4.1384811 -4.1201463][-4.213975 -4.1982183 -4.1737013 -4.1614733 -4.1647167 -4.1795926 -4.1507974 -4.0957031 -4.0542622 -4.0763683 -4.1398416 -4.2024736 -4.22872 -4.2198777 -4.2086911][-4.2526469 -4.2409334 -4.221396 -4.2096143 -4.2149997 -4.2345538 -4.2230949 -4.1867328 -4.1526151 -4.1638646 -4.2038488 -4.2500877 -4.2737269 -4.2724276 -4.2645745][-4.2876577 -4.2747521 -4.2567415 -4.2417502 -4.2454329 -4.2661119 -4.2683277 -4.2506666 -4.2252297 -4.2301888 -4.25289 -4.2814164 -4.2959375 -4.2955995 -4.2908597][-4.3098879 -4.2946453 -4.2775559 -4.260314 -4.258215 -4.2753124 -4.2844305 -4.2825394 -4.2673192 -4.2693429 -4.2828412 -4.2980762 -4.3046088 -4.3022313 -4.3003807][-4.3149905 -4.298759 -4.2833948 -4.2683144 -4.2630353 -4.2726889 -4.2812219 -4.284183 -4.2751603 -4.2749057 -4.2854104 -4.2959266 -4.2995815 -4.2992892 -4.2992792]]...]
INFO - root - 2017-12-05 22:36:21.835232: step 46610, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.924 sec/batch; 73h:22m:17s remains)
INFO - root - 2017-12-05 22:36:30.741572: step 46620, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 69h:19m:45s remains)
INFO - root - 2017-12-05 22:36:39.860701: step 46630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 72h:32m:03s remains)
INFO - root - 2017-12-05 22:36:48.992450: step 46640, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.911 sec/batch; 72h:22m:09s remains)
INFO - root - 2017-12-05 22:36:58.095196: step 46650, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.884 sec/batch; 70h:10m:12s remains)
INFO - root - 2017-12-05 22:37:07.062848: step 46660, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 70h:47m:25s remains)
INFO - root - 2017-12-05 22:37:16.126204: step 46670, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 71h:36m:27s remains)
INFO - root - 2017-12-05 22:37:25.027843: step 46680, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 73h:48m:04s remains)
INFO - root - 2017-12-05 22:37:34.177987: step 46690, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 72h:33m:10s remains)
INFO - root - 2017-12-05 22:37:43.304085: step 46700, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 69h:56m:30s remains)
2017-12-05 22:37:44.127828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3063741 -4.2877455 -4.2393217 -4.1692634 -4.1087785 -4.0746155 -4.0791974 -4.1141315 -4.1684246 -4.2088828 -4.230073 -4.2282023 -4.2129769 -4.18852 -4.1543884][-4.309114 -4.2900419 -4.24341 -4.1776657 -4.118907 -4.0850549 -4.0876775 -4.1177249 -4.174016 -4.2218451 -4.2493553 -4.2528777 -4.24428 -4.2267604 -4.1930785][-4.3083096 -4.29168 -4.2484126 -4.1863155 -4.1269054 -4.0889378 -4.0841165 -4.1099939 -4.1714678 -4.2288804 -4.2624555 -4.27539 -4.2753181 -4.2657537 -4.2364845][-4.3082104 -4.2951713 -4.2555 -4.1939607 -4.1289721 -4.082058 -4.065382 -4.091845 -4.1654491 -4.2336731 -4.270452 -4.2890439 -4.2926531 -4.2868915 -4.2626295][-4.3105593 -4.3012047 -4.2647958 -4.2030749 -4.1273351 -4.0612454 -4.0235643 -4.0492392 -4.1417661 -4.2261152 -4.2696848 -4.2920036 -4.2993608 -4.2960835 -4.2782187][-4.3138123 -4.3085 -4.2777691 -4.2192073 -4.1336484 -4.0437536 -3.97205 -3.9894111 -4.1035094 -4.208415 -4.26344 -4.2888179 -4.2987227 -4.2972031 -4.2852478][-4.3159161 -4.3142786 -4.2919078 -4.24188 -4.1570544 -4.0556273 -3.9546065 -3.9497836 -4.0662937 -4.1811056 -4.2450271 -4.2741866 -4.2865887 -4.2863793 -4.2797461][-4.3173904 -4.3169465 -4.3015156 -4.2615833 -4.1847882 -4.0832973 -3.9676971 -3.9380651 -4.0361991 -4.1473436 -4.2135386 -4.247098 -4.2632422 -4.2651978 -4.2611771][-4.3177834 -4.3173785 -4.3081026 -4.27856 -4.2145734 -4.118257 -4.0006952 -3.9491112 -4.0156612 -4.1127863 -4.177753 -4.2150064 -4.233254 -4.2372475 -4.239995][-4.3181629 -4.3163853 -4.31112 -4.2904286 -4.2403965 -4.1544566 -4.0466938 -3.9837046 -4.0138612 -4.0844026 -4.1420727 -4.1812673 -4.199966 -4.2082496 -4.2221069][-4.3191147 -4.3153996 -4.3102279 -4.2943592 -4.2550197 -4.1805191 -4.0864725 -4.0219145 -4.0230455 -4.0627179 -4.1102152 -4.152113 -4.1737332 -4.1846852 -4.2061276][-4.3203154 -4.315618 -4.3091264 -4.2951026 -4.262567 -4.198638 -4.1175156 -4.0529447 -4.0320168 -4.0459814 -4.0845475 -4.1308408 -4.1578856 -4.1734552 -4.1952314][-4.3196888 -4.3146715 -4.3073945 -4.295207 -4.2685814 -4.2152781 -4.1463242 -4.0820909 -4.0429664 -4.039453 -4.0724206 -4.1215487 -4.1536908 -4.1772184 -4.2000647][-4.3157682 -4.3097839 -4.3018222 -4.2917857 -4.2722631 -4.2318907 -4.176805 -4.115859 -4.0655446 -4.0533404 -4.0855956 -4.1361709 -4.17117 -4.1998806 -4.224535][-4.3080435 -4.3017006 -4.293355 -4.2853832 -4.27312 -4.2465582 -4.2075434 -4.159936 -4.1141586 -4.1015191 -4.1306176 -4.172699 -4.202528 -4.2299695 -4.2535405]]...]
INFO - root - 2017-12-05 22:37:53.128589: step 46710, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 71h:27m:22s remains)
INFO - root - 2017-12-05 22:38:02.179922: step 46720, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 68h:58m:32s remains)
INFO - root - 2017-12-05 22:38:11.377650: step 46730, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 71h:55m:18s remains)
INFO - root - 2017-12-05 22:38:20.486604: step 46740, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 72h:34m:18s remains)
INFO - root - 2017-12-05 22:38:29.511017: step 46750, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 74h:23m:57s remains)
INFO - root - 2017-12-05 22:38:38.741242: step 46760, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 72h:28m:43s remains)
INFO - root - 2017-12-05 22:38:47.851635: step 46770, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 68h:34m:38s remains)
INFO - root - 2017-12-05 22:38:56.947307: step 46780, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 71h:58m:26s remains)
INFO - root - 2017-12-05 22:39:06.139487: step 46790, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 73h:03m:10s remains)
INFO - root - 2017-12-05 22:39:15.290411: step 46800, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 74h:31m:44s remains)
2017-12-05 22:39:16.088434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1997223 -4.2242446 -4.254106 -4.2787366 -4.2950268 -4.2913313 -4.2796717 -4.2704134 -4.2696166 -4.2708883 -4.2743783 -4.2835631 -4.288507 -4.2858882 -4.2808375][-4.18951 -4.2235913 -4.2593946 -4.2869864 -4.2998772 -4.290307 -4.27326 -4.258287 -4.2512717 -4.2482042 -4.2510862 -4.2629852 -4.2730517 -4.27362 -4.2720895][-4.1867218 -4.2201877 -4.2521148 -4.2768908 -4.2850881 -4.2750211 -4.2558665 -4.2408824 -4.2338009 -4.2276211 -4.227046 -4.2381816 -4.2513824 -4.2597408 -4.261476][-4.1974635 -4.2223878 -4.24144 -4.2540679 -4.253026 -4.2427859 -4.2235436 -4.2119761 -4.2134213 -4.2130489 -4.214036 -4.2251339 -4.2393475 -4.2520752 -4.2534103][-4.2165313 -4.2301345 -4.2365174 -4.236084 -4.2240582 -4.207056 -4.1827269 -4.1670761 -4.1752305 -4.1866837 -4.1998792 -4.2222786 -4.239285 -4.2448034 -4.2388144][-4.2200613 -4.2247448 -4.2224607 -4.2145343 -4.1973414 -4.1680431 -4.131887 -4.1052942 -4.1130109 -4.1385736 -4.1643806 -4.1979046 -4.2214184 -4.2254391 -4.2202921][-4.208241 -4.2078929 -4.2022257 -4.1980095 -4.1861095 -4.157258 -4.1153951 -4.0781622 -4.0724025 -4.0932331 -4.1179657 -4.15196 -4.1864467 -4.2036481 -4.2080946][-4.193171 -4.1919847 -4.1886182 -4.194068 -4.197525 -4.1827188 -4.1531878 -4.1172915 -4.0923204 -4.0840673 -4.0856991 -4.1119871 -4.1553483 -4.1884589 -4.2038293][-4.18992 -4.1901159 -4.1908221 -4.198173 -4.2119074 -4.2147632 -4.2068229 -4.1870546 -4.1643944 -4.1390505 -4.1165543 -4.1256065 -4.1586828 -4.1904516 -4.2083592][-4.1902313 -4.1900826 -4.1933322 -4.2017884 -4.2174158 -4.2317748 -4.2434945 -4.2470174 -4.2419925 -4.2192116 -4.1893573 -4.1820979 -4.1951022 -4.2089267 -4.2189364][-4.1803265 -4.1818829 -4.1841226 -4.19046 -4.2056265 -4.2285419 -4.2545371 -4.2757049 -4.2853823 -4.2719765 -4.2460532 -4.2321286 -4.2312765 -4.2312703 -4.2335382][-4.1735287 -4.1771894 -4.1786056 -4.1794534 -4.1884251 -4.2135468 -4.248611 -4.2824993 -4.3021493 -4.2991433 -4.2809644 -4.2665615 -4.2587652 -4.246428 -4.2325759][-4.180934 -4.1909251 -4.1954951 -4.1898966 -4.1872325 -4.2030306 -4.2400713 -4.281426 -4.306469 -4.3109322 -4.299696 -4.2860904 -4.2686343 -4.2413487 -4.2090235][-4.2016816 -4.2143536 -4.219027 -4.2095442 -4.1959805 -4.1955218 -4.2220373 -4.2621512 -4.2914453 -4.3046322 -4.3021774 -4.2895322 -4.267158 -4.2294211 -4.1819587][-4.2203383 -4.2271786 -4.2290583 -4.2191072 -4.2020292 -4.1921549 -4.2054052 -4.2354069 -4.2632685 -4.2836738 -4.2924552 -4.2889934 -4.2711487 -4.2344608 -4.1826015]]...]
INFO - root - 2017-12-05 22:39:25.008739: step 46810, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 72h:46m:51s remains)
INFO - root - 2017-12-05 22:39:34.009634: step 46820, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 69h:49m:40s remains)
INFO - root - 2017-12-05 22:39:43.015578: step 46830, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 73h:14m:43s remains)
INFO - root - 2017-12-05 22:39:52.106453: step 46840, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 73h:09m:35s remains)
INFO - root - 2017-12-05 22:40:01.074168: step 46850, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 70h:25m:38s remains)
INFO - root - 2017-12-05 22:40:10.152054: step 46860, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 72h:31m:30s remains)
INFO - root - 2017-12-05 22:40:19.284051: step 46870, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 70h:20m:44s remains)
INFO - root - 2017-12-05 22:40:28.409573: step 46880, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 71h:38m:29s remains)
INFO - root - 2017-12-05 22:40:37.365438: step 46890, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 68h:42m:30s remains)
INFO - root - 2017-12-05 22:40:46.482692: step 46900, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 72h:48m:02s remains)
2017-12-05 22:40:47.236614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1723151 -4.1662526 -4.1606784 -4.1652927 -4.1737795 -4.1713519 -4.1544361 -4.1333046 -4.1183887 -4.1240578 -4.1243005 -4.1053848 -4.0968056 -4.1229606 -4.1667776][-4.210021 -4.2065196 -4.1972008 -4.1955285 -4.1975565 -4.1913781 -4.176146 -4.1644907 -4.15673 -4.16714 -4.1662397 -4.1415176 -4.1246281 -4.139688 -4.1760883][-4.2192035 -4.2164097 -4.2082109 -4.2088776 -4.2130303 -4.2085123 -4.1972766 -4.1920195 -4.1917367 -4.2109461 -4.2131462 -4.1855111 -4.1582785 -4.1558771 -4.1783619][-4.1873627 -4.1839619 -4.1804852 -4.1914053 -4.2038074 -4.2012277 -4.1817174 -4.1730957 -4.1840768 -4.2179637 -4.2309852 -4.2073674 -4.1780744 -4.1665149 -4.1748676][-4.1235256 -4.1171956 -4.1224 -4.1494608 -4.1746631 -4.1705923 -4.1342888 -4.1110973 -4.1308656 -4.1822004 -4.2124619 -4.2017403 -4.1814985 -4.1716785 -4.1700115][-4.0527005 -4.044487 -4.0571218 -4.0977483 -4.1332445 -4.1268883 -4.0742974 -4.0353818 -4.0631509 -4.131391 -4.1786838 -4.1799984 -4.1635208 -4.1520462 -4.1475196][-4.0235443 -4.007452 -4.007174 -4.0376348 -4.068325 -4.0456891 -3.9673607 -3.910862 -3.9503798 -4.043211 -4.110846 -4.1280556 -4.1172948 -4.1058149 -4.1027193][-4.072124 -4.0505085 -4.0366821 -4.0465035 -4.0487657 -3.9894159 -3.8708477 -3.7887611 -3.840138 -3.9553804 -4.0415897 -4.0750942 -4.0773206 -4.0739212 -4.0771451][-4.1624093 -4.1479611 -4.134068 -4.1288385 -4.1105614 -4.041965 -3.9348223 -3.8669536 -3.8998983 -3.981029 -4.046227 -4.0740232 -4.0768852 -4.0790224 -4.088016][-4.2298031 -4.2261968 -4.2154675 -4.2032385 -4.1775804 -4.1173944 -4.0374131 -3.9862347 -3.995364 -4.0370908 -4.0760384 -4.0941219 -4.095593 -4.1009293 -4.1120725][-4.2709594 -4.27256 -4.2630162 -4.249866 -4.2246547 -4.1742735 -4.1151495 -4.077065 -4.07553 -4.0981069 -4.1204534 -4.1301479 -4.12929 -4.1333966 -4.1395478][-4.2997408 -4.3033061 -4.2986608 -4.28989 -4.269424 -4.2290716 -4.185782 -4.1609097 -4.1621065 -4.178762 -4.1922979 -4.1940169 -4.1857805 -4.1816182 -4.1779327][-4.3011684 -4.3092809 -4.3100843 -4.3082833 -4.2953467 -4.2644777 -4.230689 -4.2128329 -4.2171164 -4.2300782 -4.2389731 -4.2388897 -4.2295218 -4.2206006 -4.2099552][-4.265132 -4.2800627 -4.2874207 -4.2934957 -4.2885103 -4.2700305 -4.2470274 -4.2331524 -4.2353125 -4.2441587 -4.25187 -4.2543173 -4.2502713 -4.2423148 -4.2318454][-4.2211232 -4.2361608 -4.2449055 -4.2539144 -4.2554455 -4.2489395 -4.2390337 -4.23219 -4.2351456 -4.2423267 -4.250351 -4.25601 -4.2590828 -4.25729 -4.250412]]...]
INFO - root - 2017-12-05 22:40:56.305010: step 46910, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.929 sec/batch; 73h:41m:35s remains)
INFO - root - 2017-12-05 22:41:05.462880: step 46920, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 69h:34m:26s remains)
INFO - root - 2017-12-05 22:41:14.589177: step 46930, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:39m:13s remains)
INFO - root - 2017-12-05 22:41:23.564300: step 46940, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 74h:41m:01s remains)
INFO - root - 2017-12-05 22:41:32.778349: step 46950, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 69h:07m:14s remains)
INFO - root - 2017-12-05 22:41:41.916314: step 46960, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 72h:25m:54s remains)
INFO - root - 2017-12-05 22:41:50.914305: step 46970, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 71h:36m:29s remains)
INFO - root - 2017-12-05 22:42:00.064998: step 46980, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 72h:01m:02s remains)
INFO - root - 2017-12-05 22:42:08.984723: step 46990, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 65h:39m:53s remains)
INFO - root - 2017-12-05 22:42:18.032904: step 47000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 72h:09m:47s remains)
2017-12-05 22:42:18.896729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2266684 -4.2395115 -4.245719 -4.2494116 -4.2508335 -4.25243 -4.2575541 -4.2709947 -4.2889342 -4.2987776 -4.3006768 -4.29749 -4.285099 -4.2708607 -4.2640438][-4.2374258 -4.2468276 -4.2490149 -4.2486358 -4.2454081 -4.2460356 -4.2562065 -4.273767 -4.2938972 -4.305017 -4.3028831 -4.2951484 -4.2828875 -4.2670736 -4.2602234][-4.2528296 -4.2579932 -4.2541356 -4.249145 -4.2430744 -4.2414918 -4.2520556 -4.2700977 -4.2846556 -4.2903924 -4.2815957 -4.2709618 -4.2619586 -4.2475867 -4.2427278][-4.2656183 -4.2665682 -4.26064 -4.2506733 -4.2394686 -4.231462 -4.2341895 -4.2450123 -4.2540522 -4.2578545 -4.2466631 -4.2345014 -4.2288094 -4.2192726 -4.2141886][-4.2588153 -4.255743 -4.2477908 -4.2350078 -4.2197065 -4.204771 -4.1978426 -4.1938868 -4.195322 -4.1982284 -4.1929617 -4.1885915 -4.1943021 -4.1966062 -4.1926789][-4.2369251 -4.2275138 -4.2145796 -4.1974058 -4.177701 -4.1568465 -4.1350851 -4.1152978 -4.1104951 -4.11703 -4.1227221 -4.1334939 -4.1580224 -4.1777291 -4.1828794][-4.19967 -4.1872029 -4.1752205 -4.1607881 -4.1387591 -4.1100216 -4.067029 -4.0257597 -4.023746 -4.0503259 -4.0764012 -4.1042371 -4.1420293 -4.1767564 -4.1947379][-4.1570573 -4.1407709 -4.1343627 -4.1291304 -4.1132126 -4.0852461 -4.03342 -3.9785335 -3.9883628 -4.0451303 -4.0919876 -4.126122 -4.1586156 -4.1932893 -4.2188239][-4.1276312 -4.1136546 -4.113853 -4.1166563 -4.1142273 -4.1017532 -4.0643291 -4.024478 -4.0445147 -4.10544 -4.1473203 -4.1720982 -4.1905689 -4.21196 -4.2295709][-4.120759 -4.1185465 -4.128068 -4.1383972 -4.1460776 -4.1519823 -4.1383619 -4.1248274 -4.1453309 -4.1875372 -4.2106428 -4.2209287 -4.2242436 -4.2272096 -4.2286038][-4.126792 -4.1325865 -4.151556 -4.1687984 -4.1824784 -4.199039 -4.2025046 -4.19977 -4.2100224 -4.2303953 -4.2437406 -4.2446671 -4.2396317 -4.2351022 -4.2292752][-4.1304059 -4.1397581 -4.1651993 -4.1892252 -4.2080731 -4.2257671 -4.233048 -4.2291818 -4.2285662 -4.2375517 -4.2441459 -4.2450681 -4.2425904 -4.2396417 -4.2313142][-4.1276536 -4.1396828 -4.1716 -4.2009263 -4.2187095 -4.2298479 -4.2329869 -4.2282014 -4.2264886 -4.2304659 -4.23569 -4.240005 -4.2414765 -4.2384257 -4.2272611][-4.1220078 -4.140008 -4.1764278 -4.2057462 -4.2204733 -4.2285671 -4.2280331 -4.2260065 -4.2282724 -4.2313976 -4.2340245 -4.2363753 -4.2370853 -4.2324791 -4.2204957][-4.1361508 -4.1594868 -4.1938705 -4.219974 -4.2328582 -4.2378569 -4.237083 -4.2367344 -4.2419481 -4.2465882 -4.2452154 -4.2381816 -4.2334962 -4.2297478 -4.2266736]]...]
INFO - root - 2017-12-05 22:42:27.947440: step 47010, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 72h:22m:28s remains)
INFO - root - 2017-12-05 22:42:37.042861: step 47020, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 72h:27m:36s remains)
INFO - root - 2017-12-05 22:42:45.947096: step 47030, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.814 sec/batch; 64h:34m:57s remains)
INFO - root - 2017-12-05 22:42:54.905485: step 47040, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:48m:07s remains)
INFO - root - 2017-12-05 22:43:04.148413: step 47050, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.962 sec/batch; 76h:14m:52s remains)
INFO - root - 2017-12-05 22:43:13.338326: step 47060, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 72h:56m:38s remains)
INFO - root - 2017-12-05 22:43:22.460278: step 47070, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 73h:46m:00s remains)
INFO - root - 2017-12-05 22:43:31.461820: step 47080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 71h:34m:00s remains)
INFO - root - 2017-12-05 22:43:40.220309: step 47090, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 72h:19m:27s remains)
INFO - root - 2017-12-05 22:43:49.317264: step 47100, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 73h:26m:59s remains)
2017-12-05 22:43:50.112246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.183547 -4.1903982 -4.2019382 -4.2080531 -4.2051239 -4.1915679 -4.1705642 -4.1540852 -4.1548939 -4.158093 -4.1498055 -4.1323061 -4.1191239 -4.1137214 -4.1189237][-4.1533647 -4.1530437 -4.1645226 -4.1751504 -4.1786575 -4.1718426 -4.1550455 -4.1445518 -4.1500568 -4.15763 -4.1545162 -4.1421604 -4.1372061 -4.1328807 -4.13215][-4.1160421 -4.10531 -4.1156611 -4.1326065 -4.1422997 -4.1459279 -4.14125 -4.1359344 -4.1432762 -4.1540356 -4.1550412 -4.1513362 -4.1536674 -4.1517763 -4.1482959][-4.0868616 -4.06377 -4.0705671 -4.0928969 -4.1049118 -4.1204791 -4.1301751 -4.1273675 -4.1356754 -4.14363 -4.1444058 -4.1497383 -4.1623487 -4.16884 -4.1664062][-4.0809846 -4.0434828 -4.0384135 -4.05325 -4.0641451 -4.08836 -4.1088409 -4.1075788 -4.1121969 -4.1157222 -4.1235271 -4.1425056 -4.1667442 -4.1814108 -4.1797709][-4.0857987 -4.0414567 -4.0268011 -4.0258956 -4.0298047 -4.0520716 -4.0680475 -4.0605254 -4.0577927 -4.069324 -4.0969706 -4.1325464 -4.1653357 -4.1836791 -4.1861558][-4.0895772 -4.0522628 -4.0370445 -4.0266385 -4.0169125 -4.0170627 -4.0033207 -3.9800255 -3.9861968 -4.0211105 -4.0685444 -4.1133285 -4.150907 -4.1755452 -4.186687][-4.0854688 -4.0618773 -4.0532827 -4.0446782 -4.0236421 -3.9896927 -3.9372122 -3.904563 -3.9350548 -3.9873815 -4.043263 -4.0910535 -4.1296082 -4.160121 -4.1800542][-4.0880008 -4.0809689 -4.0814161 -4.0754061 -4.04692 -3.9898202 -3.9162381 -3.8904996 -3.9391456 -3.9982343 -4.050138 -4.09096 -4.1205077 -4.1467857 -4.168046][-4.0968237 -4.098609 -4.1020079 -4.0952792 -4.0657454 -4.0116243 -3.9470112 -3.9378092 -3.9899483 -4.0406036 -4.0798125 -4.1056166 -4.1237283 -4.1433139 -4.1634097][-4.1068907 -4.1035185 -4.0984569 -4.0874176 -4.0710807 -4.0439887 -4.0098944 -4.0130296 -4.0490088 -4.0819759 -4.1108704 -4.1298814 -4.1420565 -4.1541739 -4.1685295][-4.12169 -4.1060996 -4.0889168 -4.0767455 -4.071137 -4.0706062 -4.0608578 -4.06443 -4.0785966 -4.0990024 -4.1312227 -4.1585212 -4.1690531 -4.1689534 -4.1728687][-4.1312766 -4.1120868 -4.09212 -4.0790353 -4.0743947 -4.080286 -4.0813894 -4.0846543 -4.0857816 -4.1015558 -4.1411982 -4.17446 -4.1807432 -4.17334 -4.1733584][-4.1305246 -4.1131444 -4.098815 -4.0886226 -4.0814738 -4.0854397 -4.0930004 -4.0995965 -4.091073 -4.0968704 -4.1331959 -4.1673346 -4.1738667 -4.17111 -4.1786][-4.1278644 -4.1196804 -4.1115026 -4.1009746 -4.0887532 -4.0908456 -4.1075182 -4.1176209 -4.0995941 -4.0917668 -4.1201367 -4.1539388 -4.1652269 -4.1717834 -4.1835012]]...]
INFO - root - 2017-12-05 22:43:59.276961: step 47110, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 69h:46m:24s remains)
INFO - root - 2017-12-05 22:44:08.368508: step 47120, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 71h:52m:41s remains)
INFO - root - 2017-12-05 22:44:17.351063: step 47130, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 70h:43m:20s remains)
INFO - root - 2017-12-05 22:44:26.450030: step 47140, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.888 sec/batch; 70h:23m:57s remains)
INFO - root - 2017-12-05 22:44:35.549500: step 47150, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 73h:30m:04s remains)
INFO - root - 2017-12-05 22:44:44.660764: step 47160, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 71h:32m:08s remains)
INFO - root - 2017-12-05 22:44:53.688391: step 47170, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 70h:01m:15s remains)
INFO - root - 2017-12-05 22:45:02.715367: step 47180, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.955 sec/batch; 75h:39m:04s remains)
INFO - root - 2017-12-05 22:45:11.752236: step 47190, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 70h:40m:04s remains)
INFO - root - 2017-12-05 22:45:20.876050: step 47200, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 71h:49m:19s remains)
2017-12-05 22:45:21.737072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2250009 -4.2132263 -4.2046738 -4.1943445 -4.1859732 -4.1871881 -4.1915131 -4.1964521 -4.2007637 -4.2071838 -4.2150679 -4.2208176 -4.2284832 -4.2356982 -4.2450776][-4.2087474 -4.1955762 -4.188931 -4.1793947 -4.17077 -4.1720719 -4.1774926 -4.1810179 -4.181313 -4.1892338 -4.202827 -4.2101927 -4.2176247 -4.22598 -4.2385368][-4.1838603 -4.1743488 -4.1715703 -4.161634 -4.1502781 -4.1516771 -4.15835 -4.1587529 -4.1540074 -4.1639171 -4.181325 -4.1909208 -4.20128 -4.2136312 -4.2288766][-4.1623459 -4.1607943 -4.1612182 -4.1486859 -4.1328321 -4.1308455 -4.1365962 -4.1391945 -4.1331105 -4.1411557 -4.1558585 -4.163681 -4.1778703 -4.194602 -4.2132692][-4.1414804 -4.1404657 -4.1352715 -4.1127224 -4.0868659 -4.0773373 -4.0900645 -4.1071467 -4.1079345 -4.1155667 -4.1289062 -4.1374722 -4.1523724 -4.171289 -4.1916871][-4.1160159 -4.1100988 -4.0937839 -4.0531216 -4.0031977 -3.9782467 -4.0017905 -4.0374193 -4.0489144 -4.0597911 -4.0805774 -4.1030846 -4.1269627 -4.1498713 -4.1719227][-4.1088567 -4.09958 -4.0811524 -4.0344453 -3.9702141 -3.9311819 -3.9554689 -3.9934273 -4.0037 -4.0113878 -4.0358 -4.0663829 -4.096818 -4.1284103 -4.1554155][-4.1369405 -4.1280379 -4.1138763 -4.079423 -4.0273108 -3.992873 -4.0001326 -4.0151772 -4.0126805 -4.0123444 -4.0336871 -4.0642638 -4.095129 -4.1298938 -4.1580286][-4.18395 -4.178864 -4.1695981 -4.1478777 -4.1157961 -4.0909958 -4.0871229 -4.0865049 -4.0741034 -4.0659866 -4.0787759 -4.1025481 -4.127924 -4.15687 -4.1802845][-4.23142 -4.2288966 -4.2199841 -4.2023063 -4.1809034 -4.1644535 -4.1623268 -4.1589069 -4.1449103 -4.1346536 -4.1416311 -4.1579785 -4.17798 -4.202044 -4.2229476][-4.2681742 -4.2653184 -4.2559314 -4.2405944 -4.2250652 -4.2132668 -4.2130857 -4.2094011 -4.1993961 -4.1917748 -4.1963263 -4.2092724 -4.2265396 -4.2480969 -4.2639866][-4.2752767 -4.2720752 -4.2628322 -4.2513785 -4.2451344 -4.2432628 -4.2474041 -4.2511535 -4.2509537 -4.2465873 -4.2463937 -4.2541981 -4.266993 -4.2823839 -4.2905216][-4.2567191 -4.2523589 -4.243506 -4.2362761 -4.2387424 -4.2464051 -4.2572842 -4.2693553 -4.2766538 -4.2775617 -4.2774124 -4.2803221 -4.2875228 -4.2956305 -4.2973137][-4.2335749 -4.2277527 -4.2194357 -4.2144175 -4.2217569 -4.235147 -4.2505841 -4.2663765 -4.2782154 -4.2848535 -4.2879572 -4.2895279 -4.2919979 -4.2943239 -4.2920103][-4.2436652 -4.237956 -4.2286644 -4.2229047 -4.2285643 -4.2397995 -4.2526994 -4.2651582 -4.2764721 -4.2840471 -4.2882414 -4.2893767 -4.2898407 -4.2906775 -4.2887349]]...]
INFO - root - 2017-12-05 22:45:30.800705: step 47210, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 69h:35m:15s remains)
INFO - root - 2017-12-05 22:45:39.731292: step 47220, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 73h:40m:31s remains)
INFO - root - 2017-12-05 22:45:48.824031: step 47230, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.914 sec/batch; 72h:24m:24s remains)
INFO - root - 2017-12-05 22:45:58.026813: step 47240, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 71h:57m:26s remains)
INFO - root - 2017-12-05 22:46:07.114372: step 47250, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 73h:22m:58s remains)
INFO - root - 2017-12-05 22:46:16.086343: step 47260, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 71h:34m:08s remains)
INFO - root - 2017-12-05 22:46:25.295576: step 47270, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 71h:44m:05s remains)
INFO - root - 2017-12-05 22:46:34.312491: step 47280, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.926 sec/batch; 73h:23m:17s remains)
INFO - root - 2017-12-05 22:46:43.391380: step 47290, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 71h:52m:03s remains)
INFO - root - 2017-12-05 22:46:52.392552: step 47300, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 73h:24m:16s remains)
2017-12-05 22:46:53.239784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3087735 -4.3021197 -4.295465 -4.2951093 -4.3012338 -4.311049 -4.3221588 -4.3309355 -4.3353844 -4.3332753 -4.3226886 -4.3079119 -4.3035851 -4.31167 -4.3240623][-4.2861724 -4.2780819 -4.2713208 -4.2723737 -4.2785583 -4.2900596 -4.3044038 -4.3204279 -4.3343849 -4.3363171 -4.3229642 -4.3002458 -4.2892761 -4.2952566 -4.3092628][-4.2655244 -4.2560949 -4.2465343 -4.2455974 -4.2480464 -4.2534666 -4.2640548 -4.2863517 -4.3171835 -4.3353724 -4.3328032 -4.3141541 -4.2985473 -4.2965779 -4.3049297][-4.2498274 -4.2386942 -4.2244692 -4.2142234 -4.2050686 -4.1942472 -4.1910491 -4.2141314 -4.2638807 -4.3095145 -4.3311892 -4.3309965 -4.3228912 -4.3159318 -4.3158536][-4.2418261 -4.2298436 -4.2087317 -4.1855478 -4.1600451 -4.1246824 -4.0904808 -4.0921907 -4.1495619 -4.2242537 -4.2807932 -4.3129563 -4.3289118 -4.33356 -4.3335719][-4.2260332 -4.2098784 -4.1811934 -4.1453552 -4.1067224 -4.052916 -3.9827976 -3.9431391 -3.9844522 -4.0781212 -4.1714134 -4.2433739 -4.2922 -4.3233361 -4.3403444][-4.2028847 -4.1816382 -4.1467576 -4.1066918 -4.0677137 -4.0061193 -3.9059708 -3.8158078 -3.8156424 -3.9105711 -4.0284896 -4.1316037 -4.20663 -4.2642789 -4.3093023][-4.176806 -4.1526451 -4.117249 -4.0797434 -4.0476317 -3.9899981 -3.8868651 -3.7735853 -3.727072 -3.7842746 -3.8887596 -3.9940546 -4.0817771 -4.1637597 -4.2397251][-4.1532159 -4.1266379 -4.0945811 -4.065053 -4.0458179 -4.0028582 -3.9213126 -3.8285797 -3.770772 -3.7744203 -3.8231542 -3.8932056 -3.9699454 -4.0629511 -4.1579423][-4.16242 -4.1386542 -4.1136761 -4.0951314 -4.090292 -4.0688057 -4.0197835 -3.9617569 -3.9118466 -3.88269 -3.8811388 -3.9043906 -3.952306 -4.0319905 -4.1206393][-4.2149639 -4.1995616 -4.1814184 -4.1716208 -4.1764755 -4.1729307 -4.1530809 -4.1244783 -4.0906606 -4.0560727 -4.0319619 -4.0278563 -4.0493407 -4.0997052 -4.1599669][-4.2770824 -4.272141 -4.263216 -4.2597628 -4.2690454 -4.2743621 -4.2707996 -4.2589464 -4.24025 -4.2165618 -4.1937966 -4.1792407 -4.1814981 -4.2034039 -4.2328591][-4.3286943 -4.3295183 -4.3276014 -4.3292069 -4.3380785 -4.3433185 -4.3428965 -4.3371029 -4.328341 -4.3169465 -4.3036566 -4.2904649 -4.2853713 -4.2916656 -4.300724][-4.3547444 -4.355104 -4.35465 -4.3563151 -4.3616171 -4.364233 -4.3642974 -4.3623447 -4.360734 -4.3585296 -4.354434 -4.3466306 -4.3418317 -4.3422732 -4.34231][-4.3555503 -4.3544211 -4.3540759 -4.3546066 -4.3562546 -4.3565831 -4.356184 -4.355648 -4.3565426 -4.3581977 -4.3589497 -4.356565 -4.3551121 -4.3550997 -4.3533716]]...]
INFO - root - 2017-12-05 22:47:02.470089: step 47310, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 72h:11m:24s remains)
INFO - root - 2017-12-05 22:47:11.522132: step 47320, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:44m:14s remains)
INFO - root - 2017-12-05 22:47:20.499193: step 47330, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 68h:30m:30s remains)
INFO - root - 2017-12-05 22:47:29.695340: step 47340, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 72h:59m:24s remains)
INFO - root - 2017-12-05 22:47:39.023459: step 47350, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.957 sec/batch; 75h:45m:49s remains)
INFO - root - 2017-12-05 22:47:48.166106: step 47360, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 72h:26m:53s remains)
INFO - root - 2017-12-05 22:47:57.017496: step 47370, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 67h:18m:15s remains)
INFO - root - 2017-12-05 22:48:06.208482: step 47380, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 73h:41m:12s remains)
INFO - root - 2017-12-05 22:48:15.225957: step 47390, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 73h:54m:06s remains)
INFO - root - 2017-12-05 22:48:24.266275: step 47400, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 70h:21m:00s remains)
2017-12-05 22:48:25.091972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1784277 -4.1838727 -4.1763496 -4.1704278 -4.1615849 -4.161952 -4.1687732 -4.1723356 -4.169332 -4.1659374 -4.1709933 -4.1767612 -4.1688333 -4.1491432 -4.1252689][-4.1637349 -4.1548343 -4.1391106 -4.1294222 -4.119803 -4.1180439 -4.123549 -4.1275368 -4.1131434 -4.0931382 -4.0945497 -4.1084547 -4.1104183 -4.0964432 -4.0726938][-4.1171393 -4.0996184 -4.0825181 -4.0730977 -4.0666294 -4.0649428 -4.074388 -4.0841718 -4.0667987 -4.0435524 -4.0495706 -4.0671988 -4.0689092 -4.0619841 -4.0410442][-4.0611172 -4.0532908 -4.0478978 -4.0448556 -4.0388975 -4.0283008 -4.0323739 -4.0466533 -4.0279713 -4.0072355 -4.0213127 -4.0444889 -4.0471215 -4.0459332 -4.0382977][-4.031426 -4.0438051 -4.0564041 -4.0673032 -4.0650005 -4.04519 -4.0372896 -4.0374279 -4.0091853 -3.989023 -4.0104756 -4.0416794 -4.0541692 -4.0623193 -4.0696874][-4.0574718 -4.0844612 -4.1027746 -4.1130223 -4.1011763 -4.0613704 -4.0297565 -4.0011058 -3.9653373 -3.9556265 -3.9869795 -4.0352564 -4.0646081 -4.0876317 -4.1085825][-4.1235085 -4.1426039 -4.1464958 -4.1421404 -4.112402 -4.0498128 -3.9941387 -3.945303 -3.9160957 -3.9320302 -3.9760935 -4.0372276 -4.0840812 -4.1217303 -4.15468][-4.1753469 -4.1744428 -4.1640272 -4.1486521 -4.1071939 -4.0421872 -3.9873228 -3.9321043 -3.9184556 -3.9620197 -4.0129347 -4.0756683 -4.1292176 -4.1728992 -4.2032313][-4.1974106 -4.1861696 -4.1735206 -4.1593981 -4.1211805 -4.0763726 -4.0401111 -3.9967904 -4.0016952 -4.0577617 -4.1070681 -4.1548462 -4.1946392 -4.2258611 -4.2424259][-4.1942835 -4.1821618 -4.1814814 -4.1821918 -4.1584153 -4.1362815 -4.1207571 -4.0972824 -4.1144528 -4.1662688 -4.2037091 -4.2343569 -4.2560472 -4.2703824 -4.2738171][-4.1862411 -4.1778388 -4.1874075 -4.1987939 -4.1901326 -4.184896 -4.18628 -4.182013 -4.20241 -4.2389307 -4.2621412 -4.2801762 -4.2916646 -4.2964125 -4.293014][-4.1902475 -4.1841669 -4.191926 -4.2026262 -4.2032027 -4.2091355 -4.222476 -4.2301092 -4.2455812 -4.2672467 -4.2831583 -4.2979093 -4.305799 -4.3044181 -4.2944961][-4.2058749 -4.2049065 -4.2062049 -4.2106805 -4.2122369 -4.2169657 -4.2283525 -4.2338471 -4.2381711 -4.2505069 -4.26752 -4.2877512 -4.2978048 -4.2936978 -4.2813077][-4.2313948 -4.2313619 -4.2272162 -4.2259874 -4.2221394 -4.2172503 -4.2192464 -4.2187014 -4.218864 -4.2311049 -4.2502465 -4.2718086 -4.2823925 -4.2771473 -4.2632346][-4.24861 -4.251575 -4.2438836 -4.2357268 -4.2239981 -4.214139 -4.2147207 -4.214118 -4.2215915 -4.2400718 -4.2600594 -4.2764082 -4.2816529 -4.270833 -4.250556]]...]
INFO - root - 2017-12-05 22:48:34.128784: step 47410, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 73h:04m:41s remains)
INFO - root - 2017-12-05 22:48:43.133028: step 47420, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 69h:08m:02s remains)
INFO - root - 2017-12-05 22:48:52.282590: step 47430, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 72h:48m:07s remains)
INFO - root - 2017-12-05 22:49:01.342480: step 47440, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 72h:11m:21s remains)
INFO - root - 2017-12-05 22:49:10.293507: step 47450, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:29m:43s remains)
INFO - root - 2017-12-05 22:49:19.263317: step 47460, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 71h:37m:26s remains)
INFO - root - 2017-12-05 22:49:28.438399: step 47470, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 71h:44m:55s remains)
INFO - root - 2017-12-05 22:49:37.348514: step 47480, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 69h:31m:10s remains)
INFO - root - 2017-12-05 22:49:46.473369: step 47490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 72h:35m:46s remains)
INFO - root - 2017-12-05 22:49:55.362662: step 47500, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 67h:20m:13s remains)
2017-12-05 22:49:56.127842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2106681 -4.1865206 -4.1566133 -4.1417527 -4.1416121 -4.1465511 -4.1536264 -4.166492 -4.1747518 -4.1821895 -4.1864181 -4.187232 -4.1948996 -4.1974831 -4.1952109][-4.1885767 -4.1569967 -4.1255836 -4.1106153 -4.1086817 -4.1134734 -4.1267385 -4.146543 -4.1624532 -4.1763926 -4.1828861 -4.1787276 -4.1839128 -4.1868749 -4.1894727][-4.1574378 -4.1260152 -4.10432 -4.1002841 -4.0989184 -4.0969815 -4.1065993 -4.1285963 -4.1501813 -4.1699691 -4.1769862 -4.1657677 -4.1618814 -4.1630721 -4.1707025][-4.1176572 -4.0896363 -4.0829945 -4.0949378 -4.0992084 -4.0884681 -4.09089 -4.1116028 -4.1389842 -4.16106 -4.1697025 -4.1538105 -4.1408496 -4.1424594 -4.15619][-4.0791841 -4.0573425 -4.0612645 -4.0811214 -4.0841594 -4.061934 -4.0533996 -4.073842 -4.1072969 -4.1355095 -4.152071 -4.1418371 -4.1306171 -4.1374912 -4.1548567][-4.08122 -4.063292 -4.0674562 -4.0772967 -4.0641351 -4.0228615 -3.9971464 -4.0129762 -4.0515246 -4.0958719 -4.1312537 -4.1361551 -4.1331592 -4.1471334 -4.1651216][-4.1253529 -4.1105709 -4.1089592 -4.1042004 -4.0738683 -4.0133967 -3.9689124 -3.9769619 -4.0159035 -4.0676861 -4.1108537 -4.126915 -4.1336126 -4.1503034 -4.1671462][-4.1821208 -4.1698093 -4.1632104 -4.1519108 -4.1201353 -4.0600162 -4.0126657 -4.01277 -4.0406461 -4.0786667 -4.1105647 -4.1269431 -4.1424513 -4.1562161 -4.1676431][-4.21642 -4.2057815 -4.1972466 -4.1860809 -4.1620312 -4.1181731 -4.0869064 -4.0846233 -4.098094 -4.1179042 -4.1338797 -4.1442904 -4.161025 -4.1677027 -4.1722913][-4.2321644 -4.2234464 -4.216753 -4.2100043 -4.1949353 -4.1674008 -4.149529 -4.1451235 -4.14762 -4.153091 -4.159853 -4.1650181 -4.1736441 -4.1710858 -4.1725345][-4.2414036 -4.2317467 -4.2247224 -4.2218881 -4.2159252 -4.2025108 -4.1937633 -4.1883855 -4.1815376 -4.1763835 -4.1769695 -4.178031 -4.1795912 -4.1719384 -4.1678839][-4.2503881 -4.2396431 -4.2297039 -4.2243323 -4.2202759 -4.21604 -4.2142549 -4.2093353 -4.1988163 -4.1893306 -4.1873093 -4.1835427 -4.1785421 -4.1639605 -4.1536717][-4.2668595 -4.2556911 -4.2433329 -4.2326164 -4.2243481 -4.2201838 -4.2182007 -4.2131591 -4.2025528 -4.1929088 -4.1896296 -4.1807566 -4.166379 -4.1422029 -4.1250496][-4.2833986 -4.2741203 -4.2626071 -4.2501092 -4.2375741 -4.2283664 -4.21938 -4.2087517 -4.1956749 -4.1853752 -4.1811557 -4.167623 -4.1459503 -4.1157494 -4.0926986][-4.2986674 -4.291923 -4.2824621 -4.2714634 -4.2578855 -4.2448511 -4.229249 -4.2110233 -4.1897779 -4.1746035 -4.1697965 -4.1566796 -4.1349592 -4.106307 -4.0849385]]...]
INFO - root - 2017-12-05 22:50:05.132030: step 47510, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 71h:15m:32s remains)
INFO - root - 2017-12-05 22:50:14.280385: step 47520, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:21m:12s remains)
INFO - root - 2017-12-05 22:50:23.245080: step 47530, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 71h:07m:03s remains)
INFO - root - 2017-12-05 22:50:32.333603: step 47540, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 72h:13m:07s remains)
INFO - root - 2017-12-05 22:50:41.509606: step 47550, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 68h:39m:39s remains)
INFO - root - 2017-12-05 22:50:50.487702: step 47560, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 71h:14m:38s remains)
INFO - root - 2017-12-05 22:50:59.467864: step 47570, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 72h:25m:59s remains)
INFO - root - 2017-12-05 22:51:08.667570: step 47580, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.928 sec/batch; 73h:25m:22s remains)
INFO - root - 2017-12-05 22:51:17.684455: step 47590, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 70h:38m:13s remains)
INFO - root - 2017-12-05 22:51:26.611164: step 47600, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.882 sec/batch; 69h:49m:36s remains)
2017-12-05 22:51:27.436192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20506 -4.217453 -4.2287536 -4.2396078 -4.253643 -4.2685776 -4.2672405 -4.2635098 -4.2721534 -4.2814064 -4.2830238 -4.2822013 -4.280972 -4.283268 -4.2896323][-4.1970296 -4.2001357 -4.2093968 -4.2285771 -4.2546234 -4.275826 -4.2774282 -4.2745118 -4.2804084 -4.2845054 -4.2769055 -4.2696328 -4.2647614 -4.2679086 -4.2828622][-4.1504579 -4.1547008 -4.170332 -4.2019229 -4.2417684 -4.269618 -4.2712421 -4.2658863 -4.2701645 -4.2694063 -4.25063 -4.23347 -4.2249088 -4.2286739 -4.2531934][-4.0876455 -4.1046629 -4.1387849 -4.1849766 -4.231082 -4.2555208 -4.2526555 -4.2431636 -4.2461038 -4.245347 -4.2227826 -4.19995 -4.18831 -4.1919665 -4.2208314][-4.0673561 -4.1049328 -4.1524587 -4.20044 -4.235486 -4.2418461 -4.2237391 -4.2002811 -4.2033343 -4.2152848 -4.208683 -4.1965241 -4.1867938 -4.1883426 -4.2101927][-4.107626 -4.151639 -4.1883249 -4.2099943 -4.2162085 -4.194963 -4.1495981 -4.1093011 -4.1186576 -4.1580281 -4.1829028 -4.1928058 -4.1934233 -4.1942821 -4.2031693][-4.1481471 -4.1765366 -4.1837554 -4.170208 -4.1453462 -4.0954294 -4.0152221 -3.9554877 -3.9783361 -4.0492549 -4.1071992 -4.14297 -4.1596551 -4.1677938 -4.1726627][-4.1415968 -4.1511765 -4.1306472 -4.0915346 -4.0470467 -3.9775274 -3.8664842 -3.7858226 -3.8250589 -3.9238436 -3.9998245 -4.0516624 -4.0789576 -4.0882492 -4.0901661][-4.1195059 -4.1195064 -4.0863791 -4.0408278 -3.9928393 -3.9264421 -3.8204031 -3.7427278 -3.7817554 -3.8720489 -3.9363992 -3.9838834 -4.0112076 -4.0128508 -4.0029984][-4.1293092 -4.12039 -4.0808477 -4.0381947 -4.0014057 -3.9570475 -3.8916974 -3.8561497 -3.8891191 -3.9412575 -3.9737179 -4.0035009 -4.019166 -4.0039825 -3.9766788][-4.180222 -4.1613126 -4.1170812 -4.0826945 -4.0642362 -4.04394 -4.0182495 -4.0203762 -4.0485806 -4.0706511 -4.0774069 -4.0886374 -4.0916109 -4.0646482 -4.0265155][-4.24583 -4.2243142 -4.1824141 -4.1543541 -4.1461606 -4.1411724 -4.13876 -4.15514 -4.1766472 -4.1860628 -4.1833553 -4.1853318 -4.18581 -4.1624756 -4.1284227][-4.2920408 -4.2742038 -4.2395711 -4.2141485 -4.2078485 -4.2070928 -4.2110519 -4.22636 -4.2428179 -4.2505007 -4.2493997 -4.2521381 -4.2579947 -4.2489529 -4.2267413][-4.3056574 -4.292829 -4.26552 -4.2425547 -4.2351151 -4.2329955 -4.2318807 -4.2402277 -4.2552371 -4.2673597 -4.2726574 -4.2774162 -4.288054 -4.2898006 -4.2778797][-4.2958431 -4.2887607 -4.2693286 -4.2514591 -4.2451591 -4.2414427 -4.2334175 -4.2339864 -4.2471623 -4.2619224 -4.2702 -4.27156 -4.2791286 -4.2863746 -4.2811103]]...]
INFO - root - 2017-12-05 22:51:36.411045: step 47610, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 72h:25m:26s remains)
INFO - root - 2017-12-05 22:51:45.662792: step 47620, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 71h:29m:16s remains)
INFO - root - 2017-12-05 22:51:54.821196: step 47630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 72h:08m:29s remains)
INFO - root - 2017-12-05 22:52:03.793519: step 47640, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:27m:55s remains)
INFO - root - 2017-12-05 22:52:12.813085: step 47650, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 71h:39m:21s remains)
INFO - root - 2017-12-05 22:52:22.071978: step 47660, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 73h:27m:21s remains)
INFO - root - 2017-12-05 22:52:31.028458: step 47670, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.896 sec/batch; 70h:54m:26s remains)
INFO - root - 2017-12-05 22:52:40.060660: step 47680, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.933 sec/batch; 73h:47m:55s remains)
INFO - root - 2017-12-05 22:52:49.152850: step 47690, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.914 sec/batch; 72h:20m:23s remains)
INFO - root - 2017-12-05 22:52:58.259851: step 47700, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 70h:12m:59s remains)
2017-12-05 22:52:59.072028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2359242 -4.2254896 -4.2233281 -4.2250333 -4.2218547 -4.2162285 -4.2103062 -4.2197056 -4.2302885 -4.2347741 -4.241056 -4.2503381 -4.259922 -4.2686357 -4.2701969][-4.2180037 -4.2074814 -4.2021952 -4.1980748 -4.185627 -4.1696157 -4.1572018 -4.169394 -4.1858473 -4.1906157 -4.1951051 -4.2080531 -4.2233539 -4.2404895 -4.2430859][-4.1977253 -4.1869817 -4.1772976 -4.1652808 -4.14416 -4.1202116 -4.1032681 -4.1139221 -4.1340804 -4.1423688 -4.149199 -4.1620708 -4.176856 -4.1991129 -4.2054281][-4.1737957 -4.1571069 -4.13979 -4.1194124 -4.0953875 -4.0646982 -4.0366039 -4.0469861 -4.0719662 -4.0893059 -4.1010938 -4.1101303 -4.1232462 -4.150085 -4.1643796][-4.1598883 -4.1386342 -4.1150203 -4.0878687 -4.0588622 -4.015399 -3.9662344 -3.9802275 -4.0191383 -4.0491242 -4.0624614 -4.0656071 -4.0764809 -4.1073289 -4.1324492][-4.1515503 -4.1292353 -4.1022954 -4.0700068 -4.0325646 -3.9707305 -3.8882504 -3.8981738 -3.9554393 -3.9988976 -4.0172276 -4.0222945 -4.0335932 -4.0660453 -4.0986552][-4.1398196 -4.1184692 -4.0924048 -4.0580621 -4.0145926 -3.9305017 -3.8138058 -3.8167496 -3.8966844 -3.9597635 -3.9845405 -3.9913347 -4.00127 -4.0315404 -4.0650415][-4.1193008 -4.1005406 -4.0828443 -4.0576458 -4.024097 -3.951838 -3.8595195 -3.869946 -3.9225545 -3.9689703 -3.9867671 -3.9876268 -3.9961178 -4.0207672 -4.0538669][-4.1221466 -4.1042819 -4.093863 -4.076416 -4.056901 -4.0154042 -3.963943 -3.9757836 -3.9862118 -4.0028663 -4.0138149 -4.0147285 -4.0248303 -4.0465627 -4.0792289][-4.146224 -4.1246142 -4.11377 -4.0960317 -4.0800619 -4.0533814 -4.0276046 -4.0486884 -4.0478578 -4.0513477 -4.055769 -4.0575242 -4.0678535 -4.0854259 -4.1134615][-4.1822138 -4.156754 -4.1362171 -4.1140003 -4.0993342 -4.0793581 -4.0703154 -4.1011019 -4.1048818 -4.1059213 -4.1067424 -4.109942 -4.1191368 -4.129086 -4.1494665][-4.2145438 -4.187748 -4.1628675 -4.139657 -4.1282363 -4.115931 -4.113893 -4.1414833 -4.1518583 -4.1561122 -4.1585007 -4.1641879 -4.1691504 -4.1717134 -4.1841054][-4.23035 -4.2061896 -4.1850219 -4.1683621 -4.1634393 -4.1588473 -4.1604023 -4.1814766 -4.1913586 -4.1955342 -4.1979427 -4.2047219 -4.2096534 -4.2121992 -4.2224946][-4.2441387 -4.2248988 -4.2106047 -4.2013297 -4.2027946 -4.2037239 -4.2092767 -4.2256222 -4.2311659 -4.2327247 -4.2362051 -4.2454047 -4.2525725 -4.2576456 -4.2665548][-4.2594051 -4.2450061 -4.2352529 -4.2318325 -4.2381387 -4.2426915 -4.2490382 -4.2588792 -4.2619638 -4.2637181 -4.2666926 -4.2744961 -4.2803893 -4.2845435 -4.2895131]]...]
INFO - root - 2017-12-05 22:53:08.017973: step 47710, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.926 sec/batch; 73h:16m:03s remains)
INFO - root - 2017-12-05 22:53:17.117558: step 47720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 72h:33m:49s remains)
INFO - root - 2017-12-05 22:53:26.280160: step 47730, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 73h:10m:17s remains)
INFO - root - 2017-12-05 22:53:35.284407: step 47740, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.801 sec/batch; 63h:21m:14s remains)
INFO - root - 2017-12-05 22:53:44.393984: step 47750, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 68h:19m:37s remains)
INFO - root - 2017-12-05 22:53:53.406427: step 47760, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 70h:49m:06s remains)
INFO - root - 2017-12-05 22:54:02.573560: step 47770, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 69h:55m:58s remains)
INFO - root - 2017-12-05 22:54:11.872272: step 47780, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 64h:15m:46s remains)
INFO - root - 2017-12-05 22:54:21.028919: step 47790, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 75h:27m:04s remains)
INFO - root - 2017-12-05 22:54:30.194100: step 47800, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 72h:19m:23s remains)
2017-12-05 22:54:30.951915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3008938 -4.3030782 -4.2994967 -4.2903533 -4.2826357 -4.2803397 -4.28129 -4.2894177 -4.3016481 -4.3084393 -4.3074389 -4.30415 -4.2991009 -4.2947369 -4.2957597][-4.285614 -4.2883077 -4.2829857 -4.2711105 -4.2581029 -4.2489228 -4.2442975 -4.2537632 -4.2745996 -4.288507 -4.2870088 -4.2811737 -4.27257 -4.2653093 -4.2668805][-4.2775435 -4.2809181 -4.2734714 -4.2556791 -4.2322874 -4.2110066 -4.1985688 -4.2092218 -4.24275 -4.2655993 -4.2637014 -4.25282 -4.2387581 -4.2274623 -4.2330275][-4.2672992 -4.2665396 -4.2516537 -4.2252 -4.1909432 -4.160552 -4.1398444 -4.1506729 -4.1981907 -4.2301121 -4.2286706 -4.2110724 -4.1912389 -4.1777124 -4.1886826][-4.2559476 -4.2459669 -4.21801 -4.1811466 -4.1376228 -4.0983515 -4.0624828 -4.0691767 -4.1312766 -4.1751819 -4.1761179 -4.1544104 -4.1300759 -4.1160345 -4.1338744][-4.2460165 -4.2225208 -4.1782146 -4.1292744 -4.0758815 -4.0164137 -3.9474959 -3.9427404 -4.0242281 -4.08712 -4.095736 -4.0756984 -4.0500441 -4.0398564 -4.0693583][-4.2391658 -4.2033978 -4.1444645 -4.0815749 -4.0118613 -3.9204454 -3.804647 -3.790302 -3.9062178 -4.0032091 -4.0316224 -4.0214329 -3.9996266 -3.9977963 -4.0367866][-4.2367258 -4.1942329 -4.1290684 -4.0593433 -3.9847353 -3.8811643 -3.7501402 -3.7390862 -3.8738403 -3.9921334 -4.0384817 -4.03715 -4.0195651 -4.0216928 -4.057703][-4.2405686 -4.198741 -4.1371994 -4.0759716 -4.0151787 -3.9374776 -3.8457708 -3.8431401 -3.9519744 -4.0552845 -4.0985847 -4.0962853 -4.0772662 -4.0732303 -4.0938058][-4.2495232 -4.2133904 -4.1630383 -4.1166043 -4.0715995 -4.0178132 -3.9614429 -3.9584868 -4.0301585 -4.1076055 -4.1359148 -4.1258621 -4.1014247 -4.091177 -4.1007614][-4.2592821 -4.229528 -4.1892366 -4.1494474 -4.1089983 -4.0649056 -4.0258546 -4.0205464 -4.0696397 -4.1264467 -4.1416759 -4.124094 -4.0959558 -4.0859418 -4.0935926][-4.2646575 -4.2395654 -4.2057152 -4.1642003 -4.1201692 -4.0792713 -4.0505209 -4.0485168 -4.0869288 -4.13189 -4.1412535 -4.1236925 -4.0983324 -4.0928965 -4.1021423][-4.2676144 -4.246911 -4.2184157 -4.1768742 -4.130518 -4.0932307 -4.0736408 -4.0781403 -4.1127052 -4.1455193 -4.1474977 -4.1298537 -4.1127272 -4.11471 -4.1278768][-4.271143 -4.2553587 -4.2341142 -4.197937 -4.1556005 -4.1276093 -4.1206026 -4.1300368 -4.1552997 -4.1724496 -4.1668029 -4.1499872 -4.1405487 -4.1483107 -4.1623926][-4.2727408 -4.2595706 -4.244307 -4.2176566 -4.1862016 -4.1711879 -4.1768804 -4.1901555 -4.2043104 -4.205121 -4.1919117 -4.1742387 -4.1656718 -4.170907 -4.1803823]]...]
INFO - root - 2017-12-05 22:54:40.067235: step 47810, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 74h:06m:46s remains)
INFO - root - 2017-12-05 22:54:49.205464: step 47820, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 70h:05m:46s remains)
INFO - root - 2017-12-05 22:54:58.378075: step 47830, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 73h:57m:37s remains)
INFO - root - 2017-12-05 22:55:07.600327: step 47840, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.952 sec/batch; 75h:18m:23s remains)
INFO - root - 2017-12-05 22:55:16.662944: step 47850, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 71h:19m:19s remains)
INFO - root - 2017-12-05 22:55:25.617980: step 47860, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 70h:12m:02s remains)
INFO - root - 2017-12-05 22:55:34.808087: step 47870, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.948 sec/batch; 74h:55m:11s remains)
INFO - root - 2017-12-05 22:55:43.949586: step 47880, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.975 sec/batch; 77h:03m:51s remains)
INFO - root - 2017-12-05 22:55:53.000183: step 47890, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 72h:38m:00s remains)
INFO - root - 2017-12-05 22:56:02.181817: step 47900, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.955 sec/batch; 75h:28m:24s remains)
2017-12-05 22:56:02.961974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2357259 -4.2457108 -4.2546668 -4.2611322 -4.2651134 -4.2663031 -4.2601223 -4.2556896 -4.2594571 -4.2675052 -4.2714071 -4.2688904 -4.2646337 -4.2551055 -4.2422][-4.2032976 -4.2208209 -4.2342992 -4.2405252 -4.2457423 -4.25538 -4.2633381 -4.27102 -4.2801147 -4.2895803 -4.2933292 -4.2879338 -4.276063 -4.2570214 -4.2347021][-4.1789141 -4.2025137 -4.219892 -4.2247057 -4.2257934 -4.237967 -4.2570066 -4.2750111 -4.2885451 -4.2974658 -4.2993088 -4.28829 -4.268117 -4.2420163 -4.2164626][-4.1749988 -4.1967311 -4.2089939 -4.2043877 -4.1955252 -4.2020631 -4.2287016 -4.2575583 -4.2785039 -4.290906 -4.2930393 -4.2774906 -4.2513657 -4.222939 -4.2000413][-4.185307 -4.1936431 -4.1901908 -4.1703634 -4.1458526 -4.1407628 -4.168241 -4.20726 -4.2386 -4.2620177 -4.2724247 -4.2612257 -4.2361431 -4.2109232 -4.1967745][-4.204329 -4.191608 -4.1654387 -4.1255841 -4.0815454 -4.0582156 -4.0784326 -4.1294613 -4.1746211 -4.2125812 -4.239439 -4.2414289 -4.2251759 -4.2065964 -4.1998878][-4.223949 -4.1849375 -4.1303687 -4.0659246 -4.0006275 -3.956115 -3.9645822 -4.0259337 -4.0901384 -4.1471024 -4.1971431 -4.2213655 -4.2219973 -4.2132854 -4.2094932][-4.2332215 -4.170692 -4.0890894 -3.9990673 -3.9110463 -3.8400526 -3.8296192 -3.8998523 -3.9858062 -4.0652366 -4.1415377 -4.1929417 -4.2207527 -4.2307434 -4.2339587][-4.2327361 -4.1623545 -4.0702028 -3.97515 -3.8887875 -3.8178439 -3.80196 -3.8659339 -3.9500661 -4.0323811 -4.117939 -4.1847725 -4.2330284 -4.2604394 -4.2722731][-4.2304559 -4.1701193 -4.0893364 -4.0101213 -3.948082 -3.9048233 -3.9006329 -3.9527781 -4.0176339 -4.0812483 -4.1528807 -4.2141585 -4.2648354 -4.2955303 -4.3086653][-4.2412944 -4.1991577 -4.1372862 -4.0783229 -4.0421481 -4.0287137 -4.0382524 -4.08104 -4.1266437 -4.16871 -4.2179823 -4.2618904 -4.3000259 -4.321969 -4.3305006][-4.2707467 -4.2486753 -4.2095113 -4.1721373 -4.1553583 -4.1583891 -4.1719074 -4.2028093 -4.2301197 -4.253684 -4.2822008 -4.3066626 -4.3263736 -4.335433 -4.3357711][-4.3038597 -4.2986779 -4.2806983 -4.2600846 -4.2525439 -4.2589955 -4.2679615 -4.2862005 -4.3009429 -4.3129053 -4.3271413 -4.3364859 -4.3408356 -4.3398476 -4.3347635][-4.3267965 -4.3313079 -4.3274031 -4.3180389 -4.3127427 -4.3142009 -4.3165812 -4.3258696 -4.3337784 -4.3405714 -4.347218 -4.3485818 -4.3470526 -4.3422146 -4.335146][-4.336247 -4.3447061 -4.3479385 -4.3455052 -4.3405728 -4.336535 -4.332685 -4.3347955 -4.3389726 -4.3442178 -4.3489175 -4.3495522 -4.3470693 -4.3421988 -4.3360524]]...]
INFO - root - 2017-12-05 22:56:11.977652: step 47910, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 73h:01m:30s remains)
INFO - root - 2017-12-05 22:56:21.102860: step 47920, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 70h:26m:13s remains)
INFO - root - 2017-12-05 22:56:30.104085: step 47930, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 74h:40m:39s remains)
INFO - root - 2017-12-05 22:56:39.233223: step 47940, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 71h:36m:46s remains)
INFO - root - 2017-12-05 22:56:48.294806: step 47950, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 69h:48m:54s remains)
INFO - root - 2017-12-05 22:56:57.421354: step 47960, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 73h:23m:34s remains)
INFO - root - 2017-12-05 22:57:06.588763: step 47970, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:12m:01s remains)
INFO - root - 2017-12-05 22:57:15.795013: step 47980, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 72h:11m:47s remains)
INFO - root - 2017-12-05 22:57:24.763750: step 47990, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 71h:40m:34s remains)
INFO - root - 2017-12-05 22:57:33.891946: step 48000, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 72h:39m:06s remains)
2017-12-05 22:57:34.728352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2001963 -4.1955404 -4.2027316 -4.1999564 -4.1802459 -4.1736312 -4.1792917 -4.1898589 -4.1919136 -4.18768 -4.1884041 -4.19233 -4.1989655 -4.1967015 -4.1880927][-4.1857324 -4.1777377 -4.1862879 -4.1871796 -4.1672463 -4.1574178 -4.1639433 -4.170856 -4.1742682 -4.17487 -4.1768484 -4.1780796 -4.1771474 -4.174027 -4.1716981][-4.16772 -4.1542997 -4.1613269 -4.168623 -4.159533 -4.15119 -4.1542387 -4.1519785 -4.154304 -4.162498 -4.1704693 -4.1722546 -4.1693139 -4.1702895 -4.1750255][-4.1660376 -4.1476531 -4.1476154 -4.1528811 -4.151998 -4.1386433 -4.1249928 -4.1062136 -4.1056228 -4.1348853 -4.1623716 -4.1734915 -4.1748881 -4.1822767 -4.1949987][-4.1824431 -4.1556683 -4.1410341 -4.1333404 -4.1332669 -4.1111727 -4.0713778 -4.0216775 -4.0167089 -4.0800242 -4.1362982 -4.1602097 -4.1646495 -4.1736989 -4.1881781][-4.1943016 -4.1573248 -4.1262331 -4.1048265 -4.0995369 -4.0702243 -4.0022459 -3.9104104 -3.8995481 -4.008 -4.1014214 -4.1376481 -4.1423793 -4.1519666 -4.1591573][-4.2043796 -4.16663 -4.1335473 -4.1059012 -4.0907421 -4.0500231 -3.9554734 -3.8333881 -3.8127713 -3.9497025 -4.0715117 -4.1163921 -4.1169519 -4.1266708 -4.1316166][-4.2313967 -4.205584 -4.1806602 -4.1543603 -4.1383066 -4.10053 -4.0102234 -3.9100313 -3.8864896 -3.9936044 -4.0979285 -4.13331 -4.1257772 -4.1327486 -4.1361923][-4.2515669 -4.2312083 -4.2156844 -4.2039108 -4.196034 -4.1684818 -4.1068544 -4.0492516 -4.037241 -4.0994272 -4.164649 -4.1819096 -4.172462 -4.1773858 -4.1757927][-4.2495689 -4.2322226 -4.2222166 -4.2208576 -4.2166262 -4.1986761 -4.1617985 -4.1304107 -4.1275826 -4.1628337 -4.2020092 -4.2095194 -4.2081518 -4.215169 -4.208436][-4.2387886 -4.2238555 -4.2137618 -4.2112169 -4.2034364 -4.190341 -4.1719923 -4.1595526 -4.1655641 -4.189559 -4.2137041 -4.2165842 -4.2206354 -4.2288942 -4.2246156][-4.2303119 -4.2142143 -4.1997275 -4.1932511 -4.1836858 -4.1779633 -4.1701264 -4.1713409 -4.1830473 -4.2033453 -4.21896 -4.2237267 -4.2308702 -4.240716 -4.239994][-4.2377963 -4.2210088 -4.2027254 -4.1946774 -4.1859694 -4.1834335 -4.1804991 -4.1886873 -4.2042 -4.2214231 -4.2334733 -4.2371817 -4.2450671 -4.2534022 -4.2567248][-4.2579665 -4.2420855 -4.2244534 -4.2178555 -4.2107253 -4.2057266 -4.2016115 -4.207592 -4.2202439 -4.2330518 -4.2444382 -4.2489591 -4.2561483 -4.2636642 -4.267859][-4.2874365 -4.2762351 -4.2629356 -4.255991 -4.2496624 -4.2436175 -4.2378454 -4.2399931 -4.2492967 -4.2596216 -4.2697439 -4.2759027 -4.280385 -4.2846 -4.2875071]]...]
INFO - root - 2017-12-05 22:57:44.016288: step 48010, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 71h:40m:34s remains)
INFO - root - 2017-12-05 22:57:53.103568: step 48020, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 71h:37m:38s remains)
INFO - root - 2017-12-05 22:58:02.120638: step 48030, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.912 sec/batch; 72h:06m:16s remains)
INFO - root - 2017-12-05 22:58:11.203602: step 48040, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 68h:31m:38s remains)
INFO - root - 2017-12-05 22:58:20.396523: step 48050, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 73h:31m:38s remains)
INFO - root - 2017-12-05 22:58:29.568397: step 48060, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 68h:20m:08s remains)
INFO - root - 2017-12-05 22:58:38.695028: step 48070, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 72h:02m:19s remains)
INFO - root - 2017-12-05 22:58:47.668186: step 48080, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 70h:26m:27s remains)
INFO - root - 2017-12-05 22:58:56.981933: step 48090, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 73h:31m:19s remains)
INFO - root - 2017-12-05 22:59:06.182550: step 48100, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 73h:32m:36s remains)
2017-12-05 22:59:06.974099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2173228 -4.2095003 -4.2098656 -4.2194023 -4.227787 -4.2277007 -4.2273555 -4.2246575 -4.2150135 -4.2119212 -4.2215171 -4.2333474 -4.2395449 -4.2374206 -4.2344594][-4.1982603 -4.1866603 -4.1715517 -4.1702495 -4.1737747 -4.1773634 -4.1894608 -4.2019572 -4.2001467 -4.1976771 -4.2062883 -4.2184858 -4.2267318 -4.2262917 -4.2251267][-4.1709747 -4.1624212 -4.1346722 -4.1190238 -4.1171679 -4.1282482 -4.157445 -4.1855793 -4.1946616 -4.1975651 -4.2055039 -4.2147093 -4.2219257 -4.2238207 -4.2246432][-4.1520853 -4.1566863 -4.1256361 -4.09525 -4.08485 -4.0938969 -4.1267338 -4.1592951 -4.1750164 -4.1860628 -4.2007551 -4.2115808 -4.2187157 -4.2229614 -4.224699][-4.1422062 -4.1639228 -4.1361837 -4.0943871 -4.0734205 -4.0662079 -4.08335 -4.1069202 -4.1225371 -4.1409678 -4.1653714 -4.1882439 -4.2056751 -4.2208381 -4.2291126][-4.1322489 -4.1648264 -4.1446109 -4.0953116 -4.0576816 -4.03075 -4.0304103 -4.0467334 -4.0690594 -4.0974197 -4.1341062 -4.171123 -4.2012577 -4.2309256 -4.2523327][-4.1115556 -4.1356516 -4.1175575 -4.06427 -4.0170617 -3.9862752 -3.9874253 -4.0133982 -4.0446973 -4.0784593 -4.1248851 -4.1698403 -4.208849 -4.2498803 -4.2770014][-4.1015549 -4.1125655 -4.0918574 -4.0445104 -4.00517 -3.9889612 -4.0047579 -4.0378256 -4.0672956 -4.0955429 -4.1399736 -4.1807594 -4.2155104 -4.2502689 -4.2716169][-4.1305618 -4.1239791 -4.0937572 -4.0511956 -4.0227985 -4.0167551 -4.0377073 -4.0720582 -4.100462 -4.122303 -4.15635 -4.1871619 -4.2148337 -4.2375264 -4.2496819][-4.1669683 -4.1484003 -4.115912 -4.080533 -4.0542049 -4.0482841 -4.0685048 -4.1012278 -4.12703 -4.1420765 -4.1674275 -4.1894441 -4.2109938 -4.2225423 -4.2301345][-4.1909628 -4.166235 -4.1363921 -4.1123252 -4.092061 -4.0816855 -4.099257 -4.12621 -4.1492257 -4.1588454 -4.1783285 -4.1949644 -4.2075667 -4.2101665 -4.2160563][-4.1863136 -4.1590171 -4.1354737 -4.1251078 -4.1146817 -4.1071529 -4.1265106 -4.1510038 -4.1705775 -4.1701083 -4.1776991 -4.1860867 -4.1899457 -4.194344 -4.2066517][-4.1790514 -4.1495028 -4.1266956 -4.1250048 -4.1246834 -4.124485 -4.145772 -4.1668124 -4.1823153 -4.1704092 -4.1641269 -4.1646223 -4.1631279 -4.1697154 -4.1855531][-4.1825237 -4.1462936 -4.1177092 -4.11312 -4.1174922 -4.1226468 -4.1455827 -4.1665306 -4.1802068 -4.1645465 -4.1521864 -4.1498384 -4.1461296 -4.151926 -4.1677403][-4.1907487 -4.1540275 -4.1213417 -4.1102066 -4.1110134 -4.1183233 -4.1450977 -4.1685758 -4.1839051 -4.1738935 -4.1608524 -4.1480384 -4.136229 -4.1386867 -4.1506376]]...]
INFO - root - 2017-12-05 22:59:16.023259: step 48110, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 71h:59m:16s remains)
INFO - root - 2017-12-05 22:59:24.828986: step 48120, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.909 sec/batch; 71h:46m:30s remains)
INFO - root - 2017-12-05 22:59:33.912250: step 48130, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 69h:39m:53s remains)
INFO - root - 2017-12-05 22:59:42.977386: step 48140, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 71h:48m:04s remains)
INFO - root - 2017-12-05 22:59:52.267152: step 48150, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 71h:02m:20s remains)
INFO - root - 2017-12-05 23:00:01.240656: step 48160, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 71h:18m:57s remains)
INFO - root - 2017-12-05 23:00:10.393668: step 48170, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 73h:37m:25s remains)
INFO - root - 2017-12-05 23:00:19.501306: step 48180, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 72h:01m:34s remains)
INFO - root - 2017-12-05 23:00:28.507481: step 48190, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 71h:35m:39s remains)
INFO - root - 2017-12-05 23:00:37.615421: step 48200, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 72h:47m:19s remains)
2017-12-05 23:00:38.541831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2447224 -4.2449126 -4.2563233 -4.2706785 -4.2836185 -4.2878242 -4.2865138 -4.2839303 -4.2820435 -4.2775178 -4.2683187 -4.2582846 -4.2561 -4.260005 -4.2700443][-4.1986318 -4.1958466 -4.2092261 -4.2282896 -4.2473702 -4.2530785 -4.2502303 -4.2470932 -4.2460203 -4.23889 -4.2237864 -4.2120352 -4.2171812 -4.2293773 -4.2460594][-4.15861 -4.15167 -4.1641726 -4.1851759 -4.2082043 -4.2133808 -4.2037153 -4.1978288 -4.1994081 -4.1938167 -4.1732569 -4.1651425 -4.1844621 -4.2070303 -4.2303424][-4.1166949 -4.1074858 -4.1165142 -4.1359959 -4.162993 -4.1661825 -4.1461673 -4.1381283 -4.1475348 -4.1443634 -4.1180573 -4.1126261 -4.1456165 -4.1807528 -4.20933][-4.0840373 -4.0781584 -4.087883 -4.1076741 -4.132062 -4.1233783 -4.0784521 -4.064887 -4.08671 -4.0952487 -4.0747223 -4.0761981 -4.1154451 -4.1571407 -4.1876688][-4.0907664 -4.0897841 -4.0972447 -4.10552 -4.1089272 -4.0688105 -3.9844935 -3.9709265 -4.0228314 -4.0573707 -4.056572 -4.0751157 -4.1166382 -4.1585193 -4.1841927][-4.1120491 -4.1051693 -4.094233 -4.0782318 -4.0487909 -3.961246 -3.829457 -3.8301449 -3.9295652 -4.0015459 -4.025929 -4.0629869 -4.1110549 -4.1545258 -4.1771321][-4.1292977 -4.1157222 -4.0902562 -4.0607734 -4.0131469 -3.9033022 -3.7510092 -3.7556438 -3.8766985 -3.9682941 -4.0066943 -4.0500975 -4.0998635 -4.142849 -4.1671948][-4.1422405 -4.1220083 -4.0952253 -4.0724425 -4.0455356 -3.9788854 -3.8776984 -3.8664312 -3.9494853 -4.0179219 -4.0388255 -4.0639076 -4.1027813 -4.145596 -4.1756344][-4.1589308 -4.1368575 -4.1149836 -4.1018014 -4.0930548 -4.0688992 -4.0208445 -4.0102324 -4.0620623 -4.1077662 -4.1128483 -4.1175714 -4.137547 -4.1691456 -4.20019][-4.1950197 -4.1724162 -4.15317 -4.1459875 -4.1466122 -4.1464405 -4.130384 -4.1273394 -4.1594071 -4.1866975 -4.1845603 -4.17836 -4.1864934 -4.2083611 -4.2364864][-4.2455411 -4.2255669 -4.2096848 -4.2049122 -4.2080331 -4.21558 -4.2153826 -4.2211313 -4.2402768 -4.2535543 -4.246841 -4.2341642 -4.2376795 -4.2527976 -4.2745085][-4.2836032 -4.2679253 -4.2563338 -4.2528524 -4.2550287 -4.2615151 -4.2678432 -4.2767444 -4.2867918 -4.2903185 -4.2837443 -4.2705779 -4.2726717 -4.2850661 -4.303575][-4.2965193 -4.2828259 -4.2757535 -4.2737551 -4.2768292 -4.2839489 -4.2919097 -4.2996068 -4.3041906 -4.3046622 -4.3000674 -4.28794 -4.2877154 -4.2995992 -4.3157253][-4.3020864 -4.292562 -4.2912483 -4.2913966 -4.294282 -4.2993526 -4.3027835 -4.3052793 -4.3070183 -4.3088331 -4.3061514 -4.2974987 -4.2972736 -4.30799 -4.3213325]]...]
INFO - root - 2017-12-05 23:00:47.646976: step 48210, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 73h:19m:46s remains)
INFO - root - 2017-12-05 23:00:56.615556: step 48220, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 69h:33m:06s remains)
INFO - root - 2017-12-05 23:01:05.862522: step 48230, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 73h:04m:24s remains)
INFO - root - 2017-12-05 23:01:15.033635: step 48240, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 70h:23m:06s remains)
INFO - root - 2017-12-05 23:01:24.014048: step 48250, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 68h:36m:44s remains)
INFO - root - 2017-12-05 23:01:33.026857: step 48260, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 70h:25m:27s remains)
INFO - root - 2017-12-05 23:01:42.153627: step 48270, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 73h:49m:59s remains)
INFO - root - 2017-12-05 23:01:51.285856: step 48280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 72h:31m:40s remains)
INFO - root - 2017-12-05 23:02:00.362769: step 48290, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 70h:32m:31s remains)
INFO - root - 2017-12-05 23:02:09.514814: step 48300, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 74h:14m:12s remains)
2017-12-05 23:02:10.296769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2968712 -4.3020349 -4.3050733 -4.3002205 -4.2864232 -4.2752366 -4.2741194 -4.2859154 -4.303875 -4.3136568 -4.3134637 -4.311327 -4.3144245 -4.3165832 -4.3157659][-4.3024454 -4.3082628 -4.3097696 -4.3075676 -4.3016057 -4.2966018 -4.2954426 -4.3017359 -4.313252 -4.3173203 -4.3118672 -4.3045139 -4.3050818 -4.3067431 -4.3070583][-4.2888126 -4.2963905 -4.2967768 -4.296515 -4.294302 -4.2893572 -4.2850246 -4.2854996 -4.2910867 -4.2936039 -4.2874389 -4.2808843 -4.2844391 -4.291841 -4.2979288][-4.2681036 -4.2815409 -4.286346 -4.2896628 -4.2887497 -4.2786927 -4.2648807 -4.2537804 -4.2508926 -4.2559128 -4.2563066 -4.2557211 -4.2648897 -4.2807689 -4.2925539][-4.24344 -4.2662568 -4.2803154 -4.2883253 -4.2856793 -4.2628264 -4.2282419 -4.1981573 -4.1892695 -4.20315 -4.2155881 -4.2253804 -4.243351 -4.2657447 -4.2799568][-4.2121634 -4.2498693 -4.2727189 -4.2822151 -4.2722626 -4.2304354 -4.1619263 -4.1041784 -4.09257 -4.1252079 -4.1646156 -4.1975107 -4.2290292 -4.2560244 -4.2714958][-4.1734037 -4.2279482 -4.2557049 -4.2600269 -4.2395639 -4.1759243 -4.066318 -3.9693956 -3.9600365 -4.028688 -4.10749 -4.1695981 -4.216217 -4.2485323 -4.2654648][-4.1459508 -4.2055931 -4.2280874 -4.2248063 -4.1969795 -4.1177049 -3.9739966 -3.8422499 -3.8460963 -3.9624319 -4.0761595 -4.1533494 -4.2044926 -4.2359595 -4.2533684][-4.1526756 -4.1984887 -4.2080946 -4.1998725 -4.1765876 -4.1030874 -3.96661 -3.846024 -3.8670387 -3.9906325 -4.0973034 -4.1624084 -4.2030487 -4.2289805 -4.2471638][-4.1824303 -4.2046423 -4.1968412 -4.18711 -4.1813688 -4.1445756 -4.0661345 -4.0005555 -4.0218749 -4.1011243 -4.1656156 -4.2036052 -4.2277555 -4.2451825 -4.2612243][-4.2105336 -4.21552 -4.1999674 -4.1962485 -4.2088065 -4.2084842 -4.1836591 -4.158649 -4.1704359 -4.204875 -4.2309914 -4.2468009 -4.2606721 -4.2733722 -4.2870235][-4.2224889 -4.222394 -4.2119164 -4.2144189 -4.2354856 -4.25196 -4.2513652 -4.24479 -4.2499552 -4.2595534 -4.2638683 -4.268837 -4.2784853 -4.2923803 -4.3057966][-4.2161565 -4.2159352 -4.2097759 -4.2138972 -4.2320862 -4.2507782 -4.2591896 -4.2635322 -4.2698226 -4.269712 -4.2633314 -4.2639179 -4.2737489 -4.2877054 -4.2992215][-4.2086253 -4.2063389 -4.2012019 -4.202373 -4.2117381 -4.2250023 -4.2381129 -4.2496037 -4.2564921 -4.2500391 -4.234561 -4.2318993 -4.2445464 -4.2600536 -4.2711787][-4.2262363 -4.2206411 -4.2132778 -4.2074976 -4.2038908 -4.2074766 -4.2190981 -4.2317686 -4.2377067 -4.2265129 -4.2069931 -4.2037845 -4.2172804 -4.2317452 -4.2406774]]...]
INFO - root - 2017-12-05 23:02:19.402959: step 48310, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.921 sec/batch; 72h:43m:51s remains)
INFO - root - 2017-12-05 23:02:28.502479: step 48320, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 71h:14m:21s remains)
INFO - root - 2017-12-05 23:02:37.567005: step 48330, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 71h:13m:43s remains)
INFO - root - 2017-12-05 23:02:46.791212: step 48340, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 75h:14m:47s remains)
INFO - root - 2017-12-05 23:02:55.671947: step 48350, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 72h:06m:47s remains)
INFO - root - 2017-12-05 23:03:04.869840: step 48360, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 71h:22m:51s remains)
INFO - root - 2017-12-05 23:03:13.950419: step 48370, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 70h:56m:50s remains)
INFO - root - 2017-12-05 23:03:22.761235: step 48380, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 69h:25m:45s remains)
INFO - root - 2017-12-05 23:03:32.022931: step 48390, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 75h:22m:35s remains)
INFO - root - 2017-12-05 23:03:41.138475: step 48400, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 72h:25m:40s remains)
2017-12-05 23:03:41.944017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3541522 -4.3406277 -4.3092709 -4.274528 -4.2212834 -4.1425743 -4.0581594 -3.9949989 -4.0057311 -4.0855975 -4.1648927 -4.1958623 -4.1861835 -4.183619 -4.1827655][-4.3540678 -4.3411622 -4.3099995 -4.2742834 -4.2203135 -4.1406035 -4.0483308 -3.9765587 -3.9903009 -4.0783305 -4.1639066 -4.20272 -4.1987758 -4.201004 -4.2006836][-4.3544374 -4.342401 -4.3116837 -4.2759519 -4.2219748 -4.1421967 -4.0434742 -3.9650846 -3.9815342 -4.0742 -4.1589522 -4.2012091 -4.2034707 -4.21122 -4.2127662][-4.3547254 -4.3432441 -4.3124127 -4.277544 -4.2255507 -4.1480088 -4.0464377 -3.9633572 -3.9795175 -4.072093 -4.1527653 -4.1961508 -4.2037206 -4.2153382 -4.2186413][-4.3548541 -4.34382 -4.312921 -4.279182 -4.229022 -4.1537652 -4.0520625 -3.964021 -3.9778018 -4.06861 -4.1454926 -4.1884995 -4.1995263 -4.2107959 -4.2145815][-4.3548961 -4.3441944 -4.3136187 -4.281312 -4.2318482 -4.15648 -4.0555129 -3.9618034 -3.9707997 -4.0630322 -4.1383119 -4.1815376 -4.1947904 -4.2070289 -4.2115822][-4.3549962 -4.34413 -4.3141975 -4.2824559 -4.2323475 -4.1551976 -4.0540209 -3.9545815 -3.9583955 -4.05404 -4.1302843 -4.1759171 -4.1929364 -4.2100725 -4.2175827][-4.3546672 -4.3434963 -4.3139782 -4.2818356 -4.2306948 -4.1522069 -4.0509939 -3.9503057 -3.9527402 -4.0491543 -4.1257143 -4.1740427 -4.1967735 -4.2190151 -4.2284775][-4.3539071 -4.3425303 -4.3134961 -4.2805505 -4.2284856 -4.150044 -4.052527 -3.9594753 -3.9665365 -4.0616212 -4.1364818 -4.1850863 -4.210022 -4.2347217 -4.2437124][-4.3531866 -4.341639 -4.3127322 -4.278779 -4.2271571 -4.1510606 -4.060173 -3.9796038 -3.9932723 -4.0861173 -4.1600404 -4.2066545 -4.230845 -4.2533135 -4.2599754][-4.352231 -4.34018 -4.3113332 -4.2768035 -4.226975 -4.1562719 -4.0726919 -4.0031185 -4.0216475 -4.1122375 -4.1841969 -4.2289667 -4.252213 -4.2719593 -4.2762008][-4.3513122 -4.3391027 -4.3111649 -4.2770958 -4.2303162 -4.16652 -4.0916133 -4.0322952 -4.0537252 -4.1393776 -4.2086329 -4.2506208 -4.2742133 -4.2929888 -4.2945418][-4.3503008 -4.3379903 -4.3100986 -4.2758961 -4.232451 -4.1742225 -4.1079464 -4.0562205 -4.079268 -4.1594968 -4.2253022 -4.2632952 -4.2862577 -4.3047109 -4.3048644][-4.3491755 -4.336452 -4.3080463 -4.2747126 -4.2342386 -4.1801591 -4.1226869 -4.0790319 -4.09971 -4.1713753 -4.2322383 -4.2660751 -4.2875414 -4.3058906 -4.3058424][-4.3480434 -4.3343496 -4.3063593 -4.2753739 -4.2380204 -4.1878481 -4.1394229 -4.10537 -4.1215243 -4.1807294 -4.2344732 -4.265696 -4.2861032 -4.3045993 -4.3048453]]...]
INFO - root - 2017-12-05 23:03:51.231461: step 48410, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.967 sec/batch; 76h:16m:28s remains)
INFO - root - 2017-12-05 23:04:00.267356: step 48420, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 70h:07m:08s remains)
INFO - root - 2017-12-05 23:04:09.396506: step 48430, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 74h:17m:35s remains)
INFO - root - 2017-12-05 23:04:18.637828: step 48440, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 72h:49m:45s remains)
INFO - root - 2017-12-05 23:04:27.647185: step 48450, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 74h:50m:57s remains)
INFO - root - 2017-12-05 23:04:36.972219: step 48460, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.941 sec/batch; 74h:16m:15s remains)
INFO - root - 2017-12-05 23:04:46.167531: step 48470, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 70h:08m:21s remains)
INFO - root - 2017-12-05 23:04:55.217951: step 48480, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 75h:27m:45s remains)
INFO - root - 2017-12-05 23:05:04.454260: step 48490, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 67h:07m:32s remains)
INFO - root - 2017-12-05 23:05:13.613989: step 48500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 70h:03m:13s remains)
2017-12-05 23:05:14.365431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0513878 -4.083415 -4.0950961 -4.0962129 -4.0875921 -4.0822973 -4.0860996 -4.0905595 -4.1027751 -4.1213169 -4.1461787 -4.1666675 -4.1722751 -4.1587167 -4.1275082][-4.07588 -4.0976562 -4.1002016 -4.0933018 -4.08846 -4.0926747 -4.1006227 -4.101222 -4.1074891 -4.1236496 -4.14209 -4.160552 -4.1636682 -4.1435657 -4.10807][-4.1189556 -4.1328378 -4.1290054 -4.1169105 -4.1138496 -4.1158934 -4.1181636 -4.11536 -4.11807 -4.133307 -4.1479692 -4.1645751 -4.1694388 -4.1473217 -4.1070285][-4.1583428 -4.1679206 -4.158885 -4.1390672 -4.1234093 -4.1089511 -4.1027026 -4.0965428 -4.0974121 -4.1171403 -4.1401277 -4.1626487 -4.1732635 -4.1557703 -4.1189351][-4.1828957 -4.1899018 -4.1724029 -4.138442 -4.1055326 -4.0750308 -4.0514183 -4.0305772 -4.0376692 -4.0753965 -4.1171889 -4.1519017 -4.1726213 -4.1660113 -4.1376128][-4.193584 -4.2003579 -4.1766467 -4.1304193 -4.0756226 -4.0204325 -3.965579 -3.9182632 -3.9355791 -4.0045881 -4.0772104 -4.1341691 -4.1677661 -4.1747217 -4.1574459][-4.1992011 -4.2059369 -4.1737418 -4.1087093 -4.0225744 -3.9344091 -3.8443739 -3.7800879 -3.8102694 -3.9215269 -4.0319419 -4.114944 -4.1697621 -4.193078 -4.1840587][-4.1950693 -4.203023 -4.1705074 -4.0951734 -3.9965742 -3.8857517 -3.775744 -3.7134895 -3.77032 -3.9046788 -4.019124 -4.1026249 -4.1660929 -4.19859 -4.1987505][-4.1868095 -4.1959305 -4.1703644 -4.1053834 -4.0258155 -3.9282289 -3.818866 -3.7573404 -3.8311148 -3.9578598 -4.0435596 -4.1059618 -4.1671505 -4.2032409 -4.2037239][-4.1669965 -4.1832538 -4.1720924 -4.1292744 -4.0802479 -4.0117435 -3.9260283 -3.8665533 -3.9299443 -4.0249848 -4.0777946 -4.1173916 -4.1667662 -4.1972556 -4.1945877][-4.1557608 -4.1861305 -4.1890635 -4.1680937 -4.1419349 -4.0969529 -4.0400405 -3.9994361 -4.0403461 -4.0886931 -4.1089263 -4.1298618 -4.1638727 -4.1878023 -4.1858072][-4.1414495 -4.1838174 -4.1970525 -4.1862068 -4.1717262 -4.1446481 -4.1094065 -4.0783834 -4.1006327 -4.1220913 -4.1258807 -4.1390004 -4.1577239 -4.174047 -4.1767912][-4.1180544 -4.1713419 -4.1958203 -4.1922431 -4.1833744 -4.1684203 -4.1456418 -4.124002 -4.1293344 -4.1432791 -4.1522932 -4.16392 -4.1670103 -4.1720839 -4.1778703][-4.0891013 -4.14998 -4.1833954 -4.1878352 -4.1816034 -4.1767464 -4.1650276 -4.1530414 -4.1492653 -4.1632075 -4.1766558 -4.1838551 -4.1712875 -4.1645594 -4.1704826][-4.0626082 -4.1215796 -4.1531696 -4.1654134 -4.1695466 -4.1767306 -4.1770406 -4.1719337 -4.1664028 -4.1752777 -4.1830935 -4.1775365 -4.1555347 -4.14257 -4.1498671]]...]
INFO - root - 2017-12-05 23:05:23.442471: step 48510, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 71h:53m:35s remains)
INFO - root - 2017-12-05 23:05:32.527137: step 48520, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 72h:07m:10s remains)
INFO - root - 2017-12-05 23:05:41.540402: step 48530, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 71h:27m:18s remains)
INFO - root - 2017-12-05 23:05:50.695800: step 48540, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.922 sec/batch; 72h:43m:39s remains)
INFO - root - 2017-12-05 23:05:59.738847: step 48550, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 72h:09m:03s remains)
INFO - root - 2017-12-05 23:06:08.870317: step 48560, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 73h:17m:26s remains)
INFO - root - 2017-12-05 23:06:18.010202: step 48570, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 72h:22m:10s remains)
INFO - root - 2017-12-05 23:06:27.194884: step 48580, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 69h:41m:21s remains)
INFO - root - 2017-12-05 23:06:36.255539: step 48590, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 71h:48m:12s remains)
INFO - root - 2017-12-05 23:06:45.378221: step 48600, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.958 sec/batch; 75h:34m:51s remains)
2017-12-05 23:06:46.157240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3374138 -4.3341384 -4.3302259 -4.32602 -4.3208375 -4.3149595 -4.3104277 -4.3061271 -4.30837 -4.3134 -4.31506 -4.3199687 -4.3121805 -4.3099189 -4.3159704][-4.3334188 -4.3318858 -4.3297586 -4.3266611 -4.3219376 -4.3159418 -4.3114543 -4.307826 -4.3114233 -4.318 -4.3204417 -4.3241491 -4.3146076 -4.3106852 -4.315928][-4.3286338 -4.3298979 -4.3308887 -4.3307462 -4.3277311 -4.3222036 -4.3169255 -4.3114657 -4.313201 -4.3186212 -4.3200178 -4.3223934 -4.311552 -4.3063722 -4.3113961][-4.3268447 -4.3296289 -4.3306174 -4.3298941 -4.3253837 -4.3175263 -4.3086686 -4.2964425 -4.2911358 -4.2947831 -4.2972407 -4.3021955 -4.2966738 -4.2956219 -4.3026218][-4.3200712 -4.3197527 -4.3147359 -4.3073425 -4.2944722 -4.2786784 -4.262701 -4.2399397 -4.2266126 -4.2343607 -4.2449932 -4.2583847 -4.26702 -4.2775097 -4.2904496][-4.2928162 -4.2824817 -4.263916 -4.2446313 -4.2195559 -4.1926727 -4.1705761 -4.1418023 -4.1271515 -4.147378 -4.1745191 -4.2037449 -4.229279 -4.2529287 -4.2741909][-4.2299614 -4.2049737 -4.168992 -4.1350751 -4.0970645 -4.0603347 -4.0391736 -4.0162535 -4.0106206 -4.0513153 -4.1021667 -4.1513629 -4.1929173 -4.2287283 -4.258][-4.1425495 -4.1013036 -4.0490789 -4.0071387 -3.9669387 -3.9344418 -3.9296615 -3.9264398 -3.9385045 -3.995626 -4.0626163 -4.1249914 -4.1749711 -4.2163658 -4.2493968][-4.0793242 -4.0343914 -3.9838943 -3.9562364 -3.9360998 -3.9234266 -3.9360185 -3.9493556 -3.9737391 -4.0291624 -4.0906992 -4.1500115 -4.1959677 -4.2318115 -4.2591248][-4.080853 -4.0490832 -4.0212727 -4.0224 -4.0283017 -4.0332832 -4.0475779 -4.0603538 -4.0828643 -4.1248779 -4.1690097 -4.2147279 -4.2486711 -4.2719121 -4.2881546][-4.145741 -4.131947 -4.128294 -4.1484079 -4.1658344 -4.1749215 -4.1820717 -4.1868277 -4.2018137 -4.229804 -4.2580566 -4.2883711 -4.3090811 -4.3200455 -4.3248458][-4.2266674 -4.2265964 -4.2372704 -4.2612095 -4.2777672 -4.2813287 -4.2800846 -4.2794018 -4.2895126 -4.30831 -4.3267975 -4.3446374 -4.3530068 -4.3532534 -4.349309][-4.2779427 -4.2857189 -4.299726 -4.3173194 -4.3266034 -4.3226709 -4.3171673 -4.3164444 -4.3242974 -4.3387761 -4.3539691 -4.3649573 -4.3653483 -4.3596334 -4.3527374][-4.2704587 -4.2828379 -4.2948537 -4.3056316 -4.3118978 -4.3049035 -4.3008838 -4.3070884 -4.31989 -4.3359184 -4.3512959 -4.3587537 -4.3536143 -4.3453937 -4.3406549][-4.2128134 -4.2244096 -4.231842 -4.2410116 -4.2527251 -4.2520361 -4.2588181 -4.2789049 -4.3016596 -4.3224454 -4.3385429 -4.3440323 -4.3356118 -4.3269076 -4.3251538]]...]
INFO - root - 2017-12-05 23:06:55.373686: step 48610, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 71h:52m:45s remains)
INFO - root - 2017-12-05 23:07:04.658227: step 48620, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 72h:24m:14s remains)
INFO - root - 2017-12-05 23:07:13.650784: step 48630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 66h:56m:15s remains)
INFO - root - 2017-12-05 23:07:22.731867: step 48640, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.905 sec/batch; 71h:20m:01s remains)
INFO - root - 2017-12-05 23:07:31.976230: step 48650, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 73h:49m:34s remains)
INFO - root - 2017-12-05 23:07:40.934023: step 48660, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 69h:13m:58s remains)
INFO - root - 2017-12-05 23:07:49.996881: step 48670, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 72h:22m:26s remains)
INFO - root - 2017-12-05 23:07:59.259726: step 48680, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 74h:21m:08s remains)
INFO - root - 2017-12-05 23:08:08.496790: step 48690, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.907 sec/batch; 71h:30m:09s remains)
INFO - root - 2017-12-05 23:08:17.685269: step 48700, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 69h:25m:55s remains)
2017-12-05 23:08:18.447692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1488786 -4.1632595 -4.1909561 -4.2076421 -4.1915455 -4.1681738 -4.1652236 -4.1835375 -4.1967025 -4.199131 -4.1983085 -4.2020721 -4.2064767 -4.2037306 -4.1915426][-4.1403394 -4.1511045 -4.1803312 -4.2017622 -4.1896977 -4.1630435 -4.1529803 -4.1671586 -4.1809421 -4.1840472 -4.1838255 -4.187336 -4.193234 -4.1942687 -4.1883268][-4.1213665 -4.129817 -4.1575303 -4.1829329 -4.1839485 -4.1613688 -4.1441312 -4.1532578 -4.168272 -4.1746659 -4.1739736 -4.1739888 -4.1822658 -4.1885276 -4.1906605][-4.1131077 -4.1189556 -4.1405063 -4.159461 -4.1663723 -4.1464033 -4.1256146 -4.1339164 -4.1536622 -4.1657324 -4.1681428 -4.169796 -4.1821861 -4.19423 -4.1983581][-4.1105251 -4.11764 -4.13295 -4.1398993 -4.1430421 -4.12667 -4.1075063 -4.1197963 -4.1442742 -4.1587768 -4.1653795 -4.1735058 -4.1897349 -4.2017455 -4.2000504][-4.0945773 -4.111702 -4.12924 -4.1300364 -4.1286211 -4.1155109 -4.0987921 -4.1063542 -4.1250458 -4.140399 -4.1533413 -4.1701136 -4.1885738 -4.2003932 -4.1933274][-4.0709977 -4.1073179 -4.1310997 -4.1328712 -4.1278038 -4.1145563 -4.0923872 -4.0804162 -4.0846205 -4.1021609 -4.1298885 -4.162293 -4.1852622 -4.1959538 -4.1883507][-4.0542226 -4.1032128 -4.1316628 -4.1390562 -4.1324544 -4.1161036 -4.0823226 -4.0493956 -4.0399117 -4.0611696 -4.1042 -4.1514139 -4.1774416 -4.1860328 -4.177762][-4.0667214 -4.1172466 -4.1449394 -4.1522675 -4.1471586 -4.1312532 -4.0953817 -4.052742 -4.0350718 -4.0548916 -4.0977483 -4.144742 -4.1673741 -4.1677237 -4.1568489][-4.0974607 -4.144722 -4.1690879 -4.1759105 -4.1745024 -4.1617718 -4.1289754 -4.0873456 -4.065897 -4.0814757 -4.1174846 -4.1547189 -4.1693106 -4.160749 -4.1482744][-4.1253505 -4.1702194 -4.1928997 -4.2000775 -4.1995721 -4.185411 -4.1534948 -4.1131907 -4.09079 -4.1061473 -4.1400032 -4.169301 -4.1767025 -4.1665235 -4.1602564][-4.1458092 -4.1811714 -4.2008839 -4.2119222 -4.2132587 -4.1980772 -4.1663132 -4.1272511 -4.1048236 -4.1202626 -4.1564841 -4.1877456 -4.1948438 -4.1878457 -4.1864476][-4.1676564 -4.1938319 -4.2100735 -4.2223248 -4.2252502 -4.2123046 -4.1861105 -4.1523404 -4.1302085 -4.1419926 -4.175137 -4.2061243 -4.2154465 -4.2076468 -4.2049851][-4.1981735 -4.213098 -4.223968 -4.2356348 -4.241631 -4.234189 -4.216404 -4.1896677 -4.1679087 -4.1702709 -4.1939468 -4.2171979 -4.2241449 -4.2152853 -4.21][-4.2157545 -4.2218852 -4.2293043 -4.23887 -4.2468758 -4.2451673 -4.2329931 -4.2095275 -4.1853 -4.1793985 -4.1945457 -4.2133756 -4.2219381 -4.21702 -4.2101078]]...]
INFO - root - 2017-12-05 23:08:27.643120: step 48710, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 73h:01m:28s remains)
INFO - root - 2017-12-05 23:08:36.802427: step 48720, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.962 sec/batch; 75h:51m:22s remains)
INFO - root - 2017-12-05 23:08:46.005557: step 48730, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 71h:19m:11s remains)
INFO - root - 2017-12-05 23:08:55.000451: step 48740, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 72h:41m:18s remains)
INFO - root - 2017-12-05 23:09:04.126634: step 48750, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 73h:10m:29s remains)
INFO - root - 2017-12-05 23:09:13.273621: step 48760, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 70h:53m:09s remains)
INFO - root - 2017-12-05 23:09:22.295070: step 48770, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 71h:16m:04s remains)
INFO - root - 2017-12-05 23:09:31.316652: step 48780, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 73h:20m:37s remains)
INFO - root - 2017-12-05 23:09:40.425917: step 48790, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 73h:07m:46s remains)
INFO - root - 2017-12-05 23:09:49.392031: step 48800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 70h:43m:23s remains)
2017-12-05 23:09:50.166529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3152361 -4.321229 -4.3208556 -4.3139362 -4.3019261 -4.2816525 -4.242137 -4.1709614 -4.0583076 -3.9371779 -3.8957236 -3.9617715 -4.0686932 -4.1766071 -4.2601814][-4.3008723 -4.3103476 -4.3130212 -4.3098178 -4.3014879 -4.2898874 -4.2677851 -4.2287831 -4.1623297 -4.09062 -4.0622511 -4.0966496 -4.1648335 -4.2406883 -4.299571][-4.285151 -4.2955618 -4.2985539 -4.2978468 -4.2898993 -4.2836194 -4.2768021 -4.2633133 -4.2355838 -4.2045932 -4.1938024 -4.2128463 -4.2535286 -4.2994108 -4.3323903][-4.2782965 -4.2860451 -4.2860718 -4.2811418 -4.26921 -4.2619729 -4.25887 -4.2586341 -4.2570953 -4.2547703 -4.2637539 -4.2846704 -4.3139095 -4.3403029 -4.3538485][-4.2838521 -4.28389 -4.2755175 -4.257997 -4.23291 -4.212141 -4.1948671 -4.1942277 -4.2108927 -4.2413874 -4.2756166 -4.3112912 -4.3411236 -4.3599596 -4.3627362][-4.295845 -4.2835569 -4.2607074 -4.2230988 -4.176053 -4.1252356 -4.0771775 -4.0645361 -4.1019592 -4.1740794 -4.2425432 -4.2998896 -4.3409061 -4.3611107 -4.3608885][-4.3065634 -4.283896 -4.2474294 -4.1864309 -4.1082749 -4.0113611 -3.9141579 -3.8805468 -3.9456565 -4.0671086 -4.1783071 -4.2649837 -4.3217764 -4.3479095 -4.3498578][-4.3090014 -4.2835531 -4.241611 -4.168766 -4.0645123 -3.9253466 -3.7810717 -3.7257969 -3.8159945 -3.97369 -4.1193323 -4.2307982 -4.30174 -4.3331556 -4.337225][-4.3032279 -4.2819886 -4.2439384 -4.1753941 -4.0675759 -3.9205027 -3.7741141 -3.7205393 -3.8103058 -3.9637065 -4.1102977 -4.2254934 -4.296999 -4.328434 -4.3326697][-4.2968521 -4.28493 -4.2579093 -4.2037878 -4.1139183 -3.9899206 -3.8743069 -3.8380361 -3.9067643 -4.0277457 -4.1495404 -4.2479625 -4.3099461 -4.3364739 -4.3370328][-4.2952266 -4.2944827 -4.2809968 -4.2451725 -4.1804533 -4.088891 -4.0075974 -3.9827597 -4.0282564 -4.1150317 -4.2058587 -4.2795835 -4.3268623 -4.3442016 -4.3392406][-4.3002777 -4.3069911 -4.3035789 -4.2858353 -4.2469096 -4.1882463 -4.1364446 -4.1213059 -4.1487327 -4.2040081 -4.2620053 -4.3078928 -4.3356562 -4.3424807 -4.3346996][-4.3106742 -4.3187356 -4.3198256 -4.3143277 -4.2974377 -4.2673059 -4.2391748 -4.2311654 -4.2460909 -4.2758803 -4.30672 -4.3279576 -4.337388 -4.3346472 -4.3251882][-4.3233848 -4.3269763 -4.3263707 -4.3260837 -4.3228817 -4.3138857 -4.305934 -4.3040257 -4.3113804 -4.3234806 -4.3346386 -4.3398681 -4.3367295 -4.3269844 -4.3172565][-4.3326721 -4.3309517 -4.3267083 -4.3265381 -4.3285651 -4.3305254 -4.3329439 -4.3344393 -4.3382134 -4.3414865 -4.3427224 -4.3401141 -4.3318582 -4.3210797 -4.3132482]]...]
INFO - root - 2017-12-05 23:09:59.363828: step 48810, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 67h:56m:18s remains)
INFO - root - 2017-12-05 23:10:08.438720: step 48820, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 72h:40m:03s remains)
INFO - root - 2017-12-05 23:10:17.562785: step 48830, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 71h:56m:46s remains)
INFO - root - 2017-12-05 23:10:26.655766: step 48840, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 74h:25m:41s remains)
INFO - root - 2017-12-05 23:10:35.663348: step 48850, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.907 sec/batch; 71h:27m:08s remains)
INFO - root - 2017-12-05 23:10:44.793759: step 48860, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 71h:22m:41s remains)
INFO - root - 2017-12-05 23:10:53.807163: step 48870, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 69h:37m:16s remains)
INFO - root - 2017-12-05 23:11:02.847431: step 48880, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 71h:28m:44s remains)
INFO - root - 2017-12-05 23:11:12.030217: step 48890, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 70h:41m:14s remains)
INFO - root - 2017-12-05 23:11:21.206221: step 48900, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 71h:54m:12s remains)
2017-12-05 23:11:21.994732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1838779 -4.211586 -4.2527771 -4.2810268 -4.2849908 -4.2809982 -4.2716603 -4.2496963 -4.2216334 -4.19693 -4.1733184 -4.1544566 -4.1613436 -4.1863766 -4.201571][-4.1852808 -4.2145739 -4.2535768 -4.2821493 -4.2891111 -4.2885556 -4.2861729 -4.2750573 -4.258234 -4.2411752 -4.2227254 -4.2071114 -4.2084908 -4.2169695 -4.2137017][-4.1762104 -4.2046595 -4.2371068 -4.2604704 -4.2683992 -4.2723384 -4.276691 -4.277513 -4.27705 -4.2740788 -4.2659831 -4.2556796 -4.2512169 -4.2440119 -4.2229562][-4.1752267 -4.1959705 -4.2173262 -4.2294068 -4.23374 -4.2371631 -4.2436838 -4.2534747 -4.2695351 -4.28447 -4.288444 -4.2846179 -4.27666 -4.2600474 -4.228796][-4.1910253 -4.1968994 -4.2022595 -4.1991529 -4.1914749 -4.1834607 -4.1796188 -4.1881785 -4.2172337 -4.2499928 -4.2697043 -4.2781024 -4.2777143 -4.2644777 -4.2376003][-4.2075167 -4.1994162 -4.1867185 -4.1653047 -4.1374831 -4.1067805 -4.0811815 -4.0822773 -4.1256495 -4.180757 -4.2203503 -4.2452421 -4.2574649 -4.2502069 -4.2273674][-4.2247553 -4.2065144 -4.1759677 -4.1341958 -4.0835085 -4.0249672 -3.9706578 -3.9618609 -4.0227079 -4.1054535 -4.1689272 -4.2110038 -4.2328911 -4.2280655 -4.20745][-4.24488 -4.2238 -4.18047 -4.1219912 -4.0535579 -3.9704654 -3.8868113 -3.8663466 -3.9405203 -4.0448484 -4.1295667 -4.1874456 -4.219337 -4.2189474 -4.2034492][-4.2630777 -4.24301 -4.1981907 -4.13935 -4.0689564 -3.9791551 -3.8853397 -3.8602381 -3.9321189 -4.0360827 -4.1235585 -4.1863694 -4.2195826 -4.2189531 -4.2036815][-4.2754688 -4.255291 -4.2152982 -4.1653843 -4.1053114 -4.0289946 -3.9531679 -3.9370077 -3.9947348 -4.0781708 -4.1518526 -4.2061081 -4.2328687 -4.2306757 -4.2169089][-4.27991 -4.2569222 -4.2209606 -4.1825552 -4.1382322 -4.0853276 -4.0394788 -4.0400319 -4.0864043 -4.1480403 -4.2019329 -4.2391987 -4.2568283 -4.254457 -4.2425971][-4.2827086 -4.2556829 -4.2238712 -4.1991596 -4.1740155 -4.1456213 -4.1272383 -4.1408806 -4.1760035 -4.2153125 -4.2463517 -4.2632637 -4.2703218 -4.2684169 -4.259068][-4.2725239 -4.2403736 -4.2094913 -4.1960435 -4.1898856 -4.1853676 -4.1881995 -4.2092991 -4.2347589 -4.2547231 -4.2636027 -4.2608147 -4.2571597 -4.2563572 -4.2513361][-4.2529249 -4.2214789 -4.1939087 -4.1903782 -4.1976247 -4.2086635 -4.2235889 -4.2459931 -4.2622347 -4.26569 -4.254425 -4.2347865 -4.2231717 -4.2229619 -4.2227068][-4.2400723 -4.2184572 -4.2006207 -4.2043557 -4.2175512 -4.2325907 -4.2477374 -4.2637873 -4.2700219 -4.2610559 -4.2354841 -4.2052708 -4.1908755 -4.1904669 -4.1934524]]...]
INFO - root - 2017-12-05 23:11:31.009587: step 48910, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 72h:17m:48s remains)
INFO - root - 2017-12-05 23:11:40.092948: step 48920, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 73h:00m:59s remains)
INFO - root - 2017-12-05 23:11:49.212456: step 48930, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 73h:16m:10s remains)
INFO - root - 2017-12-05 23:11:58.280688: step 48940, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 71h:47m:02s remains)
INFO - root - 2017-12-05 23:12:07.434881: step 48950, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 71h:01m:19s remains)
INFO - root - 2017-12-05 23:12:16.360191: step 48960, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 66h:23m:58s remains)
INFO - root - 2017-12-05 23:12:25.661617: step 48970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 71h:36m:07s remains)
INFO - root - 2017-12-05 23:12:34.781956: step 48980, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 70h:20m:04s remains)
INFO - root - 2017-12-05 23:12:43.807934: step 48990, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 69h:09m:40s remains)
INFO - root - 2017-12-05 23:12:52.945421: step 49000, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 71h:43m:28s remains)
2017-12-05 23:12:53.815528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2754197 -4.2565141 -4.2383161 -4.2248416 -4.2057366 -4.177372 -4.1462626 -4.1135173 -4.0975208 -4.1181006 -4.1474929 -4.1768456 -4.1985631 -4.1998558 -4.1876426][-4.2818985 -4.2631259 -4.2449579 -4.2314987 -4.2122684 -4.1919608 -4.1735125 -4.1447015 -4.1197314 -4.1261544 -4.1558042 -4.1844053 -4.203815 -4.2021041 -4.1824622][-4.2797236 -4.2630453 -4.2468891 -4.2334328 -4.2112107 -4.1884456 -4.1729789 -4.14467 -4.1168308 -4.117754 -4.1477628 -4.1773977 -4.1984491 -4.1984887 -4.1726265][-4.2692494 -4.248023 -4.2276545 -4.210309 -4.1787238 -4.1474953 -4.1382942 -4.1191998 -4.096169 -4.0996742 -4.1366806 -4.1764317 -4.2020783 -4.2020369 -4.1689467][-4.2556849 -4.2289257 -4.2039208 -4.1811867 -4.14062 -4.1038132 -4.1037169 -4.0968609 -4.079546 -4.0814757 -4.1237917 -4.1761842 -4.2091255 -4.2088022 -4.1670656][-4.2426777 -4.2089915 -4.1788735 -4.1482 -4.0942097 -4.0530486 -4.0612326 -4.0684333 -4.0557108 -4.0599494 -4.1082797 -4.1695719 -4.2085533 -4.2117882 -4.1706724][-4.2391658 -4.2006779 -4.166616 -4.1247773 -4.05388 -4.0049253 -4.0163026 -4.0288696 -4.0201192 -4.0368381 -4.0902758 -4.1500249 -4.1902227 -4.196425 -4.1642275][-4.2506986 -4.2168703 -4.1853046 -4.1396451 -4.0645981 -4.0118413 -4.014936 -4.0195947 -4.0099368 -4.0329623 -4.0843315 -4.1359019 -4.1686869 -4.1732254 -4.148735][-4.2624016 -4.2392211 -4.2176061 -4.1826119 -4.1283627 -4.0827079 -4.0754871 -4.0702581 -4.0611877 -4.0797439 -4.1105146 -4.1428757 -4.1658225 -4.1713743 -4.1526976][-4.2648683 -4.2471189 -4.2343521 -4.2124667 -4.1788425 -4.14245 -4.1228418 -4.1155005 -4.1157141 -4.1273403 -4.1423745 -4.1675625 -4.1910996 -4.2032685 -4.1927319][-4.2606773 -4.2474184 -4.2418051 -4.2307892 -4.2122636 -4.183847 -4.1581154 -4.1473303 -4.149816 -4.158761 -4.1710868 -4.1966105 -4.2245779 -4.2404089 -4.23749][-4.2508416 -4.23925 -4.2367077 -4.2308373 -4.2200217 -4.2019048 -4.1827378 -4.1735678 -4.1785903 -4.1855617 -4.1959772 -4.2161837 -4.2367082 -4.2494693 -4.25104][-4.2435603 -4.232286 -4.2277808 -4.2211347 -4.2083559 -4.194109 -4.1890378 -4.1896391 -4.193841 -4.1973262 -4.2056966 -4.2173662 -4.2256393 -4.235394 -4.2419577][-4.2421322 -4.231163 -4.2233129 -4.2155018 -4.2010231 -4.1882377 -4.1931157 -4.2024341 -4.2042632 -4.206347 -4.2149019 -4.2212448 -4.2227969 -4.2296486 -4.2383366][-4.2415662 -4.2293415 -4.2200117 -4.2125587 -4.2007895 -4.192441 -4.2041955 -4.2176538 -4.218215 -4.2204976 -4.2281795 -4.2311835 -4.2305689 -4.2354016 -4.2421045]]...]
INFO - root - 2017-12-05 23:13:03.035932: step 49010, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 72h:13m:11s remains)
INFO - root - 2017-12-05 23:13:12.230207: step 49020, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 72h:01m:13s remains)
INFO - root - 2017-12-05 23:13:21.287491: step 49030, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 72h:30m:50s remains)
INFO - root - 2017-12-05 23:13:30.426745: step 49040, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 70h:22m:19s remains)
INFO - root - 2017-12-05 23:13:39.656130: step 49050, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 74h:06m:37s remains)
INFO - root - 2017-12-05 23:13:48.640713: step 49060, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 72h:02m:10s remains)
INFO - root - 2017-12-05 23:13:57.817824: step 49070, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 72h:32m:00s remains)
INFO - root - 2017-12-05 23:14:06.997246: step 49080, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 71h:48m:10s remains)
INFO - root - 2017-12-05 23:14:16.213038: step 49090, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 73h:02m:57s remains)
INFO - root - 2017-12-05 23:14:25.232598: step 49100, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 68h:58m:27s remains)
2017-12-05 23:14:26.059388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2996955 -4.2895246 -4.2888532 -4.2919164 -4.291317 -4.2859964 -4.2756834 -4.2463846 -4.2074013 -4.1865273 -4.2026286 -4.223218 -4.2366142 -4.2446804 -4.2594137][-4.2912903 -4.2805762 -4.2782521 -4.2806659 -4.2834873 -4.2805009 -4.2661495 -4.2319555 -4.1911454 -4.1722422 -4.1895318 -4.2121148 -4.22682 -4.2344937 -4.2484093][-4.2767878 -4.2698755 -4.266221 -4.2659197 -4.2696252 -4.2670217 -4.2481775 -4.2101388 -4.16947 -4.1554503 -4.1747231 -4.1993041 -4.2198062 -4.2317653 -4.246819][-4.2557135 -4.2525043 -4.2476392 -4.2436123 -4.2426443 -4.2368112 -4.2154856 -4.1770554 -4.1383777 -4.1307049 -4.1552925 -4.1878362 -4.2151904 -4.2336078 -4.2531743][-4.2383971 -4.2292662 -4.21759 -4.2121835 -4.2070351 -4.1997123 -4.1790671 -4.1446466 -4.1057062 -4.0993705 -4.1300716 -4.1720462 -4.207098 -4.2317662 -4.2584887][-4.2165661 -4.1981106 -4.1844711 -4.1794744 -4.1738405 -4.1730142 -4.1552396 -4.116981 -4.0624366 -4.0444579 -4.0784225 -4.1376829 -4.188755 -4.2264762 -4.2638569][-4.1804895 -4.15106 -4.1393766 -4.1357512 -4.1347613 -4.1461005 -4.13474 -4.0894561 -4.0075321 -3.9636669 -4.0025835 -4.086699 -4.1645193 -4.2179956 -4.2631316][-4.1464009 -4.1113014 -4.0978227 -4.0950713 -4.0991211 -4.1212616 -4.1243382 -4.0797286 -3.9736466 -3.9023027 -3.9438381 -4.0488377 -4.1442533 -4.2055922 -4.2524204][-4.1456089 -4.1079626 -4.0907969 -4.0824127 -4.0863943 -4.1161714 -4.1317177 -4.0897141 -3.9809611 -3.9070675 -3.9494081 -4.054522 -4.1431885 -4.1959929 -4.2373743][-4.1791472 -4.1446223 -4.12496 -4.1082711 -4.1106629 -4.1400576 -4.1561508 -4.1219335 -4.03104 -3.9731395 -4.0121374 -4.0968418 -4.1603465 -4.1951332 -4.22789][-4.2084818 -4.1879153 -4.1734319 -4.1552992 -4.1546731 -4.1768641 -4.1868029 -4.1612635 -4.0966797 -4.0570526 -4.0861044 -4.1426692 -4.1819444 -4.2033787 -4.2276349][-4.2397723 -4.2365918 -4.2310591 -4.2155051 -4.2103934 -4.2222009 -4.2254171 -4.2061243 -4.1643891 -4.1374588 -4.1543827 -4.1867347 -4.2078562 -4.2193613 -4.2370186][-4.2649322 -4.2717237 -4.2707372 -4.2596536 -4.2553511 -4.2586279 -4.2569656 -4.2439289 -4.2185512 -4.2010093 -4.2078085 -4.2251596 -4.2367277 -4.2447052 -4.2616048][-4.2785716 -4.2896557 -4.2931294 -4.2891865 -4.2886181 -4.290267 -4.2887068 -4.2816367 -4.2674193 -4.2561917 -4.256887 -4.2657475 -4.2740889 -4.2818727 -4.2986603][-4.298625 -4.3076053 -4.31434 -4.3156667 -4.3153987 -4.3156629 -4.3156166 -4.3118272 -4.3032646 -4.2954755 -4.2938848 -4.3004518 -4.3099756 -4.3197012 -4.3344245]]...]
INFO - root - 2017-12-05 23:14:35.139568: step 49110, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 70h:36m:42s remains)
INFO - root - 2017-12-05 23:14:44.288004: step 49120, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 70h:48m:01s remains)
INFO - root - 2017-12-05 23:14:53.566090: step 49130, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.947 sec/batch; 74h:31m:12s remains)
INFO - root - 2017-12-05 23:15:02.795311: step 49140, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 70h:30m:23s remains)
INFO - root - 2017-12-05 23:15:11.550436: step 49150, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 71h:33m:07s remains)
INFO - root - 2017-12-05 23:15:20.543551: step 49160, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.896 sec/batch; 70h:33m:05s remains)
INFO - root - 2017-12-05 23:15:29.925853: step 49170, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 74h:30m:57s remains)
INFO - root - 2017-12-05 23:15:39.022799: step 49180, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 71h:21m:07s remains)
INFO - root - 2017-12-05 23:15:48.094532: step 49190, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 72h:54m:56s remains)
INFO - root - 2017-12-05 23:15:57.227575: step 49200, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 71h:30m:08s remains)
2017-12-05 23:15:58.029936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2702718 -4.2680845 -4.2606444 -4.2476006 -4.230731 -4.2233372 -4.23263 -4.2483029 -4.2634792 -4.2763672 -4.2815251 -4.2792597 -4.2756944 -4.2711511 -4.2673359][-4.2860241 -4.2783103 -4.2630663 -4.2405486 -4.2130227 -4.1999941 -4.2130456 -4.2383294 -4.2636247 -4.2847614 -4.2970676 -4.297802 -4.2955093 -4.2917461 -4.2877145][-4.2872586 -4.2730494 -4.2482486 -4.2140965 -4.1747804 -4.1532378 -4.1650453 -4.1992536 -4.2336116 -4.2610521 -4.2816105 -4.2901335 -4.2941647 -4.2952189 -4.2949371][-4.2852597 -4.2647471 -4.2306108 -4.1850963 -4.1345139 -4.1018686 -4.105351 -4.1448674 -4.1858444 -4.2164125 -4.2458143 -4.2675467 -4.2841015 -4.2944069 -4.300807][-4.2833781 -4.2582808 -4.2169623 -4.1604052 -4.0962219 -4.0485225 -4.0346112 -4.0755415 -4.1236997 -4.1581984 -4.1963763 -4.2357907 -4.2694626 -4.291821 -4.3049774][-4.281796 -4.25544 -4.2105823 -4.1453753 -4.0662451 -3.9958763 -3.9544024 -3.9924071 -4.050416 -4.0936675 -4.142736 -4.1999931 -4.25055 -4.2846885 -4.3038034][-4.2811642 -4.2546058 -4.2102537 -4.1395698 -4.0486836 -3.9519231 -3.8743858 -3.9048555 -3.9812067 -4.0457897 -4.1111436 -4.1834311 -4.24681 -4.2862439 -4.3058391][-4.2808228 -4.2561741 -4.2129226 -4.1380353 -4.0393434 -3.9225216 -3.8165758 -3.8343878 -3.9306569 -4.0231514 -4.1077719 -4.1913242 -4.2587209 -4.2961922 -4.3101234][-4.28177 -4.2595568 -4.2203169 -4.1523113 -4.0639844 -3.9601834 -3.8664174 -3.8740351 -3.9646173 -4.0594292 -4.1457591 -4.2269921 -4.2850013 -4.3119111 -4.3162956][-4.2865748 -4.2677665 -4.2331071 -4.1778469 -4.1096411 -4.0337858 -3.9663312 -3.97158 -4.0471163 -4.1341443 -4.2132311 -4.2788773 -4.3173251 -4.3278723 -4.3214464][-4.2930927 -4.2777343 -4.2470145 -4.2016191 -4.1514897 -4.1054044 -4.0678968 -4.0776987 -4.1385107 -4.2118111 -4.2763238 -4.3211441 -4.3394227 -4.3342242 -4.3205595][-4.2926855 -4.2794905 -4.251811 -4.215229 -4.1837058 -4.1662374 -4.1593513 -4.1775866 -4.2228346 -4.2753768 -4.3204184 -4.3457212 -4.3474374 -4.3309069 -4.313941][-4.2825255 -4.2666941 -4.239728 -4.2094297 -4.1942606 -4.2003622 -4.2131467 -4.2353783 -4.2716241 -4.3077493 -4.3366446 -4.3489871 -4.3427196 -4.3228197 -4.3064961][-4.2671123 -4.2426047 -4.2107143 -4.1822329 -4.1762495 -4.2003431 -4.2291842 -4.2559428 -4.2891726 -4.3156958 -4.3341184 -4.341361 -4.3347297 -4.3162837 -4.3022146][-4.2485113 -4.21441 -4.1741457 -4.1401339 -4.1319332 -4.164753 -4.2041235 -4.2344928 -4.2683325 -4.296217 -4.3161955 -4.3267183 -4.3259244 -4.3117485 -4.3005023]]...]
INFO - root - 2017-12-05 23:16:07.250091: step 49210, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 72h:50m:33s remains)
INFO - root - 2017-12-05 23:16:16.361507: step 49220, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.900 sec/batch; 70h:49m:48s remains)
INFO - root - 2017-12-05 23:16:25.477944: step 49230, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 70h:44m:01s remains)
INFO - root - 2017-12-05 23:16:34.646908: step 49240, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 71h:27m:08s remains)
INFO - root - 2017-12-05 23:16:43.656552: step 49250, loss = 2.10, batch loss = 2.04 (8.7 examples/sec; 0.919 sec/batch; 72h:18m:24s remains)
INFO - root - 2017-12-05 23:16:52.821677: step 49260, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 73h:00m:03s remains)
INFO - root - 2017-12-05 23:17:01.999470: step 49270, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 71h:23m:19s remains)
INFO - root - 2017-12-05 23:17:10.977751: step 49280, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 71h:34m:14s remains)
INFO - root - 2017-12-05 23:17:20.114695: step 49290, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 71h:56m:07s remains)
INFO - root - 2017-12-05 23:17:29.359442: step 49300, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 72h:05m:12s remains)
2017-12-05 23:17:30.138692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2868342 -4.2828145 -4.2768936 -4.276361 -4.2743292 -4.2674093 -4.2590065 -4.2566462 -4.2641282 -4.2639809 -4.25595 -4.2510672 -4.2544746 -4.2583523 -4.2604165][-4.2734704 -4.2644262 -4.2530918 -4.2488246 -4.2416162 -4.2296929 -4.2171049 -4.21254 -4.2222223 -4.2214074 -4.2104683 -4.210382 -4.2205286 -4.2257323 -4.2297554][-4.2568283 -4.243793 -4.2297015 -4.2240438 -4.212841 -4.1954408 -4.177 -4.1661749 -4.1739554 -4.1733346 -4.1679225 -4.1775527 -4.1929955 -4.1969271 -4.202333][-4.248435 -4.23783 -4.2274475 -4.2230325 -4.2091122 -4.1821513 -4.1512909 -4.1306715 -4.13366 -4.1337214 -4.1359234 -4.1560664 -4.1767573 -4.1793165 -4.1883335][-4.25039 -4.245182 -4.2367492 -4.2278595 -4.2054253 -4.1673956 -4.1262097 -4.0958185 -4.0932627 -4.0946546 -4.1048155 -4.1374154 -4.1660581 -4.1729417 -4.1874256][-4.2595749 -4.2564449 -4.2437944 -4.2229776 -4.1861897 -4.138216 -4.0876408 -4.0487766 -4.045897 -4.0536957 -4.0753331 -4.1205144 -4.1573286 -4.1727109 -4.19532][-4.2623725 -4.2579474 -4.2394137 -4.2062879 -4.1543818 -4.0935035 -4.0324931 -3.9908528 -4.0002222 -4.0291595 -4.0714259 -4.1275067 -4.1659236 -4.1855965 -4.2101955][-4.2505851 -4.2363486 -4.2064939 -4.1639104 -4.1037831 -4.0350981 -3.974138 -3.946563 -3.9833837 -4.0395331 -4.0954833 -4.14981 -4.1819887 -4.20224 -4.224793][-4.2293859 -4.2051668 -4.1700187 -4.1311116 -4.0776267 -4.0156097 -3.9725595 -3.9704764 -4.0279183 -4.0952973 -4.148591 -4.1888561 -4.2088237 -4.2235126 -4.2439432][-4.2203846 -4.198441 -4.1694436 -4.139895 -4.1019888 -4.0618973 -4.0435367 -4.0609164 -4.1237044 -4.1799021 -4.2124352 -4.2325821 -4.2407513 -4.2489758 -4.265039][-4.2337866 -4.2201333 -4.197226 -4.1782541 -4.1600776 -4.1418781 -4.1410966 -4.1661286 -4.2159152 -4.2503443 -4.2618213 -4.2677269 -4.26788 -4.2703805 -4.2811203][-4.2581811 -4.2520432 -4.23621 -4.2242494 -4.2190657 -4.2166066 -4.2254229 -4.2485142 -4.2800512 -4.2959447 -4.294723 -4.29368 -4.2916918 -4.2894874 -4.2951984][-4.2834592 -4.2808623 -4.2680578 -4.2580147 -4.2589746 -4.2645316 -4.2766194 -4.294836 -4.3106136 -4.314239 -4.3091507 -4.3071079 -4.3053079 -4.3017864 -4.30399][-4.299706 -4.2977705 -4.2876563 -4.2792616 -4.2814207 -4.2882609 -4.2982249 -4.30864 -4.3136191 -4.3126979 -4.3085613 -4.3079696 -4.307538 -4.3056149 -4.3070731][-4.3030634 -4.298574 -4.290278 -4.2839971 -4.2841959 -4.2891 -4.2962737 -4.3024559 -4.3044004 -4.3048244 -4.3034315 -4.3037181 -4.303709 -4.3027129 -4.3044729]]...]
INFO - root - 2017-12-05 23:17:39.251508: step 49310, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 72h:19m:12s remains)
INFO - root - 2017-12-05 23:17:48.363308: step 49320, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 71h:47m:54s remains)
INFO - root - 2017-12-05 23:17:57.552285: step 49330, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 74h:24m:39s remains)
INFO - root - 2017-12-05 23:18:06.590934: step 49340, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 73h:08m:02s remains)
INFO - root - 2017-12-05 23:18:15.557907: step 49350, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 69h:19m:43s remains)
INFO - root - 2017-12-05 23:18:24.696851: step 49360, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 75h:00m:05s remains)
INFO - root - 2017-12-05 23:18:33.781126: step 49370, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 73h:57m:29s remains)
INFO - root - 2017-12-05 23:18:42.875099: step 49380, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 71h:27m:17s remains)
INFO - root - 2017-12-05 23:18:52.141683: step 49390, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 70h:50m:56s remains)
INFO - root - 2017-12-05 23:19:01.235289: step 49400, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 72h:44m:12s remains)
2017-12-05 23:19:02.085761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2022982 -4.2025967 -4.1958137 -4.1859303 -4.1824746 -4.1840324 -4.1897717 -4.1928658 -4.1861835 -4.1742496 -4.1717882 -4.1846223 -4.2033591 -4.2281847 -4.2643113][-4.1595492 -4.1583891 -4.1524916 -4.144269 -4.1424332 -4.1433039 -4.1441336 -4.1440125 -4.1362839 -4.1253457 -4.1286578 -4.1504717 -4.1784773 -4.2101326 -4.2527189][-4.1319251 -4.1289191 -4.1234083 -4.1151 -4.1128988 -4.1093936 -4.1012816 -4.0950775 -4.0909224 -4.0887141 -4.0976677 -4.1239071 -4.1590147 -4.1976061 -4.2464266][-4.1101952 -4.1048207 -4.0991864 -4.0894303 -4.0812044 -4.0651293 -4.0393462 -4.02359 -4.0295324 -4.0448604 -4.0642247 -4.0977116 -4.1431293 -4.1948075 -4.2488232][-4.1097293 -4.0935192 -4.0790529 -4.0644026 -4.0521545 -4.0267348 -3.9757314 -3.9389093 -3.9572256 -4.0056329 -4.0460305 -4.0873613 -4.1422739 -4.2022223 -4.2552094][-4.0805554 -4.0516281 -4.027945 -4.0128636 -4.0044818 -3.9759696 -3.8908379 -3.8124371 -3.8452299 -3.9415853 -4.0109282 -4.0599608 -4.1214337 -4.1848211 -4.24167][-4.0247555 -3.986532 -3.952445 -3.9342415 -3.9227328 -3.8870265 -3.7749817 -3.6595643 -3.7110164 -3.857275 -3.956584 -4.0162272 -4.0866823 -4.1634622 -4.2326412][-4.0259147 -3.9969046 -3.967999 -3.9573824 -3.9527764 -3.9271469 -3.83089 -3.7273414 -3.764535 -3.8862929 -3.9732156 -4.0243545 -4.0877652 -4.1675668 -4.2410917][-4.0754876 -4.0551505 -4.0352955 -4.0316596 -4.031858 -4.0179806 -3.955709 -3.8853724 -3.9006963 -3.9754782 -4.0360231 -4.0738287 -4.1265035 -4.1985526 -4.2619109][-4.1196208 -4.0967593 -4.07998 -4.083262 -4.0911164 -4.0884647 -4.0531626 -4.0142918 -4.021915 -4.067811 -4.1098247 -4.1378965 -4.1795363 -4.2407026 -4.2898321][-4.1626782 -4.137126 -4.122366 -4.1302748 -4.1448884 -4.1510544 -4.1356654 -4.1176329 -4.1199365 -4.1446147 -4.1743979 -4.1940579 -4.2239709 -4.2737441 -4.3095865][-4.1910777 -4.1711192 -4.1577649 -4.1632705 -4.1774483 -4.1900716 -4.1857986 -4.1710963 -4.1646733 -4.1757021 -4.1963115 -4.2125406 -4.2369828 -4.2784419 -4.3079][-4.2014561 -4.187058 -4.1774583 -4.1832204 -4.1980329 -4.2149911 -4.2126231 -4.1919222 -4.1768165 -4.1773953 -4.1886044 -4.2000279 -4.2184896 -4.254374 -4.2853627][-4.2036304 -4.1897593 -4.1821723 -4.189559 -4.2076936 -4.226398 -4.218955 -4.190856 -4.1671166 -4.1567159 -4.1603909 -4.1712 -4.1899929 -4.2251759 -4.26138][-4.2138853 -4.2037468 -4.1995764 -4.2078042 -4.2254558 -4.2418728 -4.2306018 -4.1947289 -4.1596956 -4.1384072 -4.1385293 -4.1525812 -4.1755877 -4.2137141 -4.2556591]]...]
INFO - root - 2017-12-05 23:19:11.315976: step 49410, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.965 sec/batch; 75h:54m:02s remains)
INFO - root - 2017-12-05 23:19:20.492303: step 49420, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 73h:02m:00s remains)
INFO - root - 2017-12-05 23:19:29.534853: step 49430, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 72h:18m:46s remains)
INFO - root - 2017-12-05 23:19:38.626040: step 49440, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 71h:05m:15s remains)
INFO - root - 2017-12-05 23:19:47.811845: step 49450, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 70h:06m:50s remains)
INFO - root - 2017-12-05 23:19:56.856112: step 49460, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.930 sec/batch; 73h:05m:54s remains)
INFO - root - 2017-12-05 23:20:05.817609: step 49470, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 71h:19m:18s remains)
INFO - root - 2017-12-05 23:20:14.890502: step 49480, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 72h:54m:18s remains)
INFO - root - 2017-12-05 23:20:24.075824: step 49490, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 70h:46m:54s remains)
INFO - root - 2017-12-05 23:20:33.209579: step 49500, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 70h:38m:40s remains)
2017-12-05 23:20:33.995380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2032604 -4.1934261 -4.1714039 -4.1409 -4.1207867 -4.1233311 -4.1434283 -4.1505876 -4.146646 -4.1474619 -4.1646409 -4.1920443 -4.224648 -4.2662148 -4.3067527][-4.1674938 -4.1619072 -4.1499991 -4.1261339 -4.1092248 -4.117146 -4.1411605 -4.1450915 -4.1358037 -4.1377468 -4.1610088 -4.1892877 -4.2241011 -4.2661815 -4.3063712][-4.1303577 -4.1425433 -4.1483607 -4.1367855 -4.1251187 -4.1350789 -4.1580491 -4.157701 -4.1410871 -4.1407385 -4.1659727 -4.1938977 -4.2297058 -4.2720828 -4.3087358][-4.0963974 -4.126966 -4.1545305 -4.1629076 -4.1622486 -4.1754384 -4.1980867 -4.1980548 -4.1773295 -4.1731811 -4.1957688 -4.2206407 -4.2520308 -4.2882309 -4.3161526][-4.0898919 -4.1286244 -4.1654749 -4.1840014 -4.186552 -4.1969666 -4.2167606 -4.2231364 -4.2127628 -4.21608 -4.2396612 -4.2630544 -4.2870426 -4.3142338 -4.3315992][-4.1046953 -4.1313725 -4.1556025 -4.1649055 -4.1602449 -4.1663976 -4.1880879 -4.2050896 -4.2098012 -4.2257204 -4.2563119 -4.2829723 -4.3069253 -4.33124 -4.344697][-4.1224661 -4.1230745 -4.1188602 -4.1018705 -4.0782771 -4.0749707 -4.0961885 -4.1174393 -4.1337996 -4.1690054 -4.2141004 -4.2521033 -4.2861848 -4.3202281 -4.3412657][-4.1426435 -4.1192994 -4.0864272 -4.0414257 -3.9932253 -3.9665737 -3.9676414 -3.972014 -3.9888747 -4.0482106 -4.1214895 -4.182785 -4.236115 -4.2877 -4.323051][-4.1586585 -4.1228504 -4.078207 -4.023963 -3.9665186 -3.923141 -3.8974609 -3.8651 -3.8641262 -3.9352298 -4.0314169 -4.1183052 -4.1931338 -4.2601576 -4.3062434][-4.1873288 -4.153275 -4.1162744 -4.0738358 -4.0297484 -3.9932501 -3.9635391 -3.9172535 -3.9029148 -3.9560046 -4.0378647 -4.1204982 -4.19336 -4.2585745 -4.3030992][-4.2185893 -4.198339 -4.1767058 -4.15255 -4.1250525 -4.1021242 -4.08383 -4.0507655 -4.0395279 -4.0741787 -4.1301832 -4.1882954 -4.23906 -4.2879257 -4.3204045][-4.2417727 -4.2349815 -4.2266035 -4.2156291 -4.1997566 -4.1870475 -4.175993 -4.1524382 -4.1431818 -4.1663513 -4.2052565 -4.2455068 -4.2801161 -4.3163743 -4.3398237][-4.2497492 -4.2488112 -4.24641 -4.242312 -4.2346091 -4.2269659 -4.2163272 -4.1915259 -4.1809773 -4.1997538 -4.2297525 -4.2632275 -4.2944388 -4.3274021 -4.3468103][-4.2410064 -4.2407 -4.2384129 -4.2360287 -4.2320714 -4.225359 -4.2095842 -4.1750584 -4.1607356 -4.17882 -4.2082648 -4.2412052 -4.2736049 -4.3089771 -4.3319678][-4.2173991 -4.2175736 -4.214242 -4.2101736 -4.2057047 -4.1964183 -4.1729693 -4.1253376 -4.1045442 -4.1215458 -4.1532183 -4.1893849 -4.2238193 -4.265523 -4.298203]]...]
INFO - root - 2017-12-05 23:20:43.054510: step 49510, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 71h:50m:43s remains)
INFO - root - 2017-12-05 23:20:52.203321: step 49520, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 71h:07m:42s remains)
INFO - root - 2017-12-05 23:21:01.325647: step 49530, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.949 sec/batch; 74h:35m:06s remains)
INFO - root - 2017-12-05 23:21:10.559925: step 49540, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.960 sec/batch; 75h:29m:22s remains)
INFO - root - 2017-12-05 23:21:19.642043: step 49550, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.908 sec/batch; 71h:23m:52s remains)
INFO - root - 2017-12-05 23:21:28.578596: step 49560, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 0.804 sec/batch; 63h:09m:33s remains)
INFO - root - 2017-12-05 23:21:37.711695: step 49570, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 73h:20m:30s remains)
INFO - root - 2017-12-05 23:21:46.825778: step 49580, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:56m:24s remains)
INFO - root - 2017-12-05 23:21:56.041283: step 49590, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 72h:14m:27s remains)
INFO - root - 2017-12-05 23:22:05.256497: step 49600, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 70h:42m:00s remains)
2017-12-05 23:22:06.038726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1595864 -4.1692619 -4.1755071 -4.1519418 -4.1163888 -4.10504 -4.1021757 -4.113699 -4.1563387 -4.1965432 -4.2159276 -4.2280369 -4.2405372 -4.2541161 -4.2819452][-4.1586218 -4.1616745 -4.164259 -4.1469493 -4.1188574 -4.1170793 -4.1189771 -4.1323333 -4.1741314 -4.2115445 -4.2316618 -4.2440252 -4.2588992 -4.2689824 -4.2902122][-4.1533432 -4.1510563 -4.1491332 -4.14257 -4.1283932 -4.1331081 -4.1390162 -4.1508856 -4.1798449 -4.2067971 -4.2252164 -4.2419906 -4.2618613 -4.2745895 -4.2920656][-4.1489334 -4.1350489 -4.1343322 -4.135963 -4.1291504 -4.1300883 -4.1325111 -4.1351137 -4.1526618 -4.1740155 -4.1952682 -4.2208986 -4.2478991 -4.2685208 -4.2876821][-4.1412621 -4.1212015 -4.1264982 -4.1347284 -4.1300025 -4.1196222 -4.1067991 -4.0955844 -4.1048355 -4.1270442 -4.1505218 -4.1832609 -4.2225561 -4.255774 -4.2810779][-4.1454768 -4.1257176 -4.1328168 -4.137857 -4.1288114 -4.1086822 -4.07862 -4.0492644 -4.0515032 -4.0779934 -4.1101727 -4.15021 -4.2007737 -4.2454777 -4.2775235][-4.1708717 -4.1529641 -4.1520205 -4.14758 -4.135572 -4.1124558 -4.0689764 -4.0217414 -4.010407 -4.0465951 -4.0914855 -4.1353831 -4.1913781 -4.2434459 -4.2798038][-4.2003212 -4.1896567 -4.1857343 -4.1765943 -4.168273 -4.1473522 -4.0971928 -4.0374269 -4.0064683 -4.0418787 -4.0919747 -4.1352181 -4.1928573 -4.246809 -4.2827168][-4.226048 -4.2202511 -4.217546 -4.2075877 -4.20536 -4.1954279 -4.1553483 -4.0999875 -4.0633764 -4.0872264 -4.1239724 -4.1579676 -4.210865 -4.2601175 -4.2891436][-4.2374253 -4.237391 -4.2394686 -4.2309952 -4.230546 -4.224556 -4.1952815 -4.1569424 -4.1343122 -4.1517639 -4.1764684 -4.2011919 -4.2437716 -4.2808466 -4.2981768][-4.2391467 -4.2447 -4.2508574 -4.2462325 -4.2449675 -4.2383881 -4.2185669 -4.1984892 -4.1914182 -4.2050519 -4.2217546 -4.240695 -4.2711172 -4.2944918 -4.3045287][-4.2441463 -4.2479544 -4.2535295 -4.253334 -4.2530837 -4.2514434 -4.2433681 -4.235857 -4.2361679 -4.2426829 -4.2519212 -4.2660112 -4.2843528 -4.2988524 -4.3073044][-4.260705 -4.2608161 -4.261097 -4.25853 -4.2600646 -4.2637496 -4.2639365 -4.2620831 -4.2656174 -4.2698932 -4.2767458 -4.287231 -4.2950482 -4.302496 -4.3096151][-4.2755003 -4.274055 -4.2716928 -4.26709 -4.2682667 -4.2730227 -4.2752743 -4.2758904 -4.2778487 -4.2822623 -4.2898903 -4.2967253 -4.2986865 -4.3024235 -4.3089952][-4.2889795 -4.2863126 -4.2847333 -4.2819557 -4.2801633 -4.27997 -4.279891 -4.279099 -4.2801313 -4.2838717 -4.2902842 -4.2952766 -4.2962737 -4.2999597 -4.3073745]]...]
INFO - root - 2017-12-05 23:22:15.416645: step 49610, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.968 sec/batch; 76h:05m:19s remains)
INFO - root - 2017-12-05 23:22:24.382488: step 49620, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 70h:04m:56s remains)
INFO - root - 2017-12-05 23:22:33.500956: step 49630, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 70h:50m:59s remains)
INFO - root - 2017-12-05 23:22:42.629733: step 49640, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 72h:13m:00s remains)
INFO - root - 2017-12-05 23:22:51.803368: step 49650, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 72h:47m:22s remains)
INFO - root - 2017-12-05 23:23:00.870832: step 49660, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 68h:52m:39s remains)
INFO - root - 2017-12-05 23:23:09.914401: step 49670, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 72h:57m:32s remains)
INFO - root - 2017-12-05 23:23:19.189707: step 49680, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.918 sec/batch; 72h:08m:24s remains)
INFO - root - 2017-12-05 23:23:28.359994: step 49690, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 70h:39m:45s remains)
INFO - root - 2017-12-05 23:23:37.601992: step 49700, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.951 sec/batch; 74h:41m:31s remains)
2017-12-05 23:23:38.449940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1366782 -4.1200733 -4.130064 -4.1399131 -4.1435876 -4.1213317 -4.103723 -4.1189013 -4.1519847 -4.1930308 -4.2457547 -4.2913294 -4.3203483 -4.3346181 -4.3418617][-4.1092076 -4.09034 -4.1113505 -4.1307631 -4.1394262 -4.1189342 -4.1032238 -4.1227093 -4.1645412 -4.2050681 -4.2565203 -4.3009062 -4.327374 -4.3383517 -4.3434372][-4.0976133 -4.0754595 -4.103117 -4.1273937 -4.1328959 -4.1059475 -4.0894718 -4.1162758 -4.1690054 -4.2168074 -4.2703152 -4.3150682 -4.3380117 -4.3443742 -4.3455453][-4.1155381 -4.0971012 -4.12802 -4.1498556 -4.1431994 -4.107326 -4.0888081 -4.1224689 -4.1792769 -4.2343936 -4.2867336 -4.3320374 -4.3515768 -4.3523049 -4.3492875][-4.1519427 -4.141161 -4.1688786 -4.180007 -4.1647248 -4.1247911 -4.1044025 -4.1375723 -4.1968203 -4.2549067 -4.3028669 -4.3445129 -4.3608966 -4.3585477 -4.3533983][-4.1872492 -4.1809926 -4.2000403 -4.1991658 -4.1825304 -4.1399064 -4.1133738 -4.1390023 -4.1995778 -4.2601318 -4.30378 -4.3429656 -4.3600249 -4.3601608 -4.3553214][-4.2136703 -4.2080889 -4.2161884 -4.2041507 -4.1864257 -4.1391287 -4.1078048 -4.1252241 -4.1835747 -4.242578 -4.2881742 -4.331706 -4.3553519 -4.3601274 -4.356185][-4.2280869 -4.2224431 -4.2260089 -4.2083778 -4.1830177 -4.1303844 -4.0999036 -4.1122503 -4.1667666 -4.2243447 -4.2727656 -4.3203745 -4.3501258 -4.3580785 -4.3557944][-4.235261 -4.2299814 -4.229897 -4.20889 -4.1750212 -4.1260767 -4.1027079 -4.1150112 -4.1649318 -4.2189579 -4.2684145 -4.316792 -4.3479347 -4.3565607 -4.3550587][-4.2354212 -4.2301779 -4.2240286 -4.2034788 -4.171175 -4.1337757 -4.1188588 -4.1339846 -4.1782532 -4.2249408 -4.2729692 -4.3194408 -4.346837 -4.3537116 -4.3529181][-4.2309365 -4.2243285 -4.2149754 -4.202198 -4.1826119 -4.1516633 -4.1376419 -4.1554012 -4.1966271 -4.2362761 -4.2826829 -4.3260989 -4.3467107 -4.3503695 -4.3499775][-4.2263241 -4.218298 -4.2101145 -4.2072172 -4.2027068 -4.1788864 -4.162714 -4.1807795 -4.21969 -4.2559066 -4.296751 -4.3341513 -4.3480835 -4.3479991 -4.3474584][-4.225481 -4.2166333 -4.2099609 -4.2078633 -4.20774 -4.1917877 -4.1812449 -4.2025652 -4.2442474 -4.279561 -4.3121204 -4.3415422 -4.3503609 -4.3473992 -4.3465576][-4.2368875 -4.2261729 -4.2200513 -4.2157512 -4.21309 -4.1992974 -4.1944227 -4.2191458 -4.2633133 -4.2995763 -4.3267221 -4.3481369 -4.3529396 -4.3480444 -4.3467669][-4.2685418 -4.2576323 -4.2482867 -4.2377291 -4.2284517 -4.211493 -4.2079749 -4.2347741 -4.2794657 -4.3141303 -4.3369994 -4.3518586 -4.3535824 -4.3485708 -4.3472071]]...]
INFO - root - 2017-12-05 23:23:47.501140: step 49710, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 70h:43m:52s remains)
INFO - root - 2017-12-05 23:23:56.851979: step 49720, loss = 2.09, batch loss = 2.03 (7.7 examples/sec; 1.040 sec/batch; 81h:42m:27s remains)
INFO - root - 2017-12-05 23:24:05.939719: step 49730, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.936 sec/batch; 73h:32m:16s remains)
INFO - root - 2017-12-05 23:24:14.968124: step 49740, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 70h:10m:44s remains)
INFO - root - 2017-12-05 23:24:24.023772: step 49750, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 73h:28m:42s remains)
INFO - root - 2017-12-05 23:24:33.163058: step 49760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 67h:44m:40s remains)
INFO - root - 2017-12-05 23:24:42.454313: step 49770, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 71h:50m:23s remains)
INFO - root - 2017-12-05 23:24:51.580365: step 49780, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.899 sec/batch; 70h:35m:13s remains)
INFO - root - 2017-12-05 23:25:00.823773: step 49790, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 70h:45m:44s remains)
INFO - root - 2017-12-05 23:25:10.171188: step 49800, loss = 2.07, batch loss = 2.01 (8.3 examples/sec; 0.958 sec/batch; 75h:14m:27s remains)
2017-12-05 23:25:10.950652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21749 -4.22365 -4.21048 -4.1854215 -4.1610413 -4.1319695 -4.1111465 -4.1058054 -4.1155329 -4.1469922 -4.1911426 -4.2277675 -4.2564373 -4.2821555 -4.3049607][-4.1861296 -4.1940932 -4.1816392 -4.1616249 -4.1387315 -4.1005387 -4.0688663 -4.0583515 -4.0644093 -4.1016049 -4.1609235 -4.2088871 -4.2433858 -4.2769909 -4.3056393][-4.1527929 -4.1653972 -4.1567578 -4.1440215 -4.1258249 -4.0811734 -4.0382409 -4.0241065 -4.0225005 -4.0573368 -4.1274686 -4.1864305 -4.227582 -4.2701688 -4.3061705][-4.1261067 -4.14103 -4.1396451 -4.1373825 -4.1216354 -4.0701923 -4.0125794 -3.996201 -3.9924486 -4.0227895 -4.0974422 -4.1637444 -4.2127929 -4.263164 -4.3078218][-4.0926523 -4.107089 -4.1177244 -4.1281176 -4.11033 -4.0470209 -3.9715517 -3.9588568 -3.9722826 -4.0096989 -4.0850234 -4.153039 -4.2062249 -4.25924 -4.3101573][-4.0447578 -4.0618544 -4.0896635 -4.1134858 -4.0892873 -4.0078349 -3.9037471 -3.8937938 -3.9445133 -4.008513 -4.0902925 -4.1586995 -4.2127266 -4.2607737 -4.3123484][-3.970691 -4.0003209 -4.0497627 -4.084795 -4.0542126 -3.9538438 -3.8102269 -3.7899761 -3.8914032 -4.0026474 -4.1026578 -4.1762362 -4.229053 -4.267405 -4.3142405][-3.9057777 -3.9382968 -4.0063229 -4.0483966 -4.0156484 -3.9019675 -3.7173572 -3.6695552 -3.8185985 -3.9792533 -4.10294 -4.1882792 -4.2436175 -4.2750807 -4.3137765][-3.9153464 -3.9341478 -3.9978147 -4.0414872 -4.0173478 -3.9166038 -3.7392945 -3.6665463 -3.8066554 -3.9779067 -4.1092172 -4.1983876 -4.2571363 -4.286643 -4.3153114][-3.9939022 -3.9996777 -4.0412145 -4.0753889 -4.0607386 -3.9904022 -3.8618064 -3.7947509 -3.8779027 -4.0134959 -4.130486 -4.2155795 -4.2746887 -4.3041897 -4.3227777][-4.0775056 -4.0793362 -4.1004195 -4.1214004 -4.1136937 -4.0718579 -3.9901106 -3.9404981 -3.9824157 -4.0687919 -4.1579113 -4.2359071 -4.2935276 -4.3208342 -4.332108][-4.1570826 -4.1581774 -4.1691022 -4.1770978 -4.1767411 -4.1578817 -4.1102266 -4.0766282 -4.0939083 -4.1369066 -4.1915069 -4.2565856 -4.3102584 -4.3333759 -4.3388863][-4.2203922 -4.2198062 -4.2274365 -4.233551 -4.2394676 -4.2366405 -4.2071385 -4.1783938 -4.1813469 -4.1994147 -4.2294321 -4.2794104 -4.322701 -4.340106 -4.3416681][-4.2605438 -4.2567267 -4.2591848 -4.2655239 -4.277667 -4.2803545 -4.2595544 -4.2320538 -4.2265763 -4.2357869 -4.2561035 -4.2943912 -4.3276496 -4.3406773 -4.3406739][-4.2792234 -4.2755356 -4.2721081 -4.27835 -4.293221 -4.2966018 -4.2817492 -4.258883 -4.250567 -4.2563548 -4.2738962 -4.3037381 -4.3287034 -4.3381782 -4.3374944]]...]
INFO - root - 2017-12-05 23:25:20.025090: step 49810, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.914 sec/batch; 71h:44m:47s remains)
INFO - root - 2017-12-05 23:25:29.187081: step 49820, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 72h:52m:47s remains)
INFO - root - 2017-12-05 23:25:38.399046: step 49830, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.960 sec/batch; 75h:24m:05s remains)
INFO - root - 2017-12-05 23:25:47.474595: step 49840, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 67h:59m:42s remains)
INFO - root - 2017-12-05 23:25:56.560493: step 49850, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 71h:41m:53s remains)
INFO - root - 2017-12-05 23:26:05.615955: step 49860, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 69h:31m:30s remains)
INFO - root - 2017-12-05 23:26:14.723290: step 49870, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 73h:05m:45s remains)
INFO - root - 2017-12-05 23:26:23.924758: step 49880, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 72h:13m:34s remains)
INFO - root - 2017-12-05 23:26:32.996857: step 49890, loss = 2.03, batch loss = 1.97 (8.5 examples/sec; 0.937 sec/batch; 73h:31m:53s remains)
INFO - root - 2017-12-05 23:26:42.142111: step 49900, loss = 2.04, batch loss = 1.98 (8.3 examples/sec; 0.961 sec/batch; 75h:27m:36s remains)
2017-12-05 23:26:42.958781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1632414 -4.1628284 -4.1838994 -4.2094083 -4.2317462 -4.2467117 -4.2598033 -4.2707429 -4.256937 -4.2234664 -4.1690149 -4.1211042 -4.0842104 -4.0641704 -4.0656357][-4.150126 -4.1542735 -4.1914234 -4.2389407 -4.2711306 -4.2790451 -4.2765589 -4.2628188 -4.2245388 -4.1744556 -4.1208844 -4.093523 -4.07641 -4.0748005 -4.0931425][-4.1487064 -4.1653223 -4.2142916 -4.2669568 -4.2943473 -4.2879725 -4.2641826 -4.2234826 -4.172205 -4.1281123 -4.0962448 -4.0888848 -4.0834956 -4.0897932 -4.1151209][-4.1708579 -4.2033319 -4.2535658 -4.2921462 -4.2984648 -4.2692337 -4.2213612 -4.1625266 -4.1139541 -4.0983224 -4.0951252 -4.1014605 -4.1028094 -4.1132793 -4.1349726][-4.2077708 -4.2500553 -4.2879133 -4.3017945 -4.2791867 -4.21777 -4.1376657 -4.0647779 -4.0411057 -4.0698133 -4.1029363 -4.1293368 -4.1388197 -4.1520729 -4.1662145][-4.244885 -4.2856712 -4.3052077 -4.2923942 -4.2344441 -4.1261554 -3.9961767 -3.9097784 -3.9381759 -4.0323653 -4.11345 -4.1608982 -4.181273 -4.19619 -4.2008185][-4.2791791 -4.3097725 -4.3083949 -4.2667394 -4.1684537 -4.0060191 -3.8228602 -3.7367692 -3.8420949 -4.0084944 -4.1307921 -4.1910024 -4.2169223 -4.2266612 -4.2211723][-4.3024712 -4.32047 -4.3010778 -4.2359743 -4.1133747 -3.9280274 -3.7405431 -3.7007217 -3.8585277 -4.04634 -4.1721807 -4.2306938 -4.2536273 -4.25401 -4.2395048][-4.308166 -4.3161855 -4.2856431 -4.2141166 -4.0949383 -3.9411514 -3.8165786 -3.830791 -3.9704096 -4.1228642 -4.2248311 -4.2740674 -4.2898641 -4.2817159 -4.2626443][-4.3000717 -4.3032708 -4.2732782 -4.210422 -4.1168456 -4.0192733 -3.9630034 -3.9999108 -4.09999 -4.2027068 -4.2711549 -4.3066721 -4.314744 -4.3037772 -4.2845526][-4.2924089 -4.2939391 -4.2706718 -4.2239504 -4.163692 -4.1158681 -4.1059813 -4.1503568 -4.2167854 -4.2743406 -4.3084841 -4.3266273 -4.3272548 -4.314177 -4.2983766][-4.2916627 -4.2946534 -4.2809243 -4.2504683 -4.2178493 -4.2039175 -4.2185965 -4.2600093 -4.3012848 -4.3255653 -4.3339725 -4.333385 -4.3234849 -4.3103104 -4.2998147][-4.2925949 -4.2979388 -4.2918711 -4.2749252 -4.2620864 -4.2674255 -4.2905188 -4.32276 -4.3443551 -4.3477073 -4.3388662 -4.324357 -4.3087215 -4.2982688 -4.2940822][-4.2841253 -4.2887373 -4.2872767 -4.2788134 -4.2774587 -4.2915444 -4.316546 -4.3385715 -4.3453255 -4.3327427 -4.3113208 -4.2922459 -4.2802906 -4.2762418 -4.2766232][-4.2568769 -4.2593961 -4.2566948 -4.2507157 -4.2549777 -4.273294 -4.2987947 -4.3137484 -4.3094525 -4.2874079 -4.2604489 -4.24176 -4.2365627 -4.2382393 -4.2401638]]...]
INFO - root - 2017-12-05 23:26:52.111329: step 49910, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.911 sec/batch; 71h:29m:48s remains)
INFO - root - 2017-12-05 23:27:01.105592: step 49920, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 71h:55m:36s remains)
INFO - root - 2017-12-05 23:27:10.232353: step 49930, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 70h:48m:33s remains)
INFO - root - 2017-12-05 23:27:19.315370: step 49940, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 71h:54m:32s remains)
INFO - root - 2017-12-05 23:27:28.502450: step 49950, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 70h:39m:05s remains)
INFO - root - 2017-12-05 23:27:37.587796: step 49960, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 71h:25m:46s remains)
INFO - root - 2017-12-05 23:27:46.600867: step 49970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 71h:33m:10s remains)
INFO - root - 2017-12-05 23:27:55.799218: step 49980, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 71h:59m:39s remains)
INFO - root - 2017-12-05 23:28:05.105694: step 49990, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 68h:51m:57s remains)
INFO - root - 2017-12-05 23:28:14.345396: step 50000, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 71h:26m:59s remains)
2017-12-05 23:28:15.132878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3305092 -4.3168764 -4.299932 -4.2822189 -4.2623219 -4.2421241 -4.2256637 -4.2267275 -4.23878 -4.2483296 -4.2547626 -4.2676854 -4.2757106 -4.2769833 -4.2850714][-4.3231144 -4.30567 -4.2833986 -4.2594609 -4.232923 -4.2046061 -4.1794043 -4.175828 -4.1913157 -4.2041044 -4.2125206 -4.2275677 -4.2372141 -4.2377357 -4.2476296][-4.3165379 -4.2961578 -4.2676969 -4.2370195 -4.2035394 -4.1632891 -4.1246572 -4.1091361 -4.1258407 -4.1436968 -4.1603866 -4.1800194 -4.1958094 -4.2016149 -4.215251][-4.312232 -4.2882209 -4.2543826 -4.2181206 -4.1769309 -4.1253023 -4.0688281 -4.0404992 -4.0535417 -4.0751038 -4.0988808 -4.12959 -4.1573749 -4.1778588 -4.1990347][-4.3036094 -4.2753525 -4.2305503 -4.1834583 -4.1341467 -4.077189 -4.0116959 -3.9762187 -3.9881141 -4.0186605 -4.0502391 -4.0952182 -4.1382108 -4.1705284 -4.195857][-4.2925215 -4.2559419 -4.1949911 -4.13526 -4.0750508 -4.0102754 -3.9344261 -3.88724 -3.9088726 -3.963676 -4.0168133 -4.0845814 -4.1441894 -4.1793613 -4.2021675][-4.2830768 -4.2345543 -4.1558285 -4.0778909 -3.9980817 -3.9063578 -3.7962489 -3.7148108 -3.7705092 -3.8870325 -3.9909608 -4.0820708 -4.1464505 -4.1790566 -4.2008266][-4.2728891 -4.2102389 -4.1117015 -4.0081544 -3.8950005 -3.7549791 -3.593662 -3.4659486 -3.5794902 -3.7861431 -3.9402792 -4.0438256 -4.1079707 -4.1435957 -4.1760225][-4.26748 -4.2002549 -4.0993185 -3.985734 -3.8628969 -3.7212844 -3.5723372 -3.4626744 -3.5950251 -3.7916551 -3.9214034 -4.0006957 -4.0547552 -4.102685 -4.1492643][-4.2713919 -4.2103586 -4.1205096 -4.0161304 -3.9192126 -3.8305666 -3.7530451 -3.7116346 -3.7877152 -3.8898625 -3.9552093 -4.0045133 -4.0493574 -4.1038246 -4.1578135][-4.2858691 -4.2331042 -4.1566768 -4.0692453 -3.9991367 -3.9487863 -3.9177327 -3.9074552 -3.939281 -3.9860516 -4.0202713 -4.0533915 -4.09074 -4.1385336 -4.1853461][-4.3088565 -4.2687259 -4.2115626 -4.1534338 -4.109108 -4.0773854 -4.0595179 -4.0538 -4.0691843 -4.0990639 -4.1201448 -4.1410561 -4.1646733 -4.1919475 -4.2212391][-4.3280387 -4.3020282 -4.2609773 -4.2206311 -4.193037 -4.17234 -4.164413 -4.1645679 -4.1801753 -4.2093778 -4.226789 -4.233994 -4.24008 -4.242837 -4.2525129][-4.343327 -4.32925 -4.3026953 -4.2770562 -4.2601681 -4.2499208 -4.2524366 -4.259881 -4.2726212 -4.2921124 -4.3025832 -4.3009272 -4.2943044 -4.2881083 -4.2912703][-4.3524246 -4.3457351 -4.3308592 -4.3163729 -4.3072309 -4.3043895 -4.3120065 -4.3204174 -4.3277678 -4.3366814 -4.3397255 -4.3363538 -4.3281708 -4.3219228 -4.3234968]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 23:28:24.895622: step 50010, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 69h:30m:49s remains)
INFO - root - 2017-12-05 23:28:34.071057: step 50020, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 70h:06m:13s remains)
INFO - root - 2017-12-05 23:28:43.183683: step 50030, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 72h:15m:02s remains)
INFO - root - 2017-12-05 23:28:52.231047: step 50040, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 71h:05m:05s remains)
INFO - root - 2017-12-05 23:29:01.467032: step 50050, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 72h:02m:52s remains)
INFO - root - 2017-12-05 23:29:10.711481: step 50060, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 71h:56m:16s remains)
INFO - root - 2017-12-05 23:29:19.788844: step 50070, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 72h:33m:24s remains)
INFO - root - 2017-12-05 23:29:28.816218: step 50080, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 71h:03m:33s remains)
INFO - root - 2017-12-05 23:29:37.909468: step 50090, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 69h:31m:57s remains)
INFO - root - 2017-12-05 23:29:47.076619: step 50100, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.915 sec/batch; 71h:48m:44s remains)
2017-12-05 23:29:47.841141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1336241 -4.1584778 -4.1982269 -4.2282643 -4.2198348 -4.1733456 -4.1132612 -4.0778432 -4.0887947 -4.12331 -4.1543121 -4.1705537 -4.1626039 -4.1616945 -4.16986][-4.1468525 -4.1834278 -4.2206769 -4.2394056 -4.223743 -4.1723609 -4.1163516 -4.0882015 -4.1017623 -4.1280346 -4.1478195 -4.1627083 -4.159399 -4.1658487 -4.1790824][-4.1507435 -4.1931405 -4.2270603 -4.2321935 -4.2069058 -4.1541476 -4.1040349 -4.0807886 -4.0955377 -4.1197953 -4.13697 -4.1562152 -4.1607666 -4.1705856 -4.18242][-4.155859 -4.198957 -4.2272077 -4.2233577 -4.1879921 -4.1330976 -4.0861354 -4.0675855 -4.0863447 -4.1142421 -4.1329322 -4.1581712 -4.1706719 -4.1823845 -4.1910577][-4.1568913 -4.1976357 -4.2191873 -4.20907 -4.1738081 -4.1237273 -4.0811572 -4.068109 -4.0900526 -4.1163869 -4.1349998 -4.1618338 -4.1814981 -4.1995025 -4.2100239][-4.1349955 -4.1742239 -4.195107 -4.1868372 -4.1594033 -4.1204863 -4.0859241 -4.0837054 -4.1109061 -4.1373129 -4.1586285 -4.1830926 -4.203383 -4.22107 -4.229949][-4.0976834 -4.136457 -4.1619229 -4.1591349 -4.1382961 -4.1046739 -4.0703554 -4.078877 -4.1153178 -4.1492615 -4.1783133 -4.2036252 -4.2217741 -4.2334175 -4.2373986][-4.0494294 -4.091897 -4.12444 -4.1293159 -4.1107459 -4.0721617 -4.0299587 -4.0494323 -4.0975723 -4.1435819 -4.1799965 -4.206358 -4.2218585 -4.2291751 -4.230864][-4.0264959 -4.0669184 -4.1004176 -4.1091757 -4.0900126 -4.048377 -4.007957 -4.0389957 -4.0982156 -4.1554313 -4.1950116 -4.2185769 -4.2315974 -4.2362275 -4.2294126][-4.0697436 -4.0960555 -4.1184807 -4.125246 -4.10554 -4.0657263 -4.033608 -4.0622163 -4.1197448 -4.1809349 -4.2191949 -4.238122 -4.2516551 -4.2561617 -4.2467427][-4.1280847 -4.1467896 -4.1619735 -4.1680565 -4.1548414 -4.1264586 -4.1065707 -4.1270881 -4.1697874 -4.2200427 -4.2543287 -4.2695403 -4.2804937 -4.2817016 -4.2733326][-4.1770372 -4.1917748 -4.1993818 -4.2104549 -4.2114258 -4.1970587 -4.1831827 -4.1944957 -4.2189722 -4.2509227 -4.27686 -4.2923861 -4.3016686 -4.297946 -4.2898784][-4.2275996 -4.2365255 -4.2297668 -4.2389851 -4.2496967 -4.2426958 -4.231288 -4.2385855 -4.2485604 -4.2637076 -4.2832074 -4.3043756 -4.3197551 -4.3170972 -4.3095384][-4.2580957 -4.2615175 -4.2445846 -4.24658 -4.2592859 -4.2556038 -4.2467513 -4.2524748 -4.257092 -4.2639089 -4.27944 -4.3029609 -4.3215241 -4.3253574 -4.3226023][-4.2648983 -4.2634678 -4.2407813 -4.2374516 -4.2535753 -4.2561178 -4.2502232 -4.254168 -4.256505 -4.2605457 -4.2757916 -4.2941041 -4.307992 -4.3162975 -4.3195934]]...]
INFO - root - 2017-12-05 23:29:56.803183: step 50110, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 71h:34m:05s remains)
INFO - root - 2017-12-05 23:30:06.068474: step 50120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 71h:23m:18s remains)
INFO - root - 2017-12-05 23:30:15.108436: step 50130, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.959 sec/batch; 75h:12m:37s remains)
INFO - root - 2017-12-05 23:30:24.158937: step 50140, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 71h:39m:37s remains)
INFO - root - 2017-12-05 23:30:33.300266: step 50150, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.944 sec/batch; 74h:04m:36s remains)
INFO - root - 2017-12-05 23:30:42.501533: step 50160, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.967 sec/batch; 75h:50m:02s remains)
INFO - root - 2017-12-05 23:30:51.527478: step 50170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 66h:59m:06s remains)
INFO - root - 2017-12-05 23:31:00.469388: step 50180, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:00m:52s remains)
INFO - root - 2017-12-05 23:31:09.657211: step 50190, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 72h:02m:20s remains)
INFO - root - 2017-12-05 23:31:18.818679: step 50200, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 70h:56m:03s remains)
2017-12-05 23:31:19.610245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3180485 -4.318522 -4.3242459 -4.3324947 -4.3381844 -4.3385415 -4.3378024 -4.3362036 -4.3335733 -4.3295765 -4.3216424 -4.3159161 -4.3127265 -4.3065557 -4.3022885][-4.2878723 -4.2860808 -4.2972846 -4.3138204 -4.3245392 -4.3250251 -4.3215356 -4.3168116 -4.3116956 -4.3045917 -4.2896595 -4.2760448 -4.2686672 -4.263155 -4.2638836][-4.2551165 -4.242897 -4.2538838 -4.2791023 -4.29528 -4.2952638 -4.2894773 -4.2792168 -4.2709684 -4.264504 -4.2494478 -4.2323093 -4.2250185 -4.2244067 -4.232832][-4.2254944 -4.2032065 -4.2081461 -4.2369728 -4.2573171 -4.2557983 -4.2452774 -4.2265844 -4.2153025 -4.21561 -4.2064147 -4.1920033 -4.1910591 -4.19593 -4.209239][-4.1864214 -4.1536822 -4.1513529 -4.1802335 -4.2005439 -4.1919289 -4.1659994 -4.1330414 -4.125803 -4.1431355 -4.1468863 -4.1403103 -4.1478944 -4.1554327 -4.1743832][-4.1438384 -4.1032629 -4.0913196 -4.1133485 -4.1272607 -4.1045828 -4.0459037 -3.991657 -4.0042343 -4.0548038 -4.0784259 -4.0873547 -4.1051335 -4.1224456 -4.1567392][-4.1165113 -4.0685892 -4.0452642 -4.0494137 -4.0458093 -3.9972045 -3.894465 -3.8171122 -3.8614168 -3.9575038 -4.006587 -4.0352869 -4.0676293 -4.1040893 -4.1560507][-4.103929 -4.0538688 -4.0239239 -4.0189147 -4.0050974 -3.9352093 -3.8079557 -3.7294788 -3.8054435 -3.9235373 -3.9834728 -4.0246296 -4.0699482 -4.118916 -4.1737185][-4.1078467 -4.0677028 -4.049592 -4.0574245 -4.0536284 -3.9953439 -3.9040105 -3.8650265 -3.9295893 -4.0062919 -4.0372095 -4.0665646 -4.1098642 -4.1580939 -4.20312][-4.1337051 -4.1022615 -4.0931277 -4.110549 -4.1186204 -4.0835571 -4.0358772 -4.0273185 -4.0659986 -4.0992837 -4.1044865 -4.1167769 -4.1523485 -4.1968203 -4.2353196][-4.1588521 -4.1336164 -4.130506 -4.1538763 -4.1708636 -4.1592894 -4.1440625 -4.1462021 -4.1684813 -4.1785831 -4.1691213 -4.168644 -4.1913338 -4.2249842 -4.2558379][-4.1907387 -4.1687188 -4.1666861 -4.1914635 -4.2105083 -4.2067504 -4.2044182 -4.2112484 -4.2254448 -4.2251844 -4.2094626 -4.2062578 -4.2238107 -4.2484407 -4.2694855][-4.2265563 -4.2091956 -4.20775 -4.227304 -4.2392006 -4.2346482 -4.2343154 -4.2442551 -4.2560673 -4.2496643 -4.2308674 -4.230474 -4.247355 -4.2661982 -4.2811623][-4.2507744 -4.2386837 -4.2387729 -4.253109 -4.2590628 -4.2548194 -4.2547112 -4.2642913 -4.2701263 -4.2582674 -4.2413025 -4.2440319 -4.260303 -4.2764769 -4.2885766][-4.2630272 -4.2534528 -4.2530046 -4.2618828 -4.2649136 -4.26115 -4.261097 -4.269063 -4.2736611 -4.263567 -4.2512441 -4.2539015 -4.2663 -4.2812395 -4.2936368]]...]
INFO - root - 2017-12-05 23:31:28.585944: step 50210, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 69h:25m:51s remains)
INFO - root - 2017-12-05 23:31:37.621473: step 50220, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.944 sec/batch; 73h:59m:23s remains)
INFO - root - 2017-12-05 23:31:46.772635: step 50230, loss = 2.02, batch loss = 1.96 (8.7 examples/sec; 0.922 sec/batch; 72h:17m:03s remains)
INFO - root - 2017-12-05 23:31:55.921783: step 50240, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 71h:51m:47s remains)
INFO - root - 2017-12-05 23:32:05.125867: step 50250, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 71h:27m:26s remains)
INFO - root - 2017-12-05 23:32:14.301880: step 50260, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 70h:02m:47s remains)
INFO - root - 2017-12-05 23:32:23.290834: step 50270, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 71h:17m:23s remains)
INFO - root - 2017-12-05 23:32:32.155268: step 50280, loss = 2.02, batch loss = 1.96 (9.0 examples/sec; 0.887 sec/batch; 69h:33m:20s remains)
INFO - root - 2017-12-05 23:32:41.379658: step 50290, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 71h:50m:18s remains)
INFO - root - 2017-12-05 23:32:50.460325: step 50300, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 71h:18m:14s remains)
2017-12-05 23:32:51.279632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1459727 -4.160079 -4.1693487 -4.1722956 -4.1634741 -4.1546779 -4.1486673 -4.1465082 -4.1541185 -4.1738863 -4.1973691 -4.2060428 -4.2118912 -4.2148328 -4.2166462][-4.1187897 -4.1359658 -4.1428194 -4.1412749 -4.1262374 -4.1090817 -4.0979328 -4.0940852 -4.1028557 -4.129467 -4.1659431 -4.1857591 -4.1992579 -4.2042952 -4.2023635][-4.0955119 -4.1202579 -4.132318 -4.1318331 -4.1107464 -4.0818834 -4.0645881 -4.05903 -4.0671821 -4.0982671 -4.1439 -4.1722245 -4.1933 -4.1983395 -4.190218][-4.0800133 -4.1160254 -4.1335564 -4.1330023 -4.1077366 -4.069067 -4.038969 -4.0257778 -4.0289721 -4.0630174 -4.1169267 -4.1552858 -4.1816382 -4.1873021 -4.1806355][-4.0716467 -4.1108704 -4.1281338 -4.1286983 -4.1022925 -4.0545216 -4.0067096 -3.9720585 -3.9608219 -4.0054049 -4.0783987 -4.1334567 -4.1718178 -4.1828432 -4.18135][-4.0683866 -4.1020241 -4.1131711 -4.1116657 -4.0900884 -4.038475 -3.9619355 -3.8823102 -3.8507566 -3.9232259 -4.03152 -4.1121073 -4.1645179 -4.18338 -4.1881671][-4.064909 -4.0831962 -4.0771837 -4.0672679 -4.0497856 -3.9916997 -3.885668 -3.7554169 -3.7042379 -3.8292127 -3.9894018 -4.0960374 -4.1574092 -4.1793032 -4.1898513][-4.0886254 -4.093555 -4.0705528 -4.0490913 -4.026762 -3.966677 -3.8571138 -3.7155371 -3.6742294 -3.8309202 -4.0058455 -4.1127253 -4.161521 -4.1765466 -4.1880107][-4.1286244 -4.1292844 -4.1036372 -4.0724173 -4.0477886 -4.0012622 -3.9260483 -3.832593 -3.824594 -3.9396334 -4.0667782 -4.14583 -4.1748538 -4.18192 -4.1939864][-4.1763635 -4.1820126 -4.1546364 -4.1148715 -4.0857911 -4.0589056 -4.0230875 -3.9772491 -3.9872713 -4.0579095 -4.1316609 -4.1791749 -4.1937327 -4.1946769 -4.2015524][-4.2126579 -4.223484 -4.2002759 -4.1609011 -4.1335711 -4.123312 -4.1134515 -4.0919676 -4.0997295 -4.1382174 -4.1817622 -4.2132421 -4.2164278 -4.2082624 -4.20514][-4.2235804 -4.2361045 -4.2201867 -4.1908565 -4.1720533 -4.1706247 -4.1714497 -4.1597171 -4.1602187 -4.1791592 -4.2094746 -4.2307363 -4.2250452 -4.2135344 -4.2043667][-4.2333732 -4.2364359 -4.2233043 -4.2039075 -4.1932759 -4.1961513 -4.2001829 -4.193285 -4.1908875 -4.2038403 -4.2272844 -4.2405434 -4.2331266 -4.2217011 -4.2087684][-4.2527857 -4.2457161 -4.2324305 -4.2197347 -4.2146683 -4.2189059 -4.2221971 -4.2178054 -4.2142811 -4.2215543 -4.2376051 -4.2480745 -4.2437086 -4.2344637 -4.2213464][-4.2735009 -4.2622075 -4.2499442 -4.2426319 -4.2397094 -4.2411332 -4.2425103 -4.24067 -4.2367229 -4.2372293 -4.2448192 -4.2517037 -4.2518034 -4.2477684 -4.2399411]]...]
INFO - root - 2017-12-05 23:33:00.432071: step 50310, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.959 sec/batch; 75h:11m:47s remains)
INFO - root - 2017-12-05 23:33:09.682537: step 50320, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 69h:48m:54s remains)
INFO - root - 2017-12-05 23:33:18.921948: step 50330, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 73h:02m:25s remains)
INFO - root - 2017-12-05 23:33:27.978461: step 50340, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:42m:53s remains)
INFO - root - 2017-12-05 23:33:37.083873: step 50350, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 69h:44m:26s remains)
INFO - root - 2017-12-05 23:33:46.208832: step 50360, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.941 sec/batch; 73h:46m:53s remains)
INFO - root - 2017-12-05 23:33:55.311770: step 50370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 73h:10m:47s remains)
INFO - root - 2017-12-05 23:34:04.381407: step 50380, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 73h:36m:58s remains)
INFO - root - 2017-12-05 23:34:13.602080: step 50390, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 70h:00m:11s remains)
INFO - root - 2017-12-05 23:34:22.599515: step 50400, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 71h:44m:45s remains)
2017-12-05 23:34:23.379795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1066275 -4.11817 -4.1195951 -4.117157 -4.1189427 -4.1247849 -4.1281271 -4.1308694 -4.1345458 -4.1455522 -4.1584125 -4.1629076 -4.155046 -4.1393714 -4.1222935][-4.1137962 -4.1257391 -4.130919 -4.1366758 -4.1431355 -4.1487279 -4.1511707 -4.1545477 -4.1604867 -4.1715817 -4.1823916 -4.1861711 -4.1791058 -4.1631236 -4.1435623][-4.1299944 -4.1457939 -4.1557083 -4.1675687 -4.1817336 -4.1907353 -4.1943841 -4.2001252 -4.2076454 -4.2169447 -4.2253127 -4.2270966 -4.2187448 -4.1991158 -4.1748991][-4.1380191 -4.1549487 -4.1650558 -4.1808648 -4.2044988 -4.2186966 -4.2200727 -4.2204542 -4.2259026 -4.2375731 -4.2507391 -4.2571368 -4.2513466 -4.2320161 -4.2045746][-4.1339049 -4.15002 -4.1604753 -4.1758652 -4.1962423 -4.2019844 -4.18952 -4.1809783 -4.1900873 -4.2153978 -4.241797 -4.2605338 -4.2641459 -4.2509179 -4.2229347][-4.136292 -4.1522546 -4.1600552 -4.1641684 -4.1648293 -4.1445971 -4.1077065 -4.0863643 -4.1016135 -4.1488447 -4.1967216 -4.2293692 -4.24696 -4.2472787 -4.2273078][-4.1637287 -4.177639 -4.1770577 -4.1575947 -4.1250639 -4.0701747 -3.9991255 -3.9499333 -3.9669569 -4.0457253 -4.1281514 -4.1836543 -4.2152457 -4.2278752 -4.2206907][-4.1951928 -4.2048235 -4.1948867 -4.15536 -4.0934477 -4.0062962 -3.8926332 -3.7924843 -3.7962701 -3.9146421 -4.0405354 -4.1227617 -4.1721053 -4.2005706 -4.20743][-4.2025776 -4.2088413 -4.199481 -4.1640625 -4.1003084 -4.0068269 -3.8813243 -3.7592373 -3.7456894 -3.8693042 -4.0041847 -4.093236 -4.1525297 -4.190948 -4.20823][-4.1926379 -4.1961837 -4.1957431 -4.1805887 -4.141542 -4.0713682 -3.9802809 -3.8969791 -3.8764687 -3.9444761 -4.036314 -4.1062856 -4.160089 -4.197916 -4.2166972][-4.1860371 -4.1859713 -4.1903386 -4.1916742 -4.1775241 -4.1400104 -4.0880036 -4.0376792 -4.0113883 -4.0318022 -4.079113 -4.125432 -4.166707 -4.1944084 -4.2079339][-4.1749034 -4.1736212 -4.1805358 -4.19189 -4.1963105 -4.1821995 -4.160718 -4.1334062 -4.1049137 -4.0987134 -4.1179667 -4.1476221 -4.1733427 -4.1878815 -4.1910472][-4.1565804 -4.1593375 -4.1727924 -4.1922021 -4.2074761 -4.2049875 -4.1978316 -4.1865444 -4.16467 -4.145463 -4.1453929 -4.1616073 -4.1742263 -4.1788754 -4.1735945][-4.1402225 -4.1543813 -4.1785951 -4.2030807 -4.2180104 -4.218204 -4.21233 -4.2049875 -4.1866579 -4.159595 -4.1430678 -4.1451097 -4.155602 -4.1656661 -4.1606092][-4.1350617 -4.1599903 -4.1920094 -4.214426 -4.221065 -4.2153554 -4.2051659 -4.196321 -4.1834373 -4.15762 -4.1366796 -4.1278138 -4.1369019 -4.1537905 -4.1559868]]...]
INFO - root - 2017-12-05 23:34:32.387468: step 50410, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 71h:23m:26s remains)
INFO - root - 2017-12-05 23:34:41.520996: step 50420, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.939 sec/batch; 73h:35m:23s remains)
INFO - root - 2017-12-05 23:34:50.573209: step 50430, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 72h:32m:11s remains)
INFO - root - 2017-12-05 23:34:59.780615: step 50440, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 71h:49m:30s remains)
INFO - root - 2017-12-05 23:35:09.008119: step 50450, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 67h:57m:45s remains)
INFO - root - 2017-12-05 23:35:18.133565: step 50460, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 74h:55m:27s remains)
INFO - root - 2017-12-05 23:35:27.202638: step 50470, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 72h:19m:18s remains)
INFO - root - 2017-12-05 23:35:36.181581: step 50480, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 70h:38m:14s remains)
INFO - root - 2017-12-05 23:35:45.486929: step 50490, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 70h:34m:17s remains)
INFO - root - 2017-12-05 23:35:54.445268: step 50500, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 70h:44m:13s remains)
2017-12-05 23:35:55.232237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3217382 -4.3185382 -4.3174691 -4.3187914 -4.3200579 -4.3221865 -4.3260083 -4.3299 -4.3325391 -4.3349848 -4.336699 -4.337975 -4.33952 -4.3402576 -4.3408895][-4.3077545 -4.3013864 -4.2965865 -4.2942619 -4.2934136 -4.2962613 -4.3018432 -4.3084946 -4.3140907 -4.3185039 -4.3211327 -4.3235016 -4.3258953 -4.3271646 -4.3272519][-4.2916942 -4.2814631 -4.27429 -4.2676687 -4.2602739 -4.2573619 -4.2600665 -4.2721949 -4.2843103 -4.2922025 -4.2971935 -4.3020787 -4.3067784 -4.3090229 -4.3081069][-4.2685127 -4.2507992 -4.2400851 -4.2282062 -4.2118392 -4.2032242 -4.2038403 -4.225009 -4.2470326 -4.2586 -4.2654014 -4.2735896 -4.2815008 -4.2852592 -4.2853293][-4.2377343 -4.2050714 -4.1848226 -4.1647873 -4.1387596 -4.1288919 -4.1398878 -4.1746917 -4.2031116 -4.2105384 -4.2178292 -4.2343364 -4.251101 -4.2584109 -4.2622929][-4.2177429 -4.1658669 -4.1266217 -4.0848875 -4.0383244 -4.0275817 -4.0580721 -4.1127663 -4.1445832 -4.1419277 -4.145936 -4.1732149 -4.2050772 -4.2241426 -4.2380252][-4.2121215 -4.1440525 -4.0858364 -4.0252919 -3.9616876 -3.9455214 -3.9929984 -4.0637169 -4.0894861 -4.0728717 -4.0697474 -4.1072593 -4.1545277 -4.1861176 -4.21336][-4.2259207 -4.1526122 -4.0862432 -4.025423 -3.969856 -3.9480577 -3.993124 -4.0547423 -4.0586381 -4.0231872 -4.01176 -4.05563 -4.1161919 -4.1558628 -4.191956][-4.2535472 -4.1882677 -4.1242013 -4.0675383 -4.0217867 -3.9991119 -4.0334258 -4.0701818 -4.0480638 -3.995023 -3.9770949 -4.0292926 -4.1030207 -4.1467838 -4.1823139][-4.2706523 -4.2187896 -4.1645231 -4.118094 -4.0839362 -4.0652971 -4.086103 -4.0961432 -4.0539832 -3.9921808 -3.9695411 -4.0276217 -4.1119084 -4.1595287 -4.1911964][-4.2798557 -4.2431526 -4.2020197 -4.1659727 -4.1399484 -4.1269484 -4.1288171 -4.1178923 -4.0726209 -4.0172443 -3.9952316 -4.047596 -4.1298389 -4.1774697 -4.2065282][-4.2862687 -4.2624192 -4.2346234 -4.210463 -4.1940923 -4.1839733 -4.1702409 -4.1418786 -4.0972443 -4.0536332 -4.0370812 -4.0781918 -4.1466928 -4.1883459 -4.2172771][-4.2951689 -4.2776561 -4.25702 -4.2393661 -4.2257957 -4.2112489 -4.1833806 -4.1369863 -4.0857568 -4.0510936 -4.0495205 -4.0926371 -4.1565833 -4.1968565 -4.2223573][-4.3105664 -4.2963066 -4.277607 -4.2581835 -4.2405524 -4.2231874 -4.191359 -4.1392608 -4.0847545 -4.05619 -4.0660048 -4.1108265 -4.1729879 -4.2129831 -4.2322655][-4.3282485 -4.3183508 -4.3002262 -4.2777834 -4.2573519 -4.2410545 -4.2124896 -4.1678338 -4.1208873 -4.0999427 -4.1171117 -4.1582952 -4.2142782 -4.2471313 -4.2585707]]...]
INFO - root - 2017-12-05 23:36:04.505862: step 50510, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 71h:14m:49s remains)
INFO - root - 2017-12-05 23:36:13.560005: step 50520, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 70h:24m:01s remains)
INFO - root - 2017-12-05 23:36:22.596800: step 50530, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 71h:49m:43s remains)
INFO - root - 2017-12-05 23:36:31.646254: step 50540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 68h:02m:36s remains)
INFO - root - 2017-12-05 23:36:40.842020: step 50550, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 69h:25m:23s remains)
INFO - root - 2017-12-05 23:36:49.688387: step 50560, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 68h:53m:53s remains)
INFO - root - 2017-12-05 23:36:58.850306: step 50570, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.910 sec/batch; 71h:14m:11s remains)
INFO - root - 2017-12-05 23:37:08.088349: step 50580, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 70h:55m:07s remains)
INFO - root - 2017-12-05 23:37:17.191778: step 50590, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.809 sec/batch; 63h:21m:06s remains)
INFO - root - 2017-12-05 23:37:26.266102: step 50600, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 69h:52m:56s remains)
2017-12-05 23:37:27.123065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2940917 -4.3106327 -4.3136578 -4.3045516 -4.2907658 -4.2787495 -4.2796369 -4.2935324 -4.3075 -4.3182693 -4.3210826 -4.3179455 -4.3116879 -4.2999244 -4.2816792][-4.244596 -4.2567134 -4.2554412 -4.2447867 -4.235302 -4.2294803 -4.2341013 -4.2518582 -4.2686281 -4.2793174 -4.2799363 -4.2766609 -4.2727647 -4.2633157 -4.2463145][-4.2001991 -4.2046471 -4.200242 -4.1908231 -4.1875892 -4.1883016 -4.1960163 -4.2151637 -4.2325888 -4.2423449 -4.2419457 -4.2412882 -4.2442055 -4.2424212 -4.232697][-4.1821289 -4.1804395 -4.1776733 -4.1773629 -4.1837335 -4.189815 -4.1974878 -4.2119021 -4.2238822 -4.2277417 -4.2248125 -4.22803 -4.2375941 -4.2423048 -4.2374916][-4.1842794 -4.1800656 -4.1790938 -4.1847878 -4.19615 -4.2042875 -4.20978 -4.2176571 -4.2213526 -4.221725 -4.2218122 -4.227262 -4.236454 -4.2387 -4.2299128][-4.1781912 -4.1728439 -4.16889 -4.1737704 -4.184341 -4.1940079 -4.1983676 -4.1995263 -4.1936274 -4.19211 -4.1966343 -4.2010441 -4.2056541 -4.2016444 -4.1848378][-4.1292062 -4.1204371 -4.1101656 -4.1102748 -4.1189446 -4.1286821 -4.1312547 -4.1271925 -4.1136956 -4.111805 -4.1199617 -4.1228142 -4.1213078 -4.1092815 -4.0865417][-4.0389361 -4.0244088 -4.00766 -3.9989591 -4.0016537 -4.0102816 -4.0117021 -4.0048194 -3.9890852 -3.9940588 -4.011436 -4.020371 -4.0183754 -4.0012169 -3.9801466][-3.9953027 -3.9753842 -3.9517238 -3.9299662 -3.9201326 -3.9224863 -3.922471 -3.9154084 -3.8993273 -3.9097295 -3.9365339 -3.9588432 -3.9666464 -3.9556029 -3.9435456][-4.0525484 -4.0329704 -4.0086846 -3.9790614 -3.9603782 -3.9581778 -3.9582112 -3.953459 -3.941098 -3.9476635 -3.9698915 -3.9950435 -4.0102978 -4.011694 -4.0121422][-4.1595836 -4.1474476 -4.1298842 -4.1037931 -4.0859957 -4.0823112 -4.081954 -4.0787864 -4.0694857 -4.0690932 -4.0813956 -4.0993896 -4.1147208 -4.1242418 -4.1322389][-4.2283244 -4.225244 -4.2158217 -4.20162 -4.19641 -4.1988497 -4.2019238 -4.1984158 -4.1893978 -4.1843624 -4.1896567 -4.2008104 -4.2151957 -4.2294488 -4.2397718][-4.2374597 -4.2449942 -4.2437434 -4.2401419 -4.2472816 -4.2587681 -4.2667537 -4.2680736 -4.2636313 -4.2586474 -4.2609391 -4.26682 -4.2783689 -4.2920022 -4.3002086][-4.2116146 -4.2274461 -4.2333226 -4.2374158 -4.2502146 -4.2670918 -4.2800722 -4.2873535 -4.2902942 -4.2889671 -4.2883682 -4.2879782 -4.2931104 -4.3010149 -4.3040843][-4.1913619 -4.2097964 -4.2170911 -4.220902 -4.2337084 -4.2522883 -4.2689047 -4.2809978 -4.2898293 -4.2921443 -4.2885308 -4.2799511 -4.2744412 -4.2727389 -4.2705078]]...]
INFO - root - 2017-12-05 23:37:36.342069: step 50610, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 72h:21m:10s remains)
INFO - root - 2017-12-05 23:37:45.470965: step 50620, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 71h:28m:08s remains)
INFO - root - 2017-12-05 23:37:54.485734: step 50630, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.874 sec/batch; 68h:26m:00s remains)
INFO - root - 2017-12-05 23:38:03.644733: step 50640, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.959 sec/batch; 75h:04m:33s remains)
INFO - root - 2017-12-05 23:38:12.647025: step 50650, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 69h:55m:54s remains)
INFO - root - 2017-12-05 23:38:21.721161: step 50660, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 73h:10m:31s remains)
INFO - root - 2017-12-05 23:38:30.865915: step 50670, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 69h:34m:43s remains)
INFO - root - 2017-12-05 23:38:39.940478: step 50680, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 71h:24m:50s remains)
INFO - root - 2017-12-05 23:38:48.921577: step 50690, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 71h:29m:21s remains)
INFO - root - 2017-12-05 23:38:57.970875: step 50700, loss = 2.02, batch loss = 1.96 (8.7 examples/sec; 0.924 sec/batch; 72h:18m:55s remains)
2017-12-05 23:38:58.773524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0556121 -4.0921545 -4.1300645 -4.1619182 -4.1832504 -4.196867 -4.1958628 -4.1822124 -4.1801834 -4.1871996 -4.1947708 -4.1936827 -4.1832752 -4.15532 -4.1069345][-4.0778093 -4.0938706 -4.1070209 -4.1181526 -4.1306543 -4.1431684 -4.1454372 -4.1382523 -4.1443286 -4.1561284 -4.17124 -4.1790247 -4.1744871 -4.1442547 -4.0908041][-4.1012249 -4.0968657 -4.0914235 -4.0908785 -4.0959754 -4.0988359 -4.0908136 -4.0848551 -4.0990324 -4.1145611 -4.1343589 -4.1535339 -4.1629109 -4.1448088 -4.1023922][-4.13068 -4.1192465 -4.1088977 -4.1040359 -4.0983911 -4.083694 -4.0565624 -4.040875 -4.0545664 -4.0746555 -4.1043744 -4.1375747 -4.1665 -4.1713119 -4.1485391][-4.1691656 -4.1636534 -4.1563554 -4.1482897 -4.1299191 -4.099093 -4.0555463 -4.0257349 -4.0299053 -4.0530605 -4.0916891 -4.1356559 -4.1762595 -4.1939454 -4.1858759][-4.2020955 -4.2046628 -4.1984377 -4.1870213 -4.1624432 -4.122798 -4.0699277 -4.0272055 -4.0177946 -4.0393062 -4.0752664 -4.1210251 -4.1641951 -4.1880703 -4.192852][-4.2145324 -4.2215562 -4.217351 -4.2052402 -4.1796212 -4.1391745 -4.086174 -4.0354943 -4.012917 -4.0253167 -4.05529 -4.1021752 -4.1512003 -4.1819034 -4.1951504][-4.2187691 -4.2275929 -4.2236447 -4.2104459 -4.1866851 -4.1491523 -4.1027174 -4.0572076 -4.0305891 -4.034337 -4.0602846 -4.1103125 -4.164104 -4.19784 -4.2133713][-4.2268734 -4.2346859 -4.2297068 -4.2142205 -4.189477 -4.1537633 -4.1129236 -4.0789671 -4.0595565 -4.063107 -4.0897894 -4.137547 -4.1860747 -4.2165451 -4.2305808][-4.2330704 -4.2381592 -4.231112 -4.211709 -4.179894 -4.1396585 -4.1007686 -4.0761237 -4.0682011 -4.0779147 -4.107172 -4.1503606 -4.1895881 -4.2154689 -4.2287121][-4.2237525 -4.2247391 -4.2143326 -4.1907611 -4.1535015 -4.1106176 -4.0753632 -4.0575976 -4.0570755 -4.072227 -4.1018677 -4.140151 -4.171576 -4.1950192 -4.2108483][-4.199285 -4.1984944 -4.1855965 -4.1616716 -4.12539 -4.0837483 -4.0514503 -4.0353746 -4.038044 -4.056263 -4.0870457 -4.1207504 -4.1451097 -4.1660666 -4.183682][-4.1690984 -4.1691365 -4.1556792 -4.13308 -4.1001949 -4.0615025 -4.0298762 -4.0162015 -4.0260816 -4.0500021 -4.07998 -4.1064916 -4.1234379 -4.1393332 -4.15431][-4.1455569 -4.1475425 -4.1366458 -4.1178675 -4.09201 -4.0603046 -4.0339031 -4.0277433 -4.0456586 -4.0712161 -4.09344 -4.108386 -4.1166425 -4.1254206 -4.135952][-4.1300931 -4.1357512 -4.1316881 -4.121748 -4.1063881 -4.0864267 -4.0691242 -4.0679355 -4.0854826 -4.1037064 -4.11346 -4.1183882 -4.1201029 -4.1225514 -4.1282468]]...]
INFO - root - 2017-12-05 23:39:07.929534: step 50710, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 70h:33m:33s remains)
INFO - root - 2017-12-05 23:39:16.960918: step 50720, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 72h:28m:03s remains)
INFO - root - 2017-12-05 23:39:26.132028: step 50730, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 72h:23m:27s remains)
INFO - root - 2017-12-05 23:39:35.291535: step 50740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 66h:29m:03s remains)
INFO - root - 2017-12-05 23:39:44.235671: step 50750, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 70h:30m:15s remains)
INFO - root - 2017-12-05 23:39:53.306374: step 50760, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 70h:33m:06s remains)
INFO - root - 2017-12-05 23:40:02.360031: step 50770, loss = 2.04, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 71h:29m:36s remains)
INFO - root - 2017-12-05 23:40:11.334634: step 50780, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.915 sec/batch; 71h:37m:30s remains)
INFO - root - 2017-12-05 23:40:20.379731: step 50790, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 71h:54m:01s remains)
INFO - root - 2017-12-05 23:40:29.620567: step 50800, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 74h:00m:29s remains)
2017-12-05 23:40:30.413173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1930742 -4.2037067 -4.2187958 -4.2298536 -4.2333856 -4.2266779 -4.2148814 -4.2087059 -4.2142763 -4.2235427 -4.2356067 -4.2496495 -4.2619128 -4.2696266 -4.2705231][-4.1319857 -4.140274 -4.1525292 -4.1597533 -4.1590848 -4.1493921 -4.1385174 -4.1390066 -4.1550536 -4.1751294 -4.1970143 -4.2165937 -4.2363687 -4.2533169 -4.2618032][-4.0775313 -4.0765066 -4.0836339 -4.088357 -4.0874019 -4.0792565 -4.0739765 -4.0835938 -4.1111908 -4.1437035 -4.1760788 -4.1998949 -4.2217703 -4.2449746 -4.2591248][-4.0592008 -4.0449243 -4.0452724 -4.0483608 -4.048749 -4.0434847 -4.0429387 -4.057869 -4.0902138 -4.1298461 -4.1713028 -4.2027845 -4.2241855 -4.2478738 -4.2615595][-4.0803928 -4.0628304 -4.0619125 -4.0662441 -4.0687962 -4.0638413 -4.058835 -4.0649905 -4.0827279 -4.1148963 -4.1586595 -4.198225 -4.2246356 -4.2519879 -4.2647138][-4.1076088 -4.0980706 -4.1007113 -4.1077533 -4.114212 -4.1119146 -4.0963616 -4.0818753 -4.0799875 -4.1020579 -4.1413155 -4.1823945 -4.2163877 -4.2495461 -4.2648063][-4.1035814 -4.1018462 -4.1097279 -4.1167841 -4.1219773 -4.118598 -4.0944362 -4.0628781 -4.0537634 -4.0735822 -4.1141477 -4.1590185 -4.1966496 -4.2354436 -4.2578969][-4.0761967 -4.0654621 -4.0656576 -4.0669565 -4.0648313 -4.0539389 -4.0207906 -3.9846437 -3.9827247 -4.0118957 -4.0642729 -4.1230659 -4.1692543 -4.2142248 -4.2438693][-4.0529776 -4.022264 -4.0051479 -3.9941146 -3.9798687 -3.9592075 -3.9213054 -3.892314 -3.9083223 -3.9575944 -4.0294943 -4.1010609 -4.1539044 -4.2039771 -4.2385845][-4.0510497 -4.0106854 -3.9814439 -3.9625149 -3.9411609 -3.9174237 -3.8874748 -3.8785686 -3.9123781 -3.9658453 -4.0317349 -4.0932169 -4.1449113 -4.1990685 -4.2405028][-4.0532002 -4.0175829 -3.9893193 -3.9713101 -3.9544482 -3.9358935 -3.9178023 -3.9240432 -3.9609482 -4.0031981 -4.0525956 -4.0994062 -4.1441793 -4.1971097 -4.2417474][-4.0862145 -4.0681834 -4.0522952 -4.0409431 -4.0312634 -4.0203376 -4.0097823 -4.0155115 -4.0402951 -4.0649362 -4.0951948 -4.127028 -4.161891 -4.2060504 -4.2467933][-4.1354485 -4.1349688 -4.1329603 -4.1305366 -4.1292648 -4.1264482 -4.1208463 -4.1195989 -4.12983 -4.14255 -4.1608434 -4.1809478 -4.2039595 -4.23577 -4.2661743][-4.1971617 -4.2026353 -4.2051854 -4.2066383 -4.2084584 -4.2090673 -4.2058449 -4.2027316 -4.2061977 -4.2136526 -4.2264 -4.2396531 -4.2529221 -4.2729683 -4.2932515][-4.2549038 -4.2595482 -4.261786 -4.2621193 -4.2619429 -4.2610493 -4.2584672 -4.2563419 -4.2575979 -4.2618442 -4.27089 -4.2799368 -4.288177 -4.3004322 -4.3128324]]...]
INFO - root - 2017-12-05 23:40:39.703422: step 50810, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 73h:49m:41s remains)
INFO - root - 2017-12-05 23:40:48.826187: step 50820, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 68h:39m:49s remains)
INFO - root - 2017-12-05 23:40:57.835323: step 50830, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 71h:16m:19s remains)
INFO - root - 2017-12-05 23:41:06.930134: step 50840, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 69h:13m:14s remains)
INFO - root - 2017-12-05 23:41:16.004010: step 50850, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 69h:02m:16s remains)
INFO - root - 2017-12-05 23:41:25.045246: step 50860, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 70h:24m:27s remains)
INFO - root - 2017-12-05 23:41:34.152283: step 50870, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 65h:38m:22s remains)
INFO - root - 2017-12-05 23:41:43.199324: step 50880, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 71h:47m:07s remains)
INFO - root - 2017-12-05 23:41:52.239354: step 50890, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.909 sec/batch; 71h:08m:16s remains)
INFO - root - 2017-12-05 23:42:01.305832: step 50900, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.957 sec/batch; 74h:49m:14s remains)
2017-12-05 23:42:02.087109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1823993 -4.17228 -4.1684194 -4.1765575 -4.1881585 -4.1929913 -4.1935587 -4.1968565 -4.2070894 -4.2188325 -4.22988 -4.2388992 -4.2426662 -4.2463121 -4.2525849][-4.1838093 -4.1776271 -4.17356 -4.1834965 -4.2011781 -4.2118239 -4.2142072 -4.2139788 -4.2205009 -4.2278132 -4.2375941 -4.2460527 -4.2478576 -4.2525096 -4.2581525][-4.1727924 -4.1727843 -4.1714845 -4.1847253 -4.2016649 -4.212656 -4.2117815 -4.2089915 -4.2152143 -4.2262759 -4.2417531 -4.2523284 -4.2529783 -4.2533751 -4.24885][-4.162868 -4.167892 -4.1681762 -4.1793327 -4.1930003 -4.1996641 -4.1904879 -4.1875305 -4.1970963 -4.2179103 -4.2394595 -4.2496262 -4.2488623 -4.2442245 -4.2301021][-4.1593709 -4.1687932 -4.1692204 -4.1729231 -4.1795425 -4.1763391 -4.1547093 -4.1493597 -4.1678319 -4.20215 -4.235373 -4.2499743 -4.2496581 -4.2392015 -4.2185259][-4.1511669 -4.1658983 -4.1654844 -4.1654291 -4.16073 -4.1373777 -4.0903673 -4.0780673 -4.1141906 -4.1755505 -4.2278209 -4.2501874 -4.2474771 -4.232131 -4.210865][-4.1440129 -4.1574616 -4.1573772 -4.1523404 -4.1329269 -4.0710878 -3.9815662 -3.9667368 -4.0472856 -4.1448779 -4.2143264 -4.2428288 -4.2376738 -4.21888 -4.1996903][-4.1395497 -4.1498203 -4.1479406 -4.131669 -4.0904832 -3.995522 -3.8746626 -3.8784733 -4.0078783 -4.1268 -4.20057 -4.2288971 -4.2232962 -4.2072291 -4.19583][-4.140605 -4.1462331 -4.1421347 -4.1198196 -4.0703378 -3.9771338 -3.8940983 -3.9347839 -4.0538135 -4.1476593 -4.2047896 -4.2271156 -4.2234092 -4.2124147 -4.2084465][-4.1484418 -4.1480708 -4.1419263 -4.122365 -4.0797467 -4.025836 -4.0118732 -4.0648737 -4.1378441 -4.1906447 -4.2259655 -4.2391438 -4.231668 -4.221056 -4.2223554][-4.1592979 -4.1519 -4.1414247 -4.1256819 -4.1033125 -4.0946522 -4.1180673 -4.1639986 -4.197536 -4.2167621 -4.2324829 -4.2380853 -4.2259297 -4.217052 -4.228045][-4.1606379 -4.1462889 -4.1318779 -4.1252971 -4.1295671 -4.150208 -4.1811018 -4.2123885 -4.222712 -4.2185974 -4.2181463 -4.216785 -4.2059526 -4.2065997 -4.2272992][-4.159586 -4.1372628 -4.115931 -4.1188259 -4.1447539 -4.17843 -4.2092071 -4.2337012 -4.2341094 -4.2180309 -4.2090788 -4.2063131 -4.19846 -4.202158 -4.2213006][-4.1619115 -4.1313186 -4.1015677 -4.1081562 -4.1455383 -4.1870489 -4.2167449 -4.238203 -4.2386727 -4.2219996 -4.2084107 -4.2039781 -4.1979237 -4.1984797 -4.2116866][-4.1714687 -4.142096 -4.1111112 -4.1166329 -4.1527314 -4.1923323 -4.2186141 -4.2326984 -4.2305918 -4.2176013 -4.205441 -4.201694 -4.1973071 -4.1945715 -4.1988649]]...]
INFO - root - 2017-12-05 23:42:11.309702: step 50910, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 71h:33m:48s remains)
INFO - root - 2017-12-05 23:42:20.339808: step 50920, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 71h:25m:47s remains)
INFO - root - 2017-12-05 23:42:29.427835: step 50930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 72h:45m:03s remains)
INFO - root - 2017-12-05 23:42:38.550759: step 50940, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 69h:10m:47s remains)
INFO - root - 2017-12-05 23:42:47.595816: step 50950, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 73h:12m:08s remains)
INFO - root - 2017-12-05 23:42:56.795332: step 50960, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.934 sec/batch; 73h:03m:08s remains)
INFO - root - 2017-12-05 23:43:05.798622: step 50970, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 70h:59m:55s remains)
INFO - root - 2017-12-05 23:43:14.898877: step 50980, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 73h:21m:31s remains)
INFO - root - 2017-12-05 23:43:23.991152: step 50990, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 73h:33m:15s remains)
INFO - root - 2017-12-05 23:43:33.059331: step 51000, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.926 sec/batch; 72h:23m:21s remains)
2017-12-05 23:43:33.969787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1838775 -4.1700993 -4.158299 -4.1576343 -4.1677065 -4.1837063 -4.195507 -4.1937594 -4.1825786 -4.1790223 -4.1822906 -4.1903605 -4.19842 -4.2002664 -4.1934423][-4.1882133 -4.1744566 -4.1641212 -4.1682286 -4.1828966 -4.1952972 -4.1999974 -4.1934285 -4.1757779 -4.1668253 -4.1698184 -4.1795125 -4.1880789 -4.1871853 -4.175828][-4.1903868 -4.1757274 -4.1661706 -4.1803927 -4.2025194 -4.2094922 -4.2017837 -4.1850939 -4.164031 -4.1539412 -4.1601577 -4.1727438 -4.1811657 -4.1799021 -4.1672797][-4.1757607 -4.1613564 -4.1508565 -4.1729989 -4.2047653 -4.2106185 -4.1932411 -4.1707864 -4.1493177 -4.1344976 -4.1398287 -4.1545334 -4.1678348 -4.1718664 -4.1624327][-4.1337633 -4.126689 -4.11933 -4.14639 -4.1855226 -4.1945257 -4.1727595 -4.1504822 -4.1325607 -4.1126437 -4.1113458 -4.12364 -4.141746 -4.1558118 -4.1517224][-4.0808063 -4.0858335 -4.084847 -4.1124415 -4.1507587 -4.1611228 -4.1344295 -4.113502 -4.1048021 -4.0860267 -4.0766315 -4.0827513 -4.1026454 -4.1201463 -4.123898][-4.0337014 -4.0423188 -4.0464911 -4.0705552 -4.1025681 -4.10286 -4.0646644 -4.0432348 -4.0528789 -4.0483546 -4.0338264 -4.0317383 -4.0489593 -4.0635123 -4.0768104][-4.00289 -4.0055943 -4.007082 -4.0263247 -4.0484552 -4.0327468 -3.9836 -3.962846 -3.99717 -4.0111475 -3.9968944 -3.9850352 -3.9968545 -4.0109811 -4.0352368][-4.0135179 -4.0051031 -4.0007687 -4.0111718 -4.0292358 -4.0073624 -3.9632993 -3.953119 -4.0008612 -4.0205507 -4.00342 -3.9862 -3.9926853 -4.0016317 -4.0334706][-4.0548277 -4.0434704 -4.0393491 -4.0485168 -4.0623503 -4.0412841 -4.008132 -4.0016236 -4.0462551 -4.0672894 -4.0529637 -4.0358744 -4.0339241 -4.0358248 -4.0670409][-4.1054878 -4.0971651 -4.0954337 -4.1065292 -4.1185951 -4.0987716 -4.072866 -4.0646815 -4.1036668 -4.1283045 -4.1232128 -4.1071682 -4.0991387 -4.095511 -4.12311][-4.1624026 -4.1575823 -4.1585054 -4.1705284 -4.1813951 -4.1645246 -4.1449728 -4.1377463 -4.1710744 -4.1956825 -4.2015958 -4.1911726 -4.1809549 -4.1747904 -4.1928139][-4.2167678 -4.2147212 -4.2159562 -4.2258234 -4.2352557 -4.2236938 -4.210916 -4.2073936 -4.2322259 -4.2549968 -4.26528 -4.2569356 -4.247221 -4.2411242 -4.2504058][-4.2521267 -4.2511959 -4.2512794 -4.2580137 -4.2636471 -4.2577491 -4.2529254 -4.2524548 -4.2698693 -4.287941 -4.2961345 -4.2873769 -4.2797728 -4.275877 -4.2803984][-4.2687526 -4.268683 -4.2690458 -4.2739749 -4.278295 -4.2772064 -4.2776327 -4.2808304 -4.2933774 -4.30381 -4.3069496 -4.3001132 -4.2937026 -4.2907529 -4.2930431]]...]
INFO - root - 2017-12-05 23:43:43.170572: step 51010, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 71h:39m:52s remains)
INFO - root - 2017-12-05 23:43:52.288879: step 51020, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 71h:02m:40s remains)
INFO - root - 2017-12-05 23:44:01.275451: step 51030, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 71h:41m:20s remains)
INFO - root - 2017-12-05 23:44:10.643301: step 51040, loss = 2.02, batch loss = 1.96 (8.4 examples/sec; 0.947 sec/batch; 74h:03m:49s remains)
INFO - root - 2017-12-05 23:44:19.663167: step 51050, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 71h:00m:18s remains)
INFO - root - 2017-12-05 23:44:28.571608: step 51060, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 69h:57m:08s remains)
INFO - root - 2017-12-05 23:44:37.579654: step 51070, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 71h:27m:45s remains)
INFO - root - 2017-12-05 23:44:46.744267: step 51080, loss = 2.03, batch loss = 1.97 (8.4 examples/sec; 0.951 sec/batch; 74h:20m:35s remains)
INFO - root - 2017-12-05 23:44:55.925192: step 51090, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 67h:39m:22s remains)
INFO - root - 2017-12-05 23:45:04.953389: step 51100, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:06m:33s remains)
2017-12-05 23:45:05.774396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24354 -4.2448773 -4.2430887 -4.23335 -4.2049532 -4.163867 -4.12743 -4.1095719 -4.1024947 -4.10195 -4.11301 -4.1362672 -4.1720986 -4.2089105 -4.2368469][-4.2574677 -4.2490597 -4.2428732 -4.2378044 -4.2243896 -4.2005591 -4.1813955 -4.1811461 -4.1884465 -4.1992664 -4.2136674 -4.22995 -4.2516322 -4.2762847 -4.2937646][-4.2562828 -4.2411361 -4.2326155 -4.2331038 -4.2308064 -4.2163939 -4.198216 -4.1958785 -4.2041264 -4.2154827 -4.2268872 -4.2420282 -4.2639122 -4.2925696 -4.3126493][-4.2420158 -4.2193832 -4.206306 -4.2054739 -4.2047091 -4.1904197 -4.1636677 -4.149673 -4.1552382 -4.168189 -4.1828446 -4.2011695 -4.2322016 -4.274713 -4.3056445][-4.2263107 -4.1964493 -4.1747322 -4.16248 -4.1514106 -4.1249123 -4.0794325 -4.0505867 -4.0604157 -4.0879269 -4.1191039 -4.150207 -4.1958671 -4.2508135 -4.2904615][-4.2226439 -4.1885548 -4.1533985 -4.1208959 -4.0881104 -4.0375161 -3.9697423 -3.9342396 -3.9664373 -4.0270348 -4.0871439 -4.1350384 -4.1893764 -4.2465606 -4.2853551][-4.2192192 -4.1798792 -4.1289291 -4.074326 -4.019033 -3.9498909 -3.8736098 -3.8471975 -3.9112453 -4.0094757 -4.0968 -4.15624 -4.211812 -4.2620144 -4.295136][-4.2097006 -4.1651988 -4.1026535 -4.0370641 -3.9780202 -3.9186406 -3.8658447 -3.86242 -3.9405456 -4.0473866 -4.1380916 -4.1950107 -4.2449174 -4.2835932 -4.3080964][-4.2060719 -4.1621122 -4.1001616 -4.0427632 -4.0028062 -3.9764786 -3.964345 -3.9811151 -4.0460868 -4.1291389 -4.1979537 -4.2393446 -4.2737961 -4.2999849 -4.3160105][-4.2205696 -4.1867471 -4.1419034 -4.107873 -4.0934024 -4.0949373 -4.1061206 -4.1275725 -4.1682835 -4.215188 -4.2534523 -4.27244 -4.2889252 -4.3030963 -4.31385][-4.2481403 -4.2288251 -4.2055488 -4.1927238 -4.1939106 -4.20664 -4.2243209 -4.2409267 -4.26078 -4.277998 -4.2886219 -4.2853785 -4.2821288 -4.285315 -4.2950029][-4.2649755 -4.2589164 -4.2528124 -4.251843 -4.2590055 -4.2741747 -4.2901249 -4.2996426 -4.305377 -4.3042192 -4.2928824 -4.2670231 -4.243526 -4.2360716 -4.248579][-4.2520823 -4.2572432 -4.2664571 -4.2752376 -4.2865763 -4.29924 -4.3114567 -4.3162503 -4.3139458 -4.3002496 -4.2673783 -4.2167816 -4.1734824 -4.1582046 -4.1783404][-4.2232385 -4.23584 -4.2570281 -4.2755418 -4.2886558 -4.29639 -4.3016734 -4.3002877 -4.2915163 -4.2680593 -4.2176437 -4.1505437 -4.0972695 -4.0799484 -4.1110792][-4.2069063 -4.2196274 -4.2442646 -4.2634792 -4.2703838 -4.2674131 -4.2597642 -4.2511554 -4.2398839 -4.2167931 -4.16481 -4.0959153 -4.0435724 -4.0257521 -4.0617623]]...]
INFO - root - 2017-12-05 23:45:14.732154: step 51110, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 68h:21m:24s remains)
INFO - root - 2017-12-05 23:45:23.835218: step 51120, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.933 sec/batch; 72h:57m:02s remains)
INFO - root - 2017-12-05 23:45:33.105894: step 51130, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 72h:35m:24s remains)
INFO - root - 2017-12-05 23:45:42.370997: step 51140, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 68h:39m:37s remains)
INFO - root - 2017-12-05 23:45:51.404064: step 51150, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 71h:59m:39s remains)
INFO - root - 2017-12-05 23:46:00.655478: step 51160, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.941 sec/batch; 73h:31m:37s remains)
INFO - root - 2017-12-05 23:46:09.825971: step 51170, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 69h:06m:05s remains)
INFO - root - 2017-12-05 23:46:18.896563: step 51180, loss = 2.12, batch loss = 2.06 (8.5 examples/sec; 0.938 sec/batch; 73h:20m:12s remains)
INFO - root - 2017-12-05 23:46:28.062108: step 51190, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 71h:55m:57s remains)
INFO - root - 2017-12-05 23:46:37.173730: step 51200, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 67h:22m:24s remains)
2017-12-05 23:46:37.954664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3036785 -4.3036618 -4.30281 -4.3019714 -4.29935 -4.296762 -4.2962852 -4.2974825 -4.2998657 -4.3029666 -4.3054976 -4.306169 -4.3045073 -4.3024378 -4.30188][-4.2912507 -4.291491 -4.2918372 -4.2915239 -4.2873254 -4.2811356 -4.2771516 -4.2777748 -4.2825 -4.2886319 -4.29439 -4.2976732 -4.2983894 -4.2981305 -4.2973309][-4.2765622 -4.2779546 -4.2793393 -4.2779469 -4.2696409 -4.2576222 -4.2493548 -4.2494607 -4.2585607 -4.2692704 -4.2786536 -4.2842846 -4.2868919 -4.2878752 -4.2865434][-4.2673578 -4.2682939 -4.269043 -4.2640934 -4.2490473 -4.2300043 -4.2185059 -4.2198076 -4.236186 -4.2532096 -4.2652569 -4.2712703 -4.2736011 -4.2731385 -4.2686052][-4.2565846 -4.2554183 -4.2526164 -4.2423263 -4.2200975 -4.1934843 -4.1781435 -4.1840506 -4.2121897 -4.2402077 -4.2563744 -4.26065 -4.258502 -4.2526832 -4.2416878][-4.2380347 -4.2342663 -4.2245121 -4.2059107 -4.1751232 -4.1376224 -4.1131105 -4.1242476 -4.1694078 -4.2143106 -4.2412643 -4.2484407 -4.2419815 -4.2283754 -4.2093978][-4.2189727 -4.2118692 -4.1944013 -4.1662269 -4.1241355 -4.0708108 -4.0311995 -4.0439682 -4.1077929 -4.1746192 -4.2195182 -4.2352462 -4.2299595 -4.2117181 -4.186214][-4.1988153 -4.1894264 -4.170023 -4.1375842 -4.086997 -4.0206962 -3.9689856 -3.9822485 -4.0571685 -4.1371803 -4.1942363 -4.2175083 -4.216754 -4.1962786 -4.1651917][-4.2020531 -4.1936278 -4.1795053 -4.1517668 -4.1032567 -4.0390687 -3.9923279 -4.005898 -4.0720091 -4.142817 -4.1977305 -4.2218151 -4.2219219 -4.1998429 -4.1654949][-4.208591 -4.2063866 -4.203608 -4.1864114 -4.14669 -4.0960941 -4.064364 -4.0795641 -4.1285534 -4.1780176 -4.2187266 -4.2392368 -4.2374496 -4.2150693 -4.1795754][-4.1943879 -4.2018542 -4.2107873 -4.2034035 -4.1718349 -4.1333485 -4.1119032 -4.1270943 -4.1651111 -4.1998057 -4.2310233 -4.2528181 -4.2539535 -4.2388043 -4.2056847][-4.1678581 -4.1798716 -4.1968541 -4.198514 -4.1758094 -4.1454115 -4.1259418 -4.136313 -4.1713171 -4.2035842 -4.2367635 -4.2645464 -4.2722192 -4.2659888 -4.2369947][-4.1377721 -4.1518154 -4.1748857 -4.1888719 -4.180244 -4.1620774 -4.1409 -4.1410222 -4.1699724 -4.2009983 -4.2357421 -4.2682905 -4.2822208 -4.2846918 -4.2620049][-4.1282015 -4.1409554 -4.1650438 -4.1866331 -4.1945162 -4.1927972 -4.1760435 -4.1691027 -4.1854925 -4.2062554 -4.2346759 -4.2647047 -4.2815533 -4.2884293 -4.2737622][-4.1433039 -4.1496811 -4.1665111 -4.1859412 -4.2039347 -4.2197948 -4.2155118 -4.2083035 -4.2129235 -4.2203031 -4.2372046 -4.2606006 -4.2782903 -4.2868872 -4.2809997]]...]
INFO - root - 2017-12-05 23:46:46.890639: step 51210, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 72h:11m:45s remains)
INFO - root - 2017-12-05 23:46:56.104129: step 51220, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 72h:09m:55s remains)
INFO - root - 2017-12-05 23:47:05.255302: step 51230, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 72h:01m:35s remains)
INFO - root - 2017-12-05 23:47:14.276979: step 51240, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 71h:52m:51s remains)
INFO - root - 2017-12-05 23:47:23.398256: step 51250, loss = 2.11, batch loss = 2.05 (8.9 examples/sec; 0.904 sec/batch; 70h:36m:57s remains)
INFO - root - 2017-12-05 23:47:32.597298: step 51260, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 72h:52m:28s remains)
INFO - root - 2017-12-05 23:47:41.721244: step 51270, loss = 2.01, batch loss = 1.95 (8.8 examples/sec; 0.905 sec/batch; 70h:40m:04s remains)
INFO - root - 2017-12-05 23:47:50.909240: step 51280, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 71h:05m:40s remains)
INFO - root - 2017-12-05 23:48:00.098245: step 51290, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.936 sec/batch; 73h:04m:43s remains)
INFO - root - 2017-12-05 23:48:09.135544: step 51300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 70h:56m:02s remains)
2017-12-05 23:48:09.883368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.336875 -4.325748 -4.3187518 -4.3151793 -4.316618 -4.3179522 -4.3202553 -4.3198705 -4.3147373 -4.3093166 -4.3121734 -4.3195891 -4.3221035 -4.3249569 -4.3304858][-4.3186398 -4.2993369 -4.2869859 -4.2814426 -4.2819352 -4.2782917 -4.27474 -4.272439 -4.266336 -4.2614889 -4.2662125 -4.2829232 -4.2962832 -4.3047934 -4.31502][-4.2990994 -4.271925 -4.2530394 -4.2429361 -4.2381573 -4.2250733 -4.2110896 -4.2052393 -4.2024064 -4.2018819 -4.2082238 -4.2377605 -4.2686539 -4.2858524 -4.3007851][-4.2850804 -4.2549005 -4.2305193 -4.2112722 -4.1948342 -4.170785 -4.1440158 -4.1313572 -4.1351237 -4.1440787 -4.1584368 -4.1963592 -4.2413006 -4.2693443 -4.2906218][-4.272078 -4.2387533 -4.202857 -4.1677775 -4.1282582 -4.0823255 -4.0393481 -4.0216131 -4.0376768 -4.071372 -4.107502 -4.1604843 -4.2175975 -4.2544556 -4.2835813][-4.2695732 -4.2315531 -4.1828051 -4.1242094 -4.0484271 -3.9665024 -3.8980594 -3.8724067 -3.906045 -3.9712856 -4.0392771 -4.1174212 -4.1891608 -4.2365208 -4.273056][-4.2732921 -4.2297754 -4.1674533 -4.0877609 -3.9819736 -3.8640358 -3.763612 -3.7287345 -3.7804823 -3.8794303 -3.9790988 -4.0842981 -4.1700792 -4.2231669 -4.263412][-4.2716136 -4.2203288 -4.152122 -4.0653839 -3.9508994 -3.8259656 -3.717895 -3.6859789 -3.7494776 -3.8682046 -3.9867761 -4.0980239 -4.1837931 -4.2345204 -4.268055][-4.2316937 -4.1726561 -4.110734 -4.0371208 -3.9462118 -3.8669403 -3.8136497 -3.8130906 -3.8656344 -3.9658082 -4.065064 -4.1465645 -4.2101026 -4.24896 -4.2743788][-4.1713018 -4.1030607 -4.0520611 -4.0037308 -3.9499342 -3.9251246 -3.9274549 -3.9438558 -3.977185 -4.0439086 -4.1155119 -4.1700587 -4.2136655 -4.24788 -4.2722368][-4.1228237 -4.0512104 -4.0063591 -3.9721107 -3.9432602 -3.9519863 -3.9847193 -4.003768 -4.0189815 -4.0625067 -4.1174908 -4.1598296 -4.1969566 -4.2337642 -4.2631617][-4.1139178 -4.0490303 -4.0064259 -3.9750328 -3.9564879 -3.9767911 -4.016923 -4.0334191 -4.0376554 -4.0709248 -4.11679 -4.1531363 -4.1898584 -4.2284794 -4.2609043][-4.1472 -4.0915818 -4.0522637 -4.0319657 -4.0283327 -4.0546236 -4.09339 -4.1049838 -4.102273 -4.1285415 -4.167625 -4.1969061 -4.22568 -4.2566557 -4.2836552][-4.215157 -4.1767235 -4.1518211 -4.1457562 -4.1533589 -4.1815166 -4.2153945 -4.2224631 -4.21482 -4.2323847 -4.2602658 -4.2781272 -4.2917814 -4.3058887 -4.3185592][-4.2816811 -4.2605071 -4.2481565 -4.2483878 -4.2569551 -4.2790427 -4.3027616 -4.3088646 -4.3035069 -4.3115735 -4.3267522 -4.3355274 -4.3387508 -4.3404746 -4.3415256]]...]
INFO - root - 2017-12-05 23:48:18.789246: step 51310, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 70h:07m:58s remains)
INFO - root - 2017-12-05 23:48:27.997275: step 51320, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 70h:55m:11s remains)
INFO - root - 2017-12-05 23:48:36.976376: step 51330, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 70h:57m:15s remains)
INFO - root - 2017-12-05 23:48:46.093051: step 51340, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.902 sec/batch; 70h:27m:37s remains)
INFO - root - 2017-12-05 23:48:55.051382: step 51350, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 69h:08m:56s remains)
INFO - root - 2017-12-05 23:49:04.211443: step 51360, loss = 2.05, batch loss = 2.00 (8.4 examples/sec; 0.953 sec/batch; 74h:23m:07s remains)
INFO - root - 2017-12-05 23:49:13.548503: step 51370, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 71h:41m:03s remains)
INFO - root - 2017-12-05 23:49:22.660739: step 51380, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 70h:41m:15s remains)
INFO - root - 2017-12-05 23:49:32.052657: step 51390, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 72h:40m:05s remains)
INFO - root - 2017-12-05 23:49:40.913312: step 51400, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.913 sec/batch; 71h:19m:04s remains)
2017-12-05 23:49:41.778962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054614 -4.2977324 -4.292459 -4.2892933 -4.2881122 -4.2884741 -4.28958 -4.2892952 -4.2888818 -4.2901564 -4.2931981 -4.2962012 -4.2991791 -4.3026485 -4.30641][-4.2945294 -4.2851114 -4.2802353 -4.2795792 -4.2798452 -4.2795472 -4.2787862 -4.2747679 -4.2722044 -4.2742548 -4.2800155 -4.2850928 -4.29061 -4.2958465 -4.3003039][-4.2725053 -4.2613487 -4.25846 -4.2621012 -4.2645392 -4.2623167 -4.2562318 -4.2456865 -4.2395964 -4.2440248 -4.2572374 -4.2698736 -4.2803874 -4.2897167 -4.2964711][-4.238265 -4.2229953 -4.21955 -4.2250733 -4.2287874 -4.2237654 -4.211504 -4.1937194 -4.1847868 -4.1945062 -4.2204409 -4.2439933 -4.2609406 -4.2745309 -4.2863941][-4.1979823 -4.1780438 -4.1726875 -4.1773434 -4.1804533 -4.1738834 -4.1568317 -4.1332955 -4.1244445 -4.1435251 -4.1841059 -4.2179418 -4.2385941 -4.2543774 -4.2717757][-4.1665044 -4.1420016 -4.131875 -4.13249 -4.1323123 -4.1223555 -4.0990634 -4.06965 -4.0633574 -4.0952768 -4.1489148 -4.1920056 -4.2166553 -4.2350864 -4.2578063][-4.1608267 -4.1328669 -4.1172729 -4.1122332 -4.1064005 -4.0906978 -4.0613194 -4.0264359 -4.02324 -4.0647168 -4.1257792 -4.176599 -4.2066727 -4.2269268 -4.251153][-4.1807508 -4.1531396 -4.1389303 -4.1364861 -4.1308427 -4.1135807 -4.0798349 -4.0400982 -4.0350742 -4.0766072 -4.1356959 -4.1855979 -4.2159081 -4.2348771 -4.2548923][-4.1976929 -4.1740527 -4.166647 -4.1711774 -4.1733356 -4.1634121 -4.1312485 -4.0905213 -4.0822649 -4.1168814 -4.1657152 -4.2070003 -4.2325683 -4.2477722 -4.2620668][-4.1987019 -4.1803451 -4.1793056 -4.1902351 -4.19915 -4.1940966 -4.1644492 -4.1304655 -4.1266661 -4.1554012 -4.1934285 -4.226625 -4.2476449 -4.2597561 -4.2707367][-4.1991625 -4.1876378 -4.190412 -4.2010489 -4.2095361 -4.2039075 -4.1783986 -4.1567273 -4.16106 -4.187901 -4.2197576 -4.2482095 -4.2665558 -4.2758365 -4.2829914][-4.2137594 -4.2071285 -4.2121487 -4.22232 -4.2294416 -4.2232308 -4.2036324 -4.1908221 -4.199244 -4.2239504 -4.253212 -4.27797 -4.2931337 -4.2980313 -4.2998419][-4.2504091 -4.2485771 -4.255075 -4.2649078 -4.271276 -4.2652292 -4.2507968 -4.2438083 -4.2524052 -4.274282 -4.2988014 -4.3161535 -4.323842 -4.3226748 -4.31856][-4.293376 -4.2943578 -4.300499 -4.3083296 -4.3128452 -4.3088346 -4.2994685 -4.2948456 -4.3007255 -4.3164082 -4.3329983 -4.3421564 -4.342905 -4.33762 -4.3304338][-4.3206582 -4.3228626 -4.3273473 -4.3319426 -4.3346004 -4.3331485 -4.3279881 -4.3239665 -4.3263016 -4.3342242 -4.3422508 -4.3454232 -4.3434682 -4.338068 -4.3321271]]...]
INFO - root - 2017-12-05 23:49:50.738807: step 51410, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 70h:09m:17s remains)
INFO - root - 2017-12-05 23:49:59.964193: step 51420, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 70h:07m:57s remains)
INFO - root - 2017-12-05 23:50:09.230611: step 51430, loss = 2.04, batch loss = 1.99 (8.2 examples/sec; 0.981 sec/batch; 76h:33m:31s remains)
INFO - root - 2017-12-05 23:50:18.341573: step 51440, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 72h:35m:43s remains)
INFO - root - 2017-12-05 23:50:27.513087: step 51450, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 72h:52m:51s remains)
INFO - root - 2017-12-05 23:50:36.669460: step 51460, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 71h:37m:02s remains)
INFO - root - 2017-12-05 23:50:45.915897: step 51470, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 72h:29m:28s remains)
INFO - root - 2017-12-05 23:50:55.090449: step 51480, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 72h:36m:49s remains)
INFO - root - 2017-12-05 23:51:04.078413: step 51490, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 69h:39m:39s remains)
INFO - root - 2017-12-05 23:51:13.280713: step 51500, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 70h:51m:25s remains)
2017-12-05 23:51:14.077760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.264318 -4.2613764 -4.2546492 -4.2449465 -4.2397203 -4.2415481 -4.2426109 -4.2431846 -4.2468586 -4.2477713 -4.2512293 -4.2597065 -4.273622 -4.2823443 -4.2823281][-4.2576003 -4.2533226 -4.24519 -4.2337847 -4.2285142 -4.2304883 -4.2309542 -4.2290444 -4.2348609 -4.2371788 -4.2355094 -4.2388968 -4.2537694 -4.262207 -4.2589083][-4.2276511 -4.2211766 -4.2145858 -4.20424 -4.2048593 -4.2116637 -4.2098756 -4.2051549 -4.211854 -4.2189488 -4.2175517 -4.2203841 -4.2352085 -4.2412786 -4.2337775][-4.2026711 -4.1911936 -4.1847706 -4.1742077 -4.1770139 -4.1804576 -4.1716242 -4.168148 -4.1839838 -4.2020717 -4.2049727 -4.2061467 -4.2149153 -4.2140765 -4.2021065][-4.1823282 -4.1669025 -4.1564569 -4.1412735 -4.1286306 -4.1112227 -4.0809555 -4.0737581 -4.1048374 -4.1404343 -4.158494 -4.17229 -4.1843529 -4.178278 -4.1654153][-4.1736832 -4.159605 -4.1449652 -4.1170983 -4.0766129 -4.0259986 -3.9662712 -3.9512105 -4.0043745 -4.068058 -4.1092072 -4.1403403 -4.159687 -4.1547141 -4.1490469][-4.1489296 -4.1380887 -4.1235061 -4.0913167 -4.0398374 -3.9707358 -3.886373 -3.85928 -3.927702 -4.0149889 -4.0725393 -4.1126909 -4.1398087 -4.1394153 -4.139647][-4.1401467 -4.1277275 -4.1156578 -4.0898294 -4.0494351 -3.9910593 -3.9175506 -3.8869712 -3.9409997 -4.0148835 -4.0661912 -4.1065235 -4.1414638 -4.1511612 -4.1592231][-4.1848068 -4.1716228 -4.1608191 -4.1424103 -4.1205392 -4.0914235 -4.0493298 -4.0286303 -4.0579753 -4.1004586 -4.1339312 -4.1651692 -4.1994615 -4.2148395 -4.2206979][-4.2200017 -4.2093391 -4.1999679 -4.1842923 -4.171958 -4.1590648 -4.1381197 -4.1243439 -4.1380405 -4.1609216 -4.1845717 -4.2100511 -4.23572 -4.2483778 -4.2527771][-4.2277579 -4.219676 -4.2110615 -4.199748 -4.1923418 -4.1854849 -4.17402 -4.1678829 -4.1753902 -4.1915655 -4.2111149 -4.2329421 -4.2501469 -4.25862 -4.2619748][-4.2088804 -4.19831 -4.18519 -4.1771474 -4.1797886 -4.1831012 -4.1794028 -4.1812377 -4.1902227 -4.2029791 -4.2175589 -4.236825 -4.2542009 -4.2625079 -4.2636371][-4.1947308 -4.1783237 -4.1606951 -4.1527796 -4.1584082 -4.1638241 -4.1592717 -4.1606212 -4.1712856 -4.1847172 -4.196558 -4.2127495 -4.231627 -4.2440915 -4.246799][-4.2053847 -4.1849213 -4.1683574 -4.1611838 -4.1615729 -4.1615419 -4.154315 -4.1545839 -4.1650348 -4.1770253 -4.1856213 -4.1935 -4.2048235 -4.2145348 -4.2211642][-4.2479467 -4.2291646 -4.2160773 -4.2087541 -4.2041636 -4.1964855 -4.1849318 -4.1834631 -4.1949615 -4.209249 -4.2171564 -4.21794 -4.2158093 -4.2147112 -4.2185016]]...]
INFO - root - 2017-12-05 23:51:23.080965: step 51510, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 70h:56m:35s remains)
INFO - root - 2017-12-05 23:51:32.115002: step 51520, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.908 sec/batch; 70h:52m:08s remains)
INFO - root - 2017-12-05 23:51:41.133722: step 51530, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:23m:25s remains)
INFO - root - 2017-12-05 23:51:50.289287: step 51540, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.911 sec/batch; 71h:05m:46s remains)
INFO - root - 2017-12-05 23:51:59.516425: step 51550, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 71h:47m:07s remains)
INFO - root - 2017-12-05 23:52:08.461818: step 51560, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 72h:47m:20s remains)
INFO - root - 2017-12-05 23:52:17.570697: step 51570, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 70h:43m:17s remains)
INFO - root - 2017-12-05 23:52:26.717590: step 51580, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 70h:42m:01s remains)
INFO - root - 2017-12-05 23:52:35.518065: step 51590, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 67h:27m:34s remains)
INFO - root - 2017-12-05 23:52:44.685128: step 51600, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 71h:42m:58s remains)
2017-12-05 23:52:45.492473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2737355 -4.3042445 -4.31218 -4.2992187 -4.2762976 -4.250525 -4.2282391 -4.2175865 -4.217247 -4.2197566 -4.2210231 -4.2216959 -4.220439 -4.21517 -4.2069564][-4.2653155 -4.29729 -4.3042221 -4.2915511 -4.2710223 -4.2413445 -4.2126946 -4.20483 -4.2146463 -4.2254405 -4.2303658 -4.2326069 -4.2287035 -4.2166491 -4.2012196][-4.2541595 -4.2783093 -4.2808242 -4.2697129 -4.2496586 -4.213553 -4.1746535 -4.1649642 -4.1842036 -4.2098179 -4.2275 -4.2365584 -4.2311664 -4.2138805 -4.1917539][-4.2447944 -4.2558284 -4.2570667 -4.2475181 -4.2229972 -4.1779022 -4.1214433 -4.1015406 -4.1293707 -4.179718 -4.2225814 -4.2460723 -4.2444954 -4.2245312 -4.1974463][-4.2351828 -4.2355456 -4.2336345 -4.222455 -4.1897554 -4.1322722 -4.0528607 -4.0097675 -4.0378623 -4.1201806 -4.1962075 -4.2359118 -4.2439585 -4.2284937 -4.2037749][-4.228085 -4.2230835 -4.2164822 -4.1986823 -4.1544523 -4.0799179 -3.9702864 -3.8864429 -3.9089897 -4.0332394 -4.1437244 -4.1955113 -4.2138028 -4.2105808 -4.1997628][-4.2287779 -4.2255163 -4.2186928 -4.1942215 -4.1402044 -4.0503669 -3.9072111 -3.7667654 -3.7761774 -3.9461317 -4.0919394 -4.1557407 -4.1802254 -4.1920528 -4.1999431][-4.2434096 -4.2463217 -4.2404366 -4.21331 -4.1558805 -4.0611081 -3.9101076 -3.7481227 -3.739084 -3.9092278 -4.0646043 -4.13606 -4.1631036 -4.1820612 -4.2042441][-4.2532067 -4.2636418 -4.2646875 -4.2442594 -4.1972547 -4.1148405 -3.9884377 -3.8581781 -3.8325057 -3.93911 -4.0629621 -4.1334305 -4.1619511 -4.1810403 -4.208498][-4.2539024 -4.2665348 -4.2726274 -4.2635632 -4.23521 -4.1767879 -4.0872769 -3.991415 -3.9526227 -3.9931087 -4.0719748 -4.1337228 -4.1679673 -4.1887894 -4.2129827][-4.2413831 -4.2503114 -4.2578135 -4.25816 -4.247283 -4.2125726 -4.1567397 -4.0908418 -4.0494471 -4.0516424 -4.0952988 -4.1464691 -4.1825256 -4.2040162 -4.22091][-4.19811 -4.2059441 -4.2186317 -4.22983 -4.2370977 -4.2274737 -4.2009325 -4.1624517 -4.1304655 -4.1194181 -4.1403961 -4.177846 -4.2070675 -4.2233448 -4.2310467][-4.1398191 -4.1528039 -4.1736789 -4.1958795 -4.219852 -4.2343812 -4.2299824 -4.2140555 -4.196517 -4.1879478 -4.1968822 -4.2172604 -4.2335968 -4.2428145 -4.242218][-4.081368 -4.1009326 -4.1320448 -4.16938 -4.207345 -4.238215 -4.2489257 -4.2484074 -4.2451735 -4.2437792 -4.2497768 -4.2584372 -4.2636909 -4.2647038 -4.2590938][-4.0198083 -4.0449767 -4.0941415 -4.149498 -4.1988778 -4.2409563 -4.2611804 -4.2684503 -4.2722883 -4.2772942 -4.2861485 -4.2915416 -4.2935266 -4.2913394 -4.2826257]]...]
INFO - root - 2017-12-05 23:52:54.612281: step 51610, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.943 sec/batch; 73h:36m:06s remains)
INFO - root - 2017-12-05 23:53:03.393291: step 51620, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 67h:20m:21s remains)
INFO - root - 2017-12-05 23:53:12.521243: step 51630, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 70h:35m:12s remains)
INFO - root - 2017-12-05 23:53:21.713828: step 51640, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 70h:41m:51s remains)
INFO - root - 2017-12-05 23:53:30.775582: step 51650, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 69h:21m:17s remains)
INFO - root - 2017-12-05 23:53:40.031927: step 51660, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 71h:58m:31s remains)
INFO - root - 2017-12-05 23:53:49.145241: step 51670, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 72h:37m:01s remains)
INFO - root - 2017-12-05 23:53:58.142754: step 51680, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 73h:00m:21s remains)
INFO - root - 2017-12-05 23:54:07.395710: step 51690, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 72h:31m:36s remains)
INFO - root - 2017-12-05 23:54:16.572558: step 51700, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.913 sec/batch; 71h:13m:13s remains)
2017-12-05 23:54:17.436344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2377725 -4.23117 -4.2398481 -4.2535439 -4.270339 -4.2852421 -4.2955828 -4.3041792 -4.3059969 -4.3027344 -4.30024 -4.2982559 -4.2967982 -4.2958579 -4.2931123][-4.1869073 -4.1763043 -4.18965 -4.2123928 -4.2394738 -4.2643137 -4.2853746 -4.3005805 -4.3042207 -4.30185 -4.3006492 -4.2981348 -4.2954903 -4.2923832 -4.2869182][-4.1711054 -4.1600213 -4.1734915 -4.197576 -4.2242913 -4.2499619 -4.2747793 -4.292336 -4.3001318 -4.3027029 -4.3060055 -4.3047938 -4.3009725 -4.2962737 -4.2897582][-4.1837487 -4.1769271 -4.1881175 -4.2063327 -4.2230992 -4.2404208 -4.2582221 -4.2723937 -4.284214 -4.2935348 -4.3032465 -4.3090658 -4.3073874 -4.3028445 -4.2968512][-4.2067046 -4.2010446 -4.202517 -4.205596 -4.2053146 -4.2077408 -4.2142129 -4.2195673 -4.231329 -4.2484345 -4.2686486 -4.2909317 -4.3018 -4.3050137 -4.3043633][-4.210566 -4.2001405 -4.1879 -4.1731424 -4.1506319 -4.1319761 -4.1258106 -4.1276121 -4.1494017 -4.1805959 -4.2132277 -4.2534685 -4.2813864 -4.2982059 -4.307693][-4.1812425 -4.1570415 -4.1251674 -4.0901256 -4.0399132 -3.9888046 -3.9671443 -3.977354 -4.0202031 -4.0667953 -4.112174 -4.1762223 -4.2280617 -4.2622409 -4.2874384][-4.1245461 -4.0783863 -4.0255542 -3.9709237 -3.8895807 -3.7922955 -3.7354531 -3.7553208 -3.8265657 -3.8924077 -3.9546506 -4.0482359 -4.1345339 -4.1961975 -4.2442803][-4.0713062 -4.0160017 -3.9603269 -3.9041252 -3.8158085 -3.7033217 -3.6238968 -3.6330595 -3.6989694 -3.76406 -3.8296387 -3.9380705 -4.0472994 -4.1299629 -4.1974454][-4.075407 -4.0360475 -4.0030222 -3.9730325 -3.9208927 -3.8505709 -3.7857714 -3.7572553 -3.7660656 -3.7857263 -3.8212166 -3.9088197 -4.0116172 -4.0939016 -4.1614327][-4.1420112 -4.1262188 -4.1203351 -4.11748 -4.1042161 -4.0764689 -4.0351787 -3.9950843 -3.9674668 -3.9442801 -3.9362328 -3.9758904 -4.0400743 -4.098455 -4.1516027][-4.2215643 -4.2183914 -4.22504 -4.2340322 -4.2395258 -4.2357206 -4.2177014 -4.1944203 -4.1688952 -4.1352143 -4.1067119 -4.105711 -4.1250911 -4.1514945 -4.1826324][-4.2730379 -4.2691894 -4.2739339 -4.2829762 -4.2940078 -4.2969441 -4.2924385 -4.2887511 -4.2817454 -4.264009 -4.2425938 -4.2280493 -4.222249 -4.223556 -4.2340579][-4.2939949 -4.2843914 -4.2798886 -4.2812476 -4.2859159 -4.2844691 -4.2806778 -4.2875352 -4.3010807 -4.3088636 -4.3080368 -4.2998347 -4.2873077 -4.27524 -4.2705455][-4.2893448 -4.2654085 -4.2453828 -4.2341309 -4.2311082 -4.2251735 -4.2175384 -4.2308517 -4.26226 -4.2945347 -4.3173075 -4.3239379 -4.3145723 -4.2947049 -4.2775741]]...]
INFO - root - 2017-12-05 23:54:26.684443: step 51710, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 72h:26m:41s remains)
INFO - root - 2017-12-05 23:54:35.343126: step 51720, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 69h:36m:28s remains)
INFO - root - 2017-12-05 23:54:44.515254: step 51730, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.947 sec/batch; 73h:53m:22s remains)
INFO - root - 2017-12-05 23:54:53.677116: step 51740, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 72h:06m:54s remains)
INFO - root - 2017-12-05 23:55:02.672243: step 51750, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 71h:29m:54s remains)
INFO - root - 2017-12-05 23:55:11.933882: step 51760, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.948 sec/batch; 73h:54m:11s remains)
INFO - root - 2017-12-05 23:55:21.189189: step 51770, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 72h:17m:10s remains)
INFO - root - 2017-12-05 23:55:30.133825: step 51780, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.914 sec/batch; 71h:17m:43s remains)
INFO - root - 2017-12-05 23:55:39.312454: step 51790, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 70h:59m:17s remains)
INFO - root - 2017-12-05 23:55:48.476279: step 51800, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 70h:08m:42s remains)
2017-12-05 23:55:49.261856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1879725 -4.1955042 -4.2051959 -4.2147212 -4.2166414 -4.2098908 -4.2005191 -4.1909976 -4.1867194 -4.1879783 -4.1898203 -4.1929994 -4.2021365 -4.2159266 -4.2295041][-4.1838975 -4.1923437 -4.2022491 -4.2086363 -4.2062654 -4.1984744 -4.190702 -4.1835 -4.1806517 -4.1803122 -4.1798763 -4.1832023 -4.1929955 -4.2069292 -4.2206931][-4.1776133 -4.1821132 -4.1874957 -4.1867867 -4.17923 -4.1697407 -4.1631889 -4.1586151 -4.158947 -4.1607804 -4.1629658 -4.1712193 -4.1857319 -4.2031078 -4.2183][-4.1677303 -4.1672049 -4.16702 -4.1570873 -4.1429529 -4.1275082 -4.11524 -4.1114192 -4.1183076 -4.1300111 -4.1411848 -4.1564841 -4.1778736 -4.20049 -4.2188926][-4.1607208 -4.1550579 -4.1476336 -4.1316781 -4.1085815 -4.0787148 -4.0507131 -4.0415277 -4.0603595 -4.091042 -4.1156206 -4.1365685 -4.1635847 -4.1913352 -4.2157083][-4.152369 -4.140429 -4.1306767 -4.1135516 -4.0831676 -4.0292244 -3.9659951 -3.9415536 -3.979095 -4.0386329 -4.085928 -4.1163645 -4.1473765 -4.1795197 -4.2071481][-4.1331496 -4.122057 -4.1158109 -4.1025743 -4.0647073 -3.9831123 -3.8746457 -3.8175609 -3.8787386 -3.9792857 -4.0575552 -4.1050305 -4.1377869 -4.1640882 -4.1866879][-4.101418 -4.0937705 -4.092627 -4.0886049 -4.0541568 -3.9666219 -3.8381257 -3.7559328 -3.8278193 -3.9547615 -4.0539112 -4.11409 -4.14782 -4.1629972 -4.1727166][-4.0674729 -4.0527697 -4.04513 -4.0486407 -4.0408564 -3.9912121 -3.9092777 -3.8536844 -3.8933892 -3.9872172 -4.0747128 -4.134872 -4.1716323 -4.1846995 -4.1862378][-4.0525064 -4.0276651 -4.0103436 -4.0164566 -4.0369554 -4.034121 -4.0085154 -3.9861326 -3.9913013 -4.0301709 -4.0885525 -4.1430688 -4.1820045 -4.199615 -4.2020583][-4.0751815 -4.04928 -4.0278268 -4.0283685 -4.05346 -4.0762935 -4.0837421 -4.081008 -4.0717659 -4.07588 -4.1130924 -4.1595569 -4.194458 -4.20662 -4.2045932][-4.1268196 -4.1055894 -4.0842924 -4.0735965 -4.0855031 -4.111166 -4.1360722 -4.1527476 -4.1504755 -4.1454644 -4.1670051 -4.2014995 -4.2234907 -4.220437 -4.2050834][-4.1802707 -4.1601796 -4.135757 -4.1145782 -4.1099091 -4.1280918 -4.1602268 -4.1932073 -4.2036891 -4.2016869 -4.2130303 -4.2334371 -4.2414 -4.2279625 -4.2063437][-4.21412 -4.19464 -4.16834 -4.1448755 -4.1330681 -4.1423836 -4.1735811 -4.2119808 -4.2309413 -4.2338066 -4.2392235 -4.24988 -4.2512083 -4.2372875 -4.2172403][-4.2360053 -4.2186918 -4.1938214 -4.1719446 -4.1606526 -4.1667032 -4.1927805 -4.2239981 -4.2408109 -4.2463465 -4.2505894 -4.2531652 -4.2508631 -4.241848 -4.2281885]]...]
INFO - root - 2017-12-05 23:55:58.160632: step 51810, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 71h:49m:35s remains)
INFO - root - 2017-12-05 23:56:07.108675: step 51820, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 69h:27m:22s remains)
INFO - root - 2017-12-05 23:56:16.306356: step 51830, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 69h:29m:16s remains)
INFO - root - 2017-12-05 23:56:25.335895: step 51840, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 69h:55m:32s remains)
INFO - root - 2017-12-05 23:56:34.465268: step 51850, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.942 sec/batch; 73h:28m:30s remains)
INFO - root - 2017-12-05 23:56:43.663213: step 51860, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 70h:33m:27s remains)
INFO - root - 2017-12-05 23:56:52.652297: step 51870, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 71h:43m:02s remains)
INFO - root - 2017-12-05 23:57:01.780229: step 51880, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 72h:06m:49s remains)
INFO - root - 2017-12-05 23:57:10.984091: step 51890, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.929 sec/batch; 72h:22m:35s remains)
INFO - root - 2017-12-05 23:57:20.016604: step 51900, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 68h:41m:50s remains)
2017-12-05 23:57:20.820889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1974859 -4.2058654 -4.2200437 -4.2359195 -4.2530656 -4.2563834 -4.2458324 -4.2412195 -4.2420273 -4.2451005 -4.2475076 -4.2488027 -4.2615 -4.2720017 -4.2801208][-4.1643538 -4.1735168 -4.1914482 -4.2107639 -4.2309246 -4.2386584 -4.2290392 -4.2247314 -4.2286329 -4.2385483 -4.2456365 -4.246737 -4.2613192 -4.2718382 -4.2783537][-4.1420622 -4.15139 -4.1670437 -4.1834674 -4.2022643 -4.2124205 -4.2061205 -4.20458 -4.21398 -4.2332163 -4.2480397 -4.2515817 -4.2674603 -4.2769756 -4.2822576][-4.1406269 -4.1463566 -4.1559362 -4.1622181 -4.1745005 -4.18419 -4.1830468 -4.189621 -4.2073135 -4.2342582 -4.2568936 -4.2639 -4.2782373 -4.2859206 -4.2892046][-4.1481338 -4.1521115 -4.1556845 -4.1509614 -4.1578212 -4.1594391 -4.1553512 -4.1728158 -4.2008271 -4.2359538 -4.2622905 -4.2701383 -4.2812438 -4.2884431 -4.2926664][-4.1201954 -4.1237726 -4.124012 -4.1150908 -4.11619 -4.1066036 -4.0919652 -4.1152096 -4.1579986 -4.2031507 -4.2369113 -4.2525635 -4.2702627 -4.2830825 -4.2902112][-4.0324688 -4.0343418 -4.0323005 -4.0226474 -4.0207195 -4.0007048 -3.9759338 -4.0048938 -4.0692635 -4.1347094 -4.1843681 -4.2182751 -4.2516789 -4.2742386 -4.2858415][-3.9228539 -3.9248075 -3.9259496 -3.916944 -3.9078245 -3.871654 -3.8353767 -3.8770308 -3.9721768 -4.0685697 -4.1434021 -4.1973233 -4.2443328 -4.2741103 -4.2858377][-3.8886902 -3.8855069 -3.8799794 -3.8577542 -3.8308306 -3.7783535 -3.7356057 -3.7849712 -3.8967125 -4.0127368 -4.1091366 -4.1807027 -4.239759 -4.2739363 -4.2855544][-3.9921589 -3.9816775 -3.964278 -3.9310622 -3.8894198 -3.8290646 -3.7819679 -3.81495 -3.9057412 -4.0026569 -4.0891457 -4.1561642 -4.218792 -4.2580323 -4.2732229][-4.1091275 -4.0985374 -4.0798526 -4.05574 -4.0225706 -3.9762077 -3.9389882 -3.9544139 -4.0107923 -4.0723133 -4.1234288 -4.1662669 -4.2128758 -4.2443681 -4.261107][-4.1898274 -4.1797462 -4.1660733 -4.1502247 -4.1243868 -4.09079 -4.0644512 -4.070756 -4.1055303 -4.1440978 -4.1707845 -4.1944523 -4.2251859 -4.2451053 -4.2585416][-4.2370009 -4.2306142 -4.2231522 -4.2138462 -4.1963811 -4.173624 -4.1568975 -4.1582937 -4.1781855 -4.2030144 -4.2162514 -4.2256813 -4.2452583 -4.2552409 -4.2632737][-4.2656808 -4.2631693 -4.259655 -4.2558 -4.247076 -4.2336349 -4.2247748 -4.2258172 -4.236937 -4.2542691 -4.2634616 -4.2662697 -4.2743549 -4.27576 -4.2769585][-4.2734141 -4.2737012 -4.2725577 -4.2728 -4.2699137 -4.2614613 -4.2584567 -4.2644391 -4.2772503 -4.29408 -4.304719 -4.3044395 -4.3052921 -4.30158 -4.2968016]]...]
INFO - root - 2017-12-05 23:57:29.798998: step 51910, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.955 sec/batch; 74h:24m:19s remains)
INFO - root - 2017-12-05 23:57:38.956762: step 51920, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 71h:46m:51s remains)
INFO - root - 2017-12-05 23:57:47.903915: step 51930, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 69h:51m:01s remains)
INFO - root - 2017-12-05 23:57:57.027780: step 51940, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 69h:49m:51s remains)
INFO - root - 2017-12-05 23:58:06.376390: step 51950, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 71h:39m:38s remains)
INFO - root - 2017-12-05 23:58:15.306235: step 51960, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 69h:49m:59s remains)
INFO - root - 2017-12-05 23:58:24.411436: step 51970, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 70h:39m:59s remains)
INFO - root - 2017-12-05 23:58:33.534829: step 51980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 67h:02m:18s remains)
INFO - root - 2017-12-05 23:58:42.549539: step 51990, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 69h:52m:50s remains)
INFO - root - 2017-12-05 23:58:51.637621: step 52000, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 72h:09m:02s remains)
2017-12-05 23:58:52.354734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.333467 -4.3366609 -4.3388419 -4.3407907 -4.3410316 -4.3409133 -4.3397951 -4.3358865 -4.3318424 -4.3299069 -4.3271761 -4.3222947 -4.3161731 -4.311913 -4.31224][-4.3241343 -4.3291678 -4.3320317 -4.3329864 -4.3329415 -4.3326597 -4.3304472 -4.3245864 -4.3179641 -4.3156047 -4.3131294 -4.3097186 -4.3039517 -4.2973814 -4.2946959][-4.3102736 -4.314271 -4.3143139 -4.3126674 -4.3113704 -4.3108931 -4.3092661 -4.3037066 -4.2977734 -4.2963204 -4.295445 -4.2948074 -4.2922349 -4.2869391 -4.2860727][-4.2844572 -4.2856474 -4.2817121 -4.2754183 -4.2715659 -4.2730117 -4.2753515 -4.27321 -4.2730231 -4.2773948 -4.2805161 -4.2810163 -4.2806473 -4.2791424 -4.2809629][-4.2497287 -4.2460766 -4.2354884 -4.2222877 -4.2142119 -4.2167339 -4.2199793 -4.2192111 -4.2256026 -4.24269 -4.2598238 -4.2683492 -4.2718906 -4.2732506 -4.2737823][-4.2251678 -4.2116 -4.1854157 -4.1539607 -4.1300678 -4.1223011 -4.1163831 -4.1099749 -4.1242 -4.1642532 -4.2074847 -4.235456 -4.2513027 -4.259675 -4.2597551][-4.2165961 -4.1928558 -4.1483169 -4.089386 -4.03189 -3.9951191 -3.971168 -3.9545033 -3.9808481 -4.0542011 -4.1338577 -4.1906409 -4.2225266 -4.2395415 -4.2432981][-4.2224793 -4.2013 -4.15575 -4.0821314 -3.994158 -3.91736 -3.86014 -3.834214 -3.8773129 -3.9829249 -4.0923643 -4.1707435 -4.2160497 -4.2410264 -4.24997][-4.2338972 -4.22417 -4.1941314 -4.1379519 -4.0548873 -3.9634588 -3.8815427 -3.8433983 -3.8853412 -3.9881241 -4.0953612 -4.175293 -4.223484 -4.2514734 -4.26202][-4.2387624 -4.2441063 -4.2366586 -4.2089105 -4.1529341 -4.0771241 -3.9986067 -3.9554114 -3.9785953 -4.0538225 -4.1317925 -4.1942058 -4.2335005 -4.2566538 -4.2659044][-4.230144 -4.2479768 -4.2561564 -4.2459588 -4.2141776 -4.1655049 -4.112134 -4.0769997 -4.0822077 -4.1268468 -4.1740737 -4.2128768 -4.2364082 -4.2505307 -4.257906][-4.1944661 -4.2206225 -4.2403965 -4.2455063 -4.2349982 -4.2157021 -4.1936636 -4.1709251 -4.1637034 -4.1787739 -4.1979823 -4.2159972 -4.2263632 -4.2331581 -4.239048][-4.1176543 -4.1581941 -4.1958084 -4.2184505 -4.2263384 -4.2295432 -4.234149 -4.2313766 -4.226943 -4.2237086 -4.2211251 -4.2199788 -4.2176542 -4.219018 -4.2233472][-3.9969852 -4.05668 -4.1248679 -4.1759882 -4.2044168 -4.22448 -4.249157 -4.2673993 -4.2731581 -4.2648034 -4.2475209 -4.2286592 -4.213294 -4.2081866 -4.2112627][-3.8745883 -3.9570217 -4.0601721 -4.1422286 -4.1913433 -4.2229552 -4.2539792 -4.2799773 -4.2951856 -4.2900319 -4.2683592 -4.2377868 -4.2131777 -4.2026544 -4.2043977]]...]
INFO - root - 2017-12-05 23:59:01.538863: step 52010, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 72h:58m:10s remains)
INFO - root - 2017-12-05 23:59:10.514361: step 52020, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 67h:30m:32s remains)
INFO - root - 2017-12-05 23:59:19.474779: step 52030, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.896 sec/batch; 69h:47m:49s remains)
INFO - root - 2017-12-05 23:59:28.507189: step 52040, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 71h:40m:04s remains)
INFO - root - 2017-12-05 23:59:37.478343: step 52050, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.926 sec/batch; 72h:09m:53s remains)
INFO - root - 2017-12-05 23:59:46.532668: step 52060, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.930 sec/batch; 72h:27m:44s remains)
INFO - root - 2017-12-05 23:59:55.755415: step 52070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 72h:04m:28s remains)
INFO - root - 2017-12-06 00:00:04.765774: step 52080, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.905 sec/batch; 70h:28m:29s remains)
INFO - root - 2017-12-06 00:00:13.816298: step 52090, loss = 2.04, batch loss = 1.98 (10.1 examples/sec; 0.795 sec/batch; 61h:57m:28s remains)
INFO - root - 2017-12-06 00:00:23.011371: step 52100, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 69h:34m:18s remains)
2017-12-06 00:00:23.826139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2096982 -4.2424231 -4.2650919 -4.2744641 -4.2727623 -4.268086 -4.2604194 -4.2580314 -4.2703476 -4.2902927 -4.2975488 -4.2915926 -4.2812753 -4.2759061 -4.277267][-4.1563158 -4.2033792 -4.2402177 -4.2574019 -4.2554231 -4.2430353 -4.2285786 -4.2249289 -4.2450838 -4.2746768 -4.2833557 -4.2741132 -4.2583823 -4.2520213 -4.2577305][-4.1081138 -4.1668839 -4.2146974 -4.239953 -4.2361808 -4.211556 -4.1817846 -4.1724434 -4.2013407 -4.2448573 -4.259151 -4.2502742 -4.2294617 -4.2218733 -4.2350178][-4.064908 -4.1326804 -4.191596 -4.2251468 -4.2205472 -4.1814146 -4.1321774 -4.1087189 -4.1400466 -4.1986766 -4.2235737 -4.21426 -4.1867638 -4.1760931 -4.1991496][-4.0304451 -4.1029119 -4.1712027 -4.2120624 -4.2037826 -4.1508384 -4.080296 -4.0337763 -4.0658011 -4.1470652 -4.1889873 -4.178937 -4.1414962 -4.1270061 -4.1609163][-4.0515294 -4.1182523 -4.1837888 -4.2173266 -4.1960735 -4.1236706 -4.0232749 -3.9368052 -3.9634356 -4.0802221 -4.1548347 -4.1515813 -4.1095695 -4.094079 -4.135808][-4.134809 -4.1845584 -4.2306747 -4.2434382 -4.208209 -4.1156268 -3.976176 -3.8314569 -3.8366888 -3.9964211 -4.1166973 -4.1353264 -4.1014681 -4.0870204 -4.1267385][-4.2187433 -4.2504268 -4.2820349 -4.2852654 -4.2439013 -4.1392317 -3.970037 -3.7717743 -3.7356195 -3.9201677 -4.0795741 -4.1294675 -4.1188893 -4.1110339 -4.1439013][-4.2654381 -4.2861152 -4.3107657 -4.3157825 -4.2836041 -4.1889753 -4.0293269 -3.8345196 -3.7708621 -3.9215336 -4.0793839 -4.1484251 -4.1616969 -4.1648788 -4.1882553][-4.2533512 -4.2726874 -4.2992582 -4.3141994 -4.2977753 -4.229362 -4.1073875 -3.9537315 -3.8906286 -3.9912772 -4.1192918 -4.1924176 -4.2186627 -4.2291708 -4.2440233][-4.2231493 -4.2427287 -4.2716956 -4.2970409 -4.2979112 -4.2536426 -4.1669855 -4.0598292 -4.0114679 -4.0776649 -4.1782889 -4.2462053 -4.2766 -4.2912822 -4.3000979][-4.2099752 -4.2268195 -4.252574 -4.2840762 -4.2984 -4.2777014 -4.2251334 -4.1584783 -4.1276007 -4.1686749 -4.2395163 -4.2929478 -4.3208418 -4.3366928 -4.34232][-4.2139931 -4.2291927 -4.250936 -4.2829852 -4.303194 -4.3025517 -4.2791328 -4.2416525 -4.2202368 -4.2404189 -4.284719 -4.3224173 -4.3442492 -4.356348 -4.3574772][-4.2312655 -4.247632 -4.2661686 -4.2919078 -4.31124 -4.3181992 -4.3102832 -4.290668 -4.276669 -4.2839646 -4.3073378 -4.3305988 -4.3461113 -4.3535976 -4.3523607][-4.2558012 -4.2737889 -4.2905636 -4.3080068 -4.3212976 -4.3274841 -4.3254218 -4.3172412 -4.3106236 -4.31301 -4.3225217 -4.3323507 -4.3398294 -4.3431187 -4.3415031]]...]
INFO - root - 2017-12-06 00:00:32.757579: step 52110, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 69h:50m:06s remains)
INFO - root - 2017-12-06 00:00:41.852436: step 52120, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 70h:51m:43s remains)
INFO - root - 2017-12-06 00:00:50.919741: step 52130, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 70h:16m:51s remains)
INFO - root - 2017-12-06 00:00:59.928246: step 52140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 65h:35m:47s remains)
INFO - root - 2017-12-06 00:01:08.995334: step 52150, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.890 sec/batch; 69h:16m:55s remains)
INFO - root - 2017-12-06 00:01:18.122125: step 52160, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 71h:54m:26s remains)
INFO - root - 2017-12-06 00:01:27.159054: step 52170, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 70h:59m:11s remains)
INFO - root - 2017-12-06 00:01:36.306444: step 52180, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 70h:06m:26s remains)
INFO - root - 2017-12-06 00:01:45.414907: step 52190, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 71h:23m:56s remains)
INFO - root - 2017-12-06 00:01:54.503353: step 52200, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 71h:43m:55s remains)
2017-12-06 00:01:55.280126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2069826 -4.2070618 -4.2169037 -4.2219243 -4.2192111 -4.2130203 -4.2049646 -4.2046332 -4.2167511 -4.2272787 -4.2333665 -4.2385583 -4.2437592 -4.2432833 -4.2453852][-4.1813011 -4.1884 -4.1981344 -4.2038713 -4.1984558 -4.185462 -4.1684885 -4.1550069 -4.165751 -4.1872287 -4.2058363 -4.2222867 -4.2339783 -4.2315822 -4.2308474][-4.1716232 -4.18167 -4.1881433 -4.1905866 -4.1822619 -4.1607747 -4.1310792 -4.1123013 -4.1311111 -4.1653461 -4.1888332 -4.2037868 -4.2210875 -4.2205825 -4.2192845][-4.164238 -4.176681 -4.1804113 -4.1784663 -4.1697593 -4.1433816 -4.0995278 -4.0773859 -4.1141796 -4.167172 -4.1905751 -4.2022052 -4.2184472 -4.2189522 -4.2144623][-4.1743293 -4.187294 -4.1857028 -4.1762152 -4.1599855 -4.1196008 -4.051302 -4.0177455 -4.0759168 -4.15616 -4.190361 -4.202528 -4.215879 -4.217536 -4.2097063][-4.1928415 -4.2084937 -4.2038846 -4.1837358 -4.15352 -4.0912976 -3.9827704 -3.9213266 -4.005095 -4.1221714 -4.1755953 -4.189548 -4.2055326 -4.2114315 -4.2031059][-4.215055 -4.2354927 -4.2281575 -4.1991086 -4.1544743 -4.0669184 -3.9230247 -3.8260794 -3.9331977 -4.0829639 -4.148056 -4.1612835 -4.180191 -4.19292 -4.1884208][-4.2367029 -4.2555318 -4.2436271 -4.20996 -4.1581345 -4.0613194 -3.9164312 -3.8172486 -3.9132676 -4.066555 -4.1339426 -4.1418123 -4.1553559 -4.1674585 -4.1645594][-4.2482224 -4.2606068 -4.2422338 -4.2081656 -4.1614022 -4.078887 -3.9697649 -3.8994944 -3.9676733 -4.0887604 -4.1483312 -4.148561 -4.1553121 -4.1603289 -4.154985][-4.2489238 -4.2534585 -4.2325373 -4.1994567 -4.1623316 -4.1025205 -4.0309458 -3.9912961 -4.0376348 -4.1225853 -4.1706061 -4.1670809 -4.1666932 -4.1674819 -4.15682][-4.2421923 -4.2398987 -4.2208161 -4.1918511 -4.1631985 -4.1242142 -4.0814924 -4.0635009 -4.0976577 -4.1553593 -4.1893864 -4.1842809 -4.18052 -4.1800289 -4.1680942][-4.2346087 -4.2277241 -4.2140937 -4.195694 -4.178421 -4.1534381 -4.1276073 -4.1196375 -4.1434755 -4.1817312 -4.2029262 -4.1970453 -4.1910625 -4.1907368 -4.182765][-4.2369232 -4.2283611 -4.21711 -4.205689 -4.19632 -4.1809297 -4.1659279 -4.1632128 -4.17722 -4.2021203 -4.2154675 -4.2084885 -4.201107 -4.2031422 -4.2052479][-4.2518234 -4.244442 -4.237349 -4.2317924 -4.2263422 -4.2175837 -4.209147 -4.2077336 -4.2154064 -4.2298007 -4.2397747 -4.2364826 -4.2306323 -4.2336893 -4.2408757][-4.271956 -4.2674909 -4.2650523 -4.2638168 -4.2623172 -4.2589879 -4.2551417 -4.2542825 -4.2571812 -4.26387 -4.2691083 -4.2676349 -4.2643118 -4.2659044 -4.272778]]...]
INFO - root - 2017-12-06 00:02:04.335307: step 52210, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 70h:52m:47s remains)
INFO - root - 2017-12-06 00:02:13.554922: step 52220, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 72h:11m:00s remains)
INFO - root - 2017-12-06 00:02:22.548584: step 52230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 65h:55m:12s remains)
INFO - root - 2017-12-06 00:02:31.683241: step 52240, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 65h:38m:20s remains)
INFO - root - 2017-12-06 00:02:40.534549: step 52250, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 70h:53m:57s remains)
INFO - root - 2017-12-06 00:02:49.735041: step 52260, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 71h:07m:34s remains)
INFO - root - 2017-12-06 00:02:58.817385: step 52270, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 69h:15m:20s remains)
INFO - root - 2017-12-06 00:03:07.887931: step 52280, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 72h:11m:56s remains)
INFO - root - 2017-12-06 00:03:17.015745: step 52290, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 70h:23m:04s remains)
INFO - root - 2017-12-06 00:03:26.019317: step 52300, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 69h:48m:55s remains)
2017-12-06 00:03:26.927940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3511424 -4.3393893 -4.333005 -4.3317432 -4.3339167 -4.3368282 -4.3375716 -4.3397117 -4.3438177 -4.3467846 -4.3449926 -4.3428035 -4.3448558 -4.350234 -4.3559771][-4.3361735 -4.317667 -4.3074708 -4.3049045 -4.3082342 -4.3139462 -4.3170366 -4.3203907 -4.3278284 -4.3369489 -4.3378124 -4.3348374 -4.3349719 -4.3390245 -4.3444743][-4.3131156 -4.2873821 -4.2714448 -4.264389 -4.2650409 -4.2733011 -4.2793646 -4.2835174 -4.2927451 -4.3096428 -4.3202424 -4.3233285 -4.3224411 -4.3247418 -4.329361][-4.28656 -4.250464 -4.2252769 -4.2105613 -4.2048564 -4.2176547 -4.2270365 -4.2285953 -4.234911 -4.2591214 -4.2883515 -4.307982 -4.313252 -4.3139486 -4.3158035][-4.2615476 -4.2080135 -4.1670861 -4.1360264 -4.1139669 -4.1294112 -4.1405196 -4.13519 -4.1375546 -4.1737108 -4.2284813 -4.2700577 -4.2916589 -4.2978635 -4.3005505][-4.224669 -4.1450653 -4.0776896 -4.020997 -3.97537 -3.9910381 -4.0070658 -3.9908395 -3.9854989 -4.0416446 -4.1280813 -4.1991644 -4.2437034 -4.2697186 -4.281857][-4.1769824 -4.0683279 -3.9777789 -3.9030414 -3.8438764 -3.8608797 -3.88186 -3.8495831 -3.8276885 -3.89856 -4.0052361 -4.0985546 -4.166883 -4.2181873 -4.250988][-4.1580129 -4.0412664 -3.9500315 -3.873219 -3.8109128 -3.8256316 -3.8471482 -3.8082824 -3.7798848 -3.8492706 -3.9423687 -4.0329471 -4.1090674 -4.1749492 -4.2234373][-4.1829348 -4.086904 -4.0108314 -3.9480948 -3.8951161 -3.9005818 -3.9144056 -3.883677 -3.8635993 -3.9164057 -3.9794531 -4.0455403 -4.1061678 -4.166069 -4.2126827][-4.2269535 -4.1628971 -4.10562 -4.0629058 -4.0265603 -4.0215273 -4.0302596 -4.0160356 -4.0023966 -4.0319629 -4.07321 -4.118372 -4.1562891 -4.1954136 -4.2265549][-4.2678185 -4.2321076 -4.19945 -4.1776443 -4.1588845 -4.1511354 -4.1544185 -4.1478009 -4.13304 -4.1451669 -4.1748033 -4.208137 -4.2293053 -4.2479725 -4.26173][-4.2959571 -4.2793202 -4.2674923 -4.2631631 -4.2601342 -4.2539892 -4.25203 -4.246748 -4.232461 -4.235168 -4.2584743 -4.283432 -4.2952127 -4.302897 -4.3048372][-4.3187008 -4.3091359 -4.3065376 -4.3097577 -4.3121171 -4.3067789 -4.3008718 -4.2978411 -4.291986 -4.2944908 -4.3114095 -4.3289142 -4.3374753 -4.3423357 -4.3404093][-4.34082 -4.3346744 -4.3347874 -4.3383245 -4.3397703 -4.3341036 -4.3266869 -4.3244195 -4.3239875 -4.3270044 -4.3395457 -4.3530059 -4.3602057 -4.3629603 -4.3595266][-4.3584471 -4.3531561 -4.3525238 -4.353704 -4.3527784 -4.3476276 -4.341054 -4.3370366 -4.3368158 -4.3396368 -4.3477058 -4.356957 -4.36236 -4.3650656 -4.3639369]]...]
INFO - root - 2017-12-06 00:03:35.962652: step 52310, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 68h:01m:38s remains)
INFO - root - 2017-12-06 00:03:44.875227: step 52320, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 69h:28m:26s remains)
INFO - root - 2017-12-06 00:03:53.957484: step 52330, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 72h:01m:05s remains)
INFO - root - 2017-12-06 00:04:02.761274: step 52340, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 69h:52m:43s remains)
INFO - root - 2017-12-06 00:04:11.717271: step 52350, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 69h:03m:34s remains)
INFO - root - 2017-12-06 00:04:20.702295: step 52360, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 68h:20m:27s remains)
INFO - root - 2017-12-06 00:04:29.697991: step 52370, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 64h:08m:28s remains)
INFO - root - 2017-12-06 00:04:38.699316: step 52380, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 67h:37m:33s remains)
INFO - root - 2017-12-06 00:04:47.778571: step 52390, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.944 sec/batch; 73h:27m:58s remains)
INFO - root - 2017-12-06 00:04:56.722186: step 52400, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 69h:14m:03s remains)
2017-12-06 00:04:57.546362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1808114 -4.15034 -4.1266651 -4.1151419 -4.1056442 -4.0977345 -4.0907369 -4.0779514 -4.0635724 -4.0650291 -4.1021843 -4.1369977 -4.1462951 -4.1465511 -4.1359787][-4.1708446 -4.1384487 -4.1137381 -4.1043944 -4.0930066 -4.0748882 -4.0580745 -4.0413761 -4.0230808 -4.0282393 -4.0728574 -4.1087894 -4.1135259 -4.1133428 -4.1040831][-4.1642389 -4.1351023 -4.1161542 -4.1137352 -4.1063933 -4.080996 -4.05318 -4.0282631 -4.0054035 -4.0137291 -4.0577769 -4.092895 -4.1020322 -4.103673 -4.0927815][-4.1583943 -4.1356263 -4.1228848 -4.127492 -4.1287785 -4.1050391 -4.0699673 -4.0374489 -4.0134339 -4.017787 -4.0514946 -4.0819697 -4.0980091 -4.1041369 -4.0891037][-4.1601849 -4.140192 -4.1290555 -4.1356077 -4.1418853 -4.1207438 -4.0849433 -4.0538697 -4.0247707 -4.0190454 -4.047936 -4.0768161 -4.0986137 -4.1089549 -4.0935574][-4.1767197 -4.1596537 -4.1475916 -4.148088 -4.1482182 -4.1240311 -4.0873051 -4.0587058 -4.02709 -4.0104127 -4.0367374 -4.0743718 -4.1038966 -4.115078 -4.0989323][-4.1985264 -4.186923 -4.1711221 -4.1642509 -4.1496563 -4.1195345 -4.0838842 -4.0571165 -4.0260296 -3.9978831 -4.0198669 -4.0764575 -4.1196389 -4.1326523 -4.1130114][-4.2130432 -4.2050781 -4.1901402 -4.1758013 -4.1533117 -4.1267095 -4.095089 -4.0680451 -4.0317259 -3.9922781 -4.0130849 -4.0873218 -4.1441889 -4.1598768 -4.1413703][-4.2197375 -4.2124152 -4.2025142 -4.1883855 -4.1688962 -4.1544151 -4.1307588 -4.1041527 -4.0691156 -4.0328674 -4.0494003 -4.119288 -4.1713219 -4.1860924 -4.1763988][-4.2219925 -4.2145228 -4.2063284 -4.198957 -4.1920576 -4.1892095 -4.1723475 -4.14807 -4.1248808 -4.1039386 -4.1182404 -4.1683841 -4.2041984 -4.213944 -4.2069654][-4.2287974 -4.2215867 -4.2127676 -4.2027707 -4.1990356 -4.2048745 -4.1958327 -4.1791286 -4.1699424 -4.1641483 -4.1767111 -4.2072358 -4.2281122 -4.2324257 -4.2257733][-4.2347212 -4.2276764 -4.220758 -4.2090416 -4.2036591 -4.2096906 -4.2041421 -4.192863 -4.1929541 -4.1963468 -4.209166 -4.2286448 -4.2421188 -4.2457209 -4.2419705][-4.2465787 -4.2390442 -4.2347207 -4.2265844 -4.2237816 -4.227335 -4.2230878 -4.2169981 -4.2188244 -4.2251353 -4.2376518 -4.2527628 -4.2639365 -4.2689066 -4.2680235][-4.2756276 -4.2696824 -4.2673411 -4.2635388 -4.2626595 -4.2647395 -4.2618532 -4.2588949 -4.2597656 -4.2656388 -4.2758751 -4.2877712 -4.2976665 -4.3026829 -4.3017845][-4.3035975 -4.2988119 -4.2968645 -4.29577 -4.2978635 -4.3016419 -4.3014874 -4.2997603 -4.2997603 -4.3035927 -4.3103366 -4.3183789 -4.3246188 -4.3275757 -4.3267756]]...]
INFO - root - 2017-12-06 00:05:06.628977: step 52410, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 70h:05m:00s remains)
INFO - root - 2017-12-06 00:05:15.795281: step 52420, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 72h:08m:26s remains)
INFO - root - 2017-12-06 00:05:25.023988: step 52430, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.954 sec/batch; 74h:14m:47s remains)
INFO - root - 2017-12-06 00:05:33.945829: step 52440, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 67h:30m:18s remains)
INFO - root - 2017-12-06 00:05:42.951854: step 52450, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 68h:36m:56s remains)
INFO - root - 2017-12-06 00:05:52.112704: step 52460, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 71h:22m:43s remains)
INFO - root - 2017-12-06 00:06:01.073267: step 52470, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 68h:33m:35s remains)
INFO - root - 2017-12-06 00:06:10.170915: step 52480, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.927 sec/batch; 72h:06m:06s remains)
INFO - root - 2017-12-06 00:06:19.358479: step 52490, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 71h:43m:46s remains)
INFO - root - 2017-12-06 00:06:28.383223: step 52500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 69h:00m:13s remains)
2017-12-06 00:06:29.156700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2255106 -4.2382936 -4.2389994 -4.2360067 -4.2377405 -4.2383704 -4.2395935 -4.2454591 -4.2557268 -4.2725878 -4.279748 -4.2721863 -4.2450852 -4.2055979 -4.1760244][-4.2246275 -4.2366896 -4.2373042 -4.2365394 -4.2369027 -4.2349987 -4.2374139 -4.244895 -4.2559681 -4.2710323 -4.2789683 -4.2739215 -4.2520828 -4.2194428 -4.1932144][-4.2142992 -4.2222223 -4.2243361 -4.224565 -4.2251453 -4.2241745 -4.2260346 -4.2359109 -4.2513604 -4.2696271 -4.2814007 -4.2765121 -4.2558527 -4.2257714 -4.2023444][-4.2065749 -4.2106752 -4.2111154 -4.2119231 -4.2120466 -4.210247 -4.207377 -4.2136312 -4.233252 -4.259697 -4.2794733 -4.279892 -4.2614727 -4.231617 -4.2093472][-4.196445 -4.2016826 -4.20412 -4.2054205 -4.2037272 -4.1965065 -4.1792688 -4.1696582 -4.1886215 -4.2294092 -4.263391 -4.2697659 -4.2550712 -4.2266755 -4.2068086][-4.1902862 -4.1971273 -4.2027259 -4.2033873 -4.193923 -4.1712646 -4.1291442 -4.0914912 -4.1071963 -4.1700172 -4.2260056 -4.2421107 -4.2318115 -4.2081008 -4.1941175][-4.186038 -4.1937857 -4.2005544 -4.1992917 -4.1773744 -4.1315336 -4.0571408 -3.9861393 -4.0003061 -4.0905662 -4.1710181 -4.1969609 -4.1905265 -4.1740985 -4.1677666][-4.1913524 -4.2019544 -4.2052317 -4.1956763 -4.1610489 -4.0916376 -3.9900892 -3.8990116 -3.9172025 -4.0302258 -4.124692 -4.1542273 -4.1499734 -4.1407 -4.1402063][-4.2192 -4.2297158 -4.226109 -4.2058134 -4.1646709 -4.0907612 -3.9878032 -3.9051042 -3.9270554 -4.0339022 -4.1153455 -4.1352849 -4.1278191 -4.1194105 -4.1210942][-4.2535791 -4.2549286 -4.2433805 -4.2219758 -4.1894255 -4.1348972 -4.057416 -4.0012884 -4.0185003 -4.0918841 -4.1421132 -4.1436086 -4.1270432 -4.1148367 -4.1114316][-4.2795281 -4.2715168 -4.2565579 -4.2402425 -4.22288 -4.1939659 -4.1472692 -4.1158466 -4.1256886 -4.1632829 -4.183187 -4.1706452 -4.1503296 -4.1349254 -4.1250906][-4.2892895 -4.2742214 -4.2623739 -4.2562351 -4.2526588 -4.2432775 -4.2211947 -4.2060995 -4.2078738 -4.2198429 -4.2200017 -4.2012706 -4.1837778 -4.1699247 -4.1583152][-4.27383 -4.2563577 -4.2510915 -4.2545252 -4.2604003 -4.2626243 -4.260354 -4.25721 -4.2535663 -4.249311 -4.2363772 -4.215858 -4.2041845 -4.1935859 -4.1821718][-4.2443032 -4.2277508 -4.2318182 -4.243094 -4.2514668 -4.2575603 -4.2639437 -4.26819 -4.2627387 -4.2500725 -4.2318916 -4.2111044 -4.2019486 -4.192533 -4.1816645][-4.2155375 -4.200583 -4.2142315 -4.2344728 -4.2468257 -4.2541146 -4.2596922 -4.2619848 -4.2496719 -4.2304225 -4.2102966 -4.1908774 -4.1801257 -4.1708555 -4.1664281]]...]
INFO - root - 2017-12-06 00:06:38.125066: step 52510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 66h:04m:12s remains)
INFO - root - 2017-12-06 00:06:47.394387: step 52520, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 72h:53m:54s remains)
INFO - root - 2017-12-06 00:06:56.352158: step 52530, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 70h:28m:13s remains)
INFO - root - 2017-12-06 00:07:05.358510: step 52540, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.877 sec/batch; 68h:13m:01s remains)
INFO - root - 2017-12-06 00:07:14.314668: step 52550, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 70h:43m:30s remains)
INFO - root - 2017-12-06 00:07:23.427734: step 52560, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 71h:59m:12s remains)
INFO - root - 2017-12-06 00:07:32.595898: step 52570, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 68h:12m:27s remains)
INFO - root - 2017-12-06 00:07:41.543201: step 52580, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 71h:58m:00s remains)
INFO - root - 2017-12-06 00:07:50.605559: step 52590, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.887 sec/batch; 68h:59m:21s remains)
INFO - root - 2017-12-06 00:07:59.591522: step 52600, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 68h:42m:30s remains)
2017-12-06 00:08:00.401668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2936835 -4.297389 -4.2908187 -4.2736206 -4.2614837 -4.2579012 -4.2631211 -4.26838 -4.2758822 -4.2799425 -4.2739754 -4.2715459 -4.28096 -4.2965927 -4.3058624][-4.2952728 -4.2993073 -4.2913704 -4.27412 -4.2634177 -4.2601662 -4.2639494 -4.269505 -4.2784023 -4.2843385 -4.2784157 -4.2731824 -4.2789168 -4.29125 -4.2981052][-4.2958803 -4.2989244 -4.2905593 -4.275744 -4.2703047 -4.2692928 -4.2699041 -4.2711892 -4.2773008 -4.2827907 -4.2781072 -4.2719526 -4.274168 -4.2824674 -4.2879066][-4.2962255 -4.2981348 -4.2907295 -4.2801828 -4.279489 -4.2787695 -4.2741785 -4.26912 -4.2735825 -4.2796412 -4.2782755 -4.2729139 -4.2724781 -4.2778182 -4.2824945][-4.2974582 -4.2988234 -4.2935925 -4.287622 -4.2865906 -4.2809677 -4.2684426 -4.2555008 -4.257566 -4.2658529 -4.2720232 -4.2721939 -4.27285 -4.27821 -4.2835469][-4.2970371 -4.2986679 -4.2963724 -4.2941847 -4.289712 -4.2744346 -4.2509913 -4.2294669 -4.2288208 -4.2416406 -4.2569642 -4.26657 -4.2732253 -4.2815003 -4.2878113][-4.2934093 -4.2972369 -4.2983651 -4.2997732 -4.2917356 -4.2646751 -4.2271209 -4.1978636 -4.198112 -4.2171388 -4.2396288 -4.257339 -4.2708564 -4.2838888 -4.2906504][-4.2909651 -4.2964163 -4.2989311 -4.3019128 -4.2925897 -4.2582631 -4.2130246 -4.1817017 -4.1844306 -4.2060928 -4.2289429 -4.2490225 -4.2659473 -4.283937 -4.2907667][-4.2929506 -4.2969818 -4.2977114 -4.3005466 -4.291862 -4.25924 -4.2201591 -4.193758 -4.193337 -4.2053933 -4.2195811 -4.2359276 -4.2538619 -4.2770271 -4.2860909][-4.2992778 -4.3017478 -4.3000631 -4.3017607 -4.2957368 -4.27022 -4.2412076 -4.2207308 -4.2115912 -4.2070909 -4.2038274 -4.2104449 -4.2314386 -4.2624044 -4.27815][-4.3065662 -4.3101869 -4.3062611 -4.3057346 -4.3000011 -4.2806783 -4.2602892 -4.2452703 -4.23116 -4.21175 -4.1912045 -4.1877832 -4.2104864 -4.2485714 -4.2719207][-4.3099222 -4.3171272 -4.3111291 -4.3048582 -4.2947359 -4.2779713 -4.2643313 -4.2567458 -4.2463026 -4.2208 -4.189249 -4.17817 -4.201479 -4.2437019 -4.2716818][-4.3120422 -4.3195491 -4.3101673 -4.2968564 -4.2799835 -4.2639222 -4.2570229 -4.2593012 -4.2548056 -4.2315712 -4.2000713 -4.1892781 -4.2124305 -4.2516956 -4.2775168][-4.3123288 -4.3163238 -4.3045382 -4.2860661 -4.2638292 -4.2490163 -4.2494369 -4.26082 -4.2625661 -4.246861 -4.2225966 -4.2136893 -4.2313452 -4.2620091 -4.2826166][-4.3110785 -4.3124933 -4.3002291 -4.2783108 -4.2515645 -4.2379417 -4.2441092 -4.2616324 -4.2683606 -4.2578316 -4.237637 -4.2287297 -4.2390509 -4.2599664 -4.2756391]]...]
INFO - root - 2017-12-06 00:08:09.653247: step 52610, loss = 2.09, batch loss = 2.04 (8.1 examples/sec; 0.985 sec/batch; 76h:35m:55s remains)
INFO - root - 2017-12-06 00:08:18.790879: step 52620, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 71h:00m:49s remains)
INFO - root - 2017-12-06 00:08:27.980769: step 52630, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 71h:42m:53s remains)
INFO - root - 2017-12-06 00:08:36.894882: step 52640, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 71h:03m:24s remains)
INFO - root - 2017-12-06 00:08:45.903388: step 52650, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.906 sec/batch; 70h:27m:21s remains)
INFO - root - 2017-12-06 00:08:55.129275: step 52660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 71h:38m:43s remains)
INFO - root - 2017-12-06 00:09:04.086501: step 52670, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 70h:03m:24s remains)
INFO - root - 2017-12-06 00:09:13.090562: step 52680, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 71h:02m:16s remains)
INFO - root - 2017-12-06 00:09:22.162648: step 52690, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 69h:29m:30s remains)
INFO - root - 2017-12-06 00:09:31.131852: step 52700, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 69h:33m:30s remains)
2017-12-06 00:09:31.929290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2630777 -4.2611785 -4.2592716 -4.2583218 -4.2545533 -4.2513 -4.2503834 -4.258441 -4.2688017 -4.2809782 -4.2874951 -4.2862954 -4.2822428 -4.278718 -4.2752905][-4.2878003 -4.2868519 -4.2834764 -4.2784276 -4.2678227 -4.2543125 -4.242599 -4.2451262 -4.2566257 -4.2756739 -4.2924037 -4.300457 -4.3027511 -4.3004065 -4.2940907][-4.3022013 -4.299458 -4.2911983 -4.2800989 -4.2603269 -4.236753 -4.2161422 -4.2096834 -4.2197657 -4.2466116 -4.2765059 -4.2991252 -4.3135257 -4.3194695 -4.3147211][-4.3004746 -4.2953792 -4.2784915 -4.257885 -4.2299938 -4.2027936 -4.1817212 -4.173439 -4.1860261 -4.2176843 -4.2568088 -4.2898746 -4.3126917 -4.325511 -4.3244596][-4.284543 -4.272738 -4.2469039 -4.2193279 -4.1921225 -4.1714244 -4.1606183 -4.1621437 -4.1811314 -4.2129169 -4.2482319 -4.2782516 -4.3000159 -4.3147726 -4.3175659][-4.2615981 -4.2422891 -4.2118087 -4.1861777 -4.1672134 -4.1580033 -4.1597962 -4.1730156 -4.19852 -4.226913 -4.2488341 -4.2624788 -4.2746792 -4.2871728 -4.2914186][-4.2342486 -4.2120876 -4.1845336 -4.1662889 -4.1570954 -4.1566839 -4.1673579 -4.1888714 -4.2155738 -4.2373948 -4.244206 -4.2405605 -4.2420506 -4.2516875 -4.256855][-4.2086878 -4.189188 -4.16925 -4.1581044 -4.1552129 -4.1589632 -4.1732092 -4.1951022 -4.2179132 -4.2317615 -4.2290359 -4.2178078 -4.2171731 -4.2285385 -4.2363796][-4.1896343 -4.170835 -4.1554289 -4.1470308 -4.14297 -4.1451392 -4.157259 -4.1745286 -4.1928005 -4.2014933 -4.1964226 -4.1861987 -4.1883907 -4.2035966 -4.2174625][-4.1896057 -4.1746478 -4.1649518 -4.1581 -4.1493478 -4.1438403 -4.1466041 -4.1546321 -4.1652074 -4.1717534 -4.1690545 -4.1619406 -4.1674743 -4.1876192 -4.21032][-4.2127566 -4.2026486 -4.198184 -4.1929641 -4.180871 -4.1679511 -4.161437 -4.1636558 -4.1695552 -4.1766143 -4.1782069 -4.1737866 -4.1784835 -4.1985464 -4.2236652][-4.2500215 -4.2410769 -4.2386365 -4.2372952 -4.2280645 -4.2150025 -4.2054892 -4.2042346 -4.2069268 -4.2125177 -4.2146006 -4.2105384 -4.2113008 -4.223455 -4.240932][-4.2810807 -4.2755933 -4.2753258 -4.2791872 -4.2762265 -4.2681432 -4.2603211 -4.2572184 -4.2577496 -4.2597933 -4.2602491 -4.2557449 -4.2534003 -4.2572336 -4.2640524][-4.299664 -4.3005204 -4.3043594 -4.3119121 -4.3134389 -4.3093605 -4.3035374 -4.3004241 -4.3002067 -4.300961 -4.3008804 -4.2969532 -4.2927642 -4.2910542 -4.2909527][-4.3087091 -4.3122635 -4.3178811 -4.3250165 -4.3267455 -4.3231955 -4.3187842 -4.3167739 -4.316381 -4.3169541 -4.3171697 -4.3147902 -4.3113608 -4.3079524 -4.3060746]]...]
INFO - root - 2017-12-06 00:09:40.977386: step 52710, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 63h:22m:35s remains)
INFO - root - 2017-12-06 00:09:50.062171: step 52720, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 70h:22m:31s remains)
INFO - root - 2017-12-06 00:09:59.236265: step 52730, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 70h:51m:26s remains)
INFO - root - 2017-12-06 00:10:08.304247: step 52740, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 71h:41m:09s remains)
INFO - root - 2017-12-06 00:10:17.089485: step 52750, loss = 2.09, batch loss = 2.03 (8.5 examples/sec; 0.940 sec/batch; 73h:00m:32s remains)
INFO - root - 2017-12-06 00:10:26.334314: step 52760, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 72h:45m:12s remains)
INFO - root - 2017-12-06 00:10:35.407093: step 52770, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 70h:06m:37s remains)
INFO - root - 2017-12-06 00:10:44.371673: step 52780, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 70h:39m:27s remains)
INFO - root - 2017-12-06 00:10:53.381510: step 52790, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 70h:30m:28s remains)
INFO - root - 2017-12-06 00:11:02.578718: step 52800, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 72h:01m:47s remains)
2017-12-06 00:11:03.382215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3204203 -4.2959895 -4.2700214 -4.255188 -4.2485957 -4.246707 -4.2558317 -4.2806997 -4.2984376 -4.2988033 -4.2932949 -4.2935061 -4.2889943 -4.2750864 -4.2648072][-4.3209648 -4.2928944 -4.2623706 -4.2425871 -4.2294912 -4.2226596 -4.2318578 -4.2612638 -4.2876673 -4.2946982 -4.2895279 -4.2852497 -4.2776794 -4.2586837 -4.2423563][-4.3173423 -4.2876658 -4.2536874 -4.2257304 -4.2056365 -4.1957879 -4.2073064 -4.2402568 -4.2732067 -4.2861481 -4.2837615 -4.2765489 -4.2620354 -4.2362976 -4.2132788][-4.310122 -4.2780881 -4.2412109 -4.206327 -4.1780896 -4.1596971 -4.1632452 -4.1953583 -4.2370896 -4.2617316 -4.2674708 -4.2616911 -4.2433224 -4.2151031 -4.1886711][-4.3031363 -4.2678313 -4.2274132 -4.1861234 -4.1524286 -4.11957 -4.1023164 -4.1216941 -4.1731358 -4.215539 -4.2312312 -4.2288122 -4.2144151 -4.1952848 -4.1782646][-4.2999125 -4.2610822 -4.212678 -4.16002 -4.11408 -4.0611272 -4.014163 -4.0129232 -4.0840063 -4.1556106 -4.1837444 -4.1847076 -4.1749463 -4.1654463 -4.1610761][-4.2997179 -4.2578754 -4.2012477 -4.1366425 -4.075192 -3.9925568 -3.8972912 -3.856662 -3.9541173 -4.0677624 -4.1229243 -4.1351876 -4.1310921 -4.1262207 -4.1294618][-4.2990913 -4.2555871 -4.1958423 -4.1291866 -4.0673256 -3.9758306 -3.8540635 -3.7750311 -3.8655634 -3.9931421 -4.061934 -4.0856652 -4.0928478 -4.0932121 -4.0959897][-4.2969737 -4.25335 -4.1929169 -4.1294022 -4.0797362 -4.0119381 -3.9257486 -3.8714831 -3.9214747 -4.0091848 -4.0572162 -4.0742106 -4.0832634 -4.0828171 -4.0775361][-4.2990351 -4.2591968 -4.2016149 -4.1415415 -4.0998387 -4.0522289 -4.0021272 -3.9779873 -4.0081644 -4.067873 -4.1030788 -4.1206293 -4.1317997 -4.1243181 -4.10086][-4.3035488 -4.270308 -4.2219639 -4.1690683 -4.1333203 -4.0947328 -4.0585251 -4.0473585 -4.0634356 -4.1044083 -4.1382446 -4.1628933 -4.1765823 -4.1638637 -4.1298318][-4.3066592 -4.2785778 -4.2398634 -4.194665 -4.1652842 -4.1365643 -4.1102009 -4.1033893 -4.1046004 -4.1273465 -4.1555796 -4.1806569 -4.1884704 -4.1728826 -4.1403503][-4.3091493 -4.2857461 -4.2555633 -4.2193503 -4.1958318 -4.1759119 -4.1583514 -4.1544237 -4.1519408 -4.1627951 -4.183322 -4.2031422 -4.205843 -4.1903677 -4.1610689][-4.3139524 -4.2964745 -4.2736487 -4.2455888 -4.2278252 -4.2136788 -4.2029634 -4.2001953 -4.1971717 -4.20117 -4.212894 -4.225522 -4.2278223 -4.2188206 -4.1949105][-4.3174944 -4.303309 -4.2859368 -4.2655263 -4.25312 -4.2443638 -4.2391944 -4.2371721 -4.2326841 -4.2313471 -4.2344203 -4.2397318 -4.243351 -4.2414746 -4.2267809]]...]
INFO - root - 2017-12-06 00:11:12.336182: step 52810, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 68h:14m:56s remains)
INFO - root - 2017-12-06 00:11:21.227440: step 52820, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 69h:43m:14s remains)
INFO - root - 2017-12-06 00:11:30.307513: step 52830, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.926 sec/batch; 71h:58m:11s remains)
INFO - root - 2017-12-06 00:11:39.341805: step 52840, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 70h:29m:13s remains)
INFO - root - 2017-12-06 00:11:48.272059: step 52850, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.907 sec/batch; 70h:29m:40s remains)
INFO - root - 2017-12-06 00:11:57.253794: step 52860, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.905 sec/batch; 70h:18m:16s remains)
INFO - root - 2017-12-06 00:12:06.381792: step 52870, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 70h:58m:47s remains)
INFO - root - 2017-12-06 00:12:15.473965: step 52880, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 68h:37m:38s remains)
INFO - root - 2017-12-06 00:12:24.441441: step 52890, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 69h:12m:05s remains)
INFO - root - 2017-12-06 00:12:33.372848: step 52900, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 70h:31m:44s remains)
2017-12-06 00:12:34.125700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3070259 -4.3019981 -4.28458 -4.2559223 -4.2221065 -4.175621 -4.142899 -4.1546326 -4.1890535 -4.2194829 -4.2388577 -4.2394543 -4.2388024 -4.2405357 -4.2355585][-4.3001513 -4.2957578 -4.2798271 -4.250073 -4.2072573 -4.1514888 -4.1064892 -4.1062365 -4.1397018 -4.1823659 -4.2123909 -4.2202444 -4.2213769 -4.2221317 -4.2204518][-4.2926531 -4.287014 -4.269855 -4.2388186 -4.194 -4.1377048 -4.08397 -4.0675497 -4.089396 -4.1336536 -4.1687841 -4.1863112 -4.1925974 -4.1964784 -4.2002592][-4.2896833 -4.2839766 -4.2649245 -4.2303686 -4.1817212 -4.1225843 -4.0591955 -4.0253124 -4.031805 -4.071034 -4.1065183 -4.1344209 -4.148632 -4.1620522 -4.1786666][-4.2889404 -4.2828684 -4.262507 -4.221005 -4.1625385 -4.0908108 -4.0113363 -3.9639788 -3.9707284 -4.0225053 -4.0670738 -4.1006341 -4.1187634 -4.143393 -4.1754823][-4.2874575 -4.2794123 -4.2590652 -4.2108245 -4.1379852 -4.0424142 -3.9409173 -3.8866606 -3.9172227 -3.9996285 -4.0639296 -4.0994048 -4.1155667 -4.1449304 -4.1796236][-4.2865729 -4.2751918 -4.2541304 -4.1988525 -4.1073709 -3.9831152 -3.863673 -3.8164277 -3.8933568 -4.0114145 -4.0935278 -4.1282783 -4.139286 -4.1649866 -4.1910172][-4.286304 -4.2711043 -4.2485032 -4.1853828 -4.0781741 -3.9374995 -3.8202455 -3.8084853 -3.9312434 -4.0652757 -4.145998 -4.1736197 -4.1726632 -4.1817231 -4.1885386][-4.2865119 -4.2693563 -4.2437129 -4.1750045 -4.0671782 -3.9372 -3.8451266 -3.8719661 -4.0052447 -4.1250119 -4.1912007 -4.2031479 -4.1875634 -4.1708412 -4.1519327][-4.2881122 -4.27045 -4.243382 -4.1753325 -4.077641 -3.9723754 -3.915035 -3.96335 -4.0777164 -4.1702075 -4.2186146 -4.2137194 -4.1840572 -4.1424727 -4.1018872][-4.2923217 -4.2765956 -4.252212 -4.1894116 -4.0985093 -4.01252 -3.9802003 -4.0349789 -4.1289864 -4.2017322 -4.2285094 -4.2069321 -4.1627293 -4.1073208 -4.0641642][-4.2993927 -4.2870884 -4.2666345 -4.2087173 -4.1238194 -4.0493274 -4.0281706 -4.08134 -4.1602407 -4.2148128 -4.2259269 -4.1952491 -4.1476164 -4.0938225 -4.0517035][-4.3078833 -4.2983589 -4.2793493 -4.2251 -4.147512 -4.0822783 -4.0640354 -4.1094203 -4.171207 -4.2091684 -4.210063 -4.1790581 -4.1392794 -4.0967979 -4.0550876][-4.3158107 -4.3062859 -4.2862277 -4.2330518 -4.1594615 -4.0997987 -4.0779233 -4.105906 -4.1474934 -4.1748509 -4.1773334 -4.1553235 -4.1271706 -4.0990996 -4.0669489][-4.3211937 -4.313386 -4.2923822 -4.2373524 -4.1630168 -4.0987186 -4.0642562 -4.0688877 -4.0927963 -4.1220517 -4.1392341 -4.1310954 -4.1094279 -4.0905995 -4.0741234]]...]
INFO - root - 2017-12-06 00:12:43.124921: step 52910, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 71h:32m:39s remains)
INFO - root - 2017-12-06 00:12:52.055416: step 52920, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 70h:22m:03s remains)
INFO - root - 2017-12-06 00:13:01.180870: step 52930, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 70h:30m:39s remains)
INFO - root - 2017-12-06 00:13:10.416322: step 52940, loss = 2.07, batch loss = 2.01 (8.0 examples/sec; 0.995 sec/batch; 77h:15m:40s remains)
INFO - root - 2017-12-06 00:13:19.484574: step 52950, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 68h:27m:23s remains)
INFO - root - 2017-12-06 00:13:28.409914: step 52960, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 70h:14m:07s remains)
INFO - root - 2017-12-06 00:13:37.367476: step 52970, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 69h:25m:46s remains)
INFO - root - 2017-12-06 00:13:46.611103: step 52980, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 71h:51m:34s remains)
INFO - root - 2017-12-06 00:13:55.571864: step 52990, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 70h:11m:24s remains)
INFO - root - 2017-12-06 00:14:04.613724: step 53000, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 71h:27m:51s remains)
2017-12-06 00:14:05.384307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3021688 -4.2995019 -4.2918396 -4.2806873 -4.2734828 -4.2811842 -4.2997475 -4.3158731 -4.322103 -4.319757 -4.3128467 -4.3113966 -4.3138776 -4.3193254 -4.3308249][-4.27406 -4.2666535 -4.2499604 -4.2268877 -4.2096782 -4.2166 -4.2433605 -4.2681317 -4.2812877 -4.2811332 -4.2744064 -4.2731133 -4.2797122 -4.29093 -4.3129139][-4.242167 -4.2222881 -4.1910644 -4.1500568 -4.1141472 -4.1175251 -4.1526222 -4.1879458 -4.2127972 -4.2249541 -4.2272797 -4.228888 -4.2405162 -4.2584515 -4.2905807][-4.1999822 -4.1650758 -4.1172929 -4.0583653 -4.0028896 -4.0001445 -4.0378256 -4.0783362 -4.1189861 -4.1517215 -4.1731172 -4.188323 -4.2077403 -4.2311797 -4.2704434][-4.16175 -4.112833 -4.0569625 -3.9917612 -3.924077 -3.9082375 -3.9348338 -3.9730995 -4.0317559 -4.0917377 -4.13429 -4.165174 -4.1934376 -4.2188053 -4.2608428][-4.1328464 -4.0701728 -4.0135794 -3.9534926 -3.8755705 -3.8269172 -3.8209274 -3.8551204 -3.9397106 -4.0276561 -4.0927954 -4.1384163 -4.1722612 -4.1987205 -4.2424531][-4.11825 -4.0494184 -3.9946764 -3.9392943 -3.853281 -3.767333 -3.7112775 -3.735343 -3.8479943 -3.9664867 -4.0591049 -4.1201367 -4.1584182 -4.1841192 -4.2281475][-4.126862 -4.062542 -4.0154767 -3.9646866 -3.8799202 -3.7814813 -3.7018962 -3.712358 -3.82594 -3.9497113 -4.0510578 -4.1195626 -4.1618338 -4.1913323 -4.233943][-4.1365614 -4.0844069 -4.049335 -4.0079346 -3.9397755 -3.8642173 -3.8069706 -3.8136754 -3.8908806 -3.9754963 -4.0579281 -4.1211467 -4.1633739 -4.2009215 -4.2456818][-4.1339307 -4.0964642 -4.0749526 -4.0498481 -4.009398 -3.965363 -3.9338565 -3.9351463 -3.9733086 -4.0139747 -4.0688887 -4.1232886 -4.1730547 -4.2197852 -4.2627444][-4.1146712 -4.0856528 -4.0769463 -4.0677056 -4.0483603 -4.0322008 -4.025239 -4.0322876 -4.0542679 -4.0678167 -4.0978432 -4.1419287 -4.1953368 -4.2440915 -4.2783546][-4.1025567 -4.0794568 -4.0784473 -4.0804935 -4.0792985 -4.0804744 -4.0917854 -4.1098251 -4.1313558 -4.1336432 -4.1437945 -4.1740479 -4.2211018 -4.2648616 -4.2907057][-4.1183524 -4.103241 -4.1102276 -4.1259265 -4.1428208 -4.1574607 -4.1759434 -4.1996083 -4.2215075 -4.2187343 -4.2129388 -4.2236834 -4.253417 -4.2868257 -4.3054137][-4.1643081 -4.1583867 -4.1726789 -4.1975818 -4.2248816 -4.2446179 -4.2606931 -4.28103 -4.2988706 -4.2961621 -4.2855039 -4.2835007 -4.2959609 -4.3135095 -4.3229485][-4.2198706 -4.220747 -4.2399931 -4.2664075 -4.2926183 -4.310782 -4.3225994 -4.3337297 -4.3428483 -4.3403039 -4.3314605 -4.3256226 -4.328783 -4.3355875 -4.337954]]...]
INFO - root - 2017-12-06 00:14:14.408225: step 53010, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 70h:06m:02s remains)
INFO - root - 2017-12-06 00:14:23.476997: step 53020, loss = 2.07, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 73h:59m:46s remains)
INFO - root - 2017-12-06 00:14:32.543272: step 53030, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 71h:14m:52s remains)
INFO - root - 2017-12-06 00:14:41.753091: step 53040, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 70h:39m:57s remains)
INFO - root - 2017-12-06 00:14:50.698682: step 53050, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 68h:20m:47s remains)
INFO - root - 2017-12-06 00:14:59.880791: step 53060, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 71h:16m:24s remains)
INFO - root - 2017-12-06 00:15:09.039900: step 53070, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.933 sec/batch; 72h:25m:09s remains)
INFO - root - 2017-12-06 00:15:18.102736: step 53080, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 69h:04m:29s remains)
INFO - root - 2017-12-06 00:15:26.966956: step 53090, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:45m:29s remains)
INFO - root - 2017-12-06 00:15:36.090709: step 53100, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.931 sec/batch; 72h:14m:00s remains)
2017-12-06 00:15:36.851614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3272409 -4.3278928 -4.3288684 -4.3300948 -4.3302493 -4.3294835 -4.328793 -4.3279161 -4.3273768 -4.3270869 -4.3262181 -4.3248391 -4.3234825 -4.3217244 -4.3212662][-4.3255839 -4.3244753 -4.3242235 -4.32664 -4.3283319 -4.3277769 -4.3277044 -4.327158 -4.3274522 -4.3294086 -4.3300419 -4.3292885 -4.32822 -4.3254552 -4.3226385][-4.3206763 -4.3169327 -4.3143106 -4.3201265 -4.3249741 -4.32399 -4.3236246 -4.322505 -4.3220806 -4.3250442 -4.3279037 -4.3290124 -4.3293028 -4.3247085 -4.3180437][-4.297061 -4.2921634 -4.2857637 -4.2945914 -4.3031392 -4.2993431 -4.2970161 -4.2959652 -4.2950954 -4.2988091 -4.304709 -4.3104172 -4.3127542 -4.3041077 -4.2891388][-4.2562857 -4.25061 -4.2370186 -4.2469039 -4.2595487 -4.2547455 -4.2493482 -4.2485294 -4.2449675 -4.2442904 -4.2521825 -4.2679935 -4.2767739 -4.2677245 -4.2483907][-4.1992846 -4.18764 -4.1588635 -4.1651449 -4.1859803 -4.1903739 -4.1891203 -4.1905165 -4.1831717 -4.1719856 -4.1777372 -4.2067218 -4.2285905 -4.226892 -4.2085466][-4.1353121 -4.1085467 -4.0556951 -4.0544133 -4.0866537 -4.1049352 -4.1112576 -4.1169782 -4.1072607 -4.0876408 -4.0944381 -4.1389318 -4.1738982 -4.1797333 -4.1678538][-4.077548 -4.0350862 -3.9593492 -3.9473634 -3.9812427 -4.0042696 -4.0152512 -4.0270257 -4.0181766 -3.9938183 -4.0026207 -4.0640011 -4.1136627 -4.13151 -4.1337481][-4.0715494 -4.0244155 -3.9355052 -3.9084604 -3.9365697 -3.9618094 -3.9785657 -3.9925113 -3.9864998 -3.9608331 -3.9629469 -4.0274014 -4.0838585 -4.1069036 -4.1153359][-4.1046109 -4.0680261 -3.9890821 -3.9641492 -3.9912133 -4.0189061 -4.0381279 -4.0481968 -4.0465355 -4.0270295 -4.0245013 -4.0747128 -4.1230478 -4.1406393 -4.1439738][-4.14433 -4.1248527 -4.0695496 -4.0544729 -4.07902 -4.1041408 -4.1253476 -4.1368361 -4.1437964 -4.1366353 -4.13284 -4.163022 -4.1936464 -4.2001925 -4.1965065][-4.171133 -4.1611352 -4.1249828 -4.1165018 -4.1361957 -4.158318 -4.18266 -4.199923 -4.2154117 -4.2198668 -4.2207904 -4.2395911 -4.2565866 -4.256465 -4.249393][-4.1894588 -4.188221 -4.1675482 -4.1666923 -4.1845379 -4.2050853 -4.2270994 -4.2434955 -4.2586293 -4.268075 -4.2731295 -4.2850571 -4.2939925 -4.2940297 -4.2899909][-4.216835 -4.2191644 -4.2096925 -4.2170386 -4.2357965 -4.2545857 -4.2731152 -4.2839789 -4.2924466 -4.299057 -4.3047056 -4.3121486 -4.3166404 -4.3166513 -4.3148108][-4.2395811 -4.2421365 -4.2372208 -4.2479105 -4.26809 -4.2874565 -4.3028555 -4.3086858 -4.310688 -4.3117547 -4.3150535 -4.32259 -4.3257108 -4.3251042 -4.3239136]]...]
INFO - root - 2017-12-06 00:15:45.968964: step 53110, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 72h:13m:08s remains)
INFO - root - 2017-12-06 00:15:54.977505: step 53120, loss = 2.03, batch loss = 1.97 (10.0 examples/sec; 0.798 sec/batch; 61h:57m:41s remains)
INFO - root - 2017-12-06 00:16:04.095726: step 53130, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.908 sec/batch; 70h:27m:28s remains)
INFO - root - 2017-12-06 00:16:13.244478: step 53140, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 69h:50m:35s remains)
INFO - root - 2017-12-06 00:16:22.246767: step 53150, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 69h:03m:26s remains)
INFO - root - 2017-12-06 00:16:31.402285: step 53160, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 72h:40m:14s remains)
INFO - root - 2017-12-06 00:16:40.776057: step 53170, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 72h:12m:13s remains)
INFO - root - 2017-12-06 00:16:49.939882: step 53180, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 65h:56m:59s remains)
INFO - root - 2017-12-06 00:16:59.051092: step 53190, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 69h:18m:56s remains)
INFO - root - 2017-12-06 00:17:08.093312: step 53200, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:47m:03s remains)
2017-12-06 00:17:09.036504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0232115 -3.972266 -3.9864206 -4.0405335 -4.0965576 -4.1390681 -4.1609173 -4.1856227 -4.2340183 -4.2912107 -4.3341084 -4.3545666 -4.3559623 -4.3360434 -4.2992659][-4.029479 -3.9615543 -3.9473748 -3.9747715 -4.0245709 -4.0723748 -4.0926161 -4.1103535 -4.1573896 -4.2245507 -4.2820477 -4.3191185 -4.3330569 -4.3227339 -4.296978][-4.0580215 -3.990695 -3.9542372 -3.9519072 -3.9930873 -4.041543 -4.0510874 -4.0493822 -4.0850325 -4.1546092 -4.224194 -4.2759194 -4.2981553 -4.2936692 -4.2774129][-4.1020112 -4.0483789 -4.0102911 -3.9947627 -4.0250587 -4.0628152 -4.0473104 -4.0154419 -4.0297723 -4.0944266 -4.1756749 -4.241559 -4.2666922 -4.2634506 -4.25272][-4.1575069 -4.1173062 -4.0877929 -4.07259 -4.0877156 -4.0992026 -4.0494056 -3.9847932 -3.976861 -4.0420938 -4.1447992 -4.2282538 -4.2588263 -4.2601838 -4.2552276][-4.2010918 -4.17032 -4.1528153 -4.1458797 -4.1458993 -4.1274633 -4.0516019 -3.9646308 -3.9395256 -4.0075145 -4.1289091 -4.2269821 -4.267859 -4.2765627 -4.2789292][-4.2231426 -4.1985092 -4.1900382 -4.1865754 -4.1735344 -4.1346912 -4.0478315 -3.952054 -3.9181867 -3.9866321 -4.1172042 -4.2255278 -4.2791853 -4.3002496 -4.3099709][-4.2355895 -4.2151232 -4.2060623 -4.1961045 -4.1670179 -4.1163392 -4.02955 -3.9353807 -3.8988574 -3.9623692 -4.0931225 -4.211185 -4.2799397 -4.3168364 -4.3358531][-4.2540298 -4.2427583 -4.2296619 -4.2092195 -4.1703644 -4.1153717 -4.03325 -3.9432817 -3.901082 -3.9458907 -4.0583305 -4.174911 -4.2509961 -4.296927 -4.3260412][-4.2728281 -4.2738485 -4.2611794 -4.2408557 -4.2055907 -4.157867 -4.0898681 -4.0108452 -3.9568655 -3.9667125 -4.03944 -4.1353722 -4.2049875 -4.2492242 -4.2820044][-4.2854686 -4.2966089 -4.2887397 -4.275321 -4.2513862 -4.2185378 -4.1712232 -4.1066108 -4.0440989 -4.0221219 -4.0558329 -4.1220131 -4.1751986 -4.2047081 -4.2243447][-4.2873344 -4.3033895 -4.2977619 -4.2887497 -4.2785416 -4.2644653 -4.2400584 -4.1933837 -4.1345177 -4.0976639 -4.1039214 -4.143517 -4.1779523 -4.1887112 -4.1845922][-4.2629867 -4.2778416 -4.2721262 -4.2639465 -4.2661419 -4.2724724 -4.2742977 -4.25211 -4.2081079 -4.1718769 -4.162806 -4.1813312 -4.2001591 -4.197751 -4.1749964][-4.2219911 -4.2279878 -4.2176232 -4.2069011 -4.2166271 -4.2427988 -4.2698083 -4.2775078 -4.2588973 -4.2348437 -4.2224021 -4.2290359 -4.2386923 -4.2284379 -4.192347][-4.2044687 -4.1958427 -4.1789765 -4.1642075 -4.171412 -4.2034578 -4.2467175 -4.2791328 -4.2850947 -4.2778668 -4.27381 -4.2801585 -4.2862892 -4.2701941 -4.2226453]]...]
INFO - root - 2017-12-06 00:17:18.251746: step 53210, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 70h:32m:31s remains)
INFO - root - 2017-12-06 00:17:27.398374: step 53220, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 71h:47m:58s remains)
INFO - root - 2017-12-06 00:17:36.624289: step 53230, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 71h:09m:42s remains)
INFO - root - 2017-12-06 00:17:45.732103: step 53240, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:46m:24s remains)
INFO - root - 2017-12-06 00:17:54.963623: step 53250, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 70h:31m:26s remains)
INFO - root - 2017-12-06 00:18:04.052934: step 53260, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 67h:18m:26s remains)
INFO - root - 2017-12-06 00:18:13.127010: step 53270, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 71h:28m:32s remains)
INFO - root - 2017-12-06 00:18:22.204042: step 53280, loss = 2.10, batch loss = 2.05 (8.7 examples/sec; 0.921 sec/batch; 71h:27m:58s remains)
INFO - root - 2017-12-06 00:18:31.428829: step 53290, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.943 sec/batch; 73h:07m:10s remains)
INFO - root - 2017-12-06 00:18:40.403901: step 53300, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 69h:58m:51s remains)
2017-12-06 00:18:41.184769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3093023 -4.3111248 -4.3125081 -4.31362 -4.3141255 -4.3138933 -4.3128047 -4.3115249 -4.310463 -4.3101959 -4.3102655 -4.309906 -4.308876 -4.3074131 -4.3056841][-4.2927942 -4.2976885 -4.3021121 -4.3053765 -4.3062692 -4.305654 -4.3038926 -4.3017941 -4.3002095 -4.3002543 -4.301435 -4.3024788 -4.30175 -4.2982531 -4.2929497][-4.2794447 -4.2866826 -4.2933631 -4.2979069 -4.2982707 -4.2963238 -4.2937088 -4.29104 -4.2888632 -4.2889233 -4.2902875 -4.2926006 -4.2931929 -4.2890563 -4.2806706][-4.2720947 -4.2783256 -4.28488 -4.28918 -4.2877455 -4.2829208 -4.2792277 -4.2778435 -4.2767134 -4.2760248 -4.2749815 -4.2763648 -4.2769628 -4.272069 -4.2624416][-4.2714472 -4.2747774 -4.2771654 -4.2776437 -4.2727637 -4.264822 -4.2608771 -4.2627015 -4.2651844 -4.2632427 -4.2571087 -4.2545133 -4.2544575 -4.2512841 -4.2453136][-4.2710319 -4.2725468 -4.2727404 -4.2713056 -4.2652822 -4.2558289 -4.251442 -4.2549758 -4.2592254 -4.2549877 -4.243011 -4.2359748 -4.2345829 -4.23364 -4.2334418][-4.2634759 -4.2655945 -4.2664728 -4.265141 -4.2578425 -4.2465892 -4.2405849 -4.2446613 -4.250206 -4.244452 -4.226625 -4.2102857 -4.2015977 -4.199235 -4.2062764][-4.2303739 -4.2375207 -4.2427425 -4.2424641 -4.2331834 -4.2198505 -4.2125492 -4.2167048 -4.2234764 -4.2175412 -4.1955333 -4.1691651 -4.1520371 -4.1508384 -4.1708169][-4.1802835 -4.1960835 -4.2069941 -4.2056174 -4.19239 -4.1764474 -4.1687751 -4.1738091 -4.1816306 -4.1752491 -4.1523409 -4.125114 -4.1111231 -4.1194 -4.153059][-4.1362786 -4.1543264 -4.167388 -4.1651034 -4.1501389 -4.1335673 -4.1248531 -4.1289115 -4.1369333 -4.1338849 -4.1195927 -4.10631 -4.1092629 -4.1290565 -4.1646023][-4.1218886 -4.1368413 -4.1486754 -4.1466055 -4.1321144 -4.1152086 -4.1039081 -4.1070256 -4.118566 -4.1249189 -4.12569 -4.1309214 -4.1462622 -4.1656647 -4.189765][-4.1254911 -4.1355858 -4.1448412 -4.1441708 -4.1346846 -4.1227827 -4.1148114 -4.1220589 -4.1404233 -4.1561 -4.1666012 -4.1778617 -4.1904669 -4.2008781 -4.2111998][-4.1285386 -4.1369581 -4.1467805 -4.1512084 -4.1512384 -4.1462646 -4.1430588 -4.1533289 -4.1740136 -4.19193 -4.2044492 -4.2130642 -4.2183609 -4.2199097 -4.2188687][-4.1350193 -4.1468434 -4.1631284 -4.1750493 -4.18133 -4.1800203 -4.1773009 -4.1846371 -4.199852 -4.2142749 -4.2239738 -4.2280092 -4.22838 -4.2227225 -4.2127309][-4.1400771 -4.1601276 -4.1850185 -4.2050056 -4.2148695 -4.2136064 -4.208714 -4.2110257 -4.2197785 -4.2267346 -4.2281084 -4.2247295 -4.2215571 -4.2150297 -4.2067318]]...]
INFO - root - 2017-12-06 00:18:50.450726: step 53310, loss = 2.08, batch loss = 2.03 (8.6 examples/sec; 0.931 sec/batch; 72h:11m:53s remains)
INFO - root - 2017-12-06 00:18:59.900381: step 53320, loss = 2.11, batch loss = 2.05 (8.5 examples/sec; 0.938 sec/batch; 72h:44m:53s remains)
INFO - root - 2017-12-06 00:19:09.039216: step 53330, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 71h:21m:18s remains)
INFO - root - 2017-12-06 00:19:18.105730: step 53340, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 69h:54m:59s remains)
INFO - root - 2017-12-06 00:19:27.378452: step 53350, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 70h:35m:00s remains)
INFO - root - 2017-12-06 00:19:36.545607: step 53360, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 69h:47m:51s remains)
INFO - root - 2017-12-06 00:19:45.327897: step 53370, loss = 2.11, batch loss = 2.05 (8.8 examples/sec; 0.906 sec/batch; 70h:12m:33s remains)
INFO - root - 2017-12-06 00:19:54.337841: step 53380, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 68h:14m:59s remains)
INFO - root - 2017-12-06 00:20:03.418442: step 53390, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 69h:55m:48s remains)
INFO - root - 2017-12-06 00:20:12.487838: step 53400, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.943 sec/batch; 73h:06m:05s remains)
2017-12-06 00:20:13.246600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2770691 -4.2506571 -4.2265749 -4.209383 -4.20661 -4.2116323 -4.2202544 -4.2285051 -4.2402668 -4.25278 -4.2634153 -4.2698793 -4.2697754 -4.2668123 -4.26224][-4.2829342 -4.2547717 -4.2260189 -4.2016625 -4.1918354 -4.1894846 -4.1917429 -4.1970859 -4.2101912 -4.2278104 -4.2441707 -4.2544394 -4.2549467 -4.253624 -4.2521486][-4.2991714 -4.2705197 -4.23846 -4.2098756 -4.1915984 -4.1805563 -4.1754603 -4.1749115 -4.1877913 -4.209805 -4.2329693 -4.2479916 -4.2491622 -4.2494111 -4.2509913][-4.3122392 -4.2827034 -4.2492418 -4.2195253 -4.1915555 -4.1678019 -4.1520886 -4.1423922 -4.1515622 -4.1773667 -4.2104764 -4.2359462 -4.2434382 -4.2460942 -4.249887][-4.3169127 -4.2870407 -4.2512455 -4.2144771 -4.17284 -4.1351547 -4.1084356 -4.0874157 -4.0901823 -4.1185513 -4.1618481 -4.1997132 -4.2158828 -4.22227 -4.2289705][-4.312614 -4.2772226 -4.2331228 -4.1829734 -4.1226315 -4.067008 -4.024756 -3.9916754 -3.9890862 -4.0249367 -4.0813236 -4.1302161 -4.1503243 -4.1577859 -4.1686592][-4.295063 -4.2495303 -4.1944733 -4.1296811 -4.0532866 -3.9809141 -3.9182343 -3.8632953 -3.8484817 -3.8963153 -3.974005 -4.0379148 -4.0652947 -4.0763903 -4.0938821][-4.2699633 -4.2160172 -4.1527205 -4.0782628 -3.9970298 -3.9222012 -3.8511877 -3.782665 -3.7586784 -3.8146312 -3.9060495 -3.9831257 -4.0218315 -4.040206 -4.0623951][-4.2579427 -4.2041831 -4.1420684 -4.0706224 -4.0027261 -3.9494438 -3.9016929 -3.8574069 -3.845037 -3.8849311 -3.9528322 -4.0149384 -4.0531206 -4.0758214 -4.1000276][-4.2719584 -4.2286611 -4.178628 -4.1212268 -4.0724239 -4.0444074 -4.026166 -4.011693 -4.0131469 -4.0371046 -4.0754151 -4.1131949 -4.1417012 -4.1647081 -4.188231][-4.2947507 -4.2648535 -4.23056 -4.1880617 -4.1535521 -4.1385856 -4.1345878 -4.1369386 -4.1493087 -4.168807 -4.1932883 -4.216383 -4.2370605 -4.2575154 -4.278542][-4.3154192 -4.2962089 -4.2755871 -4.2485485 -4.2270222 -4.2192054 -4.2191248 -4.2259974 -4.2404504 -4.2560735 -4.2732182 -4.2881765 -4.3016968 -4.3146105 -4.3286719][-4.3332434 -4.3216715 -4.3119116 -4.3002443 -4.2908282 -4.28868 -4.2900577 -4.2952452 -4.3052821 -4.3157606 -4.3258882 -4.3343196 -4.3409762 -4.347611 -4.3541193][-4.3474631 -4.3411589 -4.3377614 -4.3353658 -4.3341532 -4.33616 -4.3389506 -4.341917 -4.3471479 -4.3528318 -4.3575177 -4.3614345 -4.364429 -4.3668895 -4.3674197][-4.3533459 -4.3493438 -4.3477244 -4.3474212 -4.3489342 -4.3522143 -4.3553715 -4.3576345 -4.3605208 -4.3635769 -4.3655992 -4.3673339 -4.36826 -4.36822 -4.366116]]...]
INFO - root - 2017-12-06 00:20:22.349595: step 53410, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 68h:26m:38s remains)
INFO - root - 2017-12-06 00:20:31.555609: step 53420, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 72h:03m:18s remains)
INFO - root - 2017-12-06 00:20:40.668182: step 53430, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 72h:38m:14s remains)
INFO - root - 2017-12-06 00:20:49.784286: step 53440, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 71h:36m:39s remains)
INFO - root - 2017-12-06 00:20:59.047251: step 53450, loss = 2.07, batch loss = 2.01 (8.2 examples/sec; 0.975 sec/batch; 75h:32m:29s remains)
INFO - root - 2017-12-06 00:21:08.184550: step 53460, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:34m:41s remains)
INFO - root - 2017-12-06 00:21:17.151255: step 53470, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 70h:53m:29s remains)
INFO - root - 2017-12-06 00:21:26.384148: step 53480, loss = 2.09, batch loss = 2.04 (8.7 examples/sec; 0.922 sec/batch; 71h:29m:14s remains)
INFO - root - 2017-12-06 00:21:35.579673: step 53490, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 69h:15m:11s remains)
INFO - root - 2017-12-06 00:21:44.594547: step 53500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 69h:15m:19s remains)
2017-12-06 00:21:45.417376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2151165 -4.2085257 -4.2070351 -4.2162247 -4.2340283 -4.2496457 -4.2501059 -4.2332439 -4.2105575 -4.1947775 -4.18306 -4.1716862 -4.1688957 -4.183774 -4.2074928][-4.2078967 -4.2009687 -4.200654 -4.2094064 -4.2262926 -4.2381015 -4.2367167 -4.2217479 -4.2007351 -4.1883616 -4.1824422 -4.1730819 -4.1680079 -4.1749249 -4.1903529][-4.2082858 -4.2027097 -4.2025023 -4.2098079 -4.2241254 -4.2331433 -4.231266 -4.2201343 -4.2064681 -4.202569 -4.2060151 -4.2041063 -4.201211 -4.2023635 -4.2060323][-4.2054029 -4.1995554 -4.1968865 -4.2002664 -4.212348 -4.22243 -4.2246604 -4.2213321 -4.2151814 -4.2162547 -4.2260075 -4.229589 -4.2292376 -4.2263985 -4.2196064][-4.1949658 -4.1873989 -4.1804819 -4.1807628 -4.1926785 -4.204134 -4.2085757 -4.2068982 -4.2040844 -4.2097335 -4.2231803 -4.2336597 -4.2390532 -4.23662 -4.2227788][-4.1900306 -4.1835151 -4.175209 -4.174583 -4.1831675 -4.1864138 -4.1794133 -4.1660748 -4.1569219 -4.1667881 -4.1892257 -4.21098 -4.2279005 -4.2311463 -4.2159414][-4.1862111 -4.1853342 -4.1787839 -4.1769528 -4.1776514 -4.1652231 -4.1367912 -4.0990148 -4.0754013 -4.0933342 -4.1346312 -4.17459 -4.2074656 -4.2203054 -4.2077441][-4.16995 -4.1734872 -4.1666451 -4.1575809 -4.1451273 -4.1142797 -4.0603862 -3.9919813 -3.9537284 -3.9926746 -4.0666118 -4.1330948 -4.18572 -4.2087193 -4.2000942][-4.1547465 -4.1594853 -4.1499071 -4.1312203 -4.1050839 -4.059875 -3.9893939 -3.9007471 -3.8531523 -3.9152322 -4.019917 -4.1054091 -4.166471 -4.1945658 -4.1913853][-4.1626358 -4.1639376 -4.1500382 -4.1282477 -4.1028872 -4.067688 -4.0148005 -3.9464116 -3.9089608 -3.9613335 -4.0503387 -4.1166472 -4.157455 -4.1752377 -4.175703][-4.186377 -4.1849465 -4.1706262 -4.1562109 -4.14366 -4.1281371 -4.1012745 -4.0616488 -4.0374541 -4.0672965 -4.12008 -4.1506844 -4.1603189 -4.1607704 -4.16194][-4.2082038 -4.2079129 -4.1974716 -4.1913204 -4.1874442 -4.1809554 -4.1666989 -4.1432838 -4.1243176 -4.1328096 -4.1561623 -4.1619987 -4.1512146 -4.1419716 -4.1465578][-4.2111511 -4.2132983 -4.2083764 -4.2084408 -4.2071047 -4.201993 -4.1944618 -4.1811261 -4.164392 -4.157548 -4.1618047 -4.1551828 -4.1347456 -4.1215544 -4.1302404][-4.1937227 -4.1956816 -4.193574 -4.1948357 -4.193192 -4.1904917 -4.1877241 -4.1800246 -4.1675806 -4.1587658 -4.1588087 -4.1528306 -4.1345778 -4.1222734 -4.12887][-4.1639724 -4.1629987 -4.1618719 -4.1654811 -4.1664615 -4.1670656 -4.1679373 -4.1655579 -4.1611261 -4.1588106 -4.1625414 -4.1618514 -4.150754 -4.1422224 -4.1434617]]...]
INFO - root - 2017-12-06 00:21:54.641642: step 53510, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 69h:53m:17s remains)
INFO - root - 2017-12-06 00:22:03.776509: step 53520, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 70h:18m:24s remains)
INFO - root - 2017-12-06 00:22:12.899733: step 53530, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.944 sec/batch; 73h:08m:38s remains)
INFO - root - 2017-12-06 00:22:22.112265: step 53540, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.918 sec/batch; 71h:06m:25s remains)
INFO - root - 2017-12-06 00:22:31.197422: step 53550, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 71h:24m:15s remains)
INFO - root - 2017-12-06 00:22:40.166542: step 53560, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.951 sec/batch; 73h:43m:24s remains)
INFO - root - 2017-12-06 00:22:49.352584: step 53570, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 66h:02m:07s remains)
INFO - root - 2017-12-06 00:22:58.443557: step 53580, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 70h:35m:54s remains)
INFO - root - 2017-12-06 00:23:07.333723: step 53590, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 68h:50m:25s remains)
INFO - root - 2017-12-06 00:23:16.509996: step 53600, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 70h:27m:55s remains)
2017-12-06 00:23:17.354024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2196307 -4.21074 -4.1989794 -4.1964469 -4.2000623 -4.2244558 -4.2608991 -4.2806845 -4.2826333 -4.2810006 -4.2787805 -4.271441 -4.2603498 -4.2453279 -4.2181616][-4.2157464 -4.2001343 -4.1779108 -4.1706777 -4.176106 -4.2045603 -4.2490473 -4.2753477 -4.2848654 -4.292841 -4.3026147 -4.30521 -4.2980881 -4.2823334 -4.2536573][-4.2070923 -4.1880326 -4.1602211 -4.1500731 -4.157187 -4.1865077 -4.2247086 -4.2480693 -4.2646947 -4.286324 -4.3093557 -4.3222704 -4.3233752 -4.31072 -4.2870088][-4.1940861 -4.174531 -4.1463823 -4.137723 -4.1477408 -4.1705918 -4.1868854 -4.1956396 -4.2165995 -4.2564945 -4.2952986 -4.3219581 -4.3336372 -4.3267746 -4.3073211][-4.1849256 -4.1715965 -4.1497169 -4.1451526 -4.1504912 -4.1534047 -4.1383662 -4.121542 -4.1410437 -4.2002535 -4.2575336 -4.2991495 -4.3243556 -4.3257017 -4.3064351][-4.190135 -4.1874948 -4.1714993 -4.165863 -4.1580229 -4.1317778 -4.0799518 -4.0288682 -4.0472808 -4.1319985 -4.2103205 -4.2658935 -4.3044105 -4.3144078 -4.2935781][-4.200089 -4.2033362 -4.1892142 -4.1788139 -4.1501842 -4.0882959 -3.9937761 -3.896106 -3.9165847 -4.0402431 -4.1466508 -4.2183733 -4.2731996 -4.2946882 -4.27457][-4.2089152 -4.2134666 -4.2038708 -4.1939168 -4.1516414 -4.0608721 -3.9277434 -3.7810168 -3.7992063 -3.9571412 -4.0886993 -4.1735916 -4.2411766 -4.2725739 -4.2570448][-4.228374 -4.2287602 -4.2204852 -4.2125216 -4.1722836 -4.0849595 -3.9531195 -3.7977557 -3.8057792 -3.9536605 -4.0791903 -4.1617403 -4.2313142 -4.2680173 -4.2603011][-4.25335 -4.2483706 -4.2381687 -4.2322388 -4.2071686 -4.1495414 -4.0583968 -3.9475369 -3.9448755 -4.0417247 -4.1287727 -4.1919861 -4.2494535 -4.2844644 -4.2836065][-4.267561 -4.2597375 -4.2505779 -4.2486148 -4.234046 -4.2001748 -4.1481009 -4.0850492 -4.0816884 -4.1357751 -4.1856337 -4.2282791 -4.2715707 -4.3025117 -4.309607][-4.2758718 -4.2700224 -4.2635322 -4.2632904 -4.2540665 -4.2341762 -4.2072434 -4.1759982 -4.1749892 -4.2027578 -4.2277727 -4.2531 -4.2844558 -4.311439 -4.3261585][-4.2880449 -4.2852454 -4.2833257 -4.2855525 -4.280786 -4.2696333 -4.2579823 -4.24406 -4.2429323 -4.2527823 -4.2598972 -4.2715116 -4.2925954 -4.314393 -4.3303261][-4.295289 -4.2957354 -4.298656 -4.3033638 -4.3024731 -4.2984214 -4.2970114 -4.2910337 -4.2859292 -4.2836537 -4.2825675 -4.2861695 -4.2972927 -4.3126431 -4.3240318][-4.2930794 -4.2951069 -4.2988257 -4.3023429 -4.3033247 -4.3040309 -4.3067336 -4.3018746 -4.2901058 -4.2802119 -4.2795944 -4.28376 -4.2914252 -4.3041487 -4.3114939]]...]
INFO - root - 2017-12-06 00:23:26.539808: step 53610, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 69h:47m:48s remains)
INFO - root - 2017-12-06 00:23:35.552118: step 53620, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 72h:40m:33s remains)
INFO - root - 2017-12-06 00:23:44.814786: step 53630, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 72h:17m:14s remains)
INFO - root - 2017-12-06 00:23:53.935279: step 53640, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 71h:44m:44s remains)
INFO - root - 2017-12-06 00:24:03.048444: step 53650, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 72h:20m:35s remains)
INFO - root - 2017-12-06 00:24:12.082106: step 53660, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 67h:56m:07s remains)
INFO - root - 2017-12-06 00:24:21.156970: step 53670, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 69h:00m:36s remains)
INFO - root - 2017-12-06 00:24:30.144459: step 53680, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.917 sec/batch; 70h:59m:01s remains)
INFO - root - 2017-12-06 00:24:39.156509: step 53690, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.926 sec/batch; 71h:43m:24s remains)
INFO - root - 2017-12-06 00:24:48.226934: step 53700, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 71h:01m:23s remains)
2017-12-06 00:24:49.024896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2295265 -4.2296433 -4.22529 -4.2150674 -4.205792 -4.203918 -4.2100368 -4.2148037 -4.2212276 -4.2321639 -4.2464628 -4.24958 -4.2490525 -4.2702751 -4.3035903][-4.2518606 -4.2475948 -4.2379913 -4.2259836 -4.2189746 -4.2201262 -4.227644 -4.2328982 -4.2415719 -4.248569 -4.2525506 -4.2461758 -4.2464943 -4.2747817 -4.3126225][-4.2731552 -4.262527 -4.2462239 -4.231935 -4.2249565 -4.2247171 -4.2308311 -4.232759 -4.2325106 -4.2348504 -4.2352805 -4.2291422 -4.2396979 -4.2755256 -4.3124228][-4.2688951 -4.2527695 -4.2374005 -4.2307472 -4.2231317 -4.2139096 -4.2100606 -4.1996469 -4.1888375 -4.1883063 -4.1903968 -4.1957474 -4.2238245 -4.2683082 -4.3043222][-4.2543468 -4.2411351 -4.236268 -4.2388358 -4.2297616 -4.2066069 -4.1845093 -4.1566706 -4.129571 -4.1262965 -4.1368012 -4.1638722 -4.205636 -4.2559829 -4.2921443][-4.2426009 -4.2369604 -4.2386127 -4.2391286 -4.2220011 -4.1911297 -4.1546597 -4.1054831 -4.0553708 -4.0460544 -4.0791464 -4.1343303 -4.1818218 -4.2417564 -4.2840595][-4.2220454 -4.2127237 -4.2040315 -4.1919646 -4.1678476 -4.137054 -4.08898 -4.0171323 -3.9382493 -3.9278631 -4.0043621 -4.0893741 -4.1484566 -4.2194328 -4.2702246][-4.1880226 -4.1608806 -4.1275525 -4.0942764 -4.0691171 -4.0453696 -3.9910784 -3.9097309 -3.8126616 -3.8033686 -3.9301786 -4.04536 -4.1171017 -4.1961851 -4.255156][-4.1585207 -4.1093984 -4.0499687 -3.9964328 -3.9729025 -3.9578829 -3.9115279 -3.847481 -3.779556 -3.7945862 -3.9290614 -4.0303669 -4.093195 -4.17628 -4.2440939][-4.1554918 -4.1011147 -4.0374413 -3.9796071 -3.9542742 -3.940774 -3.9039202 -3.8678341 -3.8552921 -3.9013987 -3.9978604 -4.0531597 -4.091578 -4.1677928 -4.2375016][-4.1718397 -4.1229281 -4.0699339 -4.018868 -3.9911671 -3.9710953 -3.9370885 -3.9196637 -3.9380424 -3.9926805 -4.0615854 -4.0891871 -4.1103868 -4.1720243 -4.2382693][-4.1942515 -4.1537209 -4.1125107 -4.0744066 -4.0505919 -4.0352244 -4.0123239 -4.0052218 -4.0250258 -4.0703707 -4.1202888 -4.1308994 -4.140636 -4.1889677 -4.2516541][-4.2116508 -4.1880016 -4.163219 -4.1359997 -4.1185222 -4.1124878 -4.1026897 -4.099164 -4.1114993 -4.1427045 -4.1787381 -4.1818223 -4.1839647 -4.2220364 -4.2745476][-4.2148576 -4.2098627 -4.2038035 -4.1896024 -4.1764808 -4.1748557 -4.1738882 -4.1732879 -4.1815209 -4.2039952 -4.2315822 -4.2327223 -4.2300324 -4.256752 -4.2952709][-4.2176857 -4.2290082 -4.24037 -4.24034 -4.2328863 -4.232636 -4.2343092 -4.2350106 -4.2413731 -4.2544637 -4.271421 -4.2663536 -4.2578135 -4.2767029 -4.3058982]]...]
INFO - root - 2017-12-06 00:24:58.188729: step 53710, loss = 2.04, batch loss = 1.98 (8.2 examples/sec; 0.978 sec/batch; 75h:44m:13s remains)
INFO - root - 2017-12-06 00:25:07.370534: step 53720, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 72h:19m:15s remains)
INFO - root - 2017-12-06 00:25:16.516178: step 53730, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 67h:26m:25s remains)
INFO - root - 2017-12-06 00:25:25.512184: step 53740, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 70h:02m:46s remains)
INFO - root - 2017-12-06 00:25:34.591512: step 53750, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.894 sec/batch; 69h:14m:42s remains)
INFO - root - 2017-12-06 00:25:43.796673: step 53760, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.964 sec/batch; 74h:39m:06s remains)
INFO - root - 2017-12-06 00:25:52.782721: step 53770, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 69h:47m:11s remains)
INFO - root - 2017-12-06 00:26:01.761679: step 53780, loss = 2.10, batch loss = 2.04 (8.4 examples/sec; 0.947 sec/batch; 73h:19m:54s remains)
INFO - root - 2017-12-06 00:26:10.847535: step 53790, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 71h:03m:52s remains)
INFO - root - 2017-12-06 00:26:19.787263: step 53800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 69h:16m:24s remains)
2017-12-06 00:26:20.589064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1708755 -4.1721506 -4.186501 -4.1980786 -4.1941466 -4.1885023 -4.1938024 -4.2040224 -4.2093611 -4.2090235 -4.1887283 -4.1486449 -4.1120353 -4.1137424 -4.1376534][-4.156189 -4.1640477 -4.1857319 -4.2068448 -4.2106342 -4.208456 -4.2142305 -4.2216821 -4.2222543 -4.2199993 -4.195725 -4.1482954 -4.1018529 -4.0913711 -4.1063094][-4.15238 -4.1623182 -4.1840291 -4.2024341 -4.205008 -4.1983352 -4.1992254 -4.2080674 -4.2165294 -4.2241459 -4.2167549 -4.1888976 -4.15431 -4.1396246 -4.1427455][-4.1759295 -4.1809874 -4.1907911 -4.194531 -4.1822224 -4.1614752 -4.1516032 -4.1593065 -4.1761961 -4.1958652 -4.2124276 -4.216032 -4.2073803 -4.2066774 -4.2110567][-4.2074285 -4.1966877 -4.1878681 -4.1681261 -4.12852 -4.0812812 -4.0515347 -4.0510192 -4.0715742 -4.1036792 -4.1427402 -4.1742625 -4.1946626 -4.2177162 -4.2401862][-4.22731 -4.2014585 -4.174386 -4.1293292 -4.0585036 -3.9770222 -3.9159675 -3.8901167 -3.9087505 -3.959414 -4.0205131 -4.0769958 -4.1215811 -4.1668859 -4.2148194][-4.2332339 -4.2010684 -4.1569052 -4.0912614 -3.9962935 -3.8844345 -3.783576 -3.7195711 -3.7321813 -3.7949047 -3.8657799 -3.9422569 -4.0101042 -4.0756283 -4.1490097][-4.2325931 -4.2110262 -4.1658478 -4.098042 -4.0068555 -3.8989744 -3.79403 -3.7214084 -3.7264471 -3.7684851 -3.8052301 -3.8704305 -3.9414062 -4.00997 -4.0917835][-4.2280211 -4.2210441 -4.1921873 -4.1495857 -4.0907507 -4.0168662 -3.9432912 -3.8947144 -3.8920851 -3.8963737 -3.8825536 -3.908 -3.9550343 -4.0044665 -4.0749121][-4.2286258 -4.234271 -4.2227178 -4.2044387 -4.1757803 -4.1338806 -4.0874209 -4.0559511 -4.0460629 -4.0300307 -3.9989867 -3.9987173 -4.018219 -4.0424967 -4.0958376][-4.2371626 -4.2511349 -4.2513537 -4.2457986 -4.2355728 -4.2170444 -4.187983 -4.1656632 -4.1527476 -4.1370869 -4.1134448 -4.1054482 -4.1047554 -4.1098976 -4.1442552][-4.2614174 -4.2766008 -4.2789307 -4.27555 -4.2702785 -4.2629533 -4.2465892 -4.2341685 -4.2271352 -4.22271 -4.2146273 -4.2086644 -4.2001204 -4.1956253 -4.2117271][-4.2883906 -4.3022952 -4.3033662 -4.2972279 -4.2910323 -4.2829537 -4.2709303 -4.2648664 -4.2684917 -4.2756476 -4.279 -4.280416 -4.2738338 -4.2667623 -4.2705426][-4.3037763 -4.3124213 -4.3109651 -4.3048849 -4.2983794 -4.2893295 -4.2802577 -4.2782321 -4.2883949 -4.3012528 -4.3082485 -4.3124094 -4.3093538 -4.3042383 -4.3040276][-4.3018303 -4.30523 -4.3014908 -4.2966595 -4.2917371 -4.2844362 -4.2809172 -4.2826324 -4.29368 -4.3056951 -4.3125091 -4.3164749 -4.3158407 -4.3129849 -4.311758]]...]
INFO - root - 2017-12-06 00:26:29.907356: step 53810, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 72h:02m:44s remains)
INFO - root - 2017-12-06 00:26:39.126080: step 53820, loss = 2.05, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 72h:27m:04s remains)
INFO - root - 2017-12-06 00:26:48.195512: step 53830, loss = 2.10, batch loss = 2.04 (8.5 examples/sec; 0.939 sec/batch; 72h:41m:09s remains)
INFO - root - 2017-12-06 00:26:57.190845: step 53840, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 70h:46m:10s remains)
INFO - root - 2017-12-06 00:27:06.332772: step 53850, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 69h:25m:26s remains)
INFO - root - 2017-12-06 00:27:15.502303: step 53860, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 68h:10m:06s remains)
INFO - root - 2017-12-06 00:27:24.473961: step 53870, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.770 sec/batch; 59h:33m:36s remains)
INFO - root - 2017-12-06 00:27:33.405921: step 53880, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 69h:00m:45s remains)
INFO - root - 2017-12-06 00:27:42.691358: step 53890, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 72h:07m:41s remains)
INFO - root - 2017-12-06 00:27:51.863602: step 53900, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.953 sec/batch; 73h:43m:58s remains)
2017-12-06 00:27:52.665699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2315092 -4.2300882 -4.2395687 -4.24919 -4.2577019 -4.263166 -4.2659717 -4.2686152 -4.2697124 -4.2685518 -4.2669554 -4.2632861 -4.2588205 -4.2545991 -4.2513757][-4.18311 -4.1805849 -4.1932263 -4.206389 -4.2164512 -4.222908 -4.2253351 -4.2284622 -4.2301679 -4.2301159 -4.2301049 -4.2250066 -4.2173533 -4.2095928 -4.2041063][-4.1426115 -4.139276 -4.1529965 -4.1630235 -4.1677246 -4.1723533 -4.1735082 -4.1764212 -4.1805687 -4.1848478 -4.1893277 -4.1820526 -4.1707258 -4.160779 -4.153389][-4.1229372 -4.1151142 -4.1233811 -4.1215949 -4.1105089 -4.1062956 -4.101851 -4.1000056 -4.105113 -4.1192975 -4.1375332 -4.1369081 -4.1289163 -4.1212263 -4.1140718][-4.1151814 -4.0992937 -4.0944548 -4.0738521 -4.0412712 -4.0195055 -4.0013933 -3.9909251 -3.998477 -4.0272236 -4.0640283 -4.0813184 -4.0886679 -4.0928693 -4.0922194][-4.10578 -4.0867934 -4.0691595 -4.0264983 -3.9666026 -3.9232948 -3.884897 -3.8591948 -3.8759594 -3.9271688 -3.985074 -4.0257425 -4.0583644 -4.0805855 -4.0870428][-4.0859494 -4.0680943 -4.03962 -3.9773004 -3.89189 -3.8167992 -3.7339797 -3.677649 -3.7189758 -3.8112502 -3.8930745 -3.9614558 -4.0213313 -4.0582514 -4.0702524][-4.0641174 -4.0491133 -4.0171614 -3.9517324 -3.8656602 -3.7720251 -3.6473045 -3.5619922 -3.6319747 -3.7506757 -3.8421049 -3.9223256 -3.9913182 -4.0327959 -4.0518894][-4.0471883 -4.0313582 -4.006196 -3.9595771 -3.9081807 -3.8440516 -3.7507675 -3.6951656 -3.7525654 -3.8363547 -3.8980331 -3.9570379 -4.0059547 -4.037303 -4.0599856][-4.0351372 -4.0111551 -3.9922791 -3.9742374 -3.9684074 -3.9464681 -3.9065118 -3.8917058 -3.9286804 -3.9649935 -3.987726 -4.0215425 -4.0512171 -4.0729847 -4.0977125][-4.0302267 -3.9957466 -3.9817662 -3.9939249 -4.0267596 -4.0331311 -4.0307708 -4.0404897 -4.0594993 -4.0654364 -4.0684648 -4.092391 -4.1175022 -4.143445 -4.1719708][-4.032557 -3.9934428 -3.9847167 -4.0183344 -4.0749159 -4.1024723 -4.1189218 -4.1343446 -4.1412277 -4.1322007 -4.1305962 -4.1513376 -4.1746206 -4.2004242 -4.223856][-4.0326867 -4.0068583 -4.0172439 -4.0708284 -4.1374617 -4.1691308 -4.1817822 -4.1836166 -4.1809959 -4.1685004 -4.1643958 -4.1757555 -4.1898346 -4.2051907 -4.2176027][-4.0620828 -4.0642023 -4.0957127 -4.1525774 -4.208735 -4.2320395 -4.2319717 -4.221014 -4.2065043 -4.184546 -4.1699557 -4.1657505 -4.1649184 -4.1676288 -4.1730642][-4.1158681 -4.1372337 -4.1729565 -4.2167907 -4.2550087 -4.2637219 -4.2537947 -4.2410755 -4.2233133 -4.1964149 -4.1690793 -4.1478376 -4.1312141 -4.1226244 -4.125948]]...]
INFO - root - 2017-12-06 00:28:01.751897: step 53910, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 71h:17m:37s remains)
INFO - root - 2017-12-06 00:28:10.694083: step 53920, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 68h:15m:03s remains)
INFO - root - 2017-12-06 00:28:19.730480: step 53930, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 71h:03m:25s remains)
INFO - root - 2017-12-06 00:28:28.903557: step 53940, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 69h:26m:54s remains)
INFO - root - 2017-12-06 00:28:37.869728: step 53950, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 69h:36m:46s remains)
INFO - root - 2017-12-06 00:28:46.895669: step 53960, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 69h:37m:27s remains)
INFO - root - 2017-12-06 00:28:55.895341: step 53970, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.927 sec/batch; 71h:43m:11s remains)
INFO - root - 2017-12-06 00:29:04.926359: step 53980, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 68h:28m:10s remains)
INFO - root - 2017-12-06 00:29:13.975806: step 53990, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 70h:10m:43s remains)
INFO - root - 2017-12-06 00:29:23.127607: step 54000, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 70h:18m:56s remains)
2017-12-06 00:29:23.956584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2696571 -4.2697749 -4.2703204 -4.2707729 -4.2710061 -4.2703528 -4.26936 -4.2677808 -4.2665329 -4.2665377 -4.2680545 -4.2704959 -4.2737541 -4.276938 -4.2795906][-4.2663851 -4.2650442 -4.2643261 -4.2638979 -4.263545 -4.2621317 -4.2600889 -4.2578917 -4.2566509 -4.2573829 -4.2600517 -4.2636209 -4.2680607 -4.2724209 -4.2759347][-4.2654042 -4.2624774 -4.2601957 -4.258214 -4.2561822 -4.2527184 -4.2482395 -4.244318 -4.2428265 -4.2443748 -4.2485442 -4.2540512 -4.260745 -4.2674584 -4.2728572][-4.263062 -4.2583046 -4.2537088 -4.24978 -4.2453785 -4.2391882 -4.2325335 -4.22782 -4.2266941 -4.2287974 -4.2339644 -4.2413316 -4.250526 -4.2598476 -4.26762][-4.254519 -4.2487669 -4.2418933 -4.2356887 -4.2284923 -4.2196136 -4.2112994 -4.2061796 -4.2046061 -4.205626 -4.2102485 -4.2184324 -4.2306333 -4.2439294 -4.2554612][-4.237123 -4.2335067 -4.2273307 -4.2212043 -4.212997 -4.2018294 -4.1914458 -4.1844335 -4.1802683 -4.1776419 -4.1787319 -4.1848788 -4.1980495 -4.2145443 -4.2301469][-4.2141423 -4.2153182 -4.2140207 -4.2129583 -4.208066 -4.1969724 -4.1837831 -4.1725545 -4.1632686 -4.1550469 -4.15065 -4.1525812 -4.1628571 -4.1776366 -4.193481][-4.2023296 -4.2068305 -4.2103767 -4.2141657 -4.2127333 -4.2032733 -4.1905565 -4.1776557 -4.1645021 -4.1518908 -4.1435304 -4.1404057 -4.1436391 -4.1513782 -4.16176][-4.2170243 -4.2203412 -4.223897 -4.2270355 -4.2238746 -4.21296 -4.201098 -4.1902895 -4.1781917 -4.1666245 -4.1597638 -4.1556411 -4.15352 -4.1538181 -4.1559448][-4.2375622 -4.2387886 -4.2413568 -4.24393 -4.2408953 -4.2325425 -4.224628 -4.2184038 -4.211154 -4.204381 -4.2008891 -4.19684 -4.1917634 -4.1869779 -4.1829305][-4.2485061 -4.2491789 -4.2521687 -4.2564464 -4.257318 -4.2553635 -4.2528067 -4.2514758 -4.2492094 -4.2471385 -4.2459564 -4.2417278 -4.2356291 -4.2288179 -4.2227592][-4.2631369 -4.2653289 -4.2698674 -4.2757068 -4.2801604 -4.28231 -4.2819295 -4.2821965 -4.2818165 -4.2814865 -4.2809653 -4.2770586 -4.2718263 -4.2661529 -4.2619433][-4.2791591 -4.2830176 -4.2884021 -4.2942514 -4.299736 -4.3027163 -4.3019924 -4.3006759 -4.2973328 -4.2946062 -4.293138 -4.2909789 -4.28944 -4.2881227 -4.2882743][-4.2956009 -4.3007665 -4.3059859 -4.3100462 -4.3137746 -4.31571 -4.3142729 -4.3106508 -4.3027983 -4.295505 -4.2906179 -4.2885647 -4.2899694 -4.2926722 -4.2967196][-4.3093162 -4.3150573 -4.31791 -4.3180623 -4.3185539 -4.3191814 -4.31834 -4.314065 -4.3045554 -4.294405 -4.2856393 -4.2810025 -4.2812848 -4.2845407 -4.2898617]]...]
INFO - root - 2017-12-06 00:29:33.035842: step 54010, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 68h:40m:51s remains)
INFO - root - 2017-12-06 00:29:42.214572: step 54020, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.902 sec/batch; 69h:48m:31s remains)
INFO - root - 2017-12-06 00:29:51.229793: step 54030, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 69h:04m:30s remains)
INFO - root - 2017-12-06 00:30:00.277236: step 54040, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.890 sec/batch; 68h:50m:45s remains)
INFO - root - 2017-12-06 00:30:09.560986: step 54050, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.957 sec/batch; 74h:03m:08s remains)
INFO - root - 2017-12-06 00:30:18.743564: step 54060, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.944 sec/batch; 73h:00m:30s remains)
INFO - root - 2017-12-06 00:30:27.824238: step 54070, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.941 sec/batch; 72h:48m:33s remains)
INFO - root - 2017-12-06 00:30:36.813066: step 54080, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 68h:31m:23s remains)
INFO - root - 2017-12-06 00:30:45.875500: step 54090, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 69h:39m:50s remains)
INFO - root - 2017-12-06 00:30:55.063162: step 54100, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.929 sec/batch; 71h:52m:34s remains)
2017-12-06 00:30:55.829957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2765026 -4.2698517 -4.2621613 -4.2528939 -4.2422194 -4.2310858 -4.2232666 -4.2201281 -4.2257357 -4.2401309 -4.2617846 -4.28664 -4.3035679 -4.3083425 -4.3048663][-4.27343 -4.2644768 -4.2533388 -4.2396512 -4.2231417 -4.2053461 -4.1913567 -4.1850786 -4.1908364 -4.2086806 -4.2373204 -4.2719584 -4.2984815 -4.3104243 -4.3115177][-4.2727723 -4.2614636 -4.2462072 -4.2272387 -4.2032657 -4.1776123 -4.1577063 -4.1493206 -4.1562119 -4.1787362 -4.2149348 -4.2580686 -4.2927456 -4.3109555 -4.3158283][-4.2720041 -4.2567029 -4.2368507 -4.2127633 -4.1826386 -4.1508188 -4.1263642 -4.1164904 -4.1257367 -4.155056 -4.199492 -4.2487903 -4.2879658 -4.3101983 -4.3182564][-4.2724166 -4.2531767 -4.2284236 -4.1996303 -4.1652403 -4.1294894 -4.1027312 -4.093709 -4.105948 -4.1397986 -4.1893344 -4.2418818 -4.2829852 -4.3080707 -4.3193736][-4.2743907 -4.2523651 -4.2232461 -4.1888914 -4.150774 -4.1120448 -4.0839787 -4.0760794 -4.0907974 -4.1252103 -4.1757412 -4.2295256 -4.2726674 -4.3013239 -4.3165774][-4.2806063 -4.2560678 -4.2239561 -4.1853004 -4.1435022 -4.1015115 -4.0705705 -4.0616884 -4.0767851 -4.1103907 -4.1598487 -4.2135787 -4.2588696 -4.2909894 -4.3107777][-4.2908525 -4.2646027 -4.231771 -4.1926379 -4.1504683 -4.1084042 -4.076489 -4.0654221 -4.0774078 -4.1069736 -4.1515937 -4.2012577 -4.2455831 -4.2794447 -4.3030338][-4.3053918 -4.2798896 -4.2489944 -4.2129836 -4.1740017 -4.1350427 -4.1044774 -4.0918751 -4.0998044 -4.1233888 -4.1590767 -4.2001038 -4.2399812 -4.2730713 -4.2976575][-4.3226752 -4.302021 -4.2765083 -4.2463803 -4.2134113 -4.1803479 -4.1535935 -4.1415262 -4.1462641 -4.1628838 -4.1873555 -4.2161975 -4.2472739 -4.2751837 -4.2970552][-4.33703 -4.3264952 -4.3099627 -4.2875147 -4.2617793 -4.235775 -4.2143774 -4.2032447 -4.2040944 -4.2132387 -4.227788 -4.2452083 -4.26589 -4.2855558 -4.3014321][-4.3353977 -4.3381243 -4.3333211 -4.31981 -4.3012795 -4.281702 -4.2651577 -4.2546263 -4.2513547 -4.2545309 -4.2623978 -4.27238 -4.2853904 -4.2979517 -4.3076925][-4.3175287 -4.3306174 -4.3375568 -4.334775 -4.3249826 -4.3128552 -4.30156 -4.2929406 -4.2877021 -4.2871203 -4.2906322 -4.2963996 -4.3037071 -4.31041 -4.3152013][-4.2975335 -4.314353 -4.3263721 -4.3316126 -4.3314795 -4.3290672 -4.3258553 -4.3216724 -4.3172374 -4.314723 -4.3148127 -4.3163495 -4.3184338 -4.3203878 -4.3217077][-4.2838264 -4.2999897 -4.3107176 -4.3173585 -4.3217769 -4.3261919 -4.3308339 -4.3336039 -4.3334155 -4.3318286 -4.3306832 -4.3300638 -4.3292279 -4.3282809 -4.3276143]]...]
INFO - root - 2017-12-06 00:31:04.964576: step 54110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 70h:07m:35s remains)
INFO - root - 2017-12-06 00:31:13.944073: step 54120, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 69h:07m:32s remains)
INFO - root - 2017-12-06 00:31:23.257285: step 54130, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 71h:20m:43s remains)
INFO - root - 2017-12-06 00:31:32.414442: step 54140, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 68h:26m:51s remains)
INFO - root - 2017-12-06 00:31:41.379260: step 54150, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 69h:48m:05s remains)
INFO - root - 2017-12-06 00:31:50.377394: step 54160, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.894 sec/batch; 69h:06m:23s remains)
INFO - root - 2017-12-06 00:31:59.534650: step 54170, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.926 sec/batch; 71h:35m:30s remains)
INFO - root - 2017-12-06 00:32:08.570139: step 54180, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 70h:46m:56s remains)
INFO - root - 2017-12-06 00:32:17.666644: step 54190, loss = 2.05, batch loss = 1.99 (8.3 examples/sec; 0.959 sec/batch; 74h:08m:06s remains)
INFO - root - 2017-12-06 00:32:26.875735: step 54200, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.911 sec/batch; 70h:24m:56s remains)
2017-12-06 00:32:27.618138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1688828 -4.1243086 -4.0928178 -4.0511532 -4.0067549 -3.9819546 -3.9423342 -3.8980608 -3.9242382 -3.9866693 -4.0359588 -4.0532165 -4.0608792 -4.0688581 -4.0694242][-4.1657062 -4.1193838 -4.0855212 -4.03847 -3.9932661 -3.9810004 -3.9560287 -3.9227123 -3.953681 -4.0140152 -4.0662546 -4.0866132 -4.0878305 -4.0885658 -4.0830693][-4.166039 -4.121861 -4.0912685 -4.0469241 -4.005322 -4.0003223 -3.9799919 -3.9454918 -3.963557 -4.01899 -4.0813608 -4.114634 -4.1185379 -4.1123862 -4.1030331][-4.1736178 -4.1310034 -4.1019487 -4.0610781 -4.0207644 -4.0134778 -3.9823964 -3.9288514 -3.9254909 -3.9869125 -4.0715804 -4.1232138 -4.1349158 -4.1292033 -4.1167445][-4.1816769 -4.1380248 -4.1068306 -4.0717435 -4.0359721 -4.0212088 -3.9673297 -3.8843114 -3.8653352 -3.94051 -4.0433626 -4.1060681 -4.1305523 -4.1388969 -4.1344647][-4.1886396 -4.1450515 -4.1151218 -4.090395 -4.0609856 -4.03199 -3.9404123 -3.8110566 -3.7729228 -3.873224 -3.9969194 -4.0675817 -4.1075954 -4.1367617 -4.1500449][-4.19467 -4.1518192 -4.12368 -4.1059818 -4.0803418 -4.04161 -3.9283438 -3.777065 -3.7485852 -3.8614233 -3.9796731 -4.0360355 -4.0746641 -4.1196575 -4.1500816][-4.198669 -4.15661 -4.1324687 -4.1232309 -4.1081557 -4.0797119 -3.9902408 -3.8725979 -3.8657715 -3.9554088 -4.033145 -4.0587549 -4.074554 -4.1131597 -4.1479874][-4.2048693 -4.1646547 -4.14313 -4.1431465 -4.1422319 -4.1310291 -4.0740428 -3.9969003 -4.0051484 -4.0718355 -4.1147933 -4.1169672 -4.1157165 -4.135273 -4.1574016][-4.2123528 -4.1729841 -4.1527152 -4.16004 -4.1676321 -4.1674743 -4.1313572 -4.08454 -4.103168 -4.1511803 -4.1742687 -4.1664424 -4.152988 -4.1537509 -4.1585035][-4.2204976 -4.184166 -4.1659222 -4.17612 -4.187469 -4.1937575 -4.1705289 -4.1396375 -4.1589179 -4.1897254 -4.1949029 -4.1777215 -4.1604128 -4.1528983 -4.1507225][-4.2285624 -4.19513 -4.1789513 -4.1876163 -4.1960044 -4.2059674 -4.1917605 -4.1618948 -4.1719527 -4.1923962 -4.1884203 -4.1699281 -4.1525021 -4.1443892 -4.1416817][-4.2359524 -4.2039084 -4.1885324 -4.1924644 -4.1953025 -4.2056975 -4.1943793 -4.1624293 -4.1686335 -4.1895647 -4.1893687 -4.1766081 -4.16027 -4.1526704 -4.1508942][-4.240829 -4.209218 -4.1927838 -4.1918616 -4.1903243 -4.199326 -4.1884565 -4.1583786 -4.1697555 -4.1994448 -4.206285 -4.1991014 -4.1866746 -4.1808391 -4.1800632][-4.2463264 -4.2160764 -4.1992612 -4.1951928 -4.1905956 -4.1975474 -4.1867886 -4.1601796 -4.1767049 -4.212111 -4.2252359 -4.2232614 -4.2163253 -4.2142715 -4.2155538]]...]
INFO - root - 2017-12-06 00:32:36.587593: step 54210, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 67h:28m:22s remains)
INFO - root - 2017-12-06 00:32:45.657775: step 54220, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 70h:50m:15s remains)
INFO - root - 2017-12-06 00:32:54.769002: step 54230, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.914 sec/batch; 70h:41m:17s remains)
INFO - root - 2017-12-06 00:33:04.057693: step 54240, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.920 sec/batch; 71h:05m:21s remains)
INFO - root - 2017-12-06 00:33:13.173996: step 54250, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 69h:40m:58s remains)
INFO - root - 2017-12-06 00:33:22.158242: step 54260, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 71h:40m:03s remains)
INFO - root - 2017-12-06 00:33:31.395155: step 54270, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.943 sec/batch; 72h:52m:56s remains)
INFO - root - 2017-12-06 00:33:40.621693: step 54280, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 68h:48m:14s remains)
INFO - root - 2017-12-06 00:33:49.731501: step 54290, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 64h:48m:23s remains)
INFO - root - 2017-12-06 00:33:58.934669: step 54300, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 68h:56m:43s remains)
2017-12-06 00:33:59.756043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2455006 -4.2326803 -4.202508 -4.1589808 -4.1123037 -4.0566163 -4.0152 -4.0375133 -4.0937719 -4.1416845 -4.142827 -4.0974336 -4.0740156 -4.0903549 -4.1182146][-4.243083 -4.2304993 -4.1985917 -4.1488743 -4.0995016 -4.0405083 -4.00203 -4.0336657 -4.0969291 -4.1504574 -4.1650705 -4.1380863 -4.1170144 -4.1165123 -4.1230106][-4.2438335 -4.2314649 -4.2019572 -4.1517549 -4.099339 -4.0303574 -3.9787107 -4.0066142 -4.0721207 -4.1315956 -4.1572595 -4.1483994 -4.1354995 -4.1277471 -4.1205463][-4.2453852 -4.2309623 -4.2002745 -4.1468163 -4.08959 -4.0079017 -3.9367406 -3.9594548 -4.033226 -4.1039734 -4.1391449 -4.1450748 -4.1438489 -4.1371131 -4.1175303][-4.2451591 -4.2292361 -4.1962624 -4.1382837 -4.0689697 -3.9690046 -3.8772931 -3.9003634 -3.9885647 -4.0723739 -4.1227264 -4.1485462 -4.1582775 -4.1465464 -4.1142125][-4.2427826 -4.225193 -4.1900792 -4.1296082 -4.0453873 -3.9183557 -3.8033226 -3.8311203 -3.9429536 -4.0474086 -4.1172256 -4.1586804 -4.1747522 -4.1579943 -4.1204214][-4.242557 -4.2221265 -4.1837988 -4.1210747 -4.02471 -3.8777325 -3.7500181 -3.7917674 -3.9291167 -4.0519333 -4.1352024 -4.1818919 -4.1951227 -4.1711178 -4.1247959][-4.2434258 -4.2219257 -4.1814427 -4.1219406 -4.0289083 -3.8858116 -3.7721 -3.8285532 -3.9697483 -4.0892172 -4.1679153 -4.2106767 -4.2136111 -4.1744785 -4.1105409][-4.2439108 -4.2221842 -4.1870384 -4.1387382 -4.0612974 -3.9372144 -3.8516164 -3.9132814 -4.0355959 -4.13379 -4.1980562 -4.2287064 -4.2206593 -4.1688132 -4.0932593][-4.2434816 -4.2231245 -4.1967497 -4.1623545 -4.0993152 -4.0012684 -3.9456682 -4.0050616 -4.1019335 -4.1745796 -4.221045 -4.2389274 -4.2246404 -4.1722875 -4.1082478][-4.2451262 -4.2293806 -4.2113781 -4.1862698 -4.1336 -4.0613875 -4.033392 -4.0860171 -4.1583657 -4.2109876 -4.2454567 -4.253252 -4.233407 -4.187901 -4.1430054][-4.2473373 -4.2372808 -4.2266297 -4.2076631 -4.1666827 -4.1196752 -4.1131186 -4.15665 -4.20783 -4.2453976 -4.2690578 -4.2718954 -4.2482243 -4.2094927 -4.1746578][-4.2507095 -4.2442913 -4.2401819 -4.2260838 -4.1963248 -4.1712461 -4.1765342 -4.2133942 -4.251853 -4.2783403 -4.2893486 -4.2872243 -4.2621956 -4.2327251 -4.2055349][-4.2535448 -4.2505631 -4.2509923 -4.2407084 -4.2195759 -4.2110291 -4.223938 -4.2537284 -4.2834625 -4.2986794 -4.2989755 -4.2919345 -4.2718997 -4.252213 -4.2328086][-4.2559361 -4.2550592 -4.257556 -4.2491517 -4.2336192 -4.2322788 -4.2451797 -4.2692523 -4.29115 -4.2976227 -4.2932296 -4.2861595 -4.2742014 -4.2627664 -4.2503738]]...]
INFO - root - 2017-12-06 00:34:08.823809: step 54310, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 70h:11m:38s remains)
INFO - root - 2017-12-06 00:34:18.012682: step 54320, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 70h:59m:03s remains)
INFO - root - 2017-12-06 00:34:27.177626: step 54330, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 71h:48m:10s remains)
INFO - root - 2017-12-06 00:34:36.476665: step 54340, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.958 sec/batch; 73h:59m:40s remains)
INFO - root - 2017-12-06 00:34:45.603870: step 54350, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.916 sec/batch; 70h:44m:38s remains)
INFO - root - 2017-12-06 00:34:54.756819: step 54360, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 69h:12m:05s remains)
INFO - root - 2017-12-06 00:35:03.770952: step 54370, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 69h:39m:10s remains)
INFO - root - 2017-12-06 00:35:12.894732: step 54380, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.934 sec/batch; 72h:08m:42s remains)
INFO - root - 2017-12-06 00:35:21.986669: step 54390, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 68h:24m:46s remains)
INFO - root - 2017-12-06 00:35:30.859219: step 54400, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 69h:26m:35s remains)
2017-12-06 00:35:31.639035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3157039 -4.3009572 -4.2783747 -4.2429767 -4.1970496 -4.1633143 -4.1475611 -4.16138 -4.1998444 -4.2457304 -4.27115 -4.2641683 -4.2507429 -4.2195759 -4.1842847][-4.3133864 -4.295073 -4.2688274 -4.2297525 -4.1811881 -4.1431737 -4.1272016 -4.1463275 -4.1871552 -4.2342224 -4.2664504 -4.2658286 -4.25565 -4.2261691 -4.1894336][-4.3096929 -4.2907825 -4.26437 -4.223803 -4.1717825 -4.1269617 -4.108243 -4.13011 -4.1722574 -4.2141552 -4.2470908 -4.2544661 -4.2543902 -4.2362638 -4.2024031][-4.3075662 -4.288908 -4.2629395 -4.222064 -4.1706104 -4.1223826 -4.102252 -4.1235852 -4.1617627 -4.1954303 -4.22088 -4.2311883 -4.2423582 -4.2406697 -4.2157078][-4.3081794 -4.2880306 -4.257082 -4.2113724 -4.1545057 -4.1023016 -4.0808873 -4.1063867 -4.147841 -4.1741686 -4.1918068 -4.2027659 -4.220376 -4.2298284 -4.2168918][-4.3096671 -4.2860289 -4.2471957 -4.1876287 -4.1160583 -4.0560045 -4.0297375 -4.0644321 -4.1191025 -4.142879 -4.1534176 -4.167963 -4.1953106 -4.2152748 -4.2154536][-4.31055 -4.2865005 -4.2430248 -4.1696882 -4.0756774 -3.9924786 -3.9435036 -3.9806485 -4.0581269 -4.0955038 -4.1136236 -4.1405044 -4.17643 -4.2019472 -4.2090206][-4.3114853 -4.2886052 -4.244205 -4.1635823 -4.0501356 -3.9400783 -3.8597994 -3.8868663 -3.9853549 -4.0452847 -4.0794244 -4.1164927 -4.1545453 -4.1798129 -4.189477][-4.3110948 -4.2875552 -4.24413 -4.16547 -4.0538206 -3.9432583 -3.857188 -3.8801856 -3.9817808 -4.0506968 -4.0893593 -4.1244054 -4.1521444 -4.1656828 -4.1676674][-4.3080664 -4.2850761 -4.2434125 -4.1733603 -4.0822315 -3.9969327 -3.9341702 -3.9596884 -4.0446496 -4.0998049 -4.1285186 -4.1504941 -4.1586137 -4.1548014 -4.1483374][-4.3015895 -4.2768474 -4.234673 -4.1742344 -4.1060333 -4.0500979 -4.0137606 -4.0420961 -4.1141891 -4.1572523 -4.1779127 -4.1870413 -4.1820669 -4.1672525 -4.1545525][-4.2928572 -4.2647495 -4.2239008 -4.1743364 -4.1245141 -4.0897055 -4.0743937 -4.1058378 -4.1730547 -4.2129855 -4.23086 -4.2324219 -4.2184873 -4.1957989 -4.176671][-4.2860446 -4.2552853 -4.217773 -4.1787286 -4.1423025 -4.1206961 -4.1172061 -4.1472483 -4.204596 -4.2403736 -4.2597232 -4.2654715 -4.2553453 -4.2363253 -4.2174249][-4.285522 -4.2558751 -4.2224603 -4.1913805 -4.163949 -4.1516614 -4.1523528 -4.1758122 -4.2219496 -4.2541094 -4.2752743 -4.2870879 -4.2886205 -4.2810593 -4.2677207][-4.291605 -4.2658863 -4.2361178 -4.2100592 -4.1887069 -4.1812658 -4.1832361 -4.2036347 -4.2396522 -4.2670355 -4.2864089 -4.2991056 -4.3066216 -4.3075442 -4.3009558]]...]
INFO - root - 2017-12-06 00:35:40.694500: step 54410, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 69h:21m:28s remains)
INFO - root - 2017-12-06 00:35:49.787281: step 54420, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 69h:42m:32s remains)
INFO - root - 2017-12-06 00:35:58.913464: step 54430, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 71h:39m:27s remains)
INFO - root - 2017-12-06 00:36:07.921942: step 54440, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 69h:22m:12s remains)
INFO - root - 2017-12-06 00:36:16.976765: step 54450, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 70h:49m:16s remains)
INFO - root - 2017-12-06 00:36:26.091090: step 54460, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 67h:24m:43s remains)
INFO - root - 2017-12-06 00:36:35.084360: step 54470, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 70h:24m:52s remains)
INFO - root - 2017-12-06 00:36:44.186573: step 54480, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 71h:03m:13s remains)
INFO - root - 2017-12-06 00:36:53.323302: step 54490, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.895 sec/batch; 69h:09m:07s remains)
INFO - root - 2017-12-06 00:37:02.368703: step 54500, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.936 sec/batch; 72h:14m:51s remains)
2017-12-06 00:37:03.122629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2086749 -4.19297 -4.1680522 -4.1591468 -4.174531 -4.1901717 -4.1964107 -4.214592 -4.2320514 -4.2350869 -4.230577 -4.2163863 -4.1958046 -4.1829615 -4.19234][-4.2158766 -4.1922178 -4.1555872 -4.1375647 -4.15121 -4.1692157 -4.1808362 -4.2020731 -4.2197957 -4.21941 -4.2079892 -4.1931648 -4.1779041 -4.1637673 -4.1689997][-4.2050357 -4.1832561 -4.1428218 -4.1178961 -4.1183524 -4.126256 -4.1366224 -4.15794 -4.1763792 -4.1782508 -4.1699057 -4.1635275 -4.1501136 -4.1353693 -4.1466618][-4.1847696 -4.1682544 -4.1339712 -4.1071029 -4.09345 -4.0846782 -4.0817327 -4.093318 -4.1145096 -4.1318841 -4.1422162 -4.1474042 -4.1368141 -4.1239543 -4.1402969][-4.1765919 -4.1638594 -4.1345463 -4.1030483 -4.0741363 -4.0452404 -4.0177 -4.0081644 -4.0369592 -4.0803595 -4.1169171 -4.1341534 -4.1259861 -4.1127963 -4.1297703][-4.1785784 -4.1673775 -4.137557 -4.0965247 -4.0492735 -3.99067 -3.9186149 -3.8663521 -3.904501 -3.988235 -4.0561914 -4.0870686 -4.0847425 -4.0745711 -4.0874519][-4.1633067 -4.1544595 -4.1241646 -4.0754323 -4.0124254 -3.9215131 -3.795357 -3.6880188 -3.7421398 -3.8784089 -3.9791322 -4.0242896 -4.0286393 -4.0227447 -4.0345521][-4.1457634 -4.148551 -4.1305766 -4.0971031 -4.0503178 -3.9699304 -3.8459311 -3.736604 -3.7792907 -3.9051859 -3.9903665 -4.0241995 -4.025444 -4.0209661 -4.02616][-4.1473317 -4.1614594 -4.1601291 -4.148623 -4.1295309 -4.0773058 -3.9861765 -3.9031994 -3.9160626 -3.987231 -4.0389943 -4.0606461 -4.0562696 -4.0441761 -4.036582][-4.144568 -4.1526666 -4.1518912 -4.1533489 -4.1523666 -4.1240416 -4.0652809 -4.0105224 -4.0115509 -4.0487337 -4.0839505 -4.1017523 -4.0912395 -4.0713124 -4.0540919][-4.1595058 -4.1557779 -4.1480031 -4.1503887 -4.1558771 -4.1380186 -4.1049423 -4.0785384 -4.0845222 -4.1088552 -4.1338396 -4.144515 -4.1288295 -4.106266 -4.0866723][-4.1891308 -4.176826 -4.1621361 -4.1591587 -4.1590047 -4.1466346 -4.1311374 -4.1237063 -4.1334271 -4.1533971 -4.1735096 -4.1816607 -4.1661873 -4.1439934 -4.1254272][-4.2159677 -4.204608 -4.18889 -4.1812115 -4.1748013 -4.1610065 -4.1509643 -4.1478629 -4.154326 -4.1724916 -4.1959009 -4.2078576 -4.195075 -4.1759582 -4.1619196][-4.2286391 -4.2190094 -4.2058735 -4.195962 -4.1848359 -4.1685233 -4.1573381 -4.1538568 -4.1602921 -4.1796031 -4.2074814 -4.2238832 -4.2158995 -4.2010489 -4.1906333][-4.2281094 -4.2184792 -4.2069116 -4.1954837 -4.1823139 -4.1673594 -4.1576338 -4.15533 -4.1625991 -4.1846728 -4.21502 -4.2312388 -4.2304139 -4.2221994 -4.2121863]]...]
INFO - root - 2017-12-06 00:37:12.204088: step 54510, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.937 sec/batch; 72h:19m:37s remains)
INFO - root - 2017-12-06 00:37:21.208151: step 54520, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 68h:59m:50s remains)
INFO - root - 2017-12-06 00:37:30.039042: step 54530, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 68h:06m:57s remains)
INFO - root - 2017-12-06 00:37:39.101915: step 54540, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 68h:52m:36s remains)
INFO - root - 2017-12-06 00:37:48.203778: step 54550, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 66h:15m:55s remains)
INFO - root - 2017-12-06 00:37:57.369110: step 54560, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 69h:24m:50s remains)
INFO - root - 2017-12-06 00:38:06.531931: step 54570, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 73h:49m:09s remains)
INFO - root - 2017-12-06 00:38:15.695537: step 54580, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 71h:12m:28s remains)
INFO - root - 2017-12-06 00:38:24.653076: step 54590, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.924 sec/batch; 71h:18m:12s remains)
INFO - root - 2017-12-06 00:38:33.625271: step 54600, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 65h:09m:27s remains)
2017-12-06 00:38:34.410526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2722516 -4.2670455 -4.2544227 -4.2415166 -4.2308989 -4.2146277 -4.2015138 -4.1980705 -4.209548 -4.2375374 -4.2720623 -4.3011217 -4.3217831 -4.3351135 -4.3380871][-4.2300944 -4.2247481 -4.2111168 -4.196456 -4.1821656 -4.163619 -4.1439805 -4.1322722 -4.1447196 -4.1828756 -4.2319 -4.2694807 -4.2968273 -4.319056 -4.3267274][-4.1892581 -4.1811 -4.1668096 -4.1520233 -4.1403875 -4.1277971 -4.1055355 -4.0862265 -4.0987959 -4.1475315 -4.2066875 -4.2487707 -4.2810802 -4.3073854 -4.3179655][-4.1736636 -4.162509 -4.1431561 -4.1272197 -4.1186929 -4.109973 -4.0855651 -4.0662055 -4.0902882 -4.148735 -4.209487 -4.2495685 -4.2824845 -4.3078256 -4.3180976][-4.1836219 -4.163033 -4.1332793 -4.1089821 -4.0968127 -4.0857673 -4.0596747 -4.0544529 -4.1031933 -4.1721354 -4.2302809 -4.2643547 -4.2934422 -4.3159795 -4.3243289][-4.1834249 -4.1506958 -4.1031909 -4.0620236 -4.0437794 -4.0287633 -4.0021467 -4.0167971 -4.0907111 -4.1756058 -4.2377615 -4.2708158 -4.3007679 -4.323853 -4.3289442][-4.1660218 -4.1288733 -4.066916 -4.0109577 -3.9839444 -3.9570765 -3.9221 -3.948715 -4.0452652 -4.15148 -4.2257833 -4.2637496 -4.29859 -4.3219318 -4.3267989][-4.1387658 -4.1052561 -4.0407114 -3.9825585 -3.9460378 -3.902462 -3.8592157 -3.8858275 -3.9919195 -4.115169 -4.2040768 -4.2488446 -4.2889538 -4.3152657 -4.3231745][-4.1166968 -4.0835042 -4.02102 -3.9642429 -3.9242535 -3.8749804 -3.8355732 -3.8691716 -3.9776423 -4.0991187 -4.189465 -4.2373605 -4.2823873 -4.3129711 -4.322751][-4.1262875 -4.0869241 -4.0251207 -3.9734049 -3.9356701 -3.8903251 -3.8619933 -3.9082026 -4.0103078 -4.1156836 -4.1940188 -4.2389784 -4.2855735 -4.3182163 -4.3252788][-4.1586275 -4.1163783 -4.0589585 -4.0147471 -3.9787276 -3.9391768 -3.9208024 -3.9705787 -4.0601754 -4.1476951 -4.2142091 -4.2541265 -4.297646 -4.326962 -4.328867][-4.2078772 -4.1686721 -4.1216764 -4.0881834 -4.0600953 -4.02977 -4.0200472 -4.0636625 -4.1326075 -4.1973238 -4.2467909 -4.2776046 -4.3124804 -4.3330359 -4.3303633][-4.2573085 -4.2259622 -4.19279 -4.1717448 -4.1553712 -4.1344228 -4.1306205 -4.1633077 -4.2106271 -4.2537808 -4.2851443 -4.3047714 -4.3259821 -4.335928 -4.3303428][-4.2918439 -4.2693996 -4.2484965 -4.23697 -4.2287483 -4.2175841 -4.2155738 -4.2353907 -4.26409 -4.2914138 -4.3123617 -4.3245635 -4.3353796 -4.3381224 -4.3319092][-4.3064203 -4.2915263 -4.2791643 -4.2725749 -4.2685518 -4.2619491 -4.2615876 -4.2723565 -4.2876987 -4.3038006 -4.3185496 -4.32779 -4.3347993 -4.3366823 -4.3332496]]...]
INFO - root - 2017-12-06 00:38:43.578717: step 54610, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 66h:17m:10s remains)
INFO - root - 2017-12-06 00:38:52.532041: step 54620, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 64h:50m:05s remains)
INFO - root - 2017-12-06 00:39:01.621504: step 54630, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 69h:16m:33s remains)
INFO - root - 2017-12-06 00:39:10.719842: step 54640, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.896 sec/batch; 69h:09m:25s remains)
INFO - root - 2017-12-06 00:39:19.938503: step 54650, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 69h:05m:35s remains)
INFO - root - 2017-12-06 00:39:29.018933: step 54660, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 69h:55m:35s remains)
INFO - root - 2017-12-06 00:39:38.025801: step 54670, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 68h:34m:31s remains)
INFO - root - 2017-12-06 00:39:47.097592: step 54680, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 0.774 sec/batch; 59h:45m:39s remains)
INFO - root - 2017-12-06 00:39:56.285399: step 54690, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.932 sec/batch; 71h:55m:34s remains)
INFO - root - 2017-12-06 00:40:05.421977: step 54700, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 69h:10m:40s remains)
2017-12-06 00:40:06.240420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1791244 -4.1872654 -4.2031627 -4.2145476 -4.2096419 -4.2029777 -4.1907282 -4.1779618 -4.1666284 -4.1506519 -4.1404104 -4.1460271 -4.1618514 -4.1763191 -4.1844115][-4.1856966 -4.1921644 -4.2081866 -4.2235522 -4.2268643 -4.22489 -4.2127428 -4.199029 -4.1895285 -4.1755824 -4.1655517 -4.1730723 -4.1917243 -4.2064414 -4.2131281][-4.1859188 -4.1931243 -4.2109036 -4.228827 -4.2348022 -4.2326212 -4.2188993 -4.2058344 -4.1991844 -4.19075 -4.1860843 -4.19573 -4.2145419 -4.2292781 -4.2341986][-4.178401 -4.1876626 -4.2076473 -4.2282186 -4.2348146 -4.2264266 -4.204988 -4.1875238 -4.1825128 -4.1842308 -4.1884365 -4.2028966 -4.2263794 -4.2454824 -4.2496505][-4.171351 -4.1813879 -4.2001438 -4.2162251 -4.2145977 -4.1958303 -4.1604614 -4.1249065 -4.1144333 -4.1334019 -4.1560903 -4.182795 -4.2145333 -4.2412329 -4.2523723][-4.1708384 -4.182857 -4.1963496 -4.1968246 -4.1719141 -4.1306992 -4.0659833 -3.9957805 -3.979574 -4.02944 -4.0858331 -4.134346 -4.1819534 -4.2196517 -4.2428966][-4.1759472 -4.18777 -4.1923938 -4.1708922 -4.1184788 -4.0466008 -3.9362111 -3.8170679 -3.8069272 -3.9039445 -4.0018826 -4.0795541 -4.1449862 -4.1949697 -4.2276735][-4.1905308 -4.204608 -4.2003942 -4.1632991 -4.0983315 -4.0140243 -3.8844745 -3.7520003 -3.7587738 -3.8721433 -3.9778883 -4.0659709 -4.13437 -4.1830049 -4.2137632][-4.2106237 -4.2292151 -4.2256656 -4.1912227 -4.1384668 -4.0736156 -3.980314 -3.8978877 -3.9090436 -3.9745488 -4.0364547 -4.0965347 -4.1394987 -4.1676412 -4.1884575][-4.2295542 -4.2493863 -4.2497964 -4.2255588 -4.1833363 -4.1438622 -4.093832 -4.0543051 -4.0592613 -4.0813007 -4.1025181 -4.1306539 -4.1480656 -4.1517663 -4.1615982][-4.2456784 -4.2620859 -4.2623105 -4.239573 -4.2020497 -4.178165 -4.1574845 -4.13929 -4.13095 -4.1215224 -4.1155362 -4.1197181 -4.1209641 -4.1203775 -4.1325974][-4.2451138 -4.2502465 -4.2435246 -4.2144279 -4.1803107 -4.168282 -4.1663785 -4.1630621 -4.1497579 -4.1236234 -4.1002951 -4.0894866 -4.0852232 -4.0985389 -4.1260543][-4.2243872 -4.2209396 -4.2107124 -4.184556 -4.1582832 -4.1539526 -4.1622128 -4.1703482 -4.1608005 -4.136641 -4.111939 -4.096499 -4.0957084 -4.122221 -4.1604729][-4.2038774 -4.1966677 -4.1894794 -4.1733313 -4.1615067 -4.1647673 -4.1799736 -4.1970143 -4.1982336 -4.1822944 -4.15807 -4.1412687 -4.143918 -4.1740732 -4.2096581][-4.1910706 -4.1778049 -4.1711211 -4.1690326 -4.1758709 -4.191052 -4.2127318 -4.2354074 -4.2467995 -4.2375364 -4.2120395 -4.1913304 -4.1912813 -4.2149253 -4.2424436]]...]
INFO - root - 2017-12-06 00:40:15.159744: step 54710, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 69h:52m:20s remains)
INFO - root - 2017-12-06 00:40:24.255292: step 54720, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 71h:03m:36s remains)
INFO - root - 2017-12-06 00:40:33.340536: step 54730, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 70h:58m:16s remains)
INFO - root - 2017-12-06 00:40:42.317725: step 54740, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 71h:08m:42s remains)
INFO - root - 2017-12-06 00:40:51.452755: step 54750, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.937 sec/batch; 72h:18m:26s remains)
INFO - root - 2017-12-06 00:41:00.443634: step 54760, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 65h:03m:18s remains)
INFO - root - 2017-12-06 00:41:09.589535: step 54770, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.935 sec/batch; 72h:09m:23s remains)
INFO - root - 2017-12-06 00:41:18.689452: step 54780, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 69h:25m:34s remains)
INFO - root - 2017-12-06 00:41:27.715876: step 54790, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 71h:40m:09s remains)
INFO - root - 2017-12-06 00:41:36.742932: step 54800, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 69h:25m:20s remains)
2017-12-06 00:41:37.615984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.236774 -4.2265434 -4.21592 -4.2173367 -4.2257771 -4.233779 -4.2343745 -4.2362185 -4.2492919 -4.2664423 -4.2808318 -4.2835851 -4.2834706 -4.2756639 -4.2658739][-4.2507219 -4.2346182 -4.2159567 -4.2108026 -4.2124496 -4.2109656 -4.2067361 -4.2080526 -4.2252765 -4.2409577 -4.2496204 -4.249743 -4.2498608 -4.2440524 -4.2303667][-4.2668533 -4.2517385 -4.2308922 -4.2193675 -4.2113528 -4.2004223 -4.1942039 -4.2001462 -4.2220016 -4.2341175 -4.2324371 -4.2270684 -4.2214122 -4.2066464 -4.1859932][-4.280251 -4.2735658 -4.2594156 -4.248076 -4.2355852 -4.2180138 -4.205739 -4.2098951 -4.2293267 -4.2396612 -4.2320862 -4.218792 -4.2027135 -4.1802921 -4.1616812][-4.254529 -4.2635255 -4.2662721 -4.2633195 -4.2544427 -4.237525 -4.2240195 -4.2197151 -4.2319932 -4.2414103 -4.2329969 -4.2160606 -4.1977134 -4.1801004 -4.1728215][-4.1902027 -4.2135954 -4.2323022 -4.2376013 -4.232852 -4.2204022 -4.2122326 -4.20617 -4.2085266 -4.2147737 -4.2149739 -4.2112184 -4.207366 -4.205204 -4.2119555][-4.0984855 -4.1344752 -4.1707091 -4.1837535 -4.1807494 -4.1742887 -4.1737428 -4.1664248 -4.160933 -4.1685147 -4.1828308 -4.2008729 -4.2178626 -4.2327628 -4.2504377][-4.0261841 -4.0551815 -4.0952654 -4.1090212 -4.1061392 -4.1089907 -4.11956 -4.1192923 -4.1220083 -4.1396756 -4.1654634 -4.1951289 -4.2233953 -4.2502131 -4.2756038][-4.0511317 -4.0600672 -4.0771236 -4.0729003 -4.0609813 -4.06378 -4.079885 -4.089385 -4.1001873 -4.1234632 -4.1550188 -4.1894546 -4.2232704 -4.2584591 -4.2895956][-4.1075745 -4.1105237 -4.1112437 -4.099649 -4.087503 -4.0863414 -4.0929384 -4.0978527 -4.1045985 -4.1225696 -4.1482029 -4.1756744 -4.2062092 -4.2433643 -4.2771235][-4.165309 -4.16861 -4.1633167 -4.1525407 -4.1457019 -4.1424832 -4.1389303 -4.1335349 -4.1331868 -4.1429625 -4.15243 -4.1588526 -4.1720414 -4.2007618 -4.2318664][-4.2170053 -4.2192655 -4.2137423 -4.2046337 -4.198257 -4.1930838 -4.1849351 -4.1760144 -4.1732864 -4.1764321 -4.1675105 -4.1477075 -4.13708 -4.1486349 -4.1698775][-4.2535806 -4.2543678 -4.2510152 -4.2432079 -4.2376986 -4.231842 -4.2222972 -4.2120008 -4.2088623 -4.2083 -4.1872234 -4.1471224 -4.1130352 -4.104382 -4.11174][-4.2773552 -4.2755823 -4.2717814 -4.2642975 -4.2602944 -4.2554693 -4.2451997 -4.2361054 -4.2360859 -4.2337437 -4.2084765 -4.1646757 -4.1229153 -4.1025867 -4.1011357][-4.2864976 -4.283071 -4.277689 -4.270874 -4.268106 -4.2629528 -4.2531571 -4.247685 -4.2528124 -4.2522392 -4.2330394 -4.2002988 -4.1701908 -4.1529264 -4.1478119]]...]
INFO - root - 2017-12-06 00:41:46.483635: step 54810, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 68h:00m:27s remains)
INFO - root - 2017-12-06 00:41:55.544135: step 54820, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 68h:33m:24s remains)
INFO - root - 2017-12-06 00:42:04.662761: step 54830, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 70h:32m:09s remains)
INFO - root - 2017-12-06 00:42:13.830123: step 54840, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 70h:54m:38s remains)
INFO - root - 2017-12-06 00:42:22.987712: step 54850, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 69h:58m:38s remains)
INFO - root - 2017-12-06 00:42:31.988292: step 54860, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 70h:00m:15s remains)
INFO - root - 2017-12-06 00:42:41.079131: step 54870, loss = 2.06, batch loss = 2.00 (8.2 examples/sec; 0.973 sec/batch; 75h:02m:42s remains)
INFO - root - 2017-12-06 00:42:50.231680: step 54880, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 69h:48m:42s remains)
INFO - root - 2017-12-06 00:42:59.422837: step 54890, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 70h:32m:11s remains)
INFO - root - 2017-12-06 00:43:08.323042: step 54900, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 70h:26m:26s remains)
2017-12-06 00:43:09.084153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1872487 -4.1962132 -4.206789 -4.2198582 -4.2343941 -4.2408218 -4.2333965 -4.2057347 -4.1887164 -4.1734095 -4.1460838 -4.1259251 -4.1223526 -4.1169138 -4.1190162][-4.2127666 -4.2264948 -4.2292304 -4.2354126 -4.2397146 -4.2294116 -4.210103 -4.1789384 -4.1570182 -4.1397386 -4.1180468 -4.1088738 -4.1185517 -4.1219177 -4.1251459][-4.2292953 -4.2458262 -4.2466321 -4.2474141 -4.2453828 -4.226522 -4.2028437 -4.1759315 -4.1539912 -4.1376004 -4.1234579 -4.1240916 -4.1432395 -4.150516 -4.1493444][-4.2206836 -4.23743 -4.2364459 -4.2342286 -4.231678 -4.2127852 -4.1871719 -4.1696281 -4.1568608 -4.1398616 -4.122519 -4.1266761 -4.1476755 -4.1559095 -4.149951][-4.1951656 -4.21145 -4.2064304 -4.1957345 -4.184269 -4.1552043 -4.115191 -4.1064196 -4.1169662 -4.1076517 -4.089952 -4.0992613 -4.118928 -4.1214304 -4.1095338][-4.1589532 -4.1715336 -4.1630292 -4.1403646 -4.1072469 -4.0497069 -3.9810472 -3.9880273 -4.0395551 -4.0540681 -4.0461693 -4.0647798 -4.08682 -4.0834618 -4.0685024][-4.12661 -4.1315765 -4.1230783 -4.0949049 -4.0442839 -3.9564378 -3.8730688 -3.9096231 -3.9986687 -4.036891 -4.0453672 -4.0743618 -4.09934 -4.0952296 -4.0771117][-4.1217294 -4.1234221 -4.1216626 -4.1028967 -4.0639448 -3.9989762 -3.9469736 -3.9903371 -4.0660739 -4.0991087 -4.1114411 -4.1355538 -4.1525869 -4.145812 -4.1270404][-4.1391439 -4.1445394 -4.1471663 -4.1476378 -4.1393538 -4.1113267 -4.0894032 -4.1214495 -4.1682997 -4.1856146 -4.18974 -4.201952 -4.2107935 -4.20528 -4.1882033][-4.1485367 -4.1562266 -4.1619568 -4.178247 -4.1954384 -4.1947279 -4.1927538 -4.2175107 -4.2480788 -4.2579341 -4.25707 -4.2602463 -4.2602057 -4.2537217 -4.2405643][-4.1451912 -4.1577783 -4.1634507 -4.1879721 -4.219727 -4.2361503 -4.250782 -4.2789831 -4.303112 -4.3088908 -4.3074636 -4.3048635 -4.297554 -4.2882762 -4.2774425][-4.1446457 -4.16265 -4.1748509 -4.2029095 -4.2390819 -4.2630134 -4.2851505 -4.3135743 -4.3312097 -4.3324213 -4.3291678 -4.326282 -4.319284 -4.3107066 -4.3035688][-4.1621532 -4.18378 -4.2043037 -4.235117 -4.2704296 -4.2933478 -4.3136215 -4.33541 -4.3473816 -4.3453426 -4.3397703 -4.3359828 -4.32756 -4.3172374 -4.3102322][-4.2127132 -4.2392869 -4.26198 -4.286932 -4.3102317 -4.323904 -4.3357172 -4.3471923 -4.3512669 -4.3460522 -4.3397522 -4.3337293 -4.3231835 -4.31215 -4.3059511][-4.2689672 -4.2970662 -4.3167906 -4.3338561 -4.3436041 -4.3454719 -4.347434 -4.3486176 -4.3455577 -4.3386989 -4.3331304 -4.3286204 -4.3211761 -4.3133507 -4.3085141]]...]
INFO - root - 2017-12-06 00:43:17.956995: step 54910, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 67h:47m:34s remains)
INFO - root - 2017-12-06 00:43:27.112479: step 54920, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 70h:28m:03s remains)
INFO - root - 2017-12-06 00:43:36.227452: step 54930, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 70h:59m:59s remains)
INFO - root - 2017-12-06 00:43:45.267912: step 54940, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 70h:42m:00s remains)
INFO - root - 2017-12-06 00:43:54.307967: step 54950, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 69h:31m:52s remains)
INFO - root - 2017-12-06 00:44:03.357934: step 54960, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 67h:56m:15s remains)
INFO - root - 2017-12-06 00:44:12.330631: step 54970, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 68h:43m:53s remains)
INFO - root - 2017-12-06 00:44:21.579540: step 54980, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 68h:37m:22s remains)
INFO - root - 2017-12-06 00:44:30.719348: step 54990, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:16m:40s remains)
INFO - root - 2017-12-06 00:44:39.616707: step 55000, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 70h:57m:50s remains)
2017-12-06 00:44:40.385852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2579188 -4.2519116 -4.2402062 -4.2193871 -4.198535 -4.1880145 -4.1862259 -4.1958308 -4.2038636 -4.2136078 -4.2251515 -4.2346954 -4.2386069 -4.2366281 -4.2359824][-4.2557096 -4.2491164 -4.2357235 -4.2132063 -4.1868653 -4.1693158 -4.1606145 -4.1676731 -4.1772118 -4.1896334 -4.20325 -4.2097859 -4.2135205 -4.2191691 -4.2284241][-4.245719 -4.2402458 -4.2266469 -4.2034307 -4.1743894 -4.1517367 -4.1373129 -4.141562 -4.1552305 -4.1758075 -4.1926413 -4.1936793 -4.189764 -4.1960387 -4.209671][-4.2330475 -4.2278552 -4.2164164 -4.1981115 -4.1726036 -4.1479454 -4.12974 -4.1291924 -4.1431508 -4.1675878 -4.187036 -4.1856093 -4.1760864 -4.1805391 -4.197144][-4.2263145 -4.220788 -4.209744 -4.1961479 -4.1739216 -4.1511645 -4.1344905 -4.1295428 -4.1372871 -4.1603684 -4.1796679 -4.1744947 -4.1644888 -4.1675553 -4.1835051][-4.2258954 -4.2218595 -4.2100272 -4.1971517 -4.1779647 -4.158289 -4.1424608 -4.1335092 -4.1318779 -4.1481843 -4.1674318 -4.1651936 -4.1563139 -4.1556015 -4.1695743][-4.2294488 -4.2287383 -4.2163372 -4.1990275 -4.1786094 -4.1565752 -4.1353345 -4.1151223 -4.1022377 -4.114296 -4.1437631 -4.1570096 -4.1576505 -4.1579752 -4.1695662][-4.231195 -4.231205 -4.2185488 -4.1991048 -4.1765637 -4.1496363 -4.1166506 -4.0762362 -4.044054 -4.0522208 -4.1061172 -4.1472459 -4.1628156 -4.1685576 -4.1759772][-4.2320685 -4.2338891 -4.2230287 -4.2045054 -4.1811709 -4.151958 -4.1074686 -4.0422587 -3.9778781 -3.9736402 -4.0552659 -4.1346521 -4.1754131 -4.19047 -4.1954279][-4.2356186 -4.2416811 -4.2347155 -4.2180123 -4.1946445 -4.1646514 -4.1206112 -4.0526495 -3.9759109 -3.9542229 -4.0354161 -4.1314168 -4.1890512 -4.2124166 -4.2185922][-4.2445121 -4.2544012 -4.2504067 -4.2345061 -4.2105417 -4.1810236 -4.1469913 -4.0979767 -4.0383468 -4.0115876 -4.0619297 -4.1407809 -4.1967373 -4.2246914 -4.2331696][-4.252562 -4.263185 -4.2595215 -4.2424273 -4.2174535 -4.1887665 -4.1643023 -4.1356344 -4.0968366 -4.0716548 -4.0944571 -4.1487851 -4.1980319 -4.2304215 -4.2439232][-4.2414513 -4.2528262 -4.2509627 -4.2363381 -4.2141209 -4.1875448 -4.1698422 -4.1526127 -4.1297545 -4.1143579 -4.1267891 -4.1669073 -4.21104 -4.2433214 -4.2597032][-4.2243643 -4.2353091 -4.2356482 -4.225523 -4.2082696 -4.1845384 -4.1703219 -4.1608233 -4.1494045 -4.1443448 -4.1580868 -4.192781 -4.2323346 -4.2609105 -4.2754307][-4.2056041 -4.212925 -4.2146306 -4.2125859 -4.2056108 -4.1907277 -4.1812172 -4.1789155 -4.1791811 -4.1840596 -4.1981554 -4.2273483 -4.2601094 -4.28201 -4.2940507]]...]
INFO - root - 2017-12-06 00:44:49.284027: step 55010, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 64h:19m:43s remains)
INFO - root - 2017-12-06 00:44:58.481514: step 55020, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 70h:11m:35s remains)
INFO - root - 2017-12-06 00:45:07.532284: step 55030, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.900 sec/batch; 69h:20m:47s remains)
INFO - root - 2017-12-06 00:45:16.486924: step 55040, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 70h:45m:52s remains)
INFO - root - 2017-12-06 00:45:25.562007: step 55050, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 71h:18m:55s remains)
INFO - root - 2017-12-06 00:45:34.675088: step 55060, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 68h:58m:53s remains)
INFO - root - 2017-12-06 00:45:43.614379: step 55070, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 69h:12m:09s remains)
INFO - root - 2017-12-06 00:45:52.672138: step 55080, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 70h:10m:21s remains)
INFO - root - 2017-12-06 00:46:01.875602: step 55090, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 65h:35m:09s remains)
INFO - root - 2017-12-06 00:46:10.766227: step 55100, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 70h:17m:47s remains)
2017-12-06 00:46:11.560501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26701 -4.2563496 -4.2521076 -4.2520857 -4.2588205 -4.2643251 -4.2655697 -4.263792 -4.266489 -4.2712226 -4.2792783 -4.2913265 -4.3033667 -4.3112717 -4.3150406][-4.2445064 -4.2274065 -4.2197905 -4.2173376 -4.2246571 -4.2331834 -4.2356358 -4.2324877 -4.2389073 -4.2463169 -4.2564487 -4.2721152 -4.2881913 -4.3020968 -4.3110557][-4.2171254 -4.1941085 -4.1838803 -4.1796446 -4.1869011 -4.1983681 -4.2019744 -4.1979256 -4.2077594 -4.2190924 -4.2315674 -4.2505245 -4.2699394 -4.2874093 -4.2993879][-4.1883483 -4.1600113 -4.1488628 -4.1431437 -4.1483994 -4.1603026 -4.1601744 -4.15061 -4.1604171 -4.1763034 -4.1961117 -4.2233596 -4.2470975 -4.2660165 -4.279882][-4.1647677 -4.1259108 -4.1092353 -4.0953655 -4.0940175 -4.1042814 -4.1044903 -4.0919008 -4.1058121 -4.1302581 -4.1585894 -4.195365 -4.2250795 -4.2444363 -4.259769][-4.1474032 -4.1001244 -4.0750494 -4.0503411 -4.0395894 -4.0474815 -4.0455561 -4.0251966 -4.0432391 -4.0831261 -4.124474 -4.1694708 -4.20278 -4.2262506 -4.2461829][-4.139884 -4.0883222 -4.0563064 -4.020381 -3.997597 -3.9929974 -3.9666669 -3.9162414 -3.9351358 -3.9996996 -4.0657425 -4.1284866 -4.1711478 -4.2017231 -4.2300134][-4.143867 -4.0884175 -4.047389 -4.0016112 -3.9663491 -3.9378009 -3.8715281 -3.7920213 -3.8267062 -3.9267616 -4.018743 -4.0976505 -4.1518121 -4.1894679 -4.2211328][-4.1385422 -4.073832 -4.0227766 -3.9640214 -3.9183693 -3.8801534 -3.8109126 -3.7481754 -3.8105044 -3.9212704 -4.0151296 -4.0964952 -4.1555147 -4.1959629 -4.2237539][-4.1406908 -4.0729628 -4.0140219 -3.9525139 -3.9126325 -3.8931561 -3.8546333 -3.8168924 -3.8795388 -3.9773879 -4.0571847 -4.1293659 -4.1842275 -4.2214928 -4.2438636][-4.1636252 -4.1054358 -4.0561838 -4.0106182 -3.9913006 -3.9924669 -3.9744515 -3.9440408 -3.988013 -4.0640903 -4.1217413 -4.1809421 -4.2274184 -4.2563262 -4.2712331][-4.1831665 -4.1399312 -4.10841 -4.0841804 -4.0831141 -4.0938053 -4.085391 -4.0635829 -4.0948524 -4.1462569 -4.1822515 -4.2271357 -4.2662411 -4.2877522 -4.295289][-4.1980848 -4.1684303 -4.151825 -4.1410775 -4.1480894 -4.1615806 -4.1601129 -4.14601 -4.1677718 -4.1989727 -4.2231245 -4.2551556 -4.285099 -4.301724 -4.3060255][-4.2216716 -4.2078285 -4.2051897 -4.2027254 -4.2119403 -4.2228622 -4.2215881 -4.209599 -4.2190518 -4.2351656 -4.2485571 -4.2695742 -4.2903509 -4.3000383 -4.3015556][-4.2585049 -4.2552433 -4.2582235 -4.2565432 -4.2604432 -4.2612667 -4.2554355 -4.2432623 -4.2484221 -4.2584572 -4.267457 -4.2780547 -4.2883663 -4.2895231 -4.2884183]]...]
INFO - root - 2017-12-06 00:46:20.715564: step 55110, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.891 sec/batch; 68h:41m:25s remains)
INFO - root - 2017-12-06 00:46:29.696645: step 55120, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 71h:38m:29s remains)
INFO - root - 2017-12-06 00:46:38.811325: step 55130, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 71h:20m:13s remains)
INFO - root - 2017-12-06 00:46:47.836493: step 55140, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 70h:23m:27s remains)
INFO - root - 2017-12-06 00:46:56.929235: step 55150, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 63h:33m:53s remains)
INFO - root - 2017-12-06 00:47:06.044722: step 55160, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 70h:04m:00s remains)
INFO - root - 2017-12-06 00:47:15.040725: step 55170, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 69h:21m:07s remains)
INFO - root - 2017-12-06 00:47:24.142047: step 55180, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 69h:31m:37s remains)
INFO - root - 2017-12-06 00:47:33.128078: step 55190, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 68h:58m:25s remains)
INFO - root - 2017-12-06 00:47:42.187032: step 55200, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.898 sec/batch; 69h:10m:24s remains)
2017-12-06 00:47:43.047850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1856179 -4.1946397 -4.2016692 -4.2082262 -4.215754 -4.2257371 -4.2332854 -4.2336135 -4.2255898 -4.2134843 -4.2027445 -4.1982951 -4.2019806 -4.2166128 -4.2399454][-4.16453 -4.1825848 -4.1967626 -4.2089872 -4.2212939 -4.2346725 -4.2421708 -4.2405224 -4.2334628 -4.2245255 -4.2170382 -4.2128315 -4.2116585 -4.2175574 -4.2299151][-4.1574526 -4.1830029 -4.2000246 -4.21356 -4.2259636 -4.2382112 -4.2426739 -4.2397037 -4.2360854 -4.2349362 -4.2366891 -4.2380629 -4.2350173 -4.2299843 -4.2239842][-4.1771774 -4.2020426 -4.21458 -4.2220087 -4.2278981 -4.2330055 -4.2308106 -4.2245431 -4.2234716 -4.2311625 -4.2436981 -4.2532363 -4.2532535 -4.2434216 -4.2233877][-4.1958632 -4.2116971 -4.2130313 -4.2073908 -4.1991315 -4.1887207 -4.1730404 -4.1597333 -4.1627479 -4.1830931 -4.2106209 -4.23331 -4.2421055 -4.2344441 -4.21009][-4.1958714 -4.1991854 -4.1845293 -4.161644 -4.1349778 -4.1018491 -4.0620866 -4.0314441 -4.0357504 -4.0750017 -4.1277308 -4.1733351 -4.1999574 -4.2019968 -4.1784964][-4.193676 -4.1859355 -4.1620488 -4.1341166 -4.101861 -4.0561256 -3.9955816 -3.9401174 -3.9310827 -3.9750574 -4.0441036 -4.10969 -4.1555204 -4.17155 -4.1562548][-4.2020373 -4.1946926 -4.1743731 -4.1548724 -4.1349721 -4.1036668 -4.0559363 -4.006618 -3.9903224 -4.0127592 -4.0606747 -4.1130691 -4.1565871 -4.1790404 -4.1753139][-4.2143536 -4.2084327 -4.1951513 -4.1855426 -4.1777234 -4.1645617 -4.1391912 -4.109972 -4.1003628 -4.1126304 -4.1386 -4.1676474 -4.1944013 -4.20948 -4.2086945][-4.2225213 -4.2180567 -4.2104793 -4.2085228 -4.2083054 -4.2054167 -4.193439 -4.1761332 -4.1694841 -4.1764503 -4.1909447 -4.2055545 -4.218 -4.2239223 -4.2205606][-4.2258306 -4.2242093 -4.2204313 -4.2222056 -4.2261677 -4.2283049 -4.2252684 -4.2174253 -4.2132764 -4.2157154 -4.2211533 -4.2242079 -4.2240567 -4.2200522 -4.2121739][-4.2229667 -4.2246561 -4.2220325 -4.222477 -4.2251067 -4.2274184 -4.2289457 -4.229845 -4.232007 -4.2346134 -4.2349081 -4.2293057 -4.2175822 -4.2028265 -4.1907649][-4.2150388 -4.2205181 -4.2192869 -4.2177143 -4.2169623 -4.2166972 -4.2186522 -4.2223086 -4.2269144 -4.23011 -4.2286053 -4.2196903 -4.2027917 -4.1820498 -4.1690655][-4.2155051 -4.2227168 -4.2223568 -4.219358 -4.2153668 -4.2119851 -4.2125683 -4.2166324 -4.2225246 -4.2263708 -4.2253628 -4.2169633 -4.200417 -4.1798277 -4.1679292][-4.2250886 -4.2296615 -4.2275677 -4.2231359 -4.2173853 -4.2123537 -4.2118478 -4.2157946 -4.2221918 -4.2263818 -4.2266774 -4.2209473 -4.2083926 -4.1920824 -4.1832218]]...]
INFO - root - 2017-12-06 00:47:52.081669: step 55210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 66h:35m:43s remains)
INFO - root - 2017-12-06 00:48:01.091403: step 55220, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 70h:16m:17s remains)
INFO - root - 2017-12-06 00:48:10.086290: step 55230, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 68h:49m:59s remains)
INFO - root - 2017-12-06 00:48:19.133641: step 55240, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 68h:57m:30s remains)
INFO - root - 2017-12-06 00:48:28.154852: step 55250, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 71h:10m:05s remains)
INFO - root - 2017-12-06 00:48:37.312179: step 55260, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.895 sec/batch; 68h:55m:18s remains)
INFO - root - 2017-12-06 00:48:46.360388: step 55270, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 65h:35m:29s remains)
INFO - root - 2017-12-06 00:48:55.250550: step 55280, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 68h:07m:22s remains)
INFO - root - 2017-12-06 00:49:04.384860: step 55290, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.907 sec/batch; 69h:51m:26s remains)
INFO - root - 2017-12-06 00:49:13.225984: step 55300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 66h:56m:40s remains)
2017-12-06 00:49:14.027415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1751394 -4.15898 -4.1440387 -4.146687 -4.1635208 -4.183455 -4.2034636 -4.2184172 -4.2168427 -4.208065 -4.2011633 -4.1901813 -4.1808372 -4.1717558 -4.1784916][-4.2089505 -4.2009478 -4.195744 -4.2060571 -4.224402 -4.23862 -4.2484055 -4.2536149 -4.2457237 -4.2303085 -4.2170806 -4.2040048 -4.1987548 -4.1951761 -4.2045794][-4.21389 -4.2138104 -4.2155142 -4.2304215 -4.248455 -4.2567735 -4.2560711 -4.2503867 -4.2401648 -4.2239804 -4.2106709 -4.2020969 -4.201087 -4.1979756 -4.2016754][-4.1936231 -4.1933169 -4.1943512 -4.2084103 -4.2252507 -4.231421 -4.2242255 -4.2091379 -4.1942191 -4.1824751 -4.1753931 -4.1736641 -4.1782055 -4.1728535 -4.1705084][-4.1422982 -4.1320372 -4.1247106 -4.1365852 -4.1555634 -4.1651497 -4.1517715 -4.1227789 -4.1014271 -4.0992684 -4.1071935 -4.1180496 -4.1297064 -4.1270833 -4.1266422][-4.0614824 -4.0368357 -4.023972 -4.0399246 -4.0623603 -4.06998 -4.0462818 -3.9994309 -3.9712837 -3.9836032 -4.0123034 -4.0426416 -4.0699196 -4.0808821 -4.0935574][-4.0054526 -3.9819925 -3.9722328 -3.9846063 -3.9981546 -3.9966455 -3.9613714 -3.8979936 -3.8646646 -3.8844993 -3.9222276 -3.9658542 -4.0099449 -4.041235 -4.0740032][-4.0040193 -3.9897785 -3.9855914 -3.9940519 -4.0006328 -3.9945261 -3.9665377 -3.9153261 -3.8885021 -3.8967571 -3.9191978 -3.9558053 -3.9970412 -4.0308771 -4.0623312][-4.0501075 -4.0433326 -4.0432634 -4.0490551 -4.0522275 -4.0466304 -4.0289426 -3.9990635 -3.9836557 -3.9829078 -3.99337 -4.0168676 -4.0420079 -4.0582037 -4.0664973][-4.1140466 -4.1122503 -4.1147208 -4.1178451 -4.1177759 -4.1148281 -4.1038637 -4.0871778 -4.0788817 -4.074594 -4.0783229 -4.0909252 -4.1029062 -4.099133 -4.0777059][-4.1714211 -4.1710362 -4.1739955 -4.1766052 -4.1776547 -4.177309 -4.1719432 -4.1614103 -4.1537619 -4.1459703 -4.1452804 -4.1498194 -4.1484184 -4.128664 -4.0931854][-4.2163305 -4.2137938 -4.2127943 -4.2131915 -4.2144842 -4.215867 -4.2147245 -4.2064471 -4.1961145 -4.1842122 -4.1792536 -4.1782961 -4.1669149 -4.1383538 -4.10613][-4.2438259 -4.2401071 -4.2361073 -4.234282 -4.2354259 -4.2375135 -4.2372656 -4.2295513 -4.2175355 -4.2025747 -4.1921563 -4.1851358 -4.167768 -4.13995 -4.1183734][-4.2540565 -4.2494459 -4.2453151 -4.2432165 -4.2438912 -4.2459569 -4.2460756 -4.2401447 -4.2294078 -4.215508 -4.1998191 -4.1855884 -4.164535 -4.1424713 -4.132503][-4.2513394 -4.2466478 -4.2450132 -4.2451596 -4.2458878 -4.246367 -4.2452211 -4.2401361 -4.231113 -4.2198973 -4.2056117 -4.1903667 -4.1712461 -4.1562848 -4.1543856]]...]
INFO - root - 2017-12-06 00:49:23.082908: step 55310, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 70h:25m:20s remains)
INFO - root - 2017-12-06 00:49:32.086414: step 55320, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 67h:58m:32s remains)
INFO - root - 2017-12-06 00:49:41.203832: step 55330, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 69h:06m:34s remains)
INFO - root - 2017-12-06 00:49:50.380012: step 55340, loss = 2.06, batch loss = 2.00 (7.7 examples/sec; 1.041 sec/batch; 80h:09m:59s remains)
INFO - root - 2017-12-06 00:49:59.512800: step 55350, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.897 sec/batch; 69h:03m:14s remains)
INFO - root - 2017-12-06 00:50:08.426717: step 55360, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 68h:05m:12s remains)
INFO - root - 2017-12-06 00:50:17.510942: step 55370, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 70h:22m:23s remains)
INFO - root - 2017-12-06 00:50:26.578613: step 55380, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 71h:39m:35s remains)
INFO - root - 2017-12-06 00:50:35.737427: step 55390, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 69h:10m:36s remains)
INFO - root - 2017-12-06 00:50:44.796464: step 55400, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 67h:09m:46s remains)
2017-12-06 00:50:45.560327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.289237 -4.2772284 -4.2647038 -4.2433176 -4.2228041 -4.2160468 -4.2324386 -4.2477617 -4.2505221 -4.2496996 -4.2303104 -4.2104464 -4.2146206 -4.22267 -4.2446856][-4.2722282 -4.2579532 -4.2442675 -4.2177348 -4.1886115 -4.17342 -4.1962118 -4.2175369 -4.2250276 -4.2340364 -4.2209206 -4.20287 -4.20492 -4.2086368 -4.232224][-4.2532682 -4.2381749 -4.2219524 -4.1877432 -4.1499877 -4.1263003 -4.1495256 -4.179647 -4.2005124 -4.22138 -4.22145 -4.2095714 -4.2056623 -4.2051468 -4.229538][-4.2390542 -4.2257504 -4.2072759 -4.1659594 -4.1196671 -4.0838442 -4.0917835 -4.1248879 -4.1628194 -4.19292 -4.2033267 -4.1999512 -4.1954188 -4.1976223 -4.2307272][-4.2211761 -4.2092128 -4.1892 -4.1458249 -4.0874157 -4.0380025 -4.024591 -4.0532436 -4.1137252 -4.1568413 -4.1742868 -4.1800246 -4.1829047 -4.1955066 -4.23466][-4.2009945 -4.1885581 -4.1643753 -4.1171589 -4.0440793 -3.9655035 -3.9018443 -3.9063725 -4.0046535 -4.0829239 -4.12379 -4.1464992 -4.1620784 -4.1904459 -4.2359419][-4.1817536 -4.1681137 -4.1433668 -4.0955281 -4.0118141 -3.8971963 -3.7646523 -3.7233391 -3.8636189 -3.9841521 -4.0549321 -4.0993943 -4.132987 -4.1789613 -4.2328606][-4.1750374 -4.1649966 -4.155426 -4.1271987 -4.0590973 -3.9512033 -3.8213649 -3.771987 -3.8964076 -4.0103884 -4.0808611 -4.1310692 -4.1680093 -4.2109365 -4.2546825][-4.1926503 -4.1899076 -4.1980991 -4.1942229 -4.1572824 -4.087656 -4.0119863 -3.9912868 -4.0626235 -4.1343188 -4.1813593 -4.2206149 -4.2476478 -4.2777476 -4.3036127][-4.2134147 -4.2122569 -4.2261448 -4.234179 -4.2176924 -4.1699886 -4.1256704 -4.1225424 -4.1633453 -4.20838 -4.2409019 -4.2696743 -4.2911487 -4.3146005 -4.3323979][-4.2257562 -4.2202187 -4.2312922 -4.2436819 -4.2389283 -4.20517 -4.1732359 -4.1728907 -4.19755 -4.2282376 -4.2528844 -4.2760873 -4.2948112 -4.3161259 -4.3318558][-4.2262273 -4.2130756 -4.2193685 -4.2322464 -4.23679 -4.2171454 -4.1986213 -4.2019329 -4.2188416 -4.2428155 -4.2623625 -4.2785621 -4.2904606 -4.3048644 -4.3165131][-4.2123432 -4.1912975 -4.1932373 -4.2090645 -4.2212977 -4.2159953 -4.2076807 -4.2142396 -4.2276764 -4.2488656 -4.2655959 -4.2764125 -4.2850437 -4.295053 -4.3047714][-4.2096486 -4.1852984 -4.1811519 -4.1950049 -4.2104645 -4.2123075 -4.2071838 -4.2137313 -4.2280073 -4.2473989 -4.2630115 -4.2752757 -4.286603 -4.2963409 -4.3057923][-4.236486 -4.2163892 -4.2100215 -4.2195358 -4.2311411 -4.233779 -4.2312508 -4.2397494 -4.2545481 -4.2693257 -4.2812819 -4.293438 -4.3043694 -4.3109531 -4.3154874]]...]
INFO - root - 2017-12-06 00:50:54.782503: step 55410, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 69h:25m:57s remains)
INFO - root - 2017-12-06 00:51:03.930801: step 55420, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 71h:30m:12s remains)
INFO - root - 2017-12-06 00:51:13.052979: step 55430, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 71h:01m:51s remains)
INFO - root - 2017-12-06 00:51:22.049106: step 55440, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 67h:02m:55s remains)
INFO - root - 2017-12-06 00:51:31.057791: step 55450, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.911 sec/batch; 70h:06m:49s remains)
INFO - root - 2017-12-06 00:51:40.143189: step 55460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 68h:41m:13s remains)
INFO - root - 2017-12-06 00:51:49.029097: step 55470, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 68h:35m:24s remains)
INFO - root - 2017-12-06 00:51:58.241890: step 55480, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 71h:35m:53s remains)
INFO - root - 2017-12-06 00:52:07.317556: step 55490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 71h:06m:00s remains)
INFO - root - 2017-12-06 00:52:16.441300: step 55500, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.877 sec/batch; 67h:30m:13s remains)
2017-12-06 00:52:17.201668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1194267 -4.1349053 -4.163661 -4.1922469 -4.1921015 -4.15485 -4.1055365 -4.1031122 -4.1328139 -4.181756 -4.2335825 -4.2829056 -4.3192563 -4.335722 -4.3380551][-4.1453204 -4.1703849 -4.1969428 -4.2154174 -4.207211 -4.1753688 -4.1296344 -4.1121211 -4.1195807 -4.1562896 -4.213222 -4.2745795 -4.3186393 -4.3377714 -4.3399916][-4.1794224 -4.20694 -4.2266822 -4.2375689 -4.2260585 -4.1966639 -4.1475992 -4.1102543 -4.094553 -4.1249018 -4.1936073 -4.2641873 -4.3121529 -4.333663 -4.3343835][-4.2144823 -4.2384944 -4.249248 -4.254035 -4.2452893 -4.2178607 -4.167459 -4.1174088 -4.0878544 -4.1177163 -4.1910529 -4.2620087 -4.3100657 -4.3322625 -4.3313365][-4.246839 -4.2629733 -4.2647686 -4.2641621 -4.2563396 -4.2306309 -4.1776752 -4.1133256 -4.079782 -4.1195388 -4.1971545 -4.2700605 -4.3155484 -4.3342438 -4.331286][-4.2676921 -4.2763443 -4.2722697 -4.2676091 -4.2566938 -4.2265291 -4.1602612 -4.0720983 -4.0404315 -4.1084638 -4.2054739 -4.28499 -4.3271508 -4.3407316 -4.3350291][-4.2686863 -4.273993 -4.2701125 -4.2645512 -4.2462921 -4.2003279 -4.1070871 -3.9893579 -3.9716828 -4.0868454 -4.2078791 -4.2907319 -4.32791 -4.3387842 -4.3320975][-4.2506032 -4.2507973 -4.2487841 -4.2460389 -4.2262278 -4.1647143 -4.0419793 -3.9042468 -3.9189949 -4.0719614 -4.2046304 -4.283638 -4.3157148 -4.3263068 -4.3206668][-4.2283335 -4.2274189 -4.2298269 -4.2353239 -4.2197809 -4.1559949 -4.0341244 -3.9232121 -3.969063 -4.1100883 -4.2212925 -4.283927 -4.309051 -4.3174872 -4.3126774][-4.2360544 -4.2367024 -4.2438188 -4.2512269 -4.2387986 -4.1844335 -4.0884 -4.0210648 -4.0749283 -4.1804461 -4.2575955 -4.3005872 -4.3173594 -4.3211207 -4.3138938][-4.2614045 -4.2585063 -4.2622333 -4.2646036 -4.2536216 -4.2119431 -4.1453032 -4.1137705 -4.1647835 -4.2416043 -4.2928138 -4.3184929 -4.3263769 -4.3234606 -4.3138533][-4.27242 -4.2693496 -4.2703457 -4.2685933 -4.2580814 -4.22908 -4.18946 -4.1827192 -4.2286539 -4.28363 -4.3141685 -4.3248053 -4.3256054 -4.3200107 -4.3106189][-4.2653217 -4.2678065 -4.271349 -4.2675252 -4.2579989 -4.2406378 -4.221693 -4.2285457 -4.2655735 -4.3015165 -4.3157148 -4.3181481 -4.3173914 -4.311564 -4.3011274][-4.2584887 -4.263011 -4.2669306 -4.2649426 -4.2573967 -4.2495 -4.2439456 -4.254456 -4.2797661 -4.3008847 -4.3066463 -4.3067827 -4.3077226 -4.3031592 -4.2919426][-4.269062 -4.271503 -4.2726583 -4.2717896 -4.2666612 -4.2639208 -4.2635293 -4.2715435 -4.2869015 -4.2972274 -4.2992673 -4.3022356 -4.3064661 -4.3042026 -4.2940731]]...]
INFO - root - 2017-12-06 00:52:26.247024: step 55510, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 68h:55m:47s remains)
INFO - root - 2017-12-06 00:52:35.392212: step 55520, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 68h:57m:07s remains)
INFO - root - 2017-12-06 00:52:44.272168: step 55530, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 70h:11m:10s remains)
INFO - root - 2017-12-06 00:52:53.360987: step 55540, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 70h:27m:42s remains)
INFO - root - 2017-12-06 00:53:02.383396: step 55550, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 68h:33m:39s remains)
INFO - root - 2017-12-06 00:53:11.424028: step 55560, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 68h:19m:13s remains)
INFO - root - 2017-12-06 00:53:20.486831: step 55570, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 70h:19m:22s remains)
INFO - root - 2017-12-06 00:53:29.475645: step 55580, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 67h:44m:40s remains)
INFO - root - 2017-12-06 00:53:38.529514: step 55590, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 70h:16m:54s remains)
INFO - root - 2017-12-06 00:53:47.610401: step 55600, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.901 sec/batch; 69h:16m:14s remains)
2017-12-06 00:53:48.407156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1249661 -4.1344829 -4.151866 -4.1619949 -4.1684294 -4.1713018 -4.1678786 -4.16505 -4.1752267 -4.1811714 -4.176393 -4.1775236 -4.1900158 -4.2160854 -4.2469583][-4.0523643 -4.0666213 -4.0961294 -4.1145964 -4.122623 -4.122508 -4.1201124 -4.1272845 -4.147099 -4.164392 -4.1671228 -4.1706219 -4.18607 -4.2136707 -4.246624][-3.9931669 -4.013515 -4.0515919 -4.0808759 -4.0914555 -4.0810151 -4.0728574 -4.0888081 -4.1199503 -4.1518598 -4.1663752 -4.1755686 -4.1928992 -4.2211771 -4.2538695][-3.953444 -3.9770892 -4.018352 -4.0546408 -4.0627995 -4.0412483 -4.0224872 -4.0414085 -4.0849295 -4.1342592 -4.16279 -4.1799078 -4.1999426 -4.2280855 -4.2600226][-3.9479122 -3.9662457 -4.00145 -4.03487 -4.0376925 -4.0091577 -3.9807403 -3.9954247 -4.0464759 -4.107142 -4.1466103 -4.1705489 -4.1939106 -4.224484 -4.2583275][-3.9743004 -3.9786139 -3.9949577 -4.014761 -4.0114751 -3.9872642 -3.9585559 -3.9600782 -4.0034976 -4.0693474 -4.1200762 -4.1532454 -4.1820488 -4.2170258 -4.2538366][-4.0084286 -4.0019603 -3.992429 -3.9889848 -3.9810491 -3.9624302 -3.9358497 -3.9233704 -3.9569795 -4.0285077 -4.0897341 -4.1339684 -4.16967 -4.2113953 -4.2508378][-4.0365391 -4.0246019 -3.9939442 -3.9676726 -3.960433 -3.9527454 -3.9244239 -3.8982875 -3.9210782 -3.9919398 -4.0599484 -4.1132746 -4.1588049 -4.2054782 -4.2479897][-4.0605707 -4.0452418 -4.0027642 -3.9667027 -3.9723258 -3.9849427 -3.9631217 -3.9344749 -3.9443693 -3.9999115 -4.0576811 -4.1091866 -4.1572003 -4.2041521 -4.244729][-4.0824652 -4.0719008 -4.0321908 -3.9976745 -4.0125737 -4.0422344 -4.039 -4.0263109 -4.0312939 -4.0627909 -4.0971069 -4.1339488 -4.1719108 -4.2077742 -4.2409754][-4.1025572 -4.1004062 -4.0701189 -4.0452294 -4.0604768 -4.0924115 -4.1040864 -4.106647 -4.1064672 -4.1192279 -4.1388216 -4.1625109 -4.1902542 -4.2157454 -4.2408638][-4.1279931 -4.1343431 -4.1177173 -4.1016722 -4.1113224 -4.1335254 -4.1469369 -4.1521597 -4.15098 -4.1579003 -4.1672964 -4.1766596 -4.196166 -4.2192636 -4.2423229][-4.1535149 -4.163044 -4.1569891 -4.1505761 -4.15844 -4.1711578 -4.1805358 -4.1831894 -4.1826782 -4.1873403 -4.1902456 -4.1851826 -4.1966915 -4.2193561 -4.2449956][-4.1858253 -4.1910496 -4.1897864 -4.1891618 -4.1944366 -4.2004933 -4.2094154 -4.2125812 -4.2135792 -4.2152781 -4.2147713 -4.2052236 -4.2093573 -4.2301636 -4.2560549][-4.2160649 -4.21738 -4.2173343 -4.2173166 -4.2186904 -4.2202363 -4.2283921 -4.2350283 -4.2398977 -4.2431355 -4.2455807 -4.2396226 -4.2402773 -4.2560878 -4.2768235]]...]
INFO - root - 2017-12-06 00:53:57.444172: step 55610, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 69h:20m:52s remains)
INFO - root - 2017-12-06 00:54:06.510000: step 55620, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.897 sec/batch; 68h:59m:15s remains)
INFO - root - 2017-12-06 00:54:15.627814: step 55630, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 68h:45m:39s remains)
INFO - root - 2017-12-06 00:54:24.562714: step 55640, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 66h:47m:19s remains)
INFO - root - 2017-12-06 00:54:33.548242: step 55650, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 71h:42m:38s remains)
INFO - root - 2017-12-06 00:54:42.554040: step 55660, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 69h:15m:27s remains)
INFO - root - 2017-12-06 00:54:51.588477: step 55670, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.890 sec/batch; 68h:26m:40s remains)
INFO - root - 2017-12-06 00:55:00.689996: step 55680, loss = 2.04, batch loss = 1.98 (8.4 examples/sec; 0.956 sec/batch; 73h:31m:46s remains)
INFO - root - 2017-12-06 00:55:09.801061: step 55690, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 67h:27m:12s remains)
INFO - root - 2017-12-06 00:55:18.820206: step 55700, loss = 2.02, batch loss = 1.97 (8.9 examples/sec; 0.899 sec/batch; 69h:05m:10s remains)
2017-12-06 00:55:19.630690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1821218 -4.1847348 -4.1934695 -4.2089462 -4.2204742 -4.2380838 -4.2558966 -4.2656355 -4.259264 -4.2419858 -4.2130418 -4.1793027 -4.1601281 -4.177557 -4.2215934][-4.1986442 -4.2032342 -4.2061896 -4.2080345 -4.2063603 -4.2174363 -4.2336197 -4.244957 -4.2410784 -4.2257195 -4.1983752 -4.1652246 -4.1504784 -4.173182 -4.2162676][-4.1955166 -4.2014151 -4.2013993 -4.1984067 -4.1927562 -4.2011681 -4.215836 -4.2251959 -4.2190871 -4.2035761 -4.1768146 -4.1429543 -4.1325774 -4.1595449 -4.2000065][-4.1833725 -4.1919503 -4.1913848 -4.189189 -4.1837544 -4.1874833 -4.199913 -4.2065797 -4.1998177 -4.1855907 -4.1631889 -4.1354356 -4.1312246 -4.159832 -4.1948481][-4.1714211 -4.1843324 -4.186687 -4.18538 -4.1745229 -4.1684685 -4.1730042 -4.1779552 -4.1754613 -4.1713643 -4.1586738 -4.1368756 -4.1358161 -4.1616087 -4.1906848][-4.1533995 -4.1705885 -4.1743064 -4.1680679 -4.1491814 -4.1355295 -4.1358657 -4.1449766 -4.1529932 -4.1630449 -4.1605387 -4.1432853 -4.1434479 -4.1663041 -4.191937][-4.1383066 -4.1493006 -4.1461954 -4.1267366 -4.0915256 -4.0615044 -4.0554781 -4.0789876 -4.108882 -4.142406 -4.1570992 -4.1520052 -4.1609049 -4.1875257 -4.2107396][-4.1312795 -4.1334891 -4.1224012 -4.0904388 -4.0391278 -3.9855816 -3.963964 -3.9952693 -4.0405316 -4.092412 -4.1240973 -4.1328697 -4.1528187 -4.1869564 -4.2194958][-4.1405859 -4.1437144 -4.1393437 -4.1147485 -4.0713592 -4.0214419 -3.9904044 -4.0008483 -4.0300407 -4.0761867 -4.1082869 -4.1215258 -4.1480784 -4.1851377 -4.2225013][-4.1739745 -4.1795893 -4.1824636 -4.1692581 -4.1431775 -4.1122732 -4.0903955 -4.0881691 -4.0994573 -4.1296344 -4.1519613 -4.1574221 -4.1739345 -4.2023592 -4.2342439][-4.200017 -4.2075071 -4.2140961 -4.2069063 -4.1913838 -4.173172 -4.1596966 -4.1551142 -4.1601486 -4.1816077 -4.1965852 -4.1950941 -4.2000213 -4.219224 -4.2458692][-4.2119918 -4.2224874 -4.2294059 -4.226099 -4.2178612 -4.2068667 -4.1928205 -4.1799293 -4.1795969 -4.1955528 -4.2047153 -4.1977234 -4.1993828 -4.22036 -4.2490439][-4.1977277 -4.2067561 -4.2123623 -4.2114964 -4.2060218 -4.2006512 -4.191011 -4.1762977 -4.172914 -4.1801124 -4.1784711 -4.1567421 -4.1541224 -4.1835661 -4.2245922][-4.1613455 -4.1670074 -4.1690445 -4.1678176 -4.1612897 -4.1549568 -4.1475072 -4.1363554 -4.1355543 -4.1404777 -4.1318817 -4.09881 -4.092844 -4.1317134 -4.1895971][-4.1393919 -4.145494 -4.1486564 -4.1491666 -4.1415744 -4.1344934 -4.1260166 -4.1138234 -4.1102667 -4.1128874 -4.1011281 -4.0662231 -4.0597267 -4.0970678 -4.1599874]]...]
INFO - root - 2017-12-06 00:55:28.674614: step 55710, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 69h:26m:14s remains)
INFO - root - 2017-12-06 00:55:37.356461: step 55720, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 71h:59m:19s remains)
INFO - root - 2017-12-06 00:55:46.517657: step 55730, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 70h:33m:51s remains)
INFO - root - 2017-12-06 00:55:55.424870: step 55740, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 68h:21m:52s remains)
INFO - root - 2017-12-06 00:56:04.346527: step 55750, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 69h:26m:39s remains)
INFO - root - 2017-12-06 00:56:13.524042: step 55760, loss = 2.08, batch loss = 2.02 (8.3 examples/sec; 0.961 sec/batch; 73h:53m:57s remains)
INFO - root - 2017-12-06 00:56:22.763218: step 55770, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 70h:06m:27s remains)
INFO - root - 2017-12-06 00:56:31.822675: step 55780, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 68h:37m:20s remains)
INFO - root - 2017-12-06 00:56:40.892133: step 55790, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 70h:41m:15s remains)
INFO - root - 2017-12-06 00:56:50.102180: step 55800, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.936 sec/batch; 71h:54m:33s remains)
2017-12-06 00:56:50.868195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1983633 -4.2000413 -4.1988831 -4.1996007 -4.20753 -4.2188573 -4.2267084 -4.2264147 -4.222259 -4.2202539 -4.2208562 -4.2226286 -4.2247972 -4.2279878 -4.2323995][-4.1840587 -4.1845427 -4.1835127 -4.1875963 -4.1994715 -4.2120266 -4.22068 -4.2219973 -4.2186995 -4.2161503 -4.2149215 -4.2121968 -4.2082944 -4.2062197 -4.2057171][-4.1918955 -4.1960993 -4.1979017 -4.2060947 -4.2192497 -4.2284861 -4.2334805 -4.2337689 -4.2307315 -4.2282295 -4.226542 -4.2214279 -4.2105851 -4.1982479 -4.18524][-4.2047319 -4.2171979 -4.2248626 -4.2354784 -4.2451763 -4.24956 -4.2509613 -4.251009 -4.249856 -4.2508903 -4.2541471 -4.25426 -4.2428961 -4.2246475 -4.2032533][-4.1957021 -4.2210422 -4.2364497 -4.245995 -4.2458067 -4.2409687 -4.235188 -4.2304955 -4.2307115 -4.2402577 -4.2567935 -4.2696691 -4.265131 -4.2501912 -4.2336564][-4.169652 -4.2039542 -4.2219024 -4.2257447 -4.212038 -4.1938291 -4.1767678 -4.1588469 -4.15703 -4.1780052 -4.2096605 -4.2374291 -4.2447925 -4.240416 -4.2397203][-4.1316886 -4.1604776 -4.1689415 -4.1567636 -4.1246471 -4.0925989 -4.0611291 -4.0222917 -4.0181046 -4.0583305 -4.1099796 -4.1540504 -4.173357 -4.1830368 -4.2011609][-4.1092606 -4.1261082 -4.1214032 -4.0920105 -4.0494666 -4.0156479 -3.9754109 -3.9178145 -3.9078155 -3.9563093 -4.0119481 -4.0587249 -4.0821114 -4.1011281 -4.1334066][-4.1524634 -4.1615305 -4.1511264 -4.1202569 -4.0870786 -4.0654721 -4.0385981 -3.9932029 -3.9801846 -4.0089779 -4.0382533 -4.0673981 -4.0824976 -4.0984788 -4.1264305][-4.2369695 -4.2418962 -4.2316194 -4.206604 -4.1845512 -4.1717634 -4.1561837 -4.1288176 -4.1186318 -4.12994 -4.1387787 -4.1524887 -4.1609888 -4.172112 -4.1910667][-4.3001137 -4.3028007 -4.2940707 -4.2771449 -4.2634225 -4.2551265 -4.2458959 -4.2312131 -4.2254891 -4.2297363 -4.2330232 -4.2417107 -4.2492423 -4.2568617 -4.2669754][-4.3238072 -4.3237696 -4.317697 -4.3079662 -4.3007 -4.2958016 -4.2900009 -4.2828755 -4.2805204 -4.28266 -4.2856989 -4.2907462 -4.293623 -4.2922564 -4.292623][-4.3243241 -4.3199611 -4.3162003 -4.3127832 -4.3100562 -4.3054581 -4.2988186 -4.2927823 -4.2917161 -4.2938104 -4.2957063 -4.2930579 -4.2871337 -4.2777615 -4.2730517][-4.3173962 -4.3114352 -4.3095751 -4.3094749 -4.3080044 -4.3015962 -4.289885 -4.2797217 -4.2779226 -4.2835073 -4.2856073 -4.2749476 -4.2599554 -4.243278 -4.2350469][-4.308919 -4.301342 -4.2975817 -4.2960286 -4.2914934 -4.2783837 -4.2582965 -4.2438583 -4.2461481 -4.2595911 -4.2644649 -4.2478886 -4.2244315 -4.2009473 -4.1879959]]...]
INFO - root - 2017-12-06 00:57:00.114128: step 55810, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 69h:56m:51s remains)
INFO - root - 2017-12-06 00:57:09.274853: step 55820, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 71h:29m:56s remains)
INFO - root - 2017-12-06 00:57:18.409941: step 55830, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.916 sec/batch; 70h:23m:23s remains)
INFO - root - 2017-12-06 00:57:27.394552: step 55840, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 71h:55m:44s remains)
INFO - root - 2017-12-06 00:57:36.415770: step 55850, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 70h:16m:48s remains)
INFO - root - 2017-12-06 00:57:45.468403: step 55860, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 66h:23m:03s remains)
INFO - root - 2017-12-06 00:57:54.615960: step 55870, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 70h:42m:23s remains)
INFO - root - 2017-12-06 00:58:03.928620: step 55880, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.938 sec/batch; 72h:03m:08s remains)
INFO - root - 2017-12-06 00:58:12.921600: step 55890, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 68h:20m:17s remains)
INFO - root - 2017-12-06 00:58:22.065272: step 55900, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 70h:51m:52s remains)
2017-12-06 00:58:22.873341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2656465 -4.2448649 -4.2212138 -4.2099843 -4.2026463 -4.2105951 -4.2277603 -4.2438116 -4.2589 -4.2581463 -4.2452216 -4.225049 -4.2085953 -4.2016048 -4.2155848][-4.2542844 -4.2290568 -4.1978245 -4.1817493 -4.1716876 -4.1770964 -4.1888003 -4.2005515 -4.2136188 -4.2099652 -4.20026 -4.1797967 -4.1613674 -4.1513629 -4.1674447][-4.2399268 -4.2070522 -4.1656175 -4.1408997 -4.1284752 -4.1337056 -4.1424484 -4.1537433 -4.1668444 -4.1613164 -4.1604424 -4.1499858 -4.1326046 -4.1215515 -4.1396089][-4.22826 -4.186728 -4.1359963 -4.0992 -4.079401 -4.0792809 -4.0791388 -4.0900493 -4.1113052 -4.1154623 -4.1299229 -4.141057 -4.1344595 -4.1256227 -4.1432786][-4.2202744 -4.1726027 -4.1160965 -4.0698977 -4.0373454 -4.0238671 -4.0026379 -4.0033574 -4.0397058 -4.0647268 -4.0990992 -4.1340404 -4.1389856 -4.1353765 -4.1526732][-4.2041488 -4.1514006 -4.09039 -4.0316734 -3.9766955 -3.9384916 -3.879292 -3.855576 -3.9180074 -3.9824092 -4.0461588 -4.1047759 -4.1257677 -4.1342583 -4.1551442][-4.1910353 -4.1368628 -4.0722489 -4.0003481 -3.9237165 -3.8577394 -3.7597408 -3.7013958 -3.7871504 -3.9078662 -4.0026269 -4.0791287 -4.1097784 -4.1266627 -4.1485367][-4.1937165 -4.1414504 -4.0754309 -4.0022264 -3.9266813 -3.8575835 -3.7573774 -3.6884527 -3.7692082 -3.9054601 -3.9987297 -4.0682621 -4.0899863 -4.0991812 -4.1170597][-4.2054062 -4.1604733 -4.1006274 -4.0403404 -3.982686 -3.9321475 -3.8671792 -3.8253613 -3.8878577 -3.9909682 -4.0555868 -4.093523 -4.0941124 -4.09361 -4.1051927][-4.2165637 -4.1759934 -4.1266851 -4.0880556 -4.0527477 -4.02025 -3.98842 -3.9752045 -4.0245709 -4.0935321 -4.1298714 -4.1406479 -4.1282048 -4.1254792 -4.1353006][-4.2231126 -4.1857443 -4.1470237 -4.1233091 -4.1016459 -4.0832548 -4.0738273 -4.0749645 -4.110229 -4.1522093 -4.1709633 -4.1701112 -4.1560984 -4.1528406 -4.1638913][-4.2365294 -4.2046223 -4.1707606 -4.1506481 -4.1380749 -4.1362557 -4.1398335 -4.1462159 -4.1705503 -4.1939435 -4.20022 -4.1947985 -4.1818142 -4.1786256 -4.1906915][-4.2591472 -4.2293367 -4.1949205 -4.1719661 -4.1641731 -4.1753397 -4.1906142 -4.1988497 -4.2135115 -4.2270069 -4.2299376 -4.2258596 -4.2175832 -4.2141633 -4.2225542][-4.2823329 -4.2547169 -4.2226706 -4.2021294 -4.1965632 -4.20781 -4.2224684 -4.2275147 -4.2319818 -4.2386518 -4.24263 -4.2420096 -4.2407146 -4.2433319 -4.2552629][-4.30102 -4.2780423 -4.2550297 -4.2396841 -4.2338905 -4.2411251 -4.25002 -4.2506056 -4.2481012 -4.2482224 -4.2498007 -4.2511458 -4.2541175 -4.2621207 -4.2770133]]...]
INFO - root - 2017-12-06 00:58:31.840672: step 55910, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.956 sec/batch; 73h:25m:34s remains)
INFO - root - 2017-12-06 00:58:41.032502: step 55920, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 70h:50m:23s remains)
INFO - root - 2017-12-06 00:58:50.108953: step 55930, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 69h:20m:24s remains)
INFO - root - 2017-12-06 00:58:59.019551: step 55940, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 69h:15m:27s remains)
INFO - root - 2017-12-06 00:59:08.106968: step 55950, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 67h:45m:13s remains)
INFO - root - 2017-12-06 00:59:17.262956: step 55960, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.936 sec/batch; 71h:54m:58s remains)
INFO - root - 2017-12-06 00:59:26.463475: step 55970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 69h:52m:56s remains)
INFO - root - 2017-12-06 00:59:35.611579: step 55980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 65h:44m:30s remains)
INFO - root - 2017-12-06 00:59:44.634146: step 55990, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 69h:35m:15s remains)
INFO - root - 2017-12-06 00:59:53.706108: step 56000, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 66h:36m:55s remains)
2017-12-06 00:59:54.536337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.187326 -4.1973557 -4.2049294 -4.2115436 -4.2176623 -4.228241 -4.2396984 -4.24704 -4.2432933 -4.2404532 -4.2487693 -4.269136 -4.2845178 -4.2840109 -4.2750087][-4.2006054 -4.2083297 -4.2113485 -4.2165031 -4.2216573 -4.2324305 -4.2455292 -4.2565022 -4.25944 -4.2594523 -4.2637358 -4.2763333 -4.2842941 -4.2781425 -4.2648039][-4.1980457 -4.2069106 -4.2101684 -4.2111983 -4.21211 -4.2213335 -4.23677 -4.25604 -4.27097 -4.280201 -4.2820888 -4.2858543 -4.2868209 -4.2785015 -4.2629013][-4.1791267 -4.192759 -4.1976604 -4.1879158 -4.1778913 -4.1851134 -4.211277 -4.2440515 -4.2718024 -4.2909722 -4.29505 -4.2947922 -4.2899971 -4.2811546 -4.2653484][-4.1617746 -4.1745362 -4.1721721 -4.1460824 -4.1231241 -4.1297731 -4.1693082 -4.2178984 -4.2586203 -4.2874579 -4.2965245 -4.2965951 -4.2903433 -4.28177 -4.2653966][-4.147469 -4.1488328 -4.1324873 -4.0957451 -4.0682597 -4.0755925 -4.1260843 -4.1879454 -4.2406235 -4.2751937 -4.2865825 -4.2869282 -4.2824407 -4.277657 -4.2612071][-4.1463752 -4.1434965 -4.1180434 -4.0763421 -4.0457816 -4.0536766 -4.1086679 -4.1726556 -4.225162 -4.2590294 -4.2673817 -4.264523 -4.2619538 -4.2618284 -4.247623][-4.1655989 -4.1682239 -4.1422858 -4.1005545 -4.0711479 -4.0811577 -4.13207 -4.1858773 -4.2278132 -4.2549124 -4.2570581 -4.2509079 -4.2486072 -4.2489061 -4.234633][-4.1839809 -4.194315 -4.1756067 -4.1418586 -4.1161232 -4.1239347 -4.1648498 -4.2037625 -4.2310576 -4.246202 -4.2438054 -4.2356691 -4.2375555 -4.2455754 -4.2349977][-4.1944232 -4.2099562 -4.19919 -4.1726623 -4.14785 -4.1495194 -4.1805344 -4.2085838 -4.2220559 -4.2262006 -4.2220945 -4.217741 -4.225316 -4.2417655 -4.2415986][-4.202796 -4.2208 -4.215313 -4.1889739 -4.1580992 -4.1509857 -4.1767964 -4.2013392 -4.2085509 -4.2105708 -4.2108016 -4.2128019 -4.2244663 -4.2432995 -4.2498112][-4.1936674 -4.2139468 -4.2136397 -4.1877289 -4.1539497 -4.1426296 -4.1621237 -4.1829948 -4.1913509 -4.1969366 -4.203372 -4.2108 -4.2246642 -4.2427683 -4.252223][-4.1737151 -4.1975393 -4.2073617 -4.190269 -4.163413 -4.1523671 -4.1651459 -4.1814356 -4.1892319 -4.1970315 -4.2044296 -4.2129927 -4.2272849 -4.2445354 -4.251111][-4.1772184 -4.2023988 -4.2173944 -4.2092772 -4.1904316 -4.1812892 -4.1899662 -4.202734 -4.2091246 -4.2164817 -4.2256503 -4.2350979 -4.2472906 -4.2611809 -4.2625685][-4.2250838 -4.24427 -4.2557545 -4.2499232 -4.2350039 -4.2264733 -4.229681 -4.2391391 -4.2471342 -4.2561846 -4.266726 -4.2766314 -4.2864447 -4.2956533 -4.29185]]...]
INFO - root - 2017-12-06 01:00:03.729394: step 56010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.921 sec/batch; 70h:45m:07s remains)
INFO - root - 2017-12-06 01:00:13.008654: step 56020, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 70h:49m:55s remains)
INFO - root - 2017-12-06 01:00:22.137354: step 56030, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.901 sec/batch; 69h:12m:31s remains)
INFO - root - 2017-12-06 01:00:31.118383: step 56040, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:33m:12s remains)
INFO - root - 2017-12-06 01:00:40.154239: step 56050, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.939 sec/batch; 72h:04m:23s remains)
INFO - root - 2017-12-06 01:00:49.409721: step 56060, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.956 sec/batch; 73h:26m:49s remains)
INFO - root - 2017-12-06 01:00:58.502109: step 56070, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.922 sec/batch; 70h:49m:39s remains)
INFO - root - 2017-12-06 01:01:07.639724: step 56080, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 68h:48m:25s remains)
INFO - root - 2017-12-06 01:01:16.773626: step 56090, loss = 2.09, batch loss = 2.03 (8.3 examples/sec; 0.963 sec/batch; 73h:55m:26s remains)
INFO - root - 2017-12-06 01:01:25.856308: step 56100, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 68h:51m:22s remains)
2017-12-06 01:01:26.670777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1921911 -4.1845317 -4.1895456 -4.2087989 -4.224565 -4.2213955 -4.2037315 -4.1917329 -4.1944113 -4.2005124 -4.2026343 -4.1984744 -4.2028885 -4.214529 -4.2245483][-4.2134948 -4.2160711 -4.2230263 -4.2364864 -4.24602 -4.2415562 -4.2274742 -4.2154894 -4.2192607 -4.2316341 -4.2411013 -4.243444 -4.2518563 -4.2629194 -4.2727237][-4.2442684 -4.2434058 -4.2419491 -4.2423859 -4.24194 -4.238255 -4.2284231 -4.2170424 -4.22313 -4.2435145 -4.2643061 -4.2770834 -4.2913628 -4.3042793 -4.3129177][-4.2786083 -4.267869 -4.254631 -4.240705 -4.2306428 -4.2201991 -4.2024832 -4.1813498 -4.1859059 -4.2135177 -4.2464628 -4.2710929 -4.2941346 -4.3106441 -4.3190308][-4.2978959 -4.2759104 -4.247735 -4.2193713 -4.1971879 -4.1751437 -4.1407681 -4.0960298 -4.0902057 -4.1282406 -4.1763506 -4.2142925 -4.2476439 -4.2701063 -4.2807794][-4.2920146 -4.2632713 -4.2222533 -4.1785913 -4.1410322 -4.1024714 -4.0430155 -3.965374 -3.9414217 -3.9922745 -4.0634661 -4.123313 -4.1764216 -4.2134018 -4.2344537][-4.2625413 -4.2358136 -4.1953626 -4.1458306 -4.0959277 -4.0367022 -3.9515562 -3.841599 -3.7943604 -3.8545337 -3.9479837 -4.0348554 -4.1137638 -4.1742015 -4.213522][-4.2299819 -4.2132382 -4.1875567 -4.1508932 -4.1081438 -4.0525756 -3.9738569 -3.8664944 -3.8031685 -3.845685 -3.9327781 -4.0274992 -4.1179991 -4.1849155 -4.2280412][-4.2321033 -4.22551 -4.2147388 -4.1935883 -4.1684294 -4.1313257 -4.0762739 -3.9974918 -3.9380066 -3.9546635 -4.0181518 -4.0997157 -4.1786404 -4.2327609 -4.2662239][-4.2669916 -4.263267 -4.255661 -4.2438126 -4.2289896 -4.2058816 -4.1706109 -4.1174707 -4.07394 -4.0755248 -4.1160679 -4.1772642 -4.2340164 -4.2733574 -4.298759][-4.2969112 -4.2919235 -4.2832541 -4.2751279 -4.2651572 -4.2495017 -4.2277436 -4.19415 -4.1641212 -4.1579971 -4.1858945 -4.2322636 -4.2721076 -4.3015947 -4.3201885][-4.3014145 -4.2931743 -4.2834287 -4.276937 -4.2712612 -4.2623191 -4.2527404 -4.2333632 -4.2115746 -4.2014012 -4.219202 -4.2530422 -4.284194 -4.3088589 -4.3230138][-4.28033 -4.2682214 -4.2587819 -4.2547026 -4.2515578 -4.2468319 -4.2435708 -4.2353058 -4.2229457 -4.2158728 -4.2321639 -4.2639441 -4.290719 -4.3085055 -4.3166456][-4.2535524 -4.2407765 -4.2312541 -4.2264547 -4.2247086 -4.2240796 -4.2254057 -4.2241521 -4.2222528 -4.2232957 -4.2403936 -4.2705255 -4.2957854 -4.30802 -4.3113565][-4.2387867 -4.2278442 -4.220192 -4.2165117 -4.217813 -4.2210355 -4.2247915 -4.2269912 -4.2298412 -4.2340159 -4.2478809 -4.2721062 -4.2927895 -4.3010378 -4.3025389]]...]
INFO - root - 2017-12-06 01:01:35.806873: step 56110, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 68h:07m:28s remains)
INFO - root - 2017-12-06 01:01:44.966548: step 56120, loss = 2.05, batch loss = 2.00 (10.0 examples/sec; 0.798 sec/batch; 61h:16m:31s remains)
INFO - root - 2017-12-06 01:01:54.253365: step 56130, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 69h:39m:53s remains)
INFO - root - 2017-12-06 01:02:03.313867: step 56140, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.921 sec/batch; 70h:40m:22s remains)
INFO - root - 2017-12-06 01:02:12.368615: step 56150, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 68h:17m:17s remains)
INFO - root - 2017-12-06 01:02:21.446367: step 56160, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 69h:16m:57s remains)
INFO - root - 2017-12-06 01:02:30.671005: step 56170, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 70h:30m:58s remains)
INFO - root - 2017-12-06 01:02:39.745458: step 56180, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 68h:58m:14s remains)
INFO - root - 2017-12-06 01:02:48.883421: step 56190, loss = 2.08, batch loss = 2.02 (7.9 examples/sec; 1.010 sec/batch; 77h:30m:57s remains)
INFO - root - 2017-12-06 01:02:58.193478: step 56200, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 69h:12m:30s remains)
2017-12-06 01:02:59.004410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626114 -4.260632 -4.2551112 -4.2481518 -4.238059 -4.2323017 -4.2306619 -4.2355433 -4.2425308 -4.2381687 -4.2286882 -4.2241654 -4.229774 -4.2440667 -4.2554083][-4.2598958 -4.2576637 -4.2502718 -4.2426395 -4.2321181 -4.2257514 -4.2215052 -4.2244329 -4.2300696 -4.2220979 -4.2086577 -4.20316 -4.2097073 -4.2277236 -4.2450776][-4.2561779 -4.2554846 -4.2486081 -4.2417889 -4.2299647 -4.2199779 -4.2136712 -4.2149434 -4.2185636 -4.2103353 -4.1959958 -4.1880064 -4.1903319 -4.2055345 -4.2254839][-4.2452941 -4.2463741 -4.2424431 -4.2382646 -4.2259889 -4.212328 -4.2046022 -4.2045059 -4.2084951 -4.207346 -4.1995726 -4.189846 -4.1863303 -4.1948853 -4.2147946][-4.22271 -4.2236004 -4.2233987 -4.2217221 -4.2067766 -4.184721 -4.1677165 -4.1627049 -4.17256 -4.1891952 -4.20156 -4.2026238 -4.1997004 -4.2044005 -4.2202005][-4.2062764 -4.2001395 -4.1961784 -4.1904726 -4.1685939 -4.1292982 -4.0892625 -4.0699606 -4.0920067 -4.1395268 -4.1830244 -4.2033734 -4.2029967 -4.2058029 -4.2163773][-4.2046523 -4.1905546 -4.176013 -4.1597795 -4.1256018 -4.0594335 -3.9811387 -3.9324694 -3.9679518 -4.0507231 -4.1274877 -4.1714311 -4.1765561 -4.1762671 -4.1870012][-4.1998158 -4.1814857 -4.1576 -4.1322179 -4.0876274 -3.9999588 -3.8897021 -3.8140278 -3.8586817 -3.9681673 -4.0684719 -4.129179 -4.1413636 -4.1404924 -4.1550369][-4.2016764 -4.1866226 -4.163763 -4.1425996 -4.1069584 -4.0322723 -3.9394784 -3.878675 -3.9113748 -4.0015597 -4.0869164 -4.13554 -4.1383677 -4.1309867 -4.14237][-4.2238617 -4.2168593 -4.2015653 -4.1894412 -4.1688461 -4.1206722 -4.0643139 -4.02773 -4.0412307 -4.0945253 -4.149827 -4.1752934 -4.1637487 -4.1481323 -4.152739][-4.2411137 -4.2381668 -4.2286277 -4.2242961 -4.216917 -4.1933236 -4.164145 -4.1442943 -4.1474524 -4.1748538 -4.2050734 -4.2156677 -4.2041211 -4.1909647 -4.1937575][-4.2450438 -4.2473688 -4.2472844 -4.2521019 -4.2555571 -4.2485223 -4.2379556 -4.2315922 -4.2329059 -4.2438726 -4.2553382 -4.2570081 -4.2495327 -4.2408214 -4.2412734][-4.2296777 -4.2375512 -4.247118 -4.2587209 -4.2685089 -4.270782 -4.2711725 -4.2730789 -4.2748885 -4.2783122 -4.2806549 -4.2789254 -4.2747984 -4.2698894 -4.2683129][-4.1836715 -4.1977062 -4.2178888 -4.2366624 -4.25192 -4.2596712 -4.26322 -4.2663093 -4.2681 -4.2707281 -4.2757535 -4.2802434 -4.2817907 -4.2806315 -4.2781219][-4.1394076 -4.1565809 -4.184164 -4.2093596 -4.2314854 -4.2456121 -4.2502747 -4.2517672 -4.2520604 -4.2542176 -4.2625613 -4.2730417 -4.2799416 -4.282517 -4.2807622]]...]
INFO - root - 2017-12-06 01:03:07.953846: step 56210, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 65h:07m:40s remains)
INFO - root - 2017-12-06 01:03:16.799766: step 56220, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 68h:35m:29s remains)
INFO - root - 2017-12-06 01:03:25.997946: step 56230, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 68h:47m:11s remains)
INFO - root - 2017-12-06 01:03:35.034486: step 56240, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 68h:40m:41s remains)
INFO - root - 2017-12-06 01:03:43.901614: step 56250, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 68h:07m:02s remains)
INFO - root - 2017-12-06 01:03:53.149529: step 56260, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 69h:40m:14s remains)
INFO - root - 2017-12-06 01:04:02.362471: step 56270, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 68h:28m:44s remains)
INFO - root - 2017-12-06 01:04:11.264158: step 56280, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 69h:30m:27s remains)
INFO - root - 2017-12-06 01:04:20.466275: step 56290, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.945 sec/batch; 72h:30m:43s remains)
INFO - root - 2017-12-06 01:04:29.596864: step 56300, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.914 sec/batch; 70h:07m:59s remains)
2017-12-06 01:04:30.412996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2262907 -4.2728662 -4.2960176 -4.2938228 -4.2813444 -4.2675247 -4.2421908 -4.2186723 -4.2101259 -4.2167912 -4.2415347 -4.2672424 -4.2761464 -4.2753921 -4.2747459][-4.2372341 -4.2824697 -4.3021746 -4.2952108 -4.275074 -4.2459188 -4.2057252 -4.1844339 -4.1961355 -4.2246375 -4.2565422 -4.2782254 -4.2816067 -4.277977 -4.2731767][-4.2491956 -4.2886653 -4.302494 -4.28559 -4.2475357 -4.1961122 -4.1425295 -4.1371632 -4.1843481 -4.2370172 -4.2729611 -4.2889476 -4.28703 -4.2811809 -4.2763095][-4.2535233 -4.2890153 -4.2979522 -4.2667308 -4.2022996 -4.1204596 -4.0562987 -4.0806766 -4.1653638 -4.2384849 -4.2797441 -4.2935166 -4.2887735 -4.2835732 -4.2825022][-4.2558546 -4.2878304 -4.2869358 -4.2368722 -4.1424866 -4.0292006 -3.9646647 -4.033092 -4.153378 -4.237793 -4.283525 -4.296236 -4.2884011 -4.2850065 -4.283216][-4.2607284 -4.2848334 -4.2716002 -4.200345 -4.0767817 -3.9343743 -3.8842828 -4.00549 -4.1552491 -4.2415648 -4.28726 -4.2971134 -4.2908778 -4.2899737 -4.2833261][-4.2703857 -4.2803364 -4.2543759 -4.1679397 -4.0263295 -3.8712473 -3.8482594 -4.0125952 -4.1726551 -4.2511282 -4.2884703 -4.2934542 -4.2920961 -4.2926326 -4.2815928][-4.2751317 -4.2752132 -4.243803 -4.1580858 -4.0288849 -3.8985987 -3.9048042 -4.0685587 -4.2081642 -4.2671385 -4.2889733 -4.2936029 -4.3009195 -4.3020139 -4.2870803][-4.2740641 -4.270885 -4.2420368 -4.1675882 -4.0649452 -3.9743724 -4.0044036 -4.1404819 -4.2447968 -4.2832351 -4.2939 -4.3000841 -4.310329 -4.30862 -4.293036][-4.2687755 -4.2642303 -4.239274 -4.1781268 -4.1042585 -4.0536151 -4.0987644 -4.2038 -4.2748365 -4.2936058 -4.2964821 -4.3036256 -4.3119531 -4.3098488 -4.2929049][-4.2644887 -4.2550488 -4.2306852 -4.1825333 -4.1382456 -4.126092 -4.1798487 -4.2584167 -4.3018103 -4.3065066 -4.3071961 -4.3126197 -4.3153176 -4.3081517 -4.2836733][-4.2635584 -4.2485085 -4.2221184 -4.1884117 -4.1682 -4.181282 -4.2339039 -4.2937031 -4.3186412 -4.3179793 -4.3204532 -4.324544 -4.3233643 -4.3066316 -4.2707291][-4.2567506 -4.2451553 -4.2268357 -4.2094769 -4.2052464 -4.2256885 -4.2679658 -4.3088012 -4.3213568 -4.3196683 -4.3218884 -4.3271117 -4.3266506 -4.3060117 -4.2660117][-4.2474647 -4.2478929 -4.2394128 -4.232336 -4.2357707 -4.2555928 -4.2847772 -4.3055172 -4.3057809 -4.3023419 -4.3067069 -4.3163896 -4.32012 -4.3020935 -4.269268][-4.2409072 -4.250854 -4.2471857 -4.2451758 -4.2536964 -4.2722945 -4.2889466 -4.2940273 -4.2839546 -4.2808557 -4.290586 -4.3016667 -4.3057046 -4.2913847 -4.2664671]]...]
INFO - root - 2017-12-06 01:04:39.518291: step 56310, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.920 sec/batch; 70h:33m:04s remains)
INFO - root - 2017-12-06 01:04:48.611056: step 56320, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 67h:40m:12s remains)
INFO - root - 2017-12-06 01:04:57.722566: step 56330, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 71h:04m:17s remains)
INFO - root - 2017-12-06 01:05:07.080571: step 56340, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 71h:17m:18s remains)
INFO - root - 2017-12-06 01:05:16.047977: step 56350, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 68h:22m:01s remains)
INFO - root - 2017-12-06 01:05:25.247073: step 56360, loss = 2.03, batch loss = 1.98 (8.5 examples/sec; 0.941 sec/batch; 72h:09m:44s remains)
INFO - root - 2017-12-06 01:05:34.375471: step 56370, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 71h:10m:38s remains)
INFO - root - 2017-12-06 01:05:43.468636: step 56380, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 69h:02m:53s remains)
INFO - root - 2017-12-06 01:05:52.596325: step 56390, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 71h:25m:54s remains)
INFO - root - 2017-12-06 01:06:01.683547: step 56400, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 70h:03m:29s remains)
2017-12-06 01:06:02.441270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2424407 -4.2389216 -4.2379203 -4.2419553 -4.2436609 -4.2428145 -4.2432728 -4.2450581 -4.25358 -4.2625289 -4.265306 -4.2664061 -4.2706814 -4.2731619 -4.2750988][-4.2306314 -4.2224388 -4.2181687 -4.2185574 -4.2146654 -4.2106066 -4.2115436 -4.2183166 -4.235395 -4.2538095 -4.2675071 -4.2741828 -4.279211 -4.2773433 -4.2732735][-4.2274432 -4.2163515 -4.2070556 -4.2002673 -4.1842971 -4.1709566 -4.1692457 -4.1806712 -4.2070403 -4.2351494 -4.2620244 -4.2793508 -4.2874451 -4.2835245 -4.27368][-4.2267451 -4.2158194 -4.2046819 -4.1885967 -4.1553974 -4.1232796 -4.1108637 -4.1209245 -4.1589074 -4.1996827 -4.2412882 -4.2695432 -4.2806334 -4.27816 -4.2662888][-4.2243176 -4.2160993 -4.2045527 -4.1758676 -4.1207724 -4.06125 -4.0252995 -4.0281849 -4.0844383 -4.1507249 -4.2135992 -4.2545915 -4.265264 -4.2616167 -4.2477984][-4.2227807 -4.2165823 -4.2032328 -4.1627293 -4.0872369 -3.996994 -3.9254775 -3.9093051 -3.9846535 -4.0844288 -4.1718249 -4.2274613 -4.2418203 -4.23857 -4.2235384][-4.2215815 -4.2151065 -4.1967173 -4.1478753 -4.0613351 -3.9482098 -3.8345084 -3.7848086 -3.8714135 -4.0060992 -4.1175628 -4.1889725 -4.2150397 -4.2158647 -4.1992683][-4.2199006 -4.2135262 -4.1935835 -4.1467042 -4.0670133 -3.9578087 -3.833817 -3.7679217 -3.8444564 -3.9759541 -4.0844259 -4.1603341 -4.1965632 -4.2043777 -4.1911936][-4.2054558 -4.2011781 -4.1879811 -4.157959 -4.1015215 -4.0220537 -3.9326997 -3.8829837 -3.9286783 -4.0119824 -4.0863738 -4.1509724 -4.193181 -4.2088041 -4.2037988][-4.1804295 -4.1830282 -4.1817341 -4.1695833 -4.1387434 -4.0949478 -4.0504518 -4.0252056 -4.048409 -4.0864859 -4.1249514 -4.16865 -4.2044034 -4.2202435 -4.2213364][-4.1571336 -4.166368 -4.1702418 -4.1655569 -4.1508627 -4.1367645 -4.1295056 -4.1263294 -4.1424503 -4.1639948 -4.1857209 -4.2098961 -4.2265167 -4.2267556 -4.223176][-4.1415772 -4.1519456 -4.1510429 -4.1415472 -4.133296 -4.1363077 -4.145577 -4.152534 -4.166162 -4.1886287 -4.2127948 -4.2298303 -4.2367063 -4.2261138 -4.2109637][-4.1393719 -4.1413245 -4.1301003 -4.107439 -4.0927782 -4.0956388 -4.1054559 -4.1128082 -4.1264 -4.1584673 -4.1946054 -4.2168818 -4.2277875 -4.2185736 -4.1957459][-4.1488 -4.1472578 -4.1260538 -4.0905104 -4.0613942 -4.0483718 -4.0427232 -4.039607 -4.0514832 -4.0994177 -4.1552563 -4.1951566 -4.2196984 -4.2177205 -4.1900225][-4.1514764 -4.156682 -4.1387448 -4.0981174 -4.0550175 -4.0238595 -3.9922204 -3.96446 -3.9729948 -4.0382037 -4.116075 -4.1783795 -4.2186451 -4.2289195 -4.2071753]]...]
INFO - root - 2017-12-06 01:06:11.471517: step 56410, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 67h:40m:20s remains)
INFO - root - 2017-12-06 01:06:20.504526: step 56420, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.883 sec/batch; 67h:41m:39s remains)
INFO - root - 2017-12-06 01:06:29.779004: step 56430, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.953 sec/batch; 73h:06m:00s remains)
INFO - root - 2017-12-06 01:06:38.885877: step 56440, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 67h:59m:17s remains)
INFO - root - 2017-12-06 01:06:47.949178: step 56450, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 69h:57m:51s remains)
INFO - root - 2017-12-06 01:06:56.975426: step 56460, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.928 sec/batch; 71h:08m:38s remains)
INFO - root - 2017-12-06 01:07:05.896216: step 56470, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 67h:45m:21s remains)
INFO - root - 2017-12-06 01:07:14.926973: step 56480, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 69h:19m:32s remains)
INFO - root - 2017-12-06 01:07:24.147145: step 56490, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 70h:49m:10s remains)
INFO - root - 2017-12-06 01:07:33.213159: step 56500, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 70h:10m:46s remains)
2017-12-06 01:07:34.003426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1486483 -4.1476955 -4.1471987 -4.1475382 -4.14967 -4.16012 -4.1810131 -4.2056136 -4.2261543 -4.2351093 -4.2310791 -4.2145796 -4.192718 -4.1792207 -4.1689281][-4.165935 -4.16438 -4.1641188 -4.1640186 -4.1590667 -4.15377 -4.1634946 -4.1873369 -4.2187076 -4.2459421 -4.2643781 -4.2671356 -4.2530475 -4.2382026 -4.2204561][-4.1978178 -4.1956692 -4.19614 -4.1969957 -4.187398 -4.1648178 -4.150537 -4.1614823 -4.1975651 -4.2394958 -4.2726626 -4.2850876 -4.2752748 -4.26272 -4.24596][-4.2247734 -4.2228994 -4.2223654 -4.225605 -4.2161722 -4.1831546 -4.1466637 -4.137774 -4.1698432 -4.2191586 -4.2600818 -4.2764406 -4.2687888 -4.262176 -4.2526312][-4.2322469 -4.2283731 -4.2284431 -4.2386193 -4.2368221 -4.2025871 -4.1479812 -4.1137071 -4.1323977 -4.1859393 -4.2333469 -4.2516618 -4.246623 -4.2467537 -4.246541][-4.209672 -4.2018161 -4.2071795 -4.2314715 -4.2456827 -4.2181115 -4.1480689 -4.0844855 -4.0829368 -4.1337743 -4.1854906 -4.2094693 -4.2126012 -4.2217073 -4.2328725][-4.1637797 -4.154407 -4.1687355 -4.2083621 -4.2399592 -4.2233686 -4.1513786 -4.0700631 -4.0473175 -4.0877385 -4.1383204 -4.1700945 -4.1857138 -4.2047944 -4.2236538][-4.1215005 -4.1135416 -4.1346512 -4.1840329 -4.2255511 -4.2192478 -4.1553273 -4.0726137 -4.0376163 -4.0618048 -4.1033516 -4.1397562 -4.1665254 -4.1902122 -4.2134061][-4.107975 -4.1058125 -4.1275554 -4.1721797 -4.2098136 -4.2074971 -4.1562457 -4.0866122 -4.0518227 -4.0634322 -4.0917158 -4.1223936 -4.1496406 -4.1719966 -4.1954803][-4.1273775 -4.1303058 -4.1450996 -4.1728711 -4.1979809 -4.1930671 -4.150588 -4.1008859 -4.0807447 -4.0921197 -4.1122704 -4.1297855 -4.14578 -4.1589427 -4.1792235][-4.1652431 -4.1710591 -4.1746535 -4.1828957 -4.1933823 -4.1847234 -4.1504464 -4.1170759 -4.11471 -4.1322203 -4.1475759 -4.1544504 -4.157392 -4.1548085 -4.1651468][-4.2049913 -4.2097559 -4.205276 -4.20117 -4.2029438 -4.1938262 -4.1671367 -4.1437745 -4.1517396 -4.1754484 -4.1896138 -4.1889863 -4.1805382 -4.161705 -4.1575212][-4.2374816 -4.2428555 -4.2354951 -4.226 -4.2229443 -4.2119503 -4.1910992 -4.1722403 -4.1822767 -4.2072048 -4.2226725 -4.2192678 -4.201376 -4.1702905 -4.1551175][-4.2525687 -4.2606397 -4.2536459 -4.2413578 -4.2342958 -4.2217841 -4.2039557 -4.1868119 -4.1944962 -4.2187471 -4.2355108 -4.2313323 -4.2059875 -4.16628 -4.1416545][-4.2531033 -4.2628245 -4.2569709 -4.2449031 -4.2379742 -4.2273612 -4.2117329 -4.1960826 -4.20123 -4.2222929 -4.2369061 -4.2324071 -4.2033749 -4.1561427 -4.1228824]]...]
INFO - root - 2017-12-06 01:07:43.080911: step 56510, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 70h:02m:49s remains)
INFO - root - 2017-12-06 01:07:52.251415: step 56520, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 69h:32m:02s remains)
INFO - root - 2017-12-06 01:08:01.420203: step 56530, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 68h:31m:58s remains)
INFO - root - 2017-12-06 01:08:10.635853: step 56540, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 67h:59m:22s remains)
INFO - root - 2017-12-06 01:08:19.776779: step 56550, loss = 2.06, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 70h:05m:25s remains)
INFO - root - 2017-12-06 01:08:28.688544: step 56560, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 65h:38m:40s remains)
INFO - root - 2017-12-06 01:08:37.891738: step 56570, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.927 sec/batch; 71h:05m:14s remains)
INFO - root - 2017-12-06 01:08:46.976847: step 56580, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 70h:08m:22s remains)
INFO - root - 2017-12-06 01:08:55.897504: step 56590, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 63h:40m:24s remains)
INFO - root - 2017-12-06 01:09:05.249155: step 56600, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.930 sec/batch; 71h:15m:50s remains)
2017-12-06 01:09:06.115911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2758932 -4.2684293 -4.2640519 -4.264091 -4.2659965 -4.2678304 -4.2713428 -4.2752347 -4.2778192 -4.2742105 -4.2726283 -4.2741413 -4.2764492 -4.2815304 -4.2881904][-4.2548504 -4.2451391 -4.2388396 -4.2381997 -4.2430873 -4.249773 -4.2586594 -4.267642 -4.2739186 -4.2698507 -4.2677269 -4.2687135 -4.26862 -4.2717004 -4.2778797][-4.2358069 -4.2211967 -4.2113681 -4.2081656 -4.2118883 -4.220221 -4.2339678 -4.24883 -4.259757 -4.257637 -4.2580218 -4.2612886 -4.2620859 -4.2650928 -4.2719073][-4.2245765 -4.2041993 -4.1900225 -4.1785717 -4.174871 -4.1798096 -4.1945233 -4.2129622 -4.2293029 -4.2318635 -4.2355323 -4.2440047 -4.2503686 -4.25814 -4.2682633][-4.2261348 -4.2004623 -4.1785536 -4.1561956 -4.1451545 -4.1472979 -4.1604905 -4.1837182 -4.20791 -4.216146 -4.2211914 -4.2351685 -4.24827 -4.2605329 -4.2734089][-4.2270994 -4.1911182 -4.1577797 -4.1250987 -4.1058421 -4.0993214 -4.1095057 -4.1391382 -4.1764126 -4.1924362 -4.1999578 -4.220686 -4.24372 -4.2632546 -4.2786894][-4.2157726 -4.1647444 -4.115294 -4.0684948 -4.02952 -4.0051508 -4.0078821 -4.049612 -4.1005774 -4.1251469 -4.1420927 -4.1749887 -4.21351 -4.2437482 -4.266726][-4.1891418 -4.1203308 -4.051754 -3.9870222 -3.9193945 -3.8585455 -3.8443482 -3.90286 -3.9730208 -4.0043974 -4.0296011 -4.0764914 -4.1367884 -4.185008 -4.2243128][-4.174552 -4.1045117 -4.0307474 -3.9567406 -3.8751416 -3.7912004 -3.7647035 -3.8293805 -3.908854 -3.9419334 -3.9642913 -4.0106325 -4.0795245 -4.1385937 -4.1866913][-4.1957674 -4.1438389 -4.0874062 -4.027657 -3.9665656 -3.907 -3.8919678 -3.9399936 -4.0050831 -4.0296721 -4.0377207 -4.0654707 -4.1165247 -4.1611218 -4.1944489][-4.2233481 -4.1878581 -4.1505709 -4.1112471 -4.071847 -4.0370946 -4.0371246 -4.0727482 -4.115725 -4.1258745 -4.1212349 -4.1318216 -4.161211 -4.1910214 -4.2131052][-4.2433705 -4.2155175 -4.1878371 -4.1594453 -4.1350083 -4.1188431 -4.1293268 -4.1561322 -4.1826692 -4.1853676 -4.1770287 -4.1796632 -4.1982689 -4.2202024 -4.2354822][-4.2582521 -4.2330141 -4.2087979 -4.1847887 -4.1677842 -4.1611843 -4.1742969 -4.1948843 -4.2121062 -4.2131243 -4.2074771 -4.2106276 -4.2241087 -4.2421265 -4.25299][-4.2700844 -4.2476382 -4.2249031 -4.202064 -4.1866779 -4.1855335 -4.2001591 -4.217411 -4.2310362 -4.2338452 -4.231564 -4.2347474 -4.2440181 -4.2567754 -4.2651815][-4.2773442 -4.2578096 -4.2368679 -4.21537 -4.202127 -4.2040286 -4.2181749 -4.2321281 -4.2411871 -4.2447457 -4.245048 -4.2485304 -4.25512 -4.2644482 -4.2726526]]...]
INFO - root - 2017-12-06 01:09:15.295591: step 56610, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 68h:51m:14s remains)
INFO - root - 2017-12-06 01:09:24.348968: step 56620, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 69h:12m:23s remains)
INFO - root - 2017-12-06 01:09:33.462895: step 56630, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.931 sec/batch; 71h:19m:23s remains)
INFO - root - 2017-12-06 01:09:42.556069: step 56640, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 71h:12m:35s remains)
INFO - root - 2017-12-06 01:09:51.657474: step 56650, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.904 sec/batch; 69h:18m:16s remains)
INFO - root - 2017-12-06 01:10:00.518704: step 56660, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 68h:49m:16s remains)
INFO - root - 2017-12-06 01:10:09.627715: step 56670, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 69h:18m:21s remains)
INFO - root - 2017-12-06 01:10:18.753473: step 56680, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.900 sec/batch; 68h:56m:42s remains)
INFO - root - 2017-12-06 01:10:27.878791: step 56690, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 67h:44m:43s remains)
INFO - root - 2017-12-06 01:10:36.963941: step 56700, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 66h:43m:35s remains)
2017-12-06 01:10:37.772867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2455029 -4.2406988 -4.242527 -4.2355871 -4.2207317 -4.216753 -4.2265058 -4.242384 -4.2546544 -4.2589331 -4.253921 -4.2423925 -4.22394 -4.1988883 -4.1856828][-4.245657 -4.2398777 -4.2457414 -4.2472658 -4.2377286 -4.2334971 -4.2333026 -4.235198 -4.2402458 -4.2452087 -4.2488952 -4.2454996 -4.2327991 -4.2096834 -4.1927781][-4.2355013 -4.2301879 -4.2444549 -4.2592077 -4.2607837 -4.2590737 -4.2498407 -4.2372522 -4.2335415 -4.2386684 -4.2484131 -4.2506142 -4.2404213 -4.2195311 -4.2035508][-4.2192187 -4.213141 -4.2369542 -4.2653012 -4.2761273 -4.2758684 -4.2623763 -4.2406483 -4.2298412 -4.2336874 -4.2440104 -4.2476482 -4.2419081 -4.2303519 -4.2222185][-4.1988029 -4.1932273 -4.2265787 -4.2658553 -4.2783051 -4.2741756 -4.256917 -4.2333055 -4.2209511 -4.2213349 -4.2281651 -4.2315774 -4.2302694 -4.2293682 -4.2300072][-4.1658607 -4.1642551 -4.2089872 -4.2534809 -4.2578425 -4.2388129 -4.212553 -4.1905847 -4.1850057 -4.1898146 -4.1962242 -4.1995749 -4.2004976 -4.2044864 -4.2100182][-4.106606 -4.1143513 -4.1746721 -4.2196159 -4.2064471 -4.1629162 -4.1215143 -4.1039181 -4.1169105 -4.1382113 -4.1519642 -4.157002 -4.1605926 -4.1665287 -4.1740565][-4.0451379 -4.0646815 -4.1330504 -4.1692319 -4.1336102 -4.0609441 -3.9990423 -3.9926927 -4.0412078 -4.0910249 -4.1190672 -4.132349 -4.1411486 -4.1488528 -4.1576047][-4.0417728 -4.0684929 -4.1278934 -4.1481447 -4.0940213 -3.9976356 -3.9220104 -3.9314013 -4.01292 -4.0869284 -4.1286297 -4.1512151 -4.1637583 -4.1703138 -4.1768031][-4.1082821 -4.1334252 -4.1718578 -4.1771374 -4.1228218 -4.0349646 -3.9732738 -3.9929821 -4.0695868 -4.1366158 -4.1756358 -4.1993294 -4.212224 -4.2172022 -4.2202682][-4.1886091 -4.2080626 -4.2264109 -4.22243 -4.1801982 -4.1205473 -4.0863886 -4.1074519 -4.1596751 -4.2028317 -4.2298436 -4.2476535 -4.2569423 -4.2598839 -4.260026][-4.2357945 -4.2503386 -4.2591109 -4.2542286 -4.2270908 -4.1930308 -4.1783862 -4.19426 -4.2224121 -4.2443166 -4.260488 -4.2719626 -4.2776961 -4.2787056 -4.2776966][-4.2469673 -4.2568731 -4.2621317 -4.25885 -4.2429547 -4.22575 -4.2219172 -4.231535 -4.2439494 -4.2541738 -4.263628 -4.270741 -4.2736783 -4.2737045 -4.273201][-4.2416763 -4.2488327 -4.2541218 -4.2536178 -4.2459025 -4.2373114 -4.2362828 -4.2408581 -4.2462053 -4.2515512 -4.2576265 -4.2623897 -4.2643147 -4.2641325 -4.2640553][-4.2412519 -4.245182 -4.2498593 -4.251194 -4.2483215 -4.2446551 -4.2443523 -4.2463684 -4.24874 -4.2517471 -4.2552156 -4.25761 -4.258595 -4.2585835 -4.2589121]]...]
INFO - root - 2017-12-06 01:10:46.862602: step 56710, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 69h:16m:19s remains)
INFO - root - 2017-12-06 01:10:56.191783: step 56720, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 71h:14m:52s remains)
INFO - root - 2017-12-06 01:11:05.528395: step 56730, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.924 sec/batch; 70h:48m:43s remains)
INFO - root - 2017-12-06 01:11:14.648277: step 56740, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 72h:08m:57s remains)
INFO - root - 2017-12-06 01:11:23.694768: step 56750, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 69h:40m:21s remains)
INFO - root - 2017-12-06 01:11:32.597922: step 56760, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 69h:50m:54s remains)
INFO - root - 2017-12-06 01:11:41.661362: step 56770, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 66h:34m:02s remains)
INFO - root - 2017-12-06 01:11:50.690256: step 56780, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.913 sec/batch; 69h:57m:32s remains)
INFO - root - 2017-12-06 01:11:59.748538: step 56790, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.931 sec/batch; 71h:18m:38s remains)
INFO - root - 2017-12-06 01:12:08.857448: step 56800, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 66h:36m:13s remains)
2017-12-06 01:12:09.670502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1379056 -4.1457 -4.1549063 -4.1661415 -4.1881862 -4.2155471 -4.2372713 -4.2505584 -4.2498584 -4.2428412 -4.2402658 -4.2351933 -4.2217913 -4.2126255 -4.2151031][-4.171885 -4.1677389 -4.1662574 -4.1720214 -4.1926332 -4.2215071 -4.2447948 -4.2616038 -4.2625384 -4.2528777 -4.2465258 -4.2325215 -4.2082949 -4.1927514 -4.1908631][-4.196898 -4.1782546 -4.1629448 -4.1592841 -4.1711106 -4.1939869 -4.2158227 -4.2384515 -4.2499952 -4.2461615 -4.2420688 -4.2244539 -4.1867504 -4.1511526 -4.1282091][-4.2084503 -4.1761689 -4.140779 -4.1174622 -4.1143355 -4.1286526 -4.1521459 -4.1860003 -4.2146173 -4.2306724 -4.2364173 -4.2207265 -4.1744051 -4.1208167 -4.075141][-4.2129054 -4.1717081 -4.1187057 -4.0747309 -4.051434 -4.0525007 -4.069509 -4.1057811 -4.148562 -4.193428 -4.2239475 -4.2204208 -4.1745138 -4.1142826 -4.0535855][-4.215373 -4.167706 -4.1041107 -4.0450916 -4.0024805 -3.9817092 -3.9737279 -3.9844038 -4.0295672 -4.1134129 -4.1842422 -4.2128148 -4.1910615 -4.1435409 -4.0842333][-4.21845 -4.1727781 -4.1093884 -4.04966 -3.9967105 -3.9438627 -3.8873887 -3.8490176 -3.8827162 -4.00264 -4.1185536 -4.1890321 -4.2070417 -4.190609 -4.1544561][-4.2272887 -4.1933212 -4.1450653 -4.1012597 -4.06109 -4.0027375 -3.9194429 -3.838624 -3.8304632 -3.9367967 -4.0616775 -4.1546674 -4.20258 -4.215734 -4.2047229][-4.2342067 -4.2124047 -4.1827807 -4.1576748 -4.1378088 -4.0947518 -4.0293994 -3.9609761 -3.9378924 -3.9933054 -4.0774407 -4.151114 -4.200995 -4.2277932 -4.23365][-4.238152 -4.2263885 -4.211782 -4.2022042 -4.1984425 -4.1773186 -4.1390424 -4.0961618 -4.0769305 -4.0977798 -4.1426296 -4.1911416 -4.22571 -4.2484684 -4.2612977][-4.2384119 -4.2330246 -4.2258 -4.226007 -4.2356873 -4.2350326 -4.2221131 -4.2031178 -4.1916442 -4.1967158 -4.2175269 -4.2448368 -4.2657709 -4.2788062 -4.2890759][-4.237669 -4.2349606 -4.2310805 -4.2358127 -4.2552943 -4.2675376 -4.2732973 -4.275044 -4.2717838 -4.2717066 -4.2813768 -4.2948589 -4.3008728 -4.3040071 -4.307838][-4.23819 -4.2376719 -4.2359915 -4.2413545 -4.2640071 -4.2820334 -4.2953959 -4.3103466 -4.3172135 -4.3179426 -4.3196778 -4.322134 -4.3179579 -4.3144808 -4.3136334][-4.2402325 -4.2411337 -4.2396636 -4.2443795 -4.2672038 -4.2860761 -4.2983871 -4.3138943 -4.3240981 -4.3257804 -4.3225517 -4.3187609 -4.3120627 -4.30681 -4.304153][-4.2431588 -4.2449741 -4.2430754 -4.2465024 -4.2677293 -4.2842712 -4.2933941 -4.3060288 -4.3143492 -4.3154716 -4.30981 -4.304285 -4.298171 -4.2951746 -4.2947507]]...]
INFO - root - 2017-12-06 01:12:18.930498: step 56810, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 71h:07m:12s remains)
INFO - root - 2017-12-06 01:12:28.028631: step 56820, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 69h:35m:47s remains)
INFO - root - 2017-12-06 01:12:37.075816: step 56830, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 69h:53m:28s remains)
INFO - root - 2017-12-06 01:12:46.099627: step 56840, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 70h:03m:05s remains)
INFO - root - 2017-12-06 01:12:55.219544: step 56850, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 72h:22m:56s remains)
INFO - root - 2017-12-06 01:13:04.356920: step 56860, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 67h:53m:06s remains)
INFO - root - 2017-12-06 01:13:13.313404: step 56870, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.777 sec/batch; 59h:28m:34s remains)
INFO - root - 2017-12-06 01:13:22.351170: step 56880, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 69h:41m:48s remains)
INFO - root - 2017-12-06 01:13:31.637030: step 56890, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.949 sec/batch; 72h:41m:05s remains)
INFO - root - 2017-12-06 01:13:40.882779: step 56900, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 70h:12m:50s remains)
2017-12-06 01:13:41.716299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3092995 -4.2984524 -4.2779846 -4.2683048 -4.2737346 -4.2773623 -4.2715259 -4.2614021 -4.2505054 -4.25272 -4.2639985 -4.2791691 -4.2961807 -4.3111362 -4.3209171][-4.304882 -4.29836 -4.2821612 -4.2756972 -4.2789645 -4.27176 -4.2487164 -4.2238064 -4.2054186 -4.2082534 -4.2260089 -4.2484803 -4.2752905 -4.298954 -4.3157368][-4.2991657 -4.2994657 -4.2864761 -4.279357 -4.2769332 -4.258884 -4.2184954 -4.1749797 -4.144968 -4.1473947 -4.1728559 -4.2040758 -4.2415848 -4.2777843 -4.305254][-4.2811394 -4.2895861 -4.2819309 -4.2755246 -4.2696609 -4.2445917 -4.18785 -4.123847 -4.0783949 -4.0812225 -4.1160674 -4.1580329 -4.2072315 -4.2557697 -4.2935686][-4.2382932 -4.2574348 -4.2582631 -4.2553296 -4.2481475 -4.2148867 -4.1454639 -4.0655537 -4.0109835 -4.020927 -4.0733714 -4.1302 -4.1898437 -4.2464805 -4.288506][-4.1784167 -4.2102885 -4.2220573 -4.2266593 -4.2186575 -4.1807308 -4.1029115 -4.0108714 -3.952018 -3.9765909 -4.0516834 -4.121716 -4.1891346 -4.24827 -4.2905884][-4.1153069 -4.1631136 -4.1909637 -4.2044311 -4.1924286 -4.1470914 -4.0557423 -3.9479074 -3.8875651 -3.9318175 -4.0285091 -4.1141796 -4.190927 -4.2525616 -4.2930946][-4.0589786 -4.1247072 -4.1731033 -4.1977096 -4.1843429 -4.1287603 -4.0237379 -3.9077322 -3.8523798 -3.909421 -4.0202656 -4.1222305 -4.2119417 -4.2740183 -4.3070908][-4.0546646 -4.1244144 -4.1823964 -4.2139783 -4.2021852 -4.1476622 -4.0519018 -3.9553242 -3.9167233 -3.9668267 -4.0661469 -4.1634836 -4.2499871 -4.3043532 -4.3264494][-4.1201043 -4.1718917 -4.2160115 -4.2406955 -4.2296109 -4.1868248 -4.117053 -4.0504289 -4.0221739 -4.0502224 -4.1213031 -4.1996284 -4.2731051 -4.320097 -4.3365541][-4.1869159 -4.21234 -4.2324557 -4.2415781 -4.2289109 -4.2031269 -4.1636677 -4.1204967 -4.093112 -4.1008153 -4.1474004 -4.2082448 -4.2684517 -4.3108721 -4.3292584][-4.2185907 -4.2214093 -4.2180972 -4.2095084 -4.1960406 -4.1885853 -4.1753206 -4.1467505 -4.1193666 -4.1177235 -4.150867 -4.20005 -4.2510443 -4.2907653 -4.3139663][-4.2126632 -4.2050896 -4.1905808 -4.1757793 -4.1678791 -4.174829 -4.17842 -4.1582012 -4.1316609 -4.1292515 -4.15526 -4.1958132 -4.2391791 -4.2761474 -4.3008661][-4.2118645 -4.2024617 -4.1852617 -4.1726317 -4.1701908 -4.1814418 -4.1903086 -4.1766591 -4.1581078 -4.1617317 -4.1856871 -4.218513 -4.2518282 -4.2805066 -4.2996988][-4.2449126 -4.2372131 -4.22403 -4.21598 -4.2163372 -4.2263851 -4.2349019 -4.2284288 -4.2179241 -4.2242541 -4.2434711 -4.2674036 -4.28957 -4.3060145 -4.3156114]]...]
INFO - root - 2017-12-06 01:13:50.855357: step 56910, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.902 sec/batch; 69h:03m:12s remains)
INFO - root - 2017-12-06 01:13:59.952674: step 56920, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 70h:19m:24s remains)
INFO - root - 2017-12-06 01:14:09.067697: step 56930, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 70h:11m:26s remains)
INFO - root - 2017-12-06 01:14:18.260414: step 56940, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 67h:49m:32s remains)
INFO - root - 2017-12-06 01:14:27.272639: step 56950, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.912 sec/batch; 69h:47m:48s remains)
INFO - root - 2017-12-06 01:14:36.330827: step 56960, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 66h:24m:14s remains)
INFO - root - 2017-12-06 01:14:45.198159: step 56970, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 69h:15m:59s remains)
INFO - root - 2017-12-06 01:14:54.353446: step 56980, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 69h:35m:10s remains)
INFO - root - 2017-12-06 01:15:03.404244: step 56990, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 69h:05m:08s remains)
INFO - root - 2017-12-06 01:15:12.498717: step 57000, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 70h:06m:46s remains)
2017-12-06 01:15:13.281457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2548728 -4.2891011 -4.3031731 -4.2983608 -4.2827687 -4.2693868 -4.2595916 -4.2580113 -4.2700553 -4.2871523 -4.3067975 -4.3216157 -4.3328953 -4.3365116 -4.3375835][-4.2156496 -4.2655869 -4.2910771 -4.2900057 -4.2712889 -4.2406416 -4.2085295 -4.1980767 -4.2157111 -4.2461791 -4.2800612 -4.3051462 -4.323637 -4.3310137 -4.3314304][-4.1870589 -4.246141 -4.2756381 -4.2721076 -4.2438064 -4.1924767 -4.13912 -4.1234622 -4.1510344 -4.1997457 -4.2499561 -4.2867351 -4.3126431 -4.3236918 -4.3238835][-4.1740236 -4.235877 -4.264287 -4.2560544 -4.2127795 -4.1388969 -4.0670252 -4.04694 -4.0853519 -4.1537666 -4.2213616 -4.269001 -4.3019204 -4.3170242 -4.3172398][-4.1819658 -4.2410054 -4.2639513 -4.2464151 -4.1876221 -4.0934148 -4.0042734 -3.9764943 -4.0267048 -4.1147113 -4.1976495 -4.2547789 -4.2939272 -4.313055 -4.3144007][-4.1939187 -4.2490697 -4.2677841 -4.2434568 -4.1760325 -4.0663748 -3.9565895 -3.9142416 -3.9733882 -4.0815229 -4.1774931 -4.2430286 -4.2864609 -4.3091364 -4.3133135][-4.1865945 -4.2408586 -4.2618179 -4.2397242 -4.1721244 -4.0535569 -3.9202526 -3.856497 -3.9267833 -4.0552969 -4.1629982 -4.2334208 -4.2794476 -4.3042316 -4.3115907][-4.1633124 -4.222527 -4.2496524 -4.2345705 -4.172451 -4.05147 -3.9018881 -3.8180959 -3.8967023 -4.037848 -4.1523728 -4.2244692 -4.2715883 -4.297452 -4.3067575][-4.1457443 -4.204247 -4.2350659 -4.2293835 -4.1786118 -4.0691218 -3.9239049 -3.8405356 -3.9147496 -4.0492525 -4.1552882 -4.2195468 -4.2623734 -4.2886214 -4.3008513][-4.1528063 -4.1981883 -4.2249322 -4.2282686 -4.1925893 -4.105864 -3.9851017 -3.9232974 -3.9858406 -4.0935063 -4.1750574 -4.222218 -4.2548652 -4.2786088 -4.2924747][-4.1629696 -4.1940675 -4.214993 -4.2249212 -4.206039 -4.1446443 -4.0542393 -4.0159931 -4.0664363 -4.1459513 -4.1997509 -4.2275062 -4.2483649 -4.268013 -4.2838945][-4.1673632 -4.1885595 -4.2029538 -4.2170358 -4.2150331 -4.1795 -4.1220236 -4.0989447 -4.1350861 -4.1900558 -4.2206984 -4.2323632 -4.2428794 -4.2591929 -4.2787442][-4.1730146 -4.1862731 -4.1922731 -4.2048368 -4.2164865 -4.2049646 -4.1781693 -4.1664572 -4.1901588 -4.2241788 -4.2386327 -4.2373462 -4.238472 -4.2528906 -4.2767072][-4.1852784 -4.1905971 -4.1849408 -4.1920061 -4.2107277 -4.2160983 -4.2110157 -4.2103372 -4.2265697 -4.2434039 -4.2459927 -4.2352009 -4.2308168 -4.2437973 -4.270844][-4.2145185 -4.2150254 -4.1985884 -4.1962819 -4.2112665 -4.22187 -4.22569 -4.2297955 -4.2394261 -4.2435174 -4.2385964 -4.2271891 -4.2226119 -4.2375975 -4.2682281]]...]
INFO - root - 2017-12-06 01:15:22.334884: step 57010, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 70h:54m:53s remains)
INFO - root - 2017-12-06 01:15:31.470190: step 57020, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 69h:31m:51s remains)
INFO - root - 2017-12-06 01:15:40.519815: step 57030, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 69h:19m:03s remains)
INFO - root - 2017-12-06 01:15:49.566116: step 57040, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 69h:38m:32s remains)
INFO - root - 2017-12-06 01:15:58.758736: step 57050, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.910 sec/batch; 69h:36m:08s remains)
INFO - root - 2017-12-06 01:16:07.800047: step 57060, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 67h:05m:20s remains)
INFO - root - 2017-12-06 01:16:16.634484: step 57070, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.816 sec/batch; 62h:27m:48s remains)
INFO - root - 2017-12-06 01:16:25.804143: step 57080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 68h:41m:57s remains)
INFO - root - 2017-12-06 01:16:34.920093: step 57090, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.904 sec/batch; 69h:10m:40s remains)
INFO - root - 2017-12-06 01:16:43.953989: step 57100, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 71h:54m:04s remains)
2017-12-06 01:16:44.824091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3098946 -4.3057485 -4.2982564 -4.2819276 -4.2650285 -4.2517195 -4.2362795 -4.2200069 -4.2139378 -4.2285714 -4.2486477 -4.2555509 -4.2555733 -4.2570229 -4.2558832][-4.3093605 -4.3030639 -4.292696 -4.2749357 -4.2555342 -4.2359991 -4.2118874 -4.184412 -4.1722345 -4.194931 -4.2274184 -4.242075 -4.2463322 -4.2516532 -4.252943][-4.3048711 -4.297864 -4.2883329 -4.2712941 -4.2479396 -4.2189689 -4.1850185 -4.1487074 -4.136476 -4.1700211 -4.2096868 -4.2250547 -4.2271657 -4.2312074 -4.2317414][-4.3008628 -4.29602 -4.2886615 -4.270648 -4.2434363 -4.2059469 -4.163033 -4.1226468 -4.1144576 -4.1557145 -4.1952319 -4.2056742 -4.20192 -4.2045317 -4.2060103][-4.3000894 -4.2974668 -4.2884512 -4.2656112 -4.23213 -4.1849394 -4.1331739 -4.0911679 -4.0873013 -4.1360245 -4.17743 -4.1901474 -4.1876369 -4.191155 -4.1936421][-4.3016834 -4.2996774 -4.2848282 -4.2529979 -4.2108374 -4.1533432 -4.0932989 -4.0479431 -4.0496297 -4.1096272 -4.1628046 -4.18814 -4.1913795 -4.195056 -4.1974325][-4.3051357 -4.3027749 -4.2828751 -4.2407541 -4.1879754 -4.1241822 -4.0662589 -4.0257497 -4.0367923 -4.1057215 -4.1692023 -4.2024512 -4.2064939 -4.2090549 -4.2133288][-4.3072786 -4.3039536 -4.2812586 -4.2323036 -4.1758528 -4.1188807 -4.0766544 -4.0519218 -4.0721068 -4.1368742 -4.1960487 -4.2267551 -4.2300005 -4.2307305 -4.2344913][-4.3058538 -4.3006225 -4.2754445 -4.2237115 -4.1696143 -4.1266985 -4.1036487 -4.0959349 -4.1221852 -4.1793323 -4.23077 -4.2572155 -4.2590361 -4.2581444 -4.2584124][-4.3011365 -4.2918496 -4.26379 -4.2129645 -4.164052 -4.1361394 -4.130609 -4.13548 -4.16587 -4.2155523 -4.2575259 -4.2794361 -4.2836909 -4.2838893 -4.2809863][-4.294672 -4.2808933 -4.2521124 -4.2067833 -4.1663303 -4.1505723 -4.1587477 -4.17262 -4.205502 -4.2478032 -4.2762251 -4.2890649 -4.2930422 -4.2962174 -4.29533][-4.2890768 -4.2712183 -4.2448335 -4.2096267 -4.1807761 -4.1747017 -4.1914463 -4.2126455 -4.244432 -4.2767243 -4.2906561 -4.2920918 -4.2928119 -4.2981658 -4.3001428][-4.2854733 -4.2666984 -4.2452421 -4.2215891 -4.2039227 -4.2047572 -4.2243547 -4.2471619 -4.2723136 -4.2941537 -4.2967191 -4.2890587 -4.2864804 -4.2924137 -4.2962842][-4.2858467 -4.2700047 -4.2552328 -4.242249 -4.2339005 -4.2372856 -4.2525725 -4.2688613 -4.2851691 -4.2972021 -4.2933478 -4.2827144 -4.2801843 -4.2880535 -4.2937522][-4.2909765 -4.28004 -4.2728109 -4.2679124 -4.2637463 -4.2643504 -4.2709312 -4.2779436 -4.286674 -4.2918606 -4.2850709 -4.2743998 -4.2727246 -4.2812304 -4.2875495]]...]
INFO - root - 2017-12-06 01:16:54.037876: step 57110, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 71h:01m:37s remains)
INFO - root - 2017-12-06 01:17:03.027763: step 57120, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 68h:29m:53s remains)
INFO - root - 2017-12-06 01:17:12.143514: step 57130, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.933 sec/batch; 71h:20m:17s remains)
INFO - root - 2017-12-06 01:17:21.193271: step 57140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 66h:57m:38s remains)
INFO - root - 2017-12-06 01:17:30.219102: step 57150, loss = 2.03, batch loss = 1.98 (8.6 examples/sec; 0.935 sec/batch; 71h:31m:14s remains)
INFO - root - 2017-12-06 01:17:39.183987: step 57160, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 68h:19m:46s remains)
INFO - root - 2017-12-06 01:17:48.359934: step 57170, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 69h:02m:28s remains)
INFO - root - 2017-12-06 01:17:57.500495: step 57180, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 68h:14m:42s remains)
INFO - root - 2017-12-06 01:18:06.662404: step 57190, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.944 sec/batch; 72h:10m:47s remains)
INFO - root - 2017-12-06 01:18:15.713039: step 57200, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 70h:41m:38s remains)
2017-12-06 01:18:16.500888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3161716 -4.3244052 -4.3261147 -4.3248463 -4.3222375 -4.3247318 -4.3323941 -4.3398204 -4.345788 -4.348649 -4.3494182 -4.3512621 -4.3561435 -4.3623242 -4.3681026][-4.2900276 -4.297821 -4.3010435 -4.3003817 -4.2966695 -4.3022442 -4.3150516 -4.3245659 -4.3298273 -4.3315754 -4.3327389 -4.3367119 -4.3449469 -4.3548574 -4.3665724][-4.2601395 -4.2667403 -4.2729397 -4.270545 -4.2619853 -4.2702551 -4.2865548 -4.2950368 -4.2995429 -4.3019633 -4.3076606 -4.3181868 -4.3315597 -4.3443527 -4.3618569][-4.2241235 -4.2326231 -4.2413325 -4.2320466 -4.2133217 -4.2204962 -4.2374434 -4.2444954 -4.2496824 -4.2565579 -4.2695208 -4.2910032 -4.3128414 -4.3312321 -4.3552108][-4.1854219 -4.2023354 -4.211906 -4.1974096 -4.170588 -4.1745887 -4.1867123 -4.1871309 -4.1922665 -4.2059793 -4.2286386 -4.2636018 -4.2956872 -4.3209443 -4.3512716][-4.1542807 -4.1749849 -4.1792674 -4.158917 -4.1268291 -4.1256046 -4.1314259 -4.12358 -4.1292195 -4.1534767 -4.1911831 -4.2390637 -4.2797527 -4.3119125 -4.3491793][-4.1273127 -4.1462712 -4.1455579 -4.1218476 -4.0852513 -4.0777206 -4.0777044 -4.0683 -4.0808182 -4.1190004 -4.1703296 -4.2247057 -4.2699337 -4.3065009 -4.3467765][-4.1161251 -4.1332626 -4.1352096 -4.1123648 -4.0723724 -4.0547991 -4.0485511 -4.0444341 -4.0624638 -4.1037283 -4.1579823 -4.2150149 -4.2633677 -4.3012056 -4.3399525][-4.1322632 -4.1460404 -4.1508403 -4.1317081 -4.09188 -4.0669475 -4.057024 -4.0575542 -4.0690069 -4.0980492 -4.1414418 -4.1988268 -4.2502074 -4.2920966 -4.3297219][-4.1846547 -4.1930695 -4.1934648 -4.1784649 -4.1455517 -4.1178646 -4.1037054 -4.1019073 -4.1011925 -4.11111 -4.1380343 -4.1900568 -4.2425618 -4.2869697 -4.3221779][-4.2245569 -4.22612 -4.2244058 -4.2123342 -4.1842818 -4.1602306 -4.1488938 -4.1475806 -4.1435103 -4.1418495 -4.1588426 -4.2011824 -4.2490654 -4.289052 -4.31988][-4.2260308 -4.2171588 -4.2123542 -4.2070222 -4.189703 -4.1771278 -4.1760917 -4.1815104 -4.1827269 -4.1805215 -4.1939206 -4.2249179 -4.2620544 -4.29312 -4.3190928][-4.2159958 -4.1910534 -4.1763062 -4.1799984 -4.1825681 -4.1859808 -4.198688 -4.2141352 -4.2214866 -4.2176666 -4.2224045 -4.2415171 -4.2696209 -4.2934737 -4.31514][-4.2172623 -4.1761575 -4.1501632 -4.1549478 -4.1727376 -4.19409 -4.2204494 -4.2440276 -4.2545781 -4.2436876 -4.2343493 -4.2429709 -4.2663 -4.2872925 -4.3075051][-4.2409897 -4.1920295 -4.1517467 -4.1454687 -4.1661935 -4.1976581 -4.2339053 -4.2637353 -4.2726088 -4.2542043 -4.236526 -4.2383928 -4.2588763 -4.27799 -4.2984047]]...]
INFO - root - 2017-12-06 01:18:25.755950: step 57210, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.919 sec/batch; 70h:15m:09s remains)
INFO - root - 2017-12-06 01:18:34.745512: step 57220, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 70h:55m:45s remains)
INFO - root - 2017-12-06 01:18:43.751686: step 57230, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 69h:37m:02s remains)
INFO - root - 2017-12-06 01:18:52.976020: step 57240, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 68h:18m:27s remains)
INFO - root - 2017-12-06 01:19:02.138800: step 57250, loss = 2.05, batch loss = 1.99 (7.7 examples/sec; 1.033 sec/batch; 78h:59m:05s remains)
INFO - root - 2017-12-06 01:19:11.136851: step 57260, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 69h:11m:32s remains)
INFO - root - 2017-12-06 01:19:20.284457: step 57270, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 69h:06m:05s remains)
INFO - root - 2017-12-06 01:19:29.365730: step 57280, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 69h:29m:13s remains)
INFO - root - 2017-12-06 01:19:38.525587: step 57290, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.915 sec/batch; 69h:54m:41s remains)
INFO - root - 2017-12-06 01:19:47.672911: step 57300, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.948 sec/batch; 72h:26m:52s remains)
2017-12-06 01:19:48.462657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2354417 -4.235002 -4.2372479 -4.2381525 -4.2260475 -4.2147174 -4.2180123 -4.2226844 -4.2271733 -4.2334228 -4.23195 -4.218873 -4.2135606 -4.216485 -4.2292886][-4.2041817 -4.2090921 -4.211442 -4.2025952 -4.181644 -4.1630287 -4.1671672 -4.1747046 -4.1837306 -4.1957488 -4.1986217 -4.1832614 -4.1722212 -4.1717739 -4.1870956][-4.1704178 -4.1788769 -4.181675 -4.1642389 -4.1328287 -4.1071811 -4.1158361 -4.1299787 -4.1469955 -4.159832 -4.1625934 -4.1453514 -4.1332674 -4.1275024 -4.1397214][-4.131393 -4.1346498 -4.1327949 -4.1134152 -4.0847497 -4.0576487 -4.0635376 -4.0775189 -4.0979123 -4.1146741 -4.1163282 -4.0972986 -4.083221 -4.073638 -4.0802727][-4.1005473 -4.096436 -4.0912409 -4.0710816 -4.0426865 -3.9988229 -3.9730158 -3.9833567 -4.0191908 -4.0523515 -4.0610962 -4.0470047 -4.0301495 -4.0129056 -4.0196915][-4.0935616 -4.0844121 -4.072722 -4.0445766 -4.0023832 -3.9213624 -3.8242307 -3.822757 -3.9060748 -3.9790437 -4.009829 -4.0096455 -4.0001779 -3.9842048 -3.9918084][-4.11334 -4.1050472 -4.0843506 -4.038312 -3.9752116 -3.8740156 -3.7314789 -3.7195952 -3.8412809 -3.9499125 -4.0015368 -4.006423 -3.9958057 -3.9750826 -3.9723468][-4.1363926 -4.1292467 -4.1040545 -4.0551972 -4.0038371 -3.9427342 -3.8484511 -3.8249838 -3.8958392 -3.9834173 -4.0344892 -4.0426321 -4.0190444 -3.9860353 -3.9789293][-4.1363192 -4.1352839 -4.1207194 -4.086822 -4.0544567 -4.03095 -3.9938705 -3.97616 -3.9915295 -4.0407505 -4.0838027 -4.0980539 -4.0768743 -4.050972 -4.0497313][-4.1260228 -4.1294756 -4.1296134 -4.1097741 -4.085475 -4.07587 -4.0659313 -4.0547209 -4.0408859 -4.0619736 -4.097281 -4.1146331 -4.1049213 -4.0985036 -4.1078482][-4.1158423 -4.123754 -4.1397753 -4.1341009 -4.1163988 -4.1035008 -4.1012468 -4.0990219 -4.07995 -4.0757952 -4.09871 -4.1066589 -4.0982661 -4.0994735 -4.1166425][-4.1185789 -4.1310372 -4.1558242 -4.1596708 -4.14983 -4.1374125 -4.1394806 -4.1431408 -4.1302977 -4.1178455 -4.1212482 -4.1227059 -4.1125631 -4.117053 -4.13921][-4.1538944 -4.1651373 -4.1885834 -4.19861 -4.1965585 -4.1894073 -4.1916456 -4.1901221 -4.1815147 -4.1713095 -4.1640277 -4.1598854 -4.1533179 -4.1603174 -4.181282][-4.203485 -4.2111554 -4.2278819 -4.2391415 -4.2431812 -4.2427158 -4.2388835 -4.2306962 -4.2249031 -4.218699 -4.2104158 -4.2029281 -4.2018104 -4.2094588 -4.2229748][-4.251658 -4.2551446 -4.2640634 -4.27494 -4.281816 -4.2847185 -4.2787137 -4.2677979 -4.2637963 -4.2605262 -4.2563467 -4.2498531 -4.2487378 -4.2513137 -4.2562103]]...]
INFO - root - 2017-12-06 01:19:57.519471: step 57310, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 69h:01m:28s remains)
INFO - root - 2017-12-06 01:20:06.538963: step 57320, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 68h:18m:47s remains)
INFO - root - 2017-12-06 01:20:15.617824: step 57330, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.922 sec/batch; 70h:27m:08s remains)
INFO - root - 2017-12-06 01:20:24.755783: step 57340, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 69h:14m:28s remains)
INFO - root - 2017-12-06 01:20:33.836385: step 57350, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 68h:20m:01s remains)
INFO - root - 2017-12-06 01:20:42.995808: step 57360, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.931 sec/batch; 71h:09m:48s remains)
INFO - root - 2017-12-06 01:20:52.126059: step 57370, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 69h:02m:33s remains)
INFO - root - 2017-12-06 01:21:01.049816: step 57380, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.771 sec/batch; 58h:54m:48s remains)
INFO - root - 2017-12-06 01:21:09.985487: step 57390, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 68h:11m:09s remains)
INFO - root - 2017-12-06 01:21:19.057060: step 57400, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 71h:10m:36s remains)
2017-12-06 01:21:19.851698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3368573 -4.3356934 -4.3323379 -4.3303232 -4.3299947 -4.33091 -4.3327208 -4.3352084 -4.337163 -4.3373613 -4.3340812 -4.3257828 -4.3051853 -4.2636847 -4.2079349][-4.3252411 -4.3261337 -4.3269763 -4.32847 -4.3300128 -4.3307552 -4.3309088 -4.331975 -4.333941 -4.3358245 -4.3375263 -4.3386288 -4.3334656 -4.3146825 -4.288022][-4.3274055 -4.327672 -4.3285513 -4.3290739 -4.3279867 -4.3228989 -4.3155742 -4.3102088 -4.3096762 -4.3136559 -4.3219566 -4.3330884 -4.3417397 -4.3403268 -4.3328743][-4.3307638 -4.3293066 -4.3273849 -4.3230624 -4.3140674 -4.2954955 -4.2703152 -4.249537 -4.2427816 -4.25096 -4.2737088 -4.3027897 -4.3280516 -4.3409882 -4.3450918][-4.3302336 -4.325963 -4.3175693 -4.3017569 -4.2733979 -4.2258778 -4.168129 -4.1235976 -4.1116748 -4.13562 -4.1873078 -4.247016 -4.2955837 -4.3224869 -4.3341813][-4.3270111 -4.3171153 -4.2983141 -4.2640214 -4.2048392 -4.11571 -4.0163059 -3.945493 -3.9368744 -3.9908736 -4.0846853 -4.1823382 -4.255827 -4.2963285 -4.3162336][-4.3222108 -4.3052411 -4.2739439 -4.2182894 -4.12633 -3.9984629 -3.8680887 -3.7863269 -3.7980309 -3.8913147 -4.0224719 -4.146656 -4.2345619 -4.2835426 -4.3079352][-4.3197837 -4.2990403 -4.2605577 -4.1943026 -4.0889907 -3.9547818 -3.8342624 -3.7737217 -3.8097091 -3.9170609 -4.0506644 -4.1687865 -4.2490635 -4.2929235 -4.3128381][-4.3212919 -4.3030715 -4.2693124 -4.2131028 -4.1279974 -4.0286093 -3.9515095 -3.9237933 -3.96421 -4.0503693 -4.1514049 -4.23566 -4.2890725 -4.3156118 -4.3244405][-4.3254604 -4.3133683 -4.2912588 -4.2562938 -4.2057915 -4.151926 -4.1177306 -4.1131959 -4.1459594 -4.2015529 -4.2615051 -4.3064837 -4.3298063 -4.3376741 -4.3358583][-4.3308563 -4.3256712 -4.3168015 -4.3031678 -4.2825303 -4.2631931 -4.2565684 -4.2631516 -4.2847228 -4.3129854 -4.338778 -4.3524003 -4.3535166 -4.348731 -4.3409181][-4.3350434 -4.3345394 -4.334374 -4.3339391 -4.3309345 -4.3293138 -4.3337588 -4.3420172 -4.3538518 -4.3637929 -4.3682275 -4.3650789 -4.3563805 -4.3472276 -4.3387346][-4.3358445 -4.3365602 -4.3386426 -4.3417554 -4.3439503 -4.346518 -4.3510756 -4.3559103 -4.3601804 -4.3608103 -4.3576484 -4.3515782 -4.3444324 -4.3382282 -4.3335152][-4.3336234 -4.3328695 -4.3330493 -4.3337541 -4.3339734 -4.3343921 -4.335722 -4.33727 -4.3383532 -4.3381872 -4.3369021 -4.3350034 -4.3329129 -4.3312821 -4.3305068][-4.3309813 -4.3286924 -4.326798 -4.3252931 -4.3239985 -4.3229637 -4.3231225 -4.3240986 -4.32533 -4.3267193 -4.3275704 -4.3272624 -4.3260441 -4.3259535 -4.3270946]]...]
INFO - root - 2017-12-06 01:21:28.814988: step 57410, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.884 sec/batch; 67h:32m:36s remains)
INFO - root - 2017-12-06 01:21:37.802785: step 57420, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.924 sec/batch; 70h:34m:20s remains)
INFO - root - 2017-12-06 01:21:47.125066: step 57430, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 70h:12m:07s remains)
INFO - root - 2017-12-06 01:21:56.322381: step 57440, loss = 2.04, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 70h:12m:27s remains)
INFO - root - 2017-12-06 01:22:05.546848: step 57450, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 71h:46m:08s remains)
INFO - root - 2017-12-06 01:22:14.590417: step 57460, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 68h:12m:25s remains)
INFO - root - 2017-12-06 01:22:23.620617: step 57470, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 67h:37m:56s remains)
INFO - root - 2017-12-06 01:22:32.817006: step 57480, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 70h:43m:20s remains)
INFO - root - 2017-12-06 01:22:41.953494: step 57490, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.940 sec/batch; 71h:46m:53s remains)
INFO - root - 2017-12-06 01:22:50.941242: step 57500, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.914 sec/batch; 69h:50m:02s remains)
2017-12-06 01:22:51.761910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3048325 -4.3061142 -4.2934289 -4.2752123 -4.255734 -4.2186456 -4.1669397 -4.128973 -4.1198483 -4.1531348 -4.2118707 -4.26905 -4.3074017 -4.3334231 -4.342577][-4.3112116 -4.3070025 -4.2926092 -4.26857 -4.2433686 -4.1984086 -4.1395736 -4.1014056 -4.095109 -4.14394 -4.2146411 -4.2750244 -4.3109345 -4.3326554 -4.3386784][-4.303277 -4.297051 -4.2838345 -4.2582836 -4.2298574 -4.1804657 -4.1194749 -4.0846882 -4.0877061 -4.1426544 -4.2114325 -4.2638264 -4.29543 -4.3185406 -4.3280778][-4.2946739 -4.2854748 -4.2702236 -4.242909 -4.2114944 -4.15889 -4.1009831 -4.0752292 -4.0873208 -4.1398425 -4.1988506 -4.2430778 -4.27289 -4.2996874 -4.3160853][-4.2902732 -4.2758231 -4.2532153 -4.2198992 -4.1790729 -4.1167545 -4.064045 -4.043128 -4.0634203 -4.1173525 -4.1656752 -4.2082458 -4.2470098 -4.2848115 -4.3081074][-4.2851386 -4.2623138 -4.2259336 -4.1797113 -4.1245089 -4.0513492 -3.9998503 -3.9789038 -4.00434 -4.0637083 -4.1149149 -4.1688957 -4.2243924 -4.2737145 -4.3025689][-4.2727785 -4.2404847 -4.189889 -4.1284933 -4.0610332 -3.9805079 -3.9255366 -3.9019961 -3.9314976 -3.9982834 -4.062273 -4.1318135 -4.2031388 -4.2581224 -4.2888217][-4.2573524 -4.2139688 -4.1464853 -4.0681796 -3.9926972 -3.9148109 -3.8619308 -3.8415329 -3.8768196 -3.9494948 -4.0214696 -4.1027436 -4.17771 -4.2306452 -4.2596626][-4.2497325 -4.2012224 -4.1282635 -4.0437407 -3.9668214 -3.8983803 -3.8598409 -3.8538375 -3.8847632 -3.9452591 -4.0101781 -4.0860476 -4.1486139 -4.1904759 -4.2143536][-4.2537746 -4.2127023 -4.1488824 -4.0725503 -4.006248 -3.9576862 -3.942389 -3.9496288 -3.9649003 -3.9996111 -4.0471516 -4.1022787 -4.1393852 -4.1563249 -4.1586261][-4.2729216 -4.2516479 -4.213459 -4.1597862 -4.1151147 -4.085731 -4.0810618 -4.0871015 -4.0841651 -4.0904675 -4.1148596 -4.1424737 -4.1469345 -4.1341376 -4.10884][-4.2901292 -4.2896295 -4.2756634 -4.2434115 -4.2156358 -4.1998658 -4.1930375 -4.1916943 -4.1767964 -4.1613512 -4.1727195 -4.1873465 -4.1748796 -4.1387429 -4.0901823][-4.2827797 -4.2974982 -4.3008218 -4.2824745 -4.2602592 -4.2479115 -4.2366362 -4.2312794 -4.2128463 -4.1962528 -4.2099209 -4.2231779 -4.2083864 -4.1672997 -4.1081357][-4.2483397 -4.2698092 -4.2838812 -4.2727094 -4.2462835 -4.2298226 -4.2150159 -4.2122 -4.204741 -4.2048512 -4.2282014 -4.248601 -4.2400041 -4.2043257 -4.1472335][-4.2067432 -4.2361069 -4.2580733 -4.2477112 -4.213841 -4.1860495 -4.163434 -4.1605887 -4.1614285 -4.1820135 -4.2202153 -4.2510366 -4.2533488 -4.2270446 -4.1847119]]...]
INFO - root - 2017-12-06 01:23:00.952996: step 57510, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 72h:33m:57s remains)
INFO - root - 2017-12-06 01:23:10.131775: step 57520, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 69h:51m:14s remains)
INFO - root - 2017-12-06 01:23:19.095189: step 57530, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 69h:41m:18s remains)
INFO - root - 2017-12-06 01:23:28.275894: step 57540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 66h:13m:57s remains)
INFO - root - 2017-12-06 01:23:37.303550: step 57550, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 69h:39m:42s remains)
INFO - root - 2017-12-06 01:23:46.381007: step 57560, loss = 2.05, batch loss = 2.00 (8.6 examples/sec; 0.932 sec/batch; 71h:08m:34s remains)
INFO - root - 2017-12-06 01:23:55.641851: step 57570, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 70h:00m:37s remains)
INFO - root - 2017-12-06 01:24:04.675421: step 57580, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 70h:38m:40s remains)
INFO - root - 2017-12-06 01:24:13.574075: step 57590, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 68h:08m:14s remains)
INFO - root - 2017-12-06 01:24:22.646759: step 57600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 71h:38m:20s remains)
2017-12-06 01:24:23.430229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2430172 -4.2286549 -4.2202229 -4.2072091 -4.1948795 -4.1959848 -4.2047534 -4.2194629 -4.2283082 -4.2264762 -4.22389 -4.22952 -4.2413549 -4.2437286 -4.2342715][-4.2484226 -4.2363067 -4.2273307 -4.215941 -4.201067 -4.1958237 -4.1987934 -4.2108564 -4.2211542 -4.2221041 -4.2204814 -4.2253895 -4.2351079 -4.2342696 -4.2193694][-4.2518544 -4.239902 -4.2288356 -4.2140818 -4.1940331 -4.1818428 -4.1821618 -4.1946554 -4.21153 -4.2208657 -4.2227812 -4.2256675 -4.2328415 -4.2311625 -4.2126818][-4.2445221 -4.2288094 -4.2158351 -4.1989584 -4.172452 -4.1489778 -4.153419 -4.1767817 -4.2091856 -4.2313581 -4.2397346 -4.2399621 -4.2408347 -4.2350354 -4.2129297][-4.2402654 -4.2217879 -4.2066922 -4.1823936 -4.1416268 -4.0979834 -4.105217 -4.1489863 -4.197855 -4.2314649 -4.24176 -4.2380033 -4.2309971 -4.2194905 -4.1925287][-4.2425056 -4.2214546 -4.1980853 -4.1516709 -4.075314 -3.9952939 -4.0020618 -4.0769696 -4.1481233 -4.1918011 -4.2065911 -4.2050185 -4.1937308 -4.1794567 -4.150806][-4.2349916 -4.2013993 -4.1503506 -4.0598626 -3.9304318 -3.7858696 -3.7949772 -3.9264364 -4.0414305 -4.1107492 -4.1404219 -4.1498694 -4.1413393 -4.1266556 -4.1045766][-4.2272453 -4.1829076 -4.1098013 -3.9850118 -3.8198214 -3.6280975 -3.6387911 -3.8147755 -3.9600368 -4.0466337 -4.0862293 -4.1115227 -4.1136365 -4.1085229 -4.0955544][-4.2369852 -4.2066426 -4.1476912 -4.0401435 -3.915998 -3.7751286 -3.7719104 -3.890295 -3.9905539 -4.0527534 -4.0814042 -4.1128926 -4.1270347 -4.1367569 -4.1323433][-4.2504239 -4.2367496 -4.2048969 -4.1405015 -4.0785303 -4.0063114 -3.9888902 -4.0316606 -4.070466 -4.09303 -4.093647 -4.113265 -4.1359386 -4.157548 -4.1610374][-4.2489538 -4.2352047 -4.2126174 -4.1754112 -4.1518154 -4.1273851 -4.1093736 -4.1187353 -4.1285253 -4.1321754 -4.1155038 -4.1198316 -4.1394606 -4.1646023 -4.1760821][-4.2293429 -4.2115159 -4.189486 -4.1695991 -4.1681538 -4.1753764 -4.1660995 -4.1637549 -4.161849 -4.1543212 -4.1269422 -4.1196337 -4.134481 -4.1546712 -4.1670246][-4.2006059 -4.1789317 -4.1610675 -4.1556759 -4.1704011 -4.1961231 -4.1960592 -4.1862264 -4.1725507 -4.1512833 -4.1146293 -4.0932312 -4.100276 -4.114481 -4.123733][-4.1781898 -4.1519775 -4.1390543 -4.1422358 -4.1596642 -4.188457 -4.1993747 -4.1969304 -4.18206 -4.1557784 -4.1215982 -4.0959682 -4.0888548 -4.08737 -4.0872006][-4.1612616 -4.1305757 -4.1194196 -4.1264997 -4.1427841 -4.1709161 -4.1900654 -4.2004113 -4.1942639 -4.1780224 -4.1576071 -4.1408472 -4.1292415 -4.11418 -4.0977392]]...]
INFO - root - 2017-12-06 01:24:32.436130: step 57610, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.905 sec/batch; 69h:06m:28s remains)
INFO - root - 2017-12-06 01:24:41.426025: step 57620, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 0.800 sec/batch; 61h:07m:14s remains)
INFO - root - 2017-12-06 01:24:50.623713: step 57630, loss = 2.04, batch loss = 1.98 (8.6 examples/sec; 0.934 sec/batch; 71h:16m:47s remains)
INFO - root - 2017-12-06 01:24:59.639365: step 57640, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 67h:40m:15s remains)
INFO - root - 2017-12-06 01:25:08.737401: step 57650, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 70h:03m:49s remains)
INFO - root - 2017-12-06 01:25:17.905105: step 57660, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.918 sec/batch; 70h:03m:05s remains)
INFO - root - 2017-12-06 01:25:26.987366: step 57670, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 67h:40m:51s remains)
INFO - root - 2017-12-06 01:25:36.071474: step 57680, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 67h:40m:26s remains)
INFO - root - 2017-12-06 01:25:44.972027: step 57690, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 63h:44m:46s remains)
INFO - root - 2017-12-06 01:25:54.187867: step 57700, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 70h:08m:54s remains)
2017-12-06 01:25:54.975734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2490821 -4.2375832 -4.2221584 -4.2012525 -4.1745672 -4.1499434 -4.1353846 -4.1310682 -4.1438618 -4.165875 -4.1870809 -4.1966047 -4.1955876 -4.1979613 -4.2041664][-4.2286496 -4.2251835 -4.2175865 -4.2010736 -4.1709638 -4.1381373 -4.1206908 -4.1193624 -4.135571 -4.1607137 -4.1818075 -4.1847057 -4.1806641 -4.1853461 -4.1929526][-4.210309 -4.2117047 -4.2074547 -4.1935091 -4.1620097 -4.1247473 -4.1050115 -4.1067247 -4.12652 -4.15522 -4.1748691 -4.1798863 -4.1843648 -4.19733 -4.2076926][-4.198102 -4.1992359 -4.1943054 -4.1796861 -4.1471457 -4.106503 -4.090847 -4.1041818 -4.1296315 -4.1571269 -4.1770506 -4.1914616 -4.2093763 -4.2301559 -4.2415786][-4.1905446 -4.1910343 -4.1859117 -4.1705923 -4.1362777 -4.0967793 -4.0905681 -4.1140633 -4.1424007 -4.168437 -4.1901283 -4.215374 -4.2426062 -4.2637072 -4.2716346][-4.1851335 -4.1836119 -4.1787972 -4.1623583 -4.1299028 -4.0991316 -4.1002817 -4.1254177 -4.1499076 -4.1729579 -4.1992526 -4.2341237 -4.2660503 -4.2852516 -4.2884865][-4.1762753 -4.1736546 -4.1680169 -4.1501036 -4.1223617 -4.1028924 -4.1101484 -4.1319489 -4.1516905 -4.173533 -4.2036166 -4.24168 -4.2741032 -4.2884312 -4.2854233][-4.1542888 -4.147625 -4.1428785 -4.1320171 -4.1177788 -4.107985 -4.1132936 -4.1318345 -4.1524186 -4.174788 -4.2029634 -4.2381992 -4.2654161 -4.2729311 -4.260685][-4.1442552 -4.1345453 -4.1335998 -4.1325641 -4.1272955 -4.1179652 -4.1162276 -4.1291447 -4.148993 -4.1722746 -4.1989946 -4.2280803 -4.2467475 -4.2457156 -4.22689][-4.169313 -4.1582952 -4.1579175 -4.159934 -4.1568441 -4.1468596 -4.1406517 -4.1478062 -4.16378 -4.1855817 -4.2090521 -4.2299471 -4.2379136 -4.2292829 -4.2065411][-4.2131553 -4.2040429 -4.2028408 -4.2053533 -4.2050352 -4.1988535 -4.1919365 -4.1949005 -4.2063742 -4.2241588 -4.2429876 -4.2552409 -4.2539597 -4.2405467 -4.217402][-4.2652278 -4.258738 -4.2552872 -4.2558761 -4.2556872 -4.2519755 -4.2465119 -4.2470918 -4.2540393 -4.2660766 -4.2782722 -4.2838597 -4.2789783 -4.2655334 -4.2455006][-4.3003831 -4.2965546 -4.2925677 -4.2904429 -4.2884393 -4.2849178 -4.2814808 -4.2810383 -4.2831969 -4.2887144 -4.294878 -4.2967753 -4.2929597 -4.2842917 -4.270515][-4.3067269 -4.3061581 -4.3031955 -4.2999125 -4.2970638 -4.2935715 -4.2900362 -4.2873683 -4.2853317 -4.2859473 -4.2879119 -4.2877421 -4.2861142 -4.2836418 -4.2778625][-4.297543 -4.2997708 -4.2986622 -4.2961431 -4.2938838 -4.2897024 -4.2845349 -4.2796383 -4.2750869 -4.2726526 -4.2717009 -4.2690063 -4.2677484 -4.2673764 -4.2664781]]...]
INFO - root - 2017-12-06 01:26:04.079105: step 57710, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 65h:04m:15s remains)
INFO - root - 2017-12-06 01:26:13.073077: step 57720, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 68h:48m:30s remains)
INFO - root - 2017-12-06 01:26:22.248434: step 57730, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 71h:04m:54s remains)
INFO - root - 2017-12-06 01:26:31.464816: step 57740, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 68h:15m:06s remains)
INFO - root - 2017-12-06 01:26:40.601161: step 57750, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 69h:23m:34s remains)
INFO - root - 2017-12-06 01:26:49.673660: step 57760, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 67h:20m:39s remains)
INFO - root - 2017-12-06 01:26:58.948673: step 57770, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.924 sec/batch; 70h:29m:53s remains)
INFO - root - 2017-12-06 01:27:08.045424: step 57780, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.903 sec/batch; 68h:54m:31s remains)
INFO - root - 2017-12-06 01:27:17.227343: step 57790, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 70h:48m:49s remains)
INFO - root - 2017-12-06 01:27:26.202302: step 57800, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.918 sec/batch; 70h:01m:02s remains)
2017-12-06 01:27:27.060503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1736493 -4.2075081 -4.2300396 -4.2468166 -4.2524414 -4.2510505 -4.246243 -4.239687 -4.2308578 -4.2221336 -4.2307019 -4.2482443 -4.2712183 -4.2836862 -4.2873344][-4.1494379 -4.188148 -4.2178893 -4.2473488 -4.2657223 -4.2654061 -4.253108 -4.2388306 -4.2251177 -4.2167559 -4.2281256 -4.2513447 -4.2783232 -4.2898211 -4.291533][-4.1353378 -4.1766376 -4.2100835 -4.2483153 -4.27193 -4.2670765 -4.2463913 -4.2240238 -4.2119303 -4.2073627 -4.2213707 -4.2492757 -4.281322 -4.2923875 -4.2906137][-4.131341 -4.1682339 -4.1943212 -4.2289896 -4.2500868 -4.2387776 -4.2080216 -4.1824627 -4.1761775 -4.1794338 -4.198997 -4.2308164 -4.2700548 -4.2878909 -4.2877593][-4.1443682 -4.1753378 -4.1952519 -4.2210746 -4.2295742 -4.20587 -4.1653619 -4.141861 -4.1473541 -4.1599936 -4.1830463 -4.2159977 -4.256186 -4.2798409 -4.2861233][-4.153276 -4.1813879 -4.1993685 -4.2167225 -4.2097435 -4.1710038 -4.1224995 -4.1043167 -4.1299782 -4.1611423 -4.1906209 -4.2226892 -4.2557063 -4.2764916 -4.2873025][-4.1648474 -4.1907473 -4.2103519 -4.2196283 -4.2010713 -4.1507006 -4.0944848 -4.0797849 -4.1159997 -4.1645555 -4.2044554 -4.235878 -4.2622428 -4.277462 -4.2877355][-4.1824474 -4.20259 -4.21845 -4.2217875 -4.1978693 -4.1452017 -4.0896568 -4.0794635 -4.1178932 -4.1719952 -4.2176051 -4.2480335 -4.268682 -4.2786727 -4.2851677][-4.1781187 -4.1943693 -4.2068934 -4.2096214 -4.1906633 -4.1479726 -4.1027431 -4.0990853 -4.1336765 -4.1802111 -4.2245784 -4.2552013 -4.271666 -4.2758741 -4.2795448][-4.1775355 -4.1910052 -4.2047863 -4.212544 -4.2020321 -4.1671314 -4.1272812 -4.1230168 -4.1521249 -4.1922984 -4.2290974 -4.2585831 -4.2760878 -4.2786422 -4.2800021][-4.1896915 -4.1995535 -4.2104945 -4.2196288 -4.2148838 -4.1838236 -4.1415224 -4.1323133 -4.1583929 -4.1964293 -4.2295189 -4.2585311 -4.2793431 -4.2844887 -4.2862329][-4.2163529 -4.223393 -4.2330317 -4.2425308 -4.2415991 -4.2196307 -4.1781087 -4.1619291 -4.1820407 -4.2151866 -4.2430444 -4.267868 -4.2864113 -4.2930117 -4.297163][-4.2606049 -4.2649021 -4.2743077 -4.2819562 -4.2816992 -4.2674847 -4.2331634 -4.2135153 -4.2236123 -4.2480049 -4.269793 -4.2877135 -4.3013239 -4.3073082 -4.3108969][-4.2922897 -4.2947693 -4.3035703 -4.3092208 -4.3099251 -4.3014603 -4.277585 -4.2586336 -4.26129 -4.2783675 -4.2943764 -4.3058825 -4.3150678 -4.3195744 -4.3210545][-4.3029528 -4.30398 -4.311255 -4.3172836 -4.3207569 -4.3174663 -4.3025122 -4.2879558 -4.2866673 -4.2974977 -4.3095527 -4.3174839 -4.3219314 -4.3239274 -4.3236623]]...]
INFO - root - 2017-12-06 01:27:36.288745: step 57810, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 68h:41m:00s remains)
INFO - root - 2017-12-06 01:27:45.469143: step 57820, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 68h:53m:28s remains)
INFO - root - 2017-12-06 01:27:54.502360: step 57830, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 66h:57m:12s remains)
INFO - root - 2017-12-06 01:28:03.503570: step 57840, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 65h:46m:47s remains)
INFO - root - 2017-12-06 01:28:12.615870: step 57850, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 68h:40m:01s remains)
INFO - root - 2017-12-06 01:28:21.790830: step 57860, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.915 sec/batch; 69h:46m:55s remains)
INFO - root - 2017-12-06 01:28:30.804489: step 57870, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.927 sec/batch; 70h:41m:11s remains)
INFO - root - 2017-12-06 01:28:39.698595: step 57880, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 69h:55m:08s remains)
INFO - root - 2017-12-06 01:28:48.879704: step 57890, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 69h:34m:11s remains)
INFO - root - 2017-12-06 01:28:57.899460: step 57900, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.940 sec/batch; 71h:42m:42s remains)
2017-12-06 01:28:58.747353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1527 -4.145987 -4.1376581 -4.1664524 -4.1904769 -4.2036109 -4.2169557 -4.2102671 -4.1869559 -4.1511278 -4.1158762 -4.1012669 -4.1162796 -4.1285253 -4.1231222][-4.1728888 -4.1695638 -4.1604366 -4.1849642 -4.2110453 -4.2239738 -4.2310028 -4.2214165 -4.2005792 -4.1667981 -4.1338248 -4.1173515 -4.1272531 -4.1338568 -4.1288643][-4.1900463 -4.1814957 -4.1686931 -4.18603 -4.2129316 -4.2285514 -4.2340117 -4.2263451 -4.2121129 -4.1859827 -4.154583 -4.1324782 -4.1319032 -4.1324234 -4.1305003][-4.194612 -4.1767449 -4.1532021 -4.1594496 -4.1845737 -4.2084303 -4.2215347 -4.219563 -4.21208 -4.1925631 -4.1628418 -4.1392508 -4.1310415 -4.1252627 -4.1240053][-4.1889281 -4.1643891 -4.1318092 -4.1233487 -4.1445689 -4.17732 -4.2005806 -4.2056394 -4.2015548 -4.184031 -4.1557288 -4.137516 -4.1300764 -4.1231618 -4.1234312][-4.1793251 -4.1497259 -4.11274 -4.094789 -4.1096654 -4.1472421 -4.1800771 -4.1940384 -4.1937189 -4.1767554 -4.1513257 -4.1384153 -4.1347017 -4.1278696 -4.1309881][-4.1734233 -4.1449938 -4.1076045 -4.0891571 -4.0990973 -4.1331863 -4.1670842 -4.1876426 -4.1912045 -4.1737595 -4.1485696 -4.1352429 -4.1314759 -4.1285429 -4.1373205][-4.1700888 -4.1460447 -4.1158819 -4.1060829 -4.1091433 -4.1314054 -4.159842 -4.1830015 -4.1913552 -4.1748042 -4.1464906 -4.1281695 -4.1266232 -4.1323051 -4.1519918][-4.1638255 -4.144824 -4.1246004 -4.1228347 -4.1210165 -4.1289086 -4.1475739 -4.1703877 -4.1845684 -4.1697974 -4.1414056 -4.1222119 -4.1260033 -4.1409864 -4.1679211][-4.1593633 -4.1475625 -4.1324143 -4.1305294 -4.1270251 -4.1276011 -4.1355443 -4.1518335 -4.1663742 -4.1598911 -4.1380749 -4.1191254 -4.1184669 -4.1349778 -4.1650066][-4.1594639 -4.1528153 -4.1371908 -4.1339774 -4.1347251 -4.1365805 -4.1315708 -4.1347814 -4.14245 -4.1425505 -4.1344032 -4.1231313 -4.1175733 -4.12784 -4.1513362][-4.1661339 -4.1619244 -4.142314 -4.1351085 -4.1409178 -4.14513 -4.1344814 -4.1236544 -4.1195092 -4.124321 -4.1358557 -4.1400018 -4.137578 -4.1413293 -4.1536231][-4.1826744 -4.1783 -4.1548648 -4.1400318 -4.1462 -4.1550555 -4.1488004 -4.1322803 -4.115715 -4.1163392 -4.1395941 -4.1601143 -4.1676311 -4.1705589 -4.1759238][-4.207067 -4.1968708 -4.1682968 -4.1465321 -4.15415 -4.1715584 -4.17446 -4.1632371 -4.1424127 -4.1352344 -4.1547093 -4.1805158 -4.1963835 -4.2027874 -4.2039933][-4.2289367 -4.2173553 -4.1875868 -4.1650057 -4.1722126 -4.1931791 -4.2028522 -4.202282 -4.1867518 -4.1726108 -4.1826477 -4.202837 -4.2188754 -4.227562 -4.2286091]]...]
INFO - root - 2017-12-06 01:29:07.688529: step 57910, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.924 sec/batch; 70h:30m:46s remains)
INFO - root - 2017-12-06 01:29:16.827401: step 57920, loss = 2.11, batch loss = 2.05 (8.6 examples/sec; 0.933 sec/batch; 71h:11m:29s remains)
INFO - root - 2017-12-06 01:29:25.856296: step 57930, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 67h:21m:00s remains)
INFO - root - 2017-12-06 01:29:35.029756: step 57940, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.919 sec/batch; 70h:06m:36s remains)
INFO - root - 2017-12-06 01:29:44.263028: step 57950, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.904 sec/batch; 68h:54m:56s remains)
INFO - root - 2017-12-06 01:29:53.352311: step 57960, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 66h:54m:52s remains)
INFO - root - 2017-12-06 01:30:02.243033: step 57970, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 68h:06m:22s remains)
INFO - root - 2017-12-06 01:30:11.394376: step 57980, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 68h:56m:24s remains)
INFO - root - 2017-12-06 01:30:20.594426: step 57990, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.947 sec/batch; 72h:11m:01s remains)
INFO - root - 2017-12-06 01:30:29.516017: step 58000, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 68h:49m:11s remains)
2017-12-06 01:30:30.284980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2913733 -4.2920485 -4.2870321 -4.283957 -4.2858796 -4.2904882 -4.2978973 -4.3057261 -4.3121686 -4.3142905 -4.3167214 -4.3209405 -4.3254023 -4.3297582 -4.3350372][-4.2480574 -4.2462492 -4.2400293 -4.2406178 -4.2456994 -4.2520947 -4.2649126 -4.2804933 -4.29379 -4.29733 -4.2995977 -4.3037982 -4.3090587 -4.3158188 -4.3251882][-4.1898093 -4.1832991 -4.1758547 -4.1780295 -4.1865253 -4.1986985 -4.2216687 -4.2469268 -4.2664084 -4.272254 -4.2754788 -4.2810335 -4.2888618 -4.2992349 -4.3134623][-4.1329713 -4.1162376 -4.1007943 -4.0954823 -4.1012678 -4.1239734 -4.165494 -4.2047491 -4.2295623 -4.2367854 -4.24098 -4.2490993 -4.2623239 -4.2790279 -4.2991352][-4.0724449 -4.043241 -4.0164542 -4.0000911 -4.0031829 -4.0387859 -4.0993891 -4.1562772 -4.1921515 -4.2033725 -4.2066264 -4.21562 -4.2332215 -4.2560382 -4.2821307][-4.0141578 -3.9745481 -3.9327655 -3.8999887 -3.9016953 -3.9491959 -4.0265393 -4.0983329 -4.1449795 -4.1634965 -4.170259 -4.183136 -4.2066803 -4.2364111 -4.2691727][-3.9835761 -3.9350672 -3.884762 -3.8477144 -3.8526597 -3.907407 -3.9918151 -4.0637007 -4.108304 -4.126893 -4.1346731 -4.1526833 -4.1838703 -4.2229233 -4.2637806][-3.983603 -3.9422576 -3.8977683 -3.8654263 -3.871397 -3.9189706 -3.9914732 -4.0558286 -4.0960269 -4.1154761 -4.1250429 -4.1446748 -4.1783957 -4.2221737 -4.2680855][-4.0012136 -3.9885623 -3.9593453 -3.9349039 -3.932435 -3.9619451 -4.0147562 -4.0627971 -4.0947332 -4.1099524 -4.1192312 -4.1428957 -4.1786189 -4.227932 -4.276814][-3.9951982 -4.0112081 -4.0019684 -3.9930151 -3.9935231 -4.0075469 -4.0387187 -4.0708165 -4.0928903 -4.1018963 -4.1107035 -4.1400805 -4.1792741 -4.23141 -4.2809153][-3.9398921 -3.9647646 -3.9700582 -3.9786015 -3.9895878 -4.0004692 -4.02362 -4.05031 -4.0701685 -4.0786338 -4.0908914 -4.1280937 -4.1763811 -4.2333593 -4.2817197][-3.89791 -3.9022706 -3.9076917 -3.9221163 -3.9427483 -3.9631038 -3.9987557 -4.0396347 -4.0667405 -4.0761733 -4.089076 -4.129838 -4.1835265 -4.2436452 -4.2891073][-3.9058621 -3.8888438 -3.8806977 -3.891614 -3.9174967 -3.9511814 -4.0083747 -4.069 -4.1048856 -4.110939 -4.1148734 -4.145834 -4.1941681 -4.2520809 -4.2960725][-3.9555545 -3.9339614 -3.9153256 -3.9204056 -3.9422088 -3.9841812 -4.0536089 -4.1199627 -4.1570873 -4.1616454 -4.1585631 -4.1789246 -4.2153268 -4.2647967 -4.3056312][-4.0034289 -3.9844692 -3.9699211 -3.978451 -4.0022931 -4.047617 -4.115087 -4.1716404 -4.19713 -4.1983595 -4.1917729 -4.2036877 -4.2329774 -4.2767467 -4.3154535]]...]
INFO - root - 2017-12-06 01:30:39.404760: step 58010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 69h:48m:31s remains)
INFO - root - 2017-12-06 01:30:48.497750: step 58020, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.899 sec/batch; 68h:31m:08s remains)
INFO - root - 2017-12-06 01:30:57.551905: step 58030, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.908 sec/batch; 69h:13m:23s remains)
INFO - root - 2017-12-06 01:31:06.672908: step 58040, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.906 sec/batch; 69h:02m:21s remains)
INFO - root - 2017-12-06 01:31:15.868138: step 58050, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 68h:03m:01s remains)
INFO - root - 2017-12-06 01:31:24.912809: step 58060, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 68h:36m:23s remains)
INFO - root - 2017-12-06 01:31:34.105693: step 58070, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 70h:41m:44s remains)
INFO - root - 2017-12-06 01:31:43.209389: step 58080, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 68h:17m:32s remains)
INFO - root - 2017-12-06 01:31:52.201388: step 58090, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 64h:16m:01s remains)
INFO - root - 2017-12-06 01:32:01.482904: step 58100, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.923 sec/batch; 70h:20m:49s remains)
2017-12-06 01:32:02.318356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1959529 -4.2436423 -4.2718887 -4.2727556 -4.239769 -4.1849456 -4.124404 -4.0809917 -4.0911217 -4.1316962 -4.1536689 -4.1357985 -4.097806 -4.0440755 -4.0051818][-4.1601825 -4.2270103 -4.2649641 -4.2687912 -4.2349072 -4.1849194 -4.1325345 -4.0936842 -4.0964165 -4.1328921 -4.1608086 -4.1499634 -4.1055665 -4.0345325 -3.9818039][-4.1209393 -4.197804 -4.2399879 -4.2403 -4.2047443 -4.1630855 -4.1283326 -4.101759 -4.09993 -4.1299162 -4.1636634 -4.1595669 -4.1089816 -4.021843 -3.9608788][-4.0870261 -4.1530328 -4.192678 -4.1944504 -4.1570239 -4.1239448 -4.105269 -4.0915618 -4.0915184 -4.1202583 -4.1612492 -4.1599445 -4.0989051 -4.0039139 -3.9452918][-4.0665755 -4.1144333 -4.1513028 -4.1521745 -4.1179261 -4.0984731 -4.0913095 -4.0891309 -4.1021008 -4.13651 -4.1772189 -4.1714315 -4.0991564 -4.0064945 -3.9609659][-4.0548739 -4.0867777 -4.1190677 -4.1173992 -4.0859551 -4.075624 -4.0744557 -4.0801749 -4.1043587 -4.1451273 -4.181489 -4.16869 -4.0913048 -4.0052638 -3.97712][-4.046886 -4.065309 -4.0916977 -4.0886416 -4.054255 -4.0385628 -4.029665 -4.0331368 -4.0618219 -4.1140227 -4.1584835 -4.142951 -4.0594959 -3.9810853 -3.9717984][-4.0372562 -4.0477357 -4.0699863 -4.0649686 -4.0307779 -4.009007 -3.9872024 -3.983947 -4.0120964 -4.0649529 -4.1104631 -4.0955176 -4.0131178 -3.9471209 -3.9538808][-4.057704 -4.0610023 -4.0790038 -4.0755272 -4.0524926 -4.0325837 -4.0022783 -3.9946234 -4.0155478 -4.061388 -4.0967393 -4.0783153 -4.0053167 -3.9552257 -3.9699037][-4.1090055 -4.1093521 -4.1207848 -4.1206107 -4.1100845 -4.0952897 -4.0653067 -4.0584383 -4.0750365 -4.1098371 -4.1309934 -4.1104488 -4.0533738 -4.0199561 -4.0327535][-4.1554685 -4.1523137 -4.1607895 -4.1624022 -4.1602316 -4.15502 -4.134706 -4.1372929 -4.1554456 -4.1773276 -4.1830883 -4.1606336 -4.1210775 -4.1031637 -4.1123958][-4.2018633 -4.1981311 -4.2005062 -4.1991525 -4.1977887 -4.1978936 -4.1888943 -4.1992488 -4.2211695 -4.2395496 -4.2390652 -4.21694 -4.1878252 -4.177803 -4.1825209][-4.2428794 -4.243073 -4.2454343 -4.2425671 -4.2385736 -4.237628 -4.2341061 -4.24596 -4.2653666 -4.2826171 -4.285737 -4.2715912 -4.2500219 -4.2407889 -4.2400174][-4.2823629 -4.283987 -4.2859435 -4.2860394 -4.2841535 -4.2839789 -4.2815552 -4.2879953 -4.2987237 -4.3103614 -4.3181481 -4.3127995 -4.2983727 -4.2891088 -4.2854266][-4.30515 -4.3078332 -4.3085103 -4.3082619 -4.3081856 -4.3093486 -4.30853 -4.3111291 -4.3143687 -4.3209677 -4.328723 -4.3290391 -4.321589 -4.3157539 -4.3126512]]...]
INFO - root - 2017-12-06 01:32:11.459863: step 58110, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.923 sec/batch; 70h:19m:12s remains)
INFO - root - 2017-12-06 01:32:20.610523: step 58120, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.931 sec/batch; 70h:55m:45s remains)
INFO - root - 2017-12-06 01:32:29.721412: step 58130, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 70h:45m:32s remains)
INFO - root - 2017-12-06 01:32:38.844990: step 58140, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.955 sec/batch; 72h:48m:22s remains)
INFO - root - 2017-12-06 01:32:48.048711: step 58150, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.930 sec/batch; 70h:52m:44s remains)
INFO - root - 2017-12-06 01:32:57.203832: step 58160, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 70h:22m:20s remains)
INFO - root - 2017-12-06 01:33:06.067211: step 58170, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 67h:04m:18s remains)
INFO - root - 2017-12-06 01:33:15.228471: step 58180, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 67h:58m:10s remains)
INFO - root - 2017-12-06 01:33:24.133173: step 58190, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 68h:39m:30s remains)
INFO - root - 2017-12-06 01:33:33.133256: step 58200, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.910 sec/batch; 69h:19m:12s remains)
2017-12-06 01:33:33.886854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1987953 -4.20853 -4.219327 -4.2359343 -4.2457223 -4.2429495 -4.2317104 -4.2200079 -4.2092509 -4.1982837 -4.190701 -4.198626 -4.2222071 -4.2517991 -4.2796][-4.1766233 -4.1907434 -4.2000761 -4.213366 -4.2228694 -4.2245693 -4.2224689 -4.2199812 -4.2163324 -4.210937 -4.2075095 -4.2187 -4.2446327 -4.2745714 -4.300139][-4.1588259 -4.1703739 -4.1727161 -4.1756148 -4.1797986 -4.18654 -4.1988349 -4.2122087 -4.2188759 -4.2196484 -4.21974 -4.2317958 -4.2538652 -4.2761393 -4.2955141][-4.1331463 -4.1418238 -4.140717 -4.1354613 -4.132525 -4.1383963 -4.1552067 -4.1787062 -4.1987734 -4.2118125 -4.2207279 -4.2341886 -4.2522163 -4.2662482 -4.2798204][-4.1076765 -4.1119919 -4.1126313 -4.1077971 -4.1029978 -4.1023941 -4.1064911 -4.12077 -4.1429076 -4.16602 -4.1903558 -4.2154646 -4.23917 -4.2526169 -4.265893][-4.100472 -4.0973229 -4.0960689 -4.0939522 -4.0919271 -4.083869 -4.0638566 -4.0481262 -4.0527773 -4.0777645 -4.1206546 -4.1693535 -4.2106833 -4.2353649 -4.2579026][-4.1269989 -4.1194367 -4.1142349 -4.1084175 -4.106019 -4.0920739 -4.0534868 -4.0071969 -3.9849818 -4.0021334 -4.0551381 -4.1229877 -4.17879 -4.2124953 -4.2439542][-4.1761627 -4.1761775 -4.1740537 -4.1664982 -4.1590052 -4.139359 -4.0982995 -4.0474944 -4.0119233 -4.0124025 -4.0541844 -4.11679 -4.1698403 -4.1997576 -4.2300739][-4.2035627 -4.2155771 -4.2179279 -4.21264 -4.2047453 -4.1885953 -4.1602144 -4.1255374 -4.0941792 -4.0806389 -4.0987668 -4.139183 -4.1760564 -4.1941581 -4.2178698][-4.2207537 -4.2446251 -4.2501273 -4.2466111 -4.239264 -4.2292075 -4.2162528 -4.1988916 -4.1773953 -4.1606259 -4.1623673 -4.1804695 -4.1982179 -4.2048635 -4.21964][-4.243207 -4.2732515 -4.2808609 -4.2805128 -4.2757759 -4.2703834 -4.2664909 -4.2588315 -4.2447853 -4.2314115 -4.2279987 -4.234107 -4.238071 -4.2375455 -4.2455897][-4.2494025 -4.279582 -4.2903819 -4.2964125 -4.2987223 -4.2994246 -4.3008847 -4.29799 -4.2884927 -4.2801495 -4.2783718 -4.2813663 -4.2818446 -4.2809105 -4.2860003][-4.2478342 -4.2713461 -4.282598 -4.2927642 -4.300487 -4.3061194 -4.3103185 -4.3091679 -4.3038049 -4.301394 -4.3049912 -4.3108516 -4.3147306 -4.3175 -4.3220854][-4.2584205 -4.2747846 -4.2827325 -4.29164 -4.2996216 -4.3059735 -4.3106966 -4.3107643 -4.309236 -4.311492 -4.3182964 -4.3267994 -4.3345313 -4.3398566 -4.3441772][-4.2708769 -4.2794213 -4.282156 -4.2859936 -4.2895937 -4.2925758 -4.2945943 -4.2945433 -4.2950249 -4.2992125 -4.3075671 -4.3179421 -4.3287244 -4.33803 -4.3451166]]...]
INFO - root - 2017-12-06 01:33:42.999261: step 58210, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.925 sec/batch; 70h:28m:23s remains)
INFO - root - 2017-12-06 01:33:52.225770: step 58220, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 68h:52m:03s remains)
INFO - root - 2017-12-06 01:34:01.568998: step 58230, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.940 sec/batch; 71h:34m:55s remains)
INFO - root - 2017-12-06 01:34:10.601665: step 58240, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.916 sec/batch; 69h:46m:21s remains)
INFO - root - 2017-12-06 01:34:19.584124: step 58250, loss = 2.04, batch loss = 1.99 (8.6 examples/sec; 0.933 sec/batch; 71h:03m:45s remains)
INFO - root - 2017-12-06 01:34:28.660359: step 58260, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 69h:53m:17s remains)
INFO - root - 2017-12-06 01:34:37.684393: step 58270, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 65h:40m:53s remains)
INFO - root - 2017-12-06 01:34:46.672446: step 58280, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.928 sec/batch; 70h:39m:27s remains)
INFO - root - 2017-12-06 01:34:55.602266: step 58290, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.909 sec/batch; 69h:13m:28s remains)
INFO - root - 2017-12-06 01:35:04.726965: step 58300, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 69h:02m:58s remains)
2017-12-06 01:35:05.549849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2531958 -4.2642794 -4.2655683 -4.2590818 -4.255825 -4.2545886 -4.25943 -4.2731862 -4.2896624 -4.2965493 -4.287364 -4.2746987 -4.2678523 -4.2681303 -4.274045][-4.1980376 -4.2124043 -4.2244639 -4.2289839 -4.2308221 -4.2304335 -4.2331705 -4.2450242 -4.2664747 -4.2830749 -4.2755289 -4.2636166 -4.259933 -4.2552476 -4.25741][-4.1412811 -4.1509762 -4.17412 -4.194128 -4.2057738 -4.209291 -4.2115145 -4.2184119 -4.2390513 -4.2639709 -4.2635851 -4.2525296 -4.248795 -4.2365966 -4.233233][-4.1034007 -4.1071444 -4.134624 -4.1638937 -4.178369 -4.1809044 -4.176825 -4.17261 -4.1916094 -4.2262063 -4.239934 -4.2358246 -4.2344122 -4.2198639 -4.213439][-4.0887518 -4.0853214 -4.1070232 -4.1387682 -4.1488094 -4.1387677 -4.1124268 -4.0868163 -4.1022725 -4.1547589 -4.1915073 -4.2068715 -4.2148252 -4.2032862 -4.1995354][-4.0965552 -4.0900574 -4.1046791 -4.1287384 -4.1273489 -4.094923 -4.0354638 -3.9701068 -3.9828846 -4.0661364 -4.1343155 -4.1686716 -4.1861672 -4.1828737 -4.1830416][-4.1124845 -4.1021061 -4.1057796 -4.1169071 -4.1049538 -4.0547018 -3.96122 -3.8422227 -3.839704 -3.9589381 -4.0584 -4.11083 -4.1413107 -4.1537366 -4.1601725][-4.1364694 -4.1152096 -4.1091218 -4.1156559 -4.1030078 -4.0585227 -3.9638958 -3.8183827 -3.7810798 -3.9025662 -4.0090928 -4.0700955 -4.1114578 -4.1351657 -4.1474614][-4.1560507 -4.1279807 -4.1164455 -4.1290932 -4.1303463 -4.1084671 -4.0494456 -3.9447646 -3.8896847 -3.949079 -4.0174108 -4.0653615 -4.1080732 -4.1369452 -4.1540308][-4.1613693 -4.128242 -4.1140022 -4.1370316 -4.1532159 -4.1525831 -4.1286821 -4.0716624 -4.0175438 -4.0195508 -4.0427232 -4.0703397 -4.1067095 -4.1340761 -4.1529131][-4.1745291 -4.1388373 -4.1215887 -4.1474409 -4.1729317 -4.1830497 -4.1778555 -4.1496787 -4.1066132 -4.0826015 -4.0768709 -4.0875845 -4.1072831 -4.1189461 -4.1296954][-4.1970477 -4.1663189 -4.146718 -4.1630306 -4.1857667 -4.2023425 -4.2058315 -4.1948023 -4.1652808 -4.1343522 -4.1145473 -4.1149306 -4.1190295 -4.1095662 -4.1043196][-4.214035 -4.19062 -4.1716528 -4.1768913 -4.1916046 -4.2102146 -4.2187743 -4.22081 -4.2100377 -4.1829944 -4.1587105 -4.1541429 -4.148963 -4.1252866 -4.1119761][-4.2240105 -4.2075653 -4.1926494 -4.1926026 -4.2029738 -4.2217779 -4.2336674 -4.2472968 -4.2528419 -4.232615 -4.2038231 -4.194562 -4.1867771 -4.1643128 -4.1547937][-4.2294984 -4.2224364 -4.2156625 -4.2152281 -4.2263689 -4.2429581 -4.256628 -4.2755175 -4.2857947 -4.2699156 -4.2384267 -4.2253175 -4.2185307 -4.2079849 -4.2124119]]...]
INFO - root - 2017-12-06 01:35:14.599163: step 58310, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.932 sec/batch; 70h:58m:28s remains)
INFO - root - 2017-12-06 01:35:23.568037: step 58320, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.919 sec/batch; 69h:59m:02s remains)
INFO - root - 2017-12-06 01:35:32.701766: step 58330, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.914 sec/batch; 69h:37m:42s remains)
INFO - root - 2017-12-06 01:35:41.814336: step 58340, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.921 sec/batch; 70h:06m:30s remains)
INFO - root - 2017-12-06 01:35:50.961169: step 58350, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:56m:48s remains)
INFO - root - 2017-12-06 01:36:00.170106: step 58360, loss = 2.04, batch loss = 1.99 (8.4 examples/sec; 0.956 sec/batch; 72h:49m:10s remains)
INFO - root - 2017-12-06 01:36:09.547585: step 58370, loss = 2.09, batch loss = 2.03 (8.6 examples/sec; 0.935 sec/batch; 71h:10m:31s remains)
INFO - root - 2017-12-06 01:36:18.634499: step 58380, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.934 sec/batch; 71h:07m:17s remains)
INFO - root - 2017-12-06 01:36:27.681577: step 58390, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 68h:38m:26s remains)
INFO - root - 2017-12-06 01:36:36.818590: step 58400, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.874 sec/batch; 66h:32m:37s remains)
2017-12-06 01:36:37.641229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1629243 -4.176137 -4.1914234 -4.1953239 -4.1846957 -4.1882377 -4.210041 -4.2216811 -4.2222352 -4.215076 -4.2048426 -4.1841612 -4.1642156 -4.1612945 -4.1827669][-4.1492653 -4.1651745 -4.1799636 -4.1832476 -4.171752 -4.1713781 -4.1925831 -4.2069831 -4.2075377 -4.198154 -4.1824932 -4.1605058 -4.1376691 -4.1372862 -4.1644788][-4.1273537 -4.144752 -4.1570768 -4.1588316 -4.1480589 -4.1462159 -4.1639276 -4.1800609 -4.1804972 -4.1715717 -4.1595387 -4.1388178 -4.1144161 -4.1141381 -4.1444416][-4.1306438 -4.14455 -4.1524487 -4.1488318 -4.1389451 -4.1386609 -4.1561885 -4.1730747 -4.1760821 -4.1704397 -4.1613822 -4.1451659 -4.1260667 -4.1280661 -4.15306][-4.1551175 -4.1593046 -4.1512241 -4.1290655 -4.1153569 -4.12209 -4.1485848 -4.1723266 -4.1799493 -4.1771121 -4.1696138 -4.160605 -4.1546426 -4.1617513 -4.1825318][-4.1541367 -4.1520939 -4.1300035 -4.0858512 -4.0582333 -4.0703926 -4.1122589 -4.1467738 -4.1568303 -4.1541224 -4.1484709 -4.1465006 -4.1493592 -4.1614628 -4.1843958][-4.1218462 -4.1235795 -4.097856 -4.0422316 -3.9995067 -4.007565 -4.0544672 -4.0995131 -4.1170864 -4.114655 -4.1065311 -4.1045222 -4.1118846 -4.1297889 -4.1593094][-4.096859 -4.111454 -4.0932946 -4.04297 -4.0003791 -3.99754 -4.0286827 -4.0663176 -4.0859537 -4.0838022 -4.0685482 -4.0620637 -4.0774512 -4.1045656 -4.1374044][-4.0913391 -4.1123896 -4.1019044 -4.064723 -4.0294847 -4.0194993 -4.0332189 -4.0598903 -4.0800023 -4.0772476 -4.0549569 -4.043612 -4.0670533 -4.1030016 -4.1314111][-4.0832429 -4.1022582 -4.0972657 -4.0726252 -4.0471296 -4.0350008 -4.0414214 -4.0610876 -4.0772753 -4.073215 -4.0476112 -4.0330548 -4.0602756 -4.1018138 -4.1260219][-4.0951405 -4.1093111 -4.1087513 -4.0976191 -4.0864224 -4.0801263 -4.0835576 -4.0947394 -4.1034579 -4.0973797 -4.0727181 -4.0552855 -4.0752306 -4.1130838 -4.1333714][-4.1356311 -4.1429906 -4.1446695 -4.140595 -4.1365576 -4.1334534 -4.1342745 -4.1394587 -4.1446314 -4.1410336 -4.1218963 -4.1056204 -4.1169167 -4.1468163 -4.1622934][-4.1870995 -4.1892004 -4.188828 -4.1847777 -4.181241 -4.1784172 -4.1777391 -4.1795979 -4.1830397 -4.1850138 -4.1775365 -4.1690774 -4.176537 -4.197197 -4.2102089][-4.236908 -4.2379375 -4.2379956 -4.2356639 -4.2327127 -4.2296386 -4.2270751 -4.2254224 -4.2262559 -4.2299452 -4.2305336 -4.2281222 -4.2320113 -4.2433491 -4.2527475][-4.2643852 -4.2653389 -4.2655272 -4.2646356 -4.2632074 -4.2613478 -4.2592154 -4.2573943 -4.2575221 -4.2605963 -4.2637515 -4.2642384 -4.263875 -4.2664404 -4.2699332]]...]
INFO - root - 2017-12-06 01:36:46.624855: step 58410, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.920 sec/batch; 70h:00m:56s remains)
INFO - root - 2017-12-06 01:36:55.819341: step 58420, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 69h:31m:13s remains)
INFO - root - 2017-12-06 01:37:04.955159: step 58430, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.898 sec/batch; 68h:23m:44s remains)
INFO - root - 2017-12-06 01:37:13.897257: step 58440, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 67h:15m:16s remains)
INFO - root - 2017-12-06 01:37:22.905705: step 58450, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 68h:04m:13s remains)
INFO - root - 2017-12-06 01:37:32.045319: step 58460, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 68h:18m:44s remains)
INFO - root - 2017-12-06 01:37:40.895988: step 58470, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.926 sec/batch; 70h:30m:50s remains)
INFO - root - 2017-12-06 01:37:49.908387: step 58480, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 68h:01m:12s remains)
INFO - root - 2017-12-06 01:37:58.936802: step 58490, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 68h:11m:54s remains)
INFO - root - 2017-12-06 01:38:08.028158: step 58500, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 71h:53m:40s remains)
2017-12-06 01:38:08.874390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2591672 -4.2656937 -4.2662039 -4.2614503 -4.2551579 -4.2519679 -4.2531862 -4.2595735 -4.2605743 -4.2610683 -4.2648287 -4.2754874 -4.2865915 -4.2880397 -4.2715325][-4.2656593 -4.2718558 -4.2686977 -4.25956 -4.250339 -4.2440581 -4.242249 -4.2486224 -4.2520313 -4.257298 -4.2657514 -4.2828684 -4.2974219 -4.2963891 -4.270648][-4.2725954 -4.2782431 -4.2724266 -4.2597532 -4.2475719 -4.2356253 -4.22831 -4.2316322 -4.2361326 -4.2485619 -4.2643361 -4.2895632 -4.3074446 -4.3043504 -4.2712846][-4.2785783 -4.2840638 -4.2792335 -4.2664447 -4.2527351 -4.234621 -4.2204485 -4.2178745 -4.2217941 -4.2396703 -4.260397 -4.290185 -4.3099194 -4.3028655 -4.2617235][-4.2713013 -4.2807589 -4.2822332 -4.2723246 -4.2574697 -4.2328987 -4.2093744 -4.1971312 -4.1980586 -4.2202764 -4.2460761 -4.2795877 -4.3026304 -4.2955656 -4.2514839][-4.2608204 -4.2790713 -4.2880993 -4.2806358 -4.2617979 -4.2279963 -4.1922507 -4.1700554 -4.1720586 -4.2010841 -4.2341404 -4.2715578 -4.3001008 -4.2980857 -4.2570782][-4.2535839 -4.279573 -4.2941446 -4.2869115 -4.2593174 -4.2121463 -4.1605458 -4.1270561 -4.1373935 -4.1801276 -4.2242689 -4.2670388 -4.2994027 -4.3033986 -4.2702456][-4.247611 -4.2764039 -4.2920961 -4.2824454 -4.2442918 -4.1799903 -4.1103988 -4.0685024 -4.0937104 -4.1566815 -4.2143722 -4.2619677 -4.2961783 -4.304821 -4.2811656][-4.2445569 -4.2700634 -4.2830367 -4.2703762 -4.2283916 -4.1627307 -4.0984077 -4.0673118 -4.1031966 -4.1687469 -4.2229543 -4.2654777 -4.2941537 -4.30208 -4.2840028][-4.2514687 -4.270237 -4.2792873 -4.2670174 -4.2318611 -4.1834774 -4.1428742 -4.129766 -4.1597681 -4.2048535 -4.2389808 -4.2695746 -4.2912455 -4.2990279 -4.2849331][-4.2514033 -4.2655306 -4.2734308 -4.2653575 -4.2409773 -4.2087035 -4.1856585 -4.1810737 -4.1988292 -4.222527 -4.2403207 -4.2618194 -4.2821522 -4.2932229 -4.2834058][-4.2489657 -4.2604117 -4.26682 -4.2641711 -4.251307 -4.2320395 -4.2166095 -4.2131691 -4.2214513 -4.2301021 -4.238349 -4.2533317 -4.2729168 -4.2861505 -4.2788072][-4.2466154 -4.2561812 -4.2619576 -4.2630339 -4.2573986 -4.2451906 -4.2330623 -4.227798 -4.2285295 -4.2266741 -4.2273779 -4.2398133 -4.2610078 -4.2762961 -4.272089][-4.24524 -4.253089 -4.2582588 -4.2596765 -4.2558608 -4.24497 -4.2328453 -4.2242012 -4.2189617 -4.2098045 -4.2047658 -4.218214 -4.2444916 -4.2660336 -4.2673512][-4.2521911 -4.2566662 -4.2576337 -4.25392 -4.2455268 -4.2310181 -4.2149949 -4.2006311 -4.1914835 -4.1823359 -4.1798315 -4.1997724 -4.2342958 -4.2639604 -4.2711062]]...]
INFO - root - 2017-12-06 01:38:17.794780: step 58510, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 67h:35m:24s remains)
INFO - root - 2017-12-06 01:38:26.770920: step 58520, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 71h:58m:30s remains)
INFO - root - 2017-12-06 01:38:35.812434: step 58530, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 0.810 sec/batch; 61h:39m:41s remains)
INFO - root - 2017-12-06 01:38:44.934234: step 58540, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.913 sec/batch; 69h:30m:21s remains)
INFO - root - 2017-12-06 01:38:54.150348: step 58550, loss = 2.04, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 71h:28m:45s remains)
INFO - root - 2017-12-06 01:39:03.035058: step 58560, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 68h:53m:34s remains)
INFO - root - 2017-12-06 01:39:12.133883: step 58570, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 68h:01m:18s remains)
INFO - root - 2017-12-06 01:39:21.354200: step 58580, loss = 2.08, batch loss = 2.02 (8.6 examples/sec; 0.934 sec/batch; 71h:03m:36s remains)
INFO - root - 2017-12-06 01:39:30.701901: step 58590, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.939 sec/batch; 71h:28m:55s remains)
INFO - root - 2017-12-06 01:39:39.894548: step 58600, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.906 sec/batch; 68h:56m:30s remains)
2017-12-06 01:39:40.657735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1882534 -4.2100353 -4.2231135 -4.2175727 -4.2113152 -4.211154 -4.197772 -4.1799917 -4.1730647 -4.1796455 -4.1823459 -4.1829019 -4.1979127 -4.2317924 -4.2674217][-4.1642604 -4.1851721 -4.1927776 -4.1857347 -4.1809955 -4.1806135 -4.171515 -4.1550689 -4.1471949 -4.1566286 -4.1607122 -4.1633625 -4.1773005 -4.2147164 -4.2571526][-4.1678677 -4.18349 -4.1832266 -4.1702056 -4.1657252 -4.1658788 -4.1596279 -4.1501236 -4.1480341 -4.1608691 -4.1643963 -4.1677055 -4.1788807 -4.2132168 -4.2545195][-4.1702347 -4.1757231 -4.1676559 -4.154398 -4.1483564 -4.1469646 -4.1374216 -4.1262169 -4.1340685 -4.1555686 -4.1638708 -4.1722994 -4.1831627 -4.2131257 -4.2520275][-4.171566 -4.1641316 -4.1535645 -4.1403956 -4.1261082 -4.1157126 -4.088316 -4.063592 -4.0863023 -4.12453 -4.1440392 -4.1632843 -4.1804771 -4.2100377 -4.2481384][-4.1743789 -4.1579356 -4.1458173 -4.1291065 -4.103344 -4.0723767 -4.0102539 -3.9625537 -4.003417 -4.0709929 -4.1109986 -4.1450944 -4.17188 -4.2048593 -4.2463059][-4.1842241 -4.1613188 -4.1416531 -4.1141453 -4.0694017 -4.0137591 -3.9211979 -3.8517077 -3.9224491 -4.0274854 -4.0880404 -4.1342134 -4.16915 -4.2082043 -4.2515311][-4.1963277 -4.1723409 -4.1499081 -4.1134467 -4.0566349 -3.9892979 -3.8925238 -3.8317006 -3.9214566 -4.0337362 -4.0955787 -4.1439633 -4.1825275 -4.2214818 -4.2604651][-4.1850848 -4.1594372 -4.1426864 -4.1112905 -4.0608945 -4.0070691 -3.937346 -3.8985431 -3.9733565 -4.0595546 -4.1097903 -4.1535106 -4.1905751 -4.2273731 -4.2623806][-4.188571 -4.1553087 -4.1387858 -4.1162472 -4.0792141 -4.0468335 -4.0094571 -3.9858103 -4.0359931 -4.1003714 -4.13953 -4.1761141 -4.2088904 -4.2396774 -4.2689033][-4.2160292 -4.1774416 -4.1593957 -4.1399879 -4.1092291 -4.0867367 -4.0681458 -4.0562105 -4.0899115 -4.1384559 -4.1700635 -4.20068 -4.2294297 -4.2548871 -4.2791834][-4.2457905 -4.2087212 -4.1837068 -4.1561947 -4.121769 -4.0991521 -4.0910273 -4.0939174 -4.1239758 -4.163929 -4.1923642 -4.2183723 -4.2424145 -4.2634335 -4.2849994][-4.2737055 -4.2452283 -4.217762 -4.178443 -4.1345553 -4.1123624 -4.1205082 -4.1384635 -4.172225 -4.2035565 -4.22595 -4.2454128 -4.2619648 -4.2766991 -4.2927723][-4.2830534 -4.2658072 -4.2414188 -4.2044735 -4.1700821 -4.1562133 -4.1737986 -4.1967626 -4.2259693 -4.2455716 -4.2576709 -4.2690616 -4.2785935 -4.2884555 -4.301743][-4.2864661 -4.280766 -4.2638221 -4.2375565 -4.2149649 -4.2068315 -4.2229204 -4.2430282 -4.2622104 -4.272007 -4.2762208 -4.2819915 -4.2899303 -4.2985144 -4.3123593]]...]
INFO - root - 2017-12-06 01:39:49.866049: step 58610, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.945 sec/batch; 71h:54m:17s remains)
INFO - root - 2017-12-06 01:39:58.960433: step 58620, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 68h:24m:52s remains)
INFO - root - 2017-12-06 01:40:08.067192: step 58630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 68h:48m:59s remains)
INFO - root - 2017-12-06 01:40:17.155830: step 58640, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.878 sec/batch; 66h:46m:00s remains)
INFO - root - 2017-12-06 01:40:26.255428: step 58650, loss = 2.03, batch loss = 1.98 (8.7 examples/sec; 0.923 sec/batch; 70h:10m:37s remains)
INFO - root - 2017-12-06 01:40:35.329019: step 58660, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 69h:33m:07s remains)
INFO - root - 2017-12-06 01:40:44.419699: step 58670, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 69h:48m:52s remains)
INFO - root - 2017-12-06 01:40:53.398714: step 58680, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.897 sec/batch; 68h:11m:42s remains)
INFO - root - 2017-12-06 01:41:02.581140: step 58690, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.938 sec/batch; 71h:18m:29s remains)
INFO - root - 2017-12-06 01:41:11.807204: step 58700, loss = 2.06, batch loss = 2.00 (7.8 examples/sec; 1.023 sec/batch; 77h:47m:44s remains)
2017-12-06 01:41:12.589366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1937604 -4.1913819 -4.1837606 -4.1720114 -4.1710448 -4.1723843 -4.1702051 -4.1605248 -4.1632009 -4.1860971 -4.2080817 -4.2208376 -4.2244911 -4.2328258 -4.2534833][-4.219615 -4.2208791 -4.2120862 -4.1949139 -4.1923776 -4.1961737 -4.1938109 -4.1823163 -4.1815963 -4.2011681 -4.2237124 -4.2345562 -4.2309837 -4.2363858 -4.2566104][-4.2289748 -4.2359357 -4.2283597 -4.2100921 -4.2019343 -4.2006168 -4.1939983 -4.1812844 -4.1797171 -4.2006168 -4.2329931 -4.2508535 -4.247509 -4.2535076 -4.2745776][-4.2000661 -4.2077885 -4.2031846 -4.1891141 -4.1784768 -4.1690822 -4.1540127 -4.1361871 -4.1361942 -4.1669154 -4.2165141 -4.2494307 -4.2551332 -4.2627683 -4.2834687][-4.155303 -4.1627731 -4.1644082 -4.154932 -4.1400537 -4.115355 -4.0846367 -4.0573449 -4.0567789 -4.1018252 -4.1720209 -4.2208004 -4.2351708 -4.2501636 -4.2766404][-4.1338096 -4.13859 -4.1441584 -4.1380177 -4.11732 -4.0793257 -4.0357647 -3.9943359 -3.9811466 -4.0344057 -4.1213007 -4.1838217 -4.2102752 -4.2351327 -4.2690492][-4.1537204 -4.1552525 -4.1577978 -4.1510911 -4.130784 -4.0951934 -4.0593929 -4.0170093 -3.99403 -4.0384951 -4.1187053 -4.1787238 -4.2085257 -4.2370796 -4.2730651][-4.1947193 -4.1940608 -4.1891937 -4.1783986 -4.1592631 -4.1351151 -4.1167989 -4.0892253 -4.0745878 -4.1105762 -4.1721525 -4.21308 -4.2332892 -4.2556543 -4.2850676][-4.2157831 -4.2158923 -4.2065229 -4.1928649 -4.1797056 -4.1688504 -4.1617446 -4.1481419 -4.1466546 -4.1799197 -4.2274613 -4.2528605 -4.2652931 -4.2800589 -4.3007288][-4.2086849 -4.208405 -4.1982889 -4.1858377 -4.1778121 -4.1722264 -4.1675348 -4.1623888 -4.1700783 -4.2020812 -4.2416887 -4.2632952 -4.2751522 -4.2880445 -4.3048377][-4.1891961 -4.1859488 -4.1759682 -4.1655016 -4.1575055 -4.1496506 -4.1386356 -4.12811 -4.1368809 -4.172 -4.2074237 -4.22855 -4.2442608 -4.2624025 -4.285605][-4.1924825 -4.1834168 -4.1717372 -4.1585565 -4.1421118 -4.1238065 -4.1019397 -4.0835824 -4.0933166 -4.1303425 -4.1638145 -4.1849723 -4.2040272 -4.2285581 -4.2604723][-4.2199278 -4.2069554 -4.1938763 -4.1763148 -4.1514421 -4.124918 -4.0988712 -4.0840039 -4.0940166 -4.1223049 -4.1459317 -4.1623387 -4.1795292 -4.2044106 -4.2424583][-4.2351551 -4.2209969 -4.2083836 -4.1883373 -4.1592889 -4.1335297 -4.1203613 -4.118104 -4.1274881 -4.1413727 -4.151463 -4.1590867 -4.1702271 -4.1924429 -4.2343011][-4.2269597 -4.2122765 -4.1984115 -4.1756 -4.1451058 -4.1321454 -4.137104 -4.1489096 -4.1606421 -4.1670508 -4.1679444 -4.1682816 -4.1752005 -4.1962309 -4.2381883]]...]
INFO - root - 2017-12-06 01:41:21.585597: step 58710, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.914 sec/batch; 69h:32m:42s remains)
INFO - root - 2017-12-06 01:41:30.451934: step 58720, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 66h:33m:44s remains)
INFO - root - 2017-12-06 01:41:39.703937: step 58730, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 69h:13m:40s remains)
INFO - root - 2017-12-06 01:41:48.715839: step 58740, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 68h:35m:01s remains)
INFO - root - 2017-12-06 01:41:57.721178: step 58750, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 67h:50m:18s remains)
INFO - root - 2017-12-06 01:42:06.830743: step 58760, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.912 sec/batch; 69h:22m:59s remains)
INFO - root - 2017-12-06 01:42:15.834280: step 58770, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.922 sec/batch; 70h:07m:46s remains)
INFO - root - 2017-12-06 01:42:24.864938: step 58780, loss = 2.02, batch loss = 1.96 (8.9 examples/sec; 0.902 sec/batch; 68h:33m:56s remains)
INFO - root - 2017-12-06 01:42:33.960173: step 58790, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.927 sec/batch; 70h:26m:59s remains)
INFO - root - 2017-12-06 01:42:43.243850: step 58800, loss = 2.07, batch loss = 2.02 (8.6 examples/sec; 0.932 sec/batch; 70h:50m:35s remains)
2017-12-06 01:42:44.046057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1999063 -4.2038741 -4.203898 -4.1980405 -4.19338 -4.1780744 -4.1619773 -4.1519051 -4.1453705 -4.1549549 -4.1667638 -4.1713443 -4.1721077 -4.1696029 -4.1521482][-4.1976824 -4.2027445 -4.1958885 -4.1798549 -4.1659846 -4.1459379 -4.1322851 -4.12572 -4.1200418 -4.1301193 -4.1452885 -4.1540647 -4.1538339 -4.1492748 -4.1343956][-4.2230353 -4.2270737 -4.2177815 -4.2004957 -4.1781068 -4.1503778 -4.136601 -4.1290503 -4.1245265 -4.1367617 -4.1524191 -4.1656971 -4.1686225 -4.1620369 -4.152276][-4.2584052 -4.2599816 -4.2556281 -4.2459192 -4.2212806 -4.1874895 -4.164052 -4.1512165 -4.1508007 -4.1697035 -4.1848836 -4.2003603 -4.2048926 -4.1958594 -4.1883836][-4.2756968 -4.2751241 -4.27055 -4.2640505 -4.2423 -4.2040238 -4.1664696 -4.14817 -4.1602106 -4.1923456 -4.2141337 -4.2314639 -4.2367549 -4.2301445 -4.2326961][-4.2634358 -4.2584071 -4.2450533 -4.2307582 -4.2073889 -4.1607842 -4.1035724 -4.0820189 -4.118516 -4.1775517 -4.2175236 -4.2406073 -4.2521276 -4.2500129 -4.2577562][-4.2309947 -4.2203264 -4.1951118 -4.1694965 -4.1378431 -4.06996 -3.977905 -3.9485497 -4.0219116 -4.12222 -4.1917849 -4.2344551 -4.2576675 -4.26074 -4.2670341][-4.2010736 -4.1893058 -4.1640234 -4.1402669 -4.1079116 -4.0286551 -3.9167302 -3.8720632 -3.9563785 -4.0797715 -4.1695256 -4.2267189 -4.2622681 -4.2752771 -4.2806935][-4.2001948 -4.1875253 -4.1677828 -4.1549153 -4.1357174 -4.0772886 -3.9962602 -3.9557393 -4.0087891 -4.1045909 -4.1804662 -4.231586 -4.2668285 -4.2825408 -4.2862959][-4.219655 -4.2074604 -4.1956882 -4.1930337 -4.185667 -4.1570187 -4.1174245 -4.0942965 -4.1184278 -4.1737571 -4.2251487 -4.2589421 -4.2797656 -4.2853661 -4.2794356][-4.2414784 -4.2334557 -4.2273626 -4.2287679 -4.2262516 -4.2158189 -4.2051854 -4.1995492 -4.2074752 -4.23104 -4.26208 -4.2845654 -4.2937112 -4.2886534 -4.2727642][-4.2547894 -4.2509356 -4.2461181 -4.247468 -4.2455335 -4.2405524 -4.2435627 -4.2467103 -4.2486281 -4.2527571 -4.26983 -4.2912817 -4.2982135 -4.2897873 -4.2716231][-4.2575107 -4.2556686 -4.25107 -4.2522016 -4.2484388 -4.2416635 -4.2449064 -4.2480164 -4.2486482 -4.2447109 -4.2504625 -4.2709055 -4.2819772 -4.2788134 -4.2620997][-4.2447414 -4.2395954 -4.2357531 -4.240665 -4.2427998 -4.2378712 -4.2337022 -4.2320018 -4.233243 -4.2296705 -4.2310143 -4.2507691 -4.2646809 -4.266448 -4.2485571][-4.2295132 -4.2170863 -4.2102566 -4.220665 -4.2288384 -4.2272348 -4.221046 -4.2181759 -4.2242179 -4.2248726 -4.2261291 -4.2438831 -4.252346 -4.2516108 -4.2353067]]...]
INFO - root - 2017-12-06 01:42:53.180075: step 58810, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 66h:11m:01s remains)
INFO - root - 2017-12-06 01:43:02.045000: step 58820, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 67h:41m:11s remains)
INFO - root - 2017-12-06 01:43:11.001349: step 58830, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 68h:35m:22s remains)
INFO - root - 2017-12-06 01:43:20.206797: step 58840, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 70h:06m:54s remains)
INFO - root - 2017-12-06 01:43:29.482700: step 58850, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 71h:58m:00s remains)
INFO - root - 2017-12-06 01:43:38.489876: step 58860, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 68h:37m:41s remains)
INFO - root - 2017-12-06 01:43:47.515978: step 58870, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 66h:37m:10s remains)
INFO - root - 2017-12-06 01:43:56.759922: step 58880, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.935 sec/batch; 71h:01m:50s remains)
INFO - root - 2017-12-06 01:44:05.911906: step 58890, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.954 sec/batch; 72h:30m:08s remains)
INFO - root - 2017-12-06 01:44:14.942640: step 58900, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 65h:34m:48s remains)
2017-12-06 01:44:15.803350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3073006 -4.293839 -4.2726312 -4.2472167 -4.2242174 -4.1993837 -4.17361 -4.1564722 -4.1535912 -4.1656718 -4.1922145 -4.2278643 -4.258019 -4.2799425 -4.2967386][-4.2879443 -4.2658515 -4.2320681 -4.1925831 -4.1552587 -4.1159239 -4.0753655 -4.047873 -4.0457392 -4.065547 -4.1067553 -4.1594367 -4.2040105 -4.2358508 -4.2621388][-4.2674994 -4.2367368 -4.1898732 -4.1371803 -4.0840921 -4.0235438 -3.9648569 -3.9302945 -3.9356787 -3.9704254 -4.0291615 -4.0943828 -4.145052 -4.1848536 -4.2216682][-4.250669 -4.2131939 -4.15889 -4.0943584 -4.0214429 -3.9349451 -3.8618052 -3.8353384 -3.86291 -3.917562 -3.9860022 -4.0500493 -4.1004453 -4.1461577 -4.1922836][-4.2338085 -4.1917953 -4.1314073 -4.0542269 -3.9602385 -3.8506432 -3.7800817 -3.7842293 -3.8439269 -3.9169128 -3.9857972 -4.0400109 -4.0851893 -4.1327424 -4.184341][-4.2192116 -4.1737556 -4.1107392 -4.0256262 -3.9211602 -3.8091066 -3.7582643 -3.7956541 -3.8794997 -3.9588928 -4.0202475 -4.0647426 -4.1016626 -4.1452441 -4.1956806][-4.2157755 -4.1695361 -4.1096215 -4.0248652 -3.9217825 -3.8249953 -3.7971539 -3.8528886 -3.9466791 -4.0238276 -4.0752282 -4.1091871 -4.1355262 -4.1703696 -4.2160268][-4.2201996 -4.1772346 -4.1219106 -4.0397048 -3.9405382 -3.8547442 -3.8375885 -3.9004226 -4.002264 -4.0837488 -4.1302557 -4.1541538 -4.1705608 -4.1968484 -4.2364073][-4.2235956 -4.1880198 -4.1385412 -4.06309 -3.9700928 -3.8854856 -3.8647079 -3.9286246 -4.03715 -4.1258163 -4.1685529 -4.1853061 -4.1944432 -4.2119417 -4.2425709][-4.2264352 -4.1970115 -4.1537967 -4.0863605 -3.9995174 -3.9192581 -3.9015372 -3.9692407 -4.0729833 -4.1550884 -4.1937885 -4.2035236 -4.2029381 -4.2104287 -4.2335405][-4.2332549 -4.2087731 -4.171463 -4.1151452 -4.0405831 -3.9755986 -3.9655113 -4.0277448 -4.111454 -4.1800156 -4.2131276 -4.2150145 -4.204484 -4.2047472 -4.2251506][-4.2422266 -4.2212567 -4.1916218 -4.151175 -4.0987425 -4.0559845 -4.0513453 -4.0992465 -4.160696 -4.2147079 -4.2374735 -4.229641 -4.209938 -4.2053223 -4.2255754][-4.2575917 -4.2415929 -4.2205319 -4.1949162 -4.1629982 -4.13856 -4.1395445 -4.1758728 -4.2184815 -4.2542815 -4.2643685 -4.2455993 -4.2203155 -4.2162595 -4.2380166][-4.27987 -4.2685871 -4.2564735 -4.2432513 -4.22817 -4.2157669 -4.2200136 -4.2439346 -4.2679558 -4.2877116 -4.28766 -4.2644677 -4.2406716 -4.2406363 -4.263413][-4.3012719 -4.2937975 -4.2911921 -4.2902441 -4.2874751 -4.2837086 -4.2850418 -4.2957816 -4.3035793 -4.3106327 -4.3061976 -4.2868638 -4.2691517 -4.273303 -4.2961078]]...]
INFO - root - 2017-12-06 01:44:24.726782: step 58910, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 70h:09m:21s remains)
INFO - root - 2017-12-06 01:44:33.848186: step 58920, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.906 sec/batch; 68h:49m:50s remains)
INFO - root - 2017-12-06 01:44:42.852445: step 58930, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 66h:02m:51s remains)
INFO - root - 2017-12-06 01:44:51.805653: step 58940, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.917 sec/batch; 69h:42m:27s remains)
INFO - root - 2017-12-06 01:45:00.861377: step 58950, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 67h:12m:40s remains)
INFO - root - 2017-12-06 01:45:10.031026: step 58960, loss = 2.05, batch loss = 2.00 (8.7 examples/sec; 0.923 sec/batch; 70h:09m:18s remains)
INFO - root - 2017-12-06 01:45:18.997660: step 58970, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 68h:35m:52s remains)
INFO - root - 2017-12-06 01:45:28.101440: step 58980, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 69h:12m:14s remains)
INFO - root - 2017-12-06 01:45:37.215851: step 58990, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 67h:53m:20s remains)
INFO - root - 2017-12-06 01:45:46.286915: step 59000, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 70h:15m:17s remains)
2017-12-06 01:45:47.042524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1294346 -4.1346703 -4.1249228 -4.1181688 -4.1180482 -4.1202488 -4.113636 -4.0985894 -4.1027846 -4.1221991 -4.1442947 -4.1586847 -4.1458092 -4.1214986 -4.1170979][-4.1453342 -4.1413517 -4.13261 -4.1295085 -4.1283917 -4.1327624 -4.1317148 -4.1157823 -4.1083617 -4.1155009 -4.1337433 -4.154304 -4.1515775 -4.1372094 -4.1391082][-4.1592946 -4.1521583 -4.1416745 -4.14082 -4.1431808 -4.1519961 -4.1622748 -4.1592989 -4.1425743 -4.1243982 -4.1202621 -4.1308551 -4.1353936 -4.1382432 -4.1524644][-4.1562696 -4.1498637 -4.1397414 -4.1408606 -4.1441393 -4.1547122 -4.1780086 -4.1908317 -4.1750669 -4.1381736 -4.1123614 -4.1095285 -4.1149564 -4.1301618 -4.1462817][-4.135973 -4.1302853 -4.1241994 -4.125062 -4.1228185 -4.1246948 -4.1537485 -4.1840248 -4.1817207 -4.1489367 -4.1185675 -4.1084986 -4.1091938 -4.1195207 -4.1296158][-4.1141276 -4.1140847 -4.1075559 -4.0964231 -4.06925 -4.0459843 -4.0783687 -4.134119 -4.1590395 -4.149529 -4.130444 -4.1159334 -4.1050105 -4.1032777 -4.1091313][-4.1083074 -4.112361 -4.098371 -4.0642214 -3.9962728 -3.9365163 -3.9865372 -4.079752 -4.1271825 -4.13933 -4.1329465 -4.1158128 -4.0966024 -4.0903339 -4.1023498][-4.1225142 -4.1266046 -4.1090641 -4.0654621 -3.976722 -3.9032416 -3.9645903 -4.0614781 -4.1056185 -4.120563 -4.1170883 -4.103025 -4.08614 -4.0824237 -4.0999727][-4.1572604 -4.1620116 -4.15428 -4.1249132 -4.0605941 -4.0135651 -4.0475159 -4.0897994 -4.092679 -4.0882893 -4.0857038 -4.0814095 -4.0716152 -4.0672021 -4.077426][-4.1915245 -4.1890507 -4.1876769 -4.1758075 -4.139123 -4.1162663 -4.1235404 -4.11477 -4.0844359 -4.068727 -4.0773511 -4.0883145 -4.0795956 -4.0567703 -4.04683][-4.1976614 -4.18296 -4.1853986 -4.1901836 -4.1754236 -4.1653109 -4.1637344 -4.1369553 -4.1049633 -4.0949974 -4.1087551 -4.1195092 -4.0948734 -4.0526309 -4.0281949][-4.1756482 -4.1495442 -4.156558 -4.1791039 -4.1807714 -4.1788936 -4.1809483 -4.1601467 -4.1384416 -4.1323051 -4.1393538 -4.1376486 -4.1020546 -4.0649452 -4.0534539][-4.1577511 -4.1237006 -4.1224251 -4.1459837 -4.1553459 -4.1632977 -4.1755981 -4.1640658 -4.1435657 -4.1351876 -4.1382647 -4.1354609 -4.1054311 -4.0872364 -4.0859971][-4.1584182 -4.1276 -4.1100793 -4.11814 -4.1257472 -4.1383405 -4.1513004 -4.1392837 -4.1155314 -4.1072307 -4.1145005 -4.1193428 -4.1040978 -4.096066 -4.0913558][-4.1685591 -4.1488414 -4.1250615 -4.1179209 -4.1194077 -4.1294231 -4.1303921 -4.112988 -4.0948324 -4.0976772 -4.1148758 -4.1224651 -4.1086383 -4.0900936 -4.0746861]]...]
INFO - root - 2017-12-06 01:45:56.012436: step 59010, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 67h:23m:46s remains)
INFO - root - 2017-12-06 01:46:05.025337: step 59020, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 69h:38m:36s remains)
INFO - root - 2017-12-06 01:46:13.877696: step 59030, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.910 sec/batch; 69h:07m:16s remains)
INFO - root - 2017-12-06 01:46:23.107009: step 59040, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 69h:42m:43s remains)
INFO - root - 2017-12-06 01:46:32.139288: step 59050, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 68h:33m:29s remains)
INFO - root - 2017-12-06 01:46:41.145087: step 59060, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 67h:52m:47s remains)
INFO - root - 2017-12-06 01:46:50.267517: step 59070, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 67h:13m:40s remains)
INFO - root - 2017-12-06 01:46:59.389661: step 59080, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 68h:35m:25s remains)
INFO - root - 2017-12-06 01:47:08.381268: step 59090, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.915 sec/batch; 69h:29m:28s remains)
INFO - root - 2017-12-06 01:47:17.412844: step 59100, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 68h:13m:43s remains)
2017-12-06 01:47:18.206211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2081761 -4.212563 -4.2242689 -4.235992 -4.2302861 -4.2112789 -4.1957555 -4.1930408 -4.1963873 -4.2045302 -4.2100596 -4.2092829 -4.2089067 -4.2144976 -4.2260609][-4.2150478 -4.2210522 -4.224381 -4.2262764 -4.2133565 -4.1890697 -4.1703615 -4.1705694 -4.1799173 -4.1940045 -4.2023773 -4.1982594 -4.1921215 -4.1938219 -4.2105365][-4.2329745 -4.2405767 -4.2412968 -4.2370062 -4.2234221 -4.2044678 -4.1922097 -4.1927128 -4.1972518 -4.2062244 -4.2141128 -4.2095046 -4.1991796 -4.1974792 -4.2112389][-4.2478094 -4.2549343 -4.2613506 -4.2629423 -4.258285 -4.2432008 -4.22897 -4.2219391 -4.215703 -4.2170453 -4.2242546 -4.2255931 -4.2193422 -4.2172618 -4.2281132][-4.2481165 -4.253098 -4.2660317 -4.2760077 -4.2780347 -4.265018 -4.2415042 -4.2182984 -4.1991057 -4.2006783 -4.2104492 -4.2201571 -4.2302837 -4.24194 -4.2547846][-4.231441 -4.2335758 -4.249186 -4.2619686 -4.2619805 -4.2437429 -4.2068954 -4.1653595 -4.1377611 -4.1490436 -4.1664405 -4.1850572 -4.2160549 -4.2473092 -4.2660356][-4.2025247 -4.2037535 -4.2158189 -4.2261629 -4.2225962 -4.1962681 -4.1487513 -4.092886 -4.0620966 -4.0867677 -4.1166039 -4.1424365 -4.1862731 -4.2276273 -4.2484202][-4.1635604 -4.159977 -4.1675262 -4.1771555 -4.173532 -4.1449852 -4.1009669 -4.0501952 -4.0190487 -4.0419436 -4.0751553 -4.1022973 -4.1423697 -4.1803427 -4.1980414][-4.1294322 -4.1177583 -4.1183271 -4.1252003 -4.1246176 -4.1072707 -4.0796976 -4.0461664 -4.0195308 -4.028131 -4.054019 -4.0751905 -4.1005979 -4.126102 -4.1354065][-4.1137123 -4.0928316 -4.0891242 -4.0954127 -4.1009474 -4.0983653 -4.0854831 -4.0673079 -4.0478396 -4.0442533 -4.0573583 -4.0688691 -4.0747929 -4.078651 -4.0765114][-4.1376696 -4.1127028 -4.1065483 -4.1121769 -4.1232386 -4.130435 -4.1261616 -4.1151409 -4.1026134 -4.0953531 -4.0974059 -4.0973592 -4.0858321 -4.0718179 -4.0653372][-4.1968088 -4.1717277 -4.1638203 -4.1708612 -4.1846056 -4.1965494 -4.1961322 -4.1896591 -4.1831465 -4.1792917 -4.1767325 -4.1678524 -4.1498337 -4.1330824 -4.1297979][-4.2660546 -4.244173 -4.2347941 -4.2382855 -4.2468157 -4.2556758 -4.2563853 -4.2548709 -4.2547479 -4.2563477 -4.255651 -4.24672 -4.2317376 -4.2204571 -4.2206268][-4.3206925 -4.3058352 -4.2966986 -4.2945232 -4.2956281 -4.2984748 -4.300158 -4.3014479 -4.3028989 -4.3057833 -4.3079453 -4.3046794 -4.2970595 -4.29128 -4.292038][-4.3510461 -4.3431592 -4.3357062 -4.3307633 -4.3283625 -4.3284245 -4.3305931 -4.3332214 -4.3347325 -4.3364525 -4.3386974 -4.33803 -4.3350134 -4.3329344 -4.3343835]]...]
INFO - root - 2017-12-06 01:47:27.338144: step 59110, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 68h:40m:31s remains)
INFO - root - 2017-12-06 01:47:36.276840: step 59120, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 63h:40m:40s remains)
INFO - root - 2017-12-06 01:47:45.382392: step 59130, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.900 sec/batch; 68h:21m:05s remains)
INFO - root - 2017-12-06 01:47:54.628693: step 59140, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 68h:05m:54s remains)
INFO - root - 2017-12-06 01:48:03.821942: step 59150, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 67h:25m:59s remains)
INFO - root - 2017-12-06 01:48:12.906086: step 59160, loss = 2.11, batch loss = 2.05 (8.8 examples/sec; 0.912 sec/batch; 69h:16m:26s remains)
INFO - root - 2017-12-06 01:48:21.979469: step 59170, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 68h:20m:16s remains)
INFO - root - 2017-12-06 01:48:31.087779: step 59180, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.896 sec/batch; 68h:02m:33s remains)
INFO - root - 2017-12-06 01:48:40.137096: step 59190, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.929 sec/batch; 70h:32m:26s remains)
INFO - root - 2017-12-06 01:48:49.204502: step 59200, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 67h:37m:27s remains)
2017-12-06 01:48:49.952247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2747478 -4.2705355 -4.2552228 -4.2359509 -4.21463 -4.2024145 -4.2090564 -4.2274218 -4.2443953 -4.2584062 -4.2694468 -4.2741923 -4.2727547 -4.264535 -4.2529535][-4.2786894 -4.2700896 -4.2502351 -4.2275443 -4.2023087 -4.1871142 -4.1946478 -4.2157588 -4.2344055 -4.2507024 -4.2638683 -4.2700348 -4.2704134 -4.2619548 -4.2442017][-4.2845268 -4.2752395 -4.2569504 -4.236958 -4.2114882 -4.1921883 -4.1942658 -4.2102814 -4.2271109 -4.2475491 -4.2665873 -4.2766275 -4.2782493 -4.2682424 -4.2429223][-4.2907529 -4.2873831 -4.2767162 -4.2624903 -4.2387905 -4.2145052 -4.2054992 -4.2080555 -4.2173028 -4.24216 -4.2689524 -4.2849054 -4.2905221 -4.2819018 -4.2530379][-4.2886133 -4.2983661 -4.2990489 -4.2915611 -4.2698555 -4.2405448 -4.2183075 -4.2019048 -4.1968613 -4.2240653 -4.2602549 -4.2856193 -4.2980652 -4.2962222 -4.2690039][-4.27396 -4.2982287 -4.3089042 -4.3053207 -4.2815113 -4.2457304 -4.2090077 -4.1715393 -4.1527624 -4.1850014 -4.2322407 -4.2683873 -4.28831 -4.2940989 -4.2733126][-4.2538109 -4.2867503 -4.302485 -4.298368 -4.2685428 -4.2249331 -4.1738243 -4.1198816 -4.0950508 -4.1377568 -4.1986418 -4.246243 -4.2732778 -4.2841148 -4.2696962][-4.241076 -4.2778597 -4.2948875 -4.2882986 -4.2496772 -4.1946168 -4.129715 -4.0649118 -4.0447235 -4.1030307 -4.1772494 -4.2347217 -4.2680192 -4.2804017 -4.270875][-4.2458491 -4.281405 -4.2962279 -4.2852845 -4.2393026 -4.1772776 -4.1100802 -4.0525026 -4.045537 -4.1094875 -4.1835351 -4.2410693 -4.2738481 -4.285821 -4.278831][-4.2614288 -4.2917652 -4.3020449 -4.2882066 -4.2440453 -4.190125 -4.139719 -4.1041889 -4.1068745 -4.1574078 -4.2129407 -4.259191 -4.2868609 -4.2978606 -4.2915378][-4.2698088 -4.2959666 -4.3063235 -4.2957592 -4.259974 -4.2171516 -4.182128 -4.1602411 -4.1620765 -4.1945305 -4.2307568 -4.2640085 -4.2876887 -4.299211 -4.29566][-4.2658186 -4.29121 -4.3030944 -4.2961764 -4.2683277 -4.2349186 -4.2098651 -4.1955228 -4.1956868 -4.2144475 -4.2359524 -4.2559671 -4.2734327 -4.2839518 -4.2826424][-4.2614403 -4.2840166 -4.2949839 -4.2913642 -4.2716217 -4.24582 -4.2250814 -4.2120214 -4.2101269 -4.2212262 -4.2311611 -4.2378159 -4.2480283 -4.2590866 -4.2602758][-4.2632585 -4.2815561 -4.2875276 -4.2828875 -4.2675376 -4.2476053 -4.2300057 -4.2173457 -4.2134252 -4.2183943 -4.2189579 -4.2137356 -4.217833 -4.2308979 -4.238626][-4.2708378 -4.2836146 -4.2839961 -4.2765222 -4.2623558 -4.2450371 -4.228951 -4.2146721 -4.2070656 -4.2058496 -4.1998692 -4.1893978 -4.1913576 -4.2071733 -4.222198]]...]
INFO - root - 2017-12-06 01:48:59.032004: step 59210, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 67h:27m:38s remains)
INFO - root - 2017-12-06 01:49:07.974258: step 59220, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 65h:42m:58s remains)
INFO - root - 2017-12-06 01:49:16.874170: step 59230, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 63h:28m:26s remains)
INFO - root - 2017-12-06 01:49:25.960422: step 59240, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.937 sec/batch; 71h:09m:20s remains)
INFO - root - 2017-12-06 01:49:35.088461: step 59250, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.894 sec/batch; 67h:49m:57s remains)
INFO - root - 2017-12-06 01:49:44.110674: step 59260, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.894 sec/batch; 67h:49m:15s remains)
INFO - root - 2017-12-06 01:49:53.248326: step 59270, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.906 sec/batch; 68h:44m:38s remains)
INFO - root - 2017-12-06 01:50:02.317939: step 59280, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 66h:27m:08s remains)
INFO - root - 2017-12-06 01:50:11.208872: step 59290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.904 sec/batch; 68h:35m:05s remains)
INFO - root - 2017-12-06 01:50:20.311296: step 59300, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.925 sec/batch; 70h:12m:38s remains)
2017-12-06 01:50:21.128018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.227139 -4.2051749 -4.1761265 -4.1639 -4.1817241 -4.2225404 -4.2656384 -4.2935286 -4.3028388 -4.2987504 -4.2915654 -4.288003 -4.2824383 -4.2742529 -4.2755446][-4.2280951 -4.2077184 -4.1824055 -4.1692672 -4.1850886 -4.2243285 -4.2663279 -4.2948923 -4.3006854 -4.2879891 -4.2735524 -4.2643566 -4.252954 -4.2418714 -4.2456341][-4.2372169 -4.226 -4.20619 -4.1908956 -4.200211 -4.2294927 -4.2592049 -4.2764335 -4.2687221 -4.2459702 -4.2270074 -4.2123232 -4.1974277 -4.1873188 -4.1990728][-4.2417641 -4.2368445 -4.2185206 -4.200212 -4.2031841 -4.2186227 -4.2298174 -4.2242351 -4.1981993 -4.171845 -4.1538262 -4.1402917 -4.1316824 -4.1358957 -4.165451][-4.241075 -4.2392654 -4.2227721 -4.2030354 -4.1958094 -4.1896496 -4.1740103 -4.138835 -4.1005793 -4.0818205 -4.0722928 -4.069664 -4.0801268 -4.1118946 -4.1651473][-4.2385612 -4.2358718 -4.2187495 -4.1918111 -4.163898 -4.1244903 -4.0705457 -4.007997 -3.9783521 -3.9870605 -4.0021682 -4.0217619 -4.0584021 -4.1212091 -4.1911268][-4.2292986 -4.2186809 -4.1953106 -4.1553879 -4.0998182 -4.0192895 -3.91885 -3.8373005 -3.8454127 -3.9109335 -3.9677274 -4.0153623 -4.0741425 -4.1505985 -4.2199316][-4.2034945 -4.1802187 -4.1488872 -4.098671 -4.0278587 -3.9169238 -3.7756305 -3.6901295 -3.7599738 -3.8904471 -3.9858327 -4.051527 -4.1210904 -4.1932068 -4.2458191][-4.1668482 -4.132164 -4.093513 -4.0434542 -3.9794261 -3.873347 -3.7430034 -3.6892953 -3.7924895 -3.9317837 -4.0284529 -4.0980644 -4.1689711 -4.2294726 -4.2619176][-4.1286793 -4.092144 -4.052525 -4.0199661 -3.9885197 -3.9263182 -3.8516049 -3.8365712 -3.9173942 -4.01852 -4.0922751 -4.1528893 -4.2154512 -4.2616835 -4.2793093][-4.09967 -4.07291 -4.0448542 -4.0384603 -4.0406218 -4.0215859 -3.9969046 -4.0022035 -4.0545478 -4.1170311 -4.1648574 -4.2111897 -4.2583604 -4.2878547 -4.2941785][-4.1023316 -4.0900946 -4.0779324 -4.09065 -4.1141863 -4.1260867 -4.1309695 -4.14542 -4.1787615 -4.2130108 -4.2411923 -4.2715545 -4.2999563 -4.3111129 -4.304698][-4.1485291 -4.1462421 -4.1426082 -4.1616092 -4.192728 -4.2186475 -4.2344251 -4.2488241 -4.2678971 -4.2860656 -4.302 -4.3153358 -4.3206191 -4.3133693 -4.2954059][-4.2156849 -4.2151918 -4.207305 -4.2190733 -4.2447567 -4.2706113 -4.288434 -4.3024125 -4.3158426 -4.3243403 -4.3287692 -4.3243175 -4.3083892 -4.2864871 -4.2617078][-4.2678576 -4.2650633 -4.2508297 -4.2499185 -4.2624578 -4.28009 -4.2960086 -4.3105636 -4.3204427 -4.3222628 -4.3166924 -4.2982635 -4.2676 -4.2370515 -4.2118282]]...]
INFO - root - 2017-12-06 01:50:30.158778: step 59310, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 69h:14m:41s remains)
INFO - root - 2017-12-06 01:50:39.306372: step 59320, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.915 sec/batch; 69h:24m:59s remains)
INFO - root - 2017-12-06 01:50:48.367270: step 59330, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.914 sec/batch; 69h:19m:39s remains)
INFO - root - 2017-12-06 01:50:57.349309: step 59340, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 69h:57m:04s remains)
INFO - root - 2017-12-06 01:51:06.288686: step 59350, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 68h:58m:13s remains)
INFO - root - 2017-12-06 01:51:15.463164: step 59360, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.929 sec/batch; 70h:29m:18s remains)
INFO - root - 2017-12-06 01:51:24.526618: step 59370, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 66h:51m:17s remains)
INFO - root - 2017-12-06 01:51:33.407699: step 59380, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 68h:10m:06s remains)
INFO - root - 2017-12-06 01:51:42.398914: step 59390, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 67h:48m:53s remains)
INFO - root - 2017-12-06 01:51:51.537814: step 59400, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 67h:52m:09s remains)
2017-12-06 01:51:52.342212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24009 -4.2527938 -4.2748556 -4.2835984 -4.2699804 -4.2517576 -4.2403841 -4.2380185 -4.2387176 -4.2495565 -4.2614388 -4.2591238 -4.2541857 -4.2553573 -4.2697868][-4.2198944 -4.2324257 -4.2544985 -4.2621336 -4.2477174 -4.2265873 -4.2113442 -4.2039566 -4.2038617 -4.2199588 -4.2374496 -4.2315841 -4.2223029 -4.2230811 -4.2407012][-4.2048769 -4.2157326 -4.233604 -4.2421122 -4.2302585 -4.2083354 -4.1840353 -4.1674075 -4.1687145 -4.1940861 -4.2182832 -4.2135987 -4.2009468 -4.1999197 -4.2198811][-4.1964574 -4.2074771 -4.2236662 -4.2331972 -4.2204056 -4.1924396 -4.1529613 -4.1253719 -4.1364646 -4.1780086 -4.2065368 -4.2036905 -4.1894112 -4.1872125 -4.2095847][-4.1972594 -4.2084527 -4.2233167 -4.2336659 -4.2156324 -4.1684675 -4.1043572 -4.0710955 -4.1086087 -4.1702528 -4.2004819 -4.1992903 -4.1869488 -4.1855464 -4.2131042][-4.2072344 -4.2152753 -4.2264323 -4.2352324 -4.2028289 -4.1219974 -4.02966 -4.0152745 -4.0990629 -4.1744781 -4.2005863 -4.1977324 -4.1882339 -4.1901031 -4.2253361][-4.2129345 -4.2134209 -4.2184329 -4.2218647 -4.1696343 -4.052351 -3.9408343 -3.9749908 -4.1047049 -4.1830621 -4.2011361 -4.19359 -4.1811213 -4.1848583 -4.2313204][-4.2025852 -4.1973243 -4.1997266 -4.1956086 -4.124073 -3.9757609 -3.8637147 -3.955035 -4.1046658 -4.1771221 -4.1889653 -4.1788092 -4.1629171 -4.1712303 -4.225841][-4.1827559 -4.176784 -4.1818151 -4.1723604 -4.0889645 -3.9447935 -3.8777616 -3.9976265 -4.12195 -4.1736317 -4.1762562 -4.1638284 -4.1515212 -4.1706896 -4.2282529][-4.1693883 -4.1650133 -4.1730785 -4.156992 -4.0708575 -3.9609721 -3.9578004 -4.0716572 -4.1534271 -4.17931 -4.1728573 -4.1573324 -4.1519394 -4.1829948 -4.2390456][-4.1549153 -4.1523423 -4.16034 -4.1371312 -4.0577631 -3.9856224 -4.0261607 -4.1236997 -4.1749277 -4.1855116 -4.1727037 -4.1547484 -4.1579671 -4.1970034 -4.2478714][-4.1640458 -4.1611423 -4.1667681 -4.1411576 -4.0769806 -4.0376153 -4.0906115 -4.1669707 -4.2027335 -4.2093196 -4.1971979 -4.1814427 -4.1890635 -4.2265811 -4.2664318][-4.2044787 -4.2043009 -4.2085695 -4.183567 -4.1397848 -4.1217871 -4.1644654 -4.2164483 -4.2404823 -4.2467771 -4.240962 -4.2319465 -4.2411132 -4.270206 -4.2963357][-4.2499385 -4.2511635 -4.2535405 -4.2358112 -4.2126222 -4.2065296 -4.23275 -4.2626548 -4.2792416 -4.2878995 -4.286202 -4.2805252 -4.285265 -4.3033385 -4.3188128][-4.2902403 -4.2905874 -4.2911191 -4.2810345 -4.2724185 -4.2709923 -4.2848516 -4.3000917 -4.3118315 -4.3182511 -4.3171678 -4.3119712 -4.3126836 -4.321105 -4.3292418]]...]
INFO - root - 2017-12-06 01:52:01.165531: step 59410, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 68h:10m:17s remains)
INFO - root - 2017-12-06 01:52:10.306128: step 59420, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 66h:11m:06s remains)
INFO - root - 2017-12-06 01:52:19.532000: step 59430, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.915 sec/batch; 69h:25m:18s remains)
INFO - root - 2017-12-06 01:52:28.467440: step 59440, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.916 sec/batch; 69h:29m:41s remains)
INFO - root - 2017-12-06 01:52:37.593566: step 59450, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.917 sec/batch; 69h:34m:48s remains)
INFO - root - 2017-12-06 01:52:46.619219: step 59460, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 69h:15m:01s remains)
INFO - root - 2017-12-06 01:52:55.693421: step 59470, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 69h:38m:46s remains)
INFO - root - 2017-12-06 01:53:04.866980: step 59480, loss = 2.10, batch loss = 2.04 (8.6 examples/sec; 0.932 sec/batch; 70h:43m:02s remains)
INFO - root - 2017-12-06 01:53:13.904358: step 59490, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 67h:04m:44s remains)
INFO - root - 2017-12-06 01:53:22.931837: step 59500, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 67h:47m:53s remains)
2017-12-06 01:53:23.764727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3230095 -4.303833 -4.2826757 -4.2627406 -4.2549934 -4.2539759 -4.2492247 -4.2422652 -4.2520828 -4.2770147 -4.2930589 -4.2839422 -4.2642436 -4.2514639 -4.2517943][-4.309659 -4.2825842 -4.25651 -4.23238 -4.2240996 -4.2226214 -4.2211471 -4.219295 -4.2338748 -4.2637553 -4.2811694 -4.2700343 -4.2512012 -4.23925 -4.2377334][-4.2983441 -4.2643108 -4.2322216 -4.2018547 -4.1892066 -4.1849132 -4.1841569 -4.1872168 -4.2068243 -4.2417588 -4.2659793 -4.2569208 -4.2426577 -4.2365685 -4.2386265][-4.2917414 -4.2522597 -4.21546 -4.1807156 -4.1587067 -4.1486349 -4.1472845 -4.153646 -4.1759834 -4.2122664 -4.2432766 -4.2399225 -4.2315736 -4.2340617 -4.2455373][-4.2911172 -4.2510638 -4.2161765 -4.1827664 -4.15674 -4.14296 -4.1437149 -4.1512251 -4.1714849 -4.2026877 -4.2332997 -4.2325144 -4.2267952 -4.2346244 -4.2497025][-4.2943435 -4.256815 -4.2278795 -4.2027655 -4.1808143 -4.1667833 -4.1661539 -4.1755648 -4.1927123 -4.214817 -4.2379069 -4.2340631 -4.2268291 -4.2313461 -4.243834][-4.2971053 -4.2610397 -4.2350965 -4.2150731 -4.1967831 -4.18323 -4.1795549 -4.1895275 -4.2062445 -4.2260876 -4.241775 -4.2359424 -4.2297978 -4.2303467 -4.2375312][-4.2976208 -4.2610574 -4.2318316 -4.2111068 -4.1947002 -4.1804724 -4.17051 -4.1765289 -4.1947093 -4.2183867 -4.2310953 -4.2284803 -4.2305861 -4.2361484 -4.2397118][-4.29592 -4.2596025 -4.2289457 -4.2073703 -4.1919589 -4.1770291 -4.1603007 -4.1625628 -4.18279 -4.2094793 -4.2214684 -4.2225075 -4.2330117 -4.242691 -4.2436357][-4.2907462 -4.2536778 -4.2230372 -4.2014837 -4.1896496 -4.1789017 -4.1645694 -4.1672835 -4.1852183 -4.210629 -4.2234268 -4.2236648 -4.2340555 -4.2447228 -4.2462735][-4.2869554 -4.2502608 -4.2206655 -4.2007489 -4.1938391 -4.1889706 -4.1785941 -4.1790805 -4.1916041 -4.2119784 -4.2223053 -4.222363 -4.2322335 -4.2439113 -4.2476144][-4.2883263 -4.2544994 -4.2270856 -4.2084818 -4.2028713 -4.1987724 -4.1900764 -4.1886668 -4.1974835 -4.2120156 -4.220808 -4.2252173 -4.2359409 -4.2470517 -4.25162][-4.2953615 -4.26474 -4.2382464 -4.2174306 -4.207375 -4.201314 -4.1958513 -4.1993437 -4.20798 -4.2183428 -4.2269154 -4.234869 -4.2472019 -4.2588558 -4.2613554][-4.3053732 -4.2790074 -4.2541575 -4.23223 -4.2164822 -4.20799 -4.207715 -4.2180896 -4.229557 -4.2394791 -4.248136 -4.2577186 -4.2690134 -4.2765846 -4.2758946][-4.3167496 -4.2952623 -4.2727213 -4.2510934 -4.2349315 -4.2277379 -4.2313981 -4.2438893 -4.2558503 -4.2639709 -4.2704768 -4.2791033 -4.287467 -4.2914491 -4.2897596]]...]
INFO - root - 2017-12-06 01:53:32.839617: step 59510, loss = 2.09, batch loss = 2.03 (8.8 examples/sec; 0.908 sec/batch; 68h:52m:58s remains)
INFO - root - 2017-12-06 01:53:41.866108: step 59520, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.897 sec/batch; 68h:02m:56s remains)
INFO - root - 2017-12-06 01:53:50.959611: step 59530, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 68h:57m:00s remains)
INFO - root - 2017-12-06 01:54:00.032027: step 59540, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.907 sec/batch; 68h:46m:11s remains)
INFO - root - 2017-12-06 01:54:09.215352: step 59550, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.917 sec/batch; 69h:31m:35s remains)
INFO - root - 2017-12-06 01:54:18.440735: step 59560, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.904 sec/batch; 68h:31m:27s remains)
INFO - root - 2017-12-06 01:54:27.421479: step 59570, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 69h:38m:36s remains)
INFO - root - 2017-12-06 01:54:36.564924: step 59580, loss = 2.03, batch loss = 1.97 (8.7 examples/sec; 0.920 sec/batch; 69h:44m:44s remains)
INFO - root - 2017-12-06 01:54:45.605939: step 59590, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 65h:01m:47s remains)
INFO - root - 2017-12-06 01:54:54.716528: step 59600, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.916 sec/batch; 69h:28m:20s remains)
2017-12-06 01:54:55.612607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3210897 -4.3236885 -4.3257761 -4.3272567 -4.3268871 -4.3272929 -4.3272943 -4.326539 -4.3258982 -4.32209 -4.3166585 -4.3147769 -4.31755 -4.3232245 -4.3302784][-4.3025627 -4.2990656 -4.2998257 -4.3009615 -4.3029742 -4.3071728 -4.3070507 -4.3063517 -4.3075047 -4.303071 -4.2927256 -4.2872615 -4.2896376 -4.2989974 -4.3113837][-4.2746372 -4.2633295 -4.2599692 -4.259799 -4.2639017 -4.273344 -4.2724891 -4.2680712 -4.2711267 -4.2679195 -4.2575169 -4.2532063 -4.2601156 -4.277709 -4.2973194][-4.2305737 -4.2156 -4.2106175 -4.2141051 -4.2242155 -4.2372675 -4.2351494 -4.2233148 -4.2274203 -4.226685 -4.2170854 -4.2168059 -4.2338352 -4.2630153 -4.2911334][-4.1776872 -4.161149 -4.1610136 -4.174911 -4.1912942 -4.2032528 -4.1960349 -4.1760468 -4.1833649 -4.1880918 -4.1808147 -4.1872888 -4.2176824 -4.2591081 -4.2920485][-4.1264033 -4.1055431 -4.1050358 -4.1240969 -4.1453276 -4.1542997 -4.1292362 -4.0958948 -4.1099019 -4.1310244 -4.1349292 -4.1569877 -4.2033024 -4.2557354 -4.2942042][-4.09361 -4.0631919 -4.0515995 -4.058681 -4.0683203 -4.06253 -4.0054793 -3.952949 -3.9914789 -4.0511017 -4.0831132 -4.1260643 -4.1842079 -4.2444196 -4.2897129][-4.0995464 -4.0652637 -4.0391421 -4.0232778 -4.0114164 -3.980478 -3.8786829 -3.7969353 -3.8719785 -3.976552 -4.0375886 -4.0937905 -4.1559076 -4.2213187 -4.2781267][-4.1547475 -4.1254587 -4.0977764 -4.0720234 -4.0473375 -4.0077782 -3.9071493 -3.8281813 -3.904037 -4.0066991 -4.0645466 -4.1105642 -4.156539 -4.2136641 -4.2731781][-4.2112875 -4.1922226 -4.1758766 -4.1574106 -4.137506 -4.1159582 -4.060461 -4.0152164 -4.0608139 -4.121263 -4.1509514 -4.1733871 -4.195416 -4.2354431 -4.2843671][-4.2370629 -4.2262363 -4.2156582 -4.2045321 -4.1947789 -4.1895857 -4.164597 -4.1416378 -4.1649089 -4.19131 -4.2018485 -4.2130208 -4.2257042 -4.2585492 -4.2978897][-4.2528782 -4.2395244 -4.2292423 -4.2156277 -4.2104654 -4.216116 -4.2086105 -4.1961751 -4.2107215 -4.22269 -4.2266011 -4.2337222 -4.2445078 -4.2765594 -4.3099132][-4.2695446 -4.2502584 -4.23703 -4.2236724 -4.218802 -4.2244639 -4.2235641 -4.2176342 -4.2328954 -4.2450571 -4.2515426 -4.2584581 -4.2702861 -4.29928 -4.3249755][-4.2893019 -4.2723231 -4.2612543 -4.2530494 -4.24922 -4.2501793 -4.2489991 -4.2452679 -4.2587295 -4.2711196 -4.2792449 -4.2860684 -4.2976232 -4.3202267 -4.3371544][-4.3107262 -4.2993255 -4.2907081 -4.2861824 -4.2836661 -4.2842913 -4.2827373 -4.2794609 -4.288909 -4.298779 -4.3050017 -4.3120174 -4.3209844 -4.3352828 -4.3451242]]...]
INFO - root - 2017-12-06 01:55:04.761064: step 59610, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.919 sec/batch; 69h:39m:12s remains)
INFO - root - 2017-12-06 01:55:14.015975: step 59620, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.937 sec/batch; 71h:03m:27s remains)
INFO - root - 2017-12-06 01:55:23.096960: step 59630, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 67h:18m:59s remains)
INFO - root - 2017-12-06 01:55:32.214309: step 59640, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 66h:31m:28s remains)
INFO - root - 2017-12-06 01:55:41.160124: step 59650, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.909 sec/batch; 68h:55m:08s remains)
INFO - root - 2017-12-06 01:55:50.234475: step 59660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 66h:40m:52s remains)
INFO - root - 2017-12-06 01:55:59.243865: step 59670, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 67h:26m:50s remains)
INFO - root - 2017-12-06 01:56:08.302289: step 59680, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.938 sec/batch; 71h:04m:37s remains)
INFO - root - 2017-12-06 01:56:17.443339: step 59690, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.919 sec/batch; 69h:37m:36s remains)
INFO - root - 2017-12-06 01:56:26.531075: step 59700, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 67h:02m:20s remains)
2017-12-06 01:56:27.392936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1701 -4.2186422 -4.2624354 -4.2873549 -4.2868605 -4.266233 -4.2215743 -4.1631789 -4.1231575 -4.1146288 -4.1358809 -4.1610518 -4.1810474 -4.1729317 -4.14034][-4.2075839 -4.2457871 -4.2754321 -4.2879953 -4.2764735 -4.244205 -4.1943369 -4.1400385 -4.1128488 -4.1197472 -4.147337 -4.1744652 -4.2037325 -4.2114143 -4.1808367][-4.2389169 -4.2662768 -4.2845907 -4.2837796 -4.2635207 -4.2257113 -4.1739297 -4.1236939 -4.1040559 -4.1192584 -4.1482582 -4.1769567 -4.2126069 -4.2368984 -4.221921][-4.2557492 -4.2764311 -4.282927 -4.2708817 -4.2477989 -4.2111096 -4.1562567 -4.1025772 -4.0794907 -4.0995026 -4.1323271 -4.169744 -4.2180018 -4.2564526 -4.2583117][-4.2565889 -4.2700424 -4.267035 -4.2501936 -4.2281485 -4.192225 -4.1307116 -4.0616007 -4.0244336 -4.04872 -4.1025362 -4.1560888 -4.21419 -4.2597313 -4.2731895][-4.23824 -4.2452765 -4.2368264 -4.2187433 -4.197803 -4.1603227 -4.0909839 -4.0003839 -3.9400046 -3.9679396 -4.0539389 -4.1377649 -4.2084093 -4.255281 -4.2687383][-4.2225804 -4.21989 -4.2034969 -4.1811552 -4.15403 -4.1127076 -4.0440059 -3.9379759 -3.8488951 -3.876853 -3.9958146 -4.1160712 -4.2027116 -4.2507906 -4.2601075][-4.2247996 -4.2107782 -4.1842384 -4.1513014 -4.1133842 -4.0693507 -4.0082126 -3.9076123 -3.8122067 -3.8347156 -3.9647055 -4.0997248 -4.1917396 -4.2365236 -4.2488842][-4.2291064 -4.2085733 -4.17322 -4.1298909 -4.0840521 -4.0350814 -3.9896784 -3.9276409 -3.8694768 -3.881881 -3.9817019 -4.1033921 -4.1888914 -4.2280293 -4.2424555][-4.2243457 -4.20794 -4.1751909 -4.1290283 -4.0795288 -4.0361724 -4.0152335 -3.9901729 -3.9637253 -3.9662762 -4.0309753 -4.1287842 -4.2003894 -4.2298112 -4.2378454][-4.2199979 -4.2117658 -4.1855907 -4.1410232 -4.0951138 -4.0706983 -4.0764003 -4.0758924 -4.0669208 -4.067555 -4.1031151 -4.1699252 -4.2257032 -4.2493191 -4.2501278][-4.2305379 -4.223105 -4.1922603 -4.1469665 -4.1121054 -4.1121049 -4.13868 -4.153657 -4.1530561 -4.1465726 -4.1601796 -4.1989946 -4.2431912 -4.2676206 -4.2671337][-4.2431479 -4.233541 -4.1982293 -4.152986 -4.1284127 -4.1436005 -4.180707 -4.2056408 -4.2081914 -4.1960988 -4.1941748 -4.2176561 -4.2541966 -4.279686 -4.282783][-4.2506752 -4.2392116 -4.2036562 -4.1622796 -4.143558 -4.1631331 -4.2016726 -4.225667 -4.2253137 -4.2126064 -4.2085528 -4.2270765 -4.2582507 -4.2822175 -4.288682][-4.2594728 -4.2463326 -4.2166705 -4.184927 -4.1708212 -4.1887469 -4.2181425 -4.2279267 -4.2179813 -4.2041864 -4.2033877 -4.2230687 -4.2537093 -4.2801003 -4.2904625]]...]
INFO - root - 2017-12-06 01:56:36.381797: step 59710, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 67h:26m:16s remains)
INFO - root - 2017-12-06 01:56:45.534134: step 59720, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.919 sec/batch; 69h:40m:13s remains)
INFO - root - 2017-12-06 01:56:54.572538: step 59730, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 68h:31m:54s remains)
INFO - root - 2017-12-06 01:57:03.751998: step 59740, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 67h:56m:20s remains)
INFO - root - 2017-12-06 01:57:12.599342: step 59750, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 66h:45m:47s remains)
INFO - root - 2017-12-06 01:57:21.661993: step 59760, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 67h:42m:01s remains)
INFO - root - 2017-12-06 01:57:30.937566: step 59770, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.936 sec/batch; 70h:53m:23s remains)
INFO - root - 2017-12-06 01:57:39.800172: step 59780, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 70h:54m:27s remains)
INFO - root - 2017-12-06 01:57:48.942658: step 59790, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.914 sec/batch; 69h:14m:42s remains)
INFO - root - 2017-12-06 01:57:58.141534: step 59800, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.917 sec/batch; 69h:25m:55s remains)
2017-12-06 01:57:58.951655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2630072 -4.2671089 -4.26726 -4.26803 -4.2711272 -4.2748728 -4.2753878 -4.2718754 -4.268826 -4.269516 -4.2744303 -4.2815247 -4.2864389 -4.2865944 -4.2834406][-4.2521262 -4.2553353 -4.2536039 -4.2526169 -4.2556796 -4.2612357 -4.26214 -4.2579589 -4.2543645 -4.256196 -4.2634778 -4.2734432 -4.2805386 -4.2826562 -4.2800035][-4.235652 -4.2374563 -4.2306294 -4.2231669 -4.2242608 -4.2322173 -4.2343144 -4.2302284 -4.2268414 -4.2314563 -4.244534 -4.2591677 -4.2681394 -4.26976 -4.26634][-4.2322922 -4.2298355 -4.2143764 -4.1969013 -4.1938095 -4.2014227 -4.2042489 -4.2016363 -4.1996 -4.2083035 -4.2318883 -4.2549853 -4.2672081 -4.267993 -4.2640491][-4.232358 -4.222137 -4.1958532 -4.1697044 -4.1617446 -4.1660271 -4.1695094 -4.1697464 -4.1682453 -4.1817527 -4.2191448 -4.2547612 -4.2737775 -4.2765155 -4.27449][-4.2261448 -4.2065744 -4.1690497 -4.1364517 -4.1255322 -4.1265712 -4.129477 -4.1304913 -4.1278787 -4.1459088 -4.19562 -4.2458777 -4.2761264 -4.2835941 -4.28318][-4.22663 -4.1987886 -4.1522837 -4.1138372 -4.0988932 -4.0956273 -4.0985069 -4.0997834 -4.09605 -4.1179533 -4.1775613 -4.2389746 -4.277463 -4.2888832 -4.287426][-4.237884 -4.2047338 -4.1547012 -4.1153369 -4.0989828 -4.09239 -4.0950232 -4.0967283 -4.0910473 -4.1131849 -4.1749949 -4.2417326 -4.2869091 -4.3005691 -4.2962985][-4.247366 -4.2121634 -4.1652155 -4.1318693 -4.118443 -4.11086 -4.1121182 -4.1129928 -4.1046691 -4.1234202 -4.1823535 -4.2493653 -4.2987347 -4.3153644 -4.3097572][-4.2533464 -4.21984 -4.1806736 -4.1570344 -4.1465468 -4.137495 -4.1381731 -4.1364279 -4.1287346 -4.1452203 -4.1991806 -4.26219 -4.3093929 -4.3246293 -4.3152537][-4.25444 -4.2264481 -4.1934071 -4.1776495 -4.172543 -4.1664066 -4.1670141 -4.1622114 -4.1555595 -4.1694031 -4.2179813 -4.2742286 -4.3148184 -4.3251657 -4.3104291][-4.2439523 -4.2224808 -4.1980762 -4.1918859 -4.1929979 -4.1901278 -4.1882071 -4.1805425 -4.1731195 -4.1826262 -4.2247915 -4.2748713 -4.3091125 -4.3144765 -4.2968807][-4.2273645 -4.2142982 -4.1993046 -4.201385 -4.2076745 -4.206378 -4.2028728 -4.1960154 -4.1885586 -4.193871 -4.2299209 -4.2749572 -4.3007126 -4.2998013 -4.2798848][-4.217885 -4.21237 -4.2031255 -4.20704 -4.2141991 -4.2128587 -4.2088065 -4.2042265 -4.1981668 -4.2000794 -4.2312012 -4.2706203 -4.2878647 -4.2816811 -4.2621717][-4.2260494 -4.2258892 -4.2180505 -4.2169576 -4.2209582 -4.2189965 -4.2142534 -4.2108469 -4.2060795 -4.2067628 -4.2321196 -4.2642646 -4.276608 -4.2687 -4.2523465]]...]
INFO - root - 2017-12-06 01:58:08.042387: step 59810, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 68h:06m:08s remains)
INFO - root - 2017-12-06 01:58:17.234624: step 59820, loss = 2.04, batch loss = 1.98 (8.5 examples/sec; 0.946 sec/batch; 71h:38m:41s remains)
INFO - root - 2017-12-06 01:58:26.223843: step 59830, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 69h:48m:07s remains)
INFO - root - 2017-12-06 01:58:35.521093: step 59840, loss = 2.02, batch loss = 1.96 (8.5 examples/sec; 0.945 sec/batch; 71h:34m:45s remains)
INFO - root - 2017-12-06 01:58:44.592228: step 59850, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.920 sec/batch; 69h:39m:43s remains)
INFO - root - 2017-12-06 01:58:53.843870: step 59860, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.952 sec/batch; 72h:07m:28s remains)
INFO - root - 2017-12-06 01:59:02.886142: step 59870, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 66h:31m:20s remains)
INFO - root - 2017-12-06 01:59:11.863194: step 59880, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 67h:27m:34s remains)
INFO - root - 2017-12-06 01:59:20.937443: step 59890, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.895 sec/batch; 67h:45m:42s remains)
INFO - root - 2017-12-06 01:59:30.235399: step 59900, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 67h:01m:35s remains)
2017-12-06 01:59:31.007223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2547479 -4.242897 -4.24199 -4.251616 -4.2602968 -4.2720623 -4.285336 -4.2969728 -4.300446 -4.2960334 -4.2809005 -4.2658262 -4.258019 -4.2597175 -4.2597885][-4.2487555 -4.2311392 -4.2229209 -4.2219195 -4.2194047 -4.2272239 -4.2428474 -4.261951 -4.2746229 -4.2757754 -4.2630305 -4.2500005 -4.2473516 -4.2586479 -4.2635083][-4.2688766 -4.2442756 -4.2238908 -4.2060556 -4.1881838 -4.1868019 -4.2019339 -4.228714 -4.2536306 -4.2621436 -4.252213 -4.2423353 -4.2439485 -4.2608185 -4.2689042][-4.299129 -4.2698736 -4.2391987 -4.2066426 -4.17269 -4.1556578 -4.1636157 -4.1961584 -4.2326765 -4.253181 -4.2511911 -4.2460008 -4.2484403 -4.2646246 -4.2740421][-4.3141074 -4.2825832 -4.2438536 -4.1980209 -4.1467085 -4.1132865 -4.1124077 -4.1453795 -4.1893358 -4.2243323 -4.2371931 -4.2429218 -4.2512403 -4.2683053 -4.2816048][-4.31716 -4.283884 -4.2356668 -4.1758761 -4.1084833 -4.0549817 -4.0377831 -4.0647597 -4.1176476 -4.1735039 -4.2076879 -4.232779 -4.2543635 -4.2765312 -4.2932367][-4.3067451 -4.2715092 -4.2161689 -4.1471591 -4.0651226 -3.9877329 -3.9494848 -3.9714386 -4.03667 -4.1144414 -4.1729865 -4.2188625 -4.2527447 -4.279953 -4.29827][-4.2946062 -4.2583942 -4.2019353 -4.1289139 -4.0340571 -3.9292829 -3.8620057 -3.8772686 -3.9592564 -4.0584483 -4.1383634 -4.1989655 -4.2395673 -4.2686152 -4.2878461][-4.2836385 -4.2520585 -4.207119 -4.1438594 -4.0526729 -3.9404941 -3.8519366 -3.8502979 -3.9321408 -4.0384345 -4.1281323 -4.1919641 -4.231916 -4.2603893 -4.2779317][-4.2726026 -4.2504482 -4.2257423 -4.183342 -4.1157117 -4.0286493 -3.9527838 -3.9376888 -3.9940565 -4.0809078 -4.1599536 -4.2130075 -4.2447782 -4.2658768 -4.2762957][-4.2628651 -4.2442541 -4.2368641 -4.2206697 -4.1863956 -4.1371889 -4.0912776 -4.0798264 -4.1118236 -4.166739 -4.2198172 -4.253686 -4.2711678 -4.2821946 -4.2828422][-4.2589378 -4.2378187 -4.2355757 -4.2379594 -4.2328644 -4.2161422 -4.19898 -4.1990337 -4.2205291 -4.2528315 -4.2801661 -4.2929721 -4.2935486 -4.29149 -4.2851958][-4.269269 -4.2438731 -4.2362318 -4.2439456 -4.2537889 -4.2568359 -4.2573962 -4.2653193 -4.281765 -4.3006392 -4.3095574 -4.3065581 -4.2951427 -4.2841072 -4.273684][-4.289505 -4.2631674 -4.2481046 -4.2498221 -4.2623749 -4.2737713 -4.281703 -4.2940564 -4.3087726 -4.3189149 -4.3163943 -4.304009 -4.2843866 -4.2670259 -4.2546453][-4.3017879 -4.2795439 -4.2618408 -4.2597575 -4.2706738 -4.2829061 -4.2909808 -4.3007417 -4.3116093 -4.3158379 -4.3078275 -4.2936468 -4.2722516 -4.2539363 -4.2434816]]...]
INFO - root - 2017-12-06 01:59:40.060757: step 59910, loss = 2.04, batch loss = 1.98 (8.7 examples/sec; 0.920 sec/batch; 69h:39m:10s remains)
INFO - root - 2017-12-06 01:59:49.255130: step 59920, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 67h:55m:25s remains)
INFO - root - 2017-12-06 01:59:58.463387: step 59930, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.928 sec/batch; 70h:16m:23s remains)
INFO - root - 2017-12-06 02:00:07.514388: step 59940, loss = 2.11, batch loss = 2.05 (9.0 examples/sec; 0.892 sec/batch; 67h:33m:28s remains)
INFO - root - 2017-12-06 02:00:16.510969: step 59950, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.897 sec/batch; 67h:55m:10s remains)
INFO - root - 2017-12-06 02:00:25.724555: step 59960, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 69h:18m:30s remains)
INFO - root - 2017-12-06 02:00:34.654288: step 59970, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 68h:12m:29s remains)
INFO - root - 2017-12-06 02:00:43.816452: step 59980, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.921 sec/batch; 69h:41m:16s remains)
INFO - root - 2017-12-06 02:00:53.167469: step 59990, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.913 sec/batch; 69h:05m:56s remains)
INFO - root - 2017-12-06 02:01:02.277652: step 60000, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.903 sec/batch; 68h:21m:26s remains)
2017-12-06 02:01:03.137863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2899761 -4.2943449 -4.3000851 -4.2983904 -4.2951355 -4.2924972 -4.2916989 -4.2928834 -4.2979579 -4.29971 -4.2962742 -4.2965913 -4.3012676 -4.3036757 -4.3004832][-4.2660112 -4.2702947 -4.2743382 -4.2686839 -4.2622089 -4.2591047 -4.2606192 -4.2645712 -4.2735362 -4.2786169 -4.2753844 -4.2763848 -4.2853475 -4.2885814 -4.2832303][-4.23953 -4.2431231 -4.2421474 -4.2294741 -4.2191215 -4.2156096 -4.2160988 -4.2218971 -4.2373719 -4.2525711 -4.2526379 -4.2538643 -4.2660317 -4.2704468 -4.2646246][-4.2191119 -4.2175608 -4.2062316 -4.1843786 -4.1689034 -4.1645422 -4.1645217 -4.1711459 -4.1938086 -4.2212629 -4.23079 -4.2331061 -4.24732 -4.2540464 -4.2513342][-4.1984034 -4.1899877 -4.1658845 -4.1344004 -4.1140318 -4.1102281 -4.111414 -4.1173544 -4.1438189 -4.1818252 -4.2035351 -4.2082133 -4.22042 -4.2288809 -4.2333765][-4.1811008 -4.1646051 -4.125155 -4.0803194 -4.05791 -4.0637302 -4.0722575 -4.0807142 -4.1091132 -4.151226 -4.1765356 -4.1825509 -4.1912541 -4.1995511 -4.2107105][-4.17241 -4.1442323 -4.0976553 -4.0451155 -4.0233741 -4.0380731 -4.0519986 -4.057343 -4.0871892 -4.1339049 -4.160193 -4.1660147 -4.1712589 -4.1763282 -4.1870241][-4.1584 -4.13191 -4.0974169 -4.0555377 -4.0341377 -4.0400586 -4.0469217 -4.0458035 -4.0732222 -4.1228185 -4.1548324 -4.163219 -4.1669731 -4.168323 -4.1756306][-4.1426377 -4.1315885 -4.1240435 -4.1034131 -4.0796 -4.0677133 -4.0609884 -4.0524354 -4.0691166 -4.1142941 -4.149807 -4.1619358 -4.1661215 -4.1686392 -4.1771183][-4.1333194 -4.1418962 -4.15773 -4.1546507 -4.1334615 -4.1125274 -4.097115 -4.0854416 -4.094614 -4.1274242 -4.1564484 -4.1674004 -4.172987 -4.1786556 -4.1896777][-4.1334095 -4.1509495 -4.1794252 -4.1893792 -4.1710744 -4.1453753 -4.1287742 -4.1208034 -4.12924 -4.1500859 -4.168252 -4.177865 -4.1888475 -4.1993933 -4.2128944][-4.1364717 -4.1526709 -4.1853871 -4.2030258 -4.1885238 -4.1644936 -4.1486073 -4.1435156 -4.15207 -4.1664195 -4.177659 -4.1902556 -4.2081513 -4.2233686 -4.2388482][-4.1450596 -4.1598787 -4.1904755 -4.2102666 -4.1999254 -4.1817989 -4.172091 -4.1706882 -4.1761122 -4.1829357 -4.1883149 -4.2020636 -4.2221193 -4.237947 -4.2506318][-4.1692004 -4.1843395 -4.2087164 -4.2257295 -4.2202692 -4.2097216 -4.2051811 -4.2059078 -4.2084484 -4.2096295 -4.2132387 -4.2252841 -4.2423229 -4.253387 -4.260602][-4.2131987 -4.2245779 -4.2410636 -4.2520056 -4.2495675 -4.2443032 -4.2415628 -4.24256 -4.2451916 -4.247817 -4.253459 -4.2650933 -4.2773328 -4.2822976 -4.2838254]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh-momentum/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 02:01:13.246173: step 60010, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.923 sec/batch; 69h:51m:00s remains)
INFO - root - 2017-12-06 02:01:22.275899: step 60020, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.889 sec/batch; 67h:19m:24s remains)
INFO - root - 2017-12-06 02:01:31.263745: step 60030, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 68h:13m:42s remains)
INFO - root - 2017-12-06 02:01:40.373460: step 60040, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 67h:22m:13s remains)
INFO - root - 2017-12-06 02:01:49.485220: step 60050, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 63h:38m:35s remains)
INFO - root - 2017-12-06 02:01:58.412916: step 60060, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 67h:06m:59s remains)
INFO - root - 2017-12-06 02:02:07.574218: step 60070, loss = 2.09, batch loss = 2.04 (8.6 examples/sec; 0.928 sec/batch; 70h:13m:21s remains)
INFO - root - 2017-12-06 02:02:16.790570: step 60080, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.950 sec/batch; 71h:54m:39s remains)
INFO - root - 2017-12-06 02:02:25.818274: step 60090, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.916 sec/batch; 69h:20m:49s remains)
INFO - root - 2017-12-06 02:02:34.943766: step 60100, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 66h:26m:36s remains)
2017-12-06 02:02:35.736963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.142828 -4.1744976 -4.1912651 -4.17386 -4.1338677 -4.1089311 -4.1309862 -4.1821847 -4.2184424 -4.2336645 -4.2405043 -4.2439404 -4.243053 -4.2471218 -4.2566867][-4.1235576 -4.1573606 -4.1802611 -4.1656566 -4.1168838 -4.0806241 -4.10463 -4.1640663 -4.2020221 -4.2189145 -4.223597 -4.2220273 -4.2192454 -4.2288551 -4.2415867][-4.1349306 -4.1653018 -4.1881337 -4.1758366 -4.1234283 -4.0782413 -4.0981636 -4.1602659 -4.197926 -4.218255 -4.2223964 -4.2161736 -4.2158895 -4.2295437 -4.2433786][-4.1521058 -4.1758194 -4.191534 -4.1765466 -4.1252151 -4.0738645 -4.087851 -4.1556516 -4.1966124 -4.2200737 -4.2220945 -4.2127209 -4.2180414 -4.2353282 -4.2485719][-4.1662307 -4.1847677 -4.19306 -4.1748934 -4.125648 -4.0672288 -4.0761652 -4.1488366 -4.1964116 -4.2179785 -4.2107453 -4.1969247 -4.2061362 -4.2292719 -4.2475181][-4.15842 -4.1793966 -4.1879816 -4.1644411 -4.1065841 -4.0327516 -4.0303874 -4.1101904 -4.1714759 -4.1971464 -4.18727 -4.1737247 -4.1896272 -4.2194791 -4.2430725][-4.140223 -4.1650105 -4.1756973 -4.1434374 -4.0640707 -3.959614 -3.9341908 -4.0238385 -4.1094689 -4.1531296 -4.1523986 -4.1443114 -4.1664238 -4.2003689 -4.2294383][-4.1285667 -4.1482477 -4.1518097 -4.1097579 -4.0148392 -3.8855975 -3.8473582 -3.9505734 -4.0594006 -4.1236348 -4.1362205 -4.1355939 -4.1560059 -4.1880469 -4.2170463][-4.1357265 -4.1554708 -4.1610618 -4.1260495 -4.0415545 -3.9229548 -3.8901067 -3.9863088 -4.0847368 -4.1468582 -4.165329 -4.1693859 -4.1830697 -4.2074785 -4.2310839][-4.1677089 -4.1867137 -4.1953969 -4.17227 -4.1086874 -4.0152187 -3.9932666 -4.0665269 -4.1373067 -4.1797762 -4.1908588 -4.193965 -4.204443 -4.2217226 -4.241816][-4.2079244 -4.22193 -4.2258015 -4.2093492 -4.1655269 -4.098712 -4.081748 -4.1303134 -4.1805272 -4.2102995 -4.2135935 -4.215045 -4.2273173 -4.2423396 -4.2606893][-4.2466187 -4.2536106 -4.2517009 -4.2388997 -4.2105188 -4.1659608 -4.1536822 -4.1850233 -4.2220812 -4.2445097 -4.2450142 -4.2430248 -4.2525778 -4.2654467 -4.2819185][-4.2684054 -4.267725 -4.2640615 -4.2549376 -4.2391834 -4.2161317 -4.210042 -4.2286572 -4.2544212 -4.2705669 -4.2687783 -4.2640767 -4.2682109 -4.27854 -4.2947292][-4.2851472 -4.2821331 -4.281867 -4.2790313 -4.2709322 -4.2600489 -4.2554173 -4.2639704 -4.2803316 -4.2908859 -4.2902775 -4.2874022 -4.2905397 -4.2991247 -4.311121][-4.3061533 -4.3023591 -4.3013625 -4.3002872 -4.2951331 -4.2899137 -4.287951 -4.2921753 -4.3022566 -4.3089027 -4.3093204 -4.3078804 -4.3100452 -4.3160477 -4.3231993]]...]
INFO - root - 2017-12-06 02:02:44.742431: step 60110, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 66h:30m:32s remains)
INFO - root - 2017-12-06 02:02:53.801641: step 60120, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.948 sec/batch; 71h:42m:11s remains)
INFO - root - 2017-12-06 02:03:02.860740: step 60130, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 66h:07m:19s remains)
INFO - root - 2017-12-06 02:03:11.972091: step 60140, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 68h:10m:45s remains)
INFO - root - 2017-12-06 02:03:20.879361: step 60150, loss = 2.09, batch loss = 2.04 (8.8 examples/sec; 0.911 sec/batch; 68h:54m:01s remains)
INFO - root - 2017-12-06 02:03:29.824112: step 60160, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.909 sec/batch; 68h:44m:17s remains)
INFO - root - 2017-12-06 02:03:38.915443: step 60170, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.922 sec/batch; 69h:45m:53s remains)
INFO - root - 2017-12-06 02:03:47.995594: step 60180, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.913 sec/batch; 69h:01m:32s remains)
INFO - root - 2017-12-06 02:03:57.035509: step 60190, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.947 sec/batch; 71h:38m:45s remains)
INFO - root - 2017-12-06 02:04:06.237212: step 60200, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 66h:40m:15s remains)
2017-12-06 02:04:07.025816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25564 -4.2553406 -4.2499881 -4.2394061 -4.2282147 -4.2203627 -4.2105646 -4.20807 -4.216311 -4.22357 -4.2302923 -4.23883 -4.2440119 -4.2454572 -4.251523][-4.2457881 -4.23947 -4.2289448 -4.2159767 -4.2024736 -4.1915822 -4.17729 -4.170867 -4.1788812 -4.1887565 -4.1974249 -4.2047157 -4.2072654 -4.20951 -4.2194448][-4.2335892 -4.217515 -4.1977625 -4.1832285 -4.1709642 -4.1614556 -4.1448913 -4.1325364 -4.1398678 -4.1535082 -4.1680603 -4.1741543 -4.1719894 -4.1732655 -4.1851816][-4.2286539 -4.2065907 -4.182436 -4.1691236 -4.1598554 -4.1520896 -4.1332579 -4.1132517 -4.1155348 -4.1309681 -4.1500421 -4.1553817 -4.1503754 -4.153295 -4.1709609][-4.2346439 -4.2131343 -4.1913185 -4.1809077 -4.1756253 -4.1686754 -4.1457677 -4.1163344 -4.1082654 -4.1198554 -4.1386995 -4.1441402 -4.1403518 -4.1470566 -4.1724167][-4.2470036 -4.2262974 -4.2043262 -4.1919107 -4.1845474 -4.1762362 -4.1515255 -4.1192575 -4.1088696 -4.1207318 -4.1412215 -4.1484756 -4.1515107 -4.1689553 -4.198205][-4.2525196 -4.2311263 -4.2086535 -4.1933794 -4.1818037 -4.1696215 -4.1454287 -4.1186328 -4.1189046 -4.1375751 -4.1580229 -4.1614585 -4.1659675 -4.1919041 -4.2232914][-4.2429862 -4.21587 -4.1899281 -4.1725316 -4.1577692 -4.1438518 -4.1246424 -4.1083813 -4.1213546 -4.149332 -4.1699743 -4.1682062 -4.1688709 -4.1989231 -4.232769][-4.23574 -4.2028522 -4.1747971 -4.1554742 -4.1370749 -4.1237011 -4.1132064 -4.1109047 -4.1357169 -4.1699371 -4.1857963 -4.1761103 -4.170814 -4.2004294 -4.2352796][-4.232029 -4.1976128 -4.170948 -4.1519136 -4.1333289 -4.1246247 -4.1251187 -4.1330209 -4.163249 -4.1957622 -4.2020335 -4.1835823 -4.1754303 -4.1996093 -4.2317424][-4.2319465 -4.2019615 -4.176589 -4.1582403 -4.1428189 -4.141314 -4.15326 -4.168056 -4.1971397 -4.220252 -4.2165928 -4.1949697 -4.1843276 -4.1997223 -4.2238474][-4.2371492 -4.2148695 -4.195117 -4.1817293 -4.1739664 -4.1794081 -4.1967959 -4.2120657 -4.2333708 -4.2464995 -4.2384958 -4.2183986 -4.2068377 -4.2138557 -4.2282376][-4.2451782 -4.2298284 -4.2171221 -4.2110209 -4.2118912 -4.2218814 -4.2375441 -4.2479544 -4.2579279 -4.2610979 -4.2506876 -4.2350006 -4.22669 -4.2288761 -4.2359715][-4.2584839 -4.245502 -4.235815 -4.233263 -4.2386231 -4.2490764 -4.2612729 -4.2669454 -4.2689919 -4.2668877 -4.2582197 -4.2475271 -4.2419519 -4.2420106 -4.2467141][-4.2833486 -4.2723503 -4.2635908 -4.259655 -4.2616038 -4.2662559 -4.2723107 -4.2749171 -4.2743936 -4.2723789 -4.2686787 -4.2638078 -4.2608376 -4.2598362 -4.2628193]]...]
INFO - root - 2017-12-06 02:04:16.214228: step 60210, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 69h:58m:39s remains)
INFO - root - 2017-12-06 02:04:25.168009: step 60220, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.885 sec/batch; 66h:55m:15s remains)
INFO - root - 2017-12-06 02:04:34.333053: step 60230, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 67h:22m:45s remains)
INFO - root - 2017-12-06 02:04:43.550567: step 60240, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.947 sec/batch; 71h:38m:41s remains)
INFO - root - 2017-12-06 02:04:52.574053: step 60250, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 68h:25m:58s remains)
INFO - root - 2017-12-06 02:05:01.465335: step 60260, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.918 sec/batch; 69h:24m:27s remains)
INFO - root - 2017-12-06 02:05:10.778960: step 60270, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.911 sec/batch; 68h:55m:18s remains)
INFO - root - 2017-12-06 02:05:20.013189: step 60280, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.930 sec/batch; 70h:18m:12s remains)
INFO - root - 2017-12-06 02:05:29.252966: step 60290, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 68h:34m:46s remains)
INFO - root - 2017-12-06 02:05:38.533894: step 60300, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 68h:44m:40s remains)
2017-12-06 02:05:39.430186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1710124 -4.1804962 -4.1773186 -4.1691904 -4.1694422 -4.1768723 -4.1930308 -4.2074513 -4.2154608 -4.2156959 -4.2146916 -4.2108812 -4.217464 -4.2354126 -4.2566776][-4.2016225 -4.20634 -4.1875267 -4.1606474 -4.1455536 -4.1496391 -4.167141 -4.1833744 -4.1972942 -4.200685 -4.2036223 -4.2009382 -4.2059031 -4.2250409 -4.2489495][-4.2293358 -4.2275968 -4.1961718 -4.1533432 -4.1268091 -4.1281695 -4.1419139 -4.1529484 -4.166151 -4.1747155 -4.1846018 -4.18733 -4.1951833 -4.2146297 -4.2407374][-4.2353058 -4.2291765 -4.1901979 -4.1378651 -4.1071644 -4.1037183 -4.10605 -4.1052608 -4.11649 -4.1364098 -4.1576834 -4.174016 -4.1912551 -4.2123237 -4.239881][-4.2259159 -4.2216425 -4.1767011 -4.1157465 -4.0821967 -4.0733628 -4.057548 -4.0304403 -4.0424843 -4.0848827 -4.1212797 -4.1538053 -4.1862597 -4.2173305 -4.246429][-4.2061462 -4.200706 -4.1533465 -4.0900726 -4.0517306 -4.0319729 -3.9836113 -3.9098916 -3.9249723 -4.0061026 -4.06411 -4.1137295 -4.1664491 -4.2131991 -4.2483177][-4.2009854 -4.1930509 -4.1463065 -4.0841875 -4.0373168 -3.9975669 -3.9088957 -3.7880957 -3.8112957 -3.9326959 -4.0101457 -4.0667996 -4.1270275 -4.187819 -4.2343531][-4.2141623 -4.2030063 -4.15595 -4.0944395 -4.0400753 -3.9904182 -3.9020231 -3.8006136 -3.8284149 -3.9339039 -3.9991937 -4.044138 -4.1005163 -4.1663122 -4.2221341][-4.2429185 -4.2354937 -4.192277 -4.1329851 -4.079814 -4.0366049 -3.9799774 -3.9331093 -3.9492476 -4.0007291 -4.0356374 -4.062716 -4.1093755 -4.1722445 -4.2297029][-4.2766223 -4.2759571 -4.2434897 -4.1989226 -4.158565 -4.1278067 -4.0959334 -4.0773396 -4.0807152 -4.0974016 -4.1156936 -4.12897 -4.1615758 -4.2120295 -4.2590318][-4.3017416 -4.3048439 -4.2857203 -4.2616968 -4.2415028 -4.2235227 -4.2055779 -4.1990685 -4.1928916 -4.1929817 -4.2034788 -4.2104888 -4.2279081 -4.2591972 -4.2891726][-4.3150806 -4.3216581 -4.3146396 -4.3049068 -4.2982516 -4.2899666 -4.2826185 -4.28021 -4.2729578 -4.2697549 -4.2754383 -4.2785544 -4.2821841 -4.293601 -4.3080354][-4.3257108 -4.3322082 -4.3317842 -4.3303218 -4.3285637 -4.325562 -4.3230352 -4.3234386 -4.3207321 -4.31913 -4.3214884 -4.32103 -4.3179221 -4.3181248 -4.3216758][-4.3356533 -4.3373866 -4.3394136 -4.34129 -4.3432846 -4.3435187 -4.3423533 -4.3417339 -4.3420515 -4.3413696 -4.3412023 -4.3398733 -4.3363719 -4.333343 -4.3333793][-4.3383584 -4.3373437 -4.3388734 -4.3399372 -4.3411832 -4.3413529 -4.3406534 -4.3408747 -4.3429112 -4.3438077 -4.342772 -4.3414817 -4.33979 -4.3367777 -4.3352928]]...]
INFO - root - 2017-12-06 02:05:48.566424: step 60310, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 64h:16m:35s remains)
INFO - root - 2017-12-06 02:05:57.571583: step 60320, loss = 2.10, batch loss = 2.05 (8.8 examples/sec; 0.913 sec/batch; 69h:03m:06s remains)
INFO - root - 2017-12-06 02:06:06.791593: step 60330, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.927 sec/batch; 70h:06m:43s remains)
INFO - root - 2017-12-06 02:06:15.986434: step 60340, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 68h:24m:15s remains)
INFO - root - 2017-12-06 02:06:25.268957: step 60350, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 68h:12m:58s remains)
INFO - root - 2017-12-06 02:06:34.170296: step 60360, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.902 sec/batch; 68h:12m:56s remains)
INFO - root - 2017-12-06 02:06:43.160881: step 60370, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 64h:45m:43s remains)
INFO - root - 2017-12-06 02:06:52.339858: step 60380, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.895 sec/batch; 67h:40m:29s remains)
INFO - root - 2017-12-06 02:07:01.426238: step 60390, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 67h:07m:50s remains)
INFO - root - 2017-12-06 02:07:10.568864: step 60400, loss = 2.06, batch loss = 2.01 (8.6 examples/sec; 0.933 sec/batch; 70h:31m:36s remains)
2017-12-06 02:07:11.416414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2961507 -4.3081341 -4.3099279 -4.3047347 -4.3002639 -4.2845764 -4.2572627 -4.2258792 -4.1983137 -4.1893568 -4.2011752 -4.2214632 -4.2440186 -4.2614851 -4.2434816][-4.3157587 -4.3186188 -4.3122711 -4.3033204 -4.2925329 -4.2682552 -4.2347045 -4.198153 -4.1728067 -4.1750979 -4.1959152 -4.22171 -4.2532477 -4.2806544 -4.2685194][-4.3076081 -4.3084259 -4.2995954 -4.286726 -4.26597 -4.2305255 -4.1841507 -4.1328659 -4.113759 -4.1407342 -4.1864786 -4.2227612 -4.2635427 -4.2990365 -4.2972078][-4.2876468 -4.29028 -4.281683 -4.2655907 -4.2318687 -4.1795211 -4.1123452 -4.0402646 -4.0284252 -4.0909972 -4.1736021 -4.22575 -4.2701588 -4.3085151 -4.3154049][-4.2666121 -4.2700639 -4.2618756 -4.2425013 -4.1925335 -4.1149869 -4.0094166 -3.9059174 -3.8952372 -4.0073042 -4.136982 -4.2113037 -4.2657094 -4.309453 -4.322896][-4.2599716 -4.2605858 -4.2508731 -4.2233796 -4.1557093 -4.0456982 -3.8934639 -3.7509854 -3.7413626 -3.9109962 -4.0927067 -4.1947794 -4.2631826 -4.3091736 -4.3236418][-4.275013 -4.2687974 -4.2503695 -4.211565 -4.1274376 -3.9936168 -3.8127933 -3.6578672 -3.6687856 -3.8702109 -4.0723557 -4.1878448 -4.2590885 -4.3022366 -4.317162][-4.3005943 -4.2876062 -4.2602158 -4.2080584 -4.1122274 -3.9644439 -3.7798367 -3.6478174 -3.6923521 -3.8908615 -4.0819888 -4.1955118 -4.2610579 -4.298934 -4.3111734][-4.3216529 -4.3072433 -4.278163 -4.2204113 -4.1166515 -3.9683855 -3.7964096 -3.693229 -3.7583938 -3.9412038 -4.1125369 -4.2175479 -4.2735972 -4.30343 -4.308774][-4.3333468 -4.3231382 -4.2984262 -4.2430511 -4.1422067 -4.0048633 -3.8604095 -3.7837451 -3.8451922 -4.0027156 -4.1525807 -4.2440877 -4.2895145 -4.3127089 -4.3123069][-4.3352427 -4.3283873 -4.30726 -4.2579379 -4.1702747 -4.0558605 -3.9462705 -3.8956985 -3.9444923 -4.0630522 -4.1874766 -4.2660108 -4.3049865 -4.32414 -4.3209562][-4.3181438 -4.3142009 -4.2957745 -4.2551622 -4.191226 -4.109858 -4.0385022 -4.013504 -4.0523448 -4.1295619 -4.2199159 -4.2845206 -4.3183064 -4.3345757 -4.3327169][-4.2814341 -4.2789431 -4.2653751 -4.238997 -4.2019606 -4.1587734 -4.129509 -4.12489 -4.1513638 -4.1952524 -4.2528558 -4.29889 -4.3260336 -4.3409219 -4.3426013][-4.2302613 -4.2256336 -4.2188373 -4.2132039 -4.2037129 -4.193397 -4.1962919 -4.2072029 -4.2255745 -4.2502971 -4.2863221 -4.3152447 -4.3333011 -4.3452363 -4.34784][-4.1735258 -4.1663876 -4.16695 -4.1802297 -4.1980963 -4.2153978 -4.2367311 -4.2547154 -4.2682505 -4.2830667 -4.3087487 -4.3280983 -4.3392849 -4.3463531 -4.34694]]...]
INFO - root - 2017-12-06 02:07:20.544232: step 60410, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 68h:42m:32s remains)
INFO - root - 2017-12-06 02:07:29.777079: step 60420, loss = 2.05, batch loss = 1.99 (8.7 examples/sec; 0.922 sec/batch; 69h:41m:40s remains)
INFO - root - 2017-12-06 02:07:38.682960: step 60430, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 61h:57m:45s remains)
INFO - root - 2017-12-06 02:07:47.723321: step 60440, loss = 2.11, batch loss = 2.05 (8.7 examples/sec; 0.918 sec/batch; 69h:24m:43s remains)
INFO - root - 2017-12-06 02:07:56.903310: step 60450, loss = 2.08, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 69h:20m:17s remains)
INFO - root - 2017-12-06 02:08:05.982100: step 60460, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.893 sec/batch; 67h:27m:40s remains)
INFO - root - 2017-12-06 02:08:15.011227: step 60470, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 68h:06m:16s remains)
INFO - root - 2017-12-06 02:08:24.172269: step 60480, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 67h:07m:06s remains)
INFO - root - 2017-12-06 02:08:33.300094: step 60490, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.928 sec/batch; 70h:06m:59s remains)
INFO - root - 2017-12-06 02:08:42.456221: step 60500, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 68h:03m:52s remains)
2017-12-06 02:08:43.243412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22824 -4.2040048 -4.1765304 -4.1477022 -4.1223974 -4.0953255 -4.077981 -4.0879545 -4.132431 -4.1877608 -4.2224107 -4.242198 -4.2607265 -4.2799773 -4.28111][-4.2426095 -4.2165632 -4.1811814 -4.1438632 -4.108747 -4.0758533 -4.0583329 -4.0712218 -4.113543 -4.172318 -4.2160797 -4.2406893 -4.2572913 -4.2783217 -4.2839265][-4.2432919 -4.2158017 -4.1753244 -4.1305919 -4.08414 -4.0439787 -4.0313225 -4.0550628 -4.1012545 -4.1612687 -4.20421 -4.2300377 -4.2487135 -4.2702951 -4.2786345][-4.2415891 -4.2153087 -4.1685157 -4.11562 -4.0564885 -4.0067177 -3.9990146 -4.0349483 -4.090704 -4.1554508 -4.1980414 -4.2262769 -4.2473326 -4.2663703 -4.2732992][-4.2405033 -4.2207966 -4.1694508 -4.1057944 -4.02804 -3.9611082 -3.9581227 -4.0086932 -4.0810814 -4.1556783 -4.2029657 -4.2304235 -4.2507687 -4.2655683 -4.2686124][-4.238409 -4.2273121 -4.1745048 -4.0988936 -3.9994564 -3.9110653 -3.9066136 -3.9715889 -4.0614238 -4.1471806 -4.203393 -4.2348108 -4.2500968 -4.2551956 -4.253273][-4.2377887 -4.23237 -4.1834092 -4.1052594 -3.9940014 -3.8851049 -3.8704782 -3.9478955 -4.0538173 -4.1468906 -4.2045236 -4.2342482 -4.2444768 -4.2397828 -4.2314153][-4.2295609 -4.22688 -4.1900973 -4.12484 -4.0227871 -3.9130857 -3.8944178 -3.9761279 -4.0822315 -4.1626778 -4.208293 -4.2308 -4.2352929 -4.2248631 -4.2135944][-4.2056484 -4.2047505 -4.1857605 -4.14256 -4.0674973 -3.980226 -3.9646943 -4.0349836 -4.1212091 -4.1797762 -4.2079453 -4.2212219 -4.2206311 -4.2099433 -4.2003126][-4.1723552 -4.17249 -4.1729307 -4.1516953 -4.1045308 -4.0466051 -4.0303993 -4.0794444 -4.1396112 -4.1767926 -4.18934 -4.1931748 -4.1948881 -4.1937342 -4.1937952][-4.1495447 -4.1464825 -4.1623111 -4.1640029 -4.1434846 -4.1088972 -4.0849223 -4.1046495 -4.1336107 -4.1496592 -4.1487646 -4.1450405 -4.1554365 -4.1724033 -4.1885538][-4.1447568 -4.1357222 -4.1587114 -4.1822786 -4.1829553 -4.1647453 -4.1325502 -4.12346 -4.1262474 -4.1280088 -4.1191139 -4.1117244 -4.1331105 -4.1651731 -4.1935115][-4.1561837 -4.1446495 -4.1672144 -4.2031755 -4.2171588 -4.2121925 -4.1778092 -4.148881 -4.1375256 -4.1357808 -4.126523 -4.1207824 -4.1470776 -4.1844773 -4.2154894][-4.1615653 -4.1544552 -4.1808457 -4.2222114 -4.24224 -4.2437649 -4.2110386 -4.1735058 -4.1554046 -4.152463 -4.14538 -4.1440396 -4.1702642 -4.2049751 -4.233552][-4.1724014 -4.1672096 -4.1996608 -4.2440848 -4.2663536 -4.2691283 -4.2440205 -4.2089462 -4.1880269 -4.1829381 -4.1792135 -4.1813436 -4.2025733 -4.2294621 -4.2523031]]...]
INFO - root - 2017-12-06 02:08:52.243582: step 60510, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 68h:57m:13s remains)
INFO - root - 2017-12-06 02:09:01.388406: step 60520, loss = 2.08, batch loss = 2.02 (8.4 examples/sec; 0.950 sec/batch; 71h:45m:20s remains)
INFO - root - 2017-12-06 02:09:10.492450: step 60530, loss = 2.07, batch loss = 2.02 (8.7 examples/sec; 0.918 sec/batch; 69h:19m:06s remains)
INFO - root - 2017-12-06 02:09:19.445521: step 60540, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 64h:18m:41s remains)
INFO - root - 2017-12-06 02:09:28.546863: step 60550, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.912 sec/batch; 68h:52m:06s remains)
INFO - root - 2017-12-06 02:09:37.700640: step 60560, loss = 2.07, batch loss = 2.01 (8.6 examples/sec; 0.925 sec/batch; 69h:53m:45s remains)
INFO - root - 2017-12-06 02:09:46.735953: step 60570, loss = 2.03, batch loss = 1.97 (8.6 examples/sec; 0.931 sec/batch; 70h:17m:27s remains)
INFO - root - 2017-12-06 02:09:55.905042: step 60580, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 66h:14m:21s remains)
INFO - root - 2017-12-06 02:10:04.984439: step 60590, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 69h:20m:57s remains)
INFO - root - 2017-12-06 02:10:14.023703: step 60600, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.925 sec/batch; 69h:50m:34s remains)
2017-12-06 02:10:14.869362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2625613 -4.2604418 -4.2509761 -4.2389073 -4.2347827 -4.236619 -4.2440729 -4.2553744 -4.2640615 -4.2580009 -4.242137 -4.2201357 -4.2057118 -4.2139487 -4.2472143][-4.2643085 -4.2575583 -4.24194 -4.2224364 -4.2111206 -4.2088995 -4.2146063 -4.2271366 -4.2479954 -4.26085 -4.2659116 -4.2584419 -4.2501793 -4.2537136 -4.273664][-4.261097 -4.2497315 -4.2271256 -4.2026882 -4.1833811 -4.1709266 -4.1663175 -4.1703568 -4.1938958 -4.2251992 -4.2531581 -4.2657843 -4.2721691 -4.2818079 -4.2949238][-4.2449093 -4.2278872 -4.2003708 -4.1743851 -4.1513886 -4.131588 -4.1094623 -4.0946622 -4.10921 -4.1517091 -4.2018542 -4.233552 -4.2529755 -4.2717247 -4.2854333][-4.23429 -4.2090564 -4.1772294 -4.1519003 -4.1311584 -4.1047273 -4.0608754 -4.0165577 -4.0102906 -4.0560646 -4.12983 -4.1861758 -4.2176495 -4.2394032 -4.2517385][-4.2428875 -4.2104111 -4.1729813 -4.1441031 -4.1203752 -4.0756292 -3.9960501 -3.9037247 -3.8569748 -3.8990982 -4.0063558 -4.1053672 -4.1598654 -4.1903286 -4.2042351][-4.2679129 -4.2332354 -4.1891627 -4.15294 -4.1179209 -4.0496726 -3.9331207 -3.790252 -3.6917219 -3.7233646 -3.8615549 -4.0033107 -4.0852127 -4.13559 -4.1625533][-4.2946682 -4.2630749 -4.2185864 -4.1800265 -4.143259 -4.0760775 -3.9640331 -3.8303456 -3.7325175 -3.7496686 -3.8643742 -3.9900348 -4.0626836 -4.1146636 -4.1491823][-4.3137884 -4.2902446 -4.2528448 -4.2186 -4.1908708 -4.147645 -4.0756383 -3.9963408 -3.9384949 -3.9460824 -4.005558 -4.0769696 -4.1101923 -4.1384826 -4.1643248][-4.328928 -4.3132534 -4.28561 -4.2594352 -4.2393284 -4.2131233 -4.171639 -4.1276889 -4.0974307 -4.0995312 -4.12532 -4.1613846 -4.1686149 -4.1767893 -4.1922784][-4.3417411 -4.3330874 -4.313437 -4.2943697 -4.2771039 -4.2524996 -4.2209768 -4.1847782 -4.16142 -4.1613679 -4.1784339 -4.2080512 -4.2173023 -4.2234955 -4.2355204][-4.3474436 -4.3458042 -4.3356733 -4.3219252 -4.3061681 -4.2796445 -4.2467184 -4.2071114 -4.1781974 -4.1733494 -4.1916156 -4.2268009 -4.2500472 -4.2677164 -4.2844834][-4.3453631 -4.3506241 -4.3490753 -4.3401871 -4.3273849 -4.3046823 -4.2715726 -4.226625 -4.1880884 -4.1744838 -4.1916885 -4.2304811 -4.265439 -4.294179 -4.3154197][-4.3319888 -4.3432274 -4.34912 -4.3470941 -4.3374271 -4.3187947 -4.288599 -4.23921 -4.1932878 -4.173389 -4.1889181 -4.22846 -4.2698579 -4.304317 -4.326715][-4.3044643 -4.3233428 -4.3370209 -4.3420415 -4.3365445 -4.321178 -4.2938495 -4.2465634 -4.1974177 -4.1714768 -4.1836705 -4.220715 -4.2630091 -4.299016 -4.3218446]]...]
INFO - root - 2017-12-06 02:10:23.964173: step 60610, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.926 sec/batch; 69h:58m:23s remains)
INFO - root - 2017-12-06 02:10:32.867579: step 60620, loss = 2.07, batch loss = 2.02 (8.8 examples/sec; 0.904 sec/batch; 68h:17m:16s remains)
INFO - root - 2017-12-06 02:10:41.889403: step 60630, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 68h:19m:44s remains)
INFO - root - 2017-12-06 02:10:51.170218: step 60640, loss = 2.07, batch loss = 2.02 (8.5 examples/sec; 0.946 sec/batch; 71h:28m:31s remains)
INFO - root - 2017-12-06 02:11:00.314746: step 60650, loss = 2.04, batch loss = 1.98 (8.8 examples/sec; 0.905 sec/batch; 68h:22m:07s remains)
INFO - root - 2017-12-06 02:11:09.300349: step 60660, loss = 2.09, batch loss = 2.03 (8.4 examples/sec; 0.956 sec/batch; 72h:10m:18s remains)
INFO - root - 2017-12-06 02:11:18.295709: step 60670, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 67h:40m:56s remains)
INFO - root - 2017-12-06 02:11:27.512984: step 60680, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.904 sec/batch; 68h:16m:30s remains)
INFO - root - 2017-12-06 02:11:36.489592: step 60690, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 66h:48m:57s remains)
INFO - root - 2017-12-06 02:11:45.568833: step 60700, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.893 sec/batch; 67h:25m:58s remains)
2017-12-06 02:11:46.350750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3065825 -4.3118773 -4.3123593 -4.3135209 -4.315074 -4.3164897 -4.3167906 -4.3163109 -4.3159881 -4.31657 -4.3177257 -4.3181052 -4.3175726 -4.3161335 -4.3138571][-4.2941208 -4.3024831 -4.3043876 -4.3070269 -4.3104782 -4.3125691 -4.3120475 -4.3102155 -4.3096871 -4.3120675 -4.31514 -4.3160925 -4.3150649 -4.3125429 -4.3090048][-4.2616129 -4.2734208 -4.2788668 -4.28363 -4.2887263 -4.2908278 -4.2891555 -4.2859712 -4.2859097 -4.2904763 -4.2951865 -4.2962823 -4.2943182 -4.2902937 -4.2856312][-4.2220521 -4.2373509 -4.2467318 -4.2526751 -4.257165 -4.2578177 -4.2552581 -4.2511153 -4.250874 -4.2565918 -4.2620149 -4.2619214 -4.2570677 -4.2513571 -4.2475824][-4.1869073 -4.2042675 -4.2148752 -4.2187705 -4.2216907 -4.2225609 -4.22063 -4.2160335 -4.2153034 -4.2190409 -4.2210426 -4.2185793 -4.2131186 -4.2093043 -4.2107921][-4.1671824 -4.1863647 -4.1953859 -4.1936717 -4.1915946 -4.1926346 -4.1927104 -4.1905103 -4.1913924 -4.1929111 -4.1909008 -4.1880231 -4.1869636 -4.1908274 -4.2005258][-4.1589732 -4.1778584 -4.1817231 -4.1734324 -4.1681919 -4.1709747 -4.176662 -4.182476 -4.1890845 -4.1920881 -4.1893058 -4.1898284 -4.196569 -4.208405 -4.2233009][-4.1707458 -4.1860023 -4.1838765 -4.171845 -4.1657519 -4.1710281 -4.1847835 -4.2017193 -4.2173557 -4.2248111 -4.2252526 -4.2299881 -4.2397337 -4.252038 -4.2651544][-4.1923981 -4.2039213 -4.2001057 -4.1876841 -4.1812954 -4.1884804 -4.2081838 -4.2335114 -4.2575464 -4.2709918 -4.2747793 -4.2805047 -4.2881746 -4.2963943 -4.3047113][-4.2057681 -4.2209697 -4.2214661 -4.2093048 -4.1994839 -4.2040358 -4.2244143 -4.2536693 -4.2832923 -4.3005776 -4.3057413 -4.310174 -4.3149638 -4.3197536 -4.3239551][-4.2153864 -4.2354355 -4.2393527 -4.2271042 -4.2125773 -4.2125287 -4.23067 -4.2603765 -4.2908883 -4.3096652 -4.31546 -4.3183985 -4.3203645 -4.3226295 -4.3248057][-4.2255721 -4.2460361 -4.2500491 -4.23811 -4.2235541 -4.224781 -4.243422 -4.2701139 -4.2957625 -4.3106117 -4.3143826 -4.3151474 -4.3147955 -4.315546 -4.3175645][-4.241848 -4.2566781 -4.2573814 -4.2463322 -4.2356834 -4.2427998 -4.2631474 -4.2853303 -4.3036623 -4.3130364 -4.3141832 -4.3123927 -4.3096995 -4.3094482 -4.3119149][-4.254405 -4.2619214 -4.2601862 -4.2534604 -4.2507148 -4.2636023 -4.2836542 -4.3002748 -4.3116374 -4.3166347 -4.3159823 -4.3125329 -4.308629 -4.3074017 -4.30924][-4.256577 -4.2613516 -4.2637296 -4.2666316 -4.271297 -4.2852945 -4.301033 -4.3109045 -4.3165679 -4.3184571 -4.3168478 -4.3127646 -4.308455 -4.3065329 -4.3070931]]...]
INFO - root - 2017-12-06 02:11:55.389632: step 60710, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 67h:54m:55s remains)
INFO - root - 2017-12-06 02:12:04.474940: step 60720, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 66h:41m:31s remains)
INFO - root - 2017-12-06 02:12:13.460211: step 60730, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 65h:53m:40s remains)
INFO - root - 2017-12-06 02:12:22.501613: step 60740, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.911 sec/batch; 68h:45m:32s remains)
INFO - root - 2017-12-06 02:12:31.732544: step 60750, loss = 2.02, batch loss = 1.97 (8.7 examples/sec; 0.916 sec/batch; 69h:09m:04s remains)
