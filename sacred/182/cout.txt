INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "182"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-lastlr0.5-clip20
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-10 04:13:19.612563: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:13:19.612601: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:13:19.612607: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:13:19.612612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:13:19.612616: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
2017-12-10 04:13:25.021667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-10 04:13:25.021702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 04:13:25.021708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 04:13:25.021716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-10 04:13:41.354875: step 0, loss = 2.28, batch loss = 2.23 (0.8 examples/sec; 10.406 sec/batch; 961h:07m:58s remains)
2017-12-10 04:13:42.177623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290228 -4.4290137 -4.4290032 -4.4289856 -4.4289627 -4.4289365 -4.428906 -4.4288712 -4.428854 -4.4288583 -4.42888 -4.4289117 -4.4289484 -4.4289846 -4.4290123][-4.4290185 -4.4290051 -4.4289827 -4.4289517 -4.4289122 -4.4288645 -4.4288125 -4.4287634 -4.4287486 -4.4287648 -4.4288011 -4.4288526 -4.428906 -4.428956 -4.4289966][-4.4290133 -4.4289956 -4.4289646 -4.4289169 -4.428853 -4.4287753 -4.4286952 -4.4286203 -4.4286 -4.4286327 -4.4286976 -4.4287806 -4.4288568 -4.4289212 -4.4289761][-4.4290028 -4.4289889 -4.4289575 -4.4288917 -4.4287982 -4.4286857 -4.428566 -4.4284463 -4.428412 -4.4284658 -4.4285717 -4.4287 -4.4288063 -4.4288878 -4.4289556][-4.4289842 -4.4289804 -4.4289508 -4.428864 -4.4287353 -4.42858 -4.4284062 -4.4282284 -4.428184 -4.4282775 -4.4284391 -4.4286246 -4.4287682 -4.4288688 -4.4289455][-4.4289589 -4.4289613 -4.42893 -4.4288211 -4.4286609 -4.4284563 -4.4282103 -4.4279613 -4.4279332 -4.4281039 -4.42834 -4.4285774 -4.4287496 -4.4288669 -4.428947][-4.4289289 -4.4289265 -4.4288855 -4.4287624 -4.4285793 -4.4283314 -4.4280167 -4.4277205 -4.4277654 -4.4280357 -4.4283257 -4.4285769 -4.4287515 -4.4288726 -4.4289508][-4.4288921 -4.4288683 -4.4288082 -4.4286833 -4.4285083 -4.4282656 -4.4279571 -4.4277077 -4.4278445 -4.4281507 -4.4284234 -4.4286389 -4.4287848 -4.4288907 -4.42896][-4.4288545 -4.4288111 -4.4287448 -4.4286408 -4.4285107 -4.4283319 -4.4281173 -4.4279909 -4.4281554 -4.4283934 -4.428586 -4.4287324 -4.4288359 -4.4289165 -4.4289737][-4.4288225 -4.4287691 -4.428709 -4.4286342 -4.4285531 -4.4284511 -4.4283385 -4.4283023 -4.4284348 -4.4285865 -4.4287038 -4.4287944 -4.428865 -4.4289317 -4.4289823][-4.4287968 -4.4287415 -4.4286895 -4.4286389 -4.4285913 -4.4285431 -4.4284978 -4.4285059 -4.4285984 -4.4286928 -4.4287629 -4.4288244 -4.4288821 -4.4289427 -4.4289885][-4.4287796 -4.4287348 -4.4286995 -4.4286733 -4.4286532 -4.4286437 -4.4286385 -4.4286594 -4.4287124 -4.428761 -4.4287987 -4.4288468 -4.4289007 -4.428957 -4.4289961][-4.42879 -4.4287653 -4.428751 -4.4287472 -4.4287486 -4.4287572 -4.4287596 -4.4287658 -4.4287786 -4.428792 -4.428812 -4.4288578 -4.4289126 -4.4289656 -4.4289994][-4.4288239 -4.4288211 -4.4288244 -4.4288321 -4.428834 -4.4288321 -4.4288177 -4.4287987 -4.428782 -4.4287772 -4.4287939 -4.4288454 -4.4289069 -4.4289594 -4.4289932][-4.4288449 -4.428853 -4.4288673 -4.4288797 -4.4288735 -4.4288492 -4.4288077 -4.4287658 -4.4287391 -4.4287353 -4.4287586 -4.4288225 -4.4288926 -4.428946 -4.4289842]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-lastlr0.5-clip20/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-lastlr0.5-clip20/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 04:13:48.483470: step 10, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:47m:33s remains)
INFO - root - 2017-12-10 04:13:53.715075: step 20, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.540 sec/batch; 49h:53m:18s remains)
INFO - root - 2017-12-10 04:13:59.018635: step 30, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:14m:12s remains)
INFO - root - 2017-12-10 04:14:04.331308: step 40, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:29m:03s remains)
INFO - root - 2017-12-10 04:14:09.591871: step 50, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.505 sec/batch; 46h:39m:34s remains)
INFO - root - 2017-12-10 04:14:14.557196: step 60, loss = 2.28, batch loss = 2.23 (18.8 examples/sec; 0.425 sec/batch; 39h:12m:29s remains)
INFO - root - 2017-12-10 04:14:19.723631: step 70, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:37m:34s remains)
INFO - root - 2017-12-10 04:14:24.986176: step 80, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:30m:25s remains)
INFO - root - 2017-12-10 04:14:30.224344: step 90, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.518 sec/batch; 47h:49m:00s remains)
INFO - root - 2017-12-10 04:14:35.511442: step 100, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:53m:06s remains)
2017-12-10 04:14:36.024060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288812 -4.4288144 -4.4287267 -4.4286389 -4.428596 -4.4286246 -4.4286723 -4.4287171 -4.4287844 -4.4288325 -4.4288425 -4.428854 -4.4288526 -4.4288082 -4.4287472][-4.4288893 -4.4288135 -4.4287095 -4.428597 -4.4285316 -4.4285517 -4.4286003 -4.4286461 -4.4287071 -4.4287562 -4.4287772 -4.4288025 -4.4288125 -4.4287815 -4.4287343][-4.428895 -4.428812 -4.4287028 -4.4285865 -4.4285178 -4.4285274 -4.4285684 -4.4286084 -4.4286613 -4.4287133 -4.4287405 -4.4287634 -4.4287729 -4.428751 -4.4287128][-4.428896 -4.42881 -4.4287009 -4.42859 -4.4285231 -4.4285192 -4.4285469 -4.4285827 -4.428637 -4.4286957 -4.4287291 -4.4287457 -4.42875 -4.4287353 -4.4287028][-4.4288945 -4.4288077 -4.4287024 -4.4285975 -4.4285288 -4.42851 -4.4285212 -4.4285483 -4.4286 -4.4286623 -4.4287033 -4.4287186 -4.42872 -4.4287114 -4.42869][-4.4288907 -4.4288063 -4.4287076 -4.4286089 -4.4285331 -4.4284968 -4.4284945 -4.4285035 -4.4285331 -4.4285889 -4.4286408 -4.4286633 -4.4286704 -4.4286742 -4.428668][-4.4288869 -4.4288054 -4.4287119 -4.4286127 -4.4285274 -4.4284759 -4.4284616 -4.4284487 -4.4284453 -4.42849 -4.4285603 -4.428596 -4.4286151 -4.4286323 -4.42864][-4.4288826 -4.4288011 -4.4287071 -4.4286089 -4.428524 -4.4284739 -4.4284606 -4.4284387 -4.4284153 -4.4284506 -4.4285293 -4.4285731 -4.4285979 -4.4286227 -4.4286375][-4.4288797 -4.4287996 -4.42871 -4.428617 -4.4285417 -4.428504 -4.4284973 -4.4284816 -4.428463 -4.4285 -4.4285727 -4.4286056 -4.428617 -4.4286356 -4.4286451][-4.4288812 -4.428802 -4.4287138 -4.4286232 -4.4285512 -4.4285178 -4.4285135 -4.42851 -4.4285111 -4.4285593 -4.4286246 -4.4286451 -4.4286437 -4.428648 -4.428647][-4.4288926 -4.4288158 -4.4287286 -4.42864 -4.4285703 -4.4285364 -4.4285297 -4.428535 -4.4285483 -4.428596 -4.42865 -4.4286628 -4.4286551 -4.4286513 -4.428647][-4.4289126 -4.4288416 -4.428761 -4.4286804 -4.4286203 -4.4285908 -4.4285865 -4.4286013 -4.4286208 -4.42866 -4.4286962 -4.4287028 -4.4286938 -4.42869 -4.4286857][-4.428937 -4.4288759 -4.4288073 -4.4287386 -4.4286876 -4.4286628 -4.428668 -4.4286957 -4.4287262 -4.4287562 -4.4287748 -4.4287748 -4.4287639 -4.4287572 -4.4287519][-4.4289689 -4.4289227 -4.4288721 -4.4288192 -4.4287758 -4.4287562 -4.4287686 -4.428803 -4.4288359 -4.4288578 -4.4288659 -4.42886 -4.4288468 -4.4288349 -4.4288287][-4.4289989 -4.42897 -4.4289403 -4.4289107 -4.4288816 -4.4288659 -4.4288764 -4.4289026 -4.4289265 -4.4289379 -4.4289389 -4.4289331 -4.4289231 -4.4289126 -4.4289088]]...]
INFO - root - 2017-12-10 04:14:41.215383: step 110, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:33m:25s remains)
INFO - root - 2017-12-10 04:14:46.390143: step 120, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:42m:25s remains)
INFO - root - 2017-12-10 04:14:51.549066: step 130, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.518 sec/batch; 47h:48m:50s remains)
INFO - root - 2017-12-10 04:14:56.799163: step 140, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:29m:10s remains)
INFO - root - 2017-12-10 04:15:02.036717: step 150, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:43m:39s remains)
INFO - root - 2017-12-10 04:15:07.109868: step 160, loss = 2.28, batch loss = 2.23 (25.8 examples/sec; 0.310 sec/batch; 28h:34m:40s remains)
INFO - root - 2017-12-10 04:15:12.239655: step 170, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:39m:44s remains)
INFO - root - 2017-12-10 04:15:17.497981: step 180, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.538 sec/batch; 49h:38m:19s remains)
INFO - root - 2017-12-10 04:15:22.818749: step 190, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:58m:46s remains)
INFO - root - 2017-12-10 04:15:28.090268: step 200, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 49h:18m:14s remains)
2017-12-10 04:15:28.646014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290051 -4.428997 -4.4289923 -4.4289875 -4.4289856 -4.4289827 -4.4289775 -4.4289727 -4.4289713 -4.4289746 -4.4289808 -4.428988 -4.4289947 -4.4289994 -4.4290037][-4.428977 -4.4289641 -4.4289527 -4.4289451 -4.428946 -4.4289489 -4.4289455 -4.42894 -4.4289351 -4.4289374 -4.4289441 -4.4289536 -4.4289632 -4.4289718 -4.42898][-4.4289393 -4.4289179 -4.4288969 -4.428885 -4.4288931 -4.4289088 -4.4289122 -4.4289031 -4.4288907 -4.4288888 -4.4288955 -4.4289126 -4.4289312 -4.428947 -4.4289594][-4.4289012 -4.4288692 -4.4288416 -4.4288311 -4.42885 -4.4288778 -4.428885 -4.428865 -4.4288316 -4.4288163 -4.4288263 -4.4288549 -4.4288869 -4.4289145 -4.4289355][-4.428885 -4.4288425 -4.4288087 -4.428803 -4.4288282 -4.4288521 -4.4288511 -4.4288144 -4.4287548 -4.4287257 -4.4287434 -4.4287891 -4.4288354 -4.4288769 -4.4289093][-4.4288821 -4.4288349 -4.4287996 -4.428791 -4.4288082 -4.428813 -4.428791 -4.4287229 -4.4286366 -4.4286137 -4.428658 -4.4287281 -4.4287906 -4.4288473 -4.4288931][-4.4288654 -4.4288173 -4.4287772 -4.4287572 -4.4287534 -4.42873 -4.428669 -4.4285517 -4.4284487 -4.4284635 -4.4285626 -4.4286685 -4.4287496 -4.4288235 -4.4288816][-4.4288259 -4.428772 -4.4287171 -4.4286714 -4.4286413 -4.428587 -4.4284744 -4.4283075 -4.428206 -4.4282985 -4.4284697 -4.4286094 -4.4287071 -4.4287972 -4.4288692][-4.42878 -4.4287224 -4.4286528 -4.4285831 -4.4285293 -4.4284449 -4.4283018 -4.4281173 -4.4280443 -4.4282026 -4.4284172 -4.428576 -4.428688 -4.4287906 -4.42887][-4.4287291 -4.4286733 -4.4286041 -4.4285369 -4.4284883 -4.42842 -4.4283051 -4.4281788 -4.4281564 -4.4283004 -4.4284816 -4.4286175 -4.4287229 -4.4288177 -4.4288917][-4.4287038 -4.4286551 -4.4286051 -4.4285631 -4.4285512 -4.4285297 -4.4284711 -4.4284058 -4.4284043 -4.4285 -4.4286189 -4.4287138 -4.4287949 -4.4288659 -4.4289236][-4.4287243 -4.4286895 -4.428669 -4.4286633 -4.4286866 -4.4287004 -4.428678 -4.4286447 -4.4286456 -4.4287 -4.4287682 -4.4288254 -4.4288754 -4.4289184 -4.4289579][-4.4287844 -4.4287648 -4.428772 -4.4287944 -4.4288273 -4.4288435 -4.4288344 -4.4288173 -4.4288182 -4.4288478 -4.428885 -4.4289165 -4.4289412 -4.4289641 -4.428988][-4.4288645 -4.4288549 -4.428875 -4.428906 -4.4289293 -4.4289351 -4.4289255 -4.428915 -4.4289179 -4.4289322 -4.4289465 -4.4289584 -4.4289722 -4.4289889 -4.4290061][-4.4289312 -4.4289274 -4.4289422 -4.4289618 -4.4289727 -4.42897 -4.4289603 -4.4289579 -4.4289656 -4.4289722 -4.4289708 -4.4289713 -4.4289813 -4.4289985 -4.4290137]]...]
INFO - root - 2017-12-10 04:15:33.812785: step 210, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.504 sec/batch; 46h:30m:19s remains)
INFO - root - 2017-12-10 04:15:39.116961: step 220, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.510 sec/batch; 47h:04m:41s remains)
INFO - root - 2017-12-10 04:15:44.375001: step 230, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:43m:13s remains)
INFO - root - 2017-12-10 04:15:49.634425: step 240, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:42m:11s remains)
INFO - root - 2017-12-10 04:15:54.928163: step 250, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 48h:04m:39s remains)
INFO - root - 2017-12-10 04:15:59.993085: step 260, loss = 2.28, batch loss = 2.23 (24.6 examples/sec; 0.326 sec/batch; 30h:04m:19s remains)
INFO - root - 2017-12-10 04:16:05.218272: step 270, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 48h:15m:16s remains)
INFO - root - 2017-12-10 04:16:10.424849: step 280, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:55m:29s remains)
INFO - root - 2017-12-10 04:16:15.626256: step 290, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.507 sec/batch; 46h:46m:23s remains)
INFO - root - 2017-12-10 04:16:20.807132: step 300, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 49h:09m:16s remains)
2017-12-10 04:16:21.312496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287305 -4.4287357 -4.4287562 -4.4288087 -4.4288445 -4.4288263 -4.4287744 -4.428699 -4.4286242 -4.4286122 -4.4286609 -4.4287524 -4.4288306 -4.4288774 -4.4288974][-4.4287639 -4.4287825 -4.4288049 -4.4288464 -4.4288707 -4.4288483 -4.4287977 -4.4287133 -4.428618 -4.4285903 -4.4286356 -4.42873 -4.4288135 -4.4288669 -4.4288921][-4.4287925 -4.4288244 -4.42885 -4.4288778 -4.4288926 -4.4288683 -4.4288135 -4.4287252 -4.428617 -4.4285793 -4.4286261 -4.4287224 -4.4288125 -4.4288688 -4.4288926][-4.4287953 -4.4288349 -4.4288669 -4.4288969 -4.4289083 -4.4288774 -4.4288106 -4.4287143 -4.4286027 -4.4285693 -4.4286218 -4.4287233 -4.4288177 -4.4288745 -4.428896][-4.4287944 -4.4288325 -4.4288635 -4.4288988 -4.4289141 -4.4288731 -4.4287896 -4.4286785 -4.4285669 -4.4285517 -4.4286175 -4.4287233 -4.4288216 -4.42888 -4.4289002][-4.4288139 -4.4288406 -4.4288616 -4.428894 -4.4289064 -4.4288592 -4.4287672 -4.4286461 -4.4285359 -4.428535 -4.4286127 -4.4287224 -4.4288244 -4.4288869 -4.4289055][-4.4288235 -4.4288435 -4.4288588 -4.4288859 -4.4288931 -4.4288445 -4.4287553 -4.4286366 -4.4285316 -4.4285364 -4.4286218 -4.4287324 -4.4288325 -4.4288945 -4.42891][-4.4288077 -4.4288273 -4.4288335 -4.4288545 -4.4288554 -4.4288073 -4.4287238 -4.4286137 -4.4285169 -4.42853 -4.4286289 -4.4287462 -4.428843 -4.4289012 -4.4289126][-4.4287691 -4.4287839 -4.42878 -4.4287977 -4.4287949 -4.4287415 -4.4286594 -4.4285564 -4.4284754 -4.4285064 -4.4286237 -4.4287519 -4.4288497 -4.4289055 -4.4289131][-4.4287586 -4.4287658 -4.4287615 -4.4287763 -4.4287629 -4.428699 -4.428616 -4.4285192 -4.4284506 -4.4284911 -4.4286132 -4.4287438 -4.4288468 -4.4289017 -4.4289093][-4.4287834 -4.4287906 -4.428791 -4.4287944 -4.4287658 -4.4286966 -4.4286146 -4.4285254 -4.4284682 -4.4285026 -4.4286122 -4.42874 -4.428844 -4.4288969 -4.4289074][-4.4288073 -4.4288158 -4.4288135 -4.4288058 -4.4287782 -4.428719 -4.42865 -4.4285803 -4.4285364 -4.42856 -4.4286456 -4.4287596 -4.4288549 -4.4289017 -4.4289122][-4.4288349 -4.4288425 -4.4288368 -4.4288282 -4.4288025 -4.4287496 -4.428688 -4.4286313 -4.4285951 -4.4286089 -4.428678 -4.4287786 -4.428864 -4.4289074 -4.4289188][-4.4288645 -4.4288645 -4.4288578 -4.4288511 -4.4288254 -4.4287658 -4.4286971 -4.4286313 -4.4285865 -4.4285955 -4.4286685 -4.42877 -4.4288559 -4.4289031 -4.4289203][-4.4288893 -4.4288831 -4.4288769 -4.4288726 -4.4288511 -4.4287944 -4.4287157 -4.4286294 -4.4285674 -4.4285746 -4.4286566 -4.4287629 -4.4288483 -4.4288979 -4.4289188]]...]
INFO - root - 2017-12-10 04:16:26.542550: step 310, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:48m:07s remains)
INFO - root - 2017-12-10 04:16:31.747391: step 320, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.546 sec/batch; 50h:22m:58s remains)
INFO - root - 2017-12-10 04:16:36.973133: step 330, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.525 sec/batch; 48h:24m:10s remains)
INFO - root - 2017-12-10 04:16:42.304366: step 340, loss = 2.28, batch loss = 2.23 (14.1 examples/sec; 0.567 sec/batch; 52h:19m:11s remains)
INFO - root - 2017-12-10 04:16:47.676711: step 350, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:50m:40s remains)
INFO - root - 2017-12-10 04:16:52.701102: step 360, loss = 2.28, batch loss = 2.23 (26.0 examples/sec; 0.307 sec/batch; 28h:20m:43s remains)
INFO - root - 2017-12-10 04:16:57.843546: step 370, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:52m:11s remains)
INFO - root - 2017-12-10 04:17:02.965972: step 380, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 48h:03m:57s remains)
INFO - root - 2017-12-10 04:17:08.203194: step 390, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 49h:25m:12s remains)
INFO - root - 2017-12-10 04:17:13.461602: step 400, loss = 2.28, batch loss = 2.23 (14.4 examples/sec; 0.554 sec/batch; 51h:09m:08s remains)
2017-12-10 04:17:13.996842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289207 -4.4289074 -4.4288926 -4.4288845 -4.4288788 -4.4288774 -4.42888 -4.4288888 -4.4288988 -4.4289036 -4.4288988 -4.4288859 -4.4288673 -4.4288511 -4.428843][-4.428937 -4.4289227 -4.4289031 -4.4288874 -4.4288797 -4.4288845 -4.4289 -4.4289227 -4.428946 -4.4289603 -4.4289613 -4.4289436 -4.4289079 -4.428864 -4.4288282][-4.4289303 -4.4289145 -4.4288898 -4.4288645 -4.4288516 -4.428853 -4.42887 -4.4289041 -4.4289451 -4.428977 -4.4289956 -4.428987 -4.4289479 -4.4288864 -4.4288211][-4.4289188 -4.4288926 -4.4288559 -4.4288163 -4.428793 -4.4287825 -4.4287891 -4.4288239 -4.4288793 -4.4289346 -4.4289808 -4.4289956 -4.4289722 -4.4289083 -4.428822][-4.4289174 -4.4288712 -4.428813 -4.4287558 -4.4287195 -4.4286914 -4.4286847 -4.4287124 -4.4287696 -4.4288421 -4.4289107 -4.4289508 -4.4289565 -4.4289112 -4.4288163][-4.4289203 -4.4288573 -4.4287715 -4.4286857 -4.428628 -4.4285741 -4.4285431 -4.428556 -4.4286141 -4.4287128 -4.4288116 -4.4288764 -4.4289093 -4.4288898 -4.4288058][-4.4289231 -4.4288545 -4.4287472 -4.4286308 -4.4285417 -4.42845 -4.4283705 -4.4283447 -4.4284072 -4.4285455 -4.428689 -4.4287896 -4.42885 -4.428865 -4.4288182][-4.4289317 -4.4288688 -4.4287591 -4.4286256 -4.4285035 -4.4283638 -4.4282165 -4.4281244 -4.4281807 -4.4283614 -4.4285455 -4.4286776 -4.4287643 -4.4288187 -4.4288335][-4.4289484 -4.4289017 -4.4288158 -4.4286971 -4.4285688 -4.4284086 -4.4282126 -4.4280415 -4.4280486 -4.428225 -4.4284143 -4.4285564 -4.4286604 -4.4287515 -4.4288259][-4.4289651 -4.4289403 -4.4288926 -4.4288163 -4.4287224 -4.4285989 -4.4284234 -4.428247 -4.428195 -4.428278 -4.428391 -4.4284911 -4.4285874 -4.4286947 -4.428802][-4.428978 -4.4289646 -4.4289451 -4.4289112 -4.4288683 -4.4288068 -4.4286976 -4.4285731 -4.4285059 -4.4285116 -4.4285398 -4.428576 -4.4286337 -4.4287186 -4.4288158][-4.4289861 -4.4289756 -4.4289651 -4.4289546 -4.4289446 -4.4289284 -4.428885 -4.4288278 -4.4287858 -4.4287663 -4.4287539 -4.4287534 -4.4287734 -4.4288192 -4.4288812][-4.4289927 -4.4289861 -4.4289775 -4.4289737 -4.4289727 -4.4289732 -4.4289651 -4.4289527 -4.4289412 -4.4289336 -4.42892 -4.4289064 -4.428906 -4.4289203 -4.4289494][-4.4290018 -4.428998 -4.4289927 -4.4289894 -4.4289904 -4.4289932 -4.4289937 -4.4289927 -4.4289937 -4.4289961 -4.4289942 -4.4289885 -4.4289837 -4.4289846 -4.4289927][-4.4290094 -4.4290051 -4.4290009 -4.4289966 -4.4289994 -4.4290037 -4.4290056 -4.4290051 -4.4290066 -4.4290123 -4.4290147 -4.4290156 -4.4290152 -4.4290142 -4.4290147]]...]
INFO - root - 2017-12-10 04:17:19.210680: step 410, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.511 sec/batch; 47h:10m:20s remains)
INFO - root - 2017-12-10 04:17:24.418621: step 420, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:56m:25s remains)
INFO - root - 2017-12-10 04:17:29.646926: step 430, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.510 sec/batch; 47h:04m:29s remains)
INFO - root - 2017-12-10 04:17:35.001145: step 440, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.547 sec/batch; 50h:26m:50s remains)
INFO - root - 2017-12-10 04:17:40.285737: step 450, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:40m:06s remains)
INFO - root - 2017-12-10 04:17:45.421656: step 460, loss = 2.28, batch loss = 2.23 (23.7 examples/sec; 0.338 sec/batch; 31h:10m:20s remains)
INFO - root - 2017-12-10 04:17:50.532341: step 470, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:27m:20s remains)
INFO - root - 2017-12-10 04:17:55.715691: step 480, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 48h:15m:09s remains)
INFO - root - 2017-12-10 04:18:00.999817: step 490, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:50m:26s remains)
INFO - root - 2017-12-10 04:18:06.152832: step 500, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:53m:49s remains)
2017-12-10 04:18:06.673093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288025 -4.4287953 -4.4287939 -4.4287839 -4.4287605 -4.4287519 -4.4287596 -4.428781 -4.4288106 -4.4288311 -4.4288259 -4.4287872 -4.4287252 -4.4286704 -4.4286089][-4.42876 -4.4287505 -4.4287491 -4.4287305 -4.4286971 -4.4286952 -4.4287186 -4.4287643 -4.4288149 -4.428834 -4.4288106 -4.4287534 -4.428678 -4.4286146 -4.4285531][-4.4287381 -4.42872 -4.4287057 -4.4286685 -4.4286246 -4.4286304 -4.4286742 -4.4287462 -4.4288106 -4.4288263 -4.4287853 -4.4287152 -4.4286337 -4.428566 -4.4285197][-4.4287257 -4.4287033 -4.42867 -4.4286046 -4.4285436 -4.4285407 -4.4285874 -4.4286723 -4.4287577 -4.4287767 -4.4287281 -4.4286571 -4.4285884 -4.4285383 -4.4285212][-4.4287081 -4.4286814 -4.4286246 -4.4285288 -4.4284472 -4.4284196 -4.4284477 -4.4285316 -4.4286327 -4.4286618 -4.4286137 -4.428565 -4.4285412 -4.4285331 -4.4285564][-4.4286861 -4.4286585 -4.4285874 -4.4284716 -4.4283605 -4.4282851 -4.4282637 -4.4283204 -4.4284372 -4.4284921 -4.4284649 -4.4284692 -4.4285073 -4.4285421 -4.4285884][-4.4286528 -4.4286275 -4.4285641 -4.42845 -4.4283104 -4.4281659 -4.4280457 -4.4280472 -4.4282041 -4.4283285 -4.4283614 -4.4284267 -4.4284968 -4.4285436 -4.4285941][-4.4286075 -4.4285846 -4.428534 -4.428443 -4.428308 -4.4281306 -4.42794 -4.4278884 -4.4280748 -4.4282589 -4.4283552 -4.4284611 -4.4285345 -4.4285645 -4.4285955][-4.4285626 -4.4285355 -4.4284992 -4.4284511 -4.4283628 -4.428225 -4.4280624 -4.4280138 -4.4281535 -4.428309 -4.4284115 -4.4285221 -4.4285841 -4.428596 -4.4286041][-4.4285245 -4.4285159 -4.4285126 -4.4285164 -4.4284887 -4.4284124 -4.4283051 -4.4282713 -4.4283385 -4.4284148 -4.4284725 -4.4285541 -4.4286079 -4.4286089 -4.4286][-4.4285369 -4.4285622 -4.4285941 -4.4286242 -4.4286232 -4.4285827 -4.4285121 -4.4284754 -4.4284811 -4.4284878 -4.4284892 -4.4285359 -4.4285812 -4.4285865 -4.4285822][-4.4286213 -4.428659 -4.428688 -4.4287105 -4.4287105 -4.4286861 -4.4286366 -4.4285874 -4.4285531 -4.4285212 -4.4284949 -4.42852 -4.4285645 -4.4285879 -4.4286084][-4.4287133 -4.4287391 -4.4287543 -4.4287634 -4.4287572 -4.4287419 -4.4287057 -4.4286542 -4.4286084 -4.4285755 -4.4285588 -4.4285746 -4.4286146 -4.428648 -4.4286804][-4.4287562 -4.4287682 -4.4287786 -4.4287815 -4.428771 -4.4287615 -4.4287362 -4.4286985 -4.4286714 -4.4286656 -4.4286685 -4.4286728 -4.4286942 -4.4287171 -4.4287324][-4.4287696 -4.4287734 -4.4287825 -4.4287844 -4.4287791 -4.4287748 -4.4287581 -4.4287415 -4.4287386 -4.4287548 -4.4287696 -4.4287663 -4.4287686 -4.428772 -4.4287643]]...]
INFO - root - 2017-12-10 04:18:11.865768: step 510, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:53m:10s remains)
INFO - root - 2017-12-10 04:18:16.994989: step 520, loss = 2.28, batch loss = 2.23 (16.0 examples/sec; 0.499 sec/batch; 46h:03m:22s remains)
INFO - root - 2017-12-10 04:18:22.236043: step 530, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.547 sec/batch; 50h:27m:18s remains)
INFO - root - 2017-12-10 04:18:27.508253: step 540, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 49h:10m:49s remains)
INFO - root - 2017-12-10 04:18:32.699531: step 550, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:24m:31s remains)
INFO - root - 2017-12-10 04:18:37.926426: step 560, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:18m:41s remains)
INFO - root - 2017-12-10 04:18:42.739542: step 570, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.505 sec/batch; 46h:32m:48s remains)
INFO - root - 2017-12-10 04:18:47.908158: step 580, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.502 sec/batch; 46h:18m:49s remains)
INFO - root - 2017-12-10 04:18:53.110829: step 590, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:52m:13s remains)
INFO - root - 2017-12-10 04:18:58.333625: step 600, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 48h:11m:55s remains)
2017-12-10 04:18:58.841206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285736 -4.4284854 -4.428452 -4.4284997 -4.4285917 -4.4286308 -4.4286151 -4.4285712 -4.4285741 -4.4286222 -4.4287043 -4.4287987 -4.4288731 -4.4289255 -4.4289584][-4.4285407 -4.4284611 -4.4284363 -4.4284925 -4.4285927 -4.428638 -4.4286313 -4.4285975 -4.4285932 -4.4286294 -4.4287028 -4.4287891 -4.4288588 -4.4289136 -4.428947][-4.4285483 -4.4284744 -4.4284534 -4.42851 -4.4286032 -4.4286432 -4.4286385 -4.4286203 -4.428627 -4.4286685 -4.4287281 -4.4287906 -4.4288349 -4.4288769 -4.428905][-4.4285889 -4.4285154 -4.4284859 -4.4285254 -4.428596 -4.4286275 -4.4286313 -4.4286346 -4.4286618 -4.4287109 -4.4287586 -4.428793 -4.4288144 -4.4288387 -4.4288597][-4.4286127 -4.4285321 -4.4284782 -4.4284821 -4.4285207 -4.4285531 -4.42858 -4.4286151 -4.4286695 -4.4287271 -4.4287715 -4.4287939 -4.4288058 -4.4288273 -4.4288459][-4.4285913 -4.4284849 -4.4283962 -4.4283671 -4.4283872 -4.428431 -4.4284892 -4.4285555 -4.4286337 -4.428709 -4.4287629 -4.428792 -4.428813 -4.4288406 -4.4288659][-4.4285769 -4.4284472 -4.4283295 -4.4282732 -4.4282804 -4.4283342 -4.4284081 -4.4284945 -4.4285903 -4.4286819 -4.4287505 -4.428792 -4.428823 -4.4288578 -4.4288883][-4.4285941 -4.4284844 -4.4283743 -4.4283166 -4.4283214 -4.4283695 -4.4284296 -4.4285059 -4.4285979 -4.4286866 -4.4287572 -4.4288039 -4.42884 -4.4288759 -4.428906][-4.4286547 -4.4285884 -4.428525 -4.4284945 -4.4285064 -4.4285445 -4.4285822 -4.4286327 -4.4286933 -4.4287505 -4.4288025 -4.4288421 -4.428874 -4.4289031 -4.4289231][-4.4287319 -4.4286895 -4.4286618 -4.4286571 -4.4286809 -4.4287219 -4.4287567 -4.42879 -4.4288206 -4.4288454 -4.4288697 -4.4288926 -4.4289126 -4.4289308 -4.4289422][-4.428793 -4.4287572 -4.4287434 -4.4287472 -4.4287767 -4.4288211 -4.4288597 -4.4288864 -4.428905 -4.4289141 -4.4289207 -4.4289289 -4.4289384 -4.428947 -4.4289522][-4.4288459 -4.428812 -4.4287982 -4.4287977 -4.4288239 -4.4288597 -4.4288907 -4.4289079 -4.4289169 -4.4289188 -4.4289212 -4.4289308 -4.4289403 -4.4289479 -4.4289541][-4.4288979 -4.428864 -4.4288449 -4.4288406 -4.4288573 -4.428875 -4.4288917 -4.4288988 -4.4289017 -4.4288993 -4.4289017 -4.4289184 -4.4289327 -4.4289422 -4.4289513][-4.42894 -4.428915 -4.4288979 -4.428896 -4.4289055 -4.4289117 -4.4289203 -4.4289207 -4.4289174 -4.4289122 -4.4289122 -4.4289308 -4.4289446 -4.4289508 -4.4289589][-4.428956 -4.4289436 -4.4289331 -4.4289317 -4.428937 -4.4289422 -4.4289527 -4.4289541 -4.42895 -4.4289474 -4.4289489 -4.4289656 -4.4289751 -4.4289765 -4.42898]]...]
INFO - root - 2017-12-10 04:19:03.960963: step 610, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.505 sec/batch; 46h:34m:00s remains)
INFO - root - 2017-12-10 04:19:09.130060: step 620, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 47h:10m:20s remains)
INFO - root - 2017-12-10 04:19:14.300308: step 630, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:27m:33s remains)
INFO - root - 2017-12-10 04:19:19.440877: step 640, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.518 sec/batch; 47h:47m:36s remains)
INFO - root - 2017-12-10 04:19:24.706326: step 650, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:29m:20s remains)
INFO - root - 2017-12-10 04:19:29.959364: step 660, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:28m:13s remains)
INFO - root - 2017-12-10 04:19:34.958068: step 670, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.511 sec/batch; 47h:08m:32s remains)
INFO - root - 2017-12-10 04:19:40.174592: step 680, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 48h:15m:05s remains)
INFO - root - 2017-12-10 04:19:45.356721: step 690, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.507 sec/batch; 46h:43m:56s remains)
INFO - root - 2017-12-10 04:19:50.543554: step 700, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:07m:52s remains)
2017-12-10 04:19:51.113580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286847 -4.428699 -4.428721 -4.4287319 -4.4287362 -4.4287581 -4.4287777 -4.428781 -4.4287729 -4.4287643 -4.4287577 -4.4287524 -4.4287372 -4.4287195 -4.428699][-4.428668 -4.428678 -4.4286971 -4.4287024 -4.4287004 -4.4287186 -4.4287395 -4.4287496 -4.4287448 -4.4287348 -4.4287205 -4.4286928 -4.42866 -4.4286361 -4.4286323][-4.4287248 -4.4287229 -4.42873 -4.428719 -4.4287019 -4.42871 -4.4287324 -4.4287548 -4.4287691 -4.4287696 -4.4287448 -4.4286857 -4.428616 -4.4285774 -4.4285855][-4.4288278 -4.4288092 -4.4287972 -4.4287629 -4.4287148 -4.4286938 -4.428699 -4.4287338 -4.4287782 -4.4288 -4.4287691 -4.42868 -4.42858 -4.4285264 -4.4285455][-4.4289045 -4.4288735 -4.4288416 -4.4287767 -4.4286871 -4.4286156 -4.4285851 -4.4286289 -4.4287109 -4.4287672 -4.4287519 -4.4286571 -4.42854 -4.42847 -4.4284935][-4.4288969 -4.4288626 -4.4288211 -4.4287381 -4.428606 -4.4284639 -4.4283729 -4.4284129 -4.4285412 -4.428659 -4.4286966 -4.4286337 -4.42852 -4.428441 -4.4284396][-4.4288392 -4.4288063 -4.4287572 -4.4286656 -4.4285007 -4.4282756 -4.4280906 -4.4280772 -4.4282422 -4.4284549 -4.4285846 -4.42859 -4.4285183 -4.4284472 -4.4284096][-4.4287934 -4.4287696 -4.4287257 -4.4286375 -4.4284687 -4.4282074 -4.4279342 -4.4278216 -4.4279962 -4.4282885 -4.4284992 -4.4285755 -4.4285588 -4.428504 -4.4284449][-4.4287524 -4.4287391 -4.4287071 -4.4286366 -4.4285131 -4.42831 -4.428062 -4.4279208 -4.4280376 -4.4283042 -4.4285231 -4.428628 -4.4286413 -4.4286022 -4.428544][-4.4287081 -4.4287057 -4.428689 -4.4286475 -4.4285846 -4.4284778 -4.4283342 -4.4282379 -4.4282875 -4.4284563 -4.4286156 -4.4286938 -4.4287014 -4.4286666 -4.4286175][-4.4286876 -4.428689 -4.4286866 -4.4286709 -4.42865 -4.4286203 -4.4285746 -4.428534 -4.42855 -4.4286251 -4.4286928 -4.4287152 -4.4287009 -4.4286656 -4.4286313][-4.428719 -4.4287186 -4.42872 -4.4287171 -4.4287133 -4.4287233 -4.4287386 -4.4287415 -4.4287486 -4.4287581 -4.4287496 -4.4287229 -4.4286928 -4.4286661 -4.4286437][-4.428772 -4.4287729 -4.4287744 -4.4287696 -4.4287634 -4.4287891 -4.4288325 -4.4288578 -4.4288659 -4.4288378 -4.4287806 -4.4287262 -4.4286957 -4.4286842 -4.4286771][-4.4288168 -4.4288211 -4.428822 -4.4288106 -4.4287987 -4.4288263 -4.4288688 -4.428896 -4.4289 -4.4288568 -4.428782 -4.4287205 -4.4286928 -4.4286914 -4.4287071][-4.428843 -4.4288473 -4.4288545 -4.428843 -4.428833 -4.4288549 -4.428885 -4.4289007 -4.428895 -4.4288521 -4.4287887 -4.4287405 -4.4287157 -4.4287176 -4.4287543]]...]
INFO - root - 2017-12-10 04:19:56.366315: step 710, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 49h:05m:16s remains)
INFO - root - 2017-12-10 04:20:01.556940: step 720, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:54m:58s remains)
INFO - root - 2017-12-10 04:20:06.830468: step 730, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:25m:31s remains)
INFO - root - 2017-12-10 04:20:12.122811: step 740, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 49h:23m:46s remains)
INFO - root - 2017-12-10 04:20:17.412891: step 750, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:56m:06s remains)
INFO - root - 2017-12-10 04:20:22.686213: step 760, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.549 sec/batch; 50h:34m:21s remains)
INFO - root - 2017-12-10 04:20:27.696145: step 770, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.528 sec/batch; 48h:40m:36s remains)
INFO - root - 2017-12-10 04:20:32.874415: step 780, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.511 sec/batch; 47h:05m:49s remains)
INFO - root - 2017-12-10 04:20:38.203383: step 790, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 49h:00m:00s remains)
INFO - root - 2017-12-10 04:20:43.533547: step 800, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:50m:58s remains)
2017-12-10 04:20:44.038431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288869 -4.4288783 -4.4288821 -4.428884 -4.42886 -4.4288292 -4.4288073 -4.4287968 -4.4287868 -4.4287791 -4.4287691 -4.4287643 -4.4287548 -4.4287353 -4.4287219][-4.4289074 -4.4288988 -4.4289002 -4.4288969 -4.42887 -4.4288354 -4.4288063 -4.4287825 -4.4287634 -4.4287486 -4.4287353 -4.42873 -4.42872 -4.4287028 -4.4286919][-4.4289112 -4.428905 -4.4289021 -4.4288912 -4.4288578 -4.4288197 -4.4287872 -4.4287543 -4.4287286 -4.4287124 -4.4287038 -4.4287062 -4.4287024 -4.4286895 -4.4286814][-4.4288888 -4.42888 -4.4288683 -4.4288445 -4.4288025 -4.4287591 -4.4287262 -4.4286871 -4.4286609 -4.4286547 -4.4286618 -4.42868 -4.4286928 -4.4286928 -4.4286895][-4.4288449 -4.4288335 -4.4288135 -4.4287767 -4.4287291 -4.4286852 -4.4286532 -4.4286137 -4.4285955 -4.4286084 -4.4286323 -4.4286623 -4.4286876 -4.4286971 -4.4286933][-4.428792 -4.4287724 -4.4287395 -4.4286871 -4.4286308 -4.4285865 -4.4285555 -4.4285207 -4.4285259 -4.4285717 -4.428617 -4.42866 -4.4286971 -4.4287181 -4.4287205][-4.4287515 -4.4287219 -4.4286761 -4.4286079 -4.4285364 -4.4284725 -4.4284172 -4.4283729 -4.4284058 -4.4284983 -4.42858 -4.4286408 -4.42868 -4.4286942 -4.4286861][-4.4287457 -4.4287176 -4.4286671 -4.4285903 -4.4284978 -4.428401 -4.4283047 -4.4282374 -4.4282837 -4.4284067 -4.4285131 -4.4285779 -4.4286084 -4.4286027 -4.4285803][-4.428762 -4.4287438 -4.428699 -4.4286289 -4.428546 -4.4284587 -4.4283724 -4.4283152 -4.4283504 -4.4284506 -4.4285378 -4.428586 -4.4286027 -4.4285855 -4.4285545][-4.4287949 -4.4287877 -4.4287548 -4.4287024 -4.4286451 -4.4285841 -4.4285207 -4.4284763 -4.428493 -4.4285564 -4.4286175 -4.428647 -4.4286523 -4.4286351 -4.4286113][-4.4288507 -4.4288449 -4.4288206 -4.4287829 -4.4287481 -4.4287071 -4.428659 -4.4286194 -4.4286203 -4.4286556 -4.4286966 -4.4287157 -4.4287138 -4.4287009 -4.4286866][-4.4289069 -4.4289021 -4.4288859 -4.4288621 -4.42884 -4.42881 -4.4287715 -4.4287372 -4.4287295 -4.4287481 -4.4287748 -4.428791 -4.4287896 -4.4287767 -4.4287653][-4.428925 -4.4289241 -4.4289155 -4.4289007 -4.4288855 -4.4288654 -4.42884 -4.4288177 -4.428812 -4.4288206 -4.4288349 -4.4288468 -4.4288483 -4.4288397 -4.428833][-4.4289203 -4.4289227 -4.42892 -4.4289117 -4.4289021 -4.4288898 -4.4288769 -4.428864 -4.4288592 -4.4288588 -4.4288626 -4.4288697 -4.428875 -4.4288759 -4.4288778][-4.4289236 -4.4289274 -4.4289269 -4.4289227 -4.4289155 -4.4289079 -4.4289012 -4.428894 -4.4288907 -4.4288883 -4.4288859 -4.4288893 -4.428895 -4.4288979 -4.4288979]]...]
INFO - root - 2017-12-10 04:20:49.184080: step 810, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:18m:50s remains)
INFO - root - 2017-12-10 04:20:54.406981: step 820, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:35m:09s remains)
INFO - root - 2017-12-10 04:20:59.689702: step 830, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 49h:22m:35s remains)
INFO - root - 2017-12-10 04:21:04.870675: step 840, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:04m:12s remains)
INFO - root - 2017-12-10 04:21:10.176975: step 850, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:46m:36s remains)
INFO - root - 2017-12-10 04:21:15.485944: step 860, loss = 2.28, batch loss = 2.23 (14.1 examples/sec; 0.566 sec/batch; 52h:09m:19s remains)
INFO - root - 2017-12-10 04:21:20.472756: step 870, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:38m:25s remains)
INFO - root - 2017-12-10 04:21:25.637027: step 880, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 48h:00m:07s remains)
INFO - root - 2017-12-10 04:21:30.793041: step 890, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:31m:56s remains)
INFO - root - 2017-12-10 04:21:35.958785: step 900, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:26m:05s remains)
2017-12-10 04:21:36.493811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286008 -4.4286036 -4.4285793 -4.4285293 -4.4284868 -4.4284782 -4.4285283 -4.4285727 -4.4285989 -4.42863 -4.428628 -4.4286165 -4.4285803 -4.4285359 -4.4285526][-4.4285054 -4.4285731 -4.428606 -4.4285779 -4.4285264 -4.4284878 -4.4285226 -4.4285755 -4.4286017 -4.4286294 -4.4286313 -4.4286079 -4.4285507 -4.4285073 -4.4285226][-4.4284086 -4.4285359 -4.4286418 -4.4286494 -4.4285984 -4.4285226 -4.4285135 -4.42854 -4.4285607 -4.4286 -4.4286227 -4.4286079 -4.428556 -4.4285131 -4.4285245][-4.4284678 -4.4285789 -4.4286857 -4.4287062 -4.428648 -4.4285417 -4.4284735 -4.4284534 -4.4284725 -4.4285331 -4.4285836 -4.4285889 -4.4285574 -4.4285207 -4.4285336][-4.4286184 -4.4286704 -4.4287257 -4.4287167 -4.4286318 -4.4285064 -4.428411 -4.4283442 -4.4283719 -4.4284635 -4.4285426 -4.428565 -4.4285426 -4.4285083 -4.4285216][-4.4287262 -4.4287376 -4.4287362 -4.4286895 -4.4285765 -4.4284167 -4.4282908 -4.4281898 -4.428247 -4.4283977 -4.428504 -4.4285359 -4.4285197 -4.4284759 -4.4284797][-4.4287906 -4.4287748 -4.4287324 -4.4286513 -4.4285011 -4.4283056 -4.4281487 -4.4280367 -4.4281387 -4.4283614 -4.4285007 -4.4285431 -4.4285197 -4.4284463 -4.4284191][-4.4288054 -4.4287658 -4.4287004 -4.4286046 -4.4284644 -4.4283047 -4.4281993 -4.4281359 -4.4282522 -4.4284649 -4.4285946 -4.4286346 -4.4286056 -4.4285059 -4.4284534][-4.4287958 -4.4287562 -4.4286966 -4.4286132 -4.4285183 -4.4284363 -4.428401 -4.4283938 -4.42849 -4.4286232 -4.4286962 -4.4287167 -4.4286985 -4.4286137 -4.4285583][-4.4287987 -4.4287815 -4.4287624 -4.4287138 -4.4286523 -4.4286141 -4.4285975 -4.428596 -4.4286523 -4.4287171 -4.4287357 -4.428731 -4.428721 -4.4286656 -4.4286189][-4.4288106 -4.4288135 -4.4288197 -4.4288049 -4.4287663 -4.428741 -4.4287248 -4.428719 -4.4287362 -4.4287586 -4.4287467 -4.4287233 -4.4287076 -4.4286637 -4.4286141][-4.4288287 -4.4288278 -4.4288359 -4.428833 -4.4288073 -4.4287887 -4.4287858 -4.4287887 -4.428793 -4.4287944 -4.4287596 -4.4287238 -4.428699 -4.4286623 -4.4286132][-4.4288239 -4.4288144 -4.4288168 -4.4288168 -4.4288063 -4.4288 -4.4288068 -4.4288173 -4.4288187 -4.4288125 -4.4287758 -4.4287424 -4.4287157 -4.4286871 -4.4286423][-4.4288182 -4.4288077 -4.4288087 -4.4288154 -4.428823 -4.42883 -4.4288435 -4.4288559 -4.4288573 -4.4288492 -4.4288158 -4.4287891 -4.4287663 -4.4287386 -4.4286909][-4.4288096 -4.4288025 -4.4288054 -4.4288139 -4.4288316 -4.4288473 -4.4288654 -4.4288788 -4.42888 -4.4288683 -4.4288349 -4.4288211 -4.4288068 -4.4287777 -4.4287305]]...]
INFO - root - 2017-12-10 04:21:41.793294: step 910, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 47h:57m:07s remains)
INFO - root - 2017-12-10 04:21:47.073612: step 920, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:40m:56s remains)
INFO - root - 2017-12-10 04:21:52.456437: step 930, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:30m:01s remains)
INFO - root - 2017-12-10 04:21:57.753518: step 940, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.547 sec/batch; 50h:22m:15s remains)
INFO - root - 2017-12-10 04:22:02.917091: step 950, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.509 sec/batch; 46h:53m:13s remains)
INFO - root - 2017-12-10 04:22:08.097635: step 960, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:43m:07s remains)
INFO - root - 2017-12-10 04:22:13.018861: step 970, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:20m:34s remains)
INFO - root - 2017-12-10 04:22:18.215887: step 980, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:56m:36s remains)
INFO - root - 2017-12-10 04:22:23.498026: step 990, loss = 2.28, batch loss = 2.23 (16.2 examples/sec; 0.494 sec/batch; 45h:31m:14s remains)
INFO - root - 2017-12-10 04:22:28.819181: step 1000, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.550 sec/batch; 50h:36m:02s remains)
2017-12-10 04:22:29.398861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289083 -4.4288831 -4.4288635 -4.4288545 -4.4288545 -4.4288626 -4.4288778 -4.4288955 -4.4289074 -4.4289064 -4.4288855 -4.4288578 -4.4288473 -4.4288573 -4.4288874][-4.4288769 -4.4288421 -4.42881 -4.4287963 -4.4287858 -4.4287848 -4.4287944 -4.4288116 -4.4288225 -4.4288206 -4.428793 -4.428761 -4.4287519 -4.4287705 -4.4288087][-4.4288549 -4.4288144 -4.4287767 -4.4287539 -4.4287319 -4.4287176 -4.4287224 -4.4287405 -4.4287543 -4.4287586 -4.4287324 -4.4287009 -4.4286938 -4.428719 -4.4287553][-4.42883 -4.4287844 -4.4287415 -4.4287148 -4.4286804 -4.428638 -4.42862 -4.4286346 -4.4286671 -4.4286947 -4.4286814 -4.4286613 -4.4286656 -4.4286928 -4.4287176][-4.4287968 -4.4287419 -4.4286928 -4.4286671 -4.428627 -4.428544 -4.4284744 -4.4284658 -4.4285269 -4.4286 -4.4286146 -4.4286175 -4.4286385 -4.4286714 -4.4286876][-4.4287963 -4.428741 -4.4286871 -4.4286594 -4.4286127 -4.4284849 -4.4283352 -4.4282718 -4.4283633 -4.4284959 -4.4285564 -4.4285884 -4.4286265 -4.4286652 -4.4286833][-4.4288211 -4.4287782 -4.4287295 -4.4287047 -4.4286575 -4.4285159 -4.4283214 -4.4282007 -4.4282784 -4.428431 -4.4285083 -4.4285512 -4.4286041 -4.4286547 -4.4286904][-4.4288321 -4.4288 -4.42876 -4.4287438 -4.428719 -4.4286203 -4.4284763 -4.4283705 -4.4283991 -4.4284811 -4.42851 -4.4285278 -4.4285765 -4.428638 -4.4286928][-4.428822 -4.4287829 -4.4287434 -4.4287333 -4.4287248 -4.42868 -4.4286036 -4.4285407 -4.4285474 -4.4285617 -4.428534 -4.4285259 -4.4285693 -4.4286304 -4.4286914][-4.4288006 -4.4287539 -4.4287229 -4.4287205 -4.4287176 -4.4287038 -4.4286795 -4.4286509 -4.428648 -4.4286237 -4.4285641 -4.4285364 -4.4285736 -4.428637 -4.4287014][-4.4288187 -4.4287777 -4.4287543 -4.4287472 -4.4287434 -4.4287491 -4.4287491 -4.4287395 -4.4287376 -4.428699 -4.4286227 -4.4285841 -4.4286246 -4.4286928 -4.428751][-4.4288778 -4.4288445 -4.4288216 -4.4288054 -4.4287944 -4.42881 -4.428823 -4.4288263 -4.4288282 -4.4287958 -4.4287214 -4.4286838 -4.428721 -4.4287767 -4.4288182][-4.4289408 -4.4289126 -4.4288855 -4.4288645 -4.4288559 -4.4288745 -4.4288931 -4.4289055 -4.4289131 -4.4288898 -4.4288287 -4.428792 -4.4288139 -4.4288526 -4.4288778][-4.4289923 -4.42897 -4.428947 -4.4289317 -4.4289284 -4.4289422 -4.4289551 -4.4289646 -4.4289718 -4.4289579 -4.4289179 -4.4288898 -4.4289021 -4.4289279 -4.4289417][-4.4290118 -4.4289961 -4.4289775 -4.4289675 -4.4289684 -4.4289761 -4.4289794 -4.4289827 -4.428988 -4.4289842 -4.4289656 -4.4289522 -4.4289603 -4.4289775 -4.4289856]]...]
INFO - root - 2017-12-10 04:22:34.703728: step 1010, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:21m:43s remains)
INFO - root - 2017-12-10 04:22:39.911250: step 1020, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.528 sec/batch; 48h:38m:15s remains)
INFO - root - 2017-12-10 04:22:45.087825: step 1030, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.514 sec/batch; 47h:17m:33s remains)
INFO - root - 2017-12-10 04:22:50.244145: step 1040, loss = 2.28, batch loss = 2.23 (16.0 examples/sec; 0.501 sec/batch; 46h:10m:03s remains)
INFO - root - 2017-12-10 04:22:55.530402: step 1050, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.539 sec/batch; 49h:37m:55s remains)
INFO - root - 2017-12-10 04:23:00.810499: step 1060, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 49h:04m:13s remains)
INFO - root - 2017-12-10 04:23:05.703634: step 1070, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.510 sec/batch; 46h:55m:58s remains)
INFO - root - 2017-12-10 04:23:10.915499: step 1080, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:23m:01s remains)
INFO - root - 2017-12-10 04:23:16.113914: step 1090, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 48h:08m:04s remains)
INFO - root - 2017-12-10 04:23:21.380757: step 1100, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:00m:51s remains)
2017-12-10 04:23:21.955040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287066 -4.4286261 -4.4285755 -4.4285865 -4.4286137 -4.4286575 -4.4287167 -4.4287653 -4.428802 -4.4288311 -4.4288521 -4.4288697 -4.4288921 -4.428906 -4.4289093][-4.4287472 -4.4286752 -4.4286151 -4.4285846 -4.4285564 -4.428566 -4.4286275 -4.4287004 -4.4287634 -4.428813 -4.4288449 -4.428865 -4.4288888 -4.4289045 -4.4289136][-4.4287634 -4.4286995 -4.4286304 -4.4285564 -4.4284787 -4.4284616 -4.4285321 -4.4286323 -4.4287243 -4.4287953 -4.42884 -4.4288588 -4.4288774 -4.4288936 -4.4289083][-4.4287672 -4.4287138 -4.4286494 -4.4285631 -4.428462 -4.4284258 -4.4284916 -4.4285984 -4.4287057 -4.4287863 -4.4288378 -4.4288578 -4.4288731 -4.4288831 -4.4289][-4.4287729 -4.4287353 -4.42868 -4.4286046 -4.4285135 -4.4284735 -4.4285154 -4.4286 -4.4286919 -4.428762 -4.4288144 -4.4288411 -4.4288635 -4.4288764 -4.428894][-4.4287539 -4.4287262 -4.4286776 -4.428616 -4.4285426 -4.4285 -4.4285145 -4.4285693 -4.4286456 -4.42871 -4.4287658 -4.42881 -4.4288473 -4.4288721 -4.4288921][-4.4287367 -4.4287119 -4.4286618 -4.4286 -4.4285321 -4.4284782 -4.4284558 -4.4284782 -4.4285469 -4.428618 -4.4286919 -4.4287639 -4.4288216 -4.4288616 -4.4288898][-4.428762 -4.4287376 -4.428688 -4.4286246 -4.4285541 -4.4284873 -4.4284329 -4.4284191 -4.4284778 -4.4285583 -4.4286461 -4.4287362 -4.4288082 -4.4288616 -4.4288969][-4.42882 -4.4287996 -4.4287543 -4.4286914 -4.4286165 -4.4285436 -4.4284797 -4.4284534 -4.4285011 -4.4285817 -4.4286685 -4.4287534 -4.4288249 -4.4288831 -4.42892][-4.4288635 -4.4288511 -4.4288149 -4.4287539 -4.4286766 -4.4286046 -4.4285493 -4.428525 -4.4285579 -4.4286275 -4.4287081 -4.4287868 -4.4288526 -4.4289064 -4.4289412][-4.42886 -4.428863 -4.4288464 -4.4287982 -4.4287291 -4.4286613 -4.4286103 -4.4285817 -4.4285913 -4.42864 -4.4287157 -4.428793 -4.4288578 -4.4289093 -4.4289432][-4.4288445 -4.4288592 -4.4288616 -4.4288335 -4.4287844 -4.4287286 -4.4286833 -4.428647 -4.4286294 -4.4286475 -4.4287086 -4.42878 -4.428843 -4.4288969 -4.4289312][-4.428863 -4.4288754 -4.4288845 -4.4288731 -4.4288449 -4.4288044 -4.4287677 -4.42873 -4.4286966 -4.4286909 -4.4287295 -4.4287848 -4.4288406 -4.4288893 -4.4289231][-4.4288964 -4.4288974 -4.4288988 -4.4288898 -4.4288764 -4.4288545 -4.4288306 -4.4288 -4.4287653 -4.4287462 -4.4287643 -4.4288011 -4.4288445 -4.4288826 -4.4289141][-4.4289083 -4.4288993 -4.4288907 -4.4288788 -4.4288721 -4.4288654 -4.4288545 -4.4288335 -4.4288015 -4.4287758 -4.42878 -4.4288049 -4.4288387 -4.4288688 -4.4288983]]...]
INFO - root - 2017-12-10 04:23:27.222002: step 1110, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:50m:47s remains)
INFO - root - 2017-12-10 04:23:32.604441: step 1120, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:47m:26s remains)
INFO - root - 2017-12-10 04:23:37.880984: step 1130, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:25m:27s remains)
INFO - root - 2017-12-10 04:23:43.272705: step 1140, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.539 sec/batch; 49h:37m:08s remains)
INFO - root - 2017-12-10 04:23:48.534646: step 1150, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:39m:17s remains)
INFO - root - 2017-12-10 04:23:53.846529: step 1160, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.507 sec/batch; 46h:41m:40s remains)
INFO - root - 2017-12-10 04:23:58.880996: step 1170, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.547 sec/batch; 50h:19m:42s remains)
INFO - root - 2017-12-10 04:24:04.146801: step 1180, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:53m:49s remains)
INFO - root - 2017-12-10 04:24:09.518026: step 1190, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.541 sec/batch; 49h:45m:56s remains)
INFO - root - 2017-12-10 04:24:14.737944: step 1200, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:26m:17s remains)
2017-12-10 04:24:15.284627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428894 -4.4288864 -4.4288936 -4.4288859 -4.4288764 -4.4288845 -4.4288931 -4.4289184 -4.4289432 -4.428956 -4.4289646 -4.4289684 -4.428968 -4.4289627 -4.4289594][-4.4288297 -4.4288235 -4.428853 -4.4288673 -4.4288697 -4.4288831 -4.4288983 -4.4289355 -4.4289613 -4.428968 -4.4289675 -4.4289632 -4.4289517 -4.4289365 -4.42893][-4.4287486 -4.4287477 -4.4288049 -4.4288526 -4.4288697 -4.4288855 -4.428896 -4.4289336 -4.428957 -4.428956 -4.4289503 -4.428947 -4.4289355 -4.4289165 -4.428905][-4.4286971 -4.4287066 -4.4287767 -4.4288349 -4.4288511 -4.4288526 -4.428854 -4.4288907 -4.4289222 -4.4289336 -4.4289408 -4.428946 -4.4289379 -4.4289126 -4.4288931][-4.4286904 -4.4287057 -4.4287672 -4.4288082 -4.4288034 -4.4287782 -4.4287658 -4.4288058 -4.4288635 -4.428905 -4.4289331 -4.4289503 -4.4289474 -4.4289227 -4.4288983][-4.4287028 -4.4287105 -4.4287515 -4.428762 -4.4287219 -4.4286571 -4.4286079 -4.4286537 -4.4287534 -4.4288387 -4.4288936 -4.4289246 -4.4289408 -4.4289308 -4.4289026][-4.4287257 -4.4287305 -4.4287395 -4.4287081 -4.4286218 -4.4284935 -4.4283748 -4.4284163 -4.4285774 -4.4287248 -4.4288225 -4.42888 -4.4289155 -4.4289246 -4.4288955][-4.4287462 -4.4287434 -4.4287267 -4.4286556 -4.4285278 -4.4283376 -4.4281363 -4.4281611 -4.4284043 -4.4286275 -4.42877 -4.4288521 -4.4288936 -4.4289055 -4.4288816][-4.4287472 -4.4287367 -4.4287081 -4.4286289 -4.4284997 -4.4283171 -4.4281254 -4.4281487 -4.4283962 -4.4286203 -4.4287686 -4.4288554 -4.428896 -4.428905 -4.428884][-4.4287415 -4.4287367 -4.42872 -4.4286647 -4.4285827 -4.4284844 -4.4283867 -4.4284043 -4.4285526 -4.4286962 -4.4288068 -4.4288797 -4.4289112 -4.4289169 -4.4289069][-4.4287634 -4.4287748 -4.4287786 -4.4287519 -4.4287152 -4.4286871 -4.4286604 -4.4286714 -4.4287338 -4.4288039 -4.4288688 -4.4289174 -4.4289346 -4.4289379 -4.4289355][-4.4288225 -4.4288363 -4.4288425 -4.4288292 -4.4288144 -4.4288254 -4.4288373 -4.428853 -4.4288745 -4.4289002 -4.4289322 -4.4289551 -4.4289532 -4.4289508 -4.42895][-4.42888 -4.4288807 -4.4288831 -4.4288726 -4.4288635 -4.428884 -4.428906 -4.4289184 -4.4289207 -4.4289269 -4.428936 -4.4289408 -4.4289231 -4.4289131 -4.4289117][-4.4289074 -4.4288974 -4.4289017 -4.4288964 -4.4288845 -4.428894 -4.4289021 -4.4289007 -4.4288883 -4.428885 -4.4288859 -4.4288821 -4.4288692 -4.428865 -4.4288616][-4.428906 -4.4288917 -4.4289002 -4.4289002 -4.4288869 -4.4288845 -4.4288769 -4.4288664 -4.4288492 -4.4288397 -4.4288378 -4.4288335 -4.4288273 -4.4288354 -4.42884]]...]
INFO - root - 2017-12-10 04:24:20.341546: step 1210, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.508 sec/batch; 46h:43m:56s remains)
INFO - root - 2017-12-10 04:24:25.614889: step 1220, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.545 sec/batch; 50h:11m:48s remains)
INFO - root - 2017-12-10 04:24:30.909944: step 1230, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.542 sec/batch; 49h:50m:31s remains)
INFO - root - 2017-12-10 04:24:36.180349: step 1240, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.540 sec/batch; 49h:42m:08s remains)
INFO - root - 2017-12-10 04:24:41.439357: step 1250, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 47h:54m:59s remains)
INFO - root - 2017-12-10 04:24:46.684398: step 1260, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:34m:24s remains)
INFO - root - 2017-12-10 04:24:51.624690: step 1270, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 47h:09m:04s remains)
INFO - root - 2017-12-10 04:24:56.842565: step 1280, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:12m:31s remains)
INFO - root - 2017-12-10 04:25:02.091468: step 1290, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.545 sec/batch; 50h:09m:54s remains)
INFO - root - 2017-12-10 04:25:07.481561: step 1300, loss = 2.28, batch loss = 2.23 (14.3 examples/sec; 0.561 sec/batch; 51h:38m:49s remains)
2017-12-10 04:25:08.036110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287672 -4.42875 -4.4287419 -4.4287419 -4.4287252 -4.4286976 -4.4287205 -4.4287672 -4.4287748 -4.4287753 -4.428792 -4.4288139 -4.4288349 -4.4288397 -4.4288292][-4.428751 -4.4287434 -4.4287343 -4.4287357 -4.4287238 -4.4286809 -4.4286861 -4.4287357 -4.4287553 -4.428761 -4.4287925 -4.4288154 -4.4288311 -4.4288406 -4.4288335][-4.4287486 -4.4287405 -4.42872 -4.4287143 -4.4286971 -4.4286442 -4.4286289 -4.4286747 -4.4287105 -4.428731 -4.4287724 -4.4287934 -4.4287963 -4.4288039 -4.4287977][-4.4287138 -4.4286976 -4.4286604 -4.4286366 -4.4286127 -4.4285579 -4.4285231 -4.4285583 -4.428616 -4.4286551 -4.4287119 -4.4287362 -4.4287305 -4.4287281 -4.4287238][-4.4286375 -4.4285979 -4.42854 -4.4284987 -4.4284697 -4.4284134 -4.4283562 -4.4283767 -4.4284606 -4.428524 -4.4285951 -4.4286289 -4.4286256 -4.4286218 -4.4286246][-4.4285531 -4.4284859 -4.4284143 -4.4283614 -4.4283285 -4.4282618 -4.4281716 -4.428175 -4.4282885 -4.4283795 -4.4284549 -4.4285026 -4.42851 -4.4285073 -4.4285126][-4.4285145 -4.42843 -4.4283562 -4.4283018 -4.4282694 -4.4282069 -4.4281139 -4.4281054 -4.4282165 -4.4283147 -4.4283776 -4.428421 -4.428431 -4.4284277 -4.4284368][-4.4285378 -4.4284539 -4.4283829 -4.428328 -4.4282994 -4.4282594 -4.4282084 -4.428206 -4.4282827 -4.4283562 -4.4283872 -4.4284105 -4.4284191 -4.4284158 -4.4284205][-4.4285789 -4.4284997 -4.4284377 -4.4283948 -4.4283752 -4.4283566 -4.4283462 -4.42835 -4.4283886 -4.4284348 -4.4284458 -4.428452 -4.4284573 -4.4284544 -4.4284472][-4.4286332 -4.4285622 -4.4285164 -4.4284949 -4.4284921 -4.4284954 -4.4285078 -4.4285111 -4.4285192 -4.4285374 -4.4285316 -4.4285154 -4.42851 -4.4285007 -4.4284787][-4.4286737 -4.428617 -4.4285874 -4.4285846 -4.428597 -4.4286165 -4.428637 -4.4286375 -4.4286246 -4.4286232 -4.4286141 -4.4285865 -4.42857 -4.4285545 -4.4285307][-4.4286904 -4.4286413 -4.428618 -4.4286222 -4.4286427 -4.4286704 -4.4286923 -4.4286966 -4.4286833 -4.4286776 -4.4286757 -4.4286528 -4.4286337 -4.4286203 -4.4286056][-4.4286976 -4.428648 -4.4286194 -4.4286189 -4.428637 -4.4286642 -4.4286861 -4.4286942 -4.428688 -4.4286895 -4.4286976 -4.4286876 -4.4286766 -4.4286723 -4.4286652][-4.4287276 -4.4286814 -4.428647 -4.4286375 -4.428647 -4.4286666 -4.4286809 -4.4286861 -4.4286842 -4.4286847 -4.4286909 -4.4286833 -4.4286747 -4.4286728 -4.4286714][-4.4287782 -4.4287443 -4.4287176 -4.4287095 -4.4287148 -4.4287271 -4.4287338 -4.4287343 -4.4287295 -4.428719 -4.428709 -4.4286914 -4.4286766 -4.428669 -4.4286647]]...]
INFO - root - 2017-12-10 04:25:13.246331: step 1310, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 49h:05m:21s remains)
INFO - root - 2017-12-10 04:25:18.395351: step 1320, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 47h:08m:40s remains)
INFO - root - 2017-12-10 04:25:23.619025: step 1330, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 47h:07m:57s remains)
INFO - root - 2017-12-10 04:25:28.775125: step 1340, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:33m:39s remains)
INFO - root - 2017-12-10 04:25:34.022183: step 1350, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:22m:35s remains)
INFO - root - 2017-12-10 04:25:39.232533: step 1360, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:25m:38s remains)
INFO - root - 2017-12-10 04:25:44.297760: step 1370, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 48h:54m:51s remains)
INFO - root - 2017-12-10 04:25:49.524483: step 1380, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:12m:20s remains)
INFO - root - 2017-12-10 04:25:54.774580: step 1390, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.543 sec/batch; 49h:58m:01s remains)
INFO - root - 2017-12-10 04:25:59.963647: step 1400, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:22m:32s remains)
2017-12-10 04:26:00.512000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288425 -4.4288316 -4.42883 -4.4288249 -4.4288268 -4.4288311 -4.4288373 -4.4288235 -4.4288011 -4.4287758 -4.4287596 -4.4287457 -4.4287362 -4.4287448 -4.428772][-4.4288192 -4.428812 -4.4288149 -4.4288135 -4.4288158 -4.4288254 -4.4288373 -4.4288316 -4.4288182 -4.4287972 -4.4287858 -4.4287729 -4.42876 -4.4287596 -4.4287763][-4.4288068 -4.4287972 -4.4287982 -4.4287968 -4.4287982 -4.4288077 -4.4288235 -4.4288287 -4.42883 -4.4288211 -4.4288187 -4.4288087 -4.4287896 -4.42878 -4.4287887][-4.4287968 -4.4287834 -4.4287763 -4.4287677 -4.4287634 -4.4287696 -4.4287853 -4.4287996 -4.4288173 -4.4288254 -4.4288344 -4.4288287 -4.4288049 -4.4287853 -4.4287877][-4.4287782 -4.4287586 -4.4287443 -4.4287319 -4.4287171 -4.4287114 -4.4287167 -4.4287276 -4.4287567 -4.4287858 -4.4288125 -4.4288211 -4.4288058 -4.4287872 -4.4287844][-4.4287491 -4.4287124 -4.4286857 -4.4286647 -4.4286356 -4.4286108 -4.4286036 -4.4286103 -4.4286504 -4.4287052 -4.4287558 -4.4287848 -4.42879 -4.4287829 -4.4287796][-4.4287548 -4.4286985 -4.428647 -4.4285979 -4.42853 -4.4284644 -4.4284363 -4.4284463 -4.4285092 -4.42861 -4.4286971 -4.4287519 -4.4287791 -4.4287882 -4.4287896][-4.4288 -4.4287338 -4.42867 -4.4286008 -4.4284935 -4.4283752 -4.4283147 -4.4283218 -4.4284053 -4.4285502 -4.4286766 -4.4287634 -4.4288125 -4.4288349 -4.4288387][-4.4288321 -4.4287705 -4.4287143 -4.428647 -4.4285364 -4.4284072 -4.428339 -4.4283419 -4.4284272 -4.4285779 -4.4287043 -4.4287987 -4.4288545 -4.428885 -4.4288969][-4.4288659 -4.4288139 -4.428771 -4.4287205 -4.4286332 -4.4285269 -4.4284763 -4.428483 -4.428556 -4.4286747 -4.4287596 -4.4288211 -4.428854 -4.4288778 -4.428895][-4.4289041 -4.428864 -4.4288292 -4.4287906 -4.4287333 -4.4286618 -4.4286313 -4.428637 -4.4286838 -4.4287519 -4.4287906 -4.4288158 -4.4288225 -4.4288349 -4.4288535][-4.4289236 -4.4288993 -4.4288707 -4.4288411 -4.4288092 -4.4287672 -4.4287434 -4.4287348 -4.4287481 -4.428772 -4.4287777 -4.428781 -4.42878 -4.428792 -4.42882][-4.4289227 -4.4289174 -4.4289031 -4.4288845 -4.4288678 -4.4288383 -4.42881 -4.4287844 -4.4287696 -4.4287648 -4.4287524 -4.4287524 -4.4287591 -4.4287806 -4.4288177][-4.428896 -4.4289107 -4.4289136 -4.4289117 -4.4289079 -4.4288878 -4.4288559 -4.4288216 -4.4287925 -4.4287691 -4.4287467 -4.4287496 -4.4287682 -4.4288015 -4.4288468][-4.4288464 -4.4288774 -4.4288955 -4.4289083 -4.4289165 -4.4289074 -4.4288855 -4.42886 -4.4288311 -4.4288015 -4.428781 -4.4287863 -4.4288039 -4.4288368 -4.4288778]]...]
INFO - root - 2017-12-10 04:26:05.668867: step 1410, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.508 sec/batch; 46h:40m:59s remains)
INFO - root - 2017-12-10 04:26:10.869604: step 1420, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.514 sec/batch; 47h:17m:01s remains)
INFO - root - 2017-12-10 04:26:16.093745: step 1430, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:30m:55s remains)
INFO - root - 2017-12-10 04:26:21.321228: step 1440, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:32m:14s remains)
INFO - root - 2017-12-10 04:26:26.577004: step 1450, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:09m:29s remains)
INFO - root - 2017-12-10 04:26:31.807844: step 1460, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:44m:57s remains)
INFO - root - 2017-12-10 04:26:36.804149: step 1470, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.548 sec/batch; 50h:23m:33s remains)
INFO - root - 2017-12-10 04:26:42.133640: step 1480, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 47h:52m:15s remains)
INFO - root - 2017-12-10 04:26:47.421872: step 1490, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:24m:16s remains)
INFO - root - 2017-12-10 04:26:52.631316: step 1500, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:22m:32s remains)
2017-12-10 04:26:53.150875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287724 -4.4287267 -4.4286695 -4.4286504 -4.4286861 -4.4287367 -4.428781 -4.4288468 -4.42888 -4.4288673 -4.4288383 -4.4288163 -4.4287782 -4.4287758 -4.4288325][-4.4287539 -4.4286928 -4.428618 -4.428586 -4.4286313 -4.4287 -4.428751 -4.428823 -4.428863 -4.428853 -4.4288254 -4.428802 -4.428751 -4.42874 -4.4288025][-4.42873 -4.4286594 -4.4285693 -4.4285183 -4.4285722 -4.4286642 -4.4287229 -4.4288015 -4.4288468 -4.4288344 -4.4288116 -4.4287891 -4.4287357 -4.4287233 -4.4287896][-4.4287062 -4.4286289 -4.4285178 -4.4284525 -4.4285154 -4.4286323 -4.4286938 -4.4287658 -4.4288073 -4.4287858 -4.4287634 -4.4287486 -4.4287066 -4.4287024 -4.4287729][-4.4286871 -4.4286036 -4.4284782 -4.4284 -4.4284639 -4.4285879 -4.4286528 -4.4287186 -4.4287553 -4.4287238 -4.4286985 -4.4286938 -4.42867 -4.4286823 -4.4287605][-4.4286647 -4.4285765 -4.4284391 -4.4283342 -4.4283962 -4.4285321 -4.4286089 -4.4286757 -4.4287095 -4.4286733 -4.4286489 -4.428658 -4.4286566 -4.4286833 -4.4287634][-4.4286523 -4.4285464 -4.4283814 -4.4282317 -4.42829 -4.42845 -4.4285583 -4.42865 -4.4286985 -4.4286675 -4.4286408 -4.42865 -4.4286618 -4.4286981 -4.4287772][-4.4286208 -4.4285054 -4.4283223 -4.4281473 -4.4282126 -4.4283981 -4.4285355 -4.42865 -4.4287047 -4.4286776 -4.4286427 -4.4286442 -4.4286604 -4.428699 -4.4287777][-4.4286404 -4.4285431 -4.4283834 -4.4282222 -4.4282842 -4.4284539 -4.4285779 -4.4286733 -4.4287128 -4.4286771 -4.428637 -4.4286361 -4.4286551 -4.4286942 -4.4287739][-4.428699 -4.4286222 -4.4284906 -4.4283552 -4.4283934 -4.42852 -4.4286137 -4.4286861 -4.4287095 -4.4286633 -4.4286304 -4.4286394 -4.4286633 -4.4287043 -4.4287858][-4.4287577 -4.4286981 -4.4286008 -4.4284949 -4.4285054 -4.4285836 -4.4286532 -4.4287186 -4.4287362 -4.4286933 -4.4286752 -4.4286938 -4.428719 -4.4287605 -4.4288359][-4.4287972 -4.4287548 -4.4287004 -4.4286385 -4.4286375 -4.4286766 -4.4287276 -4.428782 -4.4287963 -4.4287605 -4.4287457 -4.428762 -4.4287872 -4.4288292 -4.4288921][-4.4288158 -4.428791 -4.428771 -4.4287491 -4.4287524 -4.428771 -4.428802 -4.4288373 -4.4288473 -4.4288192 -4.4288039 -4.4288149 -4.4288392 -4.42888 -4.4289322][-4.4288497 -4.428833 -4.4288263 -4.42882 -4.4288244 -4.4288368 -4.4288568 -4.4288807 -4.4288907 -4.4288721 -4.4288597 -4.4288688 -4.4288874 -4.4289174 -4.428957][-4.4289079 -4.4288926 -4.428884 -4.4288783 -4.4288793 -4.4288864 -4.4289002 -4.4289188 -4.42893 -4.4289241 -4.4289193 -4.4289227 -4.4289317 -4.4289494 -4.4289775]]...]
INFO - root - 2017-12-10 04:26:58.338365: step 1510, loss = 2.28, batch loss = 2.23 (16.0 examples/sec; 0.500 sec/batch; 45h:56m:13s remains)
INFO - root - 2017-12-10 04:27:03.567189: step 1520, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.504 sec/batch; 46h:22m:38s remains)
INFO - root - 2017-12-10 04:27:08.783072: step 1530, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 47h:53m:08s remains)
INFO - root - 2017-12-10 04:27:13.936048: step 1540, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.505 sec/batch; 46h:26m:39s remains)
INFO - root - 2017-12-10 04:27:19.277188: step 1550, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:11m:03s remains)
INFO - root - 2017-12-10 04:27:24.629042: step 1560, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:28m:26s remains)
INFO - root - 2017-12-10 04:27:29.562263: step 1570, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.508 sec/batch; 46h:42m:58s remains)
INFO - root - 2017-12-10 04:27:34.744124: step 1580, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:49m:39s remains)
INFO - root - 2017-12-10 04:27:39.893997: step 1590, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.505 sec/batch; 46h:23m:51s remains)
INFO - root - 2017-12-10 04:27:45.141950: step 1600, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:46m:12s remains)
2017-12-10 04:27:45.639104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428771 -4.4287634 -4.428772 -4.4287863 -4.428803 -4.4288297 -4.4288554 -4.4288564 -4.4288349 -4.4287844 -4.4287305 -4.4287143 -4.4287052 -4.4286933 -4.4286823][-4.4287868 -4.42879 -4.4287987 -4.4288058 -4.42881 -4.42883 -4.4288554 -4.4288545 -4.42882 -4.4287519 -4.4286785 -4.4286509 -4.428638 -4.4286351 -4.4286427][-4.428792 -4.4287925 -4.4287949 -4.4287891 -4.4287815 -4.428782 -4.4287925 -4.428793 -4.4287653 -4.4287057 -4.4286423 -4.4286141 -4.4286013 -4.4286079 -4.428637][-4.4287705 -4.4287634 -4.4287562 -4.4287395 -4.4287152 -4.4286895 -4.4286785 -4.4286814 -4.428678 -4.4286447 -4.4286008 -4.4285803 -4.4285703 -4.4285836 -4.4286261][-4.4287324 -4.4287233 -4.4287071 -4.4286747 -4.4286242 -4.4285631 -4.4285254 -4.428544 -4.4285774 -4.4285793 -4.4285583 -4.4285488 -4.4285493 -4.4285707 -4.4286103][-4.42868 -4.4286742 -4.4286566 -4.428617 -4.4285383 -4.4284272 -4.4283528 -4.4283938 -4.4284844 -4.4285331 -4.4285388 -4.4285421 -4.4285507 -4.4285645 -4.4285851][-4.428616 -4.4286141 -4.4286022 -4.428566 -4.4284711 -4.4283094 -4.4281721 -4.4282222 -4.4283867 -4.4284992 -4.4285421 -4.4285626 -4.4285645 -4.4285569 -4.4285569][-4.4285803 -4.4285727 -4.4285641 -4.4285369 -4.42845 -4.4282765 -4.4280891 -4.4281139 -4.4283133 -4.4284744 -4.4285574 -4.428597 -4.4285908 -4.4285631 -4.4285393][-4.4285932 -4.4285755 -4.4285665 -4.4285555 -4.4285078 -4.428391 -4.4282422 -4.428225 -4.428349 -4.4284825 -4.4285755 -4.4286275 -4.4286213 -4.4285755 -4.4285254][-4.4286308 -4.4286046 -4.4285941 -4.4285922 -4.4285736 -4.4285169 -4.4284372 -4.4284043 -4.4284406 -4.4285054 -4.4285712 -4.4286213 -4.4286203 -4.4285769 -4.4285231][-4.4286718 -4.4286456 -4.428638 -4.4286356 -4.4286251 -4.4286056 -4.4285741 -4.4285474 -4.4285374 -4.4285407 -4.4285631 -4.4285955 -4.4286065 -4.4285913 -4.4285593][-4.4286985 -4.42868 -4.4286847 -4.4286971 -4.4286985 -4.4286928 -4.4286795 -4.4286566 -4.428628 -4.4285965 -4.4285889 -4.4286089 -4.4286265 -4.4286284 -4.4286141][-4.4287224 -4.4287124 -4.4287238 -4.4287477 -4.42876 -4.428761 -4.4287553 -4.4287372 -4.4287033 -4.4286637 -4.428648 -4.4286656 -4.4286852 -4.42869 -4.4286766][-4.4287276 -4.4287243 -4.4287391 -4.428771 -4.4287958 -4.4288034 -4.4288063 -4.4287949 -4.4287639 -4.4287248 -4.4287076 -4.4287205 -4.4287372 -4.4287419 -4.4287281][-4.4287157 -4.4287095 -4.428721 -4.4287586 -4.4287963 -4.4288096 -4.4288139 -4.4288063 -4.4287791 -4.4287424 -4.4287229 -4.4287305 -4.4287448 -4.428751 -4.4287386]]...]
INFO - root - 2017-12-10 04:27:50.922035: step 1610, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:23m:19s remains)
INFO - root - 2017-12-10 04:27:56.213242: step 1620, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.511 sec/batch; 46h:58m:24s remains)
INFO - root - 2017-12-10 04:28:01.491986: step 1630, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:25m:17s remains)
INFO - root - 2017-12-10 04:28:06.741675: step 1640, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:00m:08s remains)
INFO - root - 2017-12-10 04:28:11.989885: step 1650, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:12m:05s remains)
INFO - root - 2017-12-10 04:28:17.298860: step 1660, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.528 sec/batch; 48h:32m:19s remains)
INFO - root - 2017-12-10 04:28:22.337819: step 1670, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:40m:28s remains)
INFO - root - 2017-12-10 04:28:27.677650: step 1680, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:14m:57s remains)
INFO - root - 2017-12-10 04:28:32.945716: step 1690, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.518 sec/batch; 47h:35m:54s remains)
INFO - root - 2017-12-10 04:28:38.306283: step 1700, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.545 sec/batch; 50h:02m:57s remains)
2017-12-10 04:28:38.790156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288278 -4.4288039 -4.4287858 -4.4287791 -4.4287806 -4.42878 -4.4287839 -4.4288039 -4.4288278 -4.428844 -4.4288597 -4.4288793 -4.4288955 -4.4289083 -4.4289231][-4.4288526 -4.4288249 -4.4287977 -4.428782 -4.4287763 -4.4287667 -4.4287586 -4.4287715 -4.4287949 -4.4288149 -4.428833 -4.4288507 -4.4288673 -4.4288821 -4.4288983][-4.428865 -4.4288387 -4.4288125 -4.4287939 -4.4287806 -4.428761 -4.4287419 -4.4287457 -4.4287705 -4.4288015 -4.4288278 -4.4288464 -4.4288611 -4.4288731 -4.4288864][-4.4288592 -4.4288378 -4.4288168 -4.4287934 -4.42877 -4.42874 -4.4287119 -4.428709 -4.4287391 -4.4287844 -4.4288249 -4.4288526 -4.4288654 -4.4288712 -4.4288759][-4.4288344 -4.4288182 -4.4288 -4.4287658 -4.4287238 -4.4286766 -4.4286375 -4.4286332 -4.4286833 -4.4287529 -4.4288135 -4.4288554 -4.428875 -4.4288783 -4.428874][-4.428813 -4.4287958 -4.428772 -4.4287219 -4.4286532 -4.4285741 -4.4285045 -4.4285 -4.428586 -4.4287033 -4.4287982 -4.42886 -4.4288907 -4.4288955 -4.4288859][-4.4287858 -4.4287648 -4.4287291 -4.4286656 -4.4285784 -4.4284682 -4.4283533 -4.4283366 -4.4284658 -4.4286375 -4.4287748 -4.4288573 -4.4288979 -4.4289069 -4.4288993][-4.4287543 -4.4287291 -4.428689 -4.4286237 -4.4285378 -4.4284225 -4.4282827 -4.4282403 -4.4283791 -4.4285769 -4.42874 -4.4288378 -4.4288864 -4.4289012 -4.4289002][-4.4287367 -4.4287105 -4.4286752 -4.428627 -4.4285655 -4.4284806 -4.4283681 -4.4283123 -4.4283919 -4.4285488 -4.4287 -4.4287996 -4.4288526 -4.4288721 -4.4288788][-4.4287543 -4.4287252 -4.4286933 -4.4286623 -4.4286327 -4.4285922 -4.4285259 -4.4284687 -4.4284763 -4.4285512 -4.4286633 -4.4287567 -4.42881 -4.4288287 -4.4288378][-4.4287868 -4.4287581 -4.4287314 -4.4287128 -4.4287019 -4.4286871 -4.4286566 -4.4286122 -4.4285841 -4.4285903 -4.4286528 -4.4287329 -4.428781 -4.4287934 -4.4287925][-4.4288073 -4.4287877 -4.4287696 -4.4287572 -4.4287543 -4.4287519 -4.4287424 -4.4287171 -4.4286838 -4.4286528 -4.4286695 -4.4287281 -4.4287648 -4.4287715 -4.4287562][-4.428812 -4.4288077 -4.4287996 -4.4287906 -4.4287939 -4.4288006 -4.4288034 -4.4287934 -4.4287648 -4.4287219 -4.4287009 -4.4287238 -4.4287424 -4.4287472 -4.4287281][-4.4288073 -4.4288149 -4.4288144 -4.428812 -4.4288225 -4.4288387 -4.4288526 -4.4288535 -4.4288287 -4.4287782 -4.4287291 -4.4287162 -4.4287157 -4.4287148 -4.4287033][-4.4288049 -4.4288149 -4.4288144 -4.4288125 -4.4288259 -4.4288492 -4.4288759 -4.4288893 -4.4288678 -4.4288182 -4.4287581 -4.4287181 -4.4286971 -4.4286914 -4.42869]]...]
INFO - root - 2017-12-10 04:28:44.068832: step 1710, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:40m:41s remains)
INFO - root - 2017-12-10 04:28:49.346558: step 1720, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 48h:58m:40s remains)
INFO - root - 2017-12-10 04:28:54.669550: step 1730, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.510 sec/batch; 46h:53m:32s remains)
INFO - root - 2017-12-10 04:28:59.884898: step 1740, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.528 sec/batch; 48h:32m:30s remains)
INFO - root - 2017-12-10 04:29:05.218672: step 1750, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:20m:39s remains)
INFO - root - 2017-12-10 04:29:10.552498: step 1760, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.545 sec/batch; 50h:03m:51s remains)
INFO - root - 2017-12-10 04:29:15.514544: step 1770, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:40m:58s remains)
INFO - root - 2017-12-10 04:29:20.901201: step 1780, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.542 sec/batch; 49h:47m:32s remains)
INFO - root - 2017-12-10 04:29:26.161754: step 1790, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.508 sec/batch; 46h:40m:45s remains)
INFO - root - 2017-12-10 04:29:31.431847: step 1800, loss = 2.28, batch loss = 2.23 (14.1 examples/sec; 0.567 sec/batch; 52h:07m:41s remains)
2017-12-10 04:29:31.984438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287047 -4.4287491 -4.4287653 -4.4287214 -4.4286346 -4.4285703 -4.4285297 -4.4284692 -4.4284716 -4.4285722 -4.4286556 -4.4287028 -4.42874 -4.4287534 -4.4287453][-4.4287543 -4.4287925 -4.4288015 -4.4287586 -4.4286795 -4.4286051 -4.4285412 -4.4284544 -4.4284534 -4.4285769 -4.4286766 -4.4287381 -4.4287848 -4.4288025 -4.428791][-4.42877 -4.4287887 -4.42879 -4.4287534 -4.4286833 -4.42861 -4.42853 -4.4284239 -4.4284229 -4.4285665 -4.428679 -4.4287472 -4.4288 -4.428822 -4.4288082][-4.4287419 -4.4287434 -4.4287381 -4.4287033 -4.4286404 -4.4285665 -4.428473 -4.4283495 -4.4283547 -4.4285264 -4.4286571 -4.428731 -4.4287839 -4.4288054 -4.4287868][-4.4287004 -4.42869 -4.4286742 -4.4286408 -4.428586 -4.428503 -4.4283819 -4.4282241 -4.4282384 -4.4284568 -4.42862 -4.4287019 -4.4287515 -4.4287686 -4.42875][-4.428669 -4.42865 -4.4286308 -4.4286051 -4.4285507 -4.4284492 -4.4282842 -4.4280729 -4.4280958 -4.4283762 -4.4285836 -4.4286728 -4.4287133 -4.428721 -4.4287033][-4.428647 -4.4286323 -4.4286232 -4.4286132 -4.4285569 -4.4284325 -4.4282265 -4.427969 -4.4280005 -4.4283304 -4.4285765 -4.4286757 -4.4287062 -4.4286995 -4.4286757][-4.4286413 -4.4286475 -4.428659 -4.4286613 -4.4286017 -4.4284668 -4.4282622 -4.4280262 -4.4280806 -4.428401 -4.42864 -4.4287329 -4.4287539 -4.4287291 -4.4286933][-4.4286656 -4.4286871 -4.4287167 -4.4287257 -4.4286709 -4.4285569 -4.4284024 -4.4282455 -4.4283061 -4.4285464 -4.4287238 -4.4287925 -4.4287992 -4.4287624 -4.4287124][-4.4287143 -4.4287415 -4.4287763 -4.4287925 -4.4287572 -4.4286828 -4.4285817 -4.4284797 -4.4285145 -4.42866 -4.4287682 -4.4288125 -4.4288111 -4.428772 -4.4287148][-4.4287634 -4.428781 -4.4288116 -4.4288373 -4.428823 -4.428781 -4.4287181 -4.4286456 -4.4286423 -4.4287105 -4.4287643 -4.4287915 -4.4287939 -4.4287529 -4.4287004][-4.4287839 -4.4287753 -4.4287996 -4.4288392 -4.4288425 -4.428822 -4.4287868 -4.4287281 -4.4286928 -4.4287148 -4.4287419 -4.42876 -4.4287567 -4.42871 -4.4286633][-4.4287715 -4.4287314 -4.4287477 -4.4287963 -4.4288154 -4.428802 -4.4287825 -4.4287381 -4.4286976 -4.4287105 -4.4287343 -4.4287429 -4.4287238 -4.4286709 -4.4286346][-4.4287648 -4.4287033 -4.4287071 -4.4287477 -4.4287653 -4.4287515 -4.42874 -4.4287162 -4.4286933 -4.4287224 -4.4287529 -4.4287548 -4.428719 -4.4286656 -4.4286361][-4.4287815 -4.4287124 -4.4287 -4.4287238 -4.4287314 -4.4287133 -4.4287105 -4.428709 -4.4287076 -4.428751 -4.4287853 -4.4287868 -4.4287472 -4.4286976 -4.428668]]...]
INFO - root - 2017-12-10 04:29:37.132853: step 1810, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.502 sec/batch; 46h:08m:39s remains)
INFO - root - 2017-12-10 04:29:42.319109: step 1820, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.503 sec/batch; 46h:13m:42s remains)
INFO - root - 2017-12-10 04:29:47.563625: step 1830, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.543 sec/batch; 49h:51m:39s remains)
INFO - root - 2017-12-10 04:29:52.736486: step 1840, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.514 sec/batch; 47h:13m:08s remains)
INFO - root - 2017-12-10 04:29:57.902407: step 1850, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 47h:50m:12s remains)
INFO - root - 2017-12-10 04:30:03.198596: step 1860, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:21m:51s remains)
INFO - root - 2017-12-10 04:30:08.064387: step 1870, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.510 sec/batch; 46h:48m:44s remains)
INFO - root - 2017-12-10 04:30:13.165005: step 1880, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 46h:59m:47s remains)
INFO - root - 2017-12-10 04:30:18.292883: step 1890, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:20m:30s remains)
INFO - root - 2017-12-10 04:30:23.596809: step 1900, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.508 sec/batch; 46h:40m:02s remains)
2017-12-10 04:30:24.137111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288287 -4.4287834 -4.4287524 -4.4287562 -4.4287934 -4.4288311 -4.4288468 -4.4288526 -4.4288712 -4.4288635 -4.4288459 -4.4288397 -4.4288282 -4.4288211 -4.428834][-4.4288263 -4.4287624 -4.4286966 -4.4286809 -4.4287248 -4.4287891 -4.4288087 -4.4288025 -4.4288259 -4.4288273 -4.4288144 -4.428813 -4.4288068 -4.4288063 -4.4288139][-4.4288063 -4.4287243 -4.428637 -4.4285951 -4.4286423 -4.4287238 -4.4287577 -4.4287496 -4.428772 -4.4287748 -4.4287724 -4.4287882 -4.4287958 -4.4287968 -4.4287987][-4.4287615 -4.4286838 -4.4285975 -4.42854 -4.4285693 -4.4286518 -4.4287038 -4.4287024 -4.4287233 -4.4287257 -4.4287305 -4.42876 -4.4287858 -4.4287963 -4.4287944][-4.4287066 -4.4286523 -4.4285889 -4.4285278 -4.4285293 -4.4285889 -4.4286385 -4.4286528 -4.4286876 -4.4287028 -4.4287105 -4.4287481 -4.4287772 -4.4287896 -4.4287858][-4.4286718 -4.4286356 -4.4285936 -4.42853 -4.4284925 -4.4285097 -4.4285398 -4.428575 -4.4286323 -4.4286623 -4.4286704 -4.4287076 -4.4287376 -4.4287553 -4.4287539][-4.4286671 -4.4286418 -4.4286046 -4.4285393 -4.4284787 -4.4284511 -4.4284477 -4.4284821 -4.4285488 -4.42859 -4.4286041 -4.4286437 -4.4286795 -4.4287114 -4.4287195][-4.4286704 -4.4286566 -4.4286208 -4.42856 -4.4284997 -4.4284525 -4.4284258 -4.4284325 -4.4284744 -4.4285092 -4.4285383 -4.4285936 -4.4286404 -4.4286861 -4.4286995][-4.4286613 -4.428647 -4.428617 -4.42857 -4.4285312 -4.4284897 -4.428452 -4.4284368 -4.4284463 -4.4284625 -4.4284968 -4.4285665 -4.4286203 -4.4286647 -4.4286761][-4.4286709 -4.4286494 -4.4286251 -4.4285936 -4.4285684 -4.4285364 -4.4284949 -4.4284692 -4.4284687 -4.4284716 -4.4284925 -4.4285631 -4.4286261 -4.4286666 -4.4286761][-4.4287281 -4.4287076 -4.4286919 -4.42867 -4.4286494 -4.4286246 -4.4285936 -4.4285731 -4.4285669 -4.4285479 -4.4285393 -4.4285922 -4.42866 -4.4287038 -4.4287109][-4.428802 -4.4287915 -4.4287853 -4.428772 -4.4287558 -4.4287386 -4.4287205 -4.4287066 -4.4286976 -4.4286704 -4.4286361 -4.4286513 -4.4286985 -4.4287376 -4.4287491][-4.4288974 -4.4288893 -4.4288874 -4.4288783 -4.4288645 -4.4288511 -4.428843 -4.4288359 -4.4288263 -4.4288 -4.4287634 -4.4287562 -4.4287748 -4.4287987 -4.4288125][-4.4289708 -4.4289632 -4.4289627 -4.4289575 -4.42895 -4.4289422 -4.4289355 -4.42893 -4.4289241 -4.4289074 -4.4288793 -4.428865 -4.4288678 -4.4288769 -4.4288845][-4.4290018 -4.4289966 -4.428998 -4.4289966 -4.4289923 -4.428987 -4.4289818 -4.428977 -4.4289727 -4.4289632 -4.4289474 -4.4289384 -4.4289389 -4.4289412 -4.428946]]...]
INFO - root - 2017-12-10 04:30:29.321997: step 1910, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 48h:58m:55s remains)
INFO - root - 2017-12-10 04:30:34.504780: step 1920, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:15m:47s remains)
INFO - root - 2017-12-10 04:30:39.703200: step 1930, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.514 sec/batch; 47h:09m:17s remains)
INFO - root - 2017-12-10 04:30:44.889961: step 1940, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:07m:34s remains)
INFO - root - 2017-12-10 04:30:50.066829: step 1950, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:21m:11s remains)
INFO - root - 2017-12-10 04:30:55.250740: step 1960, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.504 sec/batch; 46h:18m:46s remains)
INFO - root - 2017-12-10 04:31:00.196851: step 1970, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.508 sec/batch; 46h:40m:10s remains)
INFO - root - 2017-12-10 04:31:05.401308: step 1980, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:15m:32s remains)
INFO - root - 2017-12-10 04:31:10.595148: step 1990, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:24m:19s remains)
INFO - root - 2017-12-10 04:31:15.818962: step 2000, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:32m:17s remains)
2017-12-10 04:31:16.370247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428813 -4.4288068 -4.4288282 -4.4288435 -4.4288406 -4.4288373 -4.4288392 -4.4288297 -4.4288192 -4.4288111 -4.4288073 -4.4288149 -4.4288273 -4.4288354 -4.4288521][-4.4288206 -4.4288049 -4.4288225 -4.4288363 -4.4288249 -4.428812 -4.4288177 -4.4288192 -4.4288063 -4.4287786 -4.4287558 -4.428751 -4.4287567 -4.4287672 -4.42879][-4.4288421 -4.4288211 -4.4288282 -4.4288344 -4.4288082 -4.4287763 -4.4287734 -4.4287872 -4.4287834 -4.4287481 -4.428709 -4.4286885 -4.4286718 -4.42867 -4.4286876][-4.4288464 -4.4288177 -4.4288068 -4.4287977 -4.4287572 -4.4287024 -4.4286842 -4.4287124 -4.4287324 -4.4287114 -4.4286776 -4.4286532 -4.428617 -4.4285994 -4.4286027][-4.4288225 -4.4287829 -4.4287524 -4.4287305 -4.4286718 -4.4285784 -4.428534 -4.428586 -4.4286532 -4.4286737 -4.428668 -4.4286571 -4.428617 -4.4285941 -4.4285903][-4.4288116 -4.4287529 -4.4287009 -4.4286695 -4.4285903 -4.4284458 -4.4283562 -4.4284358 -4.4285717 -4.4286485 -4.42868 -4.4286895 -4.42866 -4.4286447 -4.4286489][-4.4288135 -4.4287438 -4.4286761 -4.4286323 -4.4285369 -4.4283571 -4.4282241 -4.4283185 -4.4285054 -4.4286242 -4.4286847 -4.4287152 -4.4287024 -4.4287009 -4.4287214][-4.4288063 -4.4287481 -4.4286895 -4.4286542 -4.4285803 -4.4284315 -4.4283113 -4.4283862 -4.4285431 -4.428648 -4.4287062 -4.42874 -4.4287338 -4.4287286 -4.4287577][-4.4287915 -4.4287658 -4.42874 -4.4287324 -4.4287047 -4.4286218 -4.4285393 -4.4285665 -4.4286437 -4.4286976 -4.4287353 -4.428761 -4.4287558 -4.428741 -4.428771][-4.4287558 -4.428761 -4.4287724 -4.428792 -4.4288015 -4.4287705 -4.4287162 -4.4287062 -4.4287243 -4.4287424 -4.4287658 -4.4287839 -4.4287786 -4.4287567 -4.4287839][-4.4287276 -4.4287434 -4.4287705 -4.4288006 -4.4288292 -4.4288273 -4.428793 -4.4287686 -4.4287715 -4.4287872 -4.4288111 -4.4288239 -4.4288173 -4.4287982 -4.4288187][-4.4287338 -4.4287419 -4.4287653 -4.428791 -4.4288225 -4.4288354 -4.42882 -4.428802 -4.4288092 -4.4288292 -4.4288535 -4.4288645 -4.4288611 -4.428853 -4.4288697][-4.4287586 -4.4287515 -4.4287591 -4.4287682 -4.428793 -4.4288154 -4.4288149 -4.4288073 -4.4288168 -4.4288349 -4.4288559 -4.4288683 -4.4288726 -4.428874 -4.4288888][-4.4287958 -4.4287825 -4.4287777 -4.4287729 -4.4287891 -4.4288125 -4.4288173 -4.4288154 -4.4288244 -4.4288406 -4.4288564 -4.4288683 -4.4288735 -4.4288769 -4.4288888][-4.4288411 -4.4288306 -4.42882 -4.4288068 -4.4288111 -4.4288254 -4.4288282 -4.4288278 -4.4288344 -4.4288492 -4.4288635 -4.428875 -4.4288793 -4.4288783 -4.4288812]]...]
INFO - root - 2017-12-10 04:31:21.562749: step 2010, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.513 sec/batch; 47h:03m:01s remains)
INFO - root - 2017-12-10 04:31:26.781278: step 2020, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 46h:59m:07s remains)
INFO - root - 2017-12-10 04:31:32.172776: step 2030, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:18m:35s remains)
INFO - root - 2017-12-10 04:31:37.438388: step 2040, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 47h:57m:17s remains)
INFO - root - 2017-12-10 04:31:42.744677: step 2050, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:44m:17s remains)
INFO - root - 2017-12-10 04:31:47.947015: step 2060, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:12m:32s remains)
INFO - root - 2017-12-10 04:31:52.831188: step 2070, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 47h:58m:40s remains)
INFO - root - 2017-12-10 04:31:57.962814: step 2080, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 47h:53m:33s remains)
INFO - root - 2017-12-10 04:32:03.150335: step 2090, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 48h:56m:25s remains)
INFO - root - 2017-12-10 04:32:08.334696: step 2100, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.508 sec/batch; 46h:35m:37s remains)
2017-12-10 04:32:08.844568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287229 -4.4287248 -4.4287395 -4.4287653 -4.4287863 -4.4287791 -4.428771 -4.42878 -4.4287896 -4.4288034 -4.4288139 -4.4288368 -4.428844 -4.4288449 -4.4288468][-4.4287367 -4.4287467 -4.4287677 -4.4287868 -4.4288011 -4.4287934 -4.4287806 -4.4287939 -4.4288244 -4.428854 -4.4288654 -4.4288707 -4.428864 -4.4288573 -4.4288578][-4.4287677 -4.4287629 -4.428782 -4.4287982 -4.428802 -4.4287858 -4.4287653 -4.4287748 -4.4288116 -4.4288535 -4.4288692 -4.4288707 -4.4288659 -4.4288568 -4.4288507][-4.4287863 -4.4287591 -4.4287715 -4.4287839 -4.4287744 -4.4287429 -4.4287148 -4.428721 -4.4287615 -4.4288211 -4.4288492 -4.4288568 -4.4288626 -4.4288597 -4.4288487][-4.4287825 -4.4287448 -4.4287467 -4.4287519 -4.4287238 -4.4286633 -4.4286079 -4.4286027 -4.4286504 -4.4287395 -4.4288006 -4.4288306 -4.4288564 -4.4288683 -4.4288611][-4.4287796 -4.4287543 -4.42875 -4.4287405 -4.4286895 -4.4285841 -4.4284687 -4.4284325 -4.4284945 -4.4286218 -4.4287276 -4.4288 -4.4288573 -4.4288883 -4.4288917][-4.4288044 -4.428802 -4.4287972 -4.4287782 -4.4287176 -4.4285846 -4.4284158 -4.4283266 -4.4283881 -4.4285383 -4.4286733 -4.4287758 -4.4288592 -4.428905 -4.428916][-4.4288254 -4.428834 -4.4288321 -4.4288106 -4.4287586 -4.4286375 -4.4284663 -4.4283524 -4.4283929 -4.42853 -4.4286594 -4.4287629 -4.4288526 -4.4289026 -4.4289169][-4.4288106 -4.4288235 -4.4288216 -4.4287982 -4.4287605 -4.4286704 -4.4285212 -4.4284019 -4.4284167 -4.4285345 -4.4286575 -4.4287529 -4.4288373 -4.428885 -4.4288964][-4.4287882 -4.4288015 -4.4287944 -4.4287629 -4.4287276 -4.4286656 -4.4285479 -4.42844 -4.4284477 -4.4285426 -4.4286513 -4.4287419 -4.4288197 -4.4288588 -4.4288664][-4.4287763 -4.4287858 -4.4287677 -4.4287338 -4.4287081 -4.4286747 -4.4285879 -4.4284992 -4.42851 -4.4285893 -4.4286728 -4.4287457 -4.428803 -4.4288325 -4.4288459][-4.4287872 -4.4287891 -4.428762 -4.4287276 -4.4287143 -4.428709 -4.4286585 -4.4285917 -4.4285994 -4.4286613 -4.4287243 -4.4287724 -4.4287939 -4.4288015 -4.4288177][-4.4288116 -4.4288158 -4.42879 -4.4287615 -4.4287539 -4.4287567 -4.4287267 -4.4286795 -4.4286828 -4.4287286 -4.4287729 -4.4287963 -4.428781 -4.42875 -4.4287519][-4.4288278 -4.428833 -4.4288163 -4.428802 -4.4287963 -4.4287958 -4.4287782 -4.4287481 -4.4287548 -4.4287896 -4.4288177 -4.4288192 -4.4287744 -4.4287019 -4.4286714][-4.4288158 -4.4288125 -4.4288044 -4.42881 -4.4288187 -4.4288254 -4.4288249 -4.4288168 -4.4288254 -4.4288464 -4.4288573 -4.4288392 -4.4287767 -4.4286842 -4.4286265]]...]
INFO - root - 2017-12-10 04:32:14.025943: step 2110, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:41m:26s remains)
INFO - root - 2017-12-10 04:32:19.249857: step 2120, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:13m:40s remains)
INFO - root - 2017-12-10 04:32:24.675861: step 2130, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:05m:23s remains)
INFO - root - 2017-12-10 04:32:29.864639: step 2140, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.511 sec/batch; 46h:53m:30s remains)
INFO - root - 2017-12-10 04:32:35.136713: step 2150, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.539 sec/batch; 49h:27m:07s remains)
INFO - root - 2017-12-10 04:32:40.361162: step 2160, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.518 sec/batch; 47h:32m:07s remains)
INFO - root - 2017-12-10 04:32:45.197799: step 2170, loss = 2.28, batch loss = 2.23 (16.1 examples/sec; 0.498 sec/batch; 45h:40m:26s remains)
INFO - root - 2017-12-10 04:32:50.375933: step 2180, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:30m:49s remains)
INFO - root - 2017-12-10 04:32:55.587799: step 2190, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.504 sec/batch; 46h:13m:22s remains)
INFO - root - 2017-12-10 04:33:00.779109: step 2200, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:20m:46s remains)
2017-12-10 04:33:01.292271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289017 -4.4288993 -4.4289155 -4.4289336 -4.428906 -4.428833 -4.428731 -4.4286509 -4.4286346 -4.4286728 -4.4287076 -4.4287457 -4.4287763 -4.4288068 -4.4288244][-4.4289017 -4.4288974 -4.4289112 -4.4289303 -4.4289002 -4.4288197 -4.4287109 -4.428628 -4.4286184 -4.4286628 -4.4287148 -4.4287686 -4.4288087 -4.428844 -4.4288616][-4.428895 -4.4288993 -4.4289174 -4.4289341 -4.4288955 -4.4288049 -4.4286876 -4.4286003 -4.4286032 -4.4286604 -4.4287338 -4.4288044 -4.4288545 -4.4288945 -4.4289165][-4.428884 -4.4289031 -4.4289274 -4.428936 -4.4288821 -4.4287734 -4.4286318 -4.4285331 -4.4285469 -4.4286284 -4.4287376 -4.4288259 -4.4288859 -4.4289293 -4.4289479][-4.4288788 -4.4289103 -4.4289355 -4.4289312 -4.4288588 -4.4287248 -4.4285479 -4.42843 -4.4284587 -4.4285812 -4.4287324 -4.4288425 -4.4289117 -4.4289508 -4.4289579][-4.4288726 -4.428916 -4.4289446 -4.428925 -4.4288287 -4.4286623 -4.4284453 -4.4283023 -4.4283442 -4.4285235 -4.42872 -4.4288568 -4.428937 -4.4289689 -4.4289594][-4.4288688 -4.4289193 -4.4289484 -4.42892 -4.4288106 -4.428616 -4.4283614 -4.4281797 -4.4282174 -4.4284406 -4.4286795 -4.4288478 -4.4289427 -4.4289708 -4.4289465][-4.4288888 -4.428937 -4.4289613 -4.4289265 -4.4288149 -4.4286141 -4.4283462 -4.4281316 -4.428144 -4.4283733 -4.4286361 -4.42883 -4.4289393 -4.4289665 -4.4289336][-4.4289346 -4.4289746 -4.4289875 -4.4289484 -4.428844 -4.4286647 -4.42841 -4.428185 -4.4281611 -4.4283595 -4.428617 -4.4288211 -4.4289374 -4.428966 -4.4289317][-4.4289751 -4.4290071 -4.4290133 -4.428978 -4.4288917 -4.4287453 -4.4285283 -4.4283171 -4.4282584 -4.4284005 -4.4286246 -4.4288173 -4.4289303 -4.4289589 -4.4289312][-4.4289937 -4.4290166 -4.4290214 -4.428998 -4.4289341 -4.428823 -4.4286571 -4.4284806 -4.4284015 -4.4284844 -4.4286628 -4.4288254 -4.4289217 -4.4289522 -4.4289322][-4.4290032 -4.4290113 -4.4290113 -4.4289989 -4.4289527 -4.4288726 -4.4287572 -4.4286261 -4.4285493 -4.4285951 -4.4287262 -4.4288468 -4.4289212 -4.4289513 -4.4289417][-4.4290137 -4.4290061 -4.428997 -4.4289875 -4.4289565 -4.4288964 -4.4288135 -4.4287257 -4.4286714 -4.4287047 -4.4287987 -4.4288816 -4.4289365 -4.4289622 -4.4289579][-4.4290171 -4.4289985 -4.4289794 -4.4289675 -4.4289441 -4.4288988 -4.4288421 -4.42879 -4.428762 -4.4287891 -4.428854 -4.4289074 -4.4289455 -4.4289646 -4.4289637][-4.4290104 -4.4289885 -4.4289637 -4.4289465 -4.4289269 -4.4288964 -4.4288616 -4.4288335 -4.42882 -4.4288411 -4.4288855 -4.4289222 -4.4289474 -4.4289603 -4.4289603]]...]
INFO - root - 2017-12-10 04:33:06.496007: step 2210, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.540 sec/batch; 49h:35m:11s remains)
INFO - root - 2017-12-10 04:33:11.679614: step 2220, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.509 sec/batch; 46h:41m:40s remains)
INFO - root - 2017-12-10 04:33:16.850258: step 2230, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:45m:06s remains)
INFO - root - 2017-12-10 04:33:22.001968: step 2240, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:22m:56s remains)
INFO - root - 2017-12-10 04:33:27.234310: step 2250, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.508 sec/batch; 46h:36m:33s remains)
INFO - root - 2017-12-10 04:33:32.445378: step 2260, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.523 sec/batch; 47h:57m:32s remains)
INFO - root - 2017-12-10 04:33:37.341956: step 2270, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 46h:55m:39s remains)
INFO - root - 2017-12-10 04:33:42.523623: step 2280, loss = 2.28, batch loss = 2.23 (16.0 examples/sec; 0.500 sec/batch; 45h:50m:59s remains)
INFO - root - 2017-12-10 04:33:47.738212: step 2290, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:30m:57s remains)
INFO - root - 2017-12-10 04:33:53.007990: step 2300, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 47h:46m:08s remains)
2017-12-10 04:33:53.512842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286 -4.4285293 -4.4285369 -4.4286094 -4.4287004 -4.4287491 -4.4287424 -4.4287066 -4.4286795 -4.4286823 -4.42868 -4.4286838 -4.4287105 -4.4287562 -4.4288044][-4.4286084 -4.4285593 -4.4285693 -4.4286318 -4.4287148 -4.4287581 -4.42876 -4.4287472 -4.4287386 -4.4287467 -4.4287205 -4.4286928 -4.4286985 -4.4287424 -4.4287992][-4.4286308 -4.4285913 -4.4285927 -4.428637 -4.4287028 -4.4287381 -4.428751 -4.42876 -4.4287705 -4.4287825 -4.4287548 -4.4287109 -4.4286981 -4.4287281 -4.4287825][-4.4286523 -4.4286184 -4.4286036 -4.4286246 -4.4286685 -4.4287038 -4.4287291 -4.4287457 -4.4287581 -4.4287682 -4.4287438 -4.4287071 -4.4286928 -4.4287019 -4.4287362][-4.4286757 -4.4286408 -4.4286184 -4.4286218 -4.4286408 -4.4286709 -4.4287009 -4.4287186 -4.4287238 -4.4287252 -4.4287114 -4.4286942 -4.4286857 -4.42868 -4.4286895][-4.428699 -4.428668 -4.4286447 -4.4286327 -4.4286284 -4.4286461 -4.428678 -4.4286995 -4.4287024 -4.4287019 -4.4286995 -4.4286947 -4.4286914 -4.4286823 -4.4286761][-4.42872 -4.4286919 -4.428669 -4.4286447 -4.4286256 -4.4286318 -4.4286575 -4.4286742 -4.4286857 -4.4286981 -4.4287066 -4.4287081 -4.428709 -4.4287004 -4.4286842][-4.4287305 -4.4287009 -4.4286823 -4.4286509 -4.4286227 -4.4286213 -4.4286351 -4.4286389 -4.4286523 -4.4286847 -4.4287138 -4.4287267 -4.42873 -4.4287214 -4.4287024][-4.4287405 -4.4287167 -4.4287095 -4.4286814 -4.4286475 -4.428628 -4.4286256 -4.4286151 -4.4286284 -4.4286761 -4.4287224 -4.4287457 -4.428751 -4.4287457 -4.4287238][-4.4287567 -4.4287486 -4.4287577 -4.4287481 -4.4287167 -4.4286809 -4.4286542 -4.4286308 -4.4286408 -4.4287004 -4.4287548 -4.4287791 -4.428781 -4.4287729 -4.4287462][-4.4287663 -4.428771 -4.428792 -4.428802 -4.4287872 -4.4287486 -4.4287 -4.4286647 -4.4286709 -4.4287324 -4.4287834 -4.428802 -4.4287972 -4.4287796 -4.4287405][-4.4287763 -4.4287786 -4.4288006 -4.4288211 -4.4288254 -4.4287934 -4.4287333 -4.4286909 -4.4286838 -4.4287257 -4.4287624 -4.4287791 -4.4287724 -4.4287481 -4.4286976][-4.4287906 -4.4287806 -4.4287887 -4.4288073 -4.4288259 -4.428813 -4.4287605 -4.4287171 -4.4286928 -4.4286971 -4.4287124 -4.428731 -4.4287324 -4.4287133 -4.4286637][-4.4287958 -4.4287715 -4.4287696 -4.4287906 -4.4288182 -4.428822 -4.4287748 -4.4287233 -4.4286838 -4.4286604 -4.4286571 -4.428678 -4.4286952 -4.4286966 -4.4286623][-4.4287977 -4.4287677 -4.4287562 -4.4287715 -4.4288015 -4.428813 -4.4287748 -4.428719 -4.4286737 -4.4286442 -4.4286366 -4.4286652 -4.4286909 -4.4286971 -4.4286742]]...]
INFO - root - 2017-12-10 04:33:58.657013: step 2310, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.507 sec/batch; 46h:29m:01s remains)
INFO - root - 2017-12-10 04:34:03.806426: step 2320, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.510 sec/batch; 46h:48m:46s remains)
INFO - root - 2017-12-10 04:34:09.052109: step 2330, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.544 sec/batch; 49h:54m:54s remains)
INFO - root - 2017-12-10 04:34:14.272484: step 2340, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.513 sec/batch; 47h:04m:29s remains)
INFO - root - 2017-12-10 04:34:19.463902: step 2350, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 48h:54m:12s remains)
INFO - root - 2017-12-10 04:34:24.630362: step 2360, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.514 sec/batch; 47h:10m:11s remains)
INFO - root - 2017-12-10 04:34:29.515651: step 2370, loss = 2.28, batch loss = 2.23 (15.7 examples/sec; 0.509 sec/batch; 46h:42m:17s remains)
INFO - root - 2017-12-10 04:34:34.709271: step 2380, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:43m:22s remains)
INFO - root - 2017-12-10 04:34:39.965650: step 2390, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.546 sec/batch; 50h:02m:54s remains)
INFO - root - 2017-12-10 04:34:45.331145: step 2400, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:19m:51s remains)
2017-12-10 04:34:45.830171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42889 -4.4288883 -4.4288855 -4.4288826 -4.4288845 -4.428896 -4.4289069 -4.4289145 -4.4289136 -4.4289017 -4.428884 -4.4288678 -4.4288721 -4.4288945 -4.42892][-4.4288526 -4.4288459 -4.428843 -4.4288354 -4.428834 -4.4288526 -4.4288754 -4.42889 -4.4288945 -4.428884 -4.4288645 -4.4288406 -4.428834 -4.4288526 -4.4288874][-4.428844 -4.42883 -4.4288197 -4.4287958 -4.4287734 -4.42878 -4.4288087 -4.4288321 -4.4288454 -4.428843 -4.4288335 -4.42881 -4.4287977 -4.42881 -4.4288459][-4.4288554 -4.4288368 -4.4288158 -4.4287648 -4.4287038 -4.4286761 -4.4286942 -4.42873 -4.4287648 -4.428782 -4.428793 -4.4287829 -4.4287744 -4.4287815 -4.4288111][-4.4288659 -4.4288468 -4.4288173 -4.4287424 -4.4286385 -4.4285545 -4.4285431 -4.4285984 -4.4286656 -4.4287124 -4.4287491 -4.4287624 -4.4287658 -4.4287715 -4.4287939][-4.4288673 -4.428853 -4.4288278 -4.4287481 -4.4286175 -4.4284773 -4.428421 -4.4284854 -4.4285846 -4.4286613 -4.4287248 -4.4287639 -4.42878 -4.4287829 -4.4287968][-4.42885 -4.4288406 -4.4288249 -4.4287629 -4.4286475 -4.4285045 -4.42841 -4.4284425 -4.4285431 -4.4286385 -4.4287205 -4.4287782 -4.4288077 -4.4288077 -4.428812][-4.428854 -4.4288511 -4.4288478 -4.428812 -4.4287477 -4.42865 -4.4285436 -4.428503 -4.428544 -4.4286242 -4.4287119 -4.4287863 -4.4288325 -4.4288387 -4.4288411][-4.4288659 -4.4288783 -4.4288936 -4.4288878 -4.4288669 -4.4288206 -4.4287195 -4.4286146 -4.4285717 -4.428618 -4.4287033 -4.4287839 -4.4288406 -4.4288554 -4.4288611][-4.4288478 -4.4288764 -4.4289074 -4.4289227 -4.4289246 -4.4289007 -4.4288077 -4.42867 -4.428576 -4.4285913 -4.4286633 -4.4287491 -4.4288187 -4.4288478 -4.4288621][-4.4288163 -4.428854 -4.4288907 -4.42891 -4.4289165 -4.4288964 -4.4288216 -4.42869 -4.4285808 -4.4285741 -4.428637 -4.4287229 -4.4288006 -4.4288449 -4.4288688][-4.4287992 -4.4288359 -4.4288659 -4.4288788 -4.4288797 -4.428865 -4.4288082 -4.4287019 -4.4286036 -4.4285851 -4.428637 -4.4287128 -4.4287877 -4.42884 -4.428874][-4.428822 -4.4288478 -4.4288659 -4.4288707 -4.428863 -4.4288516 -4.428813 -4.4287424 -4.4286704 -4.4286475 -4.4286819 -4.4287372 -4.4287972 -4.4288473 -4.4288855][-4.428874 -4.4288864 -4.4288926 -4.4288917 -4.4288864 -4.4288793 -4.4288621 -4.428822 -4.4287786 -4.4287577 -4.4287734 -4.4288058 -4.4288468 -4.4288869 -4.4289207][-4.4289188 -4.4289212 -4.4289227 -4.4289222 -4.4289236 -4.4289193 -4.4289112 -4.42889 -4.4288688 -4.4288554 -4.4288621 -4.4288783 -4.4289021 -4.42893 -4.4289551]]...]
INFO - root - 2017-12-10 04:34:50.975745: step 2410, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.535 sec/batch; 49h:03m:02s remains)
INFO - root - 2017-12-10 04:34:56.360181: step 2420, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:22m:09s remains)
INFO - root - 2017-12-10 04:35:01.671250: step 2430, loss = 2.28, batch loss = 2.23 (16.0 examples/sec; 0.499 sec/batch; 45h:42m:47s remains)
