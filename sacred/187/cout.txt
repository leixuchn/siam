INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "187"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-10 05:05:56.422871: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:05:56.422942: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:05:56.422968: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:05:56.422991: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:05:56.423012: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 05:06:00.735027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-10 05:06:00.735060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 05:06:00.735067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 05:06:00.735075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-10 05:06:19.157090: step 0, loss = 2.28, batch loss = 2.23 (0.7 examples/sec; 10.836 sec/batch; 1000h:50m:15s remains)
2017-12-10 05:06:19.888079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290171 -4.4290004 -4.4289775 -4.4289441 -4.4289021 -4.4288535 -4.4288096 -4.4287744 -4.4287772 -4.4288063 -4.4288478 -4.428896 -4.4289446 -4.4289885 -4.4290214][-4.4290061 -4.4289804 -4.4289412 -4.428884 -4.4288158 -4.4287391 -4.4286723 -4.4286256 -4.428638 -4.428689 -4.4287553 -4.4288282 -4.428896 -4.4289556 -4.4290042][-4.4289932 -4.4289656 -4.4289107 -4.4288263 -4.4287233 -4.4286075 -4.4284987 -4.4284277 -4.428452 -4.4285364 -4.4286418 -4.4287515 -4.4288425 -4.4289193 -4.4289832][-4.4289784 -4.4289536 -4.4288821 -4.4287629 -4.4286151 -4.4284468 -4.4282794 -4.4281769 -4.42823 -4.428369 -4.428535 -4.4286933 -4.4288116 -4.4289012 -4.4289713][-4.4289579 -4.4289308 -4.4288344 -4.4286757 -4.4284768 -4.4282432 -4.428 -4.4278779 -4.4279962 -4.428215 -4.4284506 -4.4286561 -4.4287992 -4.4288993 -4.4289713][-4.4289246 -4.4288898 -4.4287734 -4.4285865 -4.4283428 -4.4280415 -4.4277229 -4.4276257 -4.4278612 -4.4281597 -4.4284368 -4.4286575 -4.4288058 -4.4289103 -4.4289794][-4.4288769 -4.4288282 -4.4287028 -4.4285121 -4.4282579 -4.4279375 -4.42762 -4.4276175 -4.4279408 -4.4282579 -4.4285159 -4.4287066 -4.4288335 -4.4289274 -4.4289889][-4.4288197 -4.4287605 -4.4286485 -4.428493 -4.4282913 -4.428051 -4.4278483 -4.427928 -4.428216 -4.4284673 -4.4286547 -4.4287868 -4.4288783 -4.4289503 -4.429][-4.428772 -4.428719 -4.4286356 -4.4285288 -4.4284072 -4.4282751 -4.4281874 -4.4282827 -4.4284825 -4.4286494 -4.4287653 -4.4288454 -4.4289079 -4.4289656 -4.429009][-4.4287424 -4.4286966 -4.428637 -4.4285679 -4.4285092 -4.4284568 -4.4284387 -4.4285173 -4.4286413 -4.4287453 -4.428813 -4.4288659 -4.4289165 -4.42897 -4.4290109][-4.4287329 -4.4286942 -4.4286509 -4.428606 -4.4285851 -4.4285827 -4.4286003 -4.42866 -4.4287314 -4.4287968 -4.4288416 -4.4288869 -4.4289322 -4.4289808 -4.4290161][-4.4287515 -4.4287252 -4.4286985 -4.4286757 -4.4286819 -4.4287076 -4.4287329 -4.4287667 -4.4287944 -4.4288306 -4.4288635 -4.4289088 -4.4289536 -4.4289947 -4.4290223][-4.4288034 -4.428791 -4.4287834 -4.4287791 -4.4287934 -4.4288158 -4.428823 -4.4288211 -4.428813 -4.4288306 -4.4288635 -4.4289145 -4.4289618 -4.4289975 -4.4290223][-4.4288516 -4.4288516 -4.4288549 -4.4288554 -4.4288578 -4.4288568 -4.4288378 -4.4288063 -4.4287806 -4.4287968 -4.4288363 -4.4288988 -4.4289522 -4.4289875 -4.4290147][-4.4288626 -4.42887 -4.4288812 -4.4288836 -4.4288692 -4.4288406 -4.4287953 -4.4287491 -4.4287248 -4.42875 -4.4288015 -4.4288778 -4.4289365 -4.4289751 -4.4290075]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr1-lastlr5-clip10000/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr1-lastlr5-clip10000/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 05:06:26.389458: step 10, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:33m:25s remains)
INFO - root - 2017-12-10 05:06:31.771446: step 20, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 49h:28m:45s remains)
INFO - root - 2017-12-10 05:06:36.753298: step 30, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:40m:58s remains)
INFO - root - 2017-12-10 05:06:42.075673: step 40, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.539 sec/batch; 49h:45m:15s remains)
INFO - root - 2017-12-10 05:06:47.414777: step 50, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 49h:18m:26s remains)
INFO - root - 2017-12-10 05:06:52.700794: step 60, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.539 sec/batch; 49h:44m:12s remains)
INFO - root - 2017-12-10 05:06:58.059363: step 70, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.542 sec/batch; 50h:00m:50s remains)
INFO - root - 2017-12-10 05:07:03.343435: step 80, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:09m:46s remains)
INFO - root - 2017-12-10 05:07:08.628325: step 90, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 49h:07m:22s remains)
INFO - root - 2017-12-10 05:07:13.926108: step 100, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.548 sec/batch; 50h:35m:01s remains)
2017-12-10 05:07:14.460872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288673 -4.4287977 -4.4287219 -4.4286838 -4.4286885 -4.4287157 -4.4287505 -4.4288154 -4.4288797 -4.4289103 -4.4289036 -4.4288764 -4.4288311 -4.4287796 -4.4287519][-4.4288731 -4.428792 -4.4286942 -4.4286323 -4.42863 -4.4286633 -4.4287047 -4.42877 -4.4288316 -4.4288645 -4.4288654 -4.4288473 -4.4288125 -4.428772 -4.4287567][-4.4288716 -4.4287839 -4.4286771 -4.4286032 -4.4285932 -4.4286222 -4.4286618 -4.4287205 -4.4287796 -4.4288144 -4.4288211 -4.4288087 -4.4287863 -4.4287558 -4.4287472][-4.4288664 -4.4287767 -4.4286737 -4.4286 -4.4285827 -4.4286022 -4.4286351 -4.4286871 -4.4287467 -4.4287863 -4.4287972 -4.4287877 -4.428772 -4.4287462 -4.4287362][-4.42886 -4.4287705 -4.4286733 -4.4285989 -4.4285693 -4.4285765 -4.4286 -4.4286475 -4.42871 -4.42876 -4.4287796 -4.4287739 -4.4287624 -4.4287434 -4.4287324][-4.4288554 -4.4287677 -4.4286752 -4.428596 -4.4285522 -4.4285445 -4.4285502 -4.4285822 -4.4286442 -4.4287105 -4.428741 -4.4287415 -4.4287386 -4.4287329 -4.42873][-4.4288545 -4.4287682 -4.4286742 -4.4285865 -4.4285274 -4.4285045 -4.4284892 -4.4284978 -4.4285564 -4.4286361 -4.428678 -4.428688 -4.4286952 -4.4287052 -4.4287128][-4.428854 -4.4287672 -4.4286704 -4.4285808 -4.4285178 -4.4284878 -4.4284616 -4.4284554 -4.4285064 -4.4285865 -4.428637 -4.428658 -4.428679 -4.428699 -4.4287128][-4.4288554 -4.4287715 -4.4286776 -4.4285951 -4.42854 -4.4285135 -4.4284925 -4.4284892 -4.428535 -4.4286003 -4.4286385 -4.4286566 -4.428678 -4.4286962 -4.4287062][-4.4288554 -4.428772 -4.42868 -4.4286036 -4.4285541 -4.4285321 -4.4285226 -4.4285378 -4.4285874 -4.4286389 -4.4286604 -4.4286685 -4.4286819 -4.4286909 -4.4286923][-4.4288583 -4.4287748 -4.4286861 -4.4286156 -4.4285684 -4.4285469 -4.4285493 -4.4285769 -4.4286242 -4.4286623 -4.4286742 -4.4286752 -4.4286809 -4.4286876 -4.4286914][-4.4288712 -4.428793 -4.4287128 -4.4286523 -4.4286108 -4.4285932 -4.428607 -4.4286385 -4.4286747 -4.428699 -4.4287047 -4.4287024 -4.4287043 -4.4287095 -4.4287138][-4.428895 -4.4288287 -4.428761 -4.4287095 -4.4286757 -4.4286656 -4.4286895 -4.4287252 -4.4287515 -4.428762 -4.428761 -4.4287548 -4.4287519 -4.4287529 -4.4287558][-4.42893 -4.42888 -4.4288287 -4.4287853 -4.4287572 -4.4287539 -4.4287815 -4.4288173 -4.4288368 -4.4288397 -4.4288349 -4.4288235 -4.4288163 -4.4288139 -4.4288173][-4.4289651 -4.4289346 -4.4289036 -4.4288735 -4.4288516 -4.4288492 -4.4288707 -4.4288974 -4.42891 -4.42891 -4.4289036 -4.4288936 -4.4288859 -4.428884 -4.4288883]]...]
INFO - root - 2017-12-10 05:07:19.845364: step 110, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 49h:20m:39s remains)
INFO - root - 2017-12-10 05:07:25.258556: step 120, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.540 sec/batch; 49h:49m:50s remains)
INFO - root - 2017-12-10 05:07:30.403346: step 130, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:35m:13s remains)
INFO - root - 2017-12-10 05:07:35.849714: step 140, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.543 sec/batch; 50h:09m:28s remains)
INFO - root - 2017-12-10 05:07:41.231060: step 150, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.549 sec/batch; 50h:39m:55s remains)
INFO - root - 2017-12-10 05:07:46.722890: step 160, loss = 2.28, batch loss = 2.23 (14.5 examples/sec; 0.553 sec/batch; 51h:05m:18s remains)
INFO - root - 2017-12-10 05:07:52.002707: step 170, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:33m:02s remains)
INFO - root - 2017-12-10 05:07:57.348085: step 180, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:32m:56s remains)
INFO - root - 2017-12-10 05:08:02.606262: step 190, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:44m:11s remains)
INFO - root - 2017-12-10 05:08:07.849761: step 200, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:29m:23s remains)
2017-12-10 05:08:08.418884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288645 -4.4288559 -4.4288511 -4.4288406 -4.4288268 -4.4288125 -4.428802 -4.4288 -4.4288063 -4.4288158 -4.4288259 -4.4288335 -4.4288363 -4.4288354 -4.4288311][-4.4288282 -4.4288168 -4.4288092 -4.4287887 -4.428762 -4.4287362 -4.42872 -4.4287219 -4.4287415 -4.4287677 -4.428793 -4.428812 -4.4288182 -4.4288125 -4.4287963][-4.4287758 -4.4287643 -4.4287553 -4.4287281 -4.4286909 -4.4286561 -4.4286361 -4.4286447 -4.4286809 -4.4287271 -4.4287691 -4.4288015 -4.42881 -4.4287939 -4.4287605][-4.4287109 -4.4287009 -4.4286966 -4.4286723 -4.4286308 -4.4285855 -4.4285541 -4.4285583 -4.4286089 -4.428678 -4.4287415 -4.42879 -4.4288025 -4.4287744 -4.4287248][-4.4286671 -4.4286551 -4.4286489 -4.4286242 -4.4285736 -4.4285049 -4.4284363 -4.4284163 -4.4284797 -4.4285865 -4.4286895 -4.4287653 -4.4287896 -4.4287572 -4.4286928][-4.4286485 -4.4286265 -4.4286103 -4.428576 -4.4285059 -4.4283967 -4.428266 -4.4282036 -4.4282866 -4.4284482 -4.4286079 -4.4287219 -4.4287653 -4.4287367 -4.4286709][-4.4286308 -4.4285994 -4.42857 -4.42852 -4.428432 -4.42829 -4.4281077 -4.428021 -4.4281459 -4.4283705 -4.4285808 -4.4287229 -4.4287772 -4.4287519 -4.428689][-4.428627 -4.4285903 -4.4285479 -4.428493 -4.4284153 -4.4282956 -4.4281411 -4.4280868 -4.4282284 -4.4284515 -4.4286504 -4.4287763 -4.4288149 -4.4287777 -4.4287128][-4.4286351 -4.4286051 -4.4285622 -4.4285221 -4.4284773 -4.4284067 -4.4283023 -4.4282646 -4.428371 -4.4285431 -4.4287004 -4.4287992 -4.4288211 -4.4287739 -4.4287138][-4.4286618 -4.4286523 -4.4286256 -4.4286 -4.4285712 -4.4285212 -4.4284339 -4.4283767 -4.4284172 -4.4285288 -4.4286537 -4.4287438 -4.428772 -4.4287419 -4.4287033][-4.4286742 -4.428688 -4.4286833 -4.428668 -4.4286385 -4.4285812 -4.4284854 -4.4283948 -4.4283681 -4.4284286 -4.4285369 -4.4286427 -4.4287109 -4.4287271 -4.4287167][-4.4286542 -4.42869 -4.4287071 -4.4287024 -4.4286752 -4.4286089 -4.4285064 -4.4284005 -4.4283381 -4.42837 -4.4284663 -4.4285836 -4.4286828 -4.4287305 -4.4287353][-4.4286227 -4.4286838 -4.4287233 -4.4287319 -4.4287128 -4.4286461 -4.4285493 -4.4284563 -4.4283991 -4.4284205 -4.4284945 -4.4285874 -4.4286785 -4.4287319 -4.4287348][-4.4285836 -4.4286709 -4.4287367 -4.428761 -4.4287529 -4.4286966 -4.428616 -4.4285445 -4.4284992 -4.4285045 -4.4285407 -4.4285855 -4.4286418 -4.4286842 -4.4286861][-4.4285407 -4.4286427 -4.4287276 -4.4287643 -4.4287682 -4.4287324 -4.4286771 -4.4286261 -4.42858 -4.4285588 -4.4285564 -4.4285526 -4.4285674 -4.4285941 -4.4286056]]...]
INFO - root - 2017-12-10 05:08:13.590103: step 210, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:37m:07s remains)
INFO - root - 2017-12-10 05:08:18.830207: step 220, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.532 sec/batch; 49h:08m:35s remains)
INFO - root - 2017-12-10 05:08:23.869765: step 230, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.520 sec/batch; 47h:59m:01s remains)
INFO - root - 2017-12-10 05:08:29.158524: step 240, loss = 2.28, batch loss = 2.23 (15.8 examples/sec; 0.508 sec/batch; 46h:52m:30s remains)
INFO - root - 2017-12-10 05:08:34.442724: step 250, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:11m:59s remains)
INFO - root - 2017-12-10 05:08:39.732586: step 260, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:26m:11s remains)
INFO - root - 2017-12-10 05:08:45.015520: step 270, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.542 sec/batch; 50h:02m:28s remains)
INFO - root - 2017-12-10 05:08:50.368211: step 280, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.524 sec/batch; 48h:20m:40s remains)
INFO - root - 2017-12-10 05:08:55.657008: step 290, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 49h:15m:50s remains)
INFO - root - 2017-12-10 05:09:01.017631: step 300, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.533 sec/batch; 49h:12m:06s remains)
2017-12-10 05:09:01.547306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286966 -4.4286752 -4.4286618 -4.42866 -4.4286623 -4.4286537 -4.4286532 -4.4286671 -4.4286995 -4.4287271 -4.4287319 -4.4287095 -4.4286647 -4.4286261 -4.4286146][-4.4286675 -4.4286733 -4.428688 -4.4287009 -4.4287043 -4.4286795 -4.4286618 -4.4286704 -4.4287009 -4.4287181 -4.4287143 -4.4286938 -4.428658 -4.4286375 -4.4286456][-4.4286447 -4.4286709 -4.4287148 -4.4287419 -4.4287467 -4.4287114 -4.4286776 -4.4286637 -4.4286685 -4.42867 -4.4286656 -4.4286623 -4.4286523 -4.4286623 -4.428688][-4.4286561 -4.4286814 -4.4287329 -4.4287643 -4.4287643 -4.4287133 -4.4286532 -4.4286094 -4.4285769 -4.4285588 -4.4285693 -4.428607 -4.428647 -4.4286909 -4.4287262][-4.42872 -4.4287267 -4.4287624 -4.4287796 -4.4287648 -4.428688 -4.4285889 -4.4285054 -4.4284387 -4.4284 -4.42843 -4.4285183 -4.4286218 -4.4287057 -4.42875][-4.428802 -4.4287958 -4.4288111 -4.4288092 -4.4287739 -4.4286752 -4.4285436 -4.428422 -4.4283247 -4.4282713 -4.4283051 -4.4284143 -4.4285588 -4.4286828 -4.4287376][-4.4288549 -4.4288454 -4.4288497 -4.4288383 -4.4287987 -4.4287038 -4.4285784 -4.4284534 -4.4283428 -4.4282765 -4.428287 -4.4283657 -4.4285 -4.4286389 -4.4287057][-4.4288874 -4.4288754 -4.4288697 -4.42886 -4.4288321 -4.4287615 -4.4286675 -4.4285727 -4.428472 -4.4283991 -4.4283743 -4.4284 -4.4284935 -4.4286146 -4.4286809][-4.4289069 -4.4288979 -4.4288907 -4.4288826 -4.428864 -4.4288216 -4.4287667 -4.4287105 -4.4286265 -4.4285507 -4.4284983 -4.4284797 -4.4285221 -4.4286013 -4.4286551][-4.4289041 -4.4289069 -4.4289074 -4.4289041 -4.4288955 -4.4288764 -4.4288526 -4.428823 -4.4287605 -4.4286919 -4.428628 -4.4285779 -4.42857 -4.4286013 -4.4286332][-4.4288783 -4.4288917 -4.4289031 -4.4289107 -4.4289174 -4.4289165 -4.4289083 -4.4288888 -4.4288378 -4.4287825 -4.4287286 -4.4286752 -4.4286351 -4.42862 -4.4286175][-4.4288554 -4.4288721 -4.4288878 -4.4288988 -4.428916 -4.4289241 -4.4289212 -4.4289093 -4.4288707 -4.4288282 -4.4287934 -4.4287553 -4.4287057 -4.4286556 -4.428618][-4.4288607 -4.4288716 -4.4288788 -4.4288826 -4.4288969 -4.4289041 -4.428906 -4.4289017 -4.4288812 -4.428853 -4.4288344 -4.428812 -4.428761 -4.4286928 -4.42863][-4.42888 -4.4288754 -4.428865 -4.4288545 -4.4288549 -4.42886 -4.428875 -4.4288826 -4.4288788 -4.4288654 -4.4288573 -4.4288449 -4.4288 -4.42872 -4.428638][-4.4288707 -4.4288545 -4.4288344 -4.4288144 -4.4288054 -4.4288087 -4.4288359 -4.4288597 -4.4288688 -4.42887 -4.428865 -4.4288549 -4.4288211 -4.4287477 -4.4286542]]...]
INFO - root - 2017-12-10 05:09:06.896192: step 310, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:34m:13s remains)
INFO - root - 2017-12-10 05:09:12.169420: step 320, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.518 sec/batch; 47h:49m:57s remains)
INFO - root - 2017-12-10 05:09:17.104235: step 330, loss = 2.28, batch loss = 2.23 (16.3 examples/sec; 0.491 sec/batch; 45h:18m:21s remains)
INFO - root - 2017-12-10 05:09:22.341223: step 340, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.516 sec/batch; 47h:34m:34s remains)
INFO - root - 2017-12-10 05:09:27.671980: step 350, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 49h:00m:25s remains)
INFO - root - 2017-12-10 05:09:32.981682: step 360, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.522 sec/batch; 48h:09m:16s remains)
INFO - root - 2017-12-10 05:09:38.327149: step 370, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:59m:48s remains)
INFO - root - 2017-12-10 05:09:43.600737: step 380, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.529 sec/batch; 48h:46m:11s remains)
INFO - root - 2017-12-10 05:09:48.883974: step 390, loss = 2.28, batch loss = 2.23 (14.6 examples/sec; 0.549 sec/batch; 50h:36m:55s remains)
INFO - root - 2017-12-10 05:09:54.188205: step 400, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.519 sec/batch; 47h:51m:52s remains)
2017-12-10 05:09:54.807326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287181 -4.42871 -4.4287043 -4.4286942 -4.4286923 -4.4287095 -4.4287267 -4.4287615 -4.4287829 -4.4288025 -4.4288549 -4.4289 -4.4289117 -4.4289074 -4.4288845][-4.4286747 -4.4286785 -4.4286981 -4.428699 -4.4286981 -4.4287066 -4.428709 -4.4287219 -4.42874 -4.4287739 -4.42884 -4.4288936 -4.4289088 -4.4289083 -4.4288878][-4.4286489 -4.4286523 -4.4286876 -4.4287038 -4.4287019 -4.4286866 -4.4286623 -4.428647 -4.4286628 -4.4287224 -4.4288087 -4.4288774 -4.428905 -4.4289083 -4.42889][-4.4286871 -4.4286823 -4.4287148 -4.4287224 -4.4286976 -4.4286489 -4.4285865 -4.4285336 -4.4285364 -4.4286284 -4.4287405 -4.4288278 -4.4288769 -4.4288831 -4.4288683][-4.4287663 -4.4287577 -4.4287591 -4.4287295 -4.4286695 -4.4285913 -4.4284835 -4.4283686 -4.4283485 -4.4284968 -4.4286532 -4.4287586 -4.4288239 -4.42883 -4.4288158][-4.4288263 -4.4288149 -4.428781 -4.4287124 -4.4286251 -4.4285264 -4.4283624 -4.4281597 -4.4281092 -4.4283357 -4.4285526 -4.4286742 -4.4287496 -4.4287643 -4.4287477][-4.42884 -4.4288244 -4.4287744 -4.4286852 -4.428586 -4.4284763 -4.428277 -4.4280033 -4.4279089 -4.4281778 -4.4284372 -4.4285736 -4.4286571 -4.4286833 -4.4286637][-4.4288373 -4.4288316 -4.4287882 -4.4287028 -4.4286132 -4.4285154 -4.4283509 -4.4281173 -4.4280071 -4.428205 -4.428421 -4.4285426 -4.4286227 -4.4286561 -4.4286284][-4.4288707 -4.4288759 -4.4288468 -4.4287763 -4.4286957 -4.4286156 -4.4285121 -4.4283805 -4.428309 -4.4284048 -4.4285316 -4.4286122 -4.4286785 -4.4287133 -4.4286842][-4.4289241 -4.4289336 -4.4289117 -4.4288583 -4.4287887 -4.428719 -4.4286571 -4.4286089 -4.4285817 -4.4286218 -4.4286852 -4.4287324 -4.4287887 -4.428822 -4.4287987][-4.4289575 -4.4289718 -4.428957 -4.4289212 -4.4288678 -4.4288044 -4.4287634 -4.4287529 -4.42875 -4.4287696 -4.4288077 -4.4288392 -4.4288831 -4.4289103 -4.4288974][-4.4289436 -4.4289613 -4.428956 -4.4289427 -4.4289069 -4.42885 -4.4288154 -4.4288096 -4.4288063 -4.428813 -4.4288387 -4.4288645 -4.4289012 -4.4289222 -4.4289145][-4.4288845 -4.4289021 -4.4289093 -4.4289188 -4.4289045 -4.4288607 -4.4288311 -4.4288239 -4.4288116 -4.4287949 -4.4288034 -4.42882 -4.4288549 -4.428874 -4.4288607][-4.428803 -4.4288192 -4.4288378 -4.4288683 -4.4288826 -4.4288659 -4.4288459 -4.4288397 -4.4288211 -4.4287829 -4.4287653 -4.4287663 -4.428791 -4.4288006 -4.4287758][-4.4287205 -4.4287357 -4.4287639 -4.4288068 -4.4288487 -4.4288621 -4.4288549 -4.4288511 -4.428833 -4.4287934 -4.4287615 -4.4287467 -4.4287558 -4.4287505 -4.4287128]]...]
INFO - root - 2017-12-10 05:10:00.169607: step 410, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 47h:16m:23s remains)
INFO - root - 2017-12-10 05:10:05.466209: step 420, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:56m:20s remains)
INFO - root - 2017-12-10 05:10:10.351074: step 430, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.530 sec/batch; 48h:51m:51s remains)
INFO - root - 2017-12-10 05:10:15.628282: step 440, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.512 sec/batch; 47h:13m:03s remains)
INFO - root - 2017-12-10 05:10:20.852397: step 450, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.527 sec/batch; 48h:35m:19s remains)
INFO - root - 2017-12-10 05:10:26.123144: step 460, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.544 sec/batch; 50h:12m:16s remains)
INFO - root - 2017-12-10 05:10:31.356691: step 470, loss = 2.28, batch loss = 2.23 (15.9 examples/sec; 0.504 sec/batch; 46h:30m:35s remains)
INFO - root - 2017-12-10 05:10:36.613388: step 480, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:23m:49s remains)
INFO - root - 2017-12-10 05:10:41.891389: step 490, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.517 sec/batch; 47h:40m:18s remains)
INFO - root - 2017-12-10 05:10:47.212029: step 500, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:24m:57s remains)
2017-12-10 05:10:47.761895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288416 -4.4288387 -4.428834 -4.4288092 -4.42879 -4.4287844 -4.4287992 -4.4288306 -4.4288554 -4.4288592 -4.4288297 -4.4287648 -4.4287047 -4.4286489 -4.4285922][-4.4288049 -4.4287992 -4.4287839 -4.4287462 -4.4287291 -4.4287376 -4.4287739 -4.4288268 -4.428854 -4.4288459 -4.4287958 -4.4287109 -4.4286404 -4.4285851 -4.4285431][-4.42878 -4.4287643 -4.4287281 -4.4286737 -4.428658 -4.4286828 -4.4287438 -4.4288144 -4.4288416 -4.4288211 -4.4287515 -4.428658 -4.4285851 -4.4285436 -4.428535][-4.4287543 -4.4287262 -4.4286685 -4.4285936 -4.4285665 -4.4285951 -4.428668 -4.4287524 -4.42879 -4.4287653 -4.4286861 -4.4286022 -4.4285536 -4.4285431 -4.4285645][-4.4287181 -4.4286728 -4.4285851 -4.4284854 -4.4284372 -4.4284558 -4.4285278 -4.428628 -4.4286823 -4.4286566 -4.4285865 -4.4285359 -4.428534 -4.428565 -4.4286127][-4.4286761 -4.428617 -4.4285111 -4.4283934 -4.4283113 -4.4282904 -4.428329 -4.4284406 -4.4285259 -4.4285159 -4.4284797 -4.4284897 -4.428535 -4.4285831 -4.4286304][-4.4286356 -4.4285793 -4.4284792 -4.4283509 -4.4282188 -4.4281096 -4.4280791 -4.428205 -4.4283562 -4.4284067 -4.4284263 -4.4284792 -4.4285355 -4.4285746 -4.4286189][-4.4285803 -4.4285326 -4.4284554 -4.4283381 -4.428184 -4.4280157 -4.4279232 -4.4280558 -4.4282622 -4.4283762 -4.428453 -4.4285226 -4.4285655 -4.4285836 -4.4286132][-4.42852 -4.4284792 -4.4284339 -4.4283581 -4.4282417 -4.4280992 -4.428019 -4.4281249 -4.4282956 -4.4284096 -4.4285107 -4.4285841 -4.4286118 -4.4286108 -4.4286242][-4.4284945 -4.4284778 -4.4284754 -4.4284549 -4.428391 -4.4283009 -4.4282508 -4.4283109 -4.4284019 -4.4284616 -4.4285431 -4.4286108 -4.4286332 -4.4286232 -4.4286208][-4.4285455 -4.4285674 -4.4285927 -4.4285932 -4.4285622 -4.4285059 -4.428462 -4.4284735 -4.4284897 -4.4284863 -4.4285293 -4.428586 -4.42861 -4.4286094 -4.4286113][-4.428648 -4.4286737 -4.4286928 -4.42869 -4.4286704 -4.4286313 -4.4285817 -4.4285555 -4.4285274 -4.4284854 -4.4285016 -4.4285541 -4.4285903 -4.428617 -4.4286466][-4.4287324 -4.4287415 -4.4287496 -4.4287443 -4.428731 -4.4287057 -4.428658 -4.428617 -4.4285803 -4.4285426 -4.4285531 -4.4285984 -4.4286423 -4.4286871 -4.4287271][-4.428771 -4.4287767 -4.428781 -4.428772 -4.42876 -4.428741 -4.4287052 -4.428678 -4.4286637 -4.4286566 -4.4286666 -4.4286914 -4.4287233 -4.4287591 -4.428782][-4.4287643 -4.4287734 -4.4287791 -4.4287715 -4.4287605 -4.4287467 -4.4287295 -4.4287248 -4.4287348 -4.42875 -4.4287548 -4.4287572 -4.4287705 -4.42878 -4.4287772]]...]
INFO - root - 2017-12-10 05:10:53.032324: step 510, loss = 2.28, batch loss = 2.23 (14.5 examples/sec; 0.552 sec/batch; 50h:55m:40s remains)
INFO - root - 2017-12-10 05:10:58.356975: step 520, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.537 sec/batch; 49h:28m:50s remains)
INFO - root - 2017-12-10 05:11:03.437676: step 530, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.542 sec/batch; 49h:59m:57s remains)
INFO - root - 2017-12-10 05:11:08.756521: step 540, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.526 sec/batch; 48h:32m:26s remains)
INFO - root - 2017-12-10 05:11:14.154947: step 550, loss = 2.28, batch loss = 2.23 (14.8 examples/sec; 0.542 sec/batch; 49h:56m:01s remains)
INFO - root - 2017-12-10 05:11:19.521737: step 560, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.531 sec/batch; 48h:58m:00s remains)
INFO - root - 2017-12-10 05:11:24.884269: step 570, loss = 2.28, batch loss = 2.23 (15.4 examples/sec; 0.521 sec/batch; 48h:01m:15s remains)
INFO - root - 2017-12-10 05:11:30.133692: step 580, loss = 2.28, batch loss = 2.23 (15.1 examples/sec; 0.532 sec/batch; 49h:00m:26s remains)
INFO - root - 2017-12-10 05:11:35.383506: step 590, loss = 2.28, batch loss = 2.23 (15.5 examples/sec; 0.515 sec/batch; 47h:29m:05s remains)
INFO - root - 2017-12-10 05:11:40.534683: step 600, loss = 2.28, batch loss = 2.23 (16.4 examples/sec; 0.488 sec/batch; 44h:58m:37s remains)
2017-12-10 05:11:41.031550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288487 -4.4288149 -4.4287939 -4.42878 -4.4287739 -4.4287648 -4.428772 -4.4288225 -4.428885 -4.4289179 -4.4289222 -4.428915 -4.428885 -4.4288492 -4.4288516][-4.428822 -4.428791 -4.4287777 -4.4287734 -4.4287658 -4.4287539 -4.4287567 -4.4288025 -4.4288626 -4.4288955 -4.4288912 -4.4288754 -4.4288568 -4.4288483 -4.4288716][-4.4287891 -4.4287539 -4.4287419 -4.428751 -4.4287505 -4.4287381 -4.4287372 -4.4287806 -4.428833 -4.4288559 -4.428844 -4.4288244 -4.4288087 -4.42881 -4.4288445][-4.4287381 -4.4287052 -4.4286981 -4.4287143 -4.4287181 -4.4286947 -4.4286842 -4.4287181 -4.4287596 -4.4287806 -4.4287791 -4.4287705 -4.4287577 -4.4287562 -4.4287839][-4.4286823 -4.4286618 -4.4286704 -4.4286857 -4.4286709 -4.4286237 -4.4285994 -4.4286227 -4.4286661 -4.4287062 -4.4287257 -4.4287333 -4.4287176 -4.4286976 -4.4287043][-4.4286337 -4.4286389 -4.428658 -4.4286537 -4.4286013 -4.4285231 -4.4284844 -4.4285035 -4.4285674 -4.4286346 -4.4286695 -4.4286785 -4.4286518 -4.4286089 -4.4285908][-4.4286227 -4.4286351 -4.4286456 -4.4286113 -4.4285231 -4.4284158 -4.4283619 -4.4283795 -4.4284558 -4.4285307 -4.4285674 -4.4285741 -4.4285383 -4.4284806 -4.4284611][-4.4286337 -4.428638 -4.4286361 -4.4285893 -4.4284921 -4.4283805 -4.4283261 -4.4283395 -4.4284019 -4.4284492 -4.4284606 -4.428453 -4.4284134 -4.4283648 -4.4283786][-4.4286833 -4.4286742 -4.4286604 -4.4286184 -4.4285398 -4.428453 -4.4284096 -4.4284062 -4.42843 -4.4284329 -4.4284167 -4.428391 -4.4283519 -4.4283261 -4.4283733][-4.4287462 -4.4287319 -4.42872 -4.4286923 -4.4286437 -4.4285927 -4.4285693 -4.4285612 -4.42856 -4.4285393 -4.4285026 -4.428462 -4.4284158 -4.4284019 -4.4284582][-4.4288049 -4.4287891 -4.4287786 -4.4287663 -4.4287472 -4.4287262 -4.4287243 -4.4287248 -4.4287167 -4.4286966 -4.4286623 -4.428617 -4.4285712 -4.42856 -4.4286046][-4.4288282 -4.4288168 -4.4288034 -4.4287953 -4.4287944 -4.4287987 -4.4288173 -4.4288316 -4.4288297 -4.4288187 -4.4287958 -4.4287639 -4.428731 -4.4287224 -4.42875][-4.4288292 -4.4288177 -4.4288039 -4.4287925 -4.4287953 -4.4288144 -4.4288459 -4.4288673 -4.4288726 -4.4288707 -4.4288597 -4.4288406 -4.4288187 -4.4288135 -4.4288316][-4.4288216 -4.4288139 -4.428802 -4.4287915 -4.4287982 -4.4288244 -4.4288564 -4.4288731 -4.4288745 -4.4288692 -4.4288588 -4.4288459 -4.4288268 -4.4288216 -4.4288373][-4.428822 -4.4288249 -4.4288216 -4.4288192 -4.4288297 -4.4288507 -4.4288683 -4.4288721 -4.4288635 -4.4288464 -4.4288259 -4.4288092 -4.428791 -4.4287891 -4.42881]]...]
INFO - root - 2017-12-10 05:11:46.419357: step 610, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.535 sec/batch; 49h:21m:05s remains)
INFO - root - 2017-12-10 05:11:51.818592: step 620, loss = 2.28, batch loss = 2.23 (15.6 examples/sec; 0.513 sec/batch; 47h:18m:30s remains)
INFO - root - 2017-12-10 05:11:56.868090: step 630, loss = 2.28, batch loss = 2.23 (14.5 examples/sec; 0.551 sec/batch; 50h:45m:23s remains)
INFO - root - 2017-12-10 05:12:02.137235: step 640, loss = 2.28, batch loss = 2.23 (15.0 examples/sec; 0.534 sec/batch; 49h:14m:04s remains)
INFO - root - 2017-12-10 05:12:07.482510: step 650, loss = 2.28, batch loss = 2.23 (14.5 examples/sec; 0.552 sec/batch; 50h:54m:22s remains)
INFO - root - 2017-12-10 05:12:12.925651: step 660, loss = 2.28, batch loss = 2.23 (14.4 examples/sec; 0.556 sec/batch; 51h:17m:08s remains)
INFO - root - 2017-12-10 05:12:18.346149: step 670, loss = 2.28, batch loss = 2.23 (14.9 examples/sec; 0.536 sec/batch; 49h:26m:21s remains)
INFO - root - 2017-12-10 05:12:23.697539: step 680, loss = 2.28, batch loss = 2.23 (14.7 examples/sec; 0.545 sec/batch; 50h:12m:28s remains)
INFO - root - 2017-12-10 05:12:29.006768: step 690, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.525 sec/batch; 48h:24m:03s remains)
INFO - root - 2017-12-10 05:12:34.244751: step 700, loss = 2.28, batch loss = 2.23 (15.3 examples/sec; 0.521 sec/batch; 48h:03m:42s remains)
2017-12-10 05:12:34.765826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42893 -4.4289021 -4.4288745 -4.4288487 -4.4288325 -4.4288445 -4.4288778 -4.4289112 -4.4289327 -4.4289408 -4.4289355 -4.4289241 -4.4289174 -4.4289193 -4.4289246][-4.4289231 -4.4288836 -4.4288383 -4.428792 -4.4287591 -4.4287705 -4.4288173 -4.4288654 -4.4289041 -4.4289284 -4.4289355 -4.4289308 -4.4289231 -4.4289212 -4.4289217][-4.4289174 -4.4288626 -4.4287915 -4.4287128 -4.4286537 -4.4286613 -4.4287238 -4.428792 -4.4288497 -4.428894 -4.4289188 -4.4289274 -4.4289279 -4.4289284 -4.4289284][-4.428915 -4.4288464 -4.4287453 -4.4286261 -4.4285293 -4.4285231 -4.4286003 -4.4286919 -4.428772 -4.4288397 -4.4288874 -4.428915 -4.4289293 -4.4289393 -4.4289412][-4.4289174 -4.4288406 -4.4287148 -4.4285645 -4.4284406 -4.4284091 -4.42848 -4.4285789 -4.4286714 -4.4287605 -4.4288373 -4.4288912 -4.4289269 -4.4289503 -4.4289556][-4.4289274 -4.4288516 -4.42872 -4.42856 -4.42842 -4.4283557 -4.4283915 -4.4284697 -4.428555 -4.428658 -4.4287643 -4.4288487 -4.4289117 -4.4289522 -4.42896][-4.4289417 -4.4288754 -4.4287558 -4.428607 -4.4284725 -4.4283872 -4.4283695 -4.4283876 -4.428432 -4.4285355 -4.4286709 -4.4287896 -4.4288826 -4.4289436 -4.4289565][-4.4289532 -4.4289021 -4.4288077 -4.4286842 -4.4285703 -4.4284883 -4.4284315 -4.4283757 -4.4283528 -4.4284286 -4.4285812 -4.4287291 -4.4288478 -4.428925 -4.4289494][-4.4289565 -4.4289246 -4.4288669 -4.4287796 -4.428688 -4.4286175 -4.4285464 -4.4284515 -4.4283681 -4.4283915 -4.4285297 -4.4286866 -4.4288177 -4.4289036 -4.4289365][-4.4289536 -4.4289389 -4.4289122 -4.4288626 -4.4288 -4.4287462 -4.4286838 -4.4285922 -4.4284911 -4.4284687 -4.4285541 -4.4286804 -4.4287972 -4.4288783 -4.4289136][-4.4289484 -4.4289436 -4.4289365 -4.4289174 -4.4288816 -4.428843 -4.4287939 -4.4287205 -4.4286289 -4.4285865 -4.4286208 -4.4287 -4.4287891 -4.4288597 -4.4288931][-4.4289408 -4.4289389 -4.4289403 -4.4289389 -4.4289246 -4.4289036 -4.4288688 -4.4288144 -4.4287391 -4.4286895 -4.4286928 -4.4287367 -4.4287972 -4.4288526 -4.4288831][-4.4289341 -4.4289303 -4.4289331 -4.4289403 -4.4289403 -4.4289341 -4.428916 -4.4288788 -4.4288235 -4.4287772 -4.4287648 -4.4287815 -4.4288139 -4.4288516 -4.4288764][-4.4289312 -4.428925 -4.4289255 -4.4289331 -4.42894 -4.4289427 -4.42894 -4.4289217 -4.428884 -4.4288459 -4.4288259 -4.4288216 -4.428833 -4.4288559 -4.4288745][-4.4289303 -4.4289231 -4.4289188 -4.4289217 -4.4289279 -4.428937 -4.4289494 -4.42895 -4.4289303 -4.4289021 -4.4288764 -4.428853 -4.4288449 -4.4288564 -4.42887]]...]
INFO - root - 2017-12-10 05:12:40.059063: step 710, loss = 2.28, batch loss = 2.23 (15.2 examples/sec; 0.528 sec/batch; 48h:39m:25s remains)
