INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "196"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip50-initconv1-4
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-10 07:28:08.155758: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:28:08.155921: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:28:08.155948: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:28:08.155970: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:28:08.155991: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:28:09.417214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 07:28:09.417250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 07:28:09.417257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 07:28:09.417265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>]
kkkkkkkkkkkkkkkkkkkkkkk [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0' INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 07:28:14.011905: step 0, loss = 0.75, batch loss = 0.69 (2.5 examples/sec; 3.235 sec/batch; 298h:45m:37s remains)
2017-12-10 07:28:14.452553: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012493899 0.00013519468 0.00014361837 0.00016238133 0.00018891321 0.00022214463 0.00026484174 0.00031590677 0.00036700204 0.00038851 0.00036977115 0.00032264812 0.000266109 0.00021263977 0.00016787782][0.00013302444 0.00014162165 0.00015744376 0.00019099015 0.00023993121 0.00029705424 0.000367391 0.00044758874 0.00052491779 0.00055332721 0.00051716715 0.00044015178 0.00034910455 0.00026868578 0.00020444325][0.00014971478 0.00015346386 0.00016905677 0.00021151172 0.00028329133 0.00036994036 0.00047709449 0.00060462474 0.00072874111 0.00077045668 0.00070925785 0.00058953924 0.000449704 0.00033216883 0.00024378505][0.00017460943 0.00017010204 0.00017641291 0.00022402257 0.00032018282 0.00044535805 0.00060200831 0.00079364429 0.000983686 0.0010351983 0.00092549314 0.00073933089 0.00053791824 0.00038018569 0.00026779066][0.00020346924 0.0001922384 0.00019342882 0.00025461655 0.0003780835 0.00054703874 0.00076410925 0.0010382725 0.0013050777 0.0013303341 0.0011322442 0.00086445367 0.00060533785 0.00041239758 0.00027954503][0.00023586584 0.00022661066 0.00023314342 0.00030707874 0.0004466862 0.00064764591 0.00091676292 0.0012598266 0.0015635836 0.0014957305 0.0011953256 0.00087898347 0.00060776586 0.00040958438 0.00027124054][0.00026668271 0.00027074388 0.00029199864 0.00037151721 0.00050720316 0.00070450758 0.00096810493 0.0012881033 0.0015143541 0.0013509159 0.0010395234 0.00075732713 0.0005343752 0.00036804582 0.00024631736][0.00029691716 0.00032125917 0.00035403235 0.00042578933 0.00053023576 0.00067664735 0.00086520205 0.0010702754 0.0011708969 0.0010006466 0.00076983427 0.00057662663 0.00042819278 0.00030848614 0.00021461194][0.00032127221 0.0003626454 0.0003935087 0.00044963995 0.00052153005 0.00060970214 0.00071126281 0.00080958934 0.000833587 0.00070586038 0.00056253694 0.0004488549 0.00035723569 0.00027079572 0.00019460026][0.00033687046 0.00038757734 0.00041382507 0.00045820881 0.00051079062 0.00055317383 0.00058763812 0.00061783753 0.00060795352 0.00052727485 0.00044555363 0.00038300507 0.00032090332 0.00025135028 0.00018236969][0.00034574993 0.00039284845 0.00041317489 0.00044635494 0.0004832159 0.00049623149 0.00049015164 0.00048292888 0.00046331662 0.00042162969 0.00037669978 0.00034081051 0.00028815671 0.00022373072 0.00016178074][0.00033844128 0.00036993457 0.00038174586 0.00040278476 0.0004202945 0.00041224575 0.00038651589 0.00037194791 0.00036450525 0.00035961476 0.00034281609 0.00031933561 0.00026797751 0.00020222816 0.00014615981][0.00030840808 0.00032153353 0.000322136 0.00033034844 0.00033358243 0.00032206925 0.00030341721 0.00030695976 0.00032657362 0.00035338395 0.00035595018 0.00033189115 0.000272128 0.00019918555 0.00014564797][0.00028354934 0.00028436878 0.00027647952 0.00027221258 0.00027066577 0.0002690719 0.00027453937 0.00030505628 0.00034987196 0.00039153732 0.00039643145 0.00036515476 0.0002912725 0.00020934152 0.00015607446][0.00028967607 0.00028253943 0.00026648527 0.00025299695 0.00024638182 0.00025626176 0.00028766919 0.00034525341 0.00040720435 0.00044855883 0.00044314604 0.00040078911 0.00031326219 0.00022526454 0.000168923]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip50-initconv1-4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip50-initconv1-4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset2/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/detection/biases/Momentum:0' shape=(1,) dtype=float32_ref>]
INFO - root - 2017-12-10 07:28:17.244688: step 10, loss = 0.75, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:47s remains)
INFO - root - 2017-12-10 07:28:19.311000: step 20, loss = 0.75, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:16m:38s remains)
INFO - root - 2017-12-10 07:28:21.389257: step 30, loss = 0.75, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:52m:01s remains)
INFO - root - 2017-12-10 07:28:23.467382: step 40, loss = 0.75, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:08m:44s remains)
INFO - root - 2017-12-10 07:28:25.576592: step 50, loss = 0.75, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:57m:43s remains)
INFO - root - 2017-12-10 07:28:27.651435: step 60, loss = 0.75, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:18m:11s remains)
INFO - root - 2017-12-10 07:28:29.718729: step 70, loss = 0.75, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:08s remains)
INFO - root - 2017-12-10 07:28:31.763277: step 80, loss = 0.75, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:59s remains)
INFO - root - 2017-12-10 07:28:33.918966: step 90, loss = 0.75, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:30m:06s remains)
INFO - root - 2017-12-10 07:28:35.975338: step 100, loss = 0.75, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:32m:45s remains)
2017-12-10 07:28:36.353717: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013501912 0.013697186 0.01332905 0.012886543 0.012458888 0.012134623 0.011921489 0.011850507 0.011815558 0.011771034 0.011702691 0.011593046 0.011501281 0.011456057 0.011452857][0.013651652 0.013914215 0.013638561 0.013273777 0.01291769 0.01262641 0.012391972 0.012283258 0.012189895 0.012092635 0.011959046 0.011801281 0.011685519 0.011619547 0.011615878][0.013644452 0.013928976 0.013703031 0.013379502 0.013056745 0.012797064 0.012567937 0.012411845 0.012271212 0.012108463 0.011907757 0.011704547 0.011543924 0.011456046 0.01144054][0.013604125 0.013896691 0.013704148 0.013401043 0.013093643 0.01282987 0.012585571 0.012402153 0.012222925 0.012005878 0.011742782 0.01149208 0.011281331 0.011142362 0.011084482][0.013504102 0.013791074 0.013633522 0.013361959 0.013086271 0.012827277 0.012578241 0.012376609 0.012172117 0.011906373 0.011583762 0.011260161 0.010966664 0.010741971 0.010603609][0.013420631 0.013671776 0.013528172 0.013295053 0.013060019 0.012817507 0.012589355 0.01238643 0.012161985 0.011856384 0.01147789 0.011063699 0.010661863 0.010324332 0.01008477][0.013410373 0.013595906 0.013439544 0.013199504 0.012963216 0.012759747 0.012569312 0.012385295 0.012162782 0.011845333 0.011433557 0.010933039 0.010422871 0.0099688834 0.009627969][0.01341281 0.013535194 0.013352029 0.013091827 0.0128392 0.012633728 0.012473661 0.01233978 0.012165122 0.011865132 0.011440529 0.010900668 0.010313264 0.0097672828 0.0093395095][0.013378803 0.013477529 0.013303157 0.013039608 0.01278157 0.012567722 0.012409458 0.01230179 0.012156758 0.011913274 0.01153707 0.011015891 0.010424453 0.0098404158 0.00936785][0.013248261 0.013341642 0.013186458 0.012935337 0.012691951 0.012502111 0.01237675 0.012302821 0.012207102 0.012038102 0.011733199 0.0112756 0.010732541 0.010179064 0.0097226612][0.013105724 0.013203308 0.013064692 0.012842896 0.012631364 0.0124743 0.012385591 0.012341928 0.012287623 0.012183831 0.011969427 0.011610229 0.011166464 0.010703556 0.010315137][0.012953236 0.013066343 0.012964569 0.012778199 0.012600115 0.012482695 0.012417445 0.012396707 0.012381181 0.012332747 0.012194743 0.011943996 0.011620857 0.011277771 0.010978182][0.012776496 0.012916882 0.012855634 0.012721633 0.012593758 0.012503427 0.012457225 0.012451542 0.012461901 0.012453544 0.01237799 0.012228315 0.012014224 0.011783166 0.011572112][0.012625764 0.012784753 0.012746451 0.012667947 0.012593911 0.01253348 0.012504198 0.012504104 0.012525355 0.012543956 0.012521907 0.012451324 0.012326583 0.012191351 0.012052434][0.012496905 0.012668121 0.012648862 0.012609128 0.012573365 0.012549796 0.01254415 0.012555167 0.012578724 0.012602832 0.01260545 0.012571418 0.012501084 0.012424224 0.012335807]]...]
INFO - root - 2017-12-10 07:28:38.429084: step 110, loss = 0.75, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:03s remains)
INFO - root - 2017-12-10 07:28:40.561674: step 120, loss = 0.75, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:44s remains)
INFO - root - 2017-12-10 07:28:42.687524: step 130, loss = 0.75, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:10s remains)
INFO - root - 2017-12-10 07:28:44.796684: step 140, loss = 0.75, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:53s remains)
INFO - root - 2017-12-10 07:28:46.920623: step 150, loss = 0.75, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:15m:09s remains)
INFO - root - 2017-12-10 07:28:49.025773: step 160, loss = 0.75, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:46m:14s remains)
INFO - root - 2017-12-10 07:28:51.115074: step 170, loss = 0.75, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:12m:04s remains)
INFO - root - 2017-12-10 07:28:53.227736: step 180, loss = 0.75, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:30m:09s remains)
INFO - root - 2017-12-10 07:28:55.345364: step 190, loss = 0.75, batch loss = 0.69 (38.0 examples/sec; 0.210 sec/batch; 19h:24m:35s remains)
INFO - root - 2017-12-10 07:28:57.427121: step 200, loss = 0.75, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:44s remains)
2017-12-10 07:28:57.809287: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.1491724e-07 -5.9093363e-06 -1.0838627e-05 -1.8082173e-05 -2.4088604e-05 -2.7901693e-05 -2.8823564e-05 -2.5133519e-05 -7.7361037e-06 3.2787641e-05 9.039999e-05 0.00014530269 0.00017033618 0.00015686073 0.00011821205][-1.5902006e-05 -1.3536486e-05 -1.104972e-05 -1.4929799e-05 -1.9418225e-05 -2.4909757e-05 -2.741336e-05 -2.0764128e-05 1.4941666e-05 9.5730371e-05 0.00020472227 0.00030018511 0.00033542956 0.00030327239 0.00022842413][-2.6193477e-05 -1.969896e-05 -1.3878634e-05 -1.3849556e-05 -1.7483868e-05 -2.3485503e-05 -2.4813944e-05 -1.1698172e-05 4.877046e-05 0.00017416615 0.00033737312 0.00047364429 0.00051267544 0.00045534672 0.00033844361][-3.4150264e-05 -2.713795e-05 -2.0155756e-05 -1.4812736e-05 -1.7035232e-05 -1.8806011e-05 -1.6186921e-05 8.6087312e-06 9.3470138e-05 0.00025170436 0.00044755149 0.00060300366 0.00063861534 0.00055699481 0.00040634361][-3.949392e-05 -3.3591448e-05 -2.6355156e-05 -1.9558818e-05 -1.804264e-05 -1.1749104e-05 4.07904e-06 5.3445619e-05 0.00016999024 0.00035863329 0.00057146739 0.00072329392 0.0007396026 0.00062705611 0.00044377981][-3.9440238e-05 -3.2305761e-05 -1.8787025e-05 2.3462489e-06 2.4069428e-05 5.3344382e-05 9.7772339e-05 0.00018218353 0.00033513759 0.0005495057 0.00076915434 0.00090458296 0.00088545645 0.0007273432 0.00049938133][-2.6360332e-05 -8.3445484e-06 2.7230133e-05 8.4852814e-05 0.00015427305 0.00023411481 0.00032750162 0.0004458616 0.00061089493 0.00081983494 0.0010209506 0.0011254522 0.0010661127 0.00085769035 0.00058037706][6.387294e-06 4.4774861e-05 0.00011156948 0.00021161042 0.00032846758 0.00045093847 0.00057096686 0.00069147506 0.00083462393 0.0010045344 0.0011577479 0.0012158714 0.0011210651 0.00088755589 0.00059481617][3.1748772e-05 8.33541e-05 0.00016442193 0.0002759046 0.00039717427 0.00051560858 0.00062646926 0.00073355035 0.00085081451 0.00098058488 0.0010868752 0.0011038671 0.00099121279 0.00076812168 0.00050483004][3.5204968e-05 8.6047017e-05 0.00015626785 0.00024786225 0.0003436798 0.00043320208 0.00051558675 0.00059883029 0.00069268426 0.00079308264 0.00086586672 0.00085957529 0.00075183407 0.00056781934 0.00036273384][1.9125902e-05 5.8187972e-05 0.00010751827 0.00016561462 0.00022528997 0.00028083782 0.00033420557 0.00039229874 0.00046237122 0.00053546857 0.00058379723 0.00056980673 0.00048333814 0.00035163411 0.00021439463][-2.9464063e-06 1.8337596e-05 4.267745e-05 7.0032009e-05 9.8512392e-05 0.00012465719 0.00015213533 0.0001879646 0.00023273211 0.00027901435 0.0003072808 0.00029258273 0.00023486106 0.00015562323 8.1082078e-05][-2.1613258e-05 -1.6299306e-05 -1.1321699e-05 -5.5857527e-06 6.456612e-07 6.7381552e-06 1.5721889e-05 3.1209274e-05 5.1630006e-05 7.5103235e-05 8.7227643e-05 7.7913472e-05 4.9136208e-05 1.4486148e-05 -1.2691031e-05][-2.6907986e-05 -2.8844635e-05 -3.2265067e-05 -3.5424328e-05 -3.8001366e-05 -3.99634e-05 -3.9871884e-05 -3.6285364e-05 -3.1031406e-05 -2.3644483e-05 -1.9703159e-05 -2.3753746e-05 -3.2618667e-05 -4.1623149e-05 -4.7123314e-05][-2.6806421e-05 -2.9595529e-05 -3.4120738e-05 -3.8409464e-05 -4.2098938e-05 -4.5037192e-05 -4.659476e-05 -4.6784226e-05 -4.6108122e-05 -4.38714e-05 -4.2377029e-05 -4.4157678e-05 -4.5521152e-05 -4.7175454e-05 -4.6583984e-05]]...]
INFO - root - 2017-12-10 07:28:59.929027: step 210, loss = 0.75, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:39s remains)
INFO - root - 2017-12-10 07:29:02.051646: step 220, loss = 0.75, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:29s remains)
INFO - root - 2017-12-10 07:29:04.174901: step 230, loss = 0.75, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:40s remains)
INFO - root - 2017-12-10 07:29:06.284141: step 240, loss = 0.75, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 19h:04m:23s remains)
INFO - root - 2017-12-10 07:29:08.444181: step 250, loss = 0.75, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:50s remains)
INFO - root - 2017-12-10 07:29:10.544022: step 260, loss = 0.75, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:51s remains)
INFO - root - 2017-12-10 07:29:12.653009: step 270, loss = 0.75, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:18s remains)
INFO - root - 2017-12-10 07:29:14.757460: step 280, loss = 0.75, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:23s remains)
INFO - root - 2017-12-10 07:29:16.879708: step 290, loss = 0.75, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:12s remains)
INFO - root - 2017-12-10 07:29:19.002657: step 300, loss = 0.75, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:17s remains)
2017-12-10 07:29:19.378389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00027133574 -0.00027066979 -0.0002703637 -0.00027018486 -0.00027035718 -0.00027087482 -0.00027133792 -0.00027145416 -0.000271093 -0.00027021306 -0.00026888284 -0.00026726522 -0.00026575336 -0.00026446179 -0.00026346915][-0.00027280304 -0.00027298351 -0.00027285988 -0.00027222862 -0.00027158842 -0.00027118871 -0.00027110236 -0.00027136394 -0.00027171118 -0.00027165259 -0.00027084944 -0.00026937237 -0.00026748082 -0.00026530743 -0.00026317747][-0.00027900634 -0.00028092664 -0.00028157234 -0.00028083808 -0.00027946639 -0.00027813512 -0.0002776123 -0.00027833669 -0.00027982466 -0.00028080508 -0.00028042318 -0.00027858984 -0.00027530902 -0.00027102936 -0.00026668108][-0.00028950241 -0.00029392066 -0.00029627327 -0.00029628072 -0.00029475414 -0.00029303343 -0.00029239454 -0.00029371376 -0.00029624885 -0.00029810923 -0.00029752747 -0.00029445611 -0.00028881719 -0.00028133774 -0.00027358945][-0.00030242154 -0.00031043091 -0.00031548171 -0.00031704662 -0.00031608739 -0.00031483988 -0.00031490403 -0.00031722442 -0.00032085041 -0.00032341026 -0.00032215152 -0.00031676848 -0.00030747312 -0.00029564512 -0.00028346572][-0.0003149132 -0.00032628205 -0.00033450377 -0.00033853351 -0.00033942686 -0.00033967089 -0.00034115851 -0.00034501738 -0.00034982161 -0.00035267827 -0.00035008235 -0.00034150985 -0.00032756929 -0.00031038976 -0.0002930878][-0.00032280182 -0.00033674986 -0.00034756161 -0.0003540793 -0.00035738162 -0.00035980565 -0.00036304176 -0.00036823907 -0.00037369321 -0.00037598988 -0.00037134622 -0.00035935998 -0.00034121377 -0.00031972866 -0.0002987402][-0.00032407403 -0.00033911905 -0.00035161508 -0.000360095 -0.00036547321 -0.00036986367 -0.00037437028 -0.000380089 -0.0003852559 -0.00038630262 -0.0003797352 -0.00036541608 -0.00034491366 -0.00032156211 -0.00029934396][-0.00031781086 -0.00033218178 -0.00034490653 -0.00035442677 -0.00036124539 -0.00036713062 -0.00037259638 -0.00037822814 -0.00038230888 -0.00038202389 -0.00037428402 -0.0003591576 -0.00033879111 -0.00031646894 -0.00029570103][-0.0003060083 -0.0003179537 -0.00032946689 -0.00033884076 -0.00034628692 -0.00035279518 -0.00035843928 -0.00036345585 -0.00036642619 -0.0003652492 -0.00035764495 -0.00034365177 -0.00032557349 -0.00030637355 -0.00028885735][-0.00029050329 -0.00029884209 -0.00030775912 -0.00031576637 -0.00032284044 -0.00032920987 -0.00033462144 -0.00033896233 -0.00034106534 -0.00033937898 -0.00033255064 -0.00032088975 -0.00030662114 -0.00029199163 -0.00027904238][-0.00027521304 -0.00027917995 -0.00028461881 -0.00029019135 -0.0002958134 -0.00030111344 -0.0003054869 -0.00030865922 -0.00030980707 -0.00030782752 -0.000302355 -0.00029413009 -0.00028480438 -0.00027576982 -0.00026815425][-0.00026469759 -0.00026532292 -0.0002678482 -0.00027078768 -0.0002740924 -0.00027740811 -0.0002802462 -0.00028224563 -0.00028286254 -0.00028140828 -0.00027798908 -0.00027325971 -0.00026818429 -0.00026360445 -0.00026007206][-0.00026013414 -0.00025904796 -0.00025978667 -0.00026098033 -0.00026252778 -0.00026424445 -0.00026581166 -0.00026687281 -0.00026708248 -0.00026629615 -0.00026454974 -0.00026224938 -0.00025998242 -0.00025816116 -0.00025702908][-0.00026038889 -0.00025923367 -0.00025957543 -0.00026019511 -0.00026104605 -0.00026205083 -0.00026306458 -0.00026385637 -0.00026427273 -0.00026400527 -0.00026312115 -0.00026186166 -0.00026053021 -0.00025932823 -0.00025848363]]...]
INFO - root - 2017-12-10 07:29:21.542265: step 310, loss = 0.75, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:36s remains)
INFO - root - 2017-12-10 07:29:23.690466: step 320, loss = 0.75, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:36s remains)
INFO - root - 2017-12-10 07:29:25.823556: step 330, loss = 0.75, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:49s remains)
INFO - root - 2017-12-10 07:29:27.980470: step 340, loss = 0.75, batch loss = 0.68 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:38s remains)
INFO - root - 2017-12-10 07:29:30.087345: step 350, loss = 0.75, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:09s remains)
INFO - root - 2017-12-10 07:29:32.215246: step 360, loss = 0.75, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:57s remains)
INFO - root - 2017-12-10 07:29:34.333520: step 370, loss = 0.75, batch loss = 0.68 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:05s remains)
INFO - root - 2017-12-10 07:29:36.483181: step 380, loss = 0.75, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:20s remains)
INFO - root - 2017-12-10 07:29:38.610822: step 390, loss = 0.76, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:59m:42s remains)
INFO - root - 2017-12-10 07:29:40.728337: step 400, loss = 0.75, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:11s remains)
2017-12-10 07:29:41.095162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013763438 -0.0013763752 -0.0013769841 -0.0013773676 -0.0013774708 -0.0013774897 -0.0013775559 -0.0013777808 -0.0013783503 -0.0013794263 -0.0013811167 -0.001383353 -0.0013859055 -0.0013883454 -0.0013901305][-0.0013734439 -0.0013742765 -0.0013758542 -0.0013772013 -0.0013779434 -0.0013782114 -0.0013780795 -0.0013777461 -0.0013775261 -0.0013778325 -0.0013790193 -0.0013811623 -0.0013841183 -0.0013874852 -0.0013905399][-0.0013713505 -0.0013733197 -0.0013766093 -0.0013798114 -0.0013820366 -0.0013831584 -0.0013831392 -0.0013822624 -0.0013809145 -0.0013797086 -0.0013795808 -0.0013809549 -0.0013838644 -0.0013878851 -0.0013919864][-0.0013673338 -0.0013708012 -0.001376397 -0.0013820123 -0.0013860656 -0.0013881514 -0.0013881974 -0.0013867166 -0.0013827769 -0.0013772498 -0.0013738511 -0.0013756276 -0.00138222 -0.0013890542 -0.0013940234][-0.0013613824 -0.0013666885 -0.001374724 -0.0013828108 -0.0013885449 -0.0013908574 -0.0013895632 -0.0013859331 -0.0013747521 -0.0013579885 -0.0013461616 -0.0013533125 -0.0013733415 -0.0013900955 -0.0013965771][-0.0013554983 -0.0013626146 -0.0013728385 -0.0013830734 -0.0013897292 -0.0013908034 -0.0013852073 -0.0013753573 -0.0013481543 -0.0013089164 -0.0012826485 -0.0013041303 -0.0013524522 -0.0013892805 -0.0013984021][-0.0013518657 -0.0013603517 -0.0013721558 -0.001384348 -0.0013916473 -0.0013902811 -0.0013776551 -0.0013566801 -0.0013055359 -0.0012360772 -0.0011938742 -0.0012366596 -0.001322866 -0.0013853976 -0.0013985474][-0.0013504727 -0.0013596922 -0.0013724861 -0.0013860804 -0.0013944221 -0.0013912513 -0.0013725579 -0.0013409317 -0.0012669015 -0.0011674446 -0.0011112668 -0.0011735498 -0.0012935747 -0.0013787164 -0.0013961934][-0.001349724 -0.0013589165 -0.0013722827 -0.0013871784 -0.0013975368 -0.0013961237 -0.001377534 -0.0013420078 -0.0012560175 -0.0011385452 -0.0010714297 -0.0011391839 -0.0012747304 -0.0013703582 -0.0013913863][-0.0013482217 -0.0013562889 -0.0013690587 -0.0013840647 -0.001396794 -0.0014006355 -0.0013897186 -0.0013611442 -0.001280129 -0.0011645813 -0.0010946497 -0.00115115 -0.0012751163 -0.0013637893 -0.0013850137][-0.0013459767 -0.0013517188 -0.0013624625 -0.0013764802 -0.0013900896 -0.0013987356 -0.0013978585 -0.0013830224 -0.0013232355 -0.0012314777 -0.0011725589 -0.0012084943 -0.0012987943 -0.0013629876 -0.0013790163][-0.0013434332 -0.0013463211 -0.0013536182 -0.0013646586 -0.0013772529 -0.0013878652 -0.0013935484 -0.0013907597 -0.0013580268 -0.0013024488 -0.0012653327 -0.0012814288 -0.0013315822 -0.0013655532 -0.0013741589][-0.0013409016 -0.0013414015 -0.0013450789 -0.0013516963 -0.0013607888 -0.0013700405 -0.0013774163 -0.0013804991 -0.0013685707 -0.0013448489 -0.0013292569 -0.001333604 -0.0013538892 -0.0013662378 -0.0013698871][-0.0013385963 -0.0013375428 -0.0013385479 -0.0013416089 -0.0013463949 -0.0013525309 -0.0013584627 -0.0013625176 -0.0013613403 -0.0013560666 -0.0013533676 -0.0013538577 -0.0013598998 -0.0013634538 -0.0013659634][-0.0013369025 -0.00133522 -0.0013345467 -0.0013349935 -0.0013367076 -0.0013395742 -0.0013430156 -0.0013463782 -0.001348023 -0.0013494377 -0.0013517946 -0.0013535215 -0.0013564085 -0.0013593353 -0.0013625943]]...]
INFO - root - 2017-12-10 07:29:43.225167: step 410, loss = 0.75, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:42s remains)
INFO - root - 2017-12-10 07:29:45.357790: step 420, loss = 0.75, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:00s remains)
INFO - root - 2017-12-10 07:29:47.474495: step 430, loss = 0.75, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:14m:04s remains)
INFO - root - 2017-12-10 07:29:49.586349: step 440, loss = 0.75, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:51s remains)
INFO - root - 2017-12-10 07:29:51.720003: step 450, loss = 0.75, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:50s remains)
INFO - root - 2017-12-10 07:29:53.860579: step 460, loss = 0.75, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:13s remains)
INFO - root - 2017-12-10 07:29:55.986467: step 470, loss = 0.75, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:25s remains)
INFO - root - 2017-12-10 07:29:58.110354: step 480, loss = 0.75, batch loss = 0.68 (38.3 examples/sec; 0.209 sec/batch; 19h:16m:04s remains)
INFO - root - 2017-12-10 07:30:00.273062: step 490, loss = 0.75, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:34s remains)
INFO - root - 2017-12-10 07:30:02.408105: step 500, loss = 0.75, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:02s remains)
2017-12-10 07:30:02.774092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027409806 -0.0027361205 -0.0027471564 -0.0027578252 -0.0027649559 -0.0027694083 -0.0027765434 -0.0027740218 -0.0027640304 -0.0027410539 -0.0026933879 -0.00252977 -0.0021446901 -0.0015016259 -0.00074654608][-0.00275695 -0.0027517269 -0.0027586871 -0.0027610655 -0.0027643496 -0.0027690548 -0.0027812587 -0.002787583 -0.0027808279 -0.0027493278 -0.0026527187 -0.0024142943 -0.0019602019 -0.0013023913 -0.00059471815][-0.0027750218 -0.0027679228 -0.0027692812 -0.0027659284 -0.0027630737 -0.0027656667 -0.0027718046 -0.002762011 -0.0027284191 -0.0026588554 -0.0025056626 -0.0022081365 -0.0017446369 -0.0011580969 -0.00055425405][-0.0027938909 -0.0027836657 -0.0027729769 -0.0027559225 -0.0027372565 -0.0027286289 -0.0027139257 -0.0026718953 -0.0025752236 -0.0024153532 -0.0021757623 -0.0018525722 -0.0014648673 -0.0010324846 -0.0005804291][-0.0027560426 -0.0027241167 -0.0026880845 -0.0026442797 -0.0025983525 -0.00255402 -0.0024992097 -0.0024086155 -0.0022473908 -0.0020021999 -0.0017047273 -0.0014125238 -0.0011706246 -0.00096306053 -0.00071700173][-0.0026112241 -0.0025324773 -0.0024430824 -0.0023369181 -0.0022359169 -0.0021386996 -0.0020277402 -0.0018743242 -0.0016693383 -0.0014238411 -0.0011854249 -0.0010327338 -0.0010089312 -0.0010531297 -0.0010172087][-0.0023414288 -0.0021880539 -0.0020437497 -0.0018843357 -0.0017283028 -0.001585218 -0.0014289051 -0.0012344592 -0.0010217144 -0.00083759381 -0.0007496269 -0.00081206835 -0.0010339034 -0.0013171955 -0.0014680827][-0.0020942476 -0.0018892164 -0.001730399 -0.0015660219 -0.001396907 -0.0012251418 -0.0010272887 -0.00078633125 -0.00055299606 -0.00041739154 -0.00047025364 -0.00075090118 -0.0012081984 -0.0016790045 -0.0019535273][-0.0020764363 -0.001883148 -0.0017515101 -0.0016264673 -0.0014745594 -0.0012734042 -0.00099736219 -0.00064647989 -0.00032013585 -0.00016519474 -0.00030676858 -0.00075717573 -0.0013849927 -0.0019589751 -0.0022874339][-0.0022912989 -0.0021815519 -0.0021150012 -0.0020345317 -0.0018914873 -0.0016484242 -0.0012655284 -0.00076650572 -0.00030449941 -8.5217645e-05 -0.0002660302 -0.00082183233 -0.0015412756 -0.0021318875 -0.0024423327][-0.00254681 -0.0025050053 -0.0024865454 -0.0024511809 -0.0023404707 -0.0020929929 -0.0016510475 -0.001050615 -0.00049010408 -0.00022094278 -0.00041882996 -0.0010202224 -0.0017581295 -0.0023113315 -0.002573512][-0.0026911106 -0.0026788637 -0.0026783335 -0.002674168 -0.0026125724 -0.0024183039 -0.0020083096 -0.0014110469 -0.00084548071 -0.0005830673 -0.00079343771 -0.001374051 -0.0020390253 -0.0024996058 -0.0027042564][-0.0027226438 -0.0027119073 -0.0027071885 -0.0027115913 -0.0026933013 -0.0025839035 -0.0023015088 -0.0018412801 -0.0013838139 -0.0011788624 -0.0013643842 -0.0018204185 -0.0022988855 -0.0026011635 -0.0027259919][-0.0027214724 -0.0027105161 -0.0027029822 -0.0026998948 -0.0026914445 -0.0026465284 -0.0025095055 -0.0022573008 -0.001984403 -0.001867973 -0.001996648 -0.002269119 -0.0025249375 -0.0026759978 -0.0027370665][-0.0027219185 -0.0027154845 -0.002716766 -0.0027101506 -0.0026938263 -0.0026693009 -0.0026210602 -0.0025339313 -0.002433093 -0.00239869 -0.0024629897 -0.002576675 -0.0026706688 -0.0027194358 -0.0027388874]]...]
INFO - root - 2017-12-10 07:30:04.911624: step 510, loss = 0.75, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:53s remains)
INFO - root - 2017-12-10 07:30:07.088430: step 520, loss = 0.76, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:27s remains)
INFO - root - 2017-12-10 07:30:09.225522: step 530, loss = 0.75, batch loss = 0.69 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:47s remains)
INFO - root - 2017-12-10 07:30:11.377899: step 540, loss = 0.76, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:12s remains)
INFO - root - 2017-12-10 07:30:13.497069: step 550, loss = 0.75, batch loss = 0.69 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:09s remains)
INFO - root - 2017-12-10 07:30:15.641359: step 560, loss = 0.76, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:46m:42s remains)
INFO - root - 2017-12-10 07:30:17.765378: step 570, loss = 0.76, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:14s remains)
INFO - root - 2017-12-10 07:30:19.893394: step 580, loss = 0.75, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:17s remains)
INFO - root - 2017-12-10 07:30:22.071429: step 590, loss = 0.75, batch loss = 0.69 (38.0 examples/sec; 0.210 sec/batch; 19h:23m:41s remains)
INFO - root - 2017-12-10 07:30:24.200623: step 600, loss = 0.76, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:45s remains)
2017-12-10 07:30:24.500384: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11354211 0.1486329 0.19183904 0.24021134 0.29181853 0.34407723 0.39248109 0.43405724 0.46541968 0.4859727 0.49463663 0.49100617 0.47593683 0.45127669 0.41900882][0.13562462 0.17798325 0.2283877 0.28322566 0.34002468 0.39515573 0.44446582 0.48498595 0.51353449 0.52987808 0.53278422 0.52192605 0.498849 0.46615791 0.42630604][0.16714357 0.21469426 0.27044314 0.32985386 0.389643 0.44486171 0.49241841 0.52945459 0.55312145 0.56359923 0.55969656 0.54116821 0.510052 0.46958852 0.42271864][0.20982268 0.26069108 0.31809875 0.37776113 0.43659005 0.49059334 0.53568357 0.56792068 0.58598894 0.590344 0.5798583 0.5542841 0.51595634 0.468659 0.41586393][0.26109377 0.3124108 0.36696398 0.42274177 0.47665673 0.52521116 0.56454939 0.59146208 0.60432082 0.60297954 0.58707064 0.55615628 0.51265085 0.46071348 0.40420672][0.3156938 0.36556914 0.41424379 0.46228206 0.50807989 0.54858369 0.58030522 0.60024613 0.60725456 0.60146356 0.58236617 0.5499202 0.50575686 0.45385024 0.3979297][0.37281206 0.419291 0.45927209 0.49677303 0.53177786 0.56229377 0.58494586 0.59706956 0.59830403 0.58919823 0.5691644 0.53768134 0.49604788 0.44760254 0.39522091][0.43429381 0.47828874 0.50981742 0.53631282 0.559943 0.57947516 0.59225196 0.59624481 0.59199274 0.58038568 0.56069577 0.53201967 0.49472731 0.45108792 0.40298524][0.49217084 0.53418535 0.55769032 0.57462043 0.58830166 0.59799486 0.60193378 0.59890449 0.59056062 0.57805687 0.56034964 0.5361734 0.5049389 0.46772885 0.42511731][0.53860587 0.57927936 0.59662819 0.60559148 0.61081022 0.61206013 0.60862786 0.60129583 0.59133792 0.57986993 0.56551659 0.54691106 0.52273941 0.4929179 0.4566744][0.57112914 0.61207581 0.62735152 0.63293958 0.63431597 0.63130361 0.6240502 0.61404645 0.60381818 0.59498709 0.58471084 0.57175273 0.5542798 0.5311991 0.50062376][0.59613556 0.63957256 0.65594447 0.66123873 0.66055375 0.65503091 0.64589405 0.63568521 0.62641412 0.61927152 0.61128223 0.59983075 0.58442068 0.563443 0.53472888][0.61608922 0.6645323 0.6846258 0.69210953 0.69248128 0.68639255 0.6772005 0.66788906 0.65966374 0.6534602 0.6459567 0.63586622 0.62175858 0.60177994 0.5745014][0.62368989 0.67390192 0.69553792 0.70511091 0.70707476 0.70308483 0.69614249 0.68884903 0.6821869 0.67746389 0.67130572 0.662961 0.65154725 0.63546371 0.61312866][0.63123482 0.68280923 0.70664871 0.71800649 0.72175735 0.72013384 0.71553665 0.71035379 0.7052232 0.70181352 0.69766438 0.69203126 0.68448329 0.67334366 0.656765]]...]
INFO - root - 2017-12-10 07:30:26.631680: step 610, loss = 0.75, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:03s remains)
INFO - root - 2017-12-10 07:30:28.730187: step 620, loss = 0.76, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:17m:45s remains)
INFO - root - 2017-12-10 07:30:30.860410: step 630, loss = 0.76, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:23s remains)
INFO - root - 2017-12-10 07:30:33.014197: step 640, loss = 0.76, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:39m:43s remains)
INFO - root - 2017-12-10 07:30:35.137890: step 650, loss = 0.75, batch loss = 0.68 (35.5 examples/sec; 0.226 sec/batch; 20h:47m:52s remains)
INFO - root - 2017-12-10 07:30:37.270610: step 660, loss = 0.75, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:07m:16s remains)
INFO - root - 2017-12-10 07:30:39.430803: step 670, loss = 0.75, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:51s remains)
INFO - root - 2017-12-10 07:30:41.574722: step 680, loss = 0.76, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:26s remains)
INFO - root - 2017-12-10 07:30:43.766374: step 690, loss = 0.75, batch loss = 0.69 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:22s remains)
INFO - root - 2017-12-10 07:30:45.911722: step 700, loss = 0.75, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:46s remains)
2017-12-10 07:30:46.256989: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014966618 0.021082858 0.0265991 0.030558718 0.032406114 0.032027245 0.029917272 0.026662095 0.022921352 0.019009344 0.015162546 0.011358546 0.0075825164 0.0039856229 0.00081197638][0.014033347 0.020214854 0.025991784 0.030397104 0.032744125 0.032788895 0.030840693 0.027425004 0.023196995 0.018720971 0.014406599 0.010394838 0.0066049164 0.0031450186 0.00014786702][0.013857063 0.020086132 0.025896722 0.03049078 0.033203572 0.0337072 0.032061152 0.028680051 0.024171194 0.019254023 0.014474204 0.010110617 0.0061600059 0.0026842505 -0.00022427132][0.014419666 0.021143364 0.027319057 0.032184504 0.035067387 0.03568168 0.034071214 0.030637035 0.025948288 0.020686343 0.015497915 0.010769995 0.00660058 0.0029923264 -1.6022939e-05][0.015106337 0.022463515 0.029191403 0.03447672 0.0376326 0.03834343 0.036687367 0.033055164 0.02808865 0.022477761 0.016951075 0.011968471 0.0076457215 0.0039268304 0.00075287372][0.015024405 0.022940323 0.030258616 0.036041163 0.0395255 0.040398993 0.038785823 0.035135813 0.030157866 0.024511516 0.018939419 0.013872169 0.0094247917 0.0055179941 0.0020562452][0.014020899 0.022208164 0.029884165 0.036043264 0.039885484 0.041065834 0.039678544 0.036222257 0.031446971 0.026029384 0.020676414 0.01575698 0.011343298 0.0073246043 0.0035757255][0.011883829 0.019813862 0.02739978 0.033633634 0.037719652 0.039341368 0.038481645 0.035554685 0.031253241 0.026294736 0.021400828 0.016889086 0.012761954 0.0088318987 0.0049797436][0.0089323316 0.016078221 0.023086425 0.02897446 0.033020951 0.034971334 0.034717761 0.032545969 0.029001584 0.024800345 0.020642841 0.016790144 0.013176588 0.0095322859 0.0057674581][0.0056316489 0.011630798 0.017637281 0.022733627 0.026362108 0.028320098 0.028505383 0.027141439 0.024574393 0.021448886 0.018329378 0.015353251 0.012367643 0.0091156522 0.0055979658][0.0023593176 0.0069212522 0.011595964 0.015554471 0.018377384 0.019945597 0.020289311 0.019564338 0.017955428 0.016000602 0.014088267 0.012266783 0.010262318 0.0077553811 0.0047396244][-0.00055225519 0.0025145477 0.0057111494 0.0084024873 0.0103034 0.011330949 0.011609813 0.011286922 0.010526374 0.0096695125 0.0088617764 0.0080695674 0.0069670836 0.0052705193 0.0029966906][-0.0027974618 -0.0010082768 0.0008731205 0.0024204506 0.0034524268 0.003929669 0.0040333327 0.0039205765 0.0037445333 0.0036772704 0.0037056943 0.0037200488 0.0033799764 0.0024354588 0.00091046][-0.0042706057 -0.0033817871 -0.0024351811 -0.0016541649 -0.001178395 -0.0010319306 -0.0010466962 -0.0010932148 -0.0010303459 -0.00079158926 -0.00043999543 -9.5768832e-05 -1.7489307e-05 -0.00038404437 -0.0012106891][-0.0050108088 -0.0046310276 -0.00422404 -0.0038747336 -0.0037041148 -0.0037112487 -0.0037812237 -0.0038386225 -0.0037808763 -0.0035928255 -0.0033050992 -0.0029708019 -0.0027267989 -0.0026966992 -0.0029580614]]...]
INFO - root - 2017-12-10 07:30:48.426071: step 710, loss = 0.77, batch loss = 0.70 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:07s remains)
INFO - root - 2017-12-10 07:30:50.571338: step 720, loss = 0.76, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:31s remains)
INFO - root - 2017-12-10 07:30:52.719698: step 730, loss = 0.76, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:46s remains)
INFO - root - 2017-12-10 07:30:54.861663: step 740, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:00s remains)
INFO - root - 2017-12-10 07:30:57.005060: step 750, loss = 0.76, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:50s remains)
INFO - root - 2017-12-10 07:30:59.125848: step 760, loss = 0.74, batch loss = 0.67 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:44s remains)
INFO - root - 2017-12-10 07:31:01.270422: step 770, loss = 0.75, batch loss = 0.68 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:21s remains)
INFO - root - 2017-12-10 07:31:03.393820: step 780, loss = 0.75, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:45s remains)
INFO - root - 2017-12-10 07:31:05.515166: step 790, loss = 0.76, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:44s remains)
INFO - root - 2017-12-10 07:31:07.680704: step 800, loss = 0.78, batch loss = 0.71 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:30s remains)
2017-12-10 07:31:08.093012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0075510289 -0.0075512426 -0.0075520603 -0.0075537763 -0.0075572841 -0.0075626452 -0.0075697014 -0.0075776693 -0.0075859344 -0.0075940364 -0.0076012397 -0.0076073953 -0.0076128026 -0.0076175085 -0.0076211812][-0.0075510992 -0.0075511248 -0.0075518973 -0.0075536733 -0.0075569819 -0.0075619365 -0.0075681256 -0.0075750072 -0.0075822561 -0.0075896638 -0.0075969957 -0.007604287 -0.0076116351 -0.007618791 -0.0076248175][-0.0075517236 -0.0075517274 -0.0075525125 -0.0075542554 -0.0075573055 -0.0075616296 -0.0075667803 -0.0075723724 -0.0075783744 -0.0075849355 -0.0075921495 -0.0076001934 -0.0076090521 -0.0076182839 -0.0076264213][-0.007552315 -0.0075522517 -0.0075530433 -0.0075547597 -0.0075575933 -0.0075613782 -0.00756564 -0.0075700912 -0.0075748707 -0.0075803748 -0.0075869574 -0.0075949538 -0.0076043457 -0.0076145744 -0.0076239435][-0.0075528892 -0.0075527276 -0.0075535174 -0.0075552282 -0.0075579216 -0.0075613209 -0.0075649219 -0.0075684669 -0.0075721126 -0.0075763343 -0.00758168 -0.0075886766 -0.0075973924 -0.0076073078 -0.0076167583][-0.0075534531 -0.0075531588 -0.0075539094 -0.0075555756 -0.0075581311 -0.0075612324 -0.0075643463 -0.007567198 -0.0075698541 -0.0075727138 -0.0075764097 -0.0075816275 -0.0075885789 -0.0075969337 -0.0076053282][-0.0075541846 -0.0075535541 -0.0075542172 -0.0075557427 -0.0075580613 -0.0075608236 -0.0075635095 -0.0075658178 -0.0075676697 -0.0075692986 -0.0075713047 -0.0075744288 -0.0075790193 -0.007585024 -0.0075914832][-0.0075548077 -0.0075539229 -0.0075544422 -0.00755572 -0.0075576743 -0.0075600003 -0.0075622424 -0.0075640888 -0.0075653135 -0.0075659892 -0.0075666234 -0.0075679105 -0.0075703063 -0.0075739557 -0.0075783404][-0.0075553008 -0.0075542838 -0.007554627 -0.0075555905 -0.0075571048 -0.0075589367 -0.0075607286 -0.0075621828 -0.0075629749 -0.0075630429 -0.0075627929 -0.0075628478 -0.0075637144 -0.0075655966 -0.0075683291][-0.0075556678 -0.007554546 -0.0075547304 -0.0075553982 -0.0075564892 -0.0075578457 -0.0075592068 -0.0075603235 -0.00756085 -0.0075606592 -0.0075600618 -0.0075595933 -0.007559726 -0.0075607309 -0.0075626583][-0.0075555448 -0.0075545278 -0.0075545958 -0.0075550261 -0.0075557688 -0.0075567206 -0.0075577041 -0.0075585349 -0.0075589172 -0.0075587258 -0.007558187 -0.0075577395 -0.0075578145 -0.0075587048 -0.0075605568][-0.0075551383 -0.007554085 -0.0075540943 -0.0075543677 -0.0075548631 -0.0075555095 -0.0075561968 -0.0075567975 -0.0075570922 -0.0075569949 -0.0075566755 -0.0075564678 -0.0075567588 -0.0075578508 -0.0075599784][-0.0075543835 -0.0075532575 -0.0075532412 -0.0075534168 -0.0075537493 -0.0075541823 -0.0075546447 -0.0075550536 -0.0075552738 -0.0075552533 -0.0075551192 -0.0075551234 -0.0075556268 -0.0075569307 -0.0075592841][-0.0075533316 -0.0075521613 -0.0075521339 -0.0075522372 -0.0075524459 -0.0075527113 -0.0075529823 -0.0075532128 -0.0075533418 -0.0075533469 -0.0075533111 -0.0075534894 -0.0075541297 -0.0075555211 -0.0075579132][-0.0075524803 -0.00755145 -0.0075514284 -0.0075514778 -0.0075515769 -0.0075516859 -0.0075517022 -0.0075516789 -0.0075516123 -0.0075515085 -0.0075514219 -0.0075516747 -0.0075523267 -0.0075536347 -0.0075557902]]...]
INFO - root - 2017-12-10 07:31:10.225091: step 810, loss = 0.75, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:47s remains)
INFO - root - 2017-12-10 07:31:12.341447: step 820, loss = 0.77, batch loss = 0.70 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:09s remains)
INFO - root - 2017-12-10 07:31:14.457958: step 830, loss = 0.76, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:36s remains)
INFO - root - 2017-12-10 07:31:16.609444: step 840, loss = 0.75, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:23s remains)
INFO - root - 2017-12-10 07:31:18.715044: step 850, loss = 0.75, batch loss = 0.68 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:53s remains)
INFO - root - 2017-12-10 07:31:20.839409: step 860, loss = 0.78, batch loss = 0.71 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:25s remains)
INFO - root - 2017-12-10 07:31:22.961153: step 870, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:20s remains)
INFO - root - 2017-12-10 07:31:25.098247: step 880, loss = 0.76, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:46s remains)
INFO - root - 2017-12-10 07:31:27.290478: step 890, loss = 0.76, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:08s remains)
INFO - root - 2017-12-10 07:31:29.404320: step 900, loss = 0.76, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:29m:09s remains)
2017-12-10 07:31:29.755083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0529707 -0.053860731 -0.053646356 -0.052405491 -0.051193275 -0.0506711 -0.050635137 -0.051372685 -0.052022435 -0.052847043 -0.053694226 -0.05451601 -0.055636868 -0.056673378 -0.0579146][-0.061333857 -0.062432028 -0.062199607 -0.059641287 -0.057034269 -0.055598512 -0.055164762 -0.055971473 -0.056912325 -0.05893001 -0.060722619 -0.062220655 -0.063980818 -0.065409146 -0.066970117][-0.061662994 -0.063255772 -0.063477129 -0.059963211 -0.056208663 -0.053256303 -0.051398709 -0.051438294 -0.051840186 -0.054803118 -0.05751884 -0.060302854 -0.063090205 -0.065003358 -0.066849954][-0.059708878 -0.06157887 -0.062077522 -0.058147386 -0.054077215 -0.050165765 -0.04741101 -0.046093412 -0.045610264 -0.049196877 -0.052633941 -0.056758679 -0.060749181 -0.064072259 -0.066968612][-0.056970619 -0.05873017 -0.058979012 -0.054769702 -0.050545052 -0.046132378 -0.042839997 -0.041148737 -0.040490709 -0.043616019 -0.046848528 -0.051451042 -0.055995084 -0.060936354 -0.065257721][-0.051252373 -0.052769907 -0.052805364 -0.049872912 -0.047210827 -0.042779997 -0.039517336 -0.037971534 -0.037479211 -0.039607495 -0.041845962 -0.046815179 -0.051595859 -0.057706065 -0.063140012][-0.047777317 -0.050452605 -0.051468082 -0.049821965 -0.048535056 -0.045706123 -0.042754285 -0.040003158 -0.038363159 -0.038838319 -0.039064456 -0.042364024 -0.045714617 -0.052223183 -0.058282949][-0.040029913 -0.043099716 -0.044845328 -0.045762755 -0.047153316 -0.04711277 -0.045918345 -0.044896029 -0.044198602 -0.042180605 -0.03966048 -0.040480841 -0.0420281 -0.046962433 -0.051969983][-0.032134384 -0.035217978 -0.037421182 -0.03926535 -0.041819528 -0.042608306 -0.043763377 -0.044667967 -0.045469783 -0.045032009 -0.043516859 -0.043272831 -0.042576835 -0.045718305 -0.049126908][-0.02291704 -0.02565009 -0.028092958 -0.031146389 -0.034506962 -0.037447918 -0.040418565 -0.04295671 -0.045418136 -0.04586412 -0.045264214 -0.044977427 -0.044133335 -0.045590661 -0.047202669][-0.015670517 -0.017865203 -0.020089904 -0.023352005 -0.027036466 -0.030776646 -0.034278095 -0.036455289 -0.038546741 -0.040199302 -0.040667757 -0.041298166 -0.041528337 -0.042791121 -0.044295117][-0.012921528 -0.01451036 -0.016156353 -0.018625516 -0.021648832 -0.024804585 -0.027473828 -0.029650953 -0.031736724 -0.032625362 -0.032103427 -0.0320889 -0.031894367 -0.032450214 -0.03348501][-0.013876898 -0.014810031 -0.015679451 -0.017678797 -0.020055082 -0.02228179 -0.023971669 -0.025136158 -0.026135273 -0.025993066 -0.025189077 -0.024263449 -0.023524754 -0.023366202 -0.023841759][-0.021408182 -0.020752132 -0.019710261 -0.020145334 -0.020845328 -0.021224385 -0.021070145 -0.021513734 -0.021773569 -0.020814788 -0.019067068 -0.017391946 -0.016008027 -0.014943357 -0.014583659][-0.025489369 -0.025949251 -0.025329979 -0.024911778 -0.024534764 -0.024263408 -0.023299521 -0.022338539 -0.021382375 -0.020136945 -0.018282633 -0.01623847 -0.014594585 -0.012243072 -0.010436794]]...]
INFO - root - 2017-12-10 07:31:31.862703: step 910, loss = 0.76, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:00s remains)
INFO - root - 2017-12-10 07:31:33.995529: step 920, loss = 0.76, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 20h:02m:45s remains)
INFO - root - 2017-12-10 07:31:36.136130: step 930, loss = 0.78, batch loss = 0.71 (34.9 examples/sec; 0.229 sec/batch; 21h:06m:45s remains)
INFO - root - 2017-12-10 07:31:38.303530: step 940, loss = 0.76, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:40s remains)
INFO - root - 2017-12-10 07:31:40.431655: step 950, loss = 0.76, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:26s remains)
INFO - root - 2017-12-10 07:31:42.585223: step 960, loss = 0.76, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:49s remains)
INFO - root - 2017-12-10 07:31:44.730258: step 970, loss = 0.76, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:22s remains)
INFO - root - 2017-12-10 07:31:46.844670: step 980, loss = 0.76, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:27s remains)
INFO - root - 2017-12-10 07:31:48.977545: step 990, loss = 0.76, batch loss = 0.70 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:35s remains)
INFO - root - 2017-12-10 07:31:51.083956: step 1000, loss = 0.76, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:56m:48s remains)
2017-12-10 07:31:51.416717: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033407398 0.03107607 0.027863935 0.024305727 0.02043194 0.016551573 0.013210303 0.010351206 0.0076838518 0.004886643 0.001792958 -0.0015658392 -0.0047293291 -0.007409805 -0.0092211068][0.054843079 0.051997062 0.048268739 0.044211328 0.039790969 0.035253871 0.031032734 0.027217776 0.023466591 0.019164298 0.01412738 0.0083344905 0.0023783073 -0.0029562335 -0.0068336464][0.076018706 0.073231705 0.06926325 0.065295577 0.061228808 0.056835603 0.05224384 0.047750715 0.042885937 0.037001353 0.029800408 0.02137484 0.012396845 0.0037687132 -0.0029691253][0.093367092 0.091453135 0.087869361 0.08435192 0.081016526 0.077524312 0.07371223 0.069083981 0.063265949 0.05576678 0.046456207 0.035234049 0.023220081 0.011586196 0.0019898973][0.10261615 0.10207612 0.099274866 0.0966812 0.094392009 0.091924161 0.089218363 0.085343041 0.07987044 0.071864374 0.061177541 0.047821518 0.033194911 0.018657945 0.0064219153][0.10519976 0.10597914 0.10372867 0.10201414 0.10115739 0.1001695 0.098813392 0.09587422 0.091109 0.083391353 0.072493583 0.058049951 0.041595798 0.024837248 0.010422382][0.1041794 0.10654224 0.10461326 0.10335104 0.10309433 0.10321603 0.10333505 0.10165562 0.097792812 0.090500519 0.079765707 0.065069713 0.047836736 0.029786363 0.013858586][0.10073736 0.10504486 0.10372403 0.1028133 0.10266981 0.10297289 0.10356382 0.10270792 0.099831916 0.093376048 0.083138406 0.06857992 0.051080391 0.032622423 0.016155623][0.096967176 0.10315574 0.10244923 0.10186642 0.10183278 0.1019884 0.10234389 0.10148452 0.098991953 0.093241 0.083795637 0.0697348 0.052291621 0.03367953 0.016888686][0.094864167 0.10147913 0.10062316 0.10004054 0.10002387 0.10013783 0.10039494 0.099562511 0.097145624 0.091825657 0.082914911 0.069355071 0.052180015 0.033531573 0.01650409][0.094135553 0.10169091 0.10107705 0.10071002 0.10083329 0.10126152 0.10150938 0.1003334 0.09771207 0.092479132 0.083577827 0.070085324 0.052992079 0.034236006 0.017021447][0.092841357 0.10046689 0.099729694 0.099332958 0.099282712 0.099375315 0.099358894 0.098335609 0.095775552 0.090405911 0.081565686 0.068375058 0.051483233 0.033015516 0.016157005][0.088615149 0.096038967 0.095119596 0.094408944 0.093941711 0.093581736 0.093190253 0.091765843 0.089047559 0.0840779 0.075619869 0.062985249 0.047029559 0.029695317 0.013793589][0.079454772 0.08607924 0.084905371 0.083802693 0.082900129 0.082007624 0.081188232 0.079681069 0.077037029 0.072112709 0.064162649 0.052711215 0.0385086 0.023379087 0.00973489][0.065020032 0.070606612 0.069408916 0.068213493 0.067307472 0.066346049 0.065546453 0.064032219 0.061577473 0.057286624 0.050409123 0.0406219 0.02866092 0.016158309 0.00508798]]...]
INFO - root - 2017-12-10 07:31:53.584263: step 1010, loss = 0.76, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:29s remains)
