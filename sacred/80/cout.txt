INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "80"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 02:18:41.674211: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:18:41.674243: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:18:41.674249: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:18:41.674253: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:18:41.674257: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:18:42.185913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.75GiB
2017-12-06 02:18:42.185950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 02:18:42.185957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 02:18:42.185970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 02:18:47.061372: step 0, loss = 0.89, batch loss = 0.68 (1.9 examples/sec; 4.105 sec/batch; 379h:11m:02s remains)
2017-12-06 02:18:47.668575: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074990354 0.074300192 0.0766978 0.078030862 0.07950972 0.080907345 0.0837942 0.085241407 0.084819034 0.08309795 0.08095324 0.078911856 0.078967571 0.079261839 0.078196652][0.076361068 0.077838629 0.080579609 0.08100868 0.0813426 0.085200951 0.095294759 0.10218699 0.10208789 0.097492374 0.094234891 0.088571675 0.085373096 0.082959622 0.080648392][0.076500557 0.07974793 0.082166173 0.084215343 0.084599 0.092048958 0.10902665 0.12831663 0.1317855 0.12425639 0.11999189 0.11006074 0.098587 0.089459784 0.084095471][0.077712119 0.08136262 0.085293889 0.09167067 0.095086388 0.11326849 0.14719807 0.1747698 0.17809632 0.16743238 0.15724494 0.13933207 0.11565559 0.099000514 0.087701358][0.078377537 0.081225313 0.083962008 0.09563721 0.12132172 0.16008961 0.21412297 0.24732512 0.24121168 0.22046696 0.20023899 0.16117917 0.13190006 0.10988609 0.091654032][0.082336389 0.08577738 0.090647556 0.11559093 0.15964086 0.22788332 0.29780897 0.34675154 0.33674279 0.2815184 0.23041505 0.17779945 0.14151065 0.11544497 0.0953991][0.085041039 0.0894569 0.10487009 0.13942948 0.20054494 0.29207879 0.39700669 0.48540705 0.44729793 0.33783731 0.25324053 0.18085539 0.13410358 0.11123423 0.094704032][0.088459142 0.0953676 0.11571258 0.15515077 0.22202933 0.3082543 0.41323617 0.50456184 0.42733261 0.31156546 0.23432997 0.16317806 0.11857616 0.10409329 0.091693148][0.0932628 0.10455856 0.12676504 0.15564021 0.2076643 0.25885189 0.31364506 0.33499673 0.28512329 0.22319981 0.17414482 0.12951119 0.1045538 0.09702222 0.089762859][0.093682989 0.10604238 0.12725644 0.15021731 0.17980234 0.20613453 0.21847364 0.20692173 0.17611793 0.15333387 0.12760673 0.10567214 0.093829229 0.093022116 0.08835987][0.09196762 0.10455076 0.12402909 0.14341904 0.15694493 0.16244276 0.15396512 0.14408314 0.12715244 0.11566808 0.10656534 0.097076632 0.09127821 0.091507323 0.087049216][0.093505263 0.10482085 0.1198466 0.12972131 0.1334814 0.13324033 0.12638931 0.12211649 0.10962235 0.099377386 0.095488869 0.090267017 0.088414662 0.090098634 0.08565598][0.094311811 0.099511854 0.10601397 0.10760778 0.10867345 0.1104274 0.11103763 0.11179537 0.10334958 0.093020268 0.088741392 0.084090643 0.084353089 0.087105364 0.08459644][0.095751211 0.097840682 0.097760744 0.099480107 0.10195276 0.10467719 0.10711385 0.11122902 0.10304467 0.094272606 0.089882806 0.085946575 0.085807554 0.086744055 0.083799891][0.099100195 0.09933079 0.099401727 0.099125996 0.10116779 0.10307547 0.10556567 0.11273325 0.10492612 0.095749743 0.0907616 0.085062228 0.085816093 0.0858139 0.083421253]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 02:18:52.717339: step 10, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.444 sec/batch; 41h:01m:54s remains)
INFO - root - 2017-12-06 02:18:57.200351: step 20, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 41h:26m:44s remains)
INFO - root - 2017-12-06 02:19:01.674278: step 30, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.448 sec/batch; 41h:23m:38s remains)
INFO - root - 2017-12-06 02:19:06.160621: step 40, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.458 sec/batch; 42h:19m:59s remains)
INFO - root - 2017-12-06 02:19:10.632048: step 50, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.443 sec/batch; 40h:53m:41s remains)
INFO - root - 2017-12-06 02:19:15.038055: step 60, loss = 0.89, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:35m:24s remains)
INFO - root - 2017-12-06 02:19:19.505611: step 70, loss = 0.93, batch loss = 0.72 (18.4 examples/sec; 0.436 sec/batch; 40h:13m:59s remains)
INFO - root - 2017-12-06 02:19:24.066725: step 80, loss = 0.89, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:46m:47s remains)
INFO - root - 2017-12-06 02:19:28.251487: step 90, loss = 0.89, batch loss = 0.68 (18.2 examples/sec; 0.440 sec/batch; 40h:39m:56s remains)
INFO - root - 2017-12-06 02:19:32.681098: step 100, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 41h:25m:41s remains)
2017-12-06 02:19:33.126144: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11435684 0.15441398 0.20491077 0.25057298 0.27651685 0.28173041 0.2642571 0.23700912 0.21655561 0.20971505 0.21847373 0.23965584 0.26042062 0.27325153 0.26713374][0.11602367 0.15986897 0.21682325 0.27259094 0.30881557 0.32184353 0.31071487 0.28665778 0.26352972 0.25058091 0.25298873 0.2633374 0.27447963 0.280441 0.27114904][0.11216858 0.15826024 0.21858047 0.27553061 0.31285894 0.32633197 0.32490087 0.31098285 0.29299054 0.28007826 0.27503005 0.27631581 0.28043368 0.27594784 0.26281685][0.10427973 0.1492545 0.20924954 0.26283488 0.29899809 0.31647226 0.32028788 0.31373233 0.30177563 0.29263294 0.28712547 0.28645465 0.28590772 0.27147597 0.25240967][0.096574843 0.14025612 0.2012537 0.25533673 0.28797045 0.305428 0.31001997 0.31060725 0.30724108 0.30520347 0.301402 0.29760608 0.2915315 0.26862559 0.24444646][0.093288936 0.1338639 0.19309665 0.25176352 0.28341773 0.29655471 0.30285755 0.31280482 0.32062641 0.32240441 0.31429639 0.30262327 0.28757966 0.26785591 0.24514726][0.09252394 0.13060378 0.18346842 0.23832068 0.27170038 0.28313708 0.28998309 0.30478597 0.32629487 0.33408329 0.32007539 0.30130446 0.28236914 0.2639797 0.24864045][0.089736611 0.12184316 0.16771916 0.21334821 0.24351709 0.25564593 0.26611736 0.28577125 0.30629414 0.31875244 0.31016964 0.29384747 0.27830556 0.26097578 0.24971446][0.085961968 0.11074432 0.14757398 0.18574658 0.20837982 0.21811111 0.23242068 0.25390917 0.2712633 0.28259382 0.28321826 0.27796018 0.27448323 0.26551005 0.25493285][0.081570946 0.10082137 0.1266543 0.15542558 0.17652412 0.18834963 0.20401289 0.22362536 0.24078391 0.250965 0.25702277 0.26239115 0.26778173 0.26391119 0.25503516][0.075594723 0.091630064 0.11541069 0.13929664 0.15841752 0.17413485 0.19099413 0.20446524 0.21484764 0.22551046 0.23996465 0.25205982 0.2585749 0.2562072 0.24756542][0.07327573 0.08668261 0.10850488 0.13233581 0.15277661 0.16934651 0.18447982 0.19123563 0.19299605 0.20074652 0.21836536 0.2357485 0.24714449 0.24478576 0.236513][0.073868781 0.086067542 0.10402919 0.12631531 0.14914939 0.16327262 0.17186329 0.17522079 0.17081155 0.17289515 0.18741205 0.20420878 0.21925779 0.22001565 0.21060568][0.073026247 0.08259099 0.096000895 0.11122977 0.12831514 0.13976337 0.14307667 0.1416743 0.13942061 0.14070585 0.15234405 0.16680841 0.17789032 0.1787504 0.16734873][0.070677876 0.075839825 0.083894543 0.093359888 0.10082229 0.10490635 0.10608591 0.10439195 0.10376661 0.10678906 0.11458929 0.12302086 0.12741609 0.12626919 0.12048249]]...]
INFO - root - 2017-12-06 02:19:37.678754: step 110, loss = 0.94, batch loss = 0.73 (18.2 examples/sec; 0.440 sec/batch; 40h:37m:34s remains)
INFO - root - 2017-12-06 02:19:42.211734: step 120, loss = 0.89, batch loss = 0.68 (17.0 examples/sec; 0.469 sec/batch; 43h:19m:54s remains)
INFO - root - 2017-12-06 02:19:46.663835: step 130, loss = 0.87, batch loss = 0.67 (18.0 examples/sec; 0.445 sec/batch; 41h:04m:12s remains)
INFO - root - 2017-12-06 02:19:51.117016: step 140, loss = 0.86, batch loss = 0.65 (18.3 examples/sec; 0.437 sec/batch; 40h:19m:39s remains)
INFO - root - 2017-12-06 02:19:55.604855: step 150, loss = 0.88, batch loss = 0.68 (17.7 examples/sec; 0.453 sec/batch; 41h:50m:38s remains)
INFO - root - 2017-12-06 02:20:00.107122: step 160, loss = 0.87, batch loss = 0.66 (18.1 examples/sec; 0.441 sec/batch; 40h:44m:31s remains)
INFO - root - 2017-12-06 02:20:04.716815: step 170, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.451 sec/batch; 41h:37m:46s remains)
INFO - root - 2017-12-06 02:20:09.230657: step 180, loss = 0.88, batch loss = 0.67 (16.8 examples/sec; 0.476 sec/batch; 43h:57m:15s remains)
INFO - root - 2017-12-06 02:20:13.743147: step 190, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.459 sec/batch; 42h:24m:40s remains)
INFO - root - 2017-12-06 02:20:18.120591: step 200, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.453 sec/batch; 41h:46m:31s remains)
2017-12-06 02:20:18.600916: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.087275177 0.089130215 0.080814511 0.079264306 0.080925085 0.080078028 0.079962835 0.076794833 0.073896736 0.068401039 0.057851739 0.049137656 0.041449856 0.035898428 0.035407927][0.10388516 0.10149231 0.085699953 0.074840307 0.075288095 0.076575927 0.074144505 0.068968579 0.065865695 0.062435292 0.056017771 0.051246755 0.045988411 0.041383795 0.04171871][0.13489029 0.13176203 0.10739881 0.083797678 0.07516747 0.076263033 0.073837876 0.0708612 0.065688163 0.062763885 0.056328282 0.055032074 0.047397852 0.0454027 0.045221709][0.16525194 0.17009154 0.14070898 0.10749245 0.090195715 0.092637166 0.093894266 0.094188228 0.08114709 0.074660748 0.062721118 0.056413822 0.046053175 0.046254423 0.046288572][0.17897719 0.18883207 0.16158706 0.12841797 0.11045733 0.1229587 0.13419604 0.14172843 0.11777272 0.10233438 0.081309251 0.067134723 0.050449789 0.047979783 0.047512025][0.17366242 0.18544263 0.17054754 0.14527193 0.13711914 0.16472885 0.19118589 0.21014196 0.1755805 0.14740443 0.11297069 0.091846347 0.072050773 0.064315364 0.060806148][0.19156799 0.20723993 0.19441187 0.17242241 0.16839868 0.1963543 0.24828207 0.30163649 0.24738103 0.19482324 0.15197644 0.12873653 0.11348876 0.10864519 0.10031056][0.20165864 0.22374889 0.20539391 0.17899194 0.16837418 0.18966851 0.22553104 0.258639 0.23119479 0.19391233 0.15822986 0.13784051 0.12397427 0.1181883 0.11297145][0.17067873 0.18924579 0.16931084 0.14245096 0.12343038 0.13586599 0.15313026 0.16682962 0.15622211 0.14182934 0.11864022 0.10908519 0.10121451 0.0985278 0.090736277][0.12620321 0.13672978 0.12221877 0.10159528 0.085703045 0.089305021 0.1005621 0.10839047 0.10498285 0.099017464 0.085362345 0.080751 0.076221108 0.071003772 0.061391607][0.0919991 0.10384277 0.099942923 0.084429316 0.075182885 0.079524979 0.087181985 0.09382119 0.0935852 0.085792623 0.075018182 0.068801954 0.062407956 0.055641942 0.047425345][0.10176987 0.11130251 0.11193217 0.10541443 0.09581127 0.09848208 0.10621227 0.10957627 0.10851025 0.10621504 0.092345864 0.081328034 0.072359793 0.064219318 0.0565318][0.12439937 0.12685451 0.12484332 0.11947655 0.11201888 0.11177942 0.11704733 0.12041836 0.119823 0.12432004 0.11610358 0.10370489 0.091692373 0.083968908 0.0746512][0.13234153 0.13758081 0.13548124 0.13155282 0.12518325 0.1238803 0.12630135 0.12957031 0.13218862 0.13262299 0.12636331 0.11274334 0.10137375 0.093554907 0.0876979][0.10697128 0.1145693 0.1139209 0.108831 0.1008073 0.096818306 0.0983899 0.10127646 0.10229529 0.10283681 0.0986311 0.088250518 0.081053264 0.077767305 0.073267065]]...]
INFO - root - 2017-12-06 02:20:23.229617: step 210, loss = 0.91, batch loss = 0.70 (16.6 examples/sec; 0.481 sec/batch; 44h:21m:11s remains)
INFO - root - 2017-12-06 02:20:27.704488: step 220, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.446 sec/batch; 41h:07m:26s remains)
INFO - root - 2017-12-06 02:20:32.189439: step 230, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.451 sec/batch; 41h:37m:50s remains)
INFO - root - 2017-12-06 02:20:36.747724: step 240, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 41h:28m:29s remains)
INFO - root - 2017-12-06 02:20:41.259955: step 250, loss = 0.88, batch loss = 0.67 (16.9 examples/sec; 0.472 sec/batch; 43h:34m:20s remains)
INFO - root - 2017-12-06 02:20:45.738774: step 260, loss = 0.85, batch loss = 0.64 (17.5 examples/sec; 0.458 sec/batch; 42h:17m:16s remains)
INFO - root - 2017-12-06 02:20:50.169851: step 270, loss = 0.89, batch loss = 0.68 (17.2 examples/sec; 0.464 sec/batch; 42h:48m:41s remains)
INFO - root - 2017-12-06 02:20:54.705587: step 280, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.444 sec/batch; 41h:00m:41s remains)
INFO - root - 2017-12-06 02:20:59.231300: step 290, loss = 0.87, batch loss = 0.66 (18.2 examples/sec; 0.439 sec/batch; 40h:30m:50s remains)
INFO - root - 2017-12-06 02:21:03.454320: step 300, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.451 sec/batch; 41h:37m:39s remains)
2017-12-06 02:21:03.913076: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039933406 0.041975819 0.036744103 0.032526426 0.028956966 0.025568755 0.024261376 0.025927676 0.030666923 0.036201894 0.040940687 0.040257283 0.033839956 0.030277694 0.028126294][0.031341292 0.034764022 0.033362582 0.032728165 0.030887751 0.028209178 0.024674622 0.023353899 0.025482832 0.029916728 0.0327883 0.03291674 0.031082084 0.0291737 0.028301084][0.027239794 0.029890055 0.031052629 0.033524819 0.032775953 0.031618863 0.025297625 0.020441176 0.01969461 0.022189109 0.02422837 0.025366062 0.026649175 0.027120875 0.027934739][0.026810771 0.030200327 0.03443896 0.035925038 0.035614811 0.035690673 0.029484162 0.024245596 0.020517701 0.019977638 0.021351343 0.023495654 0.025162259 0.027177079 0.029388605][0.025583239 0.030316791 0.036525123 0.041352853 0.043707073 0.041583985 0.034085348 0.028822182 0.023372652 0.020205347 0.019902887 0.023026148 0.026766935 0.029664641 0.032730013][0.02458678 0.030504437 0.038903914 0.047358006 0.052095927 0.045231052 0.036660515 0.030861443 0.026550712 0.022005333 0.019570472 0.023534315 0.028883303 0.032538474 0.037917309][0.014055649 0.020634824 0.031587854 0.047132321 0.055701897 0.049370736 0.040595345 0.035801746 0.028619392 0.021819593 0.019160779 0.022699157 0.026618296 0.032448173 0.042308256][0.0034291558 0.007507924 0.021901293 0.03967452 0.050668009 0.046607964 0.042509511 0.039671466 0.029563745 0.024002222 0.023515103 0.023868637 0.025382986 0.034226984 0.046635918][-0.0015440248 -0.001349723 0.012771387 0.028540207 0.037781209 0.038223937 0.039517619 0.037624285 0.032082371 0.032202952 0.034258284 0.029070219 0.027639626 0.036199637 0.04606393][-0.0018452648 -0.0039172005 0.0045258831 0.014222203 0.019441856 0.022625657 0.026959298 0.028390633 0.031178514 0.034145936 0.034827463 0.026129173 0.026281705 0.032052584 0.037508972][0.003317887 -7.8236684e-05 0.0023342967 0.0055677909 0.00816999 0.00957641 0.012946092 0.01722434 0.020355655 0.022462936 0.024829583 0.021280503 0.022062091 0.02664111 0.028870428][0.016405122 0.00983466 0.0035392251 0.0021124985 0.0025599059 0.0034103934 0.0057835113 0.0087625533 0.011188922 0.013580181 0.016871376 0.017728565 0.019616304 0.022513071 0.023842381][0.030520035 0.022281388 0.011452459 0.0070537161 0.0054646637 0.0059442651 0.0074588545 0.0084877852 0.011004226 0.014106819 0.018503243 0.020754481 0.022239959 0.022919854 0.023329498][0.034590758 0.029081551 0.020622866 0.015963143 0.014517905 0.014415605 0.015093347 0.016256174 0.018396651 0.020580357 0.023056919 0.024161318 0.02462578 0.024016907 0.023534866][0.03000796 0.029262116 0.025815492 0.024231011 0.023457186 0.023174858 0.023330165 0.023280779 0.023870258 0.024797784 0.026105301 0.026608353 0.026419925 0.025400745 0.024303252]]...]
INFO - root - 2017-12-06 02:21:08.497645: step 310, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.447 sec/batch; 41h:16m:00s remains)
INFO - root - 2017-12-06 02:21:13.028947: step 320, loss = 0.87, batch loss = 0.66 (18.1 examples/sec; 0.441 sec/batch; 40h:40m:17s remains)
INFO - root - 2017-12-06 02:21:17.492766: step 330, loss = 0.90, batch loss = 0.69 (18.0 examples/sec; 0.446 sec/batch; 41h:07m:03s remains)
INFO - root - 2017-12-06 02:21:22.020147: step 340, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.445 sec/batch; 41h:02m:26s remains)
INFO - root - 2017-12-06 02:21:26.529925: step 350, loss = 0.88, batch loss = 0.68 (18.4 examples/sec; 0.435 sec/batch; 40h:06m:43s remains)
INFO - root - 2017-12-06 02:21:31.088650: step 360, loss = 0.88, batch loss = 0.67 (17.4 examples/sec; 0.460 sec/batch; 42h:27m:48s remains)
INFO - root - 2017-12-06 02:21:35.625522: step 370, loss = 0.87, batch loss = 0.66 (18.2 examples/sec; 0.439 sec/batch; 40h:28m:02s remains)
INFO - root - 2017-12-06 02:21:40.180545: step 380, loss = 0.87, batch loss = 0.66 (17.0 examples/sec; 0.469 sec/batch; 43h:17m:50s remains)
INFO - root - 2017-12-06 02:21:44.691796: step 390, loss = 0.87, batch loss = 0.66 (18.7 examples/sec; 0.428 sec/batch; 39h:28m:18s remains)
INFO - root - 2017-12-06 02:21:49.207825: step 400, loss = 0.89, batch loss = 0.68 (26.0 examples/sec; 0.308 sec/batch; 28h:22m:57s remains)
2017-12-06 02:21:49.603780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0078898286 -0.015549822 -0.022818319 -0.028089361 -0.029329978 -0.029849172 -0.032535546 -0.036344253 -0.039711103 -0.045051396 -0.052145295 -0.05708611 -0.057272572 -0.051225618 -0.032791466][0.0052320585 -0.0033564642 -0.014394186 -0.024350548 -0.028426034 -0.03029957 -0.034541626 -0.039155636 -0.037975848 -0.035529569 -0.032846786 -0.032625332 -0.03184206 -0.028567413 -0.019710105][0.028065715 0.022408508 0.012677222 0.0014818907 -0.0061914716 -0.010458788 -0.013707039 -0.015837671 -0.0074675735 0.0092964955 0.033598572 0.052291863 0.065590352 0.064224958 0.052765258][0.061780967 0.0674257 0.069269262 0.065762915 0.060914427 0.057285018 0.054204233 0.055820376 0.069559485 0.09913791 0.14189929 0.17942864 0.20589513 0.19888675 0.16387837][0.084060684 0.10554692 0.12486732 0.13667218 0.14175542 0.14363109 0.14033432 0.14006354 0.15262191 0.18289854 0.23051605 0.27733788 0.31198731 0.30648386 0.25972456][0.067928068 0.095398478 0.1307392 0.16521607 0.19012983 0.20454039 0.20686299 0.20362422 0.2104066 0.23554945 0.27977607 0.33010456 0.36645591 0.35832274 0.30245808][0.046424679 0.069471419 0.10553817 0.14768291 0.18512923 0.2109888 0.22108971 0.2196258 0.22499134 0.24615276 0.28348663 0.33104742 0.36927062 0.36125913 0.30241245][0.034640804 0.048194885 0.074513458 0.10858016 0.1434653 0.1701774 0.18838337 0.19816908 0.20845166 0.22609441 0.258067 0.30179107 0.33840442 0.33243066 0.28143913][0.028303687 0.036869589 0.052781798 0.074386418 0.097545952 0.11589395 0.13337992 0.14814176 0.15982299 0.17640767 0.20787507 0.25326383 0.28855398 0.28544286 0.24586421][0.026458021 0.029134274 0.035583291 0.04620079 0.057226621 0.066295154 0.0789482 0.090235874 0.097793922 0.11129355 0.13647394 0.17142084 0.19853462 0.2016971 0.1792558][0.032101929 0.032098781 0.033580773 0.036536973 0.039874118 0.042336315 0.046846732 0.050721273 0.054059498 0.059811324 0.070222124 0.086333863 0.10114528 0.10714354 0.10324529][0.036891036 0.036079168 0.037094697 0.03906868 0.039950445 0.039105691 0.038357608 0.038151644 0.038240921 0.037042912 0.037484556 0.038650971 0.040536109 0.042890027 0.043279506][0.03783739 0.03994472 0.043624632 0.044997238 0.043542542 0.040978357 0.039102387 0.038668916 0.03807937 0.034927841 0.03342022 0.031918596 0.030096672 0.026583318 0.02226549][0.036844332 0.043180868 0.049649566 0.051002719 0.048070982 0.044505879 0.04161454 0.038313828 0.035491031 0.032318406 0.031233422 0.029802315 0.029314626 0.025188483 0.018433809][0.029487643 0.035873666 0.041997537 0.044346698 0.043765508 0.042103365 0.039384529 0.035407852 0.030888993 0.026751388 0.025558535 0.022556189 0.021214012 0.018245734 0.014772497]]...]
INFO - root - 2017-12-06 02:21:54.105676: step 410, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.448 sec/batch; 41h:21m:42s remains)
INFO - root - 2017-12-06 02:21:58.572833: step 420, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.436 sec/batch; 40h:14m:11s remains)
INFO - root - 2017-12-06 02:22:03.069115: step 430, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.443 sec/batch; 40h:53m:45s remains)
INFO - root - 2017-12-06 02:22:07.531789: step 440, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.450 sec/batch; 41h:31m:14s remains)
INFO - root - 2017-12-06 02:22:12.061082: step 450, loss = 0.88, batch loss = 0.68 (17.8 examples/sec; 0.450 sec/batch; 41h:29m:12s remains)
INFO - root - 2017-12-06 02:22:16.578996: step 460, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 41h:43m:31s remains)
INFO - root - 2017-12-06 02:22:21.188711: step 470, loss = 0.87, batch loss = 0.66 (17.5 examples/sec; 0.458 sec/batch; 42h:13m:33s remains)
INFO - root - 2017-12-06 02:22:25.728223: step 480, loss = 0.87, batch loss = 0.67 (17.2 examples/sec; 0.466 sec/batch; 42h:56m:06s remains)
INFO - root - 2017-12-06 02:22:30.264056: step 490, loss = 0.87, batch loss = 0.66 (18.4 examples/sec; 0.434 sec/batch; 40h:00m:00s remains)
INFO - root - 2017-12-06 02:22:34.830045: step 500, loss = 0.88, batch loss = 0.67 (16.9 examples/sec; 0.473 sec/batch; 43h:38m:12s remains)
2017-12-06 02:22:35.295693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021097828 -0.020613914 -0.021949358 -0.021018736 -0.018290279 -0.018720543 -0.019264536 -0.017652215 -0.016592102 -0.016225787 -0.016863551 -0.012402603 -0.0051627848 0.0039668586 0.016491026][-0.017887013 -0.017100614 -0.018988922 -0.01836152 -0.015303757 -0.015448944 -0.016323905 -0.015025422 -0.012088516 -0.010063876 -0.0071370192 0.00090357661 0.0083598457 0.017273564 0.029714942][-0.013426373 -0.013698529 -0.015063388 -0.013473148 -0.01165275 -0.011332316 -0.011493328 -0.010469425 -0.0052401572 0.00039681606 0.0054585431 0.01043516 0.016907349 0.025841378 0.038316131][-0.0044102035 -0.0066218562 -0.0078227594 -0.0056631695 -0.0044723973 -0.0038868934 -0.0020211581 0.00055211596 0.0081997514 0.014885247 0.017879508 0.020232666 0.022264935 0.026529849 0.0337458][0.0059395246 0.0061308518 0.0064790249 0.0081653371 0.010397047 0.013756935 0.016768415 0.019050296 0.026879884 0.031825993 0.034065548 0.030863304 0.022839483 0.021393701 0.019933905][0.012748405 0.017442208 0.020034917 0.026905328 0.035191312 0.040865988 0.043086477 0.042128868 0.046522252 0.04947795 0.048818603 0.039688282 0.02736849 0.016709656 0.012969706][0.019627336 0.024688009 0.030019727 0.044612236 0.058257706 0.0651565 0.067876726 0.064969972 0.059000887 0.054800652 0.053832404 0.046390861 0.035306152 0.026358016 0.02174915][0.032389306 0.033811152 0.041525729 0.057044595 0.068220653 0.080200404 0.08881823 0.081695288 0.061589025 0.050044686 0.049154811 0.048648447 0.044715658 0.041730031 0.034039624][0.04563871 0.046766639 0.053151451 0.06307584 0.064510889 0.06980864 0.0763664 0.070105739 0.053803973 0.041935615 0.041215584 0.042460866 0.045402646 0.049622349 0.044071071][0.057027139 0.058040939 0.059800088 0.059267811 0.053341679 0.052151404 0.051603369 0.047119558 0.039016575 0.032238819 0.032302637 0.035416532 0.039627597 0.046737909 0.045264617][0.060722254 0.058084056 0.051876202 0.044873022 0.040412061 0.038868368 0.038457565 0.037188776 0.033057943 0.029810652 0.029755775 0.031210318 0.0327115 0.037129156 0.03941109][0.048042983 0.044392839 0.041511558 0.038352966 0.035687834 0.034584332 0.034714065 0.034333274 0.032537472 0.031043567 0.030941769 0.031532791 0.032398909 0.033762753 0.035686981][0.035371087 0.035948619 0.035433047 0.035195645 0.034562726 0.034012705 0.034115188 0.033608854 0.032973524 0.031866383 0.031706642 0.032112814 0.033116482 0.033994224 0.035062551][0.029109456 0.031356182 0.032101344 0.032054167 0.031478338 0.03115464 0.031610604 0.031644214 0.031821743 0.031517286 0.031555243 0.032284088 0.033089645 0.033358108 0.033584204][0.025536347 0.02705548 0.027726162 0.027971625 0.027968645 0.028032396 0.02829605 0.028519124 0.028868377 0.028763518 0.028500523 0.028852224 0.029611327 0.0295761 0.02936326]]...]
INFO - root - 2017-12-06 02:22:39.487593: step 510, loss = 0.87, batch loss = 0.66 (17.6 examples/sec; 0.454 sec/batch; 41h:49m:37s remains)
INFO - root - 2017-12-06 02:22:43.984078: step 520, loss = 0.89, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:44m:40s remains)
INFO - root - 2017-12-06 02:22:48.434129: step 530, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.445 sec/batch; 41h:00m:05s remains)
INFO - root - 2017-12-06 02:22:53.002040: step 540, loss = 0.90, batch loss = 0.69 (16.9 examples/sec; 0.472 sec/batch; 43h:32m:33s remains)
INFO - root - 2017-12-06 02:22:57.510941: step 550, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.453 sec/batch; 41h:44m:20s remains)
INFO - root - 2017-12-06 02:23:01.993600: step 560, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.451 sec/batch; 41h:34m:23s remains)
INFO - root - 2017-12-06 02:23:06.493260: step 570, loss = 0.88, batch loss = 0.67 (17.5 examples/sec; 0.458 sec/batch; 42h:13m:00s remains)
INFO - root - 2017-12-06 02:23:10.939262: step 580, loss = 0.88, batch loss = 0.67 (18.5 examples/sec; 0.433 sec/batch; 39h:56m:15s remains)
INFO - root - 2017-12-06 02:23:15.425694: step 590, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.451 sec/batch; 41h:34m:44s remains)
INFO - root - 2017-12-06 02:23:19.950984: step 600, loss = 0.89, batch loss = 0.69 (17.4 examples/sec; 0.461 sec/batch; 42h:30m:24s remains)
2017-12-06 02:23:20.392784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039777443 -0.062757604 -0.09157981 -0.12892729 -0.1593587 -0.15469249 -0.11745144 -0.051260144 0.041504681 0.1373553 0.2205534 0.27405685 0.28187332 0.23963442 0.1640725][-0.10154154 -0.12587649 -0.14582644 -0.16764176 -0.1836451 -0.17482354 -0.14194494 -0.08175455 0.00022962131 0.08618553 0.16407503 0.21076657 0.2031455 0.14713399 0.066139765][-0.15362188 -0.16532819 -0.16627021 -0.15290855 -0.12278145 -0.064530171 0.011394879 0.096843742 0.17938167 0.25412956 0.30512539 0.31518453 0.26305419 0.15780275 0.05639071][-0.14323105 -0.13291647 -0.10451438 -0.044653118 0.043885373 0.15737619 0.27524477 0.37680197 0.45307904 0.50233233 0.50373477 0.44498417 0.33340025 0.19249602 0.069692127][-0.070092365 -0.014467366 0.066605516 0.1793392 0.31426892 0.45197573 0.56459647 0.64288008 0.67456746 0.65932178 0.58096784 0.47149491 0.33917031 0.20127977 0.0736115][0.024355663 0.11117727 0.22833493 0.3658666 0.5074532 0.640184 0.73407018 0.766453 0.73682696 0.64204776 0.51244444 0.397944 0.29452804 0.20247521 0.090598233][0.06181366 0.15567887 0.27479276 0.39922094 0.51352674 0.61818194 0.68419909 0.67991912 0.60521519 0.4883824 0.38358954 0.32321998 0.28235033 0.23337767 0.12212203][0.084102638 0.17893027 0.28036368 0.36933872 0.43968961 0.49167454 0.51631731 0.48516774 0.40515986 0.31755075 0.26904246 0.27001134 0.27384567 0.23953554 0.1181869][0.12958081 0.19164042 0.25970015 0.2993364 0.32515004 0.33762059 0.33702102 0.30021927 0.23958504 0.18410425 0.17396019 0.20145684 0.21221824 0.18316184 0.08500763][0.24141556 0.25259936 0.26172373 0.24625403 0.214362 0.17873831 0.15194507 0.13546771 0.11694501 0.093937591 0.10624861 0.13720061 0.14949794 0.12094841 0.043873504][0.3481065 0.30787557 0.28126761 0.23545635 0.17700553 0.12031838 0.0735189 0.053282849 0.045590997 0.041980043 0.05699762 0.0786239 0.094839551 0.0859433 0.037952341][0.36726412 0.33040023 0.31891838 0.28094974 0.21231958 0.14715922 0.10561489 0.094684847 0.080287471 0.0763946 0.083455563 0.092252284 0.10124206 0.091673963 0.057211965][0.32610402 0.3251583 0.35183707 0.34055412 0.28725973 0.23644307 0.2110737 0.21207818 0.20804867 0.20584448 0.19434866 0.17867054 0.16564427 0.14618069 0.12481125][0.23703209 0.27753285 0.34068546 0.35883963 0.34041464 0.32400742 0.32327342 0.336295 0.33311892 0.32718185 0.30821964 0.28153008 0.25375503 0.23143685 0.21294028][0.19304125 0.2529653 0.33185002 0.37555137 0.40486923 0.43570346 0.46539006 0.48351574 0.47815013 0.47212446 0.44832864 0.42746979 0.41093314 0.39822721 0.3915081]]...]
INFO - root - 2017-12-06 02:23:24.696288: step 610, loss = 0.89, batch loss = 0.68 (18.4 examples/sec; 0.435 sec/batch; 40h:07m:42s remains)
INFO - root - 2017-12-06 02:23:29.202689: step 620, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 41h:24m:26s remains)
INFO - root - 2017-12-06 02:23:33.598439: step 630, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.442 sec/batch; 40h:45m:35s remains)
INFO - root - 2017-12-06 02:23:38.044458: step 640, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.443 sec/batch; 40h:49m:24s remains)
INFO - root - 2017-12-06 02:23:42.551464: step 650, loss = 0.87, batch loss = 0.66 (18.1 examples/sec; 0.443 sec/batch; 40h:50m:51s remains)
INFO - root - 2017-12-06 02:23:47.093184: step 660, loss = 0.88, batch loss = 0.68 (17.2 examples/sec; 0.466 sec/batch; 42h:58m:05s remains)
INFO - root - 2017-12-06 02:23:51.640576: step 670, loss = 0.87, batch loss = 0.66 (17.0 examples/sec; 0.470 sec/batch; 43h:17m:42s remains)
INFO - root - 2017-12-06 02:23:56.153684: step 680, loss = 0.90, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 40h:40m:11s remains)
INFO - root - 2017-12-06 02:24:00.788323: step 690, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.462 sec/batch; 42h:32m:37s remains)
INFO - root - 2017-12-06 02:24:05.280338: step 700, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.446 sec/batch; 41h:08m:57s remains)
2017-12-06 02:24:05.709670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0065978393 -0.009048989 -0.014068322 -0.01904621 -0.022507722 -0.02393771 -0.023255737 -0.020927334 -0.017763214 -0.014397115 -0.011208024 -0.0088028312 -0.007299101 -0.0066895578 -0.0066451244][-0.0075492375 -0.010485165 -0.015901059 -0.021007676 -0.024457157 -0.025599347 -0.024507267 -0.021894446 -0.018422544 -0.014681333 -0.011465359 -0.0090818815 -0.0075907651 -0.0068305768 -0.0065820292][-0.0087300837 -0.012397988 -0.01844006 -0.023768062 -0.027254919 -0.028213086 -0.026734171 -0.023565497 -0.019758845 -0.015592979 -0.012044184 -0.0095343869 -0.0081665348 -0.0074685384 -0.0070413668][-0.0097680371 -0.01390337 -0.020196395 -0.025502559 -0.02903251 -0.03001589 -0.028820544 -0.025409244 -0.021301348 -0.017000673 -0.013405325 -0.010812739 -0.0093772057 -0.0087181218 -0.0081192926][-0.010178331 -0.013983302 -0.019999634 -0.024896519 -0.0280113 -0.029091502 -0.02855779 -0.026069675 -0.022726465 -0.018832322 -0.015517357 -0.013216959 -0.011815149 -0.010904908 -0.009842081][-0.010663534 -0.013921708 -0.019100035 -0.023431282 -0.025873378 -0.026493531 -0.025864536 -0.023956858 -0.021927565 -0.019242704 -0.017023295 -0.015429094 -0.014134137 -0.013010666 -0.011587664][-0.011235809 -0.013804086 -0.017626243 -0.021323938 -0.023688991 -0.024027906 -0.022778058 -0.020659028 -0.019462157 -0.018249521 -0.017318567 -0.016327027 -0.015258762 -0.01410234 -0.012616586][-0.011172323 -0.013482405 -0.016536724 -0.0195883 -0.021785906 -0.022314191 -0.020280853 -0.017300818 -0.015874427 -0.015176435 -0.01515432 -0.014938105 -0.014192004 -0.013296986 -0.012175076][-0.010145007 -0.012040708 -0.014902319 -0.017705204 -0.019517254 -0.019974491 -0.018029928 -0.014901767 -0.012506448 -0.010952149 -0.0106861 -0.010918487 -0.010968568 -0.010688515 -0.010399526][-0.008581955 -0.0096990895 -0.011830507 -0.01449242 -0.016556691 -0.017239619 -0.016282614 -0.013925955 -0.01072412 -0.0072710272 -0.0055836663 -0.0059502684 -0.0068998281 -0.0076117367 -0.0081993826][-0.0070856325 -0.0070633031 -0.0078302417 -0.0095027946 -0.011549985 -0.012952279 -0.013473047 -0.012295963 -0.0093618967 -0.0049745794 -0.0018788949 -0.0015753265 -0.0029678773 -0.0046903044 -0.0059563052][-0.0064213127 -0.0057220105 -0.0050780885 -0.0054668281 -0.0071210209 -0.0089566726 -0.010147482 -0.010212114 -0.008465074 -0.0048206225 -0.00146419 -0.00023390725 -0.001116544 -0.0028905887 -0.0044775512][-0.0065343957 -0.0058337133 -0.0048180465 -0.0045054257 -0.0057069529 -0.0076039974 -0.0090514515 -0.0095478389 -0.0088379607 -0.0065953843 -0.0036989097 -0.0018754192 -0.0015181284 -0.002434656 -0.0038867798][-0.0067188572 -0.0061041843 -0.0053853076 -0.00508273 -0.0060543828 -0.0078415349 -0.0092860106 -0.0098686535 -0.0094830636 -0.0080089848 -0.0060070492 -0.0041729249 -0.0030483119 -0.0030091871 -0.0038714297][-0.0072550066 -0.0064126235 -0.0057613272 -0.0055259448 -0.0062939152 -0.0078759938 -0.0092465412 -0.0099661388 -0.0099791642 -0.0091800131 -0.0078216139 -0.0061305854 -0.0045166481 -0.0036606025 -0.0039538797]]...]
INFO - root - 2017-12-06 02:24:10.183582: step 710, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 41h:13m:05s remains)
INFO - root - 2017-12-06 02:24:14.431876: step 720, loss = 0.89, batch loss = 0.68 (17.0 examples/sec; 0.472 sec/batch; 43h:29m:51s remains)
INFO - root - 2017-12-06 02:24:18.959288: step 730, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 41h:40m:09s remains)
INFO - root - 2017-12-06 02:24:23.431115: step 740, loss = 0.90, batch loss = 0.69 (18.6 examples/sec; 0.430 sec/batch; 39h:38m:35s remains)
INFO - root - 2017-12-06 02:24:27.916683: step 750, loss = 0.90, batch loss = 0.69 (18.0 examples/sec; 0.445 sec/batch; 40h:59m:20s remains)
INFO - root - 2017-12-06 02:24:32.378807: step 760, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.439 sec/batch; 40h:27m:00s remains)
INFO - root - 2017-12-06 02:24:36.855225: step 770, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.458 sec/batch; 42h:13m:21s remains)
INFO - root - 2017-12-06 02:24:41.355601: step 780, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.449 sec/batch; 41h:23m:59s remains)
INFO - root - 2017-12-06 02:24:45.903313: step 790, loss = 0.90, batch loss = 0.69 (17.2 examples/sec; 0.465 sec/batch; 42h:51m:01s remains)
INFO - root - 2017-12-06 02:24:50.397566: step 800, loss = 0.90, batch loss = 0.69 (17.8 examples/sec; 0.448 sec/batch; 41h:17m:57s remains)
2017-12-06 02:24:50.877408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030765716 -0.034691572 -0.048107445 -0.063824177 -0.079738773 -0.091629848 -0.10266162 -0.11620787 -0.12554887 -0.12752205 -0.12511356 -0.12116627 -0.1174342 -0.11231484 -0.10792406][-0.054390922 -0.056819871 -0.069243595 -0.082736455 -0.09605588 -0.10524912 -0.11079222 -0.1173237 -0.11942774 -0.11684056 -0.1119519 -0.10865964 -0.10703392 -0.10429849 -0.10052449][-0.07128258 -0.070059918 -0.076641873 -0.08502984 -0.091693863 -0.095881149 -0.096039191 -0.097834125 -0.094782472 -0.087484553 -0.078507908 -0.072154671 -0.069228187 -0.068869516 -0.068720758][-0.070510834 -0.066008508 -0.06724453 -0.06966649 -0.071693316 -0.071234278 -0.06854862 -0.066284046 -0.061383292 -0.054514341 -0.048049659 -0.042135857 -0.040097807 -0.036498703 -0.032365791][-0.053877175 -0.04929585 -0.047798403 -0.045120593 -0.045021228 -0.043949287 -0.039340355 -0.032015942 -0.02601748 -0.017993953 -0.011314819 -0.00673745 -0.0076332893 -0.0080905743 -0.0084796306][-0.032975126 -0.029034166 -0.025136895 -0.018655503 -0.015924841 -0.010427095 -0.0019837487 0.0060273372 0.011829354 0.021149185 0.026514824 0.030887671 0.030223276 0.024647098 0.016182788][-0.013726138 -0.0097201765 -0.0040902905 0.0063406713 0.014763705 0.023707557 0.034742456 0.045013841 0.051728588 0.055461343 0.054418806 0.053820107 0.047940608 0.039053116 0.025871653][-0.0022325758 0.0016219318 0.0090977736 0.020812355 0.030689452 0.042629708 0.053602543 0.060172189 0.061014388 0.056394141 0.05295733 0.046793748 0.038154114 0.028937992 0.015543178][0.0043723509 0.0073348843 0.012553673 0.021457087 0.030611012 0.038183939 0.043433625 0.042791579 0.040464576 0.033547264 0.026881937 0.01892902 0.013551254 0.0048867911 -0.0050570797][0.0058266446 0.0080726929 0.011370618 0.014315903 0.017221771 0.0170669 0.013647731 0.0072763786 0.0029227622 -0.0024340078 -0.010022903 -0.017219741 -0.02166369 -0.025652997 -0.026315967][-0.0012586601 0.0026986636 0.005879283 0.0039823167 0.0013322532 -0.0025833771 -0.007491108 -0.01539233 -0.023715369 -0.031955793 -0.039638195 -0.04379984 -0.0463074 -0.047333203 -0.042889129][-0.0060015451 -0.0010121614 0.0019614436 -0.0006377697 -0.0065761581 -0.013280232 -0.019851025 -0.028093724 -0.035828419 -0.044322677 -0.053533874 -0.058438372 -0.059587926 -0.054696348 -0.047019836][-0.00582744 -0.0024159737 -0.00063165464 -0.0023584329 -0.009408446 -0.018245131 -0.026031811 -0.033748854 -0.039641544 -0.046876177 -0.055113122 -0.060813632 -0.060076665 -0.0540706 -0.046770386][-0.0037353113 -0.0029872768 -0.0017751995 -0.0030080974 -0.0091611706 -0.017807636 -0.026258774 -0.033501118 -0.039115388 -0.046146452 -0.051945675 -0.054141872 -0.052006572 -0.048337698 -0.042257518][0.0003608875 -0.00084928423 -0.0019974858 -0.0036942251 -0.0074776486 -0.013881175 -0.021414332 -0.027128344 -0.032655232 -0.038214192 -0.043404132 -0.0453623 -0.042738222 -0.039209571 -0.036796674]]...]
INFO - root - 2017-12-06 02:24:55.397853: step 810, loss = 0.86, batch loss = 0.65 (17.4 examples/sec; 0.459 sec/batch; 42h:18m:56s remains)
INFO - root - 2017-12-06 02:24:59.710027: step 820, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.443 sec/batch; 40h:51m:21s remains)
INFO - root - 2017-12-06 02:25:04.269690: step 830, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.458 sec/batch; 42h:09m:34s remains)
INFO - root - 2017-12-06 02:25:08.725597: step 840, loss = 0.87, batch loss = 0.66 (18.7 examples/sec; 0.429 sec/batch; 39h:30m:23s remains)
INFO - root - 2017-12-06 02:25:13.244572: step 850, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.445 sec/batch; 40h:58m:43s remains)
INFO - root - 2017-12-06 02:25:17.751018: step 860, loss = 0.89, batch loss = 0.68 (18.7 examples/sec; 0.428 sec/batch; 39h:26m:24s remains)
INFO - root - 2017-12-06 02:25:22.290347: step 870, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.449 sec/batch; 41h:18m:59s remains)
INFO - root - 2017-12-06 02:25:26.907802: step 880, loss = 0.90, batch loss = 0.69 (16.8 examples/sec; 0.477 sec/batch; 43h:55m:44s remains)
INFO - root - 2017-12-06 02:25:31.377554: step 890, loss = 0.90, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:47m:15s remains)
INFO - root - 2017-12-06 02:25:35.950733: step 900, loss = 0.88, batch loss = 0.67 (17.1 examples/sec; 0.469 sec/batch; 43h:13m:08s remains)
2017-12-06 02:25:36.406790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019290797 -0.021903269 -0.024792667 -0.027383409 -0.029124554 -0.029640306 -0.028747924 -0.027282655 -0.026162542 -0.024947787 -0.024854135 -0.025660073 -0.026460452 -0.027457664 -0.028054204][-0.015580242 -0.018982189 -0.022837181 -0.026434164 -0.028993269 -0.0301811 -0.029371606 -0.028141646 -0.027110126 -0.026395848 -0.026445922 -0.026912902 -0.027408978 -0.027871221 -0.028473867][-0.012177948 -0.016692273 -0.021095954 -0.024435367 -0.026420765 -0.027034458 -0.025739785 -0.024258539 -0.023405336 -0.023286376 -0.02363712 -0.024390666 -0.025603563 -0.026669256 -0.027859801][-0.0087399706 -0.013607822 -0.017609395 -0.019336332 -0.019286796 -0.018325604 -0.016342299 -0.014908005 -0.014469486 -0.015689533 -0.017410994 -0.019548092 -0.022303004 -0.024764823 -0.026655491][-0.0041007344 -0.0074543338 -0.0092854779 -0.0080979969 -0.0053269695 -0.0020130798 0.0012453347 0.00290161 0.0018354394 -0.0021604113 -0.0072798654 -0.01242009 -0.017667074 -0.022107061 -0.024992447][0.0023211353 0.0018146485 0.0032987557 0.0090775006 0.017056227 0.024429131 0.029480826 0.029881742 0.025034413 0.016658042 0.0065445267 -0.0032804683 -0.012097184 -0.019148123 -0.023572113][0.010027207 0.013797238 0.01986083 0.030522477 0.043210771 0.054098237 0.060840484 0.058532875 0.048174035 0.034141045 0.018998869 0.0049463622 -0.0072896071 -0.016596999 -0.022505052][0.018541198 0.026090894 0.034966659 0.046852212 0.059838574 0.070676446 0.077018008 0.070840225 0.054179627 0.036682393 0.020024162 0.0053026192 -0.0072673019 -0.016875476 -0.023109421][0.02475559 0.032500904 0.0406675 0.049870785 0.058172811 0.061369058 0.058291953 0.048836861 0.034930851 0.020605862 0.0075532757 -0.0038879141 -0.013595443 -0.021028478 -0.025653897][0.027691148 0.031804185 0.035513591 0.039586734 0.04172245 0.038818154 0.031344842 0.020404112 0.0080802813 -0.0024687145 -0.010781059 -0.017260322 -0.022169229 -0.025779344 -0.028010268][0.02724003 0.026885822 0.026172809 0.025602054 0.023571745 0.018330362 0.0098990239 -0.00063367933 -0.011075867 -0.018653654 -0.023272861 -0.026342925 -0.028234193 -0.029103799 -0.02942081][0.023673665 0.020358142 0.016827766 0.013448536 0.0089345053 0.0029833429 -0.0043528471 -0.012819622 -0.02093802 -0.026281089 -0.029015604 -0.030126447 -0.030489895 -0.030018516 -0.029563174][0.017590322 0.013620954 0.0086792186 0.0039099455 -0.001669718 -0.007569531 -0.013555821 -0.01981923 -0.025543856 -0.028883189 -0.030439863 -0.030824959 -0.0305969 -0.030235941 -0.029959317][0.011374965 0.0080980845 0.0029029697 -0.0024637524 -0.0082582664 -0.013688942 -0.018721204 -0.023307376 -0.027123135 -0.029102525 -0.029897055 -0.030342428 -0.03049532 -0.030539641 -0.030711381][0.0046401061 0.0024675839 -0.0020581838 -0.00727259 -0.012768319 -0.017776694 -0.022054909 -0.025447525 -0.02780577 -0.02909711 -0.02975734 -0.030237235 -0.030627675 -0.030739423 -0.030888842]]...]
INFO - root - 2017-12-06 02:25:40.871968: step 910, loss = 0.88, batch loss = 0.67 (18.5 examples/sec; 0.432 sec/batch; 39h:44m:54s remains)
INFO - root - 2017-12-06 02:25:45.178412: step 920, loss = 0.88, batch loss = 0.68 (19.2 examples/sec; 0.416 sec/batch; 38h:17m:16s remains)
INFO - root - 2017-12-06 02:25:49.710638: step 930, loss = 0.87, batch loss = 0.66 (16.6 examples/sec; 0.481 sec/batch; 44h:15m:45s remains)
INFO - root - 2017-12-06 02:25:54.233212: step 940, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.452 sec/batch; 41h:36m:29s remains)
INFO - root - 2017-12-06 02:25:58.771390: step 950, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.443 sec/batch; 40h:49m:51s remains)
INFO - root - 2017-12-06 02:26:03.269019: step 960, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.444 sec/batch; 40h:51m:47s remains)
INFO - root - 2017-12-06 02:26:07.740349: step 970, loss = 0.89, batch loss = 0.68 (18.5 examples/sec; 0.431 sec/batch; 39h:43m:26s remains)
INFO - root - 2017-12-06 02:26:12.235174: step 980, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 41h:26m:57s remains)
INFO - root - 2017-12-06 02:26:16.763450: step 990, loss = 0.89, batch loss = 0.68 (17.3 examples/sec; 0.463 sec/batch; 42h:37m:35s remains)
INFO - root - 2017-12-06 02:26:21.207074: step 1000, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.451 sec/batch; 41h:29m:38s remains)
2017-12-06 02:26:21.635403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.015529795 -0.014695754 -0.015419237 -0.015874773 -0.016358698 -0.016498772 -0.016666414 -0.017013405 -0.017370092 -0.018240359 -0.018700968 -0.017639773 -0.014943523 -0.011792568 -0.01075929][-0.01133069 -0.010725498 -0.011867955 -0.012071531 -0.012444295 -0.012363236 -0.011960393 -0.012086397 -0.012356382 -0.013810342 -0.014646672 -0.013898589 -0.012065735 -0.010293256 -0.0098152906][-0.0079903528 -0.0065158736 -0.0068227556 -0.0070334468 -0.0070015769 -0.0065060426 -0.0054770987 -0.0052851178 -0.00561779 -0.0076192021 -0.0092627313 -0.00949198 -0.0090835728 -0.008663645 -0.0088943932][-0.0046335086 -0.0028131828 -0.0019241571 -0.001166407 -0.00052910671 0.00097006187 0.0025096387 0.0029716901 0.0019370504 -0.001388181 -0.0046983883 -0.0060838275 -0.0069185309 -0.0072938967 -0.008026287][0.00024635717 0.001932513 0.0034843013 0.0054650828 0.0071296282 0.0090206191 0.011102397 0.01186534 0.0096417591 0.0044504963 -0.00046490133 -0.0031951163 -0.0050806217 -0.0065645278 -0.0076365508][0.0071272515 0.0087435469 0.010807674 0.013429489 0.015722256 0.018328831 0.020924937 0.020886805 0.016901232 0.010206241 0.0037219077 -0.00080202892 -0.0038464759 -0.0061857328 -0.0072865244][0.013404988 0.014303859 0.016273998 0.018645864 0.021206446 0.02472407 0.027226415 0.026032697 0.020755555 0.013327919 0.006143678 0.00073884055 -0.0029147826 -0.0058132894 -0.0072214361][0.017956417 0.018785421 0.020476919 0.022309452 0.025003321 0.029121771 0.031936824 0.029349849 0.022370212 0.01460287 0.0073566884 0.0018910244 -0.002113238 -0.0051641166 -0.0070044752][0.021417737 0.02225684 0.023851182 0.025074579 0.027390547 0.031403132 0.034329511 0.030453041 0.022827908 0.015284617 0.00844105 0.0030340403 -0.0011841655 -0.0044295229 -0.0066335928][0.024978772 0.025840878 0.026856404 0.027446967 0.02863393 0.030903585 0.031788543 0.028006725 0.022052724 0.015685473 0.0095912889 0.0043279231 0.0001424849 -0.0034491476 -0.0060718283][0.027122065 0.027768146 0.028450757 0.028686579 0.028812762 0.029201634 0.028225515 0.024989359 0.020428807 0.015357517 0.010039784 0.0052043609 0.0010745339 -0.0025492832 -0.0057202633][0.026945569 0.027271863 0.0277653 0.027782664 0.027354375 0.026594464 0.024899334 0.022257227 0.018823087 0.014767982 0.010063052 0.0054544322 0.0016552135 -0.0020071231 -0.0055976119][0.023816723 0.024315905 0.024618663 0.024460528 0.023592845 0.022331446 0.020774566 0.018710889 0.016370647 0.013365276 0.0093073063 0.0051026419 0.0014901273 -0.0022421274 -0.0058222357][0.019003835 0.019555889 0.019582666 0.019153584 0.018173382 0.017135419 0.015908048 0.014557727 0.012853455 0.010602597 0.0073606074 0.0038197041 0.00033997372 -0.0032158103 -0.006298257][0.012523673 0.013070695 0.012945689 0.012425937 0.011743322 0.010981772 0.010215923 0.0094227754 0.0083294772 0.0067445487 0.0042434558 0.0014974028 -0.0016125552 -0.0045927968 -0.0070170313]]...]
INFO - root - 2017-12-06 02:26:26.066194: step 1010, loss = 0.87, batch loss = 0.66 (17.5 examples/sec; 0.458 sec/batch; 42h:08m:01s remains)
INFO - root - 2017-12-06 02:26:30.616318: step 1020, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.448 sec/batch; 41h:13m:19s remains)
INFO - root - 2017-12-06 02:26:34.976312: step 1030, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.462 sec/batch; 42h:31m:45s remains)
INFO - root - 2017-12-06 02:26:39.481335: step 1040, loss = 0.90, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 40h:14m:55s remains)
INFO - root - 2017-12-06 02:26:43.936102: step 1050, loss = 0.89, batch loss = 0.68 (18.1 examples/sec; 0.441 sec/batch; 40h:36m:34s remains)
INFO - root - 2017-12-06 02:26:48.567484: step 1060, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.444 sec/batch; 40h:53m:16s remains)
INFO - root - 2017-12-06 02:26:53.088917: step 1070, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.446 sec/batch; 41h:05m:59s remains)
INFO - root - 2017-12-06 02:26:57.540718: step 1080, loss = 0.89, batch loss = 0.68 (18.1 examples/sec; 0.441 sec/batch; 40h:35m:50s remains)
INFO - root - 2017-12-06 02:27:02.055058: step 1090, loss = 0.88, batch loss = 0.67 (17.5 examples/sec; 0.456 sec/batch; 41h:59m:59s remains)
INFO - root - 2017-12-06 02:27:06.495388: step 1100, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.448 sec/batch; 41h:14m:48s remains)
2017-12-06 02:27:06.945910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010237666 -0.0099567287 -0.00995218 -0.0099643264 -0.0099878255 -0.010022672 -0.010066474 -0.010119081 -0.010181803 -0.010262802 -0.010343853 -0.010416176 -0.010489339 -0.010552512 -0.010584058][-0.010149514 -0.0098475572 -0.0098490026 -0.0098671876 -0.0098984353 -0.0099415015 -0.01000029 -0.010076659 -0.010179874 -0.010319944 -0.010476846 -0.010631796 -0.010778513 -0.010893991 -0.01094725][-0.010314224 -0.010028832 -0.010047089 -0.010081002 -0.010127164 -0.010189801 -0.010279862 -0.010393308 -0.010551568 -0.010765417 -0.011008402 -0.011252949 -0.01147812 -0.011645151 -0.011715967][-0.010762064 -0.010487068 -0.010508936 -0.010541108 -0.01058564 -0.010656863 -0.010777883 -0.010932775 -0.011150055 -0.011438999 -0.011775145 -0.012106778 -0.012380265 -0.012570627 -0.012632485][-0.011480924 -0.011196313 -0.01119528 -0.011190744 -0.011195175 -0.011241723 -0.011364682 -0.011530628 -0.011789003 -0.012154153 -0.01259926 -0.013031004 -0.013353709 -0.013546094 -0.013559628][-0.012295984 -0.011950959 -0.011897545 -0.011820311 -0.011749076 -0.011739805 -0.011852356 -0.012029782 -0.012322167 -0.012771744 -0.013327153 -0.013884693 -0.014281452 -0.014485788 -0.014432808][-0.013065031 -0.01265798 -0.012540376 -0.012372242 -0.012202265 -0.012120485 -0.012197929 -0.012368761 -0.012699991 -0.013214089 -0.013859883 -0.014526119 -0.015009696 -0.015218919 -0.015117662][-0.013590882 -0.013184018 -0.013022529 -0.01279619 -0.012556052 -0.012415688 -0.012439525 -0.012567693 -0.012875983 -0.013403386 -0.014101312 -0.014817266 -0.015308281 -0.015496248 -0.015349999][-0.01384785 -0.013443248 -0.0132764 -0.013055498 -0.012801817 -0.012614137 -0.012572832 -0.012639394 -0.012875566 -0.013317263 -0.013969816 -0.014647894 -0.015090521 -0.015240202 -0.01506618][-0.013836591 -0.013500046 -0.01337588 -0.013201592 -0.012986239 -0.012764284 -0.012652511 -0.012652319 -0.012817752 -0.013162591 -0.01371241 -0.014311109 -0.014662022 -0.014716879 -0.014502183][-0.013560394 -0.013354933 -0.013337361 -0.013257524 -0.013113795 -0.012914227 -0.012761444 -0.012715288 -0.012770901 -0.012976453 -0.013369238 -0.013827723 -0.014072578 -0.014044797 -0.01380069][-0.013282726 -0.013170315 -0.013241207 -0.013254445 -0.013173094 -0.012988932 -0.012799861 -0.012696065 -0.012657728 -0.01270568 -0.01291997 -0.013210822 -0.013328722 -0.013256073 -0.013069734][-0.013017213 -0.012916736 -0.013060512 -0.0131482 -0.013159901 -0.013041131 -0.012878746 -0.012735872 -0.012618015 -0.012549009 -0.012586243 -0.012688302 -0.012685647 -0.012596533 -0.012479074][-0.012800599 -0.012697015 -0.012874285 -0.013005599 -0.013080586 -0.013039211 -0.012912678 -0.012774244 -0.01262782 -0.012517378 -0.012443986 -0.012396248 -0.012315765 -0.012220563 -0.012177203][-0.012620205 -0.012485424 -0.012647862 -0.012796234 -0.012915203 -0.012929628 -0.012856759 -0.012742415 -0.012617638 -0.012493443 -0.012378339 -0.012263134 -0.012153918 -0.012081442 -0.012072466]]...]
INFO - root - 2017-12-06 02:27:11.529737: step 1110, loss = 0.89, batch loss = 0.68 (18.1 examples/sec; 0.442 sec/batch; 40h:40m:05s remains)
INFO - root - 2017-12-06 02:27:16.043380: step 1120, loss = 0.88, batch loss = 0.68 (17.6 examples/sec; 0.454 sec/batch; 41h:46m:35s remains)
INFO - root - 2017-12-06 02:27:20.394285: step 1130, loss = 0.90, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:49m:23s remains)
INFO - root - 2017-12-06 02:27:24.931047: step 1140, loss = 0.88, batch loss = 0.67 (17.0 examples/sec; 0.472 sec/batch; 43h:26m:23s remains)
INFO - root - 2017-12-06 02:27:29.420171: step 1150, loss = 0.87, batch loss = 0.67 (18.0 examples/sec; 0.444 sec/batch; 40h:51m:01s remains)
INFO - root - 2017-12-06 02:27:33.946241: step 1160, loss = 0.87, batch loss = 0.67 (18.5 examples/sec; 0.433 sec/batch; 39h:51m:42s remains)
INFO - root - 2017-12-06 02:27:38.460408: step 1170, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.459 sec/batch; 42h:14m:16s remains)
INFO - root - 2017-12-06 02:27:42.965265: step 1180, loss = 0.85, batch loss = 0.64 (18.2 examples/sec; 0.440 sec/batch; 40h:31m:12s remains)
INFO - root - 2017-12-06 02:27:47.433570: step 1190, loss = 0.88, batch loss = 0.67 (17.6 examples/sec; 0.455 sec/batch; 41h:53m:19s remains)
INFO - root - 2017-12-06 02:27:51.966511: step 1200, loss = 0.89, batch loss = 0.68 (17.0 examples/sec; 0.472 sec/batch; 43h:25m:14s remains)
2017-12-06 02:27:52.415996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.15209635 -0.16707391 -0.18277392 -0.19932604 -0.21342914 -0.22238706 -0.22193414 -0.20958923 -0.18094224 -0.13558704 -0.088255852 -0.049612034 -0.023796678 -0.010512924 -0.0023136809][-0.065022886 -0.087823063 -0.11523089 -0.14733747 -0.1814291 -0.21529055 -0.24418999 -0.26323578 -0.26384965 -0.23498434 -0.17927007 -0.11405009 -0.059434906 -0.026194012 -0.0066330489][0.085041277 0.065290436 0.036276206 -0.00055224448 -0.042658631 -0.0905927 -0.14435804 -0.20191039 -0.25004798 -0.26950246 -0.24607621 -0.18843096 -0.11813637 -0.058474906 -0.018386485][0.20630564 0.19176953 0.16754471 0.1410338 0.10992953 0.069040664 0.015719824 -0.055482529 -0.13711149 -0.20508677 -0.23564593 -0.21756057 -0.16484533 -0.098924 -0.041497104][0.23158179 0.24910153 0.25442916 0.24804728 0.23548137 0.21279046 0.17403385 0.10998312 0.021440573 -0.075334623 -0.15511584 -0.19030221 -0.17719133 -0.13278165 -0.077983841][0.15857238 0.19996993 0.24406998 0.2749899 0.29664534 0.30766326 0.29403633 0.24878781 0.1647535 0.058768459 -0.046878796 -0.12508507 -0.15374416 -0.14063522 -0.1028432][0.049940996 0.083109364 0.13273504 0.1913078 0.2563557 0.3202672 0.36092418 0.35309416 0.28619969 0.1824843 0.06640733 -0.038360246 -0.10131188 -0.11733368 -0.099280968][-0.0055810045 0.0054138862 0.029237993 0.064253531 0.12312803 0.20413592 0.29505891 0.35154074 0.34223568 0.27500165 0.17145811 0.062899292 -0.021243069 -0.063007116 -0.065541163][-0.0099908561 -0.010800097 -0.0048314165 0.0054868609 0.03159114 0.082539581 0.16100672 0.24428184 0.30147284 0.29811484 0.24048798 0.15046477 0.056337357 -0.0073939878 -0.034173865][0.0094963051 0.005044356 0.0011360645 -0.0049215145 -0.0046977289 0.01085962 0.054672651 0.12117559 0.19149664 0.2293293 0.22316442 0.17423698 0.098767042 0.032736644 -0.0070770737][0.01376649 0.014020469 0.01238915 0.0062787384 -0.0019825622 -0.0080045592 -0.00083614513 0.025888868 0.070398666 0.11179928 0.13602793 0.13150732 0.097909927 0.056777209 0.02333777][0.0048932955 0.0054153614 0.0069163255 0.00552883 0.0017532855 -0.0064282473 -0.013045534 -0.014440963 -0.0062647816 0.013362341 0.033652008 0.042601988 0.038147375 0.028947115 0.014644369][-0.0054991636 -0.005540926 -0.0043318085 -0.0041262954 -0.0011363775 -0.0029152185 -0.0078745 -0.014342692 -0.022766639 -0.023450714 -0.019789994 -0.01480482 -0.01068962 -0.0087248329 -0.0084194448][-0.01682537 -0.016098674 -0.014615417 -0.013691379 -0.0083918143 -0.0028935336 0.003310889 0.0051329546 -0.00042105839 -0.00570588 -0.0123963 -0.017182702 -0.019876868 -0.020196173 -0.018511159][-0.022110317 -0.02143229 -0.02086102 -0.020227809 -0.0167164 -0.010737848 -0.00095963106 0.0060982332 0.0093010068 0.0095441788 0.0053369887 -0.00065879524 -0.0075095128 -0.011285167 -0.013712104]]...]
INFO - root - 2017-12-06 02:27:56.905142: step 1210, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.456 sec/batch; 41h:59m:09s remains)
INFO - root - 2017-12-06 02:28:01.512418: step 1220, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.456 sec/batch; 41h:59m:36s remains)
INFO - root - 2017-12-06 02:28:06.000494: step 1230, loss = 0.87, batch loss = 0.66 (17.6 examples/sec; 0.454 sec/batch; 41h:47m:41s remains)
INFO - root - 2017-12-06 02:28:10.304890: step 1240, loss = 0.86, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 41h:19m:00s remains)
INFO - root - 2017-12-06 02:28:14.771642: step 1250, loss = 0.85, batch loss = 0.64 (17.6 examples/sec; 0.455 sec/batch; 41h:51m:31s remains)
INFO - root - 2017-12-06 02:28:19.240120: step 1260, loss = 0.86, batch loss = 0.65 (17.8 examples/sec; 0.449 sec/batch; 41h:20m:20s remains)
INFO - root - 2017-12-06 02:28:23.719572: step 1270, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.448 sec/batch; 41h:11m:31s remains)
INFO - root - 2017-12-06 02:28:28.280407: step 1280, loss = 0.90, batch loss = 0.69 (17.9 examples/sec; 0.446 sec/batch; 41h:02m:53s remains)
INFO - root - 2017-12-06 02:28:32.764294: step 1290, loss = 0.88, batch loss = 0.67 (18.6 examples/sec; 0.431 sec/batch; 39h:38m:28s remains)
INFO - root - 2017-12-06 02:28:37.288407: step 1300, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 41h:34m:50s remains)
2017-12-06 02:28:37.797089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0066727605 -0.0046364032 -0.0027172305 -0.00089032948 0.00071464851 0.0021250769 0.0030044056 0.0025879815 0.00068377331 -0.0021671914 -0.0053840578 -0.0082478952 -0.010360882 -0.011519952 -0.012140565][-0.0053215288 -0.0032028854 -0.00131087 0.00034891814 0.0017017499 0.0028524175 0.0035344921 0.0029553585 0.00091335922 -0.0020302013 -0.0053236596 -0.0082587954 -0.010433285 -0.011620037 -0.012254039][-0.0040429682 -0.0017990209 6.5367669e-05 0.0015293807 0.0026158914 0.0035256594 0.004072547 0.0034190342 0.0013194084 -0.0016950853 -0.0050830506 -0.0081244856 -0.010386672 -0.011604922 -0.012257496][-0.0029538572 -0.0005748719 0.0012620836 0.0025423057 0.0033896491 0.0041348785 0.0046276934 0.0039759204 0.0018570013 -0.0012538247 -0.0047900118 -0.0079839062 -0.010349685 -0.011595167 -0.012259424][-0.0021947026 0.00037011877 0.0022145249 0.0033845492 0.0040845275 0.0047863685 0.0053256229 0.004725568 0.0025835633 -0.00067208707 -0.0044224486 -0.007803848 -0.010300247 -0.011584122 -0.012264285][-0.0018386692 0.00095627084 0.0028529242 0.0040107146 0.00468624 0.0055004358 0.0061956793 0.0056719594 0.0034759268 3.2104552e-05 -0.0039625615 -0.0075604543 -0.010195069 -0.011551067 -0.012266383][-0.0017557666 0.0012362376 0.0031913817 0.004401885 0.0052006692 0.0062748641 0.0071916804 0.0067417622 0.004474435 0.00083753094 -0.003410317 -0.0072317887 -0.010013422 -0.011473721 -0.012253458][-0.0018840432 0.0012385622 0.0032584071 0.0045712106 0.0055986866 0.0070058368 0.0081511214 0.0077292696 0.0053673647 0.0015692748 -0.0028892979 -0.0069087651 -0.0098175928 -0.011380665 -0.01222902][-0.0020185336 0.0010487027 0.003084287 0.0045139641 0.0058252253 0.0074957423 0.0087675229 0.0083417892 0.005912248 0.0020082258 -0.0025741234 -0.0067034196 -0.0096723717 -0.01130148 -0.012202412][-0.00210898 0.00076062232 0.0027321391 0.00425278 0.00578817 0.0075599961 0.0088032 0.0083519444 0.0059045106 0.0019759983 -0.0026025139 -0.0066999532 -0.0096186586 -0.011264626 -0.012188213][-0.0021799989 0.00031977147 0.0021226555 0.0036435463 0.0052410327 0.006951116 0.0080564842 0.007555794 0.0051522292 0.0013482422 -0.0030329302 -0.0069077872 -0.0096710175 -0.01127623 -0.012182683][-0.0027017929 -0.00056944042 0.0010064617 0.0024123229 0.0038924329 0.0054086484 0.0062980577 0.0057358257 0.0035220459 6.5308064e-05 -0.0038807653 -0.0073383376 -0.0098303128 -0.01132356 -0.012187272][-0.0038787536 -0.0021389835 -0.00084907189 0.00035813078 0.0015970543 0.002817966 0.0034384392 0.0028468855 0.000934463 -0.0019775815 -0.0052782856 -0.0081241205 -0.010193422 -0.011476155 -0.012236042][-0.0055513103 -0.0041981451 -0.0032264479 -0.0022827238 -0.0013301224 -0.0004556179 -8.481741e-05 -0.00066386163 -0.0021857098 -0.0044231266 -0.0069437511 -0.0091132876 -0.010686936 -0.011677381 -0.012285428][-0.0077324174 -0.0066829119 -0.0059728362 -0.0052589029 -0.0045719407 -0.0039948672 -0.0037943982 -0.0042638667 -0.0053756908 -0.0069329571 -0.0086641591 -0.010165434 -0.011238527 -0.011913791 -0.012346121]]...]
INFO - root - 2017-12-06 02:28:42.268600: step 1310, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.453 sec/batch; 41h:37m:57s remains)
INFO - root - 2017-12-06 02:28:46.864713: step 1320, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 41h:37m:24s remains)
INFO - root - 2017-12-06 02:28:51.330160: step 1330, loss = 0.90, batch loss = 0.69 (18.7 examples/sec; 0.429 sec/batch; 39h:25m:58s remains)
INFO - root - 2017-12-06 02:28:55.574616: step 1340, loss = 0.86, batch loss = 0.65 (18.2 examples/sec; 0.439 sec/batch; 40h:22m:52s remains)
INFO - root - 2017-12-06 02:29:00.171330: step 1350, loss = 0.88, batch loss = 0.67 (17.2 examples/sec; 0.466 sec/batch; 42h:50m:25s remains)
INFO - root - 2017-12-06 02:29:04.710995: step 1360, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.441 sec/batch; 40h:34m:30s remains)
INFO - root - 2017-12-06 02:29:09.245281: step 1370, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.442 sec/batch; 40h:36m:42s remains)
INFO - root - 2017-12-06 02:29:13.787907: step 1380, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.462 sec/batch; 42h:30m:34s remains)
INFO - root - 2017-12-06 02:29:18.274723: step 1390, loss = 0.85, batch loss = 0.64 (17.6 examples/sec; 0.455 sec/batch; 41h:53m:08s remains)
INFO - root - 2017-12-06 02:29:22.827944: step 1400, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.451 sec/batch; 41h:29m:59s remains)
2017-12-06 02:29:23.323941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0093541052 -0.0094827116 -0.010249071 -0.010881424 -0.011109291 -0.01107128 -0.010512572 -0.0090734325 -0.0068562496 -0.00446428 -0.0023643337 -0.00073399022 0.0001251176 0.00031191111 6.7744404e-05][-0.015400141 -0.016457766 -0.018361786 -0.020169349 -0.021571238 -0.022630472 -0.022784594 -0.021271676 -0.01829356 -0.014656767 -0.010894747 -0.0075340252 -0.0052900091 -0.0042306893 -0.0040512457][-0.020868938 -0.023485802 -0.027590772 -0.031553645 -0.034468092 -0.036472514 -0.03685971 -0.035028324 -0.031308178 -0.026735194 -0.021611091 -0.016831053 -0.013333328 -0.011134883 -0.010140991][-0.021795845 -0.025610849 -0.031330574 -0.036911957 -0.041541658 -0.045154039 -0.0466011 -0.045351431 -0.041869983 -0.036812965 -0.030629827 -0.024443079 -0.019642077 -0.016483031 -0.014828512][-0.01997021 -0.023957696 -0.030158095 -0.036709435 -0.043001663 -0.048440732 -0.05163762 -0.051708419 -0.049062178 -0.043753713 -0.036818024 -0.029227797 -0.02280454 -0.01837091 -0.01573508][-0.021125782 -0.024840647 -0.030848805 -0.037701592 -0.045003418 -0.051787764 -0.055344049 -0.055549286 -0.053177338 -0.047776222 -0.040557798 -0.032608092 -0.025332462 -0.01978212 -0.0162393][-0.024260074 -0.027167983 -0.032285351 -0.038853817 -0.046444081 -0.053386357 -0.057304569 -0.057718582 -0.055371791 -0.050268009 -0.043709684 -0.0366006 -0.030037507 -0.024514286 -0.020945393][-0.026256463 -0.028470621 -0.031881772 -0.037009269 -0.043111157 -0.0486562 -0.051802479 -0.052687775 -0.051935498 -0.049101613 -0.044717569 -0.039422669 -0.034108892 -0.029431429 -0.026354887][-0.026734628 -0.0288183 -0.0320855 -0.036276333 -0.040970467 -0.044403087 -0.045810107 -0.045861375 -0.045660164 -0.044351302 -0.041795492 -0.03788827 -0.033983454 -0.030553585 -0.027995918][-0.027486779 -0.028541712 -0.030896772 -0.034224752 -0.038161438 -0.04115624 -0.042420756 -0.042268023 -0.041313846 -0.039133534 -0.036525704 -0.033793479 -0.031408675 -0.029190276 -0.027112728][-0.028476495 -0.027803302 -0.028063994 -0.029258745 -0.031102428 -0.032685146 -0.03357821 -0.033754278 -0.033561449 -0.032330766 -0.030632762 -0.029237848 -0.028006231 -0.026594061 -0.025084078][-0.031410486 -0.029092541 -0.027264241 -0.026357077 -0.025871493 -0.025671709 -0.025511447 -0.025518449 -0.025577441 -0.025191773 -0.0243008 -0.023688518 -0.02356386 -0.023244763 -0.022763815][-0.035896987 -0.032634884 -0.029394425 -0.02681984 -0.025058217 -0.024285229 -0.024347156 -0.024661273 -0.02475341 -0.024468487 -0.023898115 -0.02362296 -0.023616523 -0.023392068 -0.023229089][-0.040999494 -0.037798103 -0.034538642 -0.031382512 -0.02904092 -0.027731691 -0.027220508 -0.02733402 -0.027705994 -0.028058928 -0.028050367 -0.027709562 -0.027287722 -0.026750278 -0.026437249][-0.04612308 -0.043080192 -0.03955584 -0.036151733 -0.033463426 -0.031806055 -0.031116826 -0.031108186 -0.031405896 -0.031761713 -0.031843796 -0.031541806 -0.031058323 -0.030534063 -0.029967029]]...]
INFO - root - 2017-12-06 02:29:27.818691: step 1410, loss = 0.86, batch loss = 0.65 (17.3 examples/sec; 0.462 sec/batch; 42h:28m:47s remains)
INFO - root - 2017-12-06 02:29:32.307018: step 1420, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 40h:55m:15s remains)
INFO - root - 2017-12-06 02:29:36.783101: step 1430, loss = 0.90, batch loss = 0.69 (16.7 examples/sec; 0.479 sec/batch; 44h:03m:42s remains)
INFO - root - 2017-12-06 02:29:41.170195: step 1440, loss = 0.89, batch loss = 0.68 (24.0 examples/sec; 0.333 sec/batch; 30h:37m:25s remains)
INFO - root - 2017-12-06 02:29:45.648749: step 1450, loss = 0.86, batch loss = 0.65 (17.5 examples/sec; 0.456 sec/batch; 41h:58m:35s remains)
INFO - root - 2017-12-06 02:29:50.181707: step 1460, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 40h:54m:31s remains)
INFO - root - 2017-12-06 02:29:54.687612: step 1470, loss = 0.87, batch loss = 0.67 (17.4 examples/sec; 0.460 sec/batch; 42h:19m:21s remains)
INFO - root - 2017-12-06 02:29:59.189798: step 1480, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.442 sec/batch; 40h:38m:17s remains)
INFO - root - 2017-12-06 02:30:03.637611: step 1490, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.438 sec/batch; 40h:17m:07s remains)
INFO - root - 2017-12-06 02:30:08.105931: step 1500, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 41h:47m:09s remains)
2017-12-06 02:30:08.561064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.087548375 -0.041936308 0.014547136 0.053758353 0.066954538 0.061261237 0.053142145 0.054783091 0.063226752 0.0678498 0.070940092 0.086310111 0.10924923 0.1355966 0.14532056][-0.091210224 -0.035303272 0.03629449 0.087910227 0.10413089 0.093686022 0.0842178 0.089220412 0.098616771 0.096631877 0.096337713 0.11631108 0.14854521 0.17645881 0.17837894][-0.066260129 0.0050474927 0.095713846 0.15829879 0.17548546 0.15894052 0.14375675 0.1423209 0.14604756 0.13956141 0.14208093 0.16834223 0.2053625 0.22936571 0.22521651][-0.043857113 0.042049997 0.14439067 0.21199375 0.22598049 0.20006558 0.17469522 0.15831184 0.14947134 0.13785577 0.14534962 0.18139315 0.22744972 0.25108722 0.24363351][-0.052313495 0.029285595 0.13200748 0.19722864 0.20316041 0.16624495 0.13071033 0.099772595 0.079485759 0.065597072 0.0808065 0.13153377 0.19130942 0.22141585 0.21858206][-0.07349436 -0.010486821 0.071463406 0.1244629 0.12723875 0.088247754 0.047945909 0.010360863 -0.012756156 -0.027818132 -0.015690923 0.032178551 0.093528055 0.13013184 0.13795412][-0.090675473 -0.055821441 -0.0014431067 0.03305655 0.032485098 0.0024635084 -0.024447016 -0.049714431 -0.0690865 -0.087187454 -0.088274404 -0.059341028 -0.016070498 0.015680648 0.031681411][-0.083542407 -0.076996788 -0.05412177 -0.038519479 -0.040967565 -0.060772032 -0.075075 -0.08572939 -0.092836872 -0.10240574 -0.10845911 -0.09975633 -0.080755368 -0.065188415 -0.055200472][-0.076733306 -0.082665011 -0.081796408 -0.080909982 -0.084431306 -0.091938615 -0.09450417 -0.096692711 -0.097362727 -0.099890694 -0.10322046 -0.10314682 -0.099903353 -0.09631902 -0.09154544][-0.052973531 -0.062050078 -0.068156227 -0.073212162 -0.077425987 -0.080492757 -0.07805483 -0.072633967 -0.070972368 -0.072550267 -0.0744074 -0.076547667 -0.079343095 -0.082033217 -0.081198178][-0.034793075 -0.04002481 -0.045959271 -0.0512701 -0.05558401 -0.061197218 -0.058196515 -0.051443562 -0.049429141 -0.049180169 -0.048485458 -0.049045376 -0.052326661 -0.057027616 -0.059143968][-0.026755666 -0.026347067 -0.027321532 -0.030794527 -0.035917081 -0.04028327 -0.0417699 -0.039665915 -0.0382929 -0.036625963 -0.033804223 -0.032312766 -0.034743972 -0.0391585 -0.041367423][-0.028064366 -0.025010578 -0.024138564 -0.023898568 -0.026954755 -0.03299915 -0.036286317 -0.034333166 -0.032278512 -0.030393941 -0.028852953 -0.029348373 -0.030217804 -0.033462562 -0.035179716][-0.029931627 -0.029002082 -0.026801787 -0.024359599 -0.025019951 -0.026016336 -0.028470784 -0.027405582 -0.028381413 -0.025865927 -0.023369184 -0.0237702 -0.02723252 -0.030474497 -0.032508656][-0.030283242 -0.031414948 -0.030473258 -0.028099183 -0.024192885 -0.02551116 -0.024610016 -0.021078298 -0.021918226 -0.018535288 -0.017527709 -0.019454202 -0.021589078 -0.024996147 -0.029193377]]...]
INFO - root - 2017-12-06 02:30:13.132466: step 1510, loss = 0.90, batch loss = 0.69 (17.0 examples/sec; 0.470 sec/batch; 43h:12m:48s remains)
INFO - root - 2017-12-06 02:30:17.648318: step 1520, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 41h:23m:56s remains)
INFO - root - 2017-12-06 02:30:22.119389: step 1530, loss = 0.88, batch loss = 0.67 (17.0 examples/sec; 0.472 sec/batch; 43h:21m:15s remains)
INFO - root - 2017-12-06 02:30:26.746370: step 1540, loss = 0.89, batch loss = 0.68 (16.3 examples/sec; 0.492 sec/batch; 45h:14m:01s remains)
INFO - root - 2017-12-06 02:30:31.023228: step 1550, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.437 sec/batch; 40h:13m:00s remains)
INFO - root - 2017-12-06 02:30:35.499948: step 1560, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.453 sec/batch; 41h:38m:44s remains)
INFO - root - 2017-12-06 02:30:39.971449: step 1570, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.444 sec/batch; 40h:46m:37s remains)
INFO - root - 2017-12-06 02:30:44.503870: step 1580, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.463 sec/batch; 42h:31m:44s remains)
INFO - root - 2017-12-06 02:30:48.991853: step 1590, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.460 sec/batch; 42h:15m:00s remains)
INFO - root - 2017-12-06 02:30:53.543864: step 1600, loss = 0.88, batch loss = 0.67 (16.6 examples/sec; 0.483 sec/batch; 44h:23m:51s remains)
2017-12-06 02:30:53.984415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030816369 -0.030396022 -0.031339381 -0.03333601 -0.036830932 -0.04245545 -0.049613688 -0.053626209 -0.051337555 -0.046703532 -0.045097057 -0.042262573 -0.038912475 -0.035631809 -0.032942723][-0.030661315 -0.030105622 -0.031052865 -0.035431959 -0.044713177 -0.055895276 -0.072672054 -0.0881166 -0.093770325 -0.088630989 -0.076574773 -0.061719611 -0.049599309 -0.041405007 -0.036140691][-0.030313151 -0.029855259 -0.030911233 -0.036796294 -0.05053141 -0.07267157 -0.10053026 -0.12796065 -0.14378971 -0.141539 -0.12376175 -0.097345635 -0.0711873 -0.052166175 -0.040839009][-0.029558301 -0.029007971 -0.029885743 -0.034344688 -0.041811332 -0.0562344 -0.08509478 -0.12587067 -0.16338971 -0.18053479 -0.17105003 -0.14024858 -0.10106175 -0.068178385 -0.047737796][-0.029891854 -0.028485194 -0.027748778 -0.023069251 -0.0013116412 0.030607261 0.039622515 0.0069410056 -0.058023043 -0.12411183 -0.16320328 -0.16156478 -0.1276709 -0.08648403 -0.057259977][-0.031430312 -0.029340815 -0.026636936 -0.0088062547 0.050714694 0.14350259 0.22054239 0.23017786 0.16341347 0.0529754 -0.0565417 -0.11856818 -0.12041274 -0.088403732 -0.056334544][-0.033360984 -0.030552685 -0.026022056 -0.0025552772 0.071098961 0.19554788 0.32253861 0.377939 0.33617646 0.21302603 0.059794173 -0.053973004 -0.094831936 -0.080743626 -0.052324828][-0.033307593 -0.031455196 -0.026862502 -0.0075606816 0.051744118 0.16244178 0.29516989 0.37501603 0.35280347 0.24239014 0.09452711 -0.022710063 -0.071026072 -0.067425214 -0.045772243][-0.028457243 -0.028158788 -0.026552381 -0.014967099 0.022233233 0.0887485 0.17011766 0.22981094 0.22713374 0.15466018 0.048504308 -0.03435846 -0.06387955 -0.056149483 -0.038966291][-0.026373781 -0.024174212 -0.024214908 -0.021964669 -0.007334169 0.023203716 0.059118457 0.082705133 0.076279394 0.039586768 -0.011483781 -0.04454384 -0.047040995 -0.03851255 -0.03214049][-0.029310854 -0.026311403 -0.02563696 -0.02506157 -0.022658078 -0.014193354 -0.0022284128 0.00416613 -0.0037308186 -0.022148261 -0.04041598 -0.044144154 -0.03453812 -0.027263625 -0.025158484][-0.029901592 -0.02677723 -0.025548488 -0.024905706 -0.026629534 -0.028127689 -0.028099254 -0.029711312 -0.035558946 -0.042770982 -0.044447683 -0.038997896 -0.031071991 -0.026529182 -0.02550526][-0.030900298 -0.027643833 -0.025972085 -0.024683665 -0.024265015 -0.023535684 -0.020635203 -0.021743121 -0.027059432 -0.033221111 -0.035297532 -0.03291579 -0.030157115 -0.029439747 -0.029196389][-0.03231246 -0.029194076 -0.027662233 -0.025935207 -0.023384606 -0.022546263 -0.01634144 -0.013138445 -0.016675711 -0.023023428 -0.028072285 -0.029304128 -0.028842486 -0.029175427 -0.029491525][-0.033517063 -0.030664768 -0.02942631 -0.028299022 -0.025707742 -0.022896696 -0.020093491 -0.017274942 -0.018416716 -0.022238374 -0.025857791 -0.027718382 -0.027867727 -0.028349064 -0.028866049]]...]
INFO - root - 2017-12-06 02:30:58.534048: step 1610, loss = 0.86, batch loss = 0.65 (17.5 examples/sec; 0.458 sec/batch; 42h:07m:58s remains)
INFO - root - 2017-12-06 02:31:02.996776: step 1620, loss = 0.87, batch loss = 0.66 (17.4 examples/sec; 0.460 sec/batch; 42h:19m:13s remains)
INFO - root - 2017-12-06 02:31:07.475155: step 1630, loss = 0.85, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 41h:58m:44s remains)
INFO - root - 2017-12-06 02:31:12.026563: step 1640, loss = 0.88, batch loss = 0.67 (18.4 examples/sec; 0.434 sec/batch; 39h:51m:02s remains)
INFO - root - 2017-12-06 02:31:16.439426: step 1650, loss = 0.89, batch loss = 0.68 (17.2 examples/sec; 0.465 sec/batch; 42h:46m:07s remains)
INFO - root - 2017-12-06 02:31:20.906095: step 1660, loss = 0.89, batch loss = 0.68 (18.2 examples/sec; 0.441 sec/batch; 40h:29m:34s remains)
INFO - root - 2017-12-06 02:31:25.369122: step 1670, loss = 0.83, batch loss = 0.62 (17.8 examples/sec; 0.448 sec/batch; 41h:12m:12s remains)
INFO - root - 2017-12-06 02:31:29.897181: step 1680, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.464 sec/batch; 42h:36m:36s remains)
INFO - root - 2017-12-06 02:31:34.398127: step 1690, loss = 0.86, batch loss = 0.65 (18.0 examples/sec; 0.444 sec/batch; 40h:46m:11s remains)
INFO - root - 2017-12-06 02:31:38.915032: step 1700, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.438 sec/batch; 40h:13m:41s remains)
2017-12-06 02:31:39.373611: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54302442 0.33299085 0.15215804 0.046177167 -0.049668871 -0.047707718 -0.054777052 -0.050707452 -0.043926869 -0.065807432 -0.11014749 -0.18063512 -0.30971017 -0.37149742 -0.44450787][0.037613515 -0.23949969 -0.38219315 -0.45177412 -0.45943087 -0.33697608 -0.20661421 -0.084306493 -0.01445011 -0.0044096932 -0.049226388 -0.14330353 -0.28236619 -0.36166415 -0.44696218][-0.36584857 -0.56023747 -0.6352942 -0.59440321 -0.47615653 -0.25788298 -0.052050885 0.12595412 0.22994816 0.2395446 0.17072915 0.062480491 -0.077807136 -0.1715735 -0.26204515][-0.65074587 -0.75260127 -0.69763684 -0.53005975 -0.29160306 -0.023179157 0.20296398 0.36970279 0.46878102 0.45708954 0.37874147 0.24790809 0.090628192 -0.019769127 -0.080503859][-0.73383576 -0.74857891 -0.61346108 -0.3641336 -0.083009757 0.1862811 0.3987321 0.52127129 0.59729373 0.5872702 0.51531547 0.38591614 0.21955505 0.06247225 0.032743063][-0.61749005 -0.5627476 -0.36664405 -0.0698479 0.2294533 0.4787502 0.66726577 0.77623808 0.8194809 0.76608622 0.63151103 0.44829467 0.23348895 0.054098915 0.06923531][-0.43900418 -0.30046231 -0.0402973 0.28499004 0.57749516 0.79576981 0.94365162 1.0096767 0.98911083 0.85792333 0.64633918 0.39348638 0.1421157 -0.04196411 -0.003561046][-0.16728966 0.016529091 0.29824778 0.61448169 0.86738431 1.0203508 1.0893922 1.071491 0.97357905 0.76646787 0.51489627 0.24195543 0.010200139 -0.12393197 -0.067367539][-0.010026727 0.18798912 0.47464183 0.75067729 0.93580389 1.016822 1.0023357 0.89268 0.72148407 0.4996936 0.27328929 0.028310429 -0.12782273 -0.19549936 -0.10380655][0.088721544 0.25191188 0.48177382 0.68426108 0.79836881 0.80436289 0.7165224 0.558721 0.37752688 0.20015176 0.064169824 -0.081576705 -0.13620129 -0.15567495 -0.063381694][0.045185518 0.14849965 0.27736822 0.38545874 0.42725703 0.39059454 0.28450871 0.14656687 0.026444513 -0.077280149 -0.12114199 -0.17139383 -0.13830896 -0.12431474 -0.06626153][-0.036310721 -0.0019545332 0.042392086 0.08014226 0.081084564 0.036068056 -0.033360634 -0.10313614 -0.14474235 -0.17181937 -0.1494724 -0.13749279 -0.081733227 -0.0719532 -0.053414576][-0.079590455 -0.087602131 -0.094878212 -0.10143371 -0.11911003 -0.15512288 -0.18887512 -0.21129908 -0.21173806 -0.19970551 -0.16560057 -0.12902959 -0.08149194 -0.070562817 -0.040018626][-0.078843445 -0.09379217 -0.10776508 -0.12338363 -0.13754556 -0.16107734 -0.17750476 -0.18123195 -0.17819275 -0.16553715 -0.14252479 -0.11762831 -0.096204378 -0.082009 -0.055128343][-0.062456869 -0.068935864 -0.075118065 -0.0833193 -0.091868684 -0.10766973 -0.11526601 -0.11557777 -0.11162199 -0.101076 -0.093506962 -0.086844638 -0.082191892 -0.070054859 -0.05147776]]...]
INFO - root - 2017-12-06 02:31:43.891931: step 1710, loss = 0.90, batch loss = 0.69 (17.3 examples/sec; 0.462 sec/batch; 42h:28m:03s remains)
INFO - root - 2017-12-06 02:31:48.334102: step 1720, loss = 0.88, batch loss = 0.67 (18.8 examples/sec; 0.426 sec/batch; 39h:07m:12s remains)
INFO - root - 2017-12-06 02:31:52.875522: step 1730, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 40h:50m:50s remains)
INFO - root - 2017-12-06 02:31:57.408448: step 1740, loss = 0.86, batch loss = 0.65 (17.0 examples/sec; 0.471 sec/batch; 43h:14m:37s remains)
INFO - root - 2017-12-06 02:32:01.925192: step 1750, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.463 sec/batch; 42h:33m:16s remains)
INFO - root - 2017-12-06 02:32:06.279031: step 1760, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 40h:54m:18s remains)
INFO - root - 2017-12-06 02:32:10.747035: step 1770, loss = 0.86, batch loss = 0.65 (18.2 examples/sec; 0.440 sec/batch; 40h:25m:20s remains)
INFO - root - 2017-12-06 02:32:15.368861: step 1780, loss = 0.90, batch loss = 0.69 (17.8 examples/sec; 0.449 sec/batch; 41h:14m:27s remains)
INFO - root - 2017-12-06 02:32:19.900122: step 1790, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.444 sec/batch; 40h:48m:29s remains)
INFO - root - 2017-12-06 02:32:24.380527: step 1800, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.448 sec/batch; 41h:10m:43s remains)
2017-12-06 02:32:24.842757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10537404 -0.10491589 -0.125254 -0.1590731 -0.15492895 -0.13841549 -0.12836906 -0.1300846 -0.15149063 -0.19778454 -0.26104969 -0.30100718 -0.34667766 -0.39521003 -0.43814576][-0.089230865 -0.10247907 -0.12163915 -0.16484782 -0.15486079 -0.12591679 -0.080860317 -0.05812566 -0.074717805 -0.12356691 -0.1930522 -0.27092472 -0.3324118 -0.38548583 -0.45550188][-0.1374394 -0.15981433 -0.19617265 -0.22669175 -0.19708899 -0.15237738 -0.10155915 -0.068493918 -0.07079155 -0.12193283 -0.20423257 -0.27773267 -0.37024149 -0.44896874 -0.51689655][-0.13358724 -0.18140608 -0.22500205 -0.24921286 -0.21457055 -0.16073906 -0.11138841 -0.085653462 -0.09807694 -0.14947574 -0.22424114 -0.29773229 -0.391164 -0.47715119 -0.54533756][-0.21325076 -0.27647939 -0.29820663 -0.29666618 -0.24310061 -0.1771256 -0.12142397 -0.099776089 -0.11480751 -0.16071075 -0.22701341 -0.2995466 -0.37606353 -0.45776761 -0.52583182][-0.261408 -0.32393235 -0.3440347 -0.32860959 -0.25882021 -0.18471497 -0.12832519 -0.1093507 -0.12621441 -0.17142013 -0.22847939 -0.29949841 -0.40335748 -0.45654911 -0.50404322][-0.2778489 -0.33983332 -0.37613225 -0.35196441 -0.27719504 -0.19357228 -0.13162452 -0.11550133 -0.13851045 -0.18633834 -0.2495901 -0.32078847 -0.4177731 -0.46664655 -0.47719586][-0.27226955 -0.34015787 -0.38966703 -0.36253229 -0.29131916 -0.19370905 -0.1179513 -0.099461749 -0.12018576 -0.16314754 -0.22176588 -0.2930806 -0.36795521 -0.43649539 -0.4553206][-0.2928248 -0.35830382 -0.40679362 -0.38425186 -0.32058847 -0.22635686 -0.14991805 -0.12612495 -0.14683467 -0.19431573 -0.25940689 -0.31679794 -0.3807874 -0.44628 -0.46463886][-0.29763418 -0.35045198 -0.41052029 -0.40223545 -0.34606212 -0.26288924 -0.18959856 -0.16463643 -0.17567316 -0.21095592 -0.27104566 -0.31547633 -0.38403922 -0.45530474 -0.44088873][-0.25602087 -0.3132062 -0.37639442 -0.37852758 -0.32605496 -0.25924236 -0.20363453 -0.17943296 -0.18707001 -0.22073296 -0.27806774 -0.32433596 -0.36225331 -0.43185756 -0.44902605][-0.24188375 -0.29521206 -0.3523736 -0.36106771 -0.30683854 -0.24414983 -0.19818777 -0.180273 -0.18625644 -0.21055821 -0.25801474 -0.29652146 -0.3246398 -0.39917424 -0.4500033][-0.23064461 -0.25066528 -0.28901112 -0.30079678 -0.25000194 -0.21467194 -0.19497651 -0.19306988 -0.2093991 -0.24461246 -0.29432055 -0.31447297 -0.34599012 -0.4009468 -0.44775206][-0.14713733 -0.15375696 -0.18869466 -0.21929011 -0.18593511 -0.16465484 -0.15462561 -0.16797313 -0.19235983 -0.22259054 -0.26100346 -0.28459045 -0.32082459 -0.35237196 -0.3986339][-0.16709816 -0.1746701 -0.20034847 -0.22662619 -0.19702721 -0.17330694 -0.15689385 -0.16748857 -0.20412683 -0.23753685 -0.276003 -0.298199 -0.32139218 -0.34172714 -0.37544581]]...]
INFO - root - 2017-12-06 02:32:29.365589: step 1810, loss = 0.87, batch loss = 0.67 (17.9 examples/sec; 0.448 sec/batch; 41h:10m:03s remains)
INFO - root - 2017-12-06 02:32:33.864945: step 1820, loss = 0.87, batch loss = 0.66 (17.4 examples/sec; 0.460 sec/batch; 42h:16m:51s remains)
INFO - root - 2017-12-06 02:32:38.444640: step 1830, loss = 0.89, batch loss = 0.68 (16.6 examples/sec; 0.482 sec/batch; 44h:14m:14s remains)
INFO - root - 2017-12-06 02:32:43.032105: step 1840, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.453 sec/batch; 41h:36m:05s remains)
INFO - root - 2017-12-06 02:32:47.558522: step 1850, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.439 sec/batch; 40h:21m:29s remains)
INFO - root - 2017-12-06 02:32:51.834117: step 1860, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.451 sec/batch; 41h:25m:43s remains)
INFO - root - 2017-12-06 02:32:56.291797: step 1870, loss = 0.83, batch loss = 0.62 (18.3 examples/sec; 0.436 sec/batch; 40h:03m:30s remains)
INFO - root - 2017-12-06 02:33:00.802528: step 1880, loss = 0.89, batch loss = 0.68 (17.2 examples/sec; 0.466 sec/batch; 42h:45m:37s remains)
INFO - root - 2017-12-06 02:33:05.348207: step 1890, loss = 0.92, batch loss = 0.71 (17.5 examples/sec; 0.457 sec/batch; 41h:55m:31s remains)
INFO - root - 2017-12-06 02:33:09.844736: step 1900, loss = 0.86, batch loss = 0.65 (16.8 examples/sec; 0.476 sec/batch; 43h:40m:08s remains)
2017-12-06 02:33:10.291517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11570816 -0.13527879 -0.13745493 -0.1343424 -0.12979722 -0.12881517 -0.13105744 -0.14022782 -0.15441605 -0.17016119 -0.18475226 -0.19451874 -0.19824138 -0.19216165 -0.1732049][-0.090950742 -0.10709905 -0.10922052 -0.10672855 -0.097105145 -0.088082775 -0.083214134 -0.091080636 -0.11214568 -0.13803095 -0.16036478 -0.17420617 -0.18240067 -0.18122253 -0.16813231][-0.051571064 -0.061505385 -0.064380869 -0.067142889 -0.057419084 -0.043850895 -0.035789195 -0.044455279 -0.071332261 -0.10327481 -0.1296832 -0.14629969 -0.15533321 -0.15558448 -0.14462292][-0.018889638 -0.024121335 -0.02782863 -0.031052712 -0.02223677 -0.010959215 -0.0019654147 -0.012118235 -0.041862085 -0.076433465 -0.1049903 -0.12325802 -0.13255417 -0.12787834 -0.11335719][0.00054920465 0.0003301762 -0.0011600852 -0.0041712113 0.00056129321 0.013442941 0.023410253 0.012762584 -0.010954082 -0.039621431 -0.067707755 -0.091438435 -0.10725565 -0.1046377 -0.090978][0.012926418 0.014502328 0.017805077 0.022538647 0.030932799 0.03743048 0.038608104 0.032586738 0.023779504 0.010691769 -0.013037059 -0.044510543 -0.069537051 -0.074127123 -0.062701508][0.01065721 0.013767913 0.021945931 0.033110194 0.044972815 0.052678555 0.053001456 0.048099697 0.04744155 0.048176751 0.038824193 0.014488928 -0.010820393 -0.0210486 -0.011063591][0.0039678328 0.0047731698 0.012401029 0.025757305 0.037464291 0.042981185 0.04284545 0.043320715 0.052147351 0.060484707 0.057049111 0.039849102 0.021486841 0.016402122 0.024248056][-0.0093464144 -0.007496696 -0.0023274049 0.0075265169 0.015286956 0.018967416 0.02213499 0.027669542 0.040452018 0.047810204 0.046442032 0.036282666 0.02440612 0.02259624 0.030961275][-0.018542958 -0.016994035 -0.014152708 -0.0083838515 -0.0041282438 -0.0016003288 0.0020692647 0.0086639337 0.020622827 0.027621664 0.028170377 0.020805366 0.013256129 0.013158806 0.01873317][-0.022806307 -0.0207099 -0.017508451 -0.013814103 -0.011744985 -0.01022999 -0.0080567636 -0.0019835979 0.007014975 0.013264466 0.013736244 0.0068620741 -0.00065131485 -0.003505379 -0.0027068853][-0.024023365 -0.022692055 -0.020922787 -0.019744972 -0.017896539 -0.01751429 -0.017810209 -0.015239349 -0.010893434 -0.006412562 -0.0035741888 -0.0038486719 -0.0054230392 -0.0076042712 -0.0089686662][-0.021931278 -0.021407198 -0.020366952 -0.019711751 -0.017893229 -0.016174573 -0.01611341 -0.016932245 -0.016506441 -0.015597893 -0.013226748 -0.013158338 -0.013672238 -0.015114918 -0.015591919][-0.020697745 -0.020559935 -0.020492287 -0.019615609 -0.017871952 -0.016445905 -0.018190289 -0.013193155 -0.014498804 -0.016419649 -0.012925714 -0.011875257 -0.012097215 -0.014992764 -0.016744725][-0.01876167 -0.018658727 -0.018782567 -0.018804483 -0.018561715 -0.019853825 -0.018562999 -0.013889631 -0.011832386 -0.012823697 -0.00854354 -0.0049868003 -0.0057552233 -0.010500494 -0.014068399]]...]
INFO - root - 2017-12-06 02:33:14.749845: step 1910, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.445 sec/batch; 40h:51m:05s remains)
INFO - root - 2017-12-06 02:33:19.228851: step 1920, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.448 sec/batch; 41h:09m:13s remains)
INFO - root - 2017-12-06 02:33:23.744603: step 1930, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.462 sec/batch; 42h:23m:17s remains)
INFO - root - 2017-12-06 02:33:28.317224: step 1940, loss = 0.84, batch loss = 0.63 (17.1 examples/sec; 0.468 sec/batch; 42h:58m:14s remains)
INFO - root - 2017-12-06 02:33:32.884634: step 1950, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.452 sec/batch; 41h:28m:28s remains)
INFO - root - 2017-12-06 02:33:37.267221: step 1960, loss = 0.87, batch loss = 0.66 (27.0 examples/sec; 0.296 sec/batch; 27h:11m:45s remains)
INFO - root - 2017-12-06 02:33:41.780128: step 1970, loss = 0.88, batch loss = 0.67 (16.8 examples/sec; 0.477 sec/batch; 43h:48m:11s remains)
INFO - root - 2017-12-06 02:33:46.333893: step 1980, loss = 0.88, batch loss = 0.67 (18.4 examples/sec; 0.434 sec/batch; 39h:50m:33s remains)
INFO - root - 2017-12-06 02:33:50.854986: step 1990, loss = 0.88, batch loss = 0.67 (17.6 examples/sec; 0.453 sec/batch; 41h:38m:02s remains)
INFO - root - 2017-12-06 02:33:55.400550: step 2000, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.453 sec/batch; 41h:35m:50s remains)
2017-12-06 02:33:55.855801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00040655583 -0.00024167076 -0.000355646 -0.00046972558 -0.00059005618 -0.000672929 -0.00068451092 -0.00069141388 -0.00072744116 -0.00076437369 -0.000831306 -0.00093519688 -0.00097679719 -0.00099596381 -0.0011799373][-0.00087304786 -0.00088658556 -0.0010551587 -0.0010982566 -0.0010440908 -0.00087916479 -0.00063089654 -0.00040208921 -0.00020962954 -0.00011141226 -0.00018898398 -0.00036123767 -0.00052516162 -0.00066912919 -0.00092122704][-0.0020214953 -0.0025637969 -0.0029685833 -0.0029219054 -0.0026102141 -0.0021003261 -0.0012108646 -0.00027506799 0.00036040694 0.00056939945 0.00035540015 -2.5779009e-05 -0.00032478198 -0.00053944066 -0.00077567622][-0.00476411 -0.0058579035 -0.0063293837 -0.0062544905 -0.0053583458 -0.0039426871 -0.0019336939 -0.00030732527 0.00076301023 0.0012519471 0.0011305511 0.00053197891 -1.1380762e-05 -0.00030044094 -0.00059325993][-0.006302245 -0.0082207918 -0.0090157129 -0.0091845617 -0.0081444457 -0.0060156323 -0.0035027117 -0.0011231601 0.00081512332 0.001992099 0.0020589754 0.0012963526 0.00058590621 9.5956028e-05 -0.00040754303][-0.0064313263 -0.0079456456 -0.0085731223 -0.0093052574 -0.0092036985 -0.0076908506 -0.0051416866 -0.0021015815 0.0011132509 0.0029629208 0.0029524155 0.0020328946 0.0012496449 0.00063331053 -0.00018978119][-0.0049418248 -0.00397937 -0.0022466518 -0.0030400455 -0.0056297258 -0.0059307776 -0.0038143471 -0.00090467557 0.0025040992 0.0040614009 0.0036951303 0.0027648322 0.0018706098 0.001147151 -9.75281e-06][-0.0027221739 0.0040440448 0.013785865 0.015636522 0.0076819472 0.00058700889 -0.0016164668 -0.00022506714 0.0029823445 0.0042511 0.0039821491 0.0030474737 0.0020532869 0.0012069717 -9.573251e-05][-0.0022662133 0.0089430548 0.026335511 0.031994153 0.018533707 0.0052178092 0.00048721209 0.0012260638 0.0035877191 0.0041956864 0.0037895106 0.0029523782 0.0020464323 0.0012204722 -0.00021855161][-0.0036117174 0.0019607022 0.0089933723 0.010145508 0.0044273697 3.4850091e-05 -0.00014283881 0.0024187863 0.0042851791 0.0041300505 0.0035478063 0.002861429 0.0020704232 0.0012273192 -0.00031468645][-0.0057830624 -0.0042330287 -0.0030941591 -0.0039203987 -0.0047292039 -0.003576301 -0.0011455193 0.0018117055 0.0034089833 0.0034971908 0.0032895729 0.0026474148 0.0019163303 0.0011058599 -0.00032765046][-0.0051699728 -0.0041159168 -0.0038586259 -0.0036270358 -0.0028858148 -0.0011773668 0.00060102716 0.0020385608 0.0027353838 0.0028229915 0.002542872 0.0020069666 0.0013939813 0.0007776469 -0.00053401291][-0.0040991046 -0.0040737279 -0.0043027177 -0.0034807362 -0.0021822192 -0.00032882392 0.00080404058 0.0013832264 0.0016815588 0.0016751625 0.0014672615 0.001070071 0.00063293427 0.00025189295 -0.00083478913][-0.0035199821 -0.003664732 -0.0040709712 -0.003712263 -0.0024804063 -0.0012050085 -0.00034104288 0.00026159734 0.00052494928 0.00054406375 0.00042542815 0.00016469508 -5.0526112e-05 -0.00022757426 -0.0010335743][-0.0034933165 -0.0034060515 -0.0035145842 -0.0033468865 -0.0026254579 -0.0019107349 -0.001514364 -0.0012006834 -0.0011475421 -0.0011541955 -0.0011784881 -0.0013116449 -0.0013996772 -0.0014337786 -0.0019781925]]...]
INFO - root - 2017-12-06 02:34:00.417090: step 2010, loss = 0.84, batch loss = 0.63 (17.0 examples/sec; 0.471 sec/batch; 43h:13m:58s remains)
INFO - root - 2017-12-06 02:34:04.935685: step 2020, loss = 0.87, batch loss = 0.66 (16.8 examples/sec; 0.475 sec/batch; 43h:37m:55s remains)
INFO - root - 2017-12-06 02:34:09.409871: step 2030, loss = 0.87, batch loss = 0.67 (18.2 examples/sec; 0.439 sec/batch; 40h:20m:24s remains)
INFO - root - 2017-12-06 02:34:14.011465: step 2040, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.452 sec/batch; 41h:27m:47s remains)
INFO - root - 2017-12-06 02:34:18.533240: step 2050, loss = 0.89, batch loss = 0.68 (18.2 examples/sec; 0.439 sec/batch; 40h:19m:10s remains)
INFO - root - 2017-12-06 02:34:23.086691: step 2060, loss = 0.84, batch loss = 0.63 (18.6 examples/sec; 0.431 sec/batch; 39h:34m:34s remains)
INFO - root - 2017-12-06 02:34:27.385140: step 2070, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.462 sec/batch; 42h:25m:18s remains)
INFO - root - 2017-12-06 02:34:31.929044: step 2080, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 41h:16m:16s remains)
INFO - root - 2017-12-06 02:34:36.451259: step 2090, loss = 0.86, batch loss = 0.65 (17.8 examples/sec; 0.450 sec/batch; 41h:19m:11s remains)
INFO - root - 2017-12-06 02:34:40.903005: step 2100, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.449 sec/batch; 41h:14m:35s remains)
2017-12-06 02:34:41.362433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057262424 -0.080683142 -0.12791565 -0.17753017 -0.211415 -0.2342149 -0.25908327 -0.28193945 -0.29142174 -0.27835694 -0.25115398 -0.21492746 -0.17030513 -0.12523404 -0.089804418][-0.080895424 -0.11665339 -0.15527484 -0.17251742 -0.1648159 -0.15572104 -0.1753315 -0.22546151 -0.28204617 -0.30570158 -0.29111147 -0.25755072 -0.21188027 -0.15959695 -0.1109542][-0.086801693 -0.11213612 -0.1187401 -0.083535373 -0.020500338 0.019690789 -0.00031627342 -0.079742737 -0.18597636 -0.27197781 -0.30369365 -0.29096192 -0.2499387 -0.19422555 -0.13476691][-0.089666262 -0.083424315 -0.038614761 0.058971651 0.18145469 0.25559178 0.22549668 0.10194667 -0.059661996 -0.21021649 -0.29745442 -0.31731963 -0.28599033 -0.22626108 -0.15822977][-0.080493651 -0.054244265 0.038548395 0.19055039 0.34835324 0.44750711 0.41904882 0.27263328 0.06960839 -0.13108848 -0.26913121 -0.31995323 -0.30728313 -0.25100586 -0.17968485][-0.067684107 -0.019475857 0.098144121 0.28114653 0.47977328 0.61237532 0.59982538 0.44757128 0.21730009 -0.019171659 -0.2023389 -0.28784516 -0.29715821 -0.25287446 -0.17830271][-0.057509512 -0.0051800869 0.11562035 0.30418646 0.52373725 0.70818329 0.75065976 0.61856246 0.37925115 0.11922457 -0.096545815 -0.22254339 -0.26439604 -0.23891595 -0.1674425][-0.055033334 -0.018652743 0.077750534 0.25093719 0.46904707 0.67717731 0.78368533 0.71722627 0.50951105 0.24994433 0.013505939 -0.14329356 -0.21693134 -0.21552795 -0.15678763][-0.063020319 -0.047147404 0.010027353 0.13714817 0.33403042 0.55224687 0.71015215 0.70626593 0.54468268 0.31579632 0.088551886 -0.072687246 -0.16292338 -0.18541211 -0.14559415][-0.070858315 -0.075752087 -0.056638587 0.015074264 0.16006073 0.35560986 0.52924478 0.58589119 0.49610591 0.31977394 0.11576185 -0.049987413 -0.14572246 -0.17035884 -0.13423347][-0.066014044 -0.080207504 -0.093355209 -0.072929993 0.017078001 0.16542414 0.32037365 0.39924982 0.35766989 0.2322678 0.076922454 -0.053983677 -0.12975451 -0.14743069 -0.12725788][-0.056234054 -0.067367643 -0.088486053 -0.10067228 -0.072996847 0.011428837 0.11593827 0.19204414 0.19320664 0.12020663 0.013596244 -0.08196117 -0.13719428 -0.14055568 -0.11485339][-0.060230717 -0.059575118 -0.070409909 -0.092899725 -0.10219669 -0.076742932 -0.024137827 0.028284878 0.043111116 0.016349256 -0.04147739 -0.099652275 -0.12964106 -0.12389862 -0.10245812][-0.060496613 -0.060810827 -0.067891493 -0.076511219 -0.08833839 -0.099147722 -0.089875549 -0.065546021 -0.049989618 -0.051132005 -0.068845794 -0.090410024 -0.098945573 -0.090582259 -0.079513334][-0.059693664 -0.058140513 -0.058963656 -0.062936746 -0.075552352 -0.084522069 -0.088276535 -0.09375675 -0.089477636 -0.079595059 -0.070757419 -0.065498166 -0.06322325 -0.063751549 -0.063382335]]...]
INFO - root - 2017-12-06 02:34:45.938264: step 2110, loss = 0.85, batch loss = 0.64 (17.2 examples/sec; 0.464 sec/batch; 42h:36m:46s remains)
INFO - root - 2017-12-06 02:34:50.456650: step 2120, loss = 0.90, batch loss = 0.70 (18.1 examples/sec; 0.442 sec/batch; 40h:31m:12s remains)
INFO - root - 2017-12-06 02:34:54.996123: step 2130, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.451 sec/batch; 41h:25m:43s remains)
INFO - root - 2017-12-06 02:34:59.589740: step 2140, loss = 0.86, batch loss = 0.65 (17.9 examples/sec; 0.447 sec/batch; 40h:59m:30s remains)
INFO - root - 2017-12-06 02:35:04.081344: step 2150, loss = 0.87, batch loss = 0.66 (18.2 examples/sec; 0.440 sec/batch; 40h:20m:12s remains)
INFO - root - 2017-12-06 02:35:08.599918: step 2160, loss = 0.86, batch loss = 0.65 (17.1 examples/sec; 0.468 sec/batch; 42h:54m:15s remains)
INFO - root - 2017-12-06 02:35:12.928938: step 2170, loss = 0.90, batch loss = 0.69 (16.6 examples/sec; 0.481 sec/batch; 44h:07m:40s remains)
INFO - root - 2017-12-06 02:35:17.494749: step 2180, loss = 0.88, batch loss = 0.68 (17.2 examples/sec; 0.464 sec/batch; 42h:34m:53s remains)
INFO - root - 2017-12-06 02:35:22.009830: step 2190, loss = 0.89, batch loss = 0.68 (18.2 examples/sec; 0.440 sec/batch; 40h:23m:28s remains)
INFO - root - 2017-12-06 02:35:26.526093: step 2200, loss = 0.90, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:35m:44s remains)
2017-12-06 02:35:26.990799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061328042 -0.03273527 -0.019108897 -0.019428654 -0.026484169 -0.031235892 -0.033269037 -0.033861466 -0.030540299 -0.030275431 -0.03373082 -0.034274537 -0.0353027 -0.030441053 -0.02256695][-0.063274942 -0.038045257 -0.024398787 -0.024001718 -0.025248423 -0.026002558 -0.027449312 -0.0259539 -0.022139728 -0.02283114 -0.030707467 -0.038548138 -0.046729993 -0.041158993 -0.034652088][-0.052294273 -0.033959538 -0.02665397 -0.023838909 -0.021423427 -0.018479325 -0.015512124 -0.0092416555 -0.0038212016 -0.0050371289 -0.01541659 -0.027282948 -0.039579943 -0.043145474 -0.042532057][-0.042538591 -0.032655589 -0.030633651 -0.025731279 -0.019960895 -0.014121639 -0.0093537495 -0.0011515878 0.0070776343 0.0096033663 0.0019781329 -0.013611633 -0.031598657 -0.043181643 -0.049439259][-0.036378883 -0.032031815 -0.030468863 -0.023377679 -0.013853367 -0.0077398382 -0.0029151328 0.0036863387 0.0088146776 0.0076431073 -0.0015944913 -0.019603584 -0.0416139 -0.057469174 -0.066239268][-0.031603768 -0.029394833 -0.025946794 -0.014359387 0.0034397505 0.014937665 0.021026101 0.023564395 0.021170203 0.0081196949 -0.014437148 -0.043372806 -0.070424274 -0.086565912 -0.094165131][-0.03361224 -0.029500686 -0.02177345 -0.0029902905 0.02623513 0.046880234 0.0590295 0.059442494 0.051426355 0.029869739 -0.0027651712 -0.041632436 -0.076156482 -0.098174527 -0.11067234][-0.04197678 -0.038155138 -0.028544214 -0.0043063089 0.034474384 0.066341534 0.083147243 0.083994478 0.074608028 0.051401097 0.018973116 -0.020934889 -0.054256763 -0.07396172 -0.087009594][-0.051593039 -0.043754805 -0.033013441 -0.011455938 0.024669323 0.053154927 0.0668122 0.065939769 0.058351 0.039061774 0.015879393 -0.011491932 -0.031672586 -0.041099321 -0.048139323][-0.070247643 -0.060495764 -0.044006161 -0.024951663 0.0034467429 0.019485299 0.02144834 0.013523214 0.0023311637 -0.018064583 -0.034710616 -0.049320351 -0.05309641 -0.04569342 -0.043537278][-0.096379571 -0.083212882 -0.064068571 -0.041392472 -0.016148191 -0.007800106 -0.014302067 -0.030483155 -0.050044529 -0.076102525 -0.097220369 -0.10903724 -0.10551449 -0.092063747 -0.082497627][-0.11484407 -0.10211142 -0.082685605 -0.056865282 -0.033538725 -0.022454374 -0.026546862 -0.044579297 -0.0694816 -0.10125617 -0.1252171 -0.14185019 -0.14268172 -0.13390251 -0.12291621][-0.11782712 -0.10635278 -0.08894603 -0.065673418 -0.043176305 -0.027723148 -0.025739485 -0.039541695 -0.061128162 -0.091894835 -0.1147228 -0.13213781 -0.13766989 -0.1343365 -0.1320875][-0.10758138 -0.097584173 -0.082186908 -0.062297359 -0.045254681 -0.032685831 -0.030150425 -0.039880276 -0.054902926 -0.077724248 -0.095123649 -0.10743041 -0.11131488 -0.1077916 -0.11163589][-0.093421713 -0.084405586 -0.0716646 -0.054902311 -0.040870335 -0.032233961 -0.033922762 -0.04291046 -0.054216649 -0.072202712 -0.083229154 -0.088902511 -0.089570373 -0.085059896 -0.084083885]]...]
INFO - root - 2017-12-06 02:35:31.476808: step 2210, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.452 sec/batch; 41h:28m:32s remains)
INFO - root - 2017-12-06 02:35:36.003435: step 2220, loss = 0.91, batch loss = 0.70 (17.2 examples/sec; 0.465 sec/batch; 42h:38m:21s remains)
INFO - root - 2017-12-06 02:35:40.535021: step 2230, loss = 0.88, batch loss = 0.67 (18.7 examples/sec; 0.427 sec/batch; 39h:10m:08s remains)
INFO - root - 2017-12-06 02:35:45.043777: step 2240, loss = 0.86, batch loss = 0.65 (17.4 examples/sec; 0.461 sec/batch; 42h:17m:48s remains)
INFO - root - 2017-12-06 02:35:49.532972: step 2250, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.439 sec/batch; 40h:17m:52s remains)
INFO - root - 2017-12-06 02:35:53.974992: step 2260, loss = 0.86, batch loss = 0.65 (18.1 examples/sec; 0.443 sec/batch; 40h:36m:18s remains)
INFO - root - 2017-12-06 02:35:58.409704: step 2270, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 41h:16m:31s remains)
INFO - root - 2017-12-06 02:36:02.751689: step 2280, loss = 0.90, batch loss = 0.69 (17.0 examples/sec; 0.472 sec/batch; 43h:17m:29s remains)
INFO - root - 2017-12-06 02:36:07.288032: step 2290, loss = 0.90, batch loss = 0.69 (16.7 examples/sec; 0.478 sec/batch; 43h:51m:06s remains)
INFO - root - 2017-12-06 02:36:11.775825: step 2300, loss = 0.90, batch loss = 0.69 (17.1 examples/sec; 0.468 sec/batch; 42h:58m:08s remains)
2017-12-06 02:36:12.262202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.29399958 -0.34918475 -0.36687204 -0.35003689 -0.30352971 -0.23874506 -0.15704097 -0.080568969 -0.031427458 -0.023468196 -0.044881381 -0.059394915 -0.05392674 -0.046525083 -0.045409534][-0.33627865 -0.37962 -0.3684639 -0.30996269 -0.2359288 -0.15761119 -0.064991124 0.019236807 0.059337813 0.033054564 -0.022842677 -0.054770228 -0.04952025 -0.037862316 -0.035018422][-0.23466669 -0.25420222 -0.21290442 -0.1256883 -0.039375972 0.041856047 0.12735437 0.19163735 0.18413043 0.097000688 -0.00063094869 -0.045326512 -0.035094485 -0.02012136 -0.01857236][0.011339385 0.039304938 0.1233629 0.22911984 0.2823717 0.32007408 0.34891945 0.354067 0.27689362 0.14045396 0.017841686 -0.03210631 -0.02043269 -0.0050561614 -0.0043975785][0.28566125 0.34947911 0.45862618 0.55815423 0.578012 0.56242496 0.52030581 0.45648357 0.30496228 0.12239224 -0.0061161071 -0.037230961 -0.017130557 -0.0023426786 0.00041442737][0.47089311 0.57305276 0.732126 0.82796669 0.81578946 0.78829461 0.715079 0.58066267 0.33416647 0.10101962 -0.029447621 -0.042010821 -0.010061603 0.0027792789 0.004867781][0.49477163 0.672303 0.89969492 1.0352896 1.0547384 1.0110528 0.8909682 0.70380694 0.39238963 0.11082722 -0.03172832 -0.037797414 -0.010318272 -0.0023799464 -0.0014168434][0.42797181 0.66889077 0.96493357 1.1222643 1.1225886 1.0767419 0.95783514 0.72671354 0.38390416 0.10468398 -0.032708745 -0.037877705 -0.011254322 -2.9802322e-05 0.0065276623][0.33416688 0.61896771 0.93689734 1.0871291 1.0785253 0.99878985 0.82525516 0.60508853 0.32181582 0.079217076 -0.039448429 -0.044391774 -0.026884571 -0.01690462 -0.0086053647][0.26314464 0.51677006 0.80221176 0.92521673 0.90275949 0.79716974 0.61909783 0.41129524 0.17238486 0.010287985 -0.056217458 -0.057368003 -0.042520862 -0.034384646 -0.026108615][0.19369368 0.36760867 0.5786646 0.64422441 0.607727 0.50543475 0.35364482 0.20011766 0.050239097 -0.0415698 -0.066007368 -0.054667745 -0.039953098 -0.030985577 -0.022947017][0.094629794 0.17197576 0.28479835 0.33738244 0.31039155 0.23726195 0.14684638 0.061939757 -0.0094394088 -0.047698651 -0.058859639 -0.052620918 -0.043597415 -0.038378377 -0.042483583][-0.025033645 -0.0034217127 0.045839276 0.070601359 0.072902948 0.055861767 0.030980486 0.018963587 -0.012474798 -0.02701794 -0.044497453 -0.049458351 -0.04747251 -0.043182544 -0.040106453][-0.094246387 -0.10814135 -0.10147451 -0.090214089 -0.068828717 -0.042968582 -0.012331717 0.010599535 0.0085761473 -0.0040129013 -0.032098956 -0.053615186 -0.053263918 -0.05247486 -0.050833613][-0.083639823 -0.10169376 -0.11434726 -0.11174345 -0.093850106 -0.056758903 -0.021654636 0.0015450232 0.00272087 -0.0041511618 -0.025048297 -0.045614347 -0.05417949 -0.0538331 -0.053968765]]...]
INFO - root - 2017-12-06 02:36:16.844461: step 2310, loss = 0.92, batch loss = 0.71 (17.3 examples/sec; 0.462 sec/batch; 42h:19m:47s remains)
INFO - root - 2017-12-06 02:36:21.373012: step 2320, loss = 0.86, batch loss = 0.65 (16.8 examples/sec; 0.478 sec/batch; 43h:48m:02s remains)
INFO - root - 2017-12-06 02:36:25.924100: step 2330, loss = 0.89, batch loss = 0.68 (18.4 examples/sec; 0.434 sec/batch; 39h:46m:30s remains)
INFO - root - 2017-12-06 02:36:30.424766: step 2340, loss = 0.86, batch loss = 0.65 (18.0 examples/sec; 0.444 sec/batch; 40h:40m:41s remains)
INFO - root - 2017-12-06 02:36:34.989728: step 2350, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.451 sec/batch; 41h:24m:12s remains)
INFO - root - 2017-12-06 02:36:39.443785: step 2360, loss = 0.83, batch loss = 0.62 (18.1 examples/sec; 0.442 sec/batch; 40h:32m:45s remains)
INFO - root - 2017-12-06 02:36:43.978739: step 2370, loss = 0.86, batch loss = 0.65 (17.8 examples/sec; 0.449 sec/batch; 41h:13m:02s remains)
INFO - root - 2017-12-06 02:36:48.313396: step 2380, loss = 0.86, batch loss = 0.65 (17.3 examples/sec; 0.463 sec/batch; 42h:24m:49s remains)
INFO - root - 2017-12-06 02:36:52.830326: step 2390, loss = 0.85, batch loss = 0.64 (18.0 examples/sec; 0.445 sec/batch; 40h:48m:12s remains)
INFO - root - 2017-12-06 02:36:57.307580: step 2400, loss = 0.87, batch loss = 0.66 (17.5 examples/sec; 0.456 sec/batch; 41h:50m:13s remains)
2017-12-06 02:36:57.765550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.03463389 -0.049369395 -0.0659803 -0.083897315 -0.099396214 -0.10778206 -0.10943899 -0.10454298 -0.094573259 -0.083976649 -0.070638768 -0.058214329 -0.047072478 -0.036296681 -0.028026843][-0.022349797 -0.035187226 -0.049968325 -0.0650634 -0.074186079 -0.077393584 -0.075708158 -0.06767337 -0.057606895 -0.049726907 -0.044648826 -0.040190317 -0.03497424 -0.030452525 -0.025948847][-0.024394512 -0.033186875 -0.041602343 -0.045147337 -0.0461511 -0.042520154 -0.034992002 -0.025494564 -0.017524637 -0.013165094 -0.013801467 -0.016276853 -0.019599853 -0.021174047 -0.021279559][-0.037912231 -0.040301885 -0.037178412 -0.03052273 -0.020301035 -0.0094339289 0.00073703006 0.0095094182 0.014046419 0.014292121 0.0094006807 0.001587607 -0.0056503378 -0.011804894 -0.014679853][-0.010928743 -0.011301033 -0.0044782571 0.0071637556 0.018833071 0.028980069 0.034780279 0.0364336 0.034532256 0.027761951 0.017697603 0.0073189847 -0.0017920732 -0.0085258186 -0.01259511][0.0061547197 0.00411997 0.0099379718 0.021113 0.032783851 0.039887086 0.040845446 0.036594555 0.027520977 0.015641365 0.0058679357 -0.0015950501 -0.0075310543 -0.011575744 -0.013297793][0.018544875 0.016000997 0.018366084 0.025350735 0.031900108 0.0343889 0.031060949 0.02350726 0.013168424 0.0027946867 -0.005336009 -0.011142466 -0.014136132 -0.015115233 -0.015655352][-0.0022499934 -0.0047092922 -0.0036888979 0.0007044822 0.0037968494 0.0052252859 0.0033834204 -0.00083509833 -0.0061550364 -0.011038259 -0.014233347 -0.016634844 -0.018045006 -0.018706596 -0.018839659][-0.014280178 -0.016554438 -0.015949612 -0.014759691 -0.014609659 -0.014650565 -0.015257465 -0.016493045 -0.018179497 -0.019577585 -0.020068565 -0.019970499 -0.020164888 -0.02048585 -0.020235784][-0.017398367 -0.016986607 -0.01794566 -0.018278928 -0.018937189 -0.019585032 -0.020119634 -0.020526601 -0.020916359 -0.021165185 -0.021047631 -0.021580793 -0.021888578 -0.022231421 -0.022517206][-0.017344935 -0.016979586 -0.018776419 -0.019521054 -0.020047292 -0.020569429 -0.021040937 -0.021440534 -0.021787485 -0.021532627 -0.021428723 -0.0214468 -0.021287132 -0.020956678 -0.020132344][-0.018351967 -0.018456355 -0.018741166 -0.019833215 -0.020197 -0.020564452 -0.020984465 -0.02140444 -0.021667428 -0.021846857 -0.021802576 -0.021647252 -0.021440919 -0.02107849 -0.020684797][-0.019907195 -0.019539271 -0.019645348 -0.019894239 -0.020149961 -0.020423803 -0.020683613 -0.021006672 -0.021154959 -0.021195343 -0.021048112 -0.020916741 -0.020860808 -0.020678403 -0.020496022][-0.020375572 -0.020228071 -0.019698692 -0.019809557 -0.019859811 -0.019973017 -0.020066084 -0.02026134 -0.020363897 -0.020437945 -0.020314654 -0.020291755 -0.020304598 -0.020327725 -0.020403445][-0.019163979 -0.018870063 -0.020038955 -0.020136766 -0.019938109 -0.019993721 -0.020073533 -0.020292765 -0.020408213 -0.02047096 -0.020391949 -0.020412259 -0.020445399 -0.020473205 -0.020582408]]...]
INFO - root - 2017-12-06 02:37:02.316120: step 2410, loss = 0.90, batch loss = 0.69 (16.7 examples/sec; 0.479 sec/batch; 43h:56m:55s remains)
INFO - root - 2017-12-06 02:37:06.940546: step 2420, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 40h:34m:25s remains)
INFO - root - 2017-12-06 02:37:11.415134: step 2430, loss = 0.90, batch loss = 0.69 (17.3 examples/sec; 0.464 sec/batch; 42h:30m:32s remains)
INFO - root - 2017-12-06 02:37:15.937424: step 2440, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.460 sec/batch; 42h:09m:04s remains)
INFO - root - 2017-12-06 02:37:20.355008: step 2450, loss = 0.82, batch loss = 0.61 (17.6 examples/sec; 0.453 sec/batch; 41h:33m:55s remains)
INFO - root - 2017-12-06 02:37:24.961219: step 2460, loss = 0.88, batch loss = 0.68 (17.4 examples/sec; 0.460 sec/batch; 42h:12m:00s remains)
INFO - root - 2017-12-06 02:37:29.488659: step 2470, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.444 sec/batch; 40h:42m:07s remains)
INFO - root - 2017-12-06 02:37:33.819443: step 2480, loss = 0.87, batch loss = 0.66 (26.8 examples/sec; 0.298 sec/batch; 27h:20m:59s remains)
INFO - root - 2017-12-06 02:37:38.285423: step 2490, loss = 0.91, batch loss = 0.70 (17.4 examples/sec; 0.460 sec/batch; 42h:08m:07s remains)
INFO - root - 2017-12-06 02:37:42.888164: step 2500, loss = 0.85, batch loss = 0.64 (17.3 examples/sec; 0.463 sec/batch; 42h:29m:08s remains)
2017-12-06 02:37:43.328831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.095623262 -0.17782916 -0.17485286 -0.16315164 -0.1442041 -0.1109715 -0.065801479 -0.024275146 0.0016125292 0.013176307 0.0094011314 -0.006858848 -0.020254232 -0.019515151 -0.016488811][-0.076614261 -0.17313021 -0.17356282 -0.15462604 -0.14538325 -0.1334894 -0.11568098 -0.10227703 -0.098863214 -0.10957578 -0.11475213 -0.11506388 -0.12220551 -0.13081281 -0.14701702][0.0058828816 -0.082147658 -0.077658072 -0.057043314 -0.038663898 -0.020475091 -0.0074642561 0.0068009496 -0.0045493543 -0.0430992 -0.083050281 -0.095169269 -0.11324708 -0.14044355 -0.14201184][0.088738993 0.021368217 0.035942655 0.051069569 0.067821175 0.093183234 0.11622605 0.12764427 0.12319775 0.078572705 0.029013027 -0.011333689 -0.0514658 -0.096620649 -0.10414134][0.081094325 0.054272506 0.077454805 0.086088315 0.10450505 0.12551115 0.14646718 0.16581601 0.1777878 0.14345872 0.093215764 0.045239192 0.0021379814 -0.04087944 -0.055603463][0.065548033 0.069087878 0.098027453 0.10843258 0.12576814 0.14095552 0.15688965 0.17820717 0.19188638 0.17451476 0.131308 0.082473561 0.041673396 0.020250987 0.0081502721][0.043643933 0.037885752 0.052458297 0.067978293 0.08603549 0.10616985 0.1312205 0.1641029 0.18003473 0.17651127 0.14561847 0.1043313 0.078224361 0.07371828 0.070051238][0.02773596 0.0031521842 0.0061585531 0.01367322 0.016272355 0.031180557 0.051766131 0.079479232 0.10052888 0.10696022 0.098456472 0.083357736 0.089122877 0.10868022 0.10879242][0.029505182 -0.013073388 -0.025621325 -0.025772225 -0.034856528 -0.04248783 -0.043147359 -0.035258189 -0.026260484 -0.021908265 -0.013662308 0.013528608 0.062391307 0.1196512 0.15236381][0.0047011189 -0.029979138 -0.041461971 -0.045741592 -0.06785164 -0.0880367 -0.092961714 -0.082040265 -0.072270215 -0.061216183 -0.033545431 0.011649076 0.081315577 0.14988069 0.20619252][-0.06308797 -0.048844334 -0.038349263 -0.026402909 -0.039037626 -0.059603073 -0.075746037 -0.075112656 -0.068477154 -0.067652628 -0.044434454 -0.003775727 0.060904119 0.1332337 0.20336086][-0.12161571 -0.082192354 -0.053333674 -0.014273196 -0.004851982 -0.01489001 -0.027183617 -0.03004189 -0.027552864 -0.032629993 -0.018579952 0.0084735751 0.064291 0.12229955 0.18446007][-0.17162071 -0.11665022 -0.07317391 -0.030234303 -0.0049707107 0.0065215938 0.0097377077 0.011892218 0.021676522 0.015263703 0.021848906 0.036074962 0.065481305 0.1022044 0.12989357][-0.25464931 -0.22344731 -0.20172727 -0.17486684 -0.12863651 -0.087576613 -0.056232005 -0.035869367 -0.0065843277 0.0074663386 0.017009366 0.032788504 0.054963324 0.08267802 0.10995577][-0.24640696 -0.25848159 -0.26642507 -0.25419256 -0.22087687 -0.18157358 -0.13706684 -0.10299845 -0.065254778 -0.04336112 -0.020826843 -0.0037277937 0.013387267 0.036132853 0.037358854]]...]
INFO - root - 2017-12-06 02:37:47.813633: step 2510, loss = 0.92, batch loss = 0.71 (18.3 examples/sec; 0.438 sec/batch; 40h:07m:07s remains)
INFO - root - 2017-12-06 02:37:52.274624: step 2520, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.447 sec/batch; 41h:00m:49s remains)
INFO - root - 2017-12-06 02:37:56.824845: step 2530, loss = 0.84, batch loss = 0.63 (17.2 examples/sec; 0.465 sec/batch; 42h:37m:55s remains)
INFO - root - 2017-12-06 02:38:01.359476: step 2540, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.461 sec/batch; 42h:13m:08s remains)
INFO - root - 2017-12-06 02:38:05.870425: step 2550, loss = 0.86, batch loss = 0.65 (17.0 examples/sec; 0.470 sec/batch; 43h:04m:11s remains)
INFO - root - 2017-12-06 02:38:10.389412: step 2560, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 41h:07m:41s remains)
INFO - root - 2017-12-06 02:38:14.912468: step 2570, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.452 sec/batch; 41h:26m:54s remains)
INFO - root - 2017-12-06 02:38:19.340317: step 2580, loss = 0.91, batch loss = 0.70 (18.8 examples/sec; 0.425 sec/batch; 38h:58m:12s remains)
INFO - root - 2017-12-06 02:38:23.655211: step 2590, loss = 0.93, batch loss = 0.72 (18.2 examples/sec; 0.439 sec/batch; 40h:12m:31s remains)
INFO - root - 2017-12-06 02:38:28.101831: step 2600, loss = 0.88, batch loss = 0.67 (17.5 examples/sec; 0.457 sec/batch; 41h:54m:47s remains)
2017-12-06 02:38:28.523012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.08222799 -0.08190529 -0.080069676 -0.076784216 -0.073480785 -0.074199416 -0.076030895 -0.078814268 -0.079935491 -0.077271432 -0.075504959 -0.071760386 -0.066594839 -0.060219474 -0.055581789][-0.082086414 -0.081127286 -0.078696117 -0.077660315 -0.075690664 -0.074255459 -0.072564095 -0.073010631 -0.074066937 -0.071709767 -0.0685848 -0.065102682 -0.062769078 -0.059669036 -0.05575506][-0.081302732 -0.080374 -0.077372164 -0.074035943 -0.068395026 -0.065958336 -0.063233674 -0.057974294 -0.054437943 -0.049378544 -0.045862831 -0.04254866 -0.04431092 -0.047724143 -0.0498489][-0.076623872 -0.077237345 -0.075298995 -0.071265 -0.062280666 -0.050755862 -0.037526064 -0.024127366 -0.012862369 -0.0015241057 0.0029347576 0.0035400316 -0.0060969368 -0.01738451 -0.024983656][-0.068065636 -0.069170959 -0.067877427 -0.063271277 -0.051738348 -0.032787748 -0.0078338087 0.020652253 0.046718847 0.069162577 0.080823988 0.08192642 0.065940231 0.042992566 0.021552544][-0.050619535 -0.052811798 -0.055700094 -0.052419111 -0.037935954 -0.010851551 0.025948707 0.0717462 0.11661896 0.1530049 0.17208332 0.1773681 0.1643738 0.14029312 0.11199704][-0.038095396 -0.041192386 -0.042195447 -0.0410071 -0.02678697 0.005011607 0.052227881 0.11271119 0.17315811 0.22234371 0.25057542 0.26125097 0.2543506 0.23827547 0.21773681][-0.026090322 -0.031123076 -0.033134326 -0.03150421 -0.0173467 0.01042185 0.057571355 0.12400003 0.19357419 0.24893335 0.2809107 0.29880586 0.30488864 0.30423537 0.30075321][-0.018123899 -0.023432227 -0.024756758 -0.026399916 -0.017559592 0.0071875118 0.051864464 0.1115716 0.17506394 0.22704107 0.25786781 0.27692309 0.29155824 0.31176409 0.33430341][-0.022305571 -0.027178183 -0.026699476 -0.027710443 -0.021626672 -0.0053290464 0.029005069 0.079112232 0.13128954 0.16869138 0.18931422 0.2047053 0.22273505 0.24989021 0.286133][-0.032239422 -0.041167222 -0.043684814 -0.045302987 -0.041926503 -0.030586962 -0.0048455745 0.028665397 0.061806191 0.083735347 0.092687964 0.099906147 0.11274475 0.13966519 0.17912012][-0.042628471 -0.053792626 -0.061515842 -0.067803808 -0.069329046 -0.063807108 -0.047457259 -0.028046928 -0.013423547 -0.0092367232 -0.013201062 -0.014580529 -0.0084065646 0.0094018206 0.040196065][-0.052624021 -0.063950241 -0.074662872 -0.084425643 -0.090904325 -0.091578759 -0.0845841 -0.076717496 -0.073668636 -0.079649806 -0.092385933 -0.10412179 -0.10912681 -0.1027848 -0.08545579][-0.060962692 -0.069523767 -0.0804784 -0.092530861 -0.10321625 -0.10947898 -0.1114475 -0.11310631 -0.11828732 -0.12935011 -0.14367573 -0.15656036 -0.16545972 -0.17059028 -0.16923341][-0.066733174 -0.071637519 -0.079622112 -0.090779781 -0.10336816 -0.11465529 -0.12395209 -0.13201609 -0.14139862 -0.15288375 -0.16604038 -0.17867994 -0.18843529 -0.19601598 -0.20075209]]...]
INFO - root - 2017-12-06 02:38:33.095402: step 2610, loss = 0.90, batch loss = 0.69 (16.0 examples/sec; 0.499 sec/batch; 45h:41m:43s remains)
INFO - root - 2017-12-06 02:38:37.614408: step 2620, loss = 0.85, batch loss = 0.64 (16.3 examples/sec; 0.491 sec/batch; 45h:00m:06s remains)
INFO - root - 2017-12-06 02:38:42.118455: step 2630, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 41h:15m:23s remains)
INFO - root - 2017-12-06 02:38:46.694123: step 2640, loss = 0.87, batch loss = 0.66 (17.1 examples/sec; 0.469 sec/batch; 42h:57m:17s remains)
INFO - root - 2017-12-06 02:38:51.161328: step 2650, loss = 0.93, batch loss = 0.72 (18.7 examples/sec; 0.427 sec/batch; 39h:08m:51s remains)
INFO - root - 2017-12-06 02:38:55.698377: step 2660, loss = 0.87, batch loss = 0.66 (17.2 examples/sec; 0.465 sec/batch; 42h:35m:43s remains)
INFO - root - 2017-12-06 02:39:00.259727: step 2670, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.456 sec/batch; 41h:48m:11s remains)
INFO - root - 2017-12-06 02:39:04.695830: step 2680, loss = 0.92, batch loss = 0.71 (18.6 examples/sec; 0.429 sec/batch; 39h:18m:38s remains)
INFO - root - 2017-12-06 02:39:08.962035: step 2690, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.442 sec/batch; 40h:28m:49s remains)
INFO - root - 2017-12-06 02:39:13.480216: step 2700, loss = 0.81, batch loss = 0.60 (17.7 examples/sec; 0.452 sec/batch; 41h:26m:08s remains)
2017-12-06 02:39:13.906530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038181223 -0.037683949 -0.037061192 -0.036336355 -0.035765186 -0.035490856 -0.035350237 -0.035297506 -0.035343967 -0.035416752 -0.035468973 -0.03552185 -0.035572063 -0.0356134 -0.035647023][-0.038333118 -0.037640166 -0.036850195 -0.036052074 -0.035484828 -0.035178103 -0.035020497 -0.034943588 -0.034947019 -0.03498153 -0.035034727 -0.035098303 -0.035164557 -0.035225123 -0.035282843][-0.038411938 -0.037582323 -0.0366785 -0.035862446 -0.03529451 -0.034995481 -0.034820948 -0.034736082 -0.034707081 -0.034714721 -0.03476325 -0.034845605 -0.034944046 -0.035042983 -0.035129365][-0.038209729 -0.037218262 -0.036242507 -0.035420422 -0.034890968 -0.034610949 -0.034457725 -0.034412373 -0.034399144 -0.034407783 -0.034467436 -0.034580451 -0.034717537 -0.034854118 -0.034963336][-0.037645262 -0.036568832 -0.035592187 -0.034849547 -0.03438222 -0.034150839 -0.03404288 -0.034045216 -0.034073114 -0.034094565 -0.034146335 -0.034261446 -0.034413759 -0.034585521 -0.034725785][-0.036848757 -0.035738114 -0.034875087 -0.034265988 -0.033900261 -0.033734519 -0.033696689 -0.033722211 -0.033753436 -0.033761159 -0.033786844 -0.033864543 -0.033994347 -0.0341737 -0.034354202][-0.036015391 -0.034985192 -0.034232546 -0.033735007 -0.033458419 -0.033344939 -0.033335246 -0.033361744 -0.033377577 -0.033359639 -0.033357445 -0.033414319 -0.033546224 -0.033748683 -0.033994764][-0.035435557 -0.034499422 -0.033859435 -0.033462644 -0.033252995 -0.033145756 -0.033105865 -0.033111133 -0.033118896 -0.033093043 -0.033093285 -0.033159167 -0.033315375 -0.033553172 -0.033829063][-0.035045017 -0.034278344 -0.033830103 -0.0336012 -0.033502854 -0.033432811 -0.033368491 -0.0333372 -0.033344507 -0.033312485 -0.033311825 -0.03336262 -0.033475053 -0.033653468 -0.033870053][-0.034906089 -0.034355175 -0.034097716 -0.034019206 -0.033994362 -0.033965845 -0.03390317 -0.033841215 -0.033796787 -0.033711381 -0.033658046 -0.033631429 -0.033641648 -0.033705555 -0.033840127][-0.034878206 -0.034459066 -0.034318976 -0.034293246 -0.034300178 -0.0342789 -0.034209456 -0.034119613 -0.034032628 -0.033913281 -0.033827834 -0.0337625 -0.033707783 -0.033680879 -0.033738568][-0.034766536 -0.034430142 -0.034376279 -0.034379762 -0.034378339 -0.034342669 -0.034255214 -0.034146484 -0.034033317 -0.033906803 -0.03382089 -0.033752147 -0.033673462 -0.03361214 -0.033626959][-0.034806214 -0.034476489 -0.034443725 -0.034427151 -0.034356855 -0.034274928 -0.034180224 -0.034076028 -0.033966083 -0.033864535 -0.033813857 -0.033768795 -0.033700421 -0.033641882 -0.033634406][-0.03482426 -0.034462992 -0.03438909 -0.034318704 -0.034204174 -0.034100529 -0.034018412 -0.033944312 -0.033856943 -0.033800602 -0.033792991 -0.033762656 -0.033688858 -0.033614628 -0.03358224][-0.03468778 -0.03423984 -0.034070574 -0.033907227 -0.033751987 -0.033635136 -0.0335454 -0.033483572 -0.033433981 -0.03341835 -0.033433683 -0.033426642 -0.033379972 -0.033321504 -0.033286333]]...]
INFO - root - 2017-12-06 02:39:18.382800: step 2710, loss = 0.89, batch loss = 0.68 (18.4 examples/sec; 0.435 sec/batch; 39h:49m:27s remains)
INFO - root - 2017-12-06 02:39:22.831398: step 2720, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.458 sec/batch; 41h:58m:32s remains)
INFO - root - 2017-12-06 02:39:27.429616: step 2730, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.436 sec/batch; 39h:56m:59s remains)
INFO - root - 2017-12-06 02:39:32.024062: step 2740, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.457 sec/batch; 41h:54m:04s remains)
INFO - root - 2017-12-06 02:39:36.506031: step 2750, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 41h:25m:28s remains)
INFO - root - 2017-12-06 02:39:41.017064: step 2760, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.452 sec/batch; 41h:26m:13s remains)
INFO - root - 2017-12-06 02:39:45.481208: step 2770, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.440 sec/batch; 40h:15m:57s remains)
INFO - root - 2017-12-06 02:39:50.027849: step 2780, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.464 sec/batch; 42h:27m:51s remains)
INFO - root - 2017-12-06 02:39:54.534421: step 2790, loss = 0.87, batch loss = 0.66 (17.2 examples/sec; 0.465 sec/batch; 42h:33m:14s remains)
INFO - root - 2017-12-06 02:39:58.766320: step 2800, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.438 sec/batch; 40h:04m:41s remains)
2017-12-06 02:39:59.233036: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26218417 0.20019604 0.24607828 0.2166287 0.22872314 0.26379237 0.2968545 0.30835256 0.32528245 0.30083573 0.25513631 0.23634672 0.2186262 0.1556776 0.057420965][0.076259419 0.0033332594 -6.4104795e-05 -0.014079455 0.0057275556 0.046133731 0.085395783 0.12600079 0.16141258 0.15434399 0.13610211 0.13391705 0.12077501 0.079973012 -0.017107846][0.079147547 -0.016394651 -0.094041012 -0.12235194 -0.12459226 -0.10355063 -0.085864633 -0.057450764 -0.015202146 0.0087698735 0.019503597 0.05325434 0.05113307 -0.00015095249 -0.063934192][0.19252944 0.049341727 -0.11783846 -0.20067568 -0.2674017 -0.28835765 -0.29390413 -0.2731874 -0.23194921 -0.20643264 -0.19384104 -0.17359214 -0.18558709 -0.2146915 -0.24929056][0.27969435 0.11909051 -0.09834975 -0.23515628 -0.35700029 -0.42951193 -0.48024458 -0.48889413 -0.46129736 -0.41678026 -0.39760742 -0.38812551 -0.40344378 -0.42488459 -0.44723925][0.42258021 0.29164952 0.034708571 -0.1317424 -0.28586024 -0.41143999 -0.52326161 -0.59284449 -0.60965645 -0.57607919 -0.54271871 -0.52275234 -0.52831882 -0.52496338 -0.51144648][0.53973764 0.41795769 0.15419219 0.02613226 -0.10647486 -0.25672403 -0.41391259 -0.53336149 -0.59251142 -0.57021523 -0.52524155 -0.49421406 -0.48124316 -0.46636286 -0.45845783][0.5225237 0.45732218 0.27831626 0.21769819 0.14869727 0.039615054 -0.093308061 -0.21431783 -0.29036158 -0.29713771 -0.28657943 -0.26931581 -0.26553261 -0.28974512 -0.30507353][0.3936311 0.32796824 0.20538932 0.20436743 0.20612592 0.16208756 0.10255772 0.046272974 0.019420635 0.0095218271 -0.0020683445 0.0096626133 -0.0081035569 -0.077750742 -0.14508028][0.25903738 0.19252975 0.10993464 0.078508735 0.11081053 0.1232518 0.12212777 0.11387867 0.12841727 0.15234141 0.16785629 0.17489742 0.1315873 0.047997314 -0.029054819][0.047952216 -0.016345765 -0.048794352 -0.10663528 -0.064781927 -0.010378458 0.036431823 0.070771366 0.11485647 0.1382232 0.1515968 0.15436661 0.090135768 0.0012577511 -0.10183136][-0.10757037 -0.20832393 -0.21132484 -0.2592957 -0.21942186 -0.14057177 -0.071792409 -0.037585549 0.0065454543 0.031882543 0.036360782 0.013578095 -0.0409473 -0.14209889 -0.21473028][-0.12233333 -0.24088681 -0.25754008 -0.33077627 -0.33407423 -0.2278391 -0.14897528 -0.10621432 -0.0748508 -0.072167754 -0.099018671 -0.13176772 -0.17491966 -0.23968762 -0.3056747][-0.30810434 -0.41955742 -0.4072037 -0.451385 -0.44132403 -0.39116865 -0.36455646 -0.34300047 -0.34039542 -0.34449571 -0.33813292 -0.33802429 -0.36052933 -0.40134656 -0.43266395][-0.48019293 -0.62300241 -0.65745592 -0.68289149 -0.70477796 -0.68730444 -0.62877578 -0.60230803 -0.57585645 -0.573159 -0.5560559 -0.552336 -0.54672557 -0.58656633 -0.57548219]]...]
INFO - root - 2017-12-06 02:40:03.723672: step 2810, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 41h:35m:37s remains)
INFO - root - 2017-12-06 02:40:08.238021: step 2820, loss = 0.86, batch loss = 0.65 (18.4 examples/sec; 0.435 sec/batch; 39h:47m:46s remains)
INFO - root - 2017-12-06 02:40:12.859095: step 2830, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 40h:34m:35s remains)
INFO - root - 2017-12-06 02:40:17.322263: step 2840, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.451 sec/batch; 41h:18m:58s remains)
INFO - root - 2017-12-06 02:40:21.792125: step 2850, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.446 sec/batch; 40h:50m:05s remains)
INFO - root - 2017-12-06 02:40:26.290179: step 2860, loss = 0.87, batch loss = 0.66 (18.2 examples/sec; 0.438 sec/batch; 40h:08m:46s remains)
INFO - root - 2017-12-06 02:40:30.733994: step 2870, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.448 sec/batch; 41h:01m:55s remains)
INFO - root - 2017-12-06 02:40:35.289779: step 2880, loss = 0.90, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:50m:57s remains)
INFO - root - 2017-12-06 02:40:39.857557: step 2890, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.446 sec/batch; 40h:52m:19s remains)
INFO - root - 2017-12-06 02:40:44.168290: step 2900, loss = 0.94, batch loss = 0.73 (17.6 examples/sec; 0.455 sec/batch; 41h:42m:01s remains)
2017-12-06 02:40:44.636342: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38217309 0.32742882 0.22089873 0.11027065 0.0026254915 -0.06773039 -0.084957704 -0.086808272 -0.085729405 -0.089882523 -0.086962074 -0.078999281 -0.070058525 -0.063257031 -0.058196668][0.50699592 0.42261192 0.28710374 0.15383136 0.037418347 -0.042034153 -0.068833217 -0.076573752 -0.078739718 -0.085033171 -0.086407766 -0.081504956 -0.072065577 -0.063018315 -0.059504315][0.43262962 0.33103317 0.19500779 0.079181656 -0.00914944 -0.063759878 -0.077823557 -0.08352185 -0.083689228 -0.088381641 -0.090193346 -0.085156083 -0.079223186 -0.073610082 -0.068391234][0.10827014 0.039722513 -0.0338402 -0.08526516 -0.10638791 -0.11139476 -0.0997088 -0.094446704 -0.092586905 -0.0923723 -0.092799708 -0.092922658 -0.090246767 -0.085524261 -0.082041636][-0.20889616 -0.23343229 -0.23356634 -0.21407518 -0.18149656 -0.14625745 -0.11353499 -0.1021065 -0.099850543 -0.095390022 -0.092940971 -0.0938272 -0.094981544 -0.095660329 -0.0952549][-0.30476591 -0.29700857 -0.26286155 -0.21957095 -0.17518343 -0.13615417 -0.10751205 -0.097796649 -0.097438425 -0.094887652 -0.094388574 -0.095782064 -0.09708339 -0.097905427 -0.097844511][-0.2154534 -0.21066326 -0.18835622 -0.16013002 -0.13540757 -0.116979 -0.10409249 -0.0971303 -0.096257843 -0.095366031 -0.0964699 -0.097379312 -0.098514043 -0.099118471 -0.099052861][-0.12005538 -0.12178607 -0.11585055 -0.11026131 -0.10851876 -0.10797369 -0.10147865 -0.099010393 -0.097412981 -0.096927077 -0.098024294 -0.098587677 -0.099453032 -0.099925891 -0.099837855][-0.091526732 -0.090300083 -0.090521954 -0.093765423 -0.099328905 -0.10451175 -0.099261954 -0.099708371 -0.099000171 -0.098793626 -0.099302575 -0.099503763 -0.0999928 -0.10019088 -0.10014564][-0.095989436 -0.092380084 -0.09245874 -0.093830764 -0.097672462 -0.10187149 -0.099048167 -0.099286549 -0.098989353 -0.09758573 -0.098227069 -0.098681286 -0.099221438 -0.09956117 -0.099767737][-0.094662681 -0.093662396 -0.094550028 -0.095463738 -0.098333694 -0.10342163 -0.10093738 -0.09799546 -0.098771609 -0.096108541 -0.096976407 -0.096650973 -0.097610474 -0.098241389 -0.098972335][-0.09430851 -0.092887618 -0.093831725 -0.093807913 -0.093637049 -0.095261276 -0.092200257 -0.090490207 -0.09318082 -0.092260957 -0.096416041 -0.095354453 -0.097510077 -0.098412491 -0.098315716][-0.094151884 -0.090914071 -0.089297585 -0.0874518 -0.085455239 -0.086438373 -0.083775207 -0.081593595 -0.083504111 -0.083790295 -0.090128705 -0.0909753 -0.095565379 -0.09789893 -0.098626077][-0.09571258 -0.0944872 -0.0920367 -0.087673739 -0.084502511 -0.08055599 -0.075325556 -0.071778566 -0.072781794 -0.072950669 -0.081688464 -0.084913008 -0.091406852 -0.095875978 -0.097945258][-0.097825274 -0.096106015 -0.093322873 -0.087986246 -0.081834495 -0.073693 -0.068939447 -0.064108506 -0.064453579 -0.065718368 -0.07478188 -0.079962887 -0.088393793 -0.094892845 -0.097965255]]...]
INFO - root - 2017-12-06 02:40:49.119554: step 2910, loss = 0.91, batch loss = 0.70 (17.7 examples/sec; 0.452 sec/batch; 41h:21m:16s remains)
INFO - root - 2017-12-06 02:40:53.571088: step 2920, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 41h:06m:05s remains)
INFO - root - 2017-12-06 02:40:58.126191: step 2930, loss = 0.89, batch loss = 0.68 (16.8 examples/sec; 0.477 sec/batch; 43h:42m:17s remains)
INFO - root - 2017-12-06 02:41:02.752858: step 2940, loss = 0.91, batch loss = 0.70 (18.1 examples/sec; 0.443 sec/batch; 40h:33m:08s remains)
INFO - root - 2017-12-06 02:41:07.187657: step 2950, loss = 0.86, batch loss = 0.65 (18.5 examples/sec; 0.433 sec/batch; 39h:39m:32s remains)
INFO - root - 2017-12-06 02:41:11.724704: step 2960, loss = 0.87, batch loss = 0.66 (17.4 examples/sec; 0.460 sec/batch; 42h:08m:27s remains)
INFO - root - 2017-12-06 02:41:16.251098: step 2970, loss = 0.86, batch loss = 0.65 (17.9 examples/sec; 0.446 sec/batch; 40h:48m:37s remains)
INFO - root - 2017-12-06 02:41:20.686378: step 2980, loss = 0.87, batch loss = 0.66 (16.8 examples/sec; 0.475 sec/batch; 43h:30m:42s remains)
INFO - root - 2017-12-06 02:41:25.179687: step 2990, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 41h:06m:49s remains)
INFO - root - 2017-12-06 02:41:29.619653: step 3000, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.443 sec/batch; 40h:32m:20s remains)
2017-12-06 02:41:30.014480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0010507405 -0.001352001 -0.0021292903 -0.0027062446 -0.003155861 -0.0035864562 -0.0040885359 -0.0048457012 -0.0064911172 -0.0095840357 -0.014108539 -0.019270599 -0.023903953 -0.027264494 -0.028460985][-0.00055575371 -0.000951156 -0.0020174496 -0.0027050041 -0.0030935742 -0.0035673119 -0.0043538585 -0.0054345392 -0.0075135157 -0.011422612 -0.017237017 -0.023642154 -0.029168632 -0.032662138 -0.033592708][-0.00053571165 -0.00096461549 -0.0021379516 -0.0028440654 -0.0032202564 -0.0037244186 -0.0045438595 -0.0058328547 -0.0082273 -0.012557939 -0.019108145 -0.026285008 -0.03247809 -0.036392938 -0.037426274][-0.00024430826 -0.0005524978 -0.0016513281 -0.0023059323 -0.0026690438 -0.0031315647 -0.0037863515 -0.0051145367 -0.0076653957 -0.012162078 -0.018906023 -0.026544299 -0.03377337 -0.038615026 -0.039978307][0.00019448623 0.00016489252 -0.00078253821 -0.0013260543 -0.0014903247 -0.0016958229 -0.0021096542 -0.0032933652 -0.0058083497 -0.010274984 -0.017134042 -0.025067417 -0.033202466 -0.038612291 -0.0402428][0.00082157552 0.000962466 0.00021979958 -0.00013658032 -8.5849315e-05 -3.5192817e-05 -0.00018650666 -0.0011081435 -0.0033442453 -0.0076501109 -0.014391363 -0.022297412 -0.030658364 -0.036367696 -0.0381993][0.0012279376 0.0016101785 0.001034949 0.00080069527 0.00085781515 0.0010788813 0.001184091 0.000602901 -0.0011931509 -0.0050735623 -0.011163007 -0.01826709 -0.025921637 -0.031353176 -0.033160925][0.0013665035 0.0019199848 0.0013818517 0.0010830984 0.0011121631 0.0014132373 0.0017798692 0.0016109534 0.00033833459 -0.0028039441 -0.0075274557 -0.012987733 -0.019007998 -0.02368737 -0.02563896][0.001287505 0.0019319393 0.0013823435 0.0010310002 0.0010440946 0.0013362132 0.0017723516 0.0019362904 0.0013458133 -0.00078967959 -0.0040074512 -0.007722009 -0.012032967 -0.015632909 -0.017115798][0.0012830161 0.0018498488 0.0013888553 0.0011081547 0.001126904 0.0013441406 0.0017442964 0.0020696968 0.0020916536 0.0010754503 -0.00075043738 -0.0030286312 -0.0056852065 -0.0077960342 -0.0086988062][0.0011553951 0.0018396266 0.0015549064 0.0014018118 0.0013955086 0.0014819913 0.0017334446 0.0019958206 0.0022278391 0.0020479597 0.0012673065 6.9450587e-05 -0.0013062544 -0.0023827925 -0.0029719397][0.0009840019 0.0016308837 0.0014533848 0.0013432093 0.0012793988 0.0012727603 0.00137382 0.0015310384 0.0016733631 0.0017761625 0.0016152821 0.0011676028 0.00055647641 5.8971345e-06 -0.00041968375][0.0007535331 0.001373224 0.0012669079 0.001181718 0.0011181347 0.0011000149 0.0011529401 0.0012323931 0.0012789182 0.0013357848 0.0013272651 0.0012108423 0.0010035113 0.00078681484 0.00057003647][0.00055893883 0.0012313947 0.0011663549 0.0011124872 0.0010674223 0.0010605566 0.0010760538 0.0011028759 0.0011147112 0.001145035 0.0012099557 0.0012290068 0.0011941232 0.0011652298 0.0010616854][0.00023161992 0.00087987259 0.00085614994 0.00083106384 0.00080234557 0.00078045949 0.0007731542 0.0007811524 0.00080631673 0.00084027275 0.000937745 0.0010307729 0.0010965131 0.0010895357 0.00099119172]]...]
INFO - root - 2017-12-06 02:41:34.592872: step 3010, loss = 0.89, batch loss = 0.68 (18.3 examples/sec; 0.437 sec/batch; 39h:59m:24s remains)
INFO - root - 2017-12-06 02:41:39.156226: step 3020, loss = 0.91, batch loss = 0.70 (17.4 examples/sec; 0.460 sec/batch; 42h:06m:10s remains)
INFO - root - 2017-12-06 02:41:43.703128: step 3030, loss = 0.88, batch loss = 0.67 (17.4 examples/sec; 0.459 sec/batch; 42h:00m:50s remains)
INFO - root - 2017-12-06 02:41:48.196697: step 3040, loss = 0.86, batch loss = 0.65 (18.2 examples/sec; 0.438 sec/batch; 40h:07m:07s remains)
INFO - root - 2017-12-06 02:41:52.725442: step 3050, loss = 0.90, batch loss = 0.69 (16.7 examples/sec; 0.478 sec/batch; 43h:45m:35s remains)
INFO - root - 2017-12-06 02:41:57.258288: step 3060, loss = 0.90, batch loss = 0.69 (17.3 examples/sec; 0.463 sec/batch; 42h:20m:17s remains)
INFO - root - 2017-12-06 02:42:01.749461: step 3070, loss = 0.87, batch loss = 0.66 (18.2 examples/sec; 0.439 sec/batch; 40h:10m:19s remains)
INFO - root - 2017-12-06 02:42:06.202976: step 3080, loss = 0.86, batch loss = 0.65 (18.6 examples/sec; 0.430 sec/batch; 39h:21m:29s remains)
INFO - root - 2017-12-06 02:42:10.661677: step 3090, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:55m:55s remains)
INFO - root - 2017-12-06 02:42:15.222664: step 3100, loss = 0.90, batch loss = 0.69 (17.5 examples/sec; 0.458 sec/batch; 41h:53m:33s remains)
2017-12-06 02:42:15.727015: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.040498488 0.041293629 0.043424122 0.035858713 0.034108832 0.0314401 0.028490424 0.026927412 0.02867008 0.034495018 0.035560049 0.036590658 0.035457358 0.02703087 0.018329009][0.05448246 0.062898293 0.073552974 0.072436489 0.067341268 0.061877921 0.056094617 0.046839483 0.041082293 0.04019171 0.046428405 0.05042693 0.052582026 0.045047544 0.036532491][0.074940234 0.082212217 0.0886003 0.089755394 0.085351281 0.08102686 0.075513542 0.062884271 0.055068739 0.053008921 0.061355464 0.068580665 0.076171644 0.071637556 0.065729693][0.11456612 0.12386449 0.12243495 0.12250061 0.11645358 0.11191084 0.10469342 0.09110301 0.080454521 0.076066934 0.082859315 0.091471128 0.10564253 0.10852731 0.1134193][0.18675697 0.19794035 0.20163995 0.20035523 0.19528237 0.1909011 0.18275505 0.16645384 0.1491808 0.14638424 0.15443519 0.16423944 0.18394551 0.19264615 0.21063673][0.29051706 0.30898985 0.31073141 0.31393188 0.31693786 0.31579289 0.30994433 0.29663065 0.27945632 0.27456585 0.27664334 0.28408402 0.30381349 0.31301597 0.335525][0.43461058 0.43387797 0.4251045 0.42148161 0.42799813 0.42737621 0.42367142 0.41158742 0.39301059 0.38596979 0.3804183 0.38008893 0.3941803 0.409963 0.425747][0.5304563 0.50860178 0.49461952 0.48596802 0.48921618 0.48500636 0.48324648 0.4713873 0.45240834 0.44166508 0.43039 0.423888 0.43031189 0.44010559 0.43875712][0.52517748 0.48971882 0.47752044 0.46294203 0.45870772 0.45364925 0.4526374 0.44106615 0.4271414 0.42010272 0.41191626 0.39906743 0.39504346 0.39964563 0.37662286][0.41892552 0.35351753 0.34189591 0.32944384 0.32437056 0.32575321 0.33404344 0.33013692 0.32452667 0.31850457 0.30988127 0.29491612 0.28432366 0.28556061 0.24583024][0.31211826 0.21991718 0.21054947 0.2024053 0.18727779 0.19000784 0.19694749 0.19788039 0.19738105 0.1915791 0.18305844 0.16730165 0.16952941 0.15233946 0.10931047][0.1878241 0.097714029 0.10447908 0.103096 0.095837735 0.099208616 0.10424078 0.10407437 0.099692591 0.090663232 0.080084465 0.060981855 0.072558023 0.039800361 0.0014581978][0.073759489 -0.0083806962 0.0023512505 0.010623302 0.012003634 0.021161109 0.028475262 0.020260602 0.0084009767 -0.0092878342 -0.02459302 -0.03999541 -0.01300133 -0.048962165 -0.073933087][-0.0017361455 -0.072452068 -0.063025407 -0.054267712 -0.048194826 -0.045550238 -0.041943755 -0.048944749 -0.059980735 -0.088354588 -0.11791369 -0.13892038 -0.11661115 -0.14888945 -0.14542794][-0.1177437 -0.17234825 -0.16892064 -0.15949304 -0.15468849 -0.15357158 -0.15406236 -0.16085467 -0.17107743 -0.18904793 -0.20679298 -0.22427458 -0.21272385 -0.22837663 -0.20757952]]...]
INFO - root - 2017-12-06 02:42:20.085973: step 3110, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.448 sec/batch; 40h:56m:46s remains)
INFO - root - 2017-12-06 02:42:24.681990: step 3120, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 41h:06m:28s remains)
INFO - root - 2017-12-06 02:42:29.242895: step 3130, loss = 0.92, batch loss = 0.71 (16.9 examples/sec; 0.474 sec/batch; 43h:23m:48s remains)
INFO - root - 2017-12-06 02:42:33.740417: step 3140, loss = 0.84, batch loss = 0.63 (17.7 examples/sec; 0.453 sec/batch; 41h:26m:01s remains)
INFO - root - 2017-12-06 02:42:38.213103: step 3150, loss = 0.88, batch loss = 0.67 (17.2 examples/sec; 0.464 sec/batch; 42h:29m:20s remains)
INFO - root - 2017-12-06 02:42:42.742324: step 3160, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 41h:29m:32s remains)
INFO - root - 2017-12-06 02:42:47.192647: step 3170, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.451 sec/batch; 41h:18m:04s remains)
INFO - root - 2017-12-06 02:42:51.810607: step 3180, loss = 0.89, batch loss = 0.68 (16.4 examples/sec; 0.489 sec/batch; 44h:45m:06s remains)
INFO - root - 2017-12-06 02:42:56.398422: step 3190, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:20m:28s remains)
INFO - root - 2017-12-06 02:43:00.896447: step 3200, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.451 sec/batch; 41h:14m:59s remains)
2017-12-06 02:43:01.414994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040650919 -0.040984794 -0.041184779 -0.041163445 -0.040975008 -0.040641535 -0.040393859 -0.040231265 -0.040204708 -0.04029081 -0.040385902 -0.040444255 -0.040379509 -0.040096559 -0.039607611][-0.040529985 -0.040959857 -0.041309137 -0.04138213 -0.041231386 -0.040947109 -0.040892489 -0.040899638 -0.041005392 -0.041207541 -0.041378692 -0.041377492 -0.041095674 -0.04050681 -0.039671097][-0.040435839 -0.041065969 -0.041513663 -0.041704156 -0.041780617 -0.041786786 -0.041978162 -0.042257816 -0.042553034 -0.042814802 -0.042918611 -0.042629831 -0.041942429 -0.040972225 -0.039925054][-0.040432222 -0.041295491 -0.041935593 -0.042385615 -0.042700872 -0.042961031 -0.043263037 -0.04379255 -0.044287637 -0.044403803 -0.044207655 -0.043387264 -0.042129304 -0.040826514 -0.039771073][-0.040558517 -0.041696753 -0.042648304 -0.04333039 -0.043804966 -0.044134963 -0.044483766 -0.044935539 -0.045367625 -0.045257092 -0.044626653 -0.043194223 -0.041446321 -0.040098451 -0.039230127][-0.040562842 -0.0419613 -0.043179385 -0.043928456 -0.044323247 -0.044573296 -0.044722147 -0.044795703 -0.044819009 -0.044503164 -0.043431874 -0.041765034 -0.040270392 -0.039462451 -0.038947277][-0.040576596 -0.041897878 -0.04311135 -0.043826655 -0.044238135 -0.044365916 -0.044104021 -0.043723848 -0.043233726 -0.042454258 -0.040877581 -0.039227769 -0.03858963 -0.038759414 -0.038807191][-0.040538419 -0.04149818 -0.04244278 -0.043103296 -0.043464061 -0.043383181 -0.042909332 -0.042239189 -0.04154361 -0.040679082 -0.0394064 -0.038286388 -0.038240612 -0.038708515 -0.038804118][-0.040052868 -0.040668055 -0.04137899 -0.041860893 -0.042027179 -0.041863963 -0.041429803 -0.0410749 -0.040665656 -0.039966978 -0.03929957 -0.038782459 -0.038818955 -0.03897284 -0.038910031][-0.039413575 -0.03965896 -0.04022646 -0.040634442 -0.040907718 -0.040959969 -0.040781498 -0.040451705 -0.040180929 -0.039871164 -0.039458387 -0.038990155 -0.038732842 -0.03876197 -0.038728841][-0.038804255 -0.038793549 -0.03907764 -0.039459292 -0.039826777 -0.0399789 -0.039979231 -0.039755587 -0.039593488 -0.039473556 -0.039041981 -0.038532667 -0.038317792 -0.038383003 -0.038410179][-0.03835956 -0.038121447 -0.038265407 -0.038523853 -0.038718257 -0.038839813 -0.038932644 -0.038901057 -0.038840428 -0.038779967 -0.038466379 -0.038092583 -0.0379978 -0.03808751 -0.038172461][-0.038131617 -0.037927251 -0.038017448 -0.038104154 -0.0381485 -0.038147755 -0.038160656 -0.038162578 -0.038211748 -0.038215425 -0.038010594 -0.037846126 -0.037907682 -0.037998069 -0.038068105][-0.038186107 -0.038015421 -0.037976127 -0.037983634 -0.037984479 -0.037922595 -0.037879582 -0.037883054 -0.037931792 -0.037936322 -0.037840977 -0.037802797 -0.037900597 -0.037979811 -0.038044676][-0.038200006 -0.03809854 -0.038072743 -0.038053587 -0.038016178 -0.037961259 -0.037936721 -0.037919462 -0.037928071 -0.037945937 -0.037903264 -0.037888151 -0.037941627 -0.038025372 -0.038062606]]...]
INFO - root - 2017-12-06 02:43:05.712911: step 3210, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.443 sec/batch; 40h:31m:34s remains)
INFO - root - 2017-12-06 02:43:10.282985: step 3220, loss = 0.89, batch loss = 0.68 (16.6 examples/sec; 0.481 sec/batch; 43h:58m:08s remains)
INFO - root - 2017-12-06 02:43:14.729026: step 3230, loss = 0.90, batch loss = 0.69 (17.8 examples/sec; 0.450 sec/batch; 41h:11m:22s remains)
INFO - root - 2017-12-06 02:43:19.284121: step 3240, loss = 0.89, batch loss = 0.68 (17.6 examples/sec; 0.456 sec/batch; 41h:40m:16s remains)
INFO - root - 2017-12-06 02:43:23.907338: step 3250, loss = 0.91, batch loss = 0.70 (17.5 examples/sec; 0.456 sec/batch; 41h:44m:57s remains)
INFO - root - 2017-12-06 02:43:28.403392: step 3260, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.452 sec/batch; 41h:19m:29s remains)
INFO - root - 2017-12-06 02:43:32.933804: step 3270, loss = 0.89, batch loss = 0.68 (17.2 examples/sec; 0.465 sec/batch; 42h:29m:14s remains)
INFO - root - 2017-12-06 02:43:37.436480: step 3280, loss = 0.91, batch loss = 0.70 (17.9 examples/sec; 0.447 sec/batch; 40h:52m:42s remains)
INFO - root - 2017-12-06 02:43:41.978308: step 3290, loss = 0.90, batch loss = 0.69 (17.3 examples/sec; 0.464 sec/batch; 42h:23m:32s remains)
INFO - root - 2017-12-06 02:43:46.491725: step 3300, loss = 0.87, batch loss = 0.66 (17.6 examples/sec; 0.456 sec/batch; 41h:40m:21s remains)
2017-12-06 02:43:46.944368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2965256 -0.40022177 -0.50463086 -0.5845136 -0.63856107 -0.67500609 -0.671038 -0.63313335 -0.55578852 -0.44567239 -0.33037215 -0.22606528 -0.15531878 -0.10753387 -0.075593635][-0.52326906 -0.61548609 -0.70026177 -0.747837 -0.7672658 -0.76858413 -0.73729074 -0.66523671 -0.54980534 -0.41821513 -0.2936902 -0.19268849 -0.12979905 -0.092418432 -0.075679123][-0.70749187 -0.76686347 -0.79231387 -0.78090054 -0.73338604 -0.663619 -0.57071227 -0.45391697 -0.30467543 -0.16776136 -0.076598316 -0.025992177 -0.019423787 -0.032472927 -0.053517204][-0.71837956 -0.69365531 -0.60203981 -0.45920536 -0.31722942 -0.18287534 -0.067575395 0.045973949 0.15784088 0.23014525 0.23617655 0.19801152 0.11447201 0.034285858 -0.024031606][-0.56936723 -0.45292288 -0.25731006 0.001765836 0.25416124 0.46171945 0.57091272 0.65033662 0.66754943 0.62827253 0.50544387 0.35368383 0.1923157 0.073313475 -0.0038556717][-0.2968159 -0.0673655 0.24786404 0.61033851 0.94733864 1.2399932 1.3376309 1.3088211 1.1779898 0.97845757 0.6886791 0.42058751 0.187406 0.056385607 -0.0019398071][-0.022781026 0.29488394 0.70365512 1.1621492 1.577724 1.8833869 1.9281816 1.813869 1.551878 1.2086567 0.79608667 0.44266921 0.1574693 0.021872178 -0.018242847][0.12036455 0.49046624 0.96880233 1.4672521 1.855318 2.1595349 2.207612 2.0045338 1.6222124 1.2185243 0.76560044 0.39426589 0.1191963 0.0019502379 -0.0098738][0.1453014 0.47488433 0.91735077 1.4077156 1.7958539 1.9940095 1.9844897 1.8183763 1.4862649 1.0559911 0.61623931 0.28325132 0.061141342 -0.012759876 0.00031544268][0.082371347 0.334402 0.68178606 1.0777602 1.4250921 1.6453389 1.6111364 1.4177833 1.1262877 0.77402359 0.45548004 0.18887639 0.029181138 -0.0193727 0.003381636][-0.0076158009 0.17157525 0.4098326 0.67295659 0.9425931 1.1396763 1.1502308 1.0119606 0.76665384 0.49719477 0.26309004 0.094259836 0.0024971738 -0.023476521 -0.010788411][-0.084798194 0.018578373 0.17495653 0.356894 0.53493118 0.67883348 0.72765267 0.65694195 0.49345571 0.29096803 0.13709095 0.031804152 -0.018845463 -0.039590139 -0.031646352][-0.12580267 -0.06100871 0.037516363 0.15002334 0.26641273 0.35168013 0.39036071 0.3888424 0.27814063 0.15116704 0.066347152 -0.0020738728 -0.024286309 -0.031959251 -0.030189831][-0.14957502 -0.11735354 -0.052935179 0.015385561 0.092516921 0.15326509 0.17358184 0.18931228 0.15272462 0.077549405 0.015400164 -0.012515571 -0.03162903 -0.030477475 -0.039299995][-0.12502506 -0.11662849 -0.078208029 -0.034866743 0.020097837 0.058013782 0.063253418 0.076290049 0.062351808 0.036095321 0.00015744939 -0.014790457 -0.02074603 -0.018896641 -0.031769536]]...]
INFO - root - 2017-12-06 02:43:51.615207: step 3310, loss = 0.89, batch loss = 0.67 (16.3 examples/sec; 0.491 sec/batch; 44h:53m:41s remains)
INFO - root - 2017-12-06 02:43:55.902258: step 3320, loss = 0.92, batch loss = 0.71 (18.0 examples/sec; 0.445 sec/batch; 40h:43m:28s remains)
INFO - root - 2017-12-06 02:44:00.505392: step 3330, loss = 0.88, batch loss = 0.67 (17.1 examples/sec; 0.469 sec/batch; 42h:50m:50s remains)
INFO - root - 2017-12-06 02:44:05.066235: step 3340, loss = 0.87, batch loss = 0.66 (18.1 examples/sec; 0.443 sec/batch; 40h:31m:08s remains)
INFO - root - 2017-12-06 02:44:09.493169: step 3350, loss = 0.91, batch loss = 0.70 (17.5 examples/sec; 0.458 sec/batch; 41h:54m:46s remains)
INFO - root - 2017-12-06 02:44:13.981145: step 3360, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 41h:04m:39s remains)
INFO - root - 2017-12-06 02:44:18.458173: step 3370, loss = 0.89, batch loss = 0.68 (18.6 examples/sec; 0.430 sec/batch; 39h:18m:20s remains)
INFO - root - 2017-12-06 02:44:23.118224: step 3380, loss = 0.90, batch loss = 0.69 (16.8 examples/sec; 0.477 sec/batch; 43h:35m:01s remains)
INFO - root - 2017-12-06 02:44:27.627172: step 3390, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.446 sec/batch; 40h:43m:48s remains)
INFO - root - 2017-12-06 02:44:32.185351: step 3400, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.463 sec/batch; 42h:20m:51s remains)
2017-12-06 02:44:32.610033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21874449 -0.28789797 -0.18788813 -0.073232643 -0.039597571 -0.065451488 -0.10176286 -0.11644755 -0.25066838 -0.35529491 -0.53293794 -0.73055995 -0.97234619 -1.1622629 -1.3120972][-0.38957524 -0.24848315 0.036190119 0.23299281 0.33121929 0.333648 0.31418562 0.33054945 0.17529002 -0.054307871 -0.28889984 -0.45094544 -0.53049225 -0.617755 -0.73890996][-0.056126777 0.1485464 0.5141933 0.7108103 0.8045764 0.91651785 1.0808017 1.2218305 1.1042545 0.82930195 0.46676546 0.17056233 -0.025735695 -0.055918481 -0.12278575][0.79241562 1.0034566 1.3062475 1.4443377 1.4009259 1.4352316 1.5505807 1.7385402 1.7758139 1.5775721 1.3206322 0.99724019 0.70193505 0.49580622 0.33638051][1.4866389 1.7920228 2.1867728 2.4112716 2.4105504 2.299094 2.1760697 2.1676681 2.1043327 1.9302926 1.6370353 1.3721913 1.197847 1.0356785 0.955878][1.82921 2.1297112 2.6516266 2.9933259 3.1407993 3.1164153 2.8851888 2.6649261 2.3614342 2.0537276 1.8184031 1.5298867 1.2294352 1.0677645 1.0806695][1.9306192 2.2786248 2.7975662 3.2104261 3.4779768 3.4396067 3.1335084 2.7024591 2.2427595 1.7768472 1.4830079 1.1558928 0.93887764 0.77114797 0.75784427][1.593572 2.0800409 2.6167467 2.9654875 3.1279123 3.1403422 2.93926 2.5169551 1.8917909 1.2822802 0.68972468 0.36679125 0.17652735 0.14724469 0.25524554][0.646377 1.1568736 1.7017127 2.1186306 2.3477123 2.3595827 2.2202263 1.9159093 1.3806312 0.68659478 -0.02784043 -0.587531 -0.98060149 -1.140748 -1.0256568][-0.2925207 0.059650633 0.45082068 0.83848327 1.1086609 1.1793734 1.0458236 0.7058906 0.18034744 -0.53264111 -1.2931548 -1.837808 -2.1638911 -2.4643323 -2.5021679][-1.1547426 -0.97609288 -0.7014603 -0.4237698 -0.21021992 -0.14005587 -0.23197554 -0.61931211 -1.2495563 -1.9830372 -2.7322743 -3.3368907 -3.7352076 -3.9167764 -3.8573508][-1.8907936 -1.9341739 -1.7612398 -1.6058017 -1.4312023 -1.4221848 -1.5296453 -1.9348394 -2.5903981 -3.3204632 -4.0248666 -4.6368294 -5.0703015 -5.2171116 -5.2276082][-2.5244555 -2.6532655 -2.6342337 -2.5798802 -2.5384488 -2.4465146 -2.4674149 -2.7833488 -3.256062 -3.9055777 -4.6478086 -5.2361541 -5.598928 -5.7520666 -5.6745572][-2.8420131 -3.1390977 -3.1862044 -3.1524932 -3.0248168 -2.9553609 -2.9342041 -3.1030881 -3.474864 -3.9316409 -4.5706325 -5.098691 -5.4773641 -5.668786 -5.5758338][-2.9574068 -3.1712005 -3.2093711 -3.230716 -3.1132672 -2.9835289 -2.91344 -2.9339182 -3.0936584 -3.4632208 -3.8572834 -4.2279339 -4.5864577 -4.7628865 -4.7692747]]...]
INFO - root - 2017-12-06 02:44:37.221723: step 3410, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.443 sec/batch; 40h:31m:07s remains)
INFO - root - 2017-12-06 02:44:41.571953: step 3420, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 41h:01m:06s remains)
INFO - root - 2017-12-06 02:44:46.034039: step 3430, loss = 0.85, batch loss = 0.64 (18.4 examples/sec; 0.434 sec/batch; 39h:38m:36s remains)
INFO - root - 2017-12-06 02:44:50.614143: step 3440, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.453 sec/batch; 41h:23m:10s remains)
INFO - root - 2017-12-06 02:44:55.167902: step 3450, loss = 0.94, batch loss = 0.73 (17.8 examples/sec; 0.450 sec/batch; 41h:07m:27s remains)
INFO - root - 2017-12-06 02:44:59.659922: step 3460, loss = 0.92, batch loss = 0.71 (17.6 examples/sec; 0.454 sec/batch; 41h:27m:03s remains)
INFO - root - 2017-12-06 02:45:04.158727: step 3470, loss = 0.87, batch loss = 0.66 (16.7 examples/sec; 0.480 sec/batch; 43h:52m:31s remains)
INFO - root - 2017-12-06 02:45:08.736237: step 3480, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.461 sec/batch; 42h:09m:12s remains)
INFO - root - 2017-12-06 02:45:13.278980: step 3490, loss = 0.89, batch loss = 0.68 (17.3 examples/sec; 0.462 sec/batch; 42h:10m:40s remains)
INFO - root - 2017-12-06 02:45:17.711264: step 3500, loss = 0.90, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 40h:07m:57s remains)
2017-12-06 02:45:18.173948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022559661 -0.021912508 -0.021639315 -0.021544255 -0.021380356 -0.02118036 -0.021134693 -0.021200959 -0.021192335 -0.021091606 -0.021061955 -0.02102612 -0.020975189 -0.020949969 -0.020984821][-0.023097085 -0.022403099 -0.022069711 -0.021900715 -0.021660503 -0.021314275 -0.021157047 -0.021166015 -0.021126118 -0.021029597 -0.021009177 -0.020941541 -0.020925704 -0.021001095 -0.021017207][-0.023725098 -0.023041416 -0.02268319 -0.022404965 -0.021989167 -0.02147365 -0.021205682 -0.021088637 -0.020958718 -0.020839626 -0.020792488 -0.02073439 -0.020807462 -0.020953761 -0.02101673][-0.023906091 -0.0232253 -0.022858879 -0.022421267 -0.021807143 -0.021163048 -0.020771423 -0.020555016 -0.020343352 -0.020206332 -0.0201669 -0.020184783 -0.020349326 -0.020604672 -0.020788163][-0.023774013 -0.023070266 -0.022674935 -0.022044959 -0.021215316 -0.020452816 -0.019950191 -0.019654814 -0.019418716 -0.019294355 -0.019353602 -0.019528888 -0.01983083 -0.020203035 -0.020554533][-0.02340035 -0.022629535 -0.022120902 -0.021330936 -0.020415684 -0.019626502 -0.019094568 -0.018775379 -0.018543536 -0.018479444 -0.01865156 -0.019040043 -0.019568695 -0.020109862 -0.020624304][-0.022868332 -0.022102404 -0.021526769 -0.020712357 -0.019869961 -0.019180533 -0.018671313 -0.018333372 -0.01811891 -0.018130071 -0.018419646 -0.018996108 -0.019734222 -0.020460786 -0.021090129][-0.022391388 -0.021587798 -0.021047905 -0.020370753 -0.019719657 -0.019179871 -0.018718617 -0.018383294 -0.018203884 -0.018267259 -0.018658541 -0.019402193 -0.020332512 -0.021181811 -0.021927284][-0.021902878 -0.021133175 -0.020748958 -0.020285973 -0.0198313 -0.019440662 -0.01907064 -0.01880843 -0.018675948 -0.018801071 -0.019234557 -0.020013548 -0.020953786 -0.021806778 -0.022631932][-0.021561731 -0.020881433 -0.020635702 -0.020370636 -0.020106787 -0.019875733 -0.019633988 -0.019473871 -0.01942442 -0.019608762 -0.020027723 -0.020753646 -0.021691678 -0.022545943 -0.023331657][-0.021412788 -0.020836154 -0.020683656 -0.020555442 -0.020449897 -0.020364318 -0.020207649 -0.020128073 -0.020158876 -0.020344891 -0.020694081 -0.021345582 -0.02218771 -0.022939609 -0.023610037][-0.021521732 -0.020971382 -0.020890493 -0.020853719 -0.020844253 -0.020860277 -0.020786392 -0.020755779 -0.020802174 -0.020912701 -0.021147953 -0.021635281 -0.022285329 -0.022849269 -0.023349807][-0.021690104 -0.021169456 -0.021116346 -0.021145154 -0.021199062 -0.021244068 -0.021195887 -0.021166062 -0.021218358 -0.021303941 -0.021459114 -0.021743473 -0.022125863 -0.022439217 -0.022709005][-0.021977268 -0.021483436 -0.021453414 -0.021518692 -0.021588355 -0.021634551 -0.021572772 -0.021513214 -0.021534476 -0.02157053 -0.02163111 -0.021721926 -0.021843273 -0.021922097 -0.021959007][-0.022535374 -0.022058241 -0.022044023 -0.022099761 -0.022146735 -0.022171162 -0.022113062 -0.022055561 -0.022025676 -0.021918822 -0.021807596 -0.021710202 -0.021654522 -0.021570532 -0.021474238]]...]
INFO - root - 2017-12-06 02:45:22.655082: step 3510, loss = 0.90, batch loss = 0.69 (17.9 examples/sec; 0.446 sec/batch; 40h:47m:39s remains)
INFO - root - 2017-12-06 02:45:27.038540: step 3520, loss = 0.91, batch loss = 0.70 (24.2 examples/sec; 0.331 sec/batch; 30h:14m:50s remains)
INFO - root - 2017-12-06 02:45:31.619306: step 3530, loss = 0.90, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:32m:37s remains)
INFO - root - 2017-12-06 02:45:36.141108: step 3540, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.440 sec/batch; 40h:14m:03s remains)
INFO - root - 2017-12-06 02:45:40.679348: step 3550, loss = 0.81, batch loss = 0.60 (17.0 examples/sec; 0.471 sec/batch; 43h:02m:37s remains)
INFO - root - 2017-12-06 02:45:45.221919: step 3560, loss = 0.91, batch loss = 0.70 (18.3 examples/sec; 0.437 sec/batch; 39h:53m:43s remains)
INFO - root - 2017-12-06 02:45:49.730279: step 3570, loss = 0.86, batch loss = 0.64 (17.6 examples/sec; 0.456 sec/batch; 41h:37m:20s remains)
INFO - root - 2017-12-06 02:45:54.221329: step 3580, loss = 0.87, batch loss = 0.66 (18.1 examples/sec; 0.443 sec/batch; 40h:28m:10s remains)
INFO - root - 2017-12-06 02:45:58.699339: step 3590, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.440 sec/batch; 40h:10m:35s remains)
INFO - root - 2017-12-06 02:46:03.282366: step 3600, loss = 0.88, batch loss = 0.67 (17.0 examples/sec; 0.470 sec/batch; 42h:54m:13s remains)
2017-12-06 02:46:03.754655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2965076 -0.31759912 -0.36311832 -0.40913945 -0.38642988 -0.34761375 -0.31124869 -0.29006723 -0.30058086 -0.34802213 -0.38252947 -0.40180373 -0.4083164 -0.43438765 -0.46843728][-0.20541942 -0.22065471 -0.24117731 -0.24557376 -0.17602348 -0.15725985 -0.1686489 -0.22606011 -0.27070111 -0.34079105 -0.4015767 -0.44222048 -0.44193617 -0.45754689 -0.50580871][-0.13701302 -0.17578571 -0.18675399 -0.15667206 -0.01652297 0.098538309 0.072825342 -0.024924967 -0.088480115 -0.19313845 -0.26589313 -0.31669644 -0.3508718 -0.38253763 -0.44264185][0.077175558 0.080591962 0.028797496 0.083930179 0.18859886 0.30468577 0.27960402 0.18470427 0.13802661 0.079430774 0.017676149 -0.051867429 -0.16982628 -0.23991238 -0.30365244][0.26769215 0.29321527 0.307344 0.412462 0.53601223 0.64701194 0.58375382 0.48117378 0.43458351 0.39457196 0.31652179 0.2382326 0.12213802 0.03403132 -0.094558015][0.40989375 0.37209424 0.49173746 0.705127 0.8704294 1.0345544 1.0141578 0.93047315 0.85813034 0.79899734 0.6911552 0.60997367 0.46196994 0.36941811 0.20560424][0.65511328 0.60443693 0.72857058 0.92071742 1.0772477 1.2909188 1.3390523 1.325533 1.2875341 1.2287909 1.0664368 0.9352259 0.81268597 0.65401143 0.45736554][0.837541 0.81793827 0.95752496 1.1270858 1.2868421 1.5079024 1.5860103 1.6347083 1.6386533 1.5902803 1.418704 1.2393916 1.0884079 0.87973064 0.61053324][0.98090369 0.95563847 1.1116296 1.2686116 1.435726 1.6285586 1.7628949 1.8754058 1.9269775 1.8737839 1.6584905 1.4311588 1.2219442 0.98788053 0.70250934][1.2511152 1.2249627 1.3308082 1.4018056 1.4960761 1.7056469 1.9013389 2.0216599 2.0877211 2.0558646 1.8018184 1.5136423 1.2737075 0.97955209 0.60018241][1.3781614 1.3714678 1.4751104 1.579469 1.6665071 1.6968197 1.7017972 1.7652466 1.8393517 1.8923768 1.7316768 1.4574264 1.1811242 0.78806382 0.44042024][1.4440206 1.2241861 1.195955 1.2146577 1.3009888 1.4324703 1.5899448 1.6274428 1.613222 1.6571172 1.48975 1.2692596 0.98967093 0.5438506 0.094490781][1.1203438 0.96125883 0.94127971 0.88617533 0.92408854 0.97888094 1.0635178 1.1270764 1.1211618 1.1562436 1.0338088 0.8900609 0.60866052 0.12207387 -0.36227572][0.67694086 0.49679145 0.52327603 0.45248106 0.4414576 0.4972966 0.60639769 0.67305475 0.67565948 0.64315861 0.4445428 0.26862726 -0.046290439 -0.53271633 -0.96149063][0.13902763 -0.19275734 -0.26499838 -0.21221845 -0.137088 -0.052617826 -0.0024878941 0.041551705 0.041169595 0.040539328 -0.068723962 -0.23599432 -0.50829792 -0.98903579 -1.4943678]]...]
INFO - root - 2017-12-06 02:46:08.248330: step 3610, loss = 0.89, batch loss = 0.67 (17.6 examples/sec; 0.455 sec/batch; 41h:32m:58s remains)
INFO - root - 2017-12-06 02:46:12.861566: step 3620, loss = 0.89, batch loss = 0.68 (17.5 examples/sec; 0.457 sec/batch; 41h:43m:43s remains)
INFO - root - 2017-12-06 02:46:17.215419: step 3630, loss = 0.84, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 41h:04m:01s remains)
INFO - root - 2017-12-06 02:46:21.764700: step 3640, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 41h:15m:50s remains)
INFO - root - 2017-12-06 02:46:26.246353: step 3650, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.461 sec/batch; 42h:06m:15s remains)
INFO - root - 2017-12-06 02:46:30.810454: step 3660, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.446 sec/batch; 40h:42m:59s remains)
INFO - root - 2017-12-06 02:46:35.313566: step 3670, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.448 sec/batch; 40h:54m:02s remains)
INFO - root - 2017-12-06 02:46:39.813441: step 3680, loss = 0.89, batch loss = 0.68 (16.7 examples/sec; 0.479 sec/batch; 43h:42m:22s remains)
INFO - root - 2017-12-06 02:46:44.359814: step 3690, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.451 sec/batch; 41h:10m:53s remains)
INFO - root - 2017-12-06 02:46:48.802443: step 3700, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 41h:28m:14s remains)
2017-12-06 02:46:49.282902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032637149 -0.032552131 -0.032432131 -0.032322161 -0.032363243 -0.032431394 -0.032455109 -0.032560855 -0.032656383 -0.032687496 -0.032804884 -0.033082284 -0.033338521 -0.033349931 -0.0332601][-0.03173111 -0.031554312 -0.031347938 -0.031221729 -0.031123631 -0.030992305 -0.030851074 -0.030813912 -0.030869465 -0.030986415 -0.031224871 -0.031658482 -0.032129653 -0.032319874 -0.032287583][-0.031190082 -0.030998567 -0.030839736 -0.030664921 -0.030377625 -0.030050136 -0.029605553 -0.029270183 -0.029219378 -0.029436344 -0.029850151 -0.030459534 -0.03109796 -0.031427883 -0.03143923][-0.030501522 -0.029823205 -0.029416041 -0.029086867 -0.028906265 -0.028717564 -0.027781075 -0.026905715 -0.027026948 -0.027753631 -0.028692978 -0.029631916 -0.030345954 -0.030617524 -0.03058913][-0.025601044 -0.02496572 -0.021810448 -0.016227268 -0.011580627 -0.009083569 -0.0055268519 -0.00161447 -0.0025705248 -0.0098149516 -0.0195035 -0.026344959 -0.029412132 -0.02992506 -0.029961495][-0.025372211 -0.018847328 -0.0014923289 0.02406653 0.046736307 0.061472848 0.070054874 0.069572128 0.054727085 0.028466173 0.0006047301 -0.016526286 -0.022326851 -0.025070578 -0.028113965][-0.027741987 -0.012486827 0.022241123 0.070429347 0.1117554 0.13893872 0.15454718 0.14506012 0.10832721 0.060173295 0.012767956 -0.01447726 -0.020495454 -0.020678954 -0.022776961][-0.036966838 -0.025786908 0.01123935 0.067298874 0.11846137 0.15061872 0.16061133 0.13984933 0.089280024 0.036728255 -0.0062082708 -0.025113882 -0.02550292 -0.024165055 -0.025320647][-0.05269194 -0.053672142 -0.037560023 -0.0058168657 0.028524674 0.050297953 0.051738784 0.035597995 0.0038464107 -0.021691551 -0.039466087 -0.040269509 -0.033995286 -0.030255897 -0.029640427][-0.070384622 -0.087513134 -0.098870352 -0.097613439 -0.086474814 -0.076924548 -0.07919737 -0.0783664 -0.076305591 -0.064325377 -0.055112775 -0.043502372 -0.037796307 -0.035637066 -0.034644827][-0.075883031 -0.097632773 -0.12050408 -0.138944 -0.14783463 -0.14736101 -0.14236191 -0.12388024 -0.099404156 -0.067837566 -0.05169211 -0.041850232 -0.037884038 -0.038361724 -0.038231134][-0.0654078 -0.080915347 -0.10017115 -0.11920606 -0.13281554 -0.13414435 -0.12444546 -0.10244785 -0.077181637 -0.053138979 -0.042799789 -0.038649008 -0.038565651 -0.039505992 -0.039622903][-0.057818774 -0.068714745 -0.081546456 -0.094477028 -0.10398327 -0.10170889 -0.091004819 -0.073137738 -0.057730589 -0.045253 -0.040706381 -0.039365631 -0.039348628 -0.039331708 -0.039236419][-0.051030178 -0.057153862 -0.063307248 -0.067982465 -0.06923335 -0.065296076 -0.059063166 -0.051268946 -0.047393475 -0.041679792 -0.040196806 -0.03882768 -0.038378261 -0.038105324 -0.037692621][-0.044760689 -0.047728978 -0.050008841 -0.051118966 -0.050758742 -0.048788965 -0.04501541 -0.041089319 -0.039913464 -0.037514325 -0.037383437 -0.036809489 -0.036233887 -0.036049303 -0.035858944]]...]
INFO - root - 2017-12-06 02:46:53.684164: step 3710, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 40h:26m:12s remains)
INFO - root - 2017-12-06 02:46:58.240364: step 3720, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 41h:12m:05s remains)
INFO - root - 2017-12-06 02:47:02.695170: step 3730, loss = 0.87, batch loss = 0.66 (15.7 examples/sec; 0.510 sec/batch; 46h:34m:09s remains)
INFO - root - 2017-12-06 02:47:07.398001: step 3740, loss = 0.91, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:30m:47s remains)
INFO - root - 2017-12-06 02:47:11.924513: step 3750, loss = 0.87, batch loss = 0.66 (17.1 examples/sec; 0.469 sec/batch; 42h:50m:14s remains)
INFO - root - 2017-12-06 02:47:16.525969: step 3760, loss = 0.86, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 41h:45m:46s remains)
INFO - root - 2017-12-06 02:47:21.080825: step 3770, loss = 0.89, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 41h:06m:07s remains)
INFO - root - 2017-12-06 02:47:25.595643: step 3780, loss = 0.88, batch loss = 0.67 (17.5 examples/sec; 0.456 sec/batch; 41h:40m:41s remains)
INFO - root - 2017-12-06 02:47:30.125809: step 3790, loss = 0.89, batch loss = 0.68 (18.4 examples/sec; 0.435 sec/batch; 39h:44m:34s remains)
INFO - root - 2017-12-06 02:47:34.553932: step 3800, loss = 0.90, batch loss = 0.68 (17.6 examples/sec; 0.455 sec/batch; 41h:34m:23s remains)
2017-12-06 02:47:35.046227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.06673602 -0.084663287 -0.094840407 -0.099874988 -0.10317025 -0.10500502 -0.10522046 -0.10212489 -0.09886156 -0.094380274 -0.090514407 -0.093998313 -0.094334126 -0.089478552 -0.063252248][-0.0644359 -0.078412935 -0.08631736 -0.087573647 -0.082819268 -0.079111144 -0.075679436 -0.076487117 -0.076011211 -0.074477956 -0.06849964 -0.068019688 -0.070954673 -0.074612334 -0.054936673][-0.048414428 -0.055101886 -0.058767688 -0.064742506 -0.0655784 -0.062744662 -0.058299109 -0.056993216 -0.054818526 -0.0527183 -0.050296389 -0.051707443 -0.055299174 -0.065080523 -0.056562938][-0.033095427 -0.032247648 -0.02408685 -0.018249858 -0.010867815 -0.01002866 -0.01352739 -0.022604031 -0.02902068 -0.031169942 -0.038358215 -0.045325715 -0.05399926 -0.0676432 -0.066166252][-0.019716118 -0.010386989 0.0063208193 0.026204016 0.049364682 0.062353451 0.069383442 0.05599625 0.034122635 0.010596123 -0.012120746 -0.026431605 -0.038286328 -0.057410624 -0.071367592][-0.0065895543 0.0094429553 0.030455451 0.064329028 0.10667589 0.13818225 0.1621401 0.15697594 0.12560888 0.082269654 0.042454962 0.015568305 -0.0070643798 -0.035019878 -0.061558411][0.0086063966 0.029203329 0.050724629 0.08747682 0.14386326 0.20287119 0.25536445 0.26771706 0.23844545 0.18230829 0.13017538 0.094038829 0.061715204 0.02507728 -0.019123964][-0.0044013783 0.015175495 0.039856087 0.082618594 0.1554711 0.24930523 0.33763728 0.37121049 0.34478384 0.28455135 0.22491454 0.1791632 0.12975034 0.080608919 0.023559038][-0.026383694 -0.0080184266 0.018326666 0.061758641 0.13492373 0.23269348 0.3301912 0.38485861 0.37438992 0.31828335 0.25241932 0.19370656 0.13214116 0.079791367 0.020619344][-0.013807643 -0.0038142391 0.017560739 0.052124228 0.1059335 0.17666617 0.24657677 0.2907286 0.28133681 0.23313557 0.17899793 0.13468435 0.0883214 0.048172202 0.0010876209][0.031320628 0.037809219 0.049206149 0.05908848 0.078824326 0.10452649 0.12878363 0.13700499 0.11184634 0.065133452 0.021903832 -0.0045140684 -0.02399197 -0.033952445 -0.050652515][0.042758744 0.042657297 0.038069863 0.020094562 -0.0025848858 -0.028220985 -0.04942929 -0.069428712 -0.10118498 -0.13766925 -0.16155571 -0.16103464 -0.14858831 -0.12576857 -0.10370433][0.0024540052 -0.0015571937 -0.016816352 -0.05540647 -0.10916965 -0.16871671 -0.22001922 -0.25735956 -0.2875056 -0.3046166 -0.30068749 -0.27403298 -0.23455025 -0.18866323 -0.14431632][-0.047728226 -0.056966785 -0.074037291 -0.11075412 -0.16532977 -0.22639471 -0.28057915 -0.3174724 -0.33881813 -0.33878 -0.31764793 -0.27925986 -0.23098825 -0.17918868 -0.13330489][-0.06951642 -0.0798226 -0.093540639 -0.11803845 -0.15459271 -0.19652066 -0.23455012 -0.2586996 -0.26899114 -0.26425779 -0.24349852 -0.21240506 -0.17700988 -0.13850704 -0.10600349]]...]
INFO - root - 2017-12-06 02:47:39.534607: step 3810, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.444 sec/batch; 40h:29m:37s remains)
INFO - root - 2017-12-06 02:47:44.108065: step 3820, loss = 0.85, batch loss = 0.64 (16.8 examples/sec; 0.475 sec/batch; 43h:20m:52s remains)
INFO - root - 2017-12-06 02:47:48.646118: step 3830, loss = 0.84, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 41h:02m:58s remains)
INFO - root - 2017-12-06 02:47:53.111936: step 3840, loss = 0.86, batch loss = 0.65 (16.5 examples/sec; 0.485 sec/batch; 44h:18m:24s remains)
INFO - root - 2017-12-06 02:47:57.602918: step 3850, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.445 sec/batch; 40h:37m:59s remains)
INFO - root - 2017-12-06 02:48:02.195561: step 3860, loss = 0.88, batch loss = 0.67 (17.1 examples/sec; 0.469 sec/batch; 42h:48m:48s remains)
INFO - root - 2017-12-06 02:48:06.750978: step 3870, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.452 sec/batch; 41h:13m:48s remains)
INFO - root - 2017-12-06 02:48:11.241617: step 3880, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 40h:58m:01s remains)
INFO - root - 2017-12-06 02:48:15.790537: step 3890, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 41h:04m:13s remains)
INFO - root - 2017-12-06 02:48:20.288913: step 3900, loss = 0.85, batch loss = 0.64 (18.5 examples/sec; 0.432 sec/batch; 39h:25m:02s remains)
2017-12-06 02:48:20.716013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030508026 -0.030602513 -0.030737836 -0.030748975 -0.030690795 -0.03061863 -0.030637132 -0.030734478 -0.030903494 -0.031147636 -0.031344242 -0.03155183 -0.031750891 -0.031833507 -0.031646892][-0.030424349 -0.030395417 -0.030471725 -0.030406933 -0.030270072 -0.030215472 -0.03034848 -0.030562928 -0.030791644 -0.031125626 -0.031437993 -0.031756289 -0.031943765 -0.032019503 -0.031932145][-0.030613497 -0.030515332 -0.030529115 -0.030445561 -0.030354336 -0.030346164 -0.030453863 -0.030620629 -0.030774834 -0.03104496 -0.031316519 -0.031623688 -0.031851105 -0.032050461 -0.032139353][-0.030902795 -0.030753655 -0.030748775 -0.030647814 -0.030599019 -0.030616391 -0.030715127 -0.030871972 -0.030978832 -0.031105028 -0.03120189 -0.031355977 -0.031569585 -0.031876408 -0.032169953][-0.031657688 -0.031399738 -0.031356707 -0.03113961 -0.030947601 -0.030851178 -0.03091412 -0.031124776 -0.031262457 -0.03136307 -0.031297371 -0.031236576 -0.031310041 -0.031616177 -0.032064613][-0.032954238 -0.032482285 -0.032261685 -0.03191223 -0.031488582 -0.031204809 -0.031196252 -0.031429596 -0.031613577 -0.031729218 -0.031614289 -0.031351775 -0.031279743 -0.031550452 -0.032067686][-0.034675889 -0.033835679 -0.033369459 -0.0329741 -0.032536112 -0.032046944 -0.0318724 -0.031965941 -0.032075532 -0.032106355 -0.031968314 -0.031673528 -0.03156288 -0.031843342 -0.032399304][-0.037042122 -0.035692006 -0.034903754 -0.034434889 -0.033980358 -0.033339456 -0.032823317 -0.032534875 -0.032451373 -0.032399692 -0.032342546 -0.032246977 -0.03229253 -0.032663938 -0.033246215][-0.039996743 -0.03817147 -0.036909346 -0.036070164 -0.035354681 -0.034537189 -0.033715114 -0.033128578 -0.032765277 -0.032609917 -0.032675013 -0.032877684 -0.033215635 -0.033822618 -0.034581918][-0.042834237 -0.040393736 -0.038524345 -0.03715422 -0.036066182 -0.035111189 -0.034204416 -0.033498667 -0.032990374 -0.032709062 -0.032853261 -0.033352703 -0.034002654 -0.034858923 -0.035886377][-0.044977251 -0.041918177 -0.039483786 -0.037549656 -0.036083337 -0.03498558 -0.034005471 -0.0332328 -0.032667883 -0.032424212 -0.032678626 -0.03343527 -0.034406722 -0.035513818 -0.036815446][-0.046074852 -0.042515315 -0.039504912 -0.037135538 -0.035394102 -0.034180775 -0.033212535 -0.032478757 -0.031926751 -0.031726781 -0.032090388 -0.03307851 -0.0343108 -0.035596292 -0.037109323][-0.045377307 -0.041471142 -0.038246024 -0.035738267 -0.033971813 -0.032855846 -0.032002825 -0.031394638 -0.030902063 -0.030690923 -0.031067582 -0.032129053 -0.033542391 -0.035022818 -0.036580205][-0.042786792 -0.039061554 -0.036151353 -0.034024864 -0.032557204 -0.031675883 -0.030997235 -0.030493436 -0.030013215 -0.029658074 -0.029758774 -0.030650266 -0.032001343 -0.033519506 -0.035145734][-0.039146107 -0.03603192 -0.0338995 -0.032523323 -0.0316573 -0.031085566 -0.030662144 -0.030264098 -0.029712131 -0.029127384 -0.028841434 -0.029279308 -0.030291524 -0.031681284 -0.033240918]]...]
INFO - root - 2017-12-06 02:48:25.216111: step 3910, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.451 sec/batch; 41h:08m:51s remains)
INFO - root - 2017-12-06 02:48:29.678221: step 3920, loss = 0.86, batch loss = 0.65 (18.3 examples/sec; 0.438 sec/batch; 39h:58m:42s remains)
INFO - root - 2017-12-06 02:48:34.188711: step 3930, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.440 sec/batch; 40h:10m:02s remains)
INFO - root - 2017-12-06 02:48:38.494469: step 3940, loss = 0.86, batch loss = 0.65 (17.9 examples/sec; 0.447 sec/batch; 40h:45m:46s remains)
INFO - root - 2017-12-06 02:48:43.042491: step 3950, loss = 0.87, batch loss = 0.66 (18.3 examples/sec; 0.437 sec/batch; 39h:55m:37s remains)
INFO - root - 2017-12-06 02:48:47.518999: step 3960, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.451 sec/batch; 41h:08m:18s remains)
INFO - root - 2017-12-06 02:48:52.065046: step 3970, loss = 0.86, batch loss = 0.65 (17.0 examples/sec; 0.472 sec/batch; 43h:03m:04s remains)
INFO - root - 2017-12-06 02:48:56.593090: step 3980, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.447 sec/batch; 40h:45m:45s remains)
INFO - root - 2017-12-06 02:49:01.204992: step 3990, loss = 0.90, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:50m:02s remains)
INFO - root - 2017-12-06 02:49:05.696357: step 4000, loss = 0.90, batch loss = 0.69 (17.9 examples/sec; 0.448 sec/batch; 40h:52m:55s remains)
2017-12-06 02:49:06.151385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21988116 -0.22305939 -0.20719287 -0.21417345 -0.20182146 -0.17034622 -0.12157692 -0.1078371 -0.077604257 -0.046361569 -0.013974912 0.0032334402 0.014536437 0.028528389 0.022589784][-0.18737264 -0.21369448 -0.20201306 -0.19803752 -0.16406475 -0.10759195 -0.06676963 -0.052456766 -0.066496536 -0.057525139 -0.018742681 0.030250531 0.046474669 0.059037011 0.054208893][-0.04614085 -0.081413135 -0.081085674 -0.079164684 -0.047318418 -0.015345298 0.042147335 0.073404983 0.078975111 0.082848832 0.073269427 0.057176653 0.054738734 0.062161278 0.066313118][0.082264826 0.030258913 0.008039847 -0.0051120706 0.014468398 0.074200466 0.12747663 0.13905126 0.12180448 0.09901455 0.082509384 0.085890844 0.10449706 0.10233094 0.081139266][0.18542653 0.14274658 0.12350672 0.099270135 0.12111679 0.1730569 0.20857914 0.22388332 0.19688648 0.14225152 0.11875673 0.11517859 0.12491265 0.11162753 0.10963409][0.2799665 0.24331222 0.23773812 0.24459474 0.26943809 0.33331332 0.37798598 0.37804282 0.3347806 0.28505659 0.24470381 0.21367185 0.20712556 0.17697233 0.15103777][0.32878286 0.30653134 0.2973657 0.29068488 0.33337459 0.39255908 0.42986694 0.44998008 0.41835865 0.3935627 0.39308804 0.37934035 0.37472457 0.35788634 0.30341798][0.32697207 0.31798616 0.33115754 0.32824421 0.36810428 0.42243043 0.49303198 0.55512422 0.5656693 0.58003342 0.58559883 0.60072947 0.58615965 0.53248215 0.40683147][0.34127235 0.31696045 0.33392957 0.332254 0.37260702 0.420651 0.48426569 0.55986291 0.62122351 0.63810235 0.6546008 0.68244565 0.67330849 0.60745358 0.47644371][0.31620926 0.29110503 0.31777468 0.31441152 0.3253361 0.35602155 0.4251188 0.51978946 0.60163856 0.64685237 0.69422007 0.73139989 0.7048344 0.593 0.44668764][0.22165512 0.21517472 0.25687116 0.24674816 0.24979429 0.25029024 0.28323093 0.33299696 0.39279017 0.41757017 0.46190095 0.50882167 0.4960683 0.3940405 0.21544956][0.12849678 0.088171214 0.091911912 0.086210757 0.066897243 0.040870029 0.053103011 0.045721937 0.046848435 0.036661293 0.0536375 0.07575199 0.071882471 0.02254412 -0.11292874][-0.065484412 -0.083558448 -0.080296867 -0.1193655 -0.19263436 -0.24772763 -0.28436384 -0.3222017 -0.35698822 -0.39173859 -0.39817253 -0.3977631 -0.37903821 -0.39052525 -0.44595179][-0.2436334 -0.30271894 -0.33557525 -0.37707675 -0.47366163 -0.53775805 -0.60218358 -0.661344 -0.7340765 -0.79398781 -0.820977 -0.81717074 -0.79416806 -0.77421063 -0.75558788][-0.4101277 -0.4711248 -0.49962038 -0.57074594 -0.66121775 -0.72244489 -0.78649914 -0.84065133 -0.90171242 -0.94647318 -0.98390979 -1.013242 -1.0136646 -0.99931872 -0.956003]]...]
INFO - root - 2017-12-06 02:49:10.665761: step 4010, loss = 0.87, batch loss = 0.66 (16.1 examples/sec; 0.498 sec/batch; 45h:26m:43s remains)
INFO - root - 2017-12-06 02:49:15.175794: step 4020, loss = 0.86, batch loss = 0.65 (17.8 examples/sec; 0.450 sec/batch; 41h:01m:05s remains)
INFO - root - 2017-12-06 02:49:19.705181: step 4030, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.449 sec/batch; 40h:57m:34s remains)
INFO - root - 2017-12-06 02:49:24.012457: step 4040, loss = 0.87, batch loss = 0.66 (23.0 examples/sec; 0.348 sec/batch; 31h:45m:56s remains)
INFO - root - 2017-12-06 02:49:28.534689: step 4050, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 41h:27m:47s remains)
INFO - root - 2017-12-06 02:49:33.082384: step 4060, loss = 0.85, batch loss = 0.64 (18.3 examples/sec; 0.437 sec/batch; 39h:52m:33s remains)
INFO - root - 2017-12-06 02:49:37.518512: step 4070, loss = 0.94, batch loss = 0.73 (18.0 examples/sec; 0.444 sec/batch; 40h:29m:51s remains)
INFO - root - 2017-12-06 02:49:42.064306: step 4080, loss = 0.89, batch loss = 0.68 (16.9 examples/sec; 0.474 sec/batch; 43h:15m:57s remains)
INFO - root - 2017-12-06 02:49:46.535898: step 4090, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.455 sec/batch; 41h:28m:02s remains)
INFO - root - 2017-12-06 02:49:51.075041: step 4100, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.441 sec/batch; 40h:12m:36s remains)
2017-12-06 02:49:51.496916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.15171236 0.0068571828 0.059278037 0.18199666 0.32230839 0.39475107 0.45661041 0.52631247 0.55749142 0.46397081 0.34331107 0.24340801 0.18553038 0.144109 0.084169433][0.036421966 0.18807963 0.24339442 0.33904994 0.4952642 0.56941706 0.62595779 0.68081838 0.6850915 0.697087 0.57680511 0.42638651 0.31410703 0.19831716 0.048389036][0.17730345 0.3010287 0.44221878 0.5757888 0.76773763 0.76628476 0.7707476 0.75634491 0.68619722 0.69224215 0.56555122 0.46658173 0.40242481 0.2743533 0.14761053][0.2503525 0.28483671 0.35893002 0.47553965 0.65042967 0.71060443 0.77543581 0.78759408 0.74619621 0.64425278 0.48079374 0.37964708 0.31385687 0.18852691 0.081737608][0.2796216 0.28396854 0.30342302 0.41822794 0.5432682 0.58399945 0.62038392 0.66234785 0.67426807 0.56579947 0.40219703 0.25084761 0.11252874 -0.0067404509 -0.10499677][0.25458962 0.13959529 0.054961745 0.20081951 0.32097691 0.36671257 0.41752961 0.39797702 0.44893798 0.36891645 0.25315005 0.061753 -0.16016372 -0.2618382 -0.33005112][0.20765422 -0.033246327 -0.10137621 -0.0048720427 0.075976208 0.15921235 0.22147845 0.19391908 0.23842637 0.14905475 0.026933055 -0.14460282 -0.35187408 -0.4463197 -0.49246186][0.017942984 -0.13783897 -0.21855177 -0.15782169 -0.098916933 -0.042266075 0.006641455 0.03687733 0.074160948 0.030443858 -0.034319252 -0.18723561 -0.36008793 -0.491742 -0.60212338][-0.2853258 -0.45845616 -0.39706233 -0.31529945 -0.26088378 -0.19483364 -0.15940468 -0.10624147 -0.13368417 -0.1426785 -0.17873091 -0.32049444 -0.40416721 -0.530902 -0.6118421][-0.55673951 -0.51123196 -0.49106425 -0.36683673 -0.3308036 -0.23649397 -0.20517163 -0.16291972 -0.18719828 -0.19326684 -0.24495909 -0.33263633 -0.35132438 -0.45624536 -0.46418914][-0.60564369 -0.47716644 -0.37550151 -0.22536562 -0.28047675 -0.19758235 -0.19826019 -0.2043764 -0.19370677 -0.22439091 -0.31649572 -0.4122358 -0.46110359 -0.57919687 -0.6318115][-0.40416494 -0.39013165 -0.27893949 -0.15364684 -0.20523474 -0.1848643 -0.19916633 -0.20425805 -0.20721991 -0.23576978 -0.29721656 -0.38931215 -0.47819304 -0.58138973 -0.64606446][-0.016414762 -0.012473494 0.09706144 0.081902519 0.062734306 0.013106029 -0.015396971 -0.084677145 -0.18333291 -0.27079642 -0.43716568 -0.57158631 -0.71071804 -0.77343154 -0.80382371][0.31146523 0.28863731 0.36301625 0.29574519 0.20352875 0.12474059 0.10014299 0.048291642 -0.022654222 -0.12282175 -0.32067755 -0.48847988 -0.6224699 -0.70372027 -0.75228673][0.10852563 0.11460477 0.1980844 0.082816631 0.10414226 0.042381492 -0.028956151 -0.13267308 -0.18740092 -0.31344104 -0.50838441 -0.65969777 -0.724063 -0.78744406 -0.81762058]]...]
INFO - root - 2017-12-06 02:49:56.048684: step 4110, loss = 0.88, batch loss = 0.67 (16.6 examples/sec; 0.481 sec/batch; 43h:50m:53s remains)
INFO - root - 2017-12-06 02:50:00.547356: step 4120, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.442 sec/batch; 40h:21m:26s remains)
INFO - root - 2017-12-06 02:50:05.089531: step 4130, loss = 0.88, batch loss = 0.67 (17.1 examples/sec; 0.468 sec/batch; 42h:42m:14s remains)
INFO - root - 2017-12-06 02:50:09.576384: step 4140, loss = 0.83, batch loss = 0.62 (17.3 examples/sec; 0.463 sec/batch; 42h:14m:51s remains)
INFO - root - 2017-12-06 02:50:13.867563: step 4150, loss = 0.90, batch loss = 0.68 (18.2 examples/sec; 0.440 sec/batch; 40h:06m:46s remains)
INFO - root - 2017-12-06 02:50:18.382651: step 4160, loss = 0.88, batch loss = 0.67 (17.4 examples/sec; 0.460 sec/batch; 41h:57m:18s remains)
INFO - root - 2017-12-06 02:50:22.912252: step 4170, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.461 sec/batch; 42h:02m:29s remains)
INFO - root - 2017-12-06 02:50:27.409960: step 4180, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.444 sec/batch; 40h:32m:02s remains)
INFO - root - 2017-12-06 02:50:31.896305: step 4190, loss = 0.90, batch loss = 0.69 (18.1 examples/sec; 0.443 sec/batch; 40h:21m:37s remains)
INFO - root - 2017-12-06 02:50:36.424416: step 4200, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.437 sec/batch; 39h:52m:56s remains)
2017-12-06 02:50:36.912865: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54782659 0.49242246 0.44697315 0.43493778 0.41722929 0.40835455 0.4241221 0.46878129 0.528052 0.58810747 0.62082279 0.6374827 0.64905423 0.67160356 0.66099405][0.74688143 0.67766595 0.6275878 0.59891349 0.55109507 0.52196121 0.52879924 0.58457637 0.6884433 0.80291778 0.88999528 0.93359196 0.950137 0.95861566 0.931441][0.89408207 0.83256847 0.77519172 0.71303594 0.62751561 0.55944896 0.53312463 0.57479173 0.69659615 0.84497815 0.9824264 1.0885841 1.1474724 1.1520045 1.1122204][0.94199979 0.87176865 0.80039835 0.71249759 0.59852237 0.49929214 0.43064451 0.43689135 0.5433082 0.70329005 0.88517267 1.0256222 1.1269736 1.1800531 1.1448866][0.77882946 0.72019476 0.64598417 0.54609269 0.41771713 0.30561292 0.2320305 0.22738545 0.30423087 0.44091386 0.60592961 0.76316947 0.9040345 0.9845196 0.980551][0.43533981 0.3851088 0.29442769 0.18512738 0.060878027 -0.033169113 -0.0785363 -0.072013348 -0.0012116879 0.11824374 0.25820774 0.41707936 0.56734097 0.66998369 0.68412185][-0.03856872 -0.11224467 -0.20124687 -0.30808568 -0.42053455 -0.50878048 -0.524681 -0.48683223 -0.39200297 -0.27766839 -0.15321967 -0.012420237 0.1252024 0.23962571 0.24524398][-0.58558756 -0.67399615 -0.7636236 -0.85650814 -0.95249242 -1.0168358 -1.0152279 -0.96196193 -0.85288942 -0.72271585 -0.585515 -0.46312854 -0.34921092 -0.23563489 -0.21670628][-0.91410148 -1.0383493 -1.153015 -1.2440373 -1.3440392 -1.4106688 -1.4131274 -1.3673582 -1.2724955 -1.1309587 -0.96834058 -0.83183527 -0.71917158 -0.63176191 -0.62954605][-0.9907493 -1.1383216 -1.2686263 -1.3669728 -1.4880192 -1.5760741 -1.6133938 -1.6013578 -1.5401047 -1.4288018 -1.2556117 -1.1064965 -0.9907254 -0.94190818 -0.96642542][-0.87931776 -0.98323089 -1.1140471 -1.233811 -1.3827966 -1.5285136 -1.6260537 -1.6681404 -1.6371362 -1.5400966 -1.3847699 -1.2542607 -1.1460302 -1.1263938 -1.1646216][-0.63726664 -0.70166934 -0.7713654 -0.87038404 -1.0063442 -1.1610807 -1.2944515 -1.41219 -1.4708184 -1.4505361 -1.3743916 -1.2822818 -1.184003 -1.195874 -1.2420269][-0.32491311 -0.35291696 -0.39511091 -0.45070046 -0.55798823 -0.67603338 -0.78520483 -0.92042285 -1.0232816 -1.0710078 -1.068555 -1.0470947 -1.0188668 -1.0701288 -1.1391972][-0.052600734 -0.040459163 -0.055845089 -0.090326309 -0.1462889 -0.21497071 -0.29857203 -0.40824926 -0.51453286 -0.58830589 -0.62081444 -0.63776463 -0.64665556 -0.71444273 -0.79740369][0.10297415 0.13266745 0.13821828 0.12291117 0.090711042 0.054280307 0.0073787682 -0.058314249 -0.12427829 -0.16498211 -0.18471299 -0.20737599 -0.22351749 -0.30274341 -0.38657698]]...]
INFO - root - 2017-12-06 02:50:41.396034: step 4210, loss = 0.87, batch loss = 0.66 (17.6 examples/sec; 0.455 sec/batch; 41h:28m:18s remains)
INFO - root - 2017-12-06 02:50:45.947024: step 4220, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.446 sec/batch; 40h:38m:30s remains)
INFO - root - 2017-12-06 02:50:50.421345: step 4230, loss = 0.83, batch loss = 0.62 (18.6 examples/sec; 0.430 sec/batch; 39h:12m:36s remains)
INFO - root - 2017-12-06 02:50:54.987085: step 4240, loss = 0.86, batch loss = 0.65 (17.8 examples/sec; 0.450 sec/batch; 41h:01m:10s remains)
INFO - root - 2017-12-06 02:50:59.202367: step 4250, loss = 0.86, batch loss = 0.64 (17.9 examples/sec; 0.447 sec/batch; 40h:46m:24s remains)
INFO - root - 2017-12-06 02:51:03.683634: step 4260, loss = 0.87, batch loss = 0.66 (18.4 examples/sec; 0.434 sec/batch; 39h:34m:32s remains)
INFO - root - 2017-12-06 02:51:08.177314: step 4270, loss = 0.87, batch loss = 0.66 (17.4 examples/sec; 0.460 sec/batch; 41h:53m:50s remains)
INFO - root - 2017-12-06 02:51:12.817651: step 4280, loss = 0.83, batch loss = 0.62 (17.3 examples/sec; 0.463 sec/batch; 42h:12m:19s remains)
INFO - root - 2017-12-06 02:51:17.281884: step 4290, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.446 sec/batch; 40h:39m:28s remains)
INFO - root - 2017-12-06 02:51:21.772430: step 4300, loss = 0.91, batch loss = 0.70 (17.2 examples/sec; 0.466 sec/batch; 42h:26m:35s remains)
2017-12-06 02:51:22.207095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010694433 -0.010018114 -0.0099806 -0.010131892 -0.010417309 -0.010622591 -0.010366481 -0.0095193759 -0.0082753971 -0.0068369545 -0.0054500997 -0.004820589 -0.0047804452 -0.0053464845 -0.0070895031][-0.0099724233 -0.0091968551 -0.0092411 -0.0095557831 -0.010092024 -0.010454986 -0.010225061 -0.0092824847 -0.0079409219 -0.0063308179 -0.0046583451 -0.0038077869 -0.0036348887 -0.0041852817 -0.0058651604][-0.010077018 -0.0094112456 -0.0095997974 -0.010071307 -0.010782942 -0.011257425 -0.011048429 -0.0099880472 -0.0084012151 -0.0065207593 -0.004552789 -0.003400214 -0.0030499585 -0.0035640076 -0.005176682][-0.010177452 -0.0096108653 -0.009902 -0.010499071 -0.011360757 -0.011938736 -0.011744868 -0.010552462 -0.008673273 -0.0065592229 -0.0043984577 -0.0030364506 -0.0025167093 -0.002944164 -0.004430376][-0.010216054 -0.00972655 -0.01009959 -0.010771431 -0.011670072 -0.012243476 -0.011957698 -0.01062433 -0.0085899383 -0.0064610839 -0.0042916313 -0.0028580204 -0.002204936 -0.0024911016 -0.003809467][-0.010204095 -0.0096970946 -0.010065313 -0.010703254 -0.011501383 -0.011956602 -0.011525646 -0.010176085 -0.0082268193 -0.0063175336 -0.0042808652 -0.0028515197 -0.0021004975 -0.0021919049 -0.003305424][-0.010002878 -0.0095053129 -0.0097979829 -0.010298159 -0.010888506 -0.011181239 -0.0106727 -0.0093694627 -0.0076064095 -0.0059078857 -0.0040840544 -0.0027651563 -0.002012562 -0.00199816 -0.00314207][-0.0098163038 -0.0091228969 -0.0092568472 -0.009526819 -0.009848401 -0.0099959895 -0.0095138736 -0.0083357729 -0.0067338757 -0.0052229948 -0.0037480257 -0.0027238503 -0.0021181814 -0.0021755956 -0.0034850612][-0.0095955878 -0.0085124932 -0.0083042644 -0.0081875809 -0.00815459 -0.00812446 -0.0077774376 -0.0069876947 -0.0057316758 -0.0045780763 -0.0036298111 -0.0029889569 -0.0025983751 -0.0026776232 -0.0040036663][-0.0093283579 -0.0078824237 -0.0072676204 -0.0066985264 -0.0063567646 -0.0061807185 -0.0060199127 -0.0057610497 -0.0051071122 -0.0045326427 -0.0040598176 -0.0037948936 -0.0036029629 -0.0037219338 -0.0049177334][-0.0092296973 -0.0075781941 -0.0067023486 -0.0058294386 -0.0053410046 -0.005210001 -0.0053090267 -0.0054469444 -0.0053335987 -0.0052737929 -0.0051547512 -0.0050862394 -0.0048799925 -0.0049156994 -0.0060179979][-0.0093909539 -0.0077283345 -0.00683479 -0.00586278 -0.0053255111 -0.0052856617 -0.0055560209 -0.0058459491 -0.0060349926 -0.0062429234 -0.0063818693 -0.0065269284 -0.0063215345 -0.0062472709 -0.0071469061][-0.0099195652 -0.0082477815 -0.0074551404 -0.0065812394 -0.0061422214 -0.006192822 -0.0065161623 -0.0068785138 -0.00726166 -0.0077016316 -0.0080523938 -0.0082992129 -0.0081639737 -0.0079981685 -0.0086498372][-0.010497823 -0.0089576989 -0.0083785057 -0.0077462904 -0.007528279 -0.0077421516 -0.0081603825 -0.0086118355 -0.0091376752 -0.0097342022 -0.010125734 -0.010260195 -0.010041978 -0.009745013 -0.010025594][-0.011875514 -0.010567676 -0.010276318 -0.0099286661 -0.00993545 -0.010314073 -0.010766286 -0.011204578 -0.011708904 -0.012282863 -0.012623623 -0.012648545 -0.012377664 -0.012103014 -0.01224212]]...]
INFO - root - 2017-12-06 02:51:26.759757: step 4310, loss = 0.85, batch loss = 0.64 (18.2 examples/sec; 0.441 sec/batch; 40h:10m:44s remains)
INFO - root - 2017-12-06 02:51:31.225622: step 4320, loss = 0.93, batch loss = 0.71 (18.0 examples/sec; 0.446 sec/batch; 40h:36m:52s remains)
INFO - root - 2017-12-06 02:51:35.699516: step 4330, loss = 0.87, batch loss = 0.65 (18.0 examples/sec; 0.443 sec/batch; 40h:24m:45s remains)
INFO - root - 2017-12-06 02:51:40.223236: step 4340, loss = 0.91, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:28m:20s remains)
INFO - root - 2017-12-06 02:51:44.675173: step 4350, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.444 sec/batch; 40h:30m:11s remains)
INFO - root - 2017-12-06 02:51:48.951343: step 4360, loss = 0.97, batch loss = 0.76 (18.2 examples/sec; 0.440 sec/batch; 40h:08m:09s remains)
INFO - root - 2017-12-06 02:51:53.483966: step 4370, loss = 0.94, batch loss = 0.73 (17.5 examples/sec; 0.457 sec/batch; 41h:36m:50s remains)
INFO - root - 2017-12-06 02:51:57.992035: step 4380, loss = 0.91, batch loss = 0.70 (17.6 examples/sec; 0.455 sec/batch; 41h:27m:45s remains)
INFO - root - 2017-12-06 02:52:02.491621: step 4390, loss = 0.82, batch loss = 0.61 (17.5 examples/sec; 0.457 sec/batch; 41h:38m:53s remains)
INFO - root - 2017-12-06 02:52:06.967995: step 4400, loss = 0.91, batch loss = 0.70 (18.0 examples/sec; 0.445 sec/batch; 40h:33m:13s remains)
2017-12-06 02:52:07.405532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002935715 -0.0019836277 -0.0012421459 -0.00027125329 0.00041781366 0.0010463633 0.0018871464 0.0025944822 0.0031384639 0.0036050342 0.0039286204 0.0043789931 0.0047899373 0.0054002739 0.0060105585][-0.0017891228 -0.00051622093 0.00028949603 0.0013639741 0.0021323152 0.0026528612 0.0033019297 0.0041616634 0.0048432015 0.0051657632 0.0055969171 0.0062354431 0.0069843978 0.0076785311 0.0082119144][-0.001884561 -0.00059272721 0.00020458549 0.0013579354 0.0022795871 0.0027039684 0.0031283796 0.0038838461 0.004618872 0.0052593611 0.0060701743 0.0069327094 0.0076761395 0.0081253424 0.0085541159][-0.0024445243 -0.0013679191 -0.00068949908 0.00018452853 0.00081602484 0.0011112951 0.0014916696 0.0022431463 0.0032010563 0.004267104 0.0052937828 0.0064809881 0.0074922852 0.0079438537 0.0082765892][-0.0027155913 -0.0017188042 -0.0012825318 -0.00079940259 -0.00055212155 -0.00044232234 -0.00022159517 0.00048901141 0.0016874149 0.003082525 0.0042915493 0.005644463 0.006842088 0.00739963 0.0077696368][-0.0027919672 -0.0016022213 -0.0012027659 -0.00087629631 -0.00084087625 -0.00096223503 -0.0010215864 -0.00050878152 0.00059926882 0.0019057915 0.0031240769 0.004551895 0.0057618544 0.006391868 0.0066905208][-0.00258876 -0.0012346469 -0.00080665573 -0.00052817538 -0.00055162981 -0.000875175 -0.0012744777 -0.0012182221 -0.00067855418 0.00020108372 0.0012336187 0.0024064556 0.003334906 0.0039267428 0.0041697286][-0.0036227815 -0.0020694546 -0.0016132481 -0.001409851 -0.0015348829 -0.0019000284 -0.0023862571 -0.0026268102 -0.0024624169 -0.0019649677 -0.0013118461 -0.00053653866 8.2537532e-05 0.00074030459 0.0012156256][-0.0050831139 -0.0034389123 -0.0031595565 -0.003232535 -0.0034534298 -0.0038980991 -0.0045860969 -0.0050607361 -0.0051222481 -0.0050646327 -0.0049610697 -0.004705444 -0.0041835345 -0.0033247247 -0.002442535][-0.0062885694 -0.00458527 -0.0042529367 -0.0042676553 -0.0044949576 -0.0050255358 -0.0058848746 -0.0066481046 -0.0071074553 -0.0076059513 -0.0081571825 -0.00843646 -0.0082441345 -0.0074720085 -0.0063882917][-0.0076656453 -0.0057949349 -0.005155243 -0.0049039386 -0.0049679503 -0.0053529516 -0.006099999 -0.0070181154 -0.0078922585 -0.0088904127 -0.0098581575 -0.01057554 -0.010685612 -0.010142934 -0.009045098][-0.0097100884 -0.0076429546 -0.0067411102 -0.0062123574 -0.0060342215 -0.0061589181 -0.0067414492 -0.0075956173 -0.0086466931 -0.00985663 -0.011031196 -0.011933573 -0.012170672 -0.011665925 -0.010378521][-0.012573469 -0.010472726 -0.0094080046 -0.0085877776 -0.0080118328 -0.0077653676 -0.00799907 -0.0085844211 -0.0093283318 -0.010163896 -0.011044163 -0.011710275 -0.011730466 -0.010982044 -0.00963141][-0.014986835 -0.013028581 -0.011934847 -0.010936819 -0.010094605 -0.0095427111 -0.0094028786 -0.009563189 -0.0098662749 -0.010274976 -0.010644738 -0.010746446 -0.010270398 -0.0091575794 -0.0077931397][-0.017231908 -0.015378248 -0.014259789 -0.013155401 -0.012131471 -0.011305239 -0.010781094 -0.010597616 -0.010519966 -0.010520883 -0.01041092 -0.0098443143 -0.0087757111 -0.00728805 -0.0058775581]]...]
INFO - root - 2017-12-06 02:52:11.927152: step 4410, loss = 0.94, batch loss = 0.73 (17.3 examples/sec; 0.462 sec/batch; 42h:04m:41s remains)
INFO - root - 2017-12-06 02:52:16.424984: step 4420, loss = 0.90, batch loss = 0.69 (18.1 examples/sec; 0.443 sec/batch; 40h:23m:08s remains)
INFO - root - 2017-12-06 02:52:20.904898: step 4430, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 41h:08m:48s remains)
INFO - root - 2017-12-06 02:52:25.393723: step 4440, loss = 0.83, batch loss = 0.62 (17.7 examples/sec; 0.452 sec/batch; 41h:10m:23s remains)
INFO - root - 2017-12-06 02:52:29.955855: step 4450, loss = 0.86, batch loss = 0.64 (17.6 examples/sec; 0.456 sec/batch; 41h:30m:56s remains)
INFO - root - 2017-12-06 02:52:34.374429: step 4460, loss = 0.92, batch loss = 0.71 (17.7 examples/sec; 0.452 sec/batch; 41h:12m:25s remains)
INFO - root - 2017-12-06 02:52:38.922723: step 4470, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.446 sec/batch; 40h:39m:56s remains)
INFO - root - 2017-12-06 02:52:43.375682: step 4480, loss = 0.87, batch loss = 0.65 (18.2 examples/sec; 0.440 sec/batch; 40h:07m:46s remains)
INFO - root - 2017-12-06 02:52:47.896020: step 4490, loss = 0.84, batch loss = 0.63 (17.6 examples/sec; 0.455 sec/batch; 41h:27m:09s remains)
INFO - root - 2017-12-06 02:52:52.442774: step 4500, loss = 0.86, batch loss = 0.65 (17.2 examples/sec; 0.465 sec/batch; 42h:24m:33s remains)
2017-12-06 02:52:53.000073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022930868 -0.022627477 -0.02198872 -0.0210667 -0.020217594 -0.019576535 -0.019083444 -0.018933564 -0.019269139 -0.020064279 -0.021061685 -0.022080913 -0.022950854 -0.023617754 -0.024086731][-0.022296838 -0.021720111 -0.020732325 -0.01936524 -0.018027902 -0.016913235 -0.01610066 -0.015854836 -0.016430873 -0.017619859 -0.019145187 -0.020749718 -0.022127744 -0.023227287 -0.023981264][-0.021802992 -0.020869333 -0.019415885 -0.017521612 -0.015621014 -0.014000658 -0.012841981 -0.012558963 -0.013517167 -0.01529181 -0.017472077 -0.019630354 -0.021465313 -0.022907127 -0.023846576][-0.021100752 -0.019751728 -0.017815493 -0.015392091 -0.012953762 -0.010913469 -0.0095822588 -0.009411037 -0.010776844 -0.013135202 -0.015999798 -0.018711969 -0.02096799 -0.022654694 -0.023712352][-0.020092499 -0.018317085 -0.015940264 -0.013051867 -0.01012386 -0.0078050159 -0.0064850636 -0.0066259094 -0.0084643178 -0.011367612 -0.014828485 -0.01801281 -0.020626541 -0.022504762 -0.0236365][-0.018685516 -0.01656815 -0.013835069 -0.0105795 -0.0073322915 -0.004873734 -0.0037202016 -0.0043247305 -0.0066586696 -0.010053415 -0.013966758 -0.017455697 -0.020322111 -0.022370666 -0.023595339][-0.016947109 -0.014643084 -0.011716876 -0.0082650855 -0.0049522258 -0.0024829358 -0.0014252551 -0.0024691261 -0.0053230114 -0.0091794766 -0.013407212 -0.017079592 -0.020107642 -0.02229492 -0.023605017][-0.015276771 -0.012733717 -0.009852428 -0.0065544657 -0.0035609491 -0.001349315 -0.00033635274 -0.0014613196 -0.0044752583 -0.0086031817 -0.013090473 -0.016953416 -0.020104565 -0.022369567 -0.023769729][-0.01372651 -0.011033602 -0.0083793327 -0.0055197552 -0.003113877 -0.0014434196 -0.00071916729 -0.0017555021 -0.0043859817 -0.0083383583 -0.012894459 -0.016943872 -0.02027354 -0.022669092 -0.024219282][-0.012687277 -0.0099578761 -0.0075697042 -0.0052406341 -0.0033529438 -0.0021869689 -0.0017828159 -0.0027840696 -0.0050542839 -0.00867416 -0.013080981 -0.017249741 -0.020817745 -0.023447772 -0.025275141][-0.012074847 -0.0096122064 -0.0076266229 -0.005802948 -0.0043263733 -0.0033352077 -0.0030121021 -0.0039061382 -0.0059552565 -0.0093282424 -0.01357004 -0.017803721 -0.021591779 -0.024555717 -0.026816191][-0.012118541 -0.010050494 -0.0086380318 -0.0073887818 -0.006253995 -0.0053116865 -0.0048039071 -0.0053235143 -0.0069992915 -0.010056529 -0.01407674 -0.018320359 -0.022260707 -0.025465505 -0.028092021][-0.012837853 -0.011290409 -0.0105334 -0.0098834 -0.009120632 -0.008226756 -0.007533811 -0.007636562 -0.0088246055 -0.011332504 -0.014924508 -0.018966578 -0.022864353 -0.026126353 -0.028904112][-0.014191564 -0.0131496 -0.012896638 -0.012714814 -0.012337968 -0.011685502 -0.011017814 -0.010894246 -0.011631664 -0.013511073 -0.016415346 -0.01997152 -0.023518581 -0.026595637 -0.029362727][-0.01623223 -0.015578948 -0.015618417 -0.015711825 -0.015623469 -0.015222311 -0.014690485 -0.014510438 -0.014975831 -0.016226884 -0.018352274 -0.021155238 -0.024122359 -0.0267714 -0.029254962]]...]
INFO - root - 2017-12-06 02:52:57.511709: step 4510, loss = 0.85, batch loss = 0.64 (18.2 examples/sec; 0.440 sec/batch; 40h:05m:23s remains)
INFO - root - 2017-12-06 02:53:01.985579: step 4520, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 40h:52m:23s remains)
INFO - root - 2017-12-06 02:53:06.493195: step 4530, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.459 sec/batch; 41h:47m:05s remains)
INFO - root - 2017-12-06 02:53:11.078485: step 4540, loss = 0.90, batch loss = 0.69 (17.8 examples/sec; 0.449 sec/batch; 40h:55m:08s remains)
INFO - root - 2017-12-06 02:53:15.621049: step 4550, loss = 0.91, batch loss = 0.70 (17.4 examples/sec; 0.461 sec/batch; 41h:57m:38s remains)
INFO - root - 2017-12-06 02:53:19.989967: step 4560, loss = 0.91, batch loss = 0.70 (28.0 examples/sec; 0.285 sec/batch; 25h:59m:06s remains)
INFO - root - 2017-12-06 02:53:24.521720: step 4570, loss = 0.91, batch loss = 0.70 (17.1 examples/sec; 0.468 sec/batch; 42h:36m:27s remains)
INFO - root - 2017-12-06 02:53:29.122229: step 4580, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.446 sec/batch; 40h:36m:25s remains)
INFO - root - 2017-12-06 02:53:33.572580: step 4590, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.451 sec/batch; 41h:04m:02s remains)
INFO - root - 2017-12-06 02:53:38.119728: step 4600, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.440 sec/batch; 40h:05m:17s remains)
2017-12-06 02:53:38.613717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012228742 -0.011711948 -0.011538021 -0.011459436 -0.011423558 -0.011331577 -0.011322659 -0.011306144 -0.011277057 -0.011138707 -0.010926176 -0.010679852 -0.010324664 -0.010049503 -0.009978462][-0.012602072 -0.012021679 -0.011902764 -0.011792079 -0.011744212 -0.01173589 -0.011732157 -0.011722457 -0.011704568 -0.011664372 -0.011541996 -0.011291277 -0.010916188 -0.010507233 -0.010263857][-0.013234694 -0.012672849 -0.012529865 -0.012379222 -0.012319501 -0.012338735 -0.012365971 -0.012379192 -0.012380674 -0.012371752 -0.012284748 -0.012063175 -0.011696376 -0.01127271 -0.010976542][-0.013621196 -0.013102833 -0.012932904 -0.012761351 -0.012717478 -0.012794118 -0.012899235 -0.012977283 -0.013026699 -0.013069138 -0.013026375 -0.012841303 -0.012495488 -0.012067594 -0.011727665][-0.013358448 -0.012931824 -0.012781736 -0.012603682 -0.012564626 -0.012650318 -0.012787122 -0.012927122 -0.013033383 -0.013130743 -0.013173006 -0.013084266 -0.012845099 -0.012492299 -0.0122106][-0.012617171 -0.01219409 -0.01204573 -0.011855993 -0.011849906 -0.011970662 -0.012145054 -0.012321476 -0.012470346 -0.01261922 -0.01274794 -0.012778204 -0.012663398 -0.012445044 -0.012306459][-0.011828993 -0.011400666 -0.011250291 -0.011064544 -0.011052039 -0.011176564 -0.011380292 -0.011597477 -0.011801597 -0.01201883 -0.012242664 -0.0123729 -0.012355708 -0.012233838 -0.012230389][-0.011693072 -0.011270653 -0.011155583 -0.010994382 -0.010978986 -0.011079021 -0.011253852 -0.011443552 -0.011601429 -0.011790045 -0.011994302 -0.012122363 -0.012132771 -0.012037657 -0.012122739][-0.011835523 -0.011424642 -0.011396863 -0.011303768 -0.011299975 -0.011389922 -0.011554226 -0.011721384 -0.011840701 -0.011975605 -0.012110624 -0.012165792 -0.012099195 -0.011944715 -0.012040075][-0.012246322 -0.01175192 -0.011788249 -0.011761121 -0.011786543 -0.011874247 -0.01200404 -0.01210551 -0.012151632 -0.012220409 -0.012291308 -0.012276705 -0.012118869 -0.011886854 -0.011980459][-0.012450904 -0.012006115 -0.012070592 -0.012102861 -0.012144595 -0.012226399 -0.012328535 -0.012394823 -0.012379382 -0.012376003 -0.012365542 -0.012252674 -0.011992473 -0.011692889 -0.011780478][-0.01266785 -0.0123473 -0.012436382 -0.012471281 -0.012492213 -0.012519605 -0.012547307 -0.012564346 -0.012514889 -0.012457483 -0.012390852 -0.012212977 -0.011883434 -0.011515114 -0.011560094][-0.012820374 -0.012524519 -0.012646809 -0.012708168 -0.012732893 -0.012725044 -0.012699284 -0.012661364 -0.012554619 -0.012433741 -0.012305111 -0.012087002 -0.011718266 -0.011305828 -0.01130655][-0.012809612 -0.012486823 -0.012598973 -0.012682714 -0.012724873 -0.012728222 -0.012699347 -0.01263934 -0.01249522 -0.01231689 -0.012138087 -0.011876158 -0.011494551 -0.011084326 -0.011061061][-0.013389625 -0.012963776 -0.01302477 -0.013067488 -0.013060305 -0.013049636 -0.012998156 -0.012955904 -0.012843568 -0.012676273 -0.012511633 -0.012209486 -0.011819888 -0.011464741 -0.011454612]]...]
INFO - root - 2017-12-06 02:53:43.126539: step 4610, loss = 0.88, batch loss = 0.66 (17.3 examples/sec; 0.462 sec/batch; 42h:05m:21s remains)
INFO - root - 2017-12-06 02:53:47.565883: step 4620, loss = 0.89, batch loss = 0.68 (18.5 examples/sec; 0.432 sec/batch; 39h:21m:23s remains)
INFO - root - 2017-12-06 02:53:52.126796: step 4630, loss = 0.85, batch loss = 0.64 (18.3 examples/sec; 0.438 sec/batch; 39h:53m:12s remains)
INFO - root - 2017-12-06 02:53:56.635934: step 4640, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.447 sec/batch; 40h:43m:28s remains)
INFO - root - 2017-12-06 02:54:01.155045: step 4650, loss = 0.84, batch loss = 0.63 (17.7 examples/sec; 0.452 sec/batch; 41h:10m:06s remains)
INFO - root - 2017-12-06 02:54:05.851954: step 4660, loss = 0.91, batch loss = 0.70 (17.4 examples/sec; 0.459 sec/batch; 41h:49m:31s remains)
INFO - root - 2017-12-06 02:54:10.108410: step 4670, loss = 0.89, batch loss = 0.67 (18.3 examples/sec; 0.437 sec/batch; 39h:49m:01s remains)
INFO - root - 2017-12-06 02:54:14.608676: step 4680, loss = 0.83, batch loss = 0.62 (17.8 examples/sec; 0.449 sec/batch; 40h:53m:13s remains)
INFO - root - 2017-12-06 02:54:19.089081: step 4690, loss = 0.91, batch loss = 0.70 (18.0 examples/sec; 0.444 sec/batch; 40h:24m:27s remains)
INFO - root - 2017-12-06 02:54:23.634130: step 4700, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 40h:21m:15s remains)
2017-12-06 02:54:24.071915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.075657547 -0.083894193 -0.090752482 -0.091377825 -0.092623271 -0.092561275 -0.087872811 -0.082460426 -0.084413975 -0.090565711 -0.0956576 -0.10086493 -0.10757917 -0.10806483 -0.10144429][-0.054817583 -0.064077176 -0.069388673 -0.07225287 -0.079466194 -0.079271175 -0.077290013 -0.07538195 -0.079496652 -0.083997607 -0.082490407 -0.081093214 -0.084281139 -0.088738404 -0.089983165][-0.039782949 -0.045650661 -0.045061439 -0.042591311 -0.045312323 -0.04393585 -0.042831708 -0.037481833 -0.035553716 -0.031079914 -0.025522966 -0.02977597 -0.041030876 -0.054748975 -0.0626733][-0.011664938 -0.011716612 -0.0071297735 -0.0044314004 0.00043370202 0.0090401582 0.020213153 0.028590936 0.033546936 0.03994168 0.040100392 0.028917979 0.015713897 0.0035134107 -0.0047982186][0.022889148 0.023146342 0.025563229 0.024577055 0.03171039 0.047479283 0.068136558 0.087690815 0.10898809 0.1257004 0.12867752 0.1170001 0.10896505 0.10275424 0.094713911][0.069728196 0.068032235 0.075627595 0.080706343 0.0967229 0.125728 0.15468094 0.17957816 0.1966055 0.20816591 0.20959294 0.19549173 0.19110771 0.19207187 0.18567978][0.10076398 0.10514289 0.127929 0.14444405 0.16773345 0.19650847 0.21425721 0.23102105 0.2386899 0.23973566 0.23417339 0.21906802 0.21940455 0.22074667 0.20972636][0.088930815 0.098136485 0.12981305 0.15058878 0.17579846 0.1950191 0.20220479 0.2036888 0.19770655 0.18543598 0.16595022 0.14451605 0.13986868 0.13921718 0.13167383][0.045134831 0.044469949 0.07177946 0.097432762 0.12574005 0.13844815 0.13180977 0.11262336 0.088356078 0.059936021 0.022033636 -0.0084677488 -0.016411692 -0.013590533 -0.010715824][-0.03422929 -0.053281896 -0.048148744 -0.039938688 -0.026526066 -0.025797345 -0.036503054 -0.05908135 -0.082361139 -0.10905959 -0.13996449 -0.16412163 -0.18038763 -0.18721189 -0.18371513][-0.11328349 -0.14765112 -0.15460277 -0.1637277 -0.17016216 -0.18418761 -0.19987044 -0.21697542 -0.23512994 -0.25453892 -0.27575204 -0.28998795 -0.29562539 -0.29234016 -0.27517164][-0.17521319 -0.21697246 -0.22642492 -0.23791939 -0.24489164 -0.25875539 -0.27444991 -0.28687084 -0.29767132 -0.30416203 -0.31130159 -0.31018302 -0.30212075 -0.28694665 -0.25767103][-0.21828088 -0.26216808 -0.27444142 -0.28949785 -0.299088 -0.31552631 -0.32592583 -0.330666 -0.33915862 -0.3462854 -0.36031166 -0.36433011 -0.3628262 -0.35110754 -0.32388148][-0.27053148 -0.31666693 -0.33188453 -0.34207076 -0.34532854 -0.35773736 -0.36142069 -0.36274993 -0.3639873 -0.3674784 -0.38372135 -0.39729223 -0.41623509 -0.42488733 -0.41323385][-0.362148 -0.39883196 -0.42024693 -0.43951637 -0.44875133 -0.4563421 -0.45611775 -0.45434812 -0.44697326 -0.44394258 -0.44577298 -0.44564208 -0.46070492 -0.46653667 -0.46038941]]...]
INFO - root - 2017-12-06 02:54:28.693040: step 4710, loss = 0.84, batch loss = 0.62 (17.8 examples/sec; 0.449 sec/batch; 40h:52m:23s remains)
INFO - root - 2017-12-06 02:54:33.198910: step 4720, loss = 0.91, batch loss = 0.70 (17.7 examples/sec; 0.452 sec/batch; 41h:09m:00s remains)
INFO - root - 2017-12-06 02:54:37.704487: step 4730, loss = 0.92, batch loss = 0.71 (17.2 examples/sec; 0.465 sec/batch; 42h:19m:26s remains)
INFO - root - 2017-12-06 02:54:42.245209: step 4740, loss = 0.82, batch loss = 0.61 (18.4 examples/sec; 0.434 sec/batch; 39h:31m:57s remains)
INFO - root - 2017-12-06 02:54:46.703495: step 4750, loss = 0.90, batch loss = 0.69 (18.0 examples/sec; 0.445 sec/batch; 40h:28m:19s remains)
INFO - root - 2017-12-06 02:54:51.246139: step 4760, loss = 0.85, batch loss = 0.64 (17.2 examples/sec; 0.465 sec/batch; 42h:20m:21s remains)
INFO - root - 2017-12-06 02:54:55.576330: step 4770, loss = 0.85, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 41h:37m:54s remains)
INFO - root - 2017-12-06 02:55:00.116325: step 4780, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.442 sec/batch; 40h:12m:47s remains)
INFO - root - 2017-12-06 02:55:04.546388: step 4790, loss = 0.83, batch loss = 0.62 (18.7 examples/sec; 0.428 sec/batch; 38h:59m:14s remains)
INFO - root - 2017-12-06 02:55:09.092604: step 4800, loss = 0.84, batch loss = 0.63 (17.8 examples/sec; 0.451 sec/batch; 41h:01m:26s remains)
2017-12-06 02:55:09.540398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.026762597 -0.025598165 -0.024958707 -0.024491388 -0.024017338 -0.023518033 -0.022863239 -0.02224192 -0.021850813 -0.021776818 -0.021904133 -0.022139683 -0.022401787 -0.022712462 -0.022780146][-0.03212212 -0.030736113 -0.029704191 -0.028566491 -0.027303088 -0.025923446 -0.024500195 -0.023306254 -0.022519715 -0.022255432 -0.022295788 -0.022493124 -0.022687681 -0.022839144 -0.022884551][-0.036843561 -0.035259567 -0.033790834 -0.032066565 -0.030148668 -0.028037984 -0.026026063 -0.024383709 -0.023317326 -0.022870328 -0.022752322 -0.022878308 -0.023041938 -0.023224719 -0.023181655][-0.039909713 -0.038352326 -0.036421992 -0.03429769 -0.0319357 -0.02935412 -0.027121354 -0.025323279 -0.024257597 -0.02369675 -0.023541968 -0.023776874 -0.023726013 -0.02395644 -0.023834471][-0.040525969 -0.038916856 -0.036893126 -0.034575235 -0.031983182 -0.029320326 -0.027145876 -0.025520131 -0.024720304 -0.024399199 -0.024450805 -0.024741165 -0.024951518 -0.025381915 -0.025463164][-0.038142189 -0.036622729 -0.034714673 -0.032591134 -0.030356599 -0.027986681 -0.026377875 -0.02533656 -0.02521367 -0.02585585 -0.02689724 -0.02799095 -0.028769415 -0.029530946 -0.030019844][-0.033023767 -0.0316692 -0.030199666 -0.028534548 -0.026921511 -0.025426056 -0.024684057 -0.025259834 -0.026628494 -0.02873192 -0.031051248 -0.033379823 -0.035091713 -0.036861852 -0.038272418][-0.026358355 -0.025177434 -0.024289086 -0.023391545 -0.022654295 -0.022672866 -0.02344374 -0.025060229 -0.028810915 -0.032504186 -0.036813147 -0.040690172 -0.043946646 -0.04657162 -0.048604514][-0.020364881 -0.019274142 -0.018936649 -0.018909056 -0.01915573 -0.02061699 -0.023235146 -0.02701072 -0.032252468 -0.038269091 -0.044551432 -0.049532544 -0.054161508 -0.057456527 -0.059974227][-0.016021147 -0.0151495 -0.015409991 -0.016141437 -0.017592587 -0.020788781 -0.025183156 -0.030379513 -0.037154451 -0.045004614 -0.0525527 -0.05944071 -0.065041833 -0.068394579 -0.071105488][-0.013763152 -0.013242874 -0.014244709 -0.016119756 -0.018766928 -0.02312196 -0.0292485 -0.036246344 -0.044448748 -0.052290309 -0.060520452 -0.0680688 -0.07392323 -0.078266047 -0.08014407][-0.013727527 -0.014042128 -0.015858222 -0.018518306 -0.022240948 -0.02745359 -0.034447372 -0.0425608 -0.051496852 -0.060620286 -0.068954065 -0.07536114 -0.080934636 -0.084653467 -0.085975543][-0.015985109 -0.017145462 -0.019827124 -0.023406092 -0.027934484 -0.033721313 -0.040566977 -0.04847737 -0.057835326 -0.066744275 -0.074558452 -0.080915563 -0.085718989 -0.087729342 -0.088475794][-0.01997716 -0.022410277 -0.025897615 -0.030128736 -0.035041355 -0.040836584 -0.047592722 -0.055205159 -0.063757852 -0.070869312 -0.077189922 -0.082249366 -0.085656412 -0.087384641 -0.087477751][-0.025270957 -0.028265525 -0.03216733 -0.036580734 -0.041725025 -0.047575269 -0.053782128 -0.059656069 -0.065931983 -0.0721781 -0.076750755 -0.079987094 -0.082178511 -0.083341353 -0.083309807]]...]
INFO - root - 2017-12-06 02:55:14.101292: step 4810, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.452 sec/batch; 41h:06m:29s remains)
INFO - root - 2017-12-06 02:55:18.581212: step 4820, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.453 sec/batch; 41h:13m:26s remains)
INFO - root - 2017-12-06 02:55:23.081910: step 4830, loss = 0.83, batch loss = 0.62 (18.2 examples/sec; 0.441 sec/batch; 40h:05m:54s remains)
INFO - root - 2017-12-06 02:55:27.574255: step 4840, loss = 0.91, batch loss = 0.70 (17.9 examples/sec; 0.446 sec/batch; 40h:37m:09s remains)
INFO - root - 2017-12-06 02:55:32.067661: step 4850, loss = 0.83, batch loss = 0.62 (17.0 examples/sec; 0.471 sec/batch; 42h:54m:36s remains)
INFO - root - 2017-12-06 02:55:36.643946: step 4860, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.453 sec/batch; 41h:14m:14s remains)
INFO - root - 2017-12-06 02:55:41.214362: step 4870, loss = 0.88, batch loss = 0.67 (17.5 examples/sec; 0.457 sec/batch; 41h:33m:48s remains)
INFO - root - 2017-12-06 02:55:45.588925: step 4880, loss = 0.84, batch loss = 0.63 (16.7 examples/sec; 0.478 sec/batch; 43h:29m:17s remains)
INFO - root - 2017-12-06 02:55:50.123521: step 4890, loss = 0.83, batch loss = 0.62 (17.0 examples/sec; 0.470 sec/batch; 42h:43m:40s remains)
INFO - root - 2017-12-06 02:55:54.620606: step 4900, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 40h:59m:34s remains)
2017-12-06 02:55:55.069922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.22060962 -0.10709124 -0.024482805 0.026498213 0.045310482 -0.040591221 -0.2820251 -0.54762143 -0.744489 -0.88957781 -1.0123961 -1.0173208 -0.9905442 -1.0551863 -1.1357479][-0.15300551 -0.049278494 0.06076993 0.11685112 0.030321911 -0.050373096 -0.20001262 -0.475211 -0.67548943 -0.79176724 -0.85546476 -0.85192442 -0.89081097 -0.9236775 -0.9445169][-0.34687582 -0.1901582 0.021600358 0.15399317 0.10749446 0.013077572 -0.2127858 -0.42862228 -0.62477243 -0.70213246 -0.73481035 -0.71033216 -0.64482284 -0.61124283 -0.66724449][-0.56620109 -0.44220957 -0.199889 0.090871379 0.20610115 0.19094139 -0.13235477 -0.35475567 -0.48215625 -0.65462053 -0.74588138 -0.69337511 -0.64148164 -0.5065164 -0.43950909][-0.77330357 -0.67033148 -0.39175311 -0.069805369 0.088991717 0.23910683 0.16485626 -0.13946867 -0.39419067 -0.52180696 -0.57475817 -0.65479851 -0.73215681 -0.59878623 -0.43649271][-0.79783487 -0.6802876 -0.44364202 -0.06125221 0.13540512 0.27144247 0.30776516 0.25854614 0.094622 -0.0725417 -0.13379602 -0.09258619 -0.17263398 -0.16481476 -0.25354266][-0.57770157 -0.46558595 -0.27049971 0.067710578 0.35534391 0.50625104 0.55591387 0.56477714 0.49996674 0.476152 0.48162782 0.47229993 0.4209387 0.3384133 0.35699937][-0.065868273 -0.048874073 0.10048141 0.32022369 0.53421456 0.66271996 0.77182209 0.76883584 0.7176106 0.76029253 0.78144646 0.83015734 0.80523139 0.6934973 0.56325835][0.20010519 0.17041971 0.22599751 0.40649486 0.62730008 0.67160296 0.6425125 0.65881264 0.60747504 0.5806126 0.58087718 0.62533069 0.548506 0.46114421 0.27586922][0.47254843 0.39897048 0.30804262 0.26800448 0.3454943 0.38836446 0.29264963 0.18413024 0.07580708 0.016516842 0.010193735 0.056072995 -0.02897881 -0.10167366 -0.29983878][1.1605804 1.0316589 0.76483905 0.46526349 0.11423647 -0.26606914 -0.53230184 -0.66501826 -0.796975 -0.89107275 -0.9094857 -0.84106976 -0.87999016 -0.85562515 -0.7995286][0.88533551 0.856196 0.70143193 0.41733196 0.041265652 -0.4613283 -0.85144717 -1.1807861 -1.3416448 -1.3805127 -1.3878994 -1.3693323 -1.2788959 -1.26916 -1.1320581][0.60765564 0.53639394 0.48914337 0.21651787 -0.27013224 -0.6981172 -0.95421743 -1.1940393 -1.4112775 -1.4578729 -1.4305925 -1.4330955 -1.3891094 -1.4186606 -1.418467][0.27578533 0.32533413 0.47617471 0.31065655 -0.014911745 -0.41234267 -0.72062904 -0.94623822 -1.0730636 -1.1562529 -1.3101914 -1.3235662 -1.2351344 -1.2120016 -1.3027835][-0.22699787 0.016777612 0.26821822 0.31294948 0.27403265 0.11538 -0.12773052 -0.25261933 -0.341206 -0.41237259 -0.57729322 -0.763342 -0.81201732 -0.67343569 -0.77024704]]...]
INFO - root - 2017-12-06 02:55:59.516534: step 4910, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.452 sec/batch; 41h:06m:57s remains)
INFO - root - 2017-12-06 02:56:04.152821: step 4920, loss = 0.83, batch loss = 0.62 (17.8 examples/sec; 0.450 sec/batch; 40h:55m:10s remains)
INFO - root - 2017-12-06 02:56:08.640982: step 4930, loss = 0.82, batch loss = 0.61 (17.8 examples/sec; 0.450 sec/batch; 40h:59m:05s remains)
INFO - root - 2017-12-06 02:56:13.114169: step 4940, loss = 0.87, batch loss = 0.66 (17.5 examples/sec; 0.457 sec/batch; 41h:37m:03s remains)
INFO - root - 2017-12-06 02:56:17.642836: step 4950, loss = 0.84, batch loss = 0.63 (16.6 examples/sec; 0.480 sec/batch; 43h:43m:07s remains)
INFO - root - 2017-12-06 02:56:22.162422: step 4960, loss = 0.81, batch loss = 0.60 (17.3 examples/sec; 0.461 sec/batch; 41h:58m:13s remains)
INFO - root - 2017-12-06 02:56:26.787445: step 4970, loss = 0.84, batch loss = 0.63 (18.1 examples/sec; 0.442 sec/batch; 40h:15m:23s remains)
INFO - root - 2017-12-06 02:56:31.087307: step 4980, loss = 0.86, batch loss = 0.65 (17.9 examples/sec; 0.448 sec/batch; 40h:46m:23s remains)
INFO - root - 2017-12-06 02:56:35.563484: step 4990, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.442 sec/batch; 40h:10m:50s remains)
INFO - root - 2017-12-06 02:56:40.010535: step 5000, loss = 0.90, batch loss = 0.69 (18.0 examples/sec; 0.443 sec/batch; 40h:20m:26s remains)
2017-12-06 02:56:40.439514: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0082879439 0.0091806352 0.0094024912 0.0093732476 0.0087367743 0.0076438859 0.00631994 0.0051011145 0.0043045506 0.0037441067 0.003487289 0.0033452287 0.0031309724 0.0027001202 0.0015126541][0.00915315 0.010182276 0.010312252 0.01021906 0.009530507 0.0080865175 0.0062108412 0.0043142959 0.0028753951 0.0017869622 0.001272317 0.0013110191 0.001432851 0.0013627037 0.00057188049][0.0089743584 0.009936668 0.010020658 0.0098715127 0.0091023073 0.0075650364 0.0054241642 0.0031561628 0.0012211017 -0.00022087246 -0.00092885643 -0.00083509088 -0.0003981553 -4.5903027e-05 -0.00031184033][0.008843936 0.00974074 0.0098111108 0.00967887 0.0088272318 0.0072246939 0.0049863905 0.0024931282 0.00020001084 -0.0014993437 -0.002437111 -0.0024677292 -0.0020003691 -0.0014388151 -0.0012599118][0.0087527633 0.0095446855 0.0096187368 0.0095550865 0.0087711439 0.0071133524 0.0048913285 0.0024384074 7.6889992e-06 -0.0018358007 -0.0029044561 -0.0030532777 -0.0026665702 -0.002099663 -0.0016847216][0.008585915 0.0093421414 0.0093441606 0.00941544 0.0088669211 0.0074038133 0.0053756982 0.0030734539 0.00068092346 -0.0013050623 -0.0024479888 -0.0026212111 -0.0022752546 -0.00172095 -0.0013130791][0.0085590184 0.009213753 0.0090459064 0.0090198442 0.0085316673 0.0074210763 0.0059018359 0.0039283782 0.0017184764 -0.00024228171 -0.0013631508 -0.0016084053 -0.0013258085 -0.00077110156 -0.00040764734][0.0085199624 0.009243384 0.00894659 0.0087307617 0.0081584975 0.0072359219 0.0061268657 0.0046357363 0.0027834736 0.0010493807 -8.2787126e-05 -0.00041470304 -0.00016062334 0.00041971356 0.00071097538][0.0085714161 0.00947585 0.0092474818 0.0090626478 0.008498393 0.007616587 0.0066592246 0.0054689124 0.00394506 0.0024112612 0.0013970658 0.0010364577 0.0012363717 0.0017840639 0.0019595809][0.0088381693 0.0098588988 0.0097704679 0.0097356513 0.0093312785 0.0085149333 0.0075405985 0.00632862 0.0049842149 0.0036858134 0.0028171912 0.0025656037 0.0028366856 0.0033847652 0.0033656918][0.009159565 0.010230035 0.010259971 0.010307275 0.01001513 0.009351857 0.0083880946 0.0070839822 0.0056917518 0.0044304281 0.0036623441 0.0034806542 0.0038220435 0.0044126138 0.0043823645][0.0095798746 0.010640353 0.010811947 0.010888286 0.010598086 0.0099791735 0.0089922026 0.0076471344 0.0062222481 0.005008176 0.0043254867 0.0041649714 0.0045042187 0.0051329434 0.0050477907][0.0098075271 0.010927029 0.011205912 0.011284657 0.010960579 0.010301352 0.0092291161 0.0078346357 0.0064017996 0.0051822364 0.0044463426 0.0042798519 0.0046132654 0.0052466094 0.005067572][0.0096162036 0.010762185 0.011139348 0.011256054 0.01087372 0.010173976 0.0091020912 0.007776618 0.0064458773 0.0053415969 0.0046611056 0.0043529496 0.0044741407 0.004931055 0.0047313124][0.0077551976 0.0087276474 0.0090557113 0.0091130733 0.0087761581 0.0081749186 0.0072770566 0.006278187 0.0052848458 0.0044853985 0.0040057674 0.0036884397 0.0036557019 0.0038097166 0.0034544058]]...]
INFO - root - 2017-12-06 02:56:44.950647: step 5010, loss = 0.98, batch loss = 0.76 (18.2 examples/sec; 0.438 sec/batch; 39h:53m:03s remains)
INFO - root - 2017-12-06 02:56:49.539721: step 5020, loss = 0.89, batch loss = 0.68 (17.3 examples/sec; 0.461 sec/batch; 41h:57m:17s remains)
INFO - root - 2017-12-06 02:56:54.038474: step 5030, loss = 0.91, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:45m:15s remains)
INFO - root - 2017-12-06 02:56:58.591923: step 5040, loss = 0.94, batch loss = 0.73 (17.5 examples/sec; 0.457 sec/batch; 41h:35m:45s remains)
INFO - root - 2017-12-06 02:57:03.118723: step 5050, loss = 0.92, batch loss = 0.71 (17.3 examples/sec; 0.462 sec/batch; 42h:02m:20s remains)
INFO - root - 2017-12-06 02:57:07.610003: step 5060, loss = 0.94, batch loss = 0.73 (17.4 examples/sec; 0.460 sec/batch; 41h:49m:14s remains)
INFO - root - 2017-12-06 02:57:12.066686: step 5070, loss = 0.84, batch loss = 0.63 (18.1 examples/sec; 0.442 sec/batch; 40h:12m:55s remains)
INFO - root - 2017-12-06 02:57:16.399921: step 5080, loss = 0.88, batch loss = 0.67 (24.5 examples/sec; 0.327 sec/batch; 29h:42m:05s remains)
INFO - root - 2017-12-06 02:57:20.978296: step 5090, loss = 0.90, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 39h:58m:11s remains)
INFO - root - 2017-12-06 02:57:25.478353: step 5100, loss = 0.83, batch loss = 0.62 (17.8 examples/sec; 0.450 sec/batch; 40h:58m:06s remains)
2017-12-06 02:57:25.931371: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0314753 1.3603795 1.4163958 1.2169912 0.88449347 0.45720246 0.0791451 -0.32629246 -0.71087492 -0.89772433 -1.2037262 -1.1488507 -1.315518 -1.5385544 -1.6856474][0.62613791 0.74430341 1.0273486 1.1403902 1.0213459 1.0551124 0.62836754 0.025763031 -0.47409159 -0.84023786 -1.047558 -0.9604283 -0.86541235 -1.0792866 -1.3253647][0.94051844 0.87633967 0.9726215 1.0679572 1.1238462 1.2110498 1.1055875 0.9430148 0.55995762 0.13332441 -0.27693027 -0.35116741 -0.39484671 -0.482333 -0.56368607][0.97981364 1.1953893 1.2093995 1.0973164 0.98957926 1.1815666 1.368217 1.6409639 1.8593584 1.7696834 1.5769012 1.1655467 0.69928426 0.28595683 -0.19789317][0.95987469 0.69202143 0.74048942 1.0234243 0.94517666 0.89377528 0.91716218 1.331936 1.8354983 2.1926317 2.4880676 2.6412406 2.5266075 1.9852735 1.4536141][1.3804989 0.93472517 0.75685483 0.85830194 0.924606 1.1703843 1.1329936 1.3871355 1.7831458 2.3273528 2.8787024 3.2675154 3.323014 3.2034886 2.8157175][0.93735009 0.95533234 0.95495909 0.90855014 0.95904869 1.1910435 1.4469407 1.9622585 2.483114 2.8783443 3.2819839 3.7388532 4.027473 3.96138 3.6959054][0.85614395 0.68318468 0.73034286 0.70188487 0.81508732 1.0347593 1.3413179 1.8572302 2.5132263 3.2351964 3.6530135 3.8681805 4.1986513 4.1916542 3.9467659][-0.16783579 0.013708409 0.2529926 0.55733877 0.72220522 0.9563033 1.40529 1.9260466 2.4072576 2.9755931 3.3698328 3.7391145 4.059485 4.3251247 4.1832824][-0.67500871 -0.56444812 -0.38191238 -0.20833488 0.28450355 0.86146551 1.33142 1.9610995 2.5542154 2.9532676 3.1905897 3.2275763 3.5747881 3.9066377 3.8628132][-1.0523309 -0.76880884 -0.509363 -0.22847886 0.046194922 0.48624489 0.99077481 1.5577925 2.0521941 2.3815866 2.4547927 2.4832211 2.6343763 2.7146461 2.4886937][-0.7291137 -0.78357452 -0.64184469 -0.32017019 0.084478617 0.45350894 0.71664143 1.1009407 1.4018787 1.2440954 1.2240503 1.4174753 1.4794565 1.6545569 1.6857685][-0.63660026 -0.38260594 -0.25621966 -0.23765631 -0.20709489 -0.025253728 0.16669758 0.42162424 0.80538994 0.55026406 0.39078194 0.35905015 0.35676238 0.2716594 0.30480528][-0.44079795 -0.50119579 -0.43292552 -0.30068237 -0.41876742 -0.24648654 0.10458769 -0.060436163 -0.37415847 -0.60268581 -0.73961878 -0.66362965 -0.8289566 -0.94535547 -0.76280016][-0.62616259 -0.88100636 -1.0941657 -1.0811342 -0.96440029 -0.95216453 -0.88811421 -0.80045295 -0.73305935 -1.1508362 -1.824792 -2.1630232 -2.2635124 -2.1023717 -1.8426085]]...]
INFO - root - 2017-12-06 02:57:30.427114: step 5110, loss = 0.84, batch loss = 0.62 (17.9 examples/sec; 0.447 sec/batch; 40h:40m:06s remains)
INFO - root - 2017-12-06 02:57:34.875041: step 5120, loss = 0.85, batch loss = 0.64 (18.0 examples/sec; 0.444 sec/batch; 40h:24m:17s remains)
INFO - root - 2017-12-06 02:57:39.357955: step 5130, loss = 0.85, batch loss = 0.63 (17.7 examples/sec; 0.451 sec/batch; 41h:01m:47s remains)
INFO - root - 2017-12-06 02:57:43.824498: step 5140, loss = 0.91, batch loss = 0.70 (17.2 examples/sec; 0.464 sec/batch; 42h:12m:23s remains)
INFO - root - 2017-12-06 02:57:48.317154: step 5150, loss = 0.82, batch loss = 0.60 (18.1 examples/sec; 0.441 sec/batch; 40h:06m:56s remains)
INFO - root - 2017-12-06 02:57:52.812732: step 5160, loss = 0.90, batch loss = 0.69 (17.4 examples/sec; 0.458 sec/batch; 41h:41m:21s remains)
INFO - root - 2017-12-06 02:57:57.358623: step 5170, loss = 0.86, batch loss = 0.65 (17.5 examples/sec; 0.458 sec/batch; 41h:37m:32s remains)
INFO - root - 2017-12-06 02:58:01.834572: step 5180, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.447 sec/batch; 40h:40m:11s remains)
INFO - root - 2017-12-06 02:58:06.123610: step 5190, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.443 sec/batch; 40h:17m:15s remains)
INFO - root - 2017-12-06 02:58:10.607456: step 5200, loss = 0.84, batch loss = 0.63 (17.9 examples/sec; 0.446 sec/batch; 40h:34m:43s remains)
2017-12-06 02:58:11.046568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.005184859 -0.0046883486 -0.0047439821 -0.0048116185 -0.004837703 -0.0048751645 -0.0049246922 -0.0050005354 -0.0050821975 -0.0052126795 -0.0053563975 -0.0056208745 -0.0057617836 -0.0058891922 -0.0062132441][-0.005124934 -0.0046263561 -0.0047668889 -0.0048807226 -0.0049441494 -0.0050006025 -0.0050570928 -0.005116079 -0.0052026585 -0.0053298362 -0.0054841563 -0.00564998 -0.0058021024 -0.0059415363 -0.0062669069][-0.0050855353 -0.0046402924 -0.0048539825 -0.0050015524 -0.0050773881 -0.0051412694 -0.0052112304 -0.0052801631 -0.0053921379 -0.0055489205 -0.0057322942 -0.005912967 -0.0060648508 -0.0061990134 -0.006484732][-0.0048732162 -0.0044585504 -0.00471304 -0.0048899092 -0.0049663782 -0.0050239824 -0.0051208287 -0.0052308552 -0.0053811446 -0.0055931285 -0.0058232844 -0.0060357004 -0.0062063523 -0.0063422881 -0.006585028][-0.0045206547 -0.0041324012 -0.0043850131 -0.0045545995 -0.0046140403 -0.0046569556 -0.0047795847 -0.0049585663 -0.0051901564 -0.0054796413 -0.0057741292 -0.0060344748 -0.0062447153 -0.0063986257 -0.0066271238][-0.0041496605 -0.0036787353 -0.0038860179 -0.004009828 -0.0040278435 -0.0040560365 -0.0042152666 -0.0044877492 -0.0048428588 -0.0052475743 -0.0056209676 -0.0059589706 -0.0062347949 -0.0064114332 -0.0066401288][-0.0036178008 -0.003015656 -0.0031395257 -0.0032160208 -0.0032418147 -0.0033012778 -0.0035345294 -0.0039312057 -0.004440017 -0.0049663112 -0.0054195747 -0.0058303028 -0.0061750151 -0.0063884333 -0.0066231862][-0.0029968023 -0.0020699427 -0.0020513572 -0.0020835921 -0.0021979548 -0.0024476573 -0.0028714798 -0.0034298785 -0.0040889718 -0.0047350414 -0.0052745007 -0.0057349205 -0.00610609 -0.0063477345 -0.0066120327][-0.0021483265 -0.00085558 -0.00063396618 -0.00066528842 -0.000955075 -0.001514893 -0.0022842288 -0.0031101182 -0.0038837381 -0.0045797005 -0.0051693581 -0.005661279 -0.0060471818 -0.0063002221 -0.0065980479][-0.00085733458 0.00065088272 0.0010733604 0.00103819 0.00048706308 -0.00046636909 -0.0016407631 -0.0027851015 -0.0037508123 -0.0044853091 -0.0050650425 -0.00556067 -0.005955942 -0.00622895 -0.0065788925][0.00084449723 0.0024933666 0.003097713 0.0029643625 0.0020757541 0.00065857545 -0.00097863376 -0.0024658628 -0.0036100335 -0.0044083 -0.0049759708 -0.0054253303 -0.0058151186 -0.0061222538 -0.0065188259][0.0028984025 0.004755497 0.0053861737 0.0050006658 0.0036652759 0.001731012 -0.0003972128 -0.0022142231 -0.0034840778 -0.0042976215 -0.004843343 -0.0052670091 -0.005650118 -0.0059773028 -0.0064271577][0.0052905008 0.0073470995 0.00781358 0.0069776848 0.0050919876 0.0025918111 1.4446676e-05 -0.0020579398 -0.0033713393 -0.004127834 -0.0046327859 -0.005053632 -0.0054570474 -0.0058110915 -0.0063219406][0.0079111829 0.0099616349 0.010128178 0.0087796375 0.0063141808 0.0032796189 0.00032200664 -0.0019554831 -0.0032509938 -0.0039202757 -0.0043961741 -0.004836373 -0.0052742474 -0.0056360252 -0.0061973706][0.0086084455 0.010655269 0.010590568 0.0088867694 0.0060806796 0.0028629117 -0.00014809519 -0.0024145395 -0.0036356896 -0.0042022131 -0.0046180487 -0.0052254423 -0.0058387443 -0.0063576289 -0.0070394687]]...]
INFO - root - 2017-12-06 02:58:15.552902: step 5210, loss = 0.86, batch loss = 0.65 (16.7 examples/sec; 0.480 sec/batch; 43h:40m:23s remains)
INFO - root - 2017-12-06 02:58:20.121514: step 5220, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.444 sec/batch; 40h:22m:35s remains)
INFO - root - 2017-12-06 02:58:24.602092: step 5230, loss = 0.87, batch loss = 0.65 (18.0 examples/sec; 0.444 sec/batch; 40h:22m:18s remains)
INFO - root - 2017-12-06 02:58:29.107788: step 5240, loss = 0.85, batch loss = 0.64 (16.8 examples/sec; 0.476 sec/batch; 43h:17m:24s remains)
INFO - root - 2017-12-06 02:58:33.707303: step 5250, loss = 0.87, batch loss = 0.66 (17.5 examples/sec; 0.456 sec/batch; 41h:29m:32s remains)
INFO - root - 2017-12-06 02:58:38.186057: step 5260, loss = 0.91, batch loss = 0.70 (17.9 examples/sec; 0.446 sec/batch; 40h:35m:07s remains)
INFO - root - 2017-12-06 02:58:42.702214: step 5270, loss = 0.87, batch loss = 0.66 (16.5 examples/sec; 0.485 sec/batch; 44h:07m:12s remains)
INFO - root - 2017-12-06 02:58:47.187213: step 5280, loss = 0.89, batch loss = 0.68 (18.2 examples/sec; 0.438 sec/batch; 39h:50m:39s remains)
INFO - root - 2017-12-06 02:58:51.501776: step 5290, loss = 0.85, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 41h:31m:13s remains)
INFO - root - 2017-12-06 02:58:56.091495: step 5300, loss = 0.85, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 40h:52m:12s remains)
2017-12-06 02:58:56.551780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027952906 -0.36116707 -0.59006757 -0.67423272 -0.710743 -0.6447596 -0.52508426 -0.47929585 -0.32910249 -0.27004364 -0.33612916 -0.33371827 -0.27777219 -0.27831203 -0.22059384][-0.0011386722 -0.25769508 -0.33190382 -0.29520243 -0.29458919 -0.39379489 -0.48866782 -0.42005119 -0.33718342 -0.22293997 -0.036671057 -0.025825121 -0.071204856 -0.08300969 -0.08517269][-0.29194033 -0.4253414 -0.50709224 -0.49473983 -0.5091874 -0.52366221 -0.56665123 -0.56552947 -0.56883997 -0.4976199 -0.44629249 -0.347839 -0.31389657 -0.2923629 -0.27013218][-0.44576356 -0.43234211 -0.44117802 -0.4181973 -0.30737486 -0.37814155 -0.47146857 -0.39432186 -0.35652521 -0.42674825 -0.59330404 -0.577646 -0.49911511 -0.50878423 -0.4324649][0.11476655 0.18686792 0.10373887 0.085374407 0.10626628 0.070689492 -0.020290911 -0.15844253 -0.26679963 -0.28371459 -0.36095619 -0.42206541 -0.52216148 -0.5128805 -0.35832477][0.99496537 1.0543039 0.856104 0.76899934 0.76342505 0.79904163 0.7230342 0.56370556 0.28459695 -0.031670284 -0.21319184 -0.28872037 -0.34834656 -0.35094932 -0.35013086][1.391378 1.5642084 1.6252635 1.4724789 1.4142319 1.3986739 1.3902591 1.2654271 1.1296959 0.82238322 0.52093834 0.21787041 0.050104409 -0.015436731 -0.070878416][1.8621374 2.08211 2.1871295 1.9920317 1.8572057 1.7919749 1.6880337 1.5893186 1.4658682 1.191969 0.92045695 0.56255782 0.29509062 0.1287725 -0.032522883][1.826442 1.7366499 1.6826036 1.6900377 1.5309322 1.4438748 1.3411735 1.2817496 1.3467151 1.2197117 1.0536321 0.7757293 0.5030272 0.25021487 0.074677221][0.65105563 0.50335652 0.34079677 0.31318972 0.2527054 0.22459343 0.19180354 0.22001332 0.22758773 0.24527368 0.32781848 0.37464267 0.33384714 0.247139 0.23495191][-0.45568612 -0.87188274 -1.286947 -1.255772 -1.1493828 -1.0359515 -1.1220227 -1.0816449 -1.0056099 -0.85828632 -0.67809469 -0.44706497 -0.23978573 -0.10407662 0.11908261][-1.5166343 -2.0127926 -2.3516569 -2.2187669 -2.141408 -1.9996706 -1.8554531 -1.732796 -1.6651965 -1.5170106 -1.3952584 -1.1533412 -0.90349013 -0.631946 -0.27635053][-2.1130812 -2.507201 -2.6449943 -2.6618934 -2.5868168 -2.3172605 -2.0784121 -1.9178175 -1.8421946 -1.6853257 -1.6077042 -1.4881697 -1.3503927 -1.1016803 -0.77248996][-2.0219655 -2.3601897 -2.5035908 -2.4663053 -2.2816002 -2.1800158 -1.9897759 -1.7719789 -1.61421 -1.5876334 -1.6069326 -1.5498464 -1.4579607 -1.327165 -1.1122906][-2.0128095 -2.333905 -2.468569 -2.3732145 -2.2141433 -2.0842388 -1.8761349 -1.7226263 -1.5474132 -1.4777664 -1.3978521 -1.382664 -1.3353664 -1.2314138 -1.128239]]...]
INFO - root - 2017-12-06 02:59:01.186849: step 5310, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.447 sec/batch; 40h:35m:58s remains)
INFO - root - 2017-12-06 02:59:05.732438: step 5320, loss = 0.85, batch loss = 0.64 (17.5 examples/sec; 0.456 sec/batch; 41h:29m:00s remains)
INFO - root - 2017-12-06 02:59:10.204456: step 5330, loss = 0.88, batch loss = 0.67 (16.7 examples/sec; 0.479 sec/batch; 43h:33m:21s remains)
INFO - root - 2017-12-06 02:59:14.691248: step 5340, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.453 sec/batch; 41h:11m:03s remains)
INFO - root - 2017-12-06 02:59:19.198611: step 5350, loss = 0.90, batch loss = 0.69 (17.1 examples/sec; 0.469 sec/batch; 42h:34m:50s remains)
INFO - root - 2017-12-06 02:59:23.614560: step 5360, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.448 sec/batch; 40h:40m:27s remains)
INFO - root - 2017-12-06 02:59:28.163578: step 5370, loss = 0.87, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 41h:03m:06s remains)
INFO - root - 2017-12-06 02:59:32.666309: step 5380, loss = 0.83, batch loss = 0.62 (16.3 examples/sec; 0.490 sec/batch; 44h:32m:29s remains)
INFO - root - 2017-12-06 02:59:37.224233: step 5390, loss = 0.89, batch loss = 0.68 (17.8 examples/sec; 0.450 sec/batch; 40h:51m:07s remains)
INFO - root - 2017-12-06 02:59:41.600668: step 5400, loss = 0.89, batch loss = 0.67 (17.9 examples/sec; 0.447 sec/batch; 40h:35m:26s remains)
2017-12-06 02:59:42.118714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0084038377 -0.035934508 -0.097792864 -0.32456443 -0.55329669 -0.82070285 -1.0626036 -1.2265463 -1.3524368 -1.2860173 -1.1917535 -0.99943513 -0.84417528 -0.63192832 -0.55022871][0.13747171 0.1087051 0.052546836 -0.14257906 -0.40657285 -0.76362747 -1.1388468 -1.4387404 -1.6466136 -1.6840937 -1.6531657 -1.4456887 -1.2257972 -0.99104875 -0.946691][0.31112266 0.2934792 0.26747218 0.11481234 -0.11808791 -0.47615457 -0.90091771 -1.2610997 -1.5074294 -1.6659948 -1.7311351 -1.6613483 -1.5466378 -1.3470987 -1.2142477][0.47707585 0.51681584 0.51879764 0.42448241 0.24638352 -0.082689345 -0.47769681 -0.91826636 -1.2276312 -1.4499916 -1.5845953 -1.6973751 -1.6867963 -1.597725 -1.5789444][0.50688469 0.61275077 0.69655144 0.72675985 0.69591612 0.41272271 0.058560051 -0.30769798 -0.51558566 -0.76276636 -0.9344306 -1.15609 -1.4332279 -1.5762601 -1.6878707][0.37510863 0.59963441 0.77886182 0.96164733 1.0650395 0.98089 0.73711383 0.34656367 0.062632374 -0.11092686 -0.22396493 -0.42624852 -0.64501238 -0.83689761 -1.1184618][0.23179492 0.47555134 0.69522893 1.0447518 1.2138876 1.2779236 1.1670308 0.88534719 0.64607561 0.38741022 0.26749682 0.19407964 -0.023095597 -0.21586165 -0.36725834][-0.012364302 0.1883983 0.43930486 0.79082274 0.96500725 1.1044887 1.0157051 0.89982891 0.77201205 0.66520506 0.65669352 0.52657664 0.34789813 0.27012554 0.13563418][-0.22894743 -0.22104776 -0.076814085 0.20424905 0.37238064 0.49566713 0.42631027 0.48341098 0.36320385 0.39889961 0.40263158 0.38719103 0.33582073 0.17553705 0.18852243][-0.42369372 -0.60285813 -0.67303318 -0.57194853 -0.4569774 -0.30500722 -0.35623282 -0.31845456 -0.40552244 -0.32434234 -0.341669 -0.17496477 -0.32000464 -0.2025817 -0.27761802][-0.59780985 -0.91722172 -1.092993 -1.1940736 -1.2097642 -1.171137 -1.1944928 -1.0574993 -1.1050164 -1.039338 -1.0904433 -0.98678011 -1.0309312 -0.87098509 -0.976492][-0.56002867 -0.89883322 -1.1735806 -1.3015457 -1.411505 -1.441218 -1.4774081 -1.4351181 -1.4697391 -1.4175925 -1.5802099 -1.6649947 -1.770275 -1.5917028 -1.5872287][-0.3781777 -0.65761966 -0.92987961 -1.0479084 -1.1612725 -1.2132826 -1.2882971 -1.3304985 -1.3896195 -1.4816558 -1.6797454 -1.8157719 -2.029582 -1.939979 -2.0854104][-0.091826037 -0.304214 -0.50872952 -0.60138547 -0.75153309 -0.80985665 -0.90948188 -1.052189 -1.2002465 -1.3558488 -1.5375692 -1.7296745 -1.9606932 -1.9478996 -1.9778863][0.084397055 -0.022027176 -0.14286512 -0.21551362 -0.31157032 -0.43355677 -0.65933383 -0.79802591 -0.9492498 -1.1628808 -1.3644652 -1.5399035 -1.7256886 -1.8253419 -1.8941294]]...]
INFO - root - 2017-12-06 02:59:46.640328: step 5410, loss = 0.88, batch loss = 0.66 (17.3 examples/sec; 0.462 sec/batch; 41h:59m:39s remains)
INFO - root - 2017-12-06 02:59:51.106056: step 5420, loss = 0.92, batch loss = 0.71 (18.0 examples/sec; 0.444 sec/batch; 40h:18m:11s remains)
INFO - root - 2017-12-06 02:59:55.578855: step 5430, loss = 0.86, batch loss = 0.65 (17.5 examples/sec; 0.458 sec/batch; 41h:34m:27s remains)
INFO - root - 2017-12-06 03:00:00.233212: step 5440, loss = 0.83, batch loss = 0.62 (17.7 examples/sec; 0.453 sec/batch; 41h:07m:32s remains)
INFO - root - 2017-12-06 03:00:04.664500: step 5450, loss = 0.95, batch loss = 0.74 (17.5 examples/sec; 0.458 sec/batch; 41h:37m:40s remains)
INFO - root - 2017-12-06 03:00:09.213729: step 5460, loss = 0.85, batch loss = 0.63 (17.7 examples/sec; 0.451 sec/batch; 40h:58m:38s remains)
INFO - root - 2017-12-06 03:00:13.731551: step 5470, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.444 sec/batch; 40h:18m:43s remains)
INFO - root - 2017-12-06 03:00:18.137672: step 5480, loss = 0.86, batch loss = 0.65 (18.3 examples/sec; 0.437 sec/batch; 39h:44m:21s remains)
INFO - root - 2017-12-06 03:00:22.717457: step 5490, loss = 0.90, batch loss = 0.68 (17.6 examples/sec; 0.454 sec/batch; 41h:15m:21s remains)
INFO - root - 2017-12-06 03:00:27.010303: step 5500, loss = 0.90, batch loss = 0.69 (17.8 examples/sec; 0.450 sec/batch; 40h:53m:21s remains)
2017-12-06 03:00:27.416901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2802154 -1.3495637 -1.4965376 -1.58007 -1.6013302 -1.6298318 -1.6099663 -1.5205718 -1.3733622 -1.3764598 -1.3753906 -1.3015258 -1.1838635 -1.0920328 -0.99175984][-1.2489492 -1.239311 -1.2546779 -1.2651452 -1.3731079 -1.5417645 -1.6166345 -1.5795132 -1.5063286 -1.4281682 -1.3001301 -1.2570984 -1.2011777 -1.1593723 -1.1430945][-0.37688974 -0.29782468 -0.36456183 -0.3824327 -0.47729239 -0.58622444 -0.66188914 -0.73582405 -0.81109357 -0.92210913 -1.0063515 -1.0603747 -1.0725759 -1.2167165 -1.2844907][0.30325359 0.69758087 0.89943475 0.97407317 0.86815578 0.65207839 0.4184784 0.38137382 0.469453 0.45553866 0.24911293 -0.015785258 -0.16376868 -0.26755843 -0.3782717][0.20541283 0.6580863 1.0039796 1.2550313 1.4071898 1.4080216 1.239378 1.0533276 0.89460546 0.84310812 0.70952564 0.51398891 0.50954169 0.47150967 0.43862498][0.39516863 0.69826549 0.973542 1.0936826 1.1927052 1.1667846 1.0759656 0.95236921 0.83213991 0.67515647 0.60659385 0.65860838 0.82098377 0.95897651 0.98338354][0.95162249 1.2093514 1.3330317 1.2997894 1.3015422 1.1869236 0.97149682 0.80445093 0.64004809 0.38616797 0.23333544 0.35232335 0.78807896 1.2107733 1.4501454][1.4143814 1.7067417 1.8482426 1.8113774 1.6576315 1.5132974 1.24882 0.95097089 0.65695262 0.47794709 0.39831555 0.27552453 0.45176998 0.74317247 1.0297601][1.6052613 1.9418108 2.2356424 2.3184769 2.2860923 2.1951585 1.9097832 1.5248663 1.1552823 0.86477345 0.71860653 0.7427156 0.9355467 1.2043055 1.2937026][1.4492925 1.8357843 2.1768858 2.3431554 2.3306308 2.2267046 1.9684942 1.6341083 1.2961866 1.0371125 0.93418467 1.0032452 1.2116144 1.4531897 1.7154603][1.0072927 1.4151518 1.7405021 2.0279131 2.067862 1.9985847 1.7363539 1.4255677 1.031412 0.7401948 0.64138854 0.71876878 0.84697074 1.148942 1.4113719][0.71288234 0.99010718 1.2727822 1.507955 1.7255243 1.7421194 1.5797341 1.3078276 0.98302364 0.70677739 0.4587048 0.41645965 0.50247604 0.71168935 0.96332312][0.19191173 0.45771006 0.77976942 0.97329891 1.1848539 1.3240365 1.3030145 1.1254826 0.84728438 0.61665672 0.37297294 0.25312269 0.19466725 0.2797313 0.43657383][-0.012634493 0.044746492 0.22198889 0.37194896 0.55271482 0.65978 0.74477732 0.69631135 0.56323379 0.3720634 0.19851577 0.13700242 0.1102667 0.16329557 0.25658196][-0.089061916 -0.0818793 0.0066016503 0.11580233 0.22019815 0.3634795 0.45901582 0.48596385 0.43208143 0.27073967 0.11054389 0.015831757 -0.024362426 -0.0082401074 0.056954969]]...]
INFO - root - 2017-12-06 03:00:31.953166: step 5510, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.446 sec/batch; 40h:32m:49s remains)
INFO - root - 2017-12-06 03:00:36.450030: step 5520, loss = 0.87, batch loss = 0.65 (17.5 examples/sec; 0.457 sec/batch; 41h:32m:38s remains)
INFO - root - 2017-12-06 03:00:40.900260: step 5530, loss = 0.86, batch loss = 0.65 (18.0 examples/sec; 0.445 sec/batch; 40h:24m:29s remains)
INFO - root - 2017-12-06 03:00:45.471333: step 5540, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.461 sec/batch; 41h:54m:47s remains)
INFO - root - 2017-12-06 03:00:49.967780: step 5550, loss = 0.82, batch loss = 0.61 (18.1 examples/sec; 0.443 sec/batch; 40h:12m:16s remains)
INFO - root - 2017-12-06 03:00:54.464571: step 5560, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:35m:38s remains)
INFO - root - 2017-12-06 03:00:59.045451: step 5570, loss = 0.89, batch loss = 0.67 (17.4 examples/sec; 0.459 sec/batch; 41h:39m:19s remains)
INFO - root - 2017-12-06 03:01:03.511004: step 5580, loss = 0.92, batch loss = 0.71 (18.0 examples/sec; 0.445 sec/batch; 40h:25m:13s remains)
INFO - root - 2017-12-06 03:01:08.075837: step 5590, loss = 0.84, batch loss = 0.63 (17.9 examples/sec; 0.447 sec/batch; 40h:38m:11s remains)
INFO - root - 2017-12-06 03:01:12.482427: step 5600, loss = 0.86, batch loss = 0.65 (23.4 examples/sec; 0.342 sec/batch; 31h:00m:50s remains)
2017-12-06 03:01:12.890050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029464018 -0.029132526 -0.02913738 -0.029108491 -0.029064558 -0.029088657 -0.029194653 -0.029298376 -0.029341929 -0.029291671 -0.029212758 -0.029196396 -0.029190376 -0.029155556 -0.029497348][-0.029325772 -0.028992705 -0.029080335 -0.029045645 -0.028962888 -0.028957088 -0.029049508 -0.029187392 -0.029278103 -0.029207584 -0.029080383 -0.029005509 -0.028966431 -0.028904542 -0.029227652][-0.029527526 -0.0292462 -0.029466756 -0.029505238 -0.029428288 -0.029354058 -0.029438388 -0.029645231 -0.029814539 -0.029772948 -0.02954617 -0.029318191 -0.029171765 -0.029066604 -0.029282253][-0.0296879 -0.029516771 -0.029921802 -0.030096767 -0.030052911 -0.029945737 -0.030050581 -0.030369824 -0.030684339 -0.03077531 -0.030410996 -0.029921381 -0.029560151 -0.029323608 -0.029398236][-0.029948693 -0.029920161 -0.030536724 -0.030889656 -0.030906947 -0.030752728 -0.03089086 -0.031392142 -0.0319217 -0.032154206 -0.031660326 -0.030866539 -0.030162619 -0.029679596 -0.029571263][-0.030301219 -0.030527337 -0.03144259 -0.032063521 -0.032223262 -0.0320985 -0.032210693 -0.032785945 -0.033519082 -0.033789348 -0.033199266 -0.032105993 -0.030987406 -0.0301576 -0.029877529][-0.03071218 -0.031335503 -0.03262753 -0.033617977 -0.034166768 -0.034181967 -0.034238327 -0.034687214 -0.035455652 -0.035709634 -0.034974709 -0.033552002 -0.031990889 -0.030801049 -0.030268328][-0.031082254 -0.032044895 -0.033778913 -0.035333589 -0.036502313 -0.037022687 -0.037297223 -0.037517361 -0.037996128 -0.037870981 -0.036703389 -0.034871466 -0.032876603 -0.031362839 -0.030541778][-0.031161658 -0.032364849 -0.034502834 -0.036698088 -0.038616952 -0.040030479 -0.040991008 -0.041375235 -0.041214112 -0.040228 -0.038312458 -0.035942018 -0.033506528 -0.031668857 -0.030728277][-0.031078285 -0.032394115 -0.034861419 -0.037632886 -0.040205289 -0.042403311 -0.044008076 -0.044484064 -0.043837078 -0.042108551 -0.039528891 -0.036618754 -0.033834163 -0.031765103 -0.030746758][-0.030929467 -0.0322337 -0.034880139 -0.037955906 -0.040844183 -0.04342296 -0.045277994 -0.045689806 -0.04463806 -0.042631559 -0.039809547 -0.036713738 -0.033836134 -0.031648815 -0.03058625][-0.030692605 -0.031972412 -0.034510843 -0.037491787 -0.040284488 -0.042788111 -0.044498224 -0.044657111 -0.043429904 -0.041407131 -0.038696721 -0.0357852 -0.033160143 -0.031175368 -0.030254472][-0.030375822 -0.031444702 -0.033651888 -0.036247049 -0.038643762 -0.040741384 -0.042027302 -0.042023979 -0.040824287 -0.03894968 -0.036604069 -0.034160983 -0.032030351 -0.030476136 -0.029885715][-0.02982975 -0.030621551 -0.032309253 -0.034301713 -0.036173206 -0.03778794 -0.038698219 -0.038621955 -0.037639283 -0.036150966 -0.034418508 -0.032593772 -0.03098304 -0.0298791 -0.029543856][-0.029802278 -0.030180888 -0.03119641 -0.032430127 -0.033628561 -0.03463934 -0.035236195 -0.035124 -0.034450237 -0.033462837 -0.032419417 -0.03137761 -0.030412482 -0.029781472 -0.02971009]]...]
INFO - root - 2017-12-06 03:01:17.435223: step 5610, loss = 0.90, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:36m:27s remains)
INFO - root - 2017-12-06 03:01:21.963211: step 5620, loss = 0.92, batch loss = 0.71 (17.7 examples/sec; 0.452 sec/batch; 40h:59m:52s remains)
INFO - root - 2017-12-06 03:01:26.466118: step 5630, loss = 0.92, batch loss = 0.71 (17.9 examples/sec; 0.446 sec/batch; 40h:31m:58s remains)
INFO - root - 2017-12-06 03:01:30.988963: step 5640, loss = 0.84, batch loss = 0.63 (17.9 examples/sec; 0.447 sec/batch; 40h:35m:06s remains)
INFO - root - 2017-12-06 03:01:35.571131: step 5650, loss = 0.86, batch loss = 0.65 (16.5 examples/sec; 0.485 sec/batch; 44h:04m:31s remains)
INFO - root - 2017-12-06 03:01:40.137458: step 5660, loss = 0.86, batch loss = 0.64 (16.9 examples/sec; 0.473 sec/batch; 42h:55m:24s remains)
INFO - root - 2017-12-06 03:01:44.599916: step 5670, loss = 0.92, batch loss = 0.71 (18.0 examples/sec; 0.444 sec/batch; 40h:20m:53s remains)
INFO - root - 2017-12-06 03:01:49.090597: step 5680, loss = 0.92, batch loss = 0.71 (17.8 examples/sec; 0.449 sec/batch; 40h:47m:16s remains)
INFO - root - 2017-12-06 03:01:53.542175: step 5690, loss = 0.90, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 39h:53m:32s remains)
INFO - root - 2017-12-06 03:01:58.105962: step 5700, loss = 0.90, batch loss = 0.68 (17.8 examples/sec; 0.450 sec/batch; 40h:52m:30s remains)
2017-12-06 03:01:58.541588: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0092536621 0.05561525 0.10755412 0.17664334 0.25001979 0.31375325 0.36180887 0.38238895 0.39368942 0.38026676 0.34929669 0.33140469 0.30877951 0.27155307 0.26711044][-0.00651348 0.034248043 0.075426295 0.12672141 0.19051574 0.24780227 0.28477925 0.31485856 0.31930444 0.30785576 0.28574735 0.25477946 0.23139139 0.1868206 0.17044562][-0.020229317 0.0037498735 0.028754484 0.058703963 0.098730937 0.13841498 0.16557468 0.18202153 0.19307123 0.18600003 0.16929707 0.15462439 0.12460478 0.093261927 0.07829386][-0.030723039 -0.021919724 -0.012787733 -0.0027827881 0.010700207 0.0241098 0.034360614 0.0435148 0.049938265 0.049625386 0.044969421 0.036441062 0.02327523 0.0087679662 0.0029085092][-0.031164054 -0.02605062 -0.021361794 -0.018523965 -0.01732849 -0.016997013 -0.016699404 -0.015293181 -0.012821663 -0.010503121 -0.0090258233 -0.0092989206 -0.011472572 -0.015551414 -0.017715815][-0.029946389 -0.025222663 -0.02055819 -0.017102219 -0.014885727 -0.011701874 -0.0069934055 -0.001554694 0.002210442 0.0026615001 0.00095134974 -0.003180813 -0.0091375336 -0.016175307 -0.019636743][-0.029587219 -0.025076143 -0.020992476 -0.018358696 -0.018097807 -0.018804893 -0.019810967 -0.019521225 -0.018366065 -0.017471664 -0.017211061 -0.017144941 -0.018098351 -0.022564922 -0.018157929][-0.029617824 -0.025448862 -0.021568276 -0.019282795 -0.019300193 -0.022582263 -0.027775746 -0.032528829 -0.034844458 -0.03478498 -0.034173205 -0.032545071 -0.029556166 -0.026116095 -0.024724472][-0.029809874 -0.025664765 -0.02187999 -0.019674506 -0.01953176 -0.021143805 -0.023694061 -0.026220817 -0.028002243 -0.028747305 -0.028804779 -0.027965635 -0.026456129 -0.024657421 -0.023572635][-0.030149484 -0.025939126 -0.0224582 -0.020255305 -0.019975904 -0.021028906 -0.022417665 -0.023598403 -0.023962822 -0.024429068 -0.024808154 -0.024443571 -0.02416655 -0.024132267 -0.024115786][-0.030354649 -0.026205149 -0.023291524 -0.021245573 -0.020625532 -0.021596424 -0.023340251 -0.024868108 -0.024070341 -0.024128698 -0.02399344 -0.023800705 -0.023579039 -0.023516238 -0.023533415][-0.030510988 -0.026434269 -0.021551829 -0.020838767 -0.019745115 -0.020400722 -0.021674249 -0.024312355 -0.024107177 -0.02386184 -0.023573793 -0.023330245 -0.023147851 -0.023089308 -0.023172181][-0.030549457 -0.026518986 -0.022860695 -0.021021865 -0.020646643 -0.021107871 -0.022777826 -0.024099976 -0.024516579 -0.023599889 -0.023171052 -0.023478206 -0.023003202 -0.023015123 -0.02320784][-0.030654341 -0.026653942 -0.022931252 -0.021151755 -0.020414542 -0.020913281 -0.023664527 -0.025548797 -0.024070725 -0.023176093 -0.023786057 -0.024873771 -0.022937361 -0.022999305 -0.023205571][-0.031727217 -0.027801614 -0.024140235 -0.021341309 -0.020350378 -0.020838454 -0.02420501 -0.024932347 -0.024497855 -0.025110915 -0.026040073 -0.024162568 -0.023423564 -0.023531236 -0.023818996]]...]
INFO - root - 2017-12-06 03:02:02.922670: step 5710, loss = 0.87, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 40h:59m:57s remains)
INFO - root - 2017-12-06 03:02:07.379795: step 5720, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.437 sec/batch; 39h:42m:01s remains)
INFO - root - 2017-12-06 03:02:11.945516: step 5730, loss = 0.84, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 40h:49m:12s remains)
INFO - root - 2017-12-06 03:02:16.460907: step 5740, loss = 0.87, batch loss = 0.66 (17.5 examples/sec; 0.456 sec/batch; 41h:25m:49s remains)
INFO - root - 2017-12-06 03:02:20.959133: step 5750, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.438 sec/batch; 39h:46m:56s remains)
INFO - root - 2017-12-06 03:02:25.494790: step 5760, loss = 0.89, batch loss = 0.67 (18.3 examples/sec; 0.438 sec/batch; 39h:46m:57s remains)
INFO - root - 2017-12-06 03:02:29.979842: step 5770, loss = 0.88, batch loss = 0.67 (18.4 examples/sec; 0.434 sec/batch; 39h:24m:56s remains)
INFO - root - 2017-12-06 03:02:34.486483: step 5780, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.455 sec/batch; 41h:19m:53s remains)
INFO - root - 2017-12-06 03:02:39.087740: step 5790, loss = 0.86, batch loss = 0.65 (17.2 examples/sec; 0.466 sec/batch; 42h:14m:57s remains)
INFO - root - 2017-12-06 03:02:43.675483: step 5800, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.438 sec/batch; 39h:44m:21s remains)
2017-12-06 03:02:44.105012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037412986 -0.039972473 -0.042507354 -0.044719912 -0.046782151 -0.04862117 -0.049632385 -0.049247257 -0.047173332 -0.043376476 -0.038635135 -0.0340484 -0.029742137 -0.026221205 -0.024692383][-0.040029727 -0.042042296 -0.044095404 -0.045855612 -0.047598779 -0.049364794 -0.050568864 -0.050530359 -0.049037892 -0.04593759 -0.041819684 -0.037546709 -0.033349324 -0.029556006 -0.027343702][-0.042453364 -0.043501481 -0.044687092 -0.04579483 -0.046990894 -0.048335403 -0.049353741 -0.049480364 -0.0484458 -0.046102058 -0.042833846 -0.039229847 -0.035474665 -0.031772677 -0.029196024][-0.043758772 -0.043759663 -0.04408434 -0.044514649 -0.045021478 -0.045676265 -0.046221506 -0.0462075 -0.045324836 -0.043530323 -0.0412511 -0.038591541 -0.035460111 -0.032153912 -0.029562131][-0.043121852 -0.042441376 -0.042119786 -0.042002466 -0.041862637 -0.041927435 -0.042122826 -0.041894861 -0.040996123 -0.039454751 -0.037905116 -0.036142476 -0.033724379 -0.030834233 -0.028244466][-0.040199462 -0.039243672 -0.038793053 -0.0385072 -0.037906773 -0.037578285 -0.03761613 -0.03730572 -0.036331605 -0.034814388 -0.033575382 -0.032461558 -0.030819369 -0.028435588 -0.025969487][-0.035415944 -0.034521319 -0.034380492 -0.034479126 -0.033723477 -0.032960158 -0.032919228 -0.032678351 -0.031639308 -0.030061565 -0.028897002 -0.028135464 -0.027092181 -0.0251849 -0.022963934][-0.029321834 -0.028899107 -0.029372618 -0.030288428 -0.030186847 -0.029448573 -0.029168952 -0.028883066 -0.027761571 -0.026159246 -0.02488523 -0.024117041 -0.023310103 -0.021705322 -0.019743267][-0.022046246 -0.022447411 -0.023904994 -0.025666196 -0.026547611 -0.026447181 -0.026214018 -0.025847878 -0.024509113 -0.022716802 -0.021163505 -0.02015011 -0.019071657 -0.01747968 -0.015665025][-0.014472533 -0.015528522 -0.017879713 -0.020221487 -0.021780711 -0.022495206 -0.022692837 -0.022455823 -0.0211468 -0.019081481 -0.017149325 -0.015703827 -0.014196515 -0.012421317 -0.010844328][-0.0076615177 -0.00899725 -0.011599295 -0.014196675 -0.016103644 -0.01735118 -0.017925318 -0.017847676 -0.016774114 -0.014740642 -0.012641933 -0.010929286 -0.0093362778 -0.0075681955 -0.0060653687][-0.0026152357 -0.0037702359 -0.0060182065 -0.0083643384 -0.010200728 -0.011495311 -0.012216579 -0.012278475 -0.01149261 -0.0098846927 -0.0081554241 -0.0066659264 -0.0051857755 -0.0036757477 -0.0024513975][0.00086697191 7.56979e-05 -0.0015105419 -0.0032064393 -0.0046344772 -0.0056803636 -0.0063554943 -0.0066117197 -0.0061973929 -0.0051626824 -0.0040640943 -0.0031389445 -0.0020478405 -0.00094274059 -0.00023522973][0.0027802512 0.0026034191 0.0017289445 0.00079207122 -3.0588359e-05 -0.00063712895 -0.001112286 -0.0014229789 -0.0013515353 -0.00096043944 -0.00051165745 -9.1541559e-05 0.00046334043 0.0010535866 0.0012525842][0.0029513985 0.0031087324 0.0027533323 0.0023563728 0.002151221 0.001888454 0.0015394315 0.0012127906 0.00094811618 0.00093847513 0.0010592714 0.0012177229 0.0015075281 0.0018750951 0.0018888637]]...]
INFO - root - 2017-12-06 03:02:48.437714: step 5810, loss = 0.83, batch loss = 0.62 (17.2 examples/sec; 0.465 sec/batch; 42h:10m:09s remains)
INFO - root - 2017-12-06 03:02:52.962762: step 5820, loss = 0.83, batch loss = 0.62 (17.5 examples/sec; 0.457 sec/batch; 41h:27m:30s remains)
INFO - root - 2017-12-06 03:02:57.437348: step 5830, loss = 0.83, batch loss = 0.62 (18.0 examples/sec; 0.445 sec/batch; 40h:20m:56s remains)
INFO - root - 2017-12-06 03:03:01.879265: step 5840, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 41h:11m:23s remains)
INFO - root - 2017-12-06 03:03:06.434438: step 5850, loss = 0.83, batch loss = 0.61 (17.5 examples/sec; 0.457 sec/batch; 41h:25m:51s remains)
INFO - root - 2017-12-06 03:03:10.980271: step 5860, loss = 0.90, batch loss = 0.69 (17.1 examples/sec; 0.467 sec/batch; 42h:23m:17s remains)
INFO - root - 2017-12-06 03:03:15.513701: step 5870, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.448 sec/batch; 40h:39m:02s remains)
INFO - root - 2017-12-06 03:03:20.113642: step 5880, loss = 0.90, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:03m:55s remains)
INFO - root - 2017-12-06 03:03:24.619405: step 5890, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.445 sec/batch; 40h:20m:33s remains)
INFO - root - 2017-12-06 03:03:29.147935: step 5900, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.453 sec/batch; 41h:07m:06s remains)
2017-12-06 03:03:29.574560: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013040412 0.014575798 0.015599247 0.016759571 0.01761448 0.01799633 0.018088657 0.018035803 0.017930198 0.017845716 0.01776677 0.017669376 0.017452445 0.01697154 0.015649658][0.0067687593 0.0083025582 0.0098116361 0.011741426 0.013388764 0.014402058 0.014840674 0.014855947 0.014664996 0.014498573 0.014353003 0.014211919 0.013914984 0.013339411 0.012043286][-0.0014756434 -0.00027983263 0.0016996302 0.0046297126 0.0074703731 0.00952993 0.010672096 0.010994982 0.010861319 0.01066054 0.010472815 0.010290768 0.00993159 0.00923061 0.0078596286][-0.01049925 -0.0098724477 -0.007438384 -0.0032781474 0.0010894351 0.0045432039 0.0066740848 0.007479731 0.00749116 0.0072714575 0.0070696361 0.006871406 0.0064412393 0.0056003444 0.0040082][-0.018608868 -0.018660657 -0.015887346 -0.010679089 -0.0048678815 1.9457191e-05 0.0031625889 0.0045314096 0.0047549494 0.0046109222 0.0044834055 0.0043775849 0.0039576925 0.0029920228 0.0011743046][-0.024869043 -0.025634471 -0.022774495 -0.016780246 -0.0098440163 -0.0037556998 0.00029711798 0.0022585057 0.0027368329 0.0027643703 0.0028101839 0.0029086657 0.0026198141 0.0016321354 -0.00035240129][-0.028591901 -0.030103583 -0.027332287 -0.020942628 -0.013273101 -0.0063482076 -0.0015774891 0.00091053918 0.0016905256 0.0019494332 0.0022737347 0.0026399903 0.0025636591 0.0016178824 -0.00037490577][-0.029082395 -0.031121494 -0.028608765 -0.022229873 -0.014379513 -0.0071830787 -0.0021090172 0.000652615 0.0016632639 0.002124574 0.0027275421 0.0033927672 0.0035841055 0.0027374662 0.00087132677][-0.025828805 -0.028116312 -0.025992773 -0.020056646 -0.012684178 -0.0058850236 -0.0010898113 0.0015956126 0.0026894547 0.0033242516 0.0041856281 0.0051299967 0.005595278 0.0049452893 0.0032523535][-0.01945949 -0.021755312 -0.02018147 -0.015081219 -0.0087194256 -0.0028643087 0.0011963136 0.0035412945 0.0046075322 0.0053810813 0.0064696409 0.0076633058 0.0084132291 0.0080240555 0.0066041835][-0.010982703 -0.013181582 -0.012197003 -0.0081780069 -0.0031128228 0.0015131943 0.0046700723 0.0065221228 0.0074733309 0.008271087 0.0094400756 0.010731313 0.011652905 0.011473183 0.010311004][-0.0018901043 -0.0038346425 -0.0033935346 -0.00051815063 0.0032540597 0.0066922046 0.0090254582 0.010392059 0.011216473 0.011966791 0.013075095 0.014299911 0.01520538 0.01519515 0.014278885][0.006674435 0.0054112114 0.0055647306 0.0073944069 0.0099435635 0.012281146 0.013856459 0.014750648 0.015382852 0.016027179 0.016955908 0.017973233 0.018734884 0.018798891 0.017792646][0.013079021 0.012540776 0.012646493 0.01370528 0.015222829 0.016533989 0.017319921 0.017817218 0.018254917 0.01887292 0.019703534 0.020467516 0.021130089 0.021213923 0.020137634][0.016231399 0.016375657 0.016593721 0.017142277 0.017933961 0.018435154 0.01863436 0.018721808 0.018833715 0.019268606 0.019863237 0.020365242 0.020822231 0.020925831 0.019936662]]...]
INFO - root - 2017-12-06 03:03:34.069665: step 5910, loss = 0.88, batch loss = 0.67 (17.7 examples/sec; 0.453 sec/batch; 41h:06m:56s remains)
INFO - root - 2017-12-06 03:03:38.463482: step 5920, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.463 sec/batch; 41h:57m:50s remains)
INFO - root - 2017-12-06 03:03:42.955809: step 5930, loss = 0.87, batch loss = 0.66 (17.0 examples/sec; 0.471 sec/batch; 42h:46m:01s remains)
INFO - root - 2017-12-06 03:03:47.470715: step 5940, loss = 0.87, batch loss = 0.66 (17.2 examples/sec; 0.466 sec/batch; 42h:15m:51s remains)
INFO - root - 2017-12-06 03:03:52.021757: step 5950, loss = 0.83, batch loss = 0.62 (18.1 examples/sec; 0.441 sec/batch; 40h:00m:40s remains)
INFO - root - 2017-12-06 03:03:56.528298: step 5960, loss = 0.87, batch loss = 0.66 (17.5 examples/sec; 0.456 sec/batch; 41h:21m:44s remains)
INFO - root - 2017-12-06 03:04:01.001470: step 5970, loss = 0.81, batch loss = 0.59 (17.2 examples/sec; 0.466 sec/batch; 42h:16m:29s remains)
INFO - root - 2017-12-06 03:04:05.528393: step 5980, loss = 0.83, batch loss = 0.62 (17.4 examples/sec; 0.461 sec/batch; 41h:48m:51s remains)
INFO - root - 2017-12-06 03:04:10.010264: step 5990, loss = 0.89, batch loss = 0.67 (17.6 examples/sec; 0.456 sec/batch; 41h:19m:57s remains)
INFO - root - 2017-12-06 03:04:14.513270: step 6000, loss = 0.86, batch loss = 0.64 (17.7 examples/sec; 0.452 sec/batch; 40h:59m:19s remains)
2017-12-06 03:04:14.944903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.29909903 -0.13486558 -0.050745375 -0.035841152 -0.031839076 -0.051290587 0.0089668036 0.052293055 0.071193732 0.11126579 0.11831645 0.13508958 0.12402766 0.12508133 0.13140491][-0.32718098 -0.15443054 -0.080290526 -0.11502621 -0.15434921 -0.18223697 -0.16893369 -0.10408719 -0.023837067 0.02678635 0.10029063 0.16415074 0.17791674 0.17199922 0.1412898][-0.31507894 -0.16620624 -0.020237509 -0.024866749 -0.099314533 -0.16985558 -0.22448361 -0.17734894 -0.10469069 -0.059460118 -0.019371301 0.048918441 0.12612817 0.15816227 0.16898248][-0.2508744 -0.089030378 0.032498047 0.12091612 0.003647618 -0.077032015 -0.11862062 -0.12273549 -0.10757463 -0.069105759 -0.042276755 -0.017676324 0.017702505 0.052171908 0.10183723][-0.12435984 0.0080448389 0.13143095 0.25371265 0.14482701 0.059986345 -0.014365692 -0.014964402 -0.031781919 -0.043936383 -0.045917906 -0.019333009 0.017337963 0.0054955781 0.01076512][0.027771302 0.16231948 0.24451575 0.35877758 0.31666037 0.27675024 0.12825492 0.1012565 0.10533368 0.077278979 0.026683182 0.0077011585 0.019466549 0.031326458 0.032056093][0.12075751 0.21221045 0.24619862 0.33184043 0.35165435 0.35723221 0.28126371 0.22866654 0.20496723 0.20411038 0.19323024 0.156474 0.13457391 0.10657146 0.077138372][0.080156617 0.20723811 0.17712787 0.24091324 0.23995259 0.29854435 0.31145486 0.33646438 0.34586838 0.33135384 0.33578384 0.31246427 0.24023578 0.15538144 0.10600812][-0.2098673 0.038725406 0.062781118 0.097198807 0.15071794 0.20219028 0.227725 0.31154191 0.38516182 0.44257936 0.44927534 0.43574992 0.34909132 0.23218346 0.15969422][-0.38115653 -0.22088376 -0.11836384 -0.036947452 0.0097927079 0.085809343 0.15489474 0.25084421 0.36713219 0.477883 0.52385527 0.538182 0.45006773 0.31473958 0.20342463][-0.51967746 -0.31619525 -0.16223997 -0.010961805 0.014870763 0.039151169 0.086487763 0.17847434 0.29953939 0.43777385 0.53927314 0.57554513 0.52407765 0.40384316 0.30906218][-0.48428512 -0.3080487 -0.14573869 -0.0089647286 0.062380992 0.08516816 0.090690352 0.11922785 0.16762909 0.28156993 0.43342471 0.51547521 0.48616865 0.38390943 0.31812552][-0.27603614 -0.1020524 -0.0012754425 0.045223437 0.083401956 0.11851013 0.17558762 0.16824523 0.18786705 0.22445038 0.26627353 0.34910929 0.35959628 0.30102047 0.23145851][-0.087068394 0.018652104 0.13303643 0.16116521 0.15898386 0.19155008 0.25603119 0.28457424 0.28693062 0.2375952 0.24536842 0.22722873 0.21100074 0.18928054 0.16346934][-0.11744142 0.016636372 0.12801218 0.17688078 0.2266694 0.24936935 0.30783352 0.3405104 0.330064 0.28044659 0.19603658 0.15905046 0.1226876 0.081595846 0.069983028]]...]
INFO - root - 2017-12-06 03:04:19.423915: step 6010, loss = 0.84, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 41h:08m:39s remains)
INFO - root - 2017-12-06 03:04:23.688254: step 6020, loss = 0.87, batch loss = 0.66 (18.1 examples/sec; 0.442 sec/batch; 40h:05m:00s remains)
INFO - root - 2017-12-06 03:04:28.205562: step 6030, loss = 0.85, batch loss = 0.63 (18.1 examples/sec; 0.441 sec/batch; 40h:00m:50s remains)
INFO - root - 2017-12-06 03:04:32.681800: step 6040, loss = 0.86, batch loss = 0.64 (18.1 examples/sec; 0.442 sec/batch; 40h:05m:38s remains)
INFO - root - 2017-12-06 03:04:37.292114: step 6050, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 41h:00m:41s remains)
INFO - root - 2017-12-06 03:04:41.814348: step 6060, loss = 0.89, batch loss = 0.68 (17.3 examples/sec; 0.462 sec/batch; 41h:52m:29s remains)
INFO - root - 2017-12-06 03:04:46.305565: step 6070, loss = 0.84, batch loss = 0.63 (17.7 examples/sec; 0.451 sec/batch; 40h:52m:37s remains)
INFO - root - 2017-12-06 03:04:50.820983: step 6080, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.448 sec/batch; 40h:35m:56s remains)
INFO - root - 2017-12-06 03:04:55.340476: step 6090, loss = 0.88, batch loss = 0.66 (17.5 examples/sec; 0.457 sec/batch; 41h:26m:42s remains)
INFO - root - 2017-12-06 03:04:59.855347: step 6100, loss = 0.86, batch loss = 0.64 (17.8 examples/sec; 0.449 sec/batch; 40h:43m:02s remains)
2017-12-06 03:05:00.332487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017369747 -0.018101431 -0.018955503 -0.019560464 -0.019560155 -0.01920804 -0.018662732 -0.018109541 -0.017627474 -0.017128102 -0.016865533 -0.016760435 -0.016842786 -0.017328184 -0.018829241][-0.017879341 -0.018994555 -0.020277776 -0.021026459 -0.021168217 -0.020851422 -0.020334143 -0.019730479 -0.019210104 -0.018750891 -0.018295158 -0.017899532 -0.017650552 -0.017893534 -0.019371144][-0.018560261 -0.020049863 -0.021782819 -0.02297524 -0.023437686 -0.023337156 -0.023069408 -0.022683792 -0.022321343 -0.021861847 -0.021306232 -0.020755626 -0.020233396 -0.020251896 -0.02157326][-0.01912475 -0.020987529 -0.023194928 -0.02491064 -0.025900342 -0.026354346 -0.02664515 -0.026857074 -0.0269766 -0.026752859 -0.026216127 -0.02550808 -0.024585955 -0.024174832 -0.025174536][-0.019791532 -0.022115 -0.024801087 -0.026969537 -0.028506756 -0.0295921 -0.030644596 -0.031688422 -0.03252849 -0.032829702 -0.032557778 -0.031882282 -0.0306964 -0.02986075 -0.030363359][-0.02077176 -0.023642797 -0.026798721 -0.029359166 -0.031333636 -0.032937139 -0.034689218 -0.0366652 -0.038440786 -0.039451919 -0.039739847 -0.039354466 -0.038198788 -0.0371305 -0.037147865][-0.021876447 -0.02549386 -0.02926562 -0.032314453 -0.034657568 -0.036503814 -0.038619019 -0.041180961 -0.043703035 -0.045485582 -0.046479717 -0.046568785 -0.04564612 -0.044526014 -0.044153363][-0.023164697 -0.027652897 -0.032278039 -0.036001556 -0.038702447 -0.040599443 -0.042608917 -0.045126818 -0.047775395 -0.0499395 -0.0514911 -0.052026633 -0.051524833 -0.050589837 -0.050102331][-0.024362449 -0.029781889 -0.035380948 -0.039875448 -0.0429566 -0.044786163 -0.046297662 -0.04817909 -0.050246954 -0.052191854 -0.053801421 -0.054603238 -0.054513745 -0.05395719 -0.05362919][-0.025016204 -0.031158444 -0.037562676 -0.042761415 -0.046210542 -0.047918357 -0.048760895 -0.049636029 -0.050729156 -0.051900737 -0.053046178 -0.053721253 -0.053848028 -0.053646989 -0.053636767][-0.02482558 -0.031177375 -0.037925877 -0.043550771 -0.04725945 -0.048994828 -0.049433433 -0.049552336 -0.049777556 -0.050070897 -0.050538644 -0.050780252 -0.05077149 -0.050545745 -0.050616864][-0.023569427 -0.029588941 -0.036116026 -0.041697245 -0.045465522 -0.04724399 -0.047610361 -0.047328375 -0.046880759 -0.04645979 -0.046250843 -0.045997187 -0.045666233 -0.045289211 -0.045398824][-0.0217276 -0.026838746 -0.032572124 -0.037559852 -0.041072555 -0.042887889 -0.043300681 -0.042891972 -0.0421587 -0.041336332 -0.040617645 -0.03987509 -0.039178528 -0.03859343 -0.038691379][-0.019594282 -0.023435574 -0.027977828 -0.031974684 -0.034885257 -0.0364861 -0.036840726 -0.03643522 -0.035661355 -0.034775369 -0.033863075 -0.032989249 -0.032192115 -0.031571638 -0.031700425][-0.018442668 -0.020929173 -0.024081934 -0.026875261 -0.028940644 -0.030094478 -0.030329246 -0.029918157 -0.029205829 -0.028397702 -0.027562205 -0.026818074 -0.026130576 -0.025653739 -0.025865059]]...]
INFO - root - 2017-12-06 03:05:04.795094: step 6110, loss = 1.08, batch loss = 0.86 (17.8 examples/sec; 0.449 sec/batch; 40h:43m:11s remains)
INFO - root - 2017-12-06 03:05:09.073233: step 6120, loss = 0.84, batch loss = 0.63 (33.9 examples/sec; 0.236 sec/batch; 21h:23m:31s remains)
INFO - root - 2017-12-06 03:05:13.572998: step 6130, loss = 0.84, batch loss = 0.63 (18.2 examples/sec; 0.440 sec/batch; 39h:54m:38s remains)
INFO - root - 2017-12-06 03:05:18.043029: step 6140, loss = 0.85, batch loss = 0.63 (17.0 examples/sec; 0.471 sec/batch; 42h:43m:05s remains)
INFO - root - 2017-12-06 03:05:22.581196: step 6150, loss = 0.86, batch loss = 0.65 (16.9 examples/sec; 0.475 sec/batch; 43h:02m:17s remains)
INFO - root - 2017-12-06 03:05:27.133452: step 6160, loss = 0.85, batch loss = 0.64 (17.1 examples/sec; 0.467 sec/batch; 42h:20m:29s remains)
INFO - root - 2017-12-06 03:05:31.706596: step 6170, loss = 0.80, batch loss = 0.59 (17.2 examples/sec; 0.466 sec/batch; 42h:13m:08s remains)
INFO - root - 2017-12-06 03:05:36.159115: step 6180, loss = 0.83, batch loss = 0.62 (17.8 examples/sec; 0.448 sec/batch; 40h:39m:07s remains)
INFO - root - 2017-12-06 03:05:40.611467: step 6190, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.448 sec/batch; 40h:35m:43s remains)
INFO - root - 2017-12-06 03:05:45.126602: step 6200, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.436 sec/batch; 39h:33m:34s remains)
2017-12-06 03:05:45.565126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.52545637 -0.7310214 -0.788385 -0.9842841 -1.0506202 -1.0001377 -0.8156746 -0.65527445 -0.54024178 -0.4728277 -0.33813715 -0.28064778 -0.13465336 0.10646494 0.34837162][-0.024757713 -0.21736413 -0.40253213 -0.4879173 -0.55344284 -0.54820859 -0.54075664 -0.40191138 -0.27287537 -0.088240623 0.053626582 0.15309253 0.25898492 0.3931627 0.73499548][0.65649951 0.60402173 0.34596428 0.18942481 0.18878457 0.09422306 0.0058355406 0.0043284521 0.034355767 0.070432507 0.14833242 0.15538064 0.17238835 0.2475059 0.40572947][0.94786423 0.746001 0.47528544 0.25506154 0.083164595 -0.03501609 -0.10375613 -0.12800232 0.01310949 0.0043884814 0.019186556 0.004608795 -0.026388604 -0.086526036 -0.0092170462][0.21029714 0.1850743 -0.0056978278 -0.23982888 -0.34764403 -0.42453519 -0.48336107 -0.53932482 -0.54551131 -0.51988357 -0.399539 -0.43408218 -0.46764216 -0.56008655 -0.59947336][-0.54941344 -0.61623025 -0.62400639 -0.7434721 -0.79304343 -0.77844632 -0.65021843 -0.61309385 -0.55878192 -0.61929792 -0.65930861 -0.78033775 -0.79584771 -0.88577777 -0.84592181][-0.77329093 -0.79316628 -0.68208331 -0.76984757 -0.9698481 -1.0163462 -0.92931503 -0.78820658 -0.63527381 -0.57751679 -0.57631123 -0.59832382 -0.65874553 -0.68924296 -0.74836564][-0.94173616 -0.83854645 -0.736738 -0.7514149 -0.80113363 -0.93522507 -0.87230963 -0.77372342 -0.69161248 -0.51896852 -0.53075093 -0.49420121 -0.48006198 -0.49987069 -0.45711878][-0.78514946 -0.71186912 -0.8145529 -0.82874662 -0.80829847 -0.72019678 -0.67608821 -0.54882151 -0.47837311 -0.46555689 -0.4543086 -0.45418781 -0.43052205 -0.29870126 -0.1607511][-0.58129239 -0.60586625 -0.55663055 -0.71658427 -0.83764625 -0.70995873 -0.61120075 -0.62051934 -0.56893671 -0.45157641 -0.37552071 -0.361828 -0.3069948 -0.22935918 -0.058904048][-0.84995478 -0.76443952 -0.74980181 -0.84919 -0.84831548 -0.88672769 -0.91913384 -0.85955936 -0.75170869 -0.60613251 -0.47075722 -0.48244625 -0.43378311 -0.35757202 -0.12511992][-1.1758952 -1.2455564 -1.1832919 -1.1153549 -1.0607786 -0.95499068 -0.84775382 -0.77764618 -0.75869024 -0.71569282 -0.69779694 -0.61537921 -0.47224987 -0.49469128 -0.40723315][-1.3261894 -1.2403309 -1.1332943 -1.1072475 -1.0039515 -0.85224777 -0.76435715 -0.64729738 -0.58729458 -0.53271043 -0.57133746 -0.61802518 -0.68052125 -0.61722708 -0.42827287][-1.3385816 -1.3024424 -1.2248797 -1.1593245 -0.9665677 -0.78137034 -0.5975917 -0.47482964 -0.38908947 -0.3510699 -0.39601269 -0.511655 -0.66348648 -0.7607823 -0.71932805][-1.0948732 -1.1309496 -1.1657653 -1.131724 -0.96214187 -0.70438445 -0.47819805 -0.40053338 -0.38322291 -0.35292968 -0.35742882 -0.44132814 -0.56537324 -0.70903611 -0.72895265]]...]
INFO - root - 2017-12-06 03:05:49.987844: step 6210, loss = 0.86, batch loss = 0.64 (17.9 examples/sec; 0.446 sec/batch; 40h:24m:30s remains)
INFO - root - 2017-12-06 03:05:54.507020: step 6220, loss = 0.90, batch loss = 0.69 (17.5 examples/sec; 0.456 sec/batch; 41h:20m:40s remains)
INFO - root - 2017-12-06 03:05:58.906701: step 6230, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.453 sec/batch; 41h:05m:44s remains)
INFO - root - 2017-12-06 03:06:03.435814: step 6240, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.452 sec/batch; 40h:59m:51s remains)
INFO - root - 2017-12-06 03:06:07.919997: step 6250, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 40h:39m:11s remains)
INFO - root - 2017-12-06 03:06:12.541241: step 6260, loss = 0.85, batch loss = 0.64 (17.4 examples/sec; 0.460 sec/batch; 41h:39m:52s remains)
INFO - root - 2017-12-06 03:06:17.120287: step 6270, loss = 0.84, batch loss = 0.63 (16.6 examples/sec; 0.482 sec/batch; 43h:41m:44s remains)
INFO - root - 2017-12-06 03:06:21.595555: step 6280, loss = 0.86, batch loss = 0.64 (18.2 examples/sec; 0.441 sec/batch; 39h:55m:56s remains)
INFO - root - 2017-12-06 03:06:26.117108: step 6290, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.446 sec/batch; 40h:24m:12s remains)
INFO - root - 2017-12-06 03:06:30.555475: step 6300, loss = 0.97, batch loss = 0.75 (17.8 examples/sec; 0.448 sec/batch; 40h:38m:11s remains)
2017-12-06 03:06:31.049912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049301926 -0.041294586 -0.029170983 -0.016666241 -0.0098888017 -0.01613427 -0.029115185 -0.042754773 -0.052738842 -0.055819906 -0.053135157 -0.046725821 -0.040862419 -0.030954614 -0.027294427][-0.057105523 -0.053091928 -0.04565689 -0.033996556 -0.026073191 -0.028094426 -0.037414223 -0.049591515 -0.0615234 -0.066577733 -0.067735411 -0.064910971 -0.060419694 -0.056247264 -0.053641267][-0.059116986 -0.060066763 -0.058498807 -0.051107794 -0.043108054 -0.038031779 -0.040142 -0.048449967 -0.060588297 -0.069429763 -0.076260164 -0.076082908 -0.073890641 -0.070196688 -0.068220079][-0.056910783 -0.059536703 -0.063199379 -0.061950076 -0.058649175 -0.052817889 -0.050778326 -0.050779365 -0.056563295 -0.0656074 -0.076067038 -0.081808105 -0.084049731 -0.082636818 -0.081890807][-0.055165954 -0.056516107 -0.06072754 -0.063540004 -0.066024348 -0.065196447 -0.06451337 -0.058050841 -0.055938747 -0.060216926 -0.070134051 -0.078976139 -0.084456213 -0.087215811 -0.088323027][-0.053763308 -0.054270364 -0.056548774 -0.059191339 -0.063166007 -0.064505488 -0.064832389 -0.0567305 -0.051747706 -0.051562198 -0.05996988 -0.070085615 -0.077640444 -0.081119508 -0.079155192][-0.052875444 -0.052802835 -0.054823678 -0.056935903 -0.059064172 -0.05844057 -0.056997914 -0.052560337 -0.04970026 -0.045870904 -0.050924927 -0.058111623 -0.065233275 -0.068893842 -0.066326715][-0.054286219 -0.053361721 -0.054793119 -0.056864329 -0.059584182 -0.05768469 -0.053678811 -0.047814913 -0.044722296 -0.043244734 -0.04922238 -0.050962027 -0.053997472 -0.057606436 -0.058451757][-0.055509031 -0.0552281 -0.055996381 -0.057408273 -0.061493251 -0.062496763 -0.061486 -0.0548184 -0.048820652 -0.043481965 -0.046527453 -0.048474591 -0.053303659 -0.056595534 -0.058533337][-0.05603604 -0.055731211 -0.056905393 -0.059181467 -0.063902877 -0.067769319 -0.071230963 -0.069043353 -0.064794078 -0.056458652 -0.05556909 -0.054529898 -0.058333181 -0.062237151 -0.066076025][-0.056578629 -0.055977102 -0.056973018 -0.059132833 -0.063811317 -0.070772015 -0.077782027 -0.081753358 -0.083617046 -0.07859835 -0.077583343 -0.072254956 -0.071831174 -0.072912991 -0.074853718][-0.056700908 -0.056014907 -0.056844093 -0.058514327 -0.061906926 -0.069780409 -0.078754142 -0.0881049 -0.094726488 -0.096001506 -0.097264789 -0.090231664 -0.087001428 -0.083800942 -0.082104504][-0.056809336 -0.0559485 -0.056279298 -0.057206627 -0.05926593 -0.065474972 -0.073445119 -0.084367678 -0.091073245 -0.094255343 -0.099732921 -0.097349063 -0.09479478 -0.088881686 -0.084668368][-0.056924872 -0.056106873 -0.056085896 -0.056212634 -0.056758873 -0.06004604 -0.065667316 -0.074879907 -0.081816137 -0.0865882 -0.087806821 -0.086069949 -0.087640375 -0.0858653 -0.083157144][-0.05713369 -0.056555644 -0.056457315 -0.055326529 -0.054577086 -0.056539096 -0.059281252 -0.064571135 -0.069389306 -0.073422551 -0.074742213 -0.076042034 -0.074355915 -0.072870538 -0.075009376]]...]
INFO - root - 2017-12-06 03:06:35.506464: step 6310, loss = 0.81, batch loss = 0.60 (17.9 examples/sec; 0.448 sec/batch; 40h:34m:29s remains)
INFO - root - 2017-12-06 03:06:40.056043: step 6320, loss = 0.85, batch loss = 0.63 (18.2 examples/sec; 0.441 sec/batch; 39h:55m:37s remains)
INFO - root - 2017-12-06 03:06:44.413249: step 6330, loss = 0.86, batch loss = 0.65 (17.1 examples/sec; 0.467 sec/batch; 42h:20m:14s remains)
INFO - root - 2017-12-06 03:06:48.894278: step 6340, loss = 0.85, batch loss = 0.64 (18.5 examples/sec; 0.432 sec/batch; 39h:07m:27s remains)
INFO - root - 2017-12-06 03:06:53.438650: step 6350, loss = 0.86, batch loss = 0.65 (18.5 examples/sec; 0.432 sec/batch; 39h:10m:20s remains)
INFO - root - 2017-12-06 03:06:57.955000: step 6360, loss = 0.85, batch loss = 0.64 (17.5 examples/sec; 0.458 sec/batch; 41h:31m:51s remains)
INFO - root - 2017-12-06 03:07:02.528610: step 6370, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.447 sec/batch; 40h:27m:54s remains)
INFO - root - 2017-12-06 03:07:06.991735: step 6380, loss = 0.81, batch loss = 0.59 (18.3 examples/sec; 0.436 sec/batch; 39h:31m:26s remains)
INFO - root - 2017-12-06 03:07:11.537835: step 6390, loss = 0.87, batch loss = 0.66 (17.1 examples/sec; 0.469 sec/batch; 42h:28m:31s remains)
INFO - root - 2017-12-06 03:07:16.091139: step 6400, loss = 0.87, batch loss = 0.65 (18.2 examples/sec; 0.439 sec/batch; 39h:45m:40s remains)
2017-12-06 03:07:16.565568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057404906 -0.056140345 -0.053310029 -0.051032208 -0.048287325 -0.046608068 -0.046371415 -0.047030315 -0.048714221 -0.050438605 -0.050997145 -0.050577268 -0.049138196 -0.046781912 -0.044529073][-0.070545845 -0.068830751 -0.063197382 -0.058412541 -0.052378245 -0.04861958 -0.046707086 -0.045898948 -0.046999332 -0.048374657 -0.048870604 -0.048191439 -0.046959981 -0.044666596 -0.042080969][-0.081318513 -0.078446746 -0.070515893 -0.062828347 -0.054515164 -0.050516166 -0.048545316 -0.046830118 -0.048210628 -0.049045019 -0.049452417 -0.047702417 -0.045851529 -0.043549262 -0.040508628][-0.086198777 -0.081710026 -0.071521327 -0.061496951 -0.052989285 -0.049149849 -0.050117936 -0.049452167 -0.05173419 -0.05250439 -0.052414261 -0.049431406 -0.045869417 -0.04226613 -0.038456373][-0.082352012 -0.075560741 -0.063173562 -0.053118642 -0.04511404 -0.040959194 -0.047355719 -0.05054415 -0.053487897 -0.056529526 -0.056268141 -0.052712809 -0.046966411 -0.040790506 -0.037063792][-0.076125078 -0.06652391 -0.052032776 -0.041493885 -0.034238964 -0.034599297 -0.04539736 -0.051397257 -0.054241136 -0.057965841 -0.057808068 -0.054539703 -0.048659094 -0.043122988 -0.039259918][-0.072009005 -0.063972354 -0.051453557 -0.040556647 -0.029743448 -0.03465157 -0.046757162 -0.055936236 -0.05761395 -0.061333656 -0.061264336 -0.058385596 -0.051538449 -0.046036668 -0.042235747][-0.066385448 -0.058570173 -0.048525609 -0.040411841 -0.028283276 -0.035565611 -0.047900647 -0.058125064 -0.059293371 -0.06341847 -0.063420489 -0.060613748 -0.056111328 -0.050162069 -0.046613052][-0.059415355 -0.052726567 -0.043659031 -0.039838795 -0.031537298 -0.040272817 -0.04945584 -0.057644982 -0.061086025 -0.064430773 -0.064443111 -0.061576966 -0.058994696 -0.053135477 -0.05009947][-0.054250039 -0.047579352 -0.040592026 -0.039892234 -0.037731566 -0.0453573 -0.052477576 -0.059397716 -0.062611781 -0.062870793 -0.062790737 -0.060937583 -0.059988949 -0.055494741 -0.053220898][-0.051511265 -0.047124095 -0.043120146 -0.041619889 -0.042451181 -0.0486455 -0.053447645 -0.059084635 -0.063307859 -0.061984167 -0.060608231 -0.060264826 -0.05932457 -0.058713939 -0.056351721][-0.049033739 -0.046231434 -0.044865567 -0.043802537 -0.045846161 -0.051185314 -0.055299502 -0.059187841 -0.0628203 -0.061706543 -0.059302684 -0.05894874 -0.059233624 -0.060803946 -0.060753129][-0.046420153 -0.044055372 -0.043888893 -0.044042494 -0.048216745 -0.052210648 -0.055989675 -0.058893647 -0.060294934 -0.05977717 -0.058174476 -0.057530016 -0.05832091 -0.059879098 -0.0611691][-0.046365835 -0.04472743 -0.045828678 -0.046697572 -0.048369166 -0.052630041 -0.055348374 -0.056585997 -0.056520555 -0.055810384 -0.054913409 -0.054999743 -0.056556363 -0.0586807 -0.0609751][-0.048306953 -0.048917226 -0.04925305 -0.049752273 -0.052101 -0.052864853 -0.054097787 -0.054094862 -0.053593088 -0.052691739 -0.052345008 -0.053146672 -0.055015981 -0.05764718 -0.060200687]]...]
INFO - root - 2017-12-06 03:07:21.151230: step 6410, loss = 0.84, batch loss = 0.62 (18.1 examples/sec; 0.442 sec/batch; 40h:01m:42s remains)
INFO - root - 2017-12-06 03:07:25.645958: step 6420, loss = 0.87, batch loss = 0.66 (18.2 examples/sec; 0.439 sec/batch; 39h:46m:42s remains)
INFO - root - 2017-12-06 03:07:30.139794: step 6430, loss = 0.85, batch loss = 0.64 (17.0 examples/sec; 0.470 sec/batch; 42h:33m:03s remains)
INFO - root - 2017-12-06 03:07:34.485535: step 6440, loss = 0.90, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:30m:51s remains)
INFO - root - 2017-12-06 03:07:38.969935: step 6450, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 40h:52m:34s remains)
INFO - root - 2017-12-06 03:07:43.454839: step 6460, loss = 0.88, batch loss = 0.67 (17.2 examples/sec; 0.466 sec/batch; 42h:11m:49s remains)
INFO - root - 2017-12-06 03:07:48.047019: step 6470, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.452 sec/batch; 40h:57m:27s remains)
INFO - root - 2017-12-06 03:07:52.574741: step 6480, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.443 sec/batch; 40h:05m:57s remains)
INFO - root - 2017-12-06 03:07:57.086152: step 6490, loss = 0.82, batch loss = 0.61 (17.5 examples/sec; 0.457 sec/batch; 41h:21m:02s remains)
INFO - root - 2017-12-06 03:08:01.687153: step 6500, loss = 0.90, batch loss = 0.69 (18.4 examples/sec; 0.436 sec/batch; 39h:27m:14s remains)
2017-12-06 03:08:02.121327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065299205 -0.068866894 -0.078273684 -0.098529831 -0.13786936 -0.19994801 -0.25515288 -0.27758944 -0.27637237 -0.28520066 -0.30698574 -0.32568622 -0.31419772 -0.2832168 -0.24615929][-0.066235155 -0.068634212 -0.083098136 -0.1134844 -0.16161638 -0.2390593 -0.30524868 -0.31994238 -0.30215931 -0.30849516 -0.35498157 -0.40266714 -0.4129819 -0.38184583 -0.34390104][-0.067894876 -0.071653455 -0.075056888 -0.08731842 -0.12896627 -0.19520424 -0.24500388 -0.23744565 -0.20676282 -0.21842389 -0.29680043 -0.392045 -0.44003212 -0.41905925 -0.36312655][-0.066756107 -0.070584923 -0.077958174 -0.089588068 -0.1087763 -0.15960936 -0.1643562 -0.085688874 -0.005248677 -0.002163969 -0.098459624 -0.22531103 -0.29058284 -0.29269749 -0.26562488][-0.076719217 -0.082029209 -0.098691322 -0.12413491 -0.16328403 -0.17657967 -0.11089645 0.046583004 0.18626305 0.18815097 0.051705554 -0.11534417 -0.20441979 -0.21722496 -0.20229053][-0.078584462 -0.095165931 -0.11801486 -0.15585211 -0.21847956 -0.22007526 -0.092366889 0.16887963 0.40599483 0.46041906 0.30236214 0.065365523 -0.097601667 -0.15582296 -0.15255284][-0.0868998 -0.10851485 -0.13964432 -0.17310958 -0.20964514 -0.18442786 0.0027380362 0.30672079 0.56782115 0.63787955 0.48677003 0.23674184 0.02106382 -0.092604652 -0.11936088][-0.096493363 -0.10861064 -0.12143025 -0.12879908 -0.12155361 -0.044974007 0.21965787 0.566857 0.82766724 0.86233222 0.65271479 0.39390993 0.15772013 0.0067469403 -0.068191268][-0.062658735 -0.0691661 -0.073729858 -0.063671425 -0.011599395 0.13984399 0.42859796 0.78929734 1.0165374 1.0219144 0.81051743 0.52881557 0.24110326 0.056236602 -0.010489933][-0.082756482 -0.0781087 -0.064329647 -0.035508234 0.042402975 0.22616866 0.53808331 0.90486193 1.1177415 1.0431124 0.79551518 0.52652836 0.29798791 0.14048658 0.032171674][-0.055362437 -0.043197215 -0.020376336 0.0093713924 0.065862834 0.20631477 0.45118654 0.764366 0.94042873 0.90744233 0.68647945 0.40221596 0.18677771 0.0782038 0.019949839][-0.032628808 0.028030813 0.097462445 0.14355467 0.1743774 0.25210977 0.40832034 0.61086124 0.70171714 0.6103012 0.4098765 0.2010411 0.079390481 0.011073843 -0.0215987][-0.048527323 0.018015712 0.10578553 0.18017052 0.2411257 0.29160738 0.39349797 0.51486754 0.54873204 0.4690581 0.25826588 0.074533567 -0.024239283 -0.043931764 -0.032642886][-0.078198522 -0.041670237 0.0075194761 0.077582717 0.1303447 0.16481304 0.22274336 0.30662239 0.33448836 0.30323172 0.20264485 0.059856743 -0.046830818 -0.071145713 -0.058872309][-0.093696117 -0.099781513 -0.0845387 -0.043009367 -0.00039249659 0.015323006 0.00998877 0.035844706 0.082818404 0.10519572 0.080896348 0.03968057 -0.017945673 -0.061291032 -0.078058556]]...]
INFO - root - 2017-12-06 03:08:06.644926: step 6510, loss = 0.83, batch loss = 0.61 (18.2 examples/sec; 0.439 sec/batch; 39h:47m:44s remains)
INFO - root - 2017-12-06 03:08:11.104323: step 6520, loss = 0.83, batch loss = 0.62 (17.6 examples/sec; 0.454 sec/batch; 41h:06m:44s remains)
INFO - root - 2017-12-06 03:08:15.562371: step 6530, loss = 0.88, batch loss = 0.67 (18.5 examples/sec; 0.433 sec/batch; 39h:12m:04s remains)
INFO - root - 2017-12-06 03:08:19.797217: step 6540, loss = 0.90, batch loss = 0.69 (17.9 examples/sec; 0.448 sec/batch; 40h:33m:34s remains)
INFO - root - 2017-12-06 03:08:24.316437: step 6550, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.448 sec/batch; 40h:34m:03s remains)
INFO - root - 2017-12-06 03:08:28.782667: step 6560, loss = 0.85, batch loss = 0.63 (18.0 examples/sec; 0.445 sec/batch; 40h:18m:00s remains)
INFO - root - 2017-12-06 03:08:33.287510: step 6570, loss = 0.77, batch loss = 0.56 (18.0 examples/sec; 0.444 sec/batch; 40h:12m:45s remains)
INFO - root - 2017-12-06 03:08:37.874022: step 6580, loss = 0.87, batch loss = 0.65 (17.4 examples/sec; 0.460 sec/batch; 41h:41m:24s remains)
INFO - root - 2017-12-06 03:08:42.435708: step 6590, loss = 0.84, batch loss = 0.63 (17.4 examples/sec; 0.461 sec/batch; 41h:42m:29s remains)
INFO - root - 2017-12-06 03:08:46.922045: step 6600, loss = 0.88, batch loss = 0.67 (17.2 examples/sec; 0.466 sec/batch; 42h:12m:53s remains)
2017-12-06 03:08:47.384650: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033311702 0.034336247 0.034156911 0.033845685 0.033487655 0.033086792 0.032808669 0.032739475 0.032899693 0.033232853 0.033651941 0.034115843 0.034503095 0.03477788 0.0338407][0.033723205 0.03468997 0.0342144 0.033592828 0.032887958 0.032197803 0.031677485 0.031510212 0.031745158 0.032279842 0.033002764 0.033800334 0.034547016 0.035093822 0.034275748][0.033507541 0.034210764 0.03325817 0.032108143 0.0308569 0.029636383 0.028681412 0.028375857 0.028771684 0.029705495 0.030982941 0.03239125 0.033738628 0.03478469 0.034322463][0.033120088 0.033331431 0.031733721 0.029863834 0.027897157 0.026024058 0.024634063 0.0241846 0.024778903 0.026206024 0.028157301 0.030339219 0.032473162 0.034207091 0.034195147][0.032371067 0.031931631 0.0295031 0.02669806 0.023846693 0.0212086 0.019368239 0.018794939 0.01964277 0.021688826 0.024510175 0.027712129 0.030801266 0.033300348 0.03394562][0.031237848 0.030092418 0.0266051 0.022689573 0.018824078 0.015498914 0.013306178 0.012777045 0.014113449 0.016928189 0.020818271 0.025087416 0.029139183 0.032270879 0.033454947][0.030058563 0.028152689 0.023626037 0.018603645 0.013838448 0.0099792778 0.0076466203 0.0074455142 0.0094094723 0.013184279 0.018138364 0.02334889 0.028072275 0.031552367 0.033085577][0.029072635 0.026702151 0.021382943 0.015485108 0.010084234 0.0059934855 0.0038305968 0.004115805 0.0068805367 0.011633806 0.017334305 0.022993512 0.027903974 0.031333357 0.032854475][0.028588064 0.025936946 0.020173587 0.013802245 0.0082275569 0.0042822808 0.0025609434 0.0035440996 0.0071073323 0.012408979 0.018272415 0.023866691 0.028502181 0.031359561 0.032533661][0.028770812 0.026039124 0.020303875 0.014089584 0.0088196695 0.0054058582 0.0043980628 0.0061118305 0.010058179 0.015352115 0.020889655 0.025840484 0.029685251 0.031705476 0.032389067][0.029490739 0.027223989 0.022170939 0.016735613 0.012201659 0.0096354485 0.0093270987 0.011363223 0.015154772 0.019880064 0.024535127 0.02835355 0.031165481 0.032325841 0.03240779][0.030448519 0.028932273 0.02500537 0.020754971 0.017359987 0.015652418 0.015780382 0.017737113 0.020971611 0.024774887 0.028163657 0.03075514 0.032533728 0.032908402 0.032432295][0.031506009 0.030569918 0.027857177 0.02499254 0.022775285 0.021797419 0.022166461 0.023827411 0.026284024 0.028836809 0.030931473 0.032395281 0.033270404 0.0330882 0.032158643][0.031583354 0.031085238 0.029503115 0.027909853 0.026745684 0.026336096 0.026768751 0.028078325 0.029655553 0.031143904 0.032312118 0.032937437 0.033208437 0.032617472 0.031345643][0.029295757 0.029424638 0.02874545 0.027971365 0.027410783 0.027371034 0.027713612 0.028450415 0.029202409 0.030007906 0.030637607 0.030771971 0.030747414 0.030129731 0.028995909]]...]
INFO - root - 2017-12-06 03:08:51.871842: step 6610, loss = 0.90, batch loss = 0.69 (17.2 examples/sec; 0.466 sec/batch; 42h:08m:33s remains)
INFO - root - 2017-12-06 03:08:56.302253: step 6620, loss = 0.93, batch loss = 0.72 (18.5 examples/sec; 0.434 sec/batch; 39h:15m:01s remains)
INFO - root - 2017-12-06 03:09:00.780830: step 6630, loss = 0.97, batch loss = 0.76 (17.7 examples/sec; 0.453 sec/batch; 40h:59m:39s remains)
INFO - root - 2017-12-06 03:09:05.282254: step 6640, loss = 0.90, batch loss = 0.68 (17.8 examples/sec; 0.450 sec/batch; 40h:45m:28s remains)
INFO - root - 2017-12-06 03:09:09.589307: step 6650, loss = 0.89, batch loss = 0.68 (17.6 examples/sec; 0.454 sec/batch; 41h:04m:18s remains)
INFO - root - 2017-12-06 03:09:14.164990: step 6660, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.461 sec/batch; 41h:44m:30s remains)
INFO - root - 2017-12-06 03:09:18.677919: step 6670, loss = 0.86, batch loss = 0.65 (17.5 examples/sec; 0.457 sec/batch; 41h:20m:55s remains)
INFO - root - 2017-12-06 03:09:23.166722: step 6680, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.445 sec/batch; 40h:17m:49s remains)
INFO - root - 2017-12-06 03:09:27.692057: step 6690, loss = 0.87, batch loss = 0.66 (18.1 examples/sec; 0.443 sec/batch; 40h:03m:52s remains)
INFO - root - 2017-12-06 03:09:32.285968: step 6700, loss = 0.89, batch loss = 0.68 (17.6 examples/sec; 0.455 sec/batch; 41h:08m:47s remains)
2017-12-06 03:09:32.718447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042461529 -0.046182275 -0.049790256 -0.052225664 -0.053360365 -0.053413592 -0.053029116 -0.05267629 -0.052654557 -0.052849464 -0.05291504 -0.052592326 -0.051391397 -0.048943378 -0.045508347][-0.042203404 -0.046028249 -0.049752682 -0.052222352 -0.053255383 -0.053149469 -0.052471116 -0.051749092 -0.051372319 -0.051211588 -0.05093617 -0.050235514 -0.048710138 -0.046035245 -0.042508759][-0.041554157 -0.045423232 -0.049177587 -0.05173941 -0.052925374 -0.052927665 -0.052171893 -0.051180463 -0.050392941 -0.049767867 -0.048982702 -0.04774648 -0.04582309 -0.042849012 -0.0392657][-0.040696424 -0.044499531 -0.04822021 -0.050771855 -0.052132428 -0.052357875 -0.05164665 -0.050486542 -0.049374718 -0.048345588 -0.047029495 -0.045222171 -0.042799961 -0.039634854 -0.035984859][-0.039651435 -0.043434858 -0.047175445 -0.049769208 -0.051165983 -0.051504336 -0.050797284 -0.049559735 -0.048311614 -0.047071956 -0.045339052 -0.042927649 -0.0399637 -0.036534749 -0.0329035][-0.038788836 -0.042537794 -0.046402533 -0.049137719 -0.050550748 -0.050835278 -0.049998492 -0.048531309 -0.047065079 -0.045677722 -0.043716446 -0.040831562 -0.037433647 -0.033800136 -0.030361526][-0.038638324 -0.042271428 -0.046168014 -0.049028836 -0.050466724 -0.050612204 -0.049587723 -0.047783233 -0.045908108 -0.044215072 -0.042042144 -0.038903911 -0.035248846 -0.03161661 -0.028510876][-0.038790859 -0.042207107 -0.045969866 -0.04882085 -0.050302692 -0.050350416 -0.049199365 -0.047181427 -0.044929866 -0.042859767 -0.040357158 -0.037070788 -0.033385389 -0.029924951 -0.0272577][-0.038436208 -0.041534141 -0.045099169 -0.047924455 -0.049415436 -0.049442925 -0.048226729 -0.046175346 -0.04370869 -0.041231409 -0.038439553 -0.035092894 -0.031554032 -0.028404802 -0.026246041][-0.0372078 -0.039998092 -0.043294318 -0.045986295 -0.047464229 -0.047562148 -0.046443108 -0.044447981 -0.041930728 -0.039227195 -0.036287524 -0.033038005 -0.02981282 -0.027140588 -0.025439676][-0.034994163 -0.037393726 -0.040236436 -0.042578019 -0.043942366 -0.044105023 -0.043177553 -0.041405797 -0.039107654 -0.036532819 -0.033732917 -0.030811321 -0.02809149 -0.025985453 -0.024690371][-0.0317916 -0.033685677 -0.035976287 -0.037904225 -0.039059594 -0.039293315 -0.038645979 -0.037293509 -0.035472862 -0.033336211 -0.031017512 -0.02865462 -0.026507389 -0.02487231 -0.023884024][-0.028227091 -0.029626153 -0.031318951 -0.032782342 -0.033714242 -0.033977643 -0.033565428 -0.032601971 -0.031302679 -0.029795069 -0.028171312 -0.026516534 -0.024999417 -0.023823902 -0.023202416][-0.025249496 -0.026034936 -0.027182855 -0.028172199 -0.028828144 -0.029041864 -0.028798059 -0.02814674 -0.027269777 -0.026296 -0.025313854 -0.024356492 -0.023469631 -0.022771921 -0.022539835][-0.023547798 -0.023740068 -0.024406154 -0.024900433 -0.02525628 -0.025466241 -0.02532411 -0.024911903 -0.024397381 -0.023928721 -0.02347805 -0.023016181 -0.022648823 -0.022360284 -0.022419054]]...]
INFO - root - 2017-12-06 03:09:37.195511: step 6710, loss = 0.84, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 41h:21m:41s remains)
INFO - root - 2017-12-06 03:09:41.699810: step 6720, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.437 sec/batch; 39h:34m:15s remains)
INFO - root - 2017-12-06 03:09:46.234914: step 6730, loss = 0.86, batch loss = 0.64 (18.2 examples/sec; 0.439 sec/batch; 39h:43m:29s remains)
INFO - root - 2017-12-06 03:09:50.720256: step 6740, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.448 sec/batch; 40h:34m:04s remains)
INFO - root - 2017-12-06 03:09:54.983864: step 6750, loss = 0.88, batch loss = 0.66 (18.4 examples/sec; 0.436 sec/batch; 39h:24m:24s remains)
INFO - root - 2017-12-06 03:09:59.456435: step 6760, loss = 0.89, batch loss = 0.68 (18.2 examples/sec; 0.439 sec/batch; 39h:41m:23s remains)
INFO - root - 2017-12-06 03:10:04.067209: step 6770, loss = 0.83, batch loss = 0.62 (17.5 examples/sec; 0.456 sec/batch; 41h:15m:54s remains)
INFO - root - 2017-12-06 03:10:08.591066: step 6780, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.443 sec/batch; 40h:06m:45s remains)
INFO - root - 2017-12-06 03:10:13.087327: step 6790, loss = 0.82, batch loss = 0.61 (17.5 examples/sec; 0.458 sec/batch; 41h:28m:14s remains)
INFO - root - 2017-12-06 03:10:17.551822: step 6800, loss = 0.82, batch loss = 0.60 (17.8 examples/sec; 0.449 sec/batch; 40h:39m:31s remains)
2017-12-06 03:10:17.984299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037472077 -0.038037039 -0.039663427 -0.041766584 -0.044028305 -0.046097089 -0.047468998 -0.047960527 -0.047891486 -0.047630928 -0.047463275 -0.047612943 -0.048048012 -0.048455566 -0.049282741][-0.037988953 -0.038778931 -0.04072978 -0.043178014 -0.045876775 -0.048354149 -0.050026685 -0.050736118 -0.050822973 -0.050657079 -0.050453365 -0.050532673 -0.050894469 -0.051179342 -0.05185353][-0.038340248 -0.039473854 -0.041667022 -0.044495903 -0.047639236 -0.050480966 -0.052374706 -0.053153127 -0.053214479 -0.052861862 -0.052433152 -0.05219521 -0.052124359 -0.052189983 -0.052687455][-0.03882581 -0.04012128 -0.042505991 -0.045637406 -0.04921335 -0.052348733 -0.054348394 -0.055064753 -0.054976158 -0.05424913 -0.053375073 -0.052646007 -0.051922385 -0.051396854 -0.051546536][-0.039183244 -0.040707164 -0.043377884 -0.046981018 -0.050948042 -0.054280732 -0.056183685 -0.056728661 -0.056485422 -0.055454656 -0.054078657 -0.052701272 -0.051187955 -0.049958855 -0.049461477][-0.039657123 -0.041138895 -0.044122234 -0.048207879 -0.052617129 -0.056129638 -0.057897568 -0.058320116 -0.057905287 -0.056708068 -0.0550397 -0.052898563 -0.050520044 -0.0485223 -0.047198419][-0.039982386 -0.041402292 -0.044650316 -0.049449969 -0.054565184 -0.058297172 -0.059714168 -0.059645072 -0.058869615 -0.057579592 -0.055723198 -0.053208172 -0.050124027 -0.047332361 -0.0452796][-0.039580036 -0.041189149 -0.044785231 -0.0505351 -0.056485556 -0.060757756 -0.061599843 -0.060600609 -0.058936514 -0.057371709 -0.055608161 -0.053087249 -0.049816057 -0.046542324 -0.043899737][-0.038721491 -0.039939143 -0.0428167 -0.047256351 -0.052349843 -0.05754127 -0.059913225 -0.059359387 -0.057431035 -0.055761836 -0.054195859 -0.052046537 -0.049158104 -0.045957841 -0.043138728][-0.037684429 -0.037811957 -0.038803115 -0.03907695 -0.04048913 -0.045516752 -0.052073736 -0.055135246 -0.054337721 -0.052956745 -0.051732168 -0.050144166 -0.047914807 -0.045253791 -0.042820312][-0.036838427 -0.036115088 -0.036646076 -0.037010606 -0.037786178 -0.040687907 -0.046054497 -0.050044172 -0.050075457 -0.049243584 -0.048464008 -0.047365077 -0.0458095 -0.043840948 -0.041830286][-0.036309011 -0.035585918 -0.036202844 -0.037519477 -0.038947437 -0.040687677 -0.043449454 -0.045235448 -0.045260657 -0.045029484 -0.044698313 -0.044042196 -0.043014742 -0.041692629 -0.040298879][-0.036133319 -0.035415202 -0.035552796 -0.036098443 -0.036969546 -0.038191903 -0.040054981 -0.041153416 -0.04120101 -0.041097283 -0.040965062 -0.040662158 -0.040073507 -0.039293557 -0.038527966][-0.036127921 -0.035330869 -0.035407376 -0.035918463 -0.036732219 -0.037456468 -0.038080946 -0.038342245 -0.038314305 -0.038226176 -0.038159832 -0.03808656 -0.0378656 -0.037604596 -0.037341021][-0.037283681 -0.036471955 -0.036367759 -0.036405414 -0.036722571 -0.037105426 -0.037294656 -0.03727239 -0.037193052 -0.0371038 -0.03702604 -0.037124537 -0.037238743 -0.037363641 -0.037401095]]...]
INFO - root - 2017-12-06 03:10:22.521588: step 6810, loss = 0.92, batch loss = 0.71 (17.7 examples/sec; 0.453 sec/batch; 40h:56m:21s remains)
INFO - root - 2017-12-06 03:10:26.976537: step 6820, loss = 0.84, batch loss = 0.63 (18.0 examples/sec; 0.444 sec/batch; 40h:12m:00s remains)
INFO - root - 2017-12-06 03:10:31.490957: step 6830, loss = 0.92, batch loss = 0.71 (17.3 examples/sec; 0.462 sec/batch; 41h:49m:55s remains)
INFO - root - 2017-12-06 03:10:35.966326: step 6840, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.445 sec/batch; 40h:14m:43s remains)
INFO - root - 2017-12-06 03:10:40.435389: step 6850, loss = 0.87, batch loss = 0.66 (20.1 examples/sec; 0.397 sec/batch; 35h:55m:20s remains)
INFO - root - 2017-12-06 03:10:44.854154: step 6860, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.451 sec/batch; 40h:48m:30s remains)
INFO - root - 2017-12-06 03:10:49.283007: step 6870, loss = 0.86, batch loss = 0.65 (18.0 examples/sec; 0.444 sec/batch; 40h:07m:33s remains)
INFO - root - 2017-12-06 03:10:53.800001: step 6880, loss = 0.89, batch loss = 0.68 (17.6 examples/sec; 0.456 sec/batch; 41h:13m:24s remains)
INFO - root - 2017-12-06 03:10:58.270996: step 6890, loss = 0.86, batch loss = 0.65 (18.0 examples/sec; 0.443 sec/batch; 40h:06m:28s remains)
INFO - root - 2017-12-06 03:11:02.799159: step 6900, loss = 0.88, batch loss = 0.67 (17.3 examples/sec; 0.462 sec/batch; 41h:46m:04s remains)
2017-12-06 03:11:03.234341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017689653 -0.018870607 -0.021232169 -0.023578074 -0.025447786 -0.026590161 -0.027180851 -0.027435359 -0.027615592 -0.027592245 -0.027339898 -0.026643746 -0.025501687 -0.024244443 -0.023615353][-0.018575184 -0.020033121 -0.022905048 -0.025891602 -0.028530676 -0.030473139 -0.031696428 -0.032370549 -0.032779261 -0.03290097 -0.032534331 -0.031510215 -0.029985059 -0.028466664 -0.027709831][-0.019243058 -0.020849138 -0.023953967 -0.027349923 -0.03063529 -0.033352211 -0.035324153 -0.036610719 -0.037361771 -0.037607007 -0.037046738 -0.035539806 -0.033634774 -0.032010466 -0.031341378][-0.019253168 -0.020828873 -0.023909491 -0.027440041 -0.03114862 -0.034595028 -0.037379812 -0.039397053 -0.040536623 -0.040767178 -0.039919212 -0.037960716 -0.03565513 -0.033851381 -0.033226822][-0.018677287 -0.020073697 -0.02298677 -0.026402932 -0.030197959 -0.034039307 -0.037448004 -0.040055141 -0.041557416 -0.041823119 -0.040863734 -0.038708709 -0.036071148 -0.03411559 -0.033358391][-0.017939653 -0.018841397 -0.021447591 -0.024520896 -0.028109998 -0.031969905 -0.035552219 -0.038446896 -0.040295556 -0.040960826 -0.040352806 -0.0384029 -0.035766147 -0.033612039 -0.032401014][-0.016885601 -0.01740269 -0.019634902 -0.022294678 -0.025535043 -0.029219527 -0.032569945 -0.035428081 -0.03780283 -0.039322883 -0.039608665 -0.038216963 -0.035821311 -0.033346016 -0.031435072][-0.015844144 -0.016032208 -0.017972816 -0.020404045 -0.023371078 -0.026853785 -0.03008825 -0.032837532 -0.0356203 -0.037848812 -0.038922247 -0.038188331 -0.03607849 -0.033436354 -0.031033944][-0.015093762 -0.015029877 -0.016692463 -0.018986929 -0.021877937 -0.025420059 -0.028817408 -0.031784233 -0.034782123 -0.037173234 -0.038532473 -0.038176805 -0.036180433 -0.033521403 -0.03076601][-0.014753994 -0.014545918 -0.01597539 -0.01812828 -0.021022178 -0.024791542 -0.028603457 -0.032032646 -0.035124373 -0.037251614 -0.038331144 -0.03787604 -0.035834026 -0.033113495 -0.030215684][-0.014621925 -0.014497437 -0.01580371 -0.017807551 -0.020688478 -0.024606701 -0.028799951 -0.032615166 -0.0355381 -0.037119903 -0.037604135 -0.036679406 -0.034551844 -0.031855341 -0.029145982][-0.014438417 -0.014637001 -0.015961129 -0.017824534 -0.020520274 -0.024310745 -0.028522506 -0.032216121 -0.034623507 -0.035495862 -0.035386238 -0.034075148 -0.031819385 -0.02931165 -0.027134862][-0.014300939 -0.014757056 -0.01612436 -0.017869774 -0.020289436 -0.023569137 -0.027127542 -0.030068606 -0.031756412 -0.032032304 -0.031437762 -0.029969938 -0.027897019 -0.025877159 -0.024449054][-0.014340095 -0.014639698 -0.015953165 -0.017554943 -0.019567832 -0.022075772 -0.024593707 -0.026591264 -0.027668871 -0.027583636 -0.026734438 -0.025438659 -0.023834519 -0.022413664 -0.02160598][-0.015691094 -0.015726656 -0.016819037 -0.018157549 -0.019766271 -0.021516152 -0.022938181 -0.023970548 -0.024521511 -0.024290551 -0.023380872 -0.022233233 -0.020977974 -0.020054102 -0.019770391]]...]
INFO - root - 2017-12-06 03:11:07.772211: step 6910, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 40h:54m:23s remains)
INFO - root - 2017-12-06 03:11:12.295287: step 6920, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.452 sec/batch; 40h:52m:03s remains)
INFO - root - 2017-12-06 03:11:16.789067: step 6930, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.451 sec/batch; 40h:45m:51s remains)
INFO - root - 2017-12-06 03:11:21.374410: step 6940, loss = 0.84, batch loss = 0.63 (17.7 examples/sec; 0.452 sec/batch; 40h:51m:49s remains)
INFO - root - 2017-12-06 03:11:25.896112: step 6950, loss = 0.91, batch loss = 0.70 (16.3 examples/sec; 0.491 sec/batch; 44h:22m:50s remains)
INFO - root - 2017-12-06 03:11:30.324581: step 6960, loss = 0.87, batch loss = 0.66 (17.4 examples/sec; 0.459 sec/batch; 41h:31m:42s remains)
INFO - root - 2017-12-06 03:11:34.796408: step 6970, loss = 0.85, batch loss = 0.64 (17.4 examples/sec; 0.460 sec/batch; 41h:37m:21s remains)
INFO - root - 2017-12-06 03:11:39.340172: step 6980, loss = 0.86, batch loss = 0.65 (18.1 examples/sec; 0.442 sec/batch; 39h:55m:25s remains)
INFO - root - 2017-12-06 03:11:43.856720: step 6990, loss = 0.84, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 41h:16m:39s remains)
INFO - root - 2017-12-06 03:11:48.371313: step 7000, loss = 0.94, batch loss = 0.72 (17.8 examples/sec; 0.448 sec/batch; 40h:33m:04s remains)
2017-12-06 03:11:48.795182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061837018 -0.061909895 -0.0632823 -0.065692946 -0.06845133 -0.070985869 -0.072527006 -0.074482523 -0.075107142 -0.073890463 -0.071112484 -0.068074636 -0.065705158 -0.063886322 -0.06252715][-0.061907887 -0.061740577 -0.062911987 -0.063752793 -0.062449809 -0.060790543 -0.060616679 -0.066419952 -0.073144749 -0.076720871 -0.075132005 -0.071311489 -0.067499883 -0.064715311 -0.062750161][-0.061979052 -0.061741605 -0.063012324 -0.062713757 -0.056209575 -0.04740965 -0.041034438 -0.047141097 -0.059320457 -0.069611184 -0.07265 -0.071035884 -0.067959972 -0.065337837 -0.063253455][-0.061972238 -0.061790086 -0.0628932 -0.060549147 -0.045395248 -0.024053585 -0.0078219809 -0.013879869 -0.035110626 -0.057085019 -0.06788338 -0.069480233 -0.066790946 -0.064028785 -0.062231641][-0.062014952 -0.061678849 -0.063056543 -0.061322443 -0.042666607 -0.013732638 0.010585845 0.0072820261 -0.018824983 -0.048563004 -0.0649005 -0.068942413 -0.066851452 -0.063745737 -0.061333768][-0.062039629 -0.061511844 -0.063370422 -0.06329985 -0.044996612 -0.01219774 0.018699504 0.022701643 -0.002786845 -0.037326459 -0.05877915 -0.065470465 -0.064853214 -0.062770411 -0.060871962][-0.061989713 -0.061391715 -0.0637315 -0.065891825 -0.053011037 -0.025465161 0.0012423843 0.0076942891 -0.01155962 -0.040151618 -0.058952954 -0.064309187 -0.063321516 -0.06129688 -0.059558854][-0.061960384 -0.061318807 -0.063431121 -0.067015126 -0.06114956 -0.043566681 -0.026646923 -0.022537421 -0.034866977 -0.052445292 -0.06312564 -0.064820379 -0.062965214 -0.061075442 -0.059374891][-0.062149126 -0.061474502 -0.06319645 -0.067183487 -0.067389481 -0.058898583 -0.048738487 -0.045113675 -0.05091745 -0.059960928 -0.0649033 -0.064940907 -0.063229784 -0.061644018 -0.060189959][-0.062043168 -0.061543696 -0.06255126 -0.065191917 -0.067527592 -0.065634936 -0.062025048 -0.061071713 -0.063890055 -0.066978775 -0.067440152 -0.0658397 -0.063690908 -0.06206423 -0.06080953][-0.061868917 -0.061572637 -0.062212117 -0.063847609 -0.066325977 -0.067839183 -0.06844309 -0.069166712 -0.070341542 -0.07034573 -0.068842389 -0.066838033 -0.064810485 -0.063138358 -0.061755233][-0.061763555 -0.061647348 -0.062045783 -0.06253089 -0.063108936 -0.063695736 -0.064772658 -0.06626299 -0.067814916 -0.068061329 -0.067541696 -0.066630937 -0.065284565 -0.063687496 -0.062302768][-0.061771803 -0.061645407 -0.061844982 -0.061675586 -0.060888402 -0.059847515 -0.060313664 -0.062460795 -0.065555565 -0.067100562 -0.067191318 -0.066340327 -0.064967632 -0.063458569 -0.062202122][-0.061782751 -0.061589472 -0.061554685 -0.061027687 -0.0595427 -0.057277955 -0.057410154 -0.060037814 -0.064175628 -0.066829808 -0.067549989 -0.066837192 -0.065296181 -0.063565835 -0.062182643][-0.062279768 -0.062143289 -0.061831061 -0.060943995 -0.058894947 -0.055627171 -0.05500973 -0.058039218 -0.063099273 -0.066448346 -0.067712344 -0.067287929 -0.065816008 -0.064094529 -0.062690452]]...]
INFO - root - 2017-12-06 03:11:53.309308: step 7010, loss = 0.82, batch loss = 0.61 (18.9 examples/sec; 0.423 sec/batch; 38h:13m:40s remains)
INFO - root - 2017-12-06 03:11:57.774741: step 7020, loss = 0.87, batch loss = 0.66 (17.0 examples/sec; 0.470 sec/batch; 42h:30m:34s remains)
INFO - root - 2017-12-06 03:12:02.363576: step 7030, loss = 0.86, batch loss = 0.65 (17.5 examples/sec; 0.456 sec/batch; 41h:13m:27s remains)
INFO - root - 2017-12-06 03:12:06.887151: step 7040, loss = 0.85, batch loss = 0.63 (17.3 examples/sec; 0.464 sec/batch; 41h:54m:28s remains)
INFO - root - 2017-12-06 03:12:11.401663: step 7050, loss = 0.85, batch loss = 0.64 (18.4 examples/sec; 0.435 sec/batch; 39h:18m:34s remains)
INFO - root - 2017-12-06 03:12:15.766707: step 7060, loss = 0.98, batch loss = 0.77 (19.9 examples/sec; 0.403 sec/batch; 36h:23m:14s remains)
INFO - root - 2017-12-06 03:12:20.190819: step 7070, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.441 sec/batch; 39h:53m:03s remains)
INFO - root - 2017-12-06 03:12:24.708178: step 7080, loss = 0.84, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 41h:17m:31s remains)
INFO - root - 2017-12-06 03:12:29.213864: step 7090, loss = 0.86, batch loss = 0.65 (17.4 examples/sec; 0.460 sec/batch; 41h:36m:13s remains)
INFO - root - 2017-12-06 03:12:33.747567: step 7100, loss = 0.86, batch loss = 0.65 (17.3 examples/sec; 0.463 sec/batch; 41h:51m:00s remains)
2017-12-06 03:12:34.159626: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1939697 0.25647116 0.14198162 0.0035982504 -0.16721337 -0.28534544 -0.39519855 -0.592776 -0.80985767 -0.83968765 -0.82156706 -0.749094 -0.5288198 -0.33748406 -0.092551231][1.9918661 1.6100444 1.4200737 1.4137988 1.5571488 1.6579142 1.6294755 1.4417557 1.1122254 0.71920371 0.36108503 0.41885746 0.53069 0.68335897 1.0763556][2.6777978 2.5010514 2.2058468 1.8928332 1.7491839 1.7540932 1.7291927 1.706478 1.5490007 1.4169122 1.2457259 1.0077115 0.96875405 1.1326133 1.4502891][2.6905596 2.7876377 2.5053792 2.0772834 1.6744862 1.2212391 0.87024677 0.7877655 0.72746062 0.83553147 0.96001339 1.1417571 1.3214595 1.3405619 1.450352][2.5357108 2.7380044 2.7274113 2.4082472 1.8975321 1.3520776 0.71748722 0.25883257 0.12420835 0.26044396 0.57223004 0.87516546 1.1214986 1.2267408 1.4216133][2.6788881 2.842586 2.7103286 2.4476316 2.2535715 1.9831505 1.4752594 0.98805356 0.6985414 0.36593395 0.1879572 0.27710968 0.41353571 0.6433841 0.84968412][2.8047647 2.9087248 2.8286161 2.81219 2.6583917 2.34514 2.1319294 1.8113545 1.4128165 1.0367551 0.69228882 0.54178762 0.44096142 0.3745074 0.36032453][1.0808305 1.2521644 1.5448529 1.5505694 1.5479985 1.5791923 1.5501527 1.5490018 1.5040438 1.3355716 1.1189386 0.85576242 0.5979197 0.54435456 0.43679297][-0.64813495 -0.71793014 -0.67499655 -0.44310009 -0.14241222 0.052733071 0.22585359 0.58111566 0.82986236 1.029165 1.1463895 1.2259728 1.1462673 1.0294271 0.79425418][-1.8812926 -1.9683334 -1.8891004 -1.7548699 -1.6724924 -1.3718747 -1.061636 -0.79889292 -0.48177546 -0.10388236 0.27676409 0.57081026 0.74457616 0.80835217 0.69342858][-2.4230032 -2.5976529 -2.6524019 -2.6225028 -2.4971564 -2.4148257 -2.2765558 -1.9988563 -1.6759037 -1.4076793 -1.0306691 -0.52441323 -0.10139158 0.12639001 0.20283803][-2.01828 -2.2445524 -2.5617282 -2.7317326 -2.692872 -2.5128467 -2.3068597 -2.1301763 -1.9779391 -1.7964165 -1.5965605 -1.3581531 -1.0720446 -0.73104483 -0.57197279][-1.8417062 -1.9517032 -2.1037562 -2.2186978 -2.2587762 -2.1792276 -2.0092926 -1.7573696 -1.5288867 -1.4291252 -1.4123641 -1.3404311 -1.2309635 -1.0929253 -0.83391482][-1.7737014 -1.773604 -1.7556336 -1.7667779 -1.7292713 -1.5907428 -1.487839 -1.3333356 -1.1135163 -0.92627126 -0.80789703 -0.73830217 -0.6981672 -0.75253075 -0.57307583][-1.6680686 -1.6592896 -1.6105461 -1.4608797 -1.2946507 -1.1296575 -0.99823219 -0.74861473 -0.51455641 -0.40152344 -0.28655791 -0.35074425 -0.23449178 -0.22494581 -0.28568709]]...]
INFO - root - 2017-12-06 03:12:38.717950: step 7110, loss = 0.90, batch loss = 0.69 (17.3 examples/sec; 0.463 sec/batch; 41h:52m:35s remains)
INFO - root - 2017-12-06 03:12:43.191565: step 7120, loss = 0.89, batch loss = 0.68 (18.1 examples/sec; 0.441 sec/batch; 39h:54m:08s remains)
INFO - root - 2017-12-06 03:12:47.803070: step 7130, loss = 0.86, batch loss = 0.65 (17.9 examples/sec; 0.448 sec/batch; 40h:26m:54s remains)
INFO - root - 2017-12-06 03:12:52.296443: step 7140, loss = 0.92, batch loss = 0.71 (18.2 examples/sec; 0.440 sec/batch; 39h:45m:34s remains)
INFO - root - 2017-12-06 03:12:56.768551: step 7150, loss = 0.85, batch loss = 0.63 (17.3 examples/sec; 0.463 sec/batch; 41h:51m:49s remains)
INFO - root - 2017-12-06 03:13:01.296223: step 7160, loss = 0.89, batch loss = 0.68 (17.2 examples/sec; 0.466 sec/batch; 42h:08m:48s remains)
INFO - root - 2017-12-06 03:13:05.649877: step 7170, loss = 0.91, batch loss = 0.70 (18.1 examples/sec; 0.442 sec/batch; 39h:59m:04s remains)
INFO - root - 2017-12-06 03:13:10.162723: step 7180, loss = 0.90, batch loss = 0.69 (17.3 examples/sec; 0.463 sec/batch; 41h:52m:15s remains)
INFO - root - 2017-12-06 03:13:14.684098: step 7190, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.450 sec/batch; 40h:42m:04s remains)
INFO - root - 2017-12-06 03:13:19.227814: step 7200, loss = 0.91, batch loss = 0.70 (17.9 examples/sec; 0.447 sec/batch; 40h:22m:03s remains)
2017-12-06 03:13:19.694886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064131632 -0.065276355 -0.067063659 -0.0697932 -0.073028177 -0.0756946 -0.07700564 -0.076497078 -0.074140139 -0.07082852 -0.067892015 -0.065955669 -0.0647633 -0.063914075 -0.063437171][-0.064310074 -0.065615654 -0.0679681 -0.072404765 -0.077542365 -0.081434444 -0.084256388 -0.085556127 -0.08384262 -0.078745767 -0.072570294 -0.068102106 -0.065659113 -0.064214461 -0.0631605][-0.064353012 -0.065119721 -0.067700319 -0.07367985 -0.0799543 -0.08449351 -0.088872492 -0.092697352 -0.0932153 -0.088476375 -0.079874441 -0.071701877 -0.067144394 -0.064766929 -0.063250639][-0.064082928 -0.064030752 -0.06606321 -0.070995852 -0.073309176 -0.07241255 -0.075836658 -0.086251438 -0.095027491 -0.0955049 -0.086487822 -0.074749604 -0.067238651 -0.064138226 -0.062573895][-0.063682474 -0.062931612 -0.063903823 -0.066552907 -0.060536344 -0.045059428 -0.038147636 -0.053256121 -0.07870996 -0.094302662 -0.090415716 -0.076175943 -0.063900642 -0.058933854 -0.058363646][-0.063267909 -0.06185586 -0.0601137 -0.055575423 -0.0338718 0.0043055788 0.03047993 0.016489759 -0.028791979 -0.070012793 -0.084474742 -0.076472633 -0.061022677 -0.052543685 -0.051251728][-0.06296327 -0.061000481 -0.056941897 -0.044142328 -0.0036462173 0.0623646 0.11678315 0.11301937 0.053220622 -0.018164858 -0.061862811 -0.071591936 -0.06026433 -0.051078334 -0.048899315][-0.062681839 -0.060878407 -0.0577399 -0.048365198 -0.01238418 0.060285337 0.13503289 0.15129969 0.099230446 0.019907139 -0.041216485 -0.067774 -0.064937748 -0.055549722 -0.051802471][-0.062653959 -0.061131872 -0.060214434 -0.059757389 -0.0454973 -0.0025439858 0.055325918 0.0832903 0.061019711 0.0092434362 -0.038530353 -0.065241314 -0.068958536 -0.066106133 -0.064958081][-0.0629609 -0.062130217 -0.061921187 -0.063919663 -0.061952163 -0.04524795 -0.01647301 0.0028372109 -0.0012373403 -0.02426045 -0.048216637 -0.064798743 -0.067491323 -0.065932021 -0.06494683][-0.062987193 -0.062607504 -0.062111463 -0.062172074 -0.061852988 -0.056828007 -0.047089092 -0.039497763 -0.039845146 -0.048378363 -0.058180843 -0.064901687 -0.065957829 -0.065483086 -0.064897574][-0.062926568 -0.062769435 -0.062408596 -0.062662952 -0.063977435 -0.064531 -0.06296356 -0.060399193 -0.059116259 -0.059303228 -0.061021898 -0.062560588 -0.063099794 -0.063640229 -0.063841484][-0.062908418 -0.06276314 -0.062080387 -0.061315402 -0.060885586 -0.060642179 -0.06062581 -0.060716417 -0.060801994 -0.061308969 -0.061721954 -0.061675936 -0.061983045 -0.062448628 -0.062801108][-0.063236609 -0.063170195 -0.062755831 -0.061959565 -0.061068 -0.060653698 -0.060731366 -0.060998581 -0.06146308 -0.062058136 -0.062405273 -0.062629461 -0.062849969 -0.063033983 -0.063170321][-0.064408392 -0.064283833 -0.06423106 -0.064211905 -0.063902289 -0.063434415 -0.063083559 -0.062744506 -0.062918842 -0.063268028 -0.063527256 -0.063677944 -0.063816726 -0.063944109 -0.064002343]]...]
INFO - root - 2017-12-06 03:13:24.350289: step 7210, loss = 0.88, batch loss = 0.67 (17.2 examples/sec; 0.464 sec/batch; 41h:58m:14s remains)
INFO - root - 2017-12-06 03:13:28.863583: step 7220, loss = 0.91, batch loss = 0.70 (16.7 examples/sec; 0.479 sec/batch; 43h:15m:20s remains)
INFO - root - 2017-12-06 03:13:33.376489: step 7230, loss = 0.86, batch loss = 0.65 (16.7 examples/sec; 0.478 sec/batch; 43h:13m:59s remains)
INFO - root - 2017-12-06 03:13:37.864249: step 7240, loss = 0.86, batch loss = 0.65 (17.3 examples/sec; 0.462 sec/batch; 41h:42m:33s remains)
INFO - root - 2017-12-06 03:13:42.412024: step 7250, loss = 0.91, batch loss = 0.69 (17.1 examples/sec; 0.468 sec/batch; 42h:19m:28s remains)
INFO - root - 2017-12-06 03:13:46.977787: step 7260, loss = 0.96, batch loss = 0.75 (17.7 examples/sec; 0.452 sec/batch; 40h:51m:47s remains)
INFO - root - 2017-12-06 03:13:51.318895: step 7270, loss = 0.85, batch loss = 0.63 (17.6 examples/sec; 0.453 sec/batch; 40h:57m:12s remains)
INFO - root - 2017-12-06 03:13:55.877433: step 7280, loss = 0.89, batch loss = 0.67 (17.1 examples/sec; 0.469 sec/batch; 42h:23m:12s remains)
INFO - root - 2017-12-06 03:14:00.443556: step 7290, loss = 0.83, batch loss = 0.62 (18.1 examples/sec; 0.443 sec/batch; 40h:01m:20s remains)
INFO - root - 2017-12-06 03:14:04.921615: step 7300, loss = 0.83, batch loss = 0.62 (18.3 examples/sec; 0.436 sec/batch; 39h:23m:26s remains)
2017-12-06 03:14:05.353031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.34557325 -0.389095 -0.48117974 -0.65319175 -0.89220363 -1.0550363 -1.1832279 -1.1450838 -1.0188055 -0.895587 -0.78367221 -0.83908755 -0.89714664 -0.81155032 -0.77434278][-0.30900082 -0.31055376 -0.28974965 -0.33042496 -0.44262969 -0.52130234 -0.81855667 -1.0812057 -1.2594175 -1.2313423 -1.0840406 -1.0283226 -0.93506891 -1.0911257 -1.1796006][0.03058286 0.15025496 0.15025222 0.080713205 0.00013738871 -0.13064405 -0.33837384 -0.55754191 -0.85029131 -1.1486975 -1.3910515 -1.4638352 -1.3301145 -1.3463897 -1.3312335][0.84448761 0.87264621 0.58945525 0.47540581 0.32082829 0.21039236 0.015776336 -0.25601438 -0.48769385 -0.75014174 -1.0375212 -1.3389623 -1.625298 -1.7404273 -1.6444532][1.5238645 1.3997222 1.2278298 1.0821064 0.74780124 0.44422239 0.062977143 -0.18936613 -0.3276712 -0.48809209 -0.74499166 -1.0349771 -1.3437369 -1.6128713 -1.85808][1.5407524 1.43941 1.5405273 1.5604228 1.3894083 1.1720493 0.78412372 0.36125383 0.013848737 -0.27368194 -0.54736549 -0.79570138 -1.0948821 -1.3089607 -1.4348516][1.4704621 1.2942498 1.5947291 1.6880149 1.7820847 1.6491009 1.4645503 1.253441 1.0749406 0.70992255 0.27805623 -0.036966339 -0.34396806 -0.61362725 -0.8193391][1.4567627 1.5495015 1.7821485 1.6616009 1.60736 1.6343192 1.6340443 1.5818877 1.5845083 1.4839805 1.2979742 0.94485891 0.55532378 0.39619207 0.1690512][1.2809178 1.2793477 1.3348647 1.2227862 1.1981329 1.215332 1.2359357 1.2197568 1.2568009 1.2768886 1.2680475 1.3303676 1.3662652 1.2239044 0.91804266][0.92506307 0.72626382 0.59773344 0.35897952 0.18415332 0.085228108 0.14319628 0.23405415 0.30382618 0.40041807 0.50908858 0.70677972 0.88613415 1.012428 1.0139951][0.2861065 0.13176668 -0.20154354 -0.47954369 -0.67406505 -0.88668168 -1.0496123 -1.0459083 -1.0066233 -0.95161426 -0.87383008 -0.6205337 -0.28891033 -0.014781632 0.17641446][-0.48117328 -0.73645204 -1.0874738 -1.228815 -1.3640726 -1.5223968 -1.7728028 -1.9121165 -2.0192914 -1.9902593 -1.881915 -1.7981113 -1.6523689 -1.3509024 -0.97262162][-1.1338019 -1.3823462 -1.7688358 -2.0314219 -2.1194243 -2.1047206 -2.1563644 -2.1692748 -2.2171311 -2.2835717 -2.2595646 -2.1032159 -1.9229982 -1.8364532 -1.6965544][-1.4320658 -1.6141648 -1.8976085 -2.1182642 -2.2516987 -2.346257 -2.3029444 -2.2949829 -2.3114376 -2.3582871 -2.2731709 -2.1226614 -1.9575413 -1.8269417 -1.6987826][-0.89631104 -1.2450498 -1.7465177 -2.1254206 -2.2736723 -2.3653486 -2.27967 -2.3007975 -2.3164749 -2.2778015 -2.2217195 -2.1631482 -2.0545859 -1.8795109 -1.7179803]]...]
INFO - root - 2017-12-06 03:14:09.926002: step 7310, loss = 0.84, batch loss = 0.63 (17.6 examples/sec; 0.455 sec/batch; 41h:03m:57s remains)
INFO - root - 2017-12-06 03:14:14.361271: step 7320, loss = 0.83, batch loss = 0.61 (17.6 examples/sec; 0.455 sec/batch; 41h:06m:36s remains)
INFO - root - 2017-12-06 03:14:18.833416: step 7330, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.451 sec/batch; 40h:43m:35s remains)
INFO - root - 2017-12-06 03:14:23.398583: step 7340, loss = 0.86, batch loss = 0.65 (17.9 examples/sec; 0.448 sec/batch; 40h:25m:54s remains)
INFO - root - 2017-12-06 03:14:27.910497: step 7350, loss = 0.86, batch loss = 0.65 (18.3 examples/sec; 0.437 sec/batch; 39h:28m:02s remains)
INFO - root - 2017-12-06 03:14:32.446952: step 7360, loss = 0.86, batch loss = 0.65 (17.8 examples/sec; 0.450 sec/batch; 40h:39m:28s remains)
INFO - root - 2017-12-06 03:14:36.913807: step 7370, loss = 0.83, batch loss = 0.62 (17.3 examples/sec; 0.462 sec/batch; 41h:44m:17s remains)
INFO - root - 2017-12-06 03:14:41.222675: step 7380, loss = 0.86, batch loss = 0.65 (18.2 examples/sec; 0.439 sec/batch; 39h:37m:40s remains)
INFO - root - 2017-12-06 03:14:45.642483: step 7390, loss = 0.83, batch loss = 0.61 (17.9 examples/sec; 0.447 sec/batch; 40h:22m:13s remains)
INFO - root - 2017-12-06 03:14:50.125107: step 7400, loss = 0.90, batch loss = 0.69 (18.1 examples/sec; 0.443 sec/batch; 40h:00m:47s remains)
2017-12-06 03:14:50.557329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.095569849 -0.0991923 -0.0907532 -0.10286882 -0.15168504 -0.21226555 -0.24204904 -0.228857 -0.17775734 -0.11622277 -0.065445639 -0.044626795 -0.05420772 -0.083006278 -0.15088588][-0.14671209 -0.15791768 -0.1558331 -0.1737428 -0.22451979 -0.28523538 -0.30879307 -0.30021149 -0.24776505 -0.16837737 -0.059543729 0.024903096 0.050455548 0.05489666 0.012319222][-0.21273039 -0.23837984 -0.26407772 -0.29770389 -0.34936816 -0.38838941 -0.37553012 -0.33415282 -0.25745824 -0.18061897 -0.054675817 0.071665317 0.13249069 0.14902142 0.12699388][-0.25670654 -0.308055 -0.36295903 -0.39955032 -0.42843509 -0.431633 -0.37859339 -0.29511136 -0.19314101 -0.12309906 -0.0084286816 0.09622 0.15433402 0.17607614 0.1498768][-0.23532368 -0.28578615 -0.32075268 -0.32167 -0.29105455 -0.22619322 -0.13296089 -0.046150167 0.01487226 0.029027425 0.067539513 0.12768167 0.15707861 0.12090313 0.0632668][-0.10307392 -0.090679705 -0.07301157 0.0002001375 0.14139502 0.31735533 0.46186715 0.54457504 0.55362082 0.45348018 0.34659988 0.28122032 0.23400183 0.16060337 0.092657283][0.11394523 0.21000145 0.30353522 0.45393854 0.64920586 0.90135682 1.104082 1.1870244 1.1517617 0.97337896 0.77384859 0.58212423 0.44109845 0.29737586 0.1755214][0.24204226 0.40257013 0.58276558 0.7737298 0.96654671 1.2080805 1.3867303 1.4743646 1.4523693 1.2510638 1.0214334 0.77768582 0.56997174 0.37037951 0.23225199][0.25674003 0.43338197 0.60937673 0.79293621 0.97835606 1.1388005 1.2311764 1.2584637 1.214185 1.0563427 0.908843 0.6925379 0.49269193 0.32284075 0.19454591][0.14769623 0.27107215 0.39781886 0.52452129 0.60288262 0.66629696 0.69948804 0.65222621 0.57759237 0.47423929 0.37726003 0.24942406 0.15026526 0.0426457 -0.029403515][-0.0597982 -0.0061720349 0.063306943 0.11289883 0.1141236 0.094974726 0.059928 0.0022692755 -0.045757912 -0.1237101 -0.14184147 -0.16985063 -0.20906636 -0.22503331 -0.20823334][-0.2043415 -0.2293897 -0.24076666 -0.25348127 -0.289259 -0.34828627 -0.41920912 -0.49058855 -0.5421074 -0.56724477 -0.51378793 -0.44186115 -0.3776927 -0.32416189 -0.25577381][-0.22786511 -0.28973359 -0.35557789 -0.42122537 -0.49459445 -0.57125765 -0.63635582 -0.6650154 -0.68051553 -0.66641307 -0.59892768 -0.50865483 -0.39965892 -0.2720269 -0.19610652][-0.1939458 -0.24953429 -0.30698323 -0.37045586 -0.44519961 -0.51533794 -0.5659399 -0.59118986 -0.57966834 -0.520724 -0.43727851 -0.37188417 -0.31541783 -0.2198893 -0.15721795][-0.12800741 -0.16269395 -0.19559552 -0.24123296 -0.30080321 -0.34366548 -0.35796463 -0.36446232 -0.35529137 -0.30246234 -0.25675583 -0.23197068 -0.20089985 -0.15625542 -0.12420875]]...]
INFO - root - 2017-12-06 03:14:55.065361: step 7410, loss = 0.89, batch loss = 0.68 (17.6 examples/sec; 0.453 sec/batch; 40h:56m:11s remains)
INFO - root - 2017-12-06 03:14:59.607266: step 7420, loss = 0.82, batch loss = 0.60 (17.8 examples/sec; 0.449 sec/batch; 40h:32m:30s remains)
INFO - root - 2017-12-06 03:15:04.126062: step 7430, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.448 sec/batch; 40h:27m:44s remains)
INFO - root - 2017-12-06 03:15:08.615145: step 7440, loss = 0.85, batch loss = 0.64 (17.2 examples/sec; 0.464 sec/batch; 41h:53m:04s remains)
INFO - root - 2017-12-06 03:15:13.158841: step 7450, loss = 0.87, batch loss = 0.66 (18.4 examples/sec; 0.435 sec/batch; 39h:19m:09s remains)
INFO - root - 2017-12-06 03:15:17.614819: step 7460, loss = 0.85, batch loss = 0.64 (18.0 examples/sec; 0.445 sec/batch; 40h:13m:16s remains)
INFO - root - 2017-12-06 03:15:22.169242: step 7470, loss = 0.89, batch loss = 0.68 (17.1 examples/sec; 0.469 sec/batch; 42h:18m:25s remains)
INFO - root - 2017-12-06 03:15:26.472989: step 7480, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.453 sec/batch; 40h:51m:28s remains)
INFO - root - 2017-12-06 03:15:31.151298: step 7490, loss = 0.84, batch loss = 0.63 (20.5 examples/sec; 0.391 sec/batch; 35h:17m:46s remains)
INFO - root - 2017-12-06 03:15:35.689082: step 7500, loss = 0.86, batch loss = 0.65 (18.1 examples/sec; 0.443 sec/batch; 39h:57m:38s remains)
2017-12-06 03:15:36.138726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.052726775 -0.052199937 -0.052278783 -0.052473873 -0.052771948 -0.053098492 -0.053401168 -0.05361155 -0.053684875 -0.053552452 -0.053229172 -0.052876219 -0.0525335 -0.05236011 -0.052404262][-0.052702777 -0.0521394 -0.052233785 -0.052522022 -0.052934751 -0.053400714 -0.053984962 -0.054600686 -0.054993242 -0.054830704 -0.054239362 -0.053442828 -0.052748524 -0.052326303 -0.052242078][-0.052558921 -0.052103091 -0.052239429 -0.052579552 -0.053044736 -0.053663023 -0.05466336 -0.055839896 -0.056678575 -0.056726739 -0.056054875 -0.054924257 -0.053750407 -0.05294618 -0.052627079][-0.052492365 -0.052119844 -0.052332152 -0.052731864 -0.053218134 -0.054015368 -0.055547208 -0.057410546 -0.05880709 -0.059148788 -0.058490984 -0.057132192 -0.055476166 -0.054207668 -0.053591684][-0.052531771 -0.052215118 -0.052568253 -0.053067528 -0.053618897 -0.054607477 -0.056673296 -0.05935188 -0.06130917 -0.061734837 -0.06106694 -0.05957352 -0.05762 -0.056057364 -0.055251162][-0.052760739 -0.052567557 -0.053041048 -0.053595591 -0.054128241 -0.055276394 -0.057754148 -0.061115686 -0.063400917 -0.06380149 -0.063057877 -0.061677467 -0.059810691 -0.058210008 -0.05736636][-0.053278886 -0.053189836 -0.053759057 -0.054286812 -0.05472428 -0.055945739 -0.058608424 -0.062130377 -0.064497352 -0.064843662 -0.064163379 -0.063090481 -0.061596461 -0.06025381 -0.059583563][-0.053889237 -0.0540571 -0.05475267 -0.055227198 -0.055702806 -0.05703279 -0.059505846 -0.062632658 -0.064692147 -0.065065138 -0.064571105 -0.063824832 -0.062695764 -0.061799612 -0.061575096][-0.054694176 -0.05506292 -0.055882096 -0.056338303 -0.056834571 -0.058296185 -0.060597558 -0.062989883 -0.0643739 -0.064711146 -0.064405829 -0.063831165 -0.063049048 -0.062678814 -0.062868364][-0.055422258 -0.056032244 -0.056998521 -0.057465419 -0.057983235 -0.059336551 -0.061244581 -0.06287086 -0.06371744 -0.063885286 -0.063598774 -0.063250937 -0.062985413 -0.062950224 -0.063151471][-0.056136634 -0.057033658 -0.058091473 -0.058549941 -0.058989741 -0.059978269 -0.061238028 -0.062179383 -0.06259875 -0.062602215 -0.06238528 -0.062190995 -0.062244426 -0.062357847 -0.062407415][-0.056751594 -0.057738137 -0.058666032 -0.059046168 -0.059331335 -0.059814673 -0.060360067 -0.060849503 -0.060991403 -0.060883805 -0.06064596 -0.060474407 -0.060611691 -0.06088417 -0.061030723][-0.056988172 -0.057839293 -0.058538809 -0.058769248 -0.05881051 -0.058846675 -0.05893521 -0.059065122 -0.059047427 -0.058844756 -0.058545139 -0.058445789 -0.058594733 -0.05895349 -0.059216015][-0.05628521 -0.056938373 -0.0574282 -0.057506181 -0.057338767 -0.057117637 -0.057010125 -0.0569748 -0.056885663 -0.056643054 -0.0563339 -0.05625302 -0.056475874 -0.056828447 -0.057119463][-0.055239342 -0.055502132 -0.055759713 -0.055754006 -0.055551417 -0.055301286 -0.055144407 -0.05503739 -0.054925561 -0.054768503 -0.054583933 -0.054551955 -0.054762244 -0.055038493 -0.055285979]]...]
INFO - root - 2017-12-06 03:15:40.663733: step 7510, loss = 0.84, batch loss = 0.63 (17.9 examples/sec; 0.447 sec/batch; 40h:22m:41s remains)
INFO - root - 2017-12-06 03:15:45.138678: step 7520, loss = 0.90, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 40h:07m:58s remains)
INFO - root - 2017-12-06 03:15:49.597365: step 7530, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:18m:46s remains)
INFO - root - 2017-12-06 03:15:54.131062: step 7540, loss = 0.96, batch loss = 0.75 (18.4 examples/sec; 0.435 sec/batch; 39h:17m:53s remains)
INFO - root - 2017-12-06 03:15:58.669909: step 7550, loss = 0.88, batch loss = 0.66 (17.8 examples/sec; 0.450 sec/batch; 40h:36m:06s remains)
INFO - root - 2017-12-06 03:16:03.147672: step 7560, loss = 0.90, batch loss = 0.68 (17.9 examples/sec; 0.446 sec/batch; 40h:17m:56s remains)
INFO - root - 2017-12-06 03:16:07.631733: step 7570, loss = 0.86, batch loss = 0.64 (18.0 examples/sec; 0.444 sec/batch; 40h:07m:07s remains)
INFO - root - 2017-12-06 03:16:12.122402: step 7580, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.448 sec/batch; 40h:24m:54s remains)
INFO - root - 2017-12-06 03:16:16.467207: step 7590, loss = 0.81, batch loss = 0.59 (17.8 examples/sec; 0.448 sec/batch; 40h:28m:26s remains)
INFO - root - 2017-12-06 03:16:21.008927: step 7600, loss = 0.89, batch loss = 0.68 (17.0 examples/sec; 0.472 sec/batch; 42h:35m:42s remains)
2017-12-06 03:16:21.465708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028194614 -0.028770939 -0.031382132 -0.034560651 -0.037470575 -0.039948784 -0.041958403 -0.044539824 -0.046616592 -0.047606453 -0.047662579 -0.047219388 -0.046380945 -0.045473762 -0.044366024][-0.027843479 -0.028180949 -0.030395642 -0.032846037 -0.034860786 -0.036508512 -0.038113993 -0.039732691 -0.041083977 -0.042125106 -0.0428766 -0.043544527 -0.043837287 -0.044108719 -0.04411114][-0.027650084 -0.027547453 -0.029363897 -0.03136453 -0.032656729 -0.033561613 -0.034535259 -0.035384137 -0.03628353 -0.037500538 -0.039182585 -0.041494712 -0.043954883 -0.046227552 -0.047857359][-0.027417552 -0.027031519 -0.02846216 -0.030041866 -0.030841116 -0.031160824 -0.031585317 -0.031865507 -0.032332625 -0.03367063 -0.036224719 -0.040623747 -0.046847187 -0.053843915 -0.059157841][-0.027100869 -0.026637975 -0.027753536 -0.029008001 -0.029438015 -0.029501282 -0.029657461 -0.029530127 -0.029662095 -0.030912258 -0.033199321 -0.037477802 -0.046015818 -0.057822514 -0.068616442][-0.026779681 -0.02624223 -0.027179416 -0.028137218 -0.028329425 -0.028233174 -0.028166823 -0.027932651 -0.027890615 -0.02873408 -0.029247016 -0.0278572 -0.030625314 -0.040277049 -0.055750467][-0.026492186 -0.025773548 -0.026483931 -0.027164456 -0.027253125 -0.027081717 -0.026908163 -0.026775565 -0.02671526 -0.026967574 -0.026839126 -0.021789059 -0.016629055 -0.018998716 -0.035905089][-0.02601463 -0.025284331 -0.025772203 -0.026257236 -0.026303351 -0.026141584 -0.025986325 -0.025839657 -0.025728315 -0.025553621 -0.025081672 -0.022234429 -0.017413799 -0.015311398 -0.029329304][-0.025623757 -0.024880413 -0.02518012 -0.02561453 -0.025648251 -0.025512267 -0.02536742 -0.025103267 -0.024827078 -0.024534985 -0.024099242 -0.022712491 -0.019348133 -0.016557984 -0.025904667][-0.025669545 -0.024762381 -0.024939027 -0.025230724 -0.025184192 -0.024955302 -0.02471615 -0.024426986 -0.024125408 -0.023935061 -0.023663189 -0.023374312 -0.022336986 -0.021081213 -0.02664385][-0.026086453 -0.024998609 -0.024954688 -0.024947897 -0.024779182 -0.024431471 -0.024084829 -0.023925073 -0.023716357 -0.023577258 -0.023520268 -0.023707699 -0.024066128 -0.024687372 -0.027656551][-0.026578464 -0.025485035 -0.025238805 -0.025008161 -0.024761423 -0.0244053 -0.024018873 -0.023868207 -0.023789823 -0.023669165 -0.023396172 -0.023412492 -0.023954023 -0.024941016 -0.026690055][-0.026895035 -0.025874682 -0.025511611 -0.025154773 -0.02484525 -0.024472054 -0.024148718 -0.023955066 -0.023926824 -0.023806918 -0.023511287 -0.023370236 -0.0235174 -0.02428218 -0.025372785][-0.027582273 -0.026684154 -0.026245866 -0.025869332 -0.025364 -0.025010381 -0.024757825 -0.02448817 -0.02442494 -0.024227656 -0.02402553 -0.023970891 -0.023897771 -0.024553113 -0.025367681][-0.028778434 -0.028131351 -0.027874656 -0.02759267 -0.02711077 -0.026716884 -0.026408117 -0.026162624 -0.026060846 -0.025990177 -0.025904454 -0.025960349 -0.025981106 -0.026384603 -0.026868191]]...]
INFO - root - 2017-12-06 03:16:26.098492: step 7610, loss = 0.82, batch loss = 0.61 (17.7 examples/sec; 0.452 sec/batch; 40h:48m:07s remains)
INFO - root - 2017-12-06 03:16:30.639317: step 7620, loss = 0.86, batch loss = 0.65 (17.1 examples/sec; 0.467 sec/batch; 42h:07m:09s remains)
INFO - root - 2017-12-06 03:16:35.102388: step 7630, loss = 0.82, batch loss = 0.61 (17.1 examples/sec; 0.469 sec/batch; 42h:16m:47s remains)
INFO - root - 2017-12-06 03:16:39.567675: step 7640, loss = 0.90, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 39h:40m:58s remains)
INFO - root - 2017-12-06 03:16:44.024699: step 7650, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.439 sec/batch; 39h:34m:32s remains)
INFO - root - 2017-12-06 03:16:48.557429: step 7660, loss = 0.85, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 40h:33m:50s remains)
INFO - root - 2017-12-06 03:16:53.116274: step 7670, loss = 0.84, batch loss = 0.63 (18.0 examples/sec; 0.444 sec/batch; 40h:06m:08s remains)
INFO - root - 2017-12-06 03:16:57.664817: step 7680, loss = 0.88, batch loss = 0.66 (17.3 examples/sec; 0.463 sec/batch; 41h:47m:39s remains)
INFO - root - 2017-12-06 03:17:01.932961: step 7690, loss = 0.88, batch loss = 0.66 (17.9 examples/sec; 0.446 sec/batch; 40h:17m:05s remains)
INFO - root - 2017-12-06 03:17:06.412300: step 7700, loss = 0.83, batch loss = 0.61 (18.5 examples/sec; 0.432 sec/batch; 38h:55m:53s remains)
2017-12-06 03:17:06.857480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059965536 -0.0575645 -0.054688863 -0.051105693 -0.048075467 -0.0498753 -0.058263548 -0.062302496 -0.062594809 -0.060779057 -0.060235932 -0.059794277 -0.058491033 -0.057917967 -0.057411846][-0.066119649 -0.061492234 -0.055834655 -0.05160521 -0.04681091 -0.047311496 -0.055119757 -0.061764982 -0.067021266 -0.066643991 -0.065432355 -0.062815584 -0.060782779 -0.059364937 -0.058647454][-0.079537019 -0.078854978 -0.074046835 -0.071010731 -0.064234316 -0.061450452 -0.064291932 -0.067349479 -0.070404813 -0.068585075 -0.0700237 -0.065773293 -0.062509738 -0.060741886 -0.05989515][-0.0915247 -0.089021809 -0.089943387 -0.0870368 -0.083567783 -0.085858651 -0.0940421 -0.0959801 -0.0910911 -0.079070315 -0.069549315 -0.060432408 -0.057133846 -0.05706232 -0.059210937][-0.091992356 -0.08861725 -0.08858113 -0.086450905 -0.0838712 -0.08390259 -0.092420354 -0.10018145 -0.10446756 -0.092311963 -0.073671453 -0.058773756 -0.051949531 -0.052454636 -0.057491582][-0.077482142 -0.075254671 -0.072918653 -0.065425381 -0.056031264 -0.050086223 -0.052116893 -0.066321537 -0.086160585 -0.089804456 -0.077060461 -0.059594326 -0.045328587 -0.046042129 -0.053857185][-0.053042587 -0.064080149 -0.066811107 -0.058228903 -0.042520728 -0.023036469 -0.009769775 -0.018881947 -0.056904942 -0.070660181 -0.068380073 -0.052349016 -0.048466958 -0.050411239 -0.053848445][-0.040260173 -0.06280645 -0.0752168 -0.075966887 -0.05619118 -0.020545837 0.013236634 0.01103773 -0.031215224 -0.052933648 -0.057897888 -0.042734988 -0.041255474 -0.047753654 -0.056656756][-0.029215518 -0.058765545 -0.077726349 -0.085888341 -0.066526674 -0.023998186 0.02117157 0.022758543 -0.0182615 -0.063660286 -0.085568488 -0.065264262 -0.053101841 -0.046232823 -0.04748787][-0.039755605 -0.05949704 -0.071764879 -0.083502144 -0.087338448 -0.063862473 -0.014284648 0.0057331324 -0.020107992 -0.062116448 -0.094854131 -0.092695951 -0.068935253 -0.045374431 -0.048077494][-0.060562115 -0.069707155 -0.077107094 -0.087876141 -0.091044337 -0.0780612 -0.033129245 -0.006504342 -0.011157054 -0.041158356 -0.093157448 -0.10185935 -0.081183851 -0.041282855 -0.043031286][-0.06410601 -0.072052345 -0.090521008 -0.10461944 -0.1044752 -0.089192256 -0.043687213 -0.02343696 -0.037332341 -0.074478336 -0.13063733 -0.14383677 -0.12817532 -0.074434914 -0.060616009][-0.065262936 -0.075017057 -0.099498168 -0.12496793 -0.13298392 -0.11388841 -0.05890359 -0.022182167 -0.02069369 -0.0820033 -0.17315981 -0.21934122 -0.20368466 -0.13776752 -0.10276698][-0.057811067 -0.080250211 -0.11208302 -0.14381298 -0.16947511 -0.161621 -0.11922911 -0.072616331 -0.042304482 -0.098440513 -0.17898223 -0.27010345 -0.27488339 -0.22495253 -0.16597384][-0.06648194 -0.080816887 -0.10417853 -0.13311888 -0.15180792 -0.14408386 -0.1157392 -0.067597061 -0.048386104 -0.062731192 -0.10836212 -0.21658455 -0.26027241 -0.268521 -0.21302669]]...]
INFO - root - 2017-12-06 03:17:11.357937: step 7710, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.446 sec/batch; 40h:12m:39s remains)
INFO - root - 2017-12-06 03:17:15.854059: step 7720, loss = 0.84, batch loss = 0.63 (17.7 examples/sec; 0.453 sec/batch; 40h:50m:10s remains)
INFO - root - 2017-12-06 03:17:20.354432: step 7730, loss = 0.91, batch loss = 0.70 (17.9 examples/sec; 0.446 sec/batch; 40h:16m:04s remains)
INFO - root - 2017-12-06 03:17:24.894344: step 7740, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.451 sec/batch; 40h:42m:56s remains)
INFO - root - 2017-12-06 03:17:29.513349: step 7750, loss = 0.91, batch loss = 0.70 (17.6 examples/sec; 0.454 sec/batch; 40h:59m:33s remains)
INFO - root - 2017-12-06 03:17:34.040881: step 7760, loss = 0.89, batch loss = 0.68 (17.3 examples/sec; 0.463 sec/batch; 41h:44m:02s remains)
INFO - root - 2017-12-06 03:17:38.490804: step 7770, loss = 0.86, batch loss = 0.65 (17.2 examples/sec; 0.466 sec/batch; 42h:04m:25s remains)
INFO - root - 2017-12-06 03:17:43.020363: step 7780, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 40h:31m:50s remains)
INFO - root - 2017-12-06 03:17:47.314753: step 7790, loss = 0.90, batch loss = 0.68 (28.5 examples/sec; 0.281 sec/batch; 25h:20m:40s remains)
INFO - root - 2017-12-06 03:17:51.865143: step 7800, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.436 sec/batch; 39h:19m:34s remains)
2017-12-06 03:17:52.332928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.62041944 -0.97150952 -1.2669253 -1.5534111 -1.6971883 -1.6671954 -1.8192143 -1.9786018 -1.9644262 -1.8350139 -1.7505994 -1.8007603 -1.8516682 -1.8334322 -1.7408422][-0.36483091 -0.61209774 -0.85204148 -1.0978222 -1.232537 -1.3878679 -1.5596135 -1.6709802 -1.8277282 -1.9340858 -1.9703029 -1.9853467 -1.9456983 -1.9704103 -1.9208689][-0.17615131 -0.3748399 -0.47851026 -0.55044115 -0.60811704 -0.77683842 -0.9939115 -1.2064461 -1.3656833 -1.539636 -1.7765 -1.9554754 -1.9839725 -2.0111639 -1.973863][-0.25205341 -0.25367862 -0.20385639 -0.2176514 -0.34750837 -0.51578826 -0.6924513 -0.79203194 -0.91689932 -1.1229601 -1.418058 -1.6626692 -1.7963138 -1.9620405 -1.9981668][-0.21943511 -0.22898376 -0.1101869 0.033350497 0.016562976 -0.086402729 -0.25018597 -0.37369239 -0.55199963 -0.6537717 -0.83373129 -1.0266606 -1.2613754 -1.418776 -1.5061783][0.20272367 0.18437867 0.29498214 0.24757005 0.10693116 0.12307386 0.16896102 0.2168694 0.088389635 -0.080079757 -0.35711324 -0.47199589 -0.62433791 -0.83195716 -1.0753056][0.034390092 0.28055364 0.44832551 0.466402 0.41771239 0.33688128 0.32173616 0.47391719 0.58857197 0.49925762 0.41388035 0.24274395 0.040050633 -0.085779987 -0.18133891][-0.35046405 -0.1397379 0.072449505 0.22835676 0.31269765 0.39418662 0.41975415 0.42959327 0.48088908 0.58818853 0.68074113 0.64957023 0.63053441 0.52823108 0.43428689][-1.1292886 -0.8612054 -0.65675914 -0.57030523 -0.437674 -0.15387714 -0.0485075 -0.033408333 -0.0072649121 0.0571723 0.18139903 0.34411 0.52515751 0.60448182 0.64917189][-1.7430978 -1.561655 -1.3998593 -1.2434604 -1.1127681 -0.94575745 -0.93162382 -0.87717736 -0.84901905 -0.77426821 -0.68426722 -0.52173817 -0.24207345 0.037802845 0.22138943][-1.8409507 -1.9071394 -1.9297832 -1.8501377 -1.7097986 -1.5033453 -1.4636582 -1.5523846 -1.641114 -1.6323955 -1.5696541 -1.3074325 -0.98557717 -0.588079 -0.28074086][-1.5657638 -1.6858845 -1.7535627 -1.7114631 -1.5893313 -1.5516536 -1.5260332 -1.5806708 -1.7045881 -1.8446401 -1.949103 -1.9026978 -1.7313598 -1.345965 -0.92480177][-1.4906936 -1.6422266 -1.6068817 -1.5384108 -1.4378498 -1.3292146 -1.2290636 -1.2283463 -1.271853 -1.3886269 -1.5382836 -1.5555711 -1.5413452 -1.4252836 -1.1575656][-1.461091 -1.494279 -1.4143752 -1.4144874 -1.3697957 -1.280074 -1.1679584 -1.0800158 -1.0886809 -1.0680748 -1.0559468 -1.0924431 -1.1288916 -1.0677009 -0.94547486][-1.556208 -1.5082997 -1.4204358 -1.3782549 -1.3382405 -1.2221273 -1.1054028 -1.0484483 -1.0169146 -0.979632 -0.91045159 -0.827418 -0.69513577 -0.71089768 -0.69667083]]...]
INFO - root - 2017-12-06 03:17:56.906649: step 7810, loss = 0.87, batch loss = 0.66 (18.0 examples/sec; 0.443 sec/batch; 39h:58m:44s remains)
INFO - root - 2017-12-06 03:18:01.371253: step 7820, loss = 0.87, batch loss = 0.66 (18.7 examples/sec; 0.428 sec/batch; 38h:37m:41s remains)
INFO - root - 2017-12-06 03:18:05.870577: step 7830, loss = 0.91, batch loss = 0.69 (17.6 examples/sec; 0.454 sec/batch; 40h:57m:59s remains)
INFO - root - 2017-12-06 03:18:10.475534: step 7840, loss = 0.82, batch loss = 0.60 (16.4 examples/sec; 0.489 sec/batch; 44h:05m:56s remains)
INFO - root - 2017-12-06 03:18:15.011146: step 7850, loss = 0.88, batch loss = 0.67 (17.6 examples/sec; 0.455 sec/batch; 41h:02m:36s remains)
INFO - root - 2017-12-06 03:18:19.567957: step 7860, loss = 0.84, batch loss = 0.63 (17.6 examples/sec; 0.455 sec/batch; 41h:03m:07s remains)
INFO - root - 2017-12-06 03:18:24.068857: step 7870, loss = 0.87, batch loss = 0.65 (17.3 examples/sec; 0.463 sec/batch; 41h:43m:02s remains)
INFO - root - 2017-12-06 03:18:28.639609: step 7880, loss = 0.87, batch loss = 0.66 (16.6 examples/sec; 0.481 sec/batch; 43h:23m:48s remains)
INFO - root - 2017-12-06 03:18:33.205287: step 7890, loss = 0.88, batch loss = 0.67 (17.9 examples/sec; 0.446 sec/batch; 40h:14m:28s remains)
INFO - root - 2017-12-06 03:18:37.610755: step 7900, loss = 0.88, batch loss = 0.67 (16.2 examples/sec; 0.493 sec/batch; 44h:29m:12s remains)
2017-12-06 03:18:38.073265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016174082 -0.016088359 -0.016947906 -0.018307276 -0.020121273 -0.022859152 -0.026705623 -0.031346761 -0.035967771 -0.04074144 -0.045605518 -0.050315123 -0.054232225 -0.057518825 -0.06070463][-0.016478214 -0.01645093 -0.018055964 -0.020441592 -0.023397923 -0.026921611 -0.031031277 -0.035607722 -0.040151268 -0.044980049 -0.050038274 -0.055203065 -0.05986397 -0.064046048 -0.068075716][-0.016834803 -0.017293148 -0.01982839 -0.023188341 -0.027324934 -0.032031275 -0.037027318 -0.041647978 -0.045451123 -0.049452037 -0.053939126 -0.058791269 -0.063213877 -0.067376949 -0.071574792][-0.017573781 -0.018939223 -0.02235144 -0.026999112 -0.032522511 -0.038399976 -0.044037856 -0.048499815 -0.05128096 -0.05359716 -0.056249917 -0.059468634 -0.062904321 -0.066514432 -0.070515685][-0.017433871 -0.019877728 -0.024725948 -0.031384192 -0.038916264 -0.046492696 -0.053484678 -0.058291968 -0.060164887 -0.060173295 -0.0599798 -0.060055237 -0.060886055 -0.062664032 -0.065590613][-0.017908938 -0.021386005 -0.027818736 -0.03664409 -0.046183724 -0.055613693 -0.064207107 -0.070024855 -0.071193054 -0.069072343 -0.066011772 -0.062392861 -0.059376262 -0.0578624 -0.058690272][-0.017971616 -0.022211857 -0.030471012 -0.041293673 -0.052340161 -0.063260987 -0.073069781 -0.07939747 -0.080111295 -0.076522887 -0.071649909 -0.064899318 -0.058027003 -0.052979454 -0.051184796][-0.01839558 -0.023404263 -0.032666072 -0.044167392 -0.055479519 -0.0669782 -0.077307791 -0.083329082 -0.083642185 -0.079687752 -0.074212953 -0.065786861 -0.056405682 -0.048427131 -0.044155452][-0.01748924 -0.023440778 -0.0333719 -0.045259058 -0.05637997 -0.067074046 -0.076713286 -0.081792861 -0.081046864 -0.076980144 -0.071811266 -0.063394681 -0.053046323 -0.043577261 -0.037889671][-0.015413992 -0.021552932 -0.031533066 -0.043165036 -0.053557206 -0.062883057 -0.071190715 -0.0751066 -0.073862068 -0.070194349 -0.065806158 -0.058112677 -0.048133265 -0.038642514 -0.032752][-0.013886519 -0.019745152 -0.028906647 -0.039087851 -0.047958389 -0.055642292 -0.062633224 -0.065909944 -0.064896844 -0.062239945 -0.058855038 -0.052314863 -0.043231159 -0.034214873 -0.028478097][-0.012670003 -0.017975032 -0.025691897 -0.034066375 -0.041023258 -0.047184423 -0.053293891 -0.0564019 -0.056172561 -0.054645576 -0.052182697 -0.046813093 -0.038614765 -0.030204117 -0.024905812][-0.012127969 -0.016449552 -0.022438612 -0.029024769 -0.034715604 -0.039645631 -0.045120161 -0.048501745 -0.049033806 -0.0481017 -0.045738656 -0.041058846 -0.033937432 -0.026575554 -0.022064745][-0.012068458 -0.015171498 -0.019621436 -0.02490649 -0.029677447 -0.033589013 -0.03825032 -0.042004485 -0.043142147 -0.04234086 -0.039801266 -0.035590425 -0.029780354 -0.023788799 -0.020242713][-0.014034677 -0.015857853 -0.018906832 -0.023012228 -0.026897244 -0.030318346 -0.0341882 -0.037397038 -0.038540274 -0.037536997 -0.034991369 -0.031331472 -0.026967656 -0.022787549 -0.020324338]]...]
INFO - root - 2017-12-06 03:18:42.626451: step 7910, loss = 0.87, batch loss = 0.66 (17.6 examples/sec; 0.454 sec/batch; 40h:58m:12s remains)
INFO - root - 2017-12-06 03:18:47.139350: step 7920, loss = 0.88, batch loss = 0.67 (17.1 examples/sec; 0.468 sec/batch; 42h:09m:37s remains)
INFO - root - 2017-12-06 03:18:51.623030: step 7930, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.453 sec/batch; 40h:51m:38s remains)
INFO - root - 2017-12-06 03:18:56.120043: step 7940, loss = 0.84, batch loss = 0.62 (17.8 examples/sec; 0.449 sec/batch; 40h:30m:11s remains)
INFO - root - 2017-12-06 03:19:00.586264: step 7950, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.446 sec/batch; 40h:12m:34s remains)
INFO - root - 2017-12-06 03:19:05.097745: step 7960, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.436 sec/batch; 39h:19m:02s remains)
INFO - root - 2017-12-06 03:19:09.580416: step 7970, loss = 0.85, batch loss = 0.63 (17.3 examples/sec; 0.462 sec/batch; 41h:37m:30s remains)
INFO - root - 2017-12-06 03:19:14.106982: step 7980, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 40h:26m:11s remains)
INFO - root - 2017-12-06 03:19:18.581466: step 7990, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 40h:42m:45s remains)
INFO - root - 2017-12-06 03:19:22.883953: step 8000, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.440 sec/batch; 39h:37m:52s remains)
2017-12-06 03:19:23.341414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021907471 -0.021056734 -0.02145762 -0.022684358 -0.02423612 -0.025782455 -0.026890185 -0.027170539 -0.027021941 -0.026788317 -0.026472833 -0.026307005 -0.026160501 -0.026224375 -0.026430979][-0.01927159 -0.019303285 -0.020453721 -0.022148177 -0.024058893 -0.025785785 -0.026912335 -0.027055826 -0.026820894 -0.02656189 -0.026408538 -0.026493728 -0.026668403 -0.027085129 -0.027685482][-0.017638627 -0.01783593 -0.019035522 -0.020475328 -0.022118554 -0.023554716 -0.024400298 -0.024492823 -0.024445169 -0.02458683 -0.024928194 -0.025511786 -0.026224785 -0.027090535 -0.028043684][-0.016673628 -0.01640889 -0.017031744 -0.0177221 -0.018739466 -0.01969517 -0.020166244 -0.020198144 -0.020432886 -0.021139946 -0.02211057 -0.023323145 -0.024621394 -0.025999177 -0.027492817][-0.016404144 -0.015428133 -0.015297666 -0.015114419 -0.015242256 -0.015474021 -0.015454147 -0.015281115 -0.015676044 -0.016767938 -0.018297922 -0.02012711 -0.02200599 -0.0239093 -0.0259398][-0.016873069 -0.015116561 -0.014326368 -0.01339145 -0.012575585 -0.01189765 -0.011205114 -0.010693371 -0.011033103 -0.012302376 -0.014216613 -0.016466051 -0.018906884 -0.021275051 -0.023674354][-0.017592415 -0.015352804 -0.014195517 -0.012848321 -0.011425681 -0.0099432133 -0.0085567459 -0.00761424 -0.0075839385 -0.00866726 -0.010627165 -0.013047613 -0.015775569 -0.018432271 -0.021094315][-0.018092617 -0.015733682 -0.014687255 -0.01348934 -0.011991784 -0.010149699 -0.0081975535 -0.0066721961 -0.0059340894 -0.0063133836 -0.0078201145 -0.010154549 -0.012917235 -0.015682593 -0.018600013][-0.018492293 -0.016320478 -0.015774161 -0.015213188 -0.014149193 -0.012386251 -0.010176528 -0.0079669654 -0.006100066 -0.005271025 -0.0058318377 -0.0076777637 -0.010260317 -0.013041817 -0.016126029][-0.01883411 -0.017013416 -0.017053604 -0.017192926 -0.016675942 -0.015255965 -0.013015348 -0.010224026 -0.0072580129 -0.0050958097 -0.0044523627 -0.0054664612 -0.0076393113 -0.010266822 -0.013356261][-0.019256301 -0.017797451 -0.018260229 -0.018926032 -0.018880978 -0.017711036 -0.015435278 -0.012105722 -0.0083687678 -0.0051119104 -0.0033394471 -0.0035257265 -0.0051928982 -0.007636264 -0.01055871][-0.019549958 -0.018439218 -0.019018959 -0.019900955 -0.01998983 -0.018900938 -0.016533524 -0.012897506 -0.00869558 -0.0048789829 -0.0023750141 -0.0019171461 -0.0031953007 -0.0055091232 -0.0082058311][-0.019784477 -0.018826865 -0.01926766 -0.01991161 -0.019867416 -0.018738553 -0.016291518 -0.012586098 -0.0082591325 -0.004380323 -0.0015609115 -0.00069713593 -0.0016978011 -0.0039107054 -0.0064993873][-0.019769862 -0.018774293 -0.018761262 -0.018842924 -0.018571809 -0.017299466 -0.014853865 -0.01130037 -0.0073378608 -0.0037539229 -0.00100822 -0.00015506148 -0.0011398271 -0.0033575222 -0.005864352][-0.020084627 -0.018859915 -0.018318467 -0.017685272 -0.016889267 -0.015375465 -0.012947571 -0.0098259449 -0.0065713525 -0.0036600307 -0.0015943125 -0.0011885688 -0.0022809952 -0.0044707209 -0.0069911331]]...]
INFO - root - 2017-12-06 03:19:27.936725: step 8010, loss = 0.94, batch loss = 0.72 (17.9 examples/sec; 0.447 sec/batch; 40h:19m:14s remains)
INFO - root - 2017-12-06 03:19:32.457292: step 8020, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:18m:45s remains)
INFO - root - 2017-12-06 03:19:36.955810: step 8030, loss = 0.89, batch loss = 0.68 (18.4 examples/sec; 0.435 sec/batch; 39h:10m:59s remains)
INFO - root - 2017-12-06 03:19:41.402487: step 8040, loss = 0.82, batch loss = 0.61 (18.0 examples/sec; 0.445 sec/batch; 40h:04m:15s remains)
INFO - root - 2017-12-06 03:19:45.932592: step 8050, loss = 0.85, batch loss = 0.64 (17.8 examples/sec; 0.448 sec/batch; 40h:25m:01s remains)
INFO - root - 2017-12-06 03:19:50.401181: step 8060, loss = 0.83, batch loss = 0.62 (18.1 examples/sec; 0.443 sec/batch; 39h:55m:17s remains)
INFO - root - 2017-12-06 03:19:55.042470: step 8070, loss = 0.92, batch loss = 0.71 (17.0 examples/sec; 0.470 sec/batch; 42h:21m:59s remains)
INFO - root - 2017-12-06 03:19:59.491765: step 8080, loss = 0.88, batch loss = 0.67 (18.5 examples/sec; 0.432 sec/batch; 38h:58m:04s remains)
INFO - root - 2017-12-06 03:20:04.097109: step 8090, loss = 0.85, batch loss = 0.64 (17.3 examples/sec; 0.462 sec/batch; 41h:36m:52s remains)
INFO - root - 2017-12-06 03:20:08.601067: step 8100, loss = 0.82, batch loss = 0.60 (17.6 examples/sec; 0.455 sec/batch; 41h:02m:34s remains)
2017-12-06 03:20:09.101162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027757384 -0.02749224 -0.02759213 -0.027588245 -0.027437784 -0.027037714 -0.026383281 -0.025577109 -0.02471479 -0.023797397 -0.023084849 -0.022735205 -0.022809345 -0.023178127 -0.023546409][-0.027948465 -0.027467109 -0.027370635 -0.0271434 -0.026910897 -0.026527058 -0.026009299 -0.025377896 -0.024733223 -0.024063457 -0.02349443 -0.02323404 -0.023306638 -0.02362626 -0.023910042][-0.027987771 -0.027288817 -0.026976533 -0.026588265 -0.026333109 -0.026031878 -0.025708947 -0.025316369 -0.024973541 -0.02455388 -0.024114717 -0.023926966 -0.024041794 -0.024302796 -0.024400957][-0.027791142 -0.026913881 -0.026392914 -0.025896806 -0.025661889 -0.025470294 -0.02534024 -0.025231052 -0.025200043 -0.025015734 -0.024714716 -0.024584886 -0.024726205 -0.024936356 -0.024911188][-0.027273972 -0.026321799 -0.025682822 -0.025115877 -0.024907362 -0.024900291 -0.024962988 -0.025108702 -0.025381465 -0.025415681 -0.025171585 -0.0250449 -0.025184877 -0.025405105 -0.025328033][-0.026636839 -0.02561539 -0.02493979 -0.02430243 -0.024190225 -0.024363279 -0.024607789 -0.024949104 -0.025487259 -0.025714569 -0.025458246 -0.0252771 -0.025331262 -0.025570627 -0.025498725][-0.025845908 -0.024851739 -0.024215937 -0.023635183 -0.02360075 -0.023890205 -0.02428624 -0.024762567 -0.025468968 -0.025840439 -0.025611468 -0.025386743 -0.025308382 -0.025473725 -0.025405861][-0.025179215 -0.024234757 -0.023669727 -0.023221191 -0.023230996 -0.023509648 -0.0239841 -0.02457799 -0.025368713 -0.025841024 -0.025644671 -0.0253844 -0.025157899 -0.025140893 -0.025024123][-0.024770956 -0.023921959 -0.023413256 -0.022972222 -0.022959691 -0.023169626 -0.023769058 -0.024433672 -0.025240313 -0.025776085 -0.025567003 -0.025218323 -0.024812996 -0.024603061 -0.024448507][-0.0246499 -0.023950126 -0.023599833 -0.023106437 -0.022933092 -0.023027178 -0.023632608 -0.024290379 -0.025024571 -0.025609523 -0.025440626 -0.024959411 -0.024363019 -0.02398178 -0.023823425][-0.024661366 -0.024145503 -0.023914408 -0.023425072 -0.023135141 -0.023095533 -0.023605119 -0.024246629 -0.024950545 -0.025533777 -0.025421746 -0.02488704 -0.024077225 -0.023434658 -0.023213692][-0.024808306 -0.024521902 -0.02438781 -0.023986183 -0.023650691 -0.023473486 -0.023817688 -0.024387933 -0.024976611 -0.025490783 -0.025378685 -0.024821725 -0.023916457 -0.023019113 -0.022633255][-0.025031287 -0.0249333 -0.024955798 -0.024660017 -0.02426983 -0.023979295 -0.024179388 -0.024634559 -0.025011539 -0.025422774 -0.025336038 -0.024729751 -0.023732789 -0.022647128 -0.022086836][-0.025251914 -0.025327995 -0.02552795 -0.025387522 -0.024960414 -0.024528269 -0.024524223 -0.024774224 -0.024972945 -0.025192581 -0.025118615 -0.024496913 -0.023431834 -0.022259425 -0.02161679][-0.026158854 -0.026425123 -0.026822191 -0.026835896 -0.026414994 -0.025977567 -0.025746096 -0.025721986 -0.025665987 -0.025670197 -0.025564037 -0.024960242 -0.023892328 -0.022725306 -0.022059418]]...]
INFO - root - 2017-12-06 03:20:13.461883: step 8110, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.447 sec/batch; 40h:15m:18s remains)
INFO - root - 2017-12-06 03:20:17.911841: step 8120, loss = 0.88, batch loss = 0.66 (17.8 examples/sec; 0.449 sec/batch; 40h:26m:59s remains)
INFO - root - 2017-12-06 03:20:22.427109: step 8130, loss = 0.86, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 40h:32m:09s remains)
INFO - root - 2017-12-06 03:20:26.894631: step 8140, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.436 sec/batch; 39h:17m:14s remains)
INFO - root - 2017-12-06 03:20:31.405015: step 8150, loss = 0.86, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 40h:53m:34s remains)
INFO - root - 2017-12-06 03:20:35.948535: step 8160, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 40h:40m:59s remains)
INFO - root - 2017-12-06 03:20:40.411056: step 8170, loss = 0.85, batch loss = 0.64 (18.3 examples/sec; 0.437 sec/batch; 39h:20m:52s remains)
INFO - root - 2017-12-06 03:20:44.900591: step 8180, loss = 0.84, batch loss = 0.63 (18.4 examples/sec; 0.434 sec/batch; 39h:04m:18s remains)
INFO - root - 2017-12-06 03:20:49.317474: step 8190, loss = 0.82, batch loss = 0.60 (17.9 examples/sec; 0.448 sec/batch; 40h:20m:34s remains)
INFO - root - 2017-12-06 03:20:53.904381: step 8200, loss = 0.87, batch loss = 0.66 (17.9 examples/sec; 0.447 sec/batch; 40h:15m:58s remains)
2017-12-06 03:20:54.340451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.5682472 -1.0720807 -1.4334122 -1.8813241 -2.1091456 -2.1167035 -1.9805189 -1.8620255 -1.6799515 -1.5088141 -1.3534362 -1.1422884 -0.88036966 -0.72333235 -0.60886759][-0.35990119 -0.7130388 -1.0990334 -1.5294147 -1.885246 -2.1197946 -2.1810074 -2.1999819 -2.1182797 -1.9507192 -1.7502615 -1.4737279 -1.1542989 -0.84604305 -0.5595693][0.017906919 -0.32724917 -0.67448711 -1.1282271 -1.5100724 -1.7247047 -1.7747761 -1.9340353 -1.9564681 -1.9853249 -2.0445831 -1.8962871 -1.691708 -1.394608 -1.043942][0.1340636 -0.10347889 -0.22726604 -0.44794619 -0.64847654 -0.96596408 -1.2641233 -1.380639 -1.3187295 -1.3880417 -1.4876429 -1.6690943 -1.912788 -1.9160218 -1.7959011][0.22758622 0.066150919 -0.030961886 -0.017615099 -0.063390248 -0.26294053 -0.48193836 -0.70011216 -0.79839051 -0.74360186 -0.65601522 -0.833192 -1.1319447 -1.3906953 -1.6727599][0.76845592 0.58180523 0.62318271 0.65679449 0.58679944 0.51001936 0.35356826 0.19340672 0.15213941 0.076529115 0.038783342 0.045977756 -0.073277533 -0.45223677 -0.92351085][1.0995129 1.1980486 1.1915444 1.2674295 1.3348482 1.3539368 1.2438838 1.160113 1.0163721 0.87470996 0.83462274 0.72191781 0.61877829 0.52570325 0.253277][0.81500989 1.0734473 1.300104 1.4656402 1.6393201 1.8971473 2.1001604 2.2516406 2.1582141 1.9448953 1.7783076 1.5909044 1.4805764 1.2596399 0.97379076][-0.10276707 0.14945963 0.47075349 0.842716 1.1533979 1.434642 1.733987 1.962065 2.066925 2.0757279 2.0401282 1.9163332 1.7848561 1.6080327 1.4764295][-1.7848605 -1.5086014 -1.1640229 -0.71545464 -0.2107033 0.24996983 0.58094221 0.86383313 1.0287123 1.2266778 1.4702348 1.6154952 1.6840827 1.5959704 1.4138117][-3.2386942 -3.1948521 -3.0020201 -2.6881375 -2.244915 -1.7431515 -1.3379312 -1.0169581 -0.75413233 -0.3516888 -0.018518344 0.35813552 0.6210736 0.77799219 0.92729449][-3.6333027 -3.9563277 -4.12431 -3.8398714 -3.455066 -3.1142 -2.6397605 -2.2483983 -1.9753515 -1.578683 -1.2368581 -0.86608094 -0.57132143 -0.2064871 0.060479462][-3.4153788 -3.908963 -4.172534 -4.1681123 -3.9359658 -3.5859084 -3.2015765 -2.8197172 -2.3881893 -1.9861779 -1.6629159 -1.3639537 -1.1371007 -0.82673961 -0.53175271][-2.5917363 -3.2781122 -3.6560404 -3.6609652 -3.4819245 -3.2432783 -3.05779 -2.7324154 -2.384166 -2.0575731 -1.6605088 -1.3421383 -1.1684027 -1.0170153 -0.88010919][-1.707884 -2.2845321 -2.6382694 -2.9030142 -3.0433671 -2.7584352 -2.4519932 -2.2535973 -1.9912087 -1.7124662 -1.3857056 -1.05094 -0.85008782 -0.84327626 -0.77576894]]...]
INFO - root - 2017-12-06 03:20:58.788737: step 8210, loss = 0.89, batch loss = 0.67 (17.8 examples/sec; 0.450 sec/batch; 40h:31m:58s remains)
INFO - root - 2017-12-06 03:21:03.241879: step 8220, loss = 0.87, batch loss = 0.66 (17.6 examples/sec; 0.454 sec/batch; 40h:53m:31s remains)
INFO - root - 2017-12-06 03:21:07.768700: step 8230, loss = 0.92, batch loss = 0.71 (17.4 examples/sec; 0.460 sec/batch; 41h:25m:12s remains)
INFO - root - 2017-12-06 03:21:12.252643: step 8240, loss = 0.86, batch loss = 0.65 (17.4 examples/sec; 0.459 sec/batch; 41h:22m:03s remains)
INFO - root - 2017-12-06 03:21:16.710340: step 8250, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.441 sec/batch; 39h:43m:43s remains)
INFO - root - 2017-12-06 03:21:21.225404: step 8260, loss = 0.84, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 41h:08m:17s remains)
INFO - root - 2017-12-06 03:21:25.708030: step 8270, loss = 0.87, batch loss = 0.65 (17.8 examples/sec; 0.449 sec/batch; 40h:25m:45s remains)
INFO - root - 2017-12-06 03:21:30.153983: step 8280, loss = 0.84, batch loss = 0.63 (18.5 examples/sec; 0.433 sec/batch; 38h:57m:56s remains)
INFO - root - 2017-12-06 03:21:34.695754: step 8290, loss = 0.81, batch loss = 0.60 (18.1 examples/sec; 0.443 sec/batch; 39h:51m:19s remains)
INFO - root - 2017-12-06 03:21:39.096897: step 8300, loss = 0.85, batch loss = 0.64 (18.2 examples/sec; 0.439 sec/batch; 39h:29m:47s remains)
2017-12-06 03:21:39.539328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051988885 -0.051582083 -0.051565472 -0.051564172 -0.051572762 -0.051580511 -0.05158326 -0.051577322 -0.051571712 -0.051559009 -0.051548123 -0.051546738 -0.051568642 -0.051605579 -0.051702734][-0.051822763 -0.051340766 -0.051291615 -0.051283807 -0.051298842 -0.051310383 -0.051307626 -0.051287092 -0.051269457 -0.051237054 -0.05121851 -0.05124025 -0.051313177 -0.051408209 -0.051525861][-0.051582009 -0.05101211 -0.050917249 -0.05090709 -0.0509471 -0.050975792 -0.050965548 -0.050927959 -0.050887838 -0.050822742 -0.05079785 -0.050857686 -0.051005065 -0.051178582 -0.051339924][-0.051322371 -0.050677419 -0.050549068 -0.050556265 -0.050625645 -0.050667964 -0.050641261 -0.050564539 -0.050485037 -0.050367866 -0.050326068 -0.050406933 -0.050609857 -0.050867394 -0.051106889][-0.051085129 -0.050432965 -0.050284587 -0.050310869 -0.050415963 -0.050466344 -0.050408669 -0.05027467 -0.050128266 -0.049946032 -0.049860008 -0.049932323 -0.050165959 -0.050484754 -0.050838698][-0.050926782 -0.05023285 -0.050083108 -0.050144345 -0.05030746 -0.05038248 -0.050286256 -0.050068349 -0.049839366 -0.049593568 -0.049447767 -0.049484007 -0.049718883 -0.050090693 -0.050553273][-0.050752833 -0.050083075 -0.049989335 -0.050123628 -0.050351828 -0.050434276 -0.050257891 -0.04992903 -0.04958155 -0.049257554 -0.049059156 -0.049039 -0.049274415 -0.049707655 -0.05028921][-0.050577849 -0.050042503 -0.050050057 -0.050276719 -0.050544966 -0.050593521 -0.050318986 -0.049875602 -0.049410574 -0.048989855 -0.048720494 -0.048614524 -0.048809711 -0.049276993 -0.04998133][-0.050543871 -0.050131068 -0.050211295 -0.05048053 -0.050748892 -0.050763022 -0.050438866 -0.049948268 -0.04940068 -0.048889983 -0.048523363 -0.048323773 -0.048465595 -0.048956208 -0.049753036][-0.050575398 -0.050239768 -0.050330114 -0.050584748 -0.050845977 -0.050896127 -0.050615042 -0.050159011 -0.04960186 -0.049030334 -0.04857108 -0.048288543 -0.048382655 -0.048857916 -0.0496779][-0.050650347 -0.050402455 -0.050502658 -0.050740913 -0.050985217 -0.051057488 -0.050803203 -0.050369211 -0.049821176 -0.049228802 -0.048738547 -0.048463292 -0.04854124 -0.048981119 -0.049763978][-0.050652616 -0.050525814 -0.050621092 -0.050810367 -0.050979353 -0.051027194 -0.050799735 -0.050407253 -0.049911737 -0.049371839 -0.048937641 -0.048717886 -0.048817672 -0.049240619 -0.049968474][-0.05068931 -0.05059129 -0.05062788 -0.050721232 -0.050802868 -0.050832003 -0.050677985 -0.050393596 -0.050023593 -0.049605794 -0.04927592 -0.049118921 -0.049229205 -0.049599171 -0.050240822][-0.050887354 -0.050751314 -0.050718892 -0.050721653 -0.050751135 -0.050790422 -0.050724015 -0.050558493 -0.050316948 -0.050047718 -0.049826123 -0.049734235 -0.049824588 -0.050105639 -0.050610997][-0.051383451 -0.051252015 -0.051194154 -0.051153749 -0.051143959 -0.051167712 -0.05115385 -0.051071145 -0.050934806 -0.050762407 -0.050618276 -0.050549455 -0.050584942 -0.05076078 -0.051109336]]...]
INFO - root - 2017-12-06 03:21:44.035728: step 8310, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.437 sec/batch; 39h:23m:29s remains)
INFO - root - 2017-12-06 03:21:48.408028: step 8320, loss = 0.87, batch loss = 0.65 (17.9 examples/sec; 0.447 sec/batch; 40h:15m:35s remains)
INFO - root - 2017-12-06 03:21:52.928965: step 8330, loss = 0.84, batch loss = 0.63 (17.2 examples/sec; 0.466 sec/batch; 41h:56m:54s remains)
INFO - root - 2017-12-06 03:21:57.473124: step 8340, loss = 0.88, batch loss = 0.67 (17.0 examples/sec; 0.470 sec/batch; 42h:21m:39s remains)
INFO - root - 2017-12-06 03:22:01.940356: step 8350, loss = 0.84, batch loss = 0.63 (18.1 examples/sec; 0.442 sec/batch; 39h:47m:34s remains)
INFO - root - 2017-12-06 03:22:06.480696: step 8360, loss = 0.85, batch loss = 0.64 (17.4 examples/sec; 0.460 sec/batch; 41h:27m:41s remains)
INFO - root - 2017-12-06 03:22:11.002147: step 8370, loss = 0.83, batch loss = 0.62 (18.3 examples/sec; 0.437 sec/batch; 39h:19m:51s remains)
INFO - root - 2017-12-06 03:22:15.483133: step 8380, loss = 0.79, batch loss = 0.58 (17.7 examples/sec; 0.451 sec/batch; 40h:36m:38s remains)
INFO - root - 2017-12-06 03:22:20.091197: step 8390, loss = 0.86, batch loss = 0.65 (17.1 examples/sec; 0.468 sec/batch; 42h:10m:10s remains)
INFO - root - 2017-12-06 03:22:24.564888: step 8400, loss = 0.85, batch loss = 0.63 (18.0 examples/sec; 0.443 sec/batch; 39h:54m:34s remains)
2017-12-06 03:22:25.032361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10247031 -0.099572904 -0.075492062 -0.072598547 -0.067059755 -0.047586046 -0.032076493 -0.032187723 -0.037945189 -0.024523538 -0.032840114 -0.05852177 -0.06638407 -0.088143677 -0.11997929][-0.1631273 -0.15173657 -0.13777569 -0.15387669 -0.1646547 -0.15781268 -0.13362294 -0.13260546 -0.13140181 -0.12601244 -0.12701078 -0.15439022 -0.17082429 -0.18735641 -0.20632735][-0.17429936 -0.16300637 -0.15898612 -0.17383952 -0.19308916 -0.20772213 -0.20895663 -0.20507401 -0.20038071 -0.2058433 -0.19718835 -0.19723062 -0.19868833 -0.20946714 -0.21110049][-0.14926344 -0.1367792 -0.13482201 -0.15639184 -0.18586895 -0.20725259 -0.21455172 -0.22020307 -0.21726361 -0.22147614 -0.20599237 -0.19363284 -0.18603462 -0.17765656 -0.17007424][-0.086314514 -0.081739917 -0.082730867 -0.10056242 -0.12324371 -0.13790837 -0.15120506 -0.15928471 -0.16206673 -0.17420724 -0.16535085 -0.15519643 -0.15204826 -0.15378422 -0.15321653][-0.020948581 -0.017142922 -0.018400926 -0.018835496 -0.030650504 -0.043347187 -0.054838412 -0.064447507 -0.077665038 -0.095288083 -0.095229954 -0.091642544 -0.090428054 -0.10588492 -0.11349936][0.025591895 0.042585112 0.054134123 0.067949407 0.073528893 0.069299139 0.059139602 0.043741465 0.024439812 0.010217078 0.0022234395 -0.012065571 -0.026096083 -0.048814338 -0.068319149][0.057051413 0.083685 0.10674297 0.12842155 0.14143899 0.14170936 0.13437536 0.12166125 0.10378086 0.085868381 0.0764726 0.057017229 0.034066722 0.013946742 -0.012956992][0.041801803 0.082119875 0.10307626 0.1195252 0.13685125 0.14524773 0.1408672 0.12908313 0.11639439 0.10966217 0.10066866 0.085877053 0.065024011 0.046926066 0.025965154][0.018001974 0.057969369 0.081093825 0.086348094 0.091982447 0.10126814 0.09909334 0.08666084 0.074741445 0.070049949 0.056866251 0.04727497 0.030464299 0.021351852 0.01433938][-0.019831855 0.01254075 0.029158741 0.035646163 0.041400567 0.04012911 0.038046822 0.030841529 0.021785513 0.017668493 0.007701084 0.0062377229 2.7999282e-05 -0.0029917359 0.0017825663][-0.036914147 -0.016047083 -0.0010996982 0.0056660697 0.010787547 0.0030857623 -0.0060770065 -0.016746849 -0.024839554 -0.028553955 -0.036135424 -0.034355342 -0.034246463 -0.031816334 -0.024419002][-0.059572816 -0.037634693 -0.023152504 -0.019044839 -0.016013712 -0.020058535 -0.024619587 -0.034015559 -0.038057767 -0.042349812 -0.04697486 -0.046323642 -0.046246178 -0.04263854 -0.048662536][-0.074918143 -0.050511368 -0.035608914 -0.025750563 -0.018589351 -0.016956162 -0.01851416 -0.019852377 -0.020027414 -0.022235338 -0.024654955 -0.027720939 -0.030816484 -0.030482531 -0.040587392][-0.0825412 -0.065317295 -0.050541878 -0.046390187 -0.042930052 -0.041338209 -0.042027157 -0.043991581 -0.043125682 -0.038767792 -0.034989946 -0.04093875 -0.042018965 -0.043587077 -0.04760858]]...]
INFO - root - 2017-12-06 03:22:29.477182: step 8410, loss = 0.97, batch loss = 0.75 (18.4 examples/sec; 0.436 sec/batch; 39h:14m:47s remains)
INFO - root - 2017-12-06 03:22:33.863772: step 8420, loss = 0.88, batch loss = 0.67 (17.6 examples/sec; 0.455 sec/batch; 40h:56m:54s remains)
INFO - root - 2017-12-06 03:22:38.394288: step 8430, loss = 0.83, batch loss = 0.61 (16.7 examples/sec; 0.479 sec/batch; 43h:06m:18s remains)
INFO - root - 2017-12-06 03:22:42.906881: step 8440, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.446 sec/batch; 40h:09m:21s remains)
INFO - root - 2017-12-06 03:22:47.423299: step 8450, loss = 0.78, batch loss = 0.56 (17.7 examples/sec; 0.452 sec/batch; 40h:42m:09s remains)
INFO - root - 2017-12-06 03:22:52.015779: step 8460, loss = 0.86, batch loss = 0.65 (17.2 examples/sec; 0.464 sec/batch; 41h:44m:41s remains)
INFO - root - 2017-12-06 03:22:56.530195: step 8470, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 40h:00m:40s remains)
INFO - root - 2017-12-06 03:23:00.983434: step 8480, loss = 0.85, batch loss = 0.64 (18.0 examples/sec; 0.445 sec/batch; 40h:04m:58s remains)
INFO - root - 2017-12-06 03:23:05.564209: step 8490, loss = 0.86, batch loss = 0.65 (18.1 examples/sec; 0.442 sec/batch; 39h:47m:33s remains)
INFO - root - 2017-12-06 03:23:10.043464: step 8500, loss = 0.87, batch loss = 0.66 (17.8 examples/sec; 0.448 sec/batch; 40h:20m:51s remains)
2017-12-06 03:23:10.505373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065116093 -0.0655308 -0.06570746 -0.065592989 -0.065202393 -0.064621404 -0.063964657 -0.063397087 -0.063006379 -0.062866531 -0.062983304 -0.0633126 -0.063787363 -0.064298317 -0.0647781][-0.063956439 -0.0645338 -0.064823173 -0.0647252 -0.064244285 -0.063483968 -0.062602237 -0.061797522 -0.061225571 -0.06099448 -0.061124787 -0.061575562 -0.06224101 -0.062972374 -0.063661963][-0.063385852 -0.064145729 -0.064618737 -0.06466011 -0.064253345 -0.063476123 -0.062488206 -0.061521493 -0.060770281 -0.060381442 -0.060430095 -0.060897037 -0.061673462 -0.062585741 -0.06345579][-0.063155137 -0.064097509 -0.064782336 -0.065043658 -0.064824946 -0.064153545 -0.063168928 -0.062100768 -0.06117548 -0.060586579 -0.0604741 -0.060887296 -0.061715417 -0.062744766 -0.063735306][-0.063137218 -0.064180784 -0.065002747 -0.065445043 -0.065445982 -0.064977057 -0.0641069 -0.063033357 -0.06199643 -0.061235771 -0.060949396 -0.061243311 -0.062032748 -0.063079163 -0.064114384][-0.063495427 -0.064517371 -0.0653436 -0.0658376 -0.0659768 -0.065704986 -0.065026388 -0.064059384 -0.063015327 -0.062172525 -0.061762381 -0.061926275 -0.0626031 -0.063562006 -0.064541385][-0.064270169 -0.065221131 -0.065966412 -0.066384 -0.06651663 -0.066335335 -0.065803126 -0.064964734 -0.064001821 -0.0631889 -0.062756732 -0.062840089 -0.063397557 -0.064214945 -0.065073125][-0.065487087 -0.0663461 -0.066987462 -0.067276053 -0.06730219 -0.067072906 -0.06659171 -0.06585899 -0.065022826 -0.064313665 -0.063940182 -0.064005107 -0.064439952 -0.065082066 -0.065764077][-0.066838242 -0.067606337 -0.068136737 -0.068316631 -0.068256751 -0.067995884 -0.067564964 -0.066966712 -0.066303343 -0.0657561 -0.0654636 -0.065497316 -0.065791428 -0.066239238 -0.066733286][-0.068010882 -0.068637818 -0.069024377 -0.069077067 -0.068910256 -0.068613574 -0.068231449 -0.067782819 -0.06732256 -0.066984579 -0.066836014 -0.066899821 -0.067111552 -0.067407735 -0.067737631][-0.068906605 -0.069306895 -0.06952291 -0.069477461 -0.06926062 -0.068931393 -0.068565235 -0.06819991 -0.067869231 -0.067648418 -0.067575783 -0.067647882 -0.06780681 -0.068012029 -0.068240546][-0.069364294 -0.069565088 -0.069613449 -0.0694801 -0.069225676 -0.068891168 -0.068539739 -0.06822639 -0.067964137 -0.067787543 -0.067718707 -0.067749888 -0.067839772 -0.067971677 -0.068142228][-0.069400147 -0.06948714 -0.069424465 -0.069214217 -0.068903714 -0.068538487 -0.068196632 -0.067899466 -0.067663953 -0.067498155 -0.06741941 -0.0674192 -0.067464575 -0.067547277 -0.067675546][-0.0692533 -0.069289878 -0.069193311 -0.068966791 -0.068627588 -0.068238728 -0.067867123 -0.067549683 -0.067308627 -0.067145266 -0.067060925 -0.067034878 -0.067041405 -0.067082144 -0.067171693][-0.069055468 -0.06909921 -0.069029488 -0.068827741 -0.068503767 -0.068120308 -0.067750953 -0.067443937 -0.067210913 -0.067047067 -0.066948906 -0.06689959 -0.066873595 -0.066871427 -0.066913791]]...]
INFO - root - 2017-12-06 03:23:14.963391: step 8510, loss = 0.85, batch loss = 0.64 (17.6 examples/sec; 0.456 sec/batch; 41h:00m:53s remains)
INFO - root - 2017-12-06 03:23:19.220358: step 8520, loss = 0.84, batch loss = 0.63 (23.8 examples/sec; 0.336 sec/batch; 30h:15m:09s remains)
INFO - root - 2017-12-06 03:23:23.752290: step 8530, loss = 0.89, batch loss = 0.67 (17.2 examples/sec; 0.464 sec/batch; 41h:44m:55s remains)
INFO - root - 2017-12-06 03:23:28.245295: step 8540, loss = 0.90, batch loss = 0.68 (18.1 examples/sec; 0.442 sec/batch; 39h:44m:28s remains)
INFO - root - 2017-12-06 03:23:32.709128: step 8550, loss = 0.95, batch loss = 0.74 (17.8 examples/sec; 0.451 sec/batch; 40h:32m:26s remains)
INFO - root - 2017-12-06 03:23:37.274054: step 8560, loss = 0.93, batch loss = 0.72 (17.7 examples/sec; 0.453 sec/batch; 40h:44m:45s remains)
INFO - root - 2017-12-06 03:23:41.817299: step 8570, loss = 0.89, batch loss = 0.67 (17.4 examples/sec; 0.460 sec/batch; 41h:21m:47s remains)
INFO - root - 2017-12-06 03:23:46.299361: step 8580, loss = 0.89, batch loss = 0.68 (16.9 examples/sec; 0.473 sec/batch; 42h:34m:52s remains)
INFO - root - 2017-12-06 03:23:50.778643: step 8590, loss = 0.86, batch loss = 0.64 (18.1 examples/sec; 0.441 sec/batch; 39h:42m:49s remains)
INFO - root - 2017-12-06 03:23:55.292563: step 8600, loss = 0.86, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 40h:29m:31s remains)
2017-12-06 03:23:55.729591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061087687 -0.061597068 -0.061173409 -0.060905561 -0.062413268 -0.060514636 -0.059940182 -0.059482504 -0.059366703 -0.059270009 -0.059229337 -0.059154589 -0.05916892 -0.059225988 -0.059386797][-0.062475152 -0.065377288 -0.066646069 -0.067002192 -0.066625789 -0.065147206 -0.066596791 -0.06177187 -0.059601866 -0.059018478 -0.058890395 -0.058687337 -0.058632512 -0.058737919 -0.059012696][-0.061371297 -0.067068011 -0.072264515 -0.0739244 -0.0742521 -0.0727484 -0.072250843 -0.068501294 -0.062588282 -0.059544377 -0.05903941 -0.058822412 -0.058612749 -0.058550768 -0.058965877][-0.053794734 -0.05879093 -0.067047536 -0.072678216 -0.075275034 -0.0764177 -0.076276153 -0.072680131 -0.06555526 -0.061097529 -0.060566135 -0.060091227 -0.059800904 -0.059343688 -0.059469365][-0.024066798 -0.022669598 -0.029228251 -0.039126758 -0.049326055 -0.059700731 -0.065513074 -0.066144831 -0.062632337 -0.061014168 -0.061847247 -0.062346175 -0.062219553 -0.061515439 -0.060992803][0.025313407 0.035202183 0.032795832 0.017507084 -0.0022644103 -0.025264416 -0.04334921 -0.054931387 -0.056847818 -0.058800496 -0.061158765 -0.063365966 -0.06344451 -0.06258247 -0.063110843][0.0644851 0.0829162 0.083747253 0.065926045 0.040447012 0.0088193044 -0.019253295 -0.039307903 -0.048823223 -0.054258641 -0.059568394 -0.06308686 -0.064344019 -0.063532308 -0.0629306][0.0802467 0.10026076 0.10446279 0.087169886 0.060423732 0.026603594 -0.0040248632 -0.027203623 -0.041000348 -0.050599 -0.056477733 -0.058987588 -0.06075649 -0.061621819 -0.061193213][0.084423423 0.10230643 0.1080267 0.092995629 0.069137305 0.033243902 0.0009797141 -0.023967624 -0.038867388 -0.049094245 -0.05569496 -0.059846744 -0.059021544 -0.059273642 -0.060570456][0.071665093 0.0901261 0.099356413 0.089672223 0.072210774 0.043691672 0.012338877 -0.018061444 -0.038069423 -0.0511333 -0.056051861 -0.058962237 -0.056596443 -0.059639469 -0.05824462][0.037032172 0.057573169 0.073543847 0.07436721 0.070397973 0.051238537 0.026721217 -0.0044465587 -0.029736243 -0.048470628 -0.056921039 -0.059469745 -0.058460038 -0.060313366 -0.057059847][0.0057228878 0.02124209 0.038082503 0.047921814 0.052825272 0.04210525 0.026746042 0.0026370883 -0.021044254 -0.042869702 -0.056650933 -0.060524985 -0.060723558 -0.058806844 -0.056435075][-0.010438941 -0.0012861416 0.011882782 0.024723306 0.033793569 0.029641703 0.019210003 0.0020062551 -0.015842564 -0.035320036 -0.051146492 -0.055203125 -0.057048962 -0.053240106 -0.049701277][-0.0029497594 -0.0044984519 0.0014259219 0.013136595 0.023903422 0.024843238 0.017125443 0.0037447587 -0.011614837 -0.028794449 -0.042721175 -0.047044784 -0.047199544 -0.043545932 -0.039505195][0.030972309 0.018972173 0.012971051 0.014834791 0.023396581 0.025303416 0.020547062 0.0070908144 -0.00736364 -0.023365296 -0.036703479 -0.041330814 -0.038407255 -0.03685727 -0.031006716]]...]
INFO - root - 2017-12-06 03:24:00.268567: step 8610, loss = 0.86, batch loss = 0.64 (17.1 examples/sec; 0.467 sec/batch; 42h:01m:59s remains)
INFO - root - 2017-12-06 03:24:04.784270: step 8620, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.444 sec/batch; 39h:59m:03s remains)
INFO - root - 2017-12-06 03:24:09.164069: step 8630, loss = 0.87, batch loss = 0.66 (18.3 examples/sec; 0.436 sec/batch; 39h:15m:50s remains)
INFO - root - 2017-12-06 03:24:13.667194: step 8640, loss = 0.80, batch loss = 0.59 (18.3 examples/sec; 0.438 sec/batch; 39h:23m:20s remains)
INFO - root - 2017-12-06 03:24:18.230327: step 8650, loss = 0.92, batch loss = 0.70 (17.4 examples/sec; 0.460 sec/batch; 41h:20m:37s remains)
INFO - root - 2017-12-06 03:24:22.748309: step 8660, loss = 0.87, batch loss = 0.66 (16.5 examples/sec; 0.484 sec/batch; 43h:33m:24s remains)
INFO - root - 2017-12-06 03:24:27.227689: step 8670, loss = 0.86, batch loss = 0.64 (18.4 examples/sec; 0.436 sec/batch; 39h:12m:54s remains)
INFO - root - 2017-12-06 03:24:31.773266: step 8680, loss = 0.86, batch loss = 0.65 (17.4 examples/sec; 0.459 sec/batch; 41h:18m:48s remains)
INFO - root - 2017-12-06 03:24:36.233576: step 8690, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:12m:46s remains)
INFO - root - 2017-12-06 03:24:40.692794: step 8700, loss = 0.85, batch loss = 0.64 (17.6 examples/sec; 0.455 sec/batch; 40h:55m:08s remains)
2017-12-06 03:24:41.169081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.37629575 -0.48344707 -0.55155003 -0.5649727 -0.55451894 -0.41384375 -0.34266692 -0.37277424 -0.51573247 -0.71027946 -0.83114129 -0.81427145 -0.70004469 -0.56065249 -0.50752127][-0.49036402 -0.71765929 -0.98498023 -1.1624495 -1.2597935 -1.1866815 -1.1365194 -1.0825497 -1.1335469 -1.2072738 -1.1909142 -1.0410595 -0.90889984 -0.77645254 -0.73949724][-0.47865933 -0.70554894 -1.0141627 -1.3242685 -1.563642 -1.6055876 -1.5976142 -1.6113133 -1.6997045 -1.6994462 -1.6229506 -1.3921006 -1.1783046 -0.95599383 -0.83527458][-0.13659421 -0.37825251 -0.72795147 -1.0580961 -1.2156116 -1.182368 -1.2445133 -1.2561597 -1.4047747 -1.577993 -1.5943621 -1.3261046 -1.0710862 -0.84866589 -0.74474716][0.31304717 0.28013682 0.12516762 -0.16591133 -0.25534415 -0.01505902 0.029982589 -0.271654 -0.57851827 -0.876264 -0.96907389 -0.83637846 -0.68025249 -0.50586182 -0.43633473][0.54958707 0.8357293 1.1194252 1.2131342 1.2427849 1.5722501 1.8877341 1.9168745 1.7705257 1.1400778 0.55165505 0.14052662 -0.04864959 -0.039365105 0.029296547][0.32976645 0.71585286 1.2011468 1.684489 2.2381451 2.826858 3.1423073 3.1789746 3.0236087 2.4127555 1.883538 1.4872688 1.0227308 0.73078018 0.45680457][0.43685943 0.7823844 1.272621 1.8221538 2.5349591 3.4775894 4.349782 4.4469266 3.9577525 3.2088091 2.5604117 1.8771712 1.384644 1.0550241 0.7667231][0.54248416 0.78565407 1.1551574 1.5962242 2.0109499 2.7628736 3.8512037 4.3988886 4.0933466 3.3640265 2.61538 1.9566236 1.4653628 1.029235 0.77029693][0.42917109 0.58936757 0.83286166 1.2414556 1.6833924 2.2083213 2.7774267 3.1365011 3.1945827 2.4248071 1.7815176 1.3283567 1.0166365 0.70316935 0.4613108][0.2333533 0.29527777 0.30922073 0.47116935 0.7455678 1.1565727 1.5975581 1.9706101 2.1599166 1.8581299 1.2681433 0.65288281 0.32361972 0.21402766 0.15734078][-0.1393408 -0.069489129 -0.09152513 -0.1619457 -0.27348232 -0.13590592 0.23783495 0.57617348 0.82329124 0.87333173 0.65435952 0.3321234 -0.032256898 -0.21868719 -0.21284121][-0.36993289 -0.47350693 -0.56408006 -0.66553468 -0.79976392 -0.85679621 -0.68968594 -0.45805597 -0.17078015 0.02804824 0.12727423 0.011253901 -0.17655429 -0.27364734 -0.29610246][-0.40290612 -0.51846552 -0.71830291 -0.99020141 -1.1881137 -1.2469516 -1.0848951 -0.84625483 -0.60429215 -0.36674428 -0.23696625 -0.082746729 -0.15250459 -0.25913441 -0.27277264][-0.3258602 -0.43943214 -0.57872248 -0.7512722 -1.0447633 -1.2160311 -1.2202702 -0.91460693 -0.54030633 -0.39099365 -0.24581851 -0.097644545 0.016887121 0.040245481 -0.057669491]]...]
INFO - root - 2017-12-06 03:24:45.626818: step 8710, loss = 0.84, batch loss = 0.63 (18.6 examples/sec; 0.430 sec/batch; 38h:41m:11s remains)
INFO - root - 2017-12-06 03:24:50.133888: step 8720, loss = 0.85, batch loss = 0.63 (17.6 examples/sec; 0.453 sec/batch; 40h:47m:13s remains)
INFO - root - 2017-12-06 03:24:54.503792: step 8730, loss = 0.83, batch loss = 0.62 (17.3 examples/sec; 0.461 sec/batch; 41h:29m:08s remains)
INFO - root - 2017-12-06 03:24:58.972792: step 8740, loss = 0.85, batch loss = 0.64 (18.2 examples/sec; 0.439 sec/batch; 39h:30m:50s remains)
INFO - root - 2017-12-06 03:25:03.469553: step 8750, loss = 0.84, batch loss = 0.62 (17.9 examples/sec; 0.448 sec/batch; 40h:15m:49s remains)
INFO - root - 2017-12-06 03:25:08.023529: step 8760, loss = 0.84, batch loss = 0.63 (17.7 examples/sec; 0.453 sec/batch; 40h:41m:54s remains)
INFO - root - 2017-12-06 03:25:12.549424: step 8770, loss = 0.84, batch loss = 0.62 (18.3 examples/sec; 0.438 sec/batch; 39h:24m:27s remains)
INFO - root - 2017-12-06 03:25:17.060698: step 8780, loss = 0.85, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 40h:25m:53s remains)
INFO - root - 2017-12-06 03:25:21.601072: step 8790, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.462 sec/batch; 41h:32m:01s remains)
INFO - root - 2017-12-06 03:25:26.126925: step 8800, loss = 0.86, batch loss = 0.65 (16.9 examples/sec; 0.473 sec/batch; 42h:29m:30s remains)
2017-12-06 03:25:26.577708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9051342 -2.5578403 -2.5193095 -2.2833197 -2.2358902 -1.9968905 -1.759342 -1.7731457 -2.0714529 -2.1984143 -2.0137718 -2.1357989 -2.1486006 -2.1415441 -2.2300773][-1.5709562 -2.2935107 -2.7034624 -2.7295015 -2.2716358 -1.8565784 -1.5396576 -1.2763531 -1.298835 -1.5323489 -1.7197769 -1.8399451 -1.9279726 -1.8776987 -1.8142514][-1.1689346 -1.8033566 -2.3478458 -2.4354961 -2.3119097 -2.016408 -1.4900193 -1.0621233 -0.91998535 -0.79151696 -0.87353843 -1.3243651 -1.614011 -1.8047938 -1.9190481][-0.53874248 -0.86144519 -1.2514012 -1.6062486 -1.8358748 -1.5970786 -1.1057024 -0.90715462 -0.64202487 -0.1100316 0.0082669109 0.010326937 -0.32761338 -0.68392909 -1.0529449][-0.18694347 -0.3975994 -0.3908062 -0.509784 -0.94411564 -1.3491926 -1.3612533 -1.0946274 -0.63548136 -0.34583777 0.074290514 0.33632609 0.25610051 0.12461683 -0.50072169][0.51105821 0.48941624 0.31150889 0.39821082 0.39010021 0.20039803 -0.47721574 -0.73220032 -0.60166276 -0.41348076 -0.026381917 0.68945342 1.2449546 1.0152504 0.74122858][1.3308802 0.90264171 0.68337822 0.53140926 0.302937 0.62748379 0.274507 -0.10494331 -0.50916278 -0.52055115 -0.26283592 0.39881912 0.9046464 1.0454221 1.3378303][1.2339385 0.94979888 0.61123043 0.77455193 0.85117829 0.85355985 0.75452226 0.44408035 0.2971555 0.054185286 -0.33024645 -0.28668505 -0.26397091 -0.065197818 0.41640088][0.59630924 0.55971813 0.5305863 0.41745806 0.24963066 -0.0084404349 -0.30379564 -0.22136885 -0.25009495 -0.055988692 0.054122224 -0.21283293 -0.56850529 -0.25956133 -0.06240809][-0.71384007 -0.42334136 -0.30752814 -0.24972627 -0.21698789 -0.60095376 -0.83090425 -0.82380658 -0.77749217 -0.71915644 -0.48305708 -0.64277124 -0.71487445 -0.76918435 -0.707875][-3.0274928 -2.7667167 -2.3088534 -2.1724868 -2.216759 -2.1027706 -2.0483592 -2.1656005 -2.0488665 -1.7186294 -1.3496804 -0.97522271 -0.69034284 -0.73171592 -0.6700393][-3.9069548 -4.0739536 -3.9928703 -3.5937412 -3.307246 -2.837791 -2.6487329 -2.5029767 -2.3572335 -2.1776469 -1.9768934 -1.6442704 -1.2257657 -0.91593009 -0.622292][-3.9848449 -4.166122 -4.2471581 -4.2329063 -3.9478204 -3.6217577 -3.214401 -2.8249707 -2.5100377 -2.0983686 -1.749841 -1.4145923 -1.1784723 -0.93707132 -0.72967118][-2.9781489 -3.4949896 -3.7903955 -3.8808513 -3.820529 -3.7336638 -3.4460435 -3.0036433 -2.8212616 -2.3958962 -1.7233443 -1.0972488 -0.71644592 -0.55995661 -0.45438933][-2.3466754 -2.7879026 -3.209842 -3.5680118 -3.6275964 -3.4328444 -3.370002 -3.0461187 -2.7654257 -2.3351729 -1.7133291 -1.0907645 -0.52549231 -0.027055144 0.042117096]]...]
INFO - root - 2017-12-06 03:25:31.143293: step 8810, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.444 sec/batch; 39h:55m:12s remains)
INFO - root - 2017-12-06 03:25:35.698540: step 8820, loss = 0.82, batch loss = 0.61 (17.3 examples/sec; 0.462 sec/batch; 41h:31m:52s remains)
INFO - root - 2017-12-06 03:25:40.147842: step 8830, loss = 0.85, batch loss = 0.64 (17.8 examples/sec; 0.448 sec/batch; 40h:18m:41s remains)
INFO - root - 2017-12-06 03:25:44.436791: step 8840, loss = 0.82, batch loss = 0.61 (18.1 examples/sec; 0.443 sec/batch; 39h:49m:27s remains)
INFO - root - 2017-12-06 03:25:48.942912: step 8850, loss = 0.85, batch loss = 0.64 (17.0 examples/sec; 0.471 sec/batch; 42h:19m:11s remains)
INFO - root - 2017-12-06 03:25:53.465127: step 8860, loss = 0.83, batch loss = 0.62 (17.6 examples/sec; 0.455 sec/batch; 40h:55m:05s remains)
INFO - root - 2017-12-06 03:25:57.968818: step 8870, loss = 0.79, batch loss = 0.58 (17.3 examples/sec; 0.463 sec/batch; 41h:35m:01s remains)
INFO - root - 2017-12-06 03:26:02.420170: step 8880, loss = 0.86, batch loss = 0.64 (17.9 examples/sec; 0.447 sec/batch; 40h:10m:25s remains)
INFO - root - 2017-12-06 03:26:06.869738: step 8890, loss = 0.89, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:09m:11s remains)
INFO - root - 2017-12-06 03:26:11.395643: step 8900, loss = 0.89, batch loss = 0.67 (17.9 examples/sec; 0.446 sec/batch; 40h:05m:06s remains)
2017-12-06 03:26:11.835161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.23413312 -0.29951873 -0.3062962 -0.35248986 -0.39131567 -0.4259713 -0.47191006 -0.52886307 -0.65090173 -0.701976 -0.67080635 -0.57822347 -0.41142049 -0.15339252 0.12209848][-0.088921234 -0.11060271 -0.093840852 -0.15213303 -0.21650362 -0.20155933 -0.17942919 -0.26252425 -0.35161284 -0.40142992 -0.3798559 -0.26302516 -0.19964114 -0.057076931 0.11069282][-0.34653237 -0.31995103 -0.27367154 -0.20892161 -0.10841757 -0.057887547 -0.016474325 0.025251411 -0.024645448 -0.1005263 -0.073502056 -0.025751058 0.04560034 0.11255836 0.18918046][-0.58702159 -0.69542372 -0.66552663 -0.49926421 -0.29842338 0.0050697625 0.193905 0.30370715 0.31398326 0.19925839 0.15705878 0.14174637 0.17687532 0.27525029 0.37916571][-0.44403669 -0.51316035 -0.53309518 -0.43717054 -0.21397614 0.08875341 0.30186832 0.47784281 0.48474324 0.47615409 0.48826945 0.48227668 0.5563187 0.64076138 0.66717732][-0.50374347 -0.48234168 -0.38910809 -0.25660491 -0.088694215 0.1746996 0.398217 0.57245439 0.62199819 0.563377 0.40522188 0.37133422 0.40128818 0.43726927 0.48522174][-0.377406 -0.41168839 -0.34825438 -0.18756726 0.031499624 0.27106589 0.46753043 0.6656574 0.64784724 0.54148334 0.40316898 0.28502262 0.2030077 0.23029733 0.32511058][0.046330675 -0.094380967 -0.095238224 0.095597066 0.36110619 0.62100393 0.78405619 0.8112371 0.73422736 0.64895892 0.52766407 0.41849428 0.37064433 0.41590288 0.58153677][-0.086673692 -0.2832911 -0.35975912 -0.19854784 0.1018092 0.43175811 0.66073149 0.66563129 0.46859014 0.30488271 0.23252353 0.2926884 0.41277236 0.54706854 0.68319684][-0.21881878 -0.52013618 -0.75778151 -0.76448691 -0.53028589 -0.11451091 0.27950278 0.46148533 0.35894457 0.13544744 -0.047873911 -0.074198529 0.083741568 0.30244413 0.52540159][-0.21277833 -0.4665702 -0.76387209 -0.95557719 -0.887946 -0.58323795 -0.19815642 0.15257886 0.25786668 0.12779704 -0.13435408 -0.272333 -0.22081289 -0.12059087 0.094880305][-0.19943547 -0.37894735 -0.60872948 -0.84948635 -0.96943974 -0.85182637 -0.56412977 -0.19765946 0.041557573 0.060577102 -0.12224473 -0.34062976 -0.41917875 -0.36494809 -0.25505489][-0.16629678 -0.25094786 -0.3789956 -0.51075131 -0.638317 -0.60224879 -0.44261897 -0.17713904 0.041872032 0.056547411 -0.020796251 -0.19700214 -0.35171628 -0.350834 -0.24872836][-0.12898982 -0.18497223 -0.24809617 -0.31342673 -0.34645271 -0.20512441 -0.12710826 0.034086213 0.18139154 0.19479519 0.19413355 0.0049772188 -0.1350348 -0.19289714 -0.1506635][-0.098656669 -0.13214928 -0.18257211 -0.151777 -0.22637936 -0.25105613 -0.10546337 0.11185143 0.2146765 0.24692273 0.22002432 0.060318105 -0.041854318 -0.10966209 -0.090773672]]...]
INFO - root - 2017-12-06 03:26:16.368552: step 8910, loss = 0.88, batch loss = 0.66 (18.1 examples/sec; 0.442 sec/batch; 39h:44m:20s remains)
INFO - root - 2017-12-06 03:26:20.859144: step 8920, loss = 0.85, batch loss = 0.63 (17.0 examples/sec; 0.469 sec/batch; 42h:11m:33s remains)
INFO - root - 2017-12-06 03:26:25.347825: step 8930, loss = 0.86, batch loss = 0.64 (18.3 examples/sec; 0.438 sec/batch; 39h:21m:37s remains)
INFO - root - 2017-12-06 03:26:29.651899: step 8940, loss = 0.90, batch loss = 0.69 (16.9 examples/sec; 0.472 sec/batch; 42h:25m:48s remains)
INFO - root - 2017-12-06 03:26:34.136585: step 8950, loss = 0.87, batch loss = 0.65 (18.1 examples/sec; 0.441 sec/batch; 39h:39m:49s remains)
INFO - root - 2017-12-06 03:26:38.690859: step 8960, loss = 0.91, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:15m:50s remains)
INFO - root - 2017-12-06 03:26:43.206886: step 8970, loss = 0.80, batch loss = 0.59 (17.5 examples/sec; 0.458 sec/batch; 41h:09m:15s remains)
INFO - root - 2017-12-06 03:26:47.699394: step 8980, loss = 0.83, batch loss = 0.62 (18.4 examples/sec; 0.436 sec/batch; 39h:09m:45s remains)
INFO - root - 2017-12-06 03:26:52.267894: step 8990, loss = 0.85, batch loss = 0.63 (18.4 examples/sec; 0.436 sec/batch; 39h:09m:16s remains)
INFO - root - 2017-12-06 03:26:56.759888: step 9000, loss = 0.82, batch loss = 0.61 (18.1 examples/sec; 0.442 sec/batch; 39h:45m:06s remains)
2017-12-06 03:26:57.234399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2298308 -0.30723408 -0.39205834 -0.46592006 -0.48285386 -0.49842232 -0.50236279 -0.48946744 -0.457359 -0.40046293 -0.33786067 -0.27349293 -0.21568242 -0.14900182 -0.086810157][-0.30080703 -0.45299813 -0.595369 -0.73154229 -0.79390681 -0.81934071 -0.83482689 -0.83861971 -0.82295686 -0.77906489 -0.71453792 -0.6368494 -0.5530718 -0.45297953 -0.34182194][-0.23544502 -0.39565176 -0.58985943 -0.72418821 -0.8114062 -0.87720066 -0.9317885 -0.96435964 -0.9960019 -1.0223472 -1.0235331 -0.99439 -0.94632113 -0.871123 -0.76889145][-0.21354681 -0.34095287 -0.47716963 -0.55574936 -0.61612749 -0.62983304 -0.65792537 -0.72462583 -0.8143006 -0.9150064 -0.99726862 -1.0467218 -1.0590326 -1.0440959 -1.0093428][-0.099606939 -0.2295669 -0.33428243 -0.31910592 -0.27809665 -0.22648314 -0.19389209 -0.20108631 -0.28311297 -0.44638205 -0.60686713 -0.73015535 -0.79620063 -0.82762235 -0.83514023][0.3879945 0.26350436 0.15212864 0.18695757 0.26745251 0.40407631 0.51961941 0.52808911 0.41191095 0.17181608 -0.063861951 -0.24201795 -0.34936002 -0.40267217 -0.41115454][0.74017626 0.7324059 0.73548067 0.80032384 0.87774438 1.0209763 1.1661106 1.222173 1.1203861 0.83523268 0.52343249 0.29484332 0.15559676 0.050270163 -0.019283038][0.66562212 0.74459386 0.85573018 1.0441183 1.262349 1.4972095 1.7118729 1.8045135 1.7162799 1.4444187 1.1195925 0.82485646 0.60496908 0.4309577 0.29558167][0.60373825 0.6827513 0.8481909 1.0840225 1.361971 1.6613384 1.9135026 2.0027297 1.9589674 1.7482086 1.4435831 1.1438422 0.9192034 0.75025475 0.63062745][0.79155833 0.79911524 0.86729133 1.0394323 1.2423561 1.482198 1.7171053 1.8117921 1.7601969 1.5512564 1.2769823 1.0490272 0.88139379 0.79181653 0.7542637][0.75864047 0.73726505 0.68067586 0.71597707 0.8183409 1.0396993 1.264905 1.4043338 1.4486322 1.3277844 1.1052078 0.89617991 0.73053253 0.65657675 0.63273048][0.48692575 0.41293135 0.33534873 0.32911098 0.35846686 0.48828033 0.63899475 0.8189491 0.97230381 1.0214485 0.9743436 0.82095611 0.63974655 0.52084333 0.4478046][0.29491326 0.18787891 0.10941004 0.069297962 0.059961475 0.0892384 0.11410499 0.17917031 0.21875212 0.30823886 0.41228235 0.43484631 0.42467132 0.37379548 0.30010307][0.27195129 0.12837455 -0.087577805 -0.18664676 -0.27308807 -0.26160878 -0.25617498 -0.2396569 -0.24424246 -0.23298189 -0.23659855 -0.18097495 -0.081472933 -0.053607251 -0.025107488][0.19133705 0.065504216 -0.091604471 -0.20718464 -0.36619693 -0.43909431 -0.47470716 -0.447243 -0.41866365 -0.40552706 -0.44474757 -0.49968967 -0.56443119 -0.58977056 -0.55749553]]...]
INFO - root - 2017-12-06 03:27:01.721428: step 9010, loss = 0.84, batch loss = 0.63 (18.7 examples/sec; 0.427 sec/batch; 38h:22m:44s remains)
INFO - root - 2017-12-06 03:27:06.258229: step 9020, loss = 0.78, batch loss = 0.57 (17.6 examples/sec; 0.455 sec/batch; 40h:51m:44s remains)
INFO - root - 2017-12-06 03:27:10.835658: step 9030, loss = 0.84, batch loss = 0.63 (17.7 examples/sec; 0.451 sec/batch; 40h:30m:53s remains)
INFO - root - 2017-12-06 03:27:15.286342: step 9040, loss = 0.82, batch loss = 0.61 (18.2 examples/sec; 0.439 sec/batch; 39h:28m:52s remains)
INFO - root - 2017-12-06 03:27:19.562464: step 9050, loss = 0.89, batch loss = 0.68 (17.6 examples/sec; 0.456 sec/batch; 40h:56m:43s remains)
INFO - root - 2017-12-06 03:27:24.022143: step 9060, loss = 0.87, batch loss = 0.65 (17.8 examples/sec; 0.449 sec/batch; 40h:21m:24s remains)
INFO - root - 2017-12-06 03:27:28.587332: step 9070, loss = 0.83, batch loss = 0.62 (18.1 examples/sec; 0.441 sec/batch; 39h:39m:27s remains)
INFO - root - 2017-12-06 03:27:33.052041: step 9080, loss = 0.83, batch loss = 0.62 (18.0 examples/sec; 0.445 sec/batch; 40h:00m:07s remains)
INFO - root - 2017-12-06 03:27:37.635695: step 9090, loss = 0.82, batch loss = 0.60 (16.5 examples/sec; 0.486 sec/batch; 43h:39m:02s remains)
INFO - root - 2017-12-06 03:27:42.162102: step 9100, loss = 0.85, batch loss = 0.64 (18.0 examples/sec; 0.444 sec/batch; 39h:51m:12s remains)
2017-12-06 03:27:42.619809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.031511374 -0.030625865 -0.031013109 -0.031242527 -0.031242885 -0.031206243 -0.031476796 -0.03245857 -0.03432484 -0.036798254 -0.039538305 -0.041966368 -0.04354405 -0.043872945 -0.043281294][-0.030527517 -0.029514235 -0.029884815 -0.030142039 -0.030206524 -0.030242383 -0.030567646 -0.031679522 -0.033720218 -0.036364105 -0.039393391 -0.042306814 -0.04447753 -0.045403682 -0.045094911][-0.029979825 -0.02884981 -0.029113047 -0.029380765 -0.029540319 -0.029586513 -0.029845737 -0.031005025 -0.033129245 -0.035854489 -0.038985729 -0.042177226 -0.044790484 -0.046207208 -0.046209507][-0.029802125 -0.028533593 -0.028631426 -0.028829671 -0.02904468 -0.029076103 -0.029138874 -0.030247491 -0.032368649 -0.035068776 -0.038165916 -0.041323766 -0.044061959 -0.045821063 -0.046182841][-0.030202046 -0.028795805 -0.02864524 -0.028674763 -0.028880011 -0.028922051 -0.028733782 -0.029662505 -0.031590823 -0.034008443 -0.03672839 -0.039367847 -0.041859373 -0.04376667 -0.044493962][-0.03092213 -0.029385205 -0.029049374 -0.028919954 -0.028975658 -0.029036738 -0.028624304 -0.029251669 -0.030863211 -0.032746911 -0.034844179 -0.036872458 -0.038827911 -0.040625792 -0.041683365][-0.032045543 -0.030564737 -0.030090485 -0.029750057 -0.029610336 -0.029752932 -0.029161911 -0.029430304 -0.030664958 -0.031927489 -0.033360574 -0.034773123 -0.036111988 -0.037575971 -0.038736131][-0.033584278 -0.03223753 -0.031589542 -0.030992396 -0.030722592 -0.030971341 -0.030534171 -0.030526288 -0.031345811 -0.032100216 -0.032926619 -0.033848271 -0.034834996 -0.036018446 -0.037197411][-0.03606974 -0.034514342 -0.03350269 -0.032506991 -0.031937514 -0.032092631 -0.031932987 -0.031902391 -0.032472119 -0.032919876 -0.033242911 -0.03373966 -0.0344223 -0.035324093 -0.036389317][-0.039348569 -0.037332181 -0.03575829 -0.034212261 -0.033276837 -0.033175357 -0.033185132 -0.033278607 -0.033754289 -0.034100544 -0.034262396 -0.034541354 -0.034986265 -0.035567205 -0.036312483][-0.043200035 -0.040468555 -0.038367856 -0.036282085 -0.034975886 -0.034532931 -0.034591198 -0.034806255 -0.035182148 -0.035478666 -0.035577726 -0.035744134 -0.0360531 -0.036437247 -0.036894545][-0.047637902 -0.044171456 -0.041440126 -0.038748045 -0.036909398 -0.036145076 -0.036202285 -0.036583412 -0.036996562 -0.037230421 -0.037264828 -0.037319485 -0.0374834 -0.037721012 -0.037926413][-0.052544981 -0.048144732 -0.044634391 -0.041406129 -0.039029125 -0.037949488 -0.037990931 -0.038518567 -0.039042853 -0.039230239 -0.039205361 -0.039172582 -0.039194223 -0.039243016 -0.039248295][-0.057611298 -0.052378409 -0.047989324 -0.044209648 -0.041320346 -0.039917629 -0.039878767 -0.040522017 -0.041122161 -0.041264676 -0.041128911 -0.040950142 -0.040731579 -0.040533192 -0.040377054][-0.062076274 -0.056507997 -0.051681645 -0.047578491 -0.044406261 -0.042772543 -0.042575624 -0.043077115 -0.043546624 -0.043559469 -0.043246448 -0.042915072 -0.042505238 -0.04217314 -0.041943207]]...]
INFO - root - 2017-12-06 03:27:47.121291: step 9110, loss = 0.90, batch loss = 0.69 (17.7 examples/sec; 0.453 sec/batch; 40h:40m:28s remains)
INFO - root - 2017-12-06 03:27:51.625386: step 9120, loss = 0.85, batch loss = 0.64 (17.6 examples/sec; 0.453 sec/batch; 40h:43m:03s remains)
INFO - root - 2017-12-06 03:27:56.208941: step 9130, loss = 0.82, batch loss = 0.61 (17.4 examples/sec; 0.460 sec/batch; 41h:17m:02s remains)
INFO - root - 2017-12-06 03:28:00.664327: step 9140, loss = 0.85, batch loss = 0.64 (18.4 examples/sec; 0.434 sec/batch; 38h:58m:49s remains)
INFO - root - 2017-12-06 03:28:05.053958: step 9150, loss = 0.83, batch loss = 0.62 (17.4 examples/sec; 0.460 sec/batch; 41h:19m:46s remains)
INFO - root - 2017-12-06 03:28:09.577586: step 9160, loss = 0.84, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 41h:00m:40s remains)
INFO - root - 2017-12-06 03:28:14.138498: step 9170, loss = 0.87, batch loss = 0.66 (18.4 examples/sec; 0.436 sec/batch; 39h:07m:08s remains)
INFO - root - 2017-12-06 03:28:18.671879: step 9180, loss = 0.91, batch loss = 0.70 (17.6 examples/sec; 0.454 sec/batch; 40h:44m:20s remains)
INFO - root - 2017-12-06 03:28:23.192711: step 9190, loss = 0.93, batch loss = 0.72 (18.0 examples/sec; 0.444 sec/batch; 39h:50m:11s remains)
INFO - root - 2017-12-06 03:28:27.715733: step 9200, loss = 0.89, batch loss = 0.67 (17.8 examples/sec; 0.448 sec/batch; 40h:15m:45s remains)
2017-12-06 03:28:28.162675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7289712 -4.1423335 -4.2602468 -3.9060719 -3.7329364 -2.8299694 -1.9602264 -1.5966276 -1.3174739 -1.158542 -1.2209781 -1.0320711 -0.7499631 -0.30003479 -0.018498637][-2.5306668 -3.3343852 -3.6995385 -3.8499272 -3.5638151 -3.1973202 -3.0429802 -2.6572506 -1.9114716 -1.5443873 -1.2661103 -0.896983 -0.70791054 -0.35026672 0.11760702][-0.23761791 -1.1691465 -2.5706561 -3.0775824 -3.284672 -3.2447662 -3.2156947 -3.0878763 -2.8653126 -2.4388416 -2.0042732 -1.7464246 -1.639797 -1.3806102 -0.94110245][1.0438144 0.50640863 -0.33744109 -1.4095575 -2.3039188 -2.6400752 -2.7070732 -2.7189353 -2.5310664 -2.5682914 -2.6889915 -2.587296 -2.5270357 -2.3627832 -1.9537245][2.4098876 1.9369612 1.0627854 0.063693963 -0.71701837 -1.7327278 -2.3878846 -2.1776271 -1.9216253 -1.857848 -1.6992723 -1.8562984 -2.2921405 -2.5560877 -2.4140263][3.3852863 2.9125223 2.1059606 1.0953203 0.23573342 -0.55592227 -0.96790737 -1.2630484 -1.3079917 -1.2264093 -0.98718363 -0.9945612 -0.89859778 -1.1868339 -1.4883049][3.2566097 3.0075996 2.825249 2.2419617 1.4684005 0.89366543 0.66385311 0.65918928 0.68132949 0.42384541 0.17247745 -0.064481974 -0.23739889 -0.23329705 -0.051538736][1.6303748 2.2558064 2.4288926 2.5796881 2.5752773 2.2953029 2.0068543 1.8365029 1.8569096 1.4102312 1.1681267 0.9453488 0.72412914 0.46837932 0.11782833][0.061599143 0.66571349 1.777771 2.3230219 2.9343419 3.1947892 2.5462067 1.8165474 1.3640071 1.1342396 1.1850326 1.1085092 1.3488576 1.6410631 1.5293449][-1.1001868 -0.68472093 -0.14494619 0.71425265 1.7726938 2.3813007 2.1725605 1.6982563 0.95999616 0.27921894 0.036670387 0.2820701 1.0560849 1.4387232 1.8221753][-2.6552711 -1.9674306 -1.1947827 -0.67791885 -0.11294504 0.66006428 0.71203333 0.36217204 -0.056741253 -0.55296576 -0.7831614 -0.70793808 -0.28074905 0.55592537 1.2223494][-2.5894656 -2.5169187 -1.9903773 -1.3875396 -0.86319327 -0.66197914 -0.80080765 -0.99557525 -1.2739246 -1.4715915 -1.53977 -1.3838977 -1.0790879 -0.46132195 0.25721568][-2.8327804 -2.4812109 -2.2034457 -1.8778888 -1.5734324 -1.6281441 -1.7845588 -1.9900838 -2.142617 -2.0614319 -2.0883794 -2.1203198 -1.897701 -1.3148915 -0.77383453][-3.275106 -3.1311646 -2.6824934 -2.2487035 -2.0301292 -1.9077263 -2.0615411 -2.1463451 -1.9402201 -1.8145143 -1.9329374 -2.0278523 -1.9399103 -1.7213247 -1.6180142][-3.2945375 -3.2296379 -2.9786832 -2.6721137 -2.6681254 -2.3063066 -1.9763635 -1.7752366 -1.7342192 -1.5807782 -1.5015467 -1.5684336 -1.4725528 -1.2313403 -1.0545651]]...]
INFO - root - 2017-12-06 03:28:32.632465: step 9210, loss = 0.80, batch loss = 0.59 (17.9 examples/sec; 0.447 sec/batch; 40h:06m:37s remains)
INFO - root - 2017-12-06 03:28:37.166293: step 9220, loss = 0.88, batch loss = 0.67 (18.0 examples/sec; 0.444 sec/batch; 39h:51m:09s remains)
INFO - root - 2017-12-06 03:28:41.709221: step 9230, loss = 0.89, batch loss = 0.67 (17.8 examples/sec; 0.449 sec/batch; 40h:19m:05s remains)
INFO - root - 2017-12-06 03:28:46.246019: step 9240, loss = 0.89, batch loss = 0.68 (17.1 examples/sec; 0.468 sec/batch; 41h:59m:11s remains)
INFO - root - 2017-12-06 03:28:50.527463: step 9250, loss = 0.85, batch loss = 0.63 (25.4 examples/sec; 0.315 sec/batch; 28h:15m:28s remains)
INFO - root - 2017-12-06 03:28:55.144654: step 9260, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.441 sec/batch; 39h:36m:21s remains)
INFO - root - 2017-12-06 03:28:59.623471: step 9270, loss = 0.85, batch loss = 0.64 (18.4 examples/sec; 0.436 sec/batch; 39h:07m:02s remains)
INFO - root - 2017-12-06 03:29:04.134168: step 9280, loss = 0.85, batch loss = 0.64 (17.9 examples/sec; 0.446 sec/batch; 40h:01m:33s remains)
INFO - root - 2017-12-06 03:29:08.661862: step 9290, loss = 0.85, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 40h:30m:12s remains)
INFO - root - 2017-12-06 03:29:13.162534: step 9300, loss = 0.88, batch loss = 0.67 (17.6 examples/sec; 0.454 sec/batch; 40h:43m:59s remains)
2017-12-06 03:29:13.594882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069033131 -0.068763733 -0.06865944 -0.068593509 -0.068553418 -0.068538867 -0.068559363 -0.068583 -0.068599418 -0.068591155 -0.068541676 -0.068470135 -0.068428151 -0.068364859 -0.068316571][-0.069395512 -0.069008924 -0.068803221 -0.0686607 -0.068592176 -0.068571091 -0.068594605 -0.068658836 -0.068711631 -0.068713322 -0.068658873 -0.068578437 -0.068461746 -0.068309173 -0.068173245][-0.070108794 -0.069637038 -0.069345944 -0.069131769 -0.069004372 -0.068953373 -0.068954639 -0.069013335 -0.0690663 -0.069052219 -0.068965904 -0.068828888 -0.0686525 -0.068437465 -0.068244345][-0.070716754 -0.070200764 -0.069841035 -0.069565356 -0.069379419 -0.069290251 -0.069280036 -0.069340274 -0.069395751 -0.069371074 -0.069246925 -0.069048695 -0.068808332 -0.068539716 -0.068301059][-0.071570113 -0.070998654 -0.070523731 -0.070123561 -0.0698282 -0.069643572 -0.069582976 -0.069623508 -0.069665782 -0.069620863 -0.069458321 -0.06920962 -0.068919569 -0.068608806 -0.068338826][-0.0729394 -0.072292686 -0.071652621 -0.071042471 -0.070532821 -0.070167311 -0.069988877 -0.069984183 -0.07001508 -0.069961138 -0.069771089 -0.069476508 -0.06912113 -0.068739384 -0.06842044][-0.074259318 -0.073574387 -0.0728204 -0.072049446 -0.071366869 -0.070851594 -0.070553228 -0.07048057 -0.070475154 -0.070398636 -0.070182644 -0.0698367 -0.0694058 -0.068937063 -0.068547815][-0.074936047 -0.074307747 -0.073562413 -0.072766937 -0.072029546 -0.071445689 -0.0710824 -0.070971377 -0.070946574 -0.070857145 -0.070616268 -0.07022474 -0.069718532 -0.0691591 -0.06868396][-0.074776888 -0.074264631 -0.073637195 -0.072952136 -0.072288692 -0.071751237 -0.071418732 -0.071314365 -0.071281493 -0.071172178 -0.070891835 -0.070451736 -0.069887228 -0.06927444 -0.068755321][-0.074145734 -0.07373549 -0.073255487 -0.07270401 -0.072160192 -0.071711943 -0.071431451 -0.071337856 -0.071289 -0.071161315 -0.070865951 -0.070407435 -0.069834515 -0.069229662 -0.0687227][-0.073275767 -0.072935462 -0.0725924 -0.072185755 -0.071768425 -0.071416281 -0.071185477 -0.071097963 -0.071048655 -0.0709318 -0.07065665 -0.070221461 -0.06967973 -0.069115333 -0.0686416][-0.072235927 -0.07198441 -0.071764417 -0.071486972 -0.071181685 -0.070918232 -0.0707438 -0.0706771 -0.070645943 -0.07056959 -0.07034862 -0.069968991 -0.06948559 -0.068982512 -0.068560414][-0.071241587 -0.071051523 -0.070890829 -0.070689768 -0.070454977 -0.0702495 -0.0701086 -0.070078522 -0.070114367 -0.070128545 -0.07000377 -0.069715992 -0.069320254 -0.06889005 -0.068511866][-0.070337966 -0.070160404 -0.070012324 -0.069840737 -0.069660738 -0.069503643 -0.069423072 -0.069470525 -0.069603935 -0.069721304 -0.069700055 -0.069516219 -0.069212541 -0.068848863 -0.068504587][-0.069478743 -0.069319308 -0.069192357 -0.069054075 -0.068935819 -0.068889037 -0.068929866 -0.069083288 -0.069315925 -0.069510676 -0.069562472 -0.069448762 -0.069201365 -0.068873465 -0.068540774]]...]
INFO - root - 2017-12-06 03:29:18.066970: step 9310, loss = 0.87, batch loss = 0.66 (17.7 examples/sec; 0.453 sec/batch; 40h:39m:12s remains)
INFO - root - 2017-12-06 03:29:22.640660: step 9320, loss = 0.86, batch loss = 0.64 (17.2 examples/sec; 0.466 sec/batch; 41h:50m:20s remains)
INFO - root - 2017-12-06 03:29:27.090768: step 9330, loss = 0.84, batch loss = 0.63 (17.9 examples/sec; 0.447 sec/batch; 40h:08m:51s remains)
INFO - root - 2017-12-06 03:29:31.615649: step 9340, loss = 0.92, batch loss = 0.71 (17.8 examples/sec; 0.450 sec/batch; 40h:25m:11s remains)
INFO - root - 2017-12-06 03:29:36.167092: step 9350, loss = 0.85, batch loss = 0.63 (17.2 examples/sec; 0.466 sec/batch; 41h:50m:33s remains)
INFO - root - 2017-12-06 03:29:40.542004: step 9360, loss = 0.89, batch loss = 0.68 (17.6 examples/sec; 0.454 sec/batch; 40h:42m:37s remains)
INFO - root - 2017-12-06 03:29:45.054615: step 9370, loss = 0.87, batch loss = 0.66 (17.3 examples/sec; 0.463 sec/batch; 41h:34m:44s remains)
INFO - root - 2017-12-06 03:29:49.592697: step 9380, loss = 0.88, batch loss = 0.66 (16.9 examples/sec; 0.473 sec/batch; 42h:25m:31s remains)
INFO - root - 2017-12-06 03:29:54.098259: step 9390, loss = 0.83, batch loss = 0.62 (18.0 examples/sec; 0.445 sec/batch; 39h:56m:26s remains)
INFO - root - 2017-12-06 03:29:58.617301: step 9400, loss = 0.85, batch loss = 0.63 (17.9 examples/sec; 0.448 sec/batch; 40h:12m:15s remains)
2017-12-06 03:29:59.051036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.081131861 -0.0802574 -0.078558087 -0.078561641 -0.078614533 -0.078625225 -0.078531854 -0.078435116 -0.078416012 -0.078432962 -0.078448273 -0.078397013 -0.078694396 -0.078708008 -0.078637496][-0.081202447 -0.078728922 -0.078190744 -0.078399345 -0.078606606 -0.078652389 -0.078637004 -0.078603171 -0.078525275 -0.078684285 -0.078964062 -0.080402046 -0.082122929 -0.084048033 -0.0851747][-0.078246519 -0.079075724 -0.078584246 -0.079392694 -0.079563752 -0.079306096 -0.079100423 -0.079022825 -0.078934364 -0.079033926 -0.079474658 -0.081485964 -0.084749281 -0.091029406 -0.096134707][-0.080248795 -0.082803369 -0.083203018 -0.084775962 -0.083719619 -0.0820411 -0.080702856 -0.082895368 -0.087585606 -0.09549994 -0.1064465 -0.11268306 -0.1177596 -0.12730539 -0.14012584][-0.077927485 -0.080533721 -0.081019916 -0.082722284 -0.082187913 -0.080171324 -0.078522511 -0.082338177 -0.09247081 -0.11328854 -0.14292656 -0.17301665 -0.1931904 -0.21006662 -0.22624665][-0.063166395 -0.064345635 -0.065515742 -0.067755014 -0.069877774 -0.070790939 -0.070919439 -0.058321133 -0.045933202 -0.047185495 -0.070233427 -0.092910841 -0.10766137 -0.13892746 -0.17081216][-0.056083582 -0.054385088 -0.05533509 -0.0538546 -0.056559682 -0.061024785 -0.060183309 -0.028526753 0.023910031 0.076411359 0.10533509 0.13314724 0.16046238 0.16453147 0.13584727][-0.061410744 -0.059437957 -0.059273303 -0.0547575 -0.055939816 -0.056887183 -0.051922143 -0.024266198 0.034730621 0.12228224 0.21145132 0.31262636 0.41997945 0.51125228 0.54369938][-0.073189355 -0.074360393 -0.075238079 -0.070175126 -0.0684809 -0.066172175 -0.056425191 -0.026796024 0.034487203 0.12767881 0.23774385 0.38959232 0.57011425 0.75100768 0.86529452][-0.083906367 -0.086453028 -0.088320211 -0.085904829 -0.08238022 -0.077523045 -0.0666755 -0.04243746 0.0043640733 0.090578236 0.19457453 0.34509212 0.53533971 0.75371689 0.92852306][-0.083590172 -0.08552178 -0.086168721 -0.085610434 -0.083740368 -0.0800763 -0.074788339 -0.067559443 -0.05182099 -0.0030248612 0.064501755 0.19119263 0.35919869 0.54474193 0.70888722][-0.079708487 -0.080897883 -0.080717593 -0.080287248 -0.079881549 -0.078906983 -0.080576375 -0.082506314 -0.085529894 -0.07265041 -0.054228023 0.010082245 0.10867529 0.23988122 0.35453284][-0.078713872 -0.079042085 -0.079284675 -0.0788823 -0.077555947 -0.077898078 -0.0812649 -0.089295208 -0.10525556 -0.11728987 -0.14007527 -0.15217373 -0.13927639 -0.094620392 -0.051485159][-0.078590155 -0.07896965 -0.07869941 -0.078322992 -0.07768923 -0.077934034 -0.080522761 -0.090678811 -0.11181818 -0.13586271 -0.17666012 -0.22216064 -0.26483503 -0.29171455 -0.30638221][-0.078639671 -0.078753822 -0.078713343 -0.07858739 -0.077517487 -0.07775525 -0.079421677 -0.085353866 -0.099069774 -0.11709864 -0.14885941 -0.19685288 -0.255977 -0.30910102 -0.35123321]]...]
INFO - root - 2017-12-06 03:30:03.524836: step 9410, loss = 0.90, batch loss = 0.69 (17.8 examples/sec; 0.449 sec/batch; 40h:15m:28s remains)
INFO - root - 2017-12-06 03:30:08.100681: step 9420, loss = 0.88, batch loss = 0.67 (17.6 examples/sec; 0.454 sec/batch; 40h:45m:04s remains)
INFO - root - 2017-12-06 03:30:12.578464: step 9430, loss = 0.89, batch loss = 0.68 (17.4 examples/sec; 0.460 sec/batch; 41h:16m:25s remains)
INFO - root - 2017-12-06 03:30:17.135113: step 9440, loss = 0.88, batch loss = 0.67 (17.8 examples/sec; 0.448 sec/batch; 40h:13m:55s remains)
INFO - root - 2017-12-06 03:30:21.664293: step 9450, loss = 0.86, batch loss = 0.65 (17.0 examples/sec; 0.471 sec/batch; 42h:13m:36s remains)
INFO - root - 2017-12-06 03:30:26.028138: step 9460, loss = 0.86, batch loss = 0.65 (17.7 examples/sec; 0.453 sec/batch; 40h:37m:14s remains)
INFO - root - 2017-12-06 03:30:30.592605: step 9470, loss = 0.82, batch loss = 0.61 (17.8 examples/sec; 0.451 sec/batch; 40h:25m:46s remains)
INFO - root - 2017-12-06 03:30:35.107180: step 9480, loss = 0.84, batch loss = 0.63 (18.0 examples/sec; 0.445 sec/batch; 39h:56m:36s remains)
INFO - root - 2017-12-06 03:30:39.601928: step 9490, loss = 0.81, batch loss = 0.60 (17.4 examples/sec; 0.460 sec/batch; 41h:13m:45s remains)
INFO - root - 2017-12-06 03:30:44.115048: step 9500, loss = 0.85, batch loss = 0.64 (17.1 examples/sec; 0.467 sec/batch; 41h:55m:03s remains)
2017-12-06 03:30:44.527594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.034983244 -0.034350779 -0.035220783 -0.036255058 -0.037742667 -0.039738774 -0.042111374 -0.044400159 -0.045733653 -0.045150053 -0.042564835 -0.038827613 -0.035396691 -0.033168305 -0.032156222][-0.034623131 -0.033973053 -0.035145886 -0.036747966 -0.039125193 -0.042532038 -0.04667 -0.050214898 -0.051906522 -0.050823227 -0.047300354 -0.042480092 -0.03766175 -0.033995058 -0.03199305][-0.03373602 -0.032983553 -0.034338284 -0.036407348 -0.039528038 -0.044083048 -0.04974781 -0.054694995 -0.057142261 -0.055999629 -0.051958106 -0.046230718 -0.040238146 -0.035265826 -0.032157857][-0.032339517 -0.031319357 -0.032677472 -0.034984566 -0.038520396 -0.043695696 -0.050342433 -0.056542549 -0.060051955 -0.059547268 -0.055707868 -0.049648792 -0.042778645 -0.03658836 -0.032377534][-0.030793864 -0.029359695 -0.030462056 -0.032635711 -0.036193892 -0.041584346 -0.048535138 -0.055344917 -0.05961521 -0.060001608 -0.056977231 -0.05143556 -0.044375747 -0.037617221 -0.032670341][-0.029257439 -0.027285315 -0.02798057 -0.029650588 -0.03282791 -0.037954535 -0.044691965 -0.051439613 -0.056031946 -0.057066895 -0.054985758 -0.050499581 -0.044245575 -0.03778521 -0.03276306][-0.027873531 -0.025465954 -0.025783341 -0.026811577 -0.029302508 -0.033837706 -0.039896142 -0.046037003 -0.050268978 -0.051479388 -0.049976826 -0.046549916 -0.041671544 -0.036426138 -0.032220498][-0.027089 -0.024515461 -0.024527092 -0.025095094 -0.026742473 -0.030343972 -0.035531163 -0.04071521 -0.044079892 -0.044962246 -0.043775804 -0.041255809 -0.037696965 -0.034014147 -0.031144388][-0.026986133 -0.024419997 -0.024366125 -0.0247432 -0.025762085 -0.02824479 -0.032345176 -0.036337756 -0.038561042 -0.038790185 -0.037727073 -0.035882764 -0.033663929 -0.03149917 -0.030077811][-0.027385872 -0.025083907 -0.025087234 -0.025453474 -0.02614554 -0.027750328 -0.030683443 -0.033286773 -0.03434309 -0.033934489 -0.032785419 -0.031470992 -0.030332718 -0.029523313 -0.029341418][-0.028282292 -0.026293572 -0.026289087 -0.02664588 -0.027211078 -0.028443992 -0.030389078 -0.03185289 -0.031984206 -0.031041108 -0.029763084 -0.0287931 -0.028344158 -0.028496727 -0.029023346][-0.029480103 -0.027862001 -0.027833268 -0.027981952 -0.028263923 -0.029248048 -0.03062062 -0.031429578 -0.031124253 -0.029996209 -0.028795935 -0.028094336 -0.028028045 -0.028634258 -0.02947415][-0.030799333 -0.02951777 -0.029418994 -0.029339291 -0.0293363 -0.029849812 -0.030796044 -0.03133928 -0.030995253 -0.030095391 -0.029286012 -0.028857302 -0.02897789 -0.02963762 -0.030471753][-0.031943183 -0.030852828 -0.030727774 -0.030563224 -0.030422263 -0.03053176 -0.031049304 -0.031463455 -0.031342696 -0.030834589 -0.030358009 -0.03016841 -0.030353598 -0.030859038 -0.031497322][-0.03347075 -0.032591369 -0.032481186 -0.032347787 -0.032198552 -0.03211049 -0.032259751 -0.032447878 -0.032427218 -0.032166813 -0.031868245 -0.031806033 -0.031931411 -0.032289781 -0.032771606]]...]
INFO - root - 2017-12-06 03:30:49.014532: step 9510, loss = 0.83, batch loss = 0.62 (17.9 examples/sec; 0.446 sec/batch; 40h:01m:43s remains)
INFO - root - 2017-12-06 03:30:53.497936: step 9520, loss = 0.84, batch loss = 0.63 (18.6 examples/sec; 0.430 sec/batch; 38h:35m:50s remains)
INFO - root - 2017-12-06 03:30:57.979273: step 9530, loss = 0.85, batch loss = 0.64 (18.4 examples/sec; 0.434 sec/batch; 38h:58m:04s remains)
INFO - root - 2017-12-06 03:31:02.430295: step 9540, loss = 0.83, batch loss = 0.62 (18.1 examples/sec; 0.442 sec/batch; 39h:39m:29s remains)
INFO - root - 2017-12-06 03:31:06.918494: step 9550, loss = 0.90, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 40h:17m:47s remains)
INFO - root - 2017-12-06 03:31:11.448565: step 9560, loss = 0.79, batch loss = 0.58 (17.6 examples/sec; 0.455 sec/batch; 40h:50m:14s remains)
INFO - root - 2017-12-06 03:31:15.751213: step 9570, loss = 0.76, batch loss = 0.55 (18.2 examples/sec; 0.440 sec/batch; 39h:28m:38s remains)
INFO - root - 2017-12-06 03:31:20.218087: step 9580, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.437 sec/batch; 39h:13m:17s remains)
INFO - root - 2017-12-06 03:31:24.849524: step 9590, loss = 0.80, batch loss = 0.58 (18.1 examples/sec; 0.441 sec/batch; 39h:32m:57s remains)
INFO - root - 2017-12-06 03:31:29.360521: step 9600, loss = 0.89, batch loss = 0.68 (17.7 examples/sec; 0.452 sec/batch; 40h:30m:57s remains)
2017-12-06 03:31:29.822081: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.003343828 0.0046997741 0.0047347397 0.004858166 0.0047994703 0.0045117587 0.0043187886 0.0040242448 0.0037269741 0.0037757307 0.0040686503 0.0044578537 0.0040093586 0.003375046 0.0015512854][0.0037534535 0.00519149 0.005172886 0.0052812248 0.0053273439 0.0051397607 0.0049943551 0.0047011003 0.0042337626 0.0039502755 0.0037557557 0.0037986487 0.0031420588 0.0021394044 0.00050443411][0.0048510283 0.0063334852 0.0062272549 0.0062229782 0.0062935054 0.0062322542 0.006135799 0.0058242455 0.0050705746 0.0042994246 0.0035488755 0.0030538365 0.0020552576 0.00064761937 -0.0010876581][0.0071328431 0.0085428506 0.00816381 0.0078753829 0.0078145489 0.0078033209 0.0078712925 0.0074833557 0.0063327774 0.0048763081 0.0035276636 0.002409026 0.00073979795 -0.0010462925 -0.0030753911][0.0096469522 0.011004463 0.010512426 0.010006428 0.0097752064 0.0097992122 0.010023914 0.0096746236 0.0080947205 0.0060228631 0.0040446222 0.002205275 -0.00026940554 -0.0024264157 -0.0049314052][0.011769846 0.013246231 0.012792595 0.012333103 0.012069367 0.012146972 0.012371793 0.011968255 0.010068573 0.007419005 0.0049067736 0.0024051368 -0.000815548 -0.003392756 -0.0063839629][0.013082996 0.014892235 0.014777638 0.014652073 0.014505908 0.014591962 0.014678255 0.014073715 0.011902347 0.0088188052 0.00599581 0.0029219389 -0.00082739443 -0.0038225353 -0.0072294325][0.013463937 0.01561109 0.016054414 0.016406186 0.016443223 0.016566604 0.016622685 0.015970886 0.013536535 0.010155536 0.0071703196 0.0037432611 -0.00053884089 -0.0039064586 -0.0074899197][0.012348779 0.014964342 0.016044155 0.017011978 0.017368183 0.017678775 0.017764919 0.017120048 0.014611304 0.011029802 0.0079256 0.0043469816 -0.00025802106 -0.0037913024 -0.0074048787][0.01041802 0.013319783 0.014874697 0.016310975 0.017000273 0.017383851 0.017442219 0.016890235 0.014638536 0.011205077 0.0080851316 0.0046117082 0.00017173588 -0.0033476576 -0.0068067238][0.0082422048 0.01119601 0.012894474 0.014441073 0.01518143 0.015516549 0.015504487 0.015012667 0.013177119 0.010069981 0.0071043149 0.0040735826 0.00016604364 -0.0030678064 -0.0062635392][0.0054308325 0.0080881044 0.0096121952 0.011022128 0.011744462 0.012116894 0.012320973 0.012076281 0.01066947 0.0079905689 0.0053412244 0.0027693361 -0.00053095073 -0.0033569634 -0.0061611906][0.0020623431 0.0042706504 0.0055197477 0.0065781698 0.0071450472 0.0076147765 0.0080991089 0.0081271529 0.0072372705 0.0052321479 0.0031132251 0.0010722503 -0.0015192926 -0.00394883 -0.0063631833][-0.0015006885 0.0001718998 0.0010786355 0.0018272549 0.0022531375 0.0026325211 0.0030773208 0.0032073632 0.0027617067 0.0015067905 -0.00015101582 -0.0017416552 -0.0036932081 -0.0057129115 -0.0078248829][-0.0062024146 -0.0050316676 -0.0044030622 -0.0038468018 -0.0035216808 -0.0033005103 -0.0030342117 -0.0028389841 -0.0029393882 -0.0036544651 -0.0047887117 -0.0059024692 -0.0072212666 -0.0087366626 -0.01035852]]...]
INFO - root - 2017-12-06 03:31:34.324378: step 9610, loss = 0.80, batch loss = 0.59 (18.1 examples/sec; 0.442 sec/batch; 39h:36m:16s remains)
INFO - root - 2017-12-06 03:31:38.813745: step 9620, loss = 0.84, batch loss = 0.62 (17.8 examples/sec; 0.450 sec/batch; 40h:19m:07s remains)
INFO - root - 2017-12-06 03:31:43.368110: step 9630, loss = 0.86, batch loss = 0.65 (17.1 examples/sec; 0.468 sec/batch; 41h:59m:07s remains)
INFO - root - 2017-12-06 03:31:47.834383: step 9640, loss = 0.90, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 39h:47m:43s remains)
INFO - root - 2017-12-06 03:31:52.323470: step 9650, loss = 0.82, batch loss = 0.61 (17.5 examples/sec; 0.458 sec/batch; 41h:04m:21s remains)
INFO - root - 2017-12-06 03:31:56.835691: step 9660, loss = 0.82, batch loss = 0.61 (17.0 examples/sec; 0.471 sec/batch; 42h:14m:07s remains)
INFO - root - 2017-12-06 03:32:01.185338: step 9670, loss = 0.82, batch loss = 0.61 (18.2 examples/sec; 0.440 sec/batch; 39h:26m:19s remains)
INFO - root - 2017-12-06 03:32:05.702237: step 9680, loss = 0.80, batch loss = 0.58 (18.2 examples/sec; 0.441 sec/batch; 39h:30m:44s remains)
INFO - root - 2017-12-06 03:32:10.236363: step 9690, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.437 sec/batch; 39h:11m:02s remains)
INFO - root - 2017-12-06 03:32:14.770587: step 9700, loss = 0.81, batch loss = 0.60 (17.0 examples/sec; 0.471 sec/batch; 42h:11m:29s remains)
2017-12-06 03:32:15.214545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010159791 -0.0093403757 -0.0098721012 -0.010532111 -0.011307798 -0.01202099 -0.012572601 -0.012939043 -0.013236783 -0.013646699 -0.014082447 -0.014359243 -0.01451052 -0.014995791 -0.01624335][-0.0079729185 -0.0074161589 -0.0079792216 -0.0088161454 -0.009942539 -0.011055946 -0.011749879 -0.012124702 -0.012370318 -0.012760267 -0.013194509 -0.013536029 -0.013894662 -0.014623672 -0.016092662][-0.0061222538 -0.0054810345 -0.0059563592 -0.0069328621 -0.0083234683 -0.0097843781 -0.010654785 -0.011033304 -0.011252962 -0.011689134 -0.012340389 -0.012882903 -0.013430223 -0.014347665 -0.015878364][-0.0060843304 -0.0047937632 -0.00483422 -0.0056462064 -0.0070092231 -0.0085673034 -0.0094842836 -0.0098984912 -0.010136157 -0.010575004 -0.011354864 -0.012045793 -0.012803577 -0.014014319 -0.015672296][-0.0096380711 -0.0073469505 -0.006371133 -0.0063897669 -0.007184498 -0.0083090216 -0.0088864416 -0.0091216117 -0.0093022212 -0.0097009614 -0.010481387 -0.011242919 -0.012205675 -0.013746008 -0.01551744][-0.018278174 -0.015053742 -0.012770422 -0.011327632 -0.010531455 -0.010216855 -0.0095690638 -0.0089061856 -0.0088026 -0.009172909 -0.0099472031 -0.010741986 -0.011793278 -0.013633087 -0.015598517][-0.030667912 -0.027137104 -0.024102282 -0.021368608 -0.018801011 -0.016116958 -0.012715466 -0.0095474795 -0.0084887147 -0.0089455247 -0.00989756 -0.010738701 -0.01176206 -0.013706625 -0.015744261][-0.044333361 -0.041218508 -0.038201556 -0.035138056 -0.032019269 -0.028142944 -0.023522262 -0.018802226 -0.015929747 -0.014318511 -0.013138197 -0.012587048 -0.012876555 -0.014243059 -0.015956238][-0.057020023 -0.054788128 -0.052420478 -0.050057992 -0.047743391 -0.044590969 -0.040405016 -0.035267316 -0.030211959 -0.025365662 -0.020626396 -0.017190177 -0.015492719 -0.015539359 -0.016424626][-0.066481978 -0.0654444 -0.06418933 -0.062840931 -0.061549634 -0.05904616 -0.054948829 -0.049743339 -0.043972809 -0.037499428 -0.030326426 -0.02420076 -0.01991624 -0.01779414 -0.01726545][-0.072585054 -0.072743282 -0.072827421 -0.072668649 -0.072657943 -0.071327753 -0.068244942 -0.063561037 -0.05734472 -0.049578551 -0.040435441 -0.031981554 -0.025129121 -0.020689376 -0.018420164][-0.076117411 -0.07713668 -0.078198187 -0.079131506 -0.08019232 -0.079941437 -0.078056581 -0.074106164 -0.067993581 -0.059453227 -0.049060673 -0.038896471 -0.03010013 -0.02368895 -0.019788649][-0.078451179 -0.0798777 -0.081374489 -0.082693085 -0.0840646 -0.08427044 -0.08318232 -0.080030873 -0.074201 -0.06549722 -0.0547641 -0.043863211 -0.033997439 -0.026227441 -0.021040563][-0.080573857 -0.08198934 -0.083283633 -0.084184021 -0.085095212 -0.085083656 -0.084010847 -0.081160031 -0.075944379 -0.067764208 -0.057264104 -0.046390761 -0.036324631 -0.028031267 -0.022194438][-0.081692383 -0.0829645 -0.083975852 -0.084493786 -0.084847242 -0.084557325 -0.083366267 -0.080731556 -0.076108314 -0.068554595 -0.058521107 -0.04788588 -0.037907511 -0.029515941 -0.023379859]]...]
INFO - root - 2017-12-06 03:32:19.731328: step 9710, loss = 0.81, batch loss = 0.59 (17.9 examples/sec; 0.447 sec/batch; 40h:04m:55s remains)
INFO - root - 2017-12-06 03:32:24.263953: step 9720, loss = 0.81, batch loss = 0.59 (17.6 examples/sec; 0.454 sec/batch; 40h:41m:57s remains)
INFO - root - 2017-12-06 03:32:28.823358: step 9730, loss = 0.85, batch loss = 0.64 (17.8 examples/sec; 0.448 sec/batch; 40h:11m:18s remains)
INFO - root - 2017-12-06 03:32:33.318941: step 9740, loss = 0.85, batch loss = 0.64 (17.3 examples/sec; 0.462 sec/batch; 41h:27m:29s remains)
INFO - root - 2017-12-06 03:32:37.819319: step 9750, loss = 0.84, batch loss = 0.63 (17.9 examples/sec; 0.446 sec/batch; 39h:57m:58s remains)
INFO - root - 2017-12-06 03:32:42.435112: step 9760, loss = 0.82, batch loss = 0.60 (18.3 examples/sec; 0.437 sec/batch; 39h:13m:00s remains)
INFO - root - 2017-12-06 03:32:46.943976: step 9770, loss = 0.82, batch loss = 0.61 (18.0 examples/sec; 0.444 sec/batch; 39h:45m:40s remains)
INFO - root - 2017-12-06 03:32:50.857082: step 9780, loss = 0.81, batch loss = 0.60 (16.7 examples/sec; 0.479 sec/batch; 42h:55m:41s remains)
INFO - root - 2017-12-06 03:32:55.399981: step 9790, loss = 0.86, batch loss = 0.65 (17.5 examples/sec; 0.458 sec/batch; 41h:01m:52s remains)
INFO - root - 2017-12-06 03:32:59.926477: step 9800, loss = 0.85, batch loss = 0.64 (18.2 examples/sec; 0.439 sec/batch; 39h:23m:21s remains)
2017-12-06 03:33:00.394078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.023161817 -0.021623995 -0.021855094 -0.02217222 -0.022558961 -0.022848766 -0.022962153 -0.022826154 -0.02247652 -0.021934114 -0.021222673 -0.02032461 -0.019156907 -0.018233802 -0.018118732][-0.022577006 -0.020861693 -0.021072954 -0.021445487 -0.021978855 -0.022483714 -0.022806633 -0.022851177 -0.022610743 -0.022099253 -0.021328297 -0.020224787 -0.018722527 -0.017436586 -0.017048094][-0.021781985 -0.019883942 -0.020025991 -0.020419542 -0.021058291 -0.021750223 -0.02228535 -0.022545863 -0.02247354 -0.0220506 -0.021285746 -0.020034827 -0.018246096 -0.016565543 -0.01599744][-0.021420203 -0.01940085 -0.019450441 -0.019777603 -0.020376582 -0.021128986 -0.021790415 -0.022212911 -0.022311341 -0.022051465 -0.02138659 -0.02010325 -0.018159196 -0.016124509 -0.015206546][-0.021359004 -0.019275311 -0.01918909 -0.019361049 -0.019785013 -0.02041221 -0.021042682 -0.021517657 -0.021744478 -0.021670565 -0.021194045 -0.02003004 -0.018068526 -0.015888605 -0.014639422][-0.021346271 -0.01921409 -0.018989708 -0.018992808 -0.019206766 -0.019616265 -0.020071335 -0.02044433 -0.020655476 -0.020658348 -0.020337507 -0.019320078 -0.017454676 -0.015404649 -0.014019132][-0.021483675 -0.019364558 -0.019044217 -0.018894892 -0.018876366 -0.019011967 -0.019185651 -0.019324709 -0.019378494 -0.019335195 -0.019043699 -0.018118091 -0.016354561 -0.014536731 -0.013096638][-0.022478025 -0.020466015 -0.020113919 -0.019829389 -0.019589756 -0.019433506 -0.019272633 -0.019088939 -0.018855389 -0.01860052 -0.018158756 -0.017173678 -0.015409276 -0.013767093 -0.012360983][-0.023613628 -0.02178058 -0.021432921 -0.021057211 -0.020648167 -0.020225339 -0.019751552 -0.019254252 -0.018726017 -0.018217944 -0.017560929 -0.016441051 -0.014611103 -0.013066158 -0.011644885][-0.024543751 -0.023027856 -0.022701267 -0.022287317 -0.021778006 -0.02118757 -0.020514596 -0.019762795 -0.018983599 -0.01821303 -0.017299317 -0.015966076 -0.013968885 -0.012400568 -0.010921471][-0.025536042 -0.024296947 -0.024030659 -0.02361792 -0.023052588 -0.022372287 -0.021563161 -0.020638987 -0.019643832 -0.018624067 -0.017460253 -0.015894618 -0.013716355 -0.011920914 -0.01039353][-0.026475854 -0.025526028 -0.025342181 -0.024961375 -0.024380047 -0.023664437 -0.022779528 -0.021757763 -0.020628408 -0.019433014 -0.018053796 -0.016241487 -0.013828792 -0.011708125 -0.010159917][-0.027762577 -0.027016521 -0.02688371 -0.026550513 -0.025995258 -0.025228914 -0.024281207 -0.023168277 -0.021910455 -0.020605393 -0.019066237 -0.017132331 -0.014628358 -0.012403235 -0.010905467][-0.029240645 -0.028682459 -0.028584938 -0.028311692 -0.02780854 -0.02709185 -0.026160695 -0.025089767 -0.023910299 -0.022576656 -0.020964388 -0.01899321 -0.016580041 -0.014224939 -0.012585931][-0.031827625 -0.03136766 -0.031213496 -0.030921433 -0.030399431 -0.02971673 -0.028840192 -0.027911574 -0.026849207 -0.025592703 -0.024052568 -0.022133235 -0.019792315 -0.017379418 -0.015568808]]...]
INFO - root - 2017-12-06 03:33:04.899474: step 9810, loss = 0.80, batch loss = 0.58 (18.2 examples/sec; 0.440 sec/batch; 39h:24m:33s remains)
INFO - root - 2017-12-06 03:33:09.413246: step 9820, loss = 0.79, batch loss = 0.57 (17.5 examples/sec; 0.456 sec/batch; 40h:52m:58s remains)
