INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "151"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 10:50:50.066780: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:50:50.066810: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:50:50.066816: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:50:50.066820: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:50:50.066824: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-07 10:50:55.565846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 4.09GiB
2017-12-07 10:50:55.565891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 10:50:55.565899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 10:50:55.565907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 10:51:15.135003: step 0, loss = 2.03, batch loss = 1.97 (0.6 examples/sec; 13.129 sec/batch; 1212h:36m:24s remains)
2017-12-07 10:51:16.160036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.332726 -4.3307204 -4.3305659 -4.330832 -4.3284492 -4.3214321 -4.3085055 -4.290122 -4.2698517 -4.2588191 -4.2586722 -4.26565 -4.2810192 -4.3041472 -4.32482][-4.3330197 -4.3312812 -4.3320131 -4.332077 -4.3267341 -4.3120823 -4.2869153 -4.2553692 -4.2252488 -4.2117081 -4.2152872 -4.2290783 -4.2529106 -4.2854671 -4.3125777][-4.3293343 -4.328949 -4.331274 -4.3299451 -4.3165731 -4.288167 -4.245985 -4.1976171 -4.1550336 -4.1396494 -4.1507449 -4.1777244 -4.2167015 -4.2609124 -4.296051][-4.3203015 -4.3226242 -4.3283405 -4.3259606 -4.3006744 -4.2555938 -4.1940928 -4.1244688 -4.0674567 -4.0518494 -4.0745087 -4.1184549 -4.1778393 -4.2371588 -4.2808857][-4.3063464 -4.3129821 -4.3227458 -4.3191791 -4.2815032 -4.2214861 -4.1416688 -4.048604 -3.9699774 -3.9558876 -3.9957957 -4.0601711 -4.14212 -4.2169762 -4.2703357][-4.2886672 -4.2996988 -4.3114772 -4.3053789 -4.2585773 -4.1867666 -4.0866127 -3.964762 -3.8609972 -3.8610566 -3.9286709 -4.0162048 -4.1176682 -4.2029576 -4.2635431][-4.2721744 -4.2858458 -4.296041 -4.2825313 -4.2288837 -4.1456075 -4.0217929 -3.8682039 -3.7446373 -3.7774763 -3.8814979 -3.9910588 -4.1055975 -4.1962543 -4.2599044][-4.2623138 -4.2721262 -4.2739692 -4.2504354 -4.1929431 -4.1016655 -3.9613781 -3.7834806 -3.659636 -3.7463422 -3.8841531 -4.0048342 -4.1167421 -4.201591 -4.2627463][-4.2540812 -4.2537012 -4.2455544 -4.2178712 -4.1677861 -4.0866108 -3.962625 -3.805877 -3.7274189 -3.8401818 -3.96579 -4.0646048 -4.14989 -4.2175636 -4.2702017][-4.2440686 -4.2338862 -4.22064 -4.1956253 -4.1646194 -4.1116695 -4.028542 -3.9246452 -3.8906271 -3.9786773 -4.0631938 -4.1253667 -4.1799469 -4.2308922 -4.2773376][-4.2319846 -4.216608 -4.2018471 -4.1821246 -4.1667485 -4.1361761 -4.0878325 -4.0277057 -4.0145736 -4.0712 -4.12226 -4.1562343 -4.1914535 -4.2364178 -4.2829957][-4.2170734 -4.200357 -4.18511 -4.1704769 -4.1609149 -4.1457963 -4.121717 -4.089685 -4.0881319 -4.1249847 -4.1550584 -4.1731844 -4.2006822 -4.2443461 -4.2904196][-4.2073379 -4.1945939 -4.181561 -4.1728969 -4.16971 -4.1669784 -4.1601357 -4.1451097 -4.146111 -4.1633425 -4.1769819 -4.1850638 -4.2096338 -4.2534142 -4.2976665][-4.2180786 -4.2147217 -4.2088051 -4.2062664 -4.2059007 -4.2077069 -4.2056088 -4.1932549 -4.1882672 -4.187561 -4.1884022 -4.1908908 -4.2145782 -4.2594018 -4.3020973][-4.2351651 -4.2398081 -4.2436147 -4.24648 -4.2445755 -4.2419586 -4.2328191 -4.2144427 -4.2010956 -4.1900129 -4.1848159 -4.1884093 -4.2149863 -4.2616282 -4.3036456]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 10:51:25.174762: step 10, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 63h:12m:57s remains)
INFO - root - 2017-12-07 10:51:31.846845: step 20, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 58h:51m:33s remains)
INFO - root - 2017-12-07 10:51:38.603231: step 30, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 60h:02m:04s remains)
INFO - root - 2017-12-07 10:51:45.434436: step 40, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 65h:48m:58s remains)
INFO - root - 2017-12-07 10:51:52.239288: step 50, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 66h:24m:26s remains)
INFO - root - 2017-12-07 10:51:58.983086: step 60, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.657 sec/batch; 60h:37m:34s remains)
INFO - root - 2017-12-07 10:52:05.738692: step 70, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 61h:44m:16s remains)
INFO - root - 2017-12-07 10:52:12.583243: step 80, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 60h:07m:55s remains)
INFO - root - 2017-12-07 10:52:19.288432: step 90, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.735 sec/batch; 67h:54m:42s remains)
INFO - root - 2017-12-07 10:52:26.071668: step 100, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 66h:52m:15s remains)
2017-12-07 10:52:26.774392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2503772 -4.2184205 -4.1806426 -4.1571145 -4.1581712 -4.174727 -4.1891479 -4.2032657 -4.2160511 -4.2172875 -4.2133918 -4.21482 -4.2112789 -4.1958013 -4.1836429][-4.2573104 -4.2181082 -4.1700587 -4.1336446 -4.1260238 -4.1411777 -4.1574764 -4.1704783 -4.1815925 -4.1865735 -4.1896777 -4.1954827 -4.1949863 -4.1816964 -4.17284][-4.2611523 -4.2179575 -4.1654515 -4.1235652 -4.1108584 -4.1227078 -4.13802 -4.1495838 -4.1638894 -4.1767707 -4.1854682 -4.1904712 -4.1875196 -4.1725707 -4.1619835][-4.2622581 -4.2192535 -4.16766 -4.1250496 -4.1087241 -4.1154809 -4.1277204 -4.1368823 -4.15555 -4.1780434 -4.1912031 -4.1945729 -4.188849 -4.1733885 -4.1596732][-4.2606568 -4.2182121 -4.1695547 -4.1266408 -4.1043296 -4.1021347 -4.1075597 -4.1110578 -4.1305361 -4.1603246 -4.1790023 -4.1837482 -4.180613 -4.170198 -4.1583281][-4.2579527 -4.2174559 -4.1714444 -4.1269631 -4.0977659 -4.08652 -4.0816979 -4.07207 -4.0827503 -4.1160531 -4.1447682 -4.1571736 -4.1615243 -4.1580672 -4.1506271][-4.252161 -4.2107353 -4.1623077 -4.1131806 -4.0778856 -4.0619559 -4.0517273 -4.0312285 -4.0328541 -4.0691252 -4.1072617 -4.1272902 -4.1383348 -4.14074 -4.1366973][-4.2500324 -4.2069521 -4.1556134 -4.1054578 -4.0706234 -4.0559173 -4.0469546 -4.0241876 -4.0200205 -4.0538688 -4.0917134 -4.1098738 -4.1218252 -4.1286387 -4.126791][-4.2505989 -4.2070804 -4.1549315 -4.1068935 -4.0776629 -4.0671916 -4.0607576 -4.04551 -4.0441113 -4.0719881 -4.0990853 -4.1085982 -4.1155491 -4.1203074 -4.1179304][-4.2532725 -4.2094588 -4.157475 -4.1124229 -4.08759 -4.0776072 -4.0716534 -4.0632582 -4.0707712 -4.0962157 -4.114625 -4.1155891 -4.1150346 -4.1148725 -4.1113424][-4.2605157 -4.2176275 -4.1680083 -4.1280189 -4.1089692 -4.1013765 -4.0964003 -4.09287 -4.1040664 -4.1267376 -4.1393423 -4.13449 -4.1287417 -4.1250124 -4.1211109][-4.2704244 -4.2297544 -4.1850171 -4.1514206 -4.1394105 -4.139 -4.1401939 -4.1428332 -4.1521354 -4.1673431 -4.1741433 -4.1686974 -4.161664 -4.1565962 -4.1532259][-4.2821083 -4.2461662 -4.2087784 -4.1815376 -4.1742387 -4.1798058 -4.1905251 -4.2007895 -4.2089047 -4.2159967 -4.2162967 -4.2104764 -4.2030468 -4.1977859 -4.1960516][-4.3007431 -4.2739472 -4.2463231 -4.225256 -4.2186942 -4.2253814 -4.2398858 -4.2539291 -4.262351 -4.2649279 -4.2610235 -4.2531776 -4.2452893 -4.2415128 -4.2423034][-4.3217244 -4.3058214 -4.2895765 -4.2760172 -4.2706137 -4.2745776 -4.2853174 -4.2959723 -4.3019571 -4.302465 -4.2986956 -4.2925234 -4.287571 -4.2866955 -4.2887611]]...]
INFO - root - 2017-12-07 10:52:33.712728: step 110, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 60h:35m:36s remains)
INFO - root - 2017-12-07 10:52:40.509350: step 120, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 63h:22m:40s remains)
INFO - root - 2017-12-07 10:52:47.292453: step 130, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 61h:36m:40s remains)
INFO - root - 2017-12-07 10:52:54.050249: step 140, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 60h:01m:27s remains)
INFO - root - 2017-12-07 10:53:00.761655: step 150, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.627 sec/batch; 57h:52m:22s remains)
INFO - root - 2017-12-07 10:53:07.659080: step 160, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 60h:50m:50s remains)
INFO - root - 2017-12-07 10:53:14.605474: step 170, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 66h:39m:29s remains)
INFO - root - 2017-12-07 10:53:21.442107: step 180, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 62h:36m:11s remains)
INFO - root - 2017-12-07 10:53:28.040942: step 190, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 59h:30m:53s remains)
INFO - root - 2017-12-07 10:53:34.762516: step 200, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 58h:53m:17s remains)
2017-12-07 10:53:35.503031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3210711 -4.3160167 -4.3123775 -4.3122907 -4.3161416 -4.3219862 -4.3275986 -4.331975 -4.3359656 -4.3408647 -4.3473082 -4.3532166 -4.3558164 -4.3555622 -4.3524137][-4.30853 -4.2999496 -4.2950115 -4.2973771 -4.3062997 -4.3142982 -4.3177795 -4.3185072 -4.3194141 -4.3244367 -4.3345218 -4.3463912 -4.354228 -4.3553486 -4.3515778][-4.2929859 -4.2809696 -4.2772851 -4.2860589 -4.2983027 -4.3032079 -4.2981491 -4.2882428 -4.2817249 -4.2875242 -4.305933 -4.3276396 -4.3427815 -4.3465266 -4.3437371][-4.2804351 -4.2667232 -4.265739 -4.2761197 -4.2838321 -4.278038 -4.2584429 -4.2329311 -4.2181172 -4.23013 -4.2633266 -4.2969341 -4.3206029 -4.3307624 -4.3324661][-4.2692547 -4.2561421 -4.2559152 -4.2604442 -4.2547827 -4.2300634 -4.1903958 -4.1480594 -4.1340508 -4.16258 -4.21571 -4.2618275 -4.2941079 -4.3123765 -4.3201556][-4.2460127 -4.2302756 -4.2243619 -4.2172022 -4.1944933 -4.1497512 -4.0897589 -4.0352225 -4.0389996 -4.1005917 -4.1757121 -4.2312865 -4.2704282 -4.2975464 -4.3116484][-4.2064986 -4.1798978 -4.1601686 -4.1382589 -4.0956831 -4.0275621 -3.9477153 -3.8962467 -3.9420316 -4.0486636 -4.14467 -4.2079759 -4.2542844 -4.2899265 -4.3078423][-4.1731381 -4.1367426 -4.1008515 -4.0607944 -4.0011907 -3.9239256 -3.8483396 -3.8248739 -3.9121761 -4.0437202 -4.1454329 -4.2094312 -4.2581239 -4.2946706 -4.3103962][-4.147202 -4.1094847 -4.0684867 -4.0263572 -3.9761608 -3.9249635 -3.8872058 -3.9019485 -3.9912977 -4.1031613 -4.1869349 -4.2410159 -4.2822056 -4.309845 -4.3178186][-4.1325727 -4.103229 -4.0738368 -4.0498934 -4.029295 -4.0123348 -4.00754 -4.0360622 -4.1014652 -4.1756992 -4.2334032 -4.2718167 -4.3025393 -4.3207006 -4.3234015][-4.1280594 -4.1115847 -4.1035476 -4.1046448 -4.1111941 -4.1148953 -4.118948 -4.1424956 -4.1813283 -4.2234278 -4.2591243 -4.2858567 -4.309288 -4.3242269 -4.3278413][-4.1451669 -4.14068 -4.148356 -4.1618605 -4.1782851 -4.185328 -4.1858678 -4.2003536 -4.2215128 -4.2464628 -4.2709279 -4.29126 -4.3129573 -4.3288674 -4.3350148][-4.1872396 -4.1900568 -4.2016211 -4.214642 -4.2276864 -4.2310224 -4.227458 -4.2349138 -4.2463408 -4.26237 -4.2822566 -4.299931 -4.3203583 -4.3365026 -4.3438625][-4.2382946 -4.2426996 -4.2497611 -4.2562222 -4.2608705 -4.2589974 -4.2527 -4.2555113 -4.262722 -4.2744608 -4.2912064 -4.3086 -4.3278184 -4.3420181 -4.3482938][-4.279912 -4.2819715 -4.2847886 -4.2873178 -4.287219 -4.2832284 -4.2785664 -4.2799788 -4.2843533 -4.2910547 -4.3028269 -4.3175855 -4.3330112 -4.3432217 -4.3472462]]...]
INFO - root - 2017-12-07 10:53:42.344474: step 210, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 61h:29m:20s remains)
INFO - root - 2017-12-07 10:53:49.126877: step 220, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 59h:46m:27s remains)
INFO - root - 2017-12-07 10:53:55.891579: step 230, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 58h:56m:28s remains)
INFO - root - 2017-12-07 10:54:02.563960: step 240, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 67h:32m:53s remains)
INFO - root - 2017-12-07 10:54:09.339989: step 250, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 62h:32m:02s remains)
INFO - root - 2017-12-07 10:54:16.128883: step 260, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.613 sec/batch; 56h:34m:12s remains)
INFO - root - 2017-12-07 10:54:22.925981: step 270, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 60h:28m:22s remains)
INFO - root - 2017-12-07 10:54:29.745192: step 280, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 63h:45m:34s remains)
INFO - root - 2017-12-07 10:54:36.509183: step 290, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 68h:44m:44s remains)
INFO - root - 2017-12-07 10:54:43.239390: step 300, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:43m:27s remains)
2017-12-07 10:54:43.961715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0979762 -4.0804329 -4.0780416 -4.0903788 -4.1201673 -4.1657529 -4.1998463 -4.2038007 -4.1868067 -4.1577134 -4.13665 -4.1317554 -4.1275473 -4.1211071 -4.1224561][-4.1781559 -4.162765 -4.1402941 -4.1224432 -4.1250138 -4.1591659 -4.1964927 -4.2089477 -4.2050972 -4.193089 -4.18224 -4.1765723 -4.1631327 -4.1428638 -4.12987][-4.2319717 -4.2212987 -4.1885986 -4.1465554 -4.1248417 -4.1463652 -4.1825895 -4.2016706 -4.2097898 -4.2129111 -4.207665 -4.195117 -4.1719203 -4.1405754 -4.1158462][-4.2426338 -4.23512 -4.2013683 -4.1494894 -4.11234 -4.1210785 -4.1509976 -4.1738782 -4.1933389 -4.2092228 -4.2074442 -4.1837964 -4.1489615 -4.1111259 -4.0850058][-4.2206683 -4.2139816 -4.1833963 -4.1365237 -4.0975304 -4.0942731 -4.1127076 -4.1336007 -4.1588464 -4.1800447 -4.1796193 -4.148509 -4.1056547 -4.06528 -4.04474][-4.1843 -4.1810265 -4.154964 -4.1202407 -4.0920715 -4.0825605 -4.0895376 -4.1050448 -4.1287322 -4.1440854 -4.1392879 -4.1044478 -4.0575461 -4.013998 -3.9988306][-4.1666842 -4.1637049 -4.1409264 -4.118782 -4.1040845 -4.0956287 -4.0943756 -4.1050878 -4.124126 -4.1293068 -4.1152511 -4.074945 -4.0223403 -3.9709556 -3.9591677][-4.1575971 -4.1557608 -4.1407914 -4.1320248 -4.1298089 -4.1234264 -4.1160707 -4.1196046 -4.130034 -4.1245046 -4.1040344 -4.0670638 -4.0159259 -3.964046 -3.9588838][-4.1571045 -4.1643395 -4.165379 -4.170742 -4.1770711 -4.1703997 -4.1571932 -4.1532011 -4.1554217 -4.1448545 -4.1249022 -4.095243 -4.0506597 -4.0063229 -4.007019][-4.1480694 -4.1714778 -4.1896749 -4.2037005 -4.2123623 -4.2063513 -4.1896925 -4.1793528 -4.1758108 -4.1657009 -4.1535745 -4.1384492 -4.1083474 -4.0775352 -4.0812321][-4.1299205 -4.16697 -4.2001419 -4.2210751 -4.2354712 -4.2349753 -4.2183452 -4.2034259 -4.1916947 -4.1779995 -4.1700854 -4.1697559 -4.1566877 -4.1411619 -4.1477427][-4.1133237 -4.1554365 -4.1950541 -4.2211514 -4.2431135 -4.2502379 -4.2388272 -4.2258296 -4.2081566 -4.1859741 -4.1758575 -4.1873922 -4.1886268 -4.1831346 -4.1891189][-4.1113291 -4.1462941 -4.1810875 -4.2063494 -4.229393 -4.2410979 -4.2386856 -4.2315555 -4.2113872 -4.1848726 -4.1746264 -4.1914687 -4.2013283 -4.200551 -4.2053676][-4.1298552 -4.1437025 -4.1609483 -4.1785073 -4.2000332 -4.2167473 -4.2239513 -4.2233214 -4.2037582 -4.1775002 -4.1692233 -4.1854019 -4.19585 -4.1936889 -4.1946058][-4.1633725 -4.1582189 -4.1596084 -4.1666517 -4.1814647 -4.1967759 -4.2097311 -4.2140822 -4.1998777 -4.1795053 -4.1733766 -4.1825395 -4.1858568 -4.1782265 -4.1729603]]...]
INFO - root - 2017-12-07 10:54:50.813901: step 310, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 67h:12m:01s remains)
INFO - root - 2017-12-07 10:54:57.637262: step 320, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.748 sec/batch; 69h:02m:26s remains)
INFO - root - 2017-12-07 10:55:04.537810: step 330, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 62h:20m:39s remains)
INFO - root - 2017-12-07 10:55:11.328724: step 340, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 59h:17m:54s remains)
INFO - root - 2017-12-07 10:55:18.212964: step 350, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 64h:53m:31s remains)
INFO - root - 2017-12-07 10:55:25.051841: step 360, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 67h:18m:23s remains)
INFO - root - 2017-12-07 10:55:31.846367: step 370, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 61h:06m:22s remains)
INFO - root - 2017-12-07 10:55:38.651543: step 380, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 60h:21m:21s remains)
INFO - root - 2017-12-07 10:55:45.246600: step 390, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.641 sec/batch; 59h:10m:17s remains)
INFO - root - 2017-12-07 10:55:52.026524: step 400, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 60h:57m:45s remains)
2017-12-07 10:55:52.839614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1950746 -4.1883717 -4.1884937 -4.1860013 -4.1768546 -4.1757126 -4.195549 -4.2113738 -4.2238693 -4.2459702 -4.2724056 -4.2873197 -4.2910557 -4.2922935 -4.2937851][-4.1823459 -4.1905589 -4.1939163 -4.1919575 -4.1801467 -4.1735535 -4.1862755 -4.2001133 -4.2143331 -4.2399659 -4.2713528 -4.2904959 -4.2955852 -4.2958574 -4.29577][-4.144382 -4.1723766 -4.1902 -4.1962337 -4.1844912 -4.1695337 -4.167872 -4.1760769 -4.1931677 -4.2250786 -4.2604966 -4.284812 -4.2928624 -4.2927003 -4.2897892][-4.118278 -4.1576319 -4.1887355 -4.2018385 -4.1880565 -4.1623149 -4.1392694 -4.1345773 -4.1570597 -4.2023673 -4.245213 -4.2756228 -4.2869186 -4.2861843 -4.2800794][-4.1537333 -4.1833172 -4.2007613 -4.2006335 -4.1800413 -4.1440897 -4.0936074 -4.0713358 -4.1081352 -4.1737213 -4.2245674 -4.2601929 -4.2731671 -4.2669759 -4.25791][-4.2112794 -4.2185912 -4.20529 -4.1802783 -4.1498284 -4.0986471 -4.0081816 -3.9715312 -4.0438805 -4.13811 -4.1927185 -4.2280297 -4.2373633 -4.2223177 -4.208632][-4.2329922 -4.21803 -4.1734004 -4.1255822 -4.0870233 -4.0148945 -3.87609 -3.8208585 -3.9389348 -4.0689616 -4.1291075 -4.1663365 -4.1764627 -4.1561785 -4.1401582][-4.2138448 -4.188612 -4.1296554 -4.07075 -4.0272775 -3.941364 -3.773648 -3.6959758 -3.8295269 -3.9750195 -4.0449467 -4.0890069 -4.1033111 -4.0809493 -4.0625305][-4.2050834 -4.181922 -4.1276975 -4.0703154 -4.029604 -3.9586728 -3.8258719 -3.7569304 -3.8430247 -3.9510732 -4.009089 -4.0561404 -4.0745988 -4.0496836 -4.0250959][-4.2327375 -4.2158823 -4.1730003 -4.1237388 -4.0875883 -4.0410585 -3.9682059 -3.9308615 -3.9686344 -4.0254097 -4.0610604 -4.1014428 -4.1241417 -4.104856 -4.0786791][-4.2764115 -4.2657809 -4.2351651 -4.1984634 -4.1627312 -4.130753 -4.0990038 -4.0827346 -4.0969658 -4.1252675 -4.14966 -4.1823654 -4.2062311 -4.1974978 -4.1781154][-4.3116097 -4.3057508 -4.2871928 -4.2621655 -4.2271805 -4.1978331 -4.1845765 -4.1760449 -4.178967 -4.1957507 -4.2168636 -4.2433491 -4.2646708 -4.2647753 -4.2540727][-4.3151278 -4.3115931 -4.3027496 -4.2879205 -4.2546659 -4.222456 -4.2140956 -4.2081795 -4.20285 -4.2131252 -4.2315869 -4.2536283 -4.2732925 -4.2787323 -4.27284][-4.2840681 -4.2849617 -4.2863288 -4.2820187 -4.2546549 -4.2228408 -4.212512 -4.2042441 -4.1889892 -4.1879382 -4.1992617 -4.2203469 -4.2402534 -4.2460561 -4.2416892][-4.2342639 -4.2427297 -4.2553182 -4.2631125 -4.24734 -4.2222266 -4.2123532 -4.2022281 -4.1804824 -4.1633906 -4.1609726 -4.1753564 -4.1924219 -4.1940236 -4.1873627]]...]
INFO - root - 2017-12-07 10:55:59.543848: step 410, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 60h:21m:37s remains)
INFO - root - 2017-12-07 10:56:06.249403: step 420, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 57h:38m:00s remains)
INFO - root - 2017-12-07 10:56:13.103853: step 430, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 65h:13m:35s remains)
INFO - root - 2017-12-07 10:56:19.964598: step 440, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 67h:07m:16s remains)
INFO - root - 2017-12-07 10:56:26.870894: step 450, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 63h:26m:35s remains)
INFO - root - 2017-12-07 10:56:33.742372: step 460, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 58h:14m:15s remains)
INFO - root - 2017-12-07 10:56:40.550092: step 470, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 59h:08m:17s remains)
INFO - root - 2017-12-07 10:56:47.363065: step 480, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 65h:28m:56s remains)
INFO - root - 2017-12-07 10:56:54.078226: step 490, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 65h:42m:30s remains)
INFO - root - 2017-12-07 10:57:00.976235: step 500, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.648 sec/batch; 59h:43m:30s remains)
2017-12-07 10:57:01.622100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2497864 -4.234128 -4.2282534 -4.2232151 -4.2166162 -4.2204952 -4.2282062 -4.230155 -4.2273273 -4.2123585 -4.1892452 -4.1633682 -4.1356006 -4.1143093 -4.0954294][-4.2156324 -4.1978717 -4.1906137 -4.1829715 -4.1758351 -4.1797957 -4.185637 -4.1924949 -4.2027812 -4.1996264 -4.1809936 -4.1536694 -4.119174 -4.0934486 -4.0756745][-4.1841912 -4.1641669 -4.1513562 -4.1374288 -4.1251206 -4.1303129 -4.1433272 -4.1654811 -4.1895533 -4.1942019 -4.1684828 -4.1307726 -4.0903959 -4.0656247 -4.0547123][-4.1629686 -4.1368752 -4.1123004 -4.0895128 -4.0687141 -4.0777674 -4.1067452 -4.1482148 -4.1835551 -4.1898112 -4.1544724 -4.1063223 -4.0662069 -4.0473194 -4.0433679][-4.1376238 -4.1057897 -4.0703526 -4.0404878 -4.02041 -4.0336523 -4.0710478 -4.1196346 -4.1615267 -4.1715608 -4.1329265 -4.0848856 -4.0512753 -4.0433912 -4.0467982][-4.115602 -4.0842457 -4.0459404 -4.0110283 -3.9896719 -3.9957986 -4.0244656 -4.0661254 -4.1105733 -4.1241117 -4.0834141 -4.0400462 -4.0265546 -4.0357366 -4.0497417][-4.1137547 -4.089292 -4.0535173 -4.0133662 -3.9773965 -3.956286 -3.9523838 -3.9690011 -4.0076389 -4.0289426 -3.9996798 -3.9738839 -3.9868882 -4.0158634 -4.043674][-4.1121078 -4.0970607 -4.0700717 -4.0340886 -3.9853895 -3.9288063 -3.8700473 -3.8406203 -3.8791275 -3.9319053 -3.9394753 -3.9434707 -3.9779029 -4.012784 -4.0417][-4.1022091 -4.0868158 -4.0639052 -4.0346341 -3.988276 -3.9234488 -3.837923 -3.7778618 -3.8156886 -3.8920674 -3.9291828 -3.9612861 -4.0052233 -4.0353584 -4.0567107][-4.0920482 -4.0695729 -4.0460219 -4.0258107 -3.9972181 -3.953341 -3.8880658 -3.841912 -3.8754642 -3.9357209 -3.9728277 -4.0169225 -4.0609407 -4.0827289 -4.0909681][-4.0860157 -4.0668797 -4.051847 -4.0448956 -4.0363932 -4.0168219 -3.9799337 -3.9576309 -3.9801247 -4.0095339 -4.0298505 -4.0691786 -4.1057377 -4.1194806 -4.1180038][-4.0945339 -4.0915275 -4.0884385 -4.0880485 -4.0866175 -4.0784793 -4.0600095 -4.0502505 -4.0613174 -4.0671611 -4.0672312 -4.089972 -4.116663 -4.125205 -4.1219358][-4.1190491 -4.1277761 -4.1295652 -4.1271367 -4.12133 -4.1129923 -4.1013107 -4.0926352 -4.0933895 -4.0895529 -4.080348 -4.0907912 -4.1109524 -4.1198 -4.1237][-4.140233 -4.1497936 -4.1517882 -4.1450763 -4.135 -4.1233811 -4.1145825 -4.1071515 -4.1026444 -4.1009393 -4.0960636 -4.1031895 -4.1144018 -4.1234746 -4.1345091][-4.1526504 -4.1610069 -4.1635551 -4.15616 -4.14452 -4.1334782 -4.1263814 -4.1242065 -4.124331 -4.1280437 -4.1285872 -4.1280317 -4.1272421 -4.1331148 -4.1460724]]...]
INFO - root - 2017-12-07 10:57:08.428895: step 510, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 66h:35m:16s remains)
INFO - root - 2017-12-07 10:57:15.211856: step 520, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 64h:49m:04s remains)
INFO - root - 2017-12-07 10:57:21.897061: step 530, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 63h:09m:02s remains)
INFO - root - 2017-12-07 10:57:28.659554: step 540, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 57h:35m:58s remains)
INFO - root - 2017-12-07 10:57:35.284703: step 550, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 65h:48m:27s remains)
INFO - root - 2017-12-07 10:57:42.119459: step 560, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 65h:55m:02s remains)
INFO - root - 2017-12-07 10:57:48.853056: step 570, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 61h:27m:37s remains)
INFO - root - 2017-12-07 10:57:55.666439: step 580, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 59h:04m:49s remains)
INFO - root - 2017-12-07 10:58:02.240214: step 590, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 59h:51m:08s remains)
INFO - root - 2017-12-07 10:58:09.054681: step 600, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 67h:42m:26s remains)
2017-12-07 10:58:09.729979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19446 -4.1773558 -4.1688943 -4.1689811 -4.170783 -4.172091 -4.1897106 -4.2231975 -4.2513213 -4.2671742 -4.2740993 -4.2580752 -4.2245431 -4.20392 -4.2080746][-4.1976638 -4.1885629 -4.1837945 -4.1780691 -4.1737194 -4.1712918 -4.1888971 -4.2232027 -4.2498746 -4.2624578 -4.2703533 -4.2619853 -4.2404165 -4.228982 -4.2352319][-4.1892133 -4.1819592 -4.18381 -4.1835957 -4.1817245 -4.1824255 -4.2024393 -4.2357588 -4.2561893 -4.2619939 -4.2641006 -4.2560692 -4.2416506 -4.2373772 -4.2502522][-4.1748209 -4.1652064 -4.1712704 -4.1762309 -4.1746688 -4.1736789 -4.1926684 -4.2205873 -4.2350864 -4.2371087 -4.2360916 -4.22911 -4.2184086 -4.2187872 -4.2382221][-4.1624193 -4.1512871 -4.1564555 -4.1651692 -4.1590333 -4.1481137 -4.1589208 -4.1793318 -4.1904716 -4.1945844 -4.1998043 -4.1979184 -4.1879988 -4.1873417 -4.2052112][-4.1417642 -4.1323357 -4.1355534 -4.1421885 -4.1292024 -4.107697 -4.1096573 -4.1290054 -4.14642 -4.163444 -4.177453 -4.1753731 -4.1602049 -4.1506524 -4.1590385][-4.1210341 -4.1146226 -4.1080003 -4.0971503 -4.0665345 -4.0293808 -4.0240378 -4.0549331 -4.093616 -4.1278286 -4.1481285 -4.14067 -4.11248 -4.0944653 -4.0957942][-4.1143861 -4.110878 -4.0899234 -4.0525856 -3.9972267 -3.9421597 -3.9306622 -3.9745708 -4.0306191 -4.0677013 -4.0820632 -4.0694489 -4.0320487 -4.0125709 -4.0229955][-4.108943 -4.1055222 -4.0797124 -4.0282412 -3.9622219 -3.9039133 -3.8896985 -3.9269223 -3.9731073 -3.9923122 -3.9942889 -3.981214 -3.9473004 -3.9432888 -3.9798799][-4.1258039 -4.1204252 -4.0997505 -4.0542192 -3.9984674 -3.9507263 -3.9331474 -3.9500649 -3.9684608 -3.9642522 -3.9526498 -3.9377782 -3.912158 -3.9260645 -3.9831622][-4.1590562 -4.1575603 -4.1479783 -4.1187372 -4.081892 -4.0507927 -4.0341706 -4.0338907 -4.0337529 -4.0182195 -3.9994955 -3.9818954 -3.9610398 -3.977289 -4.029901][-4.1931791 -4.1876817 -4.1816931 -4.1666307 -4.1486893 -4.1370173 -4.1330314 -4.1317744 -4.1272497 -4.1106563 -4.0926008 -4.0749454 -4.0583539 -4.0683818 -4.101717][-4.2063856 -4.1943078 -4.18846 -4.1851387 -4.1837907 -4.1908555 -4.2006416 -4.2050056 -4.2032075 -4.193646 -4.182066 -4.1681714 -4.1547112 -4.1576653 -4.1721635][-4.2032251 -4.1909494 -4.1810327 -4.1826291 -4.1944585 -4.2156596 -4.2337713 -4.2423315 -4.2438207 -4.2383461 -4.2302971 -4.2213531 -4.212347 -4.2122784 -4.2164326][-4.2005706 -4.1915083 -4.1782317 -4.17977 -4.1988053 -4.2256622 -4.2456684 -4.2545881 -4.25556 -4.250484 -4.2428608 -4.2358608 -4.2289391 -4.2291818 -4.2321639]]...]
INFO - root - 2017-12-07 10:58:16.475054: step 610, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.620 sec/batch; 57h:12m:13s remains)
INFO - root - 2017-12-07 10:58:23.249530: step 620, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 60h:30m:05s remains)
INFO - root - 2017-12-07 10:58:30.123905: step 630, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 65h:59m:52s remains)
INFO - root - 2017-12-07 10:58:36.851915: step 640, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 64h:57m:29s remains)
INFO - root - 2017-12-07 10:58:43.643493: step 650, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 62h:31m:02s remains)
INFO - root - 2017-12-07 10:58:50.331273: step 660, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 58h:22m:42s remains)
INFO - root - 2017-12-07 10:58:57.107691: step 670, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 59h:34m:56s remains)
INFO - root - 2017-12-07 10:59:03.879932: step 680, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 63h:01m:01s remains)
INFO - root - 2017-12-07 10:59:10.522329: step 690, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 65h:07m:22s remains)
INFO - root - 2017-12-07 10:59:17.317573: step 700, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 62h:43m:47s remains)
2017-12-07 10:59:18.032469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2195935 -4.2127686 -4.2074261 -4.2016959 -4.1968389 -4.1927347 -4.1883259 -4.1825428 -4.1791258 -4.1790557 -4.1799273 -4.1776819 -4.1676078 -4.1523275 -4.139554][-4.2413764 -4.2337074 -4.2280869 -4.2223673 -4.2172561 -4.2119846 -4.2051306 -4.1958036 -4.1888137 -4.185782 -4.1839066 -4.1796975 -4.1699467 -4.156651 -4.1449275][-4.2519732 -4.2441721 -4.2379532 -4.2334409 -4.231266 -4.2291765 -4.2237639 -4.2151818 -4.2083864 -4.2058764 -4.2044549 -4.2007241 -4.1927309 -4.1806893 -4.1681542][-4.2367554 -4.2262115 -4.2162619 -4.2125883 -4.2148461 -4.2180195 -4.2164669 -4.2117529 -4.210371 -4.2157607 -4.2215552 -4.2226524 -4.2179751 -4.2066789 -4.1921296][-4.2057462 -4.1875944 -4.1704545 -4.1657615 -4.1720433 -4.1782746 -4.1767073 -4.1724954 -4.177875 -4.195817 -4.2153053 -4.2278585 -4.2314582 -4.2236309 -4.209332][-4.1773129 -4.1514454 -4.1287713 -4.1231117 -4.1302409 -4.1337638 -4.1253057 -4.1156425 -4.123672 -4.1524944 -4.1858921 -4.2127414 -4.2287035 -4.2289996 -4.2193246][-4.1665163 -4.1411734 -4.1205645 -4.1157055 -4.1205406 -4.1165805 -4.0971637 -4.0769725 -4.0816441 -4.1132507 -4.1528764 -4.1887736 -4.2138376 -4.2218504 -4.2177992][-4.1659713 -4.150413 -4.1396594 -4.1401453 -4.1452665 -4.1388693 -4.1176147 -4.0944548 -4.0932307 -4.1154938 -4.1471434 -4.1782761 -4.2010741 -4.209259 -4.2068138][-4.1761088 -4.16914 -4.1654811 -4.1686354 -4.1736984 -4.1685195 -4.1531277 -4.1381836 -4.1399865 -4.1562557 -4.1785645 -4.1989117 -4.2111454 -4.2108922 -4.2028313][-4.196775 -4.1927586 -4.18878 -4.1878371 -4.1870313 -4.1791706 -4.1664605 -4.1592546 -4.1677442 -4.1859293 -4.206388 -4.222918 -4.2289944 -4.2217517 -4.2074814][-4.2233319 -4.2176304 -4.2097144 -4.2024455 -4.1940937 -4.1800876 -4.1643677 -4.1594276 -4.1701174 -4.1889467 -4.2095237 -4.2263288 -4.2331576 -4.2258606 -4.2106414][-4.2432418 -4.2355604 -4.2259293 -4.2171993 -4.2088184 -4.195992 -4.1809926 -4.1754055 -4.1826534 -4.1963286 -4.2123356 -4.2255816 -4.2309232 -4.2246823 -4.2122035][-4.2440553 -4.2364697 -4.2284517 -4.223279 -4.2200279 -4.2136259 -4.204545 -4.2001743 -4.2031913 -4.2093759 -4.2176208 -4.224597 -4.2255058 -4.2181339 -4.2081513][-4.2361927 -4.2286053 -4.2211947 -4.2175965 -4.2165165 -4.2152214 -4.21402 -4.2143173 -4.21677 -4.2186928 -4.2205958 -4.2225475 -4.2207823 -4.2127204 -4.2035012][-4.2291536 -4.2212152 -4.2130361 -4.2089677 -4.2083793 -4.2100058 -4.2134719 -4.2174592 -4.2203164 -4.2202277 -4.2191906 -4.2190595 -4.2169704 -4.2105556 -4.2035017]]...]
INFO - root - 2017-12-07 10:59:24.853648: step 710, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 67h:09m:01s remains)
INFO - root - 2017-12-07 10:59:31.692780: step 720, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 67h:25m:58s remains)
INFO - root - 2017-12-07 10:59:38.414820: step 730, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 62h:32m:04s remains)
INFO - root - 2017-12-07 10:59:45.129746: step 740, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.611 sec/batch; 56h:17m:38s remains)
INFO - root - 2017-12-07 10:59:51.898605: step 750, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 61h:11m:02s remains)
INFO - root - 2017-12-07 10:59:58.768952: step 760, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 66h:48m:24s remains)
INFO - root - 2017-12-07 11:00:05.660508: step 770, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 67h:20m:32s remains)
INFO - root - 2017-12-07 11:00:12.600458: step 780, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 61h:35m:59s remains)
INFO - root - 2017-12-07 11:00:19.196551: step 790, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 57h:53m:54s remains)
INFO - root - 2017-12-07 11:00:25.987126: step 800, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.667 sec/batch; 61h:29m:53s remains)
2017-12-07 11:00:26.792493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2857718 -4.2729697 -4.2708206 -4.2693758 -4.255353 -4.2259517 -4.2029619 -4.196218 -4.1978574 -4.2065487 -4.21211 -4.215693 -4.2119584 -4.1993737 -4.1948805][-4.2941971 -4.2819514 -4.2791462 -4.2762794 -4.262105 -4.2321639 -4.2041259 -4.18743 -4.1818514 -4.1861882 -4.1899061 -4.193923 -4.1917214 -4.179914 -4.176374][-4.2883515 -4.2763643 -4.2726736 -4.2674961 -4.2506232 -4.2211914 -4.1918077 -4.1675887 -4.1551976 -4.1572919 -4.1639023 -4.1731157 -4.1766496 -4.1704183 -4.1698256][-4.2594409 -4.245348 -4.2387033 -4.23138 -4.2142849 -4.1886878 -4.1634955 -4.1384592 -4.1248946 -4.129231 -4.1418204 -4.1547213 -4.1597519 -4.159061 -4.1623268][-4.2208986 -4.203455 -4.1955986 -4.187561 -4.173162 -4.1525497 -4.1299491 -4.103816 -4.0909281 -4.1027861 -4.1248293 -4.1407413 -4.1465077 -4.1507668 -4.1598988][-4.1789494 -4.1604357 -4.1494908 -4.1375895 -4.1227803 -4.1041288 -4.0792437 -4.0482922 -4.0407472 -4.072165 -4.11485 -4.1414127 -4.1512327 -4.15894 -4.1676741][-4.15256 -4.1367016 -4.1226993 -4.1031189 -4.0807104 -4.0523534 -4.0086842 -3.9592035 -3.9533389 -4.0104451 -4.0805521 -4.1248317 -4.14034 -4.1410146 -4.1393876][-4.1490707 -4.1402922 -4.1269522 -4.1006331 -4.0665836 -4.0252004 -3.9663777 -3.9011002 -3.8916039 -3.9614089 -4.0450149 -4.0982566 -4.1165123 -4.1103911 -4.0973425][-4.1671214 -4.1645026 -4.1526036 -4.1259193 -4.0951753 -4.0629849 -4.0196424 -3.96895 -3.9588075 -4.0113811 -4.0790062 -4.1198754 -4.1309991 -4.1192245 -4.0998278][-4.2045484 -4.2043395 -4.1926274 -4.1682348 -4.1443233 -4.1226678 -4.0920982 -4.0550756 -4.0443382 -4.0778151 -4.1255422 -4.1551895 -4.159452 -4.1457205 -4.128468][-4.2497182 -4.24766 -4.2378407 -4.2188754 -4.20077 -4.1864476 -4.1638632 -4.1347761 -4.1222053 -4.1390343 -4.1690435 -4.1890821 -4.1898117 -4.1740456 -4.1594491][-4.290987 -4.2895617 -4.2837057 -4.2719479 -4.2579989 -4.2455235 -4.2267165 -4.2045574 -4.1938968 -4.2016678 -4.2190304 -4.2310443 -4.2301354 -4.2140179 -4.2008224][-4.3137317 -4.3135338 -4.3114972 -4.3062778 -4.2966471 -4.2866635 -4.2735038 -4.2597709 -4.2549081 -4.2587748 -4.2681828 -4.274498 -4.2738566 -4.2609811 -4.2511835][-4.3203168 -4.3215423 -4.3211379 -4.3183107 -4.3127084 -4.3073163 -4.3001342 -4.2928143 -4.2916837 -4.2937965 -4.2967677 -4.2975941 -4.2972918 -4.2902155 -4.2863607][-4.3198042 -4.3215623 -4.3222108 -4.3205013 -4.3173437 -4.315454 -4.3126264 -4.3092432 -4.3091726 -4.3097277 -4.308269 -4.3044424 -4.3021865 -4.2980928 -4.2958822]]...]
INFO - root - 2017-12-07 11:00:33.475528: step 810, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 62h:59m:29s remains)
INFO - root - 2017-12-07 11:00:40.182552: step 820, loss = 2.02, batch loss = 1.97 (12.6 examples/sec; 0.637 sec/batch; 58h:39m:55s remains)
INFO - root - 2017-12-07 11:00:46.912017: step 830, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 63h:36m:55s remains)
INFO - root - 2017-12-07 11:00:53.702038: step 840, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 65h:32m:52s remains)
INFO - root - 2017-12-07 11:01:00.578305: step 850, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:26m:00s remains)
INFO - root - 2017-12-07 11:01:07.169127: step 860, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 59h:55m:50s remains)
INFO - root - 2017-12-07 11:01:13.966090: step 870, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 63h:14m:15s remains)
INFO - root - 2017-12-07 11:01:20.790175: step 880, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 64h:27m:53s remains)
INFO - root - 2017-12-07 11:01:27.336043: step 890, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 64h:42m:52s remains)
INFO - root - 2017-12-07 11:01:34.115022: step 900, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 62h:27m:44s remains)
2017-12-07 11:01:34.850223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30059 -4.2887111 -4.2808 -4.2794833 -4.2839 -4.2931261 -4.3040042 -4.3133984 -4.3191481 -4.3198304 -4.3166561 -4.3143311 -4.3141918 -4.3159647 -4.3189316][-4.2929072 -4.2741013 -4.2584052 -4.2507253 -4.2540741 -4.2651157 -4.2818737 -4.298759 -4.312664 -4.3214936 -4.3222842 -4.3214431 -4.3206749 -4.3181205 -4.3157482][-4.2794337 -4.2533545 -4.2269959 -4.2100859 -4.2091413 -4.21996 -4.242806 -4.2676678 -4.2912364 -4.3092718 -4.3188128 -4.3246617 -4.3285265 -4.3251028 -4.31726][-4.2605028 -4.2263222 -4.1858182 -4.1550093 -4.1428885 -4.149889 -4.1776052 -4.2159624 -4.2566962 -4.2846761 -4.3006058 -4.3159032 -4.3287663 -4.3300815 -4.320106][-4.2448459 -4.2025523 -4.1487556 -4.0994582 -4.065506 -4.0560904 -4.080688 -4.1344147 -4.201139 -4.2486029 -4.2735982 -4.2953973 -4.3138747 -4.3217211 -4.3165078][-4.2375221 -4.1914396 -4.1269612 -4.0565248 -3.9930968 -3.9498308 -3.9528761 -4.0133562 -4.109632 -4.1869802 -4.2308831 -4.2618804 -4.284224 -4.2969151 -4.3001633][-4.2398372 -4.1983252 -4.1352115 -4.0581784 -3.9727268 -3.8884249 -3.8442297 -3.886085 -3.9998751 -4.1028976 -4.1685696 -4.2148829 -4.2479262 -4.2675071 -4.2780819][-4.2515888 -4.2203355 -4.1700287 -4.103981 -4.02387 -3.9271364 -3.8412979 -3.8298645 -3.9161122 -4.0158815 -4.0965233 -4.1610713 -4.2126207 -4.2452054 -4.2624226][-4.2620869 -4.24205 -4.2085328 -4.1592574 -4.0985422 -4.0238619 -3.9420445 -3.8946583 -3.9185352 -3.9744217 -4.0434647 -4.1169095 -4.1843247 -4.2320476 -4.25872][-4.2680445 -4.2574692 -4.2404065 -4.2089391 -4.1653275 -4.1170793 -4.0649791 -4.0217605 -4.0088768 -4.01828 -4.0519161 -4.109478 -4.1775827 -4.2316885 -4.2634716][-4.2765212 -4.2707119 -4.2628374 -4.2463794 -4.2206631 -4.1931095 -4.1669574 -4.14151 -4.1278458 -4.1236067 -4.1293182 -4.15702 -4.2062154 -4.2510972 -4.27963][-4.2879915 -4.2819448 -4.277245 -4.2698641 -4.2575836 -4.2452035 -4.235642 -4.2263203 -4.2214651 -4.2216268 -4.2206235 -4.2292743 -4.25619 -4.2845383 -4.3040905][-4.3028579 -4.2975254 -4.2937131 -4.2896018 -4.2843661 -4.2798996 -4.2767992 -4.2741942 -4.2749915 -4.279911 -4.2815194 -4.2852488 -4.2975388 -4.3125329 -4.3242435][-4.3169131 -4.31337 -4.3103032 -4.3075023 -4.3043408 -4.3021088 -4.3015118 -4.3004112 -4.3014574 -4.3070464 -4.3109727 -4.3128672 -4.3181577 -4.3259859 -4.3329568][-4.32539 -4.3242025 -4.3222342 -4.3203282 -4.3189421 -4.3183031 -4.3187957 -4.3187642 -4.3188095 -4.321568 -4.3236227 -4.32423 -4.3256836 -4.3291388 -4.3325806]]...]
INFO - root - 2017-12-07 11:01:41.645110: step 910, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 65h:46m:57s remains)
INFO - root - 2017-12-07 11:01:48.343870: step 920, loss = 2.09, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 60h:39m:59s remains)
INFO - root - 2017-12-07 11:01:55.038909: step 930, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 58h:33m:12s remains)
INFO - root - 2017-12-07 11:02:01.776567: step 940, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 58h:20m:27s remains)
INFO - root - 2017-12-07 11:02:08.519760: step 950, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.660 sec/batch; 60h:49m:43s remains)
INFO - root - 2017-12-07 11:02:15.263389: step 960, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 67h:07m:30s remains)
INFO - root - 2017-12-07 11:02:22.105433: step 970, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 64h:13m:42s remains)
INFO - root - 2017-12-07 11:02:28.805459: step 980, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 60h:26m:50s remains)
INFO - root - 2017-12-07 11:02:35.303677: step 990, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 57h:51m:20s remains)
INFO - root - 2017-12-07 11:02:42.012315: step 1000, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 59h:42m:00s remains)
2017-12-07 11:02:42.820163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2790556 -4.2700949 -4.2650943 -4.2690024 -4.2816238 -4.2943783 -4.3025203 -4.2981577 -4.2862082 -4.27076 -4.2631865 -4.2664781 -4.2738461 -4.2897019 -4.30653][-4.2488518 -4.2393942 -4.2327695 -4.2330022 -4.2469521 -4.258822 -4.2656536 -4.259028 -4.2445183 -4.22675 -4.2190847 -4.2249174 -4.2355752 -4.2530451 -4.2713318][-4.2343373 -4.2220945 -4.2115932 -4.2045159 -4.2108507 -4.2143278 -4.216126 -4.2135749 -4.2101965 -4.2017708 -4.1968818 -4.20395 -4.2152815 -4.228797 -4.2421665][-4.2246585 -4.208581 -4.1970696 -4.1815524 -4.1670117 -4.1492872 -4.1406169 -4.151402 -4.1718483 -4.1814141 -4.1831965 -4.193902 -4.2045622 -4.21116 -4.2189336][-4.20265 -4.1834292 -4.1717591 -4.1474628 -4.1063957 -4.0573125 -4.0343304 -4.0646262 -4.1182213 -4.1496043 -4.1623864 -4.1830921 -4.2006049 -4.2046375 -4.2080564][-4.186316 -4.1635571 -4.1472073 -4.1138296 -4.0475211 -3.9613664 -3.914911 -3.9653349 -4.0553961 -4.10917 -4.1337414 -4.1636758 -4.1903687 -4.2020283 -4.2096524][-4.179831 -4.15712 -4.1380067 -4.0983319 -4.02096 -3.9186583 -3.8583171 -3.911912 -4.0108624 -4.0643892 -4.088253 -4.1222687 -4.1563587 -4.183322 -4.2064357][-4.1766119 -4.1526451 -4.13144 -4.0964108 -4.0337491 -3.9595821 -3.9190993 -3.9566698 -4.0184073 -4.0418949 -4.0536757 -4.0836372 -4.1167197 -4.1535177 -4.1885972][-4.1675258 -4.1473274 -4.1287832 -4.1051846 -4.0683179 -4.0297966 -4.0106072 -4.0323272 -4.0553169 -4.0486412 -4.0474005 -4.070785 -4.1003428 -4.13739 -4.1751828][-4.1783295 -4.1646037 -4.1530328 -4.1391153 -4.121419 -4.112093 -4.1077476 -4.1191974 -4.1190667 -4.0917683 -4.0766573 -4.0936112 -4.1212206 -4.1562529 -4.1903305][-4.218595 -4.2040787 -4.1961503 -4.1890945 -4.1830544 -4.1894851 -4.195559 -4.2046471 -4.1957288 -4.1612086 -4.1406364 -4.1537724 -4.1782756 -4.2069182 -4.2311411][-4.26914 -4.2536511 -4.2446771 -4.2381635 -4.2384462 -4.2528377 -4.2640958 -4.271884 -4.2620549 -4.2306452 -4.2097445 -4.2180519 -4.2379456 -4.2588692 -4.2738752][-4.3060517 -4.2930551 -4.2868695 -4.2836351 -4.2875714 -4.3012018 -4.3098412 -4.3141413 -4.3050656 -4.2805123 -4.2627654 -4.2670612 -4.2823234 -4.2960448 -4.30442][-4.3299 -4.3208261 -4.3170128 -4.3165393 -4.3205576 -4.3278461 -4.3319011 -4.3339896 -4.3293905 -4.3151293 -4.3033504 -4.3061633 -4.3172832 -4.326375 -4.3306465][-4.3427892 -4.337429 -4.3366446 -4.3381643 -4.3401752 -4.3413334 -4.3413415 -4.34204 -4.3408136 -4.3346043 -4.328866 -4.3305368 -4.3372264 -4.3431058 -4.3450484]]...]
INFO - root - 2017-12-07 11:02:49.566163: step 1010, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 61h:36m:17s remains)
INFO - root - 2017-12-07 11:02:56.321265: step 1020, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 58h:47m:20s remains)
INFO - root - 2017-12-07 11:03:03.146241: step 1030, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 60h:28m:36s remains)
INFO - root - 2017-12-07 11:03:10.043496: step 1040, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 67h:36m:53s remains)
INFO - root - 2017-12-07 11:03:16.819030: step 1050, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 62h:07m:05s remains)
INFO - root - 2017-12-07 11:03:23.513596: step 1060, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 62h:48m:38s remains)
INFO - root - 2017-12-07 11:03:30.273323: step 1070, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 58h:16m:44s remains)
INFO - root - 2017-12-07 11:03:37.150187: step 1080, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 66h:15m:00s remains)
INFO - root - 2017-12-07 11:03:43.791125: step 1090, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 63h:53m:09s remains)
INFO - root - 2017-12-07 11:03:50.536300: step 1100, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 61h:51m:42s remains)
2017-12-07 11:03:51.200028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1958923 -4.1569576 -4.1130824 -4.0941873 -4.1005468 -4.1222515 -4.1650825 -4.2067342 -4.2355485 -4.2491059 -4.2601833 -4.2636585 -4.2753325 -4.2867551 -4.2917662][-4.1972408 -4.1594396 -4.1032166 -4.051024 -4.0201597 -4.0283284 -4.0897946 -4.1598182 -4.2050223 -4.2322373 -4.2509584 -4.2569928 -4.2712779 -4.28575 -4.2958517][-4.2107348 -4.1741734 -4.1139646 -4.0434585 -3.9835665 -3.9727035 -4.0371656 -4.1212034 -4.1756611 -4.2105336 -4.2341533 -4.2466259 -4.2649493 -4.2820415 -4.2936931][-4.2318087 -4.2047291 -4.15738 -4.0953918 -4.0303087 -4.0002685 -4.0418487 -4.11024 -4.1617641 -4.1955757 -4.222259 -4.2395391 -4.2612867 -4.2824869 -4.2940559][-4.2323093 -4.2094383 -4.1733389 -4.1291018 -4.0750151 -4.0425444 -4.0604734 -4.100184 -4.1432128 -4.178175 -4.2112036 -4.2346992 -4.25841 -4.282011 -4.2959709][-4.2084017 -4.1885853 -4.153882 -4.1164017 -4.0716276 -4.0368671 -4.0320673 -4.0454068 -4.0842838 -4.1307006 -4.1761951 -4.2099347 -4.238462 -4.2692075 -4.2904673][-4.179729 -4.163403 -4.1354976 -4.1046643 -4.0638018 -4.0207276 -3.9942839 -3.9826279 -4.0119905 -4.0704131 -4.1300793 -4.1740866 -4.2097993 -4.2478404 -4.278605][-4.1810136 -4.1632257 -4.1370692 -4.1124759 -4.0788913 -4.0354834 -3.9987311 -3.9730296 -3.9874647 -4.0438714 -4.1039276 -4.1495562 -4.1885114 -4.2318974 -4.2701006][-4.2097821 -4.1900406 -4.164309 -4.14118 -4.1127782 -4.0784197 -4.0482693 -4.0247469 -4.02564 -4.0615783 -4.1079354 -4.1478686 -4.1876707 -4.2336764 -4.2753882][-4.2277789 -4.2143908 -4.1926117 -4.1677961 -4.1378889 -4.1074247 -4.0849805 -4.068152 -4.0635023 -4.08088 -4.1176176 -4.1567011 -4.1954384 -4.2423763 -4.2844043][-4.2171216 -4.2145805 -4.2038755 -4.1834192 -4.1529627 -4.1196222 -4.0965595 -4.0828438 -4.0762014 -4.0839982 -4.1167569 -4.1581216 -4.1984134 -4.2461615 -4.2869029][-4.2034497 -4.2099414 -4.2109518 -4.2018905 -4.1777358 -4.1458898 -4.1201305 -4.1022778 -4.0895853 -4.0898309 -4.1161227 -4.1570973 -4.1982656 -4.2431827 -4.28112][-4.2149315 -4.2247872 -4.2314506 -4.2307978 -4.2182169 -4.1943836 -4.1693811 -4.1450844 -4.1231079 -4.1129179 -4.1288977 -4.1619434 -4.1992707 -4.2402773 -4.2741175][-4.2358003 -4.2408247 -4.24523 -4.2484732 -4.2480955 -4.2378073 -4.2214117 -4.1953149 -4.1673017 -4.1479855 -4.1531167 -4.17538 -4.2059178 -4.2400947 -4.2682433][-4.2402582 -4.23993 -4.240428 -4.243484 -4.2478962 -4.2469745 -4.2406573 -4.2173486 -4.1890349 -4.1671882 -4.1674757 -4.1832232 -4.2087841 -4.2370324 -4.260427]]...]
INFO - root - 2017-12-07 11:03:58.119003: step 1110, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.732 sec/batch; 67h:23m:41s remains)
INFO - root - 2017-12-07 11:04:05.067810: step 1120, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 68h:19m:54s remains)
INFO - root - 2017-12-07 11:04:11.919426: step 1130, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 59h:51m:27s remains)
INFO - root - 2017-12-07 11:04:18.637462: step 1140, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.609 sec/batch; 56h:03m:50s remains)
INFO - root - 2017-12-07 11:04:25.482262: step 1150, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 62h:53m:53s remains)
INFO - root - 2017-12-07 11:04:32.300900: step 1160, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 65h:57m:03s remains)
INFO - root - 2017-12-07 11:04:38.813913: step 1170, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:32m:53s remains)
INFO - root - 2017-12-07 11:04:45.419577: step 1180, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 57h:17m:01s remains)
INFO - root - 2017-12-07 11:04:52.046531: step 1190, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 61h:10m:50s remains)
INFO - root - 2017-12-07 11:04:58.887860: step 1200, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 65h:35m:49s remains)
2017-12-07 11:04:59.613274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2532959 -4.2451682 -4.2533607 -4.2722697 -4.2848616 -4.2826943 -4.2814827 -4.288424 -4.3009553 -4.308465 -4.3036537 -4.2924871 -4.2793608 -4.273756 -4.2705832][-4.2153721 -4.2006774 -4.2098684 -4.24191 -4.2676625 -4.2729287 -4.2762985 -4.2900777 -4.3117971 -4.3257365 -4.3230896 -4.3047085 -4.2794867 -4.2641196 -4.2511177][-4.1714821 -4.1588688 -4.1731091 -4.2146511 -4.2454829 -4.2523489 -4.2539592 -4.2690244 -4.2957125 -4.3148136 -4.3204556 -4.3085814 -4.2846265 -4.2644973 -4.2427425][-4.1637449 -4.1627355 -4.180912 -4.2204056 -4.2456756 -4.2436695 -4.2309732 -4.2346144 -4.2607727 -4.282444 -4.2947445 -4.2965956 -4.2890882 -4.2739654 -4.2490077][-4.1802526 -4.1899948 -4.20967 -4.2400842 -4.2490549 -4.2249069 -4.1898794 -4.170958 -4.1954384 -4.2283931 -4.2522793 -4.272058 -4.2850556 -4.2827382 -4.2629013][-4.1822906 -4.1977968 -4.2159958 -4.2298174 -4.2169833 -4.1679521 -4.10112 -4.0461197 -4.0707827 -4.1389961 -4.1901221 -4.2298017 -4.2622628 -4.2799172 -4.2710829][-4.1833019 -4.2004576 -4.2146206 -4.2063265 -4.168694 -4.0946817 -3.9885275 -3.8849664 -3.9078126 -4.0262585 -4.1203356 -4.18618 -4.2347283 -4.2660275 -4.2678909][-4.1848645 -4.1961746 -4.2049823 -4.1837792 -4.1353955 -4.053021 -3.9366865 -3.822552 -3.8449709 -3.9805446 -4.0908947 -4.1671782 -4.2214594 -4.2568111 -4.2674031][-4.1741433 -4.1814857 -4.1912575 -4.1734796 -4.1370606 -4.0771918 -4.0012279 -3.9348218 -3.953464 -4.0417638 -4.1230292 -4.1847286 -4.2309341 -4.262414 -4.27583][-4.1824985 -4.1871266 -4.2004261 -4.195683 -4.1818218 -4.152545 -4.1212263 -4.098434 -4.112051 -4.1514463 -4.1905031 -4.2240725 -4.2518992 -4.2715287 -4.2851505][-4.2177277 -4.2162867 -4.2268405 -4.22981 -4.2248507 -4.214355 -4.2111969 -4.2150741 -4.2295256 -4.2429342 -4.2511868 -4.2593832 -4.2664962 -4.27359 -4.2860084][-4.2648768 -4.2606616 -4.2640934 -4.2606077 -4.2527204 -4.249886 -4.2616162 -4.2754936 -4.2844124 -4.2841854 -4.2774997 -4.2718225 -4.2701259 -4.2681141 -4.2769418][-4.2919469 -4.2842894 -4.2811322 -4.27359 -4.2634687 -4.261312 -4.2769861 -4.2889905 -4.2885027 -4.2750635 -4.2614946 -4.252069 -4.2475343 -4.2476654 -4.2548108][-4.2887678 -4.2793231 -4.2751851 -4.268455 -4.2614374 -4.2605991 -4.2707286 -4.2752872 -4.2671876 -4.2474589 -4.2338219 -4.2253923 -4.2184024 -4.2210255 -4.2331786][-4.276217 -4.2682309 -4.265492 -4.2625771 -4.2598057 -4.2622814 -4.2666812 -4.2660074 -4.257246 -4.2391362 -4.2286477 -4.2220535 -4.2152181 -4.2228122 -4.2409353]]...]
INFO - root - 2017-12-07 11:05:06.340595: step 1210, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 57h:27m:45s remains)
INFO - root - 2017-12-07 11:05:13.141658: step 1220, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.623 sec/batch; 57h:17m:13s remains)
INFO - root - 2017-12-07 11:05:19.948124: step 1230, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 65h:51m:39s remains)
INFO - root - 2017-12-07 11:05:26.767928: step 1240, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 65h:59m:38s remains)
INFO - root - 2017-12-07 11:05:33.519759: step 1250, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 60h:44m:25s remains)
INFO - root - 2017-12-07 11:05:40.258240: step 1260, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 57h:58m:15s remains)
INFO - root - 2017-12-07 11:05:47.074847: step 1270, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 57h:13m:56s remains)
INFO - root - 2017-12-07 11:05:53.906203: step 1280, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 66h:42m:47s remains)
INFO - root - 2017-12-07 11:06:00.605734: step 1290, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 63h:55m:33s remains)
INFO - root - 2017-12-07 11:06:07.361109: step 1300, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 64h:42m:55s remains)
2017-12-07 11:06:08.109377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2122493 -4.2270045 -4.2351594 -4.2345533 -4.2278104 -4.2080088 -4.15302 -4.070817 -4.0473118 -4.0868716 -4.1297464 -4.1827211 -4.237792 -4.2774587 -4.2959003][-4.1764536 -4.1913419 -4.2018404 -4.2044497 -4.2066655 -4.1964073 -4.1447906 -4.0614166 -4.0340905 -4.0715189 -4.1129413 -4.1665678 -4.2250409 -4.2699547 -4.291697][-4.1371551 -4.1492848 -4.1619759 -4.1704345 -4.1847868 -4.184031 -4.13398 -4.0494151 -4.0210423 -4.0612593 -4.1050515 -4.1613092 -4.2220531 -4.2678127 -4.2892766][-4.0990644 -4.1068487 -4.1216359 -4.136044 -4.1600032 -4.1655335 -4.1147094 -4.0268116 -4.001894 -4.0516157 -4.1028423 -4.1644106 -4.2272611 -4.2714577 -4.2903185][-4.0647564 -4.0674157 -4.0839338 -4.1002026 -4.1256504 -4.1319766 -4.0763264 -3.9826281 -3.9666777 -4.0326276 -4.0975742 -4.1684346 -4.23481 -4.2778854 -4.293694][-4.0565267 -4.0505686 -4.0624747 -4.0694375 -4.0833888 -4.083539 -4.0190768 -3.9198163 -3.9202836 -4.0075183 -4.088408 -4.1696172 -4.2405725 -4.2834516 -4.2970705][-4.0810866 -4.0690465 -4.07268 -4.0638471 -4.0600195 -4.047174 -3.9728656 -3.8693559 -3.8874719 -3.9913304 -4.0808587 -4.1683168 -4.2427654 -4.2861214 -4.2989378][-4.11324 -4.1010475 -4.0993328 -4.0795403 -4.0663962 -4.0449791 -3.9692426 -3.8732405 -3.9018731 -4.00624 -4.089601 -4.172266 -4.244586 -4.2866631 -4.2993097][-4.1488314 -4.1362233 -4.1291542 -4.1070843 -4.0938759 -4.0706697 -4.0042143 -3.927516 -3.9618073 -4.0535336 -4.120749 -4.18907 -4.2512865 -4.2878141 -4.2990646][-4.1819139 -4.168427 -4.1575413 -4.1387935 -4.1301155 -4.1069684 -4.0531287 -3.9952044 -4.0306244 -4.1075497 -4.1597667 -4.2125239 -4.2616014 -4.2908063 -4.2995839][-4.2070117 -4.1925688 -4.1810751 -4.1684127 -4.1648316 -4.1434317 -4.1018314 -4.0565996 -4.0869117 -4.1498113 -4.1914511 -4.233048 -4.2714887 -4.2945008 -4.3010049][-4.2288656 -4.2152443 -4.2040415 -4.1963797 -4.1960897 -4.1786542 -4.1480422 -4.1112366 -4.1334295 -4.1835384 -4.2173486 -4.2503314 -4.2812862 -4.2989984 -4.3035159][-4.2534151 -4.2437339 -4.2338748 -4.2290516 -4.2297263 -4.215517 -4.1927176 -4.1628876 -4.1772509 -4.2153521 -4.2419343 -4.268033 -4.2933526 -4.306509 -4.3085947][-4.2777524 -4.2717557 -4.2643156 -4.2616029 -4.262814 -4.2524981 -4.2362566 -4.2122087 -4.2203732 -4.2471857 -4.2675133 -4.2890172 -4.3092246 -4.3178124 -4.3167839][-4.296236 -4.2923098 -4.2877312 -4.2847948 -4.2858472 -4.2804995 -4.269701 -4.2507629 -4.2547936 -4.2739482 -4.2904892 -4.3092427 -4.3250289 -4.32927 -4.3258553]]...]
INFO - root - 2017-12-07 11:06:14.868629: step 1310, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 64h:43m:27s remains)
INFO - root - 2017-12-07 11:06:21.656227: step 1320, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 60h:53m:07s remains)
INFO - root - 2017-12-07 11:06:28.431781: step 1330, loss = 2.09, batch loss = 2.04 (12.8 examples/sec; 0.626 sec/batch; 57h:36m:36s remains)
INFO - root - 2017-12-07 11:06:35.326826: step 1340, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 59h:51m:41s remains)
INFO - root - 2017-12-07 11:06:42.266417: step 1350, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.760 sec/batch; 69h:54m:14s remains)
INFO - root - 2017-12-07 11:06:49.017072: step 1360, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 66h:27m:41s remains)
INFO - root - 2017-12-07 11:06:55.807831: step 1370, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 62h:54m:42s remains)
INFO - root - 2017-12-07 11:07:02.602761: step 1380, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 57h:17m:45s remains)
INFO - root - 2017-12-07 11:07:09.383284: step 1390, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 63h:23m:49s remains)
INFO - root - 2017-12-07 11:07:16.166823: step 1400, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 65h:46m:01s remains)
2017-12-07 11:07:16.983302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2041941 -4.2095337 -4.2126746 -4.2119837 -4.2068343 -4.2015095 -4.19988 -4.1953173 -4.1856594 -4.1781077 -4.1784296 -4.1807523 -4.1836095 -4.1876407 -4.1995077][-4.1863818 -4.1925654 -4.2005029 -4.203454 -4.1994529 -4.194633 -4.1939387 -4.1895514 -4.1819277 -4.1788926 -4.1846504 -4.1888442 -4.1901722 -4.1923938 -4.200171][-4.1812463 -4.1842909 -4.1909761 -4.1942482 -4.1907969 -4.1864705 -4.1865783 -4.1825867 -4.1774273 -4.1773171 -4.1866245 -4.1934776 -4.1957059 -4.1975169 -4.2024755][-4.1680441 -4.1638446 -4.1669393 -4.1701779 -4.1674566 -4.16235 -4.1621866 -4.1599455 -4.1577015 -4.1608677 -4.1746507 -4.1884184 -4.192802 -4.1938043 -4.1989202][-4.1451087 -4.1313119 -4.1293845 -4.1323495 -4.1283779 -4.119874 -4.1164355 -4.1137404 -4.1161513 -4.1247911 -4.1417909 -4.1616855 -4.1712351 -4.1734366 -4.1780696][-4.1378646 -4.1121793 -4.0984497 -4.0914354 -4.0765972 -4.0578294 -4.04686 -4.0457835 -4.0590272 -4.0833106 -4.1096044 -4.1352415 -4.1493945 -4.1553917 -4.1580434][-4.1697922 -4.1319642 -4.1015749 -4.0774183 -4.0433087 -4.0026789 -3.9756877 -3.9740403 -3.9983473 -4.046319 -4.0920134 -4.1282 -4.1499195 -4.1628275 -4.164669][-4.211802 -4.1703167 -4.131896 -4.0996704 -4.0541 -3.9949694 -3.9535215 -3.9458036 -3.9737937 -4.0377464 -4.1007786 -4.15062 -4.1835184 -4.2043629 -4.2084484][-4.2444177 -4.208375 -4.17459 -4.145905 -4.1049523 -4.0505452 -4.0134311 -4.0033917 -4.0256081 -4.0811315 -4.135201 -4.1796908 -4.2106738 -4.234189 -4.2428231][-4.2644715 -4.2359166 -4.209868 -4.189291 -4.1598248 -4.1226964 -4.1000681 -4.0964828 -4.1142063 -4.1484509 -4.1796489 -4.2031121 -4.2172751 -4.2307167 -4.2380114][-4.2737164 -4.2523737 -4.2319551 -4.216073 -4.1947064 -4.1710081 -4.1572275 -4.151722 -4.1588316 -4.1766171 -4.1918902 -4.1994596 -4.1997733 -4.2028823 -4.2086606][-4.2741346 -4.2615323 -4.2468934 -4.2331667 -4.2163854 -4.1956348 -4.1780014 -4.1639767 -4.1592736 -4.1648979 -4.1705694 -4.1752353 -4.1747236 -4.1778917 -4.189466][-4.2630053 -4.2634544 -4.2591147 -4.2507796 -4.2397327 -4.2205477 -4.1961036 -4.171834 -4.1546702 -4.1465254 -4.1459289 -4.154635 -4.1655006 -4.1810384 -4.2045317][-4.2341347 -4.2494807 -4.2576261 -4.2579651 -4.2549305 -4.242486 -4.2204475 -4.1950488 -4.1727066 -4.1554246 -4.148881 -4.1588664 -4.1760416 -4.2011371 -4.233109][-4.191082 -4.2127643 -4.228828 -4.23891 -4.2460036 -4.2464442 -4.2359266 -4.2187171 -4.1987114 -4.1804438 -4.172791 -4.1810079 -4.1966462 -4.2200046 -4.248682]]...]
INFO - root - 2017-12-07 11:07:23.713854: step 1410, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 58h:14m:14s remains)
INFO - root - 2017-12-07 11:07:30.537163: step 1420, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 64h:21m:41s remains)
INFO - root - 2017-12-07 11:07:37.314321: step 1430, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 67h:07m:30s remains)
INFO - root - 2017-12-07 11:07:44.003661: step 1440, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 63h:00m:43s remains)
INFO - root - 2017-12-07 11:07:50.834817: step 1450, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 62h:54m:34s remains)
INFO - root - 2017-12-07 11:07:57.651146: step 1460, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 58h:35m:23s remains)
INFO - root - 2017-12-07 11:08:04.475155: step 1470, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 63h:20m:54s remains)
INFO - root - 2017-12-07 11:08:11.276128: step 1480, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 61h:36m:29s remains)
INFO - root - 2017-12-07 11:08:17.862060: step 1490, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 59h:06m:02s remains)
INFO - root - 2017-12-07 11:08:24.575041: step 1500, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 61h:39m:37s remains)
2017-12-07 11:08:25.364560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2235889 -4.2131724 -4.2203941 -4.233243 -4.2340288 -4.2215195 -4.2257738 -4.2381434 -4.2483573 -4.239459 -4.2096791 -4.1970706 -4.2229624 -4.2479334 -4.259325][-4.1552143 -4.1427712 -4.1643386 -4.19639 -4.2059216 -4.1885381 -4.1907511 -4.2088027 -4.219101 -4.2061448 -4.17614 -4.1640258 -4.1948748 -4.2266121 -4.2439556][-4.118578 -4.1042562 -4.1341915 -4.1758652 -4.1840796 -4.1602688 -4.1540151 -4.1695714 -4.1796064 -4.1718006 -4.15012 -4.1439013 -4.1762481 -4.2108817 -4.23459][-4.11979 -4.0974841 -4.1275492 -4.1705723 -4.1745157 -4.1440892 -4.130486 -4.1429644 -4.1516318 -4.1524796 -4.1429458 -4.144814 -4.1769972 -4.2083335 -4.2349343][-4.1396894 -4.1092443 -4.1362123 -4.1763592 -4.1743059 -4.1349449 -4.115438 -4.1280107 -4.1383615 -4.1439557 -4.1426129 -4.1519489 -4.1858864 -4.2152538 -4.2412925][-4.1572967 -4.1291018 -4.1517444 -4.180687 -4.1692009 -4.119935 -4.092577 -4.1081953 -4.1264696 -4.1347265 -4.1314454 -4.141469 -4.17817 -4.2093563 -4.2379274][-4.1632829 -4.1434216 -4.1596322 -4.170619 -4.14831 -4.0925207 -4.0567279 -4.0671043 -4.0942063 -4.1086354 -4.101428 -4.1057034 -4.1434016 -4.183301 -4.2203889][-4.1443787 -4.13098 -4.1427522 -4.1410017 -4.1137824 -4.057322 -4.0112786 -4.0079594 -4.03735 -4.0595846 -4.0459814 -4.0401859 -4.0789766 -4.1330066 -4.1855268][-4.1288939 -4.1187806 -4.126791 -4.1212597 -4.0967631 -4.0449061 -3.99131 -3.9758964 -4.0022683 -4.0269737 -4.0099626 -3.9967675 -4.0293036 -4.0876532 -4.1483369][-4.1199164 -4.1102891 -4.1156044 -4.1122947 -4.0992413 -4.0652823 -4.018919 -3.996664 -4.0099607 -4.0274558 -4.0121756 -3.9977367 -4.0214705 -4.0737486 -4.1299891][-4.1192985 -4.1143365 -4.1227946 -4.1226606 -4.1201954 -4.1066585 -4.0757723 -4.0500126 -4.0475316 -4.0553703 -4.0435772 -4.0324082 -4.0500035 -4.0912476 -4.1378317][-4.1389318 -4.1396055 -4.1525812 -4.1554527 -4.1568689 -4.1556106 -4.1387696 -4.1158481 -4.1013217 -4.1004853 -4.0931506 -4.087235 -4.0988927 -4.12878 -4.16754][-4.176199 -4.1800647 -4.1959357 -4.2015047 -4.2031679 -4.2058372 -4.1998482 -4.1866689 -4.171567 -4.1648951 -4.1593223 -4.1563244 -4.1617923 -4.179389 -4.2093687][-4.2203188 -4.2246628 -4.2382507 -4.245604 -4.2489328 -4.2522836 -4.2511349 -4.2472782 -4.2387333 -4.2324066 -4.228065 -4.2266893 -4.229681 -4.2379446 -4.2568073][-4.2627397 -4.2661548 -4.2749929 -4.2807608 -4.2843728 -4.2874608 -4.287807 -4.2879467 -4.286273 -4.283783 -4.2809024 -4.2788644 -4.2818079 -4.2876239 -4.2984843]]...]
INFO - root - 2017-12-07 11:08:32.148439: step 1510, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 64h:59m:08s remains)
INFO - root - 2017-12-07 11:08:38.954393: step 1520, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 60h:24m:35s remains)
INFO - root - 2017-12-07 11:08:45.875573: step 1530, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 61h:23m:07s remains)
INFO - root - 2017-12-07 11:08:52.716670: step 1540, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 66h:24m:18s remains)
INFO - root - 2017-12-07 11:08:59.488795: step 1550, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 63h:13m:16s remains)
INFO - root - 2017-12-07 11:09:06.252202: step 1560, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 59h:39m:48s remains)
INFO - root - 2017-12-07 11:09:13.087998: step 1570, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 60h:45m:35s remains)
INFO - root - 2017-12-07 11:09:19.843662: step 1580, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 66h:51m:12s remains)
INFO - root - 2017-12-07 11:09:26.542196: step 1590, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 65h:51m:41s remains)
INFO - root - 2017-12-07 11:09:33.254183: step 1600, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 61h:39m:40s remains)
2017-12-07 11:09:34.013903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2014713 -4.2058406 -4.2113442 -4.209703 -4.2005739 -4.1926651 -4.1794996 -4.1642237 -4.1613345 -4.1763425 -4.1841369 -4.1801515 -4.1845345 -4.1936784 -4.1909637][-4.1706343 -4.17877 -4.1828656 -4.1780219 -4.1621943 -4.1505971 -4.1380944 -4.1197395 -4.1125751 -4.130055 -4.1429238 -4.1420951 -4.1486011 -4.1566625 -4.1454611][-4.1684704 -4.18315 -4.1887546 -4.1850905 -4.1644344 -4.14907 -4.1365657 -4.1180778 -4.1092277 -4.1251464 -4.1384654 -4.1404767 -4.1542377 -4.1628304 -4.1413641][-4.1464882 -4.1656494 -4.1765418 -4.1795497 -4.1637793 -4.1504531 -4.138206 -4.1220446 -4.1133752 -4.1200161 -4.1274123 -4.1356134 -4.1590958 -4.1711192 -4.1480851][-4.10539 -4.126462 -4.1482563 -4.16396 -4.1583896 -4.1499171 -4.1393991 -4.1260991 -4.1175404 -4.11285 -4.1124568 -4.1254892 -4.1547909 -4.1666603 -4.1451526][-4.0677633 -4.0860662 -4.1114759 -4.1317773 -4.1312675 -4.12668 -4.125134 -4.1226797 -4.1170311 -4.1076293 -4.1033521 -4.1190252 -4.1462226 -4.1501188 -4.1249042][-4.0385261 -4.0538573 -4.0762043 -4.0895696 -4.08408 -4.075314 -4.0724516 -4.0658512 -4.0520945 -4.0428276 -4.0498824 -4.0796151 -4.1115303 -4.1124821 -4.0868187][-4.038765 -4.054563 -4.070858 -4.0716753 -4.0559111 -4.0368972 -4.0223722 -3.9922307 -3.9452653 -3.9248941 -3.9561763 -4.0212374 -4.0738082 -4.0877314 -4.0761204][-4.0712852 -4.093781 -4.1042418 -4.0921669 -4.0704484 -4.0537305 -4.03974 -3.9962492 -3.9305747 -3.9018183 -3.9479029 -4.0254846 -4.0826678 -4.1039166 -4.1084585][-4.102324 -4.1293292 -4.1411242 -4.1289706 -4.1132298 -4.1134524 -4.1162457 -4.0895176 -4.0404987 -4.0209107 -4.0553732 -4.1097069 -4.1446943 -4.160758 -4.1665745][-4.140656 -4.1716552 -4.1877928 -4.1831741 -4.1728015 -4.1775451 -4.1842351 -4.1688423 -4.1409287 -4.1385527 -4.1654143 -4.1965361 -4.2128849 -4.22469 -4.2309771][-4.192337 -4.224421 -4.2436128 -4.2449794 -4.2387171 -4.2414584 -4.2445126 -4.2326331 -4.2152176 -4.2199111 -4.2444177 -4.2643509 -4.2716002 -4.2785645 -4.2822361][-4.2393346 -4.2605743 -4.2745972 -4.2788181 -4.2776189 -4.2812452 -4.2853866 -4.2774858 -4.2653084 -4.2702174 -4.2894673 -4.3029537 -4.3058844 -4.3085079 -4.3075037][-4.2708387 -4.2790351 -4.2857137 -4.293191 -4.2978225 -4.3033252 -4.3054152 -4.3003469 -4.2928114 -4.2913551 -4.2957449 -4.3001952 -4.3030138 -4.3033986 -4.30039][-4.2753968 -4.2747326 -4.2780542 -4.2879443 -4.2961559 -4.2998457 -4.2984524 -4.2948804 -4.2894945 -4.2835865 -4.2756662 -4.2694569 -4.2695222 -4.2728539 -4.27304]]...]
INFO - root - 2017-12-07 11:09:40.765751: step 1610, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 65h:38m:51s remains)
INFO - root - 2017-12-07 11:09:47.606026: step 1620, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.712 sec/batch; 65h:23m:45s remains)
INFO - root - 2017-12-07 11:09:54.371803: step 1630, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 61h:43m:43s remains)
INFO - root - 2017-12-07 11:10:01.076204: step 1640, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 60h:14m:40s remains)
INFO - root - 2017-12-07 11:10:07.934573: step 1650, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 66h:33m:29s remains)
INFO - root - 2017-12-07 11:10:14.804778: step 1660, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 67h:09m:34s remains)
INFO - root - 2017-12-07 11:10:21.532265: step 1670, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 62h:20m:00s remains)
INFO - root - 2017-12-07 11:10:28.322363: step 1680, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:36m:14s remains)
INFO - root - 2017-12-07 11:10:34.950649: step 1690, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 60h:03m:56s remains)
INFO - root - 2017-12-07 11:10:41.762150: step 1700, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 67h:18m:02s remains)
2017-12-07 11:10:42.428284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1313624 -4.0917597 -4.1190829 -4.1919775 -4.2607775 -4.3028727 -4.3135109 -4.305799 -4.3000779 -4.295114 -4.3016033 -4.3162332 -4.3188581 -4.2984028 -4.2739062][-4.0769544 -4.0270514 -4.0730457 -4.1685691 -4.245461 -4.2824354 -4.2814131 -4.2676148 -4.2699718 -4.2775917 -4.2962046 -4.3168626 -4.3194737 -4.2950211 -4.2687178][-4.063714 -4.0161238 -4.0703754 -4.1707525 -4.2479877 -4.2737408 -4.2556782 -4.2350249 -4.2418208 -4.2606707 -4.2904153 -4.3208728 -4.3202767 -4.28878 -4.258954][-4.1055641 -4.0846806 -4.1361361 -4.2130823 -4.2709 -4.276217 -4.2340069 -4.1977787 -4.2042871 -4.2336249 -4.2774596 -4.3171539 -4.3143787 -4.2789669 -4.2475491][-4.1673436 -4.1794147 -4.2176003 -4.2578149 -4.2774148 -4.2470784 -4.1713119 -4.1195979 -4.1350145 -4.1881423 -4.252635 -4.299119 -4.2959313 -4.2631516 -4.2371511][-4.2162857 -4.2427516 -4.2609377 -4.2635646 -4.2387857 -4.1559968 -4.03248 -3.9699774 -4.0185533 -4.1136909 -4.2059565 -4.2611175 -4.2615981 -4.2362561 -4.2210464][-4.2487049 -4.2715869 -4.2707419 -4.24246 -4.1775479 -4.04232 -3.8706341 -3.8125415 -3.9143152 -4.0534859 -4.16625 -4.2268114 -4.23303 -4.2137542 -4.2076268][-4.2747421 -4.2875705 -4.2708063 -4.2279692 -4.1496258 -4.0068789 -3.8448257 -3.8149881 -3.9346194 -4.0721235 -4.1723342 -4.2225461 -4.2251377 -4.2049046 -4.2023149][-4.2900033 -4.2910819 -4.268887 -4.23073 -4.16976 -4.0717688 -3.9824984 -3.9868751 -4.0739532 -4.1672621 -4.2317896 -4.2527595 -4.2377534 -4.2106967 -4.2088008][-4.3003545 -4.2978978 -4.2779603 -4.2497044 -4.2121267 -4.1581364 -4.1226759 -4.141923 -4.2016582 -4.2589474 -4.291636 -4.2878242 -4.2577329 -4.2268591 -4.2241731][-4.3078008 -4.3090596 -4.2950754 -4.2774816 -4.2581191 -4.2300544 -4.2154894 -4.2341962 -4.2743888 -4.3081222 -4.3214612 -4.3073969 -4.278717 -4.251904 -4.2459207][-4.3132305 -4.3132615 -4.30395 -4.295886 -4.2887325 -4.2740173 -4.2653108 -4.275969 -4.2982359 -4.3152809 -4.3193541 -4.3067813 -4.2884059 -4.2707791 -4.2637982][-4.3192163 -4.3159051 -4.3087196 -4.3049212 -4.3038564 -4.2993879 -4.297245 -4.3016005 -4.3100028 -4.3146172 -4.31318 -4.3054914 -4.2968903 -4.2861423 -4.2796988][-4.3236008 -4.3183928 -4.3120356 -4.3103557 -4.31316 -4.3177309 -4.3205509 -4.3218689 -4.3221707 -4.3183131 -4.3134046 -4.3082805 -4.3046608 -4.3000054 -4.29769][-4.3261414 -4.3231635 -4.3189716 -4.3171926 -4.3191381 -4.3229556 -4.3264465 -4.3280544 -4.3259516 -4.3213043 -4.3185897 -4.3173804 -4.3180189 -4.3176293 -4.3170204]]...]
INFO - root - 2017-12-07 11:10:49.316903: step 1710, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 62h:38m:33s remains)
INFO - root - 2017-12-07 11:10:56.263472: step 1720, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 61h:38m:06s remains)
INFO - root - 2017-12-07 11:11:03.114549: step 1730, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 65h:27m:42s remains)
INFO - root - 2017-12-07 11:11:09.829629: step 1740, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 61h:02m:22s remains)
INFO - root - 2017-12-07 11:11:16.518293: step 1750, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 59h:25m:38s remains)
INFO - root - 2017-12-07 11:11:23.238854: step 1760, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.625 sec/batch; 57h:23m:36s remains)
INFO - root - 2017-12-07 11:11:30.041499: step 1770, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 61h:08m:32s remains)
INFO - root - 2017-12-07 11:11:36.903686: step 1780, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.719 sec/batch; 66h:03m:44s remains)
INFO - root - 2017-12-07 11:11:43.528080: step 1790, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 59h:35m:11s remains)
INFO - root - 2017-12-07 11:11:50.345298: step 1800, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 57h:41m:29s remains)
2017-12-07 11:11:51.122128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3238897 -4.3369808 -4.3334484 -4.290112 -4.2250562 -4.1725769 -4.1375508 -4.1522708 -4.2109704 -4.2440996 -4.225225 -4.1897831 -4.1864858 -4.1994762 -4.2080112][-4.3262825 -4.3401155 -4.3351922 -4.2896204 -4.2234488 -4.1657133 -4.1252842 -4.1377783 -4.1981583 -4.237431 -4.2249894 -4.1934223 -4.192523 -4.2120023 -4.2310667][-4.3286042 -4.3426819 -4.3365321 -4.2900376 -4.2210617 -4.1533785 -4.1036243 -4.11335 -4.1746445 -4.220315 -4.2093053 -4.1807461 -4.1889305 -4.2202172 -4.2483854][-4.329483 -4.3416033 -4.3349872 -4.289361 -4.2146807 -4.1339355 -4.078948 -4.0866342 -4.1474862 -4.1944747 -4.1815643 -4.1587219 -4.1804309 -4.2242832 -4.2558432][-4.3297849 -4.3399925 -4.3340511 -4.2885261 -4.206182 -4.1151667 -4.0575175 -4.0649195 -4.1223445 -4.1629682 -4.1481714 -4.1434622 -4.1821423 -4.2334485 -4.2655134][-4.3307753 -4.3399544 -4.3343687 -4.2877626 -4.199945 -4.1025133 -4.0397573 -4.0416665 -4.0911865 -4.1157942 -4.1064858 -4.1339269 -4.1924987 -4.2432137 -4.2670245][-4.331974 -4.34031 -4.3351107 -4.2875962 -4.1969876 -4.098258 -4.0289874 -4.0211649 -4.05523 -4.0594058 -4.0646906 -4.1289263 -4.2036552 -4.2519445 -4.2691965][-4.3319454 -4.3394828 -4.33379 -4.2878952 -4.2028632 -4.1113443 -4.0407233 -4.0223675 -4.0390167 -4.0379019 -4.070085 -4.15417 -4.2297015 -4.2716689 -4.2830133][-4.328968 -4.3358412 -4.3300347 -4.2893939 -4.2147493 -4.134985 -4.0727797 -4.0491686 -4.05733 -4.0702543 -4.1223879 -4.2021732 -4.2636986 -4.2942047 -4.2915115][-4.32473 -4.3315 -4.3266759 -4.2921133 -4.2289271 -4.1616311 -4.1089468 -4.0837569 -4.09021 -4.1200161 -4.1821704 -4.2526059 -4.2991686 -4.3173037 -4.29914][-4.3221827 -4.3281078 -4.3254337 -4.29643 -4.2427955 -4.1809597 -4.1286182 -4.0960464 -4.106668 -4.1552906 -4.2234993 -4.2889824 -4.3302283 -4.33965 -4.31668][-4.3221245 -4.3277841 -4.3252325 -4.2986226 -4.2478361 -4.1843491 -4.12676 -4.0905881 -4.1120944 -4.176518 -4.2476454 -4.3105359 -4.3481393 -4.3490076 -4.3248172][-4.3249574 -4.3321619 -4.329958 -4.3013582 -4.2431636 -4.1720834 -4.1072359 -4.0798674 -4.1173363 -4.1935053 -4.2700763 -4.3282061 -4.357018 -4.3478971 -4.3197608][-4.3305578 -4.3402491 -4.3368511 -4.3022275 -4.229557 -4.1401463 -4.0687189 -4.0635815 -4.1247816 -4.21344 -4.2904873 -4.3407702 -4.3565249 -4.3373685 -4.3057847][-4.3369651 -4.3496017 -4.3445334 -4.2995429 -4.2072916 -4.0979495 -4.0293703 -4.0529952 -4.1380773 -4.2348704 -4.3061728 -4.342618 -4.3451624 -4.3204207 -4.2884741]]...]
INFO - root - 2017-12-07 11:11:57.793261: step 1810, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 58h:04m:31s remains)
INFO - root - 2017-12-07 11:12:04.707395: step 1820, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 57h:43m:39s remains)
INFO - root - 2017-12-07 11:12:11.519258: step 1830, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.635 sec/batch; 58h:20m:55s remains)
INFO - root - 2017-12-07 11:12:18.362238: step 1840, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 65h:55m:21s remains)
INFO - root - 2017-12-07 11:12:25.163263: step 1850, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 65h:38m:13s remains)
INFO - root - 2017-12-07 11:12:32.052669: step 1860, loss = 2.01, batch loss = 1.95 (12.2 examples/sec; 0.657 sec/batch; 60h:19m:18s remains)
INFO - root - 2017-12-07 11:12:38.802738: step 1870, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 60h:06m:11s remains)
INFO - root - 2017-12-07 11:12:45.494078: step 1880, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 58h:17m:58s remains)
INFO - root - 2017-12-07 11:12:52.246933: step 1890, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 67h:54m:57s remains)
INFO - root - 2017-12-07 11:12:58.976254: step 1900, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 65h:36m:13s remains)
2017-12-07 11:12:59.673588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2157063 -4.1811934 -4.1805615 -4.2031331 -4.2200685 -4.23806 -4.2552896 -4.2720942 -4.2909327 -4.2964506 -4.2736893 -4.25021 -4.2327776 -4.2280145 -4.2351723][-4.1834416 -4.1303396 -4.1139355 -4.1389637 -4.1740255 -4.20666 -4.223249 -4.2394314 -4.262538 -4.272583 -4.2538309 -4.2385473 -4.226449 -4.2211637 -4.2227325][-4.1675291 -4.106205 -4.0731316 -4.080708 -4.1154613 -4.158916 -4.1845546 -4.20328 -4.2248816 -4.2358313 -4.2255721 -4.2251072 -4.2250457 -4.2256861 -4.2199821][-4.1610188 -4.1018524 -4.0545478 -4.0359111 -4.0537729 -4.0976305 -4.1341324 -4.1643629 -4.1990843 -4.2172112 -4.21464 -4.2225637 -4.2314849 -4.2335014 -4.2213039][-4.158844 -4.1101961 -4.0574245 -4.0147858 -4.0078053 -4.0343828 -4.0694432 -4.1139274 -4.1687551 -4.20276 -4.2064748 -4.2155752 -4.2201662 -4.2144713 -4.2009821][-4.1466269 -4.1086683 -4.0602093 -4.0011339 -3.96818 -3.9712141 -3.9971344 -4.0447125 -4.1070457 -4.1519456 -4.1595697 -4.1716967 -4.1795216 -4.1810832 -4.1764941][-4.1271453 -4.0962582 -4.0566506 -4.0005994 -3.9596622 -3.9446156 -3.9538894 -3.9900529 -4.0515695 -4.0984 -4.1058836 -4.1203222 -4.1408906 -4.1587067 -4.1638775][-4.10018 -4.0737848 -4.0448194 -4.0059094 -3.9735377 -3.949435 -3.9402747 -3.9575229 -4.0080533 -4.052743 -4.0638866 -4.0846543 -4.1177864 -4.1487541 -4.1604962][-4.0792117 -4.0561891 -4.03426 -4.0085983 -3.9882288 -3.9637234 -3.9423304 -3.9449353 -3.9792418 -4.0215635 -4.0410008 -4.0714874 -4.1093197 -4.1416087 -4.15668][-4.0795379 -4.0629811 -4.0472555 -4.0296316 -4.0154233 -3.9925137 -3.968992 -3.9617672 -3.9802747 -4.0115619 -4.030026 -4.0638347 -4.1058626 -4.1398773 -4.1555734][-4.1267171 -4.1121449 -4.0983982 -4.0835662 -4.070888 -4.0528188 -4.0352564 -4.0261297 -4.0300484 -4.036387 -4.0359707 -4.0610757 -4.10283 -4.140018 -4.1584272][-4.2013264 -4.1905465 -4.1793551 -4.1677017 -4.1582685 -4.146893 -4.1351194 -4.1281924 -4.1253324 -4.1179738 -4.1021123 -4.1095352 -4.1357679 -4.1645207 -4.1812744][-4.2747951 -4.2687449 -4.2615814 -4.2546096 -4.2491932 -4.2437348 -4.2367797 -4.2309966 -4.2270746 -4.2189584 -4.2002268 -4.1937127 -4.2017817 -4.2154722 -4.2248421][-4.3212361 -4.3191581 -4.3150983 -4.3114161 -4.3090858 -4.3073292 -4.3050947 -4.3030772 -4.3013787 -4.2961 -4.2803612 -4.2681561 -4.2644348 -4.2660446 -4.2692423][-4.3363414 -4.3379922 -4.3365483 -4.3350039 -4.3343577 -4.3348665 -4.3360124 -4.3373051 -4.3379288 -4.3357019 -4.3266654 -4.3176837 -4.3113427 -4.3091869 -4.3083425]]...]
INFO - root - 2017-12-07 11:13:06.466881: step 1910, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:50m:23s remains)
INFO - root - 2017-12-07 11:13:13.240533: step 1920, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 65h:53m:15s remains)
INFO - root - 2017-12-07 11:13:20.137040: step 1930, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 66h:29m:59s remains)
INFO - root - 2017-12-07 11:13:26.984217: step 1940, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 61h:33m:09s remains)
INFO - root - 2017-12-07 11:13:33.814328: step 1950, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 57h:12m:11s remains)
INFO - root - 2017-12-07 11:13:40.545761: step 1960, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 60h:22m:44s remains)
INFO - root - 2017-12-07 11:13:47.339658: step 1970, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 0.741 sec/batch; 67h:59m:55s remains)
INFO - root - 2017-12-07 11:13:54.262938: step 1980, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 68h:22m:24s remains)
INFO - root - 2017-12-07 11:14:00.858242: step 1990, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 61h:38m:07s remains)
INFO - root - 2017-12-07 11:14:07.610411: step 2000, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 59h:38m:11s remains)
2017-12-07 11:14:08.279665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3662624 -4.3652611 -4.36397 -4.3642287 -4.3642306 -4.3650684 -4.3664618 -4.3684845 -4.3700829 -4.3694386 -4.3680105 -4.3680573 -4.368381 -4.369308 -4.3691492][-4.3626423 -4.3576674 -4.3525634 -4.3495183 -4.3470283 -4.34815 -4.3509021 -4.3508835 -4.3480144 -4.3421206 -4.3349061 -4.3337131 -4.3370776 -4.3449836 -4.3521547][-4.3554678 -4.3458858 -4.3358011 -4.3276162 -4.3204036 -4.3195076 -4.3208556 -4.3155046 -4.3059683 -4.2928848 -4.2778959 -4.2756748 -4.2827983 -4.3013082 -4.3207359][-4.3466349 -4.3313093 -4.3140793 -4.2985449 -4.2853141 -4.2795553 -4.2764211 -4.2619996 -4.2406273 -4.2146029 -4.1886792 -4.185946 -4.2001786 -4.2339363 -4.2706923][-4.3327365 -4.3098383 -4.2839084 -4.2596674 -4.2408605 -4.2304773 -4.2219086 -4.1942425 -4.1582432 -4.1183853 -4.0809965 -4.0806417 -4.1078672 -4.1599469 -4.2166409][-4.3050346 -4.2706656 -4.2328277 -4.2023335 -4.1816063 -4.1705036 -4.1583734 -4.1202903 -4.0763206 -4.0331049 -3.9994757 -4.0096312 -4.0504494 -4.1162405 -4.1836095][-4.2636442 -4.2163424 -4.1642909 -4.1245127 -4.098772 -4.0873132 -4.0783896 -4.0383859 -4.0007453 -3.9788885 -3.9690089 -3.9965663 -4.049212 -4.1192131 -4.1855154][-4.2145667 -4.150806 -4.0748239 -4.0152221 -3.9844482 -3.9817989 -3.9917035 -3.9640834 -3.9511008 -3.9698677 -3.9907587 -4.0307879 -4.0869217 -4.1552382 -4.214972][-4.1715546 -4.0915008 -3.9870098 -3.9067814 -3.8811469 -3.9049988 -3.9485745 -3.9486766 -3.9644654 -4.0069246 -4.0460873 -4.0930467 -4.1493616 -4.2092524 -4.2562103][-4.15877 -4.07425 -3.9657407 -3.8892179 -3.881964 -3.9265506 -3.9900312 -4.0149379 -4.0450082 -4.0887961 -4.1283917 -4.1715784 -4.2173376 -4.2607155 -4.2917423][-4.1955962 -4.1309066 -4.0591269 -4.0158639 -4.0173516 -4.0560045 -4.1108451 -4.138073 -4.1601396 -4.1903567 -4.2179413 -4.2476044 -4.2790256 -4.3067431 -4.3223982][-4.2562714 -4.2159615 -4.1804724 -4.1672015 -4.1765313 -4.2022867 -4.2381058 -4.2533889 -4.26084 -4.27148 -4.2839684 -4.3009424 -4.3213396 -4.336925 -4.3421154][-4.3047781 -4.2820549 -4.2680283 -4.2722931 -4.2861781 -4.3029718 -4.3224587 -4.32743 -4.3251104 -4.3244915 -4.3271618 -4.3355846 -4.3467865 -4.3536558 -4.3543139][-4.3338218 -4.3232818 -4.3185868 -4.32545 -4.3356194 -4.3460426 -4.3576241 -4.3591218 -4.3566289 -4.3535128 -4.3526073 -4.3555551 -4.359756 -4.3617587 -4.3613162][-4.3512106 -4.3484511 -4.3477855 -4.351644 -4.3568382 -4.3632245 -4.3693285 -4.3693185 -4.367116 -4.3643489 -4.3634529 -4.363914 -4.3644438 -4.3645158 -4.3639159]]...]
INFO - root - 2017-12-07 11:14:15.060504: step 2010, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 65h:14m:41s remains)
INFO - root - 2017-12-07 11:14:21.888038: step 2020, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 61h:27m:04s remains)
INFO - root - 2017-12-07 11:14:28.676080: step 2030, loss = 2.05, batch loss = 2.00 (13.3 examples/sec; 0.604 sec/batch; 55h:24m:54s remains)
INFO - root - 2017-12-07 11:14:35.496943: step 2040, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 59h:28m:19s remains)
INFO - root - 2017-12-07 11:14:42.358711: step 2050, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 65h:07m:12s remains)
INFO - root - 2017-12-07 11:14:49.163731: step 2060, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 63h:52m:27s remains)
INFO - root - 2017-12-07 11:14:55.936633: step 2070, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 61h:24m:10s remains)
INFO - root - 2017-12-07 11:15:02.638174: step 2080, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 57h:27m:45s remains)
INFO - root - 2017-12-07 11:15:09.482332: step 2090, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 65h:57m:59s remains)
INFO - root - 2017-12-07 11:15:16.177007: step 2100, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 63h:37m:11s remains)
2017-12-07 11:15:16.882913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1410365 -4.1133461 -4.1097403 -4.1281395 -4.163228 -4.1856637 -4.1886096 -4.1909761 -4.2034488 -4.22272 -4.2546062 -4.2864981 -4.3112206 -4.3207793 -4.310535][-4.1558895 -4.1309514 -4.1347942 -4.1560683 -4.1885824 -4.2107592 -4.2204986 -4.2266054 -4.2420869 -4.2632637 -4.2953868 -4.3222313 -4.3402247 -4.34639 -4.3362379][-4.1780872 -4.1518812 -4.1539397 -4.1706829 -4.1931705 -4.2106037 -4.2255578 -4.23822 -4.2579427 -4.2835841 -4.3154168 -4.3353143 -4.349484 -4.3534818 -4.3373089][-4.2029819 -4.1692915 -4.1643929 -4.1771607 -4.1952496 -4.2061672 -4.2151456 -4.2267361 -4.2434492 -4.2719564 -4.3076172 -4.3275485 -4.3455381 -4.34975 -4.3321242][-4.2165575 -4.1838374 -4.1750522 -4.1846333 -4.1970258 -4.1932297 -4.1809673 -4.1823506 -4.1966257 -4.2306752 -4.2766137 -4.3033972 -4.3278775 -4.3348107 -4.320003][-4.2315569 -4.2075706 -4.1977081 -4.1984963 -4.1961584 -4.1688194 -4.1251254 -4.1044054 -4.1188602 -4.1679163 -4.2290387 -4.2727132 -4.3080764 -4.3229361 -4.316947][-4.2371931 -4.2278852 -4.2247872 -4.2172146 -4.1979175 -4.1521482 -4.0757813 -4.0195308 -4.0246806 -4.0921865 -4.1755776 -4.2391744 -4.28941 -4.3143945 -4.3166895][-4.2388864 -4.2482576 -4.2546067 -4.2429523 -4.2142363 -4.158586 -4.0683808 -3.9844506 -3.9660938 -4.0350676 -4.1332536 -4.2124395 -4.2753358 -4.3062696 -4.307673][-4.2315869 -4.2545657 -4.2683454 -4.2556171 -4.2262087 -4.1715245 -4.08855 -4.0050087 -3.9790261 -4.0364633 -4.1263914 -4.2037463 -4.2693977 -4.3007827 -4.2964697][-4.216279 -4.2452736 -4.2613354 -4.2499638 -4.225337 -4.1792741 -4.1073179 -4.0367985 -4.0186763 -4.0683632 -4.1447644 -4.2108831 -4.2693214 -4.2997718 -4.290041][-4.2179251 -4.2405338 -4.2529831 -4.2423573 -4.2236638 -4.1889296 -4.1268549 -4.0680194 -4.0578361 -4.1072245 -4.1724186 -4.2245908 -4.2686906 -4.2946339 -4.2876148][-4.2410879 -4.2557559 -4.2557521 -4.2407908 -4.2237716 -4.198894 -4.1528368 -4.1111531 -4.1097283 -4.15472 -4.2068787 -4.2433305 -4.268703 -4.2829556 -4.2784519][-4.2708216 -4.2838159 -4.2760968 -4.256444 -4.2364273 -4.2160296 -4.1849809 -4.1613879 -4.166101 -4.2047029 -4.2420077 -4.2572517 -4.253911 -4.2457738 -4.2403717][-4.28796 -4.3013668 -4.296216 -4.2802067 -4.2624536 -4.2480559 -4.2308469 -4.2188506 -4.2254548 -4.2542834 -4.2767568 -4.274312 -4.24218 -4.2047081 -4.1919956][-4.2820611 -4.2969918 -4.2961135 -4.28905 -4.28104 -4.2748871 -4.2694464 -4.2684193 -4.2771735 -4.2960873 -4.3044915 -4.2876744 -4.2372379 -4.1803641 -4.1572094]]...]
INFO - root - 2017-12-07 11:15:23.845897: step 2110, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 0.762 sec/batch; 69h:54m:55s remains)
INFO - root - 2017-12-07 11:15:30.622690: step 2120, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.761 sec/batch; 69h:47m:55s remains)
INFO - root - 2017-12-07 11:15:37.365121: step 2130, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.688 sec/batch; 63h:08m:47s remains)
INFO - root - 2017-12-07 11:15:44.142775: step 2140, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 60h:05m:12s remains)
INFO - root - 2017-12-07 11:15:50.956460: step 2150, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 60h:57m:03s remains)
INFO - root - 2017-12-07 11:15:57.797107: step 2160, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 66h:17m:03s remains)
INFO - root - 2017-12-07 11:16:04.662569: step 2170, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.750 sec/batch; 68h:50m:27s remains)
INFO - root - 2017-12-07 11:16:11.480173: step 2180, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 62h:57m:15s remains)
INFO - root - 2017-12-07 11:16:18.085147: step 2190, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 57h:56m:41s remains)
INFO - root - 2017-12-07 11:16:24.831112: step 2200, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 59h:43m:31s remains)
2017-12-07 11:16:25.582171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1571693 -4.1014767 -4.1144371 -4.1651487 -4.2111859 -4.2486739 -4.2735839 -4.2792482 -4.2775984 -4.2848949 -4.2981992 -4.3100357 -4.3183494 -4.3211527 -4.3189178][-4.1002121 -4.027997 -4.0488095 -4.1223936 -4.1844893 -4.2262568 -4.2483606 -4.2486386 -4.246139 -4.2598152 -4.281496 -4.3031578 -4.3179293 -4.3216743 -4.3170567][-4.04485 -3.9492807 -3.976598 -4.0813708 -4.1607351 -4.2039623 -4.2170248 -4.2099962 -4.2078905 -4.2304282 -4.2621546 -4.2946486 -4.316402 -4.3216887 -4.3143678][-4.0338168 -3.9326785 -3.9667037 -4.0884724 -4.1673989 -4.1977096 -4.1899071 -4.1670084 -4.1629896 -4.193212 -4.2346835 -4.2790132 -4.3095946 -4.3195267 -4.3123808][-4.0921073 -4.0250864 -4.0622129 -4.1598029 -4.209816 -4.2077761 -4.1667509 -4.1183076 -4.1087031 -4.1497788 -4.2019715 -4.2549634 -4.2940087 -4.3098922 -4.3074951][-4.1547842 -4.1214375 -4.1558228 -4.216207 -4.2334132 -4.2000036 -4.1207986 -4.0377927 -4.0293503 -4.0975776 -4.1683946 -4.2292166 -4.273634 -4.295784 -4.2990003][-4.2009158 -4.1866856 -4.2121835 -4.2426262 -4.2325597 -4.1745896 -4.0574713 -3.9327478 -3.9306881 -4.03751 -4.13385 -4.2057443 -4.2541184 -4.2800641 -4.2889681][-4.224937 -4.2203093 -4.2454348 -4.2600474 -4.2319479 -4.1605215 -4.0249772 -3.8780062 -3.8814394 -4.0132928 -4.1225376 -4.1947904 -4.2390265 -4.2650633 -4.2783904][-4.2057333 -4.2056189 -4.2425418 -4.2605739 -4.2350492 -4.174964 -4.0677333 -3.9540048 -3.959141 -4.0669522 -4.154808 -4.208467 -4.240242 -4.25803 -4.269803][-4.1546054 -4.1482978 -4.2031903 -4.2416167 -4.2364383 -4.1985664 -4.132175 -4.0634365 -4.0691872 -4.1371274 -4.1955838 -4.2302213 -4.2496176 -4.25709 -4.2648053][-4.0918183 -4.0651169 -4.1379528 -4.2044554 -4.2268806 -4.2122679 -4.1767273 -4.1381955 -4.145 -4.1862574 -4.2228036 -4.2475405 -4.2593694 -4.2593727 -4.2638326][-4.0462751 -4.0014529 -4.0898428 -4.1860881 -4.2276888 -4.2276773 -4.2044973 -4.1819978 -4.1904492 -4.2158308 -4.2362876 -4.2565365 -4.266037 -4.2644258 -4.2677245][-4.0741677 -4.0438643 -4.1268234 -4.2166233 -4.2488866 -4.2402425 -4.2042089 -4.1797843 -4.1888976 -4.2146626 -4.2323318 -4.2533031 -4.2657285 -4.2684116 -4.2733][-4.1367211 -4.1295638 -4.1911216 -4.2525349 -4.2635555 -4.2331681 -4.1713624 -4.1220675 -4.1309981 -4.1774683 -4.215621 -4.2455354 -4.2602892 -4.2648206 -4.2685905][-4.1913886 -4.19076 -4.2284961 -4.263413 -4.255671 -4.2032166 -4.1035614 -4.0123854 -4.0255032 -4.1123514 -4.1864781 -4.2334175 -4.2490497 -4.250452 -4.2501006]]...]
INFO - root - 2017-12-07 11:16:32.376596: step 2210, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:47m:35s remains)
INFO - root - 2017-12-07 11:16:39.150761: step 2220, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:56m:14s remains)
INFO - root - 2017-12-07 11:16:46.000041: step 2230, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 62h:11m:46s remains)
INFO - root - 2017-12-07 11:16:52.857423: step 2240, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 63h:45m:07s remains)
INFO - root - 2017-12-07 11:16:59.687338: step 2250, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 61h:44m:28s remains)
INFO - root - 2017-12-07 11:17:06.573999: step 2260, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 58h:28m:16s remains)
INFO - root - 2017-12-07 11:17:13.394090: step 2270, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:59m:15s remains)
INFO - root - 2017-12-07 11:17:20.213317: step 2280, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 65h:24m:47s remains)
INFO - root - 2017-12-07 11:17:26.883865: step 2290, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 65h:28m:29s remains)
INFO - root - 2017-12-07 11:17:33.669636: step 2300, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 63h:08m:47s remains)
2017-12-07 11:17:34.349120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2136273 -4.13404 -4.0831122 -4.0814543 -4.1214461 -4.167943 -4.1959176 -4.1936107 -4.17927 -4.1725583 -4.1625595 -4.1457577 -4.1367249 -4.1504531 -4.1872082][-4.1950574 -4.129591 -4.0954137 -4.1027703 -4.1358757 -4.1646433 -4.1855383 -4.1936088 -4.1954894 -4.1972103 -4.1840892 -4.1524844 -4.1272454 -4.1345305 -4.1715603][-4.18388 -4.1322632 -4.1139908 -4.1244264 -4.1448655 -4.1575017 -4.1697884 -4.1826892 -4.1937428 -4.2002597 -4.1892905 -4.1598816 -4.1371837 -4.1366735 -4.1597271][-4.188436 -4.1432695 -4.1312547 -4.1402407 -4.1515236 -4.1605 -4.1747131 -4.1855416 -4.1917081 -4.1974945 -4.1883368 -4.1709428 -4.1614161 -4.1569958 -4.1639447][-4.206039 -4.1636114 -4.1494207 -4.1500883 -4.1546054 -4.1621714 -4.1782703 -4.1890931 -4.1858153 -4.1861038 -4.1822238 -4.1794176 -4.1827087 -4.1759024 -4.17009][-4.228662 -4.1879234 -4.17096 -4.16288 -4.1583118 -4.1618423 -4.1757994 -4.1833916 -4.1769428 -4.17373 -4.173377 -4.1801844 -4.188436 -4.1858549 -4.1786108][-4.2467761 -4.2073221 -4.1861525 -4.170022 -4.1580153 -4.1577005 -4.1632323 -4.1658111 -4.1582408 -4.1549935 -4.1639066 -4.1817427 -4.1941996 -4.19511 -4.1875262][-4.2531228 -4.2132959 -4.1873713 -4.1677837 -4.151093 -4.145946 -4.1441178 -4.1387525 -4.1290493 -4.1312675 -4.1531315 -4.183002 -4.198472 -4.2007508 -4.1920433][-4.24988 -4.2102885 -4.1875243 -4.1747875 -4.1560936 -4.1395593 -4.1265659 -4.1147351 -4.1052337 -4.1172776 -4.1538587 -4.1917696 -4.2097449 -4.2106171 -4.19754][-4.2443414 -4.2095408 -4.1956797 -4.1945653 -4.1786766 -4.1517992 -4.1264973 -4.1055307 -4.0959663 -4.1163726 -4.1611314 -4.2017078 -4.2180705 -4.2145934 -4.1974316][-4.2416024 -4.208878 -4.2021995 -4.2093229 -4.201818 -4.1776 -4.1484103 -4.115242 -4.0980954 -4.1152592 -4.1592474 -4.1943941 -4.2063203 -4.2005248 -4.1821785][-4.2485576 -4.2123847 -4.2042041 -4.2116132 -4.2117157 -4.1995077 -4.1782751 -4.1426754 -4.1182027 -4.1200824 -4.1474209 -4.1706843 -4.1788573 -4.1761141 -4.1618972][-4.2578025 -4.2184358 -4.2042971 -4.2074986 -4.2119493 -4.2116542 -4.1996584 -4.167109 -4.1391635 -4.1253896 -4.1309433 -4.1404157 -4.1456718 -4.1460118 -4.1370983][-4.2636685 -4.2236075 -4.2028604 -4.201798 -4.2097688 -4.2193933 -4.2117205 -4.1780906 -4.1473975 -4.1241508 -4.116117 -4.1211143 -4.1271329 -4.1299071 -4.1264405][-4.2637105 -4.2284489 -4.2081618 -4.20467 -4.2121539 -4.2240181 -4.217648 -4.1877494 -4.1566749 -4.1341372 -4.1260095 -4.13242 -4.1384358 -4.142262 -4.1427889]]...]
INFO - root - 2017-12-07 11:17:41.247288: step 2310, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 66h:06m:18s remains)
INFO - root - 2017-12-07 11:17:48.163585: step 2320, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.737 sec/batch; 67h:37m:57s remains)
INFO - root - 2017-12-07 11:17:54.957868: step 2330, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.665 sec/batch; 60h:59m:48s remains)
INFO - root - 2017-12-07 11:18:01.716437: step 2340, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 59h:12m:31s remains)
INFO - root - 2017-12-07 11:18:08.575501: step 2350, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 61h:41m:25s remains)
INFO - root - 2017-12-07 11:18:15.383167: step 2360, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 66h:16m:41s remains)
INFO - root - 2017-12-07 11:18:22.218580: step 2370, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:10m:32s remains)
INFO - root - 2017-12-07 11:18:28.963369: step 2380, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 57h:05m:55s remains)
INFO - root - 2017-12-07 11:18:35.569656: step 2390, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 61h:16m:06s remains)
INFO - root - 2017-12-07 11:18:42.436808: step 2400, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 65h:15m:40s remains)
2017-12-07 11:18:43.100533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2254319 -4.2440252 -4.2541862 -4.2606874 -4.274899 -4.2815514 -4.2653227 -4.2400088 -4.22543 -4.2289453 -4.2379737 -4.2545371 -4.2726231 -4.2765594 -4.28365][-4.2088013 -4.2381811 -4.2598486 -4.2702141 -4.2809277 -4.28002 -4.2561269 -4.2280159 -4.2130671 -4.2166257 -4.2273035 -4.2468219 -4.266748 -4.2709341 -4.2770438][-4.1854043 -4.2251382 -4.2583256 -4.2704549 -4.2744203 -4.2635765 -4.2374339 -4.2124157 -4.2021637 -4.2084022 -4.2221823 -4.242662 -4.2606258 -4.2650776 -4.2718897][-4.1909924 -4.2275467 -4.2594228 -4.2625828 -4.2522712 -4.2373023 -4.2161574 -4.1979117 -4.1950016 -4.2071285 -4.2261915 -4.2431135 -4.2572875 -4.2642055 -4.2739305][-4.216083 -4.2381678 -4.2520609 -4.2388935 -4.2124133 -4.1903858 -4.1740479 -4.1614161 -4.1691632 -4.1922531 -4.2176123 -4.2335296 -4.24901 -4.2618537 -4.2764792][-4.2181869 -4.2221622 -4.2188797 -4.1914449 -4.1533732 -4.119802 -4.0983872 -4.0868125 -4.1076665 -4.1478128 -4.1837854 -4.2051249 -4.2288232 -4.251596 -4.272018][-4.1846266 -4.1776757 -4.1619186 -4.12724 -4.0821671 -4.0337343 -3.9924982 -3.968266 -3.9957266 -4.0564575 -4.1079679 -4.1462507 -4.188488 -4.2268672 -4.2578983][-4.1297832 -4.116672 -4.0980921 -4.068821 -4.0325112 -3.9836414 -3.9243762 -3.8799064 -3.9020143 -3.9686124 -4.0322704 -4.0892553 -4.1495647 -4.2026367 -4.2437377][-4.1095748 -4.1001954 -4.0891919 -4.0770235 -4.0603571 -4.0274882 -3.974395 -3.9284282 -3.9338653 -3.9771123 -4.02872 -4.084774 -4.1439514 -4.1956968 -4.2390389][-4.1376786 -4.1369381 -4.1392784 -4.1414695 -4.1407633 -4.1263661 -4.0926714 -4.05781 -4.0525002 -4.0683284 -4.0950508 -4.131587 -4.1729784 -4.21202 -4.2491016][-4.2089124 -4.2104573 -4.2148042 -4.2173457 -4.2183957 -4.2128425 -4.195487 -4.1742287 -4.1689091 -4.1733942 -4.1828046 -4.1986284 -4.2210908 -4.2454143 -4.2707372][-4.2774167 -4.2781219 -4.280067 -4.2800031 -4.2796869 -4.2774687 -4.2706785 -4.2610388 -4.2579117 -4.25752 -4.2584052 -4.2622294 -4.2707782 -4.2820992 -4.2941942][-4.3095722 -4.3097029 -4.3111372 -4.3115964 -4.3135905 -4.3160219 -4.3165884 -4.3135886 -4.3114018 -4.3090148 -4.3074694 -4.3075075 -4.3081737 -4.3093247 -4.3125677][-4.3264909 -4.3268571 -4.3281536 -4.3291068 -4.3330421 -4.3374734 -4.3406596 -4.3406258 -4.3392324 -4.3363638 -4.3336439 -4.3332291 -4.3317714 -4.329565 -4.3285928][-4.3358364 -4.3353157 -4.3357644 -4.33639 -4.3389354 -4.3416505 -4.3431845 -4.3438578 -4.3439612 -4.3439689 -4.3434439 -4.3436828 -4.3437071 -4.3420181 -4.3402429]]...]
INFO - root - 2017-12-07 11:18:49.705402: step 2410, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:45m:26s remains)
INFO - root - 2017-12-07 11:18:56.538928: step 2420, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 60h:40m:15s remains)
INFO - root - 2017-12-07 11:19:03.386702: step 2430, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 65h:03m:08s remains)
INFO - root - 2017-12-07 11:19:10.226769: step 2440, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.705 sec/batch; 64h:37m:40s remains)
INFO - root - 2017-12-07 11:19:17.009434: step 2450, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 62h:25m:50s remains)
INFO - root - 2017-12-07 11:19:23.830703: step 2460, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:55m:12s remains)
INFO - root - 2017-12-07 11:19:30.614154: step 2470, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 61h:25m:08s remains)
INFO - root - 2017-12-07 11:19:37.486424: step 2480, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 64h:44m:26s remains)
INFO - root - 2017-12-07 11:19:44.160151: step 2490, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 64h:15m:21s remains)
INFO - root - 2017-12-07 11:19:50.895432: step 2500, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 61h:40m:39s remains)
2017-12-07 11:19:51.586989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3582659 -4.3591528 -4.3617783 -4.3632584 -4.3627043 -4.3624783 -4.3622141 -4.3615808 -4.3608122 -4.3605928 -4.3617973 -4.3626022 -4.3628531 -4.3636322 -4.3654881][-4.3494081 -4.3460469 -4.3491793 -4.3540545 -4.358048 -4.3621941 -4.365407 -4.3663321 -4.365531 -4.3647027 -4.3656726 -4.3657103 -4.3645921 -4.3643379 -4.3661957][-4.3269248 -4.3169708 -4.3212376 -4.3304286 -4.3381767 -4.3444695 -4.351428 -4.3539157 -4.3524642 -4.3513532 -4.3548565 -4.3578978 -4.3592882 -4.3601556 -4.3631496][-4.2994323 -4.2817693 -4.2839479 -4.2934389 -4.2996211 -4.30444 -4.3169379 -4.3238621 -4.3222156 -4.319819 -4.3263354 -4.3370109 -4.3458338 -4.3509893 -4.3564129][-4.2743759 -4.2488341 -4.2460737 -4.25064 -4.2515273 -4.2513738 -4.2686262 -4.2811532 -4.2788291 -4.2731471 -4.2800059 -4.2994604 -4.320148 -4.332901 -4.3429966][-4.2474504 -4.2107449 -4.1955848 -4.1909986 -4.1843891 -4.1776109 -4.1960349 -4.211926 -4.2093725 -4.2020578 -4.2114992 -4.2429285 -4.2806625 -4.3058105 -4.3216248][-4.2111177 -4.1575155 -4.1207662 -4.0979304 -4.0766535 -4.0627303 -4.0866313 -4.1130567 -4.1201267 -4.1213021 -4.1411676 -4.1888905 -4.2419558 -4.2801218 -4.3026867][-4.1705475 -4.1023722 -4.0498667 -4.0114312 -3.9758184 -3.9549704 -3.9862316 -4.0268235 -4.0446959 -4.057023 -4.0872035 -4.1462584 -4.2082157 -4.2562485 -4.286788][-4.1340222 -4.0588479 -4.0095849 -3.982131 -3.9581127 -3.9415522 -3.9694233 -4.0110493 -4.0289688 -4.040113 -4.0669427 -4.1239605 -4.1862836 -4.2371616 -4.2728415][-4.1050954 -4.035243 -4.0041056 -4.0097671 -4.0212593 -4.0200667 -4.0338068 -4.0582018 -4.0681539 -4.0689969 -4.0821476 -4.1274195 -4.1836157 -4.2316217 -4.2672763][-4.1030893 -4.04909 -4.0396018 -4.072679 -4.1087346 -4.1205072 -4.1235557 -4.127852 -4.1282048 -4.1223536 -4.1244273 -4.1557527 -4.2024531 -4.2447414 -4.2765145][-4.1133919 -4.0759282 -4.0812693 -4.12542 -4.1690626 -4.1864038 -4.1870117 -4.185236 -4.1835752 -4.1797123 -4.1798515 -4.2009492 -4.2339959 -4.2653551 -4.2907639][-4.1272178 -4.1019897 -4.1168075 -4.161478 -4.20268 -4.2198596 -4.2234507 -4.2275772 -4.2322254 -4.2338028 -4.2360578 -4.2513671 -4.2726951 -4.29217 -4.3101974][-4.1644893 -4.1526546 -4.1692944 -4.20448 -4.2350588 -4.2456732 -4.2482677 -4.2546763 -4.2621884 -4.2651057 -4.2682018 -4.2800813 -4.2961812 -4.3106422 -4.3250575][-4.21815 -4.2144012 -4.225203 -4.2465196 -4.2633905 -4.2686553 -4.268651 -4.2731 -4.2783551 -4.279645 -4.2816095 -4.2914882 -4.3059874 -4.3192825 -4.3329067]]...]
INFO - root - 2017-12-07 11:19:58.440890: step 2510, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.727 sec/batch; 66h:40m:46s remains)
INFO - root - 2017-12-07 11:20:05.203112: step 2520, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 64h:50m:14s remains)
INFO - root - 2017-12-07 11:20:12.018303: step 2530, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 58h:22m:54s remains)
INFO - root - 2017-12-07 11:20:18.934890: step 2540, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 63h:41m:06s remains)
INFO - root - 2017-12-07 11:20:25.787807: step 2550, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 65h:31m:38s remains)
INFO - root - 2017-12-07 11:20:32.704797: step 2560, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 67h:09m:23s remains)
INFO - root - 2017-12-07 11:20:39.507610: step 2570, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 60h:36m:08s remains)
INFO - root - 2017-12-07 11:20:46.283085: step 2580, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 57h:35m:36s remains)
INFO - root - 2017-12-07 11:20:52.891007: step 2590, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 61h:00m:38s remains)
INFO - root - 2017-12-07 11:20:59.663718: step 2600, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.731 sec/batch; 66h:58m:37s remains)
2017-12-07 11:21:00.432259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1901174 -4.1928697 -4.1963735 -4.1983037 -4.1993818 -4.2001281 -4.1987228 -4.1889434 -4.16748 -4.1468797 -4.1313219 -4.1151814 -4.1222181 -4.1469164 -4.1666117][-4.1693335 -4.1668186 -4.1740289 -4.1828227 -4.1895843 -4.1980934 -4.2037444 -4.1988831 -4.1740718 -4.1399107 -4.111062 -4.0907879 -4.0992079 -4.1207495 -4.1406908][-4.1585197 -4.1598325 -4.1742105 -4.1895623 -4.1981697 -4.20532 -4.2059669 -4.1975436 -4.1726928 -4.1341085 -4.0999112 -4.0751472 -4.0736632 -4.0802507 -4.0914931][-4.158905 -4.1692152 -4.1868396 -4.2037559 -4.2078867 -4.2088308 -4.2026215 -4.189826 -4.1706867 -4.1392412 -4.1068435 -4.0746379 -4.0544491 -4.0415049 -4.0398684][-4.1693978 -4.1843386 -4.2029619 -4.2149124 -4.2112737 -4.2029233 -4.1885867 -4.1697164 -4.1526833 -4.1323743 -4.1102319 -4.0782146 -4.043468 -4.0141964 -4.0015135][-4.1766777 -4.1896687 -4.2044396 -4.2130342 -4.2068133 -4.1920824 -4.1668692 -4.1377048 -4.1180005 -4.1070991 -4.0983562 -4.0748172 -4.0414491 -4.0112224 -4.0009956][-4.1656566 -4.1799588 -4.1959906 -4.2057858 -4.200376 -4.1800132 -4.1438794 -4.1044273 -4.0839758 -4.0819764 -4.0850172 -4.0782695 -4.0574574 -4.037004 -4.0336738][-4.1597495 -4.1861491 -4.2057586 -4.2141495 -4.2070212 -4.1800308 -4.1379189 -4.0958037 -4.0782175 -4.0784388 -4.08565 -4.0917983 -4.0833359 -4.0695333 -4.0669742][-4.1593804 -4.1991978 -4.222682 -4.228559 -4.2215962 -4.193614 -4.1510429 -4.1114173 -4.0976472 -4.0990205 -4.1087337 -4.1238208 -4.1259756 -4.1162062 -4.1147223][-4.1626859 -4.20926 -4.2356162 -4.2413492 -4.23523 -4.2103186 -4.1711745 -4.1385736 -4.1319504 -4.1420355 -4.1526775 -4.1644497 -4.1628385 -4.1538095 -4.1550007][-4.1758127 -4.2200179 -4.2467575 -4.2553258 -4.2512569 -4.2293391 -4.195456 -4.1716709 -4.1748257 -4.1908607 -4.1992354 -4.1985216 -4.1862659 -4.17673 -4.1829009][-4.1918278 -4.2300334 -4.2561951 -4.2691193 -4.2687049 -4.2511282 -4.2236309 -4.2085547 -4.21564 -4.2300348 -4.2353792 -4.2264304 -4.2093444 -4.2015204 -4.2088265][-4.2084842 -4.2367272 -4.2606354 -4.2763205 -4.279748 -4.2708311 -4.2523413 -4.2446995 -4.2517424 -4.2612724 -4.2634745 -4.251071 -4.2340302 -4.2281718 -4.2329049][-4.2354388 -4.2491174 -4.265976 -4.2813911 -4.2888269 -4.2871461 -4.2778864 -4.2754436 -4.2807827 -4.2859945 -4.286705 -4.2774925 -4.2657042 -4.2623925 -4.2646604][-4.2689338 -4.2731414 -4.2827215 -4.2927194 -4.2989445 -4.3003931 -4.296905 -4.2965927 -4.3006034 -4.3051486 -4.3072467 -4.3030853 -4.2963834 -4.2933955 -4.2938347]]...]
INFO - root - 2017-12-07 11:21:07.147711: step 2610, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 57h:41m:54s remains)
INFO - root - 2017-12-07 11:21:14.025932: step 2620, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 61h:08m:16s remains)
INFO - root - 2017-12-07 11:21:20.818580: step 2630, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 66h:11m:34s remains)
INFO - root - 2017-12-07 11:21:27.681723: step 2640, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 65h:03m:48s remains)
INFO - root - 2017-12-07 11:21:34.457721: step 2650, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 59h:47m:03s remains)
INFO - root - 2017-12-07 11:21:41.138336: step 2660, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.620 sec/batch; 56h:49m:09s remains)
INFO - root - 2017-12-07 11:21:47.991074: step 2670, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 61h:41m:29s remains)
INFO - root - 2017-12-07 11:21:55.028051: step 2680, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 65h:22m:37s remains)
INFO - root - 2017-12-07 11:22:01.643844: step 2690, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 61h:49m:50s remains)
INFO - root - 2017-12-07 11:22:08.439756: step 2700, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 57h:07m:02s remains)
2017-12-07 11:22:09.290808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2118897 -4.2348175 -4.2644362 -4.2923594 -4.3041549 -4.3068395 -4.3103652 -4.3154821 -4.3168478 -4.3138108 -4.3106341 -4.3082685 -4.3077531 -4.30734 -4.3058009][-4.2183533 -4.2384458 -4.2592621 -4.2801681 -4.288744 -4.2893634 -4.2926888 -4.2968864 -4.2987309 -4.2973 -4.2964244 -4.2971277 -4.2988768 -4.2995081 -4.2989659][-4.2468834 -4.2592936 -4.26599 -4.272788 -4.2717333 -4.2658548 -4.2674389 -4.2720437 -4.2762671 -4.2796793 -4.2839556 -4.288815 -4.2932081 -4.29401 -4.2924743][-4.2649813 -4.2683535 -4.2632027 -4.258441 -4.2489791 -4.2382336 -4.2368522 -4.2450151 -4.2534771 -4.2611828 -4.2692876 -4.277885 -4.2832227 -4.2806387 -4.2747459][-4.2378306 -4.2328978 -4.2207484 -4.209363 -4.1951118 -4.1838536 -4.1851516 -4.2021666 -4.2190166 -4.2318358 -4.2439246 -4.2576966 -4.2655711 -4.2585225 -4.2472897][-4.1809912 -4.1690645 -4.1533928 -4.1392894 -4.1254168 -4.1153016 -4.1194348 -4.146132 -4.1775947 -4.2023978 -4.2221045 -4.240613 -4.2479668 -4.23384 -4.2153258][-4.1075983 -4.0874496 -4.0648928 -4.0457644 -4.0289636 -4.0133395 -4.0142913 -4.0504727 -4.1006293 -4.1441236 -4.1751614 -4.1990008 -4.2059073 -4.1875496 -4.1669531][-4.0403075 -4.0101523 -3.9794314 -3.9564991 -3.9344969 -3.9080725 -3.8998137 -3.9379067 -4.0043621 -4.066937 -4.1128664 -4.1431913 -4.1500864 -4.1323485 -4.1151323][-4.04734 -4.0168929 -3.9945011 -3.9823821 -3.9670095 -3.9415143 -3.9280381 -3.9578803 -4.0194745 -4.0793371 -4.1212406 -4.1434894 -4.1415949 -4.1182737 -4.0925694][-4.1112976 -4.0912633 -4.0827718 -4.0854931 -4.0832391 -4.0698643 -4.0625358 -4.0825238 -4.1226087 -4.1608467 -4.187314 -4.1983442 -4.1906424 -4.1659465 -4.1318483][-4.1864624 -4.1694312 -4.1586576 -4.1607533 -4.1651378 -4.1614351 -4.1607056 -4.175765 -4.2001839 -4.22073 -4.2327256 -4.2351141 -4.2284751 -4.2119756 -4.1795053][-4.2213359 -4.2051926 -4.1865592 -4.1807694 -4.1869869 -4.1882362 -4.1893349 -4.19933 -4.2124567 -4.2186613 -4.216238 -4.2108355 -4.2079186 -4.2024679 -4.1786866][-4.2086568 -4.1901393 -4.163981 -4.1516762 -4.1576452 -4.1593118 -4.1606092 -4.1680989 -4.1756968 -4.1757789 -4.1696086 -4.167851 -4.1751032 -4.1821127 -4.1738577][-4.1927185 -4.1700697 -4.1400604 -4.1260595 -4.1324539 -4.136961 -4.1402073 -4.1461148 -4.1524453 -4.1532044 -4.1517711 -4.1581745 -4.1739454 -4.1894269 -4.1921391][-4.1945877 -4.1715736 -4.14459 -4.1317453 -4.1376739 -4.1452107 -4.1506896 -4.154953 -4.1597581 -4.1632967 -4.1661115 -4.1758752 -4.1933231 -4.211504 -4.2181973]]...]
INFO - root - 2017-12-07 11:22:16.106594: step 2710, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.685 sec/batch; 62h:44m:08s remains)
INFO - root - 2017-12-07 11:22:22.699925: step 2720, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:59m:11s remains)
INFO - root - 2017-12-07 11:22:29.571600: step 2730, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 65h:03m:55s remains)
INFO - root - 2017-12-07 11:22:36.492695: step 2740, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 0.758 sec/batch; 69h:26m:21s remains)
INFO - root - 2017-12-07 11:22:43.214156: step 2750, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 61h:48m:35s remains)
INFO - root - 2017-12-07 11:22:49.999212: step 2760, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 60h:01m:20s remains)
INFO - root - 2017-12-07 11:22:56.839551: step 2770, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 59h:11m:54s remains)
INFO - root - 2017-12-07 11:23:03.677533: step 2780, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 66h:09m:18s remains)
INFO - root - 2017-12-07 11:23:10.338638: step 2790, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 63h:19m:35s remains)
INFO - root - 2017-12-07 11:23:17.121706: step 2800, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 62h:56m:13s remains)
2017-12-07 11:23:17.848361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1618495 -4.16458 -4.1625228 -4.1640434 -4.1689153 -4.164854 -4.1270394 -4.0799322 -4.0666032 -4.1070476 -4.1696711 -4.2296 -4.275002 -4.3091254 -4.32395][-4.1333466 -4.1207337 -4.1090264 -4.111341 -4.1225967 -4.1243939 -4.0900049 -4.0471225 -4.0377526 -4.0855455 -4.1561689 -4.2213683 -4.2669358 -4.3007164 -4.3153315][-4.1488152 -4.120441 -4.0925016 -4.083528 -4.0922136 -4.1003752 -4.08053 -4.047513 -4.0417581 -4.0900645 -4.16053 -4.2210231 -4.2613831 -4.2936153 -4.3075719][-4.1923432 -4.1557808 -4.1130862 -4.0836916 -4.0743804 -4.0767241 -4.0709639 -4.0509391 -4.0513706 -4.1054645 -4.1769714 -4.2303309 -4.2669463 -4.2978168 -4.3106422][-4.2208533 -4.1806417 -4.1342707 -4.0911632 -4.0577393 -4.0367312 -4.0331783 -4.0221224 -4.0314851 -4.0992718 -4.1803579 -4.2331924 -4.2670746 -4.2996645 -4.3132267][-4.2364182 -4.1972108 -4.1458769 -4.0888138 -4.0269494 -3.9799333 -3.9674106 -3.9601281 -3.9795766 -4.0648327 -4.1585679 -4.2191839 -4.2540879 -4.2908936 -4.3091168][-4.2430935 -4.2070374 -4.1533871 -4.0751672 -3.983624 -3.9222987 -3.91118 -3.9051256 -3.9340918 -4.0329375 -4.1375208 -4.2046738 -4.2426596 -4.2839046 -4.3063173][-4.2414045 -4.2073379 -4.1513863 -4.0598793 -3.9506404 -3.8842857 -3.8783357 -3.8735619 -3.9093456 -4.0116343 -4.1207814 -4.1949811 -4.2404881 -4.2840476 -4.3076725][-4.2341232 -4.2016492 -4.1515203 -4.0758429 -3.984055 -3.9292014 -3.9205492 -3.9074008 -3.9323266 -4.0194216 -4.11932 -4.1940713 -4.2432675 -4.2873244 -4.3100672][-4.2170663 -4.1822214 -4.1468258 -4.1102319 -4.0694284 -4.0353212 -4.0133491 -3.9854164 -3.9929228 -4.0522938 -4.1326218 -4.1997786 -4.2438307 -4.2837915 -4.3066015][-4.1830769 -4.1454277 -4.1218715 -4.114851 -4.1126132 -4.0973783 -4.0762181 -4.0483456 -4.0489759 -4.0937724 -4.15685 -4.2114224 -4.2491732 -4.2833314 -4.3053155][-4.144877 -4.10655 -4.0923958 -4.0994072 -4.1106009 -4.1024151 -4.0844131 -4.0660157 -4.0786281 -4.12118 -4.1754112 -4.2215424 -4.2551436 -4.2876077 -4.3083506][-4.1364765 -4.1021481 -4.0954385 -4.1120243 -4.1223683 -4.1054769 -4.0828228 -4.072073 -4.0933089 -4.1340809 -4.1840754 -4.2278252 -4.2596846 -4.2931585 -4.3134937][-4.1423497 -4.104528 -4.1008368 -4.1237149 -4.1353579 -4.1150761 -4.0922956 -4.0890303 -4.1145496 -4.1511703 -4.1949654 -4.2330813 -4.2610087 -4.2956491 -4.3184075][-4.148458 -4.1096988 -4.0992908 -4.117065 -4.1328263 -4.1236577 -4.1081481 -4.1103444 -4.1372266 -4.169476 -4.2087326 -4.2402053 -4.2630539 -4.2953091 -4.3184505]]...]
INFO - root - 2017-12-07 11:23:24.679790: step 2810, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 65h:48m:09s remains)
INFO - root - 2017-12-07 11:23:31.523519: step 2820, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 59h:04m:18s remains)
INFO - root - 2017-12-07 11:23:38.332673: step 2830, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:12m:05s remains)
INFO - root - 2017-12-07 11:23:45.084468: step 2840, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:40m:45s remains)
INFO - root - 2017-12-07 11:23:51.986475: step 2850, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 66h:05m:14s remains)
INFO - root - 2017-12-07 11:23:58.885000: step 2860, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 64h:52m:47s remains)
INFO - root - 2017-12-07 11:24:05.754493: step 2870, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 60h:06m:27s remains)
INFO - root - 2017-12-07 11:24:12.593441: step 2880, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 57h:48m:22s remains)
INFO - root - 2017-12-07 11:24:19.208851: step 2890, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 63h:27m:05s remains)
INFO - root - 2017-12-07 11:24:26.038059: step 2900, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.743 sec/batch; 68h:01m:45s remains)
2017-12-07 11:24:26.831775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2858973 -4.2969394 -4.3039632 -4.3076096 -4.306344 -4.2932086 -4.2630181 -4.2338982 -4.2150269 -4.2169003 -4.24267 -4.2696643 -4.2793627 -4.269382 -4.2535844][-4.28819 -4.2962618 -4.2954679 -4.2917418 -4.2898912 -4.2820344 -4.2589912 -4.2355556 -4.2202759 -4.2261195 -4.2577195 -4.2908468 -4.3030109 -4.2926011 -4.2732372][-4.2871614 -4.2924423 -4.2827878 -4.2717657 -4.2712374 -4.2762222 -4.2671003 -4.2494159 -4.2315636 -4.2338271 -4.2644377 -4.2985916 -4.3105483 -4.3017583 -4.2836728][-4.2888322 -4.2896166 -4.2739639 -4.2598248 -4.2632179 -4.2792645 -4.282743 -4.2709336 -4.2502561 -4.2452168 -4.2682114 -4.2950025 -4.301796 -4.2919893 -4.27543][-4.2899327 -4.2897539 -4.2738914 -4.2605476 -4.2652645 -4.2817273 -4.2892618 -4.2821622 -4.2636347 -4.2539577 -4.2671862 -4.2835879 -4.283349 -4.2700067 -4.2541614][-4.2863297 -4.291451 -4.2818251 -4.2733541 -4.2775345 -4.2884483 -4.2937479 -4.2892222 -4.2749314 -4.2630482 -4.2668285 -4.2727275 -4.26542 -4.2481122 -4.2326903][-4.2810893 -4.2935605 -4.2922096 -4.2899623 -4.2932935 -4.2975826 -4.2988734 -4.2961812 -4.2866406 -4.2770023 -4.2782121 -4.2791662 -4.267168 -4.2460408 -4.22705][-4.2714615 -4.2923031 -4.2995439 -4.2993445 -4.2963567 -4.29264 -4.2906008 -4.2888765 -4.284586 -4.2814221 -4.2863183 -4.2892838 -4.2789178 -4.2588615 -4.2379856][-4.2509675 -4.2850113 -4.304018 -4.3038454 -4.2906165 -4.2757773 -4.2690253 -4.2659616 -4.262929 -4.2632642 -4.2703695 -4.2753696 -4.2689791 -4.2569432 -4.2439418][-4.2199683 -4.2692423 -4.3004651 -4.300683 -4.2783494 -4.2533426 -4.2439337 -4.2420039 -4.2390943 -4.2381849 -4.2419972 -4.2455549 -4.2434726 -4.2415481 -4.2406297][-4.1995616 -4.2591734 -4.2961936 -4.2966485 -4.2703328 -4.2399955 -4.2305436 -4.2311735 -4.2263737 -4.2174516 -4.2119341 -4.2100639 -4.2079344 -4.2130122 -4.2242713][-4.2112074 -4.269228 -4.3013897 -4.3002129 -4.2749271 -4.2450843 -4.2365966 -4.237988 -4.2304778 -4.2146559 -4.1971951 -4.1843038 -4.1748695 -4.182373 -4.2015481][-4.24422 -4.2860856 -4.3044124 -4.2974076 -4.2764292 -4.2548475 -4.2507877 -4.2540226 -4.2487688 -4.2328329 -4.2094445 -4.1843657 -4.1648989 -4.1707635 -4.1886635][-4.2685924 -4.28894 -4.2927909 -4.2824149 -4.2717824 -4.262527 -4.2627106 -4.2674713 -4.2654648 -4.2538142 -4.2314215 -4.2023668 -4.1811676 -4.1848726 -4.1926308][-4.266489 -4.2707644 -4.266953 -4.2594924 -4.2613053 -4.263763 -4.2678485 -4.2735462 -4.2750688 -4.2676177 -4.251153 -4.2275281 -4.2119155 -4.2141066 -4.2101884]]...]
INFO - root - 2017-12-07 11:24:33.690935: step 2910, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:23m:21s remains)
INFO - root - 2017-12-07 11:24:40.437644: step 2920, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 65h:56m:22s remains)
INFO - root - 2017-12-07 11:24:47.297836: step 2930, loss = 2.10, batch loss = 2.05 (11.2 examples/sec; 0.715 sec/batch; 65h:25m:02s remains)
INFO - root - 2017-12-07 11:24:54.086688: step 2940, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 58h:57m:23s remains)
INFO - root - 2017-12-07 11:25:00.957979: step 2950, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.681 sec/batch; 62h:17m:45s remains)
INFO - root - 2017-12-07 11:25:07.796410: step 2960, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.716 sec/batch; 65h:29m:55s remains)
INFO - root - 2017-12-07 11:25:14.564805: step 2970, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.750 sec/batch; 68h:40m:21s remains)
INFO - root - 2017-12-07 11:25:21.400171: step 2980, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 63h:08m:29s remains)
INFO - root - 2017-12-07 11:25:28.009332: step 2990, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:14m:55s remains)
INFO - root - 2017-12-07 11:25:34.811367: step 3000, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 59h:13m:49s remains)
2017-12-07 11:25:35.541678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1783247 -4.2139206 -4.2514844 -4.2672219 -4.2484612 -4.1958046 -4.150209 -4.1412525 -4.1584458 -4.1808624 -4.1950951 -4.2172794 -4.2262325 -4.2247643 -4.2245193][-4.1665387 -4.1987391 -4.2374573 -4.2561412 -4.237339 -4.1833215 -4.1356058 -4.1272678 -4.1491446 -4.1748433 -4.187078 -4.2061496 -4.2156992 -4.2170782 -4.2170739][-4.151268 -4.177352 -4.2174745 -4.24257 -4.2321858 -4.1900086 -4.1544666 -4.1535215 -4.1773577 -4.2007918 -4.2089629 -4.2200489 -4.2228537 -4.2215085 -4.2206974][-4.1466417 -4.1625152 -4.1963053 -4.2222705 -4.21762 -4.1903877 -4.1764627 -4.191288 -4.2224383 -4.2469878 -4.2552376 -4.2616897 -4.25732 -4.2525482 -4.250824][-4.1438441 -4.1413512 -4.1572013 -4.1752949 -4.1719551 -4.1519895 -4.1456156 -4.1700678 -4.2124171 -4.2483392 -4.2644157 -4.2721119 -4.2701259 -4.2733331 -4.27826][-4.1263242 -4.1018219 -4.0974531 -4.1105566 -4.1097674 -4.080822 -4.0576649 -4.0782771 -4.1397176 -4.1946397 -4.2206717 -4.2329421 -4.2367582 -4.25154 -4.266664][-4.1002941 -4.0473909 -4.0130973 -4.0121746 -4.002429 -3.947376 -3.8800044 -3.890532 -3.9959514 -4.0907211 -4.1311789 -4.1448951 -4.1585064 -4.1885734 -4.2132487][-4.1161752 -4.0459194 -3.9840093 -3.9615593 -3.935606 -3.8512626 -3.7317336 -3.7332764 -3.8862309 -4.0112977 -4.0556426 -4.0617528 -4.0765243 -4.109592 -4.1353993][-4.1722322 -4.1151242 -4.0599866 -4.0331659 -4.010674 -3.9458885 -3.8603327 -3.8684309 -3.982409 -4.0785546 -4.1021309 -4.0810466 -4.06754 -4.0795593 -4.0980754][-4.2328143 -4.1978526 -4.1618123 -4.1398172 -4.1244802 -4.0889378 -4.0476284 -4.0550423 -4.1196933 -4.1754 -4.1803269 -4.1484866 -4.1194143 -4.1133213 -4.1214337][-4.2722054 -4.2524877 -4.2320142 -4.2160883 -4.2054377 -4.1867332 -4.1659107 -4.1636362 -4.18975 -4.2145319 -4.2037168 -4.1677 -4.1360211 -4.1211815 -4.1170769][-4.2870841 -4.2769394 -4.26852 -4.2624955 -4.2588844 -4.2480478 -4.2311263 -4.2184072 -4.2191877 -4.2169409 -4.1875587 -4.1434503 -4.103786 -4.0770249 -4.056767][-4.2974176 -4.2900586 -4.2856135 -4.2839279 -4.2833467 -4.27502 -4.2584958 -4.242372 -4.232563 -4.2157421 -4.1725554 -4.1185412 -4.0760641 -4.0472164 -4.0149965][-4.302309 -4.2949376 -4.290154 -4.289207 -4.2907104 -4.2856503 -4.2720871 -4.2550969 -4.2410474 -4.219913 -4.1780219 -4.1292191 -4.0915861 -4.06831 -4.0349011][-4.3127408 -4.3061485 -4.3016491 -4.3000083 -4.2996664 -4.2961073 -4.2862005 -4.2717252 -4.2569647 -4.2395043 -4.2114592 -4.1785817 -4.1516685 -4.1335959 -4.1090951]]...]
INFO - root - 2017-12-07 11:25:42.354151: step 3010, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 62h:47m:08s remains)
INFO - root - 2017-12-07 11:25:49.175744: step 3020, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 58h:39m:36s remains)
INFO - root - 2017-12-07 11:25:55.832530: step 3030, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 61h:41m:17s remains)
INFO - root - 2017-12-07 11:26:02.634872: step 3040, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 66h:10m:43s remains)
INFO - root - 2017-12-07 11:26:09.441300: step 3050, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 66h:12m:15s remains)
INFO - root - 2017-12-07 11:26:16.345273: step 3060, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 60h:04m:02s remains)
INFO - root - 2017-12-07 11:26:23.140292: step 3070, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 59h:08m:15s remains)
INFO - root - 2017-12-07 11:26:29.970506: step 3080, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 62h:56m:11s remains)
INFO - root - 2017-12-07 11:26:36.690298: step 3090, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.750 sec/batch; 68h:38m:11s remains)
INFO - root - 2017-12-07 11:26:43.389231: step 3100, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 61h:25m:06s remains)
2017-12-07 11:26:44.109150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2077546 -4.2035193 -4.1805263 -4.1591096 -4.1606712 -4.1735353 -4.1674919 -4.1419997 -4.1346397 -4.158937 -4.1891189 -4.1966619 -4.1718807 -4.1387587 -4.1339974][-4.2136579 -4.2037082 -4.1743207 -4.1516089 -4.1546812 -4.1637158 -4.1510124 -4.1232142 -4.1167026 -4.1430054 -4.1748414 -4.182272 -4.1551609 -4.1207433 -4.1205916][-4.1973586 -4.1893435 -4.1639743 -4.1469488 -4.1521821 -4.1589756 -4.1457028 -4.1212225 -4.1203566 -4.1545014 -4.1934772 -4.205667 -4.1812677 -4.1465588 -4.1398468][-4.1805072 -4.1846251 -4.1766119 -4.1755753 -4.1865153 -4.1918783 -4.1809011 -4.1644778 -4.1711698 -4.208571 -4.2466092 -4.2586107 -4.2379575 -4.2030535 -4.1818161][-4.1929779 -4.205966 -4.2144594 -4.2242 -4.2332082 -4.2316246 -4.2191353 -4.2096639 -4.21984 -4.2497921 -4.2761903 -4.2848816 -4.2706809 -4.2410183 -4.2120709][-4.205832 -4.2162695 -4.2282829 -4.2373633 -4.2365808 -4.223258 -4.2038064 -4.1996007 -4.2139997 -4.237669 -4.2557163 -4.2634087 -4.2582922 -4.2380371 -4.2122812][-4.2022071 -4.1978512 -4.2040434 -4.2073746 -4.1930075 -4.1601868 -4.1255918 -4.1222458 -4.1471944 -4.1762867 -4.1953454 -4.2063308 -4.2116671 -4.2073126 -4.1959996][-4.176722 -4.1511836 -4.1453362 -4.1427293 -4.1147423 -4.0597157 -4.0061579 -4.0056024 -4.0511 -4.1007876 -4.1327348 -4.1531944 -4.1731591 -4.1856403 -4.1916151][-4.1557703 -4.1165767 -4.1026669 -4.0978789 -4.0655308 -4.0018816 -3.9426641 -3.9479926 -4.010859 -4.079803 -4.1261592 -4.1555896 -4.1837649 -4.2025309 -4.2137146][-4.1632352 -4.13554 -4.1313128 -4.1351118 -4.1150737 -4.0691853 -4.024848 -4.0289578 -4.080759 -4.1407223 -4.1817465 -4.2046156 -4.2221951 -4.2324929 -4.2395353][-4.1874042 -4.1805067 -4.1926746 -4.2090268 -4.2063642 -4.183959 -4.1573257 -4.158042 -4.1920233 -4.2303853 -4.2536292 -4.2603769 -4.2587676 -4.2541995 -4.2522159][-4.2206545 -4.2276158 -4.24647 -4.2658319 -4.271728 -4.2624474 -4.2466283 -4.2434831 -4.2611408 -4.281208 -4.2906828 -4.2857294 -4.2719593 -4.2583447 -4.2503333][-4.2575054 -4.2655382 -4.2792039 -4.29197 -4.297718 -4.294353 -4.2852383 -4.2794008 -4.2860255 -4.29404 -4.2957134 -4.2869987 -4.2726154 -4.2615094 -4.25529][-4.280818 -4.2829432 -4.2848463 -4.2860718 -4.285603 -4.2811279 -4.2728286 -4.2643571 -4.2648826 -4.2708445 -4.2749658 -4.2706389 -4.262713 -4.2596192 -4.2590504][-4.275259 -4.2676539 -4.2560616 -4.244525 -4.2368193 -4.2310505 -4.2233162 -4.2150278 -4.2146816 -4.225605 -4.2395892 -4.2440481 -4.2422309 -4.2451358 -4.2479582]]...]
INFO - root - 2017-12-07 11:26:50.912654: step 3110, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 66h:46m:29s remains)
INFO - root - 2017-12-07 11:26:57.553552: step 3120, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 63h:56m:05s remains)
INFO - root - 2017-12-07 11:27:04.330636: step 3130, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 62h:47m:41s remains)
INFO - root - 2017-12-07 11:27:11.057559: step 3140, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 56h:17m:33s remains)
INFO - root - 2017-12-07 11:27:17.811859: step 3150, loss = 2.03, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 59h:24m:02s remains)
INFO - root - 2017-12-07 11:27:24.650037: step 3160, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 63h:33m:09s remains)
INFO - root - 2017-12-07 11:27:31.377532: step 3170, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 65h:41m:01s remains)
INFO - root - 2017-12-07 11:27:38.063544: step 3180, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 63h:04m:41s remains)
INFO - root - 2017-12-07 11:27:44.695483: step 3190, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 57h:08m:57s remains)
INFO - root - 2017-12-07 11:27:51.419231: step 3200, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 58h:26m:29s remains)
2017-12-07 11:27:52.184582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3111491 -4.3020792 -4.292376 -4.2881656 -4.2928796 -4.2992067 -4.2983546 -4.2935171 -4.2865167 -4.282887 -4.2815547 -4.2787004 -4.2812004 -4.2892547 -4.299222][-4.2950749 -4.278327 -4.2620616 -4.2534504 -4.2602315 -4.2733674 -4.2743826 -4.2673798 -4.2594 -4.2568336 -4.2587461 -4.2577891 -4.2624912 -4.2733088 -4.2833152][-4.2794113 -4.2527556 -4.2283092 -4.2143831 -4.220017 -4.23712 -4.2369761 -4.2264686 -4.2196541 -4.2231503 -4.2326469 -4.2386541 -4.2478919 -4.2618518 -4.2718573][-4.2602882 -4.2257047 -4.1935568 -4.1772623 -4.1808658 -4.1975226 -4.1868153 -4.1661472 -4.16195 -4.1732516 -4.1910615 -4.205266 -4.2218513 -4.2431178 -4.2578321][-4.2376108 -4.1957045 -4.1567864 -4.1356325 -4.1327658 -4.1420908 -4.119977 -4.0926104 -4.0927105 -4.1116424 -4.1348329 -4.1560144 -4.1829009 -4.2171235 -4.2406726][-4.2128868 -4.1674991 -4.1214266 -4.0893846 -4.0718975 -4.06681 -4.0355215 -4.0057421 -4.00608 -4.0267792 -4.0506258 -4.0765572 -4.1200271 -4.1773 -4.2184777][-4.1751366 -4.1269803 -4.0755925 -4.0331612 -4.0030632 -3.9858096 -3.9472492 -3.9071367 -3.8973198 -3.9080105 -3.9261107 -3.9607542 -4.0291872 -4.1197953 -4.1874981][-4.1212764 -4.0683589 -4.012713 -3.9653673 -3.9308176 -3.9115758 -3.8714745 -3.8201249 -3.7988956 -3.8036888 -3.8227181 -3.8665385 -3.957931 -4.0785046 -4.1662292][-4.0923553 -4.0419335 -3.9910889 -3.9476919 -3.9146433 -3.8966942 -3.8566289 -3.8034625 -3.7867892 -3.799825 -3.8274379 -3.8753405 -3.9686446 -4.0885782 -4.1762571][-4.1151514 -4.0786705 -4.0403605 -4.0061288 -3.9789598 -3.9615028 -3.9273255 -3.8875186 -3.8842115 -3.9070659 -3.9389548 -3.9841259 -4.0573535 -4.1482458 -4.2163811][-4.16724 -4.1441984 -4.1174259 -4.0976434 -4.0805187 -4.0656824 -4.0426626 -4.0197468 -4.0239682 -4.0462842 -4.0742478 -4.1103115 -4.1613379 -4.2223639 -4.2684326][-4.2233243 -4.2121916 -4.1980548 -4.1913347 -4.1827078 -4.171751 -4.160152 -4.1552963 -4.1646013 -4.1787176 -4.1976013 -4.2227664 -4.2548456 -4.2897205 -4.3144884][-4.2665653 -4.2639108 -4.2599983 -4.2599211 -4.2574406 -4.2500939 -4.2446623 -4.2450285 -4.25282 -4.2632866 -4.2770815 -4.2939057 -4.312933 -4.3290749 -4.3388042][-4.30067 -4.3010106 -4.3008494 -4.3024764 -4.3022752 -4.2989016 -4.2962565 -4.2957907 -4.3012977 -4.3086929 -4.3169374 -4.3272462 -4.3375759 -4.3439112 -4.34716][-4.3266139 -4.3258042 -4.3265033 -4.3286433 -4.3288603 -4.3272753 -4.325542 -4.3243861 -4.3269868 -4.33091 -4.334909 -4.3404045 -4.3459859 -4.3496361 -4.3515391]]...]
INFO - root - 2017-12-07 11:27:58.876718: step 3210, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 61h:23m:19s remains)
INFO - root - 2017-12-07 11:28:05.768248: step 3220, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 60h:40m:58s remains)
INFO - root - 2017-12-07 11:28:12.568362: step 3230, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 61h:13m:23s remains)
INFO - root - 2017-12-07 11:28:19.357792: step 3240, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 66h:29m:42s remains)
INFO - root - 2017-12-07 11:28:26.262510: step 3250, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.752 sec/batch; 68h:47m:10s remains)
INFO - root - 2017-12-07 11:28:33.162635: step 3260, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 60h:23m:35s remains)
INFO - root - 2017-12-07 11:28:39.894982: step 3270, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 57h:50m:56s remains)
INFO - root - 2017-12-07 11:28:46.631309: step 3280, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 61h:30m:47s remains)
INFO - root - 2017-12-07 11:28:53.289960: step 3290, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 66h:24m:41s remains)
INFO - root - 2017-12-07 11:29:00.019903: step 3300, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 59h:58m:04s remains)
2017-12-07 11:29:00.755809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22812 -4.206285 -4.2112036 -4.2400208 -4.2725844 -4.3007784 -4.3220849 -4.3276248 -4.3188977 -4.2991977 -4.2778354 -4.2635736 -4.246789 -4.2430677 -4.2569108][-4.2216358 -4.2096996 -4.2250619 -4.2571564 -4.2882509 -4.3135686 -4.3335662 -4.3443756 -4.345788 -4.3372383 -4.3259182 -4.3164454 -4.3016009 -4.2907319 -4.286231][-4.2071304 -4.2023768 -4.2250853 -4.2592845 -4.288249 -4.309104 -4.3247781 -4.33485 -4.3387318 -4.3399973 -4.34246 -4.3444986 -4.3395615 -4.3297057 -4.3133459][-4.216506 -4.2162886 -4.2373028 -4.2621965 -4.2788558 -4.2867737 -4.2904682 -4.29133 -4.2913737 -4.2998405 -4.3160968 -4.334547 -4.3457794 -4.3464618 -4.3312659][-4.2486067 -4.2498775 -4.2591615 -4.265099 -4.2610335 -4.2469025 -4.2289519 -4.2126083 -4.2021847 -4.2148185 -4.2465434 -4.2867918 -4.3207469 -4.3396821 -4.3359175][-4.2743359 -4.2788997 -4.2763143 -4.2630424 -4.2360058 -4.196054 -4.1498947 -4.107523 -4.0798516 -4.0964513 -4.1477141 -4.2108626 -4.2704835 -4.311275 -4.3230143][-4.2767949 -4.29235 -4.2889137 -4.2644916 -4.2169309 -4.1507239 -4.0744104 -4.0000534 -3.950175 -3.9735892 -4.0488067 -4.1341939 -4.2138958 -4.2705703 -4.2945256][-4.2750988 -4.3030424 -4.3059664 -4.2797828 -4.2205524 -4.1380172 -4.0402136 -3.9378595 -3.8656225 -3.8934484 -3.9870598 -4.0871329 -4.1784825 -4.2415781 -4.2721653][-4.2881079 -4.3185425 -4.3248563 -4.3011837 -4.2429371 -4.1618962 -4.0667596 -3.9663711 -3.8970802 -3.924767 -4.0128422 -4.1046033 -4.1881795 -4.2458625 -4.2769165][-4.315289 -4.3397045 -4.3444176 -4.3232713 -4.2735872 -4.2089353 -4.1387935 -4.0713277 -4.0291157 -4.052022 -4.1113386 -4.1736846 -4.232223 -4.2752671 -4.3027964][-4.3351674 -4.3493567 -4.3484826 -4.3282804 -4.2900772 -4.2484579 -4.2100616 -4.1807508 -4.1667624 -4.1831231 -4.2107406 -4.2408752 -4.2735658 -4.3027349 -4.328126][-4.3354363 -4.3404484 -4.3339958 -4.3153591 -4.290472 -4.2726083 -4.2618217 -4.2600884 -4.2640533 -4.2728677 -4.2741866 -4.27557 -4.2847748 -4.3010283 -4.3257966][-4.324173 -4.324719 -4.3141351 -4.2966104 -4.2831683 -4.2835035 -4.2922006 -4.3067832 -4.3198204 -4.3225455 -4.3061609 -4.2847857 -4.2730889 -4.277153 -4.3004255][-4.3083382 -4.3094172 -4.2978554 -4.2804008 -4.2726083 -4.2831011 -4.3021317 -4.3233414 -4.3396206 -4.3400536 -4.317296 -4.2840714 -4.259552 -4.2537985 -4.2746139][-4.2872505 -4.2925873 -4.28718 -4.2763195 -4.27466 -4.2893662 -4.3091526 -4.3278055 -4.3406777 -4.3387284 -4.3167391 -4.2838893 -4.2573414 -4.2481136 -4.2664347]]...]
INFO - root - 2017-12-07 11:29:07.653139: step 3310, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 65h:50m:07s remains)
INFO - root - 2017-12-07 11:29:14.483648: step 3320, loss = 2.11, batch loss = 2.05 (10.8 examples/sec; 0.739 sec/batch; 67h:32m:40s remains)
INFO - root - 2017-12-07 11:29:21.258205: step 3330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:36m:50s remains)
INFO - root - 2017-12-07 11:29:27.863214: step 3340, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 61h:29m:55s remains)
INFO - root - 2017-12-07 11:29:34.805650: step 3350, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 65h:58m:08s remains)
INFO - root - 2017-12-07 11:29:41.608300: step 3360, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 63h:48m:59s remains)
INFO - root - 2017-12-07 11:29:48.410434: step 3370, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 58h:51m:14s remains)
INFO - root - 2017-12-07 11:29:55.172243: step 3380, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 59h:19m:39s remains)
INFO - root - 2017-12-07 11:30:01.857305: step 3390, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 65h:34m:11s remains)
INFO - root - 2017-12-07 11:30:08.738001: step 3400, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.713 sec/batch; 65h:09m:43s remains)
2017-12-07 11:30:09.406737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1522841 -4.1524267 -4.1657372 -4.1906896 -4.2095151 -4.2146025 -4.2119079 -4.2117105 -4.222456 -4.231338 -4.248981 -4.2631674 -4.2684488 -4.2725892 -4.281302][-4.0877681 -4.0973477 -4.125617 -4.1658416 -4.1984348 -4.2107692 -4.2117443 -4.2103896 -4.21544 -4.2234774 -4.24185 -4.2541013 -4.256206 -4.2534161 -4.2531338][-4.0444274 -4.0699854 -4.1199431 -4.1741123 -4.2134056 -4.2280364 -4.2305427 -4.228766 -4.2265687 -4.2288384 -4.2384582 -4.2435908 -4.2413044 -4.2312713 -4.2184744][-4.0562739 -4.0946431 -4.1551332 -4.208693 -4.2405653 -4.2481771 -4.24725 -4.2403431 -4.2280245 -4.2245874 -4.2295561 -4.2337213 -4.2354255 -4.2206392 -4.1925769][-4.1037979 -4.1418033 -4.1991415 -4.2419152 -4.2622142 -4.2584987 -4.2462158 -4.2301326 -4.21247 -4.2091694 -4.2184424 -4.2274275 -4.2324681 -4.2187438 -4.1848593][-4.1439934 -4.1758289 -4.2205844 -4.2488165 -4.2591362 -4.2447863 -4.2189832 -4.1878419 -4.1644011 -4.1710038 -4.1924086 -4.2083693 -4.218111 -4.2120295 -4.1904664][-4.1583948 -4.1816006 -4.2110305 -4.2247782 -4.222857 -4.1982965 -4.1580749 -4.1044474 -4.0679092 -4.085742 -4.1276212 -4.1633492 -4.1907115 -4.2031126 -4.2037115][-4.1592832 -4.1655917 -4.1745863 -4.1712265 -4.1586428 -4.1252127 -4.0698843 -3.9919615 -3.9388998 -3.9791818 -4.0673842 -4.1383934 -4.191308 -4.219408 -4.23345][-4.1752553 -4.1680236 -4.1570339 -4.1409264 -4.12169 -4.0863967 -4.0242934 -3.9342978 -3.8843729 -3.9586976 -4.0792317 -4.16591 -4.2250171 -4.254478 -4.2644606][-4.1957784 -4.1804795 -4.16331 -4.1476331 -4.1343918 -4.1124406 -4.0735173 -4.0250483 -4.0104175 -4.0685306 -4.1561027 -4.2150288 -4.2494392 -4.263948 -4.2629151][-4.2102742 -4.1903095 -4.1734424 -4.1673551 -4.1704373 -4.1663837 -4.1559515 -4.1447496 -4.1452146 -4.174788 -4.2191625 -4.2425566 -4.2470927 -4.2405019 -4.2327552][-4.2178192 -4.1941767 -4.176919 -4.1757593 -4.1875825 -4.1944408 -4.1986227 -4.2012291 -4.205513 -4.2226672 -4.2416658 -4.2413082 -4.2215505 -4.198875 -4.1892877][-4.2215967 -4.1959987 -4.1738195 -4.1716766 -4.1832318 -4.194654 -4.2026134 -4.2114139 -4.2187195 -4.22978 -4.2363644 -4.224544 -4.1910143 -4.15796 -4.1464295][-4.2115273 -4.188601 -4.1663237 -4.1613512 -4.1669693 -4.1743889 -4.182085 -4.1940618 -4.2049303 -4.2119584 -4.2134938 -4.2001038 -4.1664762 -4.13085 -4.1193638][-4.2026262 -4.186831 -4.1718669 -4.1684241 -4.1648235 -4.1593118 -4.1611724 -4.1690841 -4.1770935 -4.1846714 -4.1892 -4.18075 -4.1562734 -4.1268315 -4.1181073]]...]
INFO - root - 2017-12-07 11:30:16.253754: step 3410, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 59h:59m:21s remains)
INFO - root - 2017-12-07 11:30:23.060557: step 3420, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.777 sec/batch; 71h:02m:40s remains)
INFO - root - 2017-12-07 11:30:29.980990: step 3430, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 66h:47m:11s remains)
INFO - root - 2017-12-07 11:30:36.758874: step 3440, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 62h:33m:45s remains)
INFO - root - 2017-12-07 11:30:43.606579: step 3450, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 56h:53m:25s remains)
INFO - root - 2017-12-07 11:30:50.435320: step 3460, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 64h:55m:53s remains)
INFO - root - 2017-12-07 11:30:57.323234: step 3470, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 66h:01m:35s remains)
INFO - root - 2017-12-07 11:31:04.061860: step 3480, loss = 2.08, batch loss = 2.02 (13.7 examples/sec; 0.586 sec/batch; 53h:32m:27s remains)
INFO - root - 2017-12-07 11:31:10.722381: step 3490, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 59h:27m:28s remains)
INFO - root - 2017-12-07 11:31:17.490447: step 3500, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 60h:05m:58s remains)
2017-12-07 11:31:18.262570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2340145 -4.2350664 -4.2349186 -4.2373881 -4.241642 -4.2417283 -4.236516 -4.2274837 -4.22915 -4.2429185 -4.249187 -4.2519178 -4.2543406 -4.2524304 -4.249402][-4.2180328 -4.2315583 -4.2349577 -4.2348676 -4.2336292 -4.2281647 -4.218596 -4.2057896 -4.2048864 -4.21926 -4.22675 -4.2348762 -4.2455525 -4.2486935 -4.2488017][-4.2052493 -4.2300992 -4.2411957 -4.2428741 -4.2358871 -4.2228909 -4.2061009 -4.1889663 -4.1872978 -4.2029386 -4.2141004 -4.2284713 -4.2456679 -4.251895 -4.2563496][-4.2128949 -4.2401538 -4.2537451 -4.2575421 -4.2479768 -4.2268162 -4.2004538 -4.1761971 -4.1748538 -4.1935368 -4.2087412 -4.2265935 -4.2433567 -4.2517157 -4.2604265][-4.2276087 -4.247786 -4.2587128 -4.2644362 -4.2529283 -4.2219043 -4.1796985 -4.146513 -4.150506 -4.1781883 -4.1980205 -4.2142391 -4.2267642 -4.2395205 -4.255465][-4.2427239 -4.2504354 -4.2577591 -4.26407 -4.2472825 -4.202981 -4.1411023 -4.1030889 -4.1254458 -4.1693211 -4.19121 -4.1994033 -4.2061019 -4.2224159 -4.245327][-4.2586541 -4.2540951 -4.2561426 -4.2603722 -4.2357893 -4.1707215 -4.0788183 -4.0338831 -4.0863738 -4.1557479 -4.1832309 -4.183744 -4.1855364 -4.2060933 -4.2371416][-4.275866 -4.2612395 -4.2573051 -4.2569575 -4.2243624 -4.1423464 -4.0258265 -3.9728794 -4.050868 -4.1425362 -4.1778994 -4.1766396 -4.1749196 -4.1976414 -4.2351379][-4.2930818 -4.2742805 -4.26761 -4.2634745 -4.2288013 -4.147665 -4.0389791 -3.9931259 -4.0722046 -4.1606879 -4.1944261 -4.1922545 -4.1861997 -4.2047524 -4.241221][-4.2993569 -4.2794757 -4.2740045 -4.2703943 -4.2412329 -4.1758685 -4.0986962 -4.0741553 -4.1395698 -4.2091465 -4.2359672 -4.2319393 -4.2190471 -4.229578 -4.2579536][-4.2840123 -4.2708263 -4.2690682 -4.2681241 -4.2492986 -4.205802 -4.1584339 -4.1495662 -4.1970134 -4.2450051 -4.2618837 -4.2578082 -4.2445145 -4.2481484 -4.2661009][-4.2525821 -4.2514086 -4.2561111 -4.2576985 -4.2474051 -4.225419 -4.2024837 -4.1967955 -4.2216344 -4.2475653 -4.2569346 -4.2545133 -4.2457886 -4.2443066 -4.2514067][-4.2266722 -4.2358003 -4.2437592 -4.2439132 -4.2399263 -4.2340117 -4.2250824 -4.2162151 -4.2218881 -4.2307186 -4.2331104 -4.2346315 -4.2337174 -4.2315831 -4.23173][-4.2183676 -4.2325563 -4.2386003 -4.2355008 -4.2333689 -4.2358313 -4.234592 -4.2262449 -4.22446 -4.2240763 -4.222713 -4.2259588 -4.2300649 -4.2295594 -4.2281528][-4.2289653 -4.242589 -4.246676 -4.2429643 -4.242373 -4.2491474 -4.2510681 -4.245254 -4.2419972 -4.2375536 -4.235117 -4.2370424 -4.2420354 -4.2429948 -4.241765]]...]
INFO - root - 2017-12-07 11:31:25.062859: step 3510, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 57h:44m:27s remains)
INFO - root - 2017-12-07 11:31:31.942393: step 3520, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 61h:18m:57s remains)
INFO - root - 2017-12-07 11:31:38.692812: step 3530, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 56h:35m:11s remains)
INFO - root - 2017-12-07 11:31:45.577698: step 3540, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 68h:13m:06s remains)
INFO - root - 2017-12-07 11:31:52.443375: step 3550, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.756 sec/batch; 69h:05m:00s remains)
INFO - root - 2017-12-07 11:31:59.244226: step 3560, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 60h:38m:07s remains)
INFO - root - 2017-12-07 11:32:05.946802: step 3570, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 58h:12m:52s remains)
INFO - root - 2017-12-07 11:32:12.683267: step 3580, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:42m:48s remains)
INFO - root - 2017-12-07 11:32:19.406462: step 3590, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 65h:59m:35s remains)
INFO - root - 2017-12-07 11:32:26.135264: step 3600, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 61h:19m:26s remains)
2017-12-07 11:32:26.820320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1491857 -4.2182465 -4.28 -4.3133931 -4.3224897 -4.3188896 -4.3128853 -4.30464 -4.2921681 -4.2770886 -4.2581158 -4.2365971 -4.2191877 -4.2059584 -4.1963158][-4.1100321 -4.1782866 -4.2503123 -4.2960329 -4.306951 -4.2956982 -4.2791252 -4.2656159 -4.2583389 -4.2525826 -4.2438374 -4.2346406 -4.2369366 -4.2357192 -4.2217784][-4.1046863 -4.1591506 -4.2265215 -4.2773466 -4.2907786 -4.2758679 -4.2503505 -4.2322259 -4.2298865 -4.2368588 -4.2407584 -4.2435369 -4.2603893 -4.2654471 -4.24964][-4.1312375 -4.1707463 -4.2256627 -4.2707057 -4.2851453 -4.2682738 -4.23495 -4.2093 -4.20791 -4.2274303 -4.2451782 -4.2594752 -4.2819424 -4.2865853 -4.2712259][-4.1811709 -4.2081361 -4.2472615 -4.279027 -4.2850852 -4.2588491 -4.2154059 -4.1767797 -4.1711473 -4.2069812 -4.2469459 -4.2737131 -4.292666 -4.2946711 -4.2782445][-4.2170634 -4.2351475 -4.2628675 -4.2835217 -4.2780766 -4.2376981 -4.180234 -4.1210446 -4.102036 -4.1562757 -4.2273521 -4.2699051 -4.286294 -4.289022 -4.2743578][-4.2380843 -4.25518 -4.2778482 -4.2905211 -4.2751441 -4.2251272 -4.1584387 -4.0809021 -4.0424943 -4.0988846 -4.1921792 -4.2476034 -4.2626491 -4.2653966 -4.2551527][-4.2586713 -4.2738771 -4.29309 -4.29896 -4.2750483 -4.2219539 -4.1557255 -4.0773845 -4.0267196 -4.0626917 -4.1509495 -4.2130709 -4.2298656 -4.2314596 -4.2228546][-4.2852631 -4.2940426 -4.3060293 -4.3061657 -4.2783394 -4.2308822 -4.1767731 -4.1155457 -4.0710363 -4.0830803 -4.1430178 -4.1966558 -4.2119269 -4.2089195 -4.1957226][-4.3016949 -4.302671 -4.3057809 -4.2989717 -4.2691474 -4.2295065 -4.1927757 -4.1566463 -4.131443 -4.1331735 -4.1668148 -4.2047682 -4.2187161 -4.2107711 -4.18648][-4.3100309 -4.3030772 -4.299057 -4.2894897 -4.2634745 -4.2327852 -4.2104378 -4.1950073 -4.1851306 -4.1820927 -4.1981497 -4.2227569 -4.2334194 -4.2226195 -4.193821][-4.3000226 -4.2873898 -4.2790923 -4.2710276 -4.2570448 -4.2429214 -4.2377162 -4.2367892 -4.2341852 -4.2265258 -4.2294626 -4.2412205 -4.244112 -4.2330937 -4.2058229][-4.2770615 -4.2587047 -4.2462273 -4.2383795 -4.2367034 -4.2403083 -4.2478638 -4.2553096 -4.2576742 -4.2490869 -4.2422419 -4.240921 -4.2359867 -4.2271543 -4.20929][-4.2709327 -4.2528467 -4.2389207 -4.2303281 -4.232275 -4.242722 -4.2522144 -4.260325 -4.264533 -4.2578363 -4.2452345 -4.2338905 -4.2226081 -4.214797 -4.21005][-4.2855315 -4.2741952 -4.264781 -4.2577357 -4.2581058 -4.2648249 -4.2681861 -4.2704415 -4.2723413 -4.2639847 -4.2472534 -4.2302132 -4.2176595 -4.2113109 -4.2148581]]...]
INFO - root - 2017-12-07 11:32:33.673653: step 3610, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 65h:55m:12s remains)
INFO - root - 2017-12-07 11:32:40.517673: step 3620, loss = 2.04, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 67h:38m:04s remains)
INFO - root - 2017-12-07 11:32:47.267106: step 3630, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 62h:54m:27s remains)
INFO - root - 2017-12-07 11:32:53.997795: step 3640, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 58h:51m:08s remains)
INFO - root - 2017-12-07 11:33:00.709364: step 3650, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 64h:19m:51s remains)
INFO - root - 2017-12-07 11:33:07.727368: step 3660, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.734 sec/batch; 67h:04m:57s remains)
INFO - root - 2017-12-07 11:33:14.460154: step 3670, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 62h:37m:15s remains)
INFO - root - 2017-12-07 11:33:21.247388: step 3680, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 58h:00m:36s remains)
INFO - root - 2017-12-07 11:33:27.953346: step 3690, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 59h:36m:13s remains)
INFO - root - 2017-12-07 11:33:34.692732: step 3700, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 66h:01m:11s remains)
2017-12-07 11:33:35.398701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2478652 -4.2606096 -4.2599268 -4.2398243 -4.2047997 -4.184814 -4.1841025 -4.178885 -4.1711125 -4.15888 -4.1517038 -4.1619 -4.1758833 -4.1901903 -4.2019243][-4.2200642 -4.2358222 -4.2370334 -4.2177234 -4.1868439 -4.175643 -4.1870618 -4.186687 -4.1668515 -4.1385016 -4.1213279 -4.1390371 -4.1654673 -4.1900225 -4.207449][-4.1941161 -4.2039495 -4.2062321 -4.191247 -4.1698537 -4.1667123 -4.1873732 -4.191391 -4.1663833 -4.1335998 -4.1166825 -4.1421895 -4.1783295 -4.2059278 -4.2186046][-4.1665988 -4.1683922 -4.1697278 -4.1621194 -4.1565466 -4.1682625 -4.1946959 -4.1997685 -4.1774278 -4.1534486 -4.1519184 -4.1838794 -4.2194476 -4.2352071 -4.2264485][-4.1593056 -4.1510167 -4.1468391 -4.1439848 -4.1447716 -4.1590252 -4.1776152 -4.1759415 -4.1629405 -4.157064 -4.1718783 -4.202776 -4.228941 -4.2317424 -4.2085357][-4.1596012 -4.144702 -4.129559 -4.1177373 -4.1087651 -4.1053834 -4.0971732 -4.0817647 -4.0857368 -4.1047645 -4.1348605 -4.1654134 -4.1847477 -4.183125 -4.1609936][-4.1160536 -4.0952544 -4.0677276 -4.0389786 -4.0103545 -3.9882307 -3.9556861 -3.9282372 -3.9526765 -4.0012231 -4.0512142 -4.0924191 -4.1126051 -4.1138353 -4.1024847][-4.0620046 -4.0253015 -3.9753356 -3.9231553 -3.8766496 -3.8471365 -3.8062992 -3.7793498 -3.8341138 -3.9168758 -3.9865353 -4.0372043 -4.0569062 -4.0614219 -4.062891][-4.0659056 -4.0295835 -3.984386 -3.9383354 -3.9026365 -3.8840976 -3.8625309 -3.8533921 -3.9041853 -3.9755657 -4.0313072 -4.0673051 -4.0770264 -4.0744233 -4.0773988][-4.1380153 -4.1135726 -4.0881295 -4.0685449 -4.0555592 -4.054143 -4.0542769 -4.0548086 -4.0767622 -4.1094046 -4.1328773 -4.145021 -4.1429062 -4.1320114 -4.1271][-4.2121291 -4.2003431 -4.1902647 -4.1833177 -4.1777463 -4.1786361 -4.1830788 -4.1831279 -4.1874294 -4.1972814 -4.2039394 -4.2036958 -4.1978765 -4.186399 -4.1779704][-4.2698231 -4.2635856 -4.2585082 -4.2543845 -4.2486048 -4.2467861 -4.2474365 -4.2449689 -4.2425423 -4.2436447 -4.2443829 -4.2429719 -4.2407861 -4.235877 -4.2319474][-4.2911744 -4.2840753 -4.2787232 -4.2743073 -4.2684827 -4.265842 -4.2657123 -4.2641373 -4.2620912 -4.2632494 -4.26564 -4.2677703 -4.2700672 -4.2712364 -4.2724366][-4.2974291 -4.2892485 -4.2850065 -4.282321 -4.2788234 -4.2779303 -4.2789912 -4.2791791 -4.278161 -4.2779093 -4.2789044 -4.28182 -4.2862763 -4.2901211 -4.2929664][-4.3151231 -4.3079209 -4.3036666 -4.3003263 -4.2958975 -4.2939782 -4.2946196 -4.2956638 -4.29403 -4.2900639 -4.2869883 -4.2867193 -4.289608 -4.2937937 -4.2971053]]...]
INFO - root - 2017-12-07 11:33:42.202912: step 3710, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 58h:15m:14s remains)
INFO - root - 2017-12-07 11:33:49.078392: step 3720, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 63h:26m:28s remains)
INFO - root - 2017-12-07 11:33:56.012191: step 3730, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 65h:31m:14s remains)
INFO - root - 2017-12-07 11:34:02.912903: step 3740, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 63h:51m:46s remains)
INFO - root - 2017-12-07 11:34:09.662729: step 3750, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 57h:28m:16s remains)
INFO - root - 2017-12-07 11:34:16.427488: step 3760, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 58h:19m:30s remains)
INFO - root - 2017-12-07 11:34:23.234080: step 3770, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 64h:42m:01s remains)
INFO - root - 2017-12-07 11:34:30.037393: step 3780, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 65h:25m:49s remains)
INFO - root - 2017-12-07 11:34:36.523021: step 3790, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 61h:08m:42s remains)
INFO - root - 2017-12-07 11:34:43.238165: step 3800, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 60h:59m:48s remains)
2017-12-07 11:34:44.024596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2149839 -4.1393304 -4.0812545 -4.04988 -4.0938969 -4.1805835 -4.2249813 -4.2274156 -4.2126961 -4.1964517 -4.1861997 -4.1891737 -4.1803794 -4.1770759 -4.1998854][-4.2039137 -4.13208 -4.0821185 -4.0696216 -4.12781 -4.2079558 -4.2400746 -4.2389851 -4.2211642 -4.2009563 -4.1986508 -4.2244945 -4.2353029 -4.2402382 -4.2496266][-4.2167106 -4.160552 -4.1271129 -4.1313591 -4.1866779 -4.2399592 -4.2513828 -4.2412 -4.2176785 -4.1943793 -4.2011704 -4.2381973 -4.2612123 -4.27671 -4.2803693][-4.2509627 -4.2140942 -4.1943316 -4.2038212 -4.2442293 -4.2725959 -4.2658095 -4.2397857 -4.2019525 -4.1788645 -4.1989326 -4.2433004 -4.2714963 -4.2904339 -4.2910681][-4.2910652 -4.2734289 -4.263165 -4.2682385 -4.285604 -4.2857146 -4.2536445 -4.1942215 -4.1393471 -4.1324096 -4.1817884 -4.2370133 -4.2681785 -4.2857318 -4.2832413][-4.3111033 -4.311738 -4.3121119 -4.3096738 -4.3002095 -4.2720556 -4.2013245 -4.095335 -4.030232 -4.062922 -4.1465487 -4.2113242 -4.2488432 -4.2733254 -4.2791295][-4.3196278 -4.329257 -4.3311768 -4.3132854 -4.2778053 -4.2171054 -4.0959888 -3.9354548 -3.8813972 -3.9836838 -4.1055479 -4.1811686 -4.22633 -4.2622471 -4.2814188][-4.3242588 -4.3377142 -4.3343496 -4.301537 -4.2401743 -4.1372457 -3.9594314 -3.7484341 -3.7396817 -3.9223216 -4.0752912 -4.1633482 -4.2166576 -4.2593427 -4.2852468][-4.3239422 -4.3383517 -4.3276248 -4.2806015 -4.1959624 -4.0677729 -3.8713946 -3.6750546 -3.734961 -3.9377186 -4.0859113 -4.1683893 -4.2230906 -4.2665658 -4.2915516][-4.3181238 -4.3263645 -4.3065114 -4.2497253 -4.1568289 -4.0385742 -3.8843064 -3.7792859 -3.8707931 -4.0263042 -4.1355772 -4.200984 -4.2490473 -4.2850337 -4.3006806][-4.3130622 -4.3159747 -4.2933683 -4.235117 -4.1530933 -4.063261 -3.9712405 -3.9422028 -4.0263443 -4.1278248 -4.2006168 -4.2503219 -4.2869048 -4.3078017 -4.3122296][-4.3125262 -4.3141556 -4.292767 -4.2444177 -4.1868939 -4.1316581 -4.0893621 -4.0947666 -4.1545491 -4.2167954 -4.2640538 -4.2987242 -4.3210511 -4.32707 -4.3230553][-4.3159819 -4.3177724 -4.300703 -4.2703772 -4.2410531 -4.2172327 -4.2077641 -4.2224865 -4.2565632 -4.2911372 -4.3178515 -4.3352613 -4.3423638 -4.3381567 -4.3295612][-4.3222814 -4.3243814 -4.3154693 -4.3045149 -4.2976727 -4.2942734 -4.2975392 -4.3062677 -4.3204966 -4.33559 -4.3466482 -4.3508778 -4.3478851 -4.338634 -4.3297186][-4.3244786 -4.3281198 -4.3283377 -4.329843 -4.333169 -4.3374443 -4.3413382 -4.3407526 -4.3423219 -4.3460903 -4.34711 -4.34389 -4.3375139 -4.3291936 -4.3235226]]...]
INFO - root - 2017-12-07 11:34:50.788923: step 3810, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.758 sec/batch; 69h:13m:57s remains)
INFO - root - 2017-12-07 11:34:57.591481: step 3820, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:53m:36s remains)
INFO - root - 2017-12-07 11:35:04.300925: step 3830, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 60h:10m:16s remains)
INFO - root - 2017-12-07 11:35:11.112359: step 3840, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 60h:18m:18s remains)
INFO - root - 2017-12-07 11:35:17.925657: step 3850, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.715 sec/batch; 65h:14m:42s remains)
INFO - root - 2017-12-07 11:35:24.717287: step 3860, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 63h:32m:13s remains)
INFO - root - 2017-12-07 11:35:31.556257: step 3870, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 62h:16m:30s remains)
INFO - root - 2017-12-07 11:35:38.352209: step 3880, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 59h:42m:40s remains)
INFO - root - 2017-12-07 11:35:45.037531: step 3890, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 57h:52m:28s remains)
INFO - root - 2017-12-07 11:35:51.767221: step 3900, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 65h:00m:53s remains)
2017-12-07 11:35:52.539804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2505083 -4.1846809 -4.1232224 -4.0836477 -4.086391 -4.1232448 -4.1669841 -4.1883159 -4.1923323 -4.1838684 -4.1763306 -4.1779256 -4.1803122 -4.1730647 -4.1647091][-4.2507825 -4.1798458 -4.1054835 -4.055851 -4.066762 -4.1200223 -4.1705384 -4.1910233 -4.1957994 -4.185205 -4.1769691 -4.1783943 -4.1796374 -4.1692805 -4.1582308][-4.2532244 -4.179338 -4.0945587 -4.0378637 -4.060792 -4.1306052 -4.1836863 -4.200582 -4.1999874 -4.1865277 -4.176621 -4.1745849 -4.1734967 -4.16446 -4.1588731][-4.2604904 -4.1853857 -4.094727 -4.0288868 -4.0529747 -4.1335564 -4.1914682 -4.2084327 -4.20362 -4.1875005 -4.1744413 -4.1678209 -4.1673183 -4.1686611 -4.1744394][-4.2752252 -4.2041488 -4.11372 -4.0413861 -4.0553141 -4.1344118 -4.1927452 -4.206223 -4.2018375 -4.1881132 -4.1726351 -4.163794 -4.1652365 -4.1748443 -4.1875086][-4.2850075 -4.2226157 -4.1410208 -4.0688152 -4.071125 -4.1392694 -4.1914439 -4.2015114 -4.1992145 -4.19342 -4.174819 -4.162158 -4.160419 -4.1710277 -4.1858082][-4.2871084 -4.2315841 -4.160603 -4.096972 -4.0969744 -4.1545453 -4.2034144 -4.2145061 -4.2114964 -4.2033005 -4.1773562 -4.1541834 -4.1468687 -4.1579366 -4.1771078][-4.2916455 -4.2423844 -4.1813951 -4.1302009 -4.12798 -4.1724606 -4.2191033 -4.2348361 -4.2300062 -4.2209725 -4.1995583 -4.1752625 -4.1589475 -4.1615343 -4.1730809][-4.2914767 -4.2453747 -4.192585 -4.1519637 -4.1499681 -4.1869292 -4.23328 -4.2507176 -4.2446632 -4.2377949 -4.2239275 -4.203033 -4.1838231 -4.1752877 -4.1776252][-4.2823658 -4.2368293 -4.19047 -4.1608853 -4.1637468 -4.1961679 -4.2444472 -4.2659135 -4.2606387 -4.2561622 -4.246747 -4.2278976 -4.2079196 -4.1953511 -4.1927161][-4.2742987 -4.2311683 -4.1938462 -4.174613 -4.1805811 -4.2075949 -4.2518253 -4.2732372 -4.2698512 -4.2667046 -4.2611942 -4.24544 -4.2260804 -4.2125111 -4.208879][-4.2675962 -4.2300782 -4.2016373 -4.1904335 -4.1968732 -4.2151313 -4.2506289 -4.26941 -4.2675958 -4.2655182 -4.2629256 -4.250258 -4.231564 -4.2193842 -4.2203035][-4.2606006 -4.2313566 -4.2118092 -4.2051463 -4.2109127 -4.2236557 -4.2509265 -4.2653604 -4.2626772 -4.2598224 -4.2582273 -4.2507362 -4.2375741 -4.2297192 -4.234581][-4.26399 -4.2428746 -4.2314167 -4.2282834 -4.23406 -4.24335 -4.2627106 -4.2712364 -4.2669678 -4.2618217 -4.2587714 -4.2537141 -4.247735 -4.2464929 -4.2532077][-4.2798262 -4.264708 -4.2583895 -4.2571316 -4.2616181 -4.2677732 -4.2794147 -4.2830005 -4.277493 -4.2702179 -4.2647018 -4.2605047 -4.2581596 -4.2617702 -4.2691336]]...]
INFO - root - 2017-12-07 11:35:59.220472: step 3910, loss = 2.08, batch loss = 2.03 (13.0 examples/sec; 0.615 sec/batch; 56h:06m:22s remains)
INFO - root - 2017-12-07 11:36:06.111304: step 3920, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 66h:31m:19s remains)
INFO - root - 2017-12-07 11:36:13.149725: step 3930, loss = 2.10, batch loss = 2.04 (10.8 examples/sec; 0.739 sec/batch; 67h:26m:32s remains)
INFO - root - 2017-12-07 11:36:19.889034: step 3940, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 61h:21m:31s remains)
INFO - root - 2017-12-07 11:36:26.679836: step 3950, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 59h:40m:20s remains)
INFO - root - 2017-12-07 11:36:33.375697: step 3960, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 66h:04m:20s remains)
INFO - root - 2017-12-07 11:36:40.230781: step 3970, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 60h:24m:33s remains)
INFO - root - 2017-12-07 11:36:47.063969: step 3980, loss = 2.11, batch loss = 2.05 (12.4 examples/sec; 0.645 sec/batch; 58h:52m:20s remains)
INFO - root - 2017-12-07 11:36:53.885675: step 3990, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 61h:31m:39s remains)
INFO - root - 2017-12-07 11:37:00.744121: step 4000, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.739 sec/batch; 67h:25m:37s remains)
2017-12-07 11:37:01.538476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3237991 -4.3251462 -4.3223181 -4.3109336 -4.2924318 -4.2748933 -4.2599049 -4.2537861 -4.2655153 -4.2823915 -4.2929611 -4.297802 -4.3055587 -4.3141351 -4.32073][-4.3003044 -4.3056045 -4.3001637 -4.277987 -4.2386146 -4.1998887 -4.1681647 -4.1569934 -4.1801133 -4.212429 -4.2360134 -4.2457552 -4.2593265 -4.2797942 -4.2960429][-4.2733431 -4.2807913 -4.2714758 -4.236742 -4.171536 -4.1037869 -4.0449305 -4.0275111 -4.06486 -4.1203861 -4.1643095 -4.1873145 -4.2076054 -4.2408996 -4.2641859][-4.2395091 -4.2512479 -4.2387757 -4.1872392 -4.0936255 -3.9844339 -3.8944321 -3.8784223 -3.9497886 -4.0515466 -4.1223335 -4.158442 -4.1866469 -4.2240114 -4.2467227][-4.1850882 -4.2049365 -4.1994839 -4.1375 -4.0099092 -3.8401966 -3.7053468 -3.7127085 -3.8470592 -4.0074577 -4.1159372 -4.1682196 -4.2013016 -4.2326488 -4.2465844][-4.1081214 -4.1412559 -4.1552 -4.0939722 -3.9392018 -3.702812 -3.5019369 -3.5560184 -3.7817817 -3.9957511 -4.1340151 -4.200314 -4.2319894 -4.2527766 -4.2521877][-4.0342765 -4.0772891 -4.1124253 -4.0680361 -3.919143 -3.6695976 -3.4372373 -3.5138736 -3.7848389 -4.0195584 -4.1628904 -4.231051 -4.2582989 -4.2656231 -4.2504549][-3.9855776 -4.0310059 -4.0881004 -4.0800352 -3.978127 -3.7986202 -3.6347017 -3.6748052 -3.8733919 -4.0625467 -4.1784577 -4.2368531 -4.257802 -4.2523503 -4.2266378][-3.9776859 -4.0209465 -4.0932407 -4.1261368 -4.0831614 -3.9850352 -3.900763 -3.9153361 -4.0145159 -4.1235948 -4.1955342 -4.2342262 -4.2410083 -4.2274685 -4.1947012][-4.0065951 -4.052505 -4.1289549 -4.1806107 -4.1728449 -4.120708 -4.080843 -4.0810246 -4.1117868 -4.1572351 -4.1958613 -4.2163863 -4.2147274 -4.1989961 -4.17016][-4.0398817 -4.0889015 -4.1592298 -4.20469 -4.2030864 -4.1726289 -4.1518574 -4.142024 -4.13263 -4.1436348 -4.1722183 -4.1979737 -4.2118497 -4.2050509 -4.1809077][-4.0649314 -4.109426 -4.1682148 -4.1973519 -4.1925783 -4.17589 -4.1637206 -4.1465564 -4.1233106 -4.1165919 -4.1450882 -4.1893945 -4.2247534 -4.2313833 -4.2176332][-4.0983958 -4.1314044 -4.1746631 -4.18836 -4.1803875 -4.1698084 -4.1634464 -4.150547 -4.1322861 -4.1265616 -4.1513677 -4.1977987 -4.2412572 -4.2610188 -4.2557936][-4.1466374 -4.1718774 -4.1985545 -4.1936679 -4.1752319 -4.1646643 -4.1647859 -4.1626506 -4.15515 -4.1580238 -4.1757097 -4.2155414 -4.2628441 -4.28908 -4.2836475][-4.2017884 -4.2201004 -4.23404 -4.2163134 -4.1862059 -4.17396 -4.1799922 -4.1838045 -4.1814218 -4.185245 -4.200882 -4.2324362 -4.2781591 -4.30483 -4.29364]]...]
INFO - root - 2017-12-07 11:37:08.266241: step 4010, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 59h:15m:14s remains)
INFO - root - 2017-12-07 11:37:14.972353: step 4020, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.661 sec/batch; 60h:21m:08s remains)
INFO - root - 2017-12-07 11:37:21.646736: step 4030, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 65h:22m:55s remains)
INFO - root - 2017-12-07 11:37:28.537146: step 4040, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.745 sec/batch; 67h:59m:21s remains)
INFO - root - 2017-12-07 11:37:35.263365: step 4050, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 60h:18m:34s remains)
INFO - root - 2017-12-07 11:37:42.154648: step 4060, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 58h:17m:46s remains)
INFO - root - 2017-12-07 11:37:48.890163: step 4070, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.615 sec/batch; 56h:05m:18s remains)
INFO - root - 2017-12-07 11:37:55.670206: step 4080, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 65h:04m:53s remains)
INFO - root - 2017-12-07 11:38:02.443474: step 4090, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 65h:20m:54s remains)
INFO - root - 2017-12-07 11:38:09.218754: step 4100, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 61h:32m:55s remains)
2017-12-07 11:38:09.894334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3341026 -4.3396764 -4.3356752 -4.3291321 -4.3296075 -4.3324685 -4.3299527 -4.3271265 -4.3311262 -4.334877 -4.3359475 -4.3338571 -4.3289213 -4.3247666 -4.3233037][-4.3291793 -4.3306117 -4.3234043 -4.3174877 -4.32264 -4.3282242 -4.3260155 -4.3243647 -4.3327956 -4.3370404 -4.3328285 -4.3219562 -4.3111429 -4.30566 -4.3062596][-4.3027134 -4.2960711 -4.2843609 -4.2841234 -4.29898 -4.3104749 -4.311286 -4.3136806 -4.3245444 -4.3252487 -4.3111563 -4.2898364 -4.2722168 -4.2681026 -4.2747059][-4.2551122 -4.244092 -4.23498 -4.2452121 -4.2686868 -4.2867289 -4.2899508 -4.2953863 -4.3025103 -4.2949672 -4.271337 -4.243361 -4.2276239 -4.2334976 -4.2482805][-4.208159 -4.1980243 -4.198051 -4.2181621 -4.2457848 -4.2631278 -4.2638235 -4.2646036 -4.2617326 -4.2444015 -4.21569 -4.192162 -4.1912332 -4.2125173 -4.2326484][-4.1819968 -4.1770105 -4.1893029 -4.2166348 -4.240622 -4.2480745 -4.2351766 -4.2178326 -4.2031837 -4.1854806 -4.1676822 -4.1673861 -4.1869211 -4.2134538 -4.2302289][-4.188942 -4.1843319 -4.197587 -4.21837 -4.2254319 -4.21296 -4.1772642 -4.1336503 -4.1133428 -4.1177578 -4.1386075 -4.1717758 -4.2053847 -4.2276382 -4.2323203][-4.211195 -4.2007394 -4.2047086 -4.205955 -4.1867304 -4.1456671 -4.0763011 -4.000824 -3.9988117 -4.0582085 -4.1283073 -4.1863241 -4.2223158 -4.2346654 -4.2268271][-4.2255034 -4.2094274 -4.2064409 -4.1938238 -4.156611 -4.094883 -4.0002337 -3.9150596 -3.9528315 -4.0643845 -4.1570745 -4.2121253 -4.2329774 -4.2302322 -4.2110109][-4.2300873 -4.2130814 -4.2078457 -4.1908112 -4.1575007 -4.1073823 -4.03968 -3.9978108 -4.04769 -4.1401014 -4.2069068 -4.2339654 -4.2278409 -4.2098117 -4.18622][-4.22762 -4.2167344 -4.2139792 -4.1988397 -4.1778154 -4.153029 -4.127799 -4.1262369 -4.1660013 -4.2127743 -4.2405214 -4.2356753 -4.2061315 -4.1828761 -4.171236][-4.2210197 -4.2205424 -4.2216291 -4.2098227 -4.199223 -4.1939182 -4.1913376 -4.2048345 -4.2301049 -4.24384 -4.2425909 -4.2142806 -4.1764879 -4.1635809 -4.1731887][-4.2255473 -4.2320533 -4.2345963 -4.224422 -4.2192593 -4.2207632 -4.228446 -4.2453933 -4.2576418 -4.2518854 -4.2344084 -4.196528 -4.1616273 -4.1654267 -4.1931214][-4.2166038 -4.2227225 -4.2249007 -4.2141271 -4.210443 -4.2111716 -4.224617 -4.2426209 -4.2501063 -4.2423997 -4.2203856 -4.185781 -4.1623626 -4.17489 -4.2103338][-4.167685 -4.1717796 -4.1746106 -4.1686044 -4.169096 -4.17423 -4.1922693 -4.2110176 -4.2149229 -4.2079558 -4.1914487 -4.1705909 -4.1592245 -4.1725578 -4.2031274]]...]
INFO - root - 2017-12-07 11:38:16.763349: step 4110, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 66h:32m:33s remains)
INFO - root - 2017-12-07 11:38:23.575986: step 4120, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 64h:48m:50s remains)
INFO - root - 2017-12-07 11:38:30.437381: step 4130, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 61h:16m:59s remains)
INFO - root - 2017-12-07 11:38:37.120881: step 4140, loss = 2.03, batch loss = 1.97 (12.8 examples/sec; 0.624 sec/batch; 56h:55m:46s remains)
INFO - root - 2017-12-07 11:38:44.028355: step 4150, loss = 2.03, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 64h:23m:45s remains)
INFO - root - 2017-12-07 11:38:50.874242: step 4160, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 66h:31m:05s remains)
INFO - root - 2017-12-07 11:38:57.663833: step 4170, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.663 sec/batch; 60h:29m:53s remains)
INFO - root - 2017-12-07 11:39:04.443207: step 4180, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 60h:07m:34s remains)
INFO - root - 2017-12-07 11:39:11.140723: step 4190, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 57h:22m:34s remains)
INFO - root - 2017-12-07 11:39:18.036098: step 4200, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 62h:58m:28s remains)
2017-12-07 11:39:18.733595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1927891 -4.2147889 -4.2271695 -4.2293096 -4.2184939 -4.1951056 -4.1583 -4.134418 -4.1388445 -4.1403065 -4.1221762 -4.1043687 -4.1137781 -4.1420155 -4.1760597][-4.2034354 -4.2325711 -4.2515697 -4.2499027 -4.2341676 -4.2054191 -4.1672158 -4.1477594 -4.1526127 -4.1463394 -4.1208267 -4.0966511 -4.0996356 -4.117753 -4.1447535][-4.2148042 -4.2445307 -4.2650342 -4.2586017 -4.2333 -4.1930466 -4.1478577 -4.1319032 -4.1453123 -4.1436119 -4.118197 -4.094902 -4.0952668 -4.103169 -4.1234083][-4.2347727 -4.2591505 -4.2739482 -4.2604313 -4.2199278 -4.1574268 -4.0939302 -4.0748725 -4.1002879 -4.1177535 -4.1093631 -4.095365 -4.09834 -4.1045184 -4.1266031][-4.2655468 -4.2809548 -4.2857685 -4.2599111 -4.2005296 -4.1142406 -4.0293283 -4.0060024 -4.0400796 -4.0821972 -4.1016412 -4.1067863 -4.1156645 -4.1224537 -4.1463308][-4.2924128 -4.2987852 -4.2920027 -4.2569051 -4.1875954 -4.0932522 -4.0018115 -3.9654016 -3.9921522 -4.044374 -4.0876613 -4.1156592 -4.1355767 -4.14665 -4.1733217][-4.3034139 -4.3059378 -4.2928972 -4.2549348 -4.1899118 -4.1066308 -4.022718 -3.9713242 -3.9723713 -4.0128393 -4.0693436 -4.119287 -4.1530385 -4.1733069 -4.20621][-4.3092322 -4.3130364 -4.3018484 -4.2677531 -4.2126737 -4.1483536 -4.0792046 -4.0173931 -3.992342 -4.0117636 -4.06613 -4.1299167 -4.1757951 -4.205914 -4.2407308][-4.3151841 -4.31817 -4.3125448 -4.2868423 -4.2447186 -4.1947408 -4.1362848 -4.0717487 -4.0323825 -4.0337896 -4.07528 -4.1389294 -4.1925211 -4.2315359 -4.26504][-4.3143363 -4.3168716 -4.3173394 -4.303894 -4.2793655 -4.2418342 -4.191442 -4.1290627 -4.0824108 -4.0696211 -4.0977483 -4.1539 -4.2103658 -4.2551827 -4.2873697][-4.31723 -4.3212614 -4.3261881 -4.3238006 -4.312376 -4.289258 -4.2513027 -4.1960917 -4.1487541 -4.1241145 -4.1363454 -4.1797028 -4.2321954 -4.2781973 -4.3067312][-4.3219147 -4.326447 -4.3333893 -4.3362947 -4.3318696 -4.3195329 -4.296195 -4.2548423 -4.2141261 -4.1855888 -4.1845374 -4.2119241 -4.2525749 -4.2923918 -4.3178267][-4.3219981 -4.3252792 -4.333303 -4.3393769 -4.3380823 -4.3328638 -4.3210182 -4.2956395 -4.2654467 -4.240272 -4.2334113 -4.2458792 -4.2700067 -4.2978325 -4.3189206][-4.3183789 -4.3195939 -4.3260427 -4.3334064 -4.335309 -4.3351207 -4.330071 -4.3156929 -4.2955656 -4.2772741 -4.2697077 -4.2745218 -4.2863989 -4.3024621 -4.3157291][-4.317802 -4.3166351 -4.3189316 -4.3228326 -4.3247461 -4.3264709 -4.325778 -4.3191085 -4.3064747 -4.2947111 -4.2903857 -4.2937489 -4.3006606 -4.308217 -4.3143215]]...]
INFO - root - 2017-12-07 11:39:25.622250: step 4210, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 57h:42m:09s remains)
INFO - root - 2017-12-07 11:39:32.507710: step 4220, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 60h:07m:41s remains)
INFO - root - 2017-12-07 11:39:39.391309: step 4230, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 66h:44m:27s remains)
INFO - root - 2017-12-07 11:39:46.143951: step 4240, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 58h:55m:32s remains)
INFO - root - 2017-12-07 11:39:53.107736: step 4250, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 65h:44m:02s remains)
INFO - root - 2017-12-07 11:39:59.932138: step 4260, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 57h:34m:02s remains)
INFO - root - 2017-12-07 11:40:06.515488: step 4270, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 66h:53m:00s remains)
INFO - root - 2017-12-07 11:40:13.364446: step 4280, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.730 sec/batch; 66h:33m:52s remains)
INFO - root - 2017-12-07 11:40:19.981641: step 4290, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 62h:11m:54s remains)
INFO - root - 2017-12-07 11:40:26.694538: step 4300, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.617 sec/batch; 56h:12m:25s remains)
2017-12-07 11:40:27.406931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1678824 -4.1707668 -4.1704435 -4.1659589 -4.155376 -4.1408291 -4.1289783 -4.1286116 -4.139833 -4.1549058 -4.1656146 -4.171236 -4.170527 -4.1645031 -4.1581378][-4.1534534 -4.1612768 -4.1621995 -4.154582 -4.1357408 -4.1099119 -4.0903292 -4.0924196 -4.1108603 -4.1334052 -4.1516285 -4.1651173 -4.1682582 -4.1613345 -4.1534829][-4.1373587 -4.1489997 -4.1497993 -4.1358418 -4.1048384 -4.0656919 -4.0414553 -4.0507603 -4.0786257 -4.1085744 -4.1337218 -4.1550813 -4.1617951 -4.1541247 -4.1435947][-4.1276131 -4.14047 -4.1379642 -4.1136751 -4.0675397 -4.0153437 -3.9887624 -4.0057025 -4.0447321 -4.0850229 -4.1181865 -4.1441865 -4.1520391 -4.1422105 -4.12623][-4.1281695 -4.1371078 -4.1284142 -4.0933714 -4.0338726 -3.9713712 -3.9439862 -3.9690764 -4.0217543 -4.0750771 -4.1157312 -4.1419272 -4.146625 -4.1297646 -4.1015568][-4.141551 -4.1409874 -4.122932 -4.0770683 -4.0066762 -3.936146 -3.9087124 -3.9453247 -4.0160375 -4.0816383 -4.1284804 -4.15365 -4.150516 -4.1196604 -4.0729122][-4.15822 -4.1475511 -4.1198187 -4.0642347 -3.9843843 -3.9047639 -3.8738451 -3.9234972 -4.0129967 -4.0890093 -4.1409726 -4.1660261 -4.1568546 -4.1126285 -4.0485339][-4.1646461 -4.1465783 -4.1142287 -4.05626 -3.9738333 -3.8874841 -3.8516517 -3.9111598 -4.0138226 -4.0958772 -4.1516767 -4.17814 -4.1678743 -4.1173048 -4.0427017][-4.1602883 -4.1377258 -4.1098113 -4.0645032 -3.9966016 -3.9196689 -3.8861623 -3.9435909 -4.0423594 -4.1187687 -4.1708865 -4.1968479 -4.1890416 -4.14103 -4.0674129][-4.1504397 -4.1256661 -4.1080284 -4.0841007 -4.0428419 -3.989872 -3.9681585 -4.017725 -4.098917 -4.1577964 -4.1978455 -4.2179513 -4.2110314 -4.1704426 -4.1055622][-4.1341982 -4.1126041 -4.1098657 -4.1107688 -4.0974941 -4.0669508 -4.0531344 -4.0898862 -4.147171 -4.1865273 -4.2143455 -4.2286806 -4.2233772 -4.1917343 -4.1344757][-4.1128311 -4.0999203 -4.1161084 -4.1404057 -4.14979 -4.1358051 -4.1263642 -4.1494112 -4.1858897 -4.2096753 -4.226234 -4.2342882 -4.2283959 -4.20086 -4.1456966][-4.0939159 -4.0911465 -4.1211762 -4.159143 -4.1820483 -4.1796651 -4.1749549 -4.18922 -4.2141666 -4.2317915 -4.2431231 -4.2458081 -4.2354345 -4.2045631 -4.1473775][-4.077908 -4.0859933 -4.1218648 -4.162961 -4.1886573 -4.19201 -4.1913223 -4.2017565 -4.2202473 -4.2371879 -4.2489572 -4.2500696 -4.2363181 -4.203095 -4.1503944][-4.0648222 -4.0832868 -4.1186271 -4.1526785 -4.1715517 -4.1748524 -4.1762686 -4.183383 -4.1977563 -4.2187824 -4.2342148 -4.235333 -4.2196841 -4.1885076 -4.1481614]]...]
INFO - root - 2017-12-07 11:40:34.262123: step 4310, loss = 2.08, batch loss = 2.03 (10.6 examples/sec; 0.757 sec/batch; 69h:00m:45s remains)
INFO - root - 2017-12-07 11:40:41.061220: step 4320, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 57h:04m:04s remains)
INFO - root - 2017-12-07 11:40:47.863364: step 4330, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 60h:21m:14s remains)
INFO - root - 2017-12-07 11:40:54.658256: step 4340, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 61h:02m:11s remains)
INFO - root - 2017-12-07 11:41:01.441406: step 4350, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 63h:56m:42s remains)
INFO - root - 2017-12-07 11:41:08.166561: step 4360, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 60h:31m:07s remains)
INFO - root - 2017-12-07 11:41:14.998925: step 4370, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:26m:10s remains)
INFO - root - 2017-12-07 11:41:21.821354: step 4380, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:35m:10s remains)
INFO - root - 2017-12-07 11:41:28.503819: step 4390, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.731 sec/batch; 66h:36m:12s remains)
INFO - root - 2017-12-07 11:41:35.219371: step 4400, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 66h:13m:10s remains)
2017-12-07 11:41:36.000160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1856666 -4.18069 -4.1851382 -4.1999946 -4.215929 -4.222147 -4.2230425 -4.2249737 -4.2206521 -4.2120781 -4.2062745 -4.1952624 -4.17979 -4.1582651 -4.1398258][-4.1582193 -4.15438 -4.1556811 -4.167469 -4.1826229 -4.1869082 -4.1859508 -4.1882262 -4.1892719 -4.1861448 -4.1832643 -4.1773348 -4.1699481 -4.15326 -4.1370435][-4.1421657 -4.1385908 -4.1374378 -4.1441312 -4.1554646 -4.153904 -4.1449966 -4.1435661 -4.147366 -4.1473951 -4.1442618 -4.1429796 -4.1456017 -4.1386881 -4.1317363][-4.1285715 -4.1190186 -4.1134906 -4.1175613 -4.1262097 -4.1168823 -4.0960884 -4.0881872 -4.0944586 -4.0967617 -4.09245 -4.09225 -4.1026492 -4.10997 -4.1217141][-4.1449738 -4.1319313 -4.1209869 -4.1165309 -4.11191 -4.0891361 -4.0572767 -4.0417991 -4.0523658 -4.0642557 -4.0693445 -4.0749106 -4.0916915 -4.1099887 -4.1367359][-4.1676903 -4.1495962 -4.1267104 -4.1082911 -4.092423 -4.0653629 -4.0292654 -4.005486 -4.0131469 -4.0332723 -4.0500393 -4.0648704 -4.0888405 -4.1134262 -4.1463447][-4.1682115 -4.1426816 -4.10747 -4.077424 -4.0592222 -4.0441794 -4.0177011 -3.9892805 -3.9886298 -4.0083523 -4.0325613 -4.0520482 -4.0776377 -4.1013103 -4.1333585][-4.149806 -4.1230488 -4.0858207 -4.0559578 -4.0434995 -4.0429726 -4.0258303 -3.9889674 -3.9719429 -3.9818718 -4.011024 -4.03699 -4.0649714 -4.0882092 -4.1129637][-4.1332288 -4.1093698 -4.0814152 -4.0585461 -4.0519271 -4.0603294 -4.0562196 -4.0262022 -4.0024981 -3.9985125 -4.0153737 -4.0357828 -4.0650711 -4.0888023 -4.1028943][-4.1445656 -4.1301804 -4.117384 -4.1056137 -4.1087828 -4.1241908 -4.1285367 -4.11121 -4.0894074 -4.0733657 -4.0670342 -4.0689111 -4.0907474 -4.1122689 -4.11756][-4.1743808 -4.1664329 -4.1588416 -4.1506 -4.1561246 -4.1738148 -4.1768713 -4.1625037 -4.1424766 -4.1228914 -4.1050014 -4.0972357 -4.1151719 -4.1387458 -4.1420593][-4.2023306 -4.1907549 -4.1784825 -4.1693645 -4.1757746 -4.1924362 -4.1947379 -4.1833992 -4.1690292 -4.1517806 -4.1297512 -4.1190386 -4.1359124 -4.1642246 -4.1736946][-4.2259245 -4.2086082 -4.1914153 -4.1827326 -4.1892428 -4.2037768 -4.2064171 -4.1989841 -4.1897945 -4.1762 -4.156044 -4.1463909 -4.1614633 -4.1917038 -4.2077231][-4.2490168 -4.2292523 -4.2102361 -4.2019753 -4.2050495 -4.213551 -4.2159572 -4.2132072 -4.21009 -4.2002139 -4.1843681 -4.1774173 -4.1879148 -4.2125664 -4.2318921][-4.2715 -4.2561245 -4.2419128 -4.2356262 -4.2355018 -4.2376823 -4.2382293 -4.2364092 -4.2344089 -4.2274418 -4.2191362 -4.2161012 -4.221652 -4.2375178 -4.2536497]]...]
INFO - root - 2017-12-07 11:41:42.832731: step 4410, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 61h:50m:21s remains)
INFO - root - 2017-12-07 11:41:49.729006: step 4420, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 65h:53m:54s remains)
INFO - root - 2017-12-07 11:41:56.581469: step 4430, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 63h:07m:30s remains)
INFO - root - 2017-12-07 11:42:03.403766: step 4440, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.637 sec/batch; 58h:05m:27s remains)
INFO - root - 2017-12-07 11:42:10.249169: step 4450, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 63h:45m:10s remains)
INFO - root - 2017-12-07 11:42:17.134454: step 4460, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 66h:42m:39s remains)
INFO - root - 2017-12-07 11:42:23.949829: step 4470, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 63h:53m:49s remains)
INFO - root - 2017-12-07 11:42:30.707362: step 4480, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:41m:27s remains)
INFO - root - 2017-12-07 11:42:37.366802: step 4490, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 57h:17m:22s remains)
INFO - root - 2017-12-07 11:42:44.212596: step 4500, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.713 sec/batch; 64h:56m:42s remains)
2017-12-07 11:42:45.011647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2060561 -4.2013793 -4.19936 -4.2005682 -4.2077765 -4.1945896 -4.1652379 -4.1436372 -4.1269155 -4.1265788 -4.1477318 -4.1873589 -4.2347846 -4.2779531 -4.3091063][-4.1888514 -4.1822891 -4.1802282 -4.1807804 -4.1866984 -4.1741862 -4.1464548 -4.1209836 -4.0963011 -4.0933237 -4.1179285 -4.16455 -4.2207141 -4.2719083 -4.3080578][-4.168189 -4.1538067 -4.1456847 -4.1462588 -4.1560144 -4.1527128 -4.1338725 -4.110045 -4.0817838 -4.0775471 -4.1040206 -4.1528339 -4.2157507 -4.2721152 -4.3101263][-4.1600614 -4.1359563 -4.1131587 -4.1050038 -4.1157575 -4.1239214 -4.1167116 -4.0982971 -4.0721474 -4.072969 -4.1004009 -4.1461558 -4.2111664 -4.2720151 -4.3127618][-4.1650505 -4.1371179 -4.1050887 -4.08312 -4.0812287 -4.0880609 -4.0886989 -4.0746694 -4.0527549 -4.0629954 -4.0956249 -4.1418657 -4.20661 -4.2703452 -4.3133225][-4.1723342 -4.151938 -4.1179452 -4.0754871 -4.0433655 -4.0312872 -4.0310812 -4.0205841 -4.0043373 -4.0297537 -4.0744443 -4.1281128 -4.1960177 -4.2631307 -4.3083797][-4.1682463 -4.156702 -4.1287766 -4.0804691 -4.0293179 -3.997334 -3.9877319 -3.9754028 -3.9634857 -3.9964519 -4.0468965 -4.1055126 -4.1765761 -4.2481041 -4.2976][-4.1737738 -4.1723161 -4.1562996 -4.114274 -4.0562396 -4.0078282 -3.9816558 -3.9572392 -3.937871 -3.9673898 -4.0203204 -4.0860171 -4.1618662 -4.2376132 -4.2886][-4.1685119 -4.1847515 -4.185133 -4.1592326 -4.110333 -4.0606518 -4.0249166 -3.9900658 -3.953666 -3.9624362 -4.0075765 -4.0748653 -4.1553574 -4.2329783 -4.2832704][-4.167707 -4.1931028 -4.204174 -4.1931233 -4.1615715 -4.1218443 -4.0889258 -4.0484762 -4.0015159 -3.9893966 -4.0203166 -4.0821886 -4.1607647 -4.2351637 -4.282465][-4.1774044 -4.1990385 -4.2099605 -4.2068253 -4.1925626 -4.1660509 -4.1418862 -4.1042576 -4.0605621 -4.043786 -4.0683517 -4.120532 -4.1858311 -4.24805 -4.288238][-4.1914773 -4.1990814 -4.2070937 -4.2125449 -4.2098379 -4.1967621 -4.1835966 -4.1537871 -4.117661 -4.1052504 -4.1285038 -4.17161 -4.2204757 -4.2681623 -4.2994733][-4.2066069 -4.2008066 -4.2003555 -4.210094 -4.2161055 -4.2130189 -4.207099 -4.1864834 -4.1577287 -4.1510034 -4.1743636 -4.2112865 -4.2497778 -4.2861242 -4.3096237][-4.2275157 -4.2194552 -4.2161741 -4.2222238 -4.2270908 -4.2268095 -4.2239881 -4.2119317 -4.1914034 -4.1894712 -4.2106075 -4.2415771 -4.2721415 -4.2994275 -4.3162279][-4.251441 -4.2508383 -4.2540107 -4.2589374 -4.2600551 -4.2586265 -4.2567344 -4.2511334 -4.2407117 -4.2404056 -4.2542219 -4.2738404 -4.293611 -4.3114886 -4.3217106]]...]
INFO - root - 2017-12-07 11:42:51.768736: step 4510, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 60h:22m:55s remains)
INFO - root - 2017-12-07 11:42:58.521923: step 4520, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 60h:25m:03s remains)
INFO - root - 2017-12-07 11:43:05.466939: step 4530, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 62h:49m:03s remains)
INFO - root - 2017-12-07 11:43:12.350476: step 4540, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 67h:08m:59s remains)
INFO - root - 2017-12-07 11:43:19.220060: step 4550, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 57h:17m:40s remains)
INFO - root - 2017-12-07 11:43:26.005159: step 4560, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 61h:28m:24s remains)
INFO - root - 2017-12-07 11:43:32.943360: step 4570, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 65h:12m:56s remains)
INFO - root - 2017-12-07 11:43:39.582163: step 4580, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 59h:17m:01s remains)
INFO - root - 2017-12-07 11:43:46.291540: step 4590, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.648 sec/batch; 58h:59m:26s remains)
INFO - root - 2017-12-07 11:43:53.149415: step 4600, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.737 sec/batch; 67h:08m:29s remains)
2017-12-07 11:43:53.891854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3254347 -4.3171625 -4.3075628 -4.3014631 -4.2999711 -4.2997651 -4.301816 -4.3081717 -4.3177004 -4.3241034 -4.3247914 -4.3221211 -4.3193049 -4.3155141 -4.3067927][-4.3186207 -4.3054385 -4.2916069 -4.283926 -4.2834325 -4.2827 -4.2820454 -4.287466 -4.2999144 -4.3094969 -4.311799 -4.3101597 -4.3067064 -4.3002982 -4.2865386][-4.3078322 -4.2903504 -4.2714214 -4.2614226 -4.2613554 -4.260181 -4.2552981 -4.2588415 -4.275445 -4.2901011 -4.2956948 -4.2977753 -4.2958741 -4.2872777 -4.2690473][-4.2978582 -4.2779975 -4.2523913 -4.2366343 -4.2329469 -4.2283964 -4.2164564 -4.2181082 -4.2416062 -4.2664914 -4.2791438 -4.286263 -4.2896638 -4.282712 -4.2620587][-4.2917423 -4.2728033 -4.2430377 -4.2206149 -4.2099051 -4.1973996 -4.1704226 -4.1622648 -4.1930261 -4.2332611 -4.2575603 -4.2722058 -4.2861009 -4.2866488 -4.2687769][-4.2911172 -4.274466 -4.2431278 -4.2151675 -4.193327 -4.163434 -4.1083927 -4.0810022 -4.1226788 -4.185936 -4.2270794 -4.2524047 -4.2787285 -4.2902727 -4.280582][-4.295547 -4.2807951 -4.2489123 -4.2161713 -4.1831007 -4.1304197 -4.0373788 -3.9803061 -4.0307641 -4.1206074 -4.1820793 -4.2197595 -4.2578411 -4.281652 -4.2844133][-4.3029728 -4.2916975 -4.2629361 -4.2297587 -4.1918607 -4.1293821 -4.0135121 -3.924767 -3.9626713 -4.0595827 -4.1318579 -4.1788406 -4.2250361 -4.2571483 -4.272294][-4.3081622 -4.30129 -4.2800779 -4.2518167 -4.2168961 -4.1653132 -4.0628462 -3.9695148 -3.9766841 -4.0441022 -4.1021066 -4.1450491 -4.1902328 -4.2231064 -4.2454691][-4.3088107 -4.3061037 -4.2944627 -4.2748203 -4.2478762 -4.2120028 -4.1371 -4.0630803 -4.0522909 -4.0832763 -4.1117139 -4.1367865 -4.1692343 -4.1944504 -4.2160072][-4.3042269 -4.302525 -4.2982521 -4.2892551 -4.2747664 -4.2521138 -4.2019038 -4.1501431 -4.1349421 -4.144979 -4.1530743 -4.1590295 -4.1730576 -4.1873274 -4.2013249][-4.2979422 -4.2944555 -4.29363 -4.2920837 -4.28744 -4.2741723 -4.2419658 -4.2118716 -4.2018785 -4.2042 -4.2040634 -4.1984372 -4.1973362 -4.2021828 -4.2089338][-4.2962055 -4.2912035 -4.2921019 -4.2944169 -4.2933469 -4.2827125 -4.2609744 -4.2464089 -4.2451072 -4.2480431 -4.2489572 -4.2415147 -4.2336764 -4.2348161 -4.2376342][-4.3003354 -4.2950335 -4.2961192 -4.29862 -4.2967577 -4.2844334 -4.266191 -4.2603922 -4.2657609 -4.2721329 -4.2751713 -4.2684655 -4.2588897 -4.2594991 -4.2627277][-4.3078933 -4.3022285 -4.3029513 -4.3051252 -4.3027449 -4.2904186 -4.2740016 -4.2706461 -4.2775216 -4.2848191 -4.2874603 -4.2801933 -4.2705922 -4.2702689 -4.2757244]]...]
INFO - root - 2017-12-07 11:44:00.580092: step 4610, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 57h:16m:25s remains)
INFO - root - 2017-12-07 11:44:07.263696: step 4620, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 57h:35m:34s remains)
INFO - root - 2017-12-07 11:44:14.092464: step 4630, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 66h:35m:55s remains)
INFO - root - 2017-12-07 11:44:21.045131: step 4640, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 68h:12m:51s remains)
INFO - root - 2017-12-07 11:44:27.738110: step 4650, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 59h:34m:16s remains)
INFO - root - 2017-12-07 11:44:34.506848: step 4660, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 57h:19m:18s remains)
INFO - root - 2017-12-07 11:44:41.310222: step 4670, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 59h:53m:24s remains)
INFO - root - 2017-12-07 11:44:48.111565: step 4680, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 66h:19m:08s remains)
INFO - root - 2017-12-07 11:44:54.619109: step 4690, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 61h:13m:39s remains)
INFO - root - 2017-12-07 11:45:01.438800: step 4700, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 60h:12m:20s remains)
2017-12-07 11:45:02.091562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2601285 -4.2581105 -4.2707143 -4.2838674 -4.2795229 -4.2683725 -4.2599945 -4.2605066 -4.2662754 -4.26958 -4.2531023 -4.218812 -4.203896 -4.22159 -4.2390985][-4.2463236 -4.2539492 -4.27341 -4.2918458 -4.2859216 -4.2648425 -4.2527862 -4.2529798 -4.2600412 -4.2672486 -4.2590003 -4.2348795 -4.2266722 -4.2463689 -4.269588][-4.219985 -4.2382679 -4.266818 -4.2928367 -4.2868166 -4.255434 -4.2375426 -4.2396541 -4.2505689 -4.2588162 -4.2519565 -4.2368689 -4.2333946 -4.252512 -4.2773819][-4.1916628 -4.214499 -4.2530026 -4.2853651 -4.2775736 -4.2329526 -4.2049336 -4.2151537 -4.2377687 -4.24727 -4.2358532 -4.2263155 -4.2297926 -4.2467942 -4.2731609][-4.1854277 -4.20466 -4.2441745 -4.2700849 -4.2439389 -4.1719217 -4.1283107 -4.1569095 -4.2050338 -4.2217083 -4.2055736 -4.2004128 -4.213428 -4.2378917 -4.2702012][-4.1899991 -4.2072825 -4.2402415 -4.2469449 -4.1921644 -4.0833817 -4.0260744 -4.0817733 -4.1597004 -4.1847143 -4.1656761 -4.1595526 -4.1803327 -4.2181039 -4.2608538][-4.1997156 -4.2175922 -4.2424097 -4.23082 -4.151516 -4.0196486 -3.9552989 -4.027988 -4.1258578 -4.157135 -4.1334267 -4.120018 -4.1458306 -4.1952715 -4.2487741][-4.2222195 -4.2435102 -4.2630844 -4.2436261 -4.1624651 -4.0409479 -3.9807858 -4.0445223 -4.1373286 -4.1685057 -4.1388521 -4.1202488 -4.1470866 -4.1998825 -4.255085][-4.2605782 -4.2786212 -4.2910528 -4.2703495 -4.2054977 -4.1102414 -4.05761 -4.0995235 -4.1746249 -4.2021947 -4.1711273 -4.1499691 -4.1738596 -4.2254286 -4.27707][-4.2937522 -4.308217 -4.313827 -4.2937942 -4.2463956 -4.1764288 -4.1295223 -4.1543736 -4.2131782 -4.2354231 -4.2061458 -4.1793966 -4.198709 -4.2514014 -4.2970619][-4.308897 -4.319849 -4.3206635 -4.3043528 -4.269969 -4.2227778 -4.1881814 -4.2032132 -4.2467575 -4.2649937 -4.2409325 -4.2095246 -4.2201304 -4.2694845 -4.3064694][-4.3084269 -4.3175788 -4.3199234 -4.3109059 -4.2861705 -4.2563133 -4.2326369 -4.2429595 -4.2727513 -4.2887478 -4.2718363 -4.2424765 -4.2466063 -4.2856665 -4.3128223][-4.3049107 -4.3157535 -4.3227983 -4.3196797 -4.3024864 -4.28487 -4.2697053 -4.2776031 -4.2958927 -4.3061228 -4.295063 -4.2763615 -4.2794995 -4.3048639 -4.3206177][-4.3088832 -4.3212862 -4.3299122 -4.3313446 -4.3211861 -4.3086076 -4.2993832 -4.3060241 -4.3160605 -4.3182611 -4.3085408 -4.2993093 -4.3027649 -4.3171682 -4.3238215][-4.3128829 -4.3241358 -4.3340135 -4.3400092 -4.3353863 -4.3256588 -4.3181925 -4.3230453 -4.3286624 -4.3280239 -4.3180451 -4.31285 -4.3141756 -4.3191943 -4.3191381]]...]
INFO - root - 2017-12-07 11:45:08.882845: step 4710, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 62h:57m:45s remains)
INFO - root - 2017-12-07 11:45:15.634928: step 4720, loss = 2.03, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 64h:34m:18s remains)
INFO - root - 2017-12-07 11:45:22.456396: step 4730, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 63h:43m:14s remains)
INFO - root - 2017-12-07 11:45:29.243647: step 4740, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:55m:21s remains)
INFO - root - 2017-12-07 11:45:36.082303: step 4750, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 58h:54m:08s remains)
INFO - root - 2017-12-07 11:45:43.001994: step 4760, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 66h:41m:24s remains)
INFO - root - 2017-12-07 11:45:49.883174: step 4770, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 66h:28m:35s remains)
INFO - root - 2017-12-07 11:45:56.725603: step 4780, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 60h:36m:41s remains)
INFO - root - 2017-12-07 11:46:03.275121: step 4790, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 56h:29m:58s remains)
INFO - root - 2017-12-07 11:46:10.067910: step 4800, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 58h:09m:12s remains)
2017-12-07 11:46:10.823093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2191 -4.2234192 -4.2268748 -4.2275891 -4.2259579 -4.22697 -4.2344337 -4.2445884 -4.2595649 -4.2774096 -4.2955818 -4.3037643 -4.2994142 -4.2897849 -4.2872963][-4.16138 -4.1713853 -4.1815257 -4.1858563 -4.1842694 -4.1830206 -4.188796 -4.1972547 -4.2137547 -4.2381711 -4.2656631 -4.2850957 -4.2911634 -4.2883358 -4.2881112][-4.1156116 -4.1367388 -4.1539774 -4.1606894 -4.1575103 -4.1524143 -4.15309 -4.1563439 -4.1688952 -4.193882 -4.2268395 -4.2534728 -4.2690597 -4.2749867 -4.28084][-4.0868425 -4.1165557 -4.1375484 -4.1459045 -4.1413431 -4.1335621 -4.127728 -4.1232023 -4.1302862 -4.1529837 -4.1854138 -4.2151814 -4.2361193 -4.2504606 -4.2623739][-4.093267 -4.1173 -4.1325545 -4.1348329 -4.1233172 -4.1086197 -4.093256 -4.0777149 -4.079895 -4.10347 -4.1391449 -4.1723132 -4.1970634 -4.2158895 -4.2308569][-4.1308122 -4.1419187 -4.1447358 -4.1321092 -4.105619 -4.0756516 -4.0436831 -4.0084782 -3.9991593 -4.0307093 -4.0828309 -4.1282077 -4.1575618 -4.1768937 -4.191781][-4.1789427 -4.1841578 -4.177556 -4.1505961 -4.1075182 -4.0597429 -4.0070758 -3.9435384 -3.9097378 -3.9452844 -4.0201621 -4.0835371 -4.1199126 -4.1385417 -4.1521745][-4.2130756 -4.2234225 -4.2166023 -4.1834683 -4.130949 -4.0708094 -4.00627 -3.9261942 -3.871942 -3.8983288 -3.9774737 -4.0468845 -4.0855303 -4.1040292 -4.1185732][-4.2327237 -4.2500916 -4.248651 -4.2198668 -4.1709604 -4.1093793 -4.0476255 -3.9722862 -3.9182265 -3.9285262 -3.9842427 -4.036335 -4.0638766 -4.0792432 -4.0959225][-4.2435451 -4.2647395 -4.2719355 -4.2561216 -4.2218885 -4.1688056 -4.1150036 -4.0534763 -4.0104923 -4.0094352 -4.035799 -4.0634985 -4.0779543 -4.0852246 -4.0970087][-4.24552 -4.2642951 -4.2806911 -4.2813654 -4.2635403 -4.2251706 -4.1826715 -4.1389112 -4.1096578 -4.106554 -4.1153088 -4.1261992 -4.1315036 -4.1317959 -4.133019][-4.2424049 -4.2543573 -4.276412 -4.2886114 -4.2826262 -4.2587948 -4.2310772 -4.2056637 -4.1906247 -4.1907663 -4.1924663 -4.1936126 -4.1920962 -4.1888423 -4.1844163][-4.2347717 -4.2381654 -4.2586946 -4.2759075 -4.277524 -4.2677841 -4.254787 -4.24637 -4.2442183 -4.2474675 -4.2466455 -4.2423611 -4.2364268 -4.2312346 -4.2277327][-4.2236552 -4.2179418 -4.2311215 -4.2461772 -4.2541575 -4.255157 -4.2539887 -4.2574883 -4.2625275 -4.2691584 -4.2709107 -4.2665715 -4.260025 -4.2565866 -4.2583413][-4.2132745 -4.2001758 -4.2071958 -4.2179003 -4.2271476 -4.2322774 -4.2361507 -4.2442017 -4.2511635 -4.2591934 -4.2647009 -4.2648931 -4.2624412 -4.2626967 -4.2696652]]...]
INFO - root - 2017-12-07 11:46:17.715045: step 4810, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 60h:30m:19s remains)
INFO - root - 2017-12-07 11:46:24.462865: step 4820, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.614 sec/batch; 55h:55m:04s remains)
INFO - root - 2017-12-07 11:46:31.318492: step 4830, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:43m:10s remains)
INFO - root - 2017-12-07 11:46:38.190819: step 4840, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.763 sec/batch; 69h:29m:19s remains)
INFO - root - 2017-12-07 11:46:45.043906: step 4850, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 63h:28m:45s remains)
INFO - root - 2017-12-07 11:46:51.853147: step 4860, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 60h:02m:40s remains)
INFO - root - 2017-12-07 11:46:58.653026: step 4870, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 58h:53m:09s remains)
INFO - root - 2017-12-07 11:47:05.532042: step 4880, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 66h:37m:57s remains)
INFO - root - 2017-12-07 11:47:12.115881: step 4890, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 62h:31m:34s remains)
INFO - root - 2017-12-07 11:47:18.858377: step 4900, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 57h:02m:31s remains)
2017-12-07 11:47:19.549104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2098732 -4.1901388 -4.1695466 -4.1542759 -4.1470728 -4.1504254 -4.1630735 -4.1793585 -4.1967354 -4.214932 -4.2184873 -4.2092896 -4.1979542 -4.1954188 -4.2006717][-4.2285056 -4.2149677 -4.1950903 -4.1790218 -4.1779766 -4.1878886 -4.2015324 -4.2198548 -4.2420478 -4.2624211 -4.2661948 -4.2570505 -4.242435 -4.2316356 -4.2254567][-4.2418504 -4.2358656 -4.2179904 -4.19996 -4.19904 -4.2077069 -4.2160697 -4.2305202 -4.2559428 -4.2815003 -4.2904215 -4.2856097 -4.2714233 -4.2561097 -4.2406225][-4.2386575 -4.2326293 -4.213254 -4.1930571 -4.1885061 -4.1931252 -4.1949263 -4.2037339 -4.2295294 -4.2582283 -4.2712417 -4.2717419 -4.2625904 -4.2497797 -4.2319932][-4.2153025 -4.2042813 -4.18196 -4.1602955 -4.1527047 -4.1525087 -4.1462383 -4.1469288 -4.16538 -4.191124 -4.2065191 -4.2149692 -4.2174149 -4.2177324 -4.2121029][-4.1871634 -4.1707621 -4.1453857 -4.122551 -4.1146326 -4.1118412 -4.1011629 -4.0975947 -4.1060925 -4.1203408 -4.1313429 -4.1470394 -4.1641879 -4.1849108 -4.2002344][-4.1616888 -4.1401644 -4.1149178 -4.0939145 -4.0873728 -4.0824094 -4.070446 -4.0688925 -4.0721536 -4.06939 -4.0703373 -4.0922604 -4.1227984 -4.158267 -4.1888843][-4.1611133 -4.1339889 -4.1077023 -4.0867362 -4.0788813 -4.0710282 -4.0551906 -4.0548005 -4.05593 -4.0402746 -4.0315838 -4.058166 -4.096416 -4.1372452 -4.1758761][-4.1845064 -4.1552892 -4.128459 -4.1079259 -4.0968447 -4.0904937 -4.0793152 -4.0796819 -4.07904 -4.0585513 -4.0435681 -4.064034 -4.0954142 -4.1318383 -4.1723342][-4.2099576 -4.1827688 -4.1575117 -4.137311 -4.1274543 -4.1307144 -4.1366434 -4.1462636 -4.1486087 -4.1300468 -4.1092997 -4.1152005 -4.1324358 -4.1575747 -4.1887269][-4.2181211 -4.1880813 -4.1598115 -4.1409073 -4.1353059 -4.149951 -4.1742387 -4.2014208 -4.2189541 -4.2092285 -4.1885939 -4.1829414 -4.1871371 -4.1980643 -4.2150064][-4.2148318 -4.1758819 -4.13837 -4.1105418 -4.1035633 -4.1253648 -4.1647067 -4.2082362 -4.242661 -4.2479439 -4.2382507 -4.2325211 -4.2314787 -4.2308607 -4.23547][-4.222723 -4.1783652 -4.1272578 -4.0790639 -4.0557404 -4.0739141 -4.1178784 -4.1703348 -4.2144194 -4.2330637 -4.2378683 -4.2418852 -4.2438278 -4.2393579 -4.2348614][-4.238739 -4.1969037 -4.141397 -4.072607 -4.0236878 -4.0238757 -4.05362 -4.0984373 -4.1438456 -4.1693292 -4.1885109 -4.2099643 -4.2229328 -4.2228417 -4.2185011][-4.2670536 -4.2303209 -4.1784925 -4.1067061 -4.04441 -4.0206828 -4.0199537 -4.0427203 -4.0765328 -4.0976076 -4.1228385 -4.160336 -4.1880856 -4.1992803 -4.2022266]]...]
INFO - root - 2017-12-07 11:47:26.421719: step 4910, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 67h:37m:30s remains)
INFO - root - 2017-12-07 11:47:33.281563: step 4920, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 63h:16m:17s remains)
INFO - root - 2017-12-07 11:47:40.065468: step 4930, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 61h:06m:43s remains)
INFO - root - 2017-12-07 11:47:47.026558: step 4940, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 62h:04m:46s remains)
INFO - root - 2017-12-07 11:47:53.996420: step 4950, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.776 sec/batch; 70h:35m:03s remains)
INFO - root - 2017-12-07 11:48:00.796865: step 4960, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 61h:22m:09s remains)
INFO - root - 2017-12-07 11:48:07.620549: step 4970, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.618 sec/batch; 56h:11m:47s remains)
INFO - root - 2017-12-07 11:48:14.460062: step 4980, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 58h:21m:25s remains)
INFO - root - 2017-12-07 11:48:21.126281: step 4990, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 65h:18m:55s remains)
INFO - root - 2017-12-07 11:48:27.771647: step 5000, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 60h:35m:54s remains)
2017-12-07 11:48:28.464909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1947412 -4.2088089 -4.2208409 -4.2149811 -4.1715531 -4.1221323 -4.0983362 -4.1007485 -4.114974 -4.1412315 -4.1756854 -4.2023678 -4.2241459 -4.2435088 -4.2519569][-4.1807604 -4.2058139 -4.2222323 -4.216392 -4.1746821 -4.1231337 -4.09793 -4.10239 -4.1148109 -4.1326146 -4.1622853 -4.1898565 -4.2151384 -4.2343278 -4.2455182][-4.150198 -4.1914992 -4.218225 -4.2149444 -4.1770124 -4.1286116 -4.1017413 -4.0989947 -4.1056089 -4.1212225 -4.1461425 -4.176743 -4.2037797 -4.22137 -4.2344356][-4.1083803 -4.1709304 -4.2141256 -4.2218575 -4.1927285 -4.1516795 -4.120718 -4.0963707 -4.0895252 -4.1054306 -4.1275411 -4.1579108 -4.1848903 -4.2009335 -4.2176762][-4.0964885 -4.1741967 -4.2269917 -4.2415771 -4.2152314 -4.1787047 -4.13093 -4.0641084 -4.0406232 -4.0737958 -4.107954 -4.1422253 -4.1708188 -4.1912327 -4.215672][-4.109807 -4.1849084 -4.2304683 -4.2409043 -4.2088752 -4.1661139 -4.0822735 -3.9601166 -3.929749 -4.0050011 -4.0746026 -4.1222186 -4.1521783 -4.1780405 -4.2110939][-4.1347456 -4.1960897 -4.2295 -4.2315674 -4.1896935 -4.1327553 -4.008255 -3.8287194 -3.799758 -3.9352503 -4.0485244 -4.0981832 -4.11393 -4.1371641 -4.1785026][-4.1638527 -4.2094426 -4.236752 -4.2326026 -4.1866455 -4.1313367 -4.0016685 -3.8096333 -3.7968092 -3.95936 -4.0780392 -4.1083355 -4.0928516 -4.0973492 -4.1390796][-4.201304 -4.2307129 -4.2523036 -4.2415419 -4.1938777 -4.1514711 -4.0594449 -3.9220328 -3.932225 -4.06029 -4.1413364 -4.1440344 -4.1062174 -4.0996046 -4.1372967][-4.2491341 -4.2663236 -4.2782726 -4.2650719 -4.2213254 -4.1848235 -4.1257749 -4.0444608 -4.0628047 -4.1457582 -4.1876125 -4.1671753 -4.1171184 -4.1100559 -4.1496234][-4.2782617 -4.2859378 -4.290699 -4.2836981 -4.2543383 -4.2250404 -4.1878457 -4.1424809 -4.1544542 -4.2003222 -4.2169414 -4.1867776 -4.1380091 -4.1309295 -4.167933][-4.2921362 -4.2948341 -4.2985392 -4.2977061 -4.2804065 -4.2598267 -4.2375913 -4.2109909 -4.2150993 -4.23508 -4.2411842 -4.2193761 -4.1807847 -4.1663504 -4.1871719][-4.311676 -4.3106794 -4.3112097 -4.3122988 -4.3021464 -4.2848678 -4.268621 -4.2524447 -4.2518759 -4.2582626 -4.2602291 -4.2502956 -4.224103 -4.2016191 -4.2015758][-4.3296385 -4.329556 -4.3292007 -4.3268209 -4.3167181 -4.2994256 -4.2850742 -4.27288 -4.2688046 -4.2735252 -4.2760139 -4.2752566 -4.2620273 -4.2396736 -4.2281809][-4.3405385 -4.3408942 -4.3389444 -4.3358393 -4.3263831 -4.3107567 -4.29875 -4.2903852 -4.2845783 -4.2875862 -4.2908034 -4.2948122 -4.2908578 -4.2747116 -4.2596803]]...]
INFO - root - 2017-12-07 11:48:35.273263: step 5010, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 60h:03m:42s remains)
INFO - root - 2017-12-07 11:48:42.246131: step 5020, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 64h:33m:57s remains)
INFO - root - 2017-12-07 11:48:49.051972: step 5030, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 64h:21m:26s remains)
INFO - root - 2017-12-07 11:48:55.894683: step 5040, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 60h:12m:59s remains)
INFO - root - 2017-12-07 11:49:02.617727: step 5050, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 57h:47m:56s remains)
INFO - root - 2017-12-07 11:49:09.393627: step 5060, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 59h:26m:53s remains)
INFO - root - 2017-12-07 11:49:16.313160: step 5070, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 66h:56m:44s remains)
INFO - root - 2017-12-07 11:49:23.058502: step 5080, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 61h:26m:00s remains)
INFO - root - 2017-12-07 11:49:29.752625: step 5090, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:48m:39s remains)
INFO - root - 2017-12-07 11:49:36.522671: step 5100, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 61h:22m:22s remains)
2017-12-07 11:49:37.239684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1892252 -4.2043467 -4.2224708 -4.2245955 -4.2148404 -4.2180624 -4.2397175 -4.2498341 -4.2475286 -4.2483387 -4.2551746 -4.2672615 -4.2633452 -4.2374635 -4.2105141][-4.158432 -4.1685214 -4.1901226 -4.1989179 -4.196784 -4.2033434 -4.2227249 -4.2311044 -4.226234 -4.2253656 -4.2339597 -4.246747 -4.243772 -4.2170897 -4.1890869][-4.1864653 -4.1886339 -4.2014318 -4.2069421 -4.205822 -4.2073584 -4.2151713 -4.21676 -4.2133775 -4.2170758 -4.2308121 -4.2478213 -4.2475762 -4.2226067 -4.1980948][-4.2413893 -4.2410917 -4.2414942 -4.2374992 -4.2319603 -4.2254996 -4.2195773 -4.2060294 -4.1995511 -4.2076993 -4.2260585 -4.2433281 -4.2462497 -4.2322645 -4.220221][-4.2617083 -4.2639709 -4.2569852 -4.2467666 -4.2347975 -4.2154331 -4.1921482 -4.1597309 -4.14911 -4.1666765 -4.1926847 -4.215343 -4.2235975 -4.2227144 -4.2236276][-4.2488461 -4.2504897 -4.2398748 -4.2241139 -4.2010632 -4.1647134 -4.1171045 -4.0601664 -4.0480509 -4.0867081 -4.1308022 -4.1629457 -4.1810784 -4.1953845 -4.2105546][-4.2176924 -4.2138929 -4.1978507 -4.1742716 -4.1398211 -4.084621 -4.0059609 -3.9175887 -3.9077036 -3.9852152 -4.0638208 -4.1135521 -4.142405 -4.1674671 -4.1954889][-4.1979885 -4.1879663 -4.1704311 -4.1472421 -4.1123605 -4.0558972 -3.9741681 -3.8840516 -3.8821092 -3.9755428 -4.0669856 -4.122252 -4.152761 -4.1722693 -4.1953373][-4.2196169 -4.2084651 -4.1935039 -4.1739459 -4.145596 -4.1054068 -4.0479136 -3.9877512 -3.9916029 -4.0636706 -4.13418 -4.174933 -4.1914783 -4.1897693 -4.1950192][-4.2429466 -4.2322621 -4.2204709 -4.2072325 -4.1900573 -4.169065 -4.1330485 -4.0975213 -4.1003933 -4.1456771 -4.1938381 -4.220655 -4.2218227 -4.2033262 -4.194458][-4.2479806 -4.2420373 -4.2373576 -4.2305021 -4.2175212 -4.201695 -4.1809611 -4.1678677 -4.1730356 -4.1989746 -4.2280312 -4.2433467 -4.2380085 -4.2143021 -4.196867][-4.2478628 -4.2505665 -4.2519746 -4.2489758 -4.2396612 -4.2287989 -4.2190595 -4.2173147 -4.2232552 -4.23627 -4.2489367 -4.2533226 -4.2439928 -4.2191157 -4.1956563][-4.264605 -4.2741032 -4.2786918 -4.2797747 -4.2756648 -4.2685828 -4.2625856 -4.2611241 -4.2615485 -4.2635274 -4.265604 -4.2638025 -4.254087 -4.2323093 -4.2066216][-4.2903857 -4.3036647 -4.3105192 -4.3121796 -4.3089256 -4.3028216 -4.2971845 -4.2940888 -4.2936025 -4.2929735 -4.291326 -4.2875147 -4.2794762 -4.2626123 -4.2391553][-4.3036251 -4.3136883 -4.31818 -4.3190064 -4.3188095 -4.3172946 -4.3162761 -4.3162036 -4.3162861 -4.3152103 -4.3133578 -4.3105021 -4.3059273 -4.2974916 -4.2829695]]...]
INFO - root - 2017-12-07 11:49:44.035487: step 5110, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 65h:41m:14s remains)
INFO - root - 2017-12-07 11:49:50.843898: step 5120, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:41m:51s remains)
INFO - root - 2017-12-07 11:49:57.700674: step 5130, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 60h:19m:16s remains)
INFO - root - 2017-12-07 11:50:04.532647: step 5140, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 66h:01m:39s remains)
INFO - root - 2017-12-07 11:50:11.392655: step 5150, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.722 sec/batch; 65h:40m:59s remains)
INFO - root - 2017-12-07 11:50:18.210865: step 5160, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.632 sec/batch; 57h:26m:31s remains)
INFO - root - 2017-12-07 11:50:24.988894: step 5170, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 59h:44m:11s remains)
INFO - root - 2017-12-07 11:50:31.709019: step 5180, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 62h:55m:16s remains)
INFO - root - 2017-12-07 11:50:38.313966: step 5190, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 63h:42m:42s remains)
INFO - root - 2017-12-07 11:50:44.910582: step 5200, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 60h:41m:52s remains)
2017-12-07 11:50:45.596581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2871633 -4.279603 -4.2730346 -4.2690105 -4.2685885 -4.2708364 -4.2727509 -4.2730236 -4.2722716 -4.2723613 -4.2733173 -4.2747049 -4.2747941 -4.2724562 -4.2688][-4.2916951 -4.2871995 -4.2791419 -4.2697172 -4.2630944 -4.2630768 -4.2669678 -4.2719426 -4.2741241 -4.2743297 -4.2729912 -4.27213 -4.2710433 -4.2689486 -4.266161][-4.2825036 -4.2864923 -4.2793989 -4.2620077 -4.2452083 -4.240231 -4.2474575 -4.2622123 -4.274694 -4.2801881 -4.2790556 -4.2755356 -4.2722821 -4.269309 -4.2664957][-4.254756 -4.271194 -4.2647948 -4.2380857 -4.206912 -4.193336 -4.2041488 -4.2333136 -4.2631016 -4.2828679 -4.2891212 -4.2852788 -4.2793036 -4.2739859 -4.2695127][-4.2139859 -4.2462163 -4.2402887 -4.2000484 -4.149539 -4.1232362 -4.1372242 -4.1819043 -4.2332182 -4.2738905 -4.2936859 -4.2951169 -4.2883449 -4.2803884 -4.273222][-4.1690888 -4.2225018 -4.2206459 -4.1653652 -4.0875812 -4.0375304 -4.048306 -4.1087542 -4.1843715 -4.2493558 -4.2877731 -4.2991457 -4.2939324 -4.2841926 -4.2744832][-4.1262865 -4.2024927 -4.2118664 -4.151792 -4.0456042 -3.9575481 -3.9488442 -4.0193439 -4.1182566 -4.2088633 -4.2688627 -4.2945595 -4.2949228 -4.2841721 -4.2725143][-4.1118317 -4.1977544 -4.2169037 -4.1639433 -4.0484886 -3.9233928 -3.8720567 -3.9330924 -4.0453854 -4.1561904 -4.2366753 -4.2798853 -4.2903013 -4.2806582 -4.2670832][-4.1492085 -4.2136931 -4.2320204 -4.1902895 -4.0938177 -3.9784727 -3.9030635 -3.9208469 -4.012125 -4.1207113 -4.2077618 -4.262784 -4.284894 -4.2795582 -4.2639179][-4.2165732 -4.2475157 -4.2492905 -4.2143035 -4.1497617 -4.0780582 -4.022965 -4.0154581 -4.0594168 -4.1333537 -4.2021914 -4.2564096 -4.285769 -4.2865076 -4.2702756][-4.2668786 -4.2735796 -4.258101 -4.2248626 -4.1858459 -4.15373 -4.1265049 -4.1179767 -4.1350045 -4.172636 -4.2161088 -4.2611957 -4.2931671 -4.3003788 -4.2856746][-4.2840781 -4.2757072 -4.2475138 -4.2146978 -4.1923223 -4.1829448 -4.17317 -4.1682782 -4.1741815 -4.1897964 -4.21834 -4.260016 -4.2954135 -4.3104582 -4.3020935][-4.2790327 -4.2575316 -4.2204213 -4.1893435 -4.1773992 -4.1765022 -4.1715636 -4.1612592 -4.1521921 -4.1543841 -4.1826372 -4.2352571 -4.2829318 -4.3084207 -4.3096533][-4.2631927 -4.2306337 -4.1904025 -4.1644549 -4.1613984 -4.1645203 -4.1545415 -4.1264429 -4.0905824 -4.0758853 -4.1069727 -4.1763344 -4.243701 -4.2869029 -4.3028855][-4.2401652 -4.2046142 -4.1716294 -4.1585217 -4.1668987 -4.1750131 -4.1575522 -4.1022568 -4.0259356 -3.9775209 -4.002008 -4.0876408 -4.1770711 -4.2435632 -4.2813931]]...]
INFO - root - 2017-12-07 11:50:52.375385: step 5210, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 64h:26m:33s remains)
INFO - root - 2017-12-07 11:50:59.139809: step 5220, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 61h:48m:19s remains)
INFO - root - 2017-12-07 11:51:05.823044: step 5230, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:49m:24s remains)
INFO - root - 2017-12-07 11:51:12.593057: step 5240, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.623 sec/batch; 56h:38m:35s remains)
INFO - root - 2017-12-07 11:51:19.372173: step 5250, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 57h:57m:31s remains)
INFO - root - 2017-12-07 11:51:26.239947: step 5260, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 65h:12m:37s remains)
INFO - root - 2017-12-07 11:51:33.010129: step 5270, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 64h:52m:09s remains)
INFO - root - 2017-12-07 11:51:39.764385: step 5280, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 61h:40m:46s remains)
INFO - root - 2017-12-07 11:51:46.375086: step 5290, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:42m:52s remains)
INFO - root - 2017-12-07 11:51:53.186827: step 5300, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:57m:23s remains)
2017-12-07 11:51:53.925294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2922363 -4.2952175 -4.2991662 -4.2964416 -4.2799816 -4.2627077 -4.2483387 -4.2376361 -4.2364678 -4.2342463 -4.2326775 -4.2435684 -4.2587767 -4.2766013 -4.2914281][-4.264658 -4.2687683 -4.2725153 -4.2687044 -4.2484436 -4.2258654 -4.2088695 -4.199934 -4.2026515 -4.1996679 -4.19375 -4.2034764 -4.2212033 -4.2429485 -4.2589779][-4.2194366 -4.2261343 -4.2314343 -4.2270365 -4.2022486 -4.1745329 -4.1551924 -4.15162 -4.1636 -4.1634741 -4.1547427 -4.1624813 -4.1796536 -4.20256 -4.2177753][-4.1734614 -4.18222 -4.1929836 -4.1904449 -4.1597152 -4.1243267 -4.1002326 -4.10128 -4.1233025 -4.1307817 -4.1246309 -4.131444 -4.1481495 -4.1715479 -4.1842942][-4.1338549 -4.1429338 -4.1586723 -4.159369 -4.1249952 -4.0810676 -4.052669 -4.0594459 -4.0899887 -4.1081128 -4.1093025 -4.1170597 -4.1324215 -4.1549897 -4.1653848][-4.108068 -4.1170559 -4.1354151 -4.136796 -4.0954914 -4.0392623 -4.0053487 -4.0181293 -4.0586634 -4.089869 -4.1017537 -4.1123962 -4.1269174 -4.1457953 -4.1535521][-4.1088934 -4.1224284 -4.1413078 -4.1370583 -4.0820975 -4.00628 -3.9617083 -3.9757762 -4.0273256 -4.0747375 -4.1001711 -4.1152143 -4.13077 -4.1461973 -4.1528516][-4.1208005 -4.1393228 -4.1579051 -4.1493626 -4.0898218 -4.0060515 -3.9582183 -3.973897 -4.0260715 -4.0758 -4.1060834 -4.1202264 -4.132803 -4.1462064 -4.1578851][-4.1237087 -4.1456103 -4.1635356 -4.1557188 -4.104784 -4.0326056 -3.9957809 -4.0137668 -4.0568047 -4.0926709 -4.1126571 -4.1198297 -4.1271124 -4.1401753 -4.1583362][-4.1212845 -4.1431184 -4.1615968 -4.1592274 -4.1230283 -4.0694323 -4.043541 -4.0563345 -4.0852275 -4.1059027 -4.1149888 -4.1167755 -4.1225061 -4.1370459 -4.1594777][-4.1204023 -4.1370373 -4.1535788 -4.1568756 -4.1352477 -4.100069 -4.0845075 -4.0928454 -4.110723 -4.1204157 -4.1231494 -4.1247091 -4.1332126 -4.1498909 -4.1722503][-4.1165161 -4.1271152 -4.1411562 -4.1495194 -4.1393113 -4.1182952 -4.1125045 -4.1233873 -4.1376925 -4.1418796 -4.1411448 -4.1431055 -4.1538148 -4.1725173 -4.1920719][-4.1193433 -4.1247458 -4.136302 -4.1472731 -4.1435461 -4.1305346 -4.1314278 -4.145854 -4.1601491 -4.1621814 -4.1583238 -4.1598582 -4.17191 -4.1910148 -4.2059889][-4.1261878 -4.1282821 -4.1363029 -4.14765 -4.1489882 -4.1422796 -4.1469865 -4.1629086 -4.1777048 -4.1797266 -4.1744719 -4.1746635 -4.1855578 -4.2035227 -4.2171311][-4.130867 -4.1321592 -4.1389694 -4.1510234 -4.1561618 -4.1535244 -4.1581903 -4.1717758 -4.1854072 -4.1901913 -4.1878877 -4.1893449 -4.200851 -4.2172523 -4.2285848]]...]
INFO - root - 2017-12-07 11:52:00.699118: step 5310, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.669 sec/batch; 60h:48m:10s remains)
INFO - root - 2017-12-07 11:52:07.487680: step 5320, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 56h:36m:27s remains)
INFO - root - 2017-12-07 11:52:14.212745: step 5330, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 57h:06m:12s remains)
INFO - root - 2017-12-07 11:52:21.018831: step 5340, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.735 sec/batch; 66h:45m:20s remains)
INFO - root - 2017-12-07 11:52:27.866694: step 5350, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 63h:15m:01s remains)
INFO - root - 2017-12-07 11:52:34.644481: step 5360, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 61h:04m:41s remains)
INFO - root - 2017-12-07 11:52:41.491703: step 5370, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 57h:52m:43s remains)
INFO - root - 2017-12-07 11:52:48.367345: step 5380, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:35m:42s remains)
INFO - root - 2017-12-07 11:52:55.042318: step 5390, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.751 sec/batch; 68h:15m:02s remains)
INFO - root - 2017-12-07 11:53:01.857768: step 5400, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 65h:20m:53s remains)
2017-12-07 11:53:02.497340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3207946 -4.3104038 -4.3049965 -4.3083882 -4.318706 -4.3228531 -4.3160009 -4.2973018 -4.2759647 -4.2565908 -4.2404976 -4.2305856 -4.2306652 -4.2421064 -4.2608271][-4.2988625 -4.2874608 -4.2876744 -4.3004651 -4.3180351 -4.3244987 -4.3143473 -4.289835 -4.2651858 -4.2447767 -4.2243266 -4.2055345 -4.197588 -4.2083526 -4.230917][-4.2635069 -4.2562909 -4.2656741 -4.2876225 -4.3086934 -4.3139439 -4.3005 -4.2717824 -4.2423229 -4.2185826 -4.1932621 -4.1675396 -4.1558251 -4.1677418 -4.1934834][-4.2209563 -4.2182913 -4.2386127 -4.2701225 -4.2952514 -4.2989569 -4.28111 -4.2470527 -4.21251 -4.1824994 -4.1530504 -4.1279788 -4.11898 -4.1306524 -4.1533113][-4.1705241 -4.1723657 -4.2034974 -4.2424974 -4.2676225 -4.2683449 -4.2475777 -4.20627 -4.1636887 -4.1307292 -4.10389 -4.0892763 -4.0920405 -4.1037922 -4.11901][-4.1327629 -4.1332912 -4.1667452 -4.2066288 -4.2248678 -4.2209392 -4.1977229 -4.1528 -4.1122732 -4.0922394 -4.0822144 -4.0811124 -4.0924854 -4.1030726 -4.1099691][-4.1269093 -4.1221466 -4.1467009 -4.1773987 -4.1849155 -4.1734257 -4.1441736 -4.0976796 -4.0659966 -4.0711288 -4.0820026 -4.0863357 -4.0974269 -4.1060963 -4.1116576][-4.14666 -4.1402225 -4.1542993 -4.1687608 -4.1608624 -4.1374207 -4.0981417 -4.0513787 -4.03368 -4.0554786 -4.0731049 -4.075624 -4.0856328 -4.0975208 -4.1101136][-4.163105 -4.1585355 -4.1642294 -4.163352 -4.1429744 -4.1095953 -4.0626092 -4.0195312 -4.0101476 -4.0260429 -4.0358343 -4.0348487 -4.0490689 -4.0736947 -4.104044][-4.1646733 -4.1644764 -4.1654334 -4.1547527 -4.1288466 -4.0900311 -4.0416026 -4.00447 -3.9935296 -3.9931283 -3.9913392 -3.9874763 -4.0072803 -4.046567 -4.0963955][-4.1575365 -4.1570415 -4.1515102 -4.1313705 -4.10141 -4.0625358 -4.0187864 -3.992228 -3.9788208 -3.9660897 -3.9557669 -3.9538722 -3.9816983 -4.033536 -4.0987415][-4.1407609 -4.1379 -4.1233163 -4.0968218 -4.0666518 -4.0316305 -3.9972358 -3.9829311 -3.9740074 -3.9596834 -3.9463074 -3.9474063 -3.9796104 -4.0394659 -4.11301][-4.0995989 -4.0948133 -4.0793195 -4.0551872 -4.03209 -4.0055852 -3.9857154 -3.9852111 -3.984807 -3.9792993 -3.9704857 -3.9739923 -4.005909 -4.0669928 -4.1394219][-4.0549116 -4.0512328 -4.0435147 -4.0272975 -4.0141435 -4.0000935 -3.9950662 -4.0020237 -4.0052261 -4.0079703 -4.0103564 -4.0214214 -4.0541868 -4.1104989 -4.1719966][-4.047431 -4.0447145 -4.0451369 -4.0391393 -4.0362759 -4.0355582 -4.0428228 -4.0524549 -4.0557585 -4.0612741 -4.0700636 -4.084352 -4.1145182 -4.1602764 -4.2051558]]...]
INFO - root - 2017-12-07 11:53:09.379670: step 5410, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:09m:09s remains)
INFO - root - 2017-12-07 11:53:16.241699: step 5420, loss = 2.05, batch loss = 2.00 (10.5 examples/sec; 0.760 sec/batch; 69h:05m:00s remains)
INFO - root - 2017-12-07 11:53:23.098041: step 5430, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 67h:27m:35s remains)
INFO - root - 2017-12-07 11:53:29.852685: step 5440, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 60h:03m:05s remains)
INFO - root - 2017-12-07 11:53:36.684411: step 5450, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:14m:43s remains)
INFO - root - 2017-12-07 11:53:43.362983: step 5460, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 57h:26m:28s remains)
INFO - root - 2017-12-07 11:53:50.141002: step 5470, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 65h:38m:24s remains)
INFO - root - 2017-12-07 11:53:57.009211: step 5480, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.729 sec/batch; 66h:14m:28s remains)
INFO - root - 2017-12-07 11:54:03.650917: step 5490, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 60h:58m:30s remains)
INFO - root - 2017-12-07 11:54:10.430252: step 5500, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 55h:55m:48s remains)
2017-12-07 11:54:11.205634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2530313 -4.2393818 -4.224164 -4.1970162 -4.1591578 -4.1240687 -4.09786 -4.075314 -4.069243 -4.0767407 -4.0814862 -4.0700393 -4.0472493 -4.041255 -4.0678606][-4.2208943 -4.1964884 -4.1721735 -4.1414762 -4.09997 -4.0646248 -4.0429382 -4.0182118 -4.0017552 -4.0009918 -4.0126424 -4.024066 -4.03932 -4.0749021 -4.12748][-4.1717272 -4.1382031 -4.1096096 -4.0898371 -4.0615206 -4.0363832 -4.0214214 -3.99266 -3.9602489 -3.9463103 -3.9563026 -3.9846969 -4.0377779 -4.1100354 -4.1821179][-4.1373596 -4.0977092 -4.0693846 -4.061851 -4.0468836 -4.0281734 -4.016923 -4.0005131 -3.9807436 -3.9717333 -3.9748719 -3.9992504 -4.0645761 -4.146584 -4.2154536][-4.1168823 -4.0790586 -4.053472 -4.0475917 -4.0326905 -4.0129671 -4.0036988 -4.0035582 -4.0141597 -4.0291357 -4.0434084 -4.0684423 -4.1258974 -4.1909823 -4.2392077][-4.1120596 -4.08113 -4.0596981 -4.0444055 -4.0098548 -3.9671381 -3.9374449 -3.9414821 -3.9819374 -4.0388455 -4.0857248 -4.1293073 -4.1851506 -4.2348766 -4.2664719][-4.1169233 -4.0968509 -4.0766406 -4.0401678 -3.9658053 -3.8755267 -3.8041897 -3.8107164 -3.9001911 -4.0118041 -4.1010151 -4.1728582 -4.2301111 -4.2710471 -4.2911091][-4.1224895 -4.1130648 -4.089303 -4.0373654 -3.947377 -3.8413506 -3.7628062 -3.7795472 -3.89464 -4.0186315 -4.1188164 -4.2009559 -4.2586169 -4.2929163 -4.30568][-4.1072407 -4.1075988 -4.0956936 -4.0601454 -4.0021887 -3.9436884 -3.9111671 -3.9386759 -4.014092 -4.0937886 -4.16494 -4.2292747 -4.27696 -4.3027687 -4.3100791][-4.103755 -4.1112242 -4.1122642 -4.0949221 -4.0696621 -4.0536809 -4.0582929 -4.0902848 -4.1367679 -4.1831083 -4.2235689 -4.2632613 -4.2922778 -4.3065882 -4.3115292][-4.1196618 -4.1255584 -4.1289754 -4.1221662 -4.1156235 -4.1190357 -4.1378007 -4.1686072 -4.2031417 -4.239675 -4.26958 -4.2924018 -4.3044515 -4.3103132 -4.3145037][-4.1439471 -4.1480665 -4.1480718 -4.1382504 -4.1346478 -4.1438117 -4.1708417 -4.2052727 -4.2381167 -4.2748928 -4.2994633 -4.3110585 -4.3128719 -4.3149109 -4.3169355][-4.1559911 -4.1626081 -4.1655684 -4.1574326 -4.1554565 -4.1704011 -4.2032752 -4.2349157 -4.2612348 -4.2913566 -4.3104486 -4.3165255 -4.3156376 -4.3163052 -4.3166609][-4.1613803 -4.1737986 -4.1838245 -4.1829491 -4.1880674 -4.2051392 -4.234601 -4.2612009 -4.283226 -4.3039646 -4.315959 -4.3185477 -4.3169866 -4.317029 -4.31648][-4.191318 -4.2071609 -4.2191691 -4.2271 -4.2354488 -4.245749 -4.2657604 -4.2848215 -4.3036227 -4.3161798 -4.3208833 -4.3216963 -4.3207393 -4.319602 -4.3180137]]...]
INFO - root - 2017-12-07 11:54:17.856077: step 5510, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 58h:14m:17s remains)
INFO - root - 2017-12-07 11:54:24.653428: step 5520, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 58h:14m:47s remains)
INFO - root - 2017-12-07 11:54:31.449575: step 5530, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 64h:00m:39s remains)
INFO - root - 2017-12-07 11:54:38.326966: step 5540, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 65h:38m:15s remains)
INFO - root - 2017-12-07 11:54:45.146296: step 5550, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 60h:01m:20s remains)
INFO - root - 2017-12-07 11:54:51.916722: step 5560, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:39m:40s remains)
INFO - root - 2017-12-07 11:54:58.710577: step 5570, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 60h:29m:02s remains)
INFO - root - 2017-12-07 11:55:05.528891: step 5580, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 64h:43m:22s remains)
INFO - root - 2017-12-07 11:55:12.166385: step 5590, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 63h:37m:58s remains)
INFO - root - 2017-12-07 11:55:19.018954: step 5600, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 63h:48m:17s remains)
2017-12-07 11:55:19.706086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2143226 -4.1964664 -4.1749392 -4.1514168 -4.1338177 -4.1105223 -4.0845141 -4.0697408 -4.0654492 -4.0726447 -4.1045823 -4.1540818 -4.184494 -4.1886406 -4.1862864][-4.2145548 -4.19509 -4.1727195 -4.1484432 -4.1329942 -4.1195631 -4.0992484 -4.0748258 -4.0529213 -4.0560122 -4.0973177 -4.157393 -4.1923046 -4.1955037 -4.1852789][-4.2220893 -4.2034488 -4.1807618 -4.157268 -4.1447506 -4.1375637 -4.1204505 -4.0889549 -4.0550461 -4.0579267 -4.1013975 -4.1565328 -4.1862783 -4.1850176 -4.1685152][-4.2357211 -4.2171993 -4.195509 -4.1764092 -4.1687379 -4.165 -4.1475782 -4.1111431 -4.0723548 -4.0710936 -4.1059866 -4.1488805 -4.1714592 -4.1663537 -4.1469703][-4.2412219 -4.2210965 -4.20238 -4.1895156 -4.18715 -4.1837082 -4.165451 -4.1287394 -4.0876307 -4.0763683 -4.0975175 -4.1281009 -4.1453557 -4.1432834 -4.1290751][-4.2415509 -4.2213712 -4.2039642 -4.1926355 -4.1899548 -4.1835041 -4.1639028 -4.1290903 -4.087594 -4.0674343 -4.0792351 -4.107841 -4.125669 -4.1323738 -4.1253433][-4.242033 -4.2220278 -4.2025461 -4.1860657 -4.1780009 -4.165525 -4.1457911 -4.1160955 -4.0788326 -4.0598297 -4.0754128 -4.1095133 -4.1340642 -4.1451821 -4.140872][-4.237967 -4.2179675 -4.1939673 -4.1720738 -4.1596255 -4.1435037 -4.1256886 -4.1032228 -4.0771308 -4.0713019 -4.0988326 -4.1388607 -4.1617479 -4.1722145 -4.168817][-4.2306557 -4.2106423 -4.1856837 -4.163981 -4.1518073 -4.1376071 -4.1260052 -4.1138058 -4.1038966 -4.1123533 -4.1468525 -4.1817732 -4.1936831 -4.1961174 -4.1902637][-4.2241397 -4.2027926 -4.1818304 -4.1656141 -4.1592321 -4.1553063 -4.1545906 -4.1545315 -4.1556864 -4.1655259 -4.1902204 -4.2105823 -4.2079282 -4.2002263 -4.18956][-4.2204852 -4.1982946 -4.1815472 -4.170938 -4.1717052 -4.1752748 -4.1799912 -4.1870017 -4.1936479 -4.199439 -4.2101526 -4.2150488 -4.2009654 -4.1843762 -4.1689978][-4.2198296 -4.1969957 -4.17998 -4.1714325 -4.1781282 -4.1880174 -4.1933122 -4.1998587 -4.20484 -4.2056675 -4.2045941 -4.1981573 -4.1816506 -4.1641579 -4.147892][-4.2292871 -4.208848 -4.1900954 -4.1815753 -4.1910768 -4.2038584 -4.2091532 -4.2113571 -4.2105241 -4.2067018 -4.19825 -4.1865888 -4.1728959 -4.1622157 -4.1527696][-4.2506442 -4.2356958 -4.21668 -4.2050357 -4.2103648 -4.2199297 -4.2225704 -4.2195086 -4.2143307 -4.206883 -4.1966753 -4.1868696 -4.1785636 -4.1749816 -4.1722565][-4.273839 -4.2642446 -4.244987 -4.23061 -4.2280493 -4.2303843 -4.228446 -4.22253 -4.2163625 -4.20941 -4.2012587 -4.1945515 -4.1917143 -4.1951737 -4.1963744]]...]
INFO - root - 2017-12-07 11:55:26.603746: step 5610, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.759 sec/batch; 68h:56m:25s remains)
INFO - root - 2017-12-07 11:55:33.393939: step 5620, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 64h:05m:29s remains)
INFO - root - 2017-12-07 11:55:40.258038: step 5630, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.672 sec/batch; 61h:00m:31s remains)
INFO - root - 2017-12-07 11:55:47.023450: step 5640, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 56h:58m:05s remains)
INFO - root - 2017-12-07 11:55:53.840842: step 5650, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:06m:09s remains)
INFO - root - 2017-12-07 11:56:00.702857: step 5660, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 65h:22m:59s remains)
INFO - root - 2017-12-07 11:56:07.493614: step 5670, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 59h:48m:42s remains)
INFO - root - 2017-12-07 11:56:14.276167: step 5680, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 56h:51m:12s remains)
INFO - root - 2017-12-07 11:56:20.863184: step 5690, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 57h:27m:25s remains)
INFO - root - 2017-12-07 11:56:27.645745: step 5700, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 67h:26m:48s remains)
2017-12-07 11:56:28.326051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24356 -4.2441263 -4.2421236 -4.2412343 -4.2469025 -4.2592745 -4.2698631 -4.2729173 -4.2699413 -4.2647057 -4.2628269 -4.26648 -4.274272 -4.282342 -4.2881441][-4.2148361 -4.2120547 -4.2063437 -4.201921 -4.2070012 -4.2215495 -4.2349558 -4.2398758 -4.2386847 -4.2345114 -4.2318544 -4.2351394 -4.244864 -4.2547507 -4.2606454][-4.1940446 -4.1870031 -4.1749611 -4.1647243 -4.1682787 -4.1828609 -4.1991792 -4.2070518 -4.2094889 -4.2077112 -4.2035518 -4.2055917 -4.2163496 -4.2261691 -4.2298117][-4.1728959 -4.1611128 -4.1445827 -4.1303377 -4.1318994 -4.1442871 -4.160007 -4.1713 -4.1800494 -4.1853137 -4.1820164 -4.1808467 -4.190412 -4.1969657 -4.1949849][-4.1595111 -4.147584 -4.1320114 -4.1128144 -4.1034718 -4.1015825 -4.1084995 -4.1215534 -4.1394334 -4.1589508 -4.1654639 -4.1658044 -4.1716728 -4.1701684 -4.1553292][-4.1556911 -4.1474528 -4.1338406 -4.1095181 -4.0848174 -4.0582333 -4.0472174 -4.0596786 -4.0918951 -4.1334486 -4.1589479 -4.1659269 -4.1680307 -4.1550555 -4.1243596][-4.1765823 -4.1663752 -4.1487823 -4.1125183 -4.0643559 -4.0060492 -3.9651256 -3.9674268 -4.018003 -4.0916905 -4.1451316 -4.1666927 -4.1746821 -4.1587553 -4.1163316][-4.2236843 -4.2065411 -4.1799288 -4.1251483 -4.0469146 -3.9580026 -3.8805501 -3.8606749 -3.92307 -4.032661 -4.1207366 -4.1668544 -4.1876903 -4.17428 -4.114594][-4.2693477 -4.2483134 -4.2144313 -4.1458344 -4.0473051 -3.9352231 -3.8255205 -3.7832642 -3.8583391 -3.9998808 -4.1136961 -4.1747289 -4.2046304 -4.1925044 -4.1160588][-4.2998939 -4.2803378 -4.2442474 -4.1718936 -4.0680518 -3.9504974 -3.8317764 -3.792542 -3.8814368 -4.031857 -4.1425667 -4.1952429 -4.2179723 -4.204546 -4.1210046][-4.3157392 -4.3036246 -4.2702432 -4.2030764 -4.1109667 -4.0093617 -3.9159493 -3.9093235 -3.9988766 -4.122642 -4.2010164 -4.2250252 -4.2289 -4.2138944 -4.1400323][-4.3187723 -4.3147364 -4.2892194 -4.2385869 -4.1732116 -4.1059542 -4.0577579 -4.0776057 -4.1458015 -4.2240562 -4.2651663 -4.25943 -4.23989 -4.2196426 -4.1650934][-4.3083282 -4.3153753 -4.3038578 -4.2750783 -4.240675 -4.2096682 -4.1944933 -4.2164941 -4.255002 -4.292491 -4.3032184 -4.2786255 -4.2429338 -4.22015 -4.1906371][-4.2892823 -4.3122935 -4.3202782 -4.31034 -4.29437 -4.2830181 -4.2788162 -4.2901888 -4.3063278 -4.3157239 -4.3080354 -4.281662 -4.250772 -4.2348657 -4.2284636][-4.2653351 -4.3006172 -4.3242564 -4.3286366 -4.3217721 -4.3150225 -4.3104644 -4.3133445 -4.3169956 -4.3146534 -4.3012276 -4.28174 -4.2647405 -4.2593527 -4.2647529]]...]
INFO - root - 2017-12-07 11:56:35.123053: step 5710, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:10m:30s remains)
INFO - root - 2017-12-07 11:56:41.936739: step 5720, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 62h:27m:15s remains)
INFO - root - 2017-12-07 11:56:48.715914: step 5730, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 64h:13m:11s remains)
INFO - root - 2017-12-07 11:56:55.468800: step 5740, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 64h:44m:52s remains)
INFO - root - 2017-12-07 11:57:02.256426: step 5750, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 63h:20m:29s remains)
INFO - root - 2017-12-07 11:57:09.020226: step 5760, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 58h:45m:13s remains)
INFO - root - 2017-12-07 11:57:15.936164: step 5770, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 63h:02m:31s remains)
INFO - root - 2017-12-07 11:57:22.791758: step 5780, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.745 sec/batch; 67h:34m:49s remains)
INFO - root - 2017-12-07 11:57:29.507949: step 5790, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 66h:47m:27s remains)
INFO - root - 2017-12-07 11:57:36.377397: step 5800, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:26m:23s remains)
2017-12-07 11:57:37.088021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1581407 -4.1539412 -4.1650052 -4.1875525 -4.2186537 -4.2492704 -4.261354 -4.2542415 -4.2301955 -4.1964946 -4.1593194 -4.1477494 -4.154675 -4.1644311 -4.1757889][-4.133904 -4.1515737 -4.1777468 -4.2059307 -4.2352948 -4.2581387 -4.2619524 -4.2504878 -4.2259746 -4.1988635 -4.1700425 -4.1588993 -4.1646271 -4.1744804 -4.1846][-4.1255941 -4.1618214 -4.2013431 -4.2266126 -4.2403307 -4.2447443 -4.2372751 -4.2261567 -4.2102904 -4.1972022 -4.1832137 -4.1738486 -4.1766529 -4.1853514 -4.1887312][-4.1263385 -4.1667538 -4.2070513 -4.2190418 -4.2129254 -4.2015023 -4.1929574 -4.1880565 -4.1836929 -4.1833534 -4.179945 -4.1749535 -4.176218 -4.1804543 -4.1766138][-4.1117792 -4.1424804 -4.168941 -4.1600952 -4.1291661 -4.109694 -4.1111355 -4.1215482 -4.1261678 -4.1361871 -4.14782 -4.1520262 -4.1520524 -4.1466212 -4.1309934][-4.0752034 -4.0871906 -4.0898294 -4.0572472 -4.0008464 -3.9732425 -3.9938195 -4.029645 -4.0465088 -4.069242 -4.1012273 -4.1255345 -4.1321154 -4.118608 -4.0898466][-4.0577087 -4.0367956 -4.0076723 -3.9461231 -3.854948 -3.7969284 -3.8319716 -3.9079306 -3.9566851 -3.994447 -4.0518408 -4.1047888 -4.1317315 -4.1249733 -4.0925612][-4.0700274 -4.0211897 -3.9656138 -3.8894463 -3.777842 -3.6752081 -3.6852331 -3.7930365 -3.8771384 -3.9259982 -4.0022626 -4.0798936 -4.1239285 -4.1329427 -4.1144085][-4.1157808 -4.0611353 -4.0009131 -3.9417322 -3.8609233 -3.7705739 -3.7496436 -3.8314674 -3.911757 -3.9537461 -4.0150442 -4.0836186 -4.1251273 -4.1438465 -4.1436148][-4.1743155 -4.1310124 -4.0837054 -4.0455332 -4.0029736 -3.9550745 -3.9344609 -3.977459 -4.0326362 -4.0633688 -4.1024332 -4.1417174 -4.1621504 -4.1728468 -4.1758623][-4.2202611 -4.1951466 -4.1651392 -4.1378083 -4.1132855 -4.0946965 -4.0879869 -4.1118994 -4.1547194 -4.1820168 -4.2074966 -4.2288036 -4.2339787 -4.2312016 -4.2199712][-4.2506833 -4.2360921 -4.2231789 -4.2124252 -4.2030692 -4.2014289 -4.2031946 -4.2143526 -4.2421503 -4.2643495 -4.281775 -4.2936254 -4.294457 -4.2839317 -4.26187][-4.2747936 -4.2640114 -4.2594271 -4.2597237 -4.2595749 -4.2658992 -4.270565 -4.2766767 -4.291431 -4.3026242 -4.3138614 -4.3210735 -4.3224673 -4.314115 -4.2937403][-4.2902594 -4.2831459 -4.2799273 -4.2818332 -4.284338 -4.290803 -4.2972765 -4.3012533 -4.3076167 -4.3124537 -4.3183408 -4.3222957 -4.3235817 -4.31975 -4.3075795][-4.2963204 -4.2928061 -4.2915411 -4.2929411 -4.2960296 -4.3005428 -4.3070068 -4.3117294 -4.3146129 -4.3165679 -4.3182373 -4.3186707 -4.3176007 -4.3152223 -4.3089848]]...]
INFO - root - 2017-12-07 11:57:43.798137: step 5810, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:59m:55s remains)
INFO - root - 2017-12-07 11:57:50.384952: step 5820, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 60h:24m:17s remains)
INFO - root - 2017-12-07 11:57:57.164530: step 5830, loss = 2.05, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 55h:58m:10s remains)
INFO - root - 2017-12-07 11:58:03.977429: step 5840, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 63h:37m:02s remains)
INFO - root - 2017-12-07 11:58:10.877573: step 5850, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 66h:46m:19s remains)
INFO - root - 2017-12-07 11:58:17.689259: step 5860, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 62h:41m:04s remains)
INFO - root - 2017-12-07 11:58:24.499039: step 5870, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 57h:59m:09s remains)
INFO - root - 2017-12-07 11:58:31.306777: step 5880, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:32m:08s remains)
INFO - root - 2017-12-07 11:58:38.011161: step 5890, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 65h:44m:27s remains)
INFO - root - 2017-12-07 11:58:44.841599: step 5900, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 65h:27m:08s remains)
2017-12-07 11:58:45.546184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2107444 -4.2014656 -4.2068248 -4.2140579 -4.2226915 -4.2222605 -4.2196531 -4.2242389 -4.23116 -4.2376146 -4.2447248 -4.2514019 -4.2515979 -4.2516556 -4.2529817][-4.202692 -4.1887116 -4.1928644 -4.1990685 -4.2055531 -4.2021813 -4.1975207 -4.2053723 -4.2187905 -4.2315474 -4.2430682 -4.2513542 -4.2498746 -4.2451887 -4.239902][-4.19522 -4.1789455 -4.1870584 -4.1965971 -4.2013144 -4.1945615 -4.18561 -4.1942334 -4.2116032 -4.2287693 -4.2451878 -4.2558475 -4.2536635 -4.2452488 -4.2334428][-4.1869855 -4.169867 -4.1864004 -4.2059 -4.2138858 -4.203764 -4.1874771 -4.1916919 -4.2081428 -4.2282929 -4.2487421 -4.2621436 -4.2621837 -4.2525463 -4.2358866][-4.1770668 -4.1556478 -4.1791987 -4.2097416 -4.2224393 -4.2106509 -4.1894164 -4.1888456 -4.2024431 -4.2234468 -4.2458782 -4.2619309 -4.26677 -4.2577734 -4.2376642][-4.1562805 -4.1307468 -4.1599226 -4.1985674 -4.2120771 -4.1962104 -4.1721768 -4.1699347 -4.1830168 -4.2052255 -4.2270627 -4.2442842 -4.252923 -4.2458663 -4.224534][-4.1382904 -4.1136966 -4.149498 -4.1924667 -4.2048135 -4.1863503 -4.1628428 -4.1608477 -4.17125 -4.1904316 -4.2111254 -4.23012 -4.243619 -4.2397575 -4.2195215][-4.1243124 -4.1052718 -4.1477056 -4.1910281 -4.1991673 -4.1764269 -4.1527119 -4.1518178 -4.160059 -4.1776304 -4.1989379 -4.2210803 -4.2400432 -4.2407432 -4.2244415][-4.122335 -4.1098714 -4.1558685 -4.1984887 -4.2035823 -4.17735 -4.1551189 -4.1577687 -4.1666 -4.1817622 -4.1991839 -4.2202883 -4.2401581 -4.2425752 -4.2295117][-4.1474485 -4.1402292 -4.18276 -4.2195406 -4.2204862 -4.193141 -4.1735768 -4.1800823 -4.1908727 -4.2021513 -4.2138805 -4.2308769 -4.2476072 -4.2487197 -4.2379436][-4.1770229 -4.1740537 -4.2089343 -4.2369919 -4.2345996 -4.2094965 -4.1934729 -4.2028055 -4.2147608 -4.2243576 -4.2331753 -4.2467365 -4.2603788 -4.2613635 -4.2529621][-4.2039881 -4.2018342 -4.2273245 -4.2482066 -4.2460117 -4.2271557 -4.2154474 -4.224668 -4.2351933 -4.2413931 -4.2476649 -4.2588625 -4.2710061 -4.27352 -4.2684536][-4.2327065 -4.229785 -4.2469435 -4.2623491 -4.2634292 -4.2531757 -4.246882 -4.2539091 -4.2597947 -4.2622657 -4.2654457 -4.273612 -4.2836123 -4.2869191 -4.2859192][-4.2656379 -4.2616763 -4.2719464 -4.2835164 -4.2880025 -4.2855372 -4.2837176 -4.288166 -4.2899323 -4.2893324 -4.2898364 -4.2946496 -4.3010416 -4.30348 -4.304667][-4.2926803 -4.2884212 -4.2930317 -4.3000736 -4.3050408 -4.3070874 -4.3080945 -4.3102732 -4.3103604 -4.3098187 -4.3099174 -4.3120737 -4.314795 -4.3157434 -4.316721]]...]
INFO - root - 2017-12-07 11:58:52.312037: step 5910, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.658 sec/batch; 59h:44m:14s remains)
INFO - root - 2017-12-07 11:58:59.036437: step 5920, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 61h:51m:56s remains)
INFO - root - 2017-12-07 11:59:05.827803: step 5930, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 62h:26m:57s remains)
INFO - root - 2017-12-07 11:59:12.614197: step 5940, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 57h:54m:07s remains)
INFO - root - 2017-12-07 11:59:19.290141: step 5950, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:38m:00s remains)
INFO - root - 2017-12-07 11:59:26.060153: step 5960, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 66h:46m:03s remains)
INFO - root - 2017-12-07 11:59:32.834681: step 5970, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 65h:47m:42s remains)
INFO - root - 2017-12-07 11:59:39.627789: step 5980, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 62h:15m:18s remains)
INFO - root - 2017-12-07 11:59:46.196641: step 5990, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 56h:23m:34s remains)
INFO - root - 2017-12-07 11:59:52.936985: step 6000, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:12m:39s remains)
2017-12-07 11:59:53.745348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1834373 -4.1919866 -4.2118392 -4.23014 -4.2341633 -4.2312455 -4.2383513 -4.2551661 -4.2669797 -4.2701435 -4.2619047 -4.2415915 -4.2106776 -4.1733413 -4.1481748][-4.12965 -4.1390319 -4.1751223 -4.2169318 -4.2350197 -4.2354074 -4.2438464 -4.2578 -4.2643766 -4.2600179 -4.2507167 -4.2273126 -4.1905937 -4.1537342 -4.1370411][-4.1096058 -4.11945 -4.1572146 -4.2042294 -4.2245364 -4.2266345 -4.235661 -4.2470093 -4.2527122 -4.2499509 -4.2410455 -4.21578 -4.1715097 -4.1334667 -4.1257582][-4.1206708 -4.1313863 -4.1592793 -4.1964917 -4.2112861 -4.2114396 -4.2195287 -4.2303967 -4.2355285 -4.23804 -4.2327766 -4.2078509 -4.1657476 -4.1326637 -4.1323905][-4.1446619 -4.1507039 -4.1649208 -4.1867781 -4.188735 -4.1839094 -4.1938791 -4.2103839 -4.2181544 -4.2250385 -4.2239742 -4.2052379 -4.1710691 -4.1436019 -4.1467257][-4.1778736 -4.1802688 -4.1825724 -4.1900039 -4.1814141 -4.1673884 -4.1752243 -4.1921005 -4.1991744 -4.2095766 -4.2140212 -4.2064581 -4.18225 -4.1553712 -4.1551452][-4.210288 -4.2116766 -4.2043762 -4.1993 -4.1777349 -4.1511316 -4.1504292 -4.1618624 -4.16256 -4.1680703 -4.1840973 -4.1991472 -4.1954522 -4.17641 -4.1734519][-4.2170005 -4.2166176 -4.2086153 -4.2050896 -4.183579 -4.1492128 -4.1364131 -4.1343746 -4.1158638 -4.1016264 -4.1228485 -4.1626177 -4.1878 -4.1913166 -4.20084][-4.1933403 -4.1987214 -4.2027016 -4.2183709 -4.2113743 -4.178441 -4.1489782 -4.1165643 -4.0639725 -4.01984 -4.0402045 -4.1058054 -4.1641369 -4.197484 -4.224741][-4.1504517 -4.17046 -4.1959891 -4.2375326 -4.2478943 -4.220171 -4.1780672 -4.11412 -4.0238857 -3.9484768 -3.9635077 -4.0459347 -4.1283584 -4.1893706 -4.2347879][-4.1065078 -4.1478724 -4.2000961 -4.2636318 -4.2898331 -4.2637687 -4.212234 -4.1268663 -4.0150981 -3.9295406 -3.9374995 -4.0177269 -4.1044283 -4.1750407 -4.2277336][-4.0687017 -4.1340928 -4.205307 -4.277451 -4.3129191 -4.2916117 -4.2368603 -4.147614 -4.0392685 -3.9605446 -3.9664817 -4.0364079 -4.1126642 -4.1746936 -4.2196312][-4.0297565 -4.1134496 -4.1887922 -4.2590327 -4.2994137 -4.2847733 -4.232852 -4.1582451 -4.0761523 -4.0211439 -4.0263724 -4.077383 -4.133616 -4.1798515 -4.2129869][-3.9946592 -4.0761285 -4.1404028 -4.2071905 -4.2548184 -4.2518296 -4.211237 -4.1574535 -4.107379 -4.0801849 -4.085712 -4.1149683 -4.145575 -4.1745572 -4.1967196][-3.9867458 -4.0515542 -4.0985837 -4.1597142 -4.2126245 -4.2201467 -4.1942143 -4.1609945 -4.1357746 -4.1281867 -4.1340756 -4.1440229 -4.1537328 -4.1689062 -4.1836171]]...]
INFO - root - 2017-12-07 12:00:00.464425: step 6010, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 63h:39m:47s remains)
INFO - root - 2017-12-07 12:00:07.267827: step 6020, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 59h:15m:34s remains)
INFO - root - 2017-12-07 12:00:14.039442: step 6030, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 59h:39m:38s remains)
INFO - root - 2017-12-07 12:00:20.929418: step 6040, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 65h:43m:59s remains)
INFO - root - 2017-12-07 12:00:27.682806: step 6050, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 61h:43m:08s remains)
INFO - root - 2017-12-07 12:00:34.455492: step 6060, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 57h:30m:06s remains)
INFO - root - 2017-12-07 12:00:41.214711: step 6070, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:41m:53s remains)
INFO - root - 2017-12-07 12:00:48.080919: step 6080, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.750 sec/batch; 67h:59m:55s remains)
INFO - root - 2017-12-07 12:00:54.609134: step 6090, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 61h:22m:38s remains)
INFO - root - 2017-12-07 12:01:01.291071: step 6100, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 61h:08m:56s remains)
2017-12-07 12:01:01.958860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3246961 -4.3314247 -4.339561 -4.3414097 -4.3346906 -4.3221831 -4.3125281 -4.3104005 -4.3128309 -4.3202386 -4.3309984 -4.3387594 -4.3393517 -4.3318214 -4.32154][-4.3170366 -4.3217921 -4.3286161 -4.3288517 -4.3189745 -4.3018389 -4.2870131 -4.2842231 -4.2861323 -4.2956843 -4.312819 -4.3283544 -4.3302636 -4.3197346 -4.3100753][-4.31408 -4.3161097 -4.3210711 -4.3190346 -4.3054471 -4.2832508 -4.2639446 -4.2619796 -4.2651649 -4.2740951 -4.2945595 -4.3179555 -4.3220668 -4.308835 -4.3007951][-4.3137035 -4.3132677 -4.3129735 -4.3054562 -4.2862844 -4.2582941 -4.2351584 -4.2317495 -4.2377253 -4.2466726 -4.2688422 -4.2989321 -4.3078108 -4.2931786 -4.2858777][-4.3065119 -4.3004909 -4.2942219 -4.283268 -4.2636266 -4.2345028 -4.208169 -4.2022963 -4.211822 -4.2207909 -4.2380295 -4.2653689 -4.2747307 -4.2581654 -4.2506046][-4.2973776 -4.2857122 -4.2748804 -4.2630024 -4.2435966 -4.2153397 -4.1871824 -4.1822562 -4.200788 -4.2125921 -4.2235246 -4.2410288 -4.2422285 -4.2199893 -4.2085238][-4.2917943 -4.2755542 -4.2617183 -4.2459846 -4.2263288 -4.1980596 -4.1672239 -4.1640506 -4.1895919 -4.2089033 -4.22267 -4.2350054 -4.2276998 -4.1980767 -4.179038][-4.2886724 -4.2676983 -4.2477326 -4.2276454 -4.2078748 -4.1792707 -4.1458535 -4.139214 -4.162775 -4.187613 -4.2131157 -4.2318187 -4.2247567 -4.1947436 -4.1707859][-4.2881122 -4.2656031 -4.2439451 -4.22103 -4.1973763 -4.1666946 -4.1321568 -4.117806 -4.1318817 -4.1631231 -4.2042255 -4.2322073 -4.2281656 -4.2059007 -4.18763][-4.2923675 -4.2743335 -4.2542253 -4.23009 -4.2008824 -4.1669717 -4.133049 -4.1151023 -4.1210351 -4.1526537 -4.1965804 -4.2260122 -4.2269678 -4.2164412 -4.2073426][-4.2945223 -4.2817121 -4.2649288 -4.2419934 -4.211493 -4.1761003 -4.1396132 -4.1182394 -4.1216297 -4.1507039 -4.1913095 -4.2194653 -4.2258911 -4.2229571 -4.2173843][-4.2927041 -4.2804165 -4.2647233 -4.2426128 -4.2140784 -4.18273 -4.1491108 -4.1293035 -4.1363196 -4.1655045 -4.2015162 -4.2248254 -4.2321405 -4.2319646 -4.2276444][-4.2930059 -4.2803154 -4.2670503 -4.2479196 -4.2279139 -4.2095852 -4.1858549 -4.1716952 -4.179637 -4.2029595 -4.2261853 -4.2398758 -4.2435164 -4.2434344 -4.241291][-4.2965903 -4.2862096 -4.2787318 -4.2665267 -4.2573314 -4.2527189 -4.2425146 -4.2348785 -4.240798 -4.2550035 -4.2658362 -4.2701025 -4.2696538 -4.2691417 -4.2695289][-4.3002419 -4.2919025 -4.2882476 -4.2826829 -4.2810793 -4.2839613 -4.2830973 -4.2811136 -4.2851009 -4.2917814 -4.2948151 -4.2940645 -4.2916484 -4.2903957 -4.2905359]]...]
INFO - root - 2017-12-07 12:01:08.743480: step 6110, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:01m:53s remains)
INFO - root - 2017-12-07 12:01:15.429667: step 6120, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.704 sec/batch; 63h:48m:54s remains)
INFO - root - 2017-12-07 12:01:21.944501: step 6130, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 61h:21m:39s remains)
INFO - root - 2017-12-07 12:01:28.812082: step 6140, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:32m:39s remains)
INFO - root - 2017-12-07 12:01:35.635989: step 6150, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 62h:17m:27s remains)
INFO - root - 2017-12-07 12:01:42.424267: step 6160, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 64h:12m:36s remains)
INFO - root - 2017-12-07 12:01:49.201004: step 6170, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 65h:13m:50s remains)
INFO - root - 2017-12-07 12:01:56.003404: step 6180, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 60h:33m:57s remains)
INFO - root - 2017-12-07 12:02:02.650289: step 6190, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 57h:37m:51s remains)
INFO - root - 2017-12-07 12:02:09.334816: step 6200, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 58h:39m:36s remains)
2017-12-07 12:02:10.066854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2100296 -4.1972952 -4.20919 -4.22382 -4.2198839 -4.1923137 -4.14482 -4.0966997 -4.069046 -4.0909724 -4.140595 -4.1831756 -4.2099662 -4.2383442 -4.279923][-4.2143807 -4.2097578 -4.2246857 -4.2367263 -4.2247953 -4.1868782 -4.1360483 -4.0958347 -4.0780249 -4.0965438 -4.1433139 -4.1910582 -4.2313657 -4.2731457 -4.3134847][-4.2344089 -4.2378993 -4.2532105 -4.259006 -4.2378964 -4.1893973 -4.131937 -4.0918469 -4.0775104 -4.0956817 -4.1455474 -4.1993055 -4.2486644 -4.2956023 -4.3308654][-4.2583113 -4.26809 -4.2832413 -4.278172 -4.2435961 -4.1832833 -4.1109815 -4.06311 -4.0546937 -4.0860462 -4.1463065 -4.2111893 -4.268393 -4.3115859 -4.3356385][-4.2813468 -4.2939043 -4.3030529 -4.2826772 -4.2331481 -4.1597886 -4.0693979 -4.0147033 -4.0220313 -4.0741673 -4.1495419 -4.2261844 -4.29038 -4.3251767 -4.3310122][-4.3051758 -4.3134608 -4.3094053 -4.2742543 -4.205442 -4.1120434 -4.009284 -3.9614353 -4.0015755 -4.0828953 -4.1746035 -4.2551527 -4.3141794 -4.3302903 -4.3118153][-4.3111649 -4.3132157 -4.2975326 -4.2468629 -4.1568861 -4.0420814 -3.9421532 -3.9232883 -4.0013652 -4.1053643 -4.2021933 -4.2772975 -4.3215284 -4.3200655 -4.2816453][-4.2913618 -4.2886376 -4.2631989 -4.1975508 -4.0916796 -3.9762511 -3.908623 -3.935472 -4.03334 -4.1359339 -4.2247114 -4.28713 -4.3147521 -4.2982888 -4.245604][-4.2587538 -4.2563281 -4.2242928 -4.1487827 -4.0413127 -3.9502158 -3.9336171 -3.9942887 -4.0889864 -4.1763611 -4.2500396 -4.2979183 -4.3079433 -4.2768641 -4.2177477][-4.2355437 -4.2362351 -4.199162 -4.1194758 -4.0220509 -3.9654036 -3.9894183 -4.0589523 -4.1380258 -4.2116914 -4.2738876 -4.3047924 -4.2967687 -4.2553449 -4.194387][-4.2172856 -4.2159162 -4.1718535 -4.09352 -4.0180044 -3.995615 -4.0427613 -4.1108456 -4.1761732 -4.2406893 -4.2918591 -4.3091474 -4.2877326 -4.2390137 -4.173934][-4.1986356 -4.1876988 -4.1393223 -4.07604 -4.0321159 -4.0370469 -4.0941725 -4.156426 -4.20736 -4.2577791 -4.2974792 -4.3063021 -4.2769823 -4.2216244 -4.1542821][-4.1885037 -4.1685081 -4.1214161 -4.0800033 -4.0615845 -4.0762506 -4.1282587 -4.183362 -4.2248373 -4.2658668 -4.2962561 -4.296164 -4.2574072 -4.1959977 -4.1276126][-4.188592 -4.16334 -4.1223178 -4.0983677 -4.0965362 -4.113296 -4.1568756 -4.2029057 -4.2386951 -4.271873 -4.2924104 -4.2829185 -4.2385068 -4.173666 -4.10564][-4.1926255 -4.1710672 -4.140759 -4.130744 -4.13642 -4.1507487 -4.1828504 -4.2158327 -4.2455831 -4.2697692 -4.2810783 -4.2648487 -4.2186131 -4.1540136 -4.0906773]]...]
INFO - root - 2017-12-07 12:02:16.820720: step 6210, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 60h:12m:12s remains)
INFO - root - 2017-12-07 12:02:23.658055: step 6220, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 58h:28m:58s remains)
INFO - root - 2017-12-07 12:02:30.448802: step 6230, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:46m:59s remains)
INFO - root - 2017-12-07 12:02:37.471338: step 6240, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.768 sec/batch; 69h:37m:58s remains)
INFO - root - 2017-12-07 12:02:44.352997: step 6250, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 66h:42m:28s remains)
INFO - root - 2017-12-07 12:02:51.141980: step 6260, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 61h:57m:51s remains)
INFO - root - 2017-12-07 12:02:57.887409: step 6270, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:46m:18s remains)
INFO - root - 2017-12-07 12:03:04.751759: step 6280, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:56m:14s remains)
INFO - root - 2017-12-07 12:03:11.554680: step 6290, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 64h:19m:37s remains)
INFO - root - 2017-12-07 12:03:18.287074: step 6300, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 62h:04m:25s remains)
2017-12-07 12:03:18.961646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1844306 -4.1965795 -4.20055 -4.2021861 -4.2062726 -4.2188663 -4.2346716 -4.2382741 -4.2274685 -4.2124958 -4.2068214 -4.2057118 -4.1996446 -4.1915865 -4.1898222][-4.19971 -4.2208109 -4.2318625 -4.235323 -4.2361627 -4.2444196 -4.2533793 -4.2496786 -4.2323394 -4.2113528 -4.2013788 -4.1990423 -4.1933727 -4.1884885 -4.1935549][-4.2348237 -4.2595549 -4.2743206 -4.2776566 -4.271925 -4.2695093 -4.2635021 -4.2486658 -4.2304349 -4.2138762 -4.2047868 -4.1978812 -4.1855211 -4.1791091 -4.1875939][-4.254241 -4.2803841 -4.2981992 -4.3002563 -4.2873707 -4.2673349 -4.236824 -4.2083116 -4.1954665 -4.1955953 -4.1981392 -4.1904206 -4.1737185 -4.1652985 -4.1754379][-4.23515 -4.2565608 -4.2715073 -4.2726831 -4.2539682 -4.2126389 -4.1542845 -4.1146889 -4.1169319 -4.144485 -4.1627631 -4.1578951 -4.14074 -4.1349254 -4.1507792][-4.18649 -4.1986384 -4.2099266 -4.2154026 -4.190742 -4.1219387 -4.0266075 -3.9740653 -4.0063066 -4.0689011 -4.1051869 -4.1074567 -4.0970082 -4.10232 -4.1274967][-4.1403108 -4.1432881 -4.1522059 -4.1605916 -4.1294885 -4.0288277 -3.8841176 -3.8149276 -3.8937905 -4.0014992 -4.0534444 -4.0616007 -4.0616117 -4.0803466 -4.1133456][-4.1144686 -4.1079388 -4.1134806 -4.1201763 -4.083354 -3.9676971 -3.79222 -3.7151198 -3.8396528 -3.9802158 -4.0381374 -4.0436263 -4.0482836 -4.0743084 -4.1071548][-4.1162786 -4.1060872 -4.1147437 -4.1220694 -4.0915976 -3.993031 -3.8429797 -3.7850113 -3.8972859 -4.0198393 -4.0651808 -4.063457 -4.0679212 -4.0931888 -4.1210771][-4.1414661 -4.1350164 -4.1480217 -4.1588817 -4.1412597 -4.0752535 -3.9757519 -3.9404349 -4.00891 -4.0869632 -4.1139917 -4.1095109 -4.1134558 -4.1326094 -4.1533403][-4.1796322 -4.1798387 -4.1920018 -4.2022562 -4.1953092 -4.1567221 -4.0988312 -4.0774641 -4.1085525 -4.1477075 -4.1640058 -4.1656942 -4.1707311 -4.1833558 -4.195889][-4.2270594 -4.2318835 -4.2398877 -4.2456646 -4.2426085 -4.2225466 -4.19326 -4.1786847 -4.1874394 -4.2028971 -4.2117991 -4.2162113 -4.2209744 -4.2249918 -4.2293315][-4.2636366 -4.2713017 -4.2769251 -4.2790136 -4.2775578 -4.2701449 -4.2597036 -4.2484846 -4.2441769 -4.2451758 -4.2482305 -4.2524223 -4.2516546 -4.2455459 -4.2394743][-4.293685 -4.3018446 -4.3060308 -4.3054023 -4.3032336 -4.3024311 -4.2998338 -4.2906632 -4.2808142 -4.2753935 -4.2743344 -4.2726278 -4.2621737 -4.2440305 -4.226531][-4.3156991 -4.3230958 -4.3245335 -4.3212757 -4.3167858 -4.3168535 -4.3164268 -4.3100572 -4.3001051 -4.2928872 -4.288795 -4.2784305 -4.2572856 -4.2290096 -4.2008615]]...]
INFO - root - 2017-12-07 12:03:25.756077: step 6310, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 64h:48m:44s remains)
INFO - root - 2017-12-07 12:03:32.573098: step 6320, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 66h:27m:26s remains)
INFO - root - 2017-12-07 12:03:39.332900: step 6330, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 62h:46m:05s remains)
INFO - root - 2017-12-07 12:03:46.075757: step 6340, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 57h:56m:09s remains)
INFO - root - 2017-12-07 12:03:52.902097: step 6350, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:31m:52s remains)
INFO - root - 2017-12-07 12:03:59.790283: step 6360, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 67h:50m:39s remains)
INFO - root - 2017-12-07 12:04:06.581541: step 6370, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 63h:47m:58s remains)
INFO - root - 2017-12-07 12:04:13.343510: step 6380, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 61h:13m:05s remains)
INFO - root - 2017-12-07 12:04:20.100000: step 6390, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 57h:02m:36s remains)
INFO - root - 2017-12-07 12:04:26.896739: step 6400, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 60h:01m:04s remains)
2017-12-07 12:04:27.639648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2311325 -4.2300067 -4.225863 -4.2117529 -4.2006702 -4.19167 -4.1858172 -4.1772695 -4.1790814 -4.185648 -4.2011147 -4.2151008 -4.2131524 -4.196033 -4.179657][-4.2352133 -4.2248893 -4.2158394 -4.1979003 -4.1823711 -4.1732936 -4.1709714 -4.1665354 -4.1731124 -4.1845312 -4.2013717 -4.2159367 -4.2177882 -4.2030644 -4.188139][-4.2295337 -4.2207179 -4.2119312 -4.1917129 -4.1687827 -4.1502581 -4.1442156 -4.1460404 -4.1618052 -4.18062 -4.1952114 -4.2038836 -4.2033634 -4.1912761 -4.1769485][-4.2208662 -4.2162271 -4.2116485 -4.1901522 -4.157052 -4.1245008 -4.1112981 -4.1226921 -4.1499553 -4.1744261 -4.1833906 -4.1806149 -4.1734815 -4.1648583 -4.1551566][-4.2094293 -4.2095933 -4.2081852 -4.1847749 -4.1439772 -4.10054 -4.08178 -4.100369 -4.1373043 -4.1636906 -4.1651134 -4.1514544 -4.1381516 -4.1332245 -4.1288571][-4.1873322 -4.1906686 -4.1937847 -4.1718774 -4.1280208 -4.0792441 -4.0556116 -4.0780973 -4.1226563 -4.1505427 -4.150176 -4.1351209 -4.1209946 -4.1242218 -4.131794][-4.1641188 -4.1654191 -4.1707058 -4.1548657 -4.1138873 -4.0637479 -4.0350208 -4.0567856 -4.1032147 -4.134584 -4.1425076 -4.1424947 -4.141542 -4.1562066 -4.1691279][-4.1443787 -4.1400595 -4.1445227 -4.1380563 -4.1060352 -4.0630684 -4.0331163 -4.0498724 -4.0921192 -4.1234264 -4.1364756 -4.1542444 -4.1721711 -4.1983428 -4.2128887][-4.1489067 -4.1372108 -4.1360984 -4.1393661 -4.1229625 -4.0924063 -4.0653915 -4.0727057 -4.1011558 -4.1214919 -4.134521 -4.1629605 -4.1951065 -4.2277913 -4.2394533][-4.1655679 -4.1467175 -4.1403675 -4.1507215 -4.1522064 -4.1385512 -4.1190124 -4.1179714 -4.1301775 -4.1388617 -4.1509223 -4.1790385 -4.2095966 -4.236434 -4.2394233][-4.18535 -4.1627769 -4.1525764 -4.1629438 -4.1747575 -4.1761632 -4.1683369 -4.1640263 -4.1654186 -4.1668653 -4.1748433 -4.1916852 -4.2074714 -4.2215929 -4.2153649][-4.2102017 -4.1888313 -4.1759634 -4.1807017 -4.1932478 -4.2029138 -4.2039943 -4.2022009 -4.2022233 -4.2007089 -4.2010422 -4.2017908 -4.2001891 -4.2030163 -4.1925125][-4.2299767 -4.2126436 -4.2009163 -4.1997738 -4.2083788 -4.220983 -4.2266746 -4.2284369 -4.2303443 -4.2275667 -4.2186284 -4.2077804 -4.1949644 -4.1894603 -4.1787457][-4.2508225 -4.2352328 -4.2247496 -4.2186809 -4.2213469 -4.2320595 -4.2395062 -4.2445579 -4.2492166 -4.246758 -4.2330818 -4.2171726 -4.2045436 -4.2005844 -4.1919522][-4.2689738 -4.2557425 -4.2459679 -4.2391725 -4.23886 -4.2456241 -4.2500534 -4.253346 -4.2583351 -4.2575283 -4.2454348 -4.232759 -4.2269068 -4.22693 -4.2235532]]...]
INFO - root - 2017-12-07 12:04:34.497995: step 6410, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 61h:15m:52s remains)
INFO - root - 2017-12-07 12:04:41.279297: step 6420, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 57h:07m:24s remains)
INFO - root - 2017-12-07 12:04:48.129081: step 6430, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 61h:59m:14s remains)
INFO - root - 2017-12-07 12:04:54.720025: step 6440, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 61h:39m:29s remains)
INFO - root - 2017-12-07 12:05:01.494549: step 6450, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 61h:45m:56s remains)
INFO - root - 2017-12-07 12:05:08.326273: step 6460, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 56h:34m:56s remains)
INFO - root - 2017-12-07 12:05:15.178824: step 6470, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:46m:04s remains)
INFO - root - 2017-12-07 12:05:22.043423: step 6480, loss = 2.03, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 64h:43m:55s remains)
INFO - root - 2017-12-07 12:05:28.737209: step 6490, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:40m:26s remains)
INFO - root - 2017-12-07 12:05:35.459582: step 6500, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 56h:55m:41s remains)
2017-12-07 12:05:36.337139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3616896 -4.365375 -4.3612261 -4.3308811 -4.2817473 -4.2271996 -4.1823273 -4.1500659 -4.1376414 -4.1372724 -4.148016 -4.1780939 -4.2238884 -4.2655234 -4.2912354][-4.3653297 -4.3661432 -4.3561854 -4.31719 -4.2574463 -4.1956172 -4.147985 -4.1174188 -4.1113825 -4.1231861 -4.1405296 -4.1644645 -4.2099085 -4.2557025 -4.2840629][-4.3673444 -4.3648334 -4.3448582 -4.2953596 -4.225172 -4.1574435 -4.1111531 -4.08075 -4.0756474 -4.0950074 -4.1187096 -4.1467733 -4.198719 -4.2498717 -4.2827072][-4.369575 -4.3663406 -4.3387055 -4.2764616 -4.1939931 -4.1189976 -4.069942 -4.0387564 -4.0381517 -4.0680213 -4.099586 -4.1380496 -4.1979795 -4.2525578 -4.2879314][-4.3728518 -4.3683062 -4.3328786 -4.254355 -4.1565356 -4.06769 -4.0060444 -3.9821548 -4.0069323 -4.0516233 -4.0995145 -4.1522665 -4.2166266 -4.26944 -4.3018017][-4.37325 -4.3658371 -4.3235064 -4.22889 -4.1130967 -4.0050893 -3.9267576 -3.92042 -3.9780691 -4.0436182 -4.1096578 -4.1724787 -4.2399025 -4.2920003 -4.3172741][-4.3723736 -4.3614111 -4.3079615 -4.1951451 -4.0557156 -3.9292755 -3.8542776 -3.8785951 -3.9609578 -4.0445485 -4.1259389 -4.1955132 -4.2609286 -4.3058758 -4.3210974][-4.365478 -4.3469272 -4.28027 -4.1525321 -3.9936783 -3.8569808 -3.7983661 -3.8497672 -3.9433744 -4.0396824 -4.1343708 -4.2120609 -4.2751684 -4.3094468 -4.3107667][-4.3519926 -4.3213072 -4.2428293 -4.1172204 -3.9664087 -3.8384316 -3.792491 -3.8523226 -3.9424832 -4.0393353 -4.1385126 -4.2216578 -4.2802272 -4.3044825 -4.2940688][-4.3358941 -4.2917671 -4.2068186 -4.1014342 -3.9885664 -3.8984718 -3.8689106 -3.9154024 -3.9874501 -4.0731134 -4.1634355 -4.238059 -4.2841492 -4.3006177 -4.287343][-4.32344 -4.269155 -4.1858563 -4.107779 -4.0377479 -3.9889414 -3.9747486 -4.0087676 -4.0628209 -4.1281071 -4.1981187 -4.2550616 -4.2880983 -4.2961397 -4.2826195][-4.3199086 -4.2641578 -4.1903992 -4.1353049 -4.0912037 -4.0635734 -4.0622082 -4.0952787 -4.1389656 -4.187346 -4.2371411 -4.2753787 -4.2952337 -4.29843 -4.2875657][-4.3280215 -4.2827268 -4.2225375 -4.178421 -4.1468716 -4.1278644 -4.1319981 -4.1623831 -4.1972537 -4.2343674 -4.2709241 -4.2949753 -4.3066468 -4.3084211 -4.30101][-4.3434262 -4.3144293 -4.2698951 -4.2348766 -4.2121568 -4.2000232 -4.2048578 -4.2271271 -4.2518616 -4.276649 -4.3004918 -4.3129535 -4.3193865 -4.3208265 -4.3161788][-4.3499608 -4.3350711 -4.3059368 -4.2820807 -4.2662239 -4.2608714 -4.2645383 -4.2782788 -4.2939482 -4.3080454 -4.3193955 -4.3257 -4.3279195 -4.3282151 -4.3250437]]...]
INFO - root - 2017-12-07 12:05:43.217713: step 6510, loss = 2.03, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 63h:01m:45s remains)
INFO - root - 2017-12-07 12:05:50.040122: step 6520, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 62h:42m:01s remains)
INFO - root - 2017-12-07 12:05:56.790752: step 6530, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:21m:56s remains)
INFO - root - 2017-12-07 12:06:03.621509: step 6540, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 64h:20m:23s remains)
INFO - root - 2017-12-07 12:06:10.429420: step 6550, loss = 2.09, batch loss = 2.04 (10.7 examples/sec; 0.748 sec/batch; 67h:46m:00s remains)
INFO - root - 2017-12-07 12:06:17.193752: step 6560, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:49m:39s remains)
INFO - root - 2017-12-07 12:06:23.962396: step 6570, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.653 sec/batch; 59h:09m:05s remains)
INFO - root - 2017-12-07 12:06:30.678016: step 6580, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 58h:53m:53s remains)
INFO - root - 2017-12-07 12:06:37.243495: step 6590, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:59m:02s remains)
INFO - root - 2017-12-07 12:06:43.914261: step 6600, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 63h:42m:49s remains)
2017-12-07 12:06:44.607481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3261943 -4.3020768 -4.2647147 -4.2252512 -4.1949253 -4.1684165 -4.1445417 -4.1150165 -4.0858116 -4.073154 -4.0795264 -4.0919676 -4.1187048 -4.1539893 -4.158174][-4.3292546 -4.3037252 -4.2672071 -4.2307129 -4.2008677 -4.1722374 -4.1518922 -4.1276155 -4.1023374 -4.0904384 -4.0944333 -4.11101 -4.1437454 -4.1752276 -4.1766706][-4.3281851 -4.3001113 -4.2619643 -4.225131 -4.1950884 -4.1648121 -4.1449094 -4.1317 -4.1189985 -4.1117916 -4.1142554 -4.1275258 -4.1545186 -4.1780734 -4.178688][-4.3242283 -4.2920089 -4.2482381 -4.2067142 -4.1754842 -4.1485958 -4.1275792 -4.1178608 -4.1126494 -4.1137447 -4.12101 -4.1278715 -4.1450486 -4.1663074 -4.1782761][-4.3154798 -4.2752094 -4.2248526 -4.1839108 -4.1567707 -4.1289725 -4.0951581 -4.0795503 -4.082644 -4.0958872 -4.1126509 -4.1186662 -4.1281652 -4.1500955 -4.1705155][-4.300611 -4.2505279 -4.1934819 -4.152463 -4.1266546 -4.0883584 -4.0340567 -4.0125613 -4.0312214 -4.0624847 -4.0920324 -4.1027641 -4.1109242 -4.1344118 -4.1594806][-4.2850146 -4.2265182 -4.159833 -4.1109686 -4.0775318 -4.027904 -3.9616797 -3.940063 -3.9710214 -4.0153589 -4.053761 -4.0700669 -4.0820675 -4.1093183 -4.1370139][-4.2732387 -4.21385 -4.1467123 -4.0962992 -4.0597391 -4.009151 -3.9409556 -3.9145908 -3.9431896 -3.987864 -4.03135 -4.0549669 -4.0676308 -4.0861859 -4.107729][-4.2715192 -4.2219195 -4.1711984 -4.1326194 -4.1017332 -4.0580997 -3.9904876 -3.9545202 -3.9698648 -4.0070682 -4.0480251 -4.0723109 -4.0792913 -4.08003 -4.0838766][-4.275424 -4.2356186 -4.1969995 -4.1690884 -4.1461329 -4.1127791 -4.0542378 -4.0170503 -4.0270376 -4.0588403 -4.0939803 -4.1108747 -4.1037369 -4.0855279 -4.0752597][-4.274641 -4.2385154 -4.2053084 -4.1850533 -4.1705837 -4.1480694 -4.1065807 -4.077961 -4.0822496 -4.1041088 -4.126833 -4.1322351 -4.117075 -4.0915613 -4.07761][-4.275836 -4.24319 -4.2148709 -4.1989746 -4.190712 -4.1784716 -4.1554909 -4.138979 -4.1364012 -4.1421337 -4.146811 -4.1375217 -4.1159163 -4.0849752 -4.0735092][-4.2862711 -4.2539768 -4.2235341 -4.2034969 -4.1971636 -4.1901026 -4.1766763 -4.1670732 -4.164516 -4.1646104 -4.160996 -4.1491566 -4.1308594 -4.10359 -4.0926304][-4.3040633 -4.2747946 -4.2457957 -4.226409 -4.2213135 -4.2193389 -4.2125893 -4.2075534 -4.2051311 -4.2028918 -4.1957746 -4.1844082 -4.1683073 -4.1447396 -4.131341][-4.3227596 -4.3000307 -4.2777886 -4.2659225 -4.2659254 -4.2694616 -4.266407 -4.2608356 -4.2576036 -4.2519183 -4.2426252 -4.2329206 -4.2223434 -4.2055345 -4.1946154]]...]
INFO - root - 2017-12-07 12:06:51.348679: step 6610, loss = 2.08, batch loss = 2.03 (13.1 examples/sec; 0.609 sec/batch; 55h:09m:25s remains)
INFO - root - 2017-12-07 12:06:58.141325: step 6620, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 60h:07m:08s remains)
INFO - root - 2017-12-07 12:07:04.857829: step 6630, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 65h:43m:59s remains)
INFO - root - 2017-12-07 12:07:11.695571: step 6640, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 63h:52m:49s remains)
INFO - root - 2017-12-07 12:07:18.465372: step 6650, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:50m:16s remains)
INFO - root - 2017-12-07 12:07:25.269765: step 6660, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 57h:05m:10s remains)
INFO - root - 2017-12-07 12:07:32.136797: step 6670, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 62h:11m:55s remains)
INFO - root - 2017-12-07 12:07:38.943330: step 6680, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 65h:08m:32s remains)
INFO - root - 2017-12-07 12:07:45.583949: step 6690, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 62h:39m:16s remains)
INFO - root - 2017-12-07 12:07:52.337269: step 6700, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:54m:30s remains)
2017-12-07 12:07:53.072452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3190002 -4.3086495 -4.2814727 -4.2547398 -4.2362623 -4.2337656 -4.244729 -4.2540388 -4.2492623 -4.2291942 -4.2042975 -4.1921096 -4.19556 -4.208724 -4.2273312][-4.3228507 -4.3153448 -4.2926579 -4.2692423 -4.2530141 -4.25146 -4.2591252 -4.2572346 -4.2383718 -4.20819 -4.1766019 -4.16168 -4.1675138 -4.186873 -4.2129788][-4.3286924 -4.3234029 -4.3013363 -4.2760639 -4.25755 -4.2528181 -4.2528534 -4.2369986 -4.2092423 -4.181 -4.1561751 -4.1493678 -4.1588993 -4.179369 -4.210465][-4.3343716 -4.3309355 -4.3075657 -4.278028 -4.2515292 -4.2359209 -4.2203426 -4.1870942 -4.1550932 -4.1408539 -4.1381497 -4.1460485 -4.1561384 -4.174964 -4.2099614][-4.3384533 -4.3346953 -4.3073659 -4.268909 -4.2277861 -4.1964207 -4.1564026 -4.0986104 -4.0659213 -4.0713634 -4.1010375 -4.1302009 -4.1462426 -4.1689515 -4.2096806][-4.3381639 -4.3294935 -4.29327 -4.2411962 -4.1847272 -4.1363935 -4.0647335 -3.976635 -3.9510002 -3.988909 -4.0525341 -4.1025972 -4.1330175 -4.1664395 -4.210732][-4.3310585 -4.3142443 -4.2670207 -4.2019496 -4.1325254 -4.0667038 -3.9636416 -3.8525548 -3.8503551 -3.9306586 -4.0207129 -4.0845251 -4.1268325 -4.1660805 -4.2074][-4.3206091 -4.2968841 -4.2389712 -4.1623163 -4.0773988 -3.9877677 -3.8588095 -3.7465136 -3.7838578 -3.8999071 -4.0058942 -4.0792561 -4.1251287 -4.16552 -4.2037334][-4.3123813 -4.2830734 -4.2153916 -4.1288943 -4.03378 -3.9314241 -3.8027198 -3.7182958 -3.7885504 -3.9155514 -4.0190477 -4.0897975 -4.1333814 -4.1696434 -4.202424][-4.3060489 -4.2746115 -4.2045574 -4.1222625 -4.0362153 -3.9454758 -3.8495622 -3.8010976 -3.8730505 -3.9787986 -4.0615716 -4.1192818 -4.1560879 -4.1846614 -4.2097335][-4.3053946 -4.2796392 -4.2196436 -4.1549692 -4.0924568 -4.031354 -3.9744446 -3.9505472 -4.0017467 -4.0705271 -4.1229253 -4.1624689 -4.1925797 -4.2107759 -4.2265553][-4.308394 -4.2898097 -4.243732 -4.1985254 -4.1586814 -4.122045 -4.0926604 -4.0847945 -4.1181135 -4.1578107 -4.1912374 -4.2192373 -4.2409525 -4.2468438 -4.2547493][-4.3129511 -4.2974758 -4.2618361 -4.2295623 -4.2067409 -4.1907039 -4.1823044 -4.1861939 -4.210463 -4.2349534 -4.2570405 -4.2746334 -4.2875714 -4.28566 -4.286324][-4.3188715 -4.3073483 -4.278976 -4.2538548 -4.2392235 -4.2372875 -4.2436604 -4.25616 -4.2784677 -4.2960987 -4.3091812 -4.316968 -4.3212662 -4.31342 -4.3094654][-4.3243022 -4.3149405 -4.2889471 -4.266643 -4.2549691 -4.2582588 -4.2729659 -4.2928643 -4.3162956 -4.3326087 -4.3401127 -4.3424807 -4.3411627 -4.330358 -4.3230596]]...]
INFO - root - 2017-12-07 12:07:59.855344: step 6710, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 64h:01m:41s remains)
INFO - root - 2017-12-07 12:08:06.754862: step 6720, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 67h:08m:56s remains)
INFO - root - 2017-12-07 12:08:13.652304: step 6730, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:32m:44s remains)
INFO - root - 2017-12-07 12:08:20.413896: step 6740, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 58h:36m:35s remains)
INFO - root - 2017-12-07 12:08:27.119947: step 6750, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 65h:03m:52s remains)
INFO - root - 2017-12-07 12:08:33.961834: step 6760, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 62h:24m:12s remains)
INFO - root - 2017-12-07 12:08:40.844401: step 6770, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:35m:41s remains)
INFO - root - 2017-12-07 12:08:47.733343: step 6780, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 57h:41m:07s remains)
INFO - root - 2017-12-07 12:08:54.384964: step 6790, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 62h:31m:26s remains)
INFO - root - 2017-12-07 12:09:01.247995: step 6800, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 64h:42m:41s remains)
2017-12-07 12:09:02.022749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2280874 -4.2322745 -4.2377238 -4.2555504 -4.2611613 -4.2316308 -4.1813364 -4.1668949 -4.2006354 -4.2469482 -4.25997 -4.2541304 -4.2483788 -4.2509074 -4.260869][-4.2186332 -4.216434 -4.2187185 -4.2436233 -4.258759 -4.2284684 -4.1786566 -4.1682534 -4.20109 -4.2419376 -4.2475576 -4.2347751 -4.2287478 -4.2354331 -4.2480435][-4.207469 -4.1986313 -4.1964493 -4.2245755 -4.2441053 -4.2150307 -4.1635942 -4.1509223 -4.1837025 -4.2230597 -4.2258391 -4.2050357 -4.1945186 -4.2054377 -4.2235689][-4.218308 -4.2031269 -4.1930037 -4.2151985 -4.2315474 -4.204639 -4.1490588 -4.1270194 -4.1586061 -4.2034121 -4.2138038 -4.19187 -4.1802111 -4.1942616 -4.2144108][-4.2344227 -4.2180724 -4.2057052 -4.2194586 -4.22166 -4.1867232 -4.1212387 -4.0896482 -4.1240687 -4.1793981 -4.2034025 -4.1891584 -4.1849432 -4.201828 -4.2219753][-4.2483668 -4.234992 -4.2255521 -4.2310319 -4.2114716 -4.155426 -4.0705738 -4.0292783 -4.0688372 -4.1381545 -4.1784573 -4.1830239 -4.1922975 -4.2103152 -4.2304931][-4.25427 -4.2441864 -4.2381029 -4.2352395 -4.19776 -4.1162992 -4.0012703 -3.9419091 -3.9888594 -4.0816097 -4.1470795 -4.1740847 -4.1933484 -4.2117791 -4.23212][-4.2578616 -4.2494164 -4.2492709 -4.2425032 -4.1955285 -4.0955544 -3.9564683 -3.8768218 -3.9303117 -4.0440431 -4.125916 -4.1680446 -4.200748 -4.2239289 -4.2436604][-4.2649422 -4.2631359 -4.2679877 -4.258574 -4.2085996 -4.1083236 -3.9745932 -3.8925524 -3.9463236 -4.0571971 -4.1353016 -4.1828017 -4.22492 -4.250977 -4.2653031][-4.2674389 -4.2710023 -4.2791958 -4.2731504 -4.2325888 -4.1470633 -4.0424414 -3.9783623 -4.019093 -4.1074162 -4.1665874 -4.2060637 -4.2430406 -4.2652464 -4.2725458][-4.2585788 -4.2658157 -4.2769923 -4.2787623 -4.2497244 -4.1832323 -4.1075888 -4.0647178 -4.0921178 -4.1537709 -4.1959877 -4.2251186 -4.2500997 -4.265574 -4.271234][-4.2516236 -4.2643166 -4.2781773 -4.2831197 -4.2617607 -4.2110262 -4.1556468 -4.1288877 -4.1503305 -4.1930523 -4.221858 -4.2375045 -4.2509055 -4.2616353 -4.2679396][-4.2535019 -4.2682929 -4.2798066 -4.2819939 -4.2626729 -4.2217374 -4.1826558 -4.1689734 -4.1873236 -4.2191963 -4.238101 -4.2433043 -4.2462597 -4.2523141 -4.2588348][-4.2532926 -4.269978 -4.2777996 -4.2739682 -4.2542267 -4.2198186 -4.1941009 -4.1894903 -4.2034764 -4.223618 -4.2385678 -4.2431383 -4.242589 -4.2437596 -4.247879][-4.2332392 -4.250977 -4.2588811 -4.2559 -4.2399373 -4.2168941 -4.2060738 -4.2065425 -4.209208 -4.2122145 -4.2233672 -4.2331271 -4.2358956 -4.2391477 -4.2402859]]...]
INFO - root - 2017-12-07 12:09:08.704018: step 6810, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 57h:58m:31s remains)
INFO - root - 2017-12-07 12:09:15.564006: step 6820, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 66h:24m:54s remains)
INFO - root - 2017-12-07 12:09:22.424282: step 6830, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 64h:02m:01s remains)
INFO - root - 2017-12-07 12:09:29.206075: step 6840, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 60h:30m:23s remains)
INFO - root - 2017-12-07 12:09:35.885042: step 6850, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 56h:50m:59s remains)
INFO - root - 2017-12-07 12:09:42.616412: step 6860, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 59h:05m:56s remains)
INFO - root - 2017-12-07 12:09:49.466957: step 6870, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 65h:41m:29s remains)
INFO - root - 2017-12-07 12:09:56.289528: step 6880, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 63h:40m:05s remains)
INFO - root - 2017-12-07 12:10:02.986553: step 6890, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 60h:34m:09s remains)
INFO - root - 2017-12-07 12:10:09.820432: step 6900, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 59h:17m:17s remains)
2017-12-07 12:10:10.524988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2687368 -4.2436271 -4.2233171 -4.2097869 -4.1980681 -4.1863556 -4.1764045 -4.1714821 -4.1885295 -4.2169347 -4.2389083 -4.2490554 -4.2491007 -4.2381368 -4.2119632][-4.2574458 -4.22883 -4.2077289 -4.1962495 -4.1888337 -4.1704512 -4.149013 -4.1393194 -4.1653876 -4.2046657 -4.2302713 -4.2383022 -4.2387609 -4.2282176 -4.1968794][-4.257278 -4.2240386 -4.2007875 -4.1937833 -4.1924667 -4.1708865 -4.1399655 -4.1210737 -4.1536961 -4.2022104 -4.2315621 -4.2390175 -4.2403641 -4.2318034 -4.1957412][-4.2588539 -4.2201324 -4.1957712 -4.1932974 -4.1978312 -4.1825757 -4.1487627 -4.1218452 -4.1505361 -4.2003431 -4.2307739 -4.2418747 -4.2456527 -4.2406325 -4.2029362][-4.2598977 -4.2216468 -4.1995487 -4.1974277 -4.2030783 -4.1900792 -4.1494575 -4.1087418 -4.1292338 -4.1792455 -4.2122893 -4.2301655 -4.2422886 -4.2434487 -4.2099748][-4.2652779 -4.232429 -4.20936 -4.1990108 -4.1949468 -4.1754065 -4.1188254 -4.0566497 -4.072854 -4.1398 -4.1846185 -4.2094927 -4.2296987 -4.2410445 -4.2158656][-4.2717223 -4.2410803 -4.2124 -4.1856236 -4.1593451 -4.118742 -4.0285935 -3.9265156 -3.957391 -4.0723748 -4.1501303 -4.1953645 -4.2310348 -4.2580056 -4.2476573][-4.2751064 -4.2455177 -4.2121372 -4.1741948 -4.1246223 -4.0437956 -3.8933403 -3.7458985 -3.8232505 -4.0001559 -4.1166868 -4.1861897 -4.2423239 -4.2805915 -4.27869][-4.2746921 -4.2461863 -4.2139211 -4.1823049 -4.1303172 -4.0333142 -3.8591497 -3.7160408 -3.8200383 -3.9971359 -4.1114664 -4.1856689 -4.24779 -4.2852426 -4.2847285][-4.2723827 -4.2415738 -4.2110209 -4.191854 -4.1512322 -4.0760827 -3.9602075 -3.8831327 -3.9538026 -4.0694942 -4.144783 -4.1959829 -4.2401338 -4.2673087 -4.2647605][-4.2757568 -4.2441969 -4.2143598 -4.1976323 -4.1622024 -4.1064 -4.0407071 -4.0080776 -4.0566988 -4.1275449 -4.1732788 -4.207377 -4.2302485 -4.2398639 -4.2309141][-4.285398 -4.2574377 -4.23264 -4.2180305 -4.1840305 -4.1362066 -4.0933285 -4.0747743 -4.1084371 -4.1543217 -4.1850567 -4.2110868 -4.2243419 -4.221869 -4.2075973][-4.2949438 -4.2740531 -4.2561407 -4.2471042 -4.223083 -4.1856813 -4.1551213 -4.1430264 -4.1687813 -4.1978669 -4.2150841 -4.2313442 -4.2362194 -4.228744 -4.2141681][-4.3013906 -4.2894917 -4.2794509 -4.2721863 -4.2571855 -4.2306623 -4.2056756 -4.2000546 -4.2292123 -4.2481389 -4.255197 -4.2602739 -4.2580948 -4.2503033 -4.2378187][-4.3069954 -4.3027453 -4.3016648 -4.2978144 -4.2860484 -4.2672257 -4.2465024 -4.2462382 -4.27365 -4.2874475 -4.2888155 -4.2920132 -4.2892213 -4.2835507 -4.2722864]]...]
INFO - root - 2017-12-07 12:10:17.413738: step 6910, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 67h:04m:52s remains)
INFO - root - 2017-12-07 12:10:24.154777: step 6920, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 62h:12m:57s remains)
INFO - root - 2017-12-07 12:10:30.894966: step 6930, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 55h:32m:57s remains)
INFO - root - 2017-12-07 12:10:37.675028: step 6940, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:46m:00s remains)
INFO - root - 2017-12-07 12:10:44.487006: step 6950, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 64h:56m:20s remains)
INFO - root - 2017-12-07 12:10:51.349896: step 6960, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 61h:45m:21s remains)
INFO - root - 2017-12-07 12:10:58.229953: step 6970, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 60h:13m:29s remains)
INFO - root - 2017-12-07 12:11:05.051137: step 6980, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:31m:28s remains)
INFO - root - 2017-12-07 12:11:11.897086: step 6990, loss = 2.04, batch loss = 1.99 (10.3 examples/sec; 0.774 sec/batch; 69h:57m:40s remains)
INFO - root - 2017-12-07 12:11:18.619797: step 7000, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 62h:27m:16s remains)
2017-12-07 12:11:19.310427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2095551 -4.2247939 -4.2392259 -4.2488947 -4.2546997 -4.26226 -4.2706981 -4.2746243 -4.2762237 -4.2794213 -4.28363 -4.2799344 -4.2707496 -4.2617087 -4.2535262][-4.2020359 -4.2109785 -4.2187386 -4.2232742 -4.2287498 -4.2404094 -4.2512231 -4.2556314 -4.25522 -4.2563248 -4.2606807 -4.2587814 -4.2508364 -4.2454686 -4.2442145][-4.2098584 -4.2122903 -4.2118187 -4.2084775 -4.2087913 -4.2178278 -4.22694 -4.228291 -4.22491 -4.2250295 -4.2314796 -4.2322745 -4.2259808 -4.2236414 -4.2299471][-4.2221084 -4.2202678 -4.2170157 -4.2079334 -4.2015195 -4.2037096 -4.2053413 -4.19965 -4.1946106 -4.20057 -4.2119102 -4.2127228 -4.2061853 -4.208334 -4.2225652][-4.2200356 -4.2141385 -4.2116089 -4.1993647 -4.1867628 -4.1799517 -4.1701193 -4.1541648 -4.1522679 -4.1717443 -4.1933775 -4.1969013 -4.19346 -4.2004871 -4.2226295][-4.2202945 -4.2084751 -4.2029786 -4.18829 -4.168622 -4.1477036 -4.1206474 -4.0927172 -4.1009316 -4.1412754 -4.1747103 -4.1828833 -4.1812015 -4.1908579 -4.2163043][-4.197257 -4.1755614 -4.1607318 -4.139883 -4.1123667 -4.0729904 -4.0273838 -3.99263 -4.0211482 -4.0820642 -4.1253266 -4.138423 -4.1414404 -4.1586022 -4.1908369][-4.1614504 -4.1335011 -4.1145797 -4.097333 -4.0714593 -4.0260768 -3.9780841 -3.951745 -3.9882083 -4.0536838 -4.0974984 -4.1084933 -4.1132083 -4.134233 -4.170126][-4.14942 -4.1247582 -4.1107154 -4.1009932 -4.0859761 -4.0509858 -4.0186849 -4.0107608 -4.0380716 -4.0842624 -4.1149282 -4.1188359 -4.1188111 -4.1352286 -4.16158][-4.1470861 -4.1338153 -4.128933 -4.1255641 -4.11736 -4.0913525 -4.0729108 -4.0787635 -4.0975771 -4.1251187 -4.1420407 -4.1391993 -4.1321187 -4.1389885 -4.1522937][-4.1506915 -4.1521277 -4.1552854 -4.157413 -4.1540418 -4.1351 -4.1274819 -4.1391244 -4.1524796 -4.16914 -4.1752858 -4.165256 -4.1512146 -4.1516633 -4.1548166][-4.1617827 -4.1696844 -4.1777406 -4.1840081 -4.185267 -4.1746545 -4.175961 -4.185771 -4.1925063 -4.2004843 -4.2010226 -4.1912994 -4.1782875 -4.1764274 -4.17639][-4.1772776 -4.1861596 -4.194541 -4.2024665 -4.2071886 -4.2050781 -4.2108364 -4.2178497 -4.2191596 -4.2211328 -4.2183809 -4.2111158 -4.2022839 -4.2013493 -4.2061081][-4.2100029 -4.2178097 -4.2281737 -4.2390943 -4.2455726 -4.2459912 -4.2486696 -4.250618 -4.2486253 -4.2481804 -4.248229 -4.2472568 -4.24524 -4.2481246 -4.2565346][-4.2494769 -4.2542191 -4.2625389 -4.2751718 -4.280211 -4.2798576 -4.2804289 -4.2811527 -4.2801495 -4.2804079 -4.28316 -4.2870126 -4.2908745 -4.2979817 -4.3081565]]...]
INFO - root - 2017-12-07 12:11:26.177977: step 7010, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 64h:55m:23s remains)
INFO - root - 2017-12-07 12:11:32.961630: step 7020, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 65h:39m:57s remains)
INFO - root - 2017-12-07 12:11:39.719908: step 7030, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 60h:39m:36s remains)
INFO - root - 2017-12-07 12:11:46.617001: step 7040, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 56h:46m:21s remains)
INFO - root - 2017-12-07 12:11:53.438929: step 7050, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:31m:38s remains)
INFO - root - 2017-12-07 12:12:00.114848: step 7060, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 61h:00m:04s remains)
INFO - root - 2017-12-07 12:12:06.928838: step 7070, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 59h:59m:14s remains)
INFO - root - 2017-12-07 12:12:13.685607: step 7080, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 56h:47m:01s remains)
INFO - root - 2017-12-07 12:12:20.284660: step 7090, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:05m:28s remains)
INFO - root - 2017-12-07 12:12:27.142103: step 7100, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.722 sec/batch; 65h:17m:40s remains)
2017-12-07 12:12:27.844888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1911488 -4.1794896 -4.1591587 -4.1376219 -4.127131 -4.1311603 -4.1462684 -4.1442771 -4.1267948 -4.13507 -4.1642227 -4.1790643 -4.17037 -4.1454411 -4.1362305][-4.185358 -4.1745462 -4.1542492 -4.1305923 -4.12117 -4.1344295 -4.16342 -4.1738234 -4.162075 -4.1620626 -4.176105 -4.1799722 -4.1641078 -4.1365261 -4.1298809][-4.17753 -4.1728806 -4.1538115 -4.1273341 -4.1170359 -4.1379175 -4.1786423 -4.2011008 -4.2017794 -4.201148 -4.2038956 -4.1969271 -4.175272 -4.1483917 -4.1419435][-4.1636934 -4.17032 -4.1570735 -4.1305962 -4.1155586 -4.132164 -4.1705503 -4.1949139 -4.2066722 -4.2186842 -4.2266421 -4.21347 -4.1877213 -4.160265 -4.1519265][-4.153266 -4.1681323 -4.1628342 -4.1414 -4.1217079 -4.1241007 -4.1385155 -4.1455011 -4.16264 -4.1983347 -4.2276125 -4.2200918 -4.1928387 -4.164391 -4.15329][-4.1621161 -4.1782241 -4.178195 -4.1596785 -4.1338711 -4.112896 -4.0820818 -4.0504208 -4.0660658 -4.1321783 -4.1884389 -4.1967731 -4.1764188 -4.1544023 -4.151166][-4.1902628 -4.2009425 -4.1988139 -4.1766257 -4.1412191 -4.0918927 -4.0130568 -3.9313424 -3.93706 -4.0345178 -4.1246171 -4.1571488 -4.1507325 -4.1429968 -4.1554193][-4.2271829 -4.22914 -4.2233863 -4.2011247 -4.1598926 -4.0930123 -3.9838684 -3.869288 -3.8586297 -3.9605536 -4.0687137 -4.1220236 -4.1325221 -4.1406293 -4.1672735][-4.2489367 -4.2490315 -4.241261 -4.2265668 -4.1933379 -4.1347413 -4.0434432 -3.9501524 -3.9283814 -3.983789 -4.0540061 -4.0955076 -4.1109171 -4.130641 -4.1693811][-4.2361288 -4.2418375 -4.2362137 -4.2300138 -4.2152581 -4.1816816 -4.1309395 -4.076941 -4.0552683 -4.0667181 -4.0811229 -4.0857487 -4.0879588 -4.1070476 -4.1457114][-4.1960611 -4.2053218 -4.2088909 -4.2114229 -4.2116265 -4.1986904 -4.1798191 -4.1600432 -4.1504741 -4.1473761 -4.1331959 -4.1091466 -4.0919342 -4.0932751 -4.118248][-4.1450939 -4.1588221 -4.1769314 -4.1869822 -4.1920609 -4.188046 -4.1870036 -4.189939 -4.2032089 -4.2074537 -4.1905012 -4.1583757 -4.1237078 -4.1059628 -4.1137843][-4.1003971 -4.11876 -4.1505723 -4.1681514 -4.1715112 -4.1692123 -4.1702204 -4.1821961 -4.2117167 -4.230763 -4.2217522 -4.1930141 -4.1571035 -4.1334634 -4.1315112][-4.0939956 -4.1226745 -4.1642427 -4.1811271 -4.1742592 -4.1664014 -4.1571865 -4.1608 -4.1903205 -4.2181416 -4.220664 -4.2023754 -4.17335 -4.1522756 -4.1466851][-4.1265388 -4.1571345 -4.1974521 -4.2107339 -4.1972494 -4.1796374 -4.15988 -4.1517839 -4.170269 -4.1911631 -4.1961203 -4.1868033 -4.1695514 -4.1558924 -4.1525064]]...]
INFO - root - 2017-12-07 12:12:34.578723: step 7110, loss = 2.11, batch loss = 2.05 (12.6 examples/sec; 0.635 sec/batch; 57h:25m:08s remains)
INFO - root - 2017-12-07 12:12:41.399039: step 7120, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.681 sec/batch; 61h:32m:54s remains)
INFO - root - 2017-12-07 12:12:48.279629: step 7130, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 65h:58m:00s remains)
INFO - root - 2017-12-07 12:12:55.075680: step 7140, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 60h:03m:04s remains)
INFO - root - 2017-12-07 12:13:01.916970: step 7150, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:38m:16s remains)
INFO - root - 2017-12-07 12:13:08.650146: step 7160, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:49m:56s remains)
INFO - root - 2017-12-07 12:13:15.422526: step 7170, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 65h:11m:52s remains)
INFO - root - 2017-12-07 12:13:22.228106: step 7180, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 65h:52m:17s remains)
INFO - root - 2017-12-07 12:13:28.858204: step 7190, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 62h:19m:32s remains)
INFO - root - 2017-12-07 12:13:35.613592: step 7200, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 57h:54m:30s remains)
2017-12-07 12:13:36.373894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2715068 -4.278317 -4.28182 -4.2786641 -4.2800322 -4.28371 -4.2731276 -4.2599106 -4.2598858 -4.2562709 -4.2451563 -4.2376356 -4.241487 -4.2560606 -4.2709241][-4.30338 -4.3055 -4.3000784 -4.2909985 -4.2882071 -4.2881432 -4.2732177 -4.2527289 -4.2466531 -4.2447605 -4.2393732 -4.2320242 -4.230185 -4.2378368 -4.24628][-4.3148065 -4.3170228 -4.30796 -4.2923374 -4.2808247 -4.2697067 -4.2454033 -4.21693 -4.211535 -4.224431 -4.2371182 -4.2400856 -4.2339716 -4.2326031 -4.2329812][-4.303834 -4.3122282 -4.3059793 -4.2834997 -4.2530923 -4.2216253 -4.1784658 -4.1436057 -4.1539264 -4.1987624 -4.2437687 -4.265161 -4.2639279 -4.2571859 -4.2509236][-4.2847786 -4.3002157 -4.2959213 -4.2574039 -4.1959791 -4.1272969 -4.0541015 -4.0088506 -4.0391974 -4.1274123 -4.2135916 -4.264492 -4.2844219 -4.2895074 -4.2839327][-4.2732096 -4.2931156 -4.2860451 -4.2266331 -4.1270571 -4.0129795 -3.8924863 -3.8186579 -3.8707638 -4.012382 -4.1459832 -4.2325783 -4.284173 -4.3103919 -4.3120012][-4.2690392 -4.2967844 -4.2899532 -4.217587 -4.0962648 -3.9511447 -3.7890506 -3.6722298 -3.7286868 -3.9123378 -4.0806823 -4.1905828 -4.2660842 -4.3078127 -4.3112569][-4.2553244 -4.2954254 -4.301362 -4.2406507 -4.1303272 -3.9995885 -3.8495038 -3.7280838 -3.754698 -3.9106672 -4.0656071 -4.1729822 -4.2508349 -4.291882 -4.2900791][-4.2441044 -4.2921147 -4.31209 -4.2741294 -4.1949844 -4.103405 -4.0029111 -3.9156218 -3.9158428 -4.0053124 -4.1102529 -4.1892548 -4.2469816 -4.2731576 -4.2635732][-4.2581234 -4.3021088 -4.3238015 -4.3003964 -4.2479525 -4.1906505 -4.1351986 -4.0865583 -4.07863 -4.1205659 -4.1783786 -4.22623 -4.2585435 -4.2641625 -4.2420621][-4.2929397 -4.3202353 -4.32955 -4.3077106 -4.2678666 -4.2334948 -4.2065926 -4.1872725 -4.1857982 -4.2101183 -4.2434716 -4.2697492 -4.2819576 -4.2711029 -4.2404704][-4.3085423 -4.3231325 -4.323041 -4.3048067 -4.2747784 -4.2504807 -4.2393656 -4.240181 -4.2504945 -4.26882 -4.289804 -4.3026366 -4.3006907 -4.27898 -4.2473264][-4.3080292 -4.3194361 -4.3157315 -4.3042531 -4.2878366 -4.2731714 -4.2682033 -4.2739892 -4.2843657 -4.2975483 -4.3111072 -4.31981 -4.3136954 -4.2896729 -4.26253][-4.3114157 -4.3186436 -4.3127775 -4.3058677 -4.2979136 -4.2874618 -4.2826967 -4.2839022 -4.290297 -4.3010135 -4.3139639 -4.3224854 -4.3186579 -4.2991176 -4.2778878][-4.3164573 -4.3174582 -4.3107991 -4.3056178 -4.3015418 -4.2966709 -4.2941666 -4.2935748 -4.2946672 -4.3018727 -4.3121257 -4.3198977 -4.3221779 -4.3144579 -4.3032074]]...]
INFO - root - 2017-12-07 12:13:43.217371: step 7210, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 66h:55m:00s remains)
INFO - root - 2017-12-07 12:13:49.972296: step 7220, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 59h:52m:18s remains)
INFO - root - 2017-12-07 12:13:56.738691: step 7230, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 55h:56m:02s remains)
INFO - root - 2017-12-07 12:14:03.659404: step 7240, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.684 sec/batch; 61h:50m:29s remains)
INFO - root - 2017-12-07 12:14:10.521276: step 7250, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 66h:33m:00s remains)
INFO - root - 2017-12-07 12:14:17.372853: step 7260, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 62h:07m:26s remains)
INFO - root - 2017-12-07 12:14:24.121178: step 7270, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 58h:16m:37s remains)
INFO - root - 2017-12-07 12:14:30.936087: step 7280, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 56h:40m:04s remains)
INFO - root - 2017-12-07 12:14:37.597567: step 7290, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 65h:46m:27s remains)
INFO - root - 2017-12-07 12:14:44.358298: step 7300, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 63h:48m:36s remains)
2017-12-07 12:14:45.003857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19898 -4.2072382 -4.2157559 -4.2175817 -4.2142458 -4.1972723 -4.1659451 -4.1517844 -4.1612682 -4.1867051 -4.2058983 -4.1966677 -4.1761379 -4.1689644 -4.1851106][-4.1902723 -4.1991596 -4.2107997 -4.2177229 -4.2135925 -4.1885114 -4.1526694 -4.1445122 -4.1636739 -4.1940117 -4.2071781 -4.1936693 -4.1682711 -4.1524563 -4.1604314][-4.17038 -4.181931 -4.1972241 -4.2098207 -4.2041168 -4.1764555 -4.1433363 -4.1409106 -4.1680965 -4.1944437 -4.2003427 -4.1799574 -4.1493177 -4.130929 -4.1345191][-4.1536016 -4.1688519 -4.1853147 -4.19855 -4.1868172 -4.1545544 -4.1292281 -4.1392288 -4.17653 -4.2024522 -4.1992354 -4.1666737 -4.1329618 -4.1161561 -4.118001][-4.1468978 -4.1702662 -4.1870184 -4.1938157 -4.1748 -4.1372786 -4.1137862 -4.1366239 -4.18585 -4.2155919 -4.201375 -4.1562262 -4.122467 -4.1115127 -4.1140676][-4.1552711 -4.1825328 -4.1960082 -4.1887112 -4.1633844 -4.1212158 -4.09445 -4.1159029 -4.174552 -4.2140903 -4.200027 -4.1560392 -4.128932 -4.1226068 -4.1240616][-4.1698275 -4.1964054 -4.2042484 -4.1847472 -4.1522098 -4.1062608 -4.0794792 -4.0931864 -4.1559987 -4.2049179 -4.1964078 -4.1659808 -4.151444 -4.155858 -4.1567106][-4.1647882 -4.1864738 -4.1926889 -4.169796 -4.1377568 -4.0992904 -4.0779786 -4.087707 -4.1435666 -4.1924992 -4.19113 -4.1757817 -4.1735139 -4.1812863 -4.1787043][-4.1713433 -4.1829596 -4.1855907 -4.1667089 -4.1425009 -4.1143694 -4.1022758 -4.1051435 -4.1431675 -4.182055 -4.1822681 -4.1793437 -4.1828022 -4.1836886 -4.1760464][-4.2003489 -4.2038493 -4.2026672 -4.1887922 -4.1699471 -4.1510572 -4.141427 -4.1360297 -4.1588058 -4.18531 -4.1817737 -4.1760497 -4.1777744 -4.17453 -4.1659517][-4.2273083 -4.226892 -4.2208414 -4.2058997 -4.1835365 -4.1667614 -4.1611762 -4.158093 -4.1773334 -4.1968813 -4.1920242 -4.1818328 -4.177949 -4.1726956 -4.1666055][-4.2321711 -4.2307448 -4.2242751 -4.2112455 -4.1864476 -4.1689982 -4.168468 -4.1758275 -4.1983314 -4.2136912 -4.2062144 -4.192328 -4.1786828 -4.1656671 -4.1613789][-4.2039814 -4.2148786 -4.2175622 -4.2095456 -4.18504 -4.1706734 -4.1712751 -4.18461 -4.207252 -4.2187152 -4.2103553 -4.1962471 -4.17747 -4.1615033 -4.1595259][-4.1635032 -4.1937709 -4.210608 -4.2097163 -4.1876721 -4.1684241 -4.1583033 -4.1669593 -4.1895838 -4.2016439 -4.1997404 -4.1902556 -4.173995 -4.1647043 -4.1719632][-4.1633906 -4.1997166 -4.22039 -4.2183008 -4.1905146 -4.1561189 -4.131968 -4.1337461 -4.1617813 -4.180768 -4.1827869 -4.1787214 -4.1715097 -4.1763196 -4.1918073]]...]
INFO - root - 2017-12-07 12:14:51.703579: step 7310, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 57h:03m:26s remains)
INFO - root - 2017-12-07 12:14:58.570820: step 7320, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 61h:04m:25s remains)
INFO - root - 2017-12-07 12:15:05.402116: step 7330, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.734 sec/batch; 66h:19m:42s remains)
INFO - root - 2017-12-07 12:15:12.229721: step 7340, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.687 sec/batch; 62h:00m:58s remains)
INFO - root - 2017-12-07 12:15:19.058441: step 7350, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 57h:06m:58s remains)
INFO - root - 2017-12-07 12:15:25.972600: step 7360, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:49m:35s remains)
INFO - root - 2017-12-07 12:15:32.674636: step 7370, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 65h:26m:26s remains)
INFO - root - 2017-12-07 12:15:39.530127: step 7380, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 58h:41m:41s remains)
INFO - root - 2017-12-07 12:15:46.267606: step 7390, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 57h:53m:58s remains)
INFO - root - 2017-12-07 12:15:53.095626: step 7400, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:15m:27s remains)
2017-12-07 12:15:53.870601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2575169 -4.2880964 -4.3033786 -4.2900734 -4.2593122 -4.2116838 -4.1553578 -4.0946422 -4.0631151 -4.0934324 -4.1454492 -4.1906667 -4.2181988 -4.2343116 -4.2406178][-4.2804146 -4.3035069 -4.3088255 -4.2921538 -4.2649055 -4.2251687 -4.1714692 -4.1074615 -4.0754251 -4.1021791 -4.1461577 -4.1822829 -4.2133627 -4.23652 -4.2504635][-4.2857242 -4.3033471 -4.3028932 -4.2881732 -4.272778 -4.24973 -4.2026243 -4.1343465 -4.0975022 -4.1129737 -4.1477704 -4.1772995 -4.2147989 -4.2482295 -4.2697768][-4.2936625 -4.301868 -4.2964234 -4.2826662 -4.2725277 -4.2585363 -4.207581 -4.1303234 -4.0865688 -4.1001019 -4.1374006 -4.174314 -4.2182617 -4.257843 -4.2843065][-4.3191128 -4.3192596 -4.3077621 -4.2862086 -4.2625728 -4.2295914 -4.1536617 -4.0575347 -4.0179558 -4.0542312 -4.1105766 -4.1661339 -4.2198033 -4.2651052 -4.2936487][-4.339016 -4.3332524 -4.318471 -4.2844505 -4.2329965 -4.1591697 -4.0378294 -3.9092703 -3.8965662 -3.9843078 -4.0759068 -4.1508889 -4.2162628 -4.2688789 -4.2998338][-4.3409848 -4.3355389 -4.3212194 -4.2793088 -4.20236 -4.0862861 -3.9101937 -3.741199 -3.7665477 -3.9179409 -4.0446744 -4.1375246 -4.2147861 -4.27463 -4.3065119][-4.3305817 -4.3257036 -4.3128152 -4.2704482 -4.1874056 -4.0618443 -3.8800881 -3.7101955 -3.7440503 -3.905926 -4.0388112 -4.1383462 -4.2206306 -4.2822766 -4.3123994][-4.3208895 -4.3176904 -4.3070703 -4.2738776 -4.2068391 -4.1146412 -3.9867523 -3.8640592 -3.8626564 -3.9626079 -4.063653 -4.1541009 -4.2347889 -4.291326 -4.3181524][-4.320837 -4.3194032 -4.3101006 -4.2863059 -4.2399039 -4.1829691 -4.1102242 -4.03113 -4.0068192 -4.0475593 -4.1104331 -4.185348 -4.2559018 -4.3032064 -4.3256793][-4.3278475 -4.3275757 -4.318377 -4.2987885 -4.2668471 -4.2307625 -4.1895761 -4.1378579 -4.1117854 -4.1287556 -4.171247 -4.2301955 -4.285409 -4.3205833 -4.3353043][-4.3345227 -4.3342118 -4.3230581 -4.3037724 -4.2797122 -4.2563248 -4.2281008 -4.1935477 -4.1753726 -4.1870856 -4.222229 -4.2703738 -4.3111191 -4.333662 -4.3394027][-4.3313112 -4.3330965 -4.3253422 -4.3096662 -4.2903419 -4.2709961 -4.2483377 -4.2231774 -4.2114387 -4.222795 -4.2555962 -4.2955642 -4.3236794 -4.33583 -4.3357706][-4.3180709 -4.3246489 -4.3218713 -4.3123851 -4.2974529 -4.280642 -4.260994 -4.2429571 -4.2375994 -4.2496934 -4.27669 -4.3060951 -4.3233514 -4.3286481 -4.3267722][-4.3027449 -4.3149829 -4.3178515 -4.3131504 -4.302043 -4.2889695 -4.2738385 -4.2617416 -4.2600794 -4.2705626 -4.2895379 -4.3077083 -4.3164191 -4.3178549 -4.3155155]]...]
INFO - root - 2017-12-07 12:16:00.651919: step 7410, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:42m:19s remains)
INFO - root - 2017-12-07 12:16:07.444806: step 7420, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:17m:11s remains)
INFO - root - 2017-12-07 12:16:14.401354: step 7430, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 59h:22m:36s remains)
INFO - root - 2017-12-07 12:16:21.258164: step 7440, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 66h:23m:07s remains)
INFO - root - 2017-12-07 12:16:28.045103: step 7450, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 62h:39m:51s remains)
INFO - root - 2017-12-07 12:16:34.853098: step 7460, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 62h:42m:08s remains)
INFO - root - 2017-12-07 12:16:41.654492: step 7470, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 57h:15m:47s remains)
INFO - root - 2017-12-07 12:16:48.492230: step 7480, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 60h:29m:08s remains)
INFO - root - 2017-12-07 12:16:55.169070: step 7490, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 65h:45m:30s remains)
INFO - root - 2017-12-07 12:17:01.977059: step 7500, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 63h:25m:19s remains)
2017-12-07 12:17:02.725207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2157116 -4.2033563 -4.1839252 -4.1560531 -4.1337991 -4.1254463 -4.15488 -4.2124119 -4.259964 -4.2876558 -4.2992878 -4.2947278 -4.2752419 -4.2440662 -4.2047548][-4.1784692 -4.1706281 -4.15154 -4.1159582 -4.0844707 -4.0755243 -4.1190209 -4.1938887 -4.2512631 -4.2808146 -4.292654 -4.2907438 -4.2743464 -4.248168 -4.2130828][-4.1459293 -4.149231 -4.13815 -4.0989389 -4.0612984 -4.0488644 -4.0908709 -4.1750178 -4.2362676 -4.2637525 -4.2710094 -4.2711334 -4.2638726 -4.249248 -4.2218571][-4.1404881 -4.1537528 -4.1523385 -4.1167583 -4.075995 -4.0568042 -4.0806479 -4.15455 -4.2121029 -4.2361803 -4.2411952 -4.2411032 -4.2405529 -4.2389841 -4.2178426][-4.1690021 -4.1826029 -4.1841421 -4.15 -4.1069036 -4.0762811 -4.0716057 -4.1206903 -4.1719584 -4.1964822 -4.2043467 -4.2083406 -4.21835 -4.2279043 -4.2171278][-4.1918874 -4.2001376 -4.1982493 -4.1657248 -4.1191692 -4.0738969 -4.045804 -4.0758357 -4.1278358 -4.1644773 -4.184247 -4.1942496 -4.2079082 -4.2213244 -4.2168646][-4.1749229 -4.1797314 -4.1748266 -4.139986 -4.0877838 -4.0305367 -3.9902523 -4.0207038 -4.08958 -4.1450253 -4.1721025 -4.1835866 -4.1957464 -4.2066441 -4.2067461][-4.1222878 -4.1257133 -4.1268854 -4.0992465 -4.0493913 -3.9790349 -3.9351401 -3.9811728 -4.0719194 -4.142313 -4.173295 -4.1839781 -4.1910124 -4.1958933 -4.1924858][-4.0858326 -4.1003661 -4.1162009 -4.0969887 -4.047204 -3.9707303 -3.9363732 -3.9940367 -4.0858941 -4.1577873 -4.18889 -4.1926847 -4.1918912 -4.1911135 -4.1830997][-4.1029387 -4.1245155 -4.1524711 -4.1409912 -4.0944238 -4.0318642 -4.0123072 -4.0607667 -4.1307297 -4.1847391 -4.2044086 -4.2005444 -4.1969614 -4.195591 -4.1876168][-4.1516776 -4.168757 -4.1889148 -4.1768885 -4.1447835 -4.1082997 -4.1021867 -4.1363473 -4.17946 -4.2064462 -4.2098918 -4.1965871 -4.192534 -4.1971636 -4.1977954][-4.2079239 -4.2153153 -4.2224317 -4.2085447 -4.1871862 -4.1673789 -4.1690626 -4.1949492 -4.2234616 -4.2309561 -4.2236257 -4.2111363 -4.2095017 -4.2216091 -4.2286863][-4.2621369 -4.2638187 -4.2616262 -4.2467895 -4.2289457 -4.2154336 -4.2190409 -4.2353673 -4.2591281 -4.2614236 -4.2524042 -4.2431602 -4.2395806 -4.2514577 -4.2568283][-4.2977185 -4.2990985 -4.297595 -4.2838216 -4.2674541 -4.2561679 -4.2557411 -4.261426 -4.2818966 -4.29174 -4.2878308 -4.276546 -4.2704735 -4.2795982 -4.2844291][-4.3170772 -4.31623 -4.3148336 -4.3023067 -4.2893548 -4.2812662 -4.2769828 -4.2766314 -4.2948852 -4.3089995 -4.3057861 -4.294529 -4.2907887 -4.2997375 -4.3047028]]...]
INFO - root - 2017-12-07 12:17:09.646559: step 7510, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 67h:25m:34s remains)
INFO - root - 2017-12-07 12:17:16.475235: step 7520, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 66h:49m:40s remains)
INFO - root - 2017-12-07 12:17:23.310494: step 7530, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 58h:32m:42s remains)
INFO - root - 2017-12-07 12:17:30.008489: step 7540, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 56h:35m:58s remains)
INFO - root - 2017-12-07 12:17:36.930671: step 7550, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 61h:32m:16s remains)
INFO - root - 2017-12-07 12:17:43.713543: step 7560, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 64h:18m:28s remains)
INFO - root - 2017-12-07 12:17:50.441162: step 7570, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 58h:13m:03s remains)
INFO - root - 2017-12-07 12:17:57.286501: step 7580, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 60h:05m:27s remains)
INFO - root - 2017-12-07 12:18:03.764371: step 7590, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 56h:07m:15s remains)
INFO - root - 2017-12-07 12:18:10.559474: step 7600, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 62h:32m:11s remains)
2017-12-07 12:18:11.302710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3313956 -4.3108144 -4.2737031 -4.2334423 -4.1950026 -4.1665783 -4.1840429 -4.232523 -4.2653122 -4.2784839 -4.290565 -4.301168 -4.2953882 -4.2785153 -4.2496686][-4.3398924 -4.3164449 -4.2711277 -4.2216043 -4.1783581 -4.1486592 -4.1669436 -4.2152719 -4.2424932 -4.2523093 -4.2686915 -4.2883525 -4.2854366 -4.2652831 -4.2276397][-4.3477588 -4.3199339 -4.2651749 -4.2052035 -4.1530614 -4.1183896 -4.1308026 -4.1746855 -4.1988893 -4.2090163 -4.234345 -4.2694759 -4.2757325 -4.2573328 -4.2160678][-4.3583093 -4.3315644 -4.2751017 -4.2100177 -4.1453543 -4.09301 -4.0860996 -4.1204772 -4.1413426 -4.1490173 -4.1827826 -4.235218 -4.263166 -4.2610836 -4.2250485][-4.3638029 -4.3430572 -4.293539 -4.2355175 -4.1714635 -4.1050072 -4.07544 -4.0977683 -4.1069088 -4.101881 -4.1354122 -4.1977291 -4.2431345 -4.2574725 -4.233151][-4.3581648 -4.342021 -4.3008146 -4.2573566 -4.2024369 -4.1295185 -4.0878544 -4.1051536 -4.1030283 -4.0811987 -4.1011086 -4.1580133 -4.2087488 -4.2404838 -4.2424307][-4.3484979 -4.3343434 -4.299 -4.2648544 -4.2053018 -4.1220527 -4.0809627 -4.1038213 -4.1005311 -4.062 -4.0551987 -4.0990462 -4.1567349 -4.2092113 -4.2417669][-4.3384776 -4.3230271 -4.2882762 -4.2516756 -4.1761174 -4.0710168 -4.0286746 -4.0593729 -4.0602231 -4.0016432 -3.9703248 -4.0176506 -4.0990176 -4.1754532 -4.2286897][-4.3339839 -4.3185883 -4.284451 -4.2384176 -4.1468711 -4.0261397 -3.9822981 -4.0200844 -4.0231409 -3.9510627 -3.912925 -3.9751639 -4.0745811 -4.1621342 -4.2208915][-4.3388686 -4.325233 -4.293539 -4.2469869 -4.1601129 -4.0443745 -4.0026174 -4.0520277 -4.06365 -3.9954073 -3.9583106 -4.0122185 -4.1007566 -4.1791062 -4.2296453][-4.3431759 -4.3365545 -4.3122029 -4.2724586 -4.2035217 -4.1139393 -4.0866418 -4.1378908 -4.1532717 -4.1032853 -4.0714216 -4.0968242 -4.1553288 -4.2143035 -4.2498007][-4.3500905 -4.3498 -4.3311143 -4.2980185 -4.2507215 -4.193727 -4.1819968 -4.2257757 -4.2432346 -4.2147493 -4.1946983 -4.1999488 -4.2275829 -4.26254 -4.2797022][-4.3539529 -4.357 -4.3411956 -4.310822 -4.2749853 -4.2380533 -4.2358356 -4.2750535 -4.2968941 -4.2866316 -4.2774568 -4.2775226 -4.2877808 -4.3033776 -4.3094239][-4.3515887 -4.3552704 -4.3425274 -4.3139782 -4.2805696 -4.2498922 -4.252646 -4.2910981 -4.315691 -4.3144693 -4.3112292 -4.3116965 -4.3167396 -4.325408 -4.3287768][-4.3467155 -4.3478837 -4.334044 -4.3041749 -4.2691693 -4.24095 -4.2460055 -4.28303 -4.3124413 -4.3199973 -4.3212104 -4.3223648 -4.3262362 -4.3319983 -4.3334508]]...]
INFO - root - 2017-12-07 12:18:18.057372: step 7610, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 56h:34m:30s remains)
INFO - root - 2017-12-07 12:18:24.812644: step 7620, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 56h:32m:19s remains)
INFO - root - 2017-12-07 12:18:31.610847: step 7630, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 61h:27m:58s remains)
INFO - root - 2017-12-07 12:18:38.468436: step 7640, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 62h:21m:10s remains)
INFO - root - 2017-12-07 12:18:45.184559: step 7650, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:19m:17s remains)
INFO - root - 2017-12-07 12:18:51.965813: step 7660, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 59h:18m:52s remains)
INFO - root - 2017-12-07 12:18:58.743112: step 7670, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 58h:37m:32s remains)
INFO - root - 2017-12-07 12:19:05.470131: step 7680, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 66h:04m:59s remains)
INFO - root - 2017-12-07 12:19:12.146150: step 7690, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 60h:45m:53s remains)
INFO - root - 2017-12-07 12:19:18.875217: step 7700, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 57h:39m:13s remains)
2017-12-07 12:19:19.631405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2377181 -4.2517672 -4.2474647 -4.22953 -4.209754 -4.1993375 -4.2156134 -4.2483597 -4.2796288 -4.3025422 -4.3167586 -4.323163 -4.3133416 -4.292942 -4.2756472][-4.1781831 -4.2033124 -4.2032194 -4.1836953 -4.168067 -4.1610465 -4.1775322 -4.2124672 -4.2445812 -4.2640967 -4.2723708 -4.276691 -4.2689986 -4.2489777 -4.2320709][-4.1226392 -4.1556826 -4.1612334 -4.1412139 -4.1341553 -4.1355853 -4.1502519 -4.1808391 -4.207056 -4.2208939 -4.2241063 -4.229702 -4.2293305 -4.2168345 -4.2081289][-4.1079082 -4.1392326 -4.1480684 -4.1341028 -4.1368694 -4.1422129 -4.1479607 -4.1633677 -4.1769772 -4.1855755 -4.18541 -4.1920519 -4.2012496 -4.2028213 -4.2086477][-4.1609244 -4.1797462 -4.1814823 -4.1728315 -4.1747618 -4.1674137 -4.154892 -4.14891 -4.1536942 -4.1610765 -4.1598549 -4.1638374 -4.178472 -4.189867 -4.2086825][-4.2207212 -4.2194014 -4.2050705 -4.1896372 -4.1777997 -4.1461344 -4.1124511 -4.0936856 -4.0979614 -4.1074481 -4.1112804 -4.1177034 -4.1375704 -4.1560769 -4.1868334][-4.2405872 -4.2249269 -4.1965914 -4.1663365 -4.1290164 -4.0702858 -4.0268197 -4.0113578 -4.0224581 -4.036521 -4.0487351 -4.0663147 -4.0926337 -4.1206241 -4.1619544][-4.2445049 -4.2242131 -4.1849856 -4.1372261 -4.0730863 -3.9986858 -3.9604306 -3.9600198 -3.9818861 -4.0031233 -4.0248694 -4.0466962 -4.0715208 -4.111053 -4.1653562][-4.2492833 -4.2303019 -4.1881151 -4.1354327 -4.0678387 -4.0011239 -3.9711118 -3.9758282 -3.9986172 -4.0282626 -4.0592823 -4.0816231 -4.10457 -4.1492987 -4.2048507][-4.2525058 -4.2380681 -4.2040877 -4.1661029 -4.1185551 -4.0755763 -4.05807 -4.0621948 -4.0826025 -4.1149244 -4.1485181 -4.1697392 -4.1898603 -4.2243867 -4.2627869][-4.2482386 -4.2390938 -4.2171783 -4.20025 -4.1825161 -4.1663504 -4.1648426 -4.1769819 -4.1992788 -4.2243876 -4.24361 -4.2563891 -4.2681317 -4.2869658 -4.30677][-4.2266297 -4.2269111 -4.2247977 -4.2361951 -4.2458758 -4.2489843 -4.2573538 -4.2725821 -4.2911015 -4.30429 -4.308805 -4.3110557 -4.315156 -4.32478 -4.3331909][-4.18561 -4.19691 -4.21944 -4.2552242 -4.2850938 -4.3007913 -4.3100758 -4.3190813 -4.3284249 -4.334589 -4.3344836 -4.3344784 -4.3368235 -4.342082 -4.3466773][-4.13916 -4.1583662 -4.198812 -4.2508831 -4.2924972 -4.3121843 -4.3187294 -4.3211432 -4.3260016 -4.330544 -4.33168 -4.3328648 -4.3364873 -4.3407378 -4.3437476][-4.1150265 -4.1349111 -4.1768994 -4.2295871 -4.2728243 -4.2950187 -4.3035889 -4.3084588 -4.3136482 -4.3189774 -4.3222485 -4.3248739 -4.3277345 -4.329742 -4.3314075]]...]
INFO - root - 2017-12-07 12:19:26.442461: step 7710, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 63h:42m:41s remains)
INFO - root - 2017-12-07 12:19:33.215838: step 7720, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:28m:50s remains)
INFO - root - 2017-12-07 12:19:39.952631: step 7730, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:15m:44s remains)
INFO - root - 2017-12-07 12:19:46.707731: step 7740, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 59h:33m:11s remains)
INFO - root - 2017-12-07 12:19:53.516880: step 7750, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 63h:11m:57s remains)
INFO - root - 2017-12-07 12:20:00.422048: step 7760, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.694 sec/batch; 62h:38m:27s remains)
INFO - root - 2017-12-07 12:20:07.200565: step 7770, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 60h:04m:03s remains)
INFO - root - 2017-12-07 12:20:13.996582: step 7780, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:42m:02s remains)
INFO - root - 2017-12-07 12:20:20.640714: step 7790, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 58h:38m:57s remains)
INFO - root - 2017-12-07 12:20:27.497595: step 7800, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 65h:01m:11s remains)
2017-12-07 12:20:28.220770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.247241 -4.2329664 -4.2332234 -4.2293043 -4.2230139 -4.2238679 -4.2258573 -4.2233624 -4.2223415 -4.22409 -4.2267323 -4.2336326 -4.2488184 -4.2620983 -4.2676435][-4.2114234 -4.192924 -4.1990137 -4.2024717 -4.199213 -4.2004285 -4.1981015 -4.1926022 -4.1937613 -4.1981063 -4.2025285 -4.2107577 -4.2292161 -4.2459784 -4.2539735][-4.1901422 -4.1702209 -4.1808486 -4.1869955 -4.179245 -4.1708655 -4.155952 -4.1427412 -4.1516008 -4.1685719 -4.1796517 -4.1914382 -4.21036 -4.2265797 -4.2379427][-4.1859117 -4.1646228 -4.1705475 -4.1754327 -4.1625676 -4.1441107 -4.1168122 -4.0957737 -4.1170115 -4.1494379 -4.168004 -4.1842341 -4.2021074 -4.2165136 -4.2317929][-4.17707 -4.1545906 -4.1550131 -4.1565361 -4.1359973 -4.1027532 -4.0581288 -4.0206213 -4.0570436 -4.1165848 -4.1505837 -4.1755643 -4.1968808 -4.2142115 -4.2355175][-4.1643572 -4.1423674 -4.1366835 -4.1304183 -4.0916839 -4.0334296 -3.9528608 -3.8757532 -3.9260619 -4.0301156 -4.0971193 -4.1422844 -4.1738319 -4.1984248 -4.2248936][-4.1670938 -4.1570039 -4.1521564 -4.1325064 -4.0721245 -3.9853449 -3.8635101 -3.7369075 -3.7943127 -3.9436908 -4.0417509 -4.1027894 -4.14013 -4.1677794 -4.1957207][-4.1978731 -4.2034435 -4.2026076 -4.1790504 -4.1187239 -4.0386295 -3.9314413 -3.825876 -3.8677335 -3.9951446 -4.0803757 -4.1289706 -4.15665 -4.1757617 -4.1939564][-4.2330494 -4.250041 -4.2538786 -4.2364893 -4.1929817 -4.1343613 -4.0640788 -3.9984274 -4.0179358 -4.1064615 -4.170167 -4.1999063 -4.2163053 -4.2244086 -4.2305436][-4.2597609 -4.2787013 -4.2845125 -4.2704344 -4.2389107 -4.1949534 -4.1436949 -4.0996828 -4.1075139 -4.1702032 -4.221159 -4.2472034 -4.2641525 -4.2710958 -4.2717805][-4.2669654 -4.2890587 -4.3005986 -4.2923093 -4.2671528 -4.2312918 -4.1878672 -4.1552482 -4.1566591 -4.1985388 -4.2365451 -4.2569242 -4.2732034 -4.2814369 -4.2821808][-4.2666578 -4.2919035 -4.3073568 -4.3008256 -4.2778897 -4.2478275 -4.2174468 -4.2028871 -4.2101793 -4.2373033 -4.2597308 -4.2692161 -4.2754259 -4.2780819 -4.2753687][-4.2579451 -4.2803669 -4.2984753 -4.2971706 -4.2797003 -4.2596383 -4.2458487 -4.2487626 -4.2606878 -4.2766819 -4.2890635 -4.2928319 -4.2926216 -4.28981 -4.283834][-4.2537985 -4.2728605 -4.2896018 -4.2909594 -4.2799835 -4.2704148 -4.268857 -4.2777896 -4.2868786 -4.2948041 -4.3009577 -4.3040485 -4.3053956 -4.3040547 -4.3000011][-4.2565651 -4.2775512 -4.2938347 -4.3008208 -4.2970767 -4.2915363 -4.2910643 -4.2970972 -4.3020387 -4.3041472 -4.3058519 -4.3080959 -4.3105221 -4.3124137 -4.312634]]...]
INFO - root - 2017-12-07 12:20:35.024941: step 7810, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 59h:51m:55s remains)
INFO - root - 2017-12-07 12:20:41.871929: step 7820, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.724 sec/batch; 65h:15m:52s remains)
INFO - root - 2017-12-07 12:20:48.684006: step 7830, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 66h:55m:33s remains)
INFO - root - 2017-12-07 12:20:55.433201: step 7840, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 64h:43m:01s remains)
INFO - root - 2017-12-07 12:21:02.219747: step 7850, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 57h:50m:59s remains)
INFO - root - 2017-12-07 12:21:09.007137: step 7860, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.661 sec/batch; 59h:36m:18s remains)
INFO - root - 2017-12-07 12:21:15.905671: step 7870, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:30m:35s remains)
INFO - root - 2017-12-07 12:21:22.750397: step 7880, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 66h:05m:02s remains)
INFO - root - 2017-12-07 12:21:29.391071: step 7890, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 61h:20m:41s remains)
INFO - root - 2017-12-07 12:21:36.200362: step 7900, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 60h:37m:18s remains)
2017-12-07 12:21:36.864179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2660723 -4.24864 -4.2418633 -4.2428837 -4.2489076 -4.2563739 -4.26352 -4.2674942 -4.2687821 -4.2734179 -4.2804031 -4.2859755 -4.2858472 -4.2819042 -4.277926][-4.2280416 -4.2027164 -4.1968155 -4.202764 -4.2121816 -4.2190576 -4.2256045 -4.2300525 -4.2333312 -4.2421746 -4.2548537 -4.2647405 -4.2665172 -4.2603531 -4.2516122][-4.1996059 -4.169879 -4.1690259 -4.1821017 -4.19487 -4.1987586 -4.201436 -4.203608 -4.20671 -4.2179112 -4.2344732 -4.2474356 -4.2498336 -4.2405815 -4.2272458][-4.1866288 -4.1596055 -4.16782 -4.1880336 -4.2009659 -4.1971669 -4.1891961 -4.1833215 -4.1826763 -4.1948972 -4.2160668 -4.2337575 -4.2365479 -4.2243419 -4.2104826][-4.1834874 -4.1637535 -4.1773787 -4.1982145 -4.2096534 -4.2007265 -4.1848364 -4.1695676 -4.1609936 -4.1679082 -4.1907616 -4.2128959 -4.2167397 -4.2070646 -4.2011204][-4.1874666 -4.1736107 -4.1865826 -4.202354 -4.21123 -4.202158 -4.1851459 -4.1656742 -4.1500983 -4.1483536 -4.168189 -4.1897221 -4.195704 -4.1929259 -4.199173][-4.1914883 -4.1833224 -4.196002 -4.2057447 -4.208107 -4.1988888 -4.1829953 -4.1614609 -4.1421566 -4.133112 -4.1475687 -4.165669 -4.1738429 -4.1791987 -4.1959414][-4.1861124 -4.1792731 -4.1911716 -4.1974998 -4.1965108 -4.1898279 -4.1779785 -4.1624417 -4.1441746 -4.1315746 -4.139761 -4.1521106 -4.16002 -4.1686363 -4.187613][-4.1730423 -4.1643848 -4.1776114 -4.185112 -4.1870432 -4.184299 -4.1775961 -4.1691856 -4.1523671 -4.1353011 -4.139308 -4.1463737 -4.1493373 -4.15617 -4.1729302][-4.1548777 -4.1464529 -4.1639519 -4.1766491 -4.1860065 -4.1883168 -4.1852579 -4.1797066 -4.161685 -4.1414371 -4.1398335 -4.1411419 -4.1353374 -4.1365528 -4.1505351][-4.1378317 -4.1302562 -4.153317 -4.1726413 -4.1889462 -4.1935 -4.1920142 -4.1859188 -4.1673088 -4.1463432 -4.1396756 -4.1368256 -4.1204052 -4.1121721 -4.1239767][-4.1304688 -4.1225233 -4.1463432 -4.1687346 -4.1869121 -4.1921892 -4.1897745 -4.1804833 -4.1608257 -4.1418409 -4.1329584 -4.1264777 -4.1056013 -4.0947504 -4.108037][-4.1265936 -4.1181879 -4.1410875 -4.1659918 -4.1866651 -4.1948762 -4.1922312 -4.1791282 -4.1582255 -4.1426945 -4.1344843 -4.12855 -4.1127038 -4.1062808 -4.1197271][-4.13843 -4.1308064 -4.1489081 -4.1714826 -4.1919613 -4.2035985 -4.1996436 -4.1829429 -4.16177 -4.1507087 -4.146975 -4.145124 -4.1381803 -4.1388321 -4.1513772][-4.1683226 -4.1644492 -4.1779976 -4.1949124 -4.2106628 -4.2225833 -4.2203813 -4.2036452 -4.1846485 -4.1754656 -4.1761236 -4.1771989 -4.1734014 -4.1748853 -4.1851196]]...]
INFO - root - 2017-12-07 12:21:43.683466: step 7910, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.757 sec/batch; 68h:15m:53s remains)
INFO - root - 2017-12-07 12:21:50.396098: step 7920, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:53m:58s remains)
INFO - root - 2017-12-07 12:21:57.241566: step 7930, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 59h:58m:04s remains)
INFO - root - 2017-12-07 12:22:04.042290: step 7940, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:04m:47s remains)
INFO - root - 2017-12-07 12:22:10.862191: step 7950, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 64h:51m:57s remains)
INFO - root - 2017-12-07 12:22:17.702555: step 7960, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.760 sec/batch; 68h:30m:36s remains)
INFO - root - 2017-12-07 12:22:24.464516: step 7970, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 59h:37m:21s remains)
INFO - root - 2017-12-07 12:22:31.196858: step 7980, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 58h:05m:41s remains)
INFO - root - 2017-12-07 12:22:37.660056: step 7990, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 58h:49m:25s remains)
INFO - root - 2017-12-07 12:22:44.381557: step 8000, loss = 2.05, batch loss = 2.00 (10.5 examples/sec; 0.762 sec/batch; 68h:40m:34s remains)
2017-12-07 12:22:45.086861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1719356 -4.1709261 -4.17216 -4.1779089 -4.1825666 -4.1872621 -4.1964645 -4.1993427 -4.1932111 -4.1851435 -4.1894436 -4.1959767 -4.194037 -4.1920385 -4.1884727][-4.1800618 -4.1809554 -4.1817989 -4.1860929 -4.1888938 -4.1900096 -4.1937284 -4.1912785 -4.17794 -4.1637712 -4.1658664 -4.1728806 -4.1703944 -4.1621065 -4.1492872][-4.2095013 -4.209846 -4.21056 -4.2124581 -4.2108841 -4.2036505 -4.1974282 -4.1873908 -4.1686554 -4.1502314 -4.146121 -4.1517186 -4.1471043 -4.1268473 -4.0980835][-4.2372541 -4.2293758 -4.2222552 -4.2172909 -4.2080469 -4.1890254 -4.1689754 -4.1484146 -4.1261983 -4.1069088 -4.1003985 -4.1057076 -4.1004176 -4.07122 -4.0295839][-4.2308321 -4.2145247 -4.1964664 -4.1791592 -4.1601248 -4.125721 -4.0870304 -4.055223 -4.0338778 -4.0243864 -4.0324979 -4.0503011 -4.0496912 -4.0224013 -3.9832323][-4.1780262 -4.157022 -4.12998 -4.10185 -4.0742941 -4.0298057 -3.9810076 -3.9483306 -3.9382119 -3.9512923 -3.985913 -4.0215473 -4.0313935 -4.0183911 -3.9975293][-4.1051469 -4.090992 -4.066546 -4.0387096 -4.012712 -3.9716718 -3.9290392 -3.911643 -3.9185798 -3.9435825 -3.9854095 -4.0222888 -4.0343323 -4.02901 -4.0214143][-4.0708 -4.072032 -4.0644298 -4.0514684 -4.0352921 -4.005609 -3.9736669 -3.965816 -3.9740288 -3.9842875 -4.0043797 -4.0259933 -4.0337262 -4.0322556 -4.0331764][-4.0871859 -4.0985885 -4.1055012 -4.1087766 -4.1018162 -4.0827708 -4.0569386 -4.0468802 -4.0464816 -4.0393786 -4.0348053 -4.0387392 -4.0377388 -4.0411711 -4.0524197][-4.1223068 -4.1352258 -4.1488056 -4.1620579 -4.1661153 -4.1578994 -4.1376538 -4.1207914 -4.1098652 -4.0941153 -4.0792856 -4.0692682 -4.05781 -4.062243 -4.0803022][-4.1631379 -4.1703906 -4.1854625 -4.2019887 -4.2131448 -4.2125359 -4.1971231 -4.1746855 -4.1525517 -4.13002 -4.1118612 -4.0934358 -4.0780525 -4.0818887 -4.0973325][-4.2031465 -4.2049561 -4.2165861 -4.229598 -4.2394962 -4.2397013 -4.2257943 -4.2007575 -4.1720457 -4.1452746 -4.1265469 -4.1067352 -4.0911384 -4.0912838 -4.0991669][-4.2359433 -4.2314367 -4.2359905 -4.2406831 -4.2454767 -4.2429137 -4.2303033 -4.207294 -4.1786566 -4.1521635 -4.1375747 -4.12416 -4.1115894 -4.1069684 -4.1089954][-4.2546239 -4.2402163 -4.2354383 -4.2333627 -4.2344379 -4.2328067 -4.2264705 -4.212616 -4.1935124 -4.173058 -4.1615663 -4.1536927 -4.142827 -4.1316118 -4.1267123][-4.2629547 -4.2445912 -4.2348504 -4.2285595 -4.2266784 -4.2267623 -4.2291045 -4.2278218 -4.2206163 -4.2080097 -4.1969371 -4.1876636 -4.1745777 -4.1572008 -4.1439605]]...]
INFO - root - 2017-12-07 12:22:51.808007: step 8010, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 56h:01m:55s remains)
INFO - root - 2017-12-07 12:22:58.605363: step 8020, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 59h:14m:55s remains)
INFO - root - 2017-12-07 12:23:05.457018: step 8030, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 66h:25m:03s remains)
INFO - root - 2017-12-07 12:23:12.287679: step 8040, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 60h:52m:40s remains)
INFO - root - 2017-12-07 12:23:19.085281: step 8050, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 59h:43m:00s remains)
INFO - root - 2017-12-07 12:23:25.942185: step 8060, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 57h:21m:15s remains)
INFO - root - 2017-12-07 12:23:32.777892: step 8070, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 63h:22m:41s remains)
INFO - root - 2017-12-07 12:23:39.548875: step 8080, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 64h:24m:32s remains)
INFO - root - 2017-12-07 12:23:46.282713: step 8090, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 61h:29m:56s remains)
INFO - root - 2017-12-07 12:23:53.155017: step 8100, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 60h:53m:36s remains)
2017-12-07 12:23:53.970043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2243371 -4.1875992 -4.15361 -4.1462083 -4.1642289 -4.18704 -4.1970782 -4.1898422 -4.181139 -4.1743212 -4.166563 -4.16859 -4.1812897 -4.2067089 -4.2356682][-4.2197437 -4.1802187 -4.1422424 -4.1289587 -4.1419458 -4.1597986 -4.1644926 -4.1486197 -4.1401 -4.1440578 -4.1476841 -4.1571035 -4.1707249 -4.19498 -4.2246256][-4.2075315 -4.1732297 -4.1370268 -4.1166205 -4.1195831 -4.1317263 -4.133534 -4.1139636 -4.1098318 -4.1274738 -4.1433468 -4.1602368 -4.1722045 -4.1893687 -4.2146][-4.1881232 -4.1659107 -4.1381326 -4.1156878 -4.1068325 -4.11015 -4.1076636 -4.089448 -4.09268 -4.1212831 -4.1463041 -4.1710696 -4.1826177 -4.1911583 -4.2096138][-4.1702495 -4.1652675 -4.1489129 -4.127121 -4.1071038 -4.094985 -4.0795622 -4.0596528 -4.0715461 -4.11252 -4.146668 -4.1796556 -4.1930537 -4.195322 -4.2076564][-4.1533175 -4.1619711 -4.1562438 -4.1363354 -4.1089764 -4.07797 -4.0406833 -4.0079966 -4.0260282 -4.0859828 -4.1355724 -4.1780076 -4.195766 -4.1979113 -4.2088985][-4.1443353 -4.1551847 -4.1516256 -4.1332097 -4.1032586 -4.058198 -3.994951 -3.9377949 -3.954958 -4.0387521 -4.1116452 -4.1668444 -4.1920538 -4.1987333 -4.2146144][-4.1526327 -4.158761 -4.1490054 -4.1281519 -4.0976787 -4.0479994 -3.9680905 -3.8865247 -3.8980613 -3.9995241 -4.0926533 -4.1568656 -4.1878386 -4.1992235 -4.2223458][-4.1853728 -4.17868 -4.1551175 -4.1263623 -4.0981736 -4.0586147 -3.9878011 -3.908565 -3.9130461 -4.0068207 -4.096663 -4.1571026 -4.1904764 -4.2071567 -4.23453][-4.2204428 -4.201767 -4.1669922 -4.1307631 -4.1069283 -4.0878229 -4.0468574 -3.9962947 -4.0021334 -4.0681934 -4.1326461 -4.1788349 -4.2086053 -4.2273526 -4.25359][-4.2434573 -4.2187195 -4.1810265 -4.1444125 -4.1263809 -4.12527 -4.11408 -4.0942039 -4.1059303 -4.1459084 -4.1826982 -4.2123933 -4.2354746 -4.253499 -4.2750487][-4.2483935 -4.221838 -4.1860561 -4.1536689 -4.142468 -4.1554408 -4.1668468 -4.1688008 -4.1820307 -4.2016153 -4.2173691 -4.2333374 -4.2499056 -4.2647023 -4.2823839][-4.243989 -4.2171454 -4.1864982 -4.1610532 -4.1556869 -4.1763191 -4.2007809 -4.2130952 -4.2228003 -4.2291222 -4.2315063 -4.2381382 -4.2487679 -4.2604551 -4.2767358][-4.2389388 -4.2147079 -4.1911359 -4.1743665 -4.1730494 -4.1936316 -4.2207613 -4.2348075 -4.2391238 -4.2378335 -4.2341714 -4.2364397 -4.24294 -4.2534342 -4.2680521][-4.2395968 -4.2219477 -4.2086654 -4.2006631 -4.2016854 -4.2173343 -4.24049 -4.251873 -4.2513719 -4.2442036 -4.2356987 -4.2351851 -4.2397618 -4.24968 -4.2617192]]...]
INFO - root - 2017-12-07 12:24:00.745609: step 8110, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.720 sec/batch; 64h:50m:41s remains)
INFO - root - 2017-12-07 12:24:07.505686: step 8120, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 58h:33m:56s remains)
INFO - root - 2017-12-07 12:24:14.292432: step 8130, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 58h:53m:02s remains)
INFO - root - 2017-12-07 12:24:21.206715: step 8140, loss = 2.02, batch loss = 1.96 (10.6 examples/sec; 0.753 sec/batch; 67h:51m:47s remains)
INFO - root - 2017-12-07 12:24:28.026804: step 8150, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 65h:20m:02s remains)
INFO - root - 2017-12-07 12:24:34.773073: step 8160, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 59h:15m:15s remains)
INFO - root - 2017-12-07 12:24:41.427016: step 8170, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 58h:40m:14s remains)
INFO - root - 2017-12-07 12:24:48.111529: step 8180, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 57h:40m:10s remains)
INFO - root - 2017-12-07 12:24:54.783562: step 8190, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:24m:38s remains)
INFO - root - 2017-12-07 12:25:01.668050: step 8200, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:50m:59s remains)
2017-12-07 12:25:02.361012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3363853 -4.3355775 -4.3103876 -4.2690115 -4.2288213 -4.1974463 -4.1814985 -4.2107482 -4.236598 -4.2408876 -4.2385869 -4.2452331 -4.2603478 -4.281981 -4.3022346][-4.3262663 -4.316658 -4.2835031 -4.2345943 -4.1914039 -4.1611781 -4.1544023 -4.197226 -4.234663 -4.2489877 -4.2509928 -4.2572904 -4.2682753 -4.2823582 -4.2943239][-4.3175416 -4.3015985 -4.2633824 -4.2098064 -4.1631637 -4.1323147 -4.1324692 -4.1875868 -4.2357221 -4.2590246 -4.2669983 -4.2742853 -4.2806973 -4.2865486 -4.2898264][-4.3156967 -4.2988091 -4.2592034 -4.2031345 -4.1513042 -4.1162238 -4.1131163 -4.1693268 -4.223752 -4.2562757 -4.2721047 -4.2824931 -4.2879281 -4.2909842 -4.2920809][-4.3172188 -4.3031745 -4.265018 -4.2067719 -4.1490493 -4.101028 -4.0779467 -4.120687 -4.1759734 -4.2205977 -4.2523575 -4.2741632 -4.2851663 -4.2900853 -4.2915058][-4.32029 -4.3085103 -4.2717695 -4.2113204 -4.1439757 -4.0707788 -4.0075665 -4.0235982 -4.0784078 -4.13823 -4.1917896 -4.2345686 -4.262054 -4.2772918 -4.2839][-4.3266687 -4.3184524 -4.2815475 -4.2135344 -4.1278124 -4.0216651 -3.9137404 -3.9053848 -3.9616437 -4.0342855 -4.1087685 -4.1729727 -4.2187238 -4.2470179 -4.2606282][-4.3303442 -4.3249049 -4.2887926 -4.21797 -4.1241088 -3.9996057 -3.8699925 -3.8579288 -3.9170873 -3.9883022 -4.0640068 -4.1311531 -4.180439 -4.2107582 -4.2276859][-4.3250804 -4.3209863 -4.2907586 -4.229507 -4.1481 -4.03547 -3.9235318 -3.9272797 -3.9884202 -4.0439649 -4.0978708 -4.1443272 -4.1774049 -4.1957488 -4.2082758][-4.3168278 -4.314899 -4.2939177 -4.2529058 -4.2000523 -4.1160541 -4.03681 -4.0490303 -4.1012554 -4.1390715 -4.1688719 -4.1903057 -4.2024941 -4.2047 -4.2095175][-4.3113375 -4.3098626 -4.2987075 -4.276773 -4.2483397 -4.1919703 -4.1403637 -4.1506119 -4.1870379 -4.209094 -4.2215085 -4.2295537 -4.2308974 -4.2233672 -4.2223883][-4.3114085 -4.3083754 -4.3007092 -4.2894769 -4.276938 -4.2435441 -4.2103348 -4.2164087 -4.238833 -4.2479305 -4.2498493 -4.25219 -4.2485404 -4.2373061 -4.2354794][-4.3165636 -4.3114834 -4.3019376 -4.2936406 -4.287447 -4.2714314 -4.2521834 -4.2576919 -4.2719793 -4.2716279 -4.2662663 -4.2632742 -4.2563934 -4.2456007 -4.2456107][-4.3201723 -4.3129778 -4.2980957 -4.2828927 -4.2773876 -4.2707191 -4.2617836 -4.2704997 -4.2841482 -4.28481 -4.2793641 -4.2725239 -4.2634726 -4.2554054 -4.2566028][-4.3219213 -4.3135595 -4.2940359 -4.2703838 -4.2571278 -4.2458649 -4.236413 -4.2528386 -4.2751384 -4.2854414 -4.2885022 -4.2851987 -4.2780428 -4.2722936 -4.2709446]]...]
INFO - root - 2017-12-07 12:25:09.053175: step 8210, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 56h:19m:20s remains)
INFO - root - 2017-12-07 12:25:16.015301: step 8220, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:33m:29s remains)
INFO - root - 2017-12-07 12:25:22.839195: step 8230, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 64h:43m:43s remains)
INFO - root - 2017-12-07 12:25:29.676239: step 8240, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 63h:12m:33s remains)
INFO - root - 2017-12-07 12:25:36.439536: step 8250, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 60h:50m:14s remains)
INFO - root - 2017-12-07 12:25:43.277081: step 8260, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 58h:16m:48s remains)
INFO - root - 2017-12-07 12:25:50.087024: step 8270, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 65h:05m:50s remains)
INFO - root - 2017-12-07 12:25:56.995113: step 8280, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 66h:26m:15s remains)
INFO - root - 2017-12-07 12:26:03.784563: step 8290, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 63h:32m:20s remains)
INFO - root - 2017-12-07 12:26:10.364743: step 8300, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 58h:01m:56s remains)
2017-12-07 12:26:11.031370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3629808 -4.3594146 -4.3538451 -4.348762 -4.3460822 -4.3459029 -4.3459463 -4.3439541 -4.3402481 -4.3394613 -4.3413277 -4.3429656 -4.3434834 -4.3446436 -4.3484235][-4.3586812 -4.3546085 -4.3487468 -4.3422556 -4.337501 -4.3354278 -4.334281 -4.3308067 -4.32579 -4.3264437 -4.3299308 -4.3324151 -4.3317571 -4.3296804 -4.3309169][-4.3519182 -4.3461819 -4.3380609 -4.3288531 -4.3225236 -4.3187866 -4.3139257 -4.3056636 -4.2976532 -4.2984972 -4.3037934 -4.3072405 -4.3061528 -4.3027391 -4.3016438][-4.3414955 -4.3312416 -4.3178663 -4.3034983 -4.2928271 -4.283855 -4.2713051 -4.2550573 -4.2433462 -4.2460446 -4.2546048 -4.25985 -4.2587457 -4.2552114 -4.2544861][-4.3196068 -4.301713 -4.2805061 -4.2598677 -4.2426748 -4.2247143 -4.2010174 -4.1731172 -4.1564622 -4.1645689 -4.1823397 -4.1922483 -4.19301 -4.1935077 -4.198606][-4.2843637 -4.2568722 -4.2276669 -4.2000909 -4.1750779 -4.147624 -4.1107893 -4.0677023 -4.0438714 -4.0660405 -4.104619 -4.1261854 -4.1322265 -4.1387434 -4.1500988][-4.2513967 -4.2139335 -4.1790757 -4.1480417 -4.1175733 -4.0804815 -4.0285897 -3.9621229 -3.9270413 -3.9733384 -4.0452123 -4.0853262 -4.0974822 -4.1071568 -4.120779][-4.231626 -4.1910343 -4.1556606 -4.123642 -4.0825853 -4.0297894 -3.9626577 -3.8757212 -3.8367486 -3.9149146 -4.019526 -4.0768638 -4.090641 -4.0953059 -4.1073666][-4.2356353 -4.201405 -4.1700029 -4.1369362 -4.0851879 -4.0213442 -3.9538231 -3.8760026 -3.851347 -3.9380646 -4.0482364 -4.1066041 -4.116396 -4.1134815 -4.119319][-4.2537022 -4.2266836 -4.1969037 -4.1603131 -4.107079 -4.0510874 -4.0013466 -3.9527841 -3.9463828 -4.0156078 -4.1051974 -4.1527972 -4.161305 -4.1594377 -4.1593132][-4.2661247 -4.2430534 -4.2169719 -4.1827097 -4.1392326 -4.0985775 -4.0673828 -4.0415106 -4.0428963 -4.087163 -4.1470542 -4.18489 -4.2005291 -4.2044106 -4.201757][-4.2828054 -4.2639627 -4.2441134 -4.2190557 -4.19036 -4.1654191 -4.1493177 -4.1372805 -4.1349845 -4.15282 -4.1857824 -4.2145371 -4.2360587 -4.2460055 -4.2440095][-4.3082194 -4.2971635 -4.2855406 -4.27266 -4.2578387 -4.245965 -4.2394867 -4.2354145 -4.2303314 -4.22939 -4.2418704 -4.258913 -4.277843 -4.2889276 -4.289453][-4.3326855 -4.3289604 -4.32533 -4.3233438 -4.3197966 -4.3184543 -4.3204846 -4.3210907 -4.3144646 -4.3036404 -4.2998066 -4.3027196 -4.3139453 -4.3249764 -4.3293376][-4.3519378 -4.3519015 -4.3505311 -4.3513861 -4.3524265 -4.3555717 -4.3604159 -4.3638973 -4.3593755 -4.3482347 -4.3380656 -4.3330984 -4.3381677 -4.3474264 -4.3544192]]...]
INFO - root - 2017-12-07 12:26:17.919816: step 8310, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 61h:35m:39s remains)
INFO - root - 2017-12-07 12:26:24.768149: step 8320, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 57h:11m:03s remains)
INFO - root - 2017-12-07 12:26:31.492534: step 8330, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:38m:37s remains)
INFO - root - 2017-12-07 12:26:38.234732: step 8340, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 63h:53m:15s remains)
INFO - root - 2017-12-07 12:26:45.099298: step 8350, loss = 2.11, batch loss = 2.05 (10.9 examples/sec; 0.733 sec/batch; 66h:01m:59s remains)
INFO - root - 2017-12-07 12:26:51.800896: step 8360, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 59h:32m:54s remains)
INFO - root - 2017-12-07 12:26:58.575097: step 8370, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 56h:30m:35s remains)
INFO - root - 2017-12-07 12:27:05.392818: step 8380, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 59h:34m:24s remains)
INFO - root - 2017-12-07 12:27:12.091016: step 8390, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 66h:24m:50s remains)
INFO - root - 2017-12-07 12:27:18.932446: step 8400, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 62h:24m:18s remains)
2017-12-07 12:27:19.649653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3637877 -4.3759317 -4.3752217 -4.3444605 -4.28046 -4.1967659 -4.128942 -4.0989704 -4.1219549 -4.1758156 -4.2341928 -4.273066 -4.2916508 -4.2915058 -4.2824373][-4.371974 -4.3860359 -4.3923326 -4.37331 -4.3230109 -4.24902 -4.17891 -4.133183 -4.1291337 -4.1580296 -4.203855 -4.2457371 -4.2777348 -4.2910504 -4.2912116][-4.3808675 -4.3971272 -4.4067268 -4.3931427 -4.3512783 -4.2846003 -4.2113614 -4.1512008 -4.12161 -4.1263003 -4.1634035 -4.2129683 -4.2607856 -4.2924008 -4.3067775][-4.38629 -4.4031472 -4.4128418 -4.401475 -4.3632016 -4.3008151 -4.2247934 -4.1530037 -4.1032858 -4.0876064 -4.1200585 -4.17806 -4.2412014 -4.2917247 -4.3226309][-4.3891048 -4.405385 -4.4141669 -4.4044905 -4.3678694 -4.3076944 -4.2309165 -4.1504478 -4.0811844 -4.0422554 -4.0710568 -4.1363096 -4.2123132 -4.2794175 -4.3265543][-4.3909187 -4.405272 -4.4089522 -4.3950949 -4.3558369 -4.2950687 -4.216712 -4.1254082 -4.0341563 -3.9728692 -4.0025053 -4.0796094 -4.1703367 -4.2523222 -4.3130207][-4.3901491 -4.4016061 -4.3982277 -4.3743649 -4.324739 -4.2561846 -4.1704078 -4.0647545 -3.9500413 -3.8730245 -3.9159029 -4.0128126 -4.1207466 -4.2178435 -4.2922173][-4.3864942 -4.3938 -4.3818612 -4.3450384 -4.2834516 -4.2072811 -4.1179295 -4.0052228 -3.8781435 -3.7967429 -3.8528457 -3.9626019 -4.0791497 -4.1850414 -4.2694][-4.3810139 -4.382443 -4.3622246 -4.3158336 -4.2473431 -4.1718669 -4.0905514 -3.9903951 -3.8779709 -3.8140383 -3.867733 -3.9653685 -4.0717463 -4.1735067 -4.2580767][-4.3747253 -4.3698907 -4.3448572 -4.2952971 -4.2269959 -4.1580544 -4.092905 -4.0171552 -3.9357963 -3.9011221 -3.9426379 -4.0142107 -4.1002159 -4.1900496 -4.2661591][-4.3685369 -4.3600278 -4.3349614 -4.2888565 -4.2285423 -4.1696544 -4.1216316 -4.0728602 -4.0253863 -4.0153112 -4.0438495 -4.0915184 -4.1578093 -4.2325258 -4.2930064][-4.3644605 -4.3569098 -4.337647 -4.3022995 -4.2555456 -4.2100453 -4.1762767 -4.1496406 -4.1293464 -4.1323509 -4.1483483 -4.1782804 -4.2260561 -4.28172 -4.3231883][-4.3622317 -4.3578 -4.346653 -4.3258624 -4.2951941 -4.2636676 -4.2425609 -4.2313046 -4.2281532 -4.234076 -4.242466 -4.259882 -4.2894416 -4.3234038 -4.343626][-4.3598614 -4.35856 -4.3545289 -4.3459983 -4.3306789 -4.31316 -4.302146 -4.2997451 -4.3035841 -4.3094006 -4.3146253 -4.3249841 -4.3389831 -4.3515277 -4.3523488][-4.3560047 -4.3565345 -4.3565073 -4.3550472 -4.3503761 -4.3438864 -4.3403797 -4.3427315 -4.3483496 -4.3524756 -4.35625 -4.3613482 -4.3620224 -4.3574209 -4.3462939]]...]
INFO - root - 2017-12-07 12:27:26.388484: step 8410, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:21m:40s remains)
INFO - root - 2017-12-07 12:27:33.325878: step 8420, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.744 sec/batch; 66h:56m:32s remains)
INFO - root - 2017-12-07 12:27:40.124047: step 8430, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 63h:21m:15s remains)
INFO - root - 2017-12-07 12:27:46.953864: step 8440, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 56h:49m:18s remains)
INFO - root - 2017-12-07 12:27:53.636455: step 8450, loss = 2.03, batch loss = 1.98 (12.7 examples/sec; 0.630 sec/batch; 56h:44m:39s remains)
INFO - root - 2017-12-07 12:28:00.406258: step 8460, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 60h:29m:07s remains)
INFO - root - 2017-12-07 12:28:07.245759: step 8470, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.731 sec/batch; 65h:45m:30s remains)
INFO - root - 2017-12-07 12:28:14.035116: step 8480, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 62h:38m:41s remains)
INFO - root - 2017-12-07 12:28:20.640471: step 8490, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 55h:28m:26s remains)
INFO - root - 2017-12-07 12:28:27.420198: step 8500, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 55h:25m:51s remains)
2017-12-07 12:28:28.121028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2250228 -4.2079849 -4.2035365 -4.1978245 -4.1755013 -4.162921 -4.163177 -4.1789856 -4.2148614 -4.2480316 -4.2665968 -4.2646642 -4.2498941 -4.2255187 -4.2010074][-4.2135735 -4.1979327 -4.200892 -4.1990337 -4.1823797 -4.1753693 -4.1785278 -4.19201 -4.2186637 -4.2386703 -4.2460146 -4.2370024 -4.2201214 -4.1975713 -4.173481][-4.1946063 -4.184484 -4.1908875 -4.1921997 -4.1822391 -4.1797009 -4.1820059 -4.1916356 -4.2123632 -4.2238297 -4.2189436 -4.2026434 -4.1829715 -4.1644177 -4.1471038][-4.1896572 -4.1777849 -4.1800089 -4.1766777 -4.1651893 -4.16016 -4.1604657 -4.1701407 -4.1939969 -4.2065549 -4.1963944 -4.1731558 -4.1502428 -4.1348572 -4.1243815][-4.2022471 -4.1850467 -4.1713247 -4.1486092 -4.1193447 -4.0987654 -4.0952954 -4.1150994 -4.1513534 -4.1759653 -4.172338 -4.1508007 -4.1285553 -4.1154394 -4.1109676][-4.2120152 -4.1863923 -4.152288 -4.1072874 -4.0538473 -4.0082726 -3.9958472 -4.0248537 -4.0757585 -4.1186914 -4.1326127 -4.1264215 -4.1153369 -4.103847 -4.1002326][-4.21639 -4.1812859 -4.1280761 -4.0635433 -3.9887981 -3.9247248 -3.9014797 -3.929307 -3.994926 -4.0591269 -4.0949574 -4.1095076 -4.1135983 -4.1088734 -4.1036978][-4.2164063 -4.1717954 -4.1050897 -4.0335512 -3.9574389 -3.8961015 -3.8697028 -3.8900588 -3.9587843 -4.0331597 -4.0746531 -4.0950184 -4.1053486 -4.107944 -4.1031737][-4.218307 -4.1663551 -4.095757 -4.0314116 -3.9712887 -3.9316268 -3.9180102 -3.9359522 -3.9971237 -4.06539 -4.09742 -4.1069841 -4.1111417 -4.1154952 -4.1109772][-4.2297554 -4.1809211 -4.1219277 -4.0749054 -4.0378423 -4.0258465 -4.0249949 -4.0388479 -4.0834966 -4.128871 -4.139513 -4.12729 -4.1184058 -4.120708 -4.1211405][-4.2446709 -4.2046957 -4.1634941 -4.1335182 -4.1155615 -4.1197748 -4.1206074 -4.1257896 -4.1535435 -4.1755033 -4.1594782 -4.1203966 -4.0978055 -4.0996385 -4.1106024][-4.2535934 -4.2259531 -4.20091 -4.181571 -4.1703115 -4.1754847 -4.171052 -4.1698561 -4.182508 -4.1836228 -4.1458874 -4.0907855 -4.0643463 -4.0754576 -4.1026835][-4.2573967 -4.2405672 -4.2214813 -4.20266 -4.1879992 -4.183743 -4.1735258 -4.168189 -4.1688342 -4.1537228 -4.1072178 -4.0548506 -4.0372829 -4.0606155 -4.0997906][-4.2631416 -4.2538857 -4.2359767 -4.2113209 -4.18697 -4.1692114 -4.1501889 -4.141386 -4.1389418 -4.1205878 -4.0795059 -4.0384808 -4.0352607 -4.0695863 -4.1122651][-4.2674513 -4.2652168 -4.25079 -4.2205906 -4.183537 -4.150208 -4.1212287 -4.1143789 -4.1203084 -4.1104231 -4.0804329 -4.0535288 -4.0639844 -4.1056876 -4.1470542]]...]
INFO - root - 2017-12-07 12:28:34.998333: step 8510, loss = 2.11, batch loss = 2.05 (11.3 examples/sec; 0.708 sec/batch; 63h:41m:29s remains)
INFO - root - 2017-12-07 12:28:41.786068: step 8520, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:30m:21s remains)
INFO - root - 2017-12-07 12:28:48.589746: step 8530, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 57h:02m:30s remains)
INFO - root - 2017-12-07 12:28:55.444614: step 8540, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 62h:34m:55s remains)
INFO - root - 2017-12-07 12:29:02.277297: step 8550, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 67h:00m:16s remains)
INFO - root - 2017-12-07 12:29:09.001416: step 8560, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 59h:32m:19s remains)
INFO - root - 2017-12-07 12:29:15.735549: step 8570, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.619 sec/batch; 55h:41m:14s remains)
INFO - root - 2017-12-07 12:29:22.463120: step 8580, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:01m:32s remains)
INFO - root - 2017-12-07 12:29:29.137465: step 8590, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 63h:47m:59s remains)
INFO - root - 2017-12-07 12:29:35.890375: step 8600, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 63h:57m:31s remains)
2017-12-07 12:29:36.583755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3237042 -4.3263197 -4.3253422 -4.3216987 -4.3167253 -4.3142519 -4.311904 -4.3064227 -4.297389 -4.2863746 -4.2703295 -4.2481079 -4.2217627 -4.2015104 -4.18848][-4.3251777 -4.3251815 -4.3194556 -4.3084116 -4.2974195 -4.2904849 -4.2852769 -4.2800546 -4.2742295 -4.2706943 -4.263154 -4.2495189 -4.233953 -4.2200975 -4.2082486][-4.3249025 -4.3214431 -4.3079762 -4.2884693 -4.2698293 -4.2594361 -4.2506804 -4.2442803 -4.2405066 -4.2439308 -4.24439 -4.2371297 -4.2328887 -4.23033 -4.2266755][-4.3204207 -4.31271 -4.2938423 -4.2703648 -4.2509565 -4.238575 -4.2225513 -4.207067 -4.2017169 -4.2096381 -4.2146521 -4.20734 -4.2073097 -4.2187476 -4.2316957][-4.3136096 -4.301477 -4.2794423 -4.2558928 -4.2329926 -4.2104583 -4.1779919 -4.1467361 -4.1412354 -4.1627431 -4.1720152 -4.1607389 -4.1591763 -4.1846828 -4.2189198][-4.300374 -4.2794619 -4.2475262 -4.2202244 -4.1934028 -4.155447 -4.0932231 -4.0312333 -4.0346389 -4.088429 -4.1140208 -4.1009541 -4.0986505 -4.1416068 -4.198667][-4.28226 -4.2503619 -4.2087164 -4.1780157 -4.1486411 -4.0934525 -3.9908073 -3.8903167 -3.9041729 -3.9996765 -4.0458059 -4.0368915 -4.0476284 -4.1123753 -4.1890783][-4.267602 -4.2317019 -4.1889391 -4.156179 -4.12951 -4.0770917 -3.9672298 -3.858994 -3.8715091 -3.9696267 -4.0195456 -4.0204487 -4.0483084 -4.1226168 -4.2052112][-4.2606211 -4.2246842 -4.1848965 -4.1530242 -4.13865 -4.1122508 -4.0435448 -3.9755692 -3.9830167 -4.0443068 -4.0763354 -4.0823603 -4.1111522 -4.1712823 -4.236074][-4.263607 -4.2305083 -4.1959577 -4.1678109 -4.1661277 -4.1663384 -4.1394153 -4.1130095 -4.1190252 -4.1506438 -4.1682043 -4.1754794 -4.1926265 -4.2267442 -4.2655439][-4.2765155 -4.2526636 -4.225894 -4.2056851 -4.2128644 -4.2247944 -4.2166662 -4.2104549 -4.2189236 -4.2361765 -4.2444367 -4.2468414 -4.2531915 -4.2674031 -4.2870436][-4.2951765 -4.2829728 -4.2652664 -4.2541256 -4.2625256 -4.2743006 -4.2696443 -4.2675996 -4.2769213 -4.288218 -4.288949 -4.2869229 -4.2864909 -4.2929544 -4.3035955][-4.3135495 -4.3094234 -4.301342 -4.2968841 -4.3045759 -4.3137145 -4.3085847 -4.3017359 -4.3033867 -4.3086472 -4.3061376 -4.3032303 -4.3032851 -4.3076835 -4.3137302][-4.3271451 -4.3271403 -4.3253636 -4.3256383 -4.3316026 -4.336689 -4.3310189 -4.3216825 -4.3167629 -4.3169694 -4.3139877 -4.3143439 -4.316514 -4.3177996 -4.319448][-4.3328795 -4.3346596 -4.3354473 -4.337255 -4.3395629 -4.3399372 -4.3345079 -4.3255692 -4.3185625 -4.3173518 -4.3175812 -4.3193574 -4.3210993 -4.3210368 -4.32217]]...]
INFO - root - 2017-12-07 12:29:43.258932: step 8610, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 63h:54m:16s remains)
INFO - root - 2017-12-07 12:29:50.106559: step 8620, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 64h:04m:25s remains)
INFO - root - 2017-12-07 12:29:56.889516: step 8630, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 58h:22m:37s remains)
INFO - root - 2017-12-07 12:30:03.573665: step 8640, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.616 sec/batch; 55h:24m:15s remains)
INFO - root - 2017-12-07 12:30:10.373615: step 8650, loss = 2.03, batch loss = 1.98 (11.8 examples/sec; 0.675 sec/batch; 60h:44m:54s remains)
INFO - root - 2017-12-07 12:30:17.313734: step 8660, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.751 sec/batch; 67h:33m:30s remains)
INFO - root - 2017-12-07 12:30:24.172695: step 8670, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 60h:27m:13s remains)
INFO - root - 2017-12-07 12:30:30.913366: step 8680, loss = 2.05, batch loss = 2.00 (13.3 examples/sec; 0.599 sec/batch; 53h:54m:16s remains)
INFO - root - 2017-12-07 12:30:37.490067: step 8690, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 56h:35m:03s remains)
INFO - root - 2017-12-07 12:30:44.240317: step 8700, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 58h:59m:43s remains)
2017-12-07 12:30:45.072117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3033519 -4.2944264 -4.28581 -4.2822347 -4.2864275 -4.2873211 -4.283247 -4.2735729 -4.260766 -4.253222 -4.2522984 -4.2574658 -4.2648168 -4.276607 -4.2930989][-4.2967687 -4.2839389 -4.2702765 -4.2640557 -4.2735143 -4.2763309 -4.2702494 -4.2572293 -4.2434 -4.2416711 -4.2413125 -4.2443066 -4.2542677 -4.2681756 -4.2847614][-4.2799764 -4.2629251 -4.2427711 -4.2293224 -4.2358146 -4.2402654 -4.2288365 -4.2116065 -4.2042828 -4.2150626 -4.2185259 -4.2248874 -4.2419009 -4.2621713 -4.2795377][-4.254221 -4.2340846 -4.2097774 -4.1953883 -4.1958709 -4.1888804 -4.1619067 -4.1381831 -4.1449666 -4.1728854 -4.185318 -4.2012968 -4.2235475 -4.2502742 -4.27193][-4.2293706 -4.2043433 -4.1780739 -4.1612244 -4.1444616 -4.11082 -4.0570045 -4.0217257 -4.0534582 -4.1090984 -4.1375713 -4.1698313 -4.1987395 -4.230638 -4.2548041][-4.2117481 -4.1774955 -4.1398053 -4.11091 -4.0693564 -3.9975634 -3.9084551 -3.8728447 -3.9452348 -4.033741 -4.07966 -4.1290994 -4.1667538 -4.206131 -4.2315979][-4.2046762 -4.1607261 -4.1090093 -4.0628343 -3.9885292 -3.8640976 -3.7349923 -3.7185688 -3.8426363 -3.9581909 -4.0227175 -4.0876122 -4.1374164 -4.1850743 -4.2176909][-4.2094893 -4.1665354 -4.1094322 -4.0456762 -3.9339578 -3.7632914 -3.6098893 -3.6333885 -3.8012033 -3.9322538 -4.0055156 -4.07397 -4.1295033 -4.1793556 -4.2178831][-4.2218189 -4.1880302 -4.1292777 -4.05761 -3.9456954 -3.7851689 -3.6577227 -3.7123034 -3.8769741 -3.9939945 -4.0555983 -4.1088009 -4.1520662 -4.1958032 -4.2328253][-4.2363162 -4.206408 -4.1480093 -4.081542 -3.9948492 -3.8822711 -3.8055377 -3.8661137 -3.9955761 -4.0843682 -4.1304512 -4.1687589 -4.194571 -4.2269559 -4.2550578][-4.2645321 -4.2375021 -4.1881747 -4.1344671 -4.0793376 -4.0132122 -3.9804723 -4.0368295 -4.1266336 -4.1829848 -4.2123194 -4.2361326 -4.2489147 -4.269165 -4.2849197][-4.2946715 -4.2760787 -4.2435322 -4.2126403 -4.1843696 -4.1493125 -4.1409097 -4.1832252 -4.2376447 -4.2676592 -4.2813454 -4.2926025 -4.2968426 -4.3099356 -4.3182616][-4.3192205 -4.3085594 -4.293148 -4.2863116 -4.280993 -4.2612724 -4.2604909 -4.2823563 -4.3084388 -4.320302 -4.3219829 -4.3231568 -4.3232284 -4.3334303 -4.3396907][-4.3271956 -4.321631 -4.3168263 -4.323842 -4.3352165 -4.3284631 -4.3289104 -4.3353939 -4.3410196 -4.3417931 -4.3377652 -4.3339405 -4.3332777 -4.3403363 -4.3438821][-4.316916 -4.3133979 -4.3135042 -4.3256621 -4.34452 -4.3456445 -4.3449697 -4.3434296 -4.3397932 -4.3368773 -4.3300562 -4.3256025 -4.3253312 -4.3313279 -4.3355503]]...]
INFO - root - 2017-12-07 12:30:51.810496: step 8710, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 61h:32m:50s remains)
INFO - root - 2017-12-07 12:30:58.579975: step 8720, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 57h:02m:10s remains)
INFO - root - 2017-12-07 12:31:05.473114: step 8730, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 60h:09m:39s remains)
INFO - root - 2017-12-07 12:31:12.415662: step 8740, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.735 sec/batch; 66h:06m:20s remains)
INFO - root - 2017-12-07 12:31:19.170292: step 8750, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 59h:56m:37s remains)
INFO - root - 2017-12-07 12:31:25.990167: step 8760, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:24m:09s remains)
INFO - root - 2017-12-07 12:31:32.789378: step 8770, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:05m:22s remains)
INFO - root - 2017-12-07 12:31:39.636287: step 8780, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 65h:03m:14s remains)
INFO - root - 2017-12-07 12:31:46.304854: step 8790, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.701 sec/batch; 63h:00m:34s remains)
INFO - root - 2017-12-07 12:31:53.027836: step 8800, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 62h:02m:21s remains)
2017-12-07 12:31:53.816283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3116636 -4.3082371 -4.3120427 -4.31738 -4.3197122 -4.318903 -4.3211617 -4.3237123 -4.3223481 -4.321332 -4.3226695 -4.3223476 -4.3215714 -4.3234215 -4.3274012][-4.2928724 -4.2815433 -4.2839646 -4.2882085 -4.2890191 -4.2858486 -4.2891922 -4.2964177 -4.3022485 -4.3071165 -4.312202 -4.3089972 -4.304183 -4.3033586 -4.3068695][-4.2416434 -4.2230458 -4.2266231 -4.2335191 -4.2328372 -4.2260838 -4.2247477 -4.2352886 -4.2505183 -4.2616634 -4.2706661 -4.2664547 -4.2559776 -4.2524867 -4.255527][-4.1625729 -4.142621 -4.1460514 -4.1497035 -4.14637 -4.135077 -4.1267204 -4.1362262 -4.1594205 -4.1785603 -4.1936545 -4.1943011 -4.1820679 -4.1783776 -4.18216][-4.1130404 -4.0934439 -4.0892148 -4.0776439 -4.062077 -4.0400386 -4.0174479 -4.018714 -4.0494232 -4.0817003 -4.1113076 -4.1266637 -4.1215687 -4.1207132 -4.1289477][-4.1056185 -4.0915246 -4.0777936 -4.0506263 -4.020647 -3.9881735 -3.9554532 -3.94313 -3.9684782 -4.007288 -4.0553374 -4.0931978 -4.1036773 -4.104476 -4.1144371][-4.1267023 -4.1237617 -4.110023 -4.0824971 -4.0484667 -4.0119543 -3.979393 -3.9575241 -3.9654412 -3.9900708 -4.043047 -4.0964289 -4.1205969 -4.1243706 -4.1307449][-4.1615 -4.1720405 -4.166225 -4.1463165 -4.1169405 -4.0818658 -4.052022 -4.03407 -4.0323663 -4.0424409 -4.0839057 -4.1350613 -4.1612983 -4.1642652 -4.1653495][-4.1895695 -4.211278 -4.2196717 -4.2110367 -4.1912417 -4.1646848 -4.1402936 -4.1247492 -4.1194949 -4.124537 -4.15317 -4.1858983 -4.1980786 -4.1932554 -4.1897268][-4.191391 -4.2172608 -4.2392564 -4.2464309 -4.2408457 -4.2272639 -4.212913 -4.200038 -4.1913576 -4.1939907 -4.2096291 -4.2203069 -4.2163405 -4.2005982 -4.190135][-4.1778965 -4.20311 -4.2324548 -4.25099 -4.2569218 -4.2572627 -4.2544732 -4.2452731 -4.2338181 -4.2320366 -4.2397985 -4.242311 -4.2312737 -4.2099657 -4.192523][-4.1562028 -4.1729951 -4.2043862 -4.2283854 -4.24446 -4.2562633 -4.2640352 -4.2633295 -4.254478 -4.2457514 -4.2414823 -4.2383595 -4.2276611 -4.2075605 -4.1884236][-4.1538224 -4.1570582 -4.1816044 -4.2005954 -4.2190881 -4.2382555 -4.2541108 -4.2608166 -4.2574382 -4.2496209 -4.2399783 -4.2282844 -4.2144661 -4.1976447 -4.1840477][-4.1753635 -4.170927 -4.1865139 -4.2017474 -4.2182679 -4.2374883 -4.2516727 -4.2584558 -4.257937 -4.2552981 -4.2472787 -4.2340903 -4.2181468 -4.2008138 -4.1916471][-4.219677 -4.2116156 -4.2183418 -4.2274323 -4.2403436 -4.2574439 -4.2693586 -4.2754593 -4.2747812 -4.2711329 -4.2643857 -4.2555537 -4.2424421 -4.2265315 -4.2206669]]...]
INFO - root - 2017-12-07 12:32:00.532557: step 8810, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 65h:26m:39s remains)
INFO - root - 2017-12-07 12:32:07.347149: step 8820, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 64h:51m:13s remains)
INFO - root - 2017-12-07 12:32:14.273658: step 8830, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:25m:48s remains)
INFO - root - 2017-12-07 12:32:21.080586: step 8840, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 56h:54m:04s remains)
INFO - root - 2017-12-07 12:32:27.898933: step 8850, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 60h:32m:49s remains)
INFO - root - 2017-12-07 12:32:34.742423: step 8860, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 65h:47m:05s remains)
INFO - root - 2017-12-07 12:32:41.585763: step 8870, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 60h:27m:21s remains)
INFO - root - 2017-12-07 12:32:48.447873: step 8880, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:42m:57s remains)
INFO - root - 2017-12-07 12:32:55.061688: step 8890, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 58h:45m:09s remains)
INFO - root - 2017-12-07 12:33:01.871851: step 8900, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 63h:13m:34s remains)
2017-12-07 12:33:02.677989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1907582 -4.1898932 -4.1965313 -4.2126265 -4.2137489 -4.2115507 -4.2155585 -4.2181587 -4.2214832 -4.2293029 -4.2300606 -4.2095456 -4.1795216 -4.1556673 -4.1452966][-4.1359262 -4.1546288 -4.1733532 -4.2042055 -4.2253809 -4.2333531 -4.2363839 -4.233202 -4.2318645 -4.2368379 -4.2317061 -4.1977587 -4.146934 -4.1092591 -4.0918655][-4.07338 -4.1197243 -4.1644254 -4.210125 -4.2385597 -4.2424078 -4.2333651 -4.2217903 -4.2114949 -4.206151 -4.1909308 -4.1494589 -4.0875177 -4.0439773 -4.0298424][-4.0156655 -4.0948548 -4.1676226 -4.2199941 -4.2407517 -4.2257047 -4.1978784 -4.17591 -4.1597695 -4.1493998 -4.1363688 -4.1027722 -4.0567493 -4.0260019 -4.0235944][-4.0433044 -4.1323328 -4.1998835 -4.2298923 -4.2252731 -4.1864 -4.1330962 -4.0967183 -4.0856333 -4.0840516 -4.0847082 -4.0781593 -4.0667586 -4.0597715 -4.0680017][-4.144238 -4.2071323 -4.2397256 -4.2304177 -4.1941552 -4.1295309 -4.046133 -3.9963963 -4.0083261 -4.0341015 -4.0612822 -4.0844908 -4.1031094 -4.1119232 -4.1199837][-4.2089591 -4.2382207 -4.2331872 -4.1872096 -4.1202416 -4.0332856 -3.9194796 -3.86374 -3.9236763 -3.9995468 -4.0570955 -4.1064167 -4.140748 -4.1494265 -4.1427393][-4.2185941 -4.2237692 -4.1892591 -4.1116171 -4.016727 -3.9164233 -3.7868781 -3.7396438 -3.855742 -3.974803 -4.0540709 -4.1184049 -4.1567373 -4.1552496 -4.1285448][-4.2180042 -4.200501 -4.1471214 -4.0571494 -3.9625914 -3.8841732 -3.7973161 -3.7809269 -3.8992994 -4.0157056 -4.0844326 -4.1396646 -4.1728454 -4.1612563 -4.1264038][-4.20511 -4.1800833 -4.1274605 -4.0478144 -3.9769301 -3.9336102 -3.8992045 -3.9017296 -3.9853859 -4.071867 -4.1187334 -4.1579618 -4.1837521 -4.1714597 -4.1414022][-4.197278 -4.1678424 -4.1200628 -4.0563116 -4.0092373 -3.9928117 -3.9941216 -4.0072742 -4.0596189 -4.1168571 -4.1485734 -4.1728354 -4.1881037 -4.1793804 -4.1583862][-4.2127719 -4.1840024 -4.1477776 -4.1084776 -4.0830641 -4.0819168 -4.098536 -4.1136842 -4.1430011 -4.1750207 -4.1928067 -4.2056031 -4.2126904 -4.2091751 -4.1971893][-4.2529454 -4.2332296 -4.2122622 -4.19475 -4.1804023 -4.1823215 -4.1984143 -4.2076392 -4.2188354 -4.2341609 -4.2444158 -4.2507467 -4.255342 -4.2576494 -4.2531343][-4.2929907 -4.2812643 -4.2712355 -4.2663183 -4.25926 -4.260839 -4.274261 -4.2810822 -4.2831922 -4.2894955 -4.2952747 -4.2986665 -4.3009577 -4.3028021 -4.300601][-4.3161645 -4.3085418 -4.3042512 -4.3049526 -4.3028393 -4.3044057 -4.3141756 -4.3192105 -4.3183575 -4.3191819 -4.3220658 -4.3239918 -4.324759 -4.3253164 -4.32331]]...]
INFO - root - 2017-12-07 12:33:09.377709: step 8910, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 55h:51m:38s remains)
INFO - root - 2017-12-07 12:33:16.042633: step 8920, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.721 sec/batch; 64h:47m:45s remains)
INFO - root - 2017-12-07 12:33:22.817762: step 8930, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 66h:40m:29s remains)
INFO - root - 2017-12-07 12:33:29.667834: step 8940, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 60h:03m:10s remains)
INFO - root - 2017-12-07 12:33:36.455997: step 8950, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 56h:51m:53s remains)
INFO - root - 2017-12-07 12:33:43.252402: step 8960, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:05m:56s remains)
INFO - root - 2017-12-07 12:33:50.040826: step 8970, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 64h:22m:48s remains)
INFO - root - 2017-12-07 12:33:56.960879: step 8980, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 65h:13m:51s remains)
INFO - root - 2017-12-07 12:34:03.495241: step 8990, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 61h:52m:41s remains)
INFO - root - 2017-12-07 12:34:10.203664: step 9000, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 59h:28m:16s remains)
2017-12-07 12:34:10.939542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1510296 -4.179708 -4.1930866 -4.2218943 -4.2476463 -4.2519894 -4.22953 -4.2023349 -4.1886635 -4.192512 -4.2073259 -4.2195168 -4.2016916 -4.1712518 -4.1764741][-4.2025213 -4.2283945 -4.2417932 -4.2609839 -4.2661467 -4.2488346 -4.2089467 -4.1662283 -4.1470861 -4.1574283 -4.1920519 -4.2245808 -4.22089 -4.1923032 -4.1950097][-4.2400737 -4.262032 -4.2712722 -4.2750459 -4.2613549 -4.2331748 -4.1905513 -4.1412935 -4.1144032 -4.1274171 -4.1788721 -4.2313828 -4.2492108 -4.2337303 -4.23567][-4.2432194 -4.2669287 -4.2705889 -4.2589421 -4.232811 -4.1994853 -4.1615214 -4.1125851 -4.0769968 -4.0926013 -4.1581635 -4.2327366 -4.279614 -4.2885895 -4.2924051][-4.2001023 -4.2343979 -4.2397518 -4.2220836 -4.190228 -4.1551919 -4.1219039 -4.0664062 -4.0184741 -4.0478597 -4.1327562 -4.2229152 -4.2888169 -4.3190279 -4.3269086][-4.1410837 -4.1949816 -4.2116184 -4.1960588 -4.1567216 -4.1157165 -4.08295 -4.0215383 -3.9628739 -4.0054822 -4.1045718 -4.1991124 -4.2734079 -4.313817 -4.3246536][-4.1125526 -4.1803794 -4.2078967 -4.1954627 -4.15007 -4.1072645 -4.0718369 -4.004807 -3.9422927 -3.987844 -4.0829906 -4.1712632 -4.2432542 -4.2885327 -4.3090987][-4.1059642 -4.18256 -4.2214541 -4.2097125 -4.1629419 -4.124917 -4.0846758 -4.0150175 -3.9683089 -4.0226493 -4.1025443 -4.1686478 -4.2242427 -4.2621231 -4.2851644][-4.1242118 -4.2025294 -4.2438178 -4.2326465 -4.1920462 -4.1591606 -4.1183186 -4.0581274 -4.0411921 -4.1010084 -4.1608973 -4.2000251 -4.2291327 -4.2501106 -4.2601633][-4.1552367 -4.2277384 -4.2636886 -4.2558522 -4.2276468 -4.2038264 -4.1671543 -4.12607 -4.130528 -4.1739392 -4.2096162 -4.2330279 -4.2467933 -4.2489796 -4.2461042][-4.1866455 -4.2478609 -4.2801752 -4.2769527 -4.2557406 -4.235158 -4.2109327 -4.1922135 -4.2018275 -4.2261758 -4.2437162 -4.2583027 -4.2639589 -4.2590442 -4.2543588][-4.2226729 -4.2701244 -4.2974539 -4.2932692 -4.2721925 -4.2530088 -4.2393131 -4.2362518 -4.2476931 -4.2606053 -4.2690477 -4.2800488 -4.2837682 -4.2790751 -4.2755122][-4.2623544 -4.2926598 -4.3076081 -4.3004985 -4.2807941 -4.2646542 -4.2613182 -4.26836 -4.2788148 -4.2840919 -4.2878089 -4.2989025 -4.3019767 -4.2966671 -4.2930007][-4.287559 -4.3027353 -4.3078723 -4.2982736 -4.2797346 -4.266541 -4.2706003 -4.2834625 -4.2918396 -4.2905903 -4.29175 -4.2999368 -4.3032455 -4.2990594 -4.2954073][-4.2913828 -4.2976031 -4.2983646 -4.288764 -4.2752066 -4.2657704 -4.2706757 -4.2823367 -4.2878304 -4.2849293 -4.285593 -4.2921824 -4.2964468 -4.2930751 -4.2883096]]...]
INFO - root - 2017-12-07 12:34:17.738605: step 9010, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:59m:03s remains)
INFO - root - 2017-12-07 12:34:24.532363: step 9020, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 56h:48m:31s remains)
INFO - root - 2017-12-07 12:34:31.271136: step 9030, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 58h:34m:36s remains)
INFO - root - 2017-12-07 12:34:38.170425: step 9040, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.751 sec/batch; 67h:27m:04s remains)
INFO - root - 2017-12-07 12:34:45.062628: step 9050, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.778 sec/batch; 69h:56m:31s remains)
INFO - root - 2017-12-07 12:34:51.777772: step 9060, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.696 sec/batch; 62h:30m:45s remains)
INFO - root - 2017-12-07 12:34:58.544763: step 9070, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 56h:52m:25s remains)
INFO - root - 2017-12-07 12:35:05.337165: step 9080, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:29m:25s remains)
INFO - root - 2017-12-07 12:35:11.988773: step 9090, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:57m:09s remains)
INFO - root - 2017-12-07 12:35:18.753242: step 9100, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 61h:18m:09s remains)
2017-12-07 12:35:19.505018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3310709 -4.325449 -4.3028278 -4.2600536 -4.2116017 -4.1864834 -4.1760416 -4.1698184 -4.1722207 -4.1903925 -4.2063503 -4.2236996 -4.2420039 -4.2526646 -4.2406278][-4.3321357 -4.3238249 -4.2988758 -4.2557926 -4.2092829 -4.1860652 -4.1806321 -4.1812048 -4.1840568 -4.1959028 -4.2050104 -4.2200317 -4.2426848 -4.2601256 -4.2527919][-4.3332872 -4.3205218 -4.2908497 -4.2457614 -4.2019663 -4.1812153 -4.1777263 -4.1828008 -4.1915112 -4.2028708 -4.2082314 -4.2167192 -4.2363944 -4.2557669 -4.2522125][-4.3385129 -4.3237066 -4.290988 -4.2440329 -4.197916 -4.174037 -4.1705484 -4.1781416 -4.1968732 -4.2164111 -4.2216554 -4.2196 -4.2263989 -4.2401381 -4.2417917][-4.3468819 -4.3339229 -4.300694 -4.252203 -4.1975513 -4.16187 -4.1519856 -4.1580844 -4.1839619 -4.2119055 -4.2232208 -4.2179012 -4.2137008 -4.2162566 -4.2190905][-4.3550959 -4.3456726 -4.3139668 -4.2624025 -4.1963644 -4.1414518 -4.1108646 -4.1038361 -4.1310697 -4.1763959 -4.2085552 -4.2140045 -4.2077589 -4.2012954 -4.2000346][-4.3594708 -4.3539786 -4.3273344 -4.2748036 -4.1984372 -4.1193953 -4.0555825 -4.0172415 -4.0341892 -4.1075091 -4.1779432 -4.211432 -4.2157369 -4.2062879 -4.1996555][-4.3591747 -4.3569908 -4.3367834 -4.2889085 -4.2118492 -4.1181512 -4.0221243 -3.9327192 -3.9153304 -4.0149646 -4.1288223 -4.20021 -4.2275419 -4.2295194 -4.2217369][-4.3590965 -4.358057 -4.3433995 -4.3030257 -4.2357759 -4.148387 -4.0473289 -3.9338503 -3.8831449 -3.9712856 -4.0907884 -4.1797662 -4.2291646 -4.2484965 -4.2467828][-4.3600421 -4.3592148 -4.3471928 -4.3148437 -4.261785 -4.195178 -4.1201749 -4.0332232 -3.9865718 -4.025229 -4.0949645 -4.1661286 -4.2214427 -4.2507672 -4.2564526][-4.3599477 -4.3590822 -4.3476305 -4.3192992 -4.2740726 -4.2234035 -4.1758494 -4.1267881 -4.10051 -4.1074686 -4.1282873 -4.1691532 -4.2179384 -4.2498045 -4.2600026][-4.357583 -4.3567033 -4.3447618 -4.316802 -4.270741 -4.2253265 -4.193697 -4.168891 -4.158299 -4.1605067 -4.1693888 -4.1919804 -4.22508 -4.2477303 -4.2534781][-4.3536506 -4.3516521 -4.338665 -4.3077335 -4.258779 -4.2141833 -4.1870041 -4.1707735 -4.1686835 -4.1780133 -4.1971111 -4.21761 -4.2363439 -4.2435465 -4.23759][-4.3512764 -4.3478465 -4.333961 -4.2997046 -4.2499828 -4.20742 -4.1791492 -4.1642036 -4.1699877 -4.1900239 -4.2175288 -4.2356372 -4.2472668 -4.2427435 -4.2249446][-4.3496232 -4.3456969 -4.3311105 -4.2973733 -4.2540956 -4.2207179 -4.1961865 -4.183917 -4.1963387 -4.2229538 -4.2456307 -4.2544532 -4.2583866 -4.2482495 -4.22613]]...]
INFO - root - 2017-12-07 12:35:26.304611: step 9110, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 59h:35m:10s remains)
INFO - root - 2017-12-07 12:35:33.139669: step 9120, loss = 2.03, batch loss = 1.98 (10.9 examples/sec; 0.733 sec/batch; 65h:51m:24s remains)
INFO - root - 2017-12-07 12:35:39.892994: step 9130, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 57h:36m:28s remains)
INFO - root - 2017-12-07 12:35:46.759756: step 9140, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 60h:12m:10s remains)
INFO - root - 2017-12-07 12:35:53.491460: step 9150, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 57h:59m:28s remains)
INFO - root - 2017-12-07 12:36:00.404471: step 9160, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.713 sec/batch; 64h:02m:58s remains)
INFO - root - 2017-12-07 12:36:07.271978: step 9170, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 63h:04m:11s remains)
INFO - root - 2017-12-07 12:36:14.064733: step 9180, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.689 sec/batch; 61h:50m:27s remains)
INFO - root - 2017-12-07 12:36:20.697197: step 9190, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 56h:18m:57s remains)
INFO - root - 2017-12-07 12:36:27.442176: step 9200, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 59h:56m:47s remains)
2017-12-07 12:36:28.214994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3002453 -4.289937 -4.2784653 -4.2624688 -4.2449694 -4.2383475 -4.2487121 -4.2602854 -4.2659264 -4.268261 -4.27305 -4.2804003 -4.281004 -4.2799621 -4.2769837][-4.2978997 -4.2845578 -4.2629075 -4.2303281 -4.2001204 -4.1884851 -4.1980805 -4.2127218 -4.2262692 -4.2356567 -4.2448149 -4.2534552 -4.2537708 -4.2502766 -4.2454228][-4.2969937 -4.2803354 -4.2440372 -4.1975579 -4.1572747 -4.1405621 -4.1431136 -4.1571903 -4.1836872 -4.2030621 -4.217001 -4.2272506 -4.2298193 -4.224062 -4.2182326][-4.2975821 -4.2758846 -4.2310915 -4.1778173 -4.133769 -4.1084137 -4.0892348 -4.0967765 -4.1342149 -4.1648173 -4.1837754 -4.1926193 -4.1937323 -4.1890326 -4.185431][-4.3004427 -4.276804 -4.2298632 -4.1746507 -4.1252408 -4.0782413 -4.0236464 -4.0178876 -4.0711465 -4.1194768 -4.1468334 -4.1572332 -4.1578474 -4.1544909 -4.1550932][-4.3017716 -4.2765651 -4.2277584 -4.1695976 -4.1112328 -4.0385361 -3.9411104 -3.9039075 -3.9708006 -4.0508208 -4.0987649 -4.1181145 -4.1224194 -4.1242371 -4.1328783][-4.2939878 -4.2690668 -4.2154207 -4.1507363 -4.0790596 -3.9771085 -3.8291483 -3.7481451 -3.8396752 -3.9721928 -4.0494757 -4.0821772 -4.0922003 -4.0978885 -4.1144409][-4.2793665 -4.2506919 -4.1920662 -4.1202126 -4.0343118 -3.910336 -3.7438357 -3.6517942 -3.7673478 -3.9316525 -4.0244532 -4.0618205 -4.071949 -4.08211 -4.1037588][-4.2636018 -4.2351131 -4.179678 -4.1132216 -4.0341196 -3.9259582 -3.8093174 -3.7726259 -3.8664105 -3.9860005 -4.0550604 -4.0758271 -4.0751343 -4.0802231 -4.1009016][-4.2571888 -4.2349577 -4.1911116 -4.1406269 -4.0853958 -4.0055547 -3.9358311 -3.9299588 -3.986717 -4.0502315 -4.0897446 -4.1004996 -4.0968847 -4.0985432 -4.116394][-4.2593117 -4.246491 -4.220962 -4.1883984 -4.1474133 -4.0847683 -4.0377584 -4.0400906 -4.0685577 -4.0952234 -4.1169991 -4.128356 -4.1338997 -4.1389346 -4.1505451][-4.2689991 -4.2618275 -4.2482233 -4.2256169 -4.191503 -4.1432881 -4.1137662 -4.1216221 -4.1341548 -4.1384926 -4.1467466 -4.1587377 -4.17247 -4.1814146 -4.1886096][-4.2898455 -4.2851434 -4.2753506 -4.259316 -4.23466 -4.2050467 -4.1922827 -4.1990433 -4.1986055 -4.189774 -4.1883 -4.1989274 -4.2169781 -4.2267089 -4.2291346][-4.312149 -4.3105764 -4.30583 -4.2975693 -4.2815557 -4.2653217 -4.2620311 -4.2661986 -4.2623973 -4.2536221 -4.2475233 -4.2508755 -4.2653475 -4.2730231 -4.271049][-4.331398 -4.3311324 -4.3291864 -4.3231897 -4.3120203 -4.304388 -4.3071022 -4.3131757 -4.312119 -4.3051891 -4.2971029 -4.2962708 -4.3057418 -4.30986 -4.3064771]]...]
INFO - root - 2017-12-07 12:36:34.944904: step 9210, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 62h:13m:09s remains)
INFO - root - 2017-12-07 12:36:41.766142: step 9220, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 58h:59m:21s remains)
INFO - root - 2017-12-07 12:36:48.473032: step 9230, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 63h:32m:39s remains)
INFO - root - 2017-12-07 12:36:55.306434: step 9240, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.671 sec/batch; 60h:16m:48s remains)
INFO - root - 2017-12-07 12:37:02.056063: step 9250, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 55h:05m:35s remains)
INFO - root - 2017-12-07 12:37:08.879574: step 9260, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 60h:27m:24s remains)
INFO - root - 2017-12-07 12:37:15.647723: step 9270, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 65h:04m:28s remains)
INFO - root - 2017-12-07 12:37:22.457666: step 9280, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:55m:25s remains)
INFO - root - 2017-12-07 12:37:29.245657: step 9290, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:30m:27s remains)
INFO - root - 2017-12-07 12:37:36.008185: step 9300, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 56h:21m:40s remains)
2017-12-07 12:37:36.733316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1929345 -4.1721182 -4.1526022 -4.1330824 -4.1246858 -4.1274338 -4.1384068 -4.164053 -4.19338 -4.2060246 -4.2007551 -4.184166 -4.1590443 -4.1419358 -4.1516056][-4.16861 -4.143939 -4.1206679 -4.0958776 -4.0891604 -4.1006393 -4.1180444 -4.1462827 -4.1760411 -4.1860666 -4.1817036 -4.1730123 -4.1575241 -4.1473279 -4.1608963][-4.1294985 -4.1035275 -4.079689 -4.0542393 -4.05405 -4.0752029 -4.0969176 -4.1229744 -4.14982 -4.1593208 -4.15821 -4.1563268 -4.1484108 -4.1454477 -4.1637697][-4.100563 -4.0749412 -4.0529037 -4.0326695 -4.0397243 -4.06505 -4.084506 -4.1052017 -4.1301837 -4.1430507 -4.146615 -4.1477184 -4.1440635 -4.1447616 -4.1637321][-4.0945444 -4.0714393 -4.052568 -4.0381689 -4.0454903 -4.0662913 -4.0786443 -4.0919957 -4.1158485 -4.1325974 -4.1407113 -4.1455917 -4.147182 -4.1485591 -4.1645875][-4.0895085 -4.0668674 -4.0487285 -4.03652 -4.03848 -4.0496216 -4.054997 -4.0649261 -4.0914764 -4.1141682 -4.1270761 -4.1372719 -4.1436977 -4.1459694 -4.1620026][-4.0678468 -4.0388417 -4.0185781 -4.0057788 -4.0053115 -4.0156717 -4.024765 -4.0403409 -4.0736337 -4.1029787 -4.12001 -4.1315007 -4.1376305 -4.1401076 -4.1582456][-4.0412011 -4.00713 -3.982343 -3.9656246 -3.9662604 -3.9829261 -4.002634 -4.0298104 -4.06976 -4.1038752 -4.1228218 -4.1323547 -4.1349921 -4.1378708 -4.1589589][-4.0264745 -3.99984 -3.9798 -3.9618034 -3.9601922 -3.9771042 -4.0053473 -4.04265 -4.0854387 -4.1198421 -4.1385179 -4.1440868 -4.1432176 -4.1451368 -4.16609][-4.0425949 -4.0252314 -4.0117164 -3.993947 -3.989435 -4.0030346 -4.0311279 -4.0690794 -4.1098418 -4.1440783 -4.1644278 -4.1684051 -4.164145 -4.1623936 -4.1783237][-4.0665288 -4.0524664 -4.0389595 -4.0189233 -4.011332 -4.0229721 -4.0485005 -4.0845065 -4.1234837 -4.1597452 -4.1846828 -4.1900692 -4.1837077 -4.1786995 -4.1902313][-4.0838461 -4.0737228 -4.0618029 -4.0438428 -4.0359311 -4.045857 -4.06697 -4.0981946 -4.1346073 -4.1708827 -4.1979294 -4.2040625 -4.196816 -4.1896815 -4.1987476][-4.0849433 -4.0780249 -4.0704775 -4.06032 -4.0550365 -4.0607767 -4.0725574 -4.0956578 -4.1284461 -4.1630106 -4.19038 -4.1979814 -4.1913886 -4.1855226 -4.1959033][-4.0812 -4.0713243 -4.0630612 -4.056684 -4.052515 -4.05235 -4.0539265 -4.0670714 -4.0937023 -4.1249852 -4.1540923 -4.1690888 -4.168963 -4.16785 -4.1821728][-4.0911903 -4.0779805 -4.0650277 -4.0565844 -4.052218 -4.0516753 -4.0501885 -4.0553741 -4.0737891 -4.1009321 -4.1294675 -4.1478367 -4.1502981 -4.1499557 -4.1652331]]...]
INFO - root - 2017-12-07 12:37:43.509454: step 9310, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 63h:52m:48s remains)
INFO - root - 2017-12-07 12:37:50.260127: step 9320, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:48m:24s remains)
INFO - root - 2017-12-07 12:37:57.009326: step 9330, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 56h:47m:31s remains)
INFO - root - 2017-12-07 12:38:03.901227: step 9340, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 61h:19m:37s remains)
INFO - root - 2017-12-07 12:38:10.654157: step 9350, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 65h:09m:44s remains)
INFO - root - 2017-12-07 12:38:17.503816: step 9360, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 63h:16m:57s remains)
INFO - root - 2017-12-07 12:38:24.220902: step 9370, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.704 sec/batch; 63h:13m:40s remains)
INFO - root - 2017-12-07 12:38:30.939030: step 9380, loss = 2.08, batch loss = 2.02 (13.4 examples/sec; 0.597 sec/batch; 53h:35m:27s remains)
INFO - root - 2017-12-07 12:38:37.544450: step 9390, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 58h:56m:03s remains)
INFO - root - 2017-12-07 12:38:44.474155: step 9400, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 66h:50m:17s remains)
2017-12-07 12:38:45.126376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1502028 -4.1658292 -4.197598 -4.2158728 -4.21736 -4.223196 -4.2443538 -4.2537255 -4.23016 -4.1687703 -4.1221037 -4.1346321 -4.1714177 -4.18123 -4.1913934][-4.1645308 -4.1953211 -4.2211342 -4.2259974 -4.2234945 -4.2308617 -4.2540159 -4.2660313 -4.2379503 -4.1694436 -4.1201277 -4.1298237 -4.1642866 -4.172544 -4.1844354][-4.2024374 -4.2282281 -4.2347956 -4.2259541 -4.2213531 -4.2290311 -4.2466559 -4.255044 -4.2281065 -4.1613135 -4.1152697 -4.1275115 -4.1591992 -4.1660094 -4.1804132][-4.2314959 -4.2386122 -4.2223372 -4.2109933 -4.2135983 -4.22413 -4.2323151 -4.2345748 -4.2081709 -4.149076 -4.1114154 -4.1282392 -4.157537 -4.1639352 -4.1828179][-4.2400346 -4.22808 -4.186882 -4.1693125 -4.17924 -4.1999369 -4.2073913 -4.2018986 -4.1737409 -4.1227188 -4.0957727 -4.1218915 -4.1484909 -4.153986 -4.1780591][-4.216774 -4.1810045 -4.110796 -4.0726833 -4.0796976 -4.1086731 -4.1176581 -4.1055861 -4.0817671 -4.0491195 -4.0392146 -4.0701923 -4.0969362 -4.1079674 -4.1419826][-4.1550121 -4.0872931 -3.9819934 -3.9153204 -3.9171877 -3.9613659 -3.971735 -3.9402127 -3.9195256 -3.9158611 -3.9302933 -3.9767284 -4.0205436 -4.0502748 -4.1032114][-4.0846128 -3.9923785 -3.8660016 -3.7879972 -3.7908235 -3.8478696 -3.8629847 -3.8195767 -3.8084669 -3.8352671 -3.8658121 -3.9184608 -3.9753225 -4.0166454 -4.0801096][-4.03988 -3.9595151 -3.8503909 -3.7846737 -3.7946007 -3.8563361 -3.8737857 -3.8258667 -3.8242264 -3.8745008 -3.9129722 -3.9598312 -4.0159984 -4.0558825 -4.1113138][-4.03214 -3.984777 -3.9125161 -3.8637936 -3.8733389 -3.9229681 -3.9303441 -3.8823922 -3.8878222 -3.9522593 -4.0020156 -4.0497074 -4.1048732 -4.143404 -4.1892776][-4.0703278 -4.0516639 -4.017375 -3.9842105 -3.9822993 -4.0093279 -4.0128379 -3.9767787 -3.9856079 -4.0477962 -4.1003647 -4.1489034 -4.1961756 -4.2275119 -4.2601519][-4.1344266 -4.1357136 -4.1274524 -4.1118951 -4.1051664 -4.1135035 -4.113143 -4.091785 -4.0974908 -4.1394377 -4.1828532 -4.2265258 -4.2613964 -4.2812166 -4.2984681][-4.2082086 -4.2170415 -4.2201538 -4.2162542 -4.2097445 -4.2087035 -4.2082152 -4.1990132 -4.1986566 -4.2136488 -4.2378616 -4.2684536 -4.2902865 -4.298883 -4.3058314][-4.2631941 -4.2708182 -4.2757006 -4.2745352 -4.271616 -4.2704244 -4.2721424 -4.267921 -4.2614074 -4.2550788 -4.2595592 -4.2782793 -4.2912068 -4.2936826 -4.2969584][-4.2925587 -4.2921419 -4.2952094 -4.2950296 -4.2937574 -4.2932425 -4.2938061 -4.29009 -4.2768164 -4.2535172 -4.2414913 -4.2564774 -4.2685738 -4.2735229 -4.2822819]]...]
INFO - root - 2017-12-07 12:38:51.856250: step 9410, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:02m:11s remains)
INFO - root - 2017-12-07 12:38:58.631681: step 9420, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 58h:44m:52s remains)
INFO - root - 2017-12-07 12:39:05.450782: step 9430, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 65h:05m:06s remains)
INFO - root - 2017-12-07 12:39:12.271371: step 9440, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 65h:47m:25s remains)
INFO - root - 2017-12-07 12:39:19.121297: step 9450, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 58h:58m:00s remains)
INFO - root - 2017-12-07 12:39:25.908252: step 9460, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 57h:12m:21s remains)
INFO - root - 2017-12-07 12:39:32.741618: step 9470, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 64h:04m:14s remains)
INFO - root - 2017-12-07 12:39:39.550351: step 9480, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 61h:58m:19s remains)
INFO - root - 2017-12-07 12:39:46.248076: step 9490, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 60h:20m:47s remains)
INFO - root - 2017-12-07 12:39:52.993226: step 9500, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 58h:47m:40s remains)
2017-12-07 12:39:53.737350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1886005 -4.2219324 -4.2683787 -4.2863331 -4.2539682 -4.1675882 -4.0718827 -4.0186858 -4.0340133 -4.1123877 -4.2077007 -4.2513 -4.2689395 -4.2811694 -4.2869806][-4.20102 -4.2331438 -4.2748909 -4.2878556 -4.2484851 -4.1625485 -4.0689344 -4.0115876 -4.0218172 -4.1001029 -4.1987672 -4.2504125 -4.2750454 -4.2934456 -4.3038745][-4.2180309 -4.2450361 -4.2759986 -4.2818513 -4.2391891 -4.1570549 -4.0724545 -4.0141783 -4.0112209 -4.0770359 -4.1631236 -4.2183409 -4.2552476 -4.2887912 -4.3082676][-4.2412825 -4.2563157 -4.2685108 -4.2585421 -4.2100005 -4.1398468 -4.07215 -4.0210576 -4.0061398 -4.0527968 -4.1236525 -4.1766191 -4.2232022 -4.2695575 -4.2989163][-4.2397881 -4.2420135 -4.2422142 -4.2209239 -4.1667461 -4.1069937 -4.0633016 -4.0295124 -4.01535 -4.046206 -4.0964437 -4.1393876 -4.1881 -4.2396941 -4.2748365][-4.2135139 -4.2070112 -4.2014141 -4.1650519 -4.0989833 -4.0453463 -4.0214143 -4.00406 -3.99567 -4.0175982 -4.0556459 -4.0949121 -4.1438961 -4.1962004 -4.2373543][-4.1781516 -4.1695085 -4.162343 -4.1085172 -4.0287719 -3.9810207 -3.973913 -3.9716082 -3.9641469 -3.9738858 -4.00506 -4.049202 -4.103714 -4.1570387 -4.2003903][-4.1684213 -4.1617346 -4.1580672 -4.1070232 -4.0318604 -3.9887228 -3.9902868 -3.9942429 -3.9872894 -3.9811845 -4.0033727 -4.0512533 -4.1121264 -4.1664538 -4.2007389][-4.201962 -4.19541 -4.1946964 -4.1552629 -4.0928688 -4.050395 -4.04913 -4.0523267 -4.0494151 -4.045115 -4.0665131 -4.1167526 -4.1770148 -4.2224846 -4.2433681][-4.2502027 -4.2438726 -4.2455173 -4.2199717 -4.1753092 -4.1383638 -4.1341796 -4.1395807 -4.1406736 -4.1383896 -4.1540704 -4.1945872 -4.2433934 -4.2780809 -4.290246][-4.2801175 -4.2789035 -4.2846227 -4.2718687 -4.2443829 -4.2171578 -4.2160792 -4.2262821 -4.2291112 -4.2264028 -4.2342496 -4.2560806 -4.2903328 -4.3148565 -4.3242912][-4.2900457 -4.2947836 -4.3044267 -4.3010592 -4.2827983 -4.2624702 -4.2638068 -4.279007 -4.28573 -4.2789803 -4.2818127 -4.2942376 -4.3125544 -4.3264103 -4.3325024][-4.2869616 -4.2897725 -4.2994704 -4.3025632 -4.2931013 -4.2815342 -4.2854443 -4.2988682 -4.3049169 -4.2974496 -4.2973151 -4.3062549 -4.3164454 -4.3225489 -4.3260703][-4.2875595 -4.2875786 -4.2924428 -4.2956152 -4.2887783 -4.2796469 -4.2818894 -4.2906752 -4.29589 -4.2922096 -4.2924495 -4.2995086 -4.3071461 -4.3125076 -4.3172][-4.2940083 -4.2922788 -4.2924666 -4.2937546 -4.2890315 -4.2817965 -4.2810645 -4.2846622 -4.288404 -4.2891321 -4.2919683 -4.2973132 -4.3023586 -4.307107 -4.3116841]]...]
INFO - root - 2017-12-07 12:40:00.464157: step 9510, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 63h:39m:53s remains)
INFO - root - 2017-12-07 12:40:07.257340: step 9520, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:36m:35s remains)
INFO - root - 2017-12-07 12:40:14.031299: step 9530, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:17m:36s remains)
INFO - root - 2017-12-07 12:40:20.654722: step 9540, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 61h:58m:02s remains)
INFO - root - 2017-12-07 12:40:27.529888: step 9550, loss = 2.05, batch loss = 2.00 (10.6 examples/sec; 0.752 sec/batch; 67h:26m:29s remains)
INFO - root - 2017-12-07 12:40:34.321592: step 9560, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 58h:32m:20s remains)
INFO - root - 2017-12-07 12:40:41.066703: step 9570, loss = 2.03, batch loss = 1.97 (12.7 examples/sec; 0.629 sec/batch; 56h:27m:23s remains)
INFO - root - 2017-12-07 12:40:47.927889: step 9580, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 57h:34m:56s remains)
INFO - root - 2017-12-07 12:40:54.717384: step 9590, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 65h:53m:41s remains)
INFO - root - 2017-12-07 12:41:01.423758: step 9600, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 60h:16m:34s remains)
2017-12-07 12:41:02.075652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3055511 -4.3069825 -4.3017864 -4.2855759 -4.2587881 -4.2244563 -4.1950912 -4.1791191 -4.1745772 -4.1721582 -4.1577516 -4.1394029 -4.1094813 -4.0661111 -4.049159][-4.2882152 -4.2896757 -4.28734 -4.2698193 -4.2422209 -4.2043786 -4.1725292 -4.1615396 -4.1637979 -4.1674376 -4.160027 -4.1476121 -4.1174064 -4.0678368 -4.043467][-4.2655749 -4.2705069 -4.2726197 -4.2565856 -4.2311149 -4.1933079 -4.1666212 -4.1652346 -4.175096 -4.1821771 -4.1841264 -4.186079 -4.1646233 -4.1244922 -4.1048975][-4.2268095 -4.239676 -4.2503743 -4.2379122 -4.2166967 -4.1858249 -4.1699452 -4.1783228 -4.1957483 -4.2064176 -4.2151351 -4.229743 -4.2224984 -4.1970716 -4.1839485][-4.1670656 -4.1896343 -4.2130709 -4.2091351 -4.1953454 -4.1760125 -4.1732707 -4.1857209 -4.2069478 -4.2193375 -4.2331219 -4.25422 -4.2597904 -4.250525 -4.2474141][-4.1019573 -4.1330667 -4.1703291 -4.1778374 -4.170928 -4.1591225 -4.1635389 -4.17537 -4.1992354 -4.2187257 -4.2402349 -4.2653995 -4.2793 -4.2805104 -4.2839131][-4.0770326 -4.1049604 -4.1391335 -4.1510072 -4.148592 -4.1376033 -4.1402178 -4.149642 -4.1766357 -4.2057257 -4.2342339 -4.2654419 -4.2868381 -4.2930794 -4.3009052][-4.096724 -4.1076865 -4.1231213 -4.1266904 -4.1200957 -4.1038942 -4.1015229 -4.1070309 -4.1365366 -4.1743608 -4.2123785 -4.2517772 -4.2815261 -4.2935538 -4.3070793][-4.1322603 -4.1254439 -4.1148677 -4.0963626 -4.0734224 -4.0454569 -4.0351548 -4.0386672 -4.0775313 -4.1282191 -4.17564 -4.2260494 -4.2647686 -4.2842526 -4.3047552][-4.1597443 -4.138319 -4.1085563 -4.0686903 -4.0299706 -4.0000262 -3.9906354 -3.99442 -4.0409746 -4.0994744 -4.1505146 -4.2014871 -4.24163 -4.2646446 -4.2870378][-4.1893077 -4.1564913 -4.1155119 -4.0744548 -4.0394449 -4.0270128 -4.0287609 -4.0320091 -4.072814 -4.1259336 -4.1686559 -4.2058949 -4.233995 -4.252152 -4.2699852][-4.2022109 -4.1664348 -4.1274395 -4.1022964 -4.0869675 -4.0908594 -4.1006351 -4.1050124 -4.1342664 -4.1758208 -4.2080784 -4.2289433 -4.2443752 -4.2537236 -4.2623439][-4.2042732 -4.1736183 -4.1465759 -4.1377778 -4.1404033 -4.1532769 -4.1670785 -4.1718245 -4.1929665 -4.225637 -4.2491527 -4.2589951 -4.2654805 -4.2647624 -4.2637172][-4.2110376 -4.1913471 -4.1833715 -4.1896958 -4.205667 -4.2262254 -4.2415977 -4.2454004 -4.2568808 -4.27662 -4.2899208 -4.2922177 -4.2919178 -4.2821198 -4.2707596][-4.2445498 -4.237164 -4.2424459 -4.2541265 -4.2724662 -4.2929425 -4.3045731 -4.3058057 -4.3090291 -4.3177509 -4.32109 -4.3191996 -4.313942 -4.2961149 -4.2743063]]...]
INFO - root - 2017-12-07 12:41:08.869733: step 9610, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 58h:35m:45s remains)
INFO - root - 2017-12-07 12:41:15.668608: step 9620, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:51m:43s remains)
INFO - root - 2017-12-07 12:41:22.622862: step 9630, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 63h:59m:54s remains)
INFO - root - 2017-12-07 12:41:29.465803: step 9640, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 56h:10m:26s remains)
INFO - root - 2017-12-07 12:41:36.185301: step 9650, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:23m:10s remains)
INFO - root - 2017-12-07 12:41:43.111915: step 9660, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.752 sec/batch; 67h:26m:21s remains)
INFO - root - 2017-12-07 12:41:49.922037: step 9670, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 65h:51m:44s remains)
INFO - root - 2017-12-07 12:41:56.733862: step 9680, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 56h:30m:29s remains)
INFO - root - 2017-12-07 12:42:03.129784: step 9690, loss = 2.09, batch loss = 2.03 (14.2 examples/sec; 0.561 sec/batch; 50h:20m:37s remains)
INFO - root - 2017-12-07 12:42:09.905297: step 9700, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:43m:08s remains)
2017-12-07 12:42:10.620970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3001666 -4.3053007 -4.2992716 -4.293962 -4.2941523 -4.30233 -4.3096275 -4.306819 -4.3031154 -4.3032756 -4.308248 -4.3162332 -4.3236589 -4.3212562 -4.3164644][-4.2923365 -4.29407 -4.2831945 -4.27389 -4.2724671 -4.2843995 -4.2885809 -4.277873 -4.2733583 -4.2799692 -4.293469 -4.3097529 -4.3239112 -4.3215876 -4.3152733][-4.28144 -4.2795148 -4.2632794 -4.24847 -4.2427297 -4.2518697 -4.2495513 -4.2327933 -4.2320461 -4.2473021 -4.2705655 -4.2957563 -4.3153753 -4.3136978 -4.3064561][-4.2705197 -4.26524 -4.2395473 -4.21755 -4.2072158 -4.208509 -4.1945133 -4.1674438 -4.172349 -4.2029223 -4.24063 -4.2786503 -4.3026981 -4.3028264 -4.2960019][-4.2597876 -4.2488894 -4.2142048 -4.1859422 -4.1698561 -4.156065 -4.1202497 -4.0756578 -4.0923367 -4.1460834 -4.2003379 -4.2530069 -4.2840161 -4.2901864 -4.2866836][-4.2547708 -4.2381182 -4.198771 -4.1669807 -4.1461487 -4.1141129 -4.0434813 -3.9720287 -4.0068827 -4.0893636 -4.1606765 -4.2270479 -4.265 -4.279459 -4.2797732][-4.2643108 -4.2454944 -4.2062521 -4.1697125 -4.1356468 -4.0742335 -3.9554453 -3.8574903 -3.929605 -4.0534925 -4.1390419 -4.2106342 -4.25483 -4.2743196 -4.2773585][-4.2696285 -4.2557635 -4.2263217 -4.1844182 -4.1217065 -4.0147824 -3.8321011 -3.7063522 -3.8390875 -4.0148277 -4.121942 -4.1999269 -4.2513003 -4.27604 -4.281415][-4.2554507 -4.2542181 -4.2414837 -4.197957 -4.1110392 -3.9710352 -3.7478266 -3.6152849 -3.7992294 -4.00793 -4.12654 -4.2052259 -4.2545295 -4.2801747 -4.2857852][-4.2336988 -4.2409358 -4.2426558 -4.2046967 -4.1160188 -3.9870651 -3.7989783 -3.7027485 -3.8657532 -4.0472846 -4.1509342 -4.2180443 -4.2627254 -4.2878218 -4.2934718][-4.2205439 -4.2347393 -4.248342 -4.2212181 -4.1427674 -4.0358238 -3.8909919 -3.8264074 -3.9598041 -4.1058893 -4.1876893 -4.2415056 -4.2795248 -4.2989736 -4.3010306][-4.2064562 -4.22542 -4.2510843 -4.2368903 -4.178453 -4.0966158 -3.9878249 -3.9398186 -4.0424161 -4.1551619 -4.2210479 -4.2641673 -4.2935133 -4.3074026 -4.3073564][-4.1919308 -4.214376 -4.2491436 -4.25234 -4.2221518 -4.1737461 -4.0978413 -4.0596228 -4.1305718 -4.2113748 -4.25836 -4.2887154 -4.3085785 -4.3164639 -4.3134389][-4.1950212 -4.2204747 -4.2598572 -4.275939 -4.270823 -4.24925 -4.2004619 -4.1672149 -4.2083006 -4.2603054 -4.2935085 -4.3125815 -4.3221297 -4.323009 -4.317832][-4.2072368 -4.2332592 -4.2714443 -4.2932978 -4.3009562 -4.2938509 -4.2622924 -4.2372832 -4.2572331 -4.2913232 -4.3139691 -4.3228602 -4.3236389 -4.3204927 -4.3157625]]...]
INFO - root - 2017-12-07 12:42:17.481630: step 9710, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 60h:14m:25s remains)
INFO - root - 2017-12-07 12:42:24.364202: step 9720, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 61h:29m:57s remains)
INFO - root - 2017-12-07 12:42:31.226661: step 9730, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 65h:48m:43s remains)
INFO - root - 2017-12-07 12:42:38.083510: step 9740, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.750 sec/batch; 67h:12m:38s remains)
INFO - root - 2017-12-07 12:42:44.910208: step 9750, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 63h:41m:45s remains)
INFO - root - 2017-12-07 12:42:51.635744: step 9760, loss = 2.03, batch loss = 1.97 (12.4 examples/sec; 0.646 sec/batch; 57h:52m:58s remains)
INFO - root - 2017-12-07 12:42:58.379760: step 9770, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 61h:25m:25s remains)
INFO - root - 2017-12-07 12:43:05.201879: step 9780, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.749 sec/batch; 67h:09m:32s remains)
INFO - root - 2017-12-07 12:43:11.858315: step 9790, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.607 sec/batch; 54h:23m:17s remains)
INFO - root - 2017-12-07 12:43:18.611864: step 9800, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 59h:39m:03s remains)
2017-12-07 12:43:19.290226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2787056 -4.2698731 -4.2653127 -4.2636323 -4.2621732 -4.261714 -4.2621384 -4.2673421 -4.2771716 -4.2921448 -4.3073821 -4.3212852 -4.333003 -4.342525 -4.3507056][-4.2858109 -4.2742562 -4.2675066 -4.2638187 -4.2583008 -4.2515378 -4.2448764 -4.2460504 -4.2525196 -4.2684922 -4.2884603 -4.3072882 -4.3223729 -4.3348951 -4.3470249][-4.2892971 -4.2713504 -4.2591419 -4.2502384 -4.2401686 -4.2310805 -4.2207317 -4.2172332 -4.2216053 -4.2394209 -4.2638164 -4.2873211 -4.3065739 -4.3232565 -4.3402667][-4.2705579 -4.2453279 -4.2246985 -4.20687 -4.1935258 -4.185782 -4.1771512 -4.1728926 -4.1778398 -4.1994596 -4.2313857 -4.26224 -4.28784 -4.3095546 -4.3324409][-4.2293611 -4.1976032 -4.1664681 -4.1368532 -4.1181865 -4.1088834 -4.1023064 -4.10512 -4.1176972 -4.148972 -4.1916518 -4.23302 -4.2664919 -4.2958264 -4.3253751][-4.1842256 -4.1490669 -4.103138 -4.0608149 -4.0372777 -4.0264831 -4.0254211 -4.0389109 -4.0636749 -4.1085215 -4.1645985 -4.2153335 -4.2556329 -4.2898254 -4.3234887][-4.1534219 -4.1232281 -4.0622954 -4.0008731 -3.9681008 -3.9551809 -3.9613411 -3.9849508 -4.0251832 -4.0881619 -4.1605406 -4.219326 -4.260551 -4.2941589 -4.32658][-4.1477113 -4.1282463 -4.0586329 -3.9784822 -3.9319825 -3.9138074 -3.9315696 -3.9656956 -4.0163455 -4.0893512 -4.168592 -4.2300515 -4.2700772 -4.3005056 -4.3298111][-4.1434078 -4.1447706 -4.089891 -4.0103254 -3.9554589 -3.9267049 -3.9450617 -3.9817982 -4.0287261 -4.0958362 -4.1709447 -4.2301726 -4.269702 -4.2995248 -4.3283525][-4.1115255 -4.1317558 -4.111867 -4.061995 -4.0208693 -3.9841659 -3.9932575 -4.0188646 -4.0503912 -4.0992241 -4.162 -4.2165647 -4.2574687 -4.2900305 -4.3220196][-4.0891809 -4.1204767 -4.1288409 -4.10595 -4.0796614 -4.0377464 -4.036212 -4.0539 -4.0726261 -4.1044331 -4.1531882 -4.2011161 -4.2419181 -4.2781215 -4.3137126][-4.0829368 -4.1181903 -4.1413221 -4.1287317 -4.1061468 -4.0650215 -4.0581632 -4.07524 -4.0922713 -4.1181726 -4.1570692 -4.1982713 -4.2351165 -4.2704926 -4.30794][-4.1011992 -4.1300712 -4.1531549 -4.1406832 -4.1154966 -4.0765896 -4.0698414 -4.0876365 -4.109797 -4.1402364 -4.1770554 -4.2120423 -4.2416186 -4.2723169 -4.3075352][-4.1426945 -4.1621442 -4.1800513 -4.1661711 -4.1401167 -4.1107683 -4.107121 -4.124948 -4.1490345 -4.1800795 -4.2128968 -4.2405839 -4.2625279 -4.2867932 -4.3154683][-4.2037454 -4.2163868 -4.2273245 -4.2131119 -4.1910534 -4.1735158 -4.1726937 -4.1863856 -4.2080331 -4.2318892 -4.2559137 -4.2778621 -4.2933884 -4.3107672 -4.3300686]]...]
INFO - root - 2017-12-07 12:43:26.106263: step 9810, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 63h:20m:12s remains)
INFO - root - 2017-12-07 12:43:32.900576: step 9820, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 62h:25m:00s remains)
INFO - root - 2017-12-07 12:43:39.589319: step 9830, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 62h:17m:12s remains)
INFO - root - 2017-12-07 12:43:46.304404: step 9840, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 56h:52m:48s remains)
INFO - root - 2017-12-07 12:43:52.960895: step 9850, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 64h:47m:10s remains)
INFO - root - 2017-12-07 12:43:59.802577: step 9860, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.735 sec/batch; 65h:54m:56s remains)
INFO - root - 2017-12-07 12:44:06.589247: step 9870, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.693 sec/batch; 62h:04m:28s remains)
INFO - root - 2017-12-07 12:44:13.352879: step 9880, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 57h:32m:26s remains)
INFO - root - 2017-12-07 12:44:19.994760: step 9890, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 58h:14m:01s remains)
INFO - root - 2017-12-07 12:44:26.667135: step 9900, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 64h:17m:02s remains)
2017-12-07 12:44:27.455992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2514229 -4.2542105 -4.2642689 -4.2737522 -4.2791452 -4.2819519 -4.2803397 -4.2769938 -4.2733855 -4.2726607 -4.2669353 -4.2539454 -4.2392416 -4.22345 -4.2121096][-4.2412519 -4.2480135 -4.2570615 -4.2634931 -4.264647 -4.26422 -4.2603879 -4.2535071 -4.2480683 -4.2461162 -4.2403708 -4.2269287 -4.2103887 -4.1874094 -4.1758642][-4.231267 -4.2448506 -4.2520618 -4.2512059 -4.244482 -4.2368941 -4.2285576 -4.2166367 -4.2102838 -4.2121992 -4.2133489 -4.2059722 -4.1857595 -4.1507072 -4.1359582][-4.2203956 -4.2365575 -4.2381191 -4.2283821 -4.2161822 -4.2045622 -4.1921978 -4.1771355 -4.1745615 -4.1877441 -4.2009063 -4.2006617 -4.176415 -4.1347914 -4.1112256][-4.2184887 -4.236197 -4.2305231 -4.2109022 -4.1877985 -4.17133 -4.1557364 -4.140511 -4.1437116 -4.1709995 -4.19407 -4.2002125 -4.1803451 -4.1422033 -4.1066446][-4.2136765 -4.2335262 -4.2242155 -4.1943316 -4.1617689 -4.1377015 -4.1185646 -4.1038723 -4.1136713 -4.1508975 -4.1801534 -4.1927605 -4.1822619 -4.1524105 -4.110672][-4.1975541 -4.2126083 -4.1983557 -4.162209 -4.1232514 -4.0948381 -4.0742731 -4.0643673 -4.0833735 -4.12354 -4.1561418 -4.1781673 -4.1773453 -4.1537976 -4.1154842][-4.16589 -4.173862 -4.1612487 -4.1252618 -4.084898 -4.0548511 -4.0372095 -4.0384493 -4.0640354 -4.1032882 -4.1369944 -4.166 -4.1654992 -4.14428 -4.1165867][-4.1423378 -4.1400828 -4.1230631 -4.0856867 -4.0473084 -4.0231442 -4.0147576 -4.0260124 -4.05465 -4.0951138 -4.13328 -4.1630211 -4.15776 -4.1390605 -4.1158514][-4.1415634 -4.132093 -4.1019998 -4.0608015 -4.02593 -4.0109391 -4.0141373 -4.0309997 -4.0561032 -4.094821 -4.1307354 -4.1572533 -4.1524677 -4.134501 -4.109971][-4.1549788 -4.1410112 -4.1003165 -4.0568838 -4.0276861 -4.0274611 -4.0435286 -4.0625973 -4.0798483 -4.1042333 -4.1310287 -4.1558313 -4.1547513 -4.1367726 -4.1108766][-4.1640205 -4.1507697 -4.113832 -4.0734663 -4.0482821 -4.0579371 -4.08381 -4.1032495 -4.1098919 -4.1234555 -4.14517 -4.1710615 -4.1727285 -4.1538692 -4.1252618][-4.1765828 -4.1624193 -4.1341977 -4.1046882 -4.0892057 -4.1035457 -4.1305103 -4.1498866 -4.1545453 -4.1654339 -4.1836443 -4.2041945 -4.2014251 -4.1820078 -4.1549459][-4.2071395 -4.1926661 -4.1724172 -4.1551113 -4.1506972 -4.1662016 -4.1875591 -4.2034321 -4.2134662 -4.2255287 -4.2368908 -4.2435884 -4.2371221 -4.2227793 -4.2026758][-4.24969 -4.2391391 -4.2277565 -4.2221088 -4.2246242 -4.2365608 -4.2483082 -4.2583976 -4.26842 -4.2775574 -4.282536 -4.2799468 -4.2728167 -4.2645946 -4.2538514]]...]
INFO - root - 2017-12-07 12:44:34.175663: step 9910, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 56h:47m:35s remains)
INFO - root - 2017-12-07 12:44:41.042684: step 9920, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 66h:19m:45s remains)
INFO - root - 2017-12-07 12:44:47.839133: step 9930, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 65h:36m:23s remains)
INFO - root - 2017-12-07 12:44:54.612131: step 9940, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 60h:09m:29s remains)
INFO - root - 2017-12-07 12:45:01.323608: step 9950, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 56h:19m:04s remains)
INFO - root - 2017-12-07 12:45:08.074324: step 9960, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:33m:46s remains)
INFO - root - 2017-12-07 12:45:14.968172: step 9970, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 65h:25m:55s remains)
INFO - root - 2017-12-07 12:45:21.645622: step 9980, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 60h:37m:48s remains)
INFO - root - 2017-12-07 12:45:28.264507: step 9990, loss = 2.08, batch loss = 2.02 (13.3 examples/sec; 0.600 sec/batch; 53h:43m:32s remains)
INFO - root - 2017-12-07 12:45:35.050268: step 10000, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 57h:19m:07s remains)
2017-12-07 12:45:35.759201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2225189 -4.2340422 -4.249074 -4.2606707 -4.2628036 -4.2548423 -4.2381206 -4.2239728 -4.2149782 -4.2162032 -4.2244964 -4.2362919 -4.2497339 -4.2616019 -4.2648754][-4.2397509 -4.2562957 -4.2725763 -4.2824116 -4.2858062 -4.2766757 -4.2594128 -4.2499051 -4.2488189 -4.2545681 -4.2611456 -4.2643604 -4.2712741 -4.2790027 -4.2786603][-4.2548389 -4.2717042 -4.286272 -4.2915173 -4.2872534 -4.275218 -4.2611394 -4.2586465 -4.2647352 -4.2780905 -4.2837448 -4.2843442 -4.2896967 -4.293674 -4.2870288][-4.2558851 -4.269105 -4.2796726 -4.2790585 -4.2629981 -4.2416968 -4.2241344 -4.2209654 -4.2358689 -4.2633133 -4.2809615 -4.2901859 -4.2968855 -4.2995467 -4.2872596][-4.2378416 -4.2418275 -4.243145 -4.2311788 -4.1980062 -4.1572671 -4.1265588 -4.1279249 -4.1615448 -4.2068505 -4.241396 -4.2641563 -4.276649 -4.2832255 -4.273531][-4.2162929 -4.2027264 -4.1844058 -4.1543622 -4.0999141 -4.0289435 -3.9722288 -3.993114 -4.0690136 -4.1408129 -4.1901507 -4.2245016 -4.2462912 -4.261837 -4.257606][-4.20659 -4.1707 -4.1243014 -4.0656762 -3.9775028 -3.8592427 -3.7687845 -3.8322463 -3.9611351 -4.0579824 -4.1183314 -4.1664124 -4.208756 -4.2386618 -4.242928][-4.2139583 -4.1733775 -4.115593 -4.0387797 -3.9329586 -3.7994568 -3.7136416 -3.7963808 -3.923389 -4.00519 -4.056726 -4.1114192 -4.1681018 -4.2126441 -4.2279205][-4.2238269 -4.2048974 -4.1684475 -4.1084208 -4.0248842 -3.9360676 -3.8965604 -3.9506979 -4.0166497 -4.0499864 -4.0716624 -4.1142373 -4.1669784 -4.208364 -4.2232103][-4.2192311 -4.227273 -4.2213192 -4.1944156 -4.1461515 -4.0990057 -4.0805597 -4.1039429 -4.122911 -4.1250391 -4.1353745 -4.1695685 -4.2103987 -4.2375474 -4.2440124][-4.2055578 -4.2280869 -4.2405124 -4.2321582 -4.2069006 -4.1809072 -4.1706362 -4.1766014 -4.1712289 -4.1637278 -4.1788592 -4.2135344 -4.2510695 -4.2727051 -4.2731018][-4.200314 -4.2295265 -4.2427187 -4.2376437 -4.2138996 -4.187839 -4.1750855 -4.1732006 -4.163003 -4.1583214 -4.1816053 -4.22208 -4.2645621 -4.2894421 -4.2915587][-4.2140656 -4.23629 -4.24027 -4.2289376 -4.1988006 -4.16674 -4.1498961 -4.1457224 -4.1375647 -4.1376715 -4.1637278 -4.2062659 -4.250669 -4.2798424 -4.2867956][-4.2263865 -4.245544 -4.2425814 -4.2239475 -4.1923709 -4.1598353 -4.1437783 -4.1400251 -4.1331139 -4.1306143 -4.1507382 -4.1889286 -4.231338 -4.2632031 -4.2789984][-4.2223873 -4.2432704 -4.2440271 -4.2278471 -4.1994591 -4.1658216 -4.1504316 -4.1449866 -4.1364732 -4.1287861 -4.1436491 -4.1803737 -4.2219439 -4.2541513 -4.2753072]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 12:45:42.970373: step 10010, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 59h:35m:01s remains)
INFO - root - 2017-12-07 12:45:49.756567: step 10020, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 56h:33m:38s remains)
INFO - root - 2017-12-07 12:45:56.579268: step 10030, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 64h:32m:23s remains)
INFO - root - 2017-12-07 12:46:03.391263: step 10040, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 65h:17m:33s remains)
INFO - root - 2017-12-07 12:46:10.155684: step 10050, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 60h:47m:26s remains)
INFO - root - 2017-12-07 12:46:16.951001: step 10060, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 56h:43m:14s remains)
INFO - root - 2017-12-07 12:46:23.861434: step 10070, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:14m:03s remains)
INFO - root - 2017-12-07 12:46:30.635871: step 10080, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 63h:20m:58s remains)
INFO - root - 2017-12-07 12:46:37.335581: step 10090, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 63h:00m:19s remains)
INFO - root - 2017-12-07 12:46:44.105947: step 10100, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:17m:12s remains)
2017-12-07 12:46:44.857204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2136889 -4.2160573 -4.2419491 -4.2791753 -4.3041348 -4.3224044 -4.3286214 -4.3209682 -4.3064933 -4.2863879 -4.2751408 -4.2802248 -4.2947545 -4.3049979 -4.3081026][-4.2064824 -4.1997032 -4.2169065 -4.2487712 -4.27477 -4.2982454 -4.3124871 -4.3080511 -4.293005 -4.2701468 -4.2557068 -4.2591276 -4.2730341 -4.2846651 -4.2912955][-4.2010455 -4.1827478 -4.1859765 -4.2055216 -4.2285118 -4.254509 -4.2753377 -4.2769389 -4.2672758 -4.2490106 -4.2360482 -4.2369819 -4.2451062 -4.2559185 -4.2682853][-4.1939349 -4.1653047 -4.15342 -4.1559043 -4.1677132 -4.1864376 -4.2054105 -4.2140894 -4.2176213 -4.2153845 -4.2129679 -4.2161088 -4.2193446 -4.2293043 -4.2450871][-4.1803269 -4.1411982 -4.1168671 -4.1051564 -4.0973873 -4.09509 -4.1016326 -4.115366 -4.1372776 -4.1590118 -4.17659 -4.1922612 -4.2036972 -4.2189646 -4.2355409][-4.1761832 -4.1245041 -4.0852218 -4.057745 -4.0245051 -3.9906607 -3.9714384 -3.987391 -4.0352187 -4.0891962 -4.1362519 -4.17645 -4.206181 -4.2263947 -4.2386656][-4.1831975 -4.1246529 -4.0738688 -4.0312872 -3.9714427 -3.8967323 -3.8391955 -3.8555293 -3.9342523 -4.0238957 -4.103045 -4.1694045 -4.2176971 -4.2422967 -4.2489734][-4.1961789 -4.1403213 -4.0885186 -4.040349 -3.9656599 -3.8616991 -3.7727108 -3.7836895 -3.8755171 -3.9818454 -4.0792832 -4.1606183 -4.2209058 -4.2500644 -4.2555728][-4.1965275 -4.1531458 -4.1143951 -4.0789323 -4.0206366 -3.9348006 -3.8590641 -3.8529031 -3.9126897 -3.9952843 -4.0810332 -4.1580911 -4.2169561 -4.2432241 -4.2464252][-4.175972 -4.1516128 -4.1360197 -4.1248064 -4.0955315 -4.0459967 -3.9987214 -3.9812043 -4.005383 -4.0551929 -4.1123214 -4.166718 -4.2079754 -4.2215323 -4.2181487][-4.1485481 -4.1399627 -4.1490479 -4.165525 -4.1630573 -4.1446338 -4.1213684 -4.1039133 -4.1084757 -4.13036 -4.1552939 -4.17841 -4.19505 -4.1899996 -4.1737633][-4.1345673 -4.135088 -4.1613426 -4.1985831 -4.2120981 -4.2100415 -4.2025552 -4.190464 -4.1863513 -4.1905441 -4.194057 -4.1932459 -4.1876049 -4.1641083 -4.132936][-4.1464772 -4.1543689 -4.1898379 -4.2352252 -4.2534108 -4.2539997 -4.247108 -4.2344446 -4.22364 -4.2163644 -4.2064061 -4.1920385 -4.1762357 -4.1490154 -4.115799][-4.1931262 -4.2053118 -4.2387748 -4.2776055 -4.288373 -4.2793941 -4.2626462 -4.2413721 -4.2194481 -4.199121 -4.1812272 -4.1625881 -4.1454268 -4.1284037 -4.1119208][-4.24721 -4.2584949 -4.2855668 -4.3117962 -4.3117604 -4.2948627 -4.2710571 -4.240581 -4.2054415 -4.1710386 -4.1476622 -4.1300044 -4.1172447 -4.1161261 -4.1225739]]...]
INFO - root - 2017-12-07 12:46:51.678371: step 10110, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 64h:52m:16s remains)
INFO - root - 2017-12-07 12:46:58.414507: step 10120, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 59h:38m:22s remains)
INFO - root - 2017-12-07 12:47:05.134124: step 10130, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 57h:47m:33s remains)
INFO - root - 2017-12-07 12:47:11.933655: step 10140, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:15m:04s remains)
INFO - root - 2017-12-07 12:47:18.841084: step 10150, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 66h:09m:16s remains)
INFO - root - 2017-12-07 12:47:25.529447: step 10160, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 59h:45m:25s remains)
INFO - root - 2017-12-07 12:47:32.297178: step 10170, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 55h:46m:37s remains)
INFO - root - 2017-12-07 12:47:39.094735: step 10180, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 58h:48m:24s remains)
INFO - root - 2017-12-07 12:47:45.740011: step 10190, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 62h:36m:27s remains)
INFO - root - 2017-12-07 12:47:52.526379: step 10200, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 60h:44m:42s remains)
2017-12-07 12:47:53.211817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584381 -4.2420087 -4.2312655 -4.221036 -4.2127247 -4.2021742 -4.1901379 -4.1816373 -4.17588 -4.1761751 -4.1684971 -4.1599436 -4.15428 -4.1408892 -4.1355181][-4.237155 -4.218111 -4.2042503 -4.1913376 -4.1848226 -4.1766205 -4.1665349 -4.154758 -4.1469293 -4.1450367 -4.1349297 -4.1267948 -4.1204491 -4.1080475 -4.1064925][-4.2095757 -4.1916409 -4.175467 -4.1641889 -4.1601439 -4.1525059 -4.1393642 -4.1261239 -4.12138 -4.1230426 -4.1163588 -4.1129932 -4.10833 -4.1006227 -4.1041822][-4.1884456 -4.17514 -4.1616015 -4.1540575 -4.1525183 -4.1404982 -4.115479 -4.0974493 -4.0977907 -4.1039329 -4.1014738 -4.0997305 -4.0975146 -4.0970259 -4.1038294][-4.1771626 -4.1665025 -4.1563921 -4.1509738 -4.1514378 -4.1313429 -4.0891795 -4.0640326 -4.0660996 -4.0712366 -4.0690308 -4.0689278 -4.074244 -4.0803704 -4.0904803][-4.1655807 -4.1567378 -4.1492882 -4.1385517 -4.1297722 -4.0910025 -4.0272093 -4.0009422 -4.017787 -4.0301027 -4.0332255 -4.0378704 -4.0500059 -4.0572681 -4.0723825][-4.1501417 -4.1399803 -4.1301737 -4.1043339 -4.0757451 -4.0140224 -3.9297774 -3.9177871 -3.9648168 -3.9957786 -4.0144734 -4.0316367 -4.0509338 -4.0606384 -4.0822921][-4.1066561 -4.0828323 -4.0636907 -4.025321 -3.9830287 -3.9164419 -3.8435001 -3.8666739 -3.9438517 -3.9962983 -4.0308285 -4.0539403 -4.0728307 -4.0821691 -4.1048422][-4.0583544 -4.0209365 -3.9959121 -3.966079 -3.9386511 -3.899791 -3.8674226 -3.9095569 -3.9818575 -4.0281782 -4.0595059 -4.0780196 -4.0913382 -4.1013055 -4.1244321][-4.0570354 -4.0118217 -3.9880097 -3.9758725 -3.9667845 -3.9529471 -3.9503319 -3.9910784 -4.0396676 -4.0677619 -4.0867777 -4.0994916 -4.1088309 -4.1204138 -4.1459327][-4.1029687 -4.06073 -4.0365524 -4.0331454 -4.0342121 -4.033504 -4.03848 -4.06675 -4.093122 -4.1050434 -4.1150327 -4.1269317 -4.1383467 -4.1514544 -4.175015][-4.1699786 -4.1338606 -4.1079617 -4.1029534 -4.1057682 -4.1082611 -4.113481 -4.1328149 -4.1485081 -4.1543179 -4.1614814 -4.1720238 -4.183466 -4.195353 -4.2132387][-4.2325554 -4.2027922 -4.181509 -4.1764374 -4.178802 -4.1820235 -4.187911 -4.20246 -4.21241 -4.2163072 -4.2211695 -4.228107 -4.236659 -4.2444115 -4.2560897][-4.2876806 -4.2676435 -4.25305 -4.249897 -4.2538381 -4.2601781 -4.26731 -4.2764368 -4.2800751 -4.2805629 -4.2818923 -4.2847257 -4.2890396 -4.2928138 -4.2994394][-4.3212442 -4.3115897 -4.3049173 -4.3049283 -4.3090281 -4.3151965 -4.3207111 -4.3252435 -4.3262105 -4.3259549 -4.3260384 -4.3265438 -4.3282475 -4.3299532 -4.3326283]]...]
INFO - root - 2017-12-07 12:48:00.033900: step 10210, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 60h:40m:13s remains)
INFO - root - 2017-12-07 12:48:06.950262: step 10220, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 62h:40m:02s remains)
INFO - root - 2017-12-07 12:48:13.745549: step 10230, loss = 2.11, batch loss = 2.05 (11.7 examples/sec; 0.683 sec/batch; 61h:07m:28s remains)
INFO - root - 2017-12-07 12:48:20.566488: step 10240, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:19m:52s remains)
INFO - root - 2017-12-07 12:48:27.372575: step 10250, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 59h:15m:33s remains)
INFO - root - 2017-12-07 12:48:34.211136: step 10260, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 64h:52m:39s remains)
INFO - root - 2017-12-07 12:48:41.064097: step 10270, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 64h:57m:00s remains)
INFO - root - 2017-12-07 12:48:47.768003: step 10280, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.659 sec/batch; 58h:57m:27s remains)
INFO - root - 2017-12-07 12:48:54.341342: step 10290, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 57h:25m:54s remains)
INFO - root - 2017-12-07 12:49:01.058223: step 10300, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 57h:08m:45s remains)
2017-12-07 12:49:01.741614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1629691 -4.1670423 -4.1786084 -4.1865978 -4.1884441 -4.1888518 -4.1896 -4.1861696 -4.1809611 -4.1760755 -4.17265 -4.1726017 -4.1798248 -4.1923294 -4.2026033][-4.1283941 -4.1400695 -4.15604 -4.165123 -4.1650586 -4.1646409 -4.1654816 -4.1604085 -4.1532388 -4.1493077 -4.14688 -4.1462493 -4.1517453 -4.1600461 -4.1625638][-4.1205316 -4.134141 -4.1466861 -4.1505294 -4.1486244 -4.1500316 -4.1556325 -4.1546235 -4.1482263 -4.146112 -4.1462164 -4.145535 -4.1515517 -4.1575956 -4.1523719][-4.1218648 -4.135282 -4.1427178 -4.1391511 -4.1330676 -4.1343794 -4.1408939 -4.1412973 -4.13632 -4.1344957 -4.1330147 -4.1316466 -4.1411495 -4.1495104 -4.140666][-4.1255522 -4.1422286 -4.1473107 -4.1367993 -4.1262217 -4.1206717 -4.1186342 -4.1134 -4.1080084 -4.1050367 -4.0989113 -4.0926881 -4.1042786 -4.1178904 -4.110549][-4.1318312 -4.1485996 -4.15146 -4.1360126 -4.11933 -4.1014485 -4.0839219 -4.0672555 -4.0562067 -4.0493951 -4.040904 -4.0335732 -4.0468159 -4.0676866 -4.0695114][-4.1045251 -4.1156445 -4.1125298 -4.0895667 -4.0660934 -4.0401168 -4.0151215 -3.9925933 -3.9765325 -3.9706533 -3.9667056 -3.9643846 -3.9807065 -4.0096607 -4.023479][-4.0701761 -4.0720754 -4.0629029 -4.0359979 -4.011497 -3.99082 -3.9745905 -3.9580669 -3.9449077 -3.9428296 -3.9459538 -3.9452429 -3.9582729 -3.9883695 -4.0101871][-4.095232 -4.0923123 -4.0847669 -4.0643454 -4.0451269 -4.0297985 -4.02136 -4.0141215 -4.0088911 -4.0114522 -4.0177994 -4.0122194 -4.0143967 -4.0372009 -4.0591359][-4.1612673 -4.1582508 -4.1568532 -4.1476789 -4.1339626 -4.1202893 -4.1126475 -4.1090193 -4.1081009 -4.11059 -4.1162038 -4.108623 -4.1010942 -4.1118212 -4.12727][-4.2076244 -4.2059588 -4.2080612 -4.2052283 -4.1975522 -4.1885395 -4.1834774 -4.18102 -4.1790447 -4.1795297 -4.1829371 -4.1764107 -4.1670017 -4.1720705 -4.1829338][-4.2211761 -4.2194819 -4.2205648 -4.221015 -4.2188435 -4.214396 -4.209847 -4.2040863 -4.1976171 -4.1944561 -4.1948285 -4.1914825 -4.1855264 -4.1883988 -4.1954727][-4.209517 -4.2109227 -4.2145925 -4.2201 -4.2238822 -4.2246985 -4.2225795 -4.2169423 -4.2079244 -4.1997881 -4.1973834 -4.196321 -4.1927361 -4.1941571 -4.1997313][-4.2083793 -4.2122631 -4.2169557 -4.2267265 -4.2370539 -4.2444563 -4.2475591 -4.2463193 -4.2404084 -4.2324777 -4.2272978 -4.2249622 -4.2210312 -4.2214165 -4.225338][-4.2387075 -4.2410517 -4.2440076 -4.2536774 -4.2647047 -4.2737927 -4.2799206 -4.2823372 -4.2804327 -4.2747335 -4.2687678 -4.2637773 -4.2576251 -4.254323 -4.2560797]]...]
INFO - root - 2017-12-07 12:49:08.550669: step 10310, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 58h:34m:24s remains)
INFO - root - 2017-12-07 12:49:15.408735: step 10320, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 57h:54m:20s remains)
INFO - root - 2017-12-07 12:49:22.242486: step 10330, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 57h:43m:08s remains)
INFO - root - 2017-12-07 12:49:29.119144: step 10340, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 64h:35m:12s remains)
INFO - root - 2017-12-07 12:49:35.837409: step 10350, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 64h:17m:58s remains)
INFO - root - 2017-12-07 12:49:42.602062: step 10360, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:12m:45s remains)
INFO - root - 2017-12-07 12:49:49.393126: step 10370, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.633 sec/batch; 56h:36m:21s remains)
INFO - root - 2017-12-07 12:49:56.181375: step 10380, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:35m:12s remains)
INFO - root - 2017-12-07 12:50:02.838749: step 10390, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 66h:12m:29s remains)
INFO - root - 2017-12-07 12:50:09.716348: step 10400, loss = 2.05, batch loss = 2.00 (10.6 examples/sec; 0.755 sec/batch; 67h:31m:03s remains)
2017-12-07 12:50:10.343160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3376021 -4.3384705 -4.3329535 -4.3165755 -4.2913623 -4.268641 -4.2563853 -4.2637796 -4.2890186 -4.3132458 -4.3272719 -4.3337045 -4.3358545 -4.3353343 -4.3347616][-4.3394508 -4.3400736 -4.3327765 -4.3128948 -4.2842031 -4.2583127 -4.2466764 -4.2586904 -4.2887158 -4.3156643 -4.3317108 -4.3388653 -4.3395948 -4.3372288 -4.3353386][-4.3407116 -4.3391557 -4.327817 -4.30346 -4.2703462 -4.2406363 -4.2295141 -4.2459164 -4.2805219 -4.3107996 -4.3301244 -4.3399053 -4.3410473 -4.3381248 -4.3356004][-4.3406734 -4.3360977 -4.3201447 -4.2907472 -4.2507954 -4.2165766 -4.2067838 -4.2273741 -4.2663636 -4.3008332 -4.3233547 -4.3366537 -4.3406386 -4.3391771 -4.3373275][-4.3289852 -4.3207765 -4.2975526 -4.2576613 -4.2084165 -4.1717873 -4.1667194 -4.1928859 -4.2378969 -4.2802649 -4.3103943 -4.3300896 -4.33908 -4.3405833 -4.3398089][-4.29908 -4.2844577 -4.2486959 -4.194221 -4.1367521 -4.1012821 -4.1032448 -4.1358223 -4.1851959 -4.2372813 -4.2793484 -4.30934 -4.3277903 -4.3361211 -4.3392143][-4.2471728 -4.2233095 -4.1732464 -4.1079278 -4.0508189 -4.0239167 -4.0318494 -4.0652404 -4.1109796 -4.1695805 -4.2276897 -4.2739778 -4.3060451 -4.3239369 -4.3326478][-4.1928849 -4.1581597 -4.0970798 -4.0277686 -3.9788222 -3.9649456 -3.9796176 -4.0079975 -4.0402746 -4.0968757 -4.16761 -4.2298317 -4.2778492 -4.3075953 -4.3232832][-4.1639609 -4.1237335 -4.0610681 -3.9976692 -3.9593349 -3.9550776 -3.971684 -3.9900384 -4.0062742 -4.0537391 -4.1265388 -4.1975818 -4.2571135 -4.2959981 -4.317831][-4.158864 -4.1199956 -4.0668154 -4.01914 -3.9927807 -3.9938211 -4.0069361 -4.0122256 -4.0139313 -4.0517206 -4.119751 -4.1895866 -4.2521195 -4.2949438 -4.3187113][-4.1710038 -4.1376939 -4.09879 -4.0665607 -4.0508752 -4.0537386 -4.0601268 -4.05478 -4.0474172 -4.0760765 -4.1339254 -4.1958976 -4.2558804 -4.298605 -4.3212328][-4.2006674 -4.1751046 -4.1498322 -4.1317887 -4.1254063 -4.1301417 -4.1331987 -4.12339 -4.1116972 -4.1298523 -4.1716433 -4.2195706 -4.2684751 -4.3050346 -4.3230643][-4.2350159 -4.2205586 -4.2083545 -4.2024775 -4.2037663 -4.209549 -4.2112904 -4.2025676 -4.1907959 -4.198791 -4.2247849 -4.2579079 -4.2922273 -4.3168736 -4.32588][-4.2631721 -4.255609 -4.2516303 -4.2545376 -4.2620907 -4.26926 -4.2718248 -4.266602 -4.2572789 -4.2585397 -4.2728233 -4.2940173 -4.314847 -4.327724 -4.3280411][-4.2877469 -4.2833605 -4.2829518 -4.2880878 -4.295146 -4.3012161 -4.3036275 -4.3005886 -4.2942433 -4.2932348 -4.3001757 -4.3124251 -4.3241167 -4.3290563 -4.3251534]]...]
INFO - root - 2017-12-07 12:50:17.034489: step 10410, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 56h:59m:07s remains)
INFO - root - 2017-12-07 12:50:23.811571: step 10420, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 65h:43m:39s remains)
INFO - root - 2017-12-07 12:50:30.644391: step 10430, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 62h:03m:21s remains)
INFO - root - 2017-12-07 12:50:37.408049: step 10440, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 59h:46m:30s remains)
INFO - root - 2017-12-07 12:50:44.162001: step 10450, loss = 2.08, batch loss = 2.03 (12.8 examples/sec; 0.627 sec/batch; 56h:06m:32s remains)
INFO - root - 2017-12-07 12:50:50.881859: step 10460, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 57h:55m:42s remains)
INFO - root - 2017-12-07 12:50:57.517467: step 10470, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.747 sec/batch; 66h:51m:34s remains)
INFO - root - 2017-12-07 12:51:04.217106: step 10480, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 60h:26m:27s remains)
INFO - root - 2017-12-07 12:51:10.847207: step 10490, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:24m:23s remains)
INFO - root - 2017-12-07 12:51:17.551718: step 10500, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 57h:19m:39s remains)
2017-12-07 12:51:18.430078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2800179 -4.2694173 -4.2619572 -4.2560096 -4.2615242 -4.2738957 -4.28699 -4.2962661 -4.2958183 -4.2903442 -4.2855616 -4.2844048 -4.2804589 -4.26558 -4.2338066][-4.2518744 -4.24068 -4.234561 -4.2317667 -4.2416248 -4.2583041 -4.27365 -4.282599 -4.2792296 -4.2712045 -4.2671814 -4.2696347 -4.2651587 -4.24578 -4.2111053][-4.2223315 -4.2166452 -4.2163606 -4.2174497 -4.2282372 -4.2439833 -4.2574697 -4.264348 -4.2602506 -4.2515354 -4.2498856 -4.2543874 -4.2482347 -4.2245846 -4.19046][-4.187129 -4.1943693 -4.2033563 -4.2085066 -4.2208633 -4.236444 -4.2482171 -4.2515206 -4.2445645 -4.2338824 -4.2319322 -4.2334657 -4.2230043 -4.1986079 -4.1718154][-4.1539383 -4.1734242 -4.1904354 -4.2014804 -4.2186317 -4.2362752 -4.2441497 -4.2358913 -4.2173858 -4.2031741 -4.201323 -4.2006869 -4.1898155 -4.1716547 -4.1597171][-4.1354356 -4.1582971 -4.1764112 -4.1903834 -4.2108893 -4.2271957 -4.2261391 -4.2005959 -4.1676693 -4.1547942 -4.1604304 -4.1646752 -4.16335 -4.1607714 -4.1693635][-4.1472945 -4.1594734 -4.1668534 -4.175653 -4.1907883 -4.2012448 -4.1896939 -4.1472631 -4.1070957 -4.1046009 -4.1223478 -4.1381488 -4.1533675 -4.1694536 -4.1936584][-4.1820507 -4.1770229 -4.1702352 -4.1678967 -4.1713324 -4.1743956 -4.1511135 -4.0977683 -4.0617971 -4.078371 -4.1123595 -4.1438255 -4.1757035 -4.2024088 -4.2289553][-4.2089291 -4.19103 -4.1758847 -4.1654415 -4.1602955 -4.1579614 -4.1314435 -4.0805173 -4.0619555 -4.1021795 -4.1483521 -4.1849155 -4.2183414 -4.2417917 -4.2626553][-4.2091022 -4.1871905 -4.1725168 -4.1627016 -4.1588769 -4.1618371 -4.1462274 -4.1110096 -4.1119428 -4.1570778 -4.197125 -4.2253003 -4.248632 -4.2642674 -4.27941][-4.1966228 -4.17741 -4.168077 -4.1654835 -4.1708412 -4.1851411 -4.1838069 -4.1658297 -4.1709375 -4.2010379 -4.2234969 -4.2397604 -4.2538443 -4.2648745 -4.2757998][-4.1933427 -4.1815958 -4.1790357 -4.1847086 -4.1970382 -4.2153516 -4.2200522 -4.2091513 -4.2058082 -4.2157512 -4.223309 -4.2318068 -4.2415977 -4.2500548 -4.2564526][-4.1994834 -4.1929231 -4.1939878 -4.2008953 -4.212975 -4.2276425 -4.2322364 -4.2253089 -4.2164211 -4.2128444 -4.212852 -4.2176218 -4.2214112 -4.2249603 -4.2279058][-4.2042627 -4.1999788 -4.2007184 -4.2032151 -4.2083282 -4.2153177 -4.2172232 -4.2144833 -4.2067013 -4.1997943 -4.2002211 -4.2035108 -4.2014732 -4.1995568 -4.1996951][-4.2085962 -4.2055392 -4.2064457 -4.2059989 -4.2057385 -4.2047863 -4.2054915 -4.2024989 -4.1943879 -4.1897206 -4.1939449 -4.1957912 -4.1911159 -4.1862564 -4.1855965]]...]
INFO - root - 2017-12-07 12:51:25.228132: step 10510, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 58h:01m:23s remains)
INFO - root - 2017-12-07 12:51:31.954665: step 10520, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.624 sec/batch; 55h:50m:40s remains)
INFO - root - 2017-12-07 12:51:38.783915: step 10530, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.752 sec/batch; 67h:13m:37s remains)
INFO - root - 2017-12-07 12:51:45.647555: step 10540, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 62h:48m:07s remains)
INFO - root - 2017-12-07 12:51:52.389113: step 10550, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 60h:44m:36s remains)
INFO - root - 2017-12-07 12:51:59.208018: step 10560, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 58h:03m:43s remains)
INFO - root - 2017-12-07 12:52:05.984393: step 10570, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 60h:02m:47s remains)
INFO - root - 2017-12-07 12:52:12.846028: step 10580, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 62h:14m:04s remains)
INFO - root - 2017-12-07 12:52:19.549012: step 10590, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 63h:17m:41s remains)
INFO - root - 2017-12-07 12:52:26.293105: step 10600, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:15m:44s remains)
2017-12-07 12:52:27.036932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2679157 -4.1885452 -4.0927033 -4.008522 -4.0255456 -4.1314411 -4.2163124 -4.2526484 -4.2697096 -4.262928 -4.2322083 -4.2011223 -4.1904092 -4.1995478 -4.2242408][-4.2726674 -4.2105613 -4.1379833 -4.0708103 -4.0884504 -4.1752524 -4.237967 -4.2631269 -4.2835627 -4.28142 -4.2503772 -4.2195687 -4.2175179 -4.2242665 -4.2314925][-4.2659826 -4.2261243 -4.1805944 -4.1308537 -4.1408911 -4.2007456 -4.232008 -4.2413216 -4.2682085 -4.2816973 -4.2674417 -4.2519317 -4.2598333 -4.26287 -4.2509155][-4.2535019 -4.232101 -4.2024488 -4.158041 -4.1479249 -4.1764154 -4.178021 -4.1708884 -4.2105179 -4.2473011 -4.2611251 -4.2720575 -4.2927437 -4.2976403 -4.2752323][-4.2402215 -4.2359362 -4.2130222 -4.1689987 -4.13851 -4.1274867 -4.0893707 -4.0704961 -4.129045 -4.1884766 -4.2280178 -4.2657671 -4.2996435 -4.3095522 -4.2816653][-4.2108846 -4.2324419 -4.218667 -4.16658 -4.1033425 -4.0386105 -3.9467082 -3.9231253 -4.0277219 -4.1263127 -4.1919227 -4.2432771 -4.2821913 -4.2991662 -4.272016][-4.1522908 -4.198329 -4.2013836 -4.1521382 -4.0539217 -3.9169817 -3.745456 -3.7062292 -3.8767009 -4.0433707 -4.1479635 -4.2186589 -4.261898 -4.2837873 -4.2565775][-4.0838351 -4.1390843 -4.1570649 -4.1241741 -4.0113769 -3.8061874 -3.5370686 -3.4498 -3.6631632 -3.905561 -4.073915 -4.182961 -4.2436547 -4.2758136 -4.2566466][-4.0575123 -4.0991812 -4.1150212 -4.0986843 -4.0099454 -3.7965646 -3.4882705 -3.3378322 -3.5177324 -3.7883024 -4.0038447 -4.1516228 -4.2336483 -4.2758102 -4.2687593][-4.0956993 -4.1115208 -4.1123614 -4.1079292 -4.0591264 -3.8974354 -3.6457458 -3.4985287 -3.5965497 -3.8146112 -4.0119233 -4.1593304 -4.2442789 -4.2850094 -4.2865567][-4.1328759 -4.12586 -4.1216269 -4.1340723 -4.1256242 -4.0304728 -3.865808 -3.7560377 -3.7995634 -3.9422176 -4.0844679 -4.2023988 -4.2756042 -4.3048153 -4.3113432][-4.1257906 -4.0950956 -4.0924525 -4.1345 -4.1646791 -4.1281791 -4.0329185 -3.9607105 -3.9842367 -4.0746922 -4.17191 -4.2547855 -4.3094454 -4.3274717 -4.3343282][-4.0936933 -4.0356216 -4.0262594 -4.0877924 -4.1483326 -4.163559 -4.1234136 -4.0888867 -4.113874 -4.176085 -4.244235 -4.2973061 -4.3305621 -4.3415904 -4.348208][-4.0476694 -3.9661894 -3.9473808 -4.0150166 -4.0963626 -4.1507406 -4.1595197 -4.1646533 -4.1965852 -4.244916 -4.2898164 -4.323523 -4.3415337 -4.3478532 -4.3514233][-4.0371728 -3.9509621 -3.9233859 -3.9760418 -4.062129 -4.1442971 -4.18976 -4.2177973 -4.2513347 -4.2894545 -4.3147168 -4.3334475 -4.343821 -4.347373 -4.3478103]]...]
INFO - root - 2017-12-07 12:52:33.796202: step 10610, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 65h:37m:20s remains)
INFO - root - 2017-12-07 12:52:40.526185: step 10620, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 60h:29m:26s remains)
INFO - root - 2017-12-07 12:52:47.257767: step 10630, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 56h:10m:20s remains)
INFO - root - 2017-12-07 12:52:54.001420: step 10640, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.666 sec/batch; 59h:31m:41s remains)
INFO - root - 2017-12-07 12:53:00.836148: step 10650, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 65h:01m:10s remains)
INFO - root - 2017-12-07 12:53:07.587073: step 10660, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:09m:44s remains)
INFO - root - 2017-12-07 12:53:14.382753: step 10670, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:10m:29s remains)
INFO - root - 2017-12-07 12:53:21.164773: step 10680, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 55h:46m:52s remains)
INFO - root - 2017-12-07 12:53:27.788807: step 10690, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 58h:58m:46s remains)
INFO - root - 2017-12-07 12:53:34.581032: step 10700, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 64h:11m:49s remains)
2017-12-07 12:53:35.346589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1856837 -4.20298 -4.2264285 -4.2484875 -4.2654681 -4.2750783 -4.2803831 -4.2811289 -4.2765732 -4.2697792 -4.2629967 -4.2596126 -4.2569952 -4.2551851 -4.2530775][-4.20182 -4.2186046 -4.2403955 -4.2602668 -4.2739153 -4.2801814 -4.2831717 -4.2829885 -4.2781682 -4.2708383 -4.2629442 -4.2581806 -4.2552147 -4.2539492 -4.2540746][-4.2276387 -4.2414217 -4.2572432 -4.2697358 -4.2759352 -4.2771988 -4.2778053 -4.2770257 -4.2727456 -4.265852 -4.2584486 -4.2540212 -4.2521524 -4.2523966 -4.254807][-4.2500191 -4.2561111 -4.2617979 -4.2642579 -4.2629161 -4.2607055 -4.2600913 -4.260519 -4.2593188 -4.2557011 -4.2511282 -4.2488079 -4.2487636 -4.2506747 -4.2545834][-4.2645221 -4.2602925 -4.2546678 -4.2466679 -4.2383714 -4.2335982 -4.233603 -4.2377014 -4.2426949 -4.2460804 -4.2471585 -4.2490354 -4.2519207 -4.2552638 -4.2595253][-4.2737856 -4.2622285 -4.2484436 -4.2329707 -4.2200317 -4.2138734 -4.2154236 -4.2235975 -4.234303 -4.2440414 -4.2506013 -4.255928 -4.2611909 -4.2660022 -4.2703896][-4.2784314 -4.265728 -4.2504435 -4.2340474 -4.2212386 -4.2156577 -4.2185407 -4.2280087 -4.2393703 -4.2493949 -4.2561812 -4.2614026 -4.2668366 -4.27287 -4.2785425][-4.2799377 -4.2702289 -4.2575712 -4.2436404 -4.2330661 -4.2289872 -4.2322774 -4.2400675 -4.2478566 -4.2537184 -4.2571025 -4.2601361 -4.2643814 -4.2703505 -4.2773528][-4.2798309 -4.2729826 -4.2634611 -4.25268 -4.2447281 -4.2424254 -4.2453713 -4.2500935 -4.2535491 -4.2552891 -4.2556415 -4.2564011 -4.2589307 -4.2632589 -4.2697892][-4.2803245 -4.2756925 -4.2691145 -4.2615824 -4.2563457 -4.25541 -4.257772 -4.2601962 -4.2610617 -4.2608347 -4.2598333 -4.2592545 -4.2600989 -4.2624183 -4.2670064][-4.2823477 -4.2791705 -4.2750115 -4.2703 -4.2671556 -4.26705 -4.2689052 -4.2701769 -4.2700753 -4.2694077 -4.2684059 -4.2674694 -4.2675753 -4.2688794 -4.27174][-4.2840996 -4.2818823 -4.2793407 -4.2766109 -4.2747746 -4.2749653 -4.2764049 -4.2773991 -4.277647 -4.2775869 -4.2773361 -4.2767458 -4.2765636 -4.2772441 -4.2786593][-4.2844577 -4.2826829 -4.2811828 -4.2797394 -4.2787685 -4.2791619 -4.2804055 -4.2812076 -4.2814965 -4.281867 -4.2821965 -4.2819471 -4.2815008 -4.2813749 -4.2813387][-4.2843046 -4.2828794 -4.2820144 -4.2813635 -4.280952 -4.2813711 -4.2822566 -4.2824707 -4.2821584 -4.2820768 -4.2821941 -4.2817926 -4.2808943 -4.2801533 -4.2794051][-4.2834988 -4.283164 -4.2834611 -4.2839222 -4.2843404 -4.2850413 -4.2856359 -4.2852669 -4.28403 -4.2827978 -4.2816706 -4.2801347 -4.2781382 -4.2764654 -4.2753525]]...]
INFO - root - 2017-12-07 12:53:42.186427: step 10710, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 55h:54m:40s remains)
INFO - root - 2017-12-07 12:53:48.992508: step 10720, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 65h:18m:26s remains)
INFO - root - 2017-12-07 12:53:55.785747: step 10730, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 64h:24m:13s remains)
INFO - root - 2017-12-07 12:54:02.678215: step 10740, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 57h:46m:52s remains)
INFO - root - 2017-12-07 12:54:09.443282: step 10750, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 56h:30m:04s remains)
INFO - root - 2017-12-07 12:54:16.279348: step 10760, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 61h:48m:01s remains)
INFO - root - 2017-12-07 12:54:23.128984: step 10770, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.717 sec/batch; 64h:04m:47s remains)
INFO - root - 2017-12-07 12:54:29.809163: step 10780, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 60h:05m:34s remains)
INFO - root - 2017-12-07 12:54:36.440721: step 10790, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 56h:40m:32s remains)
INFO - root - 2017-12-07 12:54:43.239574: step 10800, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 63h:54m:55s remains)
2017-12-07 12:54:43.995506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.239759 -4.2557845 -4.2649736 -4.260066 -4.242229 -4.2268372 -4.230093 -4.2440376 -4.2444258 -4.2338343 -4.21557 -4.2058897 -4.2062478 -4.2014017 -4.2001863][-4.2169414 -4.2347503 -4.249382 -4.2529039 -4.2399373 -4.2253866 -4.2288961 -4.2380457 -4.2349977 -4.2231464 -4.20246 -4.1946282 -4.2081161 -4.2161736 -4.2171297][-4.192306 -4.2092967 -4.2271328 -4.2359939 -4.2278042 -4.215672 -4.2181711 -4.2224426 -4.2186661 -4.2061348 -4.1833262 -4.1747427 -4.1960683 -4.2154193 -4.2202806][-4.1881614 -4.2034178 -4.2249632 -4.2371569 -4.2340379 -4.2247734 -4.2247972 -4.2262135 -4.2204781 -4.2022634 -4.174233 -4.1599216 -4.1773763 -4.19862 -4.2052417][-4.202507 -4.2121353 -4.2353339 -4.2474775 -4.2453957 -4.23801 -4.2351861 -4.2335806 -4.2259021 -4.2047143 -4.1742034 -4.1562614 -4.1640673 -4.1744456 -4.1766305][-4.21689 -4.2163105 -4.2330661 -4.2419586 -4.2419691 -4.2365556 -4.229907 -4.2225046 -4.2139969 -4.1969409 -4.1750903 -4.162776 -4.1613259 -4.1550555 -4.149209][-4.227253 -4.2144842 -4.218092 -4.219429 -4.2195249 -4.2129683 -4.1955991 -4.175683 -4.1619644 -4.152873 -4.150897 -4.157114 -4.158483 -4.1464214 -4.1386271][-4.2326541 -4.2110071 -4.2004566 -4.1928639 -4.1891322 -4.1776648 -4.1512723 -4.1171503 -4.0943694 -4.0911789 -4.1093974 -4.1374989 -4.150126 -4.1441932 -4.1437359][-4.2292128 -4.2065082 -4.1932545 -4.1844177 -4.1822829 -4.1759052 -4.1547203 -4.1179628 -4.0902333 -4.0910487 -4.1169858 -4.1483221 -4.16116 -4.1574173 -4.1659522][-4.2245245 -4.2113152 -4.2062979 -4.2047009 -4.2097373 -4.2141366 -4.2038846 -4.177 -4.1522985 -4.150878 -4.1666975 -4.1817236 -4.18348 -4.17558 -4.1865692][-4.2281976 -4.2228889 -4.226233 -4.2308855 -4.2374444 -4.2423849 -4.2377887 -4.2228074 -4.2062697 -4.207427 -4.2155 -4.2121024 -4.19986 -4.1876564 -4.1945562][-4.2451892 -4.24372 -4.2492619 -4.2549939 -4.2573085 -4.2574272 -4.2502737 -4.2388153 -4.2297034 -4.2361703 -4.2407026 -4.2265124 -4.2065325 -4.1902056 -4.1881628][-4.2673588 -4.2705297 -4.276001 -4.2803655 -4.2777686 -4.270227 -4.2550087 -4.2330337 -4.2244062 -4.2394686 -4.25027 -4.2330132 -4.208518 -4.1910496 -4.1833754][-4.2774992 -4.2841268 -4.2894468 -4.2922807 -4.2897506 -4.2780123 -4.2537103 -4.21863 -4.2073345 -4.2300515 -4.2483978 -4.236166 -4.2133107 -4.1986165 -4.1907587][-4.2806506 -4.2862835 -4.2890978 -4.2892022 -4.2878675 -4.2766519 -4.2478986 -4.2074404 -4.19604 -4.2205176 -4.2389679 -4.2320247 -4.2163568 -4.2100992 -4.207006]]...]
INFO - root - 2017-12-07 12:54:50.736337: step 10810, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:11m:06s remains)
INFO - root - 2017-12-07 12:54:57.443538: step 10820, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 56h:44m:37s remains)
INFO - root - 2017-12-07 12:55:04.181769: step 10830, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 61h:10m:44s remains)
INFO - root - 2017-12-07 12:55:11.126529: step 10840, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 0.766 sec/batch; 68h:28m:43s remains)
INFO - root - 2017-12-07 12:55:17.888121: step 10850, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.657 sec/batch; 58h:42m:27s remains)
INFO - root - 2017-12-07 12:55:24.745304: step 10860, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 57h:21m:10s remains)
INFO - root - 2017-12-07 12:55:31.656434: step 10870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 60h:36m:19s remains)
INFO - root - 2017-12-07 12:55:38.542638: step 10880, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 63h:48m:14s remains)
INFO - root - 2017-12-07 12:55:45.167503: step 10890, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:04m:05s remains)
INFO - root - 2017-12-07 12:55:51.887303: step 10900, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 55h:20m:25s remains)
2017-12-07 12:55:52.565475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2359309 -4.2230864 -4.2240939 -4.2529387 -4.2984762 -4.3293524 -4.3449659 -4.3562984 -4.3447866 -4.3076305 -4.2586393 -4.213623 -4.1703525 -4.1414719 -4.1462088][-4.2036805 -4.187407 -4.1931505 -4.2266431 -4.2679858 -4.295085 -4.3156781 -4.3376641 -4.3345361 -4.3031616 -4.2574019 -4.2111259 -4.1677089 -4.1449475 -4.158854][-4.1704121 -4.1520581 -4.1615844 -4.1955152 -4.2281971 -4.2477288 -4.2706256 -4.2999806 -4.3070073 -4.2864995 -4.2518368 -4.2133622 -4.1762691 -4.1574497 -4.1712236][-4.14369 -4.1337276 -4.1520057 -4.1822963 -4.2049422 -4.2179227 -4.2394524 -4.2670865 -4.2737474 -4.2561884 -4.2306776 -4.2042351 -4.1802583 -4.1712713 -4.1861963][-4.1151943 -4.1164517 -4.1449103 -4.1717973 -4.1838703 -4.1935472 -4.2179518 -4.2401953 -4.2383051 -4.2173352 -4.2011724 -4.1908412 -4.1806688 -4.1801014 -4.1984439][-4.0949283 -4.1059279 -4.1388569 -4.1607056 -4.1644235 -4.1724591 -4.1927853 -4.2001595 -4.1818457 -4.1545095 -4.1469812 -4.154953 -4.1579628 -4.1674023 -4.1935635][-4.1083603 -4.1209016 -4.1469131 -4.1624889 -4.16471 -4.1704025 -4.1772408 -4.160212 -4.1153641 -4.0748105 -4.07565 -4.0992169 -4.11529 -4.1332693 -4.166297][-4.1342072 -4.1454997 -4.1656723 -4.1786075 -4.1828327 -4.1831059 -4.1745186 -4.1339273 -4.0663986 -4.0121307 -4.019845 -4.0610805 -4.0890689 -4.1100225 -4.1468029][-4.1490293 -4.1662469 -4.1922159 -4.2135329 -4.2171645 -4.2063036 -4.1832666 -4.1308703 -4.0506506 -3.9885726 -4.0003686 -4.0566287 -4.0926437 -4.1135759 -4.15099][-4.16168 -4.184155 -4.2108741 -4.2335234 -4.2357426 -4.2242041 -4.2010589 -4.1566277 -4.0846648 -4.0286236 -4.0421686 -4.0947537 -4.1233959 -4.1364641 -4.1697779][-4.1750393 -4.1973281 -4.2209044 -4.2390695 -4.2400541 -4.2347789 -4.2222943 -4.1910458 -4.1402025 -4.1018686 -4.1148529 -4.1522264 -4.1644545 -4.165071 -4.1883659][-4.1824574 -4.2041059 -4.2259669 -4.2378058 -4.2391229 -4.2401733 -4.2355251 -4.210011 -4.1764956 -4.1576819 -4.1768031 -4.2035742 -4.197948 -4.1817822 -4.1925483][-4.1872559 -4.208889 -4.2293215 -4.2413039 -4.2432 -4.2420158 -4.23492 -4.2123384 -4.189115 -4.1830921 -4.2069683 -4.2246704 -4.2007542 -4.1656675 -4.1655273][-4.1840596 -4.2066793 -4.229353 -4.244565 -4.2436442 -4.234601 -4.2213826 -4.2021213 -4.1848426 -4.1857285 -4.2104197 -4.2176881 -4.1773796 -4.1255841 -4.1177974][-4.1874952 -4.2148538 -4.2372856 -4.2510047 -4.2454238 -4.2328725 -4.215507 -4.1948428 -4.1804452 -4.1856284 -4.2068515 -4.2035422 -4.1528983 -4.0931187 -4.08363]]...]
INFO - root - 2017-12-07 12:55:59.451958: step 10910, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.741 sec/batch; 66h:14m:04s remains)
INFO - root - 2017-12-07 12:56:06.237033: step 10920, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 59h:03m:36s remains)
INFO - root - 2017-12-07 12:56:12.985098: step 10930, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 55h:25m:43s remains)
INFO - root - 2017-12-07 12:56:19.732674: step 10940, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 57h:38m:50s remains)
INFO - root - 2017-12-07 12:56:26.635104: step 10950, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 66h:34m:57s remains)
INFO - root - 2017-12-07 12:56:33.395202: step 10960, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 63h:46m:25s remains)
INFO - root - 2017-12-07 12:56:40.089034: step 10970, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 58h:55m:36s remains)
INFO - root - 2017-12-07 12:56:46.853065: step 10980, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 56h:05m:05s remains)
INFO - root - 2017-12-07 12:56:53.479080: step 10990, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 57h:16m:48s remains)
INFO - root - 2017-12-07 12:57:00.252579: step 11000, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 64h:54m:07s remains)
2017-12-07 12:57:00.910583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3045554 -4.303215 -4.3010688 -4.2983193 -4.2908435 -4.281271 -4.2725554 -4.278244 -4.2942977 -4.2977533 -4.2946906 -4.2916451 -4.28672 -4.280354 -4.2763038][-4.293582 -4.2927818 -4.2929649 -4.2897558 -4.2775569 -4.2617116 -4.2458882 -4.253901 -4.28097 -4.2877154 -4.2832026 -4.277565 -4.2697358 -4.2633896 -4.2631369][-4.2884822 -4.2894235 -4.2897673 -4.2821612 -4.2620893 -4.2378817 -4.2084966 -4.2152715 -4.2590976 -4.2752442 -4.2756515 -4.2719336 -4.2641168 -4.2598095 -4.2621036][-4.2799821 -4.27797 -4.278933 -4.2679667 -4.2414131 -4.2075591 -4.1599712 -4.1588168 -4.2165027 -4.245666 -4.2603469 -4.2673259 -4.2650523 -4.2645721 -4.2680197][-4.2714 -4.2684312 -4.2673016 -4.2517853 -4.2165394 -4.1722007 -4.1032543 -4.0845037 -4.1448565 -4.1869011 -4.2260466 -4.2511396 -4.25839 -4.2643232 -4.272543][-4.2745962 -4.2745657 -4.2701068 -4.2447572 -4.2002563 -4.1484561 -4.068378 -4.0286121 -4.0697241 -4.1080718 -4.1659536 -4.2105489 -4.2319889 -4.25121 -4.2680192][-4.2774191 -4.2784882 -4.2670603 -4.229454 -4.1759887 -4.1211052 -4.0444179 -3.9972453 -4.0149541 -4.0390329 -4.1057167 -4.1645226 -4.1982336 -4.2336249 -4.2591634][-4.2730923 -4.2716651 -4.2535028 -4.2117586 -4.1587553 -4.1096792 -4.0499849 -4.0194979 -4.0390854 -4.0548644 -4.1078296 -4.15724 -4.1835818 -4.2204289 -4.2482309][-4.2712841 -4.2693992 -4.2523828 -4.2153831 -4.16931 -4.1271019 -4.0811768 -4.0706816 -4.1061325 -4.1260319 -4.1589212 -4.1890969 -4.1984134 -4.2214146 -4.2438307][-4.2716002 -4.2696748 -4.2553682 -4.2225122 -4.1817627 -4.1482921 -4.1115351 -4.1100283 -4.1595197 -4.1865244 -4.2061276 -4.22092 -4.2131281 -4.2171769 -4.2342534][-4.265974 -4.262713 -4.2519846 -4.2250953 -4.1931338 -4.1712093 -4.1428881 -4.1477156 -4.2005992 -4.225822 -4.2350039 -4.2344284 -4.2130413 -4.2034044 -4.2161517][-4.2584014 -4.2559977 -4.2556095 -4.2421374 -4.2207065 -4.2076163 -4.1801105 -4.1760349 -4.218358 -4.2393055 -4.2446694 -4.241221 -4.2233095 -4.211998 -4.2217712][-4.249383 -4.2461305 -4.2569981 -4.2590132 -4.2492051 -4.2412043 -4.2149706 -4.2014012 -4.2295785 -4.2447081 -4.2519197 -4.2538505 -4.2445745 -4.2380176 -4.2473545][-4.2480917 -4.2427506 -4.2561855 -4.2682161 -4.2704511 -4.2716823 -4.2559128 -4.2421861 -4.2577844 -4.2665238 -4.2717805 -4.2717991 -4.2632103 -4.2592916 -4.2700253][-4.26501 -4.2564259 -4.2633061 -4.2728209 -4.278604 -4.2837353 -4.2770343 -4.2707472 -4.282373 -4.28969 -4.2938271 -4.2925453 -4.286407 -4.2831697 -4.2913909]]...]
INFO - root - 2017-12-07 12:57:07.618224: step 11010, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 56h:52m:21s remains)
INFO - root - 2017-12-07 12:57:14.365777: step 11020, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 59h:08m:32s remains)
INFO - root - 2017-12-07 12:57:21.123143: step 11030, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 64h:38m:09s remains)
INFO - root - 2017-12-07 12:57:27.914138: step 11040, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 65h:12m:42s remains)
INFO - root - 2017-12-07 12:57:34.732551: step 11050, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 61h:51m:04s remains)
INFO - root - 2017-12-07 12:57:41.521531: step 11060, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 58h:07m:05s remains)
INFO - root - 2017-12-07 12:57:48.264339: step 11070, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 55h:36m:21s remains)
INFO - root - 2017-12-07 12:57:55.101952: step 11080, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 63h:04m:47s remains)
INFO - root - 2017-12-07 12:58:01.600568: step 11090, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.658 sec/batch; 58h:43m:53s remains)
INFO - root - 2017-12-07 12:58:08.355864: step 11100, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 56h:02m:14s remains)
2017-12-07 12:58:09.046357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.229197 -4.2291288 -4.231811 -4.2332587 -4.2299643 -4.2206945 -4.2075796 -4.1949048 -4.18709 -4.186296 -4.1881838 -4.186841 -4.1841416 -4.1911287 -4.2059231][-4.2379966 -4.2431221 -4.2507081 -4.256072 -4.252943 -4.238647 -4.2150183 -4.1924281 -4.1812773 -4.1826615 -4.1906366 -4.1961675 -4.1977482 -4.206356 -4.2218227][-4.2494984 -4.2567515 -4.2655768 -4.2685065 -4.2578173 -4.2318845 -4.193727 -4.162663 -4.1552505 -4.1664882 -4.18489 -4.2005763 -4.2107034 -4.2221541 -4.2368345][-4.2562623 -4.2599688 -4.2627954 -4.2546849 -4.2316961 -4.1905231 -4.1354165 -4.0973191 -4.1003623 -4.1280351 -4.1635866 -4.1960726 -4.2174077 -4.2296672 -4.2389216][-4.2580185 -4.2581244 -4.2528343 -4.2281141 -4.1866546 -4.1231623 -4.0425081 -3.9917176 -4.0076041 -4.0588012 -4.1188469 -4.175396 -4.2107959 -4.2240992 -4.2258568][-4.2586961 -4.25963 -4.2491713 -4.2087183 -4.1445508 -4.05369 -3.9402795 -3.870105 -3.8906102 -3.9650838 -4.0501981 -4.1324878 -4.1841373 -4.2012439 -4.195868][-4.2567763 -4.2629766 -4.2501526 -4.1962223 -4.11221 -4.0028138 -3.8732598 -3.7908988 -3.8136733 -3.904083 -4.0047321 -4.1012316 -4.1583028 -4.173624 -4.1593318][-4.26429 -4.2777371 -4.2653475 -4.2059879 -4.1184659 -4.0137448 -3.8996358 -3.822865 -3.8360837 -3.9138579 -4.0041471 -4.0932512 -4.1424718 -4.1432137 -4.1160221][-4.2827888 -4.2943406 -4.2797813 -4.2230167 -4.1472158 -4.0677114 -3.9964447 -3.9445312 -3.9454188 -3.9897342 -4.0483656 -4.1095681 -4.1378517 -4.1180515 -4.0786419][-4.2928896 -4.2981906 -4.2821078 -4.2379861 -4.182188 -4.1300144 -4.1032376 -4.083528 -4.0816612 -4.0960975 -4.1229181 -4.1503544 -4.1565132 -4.1228595 -4.0780234][-4.2787437 -4.2817664 -4.2674975 -4.2353296 -4.1967216 -4.1658564 -4.1694684 -4.1788054 -4.18492 -4.189558 -4.1971941 -4.1950727 -4.1812415 -4.1407089 -4.0983644][-4.2500124 -4.2515187 -4.24145 -4.2202158 -4.1918616 -4.174428 -4.1901703 -4.2105918 -4.2263641 -4.2371092 -4.2400908 -4.2214961 -4.1929212 -4.152194 -4.1184678][-4.2382765 -4.2351742 -4.2252269 -4.2048855 -4.176796 -4.1604862 -4.1746216 -4.1984844 -4.2259736 -4.2465577 -4.2514124 -4.2294 -4.1927719 -4.154665 -4.1351557][-4.25589 -4.2475781 -4.2318721 -4.2081032 -4.177494 -4.1602621 -4.1678514 -4.188024 -4.219543 -4.2442884 -4.2464137 -4.2207632 -4.1789775 -4.1493273 -4.1471686][-4.2762008 -4.2641044 -4.2456346 -4.2229247 -4.19561 -4.1810617 -4.1869974 -4.2048736 -4.2334628 -4.25383 -4.2460709 -4.2103648 -4.1652455 -4.1440296 -4.15725]]...]
INFO - root - 2017-12-07 12:58:15.831057: step 11110, loss = 2.09, batch loss = 2.04 (10.8 examples/sec; 0.738 sec/batch; 65h:50m:51s remains)
INFO - root - 2017-12-07 12:58:22.553638: step 11120, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 59h:55m:04s remains)
INFO - root - 2017-12-07 12:58:29.422995: step 11130, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 55h:29m:46s remains)
INFO - root - 2017-12-07 12:58:36.303880: step 11140, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 57h:28m:59s remains)
INFO - root - 2017-12-07 12:58:43.189716: step 11150, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 64h:47m:15s remains)
INFO - root - 2017-12-07 12:58:50.081728: step 11160, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 64h:58m:51s remains)
INFO - root - 2017-12-07 12:58:56.867702: step 11170, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 58h:44m:58s remains)
INFO - root - 2017-12-07 12:59:03.626298: step 11180, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 58h:55m:57s remains)
INFO - root - 2017-12-07 12:59:10.326230: step 11190, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 63h:03m:22s remains)
INFO - root - 2017-12-07 12:59:17.113063: step 11200, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 62h:32m:30s remains)
2017-12-07 12:59:17.908551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28164 -4.2777719 -4.2692466 -4.262279 -4.2630262 -4.27446 -4.2877808 -4.2952423 -4.291326 -4.2770605 -4.2583041 -4.2456656 -4.2459521 -4.2524204 -4.2610021][-4.2659512 -4.2647042 -4.2535715 -4.2397585 -4.2353148 -4.2489753 -4.2697372 -4.285326 -4.2866874 -4.275744 -4.2586803 -4.2461395 -4.2466917 -4.254683 -4.2646337][-4.2567859 -4.2576962 -4.2411909 -4.2162118 -4.2016063 -4.2140565 -4.2429023 -4.2682724 -4.2759748 -4.2709947 -4.2624226 -4.2584147 -4.2638292 -4.2742877 -4.2837391][-4.25933 -4.2544684 -4.2246838 -4.1838803 -4.1579957 -4.1690054 -4.2065344 -4.2470427 -4.2655869 -4.2687969 -4.2713566 -4.2773023 -4.288825 -4.30063 -4.3076186][-4.2736459 -4.2609429 -4.2125773 -4.1499467 -4.1032481 -4.09924 -4.136651 -4.1960382 -4.2381835 -4.2575555 -4.2713523 -4.2867017 -4.30398 -4.3173771 -4.3209758][-4.2893763 -4.271277 -4.210392 -4.1239152 -4.0456052 -4.0049744 -4.0241933 -4.1056261 -4.1823421 -4.2278867 -4.2557955 -4.2790122 -4.3026385 -4.318573 -4.3226609][-4.3007383 -4.2842507 -4.2227573 -4.1212759 -4.0066147 -3.9089699 -3.8893704 -3.9889982 -4.1067076 -4.1841507 -4.2274423 -4.2555585 -4.2829132 -4.3026242 -4.3100319][-4.3095202 -4.3010497 -4.2504354 -4.1497359 -4.017796 -3.8787942 -3.8109303 -3.9100888 -4.0472465 -4.1472473 -4.2012372 -4.228848 -4.2531066 -4.2759113 -4.2909861][-4.3185382 -4.3234253 -4.28929 -4.2054768 -4.0930276 -3.9720252 -3.9017444 -3.9619043 -4.0656772 -4.1475725 -4.1849427 -4.1923704 -4.2026024 -4.2312284 -4.2627678][-4.325901 -4.3421049 -4.3223977 -4.260879 -4.1832886 -4.1046004 -4.0556355 -4.0794115 -4.1292114 -4.1664643 -4.1649537 -4.1302581 -4.1086311 -4.1479411 -4.2091737][-4.3268876 -4.3505421 -4.3405733 -4.2953854 -4.2441163 -4.1976213 -4.1672893 -4.1745777 -4.1899281 -4.1861873 -4.1365566 -4.0377092 -3.96675 -4.0217266 -4.1272826][-4.3251429 -4.3509297 -4.3462033 -4.3118763 -4.2781858 -4.2525063 -4.2379737 -4.242064 -4.2409229 -4.2130489 -4.1297479 -3.9856694 -3.8623469 -3.9169502 -4.0515232][-4.3231874 -4.348228 -4.3479805 -4.3247905 -4.3031449 -4.2906852 -4.2855797 -4.2917662 -4.2880597 -4.2525568 -4.1689849 -4.039525 -3.92579 -3.9504721 -4.0545449][-4.3211408 -4.3456569 -4.3482356 -4.3337612 -4.321929 -4.3181734 -4.3179221 -4.3241615 -4.3211503 -4.2888474 -4.2232885 -4.1384721 -4.0657825 -4.0683289 -4.1170378][-4.31852 -4.3427386 -4.3470182 -4.336678 -4.3300848 -4.3311119 -4.3341346 -4.3419905 -4.3400192 -4.310977 -4.2624507 -4.21197 -4.1726766 -4.1692533 -4.1794243]]...]
INFO - root - 2017-12-07 12:59:24.669914: step 11210, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 58h:43m:49s remains)
INFO - root - 2017-12-07 12:59:31.614152: step 11220, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 64h:04m:14s remains)
INFO - root - 2017-12-07 12:59:38.474100: step 11230, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.774 sec/batch; 69h:02m:29s remains)
INFO - root - 2017-12-07 12:59:45.373371: step 11240, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 61h:27m:31s remains)
INFO - root - 2017-12-07 12:59:52.119428: step 11250, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 56h:03m:12s remains)
INFO - root - 2017-12-07 12:59:58.913728: step 11260, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 58h:24m:29s remains)
INFO - root - 2017-12-07 13:00:05.729305: step 11270, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 63h:06m:53s remains)
INFO - root - 2017-12-07 13:00:12.621239: step 11280, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 63h:02m:20s remains)
INFO - root - 2017-12-07 13:00:19.239219: step 11290, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 61h:41m:31s remains)
INFO - root - 2017-12-07 13:00:26.005643: step 11300, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.635 sec/batch; 56h:40m:03s remains)
2017-12-07 13:00:26.690315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1793275 -4.1610518 -4.1555238 -4.1535592 -4.14115 -4.1290717 -4.136023 -4.1526532 -4.1529741 -4.1365442 -4.0998039 -4.062017 -4.0649123 -4.103425 -4.1424727][-4.1462951 -4.1314421 -4.1269045 -4.1240277 -4.1121073 -4.1033163 -4.1165018 -4.1358557 -4.1406994 -4.1302423 -4.0896773 -4.0446854 -4.0545063 -4.0980563 -4.135035][-4.1055164 -4.0999179 -4.1026015 -4.0999947 -4.0920219 -4.0887475 -4.1027813 -4.11905 -4.12706 -4.1250935 -4.0919261 -4.0525627 -4.0694571 -4.1117568 -4.1478143][-4.0832305 -4.0858746 -4.0948529 -4.091424 -4.0830588 -4.0801635 -4.0874977 -4.0952244 -4.1022253 -4.10833 -4.0924749 -4.0745468 -4.1011577 -4.1392159 -4.1714845][-4.0802917 -4.087976 -4.0985017 -4.0937309 -4.0793338 -4.0667558 -4.0624094 -4.0604148 -4.0649452 -4.0770431 -4.0814238 -4.0862651 -4.1221352 -4.1572323 -4.1853018][-4.085844 -4.0964665 -4.1061392 -4.0982571 -4.0758629 -4.0480175 -4.0287004 -4.019835 -4.0268307 -4.0436873 -4.0533152 -4.0624051 -4.1041946 -4.1422148 -4.1692448][-4.1035452 -4.1160626 -4.1209407 -4.1024971 -4.0662465 -4.0226135 -3.9902475 -3.9777975 -3.9878969 -4.0011611 -3.9980326 -3.9941943 -4.03948 -4.0862408 -4.116219][-4.1248455 -4.13758 -4.1358452 -4.106319 -4.0600691 -4.0079317 -3.9718463 -3.9612248 -3.9736679 -3.9837663 -3.9703372 -3.9547863 -3.9987836 -4.0469666 -4.07683][-4.1397057 -4.1449113 -4.1378865 -4.1073556 -4.0642085 -4.015223 -3.9836044 -3.9798424 -3.9979427 -4.0145755 -4.0095468 -3.9996119 -4.0341334 -4.0716825 -4.0951285][-4.1410666 -4.1410327 -4.1316915 -4.1095514 -4.0773258 -4.0396104 -4.0160084 -4.018466 -4.0408392 -4.0590739 -4.060246 -4.0553036 -4.0822759 -4.1167326 -4.1366444][-4.1292987 -4.1313429 -4.1290555 -4.1178083 -4.0989714 -4.0732408 -4.0538912 -4.0526023 -4.0693359 -4.0816388 -4.0868382 -4.0895562 -4.1163783 -4.1489596 -4.16863][-4.116323 -4.1161265 -4.1163721 -4.1149845 -4.1123962 -4.1040378 -4.0882034 -4.080018 -4.0826039 -4.0828786 -4.0851789 -4.0903568 -4.11847 -4.1524744 -4.1784687][-4.1162748 -4.1097794 -4.1079597 -4.1126046 -4.1253176 -4.1308613 -4.11825 -4.105175 -4.0997624 -4.0915556 -4.0837917 -4.0827928 -4.1109896 -4.1471109 -4.1757236][-4.1144218 -4.108325 -4.1086364 -4.1208649 -4.14434 -4.1559334 -4.1414046 -4.1242542 -4.1196771 -4.11508 -4.1064582 -4.1047163 -4.131412 -4.1597114 -4.1772175][-4.1176009 -4.1161232 -4.1199765 -4.1360364 -4.1577787 -4.1669397 -4.1489315 -4.1325574 -4.1329017 -4.138803 -4.1427727 -4.1508207 -4.1774268 -4.1949549 -4.196722]]...]
INFO - root - 2017-12-07 13:00:33.516748: step 11310, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 66h:06m:19s remains)
INFO - root - 2017-12-07 13:00:40.358956: step 11320, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 59h:06m:18s remains)
INFO - root - 2017-12-07 13:00:47.187331: step 11330, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 56h:56m:51s remains)
INFO - root - 2017-12-07 13:00:53.976785: step 11340, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 58h:50m:59s remains)
INFO - root - 2017-12-07 13:01:00.877722: step 11350, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.745 sec/batch; 66h:26m:26s remains)
INFO - root - 2017-12-07 13:01:07.726133: step 11360, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 63h:09m:04s remains)
INFO - root - 2017-12-07 13:01:14.573309: step 11370, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 60h:24m:33s remains)
INFO - root - 2017-12-07 13:01:21.317493: step 11380, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 56h:51m:01s remains)
INFO - root - 2017-12-07 13:01:27.907084: step 11390, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 58h:12m:28s remains)
INFO - root - 2017-12-07 13:01:34.513889: step 11400, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 64h:30m:23s remains)
2017-12-07 13:01:35.200766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3149962 -4.3120623 -4.3099589 -4.3099847 -4.3125782 -4.31827 -4.3163228 -4.3085628 -4.2960958 -4.28872 -4.2885 -4.3001313 -4.3170223 -4.3270698 -4.3324103][-4.2958174 -4.2887611 -4.2840438 -4.2796822 -4.2804027 -4.2860155 -4.2826138 -4.2748609 -4.2635412 -4.2599354 -4.2628937 -4.2795887 -4.3044267 -4.3188748 -4.3259735][-4.2706532 -4.2614284 -4.254261 -4.2450862 -4.2428708 -4.2475653 -4.2374382 -4.2242503 -4.2137742 -4.2189255 -4.2287092 -4.2493057 -4.2818518 -4.3030376 -4.3132491][-4.2430019 -4.231565 -4.2206144 -4.2035446 -4.1919003 -4.1941195 -4.1797485 -4.1609864 -4.1490579 -4.1660056 -4.1867657 -4.2107735 -4.2489138 -4.2749166 -4.2875805][-4.21533 -4.2012858 -4.1863785 -4.1580038 -4.1286368 -4.117939 -4.0954227 -4.0722241 -4.0662804 -4.104353 -4.1435375 -4.1704259 -4.2058167 -4.2306952 -4.245347][-4.1900024 -4.1749234 -4.1567612 -4.1200418 -4.0655332 -4.0175881 -3.9675219 -3.9294364 -3.9398589 -4.0155611 -4.0829906 -4.1207457 -4.1548166 -4.1812897 -4.2028069][-4.1779261 -4.1576052 -4.1362953 -4.0958033 -4.0255508 -3.9368026 -3.8320048 -3.746819 -3.76923 -3.8979189 -4.0088859 -4.07096 -4.1134477 -4.1491213 -4.1803555][-4.1626711 -4.1309109 -4.108676 -4.0787516 -4.0155277 -3.9098372 -3.758862 -3.6091547 -3.62352 -3.795423 -3.949404 -4.0423822 -4.10032 -4.1460681 -4.1837897][-4.1492157 -4.1100669 -4.097311 -4.0899849 -4.0549679 -3.9767814 -3.841898 -3.7000468 -3.6945412 -3.833039 -3.9718912 -4.0648975 -4.1251335 -4.1723461 -4.2080016][-4.1500883 -4.1131706 -4.113203 -4.12357 -4.1183958 -4.0775013 -3.9857433 -3.88105 -3.8641703 -3.9471493 -4.0419192 -4.1141076 -4.163775 -4.2040477 -4.23282][-4.1607685 -4.1293736 -4.136044 -4.1582212 -4.1741309 -4.1623535 -4.1076546 -4.0362687 -4.0179486 -4.0630693 -4.1222463 -4.1711454 -4.2053361 -4.2350116 -4.2556009][-4.1872511 -4.1592059 -4.1644769 -4.1873565 -4.2131929 -4.2214689 -4.1960025 -4.1534867 -4.1389008 -4.1611671 -4.1964006 -4.2254653 -4.2464128 -4.2666292 -4.2793708][-4.2288523 -4.2095575 -4.2118349 -4.2262049 -4.2474313 -4.2597876 -4.2522149 -4.2306509 -4.2220778 -4.2360291 -4.2575793 -4.2707558 -4.282238 -4.291646 -4.2957673][-4.2656555 -4.2549381 -4.2548556 -4.2620497 -4.2742691 -4.2814851 -4.280529 -4.2722335 -4.2695246 -4.2790737 -4.2903156 -4.2941303 -4.2982039 -4.3021274 -4.3045592][-4.2854977 -4.278748 -4.2771316 -4.280529 -4.2846203 -4.2870054 -4.289381 -4.2876773 -4.2851229 -4.2900639 -4.2976589 -4.3004804 -4.3035769 -4.30786 -4.3123951]]...]
INFO - root - 2017-12-07 13:01:42.048095: step 11410, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:04m:37s remains)
INFO - root - 2017-12-07 13:01:48.840104: step 11420, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 63h:20m:32s remains)
INFO - root - 2017-12-07 13:01:55.683369: step 11430, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 66h:05m:11s remains)
INFO - root - 2017-12-07 13:02:02.456806: step 11440, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 57h:51m:29s remains)
INFO - root - 2017-12-07 13:02:09.201285: step 11450, loss = 2.03, batch loss = 1.97 (12.4 examples/sec; 0.644 sec/batch; 57h:23m:45s remains)
INFO - root - 2017-12-07 13:02:16.041221: step 11460, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:15m:40s remains)
INFO - root - 2017-12-07 13:02:22.814244: step 11470, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 62h:43m:28s remains)
INFO - root - 2017-12-07 13:02:29.655284: step 11480, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 63h:33m:25s remains)
INFO - root - 2017-12-07 13:02:36.238160: step 11490, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 55h:27m:50s remains)
INFO - root - 2017-12-07 13:02:42.969898: step 11500, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 0.619 sec/batch; 55h:12m:40s remains)
2017-12-07 13:02:43.762443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3130393 -4.3103733 -4.3015161 -4.2892771 -4.273458 -4.2596765 -4.2544017 -4.2542057 -4.2593069 -4.2611361 -4.2624807 -4.2629137 -4.2673697 -4.2764544 -4.2855511][-4.3099265 -4.3049669 -4.293767 -4.2791123 -4.2609973 -4.2437844 -4.2357264 -4.2323947 -4.2377515 -4.246664 -4.2591181 -4.2702618 -4.2792883 -4.2870808 -4.2898397][-4.3075833 -4.3015838 -4.2891479 -4.2726989 -4.2523451 -4.2287588 -4.2132988 -4.2039824 -4.2074738 -4.2233915 -4.2482977 -4.2722263 -4.2896914 -4.2979732 -4.29644][-4.3085775 -4.3049507 -4.2936621 -4.2760229 -4.250114 -4.214232 -4.1855378 -4.1685147 -4.170835 -4.1946259 -4.2308674 -4.2656035 -4.290761 -4.2999258 -4.297133][-4.3066511 -4.3006206 -4.2856379 -4.2626476 -4.2268453 -4.1765103 -4.1314063 -4.1078529 -4.1173019 -4.1540122 -4.2034364 -4.2449341 -4.27333 -4.2851081 -4.2827873][-4.2924266 -4.2782497 -4.2535334 -4.2208 -4.1742554 -4.107276 -4.0374727 -3.9965625 -4.0226989 -4.08903 -4.1571941 -4.2061543 -4.2388082 -4.256835 -4.2563868][-4.2623081 -4.2345676 -4.1939077 -4.1485953 -4.0922341 -4.0094953 -3.9109416 -3.8360233 -3.8770046 -3.9890952 -4.0862045 -4.1488523 -4.1931396 -4.2210026 -4.22525][-4.2299833 -4.1897941 -4.1347942 -4.0795894 -4.0193067 -3.9351792 -3.8266847 -3.7346153 -3.7864861 -3.925477 -4.0376482 -4.1070433 -4.1613078 -4.200357 -4.215066][-4.2165518 -4.1715884 -4.11505 -4.0636463 -4.0133944 -3.9532144 -3.8804162 -3.8275108 -3.8718281 -3.9766774 -4.0663218 -4.1258068 -4.1759167 -4.2167916 -4.239121][-4.2369957 -4.1944575 -4.14347 -4.1011558 -4.0686831 -4.0396237 -4.0107355 -3.9993322 -4.0324368 -4.0930219 -4.1487384 -4.192399 -4.2313123 -4.2645044 -4.2848454][-4.2731962 -4.2382011 -4.1956811 -4.1636271 -4.1479268 -4.1416149 -4.1399021 -4.1477685 -4.1697841 -4.1986127 -4.2263908 -4.2560725 -4.2853956 -4.3093777 -4.323544][-4.30435 -4.2798538 -4.2487946 -4.2264671 -4.2199121 -4.2236814 -4.231472 -4.243135 -4.2570591 -4.2691913 -4.2800717 -4.2978725 -4.3193188 -4.3350997 -4.3433867][-4.3225374 -4.309617 -4.2921357 -4.2795467 -4.2791209 -4.2857776 -4.2949014 -4.3041406 -4.3103428 -4.3128958 -4.3147874 -4.3241906 -4.3371272 -4.3464704 -4.3500738][-4.3329134 -4.3296871 -4.3218427 -4.3138676 -4.3132915 -4.3184247 -4.3255482 -4.3316417 -4.3343205 -4.3336535 -4.3331475 -4.3366747 -4.3426538 -4.3472524 -4.3483286][-4.3368764 -4.3377647 -4.3342686 -4.32902 -4.3271141 -4.329185 -4.3332906 -4.3369923 -4.3387651 -4.3384409 -4.337996 -4.3391867 -4.3416991 -4.3440847 -4.3451266]]...]
INFO - root - 2017-12-07 13:02:50.495357: step 11510, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 58h:01m:22s remains)
INFO - root - 2017-12-07 13:02:57.342365: step 11520, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 56h:17m:48s remains)
INFO - root - 2017-12-07 13:03:04.188544: step 11530, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 57h:46m:21s remains)
INFO - root - 2017-12-07 13:03:10.966899: step 11540, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 65h:04m:59s remains)
INFO - root - 2017-12-07 13:03:17.799312: step 11550, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 64h:36m:47s remains)
INFO - root - 2017-12-07 13:03:24.603339: step 11560, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 61h:29m:19s remains)
INFO - root - 2017-12-07 13:03:31.354057: step 11570, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.676 sec/batch; 60h:13m:48s remains)
INFO - root - 2017-12-07 13:03:38.048962: step 11580, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 57h:29m:12s remains)
INFO - root - 2017-12-07 13:03:44.728309: step 11590, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 59h:14m:31s remains)
INFO - root - 2017-12-07 13:03:51.635985: step 11600, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 64h:18m:47s remains)
2017-12-07 13:03:52.366210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33037 -4.3299479 -4.3300567 -4.3333631 -4.3371739 -4.341403 -4.3450141 -4.3461494 -4.3470149 -4.348556 -4.3489466 -4.347918 -4.3469887 -4.3475647 -4.3496709][-4.3027344 -4.3008895 -4.298903 -4.2999058 -4.3020878 -4.3077612 -4.3133736 -4.31545 -4.3170409 -4.3205404 -4.3243375 -4.3266397 -4.3275251 -4.328805 -4.3338366][-4.2644482 -4.2605944 -4.2578239 -4.2561569 -4.2555532 -4.2614946 -4.2680931 -4.2720985 -4.2762551 -4.2820826 -4.2901211 -4.2978597 -4.303812 -4.31002 -4.3202581][-4.208375 -4.1972351 -4.1955614 -4.1969323 -4.1985855 -4.2067471 -4.2140675 -4.219882 -4.227633 -4.2356138 -4.2460566 -4.2575135 -4.2660933 -4.276197 -4.2933092][-4.1532722 -4.1345778 -4.1313877 -4.1339889 -4.1347218 -4.1396117 -4.145359 -4.1533327 -4.166676 -4.1831069 -4.2003312 -4.2167883 -4.2263942 -4.2375374 -4.2612262][-4.1136513 -4.0914583 -4.0834541 -4.0781832 -4.0688267 -4.0605907 -4.0564885 -4.0610151 -4.0834985 -4.11582 -4.1449323 -4.1697412 -4.1863732 -4.2044272 -4.2334433][-4.0890212 -4.0600491 -4.0423522 -4.0226135 -3.9987364 -3.9705932 -3.9402928 -3.9294949 -3.9613788 -4.0150166 -4.0657043 -4.1114116 -4.1460919 -4.1773677 -4.2119489][-4.0961723 -4.0627975 -4.0382352 -4.0088806 -3.9709418 -3.9181488 -3.8547873 -3.8190312 -3.8542275 -3.9258513 -4.0019288 -4.06897 -4.1181626 -4.1601286 -4.19909][-4.1265574 -4.1016469 -4.0844545 -4.0588255 -4.0188012 -3.9574313 -3.8824186 -3.8368888 -3.8617127 -3.9222796 -3.9956326 -4.0632591 -4.1173391 -4.1637106 -4.2013521][-4.1535492 -4.1386828 -4.1318755 -4.11569 -4.0831146 -4.0293164 -3.9635878 -3.9244323 -3.936434 -3.9772635 -4.0313392 -4.0893483 -4.140789 -4.1832557 -4.214551][-4.1783414 -4.1662035 -4.1633492 -4.1536942 -4.1281953 -4.0878668 -4.0386882 -4.0067105 -4.0077782 -4.0335755 -4.0742946 -4.1216612 -4.1639948 -4.2009306 -4.230268][-4.2066727 -4.1947956 -4.190331 -4.1819425 -4.1621566 -4.1339169 -4.1007514 -4.0769887 -4.075315 -4.0907164 -4.12121 -4.1557827 -4.1872663 -4.2196336 -4.248035][-4.2482696 -4.2415009 -4.2382903 -4.2315011 -4.2180443 -4.1977396 -4.1715178 -4.1507835 -4.1462841 -4.1564722 -4.1775227 -4.1988144 -4.2216034 -4.2504039 -4.2761054][-4.2896485 -4.2898364 -4.2899337 -4.2857294 -4.2766867 -4.2641158 -4.2454228 -4.2281 -4.2240491 -4.2318387 -4.2446914 -4.255671 -4.2694468 -4.2903342 -4.3093991][-4.319231 -4.3221936 -4.3228331 -4.3193431 -4.3133349 -4.3067703 -4.2962346 -4.2868233 -4.287 -4.2933178 -4.3008075 -4.3059568 -4.3125644 -4.324688 -4.3363104]]...]
INFO - root - 2017-12-07 13:03:59.114587: step 11610, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 57h:48m:03s remains)
INFO - root - 2017-12-07 13:04:05.988956: step 11620, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 63h:18m:24s remains)
INFO - root - 2017-12-07 13:04:12.797037: step 11630, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 65h:07m:40s remains)
INFO - root - 2017-12-07 13:04:19.630318: step 11640, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 59h:41m:50s remains)
INFO - root - 2017-12-07 13:04:26.534264: step 11650, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 57h:40m:13s remains)
INFO - root - 2017-12-07 13:04:33.265784: step 11660, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 55h:28m:09s remains)
INFO - root - 2017-12-07 13:04:40.132507: step 11670, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.734 sec/batch; 65h:26m:01s remains)
INFO - root - 2017-12-07 13:04:46.924884: step 11680, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 65h:05m:57s remains)
INFO - root - 2017-12-07 13:04:53.578636: step 11690, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 60h:22m:56s remains)
INFO - root - 2017-12-07 13:05:00.335789: step 11700, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 56h:58m:21s remains)
2017-12-07 13:05:01.110120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2977405 -4.2820792 -4.270081 -4.2600484 -4.2516294 -4.2503223 -4.2512174 -4.250083 -4.2413192 -4.2278748 -4.2173915 -4.2202215 -4.2319374 -4.2497339 -4.2652154][-4.2867403 -4.2654424 -4.2509432 -4.2389145 -4.225708 -4.2222142 -4.223938 -4.2245755 -4.2159691 -4.20123 -4.1860671 -4.1829557 -4.1916513 -4.2119203 -4.22789][-4.2775912 -4.2511268 -4.2310891 -4.2112346 -4.1914134 -4.1860418 -4.1891847 -4.1943908 -4.1942854 -4.1864271 -4.1711278 -4.1620212 -4.1660666 -4.1850848 -4.1998448][-4.2686939 -4.2390513 -4.2135191 -4.185369 -4.159853 -4.1502743 -4.154285 -4.16454 -4.1756568 -4.179008 -4.1656566 -4.1556191 -4.1616311 -4.1782103 -4.1900344][-4.2590289 -4.2292953 -4.2049518 -4.1745973 -4.1452403 -4.128304 -4.1223669 -4.1267653 -4.1402225 -4.1516109 -4.1461477 -4.1421161 -4.1542034 -4.1733408 -4.1871133][-4.2530942 -4.2217383 -4.1971955 -4.1657147 -4.1285372 -4.0966582 -4.0694013 -4.0600429 -4.075902 -4.0999951 -4.1115136 -4.1228819 -4.1434059 -4.1656575 -4.1849995][-4.2501688 -4.2145648 -4.1860347 -4.1503172 -4.1016335 -4.0474768 -3.9862385 -3.9546213 -3.984885 -4.034091 -4.0705166 -4.1017742 -4.1312795 -4.157094 -4.1794906][-4.2511411 -4.21183 -4.1768894 -4.1325097 -4.0779748 -4.0131907 -3.9280534 -3.8800309 -3.9304197 -4.0013704 -4.0578284 -4.1030588 -4.1351194 -4.1597857 -4.1795654][-4.2532492 -4.2127018 -4.1757092 -4.1320138 -4.0877733 -4.040184 -3.9745941 -3.939085 -3.9839928 -4.0423837 -4.0926819 -4.1344767 -4.1594024 -4.1748347 -4.18525][-4.2557774 -4.2194037 -4.1868358 -4.1524315 -4.1243248 -4.10082 -4.0677714 -4.0468783 -4.0694203 -4.0971279 -4.1271939 -4.1580687 -4.1707811 -4.1723089 -4.172461][-4.2612195 -4.232358 -4.2062979 -4.1816716 -4.1655936 -4.1577411 -4.1438589 -4.1280804 -4.1293983 -4.1334414 -4.1483994 -4.1700549 -4.1754603 -4.1692166 -4.1627679][-4.2683368 -4.2438793 -4.2252336 -4.2107639 -4.2034135 -4.2023368 -4.1929283 -4.1710515 -4.1519508 -4.1417804 -4.1502285 -4.1669459 -4.1724629 -4.1693754 -4.1666932][-4.2767196 -4.2538567 -4.2390661 -4.2283158 -4.221911 -4.219944 -4.2096033 -4.1824279 -4.1530452 -4.1381855 -4.1446323 -4.1584878 -4.1685929 -4.1753497 -4.179925][-4.287971 -4.2685676 -4.2547398 -4.2422028 -4.2316928 -4.2251015 -4.2134223 -4.1867485 -4.1586857 -4.1459475 -4.1531472 -4.1664696 -4.18031 -4.1940651 -4.2012577][-4.2984858 -4.2825875 -4.2683177 -4.2537165 -4.2416949 -4.23509 -4.2263846 -4.2054868 -4.1830063 -4.174655 -4.1794324 -4.187253 -4.2010894 -4.2178845 -4.2220879]]...]
INFO - root - 2017-12-07 13:05:07.830501: step 11710, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 61h:17m:33s remains)
INFO - root - 2017-12-07 13:05:14.616984: step 11720, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 55h:47m:51s remains)
INFO - root - 2017-12-07 13:05:21.478636: step 11730, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 57h:41m:00s remains)
INFO - root - 2017-12-07 13:05:28.334818: step 11740, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 64h:03m:07s remains)
INFO - root - 2017-12-07 13:05:35.186118: step 11750, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 64h:45m:18s remains)
INFO - root - 2017-12-07 13:05:42.104911: step 11760, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:13m:35s remains)
INFO - root - 2017-12-07 13:05:48.934758: step 11770, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 57h:36m:20s remains)
INFO - root - 2017-12-07 13:05:55.734712: step 11780, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 61h:04m:02s remains)
INFO - root - 2017-12-07 13:06:02.475736: step 11790, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 62h:04m:46s remains)
INFO - root - 2017-12-07 13:06:09.226047: step 11800, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:10m:49s remains)
2017-12-07 13:06:09.967263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2424283 -4.2257504 -4.2160854 -4.2167172 -4.2195921 -4.2087469 -4.1913133 -4.1704106 -4.1548367 -4.1485143 -4.1408057 -4.143116 -4.1728621 -4.2077427 -4.2252254][-4.2189736 -4.1953869 -4.178699 -4.1779647 -4.1910343 -4.1953764 -4.1896906 -4.1831527 -4.1746221 -4.1747432 -4.1777406 -4.177206 -4.1985641 -4.2237759 -4.2312379][-4.1910486 -4.1673403 -4.1467848 -4.1410179 -4.16023 -4.1765456 -4.183517 -4.1881366 -4.1784344 -4.1771512 -4.1827087 -4.1804304 -4.1969604 -4.21792 -4.2247872][-4.1719708 -4.1541786 -4.1325908 -4.1242127 -4.14557 -4.1643853 -4.1734967 -4.1858659 -4.17434 -4.1691012 -4.1709738 -4.1714444 -4.1860781 -4.2075615 -4.2191319][-4.1588597 -4.1447515 -4.1255283 -4.1211724 -4.14226 -4.1573806 -4.1658521 -4.1824007 -4.1774697 -4.1701446 -4.16815 -4.171946 -4.186357 -4.2114172 -4.2262988][-4.148787 -4.1364884 -4.1295605 -4.1331158 -4.1405897 -4.1400657 -4.1399512 -4.1591744 -4.1728468 -4.1695967 -4.1663556 -4.177598 -4.192728 -4.2200127 -4.2374263][-4.1243448 -4.1090684 -4.1087637 -4.1145439 -4.1071758 -4.0921459 -4.0799146 -4.0999117 -4.1371131 -4.145021 -4.1467361 -4.1664925 -4.1889505 -4.2207751 -4.2419658][-4.0777469 -4.0620217 -4.0646143 -4.0674882 -4.0506663 -4.0200987 -3.9920244 -4.012835 -4.0717587 -4.1012197 -4.1153889 -4.1479168 -4.1799793 -4.2176991 -4.2465677][-4.0523014 -4.045372 -4.0479746 -4.0455246 -4.0242162 -3.9878922 -3.957408 -3.9819028 -4.0553865 -4.1042285 -4.1259308 -4.1577125 -4.1893868 -4.2260971 -4.2575345][-4.0876279 -4.0892587 -4.0940523 -4.0855727 -4.0660009 -4.0386453 -4.0136075 -4.0284729 -4.0936866 -4.1437049 -4.1641588 -4.190022 -4.2182579 -4.2483368 -4.2742743][-4.1378255 -4.1505775 -4.1670537 -4.1615815 -4.1448674 -4.1253695 -4.1054044 -4.1078262 -4.1535296 -4.19438 -4.2098832 -4.2318444 -4.2571306 -4.2784476 -4.2904162][-4.1784077 -4.2006221 -4.2287269 -4.2279916 -4.212678 -4.1964531 -4.1780577 -4.1729741 -4.2003126 -4.2301164 -4.2409 -4.2565136 -4.2813044 -4.2953978 -4.2916813][-4.2130046 -4.2376475 -4.2676105 -4.267601 -4.2524185 -4.2381611 -4.2204533 -4.2117391 -4.2284827 -4.2518883 -4.261198 -4.268302 -4.290309 -4.2977705 -4.2837973][-4.2431498 -4.2632828 -4.2879181 -4.2832875 -4.2676721 -4.253912 -4.2369595 -4.2299876 -4.2448926 -4.2655864 -4.2724881 -4.2710576 -4.2867775 -4.2921686 -4.2754951][-4.2669892 -4.2796922 -4.2935519 -4.2875538 -4.2756352 -4.2617283 -4.243094 -4.2402554 -4.2549353 -4.2685604 -4.2680941 -4.2618928 -4.275732 -4.2820277 -4.2672276]]...]
INFO - root - 2017-12-07 13:06:16.773039: step 11810, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 64h:52m:43s remains)
INFO - root - 2017-12-07 13:06:23.576748: step 11820, loss = 2.04, batch loss = 1.99 (10.7 examples/sec; 0.746 sec/batch; 66h:25m:53s remains)
INFO - root - 2017-12-07 13:06:30.371919: step 11830, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 60h:42m:26s remains)
INFO - root - 2017-12-07 13:06:37.207406: step 11840, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.624 sec/batch; 55h:35m:36s remains)
INFO - root - 2017-12-07 13:06:43.987924: step 11850, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 60h:12m:53s remains)
INFO - root - 2017-12-07 13:06:50.832586: step 11860, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 63h:58m:54s remains)
INFO - root - 2017-12-07 13:06:57.676611: step 11870, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.665 sec/batch; 59h:11m:33s remains)
INFO - root - 2017-12-07 13:07:04.485973: step 11880, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 55h:27m:24s remains)
INFO - root - 2017-12-07 13:07:11.071865: step 11890, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 56h:45m:34s remains)
INFO - root - 2017-12-07 13:07:17.967271: step 11900, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 64h:11m:42s remains)
2017-12-07 13:07:18.719249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2873473 -4.285346 -4.2924075 -4.3009968 -4.3062077 -4.3091168 -4.3078012 -4.3116736 -4.3234897 -4.3328953 -4.336072 -4.3339281 -4.3282719 -4.321032 -4.3151259][-4.2808142 -4.27442 -4.2759919 -4.2771807 -4.2771387 -4.2781925 -4.2774558 -4.282527 -4.3001714 -4.317368 -4.3269839 -4.3296819 -4.3247075 -4.3153877 -4.3077693][-4.2774997 -4.2639461 -4.2537408 -4.2427378 -4.233891 -4.2311864 -4.2310548 -4.239542 -4.2638822 -4.2859507 -4.2985888 -4.3041015 -4.3014641 -4.2950578 -4.2928324][-4.2778077 -4.2547946 -4.2308125 -4.2082281 -4.187757 -4.1755323 -4.1688972 -4.1752973 -4.2046452 -4.23212 -4.2478089 -4.2571712 -4.260982 -4.2620487 -4.2703538][-4.279418 -4.2472792 -4.2136822 -4.1818271 -4.1473718 -4.1179538 -4.09588 -4.09402 -4.1279221 -4.1630783 -4.1860847 -4.2036848 -4.2150712 -4.223937 -4.2414756][-4.2846055 -4.2483964 -4.2098622 -4.1690044 -4.1170235 -4.0619988 -4.0149565 -3.9968796 -4.0370483 -4.0901489 -4.1299915 -4.1608605 -4.1791873 -4.1933808 -4.2161384][-4.2925205 -4.2593613 -4.2205877 -4.17151 -4.1027293 -4.0202017 -3.9362102 -3.8851175 -3.9316061 -4.0148487 -4.0803165 -4.1275897 -4.1560659 -4.1740761 -4.1970329][-4.2994304 -4.2720585 -4.2398338 -4.1953077 -4.1286874 -4.0395985 -3.9391923 -3.8689561 -3.9098322 -3.996664 -4.0627041 -4.1100812 -4.1368532 -4.152915 -4.1736016][-4.3040042 -4.2852979 -4.2667737 -4.2369695 -4.1864338 -4.1147003 -4.0329952 -3.9796529 -4.0034513 -4.0536304 -4.0877357 -4.1112275 -4.1248579 -4.1329861 -4.1476531][-4.3031731 -4.2912297 -4.284709 -4.2712617 -4.2408342 -4.1928253 -4.139545 -4.10787 -4.121624 -4.1399159 -4.14518 -4.142982 -4.136332 -4.1295085 -4.1308422][-4.2995687 -4.2926893 -4.2933908 -4.2893476 -4.2728333 -4.2404404 -4.2095304 -4.1969557 -4.21087 -4.2159948 -4.20457 -4.1857629 -4.1655421 -4.1475425 -4.1348414][-4.2974625 -4.2935524 -4.2953868 -4.2921906 -4.2786427 -4.2558246 -4.2389784 -4.2399426 -4.2572556 -4.26029 -4.2453284 -4.2211413 -4.1976166 -4.1747484 -4.1531482][-4.3000355 -4.2955823 -4.2965784 -4.2933674 -4.2786407 -4.2586904 -4.2468495 -4.2531137 -4.2719164 -4.2736192 -4.2604141 -4.2389464 -4.2183843 -4.198204 -4.1770458][-4.302362 -4.2947946 -4.2912612 -4.2869034 -4.2728748 -4.256917 -4.2486887 -4.255476 -4.2711797 -4.2701621 -4.2615204 -4.24807 -4.2317524 -4.2141943 -4.1956363][-4.300281 -4.2900906 -4.2831793 -4.2780948 -4.2683363 -4.2592244 -4.2558565 -4.2614694 -4.2710724 -4.2694817 -4.2667446 -4.2591295 -4.2459726 -4.22872 -4.2150836]]...]
INFO - root - 2017-12-07 13:07:25.493556: step 11910, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 57h:30m:40s remains)
INFO - root - 2017-12-07 13:07:32.307208: step 11920, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 57h:46m:17s remains)
INFO - root - 2017-12-07 13:07:39.172950: step 11930, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 65h:37m:51s remains)
INFO - root - 2017-12-07 13:07:45.969142: step 11940, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.738 sec/batch; 65h:41m:15s remains)
INFO - root - 2017-12-07 13:07:52.757380: step 11950, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 57h:55m:13s remains)
INFO - root - 2017-12-07 13:07:59.446292: step 11960, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 57h:30m:23s remains)
INFO - root - 2017-12-07 13:08:06.248438: step 11970, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 59h:17m:08s remains)
INFO - root - 2017-12-07 13:08:13.091204: step 11980, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 65h:11m:16s remains)
INFO - root - 2017-12-07 13:08:19.702222: step 11990, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 60h:22m:43s remains)
INFO - root - 2017-12-07 13:08:26.435583: step 12000, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 58h:23m:08s remains)
2017-12-07 13:08:27.099377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2605906 -4.25014 -4.2437992 -4.246418 -4.2523489 -4.2451992 -4.2295966 -4.2151151 -4.2135515 -4.21838 -4.2293649 -4.2379065 -4.2335362 -4.22663 -4.2280788][-4.2366786 -4.2233567 -4.2136879 -4.2165871 -4.2196665 -4.2061591 -4.1807365 -4.1621065 -4.1652622 -4.1763258 -4.1910987 -4.2013979 -4.1932096 -4.1836944 -4.1898532][-4.2147565 -4.196012 -4.182416 -4.1873531 -4.1886559 -4.1702328 -4.1392856 -4.1201563 -4.1283369 -4.1386633 -4.1519151 -4.1637783 -4.1554785 -4.1505046 -4.1637812][-4.2034779 -4.1781878 -4.1566563 -4.1583138 -4.1584382 -4.137475 -4.1063261 -4.0911288 -4.0993338 -4.1006346 -4.1073995 -4.1261864 -4.1262369 -4.1280456 -4.1456318][-4.2014055 -4.1672697 -4.132298 -4.1261439 -4.1205168 -4.100338 -4.0698061 -4.0548081 -4.052083 -4.0363221 -4.0371766 -4.0693064 -4.0896373 -4.1072288 -4.1286149][-4.2072182 -4.1678772 -4.1237874 -4.1089721 -4.1002407 -4.0822582 -4.052393 -4.029707 -4.0036016 -3.9641082 -3.9601531 -4.0131879 -4.0618434 -4.0986724 -4.1244764][-4.2217813 -4.1860647 -4.146523 -4.1282158 -4.1173272 -4.094552 -4.062902 -4.0321913 -3.9915967 -3.9413228 -3.9407938 -4.0023737 -4.0620484 -4.1059213 -4.13849][-4.2324405 -4.2081871 -4.1784029 -4.1571426 -4.1431851 -4.1167336 -4.0878568 -4.0596991 -4.0199037 -3.9827609 -3.9967098 -4.0486112 -4.0889044 -4.1228747 -4.16257][-4.2412605 -4.2172656 -4.1833191 -4.1606741 -4.1500688 -4.1333523 -4.117063 -4.1004405 -4.0732641 -4.05644 -4.0742307 -4.0985241 -4.106122 -4.1250944 -4.1671476][-4.2469873 -4.2146597 -4.1708064 -4.1481042 -4.1458282 -4.1462193 -4.1465597 -4.1423197 -4.1297822 -4.1233907 -4.13054 -4.1278262 -4.1129308 -4.1226044 -4.1601758][-4.2626963 -4.2213287 -4.1695037 -4.1453967 -4.149045 -4.1569376 -4.16297 -4.1653237 -4.1582346 -4.1533442 -4.1508741 -4.138483 -4.1204319 -4.1312585 -4.159215][-4.28642 -4.246048 -4.1938972 -4.169383 -4.1719732 -4.1789961 -4.1768446 -4.1756029 -4.1676545 -4.1589255 -4.1538858 -4.1414447 -4.1323881 -4.1494665 -4.1676035][-4.3062549 -4.27428 -4.2277589 -4.2054586 -4.2051096 -4.2081137 -4.194778 -4.1888838 -4.1821351 -4.1774144 -4.1794481 -4.1768379 -4.1780868 -4.1922655 -4.1948676][-4.3258462 -4.304008 -4.2644768 -4.238781 -4.2322836 -4.2279878 -4.2103424 -4.2043629 -4.2044077 -4.2117853 -4.2275114 -4.2361794 -4.2406807 -4.245429 -4.234858][-4.3307214 -4.3144803 -4.2806964 -4.2545705 -4.243052 -4.2336879 -4.2221212 -4.2224989 -4.2305517 -4.2468324 -4.2702489 -4.2779293 -4.2772913 -4.2751484 -4.2557874]]...]
INFO - root - 2017-12-07 13:08:33.912157: step 12010, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 64h:24m:18s remains)
INFO - root - 2017-12-07 13:08:40.609362: step 12020, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:08m:25s remains)
INFO - root - 2017-12-07 13:08:47.264991: step 12030, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.625 sec/batch; 55h:39m:16s remains)
INFO - root - 2017-12-07 13:08:54.049781: step 12040, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 58h:32m:28s remains)
INFO - root - 2017-12-07 13:09:00.806748: step 12050, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 62h:50m:05s remains)
INFO - root - 2017-12-07 13:09:07.579006: step 12060, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 63h:33m:14s remains)
INFO - root - 2017-12-07 13:09:14.318415: step 12070, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 62h:13m:36s remains)
INFO - root - 2017-12-07 13:09:21.195407: step 12080, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 58h:53m:23s remains)
INFO - root - 2017-12-07 13:09:27.957648: step 12090, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 61h:00m:34s remains)
INFO - root - 2017-12-07 13:09:34.875781: step 12100, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.749 sec/batch; 66h:41m:03s remains)
2017-12-07 13:09:35.548607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2207265 -4.2155237 -4.2183747 -4.2154803 -4.2014241 -4.1816182 -4.1715436 -4.1861649 -4.2127919 -4.234354 -4.2507453 -4.2559004 -4.2554865 -4.2581873 -4.264689][-4.2072783 -4.2061076 -4.2121611 -4.2125459 -4.2041774 -4.19172 -4.1865067 -4.2024407 -4.22832 -4.2452369 -4.2513 -4.2488465 -4.2452025 -4.2487745 -4.2549028][-4.1995478 -4.2006416 -4.2066908 -4.2072449 -4.2031245 -4.1953187 -4.1941867 -4.2141156 -4.2440777 -4.2633572 -4.2652164 -4.2567897 -4.2453146 -4.2406416 -4.2389197][-4.2095084 -4.2105465 -4.2140083 -4.212647 -4.2066269 -4.1925349 -4.181766 -4.1990995 -4.2367682 -4.2677736 -4.2770014 -4.269578 -4.2542324 -4.2413898 -4.230895][-4.2350149 -4.2342968 -4.2344871 -4.2303519 -4.2190261 -4.1913214 -4.1571288 -4.1528158 -4.1902633 -4.2416725 -4.2708216 -4.2735367 -4.2602339 -4.2461977 -4.2344661][-4.2594805 -4.2577224 -4.2555189 -4.2492919 -4.2290521 -4.1868935 -4.1231875 -4.0806 -4.1010408 -4.1735697 -4.2349067 -4.2594962 -4.2605567 -4.2557082 -4.2468414][-4.2811136 -4.2826738 -4.28075 -4.2729893 -4.2460036 -4.1916013 -4.1029129 -4.015202 -3.9956923 -4.0745034 -4.1723032 -4.2279663 -4.25163 -4.2604713 -4.2589235][-4.2916918 -4.3032537 -4.3073759 -4.2984409 -4.26968 -4.2185755 -4.1301808 -4.0164251 -3.9444356 -3.9930651 -4.0972762 -4.1765018 -4.2254438 -4.2525582 -4.2634892][-4.2900963 -4.3097305 -4.3214393 -4.3176908 -4.2955875 -4.2583704 -4.1942296 -4.0983591 -4.0080051 -3.9980426 -4.0568333 -4.127934 -4.1876979 -4.2263503 -4.2481065][-4.2736564 -4.2969484 -4.3159118 -4.3240957 -4.3161349 -4.2943864 -4.2555475 -4.1915317 -4.1177421 -4.0840983 -4.0976696 -4.1309762 -4.1688271 -4.1990829 -4.2229352][-4.2505317 -4.2741489 -4.2961588 -4.3138046 -4.3214636 -4.3142862 -4.2902441 -4.2469144 -4.1966853 -4.1674252 -4.1673427 -4.1782393 -4.1941681 -4.2023492 -4.2112064][-4.2362518 -4.2587743 -4.2771559 -4.295197 -4.3101048 -4.3126683 -4.29567 -4.2613549 -4.2253246 -4.2056484 -4.2093906 -4.2202229 -4.2311072 -4.2287393 -4.2217984][-4.2426643 -4.2575488 -4.2659445 -4.2756643 -4.2864938 -4.2888155 -4.2738981 -4.2442493 -4.2123485 -4.1965542 -4.2067928 -4.2293282 -4.2482414 -4.2475357 -4.2383389][-4.2585769 -4.2624779 -4.2619252 -4.2641621 -4.2673688 -4.2642879 -4.2481508 -4.2181482 -4.1858077 -4.1702948 -4.1847372 -4.2160168 -4.24192 -4.2464161 -4.2415171][-4.27243 -4.2631783 -4.257174 -4.2579756 -4.2576017 -4.2521033 -4.2340508 -4.2032671 -4.1718011 -4.1589956 -4.1779428 -4.2096486 -4.2306929 -4.2343736 -4.2328491]]...]
INFO - root - 2017-12-07 13:09:42.298624: step 12110, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 58h:22m:04s remains)
INFO - root - 2017-12-07 13:09:49.111116: step 12120, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 64h:25m:29s remains)
INFO - root - 2017-12-07 13:09:55.948776: step 12130, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 65h:40m:51s remains)
INFO - root - 2017-12-07 13:10:02.804189: step 12140, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 63h:59m:21s remains)
INFO - root - 2017-12-07 13:10:09.552491: step 12150, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.618 sec/batch; 54h:59m:26s remains)
INFO - root - 2017-12-07 13:10:16.514121: step 12160, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 60h:29m:25s remains)
INFO - root - 2017-12-07 13:10:23.423934: step 12170, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 65h:55m:33s remains)
INFO - root - 2017-12-07 13:10:30.177690: step 12180, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 62h:04m:05s remains)
INFO - root - 2017-12-07 13:10:36.776343: step 12190, loss = 2.04, batch loss = 1.99 (13.2 examples/sec; 0.608 sec/batch; 54h:06m:15s remains)
INFO - root - 2017-12-07 13:10:43.504541: step 12200, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.618 sec/batch; 55h:00m:09s remains)
2017-12-07 13:10:44.208877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3006949 -4.3041439 -4.3051186 -4.3051519 -4.3070064 -4.3094444 -4.3097715 -4.3082871 -4.3055615 -4.3014426 -4.2979856 -4.2983813 -4.3023391 -4.3085427 -4.3127823][-4.290863 -4.296721 -4.2969227 -4.2933984 -4.291575 -4.2916269 -4.2907581 -4.2909079 -4.29213 -4.2901559 -4.2885847 -4.2913251 -4.2984505 -4.3076391 -4.3127284][-4.2671413 -4.2764974 -4.2775826 -4.2708197 -4.263329 -4.2568412 -4.2527189 -4.2582483 -4.2681675 -4.2728262 -4.2774339 -4.2860942 -4.2968874 -4.3068213 -4.3097086][-4.2286496 -4.2394943 -4.2417078 -4.2311954 -4.2159133 -4.2009163 -4.1939259 -4.2068763 -4.2286139 -4.2436113 -4.2566872 -4.273387 -4.2903647 -4.3031178 -4.3054094][-4.1830416 -4.1924739 -4.1954036 -4.1830072 -4.1600022 -4.1336718 -4.1205754 -4.1394515 -4.1712523 -4.1958709 -4.2178679 -4.244803 -4.2697124 -4.285749 -4.2889366][-4.139359 -4.1469364 -4.1504965 -4.1371422 -4.1061292 -4.0654693 -4.0365834 -4.0507708 -4.0930195 -4.1319981 -4.1687388 -4.2074881 -4.2394533 -4.2557621 -4.2560234][-4.0978918 -4.1029444 -4.1045761 -4.0871973 -4.0421448 -3.9781287 -3.9220595 -3.9264588 -3.9833674 -4.0472288 -4.1046062 -4.158093 -4.199513 -4.219614 -4.2178836][-4.0599093 -4.0555944 -4.0462232 -4.0193892 -3.9608204 -3.8743412 -3.7860837 -3.779283 -3.8593624 -3.9567466 -4.0381932 -4.10081 -4.1483088 -4.1738396 -4.1752839][-4.0307536 -4.0117879 -3.9891562 -3.9549937 -3.8997045 -3.8182845 -3.7287188 -3.7183704 -3.8076074 -3.9180198 -4.0056639 -4.0632615 -4.1038704 -4.1291041 -4.1388826][-4.0194221 -3.9922338 -3.9668255 -3.9396129 -3.9101429 -3.8697157 -3.8211007 -3.8172262 -3.8756883 -3.9566038 -4.0215392 -4.0584469 -4.0809522 -4.0971909 -4.1132984][-4.0334568 -4.0082817 -3.9903262 -3.9788246 -3.9754729 -3.968461 -3.9518909 -3.9503133 -3.9766111 -4.0193377 -4.0549884 -4.0749865 -4.0868683 -4.0961614 -4.1110578][-4.0634613 -4.0460324 -4.0381627 -4.038065 -4.0452423 -4.0513177 -4.046771 -4.0442896 -4.0530996 -4.0730205 -4.0919566 -4.1070046 -4.1196742 -4.1285305 -4.1391988][-4.0982761 -4.088851 -4.0890279 -4.0950937 -4.1056929 -4.11431 -4.1130648 -4.1098342 -4.1130543 -4.1239896 -4.1366706 -4.150743 -4.164598 -4.1739578 -4.1815729][-4.1323128 -4.1273966 -4.132226 -4.1405206 -4.1508927 -4.1578178 -4.157155 -4.1550817 -4.157299 -4.1649232 -4.1766429 -4.1912432 -4.2046256 -4.2146254 -4.221755][-4.1720991 -4.1694822 -4.1762214 -4.1851654 -4.1932435 -4.1965871 -4.1951361 -4.1931977 -4.1945167 -4.1999922 -4.2093773 -4.2217269 -4.233408 -4.2426152 -4.2488484]]...]
INFO - root - 2017-12-07 13:10:51.033629: step 12210, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 61h:45m:05s remains)
INFO - root - 2017-12-07 13:10:57.829227: step 12220, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 59h:06m:55s remains)
INFO - root - 2017-12-07 13:11:04.680332: step 12230, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 56h:17m:08s remains)
INFO - root - 2017-12-07 13:11:11.581098: step 12240, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 63h:38m:32s remains)
INFO - root - 2017-12-07 13:11:18.402341: step 12250, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 62h:51m:07s remains)
INFO - root - 2017-12-07 13:11:25.127933: step 12260, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 60h:24m:08s remains)
INFO - root - 2017-12-07 13:11:31.792165: step 12270, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 58h:08m:31s remains)
INFO - root - 2017-12-07 13:11:38.571553: step 12280, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 57h:26m:06s remains)
INFO - root - 2017-12-07 13:11:45.187168: step 12290, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 65h:11m:02s remains)
INFO - root - 2017-12-07 13:11:51.922164: step 12300, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 62h:47m:39s remains)
2017-12-07 13:11:52.618360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3187871 -4.2613916 -4.1897049 -4.1234097 -4.1109657 -4.1578717 -4.2173734 -4.2576461 -4.2711897 -4.2591314 -4.2452688 -4.2443352 -4.2623315 -4.2991691 -4.3233418][-4.3137484 -4.2563186 -4.18446 -4.1150589 -4.107233 -4.1584258 -4.2100816 -4.2395349 -4.244843 -4.2330217 -4.2282519 -4.2419367 -4.2682104 -4.3059673 -4.3298974][-4.3006744 -4.2474856 -4.1866946 -4.1273518 -4.1196475 -4.1610932 -4.1900482 -4.2011895 -4.1925898 -4.1828537 -4.1893268 -4.22108 -4.2614551 -4.3018832 -4.3272672][-4.291182 -4.2408419 -4.1916871 -4.1461878 -4.134531 -4.1521916 -4.1530228 -4.1477528 -4.1301532 -4.1283336 -4.151299 -4.1988173 -4.2473121 -4.2887712 -4.3160987][-4.2971992 -4.2539487 -4.2104654 -4.17092 -4.1453733 -4.1251211 -4.0936279 -4.0799408 -4.0715313 -4.0893459 -4.1375937 -4.2001014 -4.2504048 -4.2875919 -4.3130293][-4.3250723 -4.2945328 -4.2474751 -4.1947846 -4.1387396 -4.0702744 -4.0023422 -4.0031776 -4.033392 -4.0854635 -4.1550293 -4.2229285 -4.2683139 -4.2985396 -4.3183107][-4.350842 -4.3289003 -4.277061 -4.2014375 -4.10899 -3.9987857 -3.9099841 -3.9451466 -4.0255947 -4.1111708 -4.1872363 -4.246851 -4.2852912 -4.3087239 -4.3225651][-4.3674927 -4.3483 -4.2916932 -4.2031713 -4.0925217 -3.9687445 -3.8820133 -3.9426122 -4.0512772 -4.148098 -4.2181845 -4.2657022 -4.2971444 -4.3152804 -4.3235164][-4.38258 -4.3628454 -4.3019247 -4.2148528 -4.1079979 -3.9998209 -3.940865 -4.0081444 -4.1092377 -4.1945186 -4.25405 -4.28798 -4.3080654 -4.3189425 -4.3232088][-4.3950691 -4.3718262 -4.3089557 -4.226212 -4.1356821 -4.0605783 -4.037046 -4.0985985 -4.1763129 -4.2412405 -4.2858977 -4.3057628 -4.3128858 -4.3185287 -4.322854][-4.397963 -4.3769679 -4.3194246 -4.2447257 -4.1710653 -4.1257625 -4.1273394 -4.177485 -4.2313395 -4.2781329 -4.3058829 -4.3116293 -4.3104033 -4.315022 -4.3226757][-4.391727 -4.3757486 -4.3306775 -4.2748961 -4.2211137 -4.1969428 -4.2084036 -4.24245 -4.2753649 -4.3027792 -4.3114614 -4.3026581 -4.2955642 -4.3009377 -4.3145571][-4.3795533 -4.3672123 -4.3363814 -4.2998166 -4.264482 -4.2538891 -4.2681103 -4.2894773 -4.3074269 -4.3179989 -4.3092494 -4.2893662 -4.2790942 -4.2880764 -4.30808][-4.3677859 -4.3595686 -4.3387442 -4.3161182 -4.2950759 -4.2925568 -4.3062367 -4.3222055 -4.3275352 -4.3213539 -4.2997146 -4.2751846 -4.2662578 -4.2817059 -4.3079047][-4.35641 -4.352375 -4.336442 -4.3224244 -4.3116069 -4.3123074 -4.3209157 -4.3284488 -4.3248377 -4.3084922 -4.2824063 -4.2601504 -4.25695 -4.2794204 -4.3069582]]...]
INFO - root - 2017-12-07 13:11:59.417669: step 12310, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 58h:18m:48s remains)
INFO - root - 2017-12-07 13:12:06.297367: step 12320, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 64h:15m:06s remains)
INFO - root - 2017-12-07 13:12:12.920801: step 12330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 58h:49m:16s remains)
INFO - root - 2017-12-07 13:12:19.706146: step 12340, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 59h:47m:28s remains)
INFO - root - 2017-12-07 13:12:26.427680: step 12350, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 55h:51m:00s remains)
INFO - root - 2017-12-07 13:12:33.088807: step 12360, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 57h:43m:42s remains)
INFO - root - 2017-12-07 13:12:40.006420: step 12370, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.734 sec/batch; 65h:18m:01s remains)
INFO - root - 2017-12-07 13:12:46.813530: step 12380, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.675 sec/batch; 60h:03m:43s remains)
INFO - root - 2017-12-07 13:12:53.440030: step 12390, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 64h:35m:01s remains)
INFO - root - 2017-12-07 13:13:00.144044: step 12400, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 55h:12m:05s remains)
2017-12-07 13:13:00.962806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1600242 -4.1560326 -4.1651134 -4.1788845 -4.1933928 -4.2128048 -4.2193484 -4.2100878 -4.1935272 -4.1796479 -4.172554 -4.17 -4.1686573 -4.1775217 -4.2037582][-4.1730266 -4.1704931 -4.174242 -4.180563 -4.1877689 -4.2008557 -4.201189 -4.1870389 -4.1685553 -4.1579413 -4.1563544 -4.1570725 -4.1552029 -4.1643219 -4.1939964][-4.219007 -4.2140622 -4.2064452 -4.1989169 -4.196383 -4.2010431 -4.19653 -4.1828504 -4.170938 -4.1718655 -4.1810265 -4.1893039 -4.188951 -4.1951342 -4.2174053][-4.2582464 -4.25108 -4.2375879 -4.2209778 -4.2102876 -4.2061462 -4.1970429 -4.1838322 -4.1788559 -4.1915383 -4.2118783 -4.2279763 -4.2320533 -4.236939 -4.2493591][-4.2535386 -4.2473841 -4.2420759 -4.2327886 -4.221076 -4.2061906 -4.1822538 -4.1586237 -4.1558218 -4.177249 -4.2058916 -4.2306933 -4.2431951 -4.2512703 -4.2577248][-4.2071924 -4.2007089 -4.2070689 -4.2103114 -4.2035084 -4.1793184 -4.1338458 -4.0864091 -4.0799184 -4.1058774 -4.1424475 -4.1780887 -4.201139 -4.2157726 -4.2193594][-4.1635413 -4.1518922 -4.1593885 -4.1690702 -4.1603804 -4.1228218 -4.0534844 -3.9812357 -3.9669356 -3.9925869 -4.0315232 -4.0765872 -4.1128945 -4.1359425 -4.1402559][-4.1730289 -4.1606317 -4.162838 -4.167387 -4.1491079 -4.104382 -4.025167 -3.9446828 -3.9271832 -3.9494343 -3.9771256 -4.0072513 -4.033493 -4.0498271 -4.0516124][-4.216888 -4.2067122 -4.2050691 -4.20465 -4.1858897 -4.1553802 -4.0978847 -4.0385737 -4.0224266 -4.0328484 -4.0417948 -4.0497856 -4.04961 -4.0379963 -4.0232906][-4.2311745 -4.2199707 -4.2158914 -4.216414 -4.2070107 -4.1991315 -4.1737423 -4.1415434 -4.131144 -4.134449 -4.1333528 -4.1340823 -4.1290646 -4.10821 -4.0817986][-4.2084074 -4.19613 -4.1913395 -4.192555 -4.1927791 -4.20111 -4.1991768 -4.1889677 -4.1879382 -4.1895008 -4.18225 -4.1774731 -4.1759624 -4.1636791 -4.1438947][-4.2001209 -4.1851807 -4.1768703 -4.1770458 -4.179225 -4.1908236 -4.197454 -4.1989784 -4.2051978 -4.2061887 -4.1927314 -4.1809449 -4.1794162 -4.1732836 -4.1637731][-4.2115979 -4.1985035 -4.1895471 -4.1862063 -4.1843328 -4.1908035 -4.1962829 -4.2005367 -4.2066207 -4.2049031 -4.1897221 -4.1754532 -4.1714411 -4.1650281 -4.1590877][-4.2227077 -4.2202339 -4.2165813 -4.212687 -4.2065172 -4.2040162 -4.202673 -4.2047076 -4.2068238 -4.1998429 -4.1866131 -4.1751313 -4.169517 -4.1592321 -4.1503406][-4.2113061 -4.2234454 -4.2292361 -4.2291183 -4.2231317 -4.2145839 -4.2073684 -4.2052646 -4.1995096 -4.1861143 -4.1764112 -4.1740556 -4.1719513 -4.1594214 -4.145658]]...]
INFO - root - 2017-12-07 13:13:07.761793: step 12410, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 61h:58m:33s remains)
INFO - root - 2017-12-07 13:13:14.646799: step 12420, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 58h:49m:06s remains)
INFO - root - 2017-12-07 13:13:21.364016: step 12430, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 55h:45m:21s remains)
INFO - root - 2017-12-07 13:13:28.161596: step 12440, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 58h:30m:20s remains)
INFO - root - 2017-12-07 13:13:34.999984: step 12450, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 65h:51m:48s remains)
INFO - root - 2017-12-07 13:13:41.756858: step 12460, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 57h:57m:13s remains)
INFO - root - 2017-12-07 13:13:48.495890: step 12470, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 58h:45m:46s remains)
INFO - root - 2017-12-07 13:13:55.254823: step 12480, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 55h:31m:02s remains)
INFO - root - 2017-12-07 13:14:01.819992: step 12490, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 56h:10m:08s remains)
INFO - root - 2017-12-07 13:14:08.609116: step 12500, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 65h:24m:02s remains)
2017-12-07 13:14:09.307039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2643614 -4.2673349 -4.2722754 -4.2595687 -4.2384768 -4.2132072 -4.1940813 -4.1823063 -4.170815 -4.1643004 -4.1723356 -4.2026277 -4.2362633 -4.2711468 -4.2992339][-4.26651 -4.2679129 -4.2722006 -4.2597566 -4.2370181 -4.2088933 -4.1809373 -4.162385 -4.1463928 -4.1387053 -4.147943 -4.18411 -4.2297425 -4.2722106 -4.3038111][-4.2779622 -4.2777524 -4.2829494 -4.2744517 -4.2541032 -4.2276759 -4.2005296 -4.183526 -4.1658483 -4.1544261 -4.1643858 -4.2005463 -4.2458882 -4.2863259 -4.3142982][-4.28589 -4.2885218 -4.2939143 -4.2890816 -4.2710357 -4.2453079 -4.2241168 -4.2132215 -4.1993566 -4.1928191 -4.2067537 -4.2352948 -4.2704005 -4.3038063 -4.3263144][-4.2840338 -4.2888269 -4.292469 -4.285902 -4.2676105 -4.2442031 -4.2264853 -4.2183604 -4.2067533 -4.2047582 -4.2222376 -4.2463541 -4.2748637 -4.3075891 -4.3296318][-4.2757764 -4.28033 -4.278975 -4.2630253 -4.2347207 -4.2005014 -4.1733966 -4.1589828 -4.1463575 -4.1512237 -4.182549 -4.2202411 -4.2559896 -4.2942634 -4.3205771][-4.2641273 -4.263402 -4.2501988 -4.2150617 -4.1631804 -4.1045003 -4.0549011 -4.0342464 -4.0339661 -4.0578022 -4.1119175 -4.1727242 -4.2218838 -4.2690072 -4.3016429][-4.2563004 -4.2483859 -4.2210283 -4.1683159 -4.0988269 -4.024663 -3.9700675 -3.9653246 -3.9960237 -4.041635 -4.1087747 -4.17552 -4.2250175 -4.2663684 -4.2955093][-4.2633271 -4.2509084 -4.2196012 -4.1680212 -4.1076818 -4.0531926 -4.0280504 -4.0462527 -4.0834718 -4.1232209 -4.1780229 -4.2283392 -4.2590942 -4.2823648 -4.300436][-4.2707486 -4.26167 -4.2371492 -4.1975703 -4.15978 -4.1327477 -4.1316195 -4.1552758 -4.1787658 -4.1987472 -4.2327814 -4.2630453 -4.2775164 -4.2882285 -4.2987061][-4.2615318 -4.2603569 -4.2476315 -4.2223053 -4.2015696 -4.1927466 -4.2021351 -4.2241421 -4.2348914 -4.235249 -4.2482138 -4.2613959 -4.2693725 -4.2796745 -4.2910423][-4.2547455 -4.2602592 -4.256701 -4.23994 -4.2223444 -4.2182884 -4.230865 -4.2512622 -4.2540183 -4.241034 -4.2401929 -4.243638 -4.2516241 -4.2661238 -4.2802334][-4.2456126 -4.2560763 -4.2558889 -4.2385464 -4.2177806 -4.2111325 -4.2278972 -4.2506151 -4.2543182 -4.2403021 -4.2357507 -4.2378578 -4.2449908 -4.2608662 -4.2768049][-4.2233109 -4.2342119 -4.2300525 -4.2030678 -4.1777 -4.1743641 -4.197854 -4.2281451 -4.2408309 -4.2389541 -4.2425871 -4.2499447 -4.2577987 -4.271986 -4.2842178][-4.1851072 -4.1934834 -4.1801858 -4.1444135 -4.1189947 -4.1250935 -4.1573367 -4.194459 -4.2191019 -4.2343965 -4.2489929 -4.2611117 -4.27069 -4.2816534 -4.28732]]...]
INFO - root - 2017-12-07 13:14:16.007161: step 12510, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.607 sec/batch; 53h:59m:24s remains)
INFO - root - 2017-12-07 13:14:22.796771: step 12520, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 58h:50m:42s remains)
INFO - root - 2017-12-07 13:14:29.539518: step 12530, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 64h:05m:55s remains)
INFO - root - 2017-12-07 13:14:36.427831: step 12540, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 62h:35m:12s remains)
INFO - root - 2017-12-07 13:14:43.236600: step 12550, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 55h:03m:13s remains)
INFO - root - 2017-12-07 13:14:50.030682: step 12560, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 54h:46m:00s remains)
INFO - root - 2017-12-07 13:14:56.899447: step 12570, loss = 2.04, batch loss = 1.98 (10.1 examples/sec; 0.788 sec/batch; 70h:03m:17s remains)
INFO - root - 2017-12-07 13:15:03.656064: step 12580, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 65h:27m:35s remains)
INFO - root - 2017-12-07 13:15:10.449532: step 12590, loss = 2.03, batch loss = 1.98 (11.3 examples/sec; 0.710 sec/batch; 63h:07m:43s remains)
INFO - root - 2017-12-07 13:15:17.239367: step 12600, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 55h:15m:10s remains)
2017-12-07 13:15:17.987308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1169538 -4.1145387 -4.0971465 -4.0707917 -4.0601592 -4.0711241 -4.092432 -4.1120138 -4.1182346 -4.1079016 -4.0890608 -4.0878277 -4.1158762 -4.1501904 -4.1863136][-4.1087832 -4.1044569 -4.0795765 -4.0475478 -4.0334992 -4.0358758 -4.0468807 -4.0630221 -4.0762668 -4.0760174 -4.0581264 -4.052731 -4.0834327 -4.128335 -4.1749754][-4.1006193 -4.09717 -4.0719342 -4.0393987 -4.0226479 -4.0158787 -4.0190411 -4.03725 -4.0653019 -4.0815454 -4.0706129 -4.0616875 -4.0817533 -4.1210184 -4.1690111][-4.0843096 -4.0833707 -4.0600986 -4.0280433 -4.0088243 -3.993695 -3.9934649 -4.0160036 -4.0574589 -4.0912356 -4.0917597 -4.0881324 -4.099102 -4.1254516 -4.166728][-4.0792212 -4.0769019 -4.0548182 -4.0271707 -3.9998679 -3.972209 -3.9600835 -3.9766016 -4.0257821 -4.0792313 -4.1015882 -4.1129246 -4.1228781 -4.1384006 -4.1705813][-4.0827413 -4.0832977 -4.0667534 -4.0420523 -4.0039272 -3.9547806 -3.9198265 -3.9208596 -3.9703147 -4.0352626 -4.0833058 -4.11817 -4.1394715 -4.1552873 -4.1826763][-4.0874224 -4.095953 -4.0868669 -4.0674553 -4.0244365 -3.9589458 -3.9062948 -3.896723 -3.9357982 -3.995573 -4.0521927 -4.1036363 -4.145484 -4.17306 -4.1997046][-4.1034021 -4.1140532 -4.1122537 -4.0952826 -4.0550036 -3.991925 -3.942183 -3.9306362 -3.9540625 -3.996022 -4.0440178 -4.0978327 -4.1520233 -4.1904016 -4.2154675][-4.1345687 -4.14351 -4.148663 -4.1346979 -4.1016679 -4.051538 -4.0116363 -3.9990184 -4.0082545 -4.0338058 -4.0681825 -4.1136694 -4.1668496 -4.206965 -4.2283788][-4.1729789 -4.1824031 -4.1892061 -4.175849 -4.1480761 -4.1101584 -4.0827942 -4.0746717 -4.078001 -4.090827 -4.112956 -4.1461711 -4.1897988 -4.2257586 -4.2449369][-4.2018204 -4.2121019 -4.2176685 -4.2041664 -4.1792893 -4.1506462 -4.1340036 -4.1298609 -4.1313324 -4.1376777 -4.1521211 -4.1786857 -4.2173371 -4.2497854 -4.266953][-4.223176 -4.2297263 -4.2295814 -4.2199535 -4.2024703 -4.183876 -4.1767883 -4.1753988 -4.1744084 -4.1777582 -4.1871209 -4.2086787 -4.2439737 -4.2722359 -4.286922][-4.2500677 -4.2537565 -4.2481971 -4.2404475 -4.2298908 -4.2211757 -4.2198915 -4.2192931 -4.2174726 -4.2196336 -4.2266774 -4.2432256 -4.2697868 -4.2918735 -4.302567][-4.2739182 -4.2756066 -4.2696705 -4.2643204 -4.2596087 -4.2562623 -4.2559414 -4.2541609 -4.25383 -4.2572 -4.2622614 -4.2742028 -4.2924457 -4.3070793 -4.313262][-4.2806239 -4.2812867 -4.2769818 -4.2733908 -4.2709751 -4.2700133 -4.2700529 -4.2695971 -4.2701359 -4.2748046 -4.2801785 -4.2893348 -4.3015304 -4.3119774 -4.3179712]]...]
INFO - root - 2017-12-07 13:15:24.817522: step 12610, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 60h:19m:52s remains)
INFO - root - 2017-12-07 13:15:31.568211: step 12620, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 57h:35m:43s remains)
INFO - root - 2017-12-07 13:15:38.266738: step 12630, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 56h:24m:46s remains)
INFO - root - 2017-12-07 13:15:44.968529: step 12640, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 65h:05m:45s remains)
INFO - root - 2017-12-07 13:15:51.801207: step 12650, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 63h:11m:10s remains)
INFO - root - 2017-12-07 13:15:58.576899: step 12660, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 59h:56m:04s remains)
INFO - root - 2017-12-07 13:16:05.331524: step 12670, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 58h:33m:29s remains)
INFO - root - 2017-12-07 13:16:12.146611: step 12680, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 55h:35m:46s remains)
INFO - root - 2017-12-07 13:16:18.893523: step 12690, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:14m:27s remains)
INFO - root - 2017-12-07 13:16:25.735283: step 12700, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.705 sec/batch; 62h:35m:08s remains)
2017-12-07 13:16:26.402969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31096 -4.3204679 -4.3239551 -4.3239937 -4.3211107 -4.3214412 -4.3256226 -4.3316789 -4.3374925 -4.3426871 -4.3441329 -4.336473 -4.306632 -4.2702727 -4.2361326][-4.3157029 -4.3269668 -4.3333297 -4.3350677 -4.3331852 -4.3334503 -4.3335452 -4.3325729 -4.3324041 -4.332746 -4.3315315 -4.3211703 -4.2952857 -4.2649312 -4.2355165][-4.3136754 -4.3280892 -4.3388782 -4.3436537 -4.3392978 -4.3315673 -4.3229847 -4.3140836 -4.3082323 -4.305234 -4.3032551 -4.29179 -4.2715421 -4.2502031 -4.226965][-4.3169117 -4.3301063 -4.3401351 -4.3410721 -4.3266754 -4.3036094 -4.2834496 -4.2628403 -4.2514138 -4.247664 -4.2457857 -4.237144 -4.2231507 -4.213562 -4.202487][-4.3138752 -4.322011 -4.3233404 -4.3106709 -4.2805014 -4.2454815 -4.2137876 -4.1742263 -4.157064 -4.1675086 -4.1775002 -4.1745634 -4.16997 -4.1714811 -4.1674991][-4.2935576 -4.2934995 -4.2828255 -4.2547207 -4.2118607 -4.1724105 -4.1269908 -4.0550508 -4.0253983 -4.0578861 -4.0898533 -4.0988894 -4.1086078 -4.1198273 -4.1227674][-4.253633 -4.2362175 -4.206521 -4.1615467 -4.1134887 -4.0727754 -4.0076966 -3.8962464 -3.8494897 -3.9259982 -3.996136 -4.0264592 -4.0526237 -4.0771265 -4.0833073][-4.1998916 -4.1582694 -4.1094489 -4.0594687 -4.0119328 -3.9695828 -3.9011488 -3.7848229 -3.7558196 -3.8799121 -3.976814 -4.0171213 -4.0527253 -4.0778384 -4.0736003][-4.1496463 -4.0956693 -4.0439763 -4.0052361 -3.9677277 -3.9436612 -3.925034 -3.8896551 -3.9053564 -3.9960575 -4.0654559 -4.0938869 -4.1241245 -4.1384869 -4.12421][-4.1319079 -4.0915785 -4.0632057 -4.0491242 -4.0305471 -4.0265245 -4.0459032 -4.0591197 -4.0861688 -4.13549 -4.1702471 -4.1896157 -4.2109222 -4.2144866 -4.2002945][-4.1591263 -4.146966 -4.1471672 -4.1501217 -4.141644 -4.1438289 -4.1693058 -4.1901212 -4.2068558 -4.2293367 -4.244452 -4.2554517 -4.2677441 -4.2677069 -4.2624135][-4.1994257 -4.210763 -4.2285028 -4.2385893 -4.2354789 -4.2385306 -4.2587519 -4.2730584 -4.2770214 -4.2856274 -4.2887678 -4.2898917 -4.2946167 -4.2955065 -4.2983294][-4.2120147 -4.236486 -4.2659535 -4.2846122 -4.2863603 -4.2862444 -4.2975869 -4.3055792 -4.3033328 -4.3011508 -4.2962193 -4.2923517 -4.2905517 -4.2900605 -4.2953191][-4.1819191 -4.2150455 -4.2540669 -4.2791791 -4.283319 -4.2802944 -4.2843952 -4.2876072 -4.28114 -4.2730546 -4.2653084 -4.2586927 -4.2526474 -4.2518253 -4.2583928][-4.1208344 -4.1517177 -4.1947575 -4.2216988 -4.2262092 -4.2224808 -4.2223864 -4.2209616 -4.21366 -4.2053242 -4.197978 -4.1906996 -4.1829357 -4.1828213 -4.1920338]]...]
INFO - root - 2017-12-07 13:16:33.282110: step 12710, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 61h:18m:32s remains)
INFO - root - 2017-12-07 13:16:40.143211: step 12720, loss = 2.06, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 66h:46m:52s remains)
INFO - root - 2017-12-07 13:16:46.841899: step 12730, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 59h:31m:45s remains)
INFO - root - 2017-12-07 13:16:53.593983: step 12740, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 56h:11m:21s remains)
INFO - root - 2017-12-07 13:17:00.419208: step 12750, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 56h:13m:15s remains)
INFO - root - 2017-12-07 13:17:07.319014: step 12760, loss = 2.08, batch loss = 2.03 (10.7 examples/sec; 0.746 sec/batch; 66h:15m:55s remains)
INFO - root - 2017-12-07 13:17:14.107042: step 12770, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 63h:30m:25s remains)
INFO - root - 2017-12-07 13:17:20.951426: step 12780, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 58h:56m:00s remains)
INFO - root - 2017-12-07 13:17:27.574526: step 12790, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 55h:16m:37s remains)
INFO - root - 2017-12-07 13:17:34.462998: step 12800, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 63h:30m:21s remains)
2017-12-07 13:17:35.210402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2211161 -4.2260695 -4.2374811 -4.2455697 -4.2551346 -4.2647882 -4.2720265 -4.2687874 -4.25564 -4.2356267 -4.2221451 -4.2177911 -4.2133222 -4.2047663 -4.1974483][-4.1993203 -4.2093077 -4.225174 -4.229321 -4.233295 -4.2421975 -4.2485175 -4.2434363 -4.2343807 -4.2205634 -4.2116818 -4.2065907 -4.1919909 -4.1713529 -4.1598363][-4.1895232 -4.2031012 -4.2184134 -4.2163672 -4.2131486 -4.2175593 -4.2199993 -4.2117171 -4.206224 -4.203114 -4.2061791 -4.2073832 -4.1871157 -4.1562266 -4.1463795][-4.1976442 -4.2103548 -4.21722 -4.20638 -4.1934156 -4.1917815 -4.1845121 -4.17062 -4.16863 -4.1792388 -4.1974483 -4.2095356 -4.1887035 -4.149682 -4.1443677][-4.2187681 -4.2247043 -4.2196803 -4.2000294 -4.1797485 -4.1690631 -4.1451483 -4.1140766 -4.1162877 -4.1492877 -4.1860409 -4.2124543 -4.193522 -4.1460671 -4.1437025][-4.2414842 -4.239387 -4.221983 -4.1912236 -4.1607461 -4.1393037 -4.0965281 -4.0402517 -4.043097 -4.1053123 -4.165091 -4.2059 -4.1908011 -4.1391382 -4.1406221][-4.2526393 -4.2440958 -4.2205849 -4.1808567 -4.1375928 -4.1024785 -4.0442476 -3.965708 -3.9676569 -4.0572758 -4.1369643 -4.18811 -4.1767511 -4.12823 -4.1348224][-4.2505846 -4.2428088 -4.21802 -4.1773319 -4.1322508 -4.0918918 -4.0263414 -3.9327104 -3.9284842 -4.0286632 -4.1140866 -4.168231 -4.15988 -4.1164465 -4.1300173][-4.2525139 -4.2501335 -4.2301321 -4.1967425 -4.1584926 -4.1207285 -4.0619936 -3.9752231 -3.9660482 -4.0470247 -4.1177883 -4.1649194 -4.1567245 -4.1205087 -4.1391015][-4.2536035 -4.2548566 -4.2405138 -4.2190628 -4.1937213 -4.1643543 -4.1209373 -4.0530691 -4.0400791 -4.0902343 -4.1386127 -4.1752748 -4.1680026 -4.1391907 -4.1583734][-4.2480845 -4.2491155 -4.2368155 -4.2215505 -4.2055683 -4.1871729 -4.159903 -4.1111436 -4.09586 -4.1206923 -4.1508808 -4.1778626 -4.1708722 -4.1498585 -4.1681786][-4.2392287 -4.2381687 -4.2229276 -4.2106814 -4.2006068 -4.1903276 -4.1737103 -4.1411972 -4.1293631 -4.1410332 -4.1588964 -4.1788483 -4.1727486 -4.1571045 -4.17053][-4.2356558 -4.23298 -4.2172394 -4.2077851 -4.2026606 -4.1974087 -4.1872959 -4.165184 -4.15786 -4.1656814 -4.1756878 -4.18978 -4.1849227 -4.1723475 -4.1785569][-4.2434773 -4.2397165 -4.2260656 -4.2191548 -4.2178674 -4.21612 -4.2102714 -4.1962137 -4.1923914 -4.197053 -4.2005911 -4.2094097 -4.2059503 -4.1950788 -4.1954432][-4.2579308 -4.2544003 -4.2435179 -4.2380824 -4.238884 -4.2391047 -4.2357469 -4.2277331 -4.2266936 -4.2306371 -4.2312908 -4.2361236 -4.2338386 -4.2257066 -4.2233529]]...]
INFO - root - 2017-12-07 13:17:41.989224: step 12810, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 55h:58m:15s remains)
INFO - root - 2017-12-07 13:17:48.818686: step 12820, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 57h:43m:25s remains)
INFO - root - 2017-12-07 13:17:55.600019: step 12830, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.756 sec/batch; 67h:07m:34s remains)
INFO - root - 2017-12-07 13:18:02.483921: step 12840, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 63h:47m:55s remains)
INFO - root - 2017-12-07 13:18:09.445695: step 12850, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 60h:15m:21s remains)
INFO - root - 2017-12-07 13:18:16.210218: step 12860, loss = 2.06, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 55h:27m:15s remains)
INFO - root - 2017-12-07 13:18:23.092477: step 12870, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 60h:21m:44s remains)
INFO - root - 2017-12-07 13:18:29.947294: step 12880, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 62h:47m:30s remains)
INFO - root - 2017-12-07 13:18:36.584187: step 12890, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 60h:30m:53s remains)
INFO - root - 2017-12-07 13:18:43.365333: step 12900, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 58h:47m:54s remains)
2017-12-07 13:18:44.059638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2581234 -4.26856 -4.2841578 -4.2944789 -4.2987614 -4.2991977 -4.3028455 -4.3097906 -4.3176484 -4.3213406 -4.3194981 -4.3056479 -4.2806096 -4.2558808 -4.2459373][-4.2306867 -4.2469816 -4.2724514 -4.2889376 -4.2929158 -4.2905774 -4.2922792 -4.2987747 -4.31045 -4.3181977 -4.3139358 -4.2911549 -4.250668 -4.210402 -4.1941376][-4.2013702 -4.2200837 -4.2537055 -4.2789888 -4.2856283 -4.2809653 -4.2773991 -4.2759032 -4.2826357 -4.2924061 -4.2916274 -4.2679877 -4.21824 -4.1653104 -4.1457262][-4.1784606 -4.1948643 -4.2323737 -4.2633004 -4.2719169 -4.2629576 -4.2500634 -4.2384024 -4.2389679 -4.2542305 -4.2650132 -4.2500744 -4.2039266 -4.1515841 -4.1320491][-4.1702657 -4.182519 -4.2174363 -4.246069 -4.2516308 -4.2352996 -4.20762 -4.1768441 -4.1706386 -4.2026024 -4.2366447 -4.2377906 -4.205472 -4.1639719 -4.1465569][-4.1612883 -4.1689153 -4.2012272 -4.2255869 -4.2255359 -4.1979551 -4.1419244 -4.0786605 -4.0651913 -4.1238527 -4.1900134 -4.2132049 -4.1994643 -4.1744456 -4.1638136][-4.1574831 -4.161644 -4.1930332 -4.2153749 -4.21005 -4.1681004 -4.0808754 -3.9746032 -3.94939 -4.0377517 -4.1322412 -4.1795487 -4.1856947 -4.1737504 -4.1677346][-4.1588769 -4.1637511 -4.199049 -4.2258267 -4.2204571 -4.1715913 -4.065114 -3.9277248 -3.8908253 -3.9931102 -4.1009245 -4.1633348 -4.1836305 -4.1823959 -4.1794553][-4.1644711 -4.1682281 -4.2014647 -4.235466 -4.2409949 -4.2011533 -4.1050038 -3.9827671 -3.9472177 -4.0280538 -4.1184239 -4.1745338 -4.1972523 -4.2011981 -4.1988997][-4.1747646 -4.176332 -4.2018666 -4.2342029 -4.2493954 -4.2280903 -4.1657414 -4.08463 -4.0575137 -4.102632 -4.160933 -4.2035809 -4.225769 -4.2314363 -4.2274842][-4.188159 -4.1861644 -4.2003984 -4.2192526 -4.2335768 -4.2282391 -4.1982536 -4.1523943 -4.1336727 -4.1562786 -4.1883869 -4.2152743 -4.2320008 -4.2369175 -4.2350459][-4.1971703 -4.192512 -4.1926742 -4.196444 -4.2052636 -4.2079492 -4.2002 -4.1792011 -4.1683569 -4.1786194 -4.1936951 -4.2090616 -4.2220058 -4.2288857 -4.2294297][-4.1995406 -4.1950469 -4.1890545 -4.1892958 -4.1957974 -4.2017317 -4.2043352 -4.198782 -4.19475 -4.1949492 -4.1949954 -4.2008057 -4.2103415 -4.2187271 -4.2221818][-4.1956811 -4.1886153 -4.1859946 -4.1955786 -4.20566 -4.2118573 -4.216804 -4.2173815 -4.2173362 -4.2112317 -4.2014513 -4.2005577 -4.2096105 -4.2213106 -4.2291336][-4.1877055 -4.1764417 -4.1709685 -4.1879635 -4.2087483 -4.2186737 -4.2219553 -4.2232232 -4.2257614 -4.22083 -4.2107534 -4.2083678 -4.218925 -4.2330356 -4.2427869]]...]
INFO - root - 2017-12-07 13:18:50.984837: step 12910, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 62h:59m:17s remains)
INFO - root - 2017-12-07 13:18:57.758557: step 12920, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 62h:32m:09s remains)
INFO - root - 2017-12-07 13:19:04.533307: step 12930, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 59h:50m:16s remains)
INFO - root - 2017-12-07 13:19:11.292213: step 12940, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 56h:41m:37s remains)
INFO - root - 2017-12-07 13:19:18.052815: step 12950, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 65h:47m:48s remains)
INFO - root - 2017-12-07 13:19:24.862215: step 12960, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.708 sec/batch; 62h:48m:57s remains)
INFO - root - 2017-12-07 13:19:31.562868: step 12970, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 55h:26m:02s remains)
INFO - root - 2017-12-07 13:19:38.389927: step 12980, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 56h:35m:33s remains)
INFO - root - 2017-12-07 13:19:45.125226: step 12990, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 64h:15m:34s remains)
INFO - root - 2017-12-07 13:19:51.889375: step 13000, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 64h:56m:29s remains)
2017-12-07 13:19:52.602765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31989 -4.3192143 -4.3135605 -4.3087606 -4.3065753 -4.3054295 -4.3040543 -4.3035169 -4.3046713 -4.3061423 -4.3075533 -4.3087726 -4.310689 -4.3128252 -4.3138533][-4.3162246 -4.3146868 -4.3090463 -4.3044567 -4.3022637 -4.3009143 -4.2991595 -4.29805 -4.2984791 -4.2995362 -4.3011489 -4.3040957 -4.3087668 -4.3132529 -4.3153243][-4.316195 -4.3141112 -4.308351 -4.3019261 -4.2953262 -4.288343 -4.28132 -4.2764888 -4.2753267 -4.2777119 -4.282845 -4.2902637 -4.3002925 -4.3097639 -4.3153257][-4.3139548 -4.3089151 -4.2999506 -4.2869849 -4.26952 -4.2495351 -4.231297 -4.2195988 -4.217659 -4.2255087 -4.2401524 -4.2581139 -4.2788057 -4.29776 -4.3103666][-4.3130126 -4.3029 -4.2866669 -4.2603588 -4.2228274 -4.1798835 -4.1424294 -4.1204619 -4.1185093 -4.1364236 -4.1674261 -4.2036858 -4.2418447 -4.2753859 -4.2986994][-4.3050528 -4.2936893 -4.2708969 -4.2287679 -4.1651549 -4.0920434 -4.0315771 -4.0015669 -4.0034528 -4.0351319 -4.0880446 -4.1470671 -4.204319 -4.2521992 -4.2854104][-4.27387 -4.2689848 -4.2496357 -4.2026453 -4.1243391 -4.0289845 -3.950212 -3.9170771 -3.9262297 -3.9727187 -4.0477 -4.125186 -4.1929078 -4.2463245 -4.2809072][-4.233428 -4.2417293 -4.236289 -4.2031965 -4.1372395 -4.04951 -3.9733281 -3.9428339 -3.9550719 -4.0042806 -4.0829072 -4.1597347 -4.220468 -4.2637548 -4.2877684][-4.1769171 -4.203599 -4.2236538 -4.2217836 -4.1919131 -4.1372843 -4.082612 -4.0582747 -4.0672374 -4.1053033 -4.1650624 -4.22205 -4.2631321 -4.2891078 -4.2996325][-4.1264625 -4.1642747 -4.2064056 -4.2368054 -4.2460604 -4.2274389 -4.1959715 -4.1768117 -4.1797132 -4.2032695 -4.2399869 -4.2747474 -4.2974563 -4.3076878 -4.3078666][-4.0962095 -4.1360979 -4.186264 -4.2361555 -4.2750015 -4.28707 -4.2773042 -4.2641106 -4.2609749 -4.2713342 -4.290966 -4.3095965 -4.3195391 -4.3189845 -4.3123603][-4.0805564 -4.1081533 -4.1533146 -4.2102365 -4.2672811 -4.3048549 -4.3160372 -4.3117986 -4.3054037 -4.3041849 -4.3113112 -4.3197641 -4.3234253 -4.3184066 -4.3077235][-4.0743084 -4.0860357 -4.1199231 -4.1741314 -4.2388253 -4.2923965 -4.3183274 -4.3194413 -4.305953 -4.2918034 -4.28987 -4.2945271 -4.2994547 -4.2987571 -4.2897892][-4.076406 -4.0782528 -4.1038942 -4.1526227 -4.2170472 -4.2739882 -4.299293 -4.2918725 -4.2628551 -4.2339625 -4.2254734 -4.2329555 -4.24805 -4.2596984 -4.2556167][-4.0980573 -4.0951333 -4.1150374 -4.1568122 -4.2153153 -4.2633958 -4.2735472 -4.2454352 -4.1939564 -4.1479206 -4.1345181 -4.1488471 -4.1785769 -4.2046061 -4.2048931]]...]
INFO - root - 2017-12-07 13:19:59.349922: step 13010, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 59h:38m:38s remains)
INFO - root - 2017-12-07 13:20:06.209891: step 13020, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 66h:43m:43s remains)
INFO - root - 2017-12-07 13:20:12.881467: step 13030, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 59h:40m:07s remains)
INFO - root - 2017-12-07 13:20:19.610574: step 13040, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 59h:59m:28s remains)
INFO - root - 2017-12-07 13:20:26.377541: step 13050, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.624 sec/batch; 55h:21m:49s remains)
INFO - root - 2017-12-07 13:20:33.175720: step 13060, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 60h:21m:08s remains)
INFO - root - 2017-12-07 13:20:39.931380: step 13070, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 64h:05m:16s remains)
INFO - root - 2017-12-07 13:20:46.636667: step 13080, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 61h:35m:02s remains)
INFO - root - 2017-12-07 13:20:53.162023: step 13090, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 58h:42m:50s remains)
INFO - root - 2017-12-07 13:20:59.865312: step 13100, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 55h:35m:06s remains)
2017-12-07 13:21:00.713155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3331332 -4.3302875 -4.3198161 -4.307992 -4.3013186 -4.3029604 -4.3097324 -4.31726 -4.3227715 -4.3282571 -4.3314514 -4.3329587 -4.3336048 -4.3312292 -4.3252659][-4.3043051 -4.3010283 -4.2892103 -4.2739573 -4.2628541 -4.2622509 -4.2668319 -4.2736826 -4.2819548 -4.2937565 -4.3033404 -4.3104286 -4.3169322 -4.3212547 -4.3211379][-4.2609344 -4.2573123 -4.2453976 -4.2283511 -4.2155895 -4.2122092 -4.2113447 -4.2123928 -4.2198057 -4.2362075 -4.2513561 -4.2652788 -4.2807059 -4.294436 -4.3023639][-4.2208905 -4.2144561 -4.2009954 -4.1844306 -4.172358 -4.1634927 -4.1517115 -4.14412 -4.1514373 -4.1746111 -4.195797 -4.21589 -4.2406092 -4.2627892 -4.2769094][-4.2024665 -4.1926351 -4.1775846 -4.1644959 -4.1523638 -4.1344934 -4.1080246 -4.0896621 -4.0971832 -4.127687 -4.1527448 -4.1747613 -4.2030826 -4.2289333 -4.24552][-4.199964 -4.1852956 -4.1631851 -4.146997 -4.1314235 -4.106658 -4.0730629 -4.0527892 -4.0646205 -4.1035166 -4.136971 -4.1631112 -4.1909294 -4.2139516 -4.2272849][-4.2100048 -4.1913629 -4.1609387 -4.1371222 -4.1116309 -4.0796986 -4.0441995 -4.0243444 -4.0402365 -4.0837407 -4.1254277 -4.1595507 -4.1886926 -4.2104754 -4.2220712][-4.2306643 -4.21114 -4.1780457 -4.1465979 -4.1067986 -4.0626125 -4.0257297 -4.0073586 -4.0246396 -4.065434 -4.1126924 -4.1559739 -4.1888652 -4.2109056 -4.2253852][-4.2546377 -4.2364349 -4.2088428 -4.1779366 -4.1352859 -4.0865006 -4.0504589 -4.0328803 -4.0425262 -4.0714154 -4.1157947 -4.1594477 -4.192369 -4.216064 -4.2356715][-4.2742076 -4.2593246 -4.242137 -4.2210245 -4.1863036 -4.1461806 -4.1172843 -4.1031637 -4.1049247 -4.1198721 -4.1533313 -4.1859236 -4.21083 -4.2303524 -4.2498727][-4.2899218 -4.2788448 -4.2676125 -4.25723 -4.239368 -4.2178836 -4.2038779 -4.196548 -4.1937857 -4.2003307 -4.2210841 -4.2388973 -4.2495909 -4.2575192 -4.2691169][-4.2983985 -4.2892432 -4.2785048 -4.2724681 -4.2677608 -4.2633862 -4.2632351 -4.2658715 -4.26669 -4.2721186 -4.2834492 -4.2894287 -4.2893553 -4.2878613 -4.2908192][-4.2959704 -4.2870808 -4.2752824 -4.2678337 -4.2665749 -4.2683721 -4.2740602 -4.2828255 -4.2902732 -4.299458 -4.3064938 -4.3093009 -4.3088784 -4.305716 -4.3051906][-4.281374 -4.2702141 -4.2577767 -4.2489839 -4.2447491 -4.2443805 -4.2476492 -4.2567639 -4.27047 -4.2856083 -4.2948322 -4.30165 -4.3048372 -4.3044691 -4.3055005][-4.2585969 -4.2426076 -4.2269621 -4.2182665 -4.20964 -4.1993542 -4.1930351 -4.2000442 -4.2234454 -4.2488313 -4.2657518 -4.2803607 -4.2886233 -4.2913475 -4.2953868]]...]
INFO - root - 2017-12-07 13:21:07.516094: step 13110, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 63h:03m:45s remains)
INFO - root - 2017-12-07 13:21:14.327942: step 13120, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 60h:10m:02s remains)
INFO - root - 2017-12-07 13:21:21.024850: step 13130, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 56h:15m:12s remains)
INFO - root - 2017-12-07 13:21:27.855403: step 13140, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 59h:35m:08s remains)
INFO - root - 2017-12-07 13:21:34.651903: step 13150, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 64h:05m:47s remains)
INFO - root - 2017-12-07 13:21:41.442417: step 13160, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 62h:20m:27s remains)
INFO - root - 2017-12-07 13:21:48.153137: step 13170, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.663 sec/batch; 58h:51m:10s remains)
INFO - root - 2017-12-07 13:21:54.870906: step 13180, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 55h:52m:34s remains)
INFO - root - 2017-12-07 13:22:01.632318: step 13190, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 60h:50m:00s remains)
INFO - root - 2017-12-07 13:22:08.499582: step 13200, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 63h:50m:13s remains)
2017-12-07 13:22:09.187414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2648549 -4.2767415 -4.2857876 -4.2887774 -4.2913 -4.2954655 -4.2983737 -4.2988553 -4.3001833 -4.3039846 -4.3080683 -4.3110518 -4.3123865 -4.3118873 -4.3089023][-4.2062254 -4.2288508 -4.2475042 -4.2589889 -4.2705145 -4.2813578 -4.2888565 -4.292594 -4.2967124 -4.302063 -4.3053179 -4.3066931 -4.3062029 -4.3050017 -4.3018045][-4.1392164 -4.1745448 -4.2081757 -4.233727 -4.2558589 -4.2697568 -4.2763786 -4.281518 -4.2881489 -4.2949777 -4.2976556 -4.2982097 -4.2983708 -4.2976494 -4.2941389][-4.1020417 -4.1447778 -4.1879697 -4.2210979 -4.2436452 -4.2494545 -4.2469869 -4.2507248 -4.2601972 -4.2709675 -4.2776771 -4.2812848 -4.2850614 -4.2863116 -4.2838902][-4.1335015 -4.1682787 -4.201261 -4.2204084 -4.2223978 -4.207828 -4.1881461 -4.1872954 -4.2009587 -4.217711 -4.2306938 -4.24419 -4.2586365 -4.2659316 -4.26675][-4.1941805 -4.2078714 -4.2182326 -4.2108788 -4.1835523 -4.1440659 -4.1063304 -4.09751 -4.1146016 -4.1381497 -4.1609297 -4.1900015 -4.22045 -4.2393341 -4.2480087][-4.2217207 -4.2152705 -4.2037129 -4.1750493 -4.130578 -4.0753727 -4.0223575 -4.0094948 -4.0334177 -4.0685248 -4.1031952 -4.1465611 -4.1918654 -4.2235894 -4.2409782][-4.2242837 -4.203558 -4.1776571 -4.1469822 -4.1071043 -4.0587511 -4.0085931 -4.0024953 -4.0298338 -4.065701 -4.0989232 -4.1405125 -4.1868682 -4.2229586 -4.2463903][-4.1960478 -4.1706123 -4.1490154 -4.1347904 -4.1212897 -4.1047664 -4.0809665 -4.0835195 -4.1060548 -4.1303892 -4.1508307 -4.1769624 -4.2091107 -4.2376819 -4.2573342][-4.119689 -4.0998926 -4.0928578 -4.1037779 -4.1278834 -4.1516886 -4.1596727 -4.1721587 -4.1852388 -4.1936669 -4.2005439 -4.2115026 -4.2267857 -4.2447038 -4.2520761][-4.0455704 -4.0394678 -4.0535746 -4.0847864 -4.1379218 -4.1903348 -4.2149258 -4.2194667 -4.2153211 -4.2119308 -4.2074862 -4.2078609 -4.2119722 -4.220787 -4.2171354][-4.0557752 -4.0612874 -4.0829868 -4.1209574 -4.1781874 -4.2279243 -4.2406917 -4.2265682 -4.2052402 -4.192193 -4.180377 -4.1727934 -4.1664228 -4.1649661 -4.1547956][-4.1226187 -4.1297417 -4.1430478 -4.1702886 -4.2064428 -4.2299447 -4.219593 -4.1855974 -4.147151 -4.1262474 -4.12019 -4.1186533 -4.1103654 -4.1051674 -4.0954676][-4.185905 -4.1829991 -4.1817718 -4.1911669 -4.2025166 -4.1986108 -4.171001 -4.1258373 -4.0769138 -4.0634713 -4.0716052 -4.0829673 -4.0802169 -4.0853014 -4.0919085][-4.2338161 -4.2255487 -4.2141824 -4.2060652 -4.2018404 -4.1876621 -4.1560059 -4.1131234 -4.0750737 -4.07697 -4.0942554 -4.108253 -4.1071253 -4.1190839 -4.1342974]]...]
INFO - root - 2017-12-07 13:22:15.897503: step 13210, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 55h:40m:24s remains)
INFO - root - 2017-12-07 13:22:22.708812: step 13220, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 64h:07m:37s remains)
INFO - root - 2017-12-07 13:22:29.640072: step 13230, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 63h:51m:01s remains)
INFO - root - 2017-12-07 13:22:36.357771: step 13240, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 57h:30m:32s remains)
INFO - root - 2017-12-07 13:22:43.122582: step 13250, loss = 2.10, batch loss = 2.04 (13.2 examples/sec; 0.607 sec/batch; 53h:50m:01s remains)
INFO - root - 2017-12-07 13:22:49.791730: step 13260, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 65h:04m:43s remains)
INFO - root - 2017-12-07 13:22:56.514330: step 13270, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 62h:12m:14s remains)
INFO - root - 2017-12-07 13:23:03.322375: step 13280, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 61h:03m:02s remains)
INFO - root - 2017-12-07 13:23:09.903550: step 13290, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 58h:54m:28s remains)
INFO - root - 2017-12-07 13:23:16.680148: step 13300, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 56h:06m:16s remains)
2017-12-07 13:23:17.401408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2615728 -4.2612877 -4.265893 -4.283524 -4.3031077 -4.3132696 -4.3166785 -4.3095617 -4.2894793 -4.2517691 -4.2075591 -4.1695433 -4.1410446 -4.1297126 -4.1404057][-4.2836151 -4.2871623 -4.2913909 -4.3030849 -4.3136148 -4.3122277 -4.3061376 -4.2929745 -4.2742853 -4.2374444 -4.1939926 -4.1476192 -4.1019611 -4.0814791 -4.0879283][-4.2908196 -4.2944694 -4.29952 -4.308671 -4.3123417 -4.303123 -4.2911067 -4.2743125 -4.2525978 -4.2138643 -4.1680212 -4.1031718 -4.0416079 -4.024488 -4.0434608][-4.2947426 -4.299685 -4.3043981 -4.3083482 -4.3033605 -4.2818084 -4.2617836 -4.2429886 -4.2242637 -4.1883659 -4.1383095 -4.0634642 -3.99942 -3.992902 -4.0288615][-4.2881231 -4.2937365 -4.2943325 -4.2912641 -4.2777591 -4.2425404 -4.2072759 -4.1865029 -4.1746573 -4.1548991 -4.1117382 -4.0492377 -4.0014658 -4.0088191 -4.0465307][-4.2665176 -4.2693849 -4.2595468 -4.2432594 -4.2184381 -4.1703439 -4.1157575 -4.0793176 -4.07143 -4.0754185 -4.0615978 -4.0431824 -4.03459 -4.0543733 -4.0907393][-4.2367411 -4.2259793 -4.1986694 -4.165246 -4.1227541 -4.0576239 -3.9760139 -3.9000378 -3.8871298 -3.9343486 -3.9770026 -4.0152812 -4.0511088 -4.0887775 -4.1306214][-4.1983113 -4.1689248 -4.1203475 -4.0676441 -4.0065145 -3.9209731 -3.8096023 -3.6836829 -3.655699 -3.7668414 -3.8766923 -3.9672849 -4.045001 -4.1063809 -4.1540413][-4.1882477 -4.1504555 -4.0954742 -4.0389304 -3.977098 -3.8900418 -3.7767181 -3.6469204 -3.617183 -3.7428203 -3.8678188 -3.9647725 -4.0526371 -4.1282225 -4.181561][-4.2079325 -4.1746216 -4.1297574 -4.090826 -4.0484862 -3.9845405 -3.9101305 -3.8447073 -3.8339748 -3.8997309 -3.9766257 -4.041007 -4.1063662 -4.173594 -4.221972][-4.2383089 -4.215497 -4.18808 -4.1711259 -4.1506252 -4.1134429 -4.0799408 -4.0693989 -4.0715232 -4.0882134 -4.1179886 -4.1554737 -4.1973376 -4.2423763 -4.2748475][-4.2781534 -4.27018 -4.2630048 -4.2614636 -4.255024 -4.2370143 -4.2235289 -4.2285576 -4.233458 -4.2305512 -4.2393541 -4.2626886 -4.28637 -4.3074408 -4.3233709][-4.3136821 -4.3163166 -4.3202076 -4.3247147 -4.3234 -4.3154078 -4.3083582 -4.3145289 -4.3180056 -4.3136125 -4.3142238 -4.3264976 -4.3387837 -4.3466253 -4.3517485][-4.333982 -4.3383174 -4.3435092 -4.3499818 -4.35093 -4.3455191 -4.341249 -4.3442659 -4.3473344 -4.3472991 -4.3467255 -4.3495641 -4.353281 -4.3563643 -4.357811][-4.337204 -4.3399005 -4.3425622 -4.3465123 -4.347322 -4.3456693 -4.3447657 -4.3445187 -4.3448882 -4.346199 -4.3462787 -4.3453012 -4.3460283 -4.3483887 -4.3499136]]...]
INFO - root - 2017-12-07 13:23:24.175909: step 13310, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 61h:05m:28s remains)
INFO - root - 2017-12-07 13:23:30.840803: step 13320, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 57h:55m:25s remains)
INFO - root - 2017-12-07 13:23:37.586772: step 13330, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 56h:39m:31s remains)
INFO - root - 2017-12-07 13:23:44.486519: step 13340, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 63h:36m:54s remains)
INFO - root - 2017-12-07 13:23:51.358269: step 13350, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 64h:54m:38s remains)
INFO - root - 2017-12-07 13:23:58.228736: step 13360, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 58h:17m:15s remains)
INFO - root - 2017-12-07 13:24:04.972300: step 13370, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 56h:37m:52s remains)
INFO - root - 2017-12-07 13:24:11.724006: step 13380, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 58h:41m:48s remains)
INFO - root - 2017-12-07 13:24:18.455007: step 13390, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.744 sec/batch; 65h:56m:34s remains)
INFO - root - 2017-12-07 13:24:25.217793: step 13400, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.693 sec/batch; 61h:23m:08s remains)
2017-12-07 13:24:25.923831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3548555 -4.3542976 -4.3378043 -4.3070145 -4.2644792 -4.2143168 -4.1720076 -4.1306634 -4.1053419 -4.1071315 -4.1328621 -4.1727257 -4.219101 -4.2767262 -4.3193488][-4.3551888 -4.3503041 -4.327 -4.2871647 -4.2360754 -4.1845274 -4.1453867 -4.107831 -4.0949984 -4.1210766 -4.1606407 -4.2015557 -4.2466431 -4.2985945 -4.3294973][-4.3541312 -4.3450155 -4.3170762 -4.2690926 -4.20973 -4.1577191 -4.1236677 -4.0934854 -4.0926471 -4.1350732 -4.1819735 -4.2258167 -4.2715316 -4.3198557 -4.341507][-4.3515954 -4.3403649 -4.308424 -4.2513323 -4.1830773 -4.127533 -4.0914946 -4.0673323 -4.08577 -4.1436114 -4.1958094 -4.2429914 -4.2929664 -4.3355813 -4.3500843][-4.3495679 -4.3378797 -4.3009357 -4.2324605 -4.1506886 -4.0829372 -4.03254 -4.0097957 -4.0557928 -4.13418 -4.1916289 -4.2484078 -4.3026385 -4.338851 -4.35198][-4.3460259 -4.3318906 -4.2860541 -4.20248 -4.0999737 -4.0092254 -3.9412494 -3.9318457 -4.0151134 -4.1148047 -4.1798959 -4.243988 -4.2988157 -4.3328385 -4.3497562][-4.3425441 -4.3247542 -4.2701778 -4.174284 -4.0552616 -3.9408698 -3.8640096 -3.8800435 -3.9996781 -4.1125035 -4.1814342 -4.2442565 -4.2954192 -4.3291149 -4.3492675][-4.3444662 -4.3271923 -4.2694283 -4.1715822 -4.0452576 -3.9176941 -3.8439765 -3.8802512 -4.01437 -4.1255431 -4.1936393 -4.2516046 -4.2989583 -4.3324518 -4.3526654][-4.3482084 -4.3334665 -4.2821541 -4.1903491 -4.0658817 -3.9389923 -3.8756993 -3.922863 -4.0451031 -4.1471777 -4.2125506 -4.2665334 -4.3091297 -4.339118 -4.35804][-4.3514876 -4.3416214 -4.3017106 -4.2213049 -4.1073837 -3.9924331 -3.9438672 -3.9934947 -4.0940428 -4.1820016 -4.2454567 -4.2954769 -4.3309264 -4.3534493 -4.3656912][-4.3560667 -4.3503428 -4.3216619 -4.2571177 -4.1614347 -4.0684781 -4.0378251 -4.0821972 -4.1556888 -4.2260556 -4.2833376 -4.3277044 -4.3562117 -4.3696136 -4.3738284][-4.3573451 -4.3517261 -4.3286853 -4.279048 -4.2043223 -4.1377358 -4.125649 -4.1670814 -4.2224994 -4.2766442 -4.3208756 -4.3554997 -4.3743262 -4.3795667 -4.3771272][-4.3528204 -4.3453259 -4.3242378 -4.2839718 -4.2287893 -4.186779 -4.19073 -4.2316513 -4.2759509 -4.3159194 -4.3478203 -4.3719473 -4.382761 -4.3813014 -4.3741126][-4.3497648 -4.34397 -4.3272419 -4.2951803 -4.2551918 -4.2314529 -4.2426686 -4.2780824 -4.3123403 -4.3399448 -4.3617315 -4.3763638 -4.3796916 -4.3750138 -4.3677797][-4.3496652 -4.3467884 -4.3363051 -4.3149781 -4.2882485 -4.2730532 -4.2835031 -4.3089252 -4.3342886 -4.35293 -4.3664255 -4.3734336 -4.3730936 -4.3686442 -4.3632774]]...]
INFO - root - 2017-12-07 13:24:32.815531: step 13410, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 63h:18m:10s remains)
INFO - root - 2017-12-07 13:24:39.683013: step 13420, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.747 sec/batch; 66h:12m:58s remains)
INFO - root - 2017-12-07 13:24:46.467867: step 13430, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 62h:37m:58s remains)
INFO - root - 2017-12-07 13:24:53.226727: step 13440, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 55h:39m:52s remains)
INFO - root - 2017-12-07 13:25:00.004765: step 13450, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 58h:25m:45s remains)
INFO - root - 2017-12-07 13:25:06.939493: step 13460, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 64h:32m:32s remains)
INFO - root - 2017-12-07 13:25:13.817810: step 13470, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 63h:15m:39s remains)
INFO - root - 2017-12-07 13:25:20.587804: step 13480, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 56h:25m:07s remains)
INFO - root - 2017-12-07 13:25:27.194196: step 13490, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 56h:00m:57s remains)
INFO - root - 2017-12-07 13:25:34.094804: step 13500, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.667 sec/batch; 59h:05m:51s remains)
2017-12-07 13:25:34.851275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2809744 -4.2857723 -4.2888994 -4.2890716 -4.289535 -4.2881241 -4.2836733 -4.2730641 -4.2553411 -4.2392268 -4.2264905 -4.2168813 -4.2158465 -4.2214189 -4.2242827][-4.2726312 -4.2746983 -4.278543 -4.2771482 -4.2761831 -4.273551 -4.2669792 -4.251193 -4.2267866 -4.2096443 -4.2008996 -4.1953244 -4.2000985 -4.2126794 -4.2181487][-4.2654362 -4.2646351 -4.2661996 -4.2600627 -4.254283 -4.2470956 -4.2361994 -4.2162685 -4.1897783 -4.1773744 -4.179698 -4.1840725 -4.1967835 -4.2144547 -4.2205162][-4.2600503 -4.2554331 -4.2504025 -4.2356744 -4.2200294 -4.2041216 -4.18673 -4.1648211 -4.1403818 -4.1369095 -4.1537719 -4.173265 -4.1972437 -4.2186875 -4.2235279][-4.2614264 -4.2538552 -4.2402134 -4.2140179 -4.1848807 -4.1566653 -4.1292305 -4.1015649 -4.0809755 -4.0913458 -4.1252947 -4.1599779 -4.1952209 -4.221776 -4.2260323][-4.2511115 -4.2372608 -4.2129884 -4.1754541 -4.1348209 -4.0918741 -4.0491438 -4.0116205 -4.0012164 -4.0344558 -4.0879893 -4.1370244 -4.1835036 -4.218173 -4.22643][-4.2356124 -4.217279 -4.1849656 -4.1414547 -4.0915666 -4.0335908 -3.9709373 -3.9217534 -3.9269309 -3.9885595 -4.0632806 -4.1245742 -4.1799636 -4.2217579 -4.2333479][-4.2380452 -4.2192774 -4.1849151 -4.1400976 -4.0863905 -4.0228405 -3.9512544 -3.8980539 -3.9131396 -3.9890893 -4.0725684 -4.139432 -4.19631 -4.2372265 -4.2477212][-4.2442946 -4.2268696 -4.1947479 -4.1518984 -4.103456 -4.0523582 -3.9967475 -3.9562733 -3.9674089 -4.0295687 -4.1040177 -4.1658683 -4.215035 -4.2493744 -4.2563071][-4.237525 -4.2208962 -4.190465 -4.1504569 -4.1113157 -4.0789909 -4.0470924 -4.0239682 -4.03214 -4.0772991 -4.1357174 -4.18705 -4.2275419 -4.2551222 -4.2593384][-4.232326 -4.2169452 -4.1883025 -4.1501818 -4.1176596 -4.1002331 -4.0851712 -4.074192 -4.0833163 -4.119092 -4.1641836 -4.2040691 -4.2363453 -4.2597289 -4.2626963][-4.2264156 -4.2140608 -4.190125 -4.1548667 -4.128767 -4.1216121 -4.114295 -4.1076264 -4.1171122 -4.14886 -4.1837692 -4.2123976 -4.2360559 -4.2553949 -4.2578583][-4.2050667 -4.1939359 -4.1761084 -4.1481705 -4.1286874 -4.1255364 -4.1202083 -4.11582 -4.1254148 -4.1549482 -4.185503 -4.209311 -4.2279048 -4.2438703 -4.2471046][-4.177485 -4.166441 -4.1518679 -4.1312852 -4.1172576 -4.1145411 -4.1121721 -4.1120853 -4.1218004 -4.1479673 -4.1751876 -4.1966438 -4.2132964 -4.2278924 -4.2344947][-4.1577597 -4.1484823 -4.1351819 -4.1207147 -4.1123934 -4.1115689 -4.113904 -4.1184392 -4.1274457 -4.1480875 -4.1698442 -4.1873302 -4.20209 -4.2163024 -4.2258596]]...]
INFO - root - 2017-12-07 13:25:41.597111: step 13510, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 57h:40m:57s remains)
INFO - root - 2017-12-07 13:25:48.367190: step 13520, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 59h:34m:55s remains)
INFO - root - 2017-12-07 13:25:55.129245: step 13530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 59h:01m:41s remains)
INFO - root - 2017-12-07 13:26:01.971787: step 13540, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 66h:00m:30s remains)
INFO - root - 2017-12-07 13:26:08.754345: step 13550, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 60h:32m:23s remains)
INFO - root - 2017-12-07 13:26:15.583619: step 13560, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 57h:14m:34s remains)
INFO - root - 2017-12-07 13:26:22.254856: step 13570, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.713 sec/batch; 63h:12m:00s remains)
INFO - root - 2017-12-07 13:26:29.082450: step 13580, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 62h:58m:45s remains)
INFO - root - 2017-12-07 13:26:35.658774: step 13590, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 58h:10m:36s remains)
INFO - root - 2017-12-07 13:26:42.400150: step 13600, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 56h:19m:28s remains)
2017-12-07 13:26:43.136612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2469764 -4.2409892 -4.2183924 -4.1808877 -4.1528583 -4.1350965 -4.1245589 -4.12373 -4.10106 -4.0641375 -4.0554757 -4.0605807 -4.0728612 -4.0926242 -4.1098113][-4.2538867 -4.247601 -4.2237296 -4.1855421 -4.1526089 -4.1316128 -4.1150627 -4.10761 -4.0927305 -4.0766912 -4.0761909 -4.0685797 -4.0471144 -4.0406938 -4.0615845][-4.2512341 -4.2441339 -4.2218857 -4.1848598 -4.1438451 -4.1157184 -4.0881305 -4.0723205 -4.0691566 -4.0809913 -4.0916338 -4.0784631 -4.031878 -4.0042896 -4.0268812][-4.2419066 -4.2337856 -4.2114878 -4.1739597 -4.1276388 -4.0898113 -4.0504289 -4.0298018 -4.0454555 -4.0841808 -4.1072745 -4.089335 -4.0328293 -3.9957576 -4.0168161][-4.2383814 -4.2302823 -4.2063532 -4.1710868 -4.1205935 -4.0674634 -4.0101666 -3.985889 -4.024416 -4.08224 -4.1151204 -4.1041422 -4.0490565 -4.0028772 -4.0099106][-4.2348094 -4.2281814 -4.2033639 -4.1707406 -4.1143675 -4.0338717 -3.9401207 -3.9123554 -3.9853005 -4.0686297 -4.1183763 -4.123786 -4.081584 -4.0325933 -4.026701][-4.2246151 -4.2184734 -4.1962562 -4.1627893 -4.0986137 -3.9873402 -3.8432646 -3.8047025 -3.9282231 -4.050797 -4.1194639 -4.1412277 -4.1204348 -4.0821342 -4.0722466][-4.2184768 -4.2121181 -4.1908188 -4.1517 -4.0842009 -3.9593837 -3.7843771 -3.7397261 -3.8901482 -4.0342813 -4.1143136 -4.1500344 -4.1552391 -4.1343927 -4.1204381][-4.2136703 -4.203229 -4.1796069 -4.1382432 -4.0800467 -3.9801118 -3.8527114 -3.8319407 -3.9329221 -4.0417428 -4.1145597 -4.1578746 -4.182188 -4.1826315 -4.1679287][-4.1982937 -4.1823039 -4.1580868 -4.124506 -4.0846887 -4.02373 -3.9571762 -3.9549747 -4.0026126 -4.062417 -4.118566 -4.1619482 -4.197197 -4.2137809 -4.2034731][-4.1808672 -4.161088 -4.138833 -4.1182408 -4.0973482 -4.0659904 -4.0353541 -4.0354977 -4.0541296 -4.0871334 -4.1243124 -4.1599407 -4.1943769 -4.2190447 -4.2110238][-4.1657524 -4.15457 -4.1350446 -4.1213555 -4.11279 -4.1021667 -4.0903473 -4.0865989 -4.09158 -4.1088285 -4.1294103 -4.1528373 -4.1780529 -4.2030759 -4.1978679][-4.1485586 -4.149097 -4.1321716 -4.1195416 -4.119946 -4.121963 -4.1227965 -4.1186972 -4.1165519 -4.1277442 -4.1376958 -4.1469011 -4.1569409 -4.1734695 -4.1694822][-4.12315 -4.132236 -4.1218524 -4.111618 -4.1167545 -4.1272683 -4.1332583 -4.1349692 -4.1339941 -4.1442933 -4.1479864 -4.143537 -4.1409211 -4.1477375 -4.1442833][-4.1104708 -4.1240535 -4.1177068 -4.102663 -4.1017218 -4.1159525 -4.1221604 -4.1261139 -4.1320214 -4.1485682 -4.1562252 -4.1490903 -4.1420393 -4.1451244 -4.1429014]]...]
INFO - root - 2017-12-07 13:26:50.016729: step 13610, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 64h:02m:48s remains)
INFO - root - 2017-12-07 13:26:56.906768: step 13620, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 60h:20m:29s remains)
INFO - root - 2017-12-07 13:27:03.638713: step 13630, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 55h:47m:39s remains)
INFO - root - 2017-12-07 13:27:10.473264: step 13640, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 58h:19m:35s remains)
INFO - root - 2017-12-07 13:27:17.273866: step 13650, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 64h:05m:40s remains)
INFO - root - 2017-12-07 13:27:23.984651: step 13660, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 64h:57m:12s remains)
INFO - root - 2017-12-07 13:27:30.833789: step 13670, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 61h:02m:48s remains)
INFO - root - 2017-12-07 13:27:37.685031: step 13680, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 57h:42m:15s remains)
INFO - root - 2017-12-07 13:27:44.301308: step 13690, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 57h:21m:45s remains)
INFO - root - 2017-12-07 13:27:51.159155: step 13700, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 64h:29m:43s remains)
2017-12-07 13:27:51.896543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1422486 -4.14295 -4.1570435 -4.1842384 -4.1998658 -4.2038541 -4.2102041 -4.2139664 -4.2023621 -4.1981349 -4.2034931 -4.1982579 -4.1886153 -4.1794086 -4.1818051][-4.1239824 -4.1310015 -4.1520138 -4.1853943 -4.2027712 -4.2065315 -4.2124796 -4.2147279 -4.2000976 -4.1856923 -4.1796646 -4.1772795 -4.1793857 -4.1790061 -4.1880093][-4.1131988 -4.1327281 -4.1606874 -4.1964817 -4.2119994 -4.2133241 -4.2149258 -4.2152071 -4.1984992 -4.1740732 -4.1519814 -4.1478009 -4.157959 -4.1617794 -4.1762156][-4.1155767 -4.1482329 -4.183867 -4.2137723 -4.2212129 -4.2141914 -4.2036633 -4.1957393 -4.1801538 -4.1569786 -4.1322174 -4.1305795 -4.1437182 -4.1550903 -4.1719851][-4.1291518 -4.1626315 -4.1977935 -4.218441 -4.2171588 -4.2020879 -4.1777644 -4.1530118 -4.1391454 -4.1259379 -4.105979 -4.1084766 -4.1248345 -4.1447082 -4.1651406][-4.155725 -4.1808391 -4.2100053 -4.222209 -4.2062421 -4.1731105 -4.1248889 -4.0832982 -4.0788436 -4.0802956 -4.072257 -4.0865808 -4.1075263 -4.1356649 -4.1630735][-4.182375 -4.1967611 -4.2147193 -4.2155886 -4.180829 -4.1180954 -4.0305748 -3.9731274 -4.0010443 -4.0388103 -4.0533838 -4.0775876 -4.0959296 -4.1233521 -4.1532087][-4.1811709 -4.1863904 -4.2000942 -4.1944504 -4.145009 -4.057529 -3.9407637 -3.8875885 -3.9641342 -4.0438113 -4.0773454 -4.0972404 -4.1026864 -4.1200867 -4.1437325][-4.1761727 -4.1734376 -4.1830964 -4.1785727 -4.1351118 -4.0667839 -3.9803185 -3.9541223 -4.03364 -4.1108379 -4.1420135 -4.1501455 -4.1400671 -4.1438346 -4.1550479][-4.1888375 -4.184587 -4.1929169 -4.1939373 -4.1690574 -4.1338768 -4.0895562 -4.0806212 -4.1308155 -4.1803355 -4.1979432 -4.1961484 -4.1784716 -4.1741996 -4.1787791][-4.2048335 -4.2050209 -4.2140684 -4.2165694 -4.2053885 -4.1887312 -4.1634135 -4.1566787 -4.18032 -4.202538 -4.2083712 -4.2043085 -4.185626 -4.177484 -4.1821513][-4.2187452 -4.2200894 -4.2226257 -4.2204614 -4.2139249 -4.1994858 -4.1769667 -4.1713352 -4.1819677 -4.1887903 -4.1956706 -4.1972928 -4.1810403 -4.1708765 -4.1739221][-4.2292309 -4.2243938 -4.2163754 -4.20684 -4.1977482 -4.1842074 -4.1655912 -4.1624918 -4.1682129 -4.1763124 -4.1943874 -4.2055454 -4.1965513 -4.1860347 -4.1866317][-4.2304788 -4.2173157 -4.2013869 -4.1878266 -4.1804338 -4.1723857 -4.1632824 -4.1624527 -4.1690912 -4.1809454 -4.2027106 -4.21974 -4.2213058 -4.2163854 -4.2132473][-4.216228 -4.198915 -4.1863427 -4.17925 -4.1789474 -4.1793556 -4.17596 -4.1760855 -4.1818295 -4.1883979 -4.2032123 -4.2164354 -4.2228961 -4.2221241 -4.2172465]]...]
INFO - root - 2017-12-07 13:27:58.624810: step 13710, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 56h:14m:30s remains)
INFO - root - 2017-12-07 13:28:05.475589: step 13720, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 56h:20m:07s remains)
INFO - root - 2017-12-07 13:28:12.407663: step 13730, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 63h:05m:34s remains)
INFO - root - 2017-12-07 13:28:19.192430: step 13740, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 61h:52m:51s remains)
INFO - root - 2017-12-07 13:28:25.925261: step 13750, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 61h:47m:36s remains)
INFO - root - 2017-12-07 13:28:32.641868: step 13760, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 56h:26m:56s remains)
INFO - root - 2017-12-07 13:28:39.559888: step 13770, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 58h:06m:05s remains)
INFO - root - 2017-12-07 13:28:46.398012: step 13780, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.748 sec/batch; 66h:13m:34s remains)
INFO - root - 2017-12-07 13:28:53.027324: step 13790, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 60h:50m:42s remains)
INFO - root - 2017-12-07 13:28:59.796651: step 13800, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 59h:27m:13s remains)
2017-12-07 13:29:00.501164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2740316 -4.2753053 -4.2733521 -4.2653747 -4.2615089 -4.2530718 -4.2320046 -4.2181673 -4.2153034 -4.2239194 -4.231883 -4.2421536 -4.2562814 -4.2752318 -4.2952967][-4.2338395 -4.2318292 -4.2301383 -4.2227588 -4.2200284 -4.2109351 -4.1836681 -4.163239 -4.1593733 -4.1746774 -4.1874909 -4.2029381 -4.2230587 -4.2472687 -4.2712507][-4.190484 -4.1803489 -4.1785159 -4.1745095 -4.1736093 -4.1650391 -4.1365457 -4.1079049 -4.0992012 -4.1189642 -4.1379437 -4.163147 -4.1942277 -4.2275453 -4.2548394][-4.1480765 -4.1296329 -4.12831 -4.1251583 -4.1262431 -4.1252637 -4.1011705 -4.0618749 -4.0403371 -4.0668459 -4.0964365 -4.131424 -4.1738091 -4.217319 -4.2493181][-4.1004286 -4.0703282 -4.06914 -4.0662208 -4.0677266 -4.07582 -4.05943 -4.00828 -3.9755282 -4.018301 -4.066195 -4.1103864 -4.1590767 -4.2113624 -4.2488971][-4.0649395 -4.0262008 -4.0177245 -4.0150805 -4.0145221 -4.0256834 -4.0060148 -3.9358008 -3.8957238 -3.9693327 -4.048111 -4.1009831 -4.1491737 -4.2052789 -4.2488079][-4.0452304 -4.0145259 -3.9993026 -3.9888892 -3.9778733 -3.978442 -3.9460664 -3.8524816 -3.7983313 -3.9061308 -4.0193882 -4.0836029 -4.1351132 -4.1947308 -4.2451973][-4.0458665 -4.0325561 -4.020215 -4.0078845 -3.9839995 -3.9675684 -3.9298961 -3.8342166 -3.7681494 -3.8702326 -3.9903948 -4.0554571 -4.1109314 -4.1775537 -4.2366371][-4.0679049 -4.0677066 -4.060164 -4.0508771 -4.0266466 -4.0047131 -3.9811933 -3.9091058 -3.8406861 -3.9018064 -3.9928133 -4.0394459 -4.0900421 -4.161478 -4.2260041][-4.0927649 -4.1012692 -4.1057792 -4.1091409 -4.0891953 -4.0647516 -4.0450711 -3.9837248 -3.9148402 -3.943557 -4.0075464 -4.0415497 -4.0870986 -4.156991 -4.2232852][-4.1224155 -4.1316538 -4.1435175 -4.1591611 -4.1477861 -4.1219797 -4.10244 -4.0512938 -3.9896572 -3.9995227 -4.0436611 -4.0730567 -4.1137133 -4.1745238 -4.2335544][-4.1571975 -4.1640415 -4.1739507 -4.1904831 -4.1850424 -4.162405 -4.1426086 -4.10131 -4.0539665 -4.059938 -4.0927482 -4.1194372 -4.15348 -4.2029042 -4.2509708][-4.1926823 -4.1986022 -4.2017851 -4.2123117 -4.2101345 -4.190793 -4.1714835 -4.1387086 -4.1061053 -4.1124663 -4.1410022 -4.1666765 -4.1945744 -4.2341628 -4.2713861][-4.22176 -4.2272091 -4.2255535 -4.2251253 -4.2208934 -4.2056618 -4.1892753 -4.1636825 -4.1433759 -4.1531448 -4.18263 -4.2096844 -4.233582 -4.264852 -4.290741][-4.2473478 -4.2552953 -4.2567616 -4.2532973 -4.2463231 -4.234004 -4.2195077 -4.2008772 -4.1875439 -4.1977777 -4.2246671 -4.2502217 -4.2706432 -4.2920709 -4.3079305]]...]
INFO - root - 2017-12-07 13:29:07.336341: step 13810, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 62h:58m:40s remains)
INFO - root - 2017-12-07 13:29:14.166948: step 13820, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 59h:21m:40s remains)
INFO - root - 2017-12-07 13:29:20.935315: step 13830, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 62h:28m:39s remains)
INFO - root - 2017-12-07 13:29:27.806160: step 13840, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 56h:25m:17s remains)
INFO - root - 2017-12-07 13:29:34.570378: step 13850, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 58h:22m:07s remains)
INFO - root - 2017-12-07 13:29:41.385125: step 13860, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 64h:06m:44s remains)
INFO - root - 2017-12-07 13:29:48.151110: step 13870, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 57h:56m:34s remains)
INFO - root - 2017-12-07 13:29:54.737946: step 13880, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 57h:00m:03s remains)
INFO - root - 2017-12-07 13:30:01.444811: step 13890, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 59h:41m:10s remains)
INFO - root - 2017-12-07 13:30:08.198398: step 13900, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 63h:16m:42s remains)
2017-12-07 13:30:08.964263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2787294 -4.2847934 -4.2863126 -4.2872367 -4.2882838 -4.2916131 -4.2989092 -4.3035417 -4.3032441 -4.3003254 -4.29413 -4.2917891 -4.3002896 -4.30769 -4.3089004][-4.264431 -4.2664666 -4.2674928 -4.2668824 -4.2629166 -4.2622437 -4.2667623 -4.2697229 -4.2700491 -4.2698092 -4.2709651 -4.2765656 -4.288538 -4.2967253 -4.2981291][-4.2568707 -4.2561746 -4.2533288 -4.2463374 -4.2342 -4.231112 -4.2350769 -4.2427669 -4.2493124 -4.2541046 -4.2621064 -4.2716784 -4.2839317 -4.2894673 -4.2891288][-4.2604165 -4.2573504 -4.2447553 -4.2264824 -4.2024145 -4.1948309 -4.2011695 -4.2190542 -4.2372484 -4.250205 -4.2638984 -4.2750492 -4.2844148 -4.2877359 -4.28718][-4.2647276 -4.2572412 -4.2328615 -4.1964531 -4.1545606 -4.1323442 -4.1332326 -4.1566052 -4.1876416 -4.2148275 -4.2433395 -4.2647271 -4.2782693 -4.2840309 -4.2870617][-4.2719717 -4.260355 -4.223577 -4.1656041 -4.1002216 -4.0521483 -4.0339222 -4.0488605 -4.08674 -4.1359997 -4.1916795 -4.2357793 -4.2587347 -4.2708473 -4.2783327][-4.2785621 -4.2663136 -4.2276192 -4.1625371 -4.0837007 -4.011785 -3.9653239 -3.9573097 -3.9905629 -4.055634 -4.1350975 -4.1988668 -4.2327242 -4.2517781 -4.2618847][-4.2800856 -4.2739291 -4.2450552 -4.1931281 -4.1242747 -4.0516319 -3.9912829 -3.9597838 -3.9745419 -4.0327196 -4.1101165 -4.1761723 -4.2132373 -4.2327852 -4.24405][-4.2725477 -4.2744246 -4.2593579 -4.2283921 -4.1853838 -4.1372571 -4.0902095 -4.0529871 -4.0465 -4.0802441 -4.1339579 -4.1836352 -4.2116532 -4.2226067 -4.2305017][-4.2593865 -4.2685494 -4.2645297 -4.2509146 -4.2320251 -4.2095942 -4.1829834 -4.1580458 -4.1457448 -4.1602993 -4.1878929 -4.2161393 -4.2306547 -4.2329597 -4.2375703][-4.2440572 -4.2548928 -4.2536316 -4.2476044 -4.240468 -4.2352247 -4.2254376 -4.2157125 -4.2118554 -4.2222357 -4.2362151 -4.24901 -4.2527866 -4.2491751 -4.2481122][-4.2299304 -4.2359967 -4.2295327 -4.2206106 -4.2167649 -4.2218876 -4.2251425 -4.226965 -4.2317529 -4.2441988 -4.2521672 -4.256598 -4.255775 -4.2491217 -4.24338][-4.2271204 -4.2288642 -4.2167072 -4.2022638 -4.1955504 -4.2025881 -4.2118106 -4.2174954 -4.2251315 -4.237196 -4.2423396 -4.2429276 -4.240921 -4.233542 -4.2260089][-4.2438149 -4.2482533 -4.2375298 -4.2191596 -4.2057576 -4.2077303 -4.2155252 -4.2198792 -4.2273431 -4.2372069 -4.2408156 -4.2405534 -4.2385192 -4.2323 -4.2251577][-4.2767282 -4.2893043 -4.2851219 -4.2696481 -4.2539315 -4.2487888 -4.2499862 -4.2513642 -4.2568398 -4.2638497 -4.2668266 -4.2675052 -4.2662206 -4.2616129 -4.2567515]]...]
INFO - root - 2017-12-07 13:30:15.724640: step 13910, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 55h:13m:19s remains)
INFO - root - 2017-12-07 13:30:22.456768: step 13920, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 59h:18m:58s remains)
INFO - root - 2017-12-07 13:30:29.338200: step 13930, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 64h:48m:54s remains)
INFO - root - 2017-12-07 13:30:36.047557: step 13940, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 59h:10m:55s remains)
INFO - root - 2017-12-07 13:30:42.855990: step 13950, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 55h:42m:38s remains)
INFO - root - 2017-12-07 13:30:49.621070: step 13960, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 56h:04m:00s remains)
INFO - root - 2017-12-07 13:30:56.425313: step 13970, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 61h:51m:11s remains)
INFO - root - 2017-12-07 13:31:03.149101: step 13980, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 61h:37m:07s remains)
INFO - root - 2017-12-07 13:31:09.846218: step 13990, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 59h:08m:01s remains)
INFO - root - 2017-12-07 13:31:16.556428: step 14000, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 57h:29m:21s remains)
2017-12-07 13:31:17.231238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24183 -4.241467 -4.2398386 -4.23985 -4.2395506 -4.2384434 -4.2472653 -4.2621145 -4.2716842 -4.263763 -4.2391586 -4.2079749 -4.1886511 -4.1883879 -4.1956534][-4.2362857 -4.2386808 -4.2383552 -4.2370949 -4.2401624 -4.2456918 -4.2557921 -4.2680769 -4.2677321 -4.2422075 -4.1959686 -4.1440525 -4.116395 -4.1246634 -4.1503544][-4.2226796 -4.2259655 -4.228148 -4.2296748 -4.2390556 -4.2490377 -4.2572365 -4.2589912 -4.240293 -4.1912808 -4.11848 -4.0520124 -4.0346303 -4.06835 -4.1201234][-4.2148814 -4.2196236 -4.2251639 -4.2294712 -4.2405467 -4.2501321 -4.2504234 -4.2334056 -4.1923242 -4.1201086 -4.0358706 -3.9826539 -3.9989913 -4.0621729 -4.1271472][-4.2183189 -4.2266498 -4.2305255 -4.2306566 -4.2368183 -4.2406659 -4.2229261 -4.17674 -4.1074886 -4.0233502 -3.9633183 -3.9640484 -4.0271716 -4.1047039 -4.1628561][-4.2250791 -4.23515 -4.2338758 -4.2257028 -4.2195153 -4.2031069 -4.1548042 -4.0689845 -3.9725392 -3.9000828 -3.9067969 -3.9820642 -4.0730929 -4.141933 -4.179708][-4.2269816 -4.2344847 -4.2254295 -4.199872 -4.170682 -4.1261516 -4.0389261 -3.9125433 -3.8152108 -3.7998958 -3.8858356 -4.0026183 -4.0931783 -4.1472187 -4.1641951][-4.228889 -4.2336068 -4.2194781 -4.1834569 -4.1331224 -4.0671997 -3.9606926 -3.8323889 -3.7855308 -3.8371806 -3.9453974 -4.04381 -4.1087408 -4.1400785 -4.1401186][-4.2464395 -4.2534323 -4.2444391 -4.2090626 -4.1494093 -4.0791035 -3.9870176 -3.8997037 -3.9045174 -3.970607 -4.0504785 -4.1143641 -4.1481457 -4.1591969 -4.1538][-4.2692547 -4.2781606 -4.2723494 -4.2404604 -4.1815939 -4.1193914 -4.0600891 -4.0277853 -4.0643063 -4.1158881 -4.1593862 -4.191906 -4.2012043 -4.2011733 -4.1963773][-4.29403 -4.3026 -4.2992468 -4.2743292 -4.2263045 -4.1814952 -4.1520581 -4.1521831 -4.1929173 -4.2231026 -4.2403326 -4.2510276 -4.2469926 -4.2433114 -4.2397656][-4.3131638 -4.3207307 -4.3194375 -4.3054986 -4.27224 -4.2400379 -4.2253108 -4.2333088 -4.2655048 -4.2817035 -4.2839055 -4.2839928 -4.2791834 -4.27483 -4.2700138][-4.3230953 -4.32707 -4.3285437 -4.3236003 -4.301754 -4.2787805 -4.2711864 -4.2799692 -4.3012404 -4.3097563 -4.3069215 -4.3038106 -4.2983713 -4.2917638 -4.2849154][-4.3196278 -4.3188467 -4.3202448 -4.3170667 -4.3014922 -4.2869177 -4.28288 -4.2882075 -4.3009353 -4.307229 -4.304523 -4.301446 -4.2967 -4.2900939 -4.2833552][-4.3149004 -4.3088765 -4.3060303 -4.3019 -4.2930784 -4.2864385 -4.2859459 -4.2885742 -4.2942643 -4.2982054 -4.2981257 -4.2980738 -4.2960062 -4.2905421 -4.2845678]]...]
INFO - root - 2017-12-07 13:31:24.009303: step 14010, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.746 sec/batch; 66h:01m:37s remains)
INFO - root - 2017-12-07 13:31:30.766924: step 14020, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 59h:01m:38s remains)
INFO - root - 2017-12-07 13:31:37.656019: step 14030, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 56h:17m:31s remains)
INFO - root - 2017-12-07 13:31:44.435510: step 14040, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 56h:41m:39s remains)
INFO - root - 2017-12-07 13:31:51.205519: step 14050, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:01m:03s remains)
INFO - root - 2017-12-07 13:31:58.005849: step 14060, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 63h:09m:47s remains)
INFO - root - 2017-12-07 13:32:04.769816: step 14070, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 60h:59m:32s remains)
INFO - root - 2017-12-07 13:32:11.595692: step 14080, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 57h:18m:13s remains)
INFO - root - 2017-12-07 13:32:18.119978: step 14090, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 54h:32m:38s remains)
INFO - root - 2017-12-07 13:32:24.802247: step 14100, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 58h:24m:04s remains)
2017-12-07 13:32:25.544503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2522488 -4.2494888 -4.2444363 -4.2409706 -4.2427669 -4.2411609 -4.2403026 -4.2360449 -4.225523 -4.2136345 -4.2079616 -4.2077045 -4.2173853 -4.2326622 -4.2514067][-4.2251105 -4.2152457 -4.2049441 -4.2002535 -4.203927 -4.20553 -4.2048163 -4.1990848 -4.1851711 -4.1693144 -4.1625366 -4.1641731 -4.1805215 -4.2047167 -4.2283149][-4.1891122 -4.1690884 -4.1535468 -4.1497746 -4.1597424 -4.16788 -4.169548 -4.1636229 -4.1506767 -4.1369333 -4.1325946 -4.1364527 -4.1548762 -4.1801062 -4.2064266][-4.1443577 -4.118041 -4.1000524 -4.0996318 -4.1196876 -4.1345687 -4.139554 -4.1348915 -4.1252189 -4.1146765 -4.1128969 -4.122932 -4.142302 -4.1664486 -4.1909184][-4.1061325 -4.078979 -4.0599036 -4.0616922 -4.0867095 -4.1054821 -4.112071 -4.1073737 -4.0995045 -4.0946503 -4.1023364 -4.123013 -4.1451678 -4.1692514 -4.1932135][-4.0847363 -4.0553551 -4.0321417 -4.032403 -4.0541768 -4.0694418 -4.0732465 -4.0616198 -4.0447488 -4.0458851 -4.0719461 -4.1104946 -4.1431503 -4.1747937 -4.2033][-4.0544758 -4.0173397 -3.9859385 -3.9758973 -3.9841905 -3.9890268 -3.9862349 -3.9604652 -3.9243143 -3.9327054 -3.9898505 -4.0578847 -4.1097565 -4.1574621 -4.1974411][-4.02225 -3.9747529 -3.9399638 -3.9160237 -3.9017072 -3.8909974 -3.8809001 -3.8422592 -3.7870553 -3.805346 -3.8974538 -3.9909737 -4.0566936 -4.122108 -4.1778545][-4.008091 -3.9660094 -3.9397519 -3.910495 -3.8835678 -3.8649058 -3.8535075 -3.8186619 -3.7675142 -3.7904472 -3.8876669 -3.9750757 -4.0324664 -4.0982876 -4.1601148][-4.0445852 -4.0232005 -4.0119486 -3.9865093 -3.9618292 -3.9471965 -3.932759 -3.9053764 -3.87087 -3.8908329 -3.9646242 -4.0231171 -4.06093 -4.1139603 -4.1673431][-4.1157088 -4.1124525 -4.1134152 -4.0966325 -4.0790329 -4.0717726 -4.0561905 -4.0337644 -4.0157743 -4.0295115 -4.0768433 -4.1094007 -4.129662 -4.1663737 -4.2024727][-4.1947889 -4.2000408 -4.2090516 -4.2025609 -4.194284 -4.1918416 -4.1778827 -4.16003 -4.1503081 -4.1590047 -4.1864247 -4.2026739 -4.2137051 -4.2353649 -4.254118][-4.2529902 -4.2585063 -4.2686982 -4.2682972 -4.2651525 -4.2655287 -4.2576923 -4.2461319 -4.2390771 -4.2453485 -4.2639508 -4.2726159 -4.2788734 -4.2900238 -4.2964797][-4.2707891 -4.2740965 -4.2785873 -4.2788224 -4.27735 -4.27821 -4.2748709 -4.2668233 -4.2612491 -4.2668824 -4.2822742 -4.2925477 -4.2989526 -4.307703 -4.3122244][-4.2680478 -4.2711077 -4.273025 -4.2736406 -4.2738495 -4.2734408 -4.2699571 -4.2637472 -4.2601771 -4.2636681 -4.2749457 -4.2853742 -4.2929692 -4.302228 -4.3096919]]...]
INFO - root - 2017-12-07 13:32:32.260514: step 14110, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.611 sec/batch; 54h:03m:55s remains)
INFO - root - 2017-12-07 13:32:39.071802: step 14120, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 58h:54m:32s remains)
INFO - root - 2017-12-07 13:32:45.877642: step 14130, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 63h:48m:46s remains)
INFO - root - 2017-12-07 13:32:52.806954: step 14140, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 63h:36m:05s remains)
INFO - root - 2017-12-07 13:32:59.579233: step 14150, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 59h:22m:30s remains)
INFO - root - 2017-12-07 13:33:06.404077: step 14160, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 62h:02m:11s remains)
INFO - root - 2017-12-07 13:33:13.262025: step 14170, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 56h:24m:08s remains)
INFO - root - 2017-12-07 13:33:20.040802: step 14180, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 62h:33m:24s remains)
INFO - root - 2017-12-07 13:33:26.275016: step 14190, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 57h:53m:40s remains)
INFO - root - 2017-12-07 13:33:33.075157: step 14200, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 59h:20m:00s remains)
2017-12-07 13:33:33.763441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390226 -4.3366237 -4.3353677 -4.3352137 -4.3329577 -4.3273659 -4.3211832 -4.3187642 -4.3201919 -4.324245 -4.329258 -4.3318877 -4.3337531 -4.335053 -4.3343253][-4.32961 -4.32349 -4.3196888 -4.3177457 -4.3136086 -4.3061996 -4.2984047 -4.2942996 -4.2955475 -4.3011384 -4.308897 -4.3139081 -4.316443 -4.31735 -4.3171196][-4.3146873 -4.3045993 -4.2981048 -4.2931738 -4.2876291 -4.2803411 -4.2722244 -4.2669654 -4.2661662 -4.2721038 -4.280952 -4.2895379 -4.296155 -4.2979417 -4.2971063][-4.2877212 -4.2736149 -4.2659731 -4.2594562 -4.2539024 -4.2482309 -4.2409954 -4.2339492 -4.2287488 -4.23016 -4.2388477 -4.2529397 -4.26591 -4.2715111 -4.2702007][-4.2491336 -4.2308736 -4.2230144 -4.2166424 -4.2120204 -4.207366 -4.19988 -4.1910858 -4.1806526 -4.1778655 -4.1857796 -4.2047086 -4.2227149 -4.232235 -4.2302809][-4.2208829 -4.2015271 -4.1938105 -4.1843762 -4.1748505 -4.1628261 -4.1476779 -4.1289897 -4.1091623 -4.1047525 -4.1221166 -4.1522722 -4.1750989 -4.1836691 -4.1796756][-4.2126088 -4.1969237 -4.1899681 -4.1730051 -4.1481218 -4.1178389 -4.0812793 -4.0377131 -4.0009093 -3.9972115 -4.0308933 -4.0799136 -4.1140957 -4.1247292 -4.1199942][-4.2145262 -4.2014971 -4.19548 -4.1760597 -4.1426153 -4.0992327 -4.043499 -3.9725709 -3.913408 -3.9077392 -3.9536319 -4.013659 -4.05396 -4.0728765 -4.0731711][-4.2208719 -4.2126036 -4.2092772 -4.1937332 -4.1654167 -4.12934 -4.0770063 -4.0019865 -3.9362788 -3.9228044 -3.9580984 -4.0063276 -4.0405188 -4.0621018 -4.0627356][-4.2338972 -4.2316179 -4.2308655 -4.2197471 -4.2022004 -4.1819644 -4.15047 -4.0975165 -4.0474443 -4.028995 -4.0421834 -4.0672584 -4.0859408 -4.1015248 -4.0969515][-4.2440524 -4.2431512 -4.2446566 -4.2399631 -4.2311926 -4.224925 -4.211092 -4.1802497 -4.1499653 -4.1368532 -4.1401834 -4.1510468 -4.1544762 -4.1559162 -4.1409612][-4.2443533 -4.2406087 -4.2413616 -4.2417669 -4.240685 -4.2464619 -4.2470121 -4.232276 -4.2171884 -4.213232 -4.2163405 -4.2213335 -4.2135081 -4.1990237 -4.1709237][-4.2423368 -4.2308617 -4.2264328 -4.22852 -4.2344184 -4.24774 -4.2536731 -4.2469845 -4.2404027 -4.2428989 -4.2504368 -4.2556767 -4.2440658 -4.2194567 -4.1797867][-4.2530408 -4.2326431 -4.2194786 -4.2176461 -4.225677 -4.2394 -4.242095 -4.233819 -4.2269516 -4.2313509 -4.241992 -4.2523274 -4.2450514 -4.2190776 -4.1767011][-4.264472 -4.2401977 -4.2255974 -4.222188 -4.2271347 -4.235713 -4.2338595 -4.22202 -4.2105446 -4.2114205 -4.2225528 -4.236587 -4.2368264 -4.2163668 -4.1773176]]...]
INFO - root - 2017-12-07 13:33:40.459801: step 14210, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 65h:03m:16s remains)
INFO - root - 2017-12-07 13:33:47.355911: step 14220, loss = 2.05, batch loss = 2.00 (10.6 examples/sec; 0.753 sec/batch; 66h:32m:53s remains)
INFO - root - 2017-12-07 13:33:54.102955: step 14230, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 58h:38m:42s remains)
INFO - root - 2017-12-07 13:34:00.891241: step 14240, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 56h:18m:28s remains)
INFO - root - 2017-12-07 13:34:07.753534: step 14250, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 59h:17m:15s remains)
INFO - root - 2017-12-07 13:34:14.581798: step 14260, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 64h:04m:01s remains)
INFO - root - 2017-12-07 13:34:21.393331: step 14270, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 61h:41m:26s remains)
INFO - root - 2017-12-07 13:34:28.249259: step 14280, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 58h:22m:25s remains)
INFO - root - 2017-12-07 13:34:34.945788: step 14290, loss = 2.07, batch loss = 2.01 (13.5 examples/sec; 0.591 sec/batch; 52h:15m:05s remains)
INFO - root - 2017-12-07 13:34:41.828276: step 14300, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 64h:04m:17s remains)
2017-12-07 13:34:42.552913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1343336 -4.1185784 -4.1409283 -4.1797247 -4.2106628 -4.2193656 -4.2068782 -4.1836853 -4.158483 -4.1376009 -4.1257114 -4.1285224 -4.1317158 -4.1297069 -4.120667][-4.2101078 -4.203651 -4.221046 -4.2459154 -4.2583647 -4.2529564 -4.2392259 -4.2258892 -4.2137594 -4.2028074 -4.1923518 -4.184813 -4.1735988 -4.1586061 -4.1391349][-4.2601538 -4.2584929 -4.2651286 -4.2726312 -4.2693987 -4.2566586 -4.2450376 -4.24312 -4.2510438 -4.260354 -4.2591767 -4.2492714 -4.2326393 -4.2132363 -4.1916265][-4.2721405 -4.2731681 -4.2666068 -4.2537355 -4.231926 -4.2037907 -4.1869669 -4.1966372 -4.2316256 -4.2690492 -4.28597 -4.287169 -4.2775474 -4.2654233 -4.2532563][-4.2498503 -4.2477221 -4.2250309 -4.1891184 -4.1426506 -4.0891576 -4.058598 -4.0828934 -4.15372 -4.2273149 -4.27331 -4.2962084 -4.3013349 -4.302701 -4.3016777][-4.2241635 -4.2127085 -4.1717629 -4.1103826 -4.0349422 -3.9458287 -3.8916385 -3.9344029 -4.0444493 -4.1523333 -4.2258253 -4.2714443 -4.2939615 -4.3088989 -4.3132753][-4.2185807 -4.1903811 -4.1281147 -4.044343 -3.9429202 -3.8201842 -3.7393298 -3.8010597 -3.9412367 -4.0695519 -4.1589127 -4.217402 -4.2520838 -4.2753696 -4.2800684][-4.2256556 -4.1849375 -4.1129389 -4.0253835 -3.9216464 -3.7967985 -3.71189 -3.778471 -3.9106402 -4.0249081 -4.1085305 -4.1657348 -4.2043066 -4.2301912 -4.2322235][-4.2447629 -4.2040877 -4.1412978 -4.072577 -3.9956825 -3.9068644 -3.8507032 -3.8968275 -3.9749064 -4.0426068 -4.099041 -4.14352 -4.1785879 -4.2026634 -4.1991649][-4.2683072 -4.2361808 -4.1903214 -4.1431417 -4.09241 -4.0345612 -3.9965851 -4.0157466 -4.0478387 -4.0803323 -4.1139884 -4.1488748 -4.1817803 -4.2008104 -4.1876159][-4.2850232 -4.2637753 -4.2334371 -4.2035542 -4.1712527 -4.1337457 -4.1035843 -4.0990567 -4.102838 -4.1145115 -4.1344061 -4.1623788 -4.1938047 -4.2068834 -4.1859412][-4.2905931 -4.2800918 -4.2632108 -4.2472973 -4.230082 -4.2106581 -4.1908908 -4.1762857 -4.1665678 -4.1652784 -4.1715589 -4.1869254 -4.20615 -4.208343 -4.1798477][-4.2884989 -4.28696 -4.2822533 -4.2794313 -4.2769852 -4.2735519 -4.2665472 -4.2548857 -4.2450042 -4.2382455 -4.2334805 -4.2334003 -4.2321072 -4.2168951 -4.1774931][-4.27925 -4.2825637 -4.2850718 -4.2904348 -4.2963142 -4.301578 -4.3035049 -4.3001952 -4.2985187 -4.2951956 -4.2871504 -4.2774692 -4.2577438 -4.2243676 -4.1751776][-4.2664027 -4.2682576 -4.270153 -4.2775474 -4.2873826 -4.2971678 -4.302598 -4.3024879 -4.3047757 -4.3069577 -4.3040481 -4.2937288 -4.2642722 -4.2206135 -4.1657577]]...]
INFO - root - 2017-12-07 13:34:49.354677: step 14310, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 55h:29m:11s remains)
INFO - root - 2017-12-07 13:34:56.213066: step 14320, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 57h:32m:30s remains)
INFO - root - 2017-12-07 13:35:03.028137: step 14330, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 63h:10m:30s remains)
INFO - root - 2017-12-07 13:35:09.930180: step 14340, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 63h:28m:26s remains)
INFO - root - 2017-12-07 13:35:16.712646: step 14350, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 56h:58m:33s remains)
INFO - root - 2017-12-07 13:35:23.660698: step 14360, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 57h:36m:46s remains)
INFO - root - 2017-12-07 13:35:30.557325: step 14370, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 63h:02m:54s remains)
INFO - root - 2017-12-07 13:35:37.384734: step 14380, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 62h:04m:13s remains)
INFO - root - 2017-12-07 13:35:44.092061: step 14390, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 60h:24m:39s remains)
INFO - root - 2017-12-07 13:35:50.928398: step 14400, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 54h:35m:38s remains)
2017-12-07 13:35:51.679752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2351251 -4.2327251 -4.2394123 -4.2397571 -4.2434635 -4.2425575 -4.23486 -4.220561 -4.2183161 -4.2312489 -4.2482462 -4.265192 -4.2807217 -4.2926321 -4.3022919][-4.220437 -4.216464 -4.2270107 -4.2270379 -4.2273941 -4.2295361 -4.2214127 -4.2061825 -4.20706 -4.2194128 -4.2329483 -4.24909 -4.2656732 -4.2810922 -4.2944818][-4.1984878 -4.1938138 -4.2032185 -4.1977444 -4.1919918 -4.1947536 -4.1847053 -4.1662807 -4.1745162 -4.1941042 -4.21143 -4.2306757 -4.2463779 -4.2616272 -4.2784848][-4.1796 -4.1727982 -4.1745348 -4.1595216 -4.1467443 -4.14872 -4.1307783 -4.1107321 -4.1369815 -4.1754489 -4.2011933 -4.2204113 -4.2315884 -4.2430749 -4.2603874][-4.1663504 -4.1568522 -4.1433725 -4.1146879 -4.0950961 -4.0882359 -4.0501075 -4.0209045 -4.0781317 -4.1529636 -4.1942487 -4.2138305 -4.2215462 -4.2280774 -4.2419381][-4.1537046 -4.1403804 -4.1162376 -4.0767555 -4.0465264 -4.0186982 -3.9389021 -3.8772335 -3.9732327 -4.0982776 -4.1663651 -4.192358 -4.1999454 -4.2036643 -4.21268][-4.1560669 -4.1437612 -4.1142092 -4.0702987 -4.036293 -3.9864509 -3.8614202 -3.7524037 -3.8723757 -4.0344205 -4.122735 -4.1534958 -4.1621733 -4.1653838 -4.1708016][-4.1714411 -4.1672597 -4.1478357 -4.1181006 -4.0935993 -4.038166 -3.9073205 -3.7931986 -3.8908923 -4.0329366 -4.106503 -4.1272864 -4.1307263 -4.1305227 -4.1323881][-4.1858764 -4.1907911 -4.186409 -4.1745977 -4.1595416 -4.1095438 -4.0098429 -3.9310863 -3.9940734 -4.0904245 -4.1338511 -4.1374173 -4.1301413 -4.1243567 -4.1225748][-4.1980376 -4.2019086 -4.2087007 -4.2121735 -4.2015643 -4.1554275 -4.086731 -4.0418477 -4.084764 -4.1499686 -4.1752353 -4.1669655 -4.14636 -4.1316037 -4.1275997][-4.2038069 -4.2087951 -4.2232537 -4.2341919 -4.2222533 -4.1804204 -4.1364441 -4.1145444 -4.1445794 -4.1943254 -4.2144513 -4.200387 -4.1639194 -4.1328735 -4.1218104][-4.2194977 -4.231988 -4.249 -4.2558546 -4.2402949 -4.2027383 -4.1753631 -4.168232 -4.1907353 -4.2269173 -4.240799 -4.218545 -4.1647525 -4.1172209 -4.0996718][-4.2399697 -4.2559314 -4.2736621 -4.2775025 -4.2634764 -4.2309804 -4.211575 -4.2135186 -4.2291017 -4.250967 -4.2556257 -4.2287264 -4.1670847 -4.1083355 -4.08563][-4.2546315 -4.2648993 -4.273139 -4.2721586 -4.2600932 -4.2354503 -4.2246966 -4.2337747 -4.2487922 -4.2611613 -4.2576447 -4.2323833 -4.1794448 -4.12524 -4.1049452][-4.2641182 -4.2655048 -4.2640162 -4.2550225 -4.2424746 -4.223805 -4.2202163 -4.2334466 -4.2508359 -4.2574029 -4.2493324 -4.2311625 -4.1934223 -4.1510472 -4.1365447]]...]
INFO - root - 2017-12-07 13:35:58.464429: step 14410, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 63h:39m:45s remains)
INFO - root - 2017-12-07 13:36:05.280274: step 14420, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 58h:41m:04s remains)
INFO - root - 2017-12-07 13:36:12.068013: step 14430, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 58h:45m:26s remains)
INFO - root - 2017-12-07 13:36:18.904194: step 14440, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 61h:07m:17s remains)
INFO - root - 2017-12-07 13:36:25.736960: step 14450, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.751 sec/batch; 66h:18m:33s remains)
INFO - root - 2017-12-07 13:36:32.544090: step 14460, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 62h:40m:07s remains)
INFO - root - 2017-12-07 13:36:39.354430: step 14470, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.643 sec/batch; 56h:45m:55s remains)
INFO - root - 2017-12-07 13:36:46.104822: step 14480, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 59h:45m:04s remains)
INFO - root - 2017-12-07 13:36:52.879037: step 14490, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 64h:13m:23s remains)
INFO - root - 2017-12-07 13:36:59.507298: step 14500, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 58h:39m:19s remains)
2017-12-07 13:37:00.239852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2568264 -4.2573681 -4.2658215 -4.2711911 -4.2716293 -4.269556 -4.2675333 -4.2752995 -4.2868085 -4.2977495 -4.3050041 -4.3094397 -4.3174858 -4.3230634 -4.3180032][-4.2476916 -4.2501597 -4.2634239 -4.2761807 -4.285428 -4.2876444 -4.282155 -4.2800751 -4.2859597 -4.2936497 -4.3020983 -4.3077707 -4.3146472 -4.32108 -4.3124933][-4.2510352 -4.2505655 -4.260438 -4.2728863 -4.2841039 -4.2834148 -4.2648492 -4.2422304 -4.2345915 -4.241724 -4.2576022 -4.273406 -4.2854247 -4.2965527 -4.2912722][-4.260108 -4.2513428 -4.2523222 -4.2619596 -4.2708545 -4.2606711 -4.2204351 -4.1679845 -4.14268 -4.1547952 -4.1845574 -4.2134061 -4.2356048 -4.2497635 -4.2462559][-4.2630887 -4.2440829 -4.2322888 -4.2346649 -4.23627 -4.2096567 -4.1432428 -4.0610509 -4.0253177 -4.05302 -4.1024919 -4.1461134 -4.1788449 -4.19279 -4.1870623][-4.2317414 -4.1991081 -4.172718 -4.1655397 -4.1579814 -4.11696 -4.0302453 -3.9318044 -3.9065347 -3.9615679 -4.031795 -4.0896735 -4.1330748 -4.1472864 -4.1377907][-4.1543894 -4.1107039 -4.0805659 -4.0721531 -4.0606127 -4.0156956 -3.930475 -3.8489521 -3.8487628 -3.9160967 -3.9875786 -4.0489674 -4.0988512 -4.1130314 -4.1001787][-4.0844569 -4.0445485 -4.0276523 -4.0293007 -4.0238585 -3.9877665 -3.9255142 -3.879669 -3.8967566 -3.9532349 -4.0050578 -4.052958 -4.0958395 -4.1035104 -4.0850215][-4.0640821 -4.045702 -4.0536432 -4.0672359 -4.068295 -4.0433178 -4.0058317 -3.9882209 -4.0092669 -4.0477958 -4.0760889 -4.1047225 -4.1330733 -4.1316342 -4.1079893][-4.1172462 -4.1185279 -4.13887 -4.1553621 -4.1574736 -4.1410985 -4.1224761 -4.1218371 -4.1405282 -4.1647124 -4.1775784 -4.1941757 -4.2104096 -4.2019 -4.1751776][-4.2071767 -4.2166185 -4.2365365 -4.2494049 -4.2503724 -4.2419915 -4.238277 -4.2462692 -4.2620792 -4.2775774 -4.2823753 -4.2893677 -4.2956038 -4.2828164 -4.2558317][-4.2813396 -4.2902517 -4.3039451 -4.3127971 -4.3144965 -4.3135886 -4.319725 -4.3313036 -4.343605 -4.352459 -4.3514853 -4.3493729 -4.346539 -4.3340573 -4.3132019][-4.3269644 -4.3322906 -4.3389225 -4.3436475 -4.3450222 -4.3472939 -4.3551288 -4.3649631 -4.3732853 -4.37736 -4.3746057 -4.3694534 -4.3630571 -4.3537922 -4.3416553][-4.3446956 -4.3475671 -4.3495932 -4.3514748 -4.3525615 -4.35425 -4.3591576 -4.3649387 -4.3691692 -4.3707786 -4.3691411 -4.3652239 -4.3601456 -4.3550248 -4.3497114][-4.3486843 -4.3503585 -4.35067 -4.3512607 -4.3513775 -4.3515511 -4.3529592 -4.3546891 -4.3560677 -4.3565822 -4.3559022 -4.3542976 -4.3524747 -4.35163 -4.35093]]...]
INFO - root - 2017-12-07 13:37:07.124267: step 14510, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 63h:32m:12s remains)
INFO - root - 2017-12-07 13:37:14.021589: step 14520, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 62h:55m:37s remains)
INFO - root - 2017-12-07 13:37:20.858479: step 14530, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 55h:28m:06s remains)
INFO - root - 2017-12-07 13:37:27.800690: step 14540, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 57h:27m:39s remains)
INFO - root - 2017-12-07 13:37:34.625122: step 14550, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 62h:57m:11s remains)
INFO - root - 2017-12-07 13:37:41.413084: step 14560, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 64h:44m:20s remains)
INFO - root - 2017-12-07 13:37:48.223107: step 14570, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 59h:23m:05s remains)
INFO - root - 2017-12-07 13:37:55.167739: step 14580, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 55h:28m:39s remains)
INFO - root - 2017-12-07 13:38:01.801837: step 14590, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 57h:58m:25s remains)
INFO - root - 2017-12-07 13:38:08.671003: step 14600, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 64h:37m:14s remains)
2017-12-07 13:38:09.450661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2180443 -4.2476172 -4.267755 -4.2520871 -4.1969867 -4.1270304 -4.0665565 -4.04968 -4.0880384 -4.1318851 -4.1658368 -4.1812911 -4.1798983 -4.1754227 -4.178875][-4.2467375 -4.2813106 -4.3035684 -4.2916956 -4.245122 -4.1863484 -4.1263919 -4.0899105 -4.0965872 -4.1159515 -4.139472 -4.147058 -4.1471515 -4.1566892 -4.1744061][-4.2732649 -4.3025255 -4.32154 -4.3121891 -4.2719812 -4.2195864 -4.1605945 -4.1154137 -4.1066694 -4.1071777 -4.1180134 -4.1191139 -4.1225538 -4.1450739 -4.175179][-4.2815437 -4.2997007 -4.3115878 -4.2993422 -4.257534 -4.1997108 -4.1401629 -4.1073351 -4.109973 -4.1108279 -4.1145945 -4.1149216 -4.1204653 -4.1487937 -4.1829567][-4.2838445 -4.2945161 -4.29776 -4.2737775 -4.2167006 -4.1432042 -4.0766964 -4.0612774 -4.09865 -4.1270614 -4.1394358 -4.14797 -4.1576629 -4.1782665 -4.1997004][-4.2868562 -4.2938232 -4.2866631 -4.2414942 -4.1543922 -4.0516376 -3.9683068 -3.963747 -4.044415 -4.1162357 -4.1525908 -4.1829081 -4.2043719 -4.2199459 -4.2314482][-4.2980585 -4.30134 -4.2804108 -4.2126122 -4.0926166 -3.9497983 -3.8344865 -3.8286667 -3.9524031 -4.0686812 -4.1342444 -4.1911664 -4.2321091 -4.2538867 -4.2656822][-4.3071923 -4.3065915 -4.2773914 -4.1953239 -4.0564451 -3.8900576 -3.7509346 -3.7306693 -3.8719378 -4.0189986 -4.1112785 -4.1908708 -4.2443433 -4.273345 -4.2904754][-4.2857037 -4.295557 -4.2783418 -4.2121763 -4.0943956 -3.9529326 -3.84024 -3.8217192 -3.928947 -4.0540414 -4.1411424 -4.2186437 -4.2703867 -4.2973528 -4.3131943][-4.2375779 -4.2729921 -4.285111 -4.2562675 -4.1800056 -4.0828695 -4.0097737 -3.9940162 -4.0555472 -4.1387386 -4.2016306 -4.2562714 -4.2950444 -4.3148947 -4.32441][-4.1853356 -4.2450738 -4.2857742 -4.2919888 -4.2579265 -4.2020097 -4.157464 -4.1403928 -4.1670036 -4.2166004 -4.2585344 -4.2909241 -4.313549 -4.3239183 -4.328558][-4.1502142 -4.2290893 -4.2888641 -4.3187561 -4.3162408 -4.2947865 -4.2701383 -4.2510314 -4.2556038 -4.2776632 -4.301775 -4.3205037 -4.3324342 -4.3362689 -4.3378758][-4.1461129 -4.23335 -4.2971373 -4.3356366 -4.3492551 -4.3456774 -4.3321071 -4.3144493 -4.3098249 -4.3158884 -4.3258324 -4.3356233 -4.3420835 -4.3428469 -4.3435678][-4.1747479 -4.2521257 -4.3036227 -4.3386846 -4.3568873 -4.361393 -4.3528571 -4.3399186 -4.3338666 -4.3338637 -4.3359251 -4.33968 -4.344615 -4.345572 -4.3461981][-4.2164421 -4.2744961 -4.3066349 -4.33025 -4.3446722 -4.3481646 -4.3412995 -4.3321986 -4.32714 -4.326807 -4.3281794 -4.3317685 -4.3368688 -4.3376031 -4.3366947]]...]
INFO - root - 2017-12-07 13:38:16.205366: step 14610, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 55h:26m:08s remains)
INFO - root - 2017-12-07 13:38:23.052438: step 14620, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 63h:50m:44s remains)
INFO - root - 2017-12-07 13:38:29.796360: step 14630, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.699 sec/batch; 61h:41m:26s remains)
INFO - root - 2017-12-07 13:38:36.635405: step 14640, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 58h:19m:52s remains)
INFO - root - 2017-12-07 13:38:43.438813: step 14650, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 56h:13m:46s remains)
INFO - root - 2017-12-07 13:38:50.301398: step 14660, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 57h:40m:29s remains)
INFO - root - 2017-12-07 13:38:57.190045: step 14670, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 63h:47m:19s remains)
INFO - root - 2017-12-07 13:39:04.083231: step 14680, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.767 sec/batch; 67h:43m:50s remains)
INFO - root - 2017-12-07 13:39:10.722477: step 14690, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 56h:59m:08s remains)
INFO - root - 2017-12-07 13:39:17.575777: step 14700, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 56h:32m:16s remains)
2017-12-07 13:39:18.278697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2940044 -4.2962818 -4.3085542 -4.3139715 -4.3126669 -4.3121576 -4.3125949 -4.3086219 -4.3020496 -4.300281 -4.3065276 -4.3140974 -4.317143 -4.3201528 -4.3204675][-4.2608027 -4.2603245 -4.270371 -4.2766504 -4.2765665 -4.2757549 -4.2748351 -4.2697425 -4.2624779 -4.2593417 -4.26323 -4.2688193 -4.27007 -4.2728729 -4.2748256][-4.1918807 -4.1873188 -4.1964307 -4.2050743 -4.2076788 -4.2057676 -4.2023783 -4.1947265 -4.1851492 -4.1780748 -4.1752481 -4.1744432 -4.1725297 -4.1749415 -4.1788921][-4.1045532 -4.087173 -4.0892553 -4.0991731 -4.1066895 -4.1067686 -4.1026082 -4.0934491 -4.0811024 -4.0681005 -4.0560503 -4.0450797 -4.0375676 -4.0357356 -4.0370312][-4.0083618 -3.9702299 -3.9595392 -3.965704 -3.9728932 -3.9742103 -3.9706564 -3.9638848 -3.9548097 -3.9427392 -3.9271646 -3.9096856 -3.89765 -3.8914759 -3.8889081][-3.9907982 -3.9397497 -3.9153736 -3.9099743 -3.9060509 -3.9007788 -3.8971281 -3.8988888 -3.9047236 -3.9070458 -3.9002721 -3.8874898 -3.8783565 -3.8723819 -3.8672037][-4.0721259 -4.0257177 -3.9976118 -3.98488 -3.9697714 -3.9524553 -3.94284 -3.9515672 -3.9739807 -3.9965086 -4.009449 -4.0108514 -4.0098548 -4.0064955 -3.9997401][-4.1512489 -4.1169224 -4.0935864 -4.0823913 -4.06494 -4.0414042 -4.0246749 -4.0320663 -4.0591478 -4.0900364 -4.1152658 -4.127142 -4.1328225 -4.13025 -4.1217513][-4.1858773 -4.1578946 -4.1395092 -4.133934 -4.1224937 -4.1041584 -4.0882745 -4.0921392 -4.1152372 -4.1433063 -4.1682529 -4.1816697 -4.1890135 -4.1856852 -4.17798][-4.1908364 -4.1618409 -4.1442213 -4.1433907 -4.1402397 -4.1315141 -4.1214886 -4.1209908 -4.1338925 -4.1517224 -4.169836 -4.1808844 -4.1879067 -4.1861362 -4.1833129][-4.1931524 -4.1618447 -4.1416759 -4.143465 -4.1487818 -4.150753 -4.1476088 -4.140512 -4.1381788 -4.1421323 -4.1507263 -4.1579914 -4.1653 -4.1663933 -4.1690598][-4.2069397 -4.1740394 -4.15138 -4.152986 -4.1620135 -4.1689487 -4.168962 -4.1562381 -4.14112 -4.1332464 -4.13267 -4.1354966 -4.1418743 -4.1454997 -4.1525073][-4.2266941 -4.1938286 -4.1722422 -4.1715374 -4.1761084 -4.1809545 -4.1791224 -4.1633496 -4.14203 -4.1282272 -4.1217647 -4.1204166 -4.12501 -4.129251 -4.1380486][-4.2363105 -4.2045703 -4.185638 -4.1835012 -4.1837306 -4.1869206 -4.1859713 -4.1734066 -4.155375 -4.1422119 -4.1347384 -4.1317205 -4.1344557 -4.137969 -4.1451979][-4.23958 -4.2058129 -4.1869421 -4.1830964 -4.1815839 -4.1846919 -4.1881094 -4.1835589 -4.17362 -4.164762 -4.15662 -4.1527629 -4.1529465 -4.1563454 -4.165009]]...]
INFO - root - 2017-12-07 13:39:25.130352: step 14710, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 64h:04m:53s remains)
INFO - root - 2017-12-07 13:39:31.979799: step 14720, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 57h:09m:50s remains)
INFO - root - 2017-12-07 13:39:38.808389: step 14730, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 58h:18m:01s remains)
INFO - root - 2017-12-07 13:39:45.625376: step 14740, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 65h:02m:33s remains)
INFO - root - 2017-12-07 13:39:52.450872: step 14750, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 63h:00m:01s remains)
INFO - root - 2017-12-07 13:39:59.248844: step 14760, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 57h:48m:15s remains)
INFO - root - 2017-12-07 13:40:05.955462: step 14770, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 55h:49m:55s remains)
INFO - root - 2017-12-07 13:40:12.865726: step 14780, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 63h:43m:37s remains)
INFO - root - 2017-12-07 13:40:19.656275: step 14790, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.737 sec/batch; 65h:04m:01s remains)
INFO - root - 2017-12-07 13:40:26.420169: step 14800, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 63h:09m:50s remains)
2017-12-07 13:40:27.127868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.233717 -4.2292147 -4.2263031 -4.2218881 -4.2158461 -4.2111554 -4.211606 -4.2194028 -4.2223511 -4.2253809 -4.2376127 -4.2565904 -4.2608809 -4.2622447 -4.2684875][-4.2010527 -4.2023625 -4.20117 -4.1953335 -4.1787696 -4.16208 -4.16211 -4.1749797 -4.1853585 -4.1966128 -4.2201924 -4.2512569 -4.2614031 -4.2649212 -4.2718153][-4.1638737 -4.1758294 -4.1789017 -4.1702805 -4.1440029 -4.1118493 -4.1067557 -4.1168575 -4.1345706 -4.1602755 -4.1962438 -4.2366352 -4.2564135 -4.2666492 -4.273284][-4.1268167 -4.1471024 -4.160171 -4.1516109 -4.1146369 -4.0676889 -4.0535178 -4.060647 -4.0846596 -4.1248469 -4.1692619 -4.2126923 -4.2449832 -4.2668948 -4.2773566][-4.1122932 -4.1367574 -4.156383 -4.1461644 -4.093399 -4.0262055 -3.9940863 -4.00008 -4.0298195 -4.0773778 -4.1301765 -4.1790128 -4.2189012 -4.2474294 -4.2635107][-4.1321445 -4.1442204 -4.1554704 -4.1365366 -4.0720148 -3.9875188 -3.9326639 -3.9290767 -3.9606023 -4.0116029 -4.0752473 -4.1327529 -4.1791811 -4.2103748 -4.2282257][-4.1648955 -4.1515832 -4.1452503 -4.1224031 -4.0641789 -3.9800692 -3.9093068 -3.8869061 -3.9057369 -3.9474874 -4.0080023 -4.0731354 -4.1247492 -4.1612496 -4.1825709][-4.1798453 -4.1423512 -4.1178145 -4.0977964 -4.0633421 -4.0087271 -3.9552813 -3.9320037 -3.9350972 -3.9536731 -3.9881237 -4.039412 -4.0907335 -4.1332483 -4.1609592][-4.1783767 -4.1267328 -4.0906305 -4.0742922 -4.065588 -4.0494614 -4.0371528 -4.0349393 -4.0278921 -4.0189934 -4.0208135 -4.0507936 -4.0953035 -4.1434484 -4.1775265][-4.1713037 -4.1197038 -4.0830221 -4.0732875 -4.0869985 -4.1034236 -4.1218443 -4.1359191 -4.122333 -4.0956912 -4.0831056 -4.1028156 -4.14034 -4.1835055 -4.2142687][-4.1719918 -4.131978 -4.1028271 -4.1016493 -4.1310105 -4.1658144 -4.1968989 -4.2157297 -4.2030993 -4.1775656 -4.1646724 -4.1769791 -4.2016006 -4.2300487 -4.246448][-4.1894889 -4.1645312 -4.1481237 -4.1574407 -4.1905532 -4.2275634 -4.2575684 -4.2747536 -4.266448 -4.2481208 -4.2394786 -4.2463212 -4.2596726 -4.2704983 -4.2705235][-4.2138596 -4.2056665 -4.2040553 -4.221406 -4.2528672 -4.2830849 -4.303762 -4.314559 -4.3092966 -4.2971764 -4.2917743 -4.2964253 -4.3055024 -4.3074989 -4.2956319][-4.2359419 -4.2414951 -4.2501187 -4.268786 -4.2935362 -4.3143177 -4.3269825 -4.33235 -4.3277164 -4.3172035 -4.3116517 -4.3142848 -4.3200665 -4.3175788 -4.3009348][-4.2572327 -4.271245 -4.2841091 -4.2989779 -4.3142672 -4.3258414 -4.3316278 -4.3321347 -4.3273487 -4.3165913 -4.308044 -4.3061175 -4.3076043 -4.3034444 -4.2879853]]...]
INFO - root - 2017-12-07 13:40:33.556954: step 14810, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 63h:00m:08s remains)
INFO - root - 2017-12-07 13:40:40.421746: step 14820, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 60h:13m:49s remains)
INFO - root - 2017-12-07 13:40:47.164988: step 14830, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 56h:47m:39s remains)
INFO - root - 2017-12-07 13:40:53.949135: step 14840, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 56h:02m:18s remains)
INFO - root - 2017-12-07 13:41:00.720600: step 14850, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.699 sec/batch; 61h:40m:26s remains)
INFO - root - 2017-12-07 13:41:07.643068: step 14860, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 65h:08m:04s remains)
INFO - root - 2017-12-07 13:41:14.359511: step 14870, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 58h:14m:38s remains)
INFO - root - 2017-12-07 13:41:21.204924: step 14880, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 58h:31m:55s remains)
INFO - root - 2017-12-07 13:41:27.742703: step 14890, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.618 sec/batch; 54h:30m:02s remains)
INFO - root - 2017-12-07 13:41:34.590053: step 14900, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 62h:02m:51s remains)
2017-12-07 13:41:35.338261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2860756 -4.2662492 -4.2315989 -4.1977806 -4.1780796 -4.16621 -4.1710176 -4.1865458 -4.175797 -4.1682096 -4.1823478 -4.1949973 -4.20344 -4.2206311 -4.2400422][-4.276217 -4.2607 -4.2260995 -4.1970868 -4.1948123 -4.2060204 -4.2225857 -4.2254958 -4.1852584 -4.1520844 -4.1647897 -4.1846986 -4.1969652 -4.2144513 -4.2322335][-4.2539225 -4.241715 -4.2072806 -4.1848221 -4.199409 -4.2280445 -4.2508755 -4.2536659 -4.2068572 -4.1617284 -4.1652751 -4.1843281 -4.1974497 -4.2112379 -4.2269969][-4.2203174 -4.206337 -4.1659966 -4.1469383 -4.1754079 -4.218585 -4.2487965 -4.25677 -4.2244177 -4.1925526 -4.190341 -4.1991644 -4.208662 -4.216784 -4.2256441][-4.1936064 -4.17148 -4.1247163 -4.1051807 -4.1376414 -4.1831856 -4.2131038 -4.2280741 -4.2237687 -4.2160721 -4.218564 -4.2209144 -4.2267971 -4.2311244 -4.2309251][-4.1821275 -4.155406 -4.1109295 -4.0883813 -4.0998917 -4.1175852 -4.1311922 -4.1524744 -4.1813827 -4.20996 -4.2295642 -4.2382264 -4.2450142 -4.2480416 -4.2432075][-4.2143874 -4.1892447 -4.1504421 -4.1155591 -4.0849323 -4.0419359 -4.0115957 -4.0355244 -4.1042976 -4.1783667 -4.2270803 -4.2495375 -4.261579 -4.2708273 -4.2676692][-4.266922 -4.2481341 -4.2148337 -4.1714091 -4.1102667 -4.0116396 -3.9215918 -3.9290981 -4.033607 -4.1454773 -4.2153006 -4.2478466 -4.2671041 -4.2835789 -4.2876558][-4.2970839 -4.2852345 -4.2628803 -4.2277927 -4.1694341 -4.0624814 -3.9417911 -3.9095204 -4.0045671 -4.1242428 -4.1978345 -4.2329354 -4.2569757 -4.2782755 -4.293128][-4.3013754 -4.2973061 -4.287509 -4.2698884 -4.2306662 -4.1477866 -4.0339842 -3.9641733 -4.01001 -4.1098394 -4.1820183 -4.2156811 -4.2420263 -4.2630682 -4.2851276][-4.2949042 -4.2935128 -4.2935896 -4.2879343 -4.2654281 -4.2123966 -4.1228876 -4.0409575 -4.0392985 -4.1051211 -4.171102 -4.20523 -4.2292786 -4.2452731 -4.2662773][-4.2803431 -4.2776623 -4.2790589 -4.2813845 -4.2707186 -4.241653 -4.183219 -4.1128197 -4.0871978 -4.1196179 -4.168211 -4.1987381 -4.2171288 -4.2272859 -4.2439318][-4.2585354 -4.2530189 -4.2482491 -4.2505317 -4.2471766 -4.2359123 -4.207593 -4.1627746 -4.1364431 -4.1469259 -4.1762977 -4.1992726 -4.2101874 -4.2148042 -4.224546][-4.2492132 -4.2393379 -4.2203655 -4.2127156 -4.20786 -4.2022943 -4.1983485 -4.1818204 -4.168992 -4.1721807 -4.190918 -4.2076015 -4.2098784 -4.2046738 -4.2077713][-4.252295 -4.2402415 -4.2094569 -4.1833186 -4.1666279 -4.156158 -4.1614661 -4.1643982 -4.1676931 -4.1783924 -4.1987896 -4.2152014 -4.2112341 -4.1959343 -4.1866159]]...]
INFO - root - 2017-12-07 13:41:42.170818: step 14910, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 56h:28m:47s remains)
INFO - root - 2017-12-07 13:41:49.022722: step 14920, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.668 sec/batch; 58h:56m:45s remains)
INFO - root - 2017-12-07 13:41:55.875177: step 14930, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 63h:12m:46s remains)
INFO - root - 2017-12-07 13:42:02.904989: step 14940, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 63h:32m:37s remains)
INFO - root - 2017-12-07 13:42:09.717218: step 14950, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 59h:09m:12s remains)
INFO - root - 2017-12-07 13:42:16.449065: step 14960, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 56h:48m:24s remains)
INFO - root - 2017-12-07 13:42:23.256285: step 14970, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 59h:55m:52s remains)
INFO - root - 2017-12-07 13:42:30.113446: step 14980, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 65h:26m:26s remains)
INFO - root - 2017-12-07 13:42:36.827721: step 14990, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 58h:10m:41s remains)
INFO - root - 2017-12-07 13:42:43.492988: step 15000, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.610 sec/batch; 53h:46m:41s remains)
2017-12-07 13:42:44.222228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2613258 -4.2591119 -4.2593307 -4.2553186 -4.2483444 -4.2395554 -4.2298074 -4.2239966 -4.2269254 -4.2281642 -4.2190967 -4.1991749 -4.1936331 -4.2045608 -4.2229104][-4.2716985 -4.2718272 -4.2753487 -4.2731161 -4.2666626 -4.2592721 -4.2517672 -4.2468209 -4.2481794 -4.244576 -4.2292786 -4.2019658 -4.1906457 -4.200458 -4.2176514][-4.2565074 -4.2569461 -4.260375 -4.257144 -4.2510324 -4.2481208 -4.2473068 -4.2462363 -4.2494488 -4.2452526 -4.2299805 -4.2010136 -4.1852732 -4.1927071 -4.2074728][-4.2291517 -4.2298884 -4.2337136 -4.2297549 -4.22397 -4.2257738 -4.2329869 -4.2369971 -4.2444649 -4.2449827 -4.2350802 -4.2111907 -4.1947088 -4.1978498 -4.2096643][-4.2020097 -4.2020221 -4.2059317 -4.201499 -4.1936245 -4.1972132 -4.2094975 -4.2175393 -4.2291412 -4.2364683 -4.2335882 -4.2200041 -4.20856 -4.2091608 -4.2153926][-4.1697383 -4.168077 -4.1700191 -4.1625309 -4.1476674 -4.1439228 -4.1510968 -4.1619658 -4.1856613 -4.2062879 -4.2127028 -4.2102246 -4.2060261 -4.2056985 -4.2097416][-4.1488562 -4.1426668 -4.1398926 -4.1239266 -4.0923824 -4.0650268 -4.05128 -4.0630388 -4.1068935 -4.1488562 -4.1696243 -4.176578 -4.1795096 -4.1822438 -4.1878972][-4.1447363 -4.1335754 -4.1236691 -4.0941563 -4.03795 -3.9771619 -3.9313784 -3.9413126 -4.0132289 -4.0846992 -4.122757 -4.13904 -4.1454759 -4.14974 -4.1582675][-4.1543226 -4.1407456 -4.127152 -4.0899534 -4.0185075 -3.9379563 -3.8770821 -3.8900568 -3.978493 -4.066401 -4.1136088 -4.1304369 -4.1303244 -4.1283841 -4.1350293][-4.1728992 -4.1645317 -4.1594934 -4.1325731 -4.074122 -4.0067759 -3.9595916 -3.969219 -4.0364308 -4.1081657 -4.1492243 -4.159615 -4.148447 -4.1388927 -4.1401114][-4.1992 -4.1988778 -4.2068973 -4.1989822 -4.16477 -4.1232553 -4.0932961 -4.0935659 -4.1274557 -4.1701145 -4.1986823 -4.2041206 -4.1893969 -4.1770473 -4.1761522][-4.222239 -4.2246585 -4.24061 -4.2485385 -4.2362752 -4.2138653 -4.1968627 -4.1933808 -4.2057171 -4.2250466 -4.2412596 -4.2467031 -4.238205 -4.230803 -4.2310462][-4.2310047 -4.2339191 -4.2539659 -4.27217 -4.2732582 -4.2649608 -4.2583523 -4.2567077 -4.2595663 -4.26317 -4.2677855 -4.2733316 -4.2740908 -4.2733464 -4.2756062][-4.2157426 -4.2192149 -4.2415113 -4.2643814 -4.2730575 -4.2721872 -4.2696581 -4.2687507 -4.2673445 -4.26615 -4.268507 -4.2759156 -4.2831912 -4.2864962 -4.2872257][-4.1791387 -4.1849003 -4.2092385 -4.2359638 -4.2492676 -4.2521839 -4.2511539 -4.2486691 -4.2458472 -4.2460976 -4.2521615 -4.2631803 -4.2741361 -4.2789822 -4.278368]]...]
INFO - root - 2017-12-07 13:42:51.070205: step 15010, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 65h:18m:35s remains)
INFO - root - 2017-12-07 13:42:57.842481: step 15020, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 57h:32m:40s remains)
INFO - root - 2017-12-07 13:43:04.604049: step 15030, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 55h:58m:58s remains)
INFO - root - 2017-12-07 13:43:11.316713: step 15040, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 56h:21m:02s remains)
INFO - root - 2017-12-07 13:43:18.077764: step 15050, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 61h:29m:07s remains)
INFO - root - 2017-12-07 13:43:24.839357: step 15060, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 59h:35m:43s remains)
INFO - root - 2017-12-07 13:43:31.496476: step 15070, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.664 sec/batch; 58h:33m:47s remains)
INFO - root - 2017-12-07 13:43:38.236144: step 15080, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 55h:21m:38s remains)
INFO - root - 2017-12-07 13:43:44.889836: step 15090, loss = 2.09, batch loss = 2.04 (12.6 examples/sec; 0.636 sec/batch; 56h:04m:35s remains)
INFO - root - 2017-12-07 13:43:51.714402: step 15100, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 62h:32m:57s remains)
2017-12-07 13:43:52.411469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3159771 -4.2970476 -4.2796493 -4.2633057 -4.2482076 -4.2339988 -4.2249408 -4.2162046 -4.2248068 -4.2476277 -4.2651 -4.2647829 -4.2590437 -4.2579141 -4.2578759][-4.3113384 -4.2881246 -4.2713652 -4.2545671 -4.2305288 -4.1985321 -4.1772838 -4.1703463 -4.18954 -4.222858 -4.2504449 -4.2544641 -4.2439318 -4.2386684 -4.2389321][-4.3038139 -4.2796311 -4.2620039 -4.2452269 -4.2178154 -4.17101 -4.1265192 -4.1186824 -4.1530004 -4.2010531 -4.2380643 -4.24978 -4.2389789 -4.2295465 -4.2296515][-4.2895551 -4.2649407 -4.2447519 -4.2247043 -4.1933594 -4.1327291 -4.0553713 -4.0337257 -4.089469 -4.1601977 -4.2124629 -4.2369227 -4.2339678 -4.2246537 -4.2241831][-4.2688165 -4.2443 -4.2243729 -4.2021217 -4.1648049 -4.0912704 -3.9875803 -3.9420631 -4.0102377 -4.1062522 -4.1790071 -4.216527 -4.223588 -4.2209163 -4.2195015][-4.2488117 -4.226222 -4.2081861 -4.1808743 -4.131989 -4.0405917 -3.9194083 -3.852731 -3.9266906 -4.04635 -4.1422262 -4.1891036 -4.2058997 -4.2141581 -4.2141876][-4.2335024 -4.2172108 -4.200901 -4.1684752 -4.1078591 -4.0006175 -3.8673232 -3.7945368 -3.863081 -3.9875257 -4.0958443 -4.152523 -4.1829381 -4.203166 -4.2086325][-4.2252464 -4.2167234 -4.2072177 -4.1792994 -4.1224852 -4.0262566 -3.9119854 -3.8524652 -3.8982291 -3.9885306 -4.0761795 -4.1289611 -4.165307 -4.1902905 -4.2011104][-4.228559 -4.224453 -4.219295 -4.2015476 -4.1607819 -4.093142 -4.0170074 -3.9823213 -4.005549 -4.0512409 -4.1050763 -4.1400723 -4.1712465 -4.1925859 -4.2027173][-4.2345953 -4.2362623 -4.235157 -4.2222037 -4.1895094 -4.1442537 -4.0988641 -4.0884786 -4.09912 -4.1227894 -4.1581607 -4.1799793 -4.197484 -4.2117486 -4.2184706][-4.2433281 -4.2491503 -4.2529888 -4.2482142 -4.2246976 -4.1878524 -4.1538696 -4.152946 -4.1637788 -4.184865 -4.2128119 -4.227006 -4.2321949 -4.2388959 -4.2423005][-4.2550163 -4.2643709 -4.2703319 -4.2728696 -4.2582641 -4.2264791 -4.1985283 -4.2049389 -4.2205992 -4.2372952 -4.2539549 -4.2620783 -4.2645969 -4.2682796 -4.2689509][-4.2659211 -4.2788 -4.2890263 -4.2928987 -4.2835083 -4.2578382 -4.2391262 -4.2525263 -4.2672615 -4.2749658 -4.2811451 -4.2867169 -4.2916918 -4.2970042 -4.2959442][-4.2740026 -4.2866607 -4.29904 -4.3042655 -4.3005309 -4.28551 -4.2794085 -4.2948852 -4.3021212 -4.3015752 -4.3039856 -4.3096671 -4.3138247 -4.3171315 -4.3146672][-4.2831621 -4.2880979 -4.2939806 -4.2965574 -4.2965007 -4.2923646 -4.2950172 -4.3092365 -4.3145251 -4.3146868 -4.3176131 -4.3218641 -4.32182 -4.3216615 -4.3203335]]...]
INFO - root - 2017-12-07 13:43:59.180857: step 15110, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 56h:18m:22s remains)
INFO - root - 2017-12-07 13:44:05.831068: step 15120, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 62h:33m:57s remains)
INFO - root - 2017-12-07 13:44:12.575541: step 15130, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 61h:59m:20s remains)
INFO - root - 2017-12-07 13:44:19.315063: step 15140, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 56h:40m:06s remains)
INFO - root - 2017-12-07 13:44:26.028878: step 15150, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 55h:28m:35s remains)
INFO - root - 2017-12-07 13:44:32.752542: step 15160, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 57h:15m:24s remains)
INFO - root - 2017-12-07 13:44:39.516198: step 15170, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 63h:34m:09s remains)
INFO - root - 2017-12-07 13:44:46.410853: step 15180, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 63h:03m:52s remains)
INFO - root - 2017-12-07 13:44:53.086204: step 15190, loss = 2.06, batch loss = 2.01 (13.1 examples/sec; 0.611 sec/batch; 53h:51m:25s remains)
INFO - root - 2017-12-07 13:44:59.862126: step 15200, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 57h:38m:52s remains)
2017-12-07 13:45:00.617890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.259264 -4.2646213 -4.2543683 -4.2447605 -4.2435174 -4.2465658 -4.2545795 -4.2511072 -4.2423029 -4.25128 -4.2693877 -4.286859 -4.2980027 -4.2997737 -4.2984419][-4.2691836 -4.2791882 -4.2695503 -4.2544336 -4.2455134 -4.2418537 -4.2485209 -4.2492142 -4.24221 -4.2534471 -4.2742753 -4.289885 -4.2959814 -4.2959538 -4.2961144][-4.2602253 -4.2744193 -4.2707953 -4.2559566 -4.2410307 -4.2296348 -4.23135 -4.2326279 -4.2249103 -4.2314038 -4.2499685 -4.2656612 -4.2730737 -4.2804675 -4.2876868][-4.2349529 -4.2519178 -4.2573557 -4.2462645 -4.2283716 -4.2103376 -4.2044311 -4.2010369 -4.1871963 -4.1872249 -4.2046175 -4.2232661 -4.2391787 -4.2580795 -4.2759161][-4.2051907 -4.2199388 -4.228054 -4.2211251 -4.2065716 -4.1866541 -4.1732354 -4.1582141 -4.1342473 -4.1248007 -4.1379423 -4.1674185 -4.203373 -4.2386742 -4.2687516][-4.1736059 -4.1836066 -4.1924191 -4.19059 -4.1807008 -4.1605091 -4.139257 -4.111908 -4.0805573 -4.0639071 -4.0780625 -4.1278696 -4.187552 -4.236125 -4.271379][-4.1424761 -4.1497583 -4.16116 -4.1669111 -4.1625223 -4.1448865 -4.1139517 -4.0738692 -4.0353837 -4.0215244 -4.0518532 -4.1230063 -4.19582 -4.2494125 -4.2822027][-4.1169991 -4.1230078 -4.1341591 -4.1453061 -4.1470079 -4.1333804 -4.0943294 -4.0455236 -4.0042133 -4.004354 -4.0560079 -4.139164 -4.213738 -4.2650275 -4.2931204][-4.1073089 -4.1100178 -4.1196203 -4.1290193 -4.1260529 -4.1084275 -4.0682096 -4.0223136 -3.9965568 -4.0241446 -4.0912294 -4.1703358 -4.2345209 -4.2762508 -4.3001962][-4.1196327 -4.1201167 -4.1280966 -4.1283035 -4.1149154 -4.092926 -4.0609455 -4.0372448 -4.0482297 -4.0972371 -4.158896 -4.2167411 -4.2598143 -4.2889938 -4.3053918][-4.1522336 -4.1533551 -4.158556 -4.149509 -4.1276722 -4.10571 -4.0870094 -4.0910554 -4.1259966 -4.1755962 -4.2222266 -4.2569737 -4.2836938 -4.3029313 -4.3130527][-4.1994181 -4.1994858 -4.1982403 -4.1814709 -4.16003 -4.1439934 -4.1408014 -4.1590991 -4.1927075 -4.2360563 -4.2687798 -4.2877545 -4.3062119 -4.3205743 -4.3249388][-4.2573395 -4.2556105 -4.2479596 -4.2280407 -4.2109714 -4.2031021 -4.2102027 -4.2289252 -4.2540755 -4.2861185 -4.3084211 -4.3185987 -4.3303366 -4.3392434 -4.3364291][-4.3036561 -4.2983475 -4.2876196 -4.268321 -4.2548981 -4.2530169 -4.2639 -4.2794976 -4.2981315 -4.3221693 -4.3402719 -4.3460312 -4.3505192 -4.3516655 -4.3428583][-4.3312531 -4.3238878 -4.3109064 -4.2914515 -4.2782741 -4.2773361 -4.2875686 -4.3034191 -4.32234 -4.3442259 -4.3600907 -4.36344 -4.3622036 -4.3563056 -4.3432255]]...]
INFO - root - 2017-12-07 13:45:07.510829: step 15210, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 60h:11m:57s remains)
INFO - root - 2017-12-07 13:45:14.341880: step 15220, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 54h:34m:27s remains)
INFO - root - 2017-12-07 13:45:21.073686: step 15230, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 57h:14m:21s remains)
INFO - root - 2017-12-07 13:45:27.862905: step 15240, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 60h:20m:02s remains)
INFO - root - 2017-12-07 13:45:34.596685: step 15250, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 64h:50m:35s remains)
INFO - root - 2017-12-07 13:45:41.415305: step 15260, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 61h:58m:39s remains)
INFO - root - 2017-12-07 13:45:48.179705: step 15270, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.648 sec/batch; 57h:04m:52s remains)
INFO - root - 2017-12-07 13:45:55.028267: step 15280, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 57h:21m:23s remains)
INFO - root - 2017-12-07 13:46:01.722412: step 15290, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 63h:31m:50s remains)
INFO - root - 2017-12-07 13:46:08.496945: step 15300, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 62h:49m:07s remains)
2017-12-07 13:46:09.237962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1988559 -4.2041807 -4.2173676 -4.2319784 -4.2390213 -4.2390866 -4.222374 -4.1895394 -4.1695037 -4.1548433 -4.1233854 -4.0948734 -4.1170845 -4.174407 -4.19994][-4.1905265 -4.2073655 -4.2284651 -4.2352214 -4.22652 -4.2164354 -4.1968288 -4.1687741 -4.1527438 -4.1373386 -4.1056752 -4.0802054 -4.1109481 -4.180078 -4.2223625][-4.181509 -4.2046871 -4.2196727 -4.2183466 -4.2050924 -4.1984048 -4.1855965 -4.1640496 -4.1502953 -4.1321845 -4.0966949 -4.0664563 -4.0882835 -4.1535072 -4.2069612][-4.1540933 -4.1774354 -4.1842084 -4.1821766 -4.1767855 -4.1801419 -4.1713252 -4.154367 -4.1467552 -4.1349149 -4.1038055 -4.07198 -4.0827541 -4.1381383 -4.1942863][-4.1097088 -4.1355782 -4.1384387 -4.1360865 -4.135005 -4.1416159 -4.13038 -4.1172333 -4.1204128 -4.126236 -4.1101665 -4.0880265 -4.0932789 -4.1395612 -4.1977067][-4.0776196 -4.1059036 -4.1071792 -4.0986772 -4.0952682 -4.0944619 -4.0733318 -4.0576353 -4.0724683 -4.0985112 -4.1086283 -4.1043353 -4.1097255 -4.1464329 -4.1992149][-4.0673923 -4.0944419 -4.0926151 -4.0737376 -4.0698338 -4.0633192 -4.0330486 -4.0127573 -4.0319147 -4.0764718 -4.1119666 -4.1254463 -4.1248178 -4.1437206 -4.1860566][-4.076158 -4.0919013 -4.0826917 -4.0530949 -4.0488319 -4.0399051 -4.0087752 -3.9912424 -4.0141788 -4.0669823 -4.1150565 -4.1415157 -4.138082 -4.140162 -4.1700206][-4.0945015 -4.1022243 -4.0843172 -4.0495481 -4.0438433 -4.0369887 -4.0100951 -4.0041652 -4.0339141 -4.0869794 -4.134316 -4.1575837 -4.1499724 -4.1431847 -4.16187][-4.137156 -4.1407914 -4.1215019 -4.088593 -4.08146 -4.0755692 -4.0570903 -4.0639877 -4.0923724 -4.1338272 -4.1668735 -4.1729722 -4.1554632 -4.1462984 -4.158967][-4.1873121 -4.1898956 -4.1743741 -4.1503134 -4.1462941 -4.1431975 -4.1338892 -4.1426153 -4.1644578 -4.188005 -4.2022443 -4.1900358 -4.1615586 -4.1515684 -4.1630116][-4.20971 -4.2142386 -4.2090406 -4.1979752 -4.1998277 -4.2036657 -4.2014627 -4.2092409 -4.2242665 -4.2335796 -4.2324696 -4.2115331 -4.1814408 -4.1724496 -4.1832871][-4.2085495 -4.2161856 -4.2201672 -4.220427 -4.22871 -4.2368512 -4.23969 -4.249743 -4.2664018 -4.2724581 -4.266221 -4.244647 -4.2209182 -4.2147608 -4.22335][-4.1990118 -4.2087774 -4.2200723 -4.2303143 -4.2456241 -4.2590981 -4.2667122 -4.2776785 -4.2936611 -4.3007441 -4.2936921 -4.2742496 -4.256566 -4.253778 -4.2611537][-4.1928291 -4.2061696 -4.2212725 -4.2356634 -4.2523818 -4.2679687 -4.2807865 -4.2925019 -4.305439 -4.3118105 -4.3056245 -4.28973 -4.277925 -4.2778883 -4.2827468]]...]
INFO - root - 2017-12-07 13:46:15.857576: step 15310, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 55h:07m:15s remains)
INFO - root - 2017-12-07 13:46:22.523430: step 15320, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 57h:25m:48s remains)
INFO - root - 2017-12-07 13:46:29.311803: step 15330, loss = 2.03, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 61h:07m:45s remains)
INFO - root - 2017-12-07 13:46:36.074706: step 15340, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 60h:41m:06s remains)
INFO - root - 2017-12-07 13:46:42.820633: step 15350, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 58h:36m:53s remains)
INFO - root - 2017-12-07 13:46:49.533066: step 15360, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 58h:01m:48s remains)
INFO - root - 2017-12-07 13:46:56.355920: step 15370, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 62h:40m:14s remains)
INFO - root - 2017-12-07 13:47:03.252391: step 15380, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 65h:12m:03s remains)
INFO - root - 2017-12-07 13:47:09.803106: step 15390, loss = 2.09, batch loss = 2.03 (13.4 examples/sec; 0.599 sec/batch; 52h:47m:02s remains)
INFO - root - 2017-12-07 13:47:16.594647: step 15400, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 55h:38m:13s remains)
2017-12-07 13:47:17.351429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2437587 -4.2683458 -4.29351 -4.3098459 -4.3165631 -4.3150349 -4.3066611 -4.2956161 -4.2873821 -4.2820821 -4.2779408 -4.268352 -4.25618 -4.2456284 -4.2435508][-4.2058029 -4.2451487 -4.27733 -4.2953148 -4.3013325 -4.2974315 -4.2875061 -4.27675 -4.27352 -4.2737122 -4.272058 -4.2585487 -4.2355227 -4.2146511 -4.2099791][-4.1688623 -4.2136774 -4.2465534 -4.2605538 -4.2600093 -4.2506943 -4.2406282 -4.2351284 -4.2430272 -4.2535548 -4.2558608 -4.2379608 -4.2098355 -4.1865907 -4.1854229][-4.1723905 -4.2076278 -4.2292838 -4.2277861 -4.2098541 -4.1829329 -4.1623454 -4.1621141 -4.1889062 -4.2134714 -4.2236409 -4.2099209 -4.1865606 -4.1733966 -4.1821728][-4.193121 -4.2133007 -4.2177052 -4.1907282 -4.13647 -4.0688796 -4.0177164 -4.0119233 -4.0669408 -4.12974 -4.166141 -4.1686726 -4.1576304 -4.1594911 -4.1792164][-4.19394 -4.2027183 -4.1955042 -4.1462064 -4.0457597 -3.9095993 -3.7881312 -3.7607188 -3.867475 -4.000289 -4.0847373 -4.1108093 -4.1133652 -4.1319408 -4.1652174][-4.1672592 -4.1736236 -4.1671329 -4.1127958 -3.9876935 -3.7978139 -3.6030898 -3.5505409 -3.7186568 -3.9135573 -4.0320034 -4.0685086 -4.0791011 -4.1106882 -4.1521187][-4.1294389 -4.1405396 -4.1523685 -4.1183915 -4.0205021 -3.8662415 -3.705579 -3.6659524 -3.800626 -3.9633522 -4.0588279 -4.077271 -4.0774531 -4.1086817 -4.1491909][-4.062912 -4.0808449 -4.1144266 -4.1204009 -4.0836716 -4.0088162 -3.9262526 -3.9094954 -3.980957 -4.0725822 -4.1218376 -4.1143646 -4.0956511 -4.1138062 -4.1495442][-3.9938512 -4.0062308 -4.0539055 -4.0946374 -4.1095743 -4.0948076 -4.0629745 -4.059402 -4.099401 -4.1515112 -4.1681581 -4.145576 -4.117569 -4.1248283 -4.1528044][-4.0040045 -4.0067048 -4.0483842 -4.0951385 -4.1324506 -4.1424131 -4.1329389 -4.1371536 -4.1648068 -4.20117 -4.2053618 -4.1803408 -4.1525707 -4.1504874 -4.1649294][-4.0694885 -4.0725951 -4.1000152 -4.1300316 -4.1597567 -4.1699243 -4.1683412 -4.17946 -4.2039618 -4.231885 -4.2299528 -4.2044973 -4.1782432 -4.17132 -4.1755519][-4.1049447 -4.1190562 -4.1451483 -4.1596541 -4.1720433 -4.174758 -4.1754327 -4.1878986 -4.2052851 -4.2213664 -4.2129331 -4.1899104 -4.1752834 -4.1757054 -4.1790528][-4.1224394 -4.1441984 -4.1700988 -4.172327 -4.1665897 -4.1540494 -4.1526437 -4.1646423 -4.172596 -4.1742611 -4.1650767 -4.1545749 -4.1664076 -4.1866546 -4.1942115][-4.14593 -4.1664877 -4.186965 -4.1761274 -4.1484003 -4.1134973 -4.1008878 -4.1123776 -4.1188197 -4.11672 -4.1147237 -4.1258583 -4.1673689 -4.2070603 -4.2215443]]...]
INFO - root - 2017-12-07 13:47:24.055752: step 15410, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 62h:08m:17s remains)
INFO - root - 2017-12-07 13:47:30.854424: step 15420, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 60h:17m:31s remains)
INFO - root - 2017-12-07 13:47:37.407686: step 15430, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 57h:50m:10s remains)
INFO - root - 2017-12-07 13:47:44.391796: step 15440, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.747 sec/batch; 65h:48m:28s remains)
INFO - root - 2017-12-07 13:47:51.157946: step 15450, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 59h:36m:56s remains)
INFO - root - 2017-12-07 13:47:58.111687: step 15460, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 58h:19m:48s remains)
INFO - root - 2017-12-07 13:48:04.952793: step 15470, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 56h:33m:17s remains)
INFO - root - 2017-12-07 13:48:11.733769: step 15480, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 63h:08m:45s remains)
INFO - root - 2017-12-07 13:48:18.419673: step 15490, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 61h:35m:00s remains)
INFO - root - 2017-12-07 13:48:25.252252: step 15500, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 57h:17m:12s remains)
2017-12-07 13:48:26.025759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1372657 -4.1379142 -4.1411457 -4.1412539 -4.1538873 -4.1623263 -4.16817 -4.1978559 -4.2183871 -4.2255487 -4.2423973 -4.2473578 -4.2379923 -4.2301593 -4.2215233][-4.1024809 -4.104815 -4.1163487 -4.1302342 -4.1602278 -4.1837168 -4.1973944 -4.2273531 -4.2415376 -4.2355714 -4.2441545 -4.2481561 -4.233686 -4.2231793 -4.2152061][-4.1191659 -4.1320033 -4.1504369 -4.1721454 -4.2070765 -4.2344689 -4.2451115 -4.2609205 -4.2614064 -4.2427058 -4.2431359 -4.2479692 -4.2329454 -4.2232924 -4.214994][-4.1789474 -4.2003336 -4.2209873 -4.2391453 -4.260076 -4.2746663 -4.2682467 -4.2576346 -4.241529 -4.2158618 -4.216289 -4.227488 -4.2186546 -4.21494 -4.2127504][-4.2490654 -4.2665429 -4.2782817 -4.28172 -4.2802105 -4.2675452 -4.22879 -4.1880565 -4.1658092 -4.1558857 -4.1650147 -4.1861677 -4.1913228 -4.1991096 -4.2119303][-4.2957854 -4.2974305 -4.29226 -4.2823524 -4.2605281 -4.2140341 -4.13119 -4.0477304 -4.0290661 -4.0575151 -4.0920472 -4.1354456 -4.1658988 -4.1914425 -4.2256079][-4.31974 -4.3090196 -4.2922754 -4.2641773 -4.2200575 -4.1396885 -4.0046306 -3.8701057 -3.8641038 -3.9559536 -4.0329213 -4.1022 -4.1562672 -4.1989274 -4.2473822][-4.3279982 -4.3122764 -4.2839875 -4.2357283 -4.1695495 -4.0729012 -3.9362345 -3.8196666 -3.8451831 -3.9684126 -4.0634875 -4.1412387 -4.2002506 -4.2416229 -4.2823234][-4.3269944 -4.3058052 -4.27264 -4.2199388 -4.1555452 -4.081358 -3.9950314 -3.9415019 -3.9852138 -4.0810051 -4.1547918 -4.2215209 -4.2733736 -4.3041587 -4.3263149][-4.3196831 -4.2889166 -4.251945 -4.2096596 -4.1656146 -4.1222997 -4.0847311 -4.0793285 -4.1297231 -4.1946397 -4.2407637 -4.2846427 -4.3212633 -4.339282 -4.3472052][-4.3065596 -4.2654438 -4.2213006 -4.18501 -4.1611209 -4.1434517 -4.1421442 -4.166708 -4.2190824 -4.2586274 -4.2779756 -4.300776 -4.3233323 -4.3328753 -4.3340721][-4.2757978 -4.2257705 -4.1704583 -4.1301031 -4.1190534 -4.1308074 -4.1581683 -4.2022486 -4.2532048 -4.2814813 -4.2880259 -4.2931266 -4.3008218 -4.3034978 -4.3002167][-4.2380943 -4.181716 -4.1213346 -4.0774517 -4.0787773 -4.1185179 -4.1688361 -4.2222581 -4.2664418 -4.2827268 -4.277729 -4.2674274 -4.2579465 -4.2524223 -4.2474484][-4.2143111 -4.160759 -4.1075516 -4.0707054 -4.0803437 -4.1364355 -4.1929045 -4.2343259 -4.2564626 -4.2565227 -4.2428803 -4.2225308 -4.2002115 -4.1931624 -4.1976366][-4.2287946 -4.1906071 -4.1533856 -4.1276932 -4.1398811 -4.1895823 -4.23409 -4.2530432 -4.2485194 -4.2333264 -4.2131543 -4.1911411 -4.1672683 -4.1631117 -4.1800623]]...]
INFO - root - 2017-12-07 13:48:32.772076: step 15510, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 60h:51m:10s remains)
INFO - root - 2017-12-07 13:48:39.605048: step 15520, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 60h:28m:10s remains)
INFO - root - 2017-12-07 13:48:46.367240: step 15530, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 61h:07m:13s remains)
INFO - root - 2017-12-07 13:48:53.128013: step 15540, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 56h:58m:59s remains)
INFO - root - 2017-12-07 13:48:59.960175: step 15550, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 60h:53m:15s remains)
INFO - root - 2017-12-07 13:49:06.796943: step 15560, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.737 sec/batch; 64h:55m:41s remains)
INFO - root - 2017-12-07 13:49:13.536662: step 15570, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 59h:51m:41s remains)
INFO - root - 2017-12-07 13:49:20.372097: step 15580, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 57h:03m:42s remains)
INFO - root - 2017-12-07 13:49:26.999433: step 15590, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 56h:01m:30s remains)
INFO - root - 2017-12-07 13:49:33.808978: step 15600, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.724 sec/batch; 63h:43m:31s remains)
2017-12-07 13:49:34.492166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2411914 -4.2513871 -4.2497721 -4.2312665 -4.2154613 -4.2205815 -4.2405472 -4.2638893 -4.2683024 -4.2571955 -4.2362704 -4.2098403 -4.2039666 -4.2185984 -4.2529297][-4.2309918 -4.2383227 -4.2327189 -4.2080021 -4.1854243 -4.1886406 -4.2157116 -4.2478995 -4.2632275 -4.2629485 -4.2472782 -4.2208242 -4.2142906 -4.2271323 -4.2541003][-4.236073 -4.2397275 -4.22558 -4.1894155 -4.1553316 -4.1541257 -4.1844935 -4.2206855 -4.2417946 -4.2512507 -4.2447433 -4.2241907 -4.2214665 -4.2354994 -4.259243][-4.2528892 -4.2508416 -4.2246728 -4.1748824 -4.1257682 -4.1163645 -4.1458955 -4.183064 -4.2071338 -4.2230544 -4.2302227 -4.2234554 -4.2290759 -4.2467804 -4.2679334][-4.2757373 -4.2689576 -4.2325554 -4.169734 -4.1078348 -4.0853834 -4.10566 -4.1399846 -4.1646886 -4.1843529 -4.2059393 -4.2164779 -4.2336164 -4.2542405 -4.2728238][-4.3036304 -4.2974854 -4.26126 -4.1960912 -4.1280823 -4.0845895 -4.0753083 -4.0888166 -4.1085048 -4.1318216 -4.164927 -4.191196 -4.2213893 -4.2477527 -4.2701535][-4.3348436 -4.3332691 -4.3068981 -4.2534523 -4.1871915 -4.123702 -4.0728688 -4.0467277 -4.0487828 -4.0744467 -4.1184511 -4.1575508 -4.1967883 -4.2296572 -4.2600183][-4.3582959 -4.3623052 -4.348094 -4.3114095 -4.2580514 -4.1938286 -4.1249728 -4.0705342 -4.05171 -4.0690327 -4.1111422 -4.1518507 -4.1930408 -4.2291932 -4.2635765][-4.3647218 -4.3725624 -4.3657737 -4.3419108 -4.3033309 -4.25525 -4.2001004 -4.152184 -4.1312618 -4.1328449 -4.1513538 -4.1738133 -4.2072344 -4.2440109 -4.2802153][-4.3540425 -4.3631225 -4.3594451 -4.3448734 -4.3179264 -4.2843142 -4.2498446 -4.2238564 -4.2162385 -4.2153459 -4.2145109 -4.215189 -4.2317295 -4.2607555 -4.2939115][-4.3371539 -4.343317 -4.3382268 -4.329298 -4.310051 -4.2864056 -4.2685404 -4.261941 -4.2684441 -4.2734065 -4.2702565 -4.2633786 -4.2649589 -4.2822738 -4.30619][-4.3215861 -4.3174524 -4.3044987 -4.293386 -4.2756467 -4.2559977 -4.2465305 -4.2537336 -4.2725825 -4.2873454 -4.2913589 -4.2893152 -4.2878003 -4.2977095 -4.3147745][-4.3097377 -4.2913942 -4.2636366 -4.241744 -4.2187233 -4.1977859 -4.1913714 -4.2070312 -4.2394056 -4.2657194 -4.2784033 -4.2858939 -4.2903152 -4.3004532 -4.3155208][-4.301827 -4.2724009 -4.22995 -4.1928558 -4.1565933 -4.1298146 -4.123168 -4.1453261 -4.1910024 -4.229744 -4.2511697 -4.2652864 -4.2735033 -4.2871966 -4.3060842][-4.2829566 -4.2478213 -4.19601 -4.1443195 -4.0956254 -4.0648317 -4.0597782 -4.0841 -4.1393056 -4.1895614 -4.2200727 -4.2400246 -4.2520204 -4.2702112 -4.2950187]]...]
INFO - root - 2017-12-07 13:49:41.308860: step 15610, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 57h:17m:15s remains)
INFO - root - 2017-12-07 13:49:48.102128: step 15620, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 55h:56m:20s remains)
INFO - root - 2017-12-07 13:49:54.870813: step 15630, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.711 sec/batch; 62h:36m:53s remains)
INFO - root - 2017-12-07 13:50:01.664519: step 15640, loss = 2.08, batch loss = 2.03 (10.7 examples/sec; 0.747 sec/batch; 65h:46m:14s remains)
INFO - root - 2017-12-07 13:50:08.434215: step 15650, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 58h:46m:28s remains)
INFO - root - 2017-12-07 13:50:15.207058: step 15660, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 55h:25m:22s remains)
INFO - root - 2017-12-07 13:50:22.099578: step 15670, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 59h:58m:51s remains)
INFO - root - 2017-12-07 13:50:29.043645: step 15680, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 64h:12m:31s remains)
INFO - root - 2017-12-07 13:50:35.658256: step 15690, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 60h:13m:11s remains)
INFO - root - 2017-12-07 13:50:42.363649: step 15700, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 59h:21m:28s remains)
2017-12-07 13:50:43.097711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3351727 -4.3347964 -4.3333392 -4.33019 -4.3245192 -4.314157 -4.3015409 -4.2928662 -4.2896152 -4.2880573 -4.2884879 -4.2909746 -4.2962885 -4.3039222 -4.3129377][-4.3318415 -4.3315639 -4.3260865 -4.3156495 -4.3024864 -4.285356 -4.2677221 -4.2584991 -4.2576494 -4.2552991 -4.2549858 -4.2588139 -4.2683272 -4.2801528 -4.2917953][-4.3188238 -4.314045 -4.3015885 -4.2828646 -4.2615314 -4.2388234 -4.2205997 -4.2129459 -4.22034 -4.2198133 -4.2167721 -4.2196283 -4.2313647 -4.2477918 -4.2632861][-4.3031454 -4.2882323 -4.2670288 -4.2381005 -4.2037687 -4.1704807 -4.1460857 -4.1438036 -4.170361 -4.1846189 -4.1866813 -4.1908822 -4.2010846 -4.2159042 -4.2312856][-4.2885137 -4.2645512 -4.2356296 -4.1930337 -4.138154 -4.0803642 -4.0293803 -4.0293283 -4.0879374 -4.1340256 -4.1574082 -4.1678576 -4.1781197 -4.1888223 -4.2013464][-4.2666507 -4.2380352 -4.2046094 -4.1474333 -4.0634789 -3.9618397 -3.8610938 -3.8621459 -3.9632597 -4.0520668 -4.1060319 -4.1289959 -4.1462379 -4.1593432 -4.1730871][-4.2377582 -4.2057948 -4.1705174 -4.0997062 -3.9895594 -3.8415093 -3.680145 -3.6798463 -3.8279071 -3.960438 -4.0465851 -4.0866156 -4.1148739 -4.1351686 -4.1516514][-4.1984324 -4.1667762 -4.1401358 -4.0775065 -3.9776483 -3.8343058 -3.6674759 -3.659452 -3.798986 -3.9319332 -4.0167279 -4.0647135 -4.0989394 -4.1249948 -4.1435323][-4.1585054 -4.1294904 -4.1157265 -4.0800052 -4.0228424 -3.9361961 -3.834847 -3.8216856 -3.898912 -3.9863257 -4.0416746 -4.078352 -4.1086264 -4.1362014 -4.1566863][-4.1239 -4.0991144 -4.1021 -4.0978603 -4.0810671 -4.0410986 -3.9983337 -3.9894857 -4.0243258 -4.0693235 -4.0951476 -4.1137929 -4.1355329 -4.1625042 -4.1857681][-4.112762 -4.0923328 -4.1050673 -4.1199436 -4.1275272 -4.1173229 -4.1086516 -4.1045027 -4.1166372 -4.1377177 -4.1480985 -4.1561103 -4.1725054 -4.1977615 -4.22526][-4.1376538 -4.1247048 -4.140666 -4.1614723 -4.1758685 -4.1775818 -4.1845098 -4.1839046 -4.1869416 -4.19941 -4.2085285 -4.2141919 -4.2257066 -4.2466011 -4.2710638][-4.1921539 -4.1876397 -4.20243 -4.2197394 -4.2301326 -4.23425 -4.2419863 -4.2431021 -4.2419739 -4.2481332 -4.257031 -4.2645988 -4.2767329 -4.2935905 -4.3111525][-4.2457156 -4.2450938 -4.2576594 -4.2723413 -4.2804027 -4.2850094 -4.2904663 -4.2918768 -4.2895513 -4.29137 -4.296813 -4.3041663 -4.3151646 -4.3270326 -4.3379283][-4.285605 -4.2871284 -4.2969632 -4.3065982 -4.31254 -4.3161416 -4.3199296 -4.3215995 -4.3198462 -4.3202448 -4.3236351 -4.3293238 -4.3360677 -4.3426838 -4.3480349]]...]
INFO - root - 2017-12-07 13:50:49.988999: step 15710, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.751 sec/batch; 66h:03m:25s remains)
INFO - root - 2017-12-07 13:50:56.863676: step 15720, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 59h:48m:14s remains)
INFO - root - 2017-12-07 13:51:03.580955: step 15730, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 54h:08m:32s remains)
INFO - root - 2017-12-07 13:51:10.258329: step 15740, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 61h:30m:38s remains)
INFO - root - 2017-12-07 13:51:17.118656: step 15750, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 62h:19m:03s remains)
INFO - root - 2017-12-07 13:51:24.011404: step 15760, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 61h:11m:24s remains)
INFO - root - 2017-12-07 13:51:30.712055: step 15770, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.624 sec/batch; 54h:55m:10s remains)
INFO - root - 2017-12-07 13:51:37.504438: step 15780, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 58h:12m:46s remains)
INFO - root - 2017-12-07 13:51:44.260760: step 15790, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 60h:28m:43s remains)
INFO - root - 2017-12-07 13:51:51.062817: step 15800, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 60h:38m:24s remains)
2017-12-07 13:51:51.756700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.133975 -4.1483207 -4.1703153 -4.2027979 -4.2316709 -4.2556686 -4.2762656 -4.2843785 -4.2713633 -4.2434907 -4.2054563 -4.1611319 -4.1342673 -4.1407042 -4.1644092][-4.16048 -4.1676874 -4.180151 -4.203939 -4.2327628 -4.2596664 -4.28183 -4.2864838 -4.2686191 -4.2369432 -4.2018027 -4.1655064 -4.1494694 -4.1647329 -4.1980209][-4.18067 -4.17327 -4.174161 -4.1929674 -4.2221837 -4.2479348 -4.2608004 -4.2542458 -4.2314949 -4.2022338 -4.1786489 -4.1598554 -4.1592989 -4.1827879 -4.220336][-4.2006059 -4.1873393 -4.1831441 -4.1963897 -4.2159734 -4.2258496 -4.2216167 -4.2038407 -4.1818671 -4.1628418 -4.1570749 -4.1610045 -4.1771426 -4.2044334 -4.236773][-4.2237272 -4.2133617 -4.2099042 -4.2118421 -4.2115908 -4.1984162 -4.1730771 -4.1437979 -4.1225162 -4.1193027 -4.1405296 -4.1674047 -4.193543 -4.2171288 -4.2376213][-4.2350159 -4.2301874 -4.2280893 -4.2182426 -4.1964607 -4.1552978 -4.1067762 -4.0633731 -4.0488625 -4.0744872 -4.1288056 -4.1757641 -4.2059631 -4.2222733 -4.2310572][-4.2157722 -4.2139583 -4.2113805 -4.1930423 -4.1540804 -4.0900693 -4.0211563 -3.9713972 -3.9776051 -4.0393581 -4.1206489 -4.1815991 -4.218255 -4.2330937 -4.2307019][-4.1804652 -4.1784158 -4.1739621 -4.1508789 -4.101584 -4.0267005 -3.9548209 -3.9211543 -3.9606235 -4.0491962 -4.1427865 -4.2064648 -4.2400727 -4.2466269 -4.229907][-4.1442752 -4.1331382 -4.1218839 -4.0947409 -4.04651 -3.9852602 -3.9426942 -3.9476962 -4.007504 -4.0942831 -4.1745 -4.2257247 -4.2481155 -4.2446213 -4.2199492][-4.1241903 -4.0956316 -4.0705175 -4.0391169 -4.0020709 -3.9715822 -3.9686182 -4.0023494 -4.0598207 -4.1259785 -4.1842451 -4.2204742 -4.2320781 -4.2233682 -4.1987882][-4.125536 -4.0861 -4.0536361 -4.0257125 -4.0013437 -3.991272 -4.0060444 -4.0402889 -4.082583 -4.1282434 -4.169733 -4.1972575 -4.2054424 -4.1961875 -4.1742787][-4.1382756 -4.1066933 -4.0841928 -4.0680079 -4.0550637 -4.0532117 -4.0665412 -4.0866103 -4.1090221 -4.1358738 -4.1623445 -4.1801314 -4.1816487 -4.1689968 -4.1464405][-4.1542797 -4.1414623 -4.1317544 -4.1206431 -4.1107354 -4.108088 -4.1164737 -4.1291618 -4.1440773 -4.1629286 -4.1796384 -4.1860905 -4.1773443 -4.1560135 -4.1273036][-4.1861734 -4.18779 -4.1799388 -4.1621222 -4.1449461 -4.1359043 -4.1384749 -4.1494765 -4.1646538 -4.181982 -4.1925087 -4.1886773 -4.1713338 -4.1457057 -4.1168671][-4.226562 -4.2298536 -4.2162747 -4.1927204 -4.1723633 -4.16113 -4.1605616 -4.1693592 -4.18234 -4.1938105 -4.195034 -4.1819949 -4.1605883 -4.1385059 -4.1193252]]...]
INFO - root - 2017-12-07 13:51:58.633364: step 15810, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.746 sec/batch; 65h:37m:57s remains)
INFO - root - 2017-12-07 13:52:05.483683: step 15820, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 61h:39m:30s remains)
INFO - root - 2017-12-07 13:52:12.242937: step 15830, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 60h:11m:32s remains)
INFO - root - 2017-12-07 13:52:19.153644: step 15840, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 55h:11m:53s remains)
INFO - root - 2017-12-07 13:52:26.045162: step 15850, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 57h:38m:54s remains)
INFO - root - 2017-12-07 13:52:32.900854: step 15860, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 64h:42m:29s remains)
INFO - root - 2017-12-07 13:52:39.774135: step 15870, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 59h:58m:42s remains)
INFO - root - 2017-12-07 13:52:46.627437: step 15880, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 60h:10m:21s remains)
INFO - root - 2017-12-07 13:52:53.145278: step 15890, loss = 2.08, batch loss = 2.03 (13.7 examples/sec; 0.584 sec/batch; 51h:20m:55s remains)
INFO - root - 2017-12-07 13:52:59.932114: step 15900, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 57h:31m:04s remains)
2017-12-07 13:53:00.721951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3162189 -4.3251381 -4.3304052 -4.3275127 -4.3224449 -4.32311 -4.327589 -4.3294053 -4.32384 -4.3137121 -4.3021975 -4.2952952 -4.2954731 -4.301333 -4.3125806][-4.2917919 -4.3031521 -4.3091173 -4.3068504 -4.3019629 -4.3059487 -4.3149281 -4.3206124 -4.3160434 -4.3016372 -4.2813249 -4.2655597 -4.2605686 -4.2676234 -4.2853746][-4.2613153 -4.2723422 -4.2767048 -4.2737737 -4.2693558 -4.2757988 -4.2890263 -4.2996454 -4.297101 -4.2800121 -4.2531075 -4.2294865 -4.2210155 -4.2311583 -4.25599][-4.2218175 -4.2273526 -4.2260556 -4.2223878 -4.2200742 -4.2312818 -4.2487874 -4.2621417 -4.2610731 -4.2453642 -4.2191377 -4.1963243 -4.1903811 -4.206234 -4.2370267][-4.1877022 -4.1806879 -4.169076 -4.1573343 -4.1548023 -4.1716614 -4.1908708 -4.2032161 -4.204432 -4.1978717 -4.18481 -4.1737113 -4.1772032 -4.1996846 -4.2328534][-4.1724911 -4.154727 -4.1314611 -4.1082811 -4.0981116 -4.1063175 -4.1130362 -4.1137662 -4.1173267 -4.1326785 -4.1479311 -4.1593628 -4.1768985 -4.2040234 -4.2337074][-4.1754904 -4.1546769 -4.1275072 -4.0954323 -4.0737591 -4.0591574 -4.0308294 -3.9951792 -3.9896333 -4.0362372 -4.0917511 -4.1303477 -4.1612129 -4.1916389 -4.2170539][-4.1783319 -4.1649513 -4.1470518 -4.1178365 -4.0874872 -4.0456696 -3.9730968 -3.8884339 -3.8609571 -3.933799 -4.0198064 -4.0742979 -4.1118546 -4.1474862 -4.1791306][-4.1674719 -4.1694393 -4.1731524 -4.16063 -4.1330285 -4.0793338 -3.99155 -3.8963981 -3.8616805 -3.919477 -3.9838219 -4.0187531 -4.0450606 -4.0827413 -4.128057][-4.1491504 -4.1697431 -4.1938629 -4.1999779 -4.1839089 -4.1393013 -4.0726552 -4.0097408 -3.98742 -4.0051341 -4.0137033 -4.0056891 -4.0063696 -4.0357113 -4.0855079][-4.1504974 -4.1812639 -4.2124281 -4.22907 -4.2239871 -4.1979322 -4.162365 -4.1307726 -4.1164904 -4.1085615 -4.0844331 -4.0506916 -4.033422 -4.0518336 -4.093379][-4.187521 -4.2106314 -4.2352533 -4.2540326 -4.2594085 -4.2512455 -4.2382178 -4.2230878 -4.2100372 -4.1940026 -4.16515 -4.1329103 -4.1164136 -4.1284466 -4.1545358][-4.2471128 -4.255157 -4.2681942 -4.285862 -4.2968526 -4.2974172 -4.2947226 -4.2864056 -4.2757745 -4.263144 -4.2447886 -4.226367 -4.21902 -4.2281137 -4.240757][-4.3007779 -4.2986631 -4.3032565 -4.316586 -4.3280249 -4.3306394 -4.329987 -4.3247023 -4.3190818 -4.3150191 -4.3092408 -4.3037663 -4.3029647 -4.3088331 -4.3125219][-4.3343182 -4.3308463 -4.3333383 -4.341826 -4.3491559 -4.350152 -4.3484988 -4.3434095 -4.339458 -4.3401027 -4.3419991 -4.3441029 -4.3466997 -4.3502827 -4.3490834]]...]
INFO - root - 2017-12-07 13:53:07.681381: step 15910, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.670 sec/batch; 58h:55m:34s remains)
INFO - root - 2017-12-07 13:53:14.487325: step 15920, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 57h:01m:26s remains)
INFO - root - 2017-12-07 13:53:21.402790: step 15930, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 58h:51m:09s remains)
INFO - root - 2017-12-07 13:53:28.173044: step 15940, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 63h:31m:28s remains)
INFO - root - 2017-12-07 13:53:34.979276: step 15950, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 61h:54m:31s remains)
INFO - root - 2017-12-07 13:53:41.776765: step 15960, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 61h:07m:05s remains)
INFO - root - 2017-12-07 13:53:48.570925: step 15970, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 55h:21m:03s remains)
INFO - root - 2017-12-07 13:53:55.473213: step 15980, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 58h:59m:06s remains)
INFO - root - 2017-12-07 13:54:02.165342: step 15990, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 62h:21m:37s remains)
INFO - root - 2017-12-07 13:54:08.979185: step 16000, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 60h:20m:39s remains)
2017-12-07 13:54:09.682751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3420224 -4.3399153 -4.34076 -4.342145 -4.34337 -4.3419766 -4.3374581 -4.3316245 -4.329041 -4.3311691 -4.3368182 -4.3435946 -4.3509078 -4.3566351 -4.3592215][-4.3050413 -4.3011003 -4.3057637 -4.3134928 -4.3211975 -4.3213716 -4.3126345 -4.3026175 -4.2982092 -4.3015304 -4.3119354 -4.3249941 -4.336822 -4.3447614 -4.3476238][-4.2516842 -4.2452836 -4.2550554 -4.2706819 -4.2882867 -4.2927055 -4.2793427 -4.2648487 -4.2562876 -4.25901 -4.2760043 -4.298872 -4.3173642 -4.3299904 -4.3351088][-4.2022362 -4.1937766 -4.2048264 -4.2224679 -4.2430167 -4.2493081 -4.2278624 -4.2035871 -4.1887579 -4.1923003 -4.2196064 -4.2567225 -4.2852378 -4.3057504 -4.3171782][-4.1670904 -4.153801 -4.1608362 -4.1745634 -4.19435 -4.1973062 -4.16294 -4.1229658 -4.0961237 -4.0987444 -4.1412425 -4.2005906 -4.2441978 -4.2745404 -4.2939749][-4.1426311 -4.1206741 -4.1165714 -4.1254582 -4.142765 -4.1378107 -4.0897098 -4.0337186 -3.9926314 -3.9915907 -4.0516186 -4.1387458 -4.2015271 -4.24457 -4.2732606][-4.1261253 -4.0996814 -4.0876193 -4.0930514 -4.1048656 -4.0914469 -4.0364504 -3.9722817 -3.9264674 -3.9240289 -3.9881296 -4.089716 -4.1678948 -4.2247853 -4.263607][-4.1086793 -4.0808029 -4.0693908 -4.0791125 -4.0871754 -4.0703344 -4.0196185 -3.96616 -3.936687 -3.946341 -4.005199 -4.09263 -4.1646147 -4.2235317 -4.2672114][-4.0912991 -4.0592823 -4.0499072 -4.0612121 -4.0704093 -4.0616059 -4.0250082 -3.9876676 -3.9802225 -4.0024996 -4.0476832 -4.1117339 -4.1719737 -4.2270832 -4.272543][-4.1111031 -4.0817146 -4.0787783 -4.0878296 -4.0966787 -4.0938354 -4.07157 -4.0494666 -4.0541048 -4.0762534 -4.1008377 -4.1415815 -4.1916456 -4.2406416 -4.28207][-4.1528478 -4.1331458 -4.13449 -4.1372766 -4.1411386 -4.1446457 -4.1368604 -4.1285329 -4.1405025 -4.1563206 -4.1624489 -4.183888 -4.2232127 -4.2639956 -4.297123][-4.1929054 -4.182189 -4.1849008 -4.1817288 -4.1794615 -4.1841993 -4.1856875 -4.1891274 -4.2060061 -4.2174978 -4.2166085 -4.2286434 -4.2599411 -4.2917867 -4.315052][-4.2320337 -4.226068 -4.2256603 -4.217 -4.2127075 -4.216393 -4.2209225 -4.2284851 -4.242908 -4.2515512 -4.2512159 -4.2611 -4.2865791 -4.3123078 -4.3299241][-4.2739654 -4.2699237 -4.2680006 -4.2598271 -4.2551141 -4.2576089 -4.2623682 -4.2691174 -4.2779169 -4.2817116 -4.2827182 -4.2923546 -4.3106155 -4.3275347 -4.339891][-4.30898 -4.306325 -4.3040051 -4.2989326 -4.2962956 -4.2985578 -4.3019414 -4.3068562 -4.3123035 -4.31364 -4.3150573 -4.3217587 -4.332202 -4.3419442 -4.3494573]]...]
INFO - root - 2017-12-07 13:54:16.528351: step 16010, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 63h:35m:19s remains)
INFO - root - 2017-12-07 13:54:23.365316: step 16020, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 64h:12m:59s remains)
INFO - root - 2017-12-07 13:54:30.186607: step 16030, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 59h:17m:19s remains)
INFO - root - 2017-12-07 13:54:36.953501: step 16040, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 55h:57m:51s remains)
INFO - root - 2017-12-07 13:54:43.528617: step 16050, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 60h:42m:56s remains)
INFO - root - 2017-12-07 13:54:50.351978: step 16060, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 61h:28m:12s remains)
INFO - root - 2017-12-07 13:54:57.174526: step 16070, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 59h:59m:45s remains)
INFO - root - 2017-12-07 13:55:03.856973: step 16080, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 55h:23m:37s remains)
INFO - root - 2017-12-07 13:55:10.524574: step 16090, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 55h:03m:57s remains)
INFO - root - 2017-12-07 13:55:17.346273: step 16100, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.709 sec/batch; 62h:20m:02s remains)
2017-12-07 13:55:18.066933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2748852 -4.279676 -4.2805104 -4.2819037 -4.2837863 -4.2850294 -4.2863355 -4.2867832 -4.2861166 -4.2842741 -4.2821727 -4.2805319 -4.283381 -4.2922554 -4.3034587][-4.25199 -4.2570834 -4.2593484 -4.2643981 -4.270925 -4.2754974 -4.2786078 -4.2806993 -4.2813673 -4.2794509 -4.2759347 -4.2708335 -4.2703524 -4.2778463 -4.2897911][-4.2262459 -4.2260027 -4.2241688 -4.227581 -4.235146 -4.2408996 -4.2449822 -4.2509732 -4.2566557 -4.259347 -4.2587347 -4.2528172 -4.2490287 -4.2544403 -4.2671471][-4.1877689 -4.1772804 -4.1665745 -4.1634912 -4.1650472 -4.1644874 -4.1650252 -4.1757336 -4.1912966 -4.2048116 -4.2138338 -4.2123947 -4.2080145 -4.2129788 -4.2286572][-4.1365986 -4.1128364 -4.0897956 -4.0765309 -4.065361 -4.0465035 -4.0344968 -4.0526457 -4.0870452 -4.1192889 -4.1439991 -4.1513739 -4.1505542 -4.1587653 -4.1801577][-4.0926232 -4.0569448 -4.0213141 -3.9960191 -3.9694827 -3.9239509 -3.8869743 -3.9130597 -3.9712505 -4.0210667 -4.0620723 -4.0822725 -4.091774 -4.1075926 -4.1363053][-4.0885468 -4.0465093 -4.0010052 -3.9625854 -3.9187825 -3.8451376 -3.7764087 -3.8074992 -3.8888865 -3.9522407 -4.0016551 -4.0323453 -4.0572128 -4.0820513 -4.1158657][-4.12482 -4.0907674 -4.047811 -4.0031939 -3.949975 -3.8654447 -3.7833652 -3.8055208 -3.8839314 -3.9426763 -3.9894254 -4.0256763 -4.0621963 -4.092267 -4.1252079][-4.1668124 -4.1515946 -4.125206 -4.0916815 -4.0482368 -3.9795959 -3.9151108 -3.9249945 -3.9715269 -4.0043893 -4.0363474 -4.0704594 -4.1078877 -4.1351676 -4.1621289][-4.1869922 -4.1928554 -4.1880054 -4.1738186 -4.1516452 -4.1095486 -4.0701556 -4.071197 -4.0884252 -4.0968618 -4.1095295 -4.131773 -4.160902 -4.1811547 -4.1996918][-4.1759996 -4.1974535 -4.2115035 -4.2193961 -4.2209291 -4.2043915 -4.1850944 -4.18205 -4.1815429 -4.1749525 -4.1711984 -4.1766152 -4.1919365 -4.2045851 -4.217947][-4.1444497 -4.1760311 -4.2027149 -4.2304316 -4.2530274 -4.2565517 -4.2513227 -4.2472692 -4.2374325 -4.2209244 -4.2045789 -4.1948109 -4.1976962 -4.205833 -4.2173147][-4.1337743 -4.1599965 -4.1853671 -4.2225518 -4.2604733 -4.2783909 -4.284143 -4.2829332 -4.273098 -4.2542024 -4.2308726 -4.2103596 -4.2027307 -4.20663 -4.2173343][-4.1781969 -4.1912441 -4.205555 -4.2378206 -4.2746596 -4.2942004 -4.3026743 -4.3037491 -4.2981486 -4.2855887 -4.2677374 -4.2493043 -4.2387476 -4.2384105 -4.2450519][-4.2430062 -4.2483463 -4.254621 -4.2759066 -4.3005133 -4.3125997 -4.3166161 -4.3172832 -4.3145881 -4.3087726 -4.2997103 -4.2889252 -4.2810235 -4.2786183 -4.2812495]]...]
INFO - root - 2017-12-07 13:55:24.900073: step 16110, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 56h:28m:37s remains)
INFO - root - 2017-12-07 13:55:31.740865: step 16120, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 62h:20m:39s remains)
INFO - root - 2017-12-07 13:55:38.675199: step 16130, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 0.768 sec/batch; 67h:29m:49s remains)
INFO - root - 2017-12-07 13:55:45.458682: step 16140, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.642 sec/batch; 56h:26m:43s remains)
INFO - root - 2017-12-07 13:55:52.183149: step 16150, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 56h:22m:53s remains)
INFO - root - 2017-12-07 13:55:59.126981: step 16160, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 62h:23m:24s remains)
INFO - root - 2017-12-07 13:56:05.880223: step 16170, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 63h:49m:00s remains)
INFO - root - 2017-12-07 13:56:12.687312: step 16180, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.699 sec/batch; 61h:22m:58s remains)
INFO - root - 2017-12-07 13:56:19.244080: step 16190, loss = 2.09, batch loss = 2.04 (13.4 examples/sec; 0.596 sec/batch; 52h:19m:41s remains)
INFO - root - 2017-12-07 13:56:25.956678: step 16200, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 57h:47m:25s remains)
2017-12-07 13:56:26.698930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.103334 -4.1076183 -4.1388645 -4.1743517 -4.1952314 -4.1960263 -4.1765637 -4.1492262 -4.1242237 -4.1088777 -4.1006713 -4.1016908 -4.1267066 -4.1569967 -4.174511][-4.10579 -4.1055012 -4.128974 -4.1607876 -4.1814542 -4.184413 -4.1690741 -4.1481709 -4.1311135 -4.1161509 -4.1074681 -4.1093197 -4.1326885 -4.1608338 -4.1773758][-4.1091828 -4.1087503 -4.1231833 -4.1475368 -4.1661444 -4.1676035 -4.1556726 -4.1422 -4.134829 -4.1281133 -4.1255364 -4.1298919 -4.1421585 -4.1595454 -4.172019][-4.1121006 -4.116446 -4.1314044 -4.1507015 -4.1617951 -4.15731 -4.1449833 -4.1308074 -4.1200528 -4.1160522 -4.120079 -4.1288652 -4.1359019 -4.144136 -4.1554379][-4.114274 -4.1244507 -4.1431537 -4.1598105 -4.1658249 -4.152144 -4.1298728 -4.1027884 -4.0887637 -4.0903692 -4.100307 -4.1150279 -4.1243277 -4.1275196 -4.134932][-4.1080332 -4.1235185 -4.150249 -4.1703916 -4.1733074 -4.1462011 -4.0995946 -4.050302 -4.04167 -4.0575457 -4.0758767 -4.0997887 -4.1118426 -4.110733 -4.1103654][-4.0912919 -4.1119514 -4.1468978 -4.1718364 -4.169919 -4.1216373 -4.0426917 -3.9730697 -3.9833748 -4.0277014 -4.0622005 -4.0928707 -4.1067276 -4.1021247 -4.0940866][-4.0847931 -4.1064324 -4.1410117 -4.161643 -4.1492143 -4.0844789 -3.9856551 -3.9130788 -3.946166 -4.0164242 -4.0652528 -4.0992031 -4.1101437 -4.1011086 -4.09046][-4.0898471 -4.1115417 -4.1382294 -4.1484628 -4.1290274 -4.06497 -3.9802828 -3.9280667 -3.9637673 -4.0309868 -4.0789986 -4.10864 -4.1127434 -4.1003571 -4.0887775][-4.1093893 -4.1263 -4.1416354 -4.1421843 -4.1196666 -4.0653648 -4.0100894 -3.9881268 -4.0159993 -4.0636258 -4.101912 -4.1242814 -4.1272683 -4.1173592 -4.1065555][-4.1389623 -4.1498337 -4.1556945 -4.1511207 -4.1307073 -4.0936561 -4.0669684 -4.0684023 -4.0902023 -4.120501 -4.1454597 -4.1577787 -4.1581674 -4.1522436 -4.143661][-4.1658769 -4.1692128 -4.1657953 -4.1583471 -4.1472173 -4.1309676 -4.1249022 -4.1378222 -4.1570225 -4.1784544 -4.1924396 -4.1923432 -4.1873469 -4.1828475 -4.1767306][-4.187005 -4.182755 -4.1731129 -4.1655059 -4.1639423 -4.1623 -4.1669006 -4.1838441 -4.2022862 -4.2187057 -4.2252946 -4.2176752 -4.2121878 -4.213275 -4.213738][-4.2255335 -4.2147346 -4.2035217 -4.1992235 -4.1991591 -4.2009044 -4.2084074 -4.2243724 -4.24123 -4.254519 -4.2582717 -4.2516336 -4.2511034 -4.2556024 -4.2597332][-4.2502789 -4.2382812 -4.2316256 -4.2315745 -4.2325735 -4.2359819 -4.243639 -4.2546992 -4.2665777 -4.2745156 -4.2768297 -4.2754889 -4.2790418 -4.2838407 -4.2864933]]...]
INFO - root - 2017-12-07 13:56:33.411692: step 16210, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 58h:33m:16s remains)
INFO - root - 2017-12-07 13:56:40.236543: step 16220, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 54h:24m:37s remains)
INFO - root - 2017-12-07 13:56:46.960322: step 16230, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 59h:21m:39s remains)
INFO - root - 2017-12-07 13:56:53.806293: step 16240, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 60h:56m:10s remains)
INFO - root - 2017-12-07 13:57:00.627269: step 16250, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 62h:58m:35s remains)
INFO - root - 2017-12-07 13:57:07.336344: step 16260, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 56h:58m:34s remains)
INFO - root - 2017-12-07 13:57:14.162773: step 16270, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 57h:00m:19s remains)
INFO - root - 2017-12-07 13:57:20.870861: step 16280, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 55h:37m:15s remains)
INFO - root - 2017-12-07 13:57:27.557510: step 16290, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 60h:32m:21s remains)
INFO - root - 2017-12-07 13:57:34.308255: step 16300, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 60h:15m:39s remains)
2017-12-07 13:57:34.987162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1598706 -4.1826568 -4.2085123 -4.2335029 -4.2434592 -4.2218595 -4.1901259 -4.181582 -4.20639 -4.2180181 -4.2029529 -4.1898904 -4.183929 -4.1839657 -4.1961994][-4.1511512 -4.1728435 -4.2004371 -4.229229 -4.2465773 -4.2317791 -4.2063947 -4.1991415 -4.2181268 -4.2205563 -4.1962161 -4.1777616 -4.1699495 -4.1751924 -4.1948967][-4.1549025 -4.1720848 -4.192987 -4.2183433 -4.2401366 -4.234097 -4.2189631 -4.2095876 -4.2215185 -4.2168965 -4.1874733 -4.1684933 -4.1633716 -4.173789 -4.1988158][-4.1668706 -4.1827803 -4.1959872 -4.2166624 -4.2430782 -4.2456765 -4.2358832 -4.2222066 -4.2251668 -4.2136374 -4.1826882 -4.1648254 -4.1631536 -4.1760035 -4.2011733][-4.1682906 -4.1856241 -4.1945844 -4.2129192 -4.2419405 -4.2513747 -4.2464738 -4.2274666 -4.2164917 -4.1971269 -4.1691904 -4.1569881 -4.1641612 -4.1835046 -4.2113724][-4.1690865 -4.1847639 -4.1898317 -4.2027221 -4.2286181 -4.2423825 -4.2407856 -4.2150254 -4.1837149 -4.152061 -4.1319532 -4.1375422 -4.1634541 -4.1942711 -4.2237296][-4.183744 -4.1962924 -4.1967793 -4.2004561 -4.2147293 -4.2244687 -4.22309 -4.193006 -4.141026 -4.0999784 -4.0980048 -4.1296935 -4.1739173 -4.2115107 -4.2363553][-4.2005496 -4.2101755 -4.2091746 -4.2081895 -4.2121415 -4.2118578 -4.2066069 -4.1756988 -4.11486 -4.0793481 -4.1055627 -4.161458 -4.2135582 -4.246748 -4.2616735][-4.2077031 -4.2163525 -4.2165756 -4.2120581 -4.2102809 -4.2008977 -4.1895609 -4.163259 -4.1101313 -4.0916491 -4.1434321 -4.2137394 -4.2662182 -4.291502 -4.2940679][-4.2230263 -4.2321892 -4.2341824 -4.2281113 -4.2187834 -4.1980791 -4.1809626 -4.1660757 -4.139791 -4.1417694 -4.2023392 -4.2678695 -4.3090253 -4.3238349 -4.3174539][-4.2476783 -4.2584958 -4.2626514 -4.25159 -4.2306452 -4.1995473 -4.180244 -4.179781 -4.1834521 -4.203835 -4.2611108 -4.3100896 -4.3339357 -4.3397064 -4.3290162][-4.2600946 -4.2727261 -4.2749453 -4.2548137 -4.2223816 -4.1886373 -4.1741724 -4.1899376 -4.2176123 -4.2487359 -4.2988434 -4.3314743 -4.3399649 -4.3389897 -4.3246322][-4.2577553 -4.2681046 -4.2633843 -4.2364349 -4.2031 -4.1798754 -4.1787968 -4.2066851 -4.2421432 -4.2748942 -4.315392 -4.3359728 -4.3343148 -4.3277049 -4.3086371][-4.2481847 -4.256743 -4.2521057 -4.2267027 -4.1970172 -4.18457 -4.1960063 -4.228631 -4.2639718 -4.2949739 -4.3234115 -4.3324671 -4.3250155 -4.3116727 -4.2872796][-4.2382474 -4.2460594 -4.2458534 -4.2265282 -4.2036891 -4.20147 -4.2212563 -4.25152 -4.2802844 -4.3039403 -4.3180246 -4.3152108 -4.3004694 -4.2808137 -4.2563734]]...]
INFO - root - 2017-12-07 13:57:41.742766: step 16310, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 57h:35m:18s remains)
INFO - root - 2017-12-07 13:57:48.472029: step 16320, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 63h:19m:41s remains)
INFO - root - 2017-12-07 13:57:55.250909: step 16330, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 61h:32m:47s remains)
INFO - root - 2017-12-07 13:58:02.046936: step 16340, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 57h:36m:39s remains)
INFO - root - 2017-12-07 13:58:08.811851: step 16350, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 58h:17m:46s remains)
INFO - root - 2017-12-07 13:58:15.417529: step 16360, loss = 2.11, batch loss = 2.06 (11.1 examples/sec; 0.724 sec/batch; 63h:32m:22s remains)
INFO - root - 2017-12-07 13:58:22.262684: step 16370, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 59h:33m:47s remains)
INFO - root - 2017-12-07 13:58:29.022593: step 16380, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 60h:18m:30s remains)
INFO - root - 2017-12-07 13:58:35.535807: step 16390, loss = 2.05, batch loss = 2.00 (13.1 examples/sec; 0.608 sec/batch; 53h:25m:28s remains)
INFO - root - 2017-12-07 13:58:42.331025: step 16400, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 58h:59m:02s remains)
2017-12-07 13:58:43.107442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2840142 -4.2842717 -4.2860913 -4.28784 -4.2890592 -4.2897272 -4.2897453 -4.288909 -4.2874002 -4.2859898 -4.2850623 -4.284411 -4.2837358 -4.2831798 -4.2828927][-4.27311 -4.2734723 -4.2760253 -4.2780437 -4.2789555 -4.2792768 -4.2787719 -4.27723 -4.2749705 -4.2728009 -4.2714858 -4.2706447 -4.269649 -4.2684913 -4.2673569][-4.2626314 -4.2653012 -4.2691092 -4.2714105 -4.2716494 -4.2710757 -4.2698355 -4.2680931 -4.265873 -4.2631726 -4.2614417 -4.2600164 -4.2581525 -4.2556863 -4.2527781][-4.2671928 -4.2718849 -4.2759013 -4.2776608 -4.2768364 -4.2747126 -4.2725363 -4.2708731 -4.2694416 -4.2675152 -4.266263 -4.2652268 -4.2634268 -4.260283 -4.2556853][-4.2739835 -4.2775593 -4.28058 -4.2815719 -4.2798538 -4.2762403 -4.2733665 -4.2716265 -4.2709451 -4.2703476 -4.2700806 -4.2701778 -4.2697482 -4.2679753 -4.26457][-4.2728443 -4.2752061 -4.2761903 -4.27425 -4.2690134 -4.2616343 -4.2558231 -4.2525086 -4.2514081 -4.251358 -4.2520537 -4.2535877 -4.2553792 -4.2564979 -4.2563543][-4.2648082 -4.2645988 -4.26055 -4.2522058 -4.239306 -4.2251325 -4.2143073 -4.2082281 -4.2062774 -4.2067242 -4.2087364 -4.2121224 -4.2165833 -4.2208605 -4.2241368][-4.2532411 -4.2512684 -4.2426062 -4.2268348 -4.205183 -4.183238 -4.1661873 -4.1557665 -4.1507559 -4.1491985 -4.1503754 -4.1541162 -4.1600204 -4.1663885 -4.1724863][-4.2318163 -4.2285032 -4.2181826 -4.2010946 -4.1779575 -4.1539965 -4.1341391 -4.1202822 -4.1127124 -4.109828 -4.1106725 -4.1143484 -4.1202664 -4.1273756 -4.1347666][-4.2248259 -4.2227612 -4.21589 -4.2045097 -4.1882753 -4.1706448 -4.1548557 -4.1432152 -4.1373234 -4.1355968 -4.1370206 -4.1408849 -4.1465983 -4.152832 -4.15868][-4.2507095 -4.2524338 -4.2511396 -4.2469525 -4.2390642 -4.2287207 -4.2186103 -4.2108307 -4.206821 -4.2056012 -4.2067637 -4.2096944 -4.2134905 -4.2165027 -4.218823][-4.28224 -4.2870259 -4.2892265 -4.28929 -4.2860284 -4.279727 -4.272975 -4.2675014 -4.2645092 -4.2628751 -4.2624769 -4.26327 -4.264585 -4.2650447 -4.2651234][-4.2860074 -4.2916408 -4.29635 -4.2997389 -4.3000407 -4.2970228 -4.2930827 -4.289094 -4.2860718 -4.283133 -4.2806206 -4.2790532 -4.2783165 -4.2777905 -4.2781448][-4.26336 -4.2660427 -4.270288 -4.2751784 -4.278615 -4.2799773 -4.2809443 -4.2813363 -4.281354 -4.2801723 -4.2784281 -4.2770114 -4.276319 -4.2759166 -4.276206][-4.2510076 -4.2498636 -4.2514009 -4.2554426 -4.2600546 -4.264411 -4.2700319 -4.2757363 -4.2802281 -4.2822123 -4.2826257 -4.282773 -4.2828588 -4.2822218 -4.2812772]]...]
INFO - root - 2017-12-07 13:58:49.931699: step 16410, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 55h:06m:51s remains)
INFO - root - 2017-12-07 13:58:56.663876: step 16420, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 57h:10m:22s remains)
INFO - root - 2017-12-07 13:59:03.533307: step 16430, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 63h:49m:31s remains)
INFO - root - 2017-12-07 13:59:10.348895: step 16440, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 63h:14m:03s remains)
INFO - root - 2017-12-07 13:59:17.097536: step 16450, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 56h:30m:22s remains)
INFO - root - 2017-12-07 13:59:23.945148: step 16460, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 58h:06m:17s remains)
INFO - root - 2017-12-07 13:59:30.813032: step 16470, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 63h:52m:33s remains)
INFO - root - 2017-12-07 13:59:37.687183: step 16480, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.739 sec/batch; 64h:52m:12s remains)
INFO - root - 2017-12-07 13:59:44.321278: step 16490, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 56h:47m:19s remains)
INFO - root - 2017-12-07 13:59:50.987994: step 16500, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.626 sec/batch; 54h:57m:27s remains)
2017-12-07 13:59:51.731809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3128061 -4.310473 -4.3072033 -4.3008885 -4.293879 -4.285635 -4.2794466 -4.2871027 -4.304903 -4.3183589 -4.3243618 -4.3251505 -4.3180037 -4.3059354 -4.2957997][-4.3047156 -4.2955041 -4.2847013 -4.27329 -4.2614875 -4.2469015 -4.2378531 -4.2563472 -4.2864165 -4.3031836 -4.3083 -4.31063 -4.30332 -4.2864676 -4.2711873][-4.2973862 -4.2802329 -4.2606192 -4.2425141 -4.224895 -4.2034526 -4.1908431 -4.218082 -4.2595687 -4.2799406 -4.2849088 -4.2871251 -4.2812643 -4.264432 -4.2453618][-4.2946382 -4.2721305 -4.2458725 -4.2192307 -4.1912179 -4.1593084 -4.1371479 -4.1623015 -4.2143941 -4.2479234 -4.2599683 -4.2633567 -4.2579412 -4.2423892 -4.2206507][-4.2960095 -4.2728057 -4.2428312 -4.2096424 -4.1699471 -4.1204772 -4.079555 -4.0900512 -4.1529322 -4.2061296 -4.2336073 -4.2414465 -4.2345066 -4.2174087 -4.1948881][-4.2973709 -4.27649 -4.2462463 -4.2083235 -4.1603918 -4.0996275 -4.0344853 -4.01886 -4.0856023 -4.1612291 -4.2047133 -4.21885 -4.2165852 -4.2055707 -4.1900234][-4.2966442 -4.2763863 -4.2471056 -4.2081633 -4.1596084 -4.0958457 -4.0103078 -3.9633708 -4.0237226 -4.1105146 -4.1631479 -4.1811905 -4.1896534 -4.1927576 -4.19529][-4.2934008 -4.2723837 -4.2433848 -4.2071733 -4.1619115 -4.0987945 -4.0106788 -3.9534123 -4.0025868 -4.0784578 -4.1227856 -4.1389871 -4.1506739 -4.1698079 -4.1933236][-4.286561 -4.2640305 -4.2344155 -4.2021704 -4.1673346 -4.11796 -4.0510468 -4.0088482 -4.0465508 -4.092598 -4.1086035 -4.1059728 -4.1098413 -4.139194 -4.180953][-4.275198 -4.2492552 -4.2174416 -4.1861129 -4.1614003 -4.1324754 -4.093533 -4.0711627 -4.0985718 -4.1175466 -4.1014805 -4.0705929 -4.0586162 -4.0905328 -4.1484237][-4.2622795 -4.2314181 -4.1961493 -4.1647515 -4.1461816 -4.1324048 -4.1125484 -4.1039619 -4.1246424 -4.130146 -4.0980129 -4.0508289 -4.02831 -4.0532446 -4.1128798][-4.2539034 -4.217608 -4.1793818 -4.1505747 -4.1369343 -4.1327696 -4.1258225 -4.1244764 -4.1406903 -4.1436071 -4.11306 -4.0723653 -4.0530853 -4.0652461 -4.1080647][-4.2549229 -4.2182579 -4.1803684 -4.1546464 -4.1444521 -4.144722 -4.1469049 -4.1503496 -4.1643934 -4.1717405 -4.1567974 -4.1366177 -4.1271386 -4.1287951 -4.1485715][-4.2671819 -4.2369719 -4.2044191 -4.1820135 -4.1730866 -4.1759582 -4.1847034 -4.1940184 -4.2087712 -4.2208457 -4.2184367 -4.2104621 -4.2053294 -4.203867 -4.2083521][-4.2858744 -4.2659559 -4.242209 -4.2235289 -4.21507 -4.2179585 -4.2289386 -4.2403016 -4.2537379 -4.2654181 -4.2666879 -4.2640939 -4.2633429 -4.2612982 -4.2583046]]...]
INFO - root - 2017-12-07 13:59:58.763857: step 16510, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 63h:57m:41s remains)
INFO - root - 2017-12-07 14:00:05.569145: step 16520, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 57h:40m:25s remains)
INFO - root - 2017-12-07 14:00:12.301649: step 16530, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.617 sec/batch; 54h:11m:39s remains)
INFO - root - 2017-12-07 14:00:19.072782: step 16540, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 55h:28m:03s remains)
INFO - root - 2017-12-07 14:00:25.945030: step 16550, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.727 sec/batch; 63h:46m:35s remains)
INFO - root - 2017-12-07 14:00:32.743723: step 16560, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 58h:15m:14s remains)
INFO - root - 2017-12-07 14:00:39.432920: step 16570, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 57h:55m:12s remains)
INFO - root - 2017-12-07 14:00:46.261387: step 16580, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 56h:10m:28s remains)
INFO - root - 2017-12-07 14:00:52.870902: step 16590, loss = 2.09, batch loss = 2.03 (13.5 examples/sec; 0.591 sec/batch; 51h:52m:39s remains)
INFO - root - 2017-12-07 14:00:59.601342: step 16600, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 61h:23m:43s remains)
2017-12-07 14:01:00.366490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1028962 -4.0705566 -4.087657 -4.0753098 -4.0541687 -4.0857348 -4.1439838 -4.15685 -4.1561313 -4.1542029 -4.1359673 -4.1271544 -4.1409454 -4.1548786 -4.1582055][-4.1237912 -4.0956368 -4.1112452 -4.098753 -4.0794773 -4.108151 -4.1572933 -4.1705112 -4.177351 -4.1755953 -4.1534085 -4.1529965 -4.1733475 -4.1786532 -4.173336][-4.1470609 -4.1249013 -4.1402497 -4.1249652 -4.1079712 -4.132822 -4.167181 -4.1794343 -4.1950879 -4.1944041 -4.1769075 -4.1846442 -4.2062964 -4.1992693 -4.1831818][-4.1587896 -4.1376576 -4.1502643 -4.1342311 -4.117743 -4.1432304 -4.1633453 -4.1693835 -4.1892161 -4.1914344 -4.1818557 -4.1948085 -4.2068634 -4.1850142 -4.1615815][-4.1723847 -4.1431489 -4.1454563 -4.1219897 -4.1022797 -4.1209507 -4.1254416 -4.12525 -4.1510539 -4.1552835 -4.1538725 -4.1770277 -4.1874084 -4.15423 -4.1275983][-4.1698618 -4.1317825 -4.1237326 -4.0919547 -4.0696836 -4.0683436 -4.0368462 -4.0177193 -4.0658774 -4.0956364 -4.1057954 -4.1415954 -4.1678524 -4.145484 -4.1210551][-4.1481738 -4.099597 -4.0766773 -4.0304465 -3.9941027 -3.9617963 -3.8646514 -3.8094113 -3.91785 -4.0161319 -4.0611911 -4.1169281 -4.1679006 -4.1730332 -4.1655655][-4.1343746 -4.0825992 -4.0436115 -3.9845579 -3.9320076 -3.8662136 -3.6973507 -3.5925412 -3.7767115 -3.9571233 -4.0388489 -4.1083045 -4.1810608 -4.2118144 -4.2232256][-4.1229982 -4.0737786 -4.0397868 -3.9843 -3.9353774 -3.8851349 -3.7462685 -3.6537781 -3.8130984 -3.9908454 -4.072742 -4.13055 -4.2026992 -4.2368975 -4.2503371][-4.1056447 -4.0649352 -4.0500369 -4.013483 -3.9877434 -3.9811547 -3.9222698 -3.8771098 -3.9667032 -4.0843153 -4.13719 -4.1715875 -4.2253156 -4.2491093 -4.2551584][-4.0900888 -4.053668 -4.0580897 -4.0472655 -4.0447273 -4.0673227 -4.0565939 -4.0376921 -4.0876231 -4.1561089 -4.1824145 -4.2012734 -4.2351522 -4.2486105 -4.2509422][-4.0873523 -4.0540066 -4.0656223 -4.0741558 -4.089148 -4.1288939 -4.1413488 -4.1317163 -4.16142 -4.1979551 -4.2127295 -4.22436 -4.2411942 -4.2461834 -4.2484832][-4.1005187 -4.0718527 -4.0909986 -4.1104193 -4.1328168 -4.1756926 -4.1989751 -4.1926427 -4.2054172 -4.2223182 -4.229085 -4.2359285 -4.2422466 -4.2388997 -4.2396307][-4.1320672 -4.1178575 -4.1419821 -4.1627669 -4.1800909 -4.2109785 -4.2338347 -4.2266536 -4.2276387 -4.2358313 -4.237565 -4.2366643 -4.2333717 -4.2254953 -4.2278948][-4.18661 -4.1880522 -4.2096276 -4.223897 -4.2341433 -4.2508245 -4.2638583 -4.2555113 -4.2503262 -4.2538838 -4.2512379 -4.24434 -4.2344556 -4.2251148 -4.2279987]]...]
INFO - root - 2017-12-07 14:01:07.134079: step 16610, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 56h:12m:25s remains)
INFO - root - 2017-12-07 14:01:13.946332: step 16620, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 62h:31m:13s remains)
INFO - root - 2017-12-07 14:01:20.730735: step 16630, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 59h:39m:18s remains)
INFO - root - 2017-12-07 14:01:27.526584: step 16640, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 56h:30m:54s remains)
INFO - root - 2017-12-07 14:01:34.305547: step 16650, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.632 sec/batch; 55h:29m:20s remains)
INFO - root - 2017-12-07 14:01:41.148879: step 16660, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 61h:54m:06s remains)
INFO - root - 2017-12-07 14:01:47.771685: step 16670, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 58h:13m:58s remains)
INFO - root - 2017-12-07 14:01:54.566479: step 16680, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 55h:38m:44s remains)
INFO - root - 2017-12-07 14:02:01.346301: step 16690, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 58h:44m:04s remains)
INFO - root - 2017-12-07 14:02:08.248620: step 16700, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.772 sec/batch; 67h:41m:41s remains)
2017-12-07 14:02:08.918895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3552179 -4.3518629 -4.3503609 -4.3494205 -4.3476815 -4.3447003 -4.3431892 -4.3445258 -4.3438334 -4.3404517 -4.3353157 -4.3273425 -4.3141594 -4.2975249 -4.2856188][-4.3536248 -4.3467903 -4.3436933 -4.3435693 -4.3439527 -4.3424416 -4.341907 -4.343792 -4.3434038 -4.3402705 -4.3349042 -4.3269796 -4.3159242 -4.3017988 -4.2893181][-4.3567038 -4.3464613 -4.3402295 -4.3373365 -4.3362136 -4.3342733 -4.3344474 -4.3368196 -4.3360839 -4.3318009 -4.3257666 -4.3190489 -4.3127222 -4.3036652 -4.2910085][-4.363658 -4.3491344 -4.3357825 -4.326262 -4.3211756 -4.3160257 -4.3152547 -4.3187 -4.3179746 -4.3134441 -4.3084788 -4.3039122 -4.3044887 -4.3021603 -4.2911677][-4.3675456 -4.346261 -4.3224983 -4.3017859 -4.2856736 -4.268291 -4.2563763 -4.255619 -4.2563658 -4.254312 -4.2528481 -4.2539167 -4.2676005 -4.2774234 -4.2727532][-4.3517504 -4.3180246 -4.27983 -4.2436194 -4.21113 -4.1694703 -4.1279736 -4.1132355 -4.1222525 -4.1355376 -4.1447935 -4.1565423 -4.1844325 -4.2118673 -4.220161][-4.3088689 -4.2615671 -4.2130175 -4.1671085 -4.119926 -4.0454597 -3.9538996 -3.9157777 -3.9546301 -4.00733 -4.0430503 -4.0732493 -4.1123314 -4.1500907 -4.1680803][-4.2674012 -4.2181311 -4.1732874 -4.135941 -4.0887432 -3.9960227 -3.8705482 -3.8222094 -3.8962047 -3.9821625 -4.0354347 -4.0683827 -4.0997486 -4.1256309 -4.139863][-4.2540956 -4.21309 -4.1779337 -4.1574955 -4.1338711 -4.0675473 -3.9688747 -3.9370105 -4.0021281 -4.0683432 -4.1022282 -4.1128416 -4.1201162 -4.1224537 -4.1248274][-4.2472162 -4.2106361 -4.1815553 -4.1718082 -4.1671019 -4.1339359 -4.074965 -4.0603533 -4.1023097 -4.139533 -4.1541452 -4.1522303 -4.147923 -4.1395278 -4.13789][-4.2479472 -4.2148294 -4.1903095 -4.1865959 -4.1908374 -4.17581 -4.1429777 -4.1343217 -4.1559176 -4.1758623 -4.1829686 -4.182138 -4.1797009 -4.176548 -4.1824841][-4.263731 -4.2357225 -4.2172413 -4.2145829 -4.2191324 -4.2129378 -4.1968231 -4.19286 -4.2058854 -4.2187705 -4.224359 -4.224761 -4.226697 -4.2295079 -4.2394652][-4.2922273 -4.2715945 -4.2584577 -4.2562642 -4.261652 -4.2613888 -4.2587252 -4.261898 -4.2738724 -4.2850885 -4.2889805 -4.2888 -4.2895608 -4.2917542 -4.2980866][-4.3226957 -4.3093867 -4.3022289 -4.3024058 -4.3106937 -4.3165073 -4.3204384 -4.326828 -4.3373389 -4.3465328 -4.3495846 -4.348938 -4.3477645 -4.3468289 -4.3477006][-4.3446813 -4.3379216 -4.3354983 -4.3394871 -4.3492489 -4.3564124 -4.3604155 -4.3639355 -4.3699608 -4.3755565 -4.3783088 -4.3795657 -4.379591 -4.3785439 -4.3777003]]...]
INFO - root - 2017-12-07 14:02:15.744515: step 16710, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.617 sec/batch; 54h:07m:57s remains)
INFO - root - 2017-12-07 14:02:22.574338: step 16720, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 60h:16m:50s remains)
INFO - root - 2017-12-07 14:02:29.366514: step 16730, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 62h:03m:04s remains)
INFO - root - 2017-12-07 14:02:36.091661: step 16740, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 61h:18m:17s remains)
INFO - root - 2017-12-07 14:02:42.996155: step 16750, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 58h:46m:25s remains)
INFO - root - 2017-12-07 14:02:49.762100: step 16760, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 55h:29m:08s remains)
INFO - root - 2017-12-07 14:02:56.508232: step 16770, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 58h:43m:09s remains)
INFO - root - 2017-12-07 14:03:03.387156: step 16780, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 62h:38m:14s remains)
INFO - root - 2017-12-07 14:03:10.105084: step 16790, loss = 2.11, batch loss = 2.05 (11.3 examples/sec; 0.706 sec/batch; 61h:53m:30s remains)
INFO - root - 2017-12-07 14:03:16.783986: step 16800, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 59h:04m:38s remains)
2017-12-07 14:03:17.519167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2752452 -4.2641253 -4.2540231 -4.2448945 -4.2260294 -4.1964374 -4.169723 -4.1656938 -4.1859479 -4.2043948 -4.2149591 -4.2204075 -4.2290807 -4.2358408 -4.2379351][-4.2736254 -4.2609386 -4.2510586 -4.2435989 -4.2180829 -4.1758094 -4.1434627 -4.1439271 -4.1795888 -4.2098322 -4.2242932 -4.226018 -4.2295194 -4.2304626 -4.2301345][-4.2620344 -4.2442265 -4.2369037 -4.2398148 -4.2169504 -4.1678643 -4.1275535 -4.1345668 -4.1827765 -4.2175951 -4.2321391 -4.2327094 -4.2338147 -4.2261238 -4.2211127][-4.2382522 -4.2150316 -4.2137809 -4.22487 -4.2047663 -4.1529036 -4.1082697 -4.123168 -4.1804852 -4.2173433 -4.2263122 -4.2250447 -4.2273135 -4.2157316 -4.2109523][-4.2070985 -4.1790648 -4.1797543 -4.1917572 -4.1703391 -4.1106491 -4.0581088 -4.0758858 -4.1428742 -4.1858439 -4.1906157 -4.1893735 -4.1933889 -4.1841373 -4.1860051][-4.1712155 -4.1371522 -4.1329885 -4.1379948 -4.108356 -4.0263004 -3.9425447 -3.9552407 -4.0491967 -4.1155095 -4.1310058 -4.1392336 -4.1505117 -4.1456332 -4.1520262][-4.144062 -4.1033273 -4.0914311 -4.0851893 -4.0407448 -3.933306 -3.8185451 -3.8320518 -3.959383 -4.0567193 -4.0894318 -4.1080813 -4.126379 -4.1281638 -4.1398969][-4.1361809 -4.0948129 -4.0825338 -4.0661526 -4.0133543 -3.9007821 -3.7909811 -3.8234179 -3.9604955 -4.0611987 -4.0958538 -4.1110706 -4.1301384 -4.1381044 -4.1556325][-4.1615367 -4.1341238 -4.1306744 -4.1140761 -4.0670981 -3.9839678 -3.9217577 -3.9646435 -4.0631447 -4.1301522 -4.1489735 -4.1545758 -4.1653075 -4.1736178 -4.1908484][-4.1950278 -4.1819348 -4.18789 -4.1776986 -4.1447229 -4.0935721 -4.0668044 -4.1011643 -4.1611948 -4.201643 -4.2124672 -4.2134209 -4.2185788 -4.2226791 -4.2343564][-4.2347064 -4.2325292 -4.2444019 -4.2383561 -4.2160611 -4.1809006 -4.1660881 -4.1902208 -4.22791 -4.2580385 -4.2678576 -4.2700372 -4.2729788 -4.2732444 -4.2798429][-4.2723265 -4.2760382 -4.2883358 -4.2839637 -4.2649331 -4.2363095 -4.2234168 -4.2355204 -4.2588305 -4.2846856 -4.297277 -4.3010931 -4.3016267 -4.3011951 -4.3044248][-4.2917337 -4.2987995 -4.3119588 -4.3110123 -4.2971911 -4.2752318 -4.262671 -4.2655978 -4.2802076 -4.30075 -4.3132048 -4.3169155 -4.3153777 -4.31324 -4.3136268][-4.2940812 -4.2996521 -4.311202 -4.3149137 -4.3084712 -4.293602 -4.2837496 -4.2841249 -4.2929792 -4.3072267 -4.3163061 -4.3209615 -4.3205972 -4.3169327 -4.315239][-4.2947168 -4.2959671 -4.3009973 -4.3035536 -4.3011227 -4.2929912 -4.288013 -4.2909484 -4.2975988 -4.30496 -4.3100739 -4.3145504 -4.3157063 -4.3136535 -4.312994]]...]
INFO - root - 2017-12-07 14:03:24.360992: step 16810, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 61h:43m:56s remains)
INFO - root - 2017-12-07 14:03:31.199140: step 16820, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 58h:27m:53s remains)
INFO - root - 2017-12-07 14:03:38.016342: step 16830, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 55h:32m:25s remains)
INFO - root - 2017-12-07 14:03:44.922368: step 16840, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 56h:17m:21s remains)
INFO - root - 2017-12-07 14:03:51.811496: step 16850, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 62h:04m:47s remains)
INFO - root - 2017-12-07 14:03:58.631168: step 16860, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 63h:49m:03s remains)
INFO - root - 2017-12-07 14:04:05.432191: step 16870, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 59h:50m:22s remains)
INFO - root - 2017-12-07 14:04:12.330678: step 16880, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.658 sec/batch; 57h:40m:32s remains)
INFO - root - 2017-12-07 14:04:19.031219: step 16890, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 63h:48m:26s remains)
INFO - root - 2017-12-07 14:04:25.814159: step 16900, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 59h:45m:23s remains)
2017-12-07 14:04:26.560995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2353091 -4.2278185 -4.2326665 -4.2490778 -4.2650208 -4.2720633 -4.2730446 -4.2707357 -4.270638 -4.2686462 -4.2607393 -4.2492309 -4.2439284 -4.2500029 -4.2641382][-4.2945476 -4.2911391 -4.2952919 -4.3065357 -4.3163919 -4.3204532 -4.3215661 -4.3206191 -4.3200269 -4.3162513 -4.3065853 -4.2979383 -4.2975116 -4.3053031 -4.3167424][-4.3274951 -4.3263011 -4.3285112 -4.3337765 -4.3363943 -4.334682 -4.3325086 -4.3294024 -4.3248224 -4.3162818 -4.3030095 -4.2961664 -4.3013783 -4.313828 -4.3259435][-4.3353462 -4.3330932 -4.3327146 -4.3320804 -4.3266563 -4.3176908 -4.3110957 -4.3047509 -4.2949467 -4.2815561 -4.2645864 -4.2564039 -4.262558 -4.2749357 -4.2872009][-4.3270116 -4.321763 -4.3174286 -4.3105545 -4.2944613 -4.273715 -4.2578969 -4.2443733 -4.2289486 -4.2146521 -4.201396 -4.1978164 -4.2031307 -4.2100267 -4.2191815][-4.3102732 -4.2986774 -4.28962 -4.2781839 -4.25335 -4.2201405 -4.1917448 -4.1707 -4.1551852 -4.1462083 -4.1418424 -4.14601 -4.1489143 -4.1473789 -4.1520658][-4.2902622 -4.2720842 -4.2602224 -4.2474933 -4.2183614 -4.1775966 -4.1400566 -4.1163788 -4.1093621 -4.1108079 -4.1141934 -4.1196842 -4.1173096 -4.1109815 -4.11593][-4.2740316 -4.25357 -4.2420831 -4.2289443 -4.1981258 -4.1576028 -4.1224284 -4.1055751 -4.1124158 -4.1249518 -4.1319485 -4.1331654 -4.1262264 -4.1183581 -4.1278605][-4.2659883 -4.2513866 -4.2446141 -4.2319164 -4.201808 -4.1659989 -4.1400394 -4.1350107 -4.1546779 -4.17478 -4.1795292 -4.1745267 -4.16274 -4.1524062 -4.1645107][-4.2619061 -4.2608776 -4.2615938 -4.2498198 -4.2242293 -4.1972766 -4.182961 -4.188118 -4.2128539 -4.2322955 -4.2311168 -4.2175694 -4.1970329 -4.1816692 -4.1953554][-4.2553592 -4.2733817 -4.284761 -4.2781348 -4.259974 -4.2419958 -4.2371697 -4.2489886 -4.2712121 -4.2824612 -4.2705269 -4.2463212 -4.2176824 -4.1978412 -4.2146072][-4.2487631 -4.2827415 -4.3052139 -4.3075757 -4.2985973 -4.287498 -4.288187 -4.3022738 -4.3180108 -4.319963 -4.2997727 -4.2649169 -4.2262774 -4.2020731 -4.2217903][-4.2518129 -4.2898827 -4.3164692 -4.3241773 -4.3230782 -4.318893 -4.325767 -4.3409147 -4.3499646 -4.3475227 -4.3241796 -4.2832217 -4.2352777 -4.2070093 -4.2280536][-4.2633939 -4.293942 -4.3151946 -4.3238363 -4.3279233 -4.3299417 -4.3414636 -4.3578162 -4.363585 -4.3619766 -4.3422236 -4.3018417 -4.2512789 -4.2218952 -4.2422619][-4.2774563 -4.2958913 -4.3074756 -4.3142581 -4.3207068 -4.3276553 -4.3418355 -4.3578691 -4.3625717 -4.3623018 -4.3478169 -4.3150415 -4.2690849 -4.243279 -4.2606015]]...]
INFO - root - 2017-12-07 14:04:33.434826: step 16910, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 62h:10m:30s remains)
INFO - root - 2017-12-07 14:04:40.328912: step 16920, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 64h:41m:28s remains)
INFO - root - 2017-12-07 14:04:47.097592: step 16930, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 58h:29m:32s remains)
INFO - root - 2017-12-07 14:04:53.873849: step 16940, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 59h:34m:38s remains)
INFO - root - 2017-12-07 14:05:00.576040: step 16950, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 56h:42m:51s remains)
INFO - root - 2017-12-07 14:05:07.317414: step 16960, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 59h:46m:30s remains)
INFO - root - 2017-12-07 14:05:14.079898: step 16970, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 60h:07m:52s remains)
INFO - root - 2017-12-07 14:05:20.784077: step 16980, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 59h:12m:39s remains)
INFO - root - 2017-12-07 14:05:27.429684: step 16990, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 55h:47m:21s remains)
INFO - root - 2017-12-07 14:05:34.179482: step 17000, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 61h:41m:42s remains)
2017-12-07 14:05:34.972568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1234426 -4.1474171 -4.1634479 -4.1570053 -4.1399016 -4.1307383 -4.1283569 -4.1329556 -4.1426167 -4.1566949 -4.1681924 -4.1785517 -4.1761842 -4.1629725 -4.156919][-4.1548171 -4.1777091 -4.191288 -4.1757097 -4.1440988 -4.1256032 -4.1223292 -4.1280432 -4.1415291 -4.1599588 -4.1757812 -4.191051 -4.1931791 -4.1816282 -4.1725035][-4.1632791 -4.1785984 -4.188921 -4.1702275 -4.1349211 -4.1161909 -4.1215281 -4.1355705 -4.1526151 -4.1719823 -4.1891966 -4.2058949 -4.2103052 -4.2011967 -4.1916256][-4.1294894 -4.1400356 -4.1490107 -4.1343503 -4.1048317 -4.0922112 -4.1091304 -4.1341157 -4.1528721 -4.1731787 -4.1916866 -4.2065973 -4.2117224 -4.2077847 -4.2023358][-4.07652 -4.0819912 -4.0881796 -4.0754304 -4.0505853 -4.0403285 -4.0627074 -4.0957832 -4.1149187 -4.1397057 -4.1634703 -4.1803017 -4.187695 -4.1919093 -4.1952152][-4.0240583 -4.0244741 -4.0219784 -4.00542 -3.9836333 -3.9697957 -3.9868832 -4.0232911 -4.0458274 -4.0777268 -4.1111393 -4.1350484 -4.1471229 -4.1596336 -4.1713891][-3.9910479 -3.9843094 -3.967097 -3.9421344 -3.920259 -3.8985333 -3.9014173 -3.9333465 -3.9604945 -4.0011849 -4.0503497 -4.0859661 -4.10683 -4.1291451 -4.1492567][-3.9943402 -3.975904 -3.9424684 -3.9069219 -3.8810039 -3.8528426 -3.8418794 -3.8645747 -3.8925433 -3.9430642 -4.0094404 -4.0561485 -4.0843959 -4.1152668 -4.1441393][-4.0311785 -4.0067153 -3.9679291 -3.9282711 -3.8969615 -3.8685713 -3.8564062 -3.8713398 -3.8929863 -3.9436579 -4.0151081 -4.0640984 -4.0938387 -4.1272974 -4.1589494][-4.0796432 -4.0511265 -4.0138659 -3.9770727 -3.9468768 -3.9244416 -3.9180937 -3.9298546 -3.9470608 -3.9920413 -4.0569263 -4.1033545 -4.132895 -4.165163 -4.1931539][-4.134655 -4.1065259 -4.0761452 -4.0475736 -4.0234733 -4.0084138 -4.0087018 -4.020143 -4.0320969 -4.0641441 -4.1130743 -4.1500196 -4.1753874 -4.2023354 -4.2227144][-4.1856523 -4.163825 -4.1465797 -4.1313257 -4.1159325 -4.1052933 -4.1104889 -4.1220913 -4.1280723 -4.1459 -4.1751437 -4.1973457 -4.2134452 -4.2305679 -4.2414384][-4.2187753 -4.205853 -4.201427 -4.200357 -4.1947856 -4.1886554 -4.1957293 -4.2034078 -4.2030497 -4.209363 -4.2219715 -4.2302723 -4.2367125 -4.245111 -4.24828][-4.23547 -4.226932 -4.2288203 -4.2363715 -4.2383704 -4.2370505 -4.2434506 -4.2467833 -4.2425432 -4.2414007 -4.24321 -4.2426376 -4.2439847 -4.2484722 -4.2493081][-4.2456255 -4.2397709 -4.2420912 -4.2504163 -4.2545633 -4.2559986 -4.260324 -4.2599173 -4.2546363 -4.2499008 -4.2456555 -4.2422271 -4.2434154 -4.2479091 -4.2501063]]...]
INFO - root - 2017-12-07 14:05:41.756845: step 17010, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 56h:54m:06s remains)
INFO - root - 2017-12-07 14:05:48.520144: step 17020, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 55h:28m:09s remains)
INFO - root - 2017-12-07 14:05:55.346856: step 17030, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 60h:48m:24s remains)
INFO - root - 2017-12-07 14:06:02.292184: step 17040, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 61h:34m:04s remains)
INFO - root - 2017-12-07 14:06:09.158331: step 17050, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 60h:02m:43s remains)
INFO - root - 2017-12-07 14:06:15.936305: step 17060, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 55h:58m:00s remains)
INFO - root - 2017-12-07 14:06:22.755647: step 17070, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 56h:36m:26s remains)
INFO - root - 2017-12-07 14:06:29.604958: step 17080, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.730 sec/batch; 63h:58m:46s remains)
INFO - root - 2017-12-07 14:06:36.215374: step 17090, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 57h:44m:55s remains)
INFO - root - 2017-12-07 14:06:42.987746: step 17100, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 58h:32m:49s remains)
2017-12-07 14:06:43.836848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546549 -4.255528 -4.2617836 -4.2642455 -4.2666421 -4.2712626 -4.2757235 -4.2833667 -4.2920871 -4.2942429 -4.2876806 -4.2807546 -4.27886 -4.2828746 -4.28892][-4.2467346 -4.2422881 -4.2424054 -4.2408481 -4.2407103 -4.2460203 -4.2520952 -4.264801 -4.2813148 -4.2909117 -4.2894039 -4.2843356 -4.2809687 -4.2843137 -4.2915573][-4.2251534 -4.2158189 -4.2107577 -4.1991568 -4.1869769 -4.185565 -4.1895967 -4.20731 -4.2368946 -4.2591772 -4.2675266 -4.2716451 -4.2697248 -4.2712946 -4.2777162][-4.2098742 -4.2050939 -4.1983476 -4.1766052 -4.1458206 -4.1234484 -4.1100903 -4.12505 -4.16916 -4.2071505 -4.2289176 -4.245841 -4.2501421 -4.2505322 -4.2509308][-4.2053409 -4.2084551 -4.2028041 -4.1769285 -4.1273861 -4.0706339 -4.025063 -4.0269651 -4.0819921 -4.1375661 -4.1762171 -4.20578 -4.211864 -4.2061334 -4.1934118][-4.194459 -4.204742 -4.2029285 -4.1766515 -4.1105337 -4.0171332 -3.9287372 -3.909404 -3.9814684 -4.0650573 -4.1216125 -4.1590915 -4.1623392 -4.1462317 -4.125042][-4.1782503 -4.1903262 -4.189117 -4.1624689 -4.0961628 -3.9877014 -3.864325 -3.8108459 -3.8925509 -4.0129766 -4.0936079 -4.1375303 -4.1376314 -4.1145444 -4.0911613][-4.1659365 -4.1778269 -4.1804304 -4.1569848 -4.1013513 -4.0062046 -3.8795202 -3.7962668 -3.8644419 -4.0049605 -4.0967717 -4.1462183 -4.1518235 -4.1257391 -4.0985532][-4.1752348 -4.1898222 -4.1948071 -4.1751795 -4.1393042 -4.0782824 -3.9850242 -3.8982155 -3.9281607 -4.0478005 -4.1301413 -4.1693573 -4.1708689 -4.143023 -4.1136665][-4.1977606 -4.2107658 -4.2173886 -4.2078719 -4.1925278 -4.1635051 -4.1153164 -4.0555906 -4.0538106 -4.1227093 -4.179297 -4.1991544 -4.1838918 -4.1480169 -4.1165109][-4.2140551 -4.223083 -4.2267065 -4.2225308 -4.2186551 -4.2091303 -4.1912131 -4.1571817 -4.1499939 -4.1887321 -4.2189593 -4.2216716 -4.1946626 -4.149981 -4.1152854][-4.211432 -4.21702 -4.2190208 -4.2156286 -4.2170935 -4.2196689 -4.2215748 -4.204864 -4.2020626 -4.2282424 -4.2459879 -4.2456956 -4.218709 -4.173245 -4.1404486][-4.2004509 -4.2038856 -4.20811 -4.2111425 -4.2189751 -4.2306223 -4.2437258 -4.2394562 -4.2338486 -4.2469535 -4.257401 -4.2589984 -4.244319 -4.2137361 -4.1892557][-4.2009692 -4.2068148 -4.2226057 -4.233768 -4.2403817 -4.2516627 -4.2643189 -4.2643242 -4.2560158 -4.2612653 -4.2690687 -4.2701025 -4.2623258 -4.2447472 -4.2278419][-4.2135258 -4.2211876 -4.2433434 -4.2607722 -4.2676425 -4.2731123 -4.28181 -4.2803063 -4.272377 -4.274889 -4.2805748 -4.2772374 -4.26856 -4.2564635 -4.2480373]]...]
INFO - root - 2017-12-07 14:06:50.642851: step 17110, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.749 sec/batch; 65h:38m:30s remains)
INFO - root - 2017-12-07 14:06:57.362683: step 17120, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 57h:50m:25s remains)
INFO - root - 2017-12-07 14:07:04.108964: step 17130, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 54h:17m:49s remains)
INFO - root - 2017-12-07 14:07:10.959694: step 17140, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 56h:57m:47s remains)
INFO - root - 2017-12-07 14:07:17.763161: step 17150, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 62h:01m:56s remains)
INFO - root - 2017-12-07 14:07:24.627215: step 17160, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 63h:12m:11s remains)
INFO - root - 2017-12-07 14:07:31.458260: step 17170, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 59h:56m:29s remains)
INFO - root - 2017-12-07 14:07:38.143134: step 17180, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.620 sec/batch; 54h:15m:43s remains)
INFO - root - 2017-12-07 14:07:44.838418: step 17190, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 57h:06m:29s remains)
INFO - root - 2017-12-07 14:07:51.597097: step 17200, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 61h:26m:24s remains)
2017-12-07 14:07:52.295501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.215673 -4.2162886 -4.2134614 -4.2064042 -4.1948929 -4.1827259 -4.1779637 -4.1837492 -4.1993403 -4.2175288 -4.2309141 -4.2368822 -4.2424564 -4.2471423 -4.2518206][-4.2190742 -4.2189827 -4.2136226 -4.2007513 -4.1815171 -4.1602621 -4.1486459 -4.1547971 -4.1785622 -4.2090988 -4.2342458 -4.246758 -4.2537341 -4.2565827 -4.2581949][-4.2185454 -4.2181888 -4.2112365 -4.1933894 -4.1663389 -4.1346817 -4.1117077 -4.1134272 -4.1433005 -4.1870661 -4.2259212 -4.2474823 -4.2581906 -4.2614808 -4.261477][-4.2172132 -4.217474 -4.2113805 -4.19167 -4.1596513 -4.1192479 -4.082737 -4.0734849 -4.1030951 -4.1578522 -4.2100506 -4.2414432 -4.2570362 -4.26255 -4.26298][-4.2154613 -4.2165213 -4.2123432 -4.1930776 -4.1587248 -4.1123719 -4.0637159 -4.0400696 -4.0643148 -4.1272612 -4.1919541 -4.2332444 -4.2537045 -4.261519 -4.2630386][-4.21253 -4.2141418 -4.211535 -4.193388 -4.1593351 -4.1120477 -4.0572448 -4.0211706 -4.0373888 -4.1046596 -4.1787076 -4.2280836 -4.2520957 -4.2606626 -4.2625041][-4.2101116 -4.2125759 -4.2111 -4.1950622 -4.1641335 -4.120945 -4.0671978 -4.0234313 -4.0284209 -4.0923448 -4.1706762 -4.2261906 -4.2531338 -4.261416 -4.2622585][-4.2152748 -4.21622 -4.2130547 -4.1977363 -4.169929 -4.1314516 -4.083437 -4.0391493 -4.0336108 -4.0874109 -4.1641316 -4.2234612 -4.2538657 -4.2627592 -4.262682][-4.225399 -4.2237697 -4.2174077 -4.2025332 -4.1780472 -4.1428204 -4.0998225 -4.0595121 -4.0481939 -4.0896769 -4.1586647 -4.2181678 -4.2518225 -4.2629843 -4.2631345][-4.2328877 -4.2305589 -4.2226415 -4.2099128 -4.190484 -4.1590686 -4.1186156 -4.0816007 -4.0675449 -4.0962725 -4.1516047 -4.2065406 -4.2436423 -4.2592239 -4.2615542][-4.2331986 -4.2326694 -4.2272396 -4.2198091 -4.2081022 -4.183702 -4.1459231 -4.1083522 -4.088316 -4.1030235 -4.1429205 -4.1900516 -4.2296715 -4.2510452 -4.2576184][-4.2264109 -4.2288876 -4.2282395 -4.2279434 -4.2253375 -4.2112408 -4.1796479 -4.1408839 -4.1125383 -4.1125164 -4.13887 -4.1773534 -4.2161427 -4.2417164 -4.2526541][-4.2133412 -4.219202 -4.223331 -4.2292662 -4.23428 -4.2309 -4.209445 -4.17508 -4.1426888 -4.1314 -4.1461644 -4.1746359 -4.2080069 -4.2337284 -4.2479544][-4.1994448 -4.2082438 -4.2152505 -4.2240868 -4.233314 -4.2387347 -4.2295885 -4.20551 -4.1767817 -4.1614442 -4.16776 -4.1854544 -4.2099328 -4.2313881 -4.2459459][-4.1964335 -4.2054973 -4.2119565 -4.2196479 -4.2287092 -4.2383614 -4.2396116 -4.227376 -4.207787 -4.19491 -4.1966023 -4.2053852 -4.2205405 -4.2358589 -4.2473192]]...]
INFO - root - 2017-12-07 14:07:59.078318: step 17210, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 55h:57m:17s remains)
INFO - root - 2017-12-07 14:08:05.860832: step 17220, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 59h:47m:34s remains)
INFO - root - 2017-12-07 14:08:12.771713: step 17230, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 62h:49m:56s remains)
INFO - root - 2017-12-07 14:08:19.628314: step 17240, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 62h:19m:22s remains)
INFO - root - 2017-12-07 14:08:26.375606: step 17250, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 56h:24m:57s remains)
INFO - root - 2017-12-07 14:08:33.061928: step 17260, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 56h:39m:32s remains)
INFO - root - 2017-12-07 14:08:39.943008: step 17270, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 63h:43m:39s remains)
INFO - root - 2017-12-07 14:08:46.776603: step 17280, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.774 sec/batch; 67h:44m:40s remains)
INFO - root - 2017-12-07 14:08:53.163558: step 17290, loss = 2.08, batch loss = 2.02 (13.3 examples/sec; 0.602 sec/batch; 52h:42m:26s remains)
INFO - root - 2017-12-07 14:08:59.746534: step 17300, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 55h:00m:34s remains)
2017-12-07 14:09:00.470125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2527561 -4.2477465 -4.2454877 -4.2392726 -4.2265592 -4.2250223 -4.2367792 -4.2523589 -4.26282 -4.2705264 -4.2775636 -4.2755656 -4.2737017 -4.2789927 -4.2775044][-4.2674723 -4.2562432 -4.2488942 -4.2416263 -4.2301612 -4.228457 -4.240705 -4.2553453 -4.2602329 -4.2602983 -4.2609043 -4.251358 -4.2423596 -4.2441845 -4.2409158][-4.2761364 -4.2620583 -4.2512836 -4.241796 -4.2273879 -4.2190118 -4.22724 -4.2427459 -4.2490482 -4.2453613 -4.2400436 -4.2242155 -4.2088547 -4.2048535 -4.198894][-4.2729969 -4.2589207 -4.2444572 -4.23022 -4.2121429 -4.19674 -4.1975627 -4.2131581 -4.2247038 -4.226193 -4.2209868 -4.2031965 -4.1851797 -4.1774282 -4.1698284][-4.2587652 -4.2473631 -4.2316985 -4.2155318 -4.1985788 -4.1804242 -4.1741505 -4.1871634 -4.2008953 -4.2061043 -4.2034779 -4.1871619 -4.1705418 -4.160759 -4.1499758][-4.2287378 -4.2148023 -4.1952868 -4.176528 -4.1648927 -4.1475692 -4.1394534 -4.1510258 -4.1690564 -4.1792827 -4.1829429 -4.1750565 -4.1660414 -4.1592908 -4.1492581][-4.2013292 -4.1799455 -4.1471477 -4.1166568 -4.1004939 -4.0824804 -4.0752888 -4.0871911 -4.1122289 -4.1296554 -4.1434655 -4.1487069 -4.15461 -4.1585927 -4.1576443][-4.1900792 -4.1598883 -4.1145058 -4.0678349 -4.0391288 -4.0185251 -4.0093079 -4.01812 -4.045403 -4.0750608 -4.1056886 -4.1300936 -4.1559138 -4.1757793 -4.1898446][-4.1843953 -4.1477404 -4.0956945 -4.03974 -4.0040503 -3.983737 -3.9761539 -3.9804161 -4.0038004 -4.0432935 -4.0905018 -4.1339035 -4.17691 -4.209969 -4.2358708][-4.1863828 -4.1543908 -4.1120405 -4.0635033 -4.0331249 -4.0174203 -4.0161386 -4.0189319 -4.0315852 -4.0604115 -4.1038084 -4.1496015 -4.1971169 -4.2345657 -4.2619629][-4.1882763 -4.1694212 -4.14514 -4.1166468 -4.0983105 -4.0904145 -4.0927353 -4.0970716 -4.1035771 -4.1167221 -4.1430016 -4.1755238 -4.2127943 -4.2425981 -4.26332][-4.1897187 -4.1824546 -4.1709614 -4.1590505 -4.1563053 -4.1563005 -4.1589642 -4.1658554 -4.1698618 -4.1715331 -4.1796002 -4.1930261 -4.2110515 -4.2274542 -4.2378263][-4.1740932 -4.1766553 -4.1784987 -4.1788 -4.186491 -4.1942863 -4.2011404 -4.2121692 -4.2184763 -4.2158451 -4.2104383 -4.2065539 -4.2070775 -4.2095852 -4.2082162][-4.1395283 -4.1528974 -4.1701293 -4.1802645 -4.1909046 -4.2043891 -4.2174168 -4.2317467 -4.239192 -4.2370877 -4.2267222 -4.214685 -4.2049608 -4.1970978 -4.1854696][-4.0978503 -4.1187191 -4.1442819 -4.1583543 -4.1689005 -4.18171 -4.197392 -4.2135186 -4.2225432 -4.2221508 -4.2121716 -4.1979094 -4.1845727 -4.1717663 -4.1564708]]...]
INFO - root - 2017-12-07 14:09:07.210195: step 17310, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 0.764 sec/batch; 66h:51m:44s remains)
INFO - root - 2017-12-07 14:09:14.050986: step 17320, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 59h:12m:41s remains)
INFO - root - 2017-12-07 14:09:20.726390: step 17330, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 55h:14m:29s remains)
INFO - root - 2017-12-07 14:09:27.409399: step 17340, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 57h:23m:56s remains)
INFO - root - 2017-12-07 14:09:34.213770: step 17350, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 61h:34m:29s remains)
INFO - root - 2017-12-07 14:09:40.964418: step 17360, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 61h:57m:13s remains)
INFO - root - 2017-12-07 14:09:47.775786: step 17370, loss = 2.02, batch loss = 1.96 (11.1 examples/sec; 0.721 sec/batch; 63h:05m:57s remains)
INFO - root - 2017-12-07 14:09:54.489088: step 17380, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 54h:22m:51s remains)
INFO - root - 2017-12-07 14:10:01.189584: step 17390, loss = 2.07, batch loss = 2.01 (14.1 examples/sec; 0.566 sec/batch; 49h:33m:27s remains)
INFO - root - 2017-12-07 14:10:07.903930: step 17400, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 62h:37m:48s remains)
2017-12-07 14:10:08.760569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1803379 -4.1814861 -4.1836915 -4.1815362 -4.1806674 -4.1943111 -4.2219548 -4.2356448 -4.2452626 -4.2613235 -4.2690654 -4.2699733 -4.2761374 -4.2836704 -4.2859516][-4.1875858 -4.184938 -4.1847305 -4.1807384 -4.1818714 -4.1959629 -4.2268057 -4.244658 -4.256423 -4.2696214 -4.27452 -4.2762451 -4.2821178 -4.2878084 -4.29198][-4.2089276 -4.202311 -4.2019749 -4.1979122 -4.1991367 -4.2080522 -4.2308207 -4.2496276 -4.2620254 -4.2709684 -4.2735586 -4.2761455 -4.2825165 -4.287847 -4.2956347][-4.2612596 -4.2511411 -4.2439084 -4.2368927 -4.2332597 -4.2322731 -4.2430573 -4.2551031 -4.2613893 -4.2675767 -4.2697363 -4.2718272 -4.2751017 -4.2779245 -4.2854543][-4.3076839 -4.29209 -4.2798653 -4.273201 -4.2622132 -4.2439904 -4.2358556 -4.2352643 -4.2382221 -4.2504344 -4.2577233 -4.25822 -4.2519259 -4.2490091 -4.2512078][-4.3190355 -4.2981882 -4.2834654 -4.2737927 -4.2494812 -4.2087111 -4.172966 -4.1581664 -4.1735206 -4.2102838 -4.2298584 -4.2251019 -4.2093525 -4.1983213 -4.1963243][-4.3018026 -4.2767396 -4.257987 -4.239327 -4.1998715 -4.1325679 -4.0555344 -4.0183854 -4.0634818 -4.1416864 -4.180697 -4.1743169 -4.1462879 -4.1275864 -4.1372633][-4.2639475 -4.2367735 -4.213151 -4.1816587 -4.12378 -4.02532 -3.8873434 -3.8260813 -3.9319777 -4.0666485 -4.1247444 -4.1220036 -4.08865 -4.0669146 -4.0950413][-4.2271209 -4.199914 -4.172296 -4.1286478 -4.0601921 -3.9420853 -3.7621293 -3.6969895 -3.8614488 -4.028429 -4.0953922 -4.0887794 -4.0578327 -4.0473375 -4.092854][-4.2242794 -4.2049956 -4.1836576 -4.1450644 -4.090755 -3.9948688 -3.8502846 -3.8118064 -3.949645 -4.0780721 -4.1211109 -4.0975876 -4.0678911 -4.0765438 -4.1304355][-4.2431855 -4.23467 -4.2225966 -4.1992579 -4.1673026 -4.1107683 -4.0328436 -4.0213594 -4.09905 -4.1679354 -4.1791821 -4.139946 -4.1096115 -4.1263623 -4.1769266][-4.2518845 -4.2463536 -4.2430553 -4.2368064 -4.2254457 -4.2022219 -4.1746616 -4.1747909 -4.2084079 -4.2373357 -4.2290916 -4.191164 -4.16566 -4.1773033 -4.2071133][-4.2525535 -4.2464437 -4.2474089 -4.2526374 -4.2571154 -4.2575021 -4.2538457 -4.2532334 -4.2633319 -4.2712173 -4.2580037 -4.2327862 -4.2172937 -4.2248321 -4.2356167][-4.2492762 -4.2454076 -4.2479773 -4.2518525 -4.2601862 -4.2725587 -4.2823591 -4.2858024 -4.2874861 -4.2853851 -4.2766676 -4.2674952 -4.2635703 -4.2662687 -4.2663431][-4.2582207 -4.2598462 -4.2595034 -4.2578163 -4.2622948 -4.2789087 -4.2987733 -4.308392 -4.3056612 -4.2995906 -4.2971377 -4.2967658 -4.2972775 -4.2950644 -4.2905555]]...]
INFO - root - 2017-12-07 14:10:15.541380: step 17410, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 56h:52m:37s remains)
INFO - root - 2017-12-07 14:10:22.382939: step 17420, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 63h:32m:31s remains)
INFO - root - 2017-12-07 14:10:29.202722: step 17430, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.760 sec/batch; 66h:31m:48s remains)
INFO - root - 2017-12-07 14:10:35.929952: step 17440, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 57h:49m:03s remains)
INFO - root - 2017-12-07 14:10:42.607903: step 17450, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 55h:08m:37s remains)
INFO - root - 2017-12-07 14:10:49.356258: step 17460, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 60h:13m:58s remains)
INFO - root - 2017-12-07 14:10:56.121812: step 17470, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.717 sec/batch; 62h:42m:26s remains)
INFO - root - 2017-12-07 14:11:02.858264: step 17480, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 60h:36m:45s remains)
INFO - root - 2017-12-07 14:11:09.499952: step 17490, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 0.550 sec/batch; 48h:05m:08s remains)
INFO - root - 2017-12-07 14:11:16.193912: step 17500, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.620 sec/batch; 54h:17m:08s remains)
2017-12-07 14:11:16.929565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.201951 -4.2048483 -4.20823 -4.214725 -4.2276282 -4.2358518 -4.2343545 -4.2368774 -4.2400541 -4.2411509 -4.2411919 -4.2417841 -4.2323241 -4.2188869 -4.2171187][-4.1668072 -4.1773667 -4.1857905 -4.1910939 -4.1992965 -4.2005615 -4.1930332 -4.1912327 -4.1953788 -4.20032 -4.2035084 -4.2039018 -4.1932206 -4.1850863 -4.192874][-4.145844 -4.1649952 -4.1816649 -4.1925435 -4.1931281 -4.1801162 -4.1558542 -4.1426067 -4.1493378 -4.1627221 -4.1690841 -4.1625972 -4.1494203 -4.1473975 -4.1663394][-4.1176419 -4.148828 -4.1825638 -4.2000909 -4.1890492 -4.1565661 -4.109705 -4.0853786 -4.1032786 -4.1310873 -4.1379023 -4.1238909 -4.105144 -4.100616 -4.1262751][-4.0903463 -4.1324544 -4.1803379 -4.1992431 -4.1729732 -4.1140041 -4.0365458 -4.0038462 -4.0480113 -4.1058741 -4.1224427 -4.1068554 -4.080461 -4.0662885 -4.09488][-4.0850606 -4.12599 -4.1730833 -4.1864343 -4.1421204 -4.045938 -3.9183638 -3.8792162 -3.9727545 -4.07288 -4.11064 -4.103508 -4.0713148 -4.0441351 -4.0748267][-4.1037331 -4.1370921 -4.1744041 -4.17707 -4.111268 -3.9624953 -3.7658503 -3.73312 -3.8974724 -4.0427074 -4.1072264 -4.1107912 -4.0724583 -4.0310907 -4.0615525][-4.1308117 -4.1581931 -4.1813045 -4.1668386 -4.0769973 -3.8834167 -3.6369665 -3.6327715 -3.856029 -4.0294766 -4.1058636 -4.1154623 -4.0692759 -4.021729 -4.0578632][-4.1579256 -4.1796494 -4.1902056 -4.1645937 -4.0723052 -3.8911562 -3.6878676 -3.7153769 -3.9101169 -4.060164 -4.1245422 -4.1243448 -4.0658584 -4.0214958 -4.0672312][-4.1690979 -4.1881351 -4.1952391 -4.1671667 -4.0944257 -3.9742262 -3.8701019 -3.9088845 -4.0230985 -4.1185904 -4.16151 -4.1444879 -4.0747013 -4.0341091 -4.0800896][-4.1597095 -4.1820974 -4.1881328 -4.1586075 -4.1095476 -4.0494833 -4.0178871 -4.0564466 -4.1156483 -4.171092 -4.1896544 -4.1572361 -4.0839686 -4.0474691 -4.0889997][-4.1443777 -4.1693697 -4.17416 -4.1472216 -4.1191854 -4.104362 -4.1129375 -4.1451607 -4.1714964 -4.1954007 -4.19259 -4.152894 -4.0863008 -4.0567155 -4.0943589][-4.1373491 -4.1652207 -4.1704574 -4.1526551 -4.1425147 -4.1518059 -4.1749392 -4.1983218 -4.2020521 -4.1967015 -4.1785889 -4.1433482 -4.0933075 -4.0719509 -4.1108174][-4.1579051 -4.1850147 -4.1893816 -4.1803718 -4.1817112 -4.1994672 -4.2207379 -4.2333684 -4.2208729 -4.1959066 -4.1744776 -4.1515856 -4.1186733 -4.1069593 -4.1469579][-4.2043123 -4.2276897 -4.2351089 -4.2312293 -4.238152 -4.254056 -4.2665443 -4.2674885 -4.2449813 -4.2161984 -4.202157 -4.1945038 -4.1772332 -4.1725025 -4.2063723]]...]
INFO - root - 2017-12-07 14:11:23.701741: step 17510, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 58h:50m:06s remains)
INFO - root - 2017-12-07 14:11:30.485517: step 17520, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 56h:34m:55s remains)
INFO - root - 2017-12-07 14:11:37.289463: step 17530, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 55h:18m:56s remains)
INFO - root - 2017-12-07 14:11:44.128634: step 17540, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 59h:54m:32s remains)
INFO - root - 2017-12-07 14:11:51.117005: step 17550, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.751 sec/batch; 65h:43m:18s remains)
INFO - root - 2017-12-07 14:11:57.930462: step 17560, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 60h:16m:46s remains)
INFO - root - 2017-12-07 14:12:04.711232: step 17570, loss = 2.08, batch loss = 2.03 (12.8 examples/sec; 0.627 sec/batch; 54h:50m:45s remains)
INFO - root - 2017-12-07 14:12:11.523983: step 17580, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 56h:37m:34s remains)
INFO - root - 2017-12-07 14:12:18.308557: step 17590, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.604 sec/batch; 52h:51m:04s remains)
INFO - root - 2017-12-07 14:12:24.878418: step 17600, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 59h:54m:42s remains)
2017-12-07 14:12:25.574571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2812319 -4.28574 -4.2896109 -4.2849665 -4.2704511 -4.252996 -4.2396207 -4.2374339 -4.2511296 -4.2643342 -4.2701659 -4.2696042 -4.2651749 -4.265522 -4.2797313][-4.2697005 -4.2793288 -4.2830124 -4.2706885 -4.2438331 -4.2157083 -4.2003202 -4.2049146 -4.229394 -4.2483029 -4.2523465 -4.2448277 -4.2307773 -4.2259307 -4.2443652][-4.2689724 -4.2788963 -4.2761807 -4.2539372 -4.2123857 -4.1686792 -4.1517863 -4.1700039 -4.2085905 -4.2354851 -4.2368965 -4.2197924 -4.1945934 -4.1844115 -4.206871][-4.2676144 -4.2735095 -4.2627783 -4.2345514 -4.1807189 -4.121572 -4.1004462 -4.1355085 -4.1935186 -4.2256341 -4.22351 -4.1971593 -4.1622553 -4.1471372 -4.1731696][-4.2502675 -4.2500825 -4.2336621 -4.2017078 -4.1400714 -4.0687237 -4.0451288 -4.0983038 -4.1772704 -4.2171645 -4.2146106 -4.1843853 -4.1447463 -4.1263103 -4.1520891][-4.2364392 -4.2280006 -4.2054138 -4.1668296 -4.0969095 -4.0162454 -3.9918425 -4.0629697 -4.1608582 -4.21178 -4.2120862 -4.1808376 -4.1430955 -4.1257949 -4.1486359][-4.2386594 -4.2203727 -4.1897664 -4.1407089 -4.0595684 -3.964752 -3.9410844 -4.0282984 -4.1422696 -4.2045412 -4.2094793 -4.1808643 -4.1473384 -4.134109 -4.1547265][-4.2462735 -4.2246795 -4.1928091 -4.1407256 -4.0500154 -3.9418564 -3.9183941 -4.01302 -4.135705 -4.2033706 -4.2112317 -4.1862054 -4.1559157 -4.1455832 -4.1641669][-4.2504945 -4.2327366 -4.2053623 -4.1609592 -4.0736156 -3.9687951 -3.9473102 -4.0370126 -4.1515203 -4.21237 -4.2203121 -4.1981454 -4.1694098 -4.1577525 -4.1741238][-4.2485609 -4.2342596 -4.2150869 -4.1823034 -4.101872 -4.008523 -3.9949787 -4.0747719 -4.1713843 -4.2218847 -4.2261968 -4.2062793 -4.1796565 -4.1686282 -4.1834512][-4.2512445 -4.2399821 -4.2264905 -4.1980448 -4.1210637 -4.0403466 -4.0334349 -4.1009941 -4.1805778 -4.222301 -4.2247567 -4.2062545 -4.1826649 -4.1759233 -4.1932311][-4.2549458 -4.2477345 -4.2406359 -4.21483 -4.1463008 -4.083199 -4.080966 -4.1302619 -4.1883841 -4.2224708 -4.2261939 -4.2107139 -4.1911888 -4.1893044 -4.2107673][-4.2555118 -4.2552228 -4.2554007 -4.2355185 -4.1808395 -4.1342487 -4.1322789 -4.1641722 -4.20377 -4.2326164 -4.2396111 -4.2290545 -4.2142663 -4.2150378 -4.2366395][-4.2647276 -4.2712488 -4.2775116 -4.2614884 -4.218492 -4.18212 -4.1765366 -4.1959057 -4.2259665 -4.2529426 -4.2642655 -4.2597475 -4.2491803 -4.2500105 -4.2664285][-4.2771654 -4.2832332 -4.2889395 -4.2771888 -4.2468114 -4.2192106 -4.2104158 -4.2209735 -4.2436528 -4.2677422 -4.281455 -4.2830014 -4.2784657 -4.2803378 -4.2925615]]...]
INFO - root - 2017-12-07 14:12:32.423419: step 17610, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 64h:03m:21s remains)
INFO - root - 2017-12-07 14:12:39.192229: step 17620, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 57h:12m:50s remains)
INFO - root - 2017-12-07 14:12:46.070249: step 17630, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 56h:36m:19s remains)
INFO - root - 2017-12-07 14:12:52.900213: step 17640, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 58h:58m:19s remains)
INFO - root - 2017-12-07 14:12:59.746421: step 17650, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 62h:59m:39s remains)
INFO - root - 2017-12-07 14:13:06.573425: step 17660, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 60h:15m:31s remains)
INFO - root - 2017-12-07 14:13:13.447360: step 17670, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 56h:52m:05s remains)
INFO - root - 2017-12-07 14:13:20.228576: step 17680, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 55h:34m:22s remains)
INFO - root - 2017-12-07 14:13:26.922629: step 17690, loss = 2.04, batch loss = 1.98 (13.5 examples/sec; 0.595 sec/batch; 52h:00m:06s remains)
INFO - root - 2017-12-07 14:13:33.671708: step 17700, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 62h:03m:57s remains)
2017-12-07 14:13:34.408671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2422338 -4.2366571 -4.2255044 -4.2048984 -4.1982427 -4.2053728 -4.22217 -4.2301521 -4.2147355 -4.2034163 -4.2078719 -4.2326336 -4.2607102 -4.2716327 -4.2492933][-4.2151184 -4.2061448 -4.1925497 -4.1742759 -4.1704373 -4.1803169 -4.1943331 -4.1951108 -4.1819296 -4.1848178 -4.2047434 -4.2340569 -4.2617383 -4.2712355 -4.2520332][-4.183805 -4.1763668 -4.1579285 -4.1390362 -4.1352563 -4.1458507 -4.1609077 -4.1600981 -4.1577697 -4.1782427 -4.2115955 -4.2430387 -4.2655144 -4.2677345 -4.2459354][-4.1675286 -4.1632786 -4.1426492 -4.1225643 -4.1187987 -4.1299949 -4.1435666 -4.1452141 -4.1545892 -4.1847978 -4.2218204 -4.2506537 -4.2663016 -4.2601609 -4.236033][-4.179255 -4.1709676 -4.1473517 -4.1255617 -4.1225142 -4.1387658 -4.1525335 -4.1561961 -4.1718321 -4.2013903 -4.225338 -4.2439876 -4.2569962 -4.254283 -4.2313509][-4.1823568 -4.1678381 -4.1434603 -4.1213794 -4.1155453 -4.1307659 -4.1457558 -4.1558495 -4.1749783 -4.1986861 -4.2096515 -4.2237468 -4.2421069 -4.2460127 -4.2304344][-4.1588011 -4.146666 -4.1354146 -4.1161852 -4.107101 -4.117496 -4.1322308 -4.1454544 -4.1626897 -4.1812611 -4.1895018 -4.2076764 -4.231843 -4.2341094 -4.2202449][-4.1327662 -4.1305346 -4.1393027 -4.1300282 -4.1205606 -4.1254306 -4.1383176 -4.1518764 -4.1638074 -4.1749744 -4.182651 -4.2014647 -4.2250519 -4.2271304 -4.21387][-4.1302609 -4.13181 -4.1496758 -4.1523366 -4.1482148 -4.1489449 -4.1533847 -4.1614265 -4.1672935 -4.1731806 -4.1786151 -4.192821 -4.2142963 -4.2217741 -4.211483][-4.1525874 -4.1516557 -4.1655264 -4.1689873 -4.1684952 -4.1692348 -4.1660433 -4.1681232 -4.1724896 -4.1742706 -4.1729293 -4.1815634 -4.2013583 -4.2141266 -4.2098074][-4.173945 -4.1783228 -4.1854615 -4.1815014 -4.1787271 -4.1806412 -4.1776633 -4.17644 -4.1790876 -4.1791792 -4.1710186 -4.1698551 -4.1798444 -4.1915021 -4.1909246][-4.1725812 -4.188592 -4.1929531 -4.1873827 -4.1834073 -4.1837206 -4.183445 -4.1777635 -4.1753936 -4.1730108 -4.1621261 -4.1503015 -4.1477103 -4.1514239 -4.1527896][-4.1530714 -4.1789079 -4.186739 -4.1850834 -4.18557 -4.1896758 -4.1876273 -4.1762457 -4.1657906 -4.1581154 -4.1483793 -4.13531 -4.1241269 -4.118001 -4.1199722][-4.1460776 -4.1688714 -4.1743231 -4.1703587 -4.1729445 -4.1812916 -4.1775608 -4.1651049 -4.1487007 -4.1395965 -4.1368489 -4.1302295 -4.1220021 -4.1153364 -4.120204][-4.15717 -4.1719861 -4.171093 -4.1594172 -4.1560755 -4.1597209 -4.1498537 -4.1355729 -4.1207089 -4.1160512 -4.1231341 -4.1264949 -4.1288056 -4.1300821 -4.1413436]]...]
INFO - root - 2017-12-07 14:13:41.277943: step 17710, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 54h:55m:30s remains)
INFO - root - 2017-12-07 14:13:48.108288: step 17720, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 62h:24m:10s remains)
INFO - root - 2017-12-07 14:13:54.828542: step 17730, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 61h:06m:54s remains)
INFO - root - 2017-12-07 14:14:01.565183: step 17740, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 58h:25m:23s remains)
INFO - root - 2017-12-07 14:14:08.284966: step 17750, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 55h:53m:17s remains)
INFO - root - 2017-12-07 14:14:14.949910: step 17760, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 57h:13m:01s remains)
INFO - root - 2017-12-07 14:14:21.756607: step 17770, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 61h:41m:51s remains)
INFO - root - 2017-12-07 14:14:28.566499: step 17780, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 59h:25m:18s remains)
INFO - root - 2017-12-07 14:14:35.271888: step 17790, loss = 2.09, batch loss = 2.03 (14.5 examples/sec; 0.551 sec/batch; 48h:09m:28s remains)
INFO - root - 2017-12-07 14:14:41.865186: step 17800, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 54h:54m:48s remains)
2017-12-07 14:14:42.583264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2882652 -4.2595344 -4.23617 -4.2236238 -4.2238746 -4.2347345 -4.2480111 -4.2591381 -4.2683539 -4.2754097 -4.2825789 -4.2842231 -4.27906 -4.2744713 -4.2742629][-4.2767954 -4.2431283 -4.2206383 -4.2144904 -4.2208385 -4.2319322 -4.2413912 -4.2497334 -4.2575388 -4.2626119 -4.265059 -4.2592912 -4.2481074 -4.2447624 -4.25081][-4.2695932 -4.2367978 -4.2208977 -4.2224846 -4.2296214 -4.2335024 -4.2334652 -4.2345772 -4.2362862 -4.2363567 -4.2335644 -4.2232904 -4.2132797 -4.2177796 -4.2339993][-4.2693429 -4.2419081 -4.2345009 -4.2394233 -4.2408648 -4.2303543 -4.2134151 -4.2029638 -4.1989436 -4.2005668 -4.199111 -4.1916633 -4.1904416 -4.2068071 -4.2309375][-4.2766304 -4.2570734 -4.2547703 -4.2563305 -4.2454109 -4.2148218 -4.1760116 -4.1507912 -4.1483054 -4.1622615 -4.1723261 -4.1761608 -4.1881886 -4.2137837 -4.2381878][-4.2869449 -4.2722111 -4.2700272 -4.2642283 -4.2383285 -4.1861234 -4.1214762 -4.0825844 -4.0965462 -4.1363688 -4.1639862 -4.1809735 -4.203486 -4.2301745 -4.2501912][-4.2901 -4.2769017 -4.2712626 -4.256053 -4.214716 -4.1433725 -4.0585995 -4.0204163 -4.06748 -4.1345973 -4.1723924 -4.1926551 -4.2143235 -4.2349434 -4.2490587][-4.2825289 -4.2660151 -4.2548003 -4.2305813 -4.1784806 -4.1018105 -4.0239334 -4.0112104 -4.0849938 -4.1562796 -4.1879573 -4.19908 -4.210681 -4.2262287 -4.2383108][-4.2793221 -4.2587962 -4.2410679 -4.2091956 -4.1542354 -4.0921206 -4.0515728 -4.0692587 -4.13453 -4.1842175 -4.1985035 -4.1938586 -4.1943526 -4.2080917 -4.2246509][-4.2756248 -4.2509327 -4.2269115 -4.1930737 -4.1467514 -4.1091247 -4.1029735 -4.1322026 -4.1751704 -4.2009254 -4.2005649 -4.1846805 -4.175951 -4.1890388 -4.2124][-4.2687006 -4.2401352 -4.2108722 -4.1775079 -4.1430416 -4.1238556 -4.1362467 -4.1671391 -4.1943836 -4.2025371 -4.1926761 -4.1725893 -4.1592612 -4.1719818 -4.200182][-4.2558951 -4.2249551 -4.193594 -4.1626759 -4.1351528 -4.1250739 -4.14386 -4.1751323 -4.1953912 -4.1933236 -4.177402 -4.1569195 -4.1441932 -4.1562214 -4.182303][-4.2415261 -4.2109556 -4.1815243 -4.152689 -4.1256914 -4.11573 -4.1342068 -4.1644192 -4.1820049 -4.1767821 -4.15983 -4.1420994 -4.1324058 -4.1420817 -4.1625018][-4.2307315 -4.20274 -4.1755652 -4.1477718 -4.1217875 -4.1112404 -4.1253529 -4.1509185 -4.1665282 -4.1623216 -4.1494613 -4.1364141 -4.1299729 -4.138073 -4.1522388][-4.2314429 -4.2070012 -4.1836362 -4.1586118 -4.1353388 -4.1247973 -4.1323581 -4.1505208 -4.1629562 -4.1602521 -4.1524706 -4.1446567 -4.1418843 -4.1488528 -4.15785]]...]
INFO - root - 2017-12-07 14:14:49.443440: step 17810, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 57h:07m:54s remains)
INFO - root - 2017-12-07 14:14:56.253269: step 17820, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 58h:12m:42s remains)
INFO - root - 2017-12-07 14:15:02.968931: step 17830, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 56h:17m:11s remains)
INFO - root - 2017-12-07 14:15:09.824697: step 17840, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 64h:03m:32s remains)
INFO - root - 2017-12-07 14:15:16.564195: step 17850, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.735 sec/batch; 64h:13m:54s remains)
INFO - root - 2017-12-07 14:15:23.411885: step 17860, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 59h:19m:59s remains)
INFO - root - 2017-12-07 14:15:30.213662: step 17870, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 58h:31m:59s remains)
INFO - root - 2017-12-07 14:15:36.932724: step 17880, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 56h:20m:13s remains)
INFO - root - 2017-12-07 14:15:43.704810: step 17890, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 55h:08m:24s remains)
INFO - root - 2017-12-07 14:15:50.505098: step 17900, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 64h:33m:58s remains)
2017-12-07 14:15:51.195749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1920738 -4.2238193 -4.2441978 -4.2405219 -4.2349787 -4.2295518 -4.2273073 -4.227149 -4.2353 -4.2377439 -4.2312856 -4.2287059 -4.2181058 -4.2124286 -4.2183347][-4.1755767 -4.2206531 -4.2500992 -4.2465181 -4.2410426 -4.2360687 -4.2351909 -4.2380862 -4.2468114 -4.2504187 -4.2507958 -4.2560539 -4.25545 -4.2556458 -4.2698374][-4.1815262 -4.2247186 -4.2518344 -4.242723 -4.2288909 -4.2229128 -4.2246885 -4.226449 -4.2342982 -4.2455435 -4.2564306 -4.2678485 -4.273581 -4.2804303 -4.3014321][-4.2027736 -4.2262163 -4.238236 -4.2202473 -4.2012787 -4.1938033 -4.1966715 -4.2001858 -4.2076645 -4.2218666 -4.2390132 -4.2597294 -4.2756605 -4.2905865 -4.315371][-4.22988 -4.2330794 -4.224998 -4.1996255 -4.1755805 -4.1611719 -4.1510611 -4.1433539 -4.146997 -4.1641445 -4.1884713 -4.2231851 -4.2555475 -4.281538 -4.3087149][-4.2356791 -4.2296538 -4.2099714 -4.1777124 -4.14819 -4.1186037 -4.0749512 -4.0368638 -4.0323248 -4.0612288 -4.10597 -4.1623096 -4.216229 -4.2574067 -4.2875485][-4.2067904 -4.1977205 -4.1748724 -4.1405158 -4.1083312 -4.0645342 -3.9849136 -3.8998654 -3.8745542 -3.921473 -3.9991424 -4.0858397 -4.1637254 -4.2179484 -4.2496233][-4.1589255 -4.1467876 -4.1280241 -4.0988288 -4.0673656 -4.0206413 -3.927556 -3.8116581 -3.7636166 -3.8196421 -3.9183233 -4.0223713 -4.116756 -4.181334 -4.2111893][-4.1204529 -4.0996714 -4.0899372 -4.0748053 -4.0502868 -4.0164638 -3.94754 -3.8531356 -3.8043139 -3.8413892 -3.920891 -4.0121555 -4.1019406 -4.1645236 -4.1886296][-4.1187968 -4.0945754 -4.0903344 -4.0874257 -4.07534 -4.0643454 -4.0362072 -3.9865594 -3.954514 -3.9711633 -4.0156946 -4.0748234 -4.1404324 -4.1876106 -4.2009397][-4.1718035 -4.1532745 -4.1486731 -4.1486034 -4.1444721 -4.1502929 -4.1519022 -4.1366057 -4.1204128 -4.1257391 -4.1459627 -4.17548 -4.2130294 -4.2401857 -4.2436495][-4.2505846 -4.2400489 -4.2355776 -4.2349167 -4.2336793 -4.2415571 -4.2528143 -4.2526603 -4.2445054 -4.2423134 -4.2467122 -4.2569675 -4.2738419 -4.2860079 -4.284441][-4.3054676 -4.298337 -4.2945595 -4.2940869 -4.2928233 -4.2964854 -4.3063407 -4.3104811 -4.3054242 -4.2992158 -4.2964435 -4.2972078 -4.3023262 -4.3076358 -4.3064575][-4.3294034 -4.3251348 -4.3224678 -4.3220668 -4.3205028 -4.3203297 -4.3257065 -4.3287592 -4.3265786 -4.3216619 -4.3190107 -4.3171492 -4.3180094 -4.3221745 -4.32426][-4.3382354 -4.3367848 -4.3362427 -4.3355236 -4.3337326 -4.3324623 -4.3339424 -4.3350277 -4.3342261 -4.3322806 -4.3314905 -4.3308368 -4.3311419 -4.3338442 -4.336287]]...]
INFO - root - 2017-12-07 14:15:57.896004: step 17910, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 62h:59m:47s remains)
INFO - root - 2017-12-07 14:16:04.828617: step 17920, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 0.771 sec/batch; 67h:21m:34s remains)
INFO - root - 2017-12-07 14:16:11.581773: step 17930, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 59h:21m:34s remains)
INFO - root - 2017-12-07 14:16:18.345729: step 17940, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 55h:01m:56s remains)
INFO - root - 2017-12-07 14:16:25.174665: step 17950, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 56h:11m:35s remains)
INFO - root - 2017-12-07 14:16:32.007168: step 17960, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 64h:48m:00s remains)
INFO - root - 2017-12-07 14:16:38.772628: step 17970, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 62h:17m:39s remains)
INFO - root - 2017-12-07 14:16:45.495008: step 17980, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 60h:38m:12s remains)
INFO - root - 2017-12-07 14:16:52.083396: step 17990, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 0.529 sec/batch; 46h:13m:42s remains)
INFO - root - 2017-12-07 14:16:58.743632: step 18000, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 55h:58m:38s remains)
2017-12-07 14:16:59.558151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2655129 -4.2613664 -4.2573338 -4.2524381 -4.2441888 -4.2369161 -4.2364678 -4.2397037 -4.2395415 -4.2366171 -4.2306976 -4.2279062 -4.2405033 -4.260736 -4.2776313][-4.2482386 -4.2408018 -4.2328525 -4.2267866 -4.2168179 -4.2059436 -4.2026925 -4.2057629 -4.2017584 -4.1946211 -4.187624 -4.1844816 -4.1994052 -4.2250762 -4.2482595][-4.2317877 -4.2144327 -4.1971116 -4.1908231 -4.1797018 -4.1621428 -4.1512513 -4.1511893 -4.1441069 -4.1338024 -4.1247287 -4.1195855 -4.1359863 -4.168282 -4.2013903][-4.1998296 -4.1735907 -4.1453557 -4.1367083 -4.1254387 -4.1000795 -4.07754 -4.0748606 -4.0717216 -4.0700173 -4.0617208 -4.0547719 -4.0715489 -4.1108656 -4.156249][-4.1634603 -4.13478 -4.1003771 -4.0809903 -4.0633073 -4.0267096 -3.9878781 -3.9824905 -3.9971862 -4.01668 -4.0174317 -4.0128293 -4.0336003 -4.0786943 -4.1308894][-4.1232052 -4.09744 -4.0659423 -4.0385132 -4.0126228 -3.95371 -3.8866079 -3.8837075 -3.9358141 -3.989253 -4.0077152 -4.0116143 -4.0357184 -4.08037 -4.1313081][-4.0835943 -4.0639057 -4.0387745 -4.0061684 -3.9721365 -3.8875115 -3.7766137 -3.7769015 -3.8832374 -3.9754488 -4.0160313 -4.0324326 -4.0560689 -4.0943155 -4.1384325][-4.0763588 -4.0672569 -4.0542226 -4.0243759 -3.9904218 -3.9055157 -3.7825022 -3.7790532 -3.8994436 -4.0006275 -4.0484447 -4.0640578 -4.0767369 -4.1046877 -4.1412072][-4.0969677 -4.0966697 -4.0993562 -4.0886688 -4.06512 -4.0093236 -3.9279728 -3.918498 -3.9909611 -4.061594 -4.0944524 -4.0967684 -4.0960884 -4.1132045 -4.1427054][-4.1298056 -4.1338496 -4.1439562 -4.1437836 -4.127449 -4.0928888 -4.0451894 -4.0351586 -4.0709696 -4.1103325 -4.1280308 -4.1212573 -4.112957 -4.1207418 -4.1428595][-4.1545982 -4.1607485 -4.1744642 -4.1812067 -4.1669426 -4.1357961 -4.0988984 -4.0897927 -4.1088376 -4.1286964 -4.13688 -4.1299815 -4.1210375 -4.1246982 -4.1415][-4.1528268 -4.1607289 -4.1820993 -4.1932926 -4.1756315 -4.1478491 -4.1215711 -4.1139226 -4.1248364 -4.1328087 -4.1330767 -4.1287746 -4.1251264 -4.1314707 -4.145206][-4.1438236 -4.148396 -4.1718907 -4.180141 -4.1588707 -4.1365247 -4.1207156 -4.1140857 -4.1236181 -4.1338544 -4.134542 -4.13121 -4.1344743 -4.1443739 -4.1540895][-4.1450357 -4.1422296 -4.1572952 -4.1657028 -4.1472683 -4.1279736 -4.1159878 -4.1108112 -4.1259208 -4.1474214 -4.1492915 -4.1422257 -4.1479268 -4.1586423 -4.1645818][-4.1712365 -4.1578422 -4.1568012 -4.1682663 -4.1618223 -4.1483231 -4.1395397 -4.1319938 -4.147222 -4.173943 -4.1767712 -4.1655645 -4.1695018 -4.182 -4.188169]]...]
INFO - root - 2017-12-07 14:17:06.306467: step 18010, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 57h:35m:27s remains)
INFO - root - 2017-12-07 14:17:13.117306: step 18020, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 56h:58m:40s remains)
INFO - root - 2017-12-07 14:17:20.055630: step 18030, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 64h:00m:59s remains)
INFO - root - 2017-12-07 14:17:26.943430: step 18040, loss = 2.10, batch loss = 2.04 (10.7 examples/sec; 0.750 sec/batch; 65h:29m:58s remains)
INFO - root - 2017-12-07 14:17:33.713840: step 18050, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 57h:52m:21s remains)
INFO - root - 2017-12-07 14:17:40.631010: step 18060, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 59h:35m:27s remains)
INFO - root - 2017-12-07 14:17:47.347548: step 18070, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 54h:32m:51s remains)
INFO - root - 2017-12-07 14:17:54.193517: step 18080, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 61h:32m:43s remains)
INFO - root - 2017-12-07 14:18:00.872068: step 18090, loss = 2.08, batch loss = 2.02 (13.9 examples/sec; 0.574 sec/batch; 50h:09m:26s remains)
INFO - root - 2017-12-07 14:18:07.588214: step 18100, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 58h:45m:14s remains)
2017-12-07 14:18:08.323754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.253572 -4.2438231 -4.2401252 -4.2366004 -4.2339206 -4.236362 -4.2316117 -4.2274261 -4.2360015 -4.2427425 -4.2378106 -4.2277465 -4.2201886 -4.2141271 -4.2146916][-4.222436 -4.2131724 -4.2092013 -4.2018194 -4.197577 -4.2046952 -4.1982503 -4.1866517 -4.1906629 -4.2014647 -4.2017808 -4.1930127 -4.1849566 -4.1786208 -4.1808672][-4.208395 -4.2034431 -4.2018991 -4.1959944 -4.1970315 -4.2069893 -4.1968265 -4.1735034 -4.1681428 -4.1814613 -4.1867695 -4.1834159 -4.180438 -4.1798921 -4.1880469][-4.2032728 -4.2023678 -4.2014084 -4.1987324 -4.2029672 -4.2121162 -4.2010112 -4.1738563 -4.1662159 -4.1783843 -4.1810966 -4.1765437 -4.176528 -4.1818118 -4.1962347][-4.212851 -4.21268 -4.2066112 -4.1977649 -4.1975784 -4.2035909 -4.1925421 -4.1675839 -4.1595907 -4.1625562 -4.159956 -4.1568928 -4.158227 -4.1657429 -4.1807508][-4.2024565 -4.201406 -4.1903377 -4.1719246 -4.1614313 -4.1603141 -4.1485176 -4.1233749 -4.1081457 -4.100565 -4.1010408 -4.1082687 -4.1143618 -4.1200776 -4.1294775][-4.1709423 -4.1671963 -4.1551256 -4.1329455 -4.1129441 -4.1014986 -4.0833316 -4.0568924 -4.0338664 -4.0225406 -4.0317669 -4.0528054 -4.0695457 -4.0767331 -4.0770268][-4.1497407 -4.1415639 -4.1332922 -4.1147852 -4.090538 -4.0751262 -4.0534129 -4.0276031 -4.0035248 -3.9959033 -4.0166445 -4.0489125 -4.0739131 -4.0857992 -4.0855532][-4.1551156 -4.144515 -4.1375093 -4.1272554 -4.1090994 -4.0988283 -4.0820417 -4.0616355 -4.0437522 -4.0400915 -4.0552979 -4.0851388 -4.1113725 -4.1222138 -4.1214104][-4.1725516 -4.1672359 -4.1648278 -4.1649756 -4.1585803 -4.1553578 -4.1420612 -4.1250238 -4.1127911 -4.1098719 -4.1140118 -4.1327653 -4.1521182 -4.1596913 -4.1563277][-4.19232 -4.19487 -4.2011905 -4.2105894 -4.2151423 -4.2187967 -4.2098103 -4.196991 -4.1866913 -4.1813498 -4.1762242 -4.1799016 -4.1848359 -4.1817508 -4.171968][-4.1778212 -4.1886506 -4.2030768 -4.2203016 -4.234746 -4.2468143 -4.24795 -4.2441425 -4.2380238 -4.2284546 -4.2156382 -4.2071581 -4.2003756 -4.1875138 -4.1716361][-4.1496472 -4.1623807 -4.1795564 -4.1965752 -4.212122 -4.2274504 -4.2362757 -4.2399454 -4.2388415 -4.2291474 -4.2167416 -4.2051888 -4.1941376 -4.1771855 -4.160656][-4.1514716 -4.1600351 -4.1715393 -4.1836758 -4.1934056 -4.201973 -4.2066789 -4.2125421 -4.2168288 -4.212822 -4.2074242 -4.2026687 -4.1945567 -4.1756897 -4.1583695][-4.1748061 -4.179924 -4.1863275 -4.1930528 -4.1956778 -4.1966829 -4.1982856 -4.205514 -4.2128596 -4.2124891 -4.2119179 -4.2156973 -4.2163935 -4.2033887 -4.1902556]]...]
INFO - root - 2017-12-07 14:18:15.126696: step 18110, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 61h:32m:11s remains)
INFO - root - 2017-12-07 14:18:21.917589: step 18120, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 63h:54m:18s remains)
INFO - root - 2017-12-07 14:18:28.596857: step 18130, loss = 2.03, batch loss = 1.98 (12.0 examples/sec; 0.666 sec/batch; 58h:09m:03s remains)
INFO - root - 2017-12-07 14:18:35.413532: step 18140, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 53h:46m:47s remains)
INFO - root - 2017-12-07 14:18:42.182438: step 18150, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 57h:07m:25s remains)
INFO - root - 2017-12-07 14:18:49.033369: step 18160, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.744 sec/batch; 64h:59m:42s remains)
INFO - root - 2017-12-07 14:18:55.774500: step 18170, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 57h:06m:19s remains)
INFO - root - 2017-12-07 14:19:02.544085: step 18180, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 57h:19m:12s remains)
INFO - root - 2017-12-07 14:19:09.194025: step 18190, loss = 2.06, batch loss = 2.00 (15.8 examples/sec; 0.508 sec/batch; 44h:18m:35s remains)
INFO - root - 2017-12-07 14:19:16.002081: step 18200, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.714 sec/batch; 62h:17m:40s remains)
2017-12-07 14:19:16.690588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2922096 -4.2870941 -4.289299 -4.2903085 -4.27591 -4.262054 -4.2618818 -4.266819 -4.27667 -4.2852569 -4.290657 -4.2964654 -4.2964249 -4.2919207 -4.2867169][-4.2768846 -4.2704415 -4.273273 -4.2730231 -4.2508545 -4.2318773 -4.2325959 -4.2391005 -4.2545171 -4.2663918 -4.2743278 -4.2877855 -4.2936311 -4.2900171 -4.2817378][-4.2586541 -4.2505703 -4.2497544 -4.2447238 -4.2165689 -4.1983004 -4.1981416 -4.2030325 -4.2252369 -4.2460361 -4.2589498 -4.2763667 -4.2890935 -4.2888393 -4.2794533][-4.2393594 -4.2269235 -4.2208037 -4.2094007 -4.1816192 -4.1643915 -4.159832 -4.1586676 -4.1895757 -4.2233191 -4.2430959 -4.2634125 -4.2793908 -4.2825761 -4.2743163][-4.2142138 -4.1942739 -4.1767712 -4.1641665 -4.1436491 -4.1237268 -4.1113296 -4.1041503 -4.1425786 -4.18751 -4.2149463 -4.2425714 -4.2642055 -4.2738972 -4.2665567][-4.1909389 -4.1594048 -4.1235285 -4.1077728 -4.0956826 -4.0808825 -4.062233 -4.0488219 -4.0913925 -4.1443624 -4.1757936 -4.2090445 -4.2387533 -4.2590308 -4.258378][-4.179328 -4.1313658 -4.0685973 -4.0388284 -4.0303397 -4.0283618 -4.0077419 -3.9827271 -4.027524 -4.0871811 -4.1221228 -4.1590204 -4.195148 -4.2258253 -4.2367172][-4.1873727 -4.129488 -4.0485249 -4.003541 -3.993324 -3.9985335 -3.9721584 -3.931422 -3.9697981 -4.0315375 -4.0669947 -4.0969534 -4.1345568 -4.1730714 -4.1989894][-4.2108603 -4.1554646 -4.0754495 -4.0221844 -4.0002565 -3.9991875 -3.9629245 -3.9092646 -3.9359779 -3.9957488 -4.0301476 -4.0480561 -4.0823956 -4.1253438 -4.1608682][-4.2240133 -4.1775565 -4.1123686 -4.0597816 -4.0303483 -4.0162153 -3.9796247 -3.9301903 -3.9437032 -3.9927187 -4.0183358 -4.0246906 -4.0507617 -4.0864716 -4.120605][-4.2145786 -4.1757884 -4.1312013 -4.0936365 -4.0700865 -4.0533004 -4.0274096 -3.9973781 -4.0053754 -4.0306382 -4.0374846 -4.02719 -4.0371671 -4.0632567 -4.0890651][-4.1944904 -4.1553321 -4.1259971 -4.1072044 -4.0959907 -4.0815105 -4.0668759 -4.06176 -4.0752234 -4.0840034 -4.0729966 -4.0509181 -4.0398793 -4.0566344 -4.0709023][-4.1699953 -4.1285629 -4.1080418 -4.0997586 -4.0957804 -4.0863328 -4.0864615 -4.1021833 -4.1204762 -4.1274123 -4.110137 -4.0786467 -4.0479283 -4.0549212 -4.0635142][-4.1512523 -4.1118736 -4.0973988 -4.0940742 -4.0955582 -4.0942039 -4.1047297 -4.1297612 -4.1442642 -4.14642 -4.1296935 -4.097415 -4.0596261 -4.0659947 -4.0779891][-4.1620026 -4.1312771 -4.1224384 -4.123106 -4.1301432 -4.134675 -4.1459565 -4.167346 -4.1764708 -4.1756859 -4.1628056 -4.1389332 -4.1101203 -4.1184115 -4.131711]]...]
INFO - root - 2017-12-07 14:19:23.501966: step 18210, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 61h:27m:33s remains)
INFO - root - 2017-12-07 14:19:30.189340: step 18220, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 59h:38m:35s remains)
INFO - root - 2017-12-07 14:19:37.087702: step 18230, loss = 2.03, batch loss = 1.97 (10.8 examples/sec; 0.740 sec/batch; 64h:35m:02s remains)
INFO - root - 2017-12-07 14:19:43.869178: step 18240, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 61h:30m:58s remains)
INFO - root - 2017-12-07 14:19:50.805487: step 18250, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.633 sec/batch; 55h:13m:44s remains)
INFO - root - 2017-12-07 14:19:57.589923: step 18260, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 55h:24m:07s remains)
INFO - root - 2017-12-07 14:20:04.522542: step 18270, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 63h:05m:28s remains)
INFO - root - 2017-12-07 14:20:11.465824: step 18280, loss = 2.05, batch loss = 2.00 (10.6 examples/sec; 0.755 sec/batch; 65h:55m:04s remains)
INFO - root - 2017-12-07 14:20:18.074510: step 18290, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 0.527 sec/batch; 45h:57m:36s remains)
INFO - root - 2017-12-07 14:20:24.897017: step 18300, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 57h:47m:38s remains)
2017-12-07 14:20:25.705624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2648249 -4.252244 -4.2377372 -4.2341185 -4.2391615 -4.2533712 -4.2706394 -4.2837429 -4.2877178 -4.2873139 -4.2893286 -4.2949481 -4.3042231 -4.3124814 -4.3190265][-4.2389507 -4.2158194 -4.1925673 -4.1834192 -4.1874642 -4.2042236 -4.2260761 -4.246521 -4.2558656 -4.2574873 -4.2620745 -4.2702103 -4.2807317 -4.2910337 -4.3003297][-4.2420154 -4.2164578 -4.187531 -4.1696076 -4.1648026 -4.1731744 -4.18934 -4.2089944 -4.2212577 -4.2270651 -4.2367949 -4.247539 -4.2574306 -4.2698379 -4.281292][-4.2460923 -4.2228589 -4.1926842 -4.1700964 -4.150743 -4.142386 -4.1469669 -4.1615758 -4.1742263 -4.1851096 -4.2004447 -4.2146764 -4.2236128 -4.2379518 -4.2542443][-4.2214775 -4.2033253 -4.1779084 -4.1562109 -4.1276183 -4.1052985 -4.0982556 -4.1016607 -4.1165152 -4.1349926 -4.1556129 -4.1709766 -4.1790476 -4.1954751 -4.2203612][-4.17771 -4.1642318 -4.1488361 -4.1311574 -4.0930252 -4.0546379 -4.0302258 -4.0216441 -4.0504184 -4.0888543 -4.1225543 -4.1401796 -4.1462932 -4.16151 -4.1922083][-4.1234651 -4.1105843 -4.0974803 -4.0702977 -4.0084271 -3.9362953 -3.8747334 -3.847158 -3.9014325 -3.9771085 -4.0360441 -4.0716877 -4.0938749 -4.1182523 -4.1580667][-4.087893 -4.0730162 -4.0565209 -4.0184889 -3.9345336 -3.8311007 -3.723573 -3.6652493 -3.7473342 -3.8620462 -3.9483581 -4.0034404 -4.0509076 -4.0915761 -4.1407766][-4.1126308 -4.1018095 -4.0903311 -4.0575633 -3.9877143 -3.9072323 -3.8217278 -3.7743936 -3.828186 -3.9165137 -3.9864872 -4.0326557 -4.0798125 -4.1216774 -4.1684356][-4.1896768 -4.1885524 -4.1868858 -4.1685 -4.125999 -4.0780377 -4.0305519 -4.0080142 -4.0334444 -4.0815873 -4.1200194 -4.1432762 -4.1702061 -4.197083 -4.225347][-4.2671285 -4.2755904 -4.2803597 -4.2711425 -4.249187 -4.2251053 -4.2009211 -4.1910434 -4.20022 -4.2204132 -4.2361078 -4.2457342 -4.2596803 -4.2699795 -4.2801609][-4.3065963 -4.3174019 -4.3215857 -4.3154249 -4.3047848 -4.2933383 -4.2823491 -4.2783613 -4.2812185 -4.2885103 -4.2946925 -4.2994318 -4.3072319 -4.3087487 -4.3082843][-4.3161435 -4.321609 -4.3218393 -4.3187637 -4.3141704 -4.3093352 -4.3051977 -4.3048296 -4.3064246 -4.3094864 -4.3129468 -4.3153043 -4.3187366 -4.3181977 -4.316021][-4.3197508 -4.3221412 -4.3214765 -4.3203883 -4.3182173 -4.3149529 -4.3135824 -4.3146772 -4.3162866 -4.3187294 -4.3201675 -4.3211765 -4.3214693 -4.3194022 -4.3174319][-4.3244538 -4.3250794 -4.32367 -4.3225012 -4.3214078 -4.3198352 -4.3191519 -4.3194141 -4.3209662 -4.3232064 -4.3242493 -4.3246593 -4.324749 -4.3245382 -4.3250661]]...]
INFO - root - 2017-12-07 14:20:32.354109: step 18310, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 59h:00m:59s remains)
INFO - root - 2017-12-07 14:20:39.149608: step 18320, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.611 sec/batch; 53h:17m:28s remains)
INFO - root - 2017-12-07 14:20:45.937923: step 18330, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 55h:50m:09s remains)
INFO - root - 2017-12-07 14:20:52.839738: step 18340, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 63h:50m:06s remains)
INFO - root - 2017-12-07 14:20:59.705172: step 18350, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 57h:21m:49s remains)
INFO - root - 2017-12-07 14:21:06.587132: step 18360, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 57h:20m:44s remains)
INFO - root - 2017-12-07 14:21:13.369660: step 18370, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 55h:46m:38s remains)
INFO - root - 2017-12-07 14:21:20.123288: step 18380, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 58h:58m:04s remains)
INFO - root - 2017-12-07 14:21:26.854794: step 18390, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 0.553 sec/batch; 48h:14m:53s remains)
INFO - root - 2017-12-07 14:21:33.771359: step 18400, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 59h:47m:30s remains)
2017-12-07 14:21:34.418989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2174659 -4.1996455 -4.1771641 -4.1681867 -4.1757855 -4.1884818 -4.1971602 -4.2048922 -4.2181749 -4.2405663 -4.2591782 -4.2649426 -4.2638459 -4.2712975 -4.280529][-4.2550755 -4.2399511 -4.2222848 -4.2165365 -4.2223058 -4.2332559 -4.2433586 -4.2548046 -4.2635918 -4.2719922 -4.27162 -4.2605042 -4.2523403 -4.2561488 -4.2652721][-4.2724013 -4.266923 -4.2617984 -4.2592955 -4.255785 -4.2557487 -4.2567272 -4.262373 -4.2600265 -4.2563634 -4.246335 -4.2284074 -4.223011 -4.2331338 -4.2505665][-4.2594972 -4.2592573 -4.2653813 -4.269207 -4.2601404 -4.2504144 -4.2430778 -4.2396603 -4.2266083 -4.2115664 -4.1980138 -4.1845574 -4.1892228 -4.2134452 -4.2444959][-4.2303524 -4.2309527 -4.2398643 -4.2440643 -4.2341523 -4.2197928 -4.206532 -4.196516 -4.1771879 -4.1598172 -4.1512952 -4.1515861 -4.1734767 -4.2119579 -4.2500286][-4.197473 -4.2000737 -4.2039108 -4.2031994 -4.1949835 -4.1837516 -4.1700411 -4.1569915 -4.1374149 -4.126811 -4.129106 -4.1457405 -4.1840787 -4.2265038 -4.2599497][-4.1687 -4.1750984 -4.1704426 -4.1626592 -4.1559038 -4.1459532 -4.1287117 -4.1086154 -4.0867767 -4.0896325 -4.1124387 -4.1493497 -4.1981506 -4.2390571 -4.26653][-4.15576 -4.1577892 -4.1378284 -4.1124668 -4.0949879 -4.0811653 -4.0656629 -4.0441852 -4.0220985 -4.0383859 -4.0869327 -4.1460438 -4.2027826 -4.243649 -4.270556][-4.15265 -4.1436095 -4.108943 -4.0644484 -4.0313964 -4.0144057 -4.0095692 -3.9989955 -3.9825146 -4.0101242 -4.0780406 -4.1450634 -4.2026925 -4.2432423 -4.2709951][-4.1607842 -4.1502171 -4.1110253 -4.0574713 -4.0176716 -4.0064831 -4.0154386 -4.018456 -4.0100474 -4.0434084 -4.10928 -4.1646285 -4.2113662 -4.2478051 -4.2762351][-4.1689334 -4.164835 -4.1330495 -4.089262 -4.0603409 -4.0641174 -4.0865555 -4.0972795 -4.09258 -4.1171794 -4.1627197 -4.1947904 -4.2283854 -4.2618294 -4.2872953][-4.1749916 -4.1762986 -4.1596627 -4.1411495 -4.1369843 -4.1555424 -4.181201 -4.1903887 -4.182601 -4.1910558 -4.2106395 -4.2234755 -4.2468848 -4.27673 -4.2982187][-4.1882586 -4.1923308 -4.1916733 -4.1945448 -4.2060204 -4.2286115 -4.249671 -4.2555065 -4.2427297 -4.2401528 -4.2456355 -4.2488375 -4.2679067 -4.2943764 -4.312541][-4.2262182 -4.2306376 -4.2363009 -4.2473154 -4.2614274 -4.2784381 -4.29067 -4.2899446 -4.2726245 -4.2655282 -4.2689567 -4.2700005 -4.2877865 -4.3128982 -4.3280892][-4.2732286 -4.2743616 -4.2800527 -4.2900543 -4.2999554 -4.309999 -4.3143411 -4.3077812 -4.2897391 -4.2814007 -4.2836814 -4.2841325 -4.3011556 -4.3252134 -4.3380122]]...]
INFO - root - 2017-12-07 14:21:41.330974: step 18410, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 59h:36m:05s remains)
INFO - root - 2017-12-07 14:21:48.212544: step 18420, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 64h:36m:22s remains)
INFO - root - 2017-12-07 14:21:54.926523: step 18430, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 61h:23m:40s remains)
INFO - root - 2017-12-07 14:22:01.703055: step 18440, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.623 sec/batch; 54h:22m:12s remains)
INFO - root - 2017-12-07 14:22:08.596150: step 18450, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 58h:19m:03s remains)
INFO - root - 2017-12-07 14:22:15.522991: step 18460, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 64h:25m:26s remains)
INFO - root - 2017-12-07 14:22:22.292804: step 18470, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 59h:19m:10s remains)
INFO - root - 2017-12-07 14:22:29.059502: step 18480, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 60h:08m:27s remains)
INFO - root - 2017-12-07 14:22:35.746012: step 18490, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 0.508 sec/batch; 44h:20m:16s remains)
INFO - root - 2017-12-07 14:22:42.559668: step 18500, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 61h:51m:24s remains)
2017-12-07 14:22:43.297777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1498289 -4.1369262 -4.137218 -4.1445441 -4.1571412 -4.1601782 -4.1468768 -4.1337132 -4.1250315 -4.123013 -4.1215329 -4.122385 -4.1390166 -4.1672349 -4.1942739][-4.1235466 -4.1149096 -4.1196084 -4.1334958 -4.1529441 -4.1640978 -4.1539545 -4.1419258 -4.1356893 -4.1309032 -4.1248951 -4.1212597 -4.1304836 -4.1493235 -4.163496][-4.1327791 -4.1272831 -4.1373997 -4.159924 -4.1836743 -4.2010813 -4.1959686 -4.1828423 -4.17416 -4.1616373 -4.1429057 -4.1282277 -4.1232772 -4.1274686 -4.1348019][-4.1589122 -4.1523323 -4.1616015 -4.1827607 -4.2024112 -4.2164092 -4.2114582 -4.1948333 -4.183989 -4.1679697 -4.1380496 -4.1115613 -4.0962567 -4.0930009 -4.105794][-4.1719127 -4.1629949 -4.1660552 -4.1769009 -4.1825151 -4.1825194 -4.1691055 -4.1464782 -4.1356444 -4.12928 -4.1037812 -4.0783682 -4.06588 -4.0647988 -4.0860229][-4.1656847 -4.1568952 -4.1552715 -4.1577315 -4.1465163 -4.1260176 -4.0967441 -4.0556 -4.0404696 -4.0516548 -4.0513153 -4.0489221 -4.0525985 -4.0571976 -4.078321][-4.15501 -4.1484447 -4.1484733 -4.1493411 -4.1276364 -4.0893445 -4.0462461 -3.9885676 -3.9638269 -3.9868526 -4.0136509 -4.0415597 -4.06643 -4.0775032 -4.0882521][-4.1608357 -4.1554322 -4.1563911 -4.160398 -4.1448646 -4.1123228 -4.0763569 -4.0239625 -3.9934907 -4.0104947 -4.0399361 -4.0718412 -4.1032047 -4.1175084 -4.1170077][-4.18861 -4.1818891 -4.1812119 -4.1876073 -4.1833825 -4.1660776 -4.1485004 -4.1134896 -4.0881009 -4.0977569 -4.1154246 -4.1288233 -4.1470909 -4.1554513 -4.1495128][-4.2115273 -4.2064481 -4.2059469 -4.21499 -4.2192454 -4.2120256 -4.2070923 -4.1868057 -4.1679115 -4.173676 -4.1841183 -4.1834488 -4.1845894 -4.1812983 -4.1702576][-4.2229104 -4.2199411 -4.2192936 -4.2260118 -4.2323546 -4.229013 -4.2276587 -4.2146087 -4.1998277 -4.207602 -4.2207556 -4.2183967 -4.2102623 -4.1993461 -4.1831026][-4.237164 -4.2315316 -4.23211 -4.2354956 -4.2365756 -4.2303367 -4.2275476 -4.2152553 -4.1994348 -4.2069054 -4.2258773 -4.2290587 -4.2211428 -4.2085462 -4.1906004][-4.2474456 -4.2391434 -4.2395544 -4.2382574 -4.2330852 -4.2246552 -4.2186871 -4.2059078 -4.1891556 -4.1932912 -4.2146516 -4.224791 -4.2218857 -4.2121997 -4.1977692][-4.2448592 -4.2315903 -4.23251 -4.23052 -4.2239194 -4.2184305 -4.2110152 -4.1975975 -4.1810045 -4.18335 -4.203433 -4.2197294 -4.2253761 -4.2219563 -4.2151461][-4.2478433 -4.2302451 -4.2279119 -4.2243261 -4.216825 -4.2146678 -4.20989 -4.1995144 -4.1848869 -4.1825709 -4.1958084 -4.2139926 -4.2280579 -4.2329168 -4.2340603]]...]
INFO - root - 2017-12-07 14:22:50.111907: step 18510, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 56h:41m:42s remains)
INFO - root - 2017-12-07 14:22:56.811773: step 18520, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 56h:22m:21s remains)
INFO - root - 2017-12-07 14:23:03.633556: step 18530, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 62h:48m:16s remains)
INFO - root - 2017-12-07 14:23:10.425836: step 18540, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 54h:07m:07s remains)
INFO - root - 2017-12-07 14:23:17.186936: step 18550, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 56h:27m:47s remains)
INFO - root - 2017-12-07 14:23:23.945280: step 18560, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 60h:52m:25s remains)
INFO - root - 2017-12-07 14:23:30.753070: step 18570, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 62h:10m:08s remains)
INFO - root - 2017-12-07 14:23:37.535817: step 18580, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 58h:06m:43s remains)
INFO - root - 2017-12-07 14:23:44.281221: step 18590, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 0.512 sec/batch; 44h:40m:48s remains)
INFO - root - 2017-12-07 14:23:51.019133: step 18600, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 55h:18m:28s remains)
2017-12-07 14:23:51.783476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2817163 -4.2676153 -4.2472868 -4.2353363 -4.239048 -4.2393661 -4.2353497 -4.23432 -4.24323 -4.2560897 -4.2678447 -4.2769475 -4.2879972 -4.289525 -4.2732406][-4.2815356 -4.2633114 -4.2381873 -4.2253714 -4.2290931 -4.2249708 -4.2151952 -4.206522 -4.2116022 -4.2275906 -4.2467093 -4.2607875 -4.2768927 -4.2817836 -4.264812][-4.2897134 -4.2672763 -4.24128 -4.2287531 -4.2283864 -4.215724 -4.1917372 -4.1686187 -4.1695709 -4.1963539 -4.23171 -4.2573972 -4.2794747 -4.2865987 -4.2708683][-4.293211 -4.2692986 -4.2448397 -4.2312717 -4.2229371 -4.1971836 -4.1546478 -4.1197357 -4.12325 -4.1650071 -4.2159476 -4.254756 -4.2821012 -4.2877946 -4.2726593][-4.291132 -4.2722163 -4.2514296 -4.2380152 -4.2186012 -4.1785331 -4.1208525 -4.0813856 -4.0930095 -4.1496863 -4.2117863 -4.2608662 -4.2933016 -4.2962751 -4.2804208][-4.2613926 -4.244267 -4.2294669 -4.2168026 -4.1860976 -4.12951 -4.0584559 -4.0255027 -4.0564818 -4.1286249 -4.1987453 -4.2564669 -4.2932 -4.2970352 -4.2817583][-4.2060037 -4.187633 -4.1792431 -4.170578 -4.1334009 -4.0619745 -3.9794874 -3.9537423 -4.0089579 -4.0968328 -4.1769052 -4.2412014 -4.2762833 -4.2776532 -4.2611494][-4.1473746 -4.1260085 -4.123528 -4.126718 -4.0986977 -4.0223026 -3.9314249 -3.9135568 -3.9854 -4.080122 -4.1597328 -4.2222223 -4.2519484 -4.2484555 -4.23088][-4.1193748 -4.1007972 -4.1087689 -4.1240411 -4.1108322 -4.0485539 -3.9709506 -3.9608693 -4.0296221 -4.1074047 -4.1663365 -4.210227 -4.2293129 -4.2223377 -4.20572][-4.1315374 -4.1216955 -4.1357732 -4.1563663 -4.1561794 -4.1178794 -4.065764 -4.0620856 -4.1151657 -4.165554 -4.1970072 -4.2149749 -4.2185626 -4.2074275 -4.189311][-4.1627855 -4.15613 -4.168767 -4.1877966 -4.1950026 -4.1781139 -4.1507769 -4.1493979 -4.1825962 -4.209938 -4.2251425 -4.228333 -4.2209749 -4.2044168 -4.1856909][-4.2007318 -4.196279 -4.2051821 -4.2218 -4.2336311 -4.2309761 -4.2168489 -4.2141972 -4.2319169 -4.2447863 -4.2530417 -4.2527976 -4.2452579 -4.2303071 -4.2163506][-4.2297564 -4.2317286 -4.2436342 -4.2595572 -4.2691073 -4.2705956 -4.2638597 -4.2616816 -4.269887 -4.2774215 -4.2838426 -4.2860489 -4.2839317 -4.2738233 -4.2632747][-4.2510934 -4.2559443 -4.2675314 -4.2829361 -4.2897563 -4.2897544 -4.2848487 -4.2822886 -4.287591 -4.2956543 -4.3028717 -4.3097305 -4.3145676 -4.3104267 -4.3010678][-4.2638726 -4.2690029 -4.2792745 -4.2922993 -4.2981305 -4.2952847 -4.2895379 -4.2864404 -4.2889743 -4.2967315 -4.3058558 -4.3170018 -4.3259258 -4.3261037 -4.3176122]]...]
INFO - root - 2017-12-07 14:23:58.553958: step 18610, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 59h:05m:35s remains)
INFO - root - 2017-12-07 14:24:05.287218: step 18620, loss = 2.05, batch loss = 1.99 (13.1 examples/sec; 0.611 sec/batch; 53h:18m:16s remains)
INFO - root - 2017-12-07 14:24:12.166856: step 18630, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 57h:10m:09s remains)
INFO - root - 2017-12-07 14:24:18.986016: step 18640, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 60h:50m:08s remains)
INFO - root - 2017-12-07 14:24:25.894495: step 18650, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 61h:53m:21s remains)
INFO - root - 2017-12-07 14:24:32.733235: step 18660, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 57h:44m:01s remains)
INFO - root - 2017-12-07 14:24:39.537874: step 18670, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 55h:10m:37s remains)
INFO - root - 2017-12-07 14:24:46.419972: step 18680, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.719 sec/batch; 62h:42m:46s remains)
INFO - root - 2017-12-07 14:24:53.014050: step 18690, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 0.546 sec/batch; 47h:35m:17s remains)
INFO - root - 2017-12-07 14:24:59.791150: step 18700, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 60h:13m:31s remains)
2017-12-07 14:25:00.532814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.273551 -4.2508821 -4.2264714 -4.19007 -4.1390986 -4.1357884 -4.169261 -4.2087393 -4.2425818 -4.2472172 -4.2307582 -4.2057896 -4.17663 -4.1592126 -4.15722][-4.2555375 -4.227911 -4.2032909 -4.1628947 -4.10643 -4.0943341 -4.1252275 -4.1648083 -4.2059751 -4.2230659 -4.2238703 -4.2104788 -4.188148 -4.1725831 -4.16492][-4.236166 -4.2061386 -4.1796794 -4.1354165 -4.0740619 -4.05128 -4.0738034 -4.1111131 -4.1564746 -4.1851473 -4.2032404 -4.2040877 -4.191927 -4.1774344 -4.1663604][-4.2294173 -4.2026162 -4.1777258 -4.1348171 -4.068893 -4.0301542 -4.0336776 -4.0625486 -4.1124663 -4.1549983 -4.19144 -4.2104197 -4.2078624 -4.1948919 -4.1776447][-4.2341347 -4.2100143 -4.187274 -4.151886 -4.0941396 -4.0455675 -4.0218306 -4.0291104 -4.0815449 -4.1359615 -4.1838603 -4.2177067 -4.2271833 -4.2194963 -4.1982193][-4.2425823 -4.2144756 -4.1920738 -4.1639719 -4.1185365 -4.0696821 -4.0282 -4.0212374 -4.0675611 -4.1199083 -4.1640692 -4.2023339 -4.2260695 -4.2302036 -4.2168932][-4.2536755 -4.2200117 -4.1916237 -4.161202 -4.120523 -4.0782537 -4.041152 -4.0298128 -4.0612559 -4.0949931 -4.1213813 -4.1557722 -4.1960435 -4.2187743 -4.2241611][-4.2630973 -4.2339854 -4.2094655 -4.1794987 -4.1398897 -4.10294 -4.0726037 -4.047873 -4.0471439 -4.0433469 -4.0375972 -4.0672436 -4.1315255 -4.1833858 -4.21349][-4.2666688 -4.2484217 -4.2340651 -4.2177496 -4.1921148 -4.1681013 -4.1384358 -4.0917521 -4.0490484 -4.0035119 -3.9626024 -3.9795146 -4.0586128 -4.137434 -4.1870685][-4.2702093 -4.26164 -4.2582173 -4.2589512 -4.2543178 -4.2454448 -4.2184477 -4.159564 -4.0906353 -4.020391 -3.9638057 -3.9694145 -4.0444055 -4.1249008 -4.1748633][-4.2782965 -4.2775197 -4.282002 -4.2937236 -4.3023939 -4.301332 -4.2779489 -4.2233143 -4.1540751 -4.0870085 -4.03759 -4.0386567 -4.0938063 -4.1525841 -4.1847768][-4.2873459 -4.2898254 -4.2938447 -4.3036413 -4.3128614 -4.3165774 -4.2997313 -4.2559705 -4.2004032 -4.150176 -4.1159911 -4.11865 -4.1554689 -4.1931248 -4.2088475][-4.2979126 -4.3007345 -4.3010859 -4.3026218 -4.3082476 -4.3149691 -4.304677 -4.27652 -4.2404037 -4.2052259 -4.1845322 -4.1883597 -4.20931 -4.2305832 -4.235877][-4.3003244 -4.3060045 -4.3056326 -4.3040562 -4.3077426 -4.3141484 -4.310761 -4.2978988 -4.28016 -4.25897 -4.2441645 -4.2427707 -4.2489228 -4.25487 -4.2529082][-4.296103 -4.304276 -4.3055453 -4.3039136 -4.3049912 -4.3094373 -4.3091211 -4.3062172 -4.3008337 -4.289844 -4.2780328 -4.2734208 -4.2732353 -4.2714367 -4.2652359]]...]
INFO - root - 2017-12-07 14:25:07.405260: step 18710, loss = 2.10, batch loss = 2.04 (10.8 examples/sec; 0.741 sec/batch; 64h:36m:34s remains)
INFO - root - 2017-12-07 14:25:14.260442: step 18720, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 63h:27m:26s remains)
INFO - root - 2017-12-07 14:25:21.012407: step 18730, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 58h:02m:56s remains)
INFO - root - 2017-12-07 14:25:27.776707: step 18740, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 57h:06m:14s remains)
INFO - root - 2017-12-07 14:25:34.650100: step 18750, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 63h:02m:25s remains)
INFO - root - 2017-12-07 14:25:41.491812: step 18760, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 64h:28m:15s remains)
INFO - root - 2017-12-07 14:25:48.277151: step 18770, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 59h:15m:03s remains)
INFO - root - 2017-12-07 14:25:55.172258: step 18780, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 56h:14m:00s remains)
INFO - root - 2017-12-07 14:26:01.874241: step 18790, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 0.533 sec/batch; 46h:28m:10s remains)
INFO - root - 2017-12-07 14:26:08.602904: step 18800, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 64h:16m:39s remains)
2017-12-07 14:26:09.381642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2669997 -4.2600169 -4.2467847 -4.2252793 -4.2085395 -4.1952705 -4.1845546 -4.1658368 -4.1424842 -4.1225805 -4.1153116 -4.1155744 -4.1303191 -4.1503062 -4.1365356][-4.2644548 -4.2608795 -4.2502456 -4.2248125 -4.201355 -4.1777706 -4.1570783 -4.1330771 -4.1151552 -4.0999126 -4.0932007 -4.0901761 -4.0991559 -4.1239877 -4.1264305][-4.2647891 -4.2641683 -4.2616 -4.2416825 -4.2139797 -4.1854348 -4.1544929 -4.1222429 -4.1093278 -4.1041679 -4.0992103 -4.0868311 -4.0818615 -4.1069493 -4.120975][-4.2608514 -4.2601953 -4.2624035 -4.2475748 -4.2203326 -4.195961 -4.1641922 -4.134975 -4.1292357 -4.1361508 -4.1334782 -4.11027 -4.0948286 -4.1177883 -4.1398482][-4.2314129 -4.2273159 -4.2347469 -4.2269979 -4.1978478 -4.1695328 -4.1291671 -4.1027641 -4.111917 -4.1368165 -4.1417236 -4.1161327 -4.0984626 -4.1122379 -4.1316609][-4.1856213 -4.1734409 -4.1831665 -4.1832204 -4.1496997 -4.0951123 -4.0241289 -3.9834671 -4.0205746 -4.0785208 -4.10292 -4.0914345 -4.0816631 -4.0858407 -4.0977902][-4.1336803 -4.1086211 -4.1054635 -4.0987778 -4.059567 -3.9728827 -3.8535733 -3.7909412 -3.8795035 -3.9883869 -4.0438585 -4.0584984 -4.0637479 -4.066309 -4.0693378][-4.1029453 -4.0652 -4.0544338 -4.0504274 -4.0138063 -3.9138937 -3.7641244 -3.6804972 -3.8073773 -3.948699 -4.0201554 -4.049562 -4.0560451 -4.0564923 -4.0529237][-4.1086268 -4.0735221 -4.0729227 -4.0891147 -4.0755696 -4.0070624 -3.9057913 -3.8443153 -3.9231825 -4.0274343 -4.0797496 -4.1006522 -4.0903516 -4.075839 -4.0608225][-4.1227837 -4.102519 -4.1170559 -4.1549587 -4.17007 -4.1400876 -4.0814037 -4.0335464 -4.0636973 -4.1240659 -4.155777 -4.1632118 -4.1391778 -4.1107106 -4.0828762][-4.128562 -4.1217442 -4.1430392 -4.1908488 -4.2223244 -4.2102823 -4.1714644 -4.1354365 -4.1450582 -4.1799145 -4.199461 -4.2035866 -4.1827397 -4.1530194 -4.1137242][-4.1319432 -4.1273785 -4.1484094 -4.1920347 -4.2266936 -4.224072 -4.2005506 -4.1779571 -4.1827722 -4.2063193 -4.2237563 -4.2324939 -4.2215037 -4.1907549 -4.1426039][-4.1662722 -4.1615195 -4.1784062 -4.2107029 -4.2395945 -4.2404046 -4.2265363 -4.2148495 -4.2167435 -4.2320447 -4.2459731 -4.2544341 -4.2469244 -4.215785 -4.1686931][-4.2174582 -4.2102394 -4.2196832 -4.2361059 -4.2529716 -4.2544575 -4.2495966 -4.2461643 -4.2484875 -4.255959 -4.2627158 -4.267395 -4.2595611 -4.23012 -4.1884217][-4.2720394 -4.2610693 -4.2610598 -4.2652826 -4.2723012 -4.2730594 -4.2723842 -4.2724195 -4.2754049 -4.2785978 -4.2799287 -4.2800307 -4.2711735 -4.2462211 -4.2133389]]...]
INFO - root - 2017-12-07 14:26:16.192268: step 18810, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 56h:56m:41s remains)
INFO - root - 2017-12-07 14:26:23.003928: step 18820, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 61h:28m:20s remains)
INFO - root - 2017-12-07 14:26:29.877563: step 18830, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 63h:05m:13s remains)
INFO - root - 2017-12-07 14:26:36.450311: step 18840, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 55h:16m:36s remains)
INFO - root - 2017-12-07 14:26:43.147921: step 18850, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.618 sec/batch; 53h:51m:55s remains)
INFO - root - 2017-12-07 14:26:49.970664: step 18860, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 62h:23m:24s remains)
INFO - root - 2017-12-07 14:26:56.965694: step 18870, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 62h:49m:05s remains)
INFO - root - 2017-12-07 14:27:03.831347: step 18880, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 61h:33m:24s remains)
INFO - root - 2017-12-07 14:27:10.509736: step 18890, loss = 2.05, batch loss = 1.99 (15.7 examples/sec; 0.510 sec/batch; 44h:25m:53s remains)
INFO - root - 2017-12-07 14:27:17.318164: step 18900, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 59h:38m:01s remains)
2017-12-07 14:27:18.104465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.325151 -4.3229065 -4.3197584 -4.317955 -4.3188758 -4.3202672 -4.3231091 -4.3262744 -4.3291345 -4.3299127 -4.3286848 -4.3268933 -4.3243308 -4.322648 -4.3222661][-4.3287606 -4.3259277 -4.3215847 -4.3176804 -4.3169894 -4.3178229 -4.3199992 -4.3237667 -4.3274164 -4.3257051 -4.3203568 -4.3152342 -4.3101568 -4.3074017 -4.3054967][-4.3299289 -4.3249879 -4.3169303 -4.3068614 -4.303494 -4.3040438 -4.303297 -4.3049569 -4.3081756 -4.3037453 -4.2956438 -4.2875566 -4.277513 -4.2697077 -4.2646136][-4.3246183 -4.3159103 -4.3008862 -4.2836118 -4.2768955 -4.274426 -4.2636747 -4.2579684 -4.2593637 -4.2541385 -4.2457952 -4.2352681 -4.2202535 -4.2053576 -4.1980629][-4.3077617 -4.2918806 -4.2674074 -4.2433825 -4.2293591 -4.2182064 -4.1932817 -4.1762023 -4.1804023 -4.1818514 -4.1787057 -4.1662364 -4.1449885 -4.1225276 -4.1154661][-4.2826643 -4.2567892 -4.2204485 -4.1873021 -4.1578541 -4.1233039 -4.0710196 -4.0295238 -4.0411978 -4.0670261 -4.0792089 -4.06605 -4.04291 -4.023901 -4.0262613][-4.2487411 -4.214179 -4.1654987 -4.1181774 -4.0663996 -3.99475 -3.8941271 -3.8101966 -3.8357027 -3.9079938 -3.9534283 -3.95302 -3.9415379 -3.9389107 -3.9577508][-4.2138176 -4.1782756 -4.127759 -4.0757871 -4.013751 -3.9206524 -3.7945991 -3.6910312 -3.7358904 -3.8436024 -3.9117792 -3.9250736 -3.9260695 -3.9431624 -3.9765618][-4.2082419 -4.18796 -4.1587124 -4.1301227 -4.088284 -4.0160785 -3.9222386 -3.855495 -3.8960679 -3.9727106 -4.0210218 -4.0284715 -4.0239367 -4.04329 -4.0787873][-4.2191706 -4.2150908 -4.2081914 -4.2022982 -4.1840572 -4.1419735 -4.0900455 -4.0619936 -4.093781 -4.1344056 -4.1578193 -4.1592917 -4.147562 -4.1549892 -4.1761875][-4.2270665 -4.2322469 -4.240253 -4.2499976 -4.2483387 -4.2302971 -4.207458 -4.2001777 -4.2217155 -4.2405472 -4.2491517 -4.247436 -4.2358012 -4.2303925 -4.2339988][-4.2429166 -4.2530923 -4.2678871 -4.2853823 -4.2921977 -4.2854986 -4.2725677 -4.270328 -4.2847419 -4.2931194 -4.2954311 -4.2929873 -4.2821884 -4.2700367 -4.2612596][-4.2604551 -4.2690163 -4.283639 -4.2993608 -4.3045321 -4.2995915 -4.2883248 -4.2865644 -4.29789 -4.3030715 -4.3018346 -4.2960033 -4.2836752 -4.2681389 -4.2552767][-4.2762661 -4.2809572 -4.2915354 -4.3032131 -4.3066788 -4.3037553 -4.2954197 -4.2944074 -4.3028545 -4.305027 -4.3004966 -4.2915368 -4.278543 -4.2640214 -4.2525177][-4.2924895 -4.2954087 -4.3035107 -4.3118992 -4.31273 -4.30883 -4.3015704 -4.3012414 -4.308435 -4.3113894 -4.3082767 -4.3009014 -4.2910547 -4.2809615 -4.2735353]]...]
INFO - root - 2017-12-07 14:27:24.951038: step 18910, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 58h:38m:51s remains)
INFO - root - 2017-12-07 14:27:31.725561: step 18920, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 56h:10m:13s remains)
INFO - root - 2017-12-07 14:27:38.618261: step 18930, loss = 2.09, batch loss = 2.04 (11.4 examples/sec; 0.700 sec/batch; 60h:58m:10s remains)
INFO - root - 2017-12-07 14:27:45.507969: step 18940, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 63h:19m:12s remains)
INFO - root - 2017-12-07 14:27:52.204430: step 18950, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 58h:06m:23s remains)
INFO - root - 2017-12-07 14:27:59.006150: step 18960, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 57h:57m:36s remains)
INFO - root - 2017-12-07 14:28:05.860484: step 18970, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 55h:33m:00s remains)
INFO - root - 2017-12-07 14:28:12.801353: step 18980, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 63h:29m:20s remains)
INFO - root - 2017-12-07 14:28:19.406187: step 18990, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 0.518 sec/batch; 45h:08m:24s remains)
INFO - root - 2017-12-07 14:28:26.103786: step 19000, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 59h:44m:16s remains)
2017-12-07 14:28:26.859100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2944326 -4.3004675 -4.3079834 -4.30896 -4.3045611 -4.29798 -4.2841005 -4.2645345 -4.2580404 -4.2745953 -4.3005276 -4.3191795 -4.3306317 -4.3359995 -4.3355775][-4.2828832 -4.2894835 -4.3003259 -4.3045726 -4.300817 -4.2911935 -4.2689991 -4.2403502 -4.22948 -4.2496285 -4.2790656 -4.3027864 -4.3217258 -4.3313847 -4.3314657][-4.2718725 -4.2797942 -4.2942662 -4.3005614 -4.2949615 -4.2803726 -4.2499285 -4.2106042 -4.1941023 -4.2188425 -4.2519703 -4.280302 -4.3070235 -4.3227663 -4.325314][-4.2671652 -4.2756562 -4.2915468 -4.2958589 -4.2845583 -4.2638679 -4.2238564 -4.1710281 -4.147727 -4.1806741 -4.2207394 -4.2550607 -4.2896957 -4.3121953 -4.3187304][-4.2707853 -4.2784457 -4.2917447 -4.2899575 -4.27018 -4.2415752 -4.1908431 -4.124054 -4.0966797 -4.141727 -4.1904478 -4.2308903 -4.2741761 -4.3043714 -4.3144736][-4.2749767 -4.281301 -4.2896438 -4.2797942 -4.25012 -4.2144723 -4.1507435 -4.0664988 -4.037838 -4.09649 -4.1581144 -4.2079511 -4.2610779 -4.2995667 -4.3131342][-4.2737517 -4.2788372 -4.2807493 -4.2618351 -4.2201896 -4.1760979 -4.0960484 -3.9937325 -3.9697506 -4.0474019 -4.1263504 -4.188303 -4.2501483 -4.2958527 -4.3128114][-4.2657628 -4.2705932 -4.266891 -4.2400165 -4.1905713 -4.1362424 -4.0375385 -3.9219093 -3.9118693 -4.0112119 -4.1046252 -4.1759014 -4.2428808 -4.2930317 -4.3121743][-4.2544355 -4.2640343 -4.2589765 -4.2271 -4.1758027 -4.1175237 -4.0146456 -3.9040136 -3.9082088 -4.0135593 -4.1076279 -4.1787839 -4.2446213 -4.29333 -4.3116264][-4.2510729 -4.2682228 -4.2653313 -4.2345648 -4.1909318 -4.1426797 -4.0588293 -3.9696102 -3.9754574 -4.0607395 -4.136199 -4.1962376 -4.2540593 -4.2953796 -4.3105125][-4.2663312 -4.288795 -4.2875934 -4.2623796 -4.2342806 -4.2045674 -4.1485243 -4.0807915 -4.07009 -4.1202369 -4.1729703 -4.2212906 -4.2699475 -4.3012819 -4.3113775][-4.2858095 -4.3084049 -4.3088493 -4.2916946 -4.2761078 -4.259841 -4.2243814 -4.1723504 -4.14521 -4.1644692 -4.1999025 -4.2396703 -4.281507 -4.3062968 -4.3123922][-4.2904959 -4.3077884 -4.307642 -4.2972336 -4.2899032 -4.2819738 -4.2563672 -4.2138157 -4.1798868 -4.1864266 -4.2134891 -4.2464414 -4.2837353 -4.3060675 -4.3110709][-4.2762351 -4.2865224 -4.2866507 -4.2833595 -4.2819781 -4.2776232 -4.2549596 -4.2173319 -4.186873 -4.1939096 -4.2201428 -4.2489691 -4.2835226 -4.3042355 -4.3076415][-4.2517724 -4.255651 -4.2565255 -4.2604275 -4.2663307 -4.2674656 -4.2459855 -4.208653 -4.183455 -4.1960464 -4.2239609 -4.2524905 -4.2853546 -4.3033528 -4.3039036]]...]
INFO - root - 2017-12-07 14:28:33.622853: step 19010, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 62h:32m:11s remains)
INFO - root - 2017-12-07 14:28:40.475614: step 19020, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 60h:42m:17s remains)
INFO - root - 2017-12-07 14:28:47.192609: step 19030, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 58h:10m:19s remains)
INFO - root - 2017-12-07 14:28:53.903880: step 19040, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 55h:39m:50s remains)
INFO - root - 2017-12-07 14:29:00.689834: step 19050, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 59h:35m:28s remains)
INFO - root - 2017-12-07 14:29:07.654338: step 19060, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 63h:20m:26s remains)
INFO - root - 2017-12-07 14:29:14.337612: step 19070, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 58h:25m:56s remains)
INFO - root - 2017-12-07 14:29:21.092516: step 19080, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 56h:39m:56s remains)
INFO - root - 2017-12-07 14:29:27.715777: step 19090, loss = 2.06, batch loss = 2.00 (14.6 examples/sec; 0.547 sec/batch; 47h:37m:48s remains)
INFO - root - 2017-12-07 14:29:34.606249: step 19100, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 63h:47m:36s remains)
2017-12-07 14:29:35.254343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.289886 -4.2802415 -4.2723584 -4.2700157 -4.27175 -4.271543 -4.2651529 -4.250329 -4.2277627 -4.2042904 -4.1890869 -4.1893048 -4.2052031 -4.2339706 -4.2691145][-4.2819371 -4.268496 -4.2557759 -4.2481241 -4.2468009 -4.2459588 -4.2395921 -4.2242594 -4.1986132 -4.1714334 -4.1516833 -4.1481647 -4.1635818 -4.1979713 -4.2427025][-4.2760434 -4.2595415 -4.2407675 -4.2241364 -4.2161179 -4.2132058 -4.2063665 -4.190444 -4.1638007 -4.137784 -4.1183705 -4.1135721 -4.1293139 -4.1679897 -4.2201982][-4.2616429 -4.2419062 -4.21724 -4.1925025 -4.1782455 -4.1727061 -4.1649084 -4.1473808 -4.1198149 -4.097652 -4.0823545 -4.0805678 -4.1004229 -4.1442318 -4.2027183][-4.2451744 -4.2224326 -4.1918588 -4.1576676 -4.1345625 -4.12276 -4.1106796 -4.0904036 -4.0653253 -4.0509377 -4.0454063 -4.0522604 -4.0814815 -4.1331105 -4.1966023][-4.2333903 -4.2055445 -4.1670508 -4.122694 -4.0890203 -4.0669332 -4.0456052 -4.0212641 -4.0004 -3.9976542 -4.006166 -4.0277596 -4.0703492 -4.131207 -4.198801][-4.2374649 -4.2044115 -4.1583009 -4.1051621 -4.060986 -4.0281377 -3.9977796 -3.9703858 -3.9561334 -3.9676583 -3.9899421 -4.02284 -4.0726109 -4.1359673 -4.2034431][-4.2493863 -4.2145562 -4.1661844 -4.1104589 -4.0627279 -4.0237117 -3.9877079 -3.9565399 -3.944931 -3.9650347 -3.9956996 -4.0352964 -4.0860062 -4.1458416 -4.2097387][-4.2505031 -4.216651 -4.1714935 -4.1245089 -4.0901051 -4.0634155 -4.0352378 -4.0052919 -3.989754 -4.0006766 -4.0206852 -4.05253 -4.0983858 -4.1555281 -4.2170939][-4.2573442 -4.2278147 -4.1879745 -4.1499867 -4.1278076 -4.1172752 -4.1046686 -4.0873766 -4.0754251 -4.0765433 -4.0796256 -4.0945592 -4.1279707 -4.1774044 -4.2328348][-4.2781725 -4.2546096 -4.2200761 -4.187923 -4.1718163 -4.1686463 -4.1646004 -4.1556158 -4.1501727 -4.1485987 -4.1437063 -4.1476068 -4.1709995 -4.2120628 -4.2590013][-4.304718 -4.2868361 -4.2601323 -4.2364411 -4.2245741 -4.2242246 -4.2248721 -4.2201948 -4.2169352 -4.2139421 -4.2037 -4.1995816 -4.2140517 -4.2472196 -4.2850652][-4.3177872 -4.3045921 -4.2859821 -4.2712936 -4.266634 -4.2707939 -4.2766318 -4.2751937 -4.2724323 -4.2664089 -4.2511287 -4.24066 -4.2473378 -4.273056 -4.3027749][-4.3134341 -4.3047595 -4.2945595 -4.28765 -4.2867203 -4.2926745 -4.3007045 -4.3024669 -4.3007789 -4.2933307 -4.2787466 -4.2679996 -4.2712564 -4.2905583 -4.3128672][-4.307354 -4.3014145 -4.2964563 -4.2937965 -4.2928853 -4.2956114 -4.3009672 -4.3022137 -4.3001313 -4.2945476 -4.2862911 -4.2823319 -4.2874427 -4.3029537 -4.3190355]]...]
INFO - root - 2017-12-07 14:29:41.964866: step 19110, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 54h:51m:30s remains)
INFO - root - 2017-12-07 14:29:48.815217: step 19120, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 55h:53m:42s remains)
INFO - root - 2017-12-07 14:29:55.560252: step 19130, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 63h:03m:44s remains)
INFO - root - 2017-12-07 14:30:02.302912: step 19140, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 60h:38m:07s remains)
INFO - root - 2017-12-07 14:30:08.935488: step 19150, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 56h:20m:33s remains)
INFO - root - 2017-12-07 14:30:15.787847: step 19160, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 54h:50m:28s remains)
INFO - root - 2017-12-07 14:30:22.540603: step 19170, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 61h:23m:14s remains)
INFO - root - 2017-12-07 14:30:29.322937: step 19180, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.720 sec/batch; 62h:41m:15s remains)
INFO - root - 2017-12-07 14:30:36.040722: step 19190, loss = 2.07, batch loss = 2.01 (14.3 examples/sec; 0.558 sec/batch; 48h:33m:40s remains)
INFO - root - 2017-12-07 14:30:42.799950: step 19200, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 58h:40m:48s remains)
2017-12-07 14:30:43.527287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3271289 -4.3203387 -4.3168592 -4.3137951 -4.3124452 -4.3107362 -4.3095517 -4.3076677 -4.304966 -4.3034177 -4.3069205 -4.3167391 -4.3274016 -4.3336759 -4.3379893][-4.3081017 -4.2979178 -4.2921376 -4.2890921 -4.2906742 -4.2914958 -4.2899256 -4.2851887 -4.280241 -4.2777829 -4.28221 -4.2969861 -4.3146892 -4.3253036 -4.3313785][-4.2912326 -4.2786589 -4.2706976 -4.2690187 -4.27533 -4.2826867 -4.2827811 -4.2764807 -4.270556 -4.2702646 -4.2766094 -4.2923923 -4.3113661 -4.3222 -4.328351][-4.2797136 -4.2665744 -4.2573891 -4.2549186 -4.2632804 -4.2766666 -4.2808928 -4.2728744 -4.2667303 -4.2727284 -4.2836103 -4.2992134 -4.3148613 -4.3241735 -4.3276963][-4.2697783 -4.2560749 -4.2460723 -4.2406507 -4.2485533 -4.2659431 -4.2733278 -4.26228 -4.2550831 -4.2661443 -4.2812648 -4.2967739 -4.3061433 -4.3114972 -4.314414][-4.255156 -4.2341347 -4.214911 -4.1968427 -4.1931963 -4.2055373 -4.2143879 -4.1981788 -4.185029 -4.192574 -4.2100868 -4.2339015 -4.2489223 -4.2610855 -4.2754755][-4.235528 -4.1968856 -4.1596489 -4.1191669 -4.0953612 -4.0959435 -4.1031055 -4.0801425 -4.0544143 -4.0532918 -4.0819573 -4.1349959 -4.1734128 -4.2010183 -4.2305574][-4.2243423 -4.1735744 -4.1202478 -4.0601439 -4.017065 -4.0048413 -4.00407 -3.9793983 -3.9464369 -3.9389966 -3.9779007 -4.0605512 -4.1267691 -4.1694231 -4.2096066][-4.2297773 -4.1808763 -4.1312871 -4.0786695 -4.0407834 -4.0299764 -4.029439 -4.0133829 -3.9914007 -3.9899051 -4.0270896 -4.1047068 -4.1716933 -4.2105637 -4.2415986][-4.2536263 -4.2150574 -4.1815372 -4.1521564 -4.1359692 -4.1361837 -4.1393261 -4.1296363 -4.11884 -4.1207523 -4.1439 -4.1920476 -4.2412953 -4.2685757 -4.2869468][-4.2836742 -4.2559114 -4.2386556 -4.2299995 -4.2312903 -4.2412648 -4.2471967 -4.2432241 -4.236649 -4.2359042 -4.244041 -4.2671032 -4.2952056 -4.3101077 -4.3177204][-4.3136415 -4.2936544 -4.287755 -4.2902875 -4.2984018 -4.309269 -4.3149586 -4.3143244 -4.3102274 -4.3064666 -4.3073387 -4.31641 -4.3293214 -4.3354793 -4.3360796][-4.3347869 -4.3202229 -4.3177128 -4.3215427 -4.3275738 -4.3340039 -4.3379111 -4.3379846 -4.3357406 -4.3323584 -4.3301659 -4.332727 -4.3396921 -4.3428426 -4.3425126][-4.3474312 -4.33746 -4.3343282 -4.3357 -4.3385959 -4.3417377 -4.3429904 -4.3414474 -4.3377795 -4.3343825 -4.3334756 -4.3350368 -4.339098 -4.3422675 -4.3444228][-4.3548346 -4.3489194 -4.3448186 -4.3430419 -4.3425074 -4.3431215 -4.3428416 -4.340054 -4.3361888 -4.3343492 -4.3347607 -4.3364959 -4.3395867 -4.3435063 -4.3468347]]...]
INFO - root - 2017-12-07 14:30:50.326258: step 19210, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 62h:27m:56s remains)
INFO - root - 2017-12-07 14:30:57.133059: step 19220, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 58h:24m:23s remains)
INFO - root - 2017-12-07 14:31:03.955821: step 19230, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 56h:11m:27s remains)
INFO - root - 2017-12-07 14:31:10.834078: step 19240, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 59h:12m:24s remains)
INFO - root - 2017-12-07 14:31:17.737831: step 19250, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.723 sec/batch; 62h:54m:42s remains)
INFO - root - 2017-12-07 14:31:24.574411: step 19260, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 59h:07m:23s remains)
INFO - root - 2017-12-07 14:31:31.300374: step 19270, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 54h:47m:42s remains)
INFO - root - 2017-12-07 14:31:38.124300: step 19280, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 59h:21m:57s remains)
INFO - root - 2017-12-07 14:31:44.981507: step 19290, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 55h:18m:54s remains)
INFO - root - 2017-12-07 14:31:51.755576: step 19300, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 61h:40m:13s remains)
2017-12-07 14:31:52.561679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3106623 -4.3019524 -4.2982531 -4.2941446 -4.2805324 -4.2597036 -4.2505794 -4.2619267 -4.2697968 -4.2683282 -4.271163 -4.2810845 -4.2890649 -4.2801366 -4.2596245][-4.3108559 -4.3016057 -4.2941313 -4.2848382 -4.2646775 -4.2358446 -4.222785 -4.23972 -4.2547631 -4.2596345 -4.2676644 -4.2773833 -4.2841282 -4.2680039 -4.2336588][-4.3112969 -4.3007412 -4.2882938 -4.2723417 -4.2438655 -4.2082119 -4.1931415 -4.2166615 -4.2393436 -4.2541523 -4.2696915 -4.2867613 -4.2919559 -4.2657781 -4.2140155][-4.3093772 -4.2955475 -4.2800016 -4.2596087 -4.2256427 -4.1864419 -4.1716828 -4.19664 -4.2211466 -4.2430682 -4.26805 -4.2947311 -4.307332 -4.2762685 -4.2108932][-4.3033133 -4.2879767 -4.269937 -4.247261 -4.2122879 -4.17437 -4.1567 -4.1671686 -4.1836185 -4.2058487 -4.2402887 -4.2821779 -4.3112307 -4.292944 -4.2293963][-4.3032184 -4.2884173 -4.2696867 -4.2443075 -4.206027 -4.1628675 -4.1301885 -4.1142564 -4.1161509 -4.1393213 -4.1858587 -4.2467408 -4.2972732 -4.3040714 -4.2598238][-4.3068233 -4.2951674 -4.2782979 -4.2493715 -4.2034636 -4.1483779 -4.096282 -4.0525217 -4.0378685 -4.06548 -4.1278563 -4.2054772 -4.2709293 -4.296926 -4.2805815][-4.3030543 -4.292625 -4.2756524 -4.2457991 -4.1981344 -4.1367531 -4.0779214 -4.0197225 -3.9945598 -4.0217466 -4.0935583 -4.173317 -4.235188 -4.2678361 -4.2764421][-4.2819958 -4.2679744 -4.2461047 -4.2188869 -4.180778 -4.1308508 -4.0846558 -4.0375805 -4.0185995 -4.0422673 -4.1072807 -4.1709723 -4.2120714 -4.2334642 -4.246747][-4.2574654 -4.2391572 -4.2144337 -4.189733 -4.1626544 -4.1271005 -4.1009502 -4.0775452 -4.0746861 -4.0993929 -4.1500845 -4.1888556 -4.2076354 -4.2133174 -4.2186356][-4.2511115 -4.2336421 -4.2127371 -4.19183 -4.1675797 -4.1350584 -4.1174765 -4.1091022 -4.121573 -4.1525712 -4.1945248 -4.2151756 -4.2202764 -4.2145371 -4.2104573][-4.2576423 -4.2475686 -4.2355323 -4.2207627 -4.1972752 -4.1643691 -4.1474352 -4.1456022 -4.1653705 -4.1967597 -4.2295575 -4.24156 -4.2381558 -4.2266855 -4.2164779][-4.2689285 -4.2658339 -4.2656155 -4.2613831 -4.2447648 -4.2169895 -4.2014666 -4.203361 -4.2216291 -4.2478633 -4.2666841 -4.2684679 -4.2587619 -4.2477493 -4.237474][-4.2861533 -4.2890468 -4.2989297 -4.3038211 -4.2967849 -4.2772107 -4.2622547 -4.2603521 -4.2700157 -4.2860293 -4.29277 -4.2856674 -4.27154 -4.26212 -4.2574358][-4.2978878 -4.3021054 -4.3169804 -4.3282981 -4.3285608 -4.3178163 -4.30557 -4.3003712 -4.3009377 -4.3040657 -4.302783 -4.2923822 -4.2753758 -4.2650123 -4.2653775]]...]
INFO - root - 2017-12-07 14:31:59.464572: step 19310, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 62h:51m:20s remains)
INFO - root - 2017-12-07 14:32:06.227929: step 19320, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 59h:27m:43s remains)
INFO - root - 2017-12-07 14:32:13.000571: step 19330, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 57h:28m:07s remains)
INFO - root - 2017-12-07 14:32:19.698693: step 19340, loss = 2.09, batch loss = 2.04 (12.9 examples/sec; 0.620 sec/batch; 53h:55m:38s remains)
INFO - root - 2017-12-07 14:32:26.432660: step 19350, loss = 2.06, batch loss = 2.01 (13.0 examples/sec; 0.614 sec/batch; 53h:26m:41s remains)
INFO - root - 2017-12-07 14:32:33.155012: step 19360, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 63h:55m:25s remains)
INFO - root - 2017-12-07 14:32:39.971999: step 19370, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 61h:31m:36s remains)
INFO - root - 2017-12-07 14:32:46.757191: step 19380, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 61h:07m:54s remains)
INFO - root - 2017-12-07 14:32:53.396360: step 19390, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 55h:23m:15s remains)
INFO - root - 2017-12-07 14:33:00.144693: step 19400, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 55h:29m:31s remains)
2017-12-07 14:33:00.861288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3152161 -4.3140912 -4.3133135 -4.3151894 -4.3185287 -4.3217068 -4.3231664 -4.3220525 -4.3207622 -4.319047 -4.3172388 -4.3179374 -4.31816 -4.3151007 -4.30938][-4.3098822 -4.31142 -4.3141584 -4.31889 -4.3226905 -4.3233566 -4.3212547 -4.3161516 -4.3111763 -4.3070555 -4.3042517 -4.3053446 -4.3050489 -4.3013635 -4.2954245][-4.3077 -4.3136711 -4.319705 -4.3248749 -4.3236289 -4.3165107 -4.3064866 -4.2948513 -4.2853918 -4.2811537 -4.2807403 -4.2863021 -4.289546 -4.2889209 -4.2836895][-4.3093557 -4.3205609 -4.3291407 -4.330976 -4.3179193 -4.2939634 -4.2670341 -4.2431407 -4.2300253 -4.2320228 -4.2426281 -4.2609625 -4.2767963 -4.2834339 -4.279007][-4.303823 -4.3217964 -4.3348131 -4.3303 -4.296979 -4.2451243 -4.1895022 -4.145504 -4.1264482 -4.1419973 -4.1761727 -4.2178664 -4.2572546 -4.2777629 -4.276329][-4.2862091 -4.3084421 -4.324635 -4.3087416 -4.2501793 -4.1622057 -4.0670538 -3.9938917 -3.9704483 -4.0159411 -4.0883331 -4.1650624 -4.2346673 -4.2733312 -4.2799673][-4.2648263 -4.2853341 -4.2996712 -4.26815 -4.182272 -4.0557623 -3.9205835 -3.82161 -3.8043063 -3.8956451 -4.0142403 -4.1274266 -4.2231131 -4.2784715 -4.294858][-4.2604961 -4.2753563 -4.2822704 -4.2362285 -4.1325116 -3.9799595 -3.8161635 -3.7048347 -3.707315 -3.841768 -3.9918704 -4.12491 -4.2319484 -4.2925329 -4.313158][-4.2870479 -4.2949748 -4.2913594 -4.2395968 -4.1389551 -3.9906952 -3.831212 -3.7326131 -3.7568743 -3.89702 -4.0401874 -4.1668034 -4.2642422 -4.314527 -4.330739][-4.3200603 -4.323791 -4.31491 -4.2699943 -4.1911521 -4.0767469 -3.9586329 -3.8934085 -3.9214339 -4.0255189 -4.1327667 -4.2320023 -4.3059855 -4.3374615 -4.3427448][-4.3416657 -4.3444982 -4.3367572 -4.3075171 -4.25823 -4.1863351 -4.1148458 -4.0784245 -4.0962787 -4.1558409 -4.2248693 -4.2925172 -4.3391843 -4.3513613 -4.3453641][-4.3487644 -4.3508086 -4.3474183 -4.3351231 -4.3123875 -4.2763896 -4.2387724 -4.2189374 -4.22453 -4.2509122 -4.2901754 -4.3308711 -4.3536425 -4.3528972 -4.3403726][-4.344451 -4.3459778 -4.3460321 -4.3460865 -4.3415723 -4.3276796 -4.3091412 -4.2963467 -4.2940316 -4.3038478 -4.3253026 -4.3458505 -4.3532209 -4.3453951 -4.3307018][-4.3389974 -4.3390584 -4.3398676 -4.3453269 -4.3506036 -4.3490953 -4.3414383 -4.3327656 -4.327179 -4.3291931 -4.3384032 -4.3457203 -4.3451295 -4.3359518 -4.3235283][-4.3316665 -4.330862 -4.3311634 -4.3371162 -4.34559 -4.3502765 -4.3495855 -4.3456731 -4.3413777 -4.341064 -4.3438091 -4.3439188 -4.340344 -4.3331714 -4.3244128]]...]
INFO - root - 2017-12-07 14:33:07.654685: step 19410, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 59h:31m:40s remains)
INFO - root - 2017-12-07 14:33:14.301387: step 19420, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 57h:57m:10s remains)
INFO - root - 2017-12-07 14:33:20.980777: step 19430, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 55h:07m:59s remains)
INFO - root - 2017-12-07 14:33:27.799717: step 19440, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 57h:25m:47s remains)
INFO - root - 2017-12-07 14:33:34.596001: step 19450, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 62h:33m:18s remains)
INFO - root - 2017-12-07 14:33:41.076532: step 19460, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 55h:06m:56s remains)
INFO - root - 2017-12-07 14:33:47.762439: step 19470, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.615 sec/batch; 53h:27m:26s remains)
INFO - root - 2017-12-07 14:33:54.465208: step 19480, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 54h:31m:41s remains)
INFO - root - 2017-12-07 14:34:01.143354: step 19490, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 55h:30m:02s remains)
INFO - root - 2017-12-07 14:34:07.918078: step 19500, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 60h:00m:19s remains)
2017-12-07 14:34:08.669527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3507018 -4.3556919 -4.3555059 -4.3520417 -4.3470149 -4.3423347 -4.3391671 -4.3377633 -4.3371496 -4.3372521 -4.3386555 -4.34124 -4.3440695 -4.3461862 -4.347611][-4.3506002 -4.3536692 -4.3513122 -4.345603 -4.3396153 -4.3353791 -4.3341303 -4.3350682 -4.3362222 -4.3367071 -4.3373051 -4.3387027 -4.3406682 -4.3427458 -4.3451262][-4.3449965 -4.3420773 -4.3333778 -4.3211789 -4.3106527 -4.3058863 -4.3082356 -4.31505 -4.3232365 -4.3298407 -4.3339577 -4.3367724 -4.33897 -4.3413968 -4.3443351][-4.3233223 -4.31257 -4.2961497 -4.2762804 -4.2587404 -4.2506843 -4.2550669 -4.2689161 -4.287118 -4.3044677 -4.3169565 -4.3256812 -4.33162 -4.3367386 -4.3414354][-4.2844338 -4.2684755 -4.2469583 -4.2207923 -4.1948752 -4.1805367 -4.1833072 -4.2000031 -4.2256417 -4.2541471 -4.278635 -4.2974634 -4.3111291 -4.3214245 -4.32957][-4.2432985 -4.2267966 -4.2025571 -4.1694565 -4.1322861 -4.1083593 -4.1058111 -4.1206589 -4.1487303 -4.1831703 -4.2160687 -4.2453675 -4.2696795 -4.2892795 -4.3043342][-4.1923285 -4.1780672 -4.1545749 -4.1150136 -4.0658703 -4.0288849 -4.0179644 -4.0292664 -4.0561295 -4.0916834 -4.1302629 -4.1704574 -4.2080765 -4.2408643 -4.2675705][-4.1243911 -4.1152487 -4.0995178 -4.0628266 -4.0102792 -3.9653814 -3.94688 -3.9478426 -3.963048 -3.9906271 -4.0301723 -4.0784235 -4.1298652 -4.1789379 -4.2220917][-4.0592356 -4.0526361 -4.0470343 -4.0266943 -3.9936788 -3.9601889 -3.937849 -3.9251161 -3.923882 -3.9357798 -3.9652331 -4.0114336 -4.0682993 -4.1281643 -4.1851549][-4.0545449 -4.0451202 -4.0424562 -4.0360193 -4.0288467 -4.0228219 -4.0178003 -4.0109425 -4.00372 -4.0002308 -4.0113025 -4.04232 -4.0889659 -4.1416016 -4.1944227][-4.1389751 -4.1221061 -4.1111231 -4.1051431 -4.1100392 -4.1237569 -4.1380143 -4.1456642 -4.1463084 -4.14281 -4.14392 -4.1571569 -4.1822553 -4.2140126 -4.247993][-4.2361822 -4.22028 -4.2074585 -4.2020626 -4.2092781 -4.2269726 -4.2469611 -4.2604427 -4.2662826 -4.2662067 -4.2657557 -4.2698874 -4.2794209 -4.2929854 -4.3077011][-4.3025107 -4.2944169 -4.2887993 -4.2889562 -4.296731 -4.3101053 -4.3248162 -4.3344374 -4.3378496 -4.3370142 -4.3349581 -4.3345532 -4.3361506 -4.3396015 -4.3432908][-4.3407092 -4.3373132 -4.336699 -4.3395696 -4.3451304 -4.3519483 -4.3589754 -4.36336 -4.3645267 -4.3632526 -4.3605113 -4.3577232 -4.3557553 -4.3549523 -4.3541956][-4.3574538 -4.3560352 -4.3566179 -4.3588257 -4.3608012 -4.3620095 -4.3630743 -4.3638587 -4.3642435 -4.3640709 -4.3630638 -4.3608131 -4.3580709 -4.3554397 -4.3526444]]...]
INFO - root - 2017-12-07 14:34:15.486966: step 19510, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 62h:56m:04s remains)
INFO - root - 2017-12-07 14:34:22.228977: step 19520, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 60h:40m:15s remains)
INFO - root - 2017-12-07 14:34:29.020084: step 19530, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 57h:43m:13s remains)
INFO - root - 2017-12-07 14:34:35.773665: step 19540, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 54h:31m:35s remains)
INFO - root - 2017-12-07 14:34:42.694763: step 19550, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 57h:28m:07s remains)
INFO - root - 2017-12-07 14:34:49.502016: step 19560, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 62h:21m:42s remains)
INFO - root - 2017-12-07 14:34:56.382599: step 19570, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 61h:55m:16s remains)
INFO - root - 2017-12-07 14:35:03.339042: step 19580, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 54h:36m:06s remains)
INFO - root - 2017-12-07 14:35:09.986146: step 19590, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 54h:39m:43s remains)
INFO - root - 2017-12-07 14:35:16.903257: step 19600, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 61h:23m:56s remains)
2017-12-07 14:35:17.643888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3341546 -4.3330507 -4.33358 -4.3356328 -4.3300624 -4.3167076 -4.2943535 -4.2675171 -4.245 -4.2368565 -4.2473087 -4.270277 -4.293345 -4.3067131 -4.3144722][-4.3310428 -4.3261194 -4.3238544 -4.3265562 -4.3211994 -4.304729 -4.2761168 -4.2411256 -4.2122989 -4.201684 -4.2075419 -4.230258 -4.257216 -4.2746348 -4.2876105][-4.3242316 -4.3138747 -4.3066292 -4.3089027 -4.3025737 -4.2849441 -4.2497897 -4.2072845 -4.1738782 -4.1637888 -4.1718893 -4.1932974 -4.2227664 -4.2451935 -4.2634692][-4.3010755 -4.2860351 -4.27503 -4.2764468 -4.2700706 -4.2529678 -4.2149258 -4.1658511 -4.1350303 -4.13325 -4.1479874 -4.1674562 -4.193922 -4.2163424 -4.2368889][-4.2676768 -4.2479606 -4.23142 -4.2300062 -4.2242975 -4.2105064 -4.1796103 -4.1330752 -4.1097288 -4.1244178 -4.1466589 -4.15974 -4.1733108 -4.1875191 -4.205585][-4.2299433 -4.2005043 -4.1719222 -4.1632881 -4.157196 -4.1489062 -4.1270876 -4.0812178 -4.0646458 -4.1070113 -4.1458397 -4.1572423 -4.1572433 -4.1612329 -4.1750298][-4.1941338 -4.1513219 -4.1070547 -4.0852876 -4.0732408 -4.0608621 -4.0353622 -3.9830072 -3.9751091 -4.057251 -4.1279621 -4.1542425 -4.1564555 -4.1581569 -4.1661963][-4.1694126 -4.1175 -4.0588326 -4.0163231 -3.982784 -3.9473233 -3.8971488 -3.8288391 -3.8390257 -3.9726243 -4.0845327 -4.1357718 -4.1547089 -4.1640897 -4.1710248][-4.161335 -4.111155 -4.0485749 -3.9915769 -3.9368725 -3.8704314 -3.7831268 -3.6908245 -3.7203653 -3.8919189 -4.0343509 -4.1085377 -4.1448522 -4.1646781 -4.176156][-4.1831784 -4.1431961 -4.0915318 -4.039958 -3.9816778 -3.904809 -3.8064742 -3.7117262 -3.7327614 -3.8799343 -4.0110421 -4.0890203 -4.1330886 -4.1607552 -4.183331][-4.231451 -4.2046981 -4.1672559 -4.1273093 -4.0804696 -4.01186 -3.9244218 -3.8391674 -3.8303757 -3.917856 -4.0179095 -4.0901275 -4.136189 -4.1699939 -4.204895][-4.2831049 -4.2661047 -4.2421193 -4.215703 -4.1821928 -4.13008 -4.0626945 -3.9969618 -3.972755 -4.0146103 -4.085063 -4.1446109 -4.1841784 -4.2148557 -4.2492805][-4.3239832 -4.3127246 -4.2977705 -4.2819 -4.2608824 -4.2266278 -4.1823406 -4.1408682 -4.123847 -4.1462526 -4.1933441 -4.2376242 -4.265739 -4.2824512 -4.3008852][-4.34612 -4.3394842 -4.3314414 -4.322917 -4.3100038 -4.2870979 -4.2588391 -4.2369819 -4.2325974 -4.2511225 -4.2838082 -4.3135729 -4.3300714 -4.3346043 -4.3381581][-4.3497505 -4.3463278 -4.3417854 -4.33643 -4.3285913 -4.3140249 -4.2952194 -4.284245 -4.2871118 -4.3019276 -4.3234205 -4.3428168 -4.3522558 -4.351686 -4.3499761]]...]
INFO - root - 2017-12-07 14:35:24.465459: step 19610, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 56h:00m:25s remains)
INFO - root - 2017-12-07 14:35:31.292182: step 19620, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 56h:28m:53s remains)
INFO - root - 2017-12-07 14:35:38.179041: step 19630, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.737 sec/batch; 64h:01m:51s remains)
INFO - root - 2017-12-07 14:35:45.039356: step 19640, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 63h:30m:29s remains)
INFO - root - 2017-12-07 14:35:51.829421: step 19650, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 58h:10m:03s remains)
INFO - root - 2017-12-07 14:35:58.617286: step 19660, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.632 sec/batch; 54h:57m:24s remains)
INFO - root - 2017-12-07 14:36:05.502217: step 19670, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.649 sec/batch; 56h:26m:07s remains)
INFO - root - 2017-12-07 14:36:12.403712: step 19680, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 62h:23m:32s remains)
INFO - root - 2017-12-07 14:36:18.846950: step 19690, loss = 2.05, batch loss = 1.99 (15.3 examples/sec; 0.524 sec/batch; 45h:30m:04s remains)
INFO - root - 2017-12-07 14:36:25.440705: step 19700, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 59h:25m:27s remains)
2017-12-07 14:36:26.140872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.293767 -4.2671704 -4.2278423 -4.1901045 -4.1719933 -4.1755667 -4.173811 -4.15291 -4.1224537 -4.1185679 -4.1566048 -4.1993012 -4.217577 -4.2322335 -4.2539][-4.3073778 -4.2829714 -4.24361 -4.2020378 -4.1746097 -4.1646605 -4.151782 -4.127389 -4.0988846 -4.1028337 -4.146101 -4.1940651 -4.22063 -4.23966 -4.2623763][-4.3176537 -4.2939191 -4.2530756 -4.2045903 -4.1652794 -4.1428833 -4.1267395 -4.1086254 -4.08873 -4.09486 -4.1352897 -4.1848521 -4.2224588 -4.2464619 -4.2668457][-4.3190856 -4.2971354 -4.2562361 -4.2056236 -4.1564345 -4.125977 -4.1147933 -4.1093416 -4.1008344 -4.1103292 -4.1439142 -4.1865773 -4.2258692 -4.252996 -4.2700076][-4.3074141 -4.2855649 -4.2450862 -4.1930223 -4.1426039 -4.1135969 -4.1144557 -4.1276879 -4.1330247 -4.1488938 -4.1738181 -4.2018447 -4.2363458 -4.2639346 -4.2763743][-4.2853966 -4.2596216 -4.2152562 -4.1584411 -4.1095161 -4.0882425 -4.10303 -4.1319795 -4.1533747 -4.1812449 -4.2034431 -4.22259 -4.2536612 -4.2789063 -4.2879081][-4.2589989 -4.2267466 -4.1757126 -4.1168013 -4.07067 -4.0589557 -4.0864263 -4.1271276 -4.1626468 -4.2002225 -4.2276011 -4.2457 -4.2719603 -4.2918711 -4.2970958][-4.2309656 -4.1927056 -4.1421046 -4.0883918 -4.0491996 -4.0480189 -4.0871053 -4.133543 -4.1737967 -4.2172284 -4.2489247 -4.2687821 -4.2870708 -4.2988167 -4.3006449][-4.2082186 -4.1706028 -4.1285286 -4.0880151 -4.0578175 -4.064559 -4.1054273 -4.1503954 -4.1917071 -4.2347274 -4.2657514 -4.2833037 -4.2922153 -4.2958012 -4.2958288][-4.2026992 -4.1694574 -4.1364131 -4.1075335 -4.0867085 -4.097415 -4.1339054 -4.1736135 -4.2161903 -4.2544165 -4.2770596 -4.2855358 -4.2842851 -4.2822533 -4.28324][-4.2126956 -4.1863871 -4.1593466 -4.13748 -4.1225219 -4.1298 -4.1624393 -4.2012424 -4.2431951 -4.2724104 -4.2824736 -4.2802086 -4.2723875 -4.2683997 -4.2721052][-4.2259007 -4.2083087 -4.1887426 -4.1709886 -4.1589484 -4.1622663 -4.18797 -4.2225819 -4.2584114 -4.2802181 -4.2822437 -4.2759862 -4.2672114 -4.2637863 -4.2682805][-4.2372227 -4.2272305 -4.2167597 -4.2056937 -4.1977267 -4.19927 -4.2151079 -4.239182 -4.2622514 -4.2746673 -4.2753119 -4.2731276 -4.2679582 -4.2650952 -4.2694359][-4.24102 -4.2343397 -4.232172 -4.2316608 -4.2323647 -4.2344007 -4.240654 -4.2502594 -4.2599254 -4.265974 -4.2672014 -4.2694159 -4.2687597 -4.26877 -4.2726812][-4.2414103 -4.2349 -4.2363605 -4.2439966 -4.2531443 -4.2580132 -4.2582297 -4.2569404 -4.2564692 -4.2574449 -4.2593713 -4.2625437 -4.2650204 -4.2687931 -4.275476]]...]
INFO - root - 2017-12-07 14:36:32.996584: step 19710, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 62h:35m:11s remains)
INFO - root - 2017-12-07 14:36:39.847842: step 19720, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 57h:58m:17s remains)
INFO - root - 2017-12-07 14:36:46.596469: step 19730, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 55h:46m:27s remains)
INFO - root - 2017-12-07 14:36:53.337347: step 19740, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 0.620 sec/batch; 53h:51m:56s remains)
INFO - root - 2017-12-07 14:37:00.177159: step 19750, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 62h:41m:45s remains)
INFO - root - 2017-12-07 14:37:07.090650: step 19760, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 64h:06m:18s remains)
INFO - root - 2017-12-07 14:37:13.585655: step 19770, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 58h:38m:13s remains)
INFO - root - 2017-12-07 14:37:20.292603: step 19780, loss = 2.11, batch loss = 2.05 (12.7 examples/sec; 0.630 sec/batch; 54h:41m:22s remains)
INFO - root - 2017-12-07 14:37:26.961897: step 19790, loss = 2.05, batch loss = 1.99 (14.1 examples/sec; 0.567 sec/batch; 49h:14m:45s remains)
INFO - root - 2017-12-07 14:37:33.675386: step 19800, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.733 sec/batch; 63h:39m:48s remains)
2017-12-07 14:37:34.390695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3458681 -4.3412457 -4.3268728 -4.2905493 -4.2341766 -4.1692843 -4.0948758 -4.0559106 -4.0918751 -4.148 -4.1722631 -4.205018 -4.237412 -4.2734866 -4.2988849][-4.3533492 -4.3510885 -4.3317828 -4.2803936 -4.2059479 -4.1280346 -4.0440965 -4.010591 -4.0547853 -4.105763 -4.1335793 -4.1727495 -4.214201 -4.2560287 -4.2832689][-4.3537288 -4.3559694 -4.3360229 -4.2749915 -4.1909642 -4.10577 -4.0202246 -3.9867141 -4.0117273 -4.0378976 -4.0693502 -4.1199989 -4.1701851 -4.2188115 -4.2512093][-4.3470674 -4.3482943 -4.3274293 -4.2637539 -4.179111 -4.0913534 -4.0137315 -3.9736893 -3.9569428 -3.9445646 -3.979871 -4.0543385 -4.1183853 -4.1762562 -4.2151608][-4.3361759 -4.3333979 -4.3072352 -4.2412806 -4.1541343 -4.06432 -3.9923766 -3.9444935 -3.897794 -3.8615222 -3.9133868 -4.0082817 -4.0855017 -4.1507511 -4.1966033][-4.3242517 -4.3118763 -4.2771835 -4.2059221 -4.1062737 -4.000699 -3.9239995 -3.8857746 -3.859297 -3.850266 -3.9240513 -4.0223889 -4.0988812 -4.1611872 -4.2057528][-4.3150854 -4.2926135 -4.2473564 -4.1606975 -4.0353346 -3.9148653 -3.8399792 -3.8353286 -3.8642931 -3.9079239 -3.9976053 -4.0888219 -4.1563168 -4.2108188 -4.2454624][-4.29498 -4.2652683 -4.210084 -4.1071095 -3.9643338 -3.858706 -3.8205214 -3.8605027 -3.9297202 -3.9987674 -4.0881643 -4.1641483 -4.2193308 -4.2653141 -4.2903709][-4.2708383 -4.2431884 -4.1914854 -4.0994821 -3.9798324 -3.9104061 -3.9095788 -3.9656775 -4.0316973 -4.0913954 -4.1675773 -4.2270575 -4.2726684 -4.3073497 -4.3203769][-4.2384973 -4.2197728 -4.1856441 -4.1264443 -4.0548935 -4.0212731 -4.0353551 -4.0816922 -4.1296196 -4.1724319 -4.2324638 -4.2814183 -4.3164406 -4.3368511 -4.333787][-4.2004871 -4.1972985 -4.186039 -4.1666832 -4.1436434 -4.1369047 -4.1480803 -4.175869 -4.2085848 -4.2405558 -4.2827187 -4.3172255 -4.3409748 -4.3455296 -4.3256831][-4.1819425 -4.1983128 -4.2079535 -4.2159085 -4.2237825 -4.2268486 -4.2307549 -4.2496581 -4.2735915 -4.2959185 -4.319159 -4.3353138 -4.3443308 -4.3380113 -4.309453][-4.2037573 -4.2307444 -4.2497549 -4.2671762 -4.2848754 -4.2896357 -4.2909017 -4.3043785 -4.318769 -4.3299341 -4.3369288 -4.3403168 -4.3403478 -4.3297744 -4.3021684][-4.2534642 -4.2770948 -4.293951 -4.3096762 -4.3247571 -4.3284307 -4.329978 -4.338943 -4.3450208 -4.3455253 -4.3434758 -4.3408504 -4.3365903 -4.3259497 -4.3079438][-4.2981882 -4.3130283 -4.3223057 -4.3322473 -4.3411903 -4.342608 -4.3443871 -4.3495049 -4.3502512 -4.3454995 -4.3403721 -4.3368936 -4.3317814 -4.323194 -4.315227]]...]
INFO - root - 2017-12-07 14:37:41.071349: step 19810, loss = 2.07, batch loss = 2.01 (13.3 examples/sec; 0.600 sec/batch; 52h:05m:46s remains)
INFO - root - 2017-12-07 14:37:47.739708: step 19820, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 55h:25m:31s remains)
INFO - root - 2017-12-07 14:37:54.537256: step 19830, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 63h:05m:34s remains)
INFO - root - 2017-12-07 14:38:01.267671: step 19840, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 64h:00m:48s remains)
INFO - root - 2017-12-07 14:38:08.113673: step 19850, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 59h:10m:15s remains)
INFO - root - 2017-12-07 14:38:14.881495: step 19860, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 54h:40m:46s remains)
INFO - root - 2017-12-07 14:38:21.657336: step 19870, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 57h:05m:37s remains)
INFO - root - 2017-12-07 14:38:28.559749: step 19880, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 63h:12m:31s remains)
INFO - root - 2017-12-07 14:38:35.335719: step 19890, loss = 2.10, batch loss = 2.04 (14.6 examples/sec; 0.549 sec/batch; 47h:42m:13s remains)
INFO - root - 2017-12-07 14:38:42.105500: step 19900, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 55h:27m:44s remains)
2017-12-07 14:38:42.880898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.202177 -4.1412697 -4.0869861 -4.0737228 -4.0951853 -4.1378055 -4.16928 -4.1888695 -4.1891966 -4.1849918 -4.1908026 -4.1925139 -4.1892405 -4.1834216 -4.1831465][-4.2015162 -4.1410379 -4.1000476 -4.102138 -4.1241264 -4.1543512 -4.1725111 -4.1860795 -4.1844525 -4.1758437 -4.1772628 -4.1822438 -4.1881189 -4.1887584 -4.1903067][-4.2119126 -4.158998 -4.1314254 -4.1380477 -4.1519876 -4.1704454 -4.1785903 -4.180943 -4.1749859 -4.1685081 -4.1687789 -4.1717558 -4.1761827 -4.177763 -4.1793532][-4.2188778 -4.1766849 -4.1576409 -4.1601386 -4.1641569 -4.1768112 -4.17955 -4.1705108 -4.152792 -4.1444759 -4.1443753 -4.1382322 -4.1310325 -4.1325207 -4.1369872][-4.2271004 -4.1951213 -4.1776233 -4.1728334 -4.16903 -4.1729259 -4.1639934 -4.145649 -4.1188622 -4.1044559 -4.0969615 -4.0798593 -4.0614409 -4.0657549 -4.0805635][-4.2259903 -4.197576 -4.1758289 -4.1677513 -4.1591015 -4.1480865 -4.1250691 -4.0998173 -4.0737448 -4.0529442 -4.0390372 -4.02213 -4.0039096 -4.0107617 -4.0337448][-4.2127843 -4.182261 -4.1597157 -4.1555166 -4.140861 -4.1120076 -4.0740938 -4.0425224 -4.0151091 -3.99917 -3.992352 -3.99284 -3.990797 -4.0036154 -4.02684][-4.1998444 -4.1703596 -4.1465235 -4.1360211 -4.1120472 -4.0702472 -4.0238338 -3.9853702 -3.9651113 -3.9724219 -3.9877338 -4.0089359 -4.0248103 -4.0418911 -4.0591969][-4.193512 -4.1606951 -4.1280265 -4.1081533 -4.082129 -4.0433321 -4.0047722 -3.9753237 -3.9745061 -4.0052247 -4.0337749 -4.0582933 -4.0741282 -4.088654 -4.0987387][-4.1802192 -4.143425 -4.109694 -4.0946245 -4.0795493 -4.0583715 -4.0384355 -4.0282564 -4.0395813 -4.0670753 -4.0874166 -4.1007762 -4.10537 -4.1122341 -4.1170993][-4.1633592 -4.130219 -4.10487 -4.1025267 -4.1028914 -4.0985417 -4.0934958 -4.0932431 -4.1031222 -4.1117535 -4.1157804 -4.1189885 -4.1158023 -4.1152854 -4.1141071][-4.1643786 -4.1383882 -4.1236758 -4.1322474 -4.1436434 -4.1452212 -4.1421204 -4.1424193 -4.1491842 -4.144886 -4.136878 -4.1317816 -4.1247087 -4.120573 -4.1182828][-4.19299 -4.1743855 -4.1667767 -4.1802011 -4.1956816 -4.1974392 -4.1905594 -4.1867027 -4.193048 -4.18793 -4.1762958 -4.1669641 -4.1574707 -4.154088 -4.152442][-4.2350297 -4.2274394 -4.2259641 -4.2366686 -4.2477369 -4.2479453 -4.2391787 -4.2336912 -4.2393088 -4.2362328 -4.226634 -4.2181988 -4.2095647 -4.2049403 -4.2026467][-4.2750764 -4.275929 -4.27781 -4.2835293 -4.2888684 -4.2866392 -4.2785292 -4.2727132 -4.275486 -4.272172 -4.2664275 -4.2615557 -4.2567658 -4.2540736 -4.2527442]]...]
INFO - root - 2017-12-07 14:38:49.625869: step 19910, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.744 sec/batch; 64h:38m:25s remains)
INFO - root - 2017-12-07 14:38:56.496859: step 19920, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 58h:13m:10s remains)
INFO - root - 2017-12-07 14:39:03.285959: step 19930, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 56h:13m:01s remains)
INFO - root - 2017-12-07 14:39:10.108147: step 19940, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 56h:35m:01s remains)
INFO - root - 2017-12-07 14:39:16.907126: step 19950, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 59h:45m:21s remains)
INFO - root - 2017-12-07 14:39:23.736022: step 19960, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 62h:54m:12s remains)
INFO - root - 2017-12-07 14:39:30.579880: step 19970, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 57h:48m:02s remains)
INFO - root - 2017-12-07 14:39:37.365884: step 19980, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 54h:49m:22s remains)
INFO - root - 2017-12-07 14:39:44.030544: step 19990, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 0.528 sec/batch; 45h:50m:16s remains)
INFO - root - 2017-12-07 14:39:50.818461: step 20000, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 62h:28m:10s remains)
2017-12-07 14:39:51.516900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3186073 -4.31844 -4.3153667 -4.3167677 -4.3205829 -4.3228269 -4.3236942 -4.3221545 -4.3213511 -4.3192158 -4.3125124 -4.3035126 -4.3033419 -4.3101988 -4.3168035][-4.2997675 -4.3005509 -4.294538 -4.2946391 -4.3009033 -4.3067303 -4.3080506 -4.3021388 -4.3004518 -4.297411 -4.2855711 -4.2711344 -4.2713709 -4.28254 -4.2928352][-4.2683516 -4.2698441 -4.2607393 -4.2589641 -4.2664914 -4.2764144 -4.2787261 -4.2681704 -4.2647085 -4.2598076 -4.2445049 -4.2289877 -4.2311683 -4.2454405 -4.2594914][-4.2343607 -4.2330918 -4.2210021 -4.2151632 -4.2218075 -4.2315621 -4.2295051 -4.2070465 -4.1960545 -4.1915221 -4.1786709 -4.1693068 -4.1772075 -4.19806 -4.2210054][-4.2126017 -4.206543 -4.1906347 -4.1785188 -4.1778445 -4.1779022 -4.1619792 -4.1194544 -4.0999603 -4.104598 -4.1064868 -4.110312 -4.1325932 -4.1638932 -4.1970367][-4.1956682 -4.1836443 -4.1655498 -4.1481571 -4.1353521 -4.1167932 -4.079196 -4.005343 -3.9666259 -3.9896357 -4.0154419 -4.0362659 -4.0800409 -4.1270633 -4.1698933][-4.1842122 -4.1692643 -4.1457744 -4.1170626 -4.0822988 -4.0365267 -3.9655306 -3.8359127 -3.7673004 -3.8351083 -3.9029129 -3.9387441 -4.0093541 -4.0789075 -4.1345916][-4.1904793 -4.176333 -4.1458621 -4.0985565 -4.0341487 -3.9654422 -3.8597417 -3.6616764 -3.5617146 -3.6974154 -3.8055665 -3.8488514 -3.944762 -4.0385137 -4.1048131][-4.2090969 -4.2017388 -4.1687932 -4.1109319 -4.0397463 -3.9780872 -3.8817677 -3.6973281 -3.6145284 -3.7518926 -3.8475361 -3.8727121 -3.9554048 -4.0448771 -4.105063][-4.2367272 -4.2355933 -4.2100205 -4.1577764 -4.1032639 -4.0689893 -4.0145669 -3.897393 -3.8454409 -3.9273002 -3.9802787 -3.9846044 -4.0348997 -4.1000276 -4.1443729][-4.2602334 -4.2639265 -4.2480693 -4.2116857 -4.1773829 -4.1610675 -4.1393018 -4.0785284 -4.0459242 -4.0806255 -4.1057673 -4.1057854 -4.1330996 -4.174818 -4.2026405][-4.2765822 -4.2838054 -4.2727265 -4.2523541 -4.235857 -4.23045 -4.2293463 -4.2030358 -4.177484 -4.1796508 -4.19016 -4.1888127 -4.2032847 -4.2299466 -4.2470384][-4.2892632 -4.2987289 -4.2917943 -4.2798734 -4.2756166 -4.2746077 -4.2803054 -4.2691908 -4.2501154 -4.2390141 -4.2410655 -4.24064 -4.2470455 -4.262507 -4.2736726][-4.2940955 -4.3015742 -4.2950368 -4.2844882 -4.2800407 -4.278759 -4.2851248 -4.2814517 -4.2737608 -4.2667584 -4.2679338 -4.2700377 -4.2729058 -4.2820969 -4.2892451][-4.30314 -4.308116 -4.3024297 -4.2921309 -4.2847843 -4.2805376 -4.2837758 -4.2840071 -4.284049 -4.2818527 -4.2829466 -4.2869706 -4.2911363 -4.298419 -4.3028851]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 14:39:58.992487: step 20010, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 62h:39m:13s remains)
INFO - root - 2017-12-07 14:40:05.765685: step 20020, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 57h:56m:56s remains)
INFO - root - 2017-12-07 14:40:12.585301: step 20030, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.700 sec/batch; 60h:43m:13s remains)
INFO - root - 2017-12-07 14:40:19.333745: step 20040, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 54h:22m:19s remains)
INFO - root - 2017-12-07 14:40:26.088037: step 20050, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 55h:25m:14s remains)
INFO - root - 2017-12-07 14:40:32.895870: step 20060, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.742 sec/batch; 64h:21m:47s remains)
INFO - root - 2017-12-07 14:40:39.686308: step 20070, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 58h:20m:35s remains)
INFO - root - 2017-12-07 14:40:46.230011: step 20080, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 57h:32m:42s remains)
INFO - root - 2017-12-07 14:40:52.927700: step 20090, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 63h:08m:00s remains)
INFO - root - 2017-12-07 14:40:59.679911: step 20100, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 62h:50m:42s remains)
2017-12-07 14:41:00.333768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2747588 -4.2710781 -4.2743659 -4.2769904 -4.2746134 -4.2724347 -4.2676578 -4.2606635 -4.2610927 -4.2782941 -4.3003364 -4.3194518 -4.3304868 -4.3339953 -4.3338385][-4.2350521 -4.2254772 -4.2321267 -4.2390633 -4.2369676 -4.2370133 -4.2334828 -4.2250628 -4.2241693 -4.2470722 -4.277946 -4.3036027 -4.3172841 -4.3185129 -4.3179789][-4.2007818 -4.1829853 -4.193162 -4.2039185 -4.2009811 -4.2014332 -4.202714 -4.2002358 -4.1999741 -4.2275071 -4.2629461 -4.2906518 -4.3030357 -4.3038654 -4.3044353][-4.1870408 -4.16107 -4.1676254 -4.1721725 -4.166863 -4.1710181 -4.1841807 -4.1925721 -4.1983161 -4.2254357 -4.2607222 -4.28909 -4.29947 -4.2979684 -4.2986383][-4.1852889 -4.1607208 -4.1622629 -4.1576786 -4.1494908 -4.1536388 -4.1733561 -4.1914706 -4.204906 -4.2350435 -4.2668471 -4.2909513 -4.2978 -4.2941189 -4.2970977][-4.1745081 -4.1587334 -4.1614351 -4.1583819 -4.1480341 -4.1433859 -4.1605382 -4.1832671 -4.2037644 -4.2354817 -4.2640781 -4.283484 -4.2903161 -4.2895761 -4.2969379][-4.1701961 -4.1565394 -4.1615243 -4.1618919 -4.1460476 -4.1246958 -4.1326666 -4.16314 -4.1920514 -4.2197132 -4.2455559 -4.2659216 -4.2762747 -4.2791262 -4.2904334][-4.1657133 -4.1474161 -4.1555557 -4.1603374 -4.141861 -4.1000376 -4.09739 -4.13537 -4.1709538 -4.1986213 -4.2263703 -4.253232 -4.2667313 -4.2719769 -4.2821436][-4.1533246 -4.1367116 -4.1470046 -4.1527424 -4.1345086 -4.0844326 -4.0804858 -4.1252084 -4.1657085 -4.1935639 -4.2206903 -4.2528148 -4.2678833 -4.2742257 -4.279809][-4.1464019 -4.1326089 -4.1400151 -4.1492219 -4.1445441 -4.1010408 -4.09923 -4.1457849 -4.1870494 -4.2151136 -4.2378979 -4.2686763 -4.2800212 -4.2824364 -4.2832208][-4.1414676 -4.1232052 -4.1220074 -4.1347575 -4.1405897 -4.1031814 -4.1037817 -4.151227 -4.2003675 -4.2333078 -4.2558026 -4.2828426 -4.2936182 -4.2935114 -4.2902679][-4.1629992 -4.1403089 -4.1330442 -4.1432056 -4.1485443 -4.1140494 -4.1114888 -4.1551313 -4.2081428 -4.2466211 -4.2710042 -4.2949753 -4.3077278 -4.3092284 -4.3017836][-4.197104 -4.1851592 -4.184742 -4.1916528 -4.1879106 -4.152173 -4.1391726 -4.17061 -4.2207932 -4.2641482 -4.2935929 -4.3157678 -4.325716 -4.3248725 -4.3129592][-4.2154894 -4.2125435 -4.2195005 -4.2273617 -4.2225084 -4.1919303 -4.1760583 -4.1982322 -4.2381411 -4.2777109 -4.3072567 -4.3276577 -4.3332624 -4.3281727 -4.3157191][-4.2279363 -4.2289758 -4.2396693 -4.2511163 -4.2507257 -4.2296987 -4.219893 -4.2360859 -4.2618794 -4.2907753 -4.3159962 -4.3328514 -4.3349471 -4.3274717 -4.316184]]...]
INFO - root - 2017-12-07 14:41:07.063064: step 20110, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 54h:11m:34s remains)
INFO - root - 2017-12-07 14:41:13.901928: step 20120, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.725 sec/batch; 62h:54m:37s remains)
INFO - root - 2017-12-07 14:41:20.578246: step 20130, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 61h:04m:09s remains)
INFO - root - 2017-12-07 14:41:27.554083: step 20140, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 57h:39m:04s remains)
INFO - root - 2017-12-07 14:41:34.304412: step 20150, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 54h:34m:20s remains)
INFO - root - 2017-12-07 14:41:41.164549: step 20160, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 56h:41m:29s remains)
INFO - root - 2017-12-07 14:41:47.893189: step 20170, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 62h:36m:59s remains)
INFO - root - 2017-12-07 14:41:54.681422: step 20180, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 63h:01m:06s remains)
INFO - root - 2017-12-07 14:42:01.361452: step 20190, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 57h:20m:45s remains)
INFO - root - 2017-12-07 14:42:08.099764: step 20200, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 55h:59m:41s remains)
2017-12-07 14:42:08.822670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3061862 -4.3015585 -4.2949414 -4.2906885 -4.2906404 -4.2931442 -4.2976532 -4.3039703 -4.3105774 -4.3155084 -4.3177295 -4.3172593 -4.3148441 -4.3099685 -4.3024158][-4.3041282 -4.302227 -4.2968965 -4.292418 -4.2916803 -4.2929125 -4.29633 -4.3008947 -4.3050656 -4.3085117 -4.310956 -4.3115706 -4.3116231 -4.3094387 -4.3032479][-4.305635 -4.3050838 -4.3008947 -4.2961016 -4.2938328 -4.2908325 -4.2866273 -4.2810893 -4.2752728 -4.2720504 -4.273242 -4.2769704 -4.2837787 -4.2901349 -4.2905312][-4.2998123 -4.2977829 -4.2922306 -4.284514 -4.2778397 -4.2663331 -4.2488971 -4.2288046 -4.2102766 -4.1994205 -4.2018814 -4.2144356 -4.2331333 -4.2525511 -4.2635593][-4.2744932 -4.268302 -4.2580533 -4.244524 -4.2315187 -4.2114825 -4.1814613 -4.1477046 -4.115037 -4.094285 -4.1012216 -4.1305809 -4.1651092 -4.1967087 -4.2168503][-4.22373 -4.21016 -4.1929359 -4.1725521 -4.1576805 -4.1390934 -4.1060739 -4.0657811 -4.0211062 -3.9878492 -4.0002108 -4.051765 -4.1035209 -4.1442013 -4.16878][-4.159482 -4.1388273 -4.115489 -4.0900178 -4.0776582 -4.0683713 -4.0424151 -4.0056472 -3.9589174 -3.9204857 -3.9408712 -4.00954 -4.0694852 -4.1106181 -4.1360297][-4.1104922 -4.085865 -4.0618382 -4.03585 -4.0279894 -4.0321865 -4.0268908 -4.0150857 -3.9935164 -3.9748328 -3.994626 -4.0463219 -4.0890064 -4.11998 -4.1464534][-4.1051822 -4.0805221 -4.0585637 -4.0348229 -4.0308294 -4.0486789 -4.0690503 -4.08701 -4.0940681 -4.0979152 -4.1131825 -4.1377954 -4.1571536 -4.1760316 -4.1998935][-4.1456876 -4.1259713 -4.1077557 -4.089128 -4.0881872 -4.1113834 -4.1430707 -4.1741562 -4.1938295 -4.2069283 -4.2186513 -4.2271347 -4.2322731 -4.2411871 -4.2572441][-4.2104545 -4.1975007 -4.1841803 -4.1724591 -4.173316 -4.1916704 -4.217979 -4.2447095 -4.2615919 -4.2723694 -4.2805042 -4.2826729 -4.2816076 -4.2843804 -4.2926755][-4.2641559 -4.2592015 -4.2501154 -4.2413163 -4.2400656 -4.2495127 -4.2643085 -4.2816639 -4.291256 -4.2948241 -4.2970819 -4.2966752 -4.2961459 -4.2966738 -4.2997508][-4.2869039 -4.2871776 -4.2806821 -4.2718105 -4.2671432 -4.268147 -4.2727647 -4.27962 -4.2828627 -4.283772 -4.284194 -4.2848697 -4.2870469 -4.287756 -4.2888961][-4.2787657 -4.2796483 -4.27486 -4.26722 -4.2596726 -4.2522449 -4.2491255 -4.2507935 -4.2546449 -4.260324 -4.2654982 -4.270134 -4.2743387 -4.2747445 -4.2749982][-4.2593088 -4.2595272 -4.256659 -4.2518315 -4.2444358 -4.2327275 -4.2254725 -4.2273746 -4.2363777 -4.2492161 -4.2597241 -4.266696 -4.2706108 -4.2698812 -4.26911]]...]
INFO - root - 2017-12-07 14:42:15.723503: step 20210, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 61h:27m:44s remains)
INFO - root - 2017-12-07 14:42:22.614122: step 20220, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 58h:32m:19s remains)
INFO - root - 2017-12-07 14:42:29.394481: step 20230, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 56h:44m:21s remains)
INFO - root - 2017-12-07 14:42:36.150515: step 20240, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.620 sec/batch; 53h:47m:49s remains)
INFO - root - 2017-12-07 14:42:43.003276: step 20250, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 59h:51m:55s remains)
INFO - root - 2017-12-07 14:42:49.911245: step 20260, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 61h:36m:08s remains)
INFO - root - 2017-12-07 14:42:56.670163: step 20270, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 57h:17m:03s remains)
INFO - root - 2017-12-07 14:43:03.422181: step 20280, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 54h:44m:58s remains)
INFO - root - 2017-12-07 14:43:10.074613: step 20290, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 57h:25m:52s remains)
INFO - root - 2017-12-07 14:43:16.832221: step 20300, loss = 2.09, batch loss = 2.04 (11.0 examples/sec; 0.728 sec/batch; 63h:06m:39s remains)
2017-12-07 14:43:17.572261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2888422 -4.2769332 -4.2721505 -4.2751369 -4.2833209 -4.2929125 -4.2987895 -4.3010273 -4.2980413 -4.2934942 -4.2855387 -4.2744317 -4.2592087 -4.2514391 -4.2665558][-4.2864089 -4.269794 -4.2609596 -4.2595887 -4.26494 -4.2749853 -4.2856164 -4.2948027 -4.2940025 -4.2873273 -4.2763357 -4.2611294 -4.2462592 -4.2378583 -4.2507634][-4.2695751 -4.2465668 -4.2304363 -4.2227983 -4.2275229 -4.2448616 -4.265902 -4.2859588 -4.2920594 -4.2835822 -4.2665658 -4.2443933 -4.2229133 -4.2106829 -4.22365][-4.2551765 -4.2260213 -4.1961403 -4.1752982 -4.1814456 -4.2122555 -4.2473483 -4.2779317 -4.2952142 -4.2936068 -4.2748566 -4.2456541 -4.213377 -4.1899557 -4.19887][-4.2448521 -4.2105956 -4.1682844 -4.1312175 -4.1268892 -4.162508 -4.2123771 -4.2567277 -4.2894459 -4.3025789 -4.2921124 -4.2627697 -4.2222047 -4.1860561 -4.1864824][-4.2378736 -4.1994643 -4.147387 -4.0959005 -4.074584 -4.1008615 -4.1561584 -4.2216096 -4.2736478 -4.300715 -4.3017235 -4.2842326 -4.2503071 -4.2122927 -4.2014952][-4.2385569 -4.1976295 -4.1398063 -4.073451 -4.0237212 -4.01445 -4.0516047 -4.1360855 -4.2200737 -4.2709413 -4.2878 -4.2859874 -4.2722182 -4.2481294 -4.2340946][-4.247488 -4.2116008 -4.1580429 -4.0835466 -4.0039883 -3.9389799 -3.9258084 -4.0106449 -4.1252508 -4.2084951 -4.2522345 -4.2764955 -4.2880659 -4.2811146 -4.2695117][-4.2612405 -4.2399192 -4.2041688 -4.1433821 -4.0654778 -3.9774461 -3.9187832 -3.9552796 -4.0497236 -4.137753 -4.2028594 -4.2504086 -4.285893 -4.29888 -4.2967696][-4.2612624 -4.2563415 -4.2426233 -4.2115283 -4.1615386 -4.09531 -4.0361247 -4.02208 -4.0537934 -4.1061559 -4.1608276 -4.2128568 -4.2580323 -4.290247 -4.3071952][-4.2427793 -4.24464 -4.2458482 -4.2380238 -4.2200646 -4.1917577 -4.15992 -4.13597 -4.1305094 -4.1398091 -4.1555443 -4.1811275 -4.2165961 -4.2606153 -4.3000712][-4.216002 -4.2110925 -4.2167816 -4.222724 -4.2293262 -4.2334061 -4.2307596 -4.2221217 -4.2149072 -4.209969 -4.1963487 -4.1850419 -4.19057 -4.2220392 -4.2685132][-4.1987696 -4.1792912 -4.1777778 -4.1862984 -4.2021294 -4.2271619 -4.2505245 -4.2588625 -4.2587986 -4.2572665 -4.246201 -4.2215228 -4.2002769 -4.204083 -4.2347312][-4.184978 -4.160275 -4.1470547 -4.1425257 -4.1497068 -4.1789169 -4.2202997 -4.2469511 -4.2582521 -4.2628751 -4.2597179 -4.2431383 -4.2162046 -4.1978345 -4.2043076][-4.159543 -4.1445661 -4.1298332 -4.1159649 -4.1100926 -4.1267972 -4.1646476 -4.2006793 -4.2234054 -4.2371526 -4.2443466 -4.2417493 -4.2235575 -4.1994543 -4.1881509]]...]
INFO - root - 2017-12-07 14:43:24.350179: step 20310, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 55h:36m:48s remains)
INFO - root - 2017-12-07 14:43:31.074895: step 20320, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 58h:20m:16s remains)
INFO - root - 2017-12-07 14:43:37.944852: step 20330, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 63h:59m:05s remains)
INFO - root - 2017-12-07 14:43:44.764809: step 20340, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 59h:58m:44s remains)
INFO - root - 2017-12-07 14:43:51.465699: step 20350, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 59h:12m:24s remains)
INFO - root - 2017-12-07 14:43:58.267479: step 20360, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 58h:13m:57s remains)
INFO - root - 2017-12-07 14:44:05.058156: step 20370, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 57h:25m:34s remains)
INFO - root - 2017-12-07 14:44:11.828980: step 20380, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 62h:39m:56s remains)
INFO - root - 2017-12-07 14:44:18.210262: step 20390, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 57h:54m:44s remains)
INFO - root - 2017-12-07 14:44:24.865538: step 20400, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 53h:28m:00s remains)
2017-12-07 14:44:25.628537: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1959343 -4.2101631 -4.2195296 -4.2165608 -4.2089014 -4.2021103 -4.1936545 -4.1872311 -4.1879129 -4.196279 -4.2140865 -4.2358437 -4.2523303 -4.2588325 -4.25639][-4.1564388 -4.1630983 -4.1730428 -4.1722517 -4.162497 -4.1571388 -4.1526275 -4.14985 -4.1518703 -4.1560969 -4.1681304 -4.1860633 -4.2082949 -4.2302427 -4.2472572][-4.1234765 -4.1209583 -4.1319146 -4.1356263 -4.1301136 -4.1317978 -4.1344275 -4.1356583 -4.1346178 -4.1287789 -4.1270447 -4.133328 -4.1516361 -4.1805968 -4.2159777][-4.121243 -4.1164451 -4.1301851 -4.1405654 -4.1432066 -4.1498313 -4.153389 -4.1481285 -4.1342788 -4.1144147 -4.1012516 -4.103488 -4.1218534 -4.1537876 -4.1969752][-4.1457624 -4.1382217 -4.1510921 -4.1679945 -4.1798024 -4.1873169 -4.1827054 -4.1616235 -4.1304822 -4.0974121 -4.0807014 -4.0914121 -4.120832 -4.1588902 -4.2035904][-4.191308 -4.1807365 -4.1904721 -4.2086935 -4.21924 -4.2193 -4.2019978 -4.1643291 -4.1161108 -4.0714922 -4.05818 -4.088131 -4.1376796 -4.1860914 -4.2319493][-4.2520971 -4.2420721 -4.2476492 -4.2611537 -4.2640052 -4.2509184 -4.2196579 -4.1690626 -4.1107845 -4.064887 -4.0627851 -4.1084013 -4.1695194 -4.2212186 -4.2619267][-4.2956142 -4.2906852 -4.294816 -4.3032751 -4.3001432 -4.2780962 -4.2397504 -4.1875854 -4.1338649 -4.0976114 -4.10416 -4.1529775 -4.2121387 -4.2576017 -4.2869921][-4.3134432 -4.3142142 -4.3181887 -4.3222141 -4.3163581 -4.2948174 -4.2614422 -4.2200985 -4.1808 -4.1570044 -4.164537 -4.203752 -4.2500639 -4.2821269 -4.2975488][-4.2964358 -4.3033571 -4.3110509 -4.3164716 -4.3151193 -4.3032022 -4.2834635 -4.259613 -4.2376895 -4.2235184 -4.2241025 -4.2430654 -4.2696462 -4.286593 -4.2906909][-4.2558756 -4.2713175 -4.2856245 -4.2967854 -4.3034678 -4.3035884 -4.2989049 -4.291235 -4.2824736 -4.2727466 -4.2640882 -4.2643008 -4.2721586 -4.2768011 -4.2753439][-4.2279615 -4.2504153 -4.2692695 -4.2829247 -4.2919621 -4.2973027 -4.300704 -4.3024712 -4.3004379 -4.2930942 -4.2810473 -4.2718515 -4.26865 -4.2665343 -4.2642994][-4.2244353 -4.2477307 -4.2662516 -4.2775464 -4.28305 -4.2860117 -4.2884808 -4.2908807 -4.2907243 -4.2855 -4.2759104 -4.2680025 -4.26364 -4.2605286 -4.2600412][-4.2367048 -4.257648 -4.2732143 -4.2795186 -4.2790017 -4.2755618 -4.271348 -4.2680669 -4.264771 -4.2607036 -4.2561231 -4.25432 -4.2538719 -4.2543035 -4.2588177][-4.2543154 -4.2706814 -4.2825942 -4.2855844 -4.2813368 -4.2734666 -4.2635841 -4.2537942 -4.2455492 -4.2406306 -4.2392797 -4.241982 -4.24544 -4.2500248 -4.2590346]]...]
INFO - root - 2017-12-07 14:44:32.340435: step 20410, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 58h:59m:59s remains)
INFO - root - 2017-12-07 14:44:39.090102: step 20420, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.687 sec/batch; 59h:33m:04s remains)
INFO - root - 2017-12-07 14:44:45.906382: step 20430, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 55h:28m:57s remains)
INFO - root - 2017-12-07 14:44:52.717405: step 20440, loss = 2.02, batch loss = 1.96 (11.6 examples/sec; 0.687 sec/batch; 59h:31m:49s remains)
INFO - root - 2017-12-07 14:44:59.583102: step 20450, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.744 sec/batch; 64h:29m:54s remains)
INFO - root - 2017-12-07 14:45:06.336291: step 20460, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 62h:46m:33s remains)
INFO - root - 2017-12-07 14:45:13.151994: step 20470, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 59h:49m:34s remains)
INFO - root - 2017-12-07 14:45:19.930841: step 20480, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 55h:54m:54s remains)
INFO - root - 2017-12-07 14:45:26.547667: step 20490, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.610 sec/batch; 52h:53m:25s remains)
INFO - root - 2017-12-07 14:45:33.422502: step 20500, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 59h:05m:17s remains)
2017-12-07 14:45:34.182964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1812711 -4.1731958 -4.1720533 -4.196372 -4.214191 -4.21147 -4.1980915 -4.1685228 -4.1627874 -4.1805391 -4.1879559 -4.1888046 -4.1954041 -4.1843405 -4.1757197][-4.1616745 -4.166431 -4.1783972 -4.203258 -4.2202086 -4.2259355 -4.2257595 -4.2027655 -4.1960959 -4.2100077 -4.2100191 -4.2054968 -4.2015338 -4.1843481 -4.1777277][-4.1539788 -4.1720281 -4.1912556 -4.208909 -4.2127805 -4.21495 -4.2238722 -4.2088141 -4.2006526 -4.2067876 -4.1993308 -4.1952429 -4.1887445 -4.1759429 -4.1733718][-4.1754217 -4.195857 -4.2084918 -4.2114134 -4.1926193 -4.1788988 -4.1865864 -4.1834846 -4.1822519 -4.1851234 -4.1704812 -4.1625838 -4.1565585 -4.1519608 -4.1590548][-4.1989884 -4.2161369 -4.22031 -4.2078972 -4.1678071 -4.1320386 -4.1339068 -4.1483507 -4.1603532 -4.1604562 -4.1395111 -4.1229262 -4.11283 -4.1155443 -4.1354356][-4.2116981 -4.2210593 -4.2125578 -4.1827579 -4.1268945 -4.0663686 -4.0550952 -4.0879445 -4.1236639 -4.1290159 -4.1090503 -4.0897861 -4.0756726 -4.0820627 -4.1064463][-4.2096186 -4.2114449 -4.1891985 -4.1407542 -4.0661349 -3.9740839 -3.9328072 -3.9873145 -4.0584068 -4.0872135 -4.0832295 -4.0689058 -4.0535035 -4.0578117 -4.0754695][-4.2168989 -4.2062349 -4.1685758 -4.0969262 -3.9934254 -3.8602805 -3.7724464 -3.8504596 -3.9685564 -4.0377951 -4.0592265 -4.049932 -4.033844 -4.0310388 -4.0385928][-4.2331767 -4.2195497 -4.1813383 -4.1102476 -4.0069566 -3.8694863 -3.7548537 -3.8026683 -3.9124463 -3.9983249 -4.039814 -4.0309787 -4.0129514 -4.0074821 -4.0105476][-4.2373714 -4.2286391 -4.2062521 -4.1631603 -4.092217 -3.997541 -3.9045784 -3.8922367 -3.9344568 -3.9869344 -4.0206175 -4.0133476 -3.9978027 -3.9925644 -3.9991128][-4.2235179 -4.2278976 -4.2248683 -4.210814 -4.1767836 -4.1259069 -4.0638967 -4.0237632 -4.0169249 -4.0312567 -4.04876 -4.0358791 -4.0170164 -4.0085669 -4.0117054][-4.1992912 -4.2164512 -4.2309561 -4.2372446 -4.2309918 -4.2110786 -4.1790094 -4.1390209 -4.1139984 -4.10684 -4.1078467 -4.0914526 -4.0749187 -4.0642977 -4.0636625][-4.1749907 -4.1971955 -4.2257271 -4.2445526 -4.2541614 -4.2518868 -4.2350092 -4.2078543 -4.1846704 -4.1724162 -4.1690974 -4.1597357 -4.1510658 -4.1380863 -4.1355019][-4.1527748 -4.1702008 -4.2056565 -4.2364631 -4.25447 -4.2581143 -4.2503638 -4.2376089 -4.2210169 -4.2100754 -4.2091575 -4.2110996 -4.2152724 -4.2090831 -4.2016196][-4.1512775 -4.1549683 -4.1865029 -4.2174196 -4.2310452 -4.232698 -4.2347951 -4.2414145 -4.2385459 -4.233674 -4.2346067 -4.2399483 -4.2487144 -4.2485585 -4.243216]]...]
INFO - root - 2017-12-07 14:45:40.988769: step 20510, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 55h:44m:59s remains)
INFO - root - 2017-12-07 14:45:47.793083: step 20520, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 55h:21m:49s remains)
INFO - root - 2017-12-07 14:45:54.709476: step 20530, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 63h:17m:01s remains)
INFO - root - 2017-12-07 14:46:01.543949: step 20540, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 63h:21m:36s remains)
INFO - root - 2017-12-07 14:46:08.405990: step 20550, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 56h:32m:22s remains)
INFO - root - 2017-12-07 14:46:15.225900: step 20560, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 57h:25m:02s remains)
INFO - root - 2017-12-07 14:46:22.109225: step 20570, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 57h:35m:27s remains)
INFO - root - 2017-12-07 14:46:28.911188: step 20580, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.761 sec/batch; 65h:53m:58s remains)
INFO - root - 2017-12-07 14:46:35.483908: step 20590, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 62h:52m:53s remains)
INFO - root - 2017-12-07 14:46:42.165094: step 20600, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 59h:10m:12s remains)
2017-12-07 14:46:42.959024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2090054 -4.2194905 -4.2434134 -4.2626629 -4.26741 -4.2593393 -4.256176 -4.2674761 -4.2851472 -4.296114 -4.29122 -4.2615795 -4.2279239 -4.221293 -4.245616][-4.2176933 -4.22586 -4.2503958 -4.2687807 -4.2696214 -4.2533283 -4.2402043 -4.2465787 -4.2667875 -4.2813988 -4.2784734 -4.2495589 -4.2162147 -4.20958 -4.2341895][-4.1961823 -4.2088714 -4.2442818 -4.273294 -4.2754292 -4.2519021 -4.2289639 -4.2305155 -4.254261 -4.276042 -4.2781806 -4.2501807 -4.2151709 -4.2042909 -4.2224894][-4.1408792 -4.1605725 -4.2169189 -4.2636418 -4.2687154 -4.2378058 -4.2044234 -4.2038989 -4.2365375 -4.2734394 -4.28472 -4.2584381 -4.2204547 -4.2003393 -4.2073927][-4.0529957 -4.0860887 -4.1683125 -4.2311854 -4.2367096 -4.195683 -4.1527247 -4.1528711 -4.2001181 -4.2555928 -4.2768316 -4.2565928 -4.217526 -4.187675 -4.1827536][-3.9586303 -4.0122843 -4.1161661 -4.186605 -4.1843333 -4.1275864 -4.0733595 -4.0770469 -4.14058 -4.2126269 -4.2460785 -4.2369938 -4.2025151 -4.1676016 -4.1530762][-3.9141932 -3.9847484 -4.0892649 -4.14617 -4.1235189 -4.04596 -3.9812 -3.9940159 -4.0753746 -4.1647677 -4.2126575 -4.2163997 -4.1911273 -4.1561613 -4.1346431][-3.9765568 -4.0394692 -4.1067514 -4.1238875 -4.0726223 -3.9808106 -3.9187853 -3.9481549 -4.0407472 -4.138422 -4.1958256 -4.2068429 -4.188158 -4.1544065 -4.128365][-4.073586 -4.1086135 -4.1297297 -4.1106453 -4.0512071 -3.9789164 -3.9479761 -3.9872298 -4.0646267 -4.145071 -4.1962171 -4.208003 -4.1931777 -4.1598287 -4.129962][-4.1292534 -4.138051 -4.13001 -4.0969806 -4.0531635 -4.0200553 -4.0203271 -4.0565457 -4.1068416 -4.1611023 -4.1980562 -4.2074056 -4.1956987 -4.16436 -4.1315923][-4.1543651 -4.1432295 -4.116786 -4.0792527 -4.0513396 -4.0465441 -4.0651383 -4.0996947 -4.1357765 -4.1764307 -4.2055449 -4.2120953 -4.198946 -4.1679592 -4.1337872][-4.1728406 -4.1480184 -4.1101117 -4.0714564 -4.0539441 -4.0631795 -4.089426 -4.1246738 -4.1578922 -4.1942229 -4.2200217 -4.2227364 -4.2062902 -4.1748028 -4.1390839][-4.1907053 -4.1623092 -4.1247535 -4.0939007 -4.0843353 -4.0951729 -4.115737 -4.1433039 -4.1726761 -4.2059131 -4.2296481 -4.2313075 -4.2149172 -4.1845889 -4.1449666][-4.202971 -4.1795368 -4.1524696 -4.1336961 -4.1295733 -4.137332 -4.150589 -4.169137 -4.1923652 -4.2204709 -4.2413592 -4.2432084 -4.2294369 -4.2023067 -4.1577859][-4.2058148 -4.1917272 -4.1809177 -4.1756964 -4.1766248 -4.1814771 -4.1896186 -4.2029629 -4.219717 -4.2399874 -4.2553172 -4.2561593 -4.244122 -4.2183995 -4.1712613]]...]
INFO - root - 2017-12-07 14:46:49.840253: step 20610, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 62h:41m:28s remains)
INFO - root - 2017-12-07 14:46:56.561235: step 20620, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 60h:56m:53s remains)
INFO - root - 2017-12-07 14:47:03.258172: step 20630, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 57h:18m:25s remains)
INFO - root - 2017-12-07 14:47:10.049487: step 20640, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 56h:13m:38s remains)
INFO - root - 2017-12-07 14:47:16.815767: step 20650, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 57h:20m:42s remains)
INFO - root - 2017-12-07 14:47:23.605424: step 20660, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 63h:04m:34s remains)
INFO - root - 2017-12-07 14:47:30.427238: step 20670, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.748 sec/batch; 64h:45m:42s remains)
INFO - root - 2017-12-07 14:47:37.348399: step 20680, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 54h:48m:51s remains)
INFO - root - 2017-12-07 14:47:44.002701: step 20690, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 56h:36m:08s remains)
INFO - root - 2017-12-07 14:47:50.662069: step 20700, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:59m:59s remains)
2017-12-07 14:47:51.352039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3126988 -4.3080544 -4.3005867 -4.2877917 -4.2740197 -4.2619548 -4.2517228 -4.2422214 -4.2389469 -4.2399273 -4.2452326 -4.2545795 -4.2636819 -4.2691555 -4.2710471][-4.3008666 -4.3009386 -4.2979736 -4.2860732 -4.2712555 -4.254365 -4.239141 -4.2260032 -4.22199 -4.2229977 -4.2304783 -4.2439508 -4.258656 -4.2705016 -4.2767129][-4.2620254 -4.2636719 -4.2638435 -4.255578 -4.2408047 -4.2229338 -4.20725 -4.1937623 -4.19089 -4.1946883 -4.2101564 -4.2342281 -4.2579083 -4.275362 -4.2866621][-4.2234206 -4.2227817 -4.2204747 -4.2065606 -4.1791716 -4.1519055 -4.1323519 -4.1198492 -4.122859 -4.140645 -4.1732454 -4.2120757 -4.2447433 -4.2678542 -4.2829747][-4.1901131 -4.1878252 -4.1815729 -4.1569238 -4.1148562 -4.0802865 -4.0579414 -4.0473576 -4.0610576 -4.0943141 -4.1360793 -4.1802568 -4.2171307 -4.2419538 -4.2572813][-4.1677489 -4.162426 -4.1564946 -4.1354389 -4.1005335 -4.0717034 -4.0499678 -4.0410094 -4.0544972 -4.0831175 -4.1173506 -4.1558805 -4.1908855 -4.2146635 -4.226953][-4.1708827 -4.1642246 -4.1643715 -4.1575227 -4.142343 -4.126369 -4.1110859 -4.1033645 -4.10582 -4.1155944 -4.12997 -4.151679 -4.177866 -4.1975751 -4.207058][-4.1948662 -4.1917148 -4.1973219 -4.1985793 -4.19235 -4.1811247 -4.1674147 -4.159934 -4.1611547 -4.1662903 -4.1697774 -4.1737986 -4.1838078 -4.194232 -4.2002707][-4.2079172 -4.2096004 -4.2166786 -4.2187905 -4.2121515 -4.1980224 -4.1828032 -4.1778808 -4.18872 -4.2021379 -4.2060013 -4.20035 -4.2009 -4.2055697 -4.2103992][-4.217823 -4.2209425 -4.223681 -4.2219772 -4.2120976 -4.1958585 -4.1848879 -4.1871071 -4.2049303 -4.2257175 -4.2362719 -4.2316389 -4.2282348 -4.2274828 -4.2299395][-4.2302375 -4.237514 -4.2342496 -4.2253571 -4.2122669 -4.1956639 -4.1901531 -4.1951094 -4.2112083 -4.2338896 -4.2506189 -4.2550874 -4.2548304 -4.2519469 -4.2511654][-4.2231765 -4.2318473 -4.2261634 -4.2172575 -4.2072272 -4.1931844 -4.1891685 -4.1947246 -4.210053 -4.2356062 -4.2589293 -4.2703328 -4.2726331 -4.2708549 -4.2703042][-4.2020073 -4.2007422 -4.1910367 -4.1887269 -4.1853156 -4.1772618 -4.1736069 -4.1837192 -4.2073689 -4.2410583 -4.2707095 -4.2883339 -4.2952161 -4.2950406 -4.2937593][-4.1917429 -4.1624188 -4.1390705 -4.1362114 -4.1384706 -4.138978 -4.1425438 -4.1620159 -4.1972609 -4.2392044 -4.2739205 -4.2961521 -4.3088422 -4.3158827 -4.3189964][-4.1834297 -4.1277881 -4.0923991 -4.0830483 -4.0869994 -4.0985727 -4.1160059 -4.1416211 -4.1784592 -4.2223678 -4.2580924 -4.2801332 -4.2953763 -4.3096037 -4.3200212]]...]
INFO - root - 2017-12-07 14:47:58.072573: step 20710, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 55h:16m:38s remains)
INFO - root - 2017-12-07 14:48:04.982824: step 20720, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 61h:54m:34s remains)
INFO - root - 2017-12-07 14:48:11.821004: step 20730, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 63h:05m:51s remains)
INFO - root - 2017-12-07 14:48:18.682790: step 20740, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 60h:56m:15s remains)
INFO - root - 2017-12-07 14:48:25.387233: step 20750, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 54h:54m:33s remains)
INFO - root - 2017-12-07 14:48:32.189393: step 20760, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 56h:39m:21s remains)
INFO - root - 2017-12-07 14:48:38.935676: step 20770, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 61h:00m:43s remains)
INFO - root - 2017-12-07 14:48:45.623880: step 20780, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:56m:25s remains)
INFO - root - 2017-12-07 14:48:52.268938: step 20790, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.675 sec/batch; 58h:27m:08s remains)
INFO - root - 2017-12-07 14:48:58.992074: step 20800, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.611 sec/batch; 52h:54m:23s remains)
2017-12-07 14:48:59.708111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2892394 -4.293458 -4.2926145 -4.2891655 -4.2856083 -4.281579 -4.27996 -4.2848525 -4.2948432 -4.3051844 -4.3104687 -4.309834 -4.3052492 -4.300951 -4.29949][-4.2930608 -4.300818 -4.3011589 -4.2933655 -4.2795844 -4.2626758 -4.2509012 -4.2541628 -4.2717557 -4.2939587 -4.3090267 -4.3129282 -4.3088708 -4.302918 -4.2995305][-4.2779589 -4.2895036 -4.2910218 -4.2768912 -4.2480679 -4.2129574 -4.1878023 -4.1893563 -4.2185707 -4.25951 -4.2920961 -4.3069329 -4.3078675 -4.3027267 -4.2977767][-4.2403049 -4.2565103 -4.2600031 -4.2398205 -4.1932206 -4.1352072 -4.0938663 -4.0936546 -4.137012 -4.2019062 -4.2578955 -4.2885036 -4.297936 -4.2953095 -4.2896385][-4.1925797 -4.212965 -4.2199831 -4.196147 -4.1328549 -4.0488586 -3.9876773 -3.9843996 -4.0419955 -4.1324167 -4.2129087 -4.2596974 -4.2782035 -4.2786212 -4.2733326][-4.1647239 -4.1863818 -4.1958346 -4.1699376 -4.0937333 -3.9867721 -3.9073439 -3.8993607 -3.9678934 -4.0786657 -4.17713 -4.2343016 -4.2574072 -4.2591314 -4.2541184][-4.1851854 -4.2020583 -4.2087669 -4.1820207 -4.1044679 -3.9919567 -3.90465 -3.8907704 -3.9572134 -4.0703311 -4.171061 -4.2285852 -4.2510085 -4.2525306 -4.2477536][-4.2344522 -4.2442861 -4.2461615 -4.2235079 -4.1600676 -4.0656838 -3.9891009 -3.972599 -4.0234609 -4.11499 -4.19864 -4.24659 -4.2644477 -4.2655044 -4.2614355][-4.2761459 -4.2826996 -4.2827907 -4.2692714 -4.2302103 -4.1687212 -4.1157742 -4.1013803 -4.1306677 -4.1883779 -4.2434254 -4.2752147 -4.2855673 -4.285141 -4.281775][-4.295836 -4.3018165 -4.3029156 -4.2986221 -4.2816839 -4.2513618 -4.222568 -4.2119336 -4.2236004 -4.2516789 -4.2803097 -4.2962294 -4.2996459 -4.297574 -4.2951293][-4.29594 -4.3025665 -4.3051448 -4.3065329 -4.3030267 -4.2927709 -4.280931 -4.2742219 -4.2764091 -4.28679 -4.2981582 -4.3036242 -4.3026929 -4.300252 -4.2987933][-4.2886515 -4.2948666 -4.2981377 -4.3014917 -4.3038216 -4.3032036 -4.3003678 -4.297936 -4.2977352 -4.3002324 -4.3030071 -4.3033834 -4.3012428 -4.2997608 -4.2996073][-4.285193 -4.2897954 -4.2924619 -4.2953486 -4.2984595 -4.3007603 -4.3018374 -4.3023682 -4.3028212 -4.3032236 -4.3031645 -4.3021216 -4.3005166 -4.299684 -4.300014][-4.2877765 -4.2901497 -4.291069 -4.2921963 -4.2941952 -4.296504 -4.2987113 -4.3005953 -4.3018155 -4.3024187 -4.302053 -4.300879 -4.2995143 -4.29873 -4.2986865][-4.2938662 -4.294343 -4.2936654 -4.2933445 -4.2940712 -4.2955384 -4.2972517 -4.2986097 -4.2996888 -4.3006191 -4.3008423 -4.3001904 -4.2989631 -4.2976379 -4.2965117]]...]
INFO - root - 2017-12-07 14:49:06.473781: step 20810, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.745 sec/batch; 64h:29m:26s remains)
INFO - root - 2017-12-07 14:49:13.128629: step 20820, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 56h:40m:17s remains)
INFO - root - 2017-12-07 14:49:19.883165: step 20830, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 56h:21m:05s remains)
INFO - root - 2017-12-07 14:49:26.672227: step 20840, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 57h:01m:42s remains)
INFO - root - 2017-12-07 14:49:33.518421: step 20850, loss = 2.03, batch loss = 1.97 (10.8 examples/sec; 0.741 sec/batch; 64h:08m:05s remains)
INFO - root - 2017-12-07 14:49:40.418520: step 20860, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.765 sec/batch; 66h:11m:09s remains)
INFO - root - 2017-12-07 14:49:47.209445: step 20870, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 56h:41m:58s remains)
INFO - root - 2017-12-07 14:49:54.093817: step 20880, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 55h:14m:41s remains)
INFO - root - 2017-12-07 14:50:00.712070: step 20890, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 59h:15m:32s remains)
INFO - root - 2017-12-07 14:50:07.573680: step 20900, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 61h:20m:02s remains)
2017-12-07 14:50:08.255772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2724018 -4.293118 -4.3076034 -4.3156958 -4.3191895 -4.3202896 -4.3142471 -4.298305 -4.2809076 -4.2699618 -4.266336 -4.2669606 -4.2744036 -4.2852283 -4.2911448][-4.2887549 -4.3013973 -4.3090811 -4.3123131 -4.3126168 -4.3092256 -4.2990351 -4.2750969 -4.2464166 -4.2226658 -4.2082181 -4.2098885 -4.2298193 -4.2530346 -4.2664237][-4.2740107 -4.2785482 -4.2828503 -4.2831616 -4.2792754 -4.2700019 -4.2531986 -4.2198849 -4.1799831 -4.1415515 -4.1158738 -4.1204076 -4.1566281 -4.1960211 -4.2221713][-4.2284045 -4.2253342 -4.2244573 -4.220274 -4.2111459 -4.1961427 -4.1749711 -4.1405606 -4.1012163 -4.0600328 -4.0288491 -4.0379972 -4.08394 -4.1289492 -4.1595392][-4.1535439 -4.1484113 -4.1462755 -4.1348662 -4.1184978 -4.0985966 -4.0817847 -4.059248 -4.0313444 -3.9941571 -3.9624369 -3.9760392 -4.027451 -4.0721912 -4.0984735][-4.0595803 -4.0598269 -4.0628715 -4.0485392 -4.028944 -4.007338 -4.0036788 -4.0029907 -3.9939806 -3.9644225 -3.9344714 -3.9466867 -3.998543 -4.0430126 -4.065187][-3.968503 -3.9780812 -3.9883492 -3.981019 -3.9706473 -3.9576395 -3.9674487 -3.9872031 -4.0012875 -3.9887578 -3.9608161 -3.9596658 -3.9955351 -4.0325527 -4.054564][-3.91579 -3.9342072 -3.9500403 -3.9585595 -3.968998 -3.9731433 -3.9955993 -4.0282063 -4.0558891 -4.0599103 -4.0382142 -4.0251446 -4.0374703 -4.053802 -4.0706053][-3.9358053 -3.9622028 -3.9843848 -4.007659 -4.0320034 -4.0448413 -4.0660663 -4.0967169 -4.1242247 -4.1355066 -4.1247768 -4.1105928 -4.1046219 -4.0995312 -4.1090364][-4.0213571 -4.0517874 -4.0747623 -4.0986238 -4.1186461 -4.1296296 -4.1442246 -4.1686244 -4.1912422 -4.204535 -4.19982 -4.1881185 -4.1769953 -4.1662111 -4.1695342][-4.12465 -4.1484265 -4.162015 -4.1747422 -4.182682 -4.1886296 -4.1999674 -4.2196956 -4.2361784 -4.2456908 -4.2410097 -4.2311182 -4.2210345 -4.2121944 -4.2076669][-4.212472 -4.2251005 -4.2267318 -4.2263122 -4.2270522 -4.2324815 -4.2426696 -4.255127 -4.2612972 -4.2610722 -4.2534885 -4.2454762 -4.2377324 -4.2257881 -4.209516][-4.267673 -4.2723641 -4.2661338 -4.2579556 -4.2551441 -4.2607355 -4.269846 -4.2751 -4.2706676 -4.2588639 -4.2443051 -4.2342038 -4.2260871 -4.210495 -4.1873236][-4.2859797 -4.2864552 -4.2784448 -4.2697582 -4.2672515 -4.2707334 -4.2753563 -4.2726474 -4.2583756 -4.2368646 -4.2166719 -4.2060776 -4.2019835 -4.1918178 -4.17138][-4.2700963 -4.2743831 -4.2701569 -4.2625694 -4.2588296 -4.2564368 -4.2522798 -4.2417655 -4.2242146 -4.2050233 -4.18874 -4.1840353 -4.1900206 -4.1924782 -4.1819391]]...]
INFO - root - 2017-12-07 14:50:15.009488: step 20910, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 55h:08m:17s remains)
INFO - root - 2017-12-07 14:50:21.791785: step 20920, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 61h:24m:01s remains)
INFO - root - 2017-12-07 14:50:28.700303: step 20930, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 64h:33m:02s remains)
INFO - root - 2017-12-07 14:50:35.542696: step 20940, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 56h:56m:54s remains)
INFO - root - 2017-12-07 14:50:42.273238: step 20950, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 58h:03m:03s remains)
INFO - root - 2017-12-07 14:50:49.150638: step 20960, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.687 sec/batch; 59h:26m:50s remains)
INFO - root - 2017-12-07 14:50:56.051665: step 20970, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 61h:07m:03s remains)
INFO - root - 2017-12-07 14:51:02.831885: step 20980, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 58h:30m:05s remains)
INFO - root - 2017-12-07 14:51:09.578759: step 20990, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 57h:39m:31s remains)
INFO - root - 2017-12-07 14:51:16.359870: step 21000, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 55h:02m:06s remains)
2017-12-07 14:51:17.025709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3243155 -4.3272834 -4.3325682 -4.3365083 -4.3361788 -4.32738 -4.3070655 -4.2850189 -4.2740231 -4.2740831 -4.2906308 -4.3135047 -4.3251042 -4.32936 -4.3299985][-4.3203664 -4.3243213 -4.3329377 -4.3393273 -4.3391113 -4.3249106 -4.2934451 -4.2608304 -4.246666 -4.2489796 -4.2674618 -4.2918873 -4.306663 -4.3140945 -4.3193054][-4.306623 -4.3139 -4.3256183 -4.3357067 -4.3369436 -4.3147764 -4.2704759 -4.2250547 -4.2082887 -4.2197456 -4.2435646 -4.2663932 -4.2812314 -4.2918067 -4.2994642][-4.279357 -4.2911148 -4.3054953 -4.3202209 -4.3237743 -4.2949333 -4.2370648 -4.1766768 -4.1608109 -4.1919069 -4.2277522 -4.2507882 -4.2638378 -4.2737327 -4.2786307][-4.2364635 -4.250536 -4.2661972 -4.2844548 -4.288363 -4.2553282 -4.1822052 -4.1018548 -4.0900116 -4.1518345 -4.2084084 -4.2379575 -4.2513762 -4.2591333 -4.2609][-4.1998396 -4.2127967 -4.2288246 -4.2453151 -4.2438164 -4.204093 -4.110641 -4.003582 -3.9975822 -4.09806 -4.1833663 -4.2257857 -4.2431507 -4.2495594 -4.2509217][-4.1807995 -4.1908436 -4.2067528 -4.2193537 -4.2110071 -4.1575375 -4.034327 -3.8897996 -3.8877406 -4.0304842 -4.1439495 -4.2019119 -4.228416 -4.2397981 -4.2441554][-4.1887255 -4.1957746 -4.207428 -4.2149949 -4.2056756 -4.1437511 -4.0017443 -3.8277884 -3.8238089 -3.9881635 -4.1115746 -4.1780777 -4.2125216 -4.23082 -4.2399831][-4.2079139 -4.2119389 -4.2192087 -4.2262034 -4.2260075 -4.1776705 -4.0608788 -3.9153218 -3.9071951 -4.0322151 -4.1228657 -4.1727433 -4.2011747 -4.2176256 -4.226141][-4.22802 -4.2308159 -4.2373662 -4.2480669 -4.2576804 -4.23188 -4.1612959 -4.0721989 -4.063168 -4.1312122 -4.1761889 -4.1963105 -4.2050681 -4.2067461 -4.2052617][-4.2498221 -4.2521186 -4.2568436 -4.26586 -4.2764339 -4.2641063 -4.2256575 -4.1785069 -4.1739674 -4.2076931 -4.2267876 -4.2299242 -4.2248592 -4.2121224 -4.19756][-4.2638807 -4.2601743 -4.2599192 -4.2643867 -4.2713752 -4.2596869 -4.2316022 -4.2033272 -4.2032375 -4.2248878 -4.2367492 -4.2382011 -4.2352705 -4.2205982 -4.2019358][-4.2756782 -4.2626753 -4.2550664 -4.2581053 -4.2609715 -4.241044 -4.2088909 -4.1834569 -4.1820993 -4.2020292 -4.2158732 -4.2250538 -4.2341857 -4.2248526 -4.2086711][-4.2743492 -4.2539387 -4.2412438 -4.24698 -4.2479467 -4.2195392 -4.1813445 -4.1521468 -4.1452947 -4.1658654 -4.1834087 -4.1996775 -4.220068 -4.2204738 -4.2104983][-4.2649326 -4.2419195 -4.2315369 -4.2440414 -4.2487431 -4.2221804 -4.1872897 -4.1600466 -4.1490126 -4.1602378 -4.1695337 -4.1814671 -4.2057734 -4.2163577 -4.2132878]]...]
INFO - root - 2017-12-07 14:51:23.664475: step 21010, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 54h:00m:21s remains)
INFO - root - 2017-12-07 14:51:30.394666: step 21020, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 56h:08m:34s remains)
INFO - root - 2017-12-07 14:51:37.314475: step 21030, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 63h:52m:08s remains)
INFO - root - 2017-12-07 14:51:44.256317: step 21040, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 61h:52m:43s remains)
INFO - root - 2017-12-07 14:51:51.090927: step 21050, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 54h:46m:21s remains)
INFO - root - 2017-12-07 14:51:57.915233: step 21060, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 58h:45m:16s remains)
INFO - root - 2017-12-07 14:52:04.766392: step 21070, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 63h:01m:07s remains)
INFO - root - 2017-12-07 14:52:11.689522: step 21080, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 63h:32m:45s remains)
INFO - root - 2017-12-07 14:52:18.337877: step 21090, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 57h:46m:13s remains)
INFO - root - 2017-12-07 14:52:25.091622: step 21100, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 54h:06m:16s remains)
2017-12-07 14:52:25.774393: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24774 -4.2592697 -4.2640271 -4.264586 -4.2592268 -4.247601 -4.2332077 -4.2210574 -4.2127161 -4.2078314 -4.209166 -4.2150383 -4.2211728 -4.2268472 -4.2304344][-4.2691212 -4.2834158 -4.2877464 -4.2852478 -4.2759509 -4.261251 -4.2436523 -4.2276263 -4.2134609 -4.1998324 -4.1937213 -4.19637 -4.2016778 -4.2102118 -4.217731][-4.2969303 -4.3085103 -4.3108516 -4.3068137 -4.2974806 -4.2843747 -4.2658176 -4.2455649 -4.2275743 -4.2111835 -4.2007761 -4.1967039 -4.1955404 -4.2001629 -4.2075391][-4.3205113 -4.3261986 -4.3238511 -4.3125019 -4.2956243 -4.2758708 -4.2526836 -4.23113 -4.217473 -4.2113419 -4.2105918 -4.2111082 -4.2087088 -4.204905 -4.2051697][-4.3327789 -4.3308587 -4.3193154 -4.2963643 -4.2626438 -4.2240362 -4.1873527 -4.1576424 -4.1469502 -4.1571407 -4.1797347 -4.200707 -4.2124367 -4.2122884 -4.2130442][-4.3313313 -4.3210459 -4.2987237 -4.259809 -4.1950693 -4.1231337 -4.062993 -4.023243 -4.0190506 -4.0520568 -4.1032972 -4.1546607 -4.1898179 -4.2081919 -4.2213368][-4.3222408 -4.3025637 -4.2630153 -4.1961336 -4.0907617 -3.9723849 -3.8752751 -3.8266065 -3.8450725 -3.9074779 -3.9908357 -4.079205 -4.1451774 -4.1894755 -4.2212214][-4.31583 -4.2877989 -4.2309952 -4.1384754 -4.0051746 -3.8583152 -3.739248 -3.6860616 -3.7296898 -3.8139253 -3.9121904 -4.0160131 -4.1036329 -4.1695914 -4.2162256][-4.3232312 -4.2980137 -4.24634 -4.1624479 -4.0460844 -3.9267769 -3.8430865 -3.8153727 -3.8534844 -3.9158294 -3.9872262 -4.062583 -4.1324 -4.1851907 -4.2256165][-4.3335495 -4.3191347 -4.2856622 -4.2322106 -4.1593304 -4.0887341 -4.0434446 -4.0350413 -4.0580034 -4.088748 -4.1217766 -4.1574836 -4.1965413 -4.2247233 -4.2470832][-4.3302631 -4.32211 -4.3027396 -4.2721992 -4.2331276 -4.1970487 -4.1765084 -4.1737108 -4.1804066 -4.1905465 -4.2002234 -4.2109251 -4.2292719 -4.2421327 -4.251164][-4.3144593 -4.3062367 -4.2921858 -4.2727447 -4.2487097 -4.2304316 -4.2183127 -4.2117181 -4.2046375 -4.1984377 -4.1941814 -4.2002249 -4.2163157 -4.2257586 -4.2303143][-4.2898097 -4.2787046 -4.2644286 -4.25127 -4.2354302 -4.2189369 -4.2036591 -4.1877131 -4.1704588 -4.156723 -4.1532941 -4.1627178 -4.1753278 -4.1835566 -4.188745][-4.2620382 -4.249269 -4.2337079 -4.2217383 -4.2066159 -4.1866465 -4.16551 -4.13901 -4.113059 -4.0974188 -4.0991888 -4.1130791 -4.1255584 -4.1316304 -4.1403608][-4.2492766 -4.2391615 -4.2246647 -4.210691 -4.1918492 -4.1671624 -4.1431508 -4.1139917 -4.0877838 -4.0747728 -4.0763359 -4.0893259 -4.100193 -4.10348 -4.1144891]]...]
INFO - root - 2017-12-07 14:52:32.571281: step 21110, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 59h:11m:44s remains)
INFO - root - 2017-12-07 14:52:39.301224: step 21120, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 57h:01m:34s remains)
INFO - root - 2017-12-07 14:52:46.055531: step 21130, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 56h:31m:21s remains)
INFO - root - 2017-12-07 14:52:52.906905: step 21140, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 61h:39m:40s remains)
INFO - root - 2017-12-07 14:52:59.726649: step 21150, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 62h:50m:29s remains)
INFO - root - 2017-12-07 14:53:06.441088: step 21160, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 58h:14m:38s remains)
INFO - root - 2017-12-07 14:53:13.204435: step 21170, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.618 sec/batch; 53h:25m:56s remains)
INFO - root - 2017-12-07 14:53:19.971496: step 21180, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 58h:16m:20s remains)
INFO - root - 2017-12-07 14:53:26.731621: step 21190, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:50m:07s remains)
INFO - root - 2017-12-07 14:53:33.494259: step 21200, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 58h:00m:49s remains)
2017-12-07 14:53:34.240203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1946774 -4.1574311 -4.129766 -4.1129961 -4.1178756 -4.1350408 -4.1506562 -4.1549625 -4.1526284 -4.1523204 -4.1549568 -4.1759605 -4.2038932 -4.2222586 -4.2299781][-4.1710138 -4.1372752 -4.1167803 -4.1079836 -4.1201487 -4.1450372 -4.167881 -4.1774154 -4.1783166 -4.1757817 -4.1771808 -4.19601 -4.2211456 -4.2323351 -4.2330108][-4.146698 -4.1224504 -4.116272 -4.1204481 -4.1386094 -4.1654205 -4.1888547 -4.1964331 -4.1950779 -4.1889153 -4.1894608 -4.2047219 -4.2256665 -4.2333059 -4.2289429][-4.1237621 -4.1243367 -4.1413612 -4.1573162 -4.173481 -4.18957 -4.198143 -4.1905909 -4.1831079 -4.1794696 -4.1841416 -4.1984072 -4.2165818 -4.221035 -4.2141075][-4.112093 -4.1420979 -4.1822095 -4.202702 -4.2052221 -4.2001362 -4.1788955 -4.143116 -4.1243382 -4.133636 -4.1604152 -4.1840706 -4.1994166 -4.1980319 -4.1834407][-4.1325903 -4.1794453 -4.2241364 -4.2366753 -4.2165732 -4.178772 -4.1190276 -4.0537171 -4.031292 -4.0662413 -4.1304531 -4.1791539 -4.1956649 -4.189939 -4.1659937][-4.1867256 -4.2340417 -4.2673025 -4.2621098 -4.2138667 -4.1351614 -4.02686 -3.9305444 -3.9135575 -3.9865069 -4.0991168 -4.179585 -4.2008238 -4.1891894 -4.1539254][-4.2423439 -4.2817063 -4.3021088 -4.2857003 -4.2130833 -4.0943623 -3.9401922 -3.8193326 -3.8123345 -3.9152761 -4.0650272 -4.1710196 -4.197494 -4.1791496 -4.132936][-4.2763186 -4.305716 -4.3202748 -4.3040471 -4.2276373 -4.1007004 -3.9466867 -3.8393025 -3.8427265 -3.9413195 -4.0776482 -4.176743 -4.2034736 -4.1804729 -4.1308827][-4.2868347 -4.3113461 -4.3249183 -4.314292 -4.2537036 -4.1533294 -4.0391927 -3.9787579 -3.9974089 -4.06327 -4.1446457 -4.2076306 -4.2247643 -4.1958971 -4.1466932][-4.2811427 -4.3034472 -4.3170662 -4.3100634 -4.2662888 -4.1917639 -4.1142192 -4.0924568 -4.1180911 -4.1524348 -4.1891069 -4.2194662 -4.2247567 -4.1939373 -4.146687][-4.2680788 -4.2872524 -4.2995181 -4.2916188 -4.2556415 -4.1970692 -4.1412535 -4.1427326 -4.1736875 -4.1972957 -4.2130585 -4.2218342 -4.2180138 -4.1874194 -4.1392503][-4.2669787 -4.2841625 -4.2911048 -4.2788448 -4.2461529 -4.198976 -4.1610494 -4.1732759 -4.2074032 -4.2283154 -4.2354007 -4.2312689 -4.2148643 -4.1829152 -4.13657][-4.2802892 -4.2959704 -4.2950153 -4.2782555 -4.250721 -4.2168779 -4.1948962 -4.2116694 -4.2464828 -4.2635407 -4.2643151 -4.249877 -4.2234836 -4.1896796 -4.1497436][-4.2876568 -4.2991438 -4.2906694 -4.2706223 -4.250031 -4.23187 -4.2250981 -4.2448535 -4.2770319 -4.2913027 -4.2913127 -4.274859 -4.2456446 -4.2042742 -4.1627617]]...]
INFO - root - 2017-12-07 14:53:41.022552: step 21210, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 61h:22m:55s remains)
INFO - root - 2017-12-07 14:53:47.816154: step 21220, loss = 2.03, batch loss = 1.97 (10.8 examples/sec; 0.741 sec/batch; 64h:03m:51s remains)
INFO - root - 2017-12-07 14:53:54.569068: step 21230, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 58h:43m:21s remains)
INFO - root - 2017-12-07 14:54:01.339571: step 21240, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 55h:17m:38s remains)
INFO - root - 2017-12-07 14:54:08.187792: step 21250, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 58h:21m:43s remains)
INFO - root - 2017-12-07 14:54:14.975781: step 21260, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 62h:37m:13s remains)
INFO - root - 2017-12-07 14:54:21.868964: step 21270, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 60h:11m:01s remains)
INFO - root - 2017-12-07 14:54:28.684488: step 21280, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.677 sec/batch; 58h:29m:25s remains)
INFO - root - 2017-12-07 14:54:35.315272: step 21290, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 56h:31m:44s remains)
INFO - root - 2017-12-07 14:54:42.237204: step 21300, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 61h:14m:33s remains)
2017-12-07 14:54:42.927470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1884317 -4.2091355 -4.234561 -4.2635355 -4.2638688 -4.242887 -4.1969142 -4.1549664 -4.1627989 -4.1864514 -4.2027206 -4.2282238 -4.2554369 -4.2688127 -4.2797456][-4.1640196 -4.1877747 -4.2177663 -4.2467518 -4.241209 -4.2137785 -4.166976 -4.1229911 -4.1392665 -4.1823549 -4.205884 -4.2338881 -4.2601237 -4.273767 -4.2821822][-4.1555128 -4.1785016 -4.2036514 -4.2266769 -4.2130537 -4.1774955 -4.1272507 -4.0809121 -4.1018429 -4.1558852 -4.1924443 -4.2318792 -4.2620859 -4.2768164 -4.2844882][-4.1644669 -4.1804442 -4.18972 -4.2006297 -4.1883659 -4.1585846 -4.1105618 -4.0596066 -4.0746508 -4.1317329 -4.1782207 -4.2273426 -4.263772 -4.2785316 -4.2857289][-4.185955 -4.1866169 -4.1835637 -4.1822248 -4.17413 -4.157784 -4.1241336 -4.0772977 -4.0794497 -4.1319165 -4.1801805 -4.230269 -4.2678814 -4.2801452 -4.2864571][-4.1869836 -4.1708965 -4.1521006 -4.1400127 -4.1419754 -4.1468554 -4.1386786 -4.1095128 -4.1041679 -4.1451693 -4.1896648 -4.2336779 -4.2672229 -4.2783995 -4.2855062][-4.148263 -4.1228876 -4.0938187 -4.07701 -4.0943103 -4.12205 -4.1364 -4.1244678 -4.1157365 -4.1477017 -4.1890745 -4.2291965 -4.2611618 -4.2739158 -4.283217][-4.0861 -4.0665712 -4.0413957 -4.0347004 -4.0680342 -4.112144 -4.1388597 -4.1346459 -4.124824 -4.147366 -4.1828871 -4.2208161 -4.2540379 -4.2696066 -4.2815824][-4.0325637 -4.0388432 -4.0398307 -4.0487652 -4.0897079 -4.1357427 -4.1640053 -4.1640968 -4.1553025 -4.1671438 -4.1881976 -4.2198043 -4.2516985 -4.2686515 -4.2809567][-4.0372739 -4.0674543 -4.0957317 -4.1161928 -4.1457343 -4.1762357 -4.1953936 -4.2003922 -4.1950097 -4.1962481 -4.2051244 -4.2300897 -4.2573166 -4.270752 -4.2810035][-4.0995765 -4.137691 -4.1790366 -4.2023234 -4.2151089 -4.2248597 -4.2292504 -4.2305818 -4.231482 -4.2281485 -4.2275825 -4.2468872 -4.26672 -4.2739706 -4.2814116][-4.1814013 -4.214313 -4.2489853 -4.2666135 -4.2640996 -4.2567611 -4.2512493 -4.2473974 -4.25267 -4.2495232 -4.2429109 -4.2547693 -4.2674222 -4.2737164 -4.2813044][-4.2539768 -4.2719846 -4.2911897 -4.2986493 -4.2873282 -4.2650023 -4.2498231 -4.2462077 -4.2535996 -4.2493582 -4.2404265 -4.2493992 -4.262043 -4.2707477 -4.2806187][-4.2899814 -4.2948408 -4.302474 -4.302155 -4.284255 -4.2516484 -4.2298088 -4.2238412 -4.2278476 -4.2272158 -4.2249584 -4.2380705 -4.2568731 -4.269433 -4.2804413][-4.2966709 -4.2954378 -4.2990465 -4.2957473 -4.2683244 -4.22528 -4.2001114 -4.1955895 -4.2042332 -4.2115908 -4.2182417 -4.2352953 -4.2567072 -4.2706232 -4.2812705]]...]
INFO - root - 2017-12-07 14:54:49.587728: step 21310, loss = 2.05, batch loss = 1.99 (15.4 examples/sec; 0.518 sec/batch; 44h:47m:17s remains)
INFO - root - 2017-12-07 14:54:56.343254: step 21320, loss = 2.09, batch loss = 2.04 (11.3 examples/sec; 0.709 sec/batch; 61h:14m:50s remains)
INFO - root - 2017-12-07 14:55:03.266479: step 21330, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.736 sec/batch; 63h:35m:35s remains)
INFO - root - 2017-12-07 14:55:10.130181: step 21340, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 56h:49m:10s remains)
INFO - root - 2017-12-07 14:55:17.003866: step 21350, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 55h:09m:03s remains)
INFO - root - 2017-12-07 14:55:23.779193: step 21360, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 57h:55m:39s remains)
INFO - root - 2017-12-07 14:55:30.694704: step 21370, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 61h:43m:48s remains)
INFO - root - 2017-12-07 14:55:37.610709: step 21380, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.750 sec/batch; 64h:49m:50s remains)
INFO - root - 2017-12-07 14:55:44.223184: step 21390, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 58h:41m:03s remains)
INFO - root - 2017-12-07 14:55:51.113408: step 21400, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 56h:42m:57s remains)
2017-12-07 14:55:51.844552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2251821 -4.218431 -4.2285991 -4.2441359 -4.2630372 -4.2740145 -4.2783861 -4.28342 -4.2882581 -4.2739 -4.239522 -4.204936 -4.1969419 -4.2158227 -4.2488871][-4.2173066 -4.2075238 -4.2178307 -4.2338943 -4.2527266 -4.2626009 -4.2672582 -4.27351 -4.2794433 -4.2698984 -4.2426047 -4.206769 -4.1926918 -4.2089772 -4.2449646][-4.1942844 -4.1758428 -4.1814361 -4.2009683 -4.227766 -4.2444844 -4.2519674 -4.2584014 -4.266396 -4.2663822 -4.257617 -4.2346344 -4.2198305 -4.2282243 -4.2551913][-4.1653657 -4.1256123 -4.1134443 -4.1313004 -4.1686473 -4.201201 -4.2218938 -4.238678 -4.2517905 -4.2638078 -4.2757874 -4.2711215 -4.2591305 -4.2548242 -4.2641821][-4.1543546 -4.0937948 -4.0522861 -4.0462213 -4.0742888 -4.1151452 -4.1594968 -4.2022295 -4.2343397 -4.2589989 -4.2845392 -4.2949371 -4.2896047 -4.2772369 -4.269475][-4.1657143 -4.0955458 -4.0359058 -4.0001764 -3.9903104 -4.0059147 -4.0572729 -4.1317663 -4.1996388 -4.2465248 -4.2817359 -4.3016291 -4.300056 -4.2860112 -4.2678428][-4.1980667 -4.130672 -4.0690794 -4.0185475 -3.9716 -3.9370809 -3.9599442 -4.0464096 -4.1445169 -4.217104 -4.2650409 -4.2911925 -4.2942262 -4.2785397 -4.2575369][-4.2346611 -4.1772861 -4.1236272 -4.0769358 -4.0214753 -3.9610207 -3.9486437 -4.015276 -4.1075788 -4.1857553 -4.2427378 -4.2752924 -4.2825394 -4.2684779 -4.2475114][-4.2508097 -4.2099457 -4.167944 -4.1355653 -4.0984135 -4.0471511 -4.0189757 -4.053576 -4.1197782 -4.1760359 -4.220613 -4.2516809 -4.2634988 -4.2559476 -4.2380342][-4.2505717 -4.2228518 -4.1934137 -4.17332 -4.1595817 -4.1335068 -4.1117554 -4.1222248 -4.1570005 -4.1889677 -4.2111573 -4.2253933 -4.2338119 -4.2331295 -4.2216949][-4.2387834 -4.2185655 -4.1994944 -4.1902871 -4.19105 -4.1825442 -4.1718478 -4.1716185 -4.183722 -4.1983972 -4.2087169 -4.2157016 -4.2171307 -4.2122293 -4.2035813][-4.2387953 -4.2203565 -4.2024879 -4.1939688 -4.1982617 -4.1996284 -4.197165 -4.1935315 -4.19312 -4.1952372 -4.2028241 -4.2167063 -4.2221875 -4.2123122 -4.198009][-4.255856 -4.2434907 -4.2257986 -4.2141633 -4.2162056 -4.22316 -4.2206798 -4.2106495 -4.1983271 -4.1856766 -4.1895332 -4.2122116 -4.2278757 -4.2257934 -4.2133374][-4.2765479 -4.2719731 -4.2602077 -4.2498937 -4.249012 -4.2539954 -4.249177 -4.2330637 -4.2083316 -4.1823 -4.1744838 -4.1921611 -4.2117782 -4.2179689 -4.2162719][-4.2858033 -4.285347 -4.2805128 -4.2735629 -4.2726851 -4.2762709 -4.271956 -4.2566252 -4.2298474 -4.1973805 -4.17683 -4.1770263 -4.1844954 -4.1925254 -4.20046]]...]
INFO - root - 2017-12-07 14:55:58.753992: step 21410, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 60h:27m:12s remains)
INFO - root - 2017-12-07 14:56:05.454444: step 21420, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.618 sec/batch; 53h:24m:39s remains)
INFO - root - 2017-12-07 14:56:12.247087: step 21430, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 54h:26m:17s remains)
INFO - root - 2017-12-07 14:56:19.036579: step 21440, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 59h:57m:40s remains)
INFO - root - 2017-12-07 14:56:25.887633: step 21450, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 62h:19m:06s remains)
INFO - root - 2017-12-07 14:56:32.665842: step 21460, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 57h:03m:18s remains)
INFO - root - 2017-12-07 14:56:39.547478: step 21470, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.648 sec/batch; 55h:58m:38s remains)
INFO - root - 2017-12-07 14:56:46.338715: step 21480, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 54h:29m:21s remains)
INFO - root - 2017-12-07 14:56:53.044448: step 21490, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 62h:33m:30s remains)
INFO - root - 2017-12-07 14:56:59.956105: step 21500, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.767 sec/batch; 66h:15m:55s remains)
2017-12-07 14:57:00.652890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20903 -4.2249808 -4.2343564 -4.240303 -4.2432227 -4.2333088 -4.229352 -4.2312646 -4.2339191 -4.2375536 -4.2415175 -4.2468739 -4.252264 -4.2548671 -4.2508783][-4.2284393 -4.2407165 -4.2463779 -4.2558703 -4.2652273 -4.2597947 -4.2575579 -4.2595725 -4.2596841 -4.2572823 -4.2532067 -4.2524023 -4.2526021 -4.2506523 -4.2442088][-4.271647 -4.2821178 -4.2844992 -4.2943048 -4.3038278 -4.2983642 -4.2930827 -4.290144 -4.2860155 -4.278264 -4.2673426 -4.258472 -4.2521191 -4.2456546 -4.2373972][-4.3019733 -4.320672 -4.3315363 -4.3414116 -4.3436246 -4.3288465 -4.3111854 -4.2942829 -4.2817192 -4.2704825 -4.2580485 -4.2500339 -4.2444072 -4.2350044 -4.2264104][-4.3060288 -4.3254704 -4.336206 -4.341208 -4.3350916 -4.3122621 -4.283988 -4.2568421 -4.2391667 -4.2278476 -4.2193017 -4.2196279 -4.2202215 -4.214622 -4.2090769][-4.2992973 -4.3083563 -4.3049164 -4.29635 -4.2786231 -4.2446766 -4.2039123 -4.1686182 -4.1518574 -4.15156 -4.160522 -4.1783938 -4.1908851 -4.193644 -4.193327][-4.2652736 -4.2553806 -4.2317619 -4.2059517 -4.1768789 -4.1358652 -4.0889468 -4.0556364 -4.0550189 -4.0804396 -4.114994 -4.1488953 -4.1705251 -4.1781249 -4.18082][-4.191761 -4.1654716 -4.1320143 -4.099988 -4.0651655 -4.0141673 -3.9621119 -3.9350944 -3.961205 -4.0192318 -4.0780454 -4.1237197 -4.1511083 -4.1626539 -4.1656637][-4.136642 -4.0980825 -4.056953 -4.014349 -3.9628701 -3.8939736 -3.8389697 -3.8324487 -3.8925781 -3.9793177 -4.0531211 -4.1080775 -4.141696 -4.1549721 -4.15825][-4.1372085 -4.1057353 -4.06192 -4.0008979 -3.9253449 -3.8401115 -3.7913775 -3.8141143 -3.8993819 -3.9919882 -4.0639195 -4.1187654 -4.1514339 -4.1612959 -4.1619473][-4.1568036 -4.1351471 -4.0913935 -4.0212889 -3.9386973 -3.8673017 -3.8469794 -3.8889143 -3.9714594 -4.0482674 -4.1048756 -4.147296 -4.1678157 -4.1726923 -4.170022][-4.1715488 -4.1589017 -4.120091 -4.0531087 -3.9865718 -3.9432433 -3.939301 -3.9766707 -4.0393596 -4.096715 -4.1370807 -4.1643553 -4.1733012 -4.1726418 -4.1701832][-4.1906018 -4.1927252 -4.165823 -4.113822 -4.0665526 -4.0405617 -4.0418329 -4.0698705 -4.1176219 -4.1614919 -4.1867909 -4.1951141 -4.185554 -4.174747 -4.1704345][-4.2213798 -4.2334671 -4.2184372 -4.1823239 -4.1483531 -4.13052 -4.1369781 -4.1599975 -4.1951914 -4.2253094 -4.2358704 -4.2280192 -4.2068143 -4.1923351 -4.1873012][-4.2435102 -4.2566166 -4.2486339 -4.2254934 -4.2034621 -4.1928449 -4.2016034 -4.2201324 -4.2439632 -4.2598629 -4.2568889 -4.2392178 -4.218421 -4.2097168 -4.2082162]]...]
INFO - root - 2017-12-07 14:57:07.473106: step 21510, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 58h:12m:08s remains)
INFO - root - 2017-12-07 14:57:14.318439: step 21520, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 61h:42m:27s remains)
INFO - root - 2017-12-07 14:57:21.057267: step 21530, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 60h:11m:01s remains)
INFO - root - 2017-12-07 14:57:27.951939: step 21540, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.713 sec/batch; 61h:37m:06s remains)
INFO - root - 2017-12-07 14:57:34.681464: step 21550, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 55h:10m:23s remains)
INFO - root - 2017-12-07 14:57:41.489214: step 21560, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 55h:34m:20s remains)
INFO - root - 2017-12-07 14:57:48.339826: step 21570, loss = 2.09, batch loss = 2.04 (10.6 examples/sec; 0.751 sec/batch; 64h:53m:49s remains)
INFO - root - 2017-12-07 14:57:55.138790: step 21580, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 60h:06m:17s remains)
INFO - root - 2017-12-07 14:58:01.699973: step 21590, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 56h:26m:12s remains)
INFO - root - 2017-12-07 14:58:08.544216: step 21600, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 55h:08m:20s remains)
2017-12-07 14:58:09.262806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2207775 -4.2260303 -4.2314138 -4.2275815 -4.2198763 -4.2128015 -4.2136269 -4.2149453 -4.2137294 -4.2084174 -4.2062626 -4.2095284 -4.2204194 -4.2384615 -4.2598581][-4.225131 -4.2299109 -4.2338519 -4.2289186 -4.2174349 -4.2082062 -4.2066031 -4.2060261 -4.2047515 -4.2019305 -4.204433 -4.2110004 -4.2204328 -4.2362094 -4.2565594][-4.195262 -4.1956854 -4.19548 -4.1865029 -4.1688528 -4.1572866 -4.1575117 -4.161489 -4.1667871 -4.1739016 -4.1887927 -4.2045388 -4.21643 -4.2310677 -4.2503438][-4.1432695 -4.13719 -4.1330829 -4.1220713 -4.1026525 -4.093739 -4.1021128 -4.1169891 -4.1326461 -4.1505876 -4.1750822 -4.1971927 -4.2119966 -4.2263179 -4.2451921][-4.092608 -4.0850439 -4.0806947 -4.0703268 -4.0512052 -4.0435576 -4.0592012 -4.08384 -4.1098871 -4.1368093 -4.1659007 -4.1905513 -4.2074332 -4.2226562 -4.2423105][-4.0497961 -4.0439005 -4.0416183 -4.033134 -4.0143375 -4.0061049 -4.0258131 -4.055768 -4.0874243 -4.1221571 -4.15658 -4.1840267 -4.20372 -4.221005 -4.2424455][-4.03498 -4.0319767 -4.0308423 -4.0222521 -4.0023685 -3.9895549 -4.005446 -4.0330997 -4.0639782 -4.1026921 -4.1426578 -4.174809 -4.1995454 -4.2201943 -4.2437053][-4.0418224 -4.0399346 -4.0381641 -4.0267162 -4.003449 -3.9836953 -3.9913497 -4.0115724 -4.0383329 -4.0775247 -4.120533 -4.1566958 -4.1878095 -4.2138615 -4.2417574][-4.0420542 -4.0381608 -4.0344539 -4.0179791 -3.9930854 -3.9703016 -3.9739785 -3.990113 -4.0155263 -4.0559654 -4.1006503 -4.1394911 -4.1749 -4.2051945 -4.2374825][-4.0504484 -4.0457425 -4.0412588 -4.0254359 -4.0026808 -3.9826369 -3.9839034 -3.994498 -4.0173669 -4.0552559 -4.0977349 -4.1342235 -4.1699309 -4.2015038 -4.2357235][-4.0702581 -4.0680604 -4.067554 -4.0590177 -4.0421934 -4.0246239 -4.0206189 -4.0244403 -4.0406952 -4.0701056 -4.1043987 -4.1355615 -4.1695118 -4.2023945 -4.2375479][-4.1012993 -4.098979 -4.1008263 -4.0986128 -4.09007 -4.0787358 -4.074964 -4.0772481 -4.0881667 -4.108695 -4.1322379 -4.1541238 -4.1819453 -4.2123 -4.2442045][-4.1575975 -4.1554384 -4.1565132 -4.1562157 -4.152535 -4.147387 -4.1461468 -4.1479125 -4.1533232 -4.1636572 -4.1751409 -4.1850886 -4.2037215 -4.2284369 -4.2544289][-4.2020411 -4.2048683 -4.2078547 -4.2073474 -4.2041931 -4.2007375 -4.2007723 -4.2024322 -4.2051377 -4.2091913 -4.2130685 -4.2160387 -4.2280588 -4.2467613 -4.2661939][-4.1913061 -4.1969528 -4.201406 -4.200161 -4.196012 -4.19454 -4.1992331 -4.207705 -4.2167354 -4.2251425 -4.2324214 -4.2381878 -4.2491722 -4.2635884 -4.2776036]]...]
INFO - root - 2017-12-07 14:58:16.048324: step 21610, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:46m:19s remains)
INFO - root - 2017-12-07 14:58:22.744320: step 21620, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 53h:17m:02s remains)
INFO - root - 2017-12-07 14:58:29.497692: step 21630, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 58h:16m:49s remains)
INFO - root - 2017-12-07 14:58:36.332876: step 21640, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.720 sec/batch; 62h:07m:58s remains)
INFO - root - 2017-12-07 14:58:43.063360: step 21650, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.678 sec/batch; 58h:32m:02s remains)
INFO - root - 2017-12-07 14:58:49.832921: step 21660, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 58h:30m:21s remains)
INFO - root - 2017-12-07 14:58:56.700947: step 21670, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 56h:29m:32s remains)
INFO - root - 2017-12-07 14:59:03.471188: step 21680, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 62h:06m:28s remains)
INFO - root - 2017-12-07 14:59:10.191748: step 21690, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 60h:53m:19s remains)
INFO - root - 2017-12-07 14:59:17.054867: step 21700, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 58h:27m:30s remains)
2017-12-07 14:59:17.770115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1983242 -4.2154555 -4.2323203 -4.2372661 -4.2305074 -4.2168078 -4.1988668 -4.1874771 -4.1828833 -4.1908464 -4.2102838 -4.2260251 -4.2426114 -4.252418 -4.26636][-4.1789103 -4.2004871 -4.2129908 -4.2164879 -4.2106276 -4.1972814 -4.1757846 -4.161305 -4.1525059 -4.1597409 -4.1820765 -4.2017736 -4.2215233 -4.2328806 -4.2464709][-4.1593447 -4.187273 -4.1959057 -4.198348 -4.1910157 -4.1727438 -4.1429267 -4.1197567 -4.1043487 -4.1128988 -4.1436262 -4.1725235 -4.1954117 -4.2132726 -4.2296586][-4.1446886 -4.177475 -4.1859617 -4.1864209 -4.1764383 -4.1554022 -4.1232018 -4.0876818 -4.0582361 -4.0636611 -4.1022396 -4.1415458 -4.1713634 -4.1986294 -4.2192335][-4.1401567 -4.1718197 -4.1846013 -4.1887822 -4.1785822 -4.1565714 -4.1270876 -4.0861154 -4.048337 -4.0525756 -4.0986042 -4.14281 -4.1745768 -4.2034106 -4.2217693][-4.1429667 -4.165144 -4.1798196 -4.1843758 -4.1676397 -4.1409607 -4.1070743 -4.0591664 -4.0230126 -4.03848 -4.0988684 -4.15107 -4.18594 -4.2116365 -4.2269821][-4.1553383 -4.1717329 -4.1799808 -4.1751285 -4.1495123 -4.1112084 -4.0561419 -3.9794841 -3.935216 -3.975426 -4.0670094 -4.1381655 -4.1855426 -4.2128768 -4.2292418][-4.159451 -4.1714935 -4.1724553 -4.1607451 -4.128026 -4.0650125 -3.9687524 -3.8388987 -3.7735138 -3.8615279 -4.0037723 -4.1079068 -4.17649 -4.2153506 -4.2363076][-4.1617985 -4.1761341 -4.1784887 -4.1671882 -4.1307077 -4.0503869 -3.9283266 -3.7626567 -3.6765015 -3.7921977 -3.9625454 -4.0853133 -4.1731262 -4.2217884 -4.2469406][-4.1815276 -4.196825 -4.2017751 -4.1991415 -4.1751957 -4.1114593 -4.0099487 -3.8684175 -3.7974012 -3.8798685 -4.0177822 -4.1240292 -4.2042441 -4.2442355 -4.2637405][-4.2035112 -4.2169089 -4.2254076 -4.2325306 -4.2217312 -4.1779184 -4.1057358 -3.9998605 -3.9483242 -3.9976306 -4.09167 -4.1771021 -4.2448516 -4.2706532 -4.2784443][-4.2268624 -4.2391844 -4.2536845 -4.2679005 -4.263288 -4.228302 -4.1692533 -4.0797009 -4.0363941 -4.0681615 -4.134438 -4.2054715 -4.2678208 -4.2878823 -4.288156][-4.2418242 -4.2501416 -4.2677917 -4.283287 -4.2799678 -4.2475553 -4.195981 -4.1228657 -4.0876131 -4.1115193 -4.1620765 -4.2249517 -4.2791977 -4.2962632 -4.2953219][-4.2536006 -4.25228 -4.2646236 -4.277091 -4.2773914 -4.25477 -4.2138557 -4.1625881 -4.140285 -4.1578617 -4.1956897 -4.24854 -4.2916942 -4.3036952 -4.3035254][-4.2649274 -4.2578263 -4.2637415 -4.2734976 -4.2784262 -4.2663774 -4.2366204 -4.2044792 -4.1928287 -4.2067995 -4.2350984 -4.2756004 -4.30687 -4.3137712 -4.3118505]]...]
INFO - root - 2017-12-07 14:59:24.609503: step 21710, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 61h:01m:56s remains)
INFO - root - 2017-12-07 14:59:31.469915: step 21720, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.753 sec/batch; 65h:02m:09s remains)
INFO - root - 2017-12-07 14:59:38.305960: step 21730, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 56h:43m:55s remains)
INFO - root - 2017-12-07 14:59:45.121019: step 21740, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.635 sec/batch; 54h:48m:05s remains)
INFO - root - 2017-12-07 14:59:51.888516: step 21750, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 57h:48m:04s remains)
INFO - root - 2017-12-07 14:59:58.766505: step 21760, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 62h:03m:45s remains)
INFO - root - 2017-12-07 15:00:05.584014: step 21770, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 58h:54m:01s remains)
INFO - root - 2017-12-07 15:00:12.375720: step 21780, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 59h:01m:09s remains)
INFO - root - 2017-12-07 15:00:18.954931: step 21790, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 56h:40m:33s remains)
INFO - root - 2017-12-07 15:00:25.815078: step 21800, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.712 sec/batch; 61h:28m:12s remains)
2017-12-07 15:00:26.527715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1807051 -4.1736116 -4.16619 -4.1549621 -4.1306543 -4.1030231 -4.0920959 -4.1075959 -4.1493897 -4.1837506 -4.2132192 -4.2313833 -4.2315907 -4.2320328 -4.2326245][-4.1581926 -4.1581764 -4.1557865 -4.147006 -4.1174612 -4.0881491 -4.0769157 -4.0993476 -4.147625 -4.1930723 -4.2358117 -4.26395 -4.2708282 -4.2749171 -4.2750158][-4.1222191 -4.1333795 -4.139677 -4.1374912 -4.1136117 -4.089663 -4.0836477 -4.1092157 -4.1588974 -4.2080026 -4.2511377 -4.27964 -4.2889352 -4.2955666 -4.2957544][-4.0817046 -4.101172 -4.1168089 -4.1200442 -4.1030226 -4.0883546 -4.09503 -4.1285024 -4.1816854 -4.2277045 -4.2635684 -4.287159 -4.2962036 -4.3005524 -4.2976928][-4.0634413 -4.0897689 -4.1155529 -4.1206532 -4.105361 -4.0980177 -4.1129208 -4.1528382 -4.2060328 -4.2487059 -4.28098 -4.2996464 -4.3036032 -4.2994013 -4.2878809][-4.0800476 -4.1093588 -4.1394215 -4.1395168 -4.1166983 -4.1045647 -4.1152821 -4.1538024 -4.2079215 -4.2530103 -4.2841959 -4.3013077 -4.30364 -4.2911606 -4.2716932][-4.1179638 -4.1406274 -4.165638 -4.1583405 -4.1298485 -4.10788 -4.1076989 -4.1438808 -4.2012048 -4.2483249 -4.2782421 -4.2936263 -4.29436 -4.2767916 -4.2560306][-4.1631689 -4.1652284 -4.1717448 -4.1591039 -4.1337886 -4.1065245 -4.1007147 -4.1362357 -4.1886439 -4.2322397 -4.2599964 -4.2731552 -4.2697625 -4.2500696 -4.2352304][-4.1911058 -4.169992 -4.1508641 -4.1263442 -4.1008668 -4.0782509 -4.0823092 -4.1186728 -4.1610422 -4.1977539 -4.2230573 -4.2338295 -4.2287164 -4.2166543 -4.2150435][-4.1867509 -4.1501665 -4.1138363 -4.0799594 -4.0562553 -4.0495548 -4.0746832 -4.1161642 -4.1509981 -4.1806216 -4.1997104 -4.2046428 -4.198524 -4.1952643 -4.2027903][-4.1672115 -4.1317291 -4.0980906 -4.069273 -4.0549541 -4.064271 -4.1015143 -4.1458688 -4.1788025 -4.2018681 -4.2123923 -4.2098789 -4.2018504 -4.1993504 -4.2057838][-4.1397557 -4.1218524 -4.110477 -4.1051092 -4.1081295 -4.1262445 -4.1623487 -4.2009249 -4.2312961 -4.2502112 -4.2573252 -4.2524991 -4.2422848 -4.2325749 -4.2277279][-4.1135917 -4.1122713 -4.1251869 -4.1452484 -4.1662169 -4.19021 -4.2242622 -4.2573452 -4.2836938 -4.3014884 -4.3099036 -4.3062191 -4.2933893 -4.2748413 -4.2551789][-4.1021514 -4.1110888 -4.1378727 -4.1731071 -4.2074146 -4.2386808 -4.2693892 -4.2936115 -4.3131704 -4.3276577 -4.3357606 -4.3346963 -4.3239579 -4.304687 -4.28015][-4.1108022 -4.1294317 -4.1635609 -4.2040963 -4.2411 -4.2707334 -4.292738 -4.3046837 -4.3139653 -4.3229489 -4.3287349 -4.3302917 -4.3262768 -4.3150558 -4.2977357]]...]
INFO - root - 2017-12-07 15:00:33.324961: step 21810, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 57h:14m:36s remains)
INFO - root - 2017-12-07 15:00:40.024094: step 21820, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 56h:09m:56s remains)
INFO - root - 2017-12-07 15:00:46.820833: step 21830, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 58h:56m:59s remains)
INFO - root - 2017-12-07 15:00:53.719900: step 21840, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 62h:00m:17s remains)
INFO - root - 2017-12-07 15:01:00.453091: step 21850, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 53h:16m:14s remains)
INFO - root - 2017-12-07 15:01:07.156181: step 21860, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 56h:43m:34s remains)
INFO - root - 2017-12-07 15:01:13.964233: step 21870, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 57h:43m:54s remains)
INFO - root - 2017-12-07 15:01:20.836311: step 21880, loss = 2.10, batch loss = 2.04 (10.7 examples/sec; 0.744 sec/batch; 64h:14m:03s remains)
INFO - root - 2017-12-07 15:01:27.514601: step 21890, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.731 sec/batch; 63h:02m:00s remains)
INFO - root - 2017-12-07 15:01:34.251505: step 21900, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 59h:01m:49s remains)
2017-12-07 15:01:34.959992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29401 -4.2786188 -4.2735205 -4.2693686 -4.2675757 -4.2616329 -4.2478085 -4.2386794 -4.2330747 -4.2401142 -4.2458644 -4.251287 -4.2596564 -4.2608094 -4.2591443][-4.2938657 -4.282588 -4.2809291 -4.2792625 -4.2742949 -4.2596 -4.2396307 -4.2279954 -4.2220068 -4.2274566 -4.2333536 -4.2370086 -4.2430706 -4.2426209 -4.23704][-4.2888708 -4.2814121 -4.28215 -4.2854319 -4.2795677 -4.2598629 -4.2322578 -4.2152567 -4.2074585 -4.2090869 -4.2154846 -4.2190762 -4.2219453 -4.2188973 -4.21098][-4.2716103 -4.2602243 -4.2584438 -4.2613606 -4.2535772 -4.2277451 -4.1927752 -4.1741352 -4.1720338 -4.1772184 -4.1874843 -4.1892462 -4.1868415 -4.1781392 -4.1667433][-4.2518678 -4.2340107 -4.23205 -4.2304611 -4.2125373 -4.1748757 -4.1353693 -4.1202865 -4.1300211 -4.1483536 -4.1600442 -4.157392 -4.144094 -4.123868 -4.1021895][-4.2330146 -4.211606 -4.2052145 -4.1884613 -4.149436 -4.0894117 -4.0366254 -4.0136042 -4.0320268 -4.0693297 -4.0932927 -4.1000924 -4.0877786 -4.0645108 -4.036149][-4.204164 -4.1741576 -4.1554565 -4.1172829 -4.053926 -3.9647593 -3.8798151 -3.84069 -3.8744102 -3.944062 -3.9967222 -4.0200109 -4.0175571 -4.0019617 -3.9857502][-4.1672735 -4.121984 -4.086832 -4.027967 -3.9406087 -3.8234341 -3.7133498 -3.6777205 -3.7469277 -3.8545411 -3.9294655 -3.9622984 -3.9676306 -3.9631276 -3.96619][-4.1505938 -4.1008778 -4.0593681 -3.995584 -3.8996732 -3.7750516 -3.6711528 -3.6625967 -3.7505493 -3.8617046 -3.9325247 -3.9667544 -3.9789248 -3.986196 -4.00324][-4.1653457 -4.1291242 -4.0978718 -4.0431714 -3.9558125 -3.8602626 -3.797313 -3.8068168 -3.8756976 -3.9558103 -4.0071826 -4.0382032 -4.0559688 -4.0724549 -4.0897551][-4.1885676 -4.16885 -4.1520715 -4.11389 -4.0499253 -3.9999807 -3.9779575 -3.9907923 -4.0288129 -4.0751595 -4.1030765 -4.1232328 -4.1420875 -4.1587768 -4.1677341][-4.2170143 -4.2103319 -4.2056828 -4.1854687 -4.1461282 -4.1256652 -4.1269617 -4.1383309 -4.1576576 -4.1768503 -4.1821342 -4.1861706 -4.2011991 -4.2177463 -4.2233348][-4.2511868 -4.2516522 -4.2560587 -4.2474875 -4.2226844 -4.2155395 -4.2222772 -4.2297974 -4.2388291 -4.240634 -4.232996 -4.2297273 -4.2409735 -4.2552657 -4.2587543][-4.2766814 -4.2801981 -4.2869878 -4.2826986 -4.2664614 -4.262362 -4.2665691 -4.2690606 -4.2712092 -4.2671604 -4.2614141 -4.2598314 -4.2679238 -4.2781777 -4.2796183][-4.2985492 -4.3004689 -4.3049765 -4.3051958 -4.2998791 -4.2970972 -4.2966781 -4.2967515 -4.2958851 -4.2915039 -4.2871981 -4.28761 -4.2933731 -4.2996583 -4.3008094]]...]
INFO - root - 2017-12-07 15:01:41.833873: step 21910, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 63h:36m:04s remains)
INFO - root - 2017-12-07 15:01:48.661056: step 21920, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 61h:49m:56s remains)
INFO - root - 2017-12-07 15:01:55.298838: step 21930, loss = 2.05, batch loss = 1.99 (13.5 examples/sec; 0.592 sec/batch; 51h:05m:56s remains)
INFO - root - 2017-12-07 15:02:01.965141: step 21940, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 56h:32m:32s remains)
INFO - root - 2017-12-07 15:02:08.864907: step 21950, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 62h:20m:56s remains)
INFO - root - 2017-12-07 15:02:15.667965: step 21960, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 63h:54m:14s remains)
INFO - root - 2017-12-07 15:02:22.417620: step 21970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 57h:59m:44s remains)
INFO - root - 2017-12-07 15:02:29.104841: step 21980, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 53h:36m:38s remains)
INFO - root - 2017-12-07 15:02:35.858371: step 21990, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 58h:33m:35s remains)
INFO - root - 2017-12-07 15:02:42.582885: step 22000, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 61h:12m:55s remains)
2017-12-07 15:02:43.327136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3484764 -4.34084 -4.3133988 -4.2803087 -4.2536016 -4.2340093 -4.2275624 -4.2333083 -4.2502356 -4.267695 -4.2807226 -4.2973132 -4.3019133 -4.2975993 -4.278739][-4.3484621 -4.3346047 -4.2996168 -4.2626491 -4.23518 -4.2099848 -4.19977 -4.2075992 -4.2350764 -4.25828 -4.2749915 -4.2984385 -4.3057275 -4.3007164 -4.2802973][-4.3473616 -4.329452 -4.2893834 -4.2515883 -4.2243266 -4.19443 -4.1703777 -4.1728363 -4.2082953 -4.2391353 -4.2597866 -4.2884603 -4.3010793 -4.2945032 -4.27019][-4.3471475 -4.3273993 -4.2840738 -4.2462988 -4.2203259 -4.1825862 -4.1368275 -4.1275268 -4.1662426 -4.2048106 -4.2324567 -4.2668328 -4.2830496 -4.2738504 -4.2460828][-4.3493485 -4.3292184 -4.2820358 -4.2393427 -4.2085657 -4.1567597 -4.0845213 -4.0615711 -4.1117063 -4.1638875 -4.2021465 -4.2398748 -4.2564244 -4.2418108 -4.2114925][-4.3464284 -4.3252349 -4.2733221 -4.2170877 -4.1691966 -4.0933948 -3.9920821 -3.9657047 -4.0420251 -4.1172047 -4.1686654 -4.2105422 -4.2261791 -4.2043343 -4.1775613][-4.3392277 -4.317893 -4.2633286 -4.1951966 -4.1247444 -4.0173554 -3.8864741 -3.8600273 -3.9626112 -4.0630193 -4.1317239 -4.1770682 -4.1908174 -4.1726475 -4.159843][-4.3355927 -4.3158169 -4.2624841 -4.1887746 -4.103385 -3.9746742 -3.8289337 -3.7964849 -3.905407 -4.022943 -4.1056623 -4.1541862 -4.1674814 -4.1580839 -4.164011][-4.3319068 -4.3134804 -4.2641449 -4.195128 -4.1145563 -4.0009413 -3.8800154 -3.8474183 -3.935787 -4.043797 -4.1212664 -4.1639023 -4.1738653 -4.1708288 -4.1894379][-4.3265195 -4.3093777 -4.2647676 -4.2069798 -4.1461697 -4.067205 -3.9854188 -3.9639006 -4.0261593 -4.103838 -4.15834 -4.1881504 -4.19397 -4.1931796 -4.2139325][-4.3228636 -4.3075714 -4.2710075 -4.2294579 -4.1902275 -4.1441755 -4.0945263 -4.0794168 -4.1156149 -4.1641374 -4.1985183 -4.2193193 -4.225883 -4.2276831 -4.2425928][-4.3234887 -4.3104796 -4.2868018 -4.2672811 -4.2474113 -4.2274165 -4.2043123 -4.1973562 -4.2160563 -4.2407508 -4.2617145 -4.2768292 -4.2807355 -4.2817559 -4.2890482][-4.3270736 -4.3179607 -4.3076887 -4.3066511 -4.3039236 -4.3012195 -4.2962933 -4.2946649 -4.3054667 -4.3187675 -4.331039 -4.3419528 -4.3407545 -4.3374395 -4.3397546][-4.3306761 -4.3254042 -4.3236756 -4.33103 -4.3369236 -4.3433442 -4.3449831 -4.3457837 -4.35053 -4.3542786 -4.360167 -4.3673182 -4.3675237 -4.3633056 -4.3632746][-4.3338218 -4.3311305 -4.3314114 -4.3387671 -4.3447485 -4.3512096 -4.3549004 -4.3564086 -4.3565259 -4.3537192 -4.3534322 -4.3562207 -4.3569641 -4.35452 -4.3524113]]...]
INFO - root - 2017-12-07 15:02:50.117708: step 22010, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 55h:07m:07s remains)
INFO - root - 2017-12-07 15:02:56.846214: step 22020, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 55h:55m:19s remains)
INFO - root - 2017-12-07 15:03:03.668768: step 22030, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 61h:11m:03s remains)
INFO - root - 2017-12-07 15:03:10.438782: step 22040, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 60h:26m:52s remains)
INFO - root - 2017-12-07 15:03:17.213613: step 22050, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 55h:21m:31s remains)
INFO - root - 2017-12-07 15:03:23.922984: step 22060, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 54h:12m:09s remains)
INFO - root - 2017-12-07 15:03:30.707566: step 22070, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 58h:51m:05s remains)
INFO - root - 2017-12-07 15:03:37.407860: step 22080, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.710 sec/batch; 61h:13m:41s remains)
INFO - root - 2017-12-07 15:03:44.020504: step 22090, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 60h:28m:30s remains)
INFO - root - 2017-12-07 15:03:50.815972: step 22100, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 60h:10m:24s remains)
2017-12-07 15:03:51.565521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104963 -4.3068066 -4.3047533 -4.30706 -4.3080015 -4.3056064 -4.3030405 -4.2943583 -4.2796288 -4.2692513 -4.2554474 -4.2457747 -4.2537704 -4.2663784 -4.2697206][-4.3053951 -4.3008122 -4.30097 -4.3021111 -4.2996216 -4.2938933 -4.2850504 -4.2674608 -4.249114 -4.2336583 -4.2169456 -4.2082314 -4.220871 -4.2365685 -4.2415061][-4.2867451 -4.2764688 -4.2734828 -4.2733297 -4.2665334 -4.2572432 -4.2442837 -4.2239189 -4.2048583 -4.1852021 -4.1687646 -4.1713362 -4.1947169 -4.2169571 -4.2261233][-4.2571349 -4.2387242 -4.2249188 -4.2183552 -4.2083979 -4.1967263 -4.1850448 -4.1713171 -4.1570091 -4.1360664 -4.1230335 -4.1408606 -4.1800561 -4.2135668 -4.2312288][-4.2161355 -4.1938834 -4.1688356 -4.1560903 -4.1487207 -4.1395969 -4.13377 -4.1304522 -4.1230464 -4.0998049 -4.0837684 -4.1077223 -4.1586285 -4.2035003 -4.2322855][-4.1720576 -4.1456275 -4.1125832 -4.0972466 -4.0977416 -4.0963144 -4.0941329 -4.0974836 -4.0963092 -4.0721235 -4.0485792 -4.0730457 -4.1312075 -4.1838965 -4.2212853][-4.1214137 -4.0867257 -4.0479641 -4.0320435 -4.0410986 -4.0509238 -4.0542636 -4.0606513 -4.0581336 -4.0281062 -4.0006838 -4.0331655 -4.0994282 -4.156569 -4.1991849][-4.046566 -4.0017214 -3.9643774 -3.9523675 -3.9720368 -3.9942546 -4.0043683 -4.0124092 -4.0060158 -3.9670334 -3.9375818 -3.9828379 -4.0584722 -4.1230264 -4.1746588][-3.9968216 -3.9519353 -3.925725 -3.9222665 -3.9502738 -3.9811502 -3.9988148 -4.0083218 -4.0021639 -3.9630766 -3.93384 -3.9822886 -4.0549369 -4.115881 -4.1687][-4.0391121 -4.0062618 -3.9928279 -3.9924939 -4.0170984 -4.046999 -4.0643973 -4.0728846 -4.0696626 -4.038435 -4.00973 -4.043077 -4.0975437 -4.1440139 -4.1876388][-4.1330581 -4.1127391 -4.1058755 -4.1043978 -4.1190166 -4.1424642 -4.156817 -4.1632185 -4.1616755 -4.1391859 -4.1112304 -4.1241031 -4.160605 -4.194664 -4.2269549][-4.231729 -4.2206559 -4.2170167 -4.2140303 -4.2218456 -4.2378216 -4.2470164 -4.2504234 -4.2501874 -4.2363272 -4.2153931 -4.2171125 -4.2385173 -4.2594395 -4.2802229][-4.3059969 -4.301168 -4.2989192 -4.2964225 -4.3007755 -4.30914 -4.3118625 -4.3116379 -4.3115954 -4.3049164 -4.2929735 -4.29025 -4.3023286 -4.3168139 -4.3290763][-4.3493228 -4.3462706 -4.343915 -4.3420725 -4.3431005 -4.3461523 -4.3460007 -4.3445039 -4.343308 -4.3398776 -4.3339491 -4.3310676 -4.3381405 -4.3483758 -4.3587046][-4.3572669 -4.3546047 -4.3523879 -4.3513904 -4.3518529 -4.3532772 -4.3525381 -4.3516107 -4.3506126 -4.3487353 -4.3463979 -4.3456469 -4.3503327 -4.3569527 -4.3641858]]...]
INFO - root - 2017-12-07 15:03:58.483977: step 22110, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 63h:10m:24s remains)
INFO - root - 2017-12-07 15:04:05.311284: step 22120, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 63h:46m:44s remains)
INFO - root - 2017-12-07 15:04:12.194109: step 22130, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 56h:31m:38s remains)
INFO - root - 2017-12-07 15:04:18.979707: step 22140, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 53h:26m:57s remains)
INFO - root - 2017-12-07 15:04:25.918355: step 22150, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 60h:55m:34s remains)
INFO - root - 2017-12-07 15:04:32.715393: step 22160, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 61h:02m:29s remains)
INFO - root - 2017-12-07 15:04:39.527437: step 22170, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 57h:55m:46s remains)
INFO - root - 2017-12-07 15:04:46.304401: step 22180, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.613 sec/batch; 52h:52m:51s remains)
INFO - root - 2017-12-07 15:04:52.932294: step 22190, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 56h:21m:02s remains)
INFO - root - 2017-12-07 15:04:59.716014: step 22200, loss = 2.03, batch loss = 1.98 (10.8 examples/sec; 0.740 sec/batch; 63h:45m:27s remains)
2017-12-07 15:05:00.359965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2270617 -4.2167368 -4.2134647 -4.2172275 -4.2346344 -4.251698 -4.26099 -4.2575016 -4.2556167 -4.2496452 -4.2392254 -4.2277093 -4.212472 -4.2042775 -4.1939192][-4.2130685 -4.1982102 -4.1918654 -4.1922035 -4.209796 -4.2320647 -4.2484021 -4.2504492 -4.2526593 -4.2499957 -4.2406735 -4.2261457 -4.2083392 -4.2020836 -4.1949153][-4.197835 -4.1883974 -4.1867528 -4.1903954 -4.2045803 -4.2245183 -4.2391849 -4.243135 -4.2504983 -4.251699 -4.2441587 -4.22763 -4.2099948 -4.2030797 -4.1935973][-4.1851468 -4.1815519 -4.1874194 -4.1938353 -4.1990089 -4.2072783 -4.2105184 -4.2091331 -4.219295 -4.2277713 -4.2229524 -4.2089496 -4.1936445 -4.1859312 -4.1809759][-4.1646638 -4.1617627 -4.1661878 -4.16798 -4.1646733 -4.1659665 -4.1566696 -4.1513572 -4.168745 -4.1846347 -4.1882887 -4.180541 -4.1648827 -4.1530347 -4.1500916][-4.1371422 -4.128294 -4.1163344 -4.0980911 -4.0807796 -4.0782237 -4.0721469 -4.0770655 -4.1106386 -4.1350541 -4.1397223 -4.132061 -4.116972 -4.1049709 -4.1002035][-4.0993447 -4.0819869 -4.047986 -3.9995809 -3.9654775 -3.9723766 -3.9919181 -4.0197377 -4.0666275 -4.0948911 -4.1000023 -4.0926251 -4.0837831 -4.0798116 -4.0730486][-4.089293 -4.0830865 -4.0489349 -3.991112 -3.9534733 -3.9625814 -3.9918976 -4.0289917 -4.0769444 -4.1103015 -4.122776 -4.1187363 -4.1124506 -4.1097975 -4.0975885][-4.1180382 -4.13227 -4.1233912 -4.0934281 -4.0716352 -4.0654826 -4.0711837 -4.087225 -4.1159043 -4.1422343 -4.1585336 -4.1615915 -4.1581979 -4.1528072 -4.1384163][-4.14715 -4.1688666 -4.1738453 -4.1632428 -4.1548715 -4.1482897 -4.14184 -4.1436439 -4.1546407 -4.1674066 -4.1772046 -4.1850257 -4.1890926 -4.1858673 -4.1801324][-4.1633067 -4.1819496 -4.191968 -4.1954007 -4.200789 -4.2024193 -4.1946287 -4.1871295 -4.1860876 -4.1878057 -4.1938953 -4.2039757 -4.2110834 -4.2093997 -4.206646][-4.1907206 -4.1989412 -4.2032909 -4.2088976 -4.2171736 -4.2215991 -4.2174916 -4.2091455 -4.203136 -4.2007966 -4.1997795 -4.2054505 -4.2128372 -4.2124639 -4.2086191][-4.2360387 -4.2376666 -4.2367826 -4.2358894 -4.2367897 -4.239531 -4.2404122 -4.2371817 -4.23036 -4.2243609 -4.2185583 -4.2196307 -4.2245307 -4.2240372 -4.2200246][-4.2798743 -4.2834926 -4.2824874 -4.2778044 -4.2725296 -4.2715225 -4.2722631 -4.2710567 -4.2676983 -4.2609253 -4.2550697 -4.2548761 -4.254354 -4.2515979 -4.2493882][-4.3027477 -4.3060222 -4.3049269 -4.2966256 -4.288938 -4.2871017 -4.28962 -4.29253 -4.2923441 -4.2883816 -4.2849131 -4.2851667 -4.2835221 -4.2819891 -4.2826681]]...]
INFO - root - 2017-12-07 15:05:07.108588: step 22210, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.617 sec/batch; 53h:12m:08s remains)
INFO - root - 2017-12-07 15:05:13.862281: step 22220, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.643 sec/batch; 55h:22m:48s remains)
INFO - root - 2017-12-07 15:05:20.809821: step 22230, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 63h:04m:30s remains)
INFO - root - 2017-12-07 15:05:27.612504: step 22240, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 60h:39m:07s remains)
INFO - root - 2017-12-07 15:05:34.189901: step 22250, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 54h:30m:59s remains)
INFO - root - 2017-12-07 15:05:40.977501: step 22260, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 55h:14m:37s remains)
INFO - root - 2017-12-07 15:05:47.738151: step 22270, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 61h:44m:10s remains)
INFO - root - 2017-12-07 15:05:54.506800: step 22280, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 63h:37m:56s remains)
INFO - root - 2017-12-07 15:06:01.161962: step 22290, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 58h:44m:01s remains)
INFO - root - 2017-12-07 15:06:07.896989: step 22300, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 54h:08m:23s remains)
2017-12-07 15:06:08.667718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3131404 -4.2945414 -4.2816405 -4.282032 -4.2782917 -4.2591696 -4.21924 -4.1689329 -4.1378403 -4.1338229 -4.1409874 -4.15016 -4.16551 -4.1903815 -4.2176151][-4.3041482 -4.2842503 -4.2718253 -4.270525 -4.2611694 -4.2282853 -4.170486 -4.1025457 -4.0662522 -4.0688353 -4.0878978 -4.1048737 -4.1304045 -4.1695375 -4.20763][-4.2965035 -4.2784209 -4.2648129 -4.2562542 -4.2367926 -4.1871729 -4.1084952 -4.0299654 -4.0002046 -4.0269728 -4.0647287 -4.0862737 -4.1128831 -4.1581945 -4.2033463][-4.292057 -4.274703 -4.2587929 -4.2416711 -4.2078338 -4.1370983 -4.0341558 -3.9485331 -3.9373062 -4.0051718 -4.0651169 -4.0866809 -4.108326 -4.1544847 -4.2019835][-4.2908964 -4.2726965 -4.253686 -4.2270112 -4.1745391 -4.0793295 -3.9512033 -3.8559277 -3.8733487 -3.999872 -4.088789 -4.1105261 -4.1265163 -4.1635156 -4.2026339][-4.2953825 -4.2771544 -4.2562957 -4.2187057 -4.1411786 -4.0155234 -3.8538172 -3.7352419 -3.7858605 -3.9858072 -4.1096492 -4.1375089 -4.1523237 -4.17875 -4.2061458][-4.3068719 -4.2932849 -4.2740865 -4.2249613 -4.1222386 -3.9607882 -3.7530375 -3.5886471 -3.6680992 -3.9363093 -4.0933218 -4.1338539 -4.1539927 -4.1819959 -4.2101989][-4.315258 -4.307848 -4.2921181 -4.2364807 -4.1223989 -3.9439182 -3.708652 -3.5083115 -3.5962243 -3.8886676 -4.0637064 -4.115737 -4.1413751 -4.1761103 -4.2091575][-4.3158908 -4.3127766 -4.3034048 -4.2560635 -4.1546416 -3.9920738 -3.7824407 -3.6054487 -3.6684475 -3.911258 -4.0671315 -4.1193252 -4.1488757 -4.184329 -4.2165141][-4.3142667 -4.3123045 -4.310184 -4.2777567 -4.2000666 -4.0761714 -3.9188933 -3.7832646 -3.8119483 -3.9833257 -4.105607 -4.1492472 -4.1810465 -4.2153816 -4.2396593][-4.3131905 -4.3100424 -4.3133183 -4.293848 -4.238687 -4.1581383 -4.0507207 -3.9503272 -3.958236 -4.0727143 -4.1590576 -4.189466 -4.2221804 -4.257987 -4.2714448][-4.3106422 -4.30171 -4.3033409 -4.2942095 -4.2615938 -4.2167325 -4.1504865 -4.0795355 -4.0780668 -4.1544423 -4.2109494 -4.2281361 -4.2594061 -4.2962556 -4.302012][-4.3092709 -4.2946954 -4.290813 -4.2873797 -4.2732482 -4.2537508 -4.2147493 -4.1653309 -4.1614966 -4.2141709 -4.2494287 -4.2568655 -4.2821937 -4.3176355 -4.3239121][-4.3102956 -4.2908244 -4.2833261 -4.285727 -4.2844934 -4.2772617 -4.2537041 -4.22393 -4.2259636 -4.2637453 -4.2887788 -4.2939548 -4.3092103 -4.3322816 -4.3354888][-4.3106966 -4.291307 -4.284668 -4.2909083 -4.2943606 -4.2918348 -4.2757411 -4.2574553 -4.262629 -4.2901597 -4.3117151 -4.3186545 -4.326438 -4.3385162 -4.3396664]]...]
INFO - root - 2017-12-07 15:06:15.495781: step 22310, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 59h:41m:18s remains)
INFO - root - 2017-12-07 15:06:22.274981: step 22320, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 56h:35m:36s remains)
INFO - root - 2017-12-07 15:06:29.040049: step 22330, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 55h:55m:57s remains)
INFO - root - 2017-12-07 15:06:35.923225: step 22340, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 62h:18m:09s remains)
INFO - root - 2017-12-07 15:06:42.755302: step 22350, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 64h:10m:13s remains)
INFO - root - 2017-12-07 15:06:49.564743: step 22360, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 58h:23m:58s remains)
INFO - root - 2017-12-07 15:06:56.324714: step 22370, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 53h:31m:03s remains)
INFO - root - 2017-12-07 15:07:03.223310: step 22380, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 56h:49m:19s remains)
INFO - root - 2017-12-07 15:07:09.910443: step 22390, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 62h:19m:39s remains)
INFO - root - 2017-12-07 15:07:16.748347: step 22400, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 58h:33m:27s remains)
2017-12-07 15:07:17.467649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2763648 -4.267817 -4.2605615 -4.2566938 -4.2564211 -4.2563229 -4.255208 -4.2531533 -4.2461367 -4.2381878 -4.2366881 -4.24157 -4.2464004 -4.2437515 -4.2359376][-4.257442 -4.2459235 -4.2395964 -4.237102 -4.23656 -4.2341056 -4.2292843 -4.2252107 -4.2135592 -4.1993623 -4.193604 -4.2005382 -4.2088823 -4.2077503 -4.20115][-4.2403083 -4.2299871 -4.2230687 -4.2152257 -4.2075067 -4.1973982 -4.1849375 -4.172513 -4.1520081 -4.1327209 -4.1270714 -4.1407685 -4.1562519 -4.1631794 -4.1635132][-4.2239294 -4.212842 -4.2022247 -4.1849165 -4.1636233 -4.1417122 -4.1177526 -4.0950089 -4.0741014 -4.0564489 -4.0549679 -4.0792689 -4.1049919 -4.1270037 -4.1393404][-4.2061105 -4.1895404 -4.1696486 -4.1370296 -4.1002612 -4.0671005 -4.034699 -4.0127459 -4.011476 -4.0100012 -4.017632 -4.0479374 -4.0773716 -4.1151829 -4.1430573][-4.1738749 -4.1416597 -4.1054044 -4.0566297 -4.007865 -3.9730241 -3.9457803 -3.9460745 -3.9763513 -4.00304 -4.0233383 -4.0512133 -4.0790634 -4.1244144 -4.162694][-4.1140194 -4.0641828 -4.0091519 -3.9424639 -3.8901379 -3.8745477 -3.877897 -3.9186432 -3.97983 -4.02649 -4.0553455 -4.0803442 -4.1058679 -4.1519165 -4.1922874][-4.0353274 -3.9701762 -3.902647 -3.8305831 -3.7912226 -3.821171 -3.8758228 -3.9580264 -4.0318575 -4.0838618 -4.1151319 -4.1382713 -4.1610069 -4.2007794 -4.230876][-3.9507892 -3.8867898 -3.8448732 -3.8199751 -3.8239942 -3.8801236 -3.9549239 -4.04474 -4.1101928 -4.145822 -4.1694469 -4.1953015 -4.2189245 -4.2491627 -4.2679896][-3.8808818 -3.8397415 -3.8584886 -3.8990152 -3.9404631 -3.9883149 -4.04584 -4.1205525 -4.172039 -4.1951404 -4.2121177 -4.2366281 -4.2575469 -4.2770853 -4.2910323][-3.8532851 -3.8506486 -3.9141507 -3.9900031 -4.0399752 -4.0668263 -4.0971675 -4.1518903 -4.1971083 -4.221211 -4.2385664 -4.2578278 -4.2733788 -4.2889123 -4.3053975][-3.8933344 -3.9178038 -3.9863245 -4.0577917 -4.1004181 -4.1169233 -4.13373 -4.1731596 -4.211206 -4.23748 -4.2551131 -4.2679753 -4.2837386 -4.2991214 -4.3171806][-3.9797282 -4.011559 -4.061717 -4.1073017 -4.1367569 -4.1534271 -4.1733479 -4.2064457 -4.2367697 -4.2580676 -4.2716179 -4.2835035 -4.2999477 -4.3161478 -4.3313231][-4.0692382 -4.0981884 -4.12914 -4.1504407 -4.1716046 -4.1923027 -4.2138729 -4.2393775 -4.2605071 -4.2776656 -4.2901773 -4.3030796 -4.3192387 -4.3341031 -4.3427997][-4.1330862 -4.1539288 -4.1708879 -4.1796184 -4.2003918 -4.224071 -4.2463326 -4.2638268 -4.2809219 -4.2966561 -4.3069968 -4.3181567 -4.330584 -4.3407836 -4.3431582]]...]
INFO - root - 2017-12-07 15:07:24.231075: step 22410, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 59h:31m:14s remains)
INFO - root - 2017-12-07 15:07:31.029544: step 22420, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 60h:40m:02s remains)
INFO - root - 2017-12-07 15:07:37.827966: step 22430, loss = 2.02, batch loss = 1.96 (12.3 examples/sec; 0.650 sec/batch; 55h:59m:47s remains)
INFO - root - 2017-12-07 15:07:44.685996: step 22440, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 59h:50m:46s remains)
INFO - root - 2017-12-07 15:07:51.440723: step 22450, loss = 2.03, batch loss = 1.98 (12.5 examples/sec; 0.640 sec/batch; 55h:04m:40s remains)
INFO - root - 2017-12-07 15:07:58.365152: step 22460, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 60h:15m:33s remains)
INFO - root - 2017-12-07 15:08:05.132982: step 22470, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 63h:02m:44s remains)
INFO - root - 2017-12-07 15:08:11.911625: step 22480, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.703 sec/batch; 60h:32m:43s remains)
INFO - root - 2017-12-07 15:08:18.518873: step 22490, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 59h:00m:26s remains)
INFO - root - 2017-12-07 15:08:25.210481: step 22500, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 53h:29m:57s remains)
2017-12-07 15:08:25.947220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2785673 -4.2794633 -4.2795944 -4.2795453 -4.281631 -4.2862945 -4.2925949 -4.2977557 -4.2975621 -4.2895103 -4.2750964 -4.2598991 -4.2484136 -4.2374134 -4.2240481][-4.2818565 -4.2807927 -4.27763 -4.2735276 -4.27284 -4.2774482 -4.2860522 -4.2948127 -4.2947946 -4.2839556 -4.2670722 -4.2526188 -4.2461114 -4.2428627 -4.2371726][-4.2830896 -4.280488 -4.27378 -4.2641535 -4.258441 -4.2601395 -4.2676864 -4.2764077 -4.2760444 -4.2665005 -4.254313 -4.2459288 -4.244945 -4.2462082 -4.2444758][-4.2857709 -4.2822089 -4.2724781 -4.2574072 -4.2456994 -4.2422357 -4.2449007 -4.2481952 -4.2441244 -4.2373981 -4.2339377 -4.2352867 -4.2403231 -4.2442775 -4.2445683][-4.2930689 -4.2900114 -4.2788782 -4.2596331 -4.2428379 -4.2331405 -4.2281284 -4.2225866 -4.2116342 -4.2036848 -4.2037649 -4.21063 -4.2215528 -4.2304831 -4.2320986][-4.29943 -4.2975221 -4.286365 -4.265142 -4.2450023 -4.229291 -4.2157183 -4.2003264 -4.1808167 -4.1665277 -4.1636109 -4.1729627 -4.1892872 -4.20454 -4.2104373][-4.3017426 -4.30103 -4.2907763 -4.2703938 -4.2503963 -4.2322145 -4.2115006 -4.1874537 -4.1592731 -4.1341448 -4.1221628 -4.129652 -4.1503587 -4.171422 -4.1826634][-4.30045 -4.3002362 -4.29092 -4.2723651 -4.2544017 -4.2371016 -4.2131948 -4.182179 -4.1449451 -4.1091628 -4.0864539 -4.09174 -4.1170015 -4.141932 -4.1562538][-4.2969661 -4.2970324 -4.28726 -4.2693233 -4.2509394 -4.2331262 -4.206018 -4.1689568 -4.1237464 -4.0796766 -4.05046 -4.0599084 -4.092567 -4.1202707 -4.1350431][-4.2917137 -4.2908177 -4.2782483 -4.2578731 -4.2368336 -4.2176251 -4.1895342 -4.1492486 -4.1003237 -4.05214 -4.0214558 -4.03711 -4.0755253 -4.1042504 -4.1184225][-4.2856255 -4.2839313 -4.269423 -4.2475767 -4.2245669 -4.2069421 -4.1839309 -4.1469812 -4.099359 -4.0490227 -4.0166364 -4.0303054 -4.0675397 -4.0954709 -4.1080732][-4.2749658 -4.2753119 -4.2607479 -4.2377181 -4.2124653 -4.1960535 -4.1808386 -4.1513577 -4.1066117 -4.0582318 -4.0248203 -4.0318508 -4.0643473 -4.0897889 -4.10211][-4.2633576 -4.2663503 -4.252933 -4.2296538 -4.2035761 -4.1876917 -4.178637 -4.1585889 -4.1249537 -4.0863752 -4.0554338 -4.054183 -4.0743408 -4.0936852 -4.1002908][-4.2471032 -4.25059 -4.2401137 -4.2197876 -4.1978083 -4.1878023 -4.1908073 -4.1879892 -4.1693926 -4.14083 -4.1102095 -4.0961323 -4.0984216 -4.1043043 -4.0979338][-4.2250543 -4.2275996 -4.2227445 -4.2114792 -4.1975708 -4.1943717 -4.2053452 -4.2138634 -4.2071967 -4.18647 -4.155303 -4.1288505 -4.1162267 -4.1140418 -4.1001391]]...]
INFO - root - 2017-12-07 15:08:32.729874: step 22510, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 61h:29m:11s remains)
INFO - root - 2017-12-07 15:08:39.491466: step 22520, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 57h:31m:13s remains)
INFO - root - 2017-12-07 15:08:46.300533: step 22530, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 56h:05m:05s remains)
INFO - root - 2017-12-07 15:08:53.172604: step 22540, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 62h:02m:45s remains)
INFO - root - 2017-12-07 15:09:00.026705: step 22550, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 62h:16m:46s remains)
INFO - root - 2017-12-07 15:09:06.705689: step 22560, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 54h:06m:51s remains)
INFO - root - 2017-12-07 15:09:13.522355: step 22570, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.641 sec/batch; 55h:08m:48s remains)
INFO - root - 2017-12-07 15:09:20.435382: step 22580, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 62h:15m:01s remains)
INFO - root - 2017-12-07 15:09:27.202108: step 22590, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 62h:00m:28s remains)
INFO - root - 2017-12-07 15:09:33.978905: step 22600, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 56h:03m:01s remains)
2017-12-07 15:09:34.649663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1968842 -4.1937156 -4.1759253 -4.1500139 -4.1313295 -4.1256804 -4.1354957 -4.152945 -4.1568236 -4.1563859 -4.1639504 -4.1732092 -4.1758838 -4.170712 -4.1724787][-4.212934 -4.2068281 -4.1836109 -4.1508074 -4.1235323 -4.1117377 -4.1252623 -4.1537356 -4.167171 -4.1748114 -4.1877031 -4.1937113 -4.1894994 -4.1684103 -4.1558061][-4.2112446 -4.210443 -4.1917138 -4.1620393 -4.1288781 -4.1109982 -4.1236377 -4.150383 -4.1624756 -4.1735573 -4.192318 -4.202179 -4.201128 -4.1744704 -4.1518149][-4.18665 -4.2026663 -4.2024374 -4.182106 -4.1478596 -4.1268644 -4.1263156 -4.1313329 -4.1227803 -4.1330276 -4.1702585 -4.1997108 -4.2135739 -4.1931453 -4.1673045][-4.1386094 -4.1839962 -4.2123437 -4.2098227 -4.1788373 -4.1536016 -4.1349645 -4.1068091 -4.0619335 -4.0652471 -4.1287932 -4.1825476 -4.2152548 -4.2112131 -4.200542][-4.1059542 -4.1737952 -4.2226443 -4.234374 -4.2098389 -4.1776476 -4.1329503 -4.0564218 -3.9564061 -3.949991 -4.0475426 -4.1390848 -4.2009525 -4.2226028 -4.231843][-4.0899677 -4.1617651 -4.2179537 -4.2394447 -4.2201271 -4.1811638 -4.11184 -3.9930091 -3.8447142 -3.8368969 -3.9722753 -4.0983849 -4.1870689 -4.2322493 -4.2560792][-4.0635514 -4.1270351 -4.1937571 -4.227006 -4.221241 -4.1926527 -4.1251664 -4.0107164 -3.8810601 -3.8816648 -4.0034127 -4.1160994 -4.1977086 -4.2494636 -4.27781][-4.0198784 -4.0746465 -4.1549377 -4.2070074 -4.2233562 -4.2114835 -4.1568484 -4.0718551 -3.9887605 -3.9973688 -4.0842795 -4.1666574 -4.2256222 -4.26766 -4.29269][-3.9968338 -4.036726 -4.1227889 -4.1924319 -4.2295008 -4.2340288 -4.1947203 -4.1366239 -4.0906878 -4.10472 -4.16351 -4.2202048 -4.2564149 -4.2817621 -4.2973671][-3.9860272 -4.0111847 -4.0989227 -4.1817203 -4.2352033 -4.2565289 -4.2380843 -4.2058663 -4.187974 -4.2025323 -4.234756 -4.2639294 -4.2785325 -4.2877369 -4.29454][-3.9978735 -4.0106115 -4.0908761 -4.1726003 -4.2345548 -4.2712736 -4.2734203 -4.2632008 -4.2597933 -4.2695189 -4.2826753 -4.2932854 -4.2943988 -4.2942867 -4.2971005][-4.0420551 -4.0412569 -4.1011181 -4.1699004 -4.230381 -4.2758327 -4.2918096 -4.2965031 -4.2962775 -4.2989335 -4.302794 -4.3033133 -4.3020277 -4.300981 -4.3051124][-4.0841956 -4.0779037 -4.1214123 -4.1770563 -4.2313418 -4.2766037 -4.2987771 -4.3089814 -4.31034 -4.3103051 -4.3096657 -4.3063507 -4.3065495 -4.3063965 -4.309988][-4.1035194 -4.1000123 -4.1387773 -4.1898603 -4.2399716 -4.2809243 -4.3038063 -4.3142972 -4.3159223 -4.3149352 -4.3121371 -4.3096228 -4.3113041 -4.3126311 -4.3147812]]...]
INFO - root - 2017-12-07 15:09:41.514545: step 22610, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 62h:32m:36s remains)
INFO - root - 2017-12-07 15:09:48.434779: step 22620, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 56h:53m:43s remains)
INFO - root - 2017-12-07 15:09:55.219232: step 22630, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 56h:08m:33s remains)
INFO - root - 2017-12-07 15:10:01.885256: step 22640, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 54h:08m:39s remains)
INFO - root - 2017-12-07 15:10:08.759158: step 22650, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.735 sec/batch; 63h:16m:03s remains)
INFO - root - 2017-12-07 15:10:15.629642: step 22660, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 60h:52m:43s remains)
INFO - root - 2017-12-07 15:10:22.450790: step 22670, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 55h:49m:18s remains)
INFO - root - 2017-12-07 15:10:29.196129: step 22680, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 54h:26m:25s remains)
INFO - root - 2017-12-07 15:10:35.820465: step 22690, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.626 sec/batch; 53h:54m:14s remains)
INFO - root - 2017-12-07 15:10:42.667930: step 22700, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 61h:07m:13s remains)
2017-12-07 15:10:43.432507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.302937 -4.2896786 -4.27172 -4.2498841 -4.2306018 -4.2183743 -4.2071013 -4.1990652 -4.19617 -4.2014971 -4.2119727 -4.2273726 -4.2356639 -4.2341738 -4.2293448][-4.2929959 -4.2766585 -4.2539086 -4.2247148 -4.1983871 -4.1808896 -4.1662135 -4.152648 -4.149498 -4.1543727 -4.1653743 -4.1874366 -4.1969256 -4.1872931 -4.1753798][-4.2829604 -4.260005 -4.2298412 -4.1892953 -4.14983 -4.1228766 -4.1050491 -4.0973358 -4.1061158 -4.1209211 -4.1379972 -4.1670804 -4.1730089 -4.1484246 -4.11974][-4.2720051 -4.2406211 -4.1994281 -4.142899 -4.08551 -4.041266 -4.0158038 -4.0226946 -4.0590267 -4.0945644 -4.1232944 -4.1573744 -4.1608872 -4.1269646 -4.0824485][-4.2628145 -4.2247887 -4.1729932 -4.10032 -4.0230713 -3.9567146 -3.9112895 -3.9242783 -3.997117 -4.0659361 -4.1092672 -4.1458154 -4.1513581 -4.1207128 -4.0710506][-4.2566872 -4.2125306 -4.1508904 -4.0671558 -3.9788897 -3.892406 -3.8078749 -3.8013787 -3.9056773 -4.0130172 -4.0730295 -4.1147232 -4.1269684 -4.1096926 -4.0649829][-4.2565851 -4.2097225 -4.14194 -4.0533395 -3.9601152 -3.863148 -3.7341175 -3.6822443 -3.8001156 -3.9397266 -4.012804 -4.0543094 -4.0734339 -4.0722961 -4.0455189][-4.2624197 -4.2183547 -4.1529589 -4.06738 -3.9822712 -3.8956704 -3.7711959 -3.6930234 -3.7873476 -3.919076 -3.9871285 -4.0196929 -4.034852 -4.0453382 -4.0404072][-4.2709866 -4.2360559 -4.1816754 -4.1104388 -4.0470819 -3.9937534 -3.914854 -3.8599844 -3.9015245 -3.9788895 -4.0215769 -4.042944 -4.0473571 -4.054244 -4.0643992][-4.2774143 -4.2529683 -4.2137775 -4.1611128 -4.1234651 -4.0975204 -4.0569344 -4.0285678 -4.0311852 -4.0540609 -4.0766878 -4.097044 -4.0950708 -4.0925674 -4.1031389][-4.2861929 -4.2687426 -4.2404842 -4.2034092 -4.1832318 -4.1734796 -4.15475 -4.1387215 -4.12295 -4.1139083 -4.12506 -4.1484823 -4.1493564 -4.1425853 -4.1489754][-4.2966681 -4.2825022 -4.2603226 -4.235383 -4.2219086 -4.2180343 -4.2102618 -4.2016978 -4.186913 -4.1739893 -4.1779156 -4.1965652 -4.2006955 -4.1931028 -4.1895409][-4.3029294 -4.2904811 -4.2726245 -4.2560453 -4.2462239 -4.2443037 -4.2430873 -4.2425623 -4.2374325 -4.2266088 -4.2224979 -4.2306166 -4.2376008 -4.2336597 -4.2251043][-4.3095136 -4.2995572 -4.2870712 -4.2774091 -4.2696271 -4.2666874 -4.267827 -4.2714691 -4.2714953 -4.2648382 -4.2584729 -4.2603073 -4.2674494 -4.2696266 -4.2659736][-4.316401 -4.3095956 -4.3014789 -4.2956719 -4.289598 -4.2859883 -4.2866464 -4.2908859 -4.2930374 -4.2902169 -4.2866135 -4.287611 -4.2924585 -4.29592 -4.2956057]]...]
INFO - root - 2017-12-07 15:10:50.103820: step 22710, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 54h:10m:38s remains)
INFO - root - 2017-12-07 15:10:57.003472: step 22720, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 57h:10m:33s remains)
INFO - root - 2017-12-07 15:11:03.860056: step 22730, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.737 sec/batch; 63h:26m:43s remains)
INFO - root - 2017-12-07 15:11:10.787559: step 22740, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.694 sec/batch; 59h:41m:50s remains)
INFO - root - 2017-12-07 15:11:17.668416: step 22750, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 55h:04m:40s remains)
INFO - root - 2017-12-07 15:11:24.396875: step 22760, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 54h:50m:13s remains)
INFO - root - 2017-12-07 15:11:31.311540: step 22770, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 61h:19m:09s remains)
INFO - root - 2017-12-07 15:11:38.167189: step 22780, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.751 sec/batch; 64h:36m:46s remains)
INFO - root - 2017-12-07 15:11:44.791320: step 22790, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 57h:53m:53s remains)
INFO - root - 2017-12-07 15:11:51.555804: step 22800, loss = 2.05, batch loss = 2.00 (13.2 examples/sec; 0.608 sec/batch; 52h:17m:49s remains)
2017-12-07 15:11:52.292733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29033 -4.2969155 -4.297399 -4.2972054 -4.2894783 -4.27904 -4.2685404 -4.2646074 -4.2708678 -4.2843609 -4.2985239 -4.3017607 -4.2995219 -4.29345 -4.2841425][-4.2858253 -4.2903275 -4.2922926 -4.2930183 -4.2787547 -4.2616611 -4.2537684 -4.2572722 -4.2720695 -4.2932858 -4.3087454 -4.3098774 -4.3064966 -4.302269 -4.2942386][-4.2787356 -4.2791729 -4.2794476 -4.2743659 -4.2489419 -4.2221026 -4.2109795 -4.2200794 -4.244144 -4.27264 -4.2896633 -4.2911062 -4.2920513 -4.2932816 -4.2883439][-4.2769661 -4.2693515 -4.2575965 -4.2348228 -4.1925139 -4.1529236 -4.135231 -4.1510515 -4.1890621 -4.2319589 -4.2559266 -4.2619061 -4.2672315 -4.274982 -4.2753682][-4.2735291 -4.25432 -4.2232013 -4.1738887 -4.1080952 -4.0485415 -4.02368 -4.0529666 -4.1119661 -4.175118 -4.2144356 -4.2295737 -4.24087 -4.2547655 -4.2634993][-4.2594137 -4.2293172 -4.1789193 -4.1035075 -4.01071 -3.9299636 -3.9023232 -3.9478259 -4.0305505 -4.1143384 -4.1711421 -4.1968021 -4.2134652 -4.2371736 -4.2569885][-4.2391539 -4.1990981 -4.1364527 -4.0470996 -3.9387488 -3.8437469 -3.8145566 -3.8743973 -3.9724977 -4.0703173 -4.1379185 -4.169291 -4.1886873 -4.2201824 -4.2488995][-4.2196579 -4.1775403 -4.11686 -4.0341086 -3.9329274 -3.8403592 -3.807831 -3.8614502 -3.9583344 -4.0591803 -4.1274815 -4.1572518 -4.1799378 -4.2125225 -4.2407575][-4.2175279 -4.1850772 -4.140007 -4.0787458 -4.0042911 -3.9298408 -3.8962255 -3.9270663 -4.0032883 -4.0926609 -4.1501608 -4.1772985 -4.1994848 -4.2237387 -4.2414861][-4.2386208 -4.219965 -4.1948476 -4.1605945 -4.1184497 -4.0681119 -4.0358605 -4.0461216 -4.0971332 -4.1596007 -4.1970596 -4.2191248 -4.2344756 -4.2448249 -4.252409][-4.2613111 -4.2535548 -4.2450953 -4.2326756 -4.2170568 -4.1886067 -4.1640954 -4.1622992 -4.1888814 -4.2199802 -4.2352862 -4.2504954 -4.2582989 -4.2602191 -4.2646952][-4.265945 -4.2686028 -4.2708573 -4.2698421 -4.2688875 -4.2583237 -4.2465148 -4.2397165 -4.2504368 -4.2595658 -4.2583957 -4.263968 -4.2646875 -4.2598553 -4.2646728][-4.2592854 -4.2662582 -4.27279 -4.27656 -4.2811732 -4.2832441 -4.2820644 -4.2773013 -4.2826309 -4.2790504 -4.2666712 -4.2652292 -4.2585726 -4.2493629 -4.2555485][-4.2550097 -4.2597861 -4.2624354 -4.2647433 -4.2714891 -4.284616 -4.2915125 -4.2909961 -4.2933812 -4.2810555 -4.2650957 -4.2610373 -4.2521644 -4.2427979 -4.2492061][-4.2475262 -4.2453218 -4.2404866 -4.2408929 -4.2519841 -4.2741652 -4.2887893 -4.2942562 -4.2933197 -4.2739096 -4.2538414 -4.2473865 -4.2372928 -4.2314119 -4.2395105]]...]
INFO - root - 2017-12-07 15:11:59.094736: step 22810, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 60h:10m:16s remains)
INFO - root - 2017-12-07 15:12:06.026111: step 22820, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 55h:32m:37s remains)
INFO - root - 2017-12-07 15:12:12.796271: step 22830, loss = 2.10, batch loss = 2.05 (12.3 examples/sec; 0.648 sec/batch; 55h:44m:22s remains)
INFO - root - 2017-12-07 15:12:19.606007: step 22840, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 62h:19m:23s remains)
INFO - root - 2017-12-07 15:12:26.391542: step 22850, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.720 sec/batch; 61h:58m:20s remains)
INFO - root - 2017-12-07 15:12:33.189100: step 22860, loss = 2.10, batch loss = 2.04 (12.7 examples/sec; 0.629 sec/batch; 54h:07m:12s remains)
INFO - root - 2017-12-07 15:12:39.742507: step 22870, loss = 2.03, batch loss = 1.97 (12.6 examples/sec; 0.633 sec/batch; 54h:26m:48s remains)
INFO - root - 2017-12-07 15:12:46.603191: step 22880, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 60h:52m:26s remains)
INFO - root - 2017-12-07 15:12:53.368396: step 22890, loss = 2.10, batch loss = 2.05 (11.0 examples/sec; 0.727 sec/batch; 62h:33m:50s remains)
INFO - root - 2017-12-07 15:13:00.110723: step 22900, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.673 sec/batch; 57h:54m:23s remains)
2017-12-07 15:13:00.821383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.311029 -4.3066387 -4.3018355 -4.2948895 -4.2882128 -4.2829542 -4.2770085 -4.2717762 -4.2711411 -4.2742944 -4.2766356 -4.2792082 -4.2846231 -4.2901912 -4.2941628][-4.3068457 -4.2977743 -4.2894912 -4.2791762 -4.26956 -4.2605348 -4.2495923 -4.2423053 -4.2424927 -4.248404 -4.2545938 -4.2624464 -4.2725258 -4.2813072 -4.2877073][-4.2888942 -4.27313 -4.2575321 -4.2390685 -4.2212787 -4.2061253 -4.1933484 -4.1916246 -4.2010889 -4.2138004 -4.2260151 -4.2411342 -4.2552819 -4.26538 -4.2728004][-4.2472749 -4.2238188 -4.2000623 -4.1721044 -4.1429782 -4.1148996 -4.0947189 -4.1021118 -4.1309986 -4.1591082 -4.1819415 -4.2073865 -4.2266326 -4.2377133 -4.2456059][-4.1745286 -4.1470265 -4.1184907 -4.0852408 -4.04873 -4.0059791 -3.972755 -3.989625 -4.0433264 -4.0906286 -4.12858 -4.1639128 -4.1847425 -4.1941342 -4.1997271][-4.0903015 -4.0616293 -4.03276 -4.0011177 -3.9617138 -3.9016805 -3.8477764 -3.8737803 -3.9499369 -4.0129814 -4.0650363 -4.107482 -4.1272473 -4.1343312 -4.1398482][-4.0291638 -4.00387 -3.9786909 -3.9587908 -3.931798 -3.8762364 -3.8206878 -3.845576 -3.911787 -3.9670186 -4.0202341 -4.0627952 -4.0788074 -4.0849605 -4.0894709][-4.0326891 -4.0158873 -4.0016093 -4.0010004 -4.0007992 -3.9748356 -3.9425035 -3.9536664 -3.9813461 -4.0051041 -4.0407572 -4.0739474 -4.0847011 -4.0909405 -4.095005][-4.1059642 -4.1000457 -4.0937767 -4.0998368 -4.109488 -4.1022692 -4.0869555 -4.0880947 -4.0904408 -4.0944867 -4.1155734 -4.1386824 -4.1438823 -4.1469893 -4.1488204][-4.1950059 -4.19597 -4.1929436 -4.1954632 -4.1998386 -4.1970696 -4.1878581 -4.1816235 -4.1749458 -4.1734796 -4.1869912 -4.2024455 -4.206862 -4.2114825 -4.2145443][-4.2472792 -4.2509379 -4.2499828 -4.2491512 -4.2485151 -4.2469668 -4.2431412 -4.2380853 -4.2318516 -4.2302694 -4.2371631 -4.2460556 -4.249444 -4.2527924 -4.2553287][-4.2611876 -4.2653928 -4.2650714 -4.2621565 -4.2604313 -4.2614202 -4.2617869 -4.2593064 -4.2570133 -4.2568507 -4.2595654 -4.26175 -4.2604547 -4.2589917 -4.2594914][-4.2449732 -4.2475195 -4.2468572 -4.24393 -4.2426548 -4.2448816 -4.2475629 -4.2465363 -4.2460217 -4.2452908 -4.2455235 -4.2438221 -4.2386675 -4.2330518 -4.2318492][-4.2338567 -4.2340307 -4.2320414 -4.2294073 -4.227798 -4.2279778 -4.2292957 -4.229043 -4.2286854 -4.2274723 -4.2275877 -4.2258787 -4.221242 -4.2171035 -4.217031][-4.2503185 -4.2491674 -4.2468748 -4.2446828 -4.2433228 -4.2425237 -4.242466 -4.2431355 -4.2428603 -4.2411766 -4.2404904 -4.2391744 -4.237042 -4.2358456 -4.2365708]]...]
INFO - root - 2017-12-07 15:13:07.582883: step 22910, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 62h:43m:30s remains)
INFO - root - 2017-12-07 15:13:14.367229: step 22920, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 61h:25m:20s remains)
INFO - root - 2017-12-07 15:13:21.187219: step 22930, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 57h:53m:02s remains)
INFO - root - 2017-12-07 15:13:27.954946: step 22940, loss = 2.09, batch loss = 2.04 (12.8 examples/sec; 0.627 sec/batch; 53h:53m:36s remains)
INFO - root - 2017-12-07 15:13:34.775604: step 22950, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 58h:46m:48s remains)
INFO - root - 2017-12-07 15:13:41.657179: step 22960, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 0.774 sec/batch; 66h:32m:57s remains)
INFO - root - 2017-12-07 15:13:48.426011: step 22970, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.724 sec/batch; 62h:14m:07s remains)
INFO - root - 2017-12-07 15:13:55.282128: step 22980, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 57h:12m:23s remains)
INFO - root - 2017-12-07 15:14:01.944220: step 22990, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.638 sec/batch; 54h:49m:18s remains)
INFO - root - 2017-12-07 15:14:08.788457: step 23000, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 60h:29m:13s remains)
2017-12-07 15:14:09.587648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1650238 -4.1620741 -4.1623621 -4.1595054 -4.1526446 -4.1356483 -4.1220579 -4.1365767 -4.1731009 -4.1965313 -4.2105513 -4.2243347 -4.2410564 -4.251194 -4.2646184][-4.1391625 -4.143539 -4.1589952 -4.1692467 -4.1671138 -4.1463146 -4.1271911 -4.1374059 -4.16706 -4.1845789 -4.1968646 -4.2112985 -4.2303977 -4.2406487 -4.2531762][-4.1325397 -4.13806 -4.1584654 -4.1789465 -4.1832333 -4.1641579 -4.14526 -4.1536932 -4.17827 -4.1929855 -4.2055488 -4.2207379 -4.2371864 -4.2435203 -4.2518768][-4.1467171 -4.1495056 -4.1682806 -4.1945238 -4.2035441 -4.1858797 -4.1685147 -4.1772447 -4.1990986 -4.2137585 -4.2280779 -4.2425456 -4.2532506 -4.2546272 -4.2578216][-4.1608019 -4.1607518 -4.177618 -4.2047834 -4.2131009 -4.1966643 -4.1885333 -4.2027044 -4.2229233 -4.242568 -4.2579408 -4.2651854 -4.2690959 -4.2659726 -4.2660332][-4.1510124 -4.1454554 -4.154696 -4.17342 -4.1738443 -4.1508961 -4.1511722 -4.1769304 -4.2062473 -4.2398925 -4.2627707 -4.26875 -4.2701082 -4.2653232 -4.2663708][-4.127995 -4.1132326 -4.1105709 -4.1173019 -4.1078162 -4.0707278 -4.0640707 -4.0919108 -4.1316085 -4.1807876 -4.2198296 -4.2362723 -4.2425637 -4.245676 -4.2562604][-4.1049223 -4.0841203 -4.07029 -4.0716815 -4.0591025 -4.000174 -3.9657574 -3.9787493 -4.0193 -4.079587 -4.1404643 -4.1772051 -4.1969986 -4.2155876 -4.2402339][-4.0806823 -4.0604224 -4.0432386 -4.0418534 -4.030148 -3.9647987 -3.903708 -3.8846142 -3.9094684 -3.9738431 -4.0595961 -4.1179147 -4.1539221 -4.1862607 -4.2210054][-4.0757279 -4.062531 -4.047946 -4.0466962 -4.0353284 -3.9762297 -3.9129694 -3.8787243 -3.8809681 -3.9363976 -4.0360413 -4.102078 -4.14481 -4.1808014 -4.2154174][-4.0921011 -4.0876036 -4.0826497 -4.0839796 -4.0753503 -4.02445 -3.9700155 -3.9409704 -3.9399252 -3.9879136 -4.0772996 -4.1306486 -4.1672759 -4.1982312 -4.2277374][-4.1245189 -4.1279588 -4.1317639 -4.1383715 -4.1297259 -4.0887828 -4.0496521 -4.0351534 -4.0416059 -4.0818086 -4.1444597 -4.1805286 -4.2084866 -4.2327747 -4.2557082][-4.1783042 -4.1865129 -4.1978774 -4.2080679 -4.2025752 -4.1757431 -4.150001 -4.1430387 -4.15026 -4.1762552 -4.2129464 -4.2372437 -4.2549949 -4.2708154 -4.2855868][-4.2352567 -4.2425303 -4.25155 -4.2586865 -4.2553558 -4.2428007 -4.2309046 -4.2272344 -4.2310519 -4.2467837 -4.2667351 -4.2800322 -4.2885509 -4.2980719 -4.3074031][-4.2753325 -4.2768803 -4.2802863 -4.2820764 -4.2804675 -4.2774005 -4.2749953 -4.2739973 -4.2752657 -4.2814908 -4.2909551 -4.2996426 -4.3045774 -4.3115659 -4.3189759]]...]
INFO - root - 2017-12-07 15:14:16.322066: step 23010, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 53h:02m:52s remains)
INFO - root - 2017-12-07 15:14:23.184239: step 23020, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 56h:11m:08s remains)
INFO - root - 2017-12-07 15:14:30.058576: step 23030, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 61h:33m:29s remains)
INFO - root - 2017-12-07 15:14:36.892027: step 23040, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 62h:11m:16s remains)
INFO - root - 2017-12-07 15:14:43.617783: step 23050, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 59h:21m:39s remains)
INFO - root - 2017-12-07 15:14:50.379613: step 23060, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 56h:27m:27s remains)
INFO - root - 2017-12-07 15:14:57.223887: step 23070, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 55h:38m:05s remains)
INFO - root - 2017-12-07 15:15:04.145523: step 23080, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.752 sec/batch; 64h:40m:23s remains)
INFO - root - 2017-12-07 15:15:10.900042: step 23090, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 62h:10m:35s remains)
INFO - root - 2017-12-07 15:15:17.718079: step 23100, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 57h:19m:20s remains)
2017-12-07 15:15:18.400764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2556891 -4.27435 -4.2946277 -4.3141866 -4.32661 -4.3246303 -4.311379 -4.2865262 -4.27769 -4.2913656 -4.3208442 -4.3457794 -4.3563137 -4.356638 -4.3558335][-4.2487531 -4.2673688 -4.2874727 -4.3065324 -4.3180318 -4.3167744 -4.3030119 -4.2762437 -4.2671418 -4.2857738 -4.3201275 -4.345808 -4.3533707 -4.3512206 -4.3508806][-4.2504816 -4.263299 -4.2798738 -4.296742 -4.3090568 -4.3100924 -4.2987781 -4.2722049 -4.261466 -4.27691 -4.3091483 -4.3325434 -4.3398542 -4.340631 -4.344707][-4.2618446 -4.2671933 -4.2792888 -4.2944412 -4.3073134 -4.3079839 -4.2958956 -4.2649746 -4.2448411 -4.2493973 -4.2763071 -4.3005095 -4.3151135 -4.3257461 -4.3381672][-4.27716 -4.2790465 -4.2886653 -4.2986612 -4.3062534 -4.2997541 -4.2762017 -4.23127 -4.199554 -4.1974626 -4.2228265 -4.2525325 -4.2794466 -4.3054223 -4.3277326][-4.2714515 -4.2763872 -4.2878656 -4.2923837 -4.2865591 -4.260983 -4.2120929 -4.145865 -4.1071758 -4.1091304 -4.1451721 -4.1925011 -4.2409229 -4.2847056 -4.3157463][-4.1976824 -4.21323 -4.2337203 -4.2376022 -4.2151151 -4.1638474 -4.08742 -4.0032392 -3.9746423 -4.0042124 -4.0735574 -4.1496406 -4.2217946 -4.2768521 -4.311851][-4.0682034 -4.1073503 -4.14162 -4.1439729 -4.1045156 -4.0308142 -3.9322312 -3.8430595 -3.8508568 -3.9307327 -4.0412717 -4.1435351 -4.2249627 -4.2797689 -4.3150139][-3.9517982 -4.0197129 -4.0681062 -4.0668693 -4.0147653 -3.93209 -3.8298826 -3.7560086 -3.8106811 -3.9349935 -4.0671272 -4.1724653 -4.2471442 -4.2950225 -4.3273792][-3.9338555 -4.0191135 -4.0734267 -4.0669 -4.0106812 -3.9414697 -3.8696561 -3.8317943 -3.9031203 -4.0249743 -4.1423578 -4.2286477 -4.2870655 -4.3240881 -4.3479333][-4.025846 -4.0959554 -4.1392717 -4.1325188 -4.0871396 -4.0408373 -4.0010362 -3.9891164 -4.0457993 -4.13428 -4.2187123 -4.2801886 -4.322464 -4.3497162 -4.3645325][-4.1389928 -4.1761622 -4.2003222 -4.1965337 -4.16825 -4.143754 -4.127399 -4.1295142 -4.1694655 -4.2246737 -4.277782 -4.3186307 -4.3491168 -4.3686962 -4.3767238][-4.2297363 -4.2440805 -4.2526603 -4.2492743 -4.2332883 -4.2241979 -4.2225857 -4.2298641 -4.2540789 -4.2850003 -4.3179235 -4.3466315 -4.3696022 -4.3836617 -4.3857479][-4.2834644 -4.29014 -4.2926731 -4.287951 -4.28026 -4.2801385 -4.2853642 -4.2933936 -4.3083596 -4.3270445 -4.34845 -4.3683343 -4.3837991 -4.3908396 -4.3873377][-4.316123 -4.3201375 -4.320785 -4.3181252 -4.314909 -4.3161864 -4.3218827 -4.3289189 -4.338448 -4.3497491 -4.3620772 -4.3731575 -4.38067 -4.3839097 -4.380589]]...]
INFO - root - 2017-12-07 15:15:25.247690: step 23110, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.740 sec/batch; 63h:37m:58s remains)
INFO - root - 2017-12-07 15:15:32.094412: step 23120, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 63h:17m:21s remains)
INFO - root - 2017-12-07 15:15:38.917730: step 23130, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 55h:59m:23s remains)
INFO - root - 2017-12-07 15:15:45.652582: step 23140, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 55h:57m:33s remains)
INFO - root - 2017-12-07 15:15:52.365489: step 23150, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 57h:48m:36s remains)
INFO - root - 2017-12-07 15:15:59.213584: step 23160, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 61h:34m:00s remains)
INFO - root - 2017-12-07 15:16:06.110878: step 23170, loss = 2.04, batch loss = 1.99 (10.6 examples/sec; 0.753 sec/batch; 64h:42m:18s remains)
INFO - root - 2017-12-07 15:16:12.694569: step 23180, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 54h:24m:20s remains)
INFO - root - 2017-12-07 15:16:19.337944: step 23190, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 55h:05m:09s remains)
INFO - root - 2017-12-07 15:16:26.159463: step 23200, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 60h:37m:08s remains)
2017-12-07 15:16:26.824184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390493 -4.3347158 -4.3298597 -4.3276043 -4.330071 -4.3367419 -4.3438129 -4.3466892 -4.3474846 -4.3463426 -4.3424625 -4.3383927 -4.3347473 -4.3329692 -4.3337188][-4.34297 -4.3391647 -4.3353057 -4.333704 -4.3350863 -4.339057 -4.3430481 -4.3444991 -4.3458314 -4.3460484 -4.3433933 -4.3397036 -4.3362837 -4.3351159 -4.3369818][-4.3439984 -4.3418107 -4.3401203 -4.3385363 -4.3368878 -4.3358712 -4.3361626 -4.3373623 -4.3411226 -4.3441782 -4.3438597 -4.3408575 -4.3369422 -4.3353958 -4.3381171][-4.3430414 -4.3420277 -4.3412228 -4.337564 -4.3303661 -4.3222308 -4.318337 -4.3201113 -4.3278155 -4.3373628 -4.3429627 -4.3428016 -4.3392653 -4.3366127 -4.3384833][-4.339045 -4.3384204 -4.3359532 -4.3250885 -4.3058872 -4.2853861 -4.2735186 -4.2755666 -4.2914352 -4.314085 -4.3313508 -4.3382621 -4.3371735 -4.3349543 -4.3359942][-4.3311477 -4.3294044 -4.3211613 -4.297442 -4.2581468 -4.2156606 -4.1880589 -4.1906695 -4.2207818 -4.2645793 -4.3021817 -4.3227839 -4.3286438 -4.3289337 -4.330368][-4.321208 -4.3170147 -4.3001924 -4.2574706 -4.1875477 -4.1081271 -4.0527515 -4.055315 -4.109992 -4.1879272 -4.2563505 -4.298315 -4.315773 -4.3211432 -4.3247986][-4.3125086 -4.3060255 -4.2815652 -4.2205334 -4.11552 -3.9887214 -3.8957686 -3.9014971 -3.9864874 -4.1029639 -4.2053208 -4.272059 -4.3045373 -4.3165703 -4.3225956][-4.3115444 -4.3045168 -4.2762494 -4.204432 -4.0772753 -3.91932 -3.802011 -3.817066 -3.9252014 -4.0638227 -4.1835904 -4.2641354 -4.3061891 -4.3211169 -4.3256125][-4.3211451 -4.3168807 -4.2913384 -4.2236004 -4.1042361 -3.959461 -3.8559465 -3.8700695 -3.9625807 -4.0862169 -4.1965995 -4.2751369 -4.3177109 -4.33033 -4.3307552][-4.3331189 -4.3350711 -4.3189168 -4.2701969 -4.1839495 -4.0821395 -4.0110097 -4.0142016 -4.0689445 -4.154551 -4.2355857 -4.296536 -4.3295321 -4.3376126 -4.3343797][-4.3382964 -4.348238 -4.3450861 -4.3189549 -4.2693787 -4.20906 -4.166832 -4.162077 -4.187223 -4.2354245 -4.2832661 -4.3203106 -4.340425 -4.3431921 -4.33624][-4.3383284 -4.3536391 -4.3607421 -4.3526974 -4.3296552 -4.296484 -4.2714057 -4.262753 -4.2697067 -4.2920403 -4.3163176 -4.3369379 -4.3487697 -4.3471684 -4.3376613][-4.3360376 -4.3526468 -4.3649521 -4.3674703 -4.3586311 -4.3389316 -4.3192763 -4.3059568 -4.3025289 -4.3120356 -4.32649 -4.340374 -4.3502946 -4.3482141 -4.3393354][-4.3285022 -4.3430781 -4.3578658 -4.3649507 -4.3603797 -4.3436155 -4.3228669 -4.3052216 -4.2983952 -4.3061833 -4.3212752 -4.3353834 -4.3470435 -4.3479829 -4.3419828]]...]
INFO - root - 2017-12-07 15:16:33.684203: step 23210, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 54h:00m:46s remains)
INFO - root - 2017-12-07 15:16:40.478373: step 23220, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 56h:22m:13s remains)
INFO - root - 2017-12-07 15:16:47.346101: step 23230, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.750 sec/batch; 64h:27m:46s remains)
INFO - root - 2017-12-07 15:16:54.157735: step 23240, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 62h:48m:44s remains)
INFO - root - 2017-12-07 15:17:00.958276: step 23250, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.680 sec/batch; 58h:25m:46s remains)
INFO - root - 2017-12-07 15:17:07.725691: step 23260, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 56h:15m:42s remains)
INFO - root - 2017-12-07 15:17:14.509088: step 23270, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 54h:19m:18s remains)
INFO - root - 2017-12-07 15:17:21.444184: step 23280, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 62h:08m:33s remains)
INFO - root - 2017-12-07 15:17:28.102916: step 23290, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 60h:16m:00s remains)
INFO - root - 2017-12-07 15:17:34.783979: step 23300, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 57h:49m:30s remains)
2017-12-07 15:17:35.488218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.276443 -4.2786584 -4.27976 -4.2847137 -4.277997 -4.2619276 -4.261281 -4.2940454 -4.3120527 -4.3124022 -4.30791 -4.2785 -4.249999 -4.2550564 -4.2833586][-4.2444057 -4.2366896 -4.2394347 -4.2528768 -4.2546296 -4.2449656 -4.2479992 -4.280921 -4.2996721 -4.3017335 -4.2957177 -4.2642312 -4.2366333 -4.2449369 -4.2774806][-4.25086 -4.2401857 -4.2458329 -4.2600756 -4.2625866 -4.2541981 -4.2524567 -4.2761312 -4.2934265 -4.2965488 -4.2869744 -4.2552228 -4.2304897 -4.2403669 -4.2729292][-4.270576 -4.2652369 -4.2767396 -4.2834487 -4.2754822 -4.2576652 -4.2451453 -4.25725 -4.2738814 -4.2833033 -4.2788267 -4.2533092 -4.2351241 -4.2471638 -4.2750216][-4.2946372 -4.2932205 -4.2969513 -4.283989 -4.2553511 -4.216928 -4.18878 -4.1952872 -4.2291293 -4.2601995 -4.2690625 -4.2523351 -4.2384281 -4.2505836 -4.2734165][-4.30594 -4.3026867 -4.285378 -4.2449784 -4.1876793 -4.1196175 -4.0691748 -4.0804577 -4.1525793 -4.2235603 -4.2521572 -4.2405562 -4.2268214 -4.2402868 -4.2622747][-4.3040023 -4.2915826 -4.2530556 -4.1863413 -4.1005559 -3.9914849 -3.9054163 -3.9220138 -4.0357552 -4.1507273 -4.2061729 -4.2036233 -4.1963844 -4.2189455 -4.2477236][-4.2925644 -4.2754192 -4.229063 -4.1580167 -4.0677185 -3.9484744 -3.857851 -3.8814583 -4.0000176 -4.1188755 -4.1797647 -4.1828666 -4.1859064 -4.2191548 -4.2526579][-4.2884965 -4.2744188 -4.2368736 -4.185401 -4.124186 -4.044364 -3.9902227 -4.010931 -4.0898671 -4.16631 -4.2037683 -4.2018862 -4.2071958 -4.23934 -4.2696648][-4.3018565 -4.296968 -4.2734151 -4.2427397 -4.2090425 -4.166049 -4.14038 -4.155056 -4.1983795 -4.2386 -4.2532711 -4.2431602 -4.2435222 -4.2695379 -4.29307][-4.31493 -4.315094 -4.3003182 -4.2838979 -4.2673268 -4.2454424 -4.2361045 -4.2476563 -4.2697334 -4.2892008 -4.2903938 -4.274354 -4.2686872 -4.2870822 -4.3050461][-4.3110328 -4.3141813 -4.3084354 -4.3011074 -4.2941494 -4.2836528 -4.2813983 -4.2893109 -4.2999716 -4.3057837 -4.2962708 -4.2746835 -4.2653675 -4.2794604 -4.2957649][-4.2996068 -4.3020582 -4.3014064 -4.2984324 -4.2938676 -4.2854762 -4.2839303 -4.2894664 -4.2962255 -4.2967024 -4.2812567 -4.2556162 -4.24576 -4.2634373 -4.2823415][-4.306612 -4.3052368 -4.3022461 -4.2974677 -4.2895408 -4.2795143 -4.2743959 -4.2738061 -4.2767563 -4.2745581 -4.2586637 -4.2329617 -4.2290382 -4.25425 -4.2783451][-4.3296862 -4.3219204 -4.3111057 -4.300221 -4.2878714 -4.2738533 -4.2631392 -4.2575769 -4.2587972 -4.2573805 -4.245213 -4.2198172 -4.2184839 -4.2474685 -4.2764359]]...]
INFO - root - 2017-12-07 15:17:42.177377: step 23310, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 57h:43m:26s remains)
INFO - root - 2017-12-07 15:17:49.027952: step 23320, loss = 2.04, batch loss = 1.98 (10.5 examples/sec; 0.765 sec/batch; 65h:41m:40s remains)
INFO - root - 2017-12-07 15:17:55.893052: step 23330, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.756 sec/batch; 64h:55m:51s remains)
INFO - root - 2017-12-07 15:18:02.764565: step 23340, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 59h:37m:30s remains)
INFO - root - 2017-12-07 15:18:09.481137: step 23350, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 53h:37m:40s remains)
INFO - root - 2017-12-07 15:18:16.393411: step 23360, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 62h:16m:28s remains)
INFO - root - 2017-12-07 15:18:23.349196: step 23370, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.749 sec/batch; 64h:20m:49s remains)
INFO - root - 2017-12-07 15:18:30.193298: step 23380, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 57h:30m:40s remains)
INFO - root - 2017-12-07 15:18:36.882429: step 23390, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 54h:52m:25s remains)
INFO - root - 2017-12-07 15:18:43.564908: step 23400, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.620 sec/batch; 53h:14m:19s remains)
2017-12-07 15:18:44.251759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.261384 -4.289928 -4.30956 -4.3152013 -4.3017807 -4.2641244 -4.2242146 -4.2045555 -4.2085562 -4.2274747 -4.2584925 -4.2848849 -4.2899451 -4.2826467 -4.2785239][-4.2517676 -4.2826958 -4.3097014 -4.3220181 -4.304152 -4.2578382 -4.2083263 -4.1819782 -4.1927361 -4.2186275 -4.2507434 -4.2807803 -4.2927775 -4.2942533 -4.2935352][-4.2374287 -4.2728319 -4.3038788 -4.3195806 -4.2981319 -4.250083 -4.1973567 -4.1673245 -4.1741819 -4.2003779 -4.2325563 -4.2642612 -4.2809267 -4.2932415 -4.3000336][-4.2250266 -4.2628803 -4.294497 -4.3049912 -4.2775526 -4.2295022 -4.1803427 -4.1538382 -4.1555023 -4.1778717 -4.2121758 -4.2468638 -4.2698541 -4.2932692 -4.3092318][-4.221334 -4.2600803 -4.2864256 -4.2848816 -4.2501163 -4.1983495 -4.1486773 -4.125782 -4.1317 -4.1570778 -4.192728 -4.2293186 -4.2623129 -4.2955251 -4.316988][-4.2233744 -4.2609496 -4.2797575 -4.2679591 -4.22534 -4.1657228 -4.1098604 -4.0873518 -4.1034036 -4.1401281 -4.1776676 -4.2134376 -4.2550216 -4.2946548 -4.3187571][-4.233202 -4.2641878 -4.2772632 -4.261467 -4.214715 -4.1498237 -4.0836272 -4.052247 -4.0735755 -4.1226711 -4.1647215 -4.203198 -4.2484617 -4.2915616 -4.3180285][-4.2386808 -4.2581992 -4.2664933 -4.2508612 -4.2075996 -4.1452656 -4.0746007 -4.0356922 -4.0550971 -4.1075788 -4.1516986 -4.1947656 -4.243669 -4.2878704 -4.3175507][-4.2405505 -4.248137 -4.2500358 -4.2336783 -4.1991763 -4.148016 -4.0834384 -4.0425286 -4.0546947 -4.0982842 -4.1416163 -4.1881208 -4.236186 -4.2792521 -4.3100762][-4.2348008 -4.22986 -4.2245045 -4.2116704 -4.1914554 -4.1561475 -4.104383 -4.0674028 -4.070097 -4.1007066 -4.1396694 -4.1849217 -4.2281613 -4.2668133 -4.2946792][-4.2260265 -4.2076373 -4.1950221 -4.1872044 -4.1819053 -4.1653318 -4.1328321 -4.1060133 -4.1029477 -4.1218338 -4.1510439 -4.1883922 -4.2242718 -4.2577648 -4.282371][-4.2177558 -4.19407 -4.1808238 -4.1767535 -4.1826782 -4.1816149 -4.1655316 -4.1460476 -4.140018 -4.1536121 -4.1749144 -4.2048817 -4.2322097 -4.2567658 -4.277132][-4.2162929 -4.2002726 -4.194664 -4.1990638 -4.2138996 -4.2206249 -4.2119884 -4.1959205 -4.1874013 -4.1945705 -4.2080975 -4.2305741 -4.2492 -4.2654467 -4.2811184][-4.2334318 -4.2243166 -4.2235751 -4.2345862 -4.256866 -4.270288 -4.2665153 -4.2528181 -4.2421975 -4.2399464 -4.2417688 -4.2529206 -4.2640042 -4.274663 -4.288373][-4.2653904 -4.2623353 -4.263525 -4.2784691 -4.3022709 -4.317976 -4.3159251 -4.30134 -4.2832513 -4.2697086 -4.2619004 -4.2622547 -4.2658768 -4.2729387 -4.2874303]]...]
INFO - root - 2017-12-07 15:18:51.032768: step 23410, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 56h:48m:47s remains)
INFO - root - 2017-12-07 15:18:57.719496: step 23420, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 59h:26m:38s remains)
INFO - root - 2017-12-07 15:19:04.452611: step 23430, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 54h:35m:14s remains)
INFO - root - 2017-12-07 15:19:11.306998: step 23440, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 58h:56m:01s remains)
INFO - root - 2017-12-07 15:19:18.089002: step 23450, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 62h:44m:51s remains)
INFO - root - 2017-12-07 15:19:24.918167: step 23460, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 56h:01m:55s remains)
INFO - root - 2017-12-07 15:19:31.810288: step 23470, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 56h:08m:22s remains)
INFO - root - 2017-12-07 15:19:38.613988: step 23480, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 57h:01m:17s remains)
INFO - root - 2017-12-07 15:19:44.809825: step 23490, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 56h:42m:24s remains)
INFO - root - 2017-12-07 15:19:51.578811: step 23500, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 62h:16m:09s remains)
2017-12-07 15:19:52.340115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2865124 -4.2736659 -4.2757092 -4.2851977 -4.2960835 -4.3044147 -4.3031893 -4.293252 -4.2834759 -4.2762952 -4.2692142 -4.2656188 -4.2655864 -4.2660131 -4.2705417][-4.2666779 -4.252758 -4.2603984 -4.275322 -4.2912345 -4.2989216 -4.292048 -4.2779713 -4.2663183 -4.2591176 -4.2491589 -4.2427588 -4.2418418 -4.2406182 -4.2448382][-4.2413535 -4.2291317 -4.239635 -4.2566833 -4.2732944 -4.2748938 -4.2598448 -4.2427979 -4.2368312 -4.232769 -4.2164431 -4.2048826 -4.203619 -4.2024689 -4.2102895][-4.2309408 -4.2197428 -4.2317181 -4.2466412 -4.2561631 -4.244729 -4.2118416 -4.1910968 -4.1966038 -4.2002726 -4.1793909 -4.1657166 -4.1679807 -4.1733103 -4.1893873][-4.2142868 -4.2032804 -4.2171779 -4.230125 -4.225955 -4.1935916 -4.1339474 -4.105957 -4.1277514 -4.1444097 -4.12718 -4.1154113 -4.1300917 -4.1513681 -4.1822929][-4.1921606 -4.1767192 -4.1863055 -4.1881385 -4.1592741 -4.0937157 -3.9966714 -3.965379 -4.0260077 -4.0805449 -4.0819392 -4.0790911 -4.1104074 -4.1489043 -4.1908789][-4.1610022 -4.130908 -4.128098 -4.1132421 -4.0555429 -3.9485817 -3.8048284 -3.7818723 -3.9147785 -4.0303483 -4.0615525 -4.0707741 -4.1133 -4.1612191 -4.2056217][-4.14181 -4.0933867 -4.0775118 -4.05265 -3.97959 -3.8444817 -3.6767769 -3.6713076 -3.851604 -4.0014729 -4.0552068 -4.0708055 -4.1126232 -4.1631641 -4.2070031][-4.1518583 -4.0958619 -4.0786147 -4.0626407 -4.014051 -3.9124475 -3.7930224 -3.7999156 -3.9377623 -4.0542612 -4.0986576 -4.11101 -4.1429148 -4.1821461 -4.2172346][-4.15633 -4.1090446 -4.10135 -4.1021857 -4.0860891 -4.0306768 -3.9651144 -3.9766276 -4.0597568 -4.13236 -4.1551256 -4.1605453 -4.1800761 -4.2059736 -4.2327929][-4.1841488 -4.1469574 -4.1505785 -4.1650586 -4.1656775 -4.1388531 -4.1031179 -4.1121335 -4.1610866 -4.2009177 -4.2110896 -4.2132006 -4.2271643 -4.2456918 -4.2626734][-4.2270393 -4.2007694 -4.2163773 -4.2393422 -4.2452126 -4.2340889 -4.2126732 -4.2122655 -4.2400351 -4.263432 -4.2710423 -4.2735462 -4.2831216 -4.2954082 -4.3000717][-4.2719011 -4.2597404 -4.2804613 -4.3018851 -4.3091168 -4.3048635 -4.2914314 -4.2841926 -4.2945762 -4.3102517 -4.31682 -4.3172922 -4.3209558 -4.3260903 -4.3244019][-4.3048487 -4.2993383 -4.3133359 -4.3267093 -4.3344316 -4.335197 -4.3279243 -4.3184175 -4.3180213 -4.3277688 -4.3303895 -4.3270478 -4.327342 -4.3294768 -4.3288221][-4.3229246 -4.31763 -4.3214912 -4.3262691 -4.3297205 -4.3305073 -4.3280048 -4.3216615 -4.3191252 -4.3248158 -4.3249307 -4.3203897 -4.3210068 -4.3238564 -4.3257813]]...]
INFO - root - 2017-12-07 15:19:59.162533: step 23510, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 55h:06m:48s remains)
INFO - root - 2017-12-07 15:20:06.043227: step 23520, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 58h:28m:50s remains)
INFO - root - 2017-12-07 15:20:13.021782: step 23530, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 61h:42m:28s remains)
INFO - root - 2017-12-07 15:20:19.807222: step 23540, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.669 sec/batch; 57h:24m:53s remains)
INFO - root - 2017-12-07 15:20:26.512355: step 23550, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 53h:47m:55s remains)
INFO - root - 2017-12-07 15:20:33.195620: step 23560, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 55h:39m:19s remains)
INFO - root - 2017-12-07 15:20:40.053232: step 23570, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 61h:15m:56s remains)
INFO - root - 2017-12-07 15:20:46.843104: step 23580, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 61h:09m:40s remains)
INFO - root - 2017-12-07 15:20:53.493221: step 23590, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 56h:48m:02s remains)
INFO - root - 2017-12-07 15:21:00.333827: step 23600, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 57h:15m:23s remains)
2017-12-07 15:21:01.093259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2503018 -4.2496676 -4.2546268 -4.2633386 -4.2667518 -4.2678161 -4.2649565 -4.2540822 -4.236908 -4.2252049 -4.2167568 -4.2127132 -4.2158232 -4.2218947 -4.2196107][-4.254405 -4.2508397 -4.25325 -4.2609687 -4.264523 -4.2657003 -4.2587776 -4.2416997 -4.2227111 -4.2154746 -4.2118187 -4.2149391 -4.2238789 -4.2318511 -4.2268162][-4.2566853 -4.2462034 -4.2426658 -4.246767 -4.2485847 -4.2479315 -4.2370119 -4.2160635 -4.19954 -4.2002206 -4.2060137 -4.2181573 -4.2315497 -4.2389755 -4.2323775][-4.2542462 -4.2376637 -4.2266588 -4.221911 -4.2156734 -4.2067289 -4.1903639 -4.1663022 -4.1568861 -4.168819 -4.1858225 -4.2078214 -4.2253394 -4.233871 -4.2287054][-4.2456522 -4.2231584 -4.2031703 -4.1853533 -4.165751 -4.1421738 -4.1133189 -4.0853009 -4.0871372 -4.1147985 -4.1443911 -4.1795459 -4.2083015 -4.2239566 -4.2227483][-4.2340555 -4.2053647 -4.1745825 -4.1427975 -4.1063423 -4.061676 -4.0094447 -3.9749098 -3.9936752 -4.0454082 -4.0938845 -4.1461844 -4.1920834 -4.2187262 -4.2225642][-4.2270856 -4.1948113 -4.156878 -4.1144619 -4.0618939 -3.9953952 -3.919528 -3.877624 -3.9165058 -3.992944 -4.0604076 -4.1279225 -4.1865525 -4.2203364 -4.2273459][-4.2305369 -4.2012014 -4.1657004 -4.1214895 -4.0656452 -3.9958622 -3.9187613 -3.8783109 -3.9218102 -4.0001745 -4.0683894 -4.1343322 -4.1929479 -4.226119 -4.2329268][-4.2372804 -4.2113252 -4.1785398 -4.1403933 -4.0950308 -4.042841 -3.9906945 -3.9611595 -3.9938838 -4.0556717 -4.1105456 -4.1642938 -4.2139335 -4.2411251 -4.2438812][-4.2460337 -4.2225595 -4.1923566 -4.1607232 -4.1288624 -4.0967288 -4.0694146 -4.0502653 -4.0742993 -4.1198454 -4.1594515 -4.1997995 -4.2386231 -4.2587237 -4.2575817][-4.2586274 -4.2391844 -4.2125926 -4.1837 -4.1596575 -4.1404877 -4.1250167 -4.11229 -4.1325407 -4.1682334 -4.1971312 -4.2259388 -4.2556825 -4.2696595 -4.2669897][-4.2572474 -4.2431154 -4.2226644 -4.1985931 -4.1804595 -4.1673422 -4.1542778 -4.1431413 -4.1616268 -4.1917267 -4.2150006 -4.2385831 -4.262012 -4.2708626 -4.2687507][-4.2466884 -4.2342038 -4.2184734 -4.200243 -4.1870308 -4.1764026 -4.1651173 -4.1579609 -4.1750765 -4.1996994 -4.2205558 -4.2417712 -4.260705 -4.266726 -4.2662849][-4.233418 -4.2213516 -4.2081037 -4.19547 -4.185802 -4.1769228 -4.1689343 -4.1660008 -4.1809373 -4.2023273 -4.2207975 -4.2394123 -4.255631 -4.2604666 -4.2627234][-4.2212119 -4.2087474 -4.1984029 -4.18971 -4.18305 -4.1785135 -4.1753993 -4.1758471 -4.1881461 -4.2054682 -4.2212391 -4.237505 -4.2519569 -4.2564578 -4.2599392]]...]
INFO - root - 2017-12-07 15:21:07.977011: step 23610, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 58h:06m:40s remains)
INFO - root - 2017-12-07 15:21:14.788183: step 23620, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 54h:55m:28s remains)
INFO - root - 2017-12-07 15:21:21.638823: step 23630, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 58h:15m:36s remains)
INFO - root - 2017-12-07 15:21:28.448745: step 23640, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.721 sec/batch; 61h:52m:06s remains)
INFO - root - 2017-12-07 15:21:35.357647: step 23650, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 63h:41m:15s remains)
INFO - root - 2017-12-07 15:21:42.119962: step 23660, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 56h:27m:47s remains)
INFO - root - 2017-12-07 15:21:48.926477: step 23670, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 55h:49m:01s remains)
INFO - root - 2017-12-07 15:21:55.751537: step 23680, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 58h:07m:48s remains)
INFO - root - 2017-12-07 15:22:02.353410: step 23690, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 59h:37m:48s remains)
INFO - root - 2017-12-07 15:22:09.132651: step 23700, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.747 sec/batch; 64h:05m:40s remains)
2017-12-07 15:22:09.893181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.12579 -4.1149826 -4.0957565 -4.0668068 -4.0607719 -4.0878644 -4.1359358 -4.1853943 -4.2166848 -4.2200308 -4.1972194 -4.1594205 -4.1238194 -4.102963 -4.114531][-4.1436577 -4.1333961 -4.1123943 -4.0843716 -4.0760674 -4.098341 -4.1386342 -4.1798186 -4.2045455 -4.2057695 -4.1863523 -4.1615624 -4.1462831 -4.1410351 -4.1554046][-4.1695204 -4.1628222 -4.1398931 -4.1079893 -4.094173 -4.1078672 -4.1373386 -4.1691604 -4.1882181 -4.1914492 -4.1817546 -4.1736054 -4.176466 -4.1857939 -4.2062731][-4.2024312 -4.1989832 -4.1756697 -4.1407804 -4.1166139 -4.115335 -4.1306672 -4.1515436 -4.1642118 -4.1672797 -4.1670179 -4.170763 -4.1831784 -4.20029 -4.2240572][-4.2330055 -4.2265439 -4.1973553 -4.1545153 -4.1232576 -4.1135874 -4.1171074 -4.1215506 -4.1236963 -4.12652 -4.1337562 -4.1481395 -4.1681957 -4.18966 -4.2142949][-4.2451787 -4.2257462 -4.1828008 -4.1266179 -4.0875583 -4.071156 -4.067709 -4.0644488 -4.0646238 -4.0748949 -4.0964746 -4.1290784 -4.1635094 -4.1899614 -4.2147722][-4.2298489 -4.1984868 -4.1433129 -4.0789447 -4.0358624 -4.0155921 -4.0071421 -3.999191 -3.9957187 -4.0129552 -4.0547223 -4.1091042 -4.1573076 -4.1877189 -4.213171][-4.2053852 -4.1696796 -4.1125917 -4.0468183 -4.00198 -3.9757535 -3.9570475 -3.9365978 -3.9156375 -3.9292941 -3.9929614 -4.0703707 -4.1305466 -4.1676292 -4.1925144][-4.1857882 -4.1501617 -4.0983658 -4.0389142 -3.9978149 -3.9719453 -3.947849 -3.9195585 -3.8893168 -3.9010344 -3.9703367 -4.0465178 -4.100873 -4.1355343 -4.1567841][-4.1713467 -4.1395774 -4.0962057 -4.0511503 -4.0283461 -4.018024 -4.0031161 -3.9819846 -3.9564648 -3.9599104 -4.0067453 -4.0588264 -4.0949812 -4.121223 -4.1399312][-4.1617532 -4.1358657 -4.1041245 -4.0761075 -4.0697837 -4.0730515 -4.0696187 -4.0541782 -4.0276284 -4.0199862 -4.0463123 -4.0820756 -4.1094637 -4.1332059 -4.1511121][-4.1643252 -4.1488414 -4.1310859 -4.1160135 -4.1134262 -4.1142287 -4.1097922 -4.0930829 -4.0637488 -4.0529318 -4.0753422 -4.106503 -4.1317105 -4.153708 -4.1698575][-4.1737633 -4.1719408 -4.169271 -4.164794 -4.1605244 -4.1505494 -4.1347589 -4.1095576 -4.0760412 -4.0659413 -4.0889893 -4.12133 -4.1493883 -4.1730375 -4.1876664][-4.1901035 -4.1986008 -4.2082324 -4.2121634 -4.2070308 -4.1868219 -4.1577716 -4.1204805 -4.0813918 -4.0728292 -4.0964208 -4.1305866 -4.164505 -4.1911631 -4.2020726][-4.21108 -4.2243977 -4.2411642 -4.2523432 -4.2487326 -4.2236266 -4.1888046 -4.1484909 -4.1112347 -4.1019516 -4.1217017 -4.1544113 -4.186739 -4.2061572 -4.20773]]...]
INFO - root - 2017-12-07 15:22:16.687672: step 23710, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 63h:20m:40s remains)
INFO - root - 2017-12-07 15:22:23.473998: step 23720, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 63h:46m:04s remains)
INFO - root - 2017-12-07 15:22:30.283587: step 23730, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.661 sec/batch; 56h:42m:14s remains)
INFO - root - 2017-12-07 15:22:37.069020: step 23740, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 55h:37m:19s remains)
INFO - root - 2017-12-07 15:22:44.045742: step 23750, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 55h:13m:26s remains)
INFO - root - 2017-12-07 15:22:50.825576: step 23760, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 61h:20m:48s remains)
INFO - root - 2017-12-07 15:22:57.634728: step 23770, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 61h:35m:09s remains)
INFO - root - 2017-12-07 15:23:04.443070: step 23780, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 58h:29m:48s remains)
INFO - root - 2017-12-07 15:23:11.053976: step 23790, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 55h:30m:40s remains)
INFO - root - 2017-12-07 15:23:17.553479: step 23800, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 59h:20m:30s remains)
2017-12-07 15:23:18.297929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.262259 -4.2536845 -4.2447515 -4.2327108 -4.223649 -4.2223239 -4.2261024 -4.2272668 -4.2279348 -4.2263422 -4.2191067 -4.2006335 -4.1858625 -4.1823907 -4.1914454][-4.2556229 -4.2441158 -4.2282295 -4.2088852 -4.1930532 -4.1888189 -4.1971 -4.2078066 -4.2184124 -4.2263184 -4.2289429 -4.219439 -4.2069159 -4.2012486 -4.2074618][-4.2472534 -4.2322369 -4.2080822 -4.1803641 -4.1567822 -4.1492119 -4.1629543 -4.187233 -4.2126164 -4.2339864 -4.2479792 -4.2502332 -4.2449522 -4.2402363 -4.2395463][-4.2387891 -4.2256126 -4.2012558 -4.1720595 -4.1431012 -4.1304889 -4.1453662 -4.1775527 -4.2135468 -4.2455678 -4.268971 -4.2810025 -4.2826056 -4.2772522 -4.2677703][-4.2233577 -4.2151914 -4.193027 -4.1642461 -4.1310596 -4.1111031 -4.1208839 -4.1543989 -4.1984439 -4.2374206 -4.2682681 -4.2853355 -4.2910075 -4.2838178 -4.266705][-4.1861787 -4.186964 -4.170486 -4.1414113 -4.1030817 -4.07646 -4.0773764 -4.1062465 -4.1518574 -4.1971211 -4.2338672 -4.2541819 -4.2598338 -4.2545271 -4.2384677][-4.1161227 -4.1315236 -4.1297355 -4.1074815 -4.0685282 -4.0382619 -4.030282 -4.0521688 -4.0932722 -4.1415524 -4.1823063 -4.2052636 -4.2111363 -4.2115169 -4.2072496][-4.0496264 -4.0795488 -4.0936685 -4.0842929 -4.0536327 -4.0258389 -4.01383 -4.0308456 -4.0668097 -4.1132083 -4.1531343 -4.1787086 -4.190062 -4.1995296 -4.2096443][-4.0360694 -4.0721822 -4.0953274 -4.0986919 -4.0809865 -4.0600395 -4.0478311 -4.0601573 -4.0894995 -4.1294661 -4.1635065 -4.1865807 -4.200429 -4.2145991 -4.2324343][-4.0940833 -4.12157 -4.1427441 -4.1526694 -4.1466627 -4.131681 -4.1180458 -4.1232896 -4.1414704 -4.16918 -4.1933365 -4.2071624 -4.2156358 -4.2281008 -4.2453961][-4.1764541 -4.1881456 -4.1990318 -4.2100029 -4.2107949 -4.199863 -4.1843767 -4.1821513 -4.1892662 -4.2057767 -4.2190309 -4.2244339 -4.2280755 -4.2349448 -4.2431254][-4.24505 -4.2450967 -4.2470889 -4.2537851 -4.2581277 -4.2508693 -4.2374635 -4.2315516 -4.2316151 -4.2389197 -4.2426686 -4.2408581 -4.240335 -4.2401028 -4.2354674][-4.286684 -4.2843609 -4.2855654 -4.2896757 -4.2950459 -4.2930832 -4.2862992 -4.2814965 -4.2770586 -4.2742195 -4.2691064 -4.2579641 -4.2481017 -4.2409711 -4.2287607][-4.2879896 -4.2953615 -4.306078 -4.3147039 -4.3226843 -4.3256478 -4.3236361 -4.3175669 -4.3095412 -4.3007593 -4.2901583 -4.2736712 -4.2592969 -4.2497482 -4.2364464][-4.2507515 -4.2728043 -4.2979259 -4.3171892 -4.3321323 -4.3385406 -4.3383465 -4.3303566 -4.3201032 -4.3100476 -4.3005166 -4.2864914 -4.2718596 -4.2621708 -4.2512817]]...]
INFO - root - 2017-12-07 15:23:25.050481: step 23810, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 54h:05m:45s remains)
INFO - root - 2017-12-07 15:23:31.774750: step 23820, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 54h:55m:05s remains)
INFO - root - 2017-12-07 15:23:38.579504: step 23830, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 61h:14m:06s remains)
INFO - root - 2017-12-07 15:23:45.483613: step 23840, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 61h:16m:39s remains)
INFO - root - 2017-12-07 15:23:52.227085: step 23850, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 57h:02m:09s remains)
INFO - root - 2017-12-07 15:23:59.090952: step 23860, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.608 sec/batch; 52h:06m:20s remains)
INFO - root - 2017-12-07 15:24:06.024223: step 23870, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 57h:08m:16s remains)
INFO - root - 2017-12-07 15:24:12.949364: step 23880, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 63h:11m:17s remains)
INFO - root - 2017-12-07 15:24:19.638604: step 23890, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 56h:58m:32s remains)
INFO - root - 2017-12-07 15:24:26.371648: step 23900, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 54h:37m:42s remains)
2017-12-07 15:24:27.085223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2612948 -4.2471833 -4.2374234 -4.2372637 -4.242528 -4.2505007 -4.2538028 -4.2440462 -4.2352233 -4.2410145 -4.2541623 -4.2688274 -4.2841072 -4.2939057 -4.3025174][-4.22333 -4.1994734 -4.183712 -4.1855712 -4.1934824 -4.200767 -4.1968203 -4.1669884 -4.1493011 -4.169548 -4.2054682 -4.2386804 -4.2660904 -4.2828341 -4.294837][-4.2010708 -4.1705346 -4.1497126 -4.1542997 -4.1634173 -4.1621876 -4.1299276 -4.0621266 -4.0332093 -4.0770969 -4.1426182 -4.198348 -4.2425671 -4.2682166 -4.283885][-4.1888185 -4.1572123 -4.1369543 -4.1429143 -4.1527591 -4.1443172 -4.0873494 -3.9856429 -3.9480839 -4.0151372 -4.1011367 -4.1703806 -4.2271385 -4.2604337 -4.2756925][-4.1618719 -4.1274729 -4.1123495 -4.1232958 -4.1392946 -4.1354351 -4.0756512 -3.9604108 -3.9159539 -3.9990449 -4.094099 -4.16298 -4.2226706 -4.2586341 -4.27197][-4.1358967 -4.0990992 -4.0879602 -4.097568 -4.108705 -4.095295 -4.0214057 -3.8806448 -3.8240037 -3.9421175 -4.0639081 -4.1435318 -4.2117491 -4.2515087 -4.2675595][-4.13237 -4.0998454 -4.0953684 -4.0962815 -4.0853319 -4.04029 -3.9230595 -3.7329469 -3.6652133 -3.8373756 -4.0041871 -4.108427 -4.1916056 -4.2411456 -4.26194][-4.1564169 -4.1308069 -4.1341715 -4.1317153 -4.1000357 -4.032577 -3.8958368 -3.6989305 -3.643537 -3.831327 -4.00181 -4.104002 -4.1854033 -4.23335 -4.2562551][-4.1797118 -4.164237 -4.1841598 -4.1960316 -4.1668324 -4.1045556 -3.9936914 -3.8526616 -3.8184788 -3.9591484 -4.0805745 -4.1499372 -4.2110481 -4.246696 -4.2628913][-4.2014928 -4.1874647 -4.2118764 -4.2335405 -4.2113276 -4.1601982 -4.0778322 -3.9839182 -3.9593542 -4.0596614 -4.1509576 -4.2017541 -4.2465811 -4.2705269 -4.2777739][-4.2231884 -4.2090411 -4.2246304 -4.2446184 -4.2237015 -4.1752477 -4.1041489 -4.0247331 -3.994586 -4.0724397 -4.1619697 -4.2171044 -4.2586021 -4.279336 -4.2847457][-4.2486887 -4.2404671 -4.2497025 -4.2612982 -4.2394 -4.1979208 -4.1396346 -4.0694518 -4.0312848 -4.0848565 -4.1636429 -4.2155771 -4.254518 -4.2763205 -4.2858214][-4.2717519 -4.2714558 -4.2767911 -4.2832046 -4.2683311 -4.2412033 -4.1999283 -4.1444507 -4.1081796 -4.1369667 -4.1898313 -4.226943 -4.2597809 -4.2805796 -4.2928848][-4.2959566 -4.29681 -4.2962408 -4.2960114 -4.2857089 -4.2682776 -4.2388964 -4.1943464 -4.1632552 -4.1782341 -4.2120681 -4.2410693 -4.2706327 -4.2898121 -4.3020253][-4.3151708 -4.3150053 -4.311934 -4.3068175 -4.299912 -4.2877259 -4.2648811 -4.2273226 -4.1973715 -4.2024679 -4.2240758 -4.2490144 -4.2782354 -4.2964535 -4.3073158]]...]
INFO - root - 2017-12-07 15:24:33.867349: step 23910, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 60h:41m:25s remains)
INFO - root - 2017-12-07 15:24:40.679040: step 23920, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 57h:39m:19s remains)
INFO - root - 2017-12-07 15:24:47.457588: step 23930, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 57h:08m:49s remains)
INFO - root - 2017-12-07 15:24:54.297559: step 23940, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.668 sec/batch; 57h:14m:51s remains)
INFO - root - 2017-12-07 15:25:01.074342: step 23950, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 58h:55m:02s remains)
INFO - root - 2017-12-07 15:25:08.044436: step 23960, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 63h:57m:19s remains)
INFO - root - 2017-12-07 15:25:14.863327: step 23970, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 58h:00m:11s remains)
INFO - root - 2017-12-07 15:25:21.601514: step 23980, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 55h:21m:31s remains)
INFO - root - 2017-12-07 15:25:28.212453: step 23990, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 55h:52m:53s remains)
INFO - root - 2017-12-07 15:25:35.042591: step 24000, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 61h:06m:33s remains)
2017-12-07 15:25:35.770598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2167788 -4.2192025 -4.2110891 -4.2065773 -4.2104688 -4.2207432 -4.2156439 -4.1928349 -4.162488 -4.15264 -4.1701007 -4.2029767 -4.2322721 -4.2446518 -4.2224245][-4.2180219 -4.2129378 -4.2050824 -4.2049546 -4.2071357 -4.2108707 -4.1969366 -4.1731977 -4.1454511 -4.1449738 -4.1713834 -4.205976 -4.2321057 -4.239131 -4.20835][-4.2286916 -4.210815 -4.2034926 -4.2052021 -4.2046771 -4.195631 -4.1762519 -4.1525793 -4.1297345 -4.1413736 -4.1764946 -4.2115064 -4.2330141 -4.2353392 -4.2048993][-4.2146206 -4.1832204 -4.1774516 -4.1844716 -4.1806087 -4.164093 -4.1440091 -4.122046 -4.1077471 -4.1326118 -4.176446 -4.2128944 -4.2303934 -4.2314491 -4.2080965][-4.1857634 -4.1490493 -4.1541734 -4.1750784 -4.1732044 -4.1493034 -4.1197104 -4.0886354 -4.0763779 -4.112741 -4.1669559 -4.2074585 -4.2255311 -4.2298388 -4.2131662][-4.1515274 -4.1200633 -4.1408257 -4.1809959 -4.1906443 -4.16058 -4.1088805 -4.0580235 -4.05007 -4.1002197 -4.164145 -4.2053471 -4.2206206 -4.2269578 -4.213007][-4.1198134 -4.0986166 -4.1315956 -4.1842833 -4.1992087 -4.1536894 -4.0741715 -4.000957 -4.0082374 -4.0884457 -4.1676784 -4.2080936 -4.2174058 -4.2241068 -4.2142248][-4.11206 -4.1050487 -4.1475949 -4.1984015 -4.2012434 -4.1298251 -4.0186691 -3.9213052 -3.952498 -4.0671339 -4.1611676 -4.19879 -4.2026448 -4.2102766 -4.2064853][-4.1240287 -4.1372337 -4.1860509 -4.2255716 -4.21515 -4.1350493 -4.0216432 -3.9332235 -3.9680331 -4.0742993 -4.1573992 -4.1854753 -4.1860118 -4.1965117 -4.1978173][-4.1397147 -4.1635461 -4.2116241 -4.2427034 -4.2310748 -4.16593 -4.083509 -4.0274677 -4.0470815 -4.1100931 -4.1604495 -4.1787133 -4.1803513 -4.191678 -4.1942449][-4.1477036 -4.1754732 -4.2210169 -4.2502942 -4.2439952 -4.1968069 -4.1367326 -4.1009545 -4.107512 -4.1378627 -4.1623397 -4.1774368 -4.185503 -4.1983056 -4.19633][-4.1669407 -4.1891012 -4.2296185 -4.2597775 -4.2639847 -4.2302513 -4.1814184 -4.1509619 -4.1509113 -4.1654806 -4.1758642 -4.1876974 -4.1976662 -4.2067323 -4.2004051][-4.1964583 -4.2140222 -4.246676 -4.2707171 -4.2774487 -4.2537136 -4.2149267 -4.19073 -4.1899271 -4.1996036 -4.2040086 -4.2109432 -4.2158251 -4.2187529 -4.2103209][-4.2226071 -4.2373238 -4.2592568 -4.2754397 -4.2790608 -4.2643528 -4.2389026 -4.2246761 -4.2258353 -4.23426 -4.2375531 -4.2405925 -4.23852 -4.2362528 -4.2281823][-4.2441144 -4.2527957 -4.2658253 -4.2761712 -4.2747326 -4.263762 -4.2506986 -4.2470989 -4.25071 -4.2584462 -4.2621775 -4.264461 -4.2625093 -4.2602177 -4.2538419]]...]
INFO - root - 2017-12-07 15:25:42.604885: step 24010, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 53h:00m:59s remains)
INFO - root - 2017-12-07 15:25:49.444260: step 24020, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 56h:11m:38s remains)
INFO - root - 2017-12-07 15:25:56.318888: step 24030, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 60h:53m:59s remains)
INFO - root - 2017-12-07 15:26:03.104910: step 24040, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 59h:54m:58s remains)
INFO - root - 2017-12-07 15:26:09.878544: step 24050, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 58h:02m:58s remains)
INFO - root - 2017-12-07 15:26:16.706985: step 24060, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 55h:46m:19s remains)
INFO - root - 2017-12-07 15:26:23.528754: step 24070, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 54h:19m:08s remains)
INFO - root - 2017-12-07 15:26:30.286203: step 24080, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 60h:57m:27s remains)
INFO - root - 2017-12-07 15:26:36.927309: step 24090, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 61h:38m:04s remains)
INFO - root - 2017-12-07 15:26:43.700869: step 24100, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 57h:48m:06s remains)
2017-12-07 15:26:44.386387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2952814 -4.2843885 -4.2846785 -4.2901311 -4.2991323 -4.3027163 -4.2965713 -4.2871046 -4.2879896 -4.2984929 -4.3110414 -4.3216872 -4.3258057 -4.3213334 -4.3060522][-4.2994533 -4.2931471 -4.2941961 -4.2969685 -4.3008852 -4.2959318 -4.2785478 -4.2602453 -4.2586937 -4.2710295 -4.2872186 -4.30398 -4.3147516 -4.315712 -4.3054018][-4.2982869 -4.2991285 -4.3030758 -4.3053207 -4.3027849 -4.2854877 -4.2531023 -4.2230997 -4.2168479 -4.2313848 -4.2537913 -4.2803092 -4.3003812 -4.3057089 -4.2987733][-4.2852325 -4.2948327 -4.3039455 -4.3061256 -4.2965937 -4.2666278 -4.218574 -4.1758471 -4.1641417 -4.1792932 -4.2076321 -4.2471657 -4.279798 -4.2919917 -4.2903137][-4.2621975 -4.2808776 -4.2956357 -4.2990546 -4.2849522 -4.2439551 -4.1821904 -4.1296415 -4.1123815 -4.1252451 -4.1558318 -4.2077289 -4.2538261 -4.2748036 -4.2820492][-4.235796 -4.2608619 -4.279089 -4.282475 -4.266664 -4.2217507 -4.154614 -4.0987396 -4.0781574 -4.0869088 -4.1171455 -4.1742735 -4.2251711 -4.2491488 -4.2642622][-4.2149639 -4.2399969 -4.258101 -4.2630072 -4.2517509 -4.21375 -4.1531892 -4.101387 -4.0821185 -4.0885224 -4.1135745 -4.1615391 -4.2011242 -4.2201228 -4.2384357][-4.2205491 -4.2413793 -4.2564321 -4.2622833 -4.2572813 -4.2265544 -4.1743884 -4.1300106 -4.1123362 -4.1146431 -4.1311607 -4.1620197 -4.1843286 -4.1970096 -4.2165489][-4.2479181 -4.2624664 -4.2717028 -4.2733121 -4.268868 -4.2430234 -4.2004867 -4.1639876 -4.1486859 -4.1499314 -4.1609507 -4.1765213 -4.18372 -4.1917377 -4.2108397][-4.2733054 -4.2817626 -4.2842951 -4.280324 -4.2734752 -4.2532053 -4.2232714 -4.1982222 -4.1898475 -4.1955509 -4.2069306 -4.2143059 -4.2135677 -4.2167196 -4.2309179][-4.2905169 -4.2941008 -4.2913938 -4.2844343 -4.2775583 -4.2650371 -4.2478623 -4.2329426 -4.2303081 -4.237946 -4.2488985 -4.254724 -4.2540088 -4.2553658 -4.2649479][-4.2993207 -4.3011293 -4.2974181 -4.2917576 -4.2881789 -4.2837458 -4.2762976 -4.2678366 -4.2649283 -4.2679873 -4.2753386 -4.2818871 -4.2847338 -4.2874384 -4.2949424][-4.3069634 -4.3101206 -4.3075433 -4.303844 -4.3036003 -4.303618 -4.3003774 -4.2935648 -4.2878623 -4.2849703 -4.2869253 -4.29302 -4.3002424 -4.3065691 -4.3139062][-4.312171 -4.3161778 -4.3156433 -4.3141251 -4.3153162 -4.3162465 -4.312758 -4.3047533 -4.296145 -4.2900057 -4.2891731 -4.2939868 -4.3026886 -4.3122897 -4.3216953][-4.3124104 -4.3161306 -4.3173742 -4.3179107 -4.3192449 -4.31874 -4.3134489 -4.3049951 -4.2964344 -4.2901597 -4.2884955 -4.292233 -4.2997684 -4.3102164 -4.3209176]]...]
INFO - root - 2017-12-07 15:26:51.097922: step 24110, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 62h:44m:13s remains)
INFO - root - 2017-12-07 15:26:57.986438: step 24120, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 57h:29m:03s remains)
INFO - root - 2017-12-07 15:27:04.814703: step 24130, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 53h:10m:11s remains)
INFO - root - 2017-12-07 15:27:11.721123: step 24140, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 55h:51m:45s remains)
INFO - root - 2017-12-07 15:27:18.612076: step 24150, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.743 sec/batch; 63h:40m:57s remains)
INFO - root - 2017-12-07 15:27:25.480294: step 24160, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 57h:03m:30s remains)
INFO - root - 2017-12-07 15:27:32.277330: step 24170, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 55h:42m:05s remains)
INFO - root - 2017-12-07 15:27:39.062965: step 24180, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 58h:43m:46s remains)
INFO - root - 2017-12-07 15:27:45.714351: step 24190, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 60h:29m:45s remains)
INFO - root - 2017-12-07 15:27:52.484379: step 24200, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 61h:56m:47s remains)
2017-12-07 15:27:53.222111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1806955 -4.1866555 -4.1968136 -4.2118578 -4.2199574 -4.2188721 -4.2057004 -4.1961951 -4.1999831 -4.2098875 -4.2150373 -4.2272997 -4.238811 -4.2435689 -4.2399769][-4.1773434 -4.1873846 -4.1991372 -4.2141175 -4.2237539 -4.2201591 -4.2096834 -4.2054024 -4.2129645 -4.2244277 -4.2278614 -4.235785 -4.2424259 -4.2444072 -4.2365212][-4.1794243 -4.19304 -4.207695 -4.2221336 -4.2309813 -4.2288079 -4.2248106 -4.2288427 -4.237999 -4.2469897 -4.2496748 -4.2554774 -4.2560878 -4.2539139 -4.2434921][-4.1745706 -4.1886706 -4.2029157 -4.2136207 -4.2209582 -4.2262635 -4.2329874 -4.24505 -4.2527156 -4.2568021 -4.2595019 -4.2639112 -4.2623024 -4.2590609 -4.2496157][-4.1640453 -4.1789246 -4.1897621 -4.1940784 -4.19981 -4.2092004 -4.2255697 -4.2440915 -4.2486939 -4.2464972 -4.2442045 -4.2493238 -4.2513185 -4.2492342 -4.2373939][-4.1426997 -4.1564555 -4.1599112 -4.1576381 -4.15984 -4.1667795 -4.182579 -4.202673 -4.2082171 -4.2054181 -4.2058806 -4.2174034 -4.226522 -4.22287 -4.20515][-4.112432 -4.130053 -4.1281 -4.118845 -4.1078525 -4.0964379 -4.0931225 -4.1067314 -4.1232462 -4.134892 -4.1487741 -4.1720262 -4.1897783 -4.186748 -4.1627727][-4.083776 -4.10984 -4.1086836 -4.0911355 -4.0618372 -4.01909 -3.9856586 -3.990546 -4.026701 -4.0665283 -4.1020608 -4.132463 -4.1542611 -4.151794 -4.1222272][-4.0810752 -4.1100693 -4.1104755 -4.0862026 -4.041585 -3.9768035 -3.9214988 -3.9162292 -3.9608667 -4.0191488 -4.0689511 -4.1032171 -4.1297212 -4.1318893 -4.1103034][-4.1145029 -4.1330729 -4.1304865 -4.103847 -4.0609202 -4.0066271 -3.9563744 -3.9410551 -3.9715753 -4.0239487 -4.0729833 -4.1069016 -4.1330376 -4.1403956 -4.1335239][-4.174015 -4.1771455 -4.1680326 -4.1437526 -4.112946 -4.0802159 -4.047327 -4.0330729 -4.0477514 -4.0800633 -4.1155982 -4.144259 -4.1685324 -4.1800942 -4.1838207][-4.2413263 -4.234467 -4.2222571 -4.2019095 -4.1824756 -4.1664505 -4.1490717 -4.1405716 -4.1470108 -4.163578 -4.18508 -4.2057462 -4.2248883 -4.2378592 -4.2447662][-4.297483 -4.2893133 -4.2793713 -4.2662625 -4.255702 -4.2475281 -4.2381592 -4.2327943 -4.2370844 -4.2448859 -4.2552376 -4.2662983 -4.2781739 -4.2888408 -4.2947783][-4.3277168 -4.3234687 -4.3190885 -4.3137903 -4.3105421 -4.3080111 -4.3036942 -4.2992535 -4.3001409 -4.3016829 -4.3052926 -4.3106179 -4.3169413 -4.324183 -4.3278508][-4.3325009 -4.3324456 -4.3332176 -4.3339663 -4.3353095 -4.3364444 -4.3350859 -4.3318958 -4.3301096 -4.3282533 -4.32759 -4.3285427 -4.3306594 -4.3336158 -4.3348656]]...]
INFO - root - 2017-12-07 15:28:00.008335: step 24210, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 55h:12m:10s remains)
INFO - root - 2017-12-07 15:28:06.853365: step 24220, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 61h:25m:27s remains)
INFO - root - 2017-12-07 15:28:13.591584: step 24230, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 57h:55m:44s remains)
INFO - root - 2017-12-07 15:28:20.406288: step 24240, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 56h:36m:48s remains)
INFO - root - 2017-12-07 15:28:27.203443: step 24250, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.613 sec/batch; 52h:30m:29s remains)
INFO - root - 2017-12-07 15:28:33.958487: step 24260, loss = 2.09, batch loss = 2.04 (12.6 examples/sec; 0.633 sec/batch; 54h:12m:05s remains)
INFO - root - 2017-12-07 15:28:40.874423: step 24270, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 60h:59m:51s remains)
INFO - root - 2017-12-07 15:28:47.698735: step 24280, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.757 sec/batch; 64h:47m:44s remains)
INFO - root - 2017-12-07 15:28:54.203736: step 24290, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 57h:01m:14s remains)
INFO - root - 2017-12-07 15:29:00.877928: step 24300, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 53h:25m:24s remains)
2017-12-07 15:29:01.649227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3158107 -4.3208189 -4.3269205 -4.3262234 -4.3066645 -4.2711053 -4.230845 -4.2021465 -4.1949778 -4.2114892 -4.2388906 -4.2675004 -4.294569 -4.3130279 -4.319622][-4.327908 -4.34291 -4.3559937 -4.3533206 -4.3240623 -4.2716036 -4.2102246 -4.1632638 -4.1496372 -4.1701131 -4.2063961 -4.2450581 -4.2857671 -4.3138208 -4.3227091][-4.3304 -4.3557906 -4.3724236 -4.3655519 -4.3262091 -4.2595735 -4.1773677 -4.1083269 -4.0912309 -4.1173077 -4.1637731 -4.217145 -4.2732477 -4.3101821 -4.3212538][-4.3211851 -4.3556852 -4.3753757 -4.3651991 -4.3165073 -4.2361121 -4.1307745 -4.0445085 -4.0328293 -4.0716653 -4.1323776 -4.1999078 -4.2648163 -4.3065753 -4.3188224][-4.3034511 -4.34437 -4.3637424 -4.3485384 -4.291481 -4.193913 -4.0655451 -3.9688926 -3.9671025 -4.0255914 -4.1037831 -4.1859856 -4.2542105 -4.2990918 -4.3135715][-4.2857013 -4.3274312 -4.3427463 -4.3185115 -4.250998 -4.1384158 -3.9939747 -3.8906133 -3.8916292 -3.9657645 -4.0571175 -4.1538148 -4.2319031 -4.2846775 -4.3059721][-4.2773614 -4.3133726 -4.3210354 -4.2860794 -4.209692 -4.0884619 -3.9362147 -3.8357816 -3.8445344 -3.923223 -4.0160036 -4.1226435 -4.2110829 -4.273242 -4.3015032][-4.2707624 -4.2982087 -4.2997289 -4.2558074 -4.180635 -4.065444 -3.9252391 -3.84238 -3.8604569 -3.9365273 -4.0205007 -4.1207004 -4.2067852 -4.269784 -4.3013411][-4.2520976 -4.2687221 -4.2631083 -4.2138805 -4.1457877 -4.0475721 -3.9229443 -3.8494713 -3.8720908 -3.9555609 -4.0395427 -4.13134 -4.2124419 -4.2720032 -4.3030014][-4.2337255 -4.2307487 -4.216794 -4.169713 -4.1101995 -4.0212708 -3.9031596 -3.8279011 -3.8510547 -3.9519792 -4.0498152 -4.1406684 -4.2200327 -4.276504 -4.3051014][-4.2347879 -4.2162738 -4.1972847 -4.1609411 -4.1100988 -4.0312576 -3.9251208 -3.8536267 -3.8721521 -3.9656925 -4.0599852 -4.1469169 -4.2248425 -4.2793946 -4.3070879][-4.2405329 -4.2130184 -4.193099 -4.1734171 -4.1380606 -4.0800157 -3.9969058 -3.9383655 -3.9464512 -4.0091338 -4.0791693 -4.1544361 -4.2279215 -4.281106 -4.3092][-4.2294464 -4.2010083 -4.1842875 -4.1800594 -4.1649122 -4.1291585 -4.0673394 -4.021914 -4.0309105 -4.0765157 -4.1271362 -4.1844573 -4.2425117 -4.2860842 -4.3103771][-4.2084279 -4.179296 -4.1595445 -4.1589227 -4.1581469 -4.1393213 -4.0922551 -4.0621409 -4.0837226 -4.135201 -4.1800437 -4.2245011 -4.2667294 -4.2978387 -4.3148217][-4.17505 -4.149663 -4.1284051 -4.1231513 -4.1272912 -4.1185589 -4.0856233 -4.0747023 -4.1122665 -4.1706886 -4.2153068 -4.2548146 -4.2883186 -4.3101544 -4.3192277]]...]
INFO - root - 2017-12-07 15:29:08.476631: step 24310, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.734 sec/batch; 62h:49m:41s remains)
INFO - root - 2017-12-07 15:29:15.248028: step 24320, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 57h:35m:52s remains)
INFO - root - 2017-12-07 15:29:22.020311: step 24330, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 55h:37m:55s remains)
INFO - root - 2017-12-07 15:29:28.773253: step 24340, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 58h:18m:11s remains)
INFO - root - 2017-12-07 15:29:35.572911: step 24350, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 60h:10m:14s remains)
INFO - root - 2017-12-07 15:29:42.314442: step 24360, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 57h:20m:55s remains)
INFO - root - 2017-12-07 15:29:49.185470: step 24370, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 54h:33m:23s remains)
INFO - root - 2017-12-07 15:29:55.971794: step 24380, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 55h:18m:37s remains)
INFO - root - 2017-12-07 15:30:02.702971: step 24390, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 56h:42m:31s remains)
INFO - root - 2017-12-07 15:30:09.548168: step 24400, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 61h:18m:13s remains)
2017-12-07 15:30:10.253093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2483716 -4.2525778 -4.2696505 -4.2897062 -4.3035107 -4.3106365 -4.3078122 -4.303514 -4.3081937 -4.3118916 -4.3150806 -4.3119645 -4.2979603 -4.2787018 -4.2508426][-4.2275286 -4.2317009 -4.2493963 -4.2704797 -4.2869859 -4.2949567 -4.2892938 -4.2840867 -4.2909269 -4.2960892 -4.2983475 -4.2908125 -4.2713103 -4.2458696 -4.2112832][-4.2355943 -4.2436604 -4.2616963 -4.2790542 -4.2902479 -4.2912493 -4.2769804 -4.26717 -4.2720284 -4.27445 -4.2744594 -4.2633524 -4.2396092 -4.211071 -4.178061][-4.2646146 -4.2734494 -4.2849607 -4.2879753 -4.2840977 -4.2740979 -4.2552032 -4.2409973 -4.241591 -4.2380075 -4.2363563 -4.2279496 -4.2071528 -4.1828218 -4.1600094][-4.2899752 -4.2932196 -4.2925124 -4.2770438 -4.253262 -4.2264142 -4.2011042 -4.1878467 -4.1900635 -4.1855688 -4.1905413 -4.1959858 -4.1896629 -4.1790071 -4.1692991][-4.2808914 -4.2737389 -4.2566533 -4.2201347 -4.170959 -4.1220236 -4.0837927 -4.0742822 -4.0930033 -4.1050782 -4.1308432 -4.1583877 -4.1726565 -4.1801848 -4.1813836][-4.2474341 -4.2273364 -4.1951733 -4.1371427 -4.0594077 -3.97675 -3.9126165 -3.9085302 -3.9589179 -4.0082512 -4.0658703 -4.1156673 -4.1464624 -4.1651807 -4.1728644][-4.2169166 -4.1933823 -4.1616197 -4.1044421 -4.0200949 -3.922271 -3.844053 -3.84365 -3.9135568 -3.9868724 -4.0568066 -4.1076303 -4.1333971 -4.1452675 -4.1511889][-4.1891923 -4.1789055 -4.170619 -4.1424885 -4.0917454 -4.0313468 -3.9851675 -3.9836242 -4.0306053 -4.0839481 -4.1273003 -4.1486564 -4.1479058 -4.1356478 -4.1265993][-4.1432409 -4.1456757 -4.1690369 -4.1813803 -4.1729422 -4.1535258 -4.1355691 -4.1341949 -4.1581097 -4.1805744 -4.1912718 -4.180841 -4.156642 -4.1261187 -4.1010585][-4.1026187 -4.1145182 -4.1574225 -4.1986165 -4.2204676 -4.2249055 -4.2179885 -4.2142339 -4.2230539 -4.2238879 -4.2153625 -4.1859088 -4.145052 -4.1063843 -4.0761538][-4.1073079 -4.1235642 -4.1696858 -4.2194805 -4.2544456 -4.2701178 -4.26675 -4.2595487 -4.2572436 -4.2446594 -4.2267594 -4.191596 -4.146502 -4.108067 -4.0833044][-4.177268 -4.1930213 -4.2297692 -4.2692132 -4.2980671 -4.3081193 -4.2996836 -4.2900119 -4.2839618 -4.2701864 -4.2524004 -4.2213287 -4.1850591 -4.1572952 -4.1469846][-4.2776222 -4.2882347 -4.3122969 -4.3360605 -4.3502021 -4.3491163 -4.3365965 -4.3268676 -4.3231287 -4.3157678 -4.3039441 -4.2828994 -4.2594409 -4.2442536 -4.2462564][-4.3498626 -4.3549929 -4.36691 -4.3778329 -4.3822279 -4.3768096 -4.364944 -4.3572035 -4.3564916 -4.3547 -4.3492465 -4.3386488 -4.3267236 -4.3207831 -4.3266754]]...]
INFO - root - 2017-12-07 15:30:16.950869: step 24410, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 53h:12m:04s remains)
INFO - root - 2017-12-07 15:30:23.545583: step 24420, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 60h:44m:21s remains)
INFO - root - 2017-12-07 15:30:30.310047: step 24430, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 61h:16m:41s remains)
INFO - root - 2017-12-07 15:30:37.067084: step 24440, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 58h:32m:16s remains)
INFO - root - 2017-12-07 15:30:43.865334: step 24450, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.616 sec/batch; 52h:43m:58s remains)
INFO - root - 2017-12-07 15:30:50.581674: step 24460, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 53h:25m:13s remains)
INFO - root - 2017-12-07 15:30:57.432424: step 24470, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 62h:38m:56s remains)
INFO - root - 2017-12-07 15:31:04.227151: step 24480, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 61h:21m:06s remains)
INFO - root - 2017-12-07 15:31:10.829845: step 24490, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 59h:48m:28s remains)
INFO - root - 2017-12-07 15:31:17.618816: step 24500, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 55h:01m:06s remains)
2017-12-07 15:31:18.303771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2798691 -4.2818141 -4.2942944 -4.305644 -4.3069334 -4.2990527 -4.2943764 -4.3008604 -4.3121767 -4.318603 -4.316504 -4.3084846 -4.298615 -4.2930779 -4.2875447][-4.2561774 -4.2628703 -4.2791581 -4.2932491 -4.2990055 -4.2953119 -4.2915473 -4.2979064 -4.3107014 -4.3202205 -4.3177156 -4.3023596 -4.2833347 -4.2714577 -4.2605686][-4.219512 -4.2334132 -4.258595 -4.2804494 -4.2885213 -4.284524 -4.2785511 -4.2862806 -4.3036418 -4.317328 -4.3175311 -4.3000512 -4.2763371 -4.2589083 -4.243289][-4.1892586 -4.211669 -4.2467585 -4.2711945 -4.2734704 -4.2610536 -4.2447219 -4.250236 -4.27581 -4.2985115 -4.3068075 -4.2969723 -4.2775526 -4.2628427 -4.2450075][-4.1788697 -4.2059507 -4.24592 -4.2680416 -4.257659 -4.2251244 -4.1841779 -4.1826468 -4.2221251 -4.2615156 -4.2820506 -4.2877932 -4.2811856 -4.2719336 -4.2560587][-4.170033 -4.1971612 -4.2369142 -4.2547784 -4.2311792 -4.173089 -4.0953 -4.0746946 -4.1344976 -4.2011285 -4.2419877 -4.2659855 -4.2767124 -4.2755742 -4.2668953][-4.1695709 -4.1969242 -4.2312603 -4.2404552 -4.2050424 -4.1231556 -4.0137529 -3.9677172 -4.0394392 -4.1315932 -4.1945448 -4.23633 -4.2634153 -4.2756019 -4.2778325][-4.1675363 -4.1995177 -4.2280817 -4.2266111 -4.1890125 -4.1103106 -4.0082135 -3.9553821 -4.0133114 -4.0995288 -4.1665354 -4.2161679 -4.255013 -4.2784553 -4.2900281][-4.17587 -4.210753 -4.2384138 -4.2329149 -4.198297 -4.1396046 -4.072413 -4.039186 -4.0722752 -4.1263156 -4.1749115 -4.2170243 -4.2552013 -4.2817063 -4.2999725][-4.2008681 -4.2322068 -4.2549744 -4.2534022 -4.2304144 -4.1917396 -4.1584067 -4.1467109 -4.1641989 -4.190629 -4.2181692 -4.2414026 -4.2634721 -4.2812634 -4.301055][-4.2377024 -4.2600036 -4.2774224 -4.2806835 -4.2682161 -4.2453051 -4.2330437 -4.2367435 -4.2492976 -4.2575812 -4.2645311 -4.2682233 -4.2694559 -4.27301 -4.2900438][-4.2767153 -4.2867427 -4.2975636 -4.2984695 -4.2891636 -4.2782793 -4.2827172 -4.2947407 -4.3031635 -4.2996292 -4.2879505 -4.2785234 -4.2697325 -4.2649441 -4.2772236][-4.3092842 -4.3100877 -4.311533 -4.3018389 -4.2911654 -4.2921758 -4.3064332 -4.3166556 -4.3193913 -4.3112416 -4.2936268 -4.2798443 -4.2709541 -4.2658286 -4.2740779][-4.3263679 -4.3204112 -4.3142619 -4.3013425 -4.2887559 -4.2945061 -4.3113942 -4.3179822 -4.3174367 -4.3092461 -4.2939157 -4.2783365 -4.2678366 -4.2653022 -4.27388][-4.3297119 -4.31811 -4.3086581 -4.2988267 -4.2878804 -4.29443 -4.309021 -4.3129416 -4.3087606 -4.3004394 -4.2883182 -4.2719369 -4.2597952 -4.2610979 -4.2733092]]...]
INFO - root - 2017-12-07 15:31:25.154251: step 24510, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 62h:52m:35s remains)
INFO - root - 2017-12-07 15:31:31.869031: step 24520, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 57h:39m:00s remains)
INFO - root - 2017-12-07 15:31:38.589194: step 24530, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 57h:27m:25s remains)
INFO - root - 2017-12-07 15:31:45.435584: step 24540, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 55h:19m:26s remains)
INFO - root - 2017-12-07 15:31:52.164600: step 24550, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 61h:09m:03s remains)
INFO - root - 2017-12-07 15:31:58.856984: step 24560, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 61h:05m:38s remains)
INFO - root - 2017-12-07 15:32:05.649465: step 24570, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 58h:59m:06s remains)
INFO - root - 2017-12-07 15:32:12.501392: step 24580, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 53h:13m:33s remains)
INFO - root - 2017-12-07 15:32:19.101026: step 24590, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 55h:40m:14s remains)
INFO - root - 2017-12-07 15:32:25.957687: step 24600, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 61h:38m:05s remains)
2017-12-07 15:32:26.665516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3169632 -4.3097296 -4.30237 -4.2950954 -4.2911897 -4.292057 -4.2961245 -4.3012476 -4.3074346 -4.3078489 -4.302073 -4.2967052 -4.2931776 -4.2919559 -4.294919][-4.3082366 -4.2949314 -4.2820263 -4.2702723 -4.2663984 -4.271698 -4.2782187 -4.2822309 -4.2926455 -4.2993865 -4.2973762 -4.2926345 -4.284523 -4.2736197 -4.2685714][-4.2969441 -4.2757325 -4.2529483 -4.2297454 -4.2182107 -4.2223077 -4.2282372 -4.2295537 -4.2426996 -4.2613325 -4.2728324 -4.27726 -4.27036 -4.2524557 -4.2351618][-4.28588 -4.2539687 -4.2162495 -4.175806 -4.1544814 -4.153923 -4.1545639 -4.1546474 -4.1734533 -4.2047982 -4.2296519 -4.2427874 -4.2413063 -4.2231 -4.1994848][-4.2785487 -4.2395215 -4.1873388 -4.1302624 -4.0997171 -4.091167 -4.07916 -4.0677471 -4.0920782 -4.1368761 -4.176568 -4.1956635 -4.20053 -4.1885309 -4.1674972][-4.2730117 -4.2316217 -4.1700292 -4.1004429 -4.063458 -4.0418124 -4.0031581 -3.9645634 -3.9867897 -4.047626 -4.1052408 -4.1322579 -4.1463332 -4.1491175 -4.1427574][-4.268158 -4.2279353 -4.1647711 -4.0871367 -4.0379181 -3.9928641 -3.9161367 -3.8407798 -3.8661327 -3.9552197 -4.0316458 -4.0642347 -4.0858922 -4.1091137 -4.1232767][-4.2633691 -4.2261043 -4.1680503 -4.090919 -4.0286593 -3.9580631 -3.8501458 -3.7557418 -3.7902803 -3.902571 -3.9834926 -4.0035028 -4.0129771 -4.044445 -4.0779691][-4.2587729 -4.2275519 -4.18184 -4.1160979 -4.056787 -3.9835818 -3.8849459 -3.8092291 -3.8445687 -3.9395363 -3.9896512 -3.9694204 -3.9480813 -3.967567 -4.0060749][-4.2581606 -4.2337074 -4.2012844 -4.1566815 -4.1148672 -4.0615034 -3.9987125 -3.9531708 -3.9693882 -4.01872 -4.0294681 -3.979866 -3.9325056 -3.9251606 -3.9486549][-4.2597671 -4.2385082 -4.2176418 -4.1926088 -4.169508 -4.1422014 -4.1084933 -4.078619 -4.0724006 -4.0879149 -4.0823126 -4.036305 -3.9951043 -3.9732289 -3.9707732][-4.2612367 -4.237927 -4.2210875 -4.2086158 -4.2026896 -4.1975641 -4.184094 -4.1641622 -4.1466522 -4.1460896 -4.1409211 -4.1171851 -4.1023355 -4.085 -4.0643096][-4.26538 -4.2399654 -4.2222633 -4.2145286 -4.2205758 -4.2322612 -4.2349596 -4.2229648 -4.2057014 -4.1988196 -4.193718 -4.1855745 -4.188128 -4.1762967 -4.1523433][-4.272615 -4.2456942 -4.22545 -4.2206464 -4.2334929 -4.2558613 -4.2679534 -4.26009 -4.2445087 -4.2377462 -4.23629 -4.2372756 -4.2449975 -4.2377243 -4.2213416][-4.2822738 -4.2553792 -4.2336164 -4.2299452 -4.2448459 -4.2698078 -4.2837553 -4.27842 -4.2677112 -4.26311 -4.26213 -4.2662234 -4.2760105 -4.2755284 -4.2689395]]...]
INFO - root - 2017-12-07 15:32:33.433690: step 24610, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 54h:15m:24s remains)
INFO - root - 2017-12-07 15:32:40.383944: step 24620, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 56h:02m:46s remains)
INFO - root - 2017-12-07 15:32:47.194393: step 24630, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 60h:15m:15s remains)
INFO - root - 2017-12-07 15:32:53.912845: step 24640, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 61h:04m:11s remains)
INFO - root - 2017-12-07 15:33:00.716893: step 24650, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 58h:13m:39s remains)
INFO - root - 2017-12-07 15:33:07.449156: step 24660, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 55h:31m:35s remains)
INFO - root - 2017-12-07 15:33:14.123097: step 24670, loss = 2.03, batch loss = 1.97 (12.8 examples/sec; 0.623 sec/batch; 53h:14m:43s remains)
INFO - root - 2017-12-07 15:33:20.879631: step 24680, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 60h:09m:41s remains)
INFO - root - 2017-12-07 15:33:27.630345: step 24690, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.740 sec/batch; 63h:14m:31s remains)
INFO - root - 2017-12-07 15:33:34.429291: step 24700, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 56h:20m:39s remains)
2017-12-07 15:33:35.118029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2950239 -4.2967191 -4.2991939 -4.3013997 -4.3024492 -4.3008471 -4.297678 -4.293468 -4.2887621 -4.2851481 -4.2837467 -4.284317 -4.28752 -4.2918916 -4.2955837][-4.3113275 -4.31186 -4.3135371 -4.3153567 -4.3166318 -4.3166628 -4.3154745 -4.3132396 -4.3097019 -4.3064451 -4.3041525 -4.3035803 -4.3067408 -4.311892 -4.315886][-4.3224611 -4.3207769 -4.3197675 -4.319252 -4.3191805 -4.3191047 -4.3171439 -4.31478 -4.3112135 -4.3076582 -4.3043861 -4.3016691 -4.3034873 -4.3087544 -4.3125081][-4.323699 -4.3172159 -4.3110056 -4.3057432 -4.3010192 -4.2959867 -4.2864161 -4.2783518 -4.2726326 -4.2689676 -4.2662992 -4.2657685 -4.2702146 -4.275938 -4.2807827][-4.3138328 -4.29976 -4.2844105 -4.2696447 -4.2570667 -4.2455168 -4.225956 -4.208241 -4.1993465 -4.2002687 -4.2040472 -4.2120891 -4.2259345 -4.2367158 -4.2427611][-4.2909174 -4.2665129 -4.2375183 -4.2091618 -4.18684 -4.1693683 -4.1419077 -4.1184316 -4.1122818 -4.1209688 -4.1340017 -4.15605 -4.1818895 -4.1942439 -4.1987767][-4.2576909 -4.2190237 -4.1716542 -4.1257935 -4.0933886 -4.0709438 -4.0400081 -4.0166845 -4.0191946 -4.039772 -4.0627651 -4.1007805 -4.1400294 -4.1550126 -4.1562014][-4.2266588 -4.1715879 -4.1040215 -4.0403919 -3.9970381 -3.9670703 -3.9312854 -3.9092231 -3.9266582 -3.9661083 -4.0028787 -4.0547814 -4.1021352 -4.1191216 -4.1160617][-4.2107053 -4.1443491 -4.0671611 -3.9961677 -3.9492185 -3.9148927 -3.8784704 -3.8634615 -3.8979149 -3.9542398 -4.0036349 -4.0631 -4.1095448 -4.1260319 -4.1226234][-4.226778 -4.1648912 -4.0988841 -4.0447459 -4.0136547 -3.9918189 -3.9724064 -3.967669 -3.9974113 -4.0424681 -4.083497 -4.1319494 -4.1671362 -4.1814466 -4.1836615][-4.2654095 -4.2212553 -4.1778159 -4.1451902 -4.127728 -4.1162872 -4.1087618 -4.1083279 -4.1269164 -4.1537843 -4.1819019 -4.2169003 -4.2373352 -4.2429223 -4.2443647][-4.3028679 -4.2777915 -4.2541585 -4.2375569 -4.2300243 -4.2255831 -4.2224541 -4.2227235 -4.2318659 -4.2445178 -4.2595716 -4.2779903 -4.28844 -4.2912216 -4.2911282][-4.3272085 -4.3143554 -4.3031378 -4.295867 -4.2927685 -4.2906084 -4.2876649 -4.2868948 -4.2915378 -4.2983747 -4.3071785 -4.3179522 -4.3255858 -4.3295259 -4.3291388][-4.3479233 -4.3426003 -4.3381314 -4.3341479 -4.3323989 -4.3318677 -4.3305616 -4.3292274 -4.330389 -4.3328414 -4.3359008 -4.3400555 -4.3427639 -4.3433022 -4.340663][-4.3548775 -4.3497925 -4.3451056 -4.3400044 -4.336555 -4.3356371 -4.3354788 -4.3360372 -4.3381562 -4.3400078 -4.3403139 -4.339901 -4.3390913 -4.3379407 -4.3355904]]...]
INFO - root - 2017-12-07 15:33:41.805948: step 24710, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 57h:20m:23s remains)
INFO - root - 2017-12-07 15:33:48.608583: step 24720, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 61h:53m:33s remains)
INFO - root - 2017-12-07 15:33:55.198673: step 24730, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.661 sec/batch; 56h:29m:27s remains)
INFO - root - 2017-12-07 15:34:02.013398: step 24740, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 55h:10m:29s remains)
INFO - root - 2017-12-07 15:34:08.825027: step 24750, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 61h:40m:42s remains)
INFO - root - 2017-12-07 15:34:15.806055: step 24760, loss = 2.10, batch loss = 2.04 (10.8 examples/sec; 0.740 sec/batch; 63h:15m:11s remains)
INFO - root - 2017-12-07 15:34:22.622859: step 24770, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 56h:22m:42s remains)
INFO - root - 2017-12-07 15:34:29.502347: step 24780, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 54h:27m:31s remains)
INFO - root - 2017-12-07 15:34:36.039716: step 24790, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 55h:55m:47s remains)
INFO - root - 2017-12-07 15:34:42.849188: step 24800, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 63h:28m:22s remains)
2017-12-07 15:34:43.535760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2733216 -4.2646837 -4.2541933 -4.2524428 -4.2560153 -4.2611933 -4.2623124 -4.2612519 -4.2647328 -4.2619772 -4.2531424 -4.2483754 -4.2458596 -4.2454848 -4.247931][-4.2546897 -4.2426472 -4.2285438 -4.2256742 -4.2306805 -4.2357559 -4.2357583 -4.2354932 -4.2426028 -4.2450609 -4.2412934 -4.2368965 -4.2293038 -4.2253957 -4.2298217][-4.2421412 -4.230957 -4.2176156 -4.2160325 -4.2192287 -4.2187142 -4.2130747 -4.2098761 -4.2157755 -4.2216697 -4.2239428 -4.2234044 -4.2155838 -4.2101274 -4.2132869][-4.2399139 -4.2286034 -4.2151027 -4.2089334 -4.2068148 -4.201951 -4.1939225 -4.1874523 -4.1884108 -4.1950989 -4.2025042 -4.2063 -4.2025671 -4.1987934 -4.2007403][-4.2283773 -4.2153959 -4.2019053 -4.189518 -4.1787539 -4.1661787 -4.1567135 -4.1544809 -4.1568689 -4.162693 -4.1718426 -4.1761737 -4.1750402 -4.1742616 -4.1802707][-4.2124581 -4.1954713 -4.180923 -4.163188 -4.1408172 -4.1151562 -4.1026893 -4.1072049 -4.1146126 -4.126008 -4.1379223 -4.1367221 -4.131547 -4.1315718 -4.1453667][-4.2076359 -4.190764 -4.174685 -4.1522264 -4.1181955 -4.0807486 -4.0645304 -4.0732846 -4.0838289 -4.0989218 -4.107131 -4.0980778 -4.0864811 -4.0836329 -4.0995135][-4.2088175 -4.1971736 -4.1808057 -4.1552596 -4.1192613 -4.0790758 -4.0579853 -4.0641823 -4.078011 -4.0924973 -4.0937562 -4.0812068 -4.0689383 -4.0631409 -4.0807037][-4.2180734 -4.2149253 -4.1982703 -4.16933 -4.1354327 -4.1008983 -4.0768771 -4.0769086 -4.0902071 -4.1024461 -4.1084747 -4.1026354 -4.0928469 -4.0829082 -4.0930676][-4.2264495 -4.2302475 -4.2161312 -4.1851325 -4.1565437 -4.1319532 -4.111763 -4.1081843 -4.1149139 -4.1239047 -4.1386237 -4.1404877 -4.1284294 -4.1146512 -4.11818][-4.2172866 -4.2254171 -4.2173152 -4.1908169 -4.1680202 -4.152298 -4.1408019 -4.1376705 -4.1360159 -4.1414762 -4.1638989 -4.1741953 -4.1614137 -4.1478381 -4.1495671][-4.1805892 -4.192883 -4.1926165 -4.1752281 -4.1562452 -4.1502175 -4.1512871 -4.1530266 -4.1449318 -4.1423807 -4.1628327 -4.1818933 -4.1805415 -4.1770778 -4.1823959][-4.1541414 -4.165576 -4.17099 -4.1602736 -4.1456733 -4.1476278 -4.1572628 -4.1623635 -4.1501412 -4.1371689 -4.1460571 -4.1717916 -4.1927514 -4.2066774 -4.2167859][-4.1626291 -4.1728048 -4.1756816 -4.1600981 -4.1405382 -4.1452985 -4.1616826 -4.1745806 -4.1661892 -4.1439915 -4.1380987 -4.1604986 -4.1938391 -4.2222295 -4.2407374][-4.1801128 -4.1900826 -4.1906943 -4.1731019 -4.1503563 -4.1523552 -4.1682725 -4.1897106 -4.1896572 -4.1636658 -4.1435294 -4.1530609 -4.182797 -4.2118721 -4.2382894]]...]
INFO - root - 2017-12-07 15:34:50.256345: step 24810, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.610 sec/batch; 52h:10m:41s remains)
INFO - root - 2017-12-07 15:34:57.056546: step 24820, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 54h:46m:09s remains)
INFO - root - 2017-12-07 15:35:03.901384: step 24830, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 62h:56m:51s remains)
INFO - root - 2017-12-07 15:35:10.747376: step 24840, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 61h:47m:22s remains)
INFO - root - 2017-12-07 15:35:17.546856: step 24850, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 56h:24m:47s remains)
INFO - root - 2017-12-07 15:35:24.301406: step 24860, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 55h:57m:55s remains)
INFO - root - 2017-12-07 15:35:31.199074: step 24870, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 60h:27m:55s remains)
INFO - root - 2017-12-07 15:35:38.110529: step 24880, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 62h:35m:22s remains)
INFO - root - 2017-12-07 15:35:44.794769: step 24890, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 57h:39m:55s remains)
INFO - root - 2017-12-07 15:35:51.544638: step 24900, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 56h:36m:37s remains)
2017-12-07 15:35:52.379450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3002834 -4.30219 -4.3001771 -4.3004618 -4.2963834 -4.2878389 -4.2816925 -4.2749352 -4.2584324 -4.2455058 -4.238977 -4.2360229 -4.2339807 -4.2247868 -4.2175174][-4.3039331 -4.3080921 -4.3072352 -4.3091092 -4.3043313 -4.2970386 -4.2930226 -4.285388 -4.2682095 -4.2563415 -4.254879 -4.2565937 -4.2531834 -4.2408838 -4.232729][-4.3061037 -4.310133 -4.3076062 -4.308547 -4.3032832 -4.2964487 -4.2931128 -4.2845263 -4.2687559 -4.2650957 -4.2744594 -4.2846794 -4.28245 -4.268199 -4.2552114][-4.2975392 -4.2966471 -4.2897277 -4.2879667 -4.2836251 -4.2769446 -4.2704563 -4.2574039 -4.2427158 -4.2511849 -4.2756186 -4.2963438 -4.298192 -4.2861032 -4.2721076][-4.2718897 -4.2648344 -4.2544823 -4.2475624 -4.2391343 -4.2288036 -4.2134132 -4.1867003 -4.169 -4.1917958 -4.2336459 -4.2687917 -4.2786388 -4.2735586 -4.267345][-4.2323108 -4.22503 -4.2152529 -4.2027173 -4.1857929 -4.1610641 -4.1230016 -4.065136 -4.039031 -4.0871878 -4.1531568 -4.2031951 -4.2202096 -4.2229161 -4.2252145][-4.1902657 -4.1884904 -4.1847138 -4.1695008 -4.1434035 -4.1012225 -4.0314593 -3.9289625 -3.8904748 -3.9694905 -4.0617571 -4.1245122 -4.1539879 -4.166544 -4.1737967][-4.167017 -4.1744804 -4.1789026 -4.1670008 -4.137538 -4.0898743 -4.0112333 -3.9016466 -3.8662212 -3.9445293 -4.0301404 -4.0860748 -4.1171713 -4.1327586 -4.140923][-4.1788626 -4.1916246 -4.1997962 -4.1922693 -4.1682296 -4.1294861 -4.0683374 -3.996717 -3.9829488 -4.0351415 -4.0861855 -4.1182771 -4.1392307 -4.1493754 -4.1544766][-4.2138958 -4.2283888 -4.2356563 -4.2310123 -4.2157111 -4.1913543 -4.1536455 -4.1177292 -4.1198654 -4.1516972 -4.1762171 -4.1879396 -4.1965008 -4.2000828 -4.200911][-4.2518349 -4.2658739 -4.2707891 -4.2689109 -4.2619696 -4.25006 -4.2331886 -4.2202082 -4.2283978 -4.248405 -4.2559834 -4.2536187 -4.254025 -4.2516084 -4.2480235][-4.2754951 -4.2885695 -4.2921052 -4.2906537 -4.2884235 -4.2846961 -4.2801642 -4.2801437 -4.2908177 -4.3024826 -4.300952 -4.2931075 -4.2894487 -4.2843113 -4.2790275][-4.2845445 -4.2949452 -4.2979045 -4.2966733 -4.2962146 -4.2964272 -4.2970572 -4.3005214 -4.3090582 -4.3137584 -4.3081861 -4.300838 -4.2981911 -4.2953105 -4.2928309][-4.2847095 -4.290451 -4.2924242 -4.2906432 -4.2892036 -4.290484 -4.2949429 -4.3000755 -4.3047867 -4.3047919 -4.2989984 -4.2945137 -4.2942262 -4.2951775 -4.2970982][-4.2816 -4.2818561 -4.2817683 -4.2785707 -4.2753668 -4.2781262 -4.2868752 -4.2952075 -4.2993345 -4.2984362 -4.2934942 -4.2902617 -4.2909064 -4.2940121 -4.2993193]]...]
INFO - root - 2017-12-07 15:35:59.187782: step 24910, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 57h:09m:22s remains)
INFO - root - 2017-12-07 15:36:06.011736: step 24920, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 59h:23m:13s remains)
INFO - root - 2017-12-07 15:36:12.917532: step 24930, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 55h:36m:26s remains)
INFO - root - 2017-12-07 15:36:19.666584: step 24940, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 57h:14m:45s remains)
INFO - root - 2017-12-07 15:36:26.481325: step 24950, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 61h:41m:20s remains)
INFO - root - 2017-12-07 15:36:33.252090: step 24960, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 56h:55m:48s remains)
INFO - root - 2017-12-07 15:36:40.084280: step 24970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 57h:16m:29s remains)
INFO - root - 2017-12-07 15:36:46.801232: step 24980, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 55h:59m:28s remains)
INFO - root - 2017-12-07 15:36:53.427564: step 24990, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 57h:01m:45s remains)
INFO - root - 2017-12-07 15:37:00.200513: step 25000, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 61h:33m:39s remains)
2017-12-07 15:37:00.873406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0987988 -4.082902 -4.07793 -4.1033711 -4.14048 -4.1605215 -4.171927 -4.1766982 -4.1672525 -4.1547394 -4.1405487 -4.1360087 -4.1493635 -4.1842589 -4.2281394][-4.0599642 -4.0498252 -4.0480723 -4.0776768 -4.119576 -4.1422343 -4.1501164 -4.1386213 -4.1150708 -4.0944691 -4.0740094 -4.0717096 -4.09134 -4.1400146 -4.2016273][-4.0705657 -4.0716014 -4.0738115 -4.0994196 -4.1298037 -4.1453829 -4.1437612 -4.1188579 -4.0902767 -4.069459 -4.0520473 -4.0541377 -4.0730395 -4.1247616 -4.1937852][-4.1102929 -4.1125722 -4.1124625 -4.1224217 -4.1289358 -4.1340556 -4.1286592 -4.1071191 -4.0848022 -4.0728517 -4.072053 -4.0836248 -4.1012721 -4.1465673 -4.2086248][-4.1156921 -4.1279922 -4.1349554 -4.1355524 -4.1297054 -4.1172953 -4.1006141 -4.0753355 -4.0527334 -4.0493464 -4.0699167 -4.0943141 -4.1163864 -4.1637383 -4.2258067][-4.0927968 -4.1182342 -4.1384606 -4.14014 -4.1261158 -4.0907493 -4.0553408 -4.0139809 -3.9668913 -3.956336 -4.0068784 -4.0608563 -4.1018968 -4.1613894 -4.2320623][-4.0913262 -4.1256647 -4.1472287 -4.1408796 -4.1015525 -4.0346813 -3.9803956 -3.9172575 -3.831403 -3.806473 -3.9064963 -4.0125256 -4.080307 -4.1530695 -4.2302804][-4.1289248 -4.1612916 -4.17488 -4.154779 -4.0889955 -3.9933603 -3.9211392 -3.8490686 -3.750623 -3.7358813 -3.8716829 -4.0046177 -4.082284 -4.1540408 -4.2276797][-4.1723771 -4.1886392 -4.1872053 -4.1582994 -4.0991921 -4.0169644 -3.9533181 -3.9013855 -3.8498645 -3.8654962 -3.9697647 -4.06559 -4.1196313 -4.1732745 -4.2336168][-4.1952567 -4.1927786 -4.1783528 -4.1459165 -4.1073222 -4.0672917 -4.0310478 -3.9962342 -3.9769897 -4.0005093 -4.0729051 -4.1333485 -4.163281 -4.2045932 -4.2534347][-4.1822081 -4.1723242 -4.15951 -4.1349578 -4.1152654 -4.1010566 -4.0825386 -4.0539708 -4.0397763 -4.0579967 -4.1139569 -4.1620369 -4.1869464 -4.2277255 -4.2714796][-4.1549926 -4.1401625 -4.1336112 -4.1172934 -4.1042829 -4.0914187 -4.0757675 -4.0558758 -4.0453415 -4.0625486 -4.1181989 -4.1687584 -4.199512 -4.2419596 -4.2825475][-4.1699753 -4.1520019 -4.1417232 -4.1281247 -4.1125412 -4.0958095 -4.0812678 -4.0801291 -4.0834165 -4.1060405 -4.1585469 -4.2055297 -4.2333455 -4.2664828 -4.2961025][-4.2137823 -4.1904488 -4.177125 -4.1695104 -4.1628804 -4.1509066 -4.1434045 -4.153131 -4.1659141 -4.1861157 -4.2212505 -4.251658 -4.2701063 -4.290689 -4.3095312][-4.2530293 -4.2307611 -4.2190685 -4.2197075 -4.2253537 -4.2211618 -4.2138314 -4.2198954 -4.2324657 -4.2482481 -4.266325 -4.2807803 -4.29198 -4.3054929 -4.318378]]...]
INFO - root - 2017-12-07 15:37:07.738841: step 25010, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 54h:10m:25s remains)
INFO - root - 2017-12-07 15:37:14.543739: step 25020, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 60h:39m:01s remains)
INFO - root - 2017-12-07 15:37:21.271679: step 25030, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 61h:13m:29s remains)
INFO - root - 2017-12-07 15:37:27.823646: step 25040, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 58h:25m:10s remains)
INFO - root - 2017-12-07 15:37:34.629610: step 25050, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 54h:43m:52s remains)
INFO - root - 2017-12-07 15:37:41.557262: step 25060, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 58h:45m:16s remains)
INFO - root - 2017-12-07 15:37:48.293824: step 25070, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 60h:29m:38s remains)
INFO - root - 2017-12-07 15:37:55.176288: step 25080, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 62h:31m:05s remains)
INFO - root - 2017-12-07 15:38:01.970878: step 25090, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 57h:52m:37s remains)
INFO - root - 2017-12-07 15:38:08.702828: step 25100, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 54h:19m:44s remains)
2017-12-07 15:38:09.425693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0365868 -4.0453262 -4.0556917 -4.0693789 -4.0830488 -4.1066475 -4.1053925 -4.0716887 -4.0429029 -4.0508833 -4.076529 -4.1107869 -4.1492267 -4.1877179 -4.23351][-4.1269298 -4.1283336 -4.130878 -4.1289511 -4.1307788 -4.147953 -4.1590681 -4.1424637 -4.1219149 -4.1252117 -4.1387939 -4.1572652 -4.1812162 -4.2109976 -4.2488041][-4.2088642 -4.1997585 -4.1911082 -4.1736822 -4.1610794 -4.1697106 -4.1871381 -4.1872029 -4.1811976 -4.1893911 -4.1955514 -4.2049923 -4.21809 -4.2383223 -4.2664657][-4.2526832 -4.2381644 -4.2188907 -4.1890321 -4.1675467 -4.1643434 -4.1730127 -4.168766 -4.1725178 -4.195591 -4.2128015 -4.2238441 -4.2356706 -4.2512832 -4.2751956][-4.2339725 -4.2228251 -4.2019763 -4.1682825 -4.1439228 -4.1337447 -4.1216278 -4.0861497 -4.0809669 -4.1224837 -4.16562 -4.1984425 -4.2211971 -4.2435546 -4.2709708][-4.1813493 -4.1859226 -4.1705475 -4.1376109 -4.1167369 -4.1049781 -4.0662923 -3.9849727 -3.9477713 -3.9954555 -4.0691171 -4.1366186 -4.1874447 -4.2273293 -4.26246][-4.1375675 -4.1640906 -4.1564903 -4.1235032 -4.096909 -4.0757866 -4.005856 -3.8842964 -3.8155429 -3.8653674 -3.968498 -4.06695 -4.1432705 -4.1994967 -4.2454228][-4.0911984 -4.1354818 -4.1393056 -4.1090417 -4.0765386 -4.041656 -3.9371746 -3.7810774 -3.6939163 -3.756645 -3.8890893 -4.009726 -4.1061788 -4.1745691 -4.2278247][-4.0319934 -4.0893559 -4.1102958 -4.0925288 -4.0634236 -4.0226836 -3.8969412 -3.7186594 -3.6156375 -3.6778121 -3.8141437 -3.9430041 -4.0530672 -4.1345744 -4.20196][-4.0169182 -4.0707321 -4.1048722 -4.1068358 -4.0940461 -4.064724 -3.9580238 -3.8013544 -3.7023396 -3.7345672 -3.8246253 -3.9243999 -4.0230212 -4.1052694 -4.1803279][-4.0621223 -4.1046515 -4.1472683 -4.1629481 -4.1579385 -4.1434 -4.0802932 -3.9781277 -3.90589 -3.916544 -3.9615026 -4.0161872 -4.0774612 -4.1321039 -4.1919065][-4.1239152 -4.158977 -4.2044945 -4.2272658 -4.2268558 -4.2242026 -4.1956558 -4.139 -4.0920978 -4.0914326 -4.1108484 -4.1341677 -4.1645551 -4.1910129 -4.2285442][-4.1746526 -4.1958823 -4.2347565 -4.258781 -4.261704 -4.2665539 -4.2644296 -4.2417741 -4.2148123 -4.2116084 -4.2158413 -4.2184763 -4.2282906 -4.2391419 -4.2618356][-4.1910129 -4.2037263 -4.2322783 -4.2491941 -4.2500343 -4.2604036 -4.2699122 -4.2622166 -4.246068 -4.2407446 -4.2380438 -4.236351 -4.2435212 -4.2535892 -4.2751904][-4.1773906 -4.1841974 -4.20059 -4.2098651 -4.2093129 -4.2183266 -4.2260709 -4.21804 -4.2056293 -4.20148 -4.1975241 -4.20308 -4.219552 -4.2400579 -4.2706518]]...]
INFO - root - 2017-12-07 15:38:16.294467: step 25110, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 62h:12m:13s remains)
INFO - root - 2017-12-07 15:38:23.114713: step 25120, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 58h:52m:03s remains)
INFO - root - 2017-12-07 15:38:29.908224: step 25130, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 55h:27m:47s remains)
INFO - root - 2017-12-07 15:38:36.713648: step 25140, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 58h:03m:21s remains)
INFO - root - 2017-12-07 15:38:43.551785: step 25150, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 59h:58m:55s remains)
INFO - root - 2017-12-07 15:38:50.357698: step 25160, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 58h:00m:34s remains)
INFO - root - 2017-12-07 15:38:57.240450: step 25170, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 55h:07m:26s remains)
INFO - root - 2017-12-07 15:39:04.192990: step 25180, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 55h:13m:25s remains)
INFO - root - 2017-12-07 15:39:10.845669: step 25190, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 61h:21m:07s remains)
INFO - root - 2017-12-07 15:39:17.595346: step 25200, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 59h:16m:13s remains)
2017-12-07 15:39:18.357132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.291676 -4.2753582 -4.2390432 -4.1907105 -4.1422443 -4.1007485 -4.0875211 -4.11521 -4.14676 -4.1596174 -4.1724472 -4.1976771 -4.2298536 -4.2436948 -4.2371178][-4.2906952 -4.2694035 -4.2252574 -4.1656809 -4.1047087 -4.0427122 -4.0152764 -4.0472717 -4.0949707 -4.1219091 -4.1443462 -4.1780925 -4.2157454 -4.23514 -4.2275319][-4.2878208 -4.2584777 -4.2066488 -4.1440911 -4.0804944 -4.0048623 -3.9615843 -3.9919207 -4.0514574 -4.0871654 -4.1172485 -4.1611347 -4.2099996 -4.2283692 -4.2192883][-4.2848034 -4.2499342 -4.1963634 -4.1408615 -4.0824866 -3.9995513 -3.9430771 -3.9689746 -4.02919 -4.0669456 -4.1059036 -4.1611552 -4.215775 -4.2304688 -4.2198081][-4.2846975 -4.2486567 -4.1976924 -4.1478024 -4.0930033 -4.0076451 -3.9420176 -3.9646106 -4.0324016 -4.0788059 -4.1251497 -4.1837358 -4.2282119 -4.2286196 -4.2141423][-4.2820458 -4.2479744 -4.1966863 -4.1425672 -4.0812407 -3.9851232 -3.8974185 -3.9142737 -4.0115409 -4.087307 -4.1443229 -4.1975064 -4.2219596 -4.2019706 -4.1817508][-4.2739534 -4.2361927 -4.1814761 -4.1187506 -4.0404673 -3.9188168 -3.7905965 -3.8048351 -3.9559121 -4.0673246 -4.1231856 -4.1608968 -4.1700969 -4.1379466 -4.1102867][-4.272305 -4.231514 -4.1736703 -4.1016746 -4.0106826 -3.8806918 -3.7482381 -3.7793713 -3.9481936 -4.05773 -4.0915732 -4.107018 -4.1028585 -4.0649762 -4.0334053][-4.2825871 -4.2417517 -4.1817389 -4.1069546 -4.0222058 -3.92378 -3.8412194 -3.8845873 -4.0097127 -4.0795488 -4.0826573 -4.0752115 -4.0602088 -4.0172815 -3.9856942][-4.2951488 -4.2594776 -4.20353 -4.1333022 -4.0648994 -4.0027528 -3.9587615 -3.9953403 -4.0725913 -4.1036906 -4.0860176 -4.0719595 -4.0585704 -4.0219231 -3.9967549][-4.3026552 -4.2732358 -4.22564 -4.1684527 -4.1184564 -4.0796847 -4.0537548 -4.0762606 -4.115984 -4.1205025 -4.0966015 -4.0918231 -4.0937924 -4.0762367 -4.0558696][-4.3043046 -4.2785153 -4.2425857 -4.2025137 -4.1666284 -4.1414456 -4.1290255 -4.1402278 -4.1514125 -4.1414676 -4.1226773 -4.1280432 -4.1343913 -4.1273665 -4.1143451][-4.3055749 -4.2854686 -4.2627449 -4.2383156 -4.2137566 -4.1963496 -4.19094 -4.1950006 -4.1901064 -4.1749134 -4.1621728 -4.1677032 -4.1711955 -4.1666927 -4.1582432][-4.3062062 -4.2909465 -4.2803621 -4.2700591 -4.2556696 -4.2439494 -4.2384262 -4.2390933 -4.2298083 -4.2122121 -4.2000813 -4.2016287 -4.1989908 -4.1899953 -4.1807361][-4.3065734 -4.2951031 -4.2903028 -4.2880383 -4.2802482 -4.2695804 -4.2627268 -4.2670274 -4.2598729 -4.2431498 -4.23087 -4.2272635 -4.2201571 -4.2080135 -4.1950765]]...]
INFO - root - 2017-12-07 15:39:25.192536: step 25210, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 55h:19m:08s remains)
INFO - root - 2017-12-07 15:39:32.186354: step 25220, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.750 sec/batch; 64h:02m:26s remains)
INFO - root - 2017-12-07 15:39:39.036601: step 25230, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 59h:10m:18s remains)
INFO - root - 2017-12-07 15:39:45.872300: step 25240, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 56h:07m:41s remains)
INFO - root - 2017-12-07 15:39:52.647073: step 25250, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 54h:34m:22s remains)
INFO - root - 2017-12-07 15:39:59.481915: step 25260, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 62h:04m:07s remains)
INFO - root - 2017-12-07 15:40:06.422271: step 25270, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 62h:14m:45s remains)
INFO - root - 2017-12-07 15:40:13.293544: step 25280, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 55h:44m:29s remains)
INFO - root - 2017-12-07 15:40:19.892162: step 25290, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 55h:47m:27s remains)
INFO - root - 2017-12-07 15:40:26.635329: step 25300, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 55h:40m:07s remains)
2017-12-07 15:40:27.364662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2712955 -4.2633529 -4.2524381 -4.2396488 -4.2316208 -4.2391653 -4.2564983 -4.26459 -4.2666869 -4.2703543 -4.2734857 -4.2784023 -4.2841334 -4.2900715 -4.3027616][-4.2436633 -4.2353582 -4.2221045 -4.2062626 -4.1979165 -4.2095008 -4.2322755 -4.2422047 -4.2416229 -4.2431192 -4.2439051 -4.2456055 -4.2533288 -4.2625117 -4.2809172][-4.2326169 -4.2281365 -4.2142839 -4.19245 -4.1798334 -4.1914325 -4.216712 -4.2270904 -4.2250433 -4.2254419 -4.225533 -4.2276306 -4.2365327 -4.2456341 -4.2653461][-4.2214684 -4.21743 -4.200254 -4.1744809 -4.1597228 -4.1698112 -4.1939697 -4.2002754 -4.1966052 -4.1995554 -4.2023067 -4.2062693 -4.2181892 -4.2303586 -4.2523146][-4.2053323 -4.1942677 -4.1685667 -4.1358652 -4.1160717 -4.11895 -4.1339617 -4.1330352 -4.1333494 -4.1487885 -4.1636124 -4.1753206 -4.1933613 -4.2120862 -4.2382464][-4.1947865 -4.1742992 -4.1396847 -4.1003962 -4.073195 -4.0637031 -4.0599618 -4.0466456 -4.0520587 -4.080709 -4.1113715 -4.1344786 -4.1593385 -4.1850376 -4.2164378][-4.1777711 -4.1463571 -4.0999541 -4.0488605 -4.0059233 -3.974853 -3.9479446 -3.9244602 -3.9464297 -3.9975505 -4.0473385 -4.087224 -4.1229606 -4.1587505 -4.1982026][-4.1455855 -4.106225 -4.0526328 -3.9954784 -3.9434502 -3.8970718 -3.8531728 -3.8264818 -3.8647435 -3.9336965 -4.00154 -4.0616546 -4.1101074 -4.1536722 -4.1977172][-4.1123281 -4.0752106 -4.0301957 -3.9831924 -3.9424036 -3.9086165 -3.8764503 -3.8631 -3.9042523 -3.9651513 -4.0252934 -4.0814924 -4.1246243 -4.1632876 -4.2044897][-4.0997858 -4.0703726 -4.03625 -4.001246 -3.9726086 -3.9536448 -3.9346519 -3.9307842 -3.9685738 -4.0196462 -4.067944 -4.1136284 -4.1480446 -4.1792369 -4.2152982][-4.1180043 -4.102293 -4.0853524 -4.0672169 -4.0532322 -4.0459213 -4.034369 -4.0319905 -4.0607285 -4.0998287 -4.1352859 -4.1671419 -4.1896944 -4.2102885 -4.2377124][-4.1502342 -4.1462674 -4.1433258 -4.1402049 -4.1389556 -4.1389389 -4.1330786 -4.1306872 -4.1483269 -4.174479 -4.1984196 -4.2186584 -4.2317214 -4.2437768 -4.2624407][-4.1981068 -4.1982222 -4.200428 -4.2034183 -4.207489 -4.2093468 -4.2064037 -4.203886 -4.2128987 -4.2289581 -4.2445812 -4.257391 -4.2651672 -4.2722139 -4.2847719][-4.2431445 -4.2439404 -4.24717 -4.2506919 -4.2536435 -4.2545509 -4.2529254 -4.24995 -4.2523847 -4.2610683 -4.2718658 -4.2822766 -4.2899432 -4.2962255 -4.3050528][-4.2735329 -4.2731605 -4.2741885 -4.2756448 -4.2774553 -4.2787547 -4.2788382 -4.27765 -4.2779808 -4.282773 -4.2904043 -4.2988887 -4.3062949 -4.3123951 -4.319241]]...]
INFO - root - 2017-12-07 15:40:34.214981: step 25310, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 58h:10m:49s remains)
INFO - root - 2017-12-07 15:40:41.175163: step 25320, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 57h:57m:12s remains)
INFO - root - 2017-12-07 15:40:47.984509: step 25330, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 56h:37m:20s remains)
INFO - root - 2017-12-07 15:40:54.785777: step 25340, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 60h:57m:43s remains)
INFO - root - 2017-12-07 15:41:01.376126: step 25350, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 56h:31m:23s remains)
INFO - root - 2017-12-07 15:41:08.126909: step 25360, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 54h:13m:41s remains)
INFO - root - 2017-12-07 15:41:14.889365: step 25370, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 56h:40m:36s remains)
INFO - root - 2017-12-07 15:41:21.742011: step 25380, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.727 sec/batch; 61h:59m:45s remains)
INFO - root - 2017-12-07 15:41:28.385710: step 25390, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 60h:33m:50s remains)
INFO - root - 2017-12-07 15:41:35.140593: step 25400, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 57h:13m:29s remains)
2017-12-07 15:41:35.871045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29085 -4.2765541 -4.2603259 -4.2479286 -4.2413387 -4.2382355 -4.2288742 -4.2144332 -4.2045465 -4.2104168 -4.2273908 -4.2596731 -4.2872286 -4.3074961 -4.3237772][-4.2609735 -4.23998 -4.2207804 -4.2065244 -4.1983519 -4.194283 -4.1788106 -4.1513157 -4.1336508 -4.1390071 -4.166007 -4.2144985 -4.2566628 -4.2874756 -4.3115354][-4.229784 -4.2000818 -4.17747 -4.1616659 -4.1473355 -4.1433067 -4.1235542 -4.0795574 -4.0521345 -4.0588255 -4.095757 -4.158751 -4.2172947 -4.26156 -4.2953424][-4.1972985 -4.1611519 -4.1424532 -4.1312084 -4.1103816 -4.1049385 -4.0785694 -4.0156832 -3.978467 -3.9928634 -4.04175 -4.1117396 -4.1794429 -4.2349644 -4.2785239][-4.1557655 -4.1139827 -4.1013951 -4.0958567 -4.0687966 -4.0548506 -4.0178232 -3.9378915 -3.8911343 -3.921582 -3.9923506 -4.0694394 -4.143002 -4.2089767 -4.2635093][-4.1164207 -4.0724292 -4.0613213 -4.0585985 -4.0296912 -4.0079865 -3.9579983 -3.863903 -3.8024065 -3.8511364 -3.9548848 -4.0427675 -4.119144 -4.1919112 -4.2537961][-4.0858974 -4.0471158 -4.0357389 -4.0273442 -4.0022435 -3.9760697 -3.9139683 -3.8055718 -3.719317 -3.7775564 -3.9156632 -4.0233908 -4.10852 -4.185688 -4.249939][-4.0605755 -4.0376225 -4.0311189 -4.0172915 -3.9994826 -3.9818199 -3.9181466 -3.7999957 -3.6897 -3.7432549 -3.8967285 -4.0134773 -4.1039495 -4.18205 -4.2459197][-4.0683146 -4.0650678 -4.0748935 -4.0642767 -4.0562177 -4.054709 -3.9974587 -3.8843877 -3.7785406 -3.8145161 -3.9392548 -4.0365834 -4.1143689 -4.1846647 -4.2441173][-4.1213741 -4.1340919 -4.15507 -4.153625 -4.1497498 -4.1532869 -4.1006551 -3.9966187 -3.9074023 -3.926403 -4.0085006 -4.07844 -4.1383 -4.1947331 -4.24672][-4.1906176 -4.2004905 -4.215693 -4.2185621 -4.2227726 -4.2273226 -4.1808162 -4.0900636 -4.0140009 -4.0210266 -4.0733547 -4.1226034 -4.1671538 -4.2103209 -4.2540245][-4.2563252 -4.2576375 -4.264401 -4.2678404 -4.2755723 -4.2789702 -4.2409911 -4.1644344 -4.0970025 -4.0952315 -4.128396 -4.1628828 -4.1952896 -4.2284737 -4.2668591][-4.2944803 -4.2911081 -4.2941337 -4.3019695 -4.3119411 -4.3139267 -4.2862077 -4.2273283 -4.1672435 -4.1579828 -4.1789675 -4.2035275 -4.2263541 -4.253828 -4.2859573][-4.3140645 -4.308455 -4.3100257 -4.320888 -4.3327413 -4.3359628 -4.3186784 -4.2784157 -4.2302589 -4.2162423 -4.226799 -4.2442203 -4.2605648 -4.2824531 -4.3067946][-4.326129 -4.3224139 -4.3247943 -4.3354406 -4.3451104 -4.3478312 -4.3392849 -4.3134966 -4.277967 -4.2638149 -4.2693892 -4.2814116 -4.2911987 -4.3064108 -4.3234329]]...]
INFO - root - 2017-12-07 15:41:42.759371: step 25410, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.756 sec/batch; 64h:31m:07s remains)
INFO - root - 2017-12-07 15:41:49.544918: step 25420, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 59h:51m:16s remains)
INFO - root - 2017-12-07 15:41:56.316159: step 25430, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.648 sec/batch; 55h:14m:16s remains)
INFO - root - 2017-12-07 15:42:03.134713: step 25440, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 55h:50m:44s remains)
INFO - root - 2017-12-07 15:42:09.965192: step 25450, loss = 2.03, batch loss = 1.98 (10.8 examples/sec; 0.742 sec/batch; 63h:15m:44s remains)
INFO - root - 2017-12-07 15:42:16.780098: step 25460, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 61h:15m:14s remains)
INFO - root - 2017-12-07 15:42:23.669313: step 25470, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 56h:15m:51s remains)
INFO - root - 2017-12-07 15:42:30.458795: step 25480, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 54h:31m:18s remains)
INFO - root - 2017-12-07 15:42:37.185012: step 25490, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 55h:56m:01s remains)
INFO - root - 2017-12-07 15:42:44.032971: step 25500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 60h:57m:38s remains)
2017-12-07 15:42:44.733271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3033323 -4.3113918 -4.3191471 -4.3242383 -4.3279276 -4.3299465 -4.3295188 -4.3268523 -4.3237896 -4.3193812 -4.3151197 -4.3128 -4.3114185 -4.3103461 -4.3100276][-4.278625 -4.2883005 -4.2996969 -4.3098736 -4.3196869 -4.3282862 -4.3331337 -4.3330355 -4.3319077 -4.328321 -4.324542 -4.3224936 -4.3219042 -4.3205004 -4.3191252][-4.2373366 -4.2446203 -4.2576733 -4.2732673 -4.289526 -4.3059411 -4.3175015 -4.3225765 -4.3264256 -4.3282251 -4.3285317 -4.3284903 -4.3291979 -4.3287039 -4.3262849][-4.1963954 -4.1900444 -4.1952229 -4.2119694 -4.2344966 -4.257134 -4.2751083 -4.287209 -4.29776 -4.3075733 -4.3161783 -4.3237381 -4.3283148 -4.3287945 -4.3255439][-4.1788545 -4.1541715 -4.1399736 -4.1448789 -4.1653333 -4.1893435 -4.2101545 -4.2279458 -4.2471404 -4.2679577 -4.2876863 -4.3044229 -4.3148065 -4.3175139 -4.3162227][-4.184772 -4.1457515 -4.1104589 -4.0948296 -4.0993848 -4.1144629 -4.1335087 -4.153688 -4.1777387 -4.2098761 -4.2427115 -4.2703466 -4.2867985 -4.29812 -4.303751][-4.216135 -4.17047 -4.1212378 -4.0825181 -4.0599661 -4.0547423 -4.0623326 -4.0759583 -4.1001587 -4.1391664 -4.1850605 -4.2243233 -4.2489538 -4.2719841 -4.2889509][-4.2638712 -4.226378 -4.1793056 -4.1303864 -4.0863833 -4.0539575 -4.0335431 -4.0213952 -4.0295072 -4.0665169 -4.1229591 -4.1770325 -4.2137513 -4.2472906 -4.2742205][-4.3100548 -4.2897663 -4.25593 -4.2126 -4.1640081 -4.1136489 -4.0660114 -4.0212903 -3.9926338 -4.0052447 -4.0582142 -4.1242924 -4.1765647 -4.2237043 -4.2614918][-4.33201 -4.3300686 -4.3138762 -4.2858653 -4.2460032 -4.1967173 -4.1416416 -4.081037 -4.0258689 -4.0037065 -4.0301714 -4.0901504 -4.1490288 -4.2020965 -4.2440014][-4.3216591 -4.3379345 -4.341289 -4.3317442 -4.308177 -4.2717676 -4.2234597 -4.1634521 -4.1039972 -4.0637689 -4.0601563 -4.0966449 -4.1428852 -4.1839032 -4.2181616][-4.2934375 -4.3213816 -4.3422465 -4.351459 -4.3472533 -4.3288808 -4.2933559 -4.2428794 -4.1929049 -4.151628 -4.1295185 -4.138463 -4.1583056 -4.1748657 -4.1901212][-4.2688608 -4.3001304 -4.327683 -4.3482022 -4.3587127 -4.3571086 -4.3373628 -4.3000727 -4.2622132 -4.2261348 -4.1962433 -4.1821704 -4.1773748 -4.1731143 -4.1676564][-4.2485681 -4.2745471 -4.300993 -4.3255568 -4.3456116 -4.3580632 -4.3550138 -4.332058 -4.3028464 -4.271699 -4.2401175 -4.2133732 -4.1929941 -4.1782384 -4.1612511][-4.2277913 -4.2409191 -4.2607365 -4.286582 -4.3111453 -4.3324008 -4.3438253 -4.3365703 -4.3179231 -4.29566 -4.2683916 -4.2394056 -4.2148719 -4.1988115 -4.1779976]]...]
INFO - root - 2017-12-07 15:42:51.482794: step 25510, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 54h:16m:56s remains)
INFO - root - 2017-12-07 15:42:58.255176: step 25520, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 58h:47m:07s remains)
INFO - root - 2017-12-07 15:43:05.104258: step 25530, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 60h:21m:06s remains)
INFO - root - 2017-12-07 15:43:11.859872: step 25540, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 59h:31m:11s remains)
INFO - root - 2017-12-07 15:43:18.616928: step 25550, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 57h:16m:44s remains)
INFO - root - 2017-12-07 15:43:25.334003: step 25560, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 52h:20m:17s remains)
INFO - root - 2017-12-07 15:43:32.066355: step 25570, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 55h:31m:35s remains)
INFO - root - 2017-12-07 15:43:38.947928: step 25580, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.751 sec/batch; 64h:02m:28s remains)
INFO - root - 2017-12-07 15:43:45.565845: step 25590, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 58h:58m:31s remains)
INFO - root - 2017-12-07 15:43:52.272257: step 25600, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 53h:15m:12s remains)
2017-12-07 15:43:53.038868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2547121 -4.2397833 -4.2342606 -4.2211084 -4.1981468 -4.1636539 -4.1186481 -4.0904527 -4.1175046 -4.1858435 -4.2569981 -4.3058944 -4.3300085 -4.3418841 -4.3461685][-4.2355027 -4.2153826 -4.2112875 -4.2047133 -4.18694 -4.1540465 -4.1034341 -4.0729227 -4.1047826 -4.1785331 -4.2532663 -4.3063273 -4.3315339 -4.3429236 -4.3464108][-4.2234688 -4.2005544 -4.1965165 -4.1917663 -4.1767392 -4.1409411 -4.0813913 -4.0501347 -4.0890031 -4.1669321 -4.2452526 -4.3036284 -4.3311028 -4.3421063 -4.3455105][-4.2256484 -4.1990457 -4.1888647 -4.1780624 -4.1581421 -4.1149592 -4.0485673 -4.0215912 -4.0710611 -4.155302 -4.238378 -4.301662 -4.3317108 -4.3417287 -4.3444586][-4.2421536 -4.2117829 -4.1915245 -4.1669388 -4.1312523 -4.0775237 -4.0086937 -3.9919949 -4.0537691 -4.1448026 -4.2315774 -4.2985225 -4.3317995 -4.3417263 -4.3437076][-4.2715669 -4.2430191 -4.2152514 -4.1747293 -4.1190848 -4.0510044 -3.980449 -3.9714789 -4.0397067 -4.13634 -4.2237945 -4.2934318 -4.329597 -4.3414454 -4.3436055][-4.3013926 -4.2799473 -4.2527318 -4.2050014 -4.1359906 -4.0562077 -3.9807005 -3.9662235 -4.0304356 -4.1289983 -4.2179003 -4.2888136 -4.3272 -4.3409986 -4.3438106][-4.3227773 -4.3121815 -4.2952523 -4.2542977 -4.1836314 -4.094748 -4.006341 -3.9731061 -4.0261974 -4.1237154 -4.2148633 -4.2864962 -4.3255234 -4.3406415 -4.34406][-4.33485 -4.3349423 -4.3300743 -4.3024149 -4.2389379 -4.1449265 -4.0404115 -3.9837515 -4.0223279 -4.1172256 -4.2108583 -4.2838497 -4.323195 -4.3396778 -4.3443122][-4.3380976 -4.3436503 -4.3461032 -4.3302908 -4.2782021 -4.1860991 -4.0724044 -3.9959333 -4.0189052 -4.1088295 -4.2041802 -4.2787127 -4.3193107 -4.3375645 -4.3444271][-4.336133 -4.3434596 -4.3492823 -4.3416724 -4.3015122 -4.2180381 -4.10508 -4.017056 -4.0240941 -4.1054111 -4.1991467 -4.27463 -4.3163295 -4.3356085 -4.3441796][-4.3329449 -4.3409524 -4.3485422 -4.3462634 -4.316515 -4.2436514 -4.1372271 -4.0440044 -4.0366693 -4.1088181 -4.1982431 -4.2733068 -4.3158336 -4.3347282 -4.3439193][-4.3287463 -4.3358679 -4.3446841 -4.3477936 -4.3281183 -4.2662163 -4.1705251 -4.0790458 -4.0585718 -4.1181965 -4.2026725 -4.2763491 -4.3175125 -4.3352733 -4.344409][-4.3265815 -4.3303709 -4.3385582 -4.3469791 -4.3383222 -4.289494 -4.2088494 -4.1268988 -4.0986309 -4.1407828 -4.2139959 -4.2819791 -4.3203411 -4.3373914 -4.346077][-4.324595 -4.3252244 -4.3327165 -4.3443103 -4.3435311 -4.3048978 -4.2389331 -4.1710477 -4.1422219 -4.1687479 -4.2285843 -4.2877612 -4.3229337 -4.3403778 -4.3484435]]...]
INFO - root - 2017-12-07 15:43:59.856815: step 25610, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 61h:19m:07s remains)
INFO - root - 2017-12-07 15:44:06.564191: step 25620, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 56h:05m:47s remains)
INFO - root - 2017-12-07 15:44:13.350837: step 25630, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 55h:55m:49s remains)
INFO - root - 2017-12-07 15:44:20.205652: step 25640, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 56h:34m:00s remains)
INFO - root - 2017-12-07 15:44:27.056051: step 25650, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 63h:20m:30s remains)
INFO - root - 2017-12-07 15:44:33.713210: step 25660, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 61h:00m:38s remains)
INFO - root - 2017-12-07 15:44:40.516510: step 25670, loss = 2.11, batch loss = 2.05 (11.7 examples/sec; 0.681 sec/batch; 58h:03m:27s remains)
INFO - root - 2017-12-07 15:44:47.309975: step 25680, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 54h:55m:19s remains)
INFO - root - 2017-12-07 15:44:53.987852: step 25690, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 60h:54m:32s remains)
INFO - root - 2017-12-07 15:45:00.734483: step 25700, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 58h:21m:30s remains)
2017-12-07 15:45:01.403139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2963095 -4.2929287 -4.2817254 -4.2707295 -4.2712712 -4.2843709 -4.2963042 -4.3006248 -4.2925119 -4.2670922 -4.2202382 -4.1686878 -4.140079 -4.1462131 -4.1797271][-4.2716379 -4.2637067 -4.248558 -4.2341609 -4.2300749 -4.2392445 -4.2542348 -4.27285 -4.2819343 -4.2691436 -4.229404 -4.17651 -4.144012 -4.1419358 -4.1692443][-4.2451053 -4.2260842 -4.1992216 -4.1726952 -4.1583886 -4.1604977 -4.1800528 -4.2168403 -4.2477703 -4.257134 -4.2403922 -4.2011738 -4.1677904 -4.1512828 -4.1642942][-4.2166486 -4.1848884 -4.1434956 -4.1011863 -4.0693526 -4.0585294 -4.0821037 -4.140316 -4.1988568 -4.2355537 -4.2440248 -4.2238107 -4.1912551 -4.1586275 -4.1530294][-4.188798 -4.1456652 -4.0966096 -4.0485992 -4.0011115 -3.970422 -3.9879675 -4.0624156 -4.1474996 -4.2128925 -4.2439437 -4.2386484 -4.2072506 -4.1636825 -4.1419511][-4.1695604 -4.1156178 -4.0651531 -4.0218639 -3.9683523 -3.9163229 -3.9138021 -3.986207 -4.0899072 -4.1818919 -4.2344255 -4.2427721 -4.2173018 -4.1762328 -4.1479192][-4.1696568 -4.1090531 -4.0607147 -4.0267396 -3.9809136 -3.9184737 -3.8902316 -3.9401793 -4.0419531 -4.1478248 -4.2192826 -4.2454491 -4.2356205 -4.2070932 -4.1831155][-4.1983023 -4.1406164 -4.0987272 -4.074687 -4.0419097 -3.9836452 -3.941036 -3.96248 -4.0395336 -4.1351495 -4.2099228 -4.2488737 -4.256424 -4.242907 -4.2273145][-4.24255 -4.1972375 -4.1646419 -4.1492562 -4.1282697 -4.0801678 -4.036262 -4.0420585 -4.0927815 -4.1633682 -4.2244015 -4.2631974 -4.2776766 -4.2741485 -4.26514][-4.2751112 -4.2484775 -4.2295904 -4.2250361 -4.2150989 -4.1799607 -4.1412859 -4.1386566 -4.1670189 -4.2115617 -4.2544661 -4.2836032 -4.2948041 -4.2928834 -4.2870908][-4.2707229 -4.2620392 -4.2586074 -4.267024 -4.2685666 -4.2467508 -4.2183194 -4.21378 -4.227191 -4.2521071 -4.2776613 -4.2951684 -4.3000617 -4.2967215 -4.2910633][-4.2384152 -4.2399616 -4.249465 -4.2692289 -4.2806177 -4.2678084 -4.2470651 -4.24218 -4.2464962 -4.2572474 -4.2694931 -4.2795372 -4.2824354 -4.2795315 -4.27416][-4.2013941 -4.2068133 -4.2232957 -4.2482748 -4.2638021 -4.2556748 -4.2379103 -4.23062 -4.2321248 -4.2390914 -4.2459283 -4.2529721 -4.25762 -4.2568774 -4.2533145][-4.1942854 -4.200623 -4.2158942 -4.2360864 -4.2471676 -4.2383337 -4.2202406 -4.2096243 -4.2110014 -4.220439 -4.2277613 -4.2347522 -4.2429605 -4.2478909 -4.248723][-4.2285328 -4.2351942 -4.2468586 -4.258862 -4.2621241 -4.2496905 -4.2299747 -4.2162781 -4.2154927 -4.2261558 -4.2366805 -4.2464061 -4.2574382 -4.2663555 -4.2704544]]...]
INFO - root - 2017-12-07 15:45:08.293740: step 25710, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 58h:14m:19s remains)
INFO - root - 2017-12-07 15:45:14.970367: step 25720, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 59h:49m:21s remains)
INFO - root - 2017-12-07 15:45:21.668162: step 25730, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 58h:08m:39s remains)
INFO - root - 2017-12-07 15:45:28.577612: step 25740, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 56h:05m:54s remains)
INFO - root - 2017-12-07 15:45:35.370896: step 25750, loss = 2.03, batch loss = 1.98 (12.4 examples/sec; 0.643 sec/batch; 54h:49m:18s remains)
INFO - root - 2017-12-07 15:45:42.138312: step 25760, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 54h:42m:59s remains)
INFO - root - 2017-12-07 15:45:49.045051: step 25770, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 60h:38m:13s remains)
INFO - root - 2017-12-07 15:45:55.829020: step 25780, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 61h:42m:11s remains)
INFO - root - 2017-12-07 15:46:02.442888: step 25790, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 56h:55m:27s remains)
INFO - root - 2017-12-07 15:46:09.182156: step 25800, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 54h:05m:00s remains)
2017-12-07 15:46:09.950630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1584005 -4.1351218 -4.1362495 -4.1629214 -4.1918592 -4.2074661 -4.203402 -4.18707 -4.1696129 -4.159862 -4.1560049 -4.1507421 -4.13699 -4.1285682 -4.1264853][-4.1313834 -4.105957 -4.1130266 -4.1408477 -4.1633525 -4.1733203 -4.1709194 -4.1613436 -4.1557212 -4.1530991 -4.1517706 -4.1429358 -4.1202126 -4.1032157 -4.0993185][-4.1002078 -4.0774136 -4.0928121 -4.1230912 -4.1422195 -4.1438046 -4.1372828 -4.1331496 -4.1422639 -4.1521859 -4.1536436 -4.1395531 -4.1046562 -4.0757337 -4.0663829][-4.0681086 -4.0633826 -4.0923772 -4.1182055 -4.1253572 -4.1098552 -4.0916014 -4.0880895 -4.1092081 -4.1323371 -4.1402826 -4.1231565 -4.08153 -4.0459909 -4.0352712][-4.0585222 -4.0711794 -4.1024156 -4.1140203 -4.1027484 -4.0634379 -4.0263042 -4.0243211 -4.0578151 -4.095036 -4.1117492 -4.0981846 -4.0587282 -4.0317144 -4.030581][-4.0841889 -4.0983253 -4.1153274 -4.1116033 -4.0790477 -4.0097613 -3.9459412 -3.9497998 -3.9971817 -4.0443621 -4.0733118 -4.0721788 -4.0536447 -4.0463991 -4.0550904][-4.1127157 -4.119329 -4.1188083 -4.0968962 -4.0375414 -3.9300957 -3.8383749 -3.8666728 -3.9453371 -4.0102959 -4.0548873 -4.0778885 -4.0874281 -4.0936818 -4.0989661][-4.1188922 -4.120223 -4.1081977 -4.0717363 -3.9964633 -3.8799748 -3.8011112 -3.863302 -3.9627702 -4.0383043 -4.0929203 -4.1262884 -4.1429696 -4.1429873 -4.1343355][-4.1223383 -4.1273584 -4.1140604 -4.0733919 -4.0092363 -3.9301534 -3.9041553 -3.9657419 -4.0410504 -4.0994368 -4.1409044 -4.1632376 -4.1716089 -4.1590776 -4.1378446][-4.1452413 -4.1491313 -4.1363645 -4.0983505 -4.0485148 -4.0039716 -4.0103021 -4.0632687 -4.115243 -4.149415 -4.1681838 -4.1773372 -4.1787786 -4.1575274 -4.1210613][-4.1533217 -4.1458974 -4.1282315 -4.0945091 -4.059741 -4.0436134 -4.0745239 -4.1281791 -4.1719675 -4.1892204 -4.1914768 -4.1912494 -4.1861758 -4.1562805 -4.1102047][-4.158236 -4.1395159 -4.1167817 -4.0912838 -4.07487 -4.0817585 -4.1245441 -4.1762891 -4.2157812 -4.2275162 -4.224359 -4.2191133 -4.2065258 -4.1737986 -4.1315618][-4.1835341 -4.1640577 -4.1416759 -4.1218257 -4.1167459 -4.1372504 -4.1792488 -4.2208018 -4.251615 -4.2609439 -4.260674 -4.25809 -4.2466178 -4.2206173 -4.1853137][-4.22412 -4.2117991 -4.1974368 -4.1824389 -4.177669 -4.1963768 -4.2303934 -4.259953 -4.2790718 -4.2865024 -4.2887006 -4.2873969 -4.28008 -4.2618523 -4.2325888][-4.2662477 -4.260118 -4.2557878 -4.2485747 -4.2438755 -4.2525005 -4.2702322 -4.2851095 -4.2951059 -4.3004856 -4.3018274 -4.2982235 -4.2930021 -4.2816753 -4.2621641]]...]
INFO - root - 2017-12-07 15:46:16.849682: step 25810, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.761 sec/batch; 64h:51m:53s remains)
INFO - root - 2017-12-07 15:46:23.515583: step 25820, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 54h:42m:55s remains)
INFO - root - 2017-12-07 15:46:30.268290: step 25830, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 52h:53m:25s remains)
INFO - root - 2017-12-07 15:46:37.111413: step 25840, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 55h:22m:03s remains)
INFO - root - 2017-12-07 15:46:43.980807: step 25850, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 60h:21m:00s remains)
INFO - root - 2017-12-07 15:46:50.836417: step 25860, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 59h:09m:43s remains)
INFO - root - 2017-12-07 15:46:57.646470: step 25870, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 54h:11m:24s remains)
INFO - root - 2017-12-07 15:47:04.421952: step 25880, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 53h:38m:54s remains)
INFO - root - 2017-12-07 15:47:11.145776: step 25890, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 59h:50m:40s remains)
INFO - root - 2017-12-07 15:47:17.866014: step 25900, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 58h:55m:46s remains)
2017-12-07 15:47:18.562348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3170838 -4.3024769 -4.2864857 -4.272913 -4.2648087 -4.2612782 -4.2620611 -4.265625 -4.2707458 -4.2746296 -4.2875829 -4.2950058 -4.2994585 -4.307992 -4.3200912][-4.3011818 -4.2783637 -4.254776 -4.2327318 -4.2164493 -4.2099247 -4.2119641 -4.2170715 -4.2230878 -4.2268362 -4.2462611 -4.2540441 -4.2590613 -4.2713361 -4.2885437][-4.29802 -4.2706985 -4.2433124 -4.2128863 -4.1870079 -4.1790981 -4.1802239 -4.1812663 -4.1837649 -4.1828222 -4.1996713 -4.2037616 -4.207335 -4.2226887 -4.244102][-4.3014317 -4.2776189 -4.2525806 -4.2198381 -4.1892638 -4.1810451 -4.1727405 -4.1602321 -4.1517134 -4.1363597 -4.141799 -4.1385241 -4.1372705 -4.1535935 -4.1812258][-4.3039823 -4.285501 -4.2638197 -4.2303624 -4.195395 -4.1786284 -4.1522408 -4.1212759 -4.1004343 -4.068728 -4.06406 -4.0549483 -4.0505509 -4.0723157 -4.1113639][-4.2970772 -4.2804627 -4.2580929 -4.2222629 -4.1784573 -4.1442089 -4.0964732 -4.0485139 -4.0185251 -3.9779148 -3.9716609 -3.9673398 -3.9711008 -4.0078936 -4.0603333][-4.2730169 -4.249176 -4.2182503 -4.1742668 -4.1179891 -4.0624075 -3.9988923 -3.9403632 -3.9097884 -3.8731589 -3.8713174 -3.8840902 -3.9078391 -3.9698896 -4.0383787][-4.2386975 -4.2031112 -4.1611967 -4.1068039 -4.0411878 -3.9716234 -3.904741 -3.8538446 -3.837527 -3.8213825 -3.8335781 -3.86557 -3.9061594 -3.9852946 -4.0575285][-4.2160506 -4.1720796 -4.1270013 -4.0765967 -4.0208845 -3.9627357 -3.9176526 -3.892333 -3.88945 -3.8909242 -3.9082184 -3.9410751 -3.982738 -4.05762 -4.1207571][-4.2344694 -4.1996655 -4.1685634 -4.1368237 -4.1026616 -4.0656471 -4.0420237 -4.0317163 -4.0293708 -4.0353937 -4.0509286 -4.0777774 -4.1119995 -4.1675792 -4.2129388][-4.2808671 -4.2637415 -4.2518368 -4.2368264 -4.2189431 -4.1981535 -4.1852455 -4.1791377 -4.1754861 -4.1816978 -4.194284 -4.2154016 -4.239593 -4.2726054 -4.2976236][-4.3238559 -4.3178458 -4.3163505 -4.3104591 -4.3023024 -4.292644 -4.2847338 -4.2806444 -4.2791171 -4.28403 -4.2934866 -4.3073874 -4.3200831 -4.3354163 -4.3457294][-4.3536177 -4.3512473 -4.3515906 -4.3492227 -4.3458781 -4.3425064 -4.3383222 -4.3368464 -4.3365488 -4.3387012 -4.3430476 -4.3489137 -4.3538561 -4.3605151 -4.3646841][-4.3682027 -4.3668265 -4.3671412 -4.36718 -4.3673787 -4.3674216 -4.3661051 -4.3660183 -4.36523 -4.3652706 -4.3665442 -4.3675742 -4.3684444 -4.3708434 -4.3728952][-4.3744545 -4.3741751 -4.3739104 -4.374259 -4.3745036 -4.3747482 -4.3746414 -4.3746839 -4.3742461 -4.3738556 -4.3740411 -4.3741417 -4.3743339 -4.3751645 -4.3761744]]...]
INFO - root - 2017-12-07 15:47:25.368381: step 25910, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 54h:46m:16s remains)
INFO - root - 2017-12-07 15:47:32.204504: step 25920, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 61h:02m:22s remains)
INFO - root - 2017-12-07 15:47:39.015148: step 25930, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 60h:25m:27s remains)
INFO - root - 2017-12-07 15:47:45.687706: step 25940, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 58h:01m:00s remains)
INFO - root - 2017-12-07 15:47:52.421821: step 25950, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 55h:20m:42s remains)
INFO - root - 2017-12-07 15:47:59.210477: step 25960, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 57h:13m:09s remains)
INFO - root - 2017-12-07 15:48:05.865303: step 25970, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 60h:50m:14s remains)
INFO - root - 2017-12-07 15:48:12.581612: step 25980, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 54h:53m:50s remains)
INFO - root - 2017-12-07 15:48:19.274942: step 25990, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 58h:21m:17s remains)
INFO - root - 2017-12-07 15:48:25.972556: step 26000, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 54h:19m:28s remains)
2017-12-07 15:48:26.798451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1353035 -4.1435065 -4.1459 -4.1420007 -4.1382217 -4.1378641 -4.140698 -4.1454692 -4.1487675 -4.1550393 -4.1585464 -4.1516094 -4.1340752 -4.1137962 -4.1053553][-4.2070584 -4.2065926 -4.2031617 -4.1954904 -4.1905422 -4.1869817 -4.1845756 -4.1837721 -4.1816072 -4.1852031 -4.1882339 -4.18281 -4.1660576 -4.1472092 -4.1401429][-4.2704344 -4.2651896 -4.2583961 -4.2529473 -4.250154 -4.2444305 -4.2368045 -4.2308974 -4.22848 -4.23252 -4.2367878 -4.2332931 -4.2207556 -4.2053 -4.1994829][-4.2994452 -4.2878151 -4.276979 -4.2744079 -4.2735391 -4.2674785 -4.2570577 -4.2486911 -4.2481985 -4.2581377 -4.2701793 -4.273366 -4.2683864 -4.2586403 -4.2547941][-4.295002 -4.2717609 -4.2523127 -4.2455592 -4.2394195 -4.2292848 -4.2166123 -4.2083235 -4.2135844 -4.23492 -4.2612243 -4.2760677 -4.28197 -4.2807622 -4.2785234][-4.2811651 -4.2428765 -4.2074704 -4.1829138 -4.1576982 -4.1325622 -4.113739 -4.107007 -4.1249986 -4.16367 -4.2104754 -4.2419991 -4.2607327 -4.2683592 -4.2698584][-4.2652106 -4.2181382 -4.168272 -4.1240759 -4.075068 -4.0239 -3.9870124 -3.9785497 -4.0064716 -4.0618014 -4.1290421 -4.1825666 -4.2193942 -4.2398934 -4.249105][-4.2617416 -4.216085 -4.159369 -4.1049571 -4.0440392 -3.9728618 -3.9145243 -3.8941388 -3.9193888 -3.9792259 -4.0569568 -4.12761 -4.1790485 -4.2111735 -4.2313933][-4.2752156 -4.2396436 -4.1896853 -4.1419892 -4.0910459 -4.026556 -3.972415 -3.9485154 -3.9572604 -3.9956648 -4.057209 -4.1173387 -4.1609664 -4.19166 -4.2182527][-4.2966127 -4.2759514 -4.2419748 -4.2112093 -4.1814981 -4.138412 -4.1045814 -4.0900116 -4.0867324 -4.099256 -4.1290164 -4.157763 -4.1750164 -4.1895971 -4.214735][-4.3133593 -4.3036275 -4.2865157 -4.2724042 -4.2586074 -4.2339025 -4.2182531 -4.2148709 -4.2083712 -4.2070494 -4.2119632 -4.2104578 -4.2049942 -4.2052784 -4.2269325][-4.32407 -4.3184094 -4.3126969 -4.312645 -4.3097444 -4.2960172 -4.2875223 -4.2882781 -4.28358 -4.2786703 -4.272347 -4.2528672 -4.2352433 -4.2302818 -4.2497144][-4.3275042 -4.3228173 -4.3256607 -4.3353095 -4.3382211 -4.3286242 -4.3193827 -4.317265 -4.3151774 -4.3128405 -4.3063936 -4.2822747 -4.2576947 -4.25118 -4.2686071][-4.3122487 -4.3082366 -4.31545 -4.3308949 -4.3398633 -4.3347211 -4.3267255 -4.3222961 -4.3189898 -4.314981 -4.3110266 -4.2934651 -4.269866 -4.2627459 -4.2760558][-4.2772689 -4.2724957 -4.2796421 -4.2957172 -4.3068991 -4.3067417 -4.3035593 -4.3016043 -4.2959256 -4.2924156 -4.29302 -4.2847028 -4.2680559 -4.2635221 -4.2738347]]...]
INFO - root - 2017-12-07 15:48:33.555144: step 26010, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.673 sec/batch; 57h:18m:26s remains)
INFO - root - 2017-12-07 15:48:40.300457: step 26020, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.632 sec/batch; 53h:49m:15s remains)
INFO - root - 2017-12-07 15:48:47.083989: step 26030, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 55h:54m:12s remains)
INFO - root - 2017-12-07 15:48:53.916341: step 26040, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 59h:46m:36s remains)
INFO - root - 2017-12-07 15:49:00.687998: step 26050, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 59h:34m:18s remains)
INFO - root - 2017-12-07 15:49:07.363100: step 26060, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 57h:53m:08s remains)
INFO - root - 2017-12-07 15:49:14.076856: step 26070, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 54h:34m:38s remains)
INFO - root - 2017-12-07 15:49:20.885748: step 26080, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.653 sec/batch; 55h:33m:09s remains)
INFO - root - 2017-12-07 15:49:27.712746: step 26090, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 62h:15m:08s remains)
INFO - root - 2017-12-07 15:49:34.570213: step 26100, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.754 sec/batch; 64h:09m:09s remains)
2017-12-07 15:49:35.261133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1549487 -4.1622105 -4.1648669 -4.1699338 -4.1928043 -4.2251062 -4.2445354 -4.2431459 -4.2209139 -4.1985984 -4.1787066 -4.16098 -4.1455197 -4.1274495 -4.1218081][-4.1313267 -4.137846 -4.1416 -4.1541243 -4.1867671 -4.2227707 -4.24132 -4.2392011 -4.2151508 -4.1908541 -4.1674104 -4.1442757 -4.1246166 -4.1009011 -4.0879774][-4.130599 -4.137619 -4.1454225 -4.1615338 -4.1933942 -4.2247109 -4.2370148 -4.2284756 -4.2051973 -4.1859465 -4.1671681 -4.1458216 -4.1249862 -4.1018925 -4.0869331][-4.15948 -4.1618228 -4.1700473 -4.1883345 -4.2099133 -4.2271624 -4.2299576 -4.2148805 -4.1971745 -4.17997 -4.1629562 -4.1458654 -4.1283369 -4.11253 -4.1053433][-4.194335 -4.1801524 -4.1795087 -4.1959867 -4.2068992 -4.2105117 -4.2023253 -4.1885843 -4.1795106 -4.1708426 -4.16288 -4.1533732 -4.1419783 -4.1325932 -4.13155][-4.1989255 -4.1741076 -4.16354 -4.1774774 -4.1796975 -4.1750989 -4.1656346 -4.1544533 -4.1515942 -4.1533442 -4.1567392 -4.1561694 -4.1516256 -4.14844 -4.1511989][-4.1824164 -4.1567636 -4.1352048 -4.14279 -4.1430063 -4.1385522 -4.1311393 -4.1182714 -4.1152158 -4.1244054 -4.1361632 -4.1440134 -4.14476 -4.1460886 -4.147532][-4.1692829 -4.1460795 -4.1214504 -4.1251049 -4.125586 -4.1224451 -4.1156654 -4.10162 -4.0987005 -4.1094356 -4.1188483 -4.1259518 -4.1300869 -4.1320648 -4.1324244][-4.1596322 -4.1502795 -4.1317272 -4.1324239 -4.1337767 -4.1352649 -4.1360307 -4.12828 -4.12937 -4.1329141 -4.1260705 -4.1197996 -4.1168509 -4.1149592 -4.1120558][-4.1217446 -4.1273942 -4.121912 -4.12409 -4.1287532 -4.1397409 -4.1514945 -4.1453562 -4.1461172 -4.1443481 -4.1213932 -4.1031761 -4.0936289 -4.091898 -4.0874233][-4.079236 -4.0917573 -4.0974965 -4.1013865 -4.1048341 -4.1185594 -4.1383181 -4.1358666 -4.1361027 -4.1296625 -4.099884 -4.0746775 -4.0608487 -4.0602541 -4.0579681][-4.0765476 -4.0914803 -4.1013284 -4.102304 -4.1019483 -4.1111674 -4.1243844 -4.1178961 -4.1170144 -4.1101356 -4.0795126 -4.0518904 -4.0348783 -4.0339494 -4.0404224][-4.1180353 -4.1225 -4.1198263 -4.10748 -4.098145 -4.1019268 -4.1087904 -4.1021171 -4.1058769 -4.1052194 -4.0854669 -4.0655422 -4.0509634 -4.05145 -4.0654759][-4.1775713 -4.16417 -4.147356 -4.1323128 -4.1273508 -4.1337032 -4.1405482 -4.1396737 -4.1474218 -4.1523042 -4.1425238 -4.1302695 -4.1206856 -4.1212678 -4.1362524][-4.2502918 -4.2361107 -4.2185216 -4.2103109 -4.2130737 -4.220614 -4.22694 -4.2283196 -4.2335362 -4.237639 -4.2346239 -4.2286863 -4.2234273 -4.2251081 -4.2366762]]...]
INFO - root - 2017-12-07 15:49:41.976429: step 26110, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 57h:24m:40s remains)
INFO - root - 2017-12-07 15:49:48.758040: step 26120, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 56h:53m:12s remains)
INFO - root - 2017-12-07 15:49:55.498405: step 26130, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 56h:50m:57s remains)
INFO - root - 2017-12-07 15:50:02.172175: step 26140, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 53h:57m:17s remains)
INFO - root - 2017-12-07 15:50:08.933030: step 26150, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 54h:08m:21s remains)
INFO - root - 2017-12-07 15:50:15.688302: step 26160, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 58h:17m:57s remains)
INFO - root - 2017-12-07 15:50:22.410432: step 26170, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 61h:33m:08s remains)
INFO - root - 2017-12-07 15:50:29.093105: step 26180, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 56h:26m:04s remains)
INFO - root - 2017-12-07 15:50:35.645451: step 26190, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 55h:49m:32s remains)
INFO - root - 2017-12-07 15:50:42.433435: step 26200, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 55h:12m:34s remains)
2017-12-07 15:50:43.184092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3504324 -4.3469191 -4.3348465 -4.3223085 -4.3135004 -4.3118529 -4.3175063 -4.3236494 -4.3245015 -4.3221955 -4.3163424 -4.3142366 -4.3163595 -4.3241396 -4.3348684][-4.3510652 -4.3441124 -4.324326 -4.3016524 -4.2840352 -4.2781515 -4.2828994 -4.29369 -4.2988296 -4.2992454 -4.2906523 -4.2876067 -4.2920079 -4.3038459 -4.3206367][-4.3480377 -4.3375492 -4.3115644 -4.2811151 -4.2547455 -4.2409124 -4.2413425 -4.2521982 -4.2613249 -4.2656784 -4.2557964 -4.2535295 -4.2618108 -4.2766762 -4.2966514][-4.3366957 -4.3205619 -4.2890797 -4.2517252 -4.2158265 -4.1914759 -4.1832871 -4.1852174 -4.1990857 -4.2174993 -4.2134533 -4.2136345 -4.225141 -4.2386823 -4.2600389][-4.3208084 -4.298069 -4.2602005 -4.2152262 -4.1662092 -4.1201286 -4.0909243 -4.0746274 -4.0970106 -4.1449027 -4.1597843 -4.1686659 -4.1819077 -4.1913719 -4.2131419][-4.3079615 -4.2791195 -4.2360234 -4.1864457 -4.1245317 -4.0456328 -3.9747477 -3.9202232 -3.9455204 -4.0367022 -4.0860534 -4.1138277 -4.1344547 -4.1431727 -4.1661205][-4.2975783 -4.264255 -4.2171464 -4.1624293 -4.0904369 -3.9872832 -3.8721497 -3.761898 -3.7690907 -3.9062169 -4.0028839 -4.0601544 -4.0936456 -4.1080909 -4.1327019][-4.2928066 -4.2602496 -4.2142696 -4.1627889 -4.0993519 -4.0067768 -3.8909006 -3.7662339 -3.7477195 -3.877902 -3.9855964 -4.0554667 -4.0929279 -4.1091695 -4.1318879][-4.2993588 -4.274426 -4.2400227 -4.2046766 -4.1593661 -4.097549 -4.0174155 -3.9275198 -3.8978224 -3.973352 -4.0464845 -4.0982013 -4.1328573 -4.1494265 -4.1660032][-4.3094811 -4.2937055 -4.2705183 -4.2507591 -4.2222757 -4.1811676 -4.1289911 -4.0701504 -4.0437822 -4.080236 -4.120873 -4.1545296 -4.1859756 -4.2012091 -4.2120996][-4.3187389 -4.3095765 -4.2955322 -4.2850142 -4.2696733 -4.2460017 -4.2148962 -4.1798882 -4.1590881 -4.1725297 -4.1906276 -4.2113414 -4.2348666 -4.2455244 -4.2519178][-4.3284197 -4.3242936 -4.3163266 -4.3101645 -4.3018174 -4.2911873 -4.2773552 -4.2594562 -4.2444229 -4.2454791 -4.2504468 -4.2608662 -4.2753072 -4.2805657 -4.2848129][-4.3346438 -4.3329825 -4.3280544 -4.3249431 -4.3215084 -4.3182864 -4.3138461 -4.3058395 -4.2979927 -4.2970939 -4.2982121 -4.3016334 -4.3076639 -4.3095217 -4.3124275][-4.3378291 -4.3366117 -4.3329663 -4.33018 -4.3285718 -4.32718 -4.3261113 -4.3240333 -4.3218637 -4.322576 -4.3231082 -4.3247662 -4.3272 -4.3280444 -4.331161][-4.3418312 -4.340097 -4.336894 -4.3353038 -4.3342924 -4.3334413 -4.3323178 -4.3314981 -4.3315668 -4.3334026 -4.3355851 -4.3379812 -4.3396239 -4.34053 -4.3430619]]...]
INFO - root - 2017-12-07 15:50:49.940413: step 26210, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 62h:02m:07s remains)
INFO - root - 2017-12-07 15:50:56.653342: step 26220, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 56h:11m:48s remains)
INFO - root - 2017-12-07 15:51:03.474261: step 26230, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 54h:58m:02s remains)
INFO - root - 2017-12-07 15:51:10.185098: step 26240, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 56h:27m:07s remains)
INFO - root - 2017-12-07 15:51:17.073883: step 26250, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.751 sec/batch; 63h:53m:38s remains)
INFO - root - 2017-12-07 15:51:23.882893: step 26260, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 56h:09m:27s remains)
INFO - root - 2017-12-07 15:51:30.675121: step 26270, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 54h:27m:33s remains)
INFO - root - 2017-12-07 15:51:37.326705: step 26280, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 60h:38m:48s remains)
INFO - root - 2017-12-07 15:51:43.950858: step 26290, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 58h:07m:12s remains)
INFO - root - 2017-12-07 15:51:50.737151: step 26300, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 58h:36m:30s remains)
2017-12-07 15:51:51.510358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1779871 -4.160048 -4.1527 -4.1615987 -4.2007322 -4.2441292 -4.2399821 -4.2067628 -4.1960206 -4.1936 -4.1692271 -4.1546035 -4.1402149 -4.1332273 -4.1561737][-4.15942 -4.1548743 -4.1511507 -4.1605959 -4.1955447 -4.2322736 -4.2317204 -4.2062192 -4.1945105 -4.1799693 -4.1376586 -4.1024661 -4.0754356 -4.0708957 -4.1068559][-4.1574354 -4.1667147 -4.1674838 -4.1803932 -4.2125826 -4.2374225 -4.2288761 -4.1998353 -4.1838508 -4.1700954 -4.1267881 -4.0742583 -4.0347762 -4.0272017 -4.0668058][-4.159143 -4.1754208 -4.1782956 -4.1894298 -4.216269 -4.2313318 -4.2133451 -4.1807513 -4.1693568 -4.1735806 -4.1469178 -4.096416 -4.0528173 -4.0447702 -4.0858][-4.1776342 -4.1843963 -4.1778989 -4.1763358 -4.1939316 -4.2057829 -4.1886077 -4.1544409 -4.1511693 -4.1690755 -4.1638179 -4.1368151 -4.1131649 -4.1149235 -4.152266][-4.2101474 -4.2001286 -4.178915 -4.1610293 -4.1619134 -4.1580257 -4.1220951 -4.0701485 -4.0664344 -4.1153398 -4.1458387 -4.1549358 -4.1671371 -4.1839385 -4.2050633][-4.2571616 -4.2323127 -4.1965265 -4.1606364 -4.126235 -4.0797563 -3.9943411 -3.8890464 -3.8682611 -3.9739072 -4.0625005 -4.1145482 -4.1533756 -4.1829653 -4.1983209][-4.296886 -4.2695966 -4.225574 -4.1733608 -4.1047091 -4.0149264 -3.878912 -3.701961 -3.6400054 -3.8103218 -3.9622681 -4.0488787 -4.0998082 -4.1362371 -4.1590018][-4.3141804 -4.28727 -4.2429619 -4.1872549 -4.1125903 -4.0191154 -3.8911123 -3.725265 -3.6517162 -3.807014 -3.9529657 -4.0329404 -4.072618 -4.0993166 -4.1238465][-4.318294 -4.2909865 -4.2491322 -4.2041044 -4.1468649 -4.0843477 -4.011312 -3.9209797 -3.8822331 -3.9572015 -4.0307364 -4.065908 -4.0794411 -4.09136 -4.1109929][-4.3013306 -4.2812376 -4.2475648 -4.216135 -4.1804819 -4.1494093 -4.1237803 -4.0864968 -4.0700068 -4.094625 -4.1149693 -4.1176658 -4.1111507 -4.1142583 -4.1265817][-4.2674751 -4.2589889 -4.2376642 -4.2197881 -4.2031813 -4.1931896 -4.1904073 -4.176187 -4.1660948 -4.1702485 -4.1700835 -4.1646028 -4.15284 -4.1527548 -4.1609383][-4.2375879 -4.2398062 -4.2306938 -4.2252574 -4.2246094 -4.2243085 -4.2238846 -4.2189932 -4.2105513 -4.2040057 -4.1964736 -4.1922255 -4.1845412 -4.1859603 -4.1938806][-4.2345743 -4.2439365 -4.245326 -4.2494287 -4.2547865 -4.2569122 -4.2547245 -4.2513523 -4.2444777 -4.2341013 -4.2257843 -4.2215509 -4.2138696 -4.2123985 -4.2219763][-4.254158 -4.2681122 -4.2756166 -4.2846937 -4.2889786 -4.2901077 -4.2871823 -4.2833552 -4.2785783 -4.2694755 -4.264142 -4.2614388 -4.252511 -4.2467389 -4.253365]]...]
INFO - root - 2017-12-07 15:51:58.289175: step 26310, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 60h:25m:55s remains)
INFO - root - 2017-12-07 15:52:05.000159: step 26320, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 61h:17m:16s remains)
INFO - root - 2017-12-07 15:52:11.684326: step 26330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 56h:04m:14s remains)
INFO - root - 2017-12-07 15:52:18.368169: step 26340, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 52h:58m:29s remains)
INFO - root - 2017-12-07 15:52:25.122268: step 26350, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 55h:36m:49s remains)
INFO - root - 2017-12-07 15:52:31.924576: step 26360, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 61h:15m:19s remains)
INFO - root - 2017-12-07 15:52:38.757903: step 26370, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 58h:43m:13s remains)
INFO - root - 2017-12-07 15:52:45.542283: step 26380, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 56h:12m:51s remains)
INFO - root - 2017-12-07 15:52:52.029104: step 26390, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.621 sec/batch; 52h:50m:32s remains)
INFO - root - 2017-12-07 15:52:58.782646: step 26400, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 54h:29m:50s remains)
2017-12-07 15:52:59.545872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2029634 -4.2121572 -4.23159 -4.2500043 -4.261755 -4.2717934 -4.2799726 -4.2798181 -4.2727141 -4.2657471 -4.2629371 -4.2608104 -4.2487316 -4.2203608 -4.1916246][-4.2065215 -4.2137017 -4.2250094 -4.2348223 -4.2424974 -4.2542729 -4.2719483 -4.2860255 -4.2919073 -4.2934966 -4.2935596 -4.2844214 -4.257452 -4.2127261 -4.1693664][-4.2147393 -4.212441 -4.210289 -4.2074242 -4.2047744 -4.2095709 -4.2294211 -4.25587 -4.2797551 -4.2963452 -4.3043976 -4.29517 -4.2625132 -4.2092452 -4.1519308][-4.2249432 -4.2088079 -4.1874595 -4.1648965 -4.1432261 -4.1301703 -4.1436162 -4.181879 -4.2280421 -4.2661686 -4.2907076 -4.2925267 -4.2660761 -4.21417 -4.1503763][-4.2325644 -4.2029347 -4.1582203 -4.1079154 -4.0565224 -4.016634 -4.0204878 -4.0741529 -4.1453991 -4.207201 -4.2521129 -4.2708111 -4.2573729 -4.2148685 -4.1564317][-4.2364221 -4.1998177 -4.1391354 -4.0638404 -3.9823084 -3.9129961 -3.9050636 -3.9722972 -4.0663934 -4.1474986 -4.2059989 -4.2384367 -4.2388868 -4.2138171 -4.174016][-4.2464113 -4.2126789 -4.1522522 -4.0701623 -3.9785082 -3.8986368 -3.8829379 -3.9486463 -4.0448971 -4.1281457 -4.1867542 -4.2223496 -4.231987 -4.2229981 -4.2019348][-4.2606769 -4.2335734 -4.1846828 -4.1171756 -4.0440578 -3.9817765 -3.9676366 -4.013494 -4.0869851 -4.1523781 -4.1983547 -4.2269645 -4.23938 -4.2415023 -4.2319474][-4.2645922 -4.2391329 -4.2013717 -4.1543026 -4.10768 -4.0705256 -4.0638542 -4.093379 -4.1415281 -4.1851749 -4.2159891 -4.2373605 -4.2512441 -4.2593341 -4.2551851][-4.2633529 -4.2381034 -4.2095652 -4.1805215 -4.1567297 -4.1414833 -4.14418 -4.1635818 -4.1913471 -4.21459 -4.2283692 -4.2414346 -4.2552447 -4.265398 -4.2619009][-4.2566562 -4.2330852 -4.21352 -4.1993752 -4.1930075 -4.1963868 -4.2081084 -4.2211432 -4.2324142 -4.2370644 -4.2348514 -4.2379789 -4.2474937 -4.2555604 -4.2492113][-4.2470584 -4.2300234 -4.2219491 -4.222702 -4.2292528 -4.2400365 -4.2539778 -4.261838 -4.2605309 -4.2506094 -4.2355967 -4.2289548 -4.2311296 -4.2328019 -4.2224097][-4.2364306 -4.2315063 -4.2383823 -4.2530212 -4.2676544 -4.27684 -4.2832446 -4.2807889 -4.2669954 -4.2463346 -4.2246046 -4.2115889 -4.205308 -4.1996946 -4.185729][-4.2375426 -4.2434354 -4.2598939 -4.2791796 -4.2938032 -4.2970734 -4.2931962 -4.2804141 -4.2604938 -4.2380443 -4.2178035 -4.203361 -4.1899881 -4.1751719 -4.1552162][-4.2479172 -4.2642126 -4.285008 -4.3025537 -4.3123636 -4.3098493 -4.2988858 -4.2816815 -4.2615008 -4.2432303 -4.22922 -4.2164507 -4.1964483 -4.1719255 -4.1441379]]...]
INFO - root - 2017-12-07 15:53:06.322558: step 26410, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 56h:47m:55s remains)
INFO - root - 2017-12-07 15:53:13.155724: step 26420, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 56h:12m:35s remains)
INFO - root - 2017-12-07 15:53:19.905951: step 26430, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 54h:06m:32s remains)
INFO - root - 2017-12-07 15:53:26.705888: step 26440, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 60h:41m:27s remains)
INFO - root - 2017-12-07 15:53:33.465500: step 26450, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 55h:31m:31s remains)
INFO - root - 2017-12-07 15:53:40.261506: step 26460, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 53h:33m:43s remains)
INFO - root - 2017-12-07 15:53:47.142308: step 26470, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 57h:03m:40s remains)
INFO - root - 2017-12-07 15:53:54.109191: step 26480, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 60h:58m:14s remains)
INFO - root - 2017-12-07 15:54:00.736709: step 26490, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 61h:46m:09s remains)
INFO - root - 2017-12-07 15:54:07.611006: step 26500, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 59h:16m:24s remains)
2017-12-07 15:54:08.449892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2718778 -4.2797432 -4.2824802 -4.2667279 -4.2425537 -4.2192674 -4.2046881 -4.1877384 -4.1666775 -4.1707511 -4.1986208 -4.2335782 -4.2682114 -4.282711 -4.2679534][-4.2981219 -4.3053846 -4.2954226 -4.2627044 -4.2278738 -4.2012963 -4.1916747 -4.1847482 -4.1718569 -4.1821537 -4.2108645 -4.2453833 -4.2741041 -4.2820315 -4.2614832][-4.3125544 -4.3179173 -4.295342 -4.24835 -4.2066507 -4.1886077 -4.191339 -4.1943231 -4.186635 -4.1908197 -4.2100048 -4.2384324 -4.2639284 -4.2695251 -4.2434907][-4.31608 -4.3169866 -4.2828116 -4.2262397 -4.182004 -4.1732912 -4.1860662 -4.1935024 -4.1844964 -4.1780467 -4.1898808 -4.2155037 -4.240056 -4.2389922 -4.2031221][-4.2994061 -4.2920718 -4.2500682 -4.1953821 -4.1588588 -4.1546373 -4.1665578 -4.161737 -4.1382246 -4.1248341 -4.1409812 -4.1750312 -4.200294 -4.1917405 -4.1504865][-4.2658148 -4.2501316 -4.2103391 -4.167109 -4.1427512 -4.1346245 -4.1204896 -4.0737734 -4.0257025 -4.02238 -4.0651984 -4.1094785 -4.12942 -4.1176033 -4.0826325][-4.2313938 -4.2156963 -4.1828752 -4.1490922 -4.121851 -4.0869126 -4.0184941 -3.9132705 -3.8506625 -3.8984909 -3.98219 -4.0331197 -4.0474319 -4.0394211 -4.0263371][-4.1941848 -4.18063 -4.1551542 -4.128674 -4.093 -4.0311341 -3.9239173 -3.8005481 -3.7827005 -3.8962154 -4.0127268 -4.0769544 -4.09921 -4.1048493 -4.1128168][-4.1774306 -4.1706791 -4.1617537 -4.1517196 -4.12694 -4.080349 -4.0062728 -3.94251 -3.9649169 -4.0586905 -4.147542 -4.1967607 -4.2150793 -4.2207637 -4.2248383][-4.2078676 -4.2027297 -4.2014771 -4.2029319 -4.1901965 -4.1655984 -4.1284547 -4.1045313 -4.1253181 -4.1802654 -4.2331419 -4.2609177 -4.2717171 -4.2788582 -4.2824707][-4.235137 -4.2312336 -4.2304344 -4.2333946 -4.2265806 -4.2093778 -4.1831331 -4.1654968 -4.176816 -4.2105212 -4.2468119 -4.2708621 -4.2871423 -4.30017 -4.3034868][-4.2210903 -4.2215152 -4.2215695 -4.2241082 -4.217104 -4.2004461 -4.1739826 -4.1579432 -4.1656132 -4.1913953 -4.2203245 -4.2434092 -4.2676663 -4.2913284 -4.29956][-4.2100863 -4.20575 -4.1995368 -4.1985927 -4.191134 -4.1733994 -4.1486921 -4.140594 -4.1537313 -4.1777449 -4.2040277 -4.2254586 -4.2507768 -4.2755723 -4.28501][-4.2212548 -4.2072349 -4.1877708 -4.175776 -4.1669159 -4.1528082 -4.1351137 -4.13478 -4.1523871 -4.1761656 -4.2004256 -4.2195921 -4.2412386 -4.2605672 -4.267715][-4.2402 -4.2176862 -4.1914325 -4.1777344 -4.1745815 -4.1689663 -4.158287 -4.1586356 -4.1704044 -4.1853132 -4.19782 -4.2066021 -4.2182817 -4.2288232 -4.23166]]...]
INFO - root - 2017-12-07 15:54:15.349506: step 26510, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 62h:54m:36s remains)
INFO - root - 2017-12-07 15:54:22.101162: step 26520, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 52h:57m:04s remains)
INFO - root - 2017-12-07 15:54:28.845947: step 26530, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 55h:14m:55s remains)
INFO - root - 2017-12-07 15:54:35.671897: step 26540, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.669 sec/batch; 56h:54m:00s remains)
INFO - root - 2017-12-07 15:54:42.586282: step 26550, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 61h:09m:50s remains)
INFO - root - 2017-12-07 15:54:49.384548: step 26560, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 59h:12m:29s remains)
INFO - root - 2017-12-07 15:54:56.241844: step 26570, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 54h:17m:10s remains)
INFO - root - 2017-12-07 15:55:02.961710: step 26580, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 54h:07m:26s remains)
INFO - root - 2017-12-07 15:55:09.442306: step 26590, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 62h:58m:04s remains)
INFO - root - 2017-12-07 15:55:16.286352: step 26600, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 57h:59m:12s remains)
2017-12-07 15:55:17.019978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1983147 -4.191875 -4.1920319 -4.2035394 -4.2200074 -4.2349777 -4.246479 -4.2566957 -4.2651944 -4.2643538 -4.2630324 -4.2598176 -4.257483 -4.2613497 -4.2676649][-4.1698503 -4.1603851 -4.1550517 -4.1669073 -4.19283 -4.2152433 -4.2291083 -4.2371664 -4.2420406 -4.2346172 -4.2287979 -4.2217517 -4.2171493 -4.2221479 -4.2286339][-4.1324692 -4.1221256 -4.1191258 -4.1378441 -4.17569 -4.2079406 -4.2262936 -4.2314095 -4.2310715 -4.2172422 -4.2051759 -4.1936045 -4.1881633 -4.1923456 -4.1956277][-4.0947113 -4.0930824 -4.101861 -4.1272879 -4.1722922 -4.2076807 -4.2261896 -4.2286482 -4.2245779 -4.2094336 -4.19534 -4.1819506 -4.176774 -4.1752515 -4.1694865][-4.0781345 -4.0861969 -4.1020117 -4.1254897 -4.1636744 -4.1957393 -4.2165556 -4.2235985 -4.2213478 -4.2102451 -4.1980724 -4.1842732 -4.1759162 -4.1699729 -4.1565561][-4.089941 -4.0992165 -4.10877 -4.1203418 -4.1449904 -4.1683927 -4.1839476 -4.1893058 -4.191227 -4.1862206 -4.1761889 -4.1639853 -4.1562815 -4.1487665 -4.1341743][-4.1069603 -4.1110816 -4.1098218 -4.1040454 -4.1067314 -4.1102805 -4.110498 -4.107708 -4.1141243 -4.1187196 -4.1145587 -4.1078959 -4.1074991 -4.1073952 -4.0983858][-4.1140285 -4.11674 -4.115427 -4.0990281 -4.0829563 -4.066977 -4.0501895 -4.0372872 -4.0418134 -4.0498624 -4.0500097 -4.052947 -4.0659475 -4.07835 -4.0817695][-4.1207924 -4.1252818 -4.1306682 -4.1207604 -4.1037807 -4.0792527 -4.0522747 -4.0361514 -4.0381908 -4.046176 -4.0485697 -4.0593548 -4.0783362 -4.0910954 -4.0974836][-4.1228652 -4.1277289 -4.1364212 -4.1384983 -4.1325331 -4.1132269 -4.0904784 -4.0802865 -4.0849986 -4.0945144 -4.1019974 -4.1160245 -4.1316104 -4.1353507 -4.1338568][-4.1198435 -4.1289253 -4.1416955 -4.1528287 -4.1577282 -4.1445169 -4.1257329 -4.1167645 -4.1176596 -4.1243873 -4.1327572 -4.1489863 -4.1649666 -4.1655483 -4.1598463][-4.1027594 -4.1223855 -4.1452794 -4.1683207 -4.1811023 -4.1688194 -4.1505742 -4.1396837 -4.1337276 -4.1357594 -4.1439352 -4.1621385 -4.17934 -4.1786838 -4.1693258][-4.0831909 -4.1081624 -4.1350651 -4.1637812 -4.1812649 -4.1736374 -4.1623497 -4.1540065 -4.1437798 -4.1397319 -4.1445045 -4.1624861 -4.1817603 -4.1830707 -4.1744413][-4.072288 -4.0990152 -4.1271605 -4.1543083 -4.1707873 -4.173543 -4.1766033 -4.1759434 -4.1677942 -4.161016 -4.1593938 -4.1695838 -4.1831155 -4.1826081 -4.1739383][-4.0828762 -4.1085024 -4.1339498 -4.1521926 -4.1620684 -4.1698227 -4.1809206 -4.1880631 -4.1884174 -4.1858888 -4.1816392 -4.180512 -4.1806569 -4.1751928 -4.1667666]]...]
INFO - root - 2017-12-07 15:55:23.970451: step 26610, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.755 sec/batch; 64h:07m:59s remains)
INFO - root - 2017-12-07 15:55:30.906459: step 26620, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 62h:53m:08s remains)
INFO - root - 2017-12-07 15:55:37.668104: step 26630, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 58h:49m:16s remains)
INFO - root - 2017-12-07 15:55:44.496344: step 26640, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 52h:48m:37s remains)
INFO - root - 2017-12-07 15:55:51.288241: step 26650, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 54h:41m:29s remains)
INFO - root - 2017-12-07 15:55:58.131734: step 26660, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 61h:32m:04s remains)
INFO - root - 2017-12-07 15:56:04.936499: step 26670, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.743 sec/batch; 63h:09m:10s remains)
INFO - root - 2017-12-07 15:56:11.701060: step 26680, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 55h:52m:31s remains)
INFO - root - 2017-12-07 15:56:18.364122: step 26690, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.643 sec/batch; 54h:34m:45s remains)
INFO - root - 2017-12-07 15:56:25.096603: step 26700, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 55h:22m:16s remains)
2017-12-07 15:56:25.831810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29867 -4.3007622 -4.300343 -4.2957573 -4.2886729 -4.289988 -4.2930269 -4.2882767 -4.2850208 -4.2823777 -4.2772417 -4.274775 -4.2755923 -4.269618 -4.2572303][-4.2843127 -4.2877007 -4.2866359 -4.2804856 -4.26934 -4.2691383 -4.27094 -4.260921 -4.2493863 -4.2399406 -4.2340198 -4.2373285 -4.2412443 -4.2332706 -4.2202482][-4.2662873 -4.2734938 -4.2721844 -4.2670226 -4.2558994 -4.250669 -4.2460217 -4.2254152 -4.2004981 -4.1834521 -4.1829638 -4.1945925 -4.2004986 -4.1934648 -4.1848397][-4.2482677 -4.2579274 -4.2524986 -4.2455978 -4.2343712 -4.2217236 -4.2094555 -4.1797209 -4.1432614 -4.1250496 -4.133954 -4.15514 -4.1626058 -4.1592045 -4.1628213][-4.2394838 -4.2457657 -4.2332397 -4.2197766 -4.2033339 -4.1800232 -4.1545949 -4.1141291 -4.0728889 -4.0680642 -4.1018233 -4.1405997 -4.1484275 -4.1425095 -4.1524725][-4.2265921 -4.2278471 -4.2095447 -4.1901522 -4.1679087 -4.1330061 -4.0889859 -4.0291142 -3.9818912 -4.0065556 -4.0795832 -4.1408634 -4.1544065 -4.1464567 -4.1559663][-4.1992083 -4.2015753 -4.181344 -4.1572809 -4.1334033 -4.0883646 -4.0180163 -3.9190717 -3.8555923 -3.9283092 -4.0498013 -4.1354485 -4.1657128 -4.1633024 -4.1671805][-4.1733136 -4.1733422 -4.152154 -4.1300926 -4.1136546 -4.0672989 -3.9777393 -3.8448603 -3.7693987 -3.8855705 -4.038126 -4.1366458 -4.1763926 -4.1776147 -4.1721082][-4.1648178 -4.1612315 -4.1385851 -4.1238241 -4.1209173 -4.0824571 -3.9996865 -3.8805032 -3.8265843 -3.9392009 -4.0723109 -4.1563158 -4.19036 -4.1859732 -4.1673808][-4.1772361 -4.1761608 -4.15332 -4.1407051 -4.1420856 -4.1121612 -4.0506659 -3.9726138 -3.9512055 -4.0368271 -4.1314511 -4.1904535 -4.2100568 -4.1953788 -4.1676335][-4.1983709 -4.2022185 -4.1862454 -4.1737804 -4.1776576 -4.1574078 -4.1109328 -4.0579047 -4.0542831 -4.1200166 -4.1871281 -4.2288804 -4.2413254 -4.2207942 -4.1903033][-4.2249689 -4.2292228 -4.220489 -4.2126122 -4.2205987 -4.2105956 -4.1761451 -4.1371164 -4.1378112 -4.1898727 -4.2352285 -4.2658105 -4.2766614 -4.2585077 -4.2265115][-4.2468076 -4.25669 -4.2563519 -4.2564092 -4.2634554 -4.2582803 -4.233398 -4.2081151 -4.2111468 -4.2488632 -4.276257 -4.295433 -4.3030109 -4.2881413 -4.2571425][-4.2705755 -4.2876883 -4.2920575 -4.294147 -4.2974253 -4.2880783 -4.2698121 -4.2581158 -4.2658095 -4.2910781 -4.3026638 -4.3081818 -4.3089933 -4.2930956 -4.267364][-4.2863274 -4.3052988 -4.3117723 -4.3129687 -4.3100076 -4.2960463 -4.2809186 -4.2750854 -4.2836218 -4.2986274 -4.3031478 -4.302753 -4.2990732 -4.2857246 -4.2688222]]...]
INFO - root - 2017-12-07 15:56:32.585660: step 26710, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 57h:03m:57s remains)
INFO - root - 2017-12-07 15:56:39.291476: step 26720, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 55h:29m:55s remains)
INFO - root - 2017-12-07 15:56:46.129429: step 26730, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 55h:38m:24s remains)
INFO - root - 2017-12-07 15:56:52.959772: step 26740, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 61h:12m:17s remains)
INFO - root - 2017-12-07 15:56:59.783442: step 26750, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 60h:09m:20s remains)
INFO - root - 2017-12-07 15:57:06.548086: step 26760, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.690 sec/batch; 58h:37m:40s remains)
INFO - root - 2017-12-07 15:57:13.228018: step 26770, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 52h:54m:41s remains)
INFO - root - 2017-12-07 15:57:20.005807: step 26780, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 55h:21m:33s remains)
INFO - root - 2017-12-07 15:57:26.717420: step 26790, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 61h:25m:36s remains)
INFO - root - 2017-12-07 15:57:33.644389: step 26800, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 59h:18m:47s remains)
2017-12-07 15:57:34.290796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3110671 -4.2990189 -4.2780666 -4.2575192 -4.2422094 -4.2252831 -4.1999989 -4.1769333 -4.1705394 -4.193 -4.2178183 -4.2286777 -4.2139854 -4.1896868 -4.1480503][-4.3072953 -4.2958169 -4.278285 -4.2592278 -4.2433391 -4.2225327 -4.1871023 -4.1557207 -4.1477566 -4.1723723 -4.1973114 -4.2059889 -4.1827049 -4.1499815 -4.1071534][-4.3035378 -4.2921238 -4.2747731 -4.2528749 -4.2358079 -4.2119722 -4.1672869 -4.1317825 -4.126986 -4.1525464 -4.1760573 -4.1837993 -4.1583066 -4.1238708 -4.0857792][-4.2994442 -4.286922 -4.2674222 -4.2413945 -4.2231526 -4.1993561 -4.1503043 -4.1118474 -4.106935 -4.125205 -4.14321 -4.1547146 -4.1379271 -4.1122112 -4.0854692][-4.2962503 -4.28205 -4.2610469 -4.230866 -4.2099948 -4.1827378 -4.12745 -4.0797415 -4.0684547 -4.0783491 -4.0926342 -4.1117849 -4.1101971 -4.09975 -4.0872936][-4.29341 -4.2779441 -4.2544541 -4.2178845 -4.1914721 -4.1568961 -4.0922918 -4.0234585 -4.0030384 -4.0185046 -4.0414271 -4.0753946 -4.0883083 -4.0875559 -4.08527][-4.2910132 -4.2731137 -4.2457256 -4.2041092 -4.1700034 -4.1251822 -4.0412545 -3.931849 -3.8956413 -3.9392056 -3.9979253 -4.0594959 -4.0904312 -4.0963216 -4.0967216][-4.2887764 -4.2682319 -4.2388935 -4.1947751 -4.1522465 -4.0901546 -3.9686775 -3.8001671 -3.7508683 -3.8576918 -3.975723 -4.0654645 -4.1116004 -4.1240797 -4.1239614][-4.28576 -4.2632709 -4.2318487 -4.1849108 -4.1346159 -4.0570192 -3.9107873 -3.7173686 -3.6820416 -3.8406167 -3.9888761 -4.0867448 -4.137567 -4.1572957 -4.1593609][-4.2832208 -4.259048 -4.2249551 -4.1770644 -4.12698 -4.05184 -3.9287603 -3.7933865 -3.7895432 -3.926892 -4.055254 -4.1389546 -4.1793447 -4.1979947 -4.2007341][-4.2785611 -4.2542353 -4.2212439 -4.1772356 -4.1373911 -4.0770278 -3.9930279 -3.9189911 -3.9290929 -4.0312891 -4.1261625 -4.1856136 -4.2101955 -4.2220831 -4.2258759][-4.2729793 -4.2497387 -4.2238245 -4.19269 -4.16638 -4.1227722 -4.0665436 -4.02383 -4.0390677 -4.1163344 -4.1800385 -4.2150989 -4.2331491 -4.2448916 -4.2496829][-4.2715325 -4.2507267 -4.2340455 -4.2166843 -4.201664 -4.1720214 -4.1301136 -4.100028 -4.1177506 -4.1764359 -4.217546 -4.2376971 -4.2510524 -4.2587357 -4.2585049][-4.2719746 -4.25308 -4.2428861 -4.2339916 -4.2281175 -4.2080412 -4.1740518 -4.1446295 -4.1580229 -4.1990809 -4.2240505 -4.2372842 -4.2461929 -4.2475696 -4.24328][-4.2735991 -4.2550821 -4.2482328 -4.240468 -4.23557 -4.2148323 -4.1809149 -4.14469 -4.1484852 -4.1792397 -4.1956344 -4.2075529 -4.2161927 -4.2164187 -4.21273]]...]
INFO - root - 2017-12-07 15:57:41.036786: step 26810, loss = 2.03, batch loss = 1.98 (12.1 examples/sec; 0.658 sec/batch; 55h:54m:46s remains)
INFO - root - 2017-12-07 15:57:47.974709: step 26820, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.734 sec/batch; 62h:17m:11s remains)
INFO - root - 2017-12-07 15:57:54.725758: step 26830, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 62h:42m:02s remains)
INFO - root - 2017-12-07 15:58:01.637790: step 26840, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 58h:25m:01s remains)
INFO - root - 2017-12-07 15:58:08.388718: step 26850, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 53h:59m:53s remains)
INFO - root - 2017-12-07 15:58:15.154862: step 26860, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 53h:43m:52s remains)
INFO - root - 2017-12-07 15:58:21.990246: step 26870, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 59h:47m:15s remains)
INFO - root - 2017-12-07 15:58:28.866897: step 26880, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.718 sec/batch; 60h:57m:25s remains)
INFO - root - 2017-12-07 15:58:35.528229: step 26890, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 58h:08m:23s remains)
INFO - root - 2017-12-07 15:58:42.115820: step 26900, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 56h:51m:22s remains)
2017-12-07 15:58:42.883318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3079348 -4.3082061 -4.3081088 -4.3076253 -4.3070827 -4.3063493 -4.304811 -4.3029318 -4.3015733 -4.3008728 -4.3000646 -4.2985678 -4.2968593 -4.2956438 -4.2945709][-4.3363547 -4.3384123 -4.33866 -4.3379946 -4.336792 -4.33546 -4.3337445 -4.3318763 -4.331203 -4.3318911 -4.3330669 -4.334044 -4.334609 -4.3346806 -4.3341942][-4.3405509 -4.342227 -4.3413224 -4.3381619 -4.3345008 -4.3319798 -4.3298521 -4.3283086 -4.3289533 -4.3313856 -4.3347211 -4.3382392 -4.3410482 -4.3422313 -4.3419189][-4.3351092 -4.3360591 -4.3321733 -4.32346 -4.3151 -4.3099031 -4.3070345 -4.3062754 -4.3094954 -4.3158674 -4.3227644 -4.328629 -4.3339348 -4.3366561 -4.336339][-4.30908 -4.3113704 -4.305233 -4.2908483 -4.2762179 -4.2656932 -4.2604532 -4.2608342 -4.2682071 -4.2792182 -4.2897167 -4.2987766 -4.3072691 -4.31335 -4.3145614][-4.2557759 -4.2598271 -4.2513862 -4.2312036 -4.2066531 -4.1844707 -4.1718307 -4.1726179 -4.1891637 -4.2115059 -4.2298746 -4.2437377 -4.2552795 -4.2639332 -4.2670913][-4.1675787 -4.1748557 -4.1633606 -4.1342039 -4.0942593 -4.0519347 -4.0220232 -4.0193553 -4.0493188 -4.090889 -4.1252651 -4.1511483 -4.17072 -4.1859579 -4.1934919][-4.0833917 -4.0937796 -4.0802231 -4.0438833 -3.987998 -3.9226975 -3.8684688 -3.8583117 -3.9023523 -3.9609606 -4.0077944 -4.0425806 -4.0687962 -4.0900874 -4.1029577][-4.081789 -4.0936561 -4.0850916 -4.0544329 -4.0005894 -3.9335811 -3.8751841 -3.8623977 -3.9028325 -3.9549415 -3.9928224 -4.0161357 -4.0307784 -4.0424609 -4.0542336][-4.1478477 -4.1613913 -4.1637082 -4.1490674 -4.11687 -4.0733442 -4.0355096 -4.0283337 -4.051693 -4.0790014 -4.0932655 -4.0930929 -4.0853057 -4.0794835 -4.0838451][-4.2135773 -4.2233672 -4.2306209 -4.2300658 -4.220161 -4.2025681 -4.1853576 -4.182569 -4.1918783 -4.1998525 -4.1958084 -4.1779504 -4.1544471 -4.1383023 -4.1380892][-4.2526436 -4.2540879 -4.2581596 -4.2628822 -4.265511 -4.2628827 -4.2595258 -4.2617378 -4.2677417 -4.2699766 -4.2615004 -4.2415581 -4.2172394 -4.2004366 -4.1981068][-4.2656331 -4.2612696 -4.2613778 -4.26517 -4.2690487 -4.2700672 -4.2726607 -4.2778616 -4.2843852 -4.2877784 -4.2833967 -4.2689872 -4.2490191 -4.2357049 -4.2328062][-4.2644844 -4.2553039 -4.2495432 -4.2478776 -4.2473135 -4.2470365 -4.2522411 -4.2594714 -4.2678351 -4.2746119 -4.2760625 -4.2698641 -4.2590108 -4.2517152 -4.2508197][-4.2696085 -4.2566104 -4.2430787 -4.2310543 -4.2222176 -4.2164059 -4.2188005 -4.2233305 -4.2291064 -4.2357693 -4.2430825 -4.2501297 -4.2566595 -4.2632804 -4.2699089]]...]
INFO - root - 2017-12-07 15:58:49.663707: step 26910, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 58h:23m:23s remains)
INFO - root - 2017-12-07 15:58:56.427988: step 26920, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.613 sec/batch; 52h:03m:59s remains)
INFO - root - 2017-12-07 15:59:03.318541: step 26930, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 55h:41m:31s remains)
INFO - root - 2017-12-07 15:59:10.137101: step 26940, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 61h:30m:31s remains)
INFO - root - 2017-12-07 15:59:16.979983: step 26950, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 63h:14m:08s remains)
INFO - root - 2017-12-07 15:59:23.854349: step 26960, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 58h:31m:36s remains)
INFO - root - 2017-12-07 15:59:30.588762: step 26970, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 53h:07m:14s remains)
INFO - root - 2017-12-07 15:59:37.562745: step 26980, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 58h:40m:36s remains)
INFO - root - 2017-12-07 15:59:44.218130: step 26990, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.750 sec/batch; 63h:39m:49s remains)
INFO - root - 2017-12-07 15:59:50.976464: step 27000, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 57h:39m:44s remains)
2017-12-07 15:59:51.810775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1370049 -4.1431508 -4.1508083 -4.1518421 -4.1511354 -4.1448402 -4.1371889 -4.137517 -4.1637421 -4.193841 -4.2097373 -4.227 -4.2442265 -4.2637997 -4.2870927][-4.1551833 -4.1581674 -4.1534033 -4.1425385 -4.1335816 -4.1194234 -4.1072168 -4.1029387 -4.1237597 -4.1512146 -4.166903 -4.18685 -4.210012 -4.2393723 -4.2701225][-4.1617079 -4.16657 -4.1562557 -4.1382685 -4.1242089 -4.1055636 -4.0914135 -4.0842514 -4.0982985 -4.121069 -4.1408329 -4.1665111 -4.1928244 -4.2243772 -4.258966][-4.1742125 -4.1860175 -4.1785593 -4.1607366 -4.1440582 -4.1233716 -4.11088 -4.10941 -4.121614 -4.1447067 -4.1686316 -4.1959805 -4.2159939 -4.2384586 -4.2657728][-4.1733251 -4.1921659 -4.1914091 -4.17716 -4.1575928 -4.1321158 -4.1157041 -4.1217022 -4.1430979 -4.1719632 -4.1999073 -4.2280917 -4.2459149 -4.2596135 -4.2753868][-4.1575108 -4.1777587 -4.1758184 -4.1554017 -4.1261497 -4.0884356 -4.0621896 -4.0731187 -4.1114211 -4.1543441 -4.1875134 -4.2187586 -4.2415195 -4.2555008 -4.2690659][-4.127203 -4.14323 -4.12981 -4.0894432 -4.0392303 -3.9794598 -3.9302907 -3.9386222 -4.0021071 -4.0717263 -4.1211257 -4.1676083 -4.2049828 -4.22888 -4.249413][-4.0893679 -4.0849419 -4.047574 -3.9787698 -3.9015775 -3.8117652 -3.7271242 -3.7340829 -3.8361819 -3.9472375 -4.0278482 -4.1018486 -4.1594143 -4.1978559 -4.2304573][-4.0800986 -4.0631742 -4.0138979 -3.9343638 -3.8496377 -3.755053 -3.6593804 -3.6660347 -3.7807269 -3.9019148 -3.9918129 -4.076436 -4.1431608 -4.1876969 -4.2275987][-4.1047106 -4.0873241 -4.046752 -3.9875677 -3.9320776 -3.8721075 -3.811378 -3.8169594 -3.8937263 -3.9772568 -4.0445952 -4.1144767 -4.1704955 -4.2089729 -4.2448921][-4.1638412 -4.1433678 -4.1075053 -4.0710478 -4.042583 -4.0105014 -3.9808245 -3.9902618 -4.0350227 -4.0831046 -4.1271868 -4.1774468 -4.2157049 -4.2417831 -4.2690187][-4.2380447 -4.2191567 -4.1907063 -4.1677127 -4.1514945 -4.13276 -4.1179647 -4.1266389 -4.1512723 -4.1769953 -4.2034836 -4.2345276 -4.2567134 -4.2690349 -4.2862806][-4.2918072 -4.2802992 -4.2624679 -4.2489119 -4.2376266 -4.22477 -4.2168231 -4.2231736 -4.2348442 -4.245894 -4.2580662 -4.2723556 -4.2802176 -4.2827263 -4.2937322][-4.3151689 -4.3092089 -4.2992439 -4.2925596 -4.286067 -4.2779479 -4.2746797 -4.2815156 -4.28779 -4.2880468 -4.2873735 -4.2867 -4.2854714 -4.284277 -4.2924175][-4.3205156 -4.3173885 -4.3109446 -4.3082047 -4.3055997 -4.2998309 -4.2980547 -4.3059196 -4.3099341 -4.30529 -4.2958169 -4.2850151 -4.279026 -4.2781367 -4.2863092]]...]
INFO - root - 2017-12-07 15:59:58.656994: step 27010, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 59h:52m:46s remains)
INFO - root - 2017-12-07 16:00:05.395549: step 27020, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 59h:13m:49s remains)
INFO - root - 2017-12-07 16:00:12.203387: step 27030, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 55h:21m:09s remains)
INFO - root - 2017-12-07 16:00:18.960306: step 27040, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 55h:29m:08s remains)
INFO - root - 2017-12-07 16:00:25.823433: step 27050, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.724 sec/batch; 61h:27m:46s remains)
INFO - root - 2017-12-07 16:00:32.611602: step 27060, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 62h:13m:04s remains)
INFO - root - 2017-12-07 16:00:39.535673: step 27070, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 59h:49m:10s remains)
INFO - root - 2017-12-07 16:00:46.309997: step 27080, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 54h:54m:27s remains)
INFO - root - 2017-12-07 16:00:52.951586: step 27090, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.680 sec/batch; 57h:38m:53s remains)
INFO - root - 2017-12-07 16:00:59.757391: step 27100, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 61h:19m:54s remains)
2017-12-07 16:01:00.458402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3016238 -4.3009539 -4.2988229 -4.2951155 -4.291851 -4.2909541 -4.2941418 -4.2981629 -4.3004651 -4.3007369 -4.3011069 -4.3028126 -4.3046093 -4.3042436 -4.3008761][-4.2846823 -4.2843528 -4.2831182 -4.2775879 -4.2710414 -4.2681365 -4.2728562 -4.2794819 -4.2843328 -4.2861638 -4.287571 -4.2912264 -4.2965631 -4.2972012 -4.2906666][-4.2711115 -4.2724977 -4.2711291 -4.2617493 -4.2475219 -4.2398119 -4.2453732 -4.2547412 -4.2607403 -4.2623386 -4.2647991 -4.2722821 -4.2836542 -4.2888093 -4.2838268][-4.2703958 -4.2712131 -4.2680931 -4.2542009 -4.2309456 -4.2153678 -4.21925 -4.2297473 -4.2357287 -4.2366695 -4.2383122 -4.24993 -4.2674737 -4.2810087 -4.2836442][-4.2722516 -4.2690144 -4.2643051 -4.2489519 -4.2204328 -4.1975479 -4.1968055 -4.2072558 -4.2139454 -4.2143893 -4.2129903 -4.2268925 -4.2511964 -4.27471 -4.2877674][-4.2718687 -4.2637863 -4.2578235 -4.2406111 -4.2096648 -4.1818452 -4.1731119 -4.1812592 -4.1890116 -4.189795 -4.1872673 -4.2040548 -4.2347217 -4.26457 -4.2856364][-4.2724504 -4.2610087 -4.2537723 -4.2345443 -4.2030125 -4.1734533 -4.1580386 -4.1639004 -4.1749744 -4.1749458 -4.1700735 -4.1888795 -4.2238364 -4.2557445 -4.2798557][-4.2691274 -4.25639 -4.2496266 -4.233561 -4.2039361 -4.1744623 -4.1569219 -4.1641207 -4.1830988 -4.1851387 -4.1737623 -4.185895 -4.2188883 -4.2491069 -4.2705908][-4.2551169 -4.2417307 -4.2389417 -4.2304378 -4.210454 -4.1874738 -4.1774664 -4.1888113 -4.2115068 -4.2180271 -4.2041788 -4.2050805 -4.2243891 -4.2461939 -4.2621274][-4.2477126 -4.2341404 -4.233902 -4.2299681 -4.2194343 -4.2089033 -4.2085047 -4.2196631 -4.2396288 -4.2501159 -4.2436466 -4.2381859 -4.2428856 -4.2512321 -4.2602277][-4.2485847 -4.2337523 -4.231461 -4.2267556 -4.218935 -4.2166281 -4.2217979 -4.2324171 -4.2483883 -4.2597694 -4.2640629 -4.2579865 -4.2542577 -4.2516427 -4.2524137][-4.2569528 -4.2388053 -4.2324638 -4.2268362 -4.2196116 -4.2148452 -4.2149844 -4.2231588 -4.2362285 -4.2462864 -4.2578297 -4.2558265 -4.2500191 -4.2448993 -4.2418609][-4.2673855 -4.2497492 -4.2394643 -4.2312975 -4.221272 -4.2111526 -4.2035642 -4.2047353 -4.2103758 -4.220664 -4.2378025 -4.2420721 -4.2417045 -4.2432132 -4.2423382][-4.2642813 -4.2508826 -4.2472415 -4.2429132 -4.2329144 -4.2188253 -4.2044811 -4.1936312 -4.1874542 -4.192194 -4.2092075 -4.2215757 -4.2325449 -4.2450175 -4.24681][-4.2594538 -4.2515335 -4.25481 -4.2588925 -4.2551956 -4.2381654 -4.2142835 -4.1923504 -4.1792789 -4.1781268 -4.1867275 -4.1999178 -4.2197771 -4.2417254 -4.2479353]]...]
INFO - root - 2017-12-07 16:01:07.240818: step 27110, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.649 sec/batch; 55h:01m:29s remains)
INFO - root - 2017-12-07 16:01:14.168503: step 27120, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.717 sec/batch; 60h:48m:24s remains)
INFO - root - 2017-12-07 16:01:21.042953: step 27130, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.700 sec/batch; 59h:22m:25s remains)
INFO - root - 2017-12-07 16:01:27.887853: step 27140, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 56h:30m:21s remains)
INFO - root - 2017-12-07 16:01:34.772808: step 27150, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 56h:42m:26s remains)
INFO - root - 2017-12-07 16:01:41.801217: step 27160, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 58h:00m:13s remains)
INFO - root - 2017-12-07 16:01:48.686407: step 27170, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.770 sec/batch; 65h:20m:53s remains)
INFO - root - 2017-12-07 16:01:55.452699: step 27180, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 55h:52m:11s remains)
INFO - root - 2017-12-07 16:02:02.122786: step 27190, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 53h:55m:25s remains)
INFO - root - 2017-12-07 16:02:08.914816: step 27200, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 56h:23m:26s remains)
2017-12-07 16:02:09.638924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.27912 -4.2874608 -4.3000565 -4.3077445 -4.3109713 -4.3156848 -4.3175859 -4.3180666 -4.3180432 -4.3183885 -4.3195271 -4.3196483 -4.3181453 -4.3152618 -4.3133693][-4.2559595 -4.2709956 -4.2886982 -4.2964187 -4.295795 -4.2976165 -4.2972178 -4.2989473 -4.304409 -4.3112235 -4.3174734 -4.3203177 -4.3193326 -4.3142433 -4.3101878][-4.24543 -4.2667127 -4.2835379 -4.2820377 -4.2689934 -4.2596364 -4.2510147 -4.2549224 -4.2708678 -4.2896042 -4.3049006 -4.3117361 -4.3121285 -4.3051934 -4.3000278][-4.2501826 -4.27108 -4.2762656 -4.2571225 -4.2250938 -4.195962 -4.1701341 -4.174562 -4.2092996 -4.2497377 -4.2798939 -4.2932034 -4.2954187 -4.2892475 -4.2856731][-4.2618241 -4.2774038 -4.2659078 -4.2243652 -4.1695623 -4.11089 -4.0533781 -4.0505223 -4.110774 -4.1829567 -4.2359242 -4.26285 -4.271173 -4.2681928 -4.2680173][-4.2701054 -4.2778864 -4.2519507 -4.1947961 -4.1213527 -4.0259266 -3.9206307 -3.8984065 -3.9873691 -4.0990133 -4.1778607 -4.2200909 -4.2340012 -4.23613 -4.2396846][-4.2738285 -4.2741647 -4.2386918 -4.1749005 -4.0888705 -3.9591975 -3.8056407 -3.7609344 -3.8828328 -4.0354719 -4.1335711 -4.1816072 -4.1927934 -4.1963072 -4.203784][-4.280314 -4.2756119 -4.2386603 -4.1763935 -4.0906186 -3.9495959 -3.7811389 -3.7340736 -3.8754194 -4.039155 -4.1337214 -4.1707716 -4.1678982 -4.1628151 -4.1705308][-4.2870378 -4.2802362 -4.2474766 -4.1989875 -4.1339278 -4.0268435 -3.902441 -3.8721743 -3.9813731 -4.10364 -4.1667295 -4.1764517 -4.1490173 -4.1277685 -4.1334982][-4.284431 -4.2779579 -4.2516723 -4.2209897 -4.1839404 -4.1234322 -4.0589862 -4.045547 -4.1073003 -4.1657677 -4.1822176 -4.1579285 -4.1067948 -4.0713191 -4.0795083][-4.2744741 -4.2712297 -4.2500486 -4.2278533 -4.2080812 -4.1805816 -4.1562986 -4.1543274 -4.1822777 -4.1898642 -4.1616511 -4.1061029 -4.0350337 -3.992537 -4.0062323][-4.2601147 -4.260704 -4.2451963 -4.2299533 -4.2205791 -4.2113214 -4.2036514 -4.2028289 -4.2090368 -4.1881423 -4.1339154 -4.0580144 -3.9723439 -3.9260068 -3.9369371][-4.2502818 -4.252811 -4.2440715 -4.2370009 -4.2365332 -4.2345324 -4.2268825 -4.219398 -4.2114763 -4.1775308 -4.1165586 -4.0384765 -3.9544308 -3.9146404 -3.9218726][-4.2492957 -4.2480073 -4.2428207 -4.2436423 -4.2491417 -4.2490859 -4.2381063 -4.2229548 -4.20907 -4.1737814 -4.1208005 -4.0582404 -3.9981272 -3.9799516 -3.987601][-4.25422 -4.2461281 -4.2451725 -4.2551727 -4.2650666 -4.2636166 -4.2486992 -4.2273345 -4.2131648 -4.1850376 -4.1469779 -4.1069107 -4.0737844 -4.0703669 -4.0762362]]...]
INFO - root - 2017-12-07 16:02:16.354129: step 27210, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 53h:11m:57s remains)
INFO - root - 2017-12-07 16:02:23.061552: step 27220, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 54h:23m:47s remains)
INFO - root - 2017-12-07 16:02:29.978338: step 27230, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 62h:35m:24s remains)
INFO - root - 2017-12-07 16:02:36.730866: step 27240, loss = 2.10, batch loss = 2.04 (10.8 examples/sec; 0.741 sec/batch; 62h:50m:58s remains)
INFO - root - 2017-12-07 16:02:43.564581: step 27250, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 56h:45m:46s remains)
INFO - root - 2017-12-07 16:02:50.381788: step 27260, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 53h:13m:04s remains)
INFO - root - 2017-12-07 16:02:57.288417: step 27270, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.688 sec/batch; 58h:18m:30s remains)
INFO - root - 2017-12-07 16:03:04.109185: step 27280, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 61h:28m:15s remains)
INFO - root - 2017-12-07 16:03:10.747665: step 27290, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 57h:47m:23s remains)
INFO - root - 2017-12-07 16:03:17.439828: step 27300, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 55h:53m:33s remains)
2017-12-07 16:03:18.193070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2349672 -4.2449164 -4.2523456 -4.2566547 -4.2630806 -4.2687082 -4.2685242 -4.2655258 -4.2625184 -4.2604928 -4.2595363 -4.2543545 -4.2482424 -4.2400608 -4.2201562][-4.2445145 -4.25138 -4.2546206 -4.2526088 -4.2503042 -4.2475181 -4.2385364 -4.2277308 -4.2210989 -4.2207832 -4.2250609 -4.2241569 -4.2227721 -4.2204542 -4.2042418][-4.2497931 -4.2511158 -4.25079 -4.2461386 -4.2384281 -4.2267108 -4.2073231 -4.1896062 -4.1834564 -4.1884918 -4.1964622 -4.198566 -4.20369 -4.20601 -4.1918054][-4.2496414 -4.2478471 -4.2454033 -4.2397771 -4.2255149 -4.2054281 -4.176496 -4.1546316 -4.151751 -4.1684694 -4.1860375 -4.1931329 -4.2001491 -4.1987033 -4.1798816][-4.242414 -4.2417965 -4.240315 -4.2326884 -4.211812 -4.1842804 -4.1482472 -4.1227269 -4.1242738 -4.1506562 -4.1787591 -4.1915884 -4.1976643 -4.1917009 -4.1676793][-4.2370377 -4.2368174 -4.2341857 -4.2170696 -4.1839542 -4.1437416 -4.1000223 -4.0704293 -4.0718193 -4.1057463 -4.1496019 -4.1792736 -4.194798 -4.1942606 -4.1757369][-4.2348189 -4.2381749 -4.2351179 -4.20743 -4.1562948 -4.0957327 -4.03035 -3.9810534 -3.9775052 -4.0339975 -4.1123786 -4.1685228 -4.2013478 -4.2113028 -4.2024508][-4.2389741 -4.2481208 -4.247076 -4.215044 -4.1537714 -4.0774846 -3.9858088 -3.9065301 -3.8952479 -3.9806669 -4.0898695 -4.1682324 -4.2154727 -4.2332578 -4.2283292][-4.2458968 -4.2592831 -4.2616 -4.2311792 -4.1764722 -4.105546 -4.0147743 -3.9279344 -3.8993826 -3.9822998 -4.0918765 -4.1695528 -4.2171164 -4.2365017 -4.2357659][-4.2562165 -4.26837 -4.2710571 -4.250175 -4.2128506 -4.1643634 -4.1027627 -4.0395403 -4.0118732 -4.0594749 -4.129385 -4.1831617 -4.2156105 -4.2302337 -4.2362547][-4.2655473 -4.2706232 -4.2717395 -4.2603893 -4.2403731 -4.2180796 -4.1912332 -4.1628757 -4.1493387 -4.1657128 -4.1917992 -4.2159214 -4.2309356 -4.241538 -4.25017][-4.2797365 -4.2753692 -4.2661371 -4.2551589 -4.2469478 -4.2413249 -4.2374992 -4.2341361 -4.2333059 -4.2384768 -4.2458873 -4.252655 -4.2561069 -4.2580376 -4.2581706][-4.2899947 -4.2836089 -4.2686677 -4.2542043 -4.2477469 -4.2478185 -4.2551551 -4.2633557 -4.27115 -4.2755079 -4.2759585 -4.2721791 -4.2641983 -4.2549825 -4.2450304][-4.2904959 -4.2848797 -4.2694635 -4.2548113 -4.2491379 -4.2495804 -4.256094 -4.2670808 -4.2806339 -4.2865219 -4.28301 -4.2711797 -4.2537785 -4.2349863 -4.2191281][-4.2921643 -4.2900915 -4.28193 -4.2727623 -4.2667336 -4.2651415 -4.2699337 -4.2799654 -4.2907987 -4.2938747 -4.2876034 -4.2749553 -4.2579641 -4.2364397 -4.2174354]]...]
INFO - root - 2017-12-07 16:03:24.991661: step 27310, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 61h:04m:15s remains)
INFO - root - 2017-12-07 16:03:31.825356: step 27320, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 59h:26m:09s remains)
INFO - root - 2017-12-07 16:03:38.620282: step 27330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 56h:10m:34s remains)
INFO - root - 2017-12-07 16:03:45.410051: step 27340, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 54h:16m:50s remains)
INFO - root - 2017-12-07 16:03:52.256268: step 27350, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 59h:03m:33s remains)
INFO - root - 2017-12-07 16:03:59.106462: step 27360, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 59h:58m:41s remains)
INFO - root - 2017-12-07 16:04:05.885380: step 27370, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 62h:20m:07s remains)
INFO - root - 2017-12-07 16:04:12.662986: step 27380, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 55h:49m:16s remains)
INFO - root - 2017-12-07 16:04:19.261369: step 27390, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 53h:07m:35s remains)
INFO - root - 2017-12-07 16:04:25.998838: step 27400, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 54h:09m:45s remains)
2017-12-07 16:04:26.756590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2203503 -4.1603408 -4.0865145 -4.0386224 -4.0535359 -4.11098 -4.1717882 -4.2115617 -4.2201743 -4.2015557 -4.1716948 -4.1441221 -4.1383634 -4.1466227 -4.1503453][-4.2373362 -4.1893854 -4.1241407 -4.0821137 -4.0976772 -4.1471615 -4.1953998 -4.2238741 -4.2334557 -4.226707 -4.2126107 -4.194663 -4.1838675 -4.1778593 -4.1741757][-4.2384291 -4.2042007 -4.1577263 -4.1282949 -4.1415181 -4.17425 -4.2021818 -4.2207389 -4.2325068 -4.2369204 -4.2353954 -4.2286468 -4.2198195 -4.2068176 -4.1987991][-4.22717 -4.2052708 -4.174974 -4.1538544 -4.1593246 -4.1704082 -4.181263 -4.1998324 -4.2205982 -4.2398324 -4.252068 -4.2531347 -4.245976 -4.2292409 -4.2150192][-4.2177057 -4.2082191 -4.1874757 -4.1685052 -4.1565633 -4.1395063 -4.141016 -4.1618114 -4.1904583 -4.2245111 -4.2552953 -4.2650061 -4.2581377 -4.2404346 -4.2211723][-4.2123055 -4.208869 -4.1960306 -4.180171 -4.1504283 -4.1051235 -4.0918779 -4.1203704 -4.159873 -4.206162 -4.2434607 -4.2567797 -4.2520175 -4.2380276 -4.21861][-4.2105889 -4.2034287 -4.1911349 -4.1765718 -4.1309338 -4.0620866 -4.0420384 -4.0839138 -4.138133 -4.1863961 -4.2182288 -4.2307382 -4.2315483 -4.2277279 -4.2181444][-4.210896 -4.1975126 -4.1822653 -4.1597381 -4.0988936 -4.0180612 -4.0042048 -4.0681148 -4.1330781 -4.1714396 -4.1885753 -4.1949329 -4.2021122 -4.2122774 -4.2169647][-4.2110472 -4.1993523 -4.187315 -4.1552548 -4.0892596 -4.0170665 -4.0107069 -4.0803585 -4.1476178 -4.1735821 -4.1751661 -4.1726131 -4.177474 -4.1940341 -4.2082825][-4.2131853 -4.2122955 -4.2097468 -4.1784482 -4.1206179 -4.0598307 -4.0439086 -4.0903206 -4.1397319 -4.1526756 -4.1458597 -4.1421041 -4.1524487 -4.1812086 -4.2031593][-4.2201457 -4.2292385 -4.2332883 -4.20827 -4.1615219 -4.1073518 -4.0740991 -4.0868039 -4.109838 -4.1068139 -4.0951505 -4.0968232 -4.1252785 -4.1698403 -4.2008991][-4.2310858 -4.2438087 -4.2490849 -4.2292414 -4.1901088 -4.1432476 -4.1062932 -4.0975647 -4.0980439 -4.0775461 -4.0540981 -4.0591884 -4.1004515 -4.153873 -4.1892529][-4.2327209 -4.2437534 -4.2481666 -4.2329545 -4.2003808 -4.1615119 -4.1317897 -4.120821 -4.1126881 -4.0841842 -4.0573349 -4.0619063 -4.0996032 -4.14534 -4.1744113][-4.232522 -4.2395926 -4.2431183 -4.2318239 -4.2048941 -4.1751266 -4.1542878 -4.1458421 -4.139535 -4.1213684 -4.1075773 -4.1117244 -4.132719 -4.1559978 -4.1713858][-4.2240009 -4.230351 -4.2364841 -4.2282677 -4.2051435 -4.1789203 -4.1598892 -4.1507497 -4.1471748 -4.1422024 -4.1435723 -4.1508393 -4.1599045 -4.1661363 -4.1748066]]...]
INFO - root - 2017-12-07 16:04:33.601124: step 27410, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 55h:07m:49s remains)
INFO - root - 2017-12-07 16:04:40.466838: step 27420, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 56h:40m:18s remains)
INFO - root - 2017-12-07 16:04:47.415390: step 27430, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 55h:35m:26s remains)
INFO - root - 2017-12-07 16:04:54.301863: step 27440, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 58h:53m:15s remains)
INFO - root - 2017-12-07 16:05:01.097533: step 27450, loss = 2.03, batch loss = 1.97 (10.9 examples/sec; 0.736 sec/batch; 62h:20m:31s remains)
INFO - root - 2017-12-07 16:05:07.904595: step 27460, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 57h:57m:30s remains)
INFO - root - 2017-12-07 16:05:14.697939: step 27470, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 53h:22m:56s remains)
INFO - root - 2017-12-07 16:05:21.505939: step 27480, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 53h:27m:35s remains)
INFO - root - 2017-12-07 16:05:28.214009: step 27490, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 61h:15m:09s remains)
INFO - root - 2017-12-07 16:05:34.954564: step 27500, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 59h:48m:24s remains)
2017-12-07 16:05:35.729062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1825509 -4.1636338 -4.1574678 -4.1711583 -4.2017431 -4.228888 -4.2510715 -4.2651477 -4.26628 -4.2662625 -4.2508044 -4.225112 -4.2098689 -4.2076831 -4.2131267][-4.191371 -4.1692286 -4.1494246 -4.1513786 -4.1779051 -4.2085791 -4.23547 -4.257 -4.2653871 -4.2673807 -4.2490273 -4.2235479 -4.2137947 -4.2196178 -4.2253213][-4.2015715 -4.179606 -4.1542339 -4.1394763 -4.1488919 -4.1774936 -4.2088156 -4.2396922 -4.2556305 -4.257823 -4.236526 -4.2140779 -4.2132339 -4.2289085 -4.2331805][-4.2079096 -4.1867213 -4.1599345 -4.1328316 -4.1161261 -4.12944 -4.1637449 -4.2013116 -4.22375 -4.22435 -4.2039509 -4.1903505 -4.2034183 -4.230639 -4.2369075][-4.2181096 -4.2009 -4.1755471 -4.1392536 -4.0963678 -4.0813832 -4.1065178 -4.1457615 -4.1766319 -4.1874542 -4.180244 -4.17975 -4.2034755 -4.238873 -4.2532272][-4.2261853 -4.2167563 -4.1990719 -4.161212 -4.1039481 -4.0610733 -4.0588245 -4.0863547 -4.1247969 -4.156292 -4.1709995 -4.1831369 -4.2094927 -4.2435293 -4.2568331][-4.2298241 -4.22569 -4.2203159 -4.1897416 -4.13148 -4.0684218 -4.0269194 -4.0241175 -4.0649838 -4.1274343 -4.1728244 -4.197567 -4.2215495 -4.2458563 -4.2521715][-4.2396255 -4.2372551 -4.2409325 -4.222682 -4.1708984 -4.1025405 -4.0377774 -4.005764 -4.0482464 -4.1332426 -4.2012324 -4.2314672 -4.2439008 -4.25443 -4.2539511][-4.2620549 -4.2562995 -4.2655883 -4.26468 -4.2292132 -4.1714759 -4.1087108 -4.0679851 -4.1000195 -4.1807008 -4.2496667 -4.2762971 -4.2725773 -4.2671113 -4.262434][-4.2887454 -4.2836533 -4.29693 -4.3085423 -4.2913933 -4.249855 -4.1985884 -4.159483 -4.1742792 -4.234724 -4.2912745 -4.3115788 -4.2973595 -4.2797194 -4.2707267][-4.3015509 -4.2980504 -4.3148174 -4.3342586 -4.3292966 -4.2999115 -4.2590871 -4.2242565 -4.2260041 -4.2658949 -4.3083243 -4.3227072 -4.3068333 -4.2867804 -4.2779927][-4.2983017 -4.2956257 -4.3157468 -4.3399434 -4.3433728 -4.3233924 -4.2916007 -4.2621689 -4.2561092 -4.2795649 -4.3087721 -4.31997 -4.3100538 -4.2966242 -4.2924995][-4.3020072 -4.2966681 -4.3144383 -4.3359118 -4.3428354 -4.3312254 -4.3077669 -4.2846804 -4.2772155 -4.2897272 -4.3085027 -4.3182545 -4.3145838 -4.308229 -4.3065162][-4.3100066 -4.3028774 -4.3136663 -4.3268738 -4.3319554 -4.3260336 -4.3106103 -4.2943921 -4.2863827 -4.2900386 -4.3001542 -4.3088741 -4.311265 -4.311317 -4.3117652][-4.3051267 -4.2999582 -4.3046451 -4.3107796 -4.3135705 -4.3119473 -4.3054638 -4.2974405 -4.2911897 -4.2904134 -4.2942281 -4.2994795 -4.3037977 -4.3068991 -4.3090825]]...]
INFO - root - 2017-12-07 16:05:42.462117: step 27510, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 55h:26m:16s remains)
INFO - root - 2017-12-07 16:05:48.997501: step 27520, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 57h:34m:56s remains)
INFO - root - 2017-12-07 16:05:55.807643: step 27530, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 58h:13m:09s remains)
INFO - root - 2017-12-07 16:06:02.491358: step 27540, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.609 sec/batch; 51h:36m:08s remains)
INFO - root - 2017-12-07 16:06:09.341736: step 27550, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.675 sec/batch; 57h:13m:04s remains)
INFO - root - 2017-12-07 16:06:16.117998: step 27560, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 59h:49m:33s remains)
INFO - root - 2017-12-07 16:06:22.870796: step 27570, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:38m:21s remains)
INFO - root - 2017-12-07 16:06:29.732878: step 27580, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 56h:33m:08s remains)
INFO - root - 2017-12-07 16:06:36.418629: step 27590, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 52h:12m:40s remains)
INFO - root - 2017-12-07 16:06:43.198068: step 27600, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 54h:49m:23s remains)
2017-12-07 16:06:44.068446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2405968 -4.2468557 -4.2452717 -4.2465091 -4.2368793 -4.2190328 -4.2021365 -4.1863894 -4.1686487 -4.155798 -4.1508904 -4.1576281 -4.1721616 -4.1779952 -4.1795082][-4.2437091 -4.2612867 -4.2626243 -4.2560225 -4.2358432 -4.2045331 -4.1813669 -4.1705723 -4.1586447 -4.1507468 -4.1503353 -4.1601019 -4.1734114 -4.1766133 -4.1738954][-4.2389469 -4.2681756 -4.2729597 -4.2563605 -4.216311 -4.1681232 -4.1435056 -4.1465645 -4.1540403 -4.1543956 -4.157455 -4.16901 -4.179028 -4.17901 -4.1764297][-4.2268372 -4.2664165 -4.2760649 -4.2537284 -4.1930847 -4.1186209 -4.0847754 -4.1061783 -4.1458764 -4.1623645 -4.1665192 -4.1730814 -4.1773729 -4.1754146 -4.1761627][-4.2157989 -4.2618752 -4.2722278 -4.2403922 -4.160975 -4.0632114 -4.0142865 -4.051363 -4.1274409 -4.1689987 -4.1770463 -4.1765652 -4.1729031 -4.1688666 -4.1725268][-4.2098827 -4.2558641 -4.2620416 -4.2179947 -4.11758 -3.9872096 -3.9117265 -3.9688981 -4.0864449 -4.1605983 -4.1776528 -4.175879 -4.1712747 -4.1684008 -4.1758947][-4.1907296 -4.2369895 -4.2432871 -4.1956129 -4.0809231 -3.9128547 -3.7991207 -3.8767781 -4.0346942 -4.1432638 -4.1749392 -4.1764665 -4.1737585 -4.1721144 -4.1802363][-4.1859083 -4.2239938 -4.22965 -4.1913052 -4.091598 -3.929951 -3.800859 -3.8581557 -4.0180435 -4.1395884 -4.1837745 -4.191278 -4.1885543 -4.1832118 -4.1866822][-4.2036238 -4.2315445 -4.2358036 -4.2133536 -4.1467667 -4.0351386 -3.9359326 -3.9479547 -4.0529361 -4.1522107 -4.1951942 -4.2061734 -4.2034836 -4.1967545 -4.1951208][-4.2296791 -4.2406411 -4.2430792 -4.2362857 -4.2013507 -4.1361909 -4.0711308 -4.0580239 -4.1080103 -4.1730738 -4.2049904 -4.2153053 -4.2120752 -4.2073216 -4.2042418][-4.2501245 -4.2459702 -4.2425351 -4.2409077 -4.2283039 -4.1949949 -4.1558537 -4.1372614 -4.1546078 -4.19033 -4.2127075 -4.2218065 -4.2196226 -4.2178879 -4.216002][-4.2662177 -4.252295 -4.2384081 -4.2327037 -4.2326622 -4.2218518 -4.2007132 -4.1828537 -4.1842647 -4.2019205 -4.2163396 -4.2239604 -4.2281647 -4.2337041 -4.2343674][-4.2743015 -4.2622352 -4.2458825 -4.2375331 -4.2430115 -4.2476077 -4.2390118 -4.2210784 -4.2079821 -4.208024 -4.21272 -4.2199697 -4.2339 -4.2502961 -4.2556572][-4.2635951 -4.2587986 -4.2465005 -4.2376966 -4.2418532 -4.2537794 -4.2559843 -4.2404623 -4.2185912 -4.2057343 -4.2062159 -4.2140121 -4.2343006 -4.2583704 -4.2700567][-4.2486863 -4.2471724 -4.2378964 -4.2295942 -4.2337995 -4.2498355 -4.258462 -4.2464237 -4.2208614 -4.2024026 -4.20226 -4.2120056 -4.2347679 -4.2633195 -4.2819748]]...]
INFO - root - 2017-12-07 16:06:50.776238: step 27610, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 54h:10m:37s remains)
INFO - root - 2017-12-07 16:06:57.570277: step 27620, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 54h:07m:38s remains)
INFO - root - 2017-12-07 16:07:04.524114: step 27630, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.751 sec/batch; 63h:33m:42s remains)
INFO - root - 2017-12-07 16:07:11.293143: step 27640, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 61h:11m:04s remains)
INFO - root - 2017-12-07 16:07:18.115103: step 27650, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.680 sec/batch; 57h:35m:24s remains)
INFO - root - 2017-12-07 16:07:24.950350: step 27660, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 54h:43m:44s remains)
INFO - root - 2017-12-07 16:07:31.615628: step 27670, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 54h:59m:11s remains)
INFO - root - 2017-12-07 16:07:38.496730: step 27680, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 59h:26m:25s remains)
INFO - root - 2017-12-07 16:07:45.222443: step 27690, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 59h:47m:28s remains)
INFO - root - 2017-12-07 16:07:51.941179: step 27700, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 57h:30m:22s remains)
2017-12-07 16:07:52.670663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.304564 -4.2734904 -4.2366505 -4.2142038 -4.1998596 -4.1840153 -4.1655412 -4.1625433 -4.189611 -4.2252173 -4.2481546 -4.2592487 -4.2666802 -4.2716236 -4.2456827][-4.3069444 -4.2783818 -4.2438703 -4.2180252 -4.2004552 -4.1827888 -4.1648617 -4.1672425 -4.198586 -4.2412157 -4.2710719 -4.2871895 -4.2953272 -4.2938013 -4.2591105][-4.3115954 -4.2856832 -4.2510118 -4.2188797 -4.1905832 -4.1633348 -4.1422625 -4.1444244 -4.1766329 -4.2276678 -4.270771 -4.2990065 -4.3152537 -4.3166676 -4.2828836][-4.31247 -4.2870097 -4.2486334 -4.20767 -4.1644468 -4.1230521 -4.0926743 -4.0861721 -4.1132383 -4.1694212 -4.2283235 -4.2716713 -4.2993169 -4.3082523 -4.2838945][-4.3080564 -4.2788019 -4.2329249 -4.1833787 -4.1275692 -4.0711732 -4.0201225 -3.9910641 -4.0119362 -4.0801954 -4.1565647 -4.2155647 -4.2587276 -4.2772226 -4.26361][-4.2977505 -4.2597475 -4.2042532 -4.145884 -4.0799546 -4.0066319 -3.9220946 -3.8525 -3.8658502 -3.9603002 -4.0626545 -4.1405625 -4.1979213 -4.2270741 -4.2224584][-4.2813382 -4.232482 -4.1654367 -4.0961237 -4.0235453 -3.9417717 -3.8308589 -3.7265959 -3.7357345 -3.8618953 -3.9884703 -4.0761809 -4.1390581 -4.173346 -4.1764555][-4.2668142 -4.2099919 -4.1342778 -4.0585489 -3.9867849 -3.9178576 -3.8255005 -3.7479084 -3.7738943 -3.8940244 -4.0078297 -4.08154 -4.1317558 -4.1598835 -4.1646066][-4.2608514 -4.20554 -4.1341767 -4.0656619 -4.0067749 -3.9632661 -3.9119349 -3.883826 -3.9246862 -4.0126767 -4.0902481 -4.1382494 -4.170186 -4.1872511 -4.1894588][-4.2638979 -4.2196541 -4.1615453 -4.1052322 -4.0659766 -4.043108 -4.0186534 -4.0192666 -4.0600295 -4.1215377 -4.1721578 -4.202836 -4.2273922 -4.2392964 -4.2390132][-4.2732491 -4.2410531 -4.1992192 -4.160995 -4.1425085 -4.1381612 -4.1332536 -4.1458073 -4.1785975 -4.2175393 -4.2465653 -4.2626538 -4.2785182 -4.2871246 -4.2858119][-4.2819781 -4.2603703 -4.2326984 -4.2118669 -4.2122016 -4.2233105 -4.2306271 -4.2445073 -4.2647467 -4.2853379 -4.299489 -4.3048029 -4.3117781 -4.316524 -4.3151741][-4.2884164 -4.2739887 -4.2554336 -4.2442703 -4.2523274 -4.2691932 -4.2832556 -4.2973175 -4.3078051 -4.3169975 -4.3242741 -4.3266964 -4.3293948 -4.331604 -4.3305063][-4.288558 -4.2793412 -4.264698 -4.2559571 -4.2644219 -4.2822509 -4.2984896 -4.3130884 -4.3196774 -4.3237667 -4.3273258 -4.3279552 -4.3275151 -4.32657 -4.3240151][-4.2779045 -4.2722764 -4.2597294 -4.2526226 -4.2612376 -4.2782893 -4.2920671 -4.3047991 -4.307971 -4.30885 -4.30967 -4.3083529 -4.3058915 -4.3041024 -4.3023977]]...]
INFO - root - 2017-12-07 16:07:59.568218: step 27710, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 63h:08m:14s remains)
INFO - root - 2017-12-07 16:08:06.434642: step 27720, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 56h:42m:18s remains)
INFO - root - 2017-12-07 16:08:13.271386: step 27730, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 56h:09m:14s remains)
INFO - root - 2017-12-07 16:08:20.193115: step 27740, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 55h:50m:40s remains)
INFO - root - 2017-12-07 16:08:26.986454: step 27750, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 61h:03m:27s remains)
INFO - root - 2017-12-07 16:08:33.799574: step 27760, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.719 sec/batch; 60h:51m:41s remains)
INFO - root - 2017-12-07 16:08:40.585018: step 27770, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 56h:47m:12s remains)
INFO - root - 2017-12-07 16:08:47.342794: step 27780, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.620 sec/batch; 52h:29m:43s remains)
INFO - root - 2017-12-07 16:08:53.932553: step 27790, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 57h:55m:34s remains)
INFO - root - 2017-12-07 16:09:00.721399: step 27800, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 60h:54m:03s remains)
2017-12-07 16:09:01.524228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2493138 -4.2459521 -4.2536016 -4.2739806 -4.295856 -4.2962503 -4.2828789 -4.2766747 -4.2759652 -4.2790184 -4.2853966 -4.290379 -4.2875886 -4.2762122 -4.252727][-4.2418237 -4.2369118 -4.2388873 -4.2578936 -4.2871003 -4.2964115 -4.2903266 -4.2898064 -4.294589 -4.2990875 -4.3016152 -4.3053904 -4.2981472 -4.2785053 -4.2546296][-4.2055721 -4.2055926 -4.20772 -4.2249303 -4.2585816 -4.2716832 -4.2733793 -4.2791314 -4.2914572 -4.3018074 -4.3011646 -4.3074841 -4.3040457 -4.280858 -4.2531824][-4.1556606 -4.1701589 -4.1871376 -4.215405 -4.2509246 -4.2562571 -4.2520757 -4.25518 -4.2675276 -4.2809348 -4.2804484 -4.2920566 -4.29848 -4.2856655 -4.2583532][-4.1241894 -4.1533742 -4.1882973 -4.2214437 -4.2440739 -4.2272964 -4.2005372 -4.2023964 -4.2225103 -4.2434468 -4.24981 -4.2650743 -4.2858453 -4.289753 -4.2691889][-4.1252303 -4.1589003 -4.2018647 -4.2279577 -4.2199888 -4.1637945 -4.0974884 -4.0887671 -4.1298528 -4.1816373 -4.2139778 -4.2408395 -4.2704544 -4.2890697 -4.2784][-4.1387458 -4.1555943 -4.185626 -4.19747 -4.16312 -4.06584 -3.9492111 -3.9212456 -4.001399 -4.1033964 -4.1749592 -4.222733 -4.2596326 -4.285665 -4.2837234][-4.1549034 -4.1563139 -4.164649 -4.1533279 -4.0971518 -3.9745848 -3.8234258 -3.7833776 -3.9009619 -4.0433216 -4.1457062 -4.2113037 -4.2506728 -4.2758074 -4.2779593][-4.15714 -4.1556644 -4.1527648 -4.1286588 -4.0753851 -3.9774935 -3.8569014 -3.8253808 -3.9232473 -4.052402 -4.1513982 -4.2131166 -4.2468843 -4.261889 -4.2635217][-4.155241 -4.1611919 -4.15947 -4.1352735 -4.10296 -4.0532222 -3.9927704 -3.9764922 -4.02882 -4.1125364 -4.1844826 -4.2292304 -4.2478461 -4.2504063 -4.2494345][-4.1594367 -4.1658525 -4.1683688 -4.1524425 -4.1325641 -4.1179242 -4.1084733 -4.1127286 -4.1407456 -4.184516 -4.2257318 -4.254065 -4.2598462 -4.2490029 -4.2444458][-4.1891317 -4.1913295 -4.1933284 -4.1790628 -4.1592932 -4.160872 -4.1794395 -4.1963172 -4.2164989 -4.2407446 -4.2598243 -4.2730494 -4.2690897 -4.2445636 -4.2311311][-4.2318139 -4.2304416 -4.2287169 -4.2107897 -4.1891232 -4.1984944 -4.22446 -4.2411356 -4.2564211 -4.2724676 -4.28022 -4.2784305 -4.2678676 -4.2393913 -4.2195807][-4.2659206 -4.264163 -4.2612605 -4.2424269 -4.2205877 -4.2332268 -4.2573981 -4.2668042 -4.27322 -4.2835655 -4.2854147 -4.278172 -4.2656994 -4.2413254 -4.221633][-4.2839947 -4.281775 -4.2816596 -4.2678132 -4.2529263 -4.2660255 -4.28299 -4.2830229 -4.280582 -4.282279 -4.2811737 -4.2741985 -4.2627044 -4.2453437 -4.2312007]]...]
INFO - root - 2017-12-07 16:09:08.281195: step 27810, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 53h:04m:05s remains)
INFO - root - 2017-12-07 16:09:15.098844: step 27820, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 60h:24m:32s remains)
INFO - root - 2017-12-07 16:09:21.765894: step 27830, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 56h:11m:23s remains)
INFO - root - 2017-12-07 16:09:28.441026: step 27840, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 55h:55m:01s remains)
INFO - root - 2017-12-07 16:09:35.166182: step 27850, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 54h:02m:15s remains)
INFO - root - 2017-12-07 16:09:41.905491: step 27860, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.672 sec/batch; 56h:51m:26s remains)
INFO - root - 2017-12-07 16:09:48.691189: step 27870, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 0.744 sec/batch; 62h:58m:03s remains)
INFO - root - 2017-12-07 16:09:55.378746: step 27880, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 56h:45m:37s remains)
INFO - root - 2017-12-07 16:10:01.975757: step 27890, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 55h:57m:48s remains)
INFO - root - 2017-12-07 16:10:08.734484: step 27900, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 53h:49m:58s remains)
2017-12-07 16:10:09.435292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1351514 -4.1489849 -4.150836 -4.13961 -4.131464 -4.1306543 -4.1409473 -4.1701407 -4.2025213 -4.2128267 -4.199697 -4.1714997 -4.1406674 -4.1132045 -4.0811586][-4.1333914 -4.1582637 -4.1739411 -4.175137 -4.1726704 -4.1692142 -4.170177 -4.1896191 -4.2222471 -4.2392268 -4.2315388 -4.2051888 -4.1717629 -4.1415129 -4.1119137][-4.1202688 -4.1525321 -4.1817937 -4.1973553 -4.2015557 -4.1941366 -4.1840892 -4.1907 -4.2179241 -4.2410989 -4.2449837 -4.23017 -4.200994 -4.16957 -4.1420279][-4.0948029 -4.1253366 -4.1609974 -4.1863289 -4.1941609 -4.1838737 -4.1672478 -4.1633191 -4.1853504 -4.21534 -4.2354789 -4.2366843 -4.2160573 -4.1864386 -4.1631269][-4.0754547 -4.0942802 -4.126637 -4.1524868 -4.1573458 -4.1401649 -4.1164494 -4.1048918 -4.1256385 -4.1650419 -4.2017756 -4.220089 -4.2132592 -4.1926956 -4.1777039][-4.0724645 -4.0828462 -4.1043005 -4.1175113 -4.1076951 -4.074986 -4.0371003 -4.0162048 -4.0405703 -4.0959635 -4.1490664 -4.18079 -4.1866016 -4.1767278 -4.17248][-4.0741196 -4.0841079 -4.0959835 -4.0946341 -4.0686574 -4.0197845 -3.962636 -3.9248796 -3.9540191 -4.0271773 -4.0945392 -4.1327133 -4.1436472 -4.1397433 -4.1443462][-4.0718684 -4.0850778 -4.0937757 -4.0878277 -4.0571432 -4.004797 -3.940937 -3.8954973 -3.9231811 -3.9985633 -4.06526 -4.0955992 -4.1034994 -4.1023865 -4.1159115][-4.0789685 -4.0921021 -4.0987496 -4.0948124 -4.07766 -4.0448055 -4.000011 -3.9598379 -3.9694407 -4.018291 -4.0596414 -4.0701737 -4.0676012 -4.069478 -4.09347][-4.089982 -4.0995879 -4.102191 -4.0979028 -4.0951972 -4.0871792 -4.062819 -4.0269918 -4.0159173 -4.0348673 -4.0514336 -4.0468836 -4.0379577 -4.0436068 -4.0779953][-4.091754 -4.0949607 -4.0907779 -4.0882878 -4.0974503 -4.1091328 -4.1006722 -4.0688763 -4.0435629 -4.0407848 -4.0427909 -4.0374069 -4.0302668 -4.0387893 -4.0764][-4.087635 -4.0829673 -4.077682 -4.0828409 -4.1041346 -4.132555 -4.1363292 -4.109901 -4.078126 -4.0618963 -4.0542712 -4.0519681 -4.0503817 -4.0561967 -4.0912457][-4.0976095 -4.0875387 -4.0840049 -4.0935545 -4.1218791 -4.1573563 -4.1693773 -4.1510086 -4.1204944 -4.0988283 -4.0906181 -4.0929527 -4.09437 -4.0971451 -4.1277456][-4.1211853 -4.1050286 -4.095983 -4.0992579 -4.1278796 -4.1666985 -4.1869183 -4.1768804 -4.1502585 -4.1296949 -4.1242247 -4.1327362 -4.1396961 -4.1439033 -4.1680255][-4.1393113 -4.1162682 -4.0960979 -4.0883021 -4.1149025 -4.1586246 -4.1885 -4.1898336 -4.1739192 -4.159492 -4.1545382 -4.1636624 -4.1743064 -4.1791863 -4.1949043]]...]
INFO - root - 2017-12-07 16:10:16.140632: step 27910, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 60h:33m:44s remains)
INFO - root - 2017-12-07 16:10:22.963351: step 27920, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 56h:03m:29s remains)
INFO - root - 2017-12-07 16:10:29.749086: step 27930, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 53h:31m:35s remains)
INFO - root - 2017-12-07 16:10:36.606550: step 27940, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 61h:26m:15s remains)
INFO - root - 2017-12-07 16:10:43.522053: step 27950, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 61h:31m:54s remains)
INFO - root - 2017-12-07 16:10:50.297299: step 27960, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 58h:23m:48s remains)
INFO - root - 2017-12-07 16:10:57.034295: step 27970, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 57h:03m:09s remains)
INFO - root - 2017-12-07 16:11:04.015227: step 27980, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 56h:06m:11s remains)
INFO - root - 2017-12-07 16:11:10.657769: step 27990, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 60h:18m:05s remains)
INFO - root - 2017-12-07 16:11:17.409888: step 28000, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 59h:16m:47s remains)
2017-12-07 16:11:18.182556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1960435 -4.1895714 -4.1756406 -4.1726403 -4.1748466 -4.1699858 -4.1624351 -4.1453719 -4.1103253 -4.0949779 -4.1167979 -4.1521835 -4.1868935 -4.2152019 -4.2362161][-4.1995764 -4.2003136 -4.1856456 -4.1798582 -4.1731234 -4.1534462 -4.1283112 -4.0955381 -4.0599561 -4.0598984 -4.0936937 -4.1369472 -4.1828909 -4.2185235 -4.2370796][-4.2135267 -4.2177167 -4.2047343 -4.1975451 -4.1834593 -4.1468863 -4.1026254 -4.059886 -4.0379629 -4.0624394 -4.1105132 -4.1594944 -4.2028122 -4.22975 -4.233017][-4.2394276 -4.2402639 -4.2250433 -4.2141457 -4.1891794 -4.1339765 -4.070591 -4.0282164 -4.0301161 -4.0755796 -4.13548 -4.193542 -4.2368512 -4.2505503 -4.2305741][-4.2493639 -4.2429228 -4.2259417 -4.2140188 -4.1795454 -4.1032696 -4.0153103 -3.9754467 -4.009397 -4.0774469 -4.1471372 -4.2140484 -4.2584229 -4.2582521 -4.2209482][-4.2315578 -4.2191253 -4.2023287 -4.1918535 -4.150537 -4.05326 -3.9258518 -3.8820703 -3.9598117 -4.0621161 -4.1462073 -4.2155576 -4.2609625 -4.2547407 -4.2085552][-4.188612 -4.1686268 -4.154757 -4.1525517 -4.1169515 -4.0100217 -3.8483834 -3.7939286 -3.9109271 -4.0443673 -4.144649 -4.2148814 -4.2608848 -4.25767 -4.2060847][-4.1517572 -4.1393251 -4.1343546 -4.1401176 -4.118926 -4.0356622 -3.8890114 -3.8332777 -3.9448314 -4.0721159 -4.1667361 -4.2295294 -4.2682872 -4.2622919 -4.2098775][-4.1400075 -4.1442471 -4.147316 -4.1518745 -4.1407533 -4.0906062 -3.9869773 -3.9385552 -4.0222268 -4.1240206 -4.1997948 -4.2467613 -4.271996 -4.2574253 -4.2047443][-4.133338 -4.1501317 -4.1609988 -4.1579142 -4.1503186 -4.1144414 -4.03747 -3.9975514 -4.068954 -4.1575365 -4.2177811 -4.2489705 -4.2608147 -4.2367368 -4.1858869][-4.1127357 -4.1350827 -4.15038 -4.1449313 -4.1403732 -4.117249 -4.0586505 -4.0210752 -4.0826721 -4.1619358 -4.2120652 -4.2373967 -4.24244 -4.213007 -4.1585722][-4.0892706 -4.1123676 -4.1305418 -4.1277981 -4.127213 -4.112659 -4.0667028 -4.0298276 -4.0760655 -4.1430707 -4.1879215 -4.2140546 -4.2167459 -4.1879148 -4.136281][-4.0879273 -4.1067204 -4.1214981 -4.1203775 -4.1216216 -4.1126695 -4.0824065 -4.0517764 -4.0827541 -4.1377282 -4.1718874 -4.1940336 -4.1987023 -4.1782417 -4.1421885][-4.1170311 -4.1192837 -4.122016 -4.1181664 -4.1173582 -4.1183586 -4.1108303 -4.09806 -4.1203704 -4.1559868 -4.1749234 -4.1918311 -4.2011118 -4.194448 -4.1795344][-4.145752 -4.1329269 -4.1240883 -4.1191993 -4.1202064 -4.1268606 -4.1329145 -4.131978 -4.1473122 -4.1685309 -4.1799221 -4.1954637 -4.2063866 -4.2121077 -4.2168374]]...]
INFO - root - 2017-12-07 16:11:25.098796: step 28010, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 58h:23m:35s remains)
INFO - root - 2017-12-07 16:11:31.891999: step 28020, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 61h:11m:28s remains)
INFO - root - 2017-12-07 16:11:38.704452: step 28030, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 58h:53m:05s remains)
INFO - root - 2017-12-07 16:11:45.548864: step 28040, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 57h:29m:08s remains)
INFO - root - 2017-12-07 16:11:52.312977: step 28050, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 55h:10m:08s remains)
INFO - root - 2017-12-07 16:11:59.087550: step 28060, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 55h:31m:24s remains)
INFO - root - 2017-12-07 16:12:05.947131: step 28070, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 61h:43m:31s remains)
INFO - root - 2017-12-07 16:12:12.833984: step 28080, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 62h:06m:20s remains)
INFO - root - 2017-12-07 16:12:19.505345: step 28090, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 56h:40m:36s remains)
INFO - root - 2017-12-07 16:12:26.217931: step 28100, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 53h:38m:31s remains)
2017-12-07 16:12:27.139961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584696 -4.2525616 -4.2476997 -4.2466464 -4.2502003 -4.2556338 -4.2524986 -4.2346783 -4.2213621 -4.2264638 -4.2454562 -4.2741432 -4.3040214 -4.3273535 -4.3401732][-4.2653508 -4.2618742 -4.2558393 -4.251379 -4.2519803 -4.2572474 -4.2601442 -4.2517834 -4.2467365 -4.2554779 -4.2727232 -4.2955823 -4.317832 -4.3343649 -4.3425984][-4.2522221 -4.2479587 -4.2369814 -4.226686 -4.223886 -4.2308421 -4.2423477 -4.2460232 -4.2493634 -4.2622395 -4.2813673 -4.3034067 -4.3226166 -4.3359108 -4.3418884][-4.2206435 -4.2059879 -4.1805797 -4.1613727 -4.1560869 -4.1665659 -4.1873803 -4.2024136 -4.2164526 -4.239748 -4.2680817 -4.2980356 -4.3208165 -4.3350949 -4.3412681][-4.1697774 -4.1411734 -4.102428 -4.0765142 -4.0677037 -4.0729408 -4.0896664 -4.1059103 -4.1300378 -4.1714621 -4.2201242 -4.2684007 -4.3018837 -4.3225288 -4.3327885][-4.123569 -4.0908465 -4.05237 -4.0257993 -4.0090837 -3.9976313 -3.9966338 -4.0058122 -4.0397353 -4.1028876 -4.1732669 -4.23595 -4.2776165 -4.3040915 -4.31997][-4.1061625 -4.0834007 -4.0577369 -4.0377545 -4.0169492 -3.9951315 -3.9820015 -3.98221 -4.0119343 -4.0782046 -4.1527057 -4.2186303 -4.263886 -4.2939186 -4.3130054][-4.1112151 -4.1010394 -4.08778 -4.0745397 -4.05784 -4.0379043 -4.0190744 -4.0062203 -4.0195975 -4.0712776 -4.1377373 -4.2035708 -4.2510095 -4.2830648 -4.3059511][-4.1299143 -4.1272597 -4.1212449 -4.1117167 -4.0970788 -4.0786486 -4.0559835 -4.0311279 -4.0294647 -4.0672855 -4.1253467 -4.1894474 -4.2371 -4.2699881 -4.2972808][-4.1591864 -4.1616688 -4.1584663 -4.1469269 -4.1288538 -4.1082454 -4.0821819 -4.0529981 -4.049469 -4.0833426 -4.1347189 -4.1909328 -4.233398 -4.264111 -4.2935758][-4.1938291 -4.1986542 -4.1957607 -4.1829748 -4.1643238 -4.144527 -4.1207738 -4.0962067 -4.0969224 -4.1305227 -4.1751418 -4.2197366 -4.2525945 -4.2766557 -4.3008866][-4.228642 -4.234395 -4.2317119 -4.2198906 -4.2034969 -4.1868706 -4.1680031 -4.15165 -4.1585054 -4.1894436 -4.2241068 -4.255805 -4.2798109 -4.2975 -4.3134871][-4.2515726 -4.2543344 -4.250412 -4.240726 -4.2285762 -4.2178416 -4.206821 -4.1997709 -4.2099195 -4.2350883 -4.2600904 -4.2821169 -4.3006821 -4.3146524 -4.3227038][-4.2686768 -4.2685452 -4.2642436 -4.2568607 -4.2494068 -4.2441416 -4.2392817 -4.2377234 -4.2466812 -4.2645297 -4.282032 -4.2982035 -4.3120742 -4.3218207 -4.3244324][-4.2837086 -4.282475 -4.2794085 -4.2752728 -4.2719536 -4.2700071 -4.2682047 -4.2679825 -4.2735271 -4.2832046 -4.294004 -4.3042369 -4.3137274 -4.3201051 -4.3200011]]...]
INFO - root - 2017-12-07 16:12:33.803028: step 28110, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 56h:55m:52s remains)
INFO - root - 2017-12-07 16:12:40.495155: step 28120, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 52h:28m:31s remains)
INFO - root - 2017-12-07 16:12:47.357704: step 28130, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.665 sec/batch; 56h:13m:10s remains)
INFO - root - 2017-12-07 16:12:54.072370: step 28140, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 61h:13m:14s remains)
INFO - root - 2017-12-07 16:13:00.794634: step 28150, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 55h:55m:43s remains)
INFO - root - 2017-12-07 16:13:07.668541: step 28160, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 53h:33m:34s remains)
INFO - root - 2017-12-07 16:13:14.373394: step 28170, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 54h:27m:08s remains)
INFO - root - 2017-12-07 16:13:21.115461: step 28180, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 61h:37m:13s remains)
INFO - root - 2017-12-07 16:13:27.672433: step 28190, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 58h:54m:30s remains)
INFO - root - 2017-12-07 16:13:34.403091: step 28200, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 59h:20m:53s remains)
2017-12-07 16:13:35.138538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2887845 -4.2914896 -4.2918396 -4.2840881 -4.2705584 -4.2562156 -4.2455711 -4.2362027 -4.2364092 -4.2446871 -4.2527952 -4.2570906 -4.2567024 -4.2542458 -4.2517667][-4.2498527 -4.2614884 -4.2693281 -4.2650204 -4.2530484 -4.2334785 -4.2178626 -4.2051215 -4.2079635 -4.2221088 -4.23343 -4.2407975 -4.2411418 -4.2425847 -4.2455077][-4.2174792 -4.2368369 -4.2519879 -4.2539086 -4.2448444 -4.222023 -4.196455 -4.1717515 -4.1734462 -4.1942263 -4.2101021 -4.2174768 -4.214994 -4.2194562 -4.2305737][-4.1928525 -4.21025 -4.2222438 -4.2222133 -4.211936 -4.18977 -4.1585412 -4.1241241 -4.1224442 -4.1450763 -4.1630812 -4.1744881 -4.1758351 -4.1871572 -4.2077031][-4.179471 -4.1930637 -4.1956267 -4.1841192 -4.1585727 -4.1287556 -4.0949464 -4.06305 -4.0638318 -4.0841489 -4.1025066 -4.1217165 -4.1361265 -4.1576877 -4.1865349][-4.1676431 -4.175745 -4.1753721 -4.1544771 -4.1064053 -4.05041 -4.0049777 -3.9887691 -4.0215287 -4.0604081 -4.0805793 -4.10043 -4.1199975 -4.14147 -4.170579][-4.1547513 -4.1610079 -4.1647019 -4.1472816 -4.0882149 -3.9968824 -3.9148002 -3.9044333 -3.9865866 -4.0688376 -4.1027923 -4.1164193 -4.126617 -4.1415749 -4.170835][-4.1438723 -4.1543331 -4.1631927 -4.1509337 -4.0970578 -3.9987152 -3.8870757 -3.8610148 -3.9643543 -4.06902 -4.1153889 -4.1267729 -4.128139 -4.1359921 -4.1699882][-4.1437235 -4.1546593 -4.1595869 -4.1424251 -4.09468 -4.0220366 -3.9427466 -3.927989 -4.0001554 -4.0744348 -4.108995 -4.1208124 -4.121922 -4.1285114 -4.1675568][-4.1463819 -4.163774 -4.1661544 -4.1404395 -4.09444 -4.0484414 -4.0138793 -4.0232797 -4.0716176 -4.1126118 -4.1298237 -4.1377869 -4.1378913 -4.1444807 -4.1817513][-4.1451325 -4.1735039 -4.185173 -4.1641393 -4.1236124 -4.0880113 -4.0746355 -4.0985737 -4.139235 -4.1663642 -4.1736512 -4.1776314 -4.1764884 -4.1809807 -4.2103796][-4.1602225 -4.1911459 -4.2120337 -4.2071586 -4.1813231 -4.1498833 -4.1385274 -4.1627474 -4.1983514 -4.2209635 -4.2245612 -4.2234869 -4.2198911 -4.2219391 -4.242425][-4.2046328 -4.2293615 -4.2514744 -4.2568579 -4.2449255 -4.2214155 -4.209568 -4.2226095 -4.2460694 -4.26313 -4.2667885 -4.2645912 -4.2611175 -4.2623234 -4.2760682][-4.2645082 -4.279253 -4.2944317 -4.3003197 -4.2954197 -4.281496 -4.2711635 -4.2735782 -4.2844176 -4.2944975 -4.298851 -4.300107 -4.2996931 -4.3019886 -4.3118725][-4.3128619 -4.3202248 -4.3268194 -4.3305521 -4.3295703 -4.322876 -4.3164845 -4.3152757 -4.3183432 -4.3233886 -4.328002 -4.3312497 -4.3327928 -4.3349004 -4.3402977]]...]
INFO - root - 2017-12-07 16:13:41.994732: step 28210, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 62h:07m:36s remains)
INFO - root - 2017-12-07 16:13:48.810296: step 28220, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 58h:59m:44s remains)
INFO - root - 2017-12-07 16:13:55.615341: step 28230, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 57h:36m:40s remains)
INFO - root - 2017-12-07 16:14:02.300515: step 28240, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 53h:26m:32s remains)
INFO - root - 2017-12-07 16:14:09.088383: step 28250, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 54h:44m:42s remains)
INFO - root - 2017-12-07 16:14:15.890574: step 28260, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 61h:13m:23s remains)
INFO - root - 2017-12-07 16:14:22.751622: step 28270, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 57h:26m:52s remains)
INFO - root - 2017-12-07 16:14:29.541419: step 28280, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 54h:54m:49s remains)
INFO - root - 2017-12-07 16:14:36.210774: step 28290, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.623 sec/batch; 52h:36m:56s remains)
INFO - root - 2017-12-07 16:14:42.916533: step 28300, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 54h:52m:12s remains)
2017-12-07 16:14:43.638273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3288198 -4.3380084 -4.3412666 -4.337 -4.32645 -4.3004746 -4.2599716 -4.2204771 -4.2043333 -4.2015944 -4.2154255 -4.2362685 -4.248364 -4.240016 -4.2277083][-4.3323307 -4.33892 -4.3406372 -4.3324862 -4.3139195 -4.2806578 -4.2322173 -4.1894736 -4.1701603 -4.1685767 -4.1948118 -4.2280498 -4.2443647 -4.2371 -4.2272058][-4.3361969 -4.3399658 -4.3356061 -4.3182483 -4.2896733 -4.2515359 -4.203115 -4.1591787 -4.1324458 -4.1368518 -4.1759415 -4.2143211 -4.2299056 -4.2264042 -4.2219181][-4.3369646 -4.3367195 -4.3232412 -4.2974491 -4.2611656 -4.217411 -4.1663265 -4.116189 -4.0889339 -4.1098003 -4.16025 -4.1965313 -4.2100644 -4.2091446 -4.2100863][-4.3350306 -4.33142 -4.314075 -4.2839751 -4.245729 -4.1972532 -4.1377039 -4.0789623 -4.0562077 -4.0971432 -4.1553793 -4.1931224 -4.2125568 -4.2124462 -4.21602][-4.3319492 -4.3285837 -4.312295 -4.2821956 -4.2419653 -4.1857018 -4.1120238 -4.0379739 -4.012661 -4.077466 -4.1490788 -4.1966581 -4.2300286 -4.23495 -4.2377763][-4.3249435 -4.3237886 -4.3102355 -4.2782145 -4.2282739 -4.1556849 -4.0591421 -3.9548688 -3.9253516 -4.0287509 -4.13115 -4.1992817 -4.2506852 -4.2634912 -4.2658596][-4.3171015 -4.3205252 -4.3064017 -4.2680635 -4.2047157 -4.1179581 -4.0007687 -3.873035 -3.8540709 -3.9991145 -4.1257973 -4.2035704 -4.2602139 -4.281311 -4.2876444][-4.3116217 -4.3180418 -4.3019252 -4.2563124 -4.1855803 -4.0999374 -3.9883227 -3.8845744 -3.89318 -4.0259304 -4.135715 -4.2033792 -4.2532058 -4.2761288 -4.2924681][-4.3061552 -4.3145094 -4.2992411 -4.2527556 -4.1850295 -4.1133223 -4.0231209 -3.9493732 -3.9728444 -4.0729303 -4.1551609 -4.2087226 -4.2481332 -4.2685513 -4.2885637][-4.3076572 -4.3156209 -4.3023529 -4.261085 -4.202991 -4.1440754 -4.0702744 -4.0196452 -4.049233 -4.1220207 -4.1832814 -4.2273903 -4.25924 -4.2752614 -4.2899952][-4.3133583 -4.3206716 -4.3121071 -4.2792134 -4.2327881 -4.1892204 -4.1365008 -4.1082125 -4.1355038 -4.1818628 -4.22225 -4.253366 -4.2767162 -4.2892137 -4.2983789][-4.3169203 -4.3200388 -4.3121967 -4.28839 -4.2566414 -4.2305408 -4.2014952 -4.1912885 -4.2113156 -4.2355103 -4.254292 -4.2711229 -4.2872243 -4.2989526 -4.3055744][-4.3147917 -4.3125405 -4.3054547 -4.2904911 -4.27244 -4.260128 -4.24957 -4.2505951 -4.2628741 -4.2708416 -4.2738614 -4.2814059 -4.2918496 -4.2998595 -4.30534][-4.315104 -4.3091059 -4.3024969 -4.2926626 -4.2812667 -4.2773175 -4.2757058 -4.2794623 -4.2848706 -4.2872195 -4.2873049 -4.2916012 -4.2972388 -4.3016958 -4.3065171]]...]
INFO - root - 2017-12-07 16:14:50.449925: step 28310, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 56h:36m:26s remains)
INFO - root - 2017-12-07 16:14:57.132133: step 28320, loss = 2.05, batch loss = 1.99 (13.2 examples/sec; 0.605 sec/batch; 51h:06m:19s remains)
INFO - root - 2017-12-07 16:15:03.903887: step 28330, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 54h:24m:53s remains)
INFO - root - 2017-12-07 16:15:10.719710: step 28340, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 61h:36m:58s remains)
INFO - root - 2017-12-07 16:15:17.481602: step 28350, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 58h:04m:40s remains)
INFO - root - 2017-12-07 16:15:24.235665: step 28360, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 57h:46m:20s remains)
INFO - root - 2017-12-07 16:15:30.992680: step 28370, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 53h:52m:03s remains)
INFO - root - 2017-12-07 16:15:37.850036: step 28380, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 55h:59m:20s remains)
INFO - root - 2017-12-07 16:15:44.570522: step 28390, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.750 sec/batch; 63h:23m:34s remains)
INFO - root - 2017-12-07 16:15:51.455814: step 28400, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 60h:00m:14s remains)
2017-12-07 16:15:52.153301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3430991 -4.3417354 -4.3330059 -4.3202949 -4.3057079 -4.2988405 -4.3024421 -4.306983 -4.30625 -4.2985215 -4.2839174 -4.2678704 -4.258338 -4.2577353 -4.2634969][-4.3389778 -4.339047 -4.3293056 -4.3118482 -4.2920485 -4.2836723 -4.2908978 -4.301959 -4.3093505 -4.3107371 -4.3023896 -4.2866726 -4.2727013 -4.2641416 -4.2604265][-4.3265524 -4.3236861 -4.308692 -4.2817764 -4.2524853 -4.2383995 -4.2453942 -4.2614388 -4.27922 -4.2932539 -4.297307 -4.2901382 -4.2820091 -4.2758303 -4.2707477][-4.3108521 -4.2993646 -4.2726092 -4.2306862 -4.1881633 -4.1629076 -4.1646996 -4.1847196 -4.213984 -4.2421885 -4.2607422 -4.2678204 -4.2736516 -4.2796021 -4.2837029][-4.2968369 -4.2778053 -4.23845 -4.1829581 -4.1270518 -4.0865092 -4.0748758 -4.0918093 -4.1273518 -4.1646438 -4.1956744 -4.219902 -4.2428417 -4.2640944 -4.2824183][-4.2901998 -4.2711678 -4.22823 -4.16838 -4.1034012 -4.0442505 -4.0073938 -4.0083365 -4.0376258 -4.0732579 -4.1095524 -4.1492338 -4.1899934 -4.2261505 -4.2578163][-4.2821927 -4.2702417 -4.2316527 -4.1751351 -4.1084738 -4.0392704 -3.9830456 -3.967464 -3.9870157 -4.0153627 -4.0503163 -4.0983939 -4.1485434 -4.1914115 -4.2309179][-4.2729554 -4.273212 -4.2455592 -4.1984186 -4.1402254 -4.0790672 -4.0257287 -4.0072293 -4.022378 -4.0461683 -4.0775161 -4.1218042 -4.1635828 -4.1964579 -4.2292275][-4.2735229 -4.2861032 -4.2728968 -4.2401752 -4.2013721 -4.1611214 -4.123745 -4.1094651 -4.1187387 -4.1321349 -4.1494455 -4.1778059 -4.2028151 -4.2221475 -4.2440348][-4.2845135 -4.3053722 -4.3063116 -4.2909479 -4.272161 -4.250772 -4.2239189 -4.2068214 -4.2038345 -4.2023149 -4.2031269 -4.2157416 -4.2296019 -4.2422576 -4.2577772][-4.3007622 -4.3206162 -4.3305569 -4.3307204 -4.3282084 -4.3210087 -4.3033609 -4.2825379 -4.2668309 -4.2513709 -4.2371554 -4.2340331 -4.2399063 -4.2512422 -4.2624297][-4.3128848 -4.3263359 -4.3378315 -4.3451118 -4.3506641 -4.352118 -4.3428741 -4.3235884 -4.3007178 -4.2738934 -4.2482438 -4.2311721 -4.2283211 -4.2386069 -4.2486172][-4.3207541 -4.3263288 -4.3338718 -4.3402543 -4.3462558 -4.3491144 -4.3440394 -4.3297882 -4.3066378 -4.2761974 -4.244276 -4.2161489 -4.2027316 -4.2089391 -4.2177658][-4.3216867 -4.3179965 -4.3170733 -4.3177533 -4.3198051 -4.3194966 -4.3148875 -4.3055758 -4.2874885 -4.2597728 -4.2301083 -4.201848 -4.1859355 -4.1895847 -4.1967607][-4.3145332 -4.3016438 -4.2898431 -4.2812524 -4.2754416 -4.2693996 -4.263051 -4.2566237 -4.2463264 -4.2289729 -4.2124557 -4.1978312 -4.190311 -4.194612 -4.201015]]...]
INFO - root - 2017-12-07 16:15:58.831845: step 28410, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 55h:50m:03s remains)
INFO - root - 2017-12-07 16:16:05.636227: step 28420, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 61h:02m:23s remains)
INFO - root - 2017-12-07 16:16:12.493863: step 28430, loss = 2.07, batch loss = 2.02 (10.6 examples/sec; 0.755 sec/batch; 63h:48m:11s remains)
INFO - root - 2017-12-07 16:16:19.285906: step 28440, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 57h:24m:36s remains)
INFO - root - 2017-12-07 16:16:26.024098: step 28450, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 56h:12m:47s remains)
INFO - root - 2017-12-07 16:16:32.885930: step 28460, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 61h:38m:57s remains)
INFO - root - 2017-12-07 16:16:39.666283: step 28470, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.751 sec/batch; 63h:26m:48s remains)
INFO - root - 2017-12-07 16:16:46.453674: step 28480, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 56h:59m:37s remains)
INFO - root - 2017-12-07 16:16:53.068608: step 28490, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 53h:11m:59s remains)
INFO - root - 2017-12-07 16:16:59.863899: step 28500, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 56h:21m:30s remains)
2017-12-07 16:17:00.588890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3011608 -4.2899938 -4.2862725 -4.2934346 -4.3023047 -4.3059354 -4.303565 -4.2992983 -4.2909265 -4.2805481 -4.2795291 -4.282095 -4.2777309 -4.2775846 -4.2925382][-4.2990346 -4.2828507 -4.2757206 -4.2800717 -4.2877092 -4.2889113 -4.2815747 -4.2738557 -4.2639718 -4.2524729 -4.2548623 -4.2645335 -4.2643876 -4.2650218 -4.2816572][-4.2890773 -4.270359 -4.2621069 -4.2645841 -4.27085 -4.2679939 -4.2549305 -4.24425 -4.2331548 -4.2213287 -4.2267213 -4.2458758 -4.2558026 -4.2614174 -4.2780952][-4.2770119 -4.2550855 -4.2416668 -4.2416439 -4.2436996 -4.2312517 -4.2126174 -4.2056422 -4.1979756 -4.1881843 -4.1924753 -4.2153511 -4.2360883 -4.2535558 -4.2739682][-4.2587132 -4.2321839 -4.2123871 -4.2107811 -4.2078366 -4.1798148 -4.1477804 -4.1446238 -4.1483407 -4.1446815 -4.1465335 -4.1688795 -4.1960135 -4.2264137 -4.2562723][-4.2294316 -4.1971412 -4.1708078 -4.161756 -4.1454215 -4.0950255 -4.0406766 -4.0454164 -4.0658431 -4.0736628 -4.0777588 -4.1001692 -4.136548 -4.1830611 -4.2263036][-4.196661 -4.1543756 -4.1155076 -4.0865755 -4.0445418 -3.9631743 -3.8875279 -3.9052267 -3.9502115 -3.9798725 -4.0027165 -4.0493078 -4.1071439 -4.1666269 -4.2150192][-4.1773977 -4.1219559 -4.0686069 -4.0192094 -3.9497664 -3.8454714 -3.7714634 -3.8111041 -3.8916645 -3.9583595 -4.0105028 -4.0765033 -4.1440439 -4.2023044 -4.2442403][-4.1816883 -4.1257539 -4.0766635 -4.0330911 -3.9717 -3.8852344 -3.835021 -3.8830147 -3.9686477 -4.0409112 -4.0983677 -4.1565285 -4.2134 -4.2603979 -4.2885838][-4.219501 -4.1783872 -4.1470022 -4.1189289 -4.0770574 -4.0207553 -3.9950986 -4.0357661 -4.101819 -4.1594024 -4.2072153 -4.2474346 -4.28543 -4.3163 -4.3290997][-4.2556033 -4.2332487 -4.2234106 -4.2107563 -4.1830397 -4.1527548 -4.1424356 -4.1711087 -4.2137804 -4.2494664 -4.2790051 -4.3031473 -4.3260603 -4.3426967 -4.3483849][-4.2719169 -4.2589216 -4.2621651 -4.2637005 -4.2526641 -4.2396584 -4.2361503 -4.2519064 -4.2752886 -4.29487 -4.3115144 -4.3262277 -4.3415084 -4.3511071 -4.3524203][-4.2838116 -4.2733393 -4.2804666 -4.2918339 -4.2941804 -4.2927623 -4.2944407 -4.301887 -4.3126707 -4.3228879 -4.3312411 -4.3385644 -4.3460331 -4.3511972 -4.3513389][-4.3037543 -4.2966542 -4.3016315 -4.3127565 -4.3208337 -4.3256197 -4.3287554 -4.332202 -4.3352337 -4.3378563 -4.3394256 -4.3409615 -4.3430519 -4.3465409 -4.3485703][-4.3251152 -4.3202133 -4.3220344 -4.3281369 -4.334434 -4.3387704 -4.3411765 -4.3419023 -4.3417292 -4.3409591 -4.3412781 -4.3429523 -4.3453836 -4.3486538 -4.3517103]]...]
INFO - root - 2017-12-07 16:17:07.473461: step 28510, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 59h:27m:37s remains)
INFO - root - 2017-12-07 16:17:14.324109: step 28520, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 53h:09m:25s remains)
INFO - root - 2017-12-07 16:17:21.125254: step 28530, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 53h:18m:11s remains)
INFO - root - 2017-12-07 16:17:28.006983: step 28540, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 62h:13m:11s remains)
INFO - root - 2017-12-07 16:17:34.786126: step 28550, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.724 sec/batch; 61h:07m:18s remains)
INFO - root - 2017-12-07 16:17:41.535178: step 28560, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 55h:29m:33s remains)
INFO - root - 2017-12-07 16:17:48.220061: step 28570, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 53h:15m:54s remains)
INFO - root - 2017-12-07 16:17:54.967607: step 28580, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 56h:36m:22s remains)
INFO - root - 2017-12-07 16:18:01.650404: step 28590, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 59h:04m:06s remains)
INFO - root - 2017-12-07 16:18:08.515354: step 28600, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 56h:48m:41s remains)
2017-12-07 16:18:09.247070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2445149 -4.2410622 -4.2406735 -4.2305613 -4.2196369 -4.2127419 -4.2051878 -4.1995463 -4.2027378 -4.2129159 -4.2216635 -4.2292647 -4.2325611 -4.2374463 -4.2346473][-4.2436557 -4.2409205 -4.2372274 -4.2211075 -4.2086544 -4.2005658 -4.1888704 -4.1826935 -4.1915965 -4.2080531 -4.2143784 -4.2126651 -4.2047215 -4.2040076 -4.2110949][-4.2515235 -4.2479892 -4.2427535 -4.224565 -4.2075543 -4.1910129 -4.1704187 -4.1551657 -4.1692224 -4.1985588 -4.2112679 -4.2034178 -4.1859016 -4.1844616 -4.2030869][-4.2623796 -4.2588863 -4.2571478 -4.2430172 -4.2171316 -4.181776 -4.1417809 -4.1130114 -4.133779 -4.1842518 -4.217752 -4.21596 -4.1931372 -4.1864409 -4.2074904][-4.2638035 -4.265245 -4.2732129 -4.2649403 -4.227839 -4.1646528 -4.0945592 -4.0467181 -4.0765982 -4.1626039 -4.2269311 -4.2451963 -4.2258749 -4.2088852 -4.2193608][-4.264678 -4.2696567 -4.2821164 -4.2730293 -4.2229013 -4.1339903 -4.0265527 -3.9499025 -3.9989789 -4.1266322 -4.2227893 -4.2634878 -4.2575326 -4.2357941 -4.2281976][-4.2727938 -4.2762156 -4.2863712 -4.2691483 -4.2081089 -4.1017604 -3.9536035 -3.8424757 -3.9201422 -4.0855656 -4.2054019 -4.2611394 -4.2687144 -4.2467957 -4.227499][-4.2767439 -4.27587 -4.2801018 -4.2562013 -4.1960425 -4.0922446 -3.9362926 -3.825258 -3.9212117 -4.0876522 -4.2045078 -4.2590189 -4.2694359 -4.2468905 -4.2192016][-4.2680783 -4.2608309 -4.2577038 -4.2373605 -4.1942773 -4.1220837 -4.0143266 -3.9568698 -4.0351396 -4.1525688 -4.2327862 -4.2674756 -4.2703986 -4.2446022 -4.2106543][-4.2559071 -4.2431808 -4.2308769 -4.2203808 -4.2012687 -4.165411 -4.1123672 -4.0992632 -4.1505284 -4.2141681 -4.2515216 -4.2601333 -4.2553353 -4.23355 -4.2049065][-4.2458334 -4.22956 -4.2131863 -4.2082181 -4.2056179 -4.198103 -4.1833019 -4.188592 -4.2163939 -4.2424583 -4.2470098 -4.2325158 -4.2248139 -4.21671 -4.2035837][-4.2195916 -4.2016416 -4.1837473 -4.1794515 -4.1865778 -4.2023234 -4.2123652 -4.2203035 -4.227458 -4.2276392 -4.2101455 -4.1849856 -4.18297 -4.1963415 -4.2045922][-4.1853147 -4.1746807 -4.1575241 -4.1496348 -4.1604223 -4.1854448 -4.2040992 -4.2104506 -4.2031946 -4.1885347 -4.1677485 -4.1484165 -4.1558595 -4.1860328 -4.2065129][-4.1630569 -4.1649537 -4.1559076 -4.1490831 -4.1582093 -4.1741972 -4.1887627 -4.1930604 -4.1858125 -4.1703868 -4.1588936 -4.1538796 -4.1656628 -4.1947851 -4.2111535][-4.15683 -4.1752343 -4.1804781 -4.1791606 -4.1804256 -4.1816049 -4.186337 -4.1913776 -4.1891203 -4.1826243 -4.1817827 -4.1842675 -4.1928668 -4.2139831 -4.2233057]]...]
INFO - root - 2017-12-07 16:18:16.056306: step 28610, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 62h:24m:09s remains)
INFO - root - 2017-12-07 16:18:22.995244: step 28620, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.761 sec/batch; 64h:13m:52s remains)
INFO - root - 2017-12-07 16:18:29.751458: step 28630, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 55h:28m:17s remains)
INFO - root - 2017-12-07 16:18:36.522392: step 28640, loss = 2.08, batch loss = 2.02 (13.2 examples/sec; 0.604 sec/batch; 50h:58m:37s remains)
INFO - root - 2017-12-07 16:18:43.370913: step 28650, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 59h:56m:01s remains)
INFO - root - 2017-12-07 16:18:50.301906: step 28660, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 62h:52m:10s remains)
INFO - root - 2017-12-07 16:18:57.148418: step 28670, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 57h:39m:50s remains)
INFO - root - 2017-12-07 16:19:03.833472: step 28680, loss = 2.04, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 52h:40m:05s remains)
INFO - root - 2017-12-07 16:19:10.470143: step 28690, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 52h:43m:56s remains)
INFO - root - 2017-12-07 16:19:17.232374: step 28700, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 60h:00m:36s remains)
2017-12-07 16:19:17.937987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.325788 -4.3300557 -4.3320403 -4.3293514 -4.3215036 -4.3096204 -4.3009233 -4.3001456 -4.3063345 -4.3127327 -4.3162541 -4.3169146 -4.3170195 -4.3181791 -4.3201294][-4.3243351 -4.3236928 -4.3188143 -4.3073354 -4.2897348 -4.2724533 -4.2622814 -4.2649136 -4.2778397 -4.290277 -4.2984352 -4.30205 -4.3037229 -4.3069754 -4.3108015][-4.3150454 -4.30691 -4.2909117 -4.2658386 -4.2381873 -4.216373 -4.2039771 -4.2080221 -4.2249851 -4.2436004 -4.2550321 -4.2606511 -4.2660966 -4.2746186 -4.2838221][-4.2988186 -4.2832923 -4.25527 -4.2170334 -4.1841884 -4.1611071 -4.1450887 -4.1436734 -4.1563215 -4.178412 -4.1927094 -4.1977973 -4.2070737 -4.2243562 -4.2431922][-4.2789164 -4.2583194 -4.2232375 -4.175992 -4.14053 -4.1177759 -4.0937285 -4.0784507 -4.0808682 -4.1070795 -4.1248422 -4.1284733 -4.1415029 -4.1690311 -4.1995492][-4.2581816 -4.2363815 -4.1996255 -4.1440015 -4.1038 -4.0770974 -4.0380054 -3.9949775 -3.9859352 -4.0273132 -4.0579042 -4.0631075 -4.0803294 -4.1199255 -4.1629887][-4.2410274 -4.220439 -4.1864295 -4.1276717 -4.0777569 -4.0416074 -3.9802356 -3.8932993 -3.8694365 -3.942471 -3.9989071 -4.0144825 -4.0421658 -4.0934749 -4.1478376][-4.2301307 -4.2091732 -4.1797881 -4.12467 -4.0669231 -4.0222983 -3.940433 -3.8026395 -3.757967 -3.868597 -3.9572959 -3.9941628 -4.0402451 -4.1008148 -4.1614909][-4.2297835 -4.2046785 -4.1781015 -4.1310673 -4.0731845 -4.0273361 -3.9454212 -3.7976608 -3.7397437 -3.8566756 -3.9610527 -4.0194573 -4.0786681 -4.1401839 -4.1960897][-4.2427549 -4.21362 -4.1848168 -4.1420364 -4.0890579 -4.0521622 -3.9938323 -3.889832 -3.8469923 -3.9302588 -4.0226092 -4.09015 -4.149374 -4.1993122 -4.2390318][-4.262229 -4.23128 -4.1968608 -4.1531529 -4.1052718 -4.0783091 -4.047142 -3.9939563 -3.978899 -4.0334563 -4.1066465 -4.1685386 -4.2161264 -4.2498608 -4.2720962][-4.2798605 -4.2529545 -4.2181745 -4.1760683 -4.1366844 -4.1181488 -4.103869 -4.0856047 -4.0910907 -4.12999 -4.1806097 -4.22417 -4.2566633 -4.2774286 -4.28932][-4.2922316 -4.2749686 -4.2482381 -4.2149372 -4.1877966 -4.1768403 -4.1700878 -4.1667624 -4.178359 -4.2024455 -4.2297716 -4.2538056 -4.2749596 -4.287879 -4.2954473][-4.3004003 -4.2923865 -4.2779984 -4.2568626 -4.241178 -4.2380304 -4.2362976 -4.2345457 -4.239388 -4.2514224 -4.2629514 -4.2739019 -4.2852879 -4.2925506 -4.2986193][-4.3060675 -4.303596 -4.2988319 -4.288558 -4.2803583 -4.2811279 -4.2820034 -4.2782273 -4.2749033 -4.2786527 -4.2838597 -4.2884693 -4.2928648 -4.2965732 -4.3017387]]...]
INFO - root - 2017-12-07 16:19:24.851529: step 28710, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 58h:05m:17s remains)
INFO - root - 2017-12-07 16:19:31.632225: step 28720, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 53h:34m:52s remains)
INFO - root - 2017-12-07 16:19:38.507763: step 28730, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.768 sec/batch; 64h:46m:26s remains)
INFO - root - 2017-12-07 16:19:45.361976: step 28740, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 58h:43m:44s remains)
INFO - root - 2017-12-07 16:19:52.174442: step 28750, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 55h:26m:36s remains)
INFO - root - 2017-12-07 16:19:58.715819: step 28760, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 53h:14m:08s remains)
INFO - root - 2017-12-07 16:20:05.540457: step 28770, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 60h:53m:27s remains)
INFO - root - 2017-12-07 16:20:12.416887: step 28780, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 61h:17m:08s remains)
INFO - root - 2017-12-07 16:20:18.998434: step 28790, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 56h:06m:15s remains)
INFO - root - 2017-12-07 16:20:25.690010: step 28800, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 54h:26m:23s remains)
2017-12-07 16:20:26.430076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2240286 -4.2158728 -4.2160339 -4.228971 -4.2495403 -4.2676916 -4.2758455 -4.2730203 -4.2548637 -4.2341452 -4.2205009 -4.2211051 -4.2273951 -4.2152014 -4.199707][-4.2276044 -4.2176218 -4.220705 -4.2396564 -4.2637219 -4.2797666 -4.2839255 -4.2782459 -4.2574039 -4.2358332 -4.2207913 -4.2185864 -4.2237549 -4.2089252 -4.190136][-4.2289639 -4.21997 -4.2259974 -4.248136 -4.2716904 -4.2829866 -4.2801042 -4.2718635 -4.2508378 -4.2276258 -4.2166162 -4.2171884 -4.2244883 -4.2096438 -4.1847711][-4.2264442 -4.2227325 -4.2329106 -4.2517352 -4.26573 -4.2675419 -4.2541957 -4.2415824 -4.2249174 -4.2090068 -4.206862 -4.2159181 -4.2296758 -4.215292 -4.1840591][-4.2299948 -4.2287116 -4.2361093 -4.246829 -4.2510905 -4.2413769 -4.215631 -4.1934876 -4.1752005 -4.1644549 -4.1733031 -4.1964908 -4.2217407 -4.2150083 -4.1851239][-4.2407975 -4.237637 -4.2336869 -4.2328796 -4.2271633 -4.206531 -4.1717229 -4.1409974 -4.1212864 -4.1149793 -4.1326127 -4.1695762 -4.2101493 -4.2146873 -4.1902285][-4.2443538 -4.238627 -4.2278624 -4.2198114 -4.2042751 -4.1751304 -4.1321316 -4.0928516 -4.0710611 -4.0675697 -4.09144 -4.1391306 -4.1951122 -4.2138281 -4.1970496][-4.2338862 -4.226903 -4.2203593 -4.2106094 -4.1931329 -4.1646638 -4.1173372 -4.0708776 -4.0468411 -4.0430942 -4.0664062 -4.1192017 -4.1830029 -4.2100968 -4.1993232][-4.2140169 -4.2095795 -4.2150564 -4.2135949 -4.205101 -4.183043 -4.1450748 -4.10163 -4.0765104 -4.0708132 -4.0844507 -4.1266408 -4.1815634 -4.2080069 -4.1971965][-4.1964579 -4.192102 -4.2056632 -4.2173796 -4.2212524 -4.2079687 -4.1854997 -4.155303 -4.1346211 -4.1266584 -4.1262445 -4.1502881 -4.1874628 -4.2002239 -4.1798887][-4.1927853 -4.1831994 -4.1945648 -4.2132955 -4.2268782 -4.2209873 -4.20449 -4.1838279 -4.1731606 -4.1685977 -4.1602411 -4.1715283 -4.1953487 -4.1935797 -4.1632433][-4.2017293 -4.1863518 -4.1923528 -4.2097759 -4.222867 -4.2170658 -4.2025485 -4.1882453 -4.1852441 -4.1842241 -4.1739788 -4.179584 -4.1966982 -4.189466 -4.1580944][-4.2133245 -4.1934791 -4.1969748 -4.2089992 -4.2128234 -4.2048883 -4.19266 -4.1821079 -4.1852174 -4.1866879 -4.1775379 -4.1789985 -4.1916113 -4.1851373 -4.1601644][-4.2365217 -4.2169166 -4.2208176 -4.2248173 -4.2175059 -4.2057519 -4.1945262 -4.1855364 -4.1900892 -4.1933389 -4.187479 -4.1878505 -4.1962514 -4.1901412 -4.1732635][-4.2611403 -4.2488651 -4.2571716 -4.2576714 -4.24571 -4.2336354 -4.2256761 -4.2196717 -4.2222972 -4.2236724 -4.2194829 -4.2164359 -4.2195873 -4.209794 -4.196094]]...]
INFO - root - 2017-12-07 16:20:33.169359: step 28810, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 62h:00m:47s remains)
INFO - root - 2017-12-07 16:20:39.908484: step 28820, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 56h:41m:37s remains)
INFO - root - 2017-12-07 16:20:46.738467: step 28830, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.621 sec/batch; 52h:22m:23s remains)
INFO - root - 2017-12-07 16:20:53.546628: step 28840, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 53h:06m:56s remains)
INFO - root - 2017-12-07 16:21:00.416623: step 28850, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 62h:31m:12s remains)
INFO - root - 2017-12-07 16:21:07.212980: step 28860, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 60h:32m:57s remains)
INFO - root - 2017-12-07 16:21:14.016957: step 28870, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 56h:47m:21s remains)
INFO - root - 2017-12-07 16:21:20.953288: step 28880, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 54h:38m:23s remains)
INFO - root - 2017-12-07 16:21:27.734382: step 28890, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 59h:44m:36s remains)
INFO - root - 2017-12-07 16:21:34.577706: step 28900, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 61h:10m:25s remains)
2017-12-07 16:21:35.349086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1935782 -4.1829209 -4.2060866 -4.2338419 -4.2425046 -4.219296 -4.185935 -4.1751809 -4.1846862 -4.1948905 -4.2001042 -4.1984558 -4.2025356 -4.2109284 -4.2159896][-4.22838 -4.2119646 -4.218049 -4.2314382 -4.2321124 -4.2110572 -4.183857 -4.1714139 -4.1750946 -4.1790428 -4.1804438 -4.1828494 -4.1915708 -4.2029505 -4.2075458][-4.2524686 -4.2301707 -4.2176957 -4.2147307 -4.2064238 -4.1907959 -4.1745524 -4.1656504 -4.162631 -4.1598797 -4.1591425 -4.1656289 -4.1784906 -4.1942005 -4.199708][-4.2561374 -4.2348824 -4.2114043 -4.1952815 -4.1816082 -4.1696429 -4.1618929 -4.1554508 -4.14931 -4.1424446 -4.14036 -4.1482406 -4.1629329 -4.1766014 -4.1797709][-4.2297535 -4.2146049 -4.1917086 -4.17013 -4.1504035 -4.1385651 -4.1363425 -4.1330352 -4.1285243 -4.1205039 -4.1168628 -4.124752 -4.1399465 -4.1516194 -4.1541967][-4.1925125 -4.1844168 -4.1719294 -4.1503081 -4.1203456 -4.1011844 -4.0970492 -4.09865 -4.0986476 -4.0909257 -4.0884614 -4.0954137 -4.112237 -4.1317153 -4.1389623][-4.1857343 -4.1865249 -4.1857314 -4.1633911 -4.1140785 -4.0751858 -4.0618715 -4.0711465 -4.0830092 -4.08207 -4.084631 -4.0925512 -4.1121645 -4.1421056 -4.1548152][-4.213306 -4.2162719 -4.217793 -4.1916146 -4.1300192 -4.0810447 -4.063282 -4.0834556 -4.1084394 -4.114747 -4.1157365 -4.1171904 -4.1386843 -4.1750145 -4.1884823][-4.24807 -4.2457266 -4.2460451 -4.2218895 -4.167191 -4.1250567 -4.1147933 -4.1399794 -4.1673031 -4.1748667 -4.1698689 -4.16099 -4.1778336 -4.2131948 -4.2249994][-4.2734795 -4.2651434 -4.2630963 -4.2428212 -4.2029867 -4.1762443 -4.1749825 -4.1973209 -4.2204857 -4.2305613 -4.2241926 -4.2092543 -4.2192907 -4.2479563 -4.256505][-4.2959208 -4.2895212 -4.28719 -4.267787 -4.2352538 -4.2143664 -4.21425 -4.2316046 -4.25196 -4.2664361 -4.2646685 -4.2522969 -4.2592468 -4.2806492 -4.285449][-4.3172607 -4.3145728 -4.31095 -4.2916417 -4.2600403 -4.2384424 -4.236598 -4.2511711 -4.26963 -4.2874517 -4.2910104 -4.2851005 -4.2920704 -4.3069391 -4.309689][-4.3290577 -4.3260789 -4.3190007 -4.3003182 -4.2728443 -4.2560768 -4.2567458 -4.2691669 -4.2856464 -4.3024654 -4.3087564 -4.3084474 -4.3167334 -4.3282776 -4.3305907][-4.3287549 -4.3241253 -4.3167491 -4.3030753 -4.2861066 -4.2781186 -4.28078 -4.2889848 -4.3015423 -4.3144569 -4.3195171 -4.319633 -4.3265524 -4.3351283 -4.338253][-4.3259568 -4.320888 -4.3151937 -4.3074636 -4.3007536 -4.298727 -4.3016548 -4.3081017 -4.3163047 -4.3226695 -4.3241358 -4.3216777 -4.3253627 -4.3300719 -4.3329692]]...]
INFO - root - 2017-12-07 16:21:42.095260: step 28910, loss = 2.07, batch loss = 2.02 (13.1 examples/sec; 0.612 sec/batch; 51h:34m:13s remains)
INFO - root - 2017-12-07 16:21:48.951722: step 28920, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 59h:28m:37s remains)
INFO - root - 2017-12-07 16:21:55.874204: step 28930, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 58h:33m:55s remains)
INFO - root - 2017-12-07 16:22:02.684913: step 28940, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 56h:04m:09s remains)
INFO - root - 2017-12-07 16:22:09.502218: step 28950, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 53h:33m:44s remains)
INFO - root - 2017-12-07 16:22:16.259816: step 28960, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 54h:14m:16s remains)
INFO - root - 2017-12-07 16:22:23.082055: step 28970, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.757 sec/batch; 63h:50m:23s remains)
INFO - root - 2017-12-07 16:22:29.755451: step 28980, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 59h:39m:55s remains)
INFO - root - 2017-12-07 16:22:36.436004: step 28990, loss = 2.02, batch loss = 1.96 (11.7 examples/sec; 0.684 sec/batch; 57h:41m:27s remains)
INFO - root - 2017-12-07 16:22:43.347548: step 29000, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 53h:10m:13s remains)
2017-12-07 16:22:44.092878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2676048 -4.268785 -4.2814808 -4.2970257 -4.3128505 -4.3216205 -4.3198457 -4.3132076 -4.307806 -4.3033605 -4.2915659 -4.2754536 -4.263763 -4.2556148 -4.2444873][-4.2178831 -4.2223129 -4.2418 -4.2667055 -4.2900438 -4.3010297 -4.2989345 -4.2932329 -4.2902508 -4.2857213 -4.2681751 -4.2388659 -4.2168431 -4.1998544 -4.1819386][-4.1764207 -4.1836147 -4.2094941 -4.2360287 -4.2562413 -4.2660694 -4.2654223 -4.2627358 -4.2649374 -4.2618141 -4.2404819 -4.2054167 -4.1788077 -4.1582379 -4.14109][-4.1587329 -4.1696825 -4.1913362 -4.20687 -4.2109923 -4.2134724 -4.2095213 -4.2049346 -4.21752 -4.2239327 -4.2082696 -4.1793537 -4.1610918 -4.1472607 -4.1399522][-4.147162 -4.1616964 -4.1691914 -4.1649055 -4.1437578 -4.1341558 -4.1207237 -4.1131797 -4.1419692 -4.1665282 -4.1655731 -4.1544237 -4.1518955 -4.1485386 -4.1530304][-4.1437926 -4.150929 -4.1369586 -4.1055517 -4.0507627 -4.0273252 -4.0102048 -4.00164 -4.04991 -4.1022463 -4.1250968 -4.1378822 -4.1513653 -4.1531463 -4.1645441][-4.1475658 -4.1376014 -4.1072984 -4.0622091 -3.9887455 -3.9571157 -3.944567 -3.9390025 -3.9948344 -4.0670371 -4.1158304 -4.1463327 -4.1683393 -4.1708379 -4.1814289][-4.154192 -4.1337667 -4.104075 -4.0649085 -3.9976466 -3.9767787 -3.9794674 -3.9808247 -4.0268612 -4.0884275 -4.1348934 -4.1691542 -4.1974788 -4.2050138 -4.2111163][-4.1564646 -4.1373811 -4.1225104 -4.1016769 -4.0516682 -4.0378618 -4.0474272 -4.0511217 -4.0865016 -4.12997 -4.1593266 -4.1881127 -4.220891 -4.23406 -4.2325][-4.1598015 -4.1395988 -4.1287642 -4.1215162 -4.093049 -4.0769897 -4.0717416 -4.065887 -4.0929241 -4.1325178 -4.1620207 -4.1990671 -4.2369 -4.2543077 -4.2484694][-4.1747003 -4.1506853 -4.1310506 -4.1232357 -4.1075277 -4.0918055 -4.0818944 -4.0734034 -4.097455 -4.1383352 -4.1719456 -4.2135625 -4.256753 -4.2803593 -4.2759695][-4.2022161 -4.1786847 -4.1586313 -4.1480942 -4.1363068 -4.1222544 -4.1117511 -4.1041093 -4.1262388 -4.1689997 -4.2030182 -4.24014 -4.2805257 -4.3050003 -4.3034105][-4.2273769 -4.2128959 -4.199472 -4.193356 -4.1879029 -4.1756563 -4.1664906 -4.1576967 -4.1659918 -4.1948619 -4.22103 -4.2498007 -4.28461 -4.3078566 -4.3105774][-4.2454882 -4.2446055 -4.2433925 -4.24244 -4.23997 -4.2323189 -4.2241192 -4.2110314 -4.2072973 -4.2230673 -4.2379589 -4.2542868 -4.2797523 -4.3010893 -4.3069611][-4.2643285 -4.2725625 -4.2804008 -4.2819982 -4.2775207 -4.2714372 -4.2621546 -4.2467189 -4.2379055 -4.2459354 -4.2529221 -4.2597814 -4.2786822 -4.2999234 -4.3086934]]...]
INFO - root - 2017-12-07 16:22:50.875222: step 29010, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 59h:01m:23s remains)
INFO - root - 2017-12-07 16:22:57.677809: step 29020, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 57h:50m:18s remains)
INFO - root - 2017-12-07 16:23:04.396084: step 29030, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 53h:27m:08s remains)
INFO - root - 2017-12-07 16:23:11.300545: step 29040, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 61h:33m:40s remains)
INFO - root - 2017-12-07 16:23:18.206765: step 29050, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 62h:28m:45s remains)
INFO - root - 2017-12-07 16:23:24.961559: step 29060, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 55h:38m:24s remains)
INFO - root - 2017-12-07 16:23:31.544000: step 29070, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 54h:19m:12s remains)
INFO - root - 2017-12-07 16:23:38.414085: step 29080, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 59h:40m:51s remains)
INFO - root - 2017-12-07 16:23:45.120754: step 29090, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 60h:47m:37s remains)
INFO - root - 2017-12-07 16:23:51.872551: step 29100, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.667 sec/batch; 56h:15m:07s remains)
2017-12-07 16:23:52.664397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2678113 -4.2650762 -4.2644811 -4.2652349 -4.2639513 -4.2600393 -4.2571073 -4.2594314 -4.2683539 -4.2808547 -4.2896566 -4.2896094 -4.2857122 -4.2837772 -4.2816963][-4.24631 -4.2402267 -4.2370625 -4.2327714 -4.2260914 -4.2181239 -4.21477 -4.2208967 -4.2376227 -4.2585459 -4.2749581 -4.281436 -4.2852983 -4.2890115 -4.2863493][-4.2169127 -4.2059059 -4.197051 -4.1848125 -4.1703243 -4.1578622 -4.1555805 -4.1660733 -4.1877418 -4.2161026 -4.2434473 -4.2627263 -4.2798734 -4.2938976 -4.294961][-4.1843629 -4.1637321 -4.1451154 -4.1251173 -4.1037345 -4.0884986 -4.0844917 -4.0928192 -4.1141939 -4.1504073 -4.1922803 -4.22793 -4.2601438 -4.2851558 -4.2931075][-4.1568918 -4.1311722 -4.1042681 -4.0780382 -4.0509205 -4.0286231 -4.0146937 -4.0078974 -4.0206485 -4.0634227 -4.1231351 -4.1811838 -4.2284493 -4.2629924 -4.2784224][-4.1384716 -4.1130548 -4.0849152 -4.0582523 -4.0306625 -3.999737 -3.9622784 -3.92589 -3.926825 -3.9789612 -4.0575223 -4.1318927 -4.1892319 -4.2297134 -4.2513747][-4.1355214 -4.1153803 -4.0910478 -4.0672803 -4.0386124 -3.9931762 -3.9192326 -3.8404431 -3.8313031 -3.8966982 -3.9909556 -4.0771275 -4.1412621 -4.1850939 -4.2109613][-4.146306 -4.1353979 -4.12369 -4.1088328 -4.0837884 -4.0324016 -3.9400723 -3.8383787 -3.8151026 -3.8742955 -3.9601464 -4.0400772 -4.1017694 -4.1456227 -4.1717448][-4.1646218 -4.1628847 -4.16432 -4.1599317 -4.1428704 -4.0991354 -4.023695 -3.9443998 -3.9133143 -3.9385276 -3.9889905 -4.0422258 -4.0896 -4.1304054 -4.1564736][-4.182425 -4.1874652 -4.1957173 -4.1958704 -4.186048 -4.1559653 -4.1090188 -4.0621357 -4.0329542 -4.030004 -4.0459566 -4.0730014 -4.1028304 -4.1395206 -4.1656351][-4.1963487 -4.2039313 -4.2124286 -4.2131352 -4.2080531 -4.1949129 -4.1701965 -4.140203 -4.1112523 -4.0909648 -4.0873442 -4.097837 -4.1195035 -4.1558642 -4.1880727][-4.206202 -4.2096171 -4.2115006 -4.2089767 -4.207478 -4.2068582 -4.19644 -4.1749973 -4.1473646 -4.1239653 -4.1136417 -4.1140075 -4.1342268 -4.1737623 -4.2137794][-4.1990218 -4.1935344 -4.1900225 -4.18963 -4.1958408 -4.2054667 -4.2015285 -4.1832128 -4.1589966 -4.1431365 -4.13815 -4.1396565 -4.1630864 -4.2029238 -4.2441397][-4.1896634 -4.1762829 -4.1644044 -4.1636004 -4.1745925 -4.1874247 -4.1884289 -4.1777191 -4.1620135 -4.1550927 -4.1565318 -4.1668544 -4.1945877 -4.2338395 -4.2723622][-4.18093 -4.1668205 -4.1487727 -4.1426682 -4.148653 -4.1512685 -4.148437 -4.14736 -4.1458082 -4.1516194 -4.1642189 -4.1804676 -4.2089591 -4.242382 -4.2736268]]...]
INFO - root - 2017-12-07 16:23:59.417021: step 29110, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 59h:25m:27s remains)
INFO - root - 2017-12-07 16:24:06.190651: step 29120, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 62h:50m:28s remains)
INFO - root - 2017-12-07 16:24:12.943048: step 29130, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 54h:49m:09s remains)
INFO - root - 2017-12-07 16:24:19.702504: step 29140, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 54h:40m:43s remains)
INFO - root - 2017-12-07 16:24:26.681149: step 29150, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 61h:25m:02s remains)
INFO - root - 2017-12-07 16:24:33.527828: step 29160, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 60h:31m:42s remains)
INFO - root - 2017-12-07 16:24:40.311917: step 29170, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 52h:45m:43s remains)
INFO - root - 2017-12-07 16:24:47.054143: step 29180, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 52h:53m:50s remains)
INFO - root - 2017-12-07 16:24:53.698323: step 29190, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 60h:55m:41s remains)
INFO - root - 2017-12-07 16:25:00.401835: step 29200, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 60h:56m:54s remains)
2017-12-07 16:25:01.090547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3669176 -4.3612413 -4.33721 -4.2899213 -4.2493248 -4.21694 -4.178966 -4.1937504 -4.239676 -4.2573562 -4.2383862 -4.19009 -4.1378689 -4.116375 -4.1249957][-4.3599787 -4.3503361 -4.3240857 -4.2765326 -4.2361484 -4.2046595 -4.1664343 -4.1863852 -4.2417884 -4.2656221 -4.2454119 -4.1914477 -4.1320629 -4.1041212 -4.1135254][-4.3505177 -4.3313737 -4.2924328 -4.2348609 -4.1874981 -4.1531382 -4.1218143 -4.159276 -4.2312131 -4.2678018 -4.2568412 -4.2055674 -4.1421318 -4.1034403 -4.098423][-4.3440795 -4.3149743 -4.2604656 -4.185576 -4.1208124 -4.0718145 -4.0479379 -4.1155505 -4.2124944 -4.2642584 -4.2693925 -4.2319155 -4.17548 -4.1335506 -4.1128531][-4.3417659 -4.3052087 -4.2371206 -4.1444712 -4.054359 -3.9735134 -3.9381151 -4.0380769 -4.1694021 -4.2419467 -4.2686505 -4.2527313 -4.2148509 -4.18337 -4.1554971][-4.341888 -4.2998457 -4.2186508 -4.1066318 -3.9897454 -3.8694034 -3.806057 -3.933908 -4.1040206 -4.2020383 -4.256424 -4.2680626 -4.2507963 -4.2286072 -4.1987438][-4.3404832 -4.2942338 -4.2020407 -4.07385 -3.9391665 -3.7838006 -3.6796262 -3.8201323 -4.0186467 -4.1365824 -4.2189322 -4.26002 -4.26543 -4.2492385 -4.2152014][-4.3388195 -4.29108 -4.1956935 -4.0620251 -3.9270031 -3.763521 -3.6263394 -3.7475395 -3.947341 -4.07401 -4.1757727 -4.2417116 -4.2663646 -4.2576952 -4.2220192][-4.3409352 -4.2980309 -4.2107444 -4.0913396 -3.9801371 -3.8478243 -3.7235458 -3.7961111 -3.9526067 -4.0636973 -4.16512 -4.23936 -4.2731867 -4.2735391 -4.2402139][-4.3468552 -4.30966 -4.2328582 -4.1293483 -4.0424614 -3.9462245 -3.8511953 -3.8943043 -4.0061183 -4.0939441 -4.1824694 -4.2535753 -4.2889967 -4.2945075 -4.26764][-4.357307 -4.3272362 -4.26201 -4.1734772 -4.1048832 -4.0352974 -3.9640605 -3.9940071 -4.074749 -4.1394324 -4.2111244 -4.2754464 -4.3105173 -4.3207455 -4.3043928][-4.3688183 -4.347218 -4.2969728 -4.2256365 -4.1729851 -4.1234927 -4.0735984 -4.0949821 -4.1504641 -4.191813 -4.2427688 -4.2962518 -4.32819 -4.3432527 -4.3370256][-4.3746829 -4.358655 -4.3189354 -4.2606153 -4.21993 -4.1875906 -4.1557817 -4.17028 -4.2056613 -4.2268925 -4.2598524 -4.3017316 -4.3275294 -4.343215 -4.34378][-4.3718753 -4.3555565 -4.3172565 -4.2609353 -4.2239208 -4.2030697 -4.1838169 -4.1986542 -4.2237649 -4.2326803 -4.2562757 -4.2882648 -4.3057933 -4.3209987 -4.3273764][-4.3617373 -4.3389835 -4.2926874 -4.2277136 -4.1856813 -4.1688128 -4.1596308 -4.182816 -4.2085967 -4.2111368 -4.226737 -4.2509546 -4.2628055 -4.2787976 -4.2900229]]...]
INFO - root - 2017-12-07 16:25:07.747141: step 29210, loss = 2.02, batch loss = 1.96 (12.9 examples/sec; 0.621 sec/batch; 52h:18m:04s remains)
INFO - root - 2017-12-07 16:25:14.620193: step 29220, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 60h:36m:24s remains)
INFO - root - 2017-12-07 16:25:21.464695: step 29230, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:15m:50s remains)
INFO - root - 2017-12-07 16:25:28.206194: step 29240, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 55h:56m:05s remains)
INFO - root - 2017-12-07 16:25:34.926492: step 29250, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.654 sec/batch; 55h:04m:16s remains)
INFO - root - 2017-12-07 16:25:41.724642: step 29260, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 55h:32m:29s remains)
INFO - root - 2017-12-07 16:25:48.637187: step 29270, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 60h:36m:30s remains)
INFO - root - 2017-12-07 16:25:55.519610: step 29280, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 60h:40m:46s remains)
INFO - root - 2017-12-07 16:26:02.196106: step 29290, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 56h:29m:25s remains)
INFO - root - 2017-12-07 16:26:08.926866: step 29300, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 53h:21m:29s remains)
2017-12-07 16:26:09.708935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2917213 -4.2941384 -4.2980504 -4.2995162 -4.29769 -4.2921982 -4.27661 -4.2602057 -4.2560358 -4.2628756 -4.2717505 -4.2788906 -4.285562 -4.29696 -4.3130622][-4.2618022 -4.2696691 -4.2784972 -4.2808104 -4.2782578 -4.2698503 -4.2463665 -4.2222829 -4.2198281 -4.2342567 -4.2484837 -4.2570324 -4.2657452 -4.2835016 -4.3071113][-4.2275195 -4.2417812 -4.2553754 -4.2572055 -4.251359 -4.235992 -4.2011366 -4.1702857 -4.1773095 -4.20561 -4.2261786 -4.235301 -4.2453613 -4.268991 -4.299047][-4.1977229 -4.2179761 -4.2345195 -4.2332659 -4.2198205 -4.1889482 -4.1348672 -4.0919337 -4.1137619 -4.1610842 -4.1906085 -4.2042532 -4.2180352 -4.2503538 -4.2877393][-4.1774039 -4.20014 -4.213809 -4.2062826 -4.1801348 -4.1282916 -4.0474291 -3.987571 -4.0246878 -4.092906 -4.1356821 -4.1594954 -4.1811485 -4.2254462 -4.2726369][-4.1732478 -4.1954293 -4.2013931 -4.1835022 -4.1414413 -4.0675755 -3.9618888 -3.8906035 -3.9414048 -4.0273967 -4.0863905 -4.1243415 -4.1558056 -4.2101855 -4.2659521][-4.185812 -4.2094946 -4.2086673 -4.1757193 -4.1156464 -4.0291753 -3.920743 -3.8598537 -3.9187942 -4.0086651 -4.074533 -4.1204138 -4.1566224 -4.2145748 -4.2718272][-4.2205062 -4.2449384 -4.2376332 -4.1909142 -4.1181722 -4.0364237 -3.95647 -3.9279053 -3.9848235 -4.0644512 -4.1254616 -4.1680517 -4.1978931 -4.2461963 -4.2940969][-4.2629857 -4.2897139 -4.278904 -4.2248993 -4.1500244 -4.0868225 -4.0439916 -4.0478272 -4.0992064 -4.16187 -4.2103758 -4.2425838 -4.2622952 -4.2935467 -4.323648][-4.2814527 -4.3088775 -4.2965689 -4.243556 -4.176156 -4.1351657 -4.123816 -4.1477261 -4.1904144 -4.2335062 -4.2673264 -4.2900648 -4.3030114 -4.3221526 -4.3394618][-4.2796745 -4.3040051 -4.2916288 -4.2458868 -4.19608 -4.1783442 -4.1887178 -4.2209435 -4.2528138 -4.2755513 -4.294961 -4.3109269 -4.3184628 -4.3304162 -4.3407865][-4.2763724 -4.2959228 -4.282227 -4.2429914 -4.2106695 -4.21495 -4.24126 -4.2733173 -4.2943416 -4.3009357 -4.3091364 -4.3194284 -4.3222408 -4.329339 -4.3358665][-4.28403 -4.2955718 -4.2796488 -4.2459235 -4.227078 -4.2441072 -4.2777858 -4.3053045 -4.3178363 -4.3163767 -4.3192005 -4.325037 -4.3231211 -4.3266764 -4.3315806][-4.306169 -4.312582 -4.297163 -4.2673869 -4.2544956 -4.27196 -4.3020854 -4.3233161 -4.3310232 -4.3265471 -4.3255358 -4.3261762 -4.3212585 -4.3235083 -4.3293452][-4.3273115 -4.3341465 -4.323297 -4.2999935 -4.2892704 -4.3012977 -4.321888 -4.3360848 -4.3403411 -4.3357639 -4.3335247 -4.3296123 -4.3222322 -4.3235388 -4.3305407]]...]
INFO - root - 2017-12-07 16:26:16.580517: step 29310, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 60h:34m:20s remains)
INFO - root - 2017-12-07 16:26:23.458861: step 29320, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 57h:03m:04s remains)
INFO - root - 2017-12-07 16:26:30.248136: step 29330, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 53h:25m:13s remains)
INFO - root - 2017-12-07 16:26:36.945814: step 29340, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 58h:01m:15s remains)
INFO - root - 2017-12-07 16:26:43.802842: step 29350, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 59h:59m:03s remains)
INFO - root - 2017-12-07 16:26:50.558549: step 29360, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 58h:38m:29s remains)
INFO - root - 2017-12-07 16:26:57.402250: step 29370, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 52h:12m:29s remains)
INFO - root - 2017-12-07 16:27:04.026513: step 29380, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 57h:48m:52s remains)
INFO - root - 2017-12-07 16:27:10.701616: step 29390, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 58h:33m:32s remains)
INFO - root - 2017-12-07 16:27:17.487819: step 29400, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 56h:43m:21s remains)
2017-12-07 16:27:18.234973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2959423 -4.3015761 -4.302639 -4.2951918 -4.288712 -4.2911391 -4.2970409 -4.2962127 -4.2904358 -4.282793 -4.2727513 -4.2705345 -4.2796316 -4.2950907 -4.3160529][-4.2802496 -4.287137 -4.2807503 -4.263114 -4.2486458 -4.2478127 -4.2565365 -4.2614145 -4.2603273 -4.2564383 -4.2480211 -4.2492962 -4.26015 -4.2757254 -4.2994671][-4.2641211 -4.2649727 -4.2476115 -4.2188082 -4.1988606 -4.1980262 -4.2082472 -4.2155743 -4.2193546 -4.2178521 -4.2154737 -4.2271948 -4.2472162 -4.26503 -4.288795][-4.2500114 -4.2436242 -4.2140069 -4.1726956 -4.147109 -4.1498227 -4.1641 -4.1758661 -4.188004 -4.1891689 -4.1919851 -4.2108727 -4.23762 -4.2590346 -4.2838807][-4.244523 -4.2311263 -4.1892991 -4.1305552 -4.0914617 -4.0944862 -4.1169672 -4.1426024 -4.1655674 -4.1728249 -4.1783657 -4.2009296 -4.232183 -4.2569847 -4.2826452][-4.23952 -4.2191439 -4.1672969 -4.0891647 -4.0350652 -4.0416951 -4.0776172 -4.1192293 -4.1513124 -4.1656604 -4.1710534 -4.1956644 -4.232368 -4.2604923 -4.284956][-4.2223248 -4.2010283 -4.1449451 -4.053431 -3.9902329 -3.9972117 -4.0427504 -4.09416 -4.1301026 -4.15158 -4.1609383 -4.1872892 -4.2283978 -4.2620173 -4.2876925][-4.1872349 -4.1707978 -4.1155224 -4.020812 -3.9555635 -3.9627202 -4.0102377 -4.0611873 -4.1036363 -4.135747 -4.1528764 -4.1830196 -4.2278686 -4.2644467 -4.2920561][-4.1555557 -4.1361952 -4.0761256 -3.9837348 -3.9205859 -3.9318502 -3.9838624 -4.034637 -4.0866103 -4.127624 -4.1533432 -4.1887045 -4.2342319 -4.2695355 -4.2976079][-4.1541038 -4.1234522 -4.0548563 -3.9669886 -3.9079077 -3.9238234 -3.9826481 -4.0426488 -4.1053653 -4.1521034 -4.1813297 -4.2129579 -4.2499552 -4.2790174 -4.3036714][-4.1727953 -4.1268544 -4.0550532 -3.9746959 -3.9236588 -3.9456377 -4.0095854 -4.0799055 -4.1483264 -4.1936016 -4.2212772 -4.2461157 -4.2731161 -4.2957549 -4.3150773][-4.2060447 -4.1557422 -4.0844588 -4.013732 -3.9742782 -3.9996419 -4.0628996 -4.1329217 -4.1945491 -4.2350583 -4.2612596 -4.2790914 -4.299613 -4.3168511 -4.32901][-4.2461996 -4.1993246 -4.1346292 -4.0762463 -4.0490761 -4.0736642 -4.1327705 -4.1932597 -4.2430596 -4.2787042 -4.301425 -4.3120055 -4.3247633 -4.3347578 -4.3408637][-4.284441 -4.2470775 -4.1976309 -4.1560416 -4.1418343 -4.164032 -4.2106686 -4.2558546 -4.2917776 -4.3177476 -4.3323793 -4.3353066 -4.3406506 -4.3471966 -4.3504777][-4.3159838 -4.2913651 -4.2598667 -4.2352347 -4.230834 -4.2478905 -4.2778792 -4.3058162 -4.3284569 -4.3430176 -4.3486843 -4.3457603 -4.3483658 -4.3541155 -4.356338]]...]
INFO - root - 2017-12-07 16:27:25.042919: step 29410, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 60h:27m:20s remains)
INFO - root - 2017-12-07 16:27:31.957391: step 29420, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 63h:01m:16s remains)
INFO - root - 2017-12-07 16:27:38.819389: step 29430, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 54h:47m:53s remains)
INFO - root - 2017-12-07 16:27:45.543969: step 29440, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 53h:29m:26s remains)
INFO - root - 2017-12-07 16:27:52.401278: step 29450, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 55h:38m:46s remains)
INFO - root - 2017-12-07 16:27:59.281847: step 29460, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 60h:31m:55s remains)
INFO - root - 2017-12-07 16:28:06.077281: step 29470, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 58h:53m:12s remains)
INFO - root - 2017-12-07 16:28:12.909542: step 29480, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 54h:31m:11s remains)
INFO - root - 2017-12-07 16:28:19.436017: step 29490, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 54h:00m:44s remains)
INFO - root - 2017-12-07 16:28:26.323952: step 29500, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 56h:40m:26s remains)
2017-12-07 16:28:27.102037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2385793 -4.2248459 -4.2146454 -4.2031655 -4.2110705 -4.2254725 -4.2206097 -4.2030649 -4.1855412 -4.178401 -4.1931748 -4.1983123 -4.1891971 -4.169302 -4.1466308][-4.1971893 -4.1828771 -4.173203 -4.1600189 -4.1705866 -4.1903691 -4.1850624 -4.1629524 -4.1432686 -4.144835 -4.1649032 -4.1703157 -4.1622114 -4.1443186 -4.1188564][-4.1553836 -4.1363664 -4.1183176 -4.0921049 -4.0986614 -4.1262221 -4.1308212 -4.1133981 -4.0971084 -4.1124554 -4.1359124 -4.1386938 -4.1321812 -4.122654 -4.1061263][-4.1250181 -4.0975051 -4.0687561 -4.0242491 -4.0234308 -4.0602026 -4.0858517 -4.0816393 -4.077817 -4.1054764 -4.1305151 -4.1302609 -4.1256752 -4.1281471 -4.1221437][-4.1013079 -4.0726981 -4.0404058 -3.9839582 -3.9775407 -4.019978 -4.0624766 -4.0726109 -4.0768385 -4.1142974 -4.1429248 -4.1400061 -4.1373882 -4.1449132 -4.148253][-4.0905209 -4.073947 -4.0483885 -3.9876676 -3.9635592 -3.9828556 -4.0153785 -4.0251312 -4.0393896 -4.0945678 -4.1316786 -4.1312437 -4.1296148 -4.1385689 -4.1542273][-4.0873008 -4.0793591 -4.0580263 -3.9973068 -3.954771 -3.93527 -3.9354453 -3.9356942 -3.9657249 -4.0414114 -4.0868177 -4.088306 -4.0843534 -4.0968142 -4.1261916][-4.07741 -4.0694122 -4.0456128 -3.99201 -3.9433951 -3.8938227 -3.8685889 -3.8648939 -3.9060934 -3.9853539 -4.0271277 -4.02663 -4.0200438 -4.0337119 -4.07284][-4.0810018 -4.0684257 -4.0397873 -3.9947155 -3.9490068 -3.9001064 -3.8748198 -3.8788142 -3.9169879 -3.9718232 -3.9982562 -3.9949837 -3.9823737 -3.9963546 -4.0400786][-4.10159 -4.0881791 -4.0641 -4.0283089 -3.9934359 -3.9644399 -3.9513679 -3.9596479 -3.9822326 -4.0049019 -4.0175538 -4.0121217 -3.9970269 -4.0110273 -4.0561442][-4.1363144 -4.1240945 -4.1081128 -4.0821004 -4.0569835 -4.041276 -4.0376329 -4.0496545 -4.0609736 -4.0653582 -4.0704112 -4.0649366 -4.0540323 -4.0703721 -4.1110659][-4.1900034 -4.1827178 -4.173789 -4.15142 -4.1307325 -4.1182737 -4.1188407 -4.1315165 -4.13998 -4.1358104 -4.1357775 -4.1321988 -4.1282396 -4.1456103 -4.1747432][-4.2380066 -4.2384915 -4.2367988 -4.2183084 -4.2026129 -4.194283 -4.19829 -4.2101593 -4.2179723 -4.2122817 -4.2076974 -4.2037845 -4.2045135 -4.2142715 -4.2274485][-4.2633 -4.26877 -4.2723703 -4.2623553 -4.2528739 -4.2468805 -4.2502551 -4.2617021 -4.2678051 -4.2629013 -4.2560678 -4.2523389 -4.2537193 -4.2569132 -4.2625012][-4.2706976 -4.2772846 -4.2799826 -4.2734842 -4.2693324 -4.2667661 -4.2698064 -4.2793674 -4.2856755 -4.284308 -4.2792878 -4.2764378 -4.2770333 -4.2771878 -4.2800035]]...]
INFO - root - 2017-12-07 16:28:33.845962: step 29510, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 53h:44m:51s remains)
INFO - root - 2017-12-07 16:28:40.612030: step 29520, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 54h:50m:23s remains)
INFO - root - 2017-12-07 16:28:47.323274: step 29530, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 54h:31m:40s remains)
INFO - root - 2017-12-07 16:28:54.169171: step 29540, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:15m:26s remains)
INFO - root - 2017-12-07 16:29:00.991938: step 29550, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.724 sec/batch; 60h:54m:20s remains)
INFO - root - 2017-12-07 16:29:07.875636: step 29560, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 56h:39m:55s remains)
INFO - root - 2017-12-07 16:29:14.588523: step 29570, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.642 sec/batch; 54h:00m:29s remains)
INFO - root - 2017-12-07 16:29:21.448171: step 29580, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 58h:22m:56s remains)
INFO - root - 2017-12-07 16:29:28.126885: step 29590, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.737 sec/batch; 62h:02m:28s remains)
INFO - root - 2017-12-07 16:29:34.943884: step 29600, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 57h:14m:43s remains)
2017-12-07 16:29:35.766594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.205822 -4.2081528 -4.2143822 -4.2259464 -4.2433429 -4.2537966 -4.2578621 -4.2643924 -4.27433 -4.278162 -4.2728519 -4.2531004 -4.2245536 -4.2046971 -4.2007265][-4.2306886 -4.2377505 -4.246069 -4.2525897 -4.2614446 -4.2625294 -4.260982 -4.266758 -4.2775593 -4.28208 -4.2785873 -4.2592325 -4.2256885 -4.1982851 -4.1901612][-4.2649488 -4.2724881 -4.2774539 -4.2746792 -4.269784 -4.2587094 -4.2497659 -4.255291 -4.2707133 -4.279376 -4.2806849 -4.2683473 -4.2404914 -4.2121119 -4.2019267][-4.2759085 -4.2826023 -4.2845135 -4.2736158 -4.2543983 -4.2304945 -4.2164373 -4.2251086 -4.2456546 -4.2619076 -4.274056 -4.2775493 -4.2632117 -4.2414503 -4.22958][-4.2736521 -4.285141 -4.2878432 -4.2651176 -4.2271185 -4.185523 -4.1612267 -4.1671863 -4.1950521 -4.2255983 -4.2578034 -4.2819633 -4.2834377 -4.2688169 -4.2541704][-4.2634525 -4.2804608 -4.2794485 -4.24019 -4.177527 -4.1114731 -4.06354 -4.0628238 -4.1104383 -4.1713591 -4.2303758 -4.276021 -4.296906 -4.2931137 -4.2752872][-4.2348533 -4.2488179 -4.23818 -4.1823659 -4.0944467 -3.9893656 -3.8965359 -3.8790183 -3.9647236 -4.0758719 -4.1675296 -4.2375293 -4.282506 -4.2942457 -4.2807055][-4.213357 -4.2102137 -4.174346 -4.088542 -3.9604127 -3.7994471 -3.6398897 -3.5978131 -3.7393129 -3.9196422 -4.0519018 -4.1503415 -4.223774 -4.2590218 -4.2605166][-4.2309227 -4.2120309 -4.1553211 -4.0513048 -3.8987732 -3.7051456 -3.4998133 -3.4284611 -3.5880985 -3.7995772 -3.954972 -4.0756578 -4.1707921 -4.2263803 -4.2444024][-4.269906 -4.2530551 -4.2106514 -4.1371446 -4.0288496 -3.8908625 -3.7424142 -3.685667 -3.7838254 -3.923131 -4.0255718 -4.1092482 -4.176414 -4.2175756 -4.2314744][-4.3010597 -4.2886839 -4.2630792 -4.2224431 -4.1606579 -4.0806093 -3.9988971 -3.9724636 -4.0261836 -4.0969548 -4.1427388 -4.1799388 -4.2108746 -4.2304535 -4.2317209][-4.2926154 -4.2800884 -4.2610612 -4.2371917 -4.1980376 -4.1508284 -4.1131949 -4.1152663 -4.1580973 -4.1961455 -4.2054172 -4.2117872 -4.2185616 -4.2200804 -4.2095323][-4.2611718 -4.2462053 -4.2285047 -4.2121158 -4.1851726 -4.1581736 -4.1488652 -4.1702962 -4.2100282 -4.2327495 -4.2258487 -4.2174206 -4.2143445 -4.2112174 -4.1973357][-4.2295842 -4.2095609 -4.1922569 -4.1816087 -4.1644473 -4.1555767 -4.167933 -4.2015328 -4.2381463 -4.2506309 -4.2374525 -4.2207685 -4.2129116 -4.2113633 -4.201478][-4.2439008 -4.2269969 -4.2127824 -4.2016091 -4.1845751 -4.1804128 -4.1991425 -4.2329988 -4.2638121 -4.2723336 -4.2594938 -4.2391891 -4.222342 -4.2150555 -4.2111878]]...]
INFO - root - 2017-12-07 16:29:42.513282: step 29610, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 61h:08m:01s remains)
INFO - root - 2017-12-07 16:29:49.257870: step 29620, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 60h:32m:30s remains)
INFO - root - 2017-12-07 16:29:56.024709: step 29630, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.675 sec/batch; 56h:49m:08s remains)
INFO - root - 2017-12-07 16:30:02.806845: step 29640, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 54h:26m:29s remains)
INFO - root - 2017-12-07 16:30:09.500366: step 29650, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 53h:40m:14s remains)
INFO - root - 2017-12-07 16:30:16.381028: step 29660, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.739 sec/batch; 62h:09m:17s remains)
INFO - root - 2017-12-07 16:30:23.310517: step 29670, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.761 sec/batch; 64h:00m:06s remains)
INFO - root - 2017-12-07 16:30:30.190990: step 29680, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 55h:00m:29s remains)
INFO - root - 2017-12-07 16:30:36.411982: step 29690, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 0.509 sec/batch; 42h:48m:45s remains)
INFO - root - 2017-12-07 16:30:43.139366: step 29700, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 57h:33m:32s remains)
2017-12-07 16:30:43.881481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3101139 -4.3096504 -4.2931781 -4.2614841 -4.2031307 -4.1299067 -4.0843229 -4.0751767 -4.0900707 -4.130064 -4.18402 -4.2281289 -4.2554893 -4.2630668 -4.2588916][-4.3065796 -4.3026676 -4.27975 -4.2414284 -4.1846056 -4.1154404 -4.0782871 -4.0705218 -4.0891008 -4.1336722 -4.1897645 -4.232821 -4.2625146 -4.2712502 -4.2688451][-4.2913551 -4.2848616 -4.2575889 -4.215519 -4.1532187 -4.0856347 -4.0508041 -4.0518408 -4.0843811 -4.1312976 -4.192647 -4.2403913 -4.2730908 -4.2833309 -4.2829456][-4.2624722 -4.2575631 -4.2330589 -4.1967549 -4.1353374 -4.0649095 -4.0269213 -4.0412664 -4.0914021 -4.1317959 -4.1879954 -4.2403731 -4.2780585 -4.290977 -4.2906117][-4.2199559 -4.2199025 -4.2063088 -4.1788549 -4.1275578 -4.0597973 -4.018847 -4.0385704 -4.1002021 -4.139431 -4.188096 -4.240746 -4.2788734 -4.290309 -4.287529][-4.1841884 -4.183279 -4.1743464 -4.1535978 -4.1160755 -4.0601907 -4.0244074 -4.0502853 -4.121264 -4.166038 -4.2106047 -4.2566247 -4.2882481 -4.2951889 -4.2883191][-4.1876507 -4.1831722 -4.1725168 -4.1510468 -4.1191678 -4.0699182 -4.0331979 -4.0601735 -4.1369395 -4.1897049 -4.2363162 -4.2768555 -4.3018255 -4.3044977 -4.2952375][-4.2224073 -4.2209067 -4.2100453 -4.1837072 -4.1418815 -4.0785618 -4.0260873 -4.0479388 -4.1268988 -4.1878853 -4.2392755 -4.280118 -4.3026729 -4.3025231 -4.2940831][-4.2573314 -4.2614832 -4.2504888 -4.2178507 -4.1581826 -4.0707498 -4.0005665 -4.0181532 -4.09747 -4.168541 -4.2226667 -4.261322 -4.2771063 -4.2765474 -4.2699971][-4.2743578 -4.2807393 -4.2686477 -4.2307825 -4.1571231 -4.0523977 -3.9673159 -3.9787891 -4.0583096 -4.1339531 -4.1851311 -4.2181077 -4.2301226 -4.2320518 -4.2272453][-4.2640338 -4.2688203 -4.25863 -4.2223306 -4.1526546 -4.0533586 -3.9696062 -3.9738607 -4.0444794 -4.1154594 -4.1584787 -4.1823406 -4.1877794 -4.1887617 -4.1817889][-4.2408104 -4.2400136 -4.2309966 -4.200963 -4.1474638 -4.0764084 -4.0205746 -4.0278788 -4.0855265 -4.1446147 -4.1799035 -4.1884689 -4.1794538 -4.1745372 -4.1639414][-4.2323837 -4.2249327 -4.2155404 -4.1931691 -4.1540828 -4.1060858 -4.0780883 -4.0963893 -4.1457043 -4.1928167 -4.2252412 -4.2244482 -4.2045269 -4.1917343 -4.1783657][-4.2457619 -4.2365952 -4.2293253 -4.2138839 -4.1866512 -4.155582 -4.1501932 -4.1781073 -4.2162356 -4.25143 -4.2787833 -4.2702084 -4.2401128 -4.2174883 -4.2021585][-4.27065 -4.264359 -4.2602968 -4.2525949 -4.2367172 -4.2179127 -4.22311 -4.2533588 -4.2833633 -4.3085494 -4.3253555 -4.3124776 -4.2798758 -4.2507482 -4.2339005]]...]
INFO - root - 2017-12-07 16:30:50.604703: step 29710, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 53h:29m:47s remains)
INFO - root - 2017-12-07 16:30:57.369198: step 29720, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.679 sec/batch; 57h:06m:58s remains)
INFO - root - 2017-12-07 16:31:04.112461: step 29730, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 58h:40m:18s remains)
INFO - root - 2017-12-07 16:31:10.955636: step 29740, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 57h:10m:57s remains)
INFO - root - 2017-12-07 16:31:17.779021: step 29750, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 58h:11m:52s remains)
INFO - root - 2017-12-07 16:31:24.579437: step 29760, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 56h:31m:17s remains)
INFO - root - 2017-12-07 16:31:31.529955: step 29770, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 61h:53m:02s remains)
INFO - root - 2017-12-07 16:31:38.367835: step 29780, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 57h:16m:59s remains)
INFO - root - 2017-12-07 16:31:45.041450: step 29790, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 57h:29m:12s remains)
INFO - root - 2017-12-07 16:31:51.772186: step 29800, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 53h:55m:40s remains)
2017-12-07 16:31:52.501874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2985034 -4.2926946 -4.28781 -4.283329 -4.2833619 -4.2865448 -4.2911148 -4.2960992 -4.2969923 -4.2912674 -4.2790852 -4.26583 -4.2524076 -4.2409406 -4.2327509][-4.272078 -4.2653165 -4.2584934 -4.2504864 -4.2484426 -4.2520995 -4.2586708 -4.2676997 -4.2722273 -4.2713881 -4.2646303 -4.2546821 -4.2425485 -4.2296042 -4.2167721][-4.2508516 -4.2434936 -4.2341146 -4.2230983 -4.2192225 -4.2210279 -4.225141 -4.234375 -4.2420945 -4.2457981 -4.2441912 -4.2386756 -4.2304492 -4.21974 -4.207634][-4.2366967 -4.2226629 -4.2103066 -4.1991687 -4.1938195 -4.191853 -4.1906343 -4.1963983 -4.20763 -4.2199006 -4.228271 -4.2314458 -4.2295146 -4.2255812 -4.22103][-4.2362533 -4.2076478 -4.1849227 -4.1707945 -4.1636376 -4.1551542 -4.1457925 -4.1445785 -4.1553726 -4.174787 -4.1902695 -4.2004185 -4.209065 -4.2188678 -4.230958][-4.2360582 -4.1866827 -4.1413922 -4.1135645 -4.0953169 -4.0679412 -4.0427847 -4.04 -4.0593576 -4.094295 -4.1239305 -4.1443367 -4.165895 -4.1916766 -4.2191443][-4.22665 -4.162292 -4.0913262 -4.0344033 -3.9870458 -3.9294844 -3.8860936 -3.888855 -3.9288337 -3.9837239 -4.0300531 -4.0664845 -4.10851 -4.1536732 -4.1920929][-4.2195921 -4.1632824 -4.0865564 -4.01094 -3.9343061 -3.8526142 -3.803287 -3.8175573 -3.8717666 -3.9255352 -3.9723234 -4.01877 -4.0793428 -4.1373882 -4.1805892][-4.2167683 -4.1867261 -4.1300135 -4.0683913 -3.9969192 -3.9225316 -3.8831277 -3.9005256 -3.9432359 -3.9743261 -4.0022736 -4.0459108 -4.1101155 -4.1647992 -4.2002435][-4.2021189 -4.2052011 -4.1809826 -4.1472874 -4.1015387 -4.0521321 -4.0290956 -4.0434361 -4.0652914 -4.0762038 -4.0913062 -4.1245275 -4.1741042 -4.2117929 -4.2320895][-4.1827674 -4.2116575 -4.2156663 -4.2096729 -4.1946282 -4.1746488 -4.1621814 -4.1665859 -4.1746264 -4.1791763 -4.192564 -4.2150264 -4.2472119 -4.2670841 -4.2731185][-4.1867714 -4.2251821 -4.2429972 -4.2558327 -4.2611032 -4.2599096 -4.2555866 -4.2566938 -4.2612243 -4.2686615 -4.2808695 -4.2961969 -4.3114357 -4.3154435 -4.313313][-4.2284727 -4.2586622 -4.2763371 -4.2966623 -4.30882 -4.313478 -4.3107247 -4.30977 -4.3137245 -4.3209648 -4.3289447 -4.3371587 -4.3429775 -4.3417282 -4.34054][-4.2768335 -4.2936873 -4.3056731 -4.3251863 -4.3381715 -4.3434973 -4.3413148 -4.3396416 -4.34153 -4.3456526 -4.3488445 -4.3512735 -4.3517218 -4.3508091 -4.3536668][-4.3187547 -4.3233352 -4.3293357 -4.3432474 -4.3546743 -4.3587723 -4.3564682 -4.3543162 -4.3535933 -4.3541307 -4.3546052 -4.3547516 -4.3541675 -4.3550458 -4.3585911]]...]
INFO - root - 2017-12-07 16:31:59.382582: step 29810, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 61h:45m:42s remains)
INFO - root - 2017-12-07 16:32:06.145987: step 29820, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 56h:26m:58s remains)
INFO - root - 2017-12-07 16:32:12.893689: step 29830, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 53h:49m:05s remains)
INFO - root - 2017-12-07 16:32:19.677498: step 29840, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 53h:03m:20s remains)
INFO - root - 2017-12-07 16:32:26.548908: step 29850, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.741 sec/batch; 62h:17m:29s remains)
INFO - root - 2017-12-07 16:32:33.352955: step 29860, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:10m:02s remains)
INFO - root - 2017-12-07 16:32:40.152310: step 29870, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 54h:56m:02s remains)
INFO - root - 2017-12-07 16:32:46.933619: step 29880, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 51h:36m:43s remains)
INFO - root - 2017-12-07 16:32:53.620045: step 29890, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 54h:55m:40s remains)
INFO - root - 2017-12-07 16:33:00.453981: step 29900, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.757 sec/batch; 63h:38m:59s remains)
2017-12-07 16:33:01.232031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2351279 -4.2170687 -4.2199616 -4.2326565 -4.2430763 -4.2454486 -4.2347083 -4.2093291 -4.1832247 -4.1869607 -4.2026711 -4.2269678 -4.2459273 -4.2535739 -4.26106][-4.202827 -4.1890211 -4.1933718 -4.2043543 -4.2172351 -4.2288384 -4.2315497 -4.2177582 -4.1988811 -4.2032175 -4.2123275 -4.2241693 -4.2279062 -4.226203 -4.2241836][-4.1763062 -4.1636682 -4.1703296 -4.179863 -4.1882691 -4.1967592 -4.2001481 -4.1947265 -4.1957135 -4.208703 -4.214325 -4.2157574 -4.2139645 -4.2008986 -4.1839533][-4.1485224 -4.1379142 -4.1528568 -4.164587 -4.1664381 -4.1631284 -4.1507106 -4.1329842 -4.1492705 -4.1781635 -4.1886454 -4.1913362 -4.1893177 -4.1658745 -4.1350045][-4.1434712 -4.1213679 -4.1333308 -4.1431727 -4.1372113 -4.11703 -4.07417 -4.0272045 -4.0538483 -4.1169119 -4.1484513 -4.1620054 -4.1642437 -4.1367674 -4.0982456][-4.1533289 -4.1054559 -4.10013 -4.1048865 -4.0951715 -4.0601711 -3.9721727 -3.8641818 -3.8895 -3.9998984 -4.0641088 -4.0979562 -4.1211953 -4.11216 -4.0798216][-4.1574779 -4.0840387 -4.0541492 -4.0465016 -4.0333276 -3.9860601 -3.8618238 -3.6892881 -3.7069623 -3.8740487 -3.9743943 -4.0288296 -4.0743551 -4.0889912 -4.0742831][-4.1776676 -4.0988231 -4.0546947 -4.0473137 -4.0379453 -3.9945855 -3.889622 -3.7526011 -3.7577133 -3.9000332 -3.9950542 -4.0402246 -4.0810642 -4.1030169 -4.1042514][-4.2192354 -4.1605859 -4.1223555 -4.1153135 -4.1088552 -4.0763011 -4.01169 -3.942682 -3.9412787 -4.0217695 -4.0863051 -4.1120315 -4.1382289 -4.1589694 -4.1667371][-4.254447 -4.2234597 -4.2023759 -4.1906934 -4.1794586 -4.1525965 -4.1178875 -4.0932617 -4.0892439 -4.12803 -4.1709914 -4.1938219 -4.2106633 -4.2240009 -4.2302966][-4.2596292 -4.25511 -4.2590556 -4.2547646 -4.2393122 -4.2158833 -4.1959543 -4.1914287 -4.1924748 -4.2110324 -4.234849 -4.2516251 -4.25725 -4.2570877 -4.2601814][-4.2346997 -4.2463403 -4.2640982 -4.273243 -4.2642088 -4.246767 -4.2395196 -4.243885 -4.2453966 -4.2556129 -4.2678552 -4.2730689 -4.2677321 -4.2592492 -4.2567315][-4.2092042 -4.2246046 -4.2439246 -4.2590823 -4.2586465 -4.2487531 -4.2454486 -4.2496953 -4.2521262 -4.2567506 -4.262115 -4.2600322 -4.250185 -4.2390728 -4.2346416][-4.208127 -4.2221184 -4.2360754 -4.2459211 -4.2459993 -4.2378068 -4.2324319 -4.232316 -4.2345161 -4.237833 -4.240767 -4.2409234 -4.2354746 -4.2288723 -4.2264566][-4.2368131 -4.2412043 -4.2451982 -4.2482777 -4.2475829 -4.2404737 -4.2342129 -4.2339549 -4.23612 -4.2389321 -4.2436514 -4.2478905 -4.2473083 -4.2444715 -4.24529]]...]
INFO - root - 2017-12-07 16:33:08.014637: step 29910, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.624 sec/batch; 52h:25m:38s remains)
INFO - root - 2017-12-07 16:33:14.882688: step 29920, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.759 sec/batch; 63h:49m:48s remains)
INFO - root - 2017-12-07 16:33:21.717508: step 29930, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 61h:27m:49s remains)
INFO - root - 2017-12-07 16:33:28.512068: step 29940, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 56h:22m:19s remains)
INFO - root - 2017-12-07 16:33:35.242184: step 29950, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 52h:50m:54s remains)
INFO - root - 2017-12-07 16:33:42.161196: step 29960, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 56h:48m:26s remains)
INFO - root - 2017-12-07 16:33:48.970509: step 29970, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 60h:14m:37s remains)
INFO - root - 2017-12-07 16:33:55.656343: step 29980, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 58h:09m:19s remains)
INFO - root - 2017-12-07 16:34:02.293912: step 29990, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.610 sec/batch; 51h:15m:16s remains)
INFO - root - 2017-12-07 16:34:08.759521: step 30000, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 53h:14m:10s remains)
2017-12-07 16:34:09.490766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1744332 -4.204145 -4.239543 -4.2703414 -4.2934132 -4.31175 -4.3233275 -4.3196325 -4.3085833 -4.3056083 -4.3108873 -4.3148246 -4.3133488 -4.3051028 -4.2943616][-4.1830091 -4.2158084 -4.2571244 -4.290236 -4.3079157 -4.3178129 -4.322515 -4.3182492 -4.3162322 -4.322906 -4.3291545 -4.3269215 -4.3174968 -4.3058667 -4.2929883][-4.197474 -4.2232981 -4.2565079 -4.2795138 -4.2879872 -4.2886109 -4.2857394 -4.2846603 -4.2948046 -4.313549 -4.3243127 -4.3214121 -4.3035645 -4.2800126 -4.2571354][-4.2199607 -4.2313895 -4.2427821 -4.2424293 -4.22977 -4.21603 -4.2088981 -4.2134285 -4.2366052 -4.2674809 -4.2865186 -4.2879939 -4.2671046 -4.2395635 -4.2113109][-4.2426229 -4.2375612 -4.2262568 -4.1991005 -4.1562109 -4.1182423 -4.0983682 -4.1064081 -4.1427197 -4.1864653 -4.2160797 -4.2276893 -4.2179527 -4.2014632 -4.1783352][-4.2626891 -4.2454133 -4.2187624 -4.1725125 -4.0983829 -4.0257835 -3.9826617 -3.9909415 -4.0433121 -4.09973 -4.1402531 -4.1680131 -4.1773758 -4.1776171 -4.1660357][-4.2816434 -4.258893 -4.2247667 -4.1655478 -4.0731206 -3.9847775 -3.9380178 -3.9543023 -4.0151138 -4.071362 -4.1111603 -4.1405721 -4.156806 -4.1685143 -4.1697283][-4.2862396 -4.2577624 -4.2166314 -4.1494 -4.0577459 -3.9835019 -3.9556029 -3.9856749 -4.0477514 -4.0980878 -4.1250405 -4.1397219 -4.1453385 -4.1535478 -4.1590891][-4.2564273 -4.2188311 -4.17508 -4.1134195 -4.0398664 -3.9920585 -3.995579 -4.0431738 -4.1008048 -4.1321559 -4.1338773 -4.11948 -4.1093764 -4.1166339 -4.1293917][-4.2100644 -4.1735063 -4.1404128 -4.1029572 -4.0676055 -4.0531754 -4.0727015 -4.114399 -4.15373 -4.1613145 -4.1372623 -4.09547 -4.0784426 -4.0997128 -4.1334982][-4.1901803 -4.1793675 -4.1709995 -4.1594529 -4.1461859 -4.1460543 -4.1669459 -4.1967111 -4.2126966 -4.2007103 -4.1658907 -4.126842 -4.1224174 -4.1514807 -4.1870208][-4.2259612 -4.2398782 -4.2417693 -4.2390885 -4.232347 -4.2338371 -4.248682 -4.2689228 -4.2744508 -4.2604508 -4.2305155 -4.2012773 -4.197813 -4.2143412 -4.2359786][-4.2556672 -4.2817974 -4.2872076 -4.2855482 -4.2800541 -4.2813234 -4.289247 -4.3004088 -4.3027935 -4.2941823 -4.2731271 -4.2491455 -4.2378411 -4.236474 -4.2420845][-4.2331858 -4.2695618 -4.2851048 -4.2922893 -4.2910562 -4.2915282 -4.2972245 -4.303442 -4.3034415 -4.2981215 -4.280632 -4.2553539 -4.2330303 -4.21846 -4.2120614][-4.1867518 -4.2307606 -4.2531595 -4.2654486 -4.2643189 -4.2617769 -4.2629542 -4.267107 -4.2688894 -4.2690649 -4.2572875 -4.2339158 -4.2076569 -4.1883936 -4.1774678]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 16:34:16.714462: step 30010, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 52h:59m:54s remains)
INFO - root - 2017-12-07 16:34:23.429130: step 30020, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 53h:10m:28s remains)
INFO - root - 2017-12-07 16:34:30.304241: step 30030, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.718 sec/batch; 60h:20m:48s remains)
INFO - root - 2017-12-07 16:34:37.264528: step 30040, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.749 sec/batch; 62h:57m:55s remains)
INFO - root - 2017-12-07 16:34:44.104612: step 30050, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 53h:07m:26s remains)
INFO - root - 2017-12-07 16:34:51.028519: step 30060, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.635 sec/batch; 53h:20m:45s remains)
INFO - root - 2017-12-07 16:34:57.791156: step 30070, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 56h:29m:14s remains)
INFO - root - 2017-12-07 16:35:04.608270: step 30080, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 59h:01m:50s remains)
INFO - root - 2017-12-07 16:35:11.284663: step 30090, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 55h:48m:04s remains)
INFO - root - 2017-12-07 16:35:17.992057: step 30100, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 52h:44m:29s remains)
2017-12-07 16:35:18.735392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.252027 -4.2610688 -4.2650824 -4.2645211 -4.2675252 -4.2789931 -4.2980528 -4.3166389 -4.3219314 -4.312943 -4.2979126 -4.2810593 -4.2642097 -4.2530775 -4.2477055][-4.2298512 -4.2445822 -4.2585654 -4.2661147 -4.2765322 -4.2931528 -4.3141236 -4.3306813 -4.3308792 -4.3189421 -4.3027596 -4.286365 -4.2655678 -4.2520571 -4.2490349][-4.206605 -4.2124791 -4.2207928 -4.2259364 -4.2394514 -4.2643938 -4.295198 -4.3175354 -4.321197 -4.3111053 -4.2961254 -4.2787967 -4.2558188 -4.2363968 -4.2276134][-4.1878266 -4.1703053 -4.1541028 -4.1443281 -4.1576428 -4.1931806 -4.2402358 -4.2757225 -4.289763 -4.2867188 -4.2782536 -4.267632 -4.2493877 -4.2230015 -4.2045641][-4.193728 -4.1538606 -4.1067963 -4.066 -4.0647025 -4.1070967 -4.1691275 -4.2198405 -4.2458792 -4.2523255 -4.2511878 -4.2472553 -4.23812 -4.2120495 -4.1865473][-4.2255659 -4.1822162 -4.1227937 -4.0564737 -4.0249619 -4.0465708 -4.0977983 -4.1498451 -4.1850762 -4.2023826 -4.2112036 -4.2150369 -4.2119713 -4.1879125 -4.15722][-4.2673936 -4.2366972 -4.1871095 -4.1214595 -4.0723333 -4.057251 -4.0686393 -4.0933781 -4.1195846 -4.1405687 -4.1623454 -4.1788912 -4.1798515 -4.1580744 -4.1245856][-4.3014731 -4.2860832 -4.2573204 -4.210392 -4.16555 -4.1321192 -4.1141038 -4.1065121 -4.1056356 -4.1175351 -4.1419692 -4.1654739 -4.1747103 -4.1604505 -4.1272955][-4.3109183 -4.3046184 -4.2935309 -4.269917 -4.2395096 -4.2087154 -4.1877484 -4.1706762 -4.1555905 -4.1550908 -4.1745811 -4.1961064 -4.206666 -4.2019825 -4.1775465][-4.2939653 -4.2899647 -4.2882452 -4.2820821 -4.2672319 -4.2521076 -4.2454042 -4.2380915 -4.2256565 -4.2177806 -4.2312737 -4.2494926 -4.25493 -4.2487144 -4.2303071][-4.2669306 -4.2598038 -4.2572088 -4.2574949 -4.2554092 -4.2552333 -4.2631321 -4.2711291 -4.2700977 -4.2626381 -4.2709637 -4.2848182 -4.2867188 -4.2795935 -4.2669868][-4.24027 -4.2271991 -4.2167873 -4.2097487 -4.21193 -4.2209034 -4.23892 -4.2555566 -4.2626104 -4.2649403 -4.2746458 -4.2891474 -4.2937188 -4.2929916 -4.2893968][-4.22986 -4.2082243 -4.1850886 -4.1646824 -4.1617374 -4.1704855 -4.1956582 -4.2205377 -4.2325783 -4.2477412 -4.266273 -4.2791157 -4.2824216 -4.2862148 -4.29129][-4.2373486 -4.210032 -4.178206 -4.149313 -4.1391697 -4.1453032 -4.1738482 -4.204555 -4.218811 -4.2379117 -4.2604375 -4.2725883 -4.2746778 -4.2787971 -4.2855234][-4.25793 -4.2303586 -4.1989083 -4.1703644 -4.1583734 -4.1614504 -4.1903796 -4.2229662 -4.2351413 -4.2475209 -4.2600861 -4.2686291 -4.2704706 -4.27458 -4.2822709]]...]
INFO - root - 2017-12-07 16:35:25.578973: step 30110, loss = 2.04, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 62h:08m:27s remains)
INFO - root - 2017-12-07 16:35:32.311780: step 30120, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 58h:32m:55s remains)
INFO - root - 2017-12-07 16:35:39.116743: step 30130, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 53h:22m:23s remains)
INFO - root - 2017-12-07 16:35:45.872667: step 30140, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 54h:37m:19s remains)
INFO - root - 2017-12-07 16:35:52.798753: step 30150, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 0.754 sec/batch; 63h:21m:51s remains)
INFO - root - 2017-12-07 16:35:59.629365: step 30160, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.747 sec/batch; 62h:44m:15s remains)
INFO - root - 2017-12-07 16:36:06.405731: step 30170, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 55h:42m:07s remains)
INFO - root - 2017-12-07 16:36:13.301058: step 30180, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.634 sec/batch; 53h:13m:49s remains)
INFO - root - 2017-12-07 16:36:19.958491: step 30190, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 60h:34m:15s remains)
INFO - root - 2017-12-07 16:36:26.798819: step 30200, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 57h:24m:17s remains)
2017-12-07 16:36:27.501160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2623425 -4.2340488 -4.2158313 -4.2037592 -4.1972518 -4.1987205 -4.2107348 -4.2066402 -4.1884165 -4.1782894 -4.1800966 -4.1725249 -4.1684351 -4.1708064 -4.1569748][-4.2694392 -4.23979 -4.220829 -4.2055354 -4.1982141 -4.1985006 -4.2129469 -4.2151446 -4.1981354 -4.1863937 -4.1890492 -4.1857324 -4.1853075 -4.1931605 -4.1871848][-4.2855606 -4.2563009 -4.2361112 -4.2197924 -4.2065334 -4.2011437 -4.2147641 -4.2227917 -4.2071853 -4.195539 -4.1993375 -4.2011724 -4.2058697 -4.2197523 -4.2229223][-4.2993855 -4.2736206 -4.2510395 -4.2323718 -4.2075014 -4.1899962 -4.2013006 -4.218421 -4.2133555 -4.2071075 -4.2116861 -4.2153468 -4.2246313 -4.2415495 -4.2444057][-4.3055234 -4.2816715 -4.253931 -4.227109 -4.1805215 -4.1370759 -4.1445417 -4.1763659 -4.1937943 -4.2067752 -4.2218752 -4.2269254 -4.2338 -4.2470841 -4.2483144][-4.3074932 -4.2826872 -4.2480493 -4.2066579 -4.1285696 -4.0412855 -4.04459 -4.1099396 -4.1599755 -4.1986494 -4.2261586 -4.2295594 -4.2314844 -4.2460823 -4.2487984][-4.3091016 -4.28467 -4.2432046 -4.18837 -4.084784 -3.9609447 -3.9625745 -4.0677109 -4.1455851 -4.1990085 -4.2265306 -4.219676 -4.2138 -4.2309608 -4.2481747][-4.3105097 -4.2903275 -4.2480831 -4.1883168 -4.0866742 -3.9750633 -3.9830415 -4.0846744 -4.1558924 -4.2022839 -4.2213864 -4.2014389 -4.18564 -4.205524 -4.2386751][-4.3081446 -4.2936144 -4.259974 -4.2051897 -4.1263709 -4.0574245 -4.0794172 -4.150106 -4.1942015 -4.2211189 -4.2245083 -4.192811 -4.1663322 -4.1861672 -4.2294025][-4.3018041 -4.2941551 -4.2743783 -4.2362056 -4.1851668 -4.1530166 -4.1800308 -4.226594 -4.2476859 -4.2607222 -4.2560616 -4.2195234 -4.1862359 -4.1993 -4.2369971][-4.2989068 -4.2963257 -4.2868743 -4.2660317 -4.2398696 -4.228106 -4.2482834 -4.277267 -4.2912374 -4.3016253 -4.2960877 -4.2652726 -4.2335978 -4.2421222 -4.2697291][-4.3027697 -4.30276 -4.2972565 -4.2878437 -4.2763352 -4.2734604 -4.2848353 -4.3016844 -4.3122883 -4.32387 -4.3222561 -4.3038673 -4.2831922 -4.2867618 -4.3038635][-4.3099546 -4.31064 -4.3080039 -4.30406 -4.2992063 -4.2968569 -4.3019733 -4.3121557 -4.3235292 -4.3356996 -4.33744 -4.3277783 -4.3142862 -4.3158574 -4.3240037][-4.3165922 -4.3163443 -4.3140278 -4.3114309 -4.3087115 -4.3063293 -4.30802 -4.3143582 -4.323894 -4.3329091 -4.3373 -4.3361406 -4.3301888 -4.3298416 -4.3329649][-4.3278174 -4.3272476 -4.3244958 -4.3216925 -4.3203406 -4.3195157 -4.3195076 -4.3234324 -4.3305182 -4.3362341 -4.339705 -4.3410373 -4.3395972 -4.3382397 -4.3366833]]...]
INFO - root - 2017-12-07 16:36:34.239383: step 30210, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 53h:01m:18s remains)
INFO - root - 2017-12-07 16:36:41.110074: step 30220, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 61h:14m:59s remains)
INFO - root - 2017-12-07 16:36:47.894313: step 30230, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 61h:16m:33s remains)
INFO - root - 2017-12-07 16:36:54.758464: step 30240, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 56h:10m:27s remains)
INFO - root - 2017-12-07 16:37:01.607811: step 30250, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 53h:45m:16s remains)
INFO - root - 2017-12-07 16:37:08.494367: step 30260, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 56h:02m:35s remains)
INFO - root - 2017-12-07 16:37:15.264986: step 30270, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 58h:32m:39s remains)
INFO - root - 2017-12-07 16:37:22.184986: step 30280, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.712 sec/batch; 59h:45m:27s remains)
INFO - root - 2017-12-07 16:37:28.791297: step 30290, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 57h:27m:39s remains)
INFO - root - 2017-12-07 16:37:35.458369: step 30300, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 53h:17m:05s remains)
2017-12-07 16:37:36.180529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3147259 -4.3147426 -4.3155088 -4.3178444 -4.3193979 -4.3190069 -4.3168006 -4.3122954 -4.3047323 -4.2973318 -4.2971659 -4.302547 -4.3100443 -4.3205462 -4.3322158][-4.2972903 -4.2970157 -4.2969704 -4.29894 -4.2983947 -4.2950244 -4.2879338 -4.2783923 -4.2680345 -4.2597556 -4.2614641 -4.2705398 -4.2830553 -4.2993183 -4.315074][-4.2816367 -4.2774358 -4.2713103 -4.2657471 -4.2554188 -4.2429743 -4.2268543 -4.2121577 -4.2028627 -4.2004895 -4.2090378 -4.2251744 -4.2453094 -4.2680764 -4.2884159][-4.26585 -4.2549806 -4.237165 -4.216619 -4.1908083 -4.1682668 -4.1455922 -4.1316075 -4.1285515 -4.137558 -4.1578989 -4.1843925 -4.2127872 -4.238524 -4.2629981][-4.2501326 -4.230691 -4.1988173 -4.1600556 -4.1172032 -4.0898247 -4.07269 -4.069159 -4.0769711 -4.0990553 -4.1286578 -4.1620212 -4.1946349 -4.2200727 -4.2459788][-4.2356405 -4.2072392 -4.1615543 -4.1047649 -4.0487967 -4.0284395 -4.0353022 -4.0510316 -4.0696559 -4.0985646 -4.1271052 -4.1590986 -4.1903315 -4.2144094 -4.2393126][-4.2210836 -4.1848459 -4.1280928 -4.0595727 -3.9932623 -3.9852777 -4.0231624 -4.0623455 -4.0893674 -4.1210828 -4.1444216 -4.1680765 -4.1956015 -4.2169137 -4.2393188][-4.20554 -4.1679435 -4.1081562 -4.0383415 -3.9634089 -3.9538243 -4.0096979 -4.0701632 -4.1099796 -4.1448526 -4.1640105 -4.1795959 -4.2016611 -4.2203841 -4.2415829][-4.1998177 -4.1656919 -4.1116581 -4.0505061 -3.9757617 -3.9547775 -4.007247 -4.0766611 -4.1252666 -4.1617308 -4.1805425 -4.1906662 -4.2071815 -4.2247586 -4.2469206][-4.2101502 -4.1816378 -4.1360807 -4.0889826 -4.0315523 -4.0071988 -4.0415339 -4.1003404 -4.1438322 -4.1746082 -4.1963696 -4.2086411 -4.2236691 -4.2408819 -4.2622681][-4.228291 -4.2052569 -4.171844 -4.1411619 -4.1044674 -4.0844097 -4.0990825 -4.1339984 -4.1625295 -4.1872592 -4.2128854 -4.2307148 -4.2475672 -4.2647181 -4.2849584][-4.2490082 -4.2304668 -4.2091432 -4.1930327 -4.174809 -4.15977 -4.1568651 -4.1690369 -4.1864738 -4.2062063 -4.2305365 -4.2523375 -4.2739649 -4.2929664 -4.3094854][-4.2789297 -4.2662086 -4.2528009 -4.2450314 -4.2365508 -4.2260795 -4.2147632 -4.2149062 -4.2253189 -4.236968 -4.25613 -4.2809405 -4.3052092 -4.3222213 -4.3314877][-4.3065186 -4.2995734 -4.2921824 -4.2865834 -4.2801251 -4.2758355 -4.2685671 -4.2672343 -4.2724714 -4.2789526 -4.290988 -4.3116965 -4.3327374 -4.3456578 -4.3481774][-4.3227262 -4.3190265 -4.3142643 -4.3091354 -4.3033228 -4.3026271 -4.3021746 -4.3044138 -4.3077955 -4.311461 -4.3199172 -4.3335915 -4.34904 -4.3567557 -4.3572063]]...]
INFO - root - 2017-12-07 16:37:42.785985: step 30310, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 59h:24m:20s remains)
INFO - root - 2017-12-07 16:37:49.468816: step 30320, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 52h:58m:51s remains)
INFO - root - 2017-12-07 16:37:56.202105: step 30330, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.666 sec/batch; 55h:54m:59s remains)
INFO - root - 2017-12-07 16:38:03.083743: step 30340, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 61h:30m:16s remains)
INFO - root - 2017-12-07 16:38:09.865290: step 30350, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.749 sec/batch; 62h:50m:48s remains)
INFO - root - 2017-12-07 16:38:16.734833: step 30360, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 57h:52m:05s remains)
INFO - root - 2017-12-07 16:38:23.525478: step 30370, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 55h:13m:12s remains)
INFO - root - 2017-12-07 16:38:30.417850: step 30380, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 59h:17m:27s remains)
INFO - root - 2017-12-07 16:38:37.104919: step 30390, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 59h:34m:06s remains)
INFO - root - 2017-12-07 16:38:43.840053: step 30400, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 56h:51m:05s remains)
2017-12-07 16:38:44.553281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2600565 -4.2544928 -4.2444773 -4.2283473 -4.2031569 -4.1797261 -4.187427 -4.2130513 -4.2364855 -4.2467966 -4.23861 -4.226244 -4.2171569 -4.2121658 -4.213017][-4.2521248 -4.2448335 -4.2319016 -4.2140608 -4.1900845 -4.1668215 -4.1719871 -4.1924748 -4.2098384 -4.2177477 -4.2118335 -4.2022166 -4.1949954 -4.1942077 -4.1998787][-4.2453575 -4.23819 -4.221487 -4.1985483 -4.1714897 -4.148777 -4.1533465 -4.1721721 -4.1870322 -4.19682 -4.1976914 -4.19218 -4.1835637 -4.1810241 -4.185339][-4.2423038 -4.2357783 -4.2161088 -4.1864324 -4.1547117 -4.1322122 -4.1398807 -4.1622171 -4.175848 -4.1873217 -4.1950188 -4.1941972 -4.1810966 -4.1695166 -4.1654139][-4.2368841 -4.2282004 -4.2011275 -4.1621389 -4.1246781 -4.1012135 -4.1144452 -4.1426606 -4.1539159 -4.1646118 -4.1780462 -4.1817207 -4.1672392 -4.1523752 -4.1443324][-4.2202773 -4.20786 -4.1721339 -4.1239586 -4.0774741 -4.0499272 -4.0695424 -4.1064472 -4.1216326 -4.1341543 -4.1538329 -4.1643858 -4.1555448 -4.1441884 -4.1387615][-4.1935067 -4.1791644 -4.1406989 -4.0849562 -4.0238476 -3.9871569 -4.0157418 -4.0721526 -4.1010418 -4.1228061 -4.1503763 -4.1687412 -4.1700974 -4.1629944 -4.1588759][-4.1774178 -4.1617537 -4.1252022 -4.0689306 -4.0033088 -3.9667649 -4.0018287 -4.0730124 -4.1161995 -4.1485267 -4.1821632 -4.205121 -4.2087965 -4.1997089 -4.19276][-4.18413 -4.1697164 -4.1371961 -4.0879774 -4.0350857 -4.0076237 -4.035429 -4.0992794 -4.1458721 -4.1809549 -4.215641 -4.2414141 -4.245388 -4.2336965 -4.2222333][-4.1893373 -4.1822309 -4.1567044 -4.1179414 -4.0842819 -4.0689888 -4.088017 -4.1379676 -4.1809096 -4.2142353 -4.2472444 -4.2721639 -4.274384 -4.2573757 -4.2401533][-4.1756463 -4.1771073 -4.1670914 -4.1434717 -4.1260848 -4.120739 -4.1362119 -4.1778469 -4.217483 -4.2456865 -4.2729077 -4.2908225 -4.2856984 -4.2623467 -4.2409949][-4.1506848 -4.1613731 -4.1702657 -4.1675262 -4.1661949 -4.1681061 -4.179204 -4.2079639 -4.2364635 -4.25671 -4.2772465 -4.2880192 -4.2770138 -4.2511744 -4.2284217][-4.1451635 -4.1647429 -4.1871533 -4.1991639 -4.20858 -4.2160921 -4.2229671 -4.2379136 -4.2502813 -4.2580709 -4.2675757 -4.2702274 -4.255331 -4.2287984 -4.205337][-4.1621861 -4.1842551 -4.2107038 -4.228189 -4.2430372 -4.2512431 -4.2505012 -4.2500453 -4.2463202 -4.2426357 -4.244647 -4.2418933 -4.2258067 -4.2028389 -4.1840639][-4.1813364 -4.2014556 -4.2254281 -4.2438178 -4.25644 -4.2608523 -4.25207 -4.2422085 -4.2340336 -4.2268982 -4.2237759 -4.2162347 -4.2014151 -4.1862011 -4.1762409]]...]
INFO - root - 2017-12-07 16:38:51.355910: step 30410, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 60h:47m:20s remains)
INFO - root - 2017-12-07 16:38:58.108462: step 30420, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 59h:18m:47s remains)
INFO - root - 2017-12-07 16:39:04.927356: step 30430, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 53h:02m:28s remains)
INFO - root - 2017-12-07 16:39:11.790519: step 30440, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 54h:12m:49s remains)
INFO - root - 2017-12-07 16:39:18.587200: step 30450, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 59h:07m:27s remains)
INFO - root - 2017-12-07 16:39:25.421295: step 30460, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 59h:26m:59s remains)
INFO - root - 2017-12-07 16:39:32.283558: step 30470, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 58h:52m:37s remains)
INFO - root - 2017-12-07 16:39:39.022696: step 30480, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 54h:27m:17s remains)
INFO - root - 2017-12-07 16:39:45.671813: step 30490, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 52h:50m:54s remains)
INFO - root - 2017-12-07 16:39:52.503196: step 30500, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 60h:09m:08s remains)
2017-12-07 16:39:53.266644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2291727 -4.2312412 -4.2336521 -4.2361021 -4.2377353 -4.2389336 -4.2408895 -4.241879 -4.2408028 -4.237782 -4.2334466 -4.2294478 -4.2223969 -4.2111363 -4.2015109][-4.2339454 -4.2357407 -4.2379646 -4.2396369 -4.2394757 -4.2389154 -4.2403331 -4.2415972 -4.2412653 -4.2387013 -4.2340622 -4.2302246 -4.2235312 -4.211668 -4.1987109][-4.2385349 -4.2403545 -4.24128 -4.2386789 -4.2325435 -4.2263112 -4.2246275 -4.2261419 -4.227726 -4.2281222 -4.226995 -4.2272916 -4.2254434 -4.2174048 -4.2050238][-4.2447853 -4.2457986 -4.243598 -4.2344546 -4.2169914 -4.1969743 -4.18447 -4.1833534 -4.1894526 -4.1981831 -4.2076678 -4.2188683 -4.2265635 -4.2263713 -4.2193589][-4.2467833 -4.2449412 -4.2379436 -4.2202759 -4.1863079 -4.1436586 -4.1100826 -4.1032948 -4.1211371 -4.14825 -4.1775446 -4.2032351 -4.22034 -4.2289333 -4.2308321][-4.2389889 -4.2293148 -4.2156339 -4.1907115 -4.1415639 -4.0748334 -4.0157747 -4.005332 -4.0429192 -4.0946827 -4.1416936 -4.177002 -4.1995935 -4.2162662 -4.2294869][-4.2116218 -4.1843438 -4.15864 -4.1286259 -4.0713692 -3.9871724 -3.907433 -3.9030573 -3.9630814 -4.0321088 -4.0866318 -4.1274576 -4.1595516 -4.1895919 -4.2173657][-4.178266 -4.1342573 -4.1006732 -4.0752139 -4.0266991 -3.9438152 -3.8634732 -3.8667886 -3.9326932 -4.0015006 -4.0533686 -4.0955529 -4.1356015 -4.1741285 -4.2089725][-4.1809831 -4.1386542 -4.11222 -4.10112 -4.0717583 -4.0118084 -3.953584 -3.9550724 -3.999156 -4.0482545 -4.0897112 -4.125144 -4.1590915 -4.1891408 -4.2159142][-4.213707 -4.183537 -4.1671576 -4.1661291 -4.155992 -4.1255536 -4.0922608 -4.08756 -4.107161 -4.1351862 -4.1616464 -4.1836753 -4.2031951 -4.2175326 -4.2291932][-4.2425504 -4.2233562 -4.213871 -4.2169671 -4.2195997 -4.2121229 -4.1987162 -4.1929574 -4.19853 -4.212657 -4.2255216 -4.2348113 -4.2412562 -4.2420769 -4.2394814][-4.2593908 -4.2505422 -4.2454309 -4.2488108 -4.2568035 -4.2632418 -4.26215 -4.2579064 -4.25612 -4.2594867 -4.26111 -4.2620111 -4.2603135 -4.25201 -4.2399015][-4.2642059 -4.26305 -4.2617116 -4.2645521 -4.2732782 -4.283052 -4.2868457 -4.2851725 -4.28048 -4.2767239 -4.2723665 -4.2683616 -4.26022 -4.244863 -4.2284312][-4.2591858 -4.26045 -4.2611432 -4.263958 -4.2699528 -4.2762446 -4.2792449 -4.2783041 -4.2746534 -4.2697058 -4.2631254 -4.2555389 -4.2430491 -4.2241635 -4.2077303][-4.2450609 -4.2453952 -4.2451811 -4.2456512 -4.2474141 -4.2483096 -4.2474627 -4.2455363 -4.2440605 -4.2416759 -4.2356081 -4.2261243 -4.2105327 -4.1894994 -4.1751237]]...]
INFO - root - 2017-12-07 16:40:00.018525: step 30510, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.611 sec/batch; 51h:14m:17s remains)
INFO - root - 2017-12-07 16:40:06.910170: step 30520, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 55h:51m:30s remains)
INFO - root - 2017-12-07 16:40:13.774093: step 30530, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 60h:23m:34s remains)
INFO - root - 2017-12-07 16:40:20.589443: step 30540, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 62h:14m:19s remains)
INFO - root - 2017-12-07 16:40:27.364906: step 30550, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 55h:38m:59s remains)
INFO - root - 2017-12-07 16:40:34.202911: step 30560, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 53h:48m:29s remains)
INFO - root - 2017-12-07 16:40:41.036793: step 30570, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 54h:39m:20s remains)
INFO - root - 2017-12-07 16:40:47.938288: step 30580, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.758 sec/batch; 63h:33m:53s remains)
INFO - root - 2017-12-07 16:40:54.606245: step 30590, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 58h:11m:55s remains)
INFO - root - 2017-12-07 16:41:01.347857: step 30600, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 54h:32m:27s remains)
2017-12-07 16:41:02.050351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25733 -4.2678328 -4.2869163 -4.2935476 -4.2884941 -4.2854714 -4.2844787 -4.2818174 -4.2817907 -4.2818608 -4.2809491 -4.2896428 -4.3057642 -4.3077078 -4.3032861][-4.30113 -4.3137727 -4.3288388 -4.33607 -4.3328853 -4.3297472 -4.327817 -4.325263 -4.3248949 -4.3238168 -4.3219404 -4.3255663 -4.3350649 -4.3357625 -4.3300915][-4.3230133 -4.332 -4.3412156 -4.3446589 -4.3401217 -4.3350043 -4.3325114 -4.3311119 -4.3299336 -4.3278608 -4.325592 -4.3245916 -4.3297544 -4.3307457 -4.3233166][-4.3163548 -4.3202305 -4.3233757 -4.3204532 -4.3114295 -4.3030782 -4.300705 -4.3030424 -4.3019929 -4.2987709 -4.2959366 -4.2911277 -4.2924623 -4.2914004 -4.2809439][-4.284708 -4.2791986 -4.2755785 -4.2649555 -4.2509513 -4.23751 -4.2337866 -4.2408719 -4.241529 -4.2379274 -4.2352982 -4.227809 -4.2288871 -4.2289767 -4.2189779][-4.2387538 -4.2251854 -4.2193856 -4.2036529 -4.1853247 -4.1624265 -4.1521554 -4.1675467 -4.1737962 -4.1715875 -4.171411 -4.1636739 -4.1678286 -4.1730604 -4.166451][-4.1801686 -4.1590161 -4.1494331 -4.12824 -4.1013165 -4.0603623 -4.036551 -4.0643778 -4.0846219 -4.0869088 -4.090837 -4.085269 -4.0958524 -4.1111741 -4.1128349][-4.1325526 -4.1089611 -4.1001353 -4.0770845 -4.043345 -3.9874468 -3.9461646 -3.9845774 -4.0215025 -4.0300722 -4.0381227 -4.0361867 -4.0490065 -4.0637903 -4.06912][-4.1364546 -4.1266046 -4.1315079 -4.1195369 -4.0968189 -4.0566649 -4.0230913 -4.0486774 -4.0776649 -4.0838385 -4.0901213 -4.0925131 -4.1000743 -4.0988493 -4.0954003][-4.1668773 -4.1712961 -4.1868396 -4.185441 -4.1726604 -4.150919 -4.1315317 -4.1419563 -4.153964 -4.1532407 -4.1544919 -4.1587105 -4.1655154 -4.1576343 -4.1483054][-4.1999183 -4.2114863 -4.2302256 -4.2314572 -4.2229028 -4.2114964 -4.2006116 -4.2016721 -4.2036061 -4.1993003 -4.1978722 -4.2043152 -4.2134352 -4.2062745 -4.1977386][-4.2388434 -4.2509389 -4.2693491 -4.273829 -4.2679186 -4.2609739 -4.2536364 -4.2507634 -4.2501211 -4.247839 -4.2459345 -4.25124 -4.2613192 -4.2578859 -4.2529216][-4.2860742 -4.2965083 -4.3105779 -4.3162308 -4.3122506 -4.3064651 -4.3001413 -4.2964678 -4.2955065 -4.2943797 -4.2934055 -4.2969093 -4.30573 -4.3067584 -4.305131][-4.32813 -4.336431 -4.3445263 -4.3485765 -4.3457642 -4.3402581 -4.3347573 -4.3316374 -4.3304977 -4.3296685 -4.3289094 -4.3315711 -4.3387647 -4.3433657 -4.3461852][-4.3479128 -4.3540993 -4.3579988 -4.3603463 -4.3587208 -4.3551855 -4.3515048 -4.3494678 -4.3485827 -4.34778 -4.3475394 -4.3498583 -4.3555017 -4.3608384 -4.3653708]]...]
INFO - root - 2017-12-07 16:41:08.911415: step 30610, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 61h:01m:51s remains)
INFO - root - 2017-12-07 16:41:15.550574: step 30620, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 56h:44m:49s remains)
INFO - root - 2017-12-07 16:41:22.300261: step 30630, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 53h:49m:59s remains)
INFO - root - 2017-12-07 16:41:29.190094: step 30640, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 60h:24m:33s remains)
INFO - root - 2017-12-07 16:41:35.890595: step 30650, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 56h:28m:52s remains)
INFO - root - 2017-12-07 16:41:42.654879: step 30660, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 57h:37m:22s remains)
INFO - root - 2017-12-07 16:41:49.231853: step 30670, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 54h:26m:00s remains)
INFO - root - 2017-12-07 16:41:55.926815: step 30680, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 54h:20m:32s remains)
INFO - root - 2017-12-07 16:42:02.536170: step 30690, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 54h:20m:13s remains)
INFO - root - 2017-12-07 16:42:09.439306: step 30700, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 59h:58m:38s remains)
2017-12-07 16:42:10.229682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3144765 -4.3061142 -4.3016272 -4.3003826 -4.2985573 -4.2956667 -4.2948017 -4.29431 -4.2941184 -4.2948775 -4.2924991 -4.2887621 -4.2865376 -4.2883153 -4.2913795][-4.2980471 -4.2889261 -4.2862411 -4.2837687 -4.2753081 -4.2684035 -4.268044 -4.2708931 -4.276093 -4.2812486 -4.2825155 -4.2826691 -4.2812023 -4.2816133 -4.2839761][-4.2814403 -4.2746792 -4.2735596 -4.2644572 -4.2423711 -4.2255116 -4.2210927 -4.2261395 -4.2403951 -4.2546306 -4.2648015 -4.273243 -4.2747717 -4.2734513 -4.2758675][-4.2499466 -4.2417817 -4.2388511 -4.2226481 -4.1920938 -4.1686277 -4.1607609 -4.1698532 -4.194715 -4.2201037 -4.2411504 -4.2589183 -4.2655883 -4.2623339 -4.2619934][-4.2014151 -4.1915517 -4.1901526 -4.1776967 -4.1528993 -4.1318684 -4.1219406 -4.1296506 -4.156672 -4.1892705 -4.2127213 -4.2278619 -4.2340274 -4.2297745 -4.2304783][-4.1642766 -4.1571903 -4.1621819 -4.160737 -4.1428452 -4.1179061 -4.0894437 -4.0764918 -4.1022015 -4.1407804 -4.1629481 -4.1710386 -4.17676 -4.1785383 -4.1889267][-4.1654949 -4.1620703 -4.1667886 -4.1635036 -4.1357374 -4.089982 -4.0281124 -3.9864128 -4.0149574 -4.0730524 -4.1060276 -4.1156859 -4.1256104 -4.1403503 -4.1641893][-4.1792684 -4.1724458 -4.1662312 -4.1489229 -4.1118393 -4.0508895 -3.9639621 -3.9009647 -3.9460158 -4.0355668 -4.0875254 -4.0998697 -4.1082263 -4.1308956 -4.164247][-4.1813059 -4.1647496 -4.1491852 -4.1266894 -4.0968814 -4.0516715 -3.993161 -3.9608517 -4.0101943 -4.0934405 -4.1410518 -4.1429639 -4.1403151 -4.1590371 -4.1902413][-4.1689305 -4.1506634 -4.1457672 -4.1406951 -4.1299744 -4.1069403 -4.0799274 -4.0697069 -4.1040931 -4.1603937 -4.1939077 -4.1854682 -4.1712976 -4.1809316 -4.2057891][-4.1633019 -4.1530576 -4.1660519 -4.1778159 -4.1742663 -4.1572971 -4.1403713 -4.1333632 -4.1529779 -4.1915178 -4.2163429 -4.2035356 -4.1851053 -4.19123 -4.2163978][-4.1798911 -4.1754665 -4.1931467 -4.2104673 -4.2097788 -4.193439 -4.1783056 -4.1709733 -4.1841679 -4.2111974 -4.2304139 -4.2201228 -4.2042847 -4.2081494 -4.2310562][-4.2146683 -4.2123489 -4.2249837 -4.2379007 -4.238493 -4.2248831 -4.2124777 -4.2097569 -4.2191224 -4.2350526 -4.2471027 -4.2448959 -4.2352881 -4.2377715 -4.2545433][-4.2532978 -4.2529821 -4.2620397 -4.271513 -4.271596 -4.2623258 -4.2555404 -4.2555175 -4.2611227 -4.2682309 -4.2734842 -4.2724152 -4.2682118 -4.2707834 -4.2811389][-4.2842097 -4.2844253 -4.2918453 -4.3002167 -4.301836 -4.2965178 -4.2930245 -4.2956595 -4.2998939 -4.3030224 -4.303925 -4.3008823 -4.2971911 -4.2977238 -4.3023691]]...]
INFO - root - 2017-12-07 16:42:16.967920: step 30710, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 52h:36m:55s remains)
INFO - root - 2017-12-07 16:42:23.750580: step 30720, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 55h:25m:54s remains)
INFO - root - 2017-12-07 16:42:30.738485: step 30730, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 62h:08m:23s remains)
INFO - root - 2017-12-07 16:42:37.549532: step 30740, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 58h:02m:23s remains)
INFO - root - 2017-12-07 16:42:44.323673: step 30750, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 52h:47m:03s remains)
INFO - root - 2017-12-07 16:42:51.204262: step 30760, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.673 sec/batch; 56h:22m:33s remains)
INFO - root - 2017-12-07 16:42:58.028118: step 30770, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 60h:25m:45s remains)
INFO - root - 2017-12-07 16:43:04.829869: step 30780, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 60h:24m:09s remains)
INFO - root - 2017-12-07 16:43:11.577903: step 30790, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 54h:22m:16s remains)
INFO - root - 2017-12-07 16:43:18.385566: step 30800, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 55h:37m:42s remains)
2017-12-07 16:43:19.104232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2958484 -4.2833123 -4.2761946 -4.2738404 -4.2711449 -4.2676053 -4.2593322 -4.2473187 -4.2338996 -4.2316504 -4.2364283 -4.23727 -4.2314515 -4.2292838 -4.2391791][-4.2804508 -4.2683649 -4.2625313 -4.256422 -4.2459412 -4.2335749 -4.2194457 -4.2026291 -4.1889272 -4.1898079 -4.2008905 -4.2112036 -4.2085037 -4.2060466 -4.2180519][-4.270988 -4.2624159 -4.2606735 -4.2506928 -4.2307749 -4.2066917 -4.1818771 -4.1585226 -4.1465297 -4.1538053 -4.172935 -4.1910591 -4.1893272 -4.1825781 -4.1899109][-4.2624092 -4.2535391 -4.2531152 -4.2397046 -4.2138963 -4.1830406 -4.1498661 -4.1165323 -4.1010218 -4.1173635 -4.1492257 -4.1747055 -4.1760306 -4.1653342 -4.1641722][-4.2566228 -4.2446151 -4.2433276 -4.2284145 -4.1984124 -4.1619463 -4.1158733 -4.0608387 -4.030931 -4.0603433 -4.1095614 -4.1472378 -4.1565094 -4.1498146 -4.1425929][-4.2543578 -4.241045 -4.2395611 -4.2238479 -4.1901593 -4.1421442 -4.071703 -3.9790378 -3.9248993 -3.972738 -4.0506611 -4.1054153 -4.1236353 -4.1233468 -4.1143723][-4.2482481 -4.2333765 -4.2313023 -4.216363 -4.1807227 -4.1326652 -4.0508738 -3.934129 -3.8595121 -3.921916 -4.0168428 -4.0815239 -4.1082029 -4.1107926 -4.1004395][-4.2353649 -4.2205849 -4.2191691 -4.2079206 -4.17575 -4.141192 -4.0838981 -3.9958556 -3.9374604 -3.9846165 -4.0587535 -4.1088214 -4.1310215 -4.1369052 -4.1271563][-4.2243743 -4.2100768 -4.2091665 -4.1992569 -4.1695471 -4.1480613 -4.12401 -4.0777469 -4.0467691 -4.0747523 -4.1172037 -4.1477375 -4.1619687 -4.1660271 -4.1589742][-4.2273636 -4.2166567 -4.2166986 -4.209424 -4.1856847 -4.1693506 -4.1595511 -4.1307364 -4.1122146 -4.1293163 -4.1528077 -4.1718044 -4.1834869 -4.187676 -4.1832604][-4.2466607 -4.2401128 -4.2417884 -4.236021 -4.2141933 -4.2029328 -4.1962857 -4.1719933 -4.157496 -4.1694307 -4.1861606 -4.1954274 -4.2031579 -4.2074327 -4.2054334][-4.2686744 -4.2636933 -4.2656412 -4.2595696 -4.2384429 -4.2303143 -4.2265859 -4.2036347 -4.1899981 -4.2009611 -4.2164116 -4.2235594 -4.23088 -4.2331629 -4.2279673][-4.281477 -4.2793756 -4.2842951 -4.2795553 -4.2637377 -4.2598376 -4.2589355 -4.2414746 -4.23065 -4.2409163 -4.2568088 -4.2647738 -4.27134 -4.2727318 -4.2658744][-4.2890644 -4.2896352 -4.2974391 -4.2986827 -4.2895451 -4.2894926 -4.291821 -4.283658 -4.2762 -4.2826104 -4.2960024 -4.3024564 -4.3069725 -4.3075323 -4.3026662][-4.2968311 -4.2954597 -4.3008652 -4.3048735 -4.3032913 -4.3049355 -4.3084025 -4.305337 -4.3004794 -4.3021064 -4.3100448 -4.3155756 -4.3199568 -4.3200545 -4.3172655]]...]
INFO - root - 2017-12-07 16:43:25.945044: step 30810, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 58h:46m:58s remains)
INFO - root - 2017-12-07 16:43:32.709347: step 30820, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.609 sec/batch; 51h:01m:57s remains)
INFO - root - 2017-12-07 16:43:39.569571: step 30830, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 57h:43m:35s remains)
INFO - root - 2017-12-07 16:43:46.396908: step 30840, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 60h:43m:56s remains)
INFO - root - 2017-12-07 16:43:53.278772: step 30850, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 59h:43m:18s remains)
INFO - root - 2017-12-07 16:44:00.095096: step 30860, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 55h:45m:01s remains)
INFO - root - 2017-12-07 16:44:06.810881: step 30870, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 54h:17m:20s remains)
INFO - root - 2017-12-07 16:44:13.621910: step 30880, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 53h:59m:49s remains)
INFO - root - 2017-12-07 16:44:20.352618: step 30890, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 61h:25m:41s remains)
INFO - root - 2017-12-07 16:44:27.145849: step 30900, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 57h:07m:13s remains)
2017-12-07 16:44:27.829087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1862597 -4.1757011 -4.1746106 -4.1825767 -4.1916604 -4.1999722 -4.2083654 -4.2191091 -4.2259135 -4.2256436 -4.218658 -4.2143364 -4.2103939 -4.197485 -4.1732969][-4.1688867 -4.1583691 -4.164588 -4.1832366 -4.19893 -4.2113042 -4.221343 -4.2280493 -4.2306294 -4.2296472 -4.2266283 -4.2248898 -4.2225056 -4.2101922 -4.1843691][-4.1610284 -4.151053 -4.1664772 -4.19484 -4.2145848 -4.2240939 -4.2289319 -4.2315931 -4.2344084 -4.2352495 -4.234304 -4.2372541 -4.2371941 -4.2229342 -4.1908774][-4.179204 -4.170187 -4.1841955 -4.2095671 -4.2218895 -4.2206435 -4.2147245 -4.2140789 -4.2206659 -4.2257977 -4.2320328 -4.2456093 -4.2526064 -4.236702 -4.1987524][-4.2035141 -4.1988192 -4.2077866 -4.223578 -4.2238994 -4.2057619 -4.1840811 -4.1736298 -4.1777549 -4.1854153 -4.2010622 -4.2321911 -4.2523551 -4.2389426 -4.1996164][-4.218152 -4.2219129 -4.2287407 -4.2372861 -4.2266607 -4.1913033 -4.1485887 -4.1197276 -4.115365 -4.1261363 -4.1509094 -4.1953411 -4.2272806 -4.2208333 -4.1843004][-4.2130256 -4.22437 -4.233387 -4.2403107 -4.2247581 -4.181284 -4.1257172 -4.0795116 -4.0649304 -4.073266 -4.1005225 -4.1460309 -4.1798391 -4.1788526 -4.1497793][-4.1984591 -4.2160411 -4.229497 -4.2379832 -4.2261992 -4.1854672 -4.1272459 -4.0724955 -4.0468273 -4.0485477 -4.073853 -4.1126637 -4.1397762 -4.1402874 -4.1197124][-4.1834607 -4.2015924 -4.2186623 -4.2281618 -4.2211061 -4.1893368 -4.1416292 -4.0927119 -4.0655026 -4.0631742 -4.0835824 -4.1151581 -4.1372638 -4.1361909 -4.12059][-4.1814227 -4.1973352 -4.2137275 -4.2199559 -4.21378 -4.1915021 -4.1633577 -4.1349993 -4.1194158 -4.1170354 -4.1291685 -4.1520658 -4.1712856 -4.1683974 -4.1485758][-4.1991143 -4.2109275 -4.2257438 -4.2282104 -4.2168508 -4.19698 -4.1830235 -4.1772442 -4.1767945 -4.1762972 -4.1845384 -4.20262 -4.2179475 -4.212491 -4.1845613][-4.2367792 -4.2424026 -4.25128 -4.2479396 -4.2305169 -4.2059093 -4.1967378 -4.2039294 -4.2132649 -4.2188306 -4.2267785 -4.2403288 -4.250031 -4.239809 -4.2074432][-4.2832136 -4.2845573 -4.2845354 -4.27362 -4.2537494 -4.2259574 -4.2129779 -4.22014 -4.2304912 -4.2387085 -4.2431016 -4.2492542 -4.2535319 -4.24017 -4.2072759][-4.3118567 -4.3063197 -4.2963552 -4.2793465 -4.2579284 -4.2328367 -4.2168531 -4.2184105 -4.2251759 -4.2337942 -4.2365489 -4.2389565 -4.2390161 -4.2253213 -4.1962724][-4.3081927 -4.2958794 -4.2808671 -4.2618713 -4.2431674 -4.2227154 -4.20702 -4.2017126 -4.1994696 -4.2043042 -4.2084689 -4.2175608 -4.2216535 -4.2130113 -4.191256]]...]
INFO - root - 2017-12-07 16:44:34.638471: step 30910, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 55h:15m:07s remains)
INFO - root - 2017-12-07 16:44:41.392487: step 30920, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 60h:41m:38s remains)
INFO - root - 2017-12-07 16:44:48.011317: step 30930, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.687 sec/batch; 57h:30m:30s remains)
INFO - root - 2017-12-07 16:44:54.787496: step 30940, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.614 sec/batch; 51h:24m:03s remains)
INFO - root - 2017-12-07 16:45:01.545273: step 30950, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 53h:09m:42s remains)
INFO - root - 2017-12-07 16:45:08.446362: step 30960, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.744 sec/batch; 62h:17m:24s remains)
INFO - root - 2017-12-07 16:45:15.372517: step 30970, loss = 2.10, batch loss = 2.04 (10.6 examples/sec; 0.752 sec/batch; 62h:58m:33s remains)
INFO - root - 2017-12-07 16:45:22.145613: step 30980, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 53h:35m:05s remains)
INFO - root - 2017-12-07 16:45:28.811485: step 30990, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.638 sec/batch; 53h:25m:50s remains)
INFO - root - 2017-12-07 16:45:35.500366: step 31000, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:20m:27s remains)
2017-12-07 16:45:36.273195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3293157 -4.3308721 -4.3314748 -4.3299046 -4.3289213 -4.3280478 -4.3275456 -4.3279185 -4.3261628 -4.3226962 -4.3194556 -4.3169127 -4.3155522 -4.3153672 -4.3148551][-4.3286242 -4.3288693 -4.3301091 -4.3272834 -4.3259554 -4.3246031 -4.324152 -4.3235288 -4.3185568 -4.3108487 -4.3045149 -4.2998838 -4.2993946 -4.3022242 -4.30447][-4.3061357 -4.3090644 -4.3092752 -4.3030252 -4.2986922 -4.2921724 -4.2877135 -4.2858796 -4.2807641 -4.2718325 -4.2639832 -4.2600861 -4.2655115 -4.2764964 -4.2851663][-4.266922 -4.2771583 -4.2742667 -4.2584481 -4.2439089 -4.2235746 -4.2067766 -4.20379 -4.2049642 -4.2016129 -4.19766 -4.2013025 -4.2191992 -4.2438965 -4.2623243][-4.2170897 -4.2399917 -4.2366323 -4.2088003 -4.1735644 -4.1252794 -4.0834208 -4.0800614 -4.0979443 -4.1082525 -4.116333 -4.1364923 -4.17358 -4.2144547 -4.24234][-4.1560903 -4.1962571 -4.1978855 -4.162456 -4.1001325 -4.01084 -3.9338515 -3.9361808 -3.9852061 -4.0211 -4.0535736 -4.0994892 -4.1556168 -4.204792 -4.2351871][-4.10969 -4.169621 -4.1785412 -4.1365418 -4.0497003 -3.9222474 -3.8179345 -3.8335221 -3.9179134 -3.9880428 -4.0507994 -4.1185813 -4.180016 -4.2230792 -4.245265][-4.1049809 -4.1718669 -4.184566 -4.1441445 -4.0546837 -3.9287765 -3.8384936 -3.8624077 -3.9491851 -4.0295444 -4.1024132 -4.1734085 -4.2266765 -4.2563214 -4.2652636][-4.1417065 -4.19984 -4.2118082 -4.1790781 -4.1027489 -4.008297 -3.955889 -3.9785542 -4.0406361 -4.1042104 -4.1656861 -4.2249007 -4.26407 -4.2799282 -4.2795711][-4.1902633 -4.2378945 -4.2499428 -4.226141 -4.1694574 -4.1077404 -4.0835061 -4.0984721 -4.1330791 -4.1736922 -4.2170019 -4.2578983 -4.2828746 -4.2894454 -4.2857475][-4.2328181 -4.2691832 -4.278789 -4.2622452 -4.2239027 -4.187583 -4.1777654 -4.1853495 -4.2008591 -4.2248693 -4.2520084 -4.2766109 -4.2908025 -4.2927284 -4.2889757][-4.2630692 -4.28725 -4.2919269 -4.2797422 -4.256762 -4.2395639 -4.238359 -4.242466 -4.2475553 -4.2597089 -4.2753558 -4.2885275 -4.2954063 -4.2949414 -4.291779][-4.2829304 -4.2966466 -4.2974324 -4.2893982 -4.2782345 -4.2721872 -4.2736087 -4.275054 -4.2756519 -4.280601 -4.2884059 -4.2948341 -4.2972279 -4.2956629 -4.2926483][-4.2946129 -4.3006263 -4.2988272 -4.2940607 -4.2891822 -4.2872128 -4.2882318 -4.2884531 -4.28713 -4.2880797 -4.2910457 -4.2939606 -4.2944069 -4.2923141 -4.2889404][-4.297811 -4.2999263 -4.2963839 -4.293138 -4.2906046 -4.2897024 -4.2902513 -4.2899318 -4.2883983 -4.2879429 -4.28852 -4.2897372 -4.2891645 -4.2870193 -4.2841]]...]
INFO - root - 2017-12-07 16:45:43.034937: step 31010, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 56h:40m:34s remains)
INFO - root - 2017-12-07 16:45:49.813945: step 31020, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 56h:58m:24s remains)
INFO - root - 2017-12-07 16:45:56.630054: step 31030, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 56h:44m:33s remains)
INFO - root - 2017-12-07 16:46:03.451097: step 31040, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.749 sec/batch; 62h:45m:40s remains)
INFO - root - 2017-12-07 16:46:10.282210: step 31050, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 61h:36m:18s remains)
INFO - root - 2017-12-07 16:46:16.969663: step 31060, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 54h:34m:52s remains)
INFO - root - 2017-12-07 16:46:23.797256: step 31070, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.623 sec/batch; 52h:10m:57s remains)
INFO - root - 2017-12-07 16:46:30.663805: step 31080, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 58h:15m:52s remains)
INFO - root - 2017-12-07 16:46:37.327841: step 31090, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 58h:05m:44s remains)
INFO - root - 2017-12-07 16:46:44.078322: step 31100, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 58h:16m:35s remains)
2017-12-07 16:46:44.769462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1963806 -4.1707768 -4.1518426 -4.1260643 -4.1157656 -4.1348648 -4.1752424 -4.2162137 -4.2453828 -4.2633696 -4.2704926 -4.262403 -4.2402062 -4.2073793 -4.1833239][-4.19528 -4.1709814 -4.1613097 -4.1461134 -4.1453285 -4.1711774 -4.2139344 -4.2514172 -4.2710614 -4.2791915 -4.2842703 -4.281518 -4.2640615 -4.2310591 -4.2017841][-4.1989732 -4.1842241 -4.1885686 -4.186172 -4.1926341 -4.2157807 -4.2483935 -4.2743607 -4.2863064 -4.2905726 -4.2956128 -4.2967925 -4.2849793 -4.2542915 -4.2235637][-4.2038445 -4.1989756 -4.2150745 -4.2198424 -4.2218347 -4.2326789 -4.2501392 -4.2632294 -4.2733569 -4.2813606 -4.2936182 -4.3052859 -4.2986259 -4.2694077 -4.2373509][-4.19869 -4.1934304 -4.2096248 -4.2115812 -4.1965938 -4.1840134 -4.1806521 -4.1808548 -4.1927772 -4.2213 -4.2598486 -4.2933173 -4.29807 -4.2739267 -4.244504][-4.1982632 -4.1884847 -4.1951938 -4.1853237 -4.1461596 -4.1016169 -4.0694089 -4.0494442 -4.0636482 -4.1232939 -4.2022533 -4.2661166 -4.2903705 -4.2786207 -4.2538733][-4.2001181 -4.1858954 -4.1840053 -4.159121 -4.095036 -4.0207615 -3.9530981 -3.9117293 -3.9321585 -4.0301309 -4.1509104 -4.2390122 -4.2754064 -4.2723184 -4.2484121][-4.1971846 -4.1844573 -4.1840692 -4.1526194 -4.0788307 -3.9913316 -3.9077935 -3.855979 -3.8798265 -3.997493 -4.142992 -4.2404437 -4.2782664 -4.2746377 -4.249567][-4.1916575 -4.1834307 -4.1900663 -4.1641331 -4.0998292 -4.02846 -3.9598022 -3.9185305 -3.9450414 -4.0538945 -4.1861367 -4.2667713 -4.2909102 -4.28112 -4.2555838][-4.1911306 -4.1898642 -4.2063823 -4.1999888 -4.1615057 -4.1126232 -4.0695405 -4.0460153 -4.0740967 -4.1595955 -4.2546473 -4.2983217 -4.2983174 -4.2803807 -4.2554665][-4.2016048 -4.2011914 -4.2225246 -4.2352862 -4.2232552 -4.1942925 -4.16946 -4.1596508 -4.1891146 -4.2526727 -4.3094983 -4.317265 -4.2972355 -4.274971 -4.2570987][-4.2013578 -4.1917887 -4.2099118 -4.2342777 -4.2437468 -4.2354302 -4.2280955 -4.2303219 -4.2598643 -4.3028131 -4.3320789 -4.3200407 -4.29163 -4.2683582 -4.2605138][-4.2090797 -4.1929059 -4.2054214 -4.23191 -4.2524738 -4.2594361 -4.26588 -4.2769208 -4.3025775 -4.326251 -4.3324957 -4.3081212 -4.2750468 -4.2537823 -4.2531166][-4.223207 -4.2111368 -4.2217536 -4.2431464 -4.2651305 -4.278019 -4.2907476 -4.3037939 -4.3212905 -4.3289075 -4.3206735 -4.2942328 -4.2660465 -4.2486706 -4.2453814][-4.2325039 -4.2257643 -4.2406273 -4.2624803 -4.2830777 -4.2966666 -4.3079138 -4.31888 -4.3269672 -4.3245077 -4.312418 -4.2914314 -4.2696767 -4.2510223 -4.2373934]]...]
INFO - root - 2017-12-07 16:46:51.591683: step 31110, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 57h:13m:08s remains)
INFO - root - 2017-12-07 16:46:58.490241: step 31120, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 60h:30m:45s remains)
INFO - root - 2017-12-07 16:47:05.427498: step 31130, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 58h:22m:54s remains)
INFO - root - 2017-12-07 16:47:12.191871: step 31140, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 56h:02m:10s remains)
INFO - root - 2017-12-07 16:47:18.938814: step 31150, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 54h:04m:44s remains)
INFO - root - 2017-12-07 16:47:25.863097: step 31160, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 59h:28m:49s remains)
INFO - root - 2017-12-07 16:47:32.806018: step 31170, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 61h:05m:07s remains)
INFO - root - 2017-12-07 16:47:39.642917: step 31180, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 57h:16m:10s remains)
INFO - root - 2017-12-07 16:47:46.315501: step 31190, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 54h:37m:09s remains)
INFO - root - 2017-12-07 16:47:53.061366: step 31200, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 53h:47m:23s remains)
2017-12-07 16:47:53.890355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1839447 -4.1634622 -4.1547761 -4.157701 -4.1650724 -4.1747584 -4.1889524 -4.2057047 -4.2201614 -4.2310963 -4.2446213 -4.2586613 -4.2613444 -4.2547464 -4.24878][-4.1711273 -4.1564336 -4.1521773 -4.1519046 -4.1496067 -4.1482573 -4.1554403 -4.1706524 -4.1913943 -4.2116933 -4.2319989 -4.2480793 -4.244719 -4.2281575 -4.2125187][-4.1655331 -4.1616955 -4.1639972 -4.159205 -4.1440387 -4.1281757 -4.1244354 -4.1356006 -4.1614175 -4.1907043 -4.2150679 -4.2282572 -4.2176018 -4.1912813 -4.1685266][-4.1580057 -4.1652389 -4.173202 -4.1680422 -4.1492672 -4.12318 -4.1060877 -4.1054006 -4.1292582 -4.1595097 -4.1810856 -4.1890688 -4.1792822 -4.1526484 -4.1325707][-4.1537189 -4.1675615 -4.1766043 -4.1720724 -4.1495681 -4.1111455 -4.0702772 -4.0418797 -4.0552473 -4.0976543 -4.1249924 -4.137444 -4.1353931 -4.1226172 -4.114893][-4.1546245 -4.1668115 -4.1684756 -4.1513305 -4.1134777 -4.0516667 -3.9698486 -3.8981493 -3.9094107 -3.9855773 -4.0427966 -4.0714493 -4.0814419 -4.0860605 -4.0916691][-4.1640711 -4.1690788 -4.1553845 -4.1084342 -4.0389767 -3.9435167 -3.8151271 -3.6970093 -3.7204275 -3.856935 -3.9626172 -4.0134759 -4.0377703 -4.0576386 -4.0750904][-4.182538 -4.1766539 -4.1482158 -4.0792189 -3.9841344 -3.8669987 -3.7223856 -3.6034839 -3.6563439 -3.8329787 -3.963491 -4.0247545 -4.055469 -4.0812588 -4.1024179][-4.2009864 -4.1876097 -4.1577139 -4.0913148 -3.9993923 -3.8944142 -3.78801 -3.7267983 -3.7892518 -3.9318495 -4.0349908 -4.0843973 -4.1127529 -4.1373458 -4.1567469][-4.2246757 -4.2088079 -4.1878667 -4.1406651 -4.0737095 -3.9990344 -3.9385319 -3.9209752 -3.9689503 -4.0565767 -4.1198111 -4.1512146 -4.17291 -4.1911855 -4.2059741][-4.2534308 -4.2394996 -4.2312651 -4.208344 -4.1702576 -4.126945 -4.0957355 -4.0932279 -4.1141992 -4.1555386 -4.1876564 -4.2071319 -4.2232013 -4.2340765 -4.24256][-4.2748184 -4.26734 -4.2705054 -4.2665734 -4.251709 -4.2305169 -4.2172337 -4.2144551 -4.2156167 -4.2293096 -4.2428384 -4.2526283 -4.2580094 -4.2575908 -4.2564831][-4.2754602 -4.2797341 -4.293334 -4.302515 -4.3023219 -4.2955484 -4.29026 -4.2856374 -4.2778907 -4.2744932 -4.2755313 -4.2755466 -4.2674875 -4.252492 -4.2400079][-4.2539525 -4.2722006 -4.2940564 -4.3116665 -4.3216457 -4.3213487 -4.3184075 -4.3124638 -4.2983379 -4.2838511 -4.27388 -4.2625976 -4.2440953 -4.2185049 -4.1990542][-4.2061868 -4.2351975 -4.2670403 -4.2927508 -4.3098407 -4.313529 -4.3086209 -4.2971411 -4.2749186 -4.2495923 -4.2299123 -4.2118468 -4.1910586 -4.1655078 -4.1467681]]...]
INFO - root - 2017-12-07 16:48:00.777778: step 31210, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 62h:07m:04s remains)
INFO - root - 2017-12-07 16:48:07.477671: step 31220, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 53h:22m:47s remains)
INFO - root - 2017-12-07 16:48:14.150051: step 31230, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 55h:07m:43s remains)
INFO - root - 2017-12-07 16:48:20.875916: step 31240, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.719 sec/batch; 60h:09m:37s remains)
INFO - root - 2017-12-07 16:48:27.516368: step 31250, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 55h:19m:38s remains)
INFO - root - 2017-12-07 16:48:34.269320: step 31260, loss = 2.10, batch loss = 2.05 (12.3 examples/sec; 0.649 sec/batch; 54h:19m:37s remains)
INFO - root - 2017-12-07 16:48:41.021927: step 31270, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 52h:45m:11s remains)
INFO - root - 2017-12-07 16:48:47.863458: step 31280, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 56h:51m:08s remains)
INFO - root - 2017-12-07 16:48:54.444035: step 31290, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 60h:18m:20s remains)
INFO - root - 2017-12-07 16:49:01.175786: step 31300, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 58h:16m:34s remains)
2017-12-07 16:49:01.945905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17612 -4.1563783 -4.1393728 -4.1384945 -4.1599622 -4.1912827 -4.2189722 -4.2372112 -4.2404475 -4.2261171 -4.2075081 -4.1913619 -4.1767073 -4.1650486 -4.1553049][-4.2078338 -4.1921072 -4.1798058 -4.1816206 -4.2027292 -4.228394 -4.2471242 -4.2575378 -4.2565827 -4.2425609 -4.2251425 -4.2080507 -4.1941161 -4.1841688 -4.1754918][-4.2223873 -4.2199464 -4.2178159 -4.2230172 -4.2382708 -4.2522545 -4.2598209 -4.263278 -4.2633467 -4.2566166 -4.2482395 -4.2365451 -4.2265787 -4.2201538 -4.2147889][-4.2187805 -4.23118 -4.237041 -4.241199 -4.2454329 -4.244266 -4.2409296 -4.2412834 -4.2463975 -4.2502866 -4.2533641 -4.2503085 -4.2474546 -4.2470264 -4.2462626][-4.21013 -4.2266083 -4.2309232 -4.2276444 -4.2176542 -4.2017694 -4.1902027 -4.1914558 -4.2042327 -4.2198434 -4.2353425 -4.2440543 -4.2531171 -4.2607265 -4.2632375][-4.2071118 -4.2138896 -4.2085476 -4.1940837 -4.1698933 -4.1422658 -4.1251507 -4.1300611 -4.1518817 -4.1774545 -4.2035513 -4.2246828 -4.2456098 -4.259891 -4.2602973][-4.216104 -4.20748 -4.1884718 -4.1601977 -4.1234007 -4.086997 -4.0668755 -4.0748134 -4.1018639 -4.1322889 -4.1639791 -4.192946 -4.2227263 -4.2399397 -4.2358727][-4.2368145 -4.2169676 -4.187345 -4.1497388 -4.1083703 -4.0731821 -4.0567207 -4.0646782 -4.0896149 -4.11685 -4.1440229 -4.1706486 -4.1995568 -4.2149243 -4.2062082][-4.2650561 -4.2413273 -4.2080069 -4.1697197 -4.133461 -4.1067996 -4.0970592 -4.1031919 -4.120996 -4.1380448 -4.1534657 -4.1714153 -4.192616 -4.1977663 -4.1805291][-4.2901783 -4.2688956 -4.2390652 -4.2067928 -4.1798258 -4.1617241 -4.1545186 -4.1541524 -4.1623154 -4.1700773 -4.1753478 -4.1844783 -4.1964569 -4.1899924 -4.1630588][-4.303678 -4.2885637 -4.2667789 -4.2443233 -4.2271061 -4.2150488 -4.2075033 -4.2012358 -4.2017245 -4.2022619 -4.1988935 -4.2002587 -4.2035642 -4.1883368 -4.1537824][-4.304163 -4.2971334 -4.2859087 -4.2751627 -4.2678323 -4.2610416 -4.2535095 -4.2464705 -4.2437649 -4.2389784 -4.2288628 -4.2225718 -4.2154937 -4.1909766 -4.1512465][-4.2925119 -4.2939944 -4.2938194 -4.2944202 -4.2962403 -4.2956324 -4.2908664 -4.2859139 -4.2814341 -4.2723184 -4.256568 -4.2421007 -4.2250648 -4.1933942 -4.1504827][-4.2745595 -4.2802134 -4.2876778 -4.2959485 -4.3028908 -4.3054647 -4.3030763 -4.3011408 -4.2958827 -4.2837892 -4.2654161 -4.2448559 -4.2215228 -4.1861629 -4.142467][-4.256763 -4.2611737 -4.2688446 -4.2768235 -4.282145 -4.2829 -4.2804585 -4.2808657 -4.2786064 -4.2695069 -4.2530241 -4.2321281 -4.2062359 -4.1705546 -4.1279969]]...]
INFO - root - 2017-12-07 16:49:08.694830: step 31310, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 55h:48m:56s remains)
INFO - root - 2017-12-07 16:49:15.422560: step 31320, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 55h:17m:29s remains)
INFO - root - 2017-12-07 16:49:22.193455: step 31330, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 56h:07m:26s remains)
INFO - root - 2017-12-07 16:49:28.917255: step 31340, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 56h:37m:28s remains)
INFO - root - 2017-12-07 16:49:35.748191: step 31350, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 54h:13m:54s remains)
INFO - root - 2017-12-07 16:49:42.538117: step 31360, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 60h:25m:50s remains)
INFO - root - 2017-12-07 16:49:49.352532: step 31370, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 60h:41m:23s remains)
INFO - root - 2017-12-07 16:49:56.095227: step 31380, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.693 sec/batch; 57h:55m:51s remains)
INFO - root - 2017-12-07 16:50:02.665762: step 31390, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.620 sec/batch; 51h:53m:52s remains)
INFO - root - 2017-12-07 16:50:09.406693: step 31400, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 55h:41m:03s remains)
2017-12-07 16:50:10.133257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1910539 -4.1494927 -4.0917411 -4.0448651 -4.0383167 -4.0803552 -4.1440635 -4.2005754 -4.2367997 -4.2555013 -4.2668948 -4.2732649 -4.2783723 -4.2848549 -4.2941942][-4.1981273 -4.1649575 -4.1150355 -4.0727735 -4.0669603 -4.1040573 -4.15807 -4.2066679 -4.2410903 -4.2606583 -4.2723675 -4.2792606 -4.2837281 -4.2899113 -4.299017][-4.2146349 -4.1954727 -4.1620517 -4.135808 -4.13354 -4.1569376 -4.1871929 -4.2178965 -4.2477283 -4.2674279 -4.2776856 -4.283031 -4.2884388 -4.2945843 -4.3005538][-4.2206364 -4.2121968 -4.1932945 -4.1806684 -4.1821609 -4.1905241 -4.1953831 -4.2105718 -4.2350569 -4.2547164 -4.2658763 -4.2747145 -4.2855697 -4.2923222 -4.2944136][-4.2115769 -4.2065997 -4.1942639 -4.1906157 -4.1919703 -4.18239 -4.16574 -4.1737623 -4.2025242 -4.2280016 -4.2457881 -4.260118 -4.2744889 -4.280386 -4.2784767][-4.1923208 -4.1831493 -4.1697712 -4.1688151 -4.1642962 -4.1363168 -4.0972757 -4.1030293 -4.1476636 -4.1871238 -4.2146406 -4.2338982 -4.2497554 -4.2540145 -4.2477465][-4.1633568 -4.1442413 -4.126215 -4.1219735 -4.1069593 -4.0539441 -3.98691 -3.9932418 -4.0644026 -4.1318016 -4.1764894 -4.2019668 -4.219274 -4.2210426 -4.2080407][-4.1307445 -4.1031523 -4.0810146 -4.0694704 -4.0432577 -3.9703135 -3.8852181 -3.8994503 -4.0004611 -4.0952969 -4.1515889 -4.1779723 -4.1928706 -4.1895547 -4.1673555][-4.1177335 -4.0886345 -4.0688663 -4.0560384 -4.029635 -3.9656003 -3.8990657 -3.9258933 -4.0245762 -4.1110387 -4.1559548 -4.170948 -4.1755652 -4.1618657 -4.1256237][-4.1415524 -4.1173625 -4.1035624 -4.0952382 -4.0774236 -4.0397491 -4.0030155 -4.0250454 -4.0924606 -4.1500864 -4.1755242 -4.1774611 -4.1713729 -4.1476865 -4.0980315][-4.1892891 -4.1707473 -4.158464 -4.1509504 -4.1395278 -4.1203794 -4.101984 -4.1131744 -4.1492863 -4.1797156 -4.1888461 -4.182622 -4.169776 -4.1419649 -4.0913258][-4.23287 -4.2187829 -4.2085919 -4.20212 -4.1941247 -4.1824794 -4.1698995 -4.1713514 -4.1876707 -4.201745 -4.2018394 -4.1926842 -4.1814284 -4.1625109 -4.1297731][-4.2576027 -4.2497368 -4.2443209 -4.2402062 -4.2348928 -4.2274346 -4.2180519 -4.2166138 -4.2246318 -4.2314396 -4.2288795 -4.2231445 -4.2182631 -4.2096329 -4.193481][-4.2638931 -4.2623243 -4.2625523 -4.2617383 -4.2583613 -4.2541323 -4.2493773 -4.2498522 -4.2568083 -4.2625132 -4.2614765 -4.2570634 -4.2539082 -4.2504869 -4.2443566][-4.2692494 -4.2732234 -4.2771363 -4.27921 -4.279037 -4.2776437 -4.2751656 -4.2752533 -4.2791181 -4.2817616 -4.2799892 -4.2764335 -4.2743998 -4.272737 -4.2698917]]...]
INFO - root - 2017-12-07 16:50:16.888124: step 31410, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 56h:53m:30s remains)
INFO - root - 2017-12-07 16:50:23.643003: step 31420, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 53h:26m:45s remains)
INFO - root - 2017-12-07 16:50:30.384450: step 31430, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 55h:25m:57s remains)
INFO - root - 2017-12-07 16:50:37.154456: step 31440, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 58h:51m:54s remains)
INFO - root - 2017-12-07 16:50:43.917109: step 31450, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 58h:25m:46s remains)
INFO - root - 2017-12-07 16:50:50.668980: step 31460, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 58h:51m:20s remains)
INFO - root - 2017-12-07 16:50:57.402350: step 31470, loss = 2.05, batch loss = 1.99 (13.3 examples/sec; 0.602 sec/batch; 50h:22m:37s remains)
INFO - root - 2017-12-07 16:51:04.131100: step 31480, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 52h:55m:54s remains)
INFO - root - 2017-12-07 16:51:10.852753: step 31490, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 54h:14m:36s remains)
INFO - root - 2017-12-07 16:51:17.630932: step 31500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 59h:42m:15s remains)
2017-12-07 16:51:18.358253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2519965 -4.2695785 -4.2759075 -4.263854 -4.2348065 -4.194447 -4.1597724 -4.1424723 -4.1535206 -4.1858397 -4.2370925 -4.2788458 -4.3024235 -4.3133268 -4.3115244][-4.2870021 -4.2939839 -4.2920208 -4.2685647 -4.222291 -4.1638207 -4.1094136 -4.0788732 -4.0930529 -4.1462011 -4.2243414 -4.2863197 -4.3189664 -4.3334389 -4.3324037][-4.3207383 -4.318964 -4.3089046 -4.2731686 -4.2096825 -4.1350474 -4.05834 -4.0071049 -4.0181785 -4.088057 -4.1897492 -4.2695584 -4.3150883 -4.3367877 -4.338758][-4.3446651 -4.3356633 -4.3193083 -4.2747746 -4.1997228 -4.1153979 -4.0207014 -3.9470682 -3.9492216 -4.0263305 -4.1408434 -4.2335906 -4.29103 -4.3201923 -4.3254757][-4.3478837 -4.3342237 -4.3158526 -4.2688003 -4.1878066 -4.0948172 -3.9839919 -3.8889413 -3.882781 -3.9677536 -4.0936222 -4.1982784 -4.2663665 -4.2996974 -4.308248][-4.3397903 -4.324883 -4.3074617 -4.263031 -4.1818137 -4.0832767 -3.9616473 -3.8509121 -3.8347535 -3.9252942 -4.0632358 -4.1798396 -4.2554116 -4.2923574 -4.3020778][-4.3307281 -4.3150783 -4.2982893 -4.2579975 -4.1817441 -4.0834947 -3.9628165 -3.8557096 -3.8324924 -3.9162591 -4.0529323 -4.1757708 -4.256249 -4.29455 -4.3046055][-4.3224416 -4.3074708 -4.2888136 -4.2515678 -4.1857691 -4.0958815 -3.9856174 -3.893368 -3.8724618 -3.9440007 -4.0643797 -4.1806006 -4.2613044 -4.2995453 -4.3095908][-4.3127837 -4.3006516 -4.2806311 -4.2463641 -4.1918039 -4.1135492 -4.0222073 -3.9539316 -3.9434185 -4.0034876 -4.1012483 -4.19983 -4.2731633 -4.3074694 -4.3157315][-4.3063893 -4.29835 -4.2784538 -4.2454324 -4.1991744 -4.135128 -4.066884 -4.0255275 -4.0281997 -4.0800786 -4.1576786 -4.2364845 -4.2952147 -4.3210459 -4.3244977][-4.3086653 -4.3052521 -4.2876911 -4.2593155 -4.2233133 -4.1751838 -4.12945 -4.110755 -4.1226559 -4.1653509 -4.2243533 -4.2832317 -4.3242087 -4.3380032 -4.3350515][-4.3179455 -4.3183761 -4.3065281 -4.28678 -4.2645521 -4.2321448 -4.2023244 -4.194376 -4.2070794 -4.2378578 -4.2797322 -4.3201494 -4.3454947 -4.3496642 -4.3431916][-4.3276653 -4.3304839 -4.3250532 -4.3144455 -4.3044825 -4.2876954 -4.2703686 -4.2667389 -4.2771339 -4.2976446 -4.322927 -4.3449593 -4.3582807 -4.3568764 -4.3492503][-4.3347044 -4.3379045 -4.3372836 -4.3331995 -4.3308573 -4.3250313 -4.3182573 -4.3171415 -4.3229914 -4.3338852 -4.3460994 -4.3557053 -4.3609967 -4.3577504 -4.351501][-4.3388081 -4.3409581 -4.342514 -4.3427191 -4.3449049 -4.3462844 -4.3466177 -4.3464785 -4.3477325 -4.35064 -4.353972 -4.3567128 -4.3573055 -4.3544674 -4.3508358]]...]
INFO - root - 2017-12-07 16:51:25.121483: step 31510, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 55h:13m:28s remains)
INFO - root - 2017-12-07 16:51:31.970783: step 31520, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 61h:32m:29s remains)
INFO - root - 2017-12-07 16:51:38.707436: step 31530, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 56h:30m:47s remains)
INFO - root - 2017-12-07 16:51:45.510969: step 31540, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 56h:43m:58s remains)
INFO - root - 2017-12-07 16:51:52.049233: step 31550, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 52h:17m:41s remains)
INFO - root - 2017-12-07 16:51:58.777232: step 31560, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 59h:13m:20s remains)
INFO - root - 2017-12-07 16:52:05.478301: step 31570, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 57h:41m:00s remains)
INFO - root - 2017-12-07 16:52:12.185462: step 31580, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 54h:04m:14s remains)
INFO - root - 2017-12-07 16:52:18.751289: step 31590, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 0.535 sec/batch; 44h:41m:32s remains)
INFO - root - 2017-12-07 16:52:25.488472: step 31600, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 55h:01m:41s remains)
2017-12-07 16:52:26.210150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2695031 -4.2682872 -4.2608943 -4.2597585 -4.2683997 -4.278615 -4.28995 -4.2991481 -4.2924595 -4.2684436 -4.2427797 -4.2205477 -4.2093425 -4.2055755 -4.1989284][-4.245748 -4.2466035 -4.2343378 -4.2268677 -4.2285681 -4.2336268 -4.2387934 -4.2411079 -4.2258396 -4.1892962 -4.1575623 -4.1431575 -4.1466551 -4.1555943 -4.1554575][-4.217237 -4.2150965 -4.1974692 -4.1829724 -4.1722403 -4.1642628 -4.1575112 -4.1495419 -4.1254253 -4.0839477 -4.0559778 -4.0561128 -4.0824418 -4.1109443 -4.1229358][-4.202424 -4.1900096 -4.1611447 -4.130403 -4.0975943 -4.0686078 -4.0466342 -4.034421 -4.0226235 -4.0011525 -3.9920526 -4.0126352 -4.0573182 -4.0982285 -4.1177783][-4.2003174 -4.1725659 -4.1303639 -4.082437 -4.0267358 -3.9711235 -3.9276481 -3.9171755 -3.9413908 -3.9673259 -3.9960487 -4.0397782 -4.0934606 -4.1327925 -4.1473789][-4.1962519 -4.1540537 -4.103404 -4.0511365 -3.9881034 -3.9103012 -3.8432097 -3.8376222 -3.9007146 -3.9728448 -4.0346847 -4.09461 -4.1499467 -4.1825428 -4.1868672][-4.1883225 -4.1400709 -4.0944657 -4.0582652 -4.0101509 -3.9335465 -3.8637347 -3.868366 -3.9440694 -4.0280881 -4.0949574 -4.1514974 -4.2000084 -4.2267513 -4.2246032][-4.181386 -4.1382804 -4.1105971 -4.1045256 -4.0839505 -4.029469 -3.9778979 -3.9904969 -4.0511451 -4.116189 -4.1671176 -4.2074609 -4.24417 -4.2625766 -4.2559557][-4.1793408 -4.1524706 -4.1498532 -4.1680202 -4.1699095 -4.1412187 -4.1138625 -4.1284409 -4.1674838 -4.2072835 -4.238719 -4.259974 -4.2799268 -4.2844315 -4.2685232][-4.1905742 -4.1845541 -4.20136 -4.2289152 -4.2398338 -4.2287264 -4.2185163 -4.2300825 -4.2522674 -4.2749534 -4.2947235 -4.3031015 -4.3056569 -4.2934709 -4.2655439][-4.2072163 -4.21827 -4.2423391 -4.2660775 -4.2765946 -4.2744336 -4.27267 -4.2812848 -4.294558 -4.3097425 -4.3231268 -4.3244543 -4.313446 -4.2860813 -4.2479558][-4.21471 -4.2362633 -4.2607632 -4.2791119 -4.2876968 -4.2899809 -4.2919159 -4.2967181 -4.3044133 -4.3143649 -4.3214321 -4.3172483 -4.2966819 -4.2586641 -4.2128172][-4.2188849 -4.2439532 -4.2674809 -4.282198 -4.2892156 -4.2919564 -4.2929373 -4.2938032 -4.2967062 -4.3005557 -4.2994676 -4.2878337 -4.2593279 -4.213695 -4.1646957][-4.2384834 -4.2583866 -4.2755723 -4.2844067 -4.2863483 -4.2841134 -4.2814617 -4.2797408 -4.2796888 -4.2766671 -4.2674894 -4.24946 -4.2165489 -4.1694617 -4.123383][-4.2694921 -4.2770076 -4.2815247 -4.2801895 -4.2735176 -4.265944 -4.2599683 -4.2574492 -4.2567339 -4.2507119 -4.2371817 -4.2165794 -4.1847816 -4.1425796 -4.102901]]...]
INFO - root - 2017-12-07 16:52:33.012614: step 31610, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 60h:25m:21s remains)
INFO - root - 2017-12-07 16:52:39.725636: step 31620, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.677 sec/batch; 56h:34m:45s remains)
INFO - root - 2017-12-07 16:52:46.497713: step 31630, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 52h:56m:51s remains)
INFO - root - 2017-12-07 16:52:53.407588: step 31640, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 55h:14m:50s remains)
INFO - root - 2017-12-07 16:53:00.173938: step 31650, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.720 sec/batch; 60h:09m:38s remains)
INFO - root - 2017-12-07 16:53:06.907854: step 31660, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 58h:26m:40s remains)
INFO - root - 2017-12-07 16:53:13.807084: step 31670, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 54h:11m:02s remains)
INFO - root - 2017-12-07 16:53:20.558936: step 31680, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 54h:27m:11s remains)
INFO - root - 2017-12-07 16:53:27.189385: step 31690, loss = 2.09, batch loss = 2.03 (14.2 examples/sec; 0.564 sec/batch; 47h:08m:23s remains)
INFO - root - 2017-12-07 16:53:33.958901: step 31700, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.750 sec/batch; 62h:38m:08s remains)
2017-12-07 16:53:34.748018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3171849 -4.2961006 -4.2848039 -4.2808976 -4.2723961 -4.2532296 -4.2255559 -4.208684 -4.2011933 -4.20292 -4.2207513 -4.2476945 -4.2652378 -4.2673988 -4.2633533][-4.3033748 -4.2771387 -4.2661228 -4.2621737 -4.2481756 -4.2214465 -4.1845093 -4.1600614 -4.1537862 -4.1649323 -4.1942658 -4.2288337 -4.250051 -4.2538123 -4.2527628][-4.2933316 -4.2654266 -4.2540908 -4.2450995 -4.2213273 -4.1851449 -4.1409531 -4.1100388 -4.1060057 -4.130116 -4.1709013 -4.2090487 -4.2280025 -4.2320528 -4.2346458][-4.2874193 -4.2580872 -4.2441435 -4.2291136 -4.1971312 -4.153059 -4.1030378 -4.069407 -4.0708795 -4.1061134 -4.1530437 -4.1868248 -4.198102 -4.2033939 -4.2130079][-4.2854066 -4.2540841 -4.2363958 -4.215106 -4.1769094 -4.1273246 -4.0758977 -4.0451427 -4.054738 -4.0970778 -4.1427717 -4.1676459 -4.1748734 -4.1852012 -4.2051096][-4.2872696 -4.254415 -4.231267 -4.2025046 -4.16013 -4.1080108 -4.0591397 -4.033349 -4.0489855 -4.0917344 -4.128561 -4.1479626 -4.1612277 -4.18243 -4.210012][-4.2924314 -4.2566972 -4.2266355 -4.1908455 -4.1447868 -4.0940447 -4.0505657 -4.0257592 -4.0381322 -4.0788956 -4.1091518 -4.12511 -4.1475363 -4.179606 -4.2126083][-4.2988167 -4.2628427 -4.2314453 -4.1950231 -4.1469755 -4.0963578 -4.0517859 -4.0172968 -4.0147095 -4.0432386 -4.0719872 -4.0908465 -4.120893 -4.1623092 -4.20167][-4.3057323 -4.2716107 -4.2447104 -4.2134194 -4.1656651 -4.1112909 -4.0597653 -4.0112839 -3.9838412 -3.992465 -4.02075 -4.0509887 -4.0890408 -4.1385746 -4.185864][-4.31119 -4.2782869 -4.2544432 -4.2283897 -4.1831818 -4.1269579 -4.0720491 -4.0160422 -3.9690332 -3.9535131 -3.9709532 -4.0038204 -4.0428157 -4.0925808 -4.1435256][-4.3152509 -4.2827458 -4.2591972 -4.23528 -4.1922007 -4.1378722 -4.0846443 -4.0296125 -3.9774058 -3.9468427 -3.9443052 -3.9581165 -3.9793952 -4.0151458 -4.061501][-4.3205662 -4.2898417 -4.2661681 -4.2428217 -4.20197 -4.1523495 -4.1030817 -4.0555964 -4.0118966 -3.9798806 -3.9596119 -3.9438546 -3.9332151 -3.94173 -3.9705193][-4.3271451 -4.2996216 -4.2766333 -4.2556424 -4.222023 -4.1810336 -4.1410203 -4.1056471 -4.0741911 -4.0460486 -4.0172606 -3.9826553 -3.946115 -3.9285791 -3.9409096][-4.3348036 -4.3120041 -4.2908335 -4.2743945 -4.2499919 -4.2191911 -4.1901155 -4.1692796 -4.1536965 -4.1372027 -4.1123447 -4.0765929 -4.0347428 -4.009285 -4.0109615][-4.3429389 -4.3263364 -4.3100858 -4.298069 -4.2827973 -4.2620673 -4.241693 -4.2305107 -4.2273512 -4.2256145 -4.2154126 -4.1940541 -4.1640429 -4.1430321 -4.1409841]]...]
INFO - root - 2017-12-07 16:53:41.513657: step 31710, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 52h:05m:21s remains)
INFO - root - 2017-12-07 16:53:48.376525: step 31720, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 60h:57m:50s remains)
INFO - root - 2017-12-07 16:53:55.213834: step 31730, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.741 sec/batch; 61h:52m:41s remains)
INFO - root - 2017-12-07 16:54:02.080850: step 31740, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 58h:49m:22s remains)
INFO - root - 2017-12-07 16:54:08.784810: step 31750, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 55h:53m:08s remains)
INFO - root - 2017-12-07 16:54:15.468343: step 31760, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 52h:10m:00s remains)
INFO - root - 2017-12-07 16:54:22.271030: step 31770, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 56h:04m:09s remains)
INFO - root - 2017-12-07 16:54:29.046552: step 31780, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 57h:37m:48s remains)
INFO - root - 2017-12-07 16:54:35.740737: step 31790, loss = 2.03, batch loss = 1.97 (13.1 examples/sec; 0.610 sec/batch; 50h:56m:49s remains)
INFO - root - 2017-12-07 16:54:42.442646: step 31800, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 52h:41m:16s remains)
2017-12-07 16:54:43.112280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2843146 -4.2724857 -4.2428846 -4.2100573 -4.1919169 -4.1822352 -4.1887684 -4.2115722 -4.233634 -4.2464046 -4.2481794 -4.2396674 -4.1995487 -4.1501908 -4.1423731][-4.2724018 -4.26632 -4.2393618 -4.2023759 -4.1761875 -4.1532321 -4.1504107 -4.1824636 -4.2212319 -4.2472715 -4.2530861 -4.2377987 -4.1857495 -4.1224518 -4.1127143][-4.25973 -4.2588158 -4.236712 -4.1977563 -4.1627955 -4.1245828 -4.1042619 -4.1437488 -4.2003908 -4.23735 -4.2483749 -4.2335815 -4.18434 -4.1212378 -4.1100521][-4.2449641 -4.2453136 -4.228385 -4.190968 -4.1507125 -4.0996237 -4.0620008 -4.1044993 -4.1756468 -4.2213173 -4.2435336 -4.2418513 -4.206203 -4.15298 -4.1371331][-4.239614 -4.2335248 -4.2179146 -4.178299 -4.1264896 -4.0537243 -3.9947178 -4.040216 -4.1304822 -4.1882868 -4.2255177 -4.2435327 -4.2279449 -4.1914124 -4.1747756][-4.2454762 -4.2320867 -4.2124076 -4.1720963 -4.107379 -4.0093703 -3.9310162 -3.9755127 -4.0820646 -4.1568947 -4.2101979 -4.24375 -4.2399259 -4.2125249 -4.1936741][-4.2408524 -4.2265382 -4.2115083 -4.17904 -4.118186 -4.0092936 -3.9244542 -3.9588258 -4.0535688 -4.1288085 -4.1935449 -4.2392917 -4.2421646 -4.2160296 -4.1907907][-4.2333937 -4.2233429 -4.2144294 -4.189446 -4.1388912 -4.034801 -3.9586091 -3.9856799 -4.0547066 -4.1103144 -4.1722212 -4.2241478 -4.2354374 -4.2152495 -4.1860042][-4.2331805 -4.2327065 -4.2303343 -4.2135019 -4.1742563 -4.0842948 -4.0255857 -4.0508518 -4.0933886 -4.1216311 -4.167243 -4.2209673 -4.2379017 -4.2242041 -4.1935987][-4.2344456 -4.2467613 -4.2478032 -4.2356114 -4.2046223 -4.1305084 -4.0879331 -4.1093159 -4.1284781 -4.1308346 -4.1585975 -4.207974 -4.2329545 -4.2301912 -4.2075753][-4.2450395 -4.2602634 -4.2639656 -4.257771 -4.2345958 -4.1721373 -4.136672 -4.150918 -4.1580515 -4.1533589 -4.1740894 -4.2175717 -4.246089 -4.2513123 -4.2382612][-4.2697911 -4.2782855 -4.277493 -4.2733889 -4.2531843 -4.2049475 -4.183085 -4.1988697 -4.2053485 -4.2006598 -4.2137384 -4.2454829 -4.2696047 -4.2761846 -4.2681437][-4.2893 -4.2893324 -4.2834373 -4.2777781 -4.2608533 -4.2297587 -4.2239642 -4.2423911 -4.2494922 -4.248652 -4.2563376 -4.273675 -4.2878208 -4.2918677 -4.2862935][-4.3097625 -4.3033566 -4.2920442 -4.2842369 -4.2713132 -4.2537026 -4.2566676 -4.2737641 -4.2814832 -4.283134 -4.2853651 -4.2902226 -4.2957129 -4.296463 -4.2904587][-4.317831 -4.3087997 -4.2941818 -4.2881117 -4.2838783 -4.2776246 -4.286839 -4.3042397 -4.3126183 -4.3128152 -4.3086061 -4.3028326 -4.298892 -4.2944384 -4.2881832]]...]
INFO - root - 2017-12-07 16:54:49.953432: step 31810, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 60h:17m:48s remains)
INFO - root - 2017-12-07 16:54:56.689336: step 31820, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 58h:40m:27s remains)
INFO - root - 2017-12-07 16:55:03.474039: step 31830, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 54h:43m:45s remains)
INFO - root - 2017-12-07 16:55:10.302783: step 31840, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 52h:29m:13s remains)
INFO - root - 2017-12-07 16:55:17.159162: step 31850, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 59h:09m:37s remains)
INFO - root - 2017-12-07 16:55:23.927502: step 31860, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 57h:11m:33s remains)
INFO - root - 2017-12-07 16:55:30.698523: step 31870, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 53h:59m:50s remains)
INFO - root - 2017-12-07 16:55:37.503989: step 31880, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 53h:56m:39s remains)
INFO - root - 2017-12-07 16:55:44.380567: step 31890, loss = 2.07, batch loss = 2.01 (13.5 examples/sec; 0.593 sec/batch; 49h:28m:55s remains)
INFO - root - 2017-12-07 16:55:51.113616: step 31900, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 57h:57m:59s remains)
2017-12-07 16:55:51.850003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2534094 -4.2621341 -4.2698474 -4.2707238 -4.2667813 -4.2613993 -4.2563562 -4.2528968 -4.2457671 -4.2291765 -4.210484 -4.203506 -4.2155867 -4.2467532 -4.2845931][-4.2507377 -4.2613578 -4.2680116 -4.2639956 -4.2538419 -4.2433829 -4.23707 -4.2365489 -4.2343693 -4.221427 -4.2033997 -4.1965656 -4.2073145 -4.2374067 -4.2765431][-4.2722468 -4.2805238 -4.2808428 -4.2684 -4.249826 -4.2334228 -4.2250137 -4.2268138 -4.2297564 -4.223803 -4.2102923 -4.2041655 -4.2126484 -4.2399826 -4.2766061][-4.2944684 -4.29751 -4.2902126 -4.2716203 -4.247211 -4.2261925 -4.2152991 -4.21645 -4.2239408 -4.2263751 -4.2203169 -4.215342 -4.220758 -4.2439408 -4.2756028][-4.304574 -4.3040242 -4.2907166 -4.266387 -4.2324777 -4.2033253 -4.1904726 -4.192976 -4.2078638 -4.2204909 -4.2230511 -4.2196236 -4.2219653 -4.2403507 -4.2675076][-4.2919135 -4.2889628 -4.2727451 -4.2431436 -4.2004519 -4.16583 -4.1523514 -4.1543341 -4.1713939 -4.1911778 -4.2018871 -4.2018847 -4.2057934 -4.2244921 -4.25228][-4.2624478 -4.2558808 -4.2377467 -4.2055583 -4.160079 -4.1252894 -4.11242 -4.11164 -4.123064 -4.1456685 -4.1633458 -4.1692123 -4.17819 -4.2022495 -4.2353086][-4.2237744 -4.2195568 -4.2075438 -4.1797934 -4.1385617 -4.1075354 -4.0918136 -4.0829563 -4.0849037 -4.1062179 -4.1280608 -4.1414928 -4.1579394 -4.18696 -4.2253861][-4.1759567 -4.188581 -4.19385 -4.1766772 -4.1468878 -4.1251559 -4.1081152 -4.0942583 -4.0881267 -4.1014867 -4.1181011 -4.1316333 -4.1492448 -4.1783419 -4.2211046][-4.1373844 -4.1715345 -4.1987562 -4.1974721 -4.182229 -4.1659665 -4.1466751 -4.13201 -4.124177 -4.1292014 -4.1340728 -4.140491 -4.1520019 -4.1776104 -4.22124][-4.1314688 -4.1778603 -4.2162523 -4.2246928 -4.2210951 -4.2073388 -4.1879716 -4.1756711 -4.1704545 -4.169199 -4.1630864 -4.1610966 -4.1651025 -4.1867094 -4.2295671][-4.1510553 -4.1997595 -4.2377424 -4.2477484 -4.2485876 -4.2375746 -4.2229743 -4.2138376 -4.2089009 -4.2019386 -4.1897922 -4.1802826 -4.1783214 -4.1972871 -4.2397361][-4.173995 -4.2144647 -4.246419 -4.2549853 -4.2593102 -4.25282 -4.2447681 -4.2378254 -4.2292409 -4.21883 -4.2027373 -4.1889892 -4.1850276 -4.2050414 -4.2472167][-4.1933713 -4.2194934 -4.2414603 -4.2491255 -4.2563367 -4.2564163 -4.2541752 -4.2477078 -4.2363191 -4.2238111 -4.2066717 -4.1918516 -4.1896458 -4.2123427 -4.2546749][-4.2051015 -4.2182059 -4.2335272 -4.244823 -4.2554564 -4.2610064 -4.2596889 -4.251863 -4.2404842 -4.2288723 -4.2140183 -4.2012482 -4.2021961 -4.2256465 -4.2654734]]...]
INFO - root - 2017-12-07 16:55:58.625811: step 31910, loss = 2.09, batch loss = 2.04 (12.1 examples/sec; 0.660 sec/batch; 55h:07m:21s remains)
INFO - root - 2017-12-07 16:56:05.364655: step 31920, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 57h:16m:44s remains)
INFO - root - 2017-12-07 16:56:12.164989: step 31930, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 59h:43m:23s remains)
INFO - root - 2017-12-07 16:56:19.072174: step 31940, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 55h:36m:46s remains)
INFO - root - 2017-12-07 16:56:25.854159: step 31950, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 53h:33m:19s remains)
INFO - root - 2017-12-07 16:56:32.575957: step 31960, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 52h:46m:51s remains)
INFO - root - 2017-12-07 16:56:39.338047: step 31970, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 58h:00m:51s remains)
INFO - root - 2017-12-07 16:56:46.187624: step 31980, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 60h:20m:12s remains)
INFO - root - 2017-12-07 16:56:52.833083: step 31990, loss = 2.05, batch loss = 1.99 (13.8 examples/sec; 0.578 sec/batch; 48h:17m:03s remains)
INFO - root - 2017-12-07 16:56:59.569963: step 32000, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 53h:22m:47s remains)
2017-12-07 16:57:00.253154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3125286 -4.3188367 -4.3213549 -4.3208203 -4.317121 -4.3111343 -4.3034935 -4.2947278 -4.2873869 -4.2851596 -4.2877879 -4.2948742 -4.3041935 -4.3117633 -4.31677][-4.3147273 -4.320395 -4.3220744 -4.3203826 -4.3155885 -4.3064909 -4.29397 -4.2809753 -4.2721381 -4.2720585 -4.2813244 -4.296442 -4.3121181 -4.3210068 -4.3225088][-4.3211217 -4.3264771 -4.3271813 -4.3245163 -4.3166471 -4.3009377 -4.2795248 -4.2578311 -4.2416382 -4.2392778 -4.253686 -4.2776995 -4.3051672 -4.3231039 -4.3271379][-4.3267803 -4.3319879 -4.3312292 -4.3252993 -4.3091192 -4.2810006 -4.2447686 -4.2112179 -4.1896563 -4.1875896 -4.2095556 -4.2447591 -4.2845817 -4.3133755 -4.3255525][-4.3287516 -4.331399 -4.326839 -4.3152933 -4.2889457 -4.2468767 -4.1926885 -4.1453838 -4.1219244 -4.1242132 -4.1550879 -4.2015247 -4.2515082 -4.2932658 -4.3170066][-4.3262348 -4.3255963 -4.3151178 -4.2959523 -4.2591615 -4.202528 -4.1318617 -4.0716758 -4.0414424 -4.0484715 -4.0971293 -4.1578341 -4.215868 -4.269145 -4.304873][-4.3230381 -4.3167796 -4.2970862 -4.2661629 -4.2157779 -4.14164 -4.0495434 -3.9664717 -3.914479 -3.9228127 -4.0052133 -4.0966153 -4.1705852 -4.2366347 -4.2828116][-4.3227339 -4.3111229 -4.2810678 -4.234395 -4.1647229 -4.0708342 -3.9559381 -3.8438416 -3.762814 -3.7709854 -3.8946576 -4.0301218 -4.1278667 -4.2070084 -4.2597637][-4.3272066 -4.3144054 -4.2802258 -4.2245378 -4.1418071 -4.0346203 -3.9056497 -3.7791247 -3.6868317 -3.6956239 -3.8395896 -4.0026646 -4.1180582 -4.2036686 -4.2574425][-4.3310151 -4.3234034 -4.2974195 -4.2513375 -4.1811137 -4.0905042 -3.9812021 -3.8752041 -3.7999876 -3.8033266 -3.9151449 -4.0554938 -4.1590266 -4.2354884 -4.2794685][-4.3354106 -4.335639 -4.3235455 -4.29664 -4.2542005 -4.1965675 -4.12896 -4.0591927 -4.0068717 -3.9998972 -4.0608869 -4.1540289 -4.2286248 -4.2826042 -4.309679][-4.3406677 -4.3439045 -4.3386841 -4.3247881 -4.3037405 -4.2733703 -4.2362838 -4.1942492 -4.163054 -4.1588464 -4.1938863 -4.2536426 -4.2999654 -4.3282394 -4.3384991][-4.3413272 -4.3460226 -4.3445783 -4.3372326 -4.3241229 -4.3052869 -4.2792206 -4.2488503 -4.2312436 -4.235301 -4.2630143 -4.3080463 -4.3379436 -4.3516874 -4.3542037][-4.3372383 -4.3416677 -4.3413053 -4.3340058 -4.3177934 -4.2955847 -4.2694035 -4.2436 -4.2358451 -4.2514038 -4.2810698 -4.3215313 -4.3468118 -4.35579 -4.3532181][-4.331171 -4.3330517 -4.3287792 -4.3141007 -4.2865405 -4.2510352 -4.2183728 -4.1992812 -4.2017069 -4.2276454 -4.2655382 -4.3082709 -4.3370395 -4.3478622 -4.3436017]]...]
INFO - root - 2017-12-07 16:57:07.087504: step 32010, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 60h:32m:13s remains)
INFO - root - 2017-12-07 16:57:13.989539: step 32020, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 58h:25m:59s remains)
INFO - root - 2017-12-07 16:57:20.802577: step 32030, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 52h:30m:56s remains)
INFO - root - 2017-12-07 16:57:27.629644: step 32040, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 53h:56m:48s remains)
INFO - root - 2017-12-07 16:57:34.467409: step 32050, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 60h:24m:48s remains)
INFO - root - 2017-12-07 16:57:41.293043: step 32060, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 61h:51m:07s remains)
INFO - root - 2017-12-07 16:57:48.137778: step 32070, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 57h:44m:38s remains)
INFO - root - 2017-12-07 16:57:54.934138: step 32080, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 56h:55m:57s remains)
INFO - root - 2017-12-07 16:58:01.653062: step 32090, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 0.530 sec/batch; 44h:14m:26s remains)
INFO - root - 2017-12-07 16:58:08.412127: step 32100, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 59h:16m:50s remains)
2017-12-07 16:58:09.170397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2545209 -4.2457185 -4.2293749 -4.2195721 -4.2231493 -4.2290778 -4.2324128 -4.2372007 -4.2441297 -4.2599354 -4.2827072 -4.3011255 -4.3107438 -4.3135328 -4.31207][-4.2535949 -4.2359891 -4.2123575 -4.201118 -4.2071877 -4.2156072 -4.2226615 -4.2321186 -4.242291 -4.259892 -4.2834291 -4.2997785 -4.3063149 -4.306397 -4.3024988][-4.2522473 -4.2289295 -4.2017026 -4.18597 -4.1891217 -4.1964436 -4.2098355 -4.228807 -4.2461238 -4.2657633 -4.2869349 -4.2968178 -4.2924204 -4.2860303 -4.2814283][-4.2448759 -4.2221904 -4.1933174 -4.1716681 -4.1662393 -4.1711326 -4.1917224 -4.2178316 -4.2399573 -4.2611275 -4.27862 -4.2828197 -4.268785 -4.2559266 -4.2510214][-4.2258539 -4.2098618 -4.1826839 -4.1587291 -4.1475143 -4.15204 -4.1754441 -4.2013893 -4.2215395 -4.2400928 -4.2507973 -4.2496538 -4.2334352 -4.2182026 -4.2117109][-4.1939449 -4.1896796 -4.16982 -4.1479888 -4.1335573 -4.1377506 -4.1590457 -4.1814675 -4.19724 -4.2107911 -4.2181187 -4.2161469 -4.2002745 -4.1830411 -4.1734581][-4.1544833 -4.1611328 -4.1522436 -4.1376219 -4.1260896 -4.1302824 -4.1486168 -4.1658258 -4.1762795 -4.1856065 -4.1934237 -4.1934924 -4.1806326 -4.163178 -4.1516376][-4.1216078 -4.1319857 -4.1330147 -4.1286864 -4.1228933 -4.1237454 -4.1340137 -4.1459818 -4.1569166 -4.1734419 -4.1887584 -4.1927505 -4.1839638 -4.1682181 -4.1587253][-4.1135244 -4.1219878 -4.1285787 -4.130568 -4.1242981 -4.1120172 -4.1060028 -4.1091175 -4.1199584 -4.1469679 -4.1797838 -4.1987295 -4.2013974 -4.1935782 -4.1929073][-4.1390781 -4.1412654 -4.1455765 -4.1460104 -4.1340752 -4.1085973 -4.08468 -4.0727181 -4.0772533 -4.1074319 -4.149241 -4.1795959 -4.1934543 -4.1995959 -4.2133918][-4.174201 -4.1768389 -4.1775861 -4.1737256 -4.1585922 -4.1303988 -4.0989227 -4.0746617 -4.0653586 -4.0805039 -4.1096148 -4.1346521 -4.1518612 -4.1679487 -4.1957774][-4.1823235 -4.1967516 -4.2058382 -4.2069526 -4.1939969 -4.1690068 -4.14098 -4.1145215 -4.0947366 -4.0859661 -4.0854459 -4.0907607 -4.1029892 -4.1239662 -4.1568184][-4.1680017 -4.1903472 -4.2107134 -4.2222991 -4.2184143 -4.2033405 -4.1858492 -4.1664286 -4.1460476 -4.1258035 -4.1030073 -4.0852327 -4.0852 -4.1011519 -4.1263237][-4.1395259 -4.163631 -4.1909 -4.2097239 -4.2144542 -4.2111874 -4.2076082 -4.2025189 -4.1922278 -4.1732512 -4.1448946 -4.1180153 -4.105515 -4.1078238 -4.1159344][-4.1247611 -4.1459785 -4.1710043 -4.1893611 -4.1954455 -4.194531 -4.1986766 -4.2079883 -4.2115893 -4.1996574 -4.1766663 -4.1529756 -4.1363468 -4.1290021 -4.1234283]]...]
INFO - root - 2017-12-07 16:58:15.960526: step 32110, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 52h:14m:50s remains)
INFO - root - 2017-12-07 16:58:22.897301: step 32120, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 59h:52m:27s remains)
INFO - root - 2017-12-07 16:58:29.663003: step 32130, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 61h:25m:21s remains)
INFO - root - 2017-12-07 16:58:36.488485: step 32140, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 62h:17m:41s remains)
INFO - root - 2017-12-07 16:58:43.273192: step 32150, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 53h:33m:09s remains)
INFO - root - 2017-12-07 16:58:50.121065: step 32160, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 53h:22m:22s remains)
INFO - root - 2017-12-07 16:58:56.783036: step 32170, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 54h:59m:43s remains)
INFO - root - 2017-12-07 16:59:03.631431: step 32180, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.678 sec/batch; 56h:33m:44s remains)
INFO - root - 2017-12-07 16:59:10.442528: step 32190, loss = 2.09, batch loss = 2.03 (16.6 examples/sec; 0.481 sec/batch; 40h:05m:21s remains)
INFO - root - 2017-12-07 16:59:17.190187: step 32200, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 52h:47m:39s remains)
2017-12-07 16:59:18.011423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2064629 -4.2089548 -4.2133946 -4.2149873 -4.2101779 -4.1919341 -4.1852503 -4.1930408 -4.1997819 -4.2052407 -4.2060122 -4.2117887 -4.2260394 -4.2321649 -4.2366657][-4.1800957 -4.173708 -4.1720715 -4.1689034 -4.1680136 -4.1593924 -4.1551342 -4.1587272 -4.1644392 -4.16659 -4.16485 -4.1670904 -4.1794877 -4.1952286 -4.2140574][-4.1470122 -4.1403651 -4.1426668 -4.1477962 -4.1607242 -4.1667027 -4.1635466 -4.1582556 -4.1524634 -4.13852 -4.1256905 -4.1246724 -4.1415672 -4.1680326 -4.1949396][-4.1207647 -4.1216278 -4.1334434 -4.1523552 -4.1787391 -4.1932573 -4.1896963 -4.178113 -4.1678224 -4.1503973 -4.1334448 -4.1326251 -4.1536703 -4.177763 -4.1985135][-4.1102915 -4.1240492 -4.1460371 -4.1687646 -4.1908078 -4.2015042 -4.1948457 -4.1848545 -4.1798291 -4.1740432 -4.1684418 -4.1730652 -4.1932526 -4.2074237 -4.2167268][-4.1145034 -4.1363587 -4.1615038 -4.1757355 -4.1826758 -4.1831007 -4.1744795 -4.1691117 -4.1733508 -4.1770835 -4.1784854 -4.1875305 -4.2110987 -4.2222157 -4.22752][-4.1366906 -4.1551471 -4.1723495 -4.1779928 -4.1758289 -4.1679893 -4.1535378 -4.1404743 -4.1355929 -4.135179 -4.1368761 -4.1493311 -4.1813059 -4.2013535 -4.2167149][-4.1666565 -4.1731863 -4.1762829 -4.1806059 -4.1840835 -4.1799784 -4.160078 -4.1308722 -4.1052265 -4.0871663 -4.0807681 -4.098805 -4.1379476 -4.1681943 -4.1953979][-4.1651573 -4.1569142 -4.1499529 -4.1568961 -4.1729288 -4.1804581 -4.1636386 -4.1297693 -4.0987568 -4.0749321 -4.0632997 -4.082871 -4.1194472 -4.1488681 -4.180418][-4.1494966 -4.1306386 -4.1180882 -4.1235476 -4.1479259 -4.1688123 -4.1622477 -4.1347532 -4.1109285 -4.0922461 -4.0829186 -4.098846 -4.1251922 -4.1440878 -4.170722][-4.149323 -4.1324558 -4.1175251 -4.1177397 -4.1473827 -4.1755543 -4.1764126 -4.161694 -4.1512728 -4.1375008 -4.1276908 -4.1341596 -4.1488657 -4.1575184 -4.1730914][-4.1711659 -4.1610584 -4.1444807 -4.1405649 -4.1614647 -4.1849933 -4.1885586 -4.1907558 -4.1930413 -4.1848574 -4.1747932 -4.1716261 -4.1753783 -4.1765313 -4.1829071][-4.1860628 -4.185884 -4.1815257 -4.17988 -4.1871538 -4.1930661 -4.1872067 -4.1941719 -4.2103634 -4.209867 -4.2003765 -4.1943178 -4.191782 -4.1893911 -4.1907606][-4.1955581 -4.2040229 -4.20762 -4.2114062 -4.2198591 -4.2196417 -4.2041545 -4.2098126 -4.2275925 -4.2247753 -4.2116971 -4.2063289 -4.2074475 -4.2046275 -4.2021518][-4.2056956 -4.2122393 -4.216795 -4.2243285 -4.2349443 -4.2327752 -4.2197857 -4.2277622 -4.2412925 -4.2386112 -4.2278843 -4.2252569 -4.2338786 -4.2310405 -4.2237129]]...]
INFO - root - 2017-12-07 16:59:24.749127: step 32210, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 57h:42m:46s remains)
INFO - root - 2017-12-07 16:59:31.503204: step 32220, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.607 sec/batch; 50h:39m:47s remains)
INFO - root - 2017-12-07 16:59:38.323177: step 32230, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 52h:49m:02s remains)
INFO - root - 2017-12-07 16:59:45.229221: step 32240, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 61h:36m:34s remains)
INFO - root - 2017-12-07 16:59:52.050912: step 32250, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.756 sec/batch; 63h:03m:52s remains)
INFO - root - 2017-12-07 16:59:58.874758: step 32260, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 53h:49m:06s remains)
INFO - root - 2017-12-07 17:00:05.556680: step 32270, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 52h:28m:01s remains)
INFO - root - 2017-12-07 17:00:12.344697: step 32280, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 54h:31m:37s remains)
INFO - root - 2017-12-07 17:00:19.144528: step 32290, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 53h:31m:13s remains)
INFO - root - 2017-12-07 17:00:25.814824: step 32300, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 60h:07m:48s remains)
2017-12-07 17:00:26.488705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2520628 -4.2323828 -4.2247777 -4.2132068 -4.1964097 -4.1799726 -4.17754 -4.18592 -4.2006946 -4.2166109 -4.2234917 -4.2199697 -4.2119136 -4.1996307 -4.1895094][-4.2534156 -4.24243 -4.241498 -4.2276955 -4.2036519 -4.1800537 -4.1728034 -4.1788983 -4.197052 -4.2215176 -4.2415714 -4.2484837 -4.2393856 -4.215909 -4.1889153][-4.2476854 -4.2438817 -4.2462187 -4.2341156 -4.2099457 -4.1838369 -4.1667275 -4.1639762 -4.186305 -4.2242026 -4.2572527 -4.2739234 -4.2658458 -4.2368026 -4.2004323][-4.2387848 -4.2316375 -4.2358017 -4.2322054 -4.21485 -4.1873655 -4.154563 -4.1408229 -4.1698313 -4.2194033 -4.2602577 -4.2844968 -4.2808776 -4.2545176 -4.2209129][-4.229898 -4.2121706 -4.2157588 -4.223114 -4.2112556 -4.1733961 -4.120748 -4.1022139 -4.1493392 -4.2110696 -4.25696 -4.2844734 -4.2842197 -4.2651558 -4.2399211][-4.2180614 -4.1898112 -4.1925097 -4.20749 -4.1933975 -4.1343341 -4.0506482 -4.0339437 -4.1142 -4.1949291 -4.2489204 -4.2786374 -4.278831 -4.2650604 -4.2504678][-4.2135725 -4.1800065 -4.17944 -4.1939878 -4.1742058 -4.0923085 -3.9704683 -3.9517896 -4.0653486 -4.1703577 -4.2357373 -4.2691965 -4.2670307 -4.2570248 -4.2538652][-4.2092695 -4.1838336 -4.1867995 -4.2004795 -4.1820059 -4.0968456 -3.962049 -3.9242764 -4.0406446 -4.1554408 -4.22681 -4.2612672 -4.2543163 -4.2455583 -4.2494478][-4.2026429 -4.192317 -4.2021351 -4.21799 -4.2099409 -4.1478629 -4.040647 -3.9922686 -4.0653529 -4.1612577 -4.2275944 -4.2572646 -4.2461762 -4.23369 -4.2368045][-4.1965733 -4.1996307 -4.21292 -4.225606 -4.220696 -4.1824112 -4.1135387 -4.07473 -4.1142073 -4.1849289 -4.2418242 -4.2667112 -4.2517891 -4.2281475 -4.2228336][-4.196568 -4.2050061 -4.21752 -4.2245712 -4.217927 -4.1967731 -4.1570892 -4.1349 -4.1586766 -4.20704 -4.2540736 -4.27653 -4.2609549 -4.224781 -4.2067823][-4.20462 -4.207027 -4.2124567 -4.2134104 -4.2094431 -4.2032084 -4.1799803 -4.1674376 -4.1863461 -4.2198687 -4.2552471 -4.275702 -4.2630358 -4.2217875 -4.1908054][-4.2207613 -4.210887 -4.2024145 -4.1936073 -4.189661 -4.1919312 -4.17859 -4.17174 -4.19135 -4.2198343 -4.2477107 -4.2694087 -4.2633648 -4.2275796 -4.1875372][-4.2400508 -4.2219791 -4.2025747 -4.182507 -4.1729445 -4.1745934 -4.1625533 -4.1600747 -4.18314 -4.21195 -4.2395172 -4.260745 -4.2624407 -4.2382092 -4.2012057][-4.2537971 -4.2325168 -4.2107778 -4.188899 -4.1745543 -4.1679592 -4.1524158 -4.151114 -4.177093 -4.20797 -4.2338171 -4.2559061 -4.2626123 -4.2478318 -4.2194633]]...]
INFO - root - 2017-12-07 17:00:33.332862: step 32310, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 53h:49m:53s remains)
INFO - root - 2017-12-07 17:00:40.212179: step 32320, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.763 sec/batch; 63h:39m:23s remains)
INFO - root - 2017-12-07 17:00:46.977079: step 32330, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 60h:48m:14s remains)
INFO - root - 2017-12-07 17:00:53.667695: step 32340, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 56h:17m:00s remains)
INFO - root - 2017-12-07 17:01:00.388053: step 32350, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 52h:22m:23s remains)
INFO - root - 2017-12-07 17:01:07.119818: step 32360, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 54h:29m:02s remains)
INFO - root - 2017-12-07 17:01:13.941388: step 32370, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 58h:42m:03s remains)
INFO - root - 2017-12-07 17:01:20.720346: step 32380, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 59h:15m:26s remains)
INFO - root - 2017-12-07 17:01:27.387375: step 32390, loss = 2.06, batch loss = 2.00 (13.8 examples/sec; 0.581 sec/batch; 48h:27m:02s remains)
INFO - root - 2017-12-07 17:01:33.975348: step 32400, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.610 sec/batch; 50h:50m:49s remains)
2017-12-07 17:01:34.694282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2136331 -4.2154684 -4.2255306 -4.245934 -4.2610912 -4.2695374 -4.2800593 -4.2954535 -4.308641 -4.3120074 -4.3113689 -4.3096681 -4.3106518 -4.3124661 -4.3146486][-4.1692462 -4.1663618 -4.17606 -4.2001767 -4.2139359 -4.2190905 -4.2322836 -4.2567439 -4.2768025 -4.2840443 -4.2902522 -4.2937346 -4.2953629 -4.2973275 -4.3004303][-4.1620221 -4.1534581 -4.1559992 -4.1732287 -4.17579 -4.1708493 -4.181931 -4.2064166 -4.226068 -4.2375641 -4.2538056 -4.2646885 -4.2699914 -4.274435 -4.2778254][-4.1860466 -4.174933 -4.1687026 -4.1687574 -4.1497011 -4.1308069 -4.1343193 -4.1535082 -4.1744032 -4.19553 -4.2228622 -4.2389779 -4.2476177 -4.2552032 -4.2577891][-4.203156 -4.196228 -4.1878581 -4.1705275 -4.1283011 -4.0942426 -4.082911 -4.0918088 -4.1182036 -4.1570263 -4.1996226 -4.2203279 -4.2282996 -4.2375937 -4.241437][-4.2058439 -4.2092681 -4.2073445 -4.1796746 -4.1176748 -4.0572739 -4.0120153 -4.0017881 -4.0449247 -4.1122208 -4.1710162 -4.1949654 -4.204062 -4.2152872 -4.2214231][-4.1932387 -4.2063951 -4.2114949 -4.18052 -4.1064162 -4.008503 -3.9089336 -3.8713884 -3.9534149 -4.0631928 -4.1345711 -4.1627097 -4.1740494 -4.1852965 -4.1963143][-4.1648273 -4.1825213 -4.1938405 -4.1663008 -4.0881872 -3.9644966 -3.8270535 -3.7827029 -3.9045873 -4.0295668 -4.1020341 -4.1322317 -4.1484289 -4.158205 -4.1745825][-4.1477737 -4.1662741 -4.1823168 -4.1596532 -4.0858874 -3.9761066 -3.8747501 -3.8659189 -3.9581704 -4.0453825 -4.0969 -4.1206961 -4.1379709 -4.1488838 -4.1709385][-4.1689773 -4.1867762 -4.2031746 -4.1865239 -4.1254797 -4.05318 -4.0139413 -4.0216827 -4.0671849 -4.1092172 -4.1304679 -4.141789 -4.1539931 -4.1674194 -4.1910782][-4.2103162 -4.2234411 -4.2410722 -4.232945 -4.1936 -4.1556649 -4.146543 -4.1558394 -4.1752968 -4.1922536 -4.1919684 -4.1878462 -4.1909118 -4.203968 -4.2256484][-4.245492 -4.2529969 -4.271822 -4.2767868 -4.2596631 -4.2428508 -4.2438765 -4.2511353 -4.2605114 -4.2652006 -4.25075 -4.2301111 -4.2213116 -4.2318583 -4.2555509][-4.2745495 -4.2760677 -4.2915807 -4.3038282 -4.3016047 -4.2954993 -4.2960572 -4.3050489 -4.3103161 -4.3075376 -4.2914739 -4.2644982 -4.2455463 -4.2496772 -4.2750092][-4.3000541 -4.2999415 -4.30945 -4.3198733 -4.3237886 -4.3209043 -4.3185773 -4.3262334 -4.3287177 -4.3270164 -4.3134937 -4.2879591 -4.2667837 -4.27037 -4.2950492][-4.3123031 -4.3127952 -4.316937 -4.3205466 -4.3210306 -4.3171887 -4.3152456 -4.3211145 -4.3228145 -4.3209577 -4.3085284 -4.2913747 -4.279007 -4.2852707 -4.3058615]]...]
INFO - root - 2017-12-07 17:01:41.572322: step 32410, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 61h:00m:43s remains)
INFO - root - 2017-12-07 17:01:48.309692: step 32420, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 53h:45m:35s remains)
INFO - root - 2017-12-07 17:01:55.097329: step 32430, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 52h:01m:30s remains)
INFO - root - 2017-12-07 17:02:01.832343: step 32440, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.640 sec/batch; 53h:20m:21s remains)
INFO - root - 2017-12-07 17:02:08.646115: step 32450, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 56h:47m:19s remains)
INFO - root - 2017-12-07 17:02:15.506616: step 32460, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 61h:44m:38s remains)
INFO - root - 2017-12-07 17:02:22.245035: step 32470, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.694 sec/batch; 57h:48m:14s remains)
INFO - root - 2017-12-07 17:02:28.971222: step 32480, loss = 2.08, batch loss = 2.02 (13.8 examples/sec; 0.582 sec/batch; 48h:28m:36s remains)
INFO - root - 2017-12-07 17:02:35.646819: step 32490, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 54h:12m:44s remains)
INFO - root - 2017-12-07 17:02:42.266332: step 32500, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.734 sec/batch; 61h:10m:00s remains)
2017-12-07 17:02:42.951668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2660151 -4.2566009 -4.2477055 -4.240613 -4.241971 -4.2490792 -4.253531 -4.2516084 -4.245924 -4.252574 -4.2707529 -4.2817926 -4.2831173 -4.2809668 -4.283916][-4.2255888 -4.2114482 -4.2013354 -4.1955857 -4.1968145 -4.2022605 -4.2049923 -4.2030292 -4.1972733 -4.2051821 -4.2269559 -4.2382421 -4.23651 -4.2339034 -4.2388167][-4.2042255 -4.1896086 -4.1802964 -4.177794 -4.178122 -4.1766896 -4.1747308 -4.1761012 -4.1748695 -4.183022 -4.2009087 -4.2081575 -4.2048287 -4.2044196 -4.2146873][-4.1852078 -4.1712394 -4.1626368 -4.1618171 -4.1557736 -4.1404476 -4.1291571 -4.1328626 -4.14668 -4.165185 -4.1785045 -4.1813097 -4.1804705 -4.1832972 -4.200727][-4.1588683 -4.1451015 -4.138988 -4.1380043 -4.1185164 -4.0731568 -4.0336847 -4.0305381 -4.0668621 -4.1079884 -4.1267104 -4.1312428 -4.137579 -4.1476626 -4.1755633][-4.12944 -4.1146793 -4.1093235 -4.1021938 -4.0575943 -3.9707274 -3.8919985 -3.8778021 -3.9432714 -4.014019 -4.0438056 -4.0544577 -4.0680504 -4.0881457 -4.1259241][-4.1087923 -4.0931249 -4.0865555 -4.0722871 -4.0098929 -3.8924508 -3.7797971 -3.7577868 -3.8465695 -3.9364631 -3.9732792 -3.9880667 -4.0074048 -4.0340295 -4.0746469][-4.1236963 -4.1076279 -4.1023774 -4.0864286 -4.0264783 -3.9205251 -3.8218994 -3.8022323 -3.8715115 -3.9395673 -3.9672883 -3.9825654 -4.0037961 -4.0286174 -4.0625434][-4.1492906 -4.1379557 -4.1380672 -4.1272421 -4.0850177 -4.0144329 -3.9520185 -3.9386549 -3.9761117 -4.0103626 -4.0229211 -4.03638 -4.0550337 -4.0743995 -4.1017604][-4.1596394 -4.1547914 -4.1640739 -4.1673636 -4.1469054 -4.1068516 -4.0730352 -4.0654006 -4.0817142 -4.094543 -4.0968246 -4.1060276 -4.1205931 -4.1357126 -4.1587086][-4.1560912 -4.1544328 -4.174304 -4.192966 -4.1915479 -4.1738458 -4.1604648 -4.16144 -4.1684718 -4.1686006 -4.1635981 -4.1679931 -4.1765637 -4.1865892 -4.2050381][-4.1465483 -4.1484208 -4.1771026 -4.2046576 -4.2154264 -4.2118316 -4.2114711 -4.2181063 -4.22284 -4.2197914 -4.2121143 -4.2119312 -4.2161098 -4.2213793 -4.2340364][-4.1548371 -4.1601434 -4.1889195 -4.2163262 -4.2313495 -4.2333035 -4.2356224 -4.2430334 -4.2479095 -4.2459888 -4.2386165 -4.235743 -4.2345304 -4.2332406 -4.2367759][-4.1798053 -4.1862521 -4.21008 -4.2325282 -4.2465553 -4.2481613 -4.246892 -4.2489758 -4.2512622 -4.2470274 -4.2385736 -4.2327008 -4.2263064 -4.2180886 -4.2115417][-4.2093558 -4.2166595 -4.2351394 -4.2517066 -4.2613349 -4.259007 -4.2511687 -4.2459249 -4.243032 -4.2356253 -4.2263875 -4.2188363 -4.2095237 -4.1978049 -4.1834946]]...]
INFO - root - 2017-12-07 17:02:49.673440: step 32510, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 53h:28m:09s remains)
INFO - root - 2017-12-07 17:02:56.487520: step 32520, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 55h:21m:40s remains)
INFO - root - 2017-12-07 17:03:03.321722: step 32530, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 58h:56m:25s remains)
INFO - root - 2017-12-07 17:03:10.104054: step 32540, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.733 sec/batch; 61h:06m:11s remains)
INFO - root - 2017-12-07 17:03:16.785188: step 32550, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 57h:11m:40s remains)
INFO - root - 2017-12-07 17:03:23.588298: step 32560, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 54h:15m:45s remains)
INFO - root - 2017-12-07 17:03:30.310689: step 32570, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.618 sec/batch; 51h:27m:17s remains)
INFO - root - 2017-12-07 17:03:37.277543: step 32580, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 61h:11m:42s remains)
INFO - root - 2017-12-07 17:03:44.089289: step 32590, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 58h:53m:53s remains)
INFO - root - 2017-12-07 17:03:50.669476: step 32600, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 53h:15m:13s remains)
2017-12-07 17:03:51.415462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2569885 -4.2775064 -4.2920775 -4.2938519 -4.2838974 -4.2534633 -4.2170939 -4.1843405 -4.1701436 -4.1810021 -4.2161212 -4.2601175 -4.2975516 -4.3271546 -4.342557][-4.2458639 -4.2675552 -4.2860336 -4.2866783 -4.2763195 -4.2403316 -4.1946812 -4.1514111 -4.1332068 -4.1401906 -4.1762657 -4.2248793 -4.2709947 -4.3132057 -4.3379588][-4.24398 -4.2626033 -4.2790785 -4.2759032 -4.26334 -4.22246 -4.1701155 -4.1195993 -4.0952039 -4.0944309 -4.1280479 -4.18074 -4.2337346 -4.288475 -4.3262944][-4.2292519 -4.2471218 -4.2671905 -4.2601094 -4.2402143 -4.1929431 -4.1388817 -4.0831203 -4.0467048 -4.0324125 -4.0609641 -4.1187539 -4.1819987 -4.2500372 -4.3040171][-4.2076988 -4.2260947 -4.2475033 -4.2339959 -4.2019825 -4.1442366 -4.0793333 -4.0099978 -3.9607947 -3.9352341 -3.9605155 -4.0341969 -4.1192679 -4.2045193 -4.276556][-4.1882792 -4.2055931 -4.2176685 -4.1943278 -4.1564059 -4.0853019 -3.9919119 -3.8997884 -3.8312 -3.7973783 -3.8334675 -3.9405186 -4.0550761 -4.160275 -4.2504396][-4.1640835 -4.1748996 -4.1711526 -4.1381369 -4.1035695 -4.0257382 -3.91216 -3.804846 -3.7213054 -3.68824 -3.7496521 -3.8909655 -4.0257797 -4.1429925 -4.2416153][-4.1235557 -4.1288066 -4.111845 -4.0798616 -4.0601692 -4.0049443 -3.9170732 -3.8346725 -3.7667344 -3.7445598 -3.8107505 -3.9428816 -4.0656047 -4.1728806 -4.259707][-4.0582695 -4.0652084 -4.0563884 -4.0436506 -4.05245 -4.0415578 -4.0104995 -3.9728203 -3.9384458 -3.9302528 -3.9739177 -4.0620174 -4.1487188 -4.2294517 -4.2925029][-4.0188756 -4.0267162 -4.031177 -4.0443182 -4.0802026 -4.1086273 -4.1190739 -4.1136813 -4.1049485 -4.1009383 -4.1212363 -4.1683826 -4.2207794 -4.2757926 -4.3179235][-4.0407577 -4.0485878 -4.0600667 -4.0895371 -4.1353836 -4.1793337 -4.2031922 -4.20444 -4.2002296 -4.1915922 -4.1969428 -4.2208962 -4.2546697 -4.2960591 -4.3269305][-4.0888796 -4.1008635 -4.1221619 -4.1596169 -4.2024965 -4.2389669 -4.252934 -4.2411246 -4.2275734 -4.2137675 -4.2117343 -4.2295284 -4.2577052 -4.2947526 -4.3222318][-4.1192031 -4.1425247 -4.1813459 -4.2236371 -4.2596426 -4.2798524 -4.2769833 -4.2457886 -4.2198696 -4.2002578 -4.1979327 -4.220283 -4.2503915 -4.286931 -4.3145566][-4.1165209 -4.1564646 -4.2125111 -4.2639227 -4.29599 -4.3048353 -4.2922745 -4.2513113 -4.2181 -4.1938381 -4.1928144 -4.219203 -4.2511406 -4.2877326 -4.3145828][-4.1134086 -4.166893 -4.2319417 -4.286325 -4.316155 -4.3206558 -4.3080988 -4.2698154 -4.238441 -4.2144818 -4.2148824 -4.2399979 -4.2694831 -4.3020391 -4.3239269]]...]
INFO - root - 2017-12-07 17:03:58.232493: step 32610, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.700 sec/batch; 58h:17m:07s remains)
INFO - root - 2017-12-07 17:04:05.028483: step 32620, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.653 sec/batch; 54h:24m:53s remains)
INFO - root - 2017-12-07 17:04:11.802045: step 32630, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 52h:52m:28s remains)
INFO - root - 2017-12-07 17:04:18.547267: step 32640, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 52h:16m:22s remains)
INFO - root - 2017-12-07 17:04:25.408850: step 32650, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 60h:50m:26s remains)
INFO - root - 2017-12-07 17:04:32.158391: step 32660, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 57h:54m:58s remains)
INFO - root - 2017-12-07 17:04:38.894276: step 32670, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 56h:31m:39s remains)
INFO - root - 2017-12-07 17:04:45.634332: step 32680, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 53h:24m:18s remains)
INFO - root - 2017-12-07 17:04:52.445694: step 32690, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 55h:51m:48s remains)
INFO - root - 2017-12-07 17:04:59.067128: step 32700, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 58h:55m:22s remains)
2017-12-07 17:04:59.792195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31529 -4.3186259 -4.3205318 -4.3129382 -4.3021522 -4.2815313 -4.2639546 -4.2479029 -4.2430978 -4.2487636 -4.2547016 -4.2580628 -4.2482104 -4.2369232 -4.2377925][-4.3041878 -4.3017597 -4.300622 -4.2959328 -4.2846785 -4.2584558 -4.2352972 -4.2136445 -4.2040482 -4.2060952 -4.2131228 -4.2157006 -4.1993585 -4.180583 -4.18253][-4.2907543 -4.2855663 -4.2833805 -4.2805586 -4.27254 -4.2440491 -4.2148967 -4.189353 -4.1722841 -4.1670051 -4.1699815 -4.1655169 -4.1409774 -4.1227574 -4.1285691][-4.2884855 -4.2866087 -4.2824059 -4.2755795 -4.2666693 -4.236608 -4.1998253 -4.1626735 -4.1390166 -4.1300135 -4.1303759 -4.1241193 -4.1030622 -4.0864911 -4.0943255][-4.3049321 -4.30784 -4.3005958 -4.2836118 -4.2615833 -4.2212796 -4.1747932 -4.1321993 -4.1139817 -4.1120358 -4.1150117 -4.1093817 -4.0970335 -4.0900927 -4.0987296][-4.3237481 -4.32772 -4.317584 -4.2894 -4.2439795 -4.1821322 -4.1216354 -4.0841932 -4.0919037 -4.1148672 -4.1290774 -4.12686 -4.1228538 -4.1289148 -4.1395755][-4.3258924 -4.3305397 -4.3188648 -4.2817397 -4.212018 -4.1182647 -4.0261707 -3.9896467 -4.0440154 -4.1113439 -4.1501675 -4.1591949 -4.1682153 -4.1826372 -4.19669][-4.316319 -4.3224893 -4.3118157 -4.2684717 -4.1822109 -4.0601907 -3.9282124 -3.8766448 -3.9805887 -4.0983248 -4.1685076 -4.1965823 -4.2143903 -4.2303276 -4.2437243][-4.309288 -4.3144841 -4.30371 -4.2596951 -4.1681771 -4.0413194 -3.9076328 -3.856606 -3.9705613 -4.1041613 -4.1920342 -4.2327156 -4.2523122 -4.2630844 -4.2721953][-4.3060637 -4.3068624 -4.2970071 -4.2564592 -4.1716819 -4.0649838 -3.9703481 -3.945781 -4.03697 -4.147747 -4.2233329 -4.2616243 -4.2795606 -4.2853475 -4.2910652][-4.3010368 -4.2972288 -4.289053 -4.2559843 -4.1860528 -4.1077623 -4.0540757 -4.05223 -4.11644 -4.194869 -4.2496271 -4.2818913 -4.2977791 -4.3020639 -4.305037][-4.2974243 -4.2895551 -4.2819448 -4.2559061 -4.2002306 -4.1451545 -4.1198173 -4.1286759 -4.1751356 -4.2316623 -4.2693224 -4.2931633 -4.3071451 -4.3114004 -4.3125358][-4.2951059 -4.2851553 -4.27905 -4.2594008 -4.2133789 -4.1766129 -4.1693964 -4.1845918 -4.217392 -4.2548795 -4.2781081 -4.2935047 -4.3066106 -4.3128576 -4.316371][-4.2951941 -4.2848563 -4.2804637 -4.267879 -4.22998 -4.2072878 -4.2075257 -4.2186241 -4.2361689 -4.2555618 -4.2654057 -4.2750068 -4.2911181 -4.3034 -4.3133388][-4.2963367 -4.2871895 -4.2852206 -4.2786922 -4.2477093 -4.233562 -4.2366996 -4.2394166 -4.2399693 -4.2418242 -4.2380767 -4.242929 -4.2642884 -4.2855892 -4.3042531]]...]
INFO - root - 2017-12-07 17:05:06.443003: step 32710, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 52h:42m:22s remains)
INFO - root - 2017-12-07 17:05:13.222743: step 32720, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 54h:28m:38s remains)
INFO - root - 2017-12-07 17:05:19.936270: step 32730, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 60h:26m:16s remains)
INFO - root - 2017-12-07 17:05:26.885869: step 32740, loss = 2.04, batch loss = 1.98 (10.5 examples/sec; 0.764 sec/batch; 63h:37m:09s remains)
INFO - root - 2017-12-07 17:05:33.765194: step 32750, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 56h:52m:54s remains)
INFO - root - 2017-12-07 17:05:40.532754: step 32760, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:02m:08s remains)
INFO - root - 2017-12-07 17:05:47.337898: step 32770, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 55h:37m:52s remains)
INFO - root - 2017-12-07 17:05:54.154717: step 32780, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 59h:09m:22s remains)
INFO - root - 2017-12-07 17:06:01.040591: step 32790, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 57h:10m:23s remains)
INFO - root - 2017-12-07 17:06:07.340771: step 32800, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 54h:55m:42s remains)
2017-12-07 17:06:08.098440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2665634 -4.2739549 -4.2753325 -4.2673645 -4.2537832 -4.2488766 -4.248004 -4.2494979 -4.2536335 -4.2552791 -4.2599082 -4.2627492 -4.2721176 -4.2889347 -4.3028407][-4.2684298 -4.2761741 -4.2768583 -4.2636638 -4.242979 -4.2370119 -4.2363162 -4.2337956 -4.2339907 -4.2379003 -4.2488909 -4.2532811 -4.2608557 -4.2787504 -4.2935295][-4.2698607 -4.2755513 -4.2734842 -4.2539086 -4.2263474 -4.2208419 -4.2196379 -4.21783 -4.2162862 -4.2208819 -4.2368045 -4.2453933 -4.2530208 -4.2683167 -4.2798319][-4.265132 -4.2646604 -4.25472 -4.2320485 -4.2046318 -4.1968489 -4.1868868 -4.1809196 -4.1843863 -4.1956019 -4.2148018 -4.2320065 -4.2459006 -4.2609205 -4.2693524][-4.2502651 -4.2303705 -4.199964 -4.1686444 -4.1445379 -4.1321731 -4.113492 -4.1058145 -4.126729 -4.1536231 -4.1773024 -4.2036858 -4.2249279 -4.2494497 -4.2626085][-4.2163048 -4.1744103 -4.1241679 -4.0856 -4.0618382 -4.0363884 -3.9965 -3.977772 -4.0235496 -4.076344 -4.1104479 -4.1450782 -4.1730976 -4.2134657 -4.2428818][-4.1807537 -4.1223159 -4.0627341 -4.0217247 -3.9904265 -3.9458129 -3.8722901 -3.833056 -3.8994124 -3.9775248 -4.0243931 -4.0662203 -4.0966773 -4.1471872 -4.1937666][-4.158011 -4.0997591 -4.0519085 -4.0211673 -3.9847748 -3.9245763 -3.8271003 -3.7698021 -3.8255649 -3.8963933 -3.9408731 -3.9835751 -4.0209179 -4.07682 -4.13485][-4.1621695 -4.11763 -4.0904408 -4.078 -4.0526524 -3.9957802 -3.8994112 -3.8346784 -3.8529837 -3.888304 -3.9102108 -3.9446144 -3.9892585 -4.0490785 -4.111649][-4.19855 -4.1731186 -4.1623993 -4.1616149 -4.1477323 -4.1036286 -4.0305138 -3.9760857 -3.9650929 -3.9654641 -3.9638319 -3.9868057 -4.0290704 -4.0834622 -4.1379566][-4.2506175 -4.2416425 -4.2426929 -4.2451997 -4.23198 -4.195672 -4.1471739 -4.1109962 -4.0917516 -4.0756545 -4.0634041 -4.0799932 -4.112462 -4.1528935 -4.1945348][-4.3032856 -4.3029094 -4.3070474 -4.3075266 -4.2933164 -4.2652578 -4.2341866 -4.2125807 -4.199265 -4.1872039 -4.1774859 -4.1846704 -4.2019982 -4.22682 -4.2556443][-4.3308949 -4.3326917 -4.3330622 -4.3319654 -4.3240948 -4.3075666 -4.2910271 -4.2815909 -4.2794042 -4.27541 -4.270884 -4.2686071 -4.2716985 -4.2842197 -4.3026576][-4.3339 -4.3348417 -4.3332028 -4.3312688 -4.327817 -4.3213639 -4.3150091 -4.31259 -4.313766 -4.3155041 -4.3142381 -4.3102393 -4.3099785 -4.3159361 -4.3270817][-4.3284874 -4.3287559 -4.3266411 -4.3251023 -4.3234997 -4.3220754 -4.3203831 -4.3197026 -4.3204575 -4.3223286 -4.3241239 -4.3244972 -4.3253126 -4.3285789 -4.3347487]]...]
INFO - root - 2017-12-07 17:06:14.743307: step 32810, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 58h:01m:24s remains)
INFO - root - 2017-12-07 17:06:21.523931: step 32820, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 55h:47m:51s remains)
INFO - root - 2017-12-07 17:06:28.276078: step 32830, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.640 sec/batch; 53h:14m:00s remains)
INFO - root - 2017-12-07 17:06:34.952410: step 32840, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.640 sec/batch; 53h:18m:01s remains)
INFO - root - 2017-12-07 17:06:41.851431: step 32850, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 60h:44m:54s remains)
INFO - root - 2017-12-07 17:06:48.654683: step 32860, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.746 sec/batch; 62h:05m:04s remains)
INFO - root - 2017-12-07 17:06:55.410113: step 32870, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 54h:16m:05s remains)
INFO - root - 2017-12-07 17:07:02.213268: step 32880, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 53h:12m:20s remains)
INFO - root - 2017-12-07 17:07:08.964408: step 32890, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 52h:42m:27s remains)
INFO - root - 2017-12-07 17:07:15.565468: step 32900, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 57h:27m:28s remains)
2017-12-07 17:07:16.339071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2744031 -4.2655287 -4.2636971 -4.2544475 -4.2335835 -4.2195983 -4.2253127 -4.232338 -4.2306266 -4.2340722 -4.2445617 -4.2541189 -4.2560644 -4.250504 -4.2266822][-4.2696352 -4.2666984 -4.2703023 -4.263257 -4.2436376 -4.2328224 -4.235837 -4.247395 -4.2470007 -4.2435179 -4.2435622 -4.2444339 -4.2427206 -4.2393861 -4.2250342][-4.2617784 -4.2525749 -4.2514014 -4.2441707 -4.2260747 -4.2152414 -4.220365 -4.2357359 -4.2376757 -4.2308517 -4.22292 -4.2151704 -4.2088351 -4.2089944 -4.2048163][-4.2410188 -4.2221947 -4.2144852 -4.2044454 -4.1867337 -4.1806936 -4.1906385 -4.2016449 -4.2006874 -4.1930804 -4.1827617 -4.1729312 -4.1649055 -4.16613 -4.1711063][-4.2035027 -4.1805363 -4.1749759 -4.1678052 -4.15262 -4.1473556 -4.153276 -4.1544204 -4.1517754 -4.1472216 -4.1410661 -4.1335087 -4.1245584 -4.1278434 -4.1397963][-4.1628671 -4.1379037 -4.1351023 -4.1344728 -4.1180983 -4.104588 -4.0939426 -4.0869918 -4.09505 -4.1102462 -4.1232033 -4.1252813 -4.1193013 -4.1241312 -4.1346588][-4.1462612 -4.1166916 -4.1062803 -4.1050067 -4.0852971 -4.0525236 -4.0118856 -3.9988942 -4.0330181 -4.0846376 -4.1220336 -4.1356606 -4.1341448 -4.1408415 -4.143281][-4.1547775 -4.1217155 -4.1027131 -4.09227 -4.0646553 -4.00803 -3.940347 -3.9252915 -3.9898853 -4.071475 -4.1202912 -4.1383524 -4.143095 -4.1515274 -4.151938][-4.1734881 -4.1436911 -4.1230025 -4.1106958 -4.087935 -4.0401616 -3.984278 -3.9787266 -4.0420542 -4.1133575 -4.1502872 -4.16087 -4.1656432 -4.1690645 -4.1673183][-4.1991563 -4.1751885 -4.1580081 -4.1532822 -4.1456022 -4.1231623 -4.0943789 -4.097661 -4.1399302 -4.1828766 -4.2043867 -4.2103887 -4.2129469 -4.2111397 -4.20459][-4.2117157 -4.1953111 -4.1835079 -4.188046 -4.1949253 -4.1913357 -4.1799979 -4.1873565 -4.2155776 -4.243866 -4.2593637 -4.2662454 -4.2640071 -4.2555108 -4.2434278][-4.2039304 -4.2001152 -4.1980634 -4.2074533 -4.2243414 -4.234901 -4.2352009 -4.2455649 -4.26425 -4.2832108 -4.2950311 -4.3006387 -4.2953682 -4.2836204 -4.2688212][-4.1901507 -4.1907978 -4.1940193 -4.2059941 -4.2288284 -4.2469735 -4.2565036 -4.2679543 -4.2808409 -4.2949438 -4.3022509 -4.3040752 -4.299407 -4.28837 -4.2768755][-4.1805854 -4.1823969 -4.1873455 -4.2011714 -4.2240715 -4.2423682 -4.2561345 -4.268074 -4.2754159 -4.283926 -4.2884107 -4.2915735 -4.2931242 -4.2882166 -4.2827449][-4.1994233 -4.1960144 -4.1950097 -4.2027674 -4.2169914 -4.2266235 -4.2375326 -4.2485261 -4.25283 -4.2586761 -4.2653642 -4.2739553 -4.2833033 -4.2859859 -4.2836537]]...]
INFO - root - 2017-12-07 17:07:23.067173: step 32910, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 53h:04m:45s remains)
INFO - root - 2017-12-07 17:07:29.822509: step 32920, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 52h:39m:02s remains)
INFO - root - 2017-12-07 17:07:36.649775: step 32930, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 59h:04m:01s remains)
INFO - root - 2017-12-07 17:07:43.614043: step 32940, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.773 sec/batch; 64h:20m:58s remains)
INFO - root - 2017-12-07 17:07:50.288475: step 32950, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 55h:09m:27s remains)
INFO - root - 2017-12-07 17:07:57.029361: step 32960, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 51h:47m:38s remains)
INFO - root - 2017-12-07 17:08:03.807548: step 32970, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 53h:11m:58s remains)
INFO - root - 2017-12-07 17:08:10.768097: step 32980, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 61h:17m:21s remains)
INFO - root - 2017-12-07 17:08:17.549264: step 32990, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 59h:34m:00s remains)
INFO - root - 2017-12-07 17:08:24.077213: step 33000, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 53h:58m:30s remains)
2017-12-07 17:08:24.834557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2874694 -4.2458129 -4.2270145 -4.2257323 -4.2366862 -4.2458487 -4.2443466 -4.239769 -4.2327127 -4.2329354 -4.24797 -4.2621174 -4.2609854 -4.2563624 -4.2561841][-4.2627578 -4.2115059 -4.19316 -4.1917691 -4.2008424 -4.2045126 -4.1983013 -4.186552 -4.1727705 -4.1765313 -4.206841 -4.2330666 -4.2332458 -4.2294283 -4.2326131][-4.2463646 -4.1937003 -4.1733289 -4.1660771 -4.1695485 -4.1670613 -4.1524591 -4.1259327 -4.0994697 -4.110404 -4.1639538 -4.2055507 -4.2101049 -4.2099485 -4.2179742][-4.24053 -4.189332 -4.1634521 -4.1437378 -4.134151 -4.1284952 -4.1114078 -4.0710225 -4.0329423 -4.0581269 -4.1358256 -4.1886044 -4.1982336 -4.2024245 -4.2135868][-4.2363763 -4.190908 -4.1635156 -4.1297374 -4.10414 -4.0959659 -4.0736556 -4.0134282 -3.9644556 -4.0082626 -4.1032953 -4.1643691 -4.1837683 -4.195612 -4.2114277][-4.2398152 -4.203043 -4.1783929 -4.1409693 -4.1097312 -4.0920839 -4.0390253 -3.9319465 -3.8683825 -3.9423351 -4.061295 -4.1356535 -4.1685281 -4.1895208 -4.2072606][-4.2522674 -4.2230821 -4.1976347 -4.1619496 -4.1329837 -4.0894156 -3.9731009 -3.7843289 -3.7133546 -3.8474596 -4.0138636 -4.1127157 -4.1611209 -4.1879869 -4.2025485][-4.2669015 -4.2409382 -4.2056046 -4.1624217 -4.1257153 -4.0460334 -3.8627827 -3.5860224 -3.5158112 -3.7378137 -3.9609385 -4.0887885 -4.1522546 -4.1831279 -4.1943159][-4.2807231 -4.2528119 -4.2105193 -4.1584392 -4.1091967 -4.0122423 -3.7889314 -3.4574172 -3.4099064 -3.6936028 -3.94072 -4.0775552 -4.1439414 -4.1716814 -4.1798296][-4.2986207 -4.2717042 -4.231719 -4.1831241 -4.1366849 -4.0480318 -3.8564548 -3.5889897 -3.5736341 -3.8010538 -3.9909167 -4.0947714 -4.1458774 -4.1654277 -4.1720905][-4.3146696 -4.2958012 -4.2694817 -4.2358627 -4.1997743 -4.1277056 -3.9812472 -3.8000302 -3.8046081 -3.9546931 -4.0701256 -4.1332455 -4.1646585 -4.1799808 -4.1861968][-4.3256917 -4.3149195 -4.3001738 -4.2761917 -4.2447929 -4.1835241 -4.0711403 -3.9571366 -3.9798281 -4.0808716 -4.1452665 -4.1811109 -4.1995387 -4.2114773 -4.215589][-4.3333263 -4.3264565 -4.314044 -4.2898684 -4.2606463 -4.2114539 -4.1265531 -4.0548611 -4.086863 -4.1584163 -4.2002974 -4.2228651 -4.2363329 -4.2461796 -4.2471361][-4.3315468 -4.3288 -4.3188114 -4.2939835 -4.2667036 -4.2337732 -4.1766577 -4.1317434 -4.1608672 -4.211544 -4.2415295 -4.2573404 -4.2651024 -4.2729669 -4.2730165][-4.3235383 -4.3204179 -4.3127761 -4.2930775 -4.2726693 -4.2510023 -4.2204642 -4.198257 -4.2181444 -4.2496252 -4.2708511 -4.2800937 -4.2839503 -4.288867 -4.28924]]...]
INFO - root - 2017-12-07 17:08:31.764882: step 33010, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 60h:54m:56s remains)
INFO - root - 2017-12-07 17:08:38.588195: step 33020, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 57h:04m:08s remains)
INFO - root - 2017-12-07 17:08:45.426065: step 33030, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.620 sec/batch; 51h:33m:36s remains)
INFO - root - 2017-12-07 17:08:52.165010: step 33040, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 53h:23m:25s remains)
INFO - root - 2017-12-07 17:08:59.057218: step 33050, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 59h:28m:28s remains)
INFO - root - 2017-12-07 17:09:05.866853: step 33060, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 59h:39m:24s remains)
INFO - root - 2017-12-07 17:09:12.658680: step 33070, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 55h:06m:14s remains)
INFO - root - 2017-12-07 17:09:19.405016: step 33080, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 53h:17m:59s remains)
INFO - root - 2017-12-07 17:09:26.174121: step 33090, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 54h:04m:39s remains)
INFO - root - 2017-12-07 17:09:32.862577: step 33100, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 59h:29m:41s remains)
2017-12-07 17:09:33.556764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1925163 -4.20872 -4.2254162 -4.2335734 -4.22833 -4.2172647 -4.204597 -4.19107 -4.1763678 -4.17272 -4.1857953 -4.1991224 -4.2080741 -4.2151718 -4.2250576][-4.1889286 -4.2041168 -4.2198634 -4.2280097 -4.222487 -4.2119226 -4.1994214 -4.1863217 -4.1732197 -4.170485 -4.1840544 -4.1965108 -4.2036843 -4.2079282 -4.2159295][-4.1919022 -4.2050776 -4.2185163 -4.2258344 -4.2208076 -4.2106066 -4.19804 -4.1855116 -4.1731024 -4.1702805 -4.1831617 -4.1936274 -4.1983671 -4.2000012 -4.2060289][-4.1911969 -4.2020783 -4.21447 -4.222507 -4.2190228 -4.2089887 -4.1965814 -4.1847496 -4.1723604 -4.1685834 -4.1809144 -4.191103 -4.1939116 -4.1937685 -4.1980114][-4.1920524 -4.1996856 -4.2106261 -4.2188125 -4.2169018 -4.2066193 -4.1939969 -4.1830783 -4.1702571 -4.1656404 -4.1769762 -4.1862288 -4.1876645 -4.1875668 -4.1928868][-4.1903605 -4.1943431 -4.2030268 -4.2115984 -4.2098374 -4.1986847 -4.184967 -4.1741481 -4.1618433 -4.1588192 -4.1696033 -4.1776056 -4.1789503 -4.1817088 -4.1907992][-4.1913376 -4.1917205 -4.1982555 -4.2046704 -4.1992764 -4.1838779 -4.1663389 -4.153542 -4.1432562 -4.1462555 -4.1605415 -4.17042 -4.1741939 -4.1808739 -4.1921468][-4.1991892 -4.1947827 -4.1964 -4.1981773 -4.18745 -4.1670613 -4.1443124 -4.1266317 -4.1170559 -4.1285086 -4.1501579 -4.1652231 -4.1743422 -4.1846147 -4.1949692][-4.2097688 -4.202981 -4.1998072 -4.1959319 -4.1816411 -4.1590638 -4.1335135 -4.1106224 -4.0983944 -4.1134129 -4.1401548 -4.1606011 -4.1754079 -4.1876483 -4.1953235][-4.221272 -4.213851 -4.207273 -4.1985555 -4.1824155 -4.1620345 -4.1388006 -4.1162658 -4.1033316 -4.1157436 -4.1387897 -4.1584415 -4.1752224 -4.1870041 -4.1913662][-4.2251854 -4.2204785 -4.2145638 -4.2047982 -4.190053 -4.1743965 -4.1568675 -4.1392345 -4.1292839 -4.138185 -4.1525445 -4.1650982 -4.178268 -4.1869116 -4.1886225][-4.216825 -4.215991 -4.2135334 -4.2065992 -4.19555 -4.1840677 -4.1710253 -4.1580176 -4.1520472 -4.1588554 -4.16536 -4.1693883 -4.1769595 -4.1819773 -4.1827855][-4.2089868 -4.2115116 -4.2119927 -4.208148 -4.2021456 -4.1954236 -4.1855931 -4.1755085 -4.1704483 -4.1738482 -4.1747375 -4.1734347 -4.177249 -4.1800766 -4.1813693][-4.2005763 -4.206172 -4.2103891 -4.2104907 -4.2092166 -4.2065597 -4.198637 -4.1893535 -4.1830277 -4.1825471 -4.1800141 -4.176477 -4.1779194 -4.17966 -4.1821132][-4.1999869 -4.20706 -4.2148089 -4.2186885 -4.2205505 -4.2197113 -4.2129054 -4.2034054 -4.19572 -4.1920123 -4.1879568 -4.1846089 -4.185524 -4.1874661 -4.190372]]...]
INFO - root - 2017-12-07 17:09:40.132803: step 33110, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 52h:47m:18s remains)
INFO - root - 2017-12-07 17:09:46.946048: step 33120, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 59h:53m:29s remains)
INFO - root - 2017-12-07 17:09:53.837270: step 33130, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 59h:21m:31s remains)
INFO - root - 2017-12-07 17:10:00.658082: step 33140, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 54h:47m:10s remains)
INFO - root - 2017-12-07 17:10:07.415066: step 33150, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 54h:34m:13s remains)
INFO - root - 2017-12-07 17:10:14.109265: step 33160, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 53h:23m:46s remains)
INFO - root - 2017-12-07 17:10:20.861596: step 33170, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 56h:37m:23s remains)
INFO - root - 2017-12-07 17:10:27.748288: step 33180, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.752 sec/batch; 62h:31m:59s remains)
INFO - root - 2017-12-07 17:10:34.532200: step 33190, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 54h:33m:25s remains)
INFO - root - 2017-12-07 17:10:41.154767: step 33200, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 52h:11m:56s remains)
2017-12-07 17:10:41.915243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2824593 -4.28153 -4.2754135 -4.2604303 -4.2345195 -4.1910024 -4.1370482 -4.098875 -4.0885797 -4.1033444 -4.1309814 -4.1553335 -4.170475 -4.1766887 -4.2041292][-4.27026 -4.2717757 -4.26974 -4.2548995 -4.2208924 -4.1623616 -4.0997143 -4.0627241 -4.0625739 -4.0913849 -4.1276565 -4.1541743 -4.1731973 -4.1829824 -4.2109566][-4.25933 -4.2635117 -4.2620726 -4.245234 -4.2046695 -4.1371832 -4.076921 -4.0497437 -4.0662584 -4.1065574 -4.1379018 -4.1570215 -4.1765466 -4.1901007 -4.2168884][-4.2391319 -4.2551894 -4.25812 -4.2412672 -4.1985464 -4.1264815 -4.0682993 -4.0531869 -4.0861778 -4.1321969 -4.1535821 -4.1636047 -4.1784253 -4.1922932 -4.2181358][-4.2130146 -4.24208 -4.2529397 -4.2411356 -4.1933784 -4.1101522 -4.0420861 -4.0308552 -4.0817418 -4.1385717 -4.1585126 -4.1642232 -4.1751876 -4.1904421 -4.2172813][-4.1907039 -4.2233477 -4.2365294 -4.2202096 -4.1589289 -4.0516376 -3.948077 -3.9311783 -4.0203667 -4.1062021 -4.13929 -4.1467762 -4.1616855 -4.1852436 -4.2182212][-4.1578441 -4.1949134 -4.2095618 -4.182735 -4.1016607 -3.9625859 -3.8068893 -3.7725587 -3.917187 -4.0487132 -4.103334 -4.1228414 -4.1493692 -4.1843095 -4.2235556][-4.1022263 -4.1498528 -4.1772456 -4.1557212 -4.0754023 -3.9296107 -3.7579534 -3.7140741 -3.8829472 -4.03035 -4.0883369 -4.1133556 -4.1453452 -4.1874576 -4.2275372][-4.0792294 -4.1304512 -4.16977 -4.1642861 -4.1044011 -3.9903166 -3.8654108 -3.8353734 -3.9623015 -4.0758228 -4.1074152 -4.1165204 -4.14092 -4.1863403 -4.2217135][-4.1195574 -4.1551166 -4.1885843 -4.1888723 -4.1502571 -4.0777111 -4.005887 -3.9904358 -4.0671535 -4.136097 -4.1428709 -4.1369328 -4.1526213 -4.19499 -4.21891][-4.1924424 -4.2086658 -4.2266192 -4.2226276 -4.1938968 -4.1498 -4.1081772 -4.0983434 -4.1382542 -4.175097 -4.1708593 -4.1638975 -4.1803217 -4.2191143 -4.2330284][-4.2401934 -4.24102 -4.2466011 -4.235847 -4.2133679 -4.1833844 -4.155026 -4.1477041 -4.1719084 -4.1951628 -4.1904211 -4.1852446 -4.2027936 -4.2376118 -4.247767][-4.2510228 -4.2458324 -4.2450376 -4.229116 -4.208868 -4.1878004 -4.1706762 -4.1711864 -4.19405 -4.2135243 -4.20891 -4.200727 -4.2175021 -4.25203 -4.2650995][-4.2651186 -4.2583551 -4.2518797 -4.2345686 -4.2164268 -4.2008095 -4.1917357 -4.1983495 -4.2200689 -4.2381372 -4.2364092 -4.2282834 -4.2422848 -4.2735467 -4.2887311][-4.281693 -4.2727528 -4.26423 -4.2491283 -4.2356377 -4.2262864 -4.2251315 -4.230783 -4.24564 -4.2613 -4.2638607 -4.2637243 -4.2776327 -4.303453 -4.3173876]]...]
INFO - root - 2017-12-07 17:10:48.688467: step 33210, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.734 sec/batch; 61h:00m:34s remains)
INFO - root - 2017-12-07 17:10:55.415747: step 33220, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 56h:13m:58s remains)
INFO - root - 2017-12-07 17:11:02.204649: step 33230, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 52h:25m:13s remains)
INFO - root - 2017-12-07 17:11:09.024819: step 33240, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 57h:59m:57s remains)
INFO - root - 2017-12-07 17:11:15.908562: step 33250, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 58h:19m:24s remains)
INFO - root - 2017-12-07 17:11:22.749525: step 33260, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.723 sec/batch; 60h:07m:43s remains)
INFO - root - 2017-12-07 17:11:29.468433: step 33270, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 56h:35m:20s remains)
INFO - root - 2017-12-07 17:11:36.200254: step 33280, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 52h:47m:38s remains)
INFO - root - 2017-12-07 17:11:43.008031: step 33290, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 55h:40m:12s remains)
INFO - root - 2017-12-07 17:11:49.736444: step 33300, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.766 sec/batch; 63h:41m:45s remains)
2017-12-07 17:11:50.465447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2778196 -4.292654 -4.2968211 -4.2736921 -4.2416835 -4.2039614 -4.1573324 -4.1222234 -4.1174 -4.1566176 -4.2151222 -4.2709303 -4.3117471 -4.3318672 -4.3426008][-4.291235 -4.3097692 -4.3131614 -4.2822218 -4.2429681 -4.2006555 -4.1426835 -4.0930948 -4.0840197 -4.131321 -4.1982865 -4.2628584 -4.3092122 -4.3314052 -4.3431787][-4.2946734 -4.3147984 -4.3185592 -4.284234 -4.2402673 -4.1906338 -4.1186457 -4.0542474 -4.0458512 -4.1042714 -4.179863 -4.2532611 -4.3043013 -4.3286853 -4.3413553][-4.2756519 -4.2982383 -4.30402 -4.2719913 -4.2272372 -4.1691332 -4.0837927 -4.0114455 -4.0094509 -4.0802422 -4.1650972 -4.2453141 -4.3005285 -4.3263597 -4.3393383][-4.2546272 -4.2777948 -4.2817717 -4.2512016 -4.2022276 -4.1320219 -4.0359764 -3.9641848 -3.9770725 -4.0622191 -4.1609921 -4.2478943 -4.3039651 -4.3281884 -4.3391571][-4.2457161 -4.264987 -4.2614937 -4.2274694 -4.169734 -4.0836158 -3.9815538 -3.9245453 -3.959625 -4.057591 -4.1677995 -4.259675 -4.3135967 -4.3340616 -4.3417244][-4.233633 -4.243927 -4.2303643 -4.1901016 -4.1188536 -4.0122671 -3.9105144 -3.8904843 -3.955549 -4.0620317 -4.178874 -4.2712207 -4.3224845 -4.3401589 -4.3448634][-4.2205811 -4.2212958 -4.2017365 -4.1563458 -4.0711317 -3.9455185 -3.848927 -3.8702328 -3.9637616 -4.0777636 -4.1946964 -4.2826233 -4.3289318 -4.345221 -4.3477745][-4.2259426 -4.22053 -4.1972141 -4.1560974 -4.0755396 -3.9568338 -3.8735695 -3.9036274 -3.9918144 -4.0944562 -4.2003551 -4.2831531 -4.3293681 -4.3476515 -4.34996][-4.2356181 -4.231729 -4.210403 -4.176343 -4.1091189 -4.0104337 -3.9436164 -3.9640055 -4.0272117 -4.1089783 -4.2012606 -4.2801771 -4.3273568 -4.3472056 -4.3505058][-4.2486224 -4.2450519 -4.2255726 -4.196003 -4.1416812 -4.0639043 -4.0101261 -4.0201592 -4.0657468 -4.1314135 -4.2093458 -4.2801332 -4.3253751 -4.3456035 -4.350275][-4.2659445 -4.2602086 -4.2414179 -4.2156787 -4.1732316 -4.110095 -4.0631723 -4.0638585 -4.1009855 -4.1597934 -4.225729 -4.285532 -4.3257456 -4.3449068 -4.3501749][-4.2777815 -4.2701135 -4.2549233 -4.2359376 -4.2031679 -4.1503129 -4.1070533 -4.0990491 -4.1303134 -4.1838436 -4.2422547 -4.2932506 -4.3278003 -4.3452387 -4.3501906][-4.2864795 -4.2802997 -4.2711549 -4.2575541 -4.2301311 -4.1855621 -4.1477237 -4.136075 -4.1595039 -4.2032928 -4.2524166 -4.2965927 -4.3282442 -4.345161 -4.3497567][-4.2865663 -4.2803531 -4.2752972 -4.2634888 -4.2372751 -4.1985183 -4.1662664 -4.1524367 -4.1716366 -4.20969 -4.2555513 -4.297534 -4.3277569 -4.3447785 -4.3491468]]...]
INFO - root - 2017-12-07 17:11:57.150629: step 33310, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 52h:53m:30s remains)
INFO - root - 2017-12-07 17:12:03.962595: step 33320, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 61h:02m:11s remains)
INFO - root - 2017-12-07 17:12:10.788031: step 33330, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 60h:47m:57s remains)
INFO - root - 2017-12-07 17:12:17.597896: step 33340, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 53h:06m:08s remains)
INFO - root - 2017-12-07 17:12:24.359259: step 33350, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 52h:37m:25s remains)
INFO - root - 2017-12-07 17:12:31.068707: step 33360, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 54h:08m:33s remains)
INFO - root - 2017-12-07 17:12:37.852062: step 33370, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.741 sec/batch; 61h:36m:03s remains)
INFO - root - 2017-12-07 17:12:44.740076: step 33380, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 60h:56m:09s remains)
INFO - root - 2017-12-07 17:12:51.482674: step 33390, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.623 sec/batch; 51h:45m:17s remains)
INFO - root - 2017-12-07 17:12:58.056978: step 33400, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.643 sec/batch; 53h:23m:06s remains)
2017-12-07 17:12:58.767513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1582313 -4.1630211 -4.1685534 -4.1708665 -4.175416 -4.181839 -4.1992311 -4.2101083 -4.2184353 -4.234767 -4.2377944 -4.2307782 -4.224071 -4.2195663 -4.2041311][-4.1267529 -4.1381426 -4.1592617 -4.1817493 -4.1981792 -4.2103186 -4.2299852 -4.2378693 -4.2342587 -4.2436743 -4.2466741 -4.2404609 -4.2318411 -4.2238293 -4.2100978][-4.1361046 -4.1589866 -4.1907473 -4.2230034 -4.24357 -4.2539878 -4.2675228 -4.268652 -4.2539625 -4.2545867 -4.2627697 -4.2650633 -4.2570987 -4.2483721 -4.2357564][-4.1949444 -4.2202425 -4.248302 -4.2701235 -4.2840352 -4.2864184 -4.2854447 -4.2736826 -4.2508931 -4.2433271 -4.2536135 -4.2652726 -4.2636385 -4.2590742 -4.2487106][-4.2577515 -4.2737732 -4.2877464 -4.2930737 -4.2932992 -4.2799969 -4.2520752 -4.2220397 -4.2018976 -4.1965566 -4.210113 -4.2332439 -4.2436142 -4.2463641 -4.2454009][-4.294673 -4.2932262 -4.2909985 -4.282279 -4.263351 -4.2246594 -4.1635351 -4.1171622 -4.1167078 -4.1275744 -4.1494985 -4.187871 -4.2159586 -4.2341828 -4.2532525][-4.3040881 -4.2875514 -4.2698593 -4.2454834 -4.2065234 -4.1397891 -4.0360103 -3.9694467 -4.0021453 -4.0495305 -4.0873766 -4.1399803 -4.1851997 -4.2228451 -4.2652373][-4.3062677 -4.2842064 -4.2503395 -4.2082839 -4.1531606 -4.0698504 -3.9424746 -3.8626204 -3.9278114 -4.0082283 -4.0636883 -4.1228261 -4.1743836 -4.2219844 -4.2725625][-4.3062606 -4.2830529 -4.2393751 -4.1838803 -4.1262054 -4.0580111 -3.9647107 -3.9135268 -3.970938 -4.0381908 -4.0974059 -4.1579247 -4.2057352 -4.2485166 -4.2887836][-4.2998943 -4.273252 -4.2329893 -4.1850462 -4.1389866 -4.0961061 -4.0522895 -4.0426955 -4.087049 -4.1306443 -4.1773696 -4.229228 -4.2683244 -4.2958961 -4.3174348][-4.2828646 -4.2507 -4.2145953 -4.1810174 -4.1543469 -4.1413279 -4.13984 -4.1605158 -4.1971016 -4.2238221 -4.2522869 -4.2858233 -4.3112993 -4.3267169 -4.3361297][-4.2449894 -4.2003779 -4.1631889 -4.1427245 -4.1384039 -4.155283 -4.1878943 -4.2268357 -4.2572832 -4.2734642 -4.2874241 -4.3047786 -4.3176179 -4.3251796 -4.3280144][-4.1998982 -4.14134 -4.0993872 -4.0887966 -4.1096282 -4.1532307 -4.2079268 -4.2581034 -4.283896 -4.2884889 -4.2889156 -4.2940254 -4.3013077 -4.3056688 -4.3039789][-4.1623564 -4.0985632 -4.0568409 -4.0592012 -4.1038222 -4.1660719 -4.2259474 -4.2683721 -4.2810221 -4.2761669 -4.2686343 -4.2655659 -4.2725625 -4.2793183 -4.2730436][-4.1634436 -4.1134858 -4.082756 -4.0918708 -4.1381726 -4.194561 -4.240325 -4.2634053 -4.263721 -4.2526569 -4.2400045 -4.2282257 -4.2311792 -4.2390785 -4.2339683]]...]
INFO - root - 2017-12-07 17:13:05.572688: step 33410, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 55h:52m:25s remains)
INFO - root - 2017-12-07 17:13:12.200075: step 33420, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 52h:13m:58s remains)
INFO - root - 2017-12-07 17:13:19.020751: step 33430, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 55h:56m:10s remains)
INFO - root - 2017-12-07 17:13:25.869292: step 33440, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 59h:24m:45s remains)
INFO - root - 2017-12-07 17:13:32.639145: step 33450, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 56h:48m:33s remains)
INFO - root - 2017-12-07 17:13:39.413552: step 33460, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 56h:52m:41s remains)
INFO - root - 2017-12-07 17:13:46.197382: step 33470, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 54h:31m:08s remains)
INFO - root - 2017-12-07 17:13:52.996709: step 33480, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 58h:41m:52s remains)
INFO - root - 2017-12-07 17:13:59.801437: step 33490, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 61h:35m:49s remains)
INFO - root - 2017-12-07 17:14:06.464219: step 33500, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 56h:05m:04s remains)
2017-12-07 17:14:07.139930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1419253 -4.1189456 -4.1249895 -4.1536117 -4.1799679 -4.1917033 -4.1809978 -4.170815 -4.1818242 -4.1988292 -4.202497 -4.1935692 -4.1686316 -4.1425414 -4.1350102][-4.1494889 -4.1221995 -4.1266117 -4.1559615 -4.1851988 -4.1996927 -4.1883459 -4.1764479 -4.1871657 -4.2022452 -4.20673 -4.1968846 -4.1658835 -4.1353564 -4.12827][-4.17145 -4.1448236 -4.148036 -4.174242 -4.2009807 -4.2119703 -4.195529 -4.1811681 -4.1900043 -4.203105 -4.2049904 -4.1921058 -4.1579914 -4.125824 -4.1213317][-4.1765165 -4.1582909 -4.16322 -4.1873755 -4.2079062 -4.2136564 -4.1928711 -4.1709943 -4.173954 -4.1873388 -4.1901035 -4.1759748 -4.1444178 -4.1138082 -4.1128659][-4.1547112 -4.1521611 -4.16819 -4.1953478 -4.2109728 -4.2116261 -4.1859183 -4.1577296 -4.1542673 -4.1691985 -4.1773887 -4.1691341 -4.1483779 -4.1264796 -4.1299248][-4.1238308 -4.13403 -4.1650972 -4.1967411 -4.2041726 -4.1923141 -4.1615281 -4.1313581 -4.1267662 -4.145854 -4.1589289 -4.16144 -4.1573677 -4.1504331 -4.1604085][-4.0923858 -4.1083832 -4.1496038 -4.1798453 -4.1757 -4.1457539 -4.104713 -4.0749745 -4.078649 -4.1107178 -4.1348462 -4.1502643 -4.1642151 -4.17357 -4.1908021][-4.0854421 -4.1067452 -4.1431031 -4.1586881 -4.1357675 -4.0822783 -4.0254221 -3.993396 -4.0051751 -4.0598693 -4.106442 -4.1395054 -4.1709437 -4.1936364 -4.2153182][-4.1054935 -4.1332903 -4.1555748 -4.1526012 -4.1154561 -4.0524988 -3.9922919 -3.9626577 -3.9786794 -4.045629 -4.1078982 -4.1562734 -4.1972795 -4.2227879 -4.2387528][-4.1318226 -4.167336 -4.1854849 -4.1785374 -4.1453433 -4.0940208 -4.0495663 -4.0305691 -4.0419807 -4.0921822 -4.1477442 -4.200089 -4.2417626 -4.2632041 -4.2690039][-4.134645 -4.177403 -4.2011166 -4.2040277 -4.18916 -4.160985 -4.1383057 -4.1301327 -4.1342087 -4.1597691 -4.1958389 -4.2385654 -4.2731009 -4.2879539 -4.2857647][-4.1235366 -4.1726346 -4.1971841 -4.20858 -4.2144585 -4.2134371 -4.2137294 -4.215569 -4.2153473 -4.2237616 -4.2410283 -4.2656012 -4.2856274 -4.2913027 -4.2818713][-4.1279907 -4.17545 -4.1953888 -4.208838 -4.2265897 -4.24524 -4.2625842 -4.2700357 -4.2668567 -4.2647142 -4.2676773 -4.2763677 -4.2826056 -4.2808876 -4.2673125][-4.1417089 -4.1810374 -4.1921878 -4.2009315 -4.2214961 -4.2482142 -4.2707176 -4.2794847 -4.2739954 -4.266068 -4.2637935 -4.266037 -4.2671738 -4.2645588 -4.2504916][-4.1599631 -4.1878839 -4.1921492 -4.1964788 -4.2158175 -4.2410388 -4.2593975 -4.2644162 -4.2568035 -4.2440944 -4.2387328 -4.2399082 -4.2413063 -4.23973 -4.2268276]]...]
INFO - root - 2017-12-07 17:14:14.046584: step 33510, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 61h:17m:20s remains)
INFO - root - 2017-12-07 17:14:20.834200: step 33520, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 57h:36m:18s remains)
INFO - root - 2017-12-07 17:14:27.682567: step 33530, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 55h:03m:55s remains)
INFO - root - 2017-12-07 17:14:34.428295: step 33540, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 57h:04m:02s remains)
INFO - root - 2017-12-07 17:14:41.054793: step 33550, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 56h:02m:21s remains)
INFO - root - 2017-12-07 17:14:47.845387: step 33560, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 58h:33m:52s remains)
INFO - root - 2017-12-07 17:14:54.687795: step 33570, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 59h:21m:01s remains)
INFO - root - 2017-12-07 17:15:01.334933: step 33580, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 54h:26m:20s remains)
INFO - root - 2017-12-07 17:15:07.962496: step 33590, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 53h:03m:28s remains)
INFO - root - 2017-12-07 17:15:14.475625: step 33600, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 55h:41m:39s remains)
2017-12-07 17:15:15.248244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1883426 -4.1896029 -4.1973834 -4.2016048 -4.198257 -4.1831827 -4.1732836 -4.1708007 -4.1778426 -4.1780891 -4.1687627 -4.1485291 -4.128685 -4.1105361 -4.1008587][-4.1907167 -4.1917019 -4.1944904 -4.193573 -4.18794 -4.1740055 -4.1649156 -4.1707716 -4.1920657 -4.202003 -4.1925607 -4.1655521 -4.1322379 -4.1061559 -4.0968218][-4.1879897 -4.1868629 -4.182 -4.1704168 -4.1587873 -4.1467185 -4.142561 -4.1638317 -4.1995196 -4.2198715 -4.2104526 -4.1786585 -4.1406231 -4.1151576 -4.106708][-4.1927361 -4.1931791 -4.1850939 -4.1601734 -4.1341305 -4.1157527 -4.1114545 -4.1472607 -4.2011895 -4.2331176 -4.2271137 -4.1931744 -4.153512 -4.1311398 -4.1231737][-4.202168 -4.2087283 -4.2016978 -4.1637 -4.1140985 -4.0754428 -4.0604706 -4.1057339 -4.1822209 -4.2358594 -4.2420125 -4.2134275 -4.1726928 -4.1488361 -4.1394753][-4.2033048 -4.2176251 -4.2141552 -4.1641331 -4.084094 -4.0032492 -3.96195 -4.0151157 -4.1204004 -4.2047033 -4.23941 -4.2319593 -4.1989188 -4.1721539 -4.1589675][-4.2017908 -4.2210932 -4.2186389 -4.1602845 -4.047843 -3.913718 -3.8358369 -3.8949895 -4.0323987 -4.1540956 -4.2256618 -4.2466679 -4.2291145 -4.2052445 -4.1904974][-4.2156982 -4.233264 -4.2284188 -4.1736917 -4.048214 -3.8750515 -3.752151 -3.7966197 -3.9536002 -4.1095114 -4.2134271 -4.2543497 -4.255054 -4.2377591 -4.2221107][-4.2465539 -4.2598367 -4.2543721 -4.208663 -4.0948329 -3.9186683 -3.7708058 -3.7772651 -3.9212446 -4.0860991 -4.2015276 -4.2511086 -4.2643547 -4.2556949 -4.2430716][-4.2813325 -4.2877908 -4.2806177 -4.2427726 -4.1504068 -4.0009766 -3.8666055 -3.8525608 -3.9593871 -4.0978608 -4.1960506 -4.2433276 -4.2651191 -4.2651086 -4.2581716][-4.302423 -4.30228 -4.2907543 -4.2553344 -4.1795359 -4.0607533 -3.9514642 -3.9328709 -4.0100694 -4.1172657 -4.1941285 -4.2383428 -4.26439 -4.2730551 -4.2730942][-4.3167357 -4.3132424 -4.2972584 -4.2620535 -4.1974363 -4.1010633 -4.010324 -3.9884582 -4.0467958 -4.1332207 -4.1988983 -4.2420363 -4.2708645 -4.2863 -4.2927742][-4.3321648 -4.3266525 -4.3079758 -4.2737041 -4.2178597 -4.1366339 -4.0577493 -4.0313063 -4.0716648 -4.1410036 -4.2016268 -4.246933 -4.2799115 -4.3011665 -4.3124437][-4.34618 -4.3391376 -4.3182073 -4.2837653 -4.2346807 -4.1665735 -4.1015062 -4.0759468 -4.1032004 -4.1571183 -4.2116508 -4.2560086 -4.2906008 -4.313786 -4.3269277][-4.3521366 -4.3451085 -4.3234587 -4.2911124 -4.2496214 -4.1958 -4.1467681 -4.129365 -4.1512671 -4.1932473 -4.2361627 -4.27264 -4.3029904 -4.3234982 -4.3346682]]...]
INFO - root - 2017-12-07 17:15:21.904430: step 33610, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 54h:48m:13s remains)
INFO - root - 2017-12-07 17:15:28.653095: step 33620, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 52h:12m:38s remains)
INFO - root - 2017-12-07 17:15:35.439755: step 33630, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 58h:15m:41s remains)
INFO - root - 2017-12-07 17:15:42.135989: step 33640, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.669 sec/batch; 55h:30m:02s remains)
INFO - root - 2017-12-07 17:15:48.808786: step 33650, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 55h:29m:35s remains)
INFO - root - 2017-12-07 17:15:55.552433: step 33660, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 54h:20m:31s remains)
INFO - root - 2017-12-07 17:16:02.258762: step 33670, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 51h:53m:13s remains)
INFO - root - 2017-12-07 17:16:09.109248: step 33680, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 59h:05m:33s remains)
INFO - root - 2017-12-07 17:16:15.922151: step 33690, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 57h:34m:25s remains)
INFO - root - 2017-12-07 17:16:22.455252: step 33700, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 53h:00m:13s remains)
2017-12-07 17:16:23.142200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3123817 -4.3185363 -4.3277559 -4.3398418 -4.3420043 -4.3323655 -4.328021 -4.3265405 -4.3179259 -4.3085856 -4.3105159 -4.3215976 -4.3345423 -4.3469148 -4.35244][-4.2790966 -4.2820487 -4.2959895 -4.3126235 -4.3115792 -4.2939739 -4.28903 -4.2909803 -4.2820091 -4.2722554 -4.2763982 -4.2915931 -4.3085189 -4.3270063 -4.3390408][-4.2294021 -4.2324057 -4.2545052 -4.2703714 -4.2627649 -4.2399349 -4.2388535 -4.2480807 -4.241035 -4.2345662 -4.2439413 -4.2624846 -4.283174 -4.3056631 -4.3230157][-4.1708527 -4.1812029 -4.2147088 -4.2270684 -4.2105136 -4.1828551 -4.183279 -4.1966777 -4.1967282 -4.1990161 -4.2181773 -4.2457585 -4.2710581 -4.2940764 -4.3103685][-4.1409726 -4.1523466 -4.183816 -4.1842961 -4.1547122 -4.1191344 -4.1151762 -4.1254177 -4.1362348 -4.1548929 -4.18715 -4.2291446 -4.2655215 -4.2917843 -4.302474][-4.1423221 -4.148654 -4.1678324 -4.1513767 -4.10227 -4.0445371 -4.0264087 -4.03756 -4.0592723 -4.09036 -4.136384 -4.1914477 -4.2438254 -4.2780843 -4.287281][-4.1592646 -4.1553354 -4.1560788 -4.1206331 -4.0521832 -3.9705589 -3.9356744 -3.947978 -3.9789827 -4.0212774 -4.0794477 -4.1445289 -4.208549 -4.2503181 -4.2631226][-4.1825871 -4.1638079 -4.1439886 -4.0982213 -4.0238862 -3.9345169 -3.8858485 -3.8756518 -3.8894367 -3.9382751 -4.01458 -4.0894432 -4.158216 -4.2085829 -4.2335219][-4.20386 -4.1754951 -4.1471887 -4.1063466 -4.0462513 -3.9758081 -3.9298515 -3.9000192 -3.8839178 -3.9195633 -3.998771 -4.071785 -4.1356463 -4.1873183 -4.2167244][-4.2238536 -4.198637 -4.174015 -4.1438026 -4.1026821 -4.0606279 -4.0347505 -4.0096574 -3.9866962 -3.999208 -4.0543265 -4.1068888 -4.1551085 -4.1989007 -4.2230721][-4.2436604 -4.2272196 -4.2132468 -4.1958232 -4.1713839 -4.1497936 -4.1392632 -4.1219735 -4.1004467 -4.0976834 -4.1288137 -4.1604085 -4.1926003 -4.2283325 -4.2482305][-4.2592683 -4.250319 -4.2465539 -4.2410398 -4.232461 -4.2236338 -4.2191644 -4.2034354 -4.1842079 -4.1775961 -4.1942348 -4.2127857 -4.2346678 -4.2645292 -4.2815351][-4.2761984 -4.2692871 -4.26924 -4.2697463 -4.2703362 -4.2704897 -4.2692428 -4.2564497 -4.2401562 -4.2320194 -4.2406645 -4.2548823 -4.2698231 -4.2921023 -4.3069587][-4.2967434 -4.292171 -4.2930484 -4.2949018 -4.2968869 -4.2974858 -4.2963209 -4.2900963 -4.2797666 -4.2727714 -4.277698 -4.2888227 -4.2997456 -4.3164182 -4.3279843][-4.317616 -4.3168387 -4.317924 -4.3186631 -4.3192196 -4.3192353 -4.3181992 -4.3162766 -4.3130918 -4.3125939 -4.3174996 -4.3244233 -4.3310466 -4.340404 -4.3451948]]...]
INFO - root - 2017-12-07 17:16:29.892442: step 33710, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.667 sec/batch; 55h:20m:21s remains)
INFO - root - 2017-12-07 17:16:36.754189: step 33720, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 58h:10m:46s remains)
INFO - root - 2017-12-07 17:16:43.415699: step 33730, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 51h:30m:50s remains)
INFO - root - 2017-12-07 17:16:50.250516: step 33740, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 52h:41m:52s remains)
INFO - root - 2017-12-07 17:16:57.053805: step 33750, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.749 sec/batch; 62h:08m:12s remains)
INFO - root - 2017-12-07 17:17:03.864930: step 33760, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 58h:40m:48s remains)
INFO - root - 2017-12-07 17:17:10.699041: step 33770, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 55h:29m:09s remains)
INFO - root - 2017-12-07 17:17:17.517096: step 33780, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.611 sec/batch; 50h:43m:12s remains)
INFO - root - 2017-12-07 17:17:24.283975: step 33790, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 55h:50m:01s remains)
INFO - root - 2017-12-07 17:17:30.972327: step 33800, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.772 sec/batch; 64h:03m:52s remains)
2017-12-07 17:17:31.623883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3094325 -4.332623 -4.3376217 -4.2902427 -4.2160516 -4.1220646 -4.0513296 -4.0722518 -4.1233563 -4.1721869 -4.2069063 -4.2303081 -4.2525635 -4.2704382 -4.2845206][-4.3322539 -4.3548536 -4.3596673 -4.3195939 -4.2546391 -4.1633544 -4.0836468 -4.080246 -4.1138549 -4.1620283 -4.2016525 -4.2245426 -4.2500629 -4.2726336 -4.2889171][-4.3529797 -4.37534 -4.3764811 -4.3401275 -4.274456 -4.1771379 -4.0886984 -4.0744896 -4.1076908 -4.1587353 -4.2005663 -4.223978 -4.2509265 -4.2737403 -4.2902889][-4.3653216 -4.3867564 -4.3833165 -4.3425126 -4.2645721 -4.1530371 -4.0498405 -4.0293965 -4.0782714 -4.1432285 -4.1927605 -4.2246156 -4.2543521 -4.2756996 -4.2919092][-4.3690019 -4.3828244 -4.3704057 -4.317143 -4.2193379 -4.0926161 -3.9715714 -3.9405138 -4.0107136 -4.1023097 -4.1700764 -4.21548 -4.2518711 -4.2755833 -4.2925806][-4.3658624 -4.3723865 -4.349339 -4.2841773 -4.1660185 -4.0212932 -3.8859639 -3.8512692 -3.939301 -4.0500722 -4.1367731 -4.1970973 -4.2407084 -4.2672873 -4.2856631][-4.3600488 -4.3623362 -4.3333974 -4.2589903 -4.126853 -3.9757035 -3.8493159 -3.8304935 -3.9177086 -4.0188551 -4.10781 -4.1764536 -4.2272329 -4.2590933 -4.2800608][-4.3557076 -4.3557143 -4.3283052 -4.2517881 -4.1199622 -3.9731429 -3.8613124 -3.8543248 -3.9185603 -3.9931657 -4.0768251 -4.1540308 -4.214767 -4.2563553 -4.2832217][-4.3563814 -4.3564491 -4.3353357 -4.2657795 -4.1472969 -4.0098848 -3.8934045 -3.8764839 -3.9174013 -3.9720778 -4.0563679 -4.1432691 -4.2118731 -4.2607508 -4.2891922][-4.3620553 -4.3647451 -4.3512354 -4.2907739 -4.1884694 -4.058465 -3.930932 -3.8938193 -3.9282997 -3.9884336 -4.0776882 -4.1641445 -4.22779 -4.2711005 -4.2959714][-4.3703461 -4.3753481 -4.3651614 -4.3118372 -4.2257209 -4.1102452 -3.984813 -3.9362688 -3.9691353 -4.0422039 -4.1318 -4.2060971 -4.2545748 -4.2833552 -4.2987189][-4.3793559 -4.3854342 -4.3770084 -4.3348436 -4.2680807 -4.1737494 -4.062798 -4.0057783 -4.0273781 -4.1018677 -4.1826429 -4.2428784 -4.2751193 -4.2867641 -4.2905331][-4.3856282 -4.3906288 -4.3812604 -4.3496561 -4.3011436 -4.2261 -4.133996 -4.0774879 -4.0898428 -4.1555943 -4.2229652 -4.2694507 -4.2910295 -4.2925868 -4.288054][-4.3841624 -4.3851671 -4.3731613 -4.349462 -4.311327 -4.2475753 -4.1721244 -4.1294775 -4.1468019 -4.2029781 -4.2574406 -4.2907014 -4.3068786 -4.3058529 -4.2978029][-4.3700619 -4.3618355 -4.3466372 -4.3233418 -4.2877631 -4.234148 -4.1798496 -4.1624742 -4.1898041 -4.2332869 -4.27512 -4.3029723 -4.3147454 -4.3123093 -4.3037715]]...]
INFO - root - 2017-12-07 17:17:38.438758: step 33810, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 52h:41m:21s remains)
INFO - root - 2017-12-07 17:17:45.246523: step 33820, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 55h:17m:16s remains)
INFO - root - 2017-12-07 17:17:52.043839: step 33830, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 59h:57m:47s remains)
INFO - root - 2017-12-07 17:17:58.877977: step 33840, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 59h:07m:12s remains)
INFO - root - 2017-12-07 17:18:05.655763: step 33850, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 55h:02m:11s remains)
INFO - root - 2017-12-07 17:18:12.352225: step 33860, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 53h:35m:42s remains)
INFO - root - 2017-12-07 17:18:19.142081: step 33870, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 53h:30m:27s remains)
INFO - root - 2017-12-07 17:18:25.940295: step 33880, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 60h:16m:06s remains)
INFO - root - 2017-12-07 17:18:32.630002: step 33890, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 57h:49m:12s remains)
INFO - root - 2017-12-07 17:18:39.215948: step 33900, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.664 sec/batch; 55h:02m:19s remains)
2017-12-07 17:18:39.958280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.321238 -4.3208542 -4.3193274 -4.3167124 -4.3139439 -4.3102417 -4.3032842 -4.2952409 -4.2899323 -4.2890267 -4.2910061 -4.2935739 -4.295218 -4.2964916 -4.3008471][-4.3165092 -4.3171654 -4.3165483 -4.3122835 -4.3076129 -4.2995815 -4.2854776 -4.271378 -4.2649574 -4.2665706 -4.2717352 -4.2747746 -4.2772408 -4.2789073 -4.28399][-4.3165007 -4.317286 -4.3148451 -4.306726 -4.2960572 -4.2799096 -4.2574954 -4.2370791 -4.2317438 -4.2386847 -4.2468643 -4.2512808 -4.2554913 -4.2603426 -4.2666922][-4.3102832 -4.3049312 -4.2956882 -4.28271 -4.2626877 -4.2357292 -4.2052889 -4.1813045 -4.1836576 -4.2027192 -4.2164497 -4.2233267 -4.2301507 -4.2383595 -4.247139][-4.3024726 -4.287096 -4.2675519 -4.2473235 -4.2165155 -4.1764712 -4.1324162 -4.0991287 -4.1169014 -4.1647673 -4.1937537 -4.2062178 -4.2142282 -4.2211952 -4.2270303][-4.2885437 -4.2645569 -4.23903 -4.2113042 -4.1650677 -4.1038027 -4.0299206 -3.9686973 -4.0014286 -4.0919313 -4.1537094 -4.1833458 -4.1976285 -4.204864 -4.2074671][-4.26364 -4.2298679 -4.1977539 -4.1617351 -4.0957346 -4.0063229 -3.8861337 -3.7801943 -3.827981 -3.969662 -4.0750914 -4.1339965 -4.1629395 -4.1768069 -4.1831551][-4.2306719 -4.1842523 -4.1450195 -4.1003852 -4.0206852 -3.9111905 -3.7466767 -3.5890207 -3.6491268 -3.8399415 -3.9860563 -4.0720634 -4.1201506 -4.1498156 -4.1638861][-4.2008481 -4.1446533 -4.1030784 -4.0631666 -3.9971137 -3.907053 -3.7585526 -3.6021039 -3.6453333 -3.8221865 -3.9610138 -4.0446658 -4.0980158 -4.1401286 -4.1619515][-4.1767921 -4.1184254 -4.0827465 -4.0614977 -4.0306916 -3.9903004 -3.9105976 -3.8160238 -3.824261 -3.9265821 -4.0150166 -4.0726004 -4.1155806 -4.1555967 -4.1796803][-4.1570821 -4.0980639 -4.0704417 -4.0711083 -4.0772095 -4.0796118 -4.0530772 -4.0091872 -4.0011272 -4.0470386 -4.0904288 -4.1229162 -4.1522064 -4.183218 -4.2038989][-4.1470747 -4.0925403 -4.0718365 -4.0876355 -4.1138716 -4.1347871 -4.1359987 -4.1215625 -4.1123562 -4.1313982 -4.1530762 -4.173943 -4.1927338 -4.215693 -4.2346659][-4.1640162 -4.1230888 -4.1142216 -4.1375446 -4.1667805 -4.1883297 -4.2005439 -4.2016344 -4.1950374 -4.2007008 -4.2154369 -4.2336965 -4.2464728 -4.2603989 -4.2734585][-4.2105913 -4.1850529 -4.1866827 -4.2098002 -4.2313418 -4.2440777 -4.2530518 -4.2589393 -4.2555647 -4.2549829 -4.2648721 -4.2802296 -4.2892418 -4.2971783 -4.3071408][-4.2505851 -4.2353153 -4.2423239 -4.262538 -4.27697 -4.2833214 -4.2885218 -4.2947288 -4.2949619 -4.29411 -4.3006005 -4.3104391 -4.3159628 -4.3205791 -4.3273268]]...]
INFO - root - 2017-12-07 17:18:46.810041: step 33910, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 59h:03m:36s remains)
INFO - root - 2017-12-07 17:18:53.612518: step 33920, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 59h:32m:29s remains)
INFO - root - 2017-12-07 17:19:00.428924: step 33930, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:31m:57s remains)
INFO - root - 2017-12-07 17:19:07.264626: step 33940, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 52h:31m:24s remains)
INFO - root - 2017-12-07 17:19:14.037096: step 33950, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 54h:06m:37s remains)
INFO - root - 2017-12-07 17:19:20.807356: step 33960, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 60h:24m:00s remains)
INFO - root - 2017-12-07 17:19:27.575279: step 33970, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.733 sec/batch; 60h:48m:48s remains)
INFO - root - 2017-12-07 17:19:34.367702: step 33980, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 56h:02m:12s remains)
INFO - root - 2017-12-07 17:19:41.121428: step 33990, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 52h:27m:41s remains)
INFO - root - 2017-12-07 17:19:47.673762: step 34000, loss = 2.05, batch loss = 1.99 (13.1 examples/sec; 0.612 sec/batch; 50h:43m:52s remains)
2017-12-07 17:19:48.478051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2428279 -4.2349949 -4.2349381 -4.240437 -4.2498708 -4.256319 -4.2527003 -4.2450843 -4.2350707 -4.2298331 -4.228754 -4.2221775 -4.2095823 -4.1904874 -4.1773257][-4.2246537 -4.2121472 -4.2088485 -4.2115006 -4.2201695 -4.227212 -4.2219434 -4.2119107 -4.2015729 -4.2000914 -4.2036552 -4.1964779 -4.1800127 -4.1562414 -4.1423035][-4.2108855 -4.1926646 -4.1836367 -4.1802 -4.1848726 -4.191277 -4.1845841 -4.1734543 -4.1683927 -4.1770244 -4.1901517 -4.185802 -4.1720076 -4.1508684 -4.1395707][-4.1996365 -4.1742282 -4.1579466 -4.1470027 -4.1446104 -4.1452389 -4.133605 -4.1229911 -4.1297832 -4.1524944 -4.1760235 -4.1769714 -4.170073 -4.1562638 -4.1472874][-4.1925006 -4.159246 -4.1336794 -4.1138139 -4.10202 -4.0914617 -4.0685749 -4.0578203 -4.08286 -4.1221852 -4.1570511 -4.1660228 -4.16598 -4.1589494 -4.151484][-4.1892061 -4.1475592 -4.1099896 -4.0765181 -4.0480132 -4.0157242 -3.9743483 -3.9666479 -4.022119 -4.0878515 -4.1360126 -4.1537766 -4.1604886 -4.1588659 -4.1556244][-4.1877255 -4.1388836 -4.0905261 -4.0410438 -3.9881907 -3.9239097 -3.85678 -3.8554511 -3.9490311 -4.0430918 -4.1035357 -4.1305466 -4.1452589 -4.1467819 -4.1492381][-4.1813345 -4.1285534 -4.0763955 -4.0193329 -3.9487448 -3.8615274 -3.777648 -3.7829359 -3.8992844 -4.0074987 -4.0720954 -4.1019254 -4.1176934 -4.1172962 -4.1236954][-4.1755309 -4.126349 -4.0803547 -4.027844 -3.9587426 -3.8776972 -3.8101521 -3.8271246 -3.9298561 -4.0176382 -4.0680404 -4.0924716 -4.1016688 -4.0980306 -4.1015491][-4.1847129 -4.1459961 -4.1111517 -4.0696573 -4.0142913 -3.9597828 -3.9236417 -3.9506924 -4.0261607 -4.0819635 -4.1083026 -4.119792 -4.1197152 -4.1130786 -4.1116877][-4.2112255 -4.1863551 -4.1626005 -4.1306162 -4.0898 -4.0606852 -4.0489931 -4.0775576 -4.1271982 -4.1589603 -4.1700096 -4.17412 -4.1677723 -4.1558905 -4.1519165][-4.2417011 -4.2261729 -4.2100773 -4.1871386 -4.1599865 -4.1511936 -4.1566796 -4.1811857 -4.2121315 -4.2296767 -4.2335992 -4.2353153 -4.2272916 -4.2133241 -4.208714][-4.2701526 -4.2613773 -4.2507558 -4.2374377 -4.2249322 -4.2305446 -4.2421894 -4.2598071 -4.2768979 -4.2842264 -4.2838817 -4.2840385 -4.2750831 -4.2608814 -4.2540479][-4.2935848 -4.2888846 -4.2815609 -4.2740073 -4.2702932 -4.2801442 -4.29215 -4.30443 -4.3131528 -4.3152361 -4.3137112 -4.3145204 -4.3081913 -4.2969108 -4.2871675][-4.3061948 -4.304244 -4.2981305 -4.2914505 -4.290153 -4.2985768 -4.3076859 -4.3157997 -4.321867 -4.3238788 -4.3241234 -4.3254795 -4.3224459 -4.314888 -4.3075132]]...]
INFO - root - 2017-12-07 17:19:55.182062: step 34010, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.668 sec/batch; 55h:21m:35s remains)
INFO - root - 2017-12-07 17:20:02.038638: step 34020, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 53h:50m:33s remains)
INFO - root - 2017-12-07 17:20:08.867919: step 34030, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 53h:54m:58s remains)
INFO - root - 2017-12-07 17:20:15.517918: step 34040, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 59h:27m:15s remains)
INFO - root - 2017-12-07 17:20:22.441750: step 34050, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 61h:16m:10s remains)
INFO - root - 2017-12-07 17:20:29.318963: step 34060, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 57h:15m:14s remains)
INFO - root - 2017-12-07 17:20:36.101026: step 34070, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 53h:35m:58s remains)
INFO - root - 2017-12-07 17:20:42.945752: step 34080, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 59h:33m:15s remains)
INFO - root - 2017-12-07 17:20:49.841677: step 34090, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.760 sec/batch; 63h:02m:17s remains)
INFO - root - 2017-12-07 17:20:56.429259: step 34100, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 56h:14m:30s remains)
2017-12-07 17:20:57.229677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3259587 -4.3265181 -4.3242517 -4.316566 -4.3044171 -4.2913256 -4.2842817 -4.283 -4.2876148 -4.2957335 -4.3023291 -4.3063192 -4.3082223 -4.3083615 -4.3068213][-4.3375907 -4.3372512 -4.3349662 -4.3292761 -4.3213482 -4.3140211 -4.3115325 -4.3130369 -4.3183217 -4.3251634 -4.3288646 -4.3291931 -4.3275561 -4.32441 -4.3188109][-4.3428946 -4.3424416 -4.3398452 -4.3345637 -4.3287778 -4.3248391 -4.3243437 -4.3266778 -4.3332348 -4.3416109 -4.3463583 -4.3456578 -4.3409028 -4.3326344 -4.3190637][-4.3390779 -4.3368549 -4.3309155 -4.3215375 -4.3133874 -4.3084769 -4.3050613 -4.304894 -4.3136077 -4.3274474 -4.3378811 -4.3408694 -4.3369384 -4.32478 -4.3005586][-4.3231492 -4.3173156 -4.3048534 -4.2859144 -4.2696834 -4.2583 -4.2491837 -4.2471676 -4.2600079 -4.2829542 -4.30317 -4.3135 -4.3112731 -4.2927485 -4.2520595][-4.2976203 -4.2850308 -4.25931 -4.2207985 -4.1840053 -4.1557527 -4.1383944 -4.1400967 -4.1672463 -4.2099528 -4.2475247 -4.2714272 -4.2748666 -4.2506347 -4.1938124][-4.2672095 -4.2417583 -4.1928434 -4.1246734 -4.0587554 -4.005928 -3.97758 -3.9914255 -4.0453544 -4.1160192 -4.1753564 -4.2164125 -4.2316108 -4.2113481 -4.1523352][-4.2452688 -4.2090554 -4.1435428 -4.0535588 -3.9615223 -3.8800163 -3.8367813 -3.8641977 -3.9468884 -4.0437202 -4.1194868 -4.173316 -4.2009449 -4.1908894 -4.1460466][-4.2363205 -4.1998606 -4.1368709 -4.0507259 -3.9610326 -3.8789864 -3.8385782 -3.8729539 -3.9588678 -4.0522604 -4.1214037 -4.1716809 -4.1997166 -4.196269 -4.1666813][-4.2390342 -4.2134504 -4.1695657 -4.1110525 -4.052834 -4.0024433 -3.9857807 -4.0195112 -4.0830131 -4.1459055 -4.1897178 -4.2179022 -4.2318931 -4.2249289 -4.1998687][-4.2573314 -4.2474189 -4.2262087 -4.1964431 -4.1681771 -4.1446943 -4.1416068 -4.1668229 -4.2055726 -4.2402267 -4.260818 -4.2695584 -4.2689619 -4.2543626 -4.2276907][-4.2815089 -4.2816114 -4.2765808 -4.2668228 -4.2567954 -4.2474437 -4.2472062 -4.2611585 -4.281147 -4.2988787 -4.3087015 -4.3110037 -4.3068628 -4.2924194 -4.2700057][-4.3031449 -4.3082628 -4.3129354 -4.3145003 -4.3138676 -4.311161 -4.310452 -4.3155994 -4.3237624 -4.3317752 -4.3363504 -4.3367157 -4.3328333 -4.3220038 -4.3069558][-4.3119054 -4.317121 -4.325222 -4.3315382 -4.3353581 -4.3362064 -4.3357205 -4.3365331 -4.3386416 -4.340868 -4.3414335 -4.3405848 -4.3380766 -4.3321834 -4.3257337][-4.3066726 -4.3095393 -4.3176446 -4.3257723 -4.3316865 -4.3347516 -4.3350096 -4.3338103 -4.3331628 -4.3328266 -4.3322449 -4.3317471 -4.3312154 -4.3302474 -4.3311906]]...]
INFO - root - 2017-12-07 17:21:04.010288: step 34110, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 59h:14m:32s remains)
INFO - root - 2017-12-07 17:21:10.742228: step 34120, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 55h:08m:45s remains)
INFO - root - 2017-12-07 17:21:17.695116: step 34130, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 56h:50m:47s remains)
INFO - root - 2017-12-07 17:21:24.428703: step 34140, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 52h:05m:46s remains)
INFO - root - 2017-12-07 17:21:31.322031: step 34150, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 57h:58m:21s remains)
INFO - root - 2017-12-07 17:21:38.201889: step 34160, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 57h:17m:56s remains)
INFO - root - 2017-12-07 17:21:45.034608: step 34170, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 57h:00m:01s remains)
INFO - root - 2017-12-07 17:21:51.757614: step 34180, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 51h:37m:38s remains)
INFO - root - 2017-12-07 17:21:58.618370: step 34190, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 57h:59m:13s remains)
INFO - root - 2017-12-07 17:22:05.221482: step 34200, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 58h:16m:35s remains)
2017-12-07 17:22:06.027580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2429266 -4.2465234 -4.2554278 -4.2640934 -4.2639556 -4.2460518 -4.2232742 -4.2156496 -4.2268214 -4.238883 -4.2387776 -4.2366505 -4.2410736 -4.2440643 -4.237545][-4.2356334 -4.2344513 -4.2407908 -4.2486825 -4.2557368 -4.2479839 -4.231492 -4.2244887 -4.2388024 -4.2569427 -4.2630725 -4.2623043 -4.2620049 -4.259943 -4.2496967][-4.2370338 -4.2340631 -4.2368865 -4.2415385 -4.24833 -4.2447491 -4.2295017 -4.223434 -4.23712 -4.2522769 -4.2636156 -4.2685494 -4.2691307 -4.2650933 -4.2560568][-4.239758 -4.2404509 -4.2428403 -4.2378492 -4.2353697 -4.2270727 -4.2110057 -4.2044086 -4.2141037 -4.2269287 -4.2439156 -4.2612095 -4.26803 -4.2619 -4.2537022][-4.2198014 -4.2253504 -4.226903 -4.2160029 -4.1994457 -4.1773524 -4.1527958 -4.1477017 -4.1648226 -4.1860027 -4.2140889 -4.2474818 -4.2624784 -4.2554626 -4.244482][-4.1727977 -4.1780705 -4.1771593 -4.1621518 -4.1320271 -4.0945659 -4.060535 -4.0623932 -4.0982256 -4.1369452 -4.179513 -4.2271447 -4.2516193 -4.2446413 -4.2298751][-4.1316442 -4.1333241 -4.1274533 -4.1062479 -4.0633469 -4.0096536 -3.9671571 -3.9779506 -4.0354066 -4.0936503 -4.1491404 -4.2048073 -4.2354865 -4.2305784 -4.2132187][-4.1270127 -4.1240277 -4.1172042 -4.0950284 -4.045743 -3.9796782 -3.9282551 -3.9366722 -4.003087 -4.0716782 -4.1269169 -4.1731105 -4.2014408 -4.2021785 -4.1919975][-4.1569724 -4.1510572 -4.1486931 -4.1337805 -4.0924511 -4.0338559 -3.9854891 -3.9816186 -4.0313153 -4.0851078 -4.122715 -4.1508012 -4.1720552 -4.1785722 -4.1783695][-4.1866484 -4.1833506 -4.1897888 -4.1890588 -4.1668434 -4.1268559 -4.093183 -4.0807829 -4.1041479 -4.1303763 -4.1462574 -4.1589394 -4.1717019 -4.179306 -4.1882577][-4.1979933 -4.1944675 -4.206665 -4.2192497 -4.2180429 -4.1997743 -4.1824427 -4.1685286 -4.1751947 -4.1841612 -4.1858406 -4.1860905 -4.1891418 -4.1958127 -4.2140121][-4.1994748 -4.191607 -4.2001076 -4.2177534 -4.2330489 -4.2336731 -4.2295432 -4.2198968 -4.2191663 -4.2199473 -4.2179437 -4.21235 -4.2031517 -4.2042294 -4.2233152][-4.2104836 -4.1974578 -4.197329 -4.208302 -4.2255578 -4.2367029 -4.2407932 -4.2374158 -4.2365937 -4.234077 -4.2270184 -4.2144742 -4.1979284 -4.1940002 -4.210608][-4.2339239 -4.2181048 -4.2106538 -4.2136726 -4.2253103 -4.2376385 -4.2488923 -4.2541666 -4.2560468 -4.2489471 -4.23022 -4.206162 -4.1821585 -4.17395 -4.1897035][-4.2584281 -4.238605 -4.2276869 -4.2261376 -4.23566 -4.2454767 -4.2607765 -4.2727928 -4.2770071 -4.2654772 -4.2375321 -4.2060547 -4.1754494 -4.1624107 -4.17495]]...]
INFO - root - 2017-12-07 17:22:12.680766: step 34210, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 51h:52m:35s remains)
INFO - root - 2017-12-07 17:22:19.474063: step 34220, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 55h:18m:07s remains)
INFO - root - 2017-12-07 17:22:26.343976: step 34230, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.724 sec/batch; 59h:58m:00s remains)
INFO - root - 2017-12-07 17:22:33.050728: step 34240, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 54h:14m:15s remains)
INFO - root - 2017-12-07 17:22:39.875533: step 34250, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 53h:12m:24s remains)
INFO - root - 2017-12-07 17:22:46.693001: step 34260, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 52h:17m:06s remains)
INFO - root - 2017-12-07 17:22:53.536327: step 34270, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 56h:31m:14s remains)
INFO - root - 2017-12-07 17:23:00.417532: step 34280, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.724 sec/batch; 59h:57m:53s remains)
INFO - root - 2017-12-07 17:23:07.174629: step 34290, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 56h:27m:25s remains)
INFO - root - 2017-12-07 17:23:13.811715: step 34300, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 51h:40m:28s remains)
2017-12-07 17:23:14.551553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2802219 -4.2670712 -4.2420759 -4.2199755 -4.2100725 -4.2143598 -4.2360697 -4.2630816 -4.2842326 -4.29106 -4.2837286 -4.2743149 -4.27544 -4.2811637 -4.2867155][-4.2499685 -4.2227983 -4.1858091 -4.1555643 -4.1425977 -4.1479731 -4.1760492 -4.2156258 -4.2536731 -4.275743 -4.2786703 -4.2742543 -4.2767372 -4.2828422 -4.2893109][-4.2013249 -4.1668806 -4.1303144 -4.1052723 -4.1003041 -4.110539 -4.136888 -4.1752591 -4.2161388 -4.24669 -4.2588835 -4.2621031 -4.2685666 -4.2777619 -4.28786][-4.1552658 -4.1283298 -4.1110282 -4.1082678 -4.1149626 -4.1256137 -4.1376629 -4.1596594 -4.1858845 -4.2117796 -4.2289319 -4.2414351 -4.2552624 -4.2704711 -4.2842622][-4.1295815 -4.1159873 -4.11793 -4.129797 -4.1369596 -4.1394267 -4.1354713 -4.1398478 -4.1508188 -4.166574 -4.1856546 -4.2095137 -4.2351723 -4.2574315 -4.2733507][-4.1289768 -4.1220942 -4.1320457 -4.1486897 -4.1492968 -4.1376824 -4.1227412 -4.1185265 -4.1197133 -4.1254973 -4.14181 -4.1717706 -4.2066674 -4.2328553 -4.2476587][-4.1493988 -4.1369138 -4.141284 -4.1593451 -4.1577368 -4.1364594 -4.1147017 -4.1056075 -4.1002665 -4.0992293 -4.11315 -4.1419458 -4.1755123 -4.1967835 -4.2041268][-4.1812053 -4.1556807 -4.1502223 -4.1672168 -4.1681867 -4.1442728 -4.120491 -4.1094532 -4.0978842 -4.0917063 -4.0992179 -4.1168814 -4.1344132 -4.1379504 -4.1302457][-4.2108579 -4.1763215 -4.1678057 -4.1832213 -4.1888227 -4.1689835 -4.1506286 -4.138835 -4.1207418 -4.1062164 -4.1020465 -4.1000991 -4.0924883 -4.0743017 -4.0563312][-4.2283688 -4.1890779 -4.1786103 -4.1943636 -4.2048364 -4.1959777 -4.1831455 -4.16721 -4.1401739 -4.1164241 -4.1003551 -4.0796356 -4.0532103 -4.026228 -4.0142713][-4.238378 -4.1962781 -4.1831579 -4.1975222 -4.212481 -4.2155547 -4.2085667 -4.1889439 -4.1565833 -4.1286573 -4.1121154 -4.0872049 -4.0603647 -4.0414171 -4.0403285][-4.2490482 -4.2116289 -4.1998096 -4.2148128 -4.2313719 -4.2395558 -4.234726 -4.2147517 -4.1849623 -4.1590238 -4.144412 -4.1256795 -4.1092014 -4.103178 -4.1075907][-4.26096 -4.2338119 -4.2257376 -4.2380385 -4.2503338 -4.2554531 -4.2483573 -4.2327929 -4.2142644 -4.19752 -4.18454 -4.169539 -4.1576223 -4.1594138 -4.1685181][-4.2694359 -4.2505016 -4.24393 -4.2488589 -4.25165 -4.2489028 -4.2388124 -4.2307081 -4.2284317 -4.2252755 -4.2182994 -4.2088504 -4.2022076 -4.2096071 -4.2247524][-4.2670341 -4.2523918 -4.2442942 -4.2407947 -4.2339511 -4.2233706 -4.2131214 -4.2143288 -4.2287912 -4.2441759 -4.2533355 -4.2547207 -4.2546949 -4.264122 -4.2787375]]...]
INFO - root - 2017-12-07 17:23:21.308272: step 34310, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.729 sec/batch; 60h:23m:59s remains)
INFO - root - 2017-12-07 17:23:28.029144: step 34320, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 57h:13m:52s remains)
INFO - root - 2017-12-07 17:23:34.793922: step 34330, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 55h:07m:37s remains)
INFO - root - 2017-12-07 17:23:41.614925: step 34340, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 55h:33m:27s remains)
INFO - root - 2017-12-07 17:23:48.206148: step 34350, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 57h:55m:35s remains)
INFO - root - 2017-12-07 17:23:54.961412: step 34360, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 58h:31m:58s remains)
INFO - root - 2017-12-07 17:24:01.668481: step 34370, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 55h:19m:50s remains)
INFO - root - 2017-12-07 17:24:08.415913: step 34380, loss = 2.10, batch loss = 2.05 (12.8 examples/sec; 0.623 sec/batch; 51h:35m:17s remains)
INFO - root - 2017-12-07 17:24:15.232137: step 34390, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 55h:34m:21s remains)
INFO - root - 2017-12-07 17:24:21.882351: step 34400, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 59h:39m:45s remains)
2017-12-07 17:24:22.603894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2154055 -4.1968985 -4.1798258 -4.1746383 -4.1852636 -4.1907525 -4.1949825 -4.199451 -4.2041254 -4.207829 -4.2103877 -4.2116079 -4.2111316 -4.2117128 -4.2043757][-4.2376585 -4.2146258 -4.197536 -4.1954575 -4.2092962 -4.21582 -4.2200232 -4.2242188 -4.2279196 -4.2285523 -4.2275691 -4.2271943 -4.2276187 -4.2296953 -4.2257032][-4.2721095 -4.2501016 -4.2346792 -4.2339249 -4.2483621 -4.2534814 -4.254252 -4.2559843 -4.2567286 -4.25224 -4.2455659 -4.2433524 -4.2452884 -4.2513609 -4.2549086][-4.2985311 -4.2792006 -4.262733 -4.2585039 -4.2697759 -4.2713022 -4.2680445 -4.266284 -4.2633028 -4.2525206 -4.2389383 -4.2347579 -4.2384815 -4.2471633 -4.2583838][-4.3152871 -4.2958655 -4.275919 -4.2652488 -4.2709813 -4.2668371 -4.2538962 -4.2448049 -4.2377038 -4.2244468 -4.2100592 -4.2049265 -4.2098494 -4.2227154 -4.2401729][-4.3169188 -4.289773 -4.25784 -4.2334633 -4.2271132 -4.2102118 -4.1828256 -4.167686 -4.1618609 -4.1521068 -4.1429105 -4.1411409 -4.1484857 -4.1679277 -4.193759][-4.3004675 -4.2551007 -4.1989141 -4.1494412 -4.1220527 -4.0831628 -4.0386615 -4.0247493 -4.0345707 -4.0397263 -4.0458074 -4.0542741 -4.0702763 -4.0983543 -4.1339874][-4.2673411 -4.1955414 -4.1044493 -4.0215158 -3.9670308 -3.9045913 -3.8478422 -3.8457975 -3.8850083 -3.9179702 -3.9547713 -3.9914258 -4.0276933 -4.0660892 -4.110436][-4.2310224 -4.137579 -4.022469 -3.9195161 -3.8526134 -3.7871017 -3.7448854 -3.7728593 -3.8427365 -3.9024184 -3.9629803 -4.0235386 -4.0700679 -4.1084952 -4.1501188][-4.2313509 -4.1448889 -4.0412889 -3.9536216 -3.9059074 -3.8673806 -3.856406 -3.9003813 -3.97015 -4.0267243 -4.08003 -4.1330447 -4.1708336 -4.1992803 -4.2249331][-4.26844 -4.2088413 -4.1398487 -4.085187 -4.0647411 -4.0513692 -4.0573683 -4.0966845 -4.1492167 -4.1870356 -4.2179742 -4.2496858 -4.2706842 -4.2823324 -4.2878914][-4.3060012 -4.2702036 -4.2313571 -4.2025542 -4.2017107 -4.2038908 -4.215158 -4.2427216 -4.2751117 -4.2952075 -4.3084245 -4.3220029 -4.3292904 -4.3296418 -4.3240213][-4.3254824 -4.3074832 -4.2889643 -4.2776489 -4.2878103 -4.2949171 -4.3053546 -4.320868 -4.3363571 -4.3455653 -4.35116 -4.3570671 -4.3559608 -4.35004 -4.3382931][-4.3195744 -4.3142138 -4.3057656 -4.3030076 -4.3182592 -4.3267703 -4.3343091 -4.34197 -4.3485956 -4.3524489 -4.3545923 -4.3567014 -4.3512154 -4.3411951 -4.323998][-4.2915072 -4.2869744 -4.2790141 -4.2798839 -4.2961311 -4.3035016 -4.3081031 -4.3129406 -4.31712 -4.3199549 -4.3212795 -4.3217087 -4.3156252 -4.3044748 -4.2842822]]...]
INFO - root - 2017-12-07 17:24:29.366533: step 34410, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 52h:33m:10s remains)
INFO - root - 2017-12-07 17:24:36.116967: step 34420, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 57h:59m:23s remains)
INFO - root - 2017-12-07 17:24:42.923104: step 34430, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 59h:47m:26s remains)
INFO - root - 2017-12-07 17:24:49.768148: step 34440, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 57h:40m:57s remains)
INFO - root - 2017-12-07 17:24:56.589771: step 34450, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 54h:23m:48s remains)
INFO - root - 2017-12-07 17:25:03.328472: step 34460, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 52h:56m:22s remains)
INFO - root - 2017-12-07 17:25:10.200314: step 34470, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 54h:45m:33s remains)
INFO - root - 2017-12-07 17:25:17.081478: step 34480, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 58h:44m:20s remains)
INFO - root - 2017-12-07 17:25:23.895832: step 34490, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 56h:58m:35s remains)
INFO - root - 2017-12-07 17:25:30.397719: step 34500, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 53h:18m:18s remains)
2017-12-07 17:25:31.121956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.208436 -4.1924944 -4.1837873 -4.1802282 -4.1841836 -4.1925712 -4.2003675 -4.2017026 -4.18319 -4.1505146 -4.1221428 -4.11327 -4.136797 -4.176424 -4.2166042][-4.2293124 -4.2152295 -4.2096672 -4.2090459 -4.2151074 -4.2239742 -4.2302613 -4.230268 -4.2115111 -4.1790924 -4.1513023 -4.143611 -4.1676517 -4.2071695 -4.2462606][-4.2523422 -4.2439251 -4.24256 -4.2443094 -4.250176 -4.2570615 -4.2612324 -4.2616053 -4.2487 -4.2251453 -4.2044339 -4.197803 -4.213521 -4.2403955 -4.2690053][-4.2687984 -4.2664008 -4.2685075 -4.2713485 -4.2752256 -4.2784362 -4.2805028 -4.2838068 -4.2800875 -4.2696266 -4.2587452 -4.2522941 -4.2565956 -4.2674069 -4.2824616][-4.2761927 -4.2771788 -4.2801447 -4.2803793 -4.275919 -4.26825 -4.2642136 -4.2718849 -4.2820454 -4.2904592 -4.2949195 -4.2932172 -4.2909665 -4.2895255 -4.2902994][-4.2714643 -4.2721171 -4.269619 -4.2571216 -4.2331352 -4.2064953 -4.1929207 -4.2083411 -4.240407 -4.2762895 -4.3024421 -4.3096318 -4.3045893 -4.29499 -4.2866392][-4.2509003 -4.2439775 -4.2284331 -4.1950722 -4.1461973 -4.0975723 -4.0760031 -4.1038203 -4.1622577 -4.2284689 -4.2773571 -4.2962894 -4.2927542 -4.2814178 -4.271512][-4.221036 -4.2029395 -4.170311 -4.1146827 -4.0436292 -3.9795618 -3.9576161 -4.0031824 -4.0893159 -4.1807346 -4.2457328 -4.2726345 -4.2728815 -4.264502 -4.2578506][-4.188478 -4.1615977 -4.1181097 -4.0531063 -3.9793348 -3.9196527 -3.9090822 -3.9700012 -4.06955 -4.1679745 -4.2351017 -4.2619119 -4.2631745 -4.2566185 -4.252409][-4.1694522 -4.1457949 -4.1100888 -4.0600114 -4.0091019 -3.9739127 -3.9768836 -4.0316319 -4.1154966 -4.19599 -4.2485843 -4.2671952 -4.265871 -4.2591505 -4.2560024][-4.1788063 -4.170085 -4.1517816 -4.1219306 -4.0939264 -4.0793085 -4.0881538 -4.1278577 -4.186295 -4.2406383 -4.2726183 -4.279768 -4.2745013 -4.2673178 -4.2649112][-4.2015662 -4.2065778 -4.2031465 -4.188879 -4.1768708 -4.174861 -4.18638 -4.2132096 -4.2480226 -4.27738 -4.2895513 -4.2849641 -4.2766466 -4.2707324 -4.2699409][-4.2252045 -4.2355127 -4.2411079 -4.2366509 -4.2346535 -4.2408838 -4.2533026 -4.2692838 -4.2847147 -4.2923636 -4.2870588 -4.2739763 -4.2652521 -4.2625122 -4.2649879][-4.2439604 -4.2507281 -4.2575021 -4.25746 -4.2617164 -4.2721972 -4.2841682 -4.2929196 -4.2951655 -4.2889919 -4.2747283 -4.2589974 -4.2518692 -4.2523384 -4.2580943][-4.2583523 -4.2584448 -4.2619028 -4.263423 -4.2700648 -4.2799191 -4.2885408 -4.2924628 -4.2894807 -4.2803144 -4.2665744 -4.254118 -4.2499957 -4.2526774 -4.2597919]]...]
INFO - root - 2017-12-07 17:25:37.882099: step 34510, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 59h:00m:59s remains)
INFO - root - 2017-12-07 17:25:44.620811: step 34520, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 55h:38m:20s remains)
INFO - root - 2017-12-07 17:25:51.298958: step 34530, loss = 2.05, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 51h:24m:46s remains)
INFO - root - 2017-12-07 17:25:57.990927: step 34540, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 53h:25m:01s remains)
INFO - root - 2017-12-07 17:26:04.871232: step 34550, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 61h:04m:32s remains)
INFO - root - 2017-12-07 17:26:11.663653: step 34560, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 57h:57m:16s remains)
INFO - root - 2017-12-07 17:26:18.405557: step 34570, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 55h:57m:29s remains)
INFO - root - 2017-12-07 17:26:25.186209: step 34580, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 52h:39m:17s remains)
INFO - root - 2017-12-07 17:26:31.992048: step 34590, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 53h:59m:09s remains)
INFO - root - 2017-12-07 17:26:38.742765: step 34600, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.745 sec/batch; 61h:40m:18s remains)
2017-12-07 17:26:39.438604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2637787 -4.2691374 -4.2779455 -4.28584 -4.2893853 -4.2876678 -4.2826133 -4.2810574 -4.2912879 -4.3053107 -4.3114195 -4.3065786 -4.2969131 -4.2886043 -4.279058][-4.2713256 -4.2843947 -4.2951479 -4.3029795 -4.3040762 -4.29772 -4.283205 -4.2752342 -4.2796021 -4.2878108 -4.2895942 -4.2887478 -4.2838192 -4.2726173 -4.2507339][-4.2932329 -4.3123636 -4.321486 -4.3244557 -4.3190565 -4.3030653 -4.2756596 -4.2545333 -4.2512746 -4.2574081 -4.2574944 -4.2591424 -4.2579212 -4.2403536 -4.2001667][-4.3170857 -4.3341603 -4.33704 -4.3321276 -4.3174491 -4.2871842 -4.2391481 -4.1977491 -4.1841855 -4.1930766 -4.1996412 -4.2103143 -4.2202272 -4.1996689 -4.138422][-4.3315506 -4.344553 -4.3401175 -4.3215761 -4.2850394 -4.2252321 -4.1424208 -4.0702348 -4.0471339 -4.0748305 -4.1121531 -4.150866 -4.1843338 -4.1740594 -4.1142488][-4.3324142 -4.341897 -4.3300881 -4.2917995 -4.2235117 -4.1269045 -4.0001869 -3.8888872 -3.8656225 -3.9417474 -4.0369034 -4.1182013 -4.1756511 -4.186408 -4.1535926][-4.3171034 -4.3202238 -4.3028879 -4.253387 -4.1672692 -4.0486107 -3.8987889 -3.7738941 -3.7723074 -3.9012365 -4.040484 -4.1403584 -4.2031331 -4.2261691 -4.2175779][-4.2856755 -4.2823257 -4.2673454 -4.2253776 -4.1492796 -4.0472875 -3.9309223 -3.8470452 -3.8649054 -3.9855595 -4.1104741 -4.1912961 -4.235393 -4.2509613 -4.2479582][-4.2420349 -4.2305946 -4.2229271 -4.2088881 -4.1735067 -4.1208296 -4.0621686 -4.0194287 -4.0328441 -4.1027813 -4.1782446 -4.2276592 -4.2490191 -4.2523623 -4.2525654][-4.2048054 -4.1836104 -4.1827149 -4.193892 -4.1964521 -4.1875958 -4.1700029 -4.1483493 -4.14668 -4.1768684 -4.2163563 -4.24552 -4.2552137 -4.259397 -4.2671652][-4.1889048 -4.1587286 -4.152164 -4.1724105 -4.1919761 -4.2106376 -4.219698 -4.2154713 -4.2134542 -4.2261262 -4.2515039 -4.2702069 -4.2773733 -4.2855964 -4.2947297][-4.196507 -4.1588621 -4.1446424 -4.162622 -4.1910338 -4.227345 -4.2564898 -4.2695413 -4.2736139 -4.2805758 -4.2948747 -4.3068004 -4.31034 -4.3141413 -4.3193712][-4.2271323 -4.1920824 -4.1782379 -4.1920457 -4.2238793 -4.2630372 -4.2973881 -4.3169522 -4.3230271 -4.3226485 -4.3252287 -4.3277426 -4.3251252 -4.3226914 -4.3219624][-4.2721434 -4.2494197 -4.2409015 -4.2472968 -4.2663574 -4.2914815 -4.3168588 -4.3336129 -4.3367958 -4.32832 -4.3208723 -4.3164177 -4.3111973 -4.3049951 -4.2998891][-4.3079162 -4.2956934 -4.2899404 -4.288806 -4.2892947 -4.2950964 -4.3064446 -4.3128886 -4.3078361 -4.2925434 -4.2802296 -4.2753177 -4.2730012 -4.2684746 -4.2632251]]...]
INFO - root - 2017-12-07 17:26:46.128323: step 34610, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 53h:09m:14s remains)
INFO - root - 2017-12-07 17:26:52.866379: step 34620, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 53h:28m:30s remains)
INFO - root - 2017-12-07 17:26:59.745340: step 34630, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.748 sec/batch; 61h:53m:51s remains)
INFO - root - 2017-12-07 17:27:06.704480: step 34640, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 59h:30m:42s remains)
INFO - root - 2017-12-07 17:27:13.505313: step 34650, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 60h:10m:19s remains)
INFO - root - 2017-12-07 17:27:20.115375: step 34660, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 56h:51m:46s remains)
INFO - root - 2017-12-07 17:27:26.921896: step 34670, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 59h:06m:27s remains)
INFO - root - 2017-12-07 17:27:33.744459: step 34680, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 55h:36m:34s remains)
INFO - root - 2017-12-07 17:27:40.594855: step 34690, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 54h:45m:07s remains)
INFO - root - 2017-12-07 17:27:47.222245: step 34700, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 53h:42m:15s remains)
2017-12-07 17:27:48.004033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2720675 -4.2281055 -4.1798129 -4.1508389 -4.1487012 -4.1504169 -4.1510949 -4.1633773 -4.19768 -4.2248149 -4.2414103 -4.245986 -4.2421513 -4.2394719 -4.2458973][-4.25375 -4.1992612 -4.1394963 -4.1016135 -4.0987282 -4.1032839 -4.1063447 -4.1241527 -4.1710873 -4.2094955 -4.2321997 -4.2401052 -4.2313328 -4.2256384 -4.2353745][-4.2400641 -4.1766133 -4.1078339 -4.0609822 -4.0572329 -4.067265 -4.0751224 -4.0991087 -4.1567478 -4.1974821 -4.2188873 -4.2304721 -4.2238889 -4.2224646 -4.234674][-4.226522 -4.1541195 -4.077311 -4.0245647 -4.0246606 -4.0444412 -4.0583062 -4.084198 -4.1486464 -4.1822052 -4.1921797 -4.19908 -4.1940317 -4.1928954 -4.2014079][-4.2220912 -4.145155 -4.0645657 -4.0112052 -4.012959 -4.0344872 -4.0408978 -4.0567513 -4.1244063 -4.1539817 -4.157639 -4.15752 -4.1483874 -4.1409745 -4.1425571][-4.2242875 -4.14663 -4.0658627 -4.0105195 -4.0029683 -4.0087476 -3.9893494 -3.9856913 -4.0559244 -4.0927062 -4.099371 -4.0958028 -4.0863876 -4.078702 -4.0773892][-4.2204862 -4.1376953 -4.05346 -3.9889352 -3.9601326 -3.9356921 -3.8775496 -3.8462543 -3.9336016 -3.996511 -4.022635 -4.0257 -4.0175791 -4.0082221 -4.004149][-4.2107348 -4.122129 -4.0336757 -3.9608169 -3.9143212 -3.8596632 -3.7593102 -3.6891139 -3.7951226 -3.8923624 -3.9425328 -3.9578989 -3.960041 -3.9576089 -3.9577384][-4.2005591 -4.1113796 -4.0236139 -3.9541924 -3.9091902 -3.8527822 -3.7547932 -3.6848795 -3.7823884 -3.8764009 -3.9237673 -3.9414494 -3.9536538 -3.9591143 -3.9667544][-4.1984944 -4.1170354 -4.0392089 -3.9824 -3.9512711 -3.9164276 -3.8548188 -3.8146181 -3.8844631 -3.9462051 -3.9705515 -3.9817636 -4.0019164 -4.017406 -4.0289974][-4.2088842 -4.1400242 -4.07985 -4.0407934 -4.0230451 -4.008677 -3.9775515 -3.957 -3.9983788 -4.0293336 -4.0310125 -4.037004 -4.06116 -4.078815 -4.0886602][-4.231761 -4.17659 -4.1335678 -4.1092415 -4.100379 -4.0996614 -4.0897818 -4.0817366 -4.1028428 -4.11318 -4.1037717 -4.1038766 -4.1222873 -4.1341691 -4.1364126][-4.2644091 -4.2253103 -4.1966071 -4.1808424 -4.1758838 -4.1815791 -4.1807179 -4.1781292 -4.189538 -4.1910367 -4.1806417 -4.1773086 -4.1872549 -4.1924024 -4.192102][-4.2959867 -4.2716894 -4.2529812 -4.24071 -4.2360997 -4.240993 -4.243783 -4.2441769 -4.2514272 -4.251895 -4.2455654 -4.2427764 -4.245759 -4.2461386 -4.2451229][-4.3195596 -4.3063431 -4.2945065 -4.2854581 -4.2814813 -4.2838697 -4.287806 -4.29147 -4.2968912 -4.2963986 -4.2922115 -4.2907996 -4.2910633 -4.2896233 -4.2886691]]...]
INFO - root - 2017-12-07 17:27:54.789426: step 34710, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:27m:18s remains)
INFO - root - 2017-12-07 17:28:01.528888: step 34720, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.643 sec/batch; 53h:08m:54s remains)
INFO - root - 2017-12-07 17:28:08.368970: step 34730, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 55h:49m:46s remains)
INFO - root - 2017-12-07 17:28:15.138347: step 34740, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 59h:31m:33s remains)
INFO - root - 2017-12-07 17:28:21.924537: step 34750, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 57h:49m:25s remains)
INFO - root - 2017-12-07 17:28:28.715976: step 34760, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 55h:39m:28s remains)
INFO - root - 2017-12-07 17:28:35.543392: step 34770, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.664 sec/batch; 54h:53m:11s remains)
INFO - root - 2017-12-07 17:28:42.370842: step 34780, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 59h:00m:04s remains)
INFO - root - 2017-12-07 17:28:49.184767: step 34790, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 59h:48m:21s remains)
INFO - root - 2017-12-07 17:28:55.816564: step 34800, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 56h:37m:22s remains)
2017-12-07 17:28:56.471070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1572814 -4.1508 -4.1303315 -4.1126671 -4.093688 -4.0867038 -4.103961 -4.1078143 -4.1001906 -4.1243582 -4.1671 -4.2031651 -4.2187243 -4.2193775 -4.2010984][-4.1122065 -4.1190619 -4.1065779 -4.0822 -4.0624104 -4.0634432 -4.0944495 -4.1100726 -4.1122308 -4.1371841 -4.1740732 -4.1971536 -4.2043118 -4.1953588 -4.1602612][-4.1024637 -4.1185584 -4.1096811 -4.0844269 -4.0608053 -4.0617495 -4.0975704 -4.1259122 -4.1413488 -4.1664209 -4.192924 -4.1992927 -4.1908164 -4.1679387 -4.1249676][-4.1488047 -4.1606951 -4.1503386 -4.1241832 -4.094934 -4.0868735 -4.1157928 -4.143744 -4.1642451 -4.1907797 -4.2141466 -4.21447 -4.1969709 -4.1676426 -4.1312122][-4.1943951 -4.2031574 -4.190249 -4.1636958 -4.1295385 -4.1103687 -4.122653 -4.1390123 -4.1576934 -4.18907 -4.2187366 -4.2274647 -4.2159009 -4.1914611 -4.162159][-4.213027 -4.2202659 -4.2057238 -4.1764388 -4.1362734 -4.101172 -4.0907068 -4.095376 -4.1224461 -4.1664839 -4.2088995 -4.2329421 -4.2314768 -4.2159181 -4.1936474][-4.2046213 -4.2067461 -4.1878119 -4.1546311 -4.1073294 -4.0482287 -4.0019016 -3.9968681 -4.0497355 -4.1165137 -4.1703668 -4.2078786 -4.221262 -4.2227249 -4.210989][-4.1729388 -4.1590662 -4.1317449 -4.0912051 -4.0255485 -3.9212189 -3.8285723 -3.8365259 -3.9451315 -4.0442972 -4.1120405 -4.1640439 -4.195857 -4.2136879 -4.2119861][-4.1468964 -4.1114149 -4.0731211 -4.0353727 -3.9625716 -3.8275814 -3.716512 -3.7567945 -3.8997545 -4.0110397 -4.0805955 -4.1362367 -4.1775122 -4.2025371 -4.203742][-4.1470776 -4.1000772 -4.0611482 -4.0415463 -3.9917917 -3.8870897 -3.8096349 -3.8517146 -3.9594305 -4.0420775 -4.0955005 -4.1397696 -4.1797781 -4.2035975 -4.2035341][-4.1360836 -4.0862513 -4.054328 -4.0518394 -4.0330677 -3.9764545 -3.9395154 -3.9657104 -4.028996 -4.0804505 -4.1191378 -4.158071 -4.1923485 -4.2097068 -4.205936][-4.1179042 -4.0713325 -4.0471191 -4.0543938 -4.0541143 -4.0295253 -4.0188031 -4.0393696 -4.0822611 -4.1147828 -4.1442 -4.179697 -4.2075424 -4.2158756 -4.204895][-4.1268287 -4.0877151 -4.0690317 -4.077054 -4.0839868 -4.074616 -4.0743003 -4.0926313 -4.1270075 -4.150671 -4.1723585 -4.19975 -4.21874 -4.2195845 -4.2061][-4.16682 -4.1347933 -4.1158414 -4.1156254 -4.1224184 -4.1219358 -4.1262703 -4.143033 -4.1695304 -4.1879697 -4.202672 -4.2188778 -4.2274313 -4.2245927 -4.2155585][-4.2167768 -4.192215 -4.1715603 -4.1631951 -4.1669788 -4.1711111 -4.1777391 -4.1902294 -4.2059622 -4.2178812 -4.2272534 -4.2351789 -4.2377496 -4.2361469 -4.2344208]]...]
INFO - root - 2017-12-07 17:29:03.227478: step 34810, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.693 sec/batch; 57h:15m:58s remains)
INFO - root - 2017-12-07 17:29:10.178744: step 34820, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.745 sec/batch; 61h:34m:32s remains)
INFO - root - 2017-12-07 17:29:16.961952: step 34830, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 56h:54m:54s remains)
INFO - root - 2017-12-07 17:29:23.744402: step 34840, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.657 sec/batch; 54h:18m:00s remains)
INFO - root - 2017-12-07 17:29:30.472932: step 34850, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 52h:40m:47s remains)
INFO - root - 2017-12-07 17:29:37.169939: step 34860, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 55h:09m:30s remains)
INFO - root - 2017-12-07 17:29:43.914934: step 34870, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 58h:27m:17s remains)
INFO - root - 2017-12-07 17:29:50.667692: step 34880, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 56h:02m:32s remains)
INFO - root - 2017-12-07 17:29:57.554126: step 34890, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 56h:09m:45s remains)
INFO - root - 2017-12-07 17:30:04.149030: step 34900, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 52h:47m:58s remains)
2017-12-07 17:30:04.931113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1571264 -4.1229525 -4.1052275 -4.1243095 -4.1684518 -4.2026749 -4.20881 -4.1722569 -4.0998526 -4.0503359 -4.0653286 -4.1130757 -4.1557221 -4.1976085 -4.2448039][-4.1876545 -4.1484308 -4.1259985 -4.1450548 -4.1878242 -4.2189846 -4.2206655 -4.1752734 -4.0923429 -4.0328045 -4.0438724 -4.0963683 -4.1482186 -4.2005014 -4.2533379][-4.2067018 -4.1588879 -4.128643 -4.1446342 -4.1859388 -4.2200265 -4.2284594 -4.1914697 -4.1202521 -4.0672297 -4.0716133 -4.1191735 -4.1750989 -4.2322688 -4.2831731][-4.2029796 -4.1489425 -4.1124334 -4.1185131 -4.1542549 -4.19521 -4.2197857 -4.2050505 -4.1596265 -4.1235747 -4.1224031 -4.1596437 -4.2108712 -4.2640085 -4.3083673][-4.1879511 -4.1283045 -4.0835652 -4.0775747 -4.1071324 -4.1553316 -4.1980944 -4.2086816 -4.1914854 -4.1710172 -4.16233 -4.1819386 -4.2198219 -4.2653174 -4.3063493][-4.1778803 -4.1140828 -4.0612912 -4.0410285 -4.0624533 -4.11545 -4.1711311 -4.2034397 -4.2068763 -4.19415 -4.17598 -4.1759949 -4.1967111 -4.2346964 -4.2758703][-4.176446 -4.1102753 -4.0539064 -4.0227962 -4.0340056 -4.0867348 -4.1479049 -4.190876 -4.206593 -4.1974163 -4.1729503 -4.1578193 -4.1607537 -4.1884246 -4.23052][-4.183516 -4.1261573 -4.0761595 -4.0413365 -4.0394211 -4.0788927 -4.1322579 -4.1750345 -4.194345 -4.18461 -4.1570687 -4.1305966 -4.118196 -4.1385322 -4.1876931][-4.1994519 -4.1605787 -4.1245489 -4.0957251 -4.0852456 -4.0984149 -4.1282792 -4.15918 -4.1754026 -4.1633773 -4.1352921 -4.1040874 -4.0846519 -4.104424 -4.1595163][-4.2127948 -4.1910076 -4.1742635 -4.1580625 -4.1441388 -4.134882 -4.1381392 -4.1535215 -4.1615167 -4.1463995 -4.1204185 -4.0911684 -4.0731516 -4.0959163 -4.1520066][-4.2026215 -4.196486 -4.1997056 -4.2026467 -4.1931481 -4.1711812 -4.154542 -4.1592126 -4.1623793 -4.1495562 -4.1313872 -4.1130338 -4.101614 -4.122777 -4.1671996][-4.1692681 -4.1777096 -4.1976643 -4.2170563 -4.2160883 -4.1909552 -4.1694989 -4.173583 -4.1815872 -4.1800013 -4.1780806 -4.1724181 -4.1644068 -4.17711 -4.2004714][-4.1325078 -4.1559043 -4.1906924 -4.2210708 -4.2262545 -4.200274 -4.17892 -4.1878963 -4.2055626 -4.2190595 -4.2327375 -4.2368832 -4.2312884 -4.234056 -4.234333][-4.1119323 -4.1458478 -4.1881471 -4.2216372 -4.2253566 -4.1979146 -4.1793914 -4.19426 -4.2235932 -4.2493172 -4.2708645 -4.2789845 -4.2752805 -4.2723475 -4.2569][-4.1153135 -4.1483264 -4.188076 -4.2162957 -4.2137876 -4.1844206 -4.172152 -4.1956196 -4.2356071 -4.268651 -4.2914476 -4.2977667 -4.2922897 -4.2842422 -4.2600155]]...]
INFO - root - 2017-12-07 17:30:11.730700: step 34910, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 53h:13m:58s remains)
INFO - root - 2017-12-07 17:30:18.543277: step 34920, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.669 sec/batch; 55h:20m:21s remains)
INFO - root - 2017-12-07 17:30:25.306005: step 34930, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 53h:42m:25s remains)
INFO - root - 2017-12-07 17:30:32.060340: step 34940, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:23m:38s remains)
INFO - root - 2017-12-07 17:30:38.887874: step 34950, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.730 sec/batch; 60h:18m:18s remains)
INFO - root - 2017-12-07 17:30:45.688202: step 34960, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 57h:32m:36s remains)
INFO - root - 2017-12-07 17:30:52.326170: step 34970, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 54h:12m:41s remains)
INFO - root - 2017-12-07 17:30:59.105750: step 34980, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 58h:28m:11s remains)
INFO - root - 2017-12-07 17:31:05.967859: step 34990, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 60h:48m:27s remains)
INFO - root - 2017-12-07 17:31:12.595063: step 35000, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 58h:40m:46s remains)
2017-12-07 17:31:13.321596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1703 -4.1306019 -4.0770483 -4.0414228 -4.0425887 -4.0457339 -4.044003 -4.0526042 -4.0797763 -4.1052456 -4.1147571 -4.1226854 -4.1325021 -4.1394854 -4.1455326][-4.167522 -4.1241965 -4.0687594 -4.0354323 -4.0446038 -4.0643048 -4.0791893 -4.0982003 -4.1273756 -4.1495056 -4.1567755 -4.1615286 -4.1655159 -4.1653624 -4.162][-4.17784 -4.1368728 -4.0856318 -4.05574 -4.0646753 -4.08342 -4.1017971 -4.12632 -4.153862 -4.170846 -4.1759319 -4.1791096 -4.1795316 -4.17618 -4.1701169][-4.1933489 -4.1589532 -4.1156721 -4.0887818 -4.094358 -4.1040883 -4.1122789 -4.133471 -4.1574936 -4.1753678 -4.1831326 -4.1826229 -4.1749954 -4.1688166 -4.1611447][-4.2133222 -4.1848879 -4.1428361 -4.1102214 -4.1055756 -4.1018209 -4.09502 -4.1069016 -4.1272974 -4.1461844 -4.1587725 -4.160162 -4.1487184 -4.139432 -4.1289587][-4.22432 -4.1985645 -4.1549487 -4.1163735 -4.0941691 -4.0682621 -4.0396676 -4.0377355 -4.0602942 -4.0873919 -4.1062441 -4.1106524 -4.0970736 -4.081697 -4.0701904][-4.2233114 -4.1956439 -4.1464629 -4.0951228 -4.0562549 -4.0119529 -3.9653573 -3.9503019 -3.9782035 -4.018249 -4.04732 -4.0580912 -4.0523286 -4.0394535 -4.0282841][-4.2155504 -4.1844687 -4.1306148 -4.0713334 -4.0262833 -3.9751823 -3.9181588 -3.8906898 -3.9190848 -3.9706948 -4.0114012 -4.0355344 -4.0456429 -4.0415087 -4.0292363][-4.2062817 -4.1753511 -4.1203513 -4.0615129 -4.0193172 -3.9668574 -3.9077389 -3.8706117 -3.8882606 -3.9410281 -3.9883115 -4.0219588 -4.0438914 -4.0473108 -4.0364242][-4.199553 -4.16747 -4.1143351 -4.0595207 -4.0230551 -3.9762363 -3.9241254 -3.8871167 -3.8960972 -3.9456527 -3.9932098 -4.0263605 -4.0550008 -4.0663438 -4.0607538][-4.2008395 -4.1669078 -4.1162486 -4.0676351 -4.0432858 -4.0165291 -3.9798839 -3.9489357 -3.9539495 -3.9939811 -4.0336246 -4.0638752 -4.0969381 -4.1131592 -4.111311][-4.2059221 -4.1735034 -4.1267557 -4.0860968 -4.07374 -4.0659161 -4.0465012 -4.0243282 -4.026782 -4.0552154 -4.0818753 -4.1067643 -4.137135 -4.152904 -4.1519642][-4.2163277 -4.1878014 -4.1471033 -4.1169739 -4.1091027 -4.1068296 -4.098433 -4.0844316 -4.0888681 -4.1128063 -4.1293974 -4.1441016 -4.1644697 -4.1732097 -4.1718736][-4.2340164 -4.2123914 -4.18293 -4.1630764 -4.156085 -4.1509595 -4.1413984 -4.1243367 -4.1241531 -4.1441989 -4.1586237 -4.1677346 -4.1800365 -4.183218 -4.1780095][-4.2559109 -4.2428269 -4.222446 -4.2067127 -4.1958613 -4.1890869 -4.1820612 -4.1687756 -4.1636982 -4.1769814 -4.1876683 -4.1907334 -4.19193 -4.1878309 -4.1744204]]...]
INFO - root - 2017-12-07 17:31:20.109124: step 35010, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 59h:04m:14s remains)
INFO - root - 2017-12-07 17:31:26.952836: step 35020, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 59h:05m:14s remains)
INFO - root - 2017-12-07 17:31:33.720529: step 35030, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 54h:08m:54s remains)
INFO - root - 2017-12-07 17:31:40.468379: step 35040, loss = 2.06, batch loss = 2.00 (13.4 examples/sec; 0.599 sec/batch; 49h:29m:25s remains)
INFO - root - 2017-12-07 17:31:47.224175: step 35050, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 52h:46m:22s remains)
INFO - root - 2017-12-07 17:31:54.067524: step 35060, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 58h:41m:55s remains)
INFO - root - 2017-12-07 17:32:00.984098: step 35070, loss = 2.10, batch loss = 2.04 (10.7 examples/sec; 0.750 sec/batch; 61h:55m:45s remains)
INFO - root - 2017-12-07 17:32:07.722725: step 35080, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 54h:49m:51s remains)
INFO - root - 2017-12-07 17:32:14.472999: step 35090, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 51h:43m:27s remains)
INFO - root - 2017-12-07 17:32:20.992398: step 35100, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 51h:32m:35s remains)
2017-12-07 17:32:21.733462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1655669 -4.1951232 -4.2037363 -4.1984463 -4.1925535 -4.1759696 -4.175138 -4.1862297 -4.1935396 -4.2044353 -4.213995 -4.2196159 -4.2117558 -4.1973524 -4.1889348][-4.1445675 -4.170023 -4.1806955 -4.1780415 -4.1734085 -4.1631823 -4.1609292 -4.1745024 -4.1867743 -4.2003369 -4.210351 -4.2168784 -4.2088819 -4.19433 -4.1843915][-4.1362433 -4.157218 -4.1671667 -4.1715307 -4.1736035 -4.1643362 -4.1558032 -4.1638632 -4.1815333 -4.1968732 -4.2063594 -4.2103472 -4.2009039 -4.1903138 -4.1794753][-4.1325874 -4.1417632 -4.1481991 -4.1588225 -4.1666279 -4.16027 -4.1472044 -4.1481018 -4.1640491 -4.1759048 -4.1814094 -4.1805778 -4.1718407 -4.1646671 -4.1572318][-4.1295652 -4.12315 -4.1223965 -4.1341786 -4.142539 -4.14126 -4.1291394 -4.1249561 -4.1367459 -4.1421824 -4.1437221 -4.1409583 -4.12848 -4.1251755 -4.1220107][-4.11953 -4.0981927 -4.0977244 -4.11674 -4.1240358 -4.1204123 -4.1088076 -4.0979519 -4.1020861 -4.1062074 -4.1090183 -4.1021295 -4.0915494 -4.0903716 -4.0874405][-4.1052685 -4.0795622 -4.0833535 -4.1083479 -4.1140041 -4.1065488 -4.0913277 -4.0688195 -4.06518 -4.07172 -4.0809755 -4.0767975 -4.066308 -4.0675917 -4.0719376][-4.1123471 -4.0802717 -4.0779657 -4.1011443 -4.1084323 -4.1019139 -4.0869327 -4.0636158 -4.0612803 -4.0741444 -4.0879974 -4.0840707 -4.0691056 -4.071167 -4.0811582][-4.1401706 -4.0998178 -4.086915 -4.1080556 -4.1178136 -4.1172462 -4.1096363 -4.090425 -4.0897751 -4.1016135 -4.1169434 -4.1166286 -4.1011372 -4.09401 -4.0926113][-4.1650448 -4.1341453 -4.1223273 -4.1379986 -4.1447916 -4.1447344 -4.14199 -4.1295152 -4.1269107 -4.1375332 -4.1557617 -4.1580119 -4.1446004 -4.1256437 -4.0976939][-4.1627569 -4.1578293 -4.1630135 -4.1775541 -4.1781855 -4.1701913 -4.1651773 -4.1562557 -4.151659 -4.1608491 -4.177784 -4.1852365 -4.1779866 -4.1503105 -4.100204][-4.1506124 -4.1677895 -4.1880584 -4.2065039 -4.2026796 -4.1893196 -4.1854758 -4.1794276 -4.1736946 -4.1780791 -4.187345 -4.1963768 -4.1960988 -4.16737 -4.1203241][-4.151402 -4.1751857 -4.1986651 -4.2177076 -4.219624 -4.2096934 -4.2087183 -4.2063732 -4.2024646 -4.2024126 -4.2022166 -4.2090659 -4.2070742 -4.1798673 -4.1395469][-4.1645312 -4.1924229 -4.2133327 -4.2282071 -4.2352352 -4.2299008 -4.2277703 -4.22697 -4.225677 -4.2240305 -4.2204247 -4.223259 -4.2148218 -4.1853042 -4.1485963][-4.1799212 -4.2077165 -4.2245903 -4.23308 -4.2411246 -4.2416945 -4.2377834 -4.2363153 -4.2360392 -4.2346087 -4.2342844 -4.2356224 -4.2202158 -4.186501 -4.1555948]]...]
INFO - root - 2017-12-07 17:32:28.510153: step 35110, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 56h:12m:50s remains)
INFO - root - 2017-12-07 17:32:35.270019: step 35120, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 51h:54m:13s remains)
INFO - root - 2017-12-07 17:32:42.054353: step 35130, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 57h:48m:43s remains)
INFO - root - 2017-12-07 17:32:48.863341: step 35140, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 58h:27m:58s remains)
INFO - root - 2017-12-07 17:32:55.549861: step 35150, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 57h:07m:27s remains)
INFO - root - 2017-12-07 17:33:02.343587: step 35160, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 56h:44m:15s remains)
INFO - root - 2017-12-07 17:33:09.206751: step 35170, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 60h:35m:59s remains)
INFO - root - 2017-12-07 17:33:15.950121: step 35180, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 53h:57m:36s remains)
INFO - root - 2017-12-07 17:33:22.720627: step 35190, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 55h:36m:50s remains)
INFO - root - 2017-12-07 17:33:29.296139: step 35200, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 60h:07m:49s remains)
2017-12-07 17:33:30.103614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2465615 -4.2316041 -4.2224922 -4.2144032 -4.1993046 -4.1754379 -4.1474504 -4.1276789 -4.123466 -4.133276 -4.1489472 -4.1649418 -4.1767082 -4.1871367 -4.2001963][-4.2515225 -4.2372084 -4.2274871 -4.2161369 -4.1961422 -4.1685691 -4.1379824 -4.1188979 -4.1181421 -4.1338124 -4.1543207 -4.16818 -4.1732407 -4.1782613 -4.1897607][-4.2569675 -4.244463 -4.2359419 -4.2250013 -4.2038918 -4.1749139 -4.143743 -4.1308289 -4.1391563 -4.1573577 -4.1757522 -4.1825728 -4.1758351 -4.1689687 -4.1758208][-4.2659311 -4.2549443 -4.2477627 -4.2378626 -4.2159553 -4.1807623 -4.1429596 -4.1337633 -4.1526666 -4.1748872 -4.1904111 -4.1905947 -4.172287 -4.1505952 -4.1523061][-4.27311 -4.2602844 -4.2507381 -4.2382927 -4.2109313 -4.1629276 -4.1101265 -4.0962381 -4.1249394 -4.1586933 -4.1793919 -4.1819396 -4.1585116 -4.1294618 -4.1279793][-4.2706609 -4.2523956 -4.2339334 -4.2118163 -4.1738691 -4.1104536 -4.038372 -4.0107594 -4.0491171 -4.1068187 -4.145761 -4.1592646 -4.1433382 -4.1188293 -4.1187868][-4.2572994 -4.2299695 -4.1983733 -4.1630321 -4.1157556 -4.0448108 -3.9620888 -3.9195747 -3.9661989 -4.0522676 -4.1161742 -4.1420689 -4.1355486 -4.1214862 -4.124577][-4.2390866 -4.2018614 -4.1575069 -4.1143432 -4.0689588 -4.0089846 -3.94067 -3.9013743 -3.9452174 -4.0381632 -4.1113677 -4.1422243 -4.1436424 -4.1394968 -4.1469989][-4.2247734 -4.1805162 -4.1309323 -4.0896411 -4.05584 -4.0175071 -3.9790154 -3.9579988 -3.9904218 -4.0624456 -4.1237583 -4.1514764 -4.1610518 -4.16976 -4.1839571][-4.2213526 -4.1773171 -4.131763 -4.098886 -4.07638 -4.0572205 -4.0429912 -4.0362453 -4.0563765 -4.1017118 -4.1439104 -4.1659632 -4.1810956 -4.1984334 -4.2152543][-4.2274747 -4.1899581 -4.1538434 -4.1288137 -4.1135964 -4.1052313 -4.1023264 -4.1025677 -4.1139688 -4.1365461 -4.159513 -4.170619 -4.1811128 -4.1966605 -4.2098093][-4.2396159 -4.209806 -4.1797605 -4.1588869 -4.1467719 -4.1424465 -4.1449909 -4.1494794 -4.1565781 -4.1627369 -4.1655474 -4.1631269 -4.1628518 -4.1685977 -4.1731129][-4.2427421 -4.2165055 -4.1884751 -4.1689758 -4.1579514 -4.1567774 -4.1625242 -4.1668239 -4.1702724 -4.1669455 -4.157269 -4.1478109 -4.1417623 -4.1395435 -4.1373968][-4.2260904 -4.1972671 -4.1672459 -4.1476622 -4.1378927 -4.1391973 -4.1466122 -4.1513157 -4.15244 -4.146244 -4.1365509 -4.1311646 -4.1282721 -4.1257629 -4.1216068][-4.197978 -4.1667571 -4.1372905 -4.1203218 -4.1132579 -4.1145558 -4.1191187 -4.1214747 -4.1218429 -4.1174693 -4.1123042 -4.1119576 -4.1118164 -4.1089196 -4.1037936]]...]
INFO - root - 2017-12-07 17:33:36.862407: step 35210, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 54h:04m:58s remains)
INFO - root - 2017-12-07 17:33:43.693713: step 35220, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 59h:32m:34s remains)
INFO - root - 2017-12-07 17:33:50.533290: step 35230, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 58h:46m:07s remains)
INFO - root - 2017-12-07 17:33:57.352608: step 35240, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 58h:18m:46s remains)
INFO - root - 2017-12-07 17:34:04.072036: step 35250, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 51h:39m:38s remains)
INFO - root - 2017-12-07 17:34:10.856206: step 35260, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 53h:09m:34s remains)
INFO - root - 2017-12-07 17:34:17.757313: step 35270, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 60h:14m:19s remains)
INFO - root - 2017-12-07 17:34:24.487310: step 35280, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 56h:14m:38s remains)
INFO - root - 2017-12-07 17:34:31.301309: step 35290, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 54h:22m:33s remains)
INFO - root - 2017-12-07 17:34:37.930050: step 35300, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 52h:38m:28s remains)
2017-12-07 17:34:38.715971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24773 -4.2505903 -4.2485557 -4.2431879 -4.2358513 -4.2313395 -4.225081 -4.20909 -4.1796 -4.1559081 -4.1661367 -4.1973171 -4.2345843 -4.2800407 -4.3212342][-4.2522078 -4.247262 -4.236114 -4.2257261 -4.2171745 -4.2123289 -4.2000628 -4.1777925 -4.1452274 -4.1227646 -4.1435871 -4.186378 -4.2296243 -4.2756443 -4.3174605][-4.2546453 -4.2454762 -4.2294445 -4.2164955 -4.2075577 -4.2003384 -4.1843605 -4.1618018 -4.1261744 -4.0988665 -4.1218567 -4.17166 -4.2198772 -4.2659807 -4.3080878][-4.2486072 -4.2396231 -4.2238336 -4.2115297 -4.2040458 -4.1991086 -4.1882515 -4.1705232 -4.1332035 -4.097434 -4.1158013 -4.1660867 -4.2139516 -4.258306 -4.2989669][-4.2292671 -4.2223368 -4.2102876 -4.1987448 -4.1938562 -4.19522 -4.1937423 -4.18232 -4.1459951 -4.1039543 -4.1168656 -4.1631618 -4.2071533 -4.2493858 -4.28617][-4.2005248 -4.1913939 -4.1795382 -4.1633563 -4.1576829 -4.1663713 -4.1740131 -4.1675062 -4.1324563 -4.0853109 -4.0916152 -4.1343532 -4.1793733 -4.226234 -4.2665343][-4.1781573 -4.1647196 -4.1456552 -4.1165056 -4.10108 -4.1090736 -4.1180925 -4.1120811 -4.075388 -4.0228438 -4.0270386 -4.0756531 -4.1340857 -4.1949248 -4.2444963][-4.1691184 -4.1506276 -4.1161313 -4.0671639 -4.0369949 -4.037961 -4.0445738 -4.0396767 -4.0051575 -3.9540811 -3.962678 -4.0209551 -4.0934844 -4.1651425 -4.2231369][-4.1727562 -4.1551509 -4.116024 -4.062273 -4.0248632 -4.019928 -4.0259118 -4.0237656 -3.9967256 -3.9568279 -3.9684882 -4.02567 -4.0975394 -4.1676831 -4.2234044][-4.1852326 -4.1763859 -4.1465344 -4.1082239 -4.0841985 -4.08413 -4.0892143 -4.0861416 -4.065362 -4.0332093 -4.0393033 -4.0832047 -4.1414862 -4.2002993 -4.2465281][-4.197052 -4.1951923 -4.1767831 -4.1559153 -4.1470256 -4.1514115 -4.1539407 -4.1493611 -4.1327896 -4.1073737 -4.1110396 -4.1457171 -4.1941257 -4.2418671 -4.2761474][-4.2001815 -4.2054543 -4.1992335 -4.1919122 -4.1917157 -4.1978664 -4.1969361 -4.1879282 -4.1719227 -4.1513176 -4.155581 -4.1856384 -4.2279406 -4.2675385 -4.2938256][-4.1863351 -4.196444 -4.2010651 -4.2042742 -4.21259 -4.22204 -4.2198606 -4.2060466 -4.1866794 -4.1678448 -4.1726422 -4.2023144 -4.2408428 -4.2757092 -4.3007874][-4.1698489 -4.1786861 -4.1897407 -4.2045045 -4.2214003 -4.2338428 -4.2299633 -4.2108827 -4.1876454 -4.1698208 -4.1745763 -4.2040238 -4.23985 -4.2716727 -4.2989197][-4.1641488 -4.1708302 -4.1841741 -4.20491 -4.2259398 -4.2379203 -4.2313924 -4.2087293 -4.1837015 -4.1668758 -4.1735535 -4.2038746 -4.2367945 -4.2647724 -4.2915125]]...]
INFO - root - 2017-12-07 17:34:45.454539: step 35310, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 54h:00m:58s remains)
INFO - root - 2017-12-07 17:34:52.167214: step 35320, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 51h:24m:22s remains)
INFO - root - 2017-12-07 17:34:58.977564: step 35330, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 54h:53m:46s remains)
INFO - root - 2017-12-07 17:35:05.882627: step 35340, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 60h:47m:10s remains)
INFO - root - 2017-12-07 17:35:12.715096: step 35350, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 58h:26m:02s remains)
INFO - root - 2017-12-07 17:35:19.546156: step 35360, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 56h:33m:14s remains)
INFO - root - 2017-12-07 17:35:26.295169: step 35370, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 55h:30m:37s remains)
INFO - root - 2017-12-07 17:35:33.142727: step 35380, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 53h:03m:20s remains)
INFO - root - 2017-12-07 17:35:40.061417: step 35390, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 58h:03m:52s remains)
INFO - root - 2017-12-07 17:35:46.729336: step 35400, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 56h:11m:54s remains)
2017-12-07 17:35:47.433688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.321095 -4.3290658 -4.314652 -4.2853255 -4.253715 -4.2291627 -4.2140317 -4.2054472 -4.2015533 -4.1964221 -4.1973681 -4.1991296 -4.209415 -4.233386 -4.25152][-4.3197536 -4.32291 -4.2988725 -4.2608514 -4.2185388 -4.1825414 -4.163341 -4.1611519 -4.1672993 -4.1692472 -4.171423 -4.1716828 -4.1835475 -4.2085 -4.2265134][-4.318367 -4.31766 -4.2866549 -4.2417316 -4.1903486 -4.1436834 -4.1173053 -4.117094 -4.1332946 -4.1446266 -4.1475296 -4.1461024 -4.1625328 -4.1883249 -4.2058573][-4.3187575 -4.315577 -4.2839956 -4.2375207 -4.18153 -4.1248283 -4.0867534 -4.0823722 -4.10492 -4.128859 -4.1407218 -4.1458268 -4.1624184 -4.1812139 -4.1929665][-4.3194022 -4.3164911 -4.2885141 -4.2439375 -4.1821308 -4.1063223 -4.04641 -4.0345254 -4.0684938 -4.1179533 -4.1534624 -4.1723228 -4.1869655 -4.1933079 -4.1911812][-4.3157549 -4.3132863 -4.287087 -4.2409244 -4.1631742 -4.0595493 -3.9664841 -3.9496946 -4.0114813 -4.0937157 -4.1528568 -4.1853738 -4.2029552 -4.2053876 -4.1955609][-4.3098683 -4.3025532 -4.2686081 -4.2110028 -4.1141181 -3.9765291 -3.8483186 -3.8501036 -3.9641542 -4.0748715 -4.1423326 -4.1794953 -4.1977234 -4.2005715 -4.1931772][-4.3017416 -4.2901621 -4.2485371 -4.1809554 -4.0764322 -3.9399142 -3.8347178 -3.8758929 -4.00094 -4.0979013 -4.1495695 -4.1736569 -4.1821032 -4.1831369 -4.183795][-4.2976179 -4.2835402 -4.2403779 -4.178091 -4.1017485 -4.0192566 -3.9725831 -4.010757 -4.09195 -4.1486049 -4.171114 -4.177103 -4.1734028 -4.1708293 -4.1798134][-4.2983561 -4.2853694 -4.2514834 -4.2073584 -4.1627903 -4.1235614 -4.105329 -4.1235638 -4.1615391 -4.1862607 -4.1880894 -4.1801429 -4.1685038 -4.1698141 -4.1913309][-4.3012853 -4.2909641 -4.268178 -4.2396746 -4.2149396 -4.19374 -4.1846032 -4.1902847 -4.2066021 -4.2150197 -4.2090435 -4.1950841 -4.1853542 -4.1945486 -4.2162595][-4.3029394 -4.2962337 -4.2809219 -4.2626472 -4.2479062 -4.2348313 -4.2297564 -4.2339 -4.2443781 -4.2437887 -4.2335463 -4.2208033 -4.2161226 -4.2274857 -4.24229][-4.30562 -4.30237 -4.2882843 -4.272964 -4.2614832 -4.2512827 -4.2471952 -4.2523088 -4.261651 -4.2594891 -4.2489729 -4.2387743 -4.2360244 -4.2456303 -4.25402][-4.3101716 -4.3093925 -4.2921667 -4.273674 -4.2600532 -4.248795 -4.2421827 -4.2467275 -4.25434 -4.2513452 -4.2426181 -4.2355037 -4.2366614 -4.2482138 -4.255352][-4.314477 -4.3158917 -4.2955084 -4.27034 -4.2481709 -4.2294044 -4.21827 -4.2221031 -4.2304096 -4.2288103 -4.2228003 -4.2210355 -4.2282619 -4.2443361 -4.2534976]]...]
INFO - root - 2017-12-07 17:35:54.301442: step 35410, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 53h:10m:56s remains)
INFO - root - 2017-12-07 17:36:01.133043: step 35420, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 58h:58m:56s remains)
INFO - root - 2017-12-07 17:36:07.917878: step 35430, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 58h:10m:46s remains)
INFO - root - 2017-12-07 17:36:14.784004: step 35440, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 57h:32m:26s remains)
INFO - root - 2017-12-07 17:36:21.611916: step 35450, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 52h:05m:18s remains)
INFO - root - 2017-12-07 17:36:28.391888: step 35460, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 54h:37m:46s remains)
INFO - root - 2017-12-07 17:36:35.167987: step 35470, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 57h:11m:51s remains)
INFO - root - 2017-12-07 17:36:42.028720: step 35480, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.735 sec/batch; 60h:37m:10s remains)
INFO - root - 2017-12-07 17:36:48.804233: step 35490, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 56h:50m:12s remains)
INFO - root - 2017-12-07 17:36:55.421407: step 35500, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.636 sec/batch; 52h:27m:25s remains)
2017-12-07 17:36:56.127938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3456264 -4.3304396 -4.3107862 -4.2959757 -4.2773986 -4.2563982 -4.2425389 -4.2509146 -4.2691 -4.2804866 -4.2867265 -4.2893109 -4.2931366 -4.2994843 -4.304297][-4.3413544 -4.3181343 -4.2902131 -4.2683821 -4.2376223 -4.201849 -4.17634 -4.189147 -4.2155843 -4.2311783 -4.2389097 -4.2408719 -4.2456083 -4.2542577 -4.2606297][-4.3351822 -4.3049431 -4.2703714 -4.2417946 -4.1984682 -4.1493683 -4.1133571 -4.1281395 -4.1624975 -4.1814785 -4.1892233 -4.1864405 -4.1922207 -4.2057629 -4.2160969][-4.3290424 -4.2926412 -4.2533393 -4.218421 -4.1678576 -4.1102543 -4.0626616 -4.0713596 -4.1060476 -4.1212559 -4.1261349 -4.1244674 -4.134243 -4.1579804 -4.1764307][-4.3203874 -4.2759128 -4.2261257 -4.1804686 -4.1249981 -4.0609031 -4.0022511 -3.9994371 -4.027699 -4.0360351 -4.0391684 -4.0452247 -4.0652547 -4.1015849 -4.1306663][-4.3063793 -4.2520037 -4.1905637 -4.135252 -4.0774679 -4.0123787 -3.9469702 -3.9277627 -3.9393697 -3.9377744 -3.9390023 -3.9517922 -3.9867938 -4.0405703 -4.08248][-4.2965784 -4.2345281 -4.1639776 -4.1044917 -4.0475626 -3.9758928 -3.889468 -3.8437991 -3.8443584 -3.8458045 -3.8511949 -3.868259 -3.9094906 -3.9791949 -4.040843][-4.2852468 -4.2169805 -4.1449723 -4.0861192 -4.0217381 -3.938019 -3.832804 -3.7720935 -3.7761383 -3.7939761 -3.814364 -3.8364995 -3.8720496 -3.9400973 -4.0102305][-4.2811713 -4.2122865 -4.1425419 -4.0864916 -4.0208473 -3.9374702 -3.8383386 -3.7878604 -3.8014896 -3.8270862 -3.8499231 -3.8670275 -3.8891742 -3.9409986 -4.0120177][-4.291266 -4.2310939 -4.1720181 -4.1282887 -4.0760727 -4.0121703 -3.9384704 -3.907438 -3.91821 -3.9359593 -3.9546478 -3.9676619 -3.9816251 -4.01466 -4.0713468][-4.3095727 -4.2652936 -4.2232547 -4.19424 -4.1594691 -4.11898 -4.0689921 -4.0519409 -4.0600705 -4.0721316 -4.0880961 -4.1047454 -4.1185637 -4.1333838 -4.1644979][-4.3323832 -4.3062348 -4.2822232 -4.2649717 -4.2422347 -4.2176819 -4.185535 -4.1783261 -4.1843891 -4.1950111 -4.2117152 -4.23084 -4.2408776 -4.2441068 -4.2565074][-4.3517876 -4.3383245 -4.3259349 -4.3163896 -4.3035569 -4.2926803 -4.2764416 -4.2748451 -4.2810717 -4.2923584 -4.30601 -4.3190312 -4.3197412 -4.3183861 -4.3244128][-4.3631654 -4.35705 -4.3493419 -4.34197 -4.3347015 -4.3308735 -4.3248858 -4.3289633 -4.3403511 -4.3519568 -4.3607607 -4.3652935 -4.363637 -4.3618283 -4.3662796][-4.3694305 -4.367094 -4.3621783 -4.3560081 -4.350246 -4.3474188 -4.3465714 -4.3522844 -4.362988 -4.3720555 -4.3773336 -4.3780379 -4.3773112 -4.3756995 -4.3780465]]...]
INFO - root - 2017-12-07 17:37:02.801482: step 35510, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 55h:13m:37s remains)
INFO - root - 2017-12-07 17:37:09.608247: step 35520, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 60h:31m:44s remains)
INFO - root - 2017-12-07 17:37:16.383488: step 35530, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 55h:17m:50s remains)
INFO - root - 2017-12-07 17:37:23.161708: step 35540, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 51h:39m:47s remains)
INFO - root - 2017-12-07 17:37:30.024673: step 35550, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 55h:46m:18s remains)
INFO - root - 2017-12-07 17:37:36.869727: step 35560, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 58h:17m:40s remains)
INFO - root - 2017-12-07 17:37:43.609389: step 35570, loss = 2.11, batch loss = 2.05 (12.2 examples/sec; 0.658 sec/batch; 54h:18m:11s remains)
INFO - root - 2017-12-07 17:37:50.375390: step 35580, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 52h:25m:16s remains)
INFO - root - 2017-12-07 17:37:57.096217: step 35590, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 59h:14m:59s remains)
INFO - root - 2017-12-07 17:38:03.739952: step 35600, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 56h:26m:10s remains)
2017-12-07 17:38:04.475182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3449087 -4.3388162 -4.3056355 -4.2388792 -4.1469617 -4.0757756 -4.0806103 -4.1431289 -4.2084761 -4.2515264 -4.2817106 -4.3096709 -4.3234057 -4.3285475 -4.3256392][-4.347683 -4.3374557 -4.29714 -4.219543 -4.1093926 -4.0278192 -4.0462914 -4.1322823 -4.2088623 -4.2531538 -4.282938 -4.3124723 -4.32728 -4.3320956 -4.3273325][-4.3491378 -4.333662 -4.2879925 -4.20005 -4.0738606 -3.9807372 -4.0077882 -4.116559 -4.206562 -4.2526584 -4.2842946 -4.3148737 -4.3296261 -4.3323436 -4.3247981][-4.3464584 -4.3265815 -4.277669 -4.1822338 -4.0458155 -3.9381156 -3.9602559 -4.0832567 -4.1912465 -4.24646 -4.28308 -4.3147621 -4.3286052 -4.3278422 -4.3173246][-4.3439221 -4.3230982 -4.2721024 -4.1736364 -4.0333753 -3.910182 -3.9177921 -4.0495672 -4.1731224 -4.2396741 -4.28215 -4.31381 -4.3276076 -4.3231387 -4.3088517][-4.3418922 -4.3237247 -4.2679782 -4.1600046 -4.0086489 -3.8676467 -3.8602123 -4.003933 -4.1452994 -4.225615 -4.2746739 -4.3072815 -4.323122 -4.317194 -4.2994103][-4.3395009 -4.3263803 -4.2692804 -4.1528225 -3.9892678 -3.8314507 -3.808001 -3.9541419 -4.1114159 -4.2065167 -4.2634749 -4.298543 -4.31625 -4.308341 -4.2865162][-4.3361049 -4.3282437 -4.2785811 -4.169023 -4.0125828 -3.8564303 -3.8151863 -3.9393728 -4.0947323 -4.1961021 -4.2560339 -4.2903409 -4.3069077 -4.2973924 -4.2725406][-4.3311429 -4.3258142 -4.2863607 -4.1975193 -4.0674486 -3.9350352 -3.8821049 -3.9685953 -4.1024117 -4.1995149 -4.2589107 -4.28999 -4.3029408 -4.2901568 -4.2621274][-4.3265028 -4.321671 -4.291914 -4.2255449 -4.1249261 -4.0204167 -3.966455 -4.0210176 -4.1284394 -4.2149057 -4.2675672 -4.2942405 -4.3036375 -4.2891159 -4.2599564][-4.3252244 -4.3223062 -4.3014178 -4.2531848 -4.1779914 -4.1009345 -4.0558667 -4.09108 -4.175025 -4.2497692 -4.2916713 -4.3104534 -4.3152618 -4.2999606 -4.2710161][-4.3237047 -4.3242192 -4.3133006 -4.2815628 -4.2286563 -4.1754432 -4.14645 -4.1729407 -4.2344027 -4.2915916 -4.319664 -4.3283963 -4.325809 -4.3091645 -4.2808232][-4.3204269 -4.3245163 -4.3206997 -4.3025546 -4.2684579 -4.2333732 -4.2169881 -4.2365236 -4.2785635 -4.3178911 -4.3360324 -4.3382363 -4.3307633 -4.3142195 -4.2890663][-4.317317 -4.32317 -4.3241754 -4.3154407 -4.2953205 -4.27257 -4.2630877 -4.2763028 -4.3033915 -4.3271842 -4.3384547 -4.3381615 -4.3302031 -4.3167362 -4.29778][-4.314332 -4.3206816 -4.3244352 -4.3216591 -4.3110051 -4.2977991 -4.2925696 -4.3012905 -4.3181152 -4.331182 -4.3363023 -4.333931 -4.3265958 -4.31642 -4.3042622]]...]
INFO - root - 2017-12-07 17:38:11.217034: step 35610, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 55h:09m:49s remains)
INFO - root - 2017-12-07 17:38:18.133150: step 35620, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.753 sec/batch; 62h:06m:29s remains)
INFO - root - 2017-12-07 17:38:25.023819: step 35630, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 55h:15m:40s remains)
INFO - root - 2017-12-07 17:38:31.807752: step 35640, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.609 sec/batch; 50h:14m:50s remains)
INFO - root - 2017-12-07 17:38:38.636751: step 35650, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 53h:57m:22s remains)
INFO - root - 2017-12-07 17:38:45.655684: step 35660, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 60h:04m:56s remains)
INFO - root - 2017-12-07 17:38:52.470124: step 35670, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 58h:22m:20s remains)
INFO - root - 2017-12-07 17:38:59.285462: step 35680, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 57h:06m:46s remains)
INFO - root - 2017-12-07 17:39:06.047433: step 35690, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 51h:17m:44s remains)
INFO - root - 2017-12-07 17:39:12.717142: step 35700, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 53h:03m:55s remains)
2017-12-07 17:39:13.476392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2050796 -4.2032185 -4.1888337 -4.1802068 -4.1902056 -4.208755 -4.222878 -4.2161112 -4.1865616 -4.1734095 -4.1989584 -4.238379 -4.2693706 -4.2746816 -4.2680373][-4.2003589 -4.1859245 -4.1565681 -4.1380105 -4.1484466 -4.176518 -4.2011766 -4.2021394 -4.1788597 -4.1686282 -4.1975536 -4.240417 -4.2746024 -4.277267 -4.2657442][-4.2153411 -4.1937718 -4.1577053 -4.1320143 -4.1351895 -4.153872 -4.1699171 -4.1666245 -4.1502604 -4.1470809 -4.1833372 -4.2325449 -4.2682443 -4.2728829 -4.2646608][-4.2376761 -4.2186313 -4.1881762 -4.1681848 -4.1653466 -4.1640248 -4.1546 -4.1328149 -4.1090364 -4.1052256 -4.1538978 -4.2171164 -4.2547669 -4.2630563 -4.2611294][-4.2372403 -4.2259197 -4.2121816 -4.2058544 -4.2066436 -4.1914749 -4.1550612 -4.1094837 -4.0631227 -4.0487351 -4.1116514 -4.1940918 -4.2386503 -4.2492595 -4.2521043][-4.2092991 -4.2044244 -4.2130642 -4.2301021 -4.2424331 -4.2211485 -4.1652088 -4.0941625 -4.0214338 -3.9987323 -4.0706477 -4.1642189 -4.2199783 -4.2387791 -4.24736][-4.1680832 -4.1709957 -4.2007351 -4.2419834 -4.2644563 -4.2417583 -4.1739159 -4.0824866 -3.9929032 -3.9721053 -4.0480857 -4.1435461 -4.2067447 -4.2360792 -4.2481761][-4.1377249 -4.1466513 -4.191236 -4.2461123 -4.2700553 -4.2426624 -4.1655579 -4.0630035 -3.9758434 -3.9694211 -4.05129 -4.1472464 -4.2106204 -4.2420621 -4.2537932][-4.130199 -4.148859 -4.200397 -4.250864 -4.2672162 -4.2332587 -4.1503878 -4.0455747 -3.9704635 -3.9798381 -4.066731 -4.1641006 -4.2260532 -4.2545509 -4.26388][-4.1336608 -4.1643729 -4.2162127 -4.2519827 -4.2560945 -4.2152238 -4.1340861 -4.0381784 -3.9815967 -4.0035095 -4.0872221 -4.1788807 -4.2376666 -4.2656889 -4.2758036][-4.1465406 -4.187459 -4.2335663 -4.2541165 -4.2413249 -4.196485 -4.1252556 -4.0511327 -4.0214038 -4.0531011 -4.1218567 -4.1936636 -4.2415776 -4.2681313 -4.2812996][-4.167345 -4.2145829 -4.2518859 -4.2580953 -4.2318335 -4.1849313 -4.1248093 -4.0769186 -4.0751438 -4.1166611 -4.1680942 -4.2087531 -4.2363176 -4.2589531 -4.27574][-4.1960635 -4.2438235 -4.2685428 -4.2601156 -4.22211 -4.1687989 -4.1151233 -4.0941 -4.1177154 -4.1681066 -4.2051048 -4.2197824 -4.2290416 -4.243876 -4.2595162][-4.2367611 -4.2800837 -4.2911963 -4.2655525 -4.212707 -4.1495376 -4.1022792 -4.102942 -4.1418266 -4.19679 -4.230906 -4.2370925 -4.23625 -4.2425375 -4.2518606][-4.2793221 -4.3128042 -4.3093014 -4.2655191 -4.1976557 -4.1296272 -4.0942278 -4.1123176 -4.1567583 -4.2114015 -4.2479978 -4.2569194 -4.2553086 -4.25475 -4.2545977]]...]
INFO - root - 2017-12-07 17:39:20.235222: step 35710, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 54h:41m:32s remains)
INFO - root - 2017-12-07 17:39:27.093571: step 35720, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 53h:05m:04s remains)
INFO - root - 2017-12-07 17:39:33.942647: step 35730, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 53h:36m:45s remains)
INFO - root - 2017-12-07 17:39:40.794701: step 35740, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 60h:00m:23s remains)
INFO - root - 2017-12-07 17:39:47.554793: step 35750, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 58h:28m:33s remains)
INFO - root - 2017-12-07 17:39:54.381880: step 35760, loss = 2.04, batch loss = 1.98 (13.0 examples/sec; 0.617 sec/batch; 50h:53m:15s remains)
INFO - root - 2017-12-07 17:40:01.127152: step 35770, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 51h:36m:30s remains)
INFO - root - 2017-12-07 17:40:07.864917: step 35780, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 53h:24m:52s remains)
INFO - root - 2017-12-07 17:40:14.728560: step 35790, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 59h:58m:31s remains)
INFO - root - 2017-12-07 17:40:21.402387: step 35800, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 59h:01m:00s remains)
2017-12-07 17:40:22.176089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.08156 -4.1424742 -4.225812 -4.2814326 -4.3167911 -4.3407078 -4.3331256 -4.2851782 -4.2032242 -4.1267886 -4.081883 -4.0928645 -4.1469574 -4.187047 -4.2342219][-4.15681 -4.2063346 -4.2685075 -4.3056512 -4.3215079 -4.329102 -4.3145471 -4.2789073 -4.2415085 -4.22109 -4.2194715 -4.2351327 -4.2583089 -4.2740955 -4.298635][-4.2264843 -4.2649703 -4.31143 -4.3322692 -4.3261023 -4.3068705 -4.2790475 -4.250227 -4.2394714 -4.2471442 -4.270412 -4.2994375 -4.3180547 -4.3241324 -4.3357711][-4.271524 -4.3040924 -4.3399258 -4.3528576 -4.3284717 -4.2810016 -4.2254834 -4.1923914 -4.20462 -4.239306 -4.2795691 -4.3166785 -4.3426147 -4.3477364 -4.348165][-4.309237 -4.3315005 -4.354116 -4.3481774 -4.2982841 -4.2135963 -4.1177998 -4.068944 -4.1116066 -4.1894941 -4.2549558 -4.3049583 -4.3409553 -4.3514695 -4.3493948][-4.3365746 -4.3466377 -4.3507466 -4.3142595 -4.2199035 -4.0861855 -3.93155 -3.8461249 -3.9253325 -4.07747 -4.1901126 -4.2629962 -4.3152218 -4.3385916 -4.3447485][-4.3446097 -4.3436852 -4.3277326 -4.2565184 -4.1138911 -3.9259322 -3.7032557 -3.5541844 -3.654722 -3.8904285 -4.0710688 -4.1831412 -4.26317 -4.309484 -4.3296409][-4.3354564 -4.3274536 -4.2987218 -4.2119236 -4.0586138 -3.8642478 -3.6090517 -3.3917751 -3.454829 -3.7195969 -3.9434633 -4.0847111 -4.1872573 -4.2577267 -4.295207][-4.3327236 -4.3251157 -4.2929573 -4.2151332 -4.0974159 -3.9551251 -3.7596819 -3.5661492 -3.5497966 -3.715034 -3.9039221 -4.0384789 -4.145153 -4.2225037 -4.2682714][-4.330339 -4.3285875 -4.301475 -4.2403626 -4.164928 -4.0829158 -3.96708 -3.8520417 -3.8119762 -3.8715451 -3.9876509 -4.0858612 -4.1692162 -4.2347136 -4.2745733][-4.3231053 -4.3271184 -4.3106766 -4.2665277 -4.2204814 -4.1780677 -4.1177006 -4.0591235 -4.0234885 -4.0265031 -4.0870914 -4.1549206 -4.2129831 -4.262289 -4.2898207][-4.3093114 -4.3207951 -4.3207626 -4.2987928 -4.2732253 -4.2548232 -4.2240276 -4.1849952 -4.1449566 -4.1158624 -4.1315279 -4.1729674 -4.2200961 -4.2657347 -4.2928748][-4.293653 -4.3113136 -4.3243017 -4.3222303 -4.3158321 -4.3122139 -4.2937732 -4.2559776 -4.203743 -4.147429 -4.1220546 -4.1365857 -4.1776986 -4.2308168 -4.2710929][-4.28259 -4.3045716 -4.326746 -4.338058 -4.3440452 -4.3458371 -4.3317094 -4.2942214 -4.23162 -4.1463675 -4.0816011 -4.0666637 -4.0985436 -4.1633973 -4.226944][-4.2806921 -4.29783 -4.317204 -4.329618 -4.3390689 -4.3449039 -4.3381376 -4.3074827 -4.2440209 -4.1476164 -4.058969 -4.0186129 -4.0321741 -4.0999408 -4.1854973]]...]
INFO - root - 2017-12-07 17:40:29.036911: step 35810, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 59h:45m:37s remains)
INFO - root - 2017-12-07 17:40:35.955748: step 35820, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.748 sec/batch; 61h:37m:44s remains)
INFO - root - 2017-12-07 17:40:42.832572: step 35830, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 56h:58m:59s remains)
INFO - root - 2017-12-07 17:40:49.584328: step 35840, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.648 sec/batch; 53h:21m:38s remains)
INFO - root - 2017-12-07 17:40:56.402140: step 35850, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 54h:45m:33s remains)
INFO - root - 2017-12-07 17:41:03.375522: step 35860, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.711 sec/batch; 58h:36m:27s remains)
INFO - root - 2017-12-07 17:41:10.198662: step 35870, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 56h:54m:01s remains)
INFO - root - 2017-12-07 17:41:17.044801: step 35880, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 53h:14m:09s remains)
INFO - root - 2017-12-07 17:41:23.795157: step 35890, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 52h:19m:38s remains)
INFO - root - 2017-12-07 17:41:30.305159: step 35900, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 60h:58m:25s remains)
2017-12-07 17:41:31.024753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1828651 -4.1863146 -4.1839709 -4.185194 -4.1966252 -4.2058597 -4.1966743 -4.1817718 -4.1722393 -4.1955519 -4.2186956 -4.2302666 -4.2361727 -4.2300234 -4.2067537][-4.1819315 -4.1871228 -4.1820588 -4.1732841 -4.1757116 -4.181931 -4.1769376 -4.1669493 -4.1619296 -4.186882 -4.2213659 -4.2442584 -4.2539668 -4.2426271 -4.2112417][-4.1875148 -4.186306 -4.1804953 -4.164588 -4.1568375 -4.1557803 -4.1551805 -4.1553755 -4.1567788 -4.1778207 -4.2167673 -4.2537179 -4.273407 -4.2655983 -4.2308416][-4.2052512 -4.1883016 -4.1740136 -4.1549978 -4.1349874 -4.123486 -4.125361 -4.1378345 -4.1489487 -4.1699929 -4.2136655 -4.258533 -4.2841067 -4.2840056 -4.2531443][-4.2310176 -4.1947303 -4.1641889 -4.1415048 -4.1127558 -4.0898366 -4.0883169 -4.1069779 -4.1278453 -4.1586428 -4.2056403 -4.2502375 -4.2801471 -4.2889 -4.268683][-4.2529626 -4.2065067 -4.1632652 -4.1357055 -4.0998487 -4.0610976 -4.0363021 -4.043726 -4.0735478 -4.1236672 -4.1846695 -4.2324467 -4.2653909 -4.2790174 -4.2689652][-4.2691641 -4.2243004 -4.1789618 -4.1453791 -4.098649 -4.0364075 -3.9708173 -3.9447081 -3.9806194 -4.0670519 -4.1542525 -4.2112961 -4.2472892 -4.2653289 -4.2662916][-4.2751555 -4.241025 -4.203485 -4.162962 -4.100709 -4.0188847 -3.9193296 -3.8570802 -3.8966637 -4.02194 -4.1292014 -4.1920915 -4.229424 -4.2459793 -4.2524209][-4.2631912 -4.2455864 -4.2181997 -4.174078 -4.100843 -4.0132113 -3.9178834 -3.8614163 -3.9037943 -4.0217371 -4.1154857 -4.1733623 -4.208343 -4.2201033 -4.2277517][-4.2297168 -4.2261987 -4.2086291 -4.1699953 -4.1016407 -4.0287275 -3.9645929 -3.9343967 -3.964721 -4.0382876 -4.101542 -4.1538444 -4.1882682 -4.1992683 -4.2106986][-4.1838975 -4.1900773 -4.1829333 -4.1607656 -4.1169705 -4.0678649 -4.029521 -4.0097609 -4.0169511 -4.0487304 -4.0890293 -4.1410704 -4.1745305 -4.1869283 -4.1979361][-4.1492262 -4.1551456 -4.154366 -4.14508 -4.1231656 -4.0956979 -4.0739312 -4.0590043 -4.0501089 -4.054791 -4.0861416 -4.13802 -4.1665564 -4.1715446 -4.1710072][-4.1396775 -4.1390924 -4.1367884 -4.1278563 -4.1131172 -4.1022353 -4.097868 -4.0893607 -4.0774035 -4.0779819 -4.1071267 -4.1445594 -4.1549788 -4.1440759 -4.1329665][-4.1564054 -4.1488204 -4.1372666 -4.1172647 -4.0994372 -4.0987654 -4.1104059 -4.1135964 -4.1107669 -4.11558 -4.137805 -4.1499305 -4.1390138 -4.1166644 -4.1029077][-4.1793685 -4.1690078 -4.1509004 -4.1224947 -4.1068735 -4.1145444 -4.1364737 -4.1516981 -4.1587238 -4.161952 -4.1660247 -4.152597 -4.1252208 -4.1012235 -4.0941086]]...]
INFO - root - 2017-12-07 17:41:37.846023: step 35910, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.637 sec/batch; 52h:31m:07s remains)
INFO - root - 2017-12-07 17:41:44.722745: step 35920, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 55h:36m:30s remains)
INFO - root - 2017-12-07 17:41:51.671588: step 35930, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.755 sec/batch; 62h:12m:51s remains)
INFO - root - 2017-12-07 17:41:58.495641: step 35940, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 58h:34m:22s remains)
INFO - root - 2017-12-07 17:42:05.316410: step 35950, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 57h:01m:22s remains)
INFO - root - 2017-12-07 17:42:12.031701: step 35960, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 53h:23m:01s remains)
INFO - root - 2017-12-07 17:42:18.888251: step 35970, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 52h:42m:13s remains)
INFO - root - 2017-12-07 17:42:25.821034: step 35980, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 60h:21m:03s remains)
INFO - root - 2017-12-07 17:42:32.578243: step 35990, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 58h:34m:28s remains)
INFO - root - 2017-12-07 17:42:39.267701: step 36000, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 51h:28m:24s remains)
2017-12-07 17:42:40.105845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654204 -4.2644625 -4.253468 -4.2417526 -4.242197 -4.2496767 -4.2509155 -4.2378 -4.2261891 -4.2171607 -4.20858 -4.2218342 -4.2430205 -4.2612581 -4.2699957][-4.2598062 -4.253756 -4.2381916 -4.2206769 -4.21772 -4.2250662 -4.2313557 -4.2274747 -4.2306843 -4.2337871 -4.2271819 -4.235311 -4.2492962 -4.2610369 -4.2648416][-4.2609463 -4.2553263 -4.2363906 -4.2108841 -4.1960163 -4.1919155 -4.1956906 -4.1965318 -4.2128997 -4.2325473 -4.236794 -4.2458339 -4.2545066 -4.2629523 -4.2655568][-4.2482882 -4.2563658 -4.2424979 -4.2149153 -4.1812696 -4.1531992 -4.1448007 -4.1433129 -4.1695218 -4.2086682 -4.2295833 -4.2455287 -4.2532239 -4.2590737 -4.2622223][-4.2241 -4.2519751 -4.2500019 -4.2258372 -4.1791248 -4.1245232 -4.0900989 -4.0774531 -4.1126785 -4.1737223 -4.2135315 -4.2402191 -4.2516084 -4.2559514 -4.257678][-4.19791 -4.2435112 -4.2581072 -4.2383747 -4.1837163 -4.103075 -4.0264435 -3.9904749 -4.0356021 -4.1237769 -4.1827536 -4.2211027 -4.2413154 -4.2496233 -4.2532887][-4.1691904 -4.2297726 -4.2618408 -4.2459993 -4.1831446 -4.0744133 -3.9437838 -3.871062 -3.9387743 -4.0637288 -4.1430821 -4.1932259 -4.22605 -4.2420406 -4.24947][-4.1416011 -4.2108774 -4.2580953 -4.2483554 -4.1845632 -4.06122 -3.8847239 -3.7672229 -3.8583727 -4.0117888 -4.1057792 -4.1677523 -4.2114377 -4.2372866 -4.2480216][-4.1497512 -4.215066 -4.2627664 -4.2588797 -4.2072949 -4.1057448 -3.9489713 -3.8339972 -3.8955498 -4.0162997 -4.0906358 -4.146781 -4.1949883 -4.2304907 -4.2469325][-4.1883993 -4.2376556 -4.2748203 -4.2716565 -4.24019 -4.17582 -4.0767365 -3.998189 -4.0089264 -4.0661478 -4.0986829 -4.1302671 -4.1714559 -4.2133784 -4.239408][-4.2056546 -4.2431459 -4.2732935 -4.2739496 -4.2563295 -4.2180557 -4.1603069 -4.1048107 -4.0856552 -4.0964208 -4.0963287 -4.1061096 -4.1426282 -4.190114 -4.2260242][-4.2128973 -4.2417321 -4.266253 -4.2666531 -4.254415 -4.2322183 -4.1970897 -4.1521058 -4.1126885 -4.0880032 -4.067112 -4.0719318 -4.1100707 -4.1606083 -4.203187][-4.22524 -4.2448745 -4.2605548 -4.2600522 -4.2491436 -4.2384987 -4.2166648 -4.1757307 -4.1293817 -4.086143 -4.0559521 -4.05778 -4.0895267 -4.1327186 -4.1787567][-4.2272129 -4.2449455 -4.2582703 -4.2614112 -4.2552118 -4.2530971 -4.2396307 -4.20394 -4.1609979 -4.1154461 -4.089643 -4.0944 -4.1163769 -4.1437678 -4.1828871][-4.2217479 -4.2402782 -4.2563138 -4.2632546 -4.2645617 -4.2700396 -4.2640085 -4.2339845 -4.1971397 -4.161737 -4.1479778 -4.1592827 -4.1732063 -4.1860394 -4.2126989]]...]
INFO - root - 2017-12-07 17:42:46.865799: step 36010, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 59h:31m:13s remains)
INFO - root - 2017-12-07 17:42:53.562378: step 36020, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 55h:47m:38s remains)
INFO - root - 2017-12-07 17:43:00.334703: step 36030, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 51h:28m:19s remains)
INFO - root - 2017-12-07 17:43:07.231747: step 36040, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 54h:41m:08s remains)
INFO - root - 2017-12-07 17:43:14.107476: step 36050, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 60h:04m:38s remains)
INFO - root - 2017-12-07 17:43:20.897622: step 36060, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 56h:52m:38s remains)
INFO - root - 2017-12-07 17:43:27.768644: step 36070, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 51h:57m:21s remains)
INFO - root - 2017-12-07 17:43:34.568606: step 36080, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 56h:17m:41s remains)
INFO - root - 2017-12-07 17:43:41.428306: step 36090, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 58h:23m:53s remains)
INFO - root - 2017-12-07 17:43:48.093411: step 36100, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 60h:01m:30s remains)
2017-12-07 17:43:48.808197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2346277 -4.24624 -4.2587013 -4.2618489 -4.2560596 -4.251225 -4.2548108 -4.2673635 -4.2787766 -4.2882724 -4.2995119 -4.3080511 -4.314877 -4.3161111 -4.3072858][-4.1749496 -4.1920409 -4.2107534 -4.2150149 -4.2054443 -4.1972971 -4.2015944 -4.2221317 -4.2431517 -4.2602015 -4.2784557 -4.2947807 -4.3081203 -4.3116364 -4.3002067][-4.1443558 -4.1644979 -4.1835957 -4.1843238 -4.1702394 -4.1580644 -4.1591206 -4.1818066 -4.2130914 -4.2390203 -4.26066 -4.2808509 -4.3003306 -4.3081346 -4.297586][-4.1364055 -4.1525612 -4.169445 -4.1650081 -4.1451912 -4.1297436 -4.1255584 -4.1434627 -4.1811032 -4.2170267 -4.2436523 -4.2665277 -4.2896762 -4.3005624 -4.2935791][-4.1412997 -4.1530218 -4.1681881 -4.1621461 -4.1382618 -4.1166248 -4.1030674 -4.1111965 -4.1492844 -4.1928234 -4.22416 -4.2481027 -4.2740855 -4.2897811 -4.2885232][-4.16765 -4.1738753 -4.1861892 -4.1804271 -4.1548176 -4.1239018 -4.09589 -4.0915413 -4.1265016 -4.1755219 -4.2102637 -4.2351346 -4.2639966 -4.28381 -4.2885141][-4.2079668 -4.203743 -4.2060442 -4.19906 -4.1729116 -4.1343155 -4.0915389 -4.0722857 -4.10032 -4.1564283 -4.2010374 -4.2300973 -4.2608395 -4.2829113 -4.2918577][-4.2542524 -4.2375579 -4.2262549 -4.216321 -4.19367 -4.1518192 -4.0953426 -4.0556068 -4.0733805 -4.1324229 -4.1891952 -4.2245927 -4.2567139 -4.2813721 -4.2928796][-4.2802711 -4.2594442 -4.2441192 -4.2350464 -4.2169833 -4.1738062 -4.1104994 -4.0592041 -4.0674891 -4.1233039 -4.1840296 -4.2212162 -4.2511425 -4.2759027 -4.2883477][-4.2803774 -4.2604775 -4.2482233 -4.2436147 -4.2316861 -4.1923952 -4.1297026 -4.0785828 -4.0797052 -4.126864 -4.1818504 -4.2119265 -4.2355165 -4.2571435 -4.27057][-4.2719331 -4.2569323 -4.2493587 -4.246738 -4.2392778 -4.2089028 -4.1578455 -4.1148896 -4.1102967 -4.1443167 -4.1849174 -4.2016883 -4.2150121 -4.2293415 -4.2422943][-4.2559118 -4.2484493 -4.2464442 -4.2440963 -4.2407942 -4.21987 -4.1827607 -4.1465716 -4.137579 -4.1588321 -4.1845851 -4.1890435 -4.1933737 -4.2031593 -4.2169666][-4.2378397 -4.2372069 -4.2416897 -4.2392826 -4.2344556 -4.2202682 -4.1924391 -4.1580234 -4.1443958 -4.1586704 -4.1728406 -4.1703935 -4.1737161 -4.1853819 -4.2030454][-4.2185097 -4.2243605 -4.2351956 -4.2334819 -4.2255278 -4.2130914 -4.1896715 -4.1582556 -4.13963 -4.148674 -4.1569247 -4.1536913 -4.1589112 -4.1763716 -4.2015038][-4.2057509 -4.2221651 -4.2428422 -4.2440367 -4.2319231 -4.2153525 -4.1905923 -4.158051 -4.1349421 -4.1374264 -4.1457267 -4.1476903 -4.1551652 -4.1795206 -4.2102842]]...]
INFO - root - 2017-12-07 17:43:55.550230: step 36110, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 53h:43m:02s remains)
INFO - root - 2017-12-07 17:44:02.401137: step 36120, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 55h:36m:26s remains)
INFO - root - 2017-12-07 17:44:09.175070: step 36130, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 58h:44m:12s remains)
INFO - root - 2017-12-07 17:44:15.918907: step 36140, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 56h:41m:08s remains)
INFO - root - 2017-12-07 17:44:22.745224: step 36150, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 55h:15m:08s remains)
INFO - root - 2017-12-07 17:44:29.437133: step 36160, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 52h:46m:02s remains)
INFO - root - 2017-12-07 17:44:36.210075: step 36170, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 56h:24m:19s remains)
INFO - root - 2017-12-07 17:44:43.054419: step 36180, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 61h:00m:58s remains)
INFO - root - 2017-12-07 17:44:49.745805: step 36190, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 53h:43m:13s remains)
INFO - root - 2017-12-07 17:44:56.353828: step 36200, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 50h:59m:08s remains)
2017-12-07 17:44:57.022120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2597451 -4.2756262 -4.2729425 -4.2441883 -4.1834164 -4.0791845 -3.9590111 -3.8964427 -3.9546273 -4.0657821 -4.1734214 -4.2595024 -4.313623 -4.3392725 -4.3483944][-4.2721043 -4.2817731 -4.2720418 -4.2354302 -4.1655021 -4.0458522 -3.9072051 -3.8312995 -3.8971217 -4.0229349 -4.14651 -4.245357 -4.30735 -4.337306 -4.3481541][-4.2745471 -4.274385 -4.2595544 -4.2245746 -4.1588206 -4.0455461 -3.912338 -3.8388674 -3.9026783 -4.0247016 -4.1477528 -4.2471437 -4.3089161 -4.3388381 -4.3486972][-4.2724128 -4.2615089 -4.2409053 -4.2116981 -4.156395 -4.0612831 -3.9493198 -3.8924792 -3.9508677 -4.0598774 -4.1698728 -4.2595291 -4.316174 -4.3420982 -4.3493843][-4.2706103 -4.251883 -4.2264471 -4.2011318 -4.1557093 -4.0764279 -3.9797194 -3.9375505 -3.9928119 -4.0943737 -4.194159 -4.2737823 -4.3235636 -4.3447108 -4.3499589][-4.269393 -4.2506757 -4.2264218 -4.2041106 -4.1613574 -4.0870757 -3.993206 -3.9544363 -4.0104966 -4.1110044 -4.2080321 -4.283052 -4.3287959 -4.3464556 -4.3500857][-4.2675395 -4.2520466 -4.2323961 -4.2114196 -4.166853 -4.0892406 -3.9906161 -3.9500086 -4.0094891 -4.1118307 -4.2100854 -4.2854638 -4.3313608 -4.3475184 -4.3501692][-4.265738 -4.2551122 -4.2424588 -4.2222772 -4.1727095 -4.0869589 -3.9798973 -3.9345756 -3.9971168 -4.10297 -4.2047024 -4.2840905 -4.3317237 -4.3477697 -4.35015][-4.2652903 -4.2598791 -4.2525225 -4.2329097 -4.1775746 -4.082706 -3.9674761 -3.9141736 -3.979068 -4.08973 -4.1958842 -4.2800808 -4.3299165 -4.3471584 -4.3501072][-4.2652025 -4.2621984 -4.2589593 -4.2405844 -4.1840792 -4.0870457 -3.9701467 -3.9137444 -3.9780626 -4.0874009 -4.1939311 -4.2792935 -4.3291368 -4.3473039 -4.3504319][-4.2651219 -4.2636237 -4.26398 -4.2480068 -4.1954923 -4.1039324 -3.9925685 -3.9394255 -3.9988379 -4.0983496 -4.19931 -4.2818923 -4.330049 -4.3477712 -4.3505049][-4.2623658 -4.2622981 -4.2643585 -4.2516246 -4.206224 -4.1235886 -4.0194335 -3.9702685 -4.0224862 -4.1109805 -4.205687 -4.2852745 -4.3318615 -4.3484874 -4.3504176][-4.2491322 -4.2498274 -4.2501812 -4.239584 -4.2017369 -4.1265869 -4.0259266 -3.9777298 -4.0263243 -4.1112213 -4.2068825 -4.2871222 -4.3334045 -4.3490252 -4.3502951][-4.2305045 -4.2323713 -4.2296047 -4.2198591 -4.1872759 -4.1159678 -4.01526 -3.9691067 -4.0178952 -4.1054268 -4.2067547 -4.2891855 -4.333899 -4.3486166 -4.3501592][-4.2219958 -4.2257667 -4.2215209 -4.2133665 -4.1841187 -4.1151781 -4.0178843 -3.9772627 -4.0271254 -4.1154866 -4.2152934 -4.2949042 -4.334765 -4.3471785 -4.3493104]]...]
INFO - root - 2017-12-07 17:45:03.685761: step 36210, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.693 sec/batch; 57h:02m:32s remains)
INFO - root - 2017-12-07 17:45:10.444776: step 36220, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 55h:30m:24s remains)
INFO - root - 2017-12-07 17:45:17.258705: step 36230, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 51h:22m:44s remains)
INFO - root - 2017-12-07 17:45:24.028518: step 36240, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.706 sec/batch; 58h:05m:02s remains)
INFO - root - 2017-12-07 17:45:30.991708: step 36250, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 61h:01m:09s remains)
INFO - root - 2017-12-07 17:45:37.811340: step 36260, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 54h:20m:14s remains)
INFO - root - 2017-12-07 17:45:44.561163: step 36270, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 52h:43m:55s remains)
INFO - root - 2017-12-07 17:45:51.345031: step 36280, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 60h:04m:34s remains)
INFO - root - 2017-12-07 17:45:58.151061: step 36290, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 57h:06m:52s remains)
INFO - root - 2017-12-07 17:46:04.801957: step 36300, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 56h:23m:39s remains)
2017-12-07 17:46:05.579720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3344264 -4.3271818 -4.3219643 -4.3181376 -4.3147745 -4.3130784 -4.3131332 -4.3128333 -4.3122106 -4.310039 -4.3080735 -4.3080177 -4.3107963 -4.3115211 -4.316493][-4.323997 -4.3096709 -4.2984409 -4.2918591 -4.28662 -4.2825336 -4.281559 -4.2805018 -4.2792797 -4.2729726 -4.2671323 -4.2686834 -4.2769928 -4.2827044 -4.2953973][-4.3074594 -4.2842298 -4.2654181 -4.2542577 -4.2480354 -4.242372 -4.2364106 -4.23037 -4.2259569 -4.2162576 -4.2102737 -4.2181759 -4.2372913 -4.2532091 -4.2752743][-4.2879381 -4.25504 -4.2291269 -4.2147269 -4.2103515 -4.2046804 -4.1898017 -4.174027 -4.168695 -4.1634583 -4.1629939 -4.1787224 -4.2090049 -4.2358479 -4.2625127][-4.2644634 -4.2236977 -4.1939378 -4.1816044 -4.1829891 -4.1773877 -4.1509447 -4.1248593 -4.1268721 -4.139154 -4.1472688 -4.165504 -4.1996927 -4.2314439 -4.2587724][-4.2398334 -4.1939507 -4.1637044 -4.1547627 -4.1590061 -4.1465788 -4.1038566 -4.068552 -4.08504 -4.1236281 -4.1432667 -4.1561818 -4.1862807 -4.2191606 -4.2451844][-4.2161522 -4.1660542 -4.1337538 -4.1255922 -4.1261349 -4.1013055 -4.0370884 -3.9881325 -4.0249319 -4.094893 -4.131773 -4.142694 -4.1644392 -4.1976919 -4.2237306][-4.1923547 -4.1373429 -4.1015291 -4.0910554 -4.081655 -4.0375938 -3.9432352 -3.8747787 -3.9368463 -4.0492425 -4.113565 -4.1309695 -4.1484 -4.180407 -4.2072048][-4.1801729 -4.1259174 -4.0918808 -4.0833511 -4.0721626 -4.0235534 -3.9202337 -3.8458245 -3.9223261 -4.0572276 -4.1317229 -4.1500177 -4.1615782 -4.1906943 -4.2140803][-4.18495 -4.1374207 -4.1080613 -4.1023331 -4.0992031 -4.0699525 -4.0007277 -3.9487209 -4.0062532 -4.1256037 -4.1956882 -4.2109203 -4.2163749 -4.2364817 -4.2532372][-4.2028537 -4.1658 -4.142127 -4.1358442 -4.1372972 -4.1264791 -4.0942039 -4.0704055 -4.1082706 -4.1965232 -4.2557144 -4.2679415 -4.2736111 -4.2896671 -4.2989354][-4.2228327 -4.195003 -4.1757216 -4.1670704 -4.1702609 -4.1739483 -4.1674166 -4.1634445 -4.1898904 -4.2491951 -4.2943645 -4.3053994 -4.3134503 -4.3299284 -4.3331618][-4.2463078 -4.2274203 -4.2112942 -4.2010126 -4.2044363 -4.2168541 -4.2217464 -4.2244873 -4.2403297 -4.2726603 -4.3036141 -4.3159013 -4.3299932 -4.3503137 -4.3520117][-4.2716322 -4.2572103 -4.2424345 -4.2309842 -4.2328968 -4.2475467 -4.258256 -4.2626209 -4.2676673 -4.2803264 -4.297904 -4.3101873 -4.3273983 -4.3477426 -4.351037][-4.2985849 -4.2883654 -4.2768378 -4.2662158 -4.2647147 -4.2726679 -4.2796817 -4.2809358 -4.2791905 -4.2814517 -4.2878246 -4.2974839 -4.3131518 -4.3302765 -4.3369985]]...]
INFO - root - 2017-12-07 17:46:12.290646: step 36310, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 58h:19m:41s remains)
INFO - root - 2017-12-07 17:46:19.106264: step 36320, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 59h:29m:52s remains)
INFO - root - 2017-12-07 17:46:25.799678: step 36330, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 56h:00m:46s remains)
INFO - root - 2017-12-07 17:46:32.573823: step 36340, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 54h:04m:58s remains)
INFO - root - 2017-12-07 17:46:39.345146: step 36350, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.640 sec/batch; 52h:39m:11s remains)
INFO - root - 2017-12-07 17:46:46.162088: step 36360, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 58h:53m:00s remains)
INFO - root - 2017-12-07 17:46:52.935117: step 36370, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 58h:55m:33s remains)
INFO - root - 2017-12-07 17:46:59.655512: step 36380, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 55h:17m:55s remains)
INFO - root - 2017-12-07 17:47:06.387248: step 36390, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 52h:35m:17s remains)
INFO - root - 2017-12-07 17:47:12.918662: step 36400, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 52h:09m:20s remains)
2017-12-07 17:47:13.688784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2570848 -4.2188807 -4.1899204 -4.1839008 -4.2000775 -4.2165961 -4.2271624 -4.2239404 -4.2034979 -4.1730733 -4.1447864 -4.1368117 -4.1562757 -4.1921048 -4.2154746][-4.2637215 -4.222405 -4.1889629 -4.1816263 -4.1998954 -4.2178907 -4.2258768 -4.22257 -4.2074084 -4.1847539 -4.1567698 -4.138 -4.1411972 -4.1730113 -4.2051086][-4.2673254 -4.2300992 -4.1984606 -4.1895151 -4.2047954 -4.2205882 -4.2268658 -4.2274952 -4.225132 -4.22006 -4.2058415 -4.1881614 -4.180337 -4.197175 -4.2228379][-4.27125 -4.2427464 -4.2174592 -4.2060032 -4.2097821 -4.2129321 -4.21099 -4.2136664 -4.2241712 -4.238975 -4.2461028 -4.2433071 -4.2387576 -4.2477126 -4.263505][-4.2745929 -4.2552156 -4.2381167 -4.2253466 -4.2147474 -4.1967888 -4.1782808 -4.1757088 -4.1923332 -4.2209959 -4.2422309 -4.2519379 -4.2556672 -4.2639589 -4.2807031][-4.2745237 -4.2621117 -4.2539353 -4.2425942 -4.2211261 -4.1807904 -4.1352549 -4.111062 -4.1206765 -4.1582961 -4.1907568 -4.2099309 -4.2221127 -4.2358112 -4.2606378][-4.2663455 -4.2574291 -4.2548013 -4.2457633 -4.2171388 -4.1623974 -4.0912061 -4.0339622 -4.0204105 -4.0565333 -4.0937562 -4.1149035 -4.1291537 -4.147531 -4.1865568][-4.2627158 -4.2514944 -4.2507529 -4.2463832 -4.2204442 -4.1681862 -4.0922585 -4.0161924 -3.9742539 -3.9933183 -4.02258 -4.0374589 -4.0414863 -4.0554118 -4.1013837][-4.2791758 -4.2662935 -4.2651377 -4.2664242 -4.25404 -4.22204 -4.1702418 -4.1110225 -4.0675316 -4.0670414 -4.0799437 -4.0855579 -4.0819983 -4.0830932 -4.1131167][-4.300539 -4.288609 -4.2853155 -4.2897434 -4.2911658 -4.2811141 -4.2565055 -4.2226753 -4.1908135 -4.1824307 -4.1862903 -4.1902146 -4.1887016 -4.1856484 -4.1958003][-4.3063874 -4.2964339 -4.2915168 -4.2969913 -4.3082623 -4.3126831 -4.3071089 -4.2906551 -4.2680168 -4.25777 -4.2589593 -4.2646408 -4.2655568 -4.2612524 -4.2605863][-4.2958379 -4.2866311 -4.2796569 -4.2843618 -4.2982106 -4.3075142 -4.3093324 -4.2998371 -4.2798643 -4.2666597 -4.2649922 -4.2709675 -4.2750974 -4.2738857 -4.270052][-4.2724395 -4.260838 -4.2475543 -4.2472415 -4.2618747 -4.2745266 -4.2778 -4.2687654 -4.2496543 -4.2364964 -4.23095 -4.23508 -4.2412133 -4.2420626 -4.2389297][-4.2560277 -4.2373238 -4.2103195 -4.19901 -4.2116423 -4.2288322 -4.2365003 -4.2281914 -4.2118278 -4.20011 -4.1926479 -4.1961341 -4.2046881 -4.206666 -4.2041278][-4.2628632 -4.2375021 -4.2003026 -4.1789556 -4.1865873 -4.205442 -4.2181859 -4.2144117 -4.2042093 -4.1954255 -4.1860313 -4.1855159 -4.1918087 -4.1935415 -4.1914349]]...]
INFO - root - 2017-12-07 17:47:20.443089: step 36410, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 55h:12m:30s remains)
INFO - root - 2017-12-07 17:47:27.231631: step 36420, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 53h:05m:41s remains)
INFO - root - 2017-12-07 17:47:33.965318: step 36430, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 53h:18m:48s remains)
INFO - root - 2017-12-07 17:47:40.831673: step 36440, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.748 sec/batch; 61h:31m:09s remains)
INFO - root - 2017-12-07 17:47:47.566121: step 36450, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 57h:53m:49s remains)
INFO - root - 2017-12-07 17:47:54.403343: step 36460, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 56h:44m:49s remains)
INFO - root - 2017-12-07 17:48:01.134272: step 36470, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 52h:12m:36s remains)
INFO - root - 2017-12-07 17:48:07.902609: step 36480, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 54h:48m:05s remains)
INFO - root - 2017-12-07 17:48:14.704181: step 36490, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 58h:38m:56s remains)
INFO - root - 2017-12-07 17:48:21.262889: step 36500, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 59h:34m:33s remains)
2017-12-07 17:48:21.977700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2490668 -4.2575884 -4.2773175 -4.2967992 -4.3089929 -4.3087831 -4.2837429 -4.2474413 -4.2344365 -4.2473025 -4.2770534 -4.3043761 -4.3270445 -4.3420296 -4.3480287][-4.2295456 -4.2442112 -4.2704396 -4.2949324 -4.3115721 -4.3147607 -4.2918 -4.2553363 -4.2416544 -4.2530427 -4.2809634 -4.3062692 -4.3272915 -4.3414879 -4.3470573][-4.20832 -4.2234545 -4.2525783 -4.279614 -4.2984848 -4.3034434 -4.28232 -4.2490711 -4.2394705 -4.2544885 -4.2832084 -4.3071537 -4.3267264 -4.3405256 -4.3456197][-4.1773419 -4.1873107 -4.216136 -4.2444916 -4.26185 -4.26619 -4.2434545 -4.2135506 -4.2147141 -4.2414165 -4.27429 -4.2986431 -4.3197823 -4.3358874 -4.34154][-4.13702 -4.1403251 -4.1674786 -4.1978769 -4.2142434 -4.2121682 -4.1789989 -4.1481605 -4.1659636 -4.20923 -4.247478 -4.2740035 -4.3016977 -4.3233657 -4.3315248][-4.0985489 -4.0914245 -4.1124225 -4.1410413 -4.15622 -4.143734 -4.0942059 -4.0587463 -4.0955467 -4.1586666 -4.2061424 -4.2405276 -4.2788396 -4.3072796 -4.3179946][-4.0663304 -4.0492883 -4.0595331 -4.0820704 -4.0930934 -4.0678048 -4.0013433 -3.9602838 -4.0169735 -4.1058764 -4.1703329 -4.2170806 -4.2648935 -4.296454 -4.3073411][-4.0615048 -4.0380645 -4.0387325 -4.051477 -4.052762 -4.0153189 -3.9430895 -3.9063222 -3.9762492 -4.0782261 -4.1549535 -4.214572 -4.268816 -4.3002229 -4.3081045][-4.1022496 -4.0829434 -4.0809293 -4.0833621 -4.07424 -4.0375538 -3.9834344 -3.9618375 -4.0209174 -4.1043205 -4.1730952 -4.2323432 -4.2832427 -4.3118038 -4.31605][-4.1525722 -4.1415496 -4.1414208 -4.135788 -4.1191616 -4.0906591 -4.063365 -4.0546641 -4.0905418 -4.1414847 -4.1921654 -4.2429724 -4.2879848 -4.31546 -4.3201694][-4.174757 -4.16829 -4.1676517 -4.1568632 -4.1380672 -4.1177192 -4.107655 -4.10501 -4.1206841 -4.1489325 -4.1889391 -4.2346673 -4.2779326 -4.306242 -4.3152714][-4.1635838 -4.1576843 -4.1556153 -4.1432261 -4.1258168 -4.1129975 -4.1094909 -4.1043282 -4.1099648 -4.1313148 -4.1702724 -4.2168789 -4.2606292 -4.2900486 -4.3045931][-4.1281385 -4.1198606 -4.1165366 -4.1053257 -4.0904069 -4.0799575 -4.0777512 -4.072597 -4.0771718 -4.1017756 -4.1454649 -4.1962218 -4.2437887 -4.277637 -4.2969208][-4.0967951 -4.088171 -4.0884328 -4.0815535 -4.0681524 -4.0590296 -4.0590925 -4.0564718 -4.0640955 -4.0943856 -4.143342 -4.1962614 -4.2440915 -4.2790914 -4.2990351][-4.1190968 -4.1134329 -4.1160026 -4.11402 -4.1061673 -4.1014466 -4.1018672 -4.1004958 -4.1094241 -4.139236 -4.1835403 -4.2297 -4.267971 -4.2941861 -4.3087916]]...]
INFO - root - 2017-12-07 17:48:28.743277: step 36510, loss = 2.03, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 52h:33m:10s remains)
INFO - root - 2017-12-07 17:48:35.289871: step 36520, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 59h:46m:11s remains)
INFO - root - 2017-12-07 17:48:42.126500: step 36530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 54h:53m:40s remains)
INFO - root - 2017-12-07 17:48:48.917051: step 36540, loss = 2.06, batch loss = 2.01 (13.2 examples/sec; 0.608 sec/batch; 49h:57m:58s remains)
INFO - root - 2017-12-07 17:48:55.735965: step 36550, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 53h:01m:48s remains)
INFO - root - 2017-12-07 17:49:02.542016: step 36560, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 57h:19m:15s remains)
INFO - root - 2017-12-07 17:49:09.379185: step 36570, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 58h:45m:37s remains)
INFO - root - 2017-12-07 17:49:16.155507: step 36580, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 54h:16m:42s remains)
INFO - root - 2017-12-07 17:49:22.947537: step 36590, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 55h:19m:15s remains)
INFO - root - 2017-12-07 17:49:29.537118: step 36600, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 53h:15m:24s remains)
2017-12-07 17:49:30.258107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2142382 -4.2125874 -4.2093525 -4.2005253 -4.1948891 -4.193193 -4.1890292 -4.1860785 -4.180532 -4.1686211 -4.1576948 -4.157794 -4.1585727 -4.1614285 -4.1612005][-4.1870861 -4.1829543 -4.1861091 -4.1923656 -4.19895 -4.1994605 -4.197762 -4.1976485 -4.1878638 -4.1746635 -4.1665049 -4.1654229 -4.161027 -4.1547914 -4.1468015][-4.1863675 -4.1843543 -4.193017 -4.2098227 -4.2217979 -4.2194595 -4.2170529 -4.2210736 -4.2169156 -4.2063718 -4.1988873 -4.1926785 -4.1796041 -4.1648321 -4.1523862][-4.1935334 -4.1907086 -4.2007966 -4.2226844 -4.2387166 -4.2348809 -4.2295485 -4.2320986 -4.2297478 -4.2221889 -4.2138071 -4.2021675 -4.181211 -4.1613235 -4.149056][-4.2060957 -4.2004752 -4.2093635 -4.232635 -4.2484674 -4.2425508 -4.2269487 -4.2185373 -4.21661 -4.2170582 -4.2173181 -4.2091775 -4.1862473 -4.1632414 -4.147397][-4.2276497 -4.2184052 -4.2244854 -4.2443428 -4.2555456 -4.2441325 -4.21314 -4.1857867 -4.1797214 -4.1941633 -4.2095027 -4.2104445 -4.187665 -4.1592083 -4.1391463][-4.2347045 -4.2242403 -4.2276511 -4.2436118 -4.2491732 -4.2273679 -4.1768274 -4.1256466 -4.1117468 -4.1387682 -4.1696019 -4.1789169 -4.1586 -4.1289649 -4.1109686][-4.2194095 -4.2062063 -4.2085133 -4.2246733 -4.2293139 -4.2001414 -4.1358838 -4.0642781 -4.0397882 -4.0733519 -4.11893 -4.1441212 -4.1427379 -4.1267056 -4.1165185][-4.1867151 -4.1668973 -4.168746 -4.1924229 -4.207365 -4.1888027 -4.1314487 -4.0593371 -4.02914 -4.057519 -4.1048689 -4.1397362 -4.1542687 -4.1512957 -4.1458645][-4.153264 -4.1318369 -4.1374803 -4.1684184 -4.1942191 -4.1936646 -4.16164 -4.1102133 -4.0841565 -4.099226 -4.1290011 -4.155478 -4.1689715 -4.1690736 -4.162313][-4.1394196 -4.1251488 -4.1359105 -4.1635203 -4.1885343 -4.1961894 -4.1838322 -4.155952 -4.1390009 -4.1435075 -4.1579914 -4.1733232 -4.1795912 -4.1790962 -4.17479][-4.1435456 -4.1403966 -4.1526265 -4.1701941 -4.1854739 -4.1917291 -4.1893234 -4.1798921 -4.1732197 -4.1748409 -4.181891 -4.1894145 -4.1935053 -4.1964869 -4.199347][-4.1639352 -4.166461 -4.1789961 -4.1910262 -4.1990995 -4.2045121 -4.2095666 -4.2124057 -4.2143779 -4.2176628 -4.2223964 -4.2273197 -4.23325 -4.2398705 -4.246829][-4.2094259 -4.2131677 -4.2256861 -4.2368808 -4.2447743 -4.2520609 -4.25887 -4.2630973 -4.2660618 -4.2697029 -4.271759 -4.2744927 -4.2793465 -4.2851186 -4.2907052][-4.2586918 -4.2638307 -4.2745156 -4.2834277 -4.2909627 -4.2979341 -4.3020248 -4.30289 -4.3033786 -4.3041253 -4.3021789 -4.3015218 -4.3027906 -4.30602 -4.3101578]]...]
INFO - root - 2017-12-07 17:49:37.039376: step 36610, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 55h:09m:51s remains)
INFO - root - 2017-12-07 17:49:43.841309: step 36620, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 54h:07m:17s remains)
INFO - root - 2017-12-07 17:49:50.647542: step 36630, loss = 2.11, batch loss = 2.05 (12.8 examples/sec; 0.626 sec/batch; 51h:25m:10s remains)
INFO - root - 2017-12-07 17:49:57.491926: step 36640, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 56h:17m:20s remains)
INFO - root - 2017-12-07 17:50:04.307208: step 36650, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 57h:35m:29s remains)
INFO - root - 2017-12-07 17:50:11.099869: step 36660, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:51m:28s remains)
INFO - root - 2017-12-07 17:50:17.804899: step 36670, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.653 sec/batch; 53h:41m:53s remains)
INFO - root - 2017-12-07 17:50:24.561212: step 36680, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 53h:01m:35s remains)
INFO - root - 2017-12-07 17:50:31.382400: step 36690, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 55h:16m:02s remains)
INFO - root - 2017-12-07 17:50:38.067996: step 36700, loss = 2.02, batch loss = 1.97 (11.2 examples/sec; 0.714 sec/batch; 58h:39m:45s remains)
2017-12-07 17:50:38.869625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2051692 -4.1949677 -4.203063 -4.2204361 -4.2391262 -4.2505445 -4.2566481 -4.2593684 -4.2603531 -4.2589421 -4.2589116 -4.2616549 -4.2695932 -4.2729316 -4.2703776][-4.1628046 -4.1531167 -4.1662316 -4.1876354 -4.2061648 -4.2127194 -4.2131066 -4.2171888 -4.2215929 -4.2258654 -4.2308493 -4.2364755 -4.2479291 -4.2530251 -4.2486172][-4.1212335 -4.1144257 -4.1318874 -4.1555834 -4.1670337 -4.157752 -4.1448112 -4.1454382 -4.1557145 -4.1701646 -4.1840725 -4.1972213 -4.21318 -4.2198343 -4.2148285][-4.0923076 -4.0900974 -4.1089997 -4.1273775 -4.1249075 -4.0910354 -4.063478 -4.057055 -4.0741029 -4.1009951 -4.1253891 -4.1505051 -4.174242 -4.1867113 -4.1859][-4.0913782 -4.0875931 -4.0979166 -4.0965509 -4.0642614 -4.0034809 -3.9685042 -3.9634905 -3.9974356 -4.0418482 -4.0786147 -4.1164069 -4.1524715 -4.1713333 -4.1766324][-4.1117206 -4.1019511 -4.0933361 -4.0593557 -3.9896967 -3.9102883 -3.8760028 -3.8835206 -3.9380889 -4.0046639 -4.056509 -4.1038566 -4.1491604 -4.1701245 -4.1793289][-4.1608887 -4.1428523 -4.1070719 -4.0390825 -3.9419923 -3.8627653 -3.8427875 -3.8673687 -3.9326508 -4.0090938 -4.069375 -4.1197739 -4.1633577 -4.1808915 -4.1894093][-4.2265306 -4.2004728 -4.1478138 -4.0669775 -3.9715557 -3.9071884 -3.9093516 -3.9532223 -4.0151978 -4.0758476 -4.1197639 -4.1561074 -4.1874266 -4.2015953 -4.2141938][-4.2890887 -4.2620497 -4.204134 -4.1266308 -4.0395508 -3.9883151 -4.0024371 -4.0516677 -4.1042438 -4.1469088 -4.1749434 -4.2025862 -4.2269421 -4.2401752 -4.2535133][-4.3226519 -4.3013306 -4.2468667 -4.1781473 -4.1032009 -4.0645919 -4.084187 -4.1294565 -4.1765289 -4.2069697 -4.2248659 -4.2457881 -4.2679062 -4.28251 -4.2923636][-4.3271036 -4.3101544 -4.261435 -4.2067914 -4.1521583 -4.1288056 -4.1509237 -4.1893854 -4.2299075 -4.2530661 -4.2588415 -4.2683539 -4.2870793 -4.3028975 -4.3090029][-4.3155608 -4.2996635 -4.2584815 -4.2198768 -4.1901813 -4.1844239 -4.2104645 -4.2453833 -4.275681 -4.2901278 -4.2863703 -4.2845936 -4.2929473 -4.3040018 -4.3069134][-4.2983785 -4.2848921 -4.2550273 -4.2334871 -4.2248321 -4.2313375 -4.2559466 -4.2827196 -4.2997189 -4.3048635 -4.2978373 -4.2912035 -4.2921033 -4.2977939 -4.296536][-4.2844634 -4.2764387 -4.26125 -4.2584076 -4.2628169 -4.2706218 -4.2880898 -4.3049521 -4.3107615 -4.3102369 -4.3032308 -4.2963543 -4.2938008 -4.29603 -4.2936654][-4.291431 -4.2883954 -4.284481 -4.2902517 -4.2961068 -4.2991266 -4.3082266 -4.3174629 -4.3200321 -4.3196211 -4.3163085 -4.3107438 -4.3062983 -4.3051271 -4.3034348]]...]
INFO - root - 2017-12-07 17:50:45.550572: step 36710, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 53h:16m:34s remains)
INFO - root - 2017-12-07 17:50:52.364877: step 36720, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 58h:37m:27s remains)
INFO - root - 2017-12-07 17:50:59.223708: step 36730, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.754 sec/batch; 61h:56m:23s remains)
INFO - root - 2017-12-07 17:51:06.027294: step 36740, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:50m:56s remains)
INFO - root - 2017-12-07 17:51:12.801459: step 36750, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 53h:46m:24s remains)
INFO - root - 2017-12-07 17:51:19.670786: step 36760, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:02m:24s remains)
INFO - root - 2017-12-07 17:51:26.513832: step 36770, loss = 2.06, batch loss = 2.01 (10.7 examples/sec; 0.748 sec/batch; 61h:25m:50s remains)
INFO - root - 2017-12-07 17:51:33.249613: step 36780, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 59h:11m:21s remains)
INFO - root - 2017-12-07 17:51:40.008073: step 36790, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 54h:58m:22s remains)
INFO - root - 2017-12-07 17:51:46.650897: step 36800, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 51h:05m:53s remains)
2017-12-07 17:51:47.401327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2655573 -4.2411284 -4.2221708 -4.2247128 -4.2475748 -4.2779794 -4.2955503 -4.2934451 -4.2820005 -4.2661147 -4.2440081 -4.2134395 -4.1795287 -4.1577649 -4.1664591][-4.2333097 -4.1966839 -4.17385 -4.184814 -4.21814 -4.2541709 -4.2734814 -4.2705231 -4.2601008 -4.2441635 -4.2206287 -4.1820359 -4.1307583 -4.0972152 -4.1143103][-4.1902905 -4.145649 -4.1262732 -4.1494136 -4.1927133 -4.2299066 -4.2435808 -4.2367125 -4.2278314 -4.2138858 -4.1909852 -4.1469607 -4.0836535 -4.0423751 -4.0695009][-4.1583905 -4.1128759 -4.101016 -4.1316657 -4.1722369 -4.1924472 -4.1866608 -4.172092 -4.170836 -4.1691589 -4.1575251 -4.1202064 -4.0594673 -4.0185208 -4.0513978][-4.1812062 -4.1441178 -4.1365056 -4.152616 -4.1634192 -4.1452923 -4.1017585 -4.0719442 -4.0865827 -4.1143885 -4.1326971 -4.1224623 -4.0834723 -4.0542564 -4.086854][-4.2108054 -4.1814337 -4.1729155 -4.16357 -4.1326203 -4.0568223 -3.9528921 -3.894016 -3.9409828 -4.0291471 -4.1009159 -4.1339693 -4.1271834 -4.1127367 -4.1383557][-4.2208896 -4.1946549 -4.1824827 -4.1535797 -4.0917673 -3.9620852 -3.7862144 -3.6839504 -3.7797108 -3.9412148 -4.0654259 -4.13669 -4.1548696 -4.1524582 -4.1732693][-4.2233696 -4.2014704 -4.1883035 -4.1534424 -4.0902772 -3.9559875 -3.7600043 -3.6383 -3.7491755 -3.9279189 -4.064147 -4.1487675 -4.1751132 -4.1778126 -4.1932592][-4.2132607 -4.2004447 -4.1974363 -4.1755557 -4.1382675 -4.0477376 -3.905762 -3.8176634 -3.8907516 -4.0108333 -4.106122 -4.1664667 -4.1820054 -4.1825452 -4.1963139][-4.1876206 -4.1872625 -4.19909 -4.1963124 -4.1901546 -4.1456151 -4.0604496 -4.008635 -4.0517583 -4.1119113 -4.15571 -4.1759658 -4.1661263 -4.153213 -4.1666608][-4.1624532 -4.1694732 -4.1944189 -4.2117629 -4.230989 -4.2226696 -4.1797934 -4.1537337 -4.1726727 -4.1863775 -4.1870179 -4.1681509 -4.1301289 -4.1009989 -4.1171427][-4.1628833 -4.17328 -4.2052746 -4.2375383 -4.2686529 -4.2796917 -4.2623034 -4.2475724 -4.2457514 -4.2262292 -4.1939592 -4.1509957 -4.1000109 -4.0647306 -4.0824261][-4.1828017 -4.1941972 -4.22346 -4.2545714 -4.2864079 -4.304894 -4.3016181 -4.2935548 -4.2806511 -4.2439647 -4.1982422 -4.15158 -4.1060123 -4.0763917 -4.0955763][-4.2090235 -4.2151036 -4.2353024 -4.2561703 -4.280467 -4.3013854 -4.3077717 -4.3051224 -4.2912531 -4.2556887 -4.2167044 -4.1781712 -4.1456847 -4.1259031 -4.141181][-4.241847 -4.2432213 -4.2546062 -4.2667356 -4.2829256 -4.3002148 -4.3101468 -4.31139 -4.3005462 -4.27333 -4.2460413 -4.2183185 -4.1980071 -4.1846347 -4.1888733]]...]
INFO - root - 2017-12-07 17:51:54.244028: step 36810, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 60h:17m:34s remains)
INFO - root - 2017-12-07 17:52:01.105024: step 36820, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 54h:05m:21s remains)
INFO - root - 2017-12-07 17:52:07.658672: step 36830, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 53h:15m:42s remains)
INFO - root - 2017-12-07 17:52:14.555485: step 36840, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 60h:07m:59s remains)
INFO - root - 2017-12-07 17:52:21.291652: step 36850, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.760 sec/batch; 62h:22m:29s remains)
INFO - root - 2017-12-07 17:52:28.044700: step 36860, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 54h:57m:20s remains)
INFO - root - 2017-12-07 17:52:34.787667: step 36870, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 50h:55m:00s remains)
INFO - root - 2017-12-07 17:52:41.567367: step 36880, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 54h:04m:55s remains)
INFO - root - 2017-12-07 17:52:48.282273: step 36890, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.705 sec/batch; 57h:54m:55s remains)
INFO - root - 2017-12-07 17:52:55.026198: step 36900, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 56h:53m:14s remains)
2017-12-07 17:52:55.736131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2942548 -4.2931385 -4.2828503 -4.2715459 -4.2747078 -4.2793045 -4.2793565 -4.271071 -4.2587109 -4.2347069 -4.2001529 -4.170125 -4.1638784 -4.1850057 -4.2185493][-4.294703 -4.2973356 -4.2894735 -4.277709 -4.27863 -4.28164 -4.2809715 -4.2695932 -4.2591615 -4.2437305 -4.2261262 -4.2134881 -4.2155061 -4.2294292 -4.249794][-4.2768064 -4.2834358 -4.2807007 -4.2699761 -4.2645955 -4.2629752 -4.2614202 -4.2528849 -4.2493625 -4.2466717 -4.2446389 -4.24674 -4.2520885 -4.258472 -4.2634277][-4.2531528 -4.2635193 -4.2638721 -4.2557321 -4.2453189 -4.2376046 -4.2306376 -4.2230878 -4.2300982 -4.2422857 -4.2484632 -4.2565341 -4.2619829 -4.26588 -4.2628522][-4.2267747 -4.2405481 -4.2414169 -4.2317863 -4.2152772 -4.1985049 -4.1871362 -4.1843281 -4.2003727 -4.2217412 -4.2303391 -4.2365823 -4.2409539 -4.2445493 -4.2408876][-4.1722493 -4.1831622 -4.1873021 -4.1814413 -4.16658 -4.1478086 -4.132215 -4.1320519 -4.1540051 -4.1781378 -4.1875849 -4.193265 -4.199533 -4.200356 -4.1978068][-4.1215477 -4.1104112 -4.1022205 -4.0920892 -4.0777678 -4.0616369 -4.0438643 -4.0478315 -4.0817795 -4.1186357 -4.1420622 -4.1592169 -4.1701565 -4.1672354 -4.1620231][-4.1290674 -4.0853271 -4.0481052 -4.0184488 -3.99644 -3.9797211 -3.9634995 -3.9756265 -4.0264583 -4.0855069 -4.1310458 -4.1615977 -4.1761403 -4.1677361 -4.1560307][-4.1795859 -4.1220493 -4.0697722 -4.0304718 -4.0072327 -3.9951994 -3.9882381 -4.0068188 -4.0628963 -4.1300049 -4.1790504 -4.2042356 -4.2099309 -4.1936779 -4.1727209][-4.2202229 -4.1656981 -4.1094289 -4.0725932 -4.0595479 -4.0620823 -4.0693107 -4.0913954 -4.1380963 -4.1918654 -4.2261963 -4.2317882 -4.218266 -4.1966572 -4.1781373][-4.2302332 -4.1795812 -4.1213732 -4.0823736 -4.0712104 -4.0787435 -4.0935388 -4.1166029 -4.1540785 -4.1940303 -4.213387 -4.2010159 -4.17282 -4.1513648 -4.1443706][-4.2279439 -4.1835155 -4.1234355 -4.0776448 -4.0567141 -4.0537281 -4.0620775 -4.0820222 -4.1141186 -4.1467972 -4.1554985 -4.131856 -4.0966821 -4.077898 -4.0823703][-4.2222643 -4.1855984 -4.1265683 -4.0725284 -4.0408378 -4.0266471 -4.0262265 -4.0403786 -4.0653839 -4.0935636 -4.1003728 -4.0775375 -4.04533 -4.0301757 -4.0409546][-4.2133574 -4.1899176 -4.143661 -4.0972843 -4.0687819 -4.0547056 -4.0508323 -4.0559239 -4.0695543 -4.0935993 -4.100265 -4.0839391 -4.0596108 -4.0497637 -4.0585237][-4.199964 -4.1957331 -4.1740608 -4.1481733 -4.134234 -4.1291294 -4.127244 -4.1269231 -4.1296153 -4.1472206 -4.1559591 -4.148808 -4.139811 -4.138927 -4.144454]]...]
INFO - root - 2017-12-07 17:53:02.548229: step 36910, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 57h:43m:23s remains)
INFO - root - 2017-12-07 17:53:09.430677: step 36920, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 60h:29m:30s remains)
INFO - root - 2017-12-07 17:53:16.217311: step 36930, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 55h:39m:03s remains)
INFO - root - 2017-12-07 17:53:23.034581: step 36940, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 55h:09m:23s remains)
INFO - root - 2017-12-07 17:53:29.847899: step 36950, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 55h:44m:09s remains)
INFO - root - 2017-12-07 17:53:36.735445: step 36960, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.766 sec/batch; 62h:53m:11s remains)
INFO - root - 2017-12-07 17:53:43.472453: step 36970, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 59h:23m:26s remains)
INFO - root - 2017-12-07 17:53:50.237251: step 36980, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.643 sec/batch; 52h:46m:07s remains)
INFO - root - 2017-12-07 17:53:56.948117: step 36990, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 52h:18m:32s remains)
INFO - root - 2017-12-07 17:54:03.565723: step 37000, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.684 sec/batch; 56h:09m:04s remains)
2017-12-07 17:54:04.383594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2964053 -4.3095083 -4.3119164 -4.3078671 -4.2966557 -4.2845531 -4.2785273 -4.2754579 -4.273737 -4.2722459 -4.276278 -4.2855763 -4.2850409 -4.2724028 -4.2715626][-4.287149 -4.2915068 -4.2874813 -4.2803974 -4.2700624 -4.2590885 -4.2554464 -4.2534275 -4.2515039 -4.2467551 -4.2508197 -4.2677217 -4.2733231 -4.2665219 -4.2711563][-4.2717242 -4.2636919 -4.2490406 -4.23986 -4.2345018 -4.2275791 -4.2282324 -4.22842 -4.22576 -4.2193842 -4.2215014 -4.2387853 -4.2479224 -4.2477703 -4.25879][-4.2663155 -4.2476993 -4.2220545 -4.2117152 -4.211812 -4.2095265 -4.2127585 -4.2148404 -4.2155328 -4.2112904 -4.2084408 -4.2162075 -4.2228546 -4.2264581 -4.2409806][-4.2761822 -4.2576108 -4.2287636 -4.2146239 -4.2112575 -4.2048087 -4.20372 -4.2075405 -4.214128 -4.2130604 -4.202754 -4.2008367 -4.2028785 -4.2081261 -4.2246923][-4.2937546 -4.2821903 -4.2538466 -4.2282844 -4.2093186 -4.1905127 -4.1797533 -4.1851969 -4.2011814 -4.2080793 -4.2000012 -4.1980858 -4.2008419 -4.2040529 -4.2147923][-4.2993269 -4.2945037 -4.2638268 -4.2249408 -4.189343 -4.1596389 -4.1414962 -4.1488228 -4.1755009 -4.197516 -4.2015843 -4.2063942 -4.2117085 -4.2133703 -4.2189975][-4.2847323 -4.2862654 -4.2560468 -4.2123818 -4.1679544 -4.1319971 -4.10952 -4.1167917 -4.1504145 -4.1842117 -4.202486 -4.2177677 -4.2303205 -4.2360444 -4.2405415][-4.2521958 -4.2633023 -4.2390451 -4.1986814 -4.1526985 -4.1145082 -4.0958977 -4.10958 -4.1492925 -4.1884403 -4.212584 -4.2329264 -4.2488122 -4.2583404 -4.2637081][-4.221735 -4.2420597 -4.2267642 -4.1942239 -4.1545682 -4.1232429 -4.1132059 -4.132853 -4.1729636 -4.2096915 -4.2326818 -4.2525616 -4.2678742 -4.2779975 -4.2844276][-4.2095509 -4.235857 -4.2287774 -4.2082515 -4.1800523 -4.1572647 -4.1520486 -4.1717281 -4.2074237 -4.2396932 -4.2583137 -4.2739048 -4.283865 -4.2881317 -4.2901897][-4.2137785 -4.2426362 -4.2413611 -4.23174 -4.213644 -4.1969867 -4.1931825 -4.208549 -4.2356286 -4.2594085 -4.2715354 -4.279 -4.2818666 -4.2809935 -4.2792234][-4.2229009 -4.2520504 -4.2553115 -4.2505012 -4.2366471 -4.22238 -4.2178249 -4.2261872 -4.243403 -4.2580891 -4.2647853 -4.2662315 -4.264143 -4.2605443 -4.2556229][-4.2233477 -4.2473474 -4.2529473 -4.2506533 -4.2384143 -4.2255793 -4.2208605 -4.2236609 -4.2342753 -4.2433004 -4.2465248 -4.2455788 -4.2425022 -4.2382679 -4.23121][-4.2146797 -4.2335148 -4.2394338 -4.2383189 -4.2283239 -4.2188969 -4.2148438 -4.2144213 -4.2193365 -4.2232504 -4.224854 -4.2252173 -4.2231426 -4.2207942 -4.2188568]]...]
INFO - root - 2017-12-07 17:54:11.188988: step 37010, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 53h:17m:43s remains)
INFO - root - 2017-12-07 17:54:17.905140: step 37020, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 52h:41m:38s remains)
INFO - root - 2017-12-07 17:54:24.721507: step 37030, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 59h:31m:10s remains)
INFO - root - 2017-12-07 17:54:31.612420: step 37040, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 58h:07m:00s remains)
INFO - root - 2017-12-07 17:54:38.411979: step 37050, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.701 sec/batch; 57h:30m:01s remains)
INFO - root - 2017-12-07 17:54:45.138403: step 37060, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.632 sec/batch; 51h:52m:28s remains)
INFO - root - 2017-12-07 17:54:51.969035: step 37070, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 52h:41m:08s remains)
INFO - root - 2017-12-07 17:54:58.759119: step 37080, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 58h:17m:04s remains)
INFO - root - 2017-12-07 17:55:05.600717: step 37090, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 56h:03m:53s remains)
INFO - root - 2017-12-07 17:55:12.141340: step 37100, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 53h:32m:42s remains)
2017-12-07 17:55:12.895463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2489572 -4.2457266 -4.2260346 -4.2184014 -4.236876 -4.2477365 -4.249248 -4.2544947 -4.26452 -4.26669 -4.2660785 -4.2744255 -4.2806211 -4.2702875 -4.2587337][-4.236578 -4.2281308 -4.2066269 -4.2045908 -4.2340407 -4.2526507 -4.258111 -4.2623053 -4.2683048 -4.2683139 -4.2684269 -4.2751217 -4.2728343 -4.2521906 -4.2339373][-4.2312913 -4.2189856 -4.1974587 -4.1981397 -4.233706 -4.2571177 -4.2637882 -4.2644768 -4.2632136 -4.2622046 -4.2665005 -4.2731438 -4.265408 -4.2382712 -4.2154126][-4.2317648 -4.2173576 -4.1963482 -4.1949673 -4.2257633 -4.2450347 -4.2471743 -4.24429 -4.23948 -4.2399607 -4.25062 -4.2600961 -4.2532678 -4.2292333 -4.2109113][-4.2323256 -4.212357 -4.1930528 -4.1914372 -4.2153363 -4.2272997 -4.2214284 -4.2126422 -4.20443 -4.2069573 -4.2238407 -4.2378693 -4.2348752 -4.2178359 -4.20972][-4.2334585 -4.2032766 -4.1810627 -4.1785727 -4.1961451 -4.1998782 -4.1823683 -4.1643934 -4.1501451 -4.1556282 -4.1822824 -4.2047873 -4.2092648 -4.1994524 -4.1988859][-4.2191591 -4.1795392 -4.1544 -4.1528831 -4.1696358 -4.1641006 -4.1307573 -4.1006546 -4.0796165 -4.0901594 -4.1317143 -4.1665311 -4.1799436 -4.1775856 -4.1797295][-4.2027144 -4.1609454 -4.1384897 -4.1397572 -4.1568828 -4.1435547 -4.0938163 -4.0500965 -4.0260887 -4.0417542 -4.0933876 -4.1363239 -4.1579046 -4.164464 -4.1710863][-4.1991568 -4.1603913 -4.1381712 -4.1406903 -4.158524 -4.1421208 -4.0841374 -4.0303545 -4.005909 -4.0244341 -4.0766573 -4.1224079 -4.1516743 -4.1658783 -4.1779876][-4.2054625 -4.1703448 -4.1486316 -4.1509848 -4.1734414 -4.1632576 -4.11196 -4.0618472 -4.0407629 -4.0566144 -4.0980453 -4.135932 -4.166688 -4.183413 -4.1920037][-4.2233796 -4.1947489 -4.1755571 -4.1773062 -4.2012835 -4.2031856 -4.1711349 -4.1372809 -4.1233692 -4.1330976 -4.1568413 -4.17998 -4.2020831 -4.2121191 -4.2107944][-4.2285056 -4.2108574 -4.1965795 -4.1965303 -4.2183838 -4.2272954 -4.2099295 -4.1889548 -4.1797476 -4.1825614 -4.1937547 -4.2065649 -4.2197146 -4.2211533 -4.2110295][-4.204854 -4.199728 -4.1929231 -4.1937785 -4.2154341 -4.2280035 -4.2163715 -4.2038541 -4.2001653 -4.2000632 -4.20371 -4.2090964 -4.2134118 -4.2061205 -4.1901612][-4.1742492 -4.1739092 -4.1704836 -4.1729083 -4.1967578 -4.2106819 -4.2021484 -4.19617 -4.1991639 -4.2014575 -4.2035632 -4.2047038 -4.2014542 -4.1898603 -4.173502][-4.1623878 -4.1595325 -4.1533093 -4.1539764 -4.1774993 -4.1898232 -4.1830025 -4.1815038 -4.1881204 -4.1921535 -4.1950078 -4.1953063 -4.1893449 -4.1795688 -4.1687269]]...]
INFO - root - 2017-12-07 17:55:19.767186: step 37110, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 57h:32m:08s remains)
INFO - root - 2017-12-07 17:55:26.579199: step 37120, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 54h:27m:56s remains)
INFO - root - 2017-12-07 17:55:33.346372: step 37130, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 51h:17m:42s remains)
INFO - root - 2017-12-07 17:55:39.982425: step 37140, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 59h:22m:37s remains)
INFO - root - 2017-12-07 17:55:46.617219: step 37150, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 58h:34m:20s remains)
INFO - root - 2017-12-07 17:55:53.358879: step 37160, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 59h:50m:40s remains)
INFO - root - 2017-12-07 17:56:00.212109: step 37170, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 51h:48m:51s remains)
INFO - root - 2017-12-07 17:56:07.046531: step 37180, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 52h:56m:39s remains)
INFO - root - 2017-12-07 17:56:13.964445: step 37190, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 60h:43m:47s remains)
INFO - root - 2017-12-07 17:56:20.497087: step 37200, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 57h:46m:44s remains)
2017-12-07 17:56:21.179855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3060513 -4.300705 -4.302072 -4.3083057 -4.3081775 -4.3009562 -4.2888708 -4.2732124 -4.2591915 -4.2445321 -4.2344513 -4.2268424 -4.2223287 -4.2215028 -4.2173247][-4.2981739 -4.292098 -4.2970119 -4.3068657 -4.3029833 -4.2872195 -4.2640138 -4.2376237 -4.2227364 -4.2133007 -4.2099795 -4.2058744 -4.1992292 -4.1978092 -4.19329][-4.2892208 -4.2833409 -4.29126 -4.3000383 -4.2889118 -4.2612944 -4.2227817 -4.1846709 -4.1720686 -4.1755881 -4.1841531 -4.1864676 -4.1806669 -4.1808057 -4.1749334][-4.2848573 -4.2791772 -4.2832289 -4.2833157 -4.2632561 -4.2265205 -4.1759639 -4.127038 -4.1176376 -4.13822 -4.160985 -4.1693444 -4.1645164 -4.167408 -4.1614103][-4.28695 -4.2792044 -4.2712688 -4.2595887 -4.2319098 -4.189003 -4.1284075 -4.067173 -4.0586357 -4.0954304 -4.1335444 -4.150948 -4.1474752 -4.1501608 -4.1466546][-4.2858906 -4.2710166 -4.2501421 -4.227633 -4.1912737 -4.1412072 -4.0713177 -3.9994113 -3.9889898 -4.039722 -4.0964427 -4.1289415 -4.1362386 -4.1451707 -4.1487646][-4.2825704 -4.2611656 -4.2344112 -4.2073298 -4.1615696 -4.0994639 -4.015039 -3.9239626 -3.9068272 -3.9734607 -4.0573215 -4.1127071 -4.1354828 -4.1500731 -4.1587806][-4.2889991 -4.2655249 -4.2360315 -4.2085423 -4.1570787 -4.0787086 -3.9724174 -3.8503892 -3.8099606 -3.8888113 -4.0056973 -4.0895333 -4.13307 -4.1592736 -4.1749711][-4.2975574 -4.2741551 -4.2415881 -4.2121263 -4.1580958 -4.0713491 -3.95964 -3.8319454 -3.7788408 -3.8602741 -3.9924035 -4.0899653 -4.1466489 -4.1821928 -4.2029929][-4.300642 -4.277739 -4.2449875 -4.2165179 -4.1665788 -4.08625 -3.9911094 -3.8944604 -3.8608332 -3.9289668 -4.0342574 -4.1154327 -4.1665387 -4.2001529 -4.22109][-4.2962065 -4.2748871 -4.2452164 -4.217062 -4.1699018 -4.1008167 -4.0215316 -3.9526591 -3.9433076 -3.9971936 -4.0727844 -4.1360073 -4.1780748 -4.2102427 -4.233551][-4.2886977 -4.2688832 -4.2473783 -4.2251883 -4.1819091 -4.12106 -4.0500207 -3.9942024 -3.995508 -4.0399981 -4.0993772 -4.1521316 -4.18592 -4.2152491 -4.24041][-4.2791848 -4.260303 -4.2488642 -4.2382755 -4.2040443 -4.1498666 -4.0842686 -4.0309014 -4.0294251 -4.0636215 -4.1117325 -4.1565447 -4.181488 -4.2087865 -4.2344308][-4.2696347 -4.2478991 -4.2416806 -4.2376604 -4.211278 -4.1650448 -4.1044621 -4.0470762 -4.0423732 -4.0716987 -4.1108994 -4.1456213 -4.1622796 -4.1893969 -4.2188978][-4.2627831 -4.2371778 -4.2322364 -4.2309175 -4.2098446 -4.1723042 -4.1202507 -4.0622039 -4.0546341 -4.0830741 -4.1162195 -4.143784 -4.1520953 -4.1765966 -4.2096353]]...]
INFO - root - 2017-12-07 17:56:27.980442: step 37210, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 53h:44m:51s remains)
INFO - root - 2017-12-07 17:56:34.787529: step 37220, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 59h:11m:30s remains)
INFO - root - 2017-12-07 17:56:41.605112: step 37230, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 58h:29m:20s remains)
INFO - root - 2017-12-07 17:56:48.371820: step 37240, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 55h:48m:19s remains)
INFO - root - 2017-12-07 17:56:55.094504: step 37250, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 51h:49m:45s remains)
INFO - root - 2017-12-07 17:57:01.838831: step 37260, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.641 sec/batch; 52h:36m:14s remains)
INFO - root - 2017-12-07 17:57:08.619236: step 37270, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 55h:09m:17s remains)
INFO - root - 2017-12-07 17:57:15.358046: step 37280, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 58h:24m:10s remains)
INFO - root - 2017-12-07 17:57:22.183662: step 37290, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.752 sec/batch; 61h:41m:58s remains)
INFO - root - 2017-12-07 17:57:28.733612: step 37300, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 54h:24m:04s remains)
2017-12-07 17:57:29.442123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3352575 -4.3337398 -4.3323741 -4.33142 -4.3327165 -4.3356581 -4.337388 -4.3364348 -4.3328404 -4.3290353 -4.32732 -4.3268747 -4.3269334 -4.3286824 -4.3307734][-4.3367867 -4.3294978 -4.3239465 -4.3209925 -4.3247647 -4.332408 -4.3354959 -4.3320756 -4.322927 -4.3157444 -4.3130994 -4.31331 -4.315239 -4.3180919 -4.32301][-4.3244009 -4.3070087 -4.2945318 -4.2912688 -4.2983847 -4.3084307 -4.3109236 -4.3040962 -4.2885389 -4.2776 -4.2762685 -4.279027 -4.2866387 -4.2956419 -4.3054814][-4.2870259 -4.2602892 -4.2430191 -4.2401876 -4.2465291 -4.25405 -4.2516246 -4.2373462 -4.2155924 -4.2000513 -4.2025614 -4.2182283 -4.2414236 -4.2592621 -4.2777286][-4.2362409 -4.2037334 -4.1858459 -4.1797185 -4.17346 -4.1643047 -4.1459732 -4.1181521 -4.0863276 -4.0737071 -4.095891 -4.1389527 -4.1821012 -4.2072659 -4.2359533][-4.1915369 -4.1560693 -4.1426296 -4.1311483 -4.1039243 -4.0638208 -4.0227556 -3.982147 -3.944061 -3.9451587 -4.0019608 -4.0751705 -4.1305161 -4.1638927 -4.2033825][-4.163764 -4.128633 -4.1234589 -4.1074519 -4.0535469 -3.9802325 -3.9211094 -3.8741937 -3.8526964 -3.8776054 -3.9570043 -4.04911 -4.1091847 -4.1491232 -4.1958008][-4.1667209 -4.1369157 -4.1369715 -4.1155434 -4.0457911 -3.96003 -3.898782 -3.8593774 -3.8606405 -3.89867 -3.9765077 -4.0658689 -4.126092 -4.1668873 -4.2166181][-4.1952243 -4.1748791 -4.1787639 -4.1604366 -4.1007352 -4.0306187 -3.977205 -3.9460235 -3.9546781 -3.9891987 -4.049572 -4.1267676 -4.1806359 -4.2157006 -4.2623882][-4.2433248 -4.2315922 -4.2351069 -4.22584 -4.1875753 -4.1423254 -4.0994377 -4.0681777 -4.0708957 -4.0970392 -4.1465273 -4.2090468 -4.2501264 -4.2741623 -4.3087583][-4.2893033 -4.2864757 -4.2870436 -4.27763 -4.2520623 -4.2237082 -4.1956363 -4.1761017 -4.1807089 -4.2010069 -4.2390795 -4.28287 -4.30769 -4.3209405 -4.3431115][-4.3192396 -4.3209872 -4.3156018 -4.3014441 -4.2809672 -4.2654495 -4.2517877 -4.2462587 -4.2579904 -4.2754416 -4.2993722 -4.3241668 -4.3358073 -4.3444743 -4.3572574][-4.336792 -4.34222 -4.3337927 -4.3150606 -4.2940192 -4.2823143 -4.2764506 -4.2789955 -4.2975121 -4.3178797 -4.33481 -4.3488545 -4.3554788 -4.3589792 -4.362102][-4.3519931 -4.3587742 -4.3489695 -4.3254437 -4.2999749 -4.2842245 -4.2785954 -4.2831883 -4.3025775 -4.3246551 -4.3428164 -4.3573475 -4.3632064 -4.3626504 -4.3606334][-4.3586693 -4.36481 -4.353579 -4.32836 -4.3011394 -4.2774224 -4.2667975 -4.2708297 -4.2905893 -4.3163676 -4.3393621 -4.355093 -4.35809 -4.3557458 -4.3515935]]...]
INFO - root - 2017-12-07 17:57:36.209939: step 37310, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 57h:26m:50s remains)
INFO - root - 2017-12-07 17:57:43.062221: step 37320, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 55h:33m:19s remains)
INFO - root - 2017-12-07 17:57:49.929904: step 37330, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 52h:39m:32s remains)
INFO - root - 2017-12-07 17:57:56.821987: step 37340, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 52h:49m:34s remains)
INFO - root - 2017-12-07 17:58:03.656596: step 37350, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 59h:08m:09s remains)
INFO - root - 2017-12-07 17:58:10.489926: step 37360, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 57h:25m:45s remains)
INFO - root - 2017-12-07 17:58:17.237788: step 37370, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 55h:16m:03s remains)
INFO - root - 2017-12-07 17:58:24.025222: step 37380, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 52h:04m:57s remains)
INFO - root - 2017-12-07 17:58:30.774408: step 37390, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 50h:48m:50s remains)
INFO - root - 2017-12-07 17:58:37.456321: step 37400, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 56h:05m:36s remains)
2017-12-07 17:58:38.236060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2755957 -4.2745466 -4.2730737 -4.2846394 -4.3035784 -4.3187 -4.3261127 -4.3196721 -4.3071442 -4.2921319 -4.2706881 -4.2361336 -4.1947861 -4.167254 -4.156198][-4.3027086 -4.30493 -4.3042736 -4.3117156 -4.323318 -4.3315868 -4.3334146 -4.3230891 -4.3086586 -4.2901516 -4.2633286 -4.223721 -4.1808491 -4.1541257 -4.1458578][-4.3088288 -4.3095841 -4.3095179 -4.313612 -4.3178697 -4.3171129 -4.3098054 -4.2952147 -4.2829132 -4.2687044 -4.2450256 -4.2090435 -4.1698031 -4.1454821 -4.1385241][-4.2965889 -4.2904763 -4.2874146 -4.2868557 -4.2820582 -4.2692156 -4.2480693 -4.2285223 -4.2252235 -4.2271018 -4.217905 -4.1914577 -4.1631184 -4.1450524 -4.1366029][-4.2590928 -4.2382879 -4.2289958 -4.2244925 -4.20919 -4.1754184 -4.1302543 -4.1106091 -4.132627 -4.1617064 -4.171701 -4.155385 -4.1414633 -4.1373849 -4.134244][-4.1977773 -4.1524286 -4.1332946 -4.1303406 -4.1049929 -4.04235 -3.9554334 -3.9334271 -4.0035348 -4.0743585 -4.1025224 -4.0901222 -4.0783315 -4.091506 -4.1058178][-4.1276703 -4.0546494 -4.0331688 -4.0461116 -4.021143 -3.9292719 -3.78642 -3.7515917 -3.8895078 -4.0088539 -4.046751 -4.0228705 -3.9986055 -4.0239353 -4.0590706][-4.0755644 -3.9930971 -3.983011 -4.0131483 -3.9964352 -3.9081676 -3.7575159 -3.7190461 -3.87753 -4.0117936 -4.0521832 -4.0180397 -3.9815216 -3.9966619 -4.031549][-4.0697017 -4.0141072 -4.0255284 -4.0574713 -4.0464149 -3.9946017 -3.9025662 -3.8769224 -3.9869828 -4.0921769 -4.12768 -4.0938177 -4.0473108 -4.03657 -4.0530539][-4.102879 -4.0833621 -4.1043596 -4.1299148 -4.1266913 -4.1080446 -4.0649576 -4.0482864 -4.1114841 -4.1802416 -4.2093291 -4.1832819 -4.1398582 -4.1153574 -4.1161489][-4.1540322 -4.1554303 -4.1734715 -4.1958542 -4.1999822 -4.1951947 -4.1758966 -4.1680245 -4.2011566 -4.2407355 -4.2628875 -4.2492127 -4.2217722 -4.2007265 -4.196661][-4.2106671 -4.2152953 -4.2260909 -4.2451019 -4.2561054 -4.2571378 -4.2508483 -4.2512059 -4.2692647 -4.2892737 -4.3019419 -4.2948532 -4.2798157 -4.2661366 -4.2615452][-4.2541981 -4.2585459 -4.2660003 -4.2797742 -4.2901268 -4.2930236 -4.29249 -4.2951641 -4.3058481 -4.3156023 -4.3208055 -4.3158851 -4.3080845 -4.3010335 -4.2982569][-4.2811847 -4.2858171 -4.2937045 -4.3041787 -4.31119 -4.3136053 -4.3125038 -4.3116107 -4.3165 -4.3206778 -4.3231339 -4.3189659 -4.3133607 -4.3102012 -4.312149][-4.2990432 -4.3049865 -4.3141837 -4.3222365 -4.3248658 -4.3222327 -4.3169422 -4.3124428 -4.3129706 -4.3157158 -4.3178411 -4.3159418 -4.312695 -4.3123217 -4.3166075]]...]
INFO - root - 2017-12-07 17:58:45.069831: step 37410, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 53h:18m:43s remains)
INFO - root - 2017-12-07 17:58:51.797762: step 37420, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 52h:09m:38s remains)
INFO - root - 2017-12-07 17:58:58.612165: step 37430, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 53h:08m:15s remains)
INFO - root - 2017-12-07 17:59:05.493164: step 37440, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 58h:53m:53s remains)
INFO - root - 2017-12-07 17:59:12.026080: step 37450, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 54h:16m:07s remains)
INFO - root - 2017-12-07 17:59:18.760359: step 37460, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 51h:09m:56s remains)
INFO - root - 2017-12-07 17:59:25.541227: step 37470, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 53h:15m:43s remains)
INFO - root - 2017-12-07 17:59:32.352322: step 37480, loss = 2.03, batch loss = 1.97 (10.9 examples/sec; 0.732 sec/batch; 60h:00m:45s remains)
INFO - root - 2017-12-07 17:59:39.153070: step 37490, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:38m:58s remains)
INFO - root - 2017-12-07 17:59:45.786141: step 37500, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 53h:48m:16s remains)
2017-12-07 17:59:46.467661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1325407 -4.1162453 -4.1021442 -4.1030235 -4.1105237 -4.1264353 -4.1524863 -4.1721988 -4.176836 -4.1731844 -4.1745429 -4.1729994 -4.1704969 -4.1633744 -4.1552086][-4.0970836 -4.0819187 -4.0843863 -4.1072206 -4.1315107 -4.1521096 -4.170393 -4.178884 -4.1755939 -4.1667414 -4.1687946 -4.1743569 -4.1742821 -4.1696806 -4.1647048][-4.0928812 -4.0809402 -4.0930252 -4.1247807 -4.1543465 -4.1726937 -4.180275 -4.1768303 -4.170723 -4.162549 -4.1673985 -4.1777568 -4.1752172 -4.1717124 -4.1671033][-4.1164231 -4.1056914 -4.1155024 -4.1429486 -4.1677794 -4.181601 -4.1812439 -4.1719117 -4.1637244 -4.1608267 -4.1715751 -4.1837125 -4.1805677 -4.1769166 -4.1693373][-4.1288218 -4.1212187 -4.1309323 -4.1563005 -4.174119 -4.1743927 -4.1596127 -4.1428337 -4.1355433 -4.1399145 -4.1655154 -4.1921606 -4.2000742 -4.2008271 -4.1913247][-4.1192226 -4.1190982 -4.1369629 -4.1626244 -4.1707511 -4.1483965 -4.1140442 -4.0890045 -4.0866346 -4.1074624 -4.1555138 -4.2050424 -4.22958 -4.2348633 -4.2216334][-4.1141453 -4.1180596 -4.1373758 -4.149806 -4.1372156 -4.0864038 -4.0274186 -3.9974234 -4.0136766 -4.0669665 -4.1441813 -4.2111392 -4.2427797 -4.2454748 -4.229486][-4.111948 -4.120647 -4.1397858 -4.1387391 -4.1006966 -4.0203238 -3.9358246 -3.9014819 -3.9425991 -4.031601 -4.1294775 -4.2025824 -4.2340903 -4.2347341 -4.2173443][-4.1116843 -4.1319776 -4.1553707 -4.1496172 -4.1013184 -4.0145397 -3.9259241 -3.8934615 -3.9454422 -4.0415344 -4.135148 -4.1997609 -4.2288423 -4.2295938 -4.2123122][-4.135736 -4.1667242 -4.1914105 -4.1856785 -4.1436586 -4.0743036 -4.0035644 -3.9758005 -4.0148721 -4.0893874 -4.1617131 -4.210937 -4.233304 -4.2335668 -4.2201438][-4.1867256 -4.2130084 -4.2264032 -4.2179737 -4.1871834 -4.142745 -4.0972919 -4.0767779 -4.1005826 -4.1527076 -4.2016888 -4.2306066 -4.2388115 -4.2316155 -4.2186294][-4.2163725 -4.2261033 -4.2257471 -4.2159534 -4.1967793 -4.1758051 -4.1581798 -4.1550417 -4.1761041 -4.2102194 -4.2331519 -4.2379909 -4.2307811 -4.2170467 -4.2056046][-4.2205319 -4.2140269 -4.2041764 -4.194088 -4.1873751 -4.187254 -4.1918769 -4.2035713 -4.22316 -4.2389812 -4.238409 -4.22629 -4.2139578 -4.2060862 -4.2056208][-4.2143269 -4.1981983 -4.18728 -4.1847014 -4.1915612 -4.2063241 -4.2209654 -4.2335587 -4.24294 -4.2436752 -4.2336354 -4.2195563 -4.2128277 -4.2171774 -4.226584][-4.1832728 -4.1624193 -4.1580958 -4.1729183 -4.1996846 -4.227788 -4.2489753 -4.2558012 -4.2501378 -4.239831 -4.2295709 -4.2247906 -4.2298303 -4.24241 -4.2529759]]...]
INFO - root - 2017-12-07 17:59:53.329622: step 37510, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 55h:56m:01s remains)
INFO - root - 2017-12-07 18:00:00.209720: step 37520, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.705 sec/batch; 57h:44m:23s remains)
INFO - root - 2017-12-07 18:00:06.927267: step 37530, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 52h:57m:30s remains)
INFO - root - 2017-12-07 18:00:13.719813: step 37540, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 53h:42m:27s remains)
INFO - root - 2017-12-07 18:00:20.536270: step 37550, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 54h:12m:06s remains)
INFO - root - 2017-12-07 18:00:27.417318: step 37560, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 59h:25m:16s remains)
INFO - root - 2017-12-07 18:00:34.202990: step 37570, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 60h:48m:43s remains)
INFO - root - 2017-12-07 18:00:40.930197: step 37580, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 53h:16m:47s remains)
INFO - root - 2017-12-07 18:00:47.683858: step 37590, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 52h:33m:48s remains)
INFO - root - 2017-12-07 18:00:54.327172: step 37600, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 52h:52m:23s remains)
2017-12-07 18:00:55.152447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2308908 -4.2285838 -4.2176361 -4.1982822 -4.18748 -4.1855774 -4.1859713 -4.1778059 -4.1640496 -4.1563663 -4.1613655 -4.1662531 -4.1650591 -4.1623631 -4.1585069][-4.2398367 -4.2247548 -4.2048364 -4.1801605 -4.162921 -4.15601 -4.1536579 -4.1480985 -4.1460853 -4.1540861 -4.1656857 -4.1644568 -4.1525531 -4.14105 -4.1340528][-4.2297959 -4.2134476 -4.1953092 -4.1743221 -4.1546488 -4.1442218 -4.1437321 -4.1444492 -4.1505575 -4.1644716 -4.1721358 -4.1582441 -4.1342978 -4.1156778 -4.1092215][-4.218339 -4.211441 -4.1987824 -4.1821494 -4.1597381 -4.1458755 -4.146924 -4.1538343 -4.1645207 -4.1779537 -4.1782846 -4.1558065 -4.1240759 -4.0993061 -4.0920067][-4.2260256 -4.2302494 -4.2197208 -4.1995649 -4.1681471 -4.1416054 -4.1359406 -4.1471076 -4.1632881 -4.1788406 -4.1784592 -4.1530161 -4.121357 -4.0969524 -4.0911884][-4.2364254 -4.249434 -4.23549 -4.20813 -4.1662526 -4.1275568 -4.1146665 -4.1324062 -4.1590796 -4.1823478 -4.1863456 -4.1679106 -4.1411266 -4.121562 -4.1154709][-4.2264466 -4.2420468 -4.2268138 -4.189074 -4.1353374 -4.0919056 -4.0838733 -4.1144147 -4.1529422 -4.1826296 -4.1931839 -4.1847596 -4.1657987 -4.1548438 -4.1495004][-4.2081547 -4.22112 -4.2056551 -4.1615024 -4.0999851 -4.052897 -4.0474434 -4.0842209 -4.1305656 -4.1675382 -4.1822538 -4.1776466 -4.1653132 -4.1619849 -4.1596355][-4.1987972 -4.2117 -4.1978879 -4.15151 -4.0902967 -4.0436339 -4.0334153 -4.0635858 -4.1119881 -4.1559343 -4.1715984 -4.1643591 -4.1504006 -4.1443143 -4.140986][-4.1986356 -4.2114081 -4.2018328 -4.16555 -4.1195946 -4.0857811 -4.0708752 -4.0888548 -4.1271358 -4.1627131 -4.1721869 -4.1598063 -4.1397481 -4.1284819 -4.122077][-4.2026234 -4.2100568 -4.2021446 -4.1771121 -4.1480203 -4.1297374 -4.1214929 -4.1329279 -4.1585841 -4.1806731 -4.1869488 -4.1722832 -4.1480875 -4.131391 -4.1226392][-4.2099037 -4.2039294 -4.1933007 -4.1783266 -4.1607504 -4.1540928 -4.1554937 -4.1667862 -4.1875572 -4.2035871 -4.2066784 -4.1895809 -4.1623254 -4.1422276 -4.1344104][-4.2162762 -4.1954346 -4.1782956 -4.169661 -4.1628637 -4.1629739 -4.1705632 -4.1823378 -4.20032 -4.2149882 -4.2198706 -4.2062621 -4.1813021 -4.1605821 -4.152061][-4.2177253 -4.186614 -4.1653848 -4.1613617 -4.1658397 -4.1709919 -4.1772633 -4.1844611 -4.1965933 -4.2120233 -4.2193084 -4.2113705 -4.1932759 -4.1747804 -4.1629124][-4.2013745 -4.1667666 -4.1488314 -4.150238 -4.1626778 -4.1702232 -4.1753659 -4.1792474 -4.186902 -4.20344 -4.2133956 -4.209703 -4.1930552 -4.1702828 -4.1525707]]...]
INFO - root - 2017-12-07 18:01:01.813431: step 37610, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 52h:07m:51s remains)
INFO - root - 2017-12-07 18:01:08.497611: step 37620, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.611 sec/batch; 50h:03m:08s remains)
INFO - root - 2017-12-07 18:01:15.386011: step 37630, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 53h:08m:27s remains)
INFO - root - 2017-12-07 18:01:22.198743: step 37640, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 57h:37m:04s remains)
INFO - root - 2017-12-07 18:01:28.941887: step 37650, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 59h:30m:03s remains)
INFO - root - 2017-12-07 18:01:35.663050: step 37660, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 54h:47m:02s remains)
INFO - root - 2017-12-07 18:01:42.493607: step 37670, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 51h:30m:26s remains)
INFO - root - 2017-12-07 18:01:49.289279: step 37680, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 54h:26m:51s remains)
INFO - root - 2017-12-07 18:01:56.132169: step 37690, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 59h:45m:31s remains)
INFO - root - 2017-12-07 18:02:02.654153: step 37700, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.715 sec/batch; 58h:30m:43s remains)
2017-12-07 18:02:03.328845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2359209 -4.2303061 -4.2359371 -4.2455988 -4.2523804 -4.2572932 -4.2613473 -4.2640581 -4.2652621 -4.263423 -4.2598805 -4.2614212 -4.2668262 -4.2670345 -4.2615881][-4.2426558 -4.24223 -4.2487435 -4.2571874 -4.2608709 -4.2615576 -4.2624607 -4.2634854 -4.2660112 -4.2671852 -4.26438 -4.2661157 -4.2726541 -4.273953 -4.2675781][-4.2537541 -4.2579341 -4.2607379 -4.2602406 -4.2553625 -4.2480888 -4.24242 -4.2394276 -4.2424326 -4.248179 -4.2499948 -4.25414 -4.2619414 -4.264894 -4.2590466][-4.2509503 -4.2532034 -4.2485595 -4.2351737 -4.2171412 -4.1999369 -4.1888843 -4.1803927 -4.1842828 -4.1964087 -4.206243 -4.2135868 -4.2217169 -4.2223725 -4.21591][-4.2223697 -4.2165346 -4.2046385 -4.1823421 -4.15519 -4.129776 -4.1155872 -4.1059823 -4.1141233 -4.133152 -4.1502132 -4.1619077 -4.1684442 -4.1649637 -4.1567411][-4.1763225 -4.1611719 -4.1453328 -4.1211891 -4.0902781 -4.056118 -4.0341372 -4.02346 -4.0383692 -4.0662646 -4.09362 -4.115788 -4.12366 -4.1142716 -4.1030669][-4.1328654 -4.1145988 -4.1005955 -4.0781813 -4.041306 -3.9955821 -3.9572291 -3.93984 -3.9601977 -3.9996605 -4.0382066 -4.074223 -4.0895777 -4.0739403 -4.0550642][-4.1101432 -4.0960712 -4.0873132 -4.0653219 -4.0221629 -3.9643021 -3.9053464 -3.8797343 -3.9071367 -3.9534261 -3.999979 -4.0482841 -4.0729804 -4.0545459 -4.026947][-4.1140924 -4.1050186 -4.102407 -4.0837274 -4.0423226 -3.9821274 -3.9114952 -3.8768125 -3.90699 -3.9531577 -4.00118 -4.0509276 -4.0766087 -4.0551429 -4.020402][-4.1381273 -4.1328177 -4.1349854 -4.1236711 -4.0940433 -4.0440016 -3.9759505 -3.9358613 -3.9582102 -4.0013094 -4.0486894 -4.0881281 -4.100121 -4.0706787 -4.0322032][-4.1656137 -4.1637325 -4.1715946 -4.1709161 -4.1576524 -4.1251621 -4.0676074 -4.0233054 -4.03014 -4.0658445 -4.1064034 -4.1249785 -4.1157479 -4.0800705 -4.0497437][-4.1817737 -4.1826239 -4.1977086 -4.2068253 -4.2077603 -4.19339 -4.1504946 -4.1060262 -4.0938878 -4.1144781 -4.1373863 -4.1366582 -4.1122894 -4.0758057 -4.0479403][-4.1903996 -4.1916928 -4.2098722 -4.2271852 -4.2390218 -4.2364073 -4.2053127 -4.1622224 -4.1322732 -4.1293 -4.1351414 -4.1253638 -4.0960264 -4.0616693 -4.0378952][-4.1998329 -4.1975765 -4.2132745 -4.23352 -4.2498074 -4.250711 -4.2305212 -4.1953182 -4.1598845 -4.1413631 -4.1368542 -4.1241217 -4.0968318 -4.0661 -4.0429025][-4.2125764 -4.2059035 -4.2126722 -4.2274613 -4.2420969 -4.2451034 -4.2352042 -4.2134352 -4.1839867 -4.1642866 -4.160603 -4.1535845 -4.1360044 -4.11591 -4.092864]]...]
INFO - root - 2017-12-07 18:02:10.192738: step 37710, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 54h:01m:12s remains)
INFO - root - 2017-12-07 18:02:17.050918: step 37720, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.728 sec/batch; 59h:39m:00s remains)
INFO - root - 2017-12-07 18:02:23.843576: step 37730, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 56h:20m:31s remains)
INFO - root - 2017-12-07 18:02:30.542477: step 37740, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 53h:55m:00s remains)
INFO - root - 2017-12-07 18:02:37.301896: step 37750, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 52h:26m:14s remains)
INFO - root - 2017-12-07 18:02:43.898208: step 37760, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.673 sec/batch; 55h:06m:28s remains)
INFO - root - 2017-12-07 18:02:50.748334: step 37770, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 55h:12m:04s remains)
INFO - root - 2017-12-07 18:02:57.525658: step 37780, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 54h:14m:30s remains)
INFO - root - 2017-12-07 18:03:04.213000: step 37790, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 50h:56m:19s remains)
INFO - root - 2017-12-07 18:03:10.896716: step 37800, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 52h:55m:19s remains)
2017-12-07 18:03:11.656525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2223263 -4.2086148 -4.180088 -4.1285162 -4.0678167 -4.0283413 -4.0319805 -4.076746 -4.1299553 -4.1641564 -4.1729236 -4.1698732 -4.1663256 -4.1687751 -4.176826][-4.2046509 -4.1962433 -4.1796255 -4.1419096 -4.0937033 -4.0615029 -4.0676847 -4.10842 -4.1551952 -4.1787133 -4.1764908 -4.1676297 -4.1615915 -4.167407 -4.1815591][-4.1872573 -4.1871614 -4.1860147 -4.1678386 -4.1372428 -4.1129704 -4.1131077 -4.13808 -4.1695004 -4.1802039 -4.1704144 -4.1608377 -4.157227 -4.1630206 -4.17535][-4.1780849 -4.1873507 -4.1981063 -4.1951823 -4.1769004 -4.154798 -4.1428695 -4.1479478 -4.1637816 -4.1636477 -4.1521635 -4.1467357 -4.1471038 -4.1546822 -4.16321][-4.18023 -4.1926832 -4.2042208 -4.2065158 -4.1925621 -4.1642671 -4.136807 -4.126574 -4.1406641 -4.1448708 -4.1354551 -4.1347551 -4.1347942 -4.1400118 -4.1482444][-4.1899652 -4.1945481 -4.1988697 -4.1991816 -4.1801119 -4.1371136 -4.0898566 -4.0716448 -4.0991745 -4.121438 -4.1226325 -4.1253037 -4.1249118 -4.1268654 -4.1342044][-4.2006197 -4.1935759 -4.1872935 -4.1803641 -4.1546521 -4.094141 -4.01933 -3.9907916 -4.0402436 -4.0940657 -4.1147032 -4.1226964 -4.1215186 -4.1187763 -4.1234441][-4.2048221 -4.1836548 -4.1637483 -4.1485357 -4.1193719 -4.051156 -3.9583006 -3.9256334 -4.0026364 -4.0873804 -4.1256466 -4.1351008 -4.1281691 -4.1171246 -4.1154437][-4.2014546 -4.1649866 -4.1302052 -4.1101251 -4.0900464 -4.0419612 -3.9706764 -3.9531827 -4.0343666 -4.119719 -4.157383 -4.1605911 -4.1447163 -4.1252828 -4.1142325][-4.1881981 -4.1459694 -4.1073952 -4.0915728 -4.0897622 -4.0760984 -4.0468554 -4.0475874 -4.1058979 -4.1633291 -4.1846247 -4.1769757 -4.1530943 -4.1251807 -4.1040449][-4.1728716 -4.1428046 -4.1162949 -4.108098 -4.1164327 -4.1211672 -4.11564 -4.1205325 -4.1525187 -4.1828451 -4.19253 -4.181108 -4.15628 -4.1235027 -4.0916224][-4.1726489 -4.1593685 -4.1463938 -4.1458774 -4.1569824 -4.1640692 -4.1610708 -4.1587296 -4.1703515 -4.1833506 -4.1894712 -4.1818552 -4.1613631 -4.1255836 -4.0833383][-4.1852541 -4.1856394 -4.1820736 -4.1844978 -4.1914263 -4.1917324 -4.1807647 -4.1665864 -4.1626282 -4.1669397 -4.1734395 -4.1743674 -4.162467 -4.1304703 -4.087513][-4.19899 -4.2083282 -4.2082109 -4.207129 -4.2091413 -4.2054262 -4.189898 -4.1670995 -4.1504407 -4.1435013 -4.1453028 -4.1522107 -4.1515331 -4.1324229 -4.1025968][-4.2089262 -4.2247658 -4.2281876 -4.2275105 -4.2270126 -4.2220445 -4.204061 -4.1773767 -4.151505 -4.130888 -4.1215878 -4.1288862 -4.1357436 -4.1321158 -4.1219788]]...]
INFO - root - 2017-12-07 18:03:18.406540: step 37810, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 55h:02m:25s remains)
INFO - root - 2017-12-07 18:03:25.155585: step 37820, loss = 2.07, batch loss = 2.02 (13.1 examples/sec; 0.611 sec/batch; 49h:58m:27s remains)
INFO - root - 2017-12-07 18:03:32.068853: step 37830, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 57h:24m:18s remains)
INFO - root - 2017-12-07 18:03:38.920674: step 37840, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 58h:34m:38s remains)
INFO - root - 2017-12-07 18:03:45.700347: step 37850, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:35m:23s remains)
INFO - root - 2017-12-07 18:03:52.564931: step 37860, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 55h:34m:36s remains)
INFO - root - 2017-12-07 18:03:59.307856: step 37870, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 51h:47m:01s remains)
INFO - root - 2017-12-07 18:04:06.176767: step 37880, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 54h:18m:57s remains)
INFO - root - 2017-12-07 18:04:13.091060: step 37890, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 58h:31m:09s remains)
INFO - root - 2017-12-07 18:04:19.810256: step 37900, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 56h:12m:45s remains)
2017-12-07 18:04:20.612612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.299911 -4.28915 -4.2606816 -4.2229495 -4.1806569 -4.1333785 -4.0814967 -4.0411797 -4.0582485 -4.0956721 -4.1213613 -4.1156621 -4.0815125 -4.0765972 -4.086235][-4.2944865 -4.2803197 -4.2448959 -4.1979809 -4.1470962 -4.0915079 -4.0328345 -4.002562 -4.0447049 -4.09846 -4.1282978 -4.1253023 -4.0874534 -4.0722151 -4.0760922][-4.2926683 -4.2797284 -4.24614 -4.1997237 -4.1500092 -4.0909061 -4.0244837 -3.9984107 -4.0558438 -4.1159463 -4.1443257 -4.1422691 -4.1119957 -4.0953455 -4.0922141][-4.2946544 -4.2865648 -4.2600479 -4.2225776 -4.1774344 -4.1157823 -4.047296 -4.0247231 -4.0866818 -4.1477175 -4.1740537 -4.1716375 -4.1582465 -4.1421466 -4.1239038][-4.2998009 -4.2959614 -4.2735672 -4.240171 -4.1978235 -4.134902 -4.0634589 -4.0379343 -4.0945969 -4.1522837 -4.1779504 -4.1782122 -4.182477 -4.1787171 -4.1571407][-4.3056 -4.301033 -4.2748289 -4.2342281 -4.1861911 -4.12232 -4.0444007 -3.9986627 -4.0263171 -4.0745668 -4.1069632 -4.1244221 -4.1498251 -4.1658874 -4.1522646][-4.3056087 -4.2909422 -4.2539124 -4.1999125 -4.1426139 -4.073823 -3.9827495 -3.8997467 -3.8713152 -3.8990331 -3.962683 -4.0170236 -4.0673776 -4.1024537 -4.1003094][-4.2917447 -4.2656674 -4.2195458 -4.1560812 -4.086462 -4.0070467 -3.8979726 -3.7820828 -3.6931064 -3.7001185 -3.8258061 -3.9232743 -3.9918928 -4.0393648 -4.0491447][-4.2730894 -4.2425823 -4.1975393 -4.1369805 -4.0634928 -3.9821198 -3.8686917 -3.7561889 -3.685729 -3.7052379 -3.8404622 -3.9335732 -3.9950094 -4.0436177 -4.059463][-4.2679782 -4.2434883 -4.2082677 -4.1648808 -4.1057329 -4.0341043 -3.9404693 -3.8646412 -3.8557854 -3.8981328 -3.9733062 -4.0249491 -4.070014 -4.1127605 -4.1311259][-4.2780337 -4.2670717 -4.2445474 -4.2140169 -4.1725941 -4.1170931 -4.0537748 -4.0081234 -4.0329156 -4.0765076 -4.108243 -4.12977 -4.1610336 -4.1943207 -4.2060905][-4.2943554 -4.294692 -4.2837181 -4.2624907 -4.2333031 -4.1912127 -4.1524777 -4.1229529 -4.1523557 -4.1906772 -4.2136135 -4.225636 -4.2426805 -4.2571459 -4.2540231][-4.3081861 -4.3139176 -4.3095684 -4.2948484 -4.2728944 -4.2412252 -4.2167196 -4.1980853 -4.2257829 -4.2592664 -4.282033 -4.2891169 -4.2950268 -4.2908835 -4.2764544][-4.3185282 -4.3225236 -4.3173094 -4.304523 -4.2870026 -4.2639303 -4.247036 -4.23617 -4.2613468 -4.28533 -4.2989345 -4.2986078 -4.2956104 -4.2803226 -4.2585535][-4.3214927 -4.3187408 -4.3052769 -4.28757 -4.2707319 -4.2504153 -4.2349277 -4.230422 -4.25665 -4.2731867 -4.2812476 -4.2805796 -4.2734652 -4.2553043 -4.2325988]]...]
INFO - root - 2017-12-07 18:04:27.359061: step 37910, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.714 sec/batch; 58h:28m:01s remains)
INFO - root - 2017-12-07 18:04:34.295617: step 37920, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 58h:26m:41s remains)
INFO - root - 2017-12-07 18:04:41.061404: step 37930, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 54h:36m:03s remains)
INFO - root - 2017-12-07 18:04:47.837981: step 37940, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.610 sec/batch; 49h:53m:28s remains)
INFO - root - 2017-12-07 18:04:54.745551: step 37950, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 55h:35m:59s remains)
INFO - root - 2017-12-07 18:05:01.566362: step 37960, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 58h:32m:08s remains)
INFO - root - 2017-12-07 18:05:08.411772: step 37970, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 58h:07m:11s remains)
INFO - root - 2017-12-07 18:05:15.160364: step 37980, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 55h:30m:36s remains)
INFO - root - 2017-12-07 18:05:21.926112: step 37990, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 50h:32m:44s remains)
INFO - root - 2017-12-07 18:05:28.703240: step 38000, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.675 sec/batch; 55h:15m:03s remains)
2017-12-07 18:05:29.457616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2115517 -4.1943126 -4.1788497 -4.1656995 -4.1556978 -4.145793 -4.1539974 -4.1550632 -4.1482978 -4.1651583 -4.1867957 -4.1877317 -4.1692214 -4.1431203 -4.1222315][-4.2270808 -4.2092638 -4.188869 -4.1626873 -4.1389637 -4.1232519 -4.1322327 -4.1326981 -4.1209903 -4.1366825 -4.1652575 -4.1737657 -4.1599336 -4.1354413 -4.109725][-4.2431726 -4.2212992 -4.1920552 -4.1598334 -4.1284127 -4.1100183 -4.1176338 -4.1115637 -4.0908461 -4.1029644 -4.1405668 -4.1629958 -4.161943 -4.1436257 -4.11498][-4.2590404 -4.2320094 -4.1918793 -4.1521277 -4.1155071 -4.0933471 -4.0950804 -4.0843821 -4.0569005 -4.0650139 -4.1164808 -4.1579132 -4.1666226 -4.1506171 -4.1175203][-4.2612514 -4.2325006 -4.1859708 -4.1329904 -4.0795741 -4.037477 -4.0278959 -4.0349574 -4.0279312 -4.0536652 -4.1224155 -4.1748066 -4.1809154 -4.15439 -4.1102486][-4.2477303 -4.2167296 -4.1602917 -4.0947566 -4.0191193 -3.948715 -3.921592 -3.9581292 -3.9999027 -4.0588593 -4.140821 -4.192306 -4.1908441 -4.1531124 -4.10747][-4.2407594 -4.2108111 -4.1537418 -4.0921435 -4.01301 -3.9297347 -3.8898468 -3.9319286 -4.0019832 -4.0748529 -4.1532989 -4.1958418 -4.1926541 -4.1545315 -4.1163473][-4.24724 -4.2240782 -4.1795549 -4.1353054 -4.0786166 -4.0118775 -3.9741514 -3.9938645 -4.0512972 -4.1132193 -4.1736827 -4.2016482 -4.1931281 -4.1606307 -4.1319342][-4.2652464 -4.2461076 -4.212646 -4.1833873 -4.1498165 -4.1045842 -4.0739536 -4.0732226 -4.1032491 -4.1460614 -4.1859403 -4.1994491 -4.1859508 -4.1618514 -4.1459932][-4.293726 -4.2810478 -4.2570548 -4.2366982 -4.2177849 -4.1855664 -4.1542048 -4.1423221 -4.1545725 -4.1797638 -4.2012229 -4.2015514 -4.1835423 -4.166048 -4.1573391][-4.3117609 -4.3046131 -4.2887254 -4.276752 -4.2668142 -4.2428565 -4.2106981 -4.1929073 -4.1995134 -4.2163239 -4.2281032 -4.2220473 -4.2008634 -4.1819291 -4.1718917][-4.3250952 -4.32283 -4.312942 -4.3041348 -4.2964821 -4.2807989 -4.2584925 -4.2433147 -4.2479053 -4.2575231 -4.2608304 -4.2513127 -4.2281342 -4.2039666 -4.19082][-4.3409314 -4.3394804 -4.3348813 -4.3301058 -4.3220482 -4.3101344 -4.3006392 -4.2963362 -4.2989397 -4.3019462 -4.2984581 -4.2847686 -4.2577295 -4.2312927 -4.2133141][-4.3521986 -4.346869 -4.3378081 -4.3304033 -4.318779 -4.3096151 -4.3134713 -4.32387 -4.331378 -4.331418 -4.3267469 -4.3123145 -4.2886257 -4.262867 -4.2355957][-4.3441529 -4.3387818 -4.3247342 -4.3080091 -4.2872028 -4.2733989 -4.2838373 -4.3050942 -4.3186688 -4.3207555 -4.3151326 -4.3022132 -4.2854161 -4.2679596 -4.2371335]]...]
INFO - root - 2017-12-07 18:05:36.217751: step 38010, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 54h:10m:30s remains)
INFO - root - 2017-12-07 18:05:43.033462: step 38020, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 52h:18m:48s remains)
INFO - root - 2017-12-07 18:05:49.795657: step 38030, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 59h:48m:24s remains)
INFO - root - 2017-12-07 18:05:56.748179: step 38040, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.694 sec/batch; 56h:46m:50s remains)
INFO - root - 2017-12-07 18:06:03.600683: step 38050, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 54h:41m:35s remains)
INFO - root - 2017-12-07 18:06:10.501266: step 38060, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 53h:36m:15s remains)
INFO - root - 2017-12-07 18:06:17.247684: step 38070, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 55h:27m:10s remains)
INFO - root - 2017-12-07 18:06:24.039783: step 38080, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 52h:21m:30s remains)
INFO - root - 2017-12-07 18:06:30.868841: step 38090, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 53h:22m:59s remains)
INFO - root - 2017-12-07 18:06:37.423108: step 38100, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 51h:42m:48s remains)
2017-12-07 18:06:38.160199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0938821 -4.0915284 -4.1118126 -4.1599422 -4.2095737 -4.2185955 -4.2066178 -4.1950564 -4.1682787 -4.1233368 -4.0721407 -4.0516343 -4.06263 -4.0855794 -4.1168718][-4.1024308 -4.106586 -4.1289697 -4.1765518 -4.2161741 -4.20935 -4.1873937 -4.1680307 -4.1363463 -4.0875139 -4.0304723 -4.0045543 -4.0186696 -4.053103 -4.0944824][-4.1309066 -4.1392279 -4.1576395 -4.193347 -4.2132277 -4.1886544 -4.1561966 -4.1306319 -4.1010656 -4.0586333 -4.0049605 -3.9808369 -4.0027537 -4.048636 -4.1070223][-4.1756964 -4.1853156 -4.1977577 -4.2136431 -4.2092924 -4.1639075 -4.1130648 -4.0805883 -4.069943 -4.0557785 -4.0209684 -4.0028887 -4.0272875 -4.0769563 -4.1466975][-4.2274284 -4.2372537 -4.2345848 -4.2256165 -4.1973758 -4.1286054 -4.054029 -4.0159593 -4.0320125 -4.0572619 -4.0566287 -4.0579538 -4.0870042 -4.1337461 -4.1951137][-4.2626309 -4.2650213 -4.2433443 -4.2101431 -4.159162 -4.0702419 -3.9749331 -3.9342239 -3.981106 -4.0446687 -4.075233 -4.0977926 -4.1330419 -4.1769552 -4.226511][-4.2695146 -4.2605824 -4.2213435 -4.1690784 -4.103148 -3.9986477 -3.8781915 -3.8323517 -3.9153733 -4.01599 -4.0698705 -4.1093559 -4.153842 -4.2004151 -4.2455134][-4.2654638 -4.2483935 -4.200295 -4.1375923 -4.07125 -3.9704213 -3.8389997 -3.7825086 -3.8746214 -3.9901004 -4.0570912 -4.1099267 -4.1605682 -4.2123051 -4.2567372][-4.2613997 -4.2437105 -4.2007513 -4.1458421 -4.0990467 -4.0283642 -3.9280808 -3.880178 -3.9392564 -4.0237503 -4.0808644 -4.1310477 -4.1774325 -4.224906 -4.2646952][-4.2683387 -4.2512045 -4.2160807 -4.1733212 -4.1425571 -4.101006 -4.0414224 -4.0147214 -4.0500174 -4.1021948 -4.1413074 -4.1817327 -4.2174768 -4.2507434 -4.2797027][-4.2915812 -4.2745905 -4.2447915 -4.2095137 -4.1852369 -4.1614084 -4.1316037 -4.1218429 -4.1452909 -4.1808176 -4.2111397 -4.244585 -4.2699385 -4.2861619 -4.30249][-4.3163476 -4.3033338 -4.2803006 -4.2537408 -4.2339597 -4.2184796 -4.2060871 -4.2072606 -4.2230444 -4.2458153 -4.2674861 -4.2943563 -4.3106418 -4.3149881 -4.3212285][-4.3310218 -4.3244109 -4.3106403 -4.2938304 -4.2784657 -4.2652149 -4.2556405 -4.2570558 -4.2673345 -4.2833524 -4.2987595 -4.3181057 -4.3283257 -4.3281407 -4.3291225][-4.3349981 -4.3332167 -4.3259835 -4.3167305 -4.3071804 -4.2977581 -4.2892003 -4.288805 -4.2943172 -4.302949 -4.3126183 -4.3252687 -4.3310308 -4.3302059 -4.3300819][-4.335381 -4.3349514 -4.3312235 -4.3264031 -4.3216205 -4.3151612 -4.3087492 -4.3084974 -4.3121924 -4.3169317 -4.3229103 -4.3301978 -4.3326759 -4.3324537 -4.3321829]]...]
INFO - root - 2017-12-07 18:06:44.941975: step 38110, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 56h:57m:35s remains)
INFO - root - 2017-12-07 18:06:51.702054: step 38120, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 54h:21m:15s remains)
INFO - root - 2017-12-07 18:06:58.460854: step 38130, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 51h:07m:03s remains)
INFO - root - 2017-12-07 18:07:05.195814: step 38140, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 56h:33m:40s remains)
INFO - root - 2017-12-07 18:07:11.999975: step 38150, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.714 sec/batch; 58h:24m:00s remains)
INFO - root - 2017-12-07 18:07:18.783269: step 38160, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.669 sec/batch; 54h:44m:08s remains)
INFO - root - 2017-12-07 18:07:25.597650: step 38170, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 55h:18m:33s remains)
INFO - root - 2017-12-07 18:07:32.396311: step 38180, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 53h:26m:02s remains)
INFO - root - 2017-12-07 18:07:39.288708: step 38190, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.738 sec/batch; 60h:22m:20s remains)
INFO - root - 2017-12-07 18:07:45.974112: step 38200, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:28m:55s remains)
2017-12-07 18:07:46.736841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3183651 -4.3097563 -4.2996655 -4.2900157 -4.2822976 -4.2788706 -4.2761979 -4.2730255 -4.2732835 -4.2742076 -4.273026 -4.2746172 -4.2787733 -4.2804308 -4.2810521][-4.296339 -4.279016 -4.2651434 -4.2525196 -4.2426972 -4.2367039 -4.2349997 -4.2311454 -4.232398 -4.236918 -4.2370286 -4.2420821 -4.2506113 -4.2495189 -4.2447553][-4.2712212 -4.2496157 -4.2356 -4.221097 -4.2060676 -4.1934338 -4.1916718 -4.1882563 -4.1897573 -4.1961417 -4.2005448 -4.2094765 -4.2194071 -4.2189655 -4.2135053][-4.2438445 -4.217885 -4.1996145 -4.1785159 -4.1529303 -4.1324883 -4.1257262 -4.1223278 -4.1299958 -4.1460338 -4.1609535 -4.1723509 -4.1848593 -4.1884632 -4.1900868][-4.22377 -4.1896009 -4.1593442 -4.1240315 -4.085628 -4.0563235 -4.0337873 -4.0215688 -4.0405741 -4.0752611 -4.0957527 -4.1085286 -4.1281505 -4.1382456 -4.1498904][-4.2114754 -4.1569614 -4.1032515 -4.0540509 -4.0046363 -3.9652603 -3.914283 -3.867223 -3.9022748 -3.9810133 -4.0154171 -4.0252047 -4.0492592 -4.0661855 -4.0903645][-4.1821737 -4.0938377 -3.9962559 -3.9132216 -3.858942 -3.8146446 -3.7231822 -3.6090138 -3.6643112 -3.8288875 -3.903235 -3.9197295 -3.9480178 -3.9839294 -4.0276766][-4.1362824 -4.0049887 -3.8509302 -3.7289762 -3.6717207 -3.6256251 -3.4737272 -3.2508545 -3.35209 -3.6481643 -3.7815609 -3.8127546 -3.8603346 -3.9258218 -3.9883876][-4.1077232 -3.9554067 -3.7746615 -3.6420851 -3.6027482 -3.5729177 -3.4118052 -3.1583278 -3.2875447 -3.6261187 -3.7751968 -3.8103862 -3.8713944 -3.9403834 -3.9933689][-4.1200848 -3.9943712 -3.848968 -3.7598336 -3.74659 -3.7442937 -3.6581762 -3.5158041 -3.5967369 -3.8035665 -3.9003687 -3.9263871 -3.9762022 -4.0241518 -4.0465593][-4.1542249 -4.0791554 -3.9996958 -3.9584463 -3.9499536 -3.95501 -3.9237187 -3.8679314 -3.8996081 -3.9898267 -4.0470176 -4.0732374 -4.1014404 -4.1203718 -4.1224532][-4.1960692 -4.1649618 -4.140306 -4.1261644 -4.1134067 -4.1128397 -4.1034784 -4.0782251 -4.0915074 -4.1346354 -4.1650162 -4.1873617 -4.2010713 -4.2054539 -4.2059255][-4.2430787 -4.2331161 -4.2333198 -4.2292852 -4.2240396 -4.2251091 -4.2212934 -4.206356 -4.2152495 -4.2415156 -4.2558341 -4.2652769 -4.2722559 -4.2782812 -4.28213][-4.2835016 -4.2807965 -4.2852683 -4.2837911 -4.2863035 -4.2904596 -4.2851911 -4.2725263 -4.274415 -4.290031 -4.2961912 -4.3009462 -4.3098083 -4.3188734 -4.3250823][-4.3118882 -4.3105445 -4.3130674 -4.3143163 -4.3209543 -4.3254485 -4.321785 -4.3142138 -4.3151965 -4.320828 -4.3194127 -4.3208609 -4.3291745 -4.3377738 -4.3418589]]...]
INFO - root - 2017-12-07 18:07:53.460267: step 38210, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 55h:01m:25s remains)
INFO - root - 2017-12-07 18:08:00.256048: step 38220, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.694 sec/batch; 56h:45m:40s remains)
INFO - root - 2017-12-07 18:08:06.979684: step 38230, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 56h:28m:10s remains)
INFO - root - 2017-12-07 18:08:13.871378: step 38240, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 54h:25m:40s remains)
INFO - root - 2017-12-07 18:08:20.608785: step 38250, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 51h:42m:36s remains)
INFO - root - 2017-12-07 18:08:27.356249: step 38260, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 53h:47m:16s remains)
INFO - root - 2017-12-07 18:08:34.361424: step 38270, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.758 sec/batch; 61h:54m:58s remains)
INFO - root - 2017-12-07 18:08:41.188417: step 38280, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 54h:57m:34s remains)
INFO - root - 2017-12-07 18:08:47.964240: step 38290, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 56h:04m:25s remains)
INFO - root - 2017-12-07 18:08:54.704660: step 38300, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 52h:35m:22s remains)
2017-12-07 18:08:55.461946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2885818 -4.281045 -4.26198 -4.2396765 -4.22736 -4.2214055 -4.2139621 -4.2095294 -4.1952796 -4.1599059 -4.1285944 -4.1167355 -4.1211329 -4.1319275 -4.1371908][-4.2919188 -4.28174 -4.261456 -4.2409105 -4.2336397 -4.2289419 -4.222507 -4.224328 -4.2165313 -4.1806779 -4.1478381 -4.1378131 -4.1405 -4.1453228 -4.1470675][-4.2870283 -4.27613 -4.2583661 -4.2410984 -4.2372727 -4.2318339 -4.2227588 -4.2264338 -4.22477 -4.1904731 -4.1544318 -4.1482019 -4.1538382 -4.156404 -4.16125][-4.2712631 -4.2612309 -4.24864 -4.2347221 -4.2310195 -4.2207327 -4.2032242 -4.1981087 -4.1981831 -4.1719313 -4.1417432 -4.1442518 -4.1573949 -4.1634254 -4.1732292][-4.2549953 -4.245523 -4.2363009 -4.2185931 -4.2021689 -4.1781712 -4.1484709 -4.1322036 -4.1348529 -4.1246405 -4.1117821 -4.1274743 -4.1489687 -4.1607356 -4.1776576][-4.2228179 -4.2115879 -4.2010279 -4.16917 -4.130374 -4.0911484 -4.0508618 -4.0290804 -4.0397377 -4.0550852 -4.0748453 -4.11367 -4.1469741 -4.1639752 -4.1853442][-4.1638012 -4.1469893 -4.133534 -4.0934987 -4.0376759 -3.9814835 -3.9255097 -3.9039714 -3.9427948 -4.0030208 -4.0666766 -4.1305485 -4.1693497 -4.1872835 -4.2049861][-4.0971942 -4.0784492 -4.0677204 -4.030149 -3.9714136 -3.9133701 -3.8598852 -3.8571856 -3.931412 -4.0213318 -4.1005344 -4.1651955 -4.1984544 -4.21083 -4.2244983][-4.0519266 -4.044703 -4.0478454 -4.0302033 -4.0013642 -3.9751229 -3.9505472 -3.9604909 -4.0213032 -4.083497 -4.13532 -4.1778216 -4.2010059 -4.2110891 -4.226624][-4.0630307 -4.0731249 -4.0879726 -4.0832314 -4.0721154 -4.0688992 -4.0681739 -4.0781455 -4.1087675 -4.1330042 -4.1474113 -4.1619663 -4.1754928 -4.1911836 -4.2117529][-4.0931511 -4.1103525 -4.1266069 -4.1293435 -4.1285915 -4.135221 -4.1444058 -4.1504192 -4.1573853 -4.1502705 -4.1368814 -4.1317143 -4.1393428 -4.1635461 -4.191071][-4.1190643 -4.1377215 -4.1526508 -4.1594534 -4.1591988 -4.1634507 -4.16901 -4.1661687 -4.155952 -4.1331506 -4.1108966 -4.1061049 -4.1205459 -4.1532702 -4.1856542][-4.1451406 -4.1558876 -4.1656532 -4.1689315 -4.1652489 -4.1637182 -4.1616797 -4.148982 -4.1301627 -4.1082363 -4.0983348 -4.1104641 -4.1379585 -4.1719985 -4.198916][-4.1679964 -4.1656356 -4.1677709 -4.1681871 -4.1610942 -4.1545687 -4.1483717 -4.138917 -4.127017 -4.1161308 -4.1142564 -4.1288042 -4.15041 -4.1724205 -4.189054][-4.17359 -4.1656356 -4.1646137 -4.1690135 -4.1657681 -4.1577377 -4.1504936 -4.1436262 -4.1354361 -4.1303329 -4.1291409 -4.1340714 -4.1415949 -4.1496091 -4.1577144]]...]
INFO - root - 2017-12-07 18:09:02.187743: step 38310, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 52h:51m:03s remains)
INFO - root - 2017-12-07 18:09:08.967895: step 38320, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 51h:40m:05s remains)
INFO - root - 2017-12-07 18:09:15.643428: step 38330, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 51h:27m:27s remains)
INFO - root - 2017-12-07 18:09:22.422560: step 38340, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 0.757 sec/batch; 61h:49m:25s remains)
INFO - root - 2017-12-07 18:09:29.230566: step 38350, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 58h:06m:07s remains)
INFO - root - 2017-12-07 18:09:35.972951: step 38360, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 55h:19m:27s remains)
INFO - root - 2017-12-07 18:09:42.851619: step 38370, loss = 2.10, batch loss = 2.04 (12.9 examples/sec; 0.622 sec/batch; 50h:48m:55s remains)
INFO - root - 2017-12-07 18:09:49.518241: step 38380, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 55h:13m:45s remains)
INFO - root - 2017-12-07 18:09:56.321269: step 38390, loss = 2.06, batch loss = 2.01 (10.5 examples/sec; 0.764 sec/batch; 62h:22m:40s remains)
INFO - root - 2017-12-07 18:10:02.945266: step 38400, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 56h:10m:00s remains)
2017-12-07 18:10:03.640060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3254943 -4.3254094 -4.3237133 -4.3247657 -4.3254156 -4.3228726 -4.3184958 -4.3160591 -4.3173566 -4.3194861 -4.3217878 -4.3243179 -4.3249311 -4.3252711 -4.3254943][-4.3141408 -4.31552 -4.3149929 -4.3189249 -4.3207827 -4.314189 -4.3054514 -4.3038445 -4.3081207 -4.3120933 -4.3167419 -4.321126 -4.3226495 -4.3226495 -4.3188825][-4.2893438 -4.286386 -4.2822256 -4.2871661 -4.2881994 -4.2768373 -4.2675996 -4.2735634 -4.2849545 -4.2928462 -4.3005962 -4.3093653 -4.316257 -4.3148479 -4.3036127][-4.2422247 -4.2281418 -4.2203007 -4.2257771 -4.2240572 -4.207562 -4.2008739 -4.2148814 -4.2327404 -4.248281 -4.263536 -4.2791171 -4.2928381 -4.291688 -4.2723808][-4.18181 -4.15728 -4.1495528 -4.1590853 -4.1592231 -4.1411314 -4.1315689 -4.1411967 -4.16594 -4.194417 -4.2190194 -4.2371778 -4.2533593 -4.2506504 -4.2259421][-4.1312542 -4.1029963 -4.0956578 -4.1007829 -4.0895739 -4.0674729 -4.0518618 -4.0555739 -4.0966244 -4.1465058 -4.1807089 -4.2015085 -4.2180758 -4.2120171 -4.1798143][-4.1033983 -4.07234 -4.0566907 -4.0387011 -4.0004706 -3.9626489 -3.9326937 -3.9328701 -4.003705 -4.0853853 -4.134429 -4.1617303 -4.1741109 -4.1649485 -4.1298871][-4.1174097 -4.0845547 -4.0549669 -4.013464 -3.9507189 -3.8960102 -3.8476763 -3.8350701 -3.9292932 -4.0310555 -4.0904741 -4.1198387 -4.1251774 -4.1151824 -4.08554][-4.16344 -4.1384764 -4.104373 -4.0622253 -4.00381 -3.9496605 -3.8914578 -3.8636811 -3.9433117 -4.0345387 -4.0965948 -4.1284862 -4.1333046 -4.1227388 -4.0935893][-4.1860495 -4.1730351 -4.1471829 -4.1224709 -4.0938134 -4.065526 -4.02386 -3.9920187 -4.0322118 -4.0945311 -4.1455207 -4.1774292 -4.1849365 -4.1747026 -4.1439948][-4.1793909 -4.1812258 -4.1671844 -4.1590514 -4.1554914 -4.1497788 -4.1300154 -4.10597 -4.1168385 -4.15133 -4.1831756 -4.2055483 -4.2183847 -4.2090845 -4.1756454][-4.1832061 -4.1936612 -4.1911616 -4.1888838 -4.1954741 -4.2043505 -4.2015157 -4.1893296 -4.1927533 -4.2057424 -4.2172074 -4.2330155 -4.2477679 -4.2396426 -4.2047687][-4.1902714 -4.1955514 -4.192677 -4.1974998 -4.217782 -4.2412405 -4.2517529 -4.2456446 -4.2462082 -4.2443862 -4.24081 -4.2525535 -4.2694507 -4.2691803 -4.2442279][-4.2141137 -4.2092028 -4.2008243 -4.2074232 -4.2309031 -4.2557826 -4.2655849 -4.2589035 -4.2606468 -4.2566195 -4.243371 -4.244905 -4.2589021 -4.2718759 -4.2638783][-4.2533107 -4.2406278 -4.2262816 -4.2287855 -4.2444286 -4.2544274 -4.2534208 -4.2450814 -4.2581439 -4.2627177 -4.2460976 -4.2319207 -4.2327137 -4.2505636 -4.2524462]]...]
INFO - root - 2017-12-07 18:10:10.467949: step 38410, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 60h:39m:18s remains)
INFO - root - 2017-12-07 18:10:17.326670: step 38420, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 58h:01m:43s remains)
INFO - root - 2017-12-07 18:10:24.134407: step 38430, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 56h:53m:00s remains)
INFO - root - 2017-12-07 18:10:30.878374: step 38440, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.618 sec/batch; 50h:27m:24s remains)
INFO - root - 2017-12-07 18:10:37.739389: step 38450, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 53h:30m:09s remains)
INFO - root - 2017-12-07 18:10:44.587581: step 38460, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 60h:34m:45s remains)
INFO - root - 2017-12-07 18:10:51.441952: step 38470, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.754 sec/batch; 61h:34m:13s remains)
INFO - root - 2017-12-07 18:10:58.274142: step 38480, loss = 2.04, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 51h:10m:26s remains)
INFO - root - 2017-12-07 18:11:04.990453: step 38490, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 51h:44m:00s remains)
INFO - root - 2017-12-07 18:11:11.620365: step 38500, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.737 sec/batch; 60h:13m:32s remains)
2017-12-07 18:11:12.326386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2690148 -4.280231 -4.2892981 -4.2976151 -4.2981524 -4.2902651 -4.2837396 -4.2830963 -4.2860551 -4.2887807 -4.2925415 -4.3004603 -4.3112192 -4.3226237 -4.3314052][-4.21339 -4.2245493 -4.2370176 -4.2510972 -4.2543106 -4.2480249 -4.24496 -4.2481894 -4.2527428 -4.2549496 -4.2580576 -4.2680349 -4.2856765 -4.3060403 -4.3212676][-4.1485252 -4.1591725 -4.1806564 -4.2033715 -4.209166 -4.2028275 -4.2017894 -4.2064786 -4.2108345 -4.2113156 -4.2148438 -4.22842 -4.2527018 -4.2809615 -4.3035283][-4.1072388 -4.112957 -4.1415215 -4.16874 -4.1731877 -4.1613703 -4.1577563 -4.166296 -4.1732049 -4.1714406 -4.1748152 -4.1895118 -4.2149696 -4.2475257 -4.2777081][-4.1010189 -4.0948482 -4.1181231 -4.1404452 -4.1384797 -4.116446 -4.0987635 -4.1061974 -4.1184945 -4.1223311 -4.1278338 -4.1421924 -4.16767 -4.2043877 -4.2439532][-4.1246181 -4.10924 -4.1194444 -4.1293273 -4.1108127 -4.0612369 -4.0151906 -4.0179672 -4.038394 -4.0555024 -4.0654845 -4.0797763 -4.1094584 -4.1573782 -4.2124887][-4.1605377 -4.1439643 -4.1442003 -4.1345043 -4.0864143 -3.9990401 -3.9196754 -3.9154205 -3.9536512 -3.9910731 -4.0105915 -4.0304074 -4.0697718 -4.1293464 -4.195106][-4.1980076 -4.1821337 -4.1759272 -4.1467562 -4.0770059 -3.9719076 -3.8816957 -3.8727853 -3.918031 -3.9701035 -4.0031796 -4.0317883 -4.0768 -4.1343813 -4.1946559][-4.2155643 -4.1999288 -4.1914868 -4.1592441 -4.0904121 -3.9991989 -3.9244938 -3.9206116 -3.9634037 -4.0152235 -4.0559211 -4.0897841 -4.1324325 -4.1766624 -4.2202291][-4.2236257 -4.2120595 -4.2056484 -4.1822977 -4.1326308 -4.0736775 -4.0209565 -4.013588 -4.0421057 -4.0846572 -4.1209393 -4.1524811 -4.1887093 -4.2221956 -4.2529616][-4.2358623 -4.2320914 -4.2319903 -4.2166328 -4.1843638 -4.1480064 -4.1111431 -4.1010637 -4.1173806 -4.1476779 -4.1723986 -4.1942034 -4.2221594 -4.2507954 -4.2749758][-4.24861 -4.2517004 -4.2535253 -4.2418303 -4.2178178 -4.1917739 -4.1661596 -4.1613874 -4.170289 -4.1872721 -4.1982937 -4.2088709 -4.2315693 -4.2596359 -4.2829189][-4.2580075 -4.2599282 -4.2612276 -4.2524333 -4.2321815 -4.2109828 -4.1945848 -4.1959043 -4.200984 -4.2073088 -4.2079325 -4.2120385 -4.2324982 -4.2606726 -4.2855921][-4.2587891 -4.2552872 -4.2530422 -4.2460132 -4.2319942 -4.2169175 -4.2099543 -4.2152038 -4.2215862 -4.2244616 -4.220654 -4.22163 -4.2378979 -4.2639127 -4.28892][-4.2585773 -4.2515612 -4.2475872 -4.2426815 -4.2341895 -4.2255912 -4.2239819 -4.2297564 -4.2370505 -4.2418332 -4.2394028 -4.2402077 -4.252533 -4.2734346 -4.2960477]]...]
INFO - root - 2017-12-07 18:11:19.106349: step 38510, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 51h:56m:24s remains)
INFO - root - 2017-12-07 18:11:25.943174: step 38520, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 52h:39m:15s remains)
INFO - root - 2017-12-07 18:11:32.718815: step 38530, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.731 sec/batch; 59h:39m:39s remains)
INFO - root - 2017-12-07 18:11:39.484324: step 38540, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.767 sec/batch; 62h:35m:48s remains)
INFO - root - 2017-12-07 18:11:46.289488: step 38550, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 53h:33m:42s remains)
INFO - root - 2017-12-07 18:11:53.019579: step 38560, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.620 sec/batch; 50h:37m:41s remains)
INFO - root - 2017-12-07 18:11:59.927553: step 38570, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 52h:32m:45s remains)
INFO - root - 2017-12-07 18:12:06.796233: step 38580, loss = 2.03, batch loss = 1.98 (10.9 examples/sec; 0.732 sec/batch; 59h:46m:24s remains)
INFO - root - 2017-12-07 18:12:13.490592: step 38590, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 58h:17m:28s remains)
INFO - root - 2017-12-07 18:12:20.148425: step 38600, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 56h:24m:46s remains)
2017-12-07 18:12:20.885018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2849216 -4.2963324 -4.3153386 -4.3342285 -4.3421631 -4.3254228 -4.2947483 -4.2755485 -4.2731924 -4.2771339 -4.2809634 -4.2802944 -4.2805023 -4.2912817 -4.3129644][-4.2660947 -4.2864442 -4.311121 -4.3312621 -4.3321681 -4.301909 -4.2549052 -4.2290325 -4.2355046 -4.2537308 -4.2695861 -4.2809672 -4.2885332 -4.2955875 -4.3134704][-4.2485104 -4.2720571 -4.3006134 -4.3167195 -4.3076458 -4.2640476 -4.2019048 -4.1714964 -4.1922965 -4.2305188 -4.2628379 -4.2876444 -4.299119 -4.3002567 -4.310576][-4.24286 -4.2620816 -4.2883058 -4.2954569 -4.270668 -4.20925 -4.1320152 -4.105329 -4.1507487 -4.2140512 -4.2610383 -4.2905893 -4.2981186 -4.2886004 -4.2909689][-4.2467546 -4.2589426 -4.2791085 -4.2745161 -4.2314682 -4.1537762 -4.0653582 -4.0467939 -4.1202483 -4.208714 -4.2642269 -4.2878342 -4.2859888 -4.2670417 -4.2626071][-4.2591805 -4.2586122 -4.2668991 -4.2533298 -4.1950178 -4.0948172 -3.9867997 -3.9779291 -4.086978 -4.2028961 -4.2684083 -4.2827234 -4.2659945 -4.2388062 -4.2328763][-4.2780643 -4.267909 -4.2605391 -4.237144 -4.1675906 -4.0364094 -3.8946905 -3.8940096 -4.0485883 -4.1955414 -4.2705922 -4.2746811 -4.2409739 -4.2071266 -4.2011266][-4.2985449 -4.2900271 -4.2740445 -4.241323 -4.1698337 -4.0193586 -3.8407946 -3.8403089 -4.030241 -4.1939564 -4.2681646 -4.2616673 -4.2199459 -4.1845174 -4.1783676][-4.3088512 -4.3083315 -4.29411 -4.26217 -4.2015519 -4.0725169 -3.8977299 -3.8773026 -4.054492 -4.2059245 -4.2695594 -4.2565894 -4.2094212 -4.1699886 -4.1625576][-4.3136153 -4.3186584 -4.3096495 -4.2872233 -4.2480345 -4.1624894 -4.0382729 -4.0021577 -4.1167345 -4.2251282 -4.2639132 -4.2392058 -4.1868529 -4.1407623 -4.1352272][-4.3152723 -4.3191004 -4.314589 -4.3031325 -4.2819839 -4.2316256 -4.1540594 -4.123641 -4.1850338 -4.246419 -4.2588172 -4.2248607 -4.1667247 -4.1126394 -4.106113][-4.3037734 -4.30527 -4.3081713 -4.3069 -4.297801 -4.2702489 -4.2243781 -4.2062869 -4.2404828 -4.2716889 -4.2690535 -4.2319756 -4.1673417 -4.102931 -4.0856638][-4.2953339 -4.2871623 -4.2898269 -4.2974739 -4.3018045 -4.2932158 -4.2700348 -4.2589231 -4.2752562 -4.29022 -4.2824879 -4.2451243 -4.1789837 -4.1144471 -4.0975509][-4.2928629 -4.2778654 -4.27501 -4.2812629 -4.2967396 -4.3040833 -4.2945824 -4.2877817 -4.2952623 -4.3026481 -4.2932725 -4.2574029 -4.1998949 -4.1467872 -4.1378031][-4.2848778 -4.271945 -4.267396 -4.2724266 -4.2889132 -4.3027849 -4.3034592 -4.3003869 -4.30415 -4.309154 -4.306417 -4.28555 -4.2449908 -4.2028403 -4.1929665]]...]
INFO - root - 2017-12-07 18:12:27.688346: step 38610, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 57h:33m:46s remains)
INFO - root - 2017-12-07 18:12:34.544194: step 38620, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.746 sec/batch; 60h:52m:06s remains)
INFO - root - 2017-12-07 18:12:41.272669: step 38630, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 55h:03m:18s remains)
INFO - root - 2017-12-07 18:12:47.934387: step 38640, loss = 2.07, batch loss = 2.01 (13.4 examples/sec; 0.596 sec/batch; 48h:39m:11s remains)
INFO - root - 2017-12-07 18:12:54.655998: step 38650, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 51h:04m:58s remains)
INFO - root - 2017-12-07 18:13:01.606480: step 38660, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:27m:26s remains)
INFO - root - 2017-12-07 18:13:08.471786: step 38670, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 58h:47m:22s remains)
INFO - root - 2017-12-07 18:13:15.450578: step 38680, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 57h:44m:54s remains)
INFO - root - 2017-12-07 18:13:22.182893: step 38690, loss = 2.06, batch loss = 2.00 (13.5 examples/sec; 0.595 sec/batch; 48h:31m:43s remains)
INFO - root - 2017-12-07 18:13:28.571850: step 38700, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:27m:39s remains)
2017-12-07 18:13:29.340099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1736374 -4.1768379 -4.1706548 -4.1613278 -4.156827 -4.1724463 -4.2005281 -4.2135582 -4.1993661 -4.1773052 -4.1618958 -4.1555095 -4.1591277 -4.1805072 -4.2077265][-4.1818337 -4.1860876 -4.1810417 -4.1764545 -4.1760011 -4.1896491 -4.2112403 -4.2141962 -4.1893616 -4.1678195 -4.1581597 -4.1546731 -4.1567245 -4.1756463 -4.2008219][-4.202281 -4.2064729 -4.2022409 -4.1995063 -4.1972728 -4.2046189 -4.2140646 -4.20446 -4.1698995 -4.1510715 -4.1529603 -4.1567903 -4.1618886 -4.1820531 -4.2091537][-4.2277846 -4.2297754 -4.2273269 -4.2243252 -4.2201858 -4.2197838 -4.21652 -4.1952815 -4.1553316 -4.1376319 -4.1440115 -4.1506071 -4.1641345 -4.1937213 -4.2292943][-4.2501364 -4.2533641 -4.2535076 -4.2467818 -4.2341537 -4.2219067 -4.2051072 -4.173749 -4.1334896 -4.1223574 -4.1324615 -4.1404147 -4.1656752 -4.2083831 -4.2534122][-4.2580872 -4.2634249 -4.2642994 -4.2526 -4.23094 -4.2079959 -4.1788158 -4.1350203 -4.0947208 -4.0921488 -4.1103659 -4.1279912 -4.1673007 -4.2203064 -4.2724371][-4.2508454 -4.2570662 -4.2571573 -4.2413421 -4.2138228 -4.1847243 -4.1478653 -4.0916715 -4.0506773 -4.0585728 -4.0907068 -4.1276364 -4.1790609 -4.2367706 -4.2904663][-4.2403755 -4.2414885 -4.235383 -4.2154436 -4.1868634 -4.160203 -4.1266627 -4.0715923 -4.0355091 -4.0512581 -4.0953045 -4.1472044 -4.2019153 -4.2582045 -4.30802][-4.234025 -4.226368 -4.2143359 -4.1954889 -4.1712351 -4.1491575 -4.1190524 -4.0736394 -4.0456572 -4.0662255 -4.1139078 -4.167778 -4.2215524 -4.2760735 -4.320415][-4.2403331 -4.2276917 -4.211791 -4.1953683 -4.1750231 -4.1537347 -4.1231985 -4.0862575 -4.0659394 -4.0877986 -4.1332417 -4.1837764 -4.2357354 -4.2891307 -4.3281965][-4.2605133 -4.2489691 -4.23295 -4.217154 -4.1987338 -4.1746745 -4.1433654 -4.1143022 -4.103302 -4.1242776 -4.1641092 -4.2090588 -4.2561121 -4.3042703 -4.3355374][-4.283577 -4.2796516 -4.2702374 -4.2578335 -4.24018 -4.2153497 -4.1861448 -4.1635914 -4.1566935 -4.1721449 -4.2037549 -4.2408309 -4.2812738 -4.3199987 -4.3417826][-4.3009768 -4.3035817 -4.3023705 -4.2958021 -4.2820368 -4.260006 -4.2349877 -4.2161446 -4.2099056 -4.2201681 -4.2436075 -4.272357 -4.3043156 -4.3316679 -4.3441906][-4.3145142 -4.3185263 -4.3221226 -4.3209333 -4.3128543 -4.2959061 -4.2746005 -4.2583437 -4.2525244 -4.2601032 -4.2783136 -4.3005667 -4.3231592 -4.3398962 -4.3453836][-4.3235993 -4.3276014 -4.3339248 -4.3367872 -4.3331866 -4.3217669 -4.3049226 -4.2916269 -4.286211 -4.2908463 -4.3052197 -4.322969 -4.3379068 -4.3467617 -4.3480611]]...]
INFO - root - 2017-12-07 18:13:36.036887: step 38710, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 53h:42m:55s remains)
INFO - root - 2017-12-07 18:13:42.892134: step 38720, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.723 sec/batch; 58h:59m:14s remains)
INFO - root - 2017-12-07 18:13:49.694367: step 38730, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 58h:08m:22s remains)
INFO - root - 2017-12-07 18:13:56.498063: step 38740, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 57h:35m:54s remains)
INFO - root - 2017-12-07 18:14:03.400136: step 38750, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 52h:42m:41s remains)
INFO - root - 2017-12-07 18:14:10.252313: step 38760, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 54h:20m:28s remains)
INFO - root - 2017-12-07 18:14:17.077427: step 38770, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 60h:35m:15s remains)
INFO - root - 2017-12-07 18:14:23.982205: step 38780, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 58h:18m:15s remains)
INFO - root - 2017-12-07 18:14:30.800775: step 38790, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 59h:51m:28s remains)
INFO - root - 2017-12-07 18:14:37.394564: step 38800, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 51h:44m:03s remains)
2017-12-07 18:14:38.195121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3397536 -4.3289285 -4.3164811 -4.3067732 -4.3044858 -4.3087325 -4.3139753 -4.3166556 -4.3154011 -4.3144026 -4.3172722 -4.3193731 -4.3170161 -4.3084583 -4.3038025][-4.3196573 -4.3013568 -4.2864985 -4.2766819 -4.2732844 -4.27666 -4.2793775 -4.2801504 -4.2770896 -4.2725353 -4.2761421 -4.2812071 -4.2804904 -4.2703185 -4.2642813][-4.3012271 -4.2759709 -4.2611804 -4.2554026 -4.2525592 -4.2535896 -4.2526774 -4.2470746 -4.2405095 -4.2349834 -4.23888 -4.2440844 -4.2428746 -4.2315784 -4.2246861][-4.2840166 -4.2517185 -4.2363186 -4.2296209 -4.2272353 -4.226234 -4.2221003 -4.21036 -4.201189 -4.1950145 -4.1955242 -4.1963506 -4.1962433 -4.1870346 -4.1818018][-4.2576089 -4.2210369 -4.1989832 -4.1864238 -4.1836252 -4.1796837 -4.1697912 -4.1532249 -4.1451235 -4.1388478 -4.1360564 -4.1334639 -4.1391783 -4.1379833 -4.1384525][-4.229579 -4.1878991 -4.1568379 -4.1345515 -4.123836 -4.1130857 -4.100657 -4.0856204 -4.0865507 -4.0896916 -4.0918365 -4.0914993 -4.1055231 -4.1105552 -4.1157479][-4.2166228 -4.1721592 -4.134944 -4.1008048 -4.074286 -4.0521965 -4.0367203 -4.0326366 -4.0530691 -4.0699821 -4.0784016 -4.0800543 -4.0976911 -4.0993733 -4.1007752][-4.2128553 -4.1726241 -4.1342249 -4.0936432 -4.0607104 -4.0344706 -4.0224819 -4.0331922 -4.0670853 -4.0860357 -4.0903215 -4.0897422 -4.1007628 -4.0917931 -4.0839849][-4.2145705 -4.1859112 -4.1515975 -4.114315 -4.0863228 -4.0697556 -4.06878 -4.0835533 -4.1134744 -4.1181536 -4.1165223 -4.1141734 -4.1182632 -4.1005177 -4.0847468][-4.2289004 -4.2088847 -4.1815519 -4.1561136 -4.1399188 -4.1337376 -4.1381965 -4.15016 -4.167675 -4.15969 -4.1550474 -4.153604 -4.1563845 -4.1398783 -4.1226344][-4.2460265 -4.2294054 -4.2080007 -4.1941886 -4.1888533 -4.1888156 -4.1951733 -4.2008758 -4.2025118 -4.1829014 -4.1751385 -4.1772861 -4.1857238 -4.1807103 -4.1692548][-4.269618 -4.254879 -4.2377772 -4.22903 -4.2281523 -4.2297821 -4.2357125 -4.2368751 -4.2290096 -4.2080731 -4.1981997 -4.2018042 -4.213665 -4.2193904 -4.2167134][-4.2949109 -4.2820339 -4.26755 -4.2614231 -4.2638359 -4.2661047 -4.2679591 -4.2657 -4.2582817 -4.2434373 -4.2360396 -4.2409868 -4.2550931 -4.26701 -4.2692738][-4.3145819 -4.3039312 -4.2931004 -4.2902575 -4.2943473 -4.2956948 -4.2940106 -4.2907238 -4.2859383 -4.2782612 -4.2747226 -4.2787881 -4.2924542 -4.3072414 -4.3129129][-4.326993 -4.3192034 -4.3122158 -4.3104258 -4.3128157 -4.3142529 -4.3143382 -4.3130665 -4.3105965 -4.3069282 -4.3051958 -4.308001 -4.3183522 -4.3296018 -4.3353205]]...]
INFO - root - 2017-12-07 18:14:45.036088: step 38810, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 59h:07m:19s remains)
INFO - root - 2017-12-07 18:14:51.880538: step 38820, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 54h:13m:46s remains)
INFO - root - 2017-12-07 18:14:58.610057: step 38830, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 52h:28m:06s remains)
INFO - root - 2017-12-07 18:15:05.568887: step 38840, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 60h:16m:32s remains)
INFO - root - 2017-12-07 18:15:12.414391: step 38850, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 56h:23m:33s remains)
INFO - root - 2017-12-07 18:15:19.169048: step 38860, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 54h:43m:32s remains)
INFO - root - 2017-12-07 18:15:26.045937: step 38870, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 51h:51m:29s remains)
INFO - root - 2017-12-07 18:15:32.944532: step 38880, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 56h:47m:32s remains)
INFO - root - 2017-12-07 18:15:39.676880: step 38890, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 56h:17m:47s remains)
INFO - root - 2017-12-07 18:15:46.392496: step 38900, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 57h:19m:58s remains)
2017-12-07 18:15:47.114027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1232224 -4.1060147 -4.1024065 -4.1080408 -4.1162992 -4.112216 -4.0919762 -4.0814977 -4.0951962 -4.1183586 -4.1302109 -4.1214967 -4.0982661 -4.0681205 -4.0629487][-4.1205711 -4.1055551 -4.0974989 -4.0993757 -4.1116271 -4.1211147 -4.1132336 -4.1091366 -4.1205425 -4.1365681 -4.1463428 -4.1437597 -4.1333723 -4.1164818 -4.1149111][-4.1432953 -4.1222763 -4.1042666 -4.102879 -4.1219049 -4.1448879 -4.1481838 -4.1516395 -4.163486 -4.1765871 -4.1891894 -4.1968775 -4.1993818 -4.1931353 -4.1910157][-4.1594124 -4.1339631 -4.1086545 -4.1047606 -4.1282372 -4.1572809 -4.1630383 -4.16815 -4.1822119 -4.20092 -4.222034 -4.2405725 -4.2521434 -4.2521977 -4.2458978][-4.1573277 -4.1358409 -4.1137338 -4.1084237 -4.127614 -4.1457243 -4.1399784 -4.134953 -4.1504226 -4.181159 -4.2149358 -4.2444754 -4.2633228 -4.2665877 -4.255455][-4.1421466 -4.1290112 -4.114306 -4.1083794 -4.1135941 -4.1065373 -4.0758085 -4.0550051 -4.0776424 -4.1269727 -4.1765428 -4.2136855 -4.2327852 -4.2325983 -4.2183366][-4.1195087 -4.1134472 -4.1064124 -4.0999446 -4.0835071 -4.0408363 -3.9728422 -3.9375014 -3.9832711 -4.063972 -4.1320662 -4.1715722 -4.1852636 -4.1774797 -4.159296][-4.1048927 -4.1050892 -4.106319 -4.0975833 -4.0616531 -3.991889 -3.8961442 -3.8546486 -3.9292278 -4.0323377 -4.1039786 -4.1344461 -4.1336031 -4.1160526 -4.098083][-4.10813 -4.1103992 -4.1180205 -4.1126957 -4.077148 -4.0106544 -3.9278631 -3.9019983 -3.9721329 -4.0565882 -4.1041718 -4.1112771 -4.0896935 -4.0690441 -4.0621095][-4.1307707 -4.12945 -4.1369004 -4.1384649 -4.1164174 -4.0706987 -4.0144382 -3.9991708 -4.041954 -4.0875177 -4.1030679 -4.0841932 -4.0497231 -4.0346093 -4.0413904][-4.1473684 -4.1446276 -4.1544461 -4.1654115 -4.1551523 -4.1267633 -4.0910726 -4.0828209 -4.1044459 -4.1208696 -4.1126118 -4.0783072 -4.0413532 -4.033885 -4.0487189][-4.1534128 -4.1520381 -4.1644335 -4.1803184 -4.1791978 -4.1635351 -4.1464925 -4.14881 -4.1623459 -4.1645117 -4.1462564 -4.1093574 -4.0766134 -4.0726314 -4.0883322][-4.1572862 -4.1619058 -4.1775527 -4.1961765 -4.2036905 -4.2010174 -4.1967182 -4.2057424 -4.2162347 -4.2149873 -4.1992912 -4.1696315 -4.1446648 -4.1411047 -4.1490026][-4.1579876 -4.173718 -4.1970177 -4.2177477 -4.2302713 -4.2354879 -4.2369981 -4.2441182 -4.251123 -4.2503619 -4.2418728 -4.2245941 -4.2086825 -4.2035575 -4.200613][-4.1688251 -4.190053 -4.2152667 -4.2338204 -4.2459192 -4.2501564 -4.25178 -4.2574911 -4.2625332 -4.2644429 -4.2624459 -4.255106 -4.2465158 -4.2401347 -4.2321091]]...]
INFO - root - 2017-12-07 18:15:53.961650: step 38910, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 58h:54m:37s remains)
INFO - root - 2017-12-07 18:16:00.810531: step 38920, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 57h:53m:29s remains)
INFO - root - 2017-12-07 18:16:07.672286: step 38930, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.693 sec/batch; 56h:28m:41s remains)
INFO - root - 2017-12-07 18:16:14.428288: step 38940, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 50h:41m:42s remains)
INFO - root - 2017-12-07 18:16:21.285424: step 38950, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 55h:18m:12s remains)
INFO - root - 2017-12-07 18:16:28.201712: step 38960, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 60h:03m:50s remains)
INFO - root - 2017-12-07 18:16:34.961052: step 38970, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 55h:47m:19s remains)
INFO - root - 2017-12-07 18:16:41.837630: step 38980, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 52h:39m:34s remains)
INFO - root - 2017-12-07 18:16:48.699820: step 38990, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 54h:17m:39s remains)
INFO - root - 2017-12-07 18:16:55.110473: step 39000, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 54h:39m:39s remains)
2017-12-07 18:16:55.828329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1503911 -4.109211 -4.0901718 -4.0884552 -4.1087818 -4.1202416 -4.1304259 -4.1428986 -4.1511326 -4.1572928 -4.1592507 -4.1559319 -4.145505 -4.1321211 -4.1380687][-4.1632438 -4.1209569 -4.0966043 -4.0944595 -4.1249442 -4.1456618 -4.1606236 -4.1727915 -4.1806369 -4.1834545 -4.1787519 -4.1689472 -4.1548805 -4.1470103 -4.1609197][-4.1449785 -4.1032934 -4.0794396 -4.0800138 -4.1191111 -4.1492777 -4.1668606 -4.1810813 -4.1941481 -4.1981139 -4.1878786 -4.1688151 -4.1502233 -4.1447077 -4.1627831][-4.1128016 -4.0663333 -4.0409522 -4.0475988 -4.090385 -4.1218619 -4.1398954 -4.1586909 -4.1803041 -4.1929684 -4.1877108 -4.1655765 -4.1438489 -4.137414 -4.1537628][-4.1032467 -4.05074 -4.0194449 -4.0205903 -4.0494714 -4.0690856 -4.0812278 -4.1044931 -4.1392808 -4.1700611 -4.1822114 -4.1655235 -4.1385841 -4.1209273 -4.1274943][-4.108427 -4.0494065 -4.0063329 -3.9871323 -3.989279 -3.9912636 -3.994379 -4.0217681 -4.0732312 -4.1298614 -4.1679564 -4.1633563 -4.1330395 -4.1001043 -4.0924592][-4.1005483 -4.034914 -3.9799342 -3.9462144 -3.9318376 -3.9172544 -3.9082513 -3.932292 -3.994519 -4.0788608 -4.1424727 -4.1560564 -4.13204 -4.09352 -4.0764322][-4.093708 -4.0243096 -3.9699812 -3.9405358 -3.9277649 -3.9131036 -3.899837 -3.9110305 -3.9647593 -4.0541105 -4.13138 -4.1593556 -4.1471405 -4.1158066 -4.1000915][-4.1075463 -4.0450821 -3.9984605 -3.9775343 -3.9756289 -3.9753132 -3.9727254 -3.9879735 -4.0312924 -4.1030035 -4.1699166 -4.1955075 -4.18211 -4.15135 -4.134234][-4.1331773 -4.0783973 -4.039062 -4.0248532 -4.0309887 -4.04031 -4.0492125 -4.0691514 -4.1069098 -4.16028 -4.2118578 -4.2340212 -4.2184639 -4.1859317 -4.1618285][-4.1554785 -4.1091695 -4.0782843 -4.0734358 -4.0878124 -4.1019921 -4.1122823 -4.1281676 -4.1568885 -4.1964831 -4.236063 -4.2583909 -4.2483139 -4.2188511 -4.1936259][-4.175909 -4.1370893 -4.1166582 -4.1245508 -4.1483936 -4.1658645 -4.1747303 -4.1840577 -4.2032876 -4.2299657 -4.25886 -4.2797132 -4.2755136 -4.2526283 -4.2317848][-4.2038054 -4.1727624 -4.1626616 -4.179491 -4.2081733 -4.227119 -4.23528 -4.2411103 -4.2519503 -4.2681608 -4.2870107 -4.3006616 -4.2966681 -4.278182 -4.2593083][-4.2299047 -4.2056789 -4.2015457 -4.2190924 -4.2451987 -4.2629228 -4.2722859 -4.2786527 -4.2851119 -4.2924733 -4.3001738 -4.3041506 -4.2953653 -4.276298 -4.2568874][-4.2449517 -4.2254238 -4.2235041 -4.2367568 -4.254869 -4.2668619 -4.273293 -4.27712 -4.2799015 -4.2832012 -4.2862768 -4.2856464 -4.2767711 -4.2608681 -4.2433753]]...]
INFO - root - 2017-12-07 18:17:02.591777: step 39010, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 53h:00m:05s remains)
INFO - root - 2017-12-07 18:17:09.511034: step 39020, loss = 2.08, batch loss = 2.03 (10.6 examples/sec; 0.757 sec/batch; 61h:41m:24s remains)
INFO - root - 2017-12-07 18:17:16.441849: step 39030, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 60h:35m:51s remains)
INFO - root - 2017-12-07 18:17:23.185753: step 39040, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 55h:06m:03s remains)
INFO - root - 2017-12-07 18:17:30.020871: step 39050, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 52h:54m:01s remains)
INFO - root - 2017-12-07 18:17:36.854054: step 39060, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 55h:50m:24s remains)
INFO - root - 2017-12-07 18:17:43.620628: step 39070, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 56h:36m:25s remains)
INFO - root - 2017-12-07 18:17:50.424050: step 39080, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 55h:57m:10s remains)
INFO - root - 2017-12-07 18:17:57.378848: step 39090, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 52h:15m:01s remains)
INFO - root - 2017-12-07 18:18:03.933908: step 39100, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 53h:38m:17s remains)
2017-12-07 18:18:04.639454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2875853 -4.2633295 -4.2358155 -4.230041 -4.2352242 -4.2348852 -4.23361 -4.22632 -4.2165785 -4.2248507 -4.24635 -4.2571707 -4.2640438 -4.2752652 -4.2862315][-4.248651 -4.2210894 -4.1889377 -4.1855097 -4.1919322 -4.1980829 -4.2077179 -4.2038693 -4.1919789 -4.2050619 -4.2383795 -4.2554908 -4.2619629 -4.2723193 -4.283082][-4.2052193 -4.17502 -4.1433206 -4.1441355 -4.1541491 -4.164196 -4.1831527 -4.185873 -4.1728687 -4.1851034 -4.2239189 -4.2466359 -4.2577186 -4.2682805 -4.2748575][-4.1687164 -4.1359768 -4.1095076 -4.1119957 -4.1208162 -4.1274881 -4.1490951 -4.1686497 -4.168622 -4.1786618 -4.2147975 -4.2482939 -4.2696218 -4.2756019 -4.2760458][-4.1515188 -4.1194496 -4.0997319 -4.0986938 -4.0961695 -4.085443 -4.0910277 -4.1281848 -4.1575923 -4.1695533 -4.1989183 -4.2429943 -4.2771888 -4.2835116 -4.2806377][-4.1574249 -4.1241708 -4.1003885 -4.084867 -4.0614696 -4.0112758 -3.9755321 -4.0292244 -4.1033564 -4.1308131 -4.1611309 -4.216321 -4.2639117 -4.2796783 -4.2803082][-4.1642594 -4.1255064 -4.0855508 -4.0497322 -4.0049925 -3.907553 -3.7976127 -3.8616285 -3.985594 -4.0391169 -4.0849242 -4.153337 -4.2177787 -4.2546682 -4.266964][-4.1677351 -4.1209536 -4.0617657 -4.009017 -3.950969 -3.8197684 -3.6451442 -3.705348 -3.8683665 -3.9449532 -4.0099621 -4.089829 -4.1673155 -4.2229896 -4.2452812][-4.17988 -4.1352558 -4.0735278 -4.0248723 -3.98275 -3.8750405 -3.7228441 -3.7505586 -3.8847885 -3.9579759 -4.0223074 -4.0901079 -4.1589622 -4.2122412 -4.2338276][-4.2092619 -4.175633 -4.1280146 -4.0975437 -4.0803032 -4.0162778 -3.9211044 -3.9277706 -4.006393 -4.0609236 -4.1130047 -4.152678 -4.1959605 -4.2296772 -4.2430029][-4.2486491 -4.2298875 -4.20097 -4.1902685 -4.1892467 -4.1582847 -4.1077271 -4.100913 -4.1391239 -4.181448 -4.2206969 -4.2401395 -4.257618 -4.2696066 -4.2704906][-4.2813148 -4.2743254 -4.2618418 -4.264946 -4.2743945 -4.2656932 -4.2432976 -4.2299533 -4.2443027 -4.2750278 -4.3003283 -4.3063755 -4.3090277 -4.3078971 -4.3006616][-4.3011794 -4.2996955 -4.2967811 -4.3040147 -4.3167472 -4.320498 -4.31563 -4.3062706 -4.3096352 -4.3239207 -4.3362246 -4.3365426 -4.3335562 -4.3279347 -4.3190842][-4.3121858 -4.31229 -4.311677 -4.3164382 -4.3264928 -4.334621 -4.3371115 -4.335526 -4.3370552 -4.3417621 -4.3459721 -4.343688 -4.338768 -4.3321681 -4.3256307][-4.3220496 -4.3223805 -4.3215609 -4.3226748 -4.3276415 -4.3333778 -4.3373523 -4.3388977 -4.3408165 -4.3422918 -4.343257 -4.3406396 -4.3359885 -4.3312531 -4.3281603]]...]
INFO - root - 2017-12-07 18:18:11.455130: step 39110, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 59h:02m:44s remains)
INFO - root - 2017-12-07 18:18:18.187791: step 39120, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.611 sec/batch; 49h:49m:21s remains)
INFO - root - 2017-12-07 18:18:25.146736: step 39130, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 56h:33m:16s remains)
INFO - root - 2017-12-07 18:18:32.049935: step 39140, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 60h:45m:32s remains)
INFO - root - 2017-12-07 18:18:38.874676: step 39150, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 51h:38m:57s remains)
INFO - root - 2017-12-07 18:18:45.682953: step 39160, loss = 2.09, batch loss = 2.04 (12.1 examples/sec; 0.661 sec/batch; 53h:50m:28s remains)
INFO - root - 2017-12-07 18:18:52.432809: step 39170, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 52h:57m:37s remains)
INFO - root - 2017-12-07 18:18:59.287074: step 39180, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 55h:39m:13s remains)
INFO - root - 2017-12-07 18:19:06.072542: step 39190, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.689 sec/batch; 56h:05m:58s remains)
INFO - root - 2017-12-07 18:19:12.737784: step 39200, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 55h:48m:56s remains)
2017-12-07 18:19:13.523954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3217821 -4.3215742 -4.3215523 -4.3218193 -4.322176 -4.32248 -4.3225617 -4.3228593 -4.32275 -4.3215723 -4.3198915 -4.3187242 -4.3180923 -4.3173018 -4.3169103][-4.316154 -4.3168607 -4.316484 -4.3156242 -4.3137546 -4.3116684 -4.3093829 -4.3073826 -4.3048449 -4.3018847 -4.2998567 -4.2996526 -4.3004827 -4.30097 -4.30139][-4.3136082 -4.3145747 -4.3121772 -4.3080497 -4.3029766 -4.2994719 -4.2966814 -4.293961 -4.2900887 -4.2858152 -4.2833037 -4.2839723 -4.2861204 -4.2875028 -4.2876639][-4.3040385 -4.3022408 -4.2966704 -4.2890096 -4.2808022 -4.2768707 -4.2772555 -4.2798085 -4.2800784 -4.2781734 -4.27644 -4.2768822 -4.2787595 -4.2795925 -4.2780776][-4.2788873 -4.2706194 -4.2587404 -4.2446475 -4.2341409 -4.23364 -4.2415061 -4.2555404 -4.2675905 -4.2721362 -4.2731209 -4.2726336 -4.2717886 -4.270092 -4.2663441][-4.2184615 -4.1972132 -4.1725326 -4.1472187 -4.132658 -4.139565 -4.1626592 -4.1986065 -4.2313452 -4.2478456 -4.2538352 -4.2515316 -4.2461867 -4.2414284 -4.236867][-4.1425271 -4.1147718 -4.0821862 -4.0469594 -4.0228672 -4.030581 -4.0641894 -4.1216121 -4.1784019 -4.2098713 -4.2236352 -4.2207074 -4.2122 -4.2068043 -4.2042913][-4.1139779 -4.0942197 -4.0661325 -4.03145 -4.0003262 -3.9967105 -4.0199561 -4.0779305 -4.1448889 -4.1865048 -4.2088642 -4.210979 -4.2060676 -4.2037497 -4.2028165][-4.1436324 -4.1394534 -4.124877 -4.1005635 -4.0719142 -4.0603676 -4.0687838 -4.1094537 -4.1656113 -4.2057328 -4.2307673 -4.2364006 -4.2339964 -4.2319155 -4.2294064][-4.1992383 -4.2044492 -4.1987357 -4.1842566 -4.1620636 -4.1494 -4.1511583 -4.1756439 -4.2134323 -4.243557 -4.264092 -4.2687359 -4.2653704 -4.2621408 -4.2594571][-4.2453794 -4.2542553 -4.2519064 -4.2421274 -4.2251358 -4.2124271 -4.2116065 -4.22729 -4.2533841 -4.276576 -4.2904515 -4.2903218 -4.2833133 -4.2787571 -4.2774315][-4.2649574 -4.2755308 -4.2756858 -4.2679758 -4.2533646 -4.2416158 -4.2400904 -4.2506475 -4.2699652 -4.2868123 -4.2911072 -4.2812672 -4.2667294 -4.2586794 -4.2557988][-4.2671661 -4.277329 -4.2775812 -4.2697806 -4.2535167 -4.2388043 -4.2319608 -4.2354426 -4.2476106 -4.2559977 -4.2531414 -4.2361135 -4.2162428 -4.2076507 -4.2054915][-4.2611008 -4.2674146 -4.2632623 -4.251545 -4.23085 -4.2106748 -4.1983261 -4.1977196 -4.2059174 -4.2103276 -4.2040396 -4.1799221 -4.1466341 -4.1274867 -4.116662][-4.2620482 -4.2626772 -4.2528934 -4.2352195 -4.2091012 -4.1839252 -4.1670184 -4.1652784 -4.1725926 -4.1780515 -4.1721392 -4.1424346 -4.0962334 -4.0653434 -4.0427413]]...]
INFO - root - 2017-12-07 18:19:20.282445: step 39210, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 59h:42m:57s remains)
INFO - root - 2017-12-07 18:19:27.183625: step 39220, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 53h:47m:49s remains)
INFO - root - 2017-12-07 18:19:34.010336: step 39230, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 52h:37m:11s remains)
INFO - root - 2017-12-07 18:19:40.817654: step 39240, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 52h:49m:08s remains)
INFO - root - 2017-12-07 18:19:47.652993: step 39250, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 58h:47m:05s remains)
INFO - root - 2017-12-07 18:19:54.517585: step 39260, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 60h:17m:49s remains)
INFO - root - 2017-12-07 18:20:01.247863: step 39270, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 54h:44m:03s remains)
INFO - root - 2017-12-07 18:20:08.004305: step 39280, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 52h:02m:45s remains)
INFO - root - 2017-12-07 18:20:14.778928: step 39290, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 51h:26m:40s remains)
INFO - root - 2017-12-07 18:20:21.437497: step 39300, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.734 sec/batch; 59h:45m:13s remains)
2017-12-07 18:20:22.121838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3309689 -4.3299756 -4.3282542 -4.326715 -4.3254938 -4.3254566 -4.3234038 -4.31779 -4.3112836 -4.3087811 -4.3100038 -4.3164988 -4.3251705 -4.3315611 -4.3326297][-4.33107 -4.3273344 -4.3244157 -4.3214393 -4.316514 -4.3117352 -4.3059087 -4.2992783 -4.2916789 -4.2889991 -4.2895565 -4.2989807 -4.312521 -4.3241668 -4.328301][-4.3250389 -4.3163004 -4.3098478 -4.3043995 -4.2944913 -4.28551 -4.2774048 -4.2707582 -4.2632155 -4.2614851 -4.2667127 -4.2793975 -4.2980857 -4.3149638 -4.321147][-4.3146515 -4.2986488 -4.2864656 -4.2766094 -4.2635803 -4.2539639 -4.2473717 -4.240726 -4.2344093 -4.2373948 -4.2480865 -4.2633095 -4.2830057 -4.2997427 -4.30347][-4.3018775 -4.2806149 -4.2628903 -4.2492328 -4.2383184 -4.2333126 -4.2274966 -4.2130861 -4.2008643 -4.2032771 -4.2193937 -4.2418127 -4.2624331 -4.2748852 -4.2730637][-4.2808757 -4.26176 -4.243783 -4.2279854 -4.2178664 -4.2148829 -4.208343 -4.1828947 -4.1582589 -4.1571383 -4.1834507 -4.2163653 -4.2418947 -4.2529392 -4.2514372][-4.2411013 -4.2241569 -4.2090492 -4.1875134 -4.1704907 -4.167129 -4.174017 -4.157505 -4.1305289 -4.1315951 -4.1662245 -4.2064009 -4.2313185 -4.239553 -4.2393956][-4.1739817 -4.1543169 -4.1433749 -4.1260056 -4.1078558 -4.1148109 -4.1469283 -4.1531181 -4.1403089 -4.1499577 -4.184258 -4.215498 -4.2293921 -4.2297707 -4.2261505][-4.0991993 -4.0853653 -4.0921187 -4.091845 -4.088428 -4.1130505 -4.157753 -4.1745138 -4.1735749 -4.1903782 -4.2123284 -4.2275429 -4.2265816 -4.2106376 -4.1938367][-4.04691 -4.0571647 -4.0965281 -4.1250834 -4.1488042 -4.1781 -4.2047043 -4.2081017 -4.2003975 -4.2070775 -4.2120624 -4.2093983 -4.1927791 -4.1610074 -4.1272573][-4.0711355 -4.1067929 -4.1541433 -4.18248 -4.1984468 -4.2146034 -4.2186575 -4.2030139 -4.1802077 -4.1689482 -4.1599555 -4.1488929 -4.1306109 -4.0933466 -4.0569673][-4.1473579 -4.1843944 -4.2147369 -4.2182689 -4.201807 -4.1877875 -4.1746545 -4.1497788 -4.1140575 -4.088274 -4.079556 -4.0817647 -4.0811658 -4.0630579 -4.0491457][-4.2124414 -4.2322011 -4.2393031 -4.2169843 -4.1685791 -4.1296663 -4.1128516 -4.0930128 -4.0571041 -4.0354295 -4.0442748 -4.0700283 -4.0895233 -4.0944805 -4.1020255][-4.2549925 -4.2528858 -4.2415085 -4.2081795 -4.149581 -4.1075344 -4.0947871 -4.0832024 -4.0595708 -4.0561152 -4.0811462 -4.111589 -4.1349311 -4.1484709 -4.1659293][-4.2859511 -4.2735491 -4.2565689 -4.2260885 -4.1811037 -4.1498837 -4.1389928 -4.1303892 -4.115325 -4.12227 -4.151125 -4.1782637 -4.19961 -4.2146235 -4.2369246]]...]
INFO - root - 2017-12-07 18:20:28.660656: step 39310, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 52h:07m:52s remains)
INFO - root - 2017-12-07 18:20:35.503116: step 39320, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.777 sec/batch; 63h:15m:37s remains)
INFO - root - 2017-12-07 18:20:42.341425: step 39330, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.758 sec/batch; 61h:41m:48s remains)
INFO - root - 2017-12-07 18:20:49.136644: step 39340, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 53h:00m:10s remains)
INFO - root - 2017-12-07 18:20:55.938744: step 39350, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.653 sec/batch; 53h:09m:48s remains)
INFO - root - 2017-12-07 18:21:02.688587: step 39360, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 53h:06m:55s remains)
INFO - root - 2017-12-07 18:21:09.609757: step 39370, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 59h:58m:31s remains)
INFO - root - 2017-12-07 18:21:16.426214: step 39380, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.677 sec/batch; 55h:09m:06s remains)
INFO - root - 2017-12-07 18:21:23.258922: step 39390, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.638 sec/batch; 51h:59m:06s remains)
INFO - root - 2017-12-07 18:21:29.787823: step 39400, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 50h:43m:13s remains)
2017-12-07 18:21:30.491395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2149572 -4.2280669 -4.2429924 -4.2486868 -4.2348261 -4.2181754 -4.2092733 -4.2010913 -4.1945071 -4.1841383 -4.173564 -4.1664519 -4.1629868 -4.1760588 -4.2103276][-4.2131119 -4.2271051 -4.2417421 -4.2454839 -4.2260394 -4.1966372 -4.174684 -4.1625056 -4.1601152 -4.1570349 -4.1530466 -4.1485505 -4.140554 -4.1479974 -4.1834664][-4.2175097 -4.2285104 -4.2380753 -4.23821 -4.2137251 -4.1757979 -4.1407132 -4.1238713 -4.1270204 -4.1367459 -4.1432009 -4.1435509 -4.1331658 -4.1324596 -4.1626234][-4.2165437 -4.2249813 -4.2324257 -4.2302351 -4.2061534 -4.1640358 -4.1149621 -4.0857687 -4.0895605 -4.1124425 -4.1338439 -4.1418991 -4.131794 -4.1262627 -4.1503515][-4.2167754 -4.2226315 -4.2289028 -4.2253857 -4.2038293 -4.15853 -4.091888 -4.0387063 -4.0386586 -4.0792217 -4.1199479 -4.1374359 -4.1331453 -4.1298203 -4.1520452][-4.2177315 -4.2234979 -4.2301407 -4.2260861 -4.2061071 -4.1566105 -4.0689044 -3.9851329 -3.9782917 -4.0435472 -4.1055489 -4.1314731 -4.1355262 -4.1404791 -4.1656132][-4.2149453 -4.2223086 -4.2293396 -4.2230721 -4.2014308 -4.1476517 -4.0463314 -3.934473 -3.9188476 -4.0097442 -4.0926785 -4.1256 -4.1380396 -4.1528344 -4.1807265][-4.2001023 -4.2062254 -4.2125673 -4.2071681 -4.1844053 -4.1344228 -4.03336 -3.9092007 -3.89121 -3.9973121 -4.0933585 -4.1321149 -4.1480255 -4.1648459 -4.1909451][-4.1895823 -4.1926851 -4.198534 -4.1944332 -4.1758361 -4.1370029 -4.057961 -3.959362 -3.9449782 -4.0311909 -4.1130581 -4.1459713 -4.1583638 -4.1725416 -4.1944861][-4.1959033 -4.1917644 -4.1893349 -4.1820641 -4.1696367 -4.1479731 -4.1057396 -4.0498381 -4.0391631 -4.0896692 -4.1408815 -4.1597419 -4.1660323 -4.175211 -4.1933112][-4.2145081 -4.2077122 -4.1960583 -4.1823068 -4.1724582 -4.1665254 -4.1549182 -4.1303363 -4.1242576 -4.1480322 -4.1742735 -4.1796212 -4.1776509 -4.1807117 -4.1942058][-4.2307177 -4.2230983 -4.2078552 -4.1890855 -4.1798687 -4.1834016 -4.1871386 -4.1816568 -4.1792164 -4.1885233 -4.202136 -4.2019606 -4.1929507 -4.1901627 -4.1997137][-4.2398214 -4.2319574 -4.2195067 -4.2048287 -4.1975064 -4.2038651 -4.2127118 -4.2133174 -4.2105141 -4.2106133 -4.2177062 -4.2188878 -4.2109809 -4.2077751 -4.2164311][-4.2367291 -4.231163 -4.222353 -4.2125874 -4.2102571 -4.2181396 -4.2269764 -4.2268877 -4.2201986 -4.2123632 -4.2153816 -4.2218323 -4.2233591 -4.2278585 -4.2403064][-4.2260051 -4.224309 -4.2180462 -4.2123156 -4.2123089 -4.2173877 -4.2241096 -4.223927 -4.2164855 -4.2074423 -4.2092848 -4.2196422 -4.2311711 -4.2446847 -4.2614832]]...]
INFO - root - 2017-12-07 18:21:37.318603: step 39410, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.731 sec/batch; 59h:28m:39s remains)
INFO - root - 2017-12-07 18:21:44.081873: step 39420, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 55h:17m:10s remains)
INFO - root - 2017-12-07 18:21:50.918943: step 39430, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 50h:53m:18s remains)
INFO - root - 2017-12-07 18:21:57.813899: step 39440, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 56h:02m:52s remains)
INFO - root - 2017-12-07 18:22:04.672100: step 39450, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.745 sec/batch; 60h:40m:52s remains)
INFO - root - 2017-12-07 18:22:11.459936: step 39460, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.663 sec/batch; 53h:55m:46s remains)
INFO - root - 2017-12-07 18:22:18.283791: step 39470, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 51h:35m:56s remains)
INFO - root - 2017-12-07 18:22:25.006943: step 39480, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 51h:23m:24s remains)
INFO - root - 2017-12-07 18:22:31.816449: step 39490, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 58h:03m:27s remains)
INFO - root - 2017-12-07 18:22:38.359881: step 39500, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 56h:33m:16s remains)
2017-12-07 18:22:39.062134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1253071 -4.144671 -4.1878786 -4.2293954 -4.2667184 -4.3019319 -4.3251972 -4.3363357 -4.3304625 -4.3121219 -4.2803006 -4.2613897 -4.2568264 -4.2597117 -4.271997][-4.0346808 -4.07007 -4.139143 -4.1960874 -4.237607 -4.2713237 -4.2935562 -4.3075066 -4.3099847 -4.3021054 -4.2784734 -4.2634544 -4.2595587 -4.2621503 -4.272634][-3.9700704 -4.0134196 -4.104948 -4.1751122 -4.2174668 -4.2440138 -4.2583857 -4.2708497 -4.2792015 -4.2819743 -4.2725406 -4.2670903 -4.2657428 -4.2658796 -4.2731047][-3.9600644 -4.0040922 -4.1013212 -4.172761 -4.208642 -4.22457 -4.231431 -4.2415257 -4.2543211 -4.2666011 -4.2704697 -4.2745218 -4.2750397 -4.2719436 -4.2747421][-4.0078311 -4.0440087 -4.1223121 -4.1762257 -4.198873 -4.2031178 -4.2020926 -4.2103763 -4.2298908 -4.2556791 -4.27253 -4.285892 -4.2900844 -4.2841024 -4.2804174][-4.0854483 -4.1075163 -4.1505785 -4.1708155 -4.1681247 -4.154388 -4.1427827 -4.1519647 -4.1820769 -4.2272906 -4.2617211 -4.2891912 -4.3009281 -4.2955995 -4.2873068][-4.1691675 -4.1787434 -4.1896305 -4.175787 -4.1390076 -4.0968857 -4.0660353 -4.0698085 -4.1103344 -4.176549 -4.2306085 -4.2731204 -4.2944994 -4.2942939 -4.2858567][-4.24815 -4.2459712 -4.2306108 -4.1871977 -4.1228828 -4.05575 -4.0066996 -4.0032334 -4.0525141 -4.1348629 -4.2008519 -4.2498074 -4.2782 -4.2842593 -4.2765918][-4.3048124 -4.29416 -4.25805 -4.1927238 -4.1157146 -4.0428762 -3.991168 -3.9818735 -4.0301046 -4.1134663 -4.1770558 -4.2248163 -4.2540913 -4.2629461 -4.2551794][-4.3242073 -4.3069191 -4.2587409 -4.1824937 -4.1066313 -4.0455332 -4.0056019 -3.9966331 -4.0334554 -4.1011724 -4.15337 -4.1929851 -4.2191286 -4.227932 -4.2196131][-4.3199806 -4.2971711 -4.2427144 -4.1679835 -4.1018529 -4.0586171 -4.0324597 -4.0260115 -4.0486221 -4.0938797 -4.131176 -4.163178 -4.1830578 -4.1889858 -4.1812062][-4.31928 -4.2979465 -4.2504125 -4.1899524 -4.1398177 -4.1094809 -4.0891786 -4.0807991 -4.0899525 -4.1148815 -4.1348825 -4.1561241 -4.1643314 -4.1605892 -4.14937][-4.3311667 -4.3169374 -4.2834196 -4.2416663 -4.208508 -4.1887422 -4.1717653 -4.1607723 -4.1575942 -4.1610403 -4.1615167 -4.1675134 -4.166368 -4.1535044 -4.1403313][-4.3411989 -4.3345246 -4.3149381 -4.2921638 -4.2758031 -4.2663665 -4.2543712 -4.24386 -4.2328262 -4.2209516 -4.2062335 -4.1999192 -4.1944447 -4.1807041 -4.1692166][-4.3446035 -4.34226 -4.3317924 -4.3216386 -4.3154311 -4.3136277 -4.3098526 -4.3030286 -4.2918091 -4.2775769 -4.2605205 -4.2499037 -4.2436028 -4.2342796 -4.2264132]]...]
INFO - root - 2017-12-07 18:22:45.847130: step 39510, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 52h:31m:47s remains)
INFO - root - 2017-12-07 18:22:52.729777: step 39520, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 57h:07m:01s remains)
INFO - root - 2017-12-07 18:22:59.594300: step 39530, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 56h:03m:55s remains)
INFO - root - 2017-12-07 18:23:06.510491: step 39540, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 55h:36m:24s remains)
INFO - root - 2017-12-07 18:23:13.318827: step 39550, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 51h:28m:39s remains)
INFO - root - 2017-12-07 18:23:20.128038: step 39560, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 57h:57m:28s remains)
INFO - root - 2017-12-07 18:23:26.880081: step 39570, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 59h:55m:44s remains)
INFO - root - 2017-12-07 18:23:33.688290: step 39580, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 56h:38m:50s remains)
INFO - root - 2017-12-07 18:23:40.499344: step 39590, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 54h:44m:48s remains)
INFO - root - 2017-12-07 18:23:47.073537: step 39600, loss = 2.10, batch loss = 2.04 (12.7 examples/sec; 0.629 sec/batch; 51h:10m:39s remains)
2017-12-07 18:23:47.781730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1291494 -4.1775408 -4.2130857 -4.2142119 -4.1932397 -4.1753373 -4.1704264 -4.1818728 -4.2050238 -4.2206874 -4.2288427 -4.2280889 -4.2248144 -4.2254839 -4.2344851][-4.1269546 -4.1785278 -4.2128067 -4.2102985 -4.1855464 -4.1636257 -4.1521921 -4.1575089 -4.1789155 -4.1962242 -4.2046165 -4.2056565 -4.2076349 -4.2115474 -4.2121329][-4.1331038 -4.1798925 -4.2108731 -4.2107162 -4.1844468 -4.151721 -4.1257272 -4.1239762 -4.1481695 -4.1720634 -4.1838694 -4.1879044 -4.1909356 -4.1888256 -4.1729612][-4.1534414 -4.1944389 -4.2226129 -4.2223725 -4.1889982 -4.1327267 -4.0835705 -4.0788803 -4.11427 -4.1505394 -4.1687374 -4.1737585 -4.1686945 -4.1532865 -4.1218505][-4.1730275 -4.2080331 -4.2336216 -4.2319274 -4.1885972 -4.1023378 -4.0202084 -4.0074821 -4.0616145 -4.1220508 -4.1577682 -4.1667113 -4.156085 -4.1334491 -4.0979056][-4.1821589 -4.2113109 -4.2308183 -4.2231946 -4.169106 -4.0532308 -3.9297261 -3.9019091 -3.9834919 -4.0805974 -4.1400356 -4.1589303 -4.1546621 -4.13571 -4.1037741][-4.1796741 -4.2076354 -4.2230043 -4.2087336 -4.14783 -4.0167985 -3.8647118 -3.8228195 -3.9268909 -4.0514474 -4.1276693 -4.1542954 -4.1567216 -4.1446648 -4.1191273][-4.1632953 -4.1929674 -4.2096386 -4.1949282 -4.1400604 -4.0251484 -3.8907855 -3.8531888 -3.9447067 -4.05827 -4.12731 -4.1518989 -4.1552839 -4.1491156 -4.1358514][-4.1394153 -4.170536 -4.1942239 -4.186419 -4.1441317 -4.0613418 -3.970479 -3.943872 -4.0017409 -4.0786982 -4.1254683 -4.1402316 -4.1412792 -4.1412458 -4.1419182][-4.1102805 -4.1407557 -4.17035 -4.1737723 -4.1448641 -4.0862112 -4.0232382 -4.0041933 -4.0368428 -4.08377 -4.1148171 -4.1249795 -4.1236234 -4.1253066 -4.12965][-4.0799551 -4.1048107 -4.13681 -4.1482439 -4.1302547 -4.0872669 -4.0413179 -4.0290737 -4.0524731 -4.0861416 -4.1109309 -4.1185956 -4.1151581 -4.1143856 -4.1162252][-4.0685558 -4.0859432 -4.1151395 -4.1288085 -4.12032 -4.0915337 -4.0631576 -4.05963 -4.0806241 -4.1061373 -4.1230326 -4.1260428 -4.1202765 -4.1186695 -4.1176476][-4.0854592 -4.0984154 -4.1192656 -4.130765 -4.1266556 -4.1095085 -4.0964866 -4.1016951 -4.1204624 -4.1365104 -4.1440659 -4.1431808 -4.13674 -4.1344094 -4.13248][-4.1175666 -4.1298857 -4.1462665 -4.1542625 -4.1511545 -4.142282 -4.1405969 -4.1492982 -4.1628189 -4.1715937 -4.173461 -4.1691442 -4.1635451 -4.163229 -4.1641541][-4.1481605 -4.1589813 -4.17323 -4.181118 -4.1819582 -4.1798959 -4.1829252 -4.1888933 -4.1960053 -4.1998849 -4.1984024 -4.1933393 -4.191915 -4.1961408 -4.2008142]]...]
INFO - root - 2017-12-07 18:23:54.578871: step 39610, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 55h:17m:35s remains)
INFO - root - 2017-12-07 18:24:01.192320: step 39620, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 53h:23m:53s remains)
INFO - root - 2017-12-07 18:24:08.036750: step 39630, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 58h:28m:12s remains)
INFO - root - 2017-12-07 18:24:14.861989: step 39640, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 58h:59m:07s remains)
INFO - root - 2017-12-07 18:24:21.634874: step 39650, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 55h:33m:32s remains)
INFO - root - 2017-12-07 18:24:28.474397: step 39660, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 53h:25m:09s remains)
INFO - root - 2017-12-07 18:24:35.237220: step 39670, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 50h:09m:37s remains)
INFO - root - 2017-12-07 18:24:42.065537: step 39680, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 58h:50m:26s remains)
INFO - root - 2017-12-07 18:24:48.908317: step 39690, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 59h:15m:53s remains)
INFO - root - 2017-12-07 18:24:55.066732: step 39700, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 54h:36m:50s remains)
2017-12-07 18:24:55.816205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2039137 -4.1809416 -4.1805067 -4.1845183 -4.1863642 -4.1766477 -4.1653094 -4.158771 -4.1644015 -4.1828694 -4.2086253 -4.2308092 -4.2415743 -4.2530622 -4.2588739][-4.1742706 -4.1501317 -4.1541152 -4.1653728 -4.1645594 -4.1486316 -4.1338081 -4.1312628 -4.141418 -4.1596155 -4.18017 -4.1901631 -4.1967368 -4.2120056 -4.2210574][-4.1487427 -4.1261635 -4.1366315 -4.1528735 -4.1499763 -4.12756 -4.1079769 -4.1102142 -4.1296735 -4.152185 -4.164639 -4.1598773 -4.1595573 -4.17765 -4.1940751][-4.1390934 -4.128262 -4.1427941 -4.1591563 -4.1551228 -4.1314483 -4.110013 -4.1192632 -4.1534824 -4.18925 -4.2023621 -4.1935153 -4.1886415 -4.1984282 -4.2094817][-4.1526752 -4.1513586 -4.1616778 -4.1713977 -4.1656113 -4.13831 -4.1021538 -4.1010823 -4.1430607 -4.1995897 -4.231082 -4.2384257 -4.2413993 -4.2499151 -4.2552643][-4.1671963 -4.1709652 -4.1731524 -4.16952 -4.1472116 -4.0965419 -4.0277586 -3.9976807 -4.0545521 -4.1422925 -4.2062483 -4.2415009 -4.2628059 -4.277319 -4.2763596][-4.1781425 -4.1802816 -4.1696677 -4.1441641 -4.0925326 -4.00676 -3.8852437 -3.8134637 -3.9021451 -4.0361915 -4.1369834 -4.2031384 -4.2470131 -4.2672973 -4.2618][-4.190588 -4.1822824 -4.1548133 -4.1160712 -4.05432 -3.9596663 -3.8194661 -3.7376122 -3.8455389 -4.0024519 -4.1165166 -4.1904788 -4.2374077 -4.2488184 -4.2311745][-4.2156754 -4.1983352 -4.1694732 -4.1457114 -4.1112089 -4.0552149 -3.9676404 -3.9178665 -3.9928792 -4.1044292 -4.1862736 -4.2394853 -4.2592425 -4.2427225 -4.2021656][-4.2477579 -4.2259383 -4.2075138 -4.2086968 -4.20612 -4.190711 -4.1523781 -4.1254129 -4.1642008 -4.2248383 -4.265749 -4.2883878 -4.2791533 -4.2432308 -4.190598][-4.2667165 -4.2490463 -4.2456503 -4.2610793 -4.2733674 -4.2746372 -4.2566595 -4.23909 -4.2565022 -4.2859411 -4.2983193 -4.2946486 -4.2671432 -4.2248025 -4.1796522][-4.2711439 -4.2591381 -4.2628016 -4.2787862 -4.2910571 -4.2956915 -4.2851992 -4.2706027 -4.2766166 -4.2898664 -4.2868962 -4.2632051 -4.2258382 -4.1939511 -4.1705546][-4.2701707 -4.2646642 -4.2651644 -4.2691493 -4.269114 -4.26789 -4.2548141 -4.2422886 -4.2470789 -4.2614584 -4.2565322 -4.2236013 -4.1892896 -4.175611 -4.1777086][-4.262044 -4.2601461 -4.25224 -4.2388887 -4.2263675 -4.2174683 -4.1965108 -4.1815734 -4.1926446 -4.2192054 -4.2250214 -4.20157 -4.1781631 -4.178093 -4.1937132][-4.2423058 -4.2348027 -4.2168665 -4.1942821 -4.1802149 -4.1692586 -4.1487007 -4.1444807 -4.1674175 -4.2015219 -4.2174253 -4.2115974 -4.2022567 -4.206778 -4.2227712]]...]
INFO - root - 2017-12-07 18:25:02.592380: step 39710, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 56h:24m:50s remains)
INFO - root - 2017-12-07 18:25:09.571396: step 39720, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 61h:08m:48s remains)
INFO - root - 2017-12-07 18:25:16.517861: step 39730, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 56h:58m:07s remains)
INFO - root - 2017-12-07 18:25:23.349747: step 39740, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 52h:23m:33s remains)
INFO - root - 2017-12-07 18:25:30.190796: step 39750, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 57h:25m:11s remains)
INFO - root - 2017-12-07 18:25:36.926984: step 39760, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 58h:26m:54s remains)
INFO - root - 2017-12-07 18:25:43.796550: step 39770, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.715 sec/batch; 58h:07m:01s remains)
INFO - root - 2017-12-07 18:25:50.619222: step 39780, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 54h:11m:53s remains)
INFO - root - 2017-12-07 18:25:57.371865: step 39790, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 51h:40m:32s remains)
INFO - root - 2017-12-07 18:26:04.030384: step 39800, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.670 sec/batch; 54h:30m:15s remains)
2017-12-07 18:26:04.831917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2720203 -4.2684646 -4.2769561 -4.2962046 -4.3175006 -4.3263617 -4.3089228 -4.2719088 -4.2346816 -4.2100267 -4.2034259 -4.2126884 -4.2353849 -4.2567177 -4.2690597][-4.2812605 -4.2867942 -4.2998252 -4.3213854 -4.3391671 -4.3358579 -4.3003035 -4.2496333 -4.2107286 -4.1912012 -4.1921697 -4.2078681 -4.2319613 -4.2549591 -4.2706528][-4.2939272 -4.3066578 -4.3231921 -4.3406954 -4.3424358 -4.3177619 -4.26444 -4.2064281 -4.1768966 -4.1739874 -4.186079 -4.2073288 -4.2306046 -4.2513676 -4.268486][-4.3147097 -4.3338823 -4.3500104 -4.3542209 -4.33361 -4.2838025 -4.2070141 -4.1405907 -4.1287889 -4.1522546 -4.1812639 -4.2104936 -4.2340178 -4.2525759 -4.2685156][-4.3455687 -4.3656411 -4.369215 -4.3498015 -4.301487 -4.2188139 -4.1112638 -4.041831 -4.0614071 -4.1209025 -4.1773624 -4.2205315 -4.2466478 -4.2602358 -4.2723289][-4.3606248 -4.3719187 -4.3568211 -4.3131661 -4.2389936 -4.1239467 -3.9887588 -3.9212317 -3.9829569 -4.08876 -4.1801481 -4.2394676 -4.2632246 -4.2677546 -4.2742167][-4.3522196 -4.3490825 -4.315218 -4.2546391 -4.1667695 -4.031599 -3.8729746 -3.812108 -3.9178131 -4.064589 -4.179204 -4.2493181 -4.2680874 -4.2689018 -4.2749181][-4.3259182 -4.3078642 -4.264358 -4.1974025 -4.1070237 -3.9668624 -3.8017719 -3.7676964 -3.9167161 -4.0819964 -4.1957221 -4.2615318 -4.2734566 -4.2730107 -4.2786732][-4.2987309 -4.2737007 -4.2306595 -4.1666393 -4.0868769 -3.9604998 -3.8238673 -3.8309278 -3.9908297 -4.1368828 -4.2267451 -4.2751675 -4.2809367 -4.2775478 -4.27862][-4.28057 -4.2591705 -4.2210708 -4.16652 -4.1050482 -4.0152607 -3.9315212 -3.9655344 -4.0943322 -4.1972332 -4.2567058 -4.2857943 -4.2859426 -4.2776966 -4.2734842][-4.2792745 -4.2610435 -4.2265282 -4.1827779 -4.1423593 -4.0880961 -4.0483966 -4.0884323 -4.1769772 -4.2429061 -4.2818689 -4.2959113 -4.2878323 -4.2737403 -4.2641068][-4.2796092 -4.2642479 -4.2379818 -4.2102485 -4.1930304 -4.164525 -4.1445289 -4.1699247 -4.2261038 -4.27018 -4.2932382 -4.2944136 -4.2818279 -4.2667928 -4.2528362][-4.2709508 -4.2598748 -4.2442851 -4.2354231 -4.23693 -4.2275372 -4.2170897 -4.22983 -4.2620659 -4.2889409 -4.2990208 -4.29006 -4.2743611 -4.2576184 -4.2395368][-4.260777 -4.2478218 -4.2377782 -4.2438121 -4.2626452 -4.2699542 -4.2686205 -4.2760634 -4.2921824 -4.3042541 -4.3050227 -4.2934318 -4.2774787 -4.2571259 -4.2331777][-4.2507133 -4.232173 -4.22252 -4.238627 -4.269969 -4.287065 -4.2911458 -4.2970676 -4.3044109 -4.3093848 -4.3088861 -4.3013363 -4.2872915 -4.2659879 -4.2398715]]...]
INFO - root - 2017-12-07 18:26:11.635934: step 39810, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.667 sec/batch; 54h:14m:08s remains)
INFO - root - 2017-12-07 18:26:18.443861: step 39820, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 50h:11m:55s remains)
INFO - root - 2017-12-07 18:26:25.345073: step 39830, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 58h:08m:15s remains)
INFO - root - 2017-12-07 18:26:32.192179: step 39840, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 58h:43m:43s remains)
INFO - root - 2017-12-07 18:26:38.944261: step 39850, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 54h:41m:11s remains)
INFO - root - 2017-12-07 18:26:45.736421: step 39860, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 52h:55m:39s remains)
INFO - root - 2017-12-07 18:26:52.455317: step 39870, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 52h:03m:41s remains)
INFO - root - 2017-12-07 18:26:59.199533: step 39880, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:09m:37s remains)
INFO - root - 2017-12-07 18:27:06.043472: step 39890, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 57h:39m:05s remains)
INFO - root - 2017-12-07 18:27:12.669878: step 39900, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 54h:58m:18s remains)
2017-12-07 18:27:13.426526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2927027 -4.3150549 -4.3255396 -4.3223424 -4.3154182 -4.3076148 -4.2971864 -4.281455 -4.2453208 -4.2096038 -4.1983657 -4.2163882 -4.238668 -4.2661023 -4.2942786][-4.2652454 -4.2902036 -4.3028789 -4.3029151 -4.2949753 -4.284903 -4.2748547 -4.2655191 -4.22989 -4.18959 -4.177587 -4.1982775 -4.2245145 -4.2551394 -4.2848134][-4.2301607 -4.261302 -4.2763915 -4.2771463 -4.2666473 -4.2552052 -4.2444873 -4.2434926 -4.2233362 -4.1947536 -4.1868176 -4.2074633 -4.2341447 -4.2619963 -4.2855587][-4.1949625 -4.23247 -4.249373 -4.2479072 -4.2356567 -4.2225733 -4.2094355 -4.2145905 -4.2178168 -4.2116947 -4.2127571 -4.2322617 -4.2557526 -4.2791619 -4.2955456][-4.1620584 -4.200387 -4.2191844 -4.2190804 -4.2070942 -4.1885719 -4.1626749 -4.1608391 -4.1884613 -4.2130971 -4.2306213 -4.2568355 -4.2806864 -4.2987051 -4.3093853][-4.155869 -4.1906743 -4.2047234 -4.19866 -4.1816239 -4.1395345 -4.0756192 -4.0497079 -4.1057477 -4.1729655 -4.2166615 -4.2571588 -4.2891717 -4.3098984 -4.3193479][-4.1845269 -4.2126503 -4.2113261 -4.1856647 -4.148375 -4.0615149 -3.9305003 -3.8675561 -3.9659321 -4.0901775 -4.1676702 -4.2273874 -4.2735887 -4.3034768 -4.31721][-4.2275281 -4.2476358 -4.2329297 -4.1870666 -4.1201072 -3.9904613 -3.7936168 -3.6876373 -3.8224769 -4.0017915 -4.1081672 -4.1812816 -4.2408981 -4.2828913 -4.3024597][-4.2499819 -4.2638497 -4.2479043 -4.1968412 -4.1167216 -3.9760356 -3.774447 -3.6619406 -3.7899961 -3.9716823 -4.08028 -4.1526313 -4.213244 -4.2591238 -4.2818546][-4.2570124 -4.2683482 -4.2578869 -4.2163787 -4.1457829 -4.0356956 -3.8941011 -3.8193226 -3.8966043 -4.0150127 -4.0892572 -4.1458793 -4.1995387 -4.2414527 -4.2632341][-4.2510729 -4.2643676 -4.2638369 -4.2377915 -4.18872 -4.1217847 -4.0418119 -3.9978557 -4.0247717 -4.07513 -4.1106973 -4.1507716 -4.1962 -4.2333369 -4.2552671][-4.2442555 -4.2640119 -4.2738585 -4.2637048 -4.2353396 -4.199594 -4.1550989 -4.1261315 -4.1210766 -4.1261935 -4.1352243 -4.1623034 -4.2030668 -4.2398596 -4.2604594][-4.2455034 -4.2708931 -4.2900105 -4.2913 -4.2762995 -4.2565594 -4.2307277 -4.2100816 -4.1921434 -4.1769547 -4.173234 -4.1911345 -4.225698 -4.2563028 -4.2721486][-4.2626157 -4.2883315 -4.3086176 -4.3139706 -4.3078332 -4.2976856 -4.2833824 -4.2688279 -4.2510743 -4.234231 -4.22512 -4.2331653 -4.2563972 -4.27502 -4.28405][-4.2925172 -4.3115416 -4.3256712 -4.3296556 -4.3268695 -4.3228092 -4.3153019 -4.3074045 -4.2971592 -4.2883482 -4.2802062 -4.2780848 -4.2853889 -4.2920365 -4.296474]]...]
INFO - root - 2017-12-07 18:27:20.216094: step 39910, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 57h:57m:23s remains)
INFO - root - 2017-12-07 18:27:26.988398: step 39920, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 57h:51m:04s remains)
INFO - root - 2017-12-07 18:27:33.588413: step 39930, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.636 sec/batch; 51h:39m:07s remains)
INFO - root - 2017-12-07 18:27:40.376555: step 39940, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 55h:20m:24s remains)
INFO - root - 2017-12-07 18:27:47.271491: step 39950, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 59h:47m:15s remains)
INFO - root - 2017-12-07 18:27:54.089841: step 39960, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 56h:26m:18s remains)
INFO - root - 2017-12-07 18:28:00.929160: step 39970, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 54h:22m:16s remains)
INFO - root - 2017-12-07 18:28:07.774747: step 39980, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 53h:29m:48s remains)
INFO - root - 2017-12-07 18:28:14.695315: step 39990, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.759 sec/batch; 61h:40m:34s remains)
INFO - root - 2017-12-07 18:28:21.355734: step 40000, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 59h:12m:01s remains)
2017-12-07 18:28:22.005160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2796769 -4.282865 -4.291882 -4.3011994 -4.3055482 -4.3047714 -4.2994471 -4.2916 -4.2868462 -4.2870989 -4.2895207 -4.2937784 -4.2978225 -4.2982321 -4.2959948][-4.2713418 -4.274003 -4.283473 -4.2913704 -4.2915521 -4.2835522 -4.2677245 -4.2496891 -4.238584 -4.2365627 -4.2392468 -4.2481594 -4.2627707 -4.275115 -4.2824583][-4.280508 -4.2820039 -4.2894268 -4.2927651 -4.284894 -4.2658281 -4.23579 -4.2058439 -4.1881962 -4.1827097 -4.183495 -4.1938534 -4.2181253 -4.2473583 -4.2709475][-4.3023553 -4.303587 -4.3085494 -4.3061748 -4.290329 -4.2616839 -4.2196856 -4.1769419 -4.1495538 -4.1372147 -4.1304846 -4.1361709 -4.1670423 -4.2134547 -4.2545743][-4.3263297 -4.3287125 -4.333075 -4.3283463 -4.3086166 -4.2765503 -4.2286477 -4.1747966 -4.1328616 -4.1050835 -4.0829782 -4.0776768 -4.1070948 -4.1616158 -4.2161942][-4.3403473 -4.3446937 -4.3493953 -4.3452187 -4.3267217 -4.2962775 -4.246994 -4.1879086 -4.1343408 -4.0896292 -4.0511165 -4.0318089 -4.0496335 -4.09957 -4.1584997][-4.3463531 -4.3520517 -4.3567672 -4.3523149 -4.3361287 -4.3069572 -4.2582936 -4.2002468 -4.1459336 -4.0958915 -4.0512328 -4.0215716 -4.0190887 -4.0505638 -4.09907][-4.3485131 -4.3545504 -4.3585124 -4.3536849 -4.3379412 -4.3066711 -4.2572718 -4.206203 -4.1622415 -4.1209755 -4.0830979 -4.0498042 -4.0268807 -4.0327806 -4.0558481][-4.34967 -4.3542094 -4.35603 -4.3508258 -4.3337765 -4.299192 -4.2497754 -4.2079482 -4.1773324 -4.1499214 -4.1237025 -4.0925956 -4.0612087 -4.048038 -4.0464253][-4.3510695 -4.3537264 -4.3521852 -4.3455811 -4.3266497 -4.2903247 -4.243082 -4.2104316 -4.1882768 -4.1693883 -4.1546659 -4.1314478 -4.1033416 -4.0850677 -4.070518][-4.352571 -4.3521895 -4.3470616 -4.3370008 -4.3134689 -4.2751732 -4.2349916 -4.211164 -4.1956506 -4.1837358 -4.1807981 -4.1681428 -4.1488118 -4.1350374 -4.1227512][-4.35225 -4.3488317 -4.3408742 -4.3263636 -4.29734 -4.2602906 -4.2295861 -4.2126431 -4.2023921 -4.1957846 -4.1986589 -4.1946373 -4.1857233 -4.1831646 -4.1846128][-4.3501635 -4.3448386 -4.3337135 -4.313406 -4.2818012 -4.2502685 -4.2266941 -4.210906 -4.2015128 -4.1962523 -4.2020531 -4.2064672 -4.2099824 -4.2209082 -4.2378516][-4.3418894 -4.3335743 -4.3172216 -4.2919669 -4.2608314 -4.2352185 -4.2178926 -4.2049894 -4.1950731 -4.18919 -4.1944122 -4.2048841 -4.2169089 -4.2366691 -4.2657328][-4.3268929 -4.3109016 -4.2865467 -4.2576666 -4.2282114 -4.2043691 -4.1861243 -4.1732793 -4.1641331 -4.1624331 -4.1713591 -4.1869426 -4.2059536 -4.2323689 -4.2672915]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 18:28:29.804246: step 40010, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 60h:06m:08s remains)
INFO - root - 2017-12-07 18:28:36.589208: step 40020, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 52h:56m:32s remains)
INFO - root - 2017-12-07 18:28:43.447953: step 40030, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 50h:58m:00s remains)
INFO - root - 2017-12-07 18:28:50.353905: step 40040, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 59h:21m:49s remains)
INFO - root - 2017-12-07 18:28:57.210764: step 40050, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 58h:54m:41s remains)
INFO - root - 2017-12-07 18:29:03.991113: step 40060, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 54h:37m:19s remains)
INFO - root - 2017-12-07 18:29:10.743080: step 40070, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 51h:32m:57s remains)
INFO - root - 2017-12-07 18:29:17.632625: step 40080, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 55h:26m:19s remains)
INFO - root - 2017-12-07 18:29:24.468816: step 40090, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 57h:33m:49s remains)
INFO - root - 2017-12-07 18:29:31.073704: step 40100, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 54h:04m:51s remains)
2017-12-07 18:29:31.804205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0511746 -4.1119976 -4.169208 -4.2066436 -4.1907082 -4.1542716 -4.1244311 -4.1374187 -4.17684 -4.2026544 -4.2225194 -4.2430768 -4.2554784 -4.2437229 -4.2089572][-3.9899766 -4.0647359 -4.1348057 -4.1868472 -4.1823673 -4.1578021 -4.1355376 -4.1518841 -4.1896825 -4.2033172 -4.2118969 -4.2267504 -4.2352424 -4.2257938 -4.1918931][-3.9749434 -4.0393124 -4.1117053 -4.17091 -4.1830263 -4.1704831 -4.1550336 -4.1699753 -4.203156 -4.2118917 -4.2107258 -4.2209868 -4.2274141 -4.2185025 -4.1859756][-4.0254135 -4.0612626 -4.1161461 -4.1617937 -4.1774235 -4.1713791 -4.162951 -4.1765256 -4.2073197 -4.2176504 -4.2155733 -4.223835 -4.2309971 -4.2217321 -4.1844845][-4.1101208 -4.12189 -4.1529765 -4.1731577 -4.1702518 -4.1576533 -4.1412024 -4.1464558 -4.1722245 -4.1934414 -4.20314 -4.2229362 -4.239996 -4.2332234 -4.1925621][-4.1688681 -4.1749291 -4.1900864 -4.1849952 -4.1555839 -4.1233783 -4.0785594 -4.0569429 -4.0861697 -4.1308055 -4.1672907 -4.2099018 -4.2375693 -4.2383161 -4.20144][-4.1832404 -4.1893687 -4.1960597 -4.1750712 -4.1261787 -4.0638537 -3.9671128 -3.8982117 -3.9447985 -4.0337338 -4.1063828 -4.173018 -4.215137 -4.2314677 -4.2078333][-4.1776366 -4.1852131 -4.1889925 -4.1646824 -4.1092029 -4.01629 -3.8511319 -3.7021613 -3.7625079 -3.9096937 -4.0270429 -4.1177669 -4.168992 -4.1935568 -4.1861124][-4.1820426 -4.1914768 -4.2024641 -4.1883292 -4.1406722 -4.0439711 -3.860847 -3.6626921 -3.6980941 -3.8594897 -3.9947574 -4.08708 -4.1313038 -4.1543736 -4.156055][-4.1833372 -4.1911383 -4.2134695 -4.2227926 -4.1949921 -4.1215 -3.9823487 -3.8308847 -3.8312104 -3.9416413 -4.0473104 -4.1157107 -4.1426377 -4.1564512 -4.1581445][-4.1716456 -4.1727095 -4.1965647 -4.2263174 -4.2299204 -4.1888642 -4.0988231 -4.0020142 -3.9905281 -4.0547652 -4.1238852 -4.1651464 -4.1738629 -4.1708865 -4.1610537][-4.1647167 -4.1581235 -4.1700954 -4.2049632 -4.2272587 -4.2053761 -4.1460023 -4.0836945 -4.070889 -4.1125355 -4.1576152 -4.1836524 -4.1810546 -4.1625109 -4.1399045][-4.1862416 -4.1729259 -4.1633182 -4.1794281 -4.1988387 -4.1783881 -4.1330738 -4.0971537 -4.0977912 -4.1315718 -4.1659679 -4.188807 -4.1815629 -4.151103 -4.1160097][-4.2175307 -4.2006855 -4.178792 -4.1731362 -4.1794648 -4.1555014 -4.1130228 -4.0914631 -4.1086054 -4.1464667 -4.1768947 -4.1992598 -4.1890073 -4.1514206 -4.109622][-4.2322049 -4.2166119 -4.1949496 -4.1851811 -4.1857204 -4.1656904 -4.125246 -4.1051679 -4.1256957 -4.1627235 -4.1907024 -4.2110939 -4.201591 -4.1621995 -4.1193843]]...]
INFO - root - 2017-12-07 18:29:38.637831: step 40110, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 58h:42m:38s remains)
INFO - root - 2017-12-07 18:29:45.431197: step 40120, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 59h:13m:40s remains)
INFO - root - 2017-12-07 18:29:52.221048: step 40130, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 56h:28m:15s remains)
INFO - root - 2017-12-07 18:29:59.035854: step 40140, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 51h:53m:52s remains)
INFO - root - 2017-12-07 18:30:05.955221: step 40150, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 57h:13m:11s remains)
INFO - root - 2017-12-07 18:30:12.834361: step 40160, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 58h:32m:09s remains)
INFO - root - 2017-12-07 18:30:19.686980: step 40170, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 54h:31m:05s remains)
INFO - root - 2017-12-07 18:30:26.532233: step 40180, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.622 sec/batch; 50h:29m:20s remains)
INFO - root - 2017-12-07 18:30:33.371640: step 40190, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 54h:03m:25s remains)
INFO - root - 2017-12-07 18:30:40.178757: step 40200, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 59h:20m:04s remains)
2017-12-07 18:30:40.853756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3133435 -4.3071847 -4.2908058 -4.2684956 -4.2535248 -4.2465115 -4.2368193 -4.2362657 -4.2516217 -4.2672882 -4.2780361 -4.2895684 -4.299088 -4.3060427 -4.3102889][-4.3170266 -4.3129792 -4.2968111 -4.269928 -4.2466021 -4.2336597 -4.224463 -4.229218 -4.2509427 -4.2689691 -4.2793369 -4.2864389 -4.28815 -4.2887592 -4.2878404][-4.3228388 -4.3232865 -4.3092656 -4.2792664 -4.24662 -4.2216878 -4.20668 -4.2130847 -4.2398171 -4.2633362 -4.2783937 -4.2852621 -4.2800651 -4.2741752 -4.2673054][-4.3292809 -4.3339682 -4.32281 -4.289947 -4.2442985 -4.2006092 -4.1714444 -4.1746902 -4.2076378 -4.244607 -4.2705221 -4.2798724 -4.2706318 -4.2591271 -4.2473083][-4.3349533 -4.3422318 -4.332458 -4.2947712 -4.2337284 -4.1667528 -4.1176896 -4.1153188 -4.1579041 -4.212297 -4.2522035 -4.2669849 -4.2592049 -4.2422018 -4.2252684][-4.3398066 -4.3470383 -4.3347425 -4.2891569 -4.2128015 -4.1230006 -4.0519061 -4.0458612 -4.1037655 -4.1771011 -4.2296405 -4.2521877 -4.2503095 -4.2288713 -4.206244][-4.3433166 -4.3487916 -4.3310556 -4.2766991 -4.1895747 -4.0855203 -3.9994657 -3.9914005 -4.0631528 -4.1518259 -4.2146668 -4.2434525 -4.2472773 -4.22415 -4.2002335][-4.3451991 -4.3494377 -4.3271351 -4.2680187 -4.1780081 -4.0722971 -3.9812293 -3.9696856 -4.0421028 -4.1357636 -4.2068744 -4.2415595 -4.2476254 -4.2212725 -4.19349][-4.3466315 -4.3500004 -4.3282018 -4.27105 -4.1867256 -4.0899568 -4.0028586 -3.9858069 -4.0450978 -4.1314387 -4.2030673 -4.2369108 -4.2400675 -4.20918 -4.1766996][-4.3470473 -4.3493853 -4.3305359 -4.280652 -4.208055 -4.126616 -4.0529227 -4.0319633 -4.0725756 -4.1413989 -4.203301 -4.2312188 -4.2293444 -4.1949677 -4.1579833][-4.3479533 -4.3504186 -4.3357511 -4.2946854 -4.2352881 -4.1701279 -4.1150069 -4.0943937 -4.1161342 -4.1636906 -4.2091146 -4.22516 -4.2165232 -4.18136 -4.1445265][-4.3486915 -4.3530159 -4.3439603 -4.3107862 -4.2603073 -4.2082877 -4.1689563 -4.1512232 -4.1566529 -4.1837091 -4.210608 -4.2160549 -4.20212 -4.1681461 -4.1372561][-4.3493814 -4.3569236 -4.3527303 -4.3254952 -4.281693 -4.2366352 -4.2049718 -4.1887484 -4.183044 -4.1947408 -4.2060471 -4.1972466 -4.1768665 -4.1484747 -4.1311026][-4.3496332 -4.3605518 -4.3607445 -4.3372154 -4.2953968 -4.2523222 -4.2231693 -4.2069426 -4.1932282 -4.1922622 -4.1901765 -4.1680408 -4.1415558 -4.11846 -4.11501][-4.3481779 -4.3605971 -4.3637686 -4.3418903 -4.2996707 -4.2546358 -4.2247872 -4.2113152 -4.1994314 -4.1925488 -4.1810842 -4.1516294 -4.1196213 -4.0960813 -4.0979366]]...]
INFO - root - 2017-12-07 18:30:47.577464: step 40210, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 50h:55m:25s remains)
INFO - root - 2017-12-07 18:30:54.456757: step 40220, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 55h:57m:51s remains)
INFO - root - 2017-12-07 18:31:01.283042: step 40230, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 58h:41m:27s remains)
INFO - root - 2017-12-07 18:31:07.915484: step 40240, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 55h:15m:15s remains)
INFO - root - 2017-12-07 18:31:14.718014: step 40250, loss = 2.09, batch loss = 2.03 (13.2 examples/sec; 0.606 sec/batch; 49h:11m:04s remains)
INFO - root - 2017-12-07 18:31:21.594644: step 40260, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 52h:44m:41s remains)
INFO - root - 2017-12-07 18:31:28.454255: step 40270, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 59h:38m:28s remains)
INFO - root - 2017-12-07 18:31:35.288183: step 40280, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 56h:41m:18s remains)
INFO - root - 2017-12-07 18:31:42.180873: step 40290, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.666 sec/batch; 54h:04m:50s remains)
INFO - root - 2017-12-07 18:31:48.699949: step 40300, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 50h:39m:41s remains)
2017-12-07 18:31:49.467408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2733092 -4.2730985 -4.2711916 -4.2605453 -4.255662 -4.2558513 -4.2470651 -4.2201571 -4.1903558 -4.1834269 -4.2012978 -4.2204952 -4.2238989 -4.2213364 -4.2328172][-4.2688451 -4.2741227 -4.2714109 -4.2567592 -4.2504735 -4.2482147 -4.2283525 -4.1860046 -4.1481614 -4.1489344 -4.1815195 -4.210659 -4.2116652 -4.2029614 -4.2134418][-4.267065 -4.2773428 -4.2745986 -4.2596359 -4.2520714 -4.2422109 -4.2067108 -4.1509428 -4.1080756 -4.1179991 -4.1671076 -4.2078471 -4.206368 -4.19163 -4.1966681][-4.2686954 -4.2795534 -4.2759619 -4.263052 -4.2529774 -4.2298918 -4.1756434 -4.1036797 -4.0578337 -4.0818639 -4.1496797 -4.20243 -4.2028494 -4.1862016 -4.18606][-4.2701659 -4.2777624 -4.2719574 -4.2576318 -4.2357121 -4.1887374 -4.1062274 -4.0123515 -3.9662423 -4.0111408 -4.105083 -4.1774344 -4.1886649 -4.1764917 -4.1764693][-4.2670512 -4.2700639 -4.259316 -4.234839 -4.1922255 -4.1151419 -3.999953 -3.87953 -3.8364716 -3.9140909 -4.042666 -4.1404972 -4.1706715 -4.1682353 -4.1689067][-4.26586 -4.2649713 -4.2444057 -4.2054982 -4.142467 -4.0417557 -3.8991036 -3.7648234 -3.7366719 -3.8528383 -4.013864 -4.1287317 -4.1672034 -4.1711388 -4.1727939][-4.2750916 -4.2720046 -4.2420073 -4.1923347 -4.1178131 -4.0050039 -3.8501623 -3.7335699 -3.7425776 -3.886322 -4.0564909 -4.1635666 -4.19379 -4.1941681 -4.1953173][-4.2830162 -4.27809 -4.2446027 -4.1951737 -4.1282086 -4.0179009 -3.8680177 -3.7873003 -3.8444085 -3.9890163 -4.1275392 -4.2065907 -4.2241545 -4.2165971 -4.21681][-4.2916427 -4.2857137 -4.2555876 -4.2184329 -4.1671023 -4.0739474 -3.9486063 -3.9011648 -3.9760501 -4.0915089 -4.1834874 -4.23252 -4.2416511 -4.2346482 -4.2352715][-4.297153 -4.2902327 -4.26856 -4.2449975 -4.2100964 -4.1442051 -4.0535808 -4.0291834 -4.0922832 -4.1710267 -4.2218504 -4.2471147 -4.2510657 -4.2467566 -4.2486429][-4.2971354 -4.2910566 -4.2758222 -4.2633615 -4.2401209 -4.1985707 -4.1437855 -4.1348276 -4.1791735 -4.2239366 -4.2422915 -4.2472634 -4.2463179 -4.24553 -4.2537675][-4.2937641 -4.2886982 -4.2788115 -4.2742276 -4.2618685 -4.2372613 -4.2097406 -4.2089691 -4.2337408 -4.2480679 -4.2453146 -4.2378807 -4.2376642 -4.24602 -4.2617946][-4.296391 -4.2915316 -4.2853603 -4.2833576 -4.2779284 -4.2634854 -4.2492223 -4.2507982 -4.2622995 -4.2617574 -4.2488818 -4.2416506 -4.249754 -4.2653713 -4.282454][-4.3063664 -4.3029594 -4.298594 -4.2950344 -4.289021 -4.28095 -4.2729917 -4.2746782 -4.2801404 -4.2746582 -4.2633405 -4.2639284 -4.2767186 -4.2910075 -4.3039856]]...]
INFO - root - 2017-12-07 18:31:56.307743: step 40310, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 57h:46m:13s remains)
INFO - root - 2017-12-07 18:32:03.114716: step 40320, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 54h:56m:33s remains)
INFO - root - 2017-12-07 18:32:10.015768: step 40330, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 52h:34m:36s remains)
INFO - root - 2017-12-07 18:32:16.833459: step 40340, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 58h:52m:17s remains)
INFO - root - 2017-12-07 18:32:23.664140: step 40350, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.744 sec/batch; 60h:24m:53s remains)
INFO - root - 2017-12-07 18:32:30.494383: step 40360, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 54h:04m:52s remains)
INFO - root - 2017-12-07 18:32:37.284069: step 40370, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 50h:44m:21s remains)
INFO - root - 2017-12-07 18:32:44.107365: step 40380, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 50h:46m:16s remains)
INFO - root - 2017-12-07 18:32:51.068705: step 40390, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 58h:44m:51s remains)
INFO - root - 2017-12-07 18:32:57.682659: step 40400, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 57h:45m:39s remains)
2017-12-07 18:32:58.338128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2942266 -4.2781081 -4.2543397 -4.2287469 -4.19076 -4.1670732 -4.1995239 -4.2431064 -4.2712874 -4.2828383 -4.287447 -4.2921548 -4.2967281 -4.3002787 -4.3022223][-4.2959027 -4.2836504 -4.2680392 -4.2529316 -4.2263021 -4.2034736 -4.2224479 -4.2539954 -4.2779212 -4.2889376 -4.2962546 -4.3035479 -4.31073 -4.3164916 -4.3204384][-4.2980075 -4.288743 -4.2769532 -4.2641921 -4.2392149 -4.2145562 -4.2206578 -4.2454958 -4.2697806 -4.2829647 -4.2920713 -4.3014488 -4.3114805 -4.3205914 -4.3278074][-4.3009968 -4.2915821 -4.2771487 -4.254426 -4.21872 -4.1891503 -4.1885824 -4.2123079 -4.2388406 -4.2558951 -4.2677627 -4.2826815 -4.2990022 -4.3134351 -4.3239193][-4.3044524 -4.2904549 -4.2692966 -4.2296576 -4.1784787 -4.1435056 -4.1420121 -4.1667585 -4.192874 -4.2095003 -4.2259116 -4.2529726 -4.280045 -4.2996778 -4.3137269][-4.31186 -4.2908559 -4.2594471 -4.2013144 -4.1332865 -4.0922952 -4.0925884 -4.117485 -4.1362991 -4.1466436 -4.1647549 -4.2084508 -4.2508087 -4.2800679 -4.2985497][-4.3197589 -4.290905 -4.2506242 -4.1795516 -4.0977325 -4.0523086 -4.0550718 -4.0744929 -4.07941 -4.0760317 -4.0957165 -4.1579981 -4.218605 -4.2597408 -4.2831378][-4.3198886 -4.2861953 -4.2432675 -4.1682777 -4.0818143 -4.0371032 -4.0460677 -4.0596123 -4.0468373 -4.0253882 -4.0459781 -4.1259089 -4.2032542 -4.2538042 -4.2791595][-4.3178449 -4.2818356 -4.239223 -4.1681089 -4.0894451 -4.0508757 -4.0684505 -4.0804186 -4.060708 -4.0295944 -4.0466928 -4.1324053 -4.2156563 -4.2679338 -4.2881041][-4.3176365 -4.283885 -4.2478528 -4.1918187 -4.1268487 -4.0912123 -4.1100445 -4.1241655 -4.1086025 -4.0778413 -4.0878153 -4.1633062 -4.2369356 -4.2816105 -4.2953749][-4.3162651 -4.2894473 -4.2647624 -4.2276673 -4.1808143 -4.1509771 -4.1680231 -4.1849537 -4.1757083 -4.1519918 -4.1568136 -4.2096171 -4.2623096 -4.2923946 -4.2986922][-4.3160849 -4.2959409 -4.2807026 -4.2611327 -4.2310872 -4.2084637 -4.2206821 -4.2393551 -4.2384038 -4.2247906 -4.2267561 -4.2554955 -4.2852721 -4.2995133 -4.3010902][-4.316134 -4.3000965 -4.2915554 -4.2832618 -4.265152 -4.2501144 -4.2594867 -4.2793107 -4.2872586 -4.28327 -4.2830977 -4.294703 -4.3065858 -4.3107152 -4.3101029][-4.3234148 -4.31009 -4.3040481 -4.3011508 -4.2904329 -4.284205 -4.2971444 -4.3168955 -4.3278503 -4.3271675 -4.3250551 -4.3269029 -4.3298817 -4.3306708 -4.3296895][-4.3328424 -4.3196697 -4.3102756 -4.3026824 -4.2910833 -4.2891383 -4.306932 -4.3281765 -4.3399906 -4.3398876 -4.33763 -4.3369532 -4.3377409 -4.3383222 -4.3376365]]...]
INFO - root - 2017-12-07 18:33:05.149414: step 40410, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 52h:57m:26s remains)
INFO - root - 2017-12-07 18:33:12.040160: step 40420, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 58h:21m:12s remains)
INFO - root - 2017-12-07 18:33:18.863918: step 40430, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.761 sec/batch; 61h:46m:47s remains)
INFO - root - 2017-12-07 18:33:25.638415: step 40440, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 55h:45m:30s remains)
INFO - root - 2017-12-07 18:33:32.384046: step 40450, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 51h:55m:48s remains)
INFO - root - 2017-12-07 18:33:39.128678: step 40460, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.661 sec/batch; 53h:36m:50s remains)
INFO - root - 2017-12-07 18:33:46.092841: step 40470, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 57h:41m:35s remains)
INFO - root - 2017-12-07 18:33:53.073464: step 40480, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.761 sec/batch; 61h:43m:26s remains)
INFO - root - 2017-12-07 18:33:59.933214: step 40490, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 53h:27m:50s remains)
INFO - root - 2017-12-07 18:34:06.565916: step 40500, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 52h:20m:03s remains)
2017-12-07 18:34:07.346811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9716249 -3.9670835 -4.0077868 -4.0578465 -4.1271834 -4.1922231 -4.2394619 -4.27053 -4.2914953 -4.2968707 -4.2878704 -4.2673893 -4.2528272 -4.2505889 -4.2479296][-4.0648036 -4.0642033 -4.0939174 -4.1327686 -4.1922846 -4.2436852 -4.2749648 -4.2927217 -4.301084 -4.2991233 -4.28979 -4.2778354 -4.2748876 -4.2791939 -4.2805][-4.1532249 -4.1552649 -4.17866 -4.2093387 -4.2496333 -4.2779775 -4.2897363 -4.2947021 -4.2922072 -4.2853651 -4.2793603 -4.2792034 -4.2891803 -4.3005753 -4.3033447][-4.23242 -4.2324591 -4.2496915 -4.27105 -4.2883091 -4.292006 -4.2890544 -4.2826786 -4.2679987 -4.258224 -4.259398 -4.2722416 -4.2912841 -4.3064661 -4.3112173][-4.2968359 -4.2983966 -4.3079858 -4.3107138 -4.3000422 -4.2745657 -4.2492046 -4.2224617 -4.1973262 -4.1926727 -4.208569 -4.2382936 -4.2688055 -4.2870016 -4.296021][-4.3158469 -4.3220096 -4.3260446 -4.3128166 -4.2740536 -4.2196035 -4.1654196 -4.1144853 -4.0894332 -4.1032991 -4.1411972 -4.186574 -4.227282 -4.2525744 -4.2695231][-4.3061814 -4.3096433 -4.3027015 -4.2703772 -4.2029643 -4.1162982 -4.0279207 -3.9587097 -3.9497149 -3.99434 -4.0583506 -4.1189079 -4.1747046 -4.2133355 -4.2410235][-4.2987051 -4.2886691 -4.2584519 -4.2036815 -4.1146774 -4.003458 -3.8881521 -3.8170218 -3.8448343 -3.9173114 -3.989305 -4.0542274 -4.1196232 -4.1697736 -4.2069526][-4.2922821 -4.2644553 -4.2160549 -4.1489439 -4.0578442 -3.9507086 -3.8472755 -3.808249 -3.8651359 -3.9394252 -4.0009794 -4.0601473 -4.1213932 -4.1688004 -4.2042031][-4.2889829 -4.25788 -4.211544 -4.157526 -4.0942435 -4.0247521 -3.9670815 -3.9586236 -4.0063367 -4.055584 -4.0966787 -4.1413693 -4.1840963 -4.2155972 -4.2381921][-4.2876606 -4.2638526 -4.2347074 -4.2088976 -4.1836953 -4.1574287 -4.1369519 -4.1352234 -4.1596823 -4.1817379 -4.2059121 -4.2340808 -4.2565904 -4.2704 -4.2800412][-4.2874889 -4.2712917 -4.2580328 -4.2532492 -4.2527475 -4.2526007 -4.2506104 -4.250051 -4.2606769 -4.2688122 -4.2827768 -4.2973156 -4.3057165 -4.3091478 -4.3107357][-4.2892008 -4.2785649 -4.2746992 -4.279665 -4.2882562 -4.2967319 -4.29812 -4.2945795 -4.2974586 -4.3017764 -4.3115873 -4.3222589 -4.3254323 -4.3244076 -4.322938][-4.298718 -4.2929921 -4.292841 -4.2984586 -4.3070827 -4.3144774 -4.3154693 -4.3112092 -4.3099127 -4.3112307 -4.3169479 -4.3224783 -4.3236866 -4.3220615 -4.3193469][-4.3072987 -4.3044076 -4.3038707 -4.3062806 -4.3110819 -4.3158236 -4.3181748 -4.3169651 -4.3148961 -4.3140841 -4.3161263 -4.3173347 -4.3168492 -4.31603 -4.3142252]]...]
INFO - root - 2017-12-07 18:34:14.160855: step 40510, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 55h:19m:20s remains)
INFO - root - 2017-12-07 18:34:21.023546: step 40520, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 53h:11m:41s remains)
INFO - root - 2017-12-07 18:34:27.884143: step 40530, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 53h:34m:33s remains)
INFO - root - 2017-12-07 18:34:34.656856: step 40540, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 59h:02m:12s remains)
INFO - root - 2017-12-07 18:34:41.293903: step 40550, loss = 2.03, batch loss = 1.98 (12.0 examples/sec; 0.669 sec/batch; 54h:17m:15s remains)
INFO - root - 2017-12-07 18:34:48.103387: step 40560, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 51h:09m:50s remains)
INFO - root - 2017-12-07 18:34:54.884789: step 40570, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 52h:13m:37s remains)
INFO - root - 2017-12-07 18:35:01.748758: step 40580, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 59h:53m:53s remains)
INFO - root - 2017-12-07 18:35:08.705741: step 40590, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 58h:07m:07s remains)
INFO - root - 2017-12-07 18:35:15.233455: step 40600, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 57h:02m:51s remains)
2017-12-07 18:35:15.958079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3324189 -4.3398809 -4.3404655 -4.3358507 -4.3319097 -4.33115 -4.3289237 -4.3277988 -4.329596 -4.3267627 -4.3177185 -4.3079853 -4.2976317 -4.2831059 -4.2706194][-4.3346534 -4.3398638 -4.3383145 -4.3308625 -4.326057 -4.3262787 -4.3245707 -4.3230133 -4.3257823 -4.32374 -4.3126349 -4.2956142 -4.2748756 -4.2531781 -4.2425776][-4.3341579 -4.3338389 -4.3282666 -4.3168745 -4.3095832 -4.3090782 -4.3058639 -4.3024526 -4.30512 -4.3052492 -4.2963529 -4.2798004 -4.2577858 -4.238102 -4.2340097][-4.3280926 -4.322526 -4.3125343 -4.2958503 -4.2823138 -4.2734342 -4.2598696 -4.2492023 -4.2511849 -4.2600946 -4.2658391 -4.2656703 -4.2584395 -4.2498894 -4.252358][-4.3138466 -4.3064246 -4.2941766 -4.2717237 -4.2478242 -4.2213507 -4.1868806 -4.1602907 -4.1587596 -4.1825347 -4.2159595 -4.2465267 -4.2639647 -4.2697353 -4.2749486][-4.3006763 -4.2958179 -4.2842116 -4.2584138 -4.2219858 -4.1698842 -4.104023 -4.0525451 -4.0448642 -4.0879235 -4.1541576 -4.2193317 -4.2630844 -4.2833853 -4.2896557][-4.2941785 -4.2922373 -4.2814317 -4.2529521 -4.206182 -4.1296549 -4.0308867 -3.9526544 -3.9410346 -4.0042176 -4.100584 -4.1952667 -4.259954 -4.2891846 -4.292398][-4.2956076 -4.2937365 -4.2815304 -4.2519288 -4.2023983 -4.112236 -3.9891069 -3.8903761 -3.8776636 -3.9557214 -4.068902 -4.178493 -4.253881 -4.286798 -4.285284][-4.2991185 -4.2948666 -4.2794218 -4.2478347 -4.2017617 -4.1144409 -3.9898419 -3.888166 -3.8809803 -3.9621618 -4.0716939 -4.1751618 -4.2479548 -4.28095 -4.2761307][-4.3026266 -4.2998538 -4.2861533 -4.2587676 -4.22205 -4.1516428 -4.0476904 -3.9640326 -3.9579525 -4.0241609 -4.1110311 -4.1904407 -4.2501974 -4.2806048 -4.277247][-4.2995191 -4.30411 -4.299161 -4.28383 -4.2617283 -4.2113013 -4.1298971 -4.0612721 -4.0504069 -4.0958171 -4.1568522 -4.2113214 -4.25858 -4.2872992 -4.290863][-4.2818937 -4.2958665 -4.3008089 -4.2964544 -4.2858024 -4.2503591 -4.1836848 -4.1224074 -4.1070318 -4.1379437 -4.1823215 -4.2235575 -4.2645206 -4.2928262 -4.3024774][-4.2713346 -4.2914462 -4.299365 -4.2984872 -4.2945819 -4.26975 -4.2144608 -4.1591129 -4.1404915 -4.1638064 -4.1993704 -4.2363439 -4.2732515 -4.2979035 -4.3061876][-4.2772903 -4.2939763 -4.2948723 -4.2912021 -4.2922225 -4.2794232 -4.2418981 -4.2046385 -4.1934948 -4.2116427 -4.2369184 -4.2621427 -4.2856016 -4.298491 -4.2994466][-4.2937913 -4.2993126 -4.2896981 -4.2822804 -4.2884727 -4.2899694 -4.2752481 -4.26381 -4.2673635 -4.2794456 -4.287993 -4.2932734 -4.2952147 -4.292345 -4.2855244]]...]
INFO - root - 2017-12-07 18:35:22.797062: step 40610, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.760 sec/batch; 61h:36m:45s remains)
INFO - root - 2017-12-07 18:35:29.541861: step 40620, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 55h:37m:44s remains)
INFO - root - 2017-12-07 18:35:36.295693: step 40630, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.678 sec/batch; 54h:56m:39s remains)
INFO - root - 2017-12-07 18:35:43.112333: step 40640, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 52h:03m:11s remains)
INFO - root - 2017-12-07 18:35:49.935301: step 40650, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 57h:15m:53s remains)
INFO - root - 2017-12-07 18:35:56.796342: step 40660, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 59h:27m:16s remains)
INFO - root - 2017-12-07 18:36:03.549720: step 40670, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 54h:17m:04s remains)
INFO - root - 2017-12-07 18:36:10.352243: step 40680, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 53h:30m:53s remains)
INFO - root - 2017-12-07 18:36:17.175448: step 40690, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 50h:55m:47s remains)
INFO - root - 2017-12-07 18:36:23.906519: step 40700, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 59h:23m:58s remains)
2017-12-07 18:36:24.622187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1634865 -4.1865063 -4.2151732 -4.2210054 -4.2016311 -4.1755042 -4.1678524 -4.1800046 -4.1945715 -4.1995869 -4.1952772 -4.204258 -4.2161837 -4.2293882 -4.2565107][-4.1938281 -4.2048488 -4.2123146 -4.2010803 -4.1767097 -4.1575031 -4.1573429 -4.17804 -4.1944227 -4.1993155 -4.1942739 -4.2048721 -4.2150111 -4.2225256 -4.24531][-4.2245154 -4.22344 -4.2133904 -4.1903992 -4.1627421 -4.1433754 -4.1471653 -4.174912 -4.1978006 -4.2088037 -4.2089753 -4.2249327 -4.2338691 -4.2334671 -4.2464862][-4.2263193 -4.215795 -4.1981611 -4.1763029 -4.149539 -4.12684 -4.1311045 -4.1673417 -4.202311 -4.2246232 -4.2326174 -4.2523975 -4.2590895 -4.2517042 -4.2557983][-4.2135491 -4.1974378 -4.1772375 -4.14971 -4.1140795 -4.0834079 -4.0835557 -4.1290107 -4.1792259 -4.2172465 -4.2358332 -4.2579451 -4.266511 -4.2624655 -4.2663436][-4.2125287 -4.1904926 -4.1611066 -4.118504 -4.0582209 -4.0038285 -3.9916902 -4.0477633 -4.1177812 -4.17293 -4.1996374 -4.2246084 -4.2346044 -4.2426538 -4.2546625][-4.210331 -4.1849546 -4.1525517 -4.1002479 -4.0151081 -3.9238024 -3.8829958 -3.9407675 -4.02554 -4.0964141 -4.1370912 -4.1663489 -4.1788611 -4.1978016 -4.22115][-4.1971951 -4.1793652 -4.1536078 -4.1073837 -4.0209432 -3.9173794 -3.8591981 -3.899534 -3.974431 -4.04645 -4.0957084 -4.1308846 -4.152082 -4.1807451 -4.2072077][-4.1876431 -4.1897249 -4.1787467 -4.1492934 -4.0933275 -4.020978 -3.9801555 -3.9996479 -4.0404854 -4.0797172 -4.1091928 -4.1356854 -4.1611419 -4.1934733 -4.2183332][-4.1959691 -4.2056265 -4.199677 -4.1844215 -4.1526127 -4.1030722 -4.0784426 -4.0906954 -4.1160955 -4.1362796 -4.1473818 -4.1655316 -4.1886382 -4.2196894 -4.2415609][-4.210309 -4.2169828 -4.2129512 -4.2043009 -4.1846509 -4.1445513 -4.1258473 -4.136961 -4.1555877 -4.1639714 -4.1671014 -4.1826272 -4.2082396 -4.2397537 -4.2604508][-4.213573 -4.2088447 -4.1990743 -4.1909103 -4.1749392 -4.1410189 -4.130352 -4.1449027 -4.1567855 -4.1511407 -4.1400547 -4.1500878 -4.1847663 -4.2193856 -4.2438645][-4.2044024 -4.1838212 -4.1664591 -4.1597939 -4.149045 -4.1255126 -4.1228814 -4.13935 -4.1473994 -4.1309485 -4.1074715 -4.1167173 -4.1579428 -4.1889529 -4.2130423][-4.1940432 -4.1706824 -4.15465 -4.1541891 -4.1522479 -4.1413145 -4.1461611 -4.162447 -4.1675658 -4.1476669 -4.1239386 -4.13968 -4.1816835 -4.2024012 -4.2181005][-4.1887245 -4.1679044 -4.1561604 -4.1583486 -4.162437 -4.1654248 -4.1782804 -4.1942186 -4.1978316 -4.1833043 -4.1676965 -4.18936 -4.22433 -4.2351642 -4.2447004]]...]
INFO - root - 2017-12-07 18:36:31.424608: step 40710, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 53h:34m:51s remains)
INFO - root - 2017-12-07 18:36:38.334408: step 40720, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 53h:05m:22s remains)
INFO - root - 2017-12-07 18:36:45.122825: step 40730, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 58h:36m:39s remains)
INFO - root - 2017-12-07 18:36:51.956373: step 40740, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 60h:07m:09s remains)
INFO - root - 2017-12-07 18:36:58.674249: step 40750, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 54h:31m:08s remains)
INFO - root - 2017-12-07 18:37:05.387571: step 40760, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.682 sec/batch; 55h:17m:42s remains)
INFO - root - 2017-12-07 18:37:12.129821: step 40770, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 53h:30m:54s remains)
INFO - root - 2017-12-07 18:37:18.930638: step 40780, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 56h:57m:30s remains)
INFO - root - 2017-12-07 18:37:25.716369: step 40790, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 56h:16m:15s remains)
INFO - root - 2017-12-07 18:37:32.325880: step 40800, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 54h:21m:11s remains)
2017-12-07 18:37:33.039840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3174372 -4.3050437 -4.290113 -4.2820754 -4.280015 -4.2813377 -4.284214 -4.287641 -4.2941208 -4.3018603 -4.3072186 -4.3036709 -4.2978978 -4.2961049 -4.2981882][-4.3160911 -4.2982368 -4.2820182 -4.2758017 -4.274303 -4.2772293 -4.2829518 -4.2890148 -4.2956042 -4.3050647 -4.3152251 -4.3205123 -4.3230667 -4.3255453 -4.3253608][-4.296895 -4.2687511 -4.2480793 -4.2400832 -4.2369003 -4.2413092 -4.2533855 -4.2689629 -4.2801104 -4.2921662 -4.3065248 -4.3196187 -4.3295565 -4.3381777 -4.3384967][-4.2745042 -4.2362256 -4.2071486 -4.193634 -4.1902161 -4.1978273 -4.2141285 -4.2372608 -4.2540255 -4.2706933 -4.2888179 -4.3046951 -4.3172665 -4.3297982 -4.3317251][-4.2505522 -4.2022424 -4.1612983 -4.1408472 -4.1397142 -4.1481757 -4.1588593 -4.1800137 -4.19989 -4.2231336 -4.2487955 -4.2702565 -4.2864656 -4.3034806 -4.3103466][-4.2267613 -4.1663761 -4.1128855 -4.0843234 -4.0809922 -4.0844779 -4.0829725 -4.0912685 -4.1045752 -4.1303105 -4.1705809 -4.2103615 -4.2411804 -4.2689171 -4.2834086][-4.223742 -4.1565266 -4.0919018 -4.0496168 -4.0352435 -4.0239844 -3.9993429 -3.9852073 -3.9915941 -4.0215325 -4.0782156 -4.1404705 -4.1921115 -4.2348685 -4.2620811][-4.2438812 -4.178309 -4.1146483 -4.07232 -4.0508633 -4.0203409 -3.9655378 -3.923918 -3.9203219 -3.9483113 -4.0137939 -4.0929365 -4.1579394 -4.2112384 -4.2490597][-4.2782831 -4.2235618 -4.1723957 -4.146256 -4.1318955 -4.0949903 -4.0217195 -3.9582131 -3.9355769 -3.9509556 -4.0091004 -4.0886145 -4.1586132 -4.2158756 -4.2557664][-4.3119631 -4.2756548 -4.2456546 -4.2354374 -4.2295675 -4.1973853 -4.1311722 -4.06696 -4.0349059 -4.0354815 -4.0705824 -4.1324658 -4.1936512 -4.2446671 -4.2782426][-4.3321643 -4.3127675 -4.3023376 -4.3042297 -4.3025985 -4.2782383 -4.2277055 -4.1800132 -4.155025 -4.1520438 -4.1693091 -4.2060428 -4.2467465 -4.283195 -4.3072724][-4.3419433 -4.3339248 -4.3336039 -4.3381014 -4.3353076 -4.3160295 -4.2840972 -4.2584367 -4.2471223 -4.2489791 -4.2601652 -4.2806277 -4.3023934 -4.322752 -4.335279][-4.3482804 -4.3458829 -4.3465271 -4.346467 -4.3403649 -4.3255434 -4.3042011 -4.2885561 -4.2853217 -4.295506 -4.311276 -4.3268781 -4.339777 -4.3487606 -4.3517776][-4.3546739 -4.3551946 -4.3556862 -4.3534942 -4.3455253 -4.3324313 -4.3163805 -4.3051844 -4.3043485 -4.3153148 -4.3303604 -4.3442121 -4.354682 -4.3581438 -4.3551831][-4.3590703 -4.3603859 -4.3602519 -4.3579597 -4.3511071 -4.3419209 -4.3325052 -4.3264651 -4.3252082 -4.3317261 -4.34203 -4.352716 -4.3598185 -4.3594642 -4.3528128]]...]
INFO - root - 2017-12-07 18:37:39.813149: step 40810, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 59h:48m:05s remains)
INFO - root - 2017-12-07 18:37:46.594709: step 40820, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 57h:49m:05s remains)
INFO - root - 2017-12-07 18:37:53.434760: step 40830, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 56h:20m:52s remains)
INFO - root - 2017-12-07 18:38:00.274744: step 40840, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 51h:15m:37s remains)
INFO - root - 2017-12-07 18:38:07.031534: step 40850, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 52h:11m:26s remains)
INFO - root - 2017-12-07 18:38:13.599119: step 40860, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 58h:12m:17s remains)
INFO - root - 2017-12-07 18:38:20.418066: step 40870, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 55h:06m:21s remains)
INFO - root - 2017-12-07 18:38:27.303843: step 40880, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 51h:27m:58s remains)
INFO - root - 2017-12-07 18:38:34.097489: step 40890, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 51h:41m:09s remains)
INFO - root - 2017-12-07 18:38:40.769502: step 40900, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 58h:25m:56s remains)
2017-12-07 18:38:41.487924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3254957 -4.3264594 -4.3305807 -4.3347335 -4.338335 -4.3368273 -4.324954 -4.3100424 -4.2955928 -4.2798839 -4.2669778 -4.2583151 -4.2543797 -4.2565632 -4.2667866][-4.3302031 -4.3319163 -4.3338318 -4.3368583 -4.3398805 -4.3384066 -4.3253937 -4.3095183 -4.2882257 -4.2656975 -4.2489648 -4.2418256 -4.2410583 -4.2467766 -4.2602434][-4.3332286 -4.3347206 -4.335464 -4.3365955 -4.3365264 -4.3328252 -4.3165369 -4.2967157 -4.2751169 -4.2528625 -4.2388482 -4.230526 -4.2310448 -4.2412353 -4.2560148][-4.3416386 -4.341682 -4.3410668 -4.3344417 -4.32697 -4.3163457 -4.2954593 -4.2754135 -4.2583652 -4.2407751 -4.2292166 -4.219038 -4.2235918 -4.2396731 -4.2573514][-4.3507576 -4.3460755 -4.337338 -4.3187456 -4.3009529 -4.2799735 -4.2593455 -4.2512641 -4.2411485 -4.2234735 -4.2132282 -4.2080808 -4.2188611 -4.2385478 -4.2602067][-4.3520579 -4.3397117 -4.3185635 -4.28835 -4.254765 -4.2133412 -4.1781254 -4.1825643 -4.1922579 -4.1913104 -4.1929226 -4.2061324 -4.2282429 -4.2496815 -4.2671967][-4.3347378 -4.3091879 -4.2728376 -4.2283835 -4.174983 -4.1034508 -4.0371113 -4.0581169 -4.1128263 -4.1508317 -4.1778207 -4.20683 -4.2305017 -4.2459693 -4.2564435][-4.301167 -4.2641468 -4.21648 -4.1615982 -4.0903816 -3.9791725 -3.864224 -3.9011984 -4.0177231 -4.10236 -4.1514482 -4.18919 -4.2093291 -4.2170677 -4.2217884][-4.2602334 -4.2154675 -4.1617765 -4.1081576 -4.0404897 -3.9224813 -3.7923837 -3.8376117 -3.9814384 -4.0822058 -4.1324663 -4.1649609 -4.1759257 -4.1761189 -4.1802754][-4.2429109 -4.2043734 -4.1592369 -4.1235552 -4.0840583 -4.002461 -3.9114504 -3.9391394 -4.0460644 -4.121129 -4.1548424 -4.1721168 -4.1708379 -4.1639256 -4.1672087][-4.2468081 -4.2215176 -4.1917291 -4.1724305 -4.1551418 -4.10618 -4.0489378 -4.057528 -4.1214576 -4.1653023 -4.1846142 -4.1935196 -4.1862206 -4.1779528 -4.17908][-4.2490745 -4.2357459 -4.2209668 -4.2124219 -4.2096925 -4.1876678 -4.1524854 -4.1496272 -4.1862736 -4.210227 -4.2200656 -4.2240191 -4.2166495 -4.2069945 -4.20247][-4.2459378 -4.2331953 -4.223948 -4.2212691 -4.2279019 -4.2254233 -4.2087278 -4.2044907 -4.2273521 -4.2419019 -4.2454019 -4.2469544 -4.2449365 -4.2361808 -4.228775][-4.257731 -4.2407432 -4.2312202 -4.2311811 -4.2436132 -4.2519097 -4.2514033 -4.2500372 -4.2620397 -4.26862 -4.265729 -4.2614837 -4.2609081 -4.2550669 -4.2499409][-4.2871065 -4.2703886 -4.2620625 -4.2645984 -4.2766304 -4.2879624 -4.29432 -4.2964883 -4.3000755 -4.2984319 -4.2926626 -4.2865348 -4.2863383 -4.2862387 -4.28654]]...]
INFO - root - 2017-12-07 18:38:48.219134: step 40910, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.614 sec/batch; 49h:45m:59s remains)
INFO - root - 2017-12-07 18:38:55.070308: step 40920, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 52h:34m:41s remains)
INFO - root - 2017-12-07 18:39:01.803635: step 40930, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 55h:33m:04s remains)
INFO - root - 2017-12-07 18:39:08.774270: step 40940, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 0.770 sec/batch; 62h:21m:55s remains)
INFO - root - 2017-12-07 18:39:15.585604: step 40950, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 51h:21m:22s remains)
INFO - root - 2017-12-07 18:39:22.350894: step 40960, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 51h:34m:11s remains)
INFO - root - 2017-12-07 18:39:29.220085: step 40970, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 54h:47m:03s remains)
INFO - root - 2017-12-07 18:39:36.070323: step 40980, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 59h:49m:32s remains)
INFO - root - 2017-12-07 18:39:42.952868: step 40990, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 54h:33m:32s remains)
INFO - root - 2017-12-07 18:39:49.505860: step 41000, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 54h:08m:13s remains)
2017-12-07 18:39:50.206972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2824106 -4.2867761 -4.2768464 -4.2662697 -4.2621636 -4.2674179 -4.2724895 -4.2609344 -4.2463741 -4.2495537 -4.2619853 -4.2550387 -4.256134 -4.2695856 -4.2878213][-4.2885914 -4.2939463 -4.2854509 -4.2726464 -4.266479 -4.2685709 -4.2695894 -4.2597852 -4.2459383 -4.2504292 -4.2675638 -4.2626653 -4.2591949 -4.2709002 -4.2882042][-4.2912416 -4.2966695 -4.2878208 -4.2747087 -4.2670407 -4.2649903 -4.264627 -4.2552004 -4.24417 -4.2579336 -4.2820187 -4.2774277 -4.2691612 -4.2783108 -4.2935734][-4.2793 -4.2824969 -4.2767477 -4.2681212 -4.25437 -4.2462354 -4.243433 -4.2349463 -4.2279196 -4.2545915 -4.2913876 -4.2910857 -4.2787814 -4.28163 -4.2939386][-4.2495871 -4.2429366 -4.2422853 -4.2425613 -4.2223415 -4.206656 -4.1986523 -4.193924 -4.1974454 -4.2361608 -4.2890863 -4.2957706 -4.2830062 -4.2821155 -4.2925911][-4.2095971 -4.1895318 -4.1894283 -4.1970887 -4.1777611 -4.1585507 -4.1412253 -4.1260824 -4.1393709 -4.1990471 -4.2702665 -4.28703 -4.2752867 -4.2716379 -4.2841911][-4.1723828 -4.1435509 -4.1406341 -4.1488495 -4.143003 -4.1258368 -4.0922432 -4.0469584 -4.0453563 -4.1249313 -4.2189984 -4.2537689 -4.252224 -4.2512474 -4.2681146][-4.1331 -4.1130452 -4.1098375 -4.113802 -4.1129785 -4.1054397 -4.0654583 -3.9847677 -3.9485092 -4.0273442 -4.1431804 -4.207552 -4.2307491 -4.2391357 -4.2594352][-4.1057544 -4.09122 -4.0911937 -4.0905409 -4.0888066 -4.0944476 -4.0616336 -3.9707499 -3.9046462 -3.9619815 -4.0786486 -4.16422 -4.2134652 -4.23554 -4.2568235][-4.0831761 -4.07476 -4.0797877 -4.0797195 -4.0848908 -4.1006103 -4.0777369 -3.9946148 -3.9167747 -3.94337 -4.0364375 -4.1229897 -4.1898165 -4.2251 -4.2479343][-4.0601358 -4.0581403 -4.0715547 -4.0824108 -4.1011977 -4.1178579 -4.0976138 -4.0262709 -3.9510047 -3.9535079 -4.0139956 -4.0840693 -4.15423 -4.2027912 -4.2313757][-4.0412822 -4.0442352 -4.0589375 -4.0756497 -4.1030359 -4.1181626 -4.09903 -4.0428257 -3.9811661 -3.973526 -4.011354 -4.0632405 -4.128931 -4.1839662 -4.215816][-4.0337014 -4.0384035 -4.0534859 -4.0690808 -4.0907836 -4.1024961 -4.0892835 -4.0499644 -4.0035458 -3.9980912 -4.0276103 -4.0687485 -4.1293774 -4.1822443 -4.2136106][-4.0355668 -4.0356007 -4.0512133 -4.0607853 -4.0672464 -4.0741539 -4.067656 -4.0392342 -4.00572 -4.0102038 -4.0363755 -4.0710578 -4.131897 -4.184865 -4.2184958][-4.0509534 -4.0433235 -4.0549879 -4.0609632 -4.0578742 -4.0586514 -4.0569739 -4.0363951 -4.0134068 -4.0259585 -4.0427332 -4.0648842 -4.1212711 -4.1761045 -4.2160931]]...]
INFO - root - 2017-12-07 18:39:57.068534: step 41010, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 58h:24m:55s remains)
INFO - root - 2017-12-07 18:40:03.818658: step 41020, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 54h:05m:07s remains)
INFO - root - 2017-12-07 18:40:10.688294: step 41030, loss = 2.09, batch loss = 2.03 (13.5 examples/sec; 0.592 sec/batch; 47h:58m:06s remains)
INFO - root - 2017-12-07 18:40:17.526006: step 41040, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 52h:32m:23s remains)
INFO - root - 2017-12-07 18:40:24.291966: step 41050, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 57h:14m:05s remains)
INFO - root - 2017-12-07 18:40:31.089707: step 41060, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 58h:23m:15s remains)
INFO - root - 2017-12-07 18:40:37.989309: step 41070, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 55h:51m:35s remains)
INFO - root - 2017-12-07 18:40:44.836784: step 41080, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 52h:10m:04s remains)
INFO - root - 2017-12-07 18:40:51.732155: step 41090, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 55h:00m:47s remains)
INFO - root - 2017-12-07 18:40:58.443731: step 41100, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 59h:56m:32s remains)
2017-12-07 18:40:59.208295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2615547 -4.2294626 -4.2205315 -4.2388434 -4.2684636 -4.2892652 -4.2909031 -4.2754478 -4.2549205 -4.2436562 -4.2443786 -4.2515821 -4.2632179 -4.2787747 -4.2939787][-4.2371311 -4.2029257 -4.1994133 -4.2236009 -4.252573 -4.2676272 -4.2629666 -4.2463989 -4.2280183 -4.221189 -4.2225666 -4.227344 -4.2356234 -4.2504377 -4.2695847][-4.2219615 -4.1892633 -4.1896 -4.2145405 -4.2389107 -4.2442088 -4.2309475 -4.2125688 -4.1973963 -4.1948137 -4.1981688 -4.202002 -4.2105742 -4.2262268 -4.2506371][-4.2341352 -4.2096062 -4.210392 -4.2239709 -4.2351403 -4.223618 -4.1921468 -4.1621885 -4.146462 -4.153172 -4.1706891 -4.1874695 -4.2061911 -4.2269912 -4.2560334][-4.267468 -4.2542715 -4.2511659 -4.2444363 -4.2300396 -4.1863494 -4.1245379 -4.0804844 -4.07066 -4.0985727 -4.1391697 -4.1811118 -4.2173018 -4.2462063 -4.2789655][-4.3058119 -4.3027821 -4.2951145 -4.2648635 -4.2129622 -4.1212158 -4.0178819 -3.964381 -3.9820998 -4.0461626 -4.116786 -4.1841211 -4.236207 -4.2729845 -4.3055005][-4.3255248 -4.3298221 -4.3189311 -4.2724586 -4.1816864 -4.0399103 -3.8915219 -3.8321974 -3.8990781 -4.0148945 -4.1154113 -4.1975083 -4.2592473 -4.2995567 -4.3245316][-4.3204117 -4.3307781 -4.3185253 -4.2618737 -4.1490669 -3.9822159 -3.8144727 -3.768733 -3.8879032 -4.0375433 -4.1475844 -4.2251744 -4.2808065 -4.3136497 -4.32681][-4.3049164 -4.3221831 -4.3086953 -4.2503881 -4.14349 -4.0003147 -3.8767962 -3.8744593 -3.9945056 -4.121757 -4.2085285 -4.2630329 -4.2989421 -4.3149052 -4.3132195][-4.2822237 -4.3079119 -4.3010192 -4.2542114 -4.1734138 -4.0811853 -4.0271225 -4.0515723 -4.137609 -4.220088 -4.2685857 -4.2954035 -4.3092537 -4.3056016 -4.2925615][-4.2609882 -4.2980576 -4.3056855 -4.2785459 -4.2282634 -4.1816764 -4.1716547 -4.1990857 -4.249248 -4.2926164 -4.3132272 -4.3173218 -4.313447 -4.2976708 -4.2803993][-4.2573214 -4.3032303 -4.3190866 -4.3085318 -4.2800217 -4.2615824 -4.2681 -4.289701 -4.3149948 -4.3285227 -4.3288817 -4.3215866 -4.3131051 -4.2958832 -4.279809][-4.2795897 -4.3178763 -4.32731 -4.319407 -4.3019395 -4.2972631 -4.309226 -4.3262229 -4.3351398 -4.3306661 -4.3205194 -4.3128734 -4.3082714 -4.2942023 -4.280179][-4.3130956 -4.3314896 -4.3266745 -4.3132691 -4.2997112 -4.3027968 -4.3151174 -4.3239346 -4.3218331 -4.3105311 -4.3016934 -4.2996974 -4.2991195 -4.2892094 -4.2769809][-4.3396487 -4.3398261 -4.3246207 -4.3075504 -4.296659 -4.30246 -4.3089504 -4.3050928 -4.2914958 -4.2789316 -4.2761545 -4.2804689 -4.2844663 -4.2786713 -4.2695947]]...]
INFO - root - 2017-12-07 18:41:05.966232: step 41110, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 52h:07m:14s remains)
INFO - root - 2017-12-07 18:41:12.747895: step 41120, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 58h:22m:02s remains)
INFO - root - 2017-12-07 18:41:19.573626: step 41130, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 58h:33m:36s remains)
INFO - root - 2017-12-07 18:41:26.449411: step 41140, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 51h:23m:49s remains)
INFO - root - 2017-12-07 18:41:33.231957: step 41150, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 53h:33m:41s remains)
INFO - root - 2017-12-07 18:41:39.913535: step 41160, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 51h:33m:13s remains)
INFO - root - 2017-12-07 18:41:46.569259: step 41170, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 59h:04m:05s remains)
INFO - root - 2017-12-07 18:41:53.361642: step 41180, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 59h:32m:10s remains)
INFO - root - 2017-12-07 18:42:00.205849: step 41190, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 54h:40m:35s remains)
INFO - root - 2017-12-07 18:42:06.765179: step 41200, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 53h:13m:38s remains)
2017-12-07 18:42:07.525409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3323984 -4.3274264 -4.3224282 -4.3194923 -4.3179913 -4.3187108 -4.31847 -4.3192611 -4.3199692 -4.3173304 -4.3131962 -4.3097863 -4.3103018 -4.3125963 -4.3176413][-4.3182478 -4.3092389 -4.3002305 -4.2940331 -4.2901096 -4.2895412 -4.2889957 -4.2923436 -4.2960162 -4.292284 -4.284781 -4.2788005 -4.2780418 -4.2784247 -4.2855306][-4.2956452 -4.2806282 -4.2680855 -4.2589564 -4.2509742 -4.2471733 -4.2434907 -4.2478886 -4.2530742 -4.2486086 -4.2385058 -4.230402 -4.2292771 -4.2282629 -4.2372804][-4.2731686 -4.2492375 -4.227284 -4.2093916 -4.1910558 -4.1788292 -4.1702805 -4.1766744 -4.1861153 -4.1858363 -4.1798458 -4.1763959 -4.1788645 -4.1773896 -4.1861563][-4.2578492 -4.2246728 -4.188787 -4.1552677 -4.1184587 -4.0864973 -4.066957 -4.0724845 -4.085506 -4.0942912 -4.1030622 -4.1142755 -4.1287317 -4.1311455 -4.1430907][-4.2563214 -4.215477 -4.1676116 -4.1172476 -4.0576959 -4.001462 -3.9630444 -3.9539707 -3.9562232 -3.9730299 -4.004168 -4.03547 -4.0669789 -4.0816956 -4.1069741][-4.2643895 -4.2203612 -4.1662307 -4.1074739 -4.0320039 -3.9617765 -3.907084 -3.8692462 -3.8376102 -3.8530362 -3.9081841 -3.9530163 -3.9990089 -4.0337868 -4.0781317][-4.2772646 -4.2380695 -4.188334 -4.1331143 -4.0590153 -3.98857 -3.9222236 -3.8509681 -3.7740891 -3.7702968 -3.8290682 -3.8722255 -3.9220943 -3.9738038 -4.0383306][-4.28798 -4.2583747 -4.2213173 -4.1799631 -4.124814 -4.06821 -4.0034342 -3.9259624 -3.8376269 -3.807178 -3.8322494 -3.8496318 -3.8851476 -3.9373081 -4.0068235][-4.2889147 -4.2696347 -4.24883 -4.2246494 -4.1916442 -4.1551385 -4.1066589 -4.0461397 -3.9791644 -3.9392357 -3.92427 -3.9107475 -3.9261806 -3.9604249 -4.0161271][-4.2951288 -4.2845159 -4.2748871 -4.263062 -4.2460828 -4.2273111 -4.1987219 -4.1612191 -4.1189337 -4.0844011 -4.0515289 -4.0198464 -4.0180097 -4.0326633 -4.0687551][-4.3157735 -4.31255 -4.3109365 -4.306138 -4.2984948 -4.2914147 -4.2786374 -4.2614174 -4.2394319 -4.2153206 -4.1838126 -4.149364 -4.1372404 -4.1394811 -4.1581068][-4.3361864 -4.3353682 -4.3354969 -4.3339539 -4.3324709 -4.3330631 -4.331655 -4.3269482 -4.3185616 -4.3047805 -4.2813296 -4.2547469 -4.24033 -4.236743 -4.2452011][-4.346736 -4.3458157 -4.3434672 -4.3411903 -4.3411503 -4.3433223 -4.3469338 -4.3483667 -4.3485179 -4.3441286 -4.3302732 -4.3149881 -4.3054886 -4.3000541 -4.3021345][-4.3513265 -4.3498006 -4.3460703 -4.3434353 -4.3430414 -4.3444328 -4.3468127 -4.3487911 -4.3513141 -4.3514633 -4.3457947 -4.3399167 -4.3356204 -4.3302712 -4.32783]]...]
INFO - root - 2017-12-07 18:42:14.365276: step 41210, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 56h:21m:15s remains)
INFO - root - 2017-12-07 18:42:21.152941: step 41220, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 50h:28m:10s remains)
INFO - root - 2017-12-07 18:42:27.945668: step 41230, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 53h:05m:18s remains)
INFO - root - 2017-12-07 18:42:34.710263: step 41240, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 52h:51m:14s remains)
INFO - root - 2017-12-07 18:42:41.484876: step 41250, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 54h:45m:55s remains)
INFO - root - 2017-12-07 18:42:48.385595: step 41260, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 58h:15m:59s remains)
INFO - root - 2017-12-07 18:42:55.213195: step 41270, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 56h:47m:20s remains)
INFO - root - 2017-12-07 18:43:01.943783: step 41280, loss = 2.05, batch loss = 1.99 (13.1 examples/sec; 0.610 sec/batch; 49h:19m:15s remains)
INFO - root - 2017-12-07 18:43:08.863944: step 41290, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 55h:46m:44s remains)
INFO - root - 2017-12-07 18:43:15.581938: step 41300, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 57h:15m:28s remains)
2017-12-07 18:43:16.375789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2163777 -4.2290196 -4.2395835 -4.2397671 -4.2466993 -4.2525187 -4.2450819 -4.2318668 -4.2059183 -4.185874 -4.1698375 -4.1486087 -4.1101069 -4.0818529 -4.1010261][-4.2240763 -4.2380166 -4.2394295 -4.2354145 -4.23985 -4.2444077 -4.2402883 -4.2370191 -4.2257972 -4.2103138 -4.1935825 -4.1720181 -4.1404467 -4.1266127 -4.1479683][-4.2246642 -4.2397962 -4.232151 -4.2190704 -4.2171326 -4.2147865 -4.2111106 -4.2201462 -4.2284832 -4.224566 -4.2104597 -4.1916633 -4.1741786 -4.1786871 -4.2024155][-4.2127452 -4.2338238 -4.2312155 -4.2163272 -4.2050433 -4.192771 -4.1846104 -4.1992364 -4.218246 -4.221652 -4.207458 -4.1901569 -4.1782465 -4.1883912 -4.2142854][-4.1901145 -4.2141614 -4.2200971 -4.2098565 -4.1908479 -4.167161 -4.1498723 -4.1633525 -4.1904478 -4.2010279 -4.1890965 -4.1693406 -4.1513557 -4.1529632 -4.1778755][-4.1748567 -4.1928587 -4.1941724 -4.1828628 -4.1568828 -4.1234188 -4.0962372 -4.1081495 -4.148952 -4.17372 -4.1659141 -4.1415043 -4.108325 -4.09398 -4.1228528][-4.1506238 -4.1483426 -4.1315284 -4.1147265 -4.0919647 -4.0652919 -4.0403118 -4.0517931 -4.1030917 -4.1421652 -4.1382818 -4.1040158 -4.0514641 -4.026475 -4.0691714][-4.1334448 -4.0994368 -4.0511732 -4.0225286 -4.0107522 -4.0057578 -3.9961021 -4.0065455 -4.0596323 -4.1051164 -4.1021442 -4.0637627 -4.0044231 -3.97835 -4.0318451][-4.1361957 -4.0788445 -4.0093608 -3.9735751 -3.9749775 -3.9925048 -4.0032964 -4.0166183 -4.0586166 -4.0943165 -4.0854249 -4.049222 -3.9991598 -3.9823117 -4.0370789][-4.16564 -4.1046996 -4.0376997 -4.0073342 -4.0208135 -4.0521412 -4.0750523 -4.0902376 -4.1130104 -4.1249375 -4.1102715 -4.0816112 -4.0459242 -4.040513 -4.08575][-4.2045174 -4.1611838 -4.1132121 -4.0963144 -4.1144609 -4.1459837 -4.1680803 -4.1784606 -4.18132 -4.17634 -4.1621943 -4.1419921 -4.1224222 -4.1240039 -4.1546426][-4.2309036 -4.2104058 -4.1838078 -4.1783481 -4.1996388 -4.2264957 -4.2427683 -4.2456017 -4.2341685 -4.2225366 -4.2121477 -4.1998014 -4.1896949 -4.1915727 -4.2090712][-4.2366872 -4.2380028 -4.2290998 -4.2281542 -4.2457795 -4.2658825 -4.2757583 -4.2735677 -4.258791 -4.2470922 -4.2400742 -4.2328391 -4.2260227 -4.2234912 -4.231319][-4.2155566 -4.2345428 -4.239924 -4.2419009 -4.2556868 -4.2695947 -4.2720609 -4.2674723 -4.256536 -4.2480817 -4.244195 -4.2393465 -4.2317767 -4.2244134 -4.2274704][-4.1619811 -4.1913109 -4.2083573 -4.21886 -4.2342634 -4.2475839 -4.2499557 -4.2482738 -4.2436929 -4.2390208 -4.2329836 -4.2251534 -4.2150393 -4.2086024 -4.2152863]]...]
INFO - root - 2017-12-07 18:43:23.033427: step 41310, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.636 sec/batch; 51h:27m:35s remains)
INFO - root - 2017-12-07 18:43:29.921764: step 41320, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 53h:46m:14s remains)
INFO - root - 2017-12-07 18:43:36.870107: step 41330, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.766 sec/batch; 61h:54m:58s remains)
INFO - root - 2017-12-07 18:43:43.593604: step 41340, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 55h:28m:32s remains)
INFO - root - 2017-12-07 18:43:50.376417: step 41350, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 51h:52m:48s remains)
INFO - root - 2017-12-07 18:43:57.138369: step 41360, loss = 2.05, batch loss = 1.99 (13.3 examples/sec; 0.602 sec/batch; 48h:41m:44s remains)
INFO - root - 2017-12-07 18:44:03.982618: step 41370, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 56h:39m:33s remains)
INFO - root - 2017-12-07 18:44:10.791439: step 41380, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 58h:17m:50s remains)
INFO - root - 2017-12-07 18:44:17.588347: step 41390, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 53h:48m:13s remains)
INFO - root - 2017-12-07 18:44:24.176056: step 41400, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 55h:39m:07s remains)
2017-12-07 18:44:24.905581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3196497 -4.3088751 -4.2990179 -4.2943916 -4.294888 -4.2962503 -4.2964277 -4.2986994 -4.3025532 -4.3051772 -4.3065348 -4.3071408 -4.3070455 -4.3089333 -4.3130145][-4.3126116 -4.3001919 -4.2885256 -4.2850657 -4.2897968 -4.2936916 -4.2935286 -4.2972965 -4.3040018 -4.30754 -4.3072939 -4.3048944 -4.2997417 -4.2976708 -4.2989726][-4.3051686 -4.2901525 -4.2754264 -4.2713752 -4.2784286 -4.2826571 -4.2820454 -4.2893577 -4.3022075 -4.3085027 -4.3087988 -4.3037114 -4.29185 -4.284708 -4.2834291][-4.2886395 -4.269783 -4.2508092 -4.2444644 -4.2512784 -4.2509451 -4.2462516 -4.254909 -4.2757363 -4.2876892 -4.2906532 -4.2863464 -4.2703404 -4.2598982 -4.2572703][-4.2635374 -4.23647 -4.2096672 -4.1928535 -4.1930542 -4.1843166 -4.1704783 -4.1777716 -4.2063227 -4.2234559 -4.229743 -4.2318716 -4.2228365 -4.2179813 -4.2195158][-4.239306 -4.1991253 -4.1568456 -4.1168475 -4.09754 -4.0718722 -4.0418329 -4.0473304 -4.09103 -4.11785 -4.1276855 -4.138988 -4.1445389 -4.1513023 -4.1661372][-4.2133789 -4.16179 -4.1060119 -4.0435629 -3.999963 -3.9487824 -3.8900588 -3.8941114 -3.9662957 -4.012197 -4.0283365 -4.0473785 -4.0625305 -4.0777617 -4.1071177][-4.1919165 -4.1365266 -4.0808392 -4.0156384 -3.9625437 -3.8901219 -3.8028607 -3.8076849 -3.9043121 -3.9670267 -3.9875748 -4.0083337 -4.0210772 -4.0332718 -4.0694923][-4.1943197 -4.1444569 -4.098424 -4.0515089 -4.0153732 -3.9592035 -3.8881702 -3.8963244 -3.9726982 -4.0184059 -4.0292068 -4.0447693 -4.0512428 -4.0512471 -4.0774331][-4.20578 -4.1622705 -4.12406 -4.0960217 -4.0820284 -4.0516596 -4.0090175 -4.0171452 -4.0645981 -4.087595 -4.0890222 -4.1027684 -4.1047859 -4.0931268 -4.1031275][-4.2196341 -4.1828918 -4.1478338 -4.1275105 -4.1238656 -4.1092257 -4.0835304 -4.0889378 -4.1181493 -4.1302319 -4.1290793 -4.1381841 -4.1360149 -4.1200991 -4.1247087][-4.23452 -4.2069945 -4.1811352 -4.167511 -4.1660228 -4.1567349 -4.1411686 -4.1437216 -4.1599689 -4.1643586 -4.1620235 -4.1658473 -4.16252 -4.1515527 -4.1595926][-4.2536368 -4.2340541 -4.2185135 -4.2105927 -4.2073846 -4.2001195 -4.191617 -4.1933112 -4.2016811 -4.2010164 -4.1974468 -4.1982074 -4.1971011 -4.1952114 -4.2062545][-4.2748618 -4.2618337 -4.2539172 -4.2505832 -4.2477145 -4.2421188 -4.2365618 -4.23923 -4.2427006 -4.2377672 -4.2345228 -4.2361298 -4.2394857 -4.241766 -4.2524467][-4.294632 -4.285789 -4.281764 -4.2805185 -4.2793336 -4.2755632 -4.2718782 -4.273984 -4.274838 -4.27066 -4.2696152 -4.2716846 -4.2760944 -4.2796369 -4.2882314]]...]
INFO - root - 2017-12-07 18:44:31.882973: step 41410, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.743 sec/batch; 60h:05m:40s remains)
INFO - root - 2017-12-07 18:44:38.742110: step 41420, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 55h:59m:25s remains)
INFO - root - 2017-12-07 18:44:45.505747: step 41430, loss = 2.07, batch loss = 2.01 (13.3 examples/sec; 0.604 sec/batch; 48h:48m:40s remains)
INFO - root - 2017-12-07 18:44:52.358734: step 41440, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 54h:31m:21s remains)
INFO - root - 2017-12-07 18:44:59.140450: step 41450, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 59h:00m:25s remains)
INFO - root - 2017-12-07 18:45:05.915743: step 41460, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 53h:42m:24s remains)
INFO - root - 2017-12-07 18:45:12.803085: step 41470, loss = 2.09, batch loss = 2.04 (13.0 examples/sec; 0.617 sec/batch; 49h:54m:23s remains)
INFO - root - 2017-12-07 18:45:19.549463: step 41480, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 55h:59m:49s remains)
INFO - root - 2017-12-07 18:45:26.513044: step 41490, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 58h:41m:14s remains)
INFO - root - 2017-12-07 18:45:33.133286: step 41500, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.687 sec/batch; 55h:33m:51s remains)
2017-12-07 18:45:33.829600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2167435 -4.2146277 -4.217442 -4.2185283 -4.2190175 -4.2194142 -4.2226562 -4.2220678 -4.2205052 -4.2153525 -4.2141566 -4.2202406 -4.2290874 -4.2387991 -4.2418661][-4.2282362 -4.226522 -4.2285247 -4.2342944 -4.238585 -4.2416692 -4.2393308 -4.2276831 -4.2149539 -4.2019768 -4.1996264 -4.2116547 -4.2249594 -4.2341895 -4.2316852][-4.2398057 -4.2359161 -4.2349076 -4.2364593 -4.2393465 -4.2426414 -4.2377262 -4.2220206 -4.2048006 -4.1858759 -4.1807814 -4.1943684 -4.212532 -4.2257032 -4.2219677][-4.2302155 -4.2272396 -4.22067 -4.2145138 -4.2121992 -4.2104244 -4.2037864 -4.1919146 -4.1838536 -4.165657 -4.1576219 -4.1726089 -4.1975102 -4.2129178 -4.2137861][-4.2116566 -4.2050886 -4.1912084 -4.1799226 -4.1724873 -4.1601171 -4.1453547 -4.1416283 -4.1516962 -4.1475315 -4.1440649 -4.1628418 -4.1910453 -4.2082357 -4.2160864][-4.2016053 -4.1907287 -4.1730251 -4.1554751 -4.139287 -4.1103344 -4.0753493 -4.0765409 -4.1133342 -4.1391659 -4.1548767 -4.1732383 -4.1956358 -4.2132711 -4.2304878][-4.207829 -4.1947351 -4.1733751 -4.145916 -4.1110215 -4.0520425 -3.9792385 -3.9817917 -4.0606337 -4.1305437 -4.170105 -4.1908813 -4.2074103 -4.2213488 -4.2368369][-4.2290268 -4.2084608 -4.1783118 -4.1370759 -4.0810165 -3.989192 -3.8789129 -3.8878808 -4.0190611 -4.1287336 -4.1857929 -4.211906 -4.2237711 -4.228899 -4.23648][-4.2453561 -4.2184143 -4.1806765 -4.1332231 -4.0753217 -3.9933352 -3.9078836 -3.9265802 -4.0439396 -4.1442342 -4.1999035 -4.2267723 -4.2347813 -4.2339177 -4.2378454][-4.2369184 -4.2100558 -4.17937 -4.1416793 -4.0997148 -4.0516548 -4.0165615 -4.0421519 -4.1143088 -4.1771069 -4.2188334 -4.2365966 -4.237618 -4.2338891 -4.2345114][-4.2251472 -4.2052035 -4.181469 -4.1543422 -4.1296978 -4.1111007 -4.1117673 -4.1383991 -4.1783195 -4.2118983 -4.2343144 -4.239922 -4.2336659 -4.2267365 -4.2266693][-4.2301931 -4.2165322 -4.1981573 -4.1826181 -4.1720581 -4.1734958 -4.1887159 -4.2092981 -4.2267404 -4.2372308 -4.239841 -4.2315016 -4.2227397 -4.2192683 -4.2235637][-4.2312126 -4.2245073 -4.2198896 -4.2185912 -4.2180047 -4.2260156 -4.2411146 -4.2541208 -4.2550516 -4.2465563 -4.2327251 -4.2168322 -4.2109275 -4.2161126 -4.22562][-4.225596 -4.228404 -4.2344651 -4.2386127 -4.2402649 -4.2475896 -4.2593942 -4.2670474 -4.2618384 -4.2464972 -4.2258506 -4.2079644 -4.2062035 -4.2167048 -4.22699][-4.2310324 -4.2394619 -4.2461939 -4.2470927 -4.2436056 -4.2460446 -4.2533751 -4.2594604 -4.2568045 -4.2479115 -4.2312384 -4.2145662 -4.2135906 -4.2208738 -4.2270408]]...]
INFO - root - 2017-12-07 18:45:40.649099: step 41510, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 57h:57m:35s remains)
INFO - root - 2017-12-07 18:45:47.497378: step 41520, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.748 sec/batch; 60h:26m:45s remains)
INFO - root - 2017-12-07 18:45:54.300444: step 41530, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 53h:00m:24s remains)
INFO - root - 2017-12-07 18:46:00.987164: step 41540, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 52h:37m:40s remains)
INFO - root - 2017-12-07 18:46:07.757105: step 41550, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 54h:02m:54s remains)
INFO - root - 2017-12-07 18:46:14.543466: step 41560, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 58h:02m:25s remains)
INFO - root - 2017-12-07 18:46:21.462694: step 41570, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 57h:31m:00s remains)
INFO - root - 2017-12-07 18:46:28.315494: step 41580, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 52h:45m:13s remains)
INFO - root - 2017-12-07 18:46:35.173558: step 41590, loss = 2.04, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 50h:30m:17s remains)
INFO - root - 2017-12-07 18:46:41.877286: step 41600, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 58h:03m:15s remains)
2017-12-07 18:46:42.601316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3092518 -4.3157349 -4.3154984 -4.3110876 -4.3098388 -4.3097768 -4.3084245 -4.3068085 -4.3072925 -4.3082013 -4.3090296 -4.3117328 -4.3159771 -4.3207474 -4.3251662][-4.3069892 -4.3158112 -4.3136978 -4.3037596 -4.29642 -4.2912879 -4.2848597 -4.2824297 -4.285481 -4.2894669 -4.2918191 -4.2961426 -4.30399 -4.3127656 -4.321928][-4.2992005 -4.3091283 -4.30219 -4.2848349 -4.2690015 -4.2557039 -4.2434583 -4.2431479 -4.252008 -4.264946 -4.2745032 -4.2839479 -4.294136 -4.3049612 -4.3162208][-4.284009 -4.2919483 -4.279922 -4.2548451 -4.2299562 -4.2078772 -4.1921229 -4.1933928 -4.2072949 -4.2285914 -4.250658 -4.2690325 -4.2822814 -4.2934675 -4.3032117][-4.2700739 -4.2700129 -4.2528076 -4.2230062 -4.1910796 -4.1617174 -4.1411643 -4.1398358 -4.1549811 -4.1830587 -4.2145767 -4.2396646 -4.2553773 -4.2683468 -4.2778893][-4.2590442 -4.2517638 -4.2320704 -4.2002048 -4.1623678 -4.1210809 -4.0852089 -4.0667834 -4.0817976 -4.1245494 -4.1703267 -4.2028074 -4.2222724 -4.2383804 -4.2502027][-4.2596836 -4.2503948 -4.23231 -4.1991162 -4.1528115 -4.0906429 -4.0219812 -3.9688861 -3.9864154 -4.0605922 -4.1288977 -4.1721425 -4.1981091 -4.2200375 -4.2351065][-4.2553353 -4.2490888 -4.2371092 -4.2048321 -4.1526217 -4.0685635 -3.9590755 -3.8550539 -3.8694823 -3.9859805 -4.0843334 -4.142056 -4.1779733 -4.2063942 -4.2294183][-4.2405124 -4.23506 -4.2337713 -4.2157483 -4.1731176 -4.0910921 -3.9775529 -3.8546066 -3.8510957 -3.9670956 -4.0631771 -4.1207638 -4.1578321 -4.1886506 -4.2175736][-4.2096462 -4.2089219 -4.2214894 -4.2245436 -4.2060671 -4.15492 -4.0849829 -4.0081453 -3.9890139 -4.0372963 -4.0837631 -4.12039 -4.1498919 -4.1756978 -4.2051711][-4.1768188 -4.1878104 -4.2161751 -4.2367349 -4.234076 -4.2052903 -4.1762571 -4.1463752 -4.1234217 -4.12183 -4.1310158 -4.1479969 -4.1631336 -4.1772676 -4.2032423][-4.1620908 -4.17534 -4.2118559 -4.2416291 -4.2473845 -4.2312522 -4.2225795 -4.2227187 -4.2074337 -4.1871991 -4.1808014 -4.185183 -4.1907072 -4.1935081 -4.2121067][-4.1786141 -4.1809 -4.2077594 -4.2374477 -4.2453089 -4.2386122 -4.2418661 -4.2557354 -4.2510076 -4.2330413 -4.2200704 -4.2157135 -4.2150545 -4.2162886 -4.2314239][-4.2321072 -4.2236047 -4.2276726 -4.2414083 -4.2449789 -4.2429266 -4.2526913 -4.2641978 -4.2597203 -4.2516079 -4.2425361 -4.238203 -4.2409759 -4.2482352 -4.2630906][-4.2875533 -4.2742872 -4.2596807 -4.254683 -4.2520022 -4.2531958 -4.2616262 -4.2687383 -4.2645235 -4.2624731 -4.2611904 -4.2651935 -4.2771072 -4.2904396 -4.3043342]]...]
INFO - root - 2017-12-07 18:46:49.334410: step 41610, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 49h:38m:21s remains)
INFO - root - 2017-12-07 18:46:56.203625: step 41620, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 53h:19m:58s remains)
INFO - root - 2017-12-07 18:47:03.023914: step 41630, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 58h:10m:19s remains)
INFO - root - 2017-12-07 18:47:09.858960: step 41640, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 57h:07m:47s remains)
INFO - root - 2017-12-07 18:47:16.605078: step 41650, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 57h:38m:40s remains)
INFO - root - 2017-12-07 18:47:23.389515: step 41660, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 52h:26m:35s remains)
INFO - root - 2017-12-07 18:47:30.238774: step 41670, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 54h:19m:43s remains)
INFO - root - 2017-12-07 18:47:37.122245: step 41680, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.722 sec/batch; 58h:19m:01s remains)
INFO - root - 2017-12-07 18:47:43.869715: step 41690, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 57h:42m:48s remains)
INFO - root - 2017-12-07 18:47:50.649997: step 41700, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 56h:35m:19s remains)
2017-12-07 18:47:51.336020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1505332 -4.130887 -4.111845 -4.1035352 -4.1151943 -4.1508026 -4.2020121 -4.2235441 -4.217433 -4.2090578 -4.2147932 -4.2214041 -4.2149005 -4.1931009 -4.1701694][-4.1506219 -4.1415272 -4.1336465 -4.1322222 -4.1402884 -4.1644907 -4.201612 -4.2176037 -4.211288 -4.2083287 -4.217464 -4.2278008 -4.2301612 -4.2196183 -4.2016296][-4.1522136 -4.1534324 -4.1596456 -4.164134 -4.1673551 -4.1811504 -4.2044044 -4.2130079 -4.2045236 -4.2039995 -4.2161341 -4.22569 -4.2356505 -4.2385139 -4.2301111][-4.1390281 -4.147912 -4.1565685 -4.1606455 -4.1633844 -4.1740022 -4.1883097 -4.190928 -4.1838861 -4.1888132 -4.2058969 -4.2220635 -4.2391381 -4.2492065 -4.2454743][-4.1249108 -4.1432447 -4.1477628 -4.1489415 -4.1512 -4.1613069 -4.1681204 -4.1604123 -4.154386 -4.1672258 -4.1930938 -4.2181287 -4.2403135 -4.2513771 -4.248508][-4.1242476 -4.149796 -4.1475286 -4.1401196 -4.1411996 -4.149807 -4.1456728 -4.1284938 -4.1238866 -4.1493239 -4.1851926 -4.2171478 -4.2413673 -4.2526851 -4.2486854][-4.1425166 -4.1677904 -4.1593308 -4.1446838 -4.1401477 -4.136229 -4.11998 -4.1019344 -4.1072984 -4.14557 -4.1879582 -4.2174091 -4.2385845 -4.2494817 -4.2428889][-4.1763372 -4.1943555 -4.180685 -4.1617355 -4.1494803 -4.1330624 -4.1089373 -4.0987225 -4.116189 -4.1606407 -4.2026381 -4.2259474 -4.2427597 -4.2505612 -4.2389946][-4.2169476 -4.2206569 -4.1974535 -4.1773195 -4.1657948 -4.1481218 -4.1232948 -4.1205969 -4.143187 -4.1865788 -4.2248664 -4.2426977 -4.255414 -4.2614188 -4.2492089][-4.2474332 -4.2380967 -4.2096968 -4.1886253 -4.1770339 -4.1602416 -4.140923 -4.1442332 -4.1688843 -4.2060604 -4.2379494 -4.2532296 -4.2647648 -4.2706165 -4.2631168][-4.2564054 -4.2426491 -4.2169843 -4.2001457 -4.1906676 -4.1783381 -4.1691751 -4.1772981 -4.1960931 -4.2166462 -4.2369237 -4.2505746 -4.2604961 -4.2689657 -4.2713509][-4.2434869 -4.2295036 -4.2155361 -4.2111292 -4.2099457 -4.20463 -4.2031736 -4.2097807 -4.2159495 -4.2181163 -4.2277136 -4.2392526 -4.2504253 -4.2613211 -4.2690005][-4.2105994 -4.196969 -4.2022138 -4.2183824 -4.227982 -4.227829 -4.2261095 -4.2226291 -4.2154417 -4.207624 -4.2136083 -4.2307291 -4.2451596 -4.2533703 -4.257081][-4.1642838 -4.1540666 -4.1792336 -4.2143426 -4.2340193 -4.2370825 -4.2318912 -4.2181926 -4.2006283 -4.1912208 -4.2029667 -4.2297692 -4.2486444 -4.2543235 -4.2521143][-4.1222596 -4.1160836 -4.1534157 -4.1979933 -4.221972 -4.2262726 -4.2216249 -4.2057095 -4.1831183 -4.1754751 -4.1972671 -4.2328062 -4.2538047 -4.2580934 -4.2528181]]...]
INFO - root - 2017-12-07 18:47:58.146859: step 41710, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 58h:46m:17s remains)
INFO - root - 2017-12-07 18:48:04.923703: step 41720, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 56h:26m:56s remains)
INFO - root - 2017-12-07 18:48:11.716279: step 41730, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 53h:37m:04s remains)
INFO - root - 2017-12-07 18:48:18.520040: step 41740, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 51h:49m:19s remains)
INFO - root - 2017-12-07 18:48:25.417190: step 41750, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 58h:39m:42s remains)
INFO - root - 2017-12-07 18:48:32.336318: step 41760, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 57h:19m:17s remains)
INFO - root - 2017-12-07 18:48:39.178198: step 41770, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.680 sec/batch; 54h:54m:32s remains)
INFO - root - 2017-12-07 18:48:45.926565: step 41780, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 54h:30m:27s remains)
INFO - root - 2017-12-07 18:48:52.630221: step 41790, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 58h:15m:08s remains)
INFO - root - 2017-12-07 18:48:59.344694: step 41800, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 57h:32m:51s remains)
2017-12-07 18:49:00.090216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2210441 -4.2170424 -4.2366858 -4.2579975 -4.2654691 -4.2629576 -4.2551503 -4.2496085 -4.2517405 -4.259614 -4.2653618 -4.2604375 -4.2484465 -4.2381716 -4.2391624][-4.20877 -4.2109056 -4.2368422 -4.2586746 -4.261961 -4.2542977 -4.2436442 -4.2387896 -4.2419786 -4.2490325 -4.251575 -4.2431951 -4.229167 -4.2204642 -4.2268624][-4.2261524 -4.2347932 -4.2620988 -4.2814074 -4.28034 -4.2667117 -4.2524137 -4.2480321 -4.2531366 -4.2602091 -4.2604933 -4.2496815 -4.2339072 -4.2247839 -4.2321773][-4.2364473 -4.2496958 -4.2769842 -4.294219 -4.2903008 -4.2734332 -4.2572289 -4.2536597 -4.2614841 -4.26987 -4.270103 -4.2597752 -4.2444568 -4.2350564 -4.2407374][-4.2148681 -4.232976 -4.2632513 -4.2807345 -4.2740388 -4.252933 -4.2334437 -4.2311006 -4.2456818 -4.2594891 -4.2643795 -4.2614989 -4.25253 -4.2454448 -4.2492719][-4.1753855 -4.1964855 -4.2295508 -4.2470593 -4.2346725 -4.2025113 -4.1705 -4.1653442 -4.192749 -4.2197671 -4.2360597 -4.2465219 -4.2475381 -4.2461228 -4.249855][-4.111474 -4.1349115 -4.1703153 -4.1864467 -4.1647735 -4.1154432 -4.0623426 -4.0519686 -4.1024518 -4.1533985 -4.1859522 -4.2090497 -4.2172513 -4.2182212 -4.2202864][-4.0645671 -4.0896378 -4.1234956 -4.1370144 -4.1081481 -4.0469122 -3.9773102 -3.9611809 -4.030786 -4.0992012 -4.1409507 -4.1700258 -4.1818643 -4.1830268 -4.1796565][-4.086957 -4.1148438 -4.1471138 -4.1587572 -4.136488 -4.0921679 -4.04035 -4.0262127 -4.0796747 -4.1318917 -4.1596532 -4.1763477 -4.1796 -4.1756625 -4.1644239][-4.1386447 -4.1634059 -4.1915131 -4.2004929 -4.1871409 -4.16281 -4.1348524 -4.1259952 -4.1556983 -4.1842523 -4.1969595 -4.2032161 -4.1991658 -4.1913548 -4.1756678][-4.1803622 -4.1957073 -4.2121792 -4.215385 -4.2063212 -4.19811 -4.1898155 -4.1866179 -4.2002821 -4.215601 -4.2231441 -4.2279572 -4.2278247 -4.2263675 -4.2135034][-4.2034783 -4.2004919 -4.2021551 -4.1992097 -4.1944027 -4.2021341 -4.2126021 -4.2224894 -4.2333121 -4.2402911 -4.2426972 -4.243968 -4.2480268 -4.2540789 -4.2470951][-4.184505 -4.1627231 -4.1565757 -4.1544433 -4.1586537 -4.1803837 -4.2026882 -4.2205648 -4.2298937 -4.2312121 -4.230082 -4.229774 -4.2367659 -4.2491226 -4.2478881][-4.1329603 -4.0975165 -4.0880046 -4.0889106 -4.0988688 -4.127912 -4.1512942 -4.16723 -4.1701941 -4.1653652 -4.1600971 -4.1628785 -4.1775546 -4.1996989 -4.20935][-4.0770683 -4.0375566 -4.0275254 -4.0326571 -4.0482755 -4.0813346 -4.1002865 -4.10755 -4.1006207 -4.0912228 -4.0867691 -4.0945582 -4.1148939 -4.14541 -4.1654186]]...]
INFO - root - 2017-12-07 18:49:06.866934: step 41810, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 51h:07m:28s remains)
INFO - root - 2017-12-07 18:49:13.685939: step 41820, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 57h:57m:46s remains)
INFO - root - 2017-12-07 18:49:20.422739: step 41830, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 59h:25m:43s remains)
INFO - root - 2017-12-07 18:49:27.205323: step 41840, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 54h:25m:53s remains)
INFO - root - 2017-12-07 18:49:33.915079: step 41850, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 51h:44m:34s remains)
INFO - root - 2017-12-07 18:49:40.718641: step 41860, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 52h:55m:08s remains)
INFO - root - 2017-12-07 18:49:47.527287: step 41870, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 58h:52m:34s remains)
INFO - root - 2017-12-07 18:49:54.422797: step 41880, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 59h:41m:01s remains)
INFO - root - 2017-12-07 18:50:01.171878: step 41890, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 54h:02m:50s remains)
INFO - root - 2017-12-07 18:50:07.748056: step 41900, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 50h:21m:39s remains)
2017-12-07 18:50:08.592619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2062039 -4.2132297 -4.1982741 -4.1942434 -4.1946607 -4.1916366 -4.1967759 -4.200232 -4.1795468 -4.1528525 -4.1450095 -4.16531 -4.211257 -4.2543788 -4.2752447][-4.2035995 -4.1985598 -4.1765928 -4.1664515 -4.155982 -4.1465154 -4.15337 -4.1620522 -4.1507874 -4.1339712 -4.1292324 -4.1476965 -4.1839271 -4.2217555 -4.2512569][-4.1928124 -4.1851692 -4.165596 -4.15103 -4.1306982 -4.1142015 -4.1170611 -4.120738 -4.1168785 -4.1153417 -4.1208677 -4.1359587 -4.1619072 -4.1910357 -4.2227163][-4.1777182 -4.1761842 -4.1635823 -4.1486282 -4.1231036 -4.1024041 -4.0993128 -4.0996971 -4.1011391 -4.1097217 -4.1199746 -4.1267395 -4.1383843 -4.1578283 -4.1882253][-4.1683192 -4.1733618 -4.1687384 -4.1543617 -4.1227789 -4.097919 -4.090045 -4.0895162 -4.0998588 -4.1144018 -4.1176386 -4.1124349 -4.1132646 -4.1210575 -4.1470747][-4.1768627 -4.1822572 -4.1742611 -4.15251 -4.111352 -4.0818353 -4.0712161 -4.0752935 -4.100338 -4.1173983 -4.1118808 -4.0943503 -4.0867314 -4.0883064 -4.1069202][-4.1936851 -4.195385 -4.1844735 -4.150691 -4.0974054 -4.064754 -4.0502028 -4.0568819 -4.0857325 -4.1005673 -4.0935459 -4.076344 -4.0737267 -4.0774784 -4.0933766][-4.212635 -4.2073445 -4.1934829 -4.1601825 -4.1092219 -4.0790715 -4.0614271 -4.064116 -4.0869312 -4.0930281 -4.0828428 -4.0762544 -4.0863132 -4.097096 -4.1016688][-4.2296333 -4.2180452 -4.2048283 -4.1787782 -4.14061 -4.1207061 -4.1079617 -4.1114745 -4.1270595 -4.1164947 -4.0977454 -4.1021943 -4.1164613 -4.1233759 -4.1134167][-4.2492771 -4.2351027 -4.2176085 -4.19647 -4.173316 -4.1687245 -4.1684947 -4.1771178 -4.1852708 -4.1643949 -4.1426024 -4.1457376 -4.14997 -4.1401019 -4.1196623][-4.2626257 -4.2557707 -4.2403722 -4.22221 -4.2070484 -4.2060022 -4.2143865 -4.22564 -4.224112 -4.2026663 -4.18672 -4.1824961 -4.17709 -4.1607409 -4.1356478][-4.2619529 -4.263226 -4.256412 -4.247664 -4.2378588 -4.2355905 -4.2446232 -4.2522035 -4.2466483 -4.2268434 -4.2142544 -4.2078543 -4.1973529 -4.1833863 -4.1603575][-4.2501554 -4.25715 -4.2624068 -4.2645473 -4.2603874 -4.2582006 -4.2641568 -4.2659864 -4.2582135 -4.2417474 -4.2339864 -4.2266035 -4.2130141 -4.20155 -4.1825838][-4.2395344 -4.2453623 -4.2544737 -4.2626905 -4.2650604 -4.266953 -4.273037 -4.2728653 -4.2672286 -4.2566757 -4.2534485 -4.2448897 -4.2291837 -4.2144909 -4.1976147][-4.2356668 -4.23756 -4.2426572 -4.25007 -4.255125 -4.2607718 -4.270175 -4.27398 -4.2719684 -4.2651725 -4.2616029 -4.2552366 -4.2414341 -4.2264428 -4.2117405]]...]
INFO - root - 2017-12-07 18:50:15.364729: step 41910, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 53h:34m:30s remains)
INFO - root - 2017-12-07 18:50:22.094533: step 41920, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 51h:08m:50s remains)
INFO - root - 2017-12-07 18:50:28.738990: step 41930, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 51h:29m:05s remains)
INFO - root - 2017-12-07 18:50:35.559828: step 41940, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 57h:53m:57s remains)
INFO - root - 2017-12-07 18:50:42.398719: step 41950, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 57h:48m:19s remains)
INFO - root - 2017-12-07 18:50:49.182329: step 41960, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 51h:55m:57s remains)
INFO - root - 2017-12-07 18:50:55.928534: step 41970, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 52h:39m:56s remains)
INFO - root - 2017-12-07 18:51:02.725867: step 41980, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 51h:59m:00s remains)
INFO - root - 2017-12-07 18:51:09.548436: step 41990, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 57h:25m:44s remains)
INFO - root - 2017-12-07 18:51:16.172368: step 42000, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 57h:10m:14s remains)
2017-12-07 18:51:16.944638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3345871 -4.3218513 -4.3032718 -4.2892113 -4.2825494 -4.2814264 -4.281188 -4.2898173 -4.3068228 -4.32574 -4.341866 -4.351295 -4.3560972 -4.3590007 -4.3610549][-4.305686 -4.2861767 -4.2595491 -4.2399454 -4.2300282 -4.2267122 -4.2234039 -4.2321725 -4.2547455 -4.28287 -4.3097472 -4.3272943 -4.338377 -4.345881 -4.3514886][-4.2668052 -4.2422395 -4.213285 -4.1937604 -4.1823573 -4.1744194 -4.1634121 -4.166029 -4.1891766 -4.224009 -4.2618208 -4.2900915 -4.31136 -4.3269963 -4.3391566][-4.2241726 -4.2073793 -4.1848545 -4.1684856 -4.1530623 -4.13704 -4.1105266 -4.0965552 -4.1137648 -4.1508665 -4.1969194 -4.2364078 -4.2716675 -4.3002682 -4.3222656][-4.1908145 -4.1858053 -4.173811 -4.1589832 -4.13356 -4.1015429 -4.0525465 -4.0128694 -4.0220232 -4.0673494 -4.127687 -4.183301 -4.2341309 -4.2752352 -4.3043904][-4.1605759 -4.1628633 -4.1566076 -4.141552 -4.1084843 -4.0580978 -3.9759645 -3.9032879 -3.9146614 -3.986881 -4.0754194 -4.1488376 -4.211802 -4.2602377 -4.2920918][-4.1297131 -4.1371531 -4.1361108 -4.1221285 -4.0862756 -4.0192914 -3.8972466 -3.7941749 -3.825444 -3.9357934 -4.0543518 -4.1397519 -4.2057009 -4.2535744 -4.2855382][-4.1211891 -4.1287317 -4.1308737 -4.1209383 -4.0841742 -4.0082645 -3.8753405 -3.7753448 -3.8291681 -3.9472775 -4.0642066 -4.1465631 -4.2065735 -4.2495308 -4.2809024][-4.1411791 -4.1455107 -4.1480036 -4.1461229 -4.1186881 -4.0583692 -3.9583945 -3.8914185 -3.9424841 -4.0235066 -4.1014867 -4.1620421 -4.2117372 -4.2483034 -4.2779074][-4.153945 -4.1578021 -4.1602788 -4.1662416 -4.1517563 -4.1129766 -4.0490489 -4.0106111 -4.0509229 -4.0954747 -4.1392565 -4.1772513 -4.2193403 -4.2507954 -4.2764626][-4.1696997 -4.1757579 -4.1752315 -4.1800914 -4.1703687 -4.1471553 -4.106925 -4.0840721 -4.1142635 -4.1403713 -4.1652169 -4.1871815 -4.2213054 -4.2501564 -4.2730589][-4.1919084 -4.1969528 -4.1862073 -4.1823635 -4.176609 -4.1658669 -4.1460223 -4.1322975 -4.1546187 -4.1724038 -4.1882224 -4.1998143 -4.2225971 -4.247354 -4.2700634][-4.2127905 -4.2139568 -4.1947031 -4.1791534 -4.1701317 -4.1657248 -4.1568666 -4.1484175 -4.1700206 -4.1903577 -4.2046666 -4.2121086 -4.2286654 -4.2518439 -4.2725773][-4.2060175 -4.2125292 -4.2008352 -4.1828542 -4.1676221 -4.1619134 -4.1573396 -4.1558228 -4.181 -4.2024989 -4.216948 -4.2243347 -4.2419109 -4.2642837 -4.281558][-4.1742215 -4.1906304 -4.1982956 -4.1916075 -4.1752219 -4.1626668 -4.1507215 -4.1450882 -4.171226 -4.1966333 -4.2174435 -4.2313633 -4.2538829 -4.2773018 -4.2920632]]...]
INFO - root - 2017-12-07 18:51:23.788787: step 42010, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 55h:37m:31s remains)
INFO - root - 2017-12-07 18:51:30.566851: step 42020, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 58h:03m:34s remains)
INFO - root - 2017-12-07 18:51:37.421440: step 42030, loss = 2.07, batch loss = 2.02 (10.6 examples/sec; 0.755 sec/batch; 60h:57m:25s remains)
INFO - root - 2017-12-07 18:51:44.146713: step 42040, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 53h:48m:25s remains)
INFO - root - 2017-12-07 18:51:50.871424: step 42050, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 52h:10m:28s remains)
INFO - root - 2017-12-07 18:51:57.713907: step 42060, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 54h:20m:03s remains)
INFO - root - 2017-12-07 18:52:04.498115: step 42070, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 56h:25m:40s remains)
INFO - root - 2017-12-07 18:52:11.381592: step 42080, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 56h:12m:45s remains)
INFO - root - 2017-12-07 18:52:18.208243: step 42090, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 55h:15m:06s remains)
INFO - root - 2017-12-07 18:52:24.571886: step 42100, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 51h:55m:17s remains)
2017-12-07 18:52:25.288601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2057691 -4.2169609 -4.2419777 -4.2524595 -4.2364349 -4.2089458 -4.1810093 -4.1570468 -4.1415997 -4.1212029 -4.1152658 -4.126421 -4.1403875 -4.1469703 -4.1492143][-4.1994286 -4.2119913 -4.2374654 -4.2464533 -4.228106 -4.2011805 -4.1783085 -4.1525769 -4.1347451 -4.1156693 -4.1081367 -4.1149583 -4.1267257 -4.1328683 -4.1309538][-4.1772528 -4.1854763 -4.2052536 -4.2089834 -4.1944776 -4.1700993 -4.1559362 -4.1345434 -4.1211758 -4.1108584 -4.1108007 -4.1181641 -4.12771 -4.1298933 -4.1190219][-4.1431236 -4.1465058 -4.1562486 -4.1580305 -4.1491532 -4.1314788 -4.1210632 -4.1117606 -4.1115189 -4.1156964 -4.1261549 -4.1379004 -4.1456556 -4.1436729 -4.1269298][-4.1096354 -4.1049962 -4.0991268 -4.0911269 -4.0792947 -4.066328 -4.0620866 -4.0614238 -4.0766168 -4.1068153 -4.134861 -4.1540365 -4.1622348 -4.1577382 -4.1436739][-4.0883017 -4.0798216 -4.0578127 -4.0323038 -4.0034394 -3.9794416 -3.9690356 -3.9649699 -3.9980674 -4.0636592 -4.1149731 -4.1388445 -4.1445751 -4.1387315 -4.1364918][-4.0797019 -4.075685 -4.051415 -4.0106654 -3.9615304 -3.9166932 -3.8779132 -3.848963 -3.8948739 -3.9972103 -4.0719533 -4.0953541 -4.0934176 -4.0852757 -4.09747][-4.0836654 -4.0885682 -4.0766449 -4.0409737 -3.9957037 -3.9516354 -3.9040363 -3.8644648 -3.9022179 -3.9939857 -4.0568161 -4.0700479 -4.0567079 -4.0420771 -4.062325][-4.089438 -4.0961242 -4.0984125 -4.0812888 -4.0613 -4.0417962 -4.0138435 -3.9955344 -4.0174227 -4.063417 -4.0943313 -4.0903578 -4.0657482 -4.0411944 -4.055655][-4.1131687 -4.1203523 -4.1311231 -4.1281862 -4.1243639 -4.1195765 -4.1055489 -4.0993662 -4.1074238 -4.1230626 -4.1334891 -4.12473 -4.1007981 -4.0742626 -4.07661][-4.1424456 -4.1553669 -4.1738358 -4.1785107 -4.1809392 -4.1801114 -4.1673594 -4.1623321 -4.1663074 -4.1693773 -4.1711578 -4.1594505 -4.137815 -4.1110477 -4.1072097][-4.1472235 -4.1672239 -4.1914244 -4.2055087 -4.2131166 -4.2129517 -4.203867 -4.2035379 -4.2108111 -4.2094259 -4.2056465 -4.192256 -4.1712403 -4.14757 -4.1438656][-4.1481342 -4.172853 -4.1978922 -4.2167068 -4.2241893 -4.2235289 -4.2205825 -4.226016 -4.2358675 -4.2348304 -4.2304888 -4.2243595 -4.209331 -4.1930361 -4.1886935][-4.151536 -4.1769662 -4.2031951 -4.2235003 -4.2319732 -4.2316856 -4.2324986 -4.241199 -4.2516904 -4.2507839 -4.2486258 -4.2503376 -4.2461104 -4.2374053 -4.2318697][-4.17303 -4.19189 -4.2113872 -4.230207 -4.241734 -4.2466407 -4.2524962 -4.2631764 -4.271853 -4.268651 -4.2658982 -4.2707157 -4.2705712 -4.2637658 -4.2563787]]...]
INFO - root - 2017-12-07 18:52:32.094147: step 42110, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.687 sec/batch; 55h:23m:29s remains)
INFO - root - 2017-12-07 18:52:38.852812: step 42120, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 51h:01m:05s remains)
INFO - root - 2017-12-07 18:52:45.652231: step 42130, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 51h:40m:50s remains)
INFO - root - 2017-12-07 18:52:52.462896: step 42140, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.732 sec/batch; 59h:01m:05s remains)
INFO - root - 2017-12-07 18:52:59.376822: step 42150, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 57h:54m:35s remains)
INFO - root - 2017-12-07 18:53:06.142292: step 42160, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 53h:48m:52s remains)
INFO - root - 2017-12-07 18:53:12.991692: step 42170, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.619 sec/batch; 49h:53m:58s remains)
INFO - root - 2017-12-07 18:53:19.744424: step 42180, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 52h:24m:10s remains)
INFO - root - 2017-12-07 18:53:26.534359: step 42190, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 58h:35m:14s remains)
INFO - root - 2017-12-07 18:53:33.180379: step 42200, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 58h:11m:36s remains)
2017-12-07 18:53:33.885185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1709065 -4.1823254 -4.2093749 -4.2089162 -4.1867681 -4.1670861 -4.1549449 -4.1397982 -4.1395044 -4.1623626 -4.1962886 -4.2176566 -4.2201972 -4.1957321 -4.1659155][-4.214468 -4.2129817 -4.2197056 -4.20633 -4.180994 -4.1607924 -4.1429043 -4.1274042 -4.1297331 -4.153409 -4.1843033 -4.2059035 -4.216651 -4.1995988 -4.17228][-4.2467422 -4.2296686 -4.2185245 -4.1996408 -4.1764617 -4.1604381 -4.1448312 -4.1336546 -4.137001 -4.1531129 -4.1726279 -4.1919546 -4.2153683 -4.2103505 -4.187964][-4.2590857 -4.2323675 -4.2105432 -4.1938677 -4.1777496 -4.1672659 -4.1535292 -4.1436262 -4.1472063 -4.1567521 -4.1670561 -4.1827126 -4.2134342 -4.2192912 -4.1972361][-4.2608109 -4.236578 -4.2132673 -4.200336 -4.1905117 -4.1835651 -4.1675663 -4.1507268 -4.1550121 -4.1662817 -4.1727519 -4.1821246 -4.2089219 -4.2124686 -4.1822877][-4.2512259 -4.2397437 -4.2265096 -4.2201843 -4.212636 -4.205668 -4.1857238 -4.1548495 -4.1584969 -4.1708088 -4.1697526 -4.1691031 -4.1879773 -4.18085 -4.1419978][-4.2467475 -4.2402306 -4.2325082 -4.2276745 -4.2204542 -4.2115231 -4.1818886 -4.1310482 -4.12537 -4.1331844 -4.1257439 -4.1224742 -4.1418648 -4.1400714 -4.1157374][-4.2436662 -4.2335992 -4.2234879 -4.2133951 -4.2041073 -4.1847491 -4.1326284 -4.0485125 -4.0236292 -4.0442405 -4.0578732 -4.0816631 -4.1270971 -4.1520762 -4.1513271][-4.2282219 -4.2131758 -4.1923957 -4.1731243 -4.1569581 -4.1217451 -4.0436506 -3.9408412 -3.9280789 -4.0001645 -4.0624976 -4.1182923 -4.1801023 -4.2122831 -4.212256][-4.1905651 -4.1664019 -4.1389413 -4.1151381 -4.0992937 -4.0679975 -4.0070028 -3.9531178 -3.9837327 -4.0733709 -4.1409655 -4.1932273 -4.2383614 -4.2571383 -4.2474532][-4.1352267 -4.112596 -4.0968094 -4.0902672 -4.0989704 -4.0998154 -4.0751438 -4.0628109 -4.101706 -4.1647668 -4.210309 -4.2413564 -4.2606039 -4.2571454 -4.2291894][-4.1031828 -4.099153 -4.1125665 -4.1302829 -4.1598029 -4.177341 -4.1659803 -4.1590619 -4.1826358 -4.21406 -4.2384167 -4.24095 -4.2326579 -4.2146592 -4.1821613][-4.1195283 -4.1323481 -4.1592989 -4.1830034 -4.2109604 -4.2222009 -4.20483 -4.1948218 -4.2032757 -4.2084107 -4.2105889 -4.1939487 -4.17137 -4.156569 -4.1396904][-4.1570845 -4.1730285 -4.1947684 -4.2066708 -4.219173 -4.21899 -4.1958117 -4.1779423 -4.1702552 -4.1573753 -4.1493869 -4.1340113 -4.1217012 -4.1229482 -4.1280594][-4.1851783 -4.1966453 -4.2108593 -4.2113523 -4.212018 -4.2071204 -4.1809864 -4.1531663 -4.1305542 -4.1056132 -4.1063213 -4.112123 -4.1162305 -4.1367207 -4.1574111]]...]
INFO - root - 2017-12-07 18:53:40.721285: step 42210, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 53h:12m:20s remains)
INFO - root - 2017-12-07 18:53:47.543743: step 42220, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 59h:08m:19s remains)
INFO - root - 2017-12-07 18:53:54.390100: step 42230, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 59h:46m:00s remains)
INFO - root - 2017-12-07 18:54:01.142330: step 42240, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.649 sec/batch; 52h:18m:19s remains)
INFO - root - 2017-12-07 18:54:07.876831: step 42250, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 50h:37m:42s remains)
INFO - root - 2017-12-07 18:54:14.687981: step 42260, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 53h:37m:12s remains)
INFO - root - 2017-12-07 18:54:21.564900: step 42270, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 57h:19m:45s remains)
INFO - root - 2017-12-07 18:54:28.311682: step 42280, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 55h:42m:19s remains)
INFO - root - 2017-12-07 18:54:35.145656: step 42290, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 50h:50m:56s remains)
INFO - root - 2017-12-07 18:54:41.770964: step 42300, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 51h:49m:36s remains)
2017-12-07 18:54:42.504431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3272839 -4.3179641 -4.305326 -4.2934623 -4.2861695 -4.2831292 -4.2778573 -4.2672753 -4.263607 -4.2674375 -4.2767143 -4.2919941 -4.2999368 -4.3098931 -4.31282][-4.3259664 -4.3118534 -4.2952166 -4.2789226 -4.2667322 -4.2563624 -4.2444367 -4.2263627 -4.2233448 -4.2307687 -4.2452159 -4.265686 -4.2769685 -4.2939262 -4.2995639][-4.3184328 -4.3040433 -4.2847533 -4.2655225 -4.2544227 -4.2412543 -4.2248788 -4.2001739 -4.1969008 -4.2020903 -4.2172055 -4.2403703 -4.253583 -4.2770109 -4.2855535][-4.3015385 -4.28623 -4.2634444 -4.2394533 -4.2314234 -4.2194285 -4.2035918 -4.179163 -4.1764784 -4.1778727 -4.1908765 -4.2159009 -4.2316179 -4.2608886 -4.2731237][-4.2767348 -4.2578425 -4.2269263 -4.1946225 -4.182518 -4.172462 -4.1601758 -4.1397891 -4.1429873 -4.1446404 -4.160706 -4.1898389 -4.2097197 -4.2473006 -4.2646489][-4.2568274 -4.2315478 -4.19018 -4.1436176 -4.1204681 -4.1056137 -4.0923991 -4.079535 -4.0966725 -4.1111164 -4.1331224 -4.1655831 -4.1903205 -4.2358828 -4.2594304][-4.2532496 -4.2237735 -4.1713438 -4.1075158 -4.0618043 -4.0255513 -3.9999323 -3.9869633 -4.0206523 -4.0599942 -4.0991721 -4.1399846 -4.1712661 -4.2218142 -4.2539048][-4.26716 -4.2391305 -4.1875854 -4.1184716 -4.056675 -3.9934425 -3.9377837 -3.9033027 -3.9317973 -3.9859254 -4.04894 -4.1068282 -4.1515036 -4.2064285 -4.242198][-4.2853127 -4.2675896 -4.2321129 -4.1793814 -4.1241589 -4.054821 -3.981391 -3.9175925 -3.9142113 -3.9530103 -4.0184774 -4.0856276 -4.1398149 -4.1986876 -4.2345285][-4.29198 -4.2880492 -4.2735753 -4.2422252 -4.206562 -4.1547046 -4.0930481 -4.0289993 -4.0055943 -4.0176244 -4.0604439 -4.1110969 -4.1543288 -4.2059288 -4.2386036][-4.2922993 -4.3014007 -4.3011413 -4.2851496 -4.2663651 -4.2331147 -4.1916332 -4.1497655 -4.1284637 -4.1259375 -4.1455665 -4.1731563 -4.1987071 -4.2339363 -4.2557988][-4.2879667 -4.3035007 -4.3111057 -4.30496 -4.2983365 -4.2807283 -4.2592878 -4.2409778 -4.2299595 -4.2250714 -4.2316318 -4.2436719 -4.2555723 -4.2735677 -4.282815][-4.2802696 -4.2932773 -4.3016367 -4.3018193 -4.30558 -4.2983179 -4.2891788 -4.2851896 -4.2832561 -4.2834268 -4.2884235 -4.2937241 -4.2964807 -4.3027568 -4.3038654][-4.2815385 -4.2867417 -4.2904291 -4.2902241 -4.2957158 -4.294651 -4.2939954 -4.2982354 -4.3027534 -4.3076029 -4.31365 -4.3179579 -4.3164978 -4.3148737 -4.3119][-4.2895827 -4.2912312 -4.292563 -4.2911377 -4.2930231 -4.291254 -4.2917051 -4.2964339 -4.3021107 -4.3083267 -4.3138986 -4.3179259 -4.3172846 -4.3152862 -4.312726]]...]
INFO - root - 2017-12-07 18:54:49.275950: step 42310, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 55h:28m:09s remains)
INFO - root - 2017-12-07 18:54:56.025128: step 42320, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 51h:04m:20s remains)
INFO - root - 2017-12-07 18:55:02.850168: step 42330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 53h:22m:27s remains)
INFO - root - 2017-12-07 18:55:09.725303: step 42340, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 58h:38m:04s remains)
INFO - root - 2017-12-07 18:55:16.535203: step 42350, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.754 sec/batch; 60h:46m:42s remains)
INFO - root - 2017-12-07 18:55:23.334099: step 42360, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 55h:56m:44s remains)
INFO - root - 2017-12-07 18:55:30.135810: step 42370, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 54h:25m:09s remains)
INFO - root - 2017-12-07 18:55:36.856467: step 42380, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 52h:24m:55s remains)
INFO - root - 2017-12-07 18:55:43.647962: step 42390, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 53h:53m:52s remains)
INFO - root - 2017-12-07 18:55:50.237397: step 42400, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 55h:11m:16s remains)
2017-12-07 18:55:50.970996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3192616 -4.3172603 -4.3169827 -4.3198662 -4.3248696 -4.3314719 -4.3354106 -4.335885 -4.3372755 -4.3391805 -4.343205 -4.3469033 -4.3464484 -4.3438649 -4.3404179][-4.2933931 -4.2863741 -4.2810936 -4.2828431 -4.2886953 -4.2980423 -4.3017373 -4.3011518 -4.3036962 -4.3085337 -4.3154745 -4.3220291 -4.3214979 -4.3181419 -4.3157353][-4.2687612 -4.2580509 -4.2483811 -4.2464585 -4.2488513 -4.2521911 -4.2461324 -4.2379293 -4.2409396 -4.2538209 -4.2682433 -4.282176 -4.2859831 -4.2872591 -4.2896686][-4.2516146 -4.237051 -4.221611 -4.2086577 -4.1992245 -4.1881437 -4.1683464 -4.15156 -4.1572371 -4.183629 -4.2096462 -4.2340512 -4.2459598 -4.253531 -4.2623253][-4.2387776 -4.2208261 -4.1935468 -4.1602974 -4.1298423 -4.1039686 -4.0754361 -4.0535183 -4.0619349 -4.103971 -4.1443853 -4.1833296 -4.2089286 -4.2246513 -4.2395463][-4.2209072 -4.1970062 -4.1574445 -4.1073895 -4.0603752 -4.0243721 -3.9945252 -3.9710391 -3.9779267 -4.0317287 -4.0839834 -4.1352563 -4.1757069 -4.2012839 -4.2226648][-4.1997728 -4.1654649 -4.1130466 -4.0569696 -4.0050054 -3.9679983 -3.93828 -3.9106367 -3.9139533 -3.971642 -4.0305514 -4.0901227 -4.1426482 -4.1790957 -4.2098336][-4.1861329 -4.1472621 -4.0926766 -4.0387411 -3.9873271 -3.9516211 -3.918575 -3.8835998 -3.8851414 -3.9424067 -4.0020413 -4.0646372 -4.1233311 -4.1664114 -4.2028389][-4.1877851 -4.1562796 -4.1162024 -4.0810585 -4.0408931 -4.0012813 -3.956975 -3.9172935 -3.9145646 -3.9664078 -4.023715 -4.0811043 -4.1330385 -4.1721349 -4.2067652][-4.2034316 -4.1783233 -4.1528268 -4.136642 -4.1110072 -4.0755129 -4.0348935 -4.005733 -4.0046043 -4.0407305 -4.086278 -4.1319323 -4.1700182 -4.1987805 -4.225183][-4.2319126 -4.20833 -4.187778 -4.1801124 -4.1644173 -4.1398077 -4.1151319 -4.1032524 -4.105514 -4.126307 -4.1571178 -4.1923404 -4.2206306 -4.239378 -4.2558594][-4.2620664 -4.2384109 -4.2203083 -4.2159476 -4.2092509 -4.2000294 -4.19215 -4.1935906 -4.1989326 -4.2108088 -4.2298179 -4.2534418 -4.2694354 -4.2786131 -4.2878857][-4.290947 -4.2712269 -4.2574821 -4.2576661 -4.2615771 -4.262423 -4.2621155 -4.267324 -4.2727413 -4.2796531 -4.2902055 -4.3031793 -4.3096461 -4.3132195 -4.3190141][-4.3131142 -4.2989521 -4.2912636 -4.2950063 -4.3022776 -4.3061719 -4.3072109 -4.3113575 -4.3150268 -4.3174014 -4.3214622 -4.327517 -4.3305345 -4.3331728 -4.3385062][-4.3365226 -4.3292203 -4.3252292 -4.3267965 -4.3299 -4.331193 -4.3316693 -4.3327761 -4.3335762 -4.3342881 -4.3360634 -4.3396654 -4.3428006 -4.3458591 -4.3499851]]...]
INFO - root - 2017-12-07 18:55:57.582918: step 42410, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 53h:33m:01s remains)
INFO - root - 2017-12-07 18:56:04.391926: step 42420, loss = 2.02, batch loss = 1.96 (11.1 examples/sec; 0.722 sec/batch; 58h:10m:39s remains)
INFO - root - 2017-12-07 18:56:11.197172: step 42430, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 54h:45m:47s remains)
INFO - root - 2017-12-07 18:56:18.036917: step 42440, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 52h:44m:31s remains)
INFO - root - 2017-12-07 18:56:24.844066: step 42450, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 51h:26m:20s remains)
INFO - root - 2017-12-07 18:56:31.593151: step 42460, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 56h:19m:15s remains)
INFO - root - 2017-12-07 18:56:38.364926: step 42470, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 55h:50m:02s remains)
INFO - root - 2017-12-07 18:56:45.172828: step 42480, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 54h:39m:30s remains)
INFO - root - 2017-12-07 18:56:51.997246: step 42490, loss = 2.08, batch loss = 2.02 (13.6 examples/sec; 0.587 sec/batch; 47h:16m:42s remains)
INFO - root - 2017-12-07 18:56:58.498938: step 42500, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 51h:25m:55s remains)
2017-12-07 18:56:59.279897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2084131 -4.2123642 -4.2370391 -4.259964 -4.2707305 -4.2632728 -4.2516985 -4.2473931 -4.2485175 -4.2536235 -4.2358947 -4.1983 -4.17231 -4.1782432 -4.2038164][-4.216433 -4.2100997 -4.2278075 -4.2513642 -4.2663269 -4.2582636 -4.2413521 -4.23089 -4.2288775 -4.2387033 -4.2317061 -4.1974568 -4.1691728 -4.1736703 -4.1989818][-4.2263904 -4.2121873 -4.2189922 -4.2355371 -4.2503705 -4.246562 -4.2295403 -4.2146678 -4.2123022 -4.2246208 -4.22634 -4.1990986 -4.1733727 -4.1763706 -4.2003865][-4.2291408 -4.2105217 -4.2071586 -4.2141261 -4.2287374 -4.2296333 -4.2159586 -4.2031469 -4.2059627 -4.2240696 -4.2309575 -4.2100258 -4.1830497 -4.1796761 -4.1971707][-4.2283883 -4.207706 -4.197649 -4.1977773 -4.2070312 -4.2044525 -4.1917105 -4.1833854 -4.19849 -4.2299824 -4.2466912 -4.2303934 -4.194643 -4.1741767 -4.1776843][-4.2387676 -4.2188163 -4.2031407 -4.1913595 -4.1868215 -4.1722608 -4.1488714 -4.1365604 -4.1657815 -4.218996 -4.2521763 -4.2466826 -4.2051544 -4.1640468 -4.1465797][-4.25651 -4.2342677 -4.2121553 -4.1864595 -4.1593194 -4.1213889 -4.0747414 -4.0474987 -4.092186 -4.1751275 -4.2297421 -4.24353 -4.2143207 -4.1718378 -4.1437478][-4.2544188 -4.2335958 -4.20994 -4.17899 -4.1358876 -4.0713739 -3.9921257 -3.937952 -3.996635 -4.1113758 -4.1915846 -4.2286296 -4.2242031 -4.2009721 -4.1804147][-4.2285337 -4.2118511 -4.193449 -4.1714215 -4.1341963 -4.0636864 -3.9682522 -3.8982434 -3.9595697 -4.0853124 -4.1782317 -4.2315888 -4.245975 -4.2399278 -4.2242093][-4.2000985 -4.19071 -4.1822829 -4.179276 -4.1651492 -4.1181097 -4.0483432 -3.9981668 -4.0385394 -4.1272359 -4.1982207 -4.2450609 -4.2601385 -4.256691 -4.2392178][-4.2005777 -4.1913133 -4.1882315 -4.1974659 -4.2024717 -4.1785021 -4.1348109 -4.1012959 -4.1200771 -4.1703057 -4.2162061 -4.2524834 -4.2653685 -4.2575827 -4.2340217][-4.2249618 -4.2076221 -4.1988864 -4.2068472 -4.2197924 -4.2072377 -4.1769247 -4.1522694 -4.1600285 -4.1925941 -4.2251368 -4.2534747 -4.26617 -4.2597151 -4.2361903][-4.2451425 -4.2237554 -4.209887 -4.214057 -4.2267 -4.21854 -4.193181 -4.1696715 -4.1739392 -4.2049942 -4.2359743 -4.2593656 -4.2720509 -4.2693563 -4.2501564][-4.2652884 -4.2459068 -4.2293205 -4.2270617 -4.2343936 -4.22524 -4.1971393 -4.1702023 -4.1743727 -4.2088323 -4.2465587 -4.2683864 -4.2763824 -4.2711797 -4.2536][-4.2721515 -4.2632842 -4.2506561 -4.2454152 -4.2506766 -4.2406154 -4.2079659 -4.1783905 -4.1791353 -4.2106733 -4.2491107 -4.2682471 -4.2661018 -4.2559643 -4.2424436]]...]
INFO - root - 2017-12-07 18:57:06.112670: step 42510, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 55h:52m:44s remains)
INFO - root - 2017-12-07 18:57:12.876156: step 42520, loss = 2.06, batch loss = 2.00 (13.3 examples/sec; 0.600 sec/batch; 48h:20m:06s remains)
INFO - root - 2017-12-07 18:57:19.606264: step 42530, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 51h:48m:31s remains)
INFO - root - 2017-12-07 18:57:26.441255: step 42540, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 57h:20m:49s remains)
INFO - root - 2017-12-07 18:57:33.331794: step 42550, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 58h:08m:43s remains)
INFO - root - 2017-12-07 18:57:40.087986: step 42560, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 52h:46m:16s remains)
INFO - root - 2017-12-07 18:57:46.922227: step 42570, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 54h:33m:51s remains)
INFO - root - 2017-12-07 18:57:53.672132: step 42580, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 51h:23m:51s remains)
INFO - root - 2017-12-07 18:58:00.438050: step 42590, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 55h:30m:10s remains)
INFO - root - 2017-12-07 18:58:07.003540: step 42600, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 58h:36m:27s remains)
2017-12-07 18:58:07.800127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3033462 -4.2961516 -4.2884359 -4.2832327 -4.2795815 -4.2837019 -4.2933207 -4.3045297 -4.3097792 -4.3078947 -4.3006105 -4.2944493 -4.2968574 -4.3092217 -4.3226814][-4.2382264 -4.2248378 -4.2180123 -4.2214365 -4.2278867 -4.2423091 -4.2581611 -4.2678871 -4.2662764 -4.2539253 -4.2383261 -4.2309117 -4.2392974 -4.2621174 -4.2860131][-4.157506 -4.1413789 -4.1393051 -4.1562829 -4.1781588 -4.2024894 -4.220489 -4.2212887 -4.2068739 -4.1830397 -4.1613421 -4.1571069 -4.1766281 -4.2108622 -4.2442551][-4.080647 -4.06248 -4.0660229 -4.09581 -4.1323786 -4.1657457 -4.1802897 -4.168313 -4.1387029 -4.1043334 -4.0813861 -4.0881953 -4.1227417 -4.1676612 -4.2079797][-4.0239387 -4.01489 -4.0325346 -4.0747066 -4.1159577 -4.1438322 -4.1410456 -4.1064987 -4.0609965 -4.0262527 -4.0201883 -4.050499 -4.0995116 -4.1489043 -4.1897168][-4.0113597 -4.0299282 -4.0689774 -4.1113687 -4.1345663 -4.1343722 -4.0974126 -4.0267177 -3.9651275 -3.9501631 -3.9878476 -4.0534029 -4.1146269 -4.1620688 -4.195663][-4.0551143 -4.0961943 -4.1414075 -4.1691146 -4.159997 -4.1196904 -4.0351076 -3.9201179 -3.85791 -3.8976233 -3.9942083 -4.0885754 -4.1525989 -4.1918488 -4.2144451][-4.1168346 -4.1569662 -4.1899819 -4.1972656 -4.1594524 -4.0846457 -3.9562986 -3.8202825 -3.7994523 -3.9130027 -4.0474882 -4.1469321 -4.2004886 -4.2249904 -4.2311554][-4.1661935 -4.1962423 -4.2104816 -4.1946363 -4.1361217 -4.04057 -3.910398 -3.8155451 -3.8579865 -4.000844 -4.1303034 -4.2117624 -4.2429433 -4.2460785 -4.2349691][-4.1896544 -4.2112241 -4.2113023 -4.1839252 -4.116508 -4.0202379 -3.9242027 -3.8948016 -3.9719582 -4.1005883 -4.2022033 -4.2537723 -4.2598505 -4.2423344 -4.2178993][-4.1940212 -4.2070327 -4.2001276 -4.1735115 -4.1122284 -4.03335 -3.9818325 -3.9991486 -4.0814033 -4.1784444 -4.2423592 -4.2622614 -4.2433105 -4.2066402 -4.1746817][-4.1967959 -4.2028484 -4.1897583 -4.165494 -4.1201429 -4.0670743 -4.0553293 -4.0986567 -4.1714225 -4.2318487 -4.2595463 -4.2504287 -4.2064419 -4.1515479 -4.1138749][-4.2071662 -4.2073846 -4.1893015 -4.1674309 -4.1339254 -4.1034079 -4.1191316 -4.1723418 -4.2286363 -4.2584209 -4.2577519 -4.2266169 -4.1603475 -4.0925093 -4.0592532][-4.2203364 -4.2236719 -4.2082176 -4.1874413 -4.1615148 -4.1484647 -4.1754384 -4.2228036 -4.2611933 -4.2671771 -4.2464657 -4.1969686 -4.1196556 -4.0505214 -4.0333123][-4.232595 -4.2378182 -4.228941 -4.2149858 -4.2005887 -4.1991124 -4.2258682 -4.2594314 -4.2769027 -4.2668605 -4.2297816 -4.1683912 -4.0891509 -4.0282464 -4.033267]]...]
INFO - root - 2017-12-07 18:58:14.596689: step 42610, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 52h:41m:30s remains)
INFO - root - 2017-12-07 18:58:21.435959: step 42620, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.755 sec/batch; 60h:46m:52s remains)
INFO - root - 2017-12-07 18:58:28.158171: step 42630, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 54h:59m:20s remains)
INFO - root - 2017-12-07 18:58:34.811963: step 42640, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 52h:33m:07s remains)
INFO - root - 2017-12-07 18:58:41.477600: step 42650, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 50h:16m:52s remains)
INFO - root - 2017-12-07 18:58:48.202140: step 42660, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 50h:30m:11s remains)
INFO - root - 2017-12-07 18:58:55.143682: step 42670, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 59h:33m:04s remains)
INFO - root - 2017-12-07 18:59:01.927184: step 42680, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 59h:43m:49s remains)
INFO - root - 2017-12-07 18:59:08.742331: step 42690, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 56h:13m:23s remains)
INFO - root - 2017-12-07 18:59:15.320351: step 42700, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 51h:15m:26s remains)
2017-12-07 18:59:16.051940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2298889 -4.2016482 -4.177453 -4.1671643 -4.1809263 -4.2069197 -4.2220254 -4.2237983 -4.2252884 -4.226624 -4.2231603 -4.2061663 -4.1804318 -4.165607 -4.1833549][-4.2245646 -4.1845593 -4.1600919 -4.1634731 -4.1928239 -4.229332 -4.2496281 -4.2505765 -4.2418032 -4.22925 -4.2160468 -4.2010326 -4.1812553 -4.1748118 -4.1945329][-4.212009 -4.1738281 -4.154356 -4.16514 -4.2012043 -4.2372031 -4.2603421 -4.2588215 -4.2403026 -4.219914 -4.2086306 -4.2050819 -4.1967783 -4.191144 -4.2049079][-4.2014794 -4.1757894 -4.167212 -4.1801686 -4.2108378 -4.234303 -4.2500629 -4.24419 -4.2248907 -4.2090788 -4.2106876 -4.2235074 -4.2251153 -4.2158418 -4.2163033][-4.2028561 -4.1891923 -4.1834936 -4.1864967 -4.1986871 -4.2073231 -4.2135463 -4.2027345 -4.1862879 -4.1826882 -4.2010593 -4.2250986 -4.2287259 -4.2147317 -4.2080536][-4.2019086 -4.19158 -4.1777029 -4.1670237 -4.1589594 -4.1529784 -4.1462426 -4.1326337 -4.12498 -4.1381922 -4.17248 -4.2057166 -4.2061825 -4.1895695 -4.1817875][-4.1957264 -4.1855459 -4.1676617 -4.1458111 -4.1259317 -4.1120629 -4.0950055 -4.0858245 -4.0908132 -4.1173778 -4.1554108 -4.18561 -4.1826344 -4.1640639 -4.1514883][-4.20172 -4.18932 -4.1717386 -4.15049 -4.1291161 -4.1183128 -4.1075506 -4.1071062 -4.1159873 -4.1369886 -4.1617613 -4.1741667 -4.1605167 -4.1370111 -4.1215219][-4.2142611 -4.19851 -4.1810369 -4.1660867 -4.1528835 -4.1482611 -4.1482024 -4.1529818 -4.159359 -4.1620364 -4.1617169 -4.1535587 -4.1340609 -4.1158848 -4.1077991][-4.2263613 -4.2113924 -4.2016063 -4.1961703 -4.1925068 -4.1906581 -4.18886 -4.1880212 -4.1810346 -4.16446 -4.1421862 -4.1218338 -4.1088696 -4.1105838 -4.12005][-4.2253776 -4.2162833 -4.2159729 -4.2191353 -4.2247448 -4.2230639 -4.2151556 -4.2068582 -4.1864462 -4.1550784 -4.1210279 -4.1020012 -4.1028438 -4.1197639 -4.1382017][-4.2161412 -4.2143044 -4.220778 -4.2274494 -4.2366018 -4.2365727 -4.2255745 -4.2087665 -4.1791115 -4.1418414 -4.1087489 -4.0990868 -4.1074824 -4.1247611 -4.1427488][-4.2108421 -4.2157335 -4.2263284 -4.2354932 -4.2416687 -4.23573 -4.2180634 -4.1948071 -4.1649647 -4.1351104 -4.1126585 -4.1079512 -4.1126246 -4.121387 -4.1373062][-4.2156873 -4.2232537 -4.2368679 -4.2449112 -4.2440505 -4.228137 -4.202651 -4.1781373 -4.1546707 -4.1394897 -4.1307478 -4.1282139 -4.1276121 -4.1282964 -4.1349883][-4.222579 -4.2353005 -4.2491755 -4.25397 -4.2456884 -4.2221055 -4.1946039 -4.175869 -4.1670804 -4.1670971 -4.1684446 -4.168489 -4.1643548 -4.1558166 -4.1485095]]...]
INFO - root - 2017-12-07 18:59:22.907807: step 42710, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 55h:33m:42s remains)
INFO - root - 2017-12-07 18:59:29.505549: step 42720, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 49h:53m:42s remains)
INFO - root - 2017-12-07 18:59:36.429297: step 42730, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.658 sec/batch; 52h:56m:34s remains)
INFO - root - 2017-12-07 18:59:43.208782: step 42740, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 57h:03m:12s remains)
INFO - root - 2017-12-07 18:59:50.009890: step 42750, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 59h:25m:30s remains)
INFO - root - 2017-12-07 18:59:56.827471: step 42760, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 52h:45m:47s remains)
INFO - root - 2017-12-07 19:00:03.538867: step 42770, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 49h:58m:18s remains)
INFO - root - 2017-12-07 19:00:10.281616: step 42780, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 50h:15m:13s remains)
INFO - root - 2017-12-07 19:00:17.136598: step 42790, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.756 sec/batch; 60h:50m:40s remains)
INFO - root - 2017-12-07 19:00:23.791116: step 42800, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 57h:02m:42s remains)
2017-12-07 19:00:24.511889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2089181 -4.2093587 -4.2067113 -4.1967149 -4.1807022 -4.1604691 -4.1502328 -4.1563854 -4.1699634 -4.17579 -4.1744542 -4.1768274 -4.1816087 -4.1872463 -4.1987977][-4.2328925 -4.2350478 -4.2365351 -4.2295666 -4.2169442 -4.1981196 -4.1782427 -4.1621981 -4.1533141 -4.1438808 -4.135036 -4.138237 -4.1495724 -4.1649528 -4.1869259][-4.2461 -4.2477765 -4.2531157 -4.2521453 -4.2402487 -4.2174993 -4.1829066 -4.1442142 -4.1127214 -4.0864406 -4.0698647 -4.077301 -4.1012378 -4.1309714 -4.1647344][-4.2374063 -4.2424712 -4.2505331 -4.2485595 -4.2265639 -4.19069 -4.1406808 -4.0860353 -4.0495448 -4.0290418 -4.0230536 -4.0466647 -4.0842032 -4.1180115 -4.1497078][-4.2199888 -4.2271504 -4.2342277 -4.2272124 -4.1947255 -4.1488957 -4.0937853 -4.0400281 -4.0183363 -4.0224419 -4.0362711 -4.0718813 -4.1105161 -4.1345677 -4.1513128][-4.2138195 -4.2139673 -4.2141771 -4.2016 -4.1697516 -4.1313376 -4.0886612 -4.0503421 -4.0436258 -4.0583234 -4.0746655 -4.1072469 -4.1387482 -4.1539736 -4.1571507][-4.2177858 -4.2064915 -4.1972961 -4.1818242 -4.1580057 -4.1349163 -4.1146245 -4.0987105 -4.1003671 -4.1112595 -4.121572 -4.1466346 -4.1692238 -4.1774859 -4.174572][-4.2192688 -4.1965747 -4.1809988 -4.1684642 -4.1596255 -4.1598449 -4.1600437 -4.1573529 -4.1599364 -4.1639485 -4.1684251 -4.1856046 -4.2003369 -4.205349 -4.2023411][-4.22335 -4.1985826 -4.1857033 -4.1809525 -4.1851215 -4.1984305 -4.2087297 -4.2087483 -4.20632 -4.2059288 -4.2069197 -4.2195916 -4.2309427 -4.23631 -4.2337766][-4.2257705 -4.2079115 -4.2064705 -4.2111797 -4.2187662 -4.2324762 -4.2436652 -4.2442307 -4.2406826 -4.2417216 -4.2433796 -4.2523017 -4.2622747 -4.2662821 -4.2623572][-4.2171426 -4.2132173 -4.2230825 -4.2343974 -4.2437139 -4.2566185 -4.2685437 -4.2714138 -4.2713852 -4.2765589 -4.2790341 -4.2843575 -4.28916 -4.2897491 -4.2833266][-4.2078185 -4.2191072 -4.2388678 -4.2598224 -4.2751822 -4.2900491 -4.3033257 -4.3055744 -4.3035636 -4.3052416 -4.3044281 -4.3051639 -4.3033795 -4.3005595 -4.2922778][-4.2158318 -4.236536 -4.2613645 -4.2881966 -4.3077779 -4.3234835 -4.3331494 -4.3310509 -4.3241816 -4.3186021 -4.3134913 -4.3105273 -4.306705 -4.3034434 -4.2963204][-4.2251921 -4.2487369 -4.2750869 -4.3016739 -4.3213511 -4.3350878 -4.3403788 -4.3340964 -4.3239622 -4.3153658 -4.3090391 -4.3053102 -4.3022056 -4.2996683 -4.2939873][-4.2270379 -4.2480779 -4.2734075 -4.2984347 -4.3165207 -4.3288708 -4.3333497 -4.3275332 -4.3193169 -4.3123007 -4.3051109 -4.2998891 -4.2957339 -4.2931585 -4.2889423]]...]
INFO - root - 2017-12-07 19:00:31.301644: step 42810, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 54h:14m:08s remains)
INFO - root - 2017-12-07 19:00:38.262183: step 42820, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.804 sec/batch; 64h:43m:59s remains)
INFO - root - 2017-12-07 19:00:44.925929: step 42830, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 52h:24m:17s remains)
INFO - root - 2017-12-07 19:00:51.748051: step 42840, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 51h:54m:25s remains)
INFO - root - 2017-12-07 19:00:58.532254: step 42850, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 51h:20m:41s remains)
INFO - root - 2017-12-07 19:01:05.415975: step 42860, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 58h:19m:58s remains)
INFO - root - 2017-12-07 19:01:12.197290: step 42870, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 57h:01m:03s remains)
INFO - root - 2017-12-07 19:01:19: step 42880, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 55h:42m:33s remains)
INFO - root - 2017-12-07 19:01:25.828182: step 42890, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 51h:10m:14s remains)
INFO - root - 2017-12-07 19:01:32.550057: step 42900, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 53h:14m:32s remains)
2017-12-07 19:01:33.302411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2281971 -4.2274537 -4.2373705 -4.2495122 -4.2623911 -4.2764716 -4.2786183 -4.282145 -4.2856131 -4.2941108 -4.3051391 -4.31935 -4.3275485 -4.3258061 -4.325191][-4.190104 -4.1882105 -4.2042747 -4.2208486 -4.2370534 -4.2537947 -4.2578216 -4.2619758 -4.268898 -4.2845278 -4.299006 -4.3127441 -4.321918 -4.3217225 -4.3219323][-4.1660762 -4.1599717 -4.1769013 -4.1945572 -4.2118669 -4.2320824 -4.239758 -4.244278 -4.2502604 -4.2669034 -4.2841706 -4.2972302 -4.3081279 -4.3113446 -4.313674][-4.1682181 -4.152523 -4.1574664 -4.1641321 -4.1768484 -4.2010555 -4.213666 -4.2173281 -4.2214432 -4.2380037 -4.2598605 -4.2739925 -4.2865186 -4.2938457 -4.2980123][-4.18186 -4.15849 -4.1426597 -4.1252117 -4.12349 -4.1470795 -4.1660495 -4.1722088 -4.1774082 -4.1971731 -4.2268353 -4.245748 -4.2612057 -4.2728772 -4.2795982][-4.1907997 -4.1676111 -4.1374068 -4.0951185 -4.0683141 -4.0816669 -4.1013761 -4.1125603 -4.1271048 -4.1552477 -4.1934109 -4.2167859 -4.2355762 -4.2529483 -4.2642741][-4.1864772 -4.1738162 -4.148243 -4.1017303 -4.0608835 -4.0554171 -4.0653214 -4.0736413 -4.0933142 -4.1284866 -4.1714869 -4.1991091 -4.22073 -4.2418337 -4.2557077][-4.1862144 -4.1889396 -4.1786776 -4.1491504 -4.115685 -4.0995865 -4.0943322 -4.0916481 -4.1022286 -4.1277628 -4.1653118 -4.1955633 -4.2206693 -4.2436547 -4.2576928][-4.2054958 -4.2184744 -4.220418 -4.2080107 -4.1883836 -4.1718988 -4.1585236 -4.1450953 -4.1399064 -4.1471367 -4.1728349 -4.204484 -4.2339416 -4.2582631 -4.269484][-4.2322903 -4.2478466 -4.2559094 -4.2536926 -4.2446628 -4.2307525 -4.2136731 -4.1921458 -4.1713557 -4.1619129 -4.1770816 -4.2099242 -4.2441525 -4.2714167 -4.2821188][-4.2585845 -4.2719436 -4.282073 -4.2834496 -4.2767305 -4.263875 -4.2464266 -4.221787 -4.193244 -4.1759968 -4.185215 -4.2163234 -4.2522879 -4.2814169 -4.29183][-4.2812877 -4.2912378 -4.2994018 -4.3001785 -4.2952194 -4.2853174 -4.2710137 -4.246973 -4.2166848 -4.1982508 -4.2067189 -4.2348056 -4.2676692 -4.2945304 -4.302824][-4.2949591 -4.3010654 -4.3072405 -4.3087444 -4.306706 -4.3013349 -4.2902431 -4.2668557 -4.238162 -4.2225461 -4.23298 -4.2588391 -4.2867184 -4.3085 -4.31309][-4.295444 -4.2974524 -4.3032718 -4.3077612 -4.3086629 -4.30726 -4.2988925 -4.2776251 -4.2528582 -4.240695 -4.2518888 -4.2749953 -4.2980933 -4.3166227 -4.3188729][-4.287375 -4.2848358 -4.28973 -4.2963305 -4.3001237 -4.3014288 -4.2946882 -4.2766852 -4.2569146 -4.248291 -4.2594709 -4.2802873 -4.3004131 -4.3172956 -4.318975]]...]
INFO - root - 2017-12-07 19:01:40.204602: step 42910, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 56h:10m:21s remains)
INFO - root - 2017-12-07 19:01:47.091898: step 42920, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 52h:33m:03s remains)
INFO - root - 2017-12-07 19:01:53.986613: step 42930, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 57h:56m:37s remains)
INFO - root - 2017-12-07 19:02:00.883715: step 42940, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 58h:24m:06s remains)
INFO - root - 2017-12-07 19:02:07.747824: step 42950, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 54h:14m:39s remains)
INFO - root - 2017-12-07 19:02:14.524401: step 42960, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 52h:20m:31s remains)
INFO - root - 2017-12-07 19:02:21.290832: step 42970, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 52h:21m:41s remains)
INFO - root - 2017-12-07 19:02:28.110522: step 42980, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 58h:00m:33s remains)
INFO - root - 2017-12-07 19:02:34.887871: step 42990, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 57h:03m:53s remains)
INFO - root - 2017-12-07 19:02:41.505109: step 43000, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 53h:56m:06s remains)
2017-12-07 19:02:42.227166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3160558 -4.3018179 -4.2858634 -4.2709465 -4.2664242 -4.2721386 -4.2812977 -4.2855916 -4.2777967 -4.2645111 -4.2457209 -4.2179465 -4.1928468 -4.1886716 -4.2193694][-4.2996993 -4.2860355 -4.2697239 -4.24872 -4.2348557 -4.2332225 -4.2392025 -4.2430196 -4.2372289 -4.2241406 -4.1986251 -4.158206 -4.1221366 -4.1177626 -4.1658344][-4.2782555 -4.26759 -4.2550912 -4.229197 -4.2040377 -4.1902823 -4.1884475 -4.1919947 -4.1941814 -4.1892605 -4.1645966 -4.1182189 -4.0760422 -4.0742373 -4.1356673][-4.2569804 -4.2504749 -4.2406845 -4.2129679 -4.1787863 -4.1470561 -4.1266522 -4.127996 -4.1467772 -4.163094 -4.1501164 -4.1072769 -4.0696821 -4.0766344 -4.1425323][-4.2329869 -4.22561 -4.2112393 -4.1811876 -4.1385689 -4.0861015 -4.0374627 -4.0266218 -4.0674615 -4.1167774 -4.125761 -4.0996466 -4.0805411 -4.1027117 -4.1690574][-4.2079124 -4.1944537 -4.173182 -4.1431112 -4.0954628 -4.0236359 -3.936307 -3.8939116 -3.9526212 -4.0402122 -4.0811477 -4.0850477 -4.0947666 -4.1351857 -4.2002292][-4.185524 -4.1696029 -4.1489038 -4.1223726 -4.0699363 -3.9778121 -3.8555641 -3.7750926 -3.8391383 -3.958813 -4.0380936 -4.0780888 -4.1154175 -4.167685 -4.2290425][-4.183589 -4.1702065 -4.1581478 -4.1368222 -4.0826163 -3.9881294 -3.8661904 -3.7787213 -3.8315289 -3.9479287 -4.041719 -4.0998569 -4.1497169 -4.2018881 -4.2552557][-4.1906962 -4.181982 -4.1819816 -4.1713166 -4.1260314 -4.0507574 -3.9603782 -3.8994479 -3.9333363 -4.0173507 -4.0956244 -4.1493464 -4.1953721 -4.2377729 -4.2790608][-4.1947107 -4.1918921 -4.2006516 -4.2002549 -4.1670485 -4.1141148 -4.0577059 -4.0270658 -4.0511971 -4.1055951 -4.1624813 -4.2043176 -4.2393842 -4.270124 -4.2994223][-4.1910729 -4.1905212 -4.2015467 -4.2083044 -4.191648 -4.1614771 -4.1323919 -4.1215773 -4.1414924 -4.1786323 -4.2191114 -4.2511773 -4.276135 -4.2967176 -4.3162169][-4.1878605 -4.1871061 -4.1937871 -4.2033372 -4.2033854 -4.197094 -4.1925545 -4.1954803 -4.2108064 -4.2361665 -4.2635889 -4.2856641 -4.3025 -4.3159208 -4.3283353][-4.1989746 -4.2003593 -4.20354 -4.2135081 -4.2263236 -4.2390614 -4.2501764 -4.2588568 -4.2682881 -4.2827282 -4.2979927 -4.3107076 -4.3210497 -4.3296309 -4.3374867][-4.2417231 -4.2457089 -4.2504082 -4.2599607 -4.2740889 -4.2878866 -4.2995543 -4.3063455 -4.3105454 -4.3163519 -4.3224683 -4.3281837 -4.3338666 -4.3391442 -4.3438206][-4.2955208 -4.2993226 -4.3037939 -4.3100963 -4.3180275 -4.3249846 -4.3300037 -4.3331175 -4.334918 -4.3368921 -4.3386841 -4.3406572 -4.3433418 -4.3459396 -4.34811]]...]
INFO - root - 2017-12-07 19:02:49.132803: step 43010, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 58h:21m:08s remains)
INFO - root - 2017-12-07 19:02:55.918995: step 43020, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 56h:55m:50s remains)
INFO - root - 2017-12-07 19:03:02.555888: step 43030, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 52h:19m:18s remains)
INFO - root - 2017-12-07 19:03:09.415657: step 43040, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 51h:51m:41s remains)
INFO - root - 2017-12-07 19:03:16.301858: step 43050, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 58h:02m:18s remains)
INFO - root - 2017-12-07 19:03:23.155495: step 43060, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 56h:48m:33s remains)
INFO - root - 2017-12-07 19:03:29.920844: step 43070, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 52h:47m:42s remains)
INFO - root - 2017-12-07 19:03:36.646465: step 43080, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 51h:40m:07s remains)
INFO - root - 2017-12-07 19:03:43.500117: step 43090, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 54h:18m:50s remains)
INFO - root - 2017-12-07 19:03:50.170274: step 43100, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 55h:08m:00s remains)
2017-12-07 19:03:50.905661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1679406 -4.1527162 -4.132195 -4.1262293 -4.1488614 -4.1802378 -4.1986747 -4.2050538 -4.2113056 -4.21748 -4.2198367 -4.2279177 -4.2421103 -4.2401929 -4.2267709][-4.2019124 -4.1809111 -4.1355429 -4.0889606 -4.0762029 -4.0908217 -4.1075358 -4.1247344 -4.1495981 -4.1738386 -4.1873422 -4.2022986 -4.2194543 -4.2184262 -4.2075076][-4.2172556 -4.2068224 -4.1587534 -4.0871339 -4.0266767 -3.9927683 -3.9894269 -4.0171819 -4.062037 -4.104022 -4.129674 -4.1643176 -4.1938396 -4.1941347 -4.18075][-4.2077346 -4.2179484 -4.1817241 -4.1085415 -4.023252 -3.9424458 -3.8960147 -3.9213407 -3.9832759 -4.0352173 -4.0741467 -4.1318707 -4.1785293 -4.1875224 -4.17661][-4.1870728 -4.2157936 -4.1982541 -4.138104 -4.0593572 -3.9670763 -3.8907096 -3.8904181 -3.9458964 -3.9970024 -4.0448947 -4.1187239 -4.1789136 -4.1987295 -4.1951056][-4.1767039 -4.2134266 -4.2106581 -4.1655445 -4.1095223 -4.0380192 -3.9669302 -3.9463158 -3.9662139 -3.9930506 -4.0383849 -4.1153922 -4.1811724 -4.2131782 -4.2160888][-4.189383 -4.2248449 -4.2248735 -4.1897073 -4.1531229 -4.1075878 -4.0580611 -4.0284429 -4.014626 -4.0160661 -4.0507045 -4.1174622 -4.1785641 -4.2145505 -4.2250018][-4.2149949 -4.2384253 -4.2350044 -4.2069054 -4.1812267 -4.1554356 -4.1302538 -4.1020427 -4.0719638 -4.0530634 -4.0703745 -4.121295 -4.173521 -4.2099991 -4.2241611][-4.2335591 -4.2444758 -4.2380285 -4.2165008 -4.195838 -4.1815786 -4.1712165 -4.1453996 -4.108398 -4.0744929 -4.0687251 -4.1022911 -4.1497717 -4.19108 -4.2124476][-4.2470617 -4.2503352 -4.243897 -4.2270517 -4.2091188 -4.20237 -4.1967082 -4.1707191 -4.1315045 -4.08176 -4.0488553 -4.0642328 -4.1094737 -4.1601019 -4.1893387][-4.265378 -4.2680821 -4.265286 -4.2538853 -4.239418 -4.2310543 -4.2239003 -4.1984143 -4.1542916 -4.0895052 -4.0329103 -4.0278707 -4.0666175 -4.12603 -4.1628375][-4.2866235 -4.2904224 -4.2914929 -4.2821136 -4.2686405 -4.2601056 -4.2559094 -4.2352276 -4.18888 -4.11299 -4.0494881 -4.0311365 -4.0538216 -4.1049085 -4.1417437][-4.3008618 -4.3043461 -4.3048897 -4.2957168 -4.2822118 -4.2762108 -4.2752228 -4.2631431 -4.2215147 -4.1512561 -4.0978885 -4.07694 -4.0804257 -4.1072283 -4.1337643][-4.3023257 -4.3035131 -4.2962489 -4.2824712 -4.2698183 -4.2730603 -4.2784081 -4.2725182 -4.2395587 -4.18175 -4.1448083 -4.1268916 -4.1196284 -4.1247849 -4.1424928][-4.2968373 -4.2931428 -4.2785659 -4.2600341 -4.2467904 -4.2597795 -4.2731862 -4.271359 -4.24567 -4.2058344 -4.1855092 -4.1696959 -4.1550121 -4.146296 -4.1578908]]...]
INFO - root - 2017-12-07 19:03:57.675969: step 43110, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 51h:16m:33s remains)
INFO - root - 2017-12-07 19:04:04.617732: step 43120, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 53h:42m:33s remains)
INFO - root - 2017-12-07 19:04:11.583990: step 43130, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 58h:45m:34s remains)
INFO - root - 2017-12-07 19:04:18.406755: step 43140, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 55h:34m:22s remains)
INFO - root - 2017-12-07 19:04:25.242546: step 43150, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 53h:01m:25s remains)
INFO - root - 2017-12-07 19:04:32.060761: step 43160, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 53h:33m:38s remains)
INFO - root - 2017-12-07 19:04:38.867161: step 43170, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 57h:54m:14s remains)
INFO - root - 2017-12-07 19:04:45.733319: step 43180, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 57h:18m:34s remains)
INFO - root - 2017-12-07 19:04:52.612993: step 43190, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 56h:58m:37s remains)
INFO - root - 2017-12-07 19:04:59.194781: step 43200, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 50h:39m:02s remains)
2017-12-07 19:04:59.950060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2050619 -4.2124033 -4.2067018 -4.2006035 -4.2018738 -4.2112341 -4.2279124 -4.2479906 -4.2578945 -4.2545843 -4.2590337 -4.2655096 -4.2697358 -4.2587476 -4.235043][-4.2028694 -4.2226596 -4.2198014 -4.2067637 -4.2024155 -4.2070589 -4.2222881 -4.2490053 -4.2658043 -4.265481 -4.2702861 -4.2723722 -4.2703967 -4.2586374 -4.2426348][-4.2212362 -4.2338762 -4.2224703 -4.200099 -4.1862845 -4.1863456 -4.2041793 -4.2381654 -4.2594519 -4.2689543 -4.2782474 -4.2770066 -4.2663589 -4.2530894 -4.2433472][-4.2393918 -4.2350974 -4.2099733 -4.17938 -4.1583061 -4.15432 -4.174932 -4.21036 -4.2369604 -4.2611213 -4.280561 -4.280745 -4.2664256 -4.2531748 -4.2462516][-4.249383 -4.2307434 -4.1970558 -4.1635189 -4.1383376 -4.1297297 -4.1442752 -4.1740141 -4.2055707 -4.2399392 -4.2640581 -4.2711239 -4.2651081 -4.2584782 -4.2539515][-4.2533951 -4.2356706 -4.203815 -4.1702819 -4.1398573 -4.1227283 -4.1261082 -4.1421113 -4.17019 -4.2063718 -4.2343922 -4.2528076 -4.2640595 -4.269084 -4.2647209][-4.2463827 -4.2441297 -4.2253876 -4.1939626 -4.156806 -4.1285429 -4.1166945 -4.1179252 -4.1424584 -4.1780877 -4.208756 -4.2396331 -4.269876 -4.281374 -4.2747498][-4.2245774 -4.2415857 -4.2390151 -4.2132292 -4.1733828 -4.1336942 -4.1097751 -4.1044126 -4.126462 -4.1579719 -4.1882882 -4.228796 -4.2730026 -4.289053 -4.2804303][-4.20605 -4.2326975 -4.24027 -4.2197628 -4.1837773 -4.1446638 -4.1194434 -4.1097984 -4.1224637 -4.1469154 -4.1745315 -4.2203922 -4.2746696 -4.295135 -4.2838469][-4.2138124 -4.2394876 -4.2498231 -4.2334561 -4.2014217 -4.1674528 -4.14546 -4.1352086 -4.13982 -4.1573739 -4.1866426 -4.2325826 -4.2848053 -4.3043165 -4.2914186][-4.2468514 -4.2650995 -4.2738633 -4.2634974 -4.239593 -4.2101789 -4.18818 -4.17686 -4.1759558 -4.1874781 -4.2108755 -4.2475328 -4.2894559 -4.3048811 -4.2950296][-4.2770033 -4.2864151 -4.2944217 -4.289432 -4.2746172 -4.2521849 -4.2293253 -4.2160163 -4.2123427 -4.2170458 -4.2309718 -4.25601 -4.28661 -4.2973671 -4.2924619][-4.2931881 -4.2944536 -4.2985525 -4.2937546 -4.2810369 -4.2619023 -4.2424941 -4.234911 -4.2354612 -4.237752 -4.2443051 -4.2595592 -4.2812967 -4.2870235 -4.28405][-4.2836375 -4.2803454 -4.278511 -4.27361 -4.2628269 -4.2447939 -4.2299986 -4.2316976 -4.2443218 -4.2521167 -4.25453 -4.2592983 -4.2681527 -4.2736826 -4.2779746][-4.2643871 -4.2595844 -4.2558923 -4.2487745 -4.2327685 -4.2126107 -4.2021465 -4.2151151 -4.2395024 -4.2527695 -4.25207 -4.2474165 -4.25045 -4.2599406 -4.2693548]]...]
INFO - root - 2017-12-07 19:05:06.743217: step 43210, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 57h:56m:51s remains)
INFO - root - 2017-12-07 19:05:13.554559: step 43220, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 54h:22m:44s remains)
INFO - root - 2017-12-07 19:05:20.432790: step 43230, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 52h:57m:34s remains)
INFO - root - 2017-12-07 19:05:27.297441: step 43240, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 53h:01m:48s remains)
INFO - root - 2017-12-07 19:05:34.152018: step 43250, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.753 sec/batch; 60h:28m:59s remains)
INFO - root - 2017-12-07 19:05:41.175990: step 43260, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 57h:43m:14s remains)
INFO - root - 2017-12-07 19:05:48.034683: step 43270, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 51h:24m:41s remains)
INFO - root - 2017-12-07 19:05:54.853874: step 43280, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 51h:58m:04s remains)
INFO - root - 2017-12-07 19:06:01.778268: step 43290, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 59h:30m:39s remains)
INFO - root - 2017-12-07 19:06:08.540822: step 43300, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 57h:17m:38s remains)
2017-12-07 19:06:09.295584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2528706 -4.263299 -4.2629824 -4.2485642 -4.2396326 -4.237493 -4.2348127 -4.2300806 -4.22738 -4.2158141 -4.2019458 -4.1903338 -4.166935 -4.142035 -4.1498318][-4.2502718 -4.2729454 -4.2750306 -4.2537332 -4.2336335 -4.2227097 -4.2139649 -4.2048016 -4.1996059 -4.1873131 -4.1724372 -4.1673737 -4.1546135 -4.1349683 -4.1403027][-4.2544575 -4.2850595 -4.2852116 -4.2571335 -4.2275925 -4.2038264 -4.1830487 -4.1719112 -4.1707358 -4.1558256 -4.1421533 -4.1483417 -4.1471119 -4.1316872 -4.1313343][-4.2557106 -4.2911077 -4.2873731 -4.2520885 -4.2141829 -4.1752324 -4.1412158 -4.1326737 -4.143157 -4.133369 -4.1236038 -4.1329494 -4.133028 -4.1208291 -4.124548][-4.2568479 -4.2927771 -4.2862329 -4.2471695 -4.2001448 -4.1420445 -4.0929379 -4.0863786 -4.1127748 -4.1180139 -4.117198 -4.1273293 -4.1178112 -4.0985956 -4.1036587][-4.2626224 -4.2896643 -4.2742572 -4.2275949 -4.1638665 -4.0827618 -4.0177336 -4.0207872 -4.0699091 -4.0985489 -4.1139674 -4.1288714 -4.1111526 -4.0795016 -4.0789213][-4.2579231 -4.2724919 -4.2471342 -4.1906042 -4.1054153 -3.9866009 -3.8978922 -3.9190238 -4.0017624 -4.060431 -4.103055 -4.131989 -4.118279 -4.0803919 -4.0736327][-4.2478642 -4.2549753 -4.2277226 -4.1689253 -4.067121 -3.9189706 -3.8097596 -3.8455424 -3.9510057 -4.029037 -4.0908375 -4.1327009 -4.1297221 -4.0993023 -4.0981407][-4.2430649 -4.2473464 -4.227777 -4.1773806 -4.0830545 -3.947181 -3.8512154 -3.8808374 -3.9649007 -4.0272388 -4.0868464 -4.137537 -4.1510849 -4.1356163 -4.1386194][-4.2324452 -4.2357874 -4.2244806 -4.1921144 -4.129046 -4.0361843 -3.9720743 -3.98303 -4.0232363 -4.0517597 -4.0949373 -4.1435728 -4.1685662 -4.1644006 -4.1670232][-4.2258406 -4.2272773 -4.2235818 -4.2107496 -4.1755443 -4.1187024 -4.076458 -4.0742579 -4.0805674 -4.0807323 -4.1089969 -4.1524863 -4.1803517 -4.1830959 -4.1818938][-4.2250276 -4.2268095 -4.2287107 -4.2277904 -4.2117004 -4.1757545 -4.1420617 -4.1265178 -4.1145282 -4.1045833 -4.1211963 -4.1511588 -4.1808739 -4.1899757 -4.1867242][-4.2192512 -4.2236738 -4.2305708 -4.237505 -4.2367711 -4.2155433 -4.1857142 -4.1587892 -4.133101 -4.1132455 -4.1100416 -4.1240525 -4.1498628 -4.1640882 -4.1616654][-4.2147255 -4.220469 -4.2274237 -4.2409973 -4.2532797 -4.2439508 -4.22188 -4.1956663 -4.159564 -4.1268454 -4.0976815 -4.0897069 -4.1091089 -4.1227865 -4.1166449][-4.2155352 -4.2232432 -4.2270336 -4.2421093 -4.2595148 -4.2567978 -4.2415838 -4.2248297 -4.1956916 -4.1623554 -4.1210728 -4.1007209 -4.1179714 -4.1306233 -4.1183386]]...]
INFO - root - 2017-12-07 19:06:16.070072: step 43310, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.721 sec/batch; 57h:56m:19s remains)
INFO - root - 2017-12-07 19:06:22.861286: step 43320, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 59h:14m:51s remains)
INFO - root - 2017-12-07 19:06:29.528016: step 43330, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 53h:57m:22s remains)
INFO - root - 2017-12-07 19:06:36.086897: step 43340, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 51h:21m:49s remains)
INFO - root - 2017-12-07 19:06:43.009367: step 43350, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 54h:30m:18s remains)
INFO - root - 2017-12-07 19:06:49.846253: step 43360, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 56h:21m:53s remains)
INFO - root - 2017-12-07 19:06:56.614925: step 43370, loss = 2.11, batch loss = 2.05 (11.2 examples/sec; 0.716 sec/batch; 57h:30m:12s remains)
INFO - root - 2017-12-07 19:07:03.395863: step 43380, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 51h:35m:39s remains)
INFO - root - 2017-12-07 19:07:10.174756: step 43390, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 49h:58m:29s remains)
INFO - root - 2017-12-07 19:07:16.749097: step 43400, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 51h:53m:39s remains)
2017-12-07 19:07:17.554140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1950636 -4.167315 -4.1761675 -4.1984138 -4.2118368 -4.2141294 -4.2272882 -4.2531557 -4.2728424 -4.2867746 -4.2947116 -4.2971263 -4.2942514 -4.2868614 -4.2707419][-4.1981072 -4.159729 -4.1598191 -4.1787119 -4.19746 -4.20959 -4.2296329 -4.2577958 -4.2775702 -4.2916012 -4.2990174 -4.2996769 -4.2946372 -4.2899847 -4.27982][-4.2094522 -4.1587892 -4.1424155 -4.1519012 -4.1720495 -4.1971331 -4.2313375 -4.2658796 -4.2874346 -4.3013005 -4.3083391 -4.3085833 -4.3033743 -4.3001804 -4.2944946][-4.228457 -4.1691251 -4.1332841 -4.1244969 -4.1362453 -4.1720734 -4.225327 -4.2734976 -4.298892 -4.3132472 -4.3202362 -4.3183084 -4.3095846 -4.3047233 -4.3016338][-4.2481308 -4.18375 -4.1318736 -4.1021867 -4.0983257 -4.1388893 -4.2101865 -4.2741852 -4.3050685 -4.318747 -4.3227839 -4.3163323 -4.3027925 -4.294611 -4.2921047][-4.2507067 -4.1842241 -4.1199613 -4.0705347 -4.047554 -4.08492 -4.1705928 -4.2537665 -4.2958241 -4.3108339 -4.3111978 -4.301023 -4.283958 -4.2717876 -4.267941][-4.2223668 -4.1594481 -4.0882592 -4.02229 -3.9821544 -4.0113587 -4.1060753 -4.2068381 -4.2649846 -4.2866426 -4.2880006 -4.27625 -4.2568517 -4.239522 -4.2298212][-4.1752391 -4.1191273 -4.0497861 -3.9751225 -3.9229794 -3.9418535 -4.0355949 -4.1466088 -4.2181025 -4.2484407 -4.2562671 -4.2473993 -4.2288465 -4.2091432 -4.194983][-4.1244698 -4.0765886 -4.0162439 -3.9430959 -3.8837502 -3.8872237 -3.9695199 -4.0818214 -4.162415 -4.2027297 -4.2196317 -4.2177553 -4.2031918 -4.1852164 -4.1692009][-4.1042485 -4.0635295 -4.012167 -3.9431098 -3.8769865 -3.8608794 -3.9241519 -4.0275764 -4.1099958 -4.15664 -4.1812525 -4.1860676 -4.1779523 -4.1651139 -4.1503091][-4.13074 -4.0967412 -4.049655 -3.9821618 -3.9079916 -3.8736928 -3.9136894 -3.99878 -4.0737324 -4.1213269 -4.1526494 -4.1643763 -4.164578 -4.1584411 -4.1459408][-4.1850972 -4.1582389 -4.1160984 -4.05384 -3.9816701 -3.9395859 -3.9568307 -4.014503 -4.0727973 -4.115983 -4.1501255 -4.1656284 -4.1672163 -4.1596689 -4.1431174][-4.2502379 -4.2301087 -4.19472 -4.1427565 -4.082202 -4.0445328 -4.0467968 -4.0770631 -4.1147671 -4.147593 -4.17604 -4.1903229 -4.1890178 -4.1731281 -4.1433115][-4.2989516 -4.2843218 -4.257515 -4.218936 -4.1743059 -4.1460409 -4.1414566 -4.1522183 -4.1729841 -4.1951709 -4.2161803 -4.2269044 -4.2199669 -4.1917868 -4.1416969][-4.3283081 -4.3187947 -4.3014174 -4.2760386 -4.2457952 -4.2267962 -4.2212176 -4.2218895 -4.2294178 -4.2422886 -4.2573104 -4.2665467 -4.2566595 -4.2173953 -4.1478534]]...]
INFO - root - 2017-12-07 19:07:24.350971: step 43410, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 54h:27m:00s remains)
INFO - root - 2017-12-07 19:07:31.121911: step 43420, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 53h:20m:41s remains)
INFO - root - 2017-12-07 19:07:37.930568: step 43430, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 55h:40m:33s remains)
INFO - root - 2017-12-07 19:07:44.848635: step 43440, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 58h:21m:46s remains)
INFO - root - 2017-12-07 19:07:51.667737: step 43450, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 53h:48m:56s remains)
INFO - root - 2017-12-07 19:07:58.444876: step 43460, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 50h:46m:01s remains)
INFO - root - 2017-12-07 19:08:05.207780: step 43470, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 51h:35m:27s remains)
INFO - root - 2017-12-07 19:08:11.974795: step 43480, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 53h:26m:45s remains)
INFO - root - 2017-12-07 19:08:18.782486: step 43490, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 58h:09m:30s remains)
INFO - root - 2017-12-07 19:08:25.430177: step 43500, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 55h:11m:46s remains)
2017-12-07 19:08:26.141181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3115144 -4.3011427 -4.2934132 -4.2859859 -4.27664 -4.2712059 -4.2687674 -4.2604942 -4.2501469 -4.2528448 -4.2694178 -4.2891822 -4.3036737 -4.3148575 -4.3210106][-4.3049521 -4.293035 -4.2853289 -4.2758818 -4.2622142 -4.2553263 -4.2541852 -4.2451949 -4.2340655 -4.2359972 -4.2591281 -4.2870536 -4.3067093 -4.317605 -4.3212428][-4.2935872 -4.2763381 -4.2652936 -4.251667 -4.235281 -4.2255321 -4.2199526 -4.2059283 -4.1959434 -4.2035794 -4.2377439 -4.2781415 -4.3070016 -4.3187566 -4.3191428][-4.269846 -4.2417645 -4.2248383 -4.211134 -4.19887 -4.1856213 -4.1677089 -4.1395192 -4.1261349 -4.1459656 -4.1968541 -4.2505045 -4.2913032 -4.3090816 -4.3108492][-4.2281866 -4.187562 -4.1648045 -4.1593819 -4.1558633 -4.1351628 -4.0912333 -4.0312891 -4.0119762 -4.0558748 -4.1343732 -4.2025948 -4.2572742 -4.2878127 -4.2983756][-4.1775036 -4.1274662 -4.1036873 -4.1079054 -4.1072993 -4.0708013 -3.976428 -3.857439 -3.834352 -3.9311397 -4.0577974 -4.1521511 -4.2194529 -4.263598 -4.2854805][-4.1408463 -4.0927396 -4.0724349 -4.0796947 -4.0688081 -4.0091777 -3.8513818 -3.6522486 -3.6366394 -3.8135481 -3.9980173 -4.1189742 -4.1959839 -4.2474813 -4.276227][-4.1350641 -4.1027932 -4.09374 -4.0954123 -4.0696926 -3.9976969 -3.8119295 -3.5917346 -3.6086011 -3.8139403 -4.0039539 -4.1231146 -4.1962252 -4.2428832 -4.2716179][-4.1637692 -4.1568885 -4.1587181 -4.1539164 -4.1238012 -4.0614367 -3.9241586 -3.7877662 -3.8144965 -3.9537153 -4.0871558 -4.1740942 -4.2270169 -4.2584481 -4.27875][-4.20653 -4.2139115 -4.2193012 -4.2138605 -4.1900549 -4.152504 -4.0734177 -4.0093555 -4.0266509 -4.0977364 -4.1748528 -4.2293048 -4.2606368 -4.2782125 -4.2891827][-4.2534523 -4.2640352 -4.2678394 -4.2640162 -4.2499189 -4.2292104 -4.1841311 -4.1481423 -4.154943 -4.1942916 -4.2428441 -4.2763948 -4.2921839 -4.2985044 -4.3025212][-4.2992883 -4.307642 -4.3065434 -4.3027935 -4.293992 -4.2837453 -4.2578778 -4.23313 -4.2343173 -4.260673 -4.293746 -4.3127022 -4.318954 -4.3184175 -4.3179622][-4.3211775 -4.3273988 -4.3237214 -4.3188238 -4.313971 -4.310029 -4.2993317 -4.2859054 -4.2870069 -4.3060346 -4.3255959 -4.3340654 -4.334095 -4.3317361 -4.331243][-4.3195319 -4.3208404 -4.3169951 -4.3120365 -4.3081026 -4.3075366 -4.30577 -4.3000817 -4.3020139 -4.3118486 -4.3204818 -4.3255582 -4.3271322 -4.3288302 -4.3321991][-4.314292 -4.3112822 -4.3072181 -4.3022928 -4.2970114 -4.2938061 -4.292767 -4.2894688 -4.2909555 -4.294796 -4.2996793 -4.3077059 -4.3152862 -4.3223209 -4.3272181]]...]
INFO - root - 2017-12-07 19:08:32.893044: step 43510, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 52h:38m:55s remains)
INFO - root - 2017-12-07 19:08:39.719733: step 43520, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 56h:32m:08s remains)
INFO - root - 2017-12-07 19:08:46.489593: step 43530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 53h:39m:01s remains)
INFO - root - 2017-12-07 19:08:53.347887: step 43540, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 54h:11m:26s remains)
INFO - root - 2017-12-07 19:09:00.227288: step 43550, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 53h:35m:37s remains)
INFO - root - 2017-12-07 19:09:07.203180: step 43560, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.744 sec/batch; 59h:42m:53s remains)
INFO - root - 2017-12-07 19:09:14.126069: step 43570, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 57h:49m:21s remains)
INFO - root - 2017-12-07 19:09:20.915205: step 43580, loss = 2.02, batch loss = 1.96 (13.2 examples/sec; 0.606 sec/batch; 48h:37m:24s remains)
INFO - root - 2017-12-07 19:09:27.713016: step 43590, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 51h:29m:34s remains)
INFO - root - 2017-12-07 19:09:34.373593: step 43600, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 58h:34m:11s remains)
2017-12-07 19:09:35.100334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2802982 -4.2730341 -4.2686348 -4.2604394 -4.2607856 -4.2688808 -4.275722 -4.2830048 -4.295362 -4.3044329 -4.3046527 -4.3028889 -4.3028684 -4.29803 -4.2869349][-4.2478533 -4.2399879 -4.2385216 -4.2393947 -4.2492495 -4.2634335 -4.2693105 -4.2738914 -4.2830272 -4.2877369 -4.2875443 -4.2870183 -4.2891841 -4.2884054 -4.2821608][-4.1950631 -4.188252 -4.195837 -4.212893 -4.2363396 -4.2573662 -4.26584 -4.2690082 -4.2741418 -4.2732191 -4.2706585 -4.2697458 -4.2737122 -4.2750645 -4.2707248][-4.1518283 -4.14648 -4.164104 -4.1950874 -4.2255974 -4.2449093 -4.2525721 -4.2544456 -4.2567964 -4.2544174 -4.2530746 -4.2528949 -4.2566309 -4.25765 -4.2517328][-4.1263208 -4.1200309 -4.138483 -4.1687818 -4.1920371 -4.19939 -4.1977806 -4.1949639 -4.1975808 -4.1992178 -4.20304 -4.2038436 -4.2072845 -4.2099328 -4.2048512][-4.098886 -4.0921173 -4.1032815 -4.1187944 -4.1217632 -4.1079021 -4.0880909 -4.0806227 -4.0883131 -4.09889 -4.1070313 -4.1072149 -4.10736 -4.1118603 -4.1116648][-4.0747914 -4.07004 -4.0733814 -4.0737171 -4.0612111 -4.0319843 -3.9990027 -3.991255 -4.0041838 -4.0212469 -4.0272942 -4.02065 -4.0101666 -4.0098481 -4.0116572][-4.0860677 -4.0828743 -4.08587 -4.0857816 -4.0768323 -4.0593338 -4.0371032 -4.0315228 -4.042582 -4.056818 -4.058126 -4.0469108 -4.0255985 -4.0124278 -4.0115986][-4.1325517 -4.1302152 -4.1335464 -4.1346383 -4.1323371 -4.1315489 -4.1259289 -4.1236649 -4.1294451 -4.1363182 -4.1364336 -4.1297932 -4.1112108 -4.0921164 -4.0840054][-4.1837 -4.1780457 -4.1773715 -4.1755404 -4.1747885 -4.1817303 -4.1860995 -4.1847553 -4.1850204 -4.1874089 -4.1873965 -4.18852 -4.1810174 -4.1640053 -4.1516557][-4.2188482 -4.20869 -4.2022328 -4.1971498 -4.1971555 -4.2064366 -4.2138605 -4.2119741 -4.2089806 -4.2100925 -4.2113934 -4.218 -4.2214003 -4.2112141 -4.1993461][-4.2449627 -4.2343669 -4.2260427 -4.2195253 -4.2179065 -4.2251396 -4.2328439 -4.2317905 -4.2280545 -4.2285147 -4.2302508 -4.2376609 -4.24586 -4.2424932 -4.2350788][-4.2610426 -4.2561274 -4.252408 -4.2483315 -4.2466469 -4.250092 -4.2549782 -4.2544656 -4.2515564 -4.2518406 -4.2524977 -4.2564197 -4.264751 -4.2671742 -4.2643838][-4.2815223 -4.2809558 -4.2806187 -4.2795596 -4.2793446 -4.2818532 -4.2855654 -4.2863293 -4.2838778 -4.2816567 -4.2794795 -4.2798634 -4.2848778 -4.2885666 -4.2853174][-4.3009186 -4.3034158 -4.3048491 -4.3047915 -4.3041697 -4.3056607 -4.3086996 -4.3094497 -4.3062177 -4.3019609 -4.2986841 -4.2981162 -4.3017797 -4.3050728 -4.3000412]]...]
INFO - root - 2017-12-07 19:09:41.904402: step 43610, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 51h:57m:55s remains)
INFO - root - 2017-12-07 19:09:48.703392: step 43620, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 51h:39m:02s remains)
INFO - root - 2017-12-07 19:09:55.598432: step 43630, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 58h:05m:54s remains)
INFO - root - 2017-12-07 19:10:02.509187: step 43640, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 58h:26m:03s remains)
INFO - root - 2017-12-07 19:10:09.009754: step 43650, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 51h:43m:07s remains)
INFO - root - 2017-12-07 19:10:15.834418: step 43660, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 53h:22m:13s remains)
INFO - root - 2017-12-07 19:10:22.666645: step 43670, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 53h:30m:24s remains)
INFO - root - 2017-12-07 19:10:29.519025: step 43680, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 58h:37m:27s remains)
INFO - root - 2017-12-07 19:10:36.468728: step 43690, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 58h:46m:19s remains)
INFO - root - 2017-12-07 19:10:43.085171: step 43700, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 51h:27m:21s remains)
2017-12-07 19:10:43.863915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1849561 -4.1609917 -4.1319532 -4.1243019 -4.1539192 -4.2032981 -4.2490215 -4.2808943 -4.2962542 -4.3067122 -4.31719 -4.32638 -4.3328977 -4.3365636 -4.3368378][-4.1308846 -4.1073885 -4.0812731 -4.0881262 -4.1373138 -4.1975255 -4.2452521 -4.2721615 -4.2783189 -4.2853403 -4.2976961 -4.3100133 -4.3209486 -4.3272438 -4.3262086][-4.0747051 -4.0736146 -4.0677295 -4.0901227 -4.1492167 -4.205328 -4.2410502 -4.2566824 -4.2501879 -4.2475958 -4.2590251 -4.2768579 -4.2973671 -4.3084693 -4.3064494][-4.0650606 -4.0888538 -4.1029992 -4.1300778 -4.1779404 -4.213254 -4.2277145 -4.2243233 -4.2046809 -4.1951895 -4.2086043 -4.2352886 -4.2640481 -4.2815747 -4.282227][-4.0995579 -4.1295743 -4.1497769 -4.1668406 -4.1906257 -4.2016921 -4.1911573 -4.1630387 -4.1289954 -4.1155758 -4.1320834 -4.1675959 -4.2048769 -4.227407 -4.2317762][-4.1649914 -4.1835289 -4.1912847 -4.1874628 -4.1758475 -4.1500659 -4.1053934 -4.0536995 -4.0187383 -4.014184 -4.0360966 -4.0811691 -4.1300983 -4.1588087 -4.1683969][-4.2276149 -4.23252 -4.2260613 -4.201858 -4.1528521 -4.0799718 -3.9955163 -3.9299717 -3.9146786 -3.9377773 -3.9803629 -4.0427051 -4.103816 -4.13807 -4.1472588][-4.264895 -4.2594128 -4.2423162 -4.2066 -4.139091 -4.0426373 -3.9433646 -3.8971996 -3.9237175 -3.9772849 -4.0354133 -4.1031218 -4.15763 -4.1828723 -4.1842031][-4.286057 -4.276124 -4.2557459 -4.2223258 -4.16807 -4.0939932 -4.0299234 -4.0178189 -4.05616 -4.10661 -4.1549964 -4.2009139 -4.2303863 -4.2416945 -4.2368965][-4.2975883 -4.2902741 -4.2752624 -4.2513189 -4.2185116 -4.1832242 -4.1604357 -4.1645656 -4.1883168 -4.2209911 -4.2535319 -4.2718468 -4.2794676 -4.2836127 -4.2788715][-4.3072324 -4.3061442 -4.2995958 -4.286243 -4.2649264 -4.2451696 -4.2388921 -4.2487912 -4.2660284 -4.2861562 -4.3033729 -4.3041654 -4.3005028 -4.3020525 -4.2993245][-4.3182564 -4.3227282 -4.3214316 -4.3151712 -4.3004627 -4.2840939 -4.2763624 -4.2800536 -4.2920537 -4.3038392 -4.3114548 -4.3078475 -4.3032742 -4.3056822 -4.3081369][-4.3236718 -4.3286295 -4.3297267 -4.3270903 -4.3168864 -4.298028 -4.2850914 -4.2818737 -4.286252 -4.2929106 -4.2982335 -4.2975745 -4.2977333 -4.3019047 -4.3063297][-4.3112893 -4.3152323 -4.3175697 -4.3170009 -4.3103361 -4.292232 -4.2745528 -4.2645006 -4.2638879 -4.2678094 -4.2731867 -4.2755518 -4.2804303 -4.2844405 -4.2873492][-4.2885513 -4.2922869 -4.296032 -4.2970228 -4.2958717 -4.2824416 -4.2641025 -4.2483568 -4.2394242 -4.2399836 -4.2493892 -4.2581964 -4.2680416 -4.2723255 -4.2757039]]...]
INFO - root - 2017-12-07 19:10:50.602107: step 43710, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 55h:40m:09s remains)
INFO - root - 2017-12-07 19:10:57.291955: step 43720, loss = 2.07, batch loss = 2.01 (13.3 examples/sec; 0.600 sec/batch; 48h:08m:37s remains)
INFO - root - 2017-12-07 19:11:03.977853: step 43730, loss = 2.03, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 50h:28m:12s remains)
INFO - root - 2017-12-07 19:11:10.758531: step 43740, loss = 2.08, batch loss = 2.03 (12.8 examples/sec; 0.625 sec/batch; 50h:07m:23s remains)
INFO - root - 2017-12-07 19:11:17.673587: step 43750, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.742 sec/batch; 59h:32m:29s remains)
INFO - root - 2017-12-07 19:11:24.412844: step 43760, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.704 sec/batch; 56h:26m:04s remains)
INFO - root - 2017-12-07 19:11:31.284164: step 43770, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 54h:55m:00s remains)
INFO - root - 2017-12-07 19:11:38.053116: step 43780, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 52h:26m:54s remains)
INFO - root - 2017-12-07 19:11:44.876978: step 43790, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 53h:08m:07s remains)
INFO - root - 2017-12-07 19:11:51.535882: step 43800, loss = 2.06, batch loss = 2.01 (10.7 examples/sec; 0.746 sec/batch; 59h:51m:51s remains)
2017-12-07 19:11:52.325055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3036194 -4.3077388 -4.3097353 -4.3098044 -4.3087883 -4.307292 -4.305552 -4.303997 -4.300271 -4.2958889 -4.2916827 -4.288476 -4.2823658 -4.2606893 -4.2258697][-4.3168678 -4.319602 -4.32017 -4.3196826 -4.3175688 -4.3153577 -4.3141294 -4.3118582 -4.3054428 -4.2987819 -4.2934966 -4.2905784 -4.2845893 -4.2593117 -4.2159486][-4.3296924 -4.3298306 -4.3272042 -4.3229456 -4.317297 -4.3127956 -4.3114758 -4.3090396 -4.2992854 -4.291214 -4.2869 -4.2891088 -4.2921319 -4.2764959 -4.2398582][-4.3350315 -4.3330064 -4.3260341 -4.3157473 -4.3034778 -4.2948136 -4.2928934 -4.2894053 -4.2738543 -4.2659388 -4.2648668 -4.2739429 -4.2896862 -4.2917972 -4.2724924][-4.3284879 -4.3238072 -4.3125105 -4.2956638 -4.2752261 -4.2598119 -4.2578492 -4.2515125 -4.2313695 -4.2239995 -4.2230034 -4.2370882 -4.2658973 -4.2865748 -4.2831349][-4.3136897 -4.3023458 -4.2856951 -4.2641716 -4.2370195 -4.2163358 -4.2125831 -4.1995339 -4.1727796 -4.1658826 -4.1660137 -4.1808228 -4.2161012 -4.2493405 -4.2567792][-4.2885137 -4.2656112 -4.2392654 -4.2100868 -4.170938 -4.1393795 -4.1347146 -4.122467 -4.0957241 -4.0897326 -4.0941324 -4.1142077 -4.1557779 -4.1973825 -4.2100673][-4.2496691 -4.2134552 -4.1726265 -4.1289721 -4.06592 -4.0075932 -4.0013561 -4.0022559 -3.9906366 -4.0007792 -4.0189 -4.0492687 -4.0968976 -4.1476684 -4.1647177][-4.2144132 -4.1682482 -4.11642 -4.056706 -3.9646158 -3.876929 -3.8730752 -3.8952267 -3.9120309 -3.9474061 -3.9885166 -4.0375781 -4.0927691 -4.1425486 -4.1569109][-4.2153888 -4.1713686 -4.1180511 -4.0547452 -3.962503 -3.8776786 -3.8838332 -3.9195051 -3.9522839 -3.9960842 -4.0452785 -4.0985918 -4.1494 -4.1878462 -4.1986461][-4.24168 -4.2120533 -4.171947 -4.1239114 -4.0581436 -4.0039463 -4.0159588 -4.044241 -4.0721097 -4.1084681 -4.1511207 -4.1927676 -4.2279339 -4.2539439 -4.2635994][-4.2606521 -4.2481532 -4.2253485 -4.1970496 -4.1629124 -4.1398606 -4.1517015 -4.1713004 -4.1920805 -4.2185035 -4.2458963 -4.2711606 -4.2895932 -4.302074 -4.3077078][-4.2668014 -4.2693453 -4.2627068 -4.2532954 -4.24487 -4.2421813 -4.2520666 -4.2629857 -4.2771831 -4.2935925 -4.3085384 -4.3186741 -4.3234558 -4.3271003 -4.3313074][-4.2678924 -4.2831516 -4.2862782 -4.2852736 -4.28699 -4.2911577 -4.2971978 -4.3023043 -4.3115659 -4.3220029 -4.3322849 -4.3358827 -4.3347144 -4.3330474 -4.33342][-4.2792187 -4.2945261 -4.298563 -4.2991285 -4.3016496 -4.3057094 -4.3063092 -4.3058162 -4.3122978 -4.3230438 -4.3331556 -4.337203 -4.3357949 -4.3319316 -4.3300838]]...]
INFO - root - 2017-12-07 19:11:59.083876: step 43810, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.653 sec/batch; 52h:23m:03s remains)
INFO - root - 2017-12-07 19:12:05.937119: step 43820, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 54h:47m:10s remains)
INFO - root - 2017-12-07 19:12:12.773180: step 43830, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.750 sec/batch; 60h:10m:30s remains)
INFO - root - 2017-12-07 19:12:19.658125: step 43840, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 55h:11m:38s remains)
INFO - root - 2017-12-07 19:12:26.453787: step 43850, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 53h:58m:50s remains)
INFO - root - 2017-12-07 19:12:33.247532: step 43860, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 50h:24m:25s remains)
INFO - root - 2017-12-07 19:12:40.021714: step 43870, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 53h:33m:10s remains)
INFO - root - 2017-12-07 19:12:46.813006: step 43880, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 58h:41m:48s remains)
INFO - root - 2017-12-07 19:12:53.690289: step 43890, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 55h:01m:53s remains)
INFO - root - 2017-12-07 19:13:00.246747: step 43900, loss = 2.08, batch loss = 2.02 (13.2 examples/sec; 0.607 sec/batch; 48h:40m:26s remains)
2017-12-07 19:13:00.948528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2234287 -4.2267542 -4.2243533 -4.2145333 -4.2056613 -4.2021041 -4.2061286 -4.2190313 -4.2367682 -4.2526832 -4.2594867 -4.266645 -4.27067 -4.2656174 -4.2492743][-4.2243924 -4.2278857 -4.2266445 -4.2187829 -4.2099452 -4.2042861 -4.2034197 -4.2129211 -4.2320809 -4.2510557 -4.2584443 -4.2655997 -4.26961 -4.2643266 -4.2474966][-4.2249527 -4.2303128 -4.2323923 -4.228868 -4.2198462 -4.2095265 -4.2009373 -4.2049837 -4.224299 -4.2464328 -4.2570057 -4.2662873 -4.2713289 -4.2657223 -4.2474289][-4.2233582 -4.2320189 -4.2376647 -4.2385554 -4.2290978 -4.2144737 -4.198576 -4.1970191 -4.2151465 -4.2395058 -4.2537193 -4.2662659 -4.273313 -4.2686186 -4.2489667][-4.2129712 -4.2263422 -4.2374249 -4.244751 -4.2371793 -4.2199674 -4.1973634 -4.1894512 -4.2047329 -4.230515 -4.2487292 -4.26537 -4.2753005 -4.2717514 -4.2508273][-4.195806 -4.21387 -4.23072 -4.2457581 -4.2428055 -4.2254119 -4.1972375 -4.1823115 -4.1937561 -4.2203197 -4.2425489 -4.2633777 -4.2760472 -4.2727137 -4.2507076][-4.1804709 -4.202157 -4.2240815 -4.2452164 -4.2469225 -4.2305923 -4.1992812 -4.1782579 -4.1850514 -4.2111931 -4.2363682 -4.2602878 -4.2748957 -4.2716703 -4.249167][-4.1694307 -4.1927295 -4.2176528 -4.2413483 -4.246736 -4.2326345 -4.2002282 -4.1747842 -4.17724 -4.2026687 -4.230248 -4.2564487 -4.2723312 -4.2691569 -4.2465773][-4.1641912 -4.1872296 -4.2126045 -4.236218 -4.2432652 -4.2319956 -4.2006059 -4.1723471 -4.170897 -4.1949739 -4.2243772 -4.2524762 -4.2696676 -4.267066 -4.2456908][-4.15823 -4.1799383 -4.2046542 -4.2276144 -4.2369938 -4.2292762 -4.2005596 -4.1715183 -4.167079 -4.1892757 -4.2194571 -4.2485962 -4.2672739 -4.2665429 -4.24785][-4.1550751 -4.1746426 -4.199523 -4.2224479 -4.2340465 -4.2294078 -4.2039623 -4.1759305 -4.1697445 -4.1893473 -4.2184472 -4.2466273 -4.26526 -4.2666082 -4.2512546][-4.1591444 -4.1764088 -4.2012281 -4.2236676 -4.2363338 -4.2335682 -4.2115655 -4.1863141 -4.1797886 -4.1968822 -4.2230906 -4.2479391 -4.2644906 -4.2670417 -4.2548509][-4.168047 -4.182807 -4.2063541 -4.2277079 -4.2405968 -4.2389479 -4.2202988 -4.1990767 -4.193459 -4.2081876 -4.2307973 -4.2515965 -4.2652006 -4.2679825 -4.2581315][-4.1796103 -4.1917858 -4.212904 -4.2327828 -4.2453852 -4.2449169 -4.2293611 -4.2119222 -4.2073841 -4.2197933 -4.2388034 -4.2561278 -4.2672529 -4.2696829 -4.261744][-4.19057 -4.1997442 -4.2175994 -4.2356777 -4.2481818 -4.2491055 -4.23674 -4.2227054 -4.2193351 -4.22956 -4.2453651 -4.2600603 -4.2694211 -4.2715654 -4.2653408]]...]
INFO - root - 2017-12-07 19:13:07.856171: step 43910, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.758 sec/batch; 60h:43m:32s remains)
INFO - root - 2017-12-07 19:13:14.637220: step 43920, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 52h:49m:12s remains)
INFO - root - 2017-12-07 19:13:21.295633: step 43930, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 50h:05m:49s remains)
INFO - root - 2017-12-07 19:13:28.074999: step 43940, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 53h:53m:22s remains)
INFO - root - 2017-12-07 19:13:34.855832: step 43950, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.727 sec/batch; 58h:15m:45s remains)
INFO - root - 2017-12-07 19:13:41.488173: step 43960, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 51h:55m:02s remains)
INFO - root - 2017-12-07 19:13:48.281547: step 43970, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 51h:10m:46s remains)
INFO - root - 2017-12-07 19:13:55.092171: step 43980, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 54h:11m:20s remains)
INFO - root - 2017-12-07 19:14:02.015847: step 43990, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 56h:56m:26s remains)
INFO - root - 2017-12-07 19:14:08.677631: step 44000, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 56h:03m:52s remains)
2017-12-07 19:14:09.435923: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2014065 -4.1786332 -4.1818051 -4.2012949 -4.2075958 -4.2049637 -4.2022204 -4.2033358 -4.2057528 -4.2060628 -4.2087436 -4.2111526 -4.2118382 -4.2105937 -4.2132235][-4.1830258 -4.1702809 -4.1844254 -4.2150087 -4.2224207 -4.21479 -4.2002292 -4.1888013 -4.1800246 -4.1707268 -4.1687584 -4.1752429 -4.1781864 -4.1822333 -4.1877689][-4.1334605 -4.1266112 -4.1548295 -4.1925607 -4.2043085 -4.1964579 -4.17894 -4.164115 -4.1496406 -4.1426482 -4.1423182 -4.1553164 -4.1601396 -4.1584396 -4.161222][-4.0797315 -4.0744996 -4.1095238 -4.1468897 -4.1601338 -4.1512 -4.13221 -4.1237621 -4.1151495 -4.1201906 -4.1217418 -4.1361914 -4.1375513 -4.1216683 -4.1190271][-4.0235004 -4.0249581 -4.06758 -4.105751 -4.1143055 -4.0911202 -4.05942 -4.066102 -4.0770712 -4.0996552 -4.1106477 -4.1281943 -4.11802 -4.0848117 -4.074038][-3.9561656 -3.961261 -4.0084052 -4.04852 -4.0470839 -3.9956849 -3.9458811 -3.9821086 -4.0312448 -4.0849457 -4.1123972 -4.1337695 -4.1171818 -4.0717239 -4.046205][-3.8993225 -3.9096711 -3.9526544 -3.9807734 -3.9539912 -3.8596725 -3.7903562 -3.871366 -3.9770594 -4.0689554 -4.1130233 -4.1355991 -4.1243505 -4.0732064 -4.0290127][-3.8847947 -3.9018207 -3.936595 -3.944324 -3.8975692 -3.7869759 -3.7212763 -3.8303015 -3.9633098 -4.06552 -4.107192 -4.127882 -4.1268864 -4.0826793 -4.0321779][-3.9313614 -3.936831 -3.9639761 -3.9726868 -3.9368958 -3.8592968 -3.8335273 -3.921015 -4.020896 -4.0954366 -4.1226859 -4.1376071 -4.1387625 -4.1036162 -4.0600891][-3.9992795 -3.9908404 -4.0064516 -4.0177431 -4.0011377 -3.9576209 -3.9568269 -4.0165939 -4.082459 -4.1275649 -4.1482692 -4.1634049 -4.1630163 -4.1334863 -4.0952168][-4.0795231 -4.06187 -4.0636873 -4.0669546 -4.0655546 -4.0517969 -4.0607777 -4.097446 -4.1334414 -4.1567388 -4.1691051 -4.18824 -4.1898732 -4.1653275 -4.1313338][-4.1650853 -4.1455975 -4.1396646 -4.1398754 -4.14831 -4.1514325 -4.1567974 -4.1685948 -4.1724467 -4.166245 -4.1629705 -4.1765809 -4.1818852 -4.1675491 -4.146172][-4.2377915 -4.2236338 -4.21273 -4.2095881 -4.2165737 -4.2176905 -4.2154064 -4.2116 -4.1918459 -4.1596284 -4.1373587 -4.1418366 -4.1489372 -4.1460729 -4.1388655][-4.2805648 -4.2724071 -4.2651482 -4.2594514 -4.259356 -4.2544494 -4.2475481 -4.2325807 -4.2032943 -4.16078 -4.1257944 -4.1223316 -4.134655 -4.1411719 -4.1383228][-4.2912831 -4.2864523 -4.2822323 -4.27379 -4.2685876 -4.2644019 -4.2591667 -4.2424135 -4.2153177 -4.174943 -4.1377559 -4.1291308 -4.1389403 -4.1442642 -4.1427116]]...]
INFO - root - 2017-12-07 19:14:16.303950: step 44010, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 59h:23m:21s remains)
INFO - root - 2017-12-07 19:14:23.118757: step 44020, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 56h:49m:52s remains)
INFO - root - 2017-12-07 19:14:29.883325: step 44030, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 52h:06m:24s remains)
INFO - root - 2017-12-07 19:14:36.609920: step 44040, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 53h:05m:58s remains)
INFO - root - 2017-12-07 19:14:43.259370: step 44050, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 51h:44m:14s remains)
INFO - root - 2017-12-07 19:14:50.005185: step 44060, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 56h:02m:36s remains)
INFO - root - 2017-12-07 19:14:56.724067: step 44070, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 56h:24m:11s remains)
INFO - root - 2017-12-07 19:15:03.424557: step 44080, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 58h:35m:49s remains)
INFO - root - 2017-12-07 19:15:10.071284: step 44090, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 50h:52m:43s remains)
INFO - root - 2017-12-07 19:15:16.697978: step 44100, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 50h:27m:57s remains)
2017-12-07 19:15:17.439653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2348433 -4.2205148 -4.1856394 -4.162282 -4.1736221 -4.1898708 -4.18504 -4.1722054 -4.1561456 -4.1399193 -4.1485353 -4.1714158 -4.1880779 -4.2019672 -4.2135606][-4.2166553 -4.2163572 -4.1881618 -4.1586685 -4.1590815 -4.1710129 -4.1704831 -4.156641 -4.1349516 -4.1288891 -4.1588612 -4.19971 -4.2270508 -4.2477827 -4.249557][-4.2067256 -4.2169862 -4.2041392 -4.1838632 -4.1841154 -4.1892695 -4.1794672 -4.1538377 -4.1263914 -4.1339831 -4.1793165 -4.234633 -4.2670336 -4.2794142 -4.2666407][-4.1924667 -4.2088709 -4.212913 -4.2121305 -4.2224588 -4.2220621 -4.2004528 -4.1588488 -4.1121969 -4.12514 -4.1842155 -4.245707 -4.2751961 -4.2693038 -4.2434707][-4.197969 -4.2118134 -4.2270875 -4.2315884 -4.2425246 -4.2353063 -4.1987786 -4.1314669 -4.0570126 -4.0733323 -4.1403346 -4.2002568 -4.2215295 -4.2022572 -4.1742473][-4.2072544 -4.2177186 -4.237215 -4.2370439 -4.2368832 -4.2169614 -4.1518292 -4.0472794 -3.9478951 -3.9808261 -4.0615807 -4.1267376 -4.1491718 -4.1367531 -4.1157103][-4.1812191 -4.1932392 -4.2142487 -4.2098131 -4.1979876 -4.1580291 -4.0653739 -3.9251852 -3.8130529 -3.8750994 -3.979213 -4.05838 -4.0965133 -4.1055079 -4.1005387][-4.1598196 -4.1604857 -4.174582 -4.1646957 -4.1438227 -4.0950108 -4.0035434 -3.8592086 -3.7596793 -3.83726 -3.9499292 -4.0390162 -4.089983 -4.1160369 -4.1235614][-4.174026 -4.1625853 -4.1575341 -4.1394715 -4.1191897 -4.0863585 -4.0310884 -3.9369075 -3.8793621 -3.9361475 -4.0211425 -4.09712 -4.1396422 -4.1599455 -4.1703806][-4.2224803 -4.2024279 -4.1791511 -4.1488118 -4.1297317 -4.1133032 -4.0911984 -4.0438485 -4.0196805 -4.0565228 -4.1158671 -4.1702108 -4.2027512 -4.2132277 -4.2219315][-4.2832937 -4.2534294 -4.2221894 -4.188571 -4.1752319 -4.1701975 -4.1644773 -4.1434045 -4.135221 -4.1561027 -4.1921263 -4.2286983 -4.2524433 -4.2557564 -4.2598777][-4.3216434 -4.2910862 -4.2620568 -4.2323642 -4.2269349 -4.2287493 -4.230082 -4.2251258 -4.2223439 -4.2349648 -4.2548723 -4.276691 -4.2939124 -4.2933812 -4.288795][-4.3293152 -4.3050585 -4.27847 -4.2572651 -4.2595263 -4.2669139 -4.2719812 -4.2720923 -4.2693458 -4.2755237 -4.28719 -4.2980013 -4.3042436 -4.298563 -4.2919717][-4.3153777 -4.2968035 -4.27084 -4.2515969 -4.2569261 -4.2669744 -4.2751141 -4.2779942 -4.2778554 -4.2813621 -4.2885714 -4.2905817 -4.2881942 -4.28065 -4.2749658][-4.3002906 -4.2868161 -4.2646685 -4.2480383 -4.248776 -4.2566514 -4.2666588 -4.271328 -4.2719412 -4.2750082 -4.2786045 -4.2776413 -4.2721744 -4.2631459 -4.256834]]...]
INFO - root - 2017-12-07 19:15:24.199603: step 44110, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 52h:49m:12s remains)
INFO - root - 2017-12-07 19:15:30.842423: step 44120, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 51h:22m:07s remains)
INFO - root - 2017-12-07 19:15:37.624891: step 44130, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 51h:33m:06s remains)
INFO - root - 2017-12-07 19:15:44.510971: step 44140, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 59h:40m:31s remains)
INFO - root - 2017-12-07 19:15:51.402300: step 44150, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 58h:19m:11s remains)
INFO - root - 2017-12-07 19:15:58.141396: step 44160, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 55h:02m:35s remains)
INFO - root - 2017-12-07 19:16:04.933302: step 44170, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 51h:36m:38s remains)
INFO - root - 2017-12-07 19:16:11.710961: step 44180, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 53h:02m:32s remains)
INFO - root - 2017-12-07 19:16:18.540782: step 44190, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 57h:15m:07s remains)
INFO - root - 2017-12-07 19:16:25.153257: step 44200, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 57h:36m:51s remains)
2017-12-07 19:16:25.828529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.333354 -4.3380904 -4.341476 -4.3415508 -4.3396816 -4.3358097 -4.3317971 -4.3286247 -4.3248553 -4.319582 -4.3166614 -4.3191118 -4.3250065 -4.3345437 -4.3443465][-4.3187823 -4.3232193 -4.3266072 -4.3250403 -4.3210869 -4.3158226 -4.3101473 -4.3058224 -4.3023281 -4.2979054 -4.2951407 -4.2983336 -4.3064423 -4.3182745 -4.3307629][-4.29675 -4.3029447 -4.3070984 -4.3027434 -4.2925839 -4.2830796 -4.2735295 -4.2690973 -4.2690167 -4.2698545 -4.2706332 -4.2747006 -4.2861452 -4.3011622 -4.3155379][-4.2641063 -4.2739811 -4.2817349 -4.2752461 -4.2552314 -4.2371707 -4.2226529 -4.2201853 -4.2278433 -4.2385178 -4.2453609 -4.2521305 -4.2675347 -4.2855687 -4.3010149][-4.2208529 -4.2353144 -4.2475672 -4.2381248 -4.2020216 -4.1657882 -4.1445074 -4.1485639 -4.174541 -4.2030935 -4.2200642 -4.2306581 -4.2470808 -4.2674313 -4.2845736][-4.1729693 -4.1898484 -4.205658 -4.1889682 -4.130496 -4.063231 -4.024363 -4.0356994 -4.0894303 -4.1456103 -4.1816492 -4.2013664 -4.2207041 -4.245297 -4.2668915][-4.13031 -4.1504445 -4.1698203 -4.1462393 -4.0626893 -3.9529481 -3.8789246 -3.8948734 -3.9834523 -4.0747395 -4.1371555 -4.1708417 -4.1939487 -4.222043 -4.2503848][-4.0914459 -4.1177654 -4.1437254 -4.119225 -4.0209923 -3.8791516 -3.7654777 -3.7761443 -3.8907073 -4.0104437 -4.0934019 -4.1389832 -4.1665616 -4.1986051 -4.2348895][-4.0614338 -4.089355 -4.1211519 -4.1040549 -4.0139542 -3.878427 -3.7596169 -3.7563882 -3.8617976 -3.980814 -4.0647798 -4.1111436 -4.1416535 -4.1789713 -4.2228413][-4.0482593 -4.07352 -4.1049333 -4.0998721 -4.0419655 -3.9479914 -3.8627269 -3.8497467 -3.9156041 -4.0014458 -4.0671587 -4.108211 -4.1392503 -4.1776104 -4.2239041][-4.0452495 -4.0648508 -4.0932689 -4.1031065 -4.0825305 -4.040184 -3.9983284 -3.9858341 -4.0129375 -4.0578585 -4.0989909 -4.1311927 -4.1610031 -4.1984644 -4.2420888][-4.0625715 -4.0826273 -4.111156 -4.130281 -4.1349063 -4.1281304 -4.1184597 -4.1140189 -4.1221137 -4.1403165 -4.161211 -4.1831007 -4.2075157 -4.2396517 -4.275054][-4.1233268 -4.1413155 -4.1685719 -4.1878333 -4.2000461 -4.2078686 -4.2133136 -4.2163181 -4.2200551 -4.2264776 -4.235044 -4.2470293 -4.2640653 -4.2876296 -4.3113246][-4.2139053 -4.2244391 -4.2428713 -4.255774 -4.2644668 -4.2726483 -4.2814755 -4.2876081 -4.2921233 -4.2953725 -4.2984624 -4.3025346 -4.3104424 -4.32383 -4.3366957][-4.2952242 -4.2985129 -4.3047628 -4.309484 -4.31311 -4.3176422 -4.323482 -4.3279772 -4.3313351 -4.3333397 -4.3348947 -4.3360748 -4.3389244 -4.3450494 -4.3505483]]...]
INFO - root - 2017-12-07 19:16:32.566031: step 44210, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 51h:38m:52s remains)
INFO - root - 2017-12-07 19:16:39.351797: step 44220, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 58h:42m:02s remains)
INFO - root - 2017-12-07 19:16:46.144838: step 44230, loss = 2.04, batch loss = 1.99 (10.4 examples/sec; 0.771 sec/batch; 61h:42m:09s remains)
INFO - root - 2017-12-07 19:16:52.859745: step 44240, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 53h:12m:16s remains)
INFO - root - 2017-12-07 19:16:59.593282: step 44250, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 51h:36m:48s remains)
INFO - root - 2017-12-07 19:17:06.291080: step 44260, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 52h:31m:01s remains)
INFO - root - 2017-12-07 19:17:12.849742: step 44270, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 53h:26m:26s remains)
INFO - root - 2017-12-07 19:17:19.445811: step 44280, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 53h:46m:55s remains)
INFO - root - 2017-12-07 19:17:26.295748: step 44290, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.640 sec/batch; 51h:12m:18s remains)
INFO - root - 2017-12-07 19:17:32.873319: step 44300, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 50h:42m:20s remains)
2017-12-07 19:17:33.637176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3377209 -4.338654 -4.3360257 -4.331213 -4.3276005 -4.3266153 -4.3271561 -4.329565 -4.3323579 -4.3346739 -4.3367205 -4.3380494 -4.338809 -4.3385148 -4.3375297][-4.3354149 -4.3345318 -4.3275537 -4.3180981 -4.3115158 -4.309227 -4.3087916 -4.3124194 -4.31866 -4.3252807 -4.3308277 -4.3332148 -4.3327737 -4.3298144 -4.3263726][-4.3235164 -4.3175545 -4.306179 -4.2924795 -4.2828426 -4.2778354 -4.275126 -4.2789049 -4.2898407 -4.3032022 -4.3132949 -4.3164878 -4.3132415 -4.3064404 -4.3014007][-4.2861695 -4.2722149 -4.2550182 -4.2349091 -4.220088 -4.2119517 -4.20924 -4.2142868 -4.233007 -4.255671 -4.27026 -4.2734594 -4.2659974 -4.2570739 -4.2568][-4.2326031 -4.2105532 -4.1869841 -4.1580415 -4.1326828 -4.118238 -4.1162567 -4.1242771 -4.1508708 -4.1817317 -4.2013731 -4.2065625 -4.1962705 -4.1889529 -4.199472][-4.1791081 -4.1534891 -4.1267986 -4.0932779 -4.0597029 -4.0377364 -4.0331078 -4.0422597 -4.0743184 -4.1091971 -4.1328416 -4.1437783 -4.1353 -4.1314211 -4.1523604][-4.1211042 -4.0985146 -4.0788589 -4.0550508 -4.0287242 -4.0071325 -3.9976315 -4.0016918 -4.0303693 -4.0618629 -4.0865517 -4.1029749 -4.09956 -4.1008897 -4.127264][-4.0636373 -4.0492744 -4.043323 -4.0381036 -4.0276494 -4.011961 -4.0007372 -3.9998534 -4.02077 -4.0442958 -4.0688238 -4.0883818 -4.0923452 -4.0983238 -4.1236391][-4.0467839 -4.0440216 -4.0500007 -4.0596061 -4.0597129 -4.0479708 -4.0381308 -4.0381951 -4.0528517 -4.0710373 -4.091476 -4.1093087 -4.1177373 -4.1232524 -4.1417189][-4.0697293 -4.0733018 -4.0859866 -4.1044445 -4.1086435 -4.0997152 -4.0932541 -4.0975885 -4.1106405 -4.1259642 -4.1427221 -4.1571851 -4.1650915 -4.1653166 -4.171865][-4.1125412 -4.1185074 -4.1337123 -4.1535215 -4.1583452 -4.1511106 -4.14751 -4.15295 -4.1643205 -4.1777415 -4.1909132 -4.2006974 -4.2074242 -4.2036033 -4.2015996][-4.1577682 -4.1628695 -4.1761189 -4.1921182 -4.1959033 -4.1930532 -4.1917458 -4.1949091 -4.1998267 -4.2068071 -4.2123113 -4.2166486 -4.2243819 -4.2208357 -4.2156949][-4.194788 -4.1994939 -4.2119422 -4.22253 -4.2231488 -4.2216396 -4.2214689 -4.2215176 -4.2187333 -4.2186317 -4.2159271 -4.2155833 -4.2234125 -4.2220321 -4.216558][-4.226707 -4.2321677 -4.242156 -4.2469282 -4.2445669 -4.241612 -4.2382755 -4.2340431 -4.2288074 -4.2267804 -4.2187653 -4.2143412 -4.220973 -4.2194004 -4.2135673][-4.2443089 -4.2514668 -4.2595034 -4.2588477 -4.2513289 -4.2438025 -4.2375879 -4.2344294 -4.2339234 -4.2329435 -4.2210412 -4.2120476 -4.2147746 -4.21108 -4.2057405]]...]
INFO - root - 2017-12-07 19:17:40.363541: step 44310, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 52h:53m:11s remains)
INFO - root - 2017-12-07 19:17:47.120480: step 44320, loss = 2.03, batch loss = 1.97 (12.7 examples/sec; 0.631 sec/batch; 50h:29m:32s remains)
INFO - root - 2017-12-07 19:17:53.826186: step 44330, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 51h:21m:48s remains)
INFO - root - 2017-12-07 19:18:00.641700: step 44340, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 59h:30m:03s remains)
INFO - root - 2017-12-07 19:18:07.376317: step 44350, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 56h:49m:12s remains)
INFO - root - 2017-12-07 19:18:14.093570: step 44360, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 52h:43m:59s remains)
INFO - root - 2017-12-07 19:18:20.815071: step 44370, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 51h:33m:03s remains)
INFO - root - 2017-12-07 19:18:27.586965: step 44380, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 50h:48m:04s remains)
INFO - root - 2017-12-07 19:18:34.423643: step 44390, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.775 sec/batch; 62h:03m:47s remains)
INFO - root - 2017-12-07 19:18:40.996357: step 44400, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 59h:25m:19s remains)
2017-12-07 19:18:41.668736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2855005 -4.2854776 -4.291543 -4.3088303 -4.3242593 -4.3316064 -4.3310165 -4.32837 -4.3220057 -4.3171754 -4.3104997 -4.2985344 -4.2818656 -4.2757339 -4.2827649][-4.2510853 -4.2544956 -4.2624874 -4.2793 -4.2925076 -4.2975049 -4.2932777 -4.2875013 -4.2805948 -4.2793241 -4.2775 -4.2647791 -4.2442203 -4.2368007 -4.243988][-4.2318888 -4.237165 -4.245616 -4.2554989 -4.2579904 -4.2545013 -4.2425275 -4.2323756 -4.2295804 -4.239996 -4.2485285 -4.2410827 -4.2230325 -4.216363 -4.2218752][-4.2220492 -4.2283459 -4.2341828 -4.2279286 -4.207983 -4.1886826 -4.1649661 -4.1511946 -4.157743 -4.1867189 -4.2122993 -4.2160244 -4.2053375 -4.2017403 -4.2059784][-4.2111969 -4.2124596 -4.2062287 -4.1742873 -4.1279335 -4.0885339 -4.0472789 -4.0258207 -4.0445018 -4.1067429 -4.1623931 -4.185287 -4.1834726 -4.1844687 -4.1922383][-4.1973572 -4.1852403 -4.1605115 -4.1049118 -4.0324392 -3.9670236 -3.8956461 -3.8520088 -3.8895288 -4.0039191 -4.105289 -4.1591597 -4.1694832 -4.17298 -4.1874781][-4.1857634 -4.1622314 -4.1250191 -4.0586915 -3.9783709 -3.9059367 -3.8253329 -3.7732928 -3.8186529 -3.962193 -4.0898266 -4.1585255 -4.1745806 -4.1802964 -4.2006345][-4.1795878 -4.1496739 -4.1110582 -4.0547829 -4.0005522 -3.9643128 -3.9265335 -3.9047256 -3.9387925 -4.04313 -4.1377153 -4.1885166 -4.1996655 -4.2049146 -4.225512][-4.17318 -4.1428766 -4.1132364 -4.0786114 -4.0534658 -4.0461287 -4.0338497 -4.0280333 -4.0496588 -4.1176019 -4.1778412 -4.2082543 -4.216258 -4.2263756 -4.2488914][-4.1613932 -4.1287189 -4.102725 -4.0848379 -4.0770354 -4.0780258 -4.0728645 -4.0719209 -4.0891757 -4.1429048 -4.1897659 -4.212275 -4.2209854 -4.2365236 -4.2608967][-4.15637 -4.1212997 -4.0952568 -4.0826797 -4.076735 -4.0722265 -4.0654354 -4.0632634 -4.0769148 -4.129117 -4.1794143 -4.2064424 -4.2197986 -4.2408319 -4.2670875][-4.1683607 -4.1403837 -4.1183567 -4.1064243 -4.0987544 -4.0882277 -4.0759468 -4.0685792 -4.0771303 -4.122766 -4.1722965 -4.2011137 -4.2151523 -4.2376227 -4.2649536][-4.1892891 -4.177587 -4.166255 -4.1608353 -4.1555834 -4.1446838 -4.1303554 -4.1176233 -4.1180859 -4.1443553 -4.1781087 -4.1992345 -4.2130394 -4.2366686 -4.2630167][-4.2083263 -4.2087021 -4.2108312 -4.213429 -4.2127576 -4.204062 -4.188972 -4.1729937 -4.1657662 -4.1749272 -4.1918664 -4.2050924 -4.2169895 -4.2352276 -4.2554288][-4.2293715 -4.23263 -4.2387514 -4.2436814 -4.2441154 -4.2368507 -4.2239 -4.2094812 -4.2019873 -4.205894 -4.2147946 -4.2226911 -4.2310948 -4.2422624 -4.2560706]]...]
INFO - root - 2017-12-07 19:18:48.429901: step 44410, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 52h:22m:59s remains)
INFO - root - 2017-12-07 19:18:55.265537: step 44420, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 57h:14m:27s remains)
INFO - root - 2017-12-07 19:19:01.988849: step 44430, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 56h:58m:12s remains)
INFO - root - 2017-12-07 19:19:08.760953: step 44440, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 56h:05m:31s remains)
INFO - root - 2017-12-07 19:19:15.586689: step 44450, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 50h:59m:45s remains)
INFO - root - 2017-12-07 19:19:22.411167: step 44460, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 52h:49m:48s remains)
INFO - root - 2017-12-07 19:19:29.316465: step 44470, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 58h:37m:19s remains)
INFO - root - 2017-12-07 19:19:36.176077: step 44480, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 57h:51m:21s remains)
INFO - root - 2017-12-07 19:19:42.986854: step 44490, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 50h:48m:51s remains)
INFO - root - 2017-12-07 19:19:49.594882: step 44500, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 50h:00m:17s remains)
2017-12-07 19:19:50.400999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3228707 -4.3180628 -4.3148928 -4.313458 -4.3119822 -4.3068995 -4.3028331 -4.3017206 -4.29959 -4.2949963 -4.2749743 -4.2449608 -4.22122 -4.2035823 -4.19788][-4.3051133 -4.2976627 -4.2860231 -4.2760835 -4.268434 -4.257113 -4.2492371 -4.2496963 -4.2550983 -4.2572708 -4.2381873 -4.2080269 -4.1841235 -4.1666961 -4.1583271][-4.2881722 -4.2755036 -4.2521682 -4.2308774 -4.2156453 -4.1990256 -4.1894336 -4.1953664 -4.2151079 -4.2290292 -4.2160659 -4.1924162 -4.1724334 -4.1546273 -4.1377382][-4.2759976 -4.2560906 -4.2214823 -4.187149 -4.1604543 -4.1376109 -4.1265774 -4.1427369 -4.1854882 -4.2206073 -4.2209558 -4.2041478 -4.184927 -4.1620879 -4.131031][-4.2724295 -4.2496247 -4.2055287 -4.1568022 -4.1139164 -4.075973 -4.05659 -4.0867147 -4.1610451 -4.2263527 -4.2461653 -4.2345214 -4.2100096 -4.177227 -4.1307583][-4.2728772 -4.2548628 -4.2058249 -4.1385107 -4.0666709 -3.9966669 -3.9565513 -4.0048604 -4.1182323 -4.2214994 -4.26578 -4.2631912 -4.2331715 -4.1903272 -4.1334791][-4.2742329 -4.2628093 -4.2120242 -4.1253967 -4.0128541 -3.8876214 -3.813354 -3.8868403 -4.0500035 -4.1919322 -4.2615175 -4.2737327 -4.2452569 -4.1963367 -4.1352615][-4.2709327 -4.2644787 -4.2130356 -4.1073012 -3.9514141 -3.7637153 -3.6510792 -3.7564573 -3.9723725 -4.1481042 -4.24139 -4.2717638 -4.2502179 -4.1959085 -4.1309638][-4.2655149 -4.2656775 -4.2198915 -4.1088338 -3.9342787 -3.7119145 -3.5676756 -3.691638 -3.9345727 -4.1252403 -4.2298474 -4.2692208 -4.2501922 -4.1869259 -4.11618][-4.2570844 -4.272367 -4.2444263 -4.1523418 -4.0016866 -3.8078227 -3.6792686 -3.776489 -3.9858234 -4.1514893 -4.2386985 -4.2683377 -4.2427425 -4.1696 -4.096674][-4.2313004 -4.2628374 -4.2565918 -4.1952019 -4.0900097 -3.9556596 -3.8683095 -3.9292641 -4.0741739 -4.1932807 -4.2519298 -4.2622342 -4.2276163 -4.1539259 -4.0848227][-4.2080474 -4.2467031 -4.2574434 -4.2236753 -4.15513 -4.069644 -4.0179744 -4.0560894 -4.1495304 -4.228663 -4.2632532 -4.2590585 -4.2210541 -4.1557937 -4.1004043][-4.2134786 -4.247736 -4.2653513 -4.2520013 -4.208528 -4.1539917 -4.1233788 -4.1496439 -4.2124925 -4.2662067 -4.2877769 -4.2775683 -4.2393823 -4.1868157 -4.1491995][-4.2448926 -4.270968 -4.2867594 -4.2840328 -4.2578826 -4.2227821 -4.202961 -4.2211962 -4.2639241 -4.299943 -4.3135314 -4.303535 -4.2722092 -4.2343769 -4.213748][-4.2880211 -4.3062363 -4.3177896 -4.3194332 -4.3065243 -4.286305 -4.2717371 -4.2811427 -4.30718 -4.3288059 -4.3344455 -4.3259993 -4.3049884 -4.2824082 -4.2733383]]...]
INFO - root - 2017-12-07 19:19:57.162486: step 44510, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 54h:21m:02s remains)
INFO - root - 2017-12-07 19:20:03.914142: step 44520, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 51h:30m:04s remains)
INFO - root - 2017-12-07 19:20:10.760133: step 44530, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 52h:19m:42s remains)
INFO - root - 2017-12-07 19:20:17.567163: step 44540, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 58h:05m:35s remains)
INFO - root - 2017-12-07 19:20:24.317180: step 44550, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 57h:57m:59s remains)
INFO - root - 2017-12-07 19:20:31.051660: step 44560, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 55h:13m:37s remains)
INFO - root - 2017-12-07 19:20:37.683535: step 44570, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.616 sec/batch; 49h:14m:22s remains)
INFO - root - 2017-12-07 19:20:44.341469: step 44580, loss = 2.05, batch loss = 1.99 (14.0 examples/sec; 0.573 sec/batch; 45h:50m:21s remains)
INFO - root - 2017-12-07 19:20:51.141510: step 44590, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 56h:51m:28s remains)
INFO - root - 2017-12-07 19:20:57.815748: step 44600, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 54h:10m:52s remains)
2017-12-07 19:20:58.615643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1803446 -4.1996369 -4.2233686 -4.2491541 -4.2696042 -4.2781816 -4.2752562 -4.2709785 -4.2624006 -4.2438169 -4.2219505 -4.2031755 -4.1968551 -4.2084184 -4.2317753][-4.2165713 -4.235549 -4.257957 -4.2819953 -4.3007636 -4.3055434 -4.2994137 -4.291975 -4.2816029 -4.2623434 -4.2436037 -4.2264175 -4.2182431 -4.2276087 -4.250864][-4.2295189 -4.2544751 -4.2804894 -4.2995439 -4.3061037 -4.2964435 -4.2799497 -4.2677021 -4.2551556 -4.2430215 -4.2404237 -4.23873 -4.234715 -4.24118 -4.261158][-4.2282767 -4.2569666 -4.2856469 -4.2982206 -4.2859411 -4.2529092 -4.2190032 -4.1974163 -4.1825457 -4.1802583 -4.2026792 -4.2263069 -4.2358012 -4.243124 -4.2545485][-4.2269621 -4.2574325 -4.28328 -4.2859097 -4.2521591 -4.1911564 -4.1346321 -4.1000566 -4.0828476 -4.0890341 -4.1357303 -4.1852283 -4.2123237 -4.2239408 -4.227416][-4.2281241 -4.2557788 -4.2739196 -4.2673411 -4.2204523 -4.1433854 -4.070044 -4.0286117 -4.0143118 -4.022644 -4.07254 -4.1363778 -4.1828055 -4.2044148 -4.2065377][-4.2267594 -4.2508116 -4.2600965 -4.2481804 -4.1996975 -4.1203232 -4.0469303 -4.0104914 -4.0083356 -4.0201154 -4.0563412 -4.1123862 -4.1652141 -4.1952481 -4.2022514][-4.2201586 -4.2454877 -4.2492948 -4.2310114 -4.1846428 -4.1149187 -4.0500827 -4.0215325 -4.0294027 -4.0455508 -4.0676365 -4.10976 -4.162704 -4.1967311 -4.2091551][-4.2116771 -4.2362642 -4.2405381 -4.2249002 -4.1877003 -4.1344504 -4.0835915 -4.0589294 -4.0669589 -4.0801029 -4.0916963 -4.1238713 -4.1717944 -4.2073917 -4.2245617][-4.199544 -4.2221589 -4.2304096 -4.2239423 -4.20486 -4.1752281 -4.143693 -4.1254954 -4.1264071 -4.1277256 -4.1272416 -4.1484122 -4.1844435 -4.2161589 -4.2362704][-4.1830168 -4.2069411 -4.2205358 -4.2248049 -4.2249866 -4.2158132 -4.1985097 -4.1845932 -4.1784391 -4.1709585 -4.1619077 -4.170197 -4.1939325 -4.217144 -4.2336087][-4.1691842 -4.1933918 -4.2097049 -4.2191143 -4.228426 -4.2310605 -4.2245021 -4.2188339 -4.2116752 -4.2039618 -4.196743 -4.1959295 -4.2039623 -4.2138124 -4.2227678][-4.1688309 -4.189424 -4.2029438 -4.2097907 -4.2195797 -4.2267723 -4.2277555 -4.2283053 -4.2220631 -4.218708 -4.2175193 -4.2139721 -4.2114162 -4.2099757 -4.2114406][-4.1942725 -4.2084756 -4.2122121 -4.2105155 -4.2114768 -4.2104034 -4.2099977 -4.2103691 -4.2055283 -4.206 -4.2102757 -4.2109437 -4.2070713 -4.2006841 -4.1930046][-4.2171011 -4.2248907 -4.2162275 -4.2030787 -4.1926837 -4.1797047 -4.1704407 -4.1682806 -4.1627064 -4.161871 -4.1666436 -4.1760569 -4.1847649 -4.1851134 -4.175982]]...]
INFO - root - 2017-12-07 19:21:05.349815: step 44610, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 57h:47m:33s remains)
INFO - root - 2017-12-07 19:21:12.214701: step 44620, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 56h:32m:42s remains)
INFO - root - 2017-12-07 19:21:19.024768: step 44630, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 51h:17m:58s remains)
INFO - root - 2017-12-07 19:21:25.789888: step 44640, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 49h:43m:19s remains)
INFO - root - 2017-12-07 19:21:32.567272: step 44650, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 52h:20m:23s remains)
INFO - root - 2017-12-07 19:21:39.379800: step 44660, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 58h:06m:54s remains)
INFO - root - 2017-12-07 19:21:46.093736: step 44670, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 57h:44m:38s remains)
INFO - root - 2017-12-07 19:21:52.852053: step 44680, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 54h:39m:01s remains)
INFO - root - 2017-12-07 19:21:59.544039: step 44690, loss = 2.05, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 49h:37m:19s remains)
INFO - root - 2017-12-07 19:22:06.140491: step 44700, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 52h:07m:53s remains)
2017-12-07 19:22:06.891878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2921028 -4.2931786 -4.2951889 -4.2970948 -4.2989664 -4.2979326 -4.2936139 -4.2865462 -4.2799578 -4.2768707 -4.2784586 -4.2787881 -4.2795177 -4.2852283 -4.2944455][-4.266037 -4.2689047 -4.2719741 -4.2716808 -4.2705603 -4.2657743 -4.254735 -4.2411604 -4.2329855 -4.2346621 -4.2405176 -4.2418313 -4.2440658 -4.2550359 -4.27018][-4.2466707 -4.2517495 -4.2545242 -4.2500434 -4.2443781 -4.2328739 -4.2108293 -4.1879172 -4.1795454 -4.1902957 -4.2029047 -4.2041092 -4.2058034 -4.2221193 -4.2443247][-4.2305918 -4.2347975 -4.2371435 -4.2282748 -4.2157836 -4.1976652 -4.1685343 -4.1431718 -4.1416478 -4.1632676 -4.1782455 -4.1763554 -4.1764569 -4.195405 -4.2226954][-4.2052441 -4.2070823 -4.2093725 -4.1997628 -4.1823611 -4.16074 -4.1300683 -4.1123552 -4.1242294 -4.1494269 -4.1596127 -4.1557212 -4.1602554 -4.1804905 -4.2062507][-4.1617074 -4.1654935 -4.16938 -4.1602726 -4.1419735 -4.1131978 -4.0766864 -4.0647368 -4.0910239 -4.1193218 -4.1228647 -4.1195469 -4.1321068 -4.1578531 -4.1841316][-4.1106987 -4.1174345 -4.1214008 -4.1155629 -4.0983448 -4.0571375 -4.003767 -3.9927773 -4.0310678 -4.0666032 -4.0691338 -4.0696778 -4.090035 -4.1218867 -4.1533279][-4.0766249 -4.0866723 -4.0888958 -4.0879807 -4.0746083 -4.0208855 -3.9449461 -3.920233 -3.9562 -3.9944921 -4.0042262 -4.0161934 -4.0500731 -4.0885477 -4.1276593][-4.071197 -4.0779338 -4.0845842 -4.09343 -4.091835 -4.0426369 -3.9588768 -3.9168468 -3.9349821 -3.9656029 -3.9759126 -3.9952793 -4.036202 -4.0762849 -4.1135845][-4.0846958 -4.0874519 -4.1030455 -4.1251488 -4.1439281 -4.1180415 -4.0529294 -4.0107203 -4.0127234 -4.0245633 -4.0272021 -4.0485115 -4.08304 -4.1067939 -4.1262589][-4.10124 -4.1060648 -4.1302495 -4.15996 -4.1872506 -4.1823468 -4.1481633 -4.1260672 -4.1229954 -4.116837 -4.1091385 -4.1241193 -4.1468406 -4.15368 -4.1603107][-4.1153879 -4.1269674 -4.1598816 -4.192524 -4.217998 -4.2214212 -4.2055769 -4.1981826 -4.1947818 -4.1791358 -4.1595836 -4.1664166 -4.1836495 -4.1903009 -4.1987724][-4.1337924 -4.1472378 -4.1803889 -4.2131877 -4.2338314 -4.2356782 -4.2247715 -4.2229586 -4.2216463 -4.2044272 -4.178751 -4.1812444 -4.1997437 -4.2143431 -4.2334995][-4.1662374 -4.1800079 -4.2068758 -4.2360787 -4.2511864 -4.2511597 -4.2417197 -4.2416544 -4.2459369 -4.235816 -4.2124529 -4.2105074 -4.2257586 -4.244524 -4.2678094][-4.2118297 -4.2246847 -4.2451029 -4.2681742 -4.2770805 -4.2734704 -4.2620707 -4.2612038 -4.2674294 -4.2660685 -4.2516007 -4.2475824 -4.2562342 -4.2722025 -4.2909889]]...]
INFO - root - 2017-12-07 19:22:13.708379: step 44710, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.675 sec/batch; 53h:59m:01s remains)
INFO - root - 2017-12-07 19:22:20.471806: step 44720, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 50h:24m:22s remains)
INFO - root - 2017-12-07 19:22:27.198796: step 44730, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:57m:51s remains)
INFO - root - 2017-12-07 19:22:33.994369: step 44740, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 57h:16m:23s remains)
INFO - root - 2017-12-07 19:22:40.766151: step 44750, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 58h:25m:02s remains)
INFO - root - 2017-12-07 19:22:47.505896: step 44760, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 53h:48m:10s remains)
INFO - root - 2017-12-07 19:22:54.292585: step 44770, loss = 2.03, batch loss = 1.98 (11.7 examples/sec; 0.683 sec/batch; 54h:34m:37s remains)
INFO - root - 2017-12-07 19:23:01.118522: step 44780, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 51h:25m:12s remains)
INFO - root - 2017-12-07 19:23:07.872429: step 44790, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 54h:42m:26s remains)
INFO - root - 2017-12-07 19:23:14.477314: step 44800, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 57h:38m:22s remains)
2017-12-07 19:23:15.203540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1373806 -4.1126151 -4.1288805 -4.1623688 -4.1973729 -4.2198563 -4.2352304 -4.2342658 -4.2315321 -4.2157974 -4.1987233 -4.2003345 -4.21065 -4.2177196 -4.2212334][-4.0706253 -4.0672297 -4.1139331 -4.1656537 -4.2055516 -4.2222638 -4.2287788 -4.218864 -4.2024083 -4.1778665 -4.1637011 -4.1782036 -4.198957 -4.2151775 -4.2217155][-4.0307603 -4.0449386 -4.104897 -4.156127 -4.1911945 -4.2020345 -4.2017169 -4.1844091 -4.1608496 -4.1350479 -4.132884 -4.1681042 -4.2033863 -4.2262187 -4.2338886][-4.0530314 -4.0686688 -4.1139083 -4.1392021 -4.155993 -4.162519 -4.1554942 -4.1322556 -4.10408 -4.0878739 -4.1111112 -4.1665092 -4.2125788 -4.2381 -4.2479758][-4.1021781 -4.1057448 -4.121511 -4.1152277 -4.1079121 -4.1059217 -4.0967603 -4.0792403 -4.056952 -4.059803 -4.10749 -4.1717291 -4.2177057 -4.2402458 -4.2509346][-4.1524243 -4.1401386 -4.1335864 -4.1108913 -4.0869823 -4.0757241 -4.0683608 -4.061861 -4.0505934 -4.0674782 -4.12603 -4.1860323 -4.2223177 -4.2388711 -4.2504125][-4.1883812 -4.1722293 -4.1550341 -4.1229753 -4.0875812 -4.0659027 -4.0547018 -4.0497527 -4.040123 -4.0624552 -4.1217484 -4.1790771 -4.2149539 -4.235642 -4.2507753][-4.210113 -4.1949592 -4.1728096 -4.1359954 -4.0928268 -4.0620403 -4.0416727 -4.0257325 -4.0066447 -4.0276961 -4.0841217 -4.1415343 -4.1852388 -4.2163291 -4.2346535][-4.2283506 -4.2127223 -4.1912704 -4.1579056 -4.1200614 -4.0900741 -4.0637922 -4.0336804 -4.0060964 -4.0238066 -4.071641 -4.1182714 -4.1582351 -4.1844368 -4.1992025][-4.2332058 -4.2173581 -4.2008686 -4.1818542 -4.1591697 -4.1384068 -4.1093683 -4.0698423 -4.0358267 -4.0389652 -4.0690117 -4.1011167 -4.1321907 -4.1533017 -4.1679525][-4.2192416 -4.2027483 -4.1921945 -4.1841512 -4.1733761 -4.1601758 -4.1336923 -4.097578 -4.0688128 -4.0693941 -4.0902653 -4.1139021 -4.1383157 -4.1566892 -4.1693554][-4.203495 -4.1850677 -4.1709266 -4.1577559 -4.1459274 -4.1396666 -4.1285539 -4.11172 -4.1013656 -4.1102052 -4.131453 -4.154285 -4.1728625 -4.1827054 -4.1857638][-4.1784978 -4.1575446 -4.13653 -4.1164255 -4.1046824 -4.1079369 -4.1155214 -4.1228619 -4.13051 -4.1455846 -4.1686325 -4.1925597 -4.2085991 -4.2114725 -4.20695][-4.1243529 -4.1075554 -4.0919442 -4.0832195 -4.08329 -4.0960851 -4.1163664 -4.1378617 -4.1582632 -4.1782088 -4.201807 -4.2243457 -4.2377214 -4.2369804 -4.2305689][-4.0560756 -4.0535579 -4.0556331 -4.0671415 -4.0840731 -4.1062593 -4.1331086 -4.1605959 -4.1865425 -4.2080674 -4.2300887 -4.248889 -4.2602711 -4.2600141 -4.2580881]]...]
INFO - root - 2017-12-07 19:23:21.944379: step 44810, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 50h:56m:11s remains)
INFO - root - 2017-12-07 19:23:28.796744: step 44820, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 56h:16m:17s remains)
INFO - root - 2017-12-07 19:23:35.640112: step 44830, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.731 sec/batch; 58h:24m:33s remains)
INFO - root - 2017-12-07 19:23:42.478090: step 44840, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 53h:51m:52s remains)
INFO - root - 2017-12-07 19:23:49.348160: step 44850, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 50h:14m:42s remains)
INFO - root - 2017-12-07 19:23:56.221854: step 44860, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 51h:28m:49s remains)
INFO - root - 2017-12-07 19:24:03.002387: step 44870, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 56h:04m:21s remains)
INFO - root - 2017-12-07 19:24:09.910466: step 44880, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 57h:46m:18s remains)
INFO - root - 2017-12-07 19:24:16.611717: step 44890, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 0.533 sec/batch; 42h:33m:54s remains)
INFO - root - 2017-12-07 19:24:23.047556: step 44900, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.640 sec/batch; 51h:06m:29s remains)
2017-12-07 19:24:23.883774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3020248 -4.2993817 -4.2952733 -4.2844272 -4.2754226 -4.2707477 -4.2657027 -4.2574544 -4.2517328 -4.2550192 -4.2620888 -4.2706304 -4.2774253 -4.2788391 -4.2786407][-4.2909842 -4.2862077 -4.2795105 -4.2703567 -4.2688313 -4.2668543 -4.2590737 -4.2449837 -4.2373304 -4.2409453 -4.2494483 -4.2603955 -4.2699594 -4.2722049 -4.2726817][-4.2619777 -4.2615252 -4.2562709 -4.2474189 -4.2495289 -4.2487106 -4.2370372 -4.21699 -4.2103066 -4.214725 -4.2234321 -4.2369418 -4.2506361 -4.2557507 -4.2569909][-4.2278409 -4.2364759 -4.2325354 -4.2187371 -4.220932 -4.2218103 -4.211267 -4.1866841 -4.1787758 -4.1863961 -4.1966443 -4.2121439 -4.2267613 -4.232029 -4.2321944][-4.1987438 -4.2157607 -4.2182198 -4.199831 -4.1982236 -4.1951103 -4.1777525 -4.1421766 -4.1343474 -4.1553149 -4.1785398 -4.2010961 -4.2173429 -4.2224979 -4.2215519][-4.1767612 -4.2002535 -4.2121444 -4.1935191 -4.1830125 -4.1650462 -4.1171508 -4.0533361 -4.0429354 -4.0881624 -4.1370745 -4.1748815 -4.2035751 -4.2174315 -4.2207575][-4.1463208 -4.1711264 -4.1919265 -4.1792374 -4.1632423 -4.11755 -4.013134 -3.90193 -3.901319 -3.990654 -4.0723939 -4.1317458 -4.1830435 -4.2118716 -4.2215581][-4.1077838 -4.1380553 -4.163115 -4.1499891 -4.1267653 -4.049684 -3.8803606 -3.7069297 -3.7416766 -3.8975146 -4.0115623 -4.0895882 -4.1562328 -4.1970406 -4.2115288][-4.0920253 -4.1292529 -4.1587577 -4.1425085 -4.1049147 -4.0130935 -3.8267074 -3.6336391 -3.7013137 -3.8921261 -4.016345 -4.0927591 -4.1600041 -4.1984816 -4.208879][-4.1185265 -4.1549211 -4.1851859 -4.1703639 -4.1386528 -4.0698686 -3.9426992 -3.8247061 -3.8760996 -4.0095792 -4.0976667 -4.1516995 -4.2000957 -4.2234516 -4.2224684][-4.1729403 -4.2011824 -4.2284489 -4.2171121 -4.1972346 -4.1586623 -4.0966353 -4.0447955 -4.0692883 -4.135325 -4.1852107 -4.2178059 -4.2462931 -4.2542887 -4.2441182][-4.2254539 -4.2435265 -4.2603664 -4.24649 -4.2306252 -4.2094197 -4.1784825 -4.1576934 -4.1707015 -4.2085586 -4.2425532 -4.2675838 -4.2837067 -4.2836375 -4.2684827][-4.2622948 -4.2708988 -4.2765369 -4.2578731 -4.2384291 -4.2206774 -4.1980133 -4.1825666 -4.1964159 -4.2315731 -4.2646518 -4.287991 -4.2992368 -4.2987566 -4.2864323][-4.2717624 -4.2706718 -4.2674613 -4.2504911 -4.233304 -4.2147241 -4.1938238 -4.1790185 -4.1935554 -4.2275915 -4.2607412 -4.2838569 -4.2967434 -4.3001294 -4.2944369][-4.2550783 -4.2508912 -4.2447891 -4.2367191 -4.2278104 -4.2114663 -4.1916046 -4.1767116 -4.1894255 -4.2219291 -4.2524066 -4.2739606 -4.2882957 -4.2947927 -4.2955666]]...]
INFO - root - 2017-12-07 19:24:30.588352: step 44910, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 52h:39m:28s remains)
INFO - root - 2017-12-07 19:24:37.372629: step 44920, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 52h:55m:36s remains)
INFO - root - 2017-12-07 19:24:44.203277: step 44930, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 55h:42m:43s remains)
INFO - root - 2017-12-07 19:24:50.980242: step 44940, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 58h:02m:23s remains)
INFO - root - 2017-12-07 19:24:57.739730: step 44950, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 52h:09m:09s remains)
INFO - root - 2017-12-07 19:25:04.470982: step 44960, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 51h:31m:17s remains)
INFO - root - 2017-12-07 19:25:11.248494: step 44970, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 50h:48m:15s remains)
INFO - root - 2017-12-07 19:25:18.060526: step 44980, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 54h:41m:27s remains)
INFO - root - 2017-12-07 19:25:25.010927: step 44990, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 57h:03m:18s remains)
INFO - root - 2017-12-07 19:25:31.628201: step 45000, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 54h:28m:49s remains)
2017-12-07 19:25:32.360345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2405386 -4.220789 -4.2011819 -4.1971383 -4.2067914 -4.2162604 -4.2179346 -4.2198706 -4.2255735 -4.2357659 -4.2479739 -4.2523794 -4.2518177 -4.255053 -4.2625813][-4.2299652 -4.2068214 -4.1875639 -4.1847491 -4.1910477 -4.1958952 -4.1950216 -4.1957316 -4.2003126 -4.210371 -4.2208328 -4.2272735 -4.2316661 -4.2351742 -4.2433772][-4.2167735 -4.1890254 -4.1676574 -4.1603594 -4.1625028 -4.1657834 -4.1676092 -4.173449 -4.1859813 -4.1994658 -4.2058563 -4.2133155 -4.2226472 -4.2253742 -4.2314167][-4.2007341 -4.1696105 -4.1498165 -4.1389675 -4.1342006 -4.1362572 -4.1465583 -4.1574059 -4.1760354 -4.1942496 -4.1959567 -4.1971688 -4.2007394 -4.1987958 -4.2066388][-4.2020245 -4.1738024 -4.160841 -4.1447492 -4.13122 -4.122757 -4.1325865 -4.1465526 -4.16468 -4.1830468 -4.1776848 -4.1697555 -4.1688685 -4.1706252 -4.185389][-4.2152939 -4.1880608 -4.1716404 -4.1410069 -4.11747 -4.1001239 -4.1032596 -4.1236639 -4.1363997 -4.1458054 -4.1371641 -4.1264386 -4.1285157 -4.1417332 -4.1662726][-4.2171097 -4.186625 -4.159452 -4.1141167 -4.0762215 -4.0495877 -4.0497265 -4.0791721 -4.0971975 -4.1046858 -4.1005921 -4.0889635 -4.0910473 -4.1131058 -4.142108][-4.2017021 -4.1691384 -4.1391287 -4.0925326 -4.05267 -4.0236378 -4.0249057 -4.0607591 -4.0861392 -4.0994577 -4.1022816 -4.092299 -4.09151 -4.107091 -4.1310868][-4.1973395 -4.175384 -4.1596422 -4.1249037 -4.0893464 -4.0576334 -4.0548539 -4.0820541 -4.10336 -4.1154437 -4.1261339 -4.121767 -4.116776 -4.1259842 -4.1467428][-4.2160273 -4.2111135 -4.2175064 -4.1995692 -4.1661577 -4.1262617 -4.1128616 -4.1263928 -4.1387687 -4.1459408 -4.1533608 -4.1498165 -4.1471167 -4.1628275 -4.1862206][-4.2304111 -4.2373338 -4.2606878 -4.2561622 -4.2258739 -4.1817913 -4.1565919 -4.1607122 -4.173121 -4.1784396 -4.1790533 -4.1725187 -4.1778584 -4.2031994 -4.2300448][-4.2237778 -4.2339306 -4.2618 -4.2629094 -4.2417903 -4.2123828 -4.1949096 -4.1949754 -4.2060852 -4.2079082 -4.202168 -4.1936169 -4.2019567 -4.2260375 -4.2467504][-4.2178674 -4.2221928 -4.2458906 -4.2490959 -4.2379904 -4.2257376 -4.2214093 -4.2238922 -4.2254972 -4.2158561 -4.2064352 -4.19498 -4.1974955 -4.2158566 -4.2320762][-4.2239695 -4.2266674 -4.2432666 -4.2466683 -4.2414141 -4.2379646 -4.2366638 -4.2346616 -4.2274141 -4.212831 -4.2011204 -4.1884627 -4.1895995 -4.2027869 -4.2159128][-4.2472692 -4.2514992 -4.2581592 -4.2520251 -4.2423882 -4.2373962 -4.2376533 -4.2368221 -4.229239 -4.2137051 -4.1984382 -4.1874037 -4.190208 -4.2026668 -4.21757]]...]
INFO - root - 2017-12-07 19:25:39.147796: step 45010, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 57h:12m:24s remains)
INFO - root - 2017-12-07 19:25:46.024983: step 45020, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 0.755 sec/batch; 60h:15m:22s remains)
INFO - root - 2017-12-07 19:25:52.911563: step 45030, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.662 sec/batch; 52h:53m:57s remains)
INFO - root - 2017-12-07 19:25:59.665821: step 45040, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.653 sec/batch; 52h:08m:03s remains)
INFO - root - 2017-12-07 19:26:06.395833: step 45050, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 51h:55m:38s remains)
INFO - root - 2017-12-07 19:26:13.270674: step 45060, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.747 sec/batch; 59h:39m:19s remains)
INFO - root - 2017-12-07 19:26:20.177153: step 45070, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 55h:45m:02s remains)
INFO - root - 2017-12-07 19:26:26.921722: step 45080, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 51h:40m:30s remains)
INFO - root - 2017-12-07 19:26:33.693207: step 45090, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 51h:24m:09s remains)
INFO - root - 2017-12-07 19:26:40.252189: step 45100, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 54h:24m:06s remains)
2017-12-07 19:26:40.996579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9751005 -3.9616804 -3.9782703 -4.00231 -4.0493355 -4.122447 -4.1648197 -4.168303 -4.1464639 -4.1088037 -4.0618687 -4.0332589 -4.0423865 -4.0584745 -4.0549936][-3.8984871 -3.8883126 -3.90971 -3.9471431 -4.0172729 -4.1075549 -4.1622963 -4.1690454 -4.141027 -4.0930567 -4.0239749 -3.9712214 -3.9607887 -3.974318 -3.9878938][-3.9482667 -3.9437594 -3.9601848 -3.9944928 -4.0516062 -4.1207056 -4.1633425 -4.1669173 -4.1380348 -4.08843 -4.0136638 -3.9481654 -3.9183731 -3.9224124 -3.949795][-4.0589724 -4.0515671 -4.0517726 -4.0642543 -4.0844197 -4.1190739 -4.1460538 -4.1503897 -4.1322632 -4.1026788 -4.0592661 -4.0170255 -3.9881186 -3.984076 -4.0147338][-4.16616 -4.1495209 -4.1273117 -4.1048622 -4.0835338 -4.0872192 -4.1022058 -4.1081934 -4.1049657 -4.1123543 -4.1202607 -4.117928 -4.1068892 -4.1000261 -4.1141825][-4.2341762 -4.2047114 -4.1536674 -4.0946956 -4.0400596 -4.0176954 -4.018847 -4.0259552 -4.0431819 -4.0963974 -4.1524158 -4.1793423 -4.1819048 -4.1778126 -4.1811423][-4.2655172 -4.2266712 -4.1503282 -4.0619717 -3.9795046 -3.9332786 -3.9224341 -3.9339733 -3.97617 -4.0624657 -4.1457281 -4.1847329 -4.1938925 -4.1987004 -4.206953][-4.2730842 -4.2316566 -4.1465473 -4.049881 -3.9637418 -3.9116 -3.8977947 -3.9207973 -3.9792771 -4.0685849 -4.1487188 -4.1887259 -4.2009144 -4.2126837 -4.226994][-4.2676711 -4.2314696 -4.1591568 -4.0825834 -4.0185604 -3.9819994 -3.976969 -4.0052037 -4.0564222 -4.1223683 -4.1794081 -4.2101946 -4.2230887 -4.237958 -4.2540207][-4.2633257 -4.2408562 -4.191422 -4.145123 -4.1094589 -4.0884919 -4.0866933 -4.1042585 -4.1361542 -4.1772542 -4.2141552 -4.236033 -4.2471838 -4.2590408 -4.2718821][-4.2609105 -4.2546272 -4.2297349 -4.212338 -4.1988611 -4.1894393 -4.1869922 -4.1921415 -4.2067666 -4.2295814 -4.2501936 -4.2626657 -4.2704339 -4.2781882 -4.2841568][-4.2605562 -4.265079 -4.2574778 -4.258914 -4.2614594 -4.2604127 -4.2591329 -4.2595725 -4.2659888 -4.2773113 -4.2869539 -4.2924814 -4.2956448 -4.2957721 -4.290709][-4.2682114 -4.2782984 -4.2804966 -4.29168 -4.3037376 -4.3084836 -4.3086042 -4.3065853 -4.3065867 -4.3087215 -4.3090324 -4.3065805 -4.3020372 -4.2934313 -4.2803135][-4.2813511 -4.2927656 -4.2986183 -4.3114243 -4.3232956 -4.3281531 -4.3288569 -4.325213 -4.3221331 -4.31895 -4.3146539 -4.3091607 -4.3017955 -4.2893171 -4.2741723][-4.2978158 -4.3109488 -4.317944 -4.3294358 -4.3374577 -4.3394203 -4.3368764 -4.3300834 -4.3241477 -4.320312 -4.3169584 -4.3128538 -4.3062506 -4.2954569 -4.2829037]]...]
INFO - root - 2017-12-07 19:26:47.740724: step 45110, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 53h:38m:10s remains)
INFO - root - 2017-12-07 19:26:54.504099: step 45120, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 50h:52m:28s remains)
INFO - root - 2017-12-07 19:27:01.252205: step 45130, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 53h:45m:09s remains)
INFO - root - 2017-12-07 19:27:08.194119: step 45140, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.737 sec/batch; 58h:48m:02s remains)
INFO - root - 2017-12-07 19:27:14.992463: step 45150, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 53h:18m:16s remains)
INFO - root - 2017-12-07 19:27:21.754420: step 45160, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 49h:16m:37s remains)
INFO - root - 2017-12-07 19:27:28.662990: step 45170, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 53h:50m:14s remains)
INFO - root - 2017-12-07 19:27:35.463355: step 45180, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 55h:31m:07s remains)
INFO - root - 2017-12-07 19:27:42.288223: step 45190, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 57h:57m:39s remains)
INFO - root - 2017-12-07 19:27:48.756871: step 45200, loss = 2.04, batch loss = 1.98 (13.3 examples/sec; 0.600 sec/batch; 47h:54m:39s remains)
2017-12-07 19:27:49.516265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1797223 -4.178133 -4.1864142 -4.187778 -4.1704726 -4.1447592 -4.1188321 -4.0855942 -4.0726724 -4.0919919 -4.1280713 -4.1764464 -4.2325983 -4.273262 -4.2941685][-4.1705465 -4.1671696 -4.1814318 -4.1835032 -4.1674833 -4.1486382 -4.124197 -4.098186 -4.0848875 -4.1015406 -4.1391678 -4.1882472 -4.2423162 -4.2773147 -4.2961855][-4.1508365 -4.1505055 -4.1740103 -4.174283 -4.1499443 -4.1334753 -4.1127625 -4.0912175 -4.0832062 -4.1063032 -4.1439214 -4.1914368 -4.2430253 -4.274756 -4.2954507][-4.1322579 -4.1447239 -4.1704741 -4.1633739 -4.1239719 -4.1102538 -4.0953093 -4.0758657 -4.0818167 -4.1171441 -4.1523652 -4.1973763 -4.2450337 -4.2753983 -4.2963428][-4.1028357 -4.1239657 -4.1430945 -4.1255522 -4.079855 -4.0733991 -4.05963 -4.0370426 -4.05781 -4.1131968 -4.1529679 -4.2017374 -4.2523923 -4.2801857 -4.2988272][-4.0698996 -4.0869031 -4.0911517 -4.0617046 -4.0140848 -4.0216975 -4.0005341 -3.9529748 -3.9819169 -4.0671535 -4.1246405 -4.1870375 -4.2493439 -4.2796812 -4.2977633][-4.0683346 -4.0663638 -4.0569983 -4.0212293 -3.9729691 -3.9749229 -3.9235854 -3.8274615 -3.855294 -3.9837046 -4.0691471 -4.1504445 -4.2287879 -4.2696829 -4.2933145][-4.1185727 -4.1034503 -4.09483 -4.0710564 -4.0245581 -4.0002184 -3.9177783 -3.7890866 -3.7996166 -3.939815 -4.0293837 -4.1138659 -4.203876 -4.2591648 -4.289185][-4.1816993 -4.1620669 -4.1587529 -4.1519942 -4.1150084 -4.0808988 -4.0111508 -3.9192305 -3.9156117 -3.995965 -4.0537586 -4.1149158 -4.1981077 -4.2574272 -4.2878962][-4.218307 -4.2030611 -4.2041364 -4.2081089 -4.1788774 -4.1492524 -4.1002169 -4.0430779 -4.0276418 -4.0576229 -4.0901771 -4.135849 -4.2046347 -4.25766 -4.2857685][-4.240768 -4.2397857 -4.2455754 -4.2506804 -4.2272153 -4.19875 -4.1575971 -4.1157174 -4.0929146 -4.0992985 -4.1252928 -4.1673746 -4.2227473 -4.2641072 -4.2871733][-4.2511225 -4.2598991 -4.2665267 -4.2712779 -4.2547369 -4.2283688 -4.19069 -4.1564713 -4.137044 -4.1381574 -4.1650257 -4.2059655 -4.2504363 -4.2804308 -4.296011][-4.265974 -4.2705655 -4.2718086 -4.2751355 -4.2632318 -4.2434831 -4.2148638 -4.1910381 -4.1795068 -4.1850548 -4.2104392 -4.2448463 -4.2807941 -4.300561 -4.3083][-4.2876921 -4.2845087 -4.2808437 -4.2793679 -4.2679687 -4.2522035 -4.2318382 -4.220469 -4.2172694 -4.226665 -4.2503395 -4.2800183 -4.3074512 -4.3190212 -4.3210874][-4.3189821 -4.3119884 -4.3087449 -4.3059635 -4.29623 -4.2822351 -4.2678485 -4.2629614 -4.2598848 -4.264451 -4.2814651 -4.30532 -4.3258204 -4.3320618 -4.33201]]...]
INFO - root - 2017-12-07 19:27:56.297885: step 45210, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 52h:14m:28s remains)
INFO - root - 2017-12-07 19:28:03.010060: step 45220, loss = 2.03, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 52h:39m:03s remains)
INFO - root - 2017-12-07 19:28:09.701923: step 45230, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.653 sec/batch; 52h:04m:58s remains)
INFO - root - 2017-12-07 19:28:16.557970: step 45240, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 54h:01m:49s remains)
INFO - root - 2017-12-07 19:28:23.417816: step 45250, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.741 sec/batch; 59h:09m:12s remains)
INFO - root - 2017-12-07 19:28:30.170686: step 45260, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 52h:26m:06s remains)
INFO - root - 2017-12-07 19:28:36.908188: step 45270, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 53h:02m:38s remains)
INFO - root - 2017-12-07 19:28:43.700990: step 45280, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 50h:59m:30s remains)
INFO - root - 2017-12-07 19:28:50.512514: step 45290, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 57h:20m:20s remains)
INFO - root - 2017-12-07 19:28:57.093029: step 45300, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 55h:12m:55s remains)
2017-12-07 19:28:57.776966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1343665 -4.1185675 -4.1440778 -4.1916661 -4.2459908 -4.292778 -4.3236723 -4.3360286 -4.3325467 -4.3137751 -4.2768178 -4.231144 -4.1868339 -4.1540642 -4.1428428][-4.1118135 -4.1321936 -4.1832237 -4.2391038 -4.2892003 -4.3241038 -4.3397255 -4.3369923 -4.320426 -4.2911334 -4.2464552 -4.19424 -4.1492286 -4.1281109 -4.1408353][-4.1257586 -4.1743855 -4.2330861 -4.2831964 -4.3173757 -4.3301215 -4.3251286 -4.3077712 -4.2853861 -4.2581868 -4.2191114 -4.173903 -4.1440482 -4.1438084 -4.1687975][-4.1659021 -4.2219567 -4.2734356 -4.3064532 -4.3143649 -4.2995882 -4.2744422 -4.2500663 -4.23335 -4.2231197 -4.2045588 -4.1835732 -4.1770172 -4.1889091 -4.2089581][-4.21728 -4.2626095 -4.2936158 -4.2966247 -4.2704272 -4.2277517 -4.1884961 -4.1665339 -4.1648583 -4.1817293 -4.1955357 -4.2077875 -4.22334 -4.2405558 -4.2508359][-4.2679977 -4.2909021 -4.2884336 -4.2517939 -4.1893654 -4.121614 -4.0738964 -4.0591269 -4.0789304 -4.129076 -4.1753254 -4.2151556 -4.2490997 -4.2740889 -4.2834234][-4.3034434 -4.2984219 -4.2570157 -4.1791868 -4.08217 -3.9936166 -3.94386 -3.9434643 -3.9895587 -4.0692825 -4.1425157 -4.2037535 -4.2527981 -4.2882018 -4.3030486][-4.3203239 -4.2954407 -4.225328 -4.1178012 -3.9969757 -3.8998575 -3.8585794 -3.8802714 -3.9488926 -4.0382395 -4.1202755 -4.192338 -4.2513442 -4.2923923 -4.3129559][-4.3267155 -4.2968841 -4.2205253 -4.1097021 -3.990016 -3.9028497 -3.881536 -3.9231436 -3.9977648 -4.0790429 -4.1528296 -4.21848 -4.2719574 -4.3091297 -4.3272672][-4.3281889 -4.3033352 -4.2432156 -4.1585751 -4.0673642 -4.0037465 -3.9977939 -4.0402031 -4.1045036 -4.1689005 -4.224339 -4.2718635 -4.3103814 -4.3352146 -4.3458986][-4.3345995 -4.317637 -4.2802939 -4.231009 -4.1772642 -4.1389632 -4.1390224 -4.1698041 -4.2135029 -4.2551761 -4.2888412 -4.3173409 -4.3396583 -4.3525515 -4.3565249][-4.3452406 -4.3348627 -4.3164158 -4.2950706 -4.2701664 -4.2492294 -4.2482719 -4.2649627 -4.2889633 -4.3112764 -4.3292475 -4.3438044 -4.3542004 -4.3594661 -4.3602595][-4.3530245 -4.3466978 -4.3388977 -4.3321385 -4.322907 -4.3130708 -4.3110123 -4.318296 -4.3305902 -4.3422289 -4.3508816 -4.3573718 -4.3607888 -4.3608036 -4.3590593][-4.3549953 -4.3506694 -4.3466182 -4.3444705 -4.3411746 -4.3367147 -4.3351388 -4.3384814 -4.345367 -4.3514686 -4.3553514 -4.3573089 -4.3571744 -4.3552823 -4.353507][-4.3520818 -4.3485923 -4.3457632 -4.344172 -4.3426218 -4.3408375 -4.3403316 -4.34261 -4.346982 -4.3502975 -4.35132 -4.3510227 -4.3493547 -4.3474541 -4.3469133]]...]
INFO - root - 2017-12-07 19:29:04.530584: step 45310, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 50h:20m:27s remains)
INFO - root - 2017-12-07 19:29:11.339805: step 45320, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 54h:10m:26s remains)
INFO - root - 2017-12-07 19:29:18.116777: step 45330, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 56h:58m:52s remains)
INFO - root - 2017-12-07 19:29:24.866121: step 45340, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 53h:39m:58s remains)
INFO - root - 2017-12-07 19:29:31.745682: step 45350, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 51h:43m:46s remains)
INFO - root - 2017-12-07 19:29:38.461731: step 45360, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 50h:15m:02s remains)
INFO - root - 2017-12-07 19:29:45.300069: step 45370, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 57h:17m:18s remains)
INFO - root - 2017-12-07 19:29:52.305323: step 45380, loss = 2.11, batch loss = 2.05 (10.7 examples/sec; 0.746 sec/batch; 59h:30m:46s remains)
INFO - root - 2017-12-07 19:29:59.164110: step 45390, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 52h:58m:53s remains)
INFO - root - 2017-12-07 19:30:05.773525: step 45400, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 50h:44m:04s remains)
2017-12-07 19:30:06.521714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2202086 -4.2010403 -4.1869421 -4.1840777 -4.192873 -4.2041802 -4.2187557 -4.2377453 -4.240644 -4.2180982 -4.17536 -4.1344743 -4.1218266 -4.1265149 -4.1289039][-4.2299566 -4.2117682 -4.1933146 -4.1867709 -4.1977305 -4.2146053 -4.235642 -4.2570515 -4.2585688 -4.236062 -4.1984625 -4.1697836 -4.1676025 -4.1739411 -4.1764803][-4.2522573 -4.2350645 -4.2147875 -4.2074757 -4.2168384 -4.2324376 -4.2555037 -4.2756786 -4.276659 -4.2570505 -4.221735 -4.1993546 -4.2024641 -4.2073483 -4.206336][-4.2661471 -4.246532 -4.2262707 -4.2202044 -4.2258325 -4.2352538 -4.2563052 -4.2760072 -4.2793889 -4.2610989 -4.2242827 -4.2058372 -4.2118683 -4.21746 -4.2105465][-4.2672544 -4.2474141 -4.2279363 -4.2160988 -4.2056556 -4.2019167 -4.2225194 -4.2444229 -4.2502322 -4.2300735 -4.1918983 -4.1823692 -4.1999269 -4.2111025 -4.1986728][-4.25129 -4.2281938 -4.1998382 -4.1651335 -4.1248126 -4.1053824 -4.1378236 -4.1747971 -4.1818514 -4.1541562 -4.1145368 -4.1234012 -4.1616678 -4.1831779 -4.1740637][-4.203999 -4.1788311 -4.1411271 -4.0804348 -4.0050721 -3.9747014 -4.0338621 -4.0962753 -4.1060452 -4.0653205 -4.0221281 -4.0553827 -4.1195927 -4.1555543 -4.1613379][-4.1514535 -4.1281114 -4.0860906 -4.0074048 -3.9123669 -3.8840489 -3.964344 -4.0421181 -4.060699 -4.0197167 -3.981154 -4.0306835 -4.1067777 -4.1541033 -4.1734405][-4.1248527 -4.1050758 -4.0689406 -3.997494 -3.9139619 -3.899446 -3.976459 -4.0499744 -4.078423 -4.0518036 -4.0254922 -4.0652614 -4.1264858 -4.1707458 -4.1939287][-4.1238031 -4.1093206 -4.0895224 -4.0474062 -4.001544 -3.9973159 -4.0515842 -4.1092567 -4.1383491 -4.1294127 -4.1175017 -4.1418624 -4.1790566 -4.2098346 -4.229846][-4.140182 -4.1361427 -4.1307817 -4.1160321 -4.099668 -4.1014519 -4.1322885 -4.168644 -4.1922984 -4.1953492 -4.1974754 -4.214972 -4.2356672 -4.2522216 -4.2631354][-4.1640534 -4.1651936 -4.1683769 -4.1689997 -4.1682882 -4.1734176 -4.1904097 -4.2106571 -4.22575 -4.2335329 -4.2423053 -4.2569489 -4.267549 -4.2753644 -4.2794003][-4.1878886 -4.1902914 -4.19554 -4.1997533 -4.2046962 -4.2087154 -4.2183681 -4.229023 -4.23852 -4.24743 -4.2573791 -4.2697248 -4.2785168 -4.2840481 -4.2865272][-4.2084303 -4.2092829 -4.2125511 -4.2166805 -4.2209816 -4.2246814 -4.2310281 -4.2394147 -4.2474937 -4.2547178 -4.2624655 -4.2725019 -4.2814741 -4.2884464 -4.2924924][-4.2214293 -4.2223778 -4.2248039 -4.2284489 -4.2313433 -4.2344689 -4.24032 -4.2491088 -4.2575192 -4.2646608 -4.2719398 -4.281991 -4.2913723 -4.2987261 -4.303308]]...]
INFO - root - 2017-12-07 19:30:13.355437: step 45410, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 53h:40m:14s remains)
INFO - root - 2017-12-07 19:30:20.147075: step 45420, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 51h:22m:34s remains)
INFO - root - 2017-12-07 19:30:26.948673: step 45430, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 51h:44m:53s remains)
INFO - root - 2017-12-07 19:30:33.756240: step 45440, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 59h:00m:20s remains)
INFO - root - 2017-12-07 19:30:40.569108: step 45450, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 57h:31m:20s remains)
INFO - root - 2017-12-07 19:30:47.406790: step 45460, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:49m:32s remains)
INFO - root - 2017-12-07 19:30:54.200525: step 45470, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 49h:21m:18s remains)
INFO - root - 2017-12-07 19:31:01.009044: step 45480, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 53h:42m:01s remains)
INFO - root - 2017-12-07 19:31:07.838612: step 45490, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 56h:01m:21s remains)
INFO - root - 2017-12-07 19:31:14.394276: step 45500, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 54h:28m:20s remains)
2017-12-07 19:31:15.079021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2139473 -4.2180791 -4.2251444 -4.2295156 -4.2318678 -4.2341166 -4.2330012 -4.2263212 -4.2124968 -4.1963015 -4.18697 -4.1783438 -4.1694741 -4.1650844 -4.1619749][-4.2165174 -4.224227 -4.2303996 -4.2266574 -4.2177839 -4.2097874 -4.2069607 -4.2095094 -4.2107077 -4.20738 -4.2061954 -4.2035966 -4.1957808 -4.1850271 -4.1740003][-4.2148485 -4.2222385 -4.2247071 -4.2120357 -4.1913347 -4.1740632 -4.1706491 -4.1834249 -4.2027879 -4.2154965 -4.2237649 -4.2238212 -4.2154756 -4.2006807 -4.1889625][-4.2160006 -4.2196178 -4.2177491 -4.1987491 -4.170186 -4.1439452 -4.1349955 -4.154366 -4.1882477 -4.2123947 -4.2253456 -4.2259879 -4.2187262 -4.2074642 -4.2000456][-4.2122836 -4.2073374 -4.1986876 -4.1749954 -4.1427522 -4.1068549 -4.0885043 -4.1112685 -4.1573029 -4.1922569 -4.2101769 -4.2098484 -4.2021546 -4.1971774 -4.196559][-4.2055311 -4.1909637 -4.1748891 -4.1459084 -4.106709 -4.0526729 -4.0159111 -4.0415821 -4.107532 -4.1603861 -4.189209 -4.1929255 -4.186233 -4.1853547 -4.1905832][-4.2086635 -4.1877069 -4.1630759 -4.1225834 -4.0656862 -3.984324 -3.920022 -3.945132 -4.0382743 -4.1188874 -4.167686 -4.1864929 -4.1870494 -4.1883712 -4.1941619][-4.2148547 -4.1921339 -4.1648254 -4.1210618 -4.0565352 -3.9624374 -3.8778124 -3.8959012 -4.00303 -4.101572 -4.1661787 -4.1991715 -4.2079816 -4.2104459 -4.2141376][-4.2169771 -4.198225 -4.1794181 -4.1512341 -4.106113 -4.0373287 -3.9711163 -3.9752383 -4.0515389 -4.1306729 -4.1875076 -4.2192874 -4.2316442 -4.2359381 -4.2363591][-4.2211661 -4.2102132 -4.201983 -4.1928167 -4.1744843 -4.1395292 -4.0970292 -4.0866647 -4.1227164 -4.1694374 -4.2085414 -4.2324619 -4.2452841 -4.2518806 -4.2530179][-4.2335629 -4.2300234 -4.2287068 -4.2295022 -4.224834 -4.2085786 -4.1784525 -4.1581683 -4.1685481 -4.1915932 -4.2139859 -4.2274113 -4.2401886 -4.2511544 -4.2567992][-4.2399945 -4.2416983 -4.2456656 -4.2495508 -4.2489948 -4.2410665 -4.2174463 -4.1914425 -4.1866732 -4.1959739 -4.2066488 -4.2126751 -4.2260189 -4.2429347 -4.2526646][-4.2275171 -4.2321115 -4.2407737 -4.2482815 -4.2509933 -4.2509923 -4.2384691 -4.2148805 -4.2029629 -4.2025805 -4.2031784 -4.2028103 -4.2164059 -4.2350631 -4.2432246][-4.2039318 -4.2077122 -4.2195644 -4.2291574 -4.23307 -4.2405519 -4.2426772 -4.2276044 -4.2154632 -4.2107091 -4.2064896 -4.2025924 -4.2139106 -4.2288618 -4.2344828][-4.1788783 -4.1823254 -4.1971221 -4.2061424 -4.207005 -4.2150016 -4.2249403 -4.2197051 -4.2133832 -4.2126665 -4.2093678 -4.20521 -4.2131734 -4.224493 -4.2315469]]...]
INFO - root - 2017-12-07 19:31:21.737220: step 45510, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 53h:11m:29s remains)
INFO - root - 2017-12-07 19:31:28.523188: step 45520, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 54h:05m:55s remains)
INFO - root - 2017-12-07 19:31:35.338192: step 45530, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 54h:00m:10s remains)
INFO - root - 2017-12-07 19:31:42.106443: step 45540, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 49h:42m:08s remains)
INFO - root - 2017-12-07 19:31:48.886931: step 45550, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 52h:27m:49s remains)
INFO - root - 2017-12-07 19:31:55.810285: step 45560, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 58h:08m:19s remains)
INFO - root - 2017-12-07 19:32:02.621575: step 45570, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 54h:29m:43s remains)
INFO - root - 2017-12-07 19:32:09.506263: step 45580, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 51h:42m:49s remains)
INFO - root - 2017-12-07 19:32:16.242942: step 45590, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 50h:09m:02s remains)
INFO - root - 2017-12-07 19:32:22.917439: step 45600, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 58h:08m:26s remains)
2017-12-07 19:32:23.700016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2623281 -4.2691169 -4.2615991 -4.2542048 -4.2512555 -4.2504907 -4.2486148 -4.249867 -4.2577567 -4.26502 -4.2694936 -4.2718239 -4.2676 -4.2643332 -4.2567306][-4.2412682 -4.2514176 -4.2469869 -4.2387376 -4.2368064 -4.2398586 -4.2407312 -4.240334 -4.2429633 -4.2464795 -4.2538738 -4.2599387 -4.25649 -4.2516918 -4.2453237][-4.21091 -4.2226 -4.2241173 -4.2190289 -4.2175074 -4.2219582 -4.2218323 -4.2199783 -4.2195039 -4.2206779 -4.2284894 -4.2338791 -4.2325373 -4.232492 -4.23386][-4.1696906 -4.1828384 -4.1899333 -4.1903162 -4.1908412 -4.1911154 -4.1832538 -4.1772923 -4.1788344 -4.1848168 -4.1896 -4.1889014 -4.1892986 -4.2008367 -4.2160873][-4.1454992 -4.1520715 -4.1545353 -4.153686 -4.1494203 -4.1362119 -4.1133614 -4.107461 -4.1206665 -4.1336293 -4.1324015 -4.121799 -4.1240549 -4.1474051 -4.1763616][-4.1484051 -4.1345034 -4.1198874 -4.10592 -4.087903 -4.0551538 -4.0158343 -4.01458 -4.0409036 -4.0594573 -4.0553451 -4.04454 -4.05537 -4.0896707 -4.1327105][-4.1488681 -4.1125588 -4.0785465 -4.0447984 -4.0059648 -3.9503775 -3.9016433 -3.9166832 -3.964792 -3.9989882 -4.005012 -4.0060453 -4.0318136 -4.0771661 -4.1283865][-4.1606965 -4.1186152 -4.0791717 -4.038981 -3.9944434 -3.9352384 -3.8960061 -3.9245195 -3.98573 -4.031096 -4.0461025 -4.057229 -4.0856228 -4.1281505 -4.1730833][-4.1876969 -4.1648669 -4.1421595 -4.1136861 -4.0828466 -4.0400405 -4.0147505 -4.0368767 -4.0824656 -4.1162195 -4.1252537 -4.1358 -4.1581931 -4.1913238 -4.2226777][-4.2169337 -4.2167377 -4.2144618 -4.2027392 -4.1847339 -4.1577954 -4.1408024 -4.1492615 -4.1729708 -4.191153 -4.1946917 -4.2012815 -4.2144608 -4.23477 -4.2517405][-4.2459173 -4.2577386 -4.2672658 -4.2663269 -4.2559347 -4.2377386 -4.2251616 -4.2238169 -4.2300386 -4.2375689 -4.2369437 -4.2377486 -4.2452092 -4.2574129 -4.265974][-4.2701988 -4.2825661 -4.2930474 -4.2971497 -4.2919283 -4.2816606 -4.2743793 -4.2716346 -4.273324 -4.2746363 -4.2679358 -4.260427 -4.2599168 -4.2666159 -4.2717052][-4.2816849 -4.2926612 -4.3006926 -4.3046536 -4.3029947 -4.2986755 -4.2957144 -4.2948408 -4.2963939 -4.2955208 -4.2870145 -4.2751927 -4.2691145 -4.269134 -4.2688313][-4.2857785 -4.295835 -4.3017921 -4.3041 -4.3032627 -4.3014412 -4.3004422 -4.3006115 -4.3016334 -4.3011651 -4.295403 -4.2860394 -4.2796965 -4.2761979 -4.2718954][-4.2896 -4.297699 -4.3021259 -4.3026834 -4.3010507 -4.2992759 -4.2988214 -4.2999277 -4.3019366 -4.303627 -4.3029714 -4.2998166 -4.2971 -4.294065 -4.287818]]...]
INFO - root - 2017-12-07 19:32:30.356949: step 45610, loss = 2.07, batch loss = 2.02 (13.1 examples/sec; 0.612 sec/batch; 48h:44m:44s remains)
INFO - root - 2017-12-07 19:32:37.151495: step 45620, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 52h:49m:02s remains)
INFO - root - 2017-12-07 19:32:43.992188: step 45630, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 58h:40m:25s remains)
INFO - root - 2017-12-07 19:32:50.802743: step 45640, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 54h:41m:19s remains)
INFO - root - 2017-12-07 19:32:57.635127: step 45650, loss = 2.09, batch loss = 2.04 (13.0 examples/sec; 0.617 sec/batch; 49h:11m:13s remains)
INFO - root - 2017-12-07 19:33:04.453315: step 45660, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 53h:05m:36s remains)
INFO - root - 2017-12-07 19:33:11.387230: step 45670, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 58h:43m:27s remains)
INFO - root - 2017-12-07 19:33:18.209318: step 45680, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 55h:24m:03s remains)
INFO - root - 2017-12-07 19:33:25.039899: step 45690, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 55h:23m:32s remains)
INFO - root - 2017-12-07 19:33:31.702643: step 45700, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 53h:16m:21s remains)
2017-12-07 19:33:32.435015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3335037 -4.3328209 -4.3252325 -4.31225 -4.2981462 -4.2872148 -4.28036 -4.2821255 -4.2864585 -4.2883015 -4.2895474 -4.2901077 -4.2900262 -4.2894917 -4.2888451][-4.3224454 -4.3202105 -4.3072839 -4.288734 -4.2691536 -4.253787 -4.2423258 -4.2428489 -4.2486477 -4.2527618 -4.257195 -4.2600851 -4.2606163 -4.2607241 -4.2617574][-4.3004122 -4.292357 -4.2714529 -4.2507157 -4.2311468 -4.21498 -4.2002964 -4.1992536 -4.20613 -4.2146425 -4.2244358 -4.2300382 -4.2310791 -4.2309885 -4.2345243][-4.2755156 -4.2573371 -4.2271156 -4.2055092 -4.1889033 -4.1751761 -4.1606116 -4.160646 -4.1692476 -4.1812053 -4.1953588 -4.2031369 -4.2038579 -4.2026634 -4.2077956][-4.2543187 -4.2296119 -4.1931953 -4.1728668 -4.1599851 -4.1522093 -4.14143 -4.1417456 -4.1524496 -4.1674623 -4.1831241 -4.1871781 -4.1851578 -4.1808486 -4.184432][-4.2314873 -4.1978664 -4.1509113 -4.12574 -4.1150308 -4.1144905 -4.1052237 -4.1054606 -4.1145062 -4.1293855 -4.1437235 -4.1443005 -4.13831 -4.1294012 -4.1300735][-4.2097373 -4.1584215 -4.0934877 -4.0575843 -4.0477133 -4.0477381 -4.031498 -4.0321889 -4.0425224 -4.0563159 -4.0701008 -4.06875 -4.0594144 -4.0486422 -4.0490074][-4.2014174 -4.1392879 -4.0641823 -4.0200734 -4.0063906 -3.99719 -3.9680705 -3.9607458 -3.9679289 -3.9778473 -3.9909482 -3.9922204 -3.9865069 -3.9831316 -3.9903355][-4.2099748 -4.1557684 -4.1015339 -4.0721416 -4.0545878 -4.0336118 -3.9947472 -3.9798543 -3.9806724 -3.9813819 -3.9880393 -3.9868851 -3.9834435 -3.9859791 -3.9919629][-4.2202954 -4.1803513 -4.1514292 -4.1383543 -4.1216445 -4.0971251 -4.0614719 -4.0515165 -4.058074 -4.0570722 -4.0552545 -4.0418096 -4.0260558 -4.0186734 -4.0159473][-4.2301011 -4.2025924 -4.190815 -4.1883035 -4.1764555 -4.1561351 -4.1292953 -4.1245675 -4.1362157 -4.1360168 -4.1281729 -4.1073685 -4.0835004 -4.0667052 -4.0572619][-4.2394691 -4.2183 -4.2112031 -4.2110214 -4.203681 -4.1895294 -4.1724715 -4.1741242 -4.1886744 -4.1906552 -4.1827269 -4.1666365 -4.1446161 -4.1263576 -4.1158047][-4.24906 -4.2312565 -4.2246995 -4.2242212 -4.2192845 -4.2071362 -4.1940174 -4.1972418 -4.210979 -4.216785 -4.2147236 -4.2068586 -4.1925812 -4.1810884 -4.1747913][-4.2588024 -4.2418661 -4.2337437 -4.2320952 -4.2264581 -4.2135572 -4.2020078 -4.2073693 -4.2201662 -4.2261944 -4.2296 -4.2306519 -4.2253838 -4.2225962 -4.2235785][-4.2690163 -4.2526779 -4.2434545 -4.2430983 -4.239007 -4.2287874 -4.2214017 -4.2286706 -4.2392945 -4.2431283 -4.2457919 -4.2494278 -4.2489958 -4.2487826 -4.2517242]]...]
INFO - root - 2017-12-07 19:33:39.251772: step 45710, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 53h:10m:56s remains)
INFO - root - 2017-12-07 19:33:45.928178: step 45720, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 52h:14m:43s remains)
INFO - root - 2017-12-07 19:33:52.572808: step 45730, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 52h:11m:56s remains)
INFO - root - 2017-12-07 19:33:59.345646: step 45740, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 52h:44m:30s remains)
INFO - root - 2017-12-07 19:34:06.199691: step 45750, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 56h:06m:17s remains)
INFO - root - 2017-12-07 19:34:12.917018: step 45760, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 55h:11m:52s remains)
INFO - root - 2017-12-07 19:34:19.795688: step 45770, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 51h:40m:28s remains)
INFO - root - 2017-12-07 19:34:26.579460: step 45780, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 52h:54m:07s remains)
INFO - root - 2017-12-07 19:34:33.409329: step 45790, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 54h:25m:20s remains)
INFO - root - 2017-12-07 19:34:40.014396: step 45800, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 57h:48m:18s remains)
2017-12-07 19:34:40.731668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3521214 -4.3507137 -4.3400979 -4.3212223 -4.2990918 -4.2788696 -4.257936 -4.2267618 -4.2155447 -4.2466755 -4.2695336 -4.2630072 -4.2284288 -4.1866517 -4.155118][-4.3513632 -4.3531284 -4.345623 -4.3274536 -4.3030143 -4.2771311 -4.2488885 -4.2085452 -4.1952286 -4.236836 -4.2676239 -4.2665124 -4.23582 -4.1968417 -4.1707244][-4.3459506 -4.3476377 -4.3403292 -4.3182144 -4.2876587 -4.2539954 -4.2197328 -4.1761141 -4.1682549 -4.2224431 -4.2650146 -4.2715673 -4.2390304 -4.1957712 -4.1671686][-4.3397923 -4.3382411 -4.3250694 -4.2931895 -4.2565432 -4.2146673 -4.1689878 -4.1203427 -4.1284738 -4.2022696 -4.2604079 -4.27764 -4.2455187 -4.1945791 -4.1573434][-4.3356485 -4.3308744 -4.3100929 -4.2662306 -4.219049 -4.1647806 -4.0988321 -4.0364389 -4.0658555 -4.1680894 -4.2478824 -4.2774148 -4.2499003 -4.1972733 -4.157352][-4.33163 -4.3240819 -4.2972627 -4.2415051 -4.176167 -4.0978317 -3.9963021 -3.9012475 -3.9503198 -4.0955939 -4.2051616 -4.2601104 -4.2521629 -4.2127976 -4.1786113][-4.3280482 -4.3170209 -4.2854891 -4.2215662 -4.1363668 -4.0341468 -3.8961658 -3.7521021 -3.8014784 -3.9921107 -4.1368423 -4.21604 -4.2365289 -4.2201209 -4.192719][-4.3274341 -4.3123226 -4.280499 -4.2192626 -4.1302772 -4.0213642 -3.8759875 -3.7046468 -3.716486 -3.9116411 -4.0694852 -4.1643848 -4.2092052 -4.2134786 -4.1925874][-4.3306093 -4.3146863 -4.2870259 -4.2388215 -4.1638012 -4.0740852 -3.961241 -3.8159778 -3.78809 -3.9253283 -4.05393 -4.1441131 -4.1981883 -4.2178245 -4.2073445][-4.3363261 -4.3222089 -4.2983079 -4.2611995 -4.2022123 -4.13956 -4.0645738 -3.9575472 -3.9182386 -3.9979959 -4.0843468 -4.1590595 -4.21137 -4.235723 -4.2362766][-4.3425937 -4.3328052 -4.3132644 -4.2845888 -4.239677 -4.1997247 -4.1530037 -4.0746131 -4.04035 -4.08341 -4.13292 -4.1828623 -4.2265921 -4.2543025 -4.2664938][-4.3479981 -4.3434587 -4.3302221 -4.3096619 -4.2771525 -4.2536674 -4.2274275 -4.1703897 -4.1421652 -4.1632395 -4.1810055 -4.2045822 -4.2365022 -4.2647748 -4.2852407][-4.349185 -4.347106 -4.3369303 -4.3180342 -4.2875962 -4.267756 -4.2510056 -4.2110314 -4.1909761 -4.2049937 -4.2086282 -4.2165713 -4.2366066 -4.2624249 -4.2868943][-4.3458962 -4.3417425 -4.3296995 -4.3069735 -4.2707353 -4.2460155 -4.2313647 -4.2009616 -4.1896677 -4.2054281 -4.2050743 -4.2037983 -4.2157307 -4.2394953 -4.2668028][-4.3410969 -4.3331337 -4.316463 -4.28806 -4.2435546 -4.2101703 -4.1893706 -4.1610365 -4.1585279 -4.1810856 -4.1821284 -4.1754222 -4.1827211 -4.2038789 -4.2316718]]...]
INFO - root - 2017-12-07 19:34:47.427631: step 45810, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.618 sec/batch; 49h:12m:32s remains)
INFO - root - 2017-12-07 19:34:54.152373: step 45820, loss = 2.11, batch loss = 2.05 (12.1 examples/sec; 0.661 sec/batch; 52h:38m:04s remains)
INFO - root - 2017-12-07 19:35:00.919743: step 45830, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 56h:44m:30s remains)
INFO - root - 2017-12-07 19:35:07.661928: step 45840, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 53h:35m:58s remains)
INFO - root - 2017-12-07 19:35:14.397406: step 45850, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 52h:45m:00s remains)
INFO - root - 2017-12-07 19:35:21.197300: step 45860, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 51h:38m:56s remains)
INFO - root - 2017-12-07 19:35:28.024510: step 45870, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.742 sec/batch; 59h:06m:31s remains)
INFO - root - 2017-12-07 19:35:34.885459: step 45880, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.771 sec/batch; 61h:25m:15s remains)
INFO - root - 2017-12-07 19:35:41.589540: step 45890, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 52h:41m:35s remains)
INFO - root - 2017-12-07 19:35:48.143373: step 45900, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 50h:45m:49s remains)
2017-12-07 19:35:48.866757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2615404 -4.2572889 -4.26123 -4.2682309 -4.2768865 -4.2765307 -4.2675371 -4.2593746 -4.2524633 -4.2587914 -4.26602 -4.28199 -4.30215 -4.3248162 -4.3366432][-4.2029948 -4.1957583 -4.2042456 -4.2132034 -4.2316484 -4.2362051 -4.2295327 -4.223114 -4.2135282 -4.2224936 -4.2341614 -4.2565031 -4.283165 -4.3130021 -4.3277764][-4.1431642 -4.1356368 -4.1499505 -4.1573224 -4.1857986 -4.1923661 -4.18621 -4.1802464 -4.1603003 -4.1680684 -4.1863012 -4.2173438 -4.2539606 -4.2933745 -4.3130336][-4.0894217 -4.0832639 -4.0986595 -4.0992994 -4.1354127 -4.1344509 -4.1225004 -4.1171179 -4.0827279 -4.0924239 -4.1234097 -4.1662416 -4.2126384 -4.2625542 -4.2904115][-4.0484452 -4.039957 -4.047708 -4.042223 -4.0810027 -4.0662441 -4.0452347 -4.03686 -3.9859371 -3.9948728 -4.0431395 -4.1018105 -4.15873 -4.2202544 -4.2584176][-4.02077 -4.0055618 -3.9917591 -3.9804442 -4.016202 -3.977119 -3.9392228 -3.923847 -3.850219 -3.8638153 -3.9404321 -4.0226283 -4.0950994 -4.1720185 -4.2227731][-4.0224953 -3.9992521 -3.9570239 -3.9324124 -3.9585404 -3.8871429 -3.8208206 -3.7894087 -3.6862741 -3.7130425 -3.8315554 -3.9409165 -4.0323081 -4.1256814 -4.1877441][-4.014472 -3.988353 -3.9330981 -3.9023619 -3.9164762 -3.8240242 -3.7365577 -3.6925747 -3.5810132 -3.6284726 -3.7802796 -3.9034686 -4.0042844 -4.1053581 -4.1721315][-4.0418477 -4.0224843 -3.9723189 -3.9455056 -3.9464507 -3.8455167 -3.7558761 -3.7073402 -3.6141288 -3.6797924 -3.8277643 -3.9405093 -4.032227 -4.1235824 -4.1839767][-4.1126809 -4.1088309 -4.082428 -4.0701456 -4.0657563 -3.975961 -3.898531 -3.847501 -3.7812243 -3.8427842 -3.9475141 -4.0311251 -4.100596 -4.1698828 -4.2157111][-4.1618571 -4.1633162 -4.15278 -4.1503911 -4.1480923 -4.0826 -4.0287361 -3.9856362 -3.9443512 -3.9960356 -4.0684161 -4.1283231 -4.179769 -4.2276554 -4.2568307][-4.2006121 -4.2031813 -4.2006 -4.2041593 -4.2077785 -4.1640739 -4.1277637 -4.0931554 -4.0706081 -4.1104193 -4.1573377 -4.1987824 -4.2373018 -4.2701735 -4.2889991][-4.2329617 -4.2370958 -4.2430434 -4.2504344 -4.2602806 -4.2412438 -4.220953 -4.1968713 -4.1870728 -4.2138848 -4.2407579 -4.2663136 -4.2870951 -4.3035674 -4.3139005][-4.2529178 -4.255085 -4.2621422 -4.270402 -4.2818422 -4.2778168 -4.2696686 -4.2586312 -4.2572193 -4.2738276 -4.289062 -4.30507 -4.314723 -4.3218937 -4.3289032][-4.2800221 -4.2776985 -4.2803278 -4.2841716 -4.2902117 -4.2887158 -4.2834826 -4.2789021 -4.2811742 -4.2914524 -4.3016057 -4.3131447 -4.32253 -4.3303947 -4.33763]]...]
INFO - root - 2017-12-07 19:35:55.492685: step 45910, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 57h:25m:24s remains)
INFO - root - 2017-12-07 19:36:02.314128: step 45920, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 56h:32m:26s remains)
INFO - root - 2017-12-07 19:36:09.036325: step 45930, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 51h:36m:00s remains)
INFO - root - 2017-12-07 19:36:15.923652: step 45940, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 52h:56m:49s remains)
INFO - root - 2017-12-07 19:36:22.729789: step 45950, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 55h:25m:29s remains)
INFO - root - 2017-12-07 19:36:29.529553: step 45960, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.739 sec/batch; 58h:47m:07s remains)
INFO - root - 2017-12-07 19:36:36.328745: step 45970, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 53h:05m:36s remains)
INFO - root - 2017-12-07 19:36:42.996192: step 45980, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.615 sec/batch; 48h:59m:04s remains)
INFO - root - 2017-12-07 19:36:49.749821: step 45990, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 51h:45m:56s remains)
INFO - root - 2017-12-07 19:36:56.376779: step 46000, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 58h:20m:34s remains)
2017-12-07 19:36:57.205640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1290255 -4.12708 -4.1179614 -4.1148524 -4.1216917 -4.1269493 -4.1410289 -4.1553278 -4.1468396 -4.1220169 -4.098722 -4.0921168 -4.0873833 -4.0780272 -4.0634389][-4.1356745 -4.1375356 -4.1278791 -4.1155496 -4.1109257 -4.1082 -4.1205068 -4.1421227 -4.15109 -4.1426253 -4.1291895 -4.1260962 -4.124814 -4.1269827 -4.1275244][-4.1115522 -4.1163836 -4.1081824 -4.0924664 -4.0858283 -4.0812354 -4.0983906 -4.1334958 -4.1628385 -4.1718316 -4.1695681 -4.1727595 -4.17964 -4.1923304 -4.2052746][-4.0885653 -4.0910344 -4.079803 -4.0633178 -4.0584087 -4.0553646 -4.0790877 -4.1239491 -4.1649523 -4.1882086 -4.2027431 -4.2172513 -4.2325697 -4.2487445 -4.2670126][-4.0865674 -4.08646 -4.07046 -4.0466304 -4.0353236 -4.0308485 -4.0530133 -4.097044 -4.1410909 -4.1811085 -4.2147884 -4.245101 -4.2715979 -4.2901626 -4.3088589][-4.1114488 -4.1075268 -4.0863729 -4.0518842 -4.0231586 -4.0074263 -4.020556 -4.056612 -4.1018968 -4.1530871 -4.2019224 -4.2456007 -4.2839403 -4.3073664 -4.3229513][-4.1483164 -4.1327467 -4.1000295 -4.050972 -4.0015621 -3.971648 -3.9793491 -4.0180984 -4.0688992 -4.1261063 -4.182663 -4.2339935 -4.2780652 -4.3060026 -4.3185163][-4.182765 -4.152319 -4.1053171 -4.04559 -3.9800444 -3.9369361 -3.9450095 -3.9920776 -4.0504212 -4.1093831 -4.1678219 -4.2217908 -4.2655182 -4.2935023 -4.3031645][-4.2118073 -4.1673722 -4.1082039 -4.0426149 -3.9735157 -3.9289961 -3.9348702 -3.9788604 -4.0358462 -4.0950737 -4.1541977 -4.2103753 -4.2554989 -4.2823453 -4.2903152][-4.2300816 -4.1723952 -4.1063457 -4.0441136 -3.9847944 -3.9495783 -3.9540741 -3.9869874 -4.035151 -4.094048 -4.1550064 -4.2132449 -4.2591085 -4.2830348 -4.2869921][-4.243875 -4.1821451 -4.1155953 -4.0580988 -4.0089655 -3.9799247 -3.9812768 -4.0093045 -4.0570722 -4.1190591 -4.1832132 -4.2399683 -4.2795029 -4.295403 -4.2916784][-4.2676654 -4.2118616 -4.1484513 -4.0941668 -4.0474548 -4.0180478 -4.019362 -4.0489125 -4.101181 -4.1655498 -4.2254605 -4.2740531 -4.3034391 -4.3088856 -4.2959962][-4.2932239 -4.2524991 -4.2034063 -4.1614184 -4.1231108 -4.0990772 -4.1003504 -4.1239214 -4.1667695 -4.21841 -4.2653079 -4.3006291 -4.3189607 -4.3150315 -4.2921562][-4.3118267 -4.2883587 -4.2582607 -4.2326155 -4.2089067 -4.1959333 -4.1988778 -4.2139034 -4.239058 -4.2711606 -4.3009024 -4.3208623 -4.3253865 -4.3102226 -4.277956][-4.3213949 -4.3069844 -4.2880721 -4.2719254 -4.2585664 -4.2544751 -4.2603083 -4.2727566 -4.2900753 -4.3091364 -4.3237576 -4.328227 -4.3176079 -4.289362 -4.2497535]]...]
INFO - root - 2017-12-07 19:37:03.999764: step 46010, loss = 2.03, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 51h:42m:56s remains)
INFO - root - 2017-12-07 19:37:10.765575: step 46020, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 56h:54m:40s remains)
INFO - root - 2017-12-07 19:37:17.551564: step 46030, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 57h:15m:25s remains)
INFO - root - 2017-12-07 19:37:24.364410: step 46040, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 56h:36m:42s remains)
INFO - root - 2017-12-07 19:37:31.182860: step 46050, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 49h:50m:28s remains)
INFO - root - 2017-12-07 19:37:37.988588: step 46060, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 54h:32m:32s remains)
INFO - root - 2017-12-07 19:37:44.843142: step 46070, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 57h:19m:59s remains)
INFO - root - 2017-12-07 19:37:51.668551: step 46080, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 54h:19m:48s remains)
INFO - root - 2017-12-07 19:37:58.388754: step 46090, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 50h:33m:34s remains)
INFO - root - 2017-12-07 19:38:04.980270: step 46100, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 51h:31m:02s remains)
2017-12-07 19:38:05.712894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3063307 -4.321887 -4.31668 -4.3034868 -4.288168 -4.27081 -4.2669764 -4.2778096 -4.2900343 -4.2997003 -4.3081908 -4.3073149 -4.3049984 -4.3067408 -4.305881][-4.303863 -4.3179264 -4.3111472 -4.2935681 -4.270082 -4.2437239 -4.2368627 -4.2498841 -4.2682056 -4.2848148 -4.3002753 -4.303978 -4.3030272 -4.3055487 -4.3055596][-4.295155 -4.3065758 -4.2970352 -4.2730317 -4.2355456 -4.1910315 -4.1749616 -4.19226 -4.2211657 -4.2474108 -4.2696109 -4.2829576 -4.2862792 -4.2865653 -4.2866421][-4.2946787 -4.3028584 -4.2918348 -4.2654142 -4.2160811 -4.14881 -4.1088157 -4.1203289 -4.1603885 -4.1934977 -4.2237272 -4.2466884 -4.2591267 -4.2613816 -4.2618141][-4.3080821 -4.3140974 -4.30094 -4.2784114 -4.2272544 -4.1377382 -4.0548196 -4.0441346 -4.0901747 -4.1292696 -4.1668773 -4.1980796 -4.2222061 -4.2343616 -4.2399244][-4.3134685 -4.3170071 -4.2990661 -4.2798014 -4.2307134 -4.1242094 -3.9916825 -3.9465055 -3.9984124 -4.0491614 -4.0948057 -4.1328053 -4.1674876 -4.1989436 -4.2174768][-4.3037896 -4.2991743 -4.2681961 -4.2405019 -4.1877513 -4.0627875 -3.8829465 -3.8060374 -3.8862219 -3.9645069 -4.020411 -4.0659189 -4.1158218 -4.1702127 -4.2030473][-4.2877769 -4.2683234 -4.2206993 -4.1830444 -4.128016 -3.9998708 -3.8108141 -3.7377257 -3.8434641 -3.9416418 -4.0100513 -4.0673113 -4.1294661 -4.1921711 -4.2270026][-4.2644606 -4.2307692 -4.175004 -4.1349669 -4.0870628 -3.9879742 -3.8573377 -3.8257163 -3.9128494 -3.9945312 -4.0605688 -4.1204939 -4.1773458 -4.2295218 -4.2554574][-4.2463732 -4.2134061 -4.167376 -4.1366959 -4.1053243 -4.0504923 -3.9933558 -4.0047336 -4.0653248 -4.1178708 -4.167603 -4.2140779 -4.2523227 -4.2845221 -4.2980461][-4.236856 -4.2118874 -4.1815643 -4.169394 -4.1573009 -4.13471 -4.1271124 -4.1679616 -4.2120037 -4.2401037 -4.2708011 -4.2995133 -4.321785 -4.3362474 -4.3402143][-4.2212734 -4.2003803 -4.180572 -4.1786776 -4.1773138 -4.16848 -4.18469 -4.2370753 -4.2704749 -4.2826948 -4.2956119 -4.3069282 -4.3169565 -4.3229823 -4.3232322][-4.1982064 -4.1801925 -4.1689606 -4.172183 -4.1706748 -4.1646867 -4.1814451 -4.2257094 -4.2454691 -4.2451115 -4.2452044 -4.247828 -4.2515078 -4.2548327 -4.2560873][-4.1803055 -4.1678672 -4.16438 -4.1690865 -4.1615448 -4.1513271 -4.15854 -4.1866012 -4.1967769 -4.1927328 -4.189909 -4.1901059 -4.1912084 -4.1931868 -4.1951561][-4.1862297 -4.1826525 -4.1853771 -4.19193 -4.1850333 -4.1738706 -4.1760674 -4.1923847 -4.1973968 -4.1957359 -4.1958427 -4.19654 -4.197824 -4.1990247 -4.199297]]...]
INFO - root - 2017-12-07 19:38:12.504255: step 46110, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 51h:59m:03s remains)
INFO - root - 2017-12-07 19:38:19.226979: step 46120, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 50h:38m:35s remains)
INFO - root - 2017-12-07 19:38:25.940318: step 46130, loss = 2.06, batch loss = 2.00 (13.6 examples/sec; 0.589 sec/batch; 46h:51m:56s remains)
INFO - root - 2017-12-07 19:38:32.691318: step 46140, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 58h:29m:15s remains)
INFO - root - 2017-12-07 19:38:39.455309: step 46150, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 53h:20m:01s remains)
INFO - root - 2017-12-07 19:38:46.223045: step 46160, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 51h:13m:50s remains)
INFO - root - 2017-12-07 19:38:52.992641: step 46170, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 50h:14m:37s remains)
INFO - root - 2017-12-07 19:38:59.923418: step 46180, loss = 2.11, batch loss = 2.05 (10.9 examples/sec; 0.732 sec/batch; 58h:13m:35s remains)
INFO - root - 2017-12-07 19:39:06.764331: step 46190, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 57h:42m:11s remains)
INFO - root - 2017-12-07 19:39:13.277823: step 46200, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 52h:21m:21s remains)
2017-12-07 19:39:14.100732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2620945 -4.2543864 -4.2390766 -4.2327867 -4.2312336 -4.2290754 -4.2445259 -4.2741485 -4.290329 -4.2834177 -4.2607679 -4.2250762 -4.1898503 -4.1824789 -4.187737][-4.2471561 -4.2346511 -4.2163615 -4.215488 -4.2217503 -4.2194028 -4.2314863 -4.2612252 -4.2703962 -4.2555294 -4.2270575 -4.1864443 -4.1535254 -4.1572337 -4.1740232][-4.2374034 -4.2275524 -4.2110739 -4.2141418 -4.2228551 -4.2155213 -4.2194424 -4.2440953 -4.246604 -4.2268233 -4.1907153 -4.1432853 -4.1088419 -4.1159449 -4.1403494][-4.2418294 -4.2452583 -4.23728 -4.2419887 -4.2462931 -4.2294612 -4.22062 -4.230217 -4.2194071 -4.1978307 -4.1589775 -4.1099496 -4.0787048 -4.087954 -4.1134462][-4.2583981 -4.2781429 -4.2752366 -4.2749915 -4.2662196 -4.2343178 -4.2100368 -4.1985269 -4.1739817 -4.1554141 -4.1227846 -4.0788827 -4.0583477 -4.0738726 -4.0986066][-4.2756987 -4.3069129 -4.3009586 -4.284977 -4.2612886 -4.2148452 -4.1682148 -4.1309361 -4.0903187 -4.0804777 -4.0696096 -4.0466924 -4.04418 -4.0670195 -4.0957847][-4.3013067 -4.3306756 -4.3167114 -4.2808146 -4.2344813 -4.1583924 -4.0701895 -3.9999244 -3.95817 -3.9805465 -4.0134258 -4.0266171 -4.0459852 -4.0775361 -4.1130991][-4.3382878 -4.3587852 -4.3315234 -4.2745304 -4.1975307 -4.0720439 -3.9260035 -3.8231912 -3.793201 -3.8660433 -3.9625356 -4.0298381 -4.0767674 -4.1139874 -4.1486497][-4.3655682 -4.3729529 -4.3353395 -4.2601242 -4.1573477 -3.999126 -3.8211498 -3.7043991 -3.6884327 -3.79599 -3.9414158 -4.0487127 -4.120882 -4.1625214 -4.193233][-4.3643432 -4.3583679 -4.3181129 -4.24388 -4.1474466 -4.0066376 -3.8617697 -3.7796128 -3.7776601 -3.8714221 -4.0029249 -4.10445 -4.1741347 -4.21092 -4.2346215][-4.34008 -4.3267179 -4.2909551 -4.2321754 -4.1592946 -4.058609 -3.9683347 -3.935117 -3.9503725 -4.0197673 -4.1133471 -4.1871843 -4.2364645 -4.2571006 -4.2674994][-4.3140082 -4.2997942 -4.2742972 -4.2382112 -4.191782 -4.1282811 -4.0806923 -4.073976 -4.0976734 -4.1506581 -4.2137084 -4.2594252 -4.2822585 -4.2820525 -4.2760234][-4.2962346 -4.2874928 -4.2756205 -4.262886 -4.2395296 -4.2013717 -4.1743588 -4.1735272 -4.1934404 -4.23002 -4.2730088 -4.2981992 -4.301065 -4.2842693 -4.2658968][-4.2804871 -4.2824059 -4.2872443 -4.2963681 -4.2935843 -4.2722468 -4.2497368 -4.2428594 -4.253747 -4.2738571 -4.3021641 -4.3125639 -4.3013787 -4.2730737 -4.2486606][-4.2577991 -4.271801 -4.2934003 -4.3163924 -4.32525 -4.3138285 -4.2929349 -4.2797422 -4.2803483 -4.29056 -4.308919 -4.3106451 -4.2882748 -4.2503104 -4.2208014]]...]
INFO - root - 2017-12-07 19:39:20.990936: step 46210, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 56h:29m:30s remains)
INFO - root - 2017-12-07 19:39:27.707750: step 46220, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 51h:50m:38s remains)
INFO - root - 2017-12-07 19:39:34.514882: step 46230, loss = 2.09, batch loss = 2.04 (12.6 examples/sec; 0.634 sec/batch; 50h:25m:56s remains)
INFO - root - 2017-12-07 19:39:41.308869: step 46240, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 50h:25m:34s remains)
INFO - root - 2017-12-07 19:39:48.153476: step 46250, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.730 sec/batch; 58h:03m:46s remains)
INFO - root - 2017-12-07 19:39:54.935966: step 46260, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 56h:48m:06s remains)
INFO - root - 2017-12-07 19:40:01.773967: step 46270, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 51h:51m:10s remains)
INFO - root - 2017-12-07 19:40:08.502167: step 46280, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.627 sec/batch; 49h:49m:53s remains)
INFO - root - 2017-12-07 19:40:15.390554: step 46290, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 57h:57m:51s remains)
INFO - root - 2017-12-07 19:40:21.941415: step 46300, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 57h:15m:37s remains)
2017-12-07 19:40:22.781193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2196856 -4.2285085 -4.2224674 -4.2366896 -4.2620974 -4.2738986 -4.2615213 -4.2344303 -4.2274332 -4.2375226 -4.25033 -4.258822 -4.2611246 -4.259871 -4.26715][-4.2196879 -4.2279735 -4.2236137 -4.242023 -4.267148 -4.2736621 -4.2578678 -4.2274127 -4.2215991 -4.2395687 -4.2614212 -4.2748 -4.2808018 -4.2779655 -4.2787023][-4.2258043 -4.2286 -4.2228832 -4.2371883 -4.2562094 -4.25728 -4.2424011 -4.2183561 -4.2179589 -4.2418141 -4.2716465 -4.295001 -4.3089728 -4.3069091 -4.3032465][-4.2148848 -4.2112651 -4.2062593 -4.2161674 -4.2301888 -4.2314081 -4.2224464 -4.2102618 -4.2147813 -4.2396154 -4.271142 -4.3000765 -4.3177605 -4.318037 -4.3159823][-4.1804619 -4.1740603 -4.1715488 -4.1807489 -4.1930456 -4.1970663 -4.1982861 -4.1976061 -4.2034593 -4.225337 -4.2573266 -4.2879362 -4.3067713 -4.3109207 -4.3135996][-4.1603336 -4.1523519 -4.1491332 -4.1560469 -4.16737 -4.1747131 -4.1818151 -4.1867085 -4.194983 -4.2171082 -4.2479315 -4.2757306 -4.2915869 -4.2981706 -4.3040524][-4.1497407 -4.1429729 -4.1380825 -4.1393805 -4.150497 -4.161644 -4.1681881 -4.171741 -4.1829629 -4.2075882 -4.2362862 -4.2613688 -4.2753973 -4.2840123 -4.2925372][-4.1255331 -4.1185036 -4.105864 -4.0986233 -4.1062155 -4.1179366 -4.1199918 -4.1238384 -4.143364 -4.1725025 -4.2029738 -4.2313538 -4.2492995 -4.2631812 -4.278358][-4.1049275 -4.0894647 -4.0664306 -4.0500813 -4.0520406 -4.0597539 -4.0535707 -4.0546355 -4.0773325 -4.1072145 -4.1373787 -4.1701884 -4.1972418 -4.2232523 -4.2496934][-4.1291494 -4.1005573 -4.066853 -4.0444613 -4.0444036 -4.0507216 -4.0384083 -4.0264544 -4.0354486 -4.0522532 -4.0736094 -4.1083117 -4.1457443 -4.1834865 -4.2203174][-4.1851845 -4.1498747 -4.113205 -4.0878596 -4.0842638 -4.0879436 -4.0723209 -4.0496006 -4.0421515 -4.0468135 -4.0587654 -4.088779 -4.1282072 -4.1689529 -4.2096949][-4.2205777 -4.1944432 -4.1662555 -4.144114 -4.1387215 -4.1399722 -4.1221724 -4.0954714 -4.0834122 -4.0839133 -4.0917416 -4.1174555 -4.1532736 -4.1882391 -4.2233906][-4.2284927 -4.2144256 -4.195796 -4.1767406 -4.1677179 -4.1609712 -4.1380005 -4.112318 -4.1068163 -4.1160369 -4.1310205 -4.1616964 -4.197536 -4.22657 -4.2535334][-4.2365193 -4.2306 -4.2181249 -4.2019868 -4.188714 -4.1737452 -4.14819 -4.12781 -4.1334348 -4.1543732 -4.1779518 -4.2148747 -4.2492738 -4.2704291 -4.2880111][-4.2386146 -4.237186 -4.2312226 -4.2259946 -4.2201023 -4.2070432 -4.1860223 -4.1723585 -4.1818991 -4.204195 -4.2276554 -4.2626662 -4.2934909 -4.3082004 -4.317781]]...]
INFO - root - 2017-12-07 19:40:29.496747: step 46310, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 50h:50m:57s remains)
INFO - root - 2017-12-07 19:40:36.276456: step 46320, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 58h:07m:46s remains)
INFO - root - 2017-12-07 19:40:43.037973: step 46330, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 58h:54m:09s remains)
INFO - root - 2017-12-07 19:40:49.833140: step 46340, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 54h:45m:04s remains)
INFO - root - 2017-12-07 19:40:56.661950: step 46350, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 53h:28m:49s remains)
INFO - root - 2017-12-07 19:41:03.522232: step 46360, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:39m:41s remains)
INFO - root - 2017-12-07 19:41:10.367450: step 46370, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 55h:20m:36s remains)
INFO - root - 2017-12-07 19:41:17.164019: step 46380, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 56h:43m:44s remains)
INFO - root - 2017-12-07 19:41:23.845252: step 46390, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 52h:33m:17s remains)
INFO - root - 2017-12-07 19:41:30.494379: step 46400, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 51h:16m:36s remains)
2017-12-07 19:41:31.255427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3430724 -4.3215122 -4.3116837 -4.300518 -4.2774992 -4.2473593 -4.2173018 -4.20158 -4.1958528 -4.1973095 -4.200057 -4.2106686 -4.2290273 -4.2546496 -4.279942][-4.3465915 -4.3275194 -4.3196864 -4.3038845 -4.2744727 -4.2353177 -4.1960173 -4.1781373 -4.1716013 -4.1725855 -4.1780663 -4.1931806 -4.2177539 -4.2492261 -4.2780175][-4.349884 -4.3342509 -4.3289595 -4.3123555 -4.2814703 -4.2359242 -4.1854343 -4.1636109 -4.1568737 -4.1575613 -4.1651936 -4.1850753 -4.2142982 -4.2478905 -4.2785759][-4.3537488 -4.342001 -4.3388786 -4.3234324 -4.2922149 -4.2422943 -4.1831069 -4.1538773 -4.1429944 -4.1429067 -4.1530924 -4.1803689 -4.2134032 -4.2432346 -4.2725163][-4.3578606 -4.3484983 -4.34617 -4.331327 -4.301271 -4.246933 -4.1790771 -4.136055 -4.115253 -4.1164412 -4.13616 -4.1692419 -4.2028127 -4.228631 -4.2569361][-4.3633752 -4.3538065 -4.346272 -4.3257709 -4.2937541 -4.2353339 -4.159936 -4.1010919 -4.0680771 -4.0780187 -4.1166234 -4.1559372 -4.1875792 -4.2100468 -4.2380924][-4.3705597 -4.360661 -4.3438187 -4.3178043 -4.2825875 -4.2204795 -4.13535 -4.0600367 -4.0185118 -4.0434222 -4.0965023 -4.1432972 -4.1779413 -4.2028117 -4.2332916][-4.37961 -4.3714175 -4.3490205 -4.3211102 -4.2826657 -4.2205853 -4.1311741 -4.0486822 -4.0089192 -4.0409412 -4.0941925 -4.1414909 -4.1795425 -4.2085304 -4.2413158][-4.3866873 -4.3835692 -4.3597097 -4.330287 -4.2898293 -4.2361078 -4.16203 -4.0872126 -4.0453434 -4.0655608 -4.1045761 -4.138948 -4.1704316 -4.2031794 -4.2403946][-4.3869286 -4.3875337 -4.3657804 -4.3350596 -4.2933865 -4.2465639 -4.1937203 -4.139626 -4.0966859 -4.0963926 -4.1117778 -4.1238141 -4.1441288 -4.1786976 -4.2237787][-4.3790011 -4.3788605 -4.3617744 -4.3349266 -4.2949619 -4.250555 -4.2089438 -4.1731715 -4.13481 -4.1228604 -4.1215668 -4.1205759 -4.1323485 -4.1668706 -4.2150803][-4.3682461 -4.3669143 -4.35577 -4.33782 -4.3069797 -4.2691479 -4.2340016 -4.2069139 -4.1741691 -4.1570745 -4.1482534 -4.14515 -4.1565957 -4.1888733 -4.2341819][-4.3581972 -4.3574991 -4.3541937 -4.3454008 -4.3219962 -4.289732 -4.2595196 -4.2363219 -4.2073669 -4.1914721 -4.1843357 -4.1839643 -4.1961832 -4.2243662 -4.262085][-4.3509994 -4.3489237 -4.3465672 -4.3405857 -4.3217964 -4.2966428 -4.2749586 -4.2579665 -4.2357469 -4.2237983 -4.2187896 -4.2183666 -4.2277679 -4.2510791 -4.282269][-4.3499146 -4.34475 -4.3403745 -4.3328681 -4.3161888 -4.2985439 -4.2821159 -4.2711024 -4.2609282 -4.2566857 -4.255446 -4.2537303 -4.2587404 -4.2742047 -4.2973909]]...]
INFO - root - 2017-12-07 19:41:37.972106: step 46410, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 53h:08m:53s remains)
INFO - root - 2017-12-07 19:41:44.820007: step 46420, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 53h:29m:13s remains)
INFO - root - 2017-12-07 19:41:51.539064: step 46430, loss = 2.11, batch loss = 2.05 (12.3 examples/sec; 0.648 sec/batch; 51h:31m:06s remains)
INFO - root - 2017-12-07 19:41:58.168019: step 46440, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 51h:51m:44s remains)
INFO - root - 2017-12-07 19:42:04.922960: step 46450, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 56h:57m:34s remains)
INFO - root - 2017-12-07 19:42:11.736693: step 46460, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 52h:32m:46s remains)
INFO - root - 2017-12-07 19:42:18.491026: step 46470, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 52h:13m:08s remains)
INFO - root - 2017-12-07 19:42:25.418853: step 46480, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 57h:56m:53s remains)
INFO - root - 2017-12-07 19:42:32.261905: step 46490, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 57h:49m:39s remains)
INFO - root - 2017-12-07 19:42:38.857598: step 46500, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 52h:58m:30s remains)
2017-12-07 19:42:39.678733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3267326 -4.3266706 -4.3204641 -4.3123617 -4.3020535 -4.2892709 -4.2792139 -4.2744322 -4.2737021 -4.2815666 -4.2906237 -4.2959433 -4.2958183 -4.2966824 -4.3023572][-4.3193736 -4.3199005 -4.3117161 -4.3008208 -4.2843108 -4.2631416 -4.2466335 -4.2413893 -4.2427883 -4.2563949 -4.2704749 -4.2752504 -4.2692761 -4.2645931 -4.2715006][-4.31027 -4.3100128 -4.3001409 -4.2840495 -4.2571092 -4.2242689 -4.1989646 -4.1964116 -4.2048216 -4.2256484 -4.2479687 -4.253231 -4.242806 -4.2329965 -4.2406321][-4.3001075 -4.29492 -4.2811418 -4.2556934 -4.214623 -4.1692581 -4.1344724 -4.1357522 -4.157198 -4.1929927 -4.2309465 -4.2432208 -4.2332115 -4.2197876 -4.2247534][-4.2959085 -4.2825217 -4.2609549 -4.22049 -4.1612115 -4.0984817 -4.0481291 -4.0550818 -4.0991688 -4.1608191 -4.222599 -4.2517934 -4.248745 -4.2337947 -4.2309227][-4.2868786 -4.2593522 -4.2223582 -4.157166 -4.0689373 -3.9829242 -3.919873 -3.9440455 -4.0230141 -4.1193004 -4.20816 -4.2577462 -4.2657657 -4.2533679 -4.2445369][-4.2732859 -4.2286878 -4.1719837 -4.0799961 -3.9636154 -3.8570046 -3.786886 -3.8287392 -3.9418244 -4.0720291 -4.1840587 -4.2492466 -4.2662272 -4.2591572 -4.250648][-4.259407 -4.202467 -4.1324167 -4.0245962 -3.8944256 -3.7809136 -3.7106853 -3.7511063 -3.8758423 -4.0229712 -4.1457233 -4.21955 -4.2451849 -4.2451029 -4.243185][-4.2567849 -4.1957636 -4.1220741 -4.0161209 -3.898083 -3.7995405 -3.7390058 -3.7677245 -3.879101 -4.0158706 -4.1304197 -4.2014923 -4.2320919 -4.2346511 -4.2360721][-4.2629552 -4.2087169 -4.145123 -4.058589 -3.9693203 -3.8928318 -3.8459868 -3.8706002 -3.960418 -4.0713806 -4.1649632 -4.2224007 -4.2449937 -4.2394462 -4.236433][-4.2701688 -4.2262025 -4.1785994 -4.1196074 -4.0651317 -4.0121861 -3.9770923 -3.9975438 -4.0655942 -4.1474676 -4.2190437 -4.2578897 -4.265646 -4.2513714 -4.2421227][-4.2844148 -4.2514954 -4.2190189 -4.1844931 -4.1579175 -4.1254745 -4.1016893 -4.1177368 -4.1650519 -4.2204847 -4.2702346 -4.2930293 -4.2903552 -4.2715569 -4.2588148][-4.3026109 -4.280416 -4.258666 -4.2417378 -4.2339144 -4.2186089 -4.2039642 -4.2110887 -4.2384744 -4.2705874 -4.300139 -4.3109069 -4.3054047 -4.2880731 -4.2737007][-4.317801 -4.3034744 -4.2885318 -4.2797318 -4.2793255 -4.275209 -4.2692165 -4.2732906 -4.2874403 -4.3038239 -4.3182774 -4.3219714 -4.3176489 -4.3046727 -4.292644][-4.3310676 -4.3227472 -4.3121781 -4.3061 -4.3067551 -4.3081837 -4.3083248 -4.3129983 -4.3220139 -4.3299713 -4.3344617 -4.3342109 -4.3307552 -4.3230004 -4.3167515]]...]
INFO - root - 2017-12-07 19:42:46.482858: step 46510, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 55h:42m:15s remains)
INFO - root - 2017-12-07 19:42:53.286164: step 46520, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.686 sec/batch; 54h:31m:20s remains)
INFO - root - 2017-12-07 19:42:59.934855: step 46530, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 53h:31m:51s remains)
INFO - root - 2017-12-07 19:43:06.774072: step 46540, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 51h:19m:07s remains)
INFO - root - 2017-12-07 19:43:13.621128: step 46550, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 53h:11m:08s remains)
INFO - root - 2017-12-07 19:43:20.414261: step 46560, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.736 sec/batch; 58h:28m:24s remains)
INFO - root - 2017-12-07 19:43:27.115219: step 46570, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 49h:18m:12s remains)
INFO - root - 2017-12-07 19:43:33.843053: step 46580, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 51h:48m:58s remains)
INFO - root - 2017-12-07 19:43:40.539639: step 46590, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 50h:22m:41s remains)
INFO - root - 2017-12-07 19:43:47.082558: step 46600, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 50h:16m:04s remains)
2017-12-07 19:43:47.858862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3172126 -4.3035803 -4.282011 -4.2533112 -4.2450061 -4.2344174 -4.2247419 -4.2259541 -4.2490788 -4.2776222 -4.2876792 -4.2852669 -4.2821493 -4.2728639 -4.2608457][-4.3230195 -4.3155193 -4.2971492 -4.2679172 -4.2593079 -4.250988 -4.2395434 -4.2339015 -4.2470727 -4.2690768 -4.2738504 -4.2699885 -4.2649288 -4.2518015 -4.24193][-4.3150873 -4.3131475 -4.2996664 -4.2726769 -4.2612634 -4.2510824 -4.2403884 -4.2336211 -4.2403049 -4.2556572 -4.2538996 -4.2457805 -4.2353425 -4.2180085 -4.2142811][-4.2845798 -4.2814517 -4.2736855 -4.25199 -4.2384667 -4.2190919 -4.2023664 -4.1888862 -4.1964674 -4.2125778 -4.216485 -4.21301 -4.189981 -4.1646047 -4.1648531][-4.2290206 -4.2104549 -4.1997566 -4.1802816 -4.1648817 -4.1297388 -4.0911779 -4.0682487 -4.093646 -4.1342707 -4.1557035 -4.1614795 -4.1217313 -4.083467 -4.0833983][-4.1678948 -4.1327238 -4.1145926 -4.0896406 -4.0669956 -4.005743 -3.9198706 -3.8751202 -3.9397323 -4.0311742 -4.0866761 -4.1067262 -4.0515394 -3.993762 -3.9919686][-4.1402779 -4.0947986 -4.0695882 -4.0384359 -4.0081415 -3.9238272 -3.7828262 -3.7011046 -3.8048098 -3.9505503 -4.0450621 -4.086113 -4.0273113 -3.9517803 -3.9453368][-4.1585903 -4.1158338 -4.0883064 -4.0519729 -4.0215511 -3.9491677 -3.8167036 -3.7268536 -3.8088727 -3.9487224 -4.0537558 -4.1097317 -4.0656147 -3.9958603 -3.9894328][-4.2036695 -4.1736794 -4.1485033 -4.1111126 -4.0919785 -4.0479469 -3.9585414 -3.8914151 -3.9338667 -4.0267158 -4.1081738 -4.160954 -4.1394162 -4.0902672 -4.0821838][-4.2460341 -4.2258816 -4.2101183 -4.1815038 -4.1744156 -4.1479354 -4.0898142 -4.0439005 -4.0666642 -4.1278791 -4.184556 -4.2267804 -4.2241769 -4.1957626 -4.1866779][-4.2775073 -4.265892 -4.2606735 -4.2454171 -4.2447596 -4.2291102 -4.1866817 -4.1507597 -4.1622853 -4.204639 -4.2497659 -4.2842073 -4.2872133 -4.2741456 -4.2663155][-4.3019581 -4.2982144 -4.2965374 -4.2868981 -4.2893028 -4.2824111 -4.2541156 -4.2273927 -4.2294269 -4.25531 -4.2925687 -4.3220325 -4.323524 -4.315444 -4.3125386][-4.3179622 -4.3154178 -4.31054 -4.2993436 -4.3008542 -4.2998419 -4.2855544 -4.2695665 -4.2691197 -4.2882257 -4.3146214 -4.3345942 -4.3320975 -4.3249516 -4.3236904][-4.3190775 -4.3124604 -4.30199 -4.288147 -4.2862496 -4.2864904 -4.2823076 -4.277688 -4.2801714 -4.2962074 -4.3135514 -4.3230605 -4.3168435 -4.3100047 -4.3079453][-4.3073049 -4.296298 -4.2835565 -4.2713065 -4.2703118 -4.2690277 -4.2662067 -4.2660093 -4.2706881 -4.2815065 -4.2924533 -4.2969456 -4.2927852 -4.2895927 -4.2866521]]...]
INFO - root - 2017-12-07 19:43:54.563873: step 46610, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 54h:26m:55s remains)
INFO - root - 2017-12-07 19:44:01.268686: step 46620, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 53h:20m:32s remains)
INFO - root - 2017-12-07 19:44:08.031085: step 46630, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 52h:04m:01s remains)
INFO - root - 2017-12-07 19:44:14.849456: step 46640, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.724 sec/batch; 57h:27m:40s remains)
INFO - root - 2017-12-07 19:44:21.698127: step 46650, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.734 sec/batch; 58h:14m:33s remains)
INFO - root - 2017-12-07 19:44:28.420521: step 46660, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 54h:25m:04s remains)
INFO - root - 2017-12-07 19:44:35.167078: step 46670, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 49h:25m:05s remains)
INFO - root - 2017-12-07 19:44:41.903637: step 46680, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 54h:20m:04s remains)
INFO - root - 2017-12-07 19:44:48.770230: step 46690, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 56h:13m:38s remains)
INFO - root - 2017-12-07 19:44:55.404053: step 46700, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 54h:22m:47s remains)
2017-12-07 19:44:56.094401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2605863 -4.2249088 -4.1717238 -4.1160407 -4.0803213 -4.085454 -4.1045051 -4.1222987 -4.1293716 -4.1334095 -4.1419368 -4.1570859 -4.1642327 -4.1620469 -4.1641145][-4.2677088 -4.2402239 -4.1871886 -4.1263995 -4.079464 -4.0650568 -4.0748386 -4.0979667 -4.1271486 -4.1563406 -4.1787271 -4.2018309 -4.2144704 -4.2164884 -4.2193933][-4.2600536 -4.2481055 -4.2049885 -4.144701 -4.0878739 -4.0469341 -4.0390854 -4.0655036 -4.1191368 -4.1778355 -4.220119 -4.2505732 -4.2661266 -4.2683845 -4.2691131][-4.2548413 -4.2598763 -4.2316532 -4.1753936 -4.10588 -4.0331707 -3.9957037 -4.0122819 -4.0846577 -4.1698108 -4.2308822 -4.2701154 -4.2891989 -4.2919717 -4.2889795][-4.2615142 -4.2721286 -4.2543225 -4.2037678 -4.1310439 -4.0403471 -3.9712472 -3.9624736 -4.0367308 -4.1367869 -4.2101059 -4.2584777 -4.2829156 -4.2857723 -4.2807341][-4.2816162 -4.2882648 -4.2698388 -4.2211637 -4.1499672 -4.0527973 -3.9627945 -3.92823 -3.9952059 -4.1047845 -4.1921515 -4.2501454 -4.277482 -4.2775412 -4.2698474][-4.2980571 -4.2976823 -4.2732062 -4.2229466 -4.1536474 -4.0636406 -3.9800017 -3.9434772 -4.0037541 -4.1117625 -4.2022362 -4.2604346 -4.2859516 -4.2786322 -4.2637324][-4.2961154 -4.2904816 -4.2589293 -4.204906 -4.1402373 -4.0715966 -4.0212479 -4.0102768 -4.0671268 -4.1599445 -4.2372241 -4.2792449 -4.2895575 -4.2723169 -4.2510519][-4.2779341 -4.2660136 -4.2294936 -4.1703706 -4.1074882 -4.0639982 -4.0547771 -4.0758638 -4.1334085 -4.2102742 -4.2701259 -4.2930884 -4.2878494 -4.2596383 -4.2299051][-4.2648058 -4.2455144 -4.2032948 -4.1399198 -4.0736747 -4.0394211 -4.0577626 -4.1069813 -4.1723304 -4.2386494 -4.28591 -4.294013 -4.2767482 -4.237937 -4.1989384][-4.2579808 -4.2393527 -4.1999578 -4.136 -4.0659485 -4.0283217 -4.0552669 -4.1214657 -4.1928682 -4.25348 -4.2944484 -4.2990255 -4.2786627 -4.2326908 -4.1866279][-4.2570515 -4.2425356 -4.2115021 -4.1545706 -4.089673 -4.0522461 -4.0793285 -4.1512814 -4.2229719 -4.2785206 -4.3143225 -4.3185124 -4.298079 -4.2528453 -4.2066593][-4.2562461 -4.2514052 -4.2317042 -4.1858892 -4.1331139 -4.1043644 -4.1317019 -4.1983 -4.260232 -4.3044968 -4.3324957 -4.3352604 -4.3176622 -4.2826686 -4.2452717][-4.240581 -4.2470613 -4.2416835 -4.2143788 -4.1774859 -4.1606393 -4.1861253 -4.2401304 -4.2884164 -4.3166218 -4.3312902 -4.329566 -4.3153934 -4.2910671 -4.2658968][-4.2148709 -4.2269721 -4.2277045 -4.2093582 -4.1845 -4.1791439 -4.2034569 -4.2449794 -4.27934 -4.2978196 -4.3062892 -4.3020649 -4.2882538 -4.2703977 -4.2579184]]...]
INFO - root - 2017-12-07 19:45:02.956514: step 46710, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 57h:31m:02s remains)
INFO - root - 2017-12-07 19:45:09.834237: step 46720, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 54h:52m:15s remains)
INFO - root - 2017-12-07 19:45:16.580006: step 46730, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 53h:25m:08s remains)
INFO - root - 2017-12-07 19:45:23.318292: step 46740, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 52h:26m:18s remains)
INFO - root - 2017-12-07 19:45:30.081028: step 46750, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 53h:33m:49s remains)
INFO - root - 2017-12-07 19:45:36.883531: step 46760, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 57h:40m:08s remains)
INFO - root - 2017-12-07 19:45:43.635656: step 46770, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 54h:39m:23s remains)
INFO - root - 2017-12-07 19:45:50.392077: step 46780, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 50h:00m:17s remains)
INFO - root - 2017-12-07 19:45:57.254441: step 46790, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 56h:52m:10s remains)
INFO - root - 2017-12-07 19:46:03.854432: step 46800, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 58h:49m:19s remains)
2017-12-07 19:46:04.613756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1957445 -4.2089071 -4.2422948 -4.2768221 -4.3042479 -4.3156052 -4.3096414 -4.2934265 -4.280654 -4.2764359 -4.2778635 -4.2795668 -4.2783895 -4.2804465 -4.281188][-4.1793876 -4.2032003 -4.2429461 -4.2765346 -4.2995915 -4.3057241 -4.29495 -4.2754908 -4.2590232 -4.25108 -4.2506428 -4.2547827 -4.2581692 -4.262774 -4.268672][-4.1765552 -4.2047071 -4.2425342 -4.2696319 -4.2837596 -4.2804608 -4.26486 -4.24666 -4.2339649 -4.2281604 -4.2268305 -4.2318258 -4.2392287 -4.2481833 -4.2598825][-4.188098 -4.2130165 -4.2450376 -4.266119 -4.2704678 -4.2535906 -4.2296243 -4.2114763 -4.2072706 -4.2102904 -4.2112722 -4.215539 -4.2251835 -4.2390833 -4.254693][-4.203618 -4.2203503 -4.2428718 -4.2564483 -4.2517786 -4.2249546 -4.1920033 -4.1684322 -4.1660161 -4.1802363 -4.1934 -4.2075772 -4.22169 -4.2381172 -4.2547169][-4.2123041 -4.2150316 -4.2221966 -4.2278094 -4.21906 -4.187211 -4.1480603 -4.1183147 -4.1098328 -4.1281672 -4.1553063 -4.1858158 -4.2102404 -4.2334404 -4.2510419][-4.2060885 -4.1997476 -4.194325 -4.1952467 -4.1920152 -4.1612444 -4.1208539 -4.0859604 -4.0676055 -4.0787263 -4.1096039 -4.1470742 -4.1805391 -4.2143073 -4.23906][-4.1902 -4.1824532 -4.1734128 -4.1775274 -4.184557 -4.1707973 -4.1453757 -4.1128225 -4.0803094 -4.064671 -4.0761857 -4.1082735 -4.1457863 -4.1894155 -4.2232637][-4.181097 -4.1781678 -4.1722107 -4.1782742 -4.1898088 -4.1943889 -4.19355 -4.179297 -4.1473179 -4.1158938 -4.099874 -4.1090789 -4.1340094 -4.1758604 -4.2102857][-4.1845942 -4.18958 -4.1880722 -4.1906595 -4.1988897 -4.212708 -4.2297344 -4.235652 -4.2216759 -4.1933188 -4.1653228 -4.1541777 -4.1576924 -4.1804919 -4.2038302][-4.1849051 -4.1932192 -4.1945438 -4.1920409 -4.1965504 -4.2129216 -4.2393417 -4.2609324 -4.2661991 -4.2515397 -4.2259235 -4.2045064 -4.1923509 -4.1952028 -4.201375][-4.1886468 -4.1960793 -4.197197 -4.1883841 -4.1830215 -4.1932034 -4.2235379 -4.258985 -4.2811913 -4.279253 -4.2650523 -4.244575 -4.2240515 -4.2080455 -4.1949162][-4.1957612 -4.20591 -4.2107911 -4.2006888 -4.1871057 -4.1884856 -4.2152319 -4.256546 -4.2896109 -4.2976112 -4.2868781 -4.2650576 -4.2388029 -4.2088108 -4.1771803][-4.2121534 -4.2245588 -4.2332664 -4.225245 -4.2076311 -4.1982889 -4.2131824 -4.2464314 -4.2808824 -4.2968974 -4.2926135 -4.2748189 -4.2479625 -4.2122574 -4.1679292][-4.2307954 -4.235393 -4.2403264 -4.2325773 -4.2138433 -4.1984973 -4.2043271 -4.2292085 -4.2585139 -4.2776084 -4.2848125 -4.277256 -4.2586575 -4.2299547 -4.1867604]]...]
INFO - root - 2017-12-07 19:46:11.520266: step 46810, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 51h:16m:50s remains)
INFO - root - 2017-12-07 19:46:18.401049: step 46820, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 57h:39m:14s remains)
INFO - root - 2017-12-07 19:46:25.259097: step 46830, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.760 sec/batch; 60h:19m:54s remains)
INFO - root - 2017-12-07 19:46:32.056912: step 46840, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 53h:29m:51s remains)
INFO - root - 2017-12-07 19:46:38.841140: step 46850, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 54h:23m:21s remains)
INFO - root - 2017-12-07 19:46:45.589082: step 46860, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 51h:03m:20s remains)
INFO - root - 2017-12-07 19:46:52.462465: step 46870, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 57h:49m:29s remains)
INFO - root - 2017-12-07 19:46:59.327941: step 46880, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 57h:44m:03s remains)
INFO - root - 2017-12-07 19:47:06.134298: step 46890, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 52h:10m:45s remains)
INFO - root - 2017-12-07 19:47:12.691150: step 46900, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 49h:25m:23s remains)
2017-12-07 19:47:13.455548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1656361 -4.1463437 -4.1228862 -4.1191015 -4.1387696 -4.15508 -4.1493196 -4.1289377 -4.1142945 -4.1136084 -4.1227183 -4.1227512 -4.1093564 -4.1054425 -4.128952][-4.2056918 -4.1889234 -4.1622634 -4.1502609 -4.1639462 -4.1778331 -4.1708031 -4.1542764 -4.1485629 -4.1533918 -4.1609483 -4.15761 -4.1419849 -4.1340284 -4.149703][-4.2187529 -4.2005286 -4.1720114 -4.1600814 -4.1759553 -4.1939764 -4.1882463 -4.1764784 -4.1771436 -4.18611 -4.1945333 -4.1918955 -4.1740575 -4.1629405 -4.1708937][-4.1913109 -4.1719623 -4.1477966 -4.1483526 -4.178122 -4.2015944 -4.1893911 -4.1685381 -4.1651783 -4.1776524 -4.197216 -4.2038827 -4.1887603 -4.1753812 -4.177084][-4.1319065 -4.1164932 -4.1035519 -4.121881 -4.1664972 -4.189559 -4.162281 -4.118187 -4.1036477 -4.1245923 -4.1636949 -4.1888289 -4.1861482 -4.1784763 -4.1777258][-4.0760808 -4.0653744 -4.0645061 -4.0984168 -4.1492443 -4.16353 -4.1163182 -4.0469928 -4.0193324 -4.0519681 -4.1136203 -4.1560316 -4.1619353 -4.1565704 -4.15569][-4.0496707 -4.0361423 -4.0341787 -4.0666609 -4.11466 -4.116653 -4.0488586 -3.9572177 -3.9223926 -3.9715848 -4.05439 -4.106719 -4.1146421 -4.1081333 -4.1064205][-4.0900822 -4.0702248 -4.0638871 -4.0838442 -4.1084137 -4.0766296 -3.971709 -3.8446288 -3.7991252 -3.8709106 -3.9791145 -4.0494194 -4.0701857 -4.0687451 -4.0700455][-4.160048 -4.1470613 -4.147347 -4.1604171 -4.1608715 -4.1062455 -3.9896469 -3.8642027 -3.8202915 -3.8800583 -3.9755247 -4.0429335 -4.0644541 -4.062943 -4.0638504][-4.22924 -4.2268538 -4.2309132 -4.23502 -4.2242451 -4.1737542 -4.0789652 -3.9851782 -3.9494686 -3.9765086 -4.0319395 -4.0768995 -4.0883927 -4.080194 -4.0794091][-4.268681 -4.2721434 -4.273098 -4.2703071 -4.2542953 -4.2131991 -4.1417642 -4.0725861 -4.0436273 -4.0560703 -4.0872006 -4.1153426 -4.1197963 -4.1082973 -4.1061916][-4.2989764 -4.3051624 -4.3035784 -4.2963171 -4.2784724 -4.2456388 -4.1931286 -4.1442404 -4.1259456 -4.1377363 -4.1579285 -4.17282 -4.1686678 -4.1501365 -4.1436806][-4.3120804 -4.3229685 -4.3207922 -4.3132091 -4.2968268 -4.2702484 -4.2302337 -4.1930976 -4.1797762 -4.1915236 -4.2078452 -4.2173524 -4.2119393 -4.193882 -4.1851377][-4.2922125 -4.3101244 -4.3139472 -4.3123307 -4.3005166 -4.277597 -4.243546 -4.2147412 -4.2038655 -4.2134 -4.2272291 -4.2349548 -4.2320409 -4.221118 -4.212986][-4.2583055 -4.2750945 -4.2839427 -4.290288 -4.2872949 -4.2731504 -4.248292 -4.22958 -4.2227173 -4.2300935 -4.2403908 -4.2441745 -4.2416954 -4.2364669 -4.23115]]...]
INFO - root - 2017-12-07 19:47:20.360459: step 46910, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 57h:39m:57s remains)
INFO - root - 2017-12-07 19:47:27.146120: step 46920, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 53h:14m:12s remains)
INFO - root - 2017-12-07 19:47:33.929416: step 46930, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 50h:55m:31s remains)
INFO - root - 2017-12-07 19:47:40.795168: step 46940, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.700 sec/batch; 55h:31m:32s remains)
INFO - root - 2017-12-07 19:47:47.612625: step 46950, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 56h:47m:16s remains)
INFO - root - 2017-12-07 19:47:54.505361: step 46960, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 51h:29m:52s remains)
INFO - root - 2017-12-07 19:48:01.356882: step 46970, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 49h:50m:21s remains)
INFO - root - 2017-12-07 19:48:08.213288: step 46980, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 50h:42m:35s remains)
INFO - root - 2017-12-07 19:48:15.035358: step 46990, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 57h:30m:27s remains)
INFO - root - 2017-12-07 19:48:21.623638: step 47000, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 57h:44m:54s remains)
2017-12-07 19:48:22.347517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2034717 -4.21391 -4.2160254 -4.2112079 -4.202291 -4.1966424 -4.2027144 -4.2133589 -4.2230392 -4.233695 -4.2462368 -4.2438726 -4.2217321 -4.2137146 -4.2184787][-4.2007771 -4.2111712 -4.2133732 -4.2089267 -4.2024331 -4.202507 -4.2145629 -4.2309742 -4.243464 -4.249897 -4.2556729 -4.2529721 -4.2324572 -4.2202911 -4.2204638][-4.2140865 -4.2185035 -4.2154503 -4.2093287 -4.2060895 -4.2105722 -4.2253213 -4.2430944 -4.2554712 -4.2571559 -4.2573385 -4.2542291 -4.239429 -4.2296391 -4.2298207][-4.2285681 -4.2285962 -4.222321 -4.2158084 -4.2146416 -4.2180996 -4.2299786 -4.2445545 -4.2542977 -4.2507648 -4.2446351 -4.2401552 -4.2301464 -4.2223258 -4.22373][-4.2390137 -4.2319431 -4.2207522 -4.2106633 -4.2068744 -4.2032623 -4.2064762 -4.2158151 -4.2232938 -4.2168336 -4.2078314 -4.2063618 -4.2004824 -4.196352 -4.1992545][-4.2303925 -4.2158356 -4.1989446 -4.1825881 -4.1688404 -4.1511889 -4.1387691 -4.1440392 -4.1520286 -4.1476369 -4.1464629 -4.1606736 -4.1648765 -4.1681795 -4.1746826][-4.1948533 -4.1747785 -4.1561751 -4.1343832 -4.1073132 -4.0660453 -4.02852 -4.0288563 -4.0486283 -4.0637345 -4.0818377 -4.1158905 -4.137249 -4.1489697 -4.1605296][-4.1349187 -4.1138153 -4.0992575 -4.0776138 -4.0437136 -3.983315 -3.9206529 -3.9219263 -3.9736021 -4.0208187 -4.0574293 -4.099678 -4.1299834 -4.1498652 -4.1635151][-4.0696535 -4.0515075 -4.0470414 -4.0343394 -4.0091863 -3.9545281 -3.893214 -3.9058003 -3.9856422 -4.0515547 -4.0886326 -4.1219559 -4.1450562 -4.1595592 -4.1652932][-4.0330234 -4.02754 -4.0361385 -4.0377955 -4.0310583 -4.0016079 -3.9689372 -3.99169 -4.0675416 -4.1226897 -4.1434727 -4.1607161 -4.1710281 -4.1733131 -4.1680512][-4.0403185 -4.0488257 -4.0656176 -4.0738673 -4.07942 -4.0705738 -4.0623779 -4.0882187 -4.1445055 -4.1828804 -4.1910648 -4.1959138 -4.1969147 -4.1903582 -4.1804256][-4.0645866 -4.0790935 -4.0997438 -4.1110921 -4.1231756 -4.127408 -4.1287365 -4.1474466 -4.182426 -4.2062693 -4.210948 -4.2083635 -4.2048373 -4.1979532 -4.193367][-4.0722122 -4.0947132 -4.1223249 -4.1377277 -4.152842 -4.1646237 -4.1669607 -4.1737442 -4.1908627 -4.2046194 -4.2101703 -4.2069888 -4.2027364 -4.1981397 -4.2000089][-4.0662937 -4.0992432 -4.1329412 -4.1528664 -4.1706257 -4.1860166 -4.1874623 -4.1864343 -4.1934581 -4.2057991 -4.2178588 -4.2194862 -4.2107697 -4.2003837 -4.2015038][-4.077251 -4.115881 -4.1508503 -4.1735783 -4.1918988 -4.2053413 -4.2042651 -4.2012749 -4.2046885 -4.2155962 -4.2247448 -4.2246528 -4.2153726 -4.2047215 -4.2085595]]...]
INFO - root - 2017-12-07 19:48:29.250584: step 47010, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 55h:12m:48s remains)
INFO - root - 2017-12-07 19:48:36.023043: step 47020, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:51m:12s remains)
INFO - root - 2017-12-07 19:48:42.831024: step 47030, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 53h:27m:44s remains)
INFO - root - 2017-12-07 19:48:49.494767: step 47040, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 52h:04m:53s remains)
INFO - root - 2017-12-07 19:48:56.223391: step 47050, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 50h:36m:28s remains)
INFO - root - 2017-12-07 19:49:02.966973: step 47060, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.626 sec/batch; 49h:37m:02s remains)
INFO - root - 2017-12-07 19:49:09.739397: step 47070, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 53h:52m:29s remains)
INFO - root - 2017-12-07 19:49:16.495455: step 47080, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.611 sec/batch; 48h:28m:08s remains)
INFO - root - 2017-12-07 19:49:23.294507: step 47090, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.625 sec/batch; 49h:32m:10s remains)
INFO - root - 2017-12-07 19:49:29.974227: step 47100, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 58h:08m:39s remains)
2017-12-07 19:49:30.665946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2729578 -4.2704587 -4.2691889 -4.2694449 -4.2708521 -4.2723312 -4.2728238 -4.2719984 -4.2703934 -4.2689338 -4.2676153 -4.2661939 -4.2643909 -4.2621903 -4.2561135][-4.28485 -4.2814651 -4.279768 -4.2814183 -4.2850547 -4.2880368 -4.2881804 -4.2854137 -4.2816668 -4.2790494 -4.2779431 -4.2772923 -4.2761393 -4.2736864 -4.2684555][-4.2922082 -4.2885032 -4.2875385 -4.2910757 -4.2975841 -4.3018332 -4.3012171 -4.2955856 -4.2880697 -4.2819686 -4.2783656 -4.2769957 -4.2770653 -4.2764769 -4.2736144][-4.266057 -4.2621279 -4.263103 -4.2707515 -4.2811432 -4.2881985 -4.2881031 -4.2808242 -4.2684789 -4.2551537 -4.2452726 -4.2394342 -4.2390056 -4.2397017 -4.2408814][-4.2182593 -4.2157125 -4.2220678 -4.2362957 -4.2543464 -4.2657046 -4.2648859 -4.2537589 -4.2336984 -4.2097363 -4.18893 -4.1761 -4.1749611 -4.178143 -4.184731][-4.1335745 -4.1354823 -4.1507678 -4.1752138 -4.2019186 -4.2193241 -4.2169523 -4.1964083 -4.162632 -4.1184936 -4.0828066 -4.0671678 -4.0735006 -4.0889173 -4.1062531][-4.0343671 -4.0429072 -4.0695014 -4.1046014 -4.1367211 -4.1532416 -4.142168 -4.1034088 -4.046452 -3.9803047 -3.9384928 -3.9336269 -3.9604087 -3.996222 -4.0274711][-3.9968517 -4.0137668 -4.0514569 -4.09216 -4.1201191 -4.1289868 -4.107604 -4.0524783 -3.977047 -3.9046147 -3.8785963 -3.8996632 -3.9483061 -3.9943712 -4.0236712][-4.068603 -4.0832248 -4.1172752 -4.1493206 -4.1660519 -4.1653447 -4.1401944 -4.0890794 -4.031714 -3.9880812 -3.9789748 -3.9989476 -4.0371103 -4.0710363 -4.091002][-4.1855092 -4.1921287 -4.2149124 -4.23465 -4.2408328 -4.2345281 -4.2148681 -4.1870222 -4.1641335 -4.1484518 -4.1419497 -4.1460834 -4.161581 -4.17703 -4.18678][-4.2703791 -4.2717404 -4.2830482 -4.2914038 -4.2904906 -4.2819352 -4.2708349 -4.2612267 -4.2565203 -4.2534862 -4.2511787 -4.2509079 -4.25526 -4.2573075 -4.2551723][-4.3015618 -4.3019905 -4.3079996 -4.3121428 -4.3098068 -4.3035769 -4.2984109 -4.2953577 -4.2950234 -4.2960186 -4.298316 -4.2997031 -4.300209 -4.2974186 -4.2900987][-4.2912431 -4.2953 -4.3015509 -4.3047585 -4.302278 -4.298152 -4.2956314 -4.29507 -4.2950106 -4.2953882 -4.29884 -4.3020597 -4.302444 -4.2984629 -4.2873945][-4.2482333 -4.2590365 -4.2645478 -4.2663565 -4.2650976 -4.2641387 -4.2628374 -4.2617116 -4.260973 -4.2621689 -4.2659435 -4.26964 -4.269136 -4.2630782 -4.2448187][-4.1713452 -4.1869831 -4.191494 -4.1926756 -4.1926756 -4.1936212 -4.19339 -4.1912732 -4.1888309 -4.1886864 -4.1905756 -4.1937461 -4.1940455 -4.1890516 -4.1670647]]...]
INFO - root - 2017-12-07 19:49:37.414447: step 47110, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 51h:00m:53s remains)
INFO - root - 2017-12-07 19:49:44.173920: step 47120, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 49h:59m:30s remains)
INFO - root - 2017-12-07 19:49:51.043289: step 47130, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 58h:30m:08s remains)
INFO - root - 2017-12-07 19:49:57.790671: step 47140, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.715 sec/batch; 56h:40m:12s remains)
INFO - root - 2017-12-07 19:50:04.576002: step 47150, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 53h:24m:53s remains)
INFO - root - 2017-12-07 19:50:11.243532: step 47160, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 49h:06m:03s remains)
INFO - root - 2017-12-07 19:50:18.071480: step 47170, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 51h:58m:30s remains)
INFO - root - 2017-12-07 19:50:25.018880: step 47180, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 0.760 sec/batch; 60h:15m:00s remains)
INFO - root - 2017-12-07 19:50:31.750503: step 47190, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 55h:30m:32s remains)
INFO - root - 2017-12-07 19:50:38.280705: step 47200, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 55h:04m:33s remains)
2017-12-07 19:50:38.965157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2627621 -4.2541313 -4.2450867 -4.233994 -4.2234139 -4.2196116 -4.2196393 -4.2210135 -4.2239046 -4.2285943 -4.2358184 -4.2441778 -4.254992 -4.2631927 -4.2687407][-4.2541509 -4.2435837 -4.2323685 -4.2181406 -4.2047434 -4.1997991 -4.1994185 -4.1987185 -4.19954 -4.2055326 -4.2172289 -4.2295442 -4.2419734 -4.2497754 -4.2566166][-4.2349291 -4.2267776 -4.2156754 -4.1987567 -4.181407 -4.1736889 -4.1711941 -4.1686068 -4.1689448 -4.1777711 -4.1959877 -4.2122059 -4.2242627 -4.2297339 -4.2367291][-4.2056236 -4.2031317 -4.1966166 -4.178894 -4.1582217 -4.1464391 -4.1418862 -4.1385803 -4.1383286 -4.1484962 -4.1692109 -4.1876116 -4.2021403 -4.2120295 -4.2221065][-4.1890731 -4.1913524 -4.1880169 -4.1650987 -4.1373787 -4.1201568 -4.1143723 -4.1166453 -4.1189575 -4.1288452 -4.1484704 -4.1645546 -4.1807666 -4.195641 -4.2091823][-4.1786995 -4.1784229 -4.1695471 -4.135447 -4.0936708 -4.06457 -4.0648956 -4.0841784 -4.0972733 -4.1077237 -4.1236815 -4.1400108 -4.1598935 -4.1807189 -4.1984067][-4.1645093 -4.1578388 -4.1377144 -4.0891051 -4.0278192 -3.9791155 -3.9848518 -4.0239687 -4.0450954 -4.0535083 -4.0669484 -4.0907364 -4.1240487 -4.1534252 -4.1741185][-4.1520314 -4.144093 -4.1242275 -4.0782862 -4.015985 -3.9590938 -3.9612663 -3.9992476 -4.0176177 -4.0175152 -4.0264006 -4.0548983 -4.0956845 -4.13097 -4.1558924][-4.1646447 -4.1592126 -4.1504211 -4.124104 -4.0869646 -4.0449166 -4.0331373 -4.0436091 -4.0430965 -4.03132 -4.0357957 -4.0656414 -4.1046009 -4.1391582 -4.1635818][-4.1963816 -4.1963854 -4.1971297 -4.1895895 -4.1768222 -4.1532626 -4.13868 -4.1326227 -4.1192489 -4.0994334 -4.0980544 -4.1198912 -4.1482911 -4.1744127 -4.1964755][-4.2352977 -4.2355289 -4.2360811 -4.2331314 -4.2307773 -4.2194448 -4.2092924 -4.2013659 -4.1860752 -4.1674666 -4.1639771 -4.178617 -4.1991024 -4.2210178 -4.2428279][-4.2682791 -4.2652779 -4.2618036 -4.2586722 -4.2602768 -4.2568917 -4.2509894 -4.2440052 -4.2306042 -4.2152047 -4.2118073 -4.2238483 -4.2413049 -4.2608976 -4.2803416][-4.2796574 -4.2752619 -4.2686915 -4.2664976 -4.2730737 -4.2778 -4.2775469 -4.2732658 -4.265234 -4.2556334 -4.2524223 -4.2621746 -4.2770085 -4.2931304 -4.3069687][-4.2671938 -4.2626138 -4.2553549 -4.2552719 -4.2657003 -4.2752981 -4.2799811 -4.2814455 -4.2808852 -4.2800817 -4.2804379 -4.2893443 -4.3018718 -4.3118596 -4.3167024][-4.2424488 -4.2390141 -4.2353768 -4.2375379 -4.249227 -4.2611661 -4.2678008 -4.2722788 -4.2772865 -4.2845936 -4.2922568 -4.3027725 -4.3114071 -4.3153586 -4.3152514]]...]
INFO - root - 2017-12-07 19:50:45.913211: step 47210, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 58h:51m:58s remains)
INFO - root - 2017-12-07 19:50:52.683825: step 47220, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 54h:03m:26s remains)
INFO - root - 2017-12-07 19:50:59.451912: step 47230, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 55h:34m:59s remains)
INFO - root - 2017-12-07 19:51:06.346020: step 47240, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 52h:21m:07s remains)
INFO - root - 2017-12-07 19:51:13.251178: step 47250, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 56h:05m:51s remains)
INFO - root - 2017-12-07 19:51:20.018749: step 47260, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 55h:01m:41s remains)
INFO - root - 2017-12-07 19:51:26.890534: step 47270, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 55h:17m:14s remains)
INFO - root - 2017-12-07 19:51:33.673455: step 47280, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 49h:18m:46s remains)
INFO - root - 2017-12-07 19:51:40.418720: step 47290, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 50h:09m:23s remains)
INFO - root - 2017-12-07 19:51:47.061501: step 47300, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 57h:27m:02s remains)
2017-12-07 19:51:47.703356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3210306 -4.3161888 -4.3129582 -4.3094511 -4.3086953 -4.3110309 -4.3158436 -4.319623 -4.3181753 -4.3121452 -4.3047061 -4.3036127 -4.3099313 -4.3210096 -4.3321414][-4.3026791 -4.2980447 -4.2946072 -4.2893353 -4.2864547 -4.2866192 -4.2915583 -4.2978663 -4.2997751 -4.295156 -4.2858233 -4.282979 -4.2876844 -4.2994685 -4.3144479][-4.2736835 -4.2738748 -4.2736506 -4.2674842 -4.2604032 -4.2541943 -4.254746 -4.2629762 -4.2719321 -4.2760139 -4.2730732 -4.27209 -4.275176 -4.2844734 -4.299593][-4.2409043 -4.2475376 -4.2503142 -4.2426758 -4.2292666 -4.2130442 -4.2049837 -4.2135243 -4.2311077 -4.24995 -4.2609868 -4.2678361 -4.2735391 -4.2813344 -4.2948995][-4.2164044 -4.2270665 -4.2298636 -4.2149134 -4.1899033 -4.1552048 -4.1305614 -4.1357226 -4.1651592 -4.2019506 -4.2319088 -4.2527719 -4.2689662 -4.28177 -4.2967329][-4.2078328 -4.2182021 -4.2153378 -4.187768 -4.1443458 -4.0849357 -4.0322776 -4.0246677 -4.0688515 -4.1287327 -4.1798429 -4.2159586 -4.2475929 -4.2744684 -4.2977972][-4.19257 -4.2007618 -4.1919012 -4.1558065 -4.09943 -4.0153241 -3.9236343 -3.8880241 -3.9424834 -4.0272789 -4.100266 -4.1544671 -4.2020049 -4.246387 -4.2834024][-4.1770606 -4.1812711 -4.1692152 -4.1313968 -4.0723629 -3.9818892 -3.87115 -3.8048368 -3.840487 -3.9240813 -4.0025945 -4.071331 -4.1353736 -4.1958909 -4.2468681][-4.1737051 -4.1728435 -4.1604867 -4.1304836 -4.0820518 -4.0083213 -3.9199438 -3.8582308 -3.8499064 -3.8813643 -3.9308546 -3.9946463 -4.06569 -4.1357741 -4.1987114][-4.192647 -4.1887436 -4.1787958 -4.1610112 -4.1316595 -4.0831933 -4.0275378 -3.9831276 -3.9490037 -3.9300656 -3.9400024 -3.9816515 -4.0384111 -4.0995526 -4.162394][-4.2411914 -4.2370253 -4.2300339 -4.2233572 -4.2156553 -4.1925 -4.1615343 -4.1297688 -4.0902562 -4.0530877 -4.0407705 -4.0557475 -4.0858569 -4.1248069 -4.1719632][-4.2889957 -4.2887697 -4.2850866 -4.2841654 -4.2881427 -4.2827878 -4.2689528 -4.2478371 -4.2173176 -4.1854119 -4.1680713 -4.1646905 -4.1727805 -4.1920514 -4.2191424][-4.3202772 -4.3228207 -4.3223734 -4.3224916 -4.3279314 -4.3296084 -4.32485 -4.3138437 -4.2975187 -4.2797127 -4.2636414 -4.2515559 -4.24867 -4.2557507 -4.2683306][-4.336184 -4.3398681 -4.34157 -4.3421788 -4.3448443 -4.3461781 -4.3435769 -4.3380651 -4.3305035 -4.3226109 -4.3137293 -4.3044209 -4.2993903 -4.2995977 -4.3027744][-4.3456378 -4.348743 -4.3505096 -4.3509212 -4.3513112 -4.3504066 -4.347918 -4.3454556 -4.3430939 -4.340291 -4.3353229 -4.3294539 -4.3257246 -4.3236375 -4.3233414]]...]
INFO - root - 2017-12-07 19:51:54.476252: step 47310, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 52h:06m:17s remains)
INFO - root - 2017-12-07 19:52:01.207913: step 47320, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 50h:21m:52s remains)
INFO - root - 2017-12-07 19:52:07.866814: step 47330, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 52h:28m:34s remains)
INFO - root - 2017-12-07 19:52:14.746049: step 47340, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:49m:22s remains)
INFO - root - 2017-12-07 19:52:21.489170: step 47350, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 53h:04m:15s remains)
INFO - root - 2017-12-07 19:52:28.199196: step 47360, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 51h:58m:58s remains)
INFO - root - 2017-12-07 19:52:34.867366: step 47370, loss = 2.07, batch loss = 2.01 (13.7 examples/sec; 0.582 sec/batch; 46h:05m:17s remains)
INFO - root - 2017-12-07 19:52:41.652529: step 47380, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.761 sec/batch; 60h:16m:18s remains)
INFO - root - 2017-12-07 19:52:48.406523: step 47390, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 54h:39m:38s remains)
INFO - root - 2017-12-07 19:52:54.964623: step 47400, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 51h:53m:53s remains)
2017-12-07 19:52:55.704917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2083306 -4.1987066 -4.2003593 -4.209825 -4.2049575 -4.1911836 -4.188457 -4.195962 -4.1934581 -4.1744938 -4.1626124 -4.171051 -4.1780667 -4.1778231 -4.1686258][-4.1789346 -4.1555457 -4.1462846 -4.15199 -4.144033 -4.1308045 -4.1338143 -4.152092 -4.1581855 -4.1370916 -4.1266751 -4.1464691 -4.1546941 -4.1502986 -4.1329803][-4.1359549 -4.11451 -4.1058407 -4.1091013 -4.0951781 -4.078927 -4.0836778 -4.1123972 -4.1304083 -4.1151237 -4.1120572 -4.1338992 -4.13718 -4.1354928 -4.1220813][-4.1150837 -4.1042423 -4.1046343 -4.1114092 -4.0958033 -4.0743413 -4.0737495 -4.0993309 -4.11506 -4.0998335 -4.0997992 -4.1220808 -4.1310434 -4.142518 -4.1378231][-4.1370926 -4.132709 -4.1358552 -4.141089 -4.1233077 -4.0903511 -4.0749331 -4.0830307 -4.0869431 -4.07565 -4.0808635 -4.1038532 -4.1233568 -4.1545281 -4.1678166][-4.1922879 -4.1876984 -4.1825223 -4.1729069 -4.1360111 -4.07675 -4.0376697 -4.0188003 -4.0139503 -4.0231409 -4.052403 -4.0845242 -4.1157327 -4.162467 -4.1946163][-4.252573 -4.2394381 -4.2177849 -4.1941137 -4.1472149 -4.0772548 -4.0217662 -3.9854517 -3.9825954 -4.0201797 -4.0677743 -4.1043568 -4.1408982 -4.1878481 -4.2237215][-4.2900033 -4.2662168 -4.2375436 -4.2106438 -4.1671219 -4.1078796 -4.0636892 -4.0308113 -4.0374522 -4.094676 -4.1466379 -4.179399 -4.2056365 -4.2306509 -4.2513766][-4.3017917 -4.2706561 -4.24211 -4.2186146 -4.1892114 -4.1556883 -4.1347156 -4.1141772 -4.1254759 -4.1808372 -4.224988 -4.2504878 -4.2658739 -4.2734046 -4.278029][-4.2906351 -4.2638316 -4.2405796 -4.2251358 -4.2125721 -4.204906 -4.2057366 -4.199398 -4.209239 -4.2464337 -4.2763214 -4.2916441 -4.2977729 -4.29591 -4.2918515][-4.2709961 -4.2556872 -4.2418914 -4.2363834 -4.238286 -4.2482772 -4.25902 -4.2554984 -4.2543168 -4.2710733 -4.2891154 -4.300662 -4.3044906 -4.3017826 -4.2979679][-4.2567906 -4.2531238 -4.2492332 -4.2512369 -4.25675 -4.2657824 -4.2725029 -4.2647195 -4.2525191 -4.2557969 -4.2702808 -4.286665 -4.296618 -4.2990527 -4.2983427][-4.2550063 -4.2572484 -4.2553821 -4.2553673 -4.2533164 -4.250627 -4.2480335 -4.2389631 -4.224524 -4.2263718 -4.243814 -4.2662792 -4.2797389 -4.2848797 -4.2863369][-4.2556686 -4.26029 -4.2585464 -4.2528057 -4.23861 -4.2240791 -4.2189617 -4.2155666 -4.2109995 -4.2225385 -4.2450314 -4.2672715 -4.2761269 -4.2781987 -4.2760739][-4.2535782 -4.2605243 -4.2598109 -4.2508559 -4.2320633 -4.21662 -4.2149205 -4.2215118 -4.2313333 -4.2513785 -4.2740397 -4.2902436 -4.2904587 -4.2848697 -4.2748919]]...]
INFO - root - 2017-12-07 19:53:02.466675: step 47410, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 54h:19m:16s remains)
INFO - root - 2017-12-07 19:53:09.255320: step 47420, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 51h:53m:08s remains)
INFO - root - 2017-12-07 19:53:16.102461: step 47430, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 54h:19m:46s remains)
INFO - root - 2017-12-07 19:53:22.846662: step 47440, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 51h:00m:29s remains)
INFO - root - 2017-12-07 19:53:29.730904: step 47450, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 56h:36m:38s remains)
INFO - root - 2017-12-07 19:53:36.607439: step 47460, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 57h:25m:52s remains)
INFO - root - 2017-12-07 19:53:43.414815: step 47470, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 54h:05m:07s remains)
INFO - root - 2017-12-07 19:53:50.282000: step 47480, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 50h:15m:53s remains)
INFO - root - 2017-12-07 19:53:57.207363: step 47490, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 54h:56m:02s remains)
INFO - root - 2017-12-07 19:54:03.855655: step 47500, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 55h:51m:24s remains)
2017-12-07 19:54:04.633914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1992054 -4.1934671 -4.1850605 -4.1785183 -4.182384 -4.1906343 -4.1960416 -4.1941705 -4.1855903 -4.1826363 -4.1804667 -4.1776929 -4.1724019 -4.172317 -4.1806884][-4.1911888 -4.1740451 -4.1536608 -4.1410375 -4.1445813 -4.1591 -4.174562 -4.1819439 -4.1857653 -4.1921549 -4.1891346 -4.1805649 -4.1684523 -4.1657786 -4.1771212][-4.1721582 -4.14334 -4.1178327 -4.1051188 -4.1097074 -4.1310883 -4.1572762 -4.1717663 -4.1841545 -4.1936035 -4.18378 -4.1691446 -4.1528363 -4.152617 -4.1711683][-4.1383991 -4.1063814 -4.0880566 -4.0847425 -4.0934391 -4.1193075 -4.1528106 -4.1747255 -4.1913371 -4.1970186 -4.1792822 -4.1550679 -4.13348 -4.1336093 -4.1549492][-4.0947633 -4.0647969 -4.0587344 -4.0709219 -4.0866351 -4.107759 -4.1366119 -4.1648822 -4.1893463 -4.1959815 -4.1765313 -4.1456738 -4.1208625 -4.1208205 -4.1409516][-4.0514269 -4.0262556 -4.0296659 -4.0519433 -4.0679312 -4.0769167 -4.0964832 -4.1255302 -4.1584015 -4.173995 -4.16162 -4.1336818 -4.1156521 -4.1211672 -4.1454086][-4.046443 -4.0283475 -4.0314679 -4.0445685 -4.041286 -4.0213513 -4.024363 -4.0514627 -4.0943637 -4.1266828 -4.1290298 -4.1153564 -4.11362 -4.1283836 -4.1581559][-4.0963979 -4.0841541 -4.0821695 -4.07783 -4.0446634 -3.9899938 -3.9710884 -3.9910154 -4.0407324 -4.0863585 -4.1010113 -4.1041842 -4.1175966 -4.1411247 -4.1726761][-4.1654611 -4.1531215 -4.1445918 -4.1307268 -4.0890775 -4.0273595 -3.9999115 -4.01059 -4.0520439 -4.0913324 -4.1054478 -4.1186161 -4.1391072 -4.1624126 -4.1878486][-4.2177048 -4.2050714 -4.1938977 -4.178421 -4.1414957 -4.09393 -4.0715375 -4.0779991 -4.1057129 -4.1297755 -4.1372881 -4.1507964 -4.169137 -4.1844707 -4.2002821][-4.2499046 -4.2390594 -4.2309794 -4.2197676 -4.1927743 -4.1615615 -4.1453481 -4.1446781 -4.1581674 -4.1704979 -4.1754031 -4.1839089 -4.1962428 -4.2051435 -4.2138672][-4.2699628 -4.2634187 -4.2597961 -4.2532129 -4.2361317 -4.2183566 -4.206779 -4.2006378 -4.2013946 -4.2043695 -4.2077875 -4.2097812 -4.2138095 -4.2166529 -4.22][-4.2796369 -4.2744341 -4.2710934 -4.2669387 -4.2580881 -4.2505527 -4.24695 -4.2427635 -4.2378359 -4.2336273 -4.2309265 -4.2240438 -4.2156348 -4.2109141 -4.2126875][-4.2914996 -4.2855139 -4.2793417 -4.2723746 -4.2642879 -4.2613935 -4.2626715 -4.2610126 -4.2567239 -4.2510519 -4.2418318 -4.2231359 -4.20039 -4.1879673 -4.1886997][-4.3080592 -4.3020205 -4.2934723 -4.2833891 -4.2722282 -4.2660933 -4.2631106 -4.2583275 -4.2542391 -4.2522669 -4.2404084 -4.2123551 -4.1776729 -4.1568871 -4.1579542]]...]
INFO - root - 2017-12-07 19:54:11.398353: step 47510, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.627 sec/batch; 49h:36m:40s remains)
INFO - root - 2017-12-07 19:54:18.303489: step 47520, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 58h:23m:06s remains)
INFO - root - 2017-12-07 19:54:25.320480: step 47530, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 58h:26m:21s remains)
INFO - root - 2017-12-07 19:54:32.100058: step 47540, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 55h:07m:59s remains)
INFO - root - 2017-12-07 19:54:38.854674: step 47550, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 49h:50m:20s remains)
INFO - root - 2017-12-07 19:54:45.560284: step 47560, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 48h:40m:44s remains)
INFO - root - 2017-12-07 19:54:52.381773: step 47570, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.755 sec/batch; 59h:43m:17s remains)
INFO - root - 2017-12-07 19:54:59.143505: step 47580, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 56h:54m:48s remains)
INFO - root - 2017-12-07 19:55:05.907728: step 47590, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 52h:28m:13s remains)
INFO - root - 2017-12-07 19:55:12.535688: step 47600, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 50h:11m:13s remains)
2017-12-07 19:55:13.282784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2175031 -4.2037435 -4.2066989 -4.215713 -4.2258821 -4.244163 -4.2441654 -4.241672 -4.2553444 -4.2719107 -4.2742858 -4.2743025 -4.2758241 -4.2752209 -4.2805543][-4.2039161 -4.1891289 -4.1950827 -4.2119393 -4.2289209 -4.2505078 -4.2495947 -4.2385468 -4.24488 -4.2576528 -4.2537875 -4.244247 -4.2410979 -4.2374487 -4.2486506][-4.1621919 -4.1618066 -4.1769953 -4.2040987 -4.2283039 -4.2489905 -4.245203 -4.2222939 -4.2217026 -4.2303677 -4.2216439 -4.2055149 -4.1994872 -4.1947351 -4.2115521][-4.1293197 -4.15563 -4.1869307 -4.2196631 -4.2425103 -4.2519293 -4.2343855 -4.1963673 -4.1883907 -4.1960421 -4.1883788 -4.1754241 -4.1741138 -4.1722927 -4.1934953][-4.1455669 -4.1911221 -4.2284989 -4.2494073 -4.2565627 -4.2438893 -4.2012806 -4.1437712 -4.1324906 -4.1549492 -4.1610446 -4.1605897 -4.1679597 -4.1693406 -4.19038][-4.181119 -4.2262716 -4.2527304 -4.2490029 -4.2332864 -4.2010145 -4.1309114 -4.0474195 -4.0369725 -4.0838404 -4.113236 -4.1322727 -4.1490889 -4.1551657 -4.1767941][-4.1888137 -4.2231059 -4.2298293 -4.2004027 -4.1648936 -4.1153226 -4.0166664 -3.9016888 -3.8941135 -3.97158 -4.0260611 -4.0644531 -4.0938292 -4.10679 -4.1242137][-4.167244 -4.1931467 -4.1844549 -4.1433296 -4.1014214 -4.0438719 -3.9329345 -3.8054285 -3.7995083 -3.8901277 -3.9569674 -4.0047469 -4.0400414 -4.0506792 -4.0532503][-4.162426 -4.1807032 -4.1622524 -4.1235723 -4.0903482 -4.043107 -3.956984 -3.8720429 -3.8770461 -3.9405994 -3.984977 -4.0199752 -4.0432305 -4.0404153 -4.0210423][-4.1906252 -4.201365 -4.1786003 -4.1462607 -4.1272469 -4.0977173 -4.0432196 -4.0052609 -4.0190139 -4.0507321 -4.0686231 -4.0865531 -4.0980916 -4.0881519 -4.0586252][-4.2415752 -4.2458282 -4.2234674 -4.1948175 -4.1852851 -4.168807 -4.1369867 -4.1221972 -4.134398 -4.1484809 -4.1541195 -4.1657562 -4.1752286 -4.1714263 -4.1476474][-4.2840014 -4.28438 -4.2656312 -4.2396636 -4.2318559 -4.2211103 -4.2004628 -4.1924281 -4.1997223 -4.2083411 -4.2144775 -4.2268248 -4.2425394 -4.2480497 -4.232749][-4.29876 -4.2969232 -4.2823305 -4.2595892 -4.2519145 -4.2447762 -4.2298484 -4.2236967 -4.2265773 -4.2328706 -4.2412615 -4.254621 -4.2753515 -4.2890458 -4.2821221][-4.2883387 -4.2869945 -4.2775126 -4.2602978 -4.2533412 -4.24503 -4.2304311 -4.2234445 -4.2236571 -4.2294922 -4.2395344 -4.2509651 -4.2719326 -4.2910752 -4.2918978][-4.25607 -4.2583613 -4.2579551 -4.2504334 -4.2470608 -4.2404661 -4.2264824 -4.2175112 -4.2142367 -4.2186327 -4.2251954 -4.2291441 -4.2424498 -4.2608128 -4.2677507]]...]
INFO - root - 2017-12-07 19:55:20.143361: step 47610, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 57h:04m:57s remains)
INFO - root - 2017-12-07 19:55:26.958332: step 47620, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 51h:42m:47s remains)
INFO - root - 2017-12-07 19:55:33.702790: step 47630, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 49h:54m:55s remains)
INFO - root - 2017-12-07 19:55:40.387032: step 47640, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 51h:59m:15s remains)
INFO - root - 2017-12-07 19:55:47.204323: step 47650, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 57h:42m:32s remains)
INFO - root - 2017-12-07 19:55:53.919512: step 47660, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 54h:47m:02s remains)
INFO - root - 2017-12-07 19:56:00.848223: step 47670, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.693 sec/batch; 54h:51m:03s remains)
INFO - root - 2017-12-07 19:56:07.478160: step 47680, loss = 2.10, batch loss = 2.04 (13.5 examples/sec; 0.595 sec/batch; 47h:02m:55s remains)
INFO - root - 2017-12-07 19:56:14.232775: step 47690, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 56h:37m:02s remains)
INFO - root - 2017-12-07 19:56:20.877770: step 47700, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 54h:00m:29s remains)
2017-12-07 19:56:21.656777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1499009 -4.1289897 -4.1144915 -4.1030021 -4.1043739 -4.1277537 -4.1617756 -4.1873527 -4.1943064 -4.1830707 -4.1600928 -4.1435776 -4.1366444 -4.1397476 -4.1545405][-4.1412911 -4.1285515 -4.1211114 -4.1212726 -4.1307368 -4.1527977 -4.17982 -4.2037654 -4.2156868 -4.2178097 -4.21115 -4.2050452 -4.1986275 -4.1944447 -4.1974983][-4.1333213 -4.1343694 -4.1361184 -4.1405349 -4.1432891 -4.1506743 -4.1644621 -4.1823373 -4.1966929 -4.2109785 -4.21896 -4.2201433 -4.2141252 -4.2132559 -4.2201962][-4.1182194 -4.136282 -4.1479797 -4.1490765 -4.1333389 -4.1127653 -4.1020885 -4.1083736 -4.1263065 -4.1542325 -4.1749249 -4.1838918 -4.1862693 -4.1967244 -4.2152438][-4.1109686 -4.1381636 -4.1528654 -4.1466427 -4.1108842 -4.0598044 -4.0182285 -4.0096521 -4.0387216 -4.0850329 -4.1154141 -4.1299024 -4.1426458 -4.1655831 -4.1929851][-4.1207619 -4.1443143 -4.1524014 -4.1386242 -4.094202 -4.032177 -3.972939 -3.950644 -3.9817693 -4.0397029 -4.0738106 -4.0872064 -4.1048603 -4.1340761 -4.165328][-4.1273394 -4.14539 -4.1496797 -4.1371 -4.0987468 -4.0488129 -3.9932358 -3.9608774 -3.9800472 -4.0302944 -4.0596881 -4.0680776 -4.0807405 -4.106873 -4.139164][-4.1372375 -4.1524711 -4.1579938 -4.153059 -4.1286411 -4.0976539 -4.0570254 -4.0260568 -4.034153 -4.0681834 -4.0875654 -4.090178 -4.0938873 -4.1084929 -4.1341057][-4.1736345 -4.1833076 -4.1906958 -4.1913409 -4.1784372 -4.1624432 -4.1379948 -4.1176472 -4.1195922 -4.1391711 -4.1478467 -4.1454215 -4.1421223 -4.1439891 -4.1589785][-4.223361 -4.22406 -4.2261963 -4.2289848 -4.2253671 -4.2198377 -4.2099872 -4.2035346 -4.2063947 -4.2129641 -4.2128992 -4.20681 -4.1993108 -4.1946764 -4.2020817][-4.2661228 -4.2601709 -4.2586741 -4.2624288 -4.2650981 -4.2676415 -4.2682161 -4.2717419 -4.2744217 -4.2697206 -4.2615204 -4.2542477 -4.2467394 -4.2412109 -4.2465873][-4.2910271 -4.2835269 -4.2802014 -4.2836881 -4.2894874 -4.2964706 -4.3036461 -4.3107209 -4.3133087 -4.3046727 -4.2923517 -4.2831841 -4.2764778 -4.2737727 -4.279789][-4.3078313 -4.3038816 -4.3007689 -4.3013415 -4.3058724 -4.3133335 -4.3215661 -4.3294253 -4.33393 -4.3279815 -4.3172116 -4.3080144 -4.3023267 -4.30148 -4.3067527][-4.3239264 -4.32411 -4.322618 -4.3217249 -4.3232203 -4.3275194 -4.3326035 -4.3380342 -4.3421154 -4.3405843 -4.3351803 -4.3295288 -4.3260474 -4.325738 -4.3287477][-4.3394771 -4.3407755 -4.3401761 -4.33862 -4.3373289 -4.3376722 -4.3390841 -4.3415852 -4.3443565 -4.3451419 -4.3439536 -4.3417182 -4.3403182 -4.3406763 -4.3417931]]...]
INFO - root - 2017-12-07 19:56:28.440677: step 47710, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 57h:50m:36s remains)
INFO - root - 2017-12-07 19:56:35.288568: step 47720, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.749 sec/batch; 59h:13m:51s remains)
INFO - root - 2017-12-07 19:56:41.976878: step 47730, loss = 2.08, batch loss = 2.03 (12.8 examples/sec; 0.625 sec/batch; 49h:26m:28s remains)
INFO - root - 2017-12-07 19:56:48.760585: step 47740, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 52h:21m:22s remains)
INFO - root - 2017-12-07 19:56:55.500959: step 47750, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.645 sec/batch; 51h:03m:11s remains)
INFO - root - 2017-12-07 19:57:02.478322: step 47760, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.753 sec/batch; 59h:32m:14s remains)
INFO - root - 2017-12-07 19:57:09.217040: step 47770, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 56h:26m:26s remains)
INFO - root - 2017-12-07 19:57:16.085617: step 47780, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 52h:30m:07s remains)
INFO - root - 2017-12-07 19:57:22.845649: step 47790, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 51h:06m:45s remains)
INFO - root - 2017-12-07 19:57:29.368460: step 47800, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 51h:48m:58s remains)
2017-12-07 19:57:30.111831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2718692 -4.2755871 -4.2739329 -4.2751536 -4.2648015 -4.2434425 -4.2255015 -4.2328649 -4.2522187 -4.2631927 -4.26752 -4.2667937 -4.2658992 -4.2629147 -4.2564192][-4.2661443 -4.2696919 -4.2682519 -4.271791 -4.2674003 -4.2490792 -4.2293053 -4.2345357 -4.2577128 -4.2745585 -4.2818956 -4.2832236 -4.2806144 -4.2722774 -4.2607007][-4.2628832 -4.2651129 -4.2618489 -4.2640672 -4.2613583 -4.24369 -4.2207713 -4.2257466 -4.25406 -4.2772245 -4.2878103 -4.29262 -4.2915196 -4.2796354 -4.262023][-4.2600231 -4.2587552 -4.2516 -4.249846 -4.247201 -4.2293139 -4.2061195 -4.2142344 -4.2481732 -4.2761617 -4.2895751 -4.2987046 -4.2987533 -4.2822394 -4.2571263][-4.2584009 -4.2529054 -4.2409358 -4.2342248 -4.2310209 -4.2149291 -4.1951461 -4.2085204 -4.2450004 -4.2743835 -4.28957 -4.3004446 -4.2982559 -4.27773 -4.2496934][-4.2562494 -4.2466469 -4.2308984 -4.2212367 -4.2187476 -4.2072067 -4.1941895 -4.2117538 -4.2454057 -4.2695737 -4.2817116 -4.2886462 -4.2804346 -4.2569275 -4.230834][-4.2539105 -4.240591 -4.2209897 -4.2089953 -4.2061148 -4.1967068 -4.1876283 -4.2079806 -4.2382932 -4.2565579 -4.2631884 -4.2639279 -4.248487 -4.2223582 -4.1980968][-4.2494717 -4.2317243 -4.2075934 -4.1919532 -4.1854887 -4.1745887 -4.1659985 -4.1897669 -4.2217841 -4.2362313 -4.2375441 -4.2328019 -4.2129431 -4.186646 -4.1680212][-4.2438412 -4.2219529 -4.1939054 -4.1744118 -4.1628838 -4.1485324 -4.1383605 -4.1645269 -4.1994429 -4.2095757 -4.205195 -4.1983871 -4.1810884 -4.1610847 -4.1500769][-4.239954 -4.2156672 -4.1859131 -4.1642933 -4.1486483 -4.1302686 -4.1165361 -4.1418872 -4.1773534 -4.1843233 -4.1760592 -4.1697187 -4.1583147 -4.14251 -4.1355247][-4.2384691 -4.2128158 -4.1831622 -4.1610289 -4.1432281 -4.1223111 -4.1037817 -4.1236773 -4.1550894 -4.1587749 -4.147687 -4.1406426 -4.1329851 -4.1200428 -4.1174183][-4.239212 -4.211967 -4.1813788 -4.1573539 -4.1378527 -4.1157231 -4.094305 -4.1092033 -4.1363306 -4.1393909 -4.1289949 -4.1251817 -4.122509 -4.1138682 -4.1170359][-4.2421522 -4.2122221 -4.179872 -4.1539035 -4.13397 -4.11362 -4.0948782 -4.1098275 -4.1344891 -4.1362882 -4.125237 -4.1217494 -4.1225781 -4.1188264 -4.126236][-4.2482114 -4.21533 -4.18023 -4.1519346 -4.1312132 -4.1134248 -4.1011782 -4.1194272 -4.1438379 -4.1457405 -4.1346459 -4.1303163 -4.1309958 -4.128592 -4.135663][-4.2573276 -4.2230272 -4.1863341 -4.1565175 -4.1351051 -4.1200671 -4.1141186 -4.1345706 -4.1572752 -4.1603594 -4.1530948 -4.1487074 -4.1485882 -4.1464252 -4.1503296]]...]
INFO - root - 2017-12-07 19:57:36.860520: step 47810, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 54h:21m:07s remains)
INFO - root - 2017-12-07 19:57:43.609222: step 47820, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 50h:19m:30s remains)
INFO - root - 2017-12-07 19:57:50.396289: step 47830, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 50h:05m:58s remains)
INFO - root - 2017-12-07 19:57:57.261714: step 47840, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 58h:03m:40s remains)
INFO - root - 2017-12-07 19:58:04.047307: step 47850, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 58h:22m:01s remains)
INFO - root - 2017-12-07 19:58:10.753088: step 47860, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.653 sec/batch; 51h:40m:03s remains)
INFO - root - 2017-12-07 19:58:17.635295: step 47870, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 51h:14m:26s remains)
INFO - root - 2017-12-07 19:58:24.421210: step 47880, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:25m:45s remains)
INFO - root - 2017-12-07 19:58:31.322662: step 47890, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.737 sec/batch; 58h:16m:52s remains)
INFO - root - 2017-12-07 19:58:37.901637: step 47900, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 55h:09m:07s remains)
2017-12-07 19:58:38.657312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981138 -4.2958393 -4.2981091 -4.2994061 -4.29896 -4.2981968 -4.2935991 -4.2893248 -4.2881947 -4.2822337 -4.2742805 -4.2616172 -4.2464905 -4.2391415 -4.2262831][-4.27355 -4.2695484 -4.2691031 -4.2659755 -4.2606487 -4.2563753 -4.2493267 -4.2444324 -4.2458696 -4.2421608 -4.23243 -4.2166905 -4.1949205 -4.1821222 -4.1604605][-4.2525244 -4.2484961 -4.2448006 -4.2340178 -4.2201557 -4.2073183 -4.193469 -4.1887116 -4.1939774 -4.1934986 -4.1854634 -4.1724277 -4.1512327 -4.1358023 -4.1086545][-4.2306857 -4.2276607 -4.2220368 -4.2052026 -4.1812634 -4.1535363 -4.1287112 -4.1232948 -4.1322236 -4.1369052 -4.1351562 -4.1322746 -4.1219773 -4.1110272 -4.0866265][-4.2201672 -4.2188334 -4.2099409 -4.1838017 -4.1456308 -4.0981784 -4.0625305 -4.057951 -4.0744171 -4.092742 -4.1100802 -4.1271772 -4.1343942 -4.1332979 -4.1167736][-4.2260914 -4.2260151 -4.2122316 -4.1715622 -4.11492 -4.0491638 -4.0027795 -3.9990692 -4.0223308 -4.0550904 -4.0948668 -4.1349072 -4.1614995 -4.1745543 -4.1735291][-4.2404432 -4.2415023 -4.2234392 -4.1747789 -4.1047196 -4.0286303 -3.9788947 -3.9809871 -4.0101104 -4.0502329 -4.1007905 -4.1550918 -4.1944075 -4.2195044 -4.2329168][-4.2504773 -4.2528791 -4.2336173 -4.1852679 -4.1155562 -4.0452347 -4.0034647 -4.0155377 -4.0460887 -4.0806723 -4.1261463 -4.1785765 -4.2204351 -4.250463 -4.2732239][-4.2535267 -4.2579517 -4.2421317 -4.2001953 -4.1440449 -4.092833 -4.0656366 -4.0777493 -4.1018682 -4.12784 -4.1638613 -4.2031393 -4.237555 -4.2658195 -4.2876678][-4.2447023 -4.25477 -4.2492986 -4.2208281 -4.181922 -4.1487927 -4.1327209 -4.14375 -4.1607785 -4.1787419 -4.2044382 -4.2279124 -4.2511406 -4.2744479 -4.292233][-4.2229557 -4.2398677 -4.2487369 -4.2389665 -4.2181625 -4.1970682 -4.187325 -4.1972122 -4.2104788 -4.2275825 -4.2479038 -4.2614326 -4.2744765 -4.2913585 -4.3045926][-4.1864066 -4.2101722 -4.2345381 -4.24184 -4.23632 -4.2253866 -4.2195711 -4.2274518 -4.2411046 -4.2597876 -4.2784939 -4.2892733 -4.296844 -4.3065529 -4.3137016][-4.1541624 -4.1785831 -4.2104821 -4.2292781 -4.2352676 -4.23421 -4.2349072 -4.24533 -4.2598333 -4.2796564 -4.2978873 -4.3072567 -4.3106537 -4.3146424 -4.3164444][-4.1532321 -4.1736889 -4.2011824 -4.2210345 -4.22846 -4.2322025 -4.2423582 -4.2599373 -4.2766352 -4.2960057 -4.3118296 -4.3184834 -4.3180532 -4.3197737 -4.3190427][-4.1876764 -4.1969833 -4.2089057 -4.2174492 -4.2189665 -4.2237964 -4.2419224 -4.2659225 -4.2874718 -4.3074183 -4.3205605 -4.3239312 -4.3203945 -4.3221793 -4.3220854]]...]
INFO - root - 2017-12-07 19:58:45.403437: step 47910, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 51h:14m:19s remains)
INFO - root - 2017-12-07 19:58:52.284987: step 47920, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 57h:22m:24s remains)
INFO - root - 2017-12-07 19:58:58.849365: step 47930, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 54h:00m:30s remains)
INFO - root - 2017-12-07 19:59:05.680451: step 47940, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 55h:08m:47s remains)
INFO - root - 2017-12-07 19:59:12.554350: step 47950, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 54h:27m:26s remains)
INFO - root - 2017-12-07 19:59:19.352451: step 47960, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 52h:18m:37s remains)
INFO - root - 2017-12-07 19:59:26.158593: step 47970, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 52h:15m:17s remains)
INFO - root - 2017-12-07 19:59:32.941731: step 47980, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 56h:45m:16s remains)
INFO - root - 2017-12-07 19:59:39.768161: step 47990, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.727 sec/batch; 57h:26m:12s remains)
INFO - root - 2017-12-07 19:59:46.088333: step 48000, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 50h:52m:49s remains)
2017-12-07 19:59:46.819237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3250213 -4.322298 -4.3200459 -4.320251 -4.3212271 -4.3221459 -4.3231282 -4.322423 -4.3212204 -4.3200374 -4.3210874 -4.3231621 -4.3238807 -4.3236618 -4.324327][-4.3177705 -4.312418 -4.3076386 -4.3075552 -4.3083982 -4.306632 -4.3022976 -4.2966766 -4.2923827 -4.2902079 -4.2941885 -4.3013105 -4.3053641 -4.3067946 -4.3101058][-4.3051653 -4.2977457 -4.2897215 -4.2873921 -4.2861776 -4.2807684 -4.2688971 -4.2563014 -4.246944 -4.2422881 -4.248075 -4.2603979 -4.2692676 -4.2746983 -4.2833786][-4.2907996 -4.2789574 -4.2646284 -4.2573876 -4.2525225 -4.2406964 -4.2186918 -4.1973467 -4.1815124 -4.1707854 -4.1782508 -4.1966949 -4.2121038 -4.22341 -4.2408743][-4.2729654 -4.2545824 -4.2325153 -4.2178 -4.2042103 -4.1807466 -4.1446643 -4.1121855 -4.0889025 -4.0721 -4.0840983 -4.1125097 -4.136939 -4.155798 -4.1843214][-4.2479305 -4.2226515 -4.1954556 -4.1719708 -4.1451035 -4.1066508 -4.0547504 -4.0143723 -3.9862528 -3.9689827 -3.9898612 -4.0282664 -4.0613055 -4.0866241 -4.1254115][-4.2253461 -4.1923566 -4.1604195 -4.1274652 -4.0834327 -4.028656 -3.9723558 -3.9374053 -3.9150076 -3.9025359 -3.9268928 -3.9668818 -4.0025821 -4.0309553 -4.0764356][-4.2118421 -4.1722832 -4.134306 -4.0939708 -4.0390086 -3.9788797 -3.9317381 -3.9152484 -3.9106905 -3.9069772 -3.9241383 -3.952811 -3.9808221 -4.0049481 -4.0509462][-4.2091107 -4.1680989 -4.1293879 -4.0899754 -4.03997 -3.9930334 -3.9628248 -3.9642694 -3.9756927 -3.9774206 -3.9864168 -4.0008287 -4.0120726 -4.02011 -4.0572958][-4.224319 -4.1904278 -4.1611185 -4.1316128 -4.0977597 -4.0701303 -4.0566969 -4.0630546 -4.0787821 -4.0812144 -4.0828028 -4.0826344 -4.0770459 -4.0689836 -4.0945268][-4.2484384 -4.2234812 -4.2049985 -4.1898785 -4.1728172 -4.1615343 -4.1566048 -4.15711 -4.1662922 -4.1663132 -4.1630735 -4.1562963 -4.1445584 -4.1317697 -4.1493411][-4.278543 -4.2636032 -4.2532716 -4.2492404 -4.2413678 -4.2367449 -4.2345171 -4.2297077 -4.231638 -4.2301283 -4.2260652 -4.22025 -4.2101388 -4.1999097 -4.2107167][-4.3000441 -4.2937551 -4.2920613 -4.293036 -4.2898445 -4.2861109 -4.2844057 -4.2771716 -4.2745771 -4.2715831 -4.2697811 -4.2687817 -4.264081 -4.2578106 -4.2634168][-4.3066993 -4.3045263 -4.3061609 -4.3089757 -4.3073125 -4.3038316 -4.3023362 -4.2979155 -4.2969775 -4.2956886 -4.2969513 -4.2995348 -4.2993107 -4.2971363 -4.2998261][-4.3064313 -4.3030992 -4.3031249 -4.303535 -4.3019538 -4.2990985 -4.298512 -4.2980294 -4.2998934 -4.302494 -4.3062749 -4.3097873 -4.3121281 -4.3139815 -4.31652]]...]
INFO - root - 2017-12-07 19:59:53.555350: step 48010, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 56h:38m:25s remains)
INFO - root - 2017-12-07 20:00:00.408899: step 48020, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 54h:26m:56s remains)
INFO - root - 2017-12-07 20:00:07.146139: step 48030, loss = 2.04, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 50h:13m:08s remains)
INFO - root - 2017-12-07 20:00:13.976910: step 48040, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.618 sec/batch; 48h:52m:05s remains)
INFO - root - 2017-12-07 20:00:20.871018: step 48050, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.739 sec/batch; 58h:25m:16s remains)
INFO - root - 2017-12-07 20:00:27.622328: step 48060, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 56h:21m:02s remains)
INFO - root - 2017-12-07 20:00:34.477762: step 48070, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 54h:37m:21s remains)
INFO - root - 2017-12-07 20:00:41.263375: step 48080, loss = 2.04, batch loss = 1.98 (13.0 examples/sec; 0.613 sec/batch; 48h:27m:10s remains)
INFO - root - 2017-12-07 20:00:48.078605: step 48090, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 51h:42m:53s remains)
INFO - root - 2017-12-07 20:00:54.793290: step 48100, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 58h:32m:20s remains)
2017-12-07 20:00:55.481106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2452779 -4.2431731 -4.2515111 -4.2612672 -4.2666721 -4.2628756 -4.2591372 -4.2582932 -4.2675743 -4.2827711 -4.2943778 -4.29795 -4.2979631 -4.2968936 -4.301229][-4.2244296 -4.2148852 -4.2121024 -4.2168636 -4.2249475 -4.220346 -4.2168732 -4.2237744 -4.2435217 -4.2660813 -4.2807822 -4.2869892 -4.2901006 -4.2913451 -4.3000422][-4.1762567 -4.1715961 -4.1651483 -4.1619158 -4.17136 -4.1746755 -4.1823711 -4.1998477 -4.2279515 -4.2465243 -4.2520628 -4.2522354 -4.25308 -4.2569556 -4.2700253][-4.1320353 -4.1446433 -4.1448593 -4.1304555 -4.1280274 -4.1306887 -4.1443677 -4.1706066 -4.2005682 -4.2174654 -4.2210636 -4.2165985 -4.2163663 -4.2211208 -4.2354393][-4.0993528 -4.1356273 -4.1517963 -4.1304884 -4.1122327 -4.1067104 -4.115109 -4.1317868 -4.1497211 -4.1617112 -4.1713657 -4.1725874 -4.175952 -4.184062 -4.2000952][-4.085485 -4.1379752 -4.167582 -4.143888 -4.1120892 -4.0900455 -4.0771427 -4.0712543 -4.075037 -4.0858474 -4.1086416 -4.1261315 -4.1427078 -4.1626186 -4.1872416][-4.1109271 -4.1535797 -4.1768804 -4.1434593 -4.0922785 -4.0517426 -4.0232306 -4.0053997 -4.0112128 -4.0317287 -4.0709548 -4.1037307 -4.1312308 -4.16366 -4.1988721][-4.1449432 -4.1607742 -4.1647458 -4.1226058 -4.0652647 -4.0262861 -4.0025129 -3.9882369 -4.0013304 -4.0289474 -4.0800419 -4.12519 -4.1559196 -4.1901321 -4.2243972][-4.1836619 -4.1760325 -4.1581993 -4.112422 -4.0664062 -4.0431747 -4.0333452 -4.0282574 -4.0446825 -4.0700326 -4.1184483 -4.1698041 -4.2012525 -4.2317753 -4.2585106][-4.2103567 -4.1929297 -4.164156 -4.12106 -4.0892882 -4.0791311 -4.0831804 -4.0899453 -4.1065917 -4.1255822 -4.158381 -4.2011566 -4.2305722 -4.2571735 -4.2789822][-4.2122731 -4.1997542 -4.1747546 -4.142067 -4.1237597 -4.12225 -4.1344724 -4.1473908 -4.1611538 -4.1715322 -4.1911664 -4.2245378 -4.247921 -4.2673173 -4.2854156][-4.20954 -4.1993294 -4.1775455 -4.156343 -4.1461511 -4.150485 -4.1676092 -4.1806445 -4.1871333 -4.1894712 -4.2003455 -4.2256484 -4.2419286 -4.2560706 -4.27235][-4.2031565 -4.1880431 -4.1658411 -4.1506691 -4.1464744 -4.1510539 -4.1669869 -4.1817789 -4.1833558 -4.1817241 -4.1880918 -4.2056823 -4.2181344 -4.2290478 -4.2469978][-4.2087207 -4.1866975 -4.1552696 -4.1314306 -4.1228628 -4.1241746 -4.1382723 -4.1559892 -4.1609826 -4.1606832 -4.1684132 -4.1829319 -4.190731 -4.1939683 -4.2102842][-4.2176332 -4.1943417 -4.1586452 -4.1243558 -4.1066656 -4.1012564 -4.1154666 -4.1382957 -4.1516242 -4.1553855 -4.1665225 -4.1810813 -4.1788039 -4.1673107 -4.1769447]]...]
INFO - root - 2017-12-07 20:01:02.237025: step 48110, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 50h:30m:45s remains)
INFO - root - 2017-12-07 20:01:09.104511: step 48120, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.683 sec/batch; 53h:56m:30s remains)
INFO - root - 2017-12-07 20:01:15.838677: step 48130, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 56h:34m:13s remains)
INFO - root - 2017-12-07 20:01:22.645629: step 48140, loss = 2.06, batch loss = 2.01 (10.7 examples/sec; 0.750 sec/batch; 59h:12m:24s remains)
INFO - root - 2017-12-07 20:01:29.487541: step 48150, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 54h:16m:03s remains)
INFO - root - 2017-12-07 20:01:36.234936: step 48160, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 50h:04m:52s remains)
INFO - root - 2017-12-07 20:01:43.055155: step 48170, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 50h:45m:53s remains)
INFO - root - 2017-12-07 20:01:49.966849: step 48180, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 56h:57m:05s remains)
INFO - root - 2017-12-07 20:01:56.767934: step 48190, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 57h:48m:09s remains)
INFO - root - 2017-12-07 20:02:03.307493: step 48200, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 51h:33m:34s remains)
2017-12-07 20:02:04.145588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.297945 -4.2838831 -4.2796865 -4.2846074 -4.2921515 -4.3016233 -4.3067045 -4.3055859 -4.3032269 -4.2997313 -4.2923994 -4.27844 -4.2653475 -4.2597704 -4.2616658][-4.2643824 -4.243228 -4.2355795 -4.2421284 -4.2535982 -4.2702937 -4.2761197 -4.2728915 -4.2706823 -4.2689085 -4.2622638 -4.2453117 -4.2306504 -4.2300792 -4.2409391][-4.2207479 -4.1904373 -4.1801677 -4.1872048 -4.2017183 -4.226161 -4.23348 -4.226841 -4.2220969 -4.2252831 -4.2251825 -4.2105408 -4.1971583 -4.204649 -4.22701][-4.1748328 -4.13807 -4.1251779 -4.131043 -4.1479974 -4.18009 -4.1870542 -4.1674852 -4.156034 -4.1643567 -4.1747026 -4.1640735 -4.1566634 -4.1742625 -4.2067661][-4.138515 -4.1029716 -4.0911088 -4.0988441 -4.1176386 -4.1456633 -4.1403923 -4.09332 -4.0725107 -4.0950365 -4.1216431 -4.1225133 -4.1244221 -4.1516395 -4.1878834][-4.1195207 -4.0939732 -4.0881352 -4.0933051 -4.1029196 -4.1112814 -4.0755205 -3.9859064 -3.9538722 -4.0028954 -4.0582662 -4.081634 -4.103693 -4.1430669 -4.1844726][-4.1158323 -4.1051435 -4.1006517 -4.0896316 -4.0731616 -4.0473866 -3.9641595 -3.8151214 -3.7676511 -3.8635609 -3.9633074 -4.019486 -4.0650077 -4.1257234 -4.1771455][-4.1114106 -4.1096478 -4.1039524 -4.0770974 -4.0375824 -3.9874933 -3.8703408 -3.67266 -3.6061776 -3.7455239 -3.8902206 -3.9771702 -4.0369573 -4.10382 -4.1555295][-4.1277919 -4.1266265 -4.1196952 -4.0908012 -4.052927 -4.0139737 -3.9282451 -3.778178 -3.7195725 -3.8278513 -3.9502671 -4.0239892 -4.0628891 -4.10715 -4.1447372][-4.1666517 -4.1651192 -4.1593866 -4.1358829 -4.1093378 -4.091733 -4.0549669 -3.9734242 -3.9379945 -4.0049925 -4.0795145 -4.1154218 -4.1226363 -4.139194 -4.1628675][-4.214921 -4.2163815 -4.2114491 -4.1966209 -4.1820259 -4.1774654 -4.1673374 -4.1302238 -4.1129384 -4.1496873 -4.1852455 -4.1936693 -4.1835361 -4.189373 -4.20879][-4.2653732 -4.2714629 -4.2689419 -4.25942 -4.2523894 -4.2544589 -4.25675 -4.2438731 -4.2372422 -4.2560716 -4.2700782 -4.2631607 -4.2454367 -4.2473226 -4.2643843][-4.2963767 -4.3046546 -4.3021812 -4.2945037 -4.2904119 -4.2941222 -4.2997437 -4.2966027 -4.2968588 -4.3060222 -4.3128963 -4.3026185 -4.2834859 -4.2819734 -4.294064][-4.3061018 -4.3110466 -4.3078489 -4.3028774 -4.3010569 -4.3050671 -4.3103094 -4.3116522 -4.3160567 -4.320735 -4.3227973 -4.313261 -4.2962213 -4.2929726 -4.3039336][-4.30977 -4.3091159 -4.3057189 -4.3036289 -4.3046517 -4.3073382 -4.3112535 -4.3135881 -4.3168292 -4.3180151 -4.3167319 -4.3091187 -4.2971153 -4.2955909 -4.3058448]]...]
INFO - root - 2017-12-07 20:02:10.923268: step 48210, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 55h:36m:41s remains)
INFO - root - 2017-12-07 20:02:17.646589: step 48220, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 54h:06m:46s remains)
INFO - root - 2017-12-07 20:02:24.410926: step 48230, loss = 2.04, batch loss = 1.98 (13.2 examples/sec; 0.607 sec/batch; 47h:54m:39s remains)
INFO - root - 2017-12-07 20:02:31.215265: step 48240, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 51h:36m:06s remains)
INFO - root - 2017-12-07 20:02:38.005551: step 48250, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 56h:38m:27s remains)
INFO - root - 2017-12-07 20:02:44.841923: step 48260, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.716 sec/batch; 56h:33m:41s remains)
INFO - root - 2017-12-07 20:02:51.607627: step 48270, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 51h:44m:14s remains)
INFO - root - 2017-12-07 20:02:58.389220: step 48280, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 51h:08m:12s remains)
INFO - root - 2017-12-07 20:03:05.135858: step 48290, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 49h:54m:25s remains)
INFO - root - 2017-12-07 20:03:11.843125: step 48300, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 54h:32m:28s remains)
2017-12-07 20:03:12.615336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2529845 -4.2435184 -4.2310085 -4.2179823 -4.2062187 -4.2037745 -4.2078891 -4.21278 -4.2272196 -4.2368836 -4.2233353 -4.210278 -4.2092528 -4.2126117 -4.2258759][-4.2628441 -4.2526746 -4.2438545 -4.2343407 -4.2236643 -4.2183514 -4.2150054 -4.2186642 -4.233737 -4.2328095 -4.2110429 -4.1985412 -4.1991897 -4.2031422 -4.21999][-4.2545786 -4.2462616 -4.2403293 -4.2331882 -4.2207613 -4.2156105 -4.2062426 -4.21278 -4.2285905 -4.2183971 -4.1951604 -4.1893253 -4.1940966 -4.1976457 -4.21313][-4.2141881 -4.209394 -4.2106495 -4.2101383 -4.198369 -4.1885791 -4.1740866 -4.1805964 -4.2017174 -4.1971254 -4.1756668 -4.1772184 -4.1914635 -4.1963925 -4.2066174][-4.1678143 -4.1620126 -4.161839 -4.1639605 -4.1495829 -4.1305132 -4.1048293 -4.1100593 -4.1503706 -4.1645665 -4.1495223 -4.1604271 -4.1858163 -4.1944695 -4.1998472][-4.1235437 -4.1063709 -4.0960736 -4.0921078 -4.0763512 -4.0485253 -4.0075541 -4.0125971 -4.0810814 -4.1177535 -4.1134529 -4.1335816 -4.1700721 -4.1881132 -4.1960087][-4.0903449 -4.05535 -4.0356016 -4.0227828 -4.0006113 -3.9567754 -3.8859332 -3.8874936 -3.9956388 -4.0634665 -4.0796776 -4.1071615 -4.1491485 -4.178823 -4.1952839][-4.099329 -4.060389 -4.0323815 -4.011097 -3.9888852 -3.944809 -3.8693855 -3.8624518 -3.9737058 -4.0531974 -4.0789595 -4.1029425 -4.138154 -4.1723542 -4.19533][-4.1261549 -4.1002841 -4.0797272 -4.0634189 -4.0576315 -4.0388589 -3.99416 -3.9904006 -4.0610662 -4.1135535 -4.1272469 -4.1370873 -4.156579 -4.1818075 -4.200902][-4.1510382 -4.1440411 -4.1360846 -4.1294117 -4.1385741 -4.142302 -4.122756 -4.1232743 -4.1618919 -4.1892624 -4.1884966 -4.1815262 -4.1840262 -4.1971807 -4.2095375][-4.1565652 -4.1613874 -4.1641006 -4.1661863 -4.1861405 -4.2029734 -4.1986246 -4.2009454 -4.22145 -4.2331095 -4.2238579 -4.2096663 -4.2076645 -4.216989 -4.22371][-4.12254 -4.1321468 -4.1430588 -4.1602111 -4.1938958 -4.2229347 -4.2254996 -4.223444 -4.2320495 -4.2314334 -4.2182007 -4.2097106 -4.216876 -4.23035 -4.2374916][-4.091814 -4.0920315 -4.1003652 -4.1227264 -4.1592736 -4.1909928 -4.1961365 -4.1929989 -4.2017474 -4.2022991 -4.1937385 -4.19759 -4.2167268 -4.2351723 -4.2433004][-4.0874386 -4.07835 -4.0800314 -4.09707 -4.1217132 -4.1404347 -4.1347766 -4.1270165 -4.1391311 -4.1507215 -4.1587782 -4.1769772 -4.202929 -4.2286181 -4.2417297][-4.1098118 -4.0948782 -4.0935774 -4.1050277 -4.1156607 -4.1164012 -4.0914159 -4.0742946 -4.0868392 -4.1106844 -4.1346741 -4.1633544 -4.1912494 -4.2190638 -4.2347975]]...]
INFO - root - 2017-12-07 20:03:19.198743: step 48310, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 50h:54m:22s remains)
INFO - root - 2017-12-07 20:03:25.917184: step 48320, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 51h:14m:54s remains)
INFO - root - 2017-12-07 20:03:32.724600: step 48330, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 57h:03m:09s remains)
INFO - root - 2017-12-07 20:03:39.673977: step 48340, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 56h:50m:22s remains)
INFO - root - 2017-12-07 20:03:46.495325: step 48350, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 52h:54m:18s remains)
INFO - root - 2017-12-07 20:03:53.355210: step 48360, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 51h:54m:22s remains)
INFO - root - 2017-12-07 20:04:00.178216: step 48370, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 51h:49m:14s remains)
INFO - root - 2017-12-07 20:04:06.934973: step 48380, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 54h:22m:12s remains)
INFO - root - 2017-12-07 20:04:13.715031: step 48390, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 53h:15m:52s remains)
INFO - root - 2017-12-07 20:04:20.221319: step 48400, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 51h:52m:21s remains)
2017-12-07 20:04:20.908899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3516788 -4.3400869 -4.3253284 -4.3018136 -4.2467585 -4.1710653 -4.0947914 -4.0378628 -4.0859752 -4.1788311 -4.2089386 -4.1939907 -4.180356 -4.1827726 -4.1783419][-4.35382 -4.3432708 -4.3304338 -4.3086762 -4.2517 -4.1706305 -4.0828166 -4.0200577 -4.0717607 -4.1683817 -4.2056336 -4.1946454 -4.1855187 -4.19147 -4.1876054][-4.3550777 -4.3452644 -4.3336949 -4.3126583 -4.2535276 -4.1667504 -4.0671864 -3.9974604 -4.0514612 -4.1509714 -4.1947351 -4.1916256 -4.1889977 -4.1994228 -4.1977][-4.3563557 -4.3465381 -4.33549 -4.3153853 -4.2559195 -4.1664677 -4.0575666 -3.9795856 -4.0344467 -4.1368728 -4.1841922 -4.1891246 -4.1914291 -4.2020392 -4.20048][-4.3569183 -4.3464193 -4.3353434 -4.3155103 -4.2562466 -4.1657357 -4.048964 -3.9613407 -4.0184522 -4.1249161 -4.1757455 -4.1871533 -4.1901093 -4.1981616 -4.1981158][-4.3569026 -4.3458104 -4.334095 -4.3131871 -4.254 -4.163444 -4.0411215 -3.9434633 -4.0016241 -4.1113706 -4.1629863 -4.1797709 -4.1847119 -4.1940494 -4.198875][-4.3570833 -4.3456063 -4.3332019 -4.3111639 -4.2520757 -4.1619124 -4.0363016 -3.9320042 -3.9890664 -4.1005411 -4.1526108 -4.1742158 -4.1854591 -4.20053 -4.2112174][-4.3568025 -4.3450804 -4.3319755 -4.30865 -4.2497978 -4.1616297 -4.0397024 -3.93779 -3.9945083 -4.1052828 -4.1574278 -4.1816993 -4.1980748 -4.2164135 -4.2283115][-4.3563251 -4.3444228 -4.3306885 -4.3067822 -4.24955 -4.1664453 -4.05451 -3.9629045 -4.0184069 -4.1246896 -4.1763844 -4.1998887 -4.2162108 -4.2326746 -4.2429538][-4.3553815 -4.3436503 -4.3298583 -4.3065434 -4.2517967 -4.1744156 -4.0725994 -3.9925098 -4.0475626 -4.148571 -4.1976228 -4.2163348 -4.2302079 -4.2437391 -4.2520909][-4.3540063 -4.3421059 -4.3286967 -4.3062487 -4.25407 -4.1810937 -4.087615 -4.018239 -4.0732651 -4.170826 -4.2161655 -4.2299089 -4.2420678 -4.2543426 -4.2616348][-4.3515954 -4.3389592 -4.3252554 -4.3034205 -4.2524629 -4.1839495 -4.0983877 -4.0393643 -4.0954847 -4.1899309 -4.23125 -4.2409778 -4.253479 -4.2657604 -4.2723436][-4.34872 -4.3349128 -4.3212566 -4.2992005 -4.2496667 -4.1865058 -4.1100578 -4.0615454 -4.117424 -4.2053685 -4.2414665 -4.2489066 -4.2617011 -4.2729006 -4.2787285][-4.3462758 -4.3316526 -4.3184481 -4.2970829 -4.2507577 -4.1957083 -4.1328797 -4.0944858 -4.1426125 -4.2180786 -4.247541 -4.25398 -4.26628 -4.2766161 -4.2821383][-4.3447523 -4.3294654 -4.3171172 -4.2978826 -4.2573109 -4.2124829 -4.1638403 -4.1326408 -4.1689496 -4.2296805 -4.2554717 -4.2640815 -4.2758994 -4.2841992 -4.2881131]]...]
INFO - root - 2017-12-07 20:04:27.676420: step 48410, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 56h:29m:04s remains)
INFO - root - 2017-12-07 20:04:34.535369: step 48420, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 57h:07m:38s remains)
INFO - root - 2017-12-07 20:04:41.293268: step 48430, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 54h:34m:33s remains)
INFO - root - 2017-12-07 20:04:48.046291: step 48440, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 50h:47m:57s remains)
INFO - root - 2017-12-07 20:04:54.812382: step 48450, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 50h:08m:03s remains)
INFO - root - 2017-12-07 20:05:01.594625: step 48460, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 57h:26m:35s remains)
INFO - root - 2017-12-07 20:05:08.350379: step 48470, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.772 sec/batch; 60h:53m:00s remains)
INFO - root - 2017-12-07 20:05:15.141539: step 48480, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 53h:28m:29s remains)
INFO - root - 2017-12-07 20:05:21.816670: step 48490, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 50h:37m:10s remains)
INFO - root - 2017-12-07 20:05:28.495867: step 48500, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 56h:42m:45s remains)
2017-12-07 20:05:29.222812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0755262 -4.1057544 -4.1454277 -4.1742969 -4.1779642 -4.1659603 -4.14279 -4.1215692 -4.11742 -4.1368971 -4.1853805 -4.2299452 -4.2442036 -4.2277284 -4.18596][-4.1178117 -4.1378694 -4.1631393 -4.1765947 -4.1766539 -4.1698642 -4.1531615 -4.1305389 -4.118753 -4.1301951 -4.1770854 -4.2225223 -4.234097 -4.2060618 -4.1536441][-4.155427 -4.1653633 -4.1794834 -4.1844916 -4.1830978 -4.1734691 -4.1550417 -4.13065 -4.1103878 -4.1109877 -4.1492553 -4.1948862 -4.211359 -4.1786447 -4.1207638][-4.191875 -4.2003694 -4.2083926 -4.2043047 -4.190835 -4.1665707 -4.1320696 -4.0945349 -4.0710311 -4.0701275 -4.1065769 -4.1545687 -4.1800594 -4.1563411 -4.1068621][-4.22908 -4.2478285 -4.2574368 -4.237865 -4.1926885 -4.1330924 -4.0657244 -4.0063314 -3.9853849 -4.005641 -4.0641656 -4.1250873 -4.1628909 -4.1559486 -4.1217122][-4.2567677 -4.2867589 -4.3007975 -4.2563462 -4.1637263 -4.0528297 -3.9372749 -3.8538299 -3.8512216 -3.9276886 -4.0396204 -4.1251378 -4.1727819 -4.1794639 -4.158761][-4.2727561 -4.3012443 -4.3109732 -4.2397 -4.1068435 -3.9481249 -3.7911491 -3.7040093 -3.7421799 -3.8759923 -4.0343113 -4.1442552 -4.200304 -4.2138262 -4.2014937][-4.2772608 -4.2995172 -4.3070164 -4.2338448 -4.1004672 -3.9355896 -3.768791 -3.6840727 -3.7555137 -3.9144549 -4.0664659 -4.1704879 -4.2269197 -4.243691 -4.2361956][-4.2667556 -4.2817836 -4.2918673 -4.2375441 -4.1351309 -4.0067787 -3.864172 -3.7831 -3.8627985 -4.0094509 -4.1242819 -4.2044988 -4.2558079 -4.2676406 -4.2585173][-4.2446795 -4.2611837 -4.2809677 -4.2574053 -4.1983752 -4.1140065 -4.0092826 -3.9477935 -4.0099444 -4.1129537 -4.1786275 -4.2266579 -4.2672992 -4.2780123 -4.2692533][-4.2266779 -4.2506666 -4.2849174 -4.2860756 -4.2559052 -4.2019281 -4.1306367 -4.0885553 -4.131423 -4.1888204 -4.2126584 -4.2362981 -4.2649159 -4.2752829 -4.2719216][-4.210712 -4.2423038 -4.285656 -4.2945876 -4.2718511 -4.2308831 -4.1810665 -4.1543789 -4.1830792 -4.2145643 -4.2273641 -4.2466373 -4.26246 -4.26571 -4.2632995][-4.1925836 -4.2333031 -4.2823787 -4.29312 -4.2721567 -4.240521 -4.2109556 -4.1985 -4.20875 -4.2291965 -4.2478027 -4.2656317 -4.2652974 -4.2532024 -4.2476406][-4.1737347 -4.2200704 -4.2663965 -4.2766242 -4.2594194 -4.2370739 -4.2266583 -4.2277932 -4.2304068 -4.2441416 -4.2659106 -4.2752962 -4.2624526 -4.2423153 -4.2283077][-4.1589637 -4.2 -4.2361178 -4.2453279 -4.2334805 -4.2187319 -4.223629 -4.2314458 -4.239707 -4.2573814 -4.2764874 -4.2763391 -4.2536507 -4.2233868 -4.1997948]]...]
INFO - root - 2017-12-07 20:05:36.016366: step 48510, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 51h:03m:01s remains)
INFO - root - 2017-12-07 20:05:42.803296: step 48520, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 51h:23m:06s remains)
INFO - root - 2017-12-07 20:05:49.715938: step 48530, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.724 sec/batch; 57h:05m:03s remains)
INFO - root - 2017-12-07 20:05:56.542209: step 48540, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 54h:50m:07s remains)
INFO - root - 2017-12-07 20:06:03.352841: step 48550, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 54h:01m:13s remains)
INFO - root - 2017-12-07 20:06:10.190506: step 48560, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 52h:12m:14s remains)
INFO - root - 2017-12-07 20:06:17.014698: step 48570, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.669 sec/batch; 52h:45m:50s remains)
INFO - root - 2017-12-07 20:06:23.911125: step 48580, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 58h:21m:50s remains)
INFO - root - 2017-12-07 20:06:30.718792: step 48590, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 53h:32m:37s remains)
INFO - root - 2017-12-07 20:06:37.304893: step 48600, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 50h:50m:15s remains)
2017-12-07 20:06:38.062848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3570881 -4.3543925 -4.3495765 -4.3460011 -4.3438773 -4.3424859 -4.3424273 -4.3444304 -4.3471475 -4.3484721 -4.3498578 -4.3514671 -4.3538542 -4.3555732 -4.3575935][-4.3529091 -4.34481 -4.3348789 -4.3273325 -4.3240457 -4.3245311 -4.327343 -4.3326349 -4.339375 -4.3439913 -4.3463111 -4.3467274 -4.3480864 -4.3488603 -4.3506894][-4.3363318 -4.3213625 -4.3052478 -4.2937746 -4.2910094 -4.2970033 -4.305387 -4.3151646 -4.32502 -4.3320107 -4.33496 -4.3338294 -4.3339281 -4.3352575 -4.339932][-4.3056426 -4.2856765 -4.2644114 -4.2480659 -4.2418971 -4.2495341 -4.2592711 -4.2686095 -4.2769003 -4.2831607 -4.2866039 -4.2881851 -4.29084 -4.2969208 -4.3097181][-4.2733545 -4.2528172 -4.2286296 -4.2081065 -4.1940427 -4.1921153 -4.193079 -4.1939192 -4.1974506 -4.202548 -4.2062421 -4.2093091 -4.2172866 -4.2342854 -4.2609963][-4.2350168 -4.2144713 -4.1929765 -4.1743889 -4.1542406 -4.1349673 -4.1156325 -4.0970268 -4.0900168 -4.0944109 -4.1016431 -4.1081157 -4.1271605 -4.1633525 -4.2093353][-4.1861467 -4.1673322 -4.15479 -4.14148 -4.113369 -4.0698624 -4.0201797 -3.9787409 -3.9652462 -3.9774714 -4.0011034 -4.0196323 -4.0547228 -4.111444 -4.1741295][-4.1439786 -4.1298184 -4.1270771 -4.1177263 -4.083272 -4.013989 -3.9322631 -3.8824916 -3.8855531 -3.922298 -3.9656293 -3.9968929 -4.0403891 -4.1058207 -4.172133][-4.1273727 -4.1238127 -4.1319909 -4.1284461 -4.0927238 -4.0167279 -3.9333658 -3.9041462 -3.9324162 -3.9822712 -4.0280995 -4.0582886 -4.0950923 -4.1514111 -4.2060313][-4.1359644 -4.1431966 -4.1604204 -4.1641593 -4.1433811 -4.0960579 -4.0462461 -4.0360026 -4.0643425 -4.1012444 -4.1297379 -4.1446285 -4.1640954 -4.2023916 -4.2416754][-4.155211 -4.1668978 -4.1832652 -4.1928105 -4.1933651 -4.1793437 -4.1585879 -4.1572328 -4.1750236 -4.1951747 -4.2032409 -4.198514 -4.2003388 -4.2246027 -4.2553258][-4.1811204 -4.1837373 -4.1871958 -4.1954613 -4.2103381 -4.2113194 -4.2027192 -4.2074485 -4.22408 -4.2385688 -4.2307105 -4.2070723 -4.1957088 -4.2109652 -4.2424145][-4.1903663 -4.1760416 -4.1648092 -4.1664529 -4.1833215 -4.1893616 -4.1897535 -4.20675 -4.2330246 -4.2471232 -4.2312846 -4.1986556 -4.1813254 -4.1945171 -4.2292309][-4.1853867 -4.1595016 -4.1351228 -4.1308904 -4.1477871 -4.1601176 -4.1677055 -4.199162 -4.228663 -4.2392344 -4.22304 -4.1904316 -4.17389 -4.18742 -4.2215424][-4.1768708 -4.1395178 -4.103189 -4.0969291 -4.116425 -4.1378193 -4.1545749 -4.1922336 -4.2161546 -4.2207 -4.2079582 -4.1833925 -4.1734595 -4.1890836 -4.2199721]]...]
INFO - root - 2017-12-07 20:06:44.949771: step 48610, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 56h:43m:26s remains)
INFO - root - 2017-12-07 20:06:51.600861: step 48620, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 50h:31m:19s remains)
INFO - root - 2017-12-07 20:06:58.456003: step 48630, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 51h:32m:47s remains)
INFO - root - 2017-12-07 20:07:05.340561: step 48640, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 57h:09m:27s remains)
INFO - root - 2017-12-07 20:07:12.289585: step 48650, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.768 sec/batch; 60h:33m:06s remains)
INFO - root - 2017-12-07 20:07:19.128963: step 48660, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 54h:25m:28s remains)
INFO - root - 2017-12-07 20:07:25.903745: step 48670, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.606 sec/batch; 47h:46m:58s remains)
INFO - root - 2017-12-07 20:07:32.758540: step 48680, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 52h:24m:00s remains)
INFO - root - 2017-12-07 20:07:39.671622: step 48690, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.722 sec/batch; 56h:54m:29s remains)
INFO - root - 2017-12-07 20:07:46.290887: step 48700, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 53h:41m:27s remains)
2017-12-07 20:07:47.064625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3265858 -4.3240008 -4.306993 -4.2664948 -4.2021041 -4.135623 -4.1068645 -4.1329088 -4.1845503 -4.224165 -4.2494259 -4.2654681 -4.273478 -4.2841191 -4.3023758][-4.3311462 -4.3262343 -4.2996445 -4.2430511 -4.16646 -4.1010962 -4.0851889 -4.1263828 -4.1809564 -4.2164969 -4.2418747 -4.2645059 -4.278275 -4.2932014 -4.3111358][-4.3348532 -4.325541 -4.2868619 -4.2157259 -4.1339684 -4.0748281 -4.0735426 -4.1254759 -4.1796365 -4.2121272 -4.2396317 -4.2647624 -4.2795496 -4.2947445 -4.311686][-4.3396363 -4.3240066 -4.2719083 -4.1902909 -4.1082735 -4.0569916 -4.0700612 -4.1311126 -4.1840534 -4.2145939 -4.2446351 -4.2715607 -4.2853875 -4.297389 -4.3113093][-4.3441429 -4.3212543 -4.2576437 -4.1700015 -4.0903525 -4.0487018 -4.0760489 -4.144702 -4.1961036 -4.2266779 -4.2586856 -4.28567 -4.2963924 -4.3041873 -4.3136525][-4.3464994 -4.3165855 -4.2450643 -4.1538992 -4.0783539 -4.0481677 -4.0860243 -4.15682 -4.2064767 -4.2373705 -4.2715821 -4.298954 -4.308517 -4.3130097 -4.3192][-4.3430014 -4.3087788 -4.2338424 -4.1422329 -4.0675235 -4.0400548 -4.0796928 -4.1516509 -4.2030067 -4.2359576 -4.274261 -4.304626 -4.3171539 -4.321126 -4.3250079][-4.3361912 -4.2992525 -4.2248011 -4.1343784 -4.0577178 -4.0233207 -4.0616822 -4.1382494 -4.1965947 -4.2352691 -4.2770195 -4.3090754 -4.3248181 -4.3280883 -4.3294921][-4.3288431 -4.2919335 -4.2197819 -4.1317949 -4.0557313 -4.0178676 -4.0580235 -4.1400557 -4.20713 -4.2521391 -4.2913756 -4.3194103 -4.3319645 -4.3318453 -4.3294759][-4.3218842 -4.2863855 -4.2175517 -4.1337318 -4.0651808 -4.0373693 -4.0811892 -4.1613655 -4.229701 -4.2722125 -4.3019743 -4.3234444 -4.3312411 -4.3291707 -4.3245859][-4.3163786 -4.2822461 -4.2176285 -4.1388927 -4.0811644 -4.0679355 -4.1140575 -4.1879883 -4.2488928 -4.2818866 -4.3019423 -4.3204441 -4.3251333 -4.3222919 -4.3166304][-4.3130007 -4.2807021 -4.2195621 -4.1455464 -4.0950818 -4.0889254 -4.132102 -4.195816 -4.2462554 -4.2720051 -4.290216 -4.3110595 -4.3182487 -4.3161311 -4.308804][-4.31077 -4.2815166 -4.2248631 -4.1541982 -4.1038046 -4.0948076 -4.1295033 -4.1840453 -4.2297091 -4.2562346 -4.2797537 -4.3075609 -4.3195629 -4.31661 -4.3056221][-4.3095722 -4.2838416 -4.2315965 -4.161252 -4.1069846 -4.087337 -4.1106892 -4.1599422 -4.2098942 -4.2457366 -4.2801394 -4.314723 -4.3273349 -4.3212585 -4.3043289][-4.3124557 -4.2898846 -4.2397547 -4.167078 -4.103137 -4.0675149 -4.0765219 -4.1241612 -4.1830463 -4.2338557 -4.2813129 -4.3210483 -4.3330474 -4.3241982 -4.3010526]]...]
INFO - root - 2017-12-07 20:07:53.859491: step 48710, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 53h:31m:11s remains)
INFO - root - 2017-12-07 20:08:00.702988: step 48720, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.759 sec/batch; 59h:49m:28s remains)
INFO - root - 2017-12-07 20:08:07.512937: step 48730, loss = 2.09, batch loss = 2.04 (11.6 examples/sec; 0.687 sec/batch; 54h:10m:07s remains)
INFO - root - 2017-12-07 20:08:14.367855: step 48740, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 50h:39m:43s remains)
INFO - root - 2017-12-07 20:08:21.175021: step 48750, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 52h:00m:50s remains)
INFO - root - 2017-12-07 20:08:27.965013: step 48760, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 57h:17m:51s remains)
INFO - root - 2017-12-07 20:08:34.687332: step 48770, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 55h:08m:30s remains)
INFO - root - 2017-12-07 20:08:41.460269: step 48780, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 54h:35m:25s remains)
INFO - root - 2017-12-07 20:08:48.271055: step 48790, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 50h:45m:03s remains)
INFO - root - 2017-12-07 20:08:54.870712: step 48800, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 51h:16m:37s remains)
2017-12-07 20:08:55.698740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.252811 -4.2117 -4.1721468 -4.1313233 -4.0796447 -4.0406003 -4.0535693 -4.1199784 -4.1996241 -4.2582159 -4.2860374 -4.29654 -4.3000445 -4.2997589 -4.2983427][-4.244585 -4.2267427 -4.2208028 -4.2090831 -4.1731133 -4.1294017 -4.1125388 -4.140605 -4.1969833 -4.2481794 -4.2797961 -4.2960186 -4.3029509 -4.3032284 -4.3013554][-4.2325268 -4.2307296 -4.2464991 -4.25228 -4.2326527 -4.1975207 -4.1714978 -4.1736827 -4.2029357 -4.24087 -4.2730865 -4.2936978 -4.3040714 -4.3055172 -4.3035874][-4.2270217 -4.2373996 -4.2633567 -4.2769494 -4.2638745 -4.2324996 -4.2003465 -4.184258 -4.1934261 -4.2229242 -4.2577438 -4.2856493 -4.3020277 -4.3070498 -4.3067155][-4.2285233 -4.2465606 -4.2751813 -4.289217 -4.2744355 -4.2368994 -4.1950207 -4.16494 -4.1603904 -4.1874089 -4.2288384 -4.2673907 -4.2926345 -4.305315 -4.309536][-4.240716 -4.2594514 -4.2846594 -4.2900057 -4.2659354 -4.2154803 -4.1606855 -4.1179528 -4.1031117 -4.1312203 -4.183723 -4.236836 -4.2746119 -4.2975717 -4.308897][-4.2616405 -4.2781425 -4.2976508 -4.2934365 -4.2539034 -4.1862087 -4.1162367 -4.0581818 -4.0312672 -4.0598741 -4.1235642 -4.1943026 -4.2502337 -4.2854438 -4.3043394][-4.2818632 -4.2965412 -4.3142428 -4.3073783 -4.26479 -4.191577 -4.1116061 -4.031209 -3.9811232 -4.0000758 -4.0658612 -4.1451206 -4.216578 -4.266274 -4.2932558][-4.2958059 -4.3079014 -4.3211184 -4.315836 -4.28128 -4.2209797 -4.1523557 -4.0750742 -4.0115266 -3.9982064 -4.036571 -4.1034131 -4.1791406 -4.2376261 -4.2722473][-4.30435 -4.311348 -4.3156834 -4.3073549 -4.2828941 -4.2438664 -4.19851 -4.1478996 -4.1004887 -4.0691395 -4.0649695 -4.0970259 -4.1570554 -4.2135043 -4.2507343][-4.3065176 -4.30835 -4.3056726 -4.29145 -4.269979 -4.2443595 -4.219645 -4.1989212 -4.1807585 -4.157763 -4.1373539 -4.1387739 -4.169826 -4.2090011 -4.2403154][-4.3003478 -4.2995162 -4.2926593 -4.2735248 -4.2475443 -4.2257137 -4.2156782 -4.2202635 -4.23048 -4.2246346 -4.2067504 -4.1969013 -4.2073741 -4.2277112 -4.2466822][-4.2902694 -4.2868285 -4.2797732 -4.2613544 -4.2345653 -4.2122912 -4.2081413 -4.2267842 -4.2555075 -4.2645955 -4.2556477 -4.2460408 -4.24806 -4.2564569 -4.2640662][-4.2869492 -4.2816219 -4.2756376 -4.2606306 -4.2370205 -4.2153792 -4.2100263 -4.2322621 -4.2694888 -4.2889276 -4.2858477 -4.2763095 -4.2736349 -4.2763906 -4.2780881][-4.2900815 -4.2845087 -4.2798886 -4.2674103 -4.2484231 -4.2281227 -4.219358 -4.2382545 -4.2740355 -4.2960277 -4.297236 -4.2896838 -4.285614 -4.2861633 -4.286098]]...]
INFO - root - 2017-12-07 20:09:02.531146: step 48810, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 52h:16m:21s remains)
INFO - root - 2017-12-07 20:09:09.384222: step 48820, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 49h:16m:02s remains)
INFO - root - 2017-12-07 20:09:16.222307: step 48830, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 50h:06m:54s remains)
INFO - root - 2017-12-07 20:09:23.078763: step 48840, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 56h:45m:21s remains)
INFO - root - 2017-12-07 20:09:30.007522: step 48850, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 57h:04m:19s remains)
INFO - root - 2017-12-07 20:09:36.823681: step 48860, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 50h:41m:47s remains)
INFO - root - 2017-12-07 20:09:43.583852: step 48870, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 52h:17m:34s remains)
INFO - root - 2017-12-07 20:09:50.375056: step 48880, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 52h:15m:53s remains)
INFO - root - 2017-12-07 20:09:57.105691: step 48890, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 55h:19m:35s remains)
INFO - root - 2017-12-07 20:10:03.789202: step 48900, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 55h:36m:00s remains)
2017-12-07 20:10:04.521205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1609483 -4.1584959 -4.1835804 -4.2204413 -4.2500052 -4.2656784 -4.2676997 -4.2577224 -4.2358131 -4.2167916 -4.2062683 -4.2012739 -4.2090292 -4.2202611 -4.2153311][-4.1684904 -4.1678352 -4.190824 -4.222734 -4.2466531 -4.2607388 -4.2684283 -4.2691622 -4.2608457 -4.2506771 -4.2441535 -4.2397065 -4.239697 -4.234674 -4.211782][-4.1640115 -4.1682997 -4.1893473 -4.2140622 -4.2304835 -4.2397933 -4.2462773 -4.252811 -4.2582531 -4.2631583 -4.2669163 -4.2670326 -4.2619572 -4.2417259 -4.201725][-4.1720147 -4.1743827 -4.1836314 -4.1954784 -4.201726 -4.2025414 -4.202774 -4.2090549 -4.2253804 -4.2467208 -4.2625885 -4.26982 -4.2658014 -4.24277 -4.2001281][-4.1885347 -4.1814466 -4.1738744 -4.1684084 -4.1591606 -4.1439781 -4.1312432 -4.1333575 -4.1614685 -4.2023692 -4.2355523 -4.2562594 -4.2622652 -4.246428 -4.2103968][-4.2010055 -4.1884961 -4.1663756 -4.1410847 -4.1088071 -4.0698786 -4.0361538 -4.0293727 -4.0697041 -4.1325932 -4.1839418 -4.2192912 -4.2371869 -4.2317944 -4.2066531][-4.2139654 -4.202364 -4.1720605 -4.1300654 -4.0735879 -4.0081062 -3.9463363 -3.9243097 -3.9739783 -4.0570889 -4.1249976 -4.1713138 -4.1983 -4.202662 -4.1906314][-4.2279267 -4.2202792 -4.19069 -4.14429 -4.0771365 -3.9986904 -3.9171407 -3.8807757 -3.9311585 -4.0199633 -4.0937562 -4.144207 -4.1754427 -4.1874442 -4.1874661][-4.2468395 -4.240159 -4.2159882 -4.175354 -4.1143146 -4.0441952 -3.9705913 -3.9381275 -3.9789729 -4.0520897 -4.1127954 -4.1529903 -4.177876 -4.1914439 -4.1991034][-4.2585773 -4.2457318 -4.2215919 -4.1857309 -4.1363654 -4.084187 -4.0350218 -4.01924 -4.051435 -4.1034665 -4.1455884 -4.1731834 -4.1907263 -4.2046857 -4.217834][-4.2612967 -4.2402058 -4.212575 -4.1819153 -4.14671 -4.1164141 -4.0924029 -4.0937543 -4.1206946 -4.1548333 -4.180234 -4.1942205 -4.2038593 -4.2153449 -4.22875][-4.2535992 -4.2267609 -4.196938 -4.174242 -4.155354 -4.1457038 -4.1437922 -4.1581116 -4.1800852 -4.1974225 -4.2046013 -4.2034092 -4.2012753 -4.204946 -4.2152882][-4.243093 -4.2179255 -4.1910868 -4.1756382 -4.1705422 -4.1766548 -4.1879478 -4.20641 -4.2206035 -4.2234349 -4.2148757 -4.1984124 -4.1840916 -4.1803279 -4.1873884][-4.2344608 -4.2207427 -4.2049079 -4.1968775 -4.1999145 -4.2115884 -4.2224493 -4.2336307 -4.2372894 -4.229681 -4.2107968 -4.1862345 -4.1682839 -4.16206 -4.1667814][-4.2298679 -4.2285662 -4.2237277 -4.2233009 -4.2311435 -4.2423816 -4.24764 -4.2480969 -4.2411938 -4.2265735 -4.2043614 -4.1809745 -4.1660604 -4.1612978 -4.1660609]]...]
INFO - root - 2017-12-07 20:10:11.308531: step 48910, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 49h:46m:50s remains)
INFO - root - 2017-12-07 20:10:18.116696: step 48920, loss = 2.10, batch loss = 2.04 (10.8 examples/sec; 0.741 sec/batch; 58h:23m:53s remains)
INFO - root - 2017-12-07 20:10:24.845989: step 48930, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 52h:45m:04s remains)
INFO - root - 2017-12-07 20:10:31.598638: step 48940, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 50h:00m:48s remains)
INFO - root - 2017-12-07 20:10:38.376897: step 48950, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 49h:57m:21s remains)
INFO - root - 2017-12-07 20:10:45.246508: step 48960, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 56h:47m:59s remains)
INFO - root - 2017-12-07 20:10:52.155959: step 48970, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 56h:42m:30s remains)
INFO - root - 2017-12-07 20:10:58.958744: step 48980, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 53h:00m:05s remains)
INFO - root - 2017-12-07 20:11:05.868935: step 48990, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 51h:40m:13s remains)
INFO - root - 2017-12-07 20:11:12.434338: step 49000, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.627 sec/batch; 49h:21m:47s remains)
2017-12-07 20:11:13.269947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3043804 -4.2931376 -4.2727747 -4.2624369 -4.2515807 -4.2188382 -4.1804686 -4.1663771 -4.1720419 -4.1916018 -4.2215185 -4.2424989 -4.2432809 -4.2325592 -4.2149162][-4.2987194 -4.2899094 -4.2731314 -4.2670803 -4.2603526 -4.2337737 -4.2003489 -4.1842508 -4.1792631 -4.1886964 -4.2129793 -4.2296791 -4.2314482 -4.2200513 -4.1966195][-4.2963595 -4.2897773 -4.2772231 -4.272831 -4.2672253 -4.2414455 -4.2084889 -4.1896982 -4.1771126 -4.1779895 -4.1944394 -4.2060475 -4.2118325 -4.2029181 -4.1725903][-4.2946038 -4.2878265 -4.2744374 -4.2666903 -4.2582259 -4.2317386 -4.202786 -4.1865878 -4.1699123 -4.1660194 -4.1814046 -4.1959887 -4.2080483 -4.1978593 -4.1522479][-4.2876716 -4.2735095 -4.2539406 -4.2390184 -4.2222447 -4.1947269 -4.1813736 -4.1768265 -4.1568327 -4.1508856 -4.1700182 -4.1920042 -4.2095428 -4.1975646 -4.1378455][-4.277534 -4.2547359 -4.2277756 -4.2035122 -4.1685066 -4.1337585 -4.1384668 -4.1477194 -4.1285076 -4.1283388 -4.1598005 -4.1873593 -4.2082486 -4.1982813 -4.13814][-4.27111 -4.244833 -4.2130551 -4.1776614 -4.1199536 -4.0714006 -4.0808969 -4.0915422 -4.0699253 -4.0825949 -4.1340423 -4.1737366 -4.1994843 -4.1998062 -4.1542983][-4.2693419 -4.2466593 -4.217535 -4.1786819 -4.1085348 -4.0495853 -4.0508504 -4.0555081 -4.0294747 -4.0468287 -4.1080527 -4.1539373 -4.1822648 -4.1954346 -4.1721954][-4.2713151 -4.2568116 -4.2416153 -4.2177148 -4.1641293 -4.1133609 -4.1029782 -4.09433 -4.0658174 -4.0790215 -4.1252284 -4.1577344 -4.18002 -4.1992931 -4.1913223][-4.2762184 -4.2676315 -4.2650704 -4.2576375 -4.2247605 -4.1871595 -4.1752234 -4.1681824 -4.14604 -4.1544228 -4.1821656 -4.1970797 -4.208456 -4.2224832 -4.2192678][-4.2775135 -4.2716541 -4.2752032 -4.2788486 -4.2658658 -4.2394753 -4.2289438 -4.2294965 -4.2192597 -4.2236347 -4.23881 -4.2413745 -4.2411742 -4.244452 -4.2417121][-4.2733727 -4.267766 -4.2714105 -4.2787814 -4.2786703 -4.2654305 -4.2577333 -4.2579551 -4.2511697 -4.2513227 -4.2584023 -4.2540827 -4.24606 -4.2426043 -4.23962][-4.2698503 -4.2637911 -4.2650166 -4.2694244 -4.27045 -4.2606659 -4.253602 -4.2547426 -4.2500854 -4.2484355 -4.2515173 -4.24528 -4.2347302 -4.2301717 -4.22908][-4.27052 -4.2627268 -4.2597146 -4.2598014 -4.2570066 -4.2474217 -4.2461643 -4.2518363 -4.2469778 -4.2441206 -4.246542 -4.2410903 -4.2319889 -4.2289057 -4.2301011][-4.2751427 -4.2642317 -4.2565341 -4.2527981 -4.24854 -4.2417679 -4.24679 -4.2569456 -4.2536039 -4.250905 -4.2545085 -4.252902 -4.2485547 -4.2466655 -4.2456055]]...]
INFO - root - 2017-12-07 20:11:20.062685: step 49010, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 52h:55m:40s remains)
INFO - root - 2017-12-07 20:11:26.904379: step 49020, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 51h:27m:41s remains)
INFO - root - 2017-12-07 20:11:33.762886: step 49030, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.664 sec/batch; 52h:18m:57s remains)
INFO - root - 2017-12-07 20:11:40.630230: step 49040, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 56h:08m:38s remains)
INFO - root - 2017-12-07 20:11:47.467874: step 49050, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 55h:02m:11s remains)
INFO - root - 2017-12-07 20:11:54.288663: step 49060, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 52h:32m:52s remains)
INFO - root - 2017-12-07 20:12:01.117857: step 49070, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 51h:21m:44s remains)
INFO - root - 2017-12-07 20:12:08.043209: step 49080, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 53h:23m:04s remains)
INFO - root - 2017-12-07 20:12:14.860118: step 49090, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.756 sec/batch; 59h:32m:43s remains)
INFO - root - 2017-12-07 20:12:21.506263: step 49100, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 54h:52m:12s remains)
2017-12-07 20:12:22.307765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2676358 -4.2685494 -4.2690997 -4.2683587 -4.2664318 -4.263123 -4.2589602 -4.2573805 -4.248673 -4.2322392 -4.2200365 -4.2180181 -4.220386 -4.219666 -4.2169151][-4.260603 -4.2686057 -4.2730885 -4.2713103 -4.2649579 -4.257668 -4.2517023 -4.2502456 -4.2456417 -4.2335486 -4.2210951 -4.216393 -4.2198825 -4.2218394 -4.2215095][-4.2467794 -4.2597156 -4.2659364 -4.2615843 -4.249506 -4.2388639 -4.2339048 -4.2350631 -4.2365565 -4.2335234 -4.2270155 -4.2231097 -4.2252064 -4.2266803 -4.2255688][-4.2450032 -4.2560539 -4.2586861 -4.2498851 -4.2332525 -4.2225986 -4.2212553 -4.2259383 -4.2323728 -4.2373843 -4.2383809 -4.2369518 -4.2353234 -4.2313981 -4.2235169][-4.2519956 -4.2569542 -4.2533274 -4.2392111 -4.22012 -4.2101569 -4.2108107 -4.2154336 -4.2217941 -4.2283835 -4.2337451 -4.23624 -4.2334533 -4.2265377 -4.2136526][-4.24792 -4.2469549 -4.2391415 -4.223865 -4.2049556 -4.1907783 -4.1860409 -4.1852183 -4.187016 -4.1908922 -4.1997128 -4.2097244 -4.2123017 -4.2092867 -4.1988764][-4.2326145 -4.2257862 -4.2147465 -4.19941 -4.1798835 -4.1597319 -4.1482153 -4.1449127 -4.1481595 -4.1563234 -4.1732163 -4.1934237 -4.2025294 -4.2022138 -4.19394][-4.2183867 -4.2029772 -4.1863017 -4.1714892 -4.1554494 -4.1365018 -4.1256547 -4.1283007 -4.1407781 -4.1583042 -4.1822248 -4.2059155 -4.2152567 -4.2123075 -4.2024732][-4.2218451 -4.2044177 -4.188211 -4.1782808 -4.1701031 -4.1570983 -4.1490903 -4.1527534 -4.1662946 -4.1840205 -4.2044377 -4.2218084 -4.2260551 -4.2189941 -4.208456][-4.24408 -4.2345977 -4.2252245 -4.2210274 -4.2176971 -4.2070265 -4.1967168 -4.1943579 -4.2017169 -4.2132335 -4.223834 -4.2295566 -4.2255025 -4.2144322 -4.2044058][-4.2624598 -4.2629566 -4.2606111 -4.2596 -4.2565942 -4.2455893 -4.2320747 -4.2257948 -4.2298441 -4.2378078 -4.2411504 -4.2370763 -4.2249923 -4.2101741 -4.1998572][-4.2728119 -4.27919 -4.2797785 -4.278914 -4.2765379 -4.2669487 -4.2538581 -4.2476816 -4.2506733 -4.2567334 -4.2559681 -4.2462106 -4.2301207 -4.2142248 -4.2035179][-4.2744684 -4.2830868 -4.285388 -4.2850676 -4.2839332 -4.278791 -4.269805 -4.2659578 -4.2688246 -4.272471 -4.2686052 -4.256701 -4.2411842 -4.2278624 -4.218925][-4.2612076 -4.27077 -4.2756219 -4.2762671 -4.2775927 -4.278265 -4.2751608 -4.2745562 -4.2778258 -4.2801471 -4.2758155 -4.2651925 -4.2536716 -4.24589 -4.241888][-4.2384415 -4.2475982 -4.2541661 -4.2560968 -4.2599392 -4.26734 -4.2706451 -4.2739277 -4.2791371 -4.2829709 -4.28043 -4.2728958 -4.2658715 -4.2629991 -4.2633166]]...]
INFO - root - 2017-12-07 20:12:29.033165: step 49110, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 56h:56m:41s remains)
INFO - root - 2017-12-07 20:12:35.910515: step 49120, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 57h:33m:38s remains)
INFO - root - 2017-12-07 20:12:42.684614: step 49130, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 54h:32m:40s remains)
INFO - root - 2017-12-07 20:12:49.495157: step 49140, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 51h:30m:25s remains)
INFO - root - 2017-12-07 20:12:56.402021: step 49150, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:27m:46s remains)
INFO - root - 2017-12-07 20:13:03.262860: step 49160, loss = 2.03, batch loss = 1.98 (11.2 examples/sec; 0.713 sec/batch; 56h:06m:32s remains)
INFO - root - 2017-12-07 20:13:10.068597: step 49170, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 54h:05m:19s remains)
INFO - root - 2017-12-07 20:13:16.907066: step 49180, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 48h:55m:48s remains)
INFO - root - 2017-12-07 20:13:23.653876: step 49190, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.640 sec/batch; 50h:21m:47s remains)
INFO - root - 2017-12-07 20:13:30.245485: step 49200, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.699 sec/batch; 54h:59m:14s remains)
2017-12-07 20:13:30.951579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.300456 -4.3047562 -4.2999063 -4.2873845 -4.2690439 -4.2530971 -4.2519817 -4.2636447 -4.275095 -4.2827449 -4.2856832 -4.2857 -4.2857666 -4.28579 -4.2857347][-4.319077 -4.3200502 -4.3082509 -4.285964 -4.2562437 -4.2320561 -4.2309394 -4.2511063 -4.2719193 -4.286469 -4.2953763 -4.3005037 -4.3037138 -4.306592 -4.3070364][-4.3291059 -4.3238063 -4.3016324 -4.2651873 -4.2188668 -4.1804786 -4.1738038 -4.2014394 -4.2360744 -4.2618232 -4.2802634 -4.2954197 -4.3060665 -4.3157883 -4.32007][-4.3268728 -4.3129282 -4.2787976 -4.2272286 -4.162848 -4.1081843 -4.0889635 -4.1179237 -4.164032 -4.2020617 -4.2330532 -4.265512 -4.2917185 -4.3116322 -4.3223224][-4.3206549 -4.2997055 -4.258801 -4.1958537 -4.1153426 -4.0429411 -4.0045438 -4.0282187 -4.0839481 -4.13097 -4.1717587 -4.2226191 -4.2696056 -4.303256 -4.3223009][-4.3149924 -4.2908273 -4.248127 -4.1779833 -4.0838442 -3.9893832 -3.9171195 -3.9234056 -3.9890866 -4.046648 -4.0999346 -4.1722507 -4.2435164 -4.2937684 -4.3218694][-4.308147 -4.2842736 -4.2444119 -4.1728415 -4.070435 -3.9531457 -3.8383079 -3.8178477 -3.8947887 -3.9734569 -4.0497518 -4.1439404 -4.231741 -4.2900476 -4.3215609][-4.3006558 -4.2780962 -4.2417121 -4.1708813 -4.0612717 -3.9253459 -3.7778707 -3.7352636 -3.827785 -3.9375978 -4.0358486 -4.1438 -4.2381649 -4.2955074 -4.3229156][-4.2989697 -4.2793117 -4.24835 -4.1863856 -4.0848341 -3.9609661 -3.8305202 -3.7854781 -3.8666313 -3.9778628 -4.0768819 -4.1785655 -4.2630868 -4.3100567 -4.32719][-4.309886 -4.2939382 -4.26537 -4.2118316 -4.1249008 -4.0291476 -3.9386864 -3.9033141 -3.9655926 -4.0654159 -4.1535978 -4.2348075 -4.2952461 -4.3241348 -4.3312278][-4.3246393 -4.3118248 -4.28412 -4.2366252 -4.1652503 -4.0968289 -4.0437055 -4.0224133 -4.0715756 -4.1554627 -4.2279816 -4.2848444 -4.319634 -4.3312287 -4.33196][-4.3308377 -4.317142 -4.2900119 -4.2507591 -4.1997018 -4.1591535 -4.1376252 -4.1326132 -4.1687331 -4.2327046 -4.2876959 -4.3226242 -4.3360391 -4.3348265 -4.33128][-4.3259482 -4.3090358 -4.2810626 -4.2464914 -4.2108274 -4.1945577 -4.1933918 -4.1972785 -4.2257881 -4.2750497 -4.317863 -4.3406591 -4.3428288 -4.3350387 -4.3298922][-4.31066 -4.2837248 -4.2476869 -4.2117577 -4.1822014 -4.18258 -4.2012925 -4.2169337 -4.2454519 -4.2860022 -4.3209968 -4.3402047 -4.3419175 -4.3343239 -4.3291163][-4.2883377 -4.2467375 -4.1957555 -4.1495256 -4.1163974 -4.1241112 -4.160327 -4.1912537 -4.225244 -4.2690606 -4.3069968 -4.330224 -4.3378525 -4.3334837 -4.3291979]]...]
INFO - root - 2017-12-07 20:13:37.705302: step 49210, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 49h:51m:42s remains)
INFO - root - 2017-12-07 20:13:44.468328: step 49220, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 50h:49m:57s remains)
INFO - root - 2017-12-07 20:13:51.379524: step 49230, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 57h:03m:35s remains)
INFO - root - 2017-12-07 20:13:58.058119: step 49240, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 54h:50m:18s remains)
INFO - root - 2017-12-07 20:14:04.799899: step 49250, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.664 sec/batch; 52h:12m:54s remains)
INFO - root - 2017-12-07 20:14:11.541159: step 49260, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 51h:20m:14s remains)
INFO - root - 2017-12-07 20:14:18.311506: step 49270, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 52h:35m:20s remains)
INFO - root - 2017-12-07 20:14:25.180464: step 49280, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 56h:45m:53s remains)
INFO - root - 2017-12-07 20:14:32.023376: step 49290, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 55h:36m:49s remains)
INFO - root - 2017-12-07 20:14:38.748764: step 49300, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 51h:14m:33s remains)
2017-12-07 20:14:39.415961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1942854 -4.1711535 -4.1577559 -4.1638417 -4.2045784 -4.2519441 -4.2717786 -4.27288 -4.2703671 -4.2665462 -4.2636628 -4.2462015 -4.2091703 -4.1598 -4.0914612][-4.1893454 -4.1730022 -4.1697845 -4.1843996 -4.2195973 -4.2489195 -4.2531633 -4.2440772 -4.2351704 -4.2334523 -4.2387495 -4.2278113 -4.190371 -4.1298246 -4.0372543][-4.1854811 -4.1724615 -4.1758 -4.1954403 -4.2227592 -4.2328568 -4.2188325 -4.1951666 -4.176518 -4.1815286 -4.2064767 -4.2142234 -4.1861029 -4.1260886 -4.029952][-4.197484 -4.1824708 -4.1853342 -4.1977148 -4.2052855 -4.1927714 -4.1593275 -4.1241169 -4.1027818 -4.1220741 -4.1760459 -4.2096314 -4.1981049 -4.151062 -4.0779][-4.22557 -4.2082596 -4.2027864 -4.1961412 -4.1748581 -4.1345224 -4.0774336 -4.0293808 -4.008853 -4.0513654 -4.1401329 -4.2053394 -4.2166986 -4.1920395 -4.1509261][-4.25944 -4.2426834 -4.227951 -4.2013936 -4.155272 -4.0896449 -4.009809 -3.9401927 -3.9116352 -3.969986 -4.0842452 -4.1778955 -4.2176008 -4.2202344 -4.2104292][-4.2881861 -4.2774639 -4.2635 -4.2316661 -4.1788206 -4.1050096 -4.0102229 -3.9165571 -3.8636069 -3.9093192 -4.0194426 -4.1218433 -4.1835155 -4.2171459 -4.2370043][-4.3026648 -4.3014464 -4.2975116 -4.2760887 -4.2326274 -4.1660929 -4.0716963 -3.9700239 -3.8947845 -3.9058747 -3.981143 -4.0681739 -4.1399841 -4.1975007 -4.2368531][-4.3015842 -4.3051252 -4.30933 -4.3011184 -4.2731152 -4.2221718 -4.1411729 -4.0481143 -3.9700267 -3.9514866 -3.9823542 -4.0428524 -4.1168571 -4.1851535 -4.23115][-4.2865815 -4.2889733 -4.2961216 -4.296299 -4.2800503 -4.2421389 -4.1776648 -4.0997081 -4.03181 -4.0072122 -4.0168695 -4.0590668 -4.1265407 -4.1888118 -4.2281718][-4.2717652 -4.2700639 -4.2751093 -4.2749329 -4.263166 -4.2320652 -4.1806679 -4.1213942 -4.076488 -4.0675483 -4.0781574 -4.11257 -4.1684504 -4.2171988 -4.2417226][-4.2627697 -4.25727 -4.2577562 -4.2539024 -4.2423573 -4.2163181 -4.1801028 -4.1429734 -4.1226649 -4.1282258 -4.1444545 -4.1780286 -4.2240348 -4.25883 -4.269362][-4.264338 -4.2544541 -4.2503862 -4.2440124 -4.2337232 -4.2133822 -4.192337 -4.1760859 -4.171227 -4.181828 -4.1993151 -4.232152 -4.2693486 -4.2910194 -4.2914252][-4.2724261 -4.2610664 -4.2562037 -4.25236 -4.2471218 -4.2347169 -4.2248335 -4.2198224 -4.219079 -4.226182 -4.2407064 -4.2684355 -4.2983084 -4.312211 -4.3065467][-4.2751689 -4.2658997 -4.2639222 -4.2646842 -4.2656884 -4.2612824 -4.2597771 -4.2611613 -4.2612324 -4.2655826 -4.2762613 -4.2952123 -4.3148327 -4.3219 -4.3116307]]...]
INFO - root - 2017-12-07 20:14:46.250585: step 49310, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 54h:28m:18s remains)
INFO - root - 2017-12-07 20:14:53.041182: step 49320, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 52h:13m:06s remains)
INFO - root - 2017-12-07 20:14:59.846844: step 49330, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 50h:08m:17s remains)
INFO - root - 2017-12-07 20:15:06.699437: step 49340, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 51h:31m:41s remains)
INFO - root - 2017-12-07 20:15:13.601434: step 49350, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 54h:39m:21s remains)
INFO - root - 2017-12-07 20:15:20.490902: step 49360, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 57h:24m:34s remains)
INFO - root - 2017-12-07 20:15:27.290519: step 49370, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 52h:13m:00s remains)
INFO - root - 2017-12-07 20:15:34.113817: step 49380, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.624 sec/batch; 49h:05m:50s remains)
INFO - root - 2017-12-07 20:15:40.982558: step 49390, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 53h:11m:55s remains)
INFO - root - 2017-12-07 20:15:47.645759: step 49400, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 53h:43m:59s remains)
2017-12-07 20:15:48.438290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3558359 -4.3485546 -4.3381958 -4.3295422 -4.3242493 -4.3203321 -4.3185205 -4.320044 -4.3229327 -4.3241353 -4.3248973 -4.3271418 -4.3305173 -4.3342276 -4.3397217][-4.3594513 -4.3511157 -4.3365397 -4.32304 -4.3138685 -4.3050246 -4.2998948 -4.302031 -4.3086691 -4.3134432 -4.3166804 -4.3184991 -4.3216314 -4.3261819 -4.3333149][-4.3625612 -4.3511004 -4.3310475 -4.3108969 -4.2938271 -4.2731614 -4.2585526 -4.2604551 -4.2725763 -4.2830071 -4.2913651 -4.2964883 -4.3047895 -4.3133802 -4.3233232][-4.3554821 -4.3380618 -4.3099427 -4.2799044 -4.2516513 -4.2145653 -4.1857905 -4.1889658 -4.2119918 -4.2289629 -4.2438068 -4.2566757 -4.2745037 -4.289927 -4.3048539][-4.3326283 -4.3054228 -4.2686687 -4.2293653 -4.1853909 -4.1222343 -4.0695357 -4.073998 -4.1160007 -4.1457987 -4.1688914 -4.1924706 -4.2230177 -4.2500625 -4.2754569][-4.2967076 -4.2544394 -4.2060184 -4.1526265 -4.0851231 -3.9819477 -3.8908706 -3.90015 -3.9822874 -4.0446696 -4.0860267 -4.1197467 -4.1608825 -4.2016993 -4.240911][-4.2688236 -4.2110519 -4.1508088 -4.0859156 -3.9990275 -3.8573513 -3.722666 -3.7387738 -3.8710589 -3.9770546 -4.0400143 -4.0802879 -4.1276197 -4.1769495 -4.2234917][-4.2595072 -4.1981759 -4.1358852 -4.0728083 -3.9905479 -3.8573818 -3.7300792 -3.7422256 -3.8784482 -3.9961221 -4.06638 -4.1064272 -4.1490955 -4.1935015 -4.2348495][-4.2666783 -4.2180543 -4.1681294 -4.1230879 -4.0688996 -3.9788952 -3.8940711 -3.8964522 -3.9912381 -4.0816455 -4.1419148 -4.1786308 -4.2119241 -4.2431216 -4.2675805][-4.2828741 -4.2538571 -4.2243857 -4.1957054 -4.16767 -4.1185694 -4.0687428 -4.0648804 -4.1215463 -4.18092 -4.2255945 -4.2521305 -4.2733841 -4.2886972 -4.2984834][-4.3001037 -4.2854648 -4.2721128 -4.2577791 -4.2459126 -4.2200589 -4.1941857 -4.1942763 -4.22807 -4.2636056 -4.2931104 -4.3064251 -4.3119435 -4.313961 -4.3144484][-4.3106961 -4.2997727 -4.294395 -4.2896237 -4.2862959 -4.2757025 -4.2644672 -4.2664685 -4.2843728 -4.3026633 -4.3202553 -4.32595 -4.3245244 -4.3225265 -4.3210258][-4.3164778 -4.305748 -4.30173 -4.301661 -4.3044572 -4.3028626 -4.2979431 -4.2997475 -4.3077135 -4.3156743 -4.3263011 -4.3296542 -4.3266549 -4.3252444 -4.3268347][-4.3257947 -4.3152356 -4.3115587 -4.3133254 -4.3179688 -4.3200293 -4.3178334 -4.3177032 -4.3208222 -4.3241372 -4.331501 -4.3345671 -4.3321486 -4.3321061 -4.3363371][-4.3404 -4.3332772 -4.331131 -4.3324623 -4.3348169 -4.3363833 -4.3355808 -4.3341074 -4.3339543 -4.3351502 -4.3401122 -4.3436093 -4.3433604 -4.3441119 -4.3479867]]...]
INFO - root - 2017-12-07 20:15:55.155481: step 49410, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 52h:34m:25s remains)
INFO - root - 2017-12-07 20:16:01.949430: step 49420, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 56h:25m:14s remains)
INFO - root - 2017-12-07 20:16:08.706505: step 49430, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 54h:02m:04s remains)
INFO - root - 2017-12-07 20:16:15.487132: step 49440, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 50h:24m:03s remains)
INFO - root - 2017-12-07 20:16:22.239284: step 49450, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 49h:32m:52s remains)
INFO - root - 2017-12-07 20:16:29.107220: step 49460, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.713 sec/batch; 56h:03m:05s remains)
INFO - root - 2017-12-07 20:16:35.980731: step 49470, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 57h:41m:54s remains)
INFO - root - 2017-12-07 20:16:42.776231: step 49480, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 49h:55m:49s remains)
INFO - root - 2017-12-07 20:16:49.541249: step 49490, loss = 2.08, batch loss = 2.02 (13.6 examples/sec; 0.587 sec/batch; 46h:09m:25s remains)
INFO - root - 2017-12-07 20:16:56.141588: step 49500, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 50h:27m:12s remains)
2017-12-07 20:16:56.852763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9617498 -3.9102278 -3.9000642 -3.9464707 -4.0429974 -4.1317897 -4.2013006 -4.254683 -4.2932153 -4.3172774 -4.3340816 -4.3419514 -4.3440142 -4.3450656 -4.34585][-4.079607 -4.0428915 -4.0293941 -4.0533042 -4.1158886 -4.1707411 -4.2117844 -4.2485991 -4.2823944 -4.3085852 -4.3284059 -4.3390265 -4.3416119 -4.341661 -4.3424778][-4.1987796 -4.1721196 -4.1528044 -4.1507368 -4.177618 -4.2003746 -4.2152977 -4.2367339 -4.263937 -4.2916136 -4.3168092 -4.3345089 -4.3409214 -4.3414598 -4.3417468][-4.2888775 -4.2680626 -4.2462296 -4.2248983 -4.2181377 -4.2069578 -4.198669 -4.2063317 -4.2292414 -4.25915 -4.2908096 -4.317863 -4.3334036 -4.3387656 -4.3405213][-4.3398008 -4.3262277 -4.3046489 -4.2696381 -4.2342086 -4.1918368 -4.1577983 -4.1500807 -4.1699491 -4.2048688 -4.2463536 -4.2831354 -4.3101554 -4.3245244 -4.3316207][-4.3522735 -4.3458209 -4.3253789 -4.2813926 -4.2240767 -4.1518135 -4.0895247 -4.0648828 -4.0832081 -4.1311884 -4.1903391 -4.2423511 -4.2815156 -4.30435 -4.3177042][-4.3465986 -4.3461595 -4.3224721 -4.2642417 -4.180512 -4.0771427 -3.983716 -3.9445231 -3.9638884 -4.0312986 -4.1195974 -4.1942139 -4.2517037 -4.2873583 -4.3096051][-4.3388257 -4.3466954 -4.3212328 -4.2468543 -4.1376042 -4.0067911 -3.89467 -3.8412607 -3.8569269 -3.9403186 -4.0566678 -4.1520619 -4.2232661 -4.2700047 -4.30057][-4.3249021 -4.3414035 -4.3231931 -4.253427 -4.1496758 -4.0280833 -3.9299271 -3.8790717 -3.8823903 -3.953958 -4.0612698 -4.146368 -4.208322 -4.2534676 -4.2874918][-4.3068318 -4.3254633 -4.3213105 -4.2803488 -4.2122936 -4.1290193 -4.0607772 -4.0196962 -4.0085473 -4.0466466 -4.1144361 -4.1671267 -4.2052159 -4.2401118 -4.2715893][-4.2859898 -4.3055048 -4.3176293 -4.3075776 -4.2780714 -4.2336235 -4.1918926 -4.1578989 -4.1327682 -4.1366005 -4.1645761 -4.1829958 -4.195529 -4.2181025 -4.2444868][-4.2557716 -4.2760897 -4.303926 -4.3213773 -4.3227873 -4.3047132 -4.2796836 -4.25369 -4.2202973 -4.1946077 -4.1866717 -4.1731625 -4.1652842 -4.1777649 -4.2018151][-4.1963668 -4.2228045 -4.2680368 -4.3090668 -4.33283 -4.3346095 -4.3216152 -4.3034439 -4.2700448 -4.2244821 -4.1818175 -4.1360254 -4.1092863 -4.1143661 -4.138236][-4.0925517 -4.1300244 -4.2011909 -4.2679243 -4.3136048 -4.3346586 -4.3363204 -4.3275008 -4.2935658 -4.230793 -4.1535997 -4.0697956 -4.0155559 -4.0103593 -4.0373521][-3.9537487 -4.00067 -4.1026573 -4.2053471 -4.2791457 -4.3184876 -4.3360109 -4.334034 -4.2987714 -4.2204094 -4.110086 -3.9893007 -3.907413 -3.8889682 -3.9175498]]...]
INFO - root - 2017-12-07 20:17:03.620618: step 49510, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 51h:24m:12s remains)
INFO - root - 2017-12-07 20:17:10.440655: step 49520, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 53h:09m:00s remains)
INFO - root - 2017-12-07 20:17:17.138210: step 49530, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.615 sec/batch; 48h:21m:38s remains)
INFO - root - 2017-12-07 20:17:23.931257: step 49540, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 52h:54m:37s remains)
INFO - root - 2017-12-07 20:17:30.607976: step 49550, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 56h:26m:01s remains)
INFO - root - 2017-12-07 20:17:37.275791: step 49560, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 51h:18m:03s remains)
INFO - root - 2017-12-07 20:17:44.036215: step 49570, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 50h:18m:03s remains)
INFO - root - 2017-12-07 20:17:50.889617: step 49580, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 52h:00m:03s remains)
INFO - root - 2017-12-07 20:17:57.826999: step 49590, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 57h:04m:37s remains)
INFO - root - 2017-12-07 20:18:04.505282: step 49600, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 55h:21m:20s remains)
2017-12-07 20:18:05.300904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2680478 -4.2588687 -4.2504654 -4.2460003 -4.2368231 -4.2291102 -4.2299924 -4.2323093 -4.2242441 -4.206533 -4.18843 -4.1706672 -4.1529355 -4.1485004 -4.1749711][-4.2471614 -4.2352238 -4.2333088 -4.2378922 -4.2300344 -4.214541 -4.2050781 -4.2025661 -4.1999969 -4.18514 -4.1685181 -4.1506662 -4.1339192 -4.1298127 -4.1625509][-4.2124982 -4.2021446 -4.2066917 -4.218277 -4.2161365 -4.1933632 -4.1705194 -4.1643367 -4.168715 -4.1645737 -4.1544719 -4.1345706 -4.1205187 -4.1205511 -4.1583781][-4.1862144 -4.1872697 -4.1933541 -4.2074609 -4.2065182 -4.1721506 -4.1311531 -4.1154881 -4.1176038 -4.12573 -4.1333513 -4.1249208 -4.1209311 -4.1256413 -4.1609454][-4.1716447 -4.1868916 -4.1933289 -4.20166 -4.1913667 -4.1366439 -4.0712466 -4.031703 -4.0270853 -4.0533128 -4.0920143 -4.1109586 -4.1219873 -4.1328912 -4.16115][-4.1807151 -4.2039475 -4.2078171 -4.202292 -4.1708794 -4.0909853 -3.9949744 -3.9222457 -3.9123592 -3.9723608 -4.0528212 -4.0972033 -4.1183362 -4.1343822 -4.1574788][-4.2040014 -4.2254786 -4.2265682 -4.2096395 -4.1612396 -4.0630078 -3.9461668 -3.8486078 -3.8429499 -3.9337051 -4.042099 -4.1029329 -4.1282716 -4.1449151 -4.1596537][-4.2200537 -4.2344627 -4.2331076 -4.2099977 -4.1580129 -4.0616765 -3.9543056 -3.8678336 -3.8769753 -3.9727392 -4.0748339 -4.1295257 -4.1497154 -4.1622028 -4.168529][-4.2240615 -4.2368197 -4.2389565 -4.2160635 -4.1609588 -4.0728993 -3.9931631 -3.9437294 -3.9711154 -4.0525064 -4.1270418 -4.160851 -4.1719337 -4.1824336 -4.188242][-4.2308021 -4.2434974 -4.2484117 -4.2176852 -4.1503272 -4.0668283 -4.0122495 -4.0049295 -4.052319 -4.1200094 -4.1686931 -4.185009 -4.1927176 -4.205812 -4.2167521][-4.2399583 -4.25101 -4.2507529 -4.2062678 -4.1243896 -4.0424595 -4.0083981 -4.0362062 -4.1014223 -4.1615167 -4.1950388 -4.2036934 -4.2112169 -4.2278686 -4.2438455][-4.2388859 -4.2431293 -4.2317486 -4.1749525 -4.0875068 -4.0128379 -3.9952478 -4.0486345 -4.1289988 -4.1853223 -4.2109952 -4.2173715 -4.2264919 -4.24463 -4.2617712][-4.2358956 -4.2291574 -4.2047596 -4.1376586 -4.048439 -3.9839079 -3.9807098 -4.0514708 -4.14088 -4.1947985 -4.2175465 -4.22421 -4.2333241 -4.2499251 -4.2661319][-4.2352548 -4.2194757 -4.1812673 -4.1045823 -4.020647 -3.9744108 -3.9934213 -4.0713787 -4.1551208 -4.20404 -4.2244506 -4.2318277 -4.2413793 -4.2553425 -4.2682328][-4.2330427 -4.214879 -4.1708064 -4.0933104 -4.022675 -3.9992561 -4.0386033 -4.1126614 -4.1808381 -4.2206688 -4.2361336 -4.2414937 -4.2503767 -4.2625146 -4.2711029]]...]
INFO - root - 2017-12-07 20:18:12.025271: step 49610, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 54h:14m:34s remains)
INFO - root - 2017-12-07 20:18:18.781485: step 49620, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 55h:58m:28s remains)
INFO - root - 2017-12-07 20:18:25.482219: step 49630, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 53h:26m:21s remains)
INFO - root - 2017-12-07 20:18:32.216732: step 49640, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 52h:06m:26s remains)
INFO - root - 2017-12-07 20:18:39.017475: step 49650, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 50h:53m:44s remains)
INFO - root - 2017-12-07 20:18:45.842844: step 49660, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 52h:38m:58s remains)
INFO - root - 2017-12-07 20:18:52.650074: step 49670, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 57h:31m:26s remains)
INFO - root - 2017-12-07 20:18:59.326645: step 49680, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 52h:17m:45s remains)
INFO - root - 2017-12-07 20:19:06.131313: step 49690, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 52h:53m:32s remains)
INFO - root - 2017-12-07 20:19:12.473670: step 49700, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 50h:19m:39s remains)
2017-12-07 20:19:13.163749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1483555 -4.1695919 -4.189692 -4.2013621 -4.2120185 -4.2240934 -4.2324924 -4.2361536 -4.2377648 -4.239079 -4.2370954 -4.2292356 -4.2149549 -4.1897473 -4.1622148][-4.18413 -4.198884 -4.2056608 -4.2012234 -4.1986694 -4.2036753 -4.2076817 -4.2081871 -4.2081671 -4.2101889 -4.2090969 -4.2012196 -4.1894526 -4.1691618 -4.141396][-4.2057977 -4.2146635 -4.2131863 -4.1981344 -4.1842742 -4.179822 -4.1744556 -4.1696038 -4.1675305 -4.169354 -4.1692538 -4.1636014 -4.161972 -4.1576362 -4.1375284][-4.208868 -4.2125273 -4.2082176 -4.1916804 -4.1710606 -4.1533117 -4.1359282 -4.1235471 -4.1237812 -4.1367006 -4.1448946 -4.1437831 -4.1507277 -4.1589231 -4.1456103][-4.2025161 -4.203011 -4.1967382 -4.1783705 -4.1500916 -4.1165895 -4.0839405 -4.0614843 -4.0710149 -4.1038818 -4.1320548 -4.1414876 -4.1487355 -4.1522975 -4.1323862][-4.1906848 -4.1909733 -4.1853042 -4.1605897 -4.1141481 -4.058866 -4.0041 -3.9668262 -3.9897475 -4.0490875 -4.1020393 -4.1267977 -4.1358643 -4.1336904 -4.10894][-4.1747088 -4.1842589 -4.1818018 -4.1481695 -4.0765061 -3.9836407 -3.884263 -3.8092384 -3.8512778 -3.9552367 -4.0452938 -4.0996284 -4.127913 -4.1363316 -4.1186342][-4.1765261 -4.1983371 -4.2021642 -4.1680422 -4.0874639 -3.9715664 -3.8332794 -3.715214 -3.7691722 -3.9083862 -4.0284324 -4.1049771 -4.1480308 -4.1628723 -4.1483912][-4.1931233 -4.2242169 -4.2363334 -4.2128539 -4.1503348 -4.0572705 -3.9471521 -3.8562913 -3.8875213 -3.9847455 -4.0761833 -4.1368785 -4.1708412 -4.1819091 -4.1680222][-4.2104578 -4.2385306 -4.2536268 -4.2442322 -4.2100124 -4.1559014 -4.0953827 -4.0506263 -4.0594554 -4.0976768 -4.1385765 -4.1637182 -4.1803136 -4.1871562 -4.18076][-4.2319155 -4.2447047 -4.255342 -4.2530808 -4.2390442 -4.2157159 -4.188808 -4.1704292 -4.1723032 -4.1839442 -4.1954632 -4.2000475 -4.2030339 -4.2050633 -4.2052183][-4.2568231 -4.255547 -4.2585316 -4.2563334 -4.2514262 -4.2445936 -4.2379956 -4.2348313 -4.2396035 -4.24824 -4.2529473 -4.2496228 -4.2411017 -4.2341061 -4.2328959][-4.2811294 -4.2715173 -4.2679372 -4.2618284 -4.2577815 -4.2568164 -4.2587438 -4.2650361 -4.2754054 -4.2877121 -4.2912111 -4.284112 -4.2657566 -4.2470536 -4.2361288][-4.2954698 -4.28592 -4.2775917 -4.2675409 -4.2613649 -4.2624636 -4.2669363 -4.2768984 -4.290657 -4.3022375 -4.3048682 -4.2953486 -4.2695727 -4.242486 -4.2248139][-4.3046651 -4.2989774 -4.2948651 -4.2868786 -4.2799783 -4.2798381 -4.2831526 -4.2908125 -4.3002191 -4.3071809 -4.3084173 -4.2991247 -4.2755961 -4.2485523 -4.23001]]...]
INFO - root - 2017-12-07 20:19:19.914539: step 49710, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 57h:08m:38s remains)
INFO - root - 2017-12-07 20:19:26.660436: step 49720, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 52h:58m:22s remains)
INFO - root - 2017-12-07 20:19:33.406164: step 49730, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 49h:54m:41s remains)
INFO - root - 2017-12-07 20:19:40.273169: step 49740, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 51h:22m:11s remains)
INFO - root - 2017-12-07 20:19:47.105161: step 49750, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 57h:08m:22s remains)
INFO - root - 2017-12-07 20:19:53.984817: step 49760, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 58h:14m:42s remains)
INFO - root - 2017-12-07 20:20:00.793691: step 49770, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 52h:51m:13s remains)
INFO - root - 2017-12-07 20:20:07.561047: step 49780, loss = 2.04, batch loss = 1.99 (13.0 examples/sec; 0.616 sec/batch; 48h:24m:03s remains)
INFO - root - 2017-12-07 20:20:14.454710: step 49790, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 53h:05m:39s remains)
INFO - root - 2017-12-07 20:20:20.985084: step 49800, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 56h:38m:19s remains)
2017-12-07 20:20:21.787524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2974882 -4.3065248 -4.3171277 -4.3245692 -4.327281 -4.3251529 -4.3148346 -4.3004436 -4.2881627 -4.2836475 -4.2895 -4.2991881 -4.3084006 -4.3129935 -4.3119731][-4.3117805 -4.3202806 -4.3275309 -4.3286409 -4.322525 -4.3091779 -4.2890897 -4.2696548 -4.2552166 -4.2512593 -4.2636271 -4.2822561 -4.3010349 -4.3139248 -4.3188796][-4.3060126 -4.3126879 -4.315433 -4.3095193 -4.2922721 -4.2645612 -4.2308054 -4.2018456 -4.1813712 -4.1764894 -4.1974549 -4.2308841 -4.2650342 -4.2898512 -4.3033814][-4.2757578 -4.2814994 -4.2809515 -4.2697797 -4.2435012 -4.2042789 -4.1592827 -4.1204796 -4.0921192 -4.0835447 -4.11082 -4.1565866 -4.2021456 -4.2355642 -4.2560945][-4.2371397 -4.2417521 -4.2399726 -4.2258897 -4.194859 -4.1495762 -4.0989881 -4.0559273 -4.0253029 -4.0161119 -4.0457835 -4.0914516 -4.1348939 -4.1669693 -4.1892424][-4.2033091 -4.2088923 -4.2088938 -4.1954679 -4.1657276 -4.1228075 -4.0759215 -4.03725 -4.0131631 -4.0103354 -4.0350952 -4.0662093 -4.0942531 -4.1135244 -4.1274824][-4.1885114 -4.1968994 -4.2002568 -4.1923213 -4.1708345 -4.1390457 -4.104116 -4.0751324 -4.0606518 -4.06365 -4.0790672 -4.0930233 -4.1022921 -4.10372 -4.1016135][-4.2030339 -4.2107782 -4.2148433 -4.2116489 -4.2004933 -4.1823106 -4.1624255 -4.1450043 -4.1373096 -4.1405482 -4.1463103 -4.1487184 -4.1458564 -4.1348977 -4.1201043][-4.2273908 -4.2306323 -4.2322164 -4.230515 -4.2264805 -4.2198067 -4.2116394 -4.2021785 -4.1977119 -4.1990895 -4.1989594 -4.1954546 -4.1873512 -4.1721463 -4.1539564][-4.2448969 -4.2434244 -4.2422323 -4.24048 -4.2408895 -4.2414322 -4.2394 -4.2345071 -4.2325983 -4.2349305 -4.2348146 -4.2299089 -4.2203078 -4.2055721 -4.189261][-4.2598934 -4.2547927 -4.251184 -4.2478819 -4.2483435 -4.2505474 -4.2499328 -4.2460079 -4.2454553 -4.2507443 -4.2548509 -4.2523389 -4.2448 -4.234036 -4.2224345][-4.2766795 -4.2708511 -4.2666745 -4.2626109 -4.2626104 -4.2654467 -4.26481 -4.2603574 -4.259305 -4.2653809 -4.27237 -4.2729654 -4.2683592 -4.2617803 -4.2550387][-4.2901587 -4.285902 -4.2825894 -4.2794633 -4.2791553 -4.2804184 -4.2773037 -4.2712173 -4.2688036 -4.2739229 -4.2809768 -4.2829828 -4.280848 -4.2778263 -4.2760448][-4.2953458 -4.2920861 -4.28978 -4.2887597 -4.2891922 -4.2886047 -4.283 -4.2750187 -4.2708178 -4.2728019 -4.2769489 -4.2781868 -4.277164 -4.2774148 -4.2806849][-4.3030643 -4.3002281 -4.2984195 -4.2988124 -4.3001323 -4.2983017 -4.2906246 -4.2805519 -4.2736158 -4.2708869 -4.2706413 -4.2701311 -4.269846 -4.2729974 -4.2803388]]...]
INFO - root - 2017-12-07 20:20:28.514691: step 49810, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.623 sec/batch; 48h:57m:06s remains)
INFO - root - 2017-12-07 20:20:35.252729: step 49820, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 51h:33m:11s remains)
INFO - root - 2017-12-07 20:20:42.118067: step 49830, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 56h:36m:17s remains)
INFO - root - 2017-12-07 20:20:48.932340: step 49840, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 54h:37m:49s remains)
INFO - root - 2017-12-07 20:20:55.719889: step 49850, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.680 sec/batch; 53h:23m:34s remains)
INFO - root - 2017-12-07 20:21:02.347849: step 49860, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 56h:10m:50s remains)
INFO - root - 2017-12-07 20:21:09.155152: step 49870, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 56h:37m:27s remains)
INFO - root - 2017-12-07 20:21:16.015610: step 49880, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 57h:41m:42s remains)
INFO - root - 2017-12-07 20:21:22.770777: step 49890, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.651 sec/batch; 51h:05m:29s remains)
INFO - root - 2017-12-07 20:21:29.350862: step 49900, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 48h:53m:36s remains)
2017-12-07 20:21:30.114077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1490355 -4.1434846 -4.1601605 -4.1903129 -4.21477 -4.2284245 -4.2398243 -4.2523661 -4.2534337 -4.2224712 -4.1683235 -4.1110921 -4.0740032 -4.0540371 -4.0624671][-4.1405239 -4.1399965 -4.1766047 -4.230197 -4.2663255 -4.273612 -4.265512 -4.2548051 -4.2365241 -4.1910553 -4.1345687 -4.0862613 -4.0603151 -4.0518346 -4.0725079][-4.1403642 -4.1523438 -4.2039471 -4.2669129 -4.3049607 -4.3029866 -4.2745166 -4.23871 -4.1988606 -4.1467867 -4.1016383 -4.0708046 -4.0549521 -4.0526447 -4.0836329][-4.153399 -4.1863928 -4.249022 -4.307919 -4.3306093 -4.3080268 -4.2547712 -4.1904249 -4.1317868 -4.0921297 -4.0784359 -4.0723567 -4.064342 -4.0665073 -4.1039448][-4.190021 -4.242393 -4.305079 -4.3423114 -4.3332806 -4.2774138 -4.1871309 -4.0889168 -4.0214653 -4.0189481 -4.05221 -4.0780969 -4.0868235 -4.1019573 -4.1371651][-4.2300787 -4.2927656 -4.3446836 -4.3542366 -4.3087645 -4.2132487 -4.0745831 -3.9286785 -3.8609967 -3.9247961 -4.0273962 -4.0964513 -4.1290116 -4.1527071 -4.1738214][-4.2620645 -4.3234959 -4.3569846 -4.336462 -4.256618 -4.1244826 -3.9368839 -3.7459726 -3.7009609 -3.847919 -4.0146995 -4.1191425 -4.1691442 -4.1905556 -4.1954241][-4.2831836 -4.3331504 -4.3464537 -4.3027573 -4.2026596 -4.0522733 -3.8549435 -3.6767905 -3.6898496 -3.8748536 -4.0528421 -4.1622224 -4.2108188 -4.2210569 -4.2121096][-4.285965 -4.3186388 -4.3187428 -4.2695246 -4.1734033 -4.0441909 -3.898948 -3.8023133 -3.8508258 -3.9962134 -4.1308193 -4.2171364 -4.2512956 -4.2501192 -4.2352452][-4.2784944 -4.2965841 -4.2926555 -4.2518687 -4.1784415 -4.0914116 -4.0135179 -3.985677 -4.0351787 -4.123075 -4.206768 -4.2624459 -4.2806392 -4.2724872 -4.2568097][-4.2730589 -4.2809029 -4.277616 -4.2511516 -4.2042084 -4.1545172 -4.1218619 -4.1306319 -4.171967 -4.2223439 -4.2693853 -4.2966833 -4.2986703 -4.2862868 -4.2702971][-4.2717409 -4.2724361 -4.2698193 -4.2565889 -4.2333651 -4.2097945 -4.2058377 -4.2304769 -4.2620673 -4.2879448 -4.3077254 -4.3122411 -4.30162 -4.2867179 -4.2716236][-4.2735119 -4.2703376 -4.2685118 -4.2643347 -4.2564564 -4.25213 -4.2657051 -4.2935686 -4.3144674 -4.3221354 -4.3206515 -4.3082995 -4.2934103 -4.2823329 -4.270998][-4.272552 -4.2679095 -4.2649217 -4.2627859 -4.2640014 -4.2752485 -4.2984242 -4.323535 -4.3340144 -4.3255148 -4.3036981 -4.2791023 -4.2664409 -4.2646689 -4.2617817][-4.2470713 -4.2420607 -4.2349219 -4.2297177 -4.2339444 -4.2550597 -4.2856979 -4.3101168 -4.3142662 -4.2922859 -4.2560072 -4.2237234 -4.2140269 -4.2226419 -4.2298365]]...]
INFO - root - 2017-12-07 20:21:36.833286: step 49910, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.687 sec/batch; 53h:53m:37s remains)
INFO - root - 2017-12-07 20:21:43.588278: step 49920, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 51h:52m:02s remains)
INFO - root - 2017-12-07 20:21:50.297682: step 49930, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 49h:05m:32s remains)
INFO - root - 2017-12-07 20:21:56.927303: step 49940, loss = 2.05, batch loss = 1.99 (13.1 examples/sec; 0.609 sec/batch; 47h:48m:35s remains)
INFO - root - 2017-12-07 20:22:03.797615: step 49950, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.747 sec/batch; 58h:38m:18s remains)
INFO - root - 2017-12-07 20:22:10.505839: step 49960, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:16m:32s remains)
INFO - root - 2017-12-07 20:22:17.282655: step 49970, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 52h:04m:10s remains)
INFO - root - 2017-12-07 20:22:24.022503: step 49980, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 50h:06m:14s remains)
INFO - root - 2017-12-07 20:22:30.733769: step 49990, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 49h:51m:49s remains)
INFO - root - 2017-12-07 20:22:37.414730: step 50000, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 52h:42m:38s remains)
2017-12-07 20:22:38.160197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3301649 -4.3247867 -4.316288 -4.3067493 -4.2862234 -4.2552662 -4.2271175 -4.2178431 -4.2240419 -4.22826 -4.2332754 -4.2441473 -4.2557621 -4.2633772 -4.2692451][-4.3289189 -4.3208184 -4.3068237 -4.2917023 -4.2652478 -4.2240181 -4.1818581 -4.1674342 -4.177135 -4.1770744 -4.1801844 -4.19302 -4.2086878 -4.2223392 -4.2305093][-4.3263736 -4.316556 -4.2960143 -4.2730894 -4.2372837 -4.1845803 -4.1258125 -4.1024742 -4.1105876 -4.1096697 -4.1184998 -4.1416974 -4.1631627 -4.1862822 -4.2008243][-4.3227906 -4.308557 -4.2800283 -4.2455134 -4.1978755 -4.1347904 -4.0613265 -4.0282478 -4.0295024 -4.0351005 -4.0600061 -4.103107 -4.1373525 -4.1699061 -4.1885233][-4.3193588 -4.2982006 -4.2582974 -4.2106586 -4.1495786 -4.0717187 -3.9804034 -3.9276991 -3.9328501 -3.9683785 -4.0269728 -4.0955949 -4.1408181 -4.1717453 -4.1856074][-4.3156528 -4.2853889 -4.2323594 -4.1699719 -4.0849876 -3.9756486 -3.8474383 -3.7663581 -3.8061554 -3.9063411 -4.0143671 -4.1045475 -4.1544132 -4.1758132 -4.1786079][-4.3154221 -4.2761478 -4.2060428 -4.1207619 -3.9981534 -3.8426344 -3.6602726 -3.546597 -3.6577694 -3.8483963 -3.9991262 -4.0896907 -4.1333737 -4.1516771 -4.15455][-4.3157349 -4.2676749 -4.1780562 -4.0643015 -3.9070759 -3.7207367 -3.5192044 -3.4171536 -3.595593 -3.8209853 -3.971338 -4.047492 -4.0806565 -4.1057415 -4.1246758][-4.3125315 -4.2602792 -4.1641483 -4.0447431 -3.9013233 -3.761714 -3.6423974 -3.6127591 -3.751843 -3.8978865 -3.9911613 -4.0333891 -4.0528517 -4.0850987 -4.1195164][-4.3084831 -4.2576704 -4.1681957 -4.072608 -3.9785123 -3.9108999 -3.8684673 -3.8709931 -3.94676 -4.0126452 -4.0530834 -4.0726051 -4.0864825 -4.1191864 -4.1537018][-4.3130345 -4.2770457 -4.2120323 -4.1511774 -4.1032686 -4.0710921 -4.0536962 -4.055912 -4.09586 -4.1238847 -4.1339107 -4.1390224 -4.1465364 -4.1691093 -4.1965604][-4.3244414 -4.3040862 -4.2626042 -4.2281432 -4.2090883 -4.192008 -4.1787868 -4.1770687 -4.2016091 -4.2178893 -4.2183061 -4.2171049 -4.219316 -4.2323766 -4.24794][-4.3313589 -4.3207445 -4.2949672 -4.2757878 -4.2705741 -4.2648664 -4.2610564 -4.259315 -4.2716875 -4.2796731 -4.2779474 -4.2750678 -4.2725616 -4.2799506 -4.2887053][-4.3350492 -4.3290925 -4.314404 -4.30573 -4.307137 -4.3099513 -4.317143 -4.321197 -4.3257632 -4.3262591 -4.3233447 -4.319263 -4.3156233 -4.3196096 -4.3257432][-4.3372989 -4.3316088 -4.3220739 -4.3189654 -4.3226352 -4.3293228 -4.3398418 -4.3458018 -4.3491483 -4.3494215 -4.3460903 -4.34164 -4.3401237 -4.3431869 -4.3489504]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 20:22:45.481835: step 50010, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 53h:41m:51s remains)
INFO - root - 2017-12-07 20:22:52.248920: step 50020, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 57h:50m:38s remains)
INFO - root - 2017-12-07 20:22:58.982359: step 50030, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 53h:46m:43s remains)
INFO - root - 2017-12-07 20:23:05.874327: step 50040, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 50h:05m:06s remains)
INFO - root - 2017-12-07 20:23:12.714327: step 50050, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 52h:04m:47s remains)
INFO - root - 2017-12-07 20:23:19.504539: step 50060, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 56h:26m:34s remains)
INFO - root - 2017-12-07 20:23:26.253959: step 50070, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 55h:42m:08s remains)
INFO - root - 2017-12-07 20:23:33.032341: step 50080, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 51h:10m:28s remains)
INFO - root - 2017-12-07 20:23:39.850440: step 50090, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 50h:05m:00s remains)
INFO - root - 2017-12-07 20:23:46.519619: step 50100, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 55h:35m:17s remains)
2017-12-07 20:23:47.258192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1071754 -4.104249 -4.136765 -4.1820683 -4.2102332 -4.2245412 -4.2216473 -4.1997132 -4.1730289 -4.1621494 -4.1668167 -4.1781855 -4.1900015 -4.2091126 -4.2167797][-4.1189384 -4.1046896 -4.1290836 -4.1788983 -4.2144485 -4.2251711 -4.219368 -4.2049513 -4.1915212 -4.1856308 -4.1921468 -4.20523 -4.2177038 -4.2338362 -4.2417216][-4.135005 -4.1169181 -4.1337752 -4.1819124 -4.2223706 -4.2279663 -4.2128711 -4.1968575 -4.1909013 -4.1898723 -4.1994762 -4.2118959 -4.2224731 -4.2329621 -4.2387195][-4.14644 -4.1268611 -4.1418858 -4.1888237 -4.2290311 -4.2287383 -4.2041779 -4.1778078 -4.171917 -4.1791873 -4.19277 -4.2014761 -4.2028875 -4.2055373 -4.2137651][-4.166563 -4.137753 -4.1468096 -4.1880441 -4.2198896 -4.2155809 -4.1836548 -4.1461439 -4.1352744 -4.1500845 -4.1666069 -4.17145 -4.16256 -4.158217 -4.1701684][-4.1832504 -4.1451445 -4.1456051 -4.1774845 -4.1965728 -4.1883225 -4.1543894 -4.1100707 -4.0894523 -4.1056175 -4.1258621 -4.1300116 -4.1144958 -4.0996118 -4.1074362][-4.2135472 -4.1694155 -4.1575036 -4.168416 -4.165092 -4.1511803 -4.1207576 -4.0730505 -4.0414915 -4.0547566 -4.083468 -4.0952368 -4.0789146 -4.0482306 -4.0404387][-4.2523761 -4.2085218 -4.1845965 -4.172205 -4.1446033 -4.1229296 -4.0984807 -4.0592017 -4.0280108 -4.0453243 -4.0840344 -4.1085238 -4.0936394 -4.0457716 -4.0133395][-4.2755375 -4.2354617 -4.20516 -4.17829 -4.1398158 -4.1121092 -4.0960827 -4.0722194 -4.0558958 -4.084177 -4.1297722 -4.1638575 -4.1523347 -4.0990367 -4.0526814][-4.2700887 -4.2387338 -4.2132564 -4.1871829 -4.1511536 -4.1303773 -4.1302619 -4.1317244 -4.1334486 -4.1616721 -4.19809 -4.2274246 -4.2212539 -4.1809788 -4.1433697][-4.2356615 -4.2158985 -4.2001243 -4.1914425 -4.1764345 -4.176209 -4.1925611 -4.2109265 -4.2193041 -4.2398286 -4.2614441 -4.2793221 -4.2791562 -4.2603559 -4.2381086][-4.1883283 -4.1830583 -4.1798477 -4.1928158 -4.2063012 -4.2236567 -4.245667 -4.2686605 -4.2777677 -4.2924585 -4.3050342 -4.3124213 -4.3112378 -4.306602 -4.2973118][-4.1437793 -4.154911 -4.1686258 -4.1965032 -4.2291045 -4.2512937 -4.2695308 -4.2887993 -4.2970991 -4.3067093 -4.3134856 -4.3145552 -4.3114123 -4.3126912 -4.312254][-4.1050134 -4.1263509 -4.1525788 -4.1881886 -4.2292371 -4.2521429 -4.2611127 -4.2760367 -4.2834659 -4.2881079 -4.292429 -4.293407 -4.2911363 -4.29498 -4.3023863][-4.0876741 -4.1095557 -4.1358533 -4.1708994 -4.21288 -4.2338676 -4.2382574 -4.2465854 -4.2507076 -4.2511096 -4.2569909 -4.2652063 -4.2683444 -4.273746 -4.2829189]]...]
INFO - root - 2017-12-07 20:23:53.979698: step 50110, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 49h:09m:30s remains)
INFO - root - 2017-12-07 20:24:00.872605: step 50120, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 51h:36m:55s remains)
INFO - root - 2017-12-07 20:24:07.706603: step 50130, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 57h:58m:56s remains)
INFO - root - 2017-12-07 20:24:14.564516: step 50140, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.739 sec/batch; 57h:56m:06s remains)
INFO - root - 2017-12-07 20:24:21.323631: step 50150, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 51h:45m:54s remains)
INFO - root - 2017-12-07 20:24:28.082486: step 50160, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 48h:47m:03s remains)
INFO - root - 2017-12-07 20:24:34.717778: step 50170, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 55h:16m:20s remains)
INFO - root - 2017-12-07 20:24:41.714601: step 50180, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 58h:08m:59s remains)
INFO - root - 2017-12-07 20:24:48.502614: step 50190, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 55h:38m:58s remains)
INFO - root - 2017-12-07 20:24:55.082256: step 50200, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 49h:55m:54s remains)
2017-12-07 20:24:55.860209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.353251 -4.3486104 -4.3488808 -4.3536716 -4.3560791 -4.35309 -4.3484859 -4.3440757 -4.3387837 -4.3335981 -4.326551 -4.3188405 -4.3118939 -4.3074708 -4.3090124][-4.3516688 -4.3425145 -4.3465209 -4.3583341 -4.3631439 -4.3553472 -4.3440285 -4.3327146 -4.3208861 -4.3093839 -4.2945518 -4.280139 -4.2714081 -4.2689748 -4.2778811][-4.3441911 -4.3254275 -4.3274031 -4.3440337 -4.3536496 -4.345768 -4.3262687 -4.3071179 -4.2902012 -4.2767491 -4.2553473 -4.2374043 -4.2278228 -4.228642 -4.2452221][-4.3310809 -4.3012414 -4.2949467 -4.3114862 -4.3247447 -4.31732 -4.2914419 -4.263042 -4.2446995 -4.236486 -4.2170029 -4.2037807 -4.1974735 -4.2036881 -4.2279611][-4.30762 -4.2660017 -4.2497225 -4.2623692 -4.2747226 -4.26504 -4.2314692 -4.1954646 -4.1841311 -4.18946 -4.1820669 -4.1808319 -4.186039 -4.1994071 -4.226016][-4.2788811 -4.2259235 -4.1958137 -4.196876 -4.2044516 -4.1880817 -4.1403193 -4.0961781 -4.1041145 -4.1303506 -4.1393642 -4.1543961 -4.1785588 -4.20209 -4.2325377][-4.2405663 -4.1742306 -4.1308 -4.1166358 -4.1104503 -4.0729876 -3.9986618 -3.9550886 -4.0042696 -4.0620322 -4.0888119 -4.119432 -4.1601014 -4.2005291 -4.2394843][-4.2005072 -4.1311617 -4.0848613 -4.0647964 -4.0460496 -3.9827969 -3.8830273 -3.8487368 -3.9396873 -4.021862 -4.0597734 -4.1004572 -4.1491518 -4.1959143 -4.2414851][-4.1888242 -4.1320319 -4.0958452 -4.0821352 -4.0660682 -4.0092216 -3.9316368 -3.9233 -4.0028977 -4.0636082 -4.0902977 -4.1254334 -4.1663442 -4.2068357 -4.2469783][-4.2039995 -4.1641479 -4.1341996 -4.1264014 -4.1196938 -4.087039 -4.051405 -4.0619669 -4.1110544 -4.1362176 -4.1446404 -4.1672111 -4.1972065 -4.228488 -4.2580771][-4.223958 -4.196476 -4.1742778 -4.1757708 -4.1815367 -4.1693649 -4.1588435 -4.1729078 -4.1998067 -4.2036538 -4.1959481 -4.2011404 -4.2196264 -4.2430086 -4.2644825][-4.2511821 -4.228353 -4.2137361 -4.2245717 -4.2348313 -4.2303195 -4.2286038 -4.2384176 -4.2501373 -4.2441387 -4.2292895 -4.2304058 -4.246253 -4.2644773 -4.2788911][-4.2777495 -4.2583284 -4.2505322 -4.2653403 -4.2739687 -4.2708068 -4.2722945 -4.2790947 -4.2818642 -4.269721 -4.2549248 -4.2609706 -4.2783723 -4.2922587 -4.301744][-4.2989087 -4.2842884 -4.2794857 -4.2912073 -4.2948194 -4.2909527 -4.2922788 -4.29593 -4.2946038 -4.2790513 -4.2688136 -4.2810206 -4.2998953 -4.3119535 -4.3183966][-4.3022223 -4.2915807 -4.288981 -4.295485 -4.2951245 -4.2918477 -4.291707 -4.2954578 -4.2936864 -4.2798767 -4.2728066 -4.2845373 -4.300796 -4.3123245 -4.3195858]]...]
INFO - root - 2017-12-07 20:25:02.714346: step 50210, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 56h:55m:28s remains)
INFO - root - 2017-12-07 20:25:09.435818: step 50220, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 50h:37m:00s remains)
INFO - root - 2017-12-07 20:25:16.263004: step 50230, loss = 2.02, batch loss = 1.97 (12.3 examples/sec; 0.651 sec/batch; 51h:00m:57s remains)
INFO - root - 2017-12-07 20:25:22.892443: step 50240, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 49h:32m:30s remains)
INFO - root - 2017-12-07 20:25:29.673033: step 50250, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 56h:28m:36s remains)
INFO - root - 2017-12-07 20:25:36.475686: step 50260, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 54h:36m:51s remains)
INFO - root - 2017-12-07 20:25:43.320462: step 50270, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 51h:42m:48s remains)
INFO - root - 2017-12-07 20:25:50.043943: step 50280, loss = 2.03, batch loss = 1.97 (12.8 examples/sec; 0.623 sec/batch; 48h:51m:33s remains)
INFO - root - 2017-12-07 20:25:56.817131: step 50290, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 49h:00m:14s remains)
INFO - root - 2017-12-07 20:26:03.493385: step 50300, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 56h:22m:41s remains)
2017-12-07 20:26:04.199310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2520819 -4.253706 -4.2510052 -4.2363415 -4.2203517 -4.2127209 -4.214067 -4.2237196 -4.2401714 -4.2466626 -4.2417212 -4.2403522 -4.2465892 -4.2427607 -4.2395506][-4.2182794 -4.2185664 -4.2081752 -4.1854124 -4.1652365 -4.1578894 -4.1624379 -4.175487 -4.1967912 -4.2141967 -4.2223506 -4.2284842 -4.2336469 -4.2271633 -4.2160921][-4.1832309 -4.1869 -4.1732135 -4.1440883 -4.1159992 -4.1057935 -4.1097889 -4.12086 -4.1457329 -4.1791406 -4.2056303 -4.2207079 -4.2210789 -4.2105188 -4.1949644][-4.164434 -4.1687512 -4.1571279 -4.1277471 -4.0940003 -4.0736847 -4.0659046 -4.0646996 -4.0908628 -4.1396408 -4.1782403 -4.2003479 -4.2011843 -4.1939554 -4.1797471][-4.1501389 -4.1514978 -4.1436157 -4.121376 -4.0813766 -4.0417175 -4.0080466 -3.99338 -4.0272207 -4.0975924 -4.1481519 -4.1767287 -4.1796985 -4.182518 -4.17297][-4.1275177 -4.1256542 -4.1261587 -4.1070323 -4.0562549 -3.9845579 -3.9084206 -3.8771927 -3.9396915 -4.0456338 -4.1178951 -4.1553917 -4.1597323 -4.1719866 -4.167603][-4.0823989 -4.0777864 -4.0802121 -4.0602479 -3.9959514 -3.8838673 -3.7522631 -3.7073798 -3.8318849 -3.9965467 -4.0950913 -4.1381731 -4.1492996 -4.1641746 -4.1610756][-4.0550904 -4.0428147 -4.0383592 -4.0108061 -3.9273064 -3.7826822 -3.6058936 -3.5542369 -3.7534108 -3.9664469 -4.0817862 -4.1275735 -4.1377163 -4.149724 -4.1490207][-4.0866394 -4.0712962 -4.0561776 -4.0192509 -3.9346123 -3.8001478 -3.6488886 -3.624079 -3.8108816 -3.9987757 -4.0979867 -4.1335754 -4.1344166 -4.1412058 -4.1402359][-4.1524539 -4.1341534 -4.1118727 -4.0740762 -4.0035987 -3.911458 -3.8249431 -3.8237538 -3.9449527 -4.068449 -4.13811 -4.1628942 -4.157979 -4.1567259 -4.1484017][-4.1961632 -4.1802311 -4.1560636 -4.123497 -4.0817122 -4.0295715 -3.9832158 -3.9886074 -4.0554528 -4.1328268 -4.1820121 -4.1963887 -4.1886606 -4.1799326 -4.1611147][-4.1986289 -4.1901207 -4.16986 -4.1475353 -4.1279244 -4.10086 -4.0742674 -4.0770469 -4.1086116 -4.16622 -4.2075992 -4.21549 -4.2072506 -4.1954575 -4.1764045][-4.2049413 -4.1958828 -4.1798854 -4.1664257 -4.1568336 -4.1418729 -4.1256547 -4.1233544 -4.1395512 -4.1850309 -4.2177873 -4.2226977 -4.2161613 -4.204514 -4.1932173][-4.2181783 -4.2057638 -4.1922116 -4.1862464 -4.1822739 -4.1731548 -4.1640239 -4.16146 -4.1724253 -4.2034855 -4.2244992 -4.2283869 -4.2267132 -4.2209039 -4.2166514][-4.2426081 -4.2279654 -4.2199593 -4.2194223 -4.216784 -4.208724 -4.2027154 -4.20228 -4.2109842 -4.231267 -4.2463937 -4.255702 -4.2612238 -4.2608671 -4.257524]]...]
INFO - root - 2017-12-07 20:26:11.014699: step 50310, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 50h:30m:57s remains)
INFO - root - 2017-12-07 20:26:17.893213: step 50320, loss = 2.03, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 54h:10m:49s remains)
INFO - root - 2017-12-07 20:26:24.788427: step 50330, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 57h:37m:52s remains)
INFO - root - 2017-12-07 20:26:31.650097: step 50340, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 53h:16m:38s remains)
INFO - root - 2017-12-07 20:26:38.464740: step 50350, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.637 sec/batch; 49h:57m:49s remains)
INFO - root - 2017-12-07 20:26:45.263416: step 50360, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.613 sec/batch; 48h:03m:52s remains)
INFO - root - 2017-12-07 20:26:52.042683: step 50370, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 52h:59m:18s remains)
INFO - root - 2017-12-07 20:26:58.875011: step 50380, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 56h:44m:40s remains)
INFO - root - 2017-12-07 20:27:05.529377: step 50390, loss = 2.07, batch loss = 2.02 (13.6 examples/sec; 0.590 sec/batch; 46h:13m:31s remains)
INFO - root - 2017-12-07 20:27:12.050308: step 50400, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 53h:30m:12s remains)
2017-12-07 20:27:12.735026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1244106 -4.0961761 -4.0945711 -4.1191192 -4.1480427 -4.1790166 -4.205049 -4.2260127 -4.2376742 -4.2315741 -4.21546 -4.1968985 -4.180831 -4.172647 -4.1764688][-4.1172171 -4.1011019 -4.1068749 -4.1334758 -4.1590695 -4.1869383 -4.2087622 -4.2217588 -4.2174187 -4.1936288 -4.1633139 -4.1324148 -4.1037307 -4.0878568 -4.0903621][-4.1604838 -4.1537395 -4.159452 -4.1784506 -4.1946983 -4.2096186 -4.2151055 -4.2103682 -4.1844592 -4.1383629 -4.0906029 -4.0518241 -4.0254836 -4.0165377 -4.0254278][-4.2045693 -4.2056246 -4.2108836 -4.2252712 -4.2313118 -4.2268744 -4.2104297 -4.1862769 -4.1461573 -4.0862312 -4.0311666 -3.9972935 -3.99006 -3.9992568 -4.017777][-4.2257867 -4.232583 -4.23878 -4.2492242 -4.2441778 -4.222846 -4.1876655 -4.1512847 -4.1044035 -4.0465231 -3.9977314 -3.9769423 -3.9888005 -4.0147152 -4.0448971][-4.2290297 -4.2307491 -4.2260857 -4.2265611 -4.2167158 -4.1877379 -4.1394053 -4.0958095 -4.05582 -4.0167017 -3.9865656 -3.9794466 -4.0027666 -4.0451064 -4.0852704][-4.2285023 -4.2129679 -4.1876049 -4.1730361 -4.1577463 -4.1252222 -4.0720191 -4.0344944 -4.013175 -4.0069094 -4.0027142 -4.0038147 -4.03179 -4.0815635 -4.1258359][-4.2279797 -4.1927967 -4.14913 -4.1226583 -4.1105537 -4.0894413 -4.0502415 -4.0232058 -4.0177007 -4.0342979 -4.0489349 -4.0506568 -4.0794706 -4.1295233 -4.1717052][-4.2089844 -4.1651912 -4.1209874 -4.0965014 -4.0996771 -4.1030803 -4.0870562 -4.0722027 -4.0749006 -4.0922384 -4.1054206 -4.1063628 -4.1346087 -4.1799011 -4.2172532][-4.1843076 -4.1523123 -4.1236529 -4.1119957 -4.1312475 -4.1554055 -4.1553984 -4.1416607 -4.1392107 -4.1445675 -4.153614 -4.1564322 -4.1841087 -4.2205372 -4.2495766][-4.1695056 -4.1573162 -4.1472559 -4.1462789 -4.1698585 -4.1954727 -4.1965542 -4.1748347 -4.1598711 -4.1602125 -4.1715422 -4.1812873 -4.2055125 -4.2326651 -4.2533851][-4.1834826 -4.1810279 -4.1785975 -4.1796427 -4.1976986 -4.2135568 -4.2053142 -4.1710434 -4.1418972 -4.14268 -4.15856 -4.1721125 -4.1970048 -4.2250676 -4.241879][-4.2222557 -4.2209368 -4.2196474 -4.2185497 -4.2261605 -4.2278786 -4.2065344 -4.1618042 -4.1283503 -4.1366386 -4.1570506 -4.1691861 -4.1917748 -4.2203608 -4.2366219][-4.2636743 -4.2617688 -4.2585635 -4.2541676 -4.2540379 -4.24849 -4.2263789 -4.1872163 -4.16225 -4.17319 -4.185997 -4.18589 -4.198246 -4.2241788 -4.2398276][-4.2812467 -4.27997 -4.2772961 -4.2736797 -4.2730789 -4.2683415 -4.2533422 -4.2242308 -4.2063389 -4.2131524 -4.2186737 -4.2085757 -4.2095394 -4.2292318 -4.242805]]...]
INFO - root - 2017-12-07 20:27:19.500179: step 50410, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.743 sec/batch; 58h:11m:58s remains)
INFO - root - 2017-12-07 20:27:26.362427: step 50420, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 0.770 sec/batch; 60h:20m:05s remains)
INFO - root - 2017-12-07 20:27:33.144217: step 50430, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 52h:48m:41s remains)
INFO - root - 2017-12-07 20:27:40.007753: step 50440, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 50h:35m:22s remains)
INFO - root - 2017-12-07 20:27:46.797241: step 50450, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 55h:31m:35s remains)
INFO - root - 2017-12-07 20:27:53.642928: step 50460, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:09m:13s remains)
INFO - root - 2017-12-07 20:28:00.485457: step 50470, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 54h:00m:27s remains)
INFO - root - 2017-12-07 20:28:07.119968: step 50480, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 51h:35m:35s remains)
INFO - root - 2017-12-07 20:28:13.990808: step 50490, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 56h:10m:17s remains)
INFO - root - 2017-12-07 20:28:20.577589: step 50500, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 56h:44m:13s remains)
2017-12-07 20:28:21.219584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533054 -4.2646179 -4.2789168 -4.2899232 -4.2870622 -4.2709136 -4.2406325 -4.2091079 -4.1943054 -4.200624 -4.2061687 -4.2107434 -4.213943 -4.2191572 -4.2249317][-4.2567344 -4.2639356 -4.2800255 -4.2945638 -4.2899461 -4.267477 -4.2265253 -4.1855702 -4.1691937 -4.1811233 -4.18841 -4.1930966 -4.2018452 -4.214179 -4.2256746][-4.2693338 -4.2713342 -4.2841058 -4.2959161 -4.2853141 -4.2550497 -4.2033806 -4.1538997 -4.1369672 -4.154407 -4.16771 -4.1769562 -4.1886625 -4.2096128 -4.2323356][-4.2838778 -4.2813463 -4.2900972 -4.29696 -4.2813964 -4.2434473 -4.179635 -4.122715 -4.1095986 -4.1350837 -4.15783 -4.1711311 -4.1829023 -4.2071443 -4.2376151][-4.2964411 -4.2909155 -4.2931581 -4.2908912 -4.2699852 -4.2247753 -4.1507835 -4.0895877 -4.0825996 -4.1186666 -4.1560726 -4.181993 -4.1973391 -4.2169452 -4.2409334][-4.3067517 -4.3033247 -4.2968459 -4.2809129 -4.24741 -4.1930084 -4.1118665 -4.0530448 -4.0570679 -4.1047935 -4.1565948 -4.1989923 -4.2261219 -4.2409406 -4.2507081][-4.3151894 -4.3170571 -4.3066206 -4.2770977 -4.2286906 -4.1651683 -4.0854497 -4.0333266 -4.0434861 -4.0953979 -4.1562681 -4.2124834 -4.2537842 -4.2708139 -4.2728705][-4.3195763 -4.3265324 -4.3170233 -4.2819715 -4.227972 -4.164443 -4.0930848 -4.04883 -4.0533719 -4.0911756 -4.1465487 -4.2078242 -4.2598991 -4.2874184 -4.2954755][-4.3197079 -4.3296576 -4.3243084 -4.2924132 -4.2439742 -4.1914549 -4.1358833 -4.1020427 -4.095983 -4.1075892 -4.14151 -4.1926994 -4.2463346 -4.284853 -4.3063884][-4.3189096 -4.3302693 -4.3310637 -4.3078222 -4.2704453 -4.2330608 -4.1952229 -4.1709037 -4.158246 -4.1517725 -4.1620684 -4.1887021 -4.2283173 -4.266758 -4.2971659][-4.3197489 -4.3298936 -4.3356018 -4.3228855 -4.2958822 -4.2668958 -4.2380347 -4.2154474 -4.1980867 -4.1844945 -4.1815395 -4.1921072 -4.2141128 -4.2406468 -4.270813][-4.321311 -4.3274636 -4.3329954 -4.3278904 -4.3087311 -4.2861443 -4.2631416 -4.2409973 -4.2189374 -4.2010446 -4.1946535 -4.19825 -4.2100444 -4.2231488 -4.2415223][-4.3208795 -4.3244214 -4.3275728 -4.3253946 -4.3125982 -4.297421 -4.2827559 -4.2647767 -4.2442231 -4.2256126 -4.2135773 -4.2082477 -4.2118359 -4.2184911 -4.2249641][-4.3186517 -4.3201237 -4.321919 -4.3189468 -4.3115039 -4.3030677 -4.2954803 -4.2857809 -4.2730041 -4.2582474 -4.2428889 -4.226346 -4.2175837 -4.2186193 -4.2187643][-4.3162565 -4.3163829 -4.3173075 -4.3125143 -4.3065128 -4.3037877 -4.3041978 -4.3031068 -4.2965755 -4.2843485 -4.2699676 -4.2501974 -4.2297177 -4.2184477 -4.2124972]]...]
INFO - root - 2017-12-07 20:28:28.048375: step 50510, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 51h:20m:15s remains)
INFO - root - 2017-12-07 20:28:34.842957: step 50520, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 56h:37m:49s remains)
INFO - root - 2017-12-07 20:28:41.614638: step 50530, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 56h:51m:51s remains)
INFO - root - 2017-12-07 20:28:48.406791: step 50540, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 53h:28m:00s remains)
INFO - root - 2017-12-07 20:28:55.146624: step 50550, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 50h:02m:04s remains)
INFO - root - 2017-12-07 20:29:01.927490: step 50560, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 51h:14m:13s remains)
INFO - root - 2017-12-07 20:29:08.800713: step 50570, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.753 sec/batch; 58h:59m:34s remains)
INFO - root - 2017-12-07 20:29:15.622353: step 50580, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 56h:17m:42s remains)
INFO - root - 2017-12-07 20:29:22.414659: step 50590, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.674 sec/batch; 52h:48m:23s remains)
INFO - root - 2017-12-07 20:29:29.053889: step 50600, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 49h:51m:04s remains)
2017-12-07 20:29:29.801767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2505517 -4.262373 -4.2730627 -4.2876844 -4.2889447 -4.2732835 -4.2555056 -4.2441859 -4.244977 -4.2567759 -4.2670832 -4.2699981 -4.2642317 -4.2654815 -4.2701664][-4.2844262 -4.2963719 -4.3147511 -4.3304434 -4.3235574 -4.2962584 -4.2618966 -4.2296128 -4.2265158 -4.2420163 -4.2618232 -4.2808175 -4.2847681 -4.2894068 -4.2932954][-4.3115363 -4.3270493 -4.3456545 -4.3557506 -4.3374877 -4.2922378 -4.237637 -4.19414 -4.1960325 -4.2216969 -4.25035 -4.284533 -4.3016877 -4.31026 -4.3128562][-4.3253 -4.3426332 -4.3547788 -4.3566494 -4.3285141 -4.2625904 -4.1848216 -4.1358833 -4.1482048 -4.189846 -4.2284932 -4.2721729 -4.2968612 -4.3079395 -4.3121705][-4.3189745 -4.3333411 -4.3393884 -4.3311949 -4.2903433 -4.2059565 -4.1128249 -4.0646729 -4.09179 -4.1526146 -4.2058969 -4.2539158 -4.2795596 -4.290453 -4.2969422][-4.3112993 -4.3184161 -4.3153367 -4.2951279 -4.2396088 -4.1402235 -4.0244727 -3.9632998 -4.0099921 -4.1019416 -4.1787109 -4.230824 -4.2571077 -4.2692928 -4.2784281][-4.3037448 -4.3025537 -4.2884326 -4.25038 -4.1786408 -4.06177 -3.9045198 -3.8126025 -3.8902946 -4.0342493 -4.1445265 -4.2043161 -4.2319527 -4.2509327 -4.2630806][-4.293221 -4.2878151 -4.2643108 -4.2078967 -4.121861 -3.9821644 -3.7739072 -3.6451755 -3.77634 -3.9828041 -4.1181474 -4.1826673 -4.2172718 -4.2466154 -4.2612371][-4.286315 -4.2766261 -4.2439208 -4.1763783 -4.0837131 -3.9356661 -3.7131085 -3.5946956 -3.7648203 -3.9898791 -4.1201348 -4.1818662 -4.2256012 -4.2613974 -4.2746286][-4.2879429 -4.2741466 -4.2333889 -4.1649141 -4.0823445 -3.9568455 -3.791657 -3.7399647 -3.8816576 -4.0470462 -4.14165 -4.1958117 -4.2455816 -4.2814221 -4.2902484][-4.2946048 -4.2772765 -4.2363939 -4.1803646 -4.1224365 -4.0396051 -3.9466152 -3.939116 -4.0249681 -4.1128922 -4.1655364 -4.2113075 -4.260438 -4.2915955 -4.2986][-4.3012767 -4.282125 -4.2474937 -4.20881 -4.1784039 -4.1375918 -4.0965767 -4.0990214 -4.1348443 -4.1667261 -4.195838 -4.2332535 -4.2718806 -4.2961845 -4.3075452][-4.3024836 -4.2814565 -4.2544737 -4.2318988 -4.2249165 -4.2166972 -4.2064886 -4.2068706 -4.2086377 -4.2077947 -4.226254 -4.2573724 -4.2851586 -4.3066587 -4.3243208][-4.3065176 -4.2864881 -4.2644773 -4.2519064 -4.254981 -4.2621465 -4.2650189 -4.2634354 -4.2568135 -4.2447953 -4.2528214 -4.2753978 -4.2984915 -4.3203068 -4.3376064][-4.3189797 -4.3030667 -4.2798572 -4.2676797 -4.2709932 -4.2823558 -4.2911859 -4.2899432 -4.2803483 -4.26601 -4.260653 -4.2719207 -4.292347 -4.3132639 -4.3244739]]...]
INFO - root - 2017-12-07 20:29:36.610419: step 50610, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 56h:36m:35s remains)
INFO - root - 2017-12-07 20:29:43.473450: step 50620, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 51h:43m:23s remains)
INFO - root - 2017-12-07 20:29:50.304980: step 50630, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.638 sec/batch; 49h:56m:27s remains)
INFO - root - 2017-12-07 20:29:57.103834: step 50640, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 53h:42m:48s remains)
INFO - root - 2017-12-07 20:30:04.092501: step 50650, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 55h:16m:24s remains)
INFO - root - 2017-12-07 20:30:10.877628: step 50660, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 53h:43m:00s remains)
INFO - root - 2017-12-07 20:30:17.668538: step 50670, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 50h:28m:00s remains)
INFO - root - 2017-12-07 20:30:24.513149: step 50680, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 51h:23m:33s remains)
INFO - root - 2017-12-07 20:30:31.345826: step 50690, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 56h:39m:09s remains)
INFO - root - 2017-12-07 20:30:38.100280: step 50700, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 56h:34m:18s remains)
2017-12-07 20:30:38.820005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1641293 -4.1883149 -4.2124176 -4.2198524 -4.2117505 -4.1980562 -4.1943212 -4.1914945 -4.1964607 -4.2097073 -4.2246156 -4.2201681 -4.1863904 -4.1421566 -4.1124797][-4.1583276 -4.184124 -4.2066884 -4.2073193 -4.1933622 -4.1769629 -4.1756091 -4.175652 -4.18176 -4.1961446 -4.20992 -4.2052565 -4.1707344 -4.1253185 -4.09318][-4.156404 -4.1869698 -4.2096 -4.2027087 -4.1805 -4.1625113 -4.1663041 -4.173635 -4.1801491 -4.1939626 -4.20592 -4.2013416 -4.1688104 -4.1281457 -4.0973978][-4.1648469 -4.1952171 -4.2144461 -4.1985693 -4.1650076 -4.1441836 -4.1548462 -4.1727138 -4.1840606 -4.1982803 -4.2097297 -4.2057161 -4.1785946 -4.1458845 -4.1194372][-4.1818976 -4.20623 -4.2171555 -4.1897364 -4.1435738 -4.1163535 -4.1275144 -4.1540442 -4.1755219 -4.1987357 -4.2158403 -4.2157845 -4.1939626 -4.1649313 -4.1378689][-4.1926656 -4.2120595 -4.2160873 -4.1821737 -4.1281304 -4.0899043 -4.09008 -4.1188126 -4.1531286 -4.1904054 -4.2159252 -4.2198219 -4.1979904 -4.1658368 -4.1362162][-4.187746 -4.205677 -4.2083197 -4.1765122 -4.1217332 -4.0714188 -4.0541143 -4.0782371 -4.1227579 -4.1739349 -4.2068157 -4.2129159 -4.1913376 -4.1574011 -4.1301565][-4.1574378 -4.1757812 -4.1816006 -4.1606827 -4.1153741 -4.0626097 -4.0339751 -4.0520291 -4.0984383 -4.1565456 -4.1933212 -4.1976619 -4.174695 -4.1420188 -4.1219974][-4.1148667 -4.1281185 -4.1376767 -4.1299014 -4.0970378 -4.0484948 -4.0177341 -4.0344005 -4.0799961 -4.138804 -4.1757507 -4.179543 -4.1581388 -4.1294479 -4.1167164][-4.1059341 -4.1002493 -4.1024714 -4.0964923 -4.0704708 -4.0306706 -4.0066133 -4.0280051 -4.0737915 -4.1275425 -4.1599679 -4.1631322 -4.1460667 -4.1234145 -4.1177063][-4.1458387 -4.124176 -4.1102805 -4.093164 -4.0656452 -4.0356374 -4.0207481 -4.0423422 -4.0799255 -4.1192245 -4.1399546 -4.1428666 -4.1310825 -4.1148233 -4.1162238][-4.1964235 -4.1766968 -4.1568742 -4.1322708 -4.10417 -4.0821385 -4.0725069 -4.0830731 -4.1002564 -4.1166625 -4.1222224 -4.1216593 -4.1153808 -4.10725 -4.114521][-4.2214088 -4.2184339 -4.2103963 -4.1940703 -4.1748 -4.1602125 -4.1501589 -4.146369 -4.1429563 -4.13657 -4.1250458 -4.1159778 -4.1093278 -4.1027484 -4.1085205][-4.2158537 -4.2313533 -4.24182 -4.2415342 -4.2353129 -4.2294388 -4.2207785 -4.2090759 -4.1921153 -4.17185 -4.1507697 -4.1355462 -4.1213617 -4.1055264 -4.0993128][-4.1910415 -4.2135086 -4.2375994 -4.2521853 -4.2592487 -4.2623806 -4.2594314 -4.2484317 -4.2285748 -4.2065597 -4.1872425 -4.1714377 -4.1497808 -4.120398 -4.0968833]]...]
INFO - root - 2017-12-07 20:30:45.601698: step 50710, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 53h:00m:36s remains)
INFO - root - 2017-12-07 20:30:52.480868: step 50720, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 57h:34m:14s remains)
INFO - root - 2017-12-07 20:30:59.316014: step 50730, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 56h:34m:13s remains)
INFO - root - 2017-12-07 20:31:06.137654: step 50740, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 54h:32m:45s remains)
INFO - root - 2017-12-07 20:31:12.962615: step 50750, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 49h:42m:57s remains)
INFO - root - 2017-12-07 20:31:19.778207: step 50760, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 50h:22m:22s remains)
INFO - root - 2017-12-07 20:31:26.722959: step 50770, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.752 sec/batch; 58h:51m:37s remains)
INFO - root - 2017-12-07 20:31:33.479933: step 50780, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 56h:10m:58s remains)
INFO - root - 2017-12-07 20:31:40.016727: step 50790, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 48h:54m:16s remains)
INFO - root - 2017-12-07 20:31:46.691349: step 50800, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 52h:06m:33s remains)
2017-12-07 20:31:47.473173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1622939 -4.1408386 -4.1383224 -4.135747 -4.1262894 -4.1125212 -4.102416 -4.098175 -4.1177316 -4.1485028 -4.17475 -4.16653 -4.1420145 -4.1349754 -4.1368127][-4.1758571 -4.1551256 -4.1573815 -4.1567287 -4.1456079 -4.1307278 -4.11763 -4.1062651 -4.1188316 -4.148561 -4.1762948 -4.1713281 -4.1509418 -4.1450357 -4.1433396][-4.1935191 -4.1814833 -4.1889963 -4.1877613 -4.1738281 -4.1607318 -4.1503029 -4.1367059 -4.1422276 -4.1710486 -4.1976161 -4.1937242 -4.175271 -4.1675959 -4.1606846][-4.1818933 -4.1864152 -4.2016797 -4.199769 -4.1827521 -4.1685929 -4.1578684 -4.1420908 -4.1430964 -4.173954 -4.2036004 -4.2019968 -4.1875377 -4.1787057 -4.1705751][-4.1507649 -4.1657434 -4.1897779 -4.19432 -4.1804805 -4.1625671 -4.1420569 -4.1147108 -4.1064291 -4.136445 -4.1669436 -4.1701536 -4.1624556 -4.1589508 -4.1571922][-4.1202111 -4.129714 -4.1490283 -4.1479192 -4.1328773 -4.1168346 -4.0863566 -4.0354681 -4.0142965 -4.0485268 -4.0851188 -4.0955296 -4.1001091 -4.1074018 -4.1197844][-4.0934796 -4.0850506 -4.0872426 -4.0703669 -4.0465288 -4.03228 -3.9918523 -3.9125082 -3.8862791 -3.9337134 -3.9798245 -4.0010996 -4.0218081 -4.0467854 -4.0759115][-4.05665 -4.0343909 -4.0183587 -3.9767678 -3.9348388 -3.9162366 -3.8567047 -3.747103 -3.7268295 -3.8079374 -3.8777254 -3.9165905 -3.9516516 -3.9875171 -4.0266609][-4.0246334 -4.0022864 -3.9830244 -3.9333119 -3.8868513 -3.874094 -3.8181324 -3.7079103 -3.7027822 -3.7973657 -3.8695974 -3.9091971 -3.9374981 -3.963614 -3.9993567][-4.003273 -3.9927692 -3.9845929 -3.9454498 -3.9159079 -3.9218152 -3.8947175 -3.8259673 -3.8325453 -3.9034972 -3.9497967 -3.9698358 -3.9733133 -3.9815266 -4.0078211][-3.9879923 -3.9911504 -3.9984334 -3.9740524 -3.9626443 -3.9801815 -3.9726 -3.9344144 -3.9413319 -3.9853497 -4.0106077 -4.0119095 -3.9973812 -3.99219 -4.0091524][-4.0148506 -4.0288715 -4.0454545 -4.0346694 -4.0356288 -4.0554628 -4.055294 -4.0323935 -4.0320458 -4.0536866 -4.0641174 -4.0538907 -4.0289764 -4.0167766 -4.0273952][-4.0771523 -4.0964766 -4.1163077 -4.1138916 -4.1184287 -4.1329727 -4.1342535 -4.1214657 -4.1186185 -4.1279731 -4.1311841 -4.1160803 -4.0877275 -4.07368 -4.0804834][-4.1573186 -4.1736093 -4.1862431 -4.1833582 -4.1832 -4.1889486 -4.1884489 -4.1825333 -4.1816964 -4.1856418 -4.1860852 -4.1724095 -4.1489668 -4.1382375 -4.1451344][-4.2359238 -4.2428055 -4.2457933 -4.2410827 -4.2371182 -4.2362227 -4.2339835 -4.2309308 -4.2301607 -4.2321434 -4.2322893 -4.2236056 -4.2095222 -4.2046571 -4.2118149]]...]
INFO - root - 2017-12-07 20:31:54.218863: step 50810, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 52h:15m:52s remains)
INFO - root - 2017-12-07 20:32:01.016346: step 50820, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.615 sec/batch; 48h:04m:56s remains)
INFO - root - 2017-12-07 20:32:07.888645: step 50830, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 50h:45m:52s remains)
INFO - root - 2017-12-07 20:32:14.805614: step 50840, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.768 sec/batch; 60h:06m:57s remains)
INFO - root - 2017-12-07 20:32:21.665627: step 50850, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 57h:23m:43s remains)
INFO - root - 2017-12-07 20:32:28.456467: step 50860, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 51h:41m:55s remains)
INFO - root - 2017-12-07 20:32:35.229559: step 50870, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 49h:17m:55s remains)
INFO - root - 2017-12-07 20:32:42.065031: step 50880, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 55h:08m:48s remains)
INFO - root - 2017-12-07 20:32:48.910977: step 50890, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 56h:49m:39s remains)
INFO - root - 2017-12-07 20:32:55.664511: step 50900, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 52h:38m:15s remains)
2017-12-07 20:32:56.455894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2193165 -4.1771398 -4.1555538 -4.1448264 -4.1478066 -4.170784 -4.1943269 -4.2140346 -4.2117143 -4.1971121 -4.178679 -4.1600986 -4.1539307 -4.1524696 -4.1503491][-4.2094231 -4.1694493 -4.150629 -4.1438894 -4.149601 -4.1710877 -4.1901779 -4.2064514 -4.2027311 -4.1930709 -4.1799 -4.1660657 -4.1593075 -4.1594868 -4.1590719][-4.1993837 -4.170794 -4.1628547 -4.1612563 -4.1628857 -4.1726 -4.1826205 -4.192925 -4.192349 -4.1906395 -4.180625 -4.166553 -4.1598749 -4.165421 -4.1710167][-4.2044215 -4.1872416 -4.1849208 -4.1786842 -4.1640954 -4.1517839 -4.146306 -4.152915 -4.1639881 -4.1747394 -4.1704569 -4.1605163 -4.1571312 -4.1680069 -4.1803236][-4.2114673 -4.1952267 -4.1874046 -4.1702967 -4.1329565 -4.0939989 -4.0677366 -4.0706887 -4.1007285 -4.1288509 -4.1363854 -4.1379633 -4.1434517 -4.1591706 -4.1798382][-4.2037263 -4.1758704 -4.1529827 -4.1217437 -4.0639124 -4.0014987 -3.9540751 -3.9583106 -4.0127139 -4.0613627 -4.082859 -4.0968518 -4.1178908 -4.1414866 -4.1714][-4.1788263 -4.134119 -4.0988736 -4.0610785 -3.9952693 -3.916873 -3.8534238 -3.8611426 -3.93325 -3.9986746 -4.0348334 -4.0629778 -4.1003919 -4.1363988 -4.1716685][-4.1681356 -4.117095 -4.0815291 -4.0534658 -4.0077171 -3.9424257 -3.8847995 -3.8933938 -3.9565825 -4.0158272 -4.0558023 -4.0846653 -4.1230512 -4.159615 -4.1839476][-4.1672583 -4.1189227 -4.0979409 -4.0969987 -4.0906553 -4.06095 -4.0239391 -4.0225649 -4.053566 -4.0889726 -4.1165428 -4.135354 -4.1643925 -4.1884303 -4.1949821][-4.1641531 -4.1172028 -4.1120172 -4.1362619 -4.1605177 -4.1553082 -4.1297479 -4.1176567 -4.1237245 -4.1414843 -4.164032 -4.18262 -4.2044964 -4.2138186 -4.2051435][-4.1470842 -4.09657 -4.101449 -4.144125 -4.1867423 -4.1946993 -4.1767273 -4.1604981 -4.1564918 -4.1687436 -4.1926403 -4.2151771 -4.2305913 -4.2272878 -4.2102513][-4.1358848 -4.0816631 -4.0882268 -4.1356621 -4.1856585 -4.2003117 -4.1899223 -4.1800041 -4.1799817 -4.1947908 -4.2172022 -4.2363367 -4.2415667 -4.22919 -4.20915][-4.1471148 -4.0971141 -4.0999765 -4.1380954 -4.1814919 -4.1983805 -4.1984348 -4.1997948 -4.2083492 -4.2228804 -4.2384477 -4.2472315 -4.2431297 -4.2252064 -4.2061524][-4.1752443 -4.1295595 -4.1248636 -4.1496277 -4.1828675 -4.2005181 -4.2114749 -4.2233357 -4.2337341 -4.242609 -4.2481475 -4.2476549 -4.2382331 -4.2233381 -4.2121005][-4.1966987 -4.1523895 -4.1404495 -4.157063 -4.1877785 -4.2099853 -4.2275414 -4.2412419 -4.2476287 -4.2492456 -4.2484818 -4.2444096 -4.2346458 -4.2247133 -4.2206717]]...]
INFO - root - 2017-12-07 20:33:03.299902: step 50910, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 56h:00m:24s remains)
INFO - root - 2017-12-07 20:33:10.051464: step 50920, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 50h:46m:19s remains)
INFO - root - 2017-12-07 20:33:16.893551: step 50930, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 49h:12m:30s remains)
INFO - root - 2017-12-07 20:33:23.735119: step 50940, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 49h:07m:00s remains)
INFO - root - 2017-12-07 20:33:30.542429: step 50950, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 56h:11m:37s remains)
INFO - root - 2017-12-07 20:33:37.402579: step 50960, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.714 sec/batch; 55h:50m:05s remains)
INFO - root - 2017-12-07 20:33:44.238512: step 50970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 52h:41m:50s remains)
INFO - root - 2017-12-07 20:33:51.039088: step 50980, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 49h:18m:15s remains)
INFO - root - 2017-12-07 20:33:57.781810: step 50990, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.623 sec/batch; 48h:40m:50s remains)
INFO - root - 2017-12-07 20:34:04.523560: step 51000, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.738 sec/batch; 57h:44m:11s remains)
2017-12-07 20:34:05.242667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2227144 -4.1989536 -4.1801734 -4.175673 -4.1862693 -4.2026515 -4.2144647 -4.2042007 -4.1913619 -4.1903334 -4.1980925 -4.2132626 -4.2223353 -4.2184734 -4.2081556][-4.2166405 -4.2002287 -4.1847358 -4.184269 -4.1964931 -4.2104959 -4.2162676 -4.19727 -4.1802306 -4.1803517 -4.1889229 -4.2031884 -4.2114363 -4.2048154 -4.18853][-4.2032914 -4.1931686 -4.1822348 -4.1879106 -4.2063508 -4.2189374 -4.218102 -4.1917958 -4.1731143 -4.1732144 -4.1804948 -4.192822 -4.199069 -4.1908431 -4.1733537][-4.1732392 -4.1651249 -4.1569214 -4.1690536 -4.1975503 -4.2143936 -4.2137341 -4.1871896 -4.1672883 -4.1600633 -4.1567125 -4.1670113 -4.1751151 -4.1717386 -4.1612959][-4.127459 -4.1236315 -4.1198807 -4.1364412 -4.1739221 -4.1923 -4.1868544 -4.1579423 -4.1419797 -4.1308169 -4.1185036 -4.1279249 -4.1344132 -4.1384749 -4.1380067][-4.0580258 -4.0613565 -4.0656395 -4.0894747 -4.1284628 -4.1414022 -4.1219807 -4.0900826 -4.0852389 -4.0794024 -4.0664439 -4.0742378 -4.0753574 -4.0789742 -4.0861187][-3.9845905 -3.9952595 -4.0089145 -4.0366807 -4.0680461 -4.0611119 -4.0216131 -3.9901931 -4.0049481 -4.0164733 -4.0059428 -4.0076704 -4.0044255 -4.0031223 -4.0148234][-3.9347451 -3.939821 -3.9536474 -3.9795661 -3.999886 -3.9737129 -3.9203529 -3.8950641 -3.932555 -3.9663467 -3.9605088 -3.9480746 -3.9405596 -3.9401865 -3.9588926][-3.9540021 -3.9460943 -3.9490182 -3.968498 -3.9802971 -3.9477684 -3.8961949 -3.8823564 -3.9322519 -3.9726789 -3.9679852 -3.9449944 -3.9337058 -3.9346631 -3.9599183][-4.0222497 -4.0085855 -4.00799 -4.0256653 -4.0329804 -4.0037794 -3.9612789 -3.9533238 -4.0000525 -4.0357337 -4.0296526 -4.0111418 -3.997452 -3.9946983 -4.0178957][-4.1041579 -4.0900016 -4.0923223 -4.1106696 -4.1145511 -4.0911427 -4.0546427 -4.04751 -4.084466 -4.1157136 -4.1100111 -4.0972104 -4.0850635 -4.0803657 -4.0971484][-4.1870136 -4.1773262 -4.181777 -4.1961079 -4.1972246 -4.1789737 -4.1509371 -4.1446786 -4.171793 -4.1971779 -4.1936545 -4.1826677 -4.1727834 -4.1679034 -4.1776915][-4.2472076 -4.2424984 -4.2446647 -4.2543411 -4.2554288 -4.2420807 -4.2245669 -4.2204075 -4.2399626 -4.2574553 -4.2569127 -4.250958 -4.2424269 -4.2388892 -4.2431369][-4.2765059 -4.2748532 -4.2749963 -4.2803559 -4.2836576 -4.2772465 -4.2685418 -4.2680492 -4.281857 -4.2939196 -4.2938876 -4.2897167 -4.2836037 -4.2811742 -4.2835751][-4.2823009 -4.2830243 -4.2844868 -4.2889514 -4.2933488 -4.2924809 -4.2909961 -4.2940764 -4.3052545 -4.3119578 -4.3115807 -4.3090758 -4.3047972 -4.3019605 -4.3032479]]...]
INFO - root - 2017-12-07 20:34:12.066922: step 51010, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 49h:24m:32s remains)
INFO - root - 2017-12-07 20:34:18.878179: step 51020, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.627 sec/batch; 48h:59m:13s remains)
INFO - root - 2017-12-07 20:34:25.696385: step 51030, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 53h:11m:33s remains)
INFO - root - 2017-12-07 20:34:32.500466: step 51040, loss = 2.03, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 55h:01m:26s remains)
INFO - root - 2017-12-07 20:34:39.305783: step 51050, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.673 sec/batch; 52h:36m:44s remains)
INFO - root - 2017-12-07 20:34:46.106705: step 51060, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 51h:46m:35s remains)
INFO - root - 2017-12-07 20:34:52.981012: step 51070, loss = 2.09, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 53h:59m:07s remains)
INFO - root - 2017-12-07 20:34:59.750879: step 51080, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 53h:53m:39s remains)
INFO - root - 2017-12-07 20:35:06.524678: step 51090, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 56h:07m:36s remains)
INFO - root - 2017-12-07 20:35:12.719428: step 51100, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 53h:38m:01s remains)
2017-12-07 20:35:13.446762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2412171 -4.2004089 -4.1423407 -4.0887814 -4.0840359 -4.1290941 -4.1783814 -4.1932397 -4.1903033 -4.169498 -4.1592216 -4.1725693 -4.2040048 -4.2218261 -4.1987958][-4.2349749 -4.208982 -4.174602 -4.1413622 -4.1451139 -4.17773 -4.2091026 -4.212234 -4.2076011 -4.1940703 -4.1861691 -4.1958356 -4.2173529 -4.2258539 -4.1987138][-4.200397 -4.178009 -4.1639562 -4.1539536 -4.1662674 -4.1881676 -4.2028594 -4.1996164 -4.1966352 -4.1960125 -4.1971531 -4.208077 -4.2251039 -4.2297306 -4.205461][-4.1431561 -4.1194792 -4.1287537 -4.1460176 -4.1651535 -4.171638 -4.1640329 -4.1497941 -4.1482072 -4.1644216 -4.1833372 -4.2013273 -4.2188458 -4.2257891 -4.2117791][-4.0974631 -4.075592 -4.1041908 -4.1406765 -4.1566558 -4.1398749 -4.1010809 -4.0697231 -4.0702519 -4.10721 -4.1520042 -4.1830778 -4.2041306 -4.2151241 -4.2102122][-4.1052237 -4.0925064 -4.1243277 -4.15968 -4.1583571 -4.1033 -4.0163407 -3.9577889 -3.9663591 -4.0323758 -4.106391 -4.1541338 -4.1805067 -4.1949844 -4.1935534][-4.1629491 -4.1602187 -4.1777935 -4.1932087 -4.1681447 -4.0788774 -3.9470625 -3.8629735 -3.8834445 -3.9756057 -4.0670547 -4.1207137 -4.1469054 -4.1612644 -4.1611733][-4.2299867 -4.2370415 -4.237299 -4.2302246 -4.1906476 -4.098299 -3.9707954 -3.8867822 -3.9055021 -3.98844 -4.0613885 -4.0989752 -4.1136875 -4.1273823 -4.1294627][-4.2847009 -4.3016257 -4.2960429 -4.2757168 -4.2328572 -4.15667 -4.0597315 -3.9961658 -4.0003572 -4.0455093 -4.0786347 -4.092474 -4.0956 -4.1081896 -4.1114116][-4.3107677 -4.3341532 -4.3281488 -4.3038945 -4.264606 -4.2046165 -4.1345749 -4.0907211 -4.0873103 -4.0988445 -4.0985947 -4.0954776 -4.0968895 -4.1085835 -4.1106534][-4.3179388 -4.3395481 -4.3326969 -4.3087516 -4.2732906 -4.2233887 -4.1688814 -4.1354671 -4.1326981 -4.137311 -4.1303878 -4.1244926 -4.126555 -4.1376896 -4.1349049][-4.3050213 -4.319499 -4.3143406 -4.2949562 -4.2643547 -4.2207317 -4.1751394 -4.1462069 -4.14734 -4.1617813 -4.1674881 -4.1654043 -4.1642647 -4.1695495 -4.1605997][-4.2748876 -4.2837152 -4.2845597 -4.2781134 -4.2588062 -4.2229114 -4.184639 -4.1602721 -4.1623769 -4.180316 -4.1956387 -4.1953545 -4.1868572 -4.1811094 -4.1674685][-4.2499557 -4.2529235 -4.2594066 -4.2632403 -4.2556443 -4.2324319 -4.2070518 -4.191267 -4.1937675 -4.2091861 -4.2229514 -4.22105 -4.2060533 -4.1908956 -4.1706109][-4.2610021 -4.259831 -4.2671285 -4.2737889 -4.2716188 -4.259953 -4.2481942 -4.2411141 -4.2430573 -4.2516212 -4.2589421 -4.2560463 -4.2428389 -4.2255154 -4.2009754]]...]
INFO - root - 2017-12-07 20:35:20.214253: step 51110, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 57h:11m:25s remains)
INFO - root - 2017-12-07 20:35:27.068940: step 51120, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.747 sec/batch; 58h:24m:24s remains)
INFO - root - 2017-12-07 20:35:33.829271: step 51130, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 51h:33m:43s remains)
INFO - root - 2017-12-07 20:35:40.517602: step 51140, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 49h:45m:58s remains)
INFO - root - 2017-12-07 20:35:47.408590: step 51150, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 54h:55m:02s remains)
INFO - root - 2017-12-07 20:35:54.300668: step 51160, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 57h:08m:22s remains)
INFO - root - 2017-12-07 20:36:00.976327: step 51170, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 53h:10m:04s remains)
INFO - root - 2017-12-07 20:36:07.740004: step 51180, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.703 sec/batch; 54h:54m:14s remains)
INFO - root - 2017-12-07 20:36:14.376931: step 51190, loss = 2.05, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 48h:31m:44s remains)
INFO - root - 2017-12-07 20:36:20.976887: step 51200, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 51h:32m:53s remains)
2017-12-07 20:36:21.691808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516651 -4.2856264 -4.3030267 -4.2937536 -4.2683744 -4.2476082 -4.228065 -4.21829 -4.2220187 -4.2270942 -4.2433434 -4.2449841 -4.2211919 -4.1986513 -4.1966295][-4.2782922 -4.3060131 -4.3100972 -4.2877994 -4.2568903 -4.2301974 -4.2112365 -4.2006149 -4.2088733 -4.2237511 -4.245266 -4.2506905 -4.2230334 -4.1968021 -4.1953254][-4.3059163 -4.3165326 -4.3017478 -4.269278 -4.232986 -4.2010546 -4.1868544 -4.18258 -4.20063 -4.2275066 -4.2572842 -4.2661166 -4.240046 -4.2104778 -4.203527][-4.3218575 -4.3174815 -4.2836885 -4.2386537 -4.2010717 -4.1704073 -4.1689177 -4.1796632 -4.2079449 -4.2429562 -4.2736459 -4.2871346 -4.2723575 -4.2444162 -4.22909][-4.3233023 -4.3092957 -4.2653475 -4.2134542 -4.1783376 -4.1529608 -4.1654572 -4.1870155 -4.2212467 -4.2553358 -4.2807641 -4.2964635 -4.294188 -4.2776775 -4.2604208][-4.305553 -4.2862024 -4.2421713 -4.1893177 -4.157999 -4.1415973 -4.1605182 -4.1798692 -4.2034883 -4.2324457 -4.2567062 -4.2789431 -4.290956 -4.2909551 -4.2814374][-4.2653069 -4.2423244 -4.202714 -4.1582003 -4.1325436 -4.1217813 -4.1394038 -4.1464443 -4.1499481 -4.1663756 -4.1904087 -4.2246628 -4.25607 -4.2754979 -4.2818222][-4.195581 -4.1546278 -4.1118493 -4.0792704 -4.0734324 -4.0759864 -4.0882416 -4.0779004 -4.0531311 -4.0530872 -4.0823841 -4.1383896 -4.1960406 -4.238729 -4.2644258][-4.0976868 -4.0290866 -3.97565 -3.9572606 -3.9809976 -4.0055032 -4.01287 -3.9900033 -3.9443815 -3.9341879 -3.9799428 -4.0609717 -4.1456194 -4.2111392 -4.2532167][-4.0120659 -3.9303284 -3.8871322 -3.8914609 -3.9349537 -3.9655771 -3.959044 -3.9298534 -3.8844993 -3.8857625 -3.9436667 -4.033319 -4.1286726 -4.208178 -4.2599425][-3.9890447 -3.9298797 -3.9157393 -3.9353094 -3.9787896 -4.0003471 -3.9775333 -3.9476306 -3.9231024 -3.9402778 -3.9937174 -4.0693736 -4.15641 -4.2317324 -4.281261][-4.0426412 -4.0140491 -4.0184851 -4.0437908 -4.0811167 -4.0959806 -4.069593 -4.0460835 -4.03511 -4.0541692 -4.0932727 -4.1486721 -4.2165079 -4.2745161 -4.3115039][-4.1334395 -4.1229568 -4.1365952 -4.1606197 -4.1894193 -4.199738 -4.1789317 -4.1642575 -4.1616106 -4.1767764 -4.20535 -4.243885 -4.2882991 -4.3223763 -4.3437581][-4.2225575 -4.2221732 -4.2378531 -4.2562 -4.2762733 -4.2839794 -4.2698584 -4.260572 -4.2607765 -4.2717338 -4.2931538 -4.3191161 -4.3446693 -4.3607883 -4.3693194][-4.2922063 -4.2962909 -4.3073854 -4.3175879 -4.32842 -4.3353152 -4.33014 -4.324492 -4.3241444 -4.3299212 -4.3419905 -4.3544741 -4.3677187 -4.3745708 -4.3761439]]...]
INFO - root - 2017-12-07 20:36:28.437950: step 51210, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 52h:57m:37s remains)
INFO - root - 2017-12-07 20:36:35.145353: step 51220, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 48h:29m:13s remains)
INFO - root - 2017-12-07 20:36:41.959781: step 51230, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 52h:06m:42s remains)
INFO - root - 2017-12-07 20:36:48.747886: step 51240, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 54h:43m:05s remains)
INFO - root - 2017-12-07 20:36:55.492767: step 51250, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.725 sec/batch; 56h:40m:15s remains)
INFO - root - 2017-12-07 20:37:02.278454: step 51260, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 55h:40m:57s remains)
INFO - root - 2017-12-07 20:37:09.102776: step 51270, loss = 2.03, batch loss = 1.97 (12.7 examples/sec; 0.632 sec/batch; 49h:23m:34s remains)
INFO - root - 2017-12-07 20:37:15.991051: step 51280, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 51h:21m:09s remains)
INFO - root - 2017-12-07 20:37:22.791555: step 51290, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 54h:46m:55s remains)
INFO - root - 2017-12-07 20:37:29.447058: step 51300, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 53h:17m:20s remains)
2017-12-07 20:37:30.085110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2814231 -4.2658758 -4.2446871 -4.2153783 -4.18891 -4.1622548 -4.1430645 -4.1300812 -4.1310978 -4.1224132 -4.1001754 -4.0849504 -4.0793247 -4.0841169 -4.1038461][-4.2609515 -4.2392092 -4.2190256 -4.1960473 -4.17763 -4.1655493 -4.1579585 -4.1561022 -4.1653142 -4.1597695 -4.1417084 -4.1202493 -4.0936956 -4.0769291 -4.0862465][-4.2349973 -4.2143373 -4.1982465 -4.1813765 -4.1683273 -4.1661606 -4.166048 -4.1644149 -4.1743059 -4.17604 -4.1701474 -4.1545687 -4.1242571 -4.0972586 -4.0989761][-4.211916 -4.2019711 -4.1976271 -4.1852064 -4.1716776 -4.1663194 -4.1577067 -4.1474614 -4.1579566 -4.1746335 -4.1859436 -4.1800914 -4.1544275 -4.1271453 -4.1214395][-4.2118311 -4.2080541 -4.2120066 -4.2004704 -4.1817837 -4.1655545 -4.1370358 -4.10628 -4.1113753 -4.1450067 -4.17731 -4.1875567 -4.172852 -4.1469293 -4.1323609][-4.2203012 -4.2225628 -4.2265005 -4.2064757 -4.1792827 -4.1493926 -4.097239 -4.0324793 -4.0215888 -4.0766373 -4.1402063 -4.1724257 -4.1735158 -4.161974 -4.1472249][-4.2284575 -4.2358203 -4.2328572 -4.20201 -4.1608839 -4.1127672 -4.0348411 -3.9304349 -3.8956532 -3.9805913 -4.0843821 -4.1405969 -4.1608105 -4.1695538 -4.1618466][-4.2410049 -4.245575 -4.2284937 -4.1819639 -4.1324234 -4.0711284 -3.97754 -3.8488121 -3.7977204 -3.9126244 -4.0438404 -4.1197071 -4.1550751 -4.1727562 -4.1650729][-4.250989 -4.2480183 -4.2176228 -4.1624546 -4.1148238 -4.06098 -3.9841805 -3.8814156 -3.8483591 -3.9461205 -4.0593877 -4.1255608 -4.1519423 -4.1572509 -4.1441669][-4.24505 -4.2304215 -4.1919456 -4.1423016 -4.1093326 -4.08189 -4.043344 -3.9897373 -3.980988 -4.0416603 -4.1108227 -4.1452579 -4.1471009 -4.1293392 -4.1079283][-4.2146091 -4.1843114 -4.1436853 -4.1085424 -4.1000161 -4.1042347 -4.1038828 -4.090692 -4.0968704 -4.127914 -4.1594834 -4.159431 -4.13878 -4.108871 -4.0825133][-4.1901159 -4.1496477 -4.1129484 -4.0945945 -4.1022706 -4.1286092 -4.1535234 -4.165978 -4.1753492 -4.1846476 -4.188849 -4.1677032 -4.1401811 -4.1062193 -4.0760741][-4.1950049 -4.1615262 -4.1344457 -4.1249022 -4.1342134 -4.1663618 -4.1962924 -4.2115583 -4.2121234 -4.2085023 -4.1965632 -4.1732244 -4.154006 -4.1239557 -4.09046][-4.2201881 -4.2006 -4.1861758 -4.1785746 -4.1791172 -4.1972661 -4.2164588 -4.2241516 -4.2162814 -4.2033124 -4.1846046 -4.1675768 -4.1616282 -4.1417861 -4.1092625][-4.2468977 -4.2374482 -4.2301621 -4.2213736 -4.2089047 -4.2054057 -4.2067647 -4.2069907 -4.1998482 -4.1822076 -4.1618624 -4.1521354 -4.1590285 -4.1497717 -4.1296015]]...]
INFO - root - 2017-12-07 20:37:36.907814: step 51310, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 51h:05m:06s remains)
INFO - root - 2017-12-07 20:37:43.797530: step 51320, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 56h:40m:08s remains)
INFO - root - 2017-12-07 20:37:50.564502: step 51330, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 51h:49m:19s remains)
INFO - root - 2017-12-07 20:37:57.263164: step 51340, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 49h:45m:27s remains)
INFO - root - 2017-12-07 20:38:04.105762: step 51350, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 51h:41m:58s remains)
INFO - root - 2017-12-07 20:38:10.903844: step 51360, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 55h:36m:28s remains)
INFO - root - 2017-12-07 20:38:17.694243: step 51370, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 53h:50m:12s remains)
INFO - root - 2017-12-07 20:38:24.524512: step 51380, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 54h:16m:07s remains)
INFO - root - 2017-12-07 20:38:31.211839: step 51390, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.645 sec/batch; 50h:23m:22s remains)
INFO - root - 2017-12-07 20:38:37.882535: step 51400, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 50h:17m:06s remains)
2017-12-07 20:38:38.728027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3297377 -4.3248644 -4.3229513 -4.3198628 -4.3185139 -4.3091211 -4.298172 -4.294879 -4.2953291 -4.2941155 -4.2950845 -4.2958913 -4.2951283 -4.2937031 -4.290421][-4.3229613 -4.3157206 -4.310411 -4.3011389 -4.2888384 -4.2684064 -4.2505422 -4.2464256 -4.2493658 -4.2551374 -4.2643304 -4.2696204 -4.2683587 -4.2656875 -4.2590227][-4.3146763 -4.3092027 -4.3041277 -4.2855673 -4.256743 -4.221364 -4.1942897 -4.1903143 -4.2045765 -4.224143 -4.244658 -4.2550058 -4.252677 -4.2450047 -4.2253528][-4.315866 -4.3128967 -4.3076839 -4.2803125 -4.2372651 -4.1842709 -4.1449533 -4.143074 -4.1762557 -4.2164454 -4.2466078 -4.2642 -4.2617021 -4.2399898 -4.1980133][-4.3141656 -4.3140039 -4.308506 -4.2766151 -4.2207565 -4.1496058 -4.0936069 -4.090662 -4.1427226 -4.2020926 -4.2393651 -4.2596006 -4.2550788 -4.2261558 -4.167398][-4.3099055 -4.3088746 -4.2958808 -4.258112 -4.1942258 -4.1083813 -4.0309072 -4.0166707 -4.082891 -4.1661348 -4.2179432 -4.243752 -4.2386427 -4.2089705 -4.146615][-4.3052235 -4.2992568 -4.27623 -4.2312851 -4.1625977 -4.0668631 -3.9667788 -3.9323213 -4.0059443 -4.1202579 -4.1954107 -4.2329006 -4.2382026 -4.2220755 -4.1714182][-4.2907028 -4.2766995 -4.2440047 -4.193872 -4.1307817 -4.0398479 -3.9265265 -3.8645477 -3.9420648 -4.0814719 -4.1770129 -4.2251539 -4.2371736 -4.2318134 -4.1995459][-4.2602372 -4.23841 -4.2049623 -4.1667223 -4.1300597 -4.0696516 -3.9825244 -3.9212198 -3.9740078 -4.0847492 -4.1693287 -4.2140875 -4.2294326 -4.2324076 -4.2148285][-4.2341485 -4.2096834 -4.1826692 -4.1623573 -4.1518345 -4.1237478 -4.0686717 -4.0275083 -4.0580115 -4.1273685 -4.1878448 -4.2213216 -4.2375736 -4.2418165 -4.2287765][-4.2282977 -4.2046862 -4.1816907 -4.1729283 -4.1748891 -4.1654973 -4.1349454 -4.1102505 -4.1279297 -4.1732812 -4.2136869 -4.2352033 -4.2479439 -4.2533264 -4.2437429][-4.2282124 -4.2058272 -4.1873584 -4.1864572 -4.1929297 -4.1956987 -4.1825352 -4.1639638 -4.17394 -4.2095551 -4.2404628 -4.256918 -4.2659173 -4.2677507 -4.2575722][-4.2314744 -4.2133923 -4.1987381 -4.1989994 -4.206882 -4.220912 -4.2249331 -4.2154803 -4.2222934 -4.2473907 -4.2719665 -4.2831678 -4.2891307 -4.2889 -4.2790003][-4.2487731 -4.2386217 -4.2279391 -4.2273674 -4.2365961 -4.2551451 -4.2680879 -4.2659616 -4.2667975 -4.2788796 -4.2956715 -4.3030839 -4.3077164 -4.3081756 -4.3004][-4.2759681 -4.2713513 -4.2674184 -4.2674751 -4.2736793 -4.2882566 -4.3009439 -4.3022394 -4.3009372 -4.3048468 -4.3134966 -4.319912 -4.3236833 -4.3243494 -4.3178172]]...]
INFO - root - 2017-12-07 20:38:45.274028: step 51410, loss = 2.07, batch loss = 2.01 (13.5 examples/sec; 0.592 sec/batch; 46h:14m:06s remains)
INFO - root - 2017-12-07 20:38:52.156136: step 51420, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 52h:31m:47s remains)
INFO - root - 2017-12-07 20:38:58.965437: step 51430, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 55h:40m:13s remains)
INFO - root - 2017-12-07 20:39:05.685625: step 51440, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 56h:28m:53s remains)
INFO - root - 2017-12-07 20:39:12.555934: step 51450, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 52h:39m:00s remains)
INFO - root - 2017-12-07 20:39:19.274323: step 51460, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 50h:42m:34s remains)
INFO - root - 2017-12-07 20:39:26.065024: step 51470, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 53h:06m:03s remains)
INFO - root - 2017-12-07 20:39:32.947871: step 51480, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 58h:01m:14s remains)
INFO - root - 2017-12-07 20:39:39.819586: step 51490, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 53h:43m:15s remains)
INFO - root - 2017-12-07 20:39:46.427037: step 51500, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 50h:14m:45s remains)
2017-12-07 20:39:47.181106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16797 -4.1689558 -4.1679368 -4.1548967 -4.154357 -4.188972 -4.2292304 -4.2561955 -4.2571573 -4.2515335 -4.2549367 -4.268085 -4.2598553 -4.2284455 -4.1894937][-4.1324253 -4.1479392 -4.1539011 -4.13894 -4.1394625 -4.1784048 -4.2187166 -4.242363 -4.2476897 -4.254848 -4.2672324 -4.2795434 -4.2683539 -4.2337909 -4.1960273][-4.1107197 -4.1279407 -4.1327987 -4.1148071 -4.11677 -4.1623912 -4.2073631 -4.2327161 -4.2453074 -4.2613306 -4.2824593 -4.296195 -4.2900114 -4.2674832 -4.24266][-4.1172218 -4.1100607 -4.091136 -4.0641994 -4.0690002 -4.1245775 -4.1833415 -4.2239017 -4.2498932 -4.2741437 -4.2988644 -4.3130054 -4.3098474 -4.2977424 -4.2860975][-4.1457872 -4.0995955 -4.0411553 -3.9958558 -4.0030231 -4.0681429 -4.1456809 -4.204246 -4.2388897 -4.2659431 -4.2927928 -4.3107758 -4.30756 -4.2979813 -4.2905354][-4.1829958 -4.103972 -4.0127225 -3.9519854 -3.9543779 -4.0167227 -4.0993934 -4.1657286 -4.2049131 -4.2336388 -4.2623267 -4.2834754 -4.2836804 -4.2772331 -4.2702169][-4.2121215 -4.1242738 -4.0272183 -3.9692063 -3.9667025 -4.0112891 -4.0765557 -4.1304941 -4.16493 -4.1935945 -4.221271 -4.243372 -4.2473478 -4.2449164 -4.2419162][-4.2313533 -4.1603308 -4.0889544 -4.0481892 -4.04219 -4.057961 -4.0864987 -4.107049 -4.1210585 -4.1431961 -4.1748343 -4.20321 -4.2118483 -4.2093577 -4.2076955][-4.2506509 -4.2084417 -4.1707239 -4.150156 -4.1427407 -4.1359043 -4.1284809 -4.1154737 -4.1056156 -4.1169052 -4.1502733 -4.1836996 -4.1951613 -4.1926351 -4.19075][-4.2593141 -4.2436275 -4.2317176 -4.2291036 -4.2273769 -4.2130003 -4.1886315 -4.1627536 -4.1442471 -4.1448183 -4.1650538 -4.1872358 -4.1942506 -4.1936841 -4.1974554][-4.2543778 -4.2569695 -4.2616997 -4.269443 -4.27254 -4.2615438 -4.2440677 -4.2271161 -4.2130771 -4.2112551 -4.2136192 -4.2173972 -4.2160983 -4.216 -4.2247319][-4.2525721 -4.2608976 -4.2676935 -4.2691336 -4.2669048 -4.2582269 -4.2516084 -4.2516084 -4.2542362 -4.2611609 -4.2637262 -4.26236 -4.2612162 -4.2623539 -4.26842][-4.246882 -4.2455945 -4.2416916 -4.234169 -4.2292037 -4.2245722 -4.2236581 -4.2329021 -4.2484264 -4.2621174 -4.2738419 -4.2825294 -4.2907214 -4.2986045 -4.3068471][-4.2311611 -4.2180448 -4.20386 -4.1892438 -4.1840878 -4.1861854 -4.1924191 -4.2034183 -4.2157755 -4.2304878 -4.2513075 -4.2729378 -4.2918496 -4.3072715 -4.3193507][-4.2164445 -4.1963272 -4.1754856 -4.1587067 -4.1610985 -4.1783395 -4.1964316 -4.20496 -4.2048526 -4.2128096 -4.2388577 -4.265626 -4.287683 -4.3047891 -4.3177958]]...]
INFO - root - 2017-12-07 20:39:54.018931: step 51510, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.759 sec/batch; 59h:12m:51s remains)
INFO - root - 2017-12-07 20:40:00.700166: step 51520, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.652 sec/batch; 50h:53m:53s remains)
INFO - root - 2017-12-07 20:40:07.533134: step 51530, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 55h:17m:12s remains)
INFO - root - 2017-12-07 20:40:14.296126: step 51540, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.625 sec/batch; 48h:46m:06s remains)
INFO - root - 2017-12-07 20:40:21.197841: step 51550, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 57h:53m:23s remains)
INFO - root - 2017-12-07 20:40:28.070510: step 51560, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 56h:15m:10s remains)
INFO - root - 2017-12-07 20:40:34.814496: step 51570, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 54h:04m:16s remains)
INFO - root - 2017-12-07 20:40:41.664461: step 51580, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 49h:17m:29s remains)
INFO - root - 2017-12-07 20:40:48.507480: step 51590, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 52h:04m:25s remains)
INFO - root - 2017-12-07 20:40:55.084786: step 51600, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.724 sec/batch; 56h:28m:41s remains)
2017-12-07 20:40:55.797033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.277566 -4.3022609 -4.3079286 -4.2982078 -4.2829022 -4.2683215 -4.2535248 -4.2403731 -4.236567 -4.2365727 -4.2318664 -4.221478 -4.2148185 -4.2138958 -4.2147231][-4.2824745 -4.31163 -4.3213449 -4.3110976 -4.2953954 -4.2803783 -4.2662725 -4.2568359 -4.261085 -4.2682786 -4.2604766 -4.2390728 -4.221282 -4.2157855 -4.2131972][-4.2642775 -4.2977743 -4.3120375 -4.3038225 -4.29274 -4.2765689 -4.2570858 -4.246902 -4.2584329 -4.2733765 -4.2661 -4.2344112 -4.2063222 -4.1975451 -4.1904292][-4.2472291 -4.2805638 -4.2937126 -4.2886009 -4.2826757 -4.2638597 -4.2314124 -4.2121391 -4.2244644 -4.247736 -4.2489867 -4.2192054 -4.18958 -4.1763368 -4.1603689][-4.2481771 -4.2732277 -4.2819328 -4.2789955 -4.2743564 -4.2510991 -4.2040658 -4.1675377 -4.1734195 -4.2061024 -4.2274346 -4.2173214 -4.1968622 -4.1798525 -4.1532793][-4.2589426 -4.273232 -4.2741055 -4.2676387 -4.2579942 -4.2230296 -4.1584558 -4.0986462 -4.0985384 -4.1487904 -4.1980643 -4.214282 -4.211103 -4.1947527 -4.1622143][-4.2704916 -4.2766843 -4.270505 -4.2561307 -4.2347093 -4.1824689 -4.09579 -3.9970031 -3.9802334 -4.0591559 -4.1445065 -4.1875243 -4.2027559 -4.1958227 -4.1691971][-4.2794209 -4.2832913 -4.2747717 -4.2495136 -4.2092977 -4.14 -4.028657 -3.8848233 -3.8320682 -3.9459205 -4.0764861 -4.1493907 -4.1811323 -4.1901627 -4.1778946][-4.2870374 -4.2909365 -4.2841325 -4.255331 -4.2020721 -4.1173258 -3.9925272 -3.825969 -3.7370162 -3.865761 -4.0258632 -4.11975 -4.1639953 -4.1893 -4.1950383][-4.2913032 -4.2950144 -4.2952805 -4.2781448 -4.2305145 -4.1458693 -4.0320163 -3.8912959 -3.8035624 -3.880229 -4.0184517 -4.113646 -4.1636825 -4.1948204 -4.211525][-4.2794771 -4.2818418 -4.28872 -4.29039 -4.2639403 -4.1990023 -4.1132922 -4.0134 -3.9407542 -3.9580443 -4.0435596 -4.1259942 -4.1766944 -4.2068458 -4.2209358][-4.2589679 -4.2581964 -4.2676635 -4.2821884 -4.277422 -4.239203 -4.1847849 -4.1190639 -4.0646076 -4.0535731 -4.093297 -4.1506104 -4.1920729 -4.2179546 -4.2265573][-4.2235885 -4.216639 -4.2277565 -4.2530742 -4.2684226 -4.2566347 -4.232511 -4.1962614 -4.1586132 -4.1437411 -4.1588693 -4.1881828 -4.2121916 -4.2290626 -4.2320118][-4.1776567 -4.1655331 -4.1812973 -4.2208519 -4.2540669 -4.2637515 -4.2597561 -4.2455707 -4.2251534 -4.2182865 -4.2244086 -4.2319007 -4.23728 -4.2411394 -4.2389135][-4.1349421 -4.124404 -4.1494441 -4.2001505 -4.2435694 -4.2673206 -4.2737584 -4.2707272 -4.2640958 -4.26523 -4.2714853 -4.2720942 -4.2665496 -4.2600608 -4.2530947]]...]
INFO - root - 2017-12-07 20:41:02.540701: step 51610, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.613 sec/batch; 47h:49m:08s remains)
INFO - root - 2017-12-07 20:41:09.346321: step 51620, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 52h:34m:57s remains)
INFO - root - 2017-12-07 20:41:16.199842: step 51630, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.715 sec/batch; 55h:47m:29s remains)
INFO - root - 2017-12-07 20:41:23.001759: step 51640, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 57h:20m:14s remains)
INFO - root - 2017-12-07 20:41:29.903407: step 51650, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 55h:19m:06s remains)
INFO - root - 2017-12-07 20:41:36.677936: step 51660, loss = 2.05, batch loss = 1.99 (13.2 examples/sec; 0.608 sec/batch; 47h:27m:02s remains)
INFO - root - 2017-12-07 20:41:43.426938: step 51670, loss = 2.05, batch loss = 2.00 (13.1 examples/sec; 0.608 sec/batch; 47h:27m:58s remains)
INFO - root - 2017-12-07 20:41:50.241757: step 51680, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 55h:51m:44s remains)
INFO - root - 2017-12-07 20:41:57.074415: step 51690, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 56h:08m:49s remains)
INFO - root - 2017-12-07 20:42:03.731440: step 51700, loss = 2.03, batch loss = 1.97 (10.7 examples/sec; 0.749 sec/batch; 58h:27m:37s remains)
2017-12-07 20:42:04.451668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2265968 -4.2168441 -4.2188296 -4.2322445 -4.244185 -4.2598066 -4.2756524 -4.2854366 -4.2910929 -4.2898874 -4.28804 -4.2885017 -4.2896581 -4.2901597 -4.2871776][-4.1877737 -4.17463 -4.1815257 -4.2045875 -4.2234492 -4.2451143 -4.2674079 -4.2829309 -4.2920294 -4.2930379 -4.2952251 -4.2988653 -4.3028607 -4.3074465 -4.3070097][-4.1868658 -4.1749659 -4.1817904 -4.2076674 -4.2280345 -4.2457213 -4.2629628 -4.2755094 -4.2839355 -4.2889266 -4.2960234 -4.3018756 -4.3063345 -4.3116403 -4.311686][-4.2184644 -4.2109532 -4.2152486 -4.2323565 -4.240469 -4.243464 -4.2463288 -4.2469258 -4.2487864 -4.2556324 -4.2665586 -4.2773833 -4.2853551 -4.2936673 -4.2953725][-4.2540345 -4.244524 -4.2389774 -4.239562 -4.2311049 -4.2166176 -4.2064214 -4.1982493 -4.1946087 -4.2020707 -4.2151895 -4.2329569 -4.2499924 -4.2662983 -4.276186][-4.2611175 -4.24439 -4.2233515 -4.2050776 -4.1815081 -4.1530762 -4.1383314 -4.1349454 -4.1375866 -4.1462517 -4.1591907 -4.1809163 -4.2064738 -4.2344284 -4.2552772][-4.2328749 -4.2004504 -4.1593642 -4.1202793 -4.06976 -4.0137534 -3.9935291 -4.01015 -4.0392947 -4.063334 -4.07764 -4.1055851 -4.1457486 -4.1883416 -4.2219267][-4.1730523 -4.1223955 -4.0656552 -4.0098748 -3.9298832 -3.8297012 -3.7808485 -3.8153317 -3.877274 -3.9270384 -3.9560976 -3.9988685 -4.0631576 -4.1273694 -4.1768146][-4.1159468 -4.0583982 -3.9999471 -3.9464223 -3.8691797 -3.7580605 -3.6884341 -3.7210069 -3.7855299 -3.839335 -3.8727572 -3.9204559 -3.9944777 -4.0679731 -4.1298518][-4.1114335 -4.0687914 -4.0272064 -3.9974279 -3.9607894 -3.8980398 -3.84975 -3.855154 -3.8752189 -3.89085 -3.9012828 -3.9314077 -3.9844558 -4.04208 -4.09817][-4.1636543 -4.1452723 -4.131258 -4.1275735 -4.1257024 -4.1062307 -4.0832677 -4.0736113 -4.0613866 -4.0432143 -4.02812 -4.0285215 -4.0457339 -4.0746131 -4.1110525][-4.2329016 -4.2318959 -4.2345357 -4.2421994 -4.2480273 -4.2428179 -4.232945 -4.2255349 -4.2113428 -4.1904726 -4.1691966 -4.152925 -4.1447539 -4.14947 -4.1650686][-4.2832866 -4.286015 -4.2885771 -4.2951918 -4.2977695 -4.2918468 -4.2839694 -4.2805557 -4.2768259 -4.2712955 -4.2630424 -4.2506676 -4.2384219 -4.2335291 -4.2343273][-4.3033304 -4.3000307 -4.294322 -4.2940593 -4.2902112 -4.2782879 -4.2673092 -4.2679682 -4.2767677 -4.2866807 -4.2924757 -4.2931132 -4.2898364 -4.2841587 -4.2761917][-4.3004684 -4.2838044 -4.2640367 -4.2512116 -4.2404356 -4.2242622 -4.2091126 -4.2110167 -4.2304764 -4.25476 -4.2749596 -4.2890682 -4.2958674 -4.2919121 -4.2787147]]...]
INFO - root - 2017-12-07 20:42:11.268311: step 51710, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 56h:15m:35s remains)
INFO - root - 2017-12-07 20:42:17.651995: step 51720, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 52h:20m:00s remains)
INFO - root - 2017-12-07 20:42:24.421452: step 51730, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 49h:44m:30s remains)
INFO - root - 2017-12-07 20:42:31.075354: step 51740, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 50h:40m:55s remains)
INFO - root - 2017-12-07 20:42:37.758352: step 51750, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 49h:38m:08s remains)
INFO - root - 2017-12-07 20:42:44.603012: step 51760, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 56h:45m:13s remains)
INFO - root - 2017-12-07 20:42:51.367830: step 51770, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 55h:53m:05s remains)
INFO - root - 2017-12-07 20:42:58.134782: step 51780, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 52h:05m:06s remains)
INFO - root - 2017-12-07 20:43:04.871319: step 51790, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 50h:28m:22s remains)
INFO - root - 2017-12-07 20:43:11.515506: step 51800, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 55h:33m:44s remains)
2017-12-07 20:43:12.269748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1992068 -4.2136536 -4.2229314 -4.2163835 -4.206336 -4.2010913 -4.1975164 -4.1949744 -4.1897283 -4.1799273 -4.1706796 -4.168705 -4.1685476 -4.1695547 -4.1802607][-4.1972919 -4.2073245 -4.2137747 -4.2068806 -4.197803 -4.1913314 -4.1845355 -4.179605 -4.173202 -4.1641951 -4.1565142 -4.1572247 -4.1616654 -4.1673408 -4.1812105][-4.1940794 -4.1954842 -4.1941242 -4.1841607 -4.171114 -4.15749 -4.1446738 -4.1399913 -4.1405125 -4.1396327 -4.137814 -4.1430788 -4.1563058 -4.1697311 -4.1864018][-4.18822 -4.1824541 -4.1727138 -4.1564116 -4.1311622 -4.101665 -4.0800729 -4.0800371 -4.09487 -4.1056108 -4.1122718 -4.1238036 -4.1464744 -4.1694546 -4.1875887][-4.1755309 -4.1651335 -4.1507559 -4.1278491 -4.0841765 -4.0282941 -3.9952259 -4.0111294 -4.0492034 -4.0739107 -4.0895061 -4.1108203 -4.1405854 -4.1686306 -4.1854277][-4.1553526 -4.1412773 -4.1268764 -4.096849 -4.0259724 -3.9321523 -3.8839839 -3.929625 -4.0066776 -4.0570021 -4.0845757 -4.1092978 -4.1398907 -4.1651263 -4.177124][-4.1335983 -4.1194229 -4.1088276 -4.0748777 -3.9844644 -3.85323 -3.7848446 -3.8640113 -3.9774282 -4.0519428 -4.0933552 -4.1212025 -4.14443 -4.1611557 -4.1666856][-4.1043906 -4.0880704 -4.0859666 -4.0685234 -3.99966 -3.8892889 -3.8266041 -3.8939795 -4.0000443 -4.0734982 -4.1176033 -4.1478562 -4.1667161 -4.1770897 -4.1767321][-4.0676193 -4.0459495 -4.0536327 -4.068388 -4.0468836 -3.9951217 -3.962846 -3.9901235 -4.0471687 -4.0990486 -4.1402659 -4.1756678 -4.1992817 -4.2109222 -4.2095661][-4.0629845 -4.042872 -4.0539284 -4.0848875 -4.0963449 -4.0857606 -4.072228 -4.0683818 -4.0835156 -4.1135511 -4.1511188 -4.1889143 -4.216361 -4.2317724 -4.2335911][-4.104125 -4.0834937 -4.0849681 -4.1120787 -4.1354289 -4.1462674 -4.1430311 -4.1263027 -4.1219635 -4.1443706 -4.1801009 -4.2108455 -4.2314525 -4.2396908 -4.2375221][-4.1604123 -4.1351576 -4.1221447 -4.1320825 -4.1537132 -4.1759081 -4.1859322 -4.1772575 -4.1717777 -4.189291 -4.2188978 -4.2399263 -4.2465305 -4.2383952 -4.2260685][-4.1996055 -4.171833 -4.1492996 -4.1419549 -4.1542068 -4.1802921 -4.2049637 -4.2129383 -4.2151365 -4.2293243 -4.25035 -4.2630868 -4.2559576 -4.2328277 -4.21061][-4.2168345 -4.1876979 -4.1635933 -4.1490159 -4.1546822 -4.1822686 -4.2135596 -4.2335262 -4.2429295 -4.2556863 -4.2696714 -4.275197 -4.2613392 -4.2315583 -4.2039313][-4.2373395 -4.211565 -4.1878085 -4.1714654 -4.17448 -4.199832 -4.22669 -4.2433653 -4.2509112 -4.2597027 -4.265295 -4.2634487 -4.2511411 -4.2286286 -4.2049975]]...]
INFO - root - 2017-12-07 20:43:19.072707: step 51810, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 50h:59m:52s remains)
INFO - root - 2017-12-07 20:43:25.927750: step 51820, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 51h:14m:52s remains)
INFO - root - 2017-12-07 20:43:32.828445: step 51830, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 55h:54m:48s remains)
INFO - root - 2017-12-07 20:43:39.789563: step 51840, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 53h:07m:04s remains)
INFO - root - 2017-12-07 20:43:46.578491: step 51850, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 49h:52m:30s remains)
INFO - root - 2017-12-07 20:43:53.366238: step 51860, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.604 sec/batch; 47h:06m:23s remains)
INFO - root - 2017-12-07 20:44:00.216106: step 51870, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 57h:35m:01s remains)
INFO - root - 2017-12-07 20:44:07.108955: step 51880, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 55h:13m:47s remains)
INFO - root - 2017-12-07 20:44:13.867771: step 51890, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 53h:19m:16s remains)
INFO - root - 2017-12-07 20:44:20.426520: step 51900, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 51h:27m:24s remains)
2017-12-07 20:44:21.167702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.260406 -4.269908 -4.2810564 -4.2901525 -4.300663 -4.3008957 -4.2869134 -4.2658591 -4.2546382 -4.2466927 -4.240993 -4.2373147 -4.2325869 -4.2384243 -4.2518153][-4.2324133 -4.2405438 -4.2525368 -4.2631145 -4.2751894 -4.2761378 -4.2616458 -4.2398429 -4.2310123 -4.2255859 -4.2212214 -4.2195764 -4.2193422 -4.2303505 -4.2468405][-4.2165623 -4.2180753 -4.2229972 -4.229744 -4.2384119 -4.2391396 -4.2276731 -4.2102065 -4.2114296 -4.2171416 -4.2191133 -4.2204614 -4.2245774 -4.2381115 -4.253027][-4.214602 -4.2149487 -4.2187266 -4.2233019 -4.2301373 -4.2316265 -4.2200761 -4.2070718 -4.2190776 -4.2321062 -4.2338037 -4.2367954 -4.2418394 -4.2550459 -4.266715][-4.188355 -4.1859555 -4.1896963 -4.1959119 -4.2076488 -4.2120676 -4.1955667 -4.1781921 -4.1987314 -4.2188296 -4.2195745 -4.2244987 -4.2327871 -4.2478595 -4.2631125][-4.1219854 -4.1175003 -4.1228867 -4.1252351 -4.1336927 -4.1292491 -4.0929642 -4.0657349 -4.0956111 -4.1322474 -4.1483541 -4.1708031 -4.1974897 -4.226613 -4.2506523][-3.997334 -3.9913826 -3.9988046 -4.0002904 -4.0016661 -3.9859459 -3.931622 -3.9005044 -3.9464264 -4.0112314 -4.0585122 -4.1086187 -4.158514 -4.2052555 -4.2384958][-3.862994 -3.8536916 -3.8569274 -3.8531063 -3.844795 -3.8166037 -3.7529836 -3.7253163 -3.7967792 -3.8945055 -3.9769027 -4.0558577 -4.1281848 -4.1883945 -4.2288589][-3.9078963 -3.8924689 -3.8749328 -3.8422987 -3.8041992 -3.753211 -3.6798356 -3.6495819 -3.7302389 -3.8404424 -3.9358361 -4.0271792 -4.1113653 -4.1779675 -4.22206][-4.0439038 -4.0300746 -4.0050993 -3.9680684 -3.9307737 -3.8827558 -3.8236632 -3.7954946 -3.8498292 -3.9283173 -3.9933186 -4.06031 -4.1263309 -4.1807775 -4.2201719][-4.1546574 -4.1478257 -4.1271248 -4.0970812 -4.0679336 -4.0297914 -3.9874642 -3.9647512 -3.9941843 -4.0396152 -4.0725017 -4.1086092 -4.1522508 -4.1932492 -4.2243814][-4.2448559 -4.2416306 -4.2242312 -4.2035904 -4.187026 -4.160675 -4.1276655 -4.105123 -4.1134639 -4.1355114 -4.1464024 -4.1625543 -4.1879907 -4.2160792 -4.2396073][-4.2899737 -4.2900529 -4.2840352 -4.2772837 -4.2747355 -4.2615409 -4.2412581 -4.22532 -4.2241383 -4.23134 -4.2300344 -4.2340174 -4.245729 -4.2594662 -4.2712493][-4.3113828 -4.31284 -4.3138437 -4.3128409 -4.3126874 -4.3051295 -4.29341 -4.2850623 -4.2870865 -4.2940221 -4.2931395 -4.2965441 -4.2997441 -4.3036871 -4.3060813][-4.32005 -4.3207803 -4.3217449 -4.3211069 -4.3210764 -4.3164477 -4.3074384 -4.301558 -4.3076911 -4.3187575 -4.3250518 -4.3322573 -4.3337808 -4.3324881 -4.3290124]]...]
INFO - root - 2017-12-07 20:44:28.030677: step 51910, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 52h:45m:53s remains)
INFO - root - 2017-12-07 20:44:34.839870: step 51920, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 51h:48m:47s remains)
INFO - root - 2017-12-07 20:44:41.653081: step 51930, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 51h:22m:26s remains)
INFO - root - 2017-12-07 20:44:48.435521: step 51940, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 55h:58m:01s remains)
INFO - root - 2017-12-07 20:44:55.155052: step 51950, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 54h:54m:01s remains)
INFO - root - 2017-12-07 20:45:01.930145: step 51960, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 51h:52m:07s remains)
INFO - root - 2017-12-07 20:45:08.576300: step 51970, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 54h:32m:13s remains)
INFO - root - 2017-12-07 20:45:15.414969: step 51980, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 49h:29m:20s remains)
INFO - root - 2017-12-07 20:45:22.331572: step 51990, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 57h:32m:24s remains)
INFO - root - 2017-12-07 20:45:28.957172: step 52000, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 54h:03m:12s remains)
2017-12-07 20:45:29.703306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3215842 -4.326973 -4.331634 -4.3325 -4.3290486 -4.3219852 -4.3137531 -4.3047018 -4.2988386 -4.3001609 -4.3065147 -4.3162341 -4.3269768 -4.3350768 -4.3353019][-4.3204136 -4.3248496 -4.3269091 -4.3228674 -4.3131394 -4.2999983 -4.2875237 -4.2769914 -4.268878 -4.2681952 -4.2746673 -4.2899566 -4.30882 -4.3253627 -4.3287716][-4.3156538 -4.3177824 -4.3158455 -4.3042688 -4.2848625 -4.2633948 -4.2463894 -4.235157 -4.224937 -4.2239113 -4.2356968 -4.2612696 -4.2887325 -4.31134 -4.3188944][-4.3037219 -4.3026147 -4.296906 -4.2762995 -4.243309 -4.20855 -4.1808267 -4.1652083 -4.1523438 -4.1544452 -4.1793613 -4.2194428 -4.2586875 -4.2904835 -4.3045025][-4.2829466 -4.2786078 -4.2673287 -4.2361407 -4.1853256 -4.1284065 -4.0818892 -4.0528064 -4.0308423 -4.037384 -4.0856 -4.1523676 -4.2104187 -4.2575526 -4.2831182][-4.2646656 -4.2574105 -4.2419324 -4.2013168 -4.1327643 -4.05142 -3.9777086 -3.922987 -3.8799534 -3.8891385 -3.9688845 -4.0712147 -4.1547828 -4.2197967 -4.2577596][-4.2580118 -4.2492023 -4.2347589 -4.1984115 -4.1351261 -4.0502381 -3.9573174 -3.8719797 -3.8018856 -3.8012242 -3.8930757 -4.0213547 -4.1268907 -4.2028761 -4.2485056][-4.2554345 -4.2464342 -4.2383041 -4.2194133 -4.1840072 -4.1270857 -4.0484619 -3.9635241 -3.8846378 -3.8599083 -3.9238887 -4.0405354 -4.14242 -4.2124028 -4.2544355][-4.2432361 -4.2337718 -4.2338324 -4.2374916 -4.2356162 -4.2159996 -4.1697016 -4.1070614 -4.0394959 -4.0025229 -4.0308213 -4.1116552 -4.1871819 -4.2345347 -4.2612967][-4.2086225 -4.2021236 -4.2149715 -4.2411065 -4.2651486 -4.2704859 -4.2484632 -4.2075648 -4.1597424 -4.1259723 -4.1309938 -4.1762 -4.2215824 -4.2451067 -4.2535295][-4.1530213 -4.150425 -4.1810789 -4.2300735 -4.2707896 -4.2885256 -4.2855916 -4.2650185 -4.2365522 -4.2109308 -4.2027221 -4.2193856 -4.24199 -4.2512035 -4.2480259][-4.0623541 -4.0543127 -4.1062918 -4.1803942 -4.2390513 -4.27091 -4.28718 -4.2868066 -4.2794595 -4.2706337 -4.2616677 -4.2613845 -4.2664981 -4.2637248 -4.2519422][-3.9380066 -3.9122245 -3.9830761 -4.0888538 -4.1729407 -4.2265506 -4.2647548 -4.2830997 -4.2945838 -4.3050151 -4.3037825 -4.2968135 -4.2893353 -4.2756891 -4.2571726][-3.8503788 -3.804702 -3.8837144 -4.0148387 -4.1238642 -4.1973433 -4.2477794 -4.2770777 -4.3009777 -4.3242378 -4.330874 -4.3222418 -4.3067551 -4.2858028 -4.26392][-3.8813229 -3.8287513 -3.8892012 -4.0137525 -4.1265464 -4.202702 -4.2505908 -4.27754 -4.2993689 -4.3240614 -4.3373952 -4.3338413 -4.3208995 -4.3009453 -4.2800646]]...]
INFO - root - 2017-12-07 20:45:36.426538: step 52010, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 49h:40m:32s remains)
INFO - root - 2017-12-07 20:45:43.232032: step 52020, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 55h:28m:58s remains)
INFO - root - 2017-12-07 20:45:49.814840: step 52030, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 53h:24m:30s remains)
INFO - root - 2017-12-07 20:45:56.525354: step 52040, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 51h:11m:57s remains)
INFO - root - 2017-12-07 20:46:03.400830: step 52050, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.666 sec/batch; 51h:53m:05s remains)
INFO - root - 2017-12-07 20:46:10.299453: step 52060, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 55h:06m:28s remains)
INFO - root - 2017-12-07 20:46:17.137829: step 52070, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 57h:06m:42s remains)
INFO - root - 2017-12-07 20:46:23.945344: step 52080, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 56h:24m:01s remains)
INFO - root - 2017-12-07 20:46:30.743198: step 52090, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 53h:23m:32s remains)
INFO - root - 2017-12-07 20:46:37.217190: step 52100, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 47h:51m:53s remains)
2017-12-07 20:46:38.022403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2466235 -4.265625 -4.2806606 -4.2869716 -4.2792773 -4.2650633 -4.2554369 -4.2604718 -4.2802768 -4.2978287 -4.3063211 -4.3029366 -4.2929254 -4.2877655 -4.2949018][-4.2389708 -4.2673888 -4.2911739 -4.3012047 -4.2893448 -4.2651529 -4.2449565 -4.2456474 -4.2709255 -4.2930875 -4.3018427 -4.2964945 -4.2822723 -4.2754912 -4.2873125][-4.224184 -4.265017 -4.2975373 -4.3084397 -4.2917881 -4.2571664 -4.2255559 -4.2238836 -4.2573643 -4.28569 -4.2932682 -4.2803836 -4.2570496 -4.2490716 -4.2680392][-4.2114525 -4.2588711 -4.2931852 -4.3022184 -4.2808375 -4.2320523 -4.1855445 -4.181488 -4.2254596 -4.2643275 -4.2750444 -4.2535009 -4.215754 -4.2051797 -4.2313628][-4.2081537 -4.2576351 -4.2893767 -4.2912035 -4.2589207 -4.1919031 -4.1262169 -4.1202378 -4.1801043 -4.2332416 -4.24604 -4.2122421 -4.1611967 -4.1504345 -4.1848941][-4.2053432 -4.254621 -4.2865415 -4.2788448 -4.2287283 -4.1373796 -4.047049 -4.0343146 -4.1169004 -4.1977086 -4.2151847 -4.169467 -4.1102877 -4.1050406 -4.1490216][-4.2352304 -4.275526 -4.2966304 -4.2733445 -4.1998396 -4.08051 -3.9512243 -3.9156613 -4.0244827 -4.1477966 -4.1831236 -4.1373963 -4.0801344 -4.08257 -4.1329031][-4.2727232 -4.3025084 -4.3156085 -4.2862697 -4.199595 -4.0513687 -3.8649065 -3.7769217 -3.9083266 -4.081759 -4.1498642 -4.1217036 -4.0759439 -4.080266 -4.1283617][-4.2935505 -4.3186374 -4.33334 -4.310463 -4.2240787 -4.0606375 -3.8365755 -3.6956921 -3.8233869 -4.0277605 -4.1315203 -4.1346388 -4.1136312 -4.1213064 -4.1570191][-4.2935357 -4.3227148 -4.3452868 -4.3381186 -4.2691488 -4.1248217 -3.9204941 -3.7826834 -3.8632255 -4.0413332 -4.1545668 -4.1810169 -4.1787286 -4.1865544 -4.2070045][-4.2711706 -4.3045239 -4.3317041 -4.3388052 -4.29553 -4.1881146 -4.03026 -3.9184055 -3.9620161 -4.0999923 -4.2043328 -4.2393932 -4.2471113 -4.2556491 -4.265923][-4.2518096 -4.278163 -4.3059893 -4.3262672 -4.30895 -4.2385764 -4.1314754 -4.0520568 -4.0755944 -4.1753407 -4.2579308 -4.2882285 -4.2979589 -4.3065538 -4.3125463][-4.253933 -4.2716794 -4.2971077 -4.323493 -4.3250089 -4.2884059 -4.2259607 -4.1761341 -4.1852312 -4.2480993 -4.3057108 -4.328382 -4.3355584 -4.3395634 -4.3408742][-4.283114 -4.2941303 -4.3119273 -4.33361 -4.3425555 -4.3277597 -4.2968016 -4.2678771 -4.267786 -4.3013906 -4.3359804 -4.3500609 -4.3527746 -4.3537107 -4.3538942][-4.3157697 -4.3226848 -4.3324695 -4.3458266 -4.3536139 -4.3488107 -4.335022 -4.3201585 -4.3154287 -4.3273058 -4.3410931 -4.346911 -4.348825 -4.3509555 -4.3528137]]...]
INFO - root - 2017-12-07 20:46:44.880853: step 52110, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 56h:14m:25s remains)
INFO - root - 2017-12-07 20:46:51.727416: step 52120, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 49h:08m:33s remains)
INFO - root - 2017-12-07 20:46:58.530363: step 52130, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 49h:24m:25s remains)
INFO - root - 2017-12-07 20:47:05.279040: step 52140, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 56h:22m:53s remains)
INFO - root - 2017-12-07 20:47:12.125155: step 52150, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 51h:43m:54s remains)
INFO - root - 2017-12-07 20:47:18.892853: step 52160, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 52h:07m:36s remains)
INFO - root - 2017-12-07 20:47:25.678192: step 52170, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 50h:05m:38s remains)
INFO - root - 2017-12-07 20:47:32.585120: step 52180, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 50h:44m:25s remains)
INFO - root - 2017-12-07 20:47:39.295672: step 52190, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 54h:25m:23s remains)
INFO - root - 2017-12-07 20:47:45.934917: step 52200, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 57h:14m:16s remains)
2017-12-07 20:47:46.624307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3010163 -4.3034511 -4.3030992 -4.3021612 -4.29166 -4.2781143 -4.2768893 -4.2863855 -4.2923274 -4.2908363 -4.2868271 -4.2835741 -4.284101 -4.2879825 -4.2882428][-4.2874832 -4.2868767 -4.2860756 -4.2863069 -4.2772889 -4.2621608 -4.260932 -4.2708278 -4.2803597 -4.287241 -4.2908936 -4.2898574 -4.2899041 -4.2925963 -4.2912951][-4.2636652 -4.2597032 -4.2578917 -4.2625084 -4.2594733 -4.2402406 -4.2303605 -4.2370176 -4.25136 -4.2686033 -4.2822042 -4.2870722 -4.2867951 -4.2905788 -4.2899508][-4.2431097 -4.2417612 -4.2391424 -4.2444768 -4.246563 -4.2222118 -4.19437 -4.1964312 -4.214602 -4.2409472 -4.262825 -4.2744632 -4.2761455 -4.2778654 -4.2796388][-4.2351437 -4.23836 -4.2335892 -4.2353888 -4.23886 -4.209321 -4.1666007 -4.159483 -4.1882253 -4.2232037 -4.2480726 -4.2652154 -4.2748413 -4.2739182 -4.271235][-4.241591 -4.2486124 -4.24314 -4.2391028 -4.2327604 -4.1905 -4.1294165 -4.1107931 -4.1584182 -4.2122397 -4.2444572 -4.2658138 -4.2794104 -4.2762547 -4.2683067][-4.2403488 -4.2581043 -4.2575855 -4.2481551 -4.2231579 -4.1586294 -4.0631537 -4.0217614 -4.0990186 -4.1828232 -4.2312407 -4.2589183 -4.2759542 -4.2752032 -4.2648611][-4.2254848 -4.2559409 -4.2634072 -4.2454824 -4.2015438 -4.1104541 -3.9647281 -3.8986809 -4.0194173 -4.1378541 -4.1978822 -4.2306175 -4.2508864 -4.2538438 -4.2462034][-4.2206006 -4.253809 -4.2652655 -4.23886 -4.183989 -4.079052 -3.9080904 -3.8214645 -3.9673195 -4.1078639 -4.1677661 -4.1947207 -4.2160172 -4.219913 -4.2133408][-4.2332258 -4.2550049 -4.2587328 -4.2288785 -4.1756577 -4.0849352 -3.9489238 -3.8849571 -3.9946032 -4.1147571 -4.1640782 -4.1766353 -4.1899176 -4.1946869 -4.1925812][-4.2581491 -4.2618856 -4.2539926 -4.2270532 -4.1854005 -4.1227584 -4.0400715 -4.0038414 -4.0658455 -4.1468849 -4.1833391 -4.1884522 -4.193327 -4.1973391 -4.1980844][-4.2821145 -4.2729278 -4.2574115 -4.237926 -4.2123318 -4.1765013 -4.1327963 -4.1138067 -4.1427884 -4.1896954 -4.2163024 -4.2198076 -4.2195344 -4.2235622 -4.2230248][-4.2975216 -4.2820649 -4.2674723 -4.2571659 -4.2470255 -4.2317743 -4.211504 -4.2024455 -4.2135162 -4.2342777 -4.2495217 -4.2526402 -4.2520051 -4.2536159 -4.2519755][-4.3067985 -4.2911949 -4.2807403 -4.2763786 -4.276288 -4.2725945 -4.2652068 -4.2597871 -4.261683 -4.2693043 -4.2762642 -4.2762547 -4.2749348 -4.2757 -4.2744474][-4.3173733 -4.3056622 -4.2997842 -4.2995396 -4.3025107 -4.3031855 -4.3005552 -4.2971058 -4.2933426 -4.2938161 -4.2952032 -4.2932081 -4.2929492 -4.2946658 -4.2960114]]...]
INFO - root - 2017-12-07 20:47:53.604759: step 52210, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 52h:56m:58s remains)
INFO - root - 2017-12-07 20:48:00.418624: step 52220, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 57h:06m:54s remains)
INFO - root - 2017-12-07 20:48:07.190568: step 52230, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 53h:52m:28s remains)
INFO - root - 2017-12-07 20:48:14.050930: step 52240, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 49h:10m:05s remains)
INFO - root - 2017-12-07 20:48:20.912180: step 52250, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 51h:34m:48s remains)
INFO - root - 2017-12-07 20:48:27.743086: step 52260, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 57h:46m:22s remains)
INFO - root - 2017-12-07 20:48:34.656535: step 52270, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 56h:10m:20s remains)
INFO - root - 2017-12-07 20:48:41.446017: step 52280, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 49h:55m:04s remains)
INFO - root - 2017-12-07 20:48:48.131394: step 52290, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 48h:12m:53s remains)
INFO - root - 2017-12-07 20:48:54.647763: step 52300, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 49h:49m:24s remains)
2017-12-07 20:48:55.336866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2884984 -4.292768 -4.2963924 -4.2920184 -4.2816439 -4.2724018 -4.2564068 -4.2359309 -4.2111721 -4.2047071 -4.2196546 -4.2286329 -4.2367611 -4.234921 -4.2266293][-4.2611995 -4.2688122 -4.2746267 -4.2679639 -4.2532191 -4.2403593 -4.2180786 -4.1913228 -4.1617694 -4.1554317 -4.1738405 -4.1826205 -4.1914544 -4.1904511 -4.1819596][-4.2400084 -4.2474093 -4.2520142 -4.2423325 -4.2217031 -4.2011576 -4.1702485 -4.1390562 -4.1122 -4.1073623 -4.1283956 -4.1387148 -4.1504507 -4.1514597 -4.1422472][-4.2235026 -4.2322321 -4.2337904 -4.217979 -4.1854072 -4.1516309 -4.1116028 -4.0768061 -4.0505209 -4.045907 -4.0687385 -4.0819888 -4.0969572 -4.1014991 -4.0941019][-4.193573 -4.2081137 -4.2077832 -4.1804295 -4.1370645 -4.0922518 -4.0460038 -4.0122352 -3.9961877 -3.9989371 -4.0246186 -4.043509 -4.06367 -4.070179 -4.0614977][-4.1470327 -4.1737905 -4.1778455 -4.1384859 -4.0854297 -4.0340972 -3.988277 -3.9517539 -3.9422965 -3.9634788 -4.0031276 -4.0355625 -4.0656676 -4.0713029 -4.0614409][-4.1186891 -4.1582665 -4.1694183 -4.1252546 -4.064548 -4.0071898 -3.9539258 -3.8925166 -3.8709748 -3.920969 -3.9849977 -4.0375957 -4.0804977 -4.0831919 -4.0690045][-4.1106267 -4.1513186 -4.1659031 -4.1230612 -4.0678697 -4.0120907 -3.9499528 -3.8635335 -3.8268504 -3.896466 -3.9796896 -4.0438337 -4.0957942 -4.101614 -4.0902271][-4.1236854 -4.1562309 -4.168973 -4.1326051 -4.0930662 -4.0512686 -4.0024571 -3.9348516 -3.9149125 -3.9758251 -4.0392604 -4.0834622 -4.12277 -4.1231608 -4.1154342][-4.1688695 -4.1902871 -4.1986637 -4.16955 -4.1388416 -4.103219 -4.0653992 -4.0174813 -4.008822 -4.054153 -4.0970235 -4.1226153 -4.1484895 -4.14476 -4.1378078][-4.2228093 -4.2372961 -4.2425227 -4.2234969 -4.2017884 -4.1743665 -4.1430087 -4.1013584 -4.0905452 -4.1278982 -4.1608682 -4.179543 -4.1975994 -4.1906376 -4.17947][-4.2676668 -4.2752147 -4.2748842 -4.2622418 -4.25036 -4.231811 -4.212914 -4.1830244 -4.1711912 -4.1943126 -4.2182636 -4.2326493 -4.24579 -4.2409229 -4.2307253][-4.2888231 -4.2941041 -4.2922931 -4.2861843 -4.2810507 -4.2719092 -4.263279 -4.2500286 -4.2410607 -4.2504234 -4.26316 -4.2711792 -4.2807579 -4.2809057 -4.27386][-4.2899175 -4.293458 -4.2918844 -4.2879515 -4.2863274 -4.28339 -4.2820311 -4.2802124 -4.2779293 -4.2828503 -4.2893481 -4.2915053 -4.2964034 -4.2977662 -4.2936378][-4.2866316 -4.2892208 -4.2869997 -4.2837396 -4.2835531 -4.2838097 -4.2867656 -4.289465 -4.2906246 -4.2939057 -4.2977538 -4.2978706 -4.2997656 -4.3012385 -4.3000088]]...]
INFO - root - 2017-12-07 20:49:02.071951: step 52310, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 50h:38m:36s remains)
INFO - root - 2017-12-07 20:49:08.819495: step 52320, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 50h:29m:41s remains)
INFO - root - 2017-12-07 20:49:15.614312: step 52330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 51h:29m:02s remains)
INFO - root - 2017-12-07 20:49:22.292443: step 52340, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 54h:41m:01s remains)
INFO - root - 2017-12-07 20:49:29.095894: step 52350, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 53h:10m:07s remains)
INFO - root - 2017-12-07 20:49:35.971777: step 52360, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 49h:27m:36s remains)
INFO - root - 2017-12-07 20:49:42.828287: step 52370, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 52h:53m:26s remains)
INFO - root - 2017-12-07 20:49:49.677766: step 52380, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.740 sec/batch; 57h:32m:49s remains)
INFO - root - 2017-12-07 20:49:56.419841: step 52390, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 52h:28m:11s remains)
INFO - root - 2017-12-07 20:50:03.016226: step 52400, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 52h:18m:05s remains)
2017-12-07 20:50:03.710464: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2773676 -4.2891722 -4.2974677 -4.300869 -4.2995329 -4.293632 -4.2784142 -4.2615519 -4.2544708 -4.2516403 -4.2451849 -4.2238927 -4.1836214 -4.1399522 -4.1069441][-4.2751527 -4.2823634 -4.2871051 -4.2881417 -4.2819567 -4.2662268 -4.2436948 -4.2283993 -4.2263393 -4.2288446 -4.2293186 -4.2150249 -4.1827846 -4.148479 -4.1256261][-4.2720428 -4.2699046 -4.2683163 -4.266459 -4.251 -4.2201543 -4.1923795 -4.1814666 -4.1863513 -4.1953306 -4.2007942 -4.1918488 -4.1685143 -4.1487536 -4.142024][-4.2610483 -4.2495584 -4.2416177 -4.2348771 -4.2102656 -4.166986 -4.1362333 -4.1311803 -4.1463428 -4.1647191 -4.1733875 -4.1631322 -4.1390066 -4.1290989 -4.1397014][-4.2400351 -4.2272558 -4.2199259 -4.2095251 -4.1756172 -4.119966 -4.0828609 -4.0820193 -4.1122766 -4.1469941 -4.1611581 -4.1441774 -4.107213 -4.0948138 -4.1167784][-4.2106347 -4.2069445 -4.203578 -4.1892586 -4.1431885 -4.0664859 -4.0027428 -3.9968803 -4.0571256 -4.1253028 -4.152432 -4.131496 -4.0836349 -4.0666723 -4.0894461][-4.1769753 -4.1865592 -4.1883335 -4.1704516 -4.11486 -4.0137672 -3.9012442 -3.872541 -3.9754953 -4.083765 -4.1294584 -4.1118345 -4.0663905 -4.05374 -4.0726004][-4.147913 -4.165441 -4.1681166 -4.1499524 -4.0965815 -3.9877541 -3.8404851 -3.7880075 -3.9158263 -4.0440764 -4.1005826 -4.0898256 -4.0642347 -4.0649729 -4.0802689][-4.1325345 -4.1531825 -4.1556125 -4.1400166 -4.1027632 -4.0273719 -3.9175577 -3.8823357 -3.9696937 -4.0642242 -4.1083293 -4.0993729 -4.0909348 -4.1037459 -4.1200495][-4.1399217 -4.1628733 -4.1656857 -4.1580739 -4.1409411 -4.1089435 -4.0605831 -4.0497036 -4.0926485 -4.1389465 -4.1570845 -4.1412754 -4.1351447 -4.1472073 -4.162291][-4.16917 -4.193562 -4.2013974 -4.2027235 -4.1978292 -4.18987 -4.1768417 -4.1797004 -4.1986117 -4.2151766 -4.2195945 -4.2028441 -4.1943812 -4.1986279 -4.2079043][-4.2085328 -4.23279 -4.2446547 -4.252389 -4.2524137 -4.2503891 -4.2467446 -4.2517109 -4.260345 -4.2699547 -4.2755876 -4.2683249 -4.2607613 -4.2560706 -4.255991][-4.2515893 -4.2710452 -4.2831068 -4.2908959 -4.2864995 -4.2801085 -4.2777534 -4.2853327 -4.2951694 -4.3053832 -4.3132257 -4.3121428 -4.3074517 -4.2991996 -4.2946839][-4.2830558 -4.2986155 -4.3099127 -4.3141823 -4.3023529 -4.2877355 -4.2834344 -4.2878032 -4.29438 -4.3019638 -4.3069425 -4.3083992 -4.3088222 -4.3030882 -4.302115][-4.2928405 -4.3058157 -4.3134189 -4.3112831 -4.2942567 -4.274179 -4.2626133 -4.25744 -4.2536407 -4.2527328 -4.2536607 -4.2577906 -4.2646751 -4.2666645 -4.2705569]]...]
INFO - root - 2017-12-07 20:50:10.637998: step 52410, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 55h:40m:22s remains)
INFO - root - 2017-12-07 20:50:17.449194: step 52420, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 53h:41m:07s remains)
INFO - root - 2017-12-07 20:50:24.190531: step 52430, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 49h:12m:43s remains)
INFO - root - 2017-12-07 20:50:31.046081: step 52440, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 50h:57m:13s remains)
INFO - root - 2017-12-07 20:50:38.044207: step 52450, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 57h:03m:59s remains)
INFO - root - 2017-12-07 20:50:44.920551: step 52460, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 56h:37m:46s remains)
INFO - root - 2017-12-07 20:50:51.630941: step 52470, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 52h:12m:24s remains)
INFO - root - 2017-12-07 20:50:58.454113: step 52480, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 49h:43m:25s remains)
INFO - root - 2017-12-07 20:51:05.393680: step 52490, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 56h:06m:04s remains)
INFO - root - 2017-12-07 20:51:12.060388: step 52500, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 56h:17m:42s remains)
2017-12-07 20:51:12.823843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.315104 -4.3100061 -4.3121839 -4.3165522 -4.3203206 -4.3209391 -4.3122268 -4.2979631 -4.2811689 -4.2640424 -4.2544985 -4.25197 -4.25131 -4.2592039 -4.2680192][-4.3211346 -4.3175931 -4.3178329 -4.3183193 -4.3188381 -4.3165183 -4.3047528 -4.2854013 -4.264019 -4.2444758 -4.232574 -4.2288823 -4.2300224 -4.2386141 -4.2470355][-4.3265109 -4.3264589 -4.3263426 -4.3233166 -4.3169756 -4.3081508 -4.29041 -4.2670641 -4.2458496 -4.2278824 -4.2169151 -4.2148037 -4.2205157 -4.2293916 -4.2377658][-4.32472 -4.3288078 -4.3289876 -4.3224454 -4.3100657 -4.2951097 -4.2724781 -4.2476354 -4.2271838 -4.2104487 -4.2004328 -4.2024837 -4.2156134 -4.2243152 -4.2318096][-4.3179169 -4.3260489 -4.3267546 -4.3170271 -4.3006339 -4.2798595 -4.2526593 -4.2267079 -4.2064185 -4.1916528 -4.1846294 -4.1960387 -4.2172694 -4.2234607 -4.2300668][-4.3030925 -4.3152304 -4.3170018 -4.3038449 -4.2826972 -4.2566648 -4.226192 -4.2022862 -4.186409 -4.1775689 -4.177669 -4.2017198 -4.2298489 -4.2355514 -4.2414174][-4.2819719 -4.2942562 -4.2964454 -4.2820692 -4.2585936 -4.22998 -4.2003169 -4.1832395 -4.1760187 -4.1731005 -4.1815157 -4.2126241 -4.2418537 -4.2454462 -4.24869][-4.261827 -4.2695289 -4.2674732 -4.2498813 -4.2249775 -4.1956034 -4.1683259 -4.1563687 -4.1523995 -4.1510873 -4.1643419 -4.1995773 -4.2269945 -4.228138 -4.2323852][-4.2536769 -4.2506123 -4.2387171 -4.2152457 -4.1859393 -4.1536307 -4.1263728 -4.1148863 -4.1080947 -4.10201 -4.1134658 -4.1513195 -4.1773825 -4.1777186 -4.1860218][-4.2619209 -4.2494841 -4.2317472 -4.2026806 -4.1684713 -4.1335168 -4.1057954 -4.0915933 -4.0794888 -4.0676937 -4.0749812 -4.1106954 -4.1336975 -4.1351733 -4.1464915][-4.279531 -4.2661324 -4.2495561 -4.2214952 -4.1890931 -4.1565251 -4.1307611 -4.1168246 -4.1038589 -4.0921078 -4.099987 -4.1298594 -4.1483393 -4.1511593 -4.1631927][-4.2945433 -4.2848277 -4.2745328 -4.2532425 -4.2277741 -4.1999454 -4.1768985 -4.1661725 -4.1590166 -4.1514845 -4.1600323 -4.1823053 -4.19526 -4.1985378 -4.2094011][-4.2976861 -4.2921543 -4.287611 -4.27405 -4.25677 -4.2365336 -4.2211766 -4.2190452 -4.223413 -4.225585 -4.2356172 -4.2487969 -4.2524185 -4.2493954 -4.2535572][-4.2858405 -4.2807364 -4.2804117 -4.2761245 -4.2693071 -4.2600107 -4.2562838 -4.2637506 -4.27722 -4.2858119 -4.2945132 -4.3000617 -4.2972345 -4.2879858 -4.2825832][-4.2596006 -4.2515345 -4.2544923 -4.2607203 -4.2641087 -4.2645297 -4.2700047 -4.2827926 -4.2980204 -4.3076863 -4.3151155 -4.3169255 -4.3112082 -4.2983103 -4.2851415]]...]
INFO - root - 2017-12-07 20:51:19.701666: step 52510, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 50h:37m:52s remains)
INFO - root - 2017-12-07 20:51:26.537370: step 52520, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 55h:45m:54s remains)
INFO - root - 2017-12-07 20:51:33.284272: step 52530, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.767 sec/batch; 59h:38m:40s remains)
INFO - root - 2017-12-07 20:51:40.114800: step 52540, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 54h:08m:04s remains)
INFO - root - 2017-12-07 20:51:46.999050: step 52550, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 48h:47m:53s remains)
INFO - root - 2017-12-07 20:51:53.885651: step 52560, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 54h:34m:39s remains)
INFO - root - 2017-12-07 20:52:00.704278: step 52570, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 55h:49m:19s remains)
INFO - root - 2017-12-07 20:52:07.489270: step 52580, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 53h:47m:10s remains)
INFO - root - 2017-12-07 20:52:14.305184: step 52590, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.673 sec/batch; 52h:21m:14s remains)
INFO - root - 2017-12-07 20:52:20.889548: step 52600, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 51h:26m:07s remains)
2017-12-07 20:52:21.644237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2787023 -4.2830834 -4.2803946 -4.26931 -4.2632318 -4.2668324 -4.2713819 -4.269042 -4.263979 -4.257113 -4.2500753 -4.2580528 -4.2856512 -4.3112555 -4.3206477][-4.2734714 -4.2770643 -4.2751465 -4.2654128 -4.2611532 -4.2664747 -4.2724538 -4.2714944 -4.267199 -4.2611766 -4.2527246 -4.2587848 -4.2836227 -4.305985 -4.3126826][-4.2705631 -4.274148 -4.27266 -4.2664685 -4.2665949 -4.273262 -4.2779784 -4.275825 -4.2706432 -4.2652731 -4.2560587 -4.2600226 -4.2796311 -4.29592 -4.2995777][-4.2681017 -4.2721548 -4.2719169 -4.270452 -4.2758574 -4.2814684 -4.2808723 -4.2753029 -4.2695756 -4.2651772 -4.2585397 -4.2625308 -4.2771792 -4.2870784 -4.2870469][-4.2667279 -4.272861 -4.2758932 -4.2768259 -4.2817125 -4.2807207 -4.2699394 -4.2592425 -4.2566795 -4.2592664 -4.2596464 -4.2660689 -4.2781019 -4.2839346 -4.2803655][-4.2660007 -4.2760859 -4.2827721 -4.2830358 -4.2804117 -4.2683949 -4.2450609 -4.2298713 -4.2330866 -4.24615 -4.2561035 -4.2666521 -4.2779994 -4.2819037 -4.2764049][-4.2631755 -4.2765775 -4.2868137 -4.2851858 -4.2729197 -4.2489438 -4.2192974 -4.2056637 -4.2161889 -4.2362471 -4.2516813 -4.2653632 -4.2768297 -4.2792277 -4.273][-4.2579241 -4.2731309 -4.28559 -4.2842512 -4.2659626 -4.2348475 -4.2040739 -4.1943054 -4.206502 -4.2267647 -4.2416849 -4.2562017 -4.2695203 -4.2740464 -4.2693448][-4.2572079 -4.2703195 -4.28155 -4.281333 -4.2635484 -4.2352118 -4.2098565 -4.2015643 -4.206553 -4.216599 -4.2243495 -4.2379103 -4.2574658 -4.26837 -4.2665792][-4.2647953 -4.2741594 -4.28237 -4.2831564 -4.2699375 -4.2486763 -4.2310414 -4.2210436 -4.2142854 -4.2083836 -4.204217 -4.2178493 -4.2448454 -4.2637167 -4.2660418][-4.2763014 -4.2846155 -4.2889028 -4.2893825 -4.2792797 -4.2640295 -4.2522855 -4.2412086 -4.2256217 -4.2054744 -4.1899271 -4.2022214 -4.2351193 -4.2604914 -4.2668567][-4.28355 -4.2914844 -4.2920346 -4.2881522 -4.2776809 -4.2665019 -4.260417 -4.2525373 -4.2339678 -4.2066426 -4.1843519 -4.1963897 -4.2337279 -4.2628317 -4.2710938][-4.2892218 -4.2944827 -4.28972 -4.2787275 -4.2654729 -4.258018 -4.2582164 -4.2546453 -4.2378821 -4.2122326 -4.1913924 -4.2055469 -4.2433834 -4.2715387 -4.2778544][-4.2923346 -4.2940035 -4.2852955 -4.2696176 -4.2559404 -4.2529726 -4.2580528 -4.2580342 -4.2439795 -4.2217321 -4.2048283 -4.2193189 -4.2544847 -4.2799883 -4.2834826][-4.2900739 -4.2904644 -4.2818418 -4.2642264 -4.2496862 -4.24877 -4.2564225 -4.2586832 -4.2476606 -4.2284303 -4.21465 -4.226903 -4.2575679 -4.2797 -4.2810936]]...]
INFO - root - 2017-12-07 20:52:28.420912: step 52610, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 50h:26m:25s remains)
INFO - root - 2017-12-07 20:52:35.348997: step 52620, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 50h:08m:49s remains)
INFO - root - 2017-12-07 20:52:42.095586: step 52630, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 50h:54m:51s remains)
INFO - root - 2017-12-07 20:52:49.067917: step 52640, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 57h:41m:10s remains)
INFO - root - 2017-12-07 20:52:55.732264: step 52650, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 53h:32m:40s remains)
INFO - root - 2017-12-07 20:53:02.424436: step 52660, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 48h:51m:54s remains)
INFO - root - 2017-12-07 20:53:09.258627: step 52670, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 49h:55m:03s remains)
INFO - root - 2017-12-07 20:53:16.164061: step 52680, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 56h:09m:22s remains)
INFO - root - 2017-12-07 20:53:22.918445: step 52690, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 56h:41m:54s remains)
INFO - root - 2017-12-07 20:53:29.445806: step 52700, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 51h:26m:19s remains)
2017-12-07 20:53:30.157439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.192462 -4.1886559 -4.1951165 -4.20194 -4.210712 -4.2207727 -4.2272739 -4.2387948 -4.2509575 -4.2547226 -4.246582 -4.230567 -4.2163181 -4.2131786 -4.2176518][-4.2239809 -4.222724 -4.2281308 -4.231832 -4.2338853 -4.23501 -4.2327595 -4.2368989 -4.2494531 -4.2614374 -4.2659531 -4.2601314 -4.2524815 -4.2490158 -4.2474017][-4.2594523 -4.2637358 -4.2694292 -4.2717605 -4.2674842 -4.2573214 -4.2427468 -4.2370377 -4.2476687 -4.2667608 -4.2829785 -4.287364 -4.2875328 -4.2871265 -4.2824016][-4.2829289 -4.2909055 -4.294723 -4.2934451 -4.2806759 -4.2554274 -4.2267642 -4.2123146 -4.2205052 -4.2455163 -4.2743592 -4.2932253 -4.3062015 -4.3121777 -4.3085775][-4.2899132 -4.2989035 -4.2968836 -4.2865996 -4.2631311 -4.2267385 -4.192596 -4.1794968 -4.1892138 -4.2162952 -4.2502465 -4.278717 -4.3009281 -4.313622 -4.3146768][-4.2855625 -4.2902374 -4.2770867 -4.2545605 -4.2239013 -4.1871157 -4.1623211 -4.1611857 -4.176002 -4.1989174 -4.2260485 -4.25183 -4.2747207 -4.2918491 -4.2995925][-4.2715535 -4.26461 -4.2386284 -4.2076321 -4.1771922 -4.1498933 -4.1407042 -4.153306 -4.174695 -4.1955791 -4.2129049 -4.2268853 -4.2389655 -4.2529235 -4.2637048][-4.2450819 -4.2236814 -4.1887059 -4.1568594 -4.1293788 -4.1104031 -4.1137 -4.1367378 -4.1639977 -4.182847 -4.1918178 -4.1937852 -4.1932745 -4.201077 -4.2138038][-4.2102761 -4.179101 -4.1452837 -4.117692 -4.0935116 -4.0799284 -4.0911803 -4.1194134 -4.1479487 -4.1628723 -4.1647406 -4.1580839 -4.151391 -4.1581893 -4.1753473][-4.1798835 -4.1431203 -4.1164055 -4.0955062 -4.0735993 -4.0603647 -4.073071 -4.1001387 -4.126193 -4.1394444 -4.1375957 -4.1254697 -4.1167912 -4.1276765 -4.1523452][-4.1734853 -4.1375284 -4.1221271 -4.1119242 -4.0947618 -4.0777841 -4.0803881 -4.0958018 -4.1124644 -4.1225247 -4.1197753 -4.1062546 -4.0976996 -4.1115818 -4.1421204][-4.1938834 -4.16387 -4.1577582 -4.1565571 -4.1461239 -4.12916 -4.1223874 -4.1260819 -4.1327882 -4.1400695 -4.1375823 -4.125083 -4.1157022 -4.1263103 -4.15496][-4.2308264 -4.2076259 -4.20688 -4.213491 -4.2119093 -4.2003093 -4.1896477 -4.185564 -4.1851468 -4.1893063 -4.1844344 -4.1713681 -4.1595531 -4.1619349 -4.1783895][-4.2657623 -4.25262 -4.2571635 -4.2713776 -4.2790465 -4.2750773 -4.265162 -4.2552962 -4.2480297 -4.2465792 -4.2396069 -4.227169 -4.2136135 -4.2087464 -4.2138319][-4.28691 -4.2833505 -4.2915516 -4.3085532 -4.3209572 -4.3215103 -4.3147388 -4.3050332 -4.2954516 -4.2906265 -4.2844782 -4.2755713 -4.2641554 -4.2565994 -4.2562819]]...]
INFO - root - 2017-12-07 20:53:36.916103: step 52710, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 51h:01m:10s remains)
INFO - root - 2017-12-07 20:53:43.707377: step 52720, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 56h:27m:00s remains)
INFO - root - 2017-12-07 20:53:50.459170: step 52730, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.708 sec/batch; 55h:02m:52s remains)
INFO - root - 2017-12-07 20:53:57.041099: step 52740, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 52h:20m:13s remains)
INFO - root - 2017-12-07 20:54:03.792112: step 52750, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 49h:13m:26s remains)
INFO - root - 2017-12-07 20:54:10.476572: step 52760, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 52h:11m:19s remains)
INFO - root - 2017-12-07 20:54:17.337343: step 52770, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 57h:04m:50s remains)
INFO - root - 2017-12-07 20:54:24.190923: step 52780, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 53h:42m:01s remains)
INFO - root - 2017-12-07 20:54:31.046128: step 52790, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 52h:22m:50s remains)
INFO - root - 2017-12-07 20:54:37.647300: step 52800, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 50h:23m:09s remains)
2017-12-07 20:54:38.431690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.346962 -4.333837 -4.3203092 -4.308105 -4.2942767 -4.2769079 -4.2694044 -4.2833018 -4.3027663 -4.3152709 -4.3212547 -4.3247371 -4.3188157 -4.3119097 -4.3072944][-4.3467379 -4.3301435 -4.3127 -4.2992759 -4.2832117 -4.2616711 -4.2504883 -4.266686 -4.29717 -4.3218713 -4.331069 -4.3324966 -4.3214612 -4.3072867 -4.2968283][-4.3410382 -4.320683 -4.2989163 -4.27983 -4.2579422 -4.235044 -4.2224622 -4.2402487 -4.2808638 -4.3170462 -4.3276725 -4.3244863 -4.3071713 -4.2861571 -4.2708287][-4.3361564 -4.3119073 -4.2851591 -4.2583928 -4.2297049 -4.2009482 -4.1812558 -4.1970534 -4.246479 -4.2910261 -4.3043427 -4.2987328 -4.2773132 -4.2531881 -4.2328725][-4.3355103 -4.30897 -4.2773008 -4.2426848 -4.2070928 -4.1627688 -4.1258955 -4.13362 -4.1898956 -4.2469883 -4.2669778 -4.26275 -4.243474 -4.221735 -4.205018][-4.3370032 -4.3087363 -4.2711186 -4.2274237 -4.1768956 -4.1116538 -4.0506864 -4.0385227 -4.10551 -4.1876903 -4.2235804 -4.2247977 -4.2114272 -4.1954346 -4.18604][-4.3379531 -4.3101091 -4.2704773 -4.2188468 -4.1494255 -4.0547137 -3.9509914 -3.8980398 -3.9812825 -4.1021943 -4.1649284 -4.1780763 -4.1687155 -4.1558619 -4.1510725][-4.3335333 -4.305501 -4.2680449 -4.2162161 -4.1422229 -4.0326347 -3.8921123 -3.788486 -3.8698153 -4.0148716 -4.1001339 -4.12924 -4.125967 -4.1149554 -4.108974][-4.3251224 -4.293407 -4.2542219 -4.2076378 -4.147471 -4.0586481 -3.9354067 -3.836705 -3.885649 -4.0017858 -4.077507 -4.1044178 -4.1026192 -4.0908241 -4.0766864][-4.3214235 -4.2904968 -4.2506704 -4.2076368 -4.1582 -4.092423 -4.0062418 -3.9402208 -3.9654188 -4.045186 -4.1052804 -4.1333337 -4.1359315 -4.1196284 -4.0886707][-4.3238492 -4.2989759 -4.2666793 -4.231442 -4.1906219 -4.1400375 -4.0793538 -4.0314045 -4.0356121 -4.085536 -4.1350408 -4.1711679 -4.1818786 -4.1649084 -4.1233296][-4.328598 -4.3110757 -4.2898164 -4.2644024 -4.2344484 -4.1992288 -4.1573882 -4.11711 -4.1025457 -4.1255679 -4.1654029 -4.2041492 -4.2166829 -4.1976867 -4.1545858][-4.335331 -4.3252306 -4.3135347 -4.2964969 -4.2769914 -4.2538671 -4.2265987 -4.196434 -4.1771111 -4.1864634 -4.2187529 -4.2523193 -4.2598262 -4.2391071 -4.1981111][-4.3428593 -4.3387794 -4.33343 -4.3232021 -4.3130074 -4.2997828 -4.2813087 -4.2593036 -4.2444 -4.2495947 -4.2737155 -4.2986917 -4.3022337 -4.2849903 -4.2529244][-4.3441849 -4.3417821 -4.3387685 -4.3328123 -4.3278012 -4.3203049 -4.3096662 -4.2964511 -4.2848992 -4.2852573 -4.2992678 -4.3153343 -4.31781 -4.308043 -4.288866]]...]
INFO - root - 2017-12-07 20:54:45.314123: step 52810, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 52h:28m:32s remains)
INFO - root - 2017-12-07 20:54:52.027887: step 52820, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 48h:37m:23s remains)
INFO - root - 2017-12-07 20:54:58.835951: step 52830, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 49h:23m:41s remains)
INFO - root - 2017-12-07 20:55:05.645163: step 52840, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 56h:24m:40s remains)
INFO - root - 2017-12-07 20:55:12.514242: step 52850, loss = 2.09, batch loss = 2.04 (10.7 examples/sec; 0.746 sec/batch; 57h:56m:49s remains)
INFO - root - 2017-12-07 20:55:19.254895: step 52860, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 51h:39m:19s remains)
INFO - root - 2017-12-07 20:55:26.096007: step 52870, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.613 sec/batch; 47h:38m:41s remains)
INFO - root - 2017-12-07 20:55:32.888670: step 52880, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 50h:54m:01s remains)
INFO - root - 2017-12-07 20:55:39.756092: step 52890, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 55h:05m:40s remains)
INFO - root - 2017-12-07 20:55:46.442216: step 52900, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 54h:55m:15s remains)
2017-12-07 20:55:47.140766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3562908 -4.3488088 -4.3237758 -4.2865176 -4.25189 -4.2262225 -4.2126293 -4.2144852 -4.2226825 -4.2249002 -4.2150288 -4.2078 -4.2098532 -4.2146449 -4.2264805][-4.3617821 -4.3528652 -4.3193946 -4.2701259 -4.225472 -4.1934123 -4.1761751 -4.1793528 -4.19044 -4.1974788 -4.1944914 -4.1875758 -4.1861181 -4.1875882 -4.1970396][-4.3628683 -4.35198 -4.3121428 -4.2575593 -4.2120662 -4.1813149 -4.1624513 -4.165555 -4.1778073 -4.1883907 -4.1965356 -4.1949964 -4.1907754 -4.1882672 -4.1953483][-4.3604817 -4.3453274 -4.3000264 -4.2432323 -4.1997094 -4.1746759 -4.1589985 -4.1628308 -4.1746826 -4.1871548 -4.2069411 -4.215529 -4.2135911 -4.2102981 -4.218009][-4.3543916 -4.3326669 -4.2822828 -4.2244444 -4.1811309 -4.1580095 -4.1431618 -4.1467681 -4.1548367 -4.1679206 -4.1997528 -4.2221241 -4.2280784 -4.2264776 -4.2360229][-4.3461995 -4.3174634 -4.2629051 -4.2029243 -4.1582284 -4.1322155 -4.1123009 -4.1106386 -4.1130157 -4.1266236 -4.1694837 -4.2067213 -4.2234716 -4.2254696 -4.234704][-4.3375731 -4.301507 -4.2420225 -4.178863 -4.1319675 -4.1014514 -4.0785189 -4.0730667 -4.0684447 -4.0800142 -4.1308012 -4.1814947 -4.2086806 -4.2152448 -4.2232127][-4.3311176 -4.29141 -4.2310815 -4.1706553 -4.12728 -4.0963736 -4.07781 -4.0744333 -4.0657439 -4.0703607 -4.11603 -4.1724405 -4.2048082 -4.2118349 -4.2167635][-4.3294625 -4.2917647 -4.2379441 -4.1889329 -4.157392 -4.1334739 -4.1232433 -4.1215634 -4.1093092 -4.1059141 -4.1379681 -4.187953 -4.2178111 -4.2219753 -4.2209654][-4.3294039 -4.2962484 -4.2516594 -4.2134733 -4.1907558 -4.1734943 -4.168232 -4.1654286 -4.1534257 -4.1468925 -4.1708369 -4.2129726 -4.2403512 -4.244453 -4.2404213][-4.329103 -4.2993956 -4.2614384 -4.2291942 -4.2125525 -4.2000341 -4.1967463 -4.1917953 -4.1806059 -4.174201 -4.1944551 -4.2335763 -4.2614307 -4.2684193 -4.2640562][-4.3298831 -4.3019748 -4.2667174 -4.2369456 -4.2254047 -4.2184558 -4.2150216 -4.2078376 -4.1980009 -4.1953731 -4.2150617 -4.2501359 -4.2780457 -4.2893739 -4.28785][-4.3316321 -4.3054605 -4.2713952 -4.2409286 -4.2300024 -4.2265863 -4.2254968 -4.2239051 -4.2207375 -4.2239909 -4.2419233 -4.2700877 -4.2954454 -4.3085923 -4.308825][-4.3320394 -4.3071337 -4.2753644 -4.2453008 -4.2335968 -4.2315087 -4.2366014 -4.2427258 -4.2451344 -4.25076 -4.2657547 -4.287828 -4.3092031 -4.3203611 -4.3207359][-4.3325167 -4.3078279 -4.27795 -4.2475014 -4.2334895 -4.2334185 -4.2440944 -4.2547741 -4.2590537 -4.26521 -4.2785392 -4.2953615 -4.3120341 -4.3204851 -4.3202772]]...]
INFO - root - 2017-12-07 20:55:54.016144: step 52910, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 55h:53m:41s remains)
INFO - root - 2017-12-07 20:56:00.902150: step 52920, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.753 sec/batch; 58h:28m:16s remains)
INFO - root - 2017-12-07 20:56:07.698068: step 52930, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 53h:27m:35s remains)
INFO - root - 2017-12-07 20:56:14.419434: step 52940, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 52h:44m:38s remains)
INFO - root - 2017-12-07 20:56:21.232102: step 52950, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 47h:46m:15s remains)
INFO - root - 2017-12-07 20:56:27.934617: step 52960, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 56h:08m:33s remains)
INFO - root - 2017-12-07 20:56:34.867238: step 52970, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 57h:20m:24s remains)
INFO - root - 2017-12-07 20:56:41.645397: step 52980, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 52h:52m:55s remains)
INFO - root - 2017-12-07 20:56:48.398525: step 52990, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 49h:12m:40s remains)
INFO - root - 2017-12-07 20:56:54.916716: step 53000, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 49h:36m:03s remains)
2017-12-07 20:56:55.601664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3157172 -4.3153811 -4.3178649 -4.3197904 -4.32038 -4.3183918 -4.3164539 -4.3122458 -4.30517 -4.2970781 -4.2914658 -4.2912378 -4.2987766 -4.315496 -4.3347688][-4.2953172 -4.2946606 -4.2984319 -4.30319 -4.3081985 -4.3094382 -4.3074603 -4.3013482 -4.2878971 -4.2718639 -4.2599759 -4.2546768 -4.260273 -4.2828026 -4.3143668][-4.2758846 -4.2721481 -4.2756519 -4.2847657 -4.2959781 -4.3014574 -4.2990422 -4.2911997 -4.2733235 -4.2510967 -4.2339683 -4.2250485 -4.2264967 -4.2515612 -4.2936935][-4.2618413 -4.2548785 -4.2548079 -4.2648416 -4.2804089 -4.2899361 -4.2911487 -4.2845964 -4.2655854 -4.2392774 -4.2185292 -4.2065492 -4.2033186 -4.2261453 -4.27485][-4.2437258 -4.2331038 -4.2273493 -4.2344155 -4.2533045 -4.2675028 -4.2738237 -4.2685113 -4.2466669 -4.214932 -4.1908603 -4.1814752 -4.1820149 -4.2054005 -4.2575684][-4.2192993 -4.2010851 -4.1881952 -4.191731 -4.2126784 -4.2315145 -4.241137 -4.23349 -4.2048497 -4.16657 -4.1380544 -4.1357422 -4.1503234 -4.1833973 -4.2400889][-4.1950684 -4.1683669 -4.1496644 -4.15113 -4.1720843 -4.1943154 -4.2089343 -4.2032495 -4.1741982 -4.1334786 -4.0996904 -4.1021695 -4.1276135 -4.1682138 -4.2276239][-4.1810627 -4.1518588 -4.1303658 -4.128953 -4.146338 -4.1691 -4.1896548 -4.1909814 -4.1690035 -4.1319218 -4.0957289 -4.0944805 -4.1188273 -4.1612682 -4.2211046][-4.1857562 -4.1625571 -4.1443787 -4.1412916 -4.1538625 -4.1745281 -4.1970158 -4.2045803 -4.1905704 -4.1590862 -4.1219049 -4.113379 -4.1304579 -4.1685414 -4.2234073][-4.2082052 -4.1967168 -4.1844296 -4.1789141 -4.1872892 -4.20607 -4.2281094 -4.2389965 -4.2292738 -4.2021279 -4.1679935 -4.1523447 -4.1609578 -4.1902876 -4.2361403][-4.238378 -4.2333121 -4.2218323 -4.2114534 -4.2157679 -4.2341471 -4.2553697 -4.2678633 -4.262876 -4.2405353 -4.2117577 -4.1915064 -4.1930523 -4.2145939 -4.2527676][-4.2632561 -4.26183 -4.2526069 -4.2432394 -4.2462316 -4.2624922 -4.27896 -4.2894535 -4.2889628 -4.2747211 -4.2538586 -4.2342944 -4.2317271 -4.2448134 -4.2730188][-4.2864633 -4.2863879 -4.2825837 -4.2771297 -4.2788486 -4.290029 -4.3006096 -4.3089724 -4.3097053 -4.3021355 -4.2883954 -4.2719889 -4.2676606 -4.274682 -4.2945328][-4.2890239 -4.2907047 -4.2919364 -4.291965 -4.2918425 -4.2974854 -4.303762 -4.3107905 -4.3131976 -4.3123703 -4.3064365 -4.2973456 -4.295608 -4.3015566 -4.3162823][-4.2827134 -4.2863173 -4.2900333 -4.2932091 -4.2935677 -4.2960749 -4.3000073 -4.3050947 -4.3091969 -4.3138485 -4.3165212 -4.3153734 -4.3174229 -4.3252845 -4.3370728]]...]
INFO - root - 2017-12-07 20:57:02.414864: step 53010, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 50h:39m:57s remains)
INFO - root - 2017-12-07 20:57:09.160548: step 53020, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 50h:15m:03s remains)
INFO - root - 2017-12-07 20:57:15.835158: step 53030, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 50h:32m:34s remains)
INFO - root - 2017-12-07 20:57:22.607220: step 53040, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 53h:44m:36s remains)
INFO - root - 2017-12-07 20:57:29.476335: step 53050, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 55h:40m:05s remains)
INFO - root - 2017-12-07 20:57:36.321813: step 53060, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 51h:24m:39s remains)
INFO - root - 2017-12-07 20:57:42.913172: step 53070, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 50h:07m:00s remains)
INFO - root - 2017-12-07 20:57:49.619620: step 53080, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 48h:50m:30s remains)
INFO - root - 2017-12-07 20:57:56.545069: step 53090, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 51h:36m:08s remains)
INFO - root - 2017-12-07 20:58:03.167530: step 53100, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 55h:54m:08s remains)
2017-12-07 20:58:03.941946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654057 -4.2807341 -4.2933545 -4.296473 -4.2786617 -4.2328949 -4.1584435 -4.0913749 -4.0732193 -4.112895 -4.1841669 -4.2403736 -4.2617888 -4.2374563 -4.1906719][-4.24296 -4.25225 -4.2661161 -4.2700973 -4.2512178 -4.2050729 -4.1303058 -4.061183 -4.0419731 -4.0882397 -4.1672773 -4.2300372 -4.2584376 -4.2382298 -4.18593][-4.2336521 -4.2324181 -4.2400827 -4.2384567 -4.2192221 -4.1796875 -4.1156945 -4.0594034 -4.0529947 -4.1062474 -4.1845274 -4.2463517 -4.2765942 -4.2599859 -4.2060366][-4.2462687 -4.2408342 -4.2396269 -4.2255421 -4.2017951 -4.1680183 -4.1167026 -4.0769982 -4.0855484 -4.141458 -4.210115 -4.2628069 -4.2909007 -4.2804337 -4.232089][-4.2579165 -4.2551913 -4.2494869 -4.2224622 -4.1896644 -4.1572456 -4.1126375 -4.0801182 -4.0979948 -4.15554 -4.2188773 -4.2663312 -4.2962146 -4.2964511 -4.2560158][-4.2619324 -4.264462 -4.2586422 -4.2215843 -4.1771607 -4.1371655 -4.0827851 -4.0427556 -4.0673451 -4.1314821 -4.1991611 -4.2522936 -4.2911263 -4.303421 -4.2709374][-4.2864289 -4.2860074 -4.2699885 -4.2139478 -4.1467505 -4.0879498 -4.0137753 -3.9640932 -4.006279 -4.0865593 -4.164299 -4.2268896 -4.2720761 -4.2898731 -4.260788][-4.3135529 -4.3091822 -4.2804961 -4.2082086 -4.1158667 -4.0254297 -3.9167511 -3.8465271 -3.9130564 -4.022336 -4.12442 -4.2059059 -4.2586737 -4.2774024 -4.249681][-4.328321 -4.3305264 -4.3029976 -4.2306509 -4.1289673 -4.0152612 -3.8843021 -3.7998867 -3.8657827 -3.9924366 -4.1161647 -4.2132812 -4.2684383 -4.2841053 -4.2605467][-4.3262916 -4.3378773 -4.3227673 -4.269959 -4.1870937 -4.0905337 -3.9867876 -3.9209104 -3.9558969 -4.0492558 -4.1509628 -4.2343116 -4.2814407 -4.2924957 -4.2721395][-4.3218813 -4.3378243 -4.3346925 -4.3038673 -4.2413111 -4.1598597 -4.0756497 -4.0221071 -4.0368433 -4.1018472 -4.1768484 -4.2449021 -4.2835088 -4.2912273 -4.2729707][-4.3207874 -4.3331718 -4.3307624 -4.3093457 -4.2593431 -4.1885147 -4.1174774 -4.0731421 -4.0860329 -4.1409564 -4.1998458 -4.2524543 -4.2798281 -4.281888 -4.2648816][-4.3150005 -4.3198204 -4.3148084 -4.2988076 -4.2611513 -4.202251 -4.1392789 -4.0974312 -4.1059408 -4.1497431 -4.1948676 -4.2388287 -4.2622323 -4.2610741 -4.246408][-4.31018 -4.304678 -4.2914166 -4.2740231 -4.2404027 -4.1904674 -4.1353126 -4.0979867 -4.104259 -4.141192 -4.1799397 -4.21793 -4.2397504 -4.2401214 -4.2347393][-4.3074894 -4.2902031 -4.2691193 -4.2500606 -4.2214365 -4.1801591 -4.1356692 -4.10781 -4.1145639 -4.1451163 -4.1767468 -4.2063665 -4.2271729 -4.2330208 -4.2363605]]...]
INFO - root - 2017-12-07 20:58:10.760508: step 53110, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 50h:37m:46s remains)
INFO - root - 2017-12-07 20:58:17.585284: step 53120, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.688 sec/batch; 53h:23m:52s remains)
INFO - root - 2017-12-07 20:58:24.498489: step 53130, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 57h:10m:40s remains)
INFO - root - 2017-12-07 20:58:31.360756: step 53140, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 52h:30m:33s remains)
INFO - root - 2017-12-07 20:58:38.239571: step 53150, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 54h:45m:21s remains)
INFO - root - 2017-12-07 20:58:45.010478: step 53160, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 49h:16m:49s remains)
INFO - root - 2017-12-07 20:58:51.793223: step 53170, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 52h:24m:22s remains)
INFO - root - 2017-12-07 20:58:58.590505: step 53180, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 56h:56m:31s remains)
INFO - root - 2017-12-07 20:59:05.402013: step 53190, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 51h:35m:00s remains)
INFO - root - 2017-12-07 20:59:12.016182: step 53200, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 51h:29m:08s remains)
2017-12-07 20:59:12.777829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0427175 -4.0022793 -4.0273151 -4.0852528 -4.147079 -4.1887469 -4.2007127 -4.2134051 -4.2462049 -4.2913785 -4.32867 -4.3473058 -4.3534431 -4.3475327 -4.323257][-4.0294924 -3.9737153 -3.9725008 -4.0120611 -4.07321 -4.1223583 -4.1407123 -4.1545753 -4.1898127 -4.2412939 -4.2907829 -4.3243909 -4.3456612 -4.3522472 -4.3367143][-4.0546927 -3.9874814 -3.9549384 -3.9624605 -4.0145869 -4.0662446 -4.0833449 -4.088778 -4.1189704 -4.1754222 -4.2394266 -4.2850342 -4.3136744 -4.3229141 -4.3097878][-4.09282 -4.027986 -3.9833772 -3.9747274 -4.0222034 -4.0729403 -4.0775604 -4.0572677 -4.0604725 -4.110023 -4.18253 -4.2408104 -4.2751117 -4.2835712 -4.2681074][-4.1388497 -4.0878491 -4.0503178 -4.0430517 -4.08146 -4.1148343 -4.0945005 -4.042213 -4.0135813 -4.0550747 -4.1360912 -4.2078595 -4.2490759 -4.2571583 -4.2421846][-4.1922331 -4.153892 -4.1271744 -4.1231251 -4.1428418 -4.1454577 -4.0960484 -4.0180416 -3.9683528 -4.0097127 -4.1062493 -4.1936259 -4.2420945 -4.2560325 -4.2507548][-4.2308826 -4.201982 -4.1825938 -4.1802511 -4.1810536 -4.1555295 -4.0869288 -3.9971137 -3.9392838 -3.9830794 -4.0926218 -4.1909962 -4.248642 -4.273591 -4.2814884][-4.246696 -4.2209392 -4.2037525 -4.1996465 -4.1846671 -4.1395364 -4.0650191 -3.9771566 -3.9212897 -3.9673507 -4.0817823 -4.1867018 -4.2543507 -4.292655 -4.3096995][-4.2475634 -4.2235508 -4.208262 -4.2009048 -4.1729436 -4.1155982 -4.0360866 -3.9516988 -3.9003863 -3.9434218 -4.0533204 -4.1604381 -4.2380114 -4.2871432 -4.3091578][-4.253015 -4.2346563 -4.2205133 -4.2067842 -4.1748562 -4.1168509 -4.0380735 -3.9576778 -3.9041498 -3.9287038 -4.015758 -4.1133895 -4.1956425 -4.2513437 -4.2782593][-4.27098 -4.2606125 -4.2469106 -4.2286029 -4.2032604 -4.1613836 -4.1002359 -4.0303836 -3.9675086 -3.9537332 -3.9963346 -4.0705609 -4.1475677 -4.2041183 -4.2338529][-4.2945929 -4.2915797 -4.277123 -4.2578254 -4.2419968 -4.2219348 -4.1869006 -4.131093 -4.0601878 -4.0133004 -4.0169277 -4.064714 -4.1266961 -4.1699672 -4.1907735][-4.3059816 -4.3056893 -4.2931652 -4.2763848 -4.2685781 -4.263905 -4.2514844 -4.2091618 -4.1403804 -4.0817695 -4.065299 -4.0952673 -4.1406436 -4.1644611 -4.1674051][-4.2892656 -4.2910438 -4.2820406 -4.2704949 -4.2689185 -4.2763653 -4.2816625 -4.2556233 -4.1988487 -4.1448183 -4.121953 -4.1389785 -4.1713495 -4.1832061 -4.1716661][-4.2473245 -4.2477908 -4.2408457 -4.2334423 -4.2408915 -4.2625408 -4.2837782 -4.2808018 -4.2452774 -4.2040839 -4.1822309 -4.1907806 -4.2119913 -4.2173133 -4.1972685]]...]
INFO - root - 2017-12-07 20:59:19.635626: step 53210, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.770 sec/batch; 59h:42m:25s remains)
INFO - root - 2017-12-07 20:59:26.466054: step 53220, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 52h:22m:44s remains)
INFO - root - 2017-12-07 20:59:33.311257: step 53230, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 49h:25m:54s remains)
INFO - root - 2017-12-07 20:59:40.242204: step 53240, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 51h:15m:21s remains)
INFO - root - 2017-12-07 20:59:47.113693: step 53250, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.715 sec/batch; 55h:29m:49s remains)
INFO - root - 2017-12-07 20:59:53.934552: step 53260, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 56h:51m:35s remains)
INFO - root - 2017-12-07 21:00:00.602156: step 53270, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 50h:16m:48s remains)
INFO - root - 2017-12-07 21:00:07.423909: step 53280, loss = 2.11, batch loss = 2.05 (11.7 examples/sec; 0.684 sec/batch; 53h:02m:35s remains)
INFO - root - 2017-12-07 21:00:14.262185: step 53290, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 55h:05m:38s remains)
INFO - root - 2017-12-07 21:00:20.924500: step 53300, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 50h:08m:38s remains)
2017-12-07 21:00:21.589804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2672119 -4.2729278 -4.2790008 -4.27483 -4.2661109 -4.2641478 -4.265305 -4.2658577 -4.2652316 -4.2624383 -4.2619352 -4.2681193 -4.2711725 -4.266417 -4.2584014][-4.2157364 -4.22397 -4.2355151 -4.2345967 -4.2183604 -4.2136664 -4.217176 -4.2192392 -4.22183 -4.2189193 -4.2151656 -4.2158809 -4.216423 -4.2047787 -4.1816874][-4.1585655 -4.1761432 -4.19911 -4.2047334 -4.1811514 -4.1732488 -4.1751966 -4.1738644 -4.1795425 -4.1802082 -4.1729536 -4.1702261 -4.1678629 -4.1479936 -4.1068878][-4.1003866 -4.135551 -4.1667943 -4.1762524 -4.1487579 -4.1286159 -4.1236968 -4.1143823 -4.1192737 -4.1277285 -4.1271882 -4.1276374 -4.1247473 -4.1060977 -4.0583134][-4.0502291 -4.1014495 -4.1386428 -4.1427078 -4.112102 -4.0826945 -4.065762 -4.0466604 -4.0495667 -4.0635214 -4.0689874 -4.0764871 -4.0789552 -4.076231 -4.0398359][-4.0352473 -4.0882058 -4.1283336 -4.125865 -4.0928345 -4.0596938 -4.031281 -3.9971611 -3.9940457 -4.0054593 -4.0130186 -4.0313177 -4.0441089 -4.0622911 -4.0504484][-4.0310473 -4.0740962 -4.1143885 -4.1126952 -4.0847187 -4.0510182 -4.0095234 -3.9519651 -3.9326992 -3.940213 -3.9496679 -3.9783266 -4.0078745 -4.0423031 -4.0554905][-4.01069 -4.03851 -4.077476 -4.0819979 -4.0598245 -4.0335412 -3.9843695 -3.9059367 -3.8671234 -3.8732927 -3.88194 -3.9125493 -3.9519267 -3.9958804 -4.0232697][-3.9682043 -3.9742086 -4.0089455 -4.027329 -4.0295281 -4.022788 -3.9828677 -3.9104905 -3.8631217 -3.84758 -3.8341806 -3.8469257 -3.8855531 -3.93891 -3.9815078][-3.9067767 -3.90515 -3.9436226 -3.9897459 -4.023881 -4.0358777 -4.0199461 -3.9857972 -3.9515846 -3.920763 -3.8880847 -3.8777683 -3.8987246 -3.9459326 -3.9959631][-3.8747494 -3.8776166 -3.9220641 -3.98417 -4.0447235 -4.0780563 -4.0922179 -4.0988989 -4.0913839 -4.0664983 -4.0321603 -4.0039773 -3.9962513 -4.0194583 -4.0583377][-3.9284179 -3.9318936 -3.9689295 -4.0218096 -4.086206 -4.136426 -4.1684847 -4.1936226 -4.2014456 -4.186552 -4.1614676 -4.1290073 -4.1003981 -4.0946894 -4.1169066][-4.0270877 -4.0250177 -4.0507092 -4.08536 -4.1314015 -4.1739469 -4.2079315 -4.2331171 -4.241776 -4.2362556 -4.2235651 -4.2004561 -4.1631546 -4.1295381 -4.1248622][-4.1208057 -4.1204948 -4.13688 -4.1499939 -4.160203 -4.1738486 -4.1982479 -4.2197576 -4.2308769 -4.2269325 -4.2198181 -4.2065573 -4.1650777 -4.1062517 -4.0767527][-4.1732574 -4.1768847 -4.1867476 -4.1852684 -4.1645017 -4.1418962 -4.1543455 -4.1796412 -4.1959291 -4.1936955 -4.1893196 -4.1881976 -4.1510005 -4.0819044 -4.0386567]]...]
INFO - root - 2017-12-07 21:00:28.409319: step 53310, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 49h:41m:59s remains)
INFO - root - 2017-12-07 21:00:35.376090: step 53320, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.702 sec/batch; 54h:24m:52s remains)
INFO - root - 2017-12-07 21:00:42.172007: step 53330, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 51h:48m:38s remains)
INFO - root - 2017-12-07 21:00:48.956658: step 53340, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 49h:20m:37s remains)
INFO - root - 2017-12-07 21:00:55.731345: step 53350, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 50h:04m:50s remains)
INFO - root - 2017-12-07 21:01:02.631478: step 53360, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 55h:33m:27s remains)
INFO - root - 2017-12-07 21:01:09.444533: step 53370, loss = 2.11, batch loss = 2.05 (11.3 examples/sec; 0.707 sec/batch; 54h:50m:34s remains)
INFO - root - 2017-12-07 21:01:16.253276: step 53380, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 53h:56m:47s remains)
INFO - root - 2017-12-07 21:01:22.954252: step 53390, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 48h:49m:27s remains)
INFO - root - 2017-12-07 21:01:29.672259: step 53400, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 49h:48m:26s remains)
2017-12-07 21:01:30.503706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3406134 -4.3409781 -4.3363843 -4.3299556 -4.3257332 -4.3242831 -4.3274784 -4.3362637 -4.3461981 -4.3510308 -4.3456259 -4.3317909 -4.3166881 -4.3015385 -4.2868404][-4.3416615 -4.3434973 -4.3392038 -4.3329911 -4.3263 -4.3195558 -4.316762 -4.321928 -4.3321624 -4.3389654 -4.335258 -4.322834 -4.3084517 -4.295043 -4.2826781][-4.3441396 -4.3466187 -4.3406672 -4.330482 -4.3145766 -4.2954011 -4.2816944 -4.278399 -4.284687 -4.2908044 -4.2891841 -4.2803411 -4.2684312 -4.260097 -4.2557178][-4.3510113 -4.3537879 -4.3460207 -4.3307357 -4.3037987 -4.2689204 -4.2416525 -4.226202 -4.2246132 -4.2306085 -4.2339263 -4.2333097 -4.22752 -4.2238054 -4.2248597][-4.3587341 -4.3608651 -4.3490977 -4.3260908 -4.2863603 -4.2359877 -4.1944518 -4.1663895 -4.1582623 -4.169836 -4.184937 -4.195118 -4.1964865 -4.1973028 -4.1994514][-4.363729 -4.3633943 -4.3428011 -4.3068352 -4.2501068 -4.1828561 -4.1248913 -4.0827017 -4.0714712 -4.090755 -4.1216445 -4.1466846 -4.1548214 -4.157732 -4.1589036][-4.3629255 -4.3548408 -4.3210406 -4.2682791 -4.1937189 -4.1072097 -4.0296545 -3.9694104 -3.9509497 -3.9812858 -4.0310698 -4.072825 -4.0869846 -4.0881014 -4.0866246][-4.351357 -4.3315392 -4.2828069 -4.2133374 -4.1239028 -4.0224962 -3.9279964 -3.8480544 -3.8135386 -3.853548 -3.9263716 -3.9873223 -4.0102983 -4.0127568 -4.0129061][-4.3357677 -4.3071275 -4.2492261 -4.1709576 -4.0769658 -3.9768422 -3.8867319 -3.8108444 -3.7755671 -3.8172917 -3.8974092 -3.9647546 -3.9910436 -3.9932022 -3.9944038][-4.3317337 -4.303772 -4.2499437 -4.1800876 -4.1022949 -4.0269303 -3.9677083 -3.9257488 -3.9099135 -3.9392643 -3.9915259 -4.0356827 -4.0504541 -4.0480447 -4.0484862][-4.3452659 -4.3292508 -4.2937069 -4.2436447 -4.188199 -4.1396461 -4.1086049 -4.0929732 -4.0919571 -4.1111236 -4.1387095 -4.1577291 -4.1585846 -4.1509571 -4.1489019][-4.3591104 -4.3573346 -4.3432302 -4.3156481 -4.2810893 -4.253046 -4.2393246 -4.2346473 -4.2383661 -4.2512832 -4.2656584 -4.26979 -4.26339 -4.2534175 -4.2487411][-4.3634968 -4.3664541 -4.3629951 -4.3505082 -4.3321433 -4.3197703 -4.31581 -4.3151445 -4.3194394 -4.3265209 -4.3314638 -4.3296309 -4.3228707 -4.3148956 -4.309803][-4.3654232 -4.36751 -4.3657513 -4.3612223 -4.3545022 -4.3527012 -4.3542414 -4.3551922 -4.3572946 -4.3588328 -4.3577285 -4.3531981 -4.3477669 -4.3417354 -4.33657][-4.365346 -4.3664069 -4.3642797 -4.3624573 -4.3606052 -4.3618689 -4.3639936 -4.3645506 -4.3649025 -4.3644571 -4.3632441 -4.3610034 -4.3579063 -4.3536816 -4.3489404]]...]
INFO - root - 2017-12-07 21:01:37.196785: step 53410, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 52h:47m:22s remains)
INFO - root - 2017-12-07 21:01:43.902483: step 53420, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.612 sec/batch; 47h:25m:11s remains)
INFO - root - 2017-12-07 21:01:50.710985: step 53430, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 50h:09m:04s remains)
INFO - root - 2017-12-07 21:01:57.588609: step 53440, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.737 sec/batch; 57h:09m:31s remains)
INFO - root - 2017-12-07 21:02:04.392857: step 53450, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 56h:11m:36s remains)
INFO - root - 2017-12-07 21:02:11.240227: step 53460, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 52h:15m:43s remains)
INFO - root - 2017-12-07 21:02:17.866728: step 53470, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.611 sec/batch; 47h:20m:13s remains)
INFO - root - 2017-12-07 21:02:24.639403: step 53480, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.631 sec/batch; 48h:54m:19s remains)
INFO - root - 2017-12-07 21:02:31.527032: step 53490, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 56h:19m:28s remains)
INFO - root - 2017-12-07 21:02:38.056003: step 53500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 55h:17m:42s remains)
2017-12-07 21:02:38.758917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2308955 -4.2076721 -4.2006192 -4.2110248 -4.2312207 -4.2490859 -4.2605219 -4.2727695 -4.2745438 -4.2733364 -4.2728286 -4.2719159 -4.2700987 -4.2694187 -4.2640271][-4.2322903 -4.2104921 -4.2016373 -4.2110662 -4.2291861 -4.2444777 -4.252604 -4.2647486 -4.2644506 -4.25865 -4.2540278 -4.2508912 -4.2492404 -4.2529125 -4.2546225][-4.2361064 -4.220458 -4.2106509 -4.2105918 -4.219213 -4.2289295 -4.2361507 -4.2503643 -4.25202 -4.2481093 -4.2444406 -4.2372327 -4.2289033 -4.2282209 -4.2310619][-4.2258224 -4.2201152 -4.2162828 -4.2140846 -4.2145734 -4.2174234 -4.2231636 -4.2383118 -4.2389188 -4.2339187 -4.2293882 -4.2188454 -4.2048807 -4.1998186 -4.1990047][-4.2049036 -4.2049561 -4.2075772 -4.207345 -4.2003675 -4.196115 -4.2010736 -4.2177043 -4.2211 -4.21355 -4.205925 -4.1915379 -4.1757259 -4.1677895 -4.1647258][-4.1655917 -4.1727886 -4.1786737 -4.17704 -4.1639585 -4.1521192 -4.1545997 -4.1758013 -4.1930361 -4.1978407 -4.1970468 -4.1881461 -4.1777148 -4.1690063 -4.1605039][-4.1242371 -4.1334705 -4.1384106 -4.1323986 -4.111033 -4.0876827 -4.0836363 -4.1117268 -4.1471338 -4.1702747 -4.1831646 -4.1862054 -4.1878343 -4.1869698 -4.1805415][-4.1182313 -4.1173964 -4.1091971 -4.0928483 -4.0604777 -4.0249267 -4.0135231 -4.0415764 -4.0865254 -4.1260619 -4.1580267 -4.178556 -4.1930847 -4.200079 -4.2012124][-4.1453128 -4.139955 -4.1260233 -4.1060205 -4.0752826 -4.0454087 -4.0311141 -4.0427456 -4.0737753 -4.1118164 -4.151001 -4.181006 -4.2023091 -4.2115974 -4.2151828][-4.1859174 -4.1851358 -4.174305 -4.156415 -4.129375 -4.1046605 -4.0884724 -4.0871558 -4.1026173 -4.1262765 -4.15891 -4.1879721 -4.2060266 -4.2124147 -4.2161622][-4.2117734 -4.216085 -4.21182 -4.1992049 -4.1797748 -4.1622548 -4.1520081 -4.1505613 -4.1646581 -4.181623 -4.2033062 -4.2222753 -4.2286077 -4.2264676 -4.2208781][-4.2203908 -4.2269464 -4.2286477 -4.22908 -4.2275767 -4.2256875 -4.2227888 -4.2193971 -4.2284 -4.2371297 -4.2481289 -4.2567692 -4.2549491 -4.2473888 -4.2344308][-4.2310972 -4.2358365 -4.2396541 -4.2463117 -4.2531977 -4.2587991 -4.2595081 -4.2535963 -4.2528763 -4.2542462 -4.2619228 -4.2684441 -4.2630892 -4.2508049 -4.233037][-4.2459021 -4.2466722 -4.25052 -4.2584496 -4.2671456 -4.2742243 -4.2744927 -4.2653995 -4.2566905 -4.2510924 -4.2560849 -4.2623491 -4.2550454 -4.2391891 -4.2167735][-4.2589912 -4.2560906 -4.2597828 -4.2686405 -4.2767987 -4.2812805 -4.2778745 -4.2661448 -4.254281 -4.2471051 -4.2519679 -4.2563791 -4.2470813 -4.2262235 -4.1955733]]...]
INFO - root - 2017-12-07 21:02:45.516789: step 53510, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 50h:47m:26s remains)
INFO - root - 2017-12-07 21:02:52.407963: step 53520, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 56h:23m:03s remains)
INFO - root - 2017-12-07 21:02:59.143978: step 53530, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 51h:20m:00s remains)
INFO - root - 2017-12-07 21:03:06.000130: step 53540, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 49h:24m:34s remains)
INFO - root - 2017-12-07 21:03:12.779296: step 53550, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.637 sec/batch; 49h:21m:14s remains)
INFO - root - 2017-12-07 21:03:19.633022: step 53560, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 55h:30m:10s remains)
INFO - root - 2017-12-07 21:03:26.662240: step 53570, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 55h:28m:33s remains)
INFO - root - 2017-12-07 21:03:33.310925: step 53580, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 49h:50m:30s remains)
INFO - root - 2017-12-07 21:03:40.065098: step 53590, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 51h:09m:48s remains)
INFO - root - 2017-12-07 21:03:46.747071: step 53600, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 54h:58m:38s remains)
2017-12-07 21:03:47.478404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2126608 -4.2159815 -4.2152662 -4.2188683 -4.2357516 -4.2646651 -4.2828169 -4.2892632 -4.29884 -4.3039055 -4.3017573 -4.291667 -4.2714982 -4.2494807 -4.2390871][-4.209785 -4.2079754 -4.2051468 -4.2064595 -4.2207532 -4.2452121 -4.26104 -4.2715707 -4.293437 -4.3130722 -4.32379 -4.3230481 -4.306282 -4.2860308 -4.2740755][-4.2031522 -4.2019467 -4.2012324 -4.1987829 -4.20258 -4.2079992 -4.2056656 -4.2155466 -4.2511368 -4.290575 -4.3196011 -4.3303208 -4.3188205 -4.3031573 -4.2954965][-4.2025738 -4.2029676 -4.2038927 -4.1906161 -4.172574 -4.1441269 -4.1072049 -4.1149931 -4.1708589 -4.2376218 -4.2902107 -4.3155 -4.3092227 -4.2954907 -4.2914166][-4.2084041 -4.2077827 -4.2049389 -4.1761861 -4.1272106 -4.0535612 -3.9700096 -3.9762137 -4.061502 -4.1587672 -4.2349138 -4.2771497 -4.2773204 -4.2640352 -4.2611094][-4.2168794 -4.2132831 -4.2035227 -4.1552639 -4.0749817 -3.9579749 -3.8208117 -3.831089 -3.9576893 -4.0830445 -4.1761837 -4.232698 -4.2399731 -4.2252445 -4.2168779][-4.2187939 -4.2139521 -4.2001019 -4.1425281 -4.0477376 -3.9076486 -3.7377522 -3.7539117 -3.9045916 -4.0379324 -4.1381497 -4.2002077 -4.2142916 -4.1996293 -4.1831431][-4.2295604 -4.225174 -4.2106776 -4.1622949 -4.086782 -3.9741664 -3.8329153 -3.8417881 -3.9603221 -4.0634155 -4.1475744 -4.2045918 -4.21976 -4.2071619 -4.186192][-4.2486672 -4.2415209 -4.22582 -4.1947546 -4.1542511 -4.0883603 -4.0012889 -4.0067616 -4.0761313 -4.1353168 -4.1880856 -4.2312851 -4.2475843 -4.2416286 -4.2249646][-4.2711587 -4.2641168 -4.2504525 -4.2326117 -4.2140212 -4.18064 -4.1376977 -4.1462502 -4.1833434 -4.2074461 -4.2309055 -4.2583256 -4.2750998 -4.2779946 -4.2710085][-4.2891078 -4.2847691 -4.2758336 -4.2634892 -4.2554712 -4.2382517 -4.2195554 -4.2291064 -4.2483349 -4.2529488 -4.2591686 -4.2752671 -4.291986 -4.2998638 -4.2992406][-4.2969646 -4.2950668 -4.2901306 -4.282258 -4.280232 -4.2734456 -4.2658834 -4.27011 -4.2771678 -4.2735906 -4.2727604 -4.2844687 -4.298728 -4.306457 -4.3077812][-4.294476 -4.2957993 -4.2948818 -4.2897797 -4.2902064 -4.2904692 -4.2877793 -4.2846189 -4.281765 -4.2752953 -4.2735071 -4.2837925 -4.295619 -4.3018694 -4.3029776][-4.2871017 -4.2929893 -4.2958374 -4.2935424 -4.2944036 -4.2961583 -4.291944 -4.2810659 -4.2717743 -4.2667112 -4.2672596 -4.2771778 -4.28679 -4.2902932 -4.2888231][-4.2772803 -4.2852416 -4.2909384 -4.2913713 -4.2931905 -4.2932291 -4.2836628 -4.2659149 -4.253356 -4.2515826 -4.2566071 -4.2666888 -4.2734637 -4.2735043 -4.2697473]]...]
INFO - root - 2017-12-07 21:03:54.336650: step 53610, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 50h:35m:15s remains)
INFO - root - 2017-12-07 21:04:01.234067: step 53620, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.751 sec/batch; 58h:08m:23s remains)
INFO - root - 2017-12-07 21:04:08.050008: step 53630, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 55h:30m:19s remains)
INFO - root - 2017-12-07 21:04:14.846571: step 53640, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 54h:00m:57s remains)
INFO - root - 2017-12-07 21:04:21.597601: step 53650, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 49h:31m:25s remains)
INFO - root - 2017-12-07 21:04:28.294376: step 53660, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 50h:33m:18s remains)
INFO - root - 2017-12-07 21:04:35.168519: step 53670, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 56h:16m:08s remains)
INFO - root - 2017-12-07 21:04:41.947048: step 53680, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 54h:26m:20s remains)
INFO - root - 2017-12-07 21:04:48.670173: step 53690, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 52h:21m:35s remains)
INFO - root - 2017-12-07 21:04:55.320059: step 53700, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 49h:41m:29s remains)
2017-12-07 21:04:56.079498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2635937 -4.26768 -4.2718077 -4.2681484 -4.2592053 -4.2429056 -4.23249 -4.223208 -4.2121296 -4.2036424 -4.2145462 -4.2304215 -4.2389264 -4.2525692 -4.26881][-4.3041396 -4.3060794 -4.306426 -4.30014 -4.2925243 -4.2802215 -4.2697129 -4.2629013 -4.2572513 -4.2438469 -4.2375507 -4.2348132 -4.2297521 -4.246016 -4.2675481][-4.3274512 -4.3283663 -4.3251038 -4.3161988 -4.3083267 -4.2943883 -4.2792077 -4.2707562 -4.2742181 -4.2666092 -4.2506032 -4.2326779 -4.2209449 -4.2432718 -4.2673583][-4.3081627 -4.3069673 -4.3021054 -4.2955122 -4.2886157 -4.2687969 -4.250206 -4.2462549 -4.2568612 -4.2549229 -4.2417889 -4.2228856 -4.2144413 -4.2415686 -4.2665787][-4.2935309 -4.2893648 -4.2835684 -4.279788 -4.2732086 -4.2467794 -4.2171021 -4.2039738 -4.2101736 -4.2097135 -4.204318 -4.1979718 -4.206707 -4.2400713 -4.2641125][-4.2927222 -4.2896714 -4.2829294 -4.2766 -4.2642794 -4.2326779 -4.1858935 -4.1487145 -4.1353722 -4.1286917 -4.1324797 -4.1475759 -4.1819658 -4.2297149 -4.2564449][-4.2811589 -4.2788286 -4.26299 -4.24058 -4.2210217 -4.1902857 -4.1320419 -4.0571933 -4.0028605 -3.9869654 -4.0185347 -4.0754447 -4.1429195 -4.2103758 -4.2450294][-4.2480583 -4.2368855 -4.2005806 -4.1560426 -4.1301703 -4.1089597 -4.0466957 -3.94302 -3.8484442 -3.8204007 -3.900038 -4.0080838 -4.1065316 -4.1901984 -4.2350893][-4.2169247 -4.1896954 -4.1279693 -4.0581717 -4.0239153 -4.01206 -3.9538951 -3.8519258 -3.7613771 -3.7478433 -3.8580356 -3.9857154 -4.0945678 -4.1803374 -4.2306051][-4.2088566 -4.1720877 -4.1023183 -4.0267677 -3.987149 -3.9766417 -3.9266181 -3.8485022 -3.809057 -3.834636 -3.9295394 -4.0233741 -4.1061625 -4.1774421 -4.2255292][-4.21366 -4.1774907 -4.1220245 -4.0639997 -4.0290337 -4.0117517 -3.9689364 -3.9194155 -3.9141483 -3.9567208 -4.0247192 -4.0806422 -4.1353436 -4.1902823 -4.2313271][-4.2185135 -4.1878428 -4.1529408 -4.1152883 -4.0895119 -4.07384 -4.0425076 -4.0095692 -4.0101376 -4.0447922 -4.0934367 -4.1328692 -4.1720281 -4.2149463 -4.2491293][-4.2068224 -4.1863446 -4.1755147 -4.1594696 -4.1479568 -4.1454234 -4.1314783 -4.1083107 -4.1036906 -4.121006 -4.15419 -4.186058 -4.2136211 -4.243844 -4.27155][-4.1923976 -4.1890635 -4.1975551 -4.2012954 -4.2005744 -4.2033048 -4.1970096 -4.1808624 -4.175559 -4.1852503 -4.2087293 -4.2341385 -4.2520738 -4.2718959 -4.2936811][-4.1763453 -4.193047 -4.2170076 -4.2355142 -4.2412763 -4.246295 -4.2417521 -4.2306094 -4.2252226 -4.2331076 -4.2490578 -4.2653985 -4.2747564 -4.2878528 -4.3037572]]...]
INFO - root - 2017-12-07 21:05:02.996207: step 53710, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.746 sec/batch; 57h:44m:57s remains)
INFO - root - 2017-12-07 21:05:09.862597: step 53720, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 53h:29m:06s remains)
INFO - root - 2017-12-07 21:05:16.674201: step 53730, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 48h:21m:15s remains)
INFO - root - 2017-12-07 21:05:23.568989: step 53740, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 55h:11m:37s remains)
INFO - root - 2017-12-07 21:05:30.451437: step 53750, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 56h:41m:47s remains)
INFO - root - 2017-12-07 21:05:37.240220: step 53760, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 54h:53m:28s remains)
INFO - root - 2017-12-07 21:05:44.092134: step 53770, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 49h:12m:22s remains)
INFO - root - 2017-12-07 21:05:50.803279: step 53780, loss = 2.11, batch loss = 2.05 (12.6 examples/sec; 0.635 sec/batch; 49h:11m:07s remains)
INFO - root - 2017-12-07 21:05:57.539471: step 53790, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 52h:00m:59s remains)
INFO - root - 2017-12-07 21:06:04.227740: step 53800, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 55h:37m:48s remains)
2017-12-07 21:06:04.984532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2132239 -4.2221303 -4.2453289 -4.2618337 -4.2643156 -4.2562518 -4.2521405 -4.2528448 -4.2530575 -4.2483578 -4.2287531 -4.2016196 -4.1609068 -4.1268468 -4.1435738][-4.1986942 -4.2215862 -4.2579112 -4.2799153 -4.2837505 -4.2732396 -4.2688613 -4.2734723 -4.2761927 -4.269619 -4.2521653 -4.2320967 -4.1964498 -4.1520534 -4.1520534][-4.2099366 -4.2376232 -4.2735314 -4.2866135 -4.2767506 -4.2560816 -4.2485824 -4.2543068 -4.2645097 -4.263844 -4.2564583 -4.2560205 -4.2436023 -4.2142324 -4.2073307][-4.251245 -4.2697015 -4.2914319 -4.2867026 -4.2490225 -4.2103891 -4.1984568 -4.2054358 -4.2249432 -4.233882 -4.2354946 -4.2493992 -4.2600918 -4.2600403 -4.2624731][-4.2850857 -4.2885423 -4.2912216 -4.2637439 -4.1943712 -4.1296711 -4.099577 -4.1003895 -4.13256 -4.158906 -4.1696291 -4.1925607 -4.2187219 -4.2398405 -4.2607694][-4.3016973 -4.2902579 -4.2743196 -4.2235785 -4.1250925 -4.0257568 -3.9609988 -3.9400773 -3.9790578 -4.0248985 -4.0460243 -4.0806017 -4.1236253 -4.163414 -4.2080483][-4.3007393 -4.2780104 -4.2450352 -4.1760621 -4.0621262 -3.938971 -3.8414822 -3.7956705 -3.8354566 -3.8933611 -3.9133532 -3.9533374 -4.0095696 -4.0641918 -4.1314268][-4.283525 -4.2619033 -4.2264824 -4.1617303 -4.0609565 -3.9510012 -3.8616624 -3.8185003 -3.8499379 -3.8925347 -3.8836293 -3.9013028 -3.9535677 -4.0046191 -4.0738592][-4.2683363 -4.2576752 -4.2340279 -4.1906223 -4.1247773 -4.0507903 -3.9970341 -3.9757872 -3.987813 -3.9974086 -3.9569211 -3.9444623 -3.974874 -4.0073266 -4.0592632][-4.2585335 -4.2627807 -4.2579036 -4.2383423 -4.2044983 -4.1601753 -4.1279783 -4.1141219 -4.1093774 -4.0992737 -4.0528059 -4.0250926 -4.0346737 -4.0480404 -4.0839996][-4.2634068 -4.2768345 -4.2836027 -4.2811527 -4.2679925 -4.243361 -4.2202849 -4.206769 -4.1944256 -4.1824412 -4.1476784 -4.1195025 -4.1160455 -4.1192555 -4.1447024][-4.2807384 -4.2916431 -4.2963929 -4.2956495 -4.2907376 -4.2782083 -4.263628 -4.2552009 -4.2498517 -4.2490563 -4.234849 -4.2166662 -4.20916 -4.2065234 -4.2192497][-4.2999415 -4.3065162 -4.3064332 -4.3031 -4.2992554 -4.2901955 -4.2817736 -4.2813554 -4.2866497 -4.2936783 -4.2935438 -4.2868557 -4.2822986 -4.2766905 -4.2799673][-4.3121519 -4.3147573 -4.31188 -4.3075223 -4.3038526 -4.2958755 -4.2919354 -4.2972918 -4.3091283 -4.319212 -4.3223653 -4.3199716 -4.3179016 -4.3144383 -4.3153877][-4.313808 -4.3121972 -4.3076358 -4.3048229 -4.3020158 -4.2981834 -4.2993288 -4.30775 -4.3197274 -4.3278804 -4.331089 -4.3307009 -4.3308592 -4.3285322 -4.3291574]]...]
INFO - root - 2017-12-07 21:06:11.860867: step 53810, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 50h:32m:51s remains)
INFO - root - 2017-12-07 21:06:18.633717: step 53820, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 55h:59m:35s remains)
INFO - root - 2017-12-07 21:06:25.511201: step 53830, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.716 sec/batch; 55h:24m:24s remains)
INFO - root - 2017-12-07 21:06:32.283825: step 53840, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 52h:15m:52s remains)
INFO - root - 2017-12-07 21:06:39.000115: step 53850, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 49h:39m:41s remains)
INFO - root - 2017-12-07 21:06:45.831118: step 53860, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 52h:02m:24s remains)
INFO - root - 2017-12-07 21:06:52.671816: step 53870, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 57h:01m:05s remains)
INFO - root - 2017-12-07 21:06:59.572768: step 53880, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 55h:22m:58s remains)
INFO - root - 2017-12-07 21:07:06.181582: step 53890, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 49h:50m:44s remains)
INFO - root - 2017-12-07 21:07:12.854704: step 53900, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 50h:57m:54s remains)
2017-12-07 21:07:13.660730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3074856 -4.3004971 -4.2970934 -4.2940683 -4.2868533 -4.2770481 -4.2643533 -4.2459321 -4.2385054 -4.24571 -4.2576919 -4.2708488 -4.2838869 -4.2963533 -4.3048973][-4.2993994 -4.2979341 -4.3019314 -4.3060079 -4.3018131 -4.2862883 -4.264626 -4.2344375 -4.2220922 -4.228816 -4.2393665 -4.2506013 -4.2597232 -4.2697821 -4.2782979][-4.299737 -4.3051538 -4.316061 -4.3247018 -4.3240871 -4.3041897 -4.2771873 -4.2459216 -4.2345581 -4.2405267 -4.2478261 -4.2531419 -4.254488 -4.2556534 -4.2585635][-4.2970471 -4.3050747 -4.31496 -4.32112 -4.3183022 -4.2906828 -4.2611504 -4.2387338 -4.2384057 -4.250329 -4.25987 -4.2630916 -4.2577429 -4.2516952 -4.2480021][-4.2899308 -4.2971931 -4.3022351 -4.2988353 -4.2851429 -4.2450409 -4.2061057 -4.1871619 -4.1995392 -4.228322 -4.2526665 -4.2645626 -4.2610345 -4.2532568 -4.2467456][-4.2670364 -4.2654357 -4.2599854 -4.2449155 -4.2152958 -4.1608753 -4.1097589 -4.0867004 -4.1156945 -4.1687336 -4.2120557 -4.2387505 -4.2450738 -4.243639 -4.2419195][-4.2295337 -4.2128944 -4.1944633 -4.1695356 -4.1247654 -4.0481663 -3.9714258 -3.9314775 -3.9813211 -4.073349 -4.1423254 -4.186161 -4.2050796 -4.2132106 -4.2210555][-4.1969748 -4.1680822 -4.1407619 -4.1106267 -4.0519867 -3.9532595 -3.8430414 -3.7658138 -3.8179555 -3.9454536 -4.0422435 -4.1018372 -4.1364851 -4.1607103 -4.1847348][-4.1929579 -4.1602707 -4.1325006 -4.1080308 -4.0568762 -3.9681489 -3.8624876 -3.7689533 -3.7937779 -3.9044118 -3.9937487 -4.0503445 -4.0887151 -4.1220465 -4.1564198][-4.2216525 -4.2016649 -4.1880746 -4.1758084 -4.141685 -4.0822816 -4.0056067 -3.9303775 -3.9298148 -3.9929688 -4.0449815 -4.07758 -4.103745 -4.129467 -4.1566854][-4.2532396 -4.2474718 -4.24705 -4.2444448 -4.2243 -4.1863451 -4.135654 -4.082756 -4.0694566 -4.1019821 -4.1311626 -4.1465158 -4.1601543 -4.1738181 -4.1840525][-4.2698135 -4.2750888 -4.2840781 -4.2854662 -4.2749453 -4.25581 -4.2257533 -4.18965 -4.172874 -4.1912632 -4.212204 -4.2209458 -4.227705 -4.2337518 -4.2315917][-4.2742286 -4.2832804 -4.2933817 -4.2975273 -4.2952061 -4.2879715 -4.2702775 -4.245615 -4.2329774 -4.249126 -4.2708049 -4.27968 -4.2815003 -4.2822056 -4.27443][-4.2739067 -4.281527 -4.2895508 -4.2919283 -4.2894859 -4.28416 -4.271544 -4.2542572 -4.2482367 -4.2649474 -4.2895045 -4.3044333 -4.3074379 -4.3072195 -4.3001132][-4.2759795 -4.2783194 -4.2813945 -4.2810369 -4.277287 -4.2709732 -4.261045 -4.248239 -4.2437687 -4.2570753 -4.279645 -4.2986174 -4.3077941 -4.3135309 -4.3126626]]...]
INFO - root - 2017-12-07 21:07:20.482750: step 53910, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 51h:22m:00s remains)
INFO - root - 2017-12-07 21:07:27.189914: step 53920, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 49h:35m:31s remains)
INFO - root - 2017-12-07 21:07:33.980966: step 53930, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 54h:46m:27s remains)
INFO - root - 2017-12-07 21:07:40.778021: step 53940, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 55h:42m:07s remains)
INFO - root - 2017-12-07 21:07:47.688768: step 53950, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 53h:51m:38s remains)
INFO - root - 2017-12-07 21:07:54.469399: step 53960, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 52h:42m:25s remains)
INFO - root - 2017-12-07 21:08:01.294693: step 53970, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.648 sec/batch; 50h:10m:13s remains)
INFO - root - 2017-12-07 21:08:08.173642: step 53980, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 52h:35m:00s remains)
INFO - root - 2017-12-07 21:08:15.043516: step 53990, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 56h:35m:28s remains)
INFO - root - 2017-12-07 21:08:21.736138: step 54000, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 52h:50m:35s remains)
2017-12-07 21:08:22.421104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2718987 -4.2741046 -4.2753778 -4.2756877 -4.27476 -4.2727108 -4.2700243 -4.2681441 -4.2682891 -4.2695131 -4.2712331 -4.2737045 -4.2762814 -4.2791276 -4.281672][-4.2714505 -4.2727113 -4.2730289 -4.2725353 -4.2708788 -4.268136 -4.265182 -4.263443 -4.2634168 -4.2642536 -4.2660093 -4.2688975 -4.2722845 -4.2761354 -4.2795668][-4.2678385 -4.266201 -4.2642035 -4.2622404 -4.2600913 -4.256846 -4.253685 -4.252471 -4.252759 -4.2539506 -4.2559776 -4.2597156 -4.264575 -4.2701983 -4.2755008][-4.2589722 -4.2548985 -4.2510533 -4.2487369 -4.2470136 -4.244061 -4.2411871 -4.2393937 -4.237875 -4.2375674 -4.2390871 -4.2437639 -4.2507782 -4.2594376 -4.2683363][-4.2422805 -4.2373586 -4.23352 -4.2321534 -4.2310104 -4.2278075 -4.2246838 -4.2216439 -4.2173495 -4.2139707 -4.2139144 -4.2189832 -4.2281251 -4.23977 -4.2524037][-4.2222633 -4.219748 -4.2195463 -4.2220116 -4.2225595 -4.2183175 -4.213449 -4.2076588 -4.199892 -4.1922264 -4.1882834 -4.1904674 -4.1979871 -4.2090683 -4.2231879][-4.2084827 -4.2085004 -4.2120042 -4.2174292 -4.2188568 -4.2140107 -4.2078991 -4.200119 -4.1897597 -4.178514 -4.1698079 -4.166635 -4.168695 -4.1753111 -4.1875024][-4.2154269 -4.2150335 -4.2182188 -4.2222171 -4.221755 -4.2169275 -4.2111797 -4.2034607 -4.1943989 -4.1842008 -4.1737823 -4.1663404 -4.162066 -4.1609941 -4.1668472][-4.2301512 -4.2284784 -4.2309003 -4.2350616 -4.2359776 -4.2343359 -4.2321815 -4.2284384 -4.2239647 -4.2169185 -4.2066717 -4.1968484 -4.1884089 -4.1816607 -4.1820741][-4.2374649 -4.23574 -4.2393136 -4.2467694 -4.2511425 -4.2536235 -4.2557054 -4.257422 -4.2598467 -4.2582483 -4.25022 -4.2402658 -4.2308974 -4.2221146 -4.2193751][-4.2453475 -4.2439904 -4.2494421 -4.2595415 -4.2662711 -4.2702932 -4.2740421 -4.2784162 -4.2843437 -4.2870822 -4.2826948 -4.2752724 -4.2686391 -4.2620254 -4.2594643][-4.2483444 -4.24816 -4.2546253 -4.264853 -4.2726731 -4.278286 -4.2819953 -4.2851133 -4.2891817 -4.2929468 -4.2916055 -4.2882156 -4.2860017 -4.2836132 -4.2836556][-4.2514148 -4.2507348 -4.2554946 -4.2625155 -4.2683015 -4.272687 -4.2741914 -4.2741728 -4.2754316 -4.2788115 -4.2807169 -4.2825947 -4.2846503 -4.2860775 -4.2888002][-4.261025 -4.259716 -4.2613349 -4.2633982 -4.2658372 -4.2670469 -4.2654195 -4.2623806 -4.2608452 -4.2622857 -4.2653565 -4.2710066 -4.27585 -4.279377 -4.2831316][-4.2716355 -4.2700944 -4.2696624 -4.2685361 -4.2684522 -4.2665229 -4.2619753 -4.2568412 -4.2535686 -4.2528558 -4.2548833 -4.2614059 -4.2666855 -4.2696638 -4.2723536]]...]
INFO - root - 2017-12-07 21:08:29.225034: step 54010, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 53h:19m:03s remains)
INFO - root - 2017-12-07 21:08:36.042552: step 54020, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 56h:08m:39s remains)
INFO - root - 2017-12-07 21:08:42.855764: step 54030, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 51h:52m:38s remains)
INFO - root - 2017-12-07 21:08:49.625717: step 54040, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 50h:30m:51s remains)
INFO - root - 2017-12-07 21:08:56.428212: step 54050, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 49h:21m:07s remains)
INFO - root - 2017-12-07 21:09:03.245917: step 54060, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 50h:48m:21s remains)
INFO - root - 2017-12-07 21:09:10.027747: step 54070, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 54h:49m:41s remains)
INFO - root - 2017-12-07 21:09:16.848213: step 54080, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 52h:44m:08s remains)
INFO - root - 2017-12-07 21:09:23.734526: step 54090, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 52h:56m:20s remains)
INFO - root - 2017-12-07 21:09:30.309298: step 54100, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 49h:12m:25s remains)
2017-12-07 21:09:31.044963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2198758 -4.213613 -4.2139697 -4.2197027 -4.2267351 -4.2317677 -4.2331491 -4.2328806 -4.2383156 -4.2480388 -4.2580848 -4.2650003 -4.2663341 -4.2644811 -4.2650695][-4.2181849 -4.2116427 -4.2106357 -4.2135305 -4.2181916 -4.2196541 -4.2147932 -4.2101054 -4.2141762 -4.2247262 -4.2361851 -4.2418222 -4.2377996 -4.2300067 -4.2290983][-4.2223763 -4.2184734 -4.2168918 -4.21591 -4.2183022 -4.2179508 -4.2075152 -4.1982331 -4.2002625 -4.2117753 -4.2242484 -4.2273064 -4.2168918 -4.2030096 -4.2000227][-4.217793 -4.2173319 -4.2148037 -4.2109742 -4.2142625 -4.2149591 -4.2035336 -4.192976 -4.1933503 -4.2015643 -4.2116966 -4.2125392 -4.1994238 -4.1824141 -4.1785865][-4.194252 -4.1957483 -4.1970673 -4.19539 -4.2030911 -4.2069511 -4.196701 -4.184937 -4.1801591 -4.181129 -4.1880875 -4.1895304 -4.1769624 -4.1603847 -4.1590018][-4.1508222 -4.1491351 -4.155417 -4.1587434 -4.1709914 -4.176908 -4.1686234 -4.1542139 -4.1440105 -4.1380949 -4.1433144 -4.1502624 -4.1451526 -4.1344733 -4.1368027][-4.1086664 -4.0979772 -4.109993 -4.1220636 -4.1397171 -4.1469779 -4.1371965 -4.1168728 -4.102417 -4.0964332 -4.108 -4.1256347 -4.1301675 -4.1240034 -4.12631][-4.1180143 -4.0955706 -4.1036777 -4.1183052 -4.1371193 -4.1443229 -4.128829 -4.1008606 -4.0826578 -4.0845566 -4.1077719 -4.1314793 -4.1399155 -4.1352415 -4.1362276][-4.1582155 -4.1273813 -4.1235085 -4.1322241 -4.1483335 -4.1555781 -4.1382613 -4.108284 -4.0915403 -4.1033769 -4.136457 -4.1630459 -4.1720052 -4.16844 -4.1682339][-4.1938429 -4.1587 -4.1426535 -4.1429596 -4.1545072 -4.1597514 -4.1455946 -4.1222053 -4.11345 -4.1320066 -4.1659141 -4.191071 -4.2014523 -4.2020636 -4.2024007][-4.2162752 -4.1817684 -4.1604447 -4.1567035 -4.1648064 -4.1687107 -4.160428 -4.14411 -4.1376648 -4.150856 -4.1771135 -4.1998076 -4.2146144 -4.2220583 -4.2270451][-4.2282329 -4.2041707 -4.1867704 -4.1822186 -4.1872754 -4.1917152 -4.1900516 -4.1818504 -4.1782589 -4.186975 -4.2035637 -4.2208366 -4.2345238 -4.24384 -4.2508593][-4.2399626 -4.2285266 -4.2180176 -4.2147684 -4.2166386 -4.2187047 -4.2192626 -4.2170081 -4.2191014 -4.22827 -4.241745 -4.2561159 -4.2671041 -4.2744207 -4.2800107][-4.25301 -4.2496953 -4.2437367 -4.2405748 -4.23984 -4.2398252 -4.2413907 -4.2420382 -4.2472348 -4.2575474 -4.2716956 -4.2864995 -4.297164 -4.3036942 -4.3080678][-4.2615194 -4.2611446 -4.2569647 -4.2526779 -4.2492032 -4.2469544 -4.2489667 -4.2534723 -4.2609758 -4.2720027 -4.2868505 -4.3023067 -4.3150163 -4.3239512 -4.3300743]]...]
INFO - root - 2017-12-07 21:09:37.870939: step 54110, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 55h:09m:40s remains)
INFO - root - 2017-12-07 21:09:44.722005: step 54120, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 50h:19m:07s remains)
INFO - root - 2017-12-07 21:09:51.482407: step 54130, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 49h:10m:17s remains)
INFO - root - 2017-12-07 21:09:58.278870: step 54140, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 52h:43m:00s remains)
INFO - root - 2017-12-07 21:10:05.097053: step 54150, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 53h:59m:03s remains)
INFO - root - 2017-12-07 21:10:11.847784: step 54160, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 54h:56m:42s remains)
INFO - root - 2017-12-07 21:10:18.439515: step 54170, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 50h:31m:28s remains)
INFO - root - 2017-12-07 21:10:25.171191: step 54180, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 49h:36m:50s remains)
INFO - root - 2017-12-07 21:10:31.857227: step 54190, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 48h:52m:49s remains)
INFO - root - 2017-12-07 21:10:38.356727: step 54200, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 49h:55m:14s remains)
2017-12-07 21:10:39.049515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1682143 -4.1324344 -4.0914583 -4.0539808 -4.0306706 -4.019381 -3.9951291 -3.981153 -4.0097609 -4.0575271 -4.0914831 -4.1068616 -4.1177053 -4.1100249 -4.108779][-4.16077 -4.1258206 -4.0819092 -4.0405426 -4.0260286 -4.0310831 -4.0205965 -4.0101652 -4.0447245 -4.0984263 -4.1333208 -4.1436849 -4.1510816 -4.1368008 -4.1257854][-4.1611819 -4.1308584 -4.0929856 -4.0585265 -4.055006 -4.0681911 -4.0592995 -4.0364914 -4.0678244 -4.1269474 -4.1681828 -4.178134 -4.1799083 -4.1602969 -4.1423812][-4.1689911 -4.144022 -4.1156073 -4.08969 -4.0865889 -4.0946536 -4.0675282 -4.0205379 -4.0452061 -4.1117077 -4.1701674 -4.1917639 -4.1976585 -4.18258 -4.1654897][-4.1756816 -4.1528311 -4.1288328 -4.1076937 -4.102684 -4.0962248 -4.0396295 -3.9684317 -3.9913445 -4.0721941 -4.1474543 -4.1806288 -4.198153 -4.1948328 -4.1838303][-4.1805668 -4.1570024 -4.1371455 -4.1220679 -4.1118217 -4.0773458 -3.9782059 -3.8708668 -3.9024558 -4.0177345 -4.1108084 -4.1568241 -4.1890712 -4.2043347 -4.203444][-4.1847291 -4.1621671 -4.1463957 -4.1348791 -4.1192379 -4.057332 -3.9157658 -3.7803774 -3.8436961 -3.9874721 -4.0851855 -4.1323404 -4.1747994 -4.2102017 -4.2200274][-4.1861219 -4.1655688 -4.1559625 -4.1529427 -4.1423297 -4.0829878 -3.9513738 -3.8396745 -3.9115458 -4.0319037 -4.1034422 -4.1327229 -4.1643772 -4.201056 -4.2177811][-4.1911249 -4.1748281 -4.1735396 -4.1826549 -4.1852007 -4.1451063 -4.0495276 -3.9771421 -4.0381131 -4.1221533 -4.1626687 -4.1690607 -4.1814394 -4.2069087 -4.2225294][-4.1990337 -4.1852961 -4.1901693 -4.2077689 -4.2200289 -4.1946959 -4.13116 -4.0888815 -4.1431141 -4.1989303 -4.2151637 -4.2082086 -4.2052946 -4.2147675 -4.2211151][-4.2076373 -4.1952267 -4.2037487 -4.2259097 -4.2405882 -4.2247858 -4.1832552 -4.15586 -4.1979675 -4.2330952 -4.2343736 -4.22052 -4.2087259 -4.2053461 -4.2029696][-4.2172174 -4.2057071 -4.213665 -4.2349844 -4.2494135 -4.2392082 -4.2105365 -4.1834197 -4.2073646 -4.2293353 -4.2229714 -4.205646 -4.1948118 -4.1895823 -4.1883826][-4.2274961 -4.2159762 -4.2202282 -4.2363358 -4.2473316 -4.2404537 -4.2174292 -4.1887741 -4.2047014 -4.2259059 -4.2221317 -4.2071285 -4.1988854 -4.1967287 -4.1996765][-4.2354984 -4.2233763 -4.2236838 -4.2341871 -4.2426476 -4.2366261 -4.2145805 -4.1881618 -4.2073812 -4.2357869 -4.24173 -4.2364993 -4.2311683 -4.2290955 -4.2335191][-4.241293 -4.229929 -4.2294869 -4.2375383 -4.24368 -4.2375107 -4.2143636 -4.1893206 -4.21362 -4.2494812 -4.2652445 -4.2683 -4.2667093 -4.2668047 -4.2712326]]...]
INFO - root - 2017-12-07 21:10:45.846102: step 54210, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 49h:41m:21s remains)
INFO - root - 2017-12-07 21:10:52.805702: step 54220, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 56h:41m:07s remains)
INFO - root - 2017-12-07 21:10:59.631358: step 54230, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 53h:22m:06s remains)
INFO - root - 2017-12-07 21:11:06.447387: step 54240, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 53h:35m:47s remains)
INFO - root - 2017-12-07 21:11:13.344614: step 54250, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 48h:54m:41s remains)
INFO - root - 2017-12-07 21:11:20.236688: step 54260, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 50h:34m:26s remains)
INFO - root - 2017-12-07 21:11:27.104558: step 54270, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 55h:57m:22s remains)
INFO - root - 2017-12-07 21:11:34.028963: step 54280, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 55h:00m:24s remains)
INFO - root - 2017-12-07 21:11:40.890284: step 54290, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 48h:53m:20s remains)
INFO - root - 2017-12-07 21:11:47.529835: step 54300, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 49h:51m:06s remains)
2017-12-07 21:11:48.351510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3547187 -4.3417568 -4.289012 -4.2147927 -4.1470804 -4.0989103 -4.0878124 -4.1215162 -4.181448 -4.2068014 -4.1893525 -4.1561408 -4.14757 -4.169178 -4.208818][-4.3575244 -4.3446178 -4.29355 -4.2196121 -4.1510577 -4.10074 -4.0927758 -4.1343808 -4.2021389 -4.241405 -4.2449236 -4.232852 -4.2255478 -4.2285037 -4.2440581][-4.3600984 -4.3482809 -4.2986093 -4.2278123 -4.1519775 -4.0875626 -4.0758734 -4.1278796 -4.2036314 -4.2547145 -4.2802358 -4.2884207 -4.2809529 -4.2667041 -4.2568521][-4.363461 -4.3502674 -4.2982311 -4.225956 -4.1431766 -4.0623045 -4.0396857 -4.0959082 -4.1779618 -4.2353077 -4.2738676 -4.3005018 -4.2971592 -4.2779274 -4.249238][-4.3667216 -4.3498554 -4.2901735 -4.2093654 -4.1190352 -4.0245476 -3.9831769 -4.0362058 -4.1222572 -4.1892805 -4.2423596 -4.2863336 -4.2966661 -4.2789292 -4.2365003][-4.3676472 -4.3467669 -4.2784967 -4.1892266 -4.0872197 -3.9701545 -3.9039242 -3.9545412 -4.0541105 -4.1391721 -4.2136273 -4.2726307 -4.291223 -4.2745252 -4.220952][-4.3630171 -4.3367825 -4.2623487 -4.1643662 -4.0478086 -3.9096911 -3.8180728 -3.8709989 -3.9931891 -4.1083012 -4.20345 -4.272974 -4.2933555 -4.2732739 -4.2085032][-4.3524108 -4.322257 -4.2427516 -4.1352434 -4.006248 -3.8533585 -3.7529447 -3.8180494 -3.9589159 -4.0981565 -4.2094536 -4.2821527 -4.300766 -4.2711344 -4.1908965][-4.3444347 -4.3136406 -4.2347345 -4.1272612 -3.9963739 -3.8490505 -3.7703285 -3.8503113 -3.9896786 -4.1265512 -4.2335052 -4.296731 -4.3060746 -4.2594147 -4.1611662][-4.3390803 -4.3108783 -4.2381849 -4.14352 -4.0324759 -3.9143031 -3.8703098 -3.9497852 -4.0663915 -4.1785021 -4.2652888 -4.3112221 -4.3084416 -4.2458 -4.1335397][-4.3333035 -4.3099928 -4.2494993 -4.1755538 -4.0969596 -4.0172324 -4.0012608 -4.0680795 -4.1534953 -4.2355156 -4.2950721 -4.3206434 -4.3062468 -4.2374759 -4.1326933][-4.3279953 -4.3130455 -4.2674823 -4.2130671 -4.1653142 -4.123466 -4.126287 -4.1764979 -4.2319832 -4.2868128 -4.3227749 -4.3321118 -4.3097758 -4.2430191 -4.1586761][-4.3257484 -4.3167386 -4.2833943 -4.2459121 -4.2230434 -4.2099261 -4.2216415 -4.2547545 -4.2907052 -4.3269329 -4.3483262 -4.3479586 -4.3210888 -4.2625818 -4.1977172][-4.3231654 -4.3185873 -4.2972059 -4.2752624 -4.267756 -4.2696986 -4.2823119 -4.3011346 -4.3250318 -4.3494067 -4.3648357 -4.3588886 -4.3312206 -4.2847652 -4.2374482][-4.318367 -4.3182697 -4.308311 -4.2972937 -4.2975163 -4.306015 -4.3176169 -4.326714 -4.3421826 -4.3585606 -4.3687925 -4.3611135 -4.3366504 -4.30406 -4.2759428]]...]
INFO - root - 2017-12-07 21:11:55.181757: step 54310, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 54h:05m:58s remains)
INFO - root - 2017-12-07 21:12:01.926864: step 54320, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 51h:30m:43s remains)
INFO - root - 2017-12-07 21:12:08.781271: step 54330, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 52h:47m:21s remains)
INFO - root - 2017-12-07 21:12:15.534288: step 54340, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 55h:58m:08s remains)
INFO - root - 2017-12-07 21:12:22.371937: step 54350, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 55h:39m:16s remains)
INFO - root - 2017-12-07 21:12:29.152762: step 54360, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 49h:57m:19s remains)
INFO - root - 2017-12-07 21:12:35.948497: step 54370, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.617 sec/batch; 47h:40m:36s remains)
INFO - root - 2017-12-07 21:12:42.708631: step 54380, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 51h:29m:26s remains)
INFO - root - 2017-12-07 21:12:49.477535: step 54390, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 55h:14m:37s remains)
INFO - root - 2017-12-07 21:12:56.157585: step 54400, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.744 sec/batch; 57h:29m:56s remains)
2017-12-07 21:12:56.837361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3219275 -4.3101296 -4.2856216 -4.25574 -4.2240114 -4.1825814 -4.1590185 -4.1688247 -4.200573 -4.2362852 -4.2687945 -4.2924 -4.2934809 -4.2732673 -4.2362456][-4.3203831 -4.3098879 -4.2863173 -4.2537103 -4.2112789 -4.15887 -4.1308389 -4.1421247 -4.1780357 -4.2174954 -4.2492189 -4.2703419 -4.2742529 -4.2620578 -4.2346606][-4.3189449 -4.3118424 -4.29012 -4.2538095 -4.206574 -4.1487894 -4.1181669 -4.13345 -4.1724386 -4.2073169 -4.2298632 -4.2403145 -4.2445831 -4.2409911 -4.2234478][-4.318161 -4.3149071 -4.2958426 -4.2575092 -4.2114525 -4.157692 -4.1272411 -4.14286 -4.1771975 -4.2019677 -4.21245 -4.2125735 -4.2171183 -4.2208495 -4.2125392][-4.31925 -4.31603 -4.2955146 -4.2526641 -4.2043228 -4.1545334 -4.122992 -4.1399531 -4.1756964 -4.1951432 -4.1971235 -4.18798 -4.1883607 -4.1971498 -4.1960554][-4.3203955 -4.3121972 -4.2835555 -4.2301221 -4.1691451 -4.1125121 -4.0785413 -4.104197 -4.1557627 -4.1790295 -4.1776295 -4.1614952 -4.1576943 -4.1679368 -4.1743031][-4.3199291 -4.3057284 -4.2694511 -4.204246 -4.122478 -4.044909 -3.9956326 -4.0231433 -4.0963621 -4.1374631 -4.14562 -4.1354389 -4.1383147 -4.1540289 -4.1645641][-4.3198128 -4.3033142 -4.264277 -4.1892643 -4.0853062 -3.9774778 -3.894583 -3.9083173 -4.0017228 -4.0693073 -4.0978456 -4.106328 -4.1205292 -4.1418781 -4.1544127][-4.3181882 -4.3021011 -4.2634258 -4.1837268 -4.0669842 -3.9451118 -3.8418369 -3.84321 -3.9438694 -4.0291119 -4.0724645 -4.0949836 -4.1175423 -4.140861 -4.1524444][-4.31273 -4.2966104 -4.2612033 -4.1851277 -4.0752997 -3.9716184 -3.887023 -3.8947272 -3.9823513 -4.0583467 -4.0962944 -4.1154184 -4.1343083 -4.1490321 -4.1543808][-4.3044314 -4.2851496 -4.2498465 -4.183538 -4.0950127 -4.0229917 -3.9669368 -3.9820518 -4.0534062 -4.1112509 -4.1390114 -4.1495981 -4.1592879 -4.1642747 -4.1636834][-4.2951288 -4.270227 -4.2335629 -4.1776638 -4.1095638 -4.0639968 -4.0374417 -4.0650616 -4.12868 -4.174871 -4.1939569 -4.1953397 -4.1974669 -4.1992178 -4.195262][-4.2888217 -4.2607861 -4.226078 -4.1820917 -4.1339588 -4.1097722 -4.105125 -4.1402459 -4.1946917 -4.2304225 -4.2422357 -4.2397 -4.2391348 -4.2387428 -4.2325726][-4.2894163 -4.26406 -4.2355647 -4.2038655 -4.1722794 -4.1614261 -4.1675024 -4.1982756 -4.2398829 -4.2669568 -4.2765961 -4.2752857 -4.2754254 -4.2755775 -4.2716775][-4.2962422 -4.2754469 -4.2524996 -4.2290115 -4.2073326 -4.2022028 -4.2110295 -4.2371259 -4.2692008 -4.2901688 -4.2990608 -4.30075 -4.3027444 -4.303926 -4.3021073]]...]
INFO - root - 2017-12-07 21:13:03.615776: step 54410, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 49h:53m:39s remains)
INFO - root - 2017-12-07 21:13:10.441735: step 54420, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 56h:12m:06s remains)
INFO - root - 2017-12-07 21:13:17.220467: step 54430, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.731 sec/batch; 56h:27m:20s remains)
INFO - root - 2017-12-07 21:13:24.044343: step 54440, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 55h:19m:25s remains)
INFO - root - 2017-12-07 21:13:30.802618: step 54450, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 50h:50m:05s remains)
INFO - root - 2017-12-07 21:13:37.589980: step 54460, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 50h:37m:39s remains)
INFO - root - 2017-12-07 21:13:44.461685: step 54470, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 56h:42m:47s remains)
INFO - root - 2017-12-07 21:13:51.327294: step 54480, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 55h:27m:48s remains)
INFO - root - 2017-12-07 21:13:58.152671: step 54490, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 52h:21m:17s remains)
INFO - root - 2017-12-07 21:14:04.782594: step 54500, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 48h:21m:56s remains)
2017-12-07 21:14:05.554169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2203746 -4.2319922 -4.2336035 -4.2352228 -4.2399445 -4.2448559 -4.2375369 -4.2303786 -4.2314291 -4.2358551 -4.2361231 -4.2309747 -4.2209296 -4.2199712 -4.2259808][-4.2296143 -4.2359614 -4.2317452 -4.2302194 -4.2334361 -4.2381968 -4.2276826 -4.2214537 -4.2278571 -4.2312326 -4.2279277 -4.2192674 -4.2081184 -4.2037072 -4.2100878][-4.2404776 -4.2397852 -4.2277122 -4.2178907 -4.2177649 -4.2252078 -4.2186871 -4.2158709 -4.2240663 -4.2229939 -4.2128077 -4.2037735 -4.1985269 -4.1962352 -4.1994529][-4.2558537 -4.2474818 -4.2245045 -4.2035809 -4.192946 -4.1975207 -4.2050815 -4.2130642 -4.2217078 -4.2188311 -4.2073779 -4.1969967 -4.1962547 -4.1942139 -4.1954784][-4.2661834 -4.256845 -4.2314825 -4.2008929 -4.1753268 -4.1687989 -4.1822448 -4.1957812 -4.203506 -4.2063112 -4.2037196 -4.1988893 -4.1990805 -4.1940389 -4.1969714][-4.2622037 -4.25644 -4.2355919 -4.2028813 -4.1638303 -4.1413474 -4.1443539 -4.1462474 -4.1520705 -4.1759796 -4.1967649 -4.2054181 -4.2032456 -4.1914549 -4.1948376][-4.2504163 -4.24889 -4.2324791 -4.1988945 -4.1461592 -4.1027164 -4.0786638 -4.0448165 -4.0421 -4.0990539 -4.1547031 -4.18225 -4.1792979 -4.1585307 -4.1564817][-4.2279587 -4.2267256 -4.2070465 -4.1648593 -4.0986671 -4.02995 -3.9636288 -3.8756082 -3.8550763 -3.9599159 -4.0619149 -4.1148281 -4.1184115 -4.088541 -4.0761924][-4.1701703 -4.1713891 -4.1544933 -4.1142426 -4.0497651 -3.9727416 -3.8795691 -3.7564859 -3.7242193 -3.8694811 -4.0023866 -4.0644345 -4.0667677 -4.0316033 -4.0189528][-4.1302509 -4.1323204 -4.1285505 -4.1106787 -4.0786924 -4.0325871 -3.9652874 -3.8760464 -3.853652 -3.9591155 -4.0590019 -4.0928626 -4.0787458 -4.0394473 -4.0279856][-4.1311865 -4.1259451 -4.1269569 -4.1297684 -4.1289625 -4.1154208 -4.0794358 -4.0293665 -4.0194492 -4.0798492 -4.136065 -4.1475759 -4.1241603 -4.0835695 -4.0693946][-4.1630588 -4.1463885 -4.1414948 -4.1490188 -4.1631107 -4.1686139 -4.1575813 -4.1382737 -4.1418185 -4.1765156 -4.2020292 -4.2012339 -4.1756649 -4.134665 -4.110024][-4.223906 -4.2033277 -4.192193 -4.1960421 -4.2130432 -4.2253046 -4.2262526 -4.2227569 -4.22763 -4.2441158 -4.24676 -4.2387919 -4.2134814 -4.1743455 -4.1471348][-4.2730451 -4.2562528 -4.2431111 -4.2415543 -4.2532568 -4.2631907 -4.2674489 -4.2686477 -4.2658057 -4.264122 -4.2568874 -4.2514133 -4.2349844 -4.2046347 -4.1815562][-4.2937832 -4.283814 -4.2724218 -4.2691875 -4.2756424 -4.2822561 -4.2844086 -4.2803707 -4.2679806 -4.2574034 -4.2528472 -4.2572002 -4.2524657 -4.2316856 -4.2166286]]...]
INFO - root - 2017-12-07 21:14:11.805437: step 54510, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 33h:44m:39s remains)
INFO - root - 2017-12-07 21:14:18.606238: step 54520, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 57h:33m:21s remains)
INFO - root - 2017-12-07 21:14:25.413085: step 54530, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 54h:21m:57s remains)
INFO - root - 2017-12-07 21:14:32.165721: step 54540, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 48h:41m:55s remains)
INFO - root - 2017-12-07 21:14:38.936771: step 54550, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 54h:31m:55s remains)
INFO - root - 2017-12-07 21:14:45.773268: step 54560, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 56h:23m:08s remains)
INFO - root - 2017-12-07 21:14:52.615476: step 54570, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 50h:33m:19s remains)
INFO - root - 2017-12-07 21:14:59.420552: step 54580, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 52h:36m:04s remains)
INFO - root - 2017-12-07 21:15:06.151327: step 54590, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 51h:21m:30s remains)
INFO - root - 2017-12-07 21:15:12.747746: step 54600, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 52h:44m:56s remains)
2017-12-07 21:15:13.573604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3081303 -4.29811 -4.2858286 -4.2706013 -4.248992 -4.2234869 -4.2055225 -4.2010994 -4.2067013 -4.2227917 -4.2513957 -4.2838755 -4.3105845 -4.3295178 -4.3375382][-4.2872415 -4.27265 -4.2538929 -4.2292166 -4.1993446 -4.1691146 -4.1485438 -4.1462173 -4.1600003 -4.1858778 -4.225678 -4.2656975 -4.2992415 -4.3209162 -4.3294668][-4.2651234 -4.2488785 -4.22528 -4.1967049 -4.1661758 -4.1389871 -4.1213131 -4.1207676 -4.1421189 -4.1796017 -4.2258592 -4.26622 -4.29963 -4.317944 -4.3246317][-4.2429285 -4.223917 -4.1994505 -4.1736808 -4.1491046 -4.1266584 -4.1134033 -4.115725 -4.14103 -4.1842685 -4.2320242 -4.2696123 -4.3018885 -4.3179007 -4.3232174][-4.2273507 -4.20192 -4.1760635 -4.1538272 -4.1325603 -4.1125512 -4.1030788 -4.1129851 -4.147687 -4.1915722 -4.2331514 -4.2676826 -4.2980547 -4.3149891 -4.3217516][-4.2114916 -4.1787038 -4.1445308 -4.1092091 -4.0819664 -4.0645108 -4.0652905 -4.0930243 -4.1436195 -4.1903577 -4.2281089 -4.262713 -4.2931333 -4.3130116 -4.3205233][-4.1896052 -4.1518126 -4.1035695 -4.0493941 -4.0110755 -3.9887872 -3.9901803 -4.0297971 -4.1003089 -4.1584573 -4.2048345 -4.2453246 -4.2804461 -4.3049173 -4.3151636][-4.1652594 -4.1278911 -4.0703712 -4.0016203 -3.9437661 -3.9031091 -3.8961596 -3.9450192 -4.0386024 -4.1199551 -4.1812091 -4.2308903 -4.2693043 -4.2973313 -4.3108191][-4.1295862 -4.0944963 -4.0367031 -3.9617031 -3.8886409 -3.8329487 -3.8257356 -3.8781357 -3.9850061 -4.0824566 -4.1534719 -4.21049 -4.2571015 -4.2899837 -4.3070784][-4.0997896 -4.0648937 -4.0107727 -3.9431574 -3.870585 -3.8139932 -3.810071 -3.8579731 -3.9615743 -4.0622334 -4.1373715 -4.1990123 -4.2508678 -4.2859445 -4.3050981][-4.120913 -4.0867529 -4.0405445 -3.9825118 -3.9191499 -3.8688712 -3.8707912 -3.91672 -4.0051208 -4.0940237 -4.1629419 -4.2202706 -4.2670236 -4.2968888 -4.3118434][-4.1868052 -4.1533051 -4.113308 -4.068769 -4.0163288 -3.9745128 -3.976882 -4.0182552 -4.0922165 -4.1638541 -4.2191916 -4.2652149 -4.3018851 -4.32058 -4.3271985][-4.2583289 -4.22592 -4.194159 -4.1634111 -4.1254745 -4.0950255 -4.096652 -4.1307631 -4.1885214 -4.2415714 -4.2812786 -4.31551 -4.3422704 -4.3486657 -4.3448844][-4.306004 -4.2780452 -4.2571068 -4.2387476 -4.2146287 -4.195415 -4.1981649 -4.2231488 -4.2636633 -4.301137 -4.3281112 -4.3509374 -4.3677511 -4.3664517 -4.3566036][-4.329905 -4.3094888 -4.295939 -4.2848268 -4.2722816 -4.2609873 -4.2624049 -4.2779164 -4.3042192 -4.3313603 -4.3504581 -4.3660836 -4.3751454 -4.3702464 -4.3602877]]...]
INFO - root - 2017-12-07 21:15:20.384363: step 54610, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 51h:49m:25s remains)
INFO - root - 2017-12-07 21:15:27.213555: step 54620, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 51h:31m:46s remains)
INFO - root - 2017-12-07 21:15:34.160782: step 54630, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 0.772 sec/batch; 59h:36m:25s remains)
INFO - root - 2017-12-07 21:15:40.932440: step 54640, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 53h:55m:44s remains)
INFO - root - 2017-12-07 21:15:47.825028: step 54650, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 49h:50m:43s remains)
INFO - root - 2017-12-07 21:15:54.524639: step 54660, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 48h:54m:58s remains)
INFO - root - 2017-12-07 21:16:01.272050: step 54670, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 53h:11m:50s remains)
INFO - root - 2017-12-07 21:16:08.112617: step 54680, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 56h:11m:34s remains)
INFO - root - 2017-12-07 21:16:14.958439: step 54690, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 52h:13m:50s remains)
INFO - root - 2017-12-07 21:16:21.554700: step 54700, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 50h:25m:54s remains)
2017-12-07 21:16:22.287152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2934623 -4.2824211 -4.2667632 -4.2508659 -4.2360983 -4.2219672 -4.210598 -4.2115951 -4.2175384 -4.2212052 -4.2192831 -4.2249513 -4.2340722 -4.2408834 -4.25282][-4.2854486 -4.2697997 -4.2486472 -4.2257857 -4.2030482 -4.1850486 -4.1752534 -4.1810374 -4.1935773 -4.2050529 -4.2077422 -4.2141895 -4.2211785 -4.2228785 -4.2319832][-4.2793932 -4.2611957 -4.2380714 -4.2121744 -4.1834021 -4.1621103 -4.1501141 -4.154335 -4.1697173 -4.1895852 -4.2009797 -4.210968 -4.2178659 -4.2171736 -4.2226739][-4.2742534 -4.2550235 -4.2319531 -4.2055564 -4.1730986 -4.1470761 -4.1295919 -4.1294074 -4.1459751 -4.1719341 -4.1902041 -4.20418 -4.215445 -4.2204666 -4.2273464][-4.26961 -4.2530155 -4.2337132 -4.2124515 -4.1826386 -4.1542192 -4.1290374 -4.1170969 -4.1277189 -4.1509113 -4.1683512 -4.1845422 -4.2043509 -4.2211938 -4.2346649][-4.2638717 -4.2494946 -4.2355237 -4.2242441 -4.2036643 -4.1786242 -4.1457686 -4.113358 -4.1042795 -4.1162233 -4.1292028 -4.1470504 -4.1743064 -4.2026334 -4.2254696][-4.2585859 -4.241262 -4.227457 -4.2220244 -4.2094765 -4.1884336 -4.1561651 -4.1163626 -4.0893626 -4.0837517 -4.0882339 -4.1042194 -4.1316423 -4.167726 -4.2031841][-4.2506657 -4.2250543 -4.2044597 -4.1966586 -4.1845579 -4.1624174 -4.1343155 -4.1020603 -4.0743332 -4.0605516 -4.0562277 -4.0642357 -4.0876536 -4.1285887 -4.174962][-4.2393579 -4.2006531 -4.17167 -4.1599369 -4.1445656 -4.1207333 -4.0993958 -4.0817013 -4.0670042 -4.0577888 -4.0502782 -4.0509243 -4.0676961 -4.1074705 -4.1575603][-4.2351961 -4.1929479 -4.163641 -4.1483479 -4.1250281 -4.097054 -4.0834041 -4.0818572 -4.0804434 -4.0798831 -4.0784755 -4.0813007 -4.0939903 -4.1230192 -4.1641183][-4.2439451 -4.2089734 -4.1870275 -4.1702747 -4.1397657 -4.1105356 -4.1005621 -4.1059365 -4.1140532 -4.1224632 -4.131649 -4.1393781 -4.1477571 -4.1629658 -4.188004][-4.2655535 -4.2433138 -4.2305474 -4.21511 -4.1868758 -4.1627626 -4.1531506 -4.1515279 -4.1579971 -4.1683712 -4.1808076 -4.1910405 -4.1968594 -4.2050352 -4.2194896][-4.2880492 -4.2747107 -4.2657642 -4.2523527 -4.2337451 -4.22056 -4.2109609 -4.201901 -4.2032933 -4.2110295 -4.2206135 -4.2291694 -4.2343698 -4.2399178 -4.2471604][-4.3046694 -4.2963157 -4.2877126 -4.2760787 -4.2646451 -4.2576809 -4.2488909 -4.2388968 -4.2392817 -4.2447696 -4.2511177 -4.2571974 -4.2606888 -4.2637935 -4.2665477][-4.3144321 -4.3087144 -4.3008323 -4.2905703 -4.2820764 -4.2766805 -4.270155 -4.2640152 -4.2649856 -4.2689242 -4.2728262 -4.2769203 -4.2794151 -4.2812748 -4.2823243]]...]
INFO - root - 2017-12-07 21:16:29.208225: step 54710, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 54h:39m:17s remains)
INFO - root - 2017-12-07 21:16:36.029180: step 54720, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 51h:59m:08s remains)
INFO - root - 2017-12-07 21:16:42.857654: step 54730, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 50h:24m:20s remains)
INFO - root - 2017-12-07 21:16:49.548776: step 54740, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 48h:22m:30s remains)
INFO - root - 2017-12-07 21:16:56.323620: step 54750, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 53h:54m:25s remains)
INFO - root - 2017-12-07 21:17:03.045653: step 54760, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 53h:55m:36s remains)
INFO - root - 2017-12-07 21:17:09.866075: step 54770, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.687 sec/batch; 52h:58m:09s remains)
INFO - root - 2017-12-07 21:17:16.637630: step 54780, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 50h:30m:42s remains)
INFO - root - 2017-12-07 21:17:23.333580: step 54790, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.610 sec/batch; 47h:04m:50s remains)
INFO - root - 2017-12-07 21:17:29.924913: step 54800, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 52h:31m:27s remains)
2017-12-07 21:17:30.668906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2188845 -4.211031 -4.2183704 -4.232789 -4.2386055 -4.2386522 -4.2331309 -4.2318544 -4.2384448 -4.235909 -4.2361655 -4.2353706 -4.2331204 -4.225935 -4.2188411][-4.212604 -4.20688 -4.2148123 -4.2277112 -4.226748 -4.2177453 -4.2041039 -4.1992197 -4.2046967 -4.2048926 -4.2065248 -4.2069616 -4.2037144 -4.1939859 -4.1813521][-4.2093196 -4.2090688 -4.220336 -4.2294579 -4.2213149 -4.20448 -4.1867127 -4.1820316 -4.1893444 -4.1909294 -4.1937013 -4.1925192 -4.18451 -4.1689553 -4.1500812][-4.2087221 -4.2180004 -4.2379174 -4.2457256 -4.2299271 -4.2053361 -4.186439 -4.1818075 -4.1924491 -4.1974287 -4.1976271 -4.190979 -4.1791234 -4.1668582 -4.1540937][-4.1799874 -4.2013774 -4.2293506 -4.2409272 -4.2255507 -4.2012138 -4.1854138 -4.1820984 -4.1948881 -4.2041264 -4.207068 -4.2017198 -4.1945529 -4.1930623 -4.1923175][-4.1105566 -4.1421213 -4.1796427 -4.1991549 -4.1896696 -4.1740808 -4.1687121 -4.1685567 -4.1800423 -4.1958737 -4.2105131 -4.2150059 -4.2183385 -4.2279043 -4.2370505][-4.0184379 -4.0550737 -4.1029587 -4.1298504 -4.1301594 -4.1274533 -4.1353226 -4.1435161 -4.1567655 -4.1780715 -4.2035065 -4.2194295 -4.2319264 -4.2460675 -4.2579265][-3.987258 -4.005599 -4.0444059 -4.0644217 -4.064723 -4.0713038 -4.0907784 -4.1106329 -4.1335998 -4.1622939 -4.1946516 -4.214088 -4.2274919 -4.2437015 -4.2552981][-4.0447335 -4.0442405 -4.0611014 -4.066391 -4.0594645 -4.0617967 -4.0798922 -4.099854 -4.1238837 -4.1502609 -4.1740584 -4.1838622 -4.1953454 -4.2150373 -4.226922][-4.120204 -4.1150537 -4.1199093 -4.1199646 -4.1133485 -4.1092825 -4.1168151 -4.1235437 -4.1347351 -4.1472816 -4.1465383 -4.13529 -4.1394744 -4.1606054 -4.1735711][-4.1817341 -4.1795735 -4.1798315 -4.1757584 -4.1697812 -4.1632547 -4.1611061 -4.1590586 -4.1588 -4.1531973 -4.1270032 -4.0935316 -4.084897 -4.0997577 -4.110888][-4.2331214 -4.2316623 -4.2293983 -4.2204 -4.2102447 -4.2005386 -4.1935959 -4.1891303 -4.1812086 -4.1635685 -4.1228886 -4.0712733 -4.0461464 -4.0486317 -4.0563512][-4.2661953 -4.2652206 -4.26179 -4.2506037 -4.2380891 -4.2260003 -4.21723 -4.2125707 -4.2013917 -4.1802258 -4.1377192 -4.0833597 -4.0519733 -4.0450873 -4.0505385][-4.276701 -4.2741637 -4.2696414 -4.2574692 -4.2455964 -4.2339149 -4.2265315 -4.22442 -4.2182689 -4.2048321 -4.1734319 -4.1328292 -4.1087561 -4.1026373 -4.1086607][-4.278688 -4.274631 -4.2698007 -4.2607031 -4.2512512 -4.2381968 -4.2286496 -4.2286334 -4.2279024 -4.225296 -4.2104945 -4.1910362 -4.1800065 -4.1771235 -4.1830144]]...]
INFO - root - 2017-12-07 21:17:37.389120: step 54810, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 53h:36m:57s remains)
INFO - root - 2017-12-07 21:17:44.054678: step 54820, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.483 sec/batch; 37h:13m:05s remains)
INFO - root - 2017-12-07 21:17:50.809919: step 54830, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.748 sec/batch; 57h:39m:53s remains)
INFO - root - 2017-12-07 21:17:57.589322: step 54840, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 55h:25m:34s remains)
INFO - root - 2017-12-07 21:18:04.380068: step 54850, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 51h:27m:34s remains)
INFO - root - 2017-12-07 21:18:11.186636: step 54860, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 49h:02m:34s remains)
INFO - root - 2017-12-07 21:18:18.028182: step 54870, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 52h:22m:57s remains)
INFO - root - 2017-12-07 21:18:24.787553: step 54880, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 54h:24m:48s remains)
INFO - root - 2017-12-07 21:18:31.567414: step 54890, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 52h:10m:19s remains)
INFO - root - 2017-12-07 21:18:38.163067: step 54900, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 50h:51m:27s remains)
2017-12-07 21:18:38.895134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1863656 -4.1935859 -4.2047725 -4.226244 -4.2536902 -4.2726789 -4.2585082 -4.2305737 -4.2117386 -4.1975064 -4.192812 -4.18216 -4.1657763 -4.1527128 -4.15297][-4.206387 -4.2105289 -4.2193885 -4.2363791 -4.2535467 -4.2560844 -4.2279696 -4.1918435 -4.1663833 -4.1448884 -4.1358838 -4.1347742 -4.1294103 -4.1205044 -4.12449][-4.2256937 -4.2225971 -4.2203064 -4.2287169 -4.2319903 -4.2192492 -4.1865888 -4.1522274 -4.1304693 -4.1140919 -4.1085443 -4.1176667 -4.1269846 -4.1301751 -4.1344595][-4.224865 -4.2143435 -4.2010932 -4.2037663 -4.2043791 -4.1860971 -4.1572328 -4.1350822 -4.1238379 -4.1170011 -4.1150031 -4.12811 -4.1430755 -4.1520448 -4.1483326][-4.2103395 -4.1989951 -4.1813507 -4.1785455 -4.1771789 -4.155077 -4.1287236 -4.1203842 -4.1190567 -4.1139021 -4.1068521 -4.1164703 -4.1319332 -4.140615 -4.1305056][-4.1862464 -4.1802731 -4.1625538 -4.1499624 -4.1328135 -4.0891042 -4.0533319 -4.0594277 -4.0759048 -4.0745039 -4.0645761 -4.0712271 -4.0815425 -4.080451 -4.067944][-4.1583638 -4.1532884 -4.1328988 -4.1023517 -4.0534468 -3.9696844 -3.909955 -3.9386582 -3.9919763 -4.008677 -4.0137515 -4.0308218 -4.0391 -4.0258918 -4.0083351][-4.1370039 -4.127079 -4.0999875 -4.0544028 -3.9757838 -3.8563833 -3.7853665 -3.8525214 -3.9492862 -3.996125 -4.0241561 -4.0558543 -4.069149 -4.0479708 -4.0191269][-4.1372647 -4.13041 -4.1104751 -4.0719128 -4.0061107 -3.9130461 -3.8730669 -3.9416113 -4.0250983 -4.0699735 -4.1002522 -4.1268678 -4.1362815 -4.1137819 -4.086679][-4.147861 -4.1490407 -4.14829 -4.1368446 -4.1076903 -4.0594697 -4.0466337 -4.0934744 -4.139915 -4.1638813 -4.1815052 -4.1918182 -4.1933193 -4.1760774 -4.15946][-4.1373687 -4.1458688 -4.1615095 -4.1790266 -4.1822491 -4.1692667 -4.1747293 -4.2088537 -4.2321024 -4.2432203 -4.2499013 -4.2491946 -4.2450147 -4.2334628 -4.2244883][-4.1024127 -4.1198111 -4.1495967 -4.1900892 -4.2198524 -4.2308049 -4.245646 -4.2751927 -4.28858 -4.2921872 -4.2929773 -4.2887888 -4.2828937 -4.2747326 -4.2691841][-4.0758023 -4.1076736 -4.1504374 -4.2031569 -4.2431703 -4.2625875 -4.2806811 -4.3055272 -4.3144884 -4.3150539 -4.3141251 -4.3096189 -4.302732 -4.2960534 -4.29144][-4.0931005 -4.1354713 -4.1861396 -4.2344818 -4.2672114 -4.2830076 -4.29732 -4.314877 -4.3228984 -4.325027 -4.3244867 -4.3210316 -4.3149538 -4.3089628 -4.3032227][-4.16159 -4.2005553 -4.2450438 -4.278851 -4.2963343 -4.3015909 -4.3072634 -4.3186493 -4.3275824 -4.3309073 -4.3311229 -4.3274841 -4.3223453 -4.3170004 -4.3109174]]...]
INFO - root - 2017-12-07 21:18:45.695891: step 54910, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 55h:06m:33s remains)
INFO - root - 2017-12-07 21:18:52.430177: step 54920, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 51h:39m:08s remains)
INFO - root - 2017-12-07 21:18:59.239660: step 54930, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 50h:14m:39s remains)
INFO - root - 2017-12-07 21:19:06.033862: step 54940, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 51h:15m:34s remains)
INFO - root - 2017-12-07 21:19:12.860663: step 54950, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 55h:21m:24s remains)
INFO - root - 2017-12-07 21:19:19.737527: step 54960, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 55h:59m:55s remains)
INFO - root - 2017-12-07 21:19:26.598458: step 54970, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 55h:25m:32s remains)
INFO - root - 2017-12-07 21:19:33.488304: step 54980, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 49h:46m:26s remains)
INFO - root - 2017-12-07 21:19:40.261800: step 54990, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 49h:56m:13s remains)
INFO - root - 2017-12-07 21:19:46.904514: step 55000, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 52h:24m:15s remains)
2017-12-07 21:19:47.688599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2865973 -4.2861862 -4.2690606 -4.235178 -4.2027993 -4.1859884 -4.1823664 -4.1876411 -4.209641 -4.2313638 -4.2409377 -4.2458491 -4.2590857 -4.272481 -4.2754][-4.288156 -4.287622 -4.2676697 -4.2282076 -4.1852064 -4.1566925 -4.1497507 -4.1576095 -4.1839523 -4.2099071 -4.218111 -4.2190094 -4.2326202 -4.2535634 -4.2637939][-4.2818389 -4.2796097 -4.2569194 -4.2136097 -4.1623654 -4.1235847 -4.1151571 -4.126224 -4.1585331 -4.192946 -4.2030473 -4.1966805 -4.2029667 -4.2237129 -4.2381639][-4.2760253 -4.2706218 -4.2468224 -4.205729 -4.1533828 -4.1122422 -4.1008291 -4.1078863 -4.1391964 -4.1773486 -4.1897135 -4.1799793 -4.1805749 -4.1975908 -4.211947][-4.2747765 -4.2668295 -4.2412758 -4.204113 -4.1556721 -4.1182318 -4.1012115 -4.0990739 -4.1259608 -4.1637893 -4.1747222 -4.1608109 -4.1602526 -4.1735468 -4.18947][-4.2750888 -4.2705 -4.2450838 -4.2071381 -4.1614003 -4.1260295 -4.1018977 -4.0899849 -4.1110778 -4.1509686 -4.1633554 -4.1513433 -4.1497889 -4.1578274 -4.1740637][-4.27709 -4.2785897 -4.2553229 -4.2145028 -4.1660156 -4.1235509 -4.0835609 -4.0524421 -4.0631833 -4.1159229 -4.1465917 -4.1509042 -4.154902 -4.1598558 -4.1728806][-4.2741747 -4.278614 -4.258224 -4.2182746 -4.1683064 -4.1156039 -4.05165 -3.9901381 -3.9838009 -4.0573735 -4.12326 -4.1539388 -4.1669035 -4.1696153 -4.178328][-4.2664585 -4.2723417 -4.2539415 -4.2161889 -4.1683588 -4.1126671 -4.0329008 -3.943315 -3.9137053 -4.0002651 -4.1011071 -4.1621146 -4.1887617 -4.19216 -4.1970782][-4.2598753 -4.2638583 -4.2462854 -4.2137747 -4.1740584 -4.1266718 -4.0563474 -3.9753766 -3.93776 -4.0046134 -4.1028185 -4.1745138 -4.211257 -4.218555 -4.2228155][-4.260561 -4.2605023 -4.2417159 -4.211792 -4.1791186 -4.1453819 -4.0991731 -4.0456982 -4.0160737 -4.0543132 -4.1250935 -4.1867285 -4.2258615 -4.2394323 -4.24694][-4.2618604 -4.2587218 -4.238286 -4.2080364 -4.1780028 -4.153852 -4.1286864 -4.098608 -4.0752912 -4.0922 -4.1400156 -4.19094 -4.2329497 -4.2551656 -4.2675323][-4.2536335 -4.2524223 -4.2372708 -4.2124233 -4.18321 -4.1603661 -4.1447239 -4.1302443 -4.1181083 -4.1271386 -4.1611614 -4.2035623 -4.2436585 -4.2701821 -4.286572][-4.233748 -4.2334261 -4.2277761 -4.2142854 -4.19057 -4.1677957 -4.1562972 -4.1521006 -4.1521606 -4.1649218 -4.1941137 -4.2313247 -4.2641153 -4.2862668 -4.3005714][-4.2114339 -4.2078924 -4.2094111 -4.2099614 -4.1993766 -4.1862068 -4.1820874 -4.1849918 -4.1937394 -4.2109404 -4.2364645 -4.2661538 -4.2905521 -4.3084888 -4.3192172]]...]
INFO - root - 2017-12-07 21:19:54.351729: step 55010, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 49h:20m:11s remains)
INFO - root - 2017-12-07 21:20:01.054040: step 55020, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 48h:38m:25s remains)
INFO - root - 2017-12-07 21:20:07.836044: step 55030, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 54h:58m:57s remains)
INFO - root - 2017-12-07 21:20:14.532532: step 55040, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 54h:03m:01s remains)
INFO - root - 2017-12-07 21:20:21.328153: step 55050, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 54h:46m:53s remains)
INFO - root - 2017-12-07 21:20:28.222488: step 55060, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 51h:58m:03s remains)
INFO - root - 2017-12-07 21:20:34.979697: step 55070, loss = 2.06, batch loss = 2.01 (13.1 examples/sec; 0.610 sec/batch; 47h:00m:14s remains)
INFO - root - 2017-12-07 21:20:41.864596: step 55080, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 55h:49m:57s remains)
INFO - root - 2017-12-07 21:20:48.636842: step 55090, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 57h:06m:59s remains)
INFO - root - 2017-12-07 21:20:55.145534: step 55100, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 56h:52m:52s remains)
2017-12-07 21:20:55.864466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3508563 -4.3505988 -4.3488979 -4.3479929 -4.3476944 -4.3466864 -4.3442225 -4.3408818 -4.3388338 -4.3402176 -4.3438339 -4.34677 -4.347199 -4.3420277 -4.3318877][-4.3511 -4.35057 -4.3480625 -4.3455081 -4.34266 -4.3371849 -4.3286982 -4.3195791 -4.3155422 -4.3200722 -4.33137 -4.3431849 -4.3513951 -4.351788 -4.345624][-4.3498273 -4.3488641 -4.3453908 -4.3395205 -4.3289237 -4.3109517 -4.2873006 -4.2655764 -4.2577944 -4.2684755 -4.2931366 -4.3216281 -4.3447347 -4.3549705 -4.3543515][-4.3475623 -4.3452907 -4.3391771 -4.3242846 -4.2964354 -4.2515693 -4.1981254 -4.1549616 -4.1429424 -4.1683793 -4.2189546 -4.2751479 -4.3203583 -4.3449988 -4.3536954][-4.3437295 -4.3384471 -4.3250303 -4.2914977 -4.2311778 -4.1426864 -4.0479474 -3.98114 -3.9734316 -4.0286465 -4.1185818 -4.2106242 -4.2813478 -4.3233848 -4.343483][-4.3377542 -4.3263717 -4.2992 -4.2391052 -4.1396351 -4.0063906 -3.8792508 -3.8044958 -3.8181698 -3.9136877 -4.0430417 -4.1630406 -4.2503881 -4.303297 -4.3314042][-4.3296876 -4.3088303 -4.2645473 -4.1808167 -4.0564756 -3.9089308 -3.78938 -3.74335 -3.79264 -3.9136927 -4.053225 -4.1723776 -4.2544327 -4.3019419 -4.326921][-4.321754 -4.2939053 -4.2407131 -4.153461 -4.038415 -3.9209003 -3.8484931 -3.8446007 -3.9087734 -4.0152206 -4.1280556 -4.2213063 -4.2830949 -4.3153725 -4.3305573][-4.3185487 -4.2915254 -4.2438583 -4.1752625 -4.0955219 -4.0253482 -3.9956019 -4.0114193 -4.06344 -4.1360378 -4.212554 -4.2757635 -4.3162985 -4.3335285 -4.3385978][-4.3227057 -4.3037891 -4.2713881 -4.2301469 -4.1877518 -4.1548181 -4.1476264 -4.1649995 -4.1979156 -4.2403336 -4.2859883 -4.3231149 -4.3451681 -4.3506808 -4.3484421][-4.331882 -4.3221474 -4.3057842 -4.2878304 -4.2717052 -4.260848 -4.2627239 -4.2763267 -4.2948351 -4.3169894 -4.3403335 -4.3574452 -4.3651166 -4.362586 -4.3562489][-4.3411388 -4.337759 -4.3315315 -4.32664 -4.3238015 -4.3237696 -4.3294444 -4.3403468 -4.351274 -4.3617797 -4.3710155 -4.3747253 -4.3722291 -4.3650608 -4.3576894][-4.3481216 -4.3479872 -4.3467913 -4.3478417 -4.350718 -4.3550787 -4.3612657 -4.3689065 -4.3739543 -4.3763118 -4.3762693 -4.3724952 -4.3657951 -4.3587642 -4.3538222][-4.3518357 -4.3523827 -4.3524551 -4.3542376 -4.3575239 -4.3610954 -4.3649173 -4.3684182 -4.3696971 -4.3684058 -4.3652096 -4.3602118 -4.35475 -4.3505731 -4.3483996][-4.353333 -4.353652 -4.3534722 -4.3539844 -4.3551764 -4.3565931 -4.3582249 -4.359355 -4.3591857 -4.3576202 -4.3549328 -4.3514071 -4.3478403 -4.3446584 -4.3424287]]...]
INFO - root - 2017-12-07 21:21:02.645676: step 55110, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 52h:14m:27s remains)
INFO - root - 2017-12-07 21:21:09.506974: step 55120, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 56h:03m:44s remains)
INFO - root - 2017-12-07 21:21:16.304345: step 55130, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 48h:41m:46s remains)
INFO - root - 2017-12-07 21:21:22.894768: step 55140, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 49h:41m:11s remains)
INFO - root - 2017-12-07 21:21:29.691386: step 55150, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 55h:55m:28s remains)
INFO - root - 2017-12-07 21:21:36.451110: step 55160, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 54h:23m:48s remains)
INFO - root - 2017-12-07 21:21:43.167443: step 55170, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 52h:48m:23s remains)
INFO - root - 2017-12-07 21:21:49.892960: step 55180, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 48h:35m:03s remains)
INFO - root - 2017-12-07 21:21:56.714089: step 55190, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 50h:34m:45s remains)
INFO - root - 2017-12-07 21:22:03.216220: step 55200, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 55h:34m:14s remains)
2017-12-07 21:22:04.089297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.215754 -4.218349 -4.2230682 -4.2264109 -4.2284884 -4.2300963 -4.2295289 -4.2276731 -4.229763 -4.2334986 -4.2362485 -4.2396312 -4.2460141 -4.2576346 -4.2753057][-4.18607 -4.1925888 -4.2048807 -4.2168703 -4.2266188 -4.2338228 -4.235971 -4.2349286 -4.2370281 -4.2414527 -4.2439375 -4.2442131 -4.2442031 -4.247407 -4.2576294][-4.1669946 -4.1758041 -4.1914468 -4.2078233 -4.2219791 -4.2323132 -4.2365832 -4.238317 -4.2436676 -4.2527943 -4.2589478 -4.259882 -4.2561121 -4.2508197 -4.2484961][-4.1731462 -4.1796708 -4.1916566 -4.2039013 -4.2139 -4.2198787 -4.2218995 -4.2254453 -4.2376142 -4.2559977 -4.2708139 -4.2779446 -4.2761478 -4.2662992 -4.252552][-4.1877093 -4.1840734 -4.1852307 -4.1862903 -4.1846876 -4.1786442 -4.1712494 -4.1734219 -4.1945105 -4.2258048 -4.2533469 -4.2706361 -4.2757978 -4.2674942 -4.2488251][-4.1796002 -4.1639695 -4.1508341 -4.1346736 -4.1134648 -4.0856004 -4.0600243 -4.0568423 -4.0903625 -4.143764 -4.1933608 -4.228128 -4.2445393 -4.2399387 -4.2182822][-4.1529922 -4.130024 -4.1076479 -4.0806823 -4.043611 -3.99288 -3.9420435 -3.9247124 -3.9630578 -4.034276 -4.1061568 -4.1605616 -4.1917272 -4.194695 -4.1729231][-4.1397533 -4.1194077 -4.1029124 -4.0845051 -4.0548162 -4.0081329 -3.9559839 -3.9287162 -3.9479392 -4.0021534 -4.0668321 -4.1239343 -4.1630206 -4.1749263 -4.1608534][-4.1525531 -4.13967 -4.1346335 -4.1314683 -4.118432 -4.0919871 -4.0611472 -4.0437903 -4.0520554 -4.0823059 -4.1219072 -4.1602855 -4.1885467 -4.1996851 -4.1934052][-4.1780672 -4.1674805 -4.1664367 -4.1705136 -4.1692419 -4.1608644 -4.149374 -4.1440344 -4.1518264 -4.1720557 -4.1964922 -4.2188029 -4.2333393 -4.2361779 -4.2296166][-4.2081695 -4.1988783 -4.1972995 -4.2015796 -4.2043014 -4.2043056 -4.2020311 -4.2011704 -4.2063775 -4.2190523 -4.2343078 -4.2469606 -4.2530208 -4.2505803 -4.24214][-4.2281456 -4.2210636 -4.2188244 -4.2214909 -4.2252235 -4.2289038 -4.2327833 -4.23638 -4.2410483 -4.2477436 -4.2544808 -4.257926 -4.2549825 -4.2468767 -4.2373624][-4.223968 -4.2196417 -4.2167692 -4.2172403 -4.2202826 -4.225976 -4.234179 -4.2422438 -4.2486992 -4.2521896 -4.2524953 -4.2491212 -4.2395835 -4.2271566 -4.2173038][-4.2165985 -4.2124748 -4.2091737 -4.2077751 -4.2088008 -4.2134018 -4.2212729 -4.2297573 -4.2358546 -4.2375703 -4.234622 -4.2278619 -4.2150917 -4.2003527 -4.1913061][-4.2195086 -4.213757 -4.2085314 -4.2042036 -4.2025385 -4.2048464 -4.2100887 -4.2173052 -4.2227082 -4.2244 -4.2213583 -4.2146029 -4.2022867 -4.186408 -4.1762547]]...]
INFO - root - 2017-12-07 21:22:10.872808: step 55210, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 48h:41m:19s remains)
INFO - root - 2017-12-07 21:22:17.616520: step 55220, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 50h:26m:02s remains)
INFO - root - 2017-12-07 21:22:24.454806: step 55230, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 56h:38m:45s remains)
INFO - root - 2017-12-07 21:22:31.339670: step 55240, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 57h:55m:43s remains)
INFO - root - 2017-12-07 21:22:38.113433: step 55250, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 50h:16m:10s remains)
INFO - root - 2017-12-07 21:22:44.865507: step 55260, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 49h:02m:02s remains)
INFO - root - 2017-12-07 21:22:51.604975: step 55270, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 51h:02m:20s remains)
INFO - root - 2017-12-07 21:22:58.497510: step 55280, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 54h:42m:57s remains)
INFO - root - 2017-12-07 21:23:05.266164: step 55290, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 56h:26m:48s remains)
INFO - root - 2017-12-07 21:23:11.915083: step 55300, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 51h:48m:09s remains)
2017-12-07 21:23:12.617032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1627488 -4.1402926 -4.1213078 -4.1259551 -4.14034 -4.1585321 -4.1740236 -4.18237 -4.1816835 -4.1807914 -4.1839027 -4.1848345 -4.1849985 -4.1836462 -4.1853671][-4.1889782 -4.1714029 -4.1593261 -4.173769 -4.1926508 -4.2082419 -4.2133307 -4.2079339 -4.1958089 -4.1874285 -4.18597 -4.1849079 -4.1808319 -4.1745224 -4.1756487][-4.1887751 -4.1782246 -4.1730042 -4.1920037 -4.2108197 -4.222713 -4.2203531 -4.2071352 -4.191453 -4.1805577 -4.1747761 -4.1719413 -4.1668587 -4.1585321 -4.1566133][-4.156455 -4.1451511 -4.1428156 -4.1658688 -4.1894741 -4.2031446 -4.1980076 -4.1794424 -4.1590729 -4.1466031 -4.1423 -4.14314 -4.1414385 -4.1355004 -4.1322236][-4.1034541 -4.0796752 -4.075315 -4.101707 -4.1308446 -4.1483827 -4.1403513 -4.112586 -4.0819454 -4.0675049 -4.0727367 -4.0850716 -4.0926795 -4.0927668 -4.09087][-4.0218444 -3.9793248 -3.9715734 -4.0019679 -4.0357237 -4.0561466 -4.043602 -4.0001054 -3.9571559 -3.9499822 -3.9741662 -4.0056028 -4.03927 -4.059484 -4.0653625][-3.9523563 -3.906193 -3.897572 -3.924612 -3.9502797 -3.9606123 -3.9324188 -3.865098 -3.8117049 -3.8187928 -3.8659348 -3.9209 -3.9872801 -4.0358324 -4.0557842][-3.9488654 -3.9114962 -3.9081502 -3.9286647 -3.9415271 -3.9404619 -3.91038 -3.8455682 -3.796798 -3.8032379 -3.8442581 -3.9012282 -3.972553 -4.0274029 -4.0483217][-4.0068245 -3.9787405 -3.9798639 -3.99646 -4.0013738 -3.9963067 -3.9766452 -3.9357119 -3.9076908 -3.9084473 -3.9322295 -3.9727087 -4.0178246 -4.0457592 -4.0442986][-4.1018224 -4.0815368 -4.0845656 -4.09597 -4.0968914 -4.08912 -4.0734057 -4.0479174 -4.0317106 -4.0262871 -4.0355625 -4.0574126 -4.0752029 -4.0725951 -4.0436282][-4.179193 -4.1618204 -4.1629586 -4.1704946 -4.1726537 -4.1678104 -4.1575441 -4.1411314 -4.1277275 -4.1176772 -4.1165648 -4.1242023 -4.1255164 -4.1046042 -4.06308][-4.2262659 -4.2106385 -4.2097988 -4.214787 -4.2183652 -4.2167792 -4.2107034 -4.1987772 -4.1866784 -4.1762023 -4.1715465 -4.17179 -4.1609988 -4.1324091 -4.0950141][-4.2550459 -4.241569 -4.2397156 -4.24284 -4.2471695 -4.2482281 -4.245007 -4.234087 -4.2200847 -4.2083569 -4.2014256 -4.1961842 -4.1782632 -4.1478004 -4.1207633][-4.272243 -4.2626538 -4.2613297 -4.2630692 -4.2668524 -4.2685962 -4.2668586 -4.2564712 -4.2410927 -4.2257891 -4.2124968 -4.201 -4.1796789 -4.1550016 -4.1411972][-4.2827072 -4.2766685 -4.2771692 -4.2791524 -4.2813029 -4.2809443 -4.2781539 -4.2694473 -4.2559295 -4.2393875 -4.2217264 -4.2066741 -4.1848435 -4.1662817 -4.1619868]]...]
INFO - root - 2017-12-07 21:23:19.400676: step 55310, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 54h:06m:10s remains)
INFO - root - 2017-12-07 21:23:26.332413: step 55320, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 56h:08m:57s remains)
INFO - root - 2017-12-07 21:23:33.155018: step 55330, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 52h:15m:33s remains)
INFO - root - 2017-12-07 21:23:39.945893: step 55340, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 51h:28m:53s remains)
INFO - root - 2017-12-07 21:23:46.692876: step 55350, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.655 sec/batch; 50h:25m:29s remains)
INFO - root - 2017-12-07 21:23:53.462277: step 55360, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 53h:20m:51s remains)
INFO - root - 2017-12-07 21:24:00.245671: step 55370, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 55h:00m:45s remains)
INFO - root - 2017-12-07 21:24:07.065717: step 55380, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 51h:39m:37s remains)
INFO - root - 2017-12-07 21:24:13.815893: step 55390, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 48h:41m:57s remains)
INFO - root - 2017-12-07 21:24:20.418870: step 55400, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 50h:31m:39s remains)
2017-12-07 21:24:21.146763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1432714 -4.1400871 -4.1593356 -4.1875277 -4.2241077 -4.2631373 -4.28872 -4.2874827 -4.2646375 -4.2332983 -4.2011957 -4.167285 -4.14408 -4.1548281 -4.1965175][-4.1405225 -4.1516414 -4.1811037 -4.2142882 -4.2526922 -4.2882805 -4.3093476 -4.3071537 -4.2860756 -4.2575278 -4.2273378 -4.1964507 -4.1743789 -4.1830468 -4.2181258][-4.1645446 -4.1873326 -4.2193513 -4.2492208 -4.2805419 -4.3021336 -4.3133779 -4.3091226 -4.2888775 -4.2654295 -4.2446041 -4.2261004 -4.2142377 -4.2222829 -4.245913][-4.1899323 -4.219522 -4.2499046 -4.2742167 -4.2910814 -4.2920456 -4.2870007 -4.2747097 -4.256424 -4.24584 -4.2416205 -4.2398667 -4.2400503 -4.2453942 -4.2551231][-4.198513 -4.2308993 -4.2598653 -4.274981 -4.2716751 -4.2485871 -4.219471 -4.1913218 -4.1781526 -4.1972404 -4.2247319 -4.2438865 -4.2526746 -4.2514634 -4.2478495][-4.1926732 -4.2257648 -4.25352 -4.2612352 -4.2359562 -4.1823878 -4.1167512 -4.0536089 -4.0454659 -4.1159291 -4.1917696 -4.236546 -4.254118 -4.2454505 -4.2289352][-4.1913996 -4.2239752 -4.2496986 -4.2523565 -4.210381 -4.1302342 -4.01692 -3.8925438 -3.8817315 -4.0177941 -4.1437712 -4.2141724 -4.244463 -4.2367568 -4.2101164][-4.2145939 -4.2418232 -4.2640028 -4.2630143 -4.2202444 -4.1364589 -4.0069942 -3.8577452 -3.8505301 -4.0013866 -4.1339955 -4.2045369 -4.237102 -4.2288103 -4.1952758][-4.2520046 -4.273602 -4.2916923 -4.290731 -4.2576032 -4.1932926 -4.0977082 -3.9963503 -3.9907057 -4.0842628 -4.1668434 -4.2096143 -4.2291961 -4.2109356 -4.1687169][-4.2907166 -4.3083429 -4.3217039 -4.3233232 -4.3024817 -4.2607021 -4.203362 -4.1468463 -4.1428285 -4.1851344 -4.213769 -4.2220931 -4.2201653 -4.1827121 -4.1255674][-4.3135738 -4.3287058 -4.3372831 -4.3390136 -4.32873 -4.3077455 -4.28004 -4.2514935 -4.2504 -4.2657628 -4.2639265 -4.2495847 -4.2286673 -4.1718493 -4.1000562][-4.3171186 -4.3302736 -4.3348565 -4.3367152 -4.3355455 -4.3302841 -4.3208904 -4.3111157 -4.3121634 -4.3137517 -4.2981687 -4.27637 -4.2477369 -4.1889915 -4.1193628][-4.3103566 -4.3176627 -4.3201137 -4.3248949 -4.3317003 -4.3364096 -4.3375044 -4.33975 -4.3461356 -4.3439617 -4.3274732 -4.3069224 -4.281054 -4.2342424 -4.1844716][-4.3043303 -4.3061838 -4.3071947 -4.3127441 -4.3216696 -4.32869 -4.3342538 -4.3431716 -4.352519 -4.3519969 -4.3430719 -4.3323951 -4.3139138 -4.2852387 -4.259151][-4.3002453 -4.2993784 -4.2998514 -4.3037844 -4.3102746 -4.3172178 -4.3241892 -4.3337264 -4.3427567 -4.3428731 -4.3394804 -4.3373203 -4.3267679 -4.3112855 -4.2996445]]...]
INFO - root - 2017-12-07 21:24:27.991764: step 55410, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 52h:01m:39s remains)
INFO - root - 2017-12-07 21:24:34.658305: step 55420, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 50h:24m:42s remains)
INFO - root - 2017-12-07 21:24:41.358399: step 55430, loss = 2.05, batch loss = 1.99 (13.2 examples/sec; 0.608 sec/batch; 46h:47m:09s remains)
INFO - root - 2017-12-07 21:24:48.193005: step 55440, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 52h:08m:11s remains)
INFO - root - 2017-12-07 21:24:54.905498: step 55450, loss = 2.04, batch loss = 1.99 (10.6 examples/sec; 0.753 sec/batch; 57h:58m:40s remains)
INFO - root - 2017-12-07 21:25:01.675810: step 55460, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 52h:38m:32s remains)
INFO - root - 2017-12-07 21:25:08.504876: step 55470, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 48h:39m:50s remains)
INFO - root - 2017-12-07 21:25:15.372538: step 55480, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 52h:31m:29s remains)
INFO - root - 2017-12-07 21:25:22.200032: step 55490, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 54h:16m:02s remains)
INFO - root - 2017-12-07 21:25:29.048249: step 55500, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 55h:33m:12s remains)
2017-12-07 21:25:29.841363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0059676 -3.985465 -3.9755127 -3.9821496 -4.015 -4.0625067 -4.1103592 -4.1419334 -4.1609221 -4.16903 -4.1706243 -4.1778822 -4.195013 -4.2164674 -4.2488542][-4.0199337 -3.99739 -3.9838331 -3.9765372 -3.9870327 -4.0204592 -4.0639696 -4.0963478 -4.1201935 -4.1391916 -4.1523929 -4.171977 -4.2009187 -4.2282839 -4.2613769][-4.0525489 -4.0364904 -4.0247874 -4.0123677 -4.007185 -4.0214882 -4.0484171 -4.0694 -4.0863066 -4.1071033 -4.1301908 -4.1616273 -4.2014403 -4.2368383 -4.2717314][-4.0880022 -4.0830469 -4.0718536 -4.0527406 -4.0408568 -4.0427017 -4.0539594 -4.0596118 -4.0650511 -4.0835071 -4.1135297 -4.1537867 -4.1993442 -4.2408347 -4.2790132][-4.1145382 -4.1201482 -4.1118059 -4.0888138 -4.0729413 -4.0659528 -4.0623336 -4.0491924 -4.0407147 -4.060348 -4.1010342 -4.1450329 -4.1903911 -4.2352753 -4.2769508][-4.12142 -4.1313877 -4.1236081 -4.0970559 -4.0750737 -4.0592871 -4.0437131 -4.014843 -3.9993443 -4.0288858 -4.0850134 -4.1349149 -4.1776695 -4.2225947 -4.2662826][-4.10759 -4.1178117 -4.1101265 -4.0832887 -4.0539942 -4.028029 -4.0019693 -3.9635634 -3.9471977 -3.984894 -4.0530744 -4.1130247 -4.1595755 -4.2079678 -4.2551794][-4.0893655 -4.0933914 -4.0825114 -4.0593896 -4.031559 -4.0060458 -3.97792 -3.9405696 -3.9273977 -3.9658988 -4.0357308 -4.1002884 -4.1508088 -4.2018952 -4.2514086][-4.0887823 -4.0868769 -4.0739155 -4.0569587 -4.039619 -4.0317512 -4.0178127 -3.9921498 -3.9790518 -4.00785 -4.0654674 -4.1213317 -4.1664281 -4.212975 -4.2582841][-4.1200161 -4.1154256 -4.1033368 -4.0884686 -4.0800452 -4.0880928 -4.090004 -4.0742235 -4.0553293 -4.0690408 -4.1098475 -4.153141 -4.1922522 -4.2329268 -4.2729526][-4.1673555 -4.159132 -4.1475215 -4.1333485 -4.1294422 -4.1444073 -4.1555181 -4.1461143 -4.1223373 -4.1206942 -4.1421351 -4.1726923 -4.2098179 -4.2485085 -4.284411][-4.2116537 -4.203104 -4.1938534 -4.1842909 -4.1841636 -4.1992259 -4.2125483 -4.20543 -4.1797237 -4.165071 -4.1693969 -4.190011 -4.2254 -4.2615461 -4.2920704][-4.234292 -4.229373 -4.2250905 -4.2208285 -4.2236929 -4.23736 -4.2493324 -4.2412744 -4.2167931 -4.1973839 -4.1942644 -4.2084394 -4.2401557 -4.2734933 -4.2992883][-4.2341352 -4.2317362 -4.2304659 -4.230278 -4.2343197 -4.2443552 -4.2526207 -4.2441487 -4.2219071 -4.2026715 -4.1992493 -4.2121778 -4.2426319 -4.2744489 -4.2993851][-4.2231736 -4.2228365 -4.2234559 -4.2248435 -4.2280126 -4.2346964 -4.2402287 -4.2315707 -4.2093487 -4.1884642 -4.1849227 -4.1981177 -4.2283931 -4.26068 -4.2895384]]...]
INFO - root - 2017-12-07 21:25:36.693389: step 55510, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 55h:45m:45s remains)
INFO - root - 2017-12-07 21:25:43.516781: step 55520, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 53h:30m:42s remains)
INFO - root - 2017-12-07 21:25:50.278124: step 55530, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 51h:52m:14s remains)
INFO - root - 2017-12-07 21:25:57.034108: step 55540, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 50h:19m:24s remains)
INFO - root - 2017-12-07 21:26:03.829977: step 55550, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 52h:25m:44s remains)
INFO - root - 2017-12-07 21:26:10.691513: step 55560, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 54h:48m:06s remains)
INFO - root - 2017-12-07 21:26:17.516900: step 55570, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 49h:19m:36s remains)
INFO - root - 2017-12-07 21:26:24.312852: step 55580, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 50h:24m:27s remains)
INFO - root - 2017-12-07 21:26:31.076034: step 55590, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 50h:12m:45s remains)
INFO - root - 2017-12-07 21:26:37.693327: step 55600, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 51h:04m:42s remains)
2017-12-07 21:26:38.579008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.11831 -4.0881248 -4.0890775 -4.0893617 -4.077755 -4.0661936 -4.0698276 -4.0950274 -4.1352577 -4.1760941 -4.2026453 -4.2157426 -4.20711 -4.1809411 -4.1428185][-4.0973144 -4.0616951 -4.0500493 -4.0452518 -4.0351219 -4.02922 -4.0435624 -4.0798821 -4.1232405 -4.1638985 -4.1908412 -4.2028165 -4.193387 -4.1657333 -4.1176529][-4.098002 -4.0647235 -4.0455136 -4.0324917 -4.0166206 -4.0070047 -4.0233984 -4.0609436 -4.1039286 -4.1446509 -4.1775436 -4.1949611 -4.1887703 -4.1655993 -4.1111155][-4.1202879 -4.0949259 -4.0826941 -4.0660744 -4.03671 -4.0141397 -4.0222182 -4.0470715 -4.0778589 -4.1133852 -4.1519408 -4.176322 -4.1763625 -4.1560378 -4.0974569][-4.166091 -4.1440682 -4.1339521 -4.1160088 -4.0735087 -4.0349965 -4.0289173 -4.03877 -4.0528917 -4.0758729 -4.1125288 -4.1444817 -4.1569271 -4.1420794 -4.0874786][-4.2001734 -4.1795092 -4.1674247 -4.1430163 -4.0937924 -4.0504551 -4.0325174 -4.0252013 -4.0268989 -4.0415988 -4.0748529 -4.1129556 -4.1367469 -4.1345329 -4.098731][-4.2035728 -4.1867 -4.1763053 -4.1488571 -4.094955 -4.0478816 -4.019362 -4.0005875 -3.9969676 -4.0130191 -4.0523577 -4.0987291 -4.1286836 -4.1341162 -4.1196508][-4.1988578 -4.1856818 -4.1741705 -4.142035 -4.088501 -4.0395555 -4.0033331 -3.97801 -3.9754615 -4.00328 -4.0569167 -4.1125026 -4.141191 -4.1421857 -4.1355782][-4.187778 -4.1714492 -4.1556439 -4.1252356 -4.0836082 -4.0433421 -4.0165033 -4.0005016 -3.9992771 -4.0236654 -4.0763812 -4.1326427 -4.1566758 -4.1506844 -4.1380525][-4.1766458 -4.15913 -4.1481986 -4.1323576 -4.1095552 -4.0876169 -4.0751791 -4.0664239 -4.059855 -4.0682325 -4.1021967 -4.14487 -4.1633339 -4.1546879 -4.1374731][-4.1765575 -4.1623073 -4.1580529 -4.1517115 -4.14293 -4.1346717 -4.1324167 -4.1285968 -4.1175885 -4.113512 -4.1267486 -4.1493611 -4.1594787 -4.1534081 -4.1375356][-4.1813288 -4.1684275 -4.1652045 -4.1573896 -4.1543655 -4.1562057 -4.1644192 -4.16496 -4.1501479 -4.13556 -4.1315074 -4.1420736 -4.148788 -4.1470413 -4.1392622][-4.1945567 -4.1816626 -4.1777167 -4.1673884 -4.1678638 -4.176724 -4.189394 -4.1909227 -4.1767974 -4.15633 -4.1431661 -4.1505742 -4.1566911 -4.1530747 -4.1487412][-4.2135348 -4.2047 -4.2018657 -4.1928983 -4.1927953 -4.2040329 -4.2178783 -4.2202778 -4.2071681 -4.1864786 -4.1735229 -4.17793 -4.1796732 -4.1723833 -4.1705289][-4.2320428 -4.22769 -4.2282119 -4.2239141 -4.2263007 -4.236104 -4.2459912 -4.2483463 -4.2390695 -4.2228703 -4.2145615 -4.2203913 -4.21962 -4.210011 -4.2063327]]...]
INFO - root - 2017-12-07 21:26:45.333054: step 55610, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 51h:51m:56s remains)
INFO - root - 2017-12-07 21:26:52.168462: step 55620, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 50h:33m:23s remains)
INFO - root - 2017-12-07 21:26:59.031837: step 55630, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 53h:59m:44s remains)
INFO - root - 2017-12-07 21:27:05.819082: step 55640, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 54h:56m:11s remains)
INFO - root - 2017-12-07 21:27:12.617192: step 55650, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 50h:49m:43s remains)
INFO - root - 2017-12-07 21:27:19.457700: step 55660, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 48h:33m:15s remains)
INFO - root - 2017-12-07 21:27:26.321308: step 55670, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.663 sec/batch; 51h:01m:07s remains)
INFO - root - 2017-12-07 21:27:33.203745: step 55680, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 57h:37m:54s remains)
INFO - root - 2017-12-07 21:27:39.941560: step 55690, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 54h:23m:26s remains)
INFO - root - 2017-12-07 21:27:46.592787: step 55700, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.712 sec/batch; 54h:46m:55s remains)
2017-12-07 21:27:47.338679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2205181 -4.1983166 -4.1892247 -4.1931977 -4.2058029 -4.2268634 -4.2480235 -4.260993 -4.2556548 -4.2342634 -4.1982827 -4.1556082 -4.1312504 -4.1360726 -4.175612][-4.2397509 -4.2237511 -4.2182913 -4.2168818 -4.2160969 -4.2201991 -4.2266474 -4.2299304 -4.2203345 -4.2008829 -4.1690273 -4.12657 -4.1033959 -4.11048 -4.1515946][-4.2468061 -4.2364621 -4.2348943 -4.229269 -4.2182808 -4.2140675 -4.2131886 -4.2097759 -4.1950979 -4.1740785 -4.1436934 -4.1017432 -4.0781193 -4.0846043 -4.124465][-4.225729 -4.2187452 -4.2224679 -4.2200742 -4.2061281 -4.1981888 -4.1945734 -4.1881351 -4.1740518 -4.15509 -4.1289773 -4.0905161 -4.0675993 -4.0720353 -4.1060872][-4.19456 -4.1926618 -4.2011757 -4.2024345 -4.184227 -4.1674466 -4.1585922 -4.1537542 -4.1495485 -4.1389275 -4.1233687 -4.0946503 -4.0757747 -4.0782089 -4.1052637][-4.1678562 -4.1669669 -4.1759882 -4.174798 -4.1489773 -4.1220708 -4.1116996 -4.1137371 -4.1224833 -4.1248732 -4.1235905 -4.1045589 -4.08907 -4.089159 -4.1090403][-4.1560373 -4.1469259 -4.1492834 -4.1390786 -4.0993042 -4.0557718 -4.0361867 -4.0422277 -4.0695825 -4.0985084 -4.1191921 -4.1149416 -4.1078639 -4.1117435 -4.128068][-4.1603079 -4.1413097 -4.1359415 -4.1176491 -4.062326 -3.9946704 -3.9517078 -3.9467664 -3.9862037 -4.0394626 -4.08184 -4.0946136 -4.1041417 -4.1232791 -4.1482849][-4.1776342 -4.1566067 -4.1544027 -4.1406789 -4.0901279 -4.0232544 -3.9725881 -3.9486618 -3.9705954 -4.0165267 -4.0586481 -4.0765162 -4.0948315 -4.1254783 -4.1616087][-4.21664 -4.2019167 -4.20549 -4.199595 -4.1654496 -4.1204023 -4.0868268 -4.0662413 -4.06993 -4.0880418 -4.1060991 -4.1110907 -4.1220727 -4.1481476 -4.180512][-4.2405729 -4.2332754 -4.2406154 -4.2396522 -4.2161851 -4.1865644 -4.1690025 -4.1610541 -4.1607904 -4.161871 -4.1567359 -4.1410809 -4.1343226 -4.1484904 -4.1760397][-4.2319555 -4.2285652 -4.238842 -4.245749 -4.2338586 -4.21259 -4.2008762 -4.1958361 -4.19006 -4.1804023 -4.1626592 -4.1313691 -4.11257 -4.1218867 -4.1486821][-4.1962981 -4.1901283 -4.1982074 -4.2089763 -4.2060194 -4.1914248 -4.1802773 -4.1710978 -4.1597805 -4.1453834 -4.1225214 -4.0811205 -4.0545855 -4.0697274 -4.1097174][-4.1502924 -4.1412067 -4.1438036 -4.1502285 -4.1500211 -4.1416774 -4.1340194 -4.1236687 -4.1094184 -4.092103 -4.064877 -4.0175557 -3.9861226 -4.0068583 -4.0631809][-4.1377425 -4.1322374 -4.1367669 -4.142056 -4.1427593 -4.1377006 -4.1330652 -4.1226315 -4.1069546 -4.0905805 -4.0670357 -4.0236492 -3.9920192 -4.0094576 -4.0651793]]...]
INFO - root - 2017-12-07 21:27:54.176068: step 55710, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 52h:44m:34s remains)
INFO - root - 2017-12-07 21:28:00.993757: step 55720, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 55h:26m:08s remains)
INFO - root - 2017-12-07 21:28:07.690890: step 55730, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 51h:26m:02s remains)
INFO - root - 2017-12-07 21:28:14.519888: step 55740, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 49h:15m:31s remains)
INFO - root - 2017-12-07 21:28:21.319425: step 55750, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.658 sec/batch; 50h:35m:23s remains)
INFO - root - 2017-12-07 21:28:28.021757: step 55760, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 53h:56m:06s remains)
INFO - root - 2017-12-07 21:28:34.823698: step 55770, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 50h:43m:02s remains)
INFO - root - 2017-12-07 21:28:41.535821: step 55780, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 48h:55m:05s remains)
INFO - root - 2017-12-07 21:28:48.350277: step 55790, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.636 sec/batch; 48h:53m:41s remains)
INFO - root - 2017-12-07 21:28:55.125186: step 55800, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 56h:54m:12s remains)
2017-12-07 21:28:55.925648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.199986 -4.1795273 -4.1628981 -4.1648855 -4.1778536 -4.196804 -4.20648 -4.2029433 -4.1986237 -4.2013779 -4.2076716 -4.2116094 -4.2186174 -4.2226071 -4.2271667][-4.1725097 -4.152041 -4.1378312 -4.1412692 -4.1541548 -4.173162 -4.1849122 -4.1862721 -4.1848364 -4.1867085 -4.1905866 -4.1912813 -4.1944518 -4.1952252 -4.19366][-4.1714168 -4.1576724 -4.1505823 -4.1557713 -4.1636448 -4.1760359 -4.1840391 -4.1851225 -4.1839147 -4.1846676 -4.1864691 -4.1845012 -4.1828971 -4.1772628 -4.16571][-4.1871066 -4.1861978 -4.1916513 -4.2007294 -4.2029219 -4.2059994 -4.2056866 -4.2043538 -4.2042875 -4.2055426 -4.2070255 -4.2042646 -4.199789 -4.1898308 -4.1704035][-4.1814604 -4.1920986 -4.2095361 -4.2206397 -4.2168303 -4.2118416 -4.201879 -4.197155 -4.2034492 -4.2109313 -4.2173452 -4.2192159 -4.2174959 -4.2118692 -4.1975288][-4.1481605 -4.16522 -4.1885266 -4.1971979 -4.1831975 -4.16899 -4.1426263 -4.1306772 -4.1440396 -4.1606035 -4.17467 -4.18327 -4.1907139 -4.1983857 -4.2045369][-4.104176 -4.1163297 -4.133287 -4.1293488 -4.0995255 -4.0733232 -4.03001 -4.0122728 -4.0379481 -4.0641317 -4.0849519 -4.1006713 -4.1170039 -4.1399927 -4.1691508][-4.0935512 -4.0920911 -4.0909009 -4.0649929 -4.0218496 -3.9893715 -3.9380751 -3.9188204 -3.9540513 -3.9838378 -4.0051756 -4.0206203 -4.0352883 -4.0612664 -4.1038747][-4.1542063 -4.1445742 -4.1286249 -4.0930805 -4.0548563 -4.0324974 -3.9957361 -3.982527 -4.0062933 -4.0169864 -4.0214148 -4.0253196 -4.0323491 -4.0528216 -4.0952663][-4.2454677 -4.2397327 -4.2261968 -4.2011447 -4.1781797 -4.1674161 -4.1481142 -4.141109 -4.1511116 -4.1492529 -4.1429973 -4.1374922 -4.138114 -4.1501141 -4.1788392][-4.306303 -4.3076105 -4.3022547 -4.2881193 -4.2750177 -4.2713628 -4.2662783 -4.2656031 -4.2701845 -4.2680721 -4.2637124 -4.256187 -4.2486877 -4.2472873 -4.25644][-4.3267031 -4.3284168 -4.3271761 -4.3190351 -4.310421 -4.3081107 -4.3081717 -4.3110638 -4.3161497 -4.3181429 -4.315455 -4.3050947 -4.2907095 -4.2775412 -4.2721167][-4.3318872 -4.3292704 -4.3275337 -4.3218913 -4.313683 -4.3073072 -4.3032708 -4.3030434 -4.3066344 -4.3088365 -4.3020315 -4.2863612 -4.2669859 -4.2516685 -4.2443504][-4.3338246 -4.3254781 -4.318985 -4.3114486 -4.299365 -4.2852316 -4.2743373 -4.2703476 -4.274086 -4.2765594 -4.2646432 -4.2426133 -4.2196412 -4.2048411 -4.1972604][-4.3222532 -4.3091621 -4.2978334 -4.285841 -4.2658033 -4.2418404 -4.2261376 -4.223753 -4.2330489 -4.2375836 -4.2191715 -4.1854277 -4.1536393 -4.1369929 -4.1313324]]...]
INFO - root - 2017-12-07 21:29:02.623963: step 55810, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 48h:32m:13s remains)
INFO - root - 2017-12-07 21:29:09.314996: step 55820, loss = 2.03, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 51h:42m:36s remains)
INFO - root - 2017-12-07 21:29:16.145163: step 55830, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 53h:44m:30s remains)
INFO - root - 2017-12-07 21:29:22.925127: step 55840, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 54h:08m:36s remains)
INFO - root - 2017-12-07 21:29:29.816296: step 55850, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 53h:56m:32s remains)
INFO - root - 2017-12-07 21:29:36.613535: step 55860, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 47h:55m:59s remains)
INFO - root - 2017-12-07 21:29:43.528011: step 55870, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.690 sec/batch; 52h:59m:58s remains)
INFO - root - 2017-12-07 21:29:50.416869: step 55880, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 54h:06m:50s remains)
INFO - root - 2017-12-07 21:29:57.194747: step 55890, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 51h:55m:39s remains)
INFO - root - 2017-12-07 21:30:03.859747: step 55900, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 53h:08m:44s remains)
2017-12-07 21:30:04.715179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3182182 -4.3085394 -4.3043661 -4.3025403 -4.3013635 -4.30229 -4.3034372 -4.306005 -4.3087091 -4.3086505 -4.3072181 -4.3009968 -4.2898412 -4.2777023 -4.26914][-4.3231 -4.3148103 -4.3112831 -4.3083558 -4.305625 -4.3051205 -4.304595 -4.3071885 -4.3113713 -4.3133359 -4.3145204 -4.31214 -4.3045735 -4.2972388 -4.2932105][-4.3189759 -4.3077579 -4.2985382 -4.2881866 -4.2798681 -4.2761922 -4.2708807 -4.2708678 -4.2759542 -4.2790594 -4.2836576 -4.2870369 -4.2840366 -4.2803378 -4.2787118][-4.2915177 -4.2718377 -4.254003 -4.2354307 -4.2218494 -4.215714 -4.2068992 -4.2061028 -4.2132921 -4.21632 -4.2225618 -4.2301888 -4.2284307 -4.2271638 -4.2275558][-4.2458286 -4.2156463 -4.18796 -4.1579733 -4.1393347 -4.131146 -4.1169062 -4.1183786 -4.1271639 -4.1276655 -4.1366329 -4.1465964 -4.1436062 -4.1451273 -4.1431222][-4.1820483 -4.1430283 -4.1077814 -4.0677581 -4.0469613 -4.0336437 -4.0136747 -4.017292 -4.0261669 -4.0235443 -4.0342484 -4.0445304 -4.042419 -4.0461063 -4.0386434][-4.074625 -4.0324984 -3.9929721 -3.9484892 -3.9244983 -3.8998148 -3.8717403 -3.8777866 -3.885299 -3.8809917 -3.895292 -3.9062202 -3.9061477 -3.9131374 -3.8993123][-3.967216 -3.9231741 -3.8809519 -3.8338628 -3.8053935 -3.7717481 -3.739275 -3.7488284 -3.7575507 -3.7559819 -3.777632 -3.7942164 -3.7983036 -3.8134637 -3.7992764][-3.9469192 -3.9061096 -3.8679414 -3.826468 -3.8017809 -3.7691543 -3.7425642 -3.75703 -3.7649562 -3.7663383 -3.7933443 -3.8129115 -3.8232472 -3.8460178 -3.8372755][-4.0480118 -4.021318 -3.9980586 -3.9744949 -3.9627852 -3.9394555 -3.9218485 -3.9363797 -3.9406285 -3.9401517 -3.9630904 -3.9755154 -3.9844053 -4.0052505 -4.00204][-4.1844149 -4.1725688 -4.1616054 -4.1511865 -4.1471977 -4.1308026 -4.1182036 -4.1275854 -4.1288519 -4.1286044 -4.142735 -4.1461883 -4.1505857 -4.1624937 -4.1590323][-4.2818122 -4.2779727 -4.2738991 -4.2703071 -4.2700582 -4.2630386 -4.2579088 -4.2646074 -4.2655869 -4.2662339 -4.2720413 -4.2704864 -4.2707567 -4.2760825 -4.2733841][-4.321157 -4.3236504 -4.3253646 -4.3250546 -4.3270984 -4.3255816 -4.3241115 -4.3277149 -4.3284063 -4.3301778 -4.3340926 -4.3327756 -4.333324 -4.3367391 -4.3349104][-4.3379812 -4.3426275 -4.346385 -4.3478212 -4.3491888 -4.3494158 -4.3498282 -4.3513908 -4.3521628 -4.3540845 -4.3570094 -4.3574071 -4.3588295 -4.3605156 -4.3595805][-4.342175 -4.3461728 -4.34829 -4.3492856 -4.3497772 -4.349072 -4.3485651 -4.3486371 -4.3492732 -4.3510237 -4.353508 -4.3555503 -4.3573136 -4.3581123 -4.35756]]...]
INFO - root - 2017-12-07 21:30:11.563074: step 55910, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 55h:54m:14s remains)
INFO - root - 2017-12-07 21:30:18.375945: step 55920, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 52h:10m:05s remains)
INFO - root - 2017-12-07 21:30:25.111545: step 55930, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.616 sec/batch; 47h:21m:22s remains)
INFO - root - 2017-12-07 21:30:31.957022: step 55940, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 49h:19m:27s remains)
INFO - root - 2017-12-07 21:30:38.797157: step 55950, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 55h:47m:42s remains)
INFO - root - 2017-12-07 21:30:45.633383: step 55960, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.737 sec/batch; 56h:35m:27s remains)
INFO - root - 2017-12-07 21:30:52.476405: step 55970, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 51h:48m:42s remains)
INFO - root - 2017-12-07 21:30:59.235343: step 55980, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 49h:42m:32s remains)
INFO - root - 2017-12-07 21:31:06.090246: step 55990, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 56h:26m:30s remains)
INFO - root - 2017-12-07 21:31:12.856364: step 56000, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 52h:48m:54s remains)
2017-12-07 21:31:13.627024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1943941 -4.1729207 -4.1554742 -4.1583548 -4.1668587 -4.1773238 -4.1925583 -4.2036958 -4.1971946 -4.1860285 -4.1738076 -4.1543145 -4.1505871 -4.1621614 -4.1777563][-4.2048864 -4.1743803 -4.1593761 -4.1723475 -4.1882558 -4.20029 -4.212122 -4.2159104 -4.2002287 -4.1818662 -4.16363 -4.1354251 -4.13586 -4.160109 -4.1836066][-4.209486 -4.1812625 -4.1710157 -4.1850524 -4.1998053 -4.2090936 -4.2138672 -4.2135863 -4.2013597 -4.1879215 -4.165679 -4.1285229 -4.1235743 -4.1447282 -4.1698384][-4.2098646 -4.1941509 -4.1901951 -4.2016082 -4.2091856 -4.2088532 -4.2029753 -4.1970124 -4.1949568 -4.1954417 -4.1806483 -4.1491909 -4.1380882 -4.1466389 -4.1644807][-4.1976457 -4.195498 -4.2027946 -4.2150893 -4.2163353 -4.2016816 -4.1784711 -4.1627707 -4.1719537 -4.1948605 -4.1996984 -4.1866355 -4.1753078 -4.1772709 -4.1907997][-4.1890216 -4.20174 -4.2206163 -4.235714 -4.2265739 -4.1912017 -4.1426506 -4.1150074 -4.1365905 -4.18328 -4.2110553 -4.2159667 -4.2109032 -4.213212 -4.2259431][-4.1893268 -4.2069788 -4.2279754 -4.2411957 -4.2205234 -4.16351 -4.09906 -4.07413 -4.1130915 -4.1757336 -4.212616 -4.2238679 -4.2250175 -4.2324262 -4.244493][-4.1734486 -4.1885958 -4.2081633 -4.2139897 -4.183197 -4.1189466 -4.0664334 -4.0642276 -4.1153011 -4.1756582 -4.2078438 -4.2158813 -4.2156425 -4.2211375 -4.2349243][-4.1415558 -4.1550641 -4.1750522 -4.1822758 -4.1566162 -4.1041479 -4.0731277 -4.0929174 -4.1453352 -4.1890306 -4.2033796 -4.1972 -4.1825767 -4.174211 -4.1836061][-4.1179829 -4.1344275 -4.1578984 -4.1728573 -4.1617446 -4.1320386 -4.1193552 -4.1461787 -4.1889176 -4.2161231 -4.2140374 -4.1926093 -4.1576591 -4.1266251 -4.1183825][-4.1228356 -4.1459484 -4.1712966 -4.1912227 -4.1913686 -4.1807466 -4.1801553 -4.2002344 -4.2285047 -4.246397 -4.24026 -4.2144923 -4.1647558 -4.1147842 -4.0851836][-4.1549025 -4.1849689 -4.2063608 -4.2245836 -4.2284474 -4.2239571 -4.2231574 -4.2293968 -4.2474842 -4.2650986 -4.2653646 -4.2443881 -4.192225 -4.1358685 -4.1015439][-4.1911068 -4.2198849 -4.2338986 -4.2476521 -4.2512755 -4.2424679 -4.2343225 -4.2306514 -4.2440777 -4.2672491 -4.2757654 -4.2594366 -4.2133532 -4.1602292 -4.1296616][-4.2167988 -4.2381334 -4.2427421 -4.2516861 -4.2516785 -4.2390413 -4.22581 -4.2224526 -4.2360754 -4.26021 -4.2693539 -4.2483559 -4.2000761 -4.1493015 -4.1234956][-4.2199 -4.2336264 -4.2323256 -4.2389932 -4.2390203 -4.2248554 -4.2119751 -4.2094493 -4.2201004 -4.2385798 -4.2451334 -4.2203722 -4.1680532 -4.1192636 -4.10082]]...]
INFO - root - 2017-12-07 21:31:20.451800: step 56010, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 53h:22m:46s remains)
INFO - root - 2017-12-07 21:31:27.249407: step 56020, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 55h:02m:45s remains)
INFO - root - 2017-12-07 21:31:33.992312: step 56030, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 52h:03m:06s remains)
INFO - root - 2017-12-07 21:31:40.842787: step 56040, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 51h:59m:32s remains)
INFO - root - 2017-12-07 21:31:47.626649: step 56050, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 48h:26m:42s remains)
INFO - root - 2017-12-07 21:31:54.438462: step 56060, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 51h:45m:43s remains)
INFO - root - 2017-12-07 21:32:01.144006: step 56070, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.748 sec/batch; 57h:24m:35s remains)
INFO - root - 2017-12-07 21:32:07.890887: step 56080, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 51h:03m:25s remains)
INFO - root - 2017-12-07 21:32:14.684625: step 56090, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 48h:58m:53s remains)
INFO - root - 2017-12-07 21:32:21.248921: step 56100, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 49h:54m:19s remains)
2017-12-07 21:32:22.042892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2294354 -4.2634187 -4.2959619 -4.308537 -4.3003473 -4.282793 -4.2665 -4.2598147 -4.2635641 -4.26772 -4.259027 -4.23826 -4.2157607 -4.1946187 -4.1898961][-4.2054496 -4.247858 -4.2895145 -4.3028774 -4.2849212 -4.2499447 -4.2144709 -4.1992655 -4.2125735 -4.2438254 -4.2666087 -4.2720103 -4.2694178 -4.26096 -4.2539873][-4.1731448 -4.2227592 -4.2728343 -4.2903647 -4.2655072 -4.212306 -4.1569681 -4.1309443 -4.152029 -4.2057447 -4.2568779 -4.2861371 -4.3018141 -4.3059034 -4.3016591][-4.1353116 -4.190105 -4.249311 -4.2733073 -4.2445354 -4.1764917 -4.101347 -4.0589671 -4.0783606 -4.1469183 -4.2197323 -4.2714839 -4.3026333 -4.316978 -4.3167448][-4.1094213 -4.164907 -4.2311268 -4.2613521 -4.233129 -4.1588616 -4.0705285 -4.00736 -4.0129166 -4.0856872 -4.1718812 -4.2412691 -4.2836289 -4.30453 -4.308239][-4.1198277 -4.1654024 -4.2294683 -4.2624197 -4.2353792 -4.1629663 -4.0746012 -4.001678 -3.9897678 -4.0561509 -4.1459374 -4.2205129 -4.2643385 -4.2867479 -4.2942834][-4.163806 -4.1956196 -4.2455058 -4.2710724 -4.2423329 -4.1741614 -4.0942106 -4.0255952 -4.0038066 -4.0591636 -4.1443305 -4.2138505 -4.2521038 -4.2721791 -4.2812071][-4.2174063 -4.2354875 -4.2678156 -4.280417 -4.2470784 -4.1799922 -4.107594 -4.048481 -4.0250435 -4.0712409 -4.1516161 -4.2145915 -4.2469487 -4.2648349 -4.2752976][-4.2561874 -4.2657657 -4.2843642 -4.2844906 -4.2448425 -4.1756711 -4.1043787 -4.0499668 -4.0273094 -4.066833 -4.144774 -4.2065735 -4.2396545 -4.2602 -4.2752361][-4.2700191 -4.2804575 -4.2906213 -4.2816939 -4.2388659 -4.1693892 -4.0983582 -4.0456996 -4.0225267 -4.0559049 -4.1317782 -4.1978292 -4.23705 -4.2641544 -4.2849035][-4.2817655 -4.2935667 -4.2997541 -4.2847095 -4.2429662 -4.17996 -4.1144042 -4.0659761 -4.0418258 -4.0667191 -4.136127 -4.2036071 -4.248415 -4.2812281 -4.3040447][-4.3022547 -4.3141785 -4.3183985 -4.3024626 -4.2633886 -4.2055678 -4.1493649 -4.1108465 -4.0898604 -4.1030831 -4.1579256 -4.2159681 -4.2593193 -4.2929225 -4.3147683][-4.3229027 -4.3344007 -4.3378158 -4.3245945 -4.2909226 -4.2392354 -4.1925273 -4.16361 -4.1481953 -4.153204 -4.1891522 -4.2280531 -4.2637577 -4.2924957 -4.3114109][-4.3298531 -4.3411241 -4.3456931 -4.3350649 -4.3062358 -4.2597914 -4.2168045 -4.1927543 -4.1859136 -4.1918707 -4.2165737 -4.2398167 -4.2647624 -4.2863836 -4.3023353][-4.3231921 -4.3343129 -4.3392324 -4.3295856 -4.3042049 -4.2650905 -4.226306 -4.2034888 -4.2034612 -4.2132044 -4.23312 -4.2503142 -4.2679176 -4.2827392 -4.2963667]]...]
INFO - root - 2017-12-07 21:32:28.888104: step 56110, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 50h:36m:01s remains)
INFO - root - 2017-12-07 21:32:35.728131: step 56120, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 49h:32m:05s remains)
INFO - root - 2017-12-07 21:32:42.542146: step 56130, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 50h:50m:09s remains)
INFO - root - 2017-12-07 21:32:49.315862: step 56140, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 55h:11m:52s remains)
INFO - root - 2017-12-07 21:32:56.201293: step 56150, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.737 sec/batch; 56h:34m:54s remains)
INFO - root - 2017-12-07 21:33:02.954698: step 56160, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 51h:25m:46s remains)
INFO - root - 2017-12-07 21:33:09.701282: step 56170, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 50h:48m:05s remains)
INFO - root - 2017-12-07 21:33:16.368974: step 56180, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 48h:34m:57s remains)
INFO - root - 2017-12-07 21:33:23.203097: step 56190, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 49h:16m:33s remains)
INFO - root - 2017-12-07 21:33:29.880947: step 56200, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 55h:42m:03s remains)
2017-12-07 21:33:30.581234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.263145 -4.2602272 -4.2565789 -4.2457213 -4.2267632 -4.2078218 -4.1998038 -4.2034163 -4.2111344 -4.2112355 -4.1994872 -4.1935592 -4.2035022 -4.2264471 -4.2483196][-4.26801 -4.2660332 -4.2649331 -4.2567554 -4.2382045 -4.2208543 -4.2153773 -4.2191348 -4.223835 -4.2172127 -4.197257 -4.1847787 -4.1960087 -4.2199578 -4.2436266][-4.2552457 -4.2511787 -4.251893 -4.2464924 -4.2292743 -4.2140565 -4.2110767 -4.2174516 -4.2220287 -4.2130094 -4.1906805 -4.1743331 -4.185626 -4.2078524 -4.22898][-4.2239704 -4.2184739 -4.2209206 -4.2196856 -4.2062874 -4.1941214 -4.1918473 -4.2002797 -4.2058835 -4.2011189 -4.1857295 -4.1718249 -4.1818223 -4.1997375 -4.2188559][-4.1970634 -4.19109 -4.1955571 -4.199801 -4.1882944 -4.1752229 -4.1674733 -4.1703925 -4.1789417 -4.1876841 -4.18962 -4.1866713 -4.1963077 -4.2080941 -4.2213082][-4.1807027 -4.1718464 -4.17664 -4.1812654 -4.1665792 -4.1437874 -4.1188788 -4.1049671 -4.1186404 -4.1496811 -4.1783867 -4.1949811 -4.2057242 -4.2122626 -4.2150836][-4.1737638 -4.15931 -4.1555924 -4.152544 -4.1273484 -4.0849714 -4.0280166 -3.9820552 -4.0003643 -4.066988 -4.130003 -4.1685638 -4.1832995 -4.1883059 -4.1853766][-4.17324 -4.1545386 -4.1409845 -4.1266088 -4.0881557 -4.0261364 -3.9361877 -3.855772 -3.8809803 -3.9864173 -4.0807662 -4.1350865 -4.1520858 -4.1560349 -4.1540513][-4.1868196 -4.1702647 -4.1535349 -4.133594 -4.0913572 -4.0250454 -3.9313407 -3.8503754 -3.878341 -3.9900947 -4.0859709 -4.1371374 -4.1464391 -4.1429009 -4.1419249][-4.2205095 -4.2100282 -4.1963282 -4.1805067 -4.148365 -4.0953331 -4.0280008 -3.975445 -3.9948468 -4.0730329 -4.1440115 -4.1800179 -4.1751056 -4.1583591 -4.1541929][-4.2479897 -4.244031 -4.2372689 -4.232903 -4.215322 -4.1817622 -4.1443825 -4.1149912 -4.1212325 -4.1622171 -4.2052355 -4.2284231 -4.2187157 -4.1988583 -4.1950741][-4.2530866 -4.2536616 -4.2558341 -4.261651 -4.2573195 -4.2393031 -4.2206264 -4.2073445 -4.2092524 -4.2287965 -4.2524047 -4.2681479 -4.2646904 -4.2517977 -4.2492967][-4.223711 -4.2311153 -4.245801 -4.2630973 -4.2714577 -4.266346 -4.2591877 -4.2544637 -4.2547536 -4.2629743 -4.2734785 -4.2836823 -4.2874193 -4.2857976 -4.2858777][-4.1739964 -4.1875992 -4.2123175 -4.2381473 -4.2546406 -4.257566 -4.2552552 -4.2539377 -4.2552733 -4.260148 -4.2679472 -4.2782412 -4.28611 -4.2911224 -4.2919312][-4.1196208 -4.1387191 -4.1706862 -4.2025981 -4.2259016 -4.2356181 -4.2356348 -4.2353773 -4.2378969 -4.2432823 -4.2529092 -4.2653913 -4.2750492 -4.2809381 -4.2809129]]...]
INFO - root - 2017-12-07 21:33:37.308023: step 56210, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 48h:29m:34s remains)
INFO - root - 2017-12-07 21:33:44.224404: step 56220, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 51h:43m:24s remains)
INFO - root - 2017-12-07 21:33:50.966036: step 56230, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.739 sec/batch; 56h:41m:08s remains)
INFO - root - 2017-12-07 21:33:57.828518: step 56240, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 55h:31m:42s remains)
INFO - root - 2017-12-07 21:34:04.595884: step 56250, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.632 sec/batch; 48h:32m:03s remains)
INFO - root - 2017-12-07 21:34:11.429380: step 56260, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 48h:38m:05s remains)
INFO - root - 2017-12-07 21:34:18.303045: step 56270, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 50h:38m:51s remains)
INFO - root - 2017-12-07 21:34:25.044352: step 56280, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 54h:58m:59s remains)
INFO - root - 2017-12-07 21:34:31.812353: step 56290, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 53h:56m:53s remains)
INFO - root - 2017-12-07 21:34:38.485062: step 56300, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 51h:06m:35s remains)
2017-12-07 21:34:39.196085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2472849 -4.25515 -4.2867126 -4.3104787 -4.3201694 -4.32201 -4.312068 -4.2892137 -4.26387 -4.2548208 -4.2635555 -4.2759652 -4.293776 -4.3080492 -4.3072524][-4.2402987 -4.252944 -4.2936077 -4.3226132 -4.3333797 -4.3294492 -4.3090067 -4.270864 -4.2358613 -4.2345896 -4.2555804 -4.2760558 -4.2975941 -4.3120694 -4.3078423][-4.2376647 -4.2541451 -4.2979984 -4.3248429 -4.3320289 -4.3180723 -4.2809587 -4.22922 -4.198844 -4.2171612 -4.2546358 -4.28383 -4.3083715 -4.3219581 -4.3167171][-4.2450876 -4.2631612 -4.3028054 -4.3252716 -4.324049 -4.2915621 -4.23001 -4.1642318 -4.1486506 -4.1979442 -4.2526445 -4.2887888 -4.3146181 -4.3274508 -4.3247156][-4.2487712 -4.27136 -4.3058071 -4.3212028 -4.3077407 -4.2489824 -4.1566181 -4.0798092 -4.0917096 -4.1749387 -4.2432 -4.2847419 -4.31212 -4.3257613 -4.3275805][-4.2461734 -4.2779846 -4.3107963 -4.3144431 -4.2799497 -4.1910052 -4.0705771 -3.9954844 -4.0494275 -4.1602364 -4.2351317 -4.2769556 -4.3065248 -4.3247194 -4.3327956][-4.2537541 -4.2933593 -4.3174338 -4.3005614 -4.2388139 -4.12232 -3.9803646 -3.9247127 -4.0263963 -4.1584115 -4.2340903 -4.2735629 -4.3002596 -4.3216357 -4.3316503][-4.2640619 -4.3039622 -4.3141279 -4.2794428 -4.1983094 -4.0669627 -3.9165168 -3.8853929 -4.0267253 -4.1679392 -4.2377257 -4.2728772 -4.2978082 -4.323267 -4.3298674][-4.2752504 -4.311224 -4.3108978 -4.2681785 -4.18646 -4.066247 -3.9377658 -3.930975 -4.0694947 -4.1936121 -4.2512541 -4.2787375 -4.3037982 -4.33005 -4.3313179][-4.2883344 -4.3109393 -4.3010011 -4.2578907 -4.1852922 -4.0900016 -4.0007591 -4.0146809 -4.1267648 -4.2226396 -4.2650638 -4.2857985 -4.3137875 -4.3380771 -4.3338923][-4.2962728 -4.3028913 -4.28563 -4.2459083 -4.1868057 -4.1197724 -4.0717707 -4.100389 -4.1853056 -4.2500954 -4.2742176 -4.2900972 -4.3196836 -4.3417215 -4.3364263][-4.3008127 -4.2957482 -4.2750435 -4.2438073 -4.20283 -4.1631041 -4.1463194 -4.1774487 -4.2336111 -4.2717671 -4.2844124 -4.2980628 -4.3222222 -4.3360305 -4.3297825][-4.3021874 -4.2937503 -4.2781076 -4.2603793 -4.2358928 -4.21439 -4.2139387 -4.2386322 -4.2712884 -4.2899818 -4.2994852 -4.3111272 -4.3269625 -4.3303819 -4.3206019][-4.2908497 -4.289031 -4.2826166 -4.274971 -4.2630553 -4.2555065 -4.2608433 -4.2779527 -4.292872 -4.3002505 -4.3074455 -4.3174787 -4.3284817 -4.3274031 -4.3147969][-4.2710948 -4.280046 -4.2828474 -4.2807746 -4.2761092 -4.2766681 -4.2835073 -4.2946916 -4.2968755 -4.2978711 -4.3058486 -4.3166389 -4.3259416 -4.3269353 -4.316668]]...]
INFO - root - 2017-12-07 21:34:46.045911: step 56310, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 55h:37m:55s remains)
INFO - root - 2017-12-07 21:34:52.927630: step 56320, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.760 sec/batch; 58h:20m:18s remains)
INFO - root - 2017-12-07 21:34:59.632591: step 56330, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 47h:41m:04s remains)
INFO - root - 2017-12-07 21:35:06.370423: step 56340, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.615 sec/batch; 47h:12m:49s remains)
INFO - root - 2017-12-07 21:35:13.223719: step 56350, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 50h:12m:38s remains)
INFO - root - 2017-12-07 21:35:20.013929: step 56360, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.713 sec/batch; 54h:39m:47s remains)
INFO - root - 2017-12-07 21:35:26.698986: step 56370, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 55h:20m:28s remains)
INFO - root - 2017-12-07 21:35:33.295386: step 56380, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 47h:57m:49s remains)
INFO - root - 2017-12-07 21:35:40.022718: step 56390, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 48h:24m:12s remains)
INFO - root - 2017-12-07 21:35:46.675492: step 56400, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 53h:07m:51s remains)
2017-12-07 21:35:47.443317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2569308 -4.2601967 -4.2737651 -4.2828574 -4.2810783 -4.2736259 -4.2681017 -4.2672119 -4.2639 -4.2526093 -4.2283564 -4.2054553 -4.19918 -4.2154193 -4.2525721][-4.2794948 -4.2780371 -4.287828 -4.2980242 -4.2988987 -4.2910633 -4.2824659 -4.2762451 -4.2715621 -4.2625346 -4.2423959 -4.2238231 -4.2195091 -4.235177 -4.2683225][-4.2984862 -4.2944145 -4.2999406 -4.3071527 -4.3074989 -4.3004327 -4.2911482 -4.2817111 -4.2758732 -4.2705326 -4.2575631 -4.2458758 -4.2442369 -4.2601385 -4.2879944][-4.2952447 -4.2907519 -4.2929735 -4.2951312 -4.2920284 -4.287096 -4.2809381 -4.2715254 -4.2655854 -4.2619834 -4.2548184 -4.2487893 -4.2513471 -4.2705073 -4.2979212][-4.2744532 -4.2688093 -4.2666659 -4.2611327 -4.2516708 -4.2474351 -4.2466612 -4.24179 -4.2379251 -4.234529 -4.2295203 -4.2294269 -4.2398119 -4.2655578 -4.296865][-4.2537565 -4.2427664 -4.2301145 -4.2127862 -4.1955771 -4.19249 -4.19815 -4.2011714 -4.1999288 -4.1925678 -4.1856561 -4.1909313 -4.2126164 -4.2480183 -4.2856402][-4.2366366 -4.220046 -4.1970987 -4.1676683 -4.1427622 -4.1399589 -4.1511827 -4.1635656 -4.1664572 -4.1553659 -4.1458154 -4.1573763 -4.1892214 -4.2322745 -4.2745738][-4.2144871 -4.1969543 -4.1707458 -4.1358995 -4.1059065 -4.1001391 -4.111156 -4.128335 -4.1353064 -4.1268911 -4.1205392 -4.1395249 -4.1782761 -4.2247481 -4.269011][-4.1891069 -4.1775932 -4.1575832 -4.1262083 -4.0956888 -4.0851264 -4.0918474 -4.1063509 -4.1152244 -4.1154156 -4.11725 -4.1412859 -4.1816144 -4.2263894 -4.2694492][-4.1644011 -4.165133 -4.1604614 -4.1436739 -4.1215343 -4.1109686 -4.114501 -4.12505 -4.1354027 -4.144834 -4.1530466 -4.1739945 -4.2061257 -4.2422915 -4.2781143][-4.1550946 -4.1675463 -4.1780334 -4.1771741 -4.167295 -4.1585388 -4.156837 -4.1637487 -4.1767411 -4.1926274 -4.2060747 -4.2217855 -4.24262 -4.2670369 -4.2933111][-4.1518545 -4.1703672 -4.1896558 -4.1984248 -4.1984091 -4.1938739 -4.1908054 -4.1996889 -4.2179079 -4.239964 -4.2563295 -4.2667193 -4.2763724 -4.2896204 -4.3070793][-4.1508579 -4.1671395 -4.1890421 -4.2040915 -4.2129164 -4.2136068 -4.2127724 -4.2256465 -4.2481718 -4.2718382 -4.288691 -4.2960663 -4.2989774 -4.3050847 -4.3163085][-4.157032 -4.167047 -4.1860318 -4.2043929 -4.2188997 -4.2255688 -4.2297945 -4.244586 -4.2655168 -4.2851028 -4.3001308 -4.3074641 -4.3092284 -4.3133621 -4.3217764][-4.1686392 -4.1704473 -4.1827474 -4.2033958 -4.2216787 -4.23197 -4.23845 -4.24934 -4.2640243 -4.2790027 -4.2944732 -4.3050733 -4.3099566 -4.3154292 -4.3235083]]...]
INFO - root - 2017-12-07 21:35:54.130897: step 56410, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 50h:34m:27s remains)
INFO - root - 2017-12-07 21:36:00.857331: step 56420, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 48h:33m:08s remains)
INFO - root - 2017-12-07 21:36:07.662776: step 56430, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 53h:39m:23s remains)
INFO - root - 2017-12-07 21:36:14.407655: step 56440, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.748 sec/batch; 57h:22m:12s remains)
INFO - root - 2017-12-07 21:36:21.188258: step 56450, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 52h:52m:30s remains)
INFO - root - 2017-12-07 21:36:28.009036: step 56460, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.607 sec/batch; 46h:31m:29s remains)
INFO - root - 2017-12-07 21:36:34.698067: step 56470, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 48h:52m:01s remains)
INFO - root - 2017-12-07 21:36:41.451211: step 56480, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 53h:54m:07s remains)
INFO - root - 2017-12-07 21:36:48.247538: step 56490, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 54h:43m:19s remains)
INFO - root - 2017-12-07 21:36:54.800675: step 56500, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 51h:59m:55s remains)
2017-12-07 21:36:55.561685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1722026 -4.1630025 -4.1552625 -4.1471186 -4.1446471 -4.1621852 -4.1918983 -4.2182 -4.2317486 -4.223587 -4.1939487 -4.1574769 -4.1359982 -4.12924 -4.1293683][-4.170876 -4.1617532 -4.1565194 -4.1506815 -4.1444216 -4.1515875 -4.1734595 -4.2032533 -4.2317381 -4.2435455 -4.2361045 -4.2174745 -4.2007942 -4.1874528 -4.17451][-4.1896758 -4.1807089 -4.1782603 -4.1799431 -4.1734285 -4.1654968 -4.1698556 -4.1930704 -4.2282038 -4.2564259 -4.2663717 -4.2581835 -4.2407022 -4.2258172 -4.2100253][-4.21655 -4.2073569 -4.2055311 -4.2121077 -4.2070026 -4.1826186 -4.1621151 -4.1699209 -4.2068715 -4.2494607 -4.2713947 -4.2676749 -4.250545 -4.23891 -4.2256331][-4.2337337 -4.2217288 -4.2187057 -4.2279353 -4.22604 -4.1892314 -4.141778 -4.1248088 -4.1595988 -4.2156463 -4.2498198 -4.2514558 -4.237031 -4.2313666 -4.2227821][-4.214942 -4.1989818 -4.197969 -4.2160797 -4.2255049 -4.1878643 -4.1168389 -4.0693135 -4.0925493 -4.1550565 -4.1992865 -4.2090368 -4.20153 -4.2042828 -4.2038951][-4.1667352 -4.1446996 -4.1482234 -4.1831493 -4.2137728 -4.187696 -4.1082296 -4.0358295 -4.0411611 -4.101861 -4.1539297 -4.1739559 -4.1721578 -4.1800523 -4.1874213][-4.1140094 -4.0878115 -4.0972571 -4.1476059 -4.198761 -4.19354 -4.1257563 -4.04178 -4.0237694 -4.06901 -4.1182089 -4.1447611 -4.1525588 -4.1669626 -4.1820397][-4.0946012 -4.0713568 -4.083354 -4.1351447 -4.1913362 -4.2005291 -4.1507163 -4.073842 -4.0419254 -4.0631437 -4.0988054 -4.1246514 -4.1390057 -4.1591558 -4.1783075][-4.1169505 -4.1030736 -4.1117444 -4.1471314 -4.1891322 -4.19891 -4.1628647 -4.1045632 -4.0765672 -4.0866461 -4.1076903 -4.122818 -4.1330905 -4.152226 -4.17417][-4.163712 -4.16009 -4.163816 -4.1785984 -4.1997609 -4.2004242 -4.1682096 -4.12476 -4.1094637 -4.12067 -4.1356473 -4.1415834 -4.142869 -4.1535311 -4.1730862][-4.2134809 -4.2167234 -4.2178717 -4.2203298 -4.2295012 -4.2234168 -4.1906319 -4.1571608 -4.1561337 -4.1731377 -4.1854224 -4.1817055 -4.1699705 -4.16395 -4.1731768][-4.246851 -4.2571507 -4.2591205 -4.2562246 -4.2606258 -4.2537746 -4.2243633 -4.1969776 -4.20163 -4.2213345 -4.2324681 -4.2226267 -4.197648 -4.1747837 -4.1736875][-4.2528052 -4.2698188 -4.2739744 -4.2689667 -4.2708173 -4.2649546 -4.240303 -4.2184682 -4.2239833 -4.2441182 -4.2571583 -4.24808 -4.2136197 -4.1746125 -4.162374][-4.2452335 -4.2651229 -4.2705126 -4.2661791 -4.2693839 -4.2683158 -4.2501822 -4.2327228 -4.23745 -4.2559295 -4.2697616 -4.2639947 -4.22857 -4.1771245 -4.1525464]]...]
INFO - root - 2017-12-07 21:37:02.336485: step 56510, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 56h:17m:00s remains)
INFO - root - 2017-12-07 21:37:09.090268: step 56520, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 55h:28m:16s remains)
INFO - root - 2017-12-07 21:37:15.867073: step 56530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 50h:55m:27s remains)
INFO - root - 2017-12-07 21:37:22.513137: step 56540, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 49h:56m:09s remains)
INFO - root - 2017-12-07 21:37:29.257835: step 56550, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 49h:02m:23s remains)
INFO - root - 2017-12-07 21:37:36.068421: step 56560, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 54h:22m:18s remains)
INFO - root - 2017-12-07 21:37:42.909455: step 56570, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.757 sec/batch; 58h:01m:41s remains)
INFO - root - 2017-12-07 21:37:49.677581: step 56580, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 51h:23m:57s remains)
INFO - root - 2017-12-07 21:37:56.520471: step 56590, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 49h:10m:28s remains)
INFO - root - 2017-12-07 21:38:03.156105: step 56600, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 52h:34m:56s remains)
2017-12-07 21:38:03.855378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.339705 -4.3155065 -4.2745056 -4.2333608 -4.2019749 -4.1800733 -4.1624551 -4.1700525 -4.2038155 -4.2266459 -4.2295265 -4.2126961 -4.1908994 -4.1785445 -4.1797576][-4.3383765 -4.3151631 -4.2720151 -4.22826 -4.1924486 -4.1643457 -4.1412563 -4.1488991 -4.1813183 -4.2009664 -4.2041078 -4.1850419 -4.1640491 -4.1552296 -4.1591587][-4.3377857 -4.3147573 -4.2706823 -4.2279177 -4.1925917 -4.1652036 -4.1438522 -4.1542077 -4.1815214 -4.192296 -4.1939473 -4.1799059 -4.1650324 -4.1597624 -4.1635303][-4.3345232 -4.3054562 -4.2543192 -4.2078938 -4.1749387 -4.1512532 -4.1381755 -4.1560688 -4.1814151 -4.1834636 -4.1786308 -4.1663847 -4.1561375 -4.1523037 -4.1562915][-4.3329635 -4.2950344 -4.2290068 -4.1705117 -4.134377 -4.1102481 -4.1003842 -4.1261234 -4.1563854 -4.1590142 -4.146297 -4.1269035 -4.1139059 -4.1083274 -4.113462][-4.3285913 -4.28168 -4.2016306 -4.1284165 -4.0837193 -4.05428 -4.042099 -4.0798521 -4.1275353 -4.13956 -4.1198058 -4.0842948 -4.0608511 -4.0491414 -4.0476365][-4.3198018 -4.2650566 -4.1724668 -4.0846515 -4.0207925 -3.9700272 -3.9473059 -4.0020056 -4.0830927 -4.1236606 -4.1179209 -4.082366 -4.0489411 -4.0227652 -4.0033903][-4.3090878 -4.2499075 -4.1541576 -4.0540862 -3.9655919 -3.8698027 -3.8083844 -3.8821933 -4.0173578 -4.1054564 -4.1381817 -4.1280713 -4.0981474 -4.0566597 -4.0142074][-4.3015676 -4.2443042 -4.1583915 -4.0656466 -3.9723053 -3.8517704 -3.7425952 -3.8028898 -3.9645143 -4.0851121 -4.1486158 -4.1669316 -4.1479287 -4.0988517 -4.0445032][-4.3004465 -4.2485218 -4.1765404 -4.1050258 -4.0386009 -3.9525456 -3.8589165 -3.8788738 -3.9925454 -4.097105 -4.1663313 -4.197401 -4.1874723 -4.1406531 -4.0827551][-4.3022203 -4.2537494 -4.1903181 -4.1336479 -4.0888762 -4.0421886 -3.9851069 -3.9849911 -4.04857 -4.1231823 -4.1836629 -4.2173491 -4.2136116 -4.1750064 -4.1228361][-4.3084359 -4.26448 -4.2047324 -4.1510038 -4.1127472 -4.0805125 -4.043344 -4.0408797 -4.083981 -4.1402049 -4.1917949 -4.2272162 -4.231245 -4.2040386 -4.1615605][-4.3169265 -4.280724 -4.2266421 -4.1743212 -4.1355491 -4.1025677 -4.0709987 -4.0709 -4.1074505 -4.153954 -4.1994796 -4.2371674 -4.2497587 -4.2330503 -4.2007451][-4.3245668 -4.2968273 -4.2521439 -4.2071066 -4.1677914 -4.1336026 -4.1083059 -4.112669 -4.1436276 -4.1803775 -4.2189822 -4.25306 -4.2679787 -4.2600837 -4.2401276][-4.3324528 -4.3138123 -4.2822838 -4.2487168 -4.2142167 -4.1844139 -4.1695533 -4.1783395 -4.2022243 -4.2288313 -4.2559485 -4.2795057 -4.29198 -4.2894974 -4.2792478]]...]
INFO - root - 2017-12-07 21:38:10.588774: step 56610, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 53h:18m:42s remains)
INFO - root - 2017-12-07 21:38:17.407277: step 56620, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 50h:11m:22s remains)
INFO - root - 2017-12-07 21:38:24.166022: step 56630, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 50h:00m:48s remains)
INFO - root - 2017-12-07 21:38:30.878122: step 56640, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 50h:55m:11s remains)
INFO - root - 2017-12-07 21:38:37.699341: step 56650, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 53h:57m:58s remains)
INFO - root - 2017-12-07 21:38:44.504775: step 56660, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 55h:32m:07s remains)
INFO - root - 2017-12-07 21:38:51.201136: step 56670, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 51h:06m:10s remains)
INFO - root - 2017-12-07 21:38:57.919202: step 56680, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 48h:49m:50s remains)
INFO - root - 2017-12-07 21:39:04.685123: step 56690, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 55h:40m:18s remains)
INFO - root - 2017-12-07 21:39:11.263101: step 56700, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 53h:48m:10s remains)
2017-12-07 21:39:11.948941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2535973 -4.262218 -4.2741733 -4.2770596 -4.271771 -4.2651138 -4.2649579 -4.2755594 -4.290566 -4.2975063 -4.2944446 -4.2907777 -4.2866197 -4.2929287 -4.3134613][-4.2589941 -4.2653594 -4.2737112 -4.275363 -4.2664495 -4.2546244 -4.249125 -4.2546196 -4.267271 -4.2768903 -4.2762723 -4.2729549 -4.2620306 -4.2620683 -4.2852688][-4.2657795 -4.2708378 -4.2807293 -4.2845116 -4.27353 -4.2534513 -4.2392879 -4.2360473 -4.2445855 -4.2563958 -4.262342 -4.2600765 -4.24087 -4.2325335 -4.2555604][-4.2619667 -4.2702627 -4.2869267 -4.2933488 -4.2737794 -4.2388153 -4.211616 -4.2032204 -4.2150812 -4.23414 -4.2489715 -4.25098 -4.2292809 -4.2163014 -4.2361073][-4.2562609 -4.2673821 -4.2854934 -4.2866154 -4.2570567 -4.2119088 -4.1787548 -4.1702123 -4.1861577 -4.2126341 -4.231976 -4.2391095 -4.2239556 -4.2120967 -4.2275124][-4.2626619 -4.2726245 -4.2834988 -4.2749705 -4.234323 -4.1841359 -4.1492023 -4.1436129 -4.164897 -4.1920314 -4.2080531 -4.2194076 -4.2177744 -4.2123852 -4.2254739][-4.2551503 -4.2616076 -4.265461 -4.2509775 -4.20465 -4.14975 -4.1196952 -4.1218038 -4.1445022 -4.1681733 -4.181633 -4.1967425 -4.2069025 -4.2093372 -4.2234664][-4.2173281 -4.2157345 -4.2139153 -4.2001743 -4.1589065 -4.1141052 -4.0974941 -4.1075096 -4.1327486 -4.1549253 -4.1700358 -4.1868949 -4.2009897 -4.2070689 -4.22195][-4.1686397 -4.1625924 -4.1631646 -4.1523023 -4.12273 -4.09525 -4.091671 -4.109839 -4.137166 -4.15996 -4.1754622 -4.190331 -4.2010207 -4.2075706 -4.2226024][-4.1158905 -4.1100497 -4.1108518 -4.1069107 -4.09299 -4.0872378 -4.0980573 -4.120728 -4.147624 -4.1703568 -4.1862893 -4.1975713 -4.2033439 -4.2094412 -4.2246914][-4.08576 -4.0747747 -4.0734286 -4.0718746 -4.0725646 -4.0857038 -4.109602 -4.1372213 -4.1647506 -4.1850986 -4.2002282 -4.2073708 -4.2122025 -4.2210083 -4.2370534][-4.1018562 -4.0957522 -4.1013927 -4.1008682 -4.1059403 -4.1275854 -4.1582675 -4.1847014 -4.2064543 -4.2204566 -4.2289209 -4.2317939 -4.2398653 -4.2513666 -4.2646933][-4.151618 -4.1512437 -4.1621971 -4.1626272 -4.1689472 -4.1907482 -4.2193437 -4.2392278 -4.2495852 -4.25522 -4.2577882 -4.2595687 -4.2712049 -4.2833891 -4.2928843][-4.2253079 -4.2299409 -4.2403946 -4.2386522 -4.2412982 -4.2520528 -4.2693505 -4.2785525 -4.2768278 -4.2747517 -4.2740493 -4.2780261 -4.292419 -4.3049531 -4.3127971][-4.2863827 -4.2957535 -4.3015575 -4.2927318 -4.2876406 -4.2893257 -4.2956958 -4.2945666 -4.2837682 -4.2761593 -4.2763872 -4.2846103 -4.30109 -4.3153305 -4.3250184]]...]
INFO - root - 2017-12-07 21:39:18.817872: step 56710, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 50h:16m:44s remains)
INFO - root - 2017-12-07 21:39:25.538094: step 56720, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 53h:32m:33s remains)
INFO - root - 2017-12-07 21:39:32.302315: step 56730, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 54h:05m:13s remains)
INFO - root - 2017-12-07 21:39:39.041497: step 56740, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 51h:19m:16s remains)
INFO - root - 2017-12-07 21:39:45.792207: step 56750, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 49h:05m:28s remains)
INFO - root - 2017-12-07 21:39:52.515118: step 56760, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 50h:58m:31s remains)
INFO - root - 2017-12-07 21:39:59.330764: step 56770, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 52h:22m:05s remains)
INFO - root - 2017-12-07 21:40:06.141747: step 56780, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 50h:57m:58s remains)
INFO - root - 2017-12-07 21:40:12.833560: step 56790, loss = 2.04, batch loss = 1.98 (13.0 examples/sec; 0.614 sec/batch; 47h:02m:42s remains)
INFO - root - 2017-12-07 21:40:19.330532: step 56800, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 48h:03m:07s remains)
2017-12-07 21:40:20.083149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2559228 -4.2558117 -4.2527709 -4.2401118 -4.22107 -4.2083325 -4.2102623 -4.219728 -4.2342162 -4.2418327 -4.2361927 -4.2223907 -4.2116132 -4.2143369 -4.2260146][-4.2398558 -4.2404928 -4.234396 -4.2110777 -4.1766005 -4.1571021 -4.1669183 -4.1900125 -4.21248 -4.221107 -4.2147322 -4.1989026 -4.1879673 -4.1918483 -4.20639][-4.2299705 -4.2305393 -4.2196946 -4.1880097 -4.1399059 -4.115005 -4.1347103 -4.1711874 -4.1988726 -4.2063956 -4.1965556 -4.1779661 -4.165791 -4.1725478 -4.1882443][-4.2244868 -4.2219253 -4.2098684 -4.1722603 -4.1155138 -4.0843525 -4.1092782 -4.155992 -4.1867867 -4.1893811 -4.1770153 -4.1569753 -4.1428323 -4.1526246 -4.1667953][-4.2148242 -4.2080021 -4.1950927 -4.156652 -4.0983357 -4.0566092 -4.0731921 -4.1239061 -4.155355 -4.1530995 -4.1433792 -4.1265574 -4.11395 -4.1245737 -4.1390738][-4.1964917 -4.1859379 -4.1739259 -4.1342664 -4.0726976 -4.021081 -4.0278344 -4.0784554 -4.1106005 -4.1110306 -4.1060281 -4.0990872 -4.094214 -4.1047707 -4.1186609][-4.1720772 -4.1693974 -4.1665678 -4.1254025 -4.05422 -3.9937916 -3.990077 -4.0333371 -4.066546 -4.0721564 -4.0735173 -4.0760508 -4.0759816 -4.0815058 -4.0878363][-4.1693435 -4.1696248 -4.1738939 -4.1332946 -4.0603471 -3.9991572 -3.9835634 -4.0091314 -4.0331745 -4.0397491 -4.0438004 -4.0563412 -4.064055 -4.0647078 -4.066185][-4.177362 -4.1710744 -4.1755881 -4.1430407 -4.0879021 -4.0411105 -4.0191393 -4.0242267 -4.0344024 -4.0331841 -4.0362306 -4.0545812 -4.0716867 -4.0752096 -4.0781527][-4.18698 -4.1749735 -4.1771808 -4.1521673 -4.1221738 -4.1018567 -4.0862527 -4.0777984 -4.0714436 -4.0592074 -4.0556126 -4.0718832 -4.0947189 -4.1097684 -4.1245847][-4.1975365 -4.1838336 -4.1828337 -4.1614175 -4.1520219 -4.1532397 -4.1501389 -4.138525 -4.117434 -4.0965533 -4.0847726 -4.09737 -4.1241026 -4.1506481 -4.1749172][-4.2030392 -4.193181 -4.1920562 -4.1752024 -4.1741972 -4.1839905 -4.1933107 -4.1869593 -4.1611929 -4.1394835 -4.1271191 -4.1357312 -4.159471 -4.1892471 -4.21367][-4.2182212 -4.2153668 -4.2151909 -4.1990643 -4.1980214 -4.211123 -4.2283187 -4.2335587 -4.2137675 -4.1920218 -4.181951 -4.1881189 -4.2045016 -4.2290373 -4.2509718][-4.2331686 -4.2349405 -4.2360797 -4.22121 -4.2202654 -4.2347178 -4.2516704 -4.2615852 -4.2473707 -4.2292452 -4.22498 -4.2305508 -4.242732 -4.2625732 -4.2802587][-4.2529316 -4.2564344 -4.2585397 -4.2511368 -4.25309 -4.2612438 -4.2700038 -4.2770653 -4.2654915 -4.2509117 -4.2497487 -4.2546291 -4.2668591 -4.2853985 -4.2995687]]...]
INFO - root - 2017-12-07 21:40:26.859235: step 56810, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 52h:26m:18s remains)
INFO - root - 2017-12-07 21:40:33.561323: step 56820, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 50h:20m:14s remains)
INFO - root - 2017-12-07 21:40:40.231105: step 56830, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 48h:20m:53s remains)
INFO - root - 2017-12-07 21:40:46.998847: step 56840, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 49h:37m:22s remains)
INFO - root - 2017-12-07 21:40:53.764154: step 56850, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 52h:27m:25s remains)
INFO - root - 2017-12-07 21:41:00.490306: step 56860, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 55h:25m:21s remains)
INFO - root - 2017-12-07 21:41:07.314710: step 56870, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 52h:55m:56s remains)
INFO - root - 2017-12-07 21:41:14.080089: step 56880, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 50h:19m:01s remains)
INFO - root - 2017-12-07 21:41:20.803118: step 56890, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 48h:41m:49s remains)
INFO - root - 2017-12-07 21:41:27.517139: step 56900, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.742 sec/batch; 56h:46m:23s remains)
2017-12-07 21:41:28.269996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3295269 -4.3125143 -4.2884407 -4.283258 -4.295269 -4.3033743 -4.2986565 -4.2789407 -4.2552915 -4.2449255 -4.2553391 -4.27758 -4.3063049 -4.3277345 -4.3377552][-4.3292723 -4.3155618 -4.2959952 -4.2910819 -4.2984867 -4.2936125 -4.2724023 -4.2387719 -4.2077651 -4.1963286 -4.2134233 -4.2456174 -4.2837758 -4.3123131 -4.3274088][-4.326684 -4.3185272 -4.3031707 -4.295104 -4.2916837 -4.2696962 -4.2306232 -4.1835527 -4.1479168 -4.1399341 -4.1680589 -4.2095718 -4.2562146 -4.2921085 -4.3138943][-4.307795 -4.3095641 -4.2991352 -4.2882781 -4.2750072 -4.2397628 -4.1847577 -4.1238756 -4.0838923 -4.0835786 -4.1246233 -4.1761479 -4.2316408 -4.2756119 -4.3037634][-4.2675133 -4.279088 -4.2748027 -4.2632694 -4.2444205 -4.2036266 -4.1413794 -4.0696535 -4.0220962 -4.0294785 -4.0825377 -4.144917 -4.2106295 -4.2627769 -4.295155][-4.2096415 -4.2303643 -4.2347474 -4.2253084 -4.203495 -4.1582837 -4.0893836 -4.009809 -3.95864 -3.9792275 -4.0482759 -4.1236935 -4.1989017 -4.2569737 -4.2905431][-4.1512017 -4.1819248 -4.1978717 -4.1926818 -4.1666021 -4.1129961 -4.0322075 -3.9384315 -3.8882377 -3.9323137 -4.0271597 -4.1213155 -4.2033982 -4.2634883 -4.2948661][-4.1152487 -4.1582813 -4.1846423 -4.1842408 -4.1544933 -4.0918832 -3.9964275 -3.8910496 -3.8491874 -3.9156644 -4.0299606 -4.1380582 -4.2251444 -4.28434 -4.3102331][-4.1263022 -4.1675658 -4.1921134 -4.1912584 -4.1625247 -4.1031709 -4.0152197 -3.925436 -3.8967509 -3.9603038 -4.0637827 -4.1660142 -4.2500887 -4.303915 -4.3236046][-4.1737394 -4.1995168 -4.2123933 -4.2081776 -4.1857195 -4.1422009 -4.0808039 -4.0173264 -3.9921982 -4.0287676 -4.1011267 -4.1850595 -4.2605915 -4.3092961 -4.3274565][-4.2057118 -4.2136588 -4.21473 -4.2086282 -4.1944957 -4.1668034 -4.1278067 -4.0815411 -4.0497036 -4.0584497 -4.105701 -4.1732259 -4.2425556 -4.2930551 -4.3174925][-4.2001028 -4.1937814 -4.1892505 -4.184041 -4.1789541 -4.1661882 -4.1440749 -4.1073241 -4.0705433 -4.0655131 -4.0990467 -4.1575027 -4.2209859 -4.2710752 -4.3017659][-4.1836228 -4.1704617 -4.1640253 -4.1615667 -4.1661758 -4.1672783 -4.1551433 -4.1230178 -4.08393 -4.0739765 -4.1024585 -4.1549549 -4.2126255 -4.2621403 -4.2958231][-4.1864638 -4.1738825 -4.1682854 -4.1684 -4.1757541 -4.1840043 -4.178782 -4.1509185 -4.113863 -4.1071529 -4.1364374 -4.1848555 -4.2361774 -4.2802072 -4.3088608][-4.2194791 -4.2110929 -4.2076788 -4.2084146 -4.2147355 -4.2232785 -4.2222471 -4.2037168 -4.1764622 -4.1745014 -4.2018533 -4.241457 -4.2808752 -4.3125172 -4.3306541]]...]
INFO - root - 2017-12-07 21:41:34.980139: step 56910, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 48h:49m:42s remains)
INFO - root - 2017-12-07 21:41:41.715834: step 56920, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 49h:04m:14s remains)
INFO - root - 2017-12-07 21:41:48.477737: step 56930, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.743 sec/batch; 56h:50m:33s remains)
INFO - root - 2017-12-07 21:41:55.236342: step 56940, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 55h:44m:14s remains)
INFO - root - 2017-12-07 21:42:02.007949: step 56950, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 51h:39m:20s remains)
INFO - root - 2017-12-07 21:42:08.955632: step 56960, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 50h:20m:03s remains)
INFO - root - 2017-12-07 21:42:15.667025: step 56970, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 49h:26m:10s remains)
INFO - root - 2017-12-07 21:42:22.574763: step 56980, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.717 sec/batch; 54h:53m:19s remains)
INFO - root - 2017-12-07 21:42:29.344223: step 56990, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 53h:02m:10s remains)
INFO - root - 2017-12-07 21:42:35.789393: step 57000, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 49h:40m:44s remains)
2017-12-07 21:42:36.509871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2606716 -4.2534275 -4.2378521 -4.2198405 -4.1947851 -4.1713319 -4.1693654 -4.1919904 -4.2196164 -4.2369046 -4.2423677 -4.2396741 -4.2357264 -4.2370543 -4.240943][-4.2741241 -4.2603197 -4.2371469 -4.2078676 -4.1730256 -4.1395793 -4.1301222 -4.159421 -4.2021585 -4.2354059 -4.2556591 -4.2657595 -4.26723 -4.2665915 -4.2690773][-4.2887964 -4.2729082 -4.2424979 -4.2015905 -4.1554642 -4.1089182 -4.0841293 -4.1112351 -4.169889 -4.2220907 -4.2579069 -4.2794342 -4.287899 -4.2882595 -4.2901874][-4.3052058 -4.2930841 -4.2605796 -4.2098875 -4.1507068 -4.0848689 -4.0350914 -4.051589 -4.1246238 -4.1951938 -4.2433348 -4.2727633 -4.2854362 -4.2885084 -4.2900491][-4.31239 -4.3071914 -4.2789145 -4.2241507 -4.1555972 -4.06985 -3.9912467 -3.9966006 -4.0872602 -4.1727386 -4.2252607 -4.2551 -4.2695065 -4.2755866 -4.280098][-4.3086524 -4.3078532 -4.2857566 -4.2350826 -4.1688304 -4.0806861 -3.9917846 -3.993247 -4.092453 -4.1776094 -4.2258444 -4.2525945 -4.2677431 -4.2738886 -4.28023][-4.2998786 -4.3014817 -4.285861 -4.2471271 -4.1961374 -4.1272345 -4.0488467 -4.0460558 -4.1285534 -4.1957493 -4.2345686 -4.2557406 -4.2691917 -4.2750645 -4.280158][-4.2892537 -4.2928786 -4.2823958 -4.2555776 -4.2196465 -4.1694775 -4.1066494 -4.1001229 -4.1616454 -4.2099648 -4.2393622 -4.2543974 -4.2622814 -4.2645187 -4.2666397][-4.2751565 -4.2797375 -4.2717409 -4.250792 -4.2215548 -4.1784043 -4.129046 -4.1289077 -4.177176 -4.2133579 -4.2351837 -4.2451921 -4.2466445 -4.2438335 -4.2433476][-4.2590408 -4.2628632 -4.2545552 -4.2328844 -4.2030892 -4.1612864 -4.1270943 -4.1426959 -4.1870236 -4.2148967 -4.2319055 -4.2405076 -4.2389421 -4.2342143 -4.2322588][-4.249475 -4.2512169 -4.2413106 -4.2143865 -4.1772647 -4.1364007 -4.1227503 -4.161315 -4.2099829 -4.2326417 -4.244091 -4.24953 -4.2454548 -4.2410398 -4.2391095][-4.2467132 -4.2443943 -4.228744 -4.1958704 -4.1502647 -4.1123877 -4.1198568 -4.1769528 -4.2278214 -4.2474809 -4.2542396 -4.2538137 -4.2480607 -4.244153 -4.2441359][-4.2471104 -4.2348919 -4.2085795 -4.1733017 -4.1301875 -4.1048889 -4.1292877 -4.1877441 -4.2319369 -4.247088 -4.2494168 -4.2452931 -4.2398272 -4.2369938 -4.2384253][-4.2544003 -4.2284474 -4.190156 -4.1540732 -4.1174126 -4.1061592 -4.1414289 -4.1943669 -4.2296419 -4.2393346 -4.2368608 -4.2307959 -4.2271538 -4.2270284 -4.230885][-4.2634306 -4.2237639 -4.1767745 -4.145143 -4.1210566 -4.1252279 -4.1639414 -4.2073121 -4.2332325 -4.2375717 -4.2305512 -4.2219167 -4.2210369 -4.22504 -4.2335443]]...]
INFO - root - 2017-12-07 21:42:43.270339: step 57010, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 55h:28m:23s remains)
INFO - root - 2017-12-07 21:42:50.044236: step 57020, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 49h:57m:23s remains)
INFO - root - 2017-12-07 21:42:56.881595: step 57030, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 48h:28m:11s remains)
INFO - root - 2017-12-07 21:43:03.554965: step 57040, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 48h:06m:32s remains)
INFO - root - 2017-12-07 21:43:10.392551: step 57050, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 54h:51m:52s remains)
INFO - root - 2017-12-07 21:43:17.309398: step 57060, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.741 sec/batch; 56h:40m:50s remains)
INFO - root - 2017-12-07 21:43:24.099812: step 57070, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 50h:40m:15s remains)
INFO - root - 2017-12-07 21:43:30.826871: step 57080, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 48h:47m:27s remains)
INFO - root - 2017-12-07 21:43:37.688575: step 57090, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 52h:09m:38s remains)
INFO - root - 2017-12-07 21:43:44.440877: step 57100, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 56h:30m:12s remains)
2017-12-07 21:43:45.224135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33632 -4.3380742 -4.3253255 -4.3031154 -4.2727323 -4.2434278 -4.21566 -4.1907649 -4.1846132 -4.2049437 -4.229485 -4.2450156 -4.2515707 -4.2550888 -4.25264][-4.3411441 -4.3431606 -4.3278985 -4.3022218 -4.2651043 -4.2263479 -4.1882949 -4.1537147 -4.1433005 -4.1688232 -4.201602 -4.2229786 -4.2351184 -4.2449231 -4.246665][-4.3431697 -4.3453755 -4.3290992 -4.2997837 -4.2579675 -4.2140331 -4.1702957 -4.1290865 -4.11617 -4.1451216 -4.1794066 -4.1990457 -4.2106528 -4.2223353 -4.2256069][-4.3427377 -4.345438 -4.3278427 -4.2945552 -4.2478123 -4.1966529 -4.1463065 -4.1017642 -4.0908809 -4.1270638 -4.1654325 -4.1828971 -4.1915741 -4.2018704 -4.2054753][-4.3411469 -4.3419228 -4.31974 -4.279839 -4.2248359 -4.1628919 -4.1057777 -4.0615215 -4.0614004 -4.1121359 -4.1608105 -4.1826487 -4.1893854 -4.1953983 -4.197679][-4.3402281 -4.3372908 -4.308845 -4.2591157 -4.192688 -4.1191516 -4.0606647 -4.0254164 -4.0430875 -4.11256 -4.1738429 -4.2004666 -4.2040205 -4.2050371 -4.2072611][-4.3393879 -4.3335257 -4.2999511 -4.2410159 -4.1665077 -4.0918179 -4.0461488 -4.0319386 -4.0664515 -4.1418991 -4.2021136 -4.2263975 -4.2249484 -4.2206969 -4.2227993][-4.3370042 -4.3295074 -4.2937121 -4.2318616 -4.1619134 -4.1034756 -4.0811777 -4.0847287 -4.1209741 -4.185956 -4.2363887 -4.2534728 -4.2463651 -4.2367887 -4.2352819][-4.3323541 -4.323175 -4.2874541 -4.2309995 -4.1742778 -4.1380606 -4.135622 -4.1467371 -4.1768537 -4.2249556 -4.2614431 -4.2732291 -4.2653661 -4.2545862 -4.2497034][-4.3267064 -4.3160963 -4.2820745 -4.235692 -4.1927056 -4.1731572 -4.1822524 -4.1955295 -4.2188368 -4.2492948 -4.2719355 -4.2792187 -4.2736239 -4.2693954 -4.2687559][-4.3220081 -4.3092632 -4.2793183 -4.2447543 -4.2142744 -4.206079 -4.2210522 -4.23587 -4.250998 -4.2652206 -4.2733607 -4.2754283 -4.274879 -4.2781262 -4.2823339][-4.3173647 -4.3025551 -4.2770977 -4.2536497 -4.2354374 -4.2359934 -4.25432 -4.2674589 -4.2742238 -4.2767015 -4.2741361 -4.2705116 -4.2723608 -4.2783732 -4.283658][-4.3142767 -4.2998915 -4.279563 -4.2647834 -4.2568464 -4.2636156 -4.2804136 -4.289073 -4.2881937 -4.2827325 -4.2737031 -4.2670665 -4.2690911 -4.2747183 -4.2779474][-4.3137655 -4.3025403 -4.2879844 -4.2788749 -4.27718 -4.2840405 -4.2948742 -4.2976847 -4.2918119 -4.2829728 -4.2712379 -4.2621322 -4.2624207 -4.2661428 -4.2677293][-4.3151665 -4.3073897 -4.2986875 -4.2939157 -4.2933025 -4.2953758 -4.299315 -4.2981129 -4.2905736 -4.2803941 -4.267653 -4.2574143 -4.2546506 -4.2534833 -4.2515588]]...]
INFO - root - 2017-12-07 21:43:51.917081: step 57110, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 49h:20m:48s remains)
INFO - root - 2017-12-07 21:43:58.703674: step 57120, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 49h:42m:28s remains)
INFO - root - 2017-12-07 21:44:05.551405: step 57130, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 54h:47m:34s remains)
INFO - root - 2017-12-07 21:44:12.397038: step 57140, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 56h:13m:49s remains)
INFO - root - 2017-12-07 21:44:19.188939: step 57150, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 50h:43m:06s remains)
INFO - root - 2017-12-07 21:44:25.979218: step 57160, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 48h:07m:22s remains)
INFO - root - 2017-12-07 21:44:32.725017: step 57170, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 50h:51m:27s remains)
INFO - root - 2017-12-07 21:44:39.517683: step 57180, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 53h:20m:38s remains)
INFO - root - 2017-12-07 21:44:46.309809: step 57190, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 54h:10m:38s remains)
INFO - root - 2017-12-07 21:44:52.997496: step 57200, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 50h:19m:49s remains)
2017-12-07 21:44:53.891537: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3373876 -4.338706 -4.3397121 -4.3391104 -4.3377523 -4.3386407 -4.3421159 -4.3470654 -4.3514109 -4.3557467 -4.35958 -4.3650217 -4.3705158 -4.374867 -4.3785644][-4.3106489 -4.3108816 -4.3146005 -4.3148508 -4.3120284 -4.3122182 -4.3186541 -4.3276877 -4.3340783 -4.3376684 -4.3406777 -4.3489013 -4.3613248 -4.3709087 -4.3783197][-4.2776027 -4.2758884 -4.2831459 -4.2832313 -4.2776504 -4.2790723 -4.2877893 -4.2991471 -4.3069267 -4.3092885 -4.3124447 -4.3259058 -4.3463931 -4.3603716 -4.3710022][-4.2464967 -4.248651 -4.2586684 -4.2538691 -4.2416945 -4.2434077 -4.250349 -4.2586451 -4.264883 -4.2670307 -4.2757363 -4.2981992 -4.3259559 -4.3428721 -4.3586087][-4.2082896 -4.2151423 -4.2236996 -4.2139149 -4.1972551 -4.1990404 -4.2035789 -4.2076836 -4.2088518 -4.2094908 -4.2264986 -4.261517 -4.2981896 -4.321517 -4.3450718][-4.1571994 -4.1667051 -4.175139 -4.1649003 -4.1456571 -4.1477108 -4.1488266 -4.1490269 -4.15041 -4.1547117 -4.179893 -4.2227669 -4.2674661 -4.3004432 -4.3327641][-4.1126108 -4.1175003 -4.1222191 -4.1105137 -4.091435 -4.0913796 -4.0918818 -4.0907297 -4.0994363 -4.1136093 -4.1462836 -4.1931925 -4.2440982 -4.2853985 -4.3250995][-4.094327 -4.0966616 -4.1023545 -4.0907087 -4.0689778 -4.0588245 -4.0564542 -4.056263 -4.0697441 -4.0899072 -4.1266737 -4.177485 -4.2324791 -4.2799077 -4.3219795][-4.1170359 -4.1193709 -4.1274252 -4.1172266 -4.0954709 -4.0743737 -4.0629992 -4.0521722 -4.0549622 -4.0714869 -4.1078429 -4.1643085 -4.2245765 -4.2771835 -4.3186793][-4.1701946 -4.1720314 -4.1771846 -4.1692953 -4.1522169 -4.1271152 -4.106143 -4.08266 -4.0677309 -4.0734215 -4.1045952 -4.1642523 -4.2269607 -4.2834792 -4.3205948][-4.2226052 -4.2221937 -4.2248497 -4.2216616 -4.2111053 -4.1894665 -4.1674886 -4.1393361 -4.1146526 -4.1080947 -4.13295 -4.1902609 -4.248075 -4.2982512 -4.3276329][-4.2378864 -4.2295823 -4.228301 -4.2322969 -4.2331696 -4.2244072 -4.2129588 -4.1924129 -4.1722274 -4.1624174 -4.1821332 -4.2299414 -4.2760334 -4.3117805 -4.3340826][-4.2367678 -4.2078762 -4.19432 -4.2022853 -4.2168264 -4.2286458 -4.2356329 -4.2335892 -4.2290578 -4.2217894 -4.232522 -4.2646456 -4.2951941 -4.3155317 -4.3332396][-4.2422781 -4.1917543 -4.1597433 -4.1596394 -4.1827841 -4.2150578 -4.2417703 -4.257863 -4.2653484 -4.2574639 -4.2594681 -4.278131 -4.2974892 -4.3088441 -4.3256478][-4.2638345 -4.2030439 -4.1536036 -4.1358695 -4.1527057 -4.1908388 -4.2309904 -4.2622113 -4.2764969 -4.26804 -4.2643652 -4.2753086 -4.2887297 -4.2960649 -4.3139377]]...]
INFO - root - 2017-12-07 21:45:00.600290: step 57210, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 52h:21m:52s remains)
INFO - root - 2017-12-07 21:45:07.392748: step 57220, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 52h:52m:16s remains)
INFO - root - 2017-12-07 21:45:14.090122: step 57230, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 48h:07m:44s remains)
INFO - root - 2017-12-07 21:45:20.799767: step 57240, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 49h:46m:53s remains)
INFO - root - 2017-12-07 21:45:27.659877: step 57250, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.744 sec/batch; 56h:51m:26s remains)
INFO - root - 2017-12-07 21:45:34.498754: step 57260, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 55h:41m:19s remains)
INFO - root - 2017-12-07 21:45:41.296917: step 57270, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 51h:16m:20s remains)
INFO - root - 2017-12-07 21:45:47.970356: step 57280, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 47h:23m:59s remains)
INFO - root - 2017-12-07 21:45:54.712685: step 57290, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 51h:32m:01s remains)
INFO - root - 2017-12-07 21:46:01.426384: step 57300, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 52h:54m:22s remains)
2017-12-07 21:46:02.234101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1302385 -4.1276951 -4.1275654 -4.1343393 -4.1638317 -4.1859941 -4.1874375 -4.1639433 -4.1466069 -4.1483212 -4.1608477 -4.18951 -4.226562 -4.249187 -4.25682][-4.1664963 -4.1705418 -4.1699963 -4.1717834 -4.1870694 -4.1927381 -4.178793 -4.1480551 -4.1254287 -4.1215563 -4.1321921 -4.166141 -4.215117 -4.244493 -4.2537684][-4.176333 -4.1799493 -4.18047 -4.1852784 -4.1953683 -4.1939211 -4.1729484 -4.1345882 -4.0997043 -4.0906887 -4.1036186 -4.145844 -4.2037687 -4.2381959 -4.2507243][-4.1818471 -4.1749053 -4.1756248 -4.1842623 -4.1889634 -4.1805277 -4.1529012 -4.1060758 -4.0651159 -4.0594459 -4.0800529 -4.1324677 -4.1943555 -4.2311172 -4.2460227][-4.190311 -4.1749516 -4.1747093 -4.1793895 -4.1779685 -4.1621637 -4.1216645 -4.0606709 -4.0225635 -4.03517 -4.0716481 -4.1310167 -4.1888123 -4.2251763 -4.2433462][-4.2145281 -4.1955566 -4.187355 -4.1794157 -4.1617355 -4.1320004 -4.0761189 -4.0042858 -3.9677615 -4.0025883 -4.0639186 -4.1307645 -4.1856565 -4.2182555 -4.2386546][-4.241303 -4.224906 -4.2128682 -4.1949544 -4.152957 -4.09267 -4.0076461 -3.9098759 -3.8714318 -3.931124 -4.0255656 -4.1084275 -4.1695666 -4.206636 -4.2292037][-4.2739921 -4.2644587 -4.248641 -4.218287 -4.1588659 -4.0764394 -3.9620764 -3.8266952 -3.7656667 -3.8291926 -3.9517446 -4.0553212 -4.1299725 -4.1801715 -4.2117252][-4.3084331 -4.3021564 -4.282732 -4.246417 -4.1820955 -4.1011896 -3.9978456 -3.8695028 -3.7977831 -3.8361363 -3.9440961 -4.0389528 -4.1059494 -4.1575375 -4.1968613][-4.3164849 -4.3160706 -4.3042493 -4.275805 -4.2234159 -4.1622396 -4.0919642 -3.9963894 -3.9322391 -3.9455898 -4.0166726 -4.08434 -4.1312842 -4.1704359 -4.2048025][-4.3013039 -4.3048267 -4.3081641 -4.2995772 -4.27016 -4.2335162 -4.1875949 -4.1172123 -4.0604229 -4.0596752 -4.1052647 -4.1510983 -4.1809444 -4.2068744 -4.2316213][-4.2838449 -4.2860613 -4.2961588 -4.3029442 -4.2945604 -4.2769718 -4.2506227 -4.20119 -4.1553869 -4.1531591 -4.1842856 -4.2135415 -4.2307606 -4.2465396 -4.262435][-4.2779064 -4.2778506 -4.2883434 -4.29856 -4.2987928 -4.2908468 -4.2777386 -4.2481546 -4.21734 -4.2188196 -4.2415633 -4.2606115 -4.2719188 -4.281858 -4.2901282][-4.2753034 -4.27581 -4.2843685 -4.292491 -4.2939038 -4.2893567 -4.2826486 -4.2668524 -4.2480664 -4.2512631 -4.2697353 -4.286016 -4.2973948 -4.305479 -4.31072][-4.27542 -4.2781181 -4.283421 -4.2874522 -4.2887564 -4.2857208 -4.2811375 -4.270865 -4.2580042 -4.2598524 -4.2734962 -4.287219 -4.3001986 -4.3119497 -4.3202829]]...]
INFO - root - 2017-12-07 21:46:08.893804: step 57310, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 49h:50m:22s remains)
INFO - root - 2017-12-07 21:46:15.716159: step 57320, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 55h:32m:31s remains)
INFO - root - 2017-12-07 21:46:22.541725: step 57330, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 54h:49m:50s remains)
INFO - root - 2017-12-07 21:46:29.355991: step 57340, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 49h:53m:25s remains)
INFO - root - 2017-12-07 21:46:36.168024: step 57350, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.641 sec/batch; 48h:57m:31s remains)
INFO - root - 2017-12-07 21:46:43.070280: step 57360, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 53h:07m:38s remains)
INFO - root - 2017-12-07 21:46:49.870668: step 57370, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.764 sec/batch; 58h:23m:03s remains)
INFO - root - 2017-12-07 21:46:56.697821: step 57380, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 55h:36m:51s remains)
INFO - root - 2017-12-07 21:47:03.434143: step 57390, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 50h:05m:02s remains)
INFO - root - 2017-12-07 21:47:09.854976: step 57400, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 48h:43m:23s remains)
2017-12-07 21:47:10.598053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32627 -4.3109684 -4.2905159 -4.2497187 -4.1719408 -4.095437 -4.0777388 -4.1044207 -4.13098 -4.1420732 -4.1521792 -4.1679525 -4.1953092 -4.225915 -4.23979][-4.32342 -4.3048935 -4.2804861 -4.2341332 -4.1493483 -4.06747 -4.0515924 -4.0853739 -4.1182919 -4.128356 -4.1333575 -4.1502709 -4.1841903 -4.2248154 -4.239419][-4.3222852 -4.3019528 -4.2755179 -4.2236824 -4.1321173 -4.0466218 -4.03433 -4.0810094 -4.1283 -4.1421733 -4.1407719 -4.1530471 -4.1875887 -4.228147 -4.2387824][-4.3228869 -4.3027897 -4.275918 -4.220993 -4.1235085 -4.0305276 -4.0192089 -4.083859 -4.1518731 -4.1733422 -4.16587 -4.1708822 -4.2008004 -4.234231 -4.2391372][-4.3241305 -4.3045845 -4.2775979 -4.2212591 -4.1181364 -4.0124373 -3.9965105 -4.0809751 -4.1730366 -4.205863 -4.1944337 -4.1931643 -4.2185287 -4.2482119 -4.251442][-4.3252144 -4.3059382 -4.2785864 -4.2205148 -4.1077609 -3.98185 -3.9507251 -4.0496688 -4.1630445 -4.2088141 -4.2015553 -4.2000804 -4.2243071 -4.2512078 -4.2540035][-4.3263307 -4.3075466 -4.2810545 -4.2223835 -4.1003609 -3.9526408 -3.8968296 -4.0001297 -4.1257715 -4.1843662 -4.1925349 -4.1992769 -4.2227297 -4.2451444 -4.2444153][-4.3270521 -4.3093534 -4.2854786 -4.2298975 -4.1096158 -3.9621344 -3.89825 -3.9890568 -4.1038966 -4.1618705 -4.1816459 -4.19581 -4.2190714 -4.234405 -4.2287617][-4.3273511 -4.30994 -4.2890592 -4.2396083 -4.1333647 -4.0125275 -3.9707048 -4.0397944 -4.12317 -4.1629024 -4.1795278 -4.1963215 -4.219727 -4.2320795 -4.2232828][-4.32708 -4.3091807 -4.2902055 -4.2469168 -4.1566458 -4.0649481 -4.0468125 -4.1039314 -4.1603446 -4.1808095 -4.1869044 -4.2032275 -4.2273879 -4.2402072 -4.2300458][-4.3269014 -4.3076735 -4.2883077 -4.248 -4.169992 -4.1010509 -4.0993576 -4.1468258 -4.1851554 -4.1960077 -4.1962576 -4.2105637 -4.2305679 -4.2436924 -4.2348776][-4.3275976 -4.3065639 -4.2847137 -4.2444677 -4.17466 -4.1216183 -4.1285715 -4.1682596 -4.1953511 -4.1994214 -4.1954312 -4.2055578 -4.2233381 -4.2375436 -4.231647][-4.3285294 -4.3078566 -4.2855773 -4.2443728 -4.1782742 -4.1331248 -4.1416259 -4.1731453 -4.1900873 -4.1893921 -4.1846857 -4.1917815 -4.2074542 -4.2222552 -4.2173281][-4.3306327 -4.3120308 -4.2930145 -4.25279 -4.1897044 -4.1465478 -4.1490116 -4.1703625 -4.1790714 -4.1757126 -4.1714859 -4.1773639 -4.1936545 -4.207221 -4.2025366][-4.3317642 -4.3161283 -4.3017049 -4.2664485 -4.2099552 -4.1683145 -4.1606979 -4.1703458 -4.17347 -4.1649575 -4.1619968 -4.1683135 -4.187005 -4.1986518 -4.1919427]]...]
INFO - root - 2017-12-07 21:47:17.462490: step 57410, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 51h:29m:14s remains)
INFO - root - 2017-12-07 21:47:24.180566: step 57420, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 52h:47m:25s remains)
INFO - root - 2017-12-07 21:47:30.862103: step 57430, loss = 2.03, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 49h:10m:13s remains)
INFO - root - 2017-12-07 21:47:37.715689: step 57440, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.719 sec/batch; 54h:55m:58s remains)
INFO - root - 2017-12-07 21:47:44.457069: step 57450, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 53h:03m:25s remains)
INFO - root - 2017-12-07 21:47:51.228333: step 57460, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 51h:07m:43s remains)
INFO - root - 2017-12-07 21:47:58.105160: step 57470, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 48h:24m:44s remains)
INFO - root - 2017-12-07 21:48:04.923977: step 57480, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 50h:12m:00s remains)
INFO - root - 2017-12-07 21:48:11.743298: step 57490, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 55h:11m:00s remains)
INFO - root - 2017-12-07 21:48:18.435878: step 57500, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 54h:25m:22s remains)
2017-12-07 21:48:19.136528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2062798 -4.2035551 -4.1965966 -4.1919174 -4.19484 -4.2031646 -4.2122431 -4.2176867 -4.2168026 -4.2098145 -4.2021666 -4.1984591 -4.1948133 -4.1805038 -4.1427517][-4.24868 -4.2444229 -4.2332263 -4.2220969 -4.2184868 -4.2240658 -4.2355127 -4.2480154 -4.2561712 -4.2570081 -4.2539034 -4.2507463 -4.2457013 -4.2334929 -4.2032342][-4.2885523 -4.2817445 -4.2645278 -4.2447329 -4.2318187 -4.2313948 -4.2440257 -4.2649217 -4.2843928 -4.2956991 -4.2986312 -4.2965374 -4.2894588 -4.2766657 -4.2514329][-4.3172684 -4.3075218 -4.283628 -4.2533112 -4.2264986 -4.2142358 -4.2233472 -4.2497039 -4.2817268 -4.307601 -4.3217812 -4.3252683 -4.3193274 -4.3058505 -4.2820115][-4.3282323 -4.3173027 -4.2886968 -4.2470279 -4.200561 -4.1666369 -4.1598086 -4.1815319 -4.223578 -4.2702408 -4.3062983 -4.3266 -4.3306651 -4.3195143 -4.2949896][-4.3282166 -4.31617 -4.2841458 -4.2308893 -4.1616282 -4.0982304 -4.0590358 -4.0575933 -4.103168 -4.1772962 -4.2455997 -4.2941976 -4.3188167 -4.3189163 -4.2997141][-4.3200445 -4.3078756 -4.2759738 -4.2178802 -4.1338181 -4.0402365 -3.9546809 -3.9066193 -3.9399734 -4.0385475 -4.140718 -4.2233772 -4.278285 -4.3000789 -4.2955718][-4.3066545 -4.2973127 -4.271944 -4.2218032 -4.1412921 -4.038939 -3.9239669 -3.8325803 -3.8331733 -3.9258509 -4.0352273 -4.1357303 -4.216754 -4.2647734 -4.2826414][-4.2890453 -4.2853508 -4.2721753 -4.2393494 -4.1796536 -4.0958376 -3.9911644 -3.8973856 -3.8692 -3.9206023 -3.9984858 -4.082891 -4.165102 -4.2262411 -4.2619324][-4.27246 -4.276154 -4.2761955 -4.2616653 -4.2257681 -4.1691365 -4.0942211 -4.0219011 -3.9855452 -4.0034804 -4.0446458 -4.09547 -4.1546316 -4.2081933 -4.2483935][-4.26287 -4.2708297 -4.2791543 -4.2776856 -4.2601275 -4.2275043 -4.1812367 -4.1332111 -4.1022906 -4.103096 -4.1203351 -4.1448154 -4.1794028 -4.2162242 -4.2486229][-4.2622075 -4.2708149 -4.2834921 -4.2918291 -4.2894826 -4.2767878 -4.2521625 -4.2228389 -4.1991491 -4.1905794 -4.1928349 -4.200357 -4.2173343 -4.2388206 -4.2601142][-4.2717743 -4.2781944 -4.2906594 -4.3028226 -4.3092036 -4.3096757 -4.3012481 -4.2859135 -4.2688503 -4.2560539 -4.2498865 -4.2482095 -4.2536855 -4.2640357 -4.2759061][-4.2824392 -4.2857919 -4.2951064 -4.3059454 -4.3156686 -4.3229909 -4.3245783 -4.3187633 -4.3077655 -4.2952518 -4.285706 -4.2794728 -4.2778482 -4.28073 -4.286396][-4.2908988 -4.2902212 -4.2948751 -4.3011785 -4.3089223 -4.317205 -4.32275 -4.322093 -4.315824 -4.3066726 -4.2979345 -4.29048 -4.2845159 -4.2815418 -4.2819548]]...]
INFO - root - 2017-12-07 21:48:25.860236: step 57510, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 51h:04m:16s remains)
INFO - root - 2017-12-07 21:48:32.597307: step 57520, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 54h:46m:22s remains)
INFO - root - 2017-12-07 21:48:39.327524: step 57530, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 55h:03m:54s remains)
INFO - root - 2017-12-07 21:48:46.040726: step 57540, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 54h:26m:19s remains)
INFO - root - 2017-12-07 21:48:52.718788: step 57550, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.669 sec/batch; 51h:07m:56s remains)
INFO - root - 2017-12-07 21:48:59.450378: step 57560, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 49h:14m:54s remains)
INFO - root - 2017-12-07 21:49:06.276427: step 57570, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 51h:48m:58s remains)
INFO - root - 2017-12-07 21:49:13.118434: step 57580, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 54h:46m:48s remains)
INFO - root - 2017-12-07 21:49:19.953773: step 57590, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 53h:27m:25s remains)
INFO - root - 2017-12-07 21:49:26.630584: step 57600, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 49h:52m:07s remains)
2017-12-07 21:49:27.342263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1954226 -4.1965075 -4.1959143 -4.1815977 -4.1651821 -4.1633911 -4.1577272 -4.1451707 -4.1499739 -4.172842 -4.20892 -4.236424 -4.242167 -4.2273364 -4.201251][-4.2178187 -4.2216434 -4.2158561 -4.1934352 -4.1702104 -4.1622496 -4.1542735 -4.1407094 -4.1481047 -4.1768851 -4.2160363 -4.2422719 -4.2439318 -4.2245789 -4.1922007][-4.2474251 -4.2506795 -4.2369361 -4.2039809 -4.1713247 -4.1530461 -4.1393867 -4.1270976 -4.1419034 -4.1804085 -4.2227621 -4.248064 -4.2492204 -4.2249589 -4.1853609][-4.2665467 -4.2657733 -4.2465162 -4.2078934 -4.167285 -4.1371322 -4.1146364 -4.1021037 -4.1233592 -4.1721373 -4.2172785 -4.2443094 -4.2464132 -4.2182174 -4.1717119][-4.2736878 -4.2704434 -4.2490053 -4.2077074 -4.1594715 -4.1180987 -4.086072 -4.07165 -4.0983744 -4.1561284 -4.2022905 -4.2301378 -4.2344527 -4.2072811 -4.1590233][-4.275403 -4.2705784 -4.2467008 -4.203177 -4.1533723 -4.1048365 -4.0666733 -4.049201 -4.079648 -4.1444883 -4.1927047 -4.2186551 -4.224112 -4.1987762 -4.1534486][-4.2675529 -4.2597589 -4.233613 -4.1912894 -4.1472692 -4.0967016 -4.0545878 -4.0345669 -4.072679 -4.145925 -4.19704 -4.2207212 -4.2253809 -4.2017703 -4.1617479][-4.2563338 -4.2440963 -4.2147231 -4.1760225 -4.1378226 -4.0852709 -4.03824 -4.0170059 -4.0657272 -4.14896 -4.2034035 -4.2284007 -4.2329555 -4.2123818 -4.1787734][-4.2472124 -4.2302737 -4.1977458 -4.1630163 -4.1311226 -4.0769053 -4.0212526 -3.9937489 -4.0503554 -4.1414957 -4.198781 -4.2289424 -4.238853 -4.2252545 -4.1980443][-4.2371168 -4.2168794 -4.1826253 -4.1518149 -4.1278133 -4.075232 -4.0091619 -3.9762032 -4.0384178 -4.1303234 -4.1882386 -4.2239776 -4.2424197 -4.2332969 -4.2083988][-4.2295437 -4.2077861 -4.1723251 -4.14404 -4.1249518 -4.0768991 -4.0098996 -3.9825313 -4.0475249 -4.1287308 -4.177916 -4.2140875 -4.2379427 -4.2313776 -4.2060838][-4.2257471 -4.2058663 -4.1740851 -4.147 -4.1282258 -4.0872397 -4.0321031 -4.0198622 -4.0781031 -4.1380429 -4.1723828 -4.2035728 -4.2280726 -4.2247057 -4.1993346][-4.2256136 -4.21209 -4.1889453 -4.1649804 -4.1442833 -4.10864 -4.0669794 -4.0662928 -4.1104937 -4.1467094 -4.1646237 -4.191144 -4.2168446 -4.218586 -4.197506][-4.2193828 -4.2145896 -4.202126 -4.1842279 -4.1624765 -4.1316895 -4.1027145 -4.105001 -4.127841 -4.1400671 -4.1444883 -4.1696372 -4.2006836 -4.209291 -4.19403][-4.2025018 -4.2040071 -4.1996117 -4.187036 -4.1660037 -4.143023 -4.1280141 -4.1285925 -4.1308026 -4.1230736 -4.1184211 -4.1457667 -4.1822724 -4.1978602 -4.188046]]...]
INFO - root - 2017-12-07 21:49:34.113270: step 57610, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 53h:36m:43s remains)
INFO - root - 2017-12-07 21:49:40.750558: step 57620, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 47h:30m:22s remains)
INFO - root - 2017-12-07 21:49:47.582566: step 57630, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 49h:23m:55s remains)
INFO - root - 2017-12-07 21:49:54.362332: step 57640, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 55h:44m:19s remains)
INFO - root - 2017-12-07 21:50:01.118526: step 57650, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 54h:19m:50s remains)
INFO - root - 2017-12-07 21:50:07.816508: step 57660, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 49h:02m:16s remains)
INFO - root - 2017-12-07 21:50:14.579329: step 57670, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 47h:15m:49s remains)
INFO - root - 2017-12-07 21:50:21.415237: step 57680, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 49h:56m:08s remains)
INFO - root - 2017-12-07 21:50:28.197876: step 57690, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.758 sec/batch; 57h:49m:40s remains)
INFO - root - 2017-12-07 21:50:34.705254: step 57700, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 52h:50m:52s remains)
2017-12-07 21:50:35.339426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2724757 -4.2801452 -4.2841029 -4.2824364 -4.2799211 -4.2702303 -4.2513456 -4.218688 -4.1813736 -4.1581016 -4.1572509 -4.171196 -4.1869774 -4.1932321 -4.1835928][-4.2964377 -4.30243 -4.302856 -4.2976422 -4.2900844 -4.2696724 -4.235858 -4.1849651 -4.1313076 -4.0998611 -4.1051021 -4.1321321 -4.1603117 -4.170939 -4.1547146][-4.3021741 -4.3053408 -4.30051 -4.2910652 -4.2777977 -4.2452254 -4.1954851 -4.1271238 -4.0586076 -4.0224648 -4.0378451 -4.0827484 -4.12782 -4.1393104 -4.1128168][-4.3000526 -4.2984591 -4.2880764 -4.27737 -4.2622008 -4.2227788 -4.1631136 -4.0858445 -4.01453 -3.9865718 -4.0193086 -4.07593 -4.1247578 -4.126286 -4.0874252][-4.2911606 -4.28079 -4.2667236 -4.2597728 -4.2490807 -4.2132106 -4.1523447 -4.0723715 -4.0099797 -4.001626 -4.0468626 -4.1051588 -4.1463594 -4.1332526 -4.0837407][-4.2695346 -4.2537913 -4.2407112 -4.2420683 -4.2406821 -4.2114191 -4.1534042 -4.0776176 -4.0268812 -4.0325346 -4.0785232 -4.1316752 -4.1610289 -4.1364431 -4.0822086][-4.2482343 -4.2290411 -4.2203717 -4.2306151 -4.2358747 -4.212122 -4.1572719 -4.0825481 -4.0382867 -4.0522943 -4.1021667 -4.1513314 -4.1686053 -4.1326408 -4.0783806][-4.2257614 -4.2081714 -4.2096167 -4.2310982 -4.2437439 -4.2221007 -4.1654334 -4.0860858 -4.0431604 -4.06474 -4.1174917 -4.163331 -4.1669006 -4.1259809 -4.0780993][-4.2016511 -4.1899867 -4.2059469 -4.2418866 -4.2641816 -4.2445378 -4.1825647 -4.1021571 -4.0665693 -4.0950704 -4.1432629 -4.1822772 -4.1749363 -4.1369896 -4.0987992][-4.1943164 -4.1889238 -4.2151976 -4.2593665 -4.28618 -4.2702413 -4.2099023 -4.1396446 -4.1153765 -4.1430974 -4.1822796 -4.2128191 -4.1991391 -4.164115 -4.1310129][-4.2123103 -4.2076745 -4.2326217 -4.27422 -4.300766 -4.2917032 -4.2439404 -4.1889825 -4.1699896 -4.1887774 -4.2200174 -4.2449713 -4.2316704 -4.2022634 -4.1719604][-4.24436 -4.2394876 -4.2547374 -4.2853665 -4.3098254 -4.3087668 -4.2782054 -4.237896 -4.2192035 -4.2282114 -4.2545042 -4.2781076 -4.2698641 -4.2471066 -4.2205143][-4.2760305 -4.2734432 -4.2796073 -4.2988338 -4.3200731 -4.3239274 -4.3069644 -4.2783732 -4.2589655 -4.2616391 -4.2831106 -4.305747 -4.3039145 -4.2884412 -4.2658105][-4.2898569 -4.2916641 -4.2974854 -4.3121991 -4.3291869 -4.3362365 -4.3282275 -4.3071136 -4.2865148 -4.2840753 -4.2986207 -4.318265 -4.3224025 -4.31359 -4.2978406][-4.2828608 -4.2911625 -4.3019247 -4.3157973 -4.32903 -4.3372278 -4.3350673 -4.3188281 -4.2972574 -4.2889347 -4.29678 -4.311852 -4.3193245 -4.3159757 -4.3095574]]...]
INFO - root - 2017-12-07 21:50:42.105828: step 57710, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 52h:35m:05s remains)
INFO - root - 2017-12-07 21:50:49.000510: step 57720, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 56h:07m:13s remains)
INFO - root - 2017-12-07 21:50:55.802396: step 57730, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 50h:13m:10s remains)
INFO - root - 2017-12-07 21:51:02.425063: step 57740, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 47h:54m:42s remains)
INFO - root - 2017-12-07 21:51:09.272384: step 57750, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 48h:40m:30s remains)
INFO - root - 2017-12-07 21:51:16.036327: step 57760, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 52h:53m:13s remains)
INFO - root - 2017-12-07 21:51:22.778550: step 57770, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 55h:10m:19s remains)
INFO - root - 2017-12-07 21:51:29.588410: step 57780, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 51h:24m:33s remains)
INFO - root - 2017-12-07 21:51:36.352953: step 57790, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 55h:53m:14s remains)
INFO - root - 2017-12-07 21:51:42.972042: step 57800, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.637 sec/batch; 48h:38m:38s remains)
2017-12-07 21:51:43.760526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1751332 -4.1824112 -4.2099023 -4.2351742 -4.2509966 -4.2608747 -4.2687168 -4.2721353 -4.2709379 -4.2588639 -4.2412333 -4.2287889 -4.220057 -4.2148805 -4.2135921][-4.1727862 -4.1773949 -4.2044287 -4.2293735 -4.2400646 -4.2415724 -4.2443013 -4.2538342 -4.2701907 -4.2739162 -4.2635169 -4.2518315 -4.238266 -4.2299123 -4.2270589][-4.1813273 -4.1824293 -4.2087502 -4.2307749 -4.2317004 -4.2181835 -4.2087579 -4.2194929 -4.2510662 -4.2701068 -4.2659516 -4.2574706 -4.2445226 -4.2357392 -4.2320809][-4.1905208 -4.1912317 -4.2130847 -4.2277923 -4.2127695 -4.1764359 -4.14803 -4.1573982 -4.20848 -4.2457533 -4.2504058 -4.2479329 -4.2376842 -4.2317047 -4.231565][-4.1943331 -4.1953959 -4.2106295 -4.2104831 -4.1747308 -4.1087632 -4.0509315 -4.0511346 -4.1258035 -4.1970654 -4.2245712 -4.2331514 -4.2265782 -4.220202 -4.2247319][-4.1939869 -4.1957331 -4.2030311 -4.1911969 -4.1372519 -4.0391178 -3.9332936 -3.9016039 -4.0006266 -4.1192684 -4.1838913 -4.211144 -4.2115068 -4.2061605 -4.2166295][-4.1928806 -4.1918821 -4.19547 -4.176249 -4.1105871 -3.9887273 -3.8341646 -3.7538733 -3.8676782 -4.0343885 -4.1434007 -4.1950831 -4.2064319 -4.2041597 -4.2171154][-4.1917052 -4.1856585 -4.1919432 -4.1787877 -4.120182 -4.005846 -3.8514161 -3.756321 -3.8475819 -4.0122538 -4.1404219 -4.2054219 -4.2217112 -4.2175856 -4.22468][-4.2120342 -4.2076154 -4.2135668 -4.2088161 -4.1725106 -4.092104 -3.9795203 -3.9043643 -3.9521942 -4.0696287 -4.1789613 -4.2358379 -4.2468715 -4.2356009 -4.2303896][-4.2378597 -4.237915 -4.2443876 -4.2426834 -4.220654 -4.17004 -4.0970416 -4.0442514 -4.0631614 -4.1366954 -4.2184377 -4.2625375 -4.270606 -4.2575383 -4.2435369][-4.2545686 -4.2588873 -4.269743 -4.2712817 -4.2593141 -4.231698 -4.186769 -4.1505132 -4.1553488 -4.1999946 -4.2562361 -4.2885823 -4.2947993 -4.2838945 -4.2668371][-4.2660432 -4.2725067 -4.2863617 -4.2912283 -4.286654 -4.2754703 -4.2528844 -4.2335496 -4.2332067 -4.2539787 -4.2867222 -4.306685 -4.3101258 -4.300437 -4.2839909][-4.2924356 -4.2998881 -4.3118072 -4.3161411 -4.3134961 -4.3079567 -4.2980857 -4.2905455 -4.2902269 -4.2977314 -4.3119574 -4.3214197 -4.3195996 -4.308301 -4.2928343][-4.3078794 -4.3136735 -4.3225231 -4.3262973 -4.3237381 -4.3188996 -4.3125744 -4.309073 -4.3100672 -4.3144445 -4.3212194 -4.3264565 -4.3237696 -4.3155794 -4.3027358][-4.304945 -4.3086662 -4.3155627 -4.3196988 -4.3182693 -4.3145761 -4.3103104 -4.3079152 -4.308506 -4.3117962 -4.3160138 -4.3194933 -4.3204694 -4.3183589 -4.312624]]...]
INFO - root - 2017-12-07 21:51:50.483756: step 57810, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 52h:34m:08s remains)
INFO - root - 2017-12-07 21:51:57.311938: step 57820, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 52h:03m:05s remains)
INFO - root - 2017-12-07 21:52:04.059679: step 57830, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 47h:58m:50s remains)
INFO - root - 2017-12-07 21:52:10.945061: step 57840, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 56h:54m:09s remains)
INFO - root - 2017-12-07 21:52:17.901597: step 57850, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 55h:34m:26s remains)
INFO - root - 2017-12-07 21:52:24.645165: step 57860, loss = 2.03, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 51h:42m:24s remains)
INFO - root - 2017-12-07 21:52:31.406202: step 57870, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 49h:41m:35s remains)
INFO - root - 2017-12-07 21:52:38.139244: step 57880, loss = 2.08, batch loss = 2.02 (13.2 examples/sec; 0.606 sec/batch; 46h:12m:11s remains)
INFO - root - 2017-12-07 21:52:44.814879: step 57890, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 51h:24m:31s remains)
INFO - root - 2017-12-07 21:52:51.541819: step 57900, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.705 sec/batch; 53h:45m:07s remains)
2017-12-07 21:52:52.313278: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2230639 -4.22623 -4.2353354 -4.2452607 -4.2560463 -4.2563391 -4.2543931 -4.2512832 -4.2455511 -4.2405796 -4.2363415 -4.2351651 -4.2374229 -4.2454591 -4.2487178][-4.1980467 -4.2041459 -4.21819 -4.2272396 -4.2338595 -4.2268825 -4.2159142 -4.2000856 -4.1889658 -4.1890826 -4.1964345 -4.2049274 -4.2147141 -4.2310505 -4.2353907][-4.1867371 -4.195672 -4.208343 -4.212749 -4.2082677 -4.1867509 -4.1519351 -4.1063194 -4.0878286 -4.1101365 -4.1469693 -4.1721354 -4.1937547 -4.2212887 -4.2293954][-4.1969328 -4.2075105 -4.2136922 -4.20967 -4.1922774 -4.1501265 -4.0734167 -3.9844666 -3.9602447 -4.0208826 -4.0984879 -4.1471367 -4.184454 -4.2163496 -4.2242303][-4.2206593 -4.2334652 -4.2357745 -4.225791 -4.1979342 -4.1345353 -4.0193458 -3.8987164 -3.8783476 -3.9705155 -4.0789671 -4.14437 -4.1863508 -4.2093635 -4.2067618][-4.2476072 -4.2637019 -4.266901 -4.252224 -4.2185006 -4.1459045 -4.0261869 -3.911428 -3.9034038 -3.9981358 -4.1039805 -4.166976 -4.1979117 -4.2040911 -4.1863527][-4.2612133 -4.2769637 -4.2819095 -4.2717357 -4.2434587 -4.1785107 -4.0760512 -3.9896021 -3.9938674 -4.0706968 -4.1531744 -4.2018247 -4.2180095 -4.2074051 -4.1748734][-4.2480736 -4.2603726 -4.2683597 -4.2660794 -4.2469187 -4.1992583 -4.1318278 -4.0793438 -4.0895934 -4.1395354 -4.1904655 -4.219583 -4.2235436 -4.2023411 -4.1657033][-4.1986938 -4.2094231 -4.2208285 -4.2250957 -4.2115912 -4.1819491 -4.1500678 -4.1304574 -4.1466913 -4.177207 -4.2028341 -4.2123737 -4.204761 -4.1797829 -4.1494603][-4.1420093 -4.1532526 -4.1676068 -4.1764674 -4.165607 -4.1440368 -4.1354623 -4.1411314 -4.1628046 -4.1832504 -4.1934242 -4.1887755 -4.1758208 -4.1579032 -4.1397128][-4.1361227 -4.1475172 -4.1613312 -4.1672139 -4.1545877 -4.1333981 -4.1333733 -4.1502891 -4.173151 -4.1898155 -4.1934052 -4.1814866 -4.168736 -4.1580372 -4.1497865][-4.1793489 -4.1851959 -4.190043 -4.1868482 -4.1745887 -4.1597123 -4.1650991 -4.1808848 -4.1991439 -4.2077031 -4.2067561 -4.1970639 -4.1908965 -4.1859531 -4.1800942][-4.2471213 -4.2460828 -4.2396507 -4.22736 -4.21537 -4.20925 -4.2213488 -4.2344189 -4.2432694 -4.2447286 -4.2398944 -4.2278337 -4.221272 -4.2208834 -4.2185311][-4.3066745 -4.3008409 -4.2887053 -4.2735105 -4.2639766 -4.2641459 -4.2791934 -4.2903352 -4.2935586 -4.2901688 -4.2831745 -4.2701936 -4.260076 -4.2583656 -4.255301][-4.3322964 -4.3293872 -4.31912 -4.3065724 -4.3019233 -4.3061051 -4.3188663 -4.3265505 -4.3281469 -4.3232212 -4.315176 -4.3055472 -4.2982721 -4.2967758 -4.2920437]]...]
INFO - root - 2017-12-07 21:52:59.130430: step 57910, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.645 sec/batch; 49h:10m:06s remains)
INFO - root - 2017-12-07 21:53:05.850078: step 57920, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.700 sec/batch; 53h:25m:04s remains)
INFO - root - 2017-12-07 21:53:12.552343: step 57930, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 51h:55m:58s remains)
INFO - root - 2017-12-07 21:53:19.304707: step 57940, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 48h:33m:39s remains)
INFO - root - 2017-12-07 21:53:26.097765: step 57950, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 50h:04m:52s remains)
INFO - root - 2017-12-07 21:53:32.897990: step 57960, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 55h:52m:19s remains)
INFO - root - 2017-12-07 21:53:39.693987: step 57970, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 56h:40m:44s remains)
INFO - root - 2017-12-07 21:53:46.547683: step 57980, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 52h:00m:44s remains)
INFO - root - 2017-12-07 21:53:53.378904: step 57990, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 48h:00m:02s remains)
INFO - root - 2017-12-07 21:53:59.954455: step 58000, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 48h:10m:30s remains)
2017-12-07 21:54:00.759008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3372955 -4.3349075 -4.33274 -4.3334274 -4.3391118 -4.3474112 -4.353559 -4.35695 -4.359498 -4.3584552 -4.3567958 -4.3583493 -4.3617935 -4.3658161 -4.3688436][-4.2904787 -4.2816324 -4.2786536 -4.2815452 -4.291111 -4.3059082 -4.3162808 -4.3224239 -4.3262086 -4.327548 -4.3295088 -4.3338289 -4.3384781 -4.34567 -4.3533931][-4.2180281 -4.1995654 -4.1955438 -4.2015333 -4.216836 -4.2386508 -4.2496939 -4.252399 -4.2550273 -4.2597594 -4.2740841 -4.2893367 -4.2989378 -4.307879 -4.3169727][-4.1536336 -4.121501 -4.1103783 -4.1119967 -4.1293225 -4.1546865 -4.1654963 -4.1621 -4.1597562 -4.1702118 -4.2027912 -4.2340627 -4.2536092 -4.2666893 -4.2758889][-4.104332 -4.049624 -4.0215688 -4.0120354 -4.0236826 -4.0460753 -4.0539336 -4.0530996 -4.0548506 -4.075686 -4.1271009 -4.1711526 -4.2004886 -4.2289143 -4.2488804][-4.0570574 -3.976203 -3.9205065 -3.8919349 -3.8864813 -3.8854487 -3.8762202 -3.8916333 -3.9235325 -3.9688311 -4.0412855 -4.0983739 -4.1413784 -4.1918449 -4.2308693][-4.0225239 -3.9288642 -3.8617692 -3.816077 -3.7772574 -3.7148261 -3.6471267 -3.6661696 -3.7412448 -3.8264844 -3.9256625 -4.0045581 -4.0671644 -4.141593 -4.2043152][-4.0112581 -3.9263709 -3.8649127 -3.8208983 -3.776587 -3.696 -3.5976655 -3.5777357 -3.6356266 -3.7256575 -3.8386889 -3.9346066 -4.0165758 -4.105618 -4.1848574][-4.02889 -3.9663379 -3.9231844 -3.8895261 -3.8658941 -3.8320394 -3.7777672 -3.7303269 -3.7065191 -3.7417028 -3.8345377 -3.9316196 -4.0137725 -4.0996695 -4.1858792][-4.0738029 -4.0334582 -4.0030279 -3.9754386 -3.9597731 -3.9567473 -3.9409685 -3.8981397 -3.8413148 -3.8374691 -3.8992078 -3.9773278 -4.0459065 -4.1203823 -4.1964407][-4.1110668 -4.0850577 -4.066165 -4.0443597 -4.0243859 -4.0173688 -4.0127578 -3.9888008 -3.9439919 -3.9307444 -3.9718151 -4.0308886 -4.0879784 -4.1494484 -4.2110558][-4.1309257 -4.1127272 -4.1028128 -4.0943689 -4.080718 -4.0666866 -4.0560932 -4.039063 -4.0078516 -3.9949517 -4.0246153 -4.0671954 -4.1156826 -4.1702695 -4.2245045][-4.1560535 -4.1388316 -4.1318154 -4.1331091 -4.1319313 -4.1248589 -4.1150417 -4.0992627 -4.0755548 -4.063529 -4.0838466 -4.1109314 -4.1437821 -4.1893473 -4.2366524][-4.1889105 -4.1745434 -4.1709805 -4.1748209 -4.1759205 -4.172328 -4.16642 -4.1563244 -4.13953 -4.127111 -4.1410308 -4.1609249 -4.1834745 -4.2160344 -4.2522097][-4.2323503 -4.2241416 -4.2219048 -4.2214928 -4.2176132 -4.2151866 -4.2117615 -4.2047057 -4.1940074 -4.1836963 -4.192636 -4.209475 -4.225616 -4.2455454 -4.2700524]]...]
INFO - root - 2017-12-07 21:54:07.569404: step 58010, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 49h:38m:38s remains)
INFO - root - 2017-12-07 21:54:14.314170: step 58020, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.653 sec/batch; 49h:47m:47s remains)
INFO - root - 2017-12-07 21:54:21.101122: step 58030, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 51h:42m:54s remains)
INFO - root - 2017-12-07 21:54:28.000119: step 58040, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.698 sec/batch; 53h:14m:22s remains)
INFO - root - 2017-12-07 21:54:34.759897: step 58050, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 51h:08m:28s remains)
INFO - root - 2017-12-07 21:54:41.609240: step 58060, loss = 2.04, batch loss = 1.99 (13.0 examples/sec; 0.617 sec/batch; 47h:00m:59s remains)
INFO - root - 2017-12-07 21:54:48.528993: step 58070, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 50h:21m:48s remains)
INFO - root - 2017-12-07 21:54:55.437791: step 58080, loss = 2.10, batch loss = 2.04 (10.7 examples/sec; 0.747 sec/batch; 56h:54m:25s remains)
INFO - root - 2017-12-07 21:55:02.257485: step 58090, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.706 sec/batch; 53h:47m:29s remains)
INFO - root - 2017-12-07 21:55:09.026997: step 58100, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 49h:51m:05s remains)
2017-12-07 21:55:09.726896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2903724 -4.3244286 -4.3293571 -4.2919869 -4.2314944 -4.1744628 -4.1483674 -4.1582379 -4.1845326 -4.2028666 -4.1929207 -4.1475968 -4.087409 -4.0446978 -4.0789189][-4.2646813 -4.3136435 -4.3239579 -4.2827196 -4.2171235 -4.161078 -4.1326528 -4.1341305 -4.1553831 -4.1816258 -4.1756253 -4.1284094 -4.0582132 -4.0073514 -4.041688][-4.2376046 -4.2831993 -4.2912731 -4.2504296 -4.1906257 -4.1447144 -4.1219206 -4.1176882 -4.1329703 -4.16089 -4.15943 -4.1121249 -4.0350885 -3.9789686 -4.0175872][-4.2180362 -4.2517877 -4.2518563 -4.2117591 -4.1611552 -4.1273351 -4.1145191 -4.1151538 -4.1316814 -4.1621661 -4.1632843 -4.1131592 -4.0336103 -3.9771631 -4.0163703][-4.1917796 -4.2191386 -4.215836 -4.1754985 -4.1326132 -4.1149964 -4.1143742 -4.12917 -4.1501966 -4.1799116 -4.1786776 -4.1254344 -4.0475149 -3.9964025 -4.032249][-4.1551285 -4.1825314 -4.1837306 -4.1468863 -4.1142211 -4.1079926 -4.1127224 -4.1355076 -4.1590223 -4.1841745 -4.182693 -4.1322417 -4.0638623 -4.0229549 -4.0508218][-4.11864 -4.1403651 -4.1533713 -4.1276574 -4.0955229 -4.0823565 -4.0799422 -4.1060648 -4.1401329 -4.170785 -4.1682553 -4.1201763 -4.0599875 -4.0310717 -4.0558619][-4.0901928 -4.1087332 -4.1255236 -4.1135278 -4.0894418 -4.065989 -4.0513496 -4.0743413 -4.1179094 -4.1542754 -4.151237 -4.1017671 -4.045012 -4.0214996 -4.0466776][-4.0965724 -4.1117773 -4.1258731 -4.1169219 -4.097528 -4.0747132 -4.0577693 -4.0765395 -4.1246777 -4.1581693 -4.1498933 -4.1001334 -4.0494442 -4.0317063 -4.0597768][-4.1524034 -4.1663547 -4.1784577 -4.1678362 -4.1500726 -4.1299233 -4.1118455 -4.1236906 -4.1636524 -4.1896448 -4.179173 -4.14127 -4.1033912 -4.0896668 -4.1115584][-4.2089844 -4.2236147 -4.2376556 -4.2322793 -4.2209854 -4.2042866 -4.1894469 -4.1972136 -4.2218561 -4.2351418 -4.2254114 -4.2025785 -4.1792927 -4.1689053 -4.1801448][-4.2628012 -4.275033 -4.2846413 -4.2811565 -4.2729292 -4.2628579 -4.2567153 -4.2647438 -4.2790589 -4.28437 -4.2739444 -4.2565322 -4.2431788 -4.2357068 -4.2403426][-4.3015447 -4.3103185 -4.3151402 -4.31066 -4.3032045 -4.29603 -4.2941189 -4.3006582 -4.3082204 -4.3120852 -4.306273 -4.2955465 -4.2890639 -4.2822838 -4.2821112][-4.3233643 -4.3296318 -4.3318172 -4.328927 -4.3237257 -4.3179073 -4.3169246 -4.3210735 -4.3238349 -4.3265057 -4.3234177 -4.3158073 -4.3103843 -4.30398 -4.3026123][-4.3236189 -4.3270674 -4.3286676 -4.3287683 -4.328722 -4.3268008 -4.3256435 -4.325 -4.3225622 -4.3212729 -4.3200626 -4.316411 -4.3131924 -4.3101134 -4.3114476]]...]
INFO - root - 2017-12-07 21:55:16.589869: step 58110, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 55h:37m:06s remains)
INFO - root - 2017-12-07 21:55:23.397133: step 58120, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 53h:58m:21s remains)
INFO - root - 2017-12-07 21:55:30.097832: step 58130, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 48h:41m:14s remains)
INFO - root - 2017-12-07 21:55:36.950239: step 58140, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 48h:56m:08s remains)
INFO - root - 2017-12-07 21:55:43.754709: step 58150, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 53h:28m:49s remains)
INFO - root - 2017-12-07 21:55:50.547148: step 58160, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 53h:50m:31s remains)
INFO - root - 2017-12-07 21:55:57.226517: step 58170, loss = 2.09, batch loss = 2.04 (11.7 examples/sec; 0.687 sec/batch; 52h:19m:20s remains)
INFO - root - 2017-12-07 21:56:04.029997: step 58180, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 50h:13m:55s remains)
INFO - root - 2017-12-07 21:56:10.794564: step 58190, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 48h:17m:07s remains)
INFO - root - 2017-12-07 21:56:17.511118: step 58200, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 53h:06m:47s remains)
2017-12-07 21:56:18.264981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3402853 -4.3362103 -4.3281617 -4.3154902 -4.3016462 -4.2926679 -4.2889676 -4.2916269 -4.2943597 -4.2996426 -4.310411 -4.3257074 -4.3370271 -4.3340473 -4.3223433][-4.3407125 -4.3320909 -4.3186364 -4.2982802 -4.2773976 -4.2635522 -4.257937 -4.2585092 -4.2609596 -4.2665229 -4.2813339 -4.3064942 -4.3236675 -4.3207836 -4.3034105][-4.3340354 -4.3242168 -4.3077316 -4.278409 -4.2490749 -4.2314987 -4.2242508 -4.2221389 -4.222486 -4.228641 -4.2475562 -4.2807751 -4.3025928 -4.302845 -4.279038][-4.3241897 -4.3114367 -4.2914262 -4.2563472 -4.22079 -4.2019863 -4.1968193 -4.19658 -4.1948504 -4.1954503 -4.2123322 -4.2474785 -4.2752118 -4.2814436 -4.2538939][-4.3069677 -4.2846675 -4.259901 -4.2239685 -4.1867952 -4.1682234 -4.1667185 -4.1716232 -4.1760731 -4.1774287 -4.18691 -4.2165046 -4.2443142 -4.25481 -4.2303963][-4.2903175 -4.2560964 -4.2216139 -4.1829481 -4.1473365 -4.1279511 -4.1261621 -4.1336622 -4.1495848 -4.1601605 -4.1654072 -4.1854448 -4.2097549 -4.2247553 -4.2142997][-4.2849164 -4.2383451 -4.1889176 -4.142746 -4.1106291 -4.0937204 -4.0893993 -4.0978131 -4.1245232 -4.1429262 -4.1477437 -4.16521 -4.1857347 -4.2044148 -4.2093434][-4.28811 -4.2379551 -4.1780562 -4.127717 -4.1079082 -4.1069164 -4.1071553 -4.1131368 -4.1413159 -4.160212 -4.161087 -4.172008 -4.1903887 -4.2116628 -4.2320623][-4.2892017 -4.2467475 -4.1865439 -4.1305656 -4.1107044 -4.1217041 -4.1312633 -4.1371284 -4.1567111 -4.1684337 -4.1669121 -4.1726837 -4.1877375 -4.21777 -4.2521424][-4.2781706 -4.2503037 -4.205976 -4.1518197 -4.1243896 -4.12702 -4.1370926 -4.1427379 -4.1549368 -4.1604266 -4.1629825 -4.1654253 -4.1766491 -4.2107177 -4.2545648][-4.2562823 -4.2450657 -4.2234344 -4.1880465 -4.1617212 -4.154623 -4.1587577 -4.1647987 -4.170836 -4.1717272 -4.1725421 -4.1719632 -4.1793895 -4.2128572 -4.2580056][-4.24791 -4.2508283 -4.2476816 -4.2334442 -4.2174597 -4.2064304 -4.2071705 -4.2107224 -4.2078872 -4.20287 -4.199183 -4.1985407 -4.20753 -4.2375317 -4.2749071][-4.2545524 -4.2652259 -4.2767334 -4.2786903 -4.2755284 -4.2693653 -4.2648077 -4.2629676 -4.2571983 -4.2476687 -4.2398772 -4.2353716 -4.2423286 -4.265624 -4.2920547][-4.26277 -4.2793512 -4.299684 -4.3104486 -4.3134961 -4.3109264 -4.3045077 -4.3012714 -4.2995591 -4.2944679 -4.2856665 -4.275455 -4.2747631 -4.2879343 -4.3047757][-4.2726693 -4.2897854 -4.3097253 -4.3213816 -4.3255649 -4.3247538 -4.3214364 -4.3216748 -4.32512 -4.3280478 -4.3221908 -4.3112259 -4.3047938 -4.306416 -4.313365]]...]
INFO - root - 2017-12-07 21:56:24.981876: step 58210, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.639 sec/batch; 48h:40m:17s remains)
INFO - root - 2017-12-07 21:56:31.857919: step 58220, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 49h:11m:29s remains)
INFO - root - 2017-12-07 21:56:38.758737: step 58230, loss = 2.06, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 57h:19m:30s remains)
INFO - root - 2017-12-07 21:56:45.367417: step 58240, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 52h:33m:34s remains)
INFO - root - 2017-12-07 21:56:52.124032: step 58250, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 53h:45m:59s remains)
INFO - root - 2017-12-07 21:56:58.926766: step 58260, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 52h:14m:57s remains)
INFO - root - 2017-12-07 21:57:05.786851: step 58270, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.737 sec/batch; 56h:10m:41s remains)
INFO - root - 2017-12-07 21:57:12.652533: step 58280, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.732 sec/batch; 55h:46m:46s remains)
INFO - root - 2017-12-07 21:57:19.375636: step 58290, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 51h:19m:55s remains)
INFO - root - 2017-12-07 21:57:25.908363: step 58300, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 47h:57m:11s remains)
2017-12-07 21:57:26.639440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.225462 -4.2269149 -4.22052 -4.215169 -4.2070112 -4.1981521 -4.1992812 -4.2227921 -4.2410235 -4.2379785 -4.228858 -4.2208581 -4.2146707 -4.2213531 -4.2336473][-4.2183261 -4.2191095 -4.2140737 -4.2113686 -4.2045221 -4.1952367 -4.194088 -4.2180023 -4.2383485 -4.2350483 -4.2264934 -4.2200041 -4.2141976 -4.2213535 -4.231195][-4.2122078 -4.213654 -4.2118888 -4.2106619 -4.2005243 -4.1869292 -4.1797605 -4.1987271 -4.2185907 -4.2171693 -4.2114735 -4.2116089 -4.2091751 -4.2161627 -4.2236953][-4.2107177 -4.2134442 -4.2101908 -4.203866 -4.1885614 -4.16986 -4.1569724 -4.1696124 -4.18711 -4.1871686 -4.1837649 -4.1904912 -4.194818 -4.2038751 -4.2118344][-4.2113519 -4.2177243 -4.2124386 -4.1953521 -4.1676025 -4.1382847 -4.1161194 -4.1163549 -4.1258345 -4.1276875 -4.1307607 -4.1493793 -4.1660156 -4.1858964 -4.199369][-4.21075 -4.2192612 -4.2092385 -4.1777792 -4.1329546 -4.0876255 -4.0507231 -4.033628 -4.0311089 -4.0351534 -4.0553818 -4.0977659 -4.132494 -4.1615505 -4.1809282][-4.2057266 -4.2088161 -4.189611 -4.1459064 -4.0846715 -4.0244126 -3.9778781 -3.9508324 -3.9421341 -3.9529603 -3.9978774 -4.0626192 -4.1080022 -4.1397429 -4.1620417][-4.1996536 -4.1880388 -4.154942 -4.1020494 -4.0360417 -3.9716704 -3.9323297 -3.9183156 -3.9206376 -3.9440575 -3.9976697 -4.0620832 -4.1026149 -4.132906 -4.1559391][-4.2087741 -4.1892438 -4.152216 -4.1039147 -4.049427 -3.9961104 -3.9720423 -3.9749141 -3.9866211 -4.0051761 -4.0477076 -4.0995169 -4.130404 -4.1558118 -4.1752639][-4.2238817 -4.2104254 -4.1836061 -4.1511564 -4.1144972 -4.0756364 -4.0605593 -4.0658283 -4.0701284 -4.0737233 -4.1016884 -4.1396494 -4.1658144 -4.1887779 -4.2071733][-4.2418871 -4.2398629 -4.2305975 -4.2150469 -4.193747 -4.1672606 -4.1537781 -4.1495991 -4.1397123 -4.1313081 -4.1494184 -4.1789436 -4.203618 -4.2246952 -4.2407188][-4.2521968 -4.2568312 -4.2615194 -4.2587023 -4.2483644 -4.2309675 -4.2180252 -4.2058597 -4.1882062 -4.1745639 -4.1888747 -4.2139335 -4.2344637 -4.2502813 -4.2616544][-4.2608705 -4.264781 -4.2731524 -4.2761588 -4.2730637 -4.2648191 -4.2583308 -4.2473249 -4.2311506 -4.217802 -4.2263579 -4.2440281 -4.2593856 -4.2702103 -4.2774258][-4.2789111 -4.2796569 -4.2841058 -4.2861533 -4.2879524 -4.288209 -4.2897229 -4.286386 -4.2775726 -4.268187 -4.2705331 -4.2781811 -4.2857261 -4.291853 -4.295289][-4.3033319 -4.3016825 -4.3030028 -4.3043218 -4.3069186 -4.3090429 -4.3121853 -4.3136353 -4.3095355 -4.3041582 -4.3039989 -4.306273 -4.3087273 -4.310298 -4.3113451]]...]
INFO - root - 2017-12-07 21:57:33.413951: step 58310, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 56h:16m:07s remains)
INFO - root - 2017-12-07 21:57:40.231831: step 58320, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 52h:43m:46s remains)
INFO - root - 2017-12-07 21:57:47.051700: step 58330, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 48h:04m:26s remains)
INFO - root - 2017-12-07 21:57:53.806368: step 58340, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 49h:17m:34s remains)
INFO - root - 2017-12-07 21:58:00.654945: step 58350, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 55h:44m:00s remains)
INFO - root - 2017-12-07 21:58:07.495892: step 58360, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 55h:28m:13s remains)
INFO - root - 2017-12-07 21:58:14.270241: step 58370, loss = 2.08, batch loss = 2.03 (13.0 examples/sec; 0.616 sec/batch; 46h:54m:55s remains)
INFO - root - 2017-12-07 21:58:21.011471: step 58380, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 49h:24m:46s remains)
INFO - root - 2017-12-07 21:58:27.799152: step 58390, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 50h:53m:27s remains)
INFO - root - 2017-12-07 21:58:34.425506: step 58400, loss = 2.09, batch loss = 2.04 (10.7 examples/sec; 0.751 sec/batch; 57h:11m:28s remains)
2017-12-07 21:58:35.159055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1688647 -4.1582561 -4.172852 -4.1838045 -4.170321 -4.1574469 -4.1755919 -4.2041121 -4.2207651 -4.218461 -4.2022777 -4.2000713 -4.1845269 -4.1691837 -4.1683903][-4.138041 -4.1359248 -4.1589608 -4.169714 -4.1520014 -4.1354628 -4.1513915 -4.1820388 -4.1980658 -4.1913619 -4.1747355 -4.1760769 -4.1624055 -4.1458182 -4.1470709][-4.1223516 -4.1274743 -4.1502051 -4.15666 -4.1341944 -4.1117239 -4.1205511 -4.1478829 -4.162509 -4.1577582 -4.1461654 -4.1522069 -4.1416025 -4.1260376 -4.1305008][-4.1073766 -4.1138487 -4.1324997 -4.1355696 -4.1119719 -4.086432 -4.0871248 -4.1089578 -4.12271 -4.1212268 -4.1166553 -4.1246624 -4.1212764 -4.1149693 -4.1233206][-4.1057181 -4.1096649 -4.1225548 -4.1176186 -4.090847 -4.0626593 -4.0610728 -4.0813489 -4.0944648 -4.0943961 -4.0945177 -4.1036329 -4.1108994 -4.1207218 -4.1346121][-4.0966921 -4.1001019 -4.1128216 -4.1038971 -4.069365 -4.0362372 -4.0418105 -4.0675497 -4.0780439 -4.0745783 -4.072042 -4.0793457 -4.0916986 -4.1124353 -4.1325736][-4.0656834 -4.072041 -4.0914121 -4.0817704 -4.0364218 -3.9965119 -4.01162 -4.0432739 -4.0510392 -4.0409422 -4.0326548 -4.0378561 -4.0540524 -4.0780711 -4.1033888][-4.0428514 -4.0608635 -4.0894713 -4.0839791 -4.0353274 -3.9888022 -3.9987593 -4.0219474 -4.0228944 -4.006793 -3.9911222 -3.994868 -4.0185542 -4.0451064 -4.0741482][-4.0235929 -4.0540485 -4.0872664 -4.0895376 -4.0521441 -4.01165 -4.0103531 -4.0165977 -4.0091491 -3.9883294 -3.9648569 -3.9634533 -3.9916818 -4.0238276 -4.0531139][-4.0283594 -4.0600562 -4.0835519 -4.0848 -4.0615568 -4.035275 -4.0311279 -4.0300732 -4.0199013 -3.9987745 -3.9719753 -3.9628108 -3.9838083 -4.0155811 -4.0446649][-4.0630326 -4.0887647 -4.1022606 -4.103663 -4.0940151 -4.0825329 -4.0784407 -4.0750422 -4.0654087 -4.0465264 -4.02104 -4.0074129 -4.0164905 -4.0384727 -4.0625138][-4.1328835 -4.1476574 -4.1529651 -4.1535449 -4.1524868 -4.1511521 -4.1498756 -4.1474113 -4.1423078 -4.1299524 -4.1123776 -4.0996132 -4.1012769 -4.1124272 -4.1262093][-4.2074466 -4.2130475 -4.2139707 -4.2137027 -4.2135625 -4.2138557 -4.2137303 -4.2126493 -4.2109814 -4.2079825 -4.2034345 -4.1974468 -4.1981773 -4.2028561 -4.2082496][-4.2646155 -4.265492 -4.2654381 -4.2653708 -4.26502 -4.2646718 -4.2635455 -4.2614059 -4.2593093 -4.2593122 -4.2608557 -4.2605472 -4.2625055 -4.264801 -4.2665763][-4.3006139 -4.3001785 -4.2994704 -4.2986369 -4.2972078 -4.2958603 -4.2942305 -4.2923026 -4.2907419 -4.2918944 -4.2957273 -4.298192 -4.2988787 -4.2977543 -4.2958717]]...]
INFO - root - 2017-12-07 21:58:41.917679: step 58410, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 48h:15m:16s remains)
INFO - root - 2017-12-07 21:58:48.748345: step 58420, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 52h:35m:17s remains)
INFO - root - 2017-12-07 21:58:55.660698: step 58430, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 55h:19m:54s remains)
INFO - root - 2017-12-07 21:59:02.402003: step 58440, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 51h:59m:25s remains)
INFO - root - 2017-12-07 21:59:09.134863: step 58450, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 48h:33m:11s remains)
INFO - root - 2017-12-07 21:59:15.888780: step 58460, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 49h:30m:57s remains)
INFO - root - 2017-12-07 21:59:22.704315: step 58470, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 53h:06m:51s remains)
INFO - root - 2017-12-07 21:59:29.486032: step 58480, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 54h:21m:31s remains)
INFO - root - 2017-12-07 21:59:36.211777: step 58490, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 50h:02m:17s remains)
INFO - root - 2017-12-07 21:59:42.938664: step 58500, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 49h:30m:17s remains)
2017-12-07 21:59:43.716633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3470092 -4.3531728 -4.3534231 -4.3501835 -4.3425632 -4.3321266 -4.3204479 -4.3109632 -4.3019958 -4.2983751 -4.3039012 -4.3135147 -4.3236213 -4.33212 -4.3383331][-4.3473616 -4.3528805 -4.3526216 -4.3455806 -4.332931 -4.3175468 -4.3025661 -4.2904072 -4.2790556 -4.2754345 -4.2840219 -4.2957921 -4.307476 -4.3196349 -4.3307176][-4.3445373 -4.3470817 -4.3417482 -4.3259578 -4.3045578 -4.2836952 -4.26803 -4.2555213 -4.2460141 -4.2470407 -4.262188 -4.2764 -4.2914276 -4.3062057 -4.3199816][-4.3371367 -4.3338671 -4.3182874 -4.2878866 -4.25215 -4.2250934 -4.2108526 -4.2026181 -4.2005143 -4.2127137 -4.2389727 -4.2584305 -4.2777867 -4.2946577 -4.3079991][-4.3213625 -4.3063345 -4.275351 -4.2281389 -4.1766086 -4.1472845 -4.1420088 -4.1453137 -4.1569328 -4.1834817 -4.2204742 -4.2466278 -4.268537 -4.2858539 -4.2970114][-4.3009753 -4.2692132 -4.2204819 -4.1506138 -4.0843291 -4.0593982 -4.0673943 -4.090488 -4.1210513 -4.1644459 -4.209671 -4.2396908 -4.2625246 -4.2789826 -4.2862][-4.2805071 -4.2296338 -4.1585588 -4.0609 -3.983223 -3.9667609 -3.9877205 -4.0281277 -4.0764523 -4.1316586 -4.1873512 -4.2274613 -4.2600636 -4.2785721 -4.2806778][-4.2623076 -4.1869802 -4.0872192 -3.9644806 -3.8838742 -3.8861613 -3.9309371 -3.9929454 -4.0562263 -4.1205182 -4.1831293 -4.2338991 -4.2729273 -4.28972 -4.2869968][-4.2552314 -4.1591339 -4.0394721 -3.9110336 -3.833035 -3.8504846 -3.9231205 -4.012126 -4.0924873 -4.1615825 -4.2229505 -4.2732911 -4.3068557 -4.3185692 -4.3093982][-4.2490211 -4.1492934 -4.037683 -3.9359906 -3.877115 -3.896456 -3.9714608 -4.0684824 -4.1549597 -4.222466 -4.2743244 -4.3167391 -4.3421946 -4.3489056 -4.3334322][-4.2306685 -4.14274 -4.0585232 -3.9965153 -3.9685938 -3.9947898 -4.0612383 -4.1424913 -4.2188015 -4.2749863 -4.3129621 -4.3435884 -4.3621254 -4.3629785 -4.3436227][-4.195684 -4.123877 -4.0708208 -4.0454278 -4.0480356 -4.0865674 -4.150176 -4.215086 -4.2730722 -4.3106709 -4.3308811 -4.3478518 -4.3573723 -4.3515806 -4.3290205][-4.1496811 -4.09003 -4.0600948 -4.0591393 -4.085165 -4.1379085 -4.2049747 -4.2639413 -4.3097444 -4.3295326 -4.3317046 -4.3313508 -4.32815 -4.3166533 -4.2930722][-4.1280775 -4.07958 -4.0644784 -4.075861 -4.1137419 -4.1715555 -4.2373543 -4.2902632 -4.3238373 -4.3321595 -4.3192472 -4.3021374 -4.2874608 -4.2711716 -4.2485538][-4.1442919 -4.1111259 -4.1050339 -4.1203203 -4.1565976 -4.2059045 -4.2595243 -4.3021712 -4.3253484 -4.325779 -4.306139 -4.2817492 -4.2598944 -4.2390995 -4.2171149]]...]
INFO - root - 2017-12-07 21:59:50.535656: step 58510, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 54h:25m:51s remains)
INFO - root - 2017-12-07 21:59:57.329095: step 58520, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 49h:27m:53s remains)
INFO - root - 2017-12-07 22:00:04.153101: step 58530, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 48h:10m:38s remains)
INFO - root - 2017-12-07 22:00:10.991006: step 58540, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 54h:31m:17s remains)
INFO - root - 2017-12-07 22:00:17.617569: step 58550, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.618 sec/batch; 47h:03m:54s remains)
INFO - root - 2017-12-07 22:00:24.415254: step 58560, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 47h:53m:11s remains)
INFO - root - 2017-12-07 22:00:31.108569: step 58570, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 49h:13m:59s remains)
INFO - root - 2017-12-07 22:00:37.965405: step 58580, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 55h:03m:29s remains)
INFO - root - 2017-12-07 22:00:44.833048: step 58590, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 54h:52m:51s remains)
INFO - root - 2017-12-07 22:00:51.470941: step 58600, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 49h:24m:12s remains)
2017-12-07 22:00:52.121846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2095776 -4.2214718 -4.22514 -4.2134957 -4.2071471 -4.2055206 -4.1882486 -4.1710539 -4.1701336 -4.1867547 -4.1897845 -4.1848774 -4.1922455 -4.2197709 -4.2495279][-4.2040625 -4.217236 -4.2209392 -4.2073832 -4.2062116 -4.206274 -4.1881247 -4.1698008 -4.1676493 -4.1858826 -4.1894178 -4.1827183 -4.1864462 -4.2106194 -4.2414131][-4.2101169 -4.2224669 -4.2258277 -4.2097077 -4.2128615 -4.2137671 -4.1945381 -4.1727114 -4.1696854 -4.18594 -4.1871309 -4.1804533 -4.1828752 -4.2050848 -4.2370968][-4.2098689 -4.222208 -4.2254963 -4.2071567 -4.2103539 -4.2084022 -4.1759019 -4.1433406 -4.1448321 -4.1706581 -4.1726284 -4.1657982 -4.1702461 -4.1941381 -4.2295566][-4.2158012 -4.2211518 -4.2197342 -4.1973939 -4.1889172 -4.1692109 -4.1119137 -4.0608768 -4.0772228 -4.1278191 -4.1415973 -4.1454682 -4.1584377 -4.1873465 -4.2247095][-4.2185559 -4.2123184 -4.2020526 -4.1754212 -4.1506791 -4.10358 -4.0110064 -3.9412403 -3.9845057 -4.0706224 -4.10728 -4.1311665 -4.1548719 -4.1915393 -4.231163][-4.2279243 -4.2131033 -4.195622 -4.1639924 -4.1221948 -4.04697 -3.9227958 -3.8364189 -3.911149 -4.0292149 -4.0841136 -4.1248331 -4.161118 -4.2028289 -4.242866][-4.2309451 -4.2161255 -4.1964684 -4.1589813 -4.1070156 -4.0218873 -3.8962743 -3.827704 -3.9203968 -4.0346918 -4.0885625 -4.1330972 -4.1720395 -4.2120619 -4.2486186][-4.2095938 -4.1958733 -4.1818542 -4.1530409 -4.1086564 -4.0373068 -3.941153 -3.9007936 -3.9783607 -4.0632687 -4.1022358 -4.1378446 -4.1721287 -4.2097907 -4.2429018][-4.2065153 -4.1896377 -4.1751204 -4.1529188 -4.1207728 -4.0694013 -4.0050077 -3.9832635 -4.0396461 -4.0978417 -4.1258421 -4.154635 -4.1822686 -4.2153316 -4.2431269][-4.2298274 -4.2113409 -4.1912966 -4.1687665 -4.137372 -4.0943079 -4.0465684 -4.034718 -4.0780029 -4.1207061 -4.1461382 -4.1757541 -4.2022204 -4.2311521 -4.2534208][-4.2553983 -4.2365909 -4.2112741 -4.1795678 -4.1440659 -4.1065383 -4.06758 -4.0632515 -4.1018443 -4.1424994 -4.1708183 -4.200984 -4.226934 -4.2508192 -4.2671261][-4.2816806 -4.2636189 -4.2362785 -4.1971531 -4.1595902 -4.1328664 -4.111712 -4.11811 -4.150743 -4.1870337 -4.2135553 -4.2399797 -4.2613668 -4.2775226 -4.2869496][-4.3008466 -4.2861104 -4.2643561 -4.2286868 -4.1987305 -4.1809745 -4.1752653 -4.1909642 -4.2172494 -4.2425542 -4.2608128 -4.2773967 -4.2920003 -4.3014483 -4.3049355][-4.3126926 -4.3035264 -4.2881384 -4.2642965 -4.2461138 -4.2348404 -4.2336893 -4.2472696 -4.2639546 -4.2794428 -4.2916064 -4.3004923 -4.3095031 -4.3135395 -4.3129053]]...]
INFO - root - 2017-12-07 22:00:58.857111: step 58610, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 51h:43m:06s remains)
INFO - root - 2017-12-07 22:01:05.653328: step 58620, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 55h:12m:11s remains)
INFO - root - 2017-12-07 22:01:12.322788: step 58630, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 48h:51m:40s remains)
INFO - root - 2017-12-07 22:01:19.105794: step 58640, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 54h:27m:49s remains)
INFO - root - 2017-12-07 22:01:25.853158: step 58650, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 48h:01m:46s remains)
INFO - root - 2017-12-07 22:01:32.552192: step 58660, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 48h:14m:01s remains)
INFO - root - 2017-12-07 22:01:39.347857: step 58670, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 55h:20m:05s remains)
INFO - root - 2017-12-07 22:01:46.007731: step 58680, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.700 sec/batch; 53h:14m:14s remains)
INFO - root - 2017-12-07 22:01:52.759978: step 58690, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 52h:06m:37s remains)
INFO - root - 2017-12-07 22:01:59.223565: step 58700, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 49h:59m:14s remains)
2017-12-07 22:01:59.967368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2369127 -4.213306 -4.1918659 -4.171308 -4.1600771 -4.1528277 -4.1582217 -4.1779008 -4.1862726 -4.1808124 -4.1639853 -4.149044 -4.1463075 -4.1545358 -4.1692634][-4.2256641 -4.2016582 -4.1807728 -4.162827 -4.1563406 -4.1518083 -4.15495 -4.1670737 -4.1723585 -4.1686907 -4.1570711 -4.1446347 -4.1415491 -4.1468687 -4.1562386][-4.198606 -4.1734772 -4.1541734 -4.1417856 -4.13988 -4.1407747 -4.1448693 -4.150846 -4.1526256 -4.1534019 -4.15299 -4.1510582 -4.1517782 -4.1535583 -4.1564527][-4.1673236 -4.1390414 -4.1192403 -4.1134095 -4.1173277 -4.124743 -4.1302495 -4.1307 -4.1311126 -4.1411366 -4.1574931 -4.1678491 -4.17118 -4.1696348 -4.168313][-4.1459823 -4.1175356 -4.0990663 -4.0952249 -4.0995426 -4.1048427 -4.1032829 -4.0918732 -4.0898676 -4.111959 -4.1432519 -4.1633377 -4.1720304 -4.1719251 -4.1695976][-4.1436067 -4.12005 -4.1042418 -4.0968566 -4.0947924 -4.0890975 -4.0739212 -4.046978 -4.0377474 -4.0704103 -4.1133981 -4.1418161 -4.1578846 -4.1632748 -4.1606178][-4.1523495 -4.1340642 -4.1227455 -4.113133 -4.099 -4.073349 -4.0347486 -3.9862733 -3.968385 -4.017602 -4.0816889 -4.12383 -4.151351 -4.1646657 -4.1628227][-4.1711106 -4.1602097 -4.1528521 -4.1387434 -4.1083922 -4.0560832 -3.9872711 -3.9071503 -3.8689394 -3.9400468 -4.0360527 -4.0978203 -4.1395907 -4.166079 -4.1641717][-4.1905785 -4.1835909 -4.18006 -4.1648045 -4.1260433 -4.0551596 -3.9645033 -3.8500228 -3.7824097 -3.8706548 -3.9905789 -4.0698476 -4.1237516 -4.1604595 -4.1590853][-4.2000127 -4.1958055 -4.1963248 -4.1864109 -4.1521163 -4.0877872 -4.0133839 -3.91672 -3.8591185 -3.9232697 -4.0099411 -4.0653839 -4.107111 -4.1397943 -4.1399446][-4.2000508 -4.1981888 -4.2010226 -4.1963067 -4.1678209 -4.1180553 -4.0683551 -4.0102472 -3.9826055 -4.02481 -4.0722175 -4.0940456 -4.108079 -4.1189575 -4.1122384][-4.1988435 -4.193604 -4.1962962 -4.19356 -4.1767111 -4.1455426 -4.11128 -4.0796857 -4.0735497 -4.1062045 -4.1333323 -4.1379166 -4.1339622 -4.124866 -4.1075559][-4.1896186 -4.18539 -4.1904917 -4.19181 -4.1878266 -4.1741443 -4.1543956 -4.1388116 -4.1437073 -4.1673021 -4.177989 -4.1741953 -4.1659217 -4.1524415 -4.1303592][-4.1779194 -4.1746154 -4.1796093 -4.186214 -4.1935754 -4.1967139 -4.1913819 -4.1822472 -4.184577 -4.1950088 -4.1941476 -4.1882448 -4.1851306 -4.1776557 -4.1589246][-4.17585 -4.1726885 -4.1778636 -4.1878471 -4.1998115 -4.20811 -4.2092147 -4.2050409 -4.206996 -4.2076855 -4.2023125 -4.2003407 -4.2042022 -4.2032175 -4.1891651]]...]
INFO - root - 2017-12-07 22:02:06.800435: step 58710, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 53h:57m:04s remains)
INFO - root - 2017-12-07 22:02:13.603117: step 58720, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 50h:59m:55s remains)
INFO - root - 2017-12-07 22:02:20.436347: step 58730, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 50h:47m:05s remains)
INFO - root - 2017-12-07 22:02:27.169056: step 58740, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 47h:50m:45s remains)
INFO - root - 2017-12-07 22:02:33.957189: step 58750, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 51h:27m:36s remains)
INFO - root - 2017-12-07 22:02:40.739635: step 58760, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 54h:30m:59s remains)
INFO - root - 2017-12-07 22:02:47.519730: step 58770, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 50h:11m:40s remains)
INFO - root - 2017-12-07 22:02:54.306200: step 58780, loss = 2.02, batch loss = 1.96 (12.3 examples/sec; 0.651 sec/batch; 49h:28m:16s remains)
INFO - root - 2017-12-07 22:03:01.015233: step 58790, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 48h:22m:27s remains)
INFO - root - 2017-12-07 22:03:07.643256: step 58800, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 49h:31m:43s remains)
2017-12-07 22:03:08.429846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3290215 -4.3262749 -4.3190374 -4.31448 -4.3076477 -4.3092804 -4.3111291 -4.3098192 -4.3077931 -4.3065872 -4.30591 -4.3055944 -4.30424 -4.3023863 -4.3000779][-4.3281326 -4.3266897 -4.3206325 -4.316854 -4.3086662 -4.3069773 -4.3039255 -4.2972422 -4.2931213 -4.2940588 -4.297997 -4.301722 -4.3029056 -4.3014493 -4.2970996][-4.3259258 -4.3251247 -4.3175516 -4.3104534 -4.2969065 -4.2872629 -4.2746944 -4.2622819 -4.2601271 -4.2662916 -4.2765265 -4.285078 -4.28932 -4.2869244 -4.2772703][-4.3189821 -4.3166962 -4.3044825 -4.2896733 -4.2650609 -4.2386112 -4.2077618 -4.1905222 -4.1980624 -4.2161303 -4.2382436 -4.2559266 -4.2650566 -4.2617278 -4.247324][-4.305357 -4.2939181 -4.2701039 -4.2392564 -4.1928992 -4.141562 -4.0934434 -4.0823574 -4.1089907 -4.1477747 -4.1910214 -4.2245026 -4.2443733 -4.2451205 -4.2331405][-4.2754259 -4.2426119 -4.1994486 -4.15 -4.0848417 -4.0228953 -3.9848652 -4.0049329 -4.0618372 -4.1261568 -4.18723 -4.2303891 -4.2536879 -4.2539692 -4.24288][-4.236712 -4.18065 -4.1239619 -4.0711203 -4.0114889 -3.9736488 -3.9805315 -4.0364294 -4.1080589 -4.17203 -4.224916 -4.25855 -4.272048 -4.2649803 -4.2500339][-4.2210789 -4.159049 -4.1071949 -4.0709634 -4.035203 -4.0283203 -4.0633321 -4.1238604 -4.1824927 -4.2245312 -4.2539792 -4.2681584 -4.2670007 -4.2543297 -4.2370992][-4.2426867 -4.1912889 -4.1518831 -4.1282411 -4.1080389 -4.1129022 -4.14829 -4.194788 -4.2332959 -4.25546 -4.2667971 -4.2656193 -4.2581582 -4.244946 -4.2294025][-4.279377 -4.2411504 -4.2114806 -4.1930361 -4.1826992 -4.1927834 -4.219502 -4.2481632 -4.2686343 -4.2787004 -4.2820044 -4.2777286 -4.269619 -4.2583714 -4.2451673][-4.3152289 -4.2868557 -4.2635531 -4.2486696 -4.2451015 -4.2564464 -4.2769184 -4.2958703 -4.307785 -4.3127236 -4.3124018 -4.3058238 -4.2947974 -4.2812581 -4.2680097][-4.3425393 -4.3214641 -4.3030734 -4.2922411 -4.2924352 -4.3030887 -4.3194695 -4.3348918 -4.3444819 -4.3480139 -4.3460722 -4.3385983 -4.3250504 -4.3100286 -4.2976575][-4.3555236 -4.3409066 -4.329174 -4.3237987 -4.3270521 -4.3353505 -4.3472061 -4.3582735 -4.3643036 -4.3645616 -4.3600354 -4.3522468 -4.3405275 -4.328249 -4.3199553][-4.3522363 -4.3444781 -4.3392968 -4.338418 -4.3412466 -4.34546 -4.3511906 -4.3567781 -4.3600297 -4.3602076 -4.3569846 -4.3514528 -4.3443727 -4.3382258 -4.3352284][-4.3444862 -4.3416643 -4.3394895 -4.3387775 -4.3387804 -4.3393574 -4.3410997 -4.343791 -4.3463798 -4.3485374 -4.3490305 -4.3475075 -4.3449945 -4.3429031 -4.3423758]]...]
INFO - root - 2017-12-07 22:03:15.177557: step 58810, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 49h:51m:53s remains)
INFO - root - 2017-12-07 22:03:21.929837: step 58820, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 48h:15m:07s remains)
INFO - root - 2017-12-07 22:03:28.663286: step 58830, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 50h:20m:57s remains)
INFO - root - 2017-12-07 22:03:35.424115: step 58840, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 53h:31m:29s remains)
INFO - root - 2017-12-07 22:03:42.231350: step 58850, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 51h:45m:01s remains)
INFO - root - 2017-12-07 22:03:48.882612: step 58860, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 46h:41m:35s remains)
INFO - root - 2017-12-07 22:03:55.460412: step 58870, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 48h:36m:49s remains)
INFO - root - 2017-12-07 22:04:02.197663: step 58880, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 54h:40m:31s remains)
INFO - root - 2017-12-07 22:04:08.925401: step 58890, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 54h:12m:57s remains)
INFO - root - 2017-12-07 22:04:15.521309: step 58900, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 51h:51m:20s remains)
2017-12-07 22:04:16.276311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3335772 -4.3283863 -4.3236623 -4.3238387 -4.3200154 -4.3050389 -4.2819295 -4.2511 -4.2237988 -4.2088385 -4.2075448 -4.2199163 -4.2445288 -4.2683597 -4.2887487][-4.3364682 -4.3309684 -4.3270521 -4.32313 -4.3064075 -4.2730184 -4.2298021 -4.1830926 -4.1453466 -4.1267805 -4.12526 -4.1419077 -4.1760063 -4.2080231 -4.2394171][-4.3383932 -4.3305659 -4.3252244 -4.3144522 -4.2819266 -4.2277975 -4.1648808 -4.1022015 -4.0576429 -4.0405903 -4.0473852 -4.0726428 -4.1113415 -4.1461306 -4.1833196][-4.3402648 -4.3273392 -4.3148475 -4.2903094 -4.238915 -4.1651731 -4.0848489 -4.0124092 -3.9720621 -3.9710343 -3.9976394 -4.0330253 -4.0716906 -4.1027145 -4.1399198][-4.3403988 -4.3201876 -4.2980232 -4.2579408 -4.187634 -4.0964413 -3.9988129 -3.9263105 -3.9071951 -3.9375849 -3.9892681 -4.0308113 -4.0621114 -4.0836921 -4.1179814][-4.3408575 -4.3158274 -4.2840981 -4.2304335 -4.1448369 -4.0383749 -3.9308057 -3.8672857 -3.878099 -3.9441535 -4.0176091 -4.0575848 -4.0728388 -4.082665 -4.1161284][-4.3401647 -4.3124652 -4.27436 -4.2095985 -4.1175513 -4.009994 -3.9053853 -3.8524516 -3.8849752 -3.9733729 -4.0597491 -4.0962763 -4.0989485 -4.1001863 -4.1333508][-4.3421421 -4.3142447 -4.273706 -4.2039661 -4.1157565 -4.0154171 -3.91948 -3.8736341 -3.9114115 -4.0081878 -4.0994148 -4.1347585 -4.1319051 -4.1274605 -4.1568427][-4.3431263 -4.316709 -4.2785983 -4.2082329 -4.1242318 -4.0288281 -3.9348483 -3.8888664 -3.9311366 -4.0354428 -4.1276627 -4.1604691 -4.1530614 -4.1467705 -4.1718092][-4.3369145 -4.3142905 -4.2817707 -4.2152095 -4.1364527 -4.0460958 -3.9469628 -3.9000282 -3.9516838 -4.0590749 -4.1437306 -4.1697803 -4.1586566 -4.1510139 -4.1706095][-4.3267736 -4.3069196 -4.2780371 -4.2184305 -4.1491852 -4.0690188 -3.9735348 -3.9287641 -3.9823811 -4.0822034 -4.1556973 -4.1782584 -4.1648431 -4.1524467 -4.1620584][-4.3201132 -4.30128 -4.27358 -4.2220511 -4.1604109 -4.09628 -4.0216131 -3.989722 -4.0402651 -4.1238 -4.1833806 -4.2020254 -4.1865292 -4.1653643 -4.1622186][-4.3188267 -4.3009944 -4.2749357 -4.2325716 -4.1809778 -4.1337204 -4.0874834 -4.07557 -4.1191015 -4.183445 -4.2263718 -4.2375908 -4.2175813 -4.1905766 -4.1813135][-4.3236265 -4.3071628 -4.2838812 -4.2506218 -4.21248 -4.1840067 -4.1671233 -4.1717086 -4.2053971 -4.2484131 -4.2728162 -4.2728062 -4.2502489 -4.2246885 -4.21842][-4.3358316 -4.3224874 -4.3027239 -4.2770586 -4.2502604 -4.2367268 -4.2394629 -4.2527127 -4.2762718 -4.3016148 -4.310204 -4.3031549 -4.283514 -4.265398 -4.26419]]...]
INFO - root - 2017-12-07 22:04:23.044722: step 58910, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 55h:16m:25s remains)
INFO - root - 2017-12-07 22:04:29.820529: step 58920, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 53h:58m:14s remains)
INFO - root - 2017-12-07 22:04:36.600790: step 58930, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 50h:55m:24s remains)
INFO - root - 2017-12-07 22:04:43.380056: step 58940, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 47h:58m:38s remains)
INFO - root - 2017-12-07 22:04:50.241315: step 58950, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 52h:02m:34s remains)
INFO - root - 2017-12-07 22:04:57.053330: step 58960, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 56h:41m:51s remains)
INFO - root - 2017-12-07 22:05:03.855864: step 58970, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 52h:33m:24s remains)
INFO - root - 2017-12-07 22:05:10.702779: step 58980, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 49h:39m:12s remains)
INFO - root - 2017-12-07 22:05:17.507472: step 58990, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 49h:05m:29s remains)
INFO - root - 2017-12-07 22:05:24.281039: step 59000, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 51h:50m:47s remains)
2017-12-07 22:05:25.099508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2425294 -4.2298145 -4.2269368 -4.2208772 -4.2077594 -4.1972265 -4.1941528 -4.1918297 -4.1862121 -4.1824374 -4.180233 -4.1708512 -4.1638041 -4.1571426 -4.1536012][-4.2401285 -4.224452 -4.220736 -4.2171006 -4.2079468 -4.1979146 -4.195715 -4.1937633 -4.1856904 -4.1815104 -4.1822691 -4.1767292 -4.1751418 -4.1616526 -4.1428061][-4.2392693 -4.2218719 -4.2184796 -4.2184925 -4.2138147 -4.20612 -4.1994948 -4.1897478 -4.1736021 -4.1620884 -4.15622 -4.15394 -4.1630988 -4.1501918 -4.1239018][-4.2375875 -4.2174916 -4.2146535 -4.2214293 -4.2262254 -4.2245774 -4.2131476 -4.1895814 -4.1589088 -4.1354022 -4.121521 -4.1217661 -4.1423111 -4.1372695 -4.1138706][-4.2336264 -4.2098064 -4.20711 -4.21833 -4.2340031 -4.2406211 -4.226562 -4.1909065 -4.1461554 -4.1129 -4.0987735 -4.1092505 -4.1401677 -4.1429124 -4.125524][-4.22626 -4.2015705 -4.1974988 -4.2105637 -4.2337542 -4.2460938 -4.2308574 -4.1886234 -4.1377273 -4.1001024 -4.0883656 -4.105525 -4.1391716 -4.151638 -4.1473289][-4.2197018 -4.1912827 -4.1813221 -4.193779 -4.2206612 -4.2357736 -4.2185926 -4.173203 -4.1261673 -4.0933132 -4.0838633 -4.1006107 -4.1294136 -4.1492562 -4.1569929][-4.22012 -4.1830368 -4.1640487 -4.174171 -4.2033148 -4.22101 -4.2057648 -4.1618652 -4.1203661 -4.0950432 -4.0876 -4.1003628 -4.1229162 -4.1473203 -4.1647105][-4.2300639 -4.1864777 -4.1599793 -4.1646881 -4.19229 -4.2140522 -4.2048874 -4.1711683 -4.141686 -4.1288662 -4.12756 -4.137609 -4.1539106 -4.1747842 -4.1919408][-4.2470551 -4.2029228 -4.1734347 -4.1723032 -4.1924849 -4.2160363 -4.2175651 -4.1962733 -4.1777229 -4.1756377 -4.1773405 -4.1823192 -4.1899347 -4.2011886 -4.2109261][-4.2652359 -4.2246552 -4.1923022 -4.1841235 -4.1961269 -4.2201557 -4.2323895 -4.2197871 -4.2058339 -4.2021976 -4.199718 -4.198009 -4.19754 -4.2030272 -4.2116704][-4.2760382 -4.2407379 -4.2053175 -4.1838846 -4.1823444 -4.2042489 -4.2258124 -4.2228975 -4.2126651 -4.2052259 -4.1963315 -4.1861796 -4.1792235 -4.1841359 -4.1984663][-4.2751718 -4.253356 -4.2154326 -4.1785231 -4.1587391 -4.1659842 -4.1866951 -4.1975913 -4.1983542 -4.1954823 -4.1845174 -4.1678443 -4.1563187 -4.1611891 -4.1770835][-4.2674131 -4.2658076 -4.2312884 -4.1812491 -4.143065 -4.1293669 -4.1390381 -4.1595907 -4.17494 -4.1817842 -4.1769071 -4.1656961 -4.1583452 -4.1646504 -4.1787939][-4.2554588 -4.2741156 -4.2485714 -4.199194 -4.1537471 -4.1276503 -4.1278195 -4.1483064 -4.1724086 -4.1871548 -4.1866312 -4.1800189 -4.1770854 -4.1831803 -4.1929979]]...]
INFO - root - 2017-12-07 22:05:31.814672: step 59010, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 50h:44m:36s remains)
INFO - root - 2017-12-07 22:05:38.522488: step 59020, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 47h:38m:12s remains)
INFO - root - 2017-12-07 22:05:45.415842: step 59030, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.711 sec/batch; 54h:02m:32s remains)
INFO - root - 2017-12-07 22:05:52.332994: step 59040, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 55h:01m:46s remains)
INFO - root - 2017-12-07 22:05:59.103467: step 59050, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 51h:03m:45s remains)
INFO - root - 2017-12-07 22:06:05.844360: step 59060, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 47h:10m:27s remains)
INFO - root - 2017-12-07 22:06:12.602345: step 59070, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 49h:40m:44s remains)
INFO - root - 2017-12-07 22:06:19.339897: step 59080, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 52h:55m:30s remains)
INFO - root - 2017-12-07 22:06:26.105149: step 59090, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 54h:43m:46s remains)
INFO - root - 2017-12-07 22:06:32.656057: step 59100, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 49h:48m:11s remains)
2017-12-07 22:06:33.358481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32782 -4.3287096 -4.3255639 -4.3199177 -4.3136311 -4.3086967 -4.30756 -4.3094144 -4.31334 -4.318131 -4.3218427 -4.3238416 -4.3243713 -4.3243828 -4.325716][-4.3315105 -4.3312745 -4.3272243 -4.31822 -4.3040509 -4.290987 -4.28448 -4.2854786 -4.2935905 -4.30483 -4.3154559 -4.3233566 -4.3268337 -4.3268352 -4.3268852][-4.3352642 -4.3335953 -4.3255577 -4.3061967 -4.2794709 -4.2568913 -4.2426543 -4.2419615 -4.2558713 -4.2754483 -4.2956691 -4.313549 -4.323597 -4.3267808 -4.3276005][-4.330915 -4.325304 -4.3060083 -4.2695971 -4.2248926 -4.185648 -4.1587353 -4.15968 -4.1890249 -4.2270155 -4.2632985 -4.2941513 -4.3121715 -4.3210626 -4.3256364][-4.3131509 -4.3034892 -4.2713351 -4.2129025 -4.1398754 -4.0661955 -4.0137396 -4.0186706 -4.0775847 -4.1468954 -4.2089682 -4.2576036 -4.2884879 -4.3071361 -4.3191714][-4.2892623 -4.2778859 -4.2383065 -4.1622424 -4.0590825 -3.9364862 -3.8406687 -3.8538914 -3.9563179 -4.0616431 -4.1474562 -4.2117682 -4.2562518 -4.2871027 -4.3091774][-4.2685423 -4.2627759 -4.2229228 -4.1445727 -4.0311122 -3.8795149 -3.7567229 -3.7824132 -3.9081197 -4.0259333 -4.1207004 -4.1921134 -4.2441454 -4.2800541 -4.3042793][-4.2570553 -4.2616334 -4.2327065 -4.1730051 -4.0828557 -3.9637072 -3.8743234 -3.8955638 -3.9868908 -4.0752358 -4.1539383 -4.2164626 -4.2612514 -4.2895422 -4.3051171][-4.2559857 -4.2694783 -4.2582316 -4.2267904 -4.1753407 -4.1085668 -4.0622721 -4.0706267 -4.1149416 -4.1636658 -4.2146091 -4.2602234 -4.2903452 -4.3030405 -4.30519][-4.2696748 -4.2860565 -4.28769 -4.2767687 -4.2525682 -4.218492 -4.1934586 -4.1900325 -4.2054472 -4.2297387 -4.2595 -4.2888107 -4.3073254 -4.3081837 -4.3013358][-4.2911768 -4.3045897 -4.3108759 -4.3085752 -4.2961264 -4.2773008 -4.2572122 -4.2442746 -4.24435 -4.2549329 -4.271317 -4.290411 -4.302474 -4.3000245 -4.2904072][-4.3002825 -4.3133445 -4.3212738 -4.3202887 -4.3112345 -4.2983031 -4.2798028 -4.2619987 -4.2523713 -4.2525725 -4.2596922 -4.2718511 -4.2819085 -4.2813096 -4.2743874][-4.2909522 -4.3055925 -4.3139596 -4.3127909 -4.3068433 -4.2975736 -4.2800035 -4.258358 -4.2434807 -4.2385378 -4.2413859 -4.2511177 -4.2599516 -4.2622271 -4.2607388][-4.275672 -4.2944188 -4.3018055 -4.297049 -4.2893414 -4.2802544 -4.263164 -4.2422819 -4.2282052 -4.2238092 -4.226337 -4.233614 -4.2410297 -4.2454 -4.2481289][-4.2669196 -4.2865539 -4.2916408 -4.2827396 -4.2706318 -4.2598419 -4.2473097 -4.2359147 -4.22921 -4.2272 -4.2266431 -4.2273664 -4.23005 -4.2345486 -4.2420454]]...]
INFO - root - 2017-12-07 22:06:40.168760: step 59110, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 55h:04m:21s remains)
INFO - root - 2017-12-07 22:06:46.998978: step 59120, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.731 sec/batch; 55h:31m:05s remains)
INFO - root - 2017-12-07 22:06:53.771813: step 59130, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 50h:25m:19s remains)
INFO - root - 2017-12-07 22:07:00.537068: step 59140, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 47h:48m:03s remains)
INFO - root - 2017-12-07 22:07:07.372251: step 59150, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 50h:27m:20s remains)
INFO - root - 2017-12-07 22:07:14.165130: step 59160, loss = 2.11, batch loss = 2.05 (10.9 examples/sec; 0.736 sec/batch; 55h:51m:38s remains)
INFO - root - 2017-12-07 22:07:20.902743: step 59170, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 54h:58m:40s remains)
INFO - root - 2017-12-07 22:07:27.453779: step 59180, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 48h:06m:43s remains)
INFO - root - 2017-12-07 22:07:34.262457: step 59190, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 50h:15m:00s remains)
INFO - root - 2017-12-07 22:07:40.893834: step 59200, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 54h:24m:00s remains)
2017-12-07 22:07:41.657814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2564359 -4.2346869 -4.2185068 -4.2012844 -4.1749115 -4.1606731 -4.1720061 -4.1934376 -4.2112923 -4.2265081 -4.2359734 -4.2354774 -4.2226944 -4.2096739 -4.1943936][-4.2507544 -4.2312484 -4.2209864 -4.2082376 -4.1817484 -4.1647477 -4.1714931 -4.184567 -4.1974344 -4.2146864 -4.227128 -4.2283058 -4.2172384 -4.2034211 -4.1798248][-4.2494311 -4.2410383 -4.2413712 -4.2340574 -4.2067986 -4.1836314 -4.1766081 -4.1735592 -4.1823869 -4.2053795 -4.223341 -4.2301593 -4.2242446 -4.2102513 -4.1792736][-4.249712 -4.2578073 -4.2676597 -4.2620883 -4.2312126 -4.1985955 -4.1744418 -4.1542654 -4.1617432 -4.1936183 -4.2189679 -4.2322631 -4.2329631 -4.2209144 -4.1885586][-4.2460322 -4.2689252 -4.2847657 -4.2763853 -4.2388535 -4.1921387 -4.1477127 -4.111577 -4.1223826 -4.1681395 -4.2048244 -4.2278757 -4.2364516 -4.2289391 -4.200727][-4.2315993 -4.2607284 -4.2777276 -4.2644715 -4.2185163 -4.1570716 -4.0924082 -4.0445051 -4.0652132 -4.1291723 -4.1797881 -4.2128019 -4.2295871 -4.2280607 -4.2076745][-4.2146983 -4.2421551 -4.2539983 -4.2318311 -4.1760468 -4.1026983 -4.0243106 -3.9740081 -4.0132551 -4.10029 -4.1672754 -4.2107873 -4.2321649 -4.2350955 -4.2209167][-4.2099938 -4.2307882 -4.2346678 -4.2030468 -4.1408124 -4.0651364 -3.9887047 -3.9487698 -4.0041432 -4.1020422 -4.1769557 -4.2243948 -4.2453742 -4.2488675 -4.2371125][-4.2230306 -4.2372475 -4.2334542 -4.1984944 -4.1400461 -4.0807486 -4.030673 -4.0151095 -4.0684352 -4.1475139 -4.2062578 -4.2426057 -4.2577848 -4.258513 -4.2483749][-4.240798 -4.2512321 -4.245985 -4.2160935 -4.1697893 -4.1301875 -4.1060476 -4.1052094 -4.1428337 -4.1920962 -4.2260556 -4.2468858 -4.2559748 -4.2564592 -4.2485657][-4.2553582 -4.2684708 -4.2681937 -4.2477813 -4.2118392 -4.182858 -4.1713448 -4.1728191 -4.19141 -4.213089 -4.2258744 -4.2337341 -4.2380943 -4.2384815 -4.2310033][-4.2741609 -4.2887173 -4.2915039 -4.2775741 -4.2497969 -4.2263451 -4.2148366 -4.2083759 -4.2086987 -4.2094355 -4.2073812 -4.2074523 -4.2092409 -4.2099209 -4.2033186][-4.2946072 -4.3036704 -4.30296 -4.290441 -4.2668214 -4.2432427 -4.2240624 -4.2054586 -4.1900039 -4.175375 -4.1647253 -4.1628613 -4.166862 -4.1734157 -4.17406][-4.3073606 -4.3069844 -4.297998 -4.2805119 -4.2550073 -4.2292933 -4.2035131 -4.176929 -4.1533456 -4.1314807 -4.118084 -4.1207685 -4.1325092 -4.148603 -4.1587524][-4.3042846 -4.2956886 -4.28008 -4.2573781 -4.228693 -4.2005253 -4.1721129 -4.1445594 -4.1225 -4.1042356 -4.0975142 -4.107502 -4.1267815 -4.1476145 -4.1623025]]...]
INFO - root - 2017-12-07 22:07:48.416645: step 59210, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 47h:44m:56s remains)
INFO - root - 2017-12-07 22:07:55.334790: step 59220, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 52h:22m:39s remains)
INFO - root - 2017-12-07 22:08:02.219802: step 59230, loss = 2.07, batch loss = 2.02 (10.6 examples/sec; 0.752 sec/batch; 57h:02m:48s remains)
INFO - root - 2017-12-07 22:08:09.093615: step 59240, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 51h:38m:35s remains)
INFO - root - 2017-12-07 22:08:15.855061: step 59250, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 49h:39m:13s remains)
INFO - root - 2017-12-07 22:08:22.671749: step 59260, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 48h:21m:24s remains)
INFO - root - 2017-12-07 22:08:29.534553: step 59270, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.736 sec/batch; 55h:51m:06s remains)
INFO - root - 2017-12-07 22:08:36.392742: step 59280, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 54h:31m:10s remains)
INFO - root - 2017-12-07 22:08:43.182667: step 59290, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 51h:49m:16s remains)
INFO - root - 2017-12-07 22:08:49.791696: step 59300, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 49h:32m:13s remains)
2017-12-07 22:08:50.592782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2039151 -4.1747069 -4.1566892 -4.1624527 -4.1918039 -4.2334533 -4.2656426 -4.2831206 -4.2814851 -4.2639813 -4.2505403 -4.2455292 -4.2404065 -4.2380815 -4.2439842][-4.2059035 -4.1864476 -4.1769924 -4.1868243 -4.2081895 -4.2309666 -4.2454643 -4.2421985 -4.2220087 -4.1934114 -4.1753278 -4.1708689 -4.1696672 -4.1776938 -4.1984458][-4.2307177 -4.2185187 -4.2117581 -4.213923 -4.2132034 -4.2057848 -4.1920266 -4.1627784 -4.1269031 -4.096663 -4.083519 -4.0906968 -4.1082182 -4.1369581 -4.1755719][-4.2546968 -4.2447705 -4.2322483 -4.2173681 -4.1883769 -4.1438351 -4.0930848 -4.0409617 -4.0035586 -3.9855869 -3.9897809 -4.0214171 -4.0689058 -4.1247964 -4.17969][-4.262713 -4.2504044 -4.2288065 -4.1912904 -4.1269417 -4.0355043 -3.9429846 -3.8815117 -3.8741016 -3.8951421 -3.931349 -3.9912491 -4.0667624 -4.1425529 -4.2029824][-4.2487426 -4.2286091 -4.1923714 -4.127079 -4.0248995 -3.8844903 -3.74994 -3.7044859 -3.7685056 -3.860414 -3.9423661 -4.0232167 -4.1055713 -4.1782827 -4.2252893][-4.2142792 -4.1814895 -4.1286263 -4.0448551 -3.9247472 -3.7543828 -3.5954614 -3.592813 -3.7486591 -3.9081535 -4.020093 -4.104692 -4.1734533 -4.2200427 -4.2383242][-4.1625047 -4.1194835 -4.0626621 -3.9892888 -3.8947759 -3.7521458 -3.630424 -3.6723013 -3.8456798 -4.0047641 -4.1102109 -4.18587 -4.2361426 -4.2536125 -4.2463331][-4.1020179 -4.0616159 -4.022944 -3.9893041 -3.9476576 -3.8696902 -3.8114526 -3.8562427 -3.9814312 -4.0978956 -4.1794176 -4.2400045 -4.2718759 -4.270412 -4.2507668][-4.0649996 -4.0370531 -4.0253835 -4.0333958 -4.0373793 -4.0128489 -3.9962904 -4.0311427 -4.1068044 -4.1788883 -4.2341423 -4.2779803 -4.2966075 -4.2848449 -4.2604036][-4.0796838 -4.0664663 -4.0736861 -4.1070719 -4.1379757 -4.14549 -4.1475248 -4.1698108 -4.2110233 -4.2501192 -4.283741 -4.3103452 -4.3145332 -4.294755 -4.2678571][-4.1338234 -4.1302819 -4.1448736 -4.1845574 -4.2230363 -4.2426639 -4.2510371 -4.2657824 -4.2856851 -4.3048062 -4.3230553 -4.3322163 -4.3221769 -4.2970304 -4.2665458][-4.2043939 -4.2016091 -4.2113171 -4.2427855 -4.2767768 -4.2998066 -4.3103881 -4.3203096 -4.3278413 -4.3340769 -4.3362889 -4.323791 -4.2979684 -4.2687678 -4.2379889][-4.25435 -4.250268 -4.2535291 -4.2717962 -4.2964616 -4.317956 -4.3294687 -4.3365245 -4.3364305 -4.3316326 -4.3189631 -4.2869844 -4.2472644 -4.2143617 -4.185945][-4.2799783 -4.2765379 -4.274601 -4.2820435 -4.2966738 -4.3110218 -4.3170934 -4.31844 -4.3127241 -4.3014321 -4.2780542 -4.23732 -4.1932197 -4.1608448 -4.1398616]]...]
INFO - root - 2017-12-07 22:08:57.540138: step 59310, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 53h:14m:23s remains)
INFO - root - 2017-12-07 22:09:04.211951: step 59320, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 47h:42m:20s remains)
INFO - root - 2017-12-07 22:09:10.928780: step 59330, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 46h:56m:19s remains)
INFO - root - 2017-12-07 22:09:17.863945: step 59340, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 52h:09m:27s remains)
INFO - root - 2017-12-07 22:09:24.691729: step 59350, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 55h:40m:42s remains)
INFO - root - 2017-12-07 22:09:31.594153: step 59360, loss = 2.09, batch loss = 2.04 (11.1 examples/sec; 0.724 sec/batch; 54h:55m:41s remains)
INFO - root - 2017-12-07 22:09:38.444098: step 59370, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 51h:26m:42s remains)
INFO - root - 2017-12-07 22:09:45.154757: step 59380, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.607 sec/batch; 46h:04m:46s remains)
INFO - root - 2017-12-07 22:09:52.018312: step 59390, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 49h:48m:32s remains)
INFO - root - 2017-12-07 22:09:58.806752: step 59400, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 56h:38m:54s remains)
2017-12-07 22:09:59.544815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2538009 -4.2429605 -4.25585 -4.2764707 -4.2790508 -4.2630444 -4.244287 -4.23022 -4.2158566 -4.2090378 -4.2132363 -4.2143965 -4.2058349 -4.198391 -4.2061067][-4.2471695 -4.2366991 -4.2532721 -4.2737255 -4.2709479 -4.2550545 -4.2365732 -4.2167721 -4.1965728 -4.1933794 -4.2070885 -4.2118311 -4.1975241 -4.1772704 -4.1779733][-4.2386422 -4.2313876 -4.250843 -4.2687273 -4.2612472 -4.2464843 -4.2246013 -4.1919594 -4.16135 -4.1666842 -4.1965809 -4.20974 -4.191258 -4.1631937 -4.1613832][-4.2303786 -4.2292986 -4.2537551 -4.2733889 -4.2634206 -4.2455106 -4.21155 -4.158164 -4.1176238 -4.1395097 -4.1884446 -4.2040081 -4.182497 -4.1532321 -4.1535811][-4.2241745 -4.232305 -4.2619319 -4.2817564 -4.2669773 -4.2382565 -4.1797872 -4.1016273 -4.0694566 -4.1245813 -4.1869178 -4.1996784 -4.1737428 -4.1455154 -4.1508961][-4.2192369 -4.229166 -4.2566361 -4.269701 -4.2527423 -4.2102013 -4.119803 -4.0157251 -4.0146027 -4.1158762 -4.1835403 -4.1873722 -4.157608 -4.1334739 -4.1460266][-4.2125344 -4.2132874 -4.228055 -4.2327638 -4.2162962 -4.1568689 -4.033 -3.9096563 -3.9536772 -4.0937848 -4.1617975 -4.159533 -4.1303153 -4.1122 -4.1341734][-4.2035017 -4.1971803 -4.2014084 -4.2013516 -4.1866384 -4.1151733 -3.9749122 -3.8647151 -3.9497409 -4.0889812 -4.1381426 -4.1267443 -4.0983806 -4.0885944 -4.1248608][-4.1991391 -4.1870189 -4.1830788 -4.1798186 -4.164639 -4.0893235 -3.965977 -3.9127264 -4.0160389 -4.1119003 -4.1247234 -4.1009493 -4.0756221 -4.0769506 -4.1268291][-4.1993127 -4.1825171 -4.1711473 -4.16348 -4.1438112 -4.0662308 -3.9683478 -3.9688447 -4.0712428 -4.1262069 -4.11479 -4.0883536 -4.0720367 -4.0848403 -4.1395421][-4.2121043 -4.1916337 -4.1760836 -4.1649938 -4.14186 -4.0699339 -3.999692 -4.0313807 -4.1179514 -4.1472621 -4.1290731 -4.1101441 -4.1028914 -4.1205578 -4.1710091][-4.2532449 -4.2342281 -4.2195597 -4.2070603 -4.1813712 -4.1227088 -4.07928 -4.1155949 -4.1725311 -4.1857595 -4.1741943 -4.1695771 -4.1723194 -4.1889915 -4.22851][-4.3009524 -4.2863173 -4.2737722 -4.2619815 -4.23909 -4.1979613 -4.1746054 -4.1996312 -4.2313457 -4.2380733 -4.2371235 -4.2459044 -4.2551227 -4.2669692 -4.2920384][-4.3413744 -4.334558 -4.326426 -4.3145485 -4.2968392 -4.2720351 -4.2607517 -4.2742772 -4.290103 -4.2935958 -4.2990026 -4.3129745 -4.3228273 -4.3280082 -4.3398585][-4.361299 -4.3604531 -4.3566856 -4.3491445 -4.3384466 -4.325099 -4.3194971 -4.3268418 -4.3351536 -4.3360357 -4.34227 -4.354321 -4.3594537 -4.358531 -4.3623314]]...]
INFO - root - 2017-12-07 22:10:06.352149: step 59410, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.679 sec/batch; 51h:28m:30s remains)
INFO - root - 2017-12-07 22:10:13.163390: step 59420, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 53h:19m:58s remains)
INFO - root - 2017-12-07 22:10:20.065163: step 59430, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 53h:31m:16s remains)
INFO - root - 2017-12-07 22:10:26.815954: step 59440, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 54h:01m:58s remains)
INFO - root - 2017-12-07 22:10:33.662110: step 59450, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 51h:08m:42s remains)
INFO - root - 2017-12-07 22:10:40.519675: step 59460, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 49h:35m:13s remains)
INFO - root - 2017-12-07 22:10:47.430873: step 59470, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 52h:38m:25s remains)
INFO - root - 2017-12-07 22:10:54.223164: step 59480, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.702 sec/batch; 53h:12m:44s remains)
INFO - root - 2017-12-07 22:11:00.823436: step 59490, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 47h:19m:52s remains)
INFO - root - 2017-12-07 22:11:07.361725: step 59500, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 49h:09m:07s remains)
2017-12-07 22:11:08.126046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2487931 -4.2308478 -4.2181053 -4.2055936 -4.1912413 -4.1783948 -4.171771 -4.1868596 -4.2037573 -4.2182708 -4.2348261 -4.2458911 -4.2514434 -4.2538996 -4.2629333][-4.2103992 -4.1902828 -4.1750221 -4.1608186 -4.1485276 -4.1393704 -4.1359625 -4.1605024 -4.1920075 -4.2171283 -4.2435179 -4.2592354 -4.2650166 -4.2698345 -4.2783985][-4.1894455 -4.1709 -4.1538968 -4.1381245 -4.1234956 -4.1127892 -4.1087937 -4.1366539 -4.1768513 -4.2090316 -4.2415018 -4.2609677 -4.269876 -4.2801256 -4.2930131][-4.1660128 -4.1522484 -4.1388316 -4.1238756 -4.1099792 -4.1012077 -4.0964589 -4.1168184 -4.1523104 -4.1861491 -4.2247682 -4.2512693 -4.2662439 -4.2848907 -4.3044777][-4.1389647 -4.1336889 -4.12638 -4.1180525 -4.1125569 -4.1119413 -4.1119022 -4.1214132 -4.1453996 -4.1746025 -4.2131171 -4.2442279 -4.2665615 -4.2918682 -4.3139086][-4.1208944 -4.1292257 -4.1365004 -4.1444144 -4.1521378 -4.15712 -4.1554933 -4.1532164 -4.1644058 -4.18531 -4.2164679 -4.2468319 -4.2742448 -4.3021059 -4.3224788][-4.1448636 -4.1708641 -4.1885066 -4.1996322 -4.2071486 -4.2051215 -4.1916924 -4.1758509 -4.1754909 -4.1893411 -4.2156773 -4.2458353 -4.2780361 -4.3076353 -4.3267565][-4.1982021 -4.2276607 -4.236979 -4.2337627 -4.2269826 -4.208858 -4.1793232 -4.1514297 -4.148416 -4.1670127 -4.2005391 -4.2399526 -4.2795014 -4.3107533 -4.3283896][-4.2172809 -4.2393765 -4.2352295 -4.21632 -4.1949487 -4.164649 -4.1246829 -4.0934596 -4.0966969 -4.1274996 -4.1772819 -4.2310281 -4.2787805 -4.31175 -4.3288455][-4.1874032 -4.1993475 -4.1832147 -4.1557522 -4.1279817 -4.0957108 -4.057878 -4.0340271 -4.05013 -4.094727 -4.1580944 -4.2222185 -4.2757845 -4.3107576 -4.3290596][-4.1500225 -4.1500249 -4.1237268 -4.093164 -4.0686765 -4.0435271 -4.0143728 -4.0019193 -4.031383 -4.0850987 -4.1541972 -4.21999 -4.2749348 -4.3118787 -4.3300214][-4.1447592 -4.138813 -4.1104603 -4.0814791 -4.059587 -4.0390277 -4.0155249 -4.0108204 -4.0467811 -4.1007748 -4.1662159 -4.226 -4.2759304 -4.3104944 -4.3277764][-4.1671591 -4.1618652 -4.138164 -4.1149197 -4.0974383 -4.0793023 -4.0591211 -4.0597987 -4.096076 -4.1431203 -4.1947284 -4.2389846 -4.2770877 -4.3053288 -4.321002][-4.208673 -4.2034397 -4.1836896 -4.168355 -4.157629 -4.1441178 -4.1305127 -4.1363497 -4.1675854 -4.1998816 -4.2315097 -4.255775 -4.2788 -4.2988253 -4.3121386][-4.244493 -4.2407975 -4.2292542 -4.2226634 -4.2195091 -4.212337 -4.2059269 -4.2120962 -4.2318773 -4.2469249 -4.257977 -4.2637668 -4.2751579 -4.2898808 -4.3034229]]...]
INFO - root - 2017-12-07 22:11:15.039254: step 59510, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.681 sec/batch; 51h:38m:58s remains)
INFO - root - 2017-12-07 22:11:21.755707: step 59520, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 48h:31m:08s remains)
INFO - root - 2017-12-07 22:11:28.598371: step 59530, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 49h:05m:49s remains)
INFO - root - 2017-12-07 22:11:35.401184: step 59540, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 55h:42m:37s remains)
INFO - root - 2017-12-07 22:11:42.206384: step 59550, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 51h:56m:10s remains)
INFO - root - 2017-12-07 22:11:49.008937: step 59560, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 53h:23m:42s remains)
INFO - root - 2017-12-07 22:11:55.892854: step 59570, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 48h:08m:27s remains)
INFO - root - 2017-12-07 22:12:02.721345: step 59580, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.638 sec/batch; 48h:21m:57s remains)
INFO - root - 2017-12-07 22:12:09.475411: step 59590, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 53h:31m:29s remains)
INFO - root - 2017-12-07 22:12:16.119550: step 59600, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 52h:30m:37s remains)
2017-12-07 22:12:16.864446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3188539 -4.3175249 -4.3128705 -4.3081837 -4.3062229 -4.3077583 -4.3085995 -4.3076377 -4.3098969 -4.3108325 -4.3080039 -4.3039184 -4.3006959 -4.3006968 -4.3043818][-4.3050871 -4.299542 -4.2920203 -4.2838912 -4.2826538 -4.2886777 -4.2917714 -4.2927322 -4.29902 -4.3009109 -4.2944484 -4.2836323 -4.2753572 -4.2727284 -4.2784114][-4.2740822 -4.2620707 -4.2543817 -4.2465086 -4.2481279 -4.2601614 -4.2660975 -4.2668633 -4.2729344 -4.2743397 -4.2661781 -4.2530127 -4.2442751 -4.2432036 -4.2526951][-4.2316256 -4.2150292 -4.2119255 -4.2130914 -4.2257471 -4.2436996 -4.24682 -4.2420168 -4.2444339 -4.2439928 -4.230926 -4.2155342 -4.2134852 -4.2223339 -4.2383089][-4.1843925 -4.1638651 -4.1712446 -4.1895938 -4.2109203 -4.2259111 -4.2189655 -4.205543 -4.2107344 -4.2122688 -4.19757 -4.1807 -4.1864529 -4.207572 -4.231832][-4.1439762 -4.1186857 -4.1308575 -4.1600966 -4.1843104 -4.1854186 -4.1616335 -4.1384249 -4.1522379 -4.1674414 -4.1608529 -4.1500931 -4.166461 -4.197516 -4.2278261][-4.1244087 -4.0921578 -4.0995841 -4.1238852 -4.1369572 -4.1109805 -4.0492449 -4.0079875 -4.0444112 -4.0945544 -4.1146173 -4.122467 -4.1501207 -4.188705 -4.2218466][-4.1433549 -4.1048303 -4.0942864 -4.0988178 -4.0903387 -4.0347223 -3.9245615 -3.8470955 -3.9150155 -4.01002 -4.0634503 -4.0919633 -4.1297283 -4.17708 -4.2165451][-4.2123108 -4.1700573 -4.1421509 -4.1274786 -4.1060524 -4.0436521 -3.9246831 -3.8408394 -3.9166389 -4.0196776 -4.078753 -4.1079817 -4.1413193 -4.1818824 -4.2191744][-4.2758093 -4.2450395 -4.2195196 -4.1988559 -4.1754761 -4.1272559 -4.0497079 -3.9996781 -4.0484328 -4.1207914 -4.1592336 -4.172339 -4.1858225 -4.2087188 -4.2344294][-4.3016729 -4.2833986 -4.2624264 -4.24243 -4.223208 -4.1903696 -4.1475945 -4.12606 -4.1571078 -4.1987534 -4.2182684 -4.223556 -4.2261796 -4.2386646 -4.2521753][-4.3109245 -4.2982225 -4.2810526 -4.2656522 -4.2509828 -4.2280331 -4.2041769 -4.1968107 -4.2181983 -4.2367349 -4.2418971 -4.2444654 -4.2445908 -4.2552748 -4.2641716][-4.3178983 -4.3038507 -4.2905016 -4.2806187 -4.2714415 -4.2597995 -4.2465062 -4.241868 -4.254817 -4.2651372 -4.2647386 -4.2646394 -4.2641535 -4.2727036 -4.2780724][-4.3252072 -4.3099079 -4.2981596 -4.2914753 -4.2867966 -4.2813048 -4.2727184 -4.2669172 -4.2740979 -4.2816057 -4.2836337 -4.2851715 -4.2858872 -4.2916107 -4.2939944][-4.3254018 -4.3119564 -4.300179 -4.293077 -4.2899213 -4.2872858 -4.2821059 -4.2777715 -4.2823882 -4.2882228 -4.2942052 -4.2992425 -4.3029304 -4.3070583 -4.30919]]...]
INFO - root - 2017-12-07 22:12:23.623369: step 59610, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 47h:51m:15s remains)
INFO - root - 2017-12-07 22:12:30.496253: step 59620, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 52h:35m:33s remains)
INFO - root - 2017-12-07 22:12:37.350418: step 59630, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 56h:13m:18s remains)
INFO - root - 2017-12-07 22:12:44.119128: step 59640, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 48h:20m:38s remains)
INFO - root - 2017-12-07 22:12:50.926613: step 59650, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 48h:01m:02s remains)
INFO - root - 2017-12-07 22:12:57.825186: step 59660, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 49h:34m:41s remains)
INFO - root - 2017-12-07 22:13:04.623426: step 59670, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 55h:16m:59s remains)
INFO - root - 2017-12-07 22:13:11.553276: step 59680, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.755 sec/batch; 57h:14m:34s remains)
INFO - root - 2017-12-07 22:13:18.286458: step 59690, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 49h:50m:35s remains)
INFO - root - 2017-12-07 22:13:24.379485: step 59700, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 47h:51m:55s remains)
2017-12-07 22:13:25.168411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2594075 -4.2299256 -4.2007117 -4.1829715 -4.1821642 -4.1940789 -4.2100625 -4.2154894 -4.2071939 -4.1827946 -4.1575732 -4.1464162 -4.1530728 -4.1558614 -4.1498394][-4.2379541 -4.2124753 -4.1905308 -4.1735311 -4.1652479 -4.1694617 -4.1904054 -4.2114716 -4.2185307 -4.1934614 -4.1601071 -4.13865 -4.1351509 -4.1305447 -4.1234131][-4.2183781 -4.1974745 -4.1801181 -4.1598129 -4.136601 -4.1288567 -4.1551647 -4.1958494 -4.22087 -4.202879 -4.1668539 -4.1371469 -4.1200433 -4.1091328 -4.1042905][-4.2068295 -4.1933136 -4.174345 -4.1422009 -4.1015759 -4.0893612 -4.1248145 -4.1796441 -4.220088 -4.2163863 -4.1843262 -4.145103 -4.1135378 -4.0959249 -4.0963669][-4.2066846 -4.2025151 -4.1809483 -4.1305914 -4.0714083 -4.0605054 -4.1065054 -4.168283 -4.2149124 -4.2269826 -4.2034845 -4.1582232 -4.1155696 -4.0947256 -4.1066422][-4.212914 -4.20805 -4.1795259 -4.1088696 -4.0322638 -4.0238433 -4.08317 -4.153791 -4.20399 -4.2247119 -4.2126074 -4.1694264 -4.1169243 -4.0936346 -4.1210384][-4.2228608 -4.2087646 -4.1691604 -4.0835171 -3.9945717 -3.983448 -4.049283 -4.1288185 -4.1848984 -4.2112403 -4.2087536 -4.1651993 -4.1063085 -4.0872703 -4.1303015][-4.2474475 -4.223526 -4.1750646 -4.0833025 -3.9889181 -3.967097 -4.0231886 -4.106061 -4.1682978 -4.1958833 -4.1918044 -4.1473346 -4.0963645 -4.095901 -4.1531734][-4.2802877 -4.2544475 -4.2064204 -4.1208334 -4.0286431 -3.9927242 -4.0260854 -4.1001148 -4.1591482 -4.182014 -4.1689119 -4.127728 -4.10203 -4.127728 -4.193913][-4.3050661 -4.2842536 -4.246428 -4.17753 -4.095243 -4.0499 -4.0653768 -4.1224804 -4.1667919 -4.1742239 -4.150167 -4.1227112 -4.1273661 -4.1712375 -4.2351546][-4.3083587 -4.2964263 -4.2748318 -4.2256103 -4.157145 -4.1122022 -4.1209965 -4.1621222 -4.1873612 -4.1763382 -4.14895 -4.137249 -4.1603003 -4.2082191 -4.2632][-4.2958379 -4.2926626 -4.2871537 -4.2568731 -4.2064505 -4.1700478 -4.1751542 -4.2039909 -4.2147164 -4.1928525 -4.1667461 -4.16342 -4.1885633 -4.2327738 -4.2782512][-4.2770128 -4.2768669 -4.2843056 -4.2714448 -4.2379036 -4.2115159 -4.2159381 -4.2358503 -4.2371073 -4.2112451 -4.1884556 -4.1866479 -4.2052393 -4.2390633 -4.2740684][-4.2559009 -4.2572446 -4.2720737 -4.2727876 -4.2545128 -4.2377238 -4.241847 -4.2551818 -4.2511992 -4.2278256 -4.2116132 -4.2133336 -4.225636 -4.24737 -4.2689719][-4.2389407 -4.2403903 -4.2549992 -4.2630343 -4.2577338 -4.2483954 -4.2508955 -4.2599592 -4.2573342 -4.24156 -4.2312284 -4.2336478 -4.2416687 -4.2544641 -4.2659478]]...]
INFO - root - 2017-12-07 22:13:32.046790: step 59710, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 54h:29m:00s remains)
INFO - root - 2017-12-07 22:13:38.878440: step 59720, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 50h:54m:25s remains)
INFO - root - 2017-12-07 22:13:45.627395: step 59730, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 47h:52m:54s remains)
INFO - root - 2017-12-07 22:13:52.468609: step 59740, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 51h:01m:28s remains)
INFO - root - 2017-12-07 22:13:59.339877: step 59750, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 55h:31m:29s remains)
INFO - root - 2017-12-07 22:14:06.134937: step 59760, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 50h:32m:30s remains)
INFO - root - 2017-12-07 22:14:12.857834: step 59770, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 48h:32m:01s remains)
INFO - root - 2017-12-07 22:14:19.652460: step 59780, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 49h:55m:05s remains)
INFO - root - 2017-12-07 22:14:26.548033: step 59790, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 54h:28m:03s remains)
INFO - root - 2017-12-07 22:14:33.128112: step 59800, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 51h:06m:29s remains)
2017-12-07 22:14:33.837625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.331347 -4.3311596 -4.3303466 -4.3293295 -4.3293052 -4.3316674 -4.3325882 -4.3331771 -4.335269 -4.3384976 -4.3413606 -4.3431044 -4.3436165 -4.34528 -4.3480678][-4.3079481 -4.3071666 -4.3062816 -4.3050914 -4.3044939 -4.3055415 -4.3030334 -4.3026371 -4.3051872 -4.308856 -4.3121161 -4.3138261 -4.3138194 -4.3153811 -4.3215694][-4.2876143 -4.2860489 -4.283895 -4.2828097 -4.2827716 -4.2796917 -4.2708569 -4.2689242 -4.2717633 -4.2726703 -4.2727451 -4.2739005 -4.2726831 -4.2739024 -4.2821827][-4.2688727 -4.262414 -4.2552795 -4.2516875 -4.2496448 -4.2428203 -4.2260814 -4.2213483 -4.2245398 -4.2274656 -4.2252116 -4.2224908 -4.2182355 -4.2214789 -4.2353349][-4.2517443 -4.2358122 -4.2159786 -4.2017589 -4.1912189 -4.1776171 -4.1495843 -4.1370735 -4.1497197 -4.1681757 -4.1713257 -4.1651917 -4.1589322 -4.1664472 -4.1891918][-4.2373061 -4.2064271 -4.1695714 -4.1402082 -4.1196904 -4.0921397 -4.0380497 -4.0043159 -4.0359974 -4.0986052 -4.1258554 -4.1244049 -4.1184139 -4.1314607 -4.1600885][-4.2192492 -4.1755395 -4.1228318 -4.0762916 -4.0353322 -3.9778032 -3.8759584 -3.802613 -3.8641891 -3.9941046 -4.0652256 -4.0835581 -4.0829444 -4.1000857 -4.1327953][-4.2123938 -4.1622615 -4.0999489 -4.0362945 -3.9714425 -3.886035 -3.7441635 -3.6340351 -3.7263236 -3.9125783 -4.0182695 -4.0457535 -4.0434942 -4.0626364 -4.1042547][-4.2220984 -4.1780319 -4.1160512 -4.043787 -3.9695587 -3.8907712 -3.7740097 -3.6853642 -3.7689297 -3.9356091 -4.0286636 -4.040946 -4.0269213 -4.0419407 -4.0900874][-4.2408037 -4.2132163 -4.16179 -4.0975204 -4.0341587 -3.9847867 -3.9248443 -3.8785853 -3.9247313 -4.0216947 -4.07379 -4.0607777 -4.0332313 -4.0449739 -4.0927229][-4.2512727 -4.2330022 -4.1969633 -4.1473565 -4.0984864 -4.0711174 -4.0485678 -4.0263705 -4.0437961 -4.0900135 -4.1097612 -4.083569 -4.0532165 -4.067946 -4.1168313][-4.2590523 -4.242979 -4.2183623 -4.18571 -4.1512041 -4.1368241 -4.1372728 -4.1325488 -4.1348281 -4.1508975 -4.150105 -4.1254416 -4.103723 -4.1216269 -4.1677122][-4.2714939 -4.2602019 -4.2447524 -4.2276287 -4.2089615 -4.2033186 -4.2108421 -4.2146974 -4.2138624 -4.2171216 -4.2119989 -4.1977205 -4.1848722 -4.1974888 -4.2319841][-4.2866282 -4.2819319 -4.2758293 -4.271184 -4.2653666 -4.2642174 -4.2686634 -4.2706065 -4.2673283 -4.2652187 -4.2600727 -4.2539387 -4.250813 -4.262219 -4.2854762][-4.3045888 -4.3032923 -4.3023686 -4.3026571 -4.3021364 -4.30081 -4.3004627 -4.2976689 -4.2932568 -4.2909288 -4.2885094 -4.2887659 -4.2926617 -4.3035741 -4.3183389]]...]
INFO - root - 2017-12-07 22:14:40.557818: step 59810, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 53h:01m:56s remains)
INFO - root - 2017-12-07 22:14:47.374057: step 59820, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 53h:42m:42s remains)
INFO - root - 2017-12-07 22:14:54.172833: step 59830, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 51h:30m:58s remains)
INFO - root - 2017-12-07 22:15:00.925333: step 59840, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 49h:37m:32s remains)
INFO - root - 2017-12-07 22:15:07.722287: step 59850, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 48h:28m:27s remains)
INFO - root - 2017-12-07 22:15:14.597181: step 59860, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.712 sec/batch; 53h:55m:08s remains)
INFO - root - 2017-12-07 22:15:21.438185: step 59870, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 54h:03m:21s remains)
INFO - root - 2017-12-07 22:15:28.211294: step 59880, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 49h:43m:21s remains)
INFO - root - 2017-12-07 22:15:35.021310: step 59890, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 48h:37m:52s remains)
INFO - root - 2017-12-07 22:15:41.699315: step 59900, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 47h:44m:11s remains)
2017-12-07 22:15:42.400445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3387532 -4.3319545 -4.3246822 -4.3200774 -4.3214812 -4.3232636 -4.3224783 -4.3163157 -4.3080578 -4.3059869 -4.3092914 -4.3132119 -4.3135443 -4.3121343 -4.3101859][-4.318954 -4.3016524 -4.2858906 -4.2767735 -4.2766109 -4.2786117 -4.2778049 -4.2699361 -4.2601953 -4.2603045 -4.2649722 -4.2674952 -4.2666454 -4.2659345 -4.2654476][-4.2935672 -4.2635751 -4.2355676 -4.2165303 -4.2115512 -4.2133851 -4.2117934 -4.2008767 -4.1895423 -4.1916795 -4.1987715 -4.2023973 -4.2048779 -4.207243 -4.2075281][-4.2702293 -4.2288113 -4.1890669 -4.1590443 -4.1467004 -4.1442232 -4.13927 -4.1244168 -4.1107364 -4.1140261 -4.1250463 -4.1337972 -4.1446695 -4.1562133 -4.1628466][-4.2582841 -4.2140794 -4.1709108 -4.1354589 -4.1144762 -4.1022377 -4.0883746 -4.063077 -4.0407796 -4.0408912 -4.0541468 -4.0638642 -4.0789361 -4.1011791 -4.1140475][-4.2441521 -4.1993279 -4.1567259 -4.119276 -4.0911384 -4.0672436 -4.0412407 -4.0066452 -3.9745684 -3.9675756 -3.9772482 -3.9863706 -4.0045176 -4.0327687 -4.0475111][-4.2342558 -4.1884913 -4.147449 -4.1126866 -4.0859323 -4.0577021 -4.02572 -3.9927149 -3.9661975 -3.9632914 -3.9722674 -3.9817569 -3.9976931 -4.0203052 -4.02936][-4.2338014 -4.1896009 -4.1520424 -4.123652 -4.1022429 -4.0742006 -4.0413823 -4.0136938 -3.9969835 -4.0041733 -4.0225863 -4.0417376 -4.0568023 -4.0672965 -4.0659165][-4.2390723 -4.1980839 -4.1671658 -4.1469765 -4.1311526 -4.1100254 -4.0793452 -4.0528288 -4.0347519 -4.0411472 -4.064168 -4.0887318 -4.1063108 -4.1126318 -4.1056266][-4.2478919 -4.2102017 -4.1838536 -4.1714954 -4.1617994 -4.1495709 -4.1293178 -4.1076579 -4.0840907 -4.0783052 -4.0880365 -4.1014996 -4.1146765 -4.1210775 -4.1167688][-4.2665009 -4.2338715 -4.2094569 -4.1984587 -4.19156 -4.1864972 -4.1750035 -4.1572032 -4.1311417 -4.1147242 -4.1140528 -4.1200156 -4.1307487 -4.1375613 -4.1361251][-4.2941227 -4.2695751 -4.25003 -4.2397127 -4.23357 -4.2265983 -4.2139769 -4.19451 -4.1676903 -4.1483827 -4.1426911 -4.1441765 -4.1523628 -4.1600962 -4.1616383][-4.321548 -4.304481 -4.288547 -4.2793913 -4.27298 -4.2647357 -4.2520628 -4.2326632 -4.2094336 -4.1914072 -4.1833558 -4.1811237 -4.1845412 -4.1883397 -4.1858277][-4.3444891 -4.3335013 -4.3217487 -4.3135114 -4.3058929 -4.2975144 -4.2882304 -4.2746792 -4.2574482 -4.2447915 -4.2379761 -4.2330904 -4.2278366 -4.220644 -4.2101536][-4.3591127 -4.3522339 -4.3432889 -4.3353424 -4.3279724 -4.3216853 -4.3159604 -4.3075595 -4.2970591 -4.2889953 -4.2828574 -4.2761035 -4.2640738 -4.2487855 -4.2321248]]...]
INFO - root - 2017-12-07 22:15:49.148821: step 59910, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 47h:19m:02s remains)
INFO - root - 2017-12-07 22:15:55.856981: step 59920, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 48h:31m:02s remains)
INFO - root - 2017-12-07 22:16:02.602229: step 59930, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 48h:51m:54s remains)
INFO - root - 2017-12-07 22:16:09.451501: step 59940, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.720 sec/batch; 54h:28m:43s remains)
INFO - root - 2017-12-07 22:16:16.345084: step 59950, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 53h:56m:22s remains)
INFO - root - 2017-12-07 22:16:23.162283: step 59960, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 49h:44m:16s remains)
INFO - root - 2017-12-07 22:16:29.888178: step 59970, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 47h:04m:19s remains)
INFO - root - 2017-12-07 22:16:36.793425: step 59980, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 50h:26m:56s remains)
INFO - root - 2017-12-07 22:16:43.530754: step 59990, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 52h:35m:15s remains)
INFO - root - 2017-12-07 22:16:50.062245: step 60000, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 53h:03m:05s remains)
2017-12-07 22:16:50.856029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3010554 -4.3007669 -4.3002934 -4.3001404 -4.2997375 -4.2975454 -4.29453 -4.293026 -4.29279 -4.292769 -4.2905097 -4.2876725 -4.2862291 -4.2872229 -4.2875719][-4.2997389 -4.3002539 -4.2985635 -4.2960477 -4.2941184 -4.2882423 -4.2825174 -4.2789531 -4.2773614 -4.2772446 -4.2747645 -4.2729526 -4.2743688 -4.2794495 -4.2809157][-4.3066607 -4.30583 -4.2991533 -4.2931709 -4.2902789 -4.2804174 -4.2715111 -4.2658319 -4.2614465 -4.2604995 -4.2593813 -4.259099 -4.2616444 -4.2709551 -4.2763357][-4.3055034 -4.2971835 -4.284306 -4.2768521 -4.2725787 -4.2580638 -4.2489667 -4.2472177 -4.2405744 -4.23755 -4.2391758 -4.2418156 -4.2455106 -4.2570806 -4.268743][-4.28814 -4.2756267 -4.2601938 -4.2527108 -4.2432389 -4.2200089 -4.2141542 -4.2203994 -4.2132845 -4.2073154 -4.2158184 -4.223927 -4.2277565 -4.2383471 -4.25264][-4.2637463 -4.2493987 -4.2325406 -4.2198176 -4.1937842 -4.1599188 -4.1636653 -4.1769023 -4.1645279 -4.1541796 -4.171803 -4.1879091 -4.1942053 -4.2085056 -4.22655][-4.23852 -4.2185373 -4.1943173 -4.1680036 -4.1241837 -4.086988 -4.1055245 -4.1225386 -4.0992827 -4.07966 -4.1073446 -4.1344967 -4.1480327 -4.1726623 -4.1985598][-4.225543 -4.1960597 -4.1617088 -4.1215773 -4.0657754 -4.0335975 -4.062603 -4.0747004 -4.0408297 -4.0114088 -4.0447826 -4.0822144 -4.1082392 -4.1462011 -4.1796165][-4.2332 -4.196671 -4.1613436 -4.1208892 -4.0682368 -4.04516 -4.0674644 -4.06633 -4.0295887 -3.9971094 -4.0259132 -4.0675917 -4.1038713 -4.1466985 -4.1804433][-4.2467051 -4.2169862 -4.1913881 -4.1651726 -4.1283178 -4.1126804 -4.1183782 -4.1055708 -4.0733056 -4.045557 -4.06662 -4.1060357 -4.1402836 -4.174767 -4.2008872][-4.254437 -4.2371569 -4.2220521 -4.2072597 -4.1861272 -4.172379 -4.1615982 -4.1452746 -4.1186895 -4.097218 -4.1119757 -4.1467595 -4.1771579 -4.2019291 -4.2208796][-4.25091 -4.2422733 -4.2329946 -4.2232013 -4.2114186 -4.1984968 -4.1817355 -4.1658678 -4.1416516 -4.123507 -4.1363559 -4.1711874 -4.1993194 -4.2172217 -4.2302585][-4.25157 -4.2450094 -4.2381725 -4.2278581 -4.2169285 -4.2048283 -4.1917205 -4.182229 -4.1621351 -4.1467156 -4.1608195 -4.1951971 -4.2205777 -4.2349296 -4.2430491][-4.2580485 -4.2533989 -4.2464662 -4.2349567 -4.2215495 -4.2116275 -4.2073932 -4.2039042 -4.1868134 -4.1727629 -4.1881342 -4.2187119 -4.2397127 -4.2523179 -4.258059][-4.2556643 -4.2543068 -4.2510667 -4.2447062 -4.2346044 -4.2302055 -4.2311692 -4.229579 -4.2135386 -4.1998386 -4.2128186 -4.2370534 -4.2541642 -4.2655206 -4.2689686]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 22:16:58.409623: step 60010, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 52h:14m:05s remains)
INFO - root - 2017-12-07 22:17:05.062341: step 60020, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.680 sec/batch; 51h:30m:01s remains)
INFO - root - 2017-12-07 22:17:11.775104: step 60030, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 47h:26m:20s remains)
INFO - root - 2017-12-07 22:17:18.542415: step 60040, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 48h:18m:36s remains)
INFO - root - 2017-12-07 22:17:25.462413: step 60050, loss = 2.03, batch loss = 1.97 (10.8 examples/sec; 0.741 sec/batch; 56h:02m:52s remains)
INFO - root - 2017-12-07 22:17:32.287786: step 60060, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 55h:57m:51s remains)
INFO - root - 2017-12-07 22:17:39.066858: step 60070, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.664 sec/batch; 50h:15m:58s remains)
INFO - root - 2017-12-07 22:17:45.804036: step 60080, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 48h:48m:29s remains)
INFO - root - 2017-12-07 22:17:52.538061: step 60090, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 51h:45m:33s remains)
INFO - root - 2017-12-07 22:17:59.338167: step 60100, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 55h:30m:29s remains)
2017-12-07 22:17:59.982863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1866417 -4.1884727 -4.1912794 -4.1741076 -4.1516366 -4.1431971 -4.1559858 -4.1884294 -4.22345 -4.2516375 -4.2695932 -4.2765346 -4.2763195 -4.2800074 -4.289598][-4.1441031 -4.1534538 -4.16353 -4.1465068 -4.1164627 -4.1018167 -4.1174221 -4.16217 -4.20632 -4.238884 -4.2554345 -4.2596393 -4.2603178 -4.2652106 -4.2748389][-4.1299162 -4.1416349 -4.1515532 -4.1342425 -4.1007109 -4.0796852 -4.0944095 -4.148387 -4.1970868 -4.2237778 -4.2324834 -4.235024 -4.24069 -4.2483826 -4.2611971][-4.1334033 -4.1369629 -4.1394196 -4.1231079 -4.0907483 -4.068553 -4.0835338 -4.1420236 -4.1889462 -4.2086253 -4.2133446 -4.2185407 -4.2304068 -4.2410445 -4.2586427][-4.1349931 -4.1376266 -4.1367445 -4.1214042 -4.0868411 -4.057909 -4.0714025 -4.1338544 -4.1761842 -4.1842256 -4.1845 -4.1900105 -4.2078686 -4.2240305 -4.2477918][-4.1352367 -4.143301 -4.1428971 -4.1235304 -4.0776725 -4.0300794 -4.0320749 -4.0943727 -4.1375766 -4.1447682 -4.1430964 -4.1543612 -4.1820889 -4.2063136 -4.2361393][-4.1253657 -4.1393981 -4.1371493 -4.1078262 -4.0406518 -3.9575675 -3.9413407 -4.0111928 -4.0700803 -4.0905375 -4.0945272 -4.1142421 -4.1508465 -4.1852422 -4.2224069][-4.1089759 -4.1189957 -4.1069045 -4.0645576 -3.9731138 -3.8555548 -3.8269806 -3.9155605 -4.0031424 -4.0427289 -4.0573258 -4.0836687 -4.12547 -4.1649456 -4.2072692][-4.10281 -4.1104259 -4.0958452 -4.0603361 -3.9729884 -3.8573961 -3.8338795 -3.9237556 -4.0118108 -4.0515423 -4.0674143 -4.0925322 -4.1285381 -4.1647367 -4.2041588][-4.1251578 -4.1375494 -4.1307073 -4.1109076 -4.0476179 -3.9621236 -3.9416099 -4.0031037 -4.0635543 -4.0908318 -4.1022415 -4.1215415 -4.1484985 -4.1771178 -4.2133384][-4.1675429 -4.1791906 -4.1752124 -4.1636281 -4.1194749 -4.0582185 -4.0384088 -4.0740952 -4.1139064 -4.134232 -4.1435757 -4.1592135 -4.1797023 -4.2027497 -4.2339478][-4.2139945 -4.2176538 -4.2120662 -4.2037258 -4.1730022 -4.131042 -4.1179495 -4.1401129 -4.17089 -4.1898632 -4.197135 -4.2075434 -4.2216811 -4.2407908 -4.267437][-4.2510939 -4.2479248 -4.2438717 -4.241498 -4.2268748 -4.2059836 -4.20053 -4.2134686 -4.232872 -4.2456641 -4.2479887 -4.2510824 -4.2605505 -4.2774363 -4.3007383][-4.2811913 -4.2773395 -4.2784324 -4.2816987 -4.2772732 -4.2686877 -4.26773 -4.2752419 -4.2867389 -4.2943592 -4.2950511 -4.2963 -4.3021393 -4.3132324 -4.3288007][-4.3157072 -4.3138947 -4.3146467 -4.3154893 -4.3130856 -4.3106456 -4.3112707 -4.31572 -4.3226852 -4.3280497 -4.3298221 -4.3312287 -4.333015 -4.3383651 -4.3468924]]...]
INFO - root - 2017-12-07 22:18:06.687860: step 60110, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 51h:07m:47s remains)
INFO - root - 2017-12-07 22:18:13.605402: step 60120, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 54h:57m:18s remains)
INFO - root - 2017-12-07 22:18:20.508188: step 60130, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 52h:17m:01s remains)
INFO - root - 2017-12-07 22:18:27.292269: step 60140, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 47h:57m:10s remains)
INFO - root - 2017-12-07 22:18:34.150678: step 60150, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.681 sec/batch; 51h:29m:49s remains)
INFO - root - 2017-12-07 22:18:40.991570: step 60160, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 53h:40m:36s remains)
INFO - root - 2017-12-07 22:18:47.827435: step 60170, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 54h:37m:01s remains)
INFO - root - 2017-12-07 22:18:54.626842: step 60180, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 49h:39m:44s remains)
INFO - root - 2017-12-07 22:19:01.438463: step 60190, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 50h:24m:11s remains)
INFO - root - 2017-12-07 22:19:08.084881: step 60200, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 50h:01m:19s remains)
2017-12-07 22:19:08.867513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2825217 -4.28403 -4.2866406 -4.2874126 -4.2769341 -4.2647181 -4.2672224 -4.2849789 -4.3023949 -4.304111 -4.2934341 -4.2690506 -4.2394314 -4.218389 -4.214016][-4.2696009 -4.2821431 -4.2882433 -4.2856317 -4.2667952 -4.248075 -4.2567248 -4.2887559 -4.3125367 -4.3126364 -4.2949224 -4.2627091 -4.2254152 -4.2020965 -4.2004347][-4.23334 -4.2603912 -4.2723522 -4.2619524 -4.2283239 -4.1986074 -4.2164588 -4.2693119 -4.30846 -4.31309 -4.2896166 -4.2468519 -4.1993914 -4.1724072 -4.1744618][-4.1990466 -4.2367125 -4.2523465 -4.2277985 -4.1666937 -4.120275 -4.1504483 -4.2266994 -4.2843347 -4.2955294 -4.2680178 -4.2184854 -4.1677012 -4.1418424 -4.1516614][-4.1834679 -4.2234554 -4.2337775 -4.1872787 -4.0888052 -4.023366 -4.0695987 -4.1735706 -4.2516055 -4.2738748 -4.2477417 -4.191844 -4.135066 -4.1095681 -4.1289983][-4.1822486 -4.2161264 -4.2129021 -4.1416035 -4.0045352 -3.9171734 -3.9806721 -4.1105142 -4.2067 -4.2436962 -4.2212882 -4.1601896 -4.0946264 -4.067287 -4.0914741][-4.1794829 -4.2005873 -4.1813545 -4.0885582 -3.9250658 -3.8229456 -3.8952181 -4.0412984 -4.1494951 -4.1966238 -4.1787167 -4.1172891 -4.0478749 -4.0240312 -4.0590806][-4.1762676 -4.1842237 -4.1555543 -4.0566134 -3.8959954 -3.8028386 -3.8706522 -4.0083032 -4.1148214 -4.1578612 -4.1377091 -4.0742197 -4.0017986 -3.9848888 -4.0332489][-4.1661754 -4.1714926 -4.1457591 -4.0605578 -3.9294236 -3.8610606 -3.9090571 -4.0162568 -4.1034431 -4.1320081 -4.106988 -4.0481772 -3.9758832 -3.9618797 -4.0134611][-4.1552639 -4.1642084 -4.1472936 -4.0816741 -3.9905968 -3.9478073 -3.9737346 -4.0428681 -4.1001167 -4.1131682 -4.0926762 -4.0473185 -3.9795384 -3.9619761 -4.0076838][-4.1570206 -4.1664453 -4.1531391 -4.1048355 -4.0487661 -4.025414 -4.0338774 -4.068356 -4.0989833 -4.1052742 -4.0942283 -4.0616307 -4.006249 -3.9859414 -4.0231242][-4.1787829 -4.184876 -4.1716757 -4.1365314 -4.1044474 -4.0911913 -4.0899115 -4.0996065 -4.1130109 -4.1201506 -4.1211028 -4.103302 -4.0620732 -4.0399985 -4.0621123][-4.2153211 -4.2154284 -4.2027035 -4.1803122 -4.1628885 -4.153697 -4.1478629 -4.1430311 -4.1475673 -4.1550603 -4.1612129 -4.1535692 -4.1271462 -4.1093321 -4.1236553][-4.2583742 -4.2539444 -4.2428632 -4.230988 -4.2231097 -4.2185745 -4.2141495 -4.2055564 -4.2038064 -4.2084036 -4.2136483 -4.2114697 -4.1988993 -4.1894794 -4.2005172][-4.3019552 -4.2977648 -4.2897463 -4.2829328 -4.2791448 -4.2770863 -4.2752514 -4.2688055 -4.2646294 -4.2656741 -4.2688856 -4.2690749 -4.2672811 -4.2684875 -4.2807279]]...]
INFO - root - 2017-12-07 22:19:15.705576: step 60210, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 51h:33m:18s remains)
INFO - root - 2017-12-07 22:19:22.539680: step 60220, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 48h:19m:54s remains)
INFO - root - 2017-12-07 22:19:29.264112: step 60230, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 50h:00m:31s remains)
INFO - root - 2017-12-07 22:19:36.021397: step 60240, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 53h:12m:17s remains)
INFO - root - 2017-12-07 22:19:42.678032: step 60250, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 53h:36m:43s remains)
INFO - root - 2017-12-07 22:19:49.403062: step 60260, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 50h:26m:25s remains)
INFO - root - 2017-12-07 22:19:56.224630: step 60270, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 47h:30m:05s remains)
INFO - root - 2017-12-07 22:20:02.983968: step 60280, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.620 sec/batch; 46h:53m:05s remains)
INFO - root - 2017-12-07 22:20:09.807907: step 60290, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 55h:57m:51s remains)
INFO - root - 2017-12-07 22:20:16.424825: step 60300, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 52h:11m:53s remains)
2017-12-07 22:20:17.216860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2158885 -4.230598 -4.2314744 -4.2250729 -4.2344394 -4.2484207 -4.2612343 -4.2633715 -4.2566614 -4.2481942 -4.2414703 -4.236073 -4.2380676 -4.2517772 -4.2729306][-4.2348442 -4.2448287 -4.2341862 -4.2090883 -4.2036133 -4.2121363 -4.2247353 -4.2268033 -4.2237816 -4.2209826 -4.2153878 -4.2083974 -4.2093964 -4.2268 -4.2530494][-4.2595434 -4.2633915 -4.2394857 -4.1971412 -4.1778979 -4.1833696 -4.1923766 -4.1911168 -4.1912713 -4.1944919 -4.1948886 -4.1932817 -4.1954517 -4.2157297 -4.2439237][-4.2677956 -4.2676744 -4.2337952 -4.1751771 -4.1470423 -4.1558857 -4.1592317 -4.1488605 -4.152318 -4.1677856 -4.1797142 -4.1854668 -4.1918736 -4.2148042 -4.2434788][-4.2611909 -4.259047 -4.2161469 -4.146471 -4.1159668 -4.1251216 -4.1156569 -4.0801516 -4.078289 -4.1141992 -4.14163 -4.1573844 -4.1746373 -4.20434 -4.2392278][-4.2384243 -4.2267551 -4.1818442 -4.1208491 -4.0892882 -4.0857019 -4.0451603 -3.9634485 -3.9566202 -4.0246882 -4.0724783 -4.1014433 -4.1365523 -4.1795063 -4.2225556][-4.2194757 -4.2034249 -4.1660171 -4.1185489 -4.0846004 -4.0580559 -3.9726698 -3.8342907 -3.8230863 -3.926589 -3.9943385 -4.0362844 -4.0844398 -4.1418185 -4.1945615][-4.2177892 -4.2048311 -4.17535 -4.1372457 -4.1004043 -4.0574245 -3.9533408 -3.8132095 -3.8077285 -3.9048514 -3.9676244 -4.0086641 -4.0590711 -4.1223674 -4.1829534][-4.2453794 -4.2413511 -4.2225823 -4.1914792 -4.1570168 -4.1128554 -4.0310345 -3.9410746 -3.9315858 -3.9797382 -4.0126228 -4.0384364 -4.0762115 -4.1357851 -4.1984897][-4.2858148 -4.29435 -4.2902937 -4.2683282 -4.2434387 -4.206131 -4.1518722 -4.0996952 -4.0834684 -4.0919652 -4.10037 -4.1110086 -4.1325479 -4.178103 -4.2320538][-4.3133039 -4.3300958 -4.3381944 -4.3288388 -4.3180614 -4.2948413 -4.2611856 -4.2282481 -4.2041359 -4.1926289 -4.1906848 -4.1942091 -4.2008734 -4.2287483 -4.2669053][-4.3214688 -4.340353 -4.3528752 -4.3532233 -4.35046 -4.3387413 -4.3205271 -4.3027091 -4.282155 -4.2667913 -4.264194 -4.2652788 -4.2610373 -4.2713609 -4.2928705][-4.3208714 -4.3374281 -4.3503389 -4.3531952 -4.3507714 -4.3429861 -4.3333144 -4.3258481 -4.3138881 -4.3037386 -4.304884 -4.3075805 -4.3007364 -4.299902 -4.3094673][-4.3289332 -4.340219 -4.3494544 -4.3526893 -4.3490281 -4.3410439 -4.3338237 -4.33178 -4.327425 -4.3235726 -4.3244972 -4.3252091 -4.3185592 -4.31502 -4.3207974][-4.3380756 -4.3451481 -4.3500676 -4.3523755 -4.3494477 -4.3440747 -4.3405843 -4.3396745 -4.3387222 -4.3383856 -4.3377695 -4.3345013 -4.3300757 -4.3286843 -4.3333254]]...]
INFO - root - 2017-12-07 22:20:23.912600: step 60310, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 51h:48m:18s remains)
INFO - root - 2017-12-07 22:20:30.773228: step 60320, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 0.766 sec/batch; 57h:55m:50s remains)
INFO - root - 2017-12-07 22:20:37.617682: step 60330, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 51h:15m:18s remains)
INFO - root - 2017-12-07 22:20:44.508362: step 60340, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 48h:36m:23s remains)
INFO - root - 2017-12-07 22:20:51.249913: step 60350, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.608 sec/batch; 45h:56m:40s remains)
INFO - root - 2017-12-07 22:20:58.072537: step 60360, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 51h:44m:58s remains)
INFO - root - 2017-12-07 22:21:04.986987: step 60370, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 53h:12m:09s remains)
INFO - root - 2017-12-07 22:21:11.835622: step 60380, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 53h:48m:06s remains)
INFO - root - 2017-12-07 22:21:18.597257: step 60390, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 49h:34m:20s remains)
INFO - root - 2017-12-07 22:21:25.131623: step 60400, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 46h:56m:10s remains)
2017-12-07 22:21:25.867890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2040133 -4.1821313 -4.1759686 -4.1859379 -4.214407 -4.2348924 -4.2334161 -4.2204423 -4.1973462 -4.1698966 -4.1552644 -4.1658053 -4.1821628 -4.1818118 -4.1931205][-4.2339115 -4.2158365 -4.207788 -4.2135968 -4.2342119 -4.2501769 -4.24595 -4.2238107 -4.1960831 -4.1774259 -4.1772389 -4.1895423 -4.1976509 -4.1920943 -4.2053671][-4.2322106 -4.214859 -4.2081485 -4.2121472 -4.2276306 -4.2376914 -4.2313485 -4.20992 -4.1874866 -4.1820893 -4.1989994 -4.2196293 -4.2207575 -4.2086492 -4.2204242][-4.2039976 -4.1828775 -4.1774368 -4.1821218 -4.192276 -4.192987 -4.1817126 -4.1636095 -4.1562943 -4.1703019 -4.2021546 -4.2317562 -4.2313795 -4.2157412 -4.224999][-4.1813936 -4.1564431 -4.1452832 -4.1419358 -4.1391745 -4.1283884 -4.1080494 -4.08874 -4.1013246 -4.1478238 -4.1952882 -4.2308145 -4.23271 -4.215858 -4.2194443][-4.1800804 -4.1518712 -4.1287918 -4.1090059 -4.0844765 -4.0560994 -4.0210729 -3.9867849 -4.0173969 -4.107924 -4.1790109 -4.2218566 -4.2292204 -4.2133222 -4.2108264][-4.1819005 -4.1549816 -4.1256361 -4.0915246 -4.0501642 -4.0102162 -3.9559479 -3.889631 -3.924911 -4.0616708 -4.1591859 -4.2106285 -4.2228532 -4.2106366 -4.2063417][-4.1698184 -4.1393213 -4.11332 -4.0813584 -4.0397635 -4.0050921 -3.955688 -3.8928216 -3.925127 -4.0595961 -4.1557412 -4.2072134 -4.2210169 -4.2085228 -4.201992][-4.1500216 -4.1077385 -4.0835881 -4.07092 -4.0514688 -4.03516 -4.0125074 -3.9867318 -4.0165892 -4.1052732 -4.176158 -4.2145 -4.2211857 -4.2065835 -4.1980672][-4.138978 -4.0935149 -4.0769424 -4.0870008 -4.0899754 -4.0845814 -4.0784931 -4.0786695 -4.1009116 -4.1508245 -4.1956468 -4.2199807 -4.2223773 -4.2080183 -4.1987786][-4.146203 -4.108633 -4.1032286 -4.1240735 -4.1369309 -4.1387458 -4.1385617 -4.1498532 -4.1637878 -4.1856503 -4.2069931 -4.2203736 -4.2240515 -4.2136669 -4.20413][-4.156888 -4.12585 -4.1301117 -4.1560268 -4.1740947 -4.1794233 -4.1832023 -4.2005491 -4.2094164 -4.2125039 -4.2137961 -4.2160721 -4.2208076 -4.2162619 -4.2070584][-4.1720924 -4.1406879 -4.1458712 -4.1742988 -4.1957717 -4.2030516 -4.2102938 -4.2288237 -4.2345672 -4.224349 -4.2088776 -4.1972508 -4.2019954 -4.2060547 -4.2056952][-4.1984034 -4.159894 -4.1580448 -4.1803684 -4.200511 -4.2119622 -4.2241826 -4.2384586 -4.2363148 -4.2160683 -4.1890316 -4.1704512 -4.1766849 -4.1906314 -4.2063661][-4.2226892 -4.1784706 -4.164957 -4.1765752 -4.1949048 -4.2138071 -4.2314348 -4.2429714 -4.231926 -4.1994872 -4.1663213 -4.1510792 -4.1640091 -4.1864219 -4.2155223]]...]
INFO - root - 2017-12-07 22:21:32.689820: step 60410, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 55h:02m:57s remains)
INFO - root - 2017-12-07 22:21:39.337224: step 60420, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 50h:16m:59s remains)
INFO - root - 2017-12-07 22:21:46.029739: step 60430, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 49h:32m:13s remains)
INFO - root - 2017-12-07 22:21:52.829707: step 60440, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.698 sec/batch; 52h:44m:26s remains)
INFO - root - 2017-12-07 22:21:59.585519: step 60450, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 56h:26m:15s remains)
INFO - root - 2017-12-07 22:22:06.347843: step 60460, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.678 sec/batch; 51h:12m:33s remains)
INFO - root - 2017-12-07 22:22:13.013924: step 60470, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 48h:47m:18s remains)
INFO - root - 2017-12-07 22:22:19.775546: step 60480, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 47h:22m:08s remains)
INFO - root - 2017-12-07 22:22:26.600725: step 60490, loss = 2.03, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 53h:07m:46s remains)
INFO - root - 2017-12-07 22:22:33.252580: step 60500, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 51h:27m:38s remains)
2017-12-07 22:22:34.035626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2949429 -4.2730255 -4.2392912 -4.1952477 -4.1535 -4.1167431 -4.1003733 -4.1231608 -4.1675358 -4.2103376 -4.2447371 -4.2603693 -4.2687192 -4.2789803 -4.288692][-4.3032336 -4.281188 -4.23776 -4.1804352 -4.1263037 -4.082531 -4.0721221 -4.1024561 -4.1497793 -4.1974521 -4.2339854 -4.249692 -4.2607851 -4.2752151 -4.2867889][-4.3022661 -4.2769551 -4.2257524 -4.1572466 -4.0916986 -4.0402207 -4.0359831 -4.0779614 -4.1321507 -4.1855755 -4.2251449 -4.2420888 -4.2542734 -4.2684727 -4.2798786][-4.3006859 -4.2716866 -4.2129025 -4.1353407 -4.0582643 -3.9994369 -3.9968 -4.0518551 -4.11529 -4.1725249 -4.2146072 -4.2352033 -4.2480984 -4.2617631 -4.2716031][-4.292892 -4.2626114 -4.2055454 -4.1234612 -4.0312052 -3.9594331 -3.9583967 -4.0298114 -4.107862 -4.1679959 -4.2116842 -4.2326708 -4.2446275 -4.2548008 -4.261302][-4.2864008 -4.2622223 -4.2113428 -4.1251216 -4.0115895 -3.9134881 -3.9095697 -4.0006571 -4.0983377 -4.1659117 -4.2134032 -4.2351656 -4.2444038 -4.25011 -4.2504129][-4.2852054 -4.2706704 -4.2289906 -4.1431403 -4.0095706 -3.8709426 -3.8455868 -3.9505286 -4.0754876 -4.1620827 -4.2148576 -4.2367821 -4.2417068 -4.2415752 -4.2349591][-4.2765441 -4.2701545 -4.246213 -4.1720009 -4.0309749 -3.8646832 -3.8147056 -3.923579 -4.068974 -4.1668477 -4.2184806 -4.2330585 -4.2292051 -4.2203517 -4.2116108][-4.2534046 -4.255765 -4.2523346 -4.2013259 -4.0806503 -3.918123 -3.8508759 -3.942276 -4.0847964 -4.1779141 -4.2198348 -4.2260466 -4.2166228 -4.205092 -4.1966286][-4.222579 -4.2295532 -4.2438025 -4.2162366 -4.1280642 -3.9931524 -3.9201605 -3.9781594 -4.098752 -4.1781754 -4.2064834 -4.2078142 -4.2000141 -4.1941676 -4.1927266][-4.1984167 -4.2036762 -4.2266665 -4.2150493 -4.1594009 -4.0591278 -3.9824595 -4.0047369 -4.0931559 -4.1569037 -4.1756482 -4.1731882 -4.1739826 -4.184824 -4.1984792][-4.1862526 -4.1815023 -4.2070632 -4.2128725 -4.1887641 -4.1201763 -4.0463953 -4.0351009 -4.0827537 -4.127337 -4.140224 -4.1414094 -4.1572938 -4.1852474 -4.211771][-4.1941171 -4.1792855 -4.1986551 -4.2188849 -4.2189622 -4.1768322 -4.1120887 -4.0761123 -4.0904841 -4.12116 -4.1359348 -4.1457853 -4.1709566 -4.2046852 -4.2330389][-4.2056813 -4.1857982 -4.2016115 -4.2288837 -4.243216 -4.2238579 -4.1749907 -4.1317329 -4.1256132 -4.1438437 -4.156764 -4.1659594 -4.1880183 -4.21806 -4.2419438][-4.2134371 -4.192524 -4.2093334 -4.2418056 -4.2642188 -4.2608738 -4.22771 -4.1882472 -4.1700287 -4.17502 -4.1823545 -4.1860819 -4.2003579 -4.2235026 -4.2455077]]...]
INFO - root - 2017-12-07 22:22:40.889531: step 60510, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 50h:51m:38s remains)
INFO - root - 2017-12-07 22:22:47.729864: step 60520, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.724 sec/batch; 54h:42m:04s remains)
INFO - root - 2017-12-07 22:22:54.583453: step 60530, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 51h:53m:44s remains)
INFO - root - 2017-12-07 22:23:01.350573: step 60540, loss = 2.11, batch loss = 2.05 (12.3 examples/sec; 0.651 sec/batch; 49h:12m:26s remains)
INFO - root - 2017-12-07 22:23:08.043116: step 60550, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 47h:58m:40s remains)
INFO - root - 2017-12-07 22:23:14.758172: step 60560, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 51h:19m:20s remains)
INFO - root - 2017-12-07 22:23:21.606131: step 60570, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.746 sec/batch; 56h:19m:32s remains)
INFO - root - 2017-12-07 22:23:28.401932: step 60580, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 51h:42m:59s remains)
INFO - root - 2017-12-07 22:23:35.158272: step 60590, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 48h:44m:26s remains)
INFO - root - 2017-12-07 22:23:41.678885: step 60600, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 48h:50m:19s remains)
2017-12-07 22:23:42.423226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2579856 -4.2715669 -4.2834797 -4.2810884 -4.2749543 -4.2700286 -4.2738161 -4.2733755 -4.2611785 -4.2435269 -4.2276731 -4.2183514 -4.2211065 -4.234838 -4.2480478][-4.2462034 -4.2687864 -4.2905412 -4.292459 -4.2803159 -4.2635355 -4.2610478 -4.2650709 -4.2558465 -4.2379045 -4.2215981 -4.2108331 -4.2152166 -4.2339005 -4.2512393][-4.2472916 -4.2815628 -4.3095841 -4.3158941 -4.3004107 -4.2697606 -4.2560706 -4.263453 -4.2621078 -4.2500782 -4.2384729 -4.2263641 -4.2255468 -4.2440886 -4.2627664][-4.2517529 -4.294693 -4.3263197 -4.3345423 -4.3186336 -4.2766738 -4.2485385 -4.2548385 -4.2620807 -4.2582555 -4.2514567 -4.2392573 -4.2338939 -4.2506423 -4.2693939][-4.2437572 -4.2883291 -4.3219738 -4.327795 -4.3125625 -4.2646894 -4.2244267 -4.2273278 -4.2433281 -4.2470984 -4.2454257 -4.2357674 -4.2302346 -4.2488279 -4.2704511][-4.2215385 -4.2595916 -4.2920275 -4.2947116 -4.2769465 -4.2261424 -4.1783442 -4.1806121 -4.2059255 -4.2215581 -4.2265449 -4.2245803 -4.2247086 -4.2475576 -4.2736435][-4.1956987 -4.2270947 -4.2593293 -4.25987 -4.235465 -4.1824036 -4.1302657 -4.1322637 -4.1659236 -4.1927285 -4.206243 -4.2144232 -4.2237835 -4.2515068 -4.2792687][-4.1716051 -4.197948 -4.2332025 -4.237865 -4.2127719 -4.1596341 -4.1082435 -4.1088729 -4.1461296 -4.1776438 -4.1949043 -4.2114358 -4.2273583 -4.2574368 -4.2843747][-4.1477542 -4.1677008 -4.2047668 -4.2169852 -4.1957426 -4.1473579 -4.1024451 -4.1054039 -4.1425414 -4.1704311 -4.1884747 -4.210505 -4.2299409 -4.25839 -4.2809787][-4.13879 -4.1515174 -4.1846504 -4.1976128 -4.18039 -4.1373439 -4.1000109 -4.1034265 -4.1334815 -4.1544805 -4.1747246 -4.2035289 -4.2277884 -4.2535605 -4.27123][-4.141839 -4.1484981 -4.1776905 -4.1909571 -4.1798739 -4.1442337 -4.1133351 -4.1123719 -4.1301632 -4.1393275 -4.1569886 -4.1884441 -4.2196045 -4.2476463 -4.2650967][-4.157289 -4.1562352 -4.1767464 -4.1879711 -4.1828852 -4.1568604 -4.137567 -4.1351581 -4.1362247 -4.1304407 -4.1418633 -4.1725597 -4.2113466 -4.244503 -4.263216][-4.1870871 -4.1730165 -4.1787162 -4.1860194 -4.1874008 -4.1714792 -4.1607141 -4.1572647 -4.1445551 -4.1268535 -4.1311259 -4.1633153 -4.20806 -4.2425556 -4.2574162][-4.2189026 -4.1957297 -4.1901875 -4.1948352 -4.2000432 -4.1906919 -4.1790819 -4.171999 -4.1510239 -4.1252909 -4.1256886 -4.1604772 -4.2082734 -4.240284 -4.2489619][-4.2520919 -4.2260394 -4.2139425 -4.2140856 -4.216835 -4.2086959 -4.1957755 -4.1840734 -4.156096 -4.1228242 -4.1198783 -4.15302 -4.1956391 -4.2256126 -4.2306714]]...]
INFO - root - 2017-12-07 22:23:49.189786: step 60610, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 52h:23m:20s remains)
INFO - root - 2017-12-07 22:23:55.910629: step 60620, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 49h:55m:05s remains)
INFO - root - 2017-12-07 22:24:02.720138: step 60630, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 48h:30m:56s remains)
INFO - root - 2017-12-07 22:24:09.517498: step 60640, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 52h:04m:00s remains)
INFO - root - 2017-12-07 22:24:16.246655: step 60650, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 51h:45m:48s remains)
INFO - root - 2017-12-07 22:24:23.000428: step 60660, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 49h:16m:35s remains)
INFO - root - 2017-12-07 22:24:29.775469: step 60670, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 49h:45m:53s remains)
INFO - root - 2017-12-07 22:24:36.573920: step 60680, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 47h:21m:08s remains)
INFO - root - 2017-12-07 22:24:43.365252: step 60690, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 51h:36m:31s remains)
INFO - root - 2017-12-07 22:24:49.977120: step 60700, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 52h:08m:04s remains)
2017-12-07 22:24:50.711265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2719269 -4.2948689 -4.3113775 -4.3194766 -4.3146825 -4.3051944 -4.2865138 -4.279624 -4.2898192 -4.2917333 -4.2844706 -4.2872524 -4.2976885 -4.3025479 -4.2987008][-4.2782331 -4.3010068 -4.3164024 -4.3245759 -4.3187065 -4.3069086 -4.2823949 -4.2698736 -4.2767892 -4.2788429 -4.2742052 -4.2812495 -4.2943182 -4.3030667 -4.3006253][-4.2872043 -4.3074965 -4.3199458 -4.3261151 -4.3193936 -4.3047104 -4.2741108 -4.2577295 -4.2689571 -4.2767811 -4.2778492 -4.2858605 -4.2956944 -4.3013363 -4.2978][-4.292417 -4.3091207 -4.319654 -4.3230433 -4.3114629 -4.2896538 -4.2502789 -4.2288866 -4.249558 -4.265645 -4.2638192 -4.2661695 -4.2725382 -4.2796049 -4.2794118][-4.2921958 -4.3029952 -4.3100767 -4.3083553 -4.2843418 -4.2459345 -4.1895571 -4.1591706 -4.1968331 -4.225719 -4.2237978 -4.22465 -4.232832 -4.2452493 -4.2489271][-4.2875938 -4.2881637 -4.2905154 -4.2844634 -4.2493453 -4.1884813 -4.1018877 -4.0594807 -4.1210327 -4.1736574 -4.1880164 -4.193078 -4.203115 -4.21852 -4.2268686][-4.284018 -4.2757444 -4.2716503 -4.2583332 -4.2082057 -4.1169977 -3.991014 -3.9364028 -4.0348697 -4.129159 -4.1731887 -4.1922393 -4.207406 -4.2206292 -4.2256694][-4.2789793 -4.2688789 -4.2611256 -4.2414751 -4.1776118 -4.0581503 -3.9023798 -3.8453555 -3.9816964 -4.107214 -4.1727886 -4.2063351 -4.2287111 -4.2340117 -4.2250342][-4.2745986 -4.2681146 -4.2625108 -4.2478251 -4.1919508 -4.0821037 -3.9367187 -3.8910105 -4.0226493 -4.1368341 -4.1982923 -4.2336493 -4.2569442 -4.2524958 -4.2311921][-4.2744751 -4.2744665 -4.2770081 -4.2753687 -4.2421479 -4.1634545 -4.0499091 -4.0106368 -4.1106048 -4.1942377 -4.2365346 -4.25996 -4.2735863 -4.26371 -4.239306][-4.2723794 -4.2758102 -4.286562 -4.296195 -4.2815089 -4.2296066 -4.1434455 -4.1116142 -4.186142 -4.24382 -4.2633829 -4.2736559 -4.2781606 -4.2654285 -4.2440047][-4.27011 -4.2766643 -4.2935262 -4.3106279 -4.3076582 -4.2680488 -4.1985011 -4.1725035 -4.2291512 -4.2661147 -4.2702641 -4.2706022 -4.2670679 -4.2531466 -4.2293663][-4.2672396 -4.2757277 -4.2976542 -4.3186965 -4.3209505 -4.286973 -4.22249 -4.1957173 -4.2354116 -4.260417 -4.2605367 -4.253088 -4.2421646 -4.2285619 -4.20545][-4.2603908 -4.2694526 -4.2940521 -4.3153286 -4.318161 -4.2864347 -4.2246909 -4.1973472 -4.22341 -4.2427864 -4.246304 -4.2375298 -4.2227354 -4.2122531 -4.1918321][-4.2549343 -4.2638245 -4.2878528 -4.3087244 -4.3129649 -4.2837496 -4.2237759 -4.1964736 -4.2164445 -4.2354836 -4.2469959 -4.2415762 -4.2262917 -4.2170267 -4.2021551]]...]
INFO - root - 2017-12-07 22:24:57.529372: step 60710, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 49h:15m:24s remains)
INFO - root - 2017-12-07 22:25:04.408906: step 60720, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 54h:29m:16s remains)
INFO - root - 2017-12-07 22:25:11.018322: step 60730, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 52h:25m:25s remains)
INFO - root - 2017-12-07 22:25:17.798875: step 60740, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 47h:59m:58s remains)
INFO - root - 2017-12-07 22:25:24.616059: step 60750, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 49h:24m:57s remains)
INFO - root - 2017-12-07 22:25:31.492631: step 60760, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.727 sec/batch; 54h:53m:06s remains)
INFO - root - 2017-12-07 22:25:38.306337: step 60770, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 53h:18m:35s remains)
INFO - root - 2017-12-07 22:25:45.058861: step 60780, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 50h:26m:52s remains)
INFO - root - 2017-12-07 22:25:51.908031: step 60790, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 48h:28m:11s remains)
INFO - root - 2017-12-07 22:25:58.485393: step 60800, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 49h:39m:43s remains)
2017-12-07 22:25:59.274415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30648 -4.29916 -4.2976632 -4.3023129 -4.3092284 -4.3135223 -4.3127074 -4.3048587 -4.2980251 -4.2953281 -4.2921305 -4.2890944 -4.2867732 -4.2892594 -4.2925529][-4.3002906 -4.2901835 -4.2884803 -4.2949896 -4.3032584 -4.3083167 -4.3040609 -4.2887039 -4.27478 -4.2686768 -4.2626586 -4.2572184 -4.257555 -4.2678294 -4.2796192][-4.2939019 -4.276968 -4.2710705 -4.2761288 -4.2837119 -4.2875452 -4.276186 -4.2479668 -4.22438 -4.2145195 -4.2057176 -4.2006297 -4.2073722 -4.2294197 -4.2535238][-4.2852116 -4.259973 -4.2466841 -4.2474961 -4.2520337 -4.2494965 -4.2255459 -4.1779971 -4.1410418 -4.1335526 -4.1305909 -4.1345334 -4.1526041 -4.1877871 -4.2258105][-4.2767096 -4.2405324 -4.216423 -4.2056642 -4.1987414 -4.1822734 -4.136004 -4.0593238 -4.0100427 -4.0184703 -4.0403957 -4.0671153 -4.1049223 -4.1486077 -4.1920624][-4.2689877 -4.2231817 -4.1876616 -4.1605597 -4.1348553 -4.09475 -4.0129976 -3.893249 -3.834826 -3.8839595 -3.9535432 -4.0144835 -4.0675149 -4.1121931 -4.1520066][-4.2726283 -4.226697 -4.1843305 -4.1396575 -4.0886273 -4.017241 -3.8998671 -3.7432525 -3.6868448 -3.785078 -3.9030347 -3.9980111 -4.0617518 -4.1065936 -4.1410217][-4.2827587 -4.2440281 -4.2034564 -4.1539159 -4.0968862 -4.0231376 -3.9173639 -3.783952 -3.7529211 -3.847563 -3.960228 -4.0505552 -4.1087861 -4.146163 -4.1715975][-4.2838769 -4.2513561 -4.2179003 -4.179543 -4.1382971 -4.0894418 -4.0225148 -3.9386251 -3.9278097 -3.9931624 -4.066958 -4.1241388 -4.16204 -4.189105 -4.2114954][-4.2691774 -4.236784 -4.2093182 -4.1850104 -4.167016 -4.1510382 -4.1226063 -4.0778265 -4.0768561 -4.1199121 -4.1634688 -4.1902523 -4.2089996 -4.2280021 -4.2460485][-4.2514024 -4.2134218 -4.1855097 -4.169066 -4.1687946 -4.175786 -4.17292 -4.1531048 -4.1551871 -4.184617 -4.2143426 -4.2299528 -4.2393317 -4.2512646 -4.26461][-4.2529097 -4.2125778 -4.1839714 -4.1703396 -4.1772928 -4.1907663 -4.1958919 -4.1864753 -4.1892519 -4.2149415 -4.2404847 -4.2532487 -4.2573853 -4.2639141 -4.2742324][-4.2699485 -4.2371583 -4.2148671 -4.2038326 -4.2078238 -4.2162952 -4.2192111 -4.2137256 -4.2157774 -4.2358437 -4.255908 -4.2634511 -4.2646542 -4.2709889 -4.2810621][-4.2883563 -4.2667546 -4.252656 -4.2451491 -4.2457294 -4.2487979 -4.2490005 -4.2448235 -4.2444782 -4.2548494 -4.2666345 -4.2721825 -4.2761974 -4.2852378 -4.2961645][-4.3044724 -4.292028 -4.2847881 -4.2816749 -4.2823882 -4.2838368 -4.2836866 -4.2815938 -4.2804804 -4.2831764 -4.2869105 -4.289989 -4.2954745 -4.3045063 -4.3125515]]...]
INFO - root - 2017-12-07 22:26:06.090086: step 60810, loss = 2.09, batch loss = 2.04 (11.4 examples/sec; 0.704 sec/batch; 53h:08m:12s remains)
INFO - root - 2017-12-07 22:26:12.922245: step 60820, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 52h:41m:13s remains)
INFO - root - 2017-12-07 22:26:19.635644: step 60830, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 47h:30m:30s remains)
INFO - root - 2017-12-07 22:26:26.508295: step 60840, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.653 sec/batch; 49h:18m:37s remains)
INFO - root - 2017-12-07 22:26:33.380033: step 60850, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.725 sec/batch; 54h:42m:34s remains)
INFO - root - 2017-12-07 22:26:40.185543: step 60860, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.764 sec/batch; 57h:40m:27s remains)
INFO - root - 2017-12-07 22:26:46.918594: step 60870, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 49h:07m:38s remains)
INFO - root - 2017-12-07 22:26:53.768769: step 60880, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 50h:03m:55s remains)
INFO - root - 2017-12-07 22:27:00.537744: step 60890, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 52h:58m:12s remains)
INFO - root - 2017-12-07 22:27:07.191766: step 60900, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 52h:41m:10s remains)
2017-12-07 22:27:08.069285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3057785 -4.3029909 -4.3011112 -4.3025312 -4.2985468 -4.2842965 -4.2696848 -4.2641864 -4.2683487 -4.2778773 -4.2928843 -4.3103065 -4.3212452 -4.3247733 -4.3272324][-4.2980909 -4.2958817 -4.2968416 -4.2995505 -4.2898254 -4.2644124 -4.2390394 -4.2277055 -4.23164 -4.242908 -4.2614522 -4.2851181 -4.304028 -4.3139791 -4.3229227][-4.2954583 -4.2953143 -4.3008113 -4.3039112 -4.2868166 -4.2510285 -4.2176685 -4.20462 -4.20851 -4.21867 -4.2355914 -4.2573414 -4.2775903 -4.2912169 -4.3060012][-4.2975221 -4.2975225 -4.3011003 -4.294672 -4.2645497 -4.2160711 -4.1778235 -4.1705966 -4.1790304 -4.1915283 -4.2089906 -4.2281156 -4.2454066 -4.2584357 -4.2756987][-4.2958279 -4.2906423 -4.28292 -4.2559323 -4.2052703 -4.1452684 -4.1097608 -4.1196661 -4.1411443 -4.1605282 -4.181664 -4.196866 -4.2077241 -4.2185783 -4.2369342][-4.2833514 -4.2669005 -4.240418 -4.1896276 -4.1185055 -4.0501189 -4.0212855 -4.0554829 -4.1015043 -4.1375618 -4.1653061 -4.1749182 -4.1765256 -4.1839819 -4.2033134][-4.2566977 -4.2224894 -4.1786122 -4.1071644 -4.014884 -3.9280639 -3.9008718 -3.9710023 -4.0575128 -4.1231904 -4.1590128 -4.1625037 -4.1529126 -4.1594677 -4.1848383][-4.2234707 -4.1703258 -4.1152935 -4.0343752 -3.9312065 -3.8351057 -3.8143451 -3.9263697 -4.0497 -4.1302958 -4.1654067 -4.1606193 -4.1400886 -4.1438408 -4.1752381][-4.1972508 -4.1335387 -4.0821767 -4.0218015 -3.9557118 -3.9030592 -3.9097748 -4.0101542 -4.1031861 -4.1542058 -4.168613 -4.1542997 -4.1270452 -4.1253047 -4.157496][-4.1960716 -4.1379561 -4.1020527 -4.075511 -4.0535154 -4.041995 -4.0577917 -4.1111655 -4.1477175 -4.1564636 -4.1491265 -4.1293974 -4.1042547 -4.1067719 -4.1378021][-4.2223034 -4.179512 -4.1562338 -4.1429758 -4.1324224 -4.1275153 -4.1353259 -4.1522021 -4.1447868 -4.123292 -4.1050777 -4.0867715 -4.072855 -4.0843434 -4.1141849][-4.2542696 -4.2252803 -4.2059007 -4.1892118 -4.1692338 -4.1513557 -4.1452665 -4.135983 -4.1040425 -4.07271 -4.0608754 -4.0521159 -4.0406771 -4.0492439 -4.0783496][-4.2674842 -4.2426333 -4.2196021 -4.1934209 -4.1593957 -4.1265903 -4.1051831 -4.0816345 -4.0466061 -4.0260868 -4.0318274 -4.0355792 -4.0232344 -4.0237279 -4.0521812][-4.2593017 -4.2283053 -4.1947718 -4.1564593 -4.1135187 -4.0732288 -4.047473 -4.0268803 -4.004931 -3.9996634 -4.0181494 -4.0343328 -4.03201 -4.0343523 -4.0609183][-4.2377791 -4.1940436 -4.1457896 -4.0967946 -4.0508928 -4.0146074 -3.9991777 -3.9954562 -3.9932013 -4.001215 -4.0227251 -4.0455422 -4.0546732 -4.0641422 -4.0928936]]...]
INFO - root - 2017-12-07 22:27:14.936779: step 60910, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 50h:15m:49s remains)
INFO - root - 2017-12-07 22:27:21.767633: step 60920, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 55h:47m:48s remains)
INFO - root - 2017-12-07 22:27:28.555678: step 60930, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 53h:50m:05s remains)
INFO - root - 2017-12-07 22:27:35.214432: step 60940, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 49h:28m:38s remains)
INFO - root - 2017-12-07 22:27:41.903942: step 60950, loss = 2.10, batch loss = 2.04 (13.0 examples/sec; 0.614 sec/batch; 46h:20m:31s remains)
INFO - root - 2017-12-07 22:27:48.758388: step 60960, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 52h:08m:33s remains)
INFO - root - 2017-12-07 22:27:55.542988: step 60970, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 54h:32m:51s remains)
INFO - root - 2017-12-07 22:28:02.365352: step 60980, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.725 sec/batch; 54h:42m:37s remains)
INFO - root - 2017-12-07 22:28:09.097044: step 60990, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 51h:41m:02s remains)
INFO - root - 2017-12-07 22:28:15.753430: step 61000, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 48h:02m:13s remains)
2017-12-07 22:28:16.513024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2500472 -4.2400041 -4.2252927 -4.2122116 -4.1992083 -4.1884551 -4.1831555 -4.1856794 -4.1991792 -4.2187438 -4.237555 -4.2510734 -4.2601724 -4.2676444 -4.2724915][-4.2379289 -4.2202435 -4.2015018 -4.1890945 -4.1746106 -4.1580539 -4.1431541 -4.142498 -4.161839 -4.1900105 -4.2169852 -4.2399788 -4.2568092 -4.2680788 -4.2751341][-4.2161546 -4.18819 -4.1640325 -4.1507506 -4.1362429 -4.1160426 -4.090981 -4.0839052 -4.1088943 -4.1449814 -4.1812181 -4.2170472 -4.2444587 -4.2609386 -4.2716494][-4.1964483 -4.1544437 -4.1182475 -4.0990219 -4.0831032 -4.0590649 -4.0230269 -4.0091324 -4.0398 -4.0837154 -4.126812 -4.175456 -4.2161579 -4.2398767 -4.2556272][-4.1849313 -4.1284981 -4.0774808 -4.0483575 -4.0275545 -3.9945354 -3.9443533 -3.9212356 -3.9570677 -4.0060496 -4.0503321 -4.1059227 -4.1597118 -4.1935215 -4.2185965][-4.1883187 -4.1256514 -4.0640116 -4.0229821 -3.9924221 -3.9453371 -3.8754582 -3.8357444 -3.872097 -3.9243283 -3.9676538 -4.0271592 -4.0920968 -4.1398382 -4.1776738][-4.2112341 -4.1554508 -4.0964365 -4.0478072 -4.001349 -3.93542 -3.8494492 -3.7971065 -3.8297329 -3.8781466 -3.9201939 -3.9812813 -4.0529604 -4.1097813 -4.1569228][-4.2473159 -4.2089252 -4.1673355 -4.1242213 -4.07263 -4.002121 -3.9249024 -3.8754768 -3.8879318 -3.9184616 -3.9532082 -4.007185 -4.0714211 -4.1238046 -4.1704631][-4.2793474 -4.2584643 -4.235971 -4.2067065 -4.1643252 -4.1073985 -4.054244 -4.0187078 -4.0152397 -4.0301814 -4.0565457 -4.0974703 -4.1421719 -4.1775947 -4.2132468][-4.2995858 -4.2926888 -4.2835312 -4.2683697 -4.2423635 -4.2066593 -4.1780267 -4.1586938 -4.1511116 -4.1570821 -4.1740594 -4.1976285 -4.2200861 -4.2370796 -4.2591014][-4.3087635 -4.31007 -4.3078947 -4.3028994 -4.29074 -4.2722316 -4.2600513 -4.2517109 -4.2461476 -4.2486286 -4.2590613 -4.2704797 -4.2770791 -4.2816563 -4.2921996][-4.3092871 -4.3136678 -4.315269 -4.3144732 -4.3093305 -4.3012943 -4.2987881 -4.2985106 -4.2970538 -4.299963 -4.3048396 -4.3056731 -4.3035007 -4.3027029 -4.3061185][-4.3066592 -4.3112578 -4.3145566 -4.3158865 -4.3143411 -4.3112607 -4.3109183 -4.3119936 -4.3126512 -4.3159237 -4.3176403 -4.3145261 -4.3099637 -4.3079195 -4.308207][-4.3035026 -4.3065834 -4.3091722 -4.3114648 -4.3126206 -4.312304 -4.3124652 -4.3129063 -4.3127456 -4.313756 -4.3132486 -4.3107424 -4.3080292 -4.3067284 -4.3059936][-4.3019872 -4.303473 -4.304419 -4.3057041 -4.3066292 -4.3070941 -4.3070397 -4.3064308 -4.3059268 -4.3058467 -4.305655 -4.3050947 -4.3046327 -4.3044782 -4.3039479]]...]
INFO - root - 2017-12-07 22:28:23.368581: step 61010, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 51h:48m:23s remains)
INFO - root - 2017-12-07 22:28:30.034384: step 61020, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 48h:43m:49s remains)
INFO - root - 2017-12-07 22:28:36.780366: step 61030, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 48h:00m:02s remains)
INFO - root - 2017-12-07 22:28:43.271658: step 61040, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 51h:05m:16s remains)
INFO - root - 2017-12-07 22:28:50.011744: step 61050, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 52h:25m:30s remains)
INFO - root - 2017-12-07 22:28:56.792882: step 61060, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 47h:16m:29s remains)
INFO - root - 2017-12-07 22:29:03.579520: step 61070, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 52h:16m:15s remains)
INFO - root - 2017-12-07 22:29:10.338174: step 61080, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 47h:57m:17s remains)
INFO - root - 2017-12-07 22:29:17.051464: step 61090, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 49h:04m:40s remains)
INFO - root - 2017-12-07 22:29:23.669413: step 61100, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.759 sec/batch; 57h:12m:01s remains)
2017-12-07 22:29:24.340415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3180227 -4.3180952 -4.31665 -4.3127 -4.3086257 -4.306097 -4.3053484 -4.3057222 -4.3061662 -4.3055577 -4.3046236 -4.3029952 -4.2982273 -4.2900105 -4.2780581][-4.3223147 -4.3205533 -4.31666 -4.3108554 -4.3056779 -4.3029146 -4.3034158 -4.3060064 -4.3083539 -4.3088303 -4.3082404 -4.306788 -4.30217 -4.2941246 -4.2819953][-4.3185019 -4.316113 -4.3086448 -4.2989416 -4.2904038 -4.2859359 -4.2888145 -4.2967033 -4.3050008 -4.3103466 -4.3122153 -4.3116145 -4.3082409 -4.3021836 -4.2926178][-4.3083467 -4.3029327 -4.2887945 -4.2715826 -4.2567983 -4.2494855 -4.2559762 -4.2719097 -4.2900429 -4.3051977 -4.3131328 -4.3152227 -4.3137188 -4.3103652 -4.3041806][-4.2888384 -4.2782121 -4.2522206 -4.2207623 -4.1943789 -4.1829472 -4.1939545 -4.2203012 -4.2527041 -4.2830024 -4.3012176 -4.3083863 -4.3112111 -4.3126287 -4.3103256][-4.2538762 -4.2371054 -4.1983423 -4.1515865 -4.1140575 -4.0981526 -4.1114969 -4.1444073 -4.1866446 -4.2311387 -4.2609525 -4.275825 -4.2862077 -4.2941394 -4.2956061][-4.2005334 -4.1834612 -4.1392641 -4.0826473 -4.0342855 -4.0115252 -4.0232596 -4.0566087 -4.1012821 -4.1527462 -4.1911764 -4.2147937 -4.2359333 -4.2511539 -4.254272][-4.1434669 -4.1367216 -4.0984006 -4.0418444 -3.9854641 -3.952038 -3.9512293 -3.9748631 -4.0149522 -4.0663924 -4.1094728 -4.1405253 -4.1670637 -4.1823788 -4.1791253][-4.1123314 -4.1203337 -4.0980854 -4.052979 -3.9979982 -3.9523017 -3.9295576 -3.9346702 -3.9632771 -4.0084372 -4.0530052 -4.0870275 -4.1100831 -4.1131854 -4.0919251][-4.125402 -4.1464915 -4.1422462 -4.1138973 -4.0656705 -4.0155487 -3.9805694 -3.9765134 -3.9963481 -4.0309591 -4.06811 -4.0960846 -4.1098328 -4.0963192 -4.0593481][-4.1867204 -4.2079411 -4.211875 -4.1974192 -4.1637554 -4.1267 -4.1010337 -4.0997286 -4.1129117 -4.1334314 -4.1552596 -4.1701212 -4.1726356 -4.1498446 -4.110312][-4.2557421 -4.2718906 -4.2772174 -4.2702942 -4.2505755 -4.228035 -4.211606 -4.20962 -4.2174592 -4.2294888 -4.2418156 -4.2488017 -4.2479272 -4.2286444 -4.1997886][-4.3005548 -4.3100805 -4.3143682 -4.3106289 -4.2991 -4.2842941 -4.2710376 -4.263361 -4.2633705 -4.2692828 -4.2804542 -4.2894697 -4.2925673 -4.282794 -4.2666311][-4.3232784 -4.3271027 -4.3296003 -4.3237963 -4.309824 -4.2909846 -4.2699337 -4.24928 -4.2388077 -4.2441783 -4.2654662 -4.2894688 -4.3045058 -4.3062339 -4.2997813][-4.3293929 -4.3296928 -4.3281312 -4.3135686 -4.2841463 -4.2467108 -4.2066965 -4.1682038 -4.147151 -4.1572757 -4.196312 -4.2433186 -4.2763948 -4.2918849 -4.2919912]]...]
INFO - root - 2017-12-07 22:29:31.136371: step 61110, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 51h:59m:07s remains)
INFO - root - 2017-12-07 22:29:37.892544: step 61120, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 48h:25m:14s remains)
INFO - root - 2017-12-07 22:29:44.690381: step 61130, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 52h:58m:20s remains)
INFO - root - 2017-12-07 22:29:51.536972: step 61140, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 56h:02m:08s remains)
INFO - root - 2017-12-07 22:29:58.361110: step 61150, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 51h:01m:00s remains)
INFO - root - 2017-12-07 22:30:05.076075: step 61160, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 46h:58m:46s remains)
INFO - root - 2017-12-07 22:30:11.852687: step 61170, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 48h:52m:37s remains)
INFO - root - 2017-12-07 22:30:18.649234: step 61180, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 55h:20m:38s remains)
INFO - root - 2017-12-07 22:30:25.457038: step 61190, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 52h:21m:04s remains)
INFO - root - 2017-12-07 22:30:32.158701: step 61200, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 50h:38m:52s remains)
2017-12-07 22:30:32.872266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2887897 -4.26832 -4.2642946 -4.2621927 -4.2429075 -4.2022996 -4.1499176 -4.09838 -4.0661545 -4.0849361 -4.1520209 -4.2198243 -4.2773252 -4.31623 -4.3373289][-4.2624168 -4.2334771 -4.223968 -4.2262526 -4.2170177 -4.1859241 -4.139802 -4.0875087 -4.0523906 -4.0749044 -4.1480393 -4.2196631 -4.2788854 -4.3180203 -4.3384953][-4.2343388 -4.1993561 -4.185205 -4.1884675 -4.1858826 -4.1660557 -4.1287737 -4.0747323 -4.037683 -4.0650349 -4.1422629 -4.2185755 -4.2800145 -4.3188038 -4.3388081][-4.2069774 -4.1701994 -4.1527519 -4.1522841 -4.1504259 -4.1377535 -4.1089697 -4.0572624 -4.0244684 -4.0570393 -4.1362705 -4.2167292 -4.2803955 -4.3191452 -4.3386636][-4.1886349 -4.1541266 -4.1351614 -4.1277413 -4.1198192 -4.1049895 -4.0787325 -4.0276833 -4.0024915 -4.0436397 -4.1276197 -4.2127428 -4.2793036 -4.3191462 -4.3385696][-4.1902623 -4.157465 -4.1336679 -4.1165423 -4.0984321 -4.07331 -4.0408773 -3.9849269 -3.9640472 -4.017868 -4.1123834 -4.2051148 -4.2776132 -4.3193345 -4.3389516][-4.21263 -4.1836934 -4.1593404 -4.1348572 -4.1039844 -4.0599217 -4.0086637 -3.9376864 -3.9148736 -3.9842856 -4.0932927 -4.1954327 -4.2747741 -4.3189268 -4.33954][-4.248014 -4.2287412 -4.2104764 -4.1852751 -4.1428084 -4.0758915 -3.9970276 -3.9024737 -3.8702168 -3.9538321 -4.0786176 -4.1887536 -4.273047 -4.3189735 -4.3406129][-4.2828007 -4.2752423 -4.2684512 -4.248879 -4.2022452 -4.1195045 -4.0187273 -3.9040248 -3.8551984 -3.9376464 -4.0694284 -4.1838255 -4.2712502 -4.3194594 -4.3418231][-4.3047729 -4.3056893 -4.3076544 -4.29646 -4.256145 -4.1741943 -4.06649 -3.9433563 -3.8729575 -3.9330728 -4.0613489 -4.178833 -4.2689381 -4.3193488 -4.3427157][-4.3199139 -4.3252263 -4.3294287 -4.3237724 -4.2938004 -4.2227292 -4.1200953 -3.996371 -3.905952 -3.9377294 -4.0586252 -4.1780438 -4.26963 -4.3198924 -4.3434548][-4.3295722 -4.3363681 -4.3408046 -4.3386226 -4.3196397 -4.2627721 -4.1704731 -4.0514383 -3.94858 -3.9562533 -4.0662975 -4.1829886 -4.2731447 -4.3217983 -4.3444767][-4.3354917 -4.3409605 -4.343349 -4.3427205 -4.3342915 -4.2925053 -4.2124977 -4.1014619 -3.9949014 -3.9846163 -4.0797377 -4.1897664 -4.2756786 -4.3234668 -4.3451843][-4.3359475 -4.3371468 -4.3364682 -4.3368921 -4.3353505 -4.3069339 -4.2416143 -4.1429944 -4.0413876 -4.0185738 -4.0953593 -4.1961074 -4.2770882 -4.3242354 -4.3457937][-4.3334942 -4.3300118 -4.3247843 -4.324296 -4.3265133 -4.3087087 -4.2578535 -4.1751561 -4.0834479 -4.0525742 -4.110682 -4.2020206 -4.2787423 -4.3247571 -4.3462324]]...]
INFO - root - 2017-12-07 22:30:39.730324: step 61210, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 53h:35m:27s remains)
INFO - root - 2017-12-07 22:30:46.655545: step 61220, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 55h:25m:54s remains)
INFO - root - 2017-12-07 22:30:53.449309: step 61230, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 48h:54m:24s remains)
INFO - root - 2017-12-07 22:31:00.208953: step 61240, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 47h:50m:13s remains)
INFO - root - 2017-12-07 22:31:07.180833: step 61250, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 50h:22m:05s remains)
INFO - root - 2017-12-07 22:31:14.108956: step 61260, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 55h:54m:36s remains)
INFO - root - 2017-12-07 22:31:20.868346: step 61270, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 51h:46m:45s remains)
INFO - root - 2017-12-07 22:31:27.703918: step 61280, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 48h:22m:22s remains)
INFO - root - 2017-12-07 22:31:34.387734: step 61290, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 48h:02m:16s remains)
INFO - root - 2017-12-07 22:31:41.029769: step 61300, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 54h:34m:49s remains)
2017-12-07 22:31:41.759464: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1777616 -4.1811719 -4.1950769 -4.2009473 -4.20323 -4.2049007 -4.2059236 -4.2066531 -4.2066236 -4.2102818 -4.2104335 -4.2106161 -4.2181163 -4.2299514 -4.2517786][-4.1870532 -4.1885548 -4.2014937 -4.2060761 -4.2098026 -4.2106104 -4.2070704 -4.2039361 -4.20143 -4.2015185 -4.1981487 -4.1944194 -4.1998472 -4.2108893 -4.2354088][-4.2227287 -4.2259564 -4.2378259 -4.2389474 -4.2373281 -4.230731 -4.2189093 -4.2119689 -4.2096043 -4.208694 -4.2044668 -4.1973863 -4.199234 -4.2077007 -4.2317085][-4.24609 -4.2475963 -4.2497549 -4.2408929 -4.2259603 -4.2069669 -4.1873231 -4.1784358 -4.1816068 -4.18997 -4.1940637 -4.1923227 -4.1933594 -4.1999493 -4.2229767][-4.2313261 -4.2289538 -4.22288 -4.2016211 -4.1692815 -4.1347756 -4.1121225 -4.1072807 -4.1204648 -4.1488328 -4.17431 -4.18531 -4.1874127 -4.1920862 -4.2151284][-4.2249074 -4.2223754 -4.2117181 -4.1802936 -4.1316986 -4.0803685 -4.0482016 -4.0430965 -4.0638733 -4.1105614 -4.1578817 -4.1849389 -4.1897936 -4.1888213 -4.2073183][-4.2127342 -4.2115936 -4.1951394 -4.1543679 -4.0937119 -4.0293083 -3.9857707 -3.9753931 -3.9962044 -4.0496831 -4.1094003 -4.1491656 -4.1591744 -4.155755 -4.1733613][-4.1546726 -4.1539936 -4.1383824 -4.0988851 -4.0400085 -3.978272 -3.9337294 -3.9199057 -3.9384995 -3.9892571 -4.0542812 -4.1007948 -4.1135349 -4.1082416 -4.1288109][-4.1225038 -4.1301541 -4.1280284 -4.1077962 -4.0734825 -4.0343208 -3.9977698 -3.9806991 -3.9922071 -4.0288248 -4.079319 -4.1135936 -4.1201687 -4.1110806 -4.1286626][-4.1543307 -4.168808 -4.1784482 -4.1763048 -4.1637058 -4.1424575 -4.1113586 -4.0897093 -4.0946717 -4.11679 -4.1467595 -4.1618862 -4.1594505 -4.1468067 -4.1583252][-4.2001848 -4.211885 -4.2224607 -4.2239552 -4.2169166 -4.20157 -4.17597 -4.1561251 -4.1620312 -4.1818213 -4.2018132 -4.206738 -4.1999478 -4.1885252 -4.1964498][-4.238462 -4.2469153 -4.2551579 -4.2559218 -4.2489491 -4.2364759 -4.218431 -4.2053504 -4.2134709 -4.2331719 -4.2496023 -4.2525458 -4.2456141 -4.2378578 -4.2425871][-4.2725754 -4.2793684 -4.2873869 -4.2879438 -4.2809477 -4.2705731 -4.2560754 -4.2458258 -4.2530303 -4.2712045 -4.2885323 -4.293941 -4.2902532 -4.28386 -4.2849245][-4.2973776 -4.3029213 -4.3111615 -4.3132639 -4.3081388 -4.30151 -4.2924042 -4.2849474 -4.2891951 -4.3052478 -4.3232169 -4.3301992 -4.3281507 -4.321732 -4.3199553][-4.3153343 -4.3179083 -4.3238025 -4.3255258 -4.322463 -4.3201694 -4.3177314 -4.3158884 -4.320631 -4.3349323 -4.35102 -4.3580647 -4.3570886 -4.3508639 -4.3457117]]...]
INFO - root - 2017-12-07 22:31:48.578062: step 61310, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 0.620 sec/batch; 46h:41m:37s remains)
INFO - root - 2017-12-07 22:31:55.365076: step 61320, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 48h:10m:14s remains)
INFO - root - 2017-12-07 22:32:02.164508: step 61330, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.732 sec/batch; 55h:10m:08s remains)
INFO - root - 2017-12-07 22:32:09.187698: step 61340, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.722 sec/batch; 54h:23m:42s remains)
INFO - root - 2017-12-07 22:32:15.923078: step 61350, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 48h:01m:55s remains)
INFO - root - 2017-12-07 22:32:22.708641: step 61360, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 48h:03m:22s remains)
INFO - root - 2017-12-07 22:32:29.557045: step 61370, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 53h:44m:54s remains)
INFO - root - 2017-12-07 22:32:36.423722: step 61380, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 52h:34m:50s remains)
INFO - root - 2017-12-07 22:32:43.243472: step 61390, loss = 2.11, batch loss = 2.06 (11.6 examples/sec; 0.691 sec/batch; 52h:00m:38s remains)
INFO - root - 2017-12-07 22:32:49.890099: step 61400, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 49h:22m:09s remains)
2017-12-07 22:32:50.636426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2501616 -4.2435308 -4.24066 -4.2436652 -4.2481604 -4.2491531 -4.2544422 -4.2632632 -4.2674189 -4.2683258 -4.2695827 -4.2687292 -4.265132 -4.2661033 -4.271481][-4.2332888 -4.2270918 -4.2251239 -4.2286253 -4.2340708 -4.237051 -4.2464895 -4.258863 -4.2657042 -4.2675118 -4.2682662 -4.2674031 -4.2640495 -4.2632008 -4.2665105][-4.19917 -4.1927361 -4.1912589 -4.1948738 -4.1997972 -4.2060318 -4.2234817 -4.2425637 -4.2540541 -4.2586732 -4.2608624 -4.2618494 -4.261539 -4.2615976 -4.2638273][-4.1505113 -4.14592 -4.1463513 -4.149508 -4.1534324 -4.1619773 -4.1887455 -4.2160935 -4.2313476 -4.2359915 -4.2362709 -4.2381582 -4.2416759 -4.2451968 -4.2486062][-4.10607 -4.1034307 -4.1065025 -4.1076884 -4.1047797 -4.1092529 -4.1393533 -4.1720352 -4.1879511 -4.1915245 -4.1869688 -4.1860423 -4.192091 -4.1994705 -4.2079496][-4.0856571 -4.0823445 -4.0833673 -4.0770779 -4.0595603 -4.0499945 -4.0727792 -4.1036696 -4.1211591 -4.1273484 -4.1215472 -4.1166105 -4.1228604 -4.1338186 -4.1475692][-4.0989933 -4.0887756 -4.0820236 -4.0634522 -4.028873 -4.0004163 -4.0022936 -4.0231853 -4.0454679 -4.0605774 -4.0605068 -4.0556316 -4.0604525 -4.0722136 -4.0837021][-4.1447811 -4.1277452 -4.1116686 -4.0848269 -4.0428834 -4.0020185 -3.9779415 -3.9763429 -3.9967139 -4.0168233 -4.0244827 -4.0226254 -4.0270591 -4.0346804 -4.03338][-4.1927347 -4.1772003 -4.1609983 -4.1372962 -4.1029272 -4.0660615 -4.0329595 -4.0148463 -4.0196452 -4.0270262 -4.0297771 -4.0283384 -4.0318003 -4.0318389 -4.0146585][-4.2201324 -4.2115641 -4.2015061 -4.1875644 -4.1670628 -4.1431851 -4.1162243 -4.0936522 -4.0836964 -4.0772691 -4.073585 -4.0711312 -4.0730748 -4.06599 -4.0410762][-4.219296 -4.2177372 -4.2135873 -4.2084808 -4.2009568 -4.1900868 -4.17239 -4.1514249 -4.1369953 -4.1276751 -4.1242414 -4.1225643 -4.1240544 -4.1155195 -4.0914397][-4.2053227 -4.2073774 -4.207355 -4.2075939 -4.2075315 -4.2059841 -4.1963487 -4.179359 -4.1653352 -4.1602411 -4.1616659 -4.1628981 -4.1660528 -4.1604457 -4.1440568][-4.2000704 -4.2010765 -4.2022057 -4.2048135 -4.2080717 -4.2109413 -4.2083821 -4.1963105 -4.1836882 -4.1821113 -4.1871605 -4.1899652 -4.1934748 -4.1915975 -4.1852517][-4.2101359 -4.2085366 -4.2086091 -4.2118087 -4.2150316 -4.218513 -4.221189 -4.2149711 -4.20551 -4.2055326 -4.2100205 -4.2110624 -4.2117372 -4.2116947 -4.2114687][-4.2349739 -4.2332044 -4.2333384 -4.236403 -4.2391405 -4.2418218 -4.2463126 -4.24409 -4.2377014 -4.2367225 -4.23854 -4.2369552 -4.2343006 -4.2333455 -4.2342758]]...]
INFO - root - 2017-12-07 22:32:57.455116: step 61410, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 54h:50m:01s remains)
INFO - root - 2017-12-07 22:33:04.357549: step 61420, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 51h:20m:24s remains)
INFO - root - 2017-12-07 22:33:11.156287: step 61430, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 47h:23m:51s remains)
INFO - root - 2017-12-07 22:33:17.933813: step 61440, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.666 sec/batch; 50h:10m:17s remains)
INFO - root - 2017-12-07 22:33:24.903638: step 61450, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 54h:43m:13s remains)
INFO - root - 2017-12-07 22:33:31.566760: step 61460, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 55h:15m:17s remains)
INFO - root - 2017-12-07 22:33:38.301973: step 61470, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 53h:44m:57s remains)
INFO - root - 2017-12-07 22:33:45.124491: step 61480, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 47h:45m:19s remains)
INFO - root - 2017-12-07 22:33:51.936288: step 61490, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 52h:07m:54s remains)
INFO - root - 2017-12-07 22:33:58.681349: step 61500, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 54h:46m:57s remains)
2017-12-07 22:33:59.457945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2686129 -4.2699575 -4.2700725 -4.26688 -4.2626152 -4.2602367 -4.259171 -4.2613459 -4.2664242 -4.2716289 -4.2760358 -4.2792344 -4.2782211 -4.2741051 -4.2683244][-4.2632074 -4.2636385 -4.2618413 -4.2571225 -4.2520466 -4.25138 -4.2538772 -4.2592216 -4.26535 -4.2703209 -4.2747426 -4.279129 -4.27991 -4.2763233 -4.2701807][-4.2556167 -4.2529039 -4.2473521 -4.2392955 -4.2308364 -4.2285576 -4.2324562 -4.2412734 -4.2503166 -4.2555881 -4.2590756 -4.2648516 -4.2699685 -4.2707038 -4.2681885][-4.2496829 -4.2426581 -4.2320714 -4.2180424 -4.2021241 -4.193089 -4.1927843 -4.2025933 -4.2160463 -4.222403 -4.2235317 -4.2307358 -4.2433844 -4.2516837 -4.2564631][-4.2444968 -4.2340665 -4.2191281 -4.198029 -4.1719589 -4.1505842 -4.1380782 -4.1461854 -4.165597 -4.1751018 -4.1741276 -4.1831841 -4.20427 -4.2206268 -4.2345371][-4.2349849 -4.2228847 -4.2044621 -4.1762676 -4.1396689 -4.1027536 -4.071445 -4.0758018 -4.1064563 -4.1236053 -4.1244807 -4.1365218 -4.16413 -4.1872568 -4.2080636][-4.2188611 -4.206306 -4.1850448 -4.151217 -4.10704 -4.054996 -4.0029798 -4.0043941 -4.051476 -4.0800133 -4.0861416 -4.1031518 -4.134779 -4.16099 -4.1830878][-4.2025208 -4.1912084 -4.1701207 -4.135232 -4.088129 -4.0272822 -3.9621804 -3.9628758 -4.0218258 -4.0594263 -4.0712948 -4.0912418 -4.1226778 -4.1467791 -4.1633558][-4.1936212 -4.185092 -4.1685376 -4.140758 -4.1005044 -4.044168 -3.9845934 -3.9845834 -4.0353427 -4.0672264 -4.0774064 -4.0941539 -4.1193323 -4.1356034 -4.1434374][-4.1946983 -4.1889858 -4.177597 -4.1604729 -4.1329608 -4.0913563 -4.046977 -4.0451431 -4.0770445 -4.0946174 -4.0982571 -4.1079621 -4.1217866 -4.1276073 -4.1274295][-4.2101398 -4.2066712 -4.198143 -4.1878843 -4.1698813 -4.1421366 -4.1115251 -4.1051068 -4.1185503 -4.1240563 -4.1219087 -4.1253819 -4.1296048 -4.127594 -4.1218438][-4.2364564 -4.2347546 -4.2277622 -4.2205195 -4.2078567 -4.1887693 -4.1673641 -4.15751 -4.1590047 -4.1572971 -4.1516314 -4.1513076 -4.1510596 -4.1456795 -4.1384711][-4.2652488 -4.2640905 -4.2587743 -4.2536464 -4.2452226 -4.2323861 -4.2175827 -4.2064223 -4.2009249 -4.1960011 -4.1913581 -4.1919718 -4.1914959 -4.1868439 -4.182157][-4.2891216 -4.2888131 -4.2857394 -4.2829361 -4.2787304 -4.2711468 -4.2612233 -4.2509985 -4.2433491 -4.2385774 -4.2367077 -4.2389188 -4.2391405 -4.2360029 -4.2340212][-4.303009 -4.3038278 -4.3027182 -4.3017907 -4.3003092 -4.2964611 -4.2904429 -4.2828026 -4.2768278 -4.2738819 -4.2739844 -4.2764239 -4.276792 -4.2752318 -4.2745008]]...]
INFO - root - 2017-12-07 22:34:06.272008: step 61510, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 49h:21m:26s remains)
INFO - root - 2017-12-07 22:34:13.084020: step 61520, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 52h:39m:20s remains)
INFO - root - 2017-12-07 22:34:19.933032: step 61530, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 52h:49m:10s remains)
INFO - root - 2017-12-07 22:34:26.824279: step 61540, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 54h:16m:03s remains)
INFO - root - 2017-12-07 22:34:33.651604: step 61550, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 47h:38m:28s remains)
INFO - root - 2017-12-07 22:34:40.450312: step 61560, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 48h:28m:28s remains)
INFO - root - 2017-12-07 22:34:47.219136: step 61570, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 52h:30m:11s remains)
INFO - root - 2017-12-07 22:34:53.997444: step 61580, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 55h:05m:47s remains)
INFO - root - 2017-12-07 22:35:00.846961: step 61590, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 50h:05m:45s remains)
INFO - root - 2017-12-07 22:35:07.483583: step 61600, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 49h:46m:41s remains)
2017-12-07 22:35:08.278506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3203578 -4.324337 -4.3395104 -4.35953 -4.3769407 -4.385828 -4.3625593 -4.2957687 -4.1971407 -4.0947671 -4.053863 -4.0990224 -4.1846852 -4.259912 -4.3100457][-4.3605247 -4.3771181 -4.396276 -4.4068308 -4.406374 -4.3940692 -4.3548932 -4.2807918 -4.185452 -4.1016617 -4.0844355 -4.1415839 -4.2220874 -4.2887983 -4.3350315][-4.3812318 -4.4042363 -4.4214835 -4.422452 -4.4038916 -4.3711243 -4.3172011 -4.2399926 -4.1600013 -4.1107216 -4.1219835 -4.1894679 -4.2648311 -4.3184514 -4.354311][-4.3933387 -4.4148874 -4.4237885 -4.4084229 -4.3656669 -4.3089471 -4.2374806 -4.1606846 -4.1160464 -4.11532 -4.1545954 -4.2230992 -4.2880883 -4.3324175 -4.35991][-4.3918171 -4.40744 -4.4039769 -4.36355 -4.2883053 -4.1992006 -4.09993 -4.0234456 -4.0267925 -4.0877218 -4.1609392 -4.2337508 -4.290328 -4.3302183 -4.3541374][-4.3786845 -4.3897204 -4.3690715 -4.2948589 -4.1770473 -4.0425882 -3.894731 -3.8104384 -3.8772466 -4.0113111 -4.1275954 -4.2173038 -4.2776327 -4.320539 -4.346993][-4.3535891 -4.3612185 -4.3252859 -4.2256136 -4.0740547 -3.8879495 -3.6848814 -3.6023884 -3.741035 -3.934587 -4.0861449 -4.1941538 -4.264297 -4.3126488 -4.343338][-4.3289623 -4.3359537 -4.2977138 -4.1967993 -4.040309 -3.8442118 -3.6477311 -3.6081343 -3.7677433 -3.955225 -4.0994177 -4.2026558 -4.2708964 -4.3163719 -4.3452048][-4.3073578 -4.3207121 -4.2960477 -4.2190218 -4.0962982 -3.9475329 -3.8219619 -3.82108 -3.9357784 -4.0667109 -4.1721816 -4.2507725 -4.3015261 -4.3337331 -4.3542504][-4.2814922 -4.3077769 -4.303812 -4.2598357 -4.1809292 -4.08831 -4.0260262 -4.0389581 -4.10611 -4.1847672 -4.2535915 -4.304503 -4.3358769 -4.3547134 -4.3656349][-4.2645354 -4.3001194 -4.3078742 -4.2841148 -4.2377772 -4.1881027 -4.1672497 -4.1857634 -4.22551 -4.2722816 -4.3113647 -4.3414063 -4.3601379 -4.369051 -4.3722363][-4.2686567 -4.3053145 -4.3150334 -4.2986221 -4.2728343 -4.24928 -4.2506919 -4.2729063 -4.3007689 -4.3271928 -4.3448553 -4.3607121 -4.3701735 -4.3728814 -4.3718519][-4.2952118 -4.3211861 -4.3221726 -4.3021789 -4.280365 -4.2689981 -4.2813663 -4.3074141 -4.3309846 -4.3464155 -4.353344 -4.3609314 -4.3665824 -4.3684459 -4.3659849][-4.324368 -4.3370419 -4.3252258 -4.293407 -4.26448 -4.2563758 -4.2751341 -4.3071389 -4.3309679 -4.341238 -4.3434935 -4.3479509 -4.3544865 -4.3599586 -4.35949][-4.3461986 -4.3471937 -4.3241081 -4.2819805 -4.2455168 -4.2374625 -4.2621136 -4.2992587 -4.3245516 -4.333333 -4.3337522 -4.336237 -4.3443632 -4.3532896 -4.3565736]]...]
INFO - root - 2017-12-07 22:35:15.160380: step 61610, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 53h:52m:03s remains)
INFO - root - 2017-12-07 22:35:21.887399: step 61620, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 51h:44m:45s remains)
INFO - root - 2017-12-07 22:35:28.709024: step 61630, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 48h:06m:57s remains)
INFO - root - 2017-12-07 22:35:35.524454: step 61640, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 49h:39m:11s remains)
INFO - root - 2017-12-07 22:35:42.334531: step 61650, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.741 sec/batch; 55h:44m:31s remains)
INFO - root - 2017-12-07 22:35:48.954076: step 61660, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 49h:27m:18s remains)
INFO - root - 2017-12-07 22:35:55.787214: step 61670, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 48h:38m:05s remains)
INFO - root - 2017-12-07 22:36:02.611211: step 61680, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 54h:01m:25s remains)
INFO - root - 2017-12-07 22:36:09.458273: step 61690, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 52h:54m:47s remains)
INFO - root - 2017-12-07 22:36:16.062479: step 61700, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 50h:22m:40s remains)
2017-12-07 22:36:16.876520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2501507 -4.2520404 -4.2508855 -4.243876 -4.229558 -4.2168179 -4.2024117 -4.2026906 -4.2077847 -4.2088675 -4.2169256 -4.2170606 -4.209157 -4.1997313 -4.2056341][-4.2427354 -4.2400246 -4.2376752 -4.230473 -4.2166266 -4.2101035 -4.2045226 -4.2068877 -4.2153616 -4.2173853 -4.2256804 -4.2243676 -4.2112041 -4.197711 -4.1981568][-4.2241616 -4.2246847 -4.2296453 -4.2312446 -4.2183285 -4.2129478 -4.2112174 -4.2078948 -4.2151575 -4.220057 -4.2297983 -4.2291245 -4.2103281 -4.1901188 -4.1861634][-4.1994643 -4.2091045 -4.2220416 -4.2315297 -4.2209225 -4.2138443 -4.2118788 -4.2001605 -4.208066 -4.2201939 -4.2328391 -4.2305965 -4.202827 -4.1763043 -4.173614][-4.1845336 -4.2002268 -4.2152319 -4.2287312 -4.2258306 -4.2182593 -4.215549 -4.1991863 -4.2082834 -4.2264018 -4.2390776 -4.2351613 -4.2042027 -4.1770158 -4.1771388][-4.1786413 -4.1996112 -4.2138796 -4.2249889 -4.2263384 -4.2174129 -4.218791 -4.2069678 -4.2189145 -4.2366323 -4.2450066 -4.242363 -4.215404 -4.1886396 -4.1883869][-4.1743584 -4.1941948 -4.2031164 -4.206162 -4.2053332 -4.1960497 -4.2000823 -4.1904621 -4.1988473 -4.216888 -4.2302856 -4.2358551 -4.2126374 -4.185565 -4.1877961][-4.1699114 -4.1811728 -4.181385 -4.1781635 -4.1777678 -4.1774035 -4.1790624 -4.1562285 -4.1517797 -4.172966 -4.1968379 -4.2114773 -4.1917372 -4.1643114 -4.1693082][-4.1622195 -4.1667628 -4.17055 -4.1767464 -4.1859188 -4.1897063 -4.1792064 -4.1382523 -4.1218128 -4.1455307 -4.1751523 -4.1928082 -4.1785846 -4.1560483 -4.1575656][-4.1556692 -4.1648474 -4.1828542 -4.2023907 -4.2181277 -4.2218456 -4.20398 -4.1617002 -4.1440177 -4.1639376 -4.1852779 -4.1976 -4.188159 -4.1713138 -4.1685462][-4.1610923 -4.1756859 -4.2030048 -4.2272191 -4.2431464 -4.2465086 -4.2325115 -4.2034883 -4.1935077 -4.2076278 -4.2179804 -4.2205338 -4.2083864 -4.1919408 -4.1872487][-4.1743054 -4.1927752 -4.2201977 -4.2392731 -4.2522874 -4.2587347 -4.250618 -4.2342038 -4.2328649 -4.2443237 -4.2494197 -4.2425036 -4.2251687 -4.2099743 -4.2073593][-4.18458 -4.2005658 -4.2243686 -4.23627 -4.2446876 -4.253716 -4.2495842 -4.2446032 -4.2499547 -4.2633924 -4.2707744 -4.2618275 -4.2455559 -4.2341652 -4.2345209][-4.1901646 -4.2092528 -4.235446 -4.2444649 -4.2501516 -4.2563043 -4.2499628 -4.2468433 -4.2532053 -4.2676711 -4.2798128 -4.2767687 -4.268126 -4.2631598 -4.2662139][-4.20201 -4.2280569 -4.2572083 -4.26426 -4.2662253 -4.2660074 -4.2576151 -4.2551107 -4.2625337 -4.2784781 -4.2931871 -4.2946577 -4.2902994 -4.2885408 -4.2916083]]...]
INFO - root - 2017-12-07 22:36:23.749372: step 61710, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 55h:17m:12s remains)
INFO - root - 2017-12-07 22:36:30.588801: step 61720, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.653 sec/batch; 49h:05m:21s remains)
INFO - root - 2017-12-07 22:36:37.335570: step 61730, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 48h:24m:13s remains)
INFO - root - 2017-12-07 22:36:44.118343: step 61740, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 49h:05m:29s remains)
INFO - root - 2017-12-07 22:36:51.035925: step 61750, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 52h:55m:19s remains)
INFO - root - 2017-12-07 22:36:57.911306: step 61760, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 53h:31m:03s remains)
INFO - root - 2017-12-07 22:37:04.680153: step 61770, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 50h:50m:59s remains)
INFO - root - 2017-12-07 22:37:11.541846: step 61780, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 48h:25m:27s remains)
INFO - root - 2017-12-07 22:37:18.389535: step 61790, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 47h:53m:23s remains)
INFO - root - 2017-12-07 22:37:25.096772: step 61800, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 52h:24m:24s remains)
2017-12-07 22:37:25.836658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2880492 -4.2940917 -4.3035588 -4.312098 -4.3166852 -4.3185382 -4.3194976 -4.3196239 -4.3191533 -4.3185635 -4.3183661 -4.3178968 -4.3157754 -4.31245 -4.3091455][-4.3062196 -4.313087 -4.3222103 -4.3300862 -4.3342657 -4.3355975 -4.3352976 -4.3336859 -4.3322229 -4.3315206 -4.3319826 -4.33307 -4.3334517 -4.3321161 -4.3290248][-4.3215761 -4.3277578 -4.3340807 -4.3384981 -4.3393645 -4.3367972 -4.3314919 -4.3258209 -4.3227291 -4.323926 -4.32877 -4.3354297 -4.340816 -4.342092 -4.3400331][-4.3200307 -4.3247705 -4.3279471 -4.3281536 -4.3242288 -4.3158865 -4.3037491 -4.2919464 -4.2865181 -4.2902317 -4.3012652 -4.316535 -4.3299389 -4.3368459 -4.3376079][-4.303618 -4.3050003 -4.30453 -4.3001156 -4.2910624 -4.2769241 -4.257338 -4.2371988 -4.22467 -4.2268496 -4.2422562 -4.2663608 -4.2919931 -4.3096476 -4.3180985][-4.2767324 -4.2755203 -4.273006 -4.2655568 -4.2523932 -4.2325888 -4.2052336 -4.1743259 -4.1506953 -4.1472821 -4.1658292 -4.1992016 -4.2363462 -4.2649632 -4.2837033][-4.2501726 -4.247797 -4.2452693 -4.2364764 -4.2210879 -4.1990385 -4.1692438 -4.1322174 -4.0995197 -4.0880575 -4.10388 -4.1386452 -4.1798306 -4.21571 -4.2437305][-4.2338486 -4.2316523 -4.2299595 -4.220912 -4.2056847 -4.1866069 -4.1625509 -4.1308908 -4.0992718 -4.0822897 -4.0880365 -4.1132994 -4.1480126 -4.183053 -4.2135816][-4.2440238 -4.2437038 -4.242981 -4.2332158 -4.2173944 -4.2014141 -4.1856403 -4.1663432 -4.1459947 -4.1333284 -4.1345339 -4.1486497 -4.1702724 -4.1935678 -4.215528][-4.270668 -4.270771 -4.2677941 -4.2547555 -4.2366676 -4.2219973 -4.2122741 -4.2046981 -4.1987157 -4.1978827 -4.2036743 -4.2130966 -4.2234716 -4.2334385 -4.243124][-4.2907534 -4.2889419 -4.2818885 -4.2656116 -4.24586 -4.2319021 -4.2263513 -4.2266121 -4.2315121 -4.2414918 -4.2544327 -4.26507 -4.2703681 -4.2729096 -4.2743878][-4.3002663 -4.2978687 -4.2897038 -4.2745681 -4.2575126 -4.246192 -4.2429595 -4.2447529 -4.250998 -4.2621784 -4.2763872 -4.2880268 -4.2923174 -4.292037 -4.2891936][-4.307539 -4.3060727 -4.2994885 -4.288497 -4.2764654 -4.2689171 -4.2670894 -4.2666512 -4.2680635 -4.2739196 -4.2833877 -4.2918563 -4.2938104 -4.2916121 -4.2859507][-4.3131418 -4.312192 -4.3072333 -4.299809 -4.2926326 -4.2898555 -4.290719 -4.2894316 -4.2857614 -4.283247 -4.283124 -4.2833214 -4.2799573 -4.274395 -4.2660646][-4.322135 -4.3213038 -4.3171558 -4.3116608 -4.307457 -4.3071408 -4.3085346 -4.3060575 -4.299 -4.2907109 -4.2829957 -4.2754083 -4.2666268 -4.2571759 -4.2462835]]...]
INFO - root - 2017-12-07 22:37:32.609570: step 61810, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 49h:03m:58s remains)
INFO - root - 2017-12-07 22:37:39.471439: step 61820, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 48h:35m:41s remains)
INFO - root - 2017-12-07 22:37:46.350426: step 61830, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 53h:40m:20s remains)
INFO - root - 2017-12-07 22:37:53.153381: step 61840, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 53h:47m:54s remains)
INFO - root - 2017-12-07 22:37:59.966535: step 61850, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.703 sec/batch; 52h:50m:09s remains)
INFO - root - 2017-12-07 22:38:06.707755: step 61860, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 49h:00m:37s remains)
INFO - root - 2017-12-07 22:38:13.550094: step 61870, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 49h:31m:35s remains)
INFO - root - 2017-12-07 22:38:20.472184: step 61880, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 54h:57m:12s remains)
INFO - root - 2017-12-07 22:38:27.274023: step 61890, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 54h:36m:17s remains)
INFO - root - 2017-12-07 22:38:33.956663: step 61900, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 50h:41m:07s remains)
2017-12-07 22:38:34.691476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2787437 -4.2984266 -4.3100257 -4.2917209 -4.251852 -4.2018971 -4.1441584 -4.1137676 -4.1196122 -4.149292 -4.1990404 -4.256259 -4.2904439 -4.3081303 -4.315743][-4.2762671 -4.2955413 -4.308435 -4.2893128 -4.2457089 -4.190784 -4.12599 -4.0943308 -4.104991 -4.1408296 -4.196744 -4.2583408 -4.2944779 -4.3131928 -4.321136][-4.275507 -4.29144 -4.3018069 -4.283958 -4.24358 -4.1889892 -4.1169481 -4.0757308 -4.0805521 -4.1158886 -4.1793356 -4.2485676 -4.2903438 -4.3132405 -4.3229804][-4.2725878 -4.2845492 -4.2914891 -4.2759 -4.2407589 -4.1898236 -4.1124196 -4.0566039 -4.0476866 -4.0803061 -4.1524038 -4.2313433 -4.2803664 -4.3104334 -4.3229871][-4.2662692 -4.2748876 -4.2780681 -4.2630644 -4.2314668 -4.1834545 -4.103363 -4.0362849 -4.0159435 -4.0491834 -4.1308107 -4.2167234 -4.2712078 -4.3063936 -4.3216658][-4.2577147 -4.263845 -4.2632632 -4.2482839 -4.2190137 -4.173131 -4.095377 -4.0270605 -4.0053172 -4.0438938 -4.1321926 -4.2185073 -4.2723174 -4.3069434 -4.3221092][-4.245389 -4.2494245 -4.2476044 -4.2351632 -4.2078233 -4.1640334 -4.0920935 -4.0295672 -4.0164557 -4.0632534 -4.1502428 -4.2309265 -4.2814527 -4.3127475 -4.325007][-4.2149382 -4.2164979 -4.2179723 -4.2136564 -4.1931195 -4.1555462 -4.0947547 -4.0464606 -4.0441623 -4.0930958 -4.1713595 -4.2431197 -4.2900934 -4.3186784 -4.3280468][-4.1650023 -4.1620159 -4.1725755 -4.1811237 -4.1717453 -4.1457672 -4.1023932 -4.0718884 -4.0778828 -4.1214085 -4.186718 -4.2516289 -4.2971487 -4.3237457 -4.3302255][-4.1255107 -4.1177473 -4.1339278 -4.1504936 -4.1511769 -4.1377974 -4.1082363 -4.09185 -4.1046124 -4.14281 -4.1970568 -4.2586164 -4.3040776 -4.3273954 -4.3318357][-4.137445 -4.1266346 -4.1380959 -4.15038 -4.1527052 -4.1436749 -4.116169 -4.1000857 -4.1132402 -4.1491446 -4.2015433 -4.2632079 -4.3079257 -4.3289204 -4.3323622][-4.1754336 -4.1629548 -4.1630545 -4.168251 -4.1688519 -4.1572289 -4.1200571 -4.0910411 -4.0988445 -4.1357284 -4.1942582 -4.2603712 -4.306314 -4.3268619 -4.3311353][-4.2130151 -4.2000041 -4.1920648 -4.1944394 -4.1977849 -4.1863379 -4.13991 -4.0936584 -4.0898609 -4.125752 -4.1899385 -4.2576995 -4.3036036 -4.3235021 -4.3294225][-4.2441173 -4.2325149 -4.2206516 -4.2211161 -4.2276788 -4.2178121 -4.1683574 -4.1117916 -4.0997815 -4.1325049 -4.1964006 -4.2605352 -4.3035479 -4.3222809 -4.3289223][-4.2634053 -4.2525229 -4.2392893 -4.238245 -4.2431679 -4.2292509 -4.1783948 -4.1205196 -4.1056175 -4.1362705 -4.1983232 -4.2612686 -4.3026943 -4.3221765 -4.3297887]]...]
INFO - root - 2017-12-07 22:38:41.613077: step 61910, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 54h:35m:31s remains)
INFO - root - 2017-12-07 22:38:48.341104: step 61920, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 52h:50m:54s remains)
INFO - root - 2017-12-07 22:38:55.060167: step 61930, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 49h:43m:37s remains)
INFO - root - 2017-12-07 22:39:01.775008: step 61940, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 47h:30m:26s remains)
INFO - root - 2017-12-07 22:39:08.640181: step 61950, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 50h:55m:52s remains)
INFO - root - 2017-12-07 22:39:15.552634: step 61960, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.768 sec/batch; 57h:44m:39s remains)
INFO - root - 2017-12-07 22:39:22.087420: step 61970, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 49h:26m:17s remains)
INFO - root - 2017-12-07 22:39:28.910574: step 61980, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 48h:17m:32s remains)
INFO - root - 2017-12-07 22:39:35.714673: step 61990, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 49h:23m:34s remains)
INFO - root - 2017-12-07 22:39:42.434463: step 62000, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 53h:25m:13s remains)
2017-12-07 22:39:43.153447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0674772 -4.0653 -4.07444 -4.1013284 -4.1322665 -4.1523132 -4.1420646 -4.0963221 -4.0278153 -3.9694932 -3.9790258 -4.0516272 -4.140965 -4.22167 -4.2894225][-4.0769525 -4.0768619 -4.0859828 -4.1114583 -4.1342182 -4.14329 -4.1227007 -4.0679746 -3.992938 -3.933465 -3.9496579 -4.0295906 -4.12511 -4.2083654 -4.2790709][-4.085968 -4.08752 -4.0984397 -4.122498 -4.1368256 -4.13615 -4.1093011 -4.0434489 -3.9563046 -3.892926 -3.9142268 -4.0060611 -4.113368 -4.2013073 -4.2733312][-4.085794 -4.0886159 -4.10193 -4.1256671 -4.1383328 -4.138526 -4.1071577 -4.0270929 -3.9267766 -3.8554711 -3.88232 -3.9891222 -4.107645 -4.2009482 -4.2735171][-4.0692029 -4.0719361 -4.0877991 -4.1121988 -4.130475 -4.1345224 -4.0985794 -4.0011096 -3.8913324 -3.8192508 -3.8575873 -3.9791408 -4.1086488 -4.2055545 -4.2767825][-4.0623517 -4.0602789 -4.0711136 -4.0887623 -4.109374 -4.111794 -4.0677886 -3.9606256 -3.8555381 -3.7944448 -3.8440135 -3.9757895 -4.1102753 -4.20941 -4.2813845][-4.0739045 -4.0685058 -4.0710177 -4.0742068 -4.0826263 -4.0710273 -4.0129957 -3.9067407 -3.8177776 -3.7735374 -3.8329353 -3.9695756 -4.10497 -4.2071071 -4.2838874][-4.0930758 -4.0874486 -4.0893159 -4.082747 -4.0734973 -4.03843 -3.9641845 -3.8607407 -3.7886448 -3.7566602 -3.819128 -3.9589503 -4.0950546 -4.2018719 -4.2834463][-4.1041808 -4.1015687 -4.1124425 -4.1084676 -4.0947752 -4.0515695 -3.971385 -3.8709002 -3.7981009 -3.7636542 -3.818181 -3.9520724 -4.087821 -4.1993074 -4.2836294][-4.1099672 -4.1088047 -4.1267467 -4.1343284 -4.1317625 -4.0973473 -4.024868 -3.9303412 -3.8467717 -3.7982001 -3.837677 -3.9574347 -4.0882783 -4.2014623 -4.2858372][-4.1252189 -4.1165214 -4.1291971 -4.1466727 -4.1535606 -4.1278706 -4.0646935 -3.9809241 -3.8981161 -3.8424349 -3.8666019 -3.9733229 -4.096035 -4.2051978 -4.2879848][-4.1432014 -4.1260715 -4.1285295 -4.1509204 -4.15911 -4.1376457 -4.0825605 -4.009974 -3.9382517 -3.8777118 -3.8889387 -3.9864359 -4.1034584 -4.2072487 -4.2895141][-4.1516619 -4.1301894 -4.1247449 -4.1530704 -4.1672535 -4.1466422 -4.09194 -4.0271516 -3.9591339 -3.8948808 -3.9002905 -3.9969285 -4.1104674 -4.2095308 -4.2906365][-4.152926 -4.1315918 -4.1247268 -4.1641421 -4.1810627 -4.1532087 -4.0991454 -4.0366292 -3.9657805 -3.9005268 -3.9115465 -4.0091772 -4.1196733 -4.2142863 -4.2935061][-4.159533 -4.1418214 -4.1357546 -4.175467 -4.1869283 -4.1571231 -4.1146584 -4.0539942 -3.9794867 -3.9161882 -3.932673 -4.0242186 -4.1287341 -4.2207789 -4.2983322]]...]
INFO - root - 2017-12-07 22:39:49.825974: step 62010, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 47h:07m:54s remains)
INFO - root - 2017-12-07 22:39:56.616303: step 62020, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 50h:27m:33s remains)
INFO - root - 2017-12-07 22:40:03.430191: step 62030, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 55h:35m:19s remains)
INFO - root - 2017-12-07 22:40:10.276280: step 62040, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 53h:37m:01s remains)
INFO - root - 2017-12-07 22:40:17.234955: step 62050, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 48h:12m:14s remains)
INFO - root - 2017-12-07 22:40:24.079858: step 62060, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 50h:13m:48s remains)
INFO - root - 2017-12-07 22:40:30.925163: step 62070, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 52h:54m:46s remains)
INFO - root - 2017-12-07 22:40:37.705839: step 62080, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 55h:32m:24s remains)
INFO - root - 2017-12-07 22:40:44.479204: step 62090, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.693 sec/batch; 52h:03m:53s remains)
INFO - root - 2017-12-07 22:40:51.012388: step 62100, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 47h:40m:39s remains)
2017-12-07 22:40:51.776182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.331635 -4.3362412 -4.33449 -4.3237462 -4.302392 -4.2758493 -4.2552428 -4.2420683 -4.2453394 -4.2573495 -4.2764158 -4.2957869 -4.3049126 -4.3042173 -4.3013206][-4.3396058 -4.3478131 -4.347167 -4.3354626 -4.3116641 -4.2793155 -4.2515507 -4.2343082 -4.2397938 -4.2563648 -4.2802019 -4.3030186 -4.3116064 -4.3087344 -4.3049297][-4.3400364 -4.3487186 -4.3473907 -4.3348069 -4.3103566 -4.2730551 -4.2367163 -4.2169185 -4.2275562 -4.2520733 -4.2813506 -4.3073735 -4.314949 -4.3109746 -4.3071218][-4.3376818 -4.344276 -4.3399878 -4.3243356 -4.2972441 -4.2552938 -4.2125487 -4.19276 -4.2110977 -4.2442918 -4.2780604 -4.3051581 -4.3112917 -4.3069053 -4.3046212][-4.3353887 -4.3391943 -4.3304276 -4.3069706 -4.2708569 -4.2189116 -4.166471 -4.1493344 -4.179431 -4.2231903 -4.2621222 -4.2892156 -4.2947135 -4.2893867 -4.2900581][-4.3345828 -4.3368664 -4.3240862 -4.2918119 -4.2423024 -4.1747117 -4.1091065 -4.0983753 -4.1444588 -4.1989627 -4.24075 -4.2660112 -4.2705355 -4.2654915 -4.269495][-4.3352957 -4.3364778 -4.3207192 -4.2832642 -4.2221894 -4.1374259 -4.0527906 -4.0477672 -4.1090431 -4.1690259 -4.208436 -4.2315774 -4.2367778 -4.2340837 -4.241406][-4.3378654 -4.3383346 -4.3211131 -4.2811131 -4.2140408 -4.1188622 -4.0185938 -4.0176644 -4.0852323 -4.142942 -4.1786575 -4.1990719 -4.2063427 -4.2082086 -4.2176819][-4.3397727 -4.3398519 -4.3223238 -4.2827191 -4.2199035 -4.1348891 -4.0459347 -4.0493417 -4.1088781 -4.1525311 -4.1780777 -4.1931806 -4.2018661 -4.207828 -4.2160659][-4.3401871 -4.3389711 -4.3201861 -4.2830782 -4.2305474 -4.1662235 -4.1033106 -4.1094532 -4.1526918 -4.1802659 -4.1968741 -4.2077746 -4.2169304 -4.2260671 -4.231185][-4.33865 -4.3354564 -4.3155856 -4.2816925 -4.2371216 -4.1904984 -4.1500816 -4.1594586 -4.1916733 -4.2096553 -4.2228727 -4.2324991 -4.2413254 -4.2512674 -4.2546649][-4.3362322 -4.3304844 -4.3091774 -4.2761993 -4.2357516 -4.1996784 -4.1716175 -4.1807132 -4.2065153 -4.2219157 -4.2342849 -4.2468371 -4.2575932 -4.2689514 -4.2731986][-4.3315291 -4.32309 -4.2990179 -4.2643576 -4.2234354 -4.1918287 -4.1699367 -4.1773691 -4.2017941 -4.2194982 -4.2342763 -4.2540917 -4.2706962 -4.2828522 -4.2871466][-4.324718 -4.3121586 -4.2846289 -4.2473912 -4.2059813 -4.1803455 -4.1674976 -4.1784506 -4.2076883 -4.2307458 -4.2470751 -4.2692175 -4.2868919 -4.2967157 -4.2980857][-4.3184948 -4.3026366 -4.2716866 -4.2316179 -4.18932 -4.1690016 -4.1650085 -4.18109 -4.2166462 -4.2466936 -4.2664757 -4.286478 -4.3004308 -4.3055525 -4.302341]]...]
INFO - root - 2017-12-07 22:40:58.597153: step 62110, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 53h:36m:18s remains)
INFO - root - 2017-12-07 22:41:05.362371: step 62120, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.675 sec/batch; 50h:43m:43s remains)
INFO - root - 2017-12-07 22:41:12.201960: step 62130, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.638 sec/batch; 47h:56m:07s remains)
INFO - root - 2017-12-07 22:41:19.046110: step 62140, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 48h:21m:39s remains)
INFO - root - 2017-12-07 22:41:25.956109: step 62150, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 54h:23m:05s remains)
INFO - root - 2017-12-07 22:41:32.811518: step 62160, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 55h:45m:00s remains)
INFO - root - 2017-12-07 22:41:39.634106: step 62170, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 48h:30m:04s remains)
INFO - root - 2017-12-07 22:41:46.401335: step 62180, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 46h:49m:31s remains)
INFO - root - 2017-12-07 22:41:53.189674: step 62190, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 49h:09m:47s remains)
INFO - root - 2017-12-07 22:41:59.821068: step 62200, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.743 sec/batch; 55h:48m:07s remains)
2017-12-07 22:42:00.578886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2168393 -4.2193851 -4.222681 -4.2302327 -4.23483 -4.2298737 -4.2237792 -4.222219 -4.214735 -4.206728 -4.2099729 -4.2268672 -4.2421355 -4.2420793 -4.23277][-4.2204814 -4.2054796 -4.1978807 -4.2037034 -4.21674 -4.2244883 -4.228601 -4.2335844 -4.2314978 -4.2244649 -4.2203994 -4.2295775 -4.2428617 -4.2427678 -4.2345595][-4.22397 -4.1973972 -4.1792731 -4.1784353 -4.192595 -4.2076144 -4.2184119 -4.2299104 -4.2345734 -4.2335248 -4.2295065 -4.2348151 -4.2437577 -4.2379909 -4.2294011][-4.2186589 -4.19287 -4.1716595 -4.1645327 -4.17275 -4.1838136 -4.1929874 -4.2078795 -4.219296 -4.226975 -4.2292852 -4.2353077 -4.2405496 -4.2304492 -4.2206993][-4.2015996 -4.1824913 -4.1650529 -4.1578536 -4.1584144 -4.1564274 -4.1532688 -4.1644239 -4.1806951 -4.1989427 -4.2130995 -4.225224 -4.2316732 -4.2214708 -4.2106323][-4.1842012 -4.1696296 -4.1577163 -4.15448 -4.1519427 -4.1387806 -4.1203146 -4.1157827 -4.1257558 -4.1518068 -4.1826653 -4.20688 -4.2175446 -4.2074666 -4.191864][-4.1846337 -4.1746683 -4.1672626 -4.1677885 -4.1647072 -4.1422305 -4.1038442 -4.0703464 -4.0611086 -4.0909071 -4.1388645 -4.1777329 -4.1947246 -4.1829343 -4.1614761][-4.1965389 -4.1947603 -4.1919851 -4.1927476 -4.18811 -4.1610656 -4.114295 -4.0622211 -4.0343442 -4.0568171 -4.1040487 -4.1441483 -4.1631508 -4.1542511 -4.1330547][-4.1946392 -4.2004137 -4.202559 -4.2048392 -4.2028804 -4.1841359 -4.1490817 -4.1048765 -4.0738616 -4.07572 -4.1014671 -4.1297765 -4.145071 -4.1399808 -4.1199656][-4.180954 -4.1913586 -4.1955194 -4.1977668 -4.1983204 -4.1931772 -4.1816254 -4.1609397 -4.1397619 -4.1285911 -4.13353 -4.1460481 -4.1535978 -4.1480722 -4.1300144][-4.1736317 -4.1817594 -4.1818023 -4.1781216 -4.1781387 -4.184999 -4.1951785 -4.1993556 -4.1948462 -4.1843028 -4.1776218 -4.1753125 -4.17357 -4.1670084 -4.1550293][-4.1771746 -4.1783147 -4.17224 -4.1619272 -4.1580434 -4.16949 -4.1932926 -4.2162938 -4.2270041 -4.2230434 -4.2118134 -4.1994591 -4.1920204 -4.18563 -4.1806922][-4.1984339 -4.1929374 -4.181427 -4.1646919 -4.1533618 -4.1604271 -4.1859989 -4.2157612 -4.2334948 -4.2350683 -4.2236018 -4.2058253 -4.1951685 -4.1912241 -4.1955204][-4.2326655 -4.2232137 -4.2087717 -4.1894174 -4.1732292 -4.1724405 -4.1883187 -4.2107749 -4.2259822 -4.230052 -4.2220097 -4.2054868 -4.19306 -4.18853 -4.19648][-4.266222 -4.25962 -4.2479668 -4.2310386 -4.2126207 -4.2050028 -4.2077017 -4.215467 -4.220798 -4.2228589 -4.2189355 -4.2079253 -4.1969047 -4.1909628 -4.1964684]]...]
INFO - root - 2017-12-07 22:42:07.271813: step 62210, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 48h:06m:40s remains)
INFO - root - 2017-12-07 22:42:14.144653: step 62220, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 49h:58m:00s remains)
INFO - root - 2017-12-07 22:42:20.965164: step 62230, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.754 sec/batch; 56h:34m:25s remains)
INFO - root - 2017-12-07 22:42:27.869580: step 62240, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 55h:23m:46s remains)
INFO - root - 2017-12-07 22:42:34.642230: step 62250, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 48h:19m:11s remains)
INFO - root - 2017-12-07 22:42:41.396059: step 62260, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 46h:08m:40s remains)
INFO - root - 2017-12-07 22:42:48.226646: step 62270, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 51h:57m:27s remains)
INFO - root - 2017-12-07 22:42:54.875888: step 62280, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.660 sec/batch; 49h:32m:21s remains)
INFO - root - 2017-12-07 22:43:01.664552: step 62290, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 48h:10m:04s remains)
INFO - root - 2017-12-07 22:43:08.208621: step 62300, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.641 sec/batch; 48h:05m:54s remains)
2017-12-07 22:43:08.919171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2986808 -4.2923179 -4.2890091 -4.29282 -4.2964087 -4.2989764 -4.2986031 -4.3001075 -4.3039703 -4.3090053 -4.3143158 -4.318491 -4.3178368 -4.3150249 -4.3145237][-4.2989354 -4.2959914 -4.2949753 -4.2976146 -4.299603 -4.3009276 -4.3004413 -4.3002353 -4.3012834 -4.3053355 -4.3099675 -4.3128076 -4.3073916 -4.2991691 -4.2954626][-4.292707 -4.2940598 -4.2976527 -4.3008747 -4.3019505 -4.3025784 -4.2993851 -4.2937145 -4.288569 -4.2915072 -4.2973123 -4.3028955 -4.2984438 -4.286375 -4.2791505][-4.28109 -4.2859378 -4.2938695 -4.2951069 -4.2940474 -4.2945852 -4.2881536 -4.2741942 -4.2634888 -4.2699246 -4.2811923 -4.2910151 -4.2906985 -4.2773352 -4.26725][-4.2661219 -4.2719946 -4.2835331 -4.2823148 -4.2770238 -4.2728057 -4.256608 -4.2297525 -4.2166519 -4.2311406 -4.25133 -4.2680507 -4.2763424 -4.2691731 -4.2568965][-4.256927 -4.2574873 -4.2670088 -4.2590561 -4.24399 -4.2304063 -4.1987786 -4.1532478 -4.1407018 -4.1701417 -4.2018795 -4.2241082 -4.2451458 -4.252594 -4.2434058][-4.253798 -4.2475114 -4.2522893 -4.2401481 -4.2165833 -4.1877255 -4.131814 -4.05579 -4.042017 -4.0958824 -4.145905 -4.1779513 -4.2140241 -4.2350497 -4.2289906][-4.2459846 -4.2386866 -4.2430243 -4.2311873 -4.2028384 -4.15957 -4.0768805 -3.9645298 -3.9421406 -4.017837 -4.0844502 -4.1296573 -4.1840487 -4.2216473 -4.2224159][-4.2328892 -4.2267513 -4.2327147 -4.2236981 -4.1976118 -4.1563692 -4.0748839 -3.9581528 -3.931989 -4.0085769 -4.0749893 -4.1241775 -4.1856155 -4.2323484 -4.237464][-4.2207026 -4.2135406 -4.2193747 -4.2159514 -4.1986046 -4.1692605 -4.1111784 -4.0320024 -4.0161233 -4.0729327 -4.1228442 -4.1668921 -4.221952 -4.2649126 -4.2712517][-4.2126188 -4.2055054 -4.2110548 -4.21169 -4.2006559 -4.180716 -4.14502 -4.1041565 -4.0999641 -4.1364875 -4.1744223 -4.2129588 -4.25809 -4.29393 -4.3005967][-4.2016487 -4.2002039 -4.2099519 -4.2147021 -4.2055569 -4.1903596 -4.1711626 -4.1546564 -4.155201 -4.1763411 -4.2063131 -4.2404771 -4.2783141 -4.306756 -4.3132014][-4.1888204 -4.1941056 -4.2071562 -4.2121158 -4.1976504 -4.17945 -4.1701169 -4.1685872 -4.1724362 -4.1853957 -4.2131686 -4.2469449 -4.2818837 -4.3054504 -4.3114209][-4.1848078 -4.1967549 -4.2105885 -4.2122107 -4.1920547 -4.1652822 -4.1558795 -4.1587381 -4.1632648 -4.1735468 -4.2005081 -4.2333355 -4.2658792 -4.2884803 -4.2960835][-4.196507 -4.2088695 -4.218914 -4.2178154 -4.198359 -4.1677036 -4.1524796 -4.1534333 -4.157393 -4.1681476 -4.1908426 -4.2178016 -4.2453957 -4.26678 -4.2734833]]...]
INFO - root - 2017-12-07 22:43:15.599849: step 62310, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 51h:59m:42s remains)
INFO - root - 2017-12-07 22:43:22.302074: step 62320, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.642 sec/batch; 48h:09m:03s remains)
INFO - root - 2017-12-07 22:43:29.044955: step 62330, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 48h:55m:15s remains)
INFO - root - 2017-12-07 22:43:35.814526: step 62340, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 53h:10m:30s remains)
INFO - root - 2017-12-07 22:43:42.533944: step 62350, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 53h:18m:44s remains)
INFO - root - 2017-12-07 22:43:49.223489: step 62360, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.688 sec/batch; 51h:37m:13s remains)
INFO - root - 2017-12-07 22:43:56.102452: step 62370, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 50h:45m:40s remains)
INFO - root - 2017-12-07 22:44:02.860171: step 62380, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 48h:21m:17s remains)
INFO - root - 2017-12-07 22:44:09.668040: step 62390, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.627 sec/batch; 47h:01m:28s remains)
INFO - root - 2017-12-07 22:44:16.340098: step 62400, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 55h:23m:59s remains)
2017-12-07 22:44:17.041717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1554232 -4.0950184 -4.0623293 -4.0612264 -4.064734 -4.0546083 -4.0333357 -4.0153241 -4.0379529 -4.0720119 -4.1171 -4.1524568 -4.1690249 -4.1933088 -4.2206168][-4.1340246 -4.0665421 -4.035778 -4.0430274 -4.0573845 -4.0523143 -4.0259547 -3.9984283 -4.01191 -4.0525713 -4.1133347 -4.1594372 -4.1772561 -4.1985841 -4.2239294][-4.1163068 -4.049922 -4.0279236 -4.0433598 -4.065309 -4.0604029 -4.0237694 -3.9878232 -3.9998965 -4.0483108 -4.1203136 -4.1717286 -4.1894245 -4.2074218 -4.2296295][-4.1132121 -4.0573831 -4.0447412 -4.0666838 -4.0875969 -4.0752277 -4.0232487 -3.9742682 -3.9848821 -4.0418358 -4.1252375 -4.1835232 -4.2031126 -4.2188425 -4.2377996][-4.1181617 -4.0786376 -4.0757971 -4.1017895 -4.1096759 -4.0727863 -3.9994829 -3.9370232 -3.9447746 -4.0142431 -4.116888 -4.1878467 -4.2147255 -4.2308888 -4.2477584][-4.125608 -4.10268 -4.104856 -4.129797 -4.1223536 -4.0568075 -3.9619217 -3.881321 -3.8917661 -3.98292 -4.1024489 -4.1826572 -4.2171874 -4.2373509 -4.25553][-4.1312966 -4.1216493 -4.1302986 -4.147594 -4.1245518 -4.0342627 -3.9097047 -3.8111641 -3.8365936 -3.9572949 -4.08972 -4.1748857 -4.2128215 -4.2393727 -4.2606788][-4.1403484 -4.1414428 -4.1563034 -4.1678166 -4.1300192 -4.0189633 -3.8661854 -3.7502351 -3.7947178 -3.9409592 -4.0813165 -4.169548 -4.2095966 -4.2411008 -4.2646012][-4.1337347 -4.1429987 -4.1614285 -4.1784043 -4.1438155 -4.036665 -3.8880179 -3.7779191 -3.8207846 -3.9603159 -4.0880756 -4.1693091 -4.2072473 -4.241096 -4.26659][-4.0990958 -4.1147366 -4.1390562 -4.1667318 -4.1444693 -4.0588984 -3.9442427 -3.8626876 -3.8999579 -4.0140247 -4.1140842 -4.1765552 -4.2064219 -4.2388964 -4.2662425][-4.07693 -4.0925522 -4.1160564 -4.14483 -4.1289482 -4.0600872 -3.971303 -3.9166696 -3.9574594 -4.0543451 -4.1331081 -4.1771526 -4.1992869 -4.2275476 -4.2553248][-4.1004424 -4.1123133 -4.1261992 -4.147963 -4.1293821 -4.0642118 -3.980269 -3.9354653 -3.9775212 -4.0648627 -4.1318884 -4.1661019 -4.1846509 -4.209681 -4.2383809][-4.1392193 -4.1491938 -4.1572556 -4.171268 -4.1516418 -4.0901346 -4.0094767 -3.9680877 -4.000083 -4.0672393 -4.1203179 -4.1513996 -4.1710486 -4.1970558 -4.2257261][-4.1739569 -4.1754727 -4.1771941 -4.1822062 -4.161243 -4.1123166 -4.0514064 -4.022007 -4.0403023 -4.0819559 -4.1167617 -4.1409559 -4.1617403 -4.1900353 -4.2169886][-4.2081184 -4.1999702 -4.1900387 -4.1836462 -4.1594911 -4.1211963 -4.083312 -4.068224 -4.0800858 -4.1065836 -4.1274347 -4.1433768 -4.1636147 -4.1914191 -4.217279]]...]
INFO - root - 2017-12-07 22:44:23.813551: step 62410, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.616 sec/batch; 46h:14m:52s remains)
INFO - root - 2017-12-07 22:44:30.614994: step 62420, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 49h:52m:37s remains)
INFO - root - 2017-12-07 22:44:37.376560: step 62430, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 53h:21m:37s remains)
INFO - root - 2017-12-07 22:44:44.123371: step 62440, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 53h:30m:15s remains)
INFO - root - 2017-12-07 22:44:50.855756: step 62450, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 51h:11m:24s remains)
INFO - root - 2017-12-07 22:44:57.590641: step 62460, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 47h:22m:52s remains)
INFO - root - 2017-12-07 22:45:04.431448: step 62470, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 47h:55m:04s remains)
INFO - root - 2017-12-07 22:45:11.284746: step 62480, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 54h:03m:50s remains)
INFO - root - 2017-12-07 22:45:18.074125: step 62490, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 48h:53m:46s remains)
INFO - root - 2017-12-07 22:45:24.726950: step 62500, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 50h:12m:27s remains)
2017-12-07 22:45:25.400304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1777468 -4.1914444 -4.2162933 -4.2469444 -4.2604494 -4.2611647 -4.2423987 -4.2250128 -4.2167621 -4.2207994 -4.23203 -4.226697 -4.2195854 -4.22735 -4.2436886][-4.126967 -4.1424189 -4.1798234 -4.2237883 -4.2446122 -4.2494588 -4.2285223 -4.206666 -4.1948423 -4.2013979 -4.2190824 -4.2163887 -4.2075152 -4.2154732 -4.2279234][-4.0642114 -4.0747361 -4.1157341 -4.163589 -4.1868439 -4.1945477 -4.1755915 -4.1530857 -4.1412554 -4.1526132 -4.1786604 -4.1844096 -4.1800995 -4.18754 -4.1998715][-4.0255704 -4.0302739 -4.0684891 -4.1133356 -4.1343813 -4.1427531 -4.1261377 -4.1054554 -4.0967417 -4.1118283 -4.1399341 -4.1514111 -4.1567039 -4.1695933 -4.185739][-4.0188675 -4.0259285 -4.0663943 -4.1055613 -4.1249528 -4.1346593 -4.1240444 -4.1019759 -4.0947633 -4.1105976 -4.1328192 -4.1438246 -4.1517277 -4.1668262 -4.1802063][-4.0064459 -4.0162864 -4.0574222 -4.0907116 -4.1103559 -4.1238713 -4.1090178 -4.077744 -4.0732956 -4.0994411 -4.1295462 -4.1464567 -4.1606741 -4.1765652 -4.182857][-3.9721789 -3.9838996 -4.0300274 -4.0674448 -4.0895853 -4.0936465 -4.0501766 -3.9847231 -3.9721742 -4.0233111 -4.0843859 -4.1214786 -4.1552129 -4.1822009 -4.1885252][-3.9327004 -3.9467885 -3.9960539 -4.0364275 -4.0528183 -4.0314012 -3.937005 -3.8168068 -3.7959535 -3.8838332 -3.9838228 -4.0437064 -4.097137 -4.1420059 -4.1614947][-3.9114218 -3.9297645 -3.9814126 -4.0249205 -4.0440712 -4.0077419 -3.8875389 -3.7470675 -3.7222338 -3.8164554 -3.919446 -3.9757624 -4.0292416 -4.0802269 -4.1118622][-3.9306579 -3.9483795 -3.9922652 -4.0333147 -4.0597076 -4.0377665 -3.9519105 -3.8567948 -3.8382988 -3.9000058 -3.9666553 -3.9933405 -4.021307 -4.0536108 -4.0818772][-4.0035605 -4.01775 -4.0497818 -4.0836487 -4.1125641 -4.1106896 -4.06571 -4.0117683 -4.0001845 -4.0333972 -4.0652556 -4.0675354 -4.0703735 -4.0786414 -4.0965261][-4.1114044 -4.1204567 -4.1422672 -4.1680393 -4.1904507 -4.195827 -4.1732941 -4.1443214 -4.1357245 -4.1534672 -4.1715288 -4.1674285 -4.1613717 -4.1576505 -4.1671424][-4.2087846 -4.215064 -4.2291689 -4.2479596 -4.2628708 -4.2656741 -4.2498155 -4.2295642 -4.2234688 -4.2352929 -4.2491426 -4.2480745 -4.2441893 -4.242064 -4.2470064][-4.2780271 -4.281353 -4.2890182 -4.3021398 -4.315043 -4.3167038 -4.302567 -4.2860765 -4.2805328 -4.2883453 -4.2983031 -4.30086 -4.3004122 -4.2995353 -4.3016706][-4.3231168 -4.323667 -4.3272505 -4.3356042 -4.3438258 -4.343029 -4.3332996 -4.32306 -4.318295 -4.3203192 -4.3250051 -4.3277845 -4.3279271 -4.3281837 -4.3295956]]...]
INFO - root - 2017-12-07 22:45:32.165324: step 62510, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 53h:31m:07s remains)
INFO - root - 2017-12-07 22:45:38.869495: step 62520, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 52h:45m:40s remains)
INFO - root - 2017-12-07 22:45:45.560820: step 62530, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 49h:02m:03s remains)
INFO - root - 2017-12-07 22:45:52.258642: step 62540, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 47h:50m:34s remains)
INFO - root - 2017-12-07 22:45:59.052680: step 62550, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 48h:50m:51s remains)
INFO - root - 2017-12-07 22:46:05.967436: step 62560, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 53h:41m:05s remains)
INFO - root - 2017-12-07 22:46:12.789842: step 62570, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 52h:18m:51s remains)
INFO - root - 2017-12-07 22:46:19.530514: step 62580, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 50h:10m:36s remains)
INFO - root - 2017-12-07 22:46:26.169201: step 62590, loss = 2.10, batch loss = 2.05 (11.8 examples/sec; 0.676 sec/batch; 50h:39m:10s remains)
INFO - root - 2017-12-07 22:46:32.829204: step 62600, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 53h:48m:26s remains)
2017-12-07 22:46:33.580965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2144136 -4.2318487 -4.2504416 -4.2565808 -4.2426724 -4.2232928 -4.2090635 -4.2092419 -4.2045693 -4.1903129 -4.1822314 -4.1882873 -4.2061539 -4.2291265 -4.2474642][-4.226944 -4.24072 -4.2555256 -4.257586 -4.2398162 -4.2116117 -4.18971 -4.1958437 -4.210784 -4.217802 -4.2235641 -4.2298374 -4.2366915 -4.2456951 -4.2535067][-4.2406793 -4.2501154 -4.2621737 -4.2617116 -4.2402611 -4.2011218 -4.1673727 -4.1713004 -4.201498 -4.2311134 -4.2517619 -4.2613411 -4.2615943 -4.2602749 -4.2617383][-4.2532315 -4.2588663 -4.2683268 -4.2654691 -4.2387767 -4.1883755 -4.1388187 -4.1304522 -4.1673622 -4.2168827 -4.253695 -4.2707715 -4.2708254 -4.2643723 -4.2617855][-4.2511082 -4.2536311 -4.2613673 -4.2557521 -4.2206259 -4.1547594 -4.0831032 -4.0560856 -4.1013169 -4.17234 -4.22911 -4.2585335 -4.2629852 -4.2524786 -4.2414622][-4.2337151 -4.2318096 -4.2364588 -4.2245045 -4.1764441 -4.0903559 -3.9920571 -3.9509685 -4.0171995 -4.1193352 -4.1992106 -4.2421169 -4.2516012 -4.2357626 -4.2145438][-4.2158055 -4.2111921 -4.2132988 -4.1969681 -4.1382723 -4.0346127 -3.9167109 -3.8697929 -3.9589839 -4.0856113 -4.1798134 -4.2320423 -4.2467728 -4.2307696 -4.2049103][-4.2122498 -4.2081909 -4.212069 -4.1997757 -4.1473804 -4.050561 -3.9375944 -3.889822 -3.9715602 -4.0911574 -4.1815252 -4.2351246 -4.2536759 -4.2435694 -4.2210741][-4.2311745 -4.2328773 -4.24199 -4.2369914 -4.1971374 -4.1156535 -4.0139027 -3.9623582 -4.0200138 -4.1169748 -4.1946111 -4.2440076 -4.2650914 -4.2612419 -4.2445278][-4.2540245 -4.2604265 -4.2731695 -4.2739048 -4.2410021 -4.1671271 -4.0733943 -4.0237322 -4.0676389 -4.1473951 -4.214273 -4.2588415 -4.2806029 -4.2791672 -4.2653403][-4.2616487 -4.2688727 -4.2858191 -4.2919765 -4.2662978 -4.2029905 -4.1242094 -4.0834908 -4.1177897 -4.182807 -4.2384491 -4.2754636 -4.2948732 -4.292623 -4.27831][-4.2555666 -4.2631888 -4.2855344 -4.3003283 -4.2889051 -4.2449379 -4.1887445 -4.1587958 -4.1790652 -4.2249231 -4.2648072 -4.291523 -4.3053584 -4.3012986 -4.2874513][-4.2399421 -4.2454891 -4.2742229 -4.3006787 -4.3033681 -4.2761383 -4.2390757 -4.2167554 -4.224504 -4.2536669 -4.2806082 -4.2972693 -4.304863 -4.3019276 -4.2935567][-4.2191572 -4.2197022 -4.2524762 -4.2892694 -4.30209 -4.2847409 -4.2577381 -4.2398081 -4.2412615 -4.2595382 -4.2737293 -4.2799144 -4.283565 -4.2837653 -4.282042][-4.20455 -4.1970015 -4.2274537 -4.2665215 -4.2835274 -4.2728224 -4.2512012 -4.2363582 -4.23357 -4.2429309 -4.2474875 -4.2466378 -4.2480865 -4.2502303 -4.2528639]]...]
INFO - root - 2017-12-07 22:46:40.385804: step 62610, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 47h:45m:42s remains)
INFO - root - 2017-12-07 22:46:47.222079: step 62620, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 54h:42m:41s remains)
INFO - root - 2017-12-07 22:46:54.198000: step 62630, loss = 2.03, batch loss = 1.98 (11.1 examples/sec; 0.721 sec/batch; 54h:02m:36s remains)
INFO - root - 2017-12-07 22:47:00.928641: step 62640, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 51h:00m:59s remains)
INFO - root - 2017-12-07 22:47:07.573364: step 62650, loss = 2.09, batch loss = 2.03 (13.1 examples/sec; 0.612 sec/batch; 45h:53m:09s remains)
INFO - root - 2017-12-07 22:47:14.223937: step 62660, loss = 2.06, batch loss = 2.01 (13.1 examples/sec; 0.610 sec/batch; 45h:44m:41s remains)
INFO - root - 2017-12-07 22:47:21.018098: step 62670, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 54h:18m:21s remains)
INFO - root - 2017-12-07 22:47:27.828521: step 62680, loss = 2.11, batch loss = 2.05 (11.3 examples/sec; 0.708 sec/batch; 53h:02m:15s remains)
INFO - root - 2017-12-07 22:47:34.725528: step 62690, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 52h:51m:54s remains)
INFO - root - 2017-12-07 22:47:41.402095: step 62700, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 48h:07m:54s remains)
2017-12-07 22:47:42.106327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1684437 -4.1236897 -4.0938344 -4.100081 -4.1351547 -4.1616845 -4.1671677 -4.1543512 -4.1284781 -4.1168242 -4.1209249 -4.1167178 -4.0996146 -4.0802059 -4.0659695][-4.1639595 -4.1263356 -4.1003275 -4.1033044 -4.130806 -4.1551557 -4.1666369 -4.1607637 -4.1436357 -4.1394458 -4.1485219 -4.1362762 -4.1071763 -4.07848 -4.0604281][-4.169106 -4.1399717 -4.1201515 -4.1221294 -4.1422048 -4.1604924 -4.1730909 -4.1750836 -4.1687183 -4.1675611 -4.172708 -4.146801 -4.104692 -4.0706983 -4.0509686][-4.18243 -4.1647081 -4.15217 -4.1514254 -4.1567593 -4.1583862 -4.16114 -4.1627221 -4.1616435 -4.1589947 -4.1614237 -4.132864 -4.0884128 -4.052784 -4.0273781][-4.1933165 -4.1816711 -4.168932 -4.1546683 -4.1342821 -4.1068106 -4.0923982 -4.0951533 -4.1085134 -4.1178126 -4.1246381 -4.1061735 -4.075263 -4.0485806 -4.0253358][-4.1821389 -4.1668673 -4.1475506 -4.1106229 -4.0536523 -3.9879661 -3.9565203 -3.9720664 -4.0122 -4.0434966 -4.0642076 -4.0687962 -4.064949 -4.0611181 -4.0572529][-4.1544352 -4.1342478 -4.10338 -4.0459619 -3.9554861 -3.8660593 -3.8299997 -3.8599579 -3.9192567 -3.9719326 -4.0138512 -4.0469055 -4.0716977 -4.0887685 -4.10542][-4.1246243 -4.1078439 -4.07601 -4.0225806 -3.9387908 -3.8616438 -3.8370237 -3.8663018 -3.9185355 -3.9771056 -4.0332427 -4.0793724 -4.1171126 -4.1436076 -4.1680083][-4.1087747 -4.0984063 -4.0777469 -4.0471072 -4.0005608 -3.9547453 -3.9418414 -3.9650977 -4.003119 -4.0488658 -4.095593 -4.1374903 -4.1753378 -4.2031417 -4.2240171][-4.1183391 -4.1157537 -4.1090918 -4.1009555 -4.0833178 -4.0612311 -4.0547247 -4.0694833 -4.0984039 -4.131094 -4.163476 -4.1955023 -4.2242274 -4.2453876 -4.2561016][-4.1403742 -4.1432152 -4.1463389 -4.1513686 -4.1496229 -4.1411853 -4.141315 -4.1584988 -4.1839519 -4.2084923 -4.2266159 -4.240346 -4.2509255 -4.2582674 -4.2583084][-4.1501331 -4.1591787 -4.1701646 -4.180963 -4.1826158 -4.1771564 -4.1809173 -4.2006369 -4.2235417 -4.2398973 -4.2471442 -4.2460833 -4.2418456 -4.2369413 -4.2307825][-4.1519403 -4.1620317 -4.1748481 -4.1854696 -4.1875424 -4.1848245 -4.1888561 -4.2049441 -4.2196369 -4.2272434 -4.2292171 -4.2247105 -4.2172141 -4.2113457 -4.2081246][-4.1613564 -4.1636939 -4.1699667 -4.1759243 -4.1776233 -4.1774011 -4.1809506 -4.1895781 -4.1954417 -4.1963286 -4.1969409 -4.1965032 -4.1963997 -4.19843 -4.2017622][-4.1691818 -4.1625638 -4.1613059 -4.16061 -4.1597185 -4.1595368 -4.1614876 -4.1658292 -4.1684294 -4.1693382 -4.1724324 -4.1772971 -4.1829095 -4.1890864 -4.1942754]]...]
INFO - root - 2017-12-07 22:47:48.929368: step 62710, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 55h:21m:20s remains)
INFO - root - 2017-12-07 22:47:55.739843: step 62720, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 48h:14m:12s remains)
INFO - root - 2017-12-07 22:48:02.516566: step 62730, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 49h:42m:02s remains)
INFO - root - 2017-12-07 22:48:09.329032: step 62740, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 54h:08m:48s remains)
INFO - root - 2017-12-07 22:48:16.188056: step 62750, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 54h:25m:08s remains)
INFO - root - 2017-12-07 22:48:22.966620: step 62760, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 48h:43m:14s remains)
INFO - root - 2017-12-07 22:48:29.749255: step 62770, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 50h:25m:26s remains)
INFO - root - 2017-12-07 22:48:36.424464: step 62780, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 47h:52m:40s remains)
INFO - root - 2017-12-07 22:48:43.321965: step 62790, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 51h:41m:22s remains)
INFO - root - 2017-12-07 22:48:49.950010: step 62800, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 51h:36m:20s remains)
2017-12-07 22:48:50.706297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.337966 -4.3310447 -4.3259339 -4.322401 -4.3202076 -4.3207955 -4.3242288 -4.3285136 -4.3328161 -4.3348832 -4.3341928 -4.333446 -4.335218 -4.3387647 -4.3429232][-4.3341041 -4.3228569 -4.3125744 -4.3008685 -4.28982 -4.2854443 -4.289361 -4.2974772 -4.3065987 -4.31278 -4.3131866 -4.3136449 -4.3190246 -4.3276162 -4.336802][-4.3290005 -4.3128924 -4.2946286 -4.2696772 -4.2441025 -4.2324009 -4.2396812 -4.2549038 -4.2719665 -4.283989 -4.2847071 -4.2849422 -4.2943897 -4.3109775 -4.3279872][-4.3263845 -4.3031545 -4.2720876 -4.2292242 -4.1855035 -4.1668978 -4.1805282 -4.2034945 -4.2280703 -4.2457561 -4.247015 -4.2481604 -4.2630973 -4.29053 -4.3172483][-4.32206 -4.2905173 -4.2447538 -4.1833949 -4.125814 -4.1032996 -4.1214995 -4.14844 -4.1753116 -4.1948023 -4.1961803 -4.1997719 -4.2222972 -4.263535 -4.3027668][-4.3195586 -4.28338 -4.2280927 -4.1526451 -4.0834103 -4.0553665 -4.0718346 -4.0980091 -4.1223159 -4.1415257 -4.1434937 -4.1489139 -4.1783915 -4.2327337 -4.2851691][-4.3178678 -4.2815042 -4.2244124 -4.1434517 -4.0626168 -4.0234365 -4.0365772 -4.0635476 -4.0873318 -4.1092138 -4.1198506 -4.129077 -4.1595445 -4.2154093 -4.2727671][-4.3147783 -4.2830863 -4.2345128 -4.1624932 -4.0804582 -4.0313559 -4.0383897 -4.0626373 -4.0895338 -4.1163487 -4.1334615 -4.1433849 -4.1680269 -4.214457 -4.2669034][-4.3171816 -4.2947493 -4.2617068 -4.2083921 -4.1365824 -4.0827541 -4.07722 -4.0902014 -4.1126051 -4.1409731 -4.1584868 -4.1664891 -4.1846242 -4.2188377 -4.26337][-4.3230834 -4.3116426 -4.295126 -4.2642255 -4.2107511 -4.1575055 -4.1343374 -4.1252017 -4.1352282 -4.1624894 -4.1843085 -4.1951671 -4.210331 -4.2358556 -4.2708883][-4.3249454 -4.3205838 -4.3156681 -4.3020878 -4.2684579 -4.2242584 -4.1906748 -4.1650004 -4.1652422 -4.1906013 -4.2151914 -4.2291484 -4.2423921 -4.2621994 -4.2876196][-4.3217268 -4.3206344 -4.3207054 -4.31866 -4.30376 -4.2757993 -4.2465882 -4.2197938 -4.2156854 -4.2329612 -4.2517481 -4.2636509 -4.274137 -4.2893291 -4.3065209][-4.3208008 -4.3218818 -4.3258915 -4.33056 -4.3286462 -4.3146839 -4.2954679 -4.27749 -4.2721148 -4.2775273 -4.28454 -4.2908144 -4.29873 -4.3109093 -4.322444][-4.3258815 -4.3271003 -4.3307066 -4.3351893 -4.3366756 -4.331151 -4.322679 -4.3145604 -4.3117332 -4.3114285 -4.3104692 -4.3111944 -4.3166571 -4.32593 -4.3331342][-4.3333492 -4.332406 -4.3333926 -4.3351035 -4.3357439 -4.3343086 -4.3321476 -4.3291473 -4.32836 -4.328167 -4.3264985 -4.3260546 -4.3284097 -4.3343287 -4.339046]]...]
INFO - root - 2017-12-07 22:48:57.398956: step 62810, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 47h:45m:52s remains)
INFO - root - 2017-12-07 22:49:04.225734: step 62820, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 55h:26m:32s remains)
INFO - root - 2017-12-07 22:49:11.138882: step 62830, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 53h:29m:32s remains)
INFO - root - 2017-12-07 22:49:17.958321: step 62840, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 50h:08m:36s remains)
INFO - root - 2017-12-07 22:49:24.657976: step 62850, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 48h:10m:05s remains)
INFO - root - 2017-12-07 22:49:31.565803: step 62860, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 51h:46m:42s remains)
INFO - root - 2017-12-07 22:49:38.447706: step 62870, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 53h:36m:28s remains)
INFO - root - 2017-12-07 22:49:45.331870: step 62880, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.737 sec/batch; 55h:12m:06s remains)
INFO - root - 2017-12-07 22:49:52.140262: step 62890, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 47h:43m:36s remains)
INFO - root - 2017-12-07 22:49:58.323547: step 62900, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.619 sec/batch; 46h:22m:35s remains)
2017-12-07 22:49:59.030405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2084188 -4.215621 -4.2246914 -4.221765 -4.2100692 -4.2025633 -4.1811266 -4.1532974 -4.1463685 -4.1757126 -4.2215514 -4.2696147 -4.3106861 -4.33581 -4.3431444][-4.2242432 -4.2296071 -4.2362137 -4.2277284 -4.2098384 -4.1985044 -4.1743808 -4.1463928 -4.1444664 -4.1740003 -4.2184076 -4.2675529 -4.3093867 -4.3344765 -4.3400354][-4.2340584 -4.2356691 -4.2355919 -4.2206669 -4.1967978 -4.1792688 -4.1483793 -4.118701 -4.1255112 -4.1628156 -4.2083917 -4.2588739 -4.3009424 -4.3266706 -4.3315806][-4.2350593 -4.2307596 -4.2220511 -4.19518 -4.161706 -4.1365929 -4.0965662 -4.0660081 -4.086668 -4.1367598 -4.1855593 -4.2406354 -4.2852325 -4.3124013 -4.3193645][-4.2325912 -4.2234025 -4.2064233 -4.1661758 -4.1189432 -4.0800958 -4.0294771 -3.9999251 -4.0350413 -4.0957866 -4.1476183 -4.2100849 -4.2637877 -4.2961464 -4.3066254][-4.2307334 -4.2237487 -4.2048779 -4.1552987 -4.0893197 -4.0276432 -3.9637496 -3.936281 -3.9813747 -4.0497551 -4.1071095 -4.1779976 -4.2431259 -4.2829447 -4.29758][-4.2340188 -4.234901 -4.2185755 -4.163239 -4.0791936 -3.9931498 -3.9202909 -3.8981576 -3.9502144 -4.0257826 -4.0911069 -4.1669235 -4.2360187 -4.2778082 -4.2942634][-4.2534404 -4.2622137 -4.2464232 -4.1816936 -4.0787439 -3.9731736 -3.8994417 -3.8896203 -3.9502525 -4.0337744 -4.1103172 -4.1869988 -4.2493129 -4.2847934 -4.3003407][-4.2822309 -4.2935052 -4.2753253 -4.2046485 -4.0925822 -3.9816592 -3.9136891 -3.912014 -3.9747546 -4.0646076 -4.1508107 -4.223208 -4.2740736 -4.3008695 -4.3148346][-4.307303 -4.3119841 -4.2874861 -4.2168856 -4.1084895 -4.0031223 -3.9438367 -3.9451823 -4.0079155 -4.1049786 -4.1995549 -4.2641973 -4.3020887 -4.321218 -4.3323603][-4.3178 -4.3113627 -4.2763133 -4.2016139 -4.0954175 -3.9946451 -3.9423261 -3.9495387 -4.0195427 -4.1285868 -4.2320046 -4.2921462 -4.3226538 -4.3370671 -4.345468][-4.3159208 -4.2964635 -4.2477665 -4.1653881 -4.0616484 -3.9725256 -3.9330149 -3.9490294 -4.0263104 -4.1420274 -4.2466292 -4.304 -4.3312387 -4.3441181 -4.3520341][-4.3059049 -4.277072 -4.2222471 -4.1405244 -4.0511131 -3.9856486 -3.9632106 -3.9868448 -4.0618925 -4.1671958 -4.2576075 -4.3075476 -4.3328261 -4.3463726 -4.3548551][-4.29957 -4.2706184 -4.2201223 -4.1520872 -4.0869331 -4.046422 -4.0389719 -4.0629745 -4.1236429 -4.2049818 -4.2736497 -4.3119121 -4.3340335 -4.3467555 -4.3553877][-4.3071642 -4.2859063 -4.2490439 -4.2027311 -4.1642604 -4.1470537 -4.1504388 -4.1687241 -4.2083874 -4.2593255 -4.3016777 -4.325851 -4.3409872 -4.3507485 -4.358201]]...]
INFO - root - 2017-12-07 22:50:05.789873: step 62910, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 50h:53m:10s remains)
INFO - root - 2017-12-07 22:50:12.636818: step 62920, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 46h:52m:16s remains)
INFO - root - 2017-12-07 22:50:19.378311: step 62930, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 48h:42m:27s remains)
INFO - root - 2017-12-07 22:50:26.235292: step 62940, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 52h:56m:25s remains)
INFO - root - 2017-12-07 22:50:33.071670: step 62950, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 53h:56m:07s remains)
INFO - root - 2017-12-07 22:50:39.834892: step 62960, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 48h:16m:07s remains)
INFO - root - 2017-12-07 22:50:46.520491: step 62970, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 47h:43m:14s remains)
INFO - root - 2017-12-07 22:50:53.335475: step 62980, loss = 2.04, batch loss = 1.99 (13.1 examples/sec; 0.611 sec/batch; 45h:45m:19s remains)
INFO - root - 2017-12-07 22:51:00.207274: step 62990, loss = 2.12, batch loss = 2.06 (10.9 examples/sec; 0.733 sec/batch; 54h:53m:56s remains)
INFO - root - 2017-12-07 22:51:06.856946: step 63000, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 55h:11m:47s remains)
2017-12-07 22:51:07.623621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1506271 -4.2122803 -4.2591538 -4.279654 -4.2788591 -4.2760749 -4.2778506 -4.2789769 -4.2855949 -4.2957296 -4.2996035 -4.2951131 -4.2885423 -4.2862353 -4.2839131][-4.1163182 -4.1863508 -4.2449183 -4.2760839 -4.2821651 -4.2787428 -4.2763247 -4.2762408 -4.2870512 -4.3030524 -4.3102641 -4.3039179 -4.2915807 -4.2827721 -4.2748189][-4.0938344 -4.161571 -4.2203727 -4.2547359 -4.2639065 -4.258441 -4.2475262 -4.2426314 -4.2544827 -4.2777662 -4.2930961 -4.2925529 -4.2823224 -4.27153 -4.2624292][-4.092123 -4.1447921 -4.19133 -4.2210174 -4.2294564 -4.2174582 -4.1914835 -4.1748667 -4.1854796 -4.217711 -4.2475677 -4.2617197 -4.2628775 -4.258069 -4.25476][-4.1119356 -4.1442018 -4.1719818 -4.1884127 -4.188571 -4.1639266 -4.1158962 -4.0778913 -4.0843434 -4.1311879 -4.1809349 -4.2144632 -4.2316761 -4.2372203 -4.2405658][-4.1439705 -4.1580515 -4.1677642 -4.1674404 -4.15227 -4.109169 -4.0372343 -3.971029 -3.9687808 -4.0328312 -4.1056848 -4.1582537 -4.1904049 -4.2064414 -4.2168336][-4.1802549 -4.1808438 -4.1752224 -4.1616869 -4.1321535 -4.07141 -3.9808564 -3.8917627 -3.8776147 -3.9516876 -4.0404873 -4.1056824 -4.1476288 -4.171092 -4.1866035][-4.1989293 -4.1992273 -4.1889224 -4.1725297 -4.1381316 -4.0700989 -3.9760661 -3.882596 -3.8585162 -3.9268491 -4.0149689 -4.0803781 -4.1219044 -4.1442266 -4.1594729][-4.2066374 -4.2145686 -4.2106991 -4.2001543 -4.1732564 -4.1140103 -4.0353928 -3.9561605 -3.9278245 -3.9712932 -4.0366139 -4.0868983 -4.1171503 -4.1322303 -4.1461554][-4.2243276 -4.2396793 -4.244266 -4.2402444 -4.2237091 -4.1817851 -4.125175 -4.0662069 -4.0375323 -4.0533681 -4.0883107 -4.1165547 -4.1302338 -4.1337719 -4.1440792][-4.25692 -4.2732706 -4.2805638 -4.2799311 -4.2716923 -4.245657 -4.2091885 -4.169621 -4.1444149 -4.1422257 -4.15375 -4.1648464 -4.1657109 -4.1581511 -4.1604924][-4.2906542 -4.3009157 -4.3067851 -4.3080993 -4.3047256 -4.2911348 -4.271872 -4.2497549 -4.2325072 -4.2234755 -4.2223864 -4.2231412 -4.2171259 -4.2043886 -4.2000122][-4.3116646 -4.3137927 -4.3172741 -4.3189144 -4.3189759 -4.3157754 -4.3111539 -4.3045325 -4.2965355 -4.2881875 -4.2820311 -4.2782812 -4.2707739 -4.2582965 -4.2505307][-4.3211331 -4.3155694 -4.3163471 -4.31925 -4.3233538 -4.3279381 -4.3326025 -4.3357782 -4.3338494 -4.327776 -4.3215613 -4.318531 -4.3135033 -4.3038936 -4.2960863][-4.320282 -4.3104072 -4.306756 -4.3091717 -4.3166642 -4.3272662 -4.3379221 -4.3465233 -4.3481536 -4.3444428 -4.3393245 -4.3372345 -4.33356 -4.3257709 -4.3168936]]...]
INFO - root - 2017-12-07 22:51:14.361437: step 63010, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 46h:59m:18s remains)
INFO - root - 2017-12-07 22:51:21.241298: step 63020, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 52h:27m:03s remains)
INFO - root - 2017-12-07 22:51:27.897860: step 63030, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 52h:28m:48s remains)
INFO - root - 2017-12-07 22:51:34.776475: step 63040, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 53h:26m:22s remains)
INFO - root - 2017-12-07 22:51:41.709356: step 63050, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 50h:09m:28s remains)
INFO - root - 2017-12-07 22:51:48.516037: step 63060, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 47h:11m:01s remains)
INFO - root - 2017-12-07 22:51:55.286453: step 63070, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 52h:37m:31s remains)
INFO - root - 2017-12-07 22:52:02.115555: step 63080, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 55h:10m:42s remains)
INFO - root - 2017-12-07 22:52:08.896855: step 63090, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 51h:03m:06s remains)
INFO - root - 2017-12-07 22:52:15.550650: step 63100, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 47h:37m:03s remains)
2017-12-07 22:52:16.348989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2998481 -4.2957954 -4.293107 -4.2936187 -4.2966886 -4.3017282 -4.3053603 -4.3056517 -4.3031707 -4.2982788 -4.2943659 -4.292305 -4.2926731 -4.294776 -4.297215][-4.2902145 -4.284936 -4.280447 -4.2789493 -4.2813048 -4.2872419 -4.2923203 -4.2940326 -4.29337 -4.2907848 -4.2882032 -4.2854633 -4.284307 -4.2852573 -4.28731][-4.2808146 -4.2737522 -4.2672243 -4.2631845 -4.2639747 -4.268929 -4.2729039 -4.2755589 -4.2793331 -4.2824416 -4.2826777 -4.2787385 -4.2733927 -4.2694116 -4.2679152][-4.2720504 -4.2618194 -4.2530794 -4.2459941 -4.24238 -4.2398953 -4.2368455 -4.2393179 -4.2508855 -4.2653551 -4.2726436 -4.2694039 -4.2596116 -4.2485766 -4.2422795][-4.2609229 -4.2473631 -4.2356148 -4.2229037 -4.2102871 -4.1939073 -4.1789889 -4.1803145 -4.2038236 -4.2354794 -4.2543306 -4.2543912 -4.2422657 -4.2255878 -4.2150846][-4.244483 -4.2293258 -4.2139988 -4.1926503 -4.1671963 -4.1365747 -4.1128249 -4.1181612 -4.1553559 -4.2042584 -4.234602 -4.2389054 -4.2265096 -4.2065287 -4.1940289][-4.2219653 -4.20633 -4.1852217 -4.1524057 -4.1151729 -4.076129 -4.0515347 -4.0678024 -4.1198397 -4.1815677 -4.2170467 -4.2229409 -4.2101312 -4.1896486 -4.1776333][-4.2050557 -4.1889009 -4.1609397 -4.1174564 -4.0705781 -4.0281129 -4.0089688 -4.0371394 -4.09856 -4.1636744 -4.2002854 -4.2080564 -4.198379 -4.1818771 -4.1746788][-4.2082548 -4.1928382 -4.1611366 -4.1122632 -4.0626984 -4.0253696 -4.0151815 -4.0455303 -4.1021461 -4.1606007 -4.1944027 -4.2055664 -4.2040462 -4.195879 -4.1937733][-4.2294297 -4.2168889 -4.1859727 -4.1390853 -4.0967293 -4.0732088 -4.0749369 -4.1001554 -4.1400161 -4.1813107 -4.2071071 -4.2204766 -4.22648 -4.2239838 -4.2209196][-4.2534814 -4.2460814 -4.2204194 -4.182807 -4.1531024 -4.1446257 -4.1550617 -4.1711087 -4.1903443 -4.2110672 -4.2265277 -4.2378345 -4.2436838 -4.2412148 -4.2368789][-4.2652607 -4.2629342 -4.2460675 -4.2230306 -4.2074804 -4.2092829 -4.2216606 -4.2287655 -4.23353 -4.2379746 -4.2425351 -4.248425 -4.2523084 -4.2489538 -4.247066][-4.275311 -4.277235 -4.2693276 -4.2584481 -4.2522326 -4.2576294 -4.2664042 -4.2668614 -4.2625828 -4.2567444 -4.2552733 -4.258637 -4.2630391 -4.263217 -4.2666535][-4.2885852 -4.29231 -4.29078 -4.2866516 -4.2853317 -4.2906084 -4.295723 -4.2933745 -4.2850089 -4.2753477 -4.27018 -4.2692304 -4.2718987 -4.2747836 -4.281528][-4.3014989 -4.306767 -4.3076291 -4.3045845 -4.302516 -4.3039913 -4.3068328 -4.3053865 -4.2990108 -4.2909789 -4.2859278 -4.2827234 -4.2820425 -4.2837296 -4.2915416]]...]
INFO - root - 2017-12-07 22:52:23.176045: step 63110, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 54h:28m:44s remains)
INFO - root - 2017-12-07 22:52:30.120187: step 63120, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 49h:00m:31s remains)
INFO - root - 2017-12-07 22:52:36.951655: step 63130, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 49h:55m:58s remains)
INFO - root - 2017-12-07 22:52:43.733259: step 63140, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 51h:15m:35s remains)
INFO - root - 2017-12-07 22:52:50.565079: step 63150, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 55h:00m:54s remains)
INFO - root - 2017-12-07 22:52:57.367151: step 63160, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.619 sec/batch; 46h:19m:24s remains)
INFO - root - 2017-12-07 22:53:04.236556: step 63170, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 53h:07m:13s remains)
INFO - root - 2017-12-07 22:53:11.012802: step 63180, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 47h:08m:22s remains)
INFO - root - 2017-12-07 22:53:17.831754: step 63190, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 51h:44m:13s remains)
INFO - root - 2017-12-07 22:53:24.264727: step 63200, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 54h:00m:51s remains)
2017-12-07 22:53:25.086159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.316134 -4.3156247 -4.3108945 -4.3075056 -4.3033624 -4.2966652 -4.2887497 -4.2779374 -4.264111 -4.2415285 -4.2204905 -4.2094841 -4.20217 -4.2080779 -4.2208295][-4.311934 -4.3076148 -4.2993846 -4.2948279 -4.2921386 -4.2886372 -4.2837114 -4.2744174 -4.2613211 -4.2431593 -4.2280965 -4.2185802 -4.2070403 -4.2082253 -4.2201052][-4.3199921 -4.3130126 -4.3030767 -4.29842 -4.2964373 -4.2928424 -4.2872195 -4.2760963 -4.2613535 -4.247499 -4.235692 -4.2248034 -4.2110844 -4.2101483 -4.221909][-4.3233914 -4.31715 -4.3081322 -4.3038707 -4.2998471 -4.2923312 -4.2811694 -4.2655692 -4.2506585 -4.2398467 -4.2304254 -4.2217894 -4.2133217 -4.2141795 -4.2271543][-4.3057976 -4.3006544 -4.2928863 -4.2861447 -4.2769866 -4.2635331 -4.246562 -4.2287726 -4.2164631 -4.2104297 -4.2038841 -4.1984186 -4.1986346 -4.2057829 -4.221271][-4.2653737 -4.2602129 -4.2534742 -4.2435379 -4.2307692 -4.2148967 -4.19695 -4.18092 -4.1727891 -4.1701546 -4.165103 -4.1608324 -4.1664081 -4.1803522 -4.1976233][-4.2170053 -4.2124376 -4.2057934 -4.1918793 -4.1753492 -4.1561451 -4.1370139 -4.1232204 -4.1210995 -4.1247444 -4.1242404 -4.1199341 -4.1267595 -4.1425543 -4.15703][-4.1801915 -4.1753478 -4.1642175 -4.1426482 -4.1207266 -4.09893 -4.079917 -4.0708027 -4.0785842 -4.0904994 -4.0908961 -4.0824218 -4.0855041 -4.0980883 -4.1088791][-4.1655149 -4.158257 -4.138237 -4.1064363 -4.0794625 -4.06124 -4.0502625 -4.0510726 -4.0708232 -4.0896664 -4.085113 -4.0677023 -4.063448 -4.0724287 -4.0803351][-4.1734681 -4.1621966 -4.1340237 -4.0969028 -4.072495 -4.0670152 -4.0703197 -4.0801263 -4.1025395 -4.1176062 -4.1055732 -4.08095 -4.0738935 -4.0847583 -4.09675][-4.1957684 -4.1839228 -4.1542974 -4.1189227 -4.1015773 -4.1059628 -4.1178451 -4.1298218 -4.1467085 -4.1554127 -4.1429453 -4.1235728 -4.1228013 -4.13949 -4.1563354][-4.2175732 -4.2065892 -4.1756477 -4.14005 -4.1263304 -4.1341553 -4.1499271 -4.1640563 -4.1787391 -4.1887217 -4.1883078 -4.1837826 -4.1919756 -4.2086706 -4.2193551][-4.2328739 -4.2210684 -4.1866069 -4.1462264 -4.130517 -4.1372118 -4.1566114 -4.1767406 -4.1958079 -4.212079 -4.2232208 -4.230844 -4.2424932 -4.2523584 -4.2500768][-4.2414951 -4.2277155 -4.1898208 -4.1440406 -4.1236644 -4.1266885 -4.1472936 -4.1709394 -4.1922064 -4.2137256 -4.2318954 -4.2453232 -4.2565489 -4.2579665 -4.2445707][-4.2437038 -4.2284055 -4.1893516 -4.1425462 -4.1210904 -4.1223121 -4.1416392 -4.1657438 -4.1876163 -4.2101769 -4.2274423 -4.2383323 -4.2438321 -4.2351604 -4.2125096]]...]
INFO - root - 2017-12-07 22:53:31.604553: step 63210, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 48h:02m:11s remains)
INFO - root - 2017-12-07 22:53:38.445989: step 63220, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.747 sec/batch; 55h:50m:22s remains)
INFO - root - 2017-12-07 22:53:45.233669: step 63230, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 53h:56m:47s remains)
INFO - root - 2017-12-07 22:53:51.996293: step 63240, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 53h:45m:45s remains)
INFO - root - 2017-12-07 22:53:58.801464: step 63250, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 46h:41m:19s remains)
INFO - root - 2017-12-07 22:54:05.448348: step 63260, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 46h:54m:10s remains)
INFO - root - 2017-12-07 22:54:12.371553: step 63270, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 53h:46m:59s remains)
INFO - root - 2017-12-07 22:54:19.152360: step 63280, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 54h:50m:44s remains)
INFO - root - 2017-12-07 22:54:25.935935: step 63290, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 49h:37m:12s remains)
INFO - root - 2017-12-07 22:54:32.493395: step 63300, loss = 2.05, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 46h:32m:14s remains)
2017-12-07 22:54:33.267463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3071694 -4.3074307 -4.3086944 -4.3096838 -4.3101077 -4.3096313 -4.3079648 -4.3053308 -4.3034167 -4.3028407 -4.3025384 -4.3017936 -4.3007822 -4.2992768 -4.2977614][-4.3089552 -4.3086858 -4.3095856 -4.3099351 -4.3092871 -4.3087173 -4.3081679 -4.3070621 -4.3056211 -4.3040423 -4.3028731 -4.3023562 -4.302495 -4.3026295 -4.3032007][-4.3053923 -4.3073554 -4.3065634 -4.3026342 -4.2972832 -4.2936616 -4.2934108 -4.2951303 -4.2979174 -4.3001723 -4.3001132 -4.298646 -4.297205 -4.2960534 -4.2974324][-4.2865381 -4.2964578 -4.2968059 -4.2858162 -4.2708807 -4.2610955 -4.2591319 -4.2625666 -4.2711754 -4.2823215 -4.2893138 -4.2882075 -4.2831869 -4.2775059 -4.2759905][-4.2457719 -4.2680078 -4.2728491 -4.2548871 -4.2249508 -4.2029438 -4.1974573 -4.2036052 -4.2202005 -4.244452 -4.2641582 -4.2667065 -4.25572 -4.2409744 -4.2332759][-4.1854272 -4.222435 -4.2365136 -4.2125492 -4.1639023 -4.119647 -4.1037879 -4.1131291 -4.1421919 -4.18326 -4.2199368 -4.2302032 -4.2108817 -4.1799746 -4.1603837][-4.1177182 -4.17278 -4.2017584 -4.179956 -4.1148934 -4.0417738 -4.0017381 -4.0067282 -4.0487814 -4.1098123 -4.1672626 -4.1906381 -4.1659465 -4.1126814 -4.0669913][-4.0714521 -4.137641 -4.1846886 -4.1748381 -4.1109176 -4.0240459 -3.9593153 -3.9436195 -3.9826951 -4.0573864 -4.1343651 -4.1738844 -4.1526871 -4.0852156 -4.0084715][-4.0685573 -4.127336 -4.1801219 -4.183753 -4.1346655 -4.0593624 -3.99621 -3.9672058 -3.9871871 -4.0540504 -4.1352148 -4.1851554 -4.1766248 -4.1137304 -4.0266142][-4.1121244 -4.1461225 -4.1849909 -4.1916275 -4.1603804 -4.1093965 -4.0668283 -4.0461664 -4.0551715 -4.0965071 -4.1573334 -4.2012486 -4.200789 -4.1531391 -4.0816288][-4.1718836 -4.1833825 -4.1986022 -4.1983867 -4.1807652 -4.1522841 -4.1316195 -4.1239305 -4.1339903 -4.158915 -4.1927514 -4.2162032 -4.2124395 -4.1795673 -4.1323051][-4.2147584 -4.21081 -4.209229 -4.2043056 -4.1930265 -4.1807532 -4.1774697 -4.1809821 -4.192688 -4.2065516 -4.2216935 -4.2258978 -4.2131348 -4.1910577 -4.1683207][-4.2378454 -4.2235785 -4.2141542 -4.2082281 -4.2013884 -4.2013617 -4.2109513 -4.2250156 -4.2362247 -4.2365775 -4.2343197 -4.2267189 -4.2102132 -4.1969485 -4.1929574][-4.2550583 -4.2369728 -4.223352 -4.2147641 -4.2116504 -4.2204647 -4.2386723 -4.2582855 -4.2636251 -4.2503476 -4.2316008 -4.213088 -4.19877 -4.1991463 -4.2126284][-4.271862 -4.2553215 -4.2381964 -4.22537 -4.2224789 -4.2336283 -4.2533512 -4.2716751 -4.270503 -4.2484441 -4.2182794 -4.1920133 -4.1826544 -4.1972189 -4.224062]]...]
INFO - root - 2017-12-07 22:54:40.132143: step 63310, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 53h:53m:57s remains)
INFO - root - 2017-12-07 22:54:46.948077: step 63320, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 49h:30m:34s remains)
INFO - root - 2017-12-07 22:54:53.768616: step 63330, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 47h:06m:02s remains)
INFO - root - 2017-12-07 22:55:00.640758: step 63340, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 54h:53m:20s remains)
INFO - root - 2017-12-07 22:55:07.516259: step 63350, loss = 2.05, batch loss = 2.00 (10.6 examples/sec; 0.752 sec/batch; 56h:11m:54s remains)
INFO - root - 2017-12-07 22:55:14.397218: step 63360, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 51h:53m:55s remains)
INFO - root - 2017-12-07 22:55:21.174123: step 63370, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 49h:06m:51s remains)
INFO - root - 2017-12-07 22:55:28.076870: step 63380, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 50h:51m:09s remains)
INFO - root - 2017-12-07 22:55:34.982838: step 63390, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 55h:14m:55s remains)
INFO - root - 2017-12-07 22:55:41.593562: step 63400, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.717 sec/batch; 53h:37m:36s remains)
2017-12-07 22:55:42.448807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2667127 -4.2927942 -4.3093362 -4.3206172 -4.3260331 -4.3252773 -4.3210411 -4.3176947 -4.31653 -4.3162603 -4.3141909 -4.3131561 -4.3159342 -4.3168364 -4.3164015][-4.2845111 -4.3046169 -4.3142824 -4.3188987 -4.3188663 -4.3167925 -4.3126812 -4.3112969 -4.3111033 -4.3109446 -4.3088942 -4.3065686 -4.3065338 -4.3076844 -4.3097849][-4.3014469 -4.3126459 -4.3114014 -4.3049169 -4.2976213 -4.2913928 -4.2784572 -4.2668786 -4.2562656 -4.2509623 -4.2527456 -4.2589049 -4.2704067 -4.2846627 -4.2953057][-4.3243804 -4.3205709 -4.3019819 -4.2754464 -4.2500172 -4.2279053 -4.1960831 -4.1677303 -4.1445251 -4.1353426 -4.1487608 -4.175477 -4.2112513 -4.2479773 -4.2717576][-4.3309031 -4.3089585 -4.268487 -4.216217 -4.1659636 -4.120522 -4.0602803 -4.0096321 -3.9778745 -3.979768 -4.0212994 -4.0779414 -4.1417923 -4.2015176 -4.2393422][-4.3216105 -4.2847905 -4.2239351 -4.1417775 -4.0648189 -3.995172 -3.9032493 -3.8166103 -3.7750533 -3.8046615 -3.8888991 -3.9761026 -4.0651374 -4.1491776 -4.204392][-4.2987142 -4.2506824 -4.1711121 -4.0572124 -3.9619927 -3.8742919 -3.7515035 -3.6189988 -3.5623114 -3.6314297 -3.7723484 -3.8932979 -4.0025229 -4.1050296 -4.1768956][-4.2627339 -4.2075338 -4.1141572 -3.9854584 -3.8938165 -3.8078895 -3.6871605 -3.551126 -3.4979379 -3.5991454 -3.7743533 -3.9110184 -4.0188508 -4.1174812 -4.184535][-4.2263646 -4.1819229 -4.1071448 -4.007103 -3.9465609 -3.8872848 -3.8098245 -3.7276919 -3.7149792 -3.8058386 -3.9392002 -4.0410342 -4.1146593 -4.1769824 -4.2145214][-4.1999545 -4.1794367 -4.1363683 -4.0787659 -4.0446696 -4.0095015 -3.9716711 -3.9393888 -3.9523337 -4.0113826 -4.087904 -4.1519709 -4.1949596 -4.2248898 -4.23532][-4.1898127 -4.1823483 -4.1565061 -4.127141 -4.1084342 -4.0915928 -4.0783281 -4.0723281 -4.0912261 -4.1266518 -4.1654544 -4.1989279 -4.223783 -4.2368159 -4.2352366][-4.1997209 -4.1976275 -4.1820707 -4.1699643 -4.1655712 -4.1631212 -4.1600351 -4.1616306 -4.1771441 -4.1985788 -4.2165728 -4.2259092 -4.2337222 -4.236815 -4.2334118][-4.2170329 -4.2135758 -4.2024751 -4.2029963 -4.2107615 -4.21707 -4.2156811 -4.2132688 -4.2158813 -4.2222934 -4.2265396 -4.2237854 -4.224896 -4.2271013 -4.2312913][-4.2397532 -4.2371492 -4.2298269 -4.2338076 -4.2427335 -4.2496071 -4.2474656 -4.243535 -4.2381487 -4.2358732 -4.2317486 -4.2231312 -4.2231297 -4.2284484 -4.237947][-4.268867 -4.2683806 -4.2644253 -4.2700291 -4.2760692 -4.2793388 -4.2781386 -4.2732863 -4.2637272 -4.2594252 -4.2559495 -4.2494469 -4.2526021 -4.2596803 -4.2650805]]...]
INFO - root - 2017-12-07 22:55:49.267261: step 63410, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.749 sec/batch; 55h:57m:53s remains)
INFO - root - 2017-12-07 22:55:56.064680: step 63420, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 51h:40m:39s remains)
INFO - root - 2017-12-07 22:56:02.827433: step 63430, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 50h:40m:26s remains)
INFO - root - 2017-12-07 22:56:09.639344: step 63440, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 46h:20m:55s remains)
INFO - root - 2017-12-07 22:56:16.468919: step 63450, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 50h:22m:54s remains)
INFO - root - 2017-12-07 22:56:23.264505: step 63460, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 53h:29m:19s remains)
INFO - root - 2017-12-07 22:56:30.091820: step 63470, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 54h:59m:02s remains)
INFO - root - 2017-12-07 22:56:36.857400: step 63480, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 48h:31m:18s remains)
INFO - root - 2017-12-07 22:56:43.599471: step 63490, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 46h:47m:07s remains)
INFO - root - 2017-12-07 22:56:50.242099: step 63500, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 53h:29m:55s remains)
2017-12-07 22:56:50.967056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2481103 -4.2250347 -4.2121291 -4.20083 -4.1972327 -4.2044811 -4.224956 -4.2485309 -4.2624803 -4.2706184 -4.2775788 -4.2845416 -4.2875862 -4.2928653 -4.2980866][-4.2681923 -4.2423124 -4.2169929 -4.1923532 -4.1822128 -4.18942 -4.2114735 -4.2384176 -4.2565136 -4.2699552 -4.2809777 -4.291007 -4.2953978 -4.3012371 -4.3061109][-4.2874832 -4.25736 -4.2219834 -4.1866655 -4.1690235 -4.173233 -4.1939764 -4.2208533 -4.2450924 -4.2679443 -4.2877717 -4.3023338 -4.3068271 -4.3110623 -4.3160682][-4.2963457 -4.2587471 -4.2132621 -4.1707082 -4.1497464 -4.1563306 -4.1757765 -4.2015696 -4.2303681 -4.260396 -4.2882953 -4.3072085 -4.3115344 -4.3129354 -4.3170748][-4.297997 -4.257021 -4.2058673 -4.15951 -4.13893 -4.1442852 -4.158011 -4.1832514 -4.2185774 -4.2553082 -4.287447 -4.3064537 -4.307694 -4.3039093 -4.3045692][-4.2966089 -4.2546034 -4.2034106 -4.159698 -4.1381669 -4.1353545 -4.1368723 -4.1588168 -4.200036 -4.2415233 -4.2756534 -4.29542 -4.2964182 -4.2915173 -4.2908511][-4.28339 -4.2373276 -4.1887937 -4.1537423 -4.136744 -4.1274242 -4.1199884 -4.1392493 -4.1806107 -4.2211819 -4.2560067 -4.2768068 -4.278976 -4.2750812 -4.2750497][-4.2538538 -4.2022648 -4.1595092 -4.140964 -4.13872 -4.1324496 -4.1248908 -4.1423845 -4.1780624 -4.2121878 -4.2421784 -4.26011 -4.2628469 -4.2612433 -4.26587][-4.2193356 -4.1665554 -4.1339321 -4.1338882 -4.1485386 -4.1495867 -4.1442881 -4.1558228 -4.1806345 -4.2057567 -4.2293692 -4.2438083 -4.2492232 -4.2516584 -4.2611442][-4.1900511 -4.141655 -4.1188178 -4.131433 -4.1560993 -4.1644397 -4.1591496 -4.1591606 -4.1692605 -4.1871972 -4.208437 -4.2247672 -4.2361617 -4.2453842 -4.2593956][-4.1653242 -4.1235213 -4.1101646 -4.129221 -4.159482 -4.1747546 -4.1678224 -4.1534214 -4.1471996 -4.1596169 -4.1837234 -4.2066841 -4.2271938 -4.2429619 -4.2561707][-4.1453977 -4.1123061 -4.1091323 -4.1326141 -4.1634507 -4.1845479 -4.1792912 -4.1578 -4.1398869 -4.1453347 -4.1706419 -4.2003503 -4.2250957 -4.24184 -4.251646][-4.1373129 -4.1178041 -4.1238575 -4.1482444 -4.1779752 -4.2017279 -4.20215 -4.1841459 -4.1642103 -4.1628218 -4.1838231 -4.21137 -4.2300668 -4.2408419 -4.2474666][-4.1457891 -4.1422582 -4.153616 -4.1769876 -4.2043858 -4.2273083 -4.2329988 -4.2223268 -4.2058845 -4.2002959 -4.2133794 -4.2298069 -4.234056 -4.2335086 -4.2376385][-4.1629286 -4.1752315 -4.1911831 -4.2144794 -4.2386618 -4.2580795 -4.2640996 -4.2585945 -4.246058 -4.2373981 -4.2408223 -4.2432752 -4.2300305 -4.2195282 -4.22504]]...]
INFO - root - 2017-12-07 22:56:57.775162: step 63510, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 49h:16m:35s remains)
INFO - root - 2017-12-07 22:57:04.456116: step 63520, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.749 sec/batch; 55h:59m:29s remains)
INFO - root - 2017-12-07 22:57:11.298435: step 63530, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 53h:33m:02s remains)
INFO - root - 2017-12-07 22:57:18.126698: step 63540, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 49h:15m:19s remains)
INFO - root - 2017-12-07 22:57:24.926715: step 63550, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 47h:30m:37s remains)
INFO - root - 2017-12-07 22:57:31.762362: step 63560, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 53h:48m:36s remains)
INFO - root - 2017-12-07 22:57:38.530292: step 63570, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 49h:57m:20s remains)
INFO - root - 2017-12-07 22:57:45.254342: step 63580, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 50h:20m:03s remains)
INFO - root - 2017-12-07 22:57:51.979992: step 63590, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 50h:16m:53s remains)
INFO - root - 2017-12-07 22:57:58.592169: step 63600, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 47h:21m:25s remains)
2017-12-07 22:57:59.388522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1214013 -4.1540079 -4.1877184 -4.2022781 -4.2056756 -4.2055211 -4.2004728 -4.2043176 -4.2184844 -4.2337093 -4.2384453 -4.2265162 -4.2068887 -4.2027411 -4.2115602][-4.1763291 -4.2014341 -4.2224188 -4.2256536 -4.2160335 -4.2063141 -4.2009764 -4.2103138 -4.22856 -4.2457895 -4.2507815 -4.2415485 -4.2284989 -4.2273736 -4.2319727][-4.2268906 -4.2459254 -4.2531948 -4.2453041 -4.2232161 -4.205657 -4.2020473 -4.2165909 -4.2373915 -4.2583528 -4.2625012 -4.254766 -4.24486 -4.2457876 -4.2458677][-4.2495685 -4.2593679 -4.2581329 -4.2426062 -4.2093959 -4.183888 -4.1841311 -4.2083859 -4.2340055 -4.2597084 -4.2621064 -4.2482014 -4.2328625 -4.2304211 -4.225863][-4.2070918 -4.2117491 -4.2118673 -4.1940231 -4.1509857 -4.1144295 -4.1171985 -4.1579475 -4.1978316 -4.2292113 -4.2266712 -4.2002 -4.1726675 -4.1560221 -4.14983][-4.119967 -4.1215391 -4.1265965 -4.1082354 -4.062623 -4.0164557 -4.0068827 -4.0592842 -4.1160889 -4.1522355 -4.1477261 -4.113162 -4.0838394 -4.0674362 -4.0688019][-4.0622721 -4.051949 -4.0507536 -4.029551 -3.984163 -3.92068 -3.8812187 -3.9264057 -4.00141 -4.0535665 -4.0569344 -4.0295405 -4.0149822 -4.0223918 -4.0468144][-4.0791106 -4.0503335 -4.0290213 -3.9944158 -3.9347546 -3.8478186 -3.7618 -3.7703714 -3.8549247 -3.9331341 -3.9665065 -3.9716022 -3.9945748 -4.0387444 -4.087647][-4.1505775 -4.109695 -4.0755033 -4.0356688 -3.9745014 -3.8881917 -3.7913249 -3.7620778 -3.8275266 -3.9094431 -3.9609349 -3.9957106 -4.0473609 -4.1103835 -4.1633735][-4.2247138 -4.1845136 -4.1474056 -4.110043 -4.0622778 -3.9988189 -3.9252691 -3.8899856 -3.9255347 -3.9845119 -4.0297532 -4.0716186 -4.1325722 -4.195765 -4.2410517][-4.2621922 -4.2318888 -4.1988578 -4.1706028 -4.1412334 -4.1012297 -4.0555644 -4.0375562 -4.0577989 -4.0895243 -4.115736 -4.1501112 -4.2035608 -4.2531357 -4.2830095][-4.2538157 -4.2372165 -4.2183342 -4.2060571 -4.1997919 -4.1830478 -4.1535149 -4.1422086 -4.15107 -4.1666012 -4.1827 -4.2077804 -4.2418947 -4.2724209 -4.2912364][-4.2470341 -4.2438612 -4.2423158 -4.249722 -4.2618122 -4.2607732 -4.242836 -4.2312627 -4.2339563 -4.2407255 -4.250802 -4.2646322 -4.2818093 -4.2980404 -4.3099408][-4.2230639 -4.2370787 -4.2507739 -4.2620931 -4.2751513 -4.2787051 -4.2730508 -4.2685018 -4.2707286 -4.2742019 -4.2816834 -4.29014 -4.2995224 -4.3085675 -4.3156838][-4.184587 -4.2145562 -4.2465796 -4.2617445 -4.2721539 -4.275703 -4.2773314 -4.2784214 -4.280232 -4.2820144 -4.2858844 -4.2894964 -4.2945838 -4.2982283 -4.3001575]]...]
INFO - root - 2017-12-07 22:58:06.305237: step 63610, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 52h:56m:12s remains)
INFO - root - 2017-12-07 22:58:13.098575: step 63620, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 47h:58m:41s remains)
INFO - root - 2017-12-07 22:58:19.854463: step 63630, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 46h:10m:08s remains)
INFO - root - 2017-12-07 22:58:26.663910: step 63640, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 50h:36m:37s remains)
INFO - root - 2017-12-07 22:58:33.495547: step 63650, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 53h:12m:00s remains)
INFO - root - 2017-12-07 22:58:40.286545: step 63660, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 51h:36m:26s remains)
INFO - root - 2017-12-07 22:58:47.079674: step 63670, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 47h:44m:00s remains)
INFO - root - 2017-12-07 22:58:53.813616: step 63680, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 48h:48m:22s remains)
INFO - root - 2017-12-07 22:59:00.659570: step 63690, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 54h:45m:05s remains)
INFO - root - 2017-12-07 22:59:07.216154: step 63700, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 52h:23m:10s remains)
2017-12-07 22:59:07.961996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.331346 -4.3310843 -4.3278813 -4.3210506 -4.3150496 -4.3113675 -4.3112774 -4.3129997 -4.3140655 -4.3153863 -4.3219662 -4.3313789 -4.3390484 -4.3422842 -4.34161][-4.3214207 -4.3220935 -4.3170338 -4.3044553 -4.29523 -4.2931042 -4.295599 -4.2973514 -4.2981935 -4.3011317 -4.3112221 -4.3238091 -4.3351736 -4.3429179 -4.3456798][-4.2953572 -4.2968254 -4.2870631 -4.26285 -4.2473564 -4.2473931 -4.2528939 -4.256547 -4.2631245 -4.2730293 -4.2902718 -4.3053904 -4.3177953 -4.3298788 -4.339376][-4.260139 -4.260323 -4.2412868 -4.2024245 -4.1809368 -4.1815405 -4.1848607 -4.1895523 -4.2051544 -4.2316804 -4.262115 -4.2789168 -4.2898035 -4.30314 -4.3198776][-4.225359 -4.2233615 -4.1961522 -4.1452003 -4.1134338 -4.1044288 -4.0978169 -4.0999742 -4.1260953 -4.1738257 -4.22123 -4.2443581 -4.2550478 -4.2691746 -4.2891512][-4.1930413 -4.1913691 -4.1611238 -4.1031346 -4.0552664 -4.0293345 -4.0084176 -4.0072412 -4.0424943 -4.1052737 -4.1676106 -4.203537 -4.2203779 -4.23545 -4.25297][-4.1766906 -4.1784415 -4.1557975 -4.1048098 -4.0517087 -4.0114889 -3.9787869 -3.9707756 -3.9994254 -4.0541439 -4.1133752 -4.1597815 -4.1865654 -4.2019706 -4.2138004][-4.176671 -4.1809154 -4.1708946 -4.139596 -4.0948849 -4.0519533 -4.0179029 -4.007926 -4.0225458 -4.0527096 -4.0921745 -4.1339211 -4.1624985 -4.1730275 -4.1775427][-4.1779962 -4.1802998 -4.1790638 -4.1657953 -4.1364894 -4.0965972 -4.0630412 -4.0540094 -4.0633221 -4.0814509 -4.1074452 -4.1368365 -4.1578703 -4.1584854 -4.1546259][-4.1809053 -4.1799626 -4.1823649 -4.1798782 -4.1661844 -4.1379194 -4.1106715 -4.1058574 -4.1147728 -4.128582 -4.1447091 -4.1579218 -4.1654844 -4.156961 -4.1469955][-4.1920104 -4.1878824 -4.18719 -4.1865239 -4.1822052 -4.1678429 -4.1536059 -4.1585655 -4.1708536 -4.1833992 -4.1901546 -4.18723 -4.1791253 -4.1645231 -4.1543941][-4.2096457 -4.2062206 -4.2066464 -4.2068911 -4.207365 -4.2024608 -4.1992788 -4.2098494 -4.2253122 -4.23693 -4.2367225 -4.2209225 -4.2019672 -4.187984 -4.1835585][-4.2187572 -4.2136574 -4.2153897 -4.2192121 -4.223371 -4.221231 -4.219172 -4.2296586 -4.2422285 -4.2502651 -4.2485666 -4.2330761 -4.2208366 -4.2184477 -4.2220688][-4.2183008 -4.2115388 -4.2136431 -4.2201886 -4.2237697 -4.2154913 -4.209384 -4.2175355 -4.2273622 -4.2352562 -4.2360258 -4.2311163 -4.2306714 -4.2370281 -4.2474289][-4.2132359 -4.2020478 -4.1997776 -4.2038522 -4.1988444 -4.1800094 -4.1673088 -4.1741319 -4.186152 -4.1983614 -4.1997046 -4.200861 -4.2096581 -4.225172 -4.2479954]]...]
INFO - root - 2017-12-07 22:59:14.743132: step 63710, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 47h:15m:05s remains)
INFO - root - 2017-12-07 22:59:21.542533: step 63720, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 53h:11m:58s remains)
INFO - root - 2017-12-07 22:59:28.205180: step 63730, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 51h:11m:57s remains)
INFO - root - 2017-12-07 22:59:35.060390: step 63740, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 49h:44m:17s remains)
INFO - root - 2017-12-07 22:59:41.906797: step 63750, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 47h:37m:55s remains)
INFO - root - 2017-12-07 22:59:48.710970: step 63760, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 48h:27m:16s remains)
INFO - root - 2017-12-07 22:59:55.559913: step 63770, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 53h:42m:59s remains)
INFO - root - 2017-12-07 23:00:02.403884: step 63780, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 54h:14m:25s remains)
INFO - root - 2017-12-07 23:00:09.196918: step 63790, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 49h:59m:41s remains)
INFO - root - 2017-12-07 23:00:15.788344: step 63800, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 46h:35m:46s remains)
2017-12-07 23:00:16.571893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1370296 -4.1234708 -4.1316328 -4.1656175 -4.1925964 -4.2040052 -4.2105227 -4.2148333 -4.2176838 -4.2188592 -4.2162108 -4.2132792 -4.2117047 -4.2094049 -4.2074771][-4.1247759 -4.1050611 -4.1130118 -4.146471 -4.1747351 -4.1891708 -4.2019434 -4.2144895 -4.2220845 -4.2258859 -4.2252784 -4.2238703 -4.2204576 -4.2145967 -4.2118759][-4.1406851 -4.1152186 -4.1185427 -4.1464462 -4.1724324 -4.1865239 -4.2030773 -4.2264032 -4.243062 -4.2489772 -4.2488966 -4.249598 -4.2435937 -4.232048 -4.2265348][-4.15591 -4.1242609 -4.1200547 -4.1394305 -4.1612239 -4.1711464 -4.1864381 -4.2173524 -4.2458396 -4.2576804 -4.2596006 -4.26465 -4.2561803 -4.2397842 -4.2309251][-4.1505632 -4.1107025 -4.0974007 -4.1113052 -4.1259904 -4.1236563 -4.126514 -4.1604767 -4.2035131 -4.2239637 -4.2312 -4.2444959 -4.2387338 -4.2226567 -4.2143993][-4.1278467 -4.076232 -4.0458369 -4.05063 -4.0573015 -4.0415106 -4.0230703 -4.0570726 -4.1196613 -4.154129 -4.1707716 -4.1960187 -4.1950073 -4.1792021 -4.1686621][-4.1184959 -4.0593281 -4.0062313 -3.9867492 -3.9771278 -3.9357891 -3.8834915 -3.9135008 -4.0151715 -4.074255 -4.104475 -4.1486382 -4.15803 -4.1423306 -4.1283092][-4.1412597 -4.0914745 -4.03585 -3.9958732 -3.9633274 -3.8850729 -3.7799792 -3.791476 -3.9308097 -4.0136728 -4.0538092 -4.1091633 -4.1325693 -4.1261768 -4.11967][-4.1775374 -4.1464529 -4.1079168 -4.0727248 -4.0434694 -3.9743392 -3.8773868 -3.860559 -3.9681282 -4.0358291 -4.0611582 -4.1062469 -4.1358089 -4.1393127 -4.1440196][-4.2137017 -4.1985149 -4.17798 -4.1557875 -4.1360922 -4.092864 -4.037993 -4.0215287 -4.0796714 -4.1172805 -4.128016 -4.1571126 -4.1798887 -4.1831112 -4.188983][-4.2457671 -4.2387424 -4.2278767 -4.2118225 -4.1975207 -4.1716123 -4.1495037 -4.1417274 -4.1743388 -4.1958294 -4.203372 -4.2232676 -4.2385902 -4.2367835 -4.2381911][-4.2639871 -4.2616706 -4.2568345 -4.2437696 -4.231761 -4.2208748 -4.21484 -4.2140412 -4.2344332 -4.2490244 -4.2550321 -4.2669773 -4.276082 -4.2736897 -4.2728491][-4.2614532 -4.2614336 -4.2612991 -4.2533903 -4.2432451 -4.2394524 -4.2368073 -4.2382474 -4.2535272 -4.2679181 -4.2763233 -4.2848539 -4.2897158 -4.2873411 -4.2851386][-4.2481027 -4.2468982 -4.250526 -4.2499828 -4.246799 -4.2488332 -4.24974 -4.2520933 -4.262816 -4.2752562 -4.282691 -4.2876263 -4.289494 -4.2867789 -4.2826505][-4.2424984 -4.2385683 -4.2433186 -4.2482104 -4.2503171 -4.2550125 -4.2592134 -4.2623272 -4.2678809 -4.2749996 -4.2786946 -4.280715 -4.2824087 -4.2819071 -4.2772346]]...]
INFO - root - 2017-12-07 23:00:23.300457: step 63810, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 53h:45m:17s remains)
INFO - root - 2017-12-07 23:00:29.996631: step 63820, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 49h:41m:12s remains)
INFO - root - 2017-12-07 23:00:36.544467: step 63830, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 48h:36m:58s remains)
INFO - root - 2017-12-07 23:00:43.395152: step 63840, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 53h:48m:17s remains)
INFO - root - 2017-12-07 23:00:50.079908: step 63850, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 53h:02m:44s remains)
INFO - root - 2017-12-07 23:00:56.943025: step 63860, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 50h:41m:41s remains)
INFO - root - 2017-12-07 23:01:03.733124: step 63870, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 47h:53m:31s remains)
INFO - root - 2017-12-07 23:01:10.511431: step 63880, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 46h:52m:07s remains)
INFO - root - 2017-12-07 23:01:17.309438: step 63890, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 54h:05m:22s remains)
INFO - root - 2017-12-07 23:01:23.989791: step 63900, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 55h:07m:23s remains)
2017-12-07 23:01:24.796568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1755376 -4.1018224 -4.07161 -4.1020379 -4.1496229 -4.1800375 -4.2050171 -4.2294993 -4.2281122 -4.2233582 -4.210166 -4.2028742 -4.1981306 -4.1851864 -4.1876493][-4.1755462 -4.0958366 -4.0556164 -4.0761633 -4.1203661 -4.153995 -4.1765318 -4.205482 -4.2137251 -4.2172666 -4.202075 -4.1861138 -4.1817846 -4.1733913 -4.1714664][-4.1619744 -4.0708094 -4.0171013 -4.0325937 -4.07919 -4.1190968 -4.1415176 -4.1747708 -4.1951513 -4.2069588 -4.19162 -4.1665974 -4.1562753 -4.1468325 -4.1436448][-4.1468205 -4.0534086 -3.9908845 -4.0059218 -4.0520806 -4.0906835 -4.1077714 -4.135582 -4.1604834 -4.1833978 -4.1817656 -4.1609864 -4.1454597 -4.1321511 -4.1233559][-4.147862 -4.0631261 -3.994889 -4.0017624 -4.0372696 -4.0692725 -4.0838952 -4.1084023 -4.1340256 -4.1679811 -4.1828136 -4.1715031 -4.1524644 -4.1327291 -4.107152][-4.1488409 -4.0739188 -4.0059438 -4.0055771 -4.0268192 -4.0524449 -4.073669 -4.1024389 -4.131856 -4.172626 -4.1923308 -4.1839457 -4.1640635 -4.1404905 -4.1011581][-4.1497068 -4.08319 -4.0218911 -4.0244389 -4.0351043 -4.0510669 -4.068541 -4.1004243 -4.1348519 -4.1726012 -4.1851826 -4.1742749 -4.1553483 -4.133532 -4.0924387][-4.1460719 -4.0855775 -4.039813 -4.0493083 -4.0538235 -4.0571904 -4.0646882 -4.0949187 -4.1315861 -4.1568909 -4.1544805 -4.1409907 -4.1254926 -4.1106367 -4.0738835][-4.136899 -4.0793476 -4.0473289 -4.0641541 -4.073741 -4.0747633 -4.0811539 -4.0994864 -4.1294603 -4.1406717 -4.1263065 -4.111692 -4.1002011 -4.0954781 -4.0678549][-4.1368976 -4.077507 -4.0509219 -4.075901 -4.0959768 -4.1070948 -4.1185808 -4.1289377 -4.1453104 -4.1407738 -4.1171246 -4.103662 -4.0977368 -4.10375 -4.0910029][-4.1461434 -4.0865922 -4.064343 -4.0969963 -4.1221905 -4.1397309 -4.1543474 -4.1596365 -4.1690297 -4.1559291 -4.1286683 -4.1163144 -4.1159515 -4.1296277 -4.1261625][-4.1485772 -4.0882659 -4.0714822 -4.111649 -4.1352663 -4.1495266 -4.1603 -4.1635756 -4.1738524 -4.1648641 -4.1418591 -4.1302752 -4.1319661 -4.144166 -4.1455827][-4.1483965 -4.0894909 -4.0729065 -4.1091428 -4.1279359 -4.1377215 -4.1418815 -4.1385016 -4.146214 -4.1432581 -4.1244793 -4.1147375 -4.1216412 -4.131484 -4.1378279][-4.1631975 -4.1046066 -4.0822878 -4.1078429 -4.1165423 -4.1238632 -4.1314063 -4.1278648 -4.1327772 -4.1289482 -4.1089311 -4.0956168 -4.0992622 -4.1052637 -4.1152239][-4.1892171 -4.12911 -4.0956941 -4.1113858 -4.1142669 -4.1208453 -4.1359673 -4.1398487 -4.1470718 -4.1427927 -4.1218319 -4.1005511 -4.0931668 -4.0904546 -4.1003256]]...]
INFO - root - 2017-12-07 23:01:31.722278: step 63910, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.754 sec/batch; 56h:17m:00s remains)
INFO - root - 2017-12-07 23:01:38.525354: step 63920, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 52h:42m:07s remains)
INFO - root - 2017-12-07 23:01:45.324586: step 63930, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 51h:03m:26s remains)
INFO - root - 2017-12-07 23:01:52.064662: step 63940, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 47h:03m:14s remains)
INFO - root - 2017-12-07 23:01:58.912629: step 63950, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 50h:05m:33s remains)
INFO - root - 2017-12-07 23:02:05.662504: step 63960, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 53h:36m:15s remains)
INFO - root - 2017-12-07 23:02:12.468504: step 63970, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 48h:29m:11s remains)
INFO - root - 2017-12-07 23:02:19.251743: step 63980, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 49h:34m:10s remains)
INFO - root - 2017-12-07 23:02:26.064933: step 63990, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 47h:21m:34s remains)
INFO - root - 2017-12-07 23:02:32.736327: step 64000, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 51h:22m:18s remains)
2017-12-07 23:02:33.672498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2875304 -4.2882142 -4.2875338 -4.2840414 -4.2780833 -4.2733412 -4.2655392 -4.2559242 -4.2374582 -4.2042685 -4.1735659 -4.1597981 -4.1633263 -4.1774793 -4.1769314][-4.2895451 -4.2887373 -4.2851176 -4.2773628 -4.2664437 -4.2553549 -4.2418094 -4.2308683 -4.2129378 -4.1797595 -4.1492534 -4.1313643 -4.1285024 -4.1414204 -4.14927][-4.2801428 -4.2761559 -4.2689877 -4.2562666 -4.2387691 -4.2209229 -4.2017903 -4.1910973 -4.1819873 -4.1579032 -4.1298814 -4.107614 -4.1010213 -4.1137285 -4.138155][-4.2706022 -4.2639394 -4.2544894 -4.2332582 -4.2038493 -4.1707377 -4.140965 -4.133368 -4.1382623 -4.1265154 -4.104228 -4.0838585 -4.0751805 -4.0951962 -4.1429505][-4.26505 -4.2571259 -4.2414293 -4.2076516 -4.1606183 -4.10719 -4.0619793 -4.0547619 -4.0744395 -4.0832095 -4.0788708 -4.0672703 -4.0604992 -4.0881667 -4.1465988][-4.2547388 -4.2431931 -4.2183628 -4.172545 -4.10827 -4.032 -3.9650078 -3.95441 -3.99446 -4.03343 -4.058651 -4.0730891 -4.0809317 -4.1071291 -4.1564851][-4.242321 -4.2220821 -4.1846452 -4.1264024 -4.04592 -3.9459224 -3.8441277 -3.821475 -3.9062736 -3.9991496 -4.0664673 -4.1082516 -4.1286368 -4.1452632 -4.1737823][-4.2322373 -4.2008109 -4.1540704 -4.0881848 -3.9983809 -3.8872323 -3.7617788 -3.7383657 -3.876729 -4.0084248 -4.0920715 -4.1401324 -4.1623206 -4.1680655 -4.1825256][-4.2289948 -4.1942487 -4.146605 -4.0864997 -4.0105991 -3.9267316 -3.8371191 -3.8301184 -3.9436426 -4.0471487 -4.1092124 -4.1462717 -4.1670718 -4.1711736 -4.1850348][-4.2335849 -4.2013016 -4.1566191 -4.1105881 -4.0619254 -4.0145674 -3.9588587 -3.9514995 -4.0180459 -4.0833688 -4.1192756 -4.1438594 -4.164886 -4.1731105 -4.1878123][-4.2325482 -4.2004628 -4.1580167 -4.1222625 -4.093575 -4.0686307 -4.0349913 -4.0293212 -4.0756488 -4.1201525 -4.1373916 -4.1512713 -4.1690865 -4.178916 -4.1955285][-4.2200041 -4.1879778 -4.1476417 -4.1185007 -4.1045218 -4.0966659 -4.0814033 -4.0803676 -4.1182203 -4.1541896 -4.1650534 -4.1714416 -4.1799688 -4.1847725 -4.1981874][-4.1992049 -4.1716309 -4.1308174 -4.1080747 -4.1098642 -4.11528 -4.113513 -4.1157222 -4.1475644 -4.1797056 -4.1923 -4.1972938 -4.2014885 -4.2039161 -4.2145214][-4.187294 -4.1623321 -4.122128 -4.1077089 -4.1193275 -4.1301737 -4.1326714 -4.1346912 -4.1615272 -4.1948123 -4.2109966 -4.2178 -4.2218766 -4.2238812 -4.2307873][-4.2025805 -4.1844726 -4.1515207 -4.1388097 -4.1452832 -4.1522274 -4.1534772 -4.1567707 -4.1775517 -4.205936 -4.2188158 -4.2208757 -4.2234621 -4.2251353 -4.2311425]]...]
INFO - root - 2017-12-07 23:02:40.387904: step 64010, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 46h:33m:24s remains)
INFO - root - 2017-12-07 23:02:47.206879: step 64020, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 51h:03m:46s remains)
INFO - root - 2017-12-07 23:02:54.030176: step 64030, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 51h:11m:08s remains)
INFO - root - 2017-12-07 23:03:00.818079: step 64040, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 49h:20m:01s remains)
INFO - root - 2017-12-07 23:03:07.677535: step 64050, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 46h:38m:38s remains)
INFO - root - 2017-12-07 23:03:14.470212: step 64060, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 48h:06m:41s remains)
INFO - root - 2017-12-07 23:03:21.224634: step 64070, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 50h:03m:07s remains)
INFO - root - 2017-12-07 23:03:28.096882: step 64080, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 52h:39m:14s remains)
INFO - root - 2017-12-07 23:03:34.919730: step 64090, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 51h:56m:46s remains)
INFO - root - 2017-12-07 23:03:41.529627: step 64100, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 47h:09m:33s remains)
2017-12-07 23:03:42.372975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2519078 -4.2607164 -4.2633119 -4.2612233 -4.2682009 -4.283659 -4.2954674 -4.3014417 -4.3027453 -4.2971482 -4.2841978 -4.2626991 -4.2347188 -4.2225995 -4.2332792][-4.2280893 -4.2368646 -4.2382412 -4.2362175 -4.247344 -4.2682114 -4.2842703 -4.2985663 -4.312119 -4.3153934 -4.3016982 -4.2736759 -4.2387853 -4.2216167 -4.2315054][-4.2029896 -4.2078271 -4.207417 -4.2065 -4.2201967 -4.2455583 -4.2662683 -4.2886395 -4.3084412 -4.3181024 -4.3108149 -4.2814713 -4.2409992 -4.2164936 -4.2218242][-4.1994114 -4.1958671 -4.1883054 -4.186316 -4.1966624 -4.2205477 -4.2449727 -4.2703948 -4.2904639 -4.3028049 -4.3026671 -4.2791238 -4.240869 -4.2149091 -4.2141929][-4.1971583 -4.1848631 -4.1686764 -4.1614375 -4.1628475 -4.1808124 -4.2073932 -4.235014 -4.2557907 -4.2704058 -4.2749114 -4.2582355 -4.2271595 -4.2049818 -4.2028265][-4.1857147 -4.1644664 -4.141068 -4.1326947 -4.1298075 -4.1420259 -4.1646695 -4.1889176 -4.2096882 -4.2250156 -4.234179 -4.224205 -4.2004323 -4.1808896 -4.1784697][-4.1678944 -4.1424556 -4.1152544 -4.1039119 -4.0972438 -4.1006393 -4.117198 -4.1371708 -4.1603632 -4.1821208 -4.1977525 -4.1926208 -4.1743655 -4.1544476 -4.1487069][-4.1555309 -4.127286 -4.0915327 -4.068501 -4.0514789 -4.041501 -4.0511661 -4.0688734 -4.0968308 -4.1340532 -4.1612978 -4.161346 -4.1479206 -4.1295681 -4.1226225][-4.1704292 -4.1391454 -4.0915604 -4.047534 -4.0126963 -3.9836483 -3.9816532 -3.992547 -4.0164208 -4.0579586 -4.1009378 -4.116818 -4.1111503 -4.0972071 -4.0947905][-4.2122121 -4.1847386 -4.1364183 -4.0826125 -4.0384016 -3.997364 -3.9787283 -3.973706 -3.9759426 -4.00258 -4.0503287 -4.0824814 -4.0869865 -4.0794168 -4.0827174][-4.2390518 -4.2258534 -4.1943207 -4.1483703 -4.1082258 -4.0739765 -4.0535769 -4.0385351 -4.0217009 -4.0261545 -4.060863 -4.0924425 -4.0995984 -4.0951004 -4.10105][-4.2178822 -4.2259517 -4.2241077 -4.2019167 -4.1756978 -4.1545157 -4.1409955 -4.1290827 -4.1131372 -4.1097908 -4.1295981 -4.14999 -4.1506863 -4.1457067 -4.1517577][-4.1721134 -4.2003098 -4.2309108 -4.238234 -4.2319064 -4.2237816 -4.2189822 -4.2140827 -4.2052364 -4.2051759 -4.21827 -4.2281985 -4.2216525 -4.2135043 -4.215106][-4.1485796 -4.1886997 -4.2375994 -4.266273 -4.2752771 -4.2765951 -4.2783389 -4.2795815 -4.2768598 -4.2820005 -4.2903738 -4.2932024 -4.2824235 -4.26968 -4.2647228][-4.1811156 -4.2164478 -4.2587309 -4.2884083 -4.3014741 -4.3065085 -4.309164 -4.3130293 -4.3137584 -4.3213205 -4.3295851 -4.3311 -4.3200903 -4.3041544 -4.2903948]]...]
INFO - root - 2017-12-07 23:03:49.124991: step 64110, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 49h:08m:30s remains)
INFO - root - 2017-12-07 23:03:55.912576: step 64120, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 48h:23m:13s remains)
INFO - root - 2017-12-07 23:04:02.673229: step 64130, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.613 sec/batch; 45h:41m:06s remains)
INFO - root - 2017-12-07 23:04:09.461734: step 64140, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 54h:25m:09s remains)
INFO - root - 2017-12-07 23:04:16.151981: step 64150, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 49h:15m:51s remains)
INFO - root - 2017-12-07 23:04:22.955637: step 64160, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 49h:02m:42s remains)
INFO - root - 2017-12-07 23:04:29.801371: step 64170, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 52h:22m:54s remains)
INFO - root - 2017-12-07 23:04:36.541613: step 64180, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 53h:51m:52s remains)
INFO - root - 2017-12-07 23:04:43.343794: step 64190, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 50h:42m:38s remains)
INFO - root - 2017-12-07 23:04:49.994373: step 64200, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 47h:35m:10s remains)
2017-12-07 23:04:50.745734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2289972 -4.2339325 -4.2301831 -4.2180948 -4.1979332 -4.1670089 -4.1422052 -4.1424594 -4.1464024 -4.1540813 -4.1707859 -4.1881719 -4.1884232 -4.1863837 -4.2008376][-4.2495017 -4.2496004 -4.2422605 -4.230423 -4.2083664 -4.1758351 -4.1455846 -4.140142 -4.1403208 -4.1463737 -4.1591024 -4.1694531 -4.1642857 -4.1594744 -4.1754627][-4.242825 -4.2436748 -4.239419 -4.2301264 -4.2119961 -4.187489 -4.1656179 -4.1626773 -4.1637678 -4.1655188 -4.1709476 -4.173315 -4.1640506 -4.1556506 -4.170424][-4.2278271 -4.2262959 -4.2208829 -4.2126136 -4.2005773 -4.1858063 -4.1734352 -4.1763368 -4.1863503 -4.19296 -4.1991787 -4.2031097 -4.1975193 -4.1897349 -4.19911][-4.2096663 -4.2044578 -4.196249 -4.1865983 -4.174232 -4.1565328 -4.1399174 -4.1406446 -4.1587152 -4.18074 -4.1964674 -4.2074752 -4.213717 -4.2133613 -4.217052][-4.2087865 -4.198154 -4.1838889 -4.1709204 -4.1512833 -4.1212196 -4.0873857 -4.0737414 -4.0940251 -4.1301575 -4.1564426 -4.1752768 -4.1946626 -4.2047687 -4.2089181][-4.2066517 -4.19086 -4.1714578 -4.1536 -4.124299 -4.0754876 -4.0131106 -3.9761658 -3.997865 -4.0537095 -4.0950141 -4.1218095 -4.1516318 -4.171155 -4.1777048][-4.2047548 -4.1827779 -4.1605883 -4.144227 -4.1155949 -4.05615 -3.9702847 -3.914284 -3.9418173 -4.0166717 -4.0666385 -4.0940371 -4.1250992 -4.14683 -4.1545024][-4.2032018 -4.181808 -4.1593328 -4.1522079 -4.1453447 -4.1109633 -4.0439477 -3.9931381 -4.01335 -4.0782037 -4.1158209 -4.1301575 -4.1467228 -4.1555161 -4.157805][-4.2022491 -4.1890087 -4.1703086 -4.1694059 -4.1827321 -4.1807289 -4.1473489 -4.1131864 -4.1231523 -4.1640587 -4.179379 -4.1744909 -4.1722593 -4.1674595 -4.16524][-4.2071543 -4.1973171 -4.180542 -4.1795049 -4.2012787 -4.2163024 -4.2044396 -4.18111 -4.1823697 -4.2019768 -4.2004089 -4.1818857 -4.1652269 -4.1531034 -4.154521][-4.2051368 -4.1915894 -4.1729546 -4.1700788 -4.1920519 -4.2081537 -4.2011638 -4.1802111 -4.1763363 -4.1852055 -4.1769562 -4.1519718 -4.1204634 -4.09829 -4.1059518][-4.2000766 -4.1830578 -4.1624 -4.1527371 -4.1640453 -4.1670508 -4.1531367 -4.1295338 -4.1184635 -4.1221828 -4.118021 -4.1007113 -4.0639014 -4.0339084 -4.0465794][-4.1868286 -4.1740322 -4.1567183 -4.145504 -4.1438327 -4.1333485 -4.1128139 -4.0881562 -4.0705519 -4.0736351 -4.0855479 -4.0850453 -4.0554996 -4.028779 -4.04569][-4.1693354 -4.1662226 -4.1598296 -4.1531162 -4.1475511 -4.135078 -4.1168451 -4.0936213 -4.0736289 -4.0772552 -4.0992179 -4.1106734 -4.0926542 -4.0749917 -4.0920362]]...]
INFO - root - 2017-12-07 23:04:57.482230: step 64210, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.751 sec/batch; 55h:59m:46s remains)
INFO - root - 2017-12-07 23:05:04.208952: step 64220, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 50h:43m:41s remains)
INFO - root - 2017-12-07 23:05:10.922065: step 64230, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 49h:15m:51s remains)
INFO - root - 2017-12-07 23:05:17.749691: step 64240, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 47h:29m:54s remains)
INFO - root - 2017-12-07 23:05:24.464831: step 64250, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 49h:39m:47s remains)
INFO - root - 2017-12-07 23:05:31.212582: step 64260, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 52h:20m:17s remains)
INFO - root - 2017-12-07 23:05:38.054607: step 64270, loss = 2.02, batch loss = 1.96 (11.3 examples/sec; 0.705 sec/batch; 52h:31m:41s remains)
INFO - root - 2017-12-07 23:05:44.899332: step 64280, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 50h:20m:18s remains)
INFO - root - 2017-12-07 23:05:51.661992: step 64290, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 47h:41m:49s remains)
INFO - root - 2017-12-07 23:05:58.412277: step 64300, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 52h:52m:07s remains)
2017-12-07 23:05:59.178006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1956096 -4.1906886 -4.1948366 -4.2024188 -4.2186847 -4.2346792 -4.24576 -4.2496257 -4.2452283 -4.2426405 -4.2431688 -4.2417636 -4.2366109 -4.2351465 -4.2412739][-4.1648006 -4.1573739 -4.1565642 -4.1624823 -4.1806 -4.1999454 -4.2139511 -4.2194552 -4.2162981 -4.2150273 -4.2160954 -4.2145524 -4.2111974 -4.2138329 -4.2223091][-4.1454511 -4.1363931 -4.1322336 -4.1394677 -4.1595187 -4.1790404 -4.1928654 -4.1972284 -4.1935043 -4.1952238 -4.1986532 -4.1939344 -4.1900439 -4.1965656 -4.206749][-4.1359029 -4.1291919 -4.123991 -4.1316857 -4.1504083 -4.1620831 -4.171041 -4.173234 -4.1692486 -4.1755567 -4.1827106 -4.1785569 -4.1735725 -4.1816649 -4.1924782][-4.1083145 -4.1093445 -4.1089454 -4.1193438 -4.1354666 -4.1365809 -4.1330829 -4.1272964 -4.1291647 -4.1445236 -4.1551638 -4.1532679 -4.1483564 -4.1550951 -4.1658678][-4.0647659 -4.0726008 -4.0781956 -4.0899653 -4.1028214 -4.0961652 -4.0735688 -4.0517225 -4.0638704 -4.0989842 -4.1197867 -4.1190395 -4.1136765 -4.12105 -4.1337061][-4.03929 -4.0428929 -4.04469 -4.0495558 -4.0528316 -4.0293465 -3.9742045 -3.9278941 -3.9610655 -4.0292969 -4.0671482 -4.0746765 -4.0784731 -4.0961065 -4.1145697][-4.0463419 -4.0398049 -4.030654 -4.0255709 -4.014863 -3.96613 -3.8675256 -3.790319 -3.8471684 -3.9480894 -4.0026336 -4.0157728 -4.0345597 -4.0728965 -4.1049619][-4.0762835 -4.0643239 -4.0520458 -4.0483966 -4.0353289 -3.9781845 -3.8703275 -3.7908807 -3.8448639 -3.936213 -3.9804063 -3.9891505 -4.015233 -4.0621719 -4.10097][-4.1109576 -4.102252 -4.0952415 -4.098681 -4.0898323 -4.0453491 -3.9717903 -3.9239733 -3.9548185 -4.0052862 -4.0246658 -4.025013 -4.0446472 -4.0805464 -4.1146913][-4.1292892 -4.1223159 -4.1203985 -4.1259947 -4.1217871 -4.0945549 -4.0548329 -4.0326643 -4.0483341 -4.0743761 -4.0807552 -4.0763388 -4.0862627 -4.1113424 -4.1383276][-4.1529651 -4.1485381 -4.1483655 -4.1535153 -4.1552944 -4.1420221 -4.1225538 -4.1122279 -4.1209641 -4.1331229 -4.13101 -4.1239676 -4.1297874 -4.1477385 -4.1668062][-4.1916223 -4.1890345 -4.189846 -4.193789 -4.1967859 -4.190948 -4.182775 -4.1803412 -4.1852522 -4.18745 -4.1791883 -4.1738029 -4.1828222 -4.1971021 -4.2071142][-4.2379904 -4.2371387 -4.23886 -4.2413969 -4.243021 -4.24066 -4.2380929 -4.2375784 -4.2387395 -4.23673 -4.2314258 -4.2321548 -4.2431378 -4.2537704 -4.2604313][-4.2865205 -4.2863393 -4.2872114 -4.2884579 -4.2892694 -4.2883711 -4.2872257 -4.2858596 -4.2837319 -4.2814784 -4.280458 -4.2836103 -4.292316 -4.3001304 -4.3049231]]...]
INFO - root - 2017-12-07 23:06:05.947310: step 64310, loss = 2.04, batch loss = 1.98 (13.1 examples/sec; 0.612 sec/batch; 45h:36m:58s remains)
INFO - root - 2017-12-07 23:06:12.855342: step 64320, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 50h:08m:35s remains)
INFO - root - 2017-12-07 23:06:19.695920: step 64330, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 54h:12m:44s remains)
INFO - root - 2017-12-07 23:06:26.622742: step 64340, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 54h:17m:21s remains)
INFO - root - 2017-12-07 23:06:33.400808: step 64350, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 51h:18m:01s remains)
INFO - root - 2017-12-07 23:06:40.104140: step 64360, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 47h:01m:21s remains)
INFO - root - 2017-12-07 23:06:46.924646: step 64370, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 48h:36m:58s remains)
INFO - root - 2017-12-07 23:06:53.775624: step 64380, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 54h:31m:06s remains)
INFO - root - 2017-12-07 23:07:00.589014: step 64390, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 47h:28m:08s remains)
INFO - root - 2017-12-07 23:07:07.165823: step 64400, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 49h:49m:48s remains)
2017-12-07 23:07:07.912098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2910657 -4.2906518 -4.2915893 -4.2900786 -4.287044 -4.2841787 -4.2827625 -4.2829909 -4.2845144 -4.2856288 -4.2837648 -4.2789969 -4.272614 -4.2626915 -4.2567258][-4.2764611 -4.2788744 -4.2860036 -4.2894192 -4.2898107 -4.2897906 -4.2903743 -4.2923512 -4.2960854 -4.2985578 -4.2962861 -4.290813 -4.28315 -4.2692695 -4.2595348][-4.2622643 -4.2655787 -4.2756991 -4.2811942 -4.2811923 -4.2791071 -4.2771339 -4.2789993 -4.283978 -4.287694 -4.2872858 -4.2849121 -4.28081 -4.2684116 -4.2588859][-4.2494478 -4.2485027 -4.25511 -4.258007 -4.2497458 -4.2380147 -4.2291245 -4.2288742 -4.2362032 -4.2433982 -4.2467475 -4.248126 -4.2475743 -4.2388935 -4.2338095][-4.245029 -4.2360716 -4.23244 -4.2250051 -4.2059193 -4.1818342 -4.1653161 -4.1644373 -4.1749587 -4.1865611 -4.194016 -4.1999321 -4.2029195 -4.1990294 -4.2006669][-4.2140837 -4.1911879 -4.1745281 -4.1513114 -4.1189532 -4.085176 -4.0627308 -4.0637369 -4.0814657 -4.1001124 -4.1173506 -4.1347847 -4.1474543 -4.1520696 -4.1645083][-4.1357865 -4.0973253 -4.0636659 -4.0231833 -3.9759355 -3.9276724 -3.8940897 -3.9007275 -3.9325728 -3.9658139 -4.0007739 -4.036541 -4.0622787 -4.0771413 -4.104526][-4.040277 -3.991801 -3.9483469 -3.8940854 -3.8295617 -3.7665639 -3.7199168 -3.7316375 -3.7794914 -3.8301294 -3.8848603 -3.9390929 -3.9764423 -3.9965239 -4.0322857][-3.9917037 -3.9562206 -3.9304674 -3.8924689 -3.8427405 -3.7975678 -3.76099 -3.7690349 -3.8136687 -3.861697 -3.9120715 -3.9593654 -3.9878898 -3.9964261 -4.0227489][-3.9930065 -3.9847155 -3.9909577 -3.9866791 -3.9676707 -3.9510586 -3.9365878 -3.9441009 -3.9762321 -4.0088143 -4.0366664 -4.0586252 -4.0652885 -4.0554295 -4.0642781][-4.0181 -4.0313058 -4.0579538 -4.0722475 -4.0699644 -4.0685987 -4.0670452 -4.0745754 -4.0944953 -4.11264 -4.1242003 -4.1314936 -4.1278872 -4.1101632 -4.1079774][-4.0835242 -4.1046395 -4.1361666 -4.1549459 -4.1566415 -4.1562572 -4.156806 -4.1625156 -4.1743774 -4.1851521 -4.1899309 -4.192975 -4.1876659 -4.17139 -4.1670332][-4.1946454 -4.2150517 -4.2396812 -4.2541804 -4.25542 -4.2546043 -4.2547345 -4.2575474 -4.2627587 -4.2676482 -4.2699313 -4.2724738 -4.2709236 -4.2614117 -4.2586951][-4.2913609 -4.3068 -4.3214483 -4.3286486 -4.3269529 -4.3238997 -4.3228173 -4.323822 -4.3252139 -4.3263106 -4.3271508 -4.3302 -4.3326931 -4.3299708 -4.3298736][-4.3381543 -4.3484278 -4.355669 -4.3581519 -4.356235 -4.3538809 -4.3526082 -4.35254 -4.3526087 -4.35251 -4.3530273 -4.355279 -4.3577437 -4.3581066 -4.3583679]]...]
INFO - root - 2017-12-07 23:07:14.762521: step 64410, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 52h:36m:36s remains)
INFO - root - 2017-12-07 23:07:21.481649: step 64420, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 50h:44m:07s remains)
INFO - root - 2017-12-07 23:07:28.225851: step 64430, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 50h:26m:20s remains)
INFO - root - 2017-12-07 23:07:35.005369: step 64440, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 49h:05m:14s remains)
INFO - root - 2017-12-07 23:07:41.642049: step 64450, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 53h:51m:25s remains)
INFO - root - 2017-12-07 23:07:48.451113: step 64460, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 54h:14m:03s remains)
INFO - root - 2017-12-07 23:07:55.213465: step 64470, loss = 2.10, batch loss = 2.05 (12.1 examples/sec; 0.662 sec/batch; 49h:18m:12s remains)
INFO - root - 2017-12-07 23:08:02.006643: step 64480, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 47h:29m:53s remains)
INFO - root - 2017-12-07 23:08:08.864161: step 64490, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 51h:06m:18s remains)
INFO - root - 2017-12-07 23:08:15.509589: step 64500, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 53h:15m:21s remains)
2017-12-07 23:08:16.291876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2564831 -4.2469139 -4.2368665 -4.2222071 -4.2034621 -4.1904607 -4.1979842 -4.2219887 -4.2406421 -4.2462463 -4.2409782 -4.2183018 -4.1930966 -4.1775742 -4.1726322][-4.2634687 -4.2514997 -4.240334 -4.224442 -4.2053351 -4.1937613 -4.1989832 -4.2165985 -4.2265992 -4.2269192 -4.2189159 -4.1987772 -4.1769714 -4.1655407 -4.1674261][-4.2702346 -4.254364 -4.2399836 -4.2214012 -4.2013059 -4.1909204 -4.19367 -4.2045383 -4.2105155 -4.2104115 -4.2039757 -4.1847463 -4.1665368 -4.1585894 -4.1608934][-4.2724509 -4.2531996 -4.235611 -4.212986 -4.1904411 -4.1791959 -4.1789446 -4.1855564 -4.193285 -4.196486 -4.1963224 -4.1803331 -4.1616616 -4.1504965 -4.1465139][-4.2648644 -4.2425909 -4.2216897 -4.1961179 -4.1703854 -4.154798 -4.1481462 -4.1491184 -4.15997 -4.1704521 -4.1793756 -4.1702414 -4.1504135 -4.133708 -4.1204357][-4.2467861 -4.2244763 -4.2024312 -4.1737175 -4.1435013 -4.11882 -4.0974827 -4.0876369 -4.0979443 -4.1159444 -4.1362195 -4.1378489 -4.122716 -4.1084538 -4.0949645][-4.2248545 -4.2034082 -4.1789837 -4.1462073 -4.1107879 -4.0723562 -4.0313053 -4.00543 -4.010911 -4.0374136 -4.0753927 -4.0984063 -4.1010532 -4.0999403 -4.0954828][-4.2037282 -4.181005 -4.1544313 -4.1204686 -4.0812659 -4.0299954 -3.9696612 -3.9250126 -3.9272654 -3.9687812 -4.0321779 -4.08541 -4.1122041 -4.1259465 -4.1310654][-4.1887674 -4.1635218 -4.1390586 -4.110548 -4.0746722 -4.023128 -3.9614344 -3.9129694 -3.9148798 -3.96279 -4.0376596 -4.1092939 -4.1554527 -4.1812954 -4.192842][-4.186367 -4.1599026 -4.1386061 -4.1171288 -4.0925169 -4.0591 -4.0205154 -3.9923396 -3.9984803 -4.0358763 -4.0949183 -4.1581388 -4.2040014 -4.2318373 -4.2453427][-4.2027349 -4.1790257 -4.1616945 -4.1471229 -4.1343589 -4.1216626 -4.1089106 -4.1019783 -4.1130629 -4.1375942 -4.172184 -4.2123442 -4.2451415 -4.266181 -4.2772088][-4.233582 -4.21775 -4.2071266 -4.199832 -4.195734 -4.194891 -4.1952806 -4.1982722 -4.2079549 -4.2200947 -4.2334256 -4.250073 -4.2649302 -4.274538 -4.280292][-4.2603993 -4.2537737 -4.249228 -4.2463546 -4.2471213 -4.2507339 -4.2547536 -4.2585812 -4.2631035 -4.266324 -4.2675323 -4.2698932 -4.2732363 -4.2755852 -4.2779155][-4.2812362 -4.2811656 -4.2795839 -4.278522 -4.279871 -4.2831182 -4.2859306 -4.2874494 -4.2885752 -4.2884245 -4.2852588 -4.2821131 -4.2818627 -4.283896 -4.287322][-4.2952003 -4.2980728 -4.2982206 -4.2975745 -4.2977862 -4.2992129 -4.3005915 -4.3015804 -4.3022552 -4.3019691 -4.2994881 -4.2972584 -4.2978258 -4.3010421 -4.3055296]]...]
INFO - root - 2017-12-07 23:08:23.097658: step 64510, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 48h:15m:18s remains)
INFO - root - 2017-12-07 23:08:30.041586: step 64520, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 51h:21m:14s remains)
INFO - root - 2017-12-07 23:08:36.856923: step 64530, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.775 sec/batch; 57h:39m:36s remains)
INFO - root - 2017-12-07 23:08:43.724993: step 64540, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 49h:52m:22s remains)
INFO - root - 2017-12-07 23:08:50.476479: step 64550, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.636 sec/batch; 47h:19m:46s remains)
INFO - root - 2017-12-07 23:08:57.268958: step 64560, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 52h:49m:15s remains)
INFO - root - 2017-12-07 23:09:04.016821: step 64570, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 53h:54m:34s remains)
INFO - root - 2017-12-07 23:09:10.777745: step 64580, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 49h:34m:55s remains)
INFO - root - 2017-12-07 23:09:17.545038: step 64590, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 49h:28m:33s remains)
INFO - root - 2017-12-07 23:09:24.229876: step 64600, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 48h:37m:12s remains)
2017-12-07 23:09:25.002864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2530694 -4.2051973 -4.1694145 -4.1511464 -4.1378512 -4.1124516 -4.0650954 -4.0662031 -4.1206803 -4.1619077 -4.1777215 -4.1855659 -4.188725 -4.2009277 -4.2126][-4.2481046 -4.2110882 -4.1897135 -4.1799374 -4.1719275 -4.1424 -4.0856123 -4.0837936 -4.133779 -4.1656008 -4.1734028 -4.181016 -4.186717 -4.1986833 -4.209476][-4.250031 -4.2223225 -4.2045441 -4.1962042 -4.1919804 -4.1596222 -4.09955 -4.1019845 -4.1464787 -4.1705613 -4.172142 -4.1785789 -4.187005 -4.1967 -4.1996551][-4.2518363 -4.2226605 -4.1983986 -4.1808629 -4.17192 -4.1355267 -4.0754113 -4.0793476 -4.1212935 -4.1463766 -4.1522918 -4.1677666 -4.1794386 -4.1826429 -4.1723127][-4.2534914 -4.2159829 -4.1806445 -4.1502676 -4.1223974 -4.0688152 -4.00102 -4.0157242 -4.0697331 -4.1062365 -4.1255198 -4.1514049 -4.1644597 -4.1546888 -4.1268091][-4.2539825 -4.2057123 -4.1554642 -4.1115327 -4.0612731 -3.9696455 -3.8677442 -3.8993943 -3.9858987 -4.0447888 -4.0859733 -4.1228733 -4.1375475 -4.1249738 -4.0828624][-4.2508316 -4.193357 -4.1288233 -4.0662103 -3.9868515 -3.8392365 -3.6659365 -3.7200394 -3.8792453 -3.9871497 -4.054143 -4.1022172 -4.1231031 -4.1156754 -4.068851][-4.2415023 -4.1787839 -4.1097937 -4.0355458 -3.9366612 -3.7575462 -3.5377085 -3.6310329 -3.8588428 -3.9932516 -4.0670681 -4.1159606 -4.1335812 -4.1201081 -4.0685673][-4.2412152 -4.1785879 -4.1135631 -4.0485158 -3.9718332 -3.8545928 -3.7234948 -3.804049 -3.9762998 -4.071322 -4.1182842 -4.1463003 -4.1482897 -4.1242943 -4.0816436][-4.2492046 -4.1945882 -4.1414175 -4.0975213 -4.0569873 -4.0021076 -3.9441068 -3.9976912 -4.0979137 -4.14646 -4.1624765 -4.1613364 -4.1503773 -4.1323295 -4.1112995][-4.2643905 -4.2217116 -4.1802707 -4.1471848 -4.1295714 -4.1057243 -4.0719314 -4.099081 -4.1561069 -4.1815929 -4.1772113 -4.1619749 -4.1488509 -4.1387119 -4.136179][-4.2720194 -4.2312031 -4.1933546 -4.1639638 -4.155036 -4.1373839 -4.0977116 -4.1070309 -4.1446648 -4.1601548 -4.1492791 -4.1394773 -4.1431637 -4.1521988 -4.163897][-4.2701697 -4.2247777 -4.1817384 -4.150527 -4.1422763 -4.1207223 -4.069077 -4.0734668 -4.1175861 -4.1394248 -4.13437 -4.1363335 -4.157917 -4.1808681 -4.1946192][-4.2677021 -4.2228465 -4.1741424 -4.1419859 -4.1347604 -4.110568 -4.0529633 -4.0539165 -4.1095157 -4.1445489 -4.1544948 -4.1674957 -4.1893935 -4.2088547 -4.21627][-4.2707419 -4.2306581 -4.187993 -4.1597967 -4.1553779 -4.1350856 -4.0845919 -4.0834169 -4.1344 -4.1735044 -4.1949277 -4.2107258 -4.2245398 -4.2384653 -4.2430372]]...]
INFO - root - 2017-12-07 23:09:31.775439: step 64610, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 50h:48m:10s remains)
INFO - root - 2017-12-07 23:09:38.519928: step 64620, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 46h:28m:03s remains)
INFO - root - 2017-12-07 23:09:45.299581: step 64630, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 48h:12m:20s remains)
INFO - root - 2017-12-07 23:09:52.257711: step 64640, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 55h:06m:52s remains)
INFO - root - 2017-12-07 23:09:59.083130: step 64650, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 53h:49m:16s remains)
INFO - root - 2017-12-07 23:10:05.800914: step 64660, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 50h:50m:14s remains)
INFO - root - 2017-12-07 23:10:12.569369: step 64670, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 46h:34m:15s remains)
INFO - root - 2017-12-07 23:10:19.209658: step 64680, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 48h:20m:22s remains)
INFO - root - 2017-12-07 23:10:26.033917: step 64690, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 53h:11m:37s remains)
INFO - root - 2017-12-07 23:10:32.740604: step 64700, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.714 sec/batch; 53h:08m:29s remains)
2017-12-07 23:10:33.483622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3452878 -4.3413115 -4.3363466 -4.3385906 -4.3435459 -4.3474813 -4.3480549 -4.3453708 -4.3378291 -4.325983 -4.3165746 -4.3137817 -4.3175797 -4.3216858 -4.3253984][-4.3252916 -4.3185744 -4.317431 -4.3273878 -4.334959 -4.3363767 -4.3313031 -4.3185458 -4.3003531 -4.283638 -4.2765956 -4.2813683 -4.2921743 -4.2976313 -4.2985511][-4.2878571 -4.2821255 -4.2867675 -4.3033981 -4.3134432 -4.31083 -4.2968588 -4.2678995 -4.2355132 -4.2154331 -4.2141047 -4.2295308 -4.2485809 -4.2574449 -4.2580037][-4.2394691 -4.242044 -4.2537041 -4.2725363 -4.2797637 -4.2702231 -4.2421708 -4.1960106 -4.1554232 -4.1429405 -4.1572418 -4.1840892 -4.2076764 -4.2197304 -4.2230244][-4.1883903 -4.1994586 -4.2163291 -4.2330475 -4.2327747 -4.2069173 -4.1563487 -4.0928111 -4.0525031 -4.0632076 -4.1076965 -4.1539016 -4.1850066 -4.2018375 -4.2071157][-4.1423354 -4.1571817 -4.1767359 -4.1887879 -4.1748466 -4.126533 -4.0518937 -3.9744427 -3.948972 -3.9927955 -4.0691752 -4.1386929 -4.1807141 -4.20258 -4.2079754][-4.1404023 -4.1504273 -4.1612844 -4.1571856 -4.124774 -4.0579453 -3.9683959 -3.8948605 -3.9021287 -3.979897 -4.0733333 -4.1486812 -4.1900821 -4.2091351 -4.2140894][-4.1762295 -4.1743612 -4.1665154 -4.1436725 -4.0959044 -4.0252266 -3.9450567 -3.9030805 -3.9460788 -4.0344238 -4.1197615 -4.1844482 -4.2196116 -4.231811 -4.2332859][-4.2114172 -4.2025824 -4.1851172 -4.1557751 -4.1116605 -4.0544076 -4.0013022 -3.9951737 -4.0515471 -4.1252313 -4.1866632 -4.2332363 -4.2547503 -4.2549272 -4.24973][-4.2310171 -4.2265334 -4.2151971 -4.1924038 -4.1586275 -4.1179924 -4.0893788 -4.1004004 -4.1482439 -4.2007442 -4.2386971 -4.2641764 -4.2707009 -4.2624431 -4.2542276][-4.2431054 -4.2491207 -4.2501707 -4.2391806 -4.2179041 -4.1890745 -4.1718383 -4.1876335 -4.2219281 -4.2525721 -4.2689304 -4.2742085 -4.2651124 -4.2511363 -4.2437654][-4.25145 -4.2655473 -4.2758551 -4.2772613 -4.2666178 -4.2507629 -4.2433572 -4.2564526 -4.2745848 -4.2846408 -4.2831254 -4.2745304 -4.2563739 -4.241118 -4.23785][-4.2564807 -4.2731771 -4.2869644 -4.2931838 -4.2913365 -4.2886376 -4.2898016 -4.2997527 -4.3059659 -4.3012881 -4.2904963 -4.2772651 -4.2597523 -4.2483172 -4.2505937][-4.2652111 -4.27669 -4.2860451 -4.2903795 -4.2920213 -4.2975273 -4.3048477 -4.3130417 -4.3140244 -4.3044381 -4.2917185 -4.2801981 -4.2707329 -4.2679482 -4.2756343][-4.2680025 -4.2776 -4.2843461 -4.2865143 -4.286603 -4.2916193 -4.2994132 -4.3068571 -4.3058753 -4.2990851 -4.2929721 -4.2874808 -4.2850161 -4.28872 -4.2987504]]...]
INFO - root - 2017-12-07 23:10:40.351295: step 64710, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 50h:44m:28s remains)
INFO - root - 2017-12-07 23:10:47.212519: step 64720, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.730 sec/batch; 54h:18m:44s remains)
INFO - root - 2017-12-07 23:10:53.997998: step 64730, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 50h:51m:21s remains)
INFO - root - 2017-12-07 23:11:00.943037: step 64740, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 47h:09m:35s remains)
INFO - root - 2017-12-07 23:11:07.734802: step 64750, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 48h:48m:01s remains)
INFO - root - 2017-12-07 23:11:14.430056: step 64760, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 51h:32m:30s remains)
INFO - root - 2017-12-07 23:11:21.204545: step 64770, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 50h:37m:02s remains)
INFO - root - 2017-12-07 23:11:28.028835: step 64780, loss = 2.07, batch loss = 2.01 (13.3 examples/sec; 0.603 sec/batch; 44h:52m:10s remains)
INFO - root - 2017-12-07 23:11:34.896301: step 64790, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 49h:47m:48s remains)
INFO - root - 2017-12-07 23:11:41.571124: step 64800, loss = 2.05, batch loss = 2.00 (10.5 examples/sec; 0.760 sec/batch; 56h:28m:54s remains)
2017-12-07 23:11:42.339658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3125205 -4.3027396 -4.3062372 -4.3181882 -4.3247795 -4.326488 -4.3267994 -4.32284 -4.30919 -4.2811613 -4.2402277 -4.1895533 -4.1531687 -4.1609683 -4.19752][-4.29102 -4.2749557 -4.2724552 -4.2833071 -4.2900443 -4.2933373 -4.2979527 -4.3012619 -4.2976689 -4.273427 -4.2332363 -4.1826611 -4.147788 -4.1556778 -4.1912985][-4.2677674 -4.2502 -4.24074 -4.245204 -4.2465029 -4.2466536 -4.2547584 -4.269093 -4.2756243 -4.2585926 -4.21791 -4.1644354 -4.1307278 -4.1438632 -4.183516][-4.2422867 -4.2273984 -4.21663 -4.2154059 -4.2109118 -4.2057681 -4.210113 -4.2290564 -4.2451143 -4.234973 -4.1929784 -4.139905 -4.109745 -4.1298156 -4.1751146][-4.2101626 -4.1974535 -4.1865525 -4.1799545 -4.1696486 -4.1555448 -4.1460047 -4.16205 -4.1896772 -4.1963325 -4.16592 -4.1214347 -4.1014662 -4.128603 -4.176301][-4.1834955 -4.1749725 -4.1644726 -4.151948 -4.1354465 -4.1113915 -4.0840898 -4.0898571 -4.1225929 -4.1477966 -4.1335845 -4.1027474 -4.0951047 -4.1323986 -4.1837425][-4.1753554 -4.1738667 -4.1635256 -4.1486659 -4.1264191 -4.0935693 -4.0564 -4.0500364 -4.0748773 -4.10531 -4.1019607 -4.0785604 -4.07732 -4.1233048 -4.1811881][-4.191081 -4.202733 -4.1983833 -4.1857519 -4.1607409 -4.1245041 -4.0849085 -4.068676 -4.0807943 -4.1060271 -4.1052523 -4.0828819 -4.0795336 -4.1263862 -4.1846185][-4.2247605 -4.2458839 -4.2524629 -4.2468319 -4.2264357 -4.1971 -4.1629839 -4.1397982 -4.1392035 -4.1543961 -4.1518965 -4.1302457 -4.123383 -4.1614542 -4.2094059][-4.2606645 -4.2867723 -4.2994337 -4.294313 -4.2786984 -4.2603126 -4.23789 -4.2165823 -4.2095656 -4.2178473 -4.2109113 -4.1922374 -4.1855445 -4.2123013 -4.24715][-4.2915659 -4.3165245 -4.3303776 -4.3245406 -4.3116674 -4.3008814 -4.2900586 -4.27462 -4.2675977 -4.2712131 -4.2623439 -4.2467356 -4.2419405 -4.2601404 -4.2854838][-4.3144994 -4.3340287 -4.3466558 -4.3433805 -4.334311 -4.3260508 -4.3204088 -4.3096294 -4.3053851 -4.3061738 -4.3002505 -4.2902489 -4.2874503 -4.2989006 -4.3143115][-4.3210425 -4.3367043 -4.3472381 -4.3469892 -4.3427787 -4.3357458 -4.3311071 -4.3255863 -4.3256869 -4.3266454 -4.324194 -4.318007 -4.3144641 -4.3192215 -4.3270216][-4.3143296 -4.3271041 -4.3350768 -4.3368106 -4.3376961 -4.3331308 -4.3298512 -4.3277407 -4.329905 -4.332655 -4.3334804 -4.3310933 -4.3275251 -4.3262143 -4.3271236][-4.3009052 -4.3099408 -4.3137894 -4.31521 -4.3174663 -4.3150206 -4.3120885 -4.3114805 -4.3150425 -4.3187914 -4.32244 -4.3234606 -4.3216233 -4.3180256 -4.3165574]]...]
INFO - root - 2017-12-07 23:11:49.147966: step 64810, loss = 2.12, batch loss = 2.06 (12.7 examples/sec; 0.632 sec/batch; 47h:00m:11s remains)
INFO - root - 2017-12-07 23:11:55.984322: step 64820, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 48h:44m:24s remains)
INFO - root - 2017-12-07 23:12:02.700222: step 64830, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 53h:37m:06s remains)
INFO - root - 2017-12-07 23:12:09.537577: step 64840, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 53h:56m:15s remains)
INFO - root - 2017-12-07 23:12:16.369785: step 64850, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 49h:10m:09s remains)
INFO - root - 2017-12-07 23:12:23.083430: step 64860, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 47h:25m:33s remains)
INFO - root - 2017-12-07 23:12:29.853103: step 64870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 50h:23m:26s remains)
INFO - root - 2017-12-07 23:12:36.733246: step 64880, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.761 sec/batch; 56h:34m:57s remains)
INFO - root - 2017-12-07 23:12:43.408644: step 64890, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 51h:47m:47s remains)
INFO - root - 2017-12-07 23:12:50.068843: step 64900, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 49h:53m:30s remains)
2017-12-07 23:12:50.850571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2451696 -4.2413626 -4.2520127 -4.2592907 -4.2708025 -4.286396 -4.3013897 -4.3176894 -4.3289781 -4.3155913 -4.2705622 -4.2231727 -4.2049603 -4.2142439 -4.2334871][-4.2468452 -4.2437124 -4.2511282 -4.2525716 -4.2606492 -4.2774239 -4.2964859 -4.3181343 -4.3321762 -4.3182731 -4.2736173 -4.2251959 -4.2013245 -4.2023754 -4.2137065][-4.2517238 -4.2486825 -4.2489424 -4.246294 -4.2501779 -4.2659397 -4.2829576 -4.3030839 -4.3190002 -4.3097291 -4.2751184 -4.2352996 -4.2124763 -4.2091393 -4.2134414][-4.2495532 -4.2440014 -4.2358685 -4.2250085 -4.2248363 -4.2375054 -4.2505555 -4.2647657 -4.2820454 -4.2840366 -4.2666135 -4.2447391 -4.2348208 -4.2333074 -4.234489][-4.2338543 -4.2202525 -4.1996741 -4.1774249 -4.1775675 -4.1925039 -4.2064676 -4.2203751 -4.240766 -4.2531695 -4.2521977 -4.250165 -4.2560768 -4.2614603 -4.2626491][-4.1975088 -4.1690645 -4.1322603 -4.0959358 -4.0958719 -4.1228666 -4.1475863 -4.1705341 -4.1977406 -4.2194023 -4.2328863 -4.2437439 -4.25833 -4.2723713 -4.2819848][-4.1490345 -4.1022272 -4.0426383 -3.9769704 -3.9687278 -4.0145688 -4.0650854 -4.1030822 -4.1425152 -4.1806946 -4.2107859 -4.2292004 -4.2469883 -4.2695789 -4.2916474][-4.1096969 -4.0498109 -3.9667439 -3.8653636 -3.8389232 -3.9019582 -3.9779551 -4.0330839 -4.0816236 -4.1335087 -4.1807914 -4.2080216 -4.2339482 -4.2681513 -4.298727][-4.1094151 -4.0477877 -3.964221 -3.8585413 -3.8153367 -3.8723185 -3.9530594 -4.0116539 -4.0625768 -4.1152468 -4.1716084 -4.2079134 -4.2423191 -4.2813931 -4.3093405][-4.1484733 -4.0975819 -4.0381627 -3.966629 -3.9253144 -3.9536355 -4.0098433 -4.0573387 -4.1010609 -4.1470656 -4.1994758 -4.2349777 -4.2674594 -4.2994385 -4.3154492][-4.1965523 -4.1612182 -4.1324916 -4.1009135 -4.07639 -4.0845079 -4.1161523 -4.150939 -4.183825 -4.2162008 -4.2504778 -4.272048 -4.290246 -4.3040652 -4.3023453][-4.2462816 -4.2276249 -4.2202411 -4.2108812 -4.1987281 -4.2003245 -4.2182283 -4.2400622 -4.2620564 -4.2804632 -4.2942295 -4.2974682 -4.2966933 -4.2892394 -4.2650924][-4.287415 -4.28018 -4.2819867 -4.2794704 -4.2706323 -4.2681189 -4.2772083 -4.2899714 -4.30265 -4.3092518 -4.3090563 -4.2990527 -4.281909 -4.2559042 -4.2134757][-4.3139882 -4.3109045 -4.3120389 -4.3093567 -4.3004646 -4.2927227 -4.2944551 -4.3014073 -4.3069835 -4.3033214 -4.2962022 -4.2802544 -4.2522206 -4.2166219 -4.168231][-4.3229117 -4.321702 -4.3200064 -4.3157997 -4.3080864 -4.2987881 -4.2962093 -4.2993546 -4.2972412 -4.2837219 -4.2688117 -4.2516713 -4.221642 -4.1895719 -4.147161]]...]
INFO - root - 2017-12-07 23:12:57.633197: step 64910, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 52h:41m:57s remains)
INFO - root - 2017-12-07 23:13:04.246369: step 64920, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 47h:55m:30s remains)
INFO - root - 2017-12-07 23:13:11.009139: step 64930, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 49h:06m:11s remains)
INFO - root - 2017-12-07 23:13:17.750664: step 64940, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 49h:42m:15s remains)
INFO - root - 2017-12-07 23:13:24.400197: step 64950, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.608 sec/batch; 45h:13m:07s remains)
INFO - root - 2017-12-07 23:13:31.129866: step 64960, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 47h:46m:37s remains)
INFO - root - 2017-12-07 23:13:37.897234: step 64970, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 53h:46m:51s remains)
INFO - root - 2017-12-07 23:13:44.641077: step 64980, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.688 sec/batch; 51h:05m:48s remains)
INFO - root - 2017-12-07 23:13:51.456701: step 64990, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 49h:19m:23s remains)
INFO - root - 2017-12-07 23:13:58.078834: step 65000, loss = 2.07, batch loss = 2.02 (13.1 examples/sec; 0.612 sec/batch; 45h:27m:11s remains)
2017-12-07 23:13:58.912759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275331 -4.265223 -4.2540107 -4.2369294 -4.2167177 -4.2052937 -4.207315 -4.2083182 -4.2019281 -4.1964073 -4.1895723 -4.1776423 -4.1692562 -4.1639709 -4.1652622][-4.265666 -4.2535458 -4.2413898 -4.2251787 -4.2073565 -4.1983814 -4.2053623 -4.2097645 -4.1995535 -4.1889777 -4.1807346 -4.1704245 -4.1658649 -4.1678224 -4.1772528][-4.2573972 -4.2433877 -4.2307196 -4.21677 -4.2002988 -4.1892362 -4.1956892 -4.20375 -4.1947966 -4.184341 -4.1782341 -4.1724038 -4.1731148 -4.1790128 -4.1907845][-4.2492576 -4.2335768 -4.2196665 -4.2053757 -4.1850433 -4.1638288 -4.159976 -4.1648059 -4.1584988 -4.1564736 -4.1597915 -4.1600852 -4.1649151 -4.1708217 -4.1817012][-4.2379174 -4.2186065 -4.1976242 -4.1740317 -4.1397934 -4.100029 -4.0764437 -4.0715141 -4.0732903 -4.0892434 -4.1099534 -4.1197867 -4.1259046 -4.1277294 -4.1333628][-4.2157693 -4.1890316 -4.1566963 -4.1188035 -4.0668774 -4.0055757 -3.9553061 -3.9345927 -3.9477675 -3.9885345 -4.0307369 -4.0537338 -4.0613432 -4.0569177 -4.0555768][-4.1843877 -4.1459527 -4.0992432 -4.0457115 -3.979084 -3.9013839 -3.8304706 -3.7966998 -3.823513 -3.8899727 -3.951932 -3.9874246 -3.9987836 -3.9918551 -3.9870596][-4.1742029 -4.1300592 -4.0784965 -4.022676 -3.9583607 -3.8884711 -3.8235841 -3.7919025 -3.8194919 -3.8828773 -3.9391847 -3.9714804 -3.9801013 -3.9723361 -3.9660866][-4.1917691 -4.15281 -4.1091132 -4.0655093 -4.017385 -3.9698098 -3.9279318 -3.9065239 -3.9228563 -3.962729 -3.9969895 -4.0160222 -4.0171747 -4.00747 -4.0002942][-4.228322 -4.2013788 -4.1695261 -4.1406207 -4.112318 -4.087945 -4.0672536 -4.0544205 -4.05905 -4.0756335 -4.0908537 -4.0986371 -4.0963697 -4.0895085 -4.0857816][-4.2711277 -4.2585254 -4.2393932 -4.221715 -4.2080207 -4.2005477 -4.1943722 -4.1860957 -4.1810613 -4.1818953 -4.1856508 -4.1893015 -4.1898513 -4.1892033 -4.1907253][-4.3012104 -4.2978749 -4.2877922 -4.2773843 -4.2712584 -4.2722397 -4.2753196 -4.2734227 -4.2669792 -4.261857 -4.2619977 -4.2657633 -4.2687273 -4.2708626 -4.2729955][-4.309629 -4.3114009 -4.3062549 -4.2990241 -4.2950764 -4.2991338 -4.3077931 -4.3126168 -4.3099108 -4.304594 -4.3022447 -4.3038087 -4.3061857 -4.3073611 -4.307919][-4.2988029 -4.3006825 -4.2966113 -4.2896724 -4.2855854 -4.2906847 -4.3022847 -4.3109455 -4.3107939 -4.3055353 -4.3016787 -4.3022757 -4.3047671 -4.3065267 -4.3056369][-4.2832432 -4.2838755 -4.2787747 -4.270833 -4.265305 -4.26941 -4.281168 -4.2903075 -4.2907972 -4.2866316 -4.2839804 -4.2860756 -4.2900829 -4.2932725 -4.2927966]]...]
INFO - root - 2017-12-07 23:14:05.737053: step 65010, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 53h:47m:28s remains)
INFO - root - 2017-12-07 23:14:12.570918: step 65020, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 47h:07m:41s remains)
INFO - root - 2017-12-07 23:14:19.335408: step 65030, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 47h:49m:35s remains)
INFO - root - 2017-12-07 23:14:26.150825: step 65040, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 52h:47m:20s remains)
INFO - root - 2017-12-07 23:14:32.999863: step 65050, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 52h:49m:54s remains)
INFO - root - 2017-12-07 23:14:39.703761: step 65060, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 49h:35m:29s remains)
INFO - root - 2017-12-07 23:14:46.277031: step 65070, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 47h:36m:22s remains)
INFO - root - 2017-12-07 23:14:53.117531: step 65080, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 53h:31m:31s remains)
INFO - root - 2017-12-07 23:14:59.995275: step 65090, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 52h:35m:33s remains)
INFO - root - 2017-12-07 23:15:06.585861: step 65100, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 50h:16m:10s remains)
2017-12-07 23:15:07.384863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3385987 -4.3403058 -4.340271 -4.3406639 -4.3354812 -4.3238788 -4.3021584 -4.2835026 -4.28542 -4.2836227 -4.2825184 -4.2895856 -4.2878885 -4.2788167 -4.2784309][-4.3377075 -4.3393307 -4.3356123 -4.3275871 -4.3110967 -4.2906861 -4.256587 -4.2244892 -4.2288966 -4.2292409 -4.2307763 -4.2445884 -4.2505865 -4.2437086 -4.236989][-4.3372498 -4.3365841 -4.3261924 -4.3087654 -4.2809553 -4.2509575 -4.2066803 -4.1546922 -4.1555548 -4.1587963 -4.1615491 -4.1841841 -4.2004642 -4.1960359 -4.1826611][-4.3338976 -4.3290844 -4.3127832 -4.2859654 -4.2469273 -4.2087116 -4.1532054 -4.0822167 -4.0744276 -4.0849676 -4.0890613 -4.1175804 -4.146832 -4.1498256 -4.1370625][-4.3270426 -4.32265 -4.3040347 -4.27013 -4.2224159 -4.1783919 -4.1112847 -4.0227704 -4.0044684 -4.0244074 -4.0277448 -4.0585608 -4.105515 -4.1225004 -4.1108189][-4.31918 -4.3193812 -4.3025923 -4.267024 -4.2139091 -4.1638508 -4.0872521 -3.9866545 -3.9650278 -3.9914505 -3.992625 -4.0262089 -4.0883374 -4.116221 -4.0999889][-4.3168173 -4.3225608 -4.3099947 -4.2736115 -4.2152839 -4.1516709 -4.0608521 -3.9490759 -3.9238663 -3.9527764 -3.9617405 -4.006217 -4.081357 -4.1135817 -4.1008186][-4.3160963 -4.3232708 -4.3105059 -4.2724781 -4.2081366 -4.124989 -4.0094519 -3.8780448 -3.8534257 -3.899826 -3.9355567 -4.0066228 -4.0914803 -4.1275907 -4.1231818][-4.3140664 -4.3165412 -4.3012862 -4.2646718 -4.2050805 -4.1122742 -3.9758737 -3.83674 -3.8228188 -3.8937213 -3.9632571 -4.0536156 -4.132257 -4.1629734 -4.1689138][-4.3142314 -4.3146992 -4.299809 -4.2686062 -4.218605 -4.1314979 -4.0083122 -3.8929715 -3.8962548 -3.9718325 -4.0487761 -4.1288767 -4.1830406 -4.2015991 -4.2100992][-4.314538 -4.3162594 -4.306385 -4.2802281 -4.2370548 -4.1720738 -4.0842886 -4.0012641 -4.008975 -4.0752878 -4.1403031 -4.1915545 -4.2202492 -4.2311749 -4.2354593][-4.3137369 -4.3133025 -4.3045177 -4.2808552 -4.2447586 -4.2024288 -4.1485763 -4.0946193 -4.1041994 -4.160912 -4.2098083 -4.2402134 -4.2574372 -4.2656884 -4.2633834][-4.3173633 -4.3147769 -4.3057957 -4.2867327 -4.2593117 -4.2323751 -4.2019339 -4.16853 -4.18083 -4.2253947 -4.2586164 -4.27698 -4.288321 -4.2926493 -4.288085][-4.3224225 -4.3190022 -4.31181 -4.2990885 -4.2810478 -4.2639341 -4.2474155 -4.2275124 -4.2386727 -4.2680788 -4.2846212 -4.2946825 -4.303236 -4.3048682 -4.3000884][-4.3257828 -4.3227334 -4.3184438 -4.3106623 -4.299468 -4.2888889 -4.280931 -4.27122 -4.279417 -4.294765 -4.2995543 -4.3019481 -4.3055363 -4.3053141 -4.302865]]...]
INFO - root - 2017-12-07 23:15:14.251037: step 65110, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 55h:05m:43s remains)
INFO - root - 2017-12-07 23:15:21.049340: step 65120, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 47h:38m:59s remains)
INFO - root - 2017-12-07 23:15:27.852244: step 65130, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 50h:05m:12s remains)
INFO - root - 2017-12-07 23:15:34.732294: step 65140, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 48h:37m:59s remains)
INFO - root - 2017-12-07 23:15:41.547748: step 65150, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 53h:54m:51s remains)
INFO - root - 2017-12-07 23:15:48.363037: step 65160, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 52h:11m:53s remains)
INFO - root - 2017-12-07 23:15:55.139051: step 65170, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 51h:11m:03s remains)
INFO - root - 2017-12-07 23:16:01.813337: step 65180, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 46h:13m:23s remains)
INFO - root - 2017-12-07 23:16:08.601214: step 65190, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.662 sec/batch; 49h:09m:21s remains)
INFO - root - 2017-12-07 23:16:15.185476: step 65200, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 52h:50m:08s remains)
2017-12-07 23:16:15.909593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3049173 -4.3078804 -4.3088117 -4.3079839 -4.3057361 -4.3018794 -4.2978191 -4.2919393 -4.2890353 -4.2890716 -4.294229 -4.3020945 -4.3094144 -4.3152328 -4.3196235][-4.3017974 -4.3058996 -4.307508 -4.3040314 -4.2965236 -4.2849035 -4.2727685 -4.259975 -4.2521863 -4.248827 -4.2552233 -4.2685361 -4.2829533 -4.296165 -4.3083057][-4.2979555 -4.30399 -4.3040195 -4.2924104 -4.2751122 -4.2528687 -4.2301087 -4.2053623 -4.1861291 -4.1754689 -4.1837854 -4.2044806 -4.2300372 -4.256341 -4.2824497][-4.2949085 -4.2997804 -4.2953682 -4.2730246 -4.2439165 -4.2090721 -4.1706266 -4.1279631 -4.0957646 -4.0833263 -4.0989566 -4.1285353 -4.163909 -4.2046456 -4.2478042][-4.2939939 -4.2949715 -4.282424 -4.2500196 -4.2085791 -4.1564112 -4.0966005 -4.0335679 -3.9935799 -3.989357 -4.0210509 -4.0604792 -4.1021247 -4.1548519 -4.2141023][-4.2893853 -4.2848172 -4.2639885 -4.2213659 -4.1649313 -4.09484 -4.0140815 -3.9316773 -3.886466 -3.9004924 -3.9520679 -4.001636 -4.0482106 -4.1115465 -4.1835737][-4.2829933 -4.27127 -4.2422929 -4.1903648 -4.1257825 -4.0482888 -3.9575183 -3.8593585 -3.8118126 -3.8485718 -3.9167056 -3.9744887 -4.0224009 -4.0882254 -4.1653852][-4.2803264 -4.2625341 -4.2282991 -4.1715889 -4.107327 -4.0363255 -3.9519937 -3.8568251 -3.8240309 -3.8820214 -3.9547665 -4.0091248 -4.0489922 -4.1028481 -4.1700163][-4.2823329 -4.2600417 -4.2214561 -4.1673689 -4.1125207 -4.0562763 -3.9965928 -3.934417 -3.9277735 -3.9861004 -4.0401444 -4.0796361 -4.1057224 -4.1404505 -4.18985][-4.2823248 -4.2588019 -4.222805 -4.1797843 -4.1402383 -4.1038818 -4.076333 -4.0503807 -4.0588331 -4.0982356 -4.1249733 -4.1461306 -4.1608462 -4.1791739 -4.2126865][-4.2841482 -4.26208 -4.2328005 -4.2034512 -4.1817179 -4.164187 -4.1564178 -4.1510968 -4.159873 -4.1787748 -4.1860676 -4.1949058 -4.2011681 -4.2096295 -4.2306352][-4.2929697 -4.2766104 -4.2537246 -4.2331328 -4.2212324 -4.2147889 -4.2139349 -4.2131968 -4.2184992 -4.2258763 -4.2249079 -4.228797 -4.2308626 -4.2338066 -4.2464557][-4.3019433 -4.2940083 -4.2785444 -4.2637024 -4.2544174 -4.2506642 -4.2497754 -4.2498069 -4.2526174 -4.2545519 -4.2515364 -4.2525058 -4.2520008 -4.2517114 -4.2601719][-4.3066607 -4.3068795 -4.2989841 -4.2889471 -4.28012 -4.2732778 -4.2690883 -4.2697868 -4.2733817 -4.2741003 -4.2696428 -4.2670007 -4.2641139 -4.2623219 -4.2693458][-4.3066192 -4.3099523 -4.3068943 -4.3014445 -4.2945442 -4.2857823 -4.2777057 -4.276341 -4.279983 -4.2819982 -4.2795615 -4.2777138 -4.276021 -4.2758417 -4.2830915]]...]
INFO - root - 2017-12-07 23:16:22.652931: step 65210, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 47h:01m:37s remains)
INFO - root - 2017-12-07 23:16:29.483368: step 65220, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 49h:27m:02s remains)
INFO - root - 2017-12-07 23:16:36.285986: step 65230, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 51h:17m:47s remains)
INFO - root - 2017-12-07 23:16:42.980938: step 65240, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 49h:39m:45s remains)
INFO - root - 2017-12-07 23:16:49.683280: step 65250, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 49h:30m:14s remains)
INFO - root - 2017-12-07 23:16:56.433948: step 65260, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 47h:01m:10s remains)
INFO - root - 2017-12-07 23:17:03.201535: step 65270, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 46h:17m:25s remains)
INFO - root - 2017-12-07 23:17:10.069576: step 65280, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.732 sec/batch; 54h:21m:52s remains)
INFO - root - 2017-12-07 23:17:16.852151: step 65290, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 52h:01m:46s remains)
INFO - root - 2017-12-07 23:17:23.466204: step 65300, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 49h:27m:26s remains)
2017-12-07 23:17:24.191653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2226834 -4.2208638 -4.2506347 -4.2942476 -4.3224053 -4.3290033 -4.2999439 -4.2299 -4.1262684 -4.0477047 -4.0586777 -4.1511345 -4.2480111 -4.3057661 -4.3282394][-4.2202854 -4.2263441 -4.2588167 -4.2933006 -4.3095732 -4.3092632 -4.272615 -4.1952586 -4.0841393 -4.0035305 -4.0231504 -4.1300688 -4.2361288 -4.301384 -4.3310232][-4.2276392 -4.2448931 -4.2778606 -4.2988563 -4.2976089 -4.2844453 -4.2370462 -4.1475649 -4.0245576 -3.9381928 -3.9696963 -4.0964813 -4.2161431 -4.2924929 -4.3307958][-4.2474852 -4.2747378 -4.3064542 -4.3147349 -4.2952218 -4.2640171 -4.2047338 -4.099575 -3.9604282 -3.8658502 -3.9120793 -4.0598874 -4.1959472 -4.284337 -4.3297572][-4.2738323 -4.3102016 -4.337728 -4.3343821 -4.2987585 -4.2499771 -4.1798191 -4.068933 -3.9333308 -3.8481057 -3.9069877 -4.056613 -4.1926174 -4.2840686 -4.3326735][-4.2874823 -4.335866 -4.3597646 -4.3466973 -4.298038 -4.2336383 -4.1554685 -4.052052 -3.9510148 -3.9119487 -3.9771113 -4.1022363 -4.215457 -4.2954488 -4.340126][-4.2828989 -4.338975 -4.361702 -4.345222 -4.290679 -4.2135296 -4.1276569 -4.0399003 -3.9832947 -3.9919319 -4.0647039 -4.1624856 -4.2479725 -4.31041 -4.3452826][-4.2726345 -4.3342028 -4.3569031 -4.3394985 -4.28441 -4.1988921 -4.1095419 -4.0411773 -4.0235348 -4.0646462 -4.1368752 -4.2105227 -4.2721057 -4.3182769 -4.343349][-4.2702136 -4.3345232 -4.3578358 -4.3411117 -4.2865381 -4.1987038 -4.1141124 -4.0636125 -4.0696826 -4.12331 -4.189949 -4.2453537 -4.2884307 -4.3229017 -4.342329][-4.2749362 -4.3326983 -4.3535838 -4.33877 -4.2849689 -4.200182 -4.1261711 -4.0935073 -4.1121078 -4.1677084 -4.2275133 -4.2742872 -4.3068318 -4.3317189 -4.3434453][-4.2782335 -4.3211346 -4.3365126 -4.3272543 -4.2841668 -4.2111979 -4.1479216 -4.1251273 -4.1452723 -4.1971226 -4.2549839 -4.3006005 -4.3280993 -4.3427839 -4.3421469][-4.2723479 -4.2980289 -4.3081107 -4.3076515 -4.2823777 -4.2269588 -4.17375 -4.1534414 -4.1696386 -4.214036 -4.2712083 -4.3168211 -4.3411813 -4.3455563 -4.3300815][-4.2557 -4.268528 -4.2749472 -4.2822375 -4.2749634 -4.2389235 -4.195622 -4.1779437 -4.1913013 -4.2283411 -4.2801614 -4.3202033 -4.33905 -4.3350396 -4.3088956][-4.2329669 -4.2382793 -4.2459383 -4.2603025 -4.2663455 -4.2492528 -4.2181373 -4.2046947 -4.2169147 -4.2464437 -4.2841377 -4.3119192 -4.3243561 -4.3174653 -4.2893457][-4.2175393 -4.2228756 -4.2345414 -4.2506404 -4.2612557 -4.258235 -4.2437515 -4.2397432 -4.2519622 -4.2704034 -4.2859416 -4.2966022 -4.3010273 -4.2961206 -4.2749829]]...]
INFO - root - 2017-12-07 23:17:31.017094: step 65310, loss = 2.09, batch loss = 2.04 (10.9 examples/sec; 0.732 sec/batch; 54h:17m:32s remains)
INFO - root - 2017-12-07 23:17:37.743669: step 65320, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 51h:52m:13s remains)
INFO - root - 2017-12-07 23:17:44.519873: step 65330, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 48h:48m:49s remains)
INFO - root - 2017-12-07 23:17:51.252901: step 65340, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 46h:37m:08s remains)
INFO - root - 2017-12-07 23:17:57.969128: step 65350, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 48h:57m:42s remains)
INFO - root - 2017-12-07 23:18:04.791965: step 65360, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 53h:33m:53s remains)
INFO - root - 2017-12-07 23:18:11.635029: step 65370, loss = 2.09, batch loss = 2.04 (11.1 examples/sec; 0.723 sec/batch; 53h:37m:29s remains)
INFO - root - 2017-12-07 23:18:18.155315: step 65380, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.633 sec/batch; 46h:56m:17s remains)
INFO - root - 2017-12-07 23:18:24.833008: step 65390, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 48h:46m:50s remains)
INFO - root - 2017-12-07 23:18:31.580648: step 65400, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 53h:12m:57s remains)
2017-12-07 23:18:32.336378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3079963 -4.3068361 -4.306736 -4.3066382 -4.3066945 -4.305788 -4.3044963 -4.303473 -4.3029685 -4.3028665 -4.3030396 -4.3052835 -4.30922 -4.3124857 -4.3124604][-4.3125138 -4.3125877 -4.313458 -4.3146992 -4.3146243 -4.3112245 -4.3070407 -4.3037333 -4.3023844 -4.3035359 -4.3065572 -4.3117623 -4.3180408 -4.3228416 -4.3229041][-4.3087821 -4.3121638 -4.3147836 -4.3182449 -4.3169541 -4.3080788 -4.2973919 -4.29021 -4.2891951 -4.2927918 -4.2995906 -4.3079848 -4.3169379 -4.3238473 -4.3246241][-4.3131704 -4.3182111 -4.321671 -4.3245397 -4.3182077 -4.3002667 -4.27904 -4.2627263 -4.2580509 -4.2635684 -4.2752914 -4.2893195 -4.3038654 -4.3168464 -4.3252821][-4.3180528 -4.3234096 -4.3252139 -4.3228602 -4.306057 -4.2728128 -4.2326427 -4.1991663 -4.1883969 -4.2002649 -4.2231226 -4.2467937 -4.2715406 -4.2959929 -4.3175488][-4.2907486 -4.2956095 -4.2918978 -4.2811546 -4.2506785 -4.195004 -4.125721 -4.0672278 -4.050221 -4.0770712 -4.1182141 -4.1597185 -4.2049117 -4.2494879 -4.2868795][-4.2195783 -4.2226634 -4.2101312 -4.1845474 -4.1365328 -4.0562134 -3.957473 -3.8721585 -3.8552539 -3.9117615 -3.9781034 -4.0385365 -4.1030765 -4.1698947 -4.224484][-4.1634541 -4.1686034 -4.1488132 -4.1110811 -4.05163 -3.9605408 -3.8414934 -3.7298234 -3.7164822 -3.8035285 -3.8901751 -3.9648061 -4.0425091 -4.122685 -4.1856866][-4.1785269 -4.1850672 -4.164104 -4.1227856 -4.0640588 -3.9857869 -3.8797321 -3.7782497 -3.7783816 -3.868789 -3.950506 -4.0166597 -4.0860491 -4.1575055 -4.2127671][-4.2332726 -4.2404194 -4.2254968 -4.1925497 -4.1461372 -4.0939441 -4.0186005 -3.9484344 -3.9594717 -4.0335641 -4.0944705 -4.1378393 -4.1855211 -4.2352643 -4.2711725][-4.283267 -4.2945113 -4.2925563 -4.2740264 -4.2440848 -4.2130771 -4.1621861 -4.1189685 -4.1296244 -4.1773887 -4.2160625 -4.2389469 -4.2661281 -4.295681 -4.3120947][-4.2913518 -4.303525 -4.3122396 -4.3082695 -4.2946234 -4.2805619 -4.2512136 -4.2275147 -4.2334518 -4.2570238 -4.2759767 -4.2855368 -4.2992554 -4.3177829 -4.3256907][-4.2719078 -4.2820005 -4.2957892 -4.3012805 -4.2984557 -4.2947149 -4.2812853 -4.272049 -4.27474 -4.2841659 -4.2887688 -4.2898588 -4.2972622 -4.3109984 -4.3183765][-4.2412553 -4.2463865 -4.259562 -4.2661 -4.2658486 -4.2629542 -4.2571564 -4.2575636 -4.263082 -4.2677584 -4.2643261 -4.259654 -4.2660155 -4.2815428 -4.2938089][-4.2002821 -4.2049556 -4.2183385 -4.2249184 -4.2211552 -4.2115426 -4.2033606 -4.2047353 -4.2111974 -4.216373 -4.2115211 -4.2062192 -4.2168903 -4.2392049 -4.2617817]]...]
INFO - root - 2017-12-07 23:18:39.153396: step 65410, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 48h:21m:15s remains)
INFO - root - 2017-12-07 23:18:45.846697: step 65420, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 51h:01m:37s remains)
INFO - root - 2017-12-07 23:18:52.613079: step 65430, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 50h:35m:18s remains)
INFO - root - 2017-12-07 23:18:59.353832: step 65440, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 48h:16m:36s remains)
INFO - root - 2017-12-07 23:19:06.039228: step 65450, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 48h:38m:41s remains)
INFO - root - 2017-12-07 23:19:12.886081: step 65460, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 47h:27m:25s remains)
INFO - root - 2017-12-07 23:19:19.620123: step 65470, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 50h:08m:48s remains)
INFO - root - 2017-12-07 23:19:26.462566: step 65480, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 55h:02m:28s remains)
INFO - root - 2017-12-07 23:19:33.223527: step 65490, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 50h:35m:58s remains)
INFO - root - 2017-12-07 23:19:39.801121: step 65500, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 49h:09m:53s remains)
2017-12-07 23:19:40.499084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3060365 -4.2983589 -4.2897491 -4.269237 -4.2459469 -4.2225375 -4.20429 -4.1979609 -4.2010469 -4.2195849 -4.2464104 -4.2597628 -4.2512536 -4.2466683 -4.2619882][-4.3029265 -4.2938638 -4.283083 -4.2597361 -4.2352476 -4.213285 -4.1992273 -4.1954937 -4.198266 -4.2175803 -4.2448006 -4.2555728 -4.2462974 -4.2438207 -4.2622075][-4.2988062 -4.2908792 -4.2798753 -4.2527137 -4.2217326 -4.193831 -4.1806593 -4.1847634 -4.1967754 -4.2204647 -4.2449236 -4.25167 -4.2448492 -4.2460022 -4.2658987][-4.2970672 -4.2914929 -4.2810817 -4.2483449 -4.2075024 -4.1710896 -4.1582851 -4.1755424 -4.2025604 -4.2320232 -4.2545257 -4.2592192 -4.2532749 -4.2554946 -4.2734594][-4.2946391 -4.2897973 -4.2787957 -4.2406554 -4.190093 -4.1427021 -4.1289806 -4.1619673 -4.2106767 -4.2498531 -4.2710676 -4.2728419 -4.2651262 -4.2655826 -4.2812853][-4.2927432 -4.2862811 -4.2727385 -4.2273626 -4.1636362 -4.1006694 -4.0763936 -4.1223021 -4.1986141 -4.2515845 -4.2711711 -4.2694163 -4.2608013 -4.2614036 -4.2773266][-4.2935934 -4.2858634 -4.2692075 -4.2177777 -4.14281 -4.0620093 -4.0154314 -4.0641141 -4.1623316 -4.2248979 -4.2404366 -4.2343946 -4.2283082 -4.23453 -4.2557373][-4.2983322 -4.2927 -4.2767143 -4.2251954 -4.15099 -4.069077 -4.0110393 -4.0480576 -4.1408043 -4.1965308 -4.1991 -4.1808777 -4.175559 -4.1882024 -4.2142844][-4.3058367 -4.3051386 -4.2935491 -4.2496538 -4.1881723 -4.1229153 -4.075984 -4.1020269 -4.1691041 -4.2061195 -4.19449 -4.1625633 -4.1498995 -4.1626244 -4.1872516][-4.3135347 -4.3182449 -4.3126407 -4.2793522 -4.2342443 -4.1894155 -4.159965 -4.1778212 -4.2191057 -4.2384982 -4.2212915 -4.1830621 -4.1636877 -4.1746674 -4.199717][-4.31928 -4.3270164 -4.3265567 -4.3048143 -4.2732954 -4.2439475 -4.2275939 -4.2388096 -4.2593756 -4.262949 -4.2434273 -4.2081347 -4.1928272 -4.204659 -4.229383][-4.3215919 -4.3279982 -4.3297148 -4.3172793 -4.2956409 -4.2775121 -4.2704291 -4.28034 -4.2879772 -4.2775416 -4.2502847 -4.2181726 -4.2118483 -4.2297239 -4.2550597][-4.3200207 -4.3221555 -4.3229632 -4.3129616 -4.2964673 -4.2863255 -4.2884278 -4.2995582 -4.3034263 -4.2848468 -4.25122 -4.219913 -4.2185874 -4.24117 -4.2672014][-4.316793 -4.3112073 -4.305728 -4.291625 -4.27471 -4.2687707 -4.2798824 -4.2983584 -4.30771 -4.2914305 -4.2590666 -4.2288265 -4.22549 -4.2454996 -4.2693162][-4.3147016 -4.3030396 -4.2904735 -4.2666917 -4.240653 -4.2303166 -4.2472715 -4.2763777 -4.29765 -4.2936263 -4.2730889 -4.2491221 -4.2425385 -4.2541256 -4.2731051]]...]
INFO - root - 2017-12-07 23:19:47.359602: step 65510, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 54h:21m:25s remains)
INFO - root - 2017-12-07 23:19:54.139097: step 65520, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 49h:53m:31s remains)
INFO - root - 2017-12-07 23:20:00.938040: step 65530, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.641 sec/batch; 47h:33m:11s remains)
INFO - root - 2017-12-07 23:20:07.742251: step 65540, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.658 sec/batch; 48h:49m:42s remains)
INFO - root - 2017-12-07 23:20:14.500143: step 65550, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 48h:40m:07s remains)
INFO - root - 2017-12-07 23:20:21.285043: step 65560, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 52h:41m:41s remains)
INFO - root - 2017-12-07 23:20:27.984461: step 65570, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 50h:26m:05s remains)
INFO - root - 2017-12-07 23:20:34.669865: step 65580, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 53h:04m:11s remains)
INFO - root - 2017-12-07 23:20:41.292041: step 65590, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 46h:19m:27s remains)
INFO - root - 2017-12-07 23:20:47.908306: step 65600, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 49h:37m:00s remains)
2017-12-07 23:20:48.656717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0436888 -4.0421309 -4.0998015 -4.1623588 -4.2124848 -4.24827 -4.276073 -4.2936335 -4.2872839 -4.2524757 -4.2023439 -4.178071 -4.1872749 -4.203383 -4.2234783][-3.9950314 -3.9900439 -4.0441236 -4.1011972 -4.1481776 -4.1814704 -4.2108383 -4.2344284 -4.2380533 -4.217247 -4.1812854 -4.1669497 -4.1822448 -4.2016511 -4.2258482][-3.9768593 -3.9662926 -4.0099173 -4.0584106 -4.0921359 -4.109499 -4.1300249 -4.151865 -4.1716576 -4.1801233 -4.1721792 -4.1724873 -4.1880198 -4.2072916 -4.2280769][-4.0044289 -3.9917829 -4.0184956 -4.0467596 -4.0516925 -4.0403757 -4.0448794 -4.0689926 -4.1068192 -4.1429834 -4.1595755 -4.1739063 -4.1883807 -4.2037344 -4.2179494][-4.06643 -4.0528855 -4.058557 -4.0550981 -4.0244431 -3.9780016 -3.9594629 -3.9876795 -4.0462441 -4.1027937 -4.1351757 -4.1589704 -4.1786437 -4.1955543 -4.2104993][-4.1434646 -4.1265349 -4.1061511 -4.065124 -3.9952838 -3.9161866 -3.8787906 -3.9181879 -3.992156 -4.0603137 -4.1084466 -4.1406 -4.1709032 -4.1994705 -4.2226696][-4.1936436 -4.1739492 -4.1267447 -4.0484262 -3.9428353 -3.8434746 -3.8044112 -3.8559475 -3.9430234 -4.0175233 -4.0755424 -4.1144142 -4.1544304 -4.1965251 -4.227839][-4.2241349 -4.1938291 -4.1191077 -4.0067635 -3.8695815 -3.7568955 -3.7255838 -3.7936873 -3.901638 -3.9835596 -4.0456805 -4.088244 -4.1276054 -4.1711521 -4.2101011][-4.2192864 -4.186048 -4.10267 -3.9759841 -3.8349366 -3.7327032 -3.7118087 -3.7867661 -3.9016695 -3.9877815 -4.0439224 -4.0741754 -4.0913024 -4.1241574 -4.165885][-4.1856236 -4.1589684 -4.0844884 -3.9733322 -3.8657856 -3.803323 -3.79803 -3.8571813 -3.9468341 -4.0196495 -4.062994 -4.065742 -4.0466447 -4.0589104 -4.097167][-4.124577 -4.1058364 -4.0520535 -3.9758902 -3.9082396 -3.8761506 -3.8798306 -3.9212997 -3.9881446 -4.0470023 -4.0815406 -4.0667334 -4.0219707 -4.0145226 -4.0470071][-4.08703 -4.0753 -4.0428305 -3.9971836 -3.9570653 -3.939518 -3.9448941 -3.9730673 -4.0207682 -4.0695114 -4.0973368 -4.0778255 -4.0268507 -4.0103168 -4.0369687][-4.0923972 -4.08515 -4.0679541 -4.0447817 -4.0273457 -4.0247602 -4.0375409 -4.055367 -4.082705 -4.1123133 -4.1242614 -4.0982714 -4.0447783 -4.023459 -4.0465646][-4.131609 -4.1303382 -4.1249571 -4.1157374 -4.1129665 -4.122941 -4.1392436 -4.1478782 -4.15667 -4.16667 -4.1662283 -4.1401739 -4.0935469 -4.0723214 -4.0909052][-4.1911125 -4.192853 -4.1925855 -4.1899557 -4.1922274 -4.2038646 -4.2156596 -4.2179818 -4.2168059 -4.2170949 -4.2140789 -4.1967878 -4.1649861 -4.1501818 -4.1630635]]...]
INFO - root - 2017-12-07 23:20:55.536728: step 65610, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 52h:29m:54s remains)
INFO - root - 2017-12-07 23:21:02.307473: step 65620, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 46h:18m:44s remains)
INFO - root - 2017-12-07 23:21:09.161801: step 65630, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 48h:18m:08s remains)
INFO - root - 2017-12-07 23:21:16.067835: step 65640, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 54h:49m:09s remains)
INFO - root - 2017-12-07 23:21:22.801253: step 65650, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 55h:17m:13s remains)
INFO - root - 2017-12-07 23:21:29.552155: step 65660, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 53h:03m:21s remains)
INFO - root - 2017-12-07 23:21:36.259975: step 65670, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 47h:05m:04s remains)
INFO - root - 2017-12-07 23:21:43.060745: step 65680, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.677 sec/batch; 50h:11m:59s remains)
INFO - root - 2017-12-07 23:21:49.713331: step 65690, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 48h:30m:34s remains)
INFO - root - 2017-12-07 23:21:56.285332: step 65700, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 48h:56m:19s remains)
2017-12-07 23:21:57.015250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2111244 -4.2052736 -4.2045159 -4.1840291 -4.1591749 -4.170166 -4.1894746 -4.1947975 -4.1956038 -4.2090435 -4.2205997 -4.2298713 -4.2334642 -4.2299051 -4.2288423][-4.232841 -4.2344766 -4.2374668 -4.2180185 -4.1863494 -4.183795 -4.1936579 -4.1934533 -4.1845951 -4.1899514 -4.2007856 -4.2099538 -4.2134533 -4.2103186 -4.2068343][-4.2526708 -4.2557096 -4.2568521 -4.2431703 -4.2167597 -4.2042975 -4.2031412 -4.1984634 -4.1824837 -4.1767416 -4.1808405 -4.191802 -4.199801 -4.193212 -4.1787043][-4.2687817 -4.2661881 -4.2612524 -4.2530508 -4.2394204 -4.2241631 -4.2092614 -4.1979156 -4.1796279 -4.1661677 -4.1628566 -4.1728973 -4.1879048 -4.1809688 -4.159339][-4.2778797 -4.2670369 -4.2535048 -4.2420859 -4.2317157 -4.2118573 -4.1847687 -4.169456 -4.164813 -4.1601863 -4.1553035 -4.1612334 -4.1728168 -4.1667786 -4.1468525][-4.2778955 -4.2606173 -4.2389097 -4.2159567 -4.1926203 -4.1605325 -4.1172671 -4.0985913 -4.127048 -4.1568222 -4.1630125 -4.1637373 -4.1619473 -4.1512589 -4.1293154][-4.2734351 -4.2539496 -4.2236357 -4.1868119 -4.1472926 -4.0977631 -4.0170841 -3.9745271 -4.0460224 -4.12689 -4.1621313 -4.1711211 -4.1615286 -4.1372747 -4.105689][-4.2593021 -4.2391119 -4.2064595 -4.1619153 -4.1118555 -4.046948 -3.9248266 -3.8362775 -3.9485254 -4.0813627 -4.1470103 -4.167141 -4.1625972 -4.1369605 -4.101964][-4.2409143 -4.2241874 -4.1978927 -4.1561351 -4.1152277 -4.0665727 -3.961967 -3.8772068 -3.9637077 -4.0826392 -4.1453466 -4.1624517 -4.163372 -4.1525841 -4.1285048][-4.2287169 -4.217452 -4.1975756 -4.1642089 -4.1364269 -4.1157637 -4.0555415 -3.999877 -4.0395737 -4.104208 -4.1375017 -4.145196 -4.1533036 -4.1654987 -4.1621976][-4.222362 -4.2176723 -4.2046542 -4.1803713 -4.1551604 -4.145566 -4.1187191 -4.0863948 -4.1010575 -4.1252909 -4.1340089 -4.1297879 -4.137918 -4.1608953 -4.17441][-4.2207966 -4.2228208 -4.2153678 -4.1939774 -4.163249 -4.1559029 -4.1542249 -4.1454444 -4.1533179 -4.1584177 -4.1549721 -4.1443319 -4.1454339 -4.1639509 -4.1732264][-4.2303381 -4.2372346 -4.2362189 -4.219995 -4.1858149 -4.171669 -4.1799932 -4.1848354 -4.1883564 -4.1818066 -4.1698065 -4.1538677 -4.1522036 -4.1692977 -4.1771092][-4.24057 -4.2476521 -4.2524471 -4.2454491 -4.2135916 -4.192194 -4.1999192 -4.205646 -4.2022996 -4.1851873 -4.1616125 -4.1436453 -4.1488791 -4.1712837 -4.1851211][-4.242466 -4.2467184 -4.2504153 -4.2487993 -4.2220669 -4.1984644 -4.2032223 -4.2025771 -4.1870732 -4.1593637 -4.129405 -4.1159234 -4.1310587 -4.1693258 -4.1988297]]...]
INFO - root - 2017-12-07 23:22:03.811541: step 65710, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 52h:50m:36s remains)
INFO - root - 2017-12-07 23:22:10.660799: step 65720, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 52h:21m:01s remains)
INFO - root - 2017-12-07 23:22:17.466943: step 65730, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 50h:53m:27s remains)
INFO - root - 2017-12-07 23:22:24.239727: step 65740, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 48h:19m:03s remains)
INFO - root - 2017-12-07 23:22:31.129099: step 65750, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 54h:01m:50s remains)
INFO - root - 2017-12-07 23:22:37.971779: step 65760, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.736 sec/batch; 54h:31m:22s remains)
INFO - root - 2017-12-07 23:22:44.812579: step 65770, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 49h:43m:45s remains)
INFO - root - 2017-12-07 23:22:51.601599: step 65780, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 50h:55m:15s remains)
INFO - root - 2017-12-07 23:22:58.355944: step 65790, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.611 sec/batch; 45h:16m:19s remains)
INFO - root - 2017-12-07 23:23:05.083716: step 65800, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 52h:18m:59s remains)
2017-12-07 23:23:05.888447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2287488 -4.2267284 -4.2239327 -4.2224469 -4.2213764 -4.2209935 -4.2183247 -4.2143087 -4.2063727 -4.1929865 -4.1765704 -4.1605711 -4.1521473 -4.1568761 -4.1728539][-4.2377162 -4.2340369 -4.2304535 -4.2283492 -4.2254558 -4.2223716 -4.2159925 -4.2062726 -4.1911092 -4.1667686 -4.1401715 -4.1178346 -4.1098108 -4.1200328 -4.1440325][-4.232636 -4.2267 -4.221724 -4.2174697 -4.2110329 -4.2044024 -4.195364 -4.18309 -4.1640296 -4.1326556 -4.0987372 -4.0716324 -4.0646915 -4.0782127 -4.103271][-4.2124443 -4.2044606 -4.1976924 -4.1899657 -4.1791196 -4.1696405 -4.1609144 -4.1503115 -4.133163 -4.1006327 -4.0624328 -4.0302153 -4.0212045 -4.0337758 -4.0547061][-4.1839 -4.1721525 -4.1603079 -4.1444945 -4.1259909 -4.1149321 -4.1120367 -4.1113939 -4.1059608 -4.0813789 -4.0469708 -4.0144787 -3.9993317 -4.00337 -4.0146542][-4.1650381 -4.14929 -4.1323538 -4.1075883 -4.0793858 -4.0620055 -4.0646634 -4.0791364 -4.0887957 -4.0718179 -4.042604 -4.0090132 -3.9867508 -3.9808266 -3.9875467][-4.1697631 -4.1577888 -4.1436849 -4.1186461 -4.0869274 -4.0637579 -4.0656762 -4.0861225 -4.1013708 -4.09046 -4.0660009 -4.0313635 -4.0036936 -3.9888921 -3.9933164][-4.1889663 -4.1845069 -4.1780386 -4.1578612 -4.1287231 -4.1055365 -4.1062379 -4.1277184 -4.1445317 -4.1386967 -4.1208487 -4.0898666 -4.0626435 -4.0425048 -4.0418458][-4.2092857 -4.2096772 -4.2092934 -4.1973844 -4.176126 -4.1576676 -4.1583982 -4.1790247 -4.1967206 -4.1961722 -4.18559 -4.164762 -4.1438918 -4.1206737 -4.1090345][-4.2208428 -4.2228088 -4.2242012 -4.2175951 -4.2041411 -4.191812 -4.1905403 -4.2077551 -4.2275562 -4.2328939 -4.2297282 -4.2198358 -4.205296 -4.1837511 -4.1654859][-4.2305017 -4.2351727 -4.2397838 -4.2372751 -4.2282557 -4.2191362 -4.216928 -4.2301497 -4.2481523 -4.2537093 -4.253242 -4.2457252 -4.2327757 -4.2138987 -4.1973419][-4.2393289 -4.246963 -4.2555366 -4.256608 -4.2506423 -4.2448897 -4.2439084 -4.2524056 -4.2615209 -4.2599535 -4.25597 -4.2475123 -4.2332983 -4.2171874 -4.2041149][-4.2374187 -4.2473688 -4.2565446 -4.257019 -4.2510524 -4.2476096 -4.2501535 -4.2579875 -4.2600942 -4.2528276 -4.2448845 -4.2352109 -4.22079 -4.2105446 -4.2035265][-4.2236362 -4.234241 -4.2439942 -4.2449837 -4.2411342 -4.2412271 -4.2464 -4.2535372 -4.2537413 -4.2443385 -4.2326212 -4.2209587 -4.2082548 -4.2033672 -4.2025881][-4.2084632 -4.2146277 -4.2228484 -4.2252073 -4.2229838 -4.2227511 -4.2261319 -4.2298284 -4.230094 -4.2216997 -4.2098646 -4.1983566 -4.1903358 -4.1904526 -4.1943369]]...]
INFO - root - 2017-12-07 23:23:12.705898: step 65810, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 46h:56m:12s remains)
INFO - root - 2017-12-07 23:23:19.487589: step 65820, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 47h:24m:05s remains)
INFO - root - 2017-12-07 23:23:26.316504: step 65830, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 52h:00m:50s remains)
INFO - root - 2017-12-07 23:23:33.227098: step 65840, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.749 sec/batch; 55h:29m:32s remains)
INFO - root - 2017-12-07 23:23:40.083065: step 65850, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 49h:35m:33s remains)
INFO - root - 2017-12-07 23:23:46.937110: step 65860, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 50h:22m:35s remains)
INFO - root - 2017-12-07 23:23:53.751522: step 65870, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 52h:39m:34s remains)
INFO - root - 2017-12-07 23:24:00.666789: step 65880, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.764 sec/batch; 56h:33m:17s remains)
INFO - root - 2017-12-07 23:24:07.433763: step 65890, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 50h:01m:12s remains)
INFO - root - 2017-12-07 23:24:14.049936: step 65900, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.627 sec/batch; 46h:25m:26s remains)
2017-12-07 23:24:14.776889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2209344 -4.2149944 -4.1869206 -4.1627569 -4.1425467 -4.108573 -4.0618968 -4.0230055 -4.0165234 -4.0381932 -4.0691519 -4.0984368 -4.1168189 -4.1319671 -4.1440372][-4.2385292 -4.2300997 -4.1976833 -4.17263 -4.1491466 -4.1152911 -4.0656548 -4.0215478 -4.0176821 -4.0433393 -4.0788927 -4.1158195 -4.1371956 -4.1489043 -4.15244][-4.2476606 -4.2393031 -4.2112384 -4.1881824 -4.1590471 -4.1187096 -4.066462 -4.0188365 -4.0172329 -4.0444312 -4.0789356 -4.1143761 -4.1391277 -4.148313 -4.144043][-4.2467303 -4.2370119 -4.2162533 -4.19563 -4.1583242 -4.1108532 -4.0630283 -4.0215521 -4.0209527 -4.0426273 -4.0690074 -4.0988293 -4.1225586 -4.1295743 -4.1223464][-4.2387586 -4.2246356 -4.2042003 -4.1799011 -4.1330409 -4.0841141 -4.046381 -4.0217147 -4.0315328 -4.0510168 -4.0685735 -4.0891457 -4.1101608 -4.1156211 -4.1065722][-4.2362485 -4.2202868 -4.1961937 -4.1652327 -4.1166921 -4.0735135 -4.0419517 -4.0278263 -4.0459976 -4.0645251 -4.0741396 -4.085629 -4.10515 -4.1164308 -4.1170459][-4.2405214 -4.2271533 -4.1994052 -4.1605287 -4.1168504 -4.0868731 -4.0607667 -4.0459847 -4.0578032 -4.0664821 -4.0674014 -4.0729218 -4.0992661 -4.123415 -4.1365752][-4.2457895 -4.2286506 -4.19106 -4.1454482 -4.1116257 -4.0999184 -4.0868464 -4.07243 -4.06872 -4.0675812 -4.0698309 -4.0814552 -4.1175146 -4.1470146 -4.1575265][-4.2381797 -4.2161655 -4.17423 -4.130374 -4.1057835 -4.1109786 -4.1083822 -4.0982251 -4.0868053 -4.0799961 -4.0829363 -4.0997171 -4.1355143 -4.1578984 -4.1588545][-4.214725 -4.1891274 -4.1513834 -4.1190734 -4.1067963 -4.1212106 -4.123075 -4.1169243 -4.1040912 -4.0930395 -4.0940595 -4.1047511 -4.1292939 -4.1449475 -4.1411734][-4.1876845 -4.1551166 -4.1233497 -4.1077275 -4.11134 -4.1305714 -4.1362057 -4.1379428 -4.1285682 -4.1153774 -4.1072955 -4.102313 -4.1091342 -4.1191311 -4.1206808][-4.1809478 -4.1383944 -4.1060619 -4.0990009 -4.1128578 -4.1387138 -4.1521435 -4.16233 -4.1576843 -4.140552 -4.1228724 -4.1029053 -4.095787 -4.1053262 -4.1187272][-4.1947737 -4.1517034 -4.1215315 -4.1166244 -4.1264839 -4.1481471 -4.1658735 -4.18438 -4.1856437 -4.1683993 -4.146944 -4.1221886 -4.1127748 -4.1260228 -4.1458468][-4.2141705 -4.1799145 -4.159441 -4.1590195 -4.1621366 -4.1749735 -4.1913185 -4.2101741 -4.2140861 -4.2002325 -4.1818247 -4.1609526 -4.1523223 -4.1636739 -4.1839266][-4.2385135 -4.2126546 -4.2006092 -4.2040882 -4.2107911 -4.2244921 -4.240016 -4.2520118 -4.2512369 -4.2350612 -4.2170677 -4.2025709 -4.1982455 -4.2086234 -4.2267256]]...]
INFO - root - 2017-12-07 23:24:21.496765: step 65910, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 46h:55m:43s remains)
INFO - root - 2017-12-07 23:24:28.257112: step 65920, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 47h:42m:15s remains)
INFO - root - 2017-12-07 23:24:35.084298: step 65930, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 48h:35m:27s remains)
INFO - root - 2017-12-07 23:24:42.026995: step 65940, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 51h:06m:02s remains)
INFO - root - 2017-12-07 23:24:48.879164: step 65950, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 53h:16m:58s remains)
INFO - root - 2017-12-07 23:24:55.655961: step 65960, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 49h:56m:01s remains)
INFO - root - 2017-12-07 23:25:02.378779: step 65970, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 47h:51m:21s remains)
INFO - root - 2017-12-07 23:25:09.127026: step 65980, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 46h:36m:27s remains)
INFO - root - 2017-12-07 23:25:15.988750: step 65990, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.724 sec/batch; 53h:38m:05s remains)
INFO - root - 2017-12-07 23:25:22.333226: step 66000, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 49h:44m:56s remains)
2017-12-07 23:25:23.081977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2060366 -4.2024446 -4.2262287 -4.2540927 -4.2645593 -4.2511563 -4.218544 -4.17686 -4.1448679 -4.1262589 -4.1253042 -4.1326284 -4.1458979 -4.1556711 -4.1725788][-4.2145734 -4.2069249 -4.2281494 -4.2540874 -4.2654285 -4.2566767 -4.2317872 -4.1946058 -4.1666274 -4.1511321 -4.1496363 -4.147975 -4.1501641 -4.1521497 -4.1707487][-4.1851768 -4.1734395 -4.194613 -4.2234497 -4.24231 -4.2454982 -4.2339249 -4.2126765 -4.1979833 -4.1874995 -4.1796193 -4.1636968 -4.1525283 -4.1518779 -4.1737814][-4.1350627 -4.11791 -4.1425428 -4.1700311 -4.1887193 -4.2040057 -4.2067242 -4.2034745 -4.2079368 -4.2079549 -4.1964688 -4.1684937 -4.1469464 -4.1437478 -4.1697316][-4.0962896 -4.064642 -4.0901918 -4.1173711 -4.1281571 -4.1478281 -4.1618161 -4.1768627 -4.199059 -4.2088451 -4.1985574 -4.1681557 -4.1446295 -4.1421409 -4.1659837][-4.1019659 -4.0614057 -4.0799894 -4.1004362 -4.1016793 -4.1160817 -4.13393 -4.1550765 -4.182261 -4.1986933 -4.1971507 -4.1724396 -4.1539936 -4.1517649 -4.166924][-4.1524959 -4.1162181 -4.1213756 -4.1282997 -4.1210303 -4.1239796 -4.1328998 -4.1449041 -4.1665483 -4.1863666 -4.1976886 -4.186142 -4.1762853 -4.1711712 -4.1740751][-4.1951075 -4.1750531 -4.1762977 -4.1708484 -4.155736 -4.14133 -4.1279993 -4.1235876 -4.1420126 -4.1719518 -4.1979265 -4.2020245 -4.2018862 -4.193264 -4.1845875][-4.2142634 -4.2144809 -4.2170053 -4.2028179 -4.1760821 -4.144125 -4.1131458 -4.0970287 -4.1135225 -4.1560984 -4.192337 -4.2093525 -4.2200246 -4.2117996 -4.1961069][-4.2290292 -4.2479887 -4.2497511 -4.2211032 -4.1741958 -4.1289349 -4.0976839 -4.0831208 -4.1009841 -4.149271 -4.1868024 -4.2083564 -4.2264137 -4.2209826 -4.2064862][-4.2423911 -4.2735548 -4.2711387 -4.2281041 -4.163744 -4.1131611 -4.0909019 -4.0817776 -4.093442 -4.1353664 -4.1717482 -4.1970224 -4.2194023 -4.2174044 -4.2066417][-4.2461772 -4.2852387 -4.2784848 -4.2234273 -4.1523395 -4.1076579 -4.0969505 -4.0890908 -4.0835633 -4.1098289 -4.1420221 -4.1714048 -4.1977596 -4.1976976 -4.1915927][-4.2514076 -4.292521 -4.2813339 -4.2225046 -4.1536055 -4.1179209 -4.1139536 -4.1079478 -4.0915976 -4.105288 -4.1305647 -4.1542931 -4.1787348 -4.1803765 -4.1791987][-4.2573261 -4.2956786 -4.2850204 -4.2308073 -4.1737423 -4.1475306 -4.1489215 -4.1496797 -4.1341395 -4.1412721 -4.1533747 -4.1624894 -4.1768427 -4.1780987 -4.1797233][-4.2665267 -4.2999315 -4.2923021 -4.2487326 -4.20565 -4.1817341 -4.1782126 -4.1823511 -4.1770916 -4.1824307 -4.1812725 -4.1757908 -4.1788082 -4.1772609 -4.1813226]]...]
INFO - root - 2017-12-07 23:25:29.867189: step 66010, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 53h:40m:50s remains)
INFO - root - 2017-12-07 23:25:36.713746: step 66020, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.733 sec/batch; 54h:14m:11s remains)
INFO - root - 2017-12-07 23:25:43.581640: step 66030, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 48h:58m:45s remains)
INFO - root - 2017-12-07 23:25:50.289146: step 66040, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 47h:47m:13s remains)
INFO - root - 2017-12-07 23:25:57.118139: step 66050, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 48h:49m:00s remains)
INFO - root - 2017-12-07 23:26:03.976936: step 66060, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 54h:38m:21s remains)
INFO - root - 2017-12-07 23:26:10.670487: step 66070, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 49h:43m:05s remains)
INFO - root - 2017-12-07 23:26:17.315662: step 66080, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 49h:56m:42s remains)
INFO - root - 2017-12-07 23:26:24.117660: step 66090, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 46h:10m:40s remains)
INFO - root - 2017-12-07 23:26:30.874326: step 66100, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 50h:00m:42s remains)
2017-12-07 23:26:31.645268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.233408 -4.2229962 -4.2238107 -4.233315 -4.24283 -4.246953 -4.2484975 -4.2539821 -4.2654114 -4.2796082 -4.2903209 -4.2960424 -4.2984633 -4.2990823 -4.2989507][-4.2429953 -4.2358632 -4.2351151 -4.240067 -4.2429476 -4.2408972 -4.2386842 -4.2436423 -4.2574205 -4.2754283 -4.289484 -4.2973008 -4.300415 -4.3007436 -4.3008695][-4.2512136 -4.245337 -4.2417932 -4.2404485 -4.2353196 -4.2263241 -4.2202845 -4.2242031 -4.2394724 -4.2602506 -4.2768722 -4.2881441 -4.2930636 -4.2931862 -4.2925944][-4.254663 -4.2483964 -4.2421312 -4.23473 -4.221189 -4.2041945 -4.1946678 -4.1984529 -4.2143216 -4.2352829 -4.2535477 -4.2693968 -4.2775578 -4.2773447 -4.2760577][-4.2568679 -4.2501907 -4.2414684 -4.2274661 -4.2029514 -4.1757388 -4.1647153 -4.1737113 -4.1929321 -4.2136908 -4.2324238 -4.2517958 -4.2624731 -4.2612157 -4.2569919][-4.2551222 -4.2452312 -4.2302642 -4.2044325 -4.1659532 -4.1304078 -4.12559 -4.147963 -4.1757178 -4.1995935 -4.218574 -4.2401719 -4.2506356 -4.2462468 -4.2357912][-4.2424774 -4.2242622 -4.1995416 -4.1616478 -4.1112108 -4.0725222 -4.0792441 -4.1156092 -4.1525593 -4.1804347 -4.2002721 -4.2222347 -4.2306871 -4.2215862 -4.2041111][-4.2066107 -4.17918 -4.1462913 -4.1015873 -4.0485249 -4.0150175 -4.0366988 -4.085536 -4.1322875 -4.1655593 -4.1867461 -4.2063594 -4.2083125 -4.1902575 -4.164011][-4.1533284 -4.124001 -4.0924892 -4.0529008 -4.0112176 -3.9938014 -4.0289869 -4.083981 -4.1356721 -4.172472 -4.1949596 -4.2092094 -4.1985388 -4.1665287 -4.1304283][-4.1204319 -4.10127 -4.0840564 -4.0620532 -4.0429068 -4.0439124 -4.0784817 -4.1242723 -4.1685772 -4.2023911 -4.2225766 -4.2268696 -4.2008038 -4.1544881 -4.11003][-4.134624 -4.1310816 -4.1291513 -4.1226506 -4.1190205 -4.1285214 -4.1548848 -4.1868224 -4.2173419 -4.2413621 -4.2545986 -4.2482114 -4.2097611 -4.1552591 -4.1099353][-4.1735883 -4.1823978 -4.1894073 -4.1900973 -4.1931815 -4.2037597 -4.2202449 -4.2366104 -4.2499404 -4.2607636 -4.2646656 -4.2518272 -4.2133875 -4.1650691 -4.1288929][-4.2064457 -4.2242351 -4.2340975 -4.2351542 -4.2372761 -4.2434998 -4.2493505 -4.248364 -4.2431097 -4.2408447 -4.2389112 -4.2298479 -4.2056251 -4.1767297 -4.1590266][-4.2264552 -4.2486587 -4.2574196 -4.2555971 -4.2529664 -4.2518539 -4.2471108 -4.2327714 -4.2140474 -4.204258 -4.2045536 -4.2076812 -4.2044821 -4.198977 -4.2008286][-4.2412448 -4.2605433 -4.2652621 -4.2600923 -4.2543468 -4.2484903 -4.2384191 -4.22046 -4.2012072 -4.1945863 -4.2016253 -4.2150254 -4.2248898 -4.2326641 -4.2443295]]...]
INFO - root - 2017-12-07 23:26:38.413350: step 66110, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 46h:33m:23s remains)
INFO - root - 2017-12-07 23:26:45.168305: step 66120, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 47h:45m:57s remains)
INFO - root - 2017-12-07 23:26:51.985441: step 66130, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 53h:08m:37s remains)
INFO - root - 2017-12-07 23:26:58.888798: step 66140, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 54h:56m:27s remains)
INFO - root - 2017-12-07 23:27:05.689899: step 66150, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 48h:09m:29s remains)
INFO - root - 2017-12-07 23:27:12.412359: step 66160, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 49h:48m:54s remains)
INFO - root - 2017-12-07 23:27:19.155006: step 66170, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 46h:55m:45s remains)
INFO - root - 2017-12-07 23:27:25.991570: step 66180, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 50h:35m:25s remains)
INFO - root - 2017-12-07 23:27:32.808257: step 66190, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.724 sec/batch; 53h:33m:23s remains)
INFO - root - 2017-12-07 23:27:39.414123: step 66200, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 47h:57m:23s remains)
2017-12-07 23:27:40.116142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3231807 -4.3207445 -4.32639 -4.329174 -4.325954 -4.3154025 -4.3005624 -4.2854595 -4.2765751 -4.2809882 -4.29831 -4.3249855 -4.3493266 -4.363533 -4.3659458][-4.3047218 -4.3073783 -4.3193779 -4.3275137 -4.3238907 -4.3051372 -4.279099 -4.2541361 -4.235661 -4.2336349 -4.2522688 -4.2904243 -4.3295569 -4.3550525 -4.36329][-4.2806797 -4.2924495 -4.3129816 -4.3275623 -4.3244195 -4.2995195 -4.2618961 -4.2239275 -4.1938386 -4.1819177 -4.1987581 -4.2464633 -4.2991319 -4.3377066 -4.35381][-4.2546682 -4.2749043 -4.3030448 -4.3230796 -4.3212438 -4.2917938 -4.2423239 -4.1915846 -4.1543937 -4.1402345 -4.1600566 -4.216 -4.2789192 -4.3255453 -4.3457355][-4.2281389 -4.2528992 -4.2841249 -4.3072124 -4.3054519 -4.26866 -4.2044187 -4.1426783 -4.1095786 -4.1073508 -4.1376905 -4.2021103 -4.2724695 -4.3224983 -4.3433805][-4.2049994 -4.23032 -4.2588348 -4.2790971 -4.2696366 -4.2199268 -4.139123 -4.0709019 -4.0547161 -4.0771255 -4.1257958 -4.2005196 -4.2763824 -4.3256254 -4.34441][-4.1955624 -4.2188473 -4.2403517 -4.249826 -4.2245517 -4.153698 -4.05201 -3.9730279 -3.9803994 -4.0398531 -4.1149874 -4.2043695 -4.2858052 -4.3314042 -4.3461976][-4.2113037 -4.2289844 -4.2425566 -4.2388053 -4.19189 -4.0921535 -3.9610529 -3.8602881 -3.8852363 -3.9847822 -4.0929432 -4.2010827 -4.2901673 -4.3350768 -4.3478212][-4.2538333 -4.2647862 -4.26986 -4.2537351 -4.1880355 -4.0627127 -3.9071379 -3.7848134 -3.8102257 -3.93163 -4.0656333 -4.1903949 -4.2867074 -4.3336854 -4.3467207][-4.3017869 -4.3093791 -4.308692 -4.2867413 -4.2148452 -4.0823908 -3.9212492 -3.7928882 -3.803324 -3.9176264 -4.05753 -4.1872187 -4.2827497 -4.329494 -4.3436079][-4.3356881 -4.3415394 -4.3397689 -4.3204513 -4.25757 -4.1380086 -3.9893508 -3.8700292 -3.8631194 -3.9537919 -4.0808325 -4.2006 -4.2866392 -4.328867 -4.3427615][-4.3513436 -4.3578973 -4.3588705 -4.3484764 -4.3038039 -4.2090907 -4.0853477 -3.9829314 -3.9639668 -4.0268564 -4.1296759 -4.2299147 -4.3012223 -4.3348112 -4.3457108][-4.3536568 -4.3623109 -4.3676577 -4.3658972 -4.341826 -4.2791991 -4.1881857 -4.1076422 -4.0830145 -4.1212616 -4.1949673 -4.2698126 -4.3231325 -4.345551 -4.3506989][-4.3544984 -4.3643565 -4.37242 -4.3755431 -4.3666964 -4.331862 -4.2736521 -4.2177725 -4.1963878 -4.2155108 -4.2610559 -4.31069 -4.3446283 -4.3552136 -4.3543591][-4.3557038 -4.3638654 -4.3708777 -4.3749495 -4.3727059 -4.356472 -4.3248653 -4.2922344 -4.2773213 -4.2855206 -4.3096738 -4.338079 -4.3559041 -4.3584723 -4.3541422]]...]
INFO - root - 2017-12-07 23:27:46.858091: step 66210, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 52h:21m:20s remains)
INFO - root - 2017-12-07 23:27:53.705862: step 66220, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.674 sec/batch; 49h:50m:37s remains)
INFO - root - 2017-12-07 23:28:00.492089: step 66230, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 48h:13m:48s remains)
INFO - root - 2017-12-07 23:28:07.185304: step 66240, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.618 sec/batch; 45h:42m:13s remains)
INFO - root - 2017-12-07 23:28:13.987009: step 66250, loss = 2.11, batch loss = 2.05 (12.1 examples/sec; 0.663 sec/batch; 48h:59m:55s remains)
INFO - root - 2017-12-07 23:28:20.845131: step 66260, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 53h:36m:16s remains)
INFO - root - 2017-12-07 23:28:27.670126: step 66270, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 53h:53m:33s remains)
INFO - root - 2017-12-07 23:28:34.449165: step 66280, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.668 sec/batch; 49h:23m:47s remains)
INFO - root - 2017-12-07 23:28:41.284548: step 66290, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 46h:22m:13s remains)
INFO - root - 2017-12-07 23:28:47.865281: step 66300, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 48h:05m:04s remains)
2017-12-07 23:28:48.633583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3465648 -4.3435326 -4.3383117 -4.3314581 -4.3255243 -4.3190436 -4.3131452 -4.3048716 -4.2884297 -4.260829 -4.2160687 -4.1825581 -4.1790996 -4.2021093 -4.2310052][-4.3488483 -4.3450336 -4.338305 -4.32983 -4.3245816 -4.3233886 -4.3249254 -4.323029 -4.3150115 -4.299499 -4.272635 -4.250452 -4.2432346 -4.2531261 -4.2642694][-4.3488913 -4.3450851 -4.3376245 -4.3277311 -4.3214355 -4.318903 -4.3204861 -4.3200951 -4.3168559 -4.3116283 -4.3005838 -4.2928715 -4.291214 -4.2934794 -4.2910461][-4.3432088 -4.3386774 -4.3299971 -4.31996 -4.3123322 -4.3048396 -4.3014464 -4.3008776 -4.29862 -4.2982244 -4.2955613 -4.2984972 -4.3045 -4.3084135 -4.3031793][-4.3362603 -4.3306322 -4.3213444 -4.31175 -4.3011923 -4.285398 -4.2735834 -4.2633615 -4.2542157 -4.2527189 -4.2533092 -4.2656922 -4.2831306 -4.2943177 -4.2951646][-4.3265815 -4.3201718 -4.313849 -4.3057833 -4.2889338 -4.2636275 -4.237442 -4.2078042 -4.1883211 -4.1886835 -4.2002525 -4.2229075 -4.2499266 -4.2695537 -4.2765036][-4.3100419 -4.3035312 -4.3010387 -4.2976742 -4.2799444 -4.2457609 -4.2041955 -4.1547184 -4.1263433 -4.1344051 -4.1624818 -4.1990652 -4.2357397 -4.2613821 -4.27162][-4.2920456 -4.2873931 -4.2878895 -4.288065 -4.2739978 -4.2418466 -4.1972585 -4.14236 -4.1129651 -4.1252 -4.1635933 -4.2063 -4.2464676 -4.2725892 -4.2824664][-4.2725592 -4.2694263 -4.2718296 -4.2750778 -4.2640314 -4.240726 -4.2095833 -4.173111 -4.1548409 -4.1661043 -4.1969128 -4.2317023 -4.26469 -4.2869682 -4.2940245][-4.2495089 -4.2375221 -4.237267 -4.2464952 -4.2473631 -4.2372651 -4.2246656 -4.2131763 -4.2083216 -4.2176051 -4.234118 -4.2523026 -4.2735553 -4.294487 -4.3058863][-4.2177811 -4.1863127 -4.1850324 -4.2098541 -4.2304697 -4.2388859 -4.2411027 -4.2425957 -4.2432008 -4.2473097 -4.2534795 -4.2608461 -4.2757316 -4.295331 -4.3111153][-4.1958814 -4.1514649 -4.1508746 -4.18828 -4.2271843 -4.2499328 -4.2606821 -4.2640257 -4.2577944 -4.2497268 -4.2464538 -4.2502522 -4.2639737 -4.2828393 -4.303421][-4.2103386 -4.1715245 -4.1688848 -4.2002616 -4.2385216 -4.2640414 -4.2729688 -4.2660632 -4.2471666 -4.2247448 -4.2073174 -4.2076874 -4.2249131 -4.2518635 -4.2822762][-4.2515249 -4.226541 -4.2211642 -4.2327614 -4.2534966 -4.2702889 -4.2699323 -4.2508521 -4.2162771 -4.1774268 -4.14448 -4.1451368 -4.1738749 -4.2148738 -4.254703][-4.2938538 -4.2746863 -4.2646489 -4.2599692 -4.2634125 -4.2673516 -4.2619004 -4.2384419 -4.1967559 -4.1455369 -4.1040497 -4.1070075 -4.14538 -4.1937714 -4.2357426]]...]
INFO - root - 2017-12-07 23:28:55.170755: step 66310, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 47h:03m:34s remains)
INFO - root - 2017-12-07 23:29:01.973116: step 66320, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 48h:33m:57s remains)
INFO - root - 2017-12-07 23:29:08.829010: step 66330, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 52h:19m:25s remains)
INFO - root - 2017-12-07 23:29:15.687236: step 66340, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.748 sec/batch; 55h:18m:57s remains)
INFO - root - 2017-12-07 23:29:22.327226: step 66350, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 47h:03m:28s remains)
INFO - root - 2017-12-07 23:29:29.038063: step 66360, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 46h:33m:24s remains)
INFO - root - 2017-12-07 23:29:35.911014: step 66370, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 50h:05m:29s remains)
INFO - root - 2017-12-07 23:29:42.647995: step 66380, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 53h:55m:08s remains)
INFO - root - 2017-12-07 23:29:49.420267: step 66390, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 51h:25m:17s remains)
INFO - root - 2017-12-07 23:29:56.064013: step 66400, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 50h:20m:13s remains)
2017-12-07 23:29:56.740593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.140667 -4.1456208 -4.1693764 -4.1876392 -4.190033 -4.1818871 -4.1802373 -4.1883345 -4.2003455 -4.2139874 -4.221735 -4.2235856 -4.2199039 -4.2141519 -4.204628][-4.1034527 -4.1123734 -4.1401711 -4.1620851 -4.169754 -4.1651249 -4.1666574 -4.1798873 -4.19614 -4.2105303 -4.2181458 -4.22243 -4.2239594 -4.2234058 -4.2155061][-4.0651073 -4.07684 -4.1093454 -4.1395235 -4.1557922 -4.1542354 -4.1573272 -4.170805 -4.1902237 -4.2045026 -4.2127376 -4.2188497 -4.2234845 -4.2277427 -4.2246141][-4.0248489 -4.03795 -4.0790577 -4.1259108 -4.15695 -4.159121 -4.1564288 -4.1572723 -4.1690869 -4.1809978 -4.1908145 -4.201438 -4.2119055 -4.2233815 -4.2285252][-4.009232 -4.018188 -4.0638061 -4.1208267 -4.1576996 -4.1598291 -4.1497312 -4.1346745 -4.1305375 -4.1375165 -4.1522908 -4.1705546 -4.1871719 -4.2060113 -4.2183914][-4.0361581 -4.0354505 -4.0766654 -4.1310124 -4.1583495 -4.1560521 -4.1389341 -4.1099553 -4.0908489 -4.090282 -4.1091032 -4.1346703 -4.1591625 -4.1865191 -4.2090392][-4.07834 -4.0694771 -4.1046324 -4.1509018 -4.1673441 -4.1609983 -4.1443524 -4.1099024 -4.0801263 -4.07054 -4.0876203 -4.1122727 -4.1388783 -4.1751428 -4.2104959][-4.1155043 -4.1009936 -4.1284833 -4.1641164 -4.1787138 -4.1739645 -4.1628556 -4.1349845 -4.1054244 -4.0939131 -4.1078997 -4.1282964 -4.1464057 -4.1754723 -4.2114787][-4.1450849 -4.12116 -4.1372166 -4.1646943 -4.1839256 -4.1910658 -4.1908531 -4.1779618 -4.1612549 -4.1519742 -4.1591163 -4.1725721 -4.1783509 -4.190752 -4.2136636][-4.1727138 -4.1406665 -4.1460648 -4.1681137 -4.1944036 -4.2141 -4.2272239 -4.2306108 -4.2239537 -4.2127724 -4.2103949 -4.2175722 -4.2181735 -4.2184234 -4.225565][-4.1942248 -4.1572728 -4.1545439 -4.1714764 -4.1988235 -4.2246375 -4.2469721 -4.2601681 -4.2582221 -4.2481852 -4.2425556 -4.24627 -4.245451 -4.2416959 -4.2402725][-4.2014132 -4.1642861 -4.1565514 -4.1682711 -4.1908226 -4.2140489 -4.237402 -4.2531333 -4.2575531 -4.2539783 -4.2489576 -4.2528858 -4.2549639 -4.2531743 -4.2518353][-4.2058368 -4.1708636 -4.1613116 -4.1676989 -4.1803975 -4.192359 -4.2069373 -4.2198753 -4.22659 -4.2314234 -4.2323089 -4.23851 -4.24305 -4.24684 -4.2517905][-4.2132635 -4.1819825 -4.1720109 -4.1776175 -4.1792216 -4.176331 -4.1742563 -4.1781869 -4.1846828 -4.1941676 -4.20262 -4.2110171 -4.217906 -4.2280164 -4.243196][-4.2193789 -4.1937566 -4.1840682 -4.1879807 -4.1822433 -4.1688962 -4.1561236 -4.1509128 -4.1532164 -4.16481 -4.1764975 -4.1846442 -4.1914454 -4.2056575 -4.2255564]]...]
INFO - root - 2017-12-07 23:30:03.609465: step 66410, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 54h:55m:37s remains)
INFO - root - 2017-12-07 23:30:10.413285: step 66420, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 50h:21m:47s remains)
INFO - root - 2017-12-07 23:30:17.212288: step 66430, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 48h:14m:09s remains)
INFO - root - 2017-12-07 23:30:23.915055: step 66440, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.612 sec/batch; 45h:15m:47s remains)
INFO - root - 2017-12-07 23:30:30.719635: step 66450, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 48h:40m:26s remains)
INFO - root - 2017-12-07 23:30:37.572441: step 66460, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.761 sec/batch; 56h:14m:57s remains)
INFO - root - 2017-12-07 23:30:44.488419: step 66470, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.759 sec/batch; 56h:06m:52s remains)
INFO - root - 2017-12-07 23:30:51.266866: step 66480, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 50h:09m:51s remains)
INFO - root - 2017-12-07 23:30:58.175802: step 66490, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 48h:23m:45s remains)
INFO - root - 2017-12-07 23:31:04.748959: step 66500, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 49h:56m:04s remains)
2017-12-07 23:31:05.591243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3144989 -4.3089252 -4.3086195 -4.3126793 -4.3181953 -4.3166862 -4.3069782 -4.3001394 -4.3032317 -4.3127642 -4.320416 -4.3240209 -4.3250055 -4.3219347 -4.3143477][-4.3307366 -4.3284154 -4.333149 -4.3404603 -4.3456516 -4.340344 -4.3236642 -4.3074703 -4.30355 -4.3128853 -4.3267531 -4.3385262 -4.3454914 -4.34605 -4.3421116][-4.3272467 -4.3240232 -4.328968 -4.3373628 -4.3430681 -4.3360343 -4.3165531 -4.296947 -4.2891893 -4.29807 -4.3156953 -4.3330622 -4.3451471 -4.3491898 -4.3474984][-4.3049855 -4.3015318 -4.3062816 -4.3137875 -4.318469 -4.3094225 -4.287993 -4.2685394 -4.2616882 -4.2729607 -4.2938814 -4.3156519 -4.3320818 -4.3378339 -4.3352008][-4.27552 -4.2726355 -4.2764182 -4.2813292 -4.282701 -4.2686725 -4.245615 -4.2288227 -4.2270656 -4.2426443 -4.2678232 -4.2941847 -4.3125563 -4.3175783 -4.3115559][-4.2541251 -4.2503581 -4.2484732 -4.2459722 -4.2403398 -4.2203703 -4.1982131 -4.1889586 -4.1968732 -4.2180662 -4.24697 -4.2767963 -4.2950244 -4.2974315 -4.2866154][-4.2398896 -4.2338052 -4.2242937 -4.2131748 -4.19903 -4.1725154 -4.1520958 -4.1505303 -4.1712227 -4.2040997 -4.2399521 -4.2719135 -4.2895479 -4.2891927 -4.2739153][-4.2259941 -4.218214 -4.2042203 -4.1846666 -4.1599 -4.1257 -4.1022854 -4.1057959 -4.1405416 -4.1910329 -4.2400408 -4.277534 -4.2955561 -4.2917066 -4.2720838][-4.2165427 -4.2019949 -4.1838045 -4.1575346 -4.1205688 -4.0746193 -4.0438333 -4.0485163 -4.0969939 -4.1677904 -4.2349143 -4.2814379 -4.3027377 -4.2975049 -4.275456][-4.2148271 -4.1879988 -4.1618128 -4.12791 -4.080905 -4.0234962 -3.9832921 -3.9845939 -4.0398211 -4.1244521 -4.2111797 -4.2727513 -4.3034649 -4.303134 -4.2839494][-4.2224116 -4.1868916 -4.1528411 -4.1134238 -4.0632157 -4.0041432 -3.9597273 -3.95194 -3.9989522 -4.0846343 -4.1802325 -4.2540889 -4.2960773 -4.3054047 -4.2945828][-4.2392335 -4.2037678 -4.1681013 -4.1302524 -4.0880084 -4.0385032 -3.9989119 -3.982404 -4.0098095 -4.0796661 -4.1651192 -4.2381773 -4.2845221 -4.3023949 -4.3013][-4.2704687 -4.2422156 -4.2082944 -4.1732249 -4.1389537 -4.1017404 -4.0735049 -4.0557032 -4.0685582 -4.1178155 -4.1822 -4.2391791 -4.2767043 -4.294529 -4.2989721][-4.2988944 -4.2837877 -4.25523 -4.2225084 -4.1930308 -4.1648078 -4.1459341 -4.1369209 -4.1489034 -4.1841993 -4.2245593 -4.2581921 -4.2770853 -4.2854233 -4.2884994][-4.3143215 -4.3106804 -4.2914891 -4.2628765 -4.2344952 -4.2107797 -4.1973572 -4.1997061 -4.2189441 -4.2463403 -4.2687163 -4.2826219 -4.2840686 -4.2786732 -4.2740355]]...]
INFO - root - 2017-12-07 23:31:12.265507: step 66510, loss = 2.10, batch loss = 2.04 (12.9 examples/sec; 0.622 sec/batch; 45h:57m:21s remains)
INFO - root - 2017-12-07 23:31:19.086756: step 66520, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 46h:52m:53s remains)
INFO - root - 2017-12-07 23:31:25.912759: step 66530, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 52h:31m:26s remains)
INFO - root - 2017-12-07 23:31:32.771414: step 66540, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 53h:17m:17s remains)
INFO - root - 2017-12-07 23:31:39.600855: step 66550, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 47h:04m:34s remains)
INFO - root - 2017-12-07 23:31:46.414936: step 66560, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 46h:40m:12s remains)
INFO - root - 2017-12-07 23:31:53.173815: step 66570, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 50h:06m:28s remains)
INFO - root - 2017-12-07 23:31:59.891803: step 66580, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 52h:05m:17s remains)
INFO - root - 2017-12-07 23:32:06.698067: step 66590, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 50h:26m:18s remains)
INFO - root - 2017-12-07 23:32:13.361171: step 66600, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 50h:28m:13s remains)
2017-12-07 23:32:14.106835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1025395 -4.1081929 -4.09222 -4.0601025 -4.0502086 -4.0841265 -4.1373444 -4.1679254 -4.1880174 -4.2078314 -4.2087369 -4.1905265 -4.1707544 -4.1639156 -4.1652184][-4.1365204 -4.1252651 -4.1031461 -4.0803809 -4.0900412 -4.1322942 -4.1737075 -4.1868391 -4.1888213 -4.1931319 -4.1886959 -4.1703968 -4.1475744 -4.1332517 -4.1262474][-4.1601644 -4.1459184 -4.1234665 -4.109077 -4.126152 -4.16188 -4.1860151 -4.1793475 -4.1625671 -4.15983 -4.159965 -4.1472144 -4.1316714 -4.1186533 -4.1083474][-4.1593156 -4.16167 -4.1496024 -4.1480441 -4.1633377 -4.1812081 -4.17424 -4.1364546 -4.0991936 -4.0936232 -4.1058059 -4.1144781 -4.1254244 -4.1276469 -4.1210151][-4.1460824 -4.1660175 -4.1643281 -4.1715264 -4.180438 -4.1770592 -4.1368947 -4.0601621 -3.9914513 -3.9920974 -4.0383072 -4.0887051 -4.1319222 -4.148222 -4.1451197][-4.1391311 -4.1707535 -4.1772184 -4.1839209 -4.1804738 -4.1507916 -4.0793958 -3.9569454 -3.8451972 -3.8770819 -3.9979532 -4.0957284 -4.1512246 -4.1673 -4.1645589][-4.1463809 -4.1770325 -4.18325 -4.1860838 -4.1720428 -4.1265354 -4.0345316 -3.8740487 -3.7385144 -3.8311429 -4.0149212 -4.1315141 -4.1795659 -4.1894941 -4.1819372][-4.1530938 -4.1748443 -4.1710539 -4.1690664 -4.1595078 -4.1214285 -4.0365515 -3.8996558 -3.81773 -3.920897 -4.0818849 -4.1710429 -4.2016535 -4.2074056 -4.1942954][-4.1492143 -4.1588316 -4.148108 -4.1471963 -4.1534381 -4.13823 -4.0834813 -4.0132141 -3.9876225 -4.0496521 -4.1421132 -4.1896343 -4.2029705 -4.2050085 -4.1912336][-4.1506214 -4.152597 -4.1391053 -4.1427622 -4.1593046 -4.1618528 -4.1386747 -4.1174645 -4.1108184 -4.1313992 -4.1712651 -4.1904426 -4.1922655 -4.1929622 -4.1841869][-4.1668139 -4.1634607 -4.1482038 -4.1472321 -4.1650238 -4.1790156 -4.1741133 -4.1703887 -4.1656094 -4.1623864 -4.1751232 -4.1820674 -4.1780553 -4.1815786 -4.185298][-4.1883554 -4.1822715 -4.1625457 -4.1480541 -4.1564584 -4.1760864 -4.179986 -4.1771679 -4.1745539 -4.1680293 -4.1639915 -4.1624746 -4.16028 -4.1712284 -4.1887841][-4.2161403 -4.2073927 -4.176981 -4.1448007 -4.1416683 -4.1641049 -4.1716886 -4.1683 -4.1728449 -4.1721411 -4.163806 -4.1536694 -4.149714 -4.1612492 -4.1854081][-4.2307711 -4.2198057 -4.1830788 -4.1476922 -4.1423006 -4.1638179 -4.1735091 -4.172647 -4.1799746 -4.1846957 -4.1739006 -4.1557813 -4.14557 -4.1517005 -4.1800494][-4.2281661 -4.2212338 -4.1903329 -4.1679149 -4.1667089 -4.1776543 -4.1814337 -4.175559 -4.178092 -4.182282 -4.1696272 -4.1495843 -4.137 -4.1423731 -4.1757064]]...]
INFO - root - 2017-12-07 23:32:20.981078: step 66610, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.743 sec/batch; 54h:54m:18s remains)
INFO - root - 2017-12-07 23:32:27.565323: step 66620, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 47h:19m:11s remains)
INFO - root - 2017-12-07 23:32:34.308524: step 66630, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 48h:37m:02s remains)
INFO - root - 2017-12-07 23:32:41.052963: step 66640, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 51h:24m:45s remains)
INFO - root - 2017-12-07 23:32:47.853999: step 66650, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 52h:45m:42s remains)
INFO - root - 2017-12-07 23:32:54.592173: step 66660, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 48h:17m:06s remains)
INFO - root - 2017-12-07 23:33:01.344764: step 66670, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 47h:42m:33s remains)
INFO - root - 2017-12-07 23:33:08.116743: step 66680, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 48h:16m:00s remains)
INFO - root - 2017-12-07 23:33:15.110054: step 66690, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 52h:20m:01s remains)
INFO - root - 2017-12-07 23:33:21.711354: step 66700, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 52h:15m:24s remains)
2017-12-07 23:33:22.533691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17907 -4.1934314 -4.2114491 -4.2257648 -4.2284517 -4.2094793 -4.1747127 -4.1340556 -4.1113844 -4.1126418 -4.1314807 -4.1609063 -4.1777768 -4.1829004 -4.1810069][-4.2256484 -4.2340536 -4.2460427 -4.2544794 -4.2526536 -4.2326732 -4.1975222 -4.1601796 -4.13886 -4.140523 -4.1557245 -4.1746454 -4.1824832 -4.1839519 -4.1826129][-4.25284 -4.2621889 -4.2725143 -4.2722335 -4.2605281 -4.2377629 -4.2056136 -4.17705 -4.1646276 -4.1715994 -4.1878781 -4.1955981 -4.1882596 -4.1833429 -4.1934962][-4.2467966 -4.2556453 -4.2648969 -4.2610593 -4.2433176 -4.2202191 -4.1970105 -4.1811557 -4.177422 -4.1845088 -4.197331 -4.1938753 -4.1721516 -4.1610417 -4.1843963][-4.2150993 -4.2203145 -4.2334995 -4.2345605 -4.2169251 -4.1943293 -4.1759491 -4.1687417 -4.1753631 -4.1864309 -4.1971741 -4.1875381 -4.157156 -4.1402788 -4.1631489][-4.1747761 -4.1755214 -4.1925287 -4.2037344 -4.1908607 -4.168695 -4.1496449 -4.1435518 -4.1525478 -4.1673245 -4.1826591 -4.17913 -4.1520505 -4.1330514 -4.1456804][-4.1498289 -4.1476426 -4.1650681 -4.1787863 -4.1713452 -4.1567235 -4.1470757 -4.1449747 -4.1512351 -4.1619034 -4.1751418 -4.173624 -4.1522808 -4.1323848 -4.1354618][-4.16168 -4.1546535 -4.1657724 -4.1748085 -4.1698761 -4.1630445 -4.166389 -4.1724329 -4.1759262 -4.1766267 -4.1809435 -4.1803861 -4.1643591 -4.141747 -4.1325707][-4.2091093 -4.193985 -4.1920886 -4.1922336 -4.1862473 -4.185802 -4.2001424 -4.212636 -4.2122841 -4.2048221 -4.2005944 -4.1995978 -4.1867237 -4.1624804 -4.1381907][-4.259285 -4.2398467 -4.2258611 -4.220233 -4.216413 -4.2193451 -4.2388344 -4.2545362 -4.2572007 -4.2502251 -4.2412815 -4.2327862 -4.2150435 -4.1866579 -4.1537666][-4.2756667 -4.2568116 -4.2396703 -4.2380967 -4.2386374 -4.2415357 -4.2621336 -4.2808795 -4.2873812 -4.2844963 -4.2735043 -4.2571936 -4.2318134 -4.2040148 -4.1757083][-4.2533774 -4.2373486 -4.2261806 -4.2330885 -4.239337 -4.2444415 -4.2664189 -4.28543 -4.2922544 -4.2898254 -4.2778788 -4.2596574 -4.234304 -4.2133102 -4.1961479][-4.2077322 -4.1894279 -4.1827669 -4.1966271 -4.2135954 -4.2306685 -4.2582965 -4.2744689 -4.2773008 -4.2734323 -4.2613416 -4.245163 -4.2275162 -4.2161293 -4.2073274][-4.152987 -4.1271524 -4.1177554 -4.1341734 -4.1586366 -4.1888876 -4.2237678 -4.2407222 -4.2418122 -4.2431755 -4.2385373 -4.2304406 -4.2208233 -4.217329 -4.2131968][-4.1162829 -4.0816975 -4.0626664 -4.0713086 -4.0969868 -4.1368818 -4.1768308 -4.1966796 -4.2029476 -4.21274 -4.2165022 -4.2165556 -4.2142482 -4.2171488 -4.216538]]...]
INFO - root - 2017-12-07 23:33:29.312892: step 66710, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 50h:54m:08s remains)
INFO - root - 2017-12-07 23:33:36.087709: step 66720, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 50h:29m:06s remains)
INFO - root - 2017-12-07 23:33:42.847907: step 66730, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 51h:20m:19s remains)
INFO - root - 2017-12-07 23:33:49.562715: step 66740, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 49h:50m:37s remains)
INFO - root - 2017-12-07 23:33:56.250585: step 66750, loss = 2.10, batch loss = 2.04 (12.9 examples/sec; 0.622 sec/batch; 45h:54m:22s remains)
INFO - root - 2017-12-07 23:34:03.100572: step 66760, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 52h:17m:55s remains)
INFO - root - 2017-12-07 23:34:09.872943: step 66770, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 51h:17m:49s remains)
INFO - root - 2017-12-07 23:34:16.643797: step 66780, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 48h:51m:46s remains)
INFO - root - 2017-12-07 23:34:23.456853: step 66790, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 49h:14m:47s remains)
INFO - root - 2017-12-07 23:34:30.036191: step 66800, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 48h:59m:02s remains)
2017-12-07 23:34:30.734616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3386579 -4.3279557 -4.3198795 -4.3210983 -4.32382 -4.3266954 -4.3209529 -4.3147516 -4.3019366 -4.2894387 -4.2793045 -4.2705693 -4.2631936 -4.2497516 -4.2472863][-4.3252864 -4.3156037 -4.3084435 -4.3087945 -4.3082471 -4.3074837 -4.3035955 -4.3039961 -4.2953458 -4.2833066 -4.2718606 -4.2618 -4.25554 -4.2422781 -4.2403822][-4.3067684 -4.2973957 -4.2915478 -4.2880707 -4.2792 -4.2709756 -4.2703595 -4.2801304 -4.2814465 -4.2778106 -4.2695432 -4.2588744 -4.2538247 -4.2426682 -4.2412977][-4.2927547 -4.2801757 -4.2697887 -4.2599788 -4.2439718 -4.22954 -4.2321138 -4.2527714 -4.27182 -4.280632 -4.2754574 -4.2620964 -4.2576313 -4.2476192 -4.245194][-4.2760453 -4.2598586 -4.2435279 -4.2282152 -4.2059851 -4.1821551 -4.178174 -4.2041874 -4.2441139 -4.2678385 -4.2675471 -4.2541614 -4.2503934 -4.242938 -4.2438903][-4.2439933 -4.222703 -4.1998415 -4.1800032 -4.1515403 -4.1126208 -4.0892081 -4.1126719 -4.1723247 -4.2151194 -4.2260456 -4.2192845 -4.2202926 -4.222918 -4.2333808][-4.1968651 -4.1653242 -4.1329508 -4.1057954 -4.0718622 -4.0147486 -3.9624958 -3.9778781 -4.0579014 -4.1228132 -4.1523685 -4.1612072 -4.17478 -4.1945581 -4.2184811][-4.1582704 -4.1184144 -4.0788455 -4.0435834 -4.0045757 -3.9357493 -3.8633027 -3.8711574 -3.967308 -4.0490475 -4.0938416 -4.1179647 -4.1436949 -4.1753874 -4.208385][-4.1566486 -4.1192112 -4.0831971 -4.0493541 -4.0155435 -3.9628284 -3.9100308 -3.9152603 -3.9914317 -4.0604973 -4.1031094 -4.1300063 -4.1563478 -4.1862373 -4.2167926][-4.190176 -4.16475 -4.139945 -4.1161852 -4.0942178 -4.0592937 -4.0248923 -4.0225315 -4.0706658 -4.1200132 -4.1555643 -4.1798315 -4.1995521 -4.2206507 -4.243093][-4.2326188 -4.2197847 -4.2072124 -4.1935906 -4.1803794 -4.1576686 -4.1327105 -4.1264305 -4.1550574 -4.1898189 -4.2176614 -4.237793 -4.2513294 -4.2646437 -4.2787442][-4.2594748 -4.2562275 -4.2497253 -4.2420263 -4.2346144 -4.2226305 -4.2080984 -4.2027378 -4.219852 -4.2443137 -4.2653351 -4.2799373 -4.2894673 -4.2983246 -4.3069458][-4.259625 -4.2596436 -4.2597132 -4.2606163 -4.2612576 -4.2592754 -4.2538481 -4.25062 -4.2589121 -4.2746096 -4.2903094 -4.3012853 -4.309804 -4.3175826 -4.3242121][-4.259151 -4.2616692 -4.2679086 -4.2755852 -4.2820945 -4.2855763 -4.2863097 -4.285007 -4.2878385 -4.2962651 -4.3062668 -4.315093 -4.323256 -4.3306518 -4.336832][-4.28481 -4.2878518 -4.2945561 -4.3020954 -4.3084927 -4.312016 -4.3132267 -4.3130045 -4.3137193 -4.3171115 -4.3226128 -4.3283525 -4.3339334 -4.3395147 -4.3446584]]...]
INFO - root - 2017-12-07 23:34:37.584666: step 66810, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 53h:14m:35s remains)
INFO - root - 2017-12-07 23:34:44.331783: step 66820, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 49h:15m:19s remains)
INFO - root - 2017-12-07 23:34:51.071466: step 66830, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 46h:05m:20s remains)
INFO - root - 2017-12-07 23:34:57.873817: step 66840, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 49h:34m:31s remains)
INFO - root - 2017-12-07 23:35:04.817323: step 66850, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 54h:05m:24s remains)
INFO - root - 2017-12-07 23:35:11.608240: step 66860, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 51h:11m:52s remains)
INFO - root - 2017-12-07 23:35:18.382220: step 66870, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 47h:21m:46s remains)
INFO - root - 2017-12-07 23:35:25.140226: step 66880, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 48h:30m:46s remains)
INFO - root - 2017-12-07 23:35:32.073595: step 66890, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.752 sec/batch; 55h:30m:00s remains)
INFO - root - 2017-12-07 23:35:38.578923: step 66900, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 51h:51m:27s remains)
2017-12-07 23:35:39.302822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1893744 -4.1821332 -4.1637478 -4.156003 -4.1748486 -4.2046113 -4.2054315 -4.1904421 -4.2086334 -4.2059922 -4.1787114 -4.169158 -4.1874757 -4.2206922 -4.2460227][-4.1886606 -4.186883 -4.17092 -4.1640396 -4.1711445 -4.1860828 -4.180644 -4.1633911 -4.1881876 -4.195714 -4.1701603 -4.1598392 -4.1795115 -4.2147155 -4.2424417][-4.1941361 -4.2040129 -4.196331 -4.1925125 -4.1883593 -4.1828074 -4.1660752 -4.1471949 -4.1765485 -4.1940532 -4.1749668 -4.1668873 -4.1857429 -4.2197366 -4.2461085][-4.2102418 -4.2300129 -4.2295461 -4.22417 -4.2098837 -4.1820555 -4.1502242 -4.1340566 -4.1738319 -4.1960068 -4.1842818 -4.1775231 -4.1941795 -4.2246666 -4.2472782][-4.218945 -4.2373819 -4.2387395 -4.2355356 -4.2122068 -4.1607466 -4.1013989 -4.0835896 -4.1402936 -4.1740704 -4.1775618 -4.1764607 -4.1922879 -4.2222247 -4.24357][-4.2107167 -4.2263665 -4.2301354 -4.2263265 -4.1956863 -4.1238928 -4.0320354 -4.0096173 -4.104805 -4.1646204 -4.1836224 -4.1878419 -4.2017636 -4.22968 -4.2464004][-4.2036514 -4.217123 -4.2165504 -4.2076173 -4.1657486 -4.0697536 -3.9338441 -3.8832088 -4.0260143 -4.128191 -4.1732812 -4.1906247 -4.2043214 -4.2285252 -4.2448263][-4.2095561 -4.2169957 -4.2090416 -4.195 -4.1494508 -4.0371366 -3.8619294 -3.7581356 -3.9239669 -4.0707593 -4.1450977 -4.1803441 -4.1961851 -4.2177792 -4.2341022][-4.230545 -4.2344766 -4.2269826 -4.2205782 -4.1868129 -4.0974159 -3.9585509 -3.8570087 -3.96694 -4.0949173 -4.1658993 -4.2022228 -4.2142181 -4.2230515 -4.2319989][-4.2446012 -4.2492437 -4.2475495 -4.2521849 -4.2321434 -4.1728258 -4.0770893 -3.9967299 -4.0431314 -4.1211419 -4.1735716 -4.2109227 -4.22897 -4.2312174 -4.2358704][-4.2580056 -4.2585688 -4.2561107 -4.2649522 -4.2558007 -4.2209516 -4.1556616 -4.098175 -4.1160588 -4.1611862 -4.1977725 -4.2311835 -4.2527895 -4.2503996 -4.2451515][-4.2653947 -4.25714 -4.2510991 -4.2587643 -4.26318 -4.2517953 -4.2152042 -4.1807885 -4.1941977 -4.2165809 -4.2329473 -4.2560263 -4.273613 -4.2688537 -4.2544689][-4.2681813 -4.2533097 -4.2396789 -4.2423096 -4.2495928 -4.2491097 -4.2322826 -4.2137027 -4.2238879 -4.2334552 -4.2366691 -4.2519693 -4.2661185 -4.2628565 -4.2514086][-4.2622576 -4.2457962 -4.2307177 -4.2300777 -4.2354164 -4.2381468 -4.2297192 -4.2201605 -4.2281332 -4.2281017 -4.2238822 -4.2336497 -4.2465796 -4.2476768 -4.2446613][-4.2529268 -4.2407351 -4.2300282 -4.2277985 -4.2283163 -4.2321286 -4.2295914 -4.2248664 -4.2290268 -4.2226458 -4.2125659 -4.2157984 -4.2285972 -4.2370629 -4.24352]]...]
INFO - root - 2017-12-07 23:35:46.162984: step 66910, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 51h:33m:07s remains)
INFO - root - 2017-12-07 23:35:52.958845: step 66920, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 51h:16m:53s remains)
INFO - root - 2017-12-07 23:35:59.462794: step 66930, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 48h:17m:14s remains)
INFO - root - 2017-12-07 23:36:06.298116: step 66940, loss = 2.06, batch loss = 2.01 (13.0 examples/sec; 0.615 sec/batch; 45h:23m:49s remains)
INFO - root - 2017-12-07 23:36:13.241144: step 66950, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 53h:21m:08s remains)
INFO - root - 2017-12-07 23:36:20.138157: step 66960, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.714 sec/batch; 52h:38m:51s remains)
INFO - root - 2017-12-07 23:36:26.905214: step 66970, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.681 sec/batch; 50h:12m:24s remains)
INFO - root - 2017-12-07 23:36:33.680953: step 66980, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 48h:51m:13s remains)
INFO - root - 2017-12-07 23:36:40.453209: step 66990, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 47h:38m:09s remains)
INFO - root - 2017-12-07 23:36:47.109734: step 67000, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.714 sec/batch; 52h:39m:03s remains)
2017-12-07 23:36:47.878573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.330833 -4.3285089 -4.3222475 -4.3141012 -4.310256 -4.3100982 -4.3078394 -4.3021564 -4.2918997 -4.2778578 -4.2674103 -4.2655864 -4.2675962 -4.2724037 -4.2803493][-4.3377032 -4.3374166 -4.3315296 -4.3213029 -4.3130684 -4.3082652 -4.3012142 -4.2948084 -4.2865906 -4.2760496 -4.2685909 -4.2671194 -4.2677193 -4.2718477 -4.2792897][-4.3388672 -4.3361111 -4.3261652 -4.3112183 -4.2977576 -4.2869363 -4.2758651 -4.2712984 -4.2682033 -4.2641692 -4.2635055 -4.2654176 -4.2647471 -4.2661839 -4.2683573][-4.3367686 -4.32924 -4.3121681 -4.2887912 -4.2657208 -4.2459831 -4.2325144 -4.2331882 -4.2374792 -4.2407489 -4.2472157 -4.2548161 -4.2549515 -4.2518172 -4.247364][-4.33482 -4.3214388 -4.2968869 -4.2644148 -4.2298956 -4.1999683 -4.1847734 -4.1929364 -4.2067361 -4.2163715 -4.2275858 -4.2407684 -4.2459226 -4.2441683 -4.2389574][-4.3334889 -4.3155336 -4.2816687 -4.2369719 -4.1879392 -4.1464448 -4.1298823 -4.1472716 -4.1730604 -4.1903076 -4.2079248 -4.2283268 -4.2435932 -4.2491016 -4.2471843][-4.3330951 -4.3113112 -4.2664175 -4.2057948 -4.1378131 -4.0800819 -4.0585918 -4.085165 -4.1246929 -4.1524243 -4.1788669 -4.2083063 -4.2325258 -4.2450442 -4.2496481][-4.3335605 -4.3080993 -4.2535281 -4.1795931 -4.0963259 -4.0227385 -3.9912324 -4.0208197 -4.072331 -4.1103444 -4.1458745 -4.181592 -4.2103257 -4.227037 -4.2368774][-4.3320541 -4.3048449 -4.2468486 -4.1686182 -4.0812516 -4.0028334 -3.9643455 -3.9892111 -4.0406556 -4.0819626 -4.1205564 -4.1558208 -4.1832461 -4.2008028 -4.2127833][-4.3306336 -4.3036537 -4.2482781 -4.1782146 -4.104826 -4.042685 -4.0105357 -4.0252676 -4.0593991 -4.0839787 -4.1085334 -4.13605 -4.1600437 -4.1770349 -4.1896582][-4.3303385 -4.3040524 -4.2546 -4.1982665 -4.1478491 -4.1099353 -4.0902076 -4.0958891 -4.1080217 -4.1084986 -4.1102471 -4.1227117 -4.139585 -4.1563187 -4.1703358][-4.3301558 -4.3072271 -4.2664566 -4.2230759 -4.1914344 -4.1725988 -4.1645594 -4.1674585 -4.1669397 -4.1514387 -4.1352749 -4.1297874 -4.1350818 -4.1479492 -4.162734][-4.331717 -4.3148994 -4.2846031 -4.2532396 -4.2343359 -4.2285166 -4.2304039 -4.2352104 -4.2307215 -4.2111616 -4.1898794 -4.1763391 -4.1735477 -4.1789827 -4.1870532][-4.3342457 -4.322937 -4.3014579 -4.2795906 -4.26875 -4.26997 -4.2773905 -4.2864046 -4.2849741 -4.2715964 -4.2575369 -4.2467523 -4.2401104 -4.2379656 -4.2368064][-4.3358603 -4.328475 -4.3147631 -4.3017735 -4.2975397 -4.3016472 -4.3092318 -4.3193893 -4.3231058 -4.3194876 -4.3169379 -4.3117704 -4.3022728 -4.2945995 -4.2873683]]...]
INFO - root - 2017-12-07 23:36:54.540741: step 67010, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 46h:05m:39s remains)
INFO - root - 2017-12-07 23:37:01.317799: step 67020, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 46h:53m:43s remains)
INFO - root - 2017-12-07 23:37:08.169077: step 67030, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 54h:30m:02s remains)
INFO - root - 2017-12-07 23:37:14.966886: step 67040, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.687 sec/batch; 50h:37m:21s remains)
INFO - root - 2017-12-07 23:37:21.632612: step 67050, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 48h:28m:09s remains)
INFO - root - 2017-12-07 23:37:28.471991: step 67060, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 46h:14m:11s remains)
INFO - root - 2017-12-07 23:37:35.285082: step 67070, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 47h:06m:56s remains)
INFO - root - 2017-12-07 23:37:42.165000: step 67080, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 54h:23m:16s remains)
INFO - root - 2017-12-07 23:37:48.966908: step 67090, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 52h:38m:07s remains)
INFO - root - 2017-12-07 23:37:55.562189: step 67100, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 48h:09m:15s remains)
2017-12-07 23:37:56.344191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1898155 -4.1533437 -4.1191559 -4.08639 -4.0674996 -4.0777025 -4.1186781 -4.1607523 -4.185616 -4.1942639 -4.1778049 -4.1353793 -4.0904937 -4.0636644 -4.0566769][-4.2046809 -4.168736 -4.1341295 -4.1069722 -4.0891809 -4.0924692 -4.1228175 -4.1583352 -4.18027 -4.1921654 -4.1884742 -4.1533217 -4.1081538 -4.0686426 -4.04195][-4.2051463 -4.1787734 -4.1506405 -4.1274266 -4.101861 -4.0854549 -4.0941458 -4.1233892 -4.1495667 -4.16932 -4.1812248 -4.163085 -4.1311307 -4.0960817 -4.0581632][-4.1922655 -4.1730618 -4.1498518 -4.1248159 -4.0916839 -4.0571804 -4.0440211 -4.0670829 -4.101593 -4.1328311 -4.1591234 -4.1612358 -4.1474681 -4.125042 -4.0941844][-4.1749978 -4.16068 -4.1421127 -4.1149373 -4.0760541 -4.0296597 -3.99333 -3.9980087 -4.0346932 -4.0817056 -4.1220765 -4.1401772 -4.142292 -4.1369529 -4.12286][-4.1548905 -4.1532612 -4.1424594 -4.1132021 -4.07307 -4.0194831 -3.9567895 -3.9288652 -3.9547586 -4.0136642 -4.0704989 -4.1062441 -4.1242027 -4.1340561 -4.1352253][-4.1169038 -4.1347923 -4.1371412 -4.1167612 -4.085423 -4.0368395 -3.9606464 -3.897145 -3.8940084 -3.9464011 -4.0197506 -4.0778055 -4.1097832 -4.12981 -4.1401262][-4.0684481 -4.1052346 -4.1259656 -4.12052 -4.1036334 -4.0677843 -3.9970279 -3.9189734 -3.8888166 -3.9215999 -3.995975 -4.0681424 -4.105793 -4.1267939 -4.1380892][-4.0391464 -4.0825849 -4.1147609 -4.1217651 -4.1160793 -4.0892043 -4.0323009 -3.9652545 -3.934135 -3.9527869 -4.0092115 -4.0726357 -4.106329 -4.1225252 -4.1336894][-4.0435882 -4.0794621 -4.1114407 -4.1269908 -4.1287913 -4.1078033 -4.0657787 -4.0196567 -4.0002346 -4.0149131 -4.05411 -4.097188 -4.12052 -4.1294155 -4.1379242][-4.0747466 -4.1021943 -4.1251497 -4.1401668 -4.1464043 -4.1309237 -4.1021118 -4.0738969 -4.063139 -4.0754538 -4.1033883 -4.1317577 -4.1466107 -4.1494589 -4.1553831][-4.1173167 -4.1395321 -4.15234 -4.1608028 -4.1657453 -4.1574259 -4.1430745 -4.1301508 -4.1249418 -4.1334195 -4.1519523 -4.1697574 -4.1793547 -4.18357 -4.1928191][-4.1669908 -4.1819038 -4.19062 -4.19514 -4.19668 -4.191462 -4.1854935 -4.18246 -4.1824212 -4.1876307 -4.1996636 -4.2129722 -4.2223711 -4.2291007 -4.2383466][-4.2155128 -4.2229662 -4.2311273 -4.2359328 -4.2359118 -4.23242 -4.2295 -4.2301035 -4.2317724 -4.2335868 -4.24143 -4.2532749 -4.2635274 -4.2697821 -4.276227][-4.2631631 -4.2652764 -4.2708282 -4.2736897 -4.2717624 -4.2686524 -4.2668991 -4.267766 -4.2698355 -4.2724423 -4.2782941 -4.2875838 -4.2954783 -4.3000169 -4.3043151]]...]
INFO - root - 2017-12-07 23:38:03.090792: step 67110, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 53h:34m:39s remains)
INFO - root - 2017-12-07 23:38:09.980364: step 67120, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 51h:18m:57s remains)
INFO - root - 2017-12-07 23:38:16.852922: step 67130, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 46h:28m:54s remains)
INFO - root - 2017-12-07 23:38:23.848176: step 67140, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.717 sec/batch; 52h:50m:40s remains)
INFO - root - 2017-12-07 23:38:30.645421: step 67150, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 53h:46m:09s remains)
INFO - root - 2017-12-07 23:38:37.375822: step 67160, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 48h:48m:18s remains)
INFO - root - 2017-12-07 23:38:44.223206: step 67170, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 48h:44m:30s remains)
INFO - root - 2017-12-07 23:38:51.028174: step 67180, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 49h:44m:27s remains)
INFO - root - 2017-12-07 23:38:57.861222: step 67190, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 52h:17m:42s remains)
INFO - root - 2017-12-07 23:39:04.494530: step 67200, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 52h:16m:29s remains)
2017-12-07 23:39:05.324299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.213119 -4.2251205 -4.2222824 -4.2117648 -4.1994786 -4.1821909 -4.1654882 -4.15975 -4.1660762 -4.1809168 -4.2064462 -4.2373781 -4.2678223 -4.2887774 -4.2929254][-4.1965113 -4.2098975 -4.2052722 -4.1864481 -4.1668649 -4.1455221 -4.131381 -4.1327815 -4.144063 -4.1601844 -4.1833348 -4.2171078 -4.2538714 -4.2804546 -4.2885137][-4.1898861 -4.2047873 -4.198482 -4.1715961 -4.1443415 -4.1193419 -4.1060314 -4.1106477 -4.1270676 -4.1432886 -4.1641655 -4.1994114 -4.2399392 -4.2695913 -4.2816095][-4.1845026 -4.20062 -4.1960154 -4.1646938 -4.1328192 -4.1064229 -4.0934849 -4.0985165 -4.1199117 -4.1385894 -4.1572876 -4.1904793 -4.2294083 -4.257791 -4.2718849][-4.1723814 -4.1884665 -4.1859627 -4.1545506 -4.1239386 -4.1011138 -4.0901728 -4.098968 -4.1255627 -4.1448755 -4.1617556 -4.18911 -4.2223411 -4.246912 -4.2607722][-4.1620612 -4.1779723 -4.174799 -4.1429982 -4.1154222 -4.091464 -4.0780172 -4.09536 -4.1298327 -4.1513262 -4.1655788 -4.1880322 -4.217113 -4.238133 -4.2515469][-4.1781712 -4.1867456 -4.176322 -4.1394334 -4.1036267 -4.0667071 -4.0482306 -4.0766993 -4.1226292 -4.1515055 -4.1660562 -4.1875792 -4.2145057 -4.2336287 -4.2476125][-4.2122917 -4.2081738 -4.1879406 -4.1468029 -4.0991197 -4.048985 -4.0288267 -4.0659103 -4.1199861 -4.157599 -4.1744423 -4.193078 -4.2145467 -4.2304792 -4.2445459][-4.2352967 -4.2215405 -4.1959357 -4.1598287 -4.11816 -4.0783191 -4.06748 -4.1042004 -4.1531453 -4.1894569 -4.2050285 -4.2161808 -4.226409 -4.2331934 -4.24253][-4.2407289 -4.2243614 -4.2003655 -4.1732125 -4.14414 -4.1239891 -4.1287537 -4.1625514 -4.1987576 -4.2249064 -4.2346888 -4.2367306 -4.2376146 -4.2348714 -4.2368531][-4.2398276 -4.2253275 -4.205534 -4.1838241 -4.1616964 -4.1507907 -4.1619706 -4.1957984 -4.2258887 -4.242301 -4.2461505 -4.2446542 -4.2417622 -4.2354169 -4.2340665][-4.2436743 -4.233871 -4.218719 -4.1998019 -4.1774616 -4.1640306 -4.1725626 -4.2048721 -4.23237 -4.2477369 -4.2532992 -4.2539573 -4.2526207 -4.2483721 -4.2474051][-4.2451706 -4.2420378 -4.2342343 -4.2196484 -4.1956849 -4.1768951 -4.1816936 -4.2136822 -4.2425814 -4.2609415 -4.2707248 -4.2739353 -4.2746024 -4.2736 -4.2746024][-4.2406354 -4.2449641 -4.2457619 -4.2422042 -4.228332 -4.2151089 -4.2205095 -4.2482352 -4.2718878 -4.2861805 -4.2948818 -4.2992148 -4.3021374 -4.3042417 -4.3075514][-4.2402492 -4.2530723 -4.2622519 -4.2683682 -4.2675328 -4.2653465 -4.2747326 -4.2962337 -4.3117371 -4.3182693 -4.3230853 -4.3268881 -4.3300877 -4.3327036 -4.3361025]]...]
INFO - root - 2017-12-07 23:39:12.163029: step 67210, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.675 sec/batch; 49h:45m:43s remains)
INFO - root - 2017-12-07 23:39:19.004536: step 67220, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 55h:11m:53s remains)
INFO - root - 2017-12-07 23:39:25.857797: step 67230, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 45h:46m:00s remains)
INFO - root - 2017-12-07 23:39:32.447155: step 67240, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 46h:31m:31s remains)
INFO - root - 2017-12-07 23:39:39.273470: step 67250, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 51h:41m:17s remains)
INFO - root - 2017-12-07 23:39:46.185848: step 67260, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 54h:22m:21s remains)
INFO - root - 2017-12-07 23:39:52.944983: step 67270, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 49h:48m:21s remains)
INFO - root - 2017-12-07 23:39:59.845976: step 67280, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 51h:03m:18s remains)
INFO - root - 2017-12-07 23:40:06.729282: step 67290, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 47h:53m:19s remains)
INFO - root - 2017-12-07 23:40:13.360948: step 67300, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.734 sec/batch; 54h:02m:21s remains)
2017-12-07 23:40:14.084591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1690769 -4.1717982 -4.1557012 -4.141427 -4.1487679 -4.1713858 -4.1887507 -4.1813803 -4.1660233 -4.148437 -4.1564856 -4.1560674 -4.1584673 -4.1782994 -4.2110648][-4.0960155 -4.0910707 -4.0764503 -4.0709734 -4.0975523 -4.13769 -4.1617484 -4.1538196 -4.1410227 -4.1298656 -4.1378584 -4.1387453 -4.1405768 -4.1581 -4.1940989][-4.0510368 -4.0456934 -4.0363178 -4.0377879 -4.0866704 -4.1449742 -4.1689081 -4.1534486 -4.1334171 -4.1316137 -4.1520324 -4.1615343 -4.1669312 -4.1861176 -4.2167392][-4.0659876 -4.0623274 -4.0512042 -4.0462189 -4.0931792 -4.152216 -4.1694136 -4.1409726 -4.1058826 -4.1130614 -4.1543245 -4.1786838 -4.1905818 -4.2170014 -4.2451558][-4.1129937 -4.1162996 -4.1023903 -4.0819392 -4.0968108 -4.1172495 -4.1059089 -4.0540328 -4.0068617 -4.0289168 -4.1012592 -4.1478024 -4.1727328 -4.210947 -4.2471185][-4.1467404 -4.1507192 -4.1310239 -4.0920777 -4.067997 -4.0398159 -3.9813035 -3.8911324 -3.830734 -3.8813603 -3.9899828 -4.0635161 -4.1129065 -4.17159 -4.22243][-4.1507082 -4.1572285 -4.134923 -4.0832553 -4.035409 -3.9783297 -3.8864722 -3.7598581 -3.6905861 -3.7691374 -3.9031205 -3.9917276 -4.0574288 -4.1322036 -4.1960812][-4.1566896 -4.1707673 -4.155591 -4.1128125 -4.0696583 -4.0170889 -3.9316254 -3.8100107 -3.753176 -3.8344631 -3.9522002 -4.0310168 -4.08634 -4.1487231 -4.2008224][-4.1747684 -4.1979356 -4.1975594 -4.1735497 -4.1502285 -4.1160116 -4.05563 -3.967608 -3.9314346 -3.9920349 -4.0702953 -4.1229019 -4.1583633 -4.19797 -4.2298679][-4.1834593 -4.2097082 -4.2240767 -4.219501 -4.2134714 -4.1994305 -4.1657004 -4.1126895 -4.0916448 -4.126956 -4.1696014 -4.200376 -4.2213225 -4.2437687 -4.2601624][-4.1989312 -4.2259197 -4.2457681 -4.2502942 -4.2528486 -4.2520504 -4.23945 -4.2118011 -4.2011089 -4.218574 -4.2385349 -4.2558622 -4.2679057 -4.2780333 -4.2843409][-4.224122 -4.246139 -4.262681 -4.2688093 -4.27375 -4.2768736 -4.2745595 -4.2630396 -4.2613511 -4.2724986 -4.2820277 -4.2909961 -4.2935052 -4.293582 -4.2952108][-4.254828 -4.2684641 -4.2780528 -4.2854924 -4.2926717 -4.2969756 -4.2951827 -4.2900362 -4.2919755 -4.3004909 -4.3071 -4.3081894 -4.301724 -4.2974129 -4.3001246][-4.2814384 -4.2874336 -4.29401 -4.3026648 -4.3098426 -4.3128657 -4.3104792 -4.3065343 -4.3061547 -4.3108211 -4.3152733 -4.3131256 -4.3052173 -4.3016186 -4.3075571][-4.3059878 -4.3061018 -4.3091331 -4.3145757 -4.3197966 -4.3217793 -4.320518 -4.3182635 -4.3164697 -4.316608 -4.3177853 -4.3153014 -4.3105693 -4.3106351 -4.318254]]...]
INFO - root - 2017-12-07 23:40:20.804873: step 67310, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.618 sec/batch; 45h:33m:15s remains)
INFO - root - 2017-12-07 23:40:27.622119: step 67320, loss = 2.02, batch loss = 1.97 (12.0 examples/sec; 0.666 sec/batch; 49h:04m:47s remains)
INFO - root - 2017-12-07 23:40:34.476748: step 67330, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 53h:06m:27s remains)
INFO - root - 2017-12-07 23:40:41.112304: step 67340, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 52h:56m:31s remains)
INFO - root - 2017-12-07 23:40:47.867262: step 67350, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 50h:10m:41s remains)
INFO - root - 2017-12-07 23:40:54.613432: step 67360, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 46h:05m:44s remains)
INFO - root - 2017-12-07 23:41:01.364307: step 67370, loss = 2.11, batch loss = 2.05 (12.2 examples/sec; 0.654 sec/batch; 48h:11m:35s remains)
INFO - root - 2017-12-07 23:41:08.174182: step 67380, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 51h:44m:34s remains)
INFO - root - 2017-12-07 23:41:14.962006: step 67390, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 50h:25m:14s remains)
INFO - root - 2017-12-07 23:41:21.693970: step 67400, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 49h:01m:22s remains)
2017-12-07 23:41:22.477992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1385 -4.1418037 -4.1477246 -4.1555047 -4.1657214 -4.1709309 -4.165669 -4.1551008 -4.1494775 -4.1436076 -4.1367731 -4.132658 -4.1345353 -4.1379819 -4.1417427][-4.1354079 -4.1315703 -4.1337128 -4.1401849 -4.15019 -4.1562014 -4.1509247 -4.1401448 -4.1354847 -4.1295438 -4.1242127 -4.1219425 -4.1233225 -4.1248937 -4.1300678][-4.1426024 -4.1396232 -4.1442943 -4.1514554 -4.1581111 -4.159791 -4.1515818 -4.1409197 -4.1352615 -4.1270976 -4.1202621 -4.1161962 -4.1126585 -4.1101279 -4.114943][-4.1452341 -4.151052 -4.1627321 -4.1746635 -4.1775942 -4.1699929 -4.1566215 -4.146975 -4.13851 -4.1257324 -4.1151719 -4.1077785 -4.0984492 -4.0926757 -4.0993824][-4.124445 -4.1375856 -4.1567 -4.1733618 -4.1726818 -4.1550775 -4.137044 -4.1296782 -4.1215057 -4.1097221 -4.0965133 -4.0825872 -4.0692167 -4.066102 -4.0808969][-4.0929589 -4.111454 -4.1344061 -4.14919 -4.1409712 -4.1133776 -4.0899787 -4.086688 -4.0816779 -4.0687156 -4.0482516 -4.0307484 -4.0233555 -4.0282669 -4.0509548][-4.0577221 -4.078311 -4.0973449 -4.0934553 -4.061388 -4.0186067 -3.9957519 -4.0037441 -4.0103474 -4.0021334 -3.9769266 -3.9592168 -3.96062 -3.9715009 -4.0006218][-4.0160828 -4.0361133 -4.0438986 -4.0110707 -3.9426796 -3.8784173 -3.8596458 -3.8932817 -3.9270601 -3.9365909 -3.9167039 -3.9022281 -3.9082863 -3.9193144 -3.9426796][-3.9877336 -4.0062342 -4.0077715 -3.9599774 -3.8753588 -3.8036056 -3.7952249 -3.8529656 -3.911016 -3.9324844 -3.9161515 -3.9053051 -3.9105146 -3.9142075 -3.9234414][-3.9892826 -4.0103235 -4.0196934 -3.9863255 -3.9276807 -3.8865271 -3.8896594 -3.9356589 -3.9847693 -4.0059686 -3.9923856 -3.9803329 -3.9759872 -3.9739401 -3.9769967][-4.0156422 -4.0329285 -4.0527997 -4.0452204 -4.0220642 -4.0072327 -4.0134625 -4.0395 -4.0697808 -4.0858245 -4.0774703 -4.0656562 -4.0596848 -4.0591908 -4.0629067][-4.0647249 -4.0731163 -4.090013 -4.0941296 -4.0900431 -4.0921354 -4.1036472 -4.1211319 -4.1399364 -4.1514125 -4.1474824 -4.1399045 -4.1334867 -4.1296034 -4.1274304][-4.0918207 -4.0924315 -4.1030374 -4.1093812 -4.1117721 -4.12137 -4.1347122 -4.1491232 -4.1610565 -4.16887 -4.1703682 -4.1687098 -4.1650367 -4.1600509 -4.1544423][-4.1146693 -4.1129556 -4.1188469 -4.1226149 -4.1247106 -4.1340246 -4.1453576 -4.1560073 -4.1649795 -4.1719747 -4.1743536 -4.173346 -4.1680837 -4.1621323 -4.1581783][-4.1519547 -4.1488175 -4.1511841 -4.1528449 -4.15331 -4.158833 -4.1663961 -4.1728077 -4.176589 -4.1793995 -4.180757 -4.1828613 -4.1809635 -4.1765418 -4.1729178]]...]
INFO - root - 2017-12-07 23:41:29.337014: step 67410, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.747 sec/batch; 55h:01m:20s remains)
INFO - root - 2017-12-07 23:41:36.098185: step 67420, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 50h:21m:58s remains)
INFO - root - 2017-12-07 23:41:42.880424: step 67430, loss = 2.07, batch loss = 2.01 (13.3 examples/sec; 0.600 sec/batch; 44h:09m:37s remains)
INFO - root - 2017-12-07 23:41:49.656875: step 67440, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 48h:13m:45s remains)
INFO - root - 2017-12-07 23:41:56.484416: step 67450, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 52h:34m:12s remains)
INFO - root - 2017-12-07 23:42:03.192584: step 67460, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 50h:58m:53s remains)
INFO - root - 2017-12-07 23:42:10.020577: step 67470, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 51h:27m:00s remains)
INFO - root - 2017-12-07 23:42:16.764025: step 67480, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 48h:17m:00s remains)
INFO - root - 2017-12-07 23:42:23.578520: step 67490, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 46h:49m:26s remains)
INFO - root - 2017-12-07 23:42:30.269306: step 67500, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 54h:15m:21s remains)
2017-12-07 23:42:31.050139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3001451 -4.2998238 -4.2968287 -4.2863417 -4.272182 -4.2633853 -4.2666187 -4.2819219 -4.2967534 -4.3048267 -4.3068705 -4.3024778 -4.2973456 -4.2948117 -4.2946486][-4.3113852 -4.3101773 -4.3019161 -4.2812419 -4.2555342 -4.238143 -4.2421737 -4.2671094 -4.2943988 -4.3138628 -4.3228374 -4.3205104 -4.313776 -4.3086796 -4.30677][-4.3107963 -4.3053312 -4.2864671 -4.2510033 -4.2111363 -4.1836047 -4.1872625 -4.2219315 -4.2633228 -4.2975893 -4.318696 -4.3245664 -4.3208156 -4.3152232 -4.3120718][-4.3062305 -4.2947769 -4.2616792 -4.2091155 -4.1536689 -4.11447 -4.115025 -4.1579776 -4.2124472 -4.2620721 -4.2987156 -4.3174553 -4.3211184 -4.3186297 -4.3168173][-4.29696 -4.2805457 -4.237154 -4.16988 -4.1006384 -4.0525584 -4.048316 -4.0944009 -4.1590142 -4.2171993 -4.2632756 -4.2938042 -4.30826 -4.3127837 -4.3142323][-4.2850266 -4.2645812 -4.2129 -4.132844 -4.0530972 -4.0012841 -3.9934089 -4.0365596 -4.1054764 -4.1692171 -4.2190332 -4.260128 -4.2855144 -4.2993116 -4.3067489][-4.2776709 -4.2548676 -4.198174 -4.1075912 -4.01802 -3.9623144 -3.9457643 -3.9763923 -4.041142 -4.1072969 -4.161602 -4.2141719 -4.2546334 -4.2804079 -4.29541][-4.2788286 -4.259048 -4.2054472 -4.1122093 -4.0128503 -3.9461148 -3.9102387 -3.9148312 -3.9676242 -4.0381546 -4.1056366 -4.1756639 -4.2323532 -4.2675052 -4.2856364][-4.2874165 -4.2759786 -4.236095 -4.1572561 -4.0585709 -3.9768877 -3.9189763 -3.9006894 -3.9422314 -4.0176053 -4.0968432 -4.1734433 -4.232265 -4.2647929 -4.2783766][-4.2949266 -4.2953849 -4.2762451 -4.2261891 -4.1465354 -4.0611396 -3.9858665 -3.9571431 -3.9941859 -4.0640144 -4.1382113 -4.2032456 -4.2469139 -4.2643132 -4.2676077][-4.2917175 -4.307199 -4.3093076 -4.2888947 -4.2369957 -4.1609831 -4.079308 -4.0446711 -4.0780382 -4.1368661 -4.1957006 -4.2396259 -4.2590642 -4.2553854 -4.2467561][-4.2666779 -4.2985148 -4.3223228 -4.3280506 -4.3040004 -4.2490487 -4.1797118 -4.1445055 -4.1639395 -4.2047057 -4.2430491 -4.2615767 -4.2549939 -4.2300434 -4.2100229][-4.2177844 -4.2640581 -4.3092222 -4.3382258 -4.3410554 -4.3151221 -4.27095 -4.242413 -4.247468 -4.2671175 -4.2774863 -4.2642837 -4.2275968 -4.1810427 -4.1506743][-4.1471853 -4.2064915 -4.2695231 -4.3182516 -4.3426685 -4.3404751 -4.3204794 -4.3051085 -4.306726 -4.3111081 -4.2933531 -4.2469168 -4.1820431 -4.1175628 -4.0821123][-4.0776687 -4.14261 -4.217298 -4.2765265 -4.314858 -4.3310947 -4.3293996 -4.3235488 -4.324636 -4.3200641 -4.2839231 -4.2132921 -4.1277456 -4.0525174 -4.0180249]]...]
INFO - root - 2017-12-07 23:42:37.744876: step 67510, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 46h:29m:19s remains)
INFO - root - 2017-12-07 23:42:44.549328: step 67520, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 50h:23m:23s remains)
INFO - root - 2017-12-07 23:42:51.328208: step 67530, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 52h:37m:25s remains)
INFO - root - 2017-12-07 23:42:58.169449: step 67540, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 47h:30m:58s remains)
INFO - root - 2017-12-07 23:43:04.739818: step 67550, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 46h:33m:27s remains)
INFO - root - 2017-12-07 23:43:11.612430: step 67560, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 49h:52m:43s remains)
INFO - root - 2017-12-07 23:43:18.383363: step 67570, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 51h:27m:26s remains)
INFO - root - 2017-12-07 23:43:25.275849: step 67580, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 53h:46m:13s remains)
INFO - root - 2017-12-07 23:43:32.045973: step 67590, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.626 sec/batch; 46h:04m:24s remains)
INFO - root - 2017-12-07 23:43:38.616872: step 67600, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 47h:41m:13s remains)
2017-12-07 23:43:39.347067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2683339 -4.2415862 -4.2008424 -4.153831 -4.1274967 -4.1357417 -4.1616573 -4.1821976 -4.1941247 -4.2016239 -4.2090697 -4.2144213 -4.22196 -4.23856 -4.25205][-4.2776041 -4.2511663 -4.2110763 -4.1598215 -4.1261835 -4.1308761 -4.1571856 -4.173862 -4.1755052 -4.1847053 -4.2011781 -4.2186742 -4.2330356 -4.249547 -4.2601137][-4.2776675 -4.2518616 -4.2094655 -4.15221 -4.1105213 -4.1037884 -4.1216588 -4.1298022 -4.1225195 -4.140079 -4.1718249 -4.2068367 -4.23099 -4.2485752 -4.2576246][-4.2758565 -4.2512879 -4.2092891 -4.1526685 -4.1052585 -4.0864992 -4.0883465 -4.0768771 -4.0579205 -4.0826011 -4.1358562 -4.1923828 -4.2298226 -4.2504621 -4.2558246][-4.2814283 -4.2636952 -4.2289648 -4.1811357 -4.1407647 -4.1162353 -4.0968056 -4.0569935 -4.0156274 -4.0430694 -4.1163125 -4.1883063 -4.2322659 -4.2525053 -4.2580457][-4.2867317 -4.27823 -4.255096 -4.2169285 -4.1794286 -4.144484 -4.1018882 -4.0359535 -3.9666638 -3.994678 -4.0817413 -4.16554 -4.214468 -4.2390494 -4.2498212][-4.2835994 -4.2785606 -4.2613635 -4.2282987 -4.1860685 -4.1332736 -4.0685077 -3.9823556 -3.8949137 -3.9301429 -4.0330367 -4.1257572 -4.1840425 -4.2176685 -4.2352753][-4.277894 -4.2700777 -4.2515521 -4.2250528 -4.1835165 -4.1234655 -4.0514069 -3.9639931 -3.8822477 -3.9220047 -4.0250864 -4.1146746 -4.1693449 -4.2031584 -4.2207537][-4.2786388 -4.2694073 -4.2517915 -4.2329187 -4.2018933 -4.1542048 -4.0931697 -4.0172849 -3.9528174 -3.9885886 -4.0723481 -4.144495 -4.1839008 -4.2085633 -4.2185631][-4.2879376 -4.2811527 -4.26712 -4.2518892 -4.2322097 -4.2015061 -4.1551433 -4.0896125 -4.0367074 -4.0627837 -4.1241117 -4.1805096 -4.2111721 -4.2275214 -4.2279115][-4.2938542 -4.2897086 -4.2796469 -4.2658882 -4.2514963 -4.2344584 -4.1988287 -4.1458 -4.1060443 -4.1231518 -4.1654811 -4.2106609 -4.238266 -4.2505236 -4.2437105][-4.2919559 -4.290658 -4.28249 -4.2713804 -4.2605524 -4.247839 -4.2160821 -4.1721272 -4.14206 -4.1554537 -4.1869388 -4.2273393 -4.2551169 -4.2658181 -4.2537365][-4.2799463 -4.2819395 -4.2747188 -4.264442 -4.2577386 -4.2509227 -4.22603 -4.1909466 -4.1706409 -4.1774573 -4.1962738 -4.2267237 -4.2560596 -4.27141 -4.2631216][-4.2705488 -4.2712326 -4.2631259 -4.2529159 -4.2495575 -4.2529159 -4.2435627 -4.2223663 -4.2058926 -4.2025647 -4.2041922 -4.2185726 -4.2448773 -4.2653661 -4.2671967][-4.2730889 -4.2720265 -4.2641449 -4.2554431 -4.2521396 -4.2584443 -4.2593255 -4.2511673 -4.2381768 -4.2251811 -4.2129531 -4.2117934 -4.2282338 -4.2484245 -4.2597685]]...]
INFO - root - 2017-12-07 23:43:46.039237: step 67610, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 49h:33m:13s remains)
INFO - root - 2017-12-07 23:43:52.734522: step 67620, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.609 sec/batch; 44h:49m:56s remains)
INFO - root - 2017-12-07 23:43:59.490352: step 67630, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 46h:44m:39s remains)
INFO - root - 2017-12-07 23:44:06.308502: step 67640, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 53h:59m:11s remains)
INFO - root - 2017-12-07 23:44:13.116172: step 67650, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 51h:05m:59s remains)
INFO - root - 2017-12-07 23:44:19.822915: step 67660, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 48h:33m:46s remains)
INFO - root - 2017-12-07 23:44:26.608867: step 67670, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 46h:34m:19s remains)
INFO - root - 2017-12-07 23:44:33.452473: step 67680, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 46h:23m:35s remains)
INFO - root - 2017-12-07 23:44:40.304818: step 67690, loss = 2.04, batch loss = 1.99 (10.3 examples/sec; 0.778 sec/batch; 57h:13m:41s remains)
INFO - root - 2017-12-07 23:44:46.979028: step 67700, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 52h:04m:15s remains)
2017-12-07 23:44:47.682243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3312125 -4.3334475 -4.3313303 -4.3220973 -4.3152418 -4.304132 -4.300683 -4.2963228 -4.2847805 -4.2723775 -4.2718511 -4.2823381 -4.3028727 -4.3211703 -4.3328366][-4.3366761 -4.3415108 -4.3358192 -4.3195219 -4.305727 -4.2880597 -4.2773886 -4.2721581 -4.2649956 -4.253407 -4.2484846 -4.252955 -4.2734804 -4.2983661 -4.31716][-4.3362288 -4.3381333 -4.3269281 -4.3054428 -4.2886844 -4.2657037 -4.2461929 -4.2361517 -4.2360048 -4.2276969 -4.2200089 -4.2158194 -4.2328272 -4.2601271 -4.282268][-4.3105545 -4.3094893 -4.2966032 -4.2778707 -4.2565203 -4.2203631 -4.1849775 -4.1653914 -4.1751652 -4.1793675 -4.1719432 -4.1639056 -4.1736383 -4.2029948 -4.2288933][-4.2777123 -4.2754278 -4.2604833 -4.2416387 -4.2145357 -4.1631217 -4.10763 -4.0822349 -4.1148987 -4.1450744 -4.1516619 -4.144824 -4.1442027 -4.1659627 -4.1883984][-4.2407827 -4.2352524 -4.2120662 -4.1881762 -4.156476 -4.0920448 -4.0158787 -3.9893191 -4.054688 -4.1120973 -4.1261058 -4.1187034 -4.1153421 -4.1341887 -4.1572223][-4.1973486 -4.1846581 -4.1492314 -4.1156855 -4.0709634 -3.990912 -3.893415 -3.8592424 -3.9585958 -4.0480156 -4.0769329 -4.082458 -4.0900726 -4.1159563 -4.1466436][-4.1708846 -4.1454377 -4.0999727 -4.0650625 -4.0207033 -3.9465473 -3.86219 -3.840457 -3.9431541 -4.0410318 -4.0817361 -4.098702 -4.1115403 -4.13479 -4.16536][-4.18703 -4.1499219 -4.1081605 -4.0866904 -4.0622492 -4.0157528 -3.9712334 -3.9690642 -4.034956 -4.1010046 -4.1322255 -4.1506572 -4.1629572 -4.1758161 -4.1962452][-4.2373581 -4.1977382 -4.1653519 -4.1524286 -4.1333332 -4.1050797 -4.0834975 -4.0848203 -4.1187806 -4.1587863 -4.1878958 -4.2101 -4.2213616 -4.2258234 -4.2377486][-4.2694354 -4.2385559 -4.2190084 -4.2137117 -4.1988244 -4.1775427 -4.1647315 -4.1628294 -4.1780491 -4.20469 -4.2313142 -4.2536426 -4.2632389 -4.2624559 -4.2669139][-4.2988048 -4.2799959 -4.2718191 -4.2725258 -4.2665968 -4.2547755 -4.2501221 -4.2471089 -4.2498722 -4.2620544 -4.2778244 -4.2932858 -4.2977228 -4.2924061 -4.2897859][-4.3278012 -4.3151736 -4.3092847 -4.3080163 -4.3053761 -4.30418 -4.306365 -4.3048258 -4.3016534 -4.3040214 -4.3102317 -4.3185024 -4.3204889 -4.3143229 -4.3082314][-4.3099947 -4.3014126 -4.295938 -4.2921619 -4.2907619 -4.2928662 -4.2958684 -4.2951765 -4.2929387 -4.2926807 -4.2931457 -4.2955928 -4.2969866 -4.2937255 -4.2895656][-4.2808561 -4.2736869 -4.2701011 -4.2677612 -4.2682152 -4.2719588 -4.2747688 -4.274333 -4.2738667 -4.2735505 -4.27344 -4.2733092 -4.2719975 -4.268424 -4.2653933]]...]
INFO - root - 2017-12-07 23:44:54.500834: step 67710, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.651 sec/batch; 47h:50m:55s remains)
INFO - root - 2017-12-07 23:45:01.309133: step 67720, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 54h:40m:59s remains)
INFO - root - 2017-12-07 23:45:08.037788: step 67730, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 50h:19m:46s remains)
INFO - root - 2017-12-07 23:45:14.778546: step 67740, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 50h:19m:46s remains)
INFO - root - 2017-12-07 23:45:21.575617: step 67750, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 47h:50m:31s remains)
INFO - root - 2017-12-07 23:45:28.431114: step 67760, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 47h:55m:57s remains)
INFO - root - 2017-12-07 23:45:35.371953: step 67770, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.741 sec/batch; 54h:29m:36s remains)
INFO - root - 2017-12-07 23:45:42.134622: step 67780, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.669 sec/batch; 49h:13m:48s remains)
INFO - root - 2017-12-07 23:45:48.983104: step 67790, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 49h:33m:54s remains)
INFO - root - 2017-12-07 23:45:55.647982: step 67800, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 46h:17m:26s remains)
2017-12-07 23:45:56.462216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1564097 -4.1781988 -4.178432 -4.1716285 -4.1670241 -4.1587334 -4.142086 -4.1316123 -4.1214728 -4.1030788 -4.08702 -4.0922709 -4.0987358 -4.0953541 -4.108974][-4.1676893 -4.1888242 -4.1918688 -4.1867094 -4.1773162 -4.1638875 -4.14847 -4.1402917 -4.1330304 -4.115572 -4.0964675 -4.0969319 -4.1022539 -4.1058059 -4.1202359][-4.170794 -4.1855669 -4.1932459 -4.1923876 -4.1836953 -4.1666288 -4.1465631 -4.1331792 -4.1288528 -4.122725 -4.1100512 -4.1086674 -4.1188922 -4.12951 -4.1376119][-4.15354 -4.1601243 -4.1631546 -4.1651783 -4.1633148 -4.14998 -4.1255894 -4.1030455 -4.1028562 -4.1154156 -4.1174107 -4.1182222 -4.1324425 -4.1468964 -4.1441312][-4.1173363 -4.1143694 -4.111661 -4.1214767 -4.1236262 -4.108923 -4.0868535 -4.0687022 -4.0792131 -4.1066656 -4.1172423 -4.1156187 -4.12898 -4.1448836 -4.1368213][-4.071197 -4.0658355 -4.061481 -4.0792513 -4.0828605 -4.064744 -4.0499358 -4.0467052 -4.0673609 -4.1022806 -4.1137042 -4.108696 -4.1146088 -4.1207738 -4.1110249][-4.0512743 -4.0503473 -4.0426269 -4.0618286 -4.0641551 -4.0447965 -4.0410357 -4.0569777 -4.08656 -4.1170454 -4.1232343 -4.1180372 -4.1096029 -4.0951567 -4.0844398][-4.0529366 -4.0639997 -4.0565009 -4.0653472 -4.0643349 -4.0514212 -4.0583262 -4.0869308 -4.1205177 -4.1506772 -4.1602335 -4.1565218 -4.1334558 -4.1025429 -4.0961637][-4.0438361 -4.0627952 -4.0580311 -4.0570197 -4.0509095 -4.0452957 -4.0636683 -4.10079 -4.1439462 -4.1837173 -4.2014761 -4.1971874 -4.1618204 -4.12193 -4.1154141][-4.0517149 -4.0680451 -4.0630741 -4.053679 -4.0398512 -4.0312934 -4.0533929 -4.0931325 -4.142076 -4.1872358 -4.2107677 -4.206233 -4.1701283 -4.1306534 -4.1205192][-4.0809674 -4.0859051 -4.0787306 -4.06499 -4.0485935 -4.0317783 -4.0426564 -4.0733466 -4.11856 -4.1634388 -4.18992 -4.1889944 -4.1565771 -4.1232171 -4.1086783][-4.1045723 -4.0981359 -4.0900197 -4.0802364 -4.0669785 -4.0445724 -4.0406575 -4.0567031 -4.0876994 -4.121439 -4.1512904 -4.1602683 -4.133873 -4.10422 -4.0836916][-4.0879836 -4.0729885 -4.0664873 -4.0634747 -4.0570741 -4.0406089 -4.0343771 -4.0426893 -4.0569367 -4.0733614 -4.1049256 -4.1313076 -4.1219721 -4.0998354 -4.0775957][-4.06455 -4.0432925 -4.0373135 -4.0365567 -4.0345707 -4.0288453 -4.0298252 -4.0317712 -4.0344529 -4.03316 -4.0607266 -4.0993848 -4.1079373 -4.098556 -4.0879951][-4.0587034 -4.0325842 -4.0239058 -4.0246143 -4.0287981 -4.0371952 -4.0442915 -4.045712 -4.0441961 -4.0291977 -4.0437565 -4.0838351 -4.1038795 -4.1045656 -4.1036158]]...]
INFO - root - 2017-12-07 23:46:03.172703: step 67810, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 48h:34m:49s remains)
INFO - root - 2017-12-07 23:46:09.930837: step 67820, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 47h:28m:46s remains)
INFO - root - 2017-12-07 23:46:16.778693: step 67830, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 46h:10m:26s remains)
INFO - root - 2017-12-07 23:46:23.572984: step 67840, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 49h:51m:00s remains)
INFO - root - 2017-12-07 23:46:30.428225: step 67850, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.735 sec/batch; 54h:03m:32s remains)
INFO - root - 2017-12-07 23:46:36.992234: step 67860, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 47h:48m:41s remains)
INFO - root - 2017-12-07 23:46:43.686133: step 67870, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 47h:41m:26s remains)
INFO - root - 2017-12-07 23:46:50.451811: step 67880, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.681 sec/batch; 50h:02m:33s remains)
INFO - root - 2017-12-07 23:46:57.251506: step 67890, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.747 sec/batch; 54h:54m:59s remains)
INFO - root - 2017-12-07 23:47:03.899858: step 67900, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 51h:51m:21s remains)
2017-12-07 23:47:04.674665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2325745 -4.2063332 -4.1953282 -4.200387 -4.2085485 -4.2220011 -4.2352266 -4.2475319 -4.261148 -4.2621956 -4.2559204 -4.2414961 -4.2165155 -4.1864271 -4.16448][-4.2053 -4.1800518 -4.1693964 -4.1763225 -4.1838384 -4.1924925 -4.2020168 -4.2173367 -4.2393136 -4.2485166 -4.2458258 -4.237329 -4.2146254 -4.1778407 -4.1479068][-4.1742959 -4.1414862 -4.1237893 -4.127821 -4.1346674 -4.1367316 -4.1393743 -4.1595531 -4.1996017 -4.22647 -4.2304554 -4.2282691 -4.2092953 -4.1674433 -4.132071][-4.1604881 -4.110147 -4.0729795 -4.0680885 -4.0707188 -4.0615191 -4.0495396 -4.0790219 -4.1419744 -4.1892533 -4.2036543 -4.213274 -4.2099347 -4.1760859 -4.1394482][-4.1619711 -4.0955253 -4.0385766 -4.0238919 -4.0186133 -3.9910302 -3.9649239 -4.00281 -4.0797067 -4.1413708 -4.1679282 -4.18857 -4.1993666 -4.1812854 -4.1520829][-4.1699662 -4.0966258 -4.0308108 -4.0074515 -3.9951451 -3.9591761 -3.9301469 -3.9759946 -4.0620666 -4.1259208 -4.1509056 -4.1672697 -4.17617 -4.1734962 -4.1608405][-4.1830196 -4.1103387 -4.0427217 -4.0136752 -3.9961238 -3.9575987 -3.9322722 -3.9887991 -4.0859771 -4.1443739 -4.15764 -4.1697507 -4.1813917 -4.1867132 -4.1895304][-4.1918616 -4.1279325 -4.0693908 -4.0446019 -4.0282369 -3.9895656 -3.9662311 -4.0275712 -4.1280513 -4.17966 -4.1857224 -4.1971531 -4.2111206 -4.2228212 -4.227138][-4.2037034 -4.1509051 -4.100513 -4.0801864 -4.0719028 -4.0422044 -4.0259271 -4.080759 -4.1671896 -4.2109442 -4.2193351 -4.2311549 -4.2447987 -4.2584596 -4.2634048][-4.2197924 -4.1753082 -4.1323037 -4.1066523 -4.0939288 -4.0650949 -4.0543103 -4.1003294 -4.1727891 -4.2171593 -4.232934 -4.2508359 -4.2630386 -4.273231 -4.2779856][-4.2287207 -4.1921849 -4.1619625 -4.1323733 -4.107018 -4.071537 -4.0559359 -4.08931 -4.1525135 -4.2060513 -4.2347965 -4.2593231 -4.2688804 -4.267982 -4.2619543][-4.2416639 -4.2151418 -4.19286 -4.1617627 -4.1315112 -4.0943222 -4.0709233 -4.0881853 -4.1361661 -4.191628 -4.230114 -4.2578907 -4.2649312 -4.2541103 -4.237164][-4.2538328 -4.2344923 -4.2177181 -4.1909161 -4.1624331 -4.125453 -4.1002655 -4.1044536 -4.13681 -4.184227 -4.2238393 -4.249743 -4.2537217 -4.2424226 -4.2230258][-4.2699623 -4.2532387 -4.2412114 -4.2187519 -4.1930027 -4.1571679 -4.1294265 -4.1272578 -4.152977 -4.1952462 -4.2274933 -4.2455831 -4.2477784 -4.2373877 -4.2172341][-4.2874804 -4.2734442 -4.2661996 -4.251173 -4.2347889 -4.2054915 -4.1793747 -4.1714416 -4.1909361 -4.2255874 -4.2470183 -4.2566051 -4.2541652 -4.24001 -4.2164426]]...]
INFO - root - 2017-12-07 23:47:11.428335: step 67910, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 52h:15m:08s remains)
INFO - root - 2017-12-07 23:47:18.172481: step 67920, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 48h:08m:10s remains)
INFO - root - 2017-12-07 23:47:25.003117: step 67930, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 49h:45m:11s remains)
INFO - root - 2017-12-07 23:47:31.715041: step 67940, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 46h:22m:52s remains)
INFO - root - 2017-12-07 23:47:38.433624: step 67950, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 47h:48m:47s remains)
INFO - root - 2017-12-07 23:47:45.276439: step 67960, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.749 sec/batch; 55h:01m:09s remains)
INFO - root - 2017-12-07 23:47:52.064911: step 67970, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 49h:12m:06s remains)
INFO - root - 2017-12-07 23:47:58.808007: step 67980, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 47h:17m:39s remains)
INFO - root - 2017-12-07 23:48:05.658860: step 67990, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 47h:50m:13s remains)
INFO - root - 2017-12-07 23:48:12.387436: step 68000, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 53h:31m:33s remains)
2017-12-07 23:48:13.106195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1449661 -4.1327696 -4.1609707 -4.1984968 -4.2343988 -4.2510276 -4.2336736 -4.1987338 -4.1845069 -4.18811 -4.1737251 -4.1419296 -4.1215687 -4.1365776 -4.1755934][-4.1668591 -4.1640558 -4.1815472 -4.20513 -4.2303305 -4.2471843 -4.2390127 -4.2160187 -4.2031918 -4.1941943 -4.1616306 -4.1162872 -4.0883293 -4.1010408 -4.1441207][-4.19669 -4.1927958 -4.196672 -4.2029548 -4.2178011 -4.2371163 -4.2427139 -4.231441 -4.2196889 -4.1966562 -4.1539726 -4.1102161 -4.0850654 -4.1041031 -4.1509075][-4.2312622 -4.2218766 -4.2114573 -4.2014604 -4.2010555 -4.217299 -4.2314568 -4.2306366 -4.2266846 -4.2088604 -4.1774912 -4.1520343 -4.1389651 -4.1597304 -4.1963549][-4.2550206 -4.2391043 -4.2199292 -4.1977959 -4.180191 -4.1763668 -4.1760464 -4.1755724 -4.1878686 -4.2002635 -4.19905 -4.1917286 -4.1931992 -4.2104464 -4.2276711][-4.2583623 -4.2394338 -4.2172852 -4.1923666 -4.1576605 -4.1163549 -4.0737872 -4.05805 -4.0981174 -4.1577125 -4.1962004 -4.209981 -4.2213893 -4.233252 -4.2345839][-4.2479458 -4.2339211 -4.2137332 -4.189568 -4.1472321 -4.0730581 -3.984431 -3.9463081 -4.0162964 -4.1189947 -4.1878181 -4.2191863 -4.232698 -4.231286 -4.2158647][-4.2353954 -4.2277579 -4.217309 -4.2002578 -4.1638317 -4.091598 -4.0005021 -3.956388 -4.0217791 -4.1156387 -4.17496 -4.2046523 -4.2172036 -4.2129893 -4.1938295][-4.207737 -4.209599 -4.2109747 -4.2044172 -4.1835275 -4.1356554 -4.0794692 -4.0585122 -4.0970073 -4.1459541 -4.1711378 -4.1822033 -4.1861925 -4.1860547 -4.1784735][-4.1627226 -4.1644826 -4.1744256 -4.1797609 -4.1769786 -4.1545386 -4.1259813 -4.1156311 -4.1275606 -4.1390123 -4.1361375 -4.126019 -4.1181431 -4.1225085 -4.131187][-4.1081038 -4.0976353 -4.10447 -4.1160889 -4.1212139 -4.11897 -4.1097345 -4.1015792 -4.0942764 -4.0832334 -4.0661688 -4.0429859 -4.0263085 -4.0273342 -4.0404229][-4.096489 -4.0805349 -4.0763679 -4.0757809 -4.0754933 -4.0807304 -4.0859075 -4.0853534 -4.07466 -4.0593638 -4.0449023 -4.02626 -4.0117841 -4.0100923 -4.0155764][-4.1505694 -4.13456 -4.1188321 -4.097589 -4.0819392 -4.084857 -4.0933914 -4.1002197 -4.0946918 -4.0822482 -4.0770655 -4.071044 -4.0633893 -4.0617743 -4.0630522][-4.215446 -4.2004113 -4.1780443 -4.1433635 -4.1193953 -4.1212296 -4.1306586 -4.1422329 -4.1452875 -4.1352048 -4.1301861 -4.1284442 -4.1252389 -4.1267748 -4.1276255][-4.2684417 -4.2548614 -4.2310214 -4.1939754 -4.1679134 -4.1653571 -4.1717477 -4.1863704 -4.1974463 -4.1931906 -4.1844726 -4.1783366 -4.1747527 -4.1800208 -4.1822543]]...]
INFO - root - 2017-12-07 23:48:19.907489: step 68010, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 47h:52m:12s remains)
INFO - root - 2017-12-07 23:48:26.684725: step 68020, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 45h:30m:36s remains)
INFO - root - 2017-12-07 23:48:33.488796: step 68030, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 52h:34m:56s remains)
INFO - root - 2017-12-07 23:48:40.247269: step 68040, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.711 sec/batch; 52h:15m:23s remains)
INFO - root - 2017-12-07 23:48:47.066613: step 68050, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 49h:11m:54s remains)
INFO - root - 2017-12-07 23:48:53.891085: step 68060, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 47h:20m:30s remains)
INFO - root - 2017-12-07 23:49:00.606378: step 68070, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.618 sec/batch; 45h:22m:06s remains)
INFO - root - 2017-12-07 23:49:07.558762: step 68080, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 51h:51m:03s remains)
INFO - root - 2017-12-07 23:49:14.261107: step 68090, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.749 sec/batch; 55h:00m:52s remains)
INFO - root - 2017-12-07 23:49:20.779213: step 68100, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 48h:15m:52s remains)
2017-12-07 23:49:21.586375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32279 -4.3204637 -4.3160419 -4.3141694 -4.3151641 -4.31599 -4.3153453 -4.3141179 -4.3153381 -4.3172121 -4.3192363 -4.3201203 -4.3191814 -4.3181887 -4.3178859][-4.3036418 -4.307107 -4.303822 -4.2991409 -4.2960205 -4.2932305 -4.2903824 -4.2892237 -4.2932363 -4.2989593 -4.30612 -4.3122425 -4.3143435 -4.3151793 -4.3161645][-4.2803526 -4.2904654 -4.28709 -4.2765913 -4.2663665 -4.2582216 -4.25292 -4.2524366 -4.2620964 -4.2740412 -4.2893047 -4.3043804 -4.3133488 -4.3181481 -4.3206644][-4.2643332 -4.2793088 -4.2717981 -4.2506928 -4.2311797 -4.2177281 -4.2086172 -4.2067542 -4.2175288 -4.233336 -4.2567544 -4.2870445 -4.308301 -4.3208294 -4.324338][-4.2647204 -4.28219 -4.2720494 -4.2410617 -4.2059164 -4.1809025 -4.1650963 -4.1567907 -4.1615262 -4.176321 -4.2120147 -4.2601666 -4.2966166 -4.3176117 -4.3208089][-4.2850924 -4.304543 -4.2951832 -4.2586479 -4.2078872 -4.1648622 -4.1371651 -4.11869 -4.1096907 -4.1152411 -4.1610556 -4.225224 -4.275516 -4.3052316 -4.3098445][-4.3073406 -4.3288741 -4.3213162 -4.2841258 -4.2243972 -4.1692429 -4.1246095 -4.0935307 -4.0703039 -4.0650368 -4.1169834 -4.194962 -4.25489 -4.2878242 -4.2927842][-4.320425 -4.3425369 -4.3374224 -4.3026352 -4.2435765 -4.184092 -4.1220441 -4.0751958 -4.0414996 -4.0297041 -4.0829711 -4.169796 -4.2366672 -4.2702146 -4.2782192][-4.3269715 -4.3463993 -4.3387818 -4.3039331 -4.2485261 -4.1899509 -4.1200681 -4.0654583 -4.026927 -4.0161891 -4.0712509 -4.15168 -4.2148566 -4.2423663 -4.249732][-4.3311234 -4.3498855 -4.3401232 -4.3032155 -4.2438326 -4.1854005 -4.1180363 -4.0647593 -4.0321016 -4.0297179 -4.0837207 -4.1522245 -4.2024283 -4.2212133 -4.2228274][-4.3327093 -4.352922 -4.344676 -4.30662 -4.2412968 -4.1786995 -4.11467 -4.0665154 -4.0428915 -4.0547028 -4.1056862 -4.1595192 -4.1911006 -4.1996412 -4.1963105][-4.332602 -4.3538179 -4.3496919 -4.3143787 -4.2476 -4.1774573 -4.1163235 -4.0767474 -4.0583782 -4.0698195 -4.1120229 -4.1583238 -4.181901 -4.1855574 -4.1811743][-4.3327932 -4.3537374 -4.3554492 -4.3284597 -4.2669997 -4.19267 -4.1320276 -4.0914593 -4.0700827 -4.0786195 -4.1152945 -4.156136 -4.1803904 -4.1864338 -4.1864109][-4.331872 -4.3533206 -4.3601823 -4.3415403 -4.2891717 -4.2142749 -4.1557646 -4.1135855 -4.08896 -4.0938797 -4.1273718 -4.1612859 -4.1835003 -4.1927514 -4.1999693][-4.3303089 -4.3533831 -4.3640456 -4.3515887 -4.308352 -4.2358136 -4.1762223 -4.1326728 -4.1069155 -4.1074133 -4.13668 -4.1684523 -4.1896005 -4.2016478 -4.2104888]]...]
INFO - root - 2017-12-07 23:49:28.329895: step 68110, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 50h:15m:13s remains)
INFO - root - 2017-12-07 23:49:35.167624: step 68120, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 49h:41m:05s remains)
INFO - root - 2017-12-07 23:49:41.805889: step 68130, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 46h:48m:39s remains)
INFO - root - 2017-12-07 23:49:48.538295: step 68140, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 48h:08m:54s remains)
INFO - root - 2017-12-07 23:49:55.289870: step 68150, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 48h:56m:26s remains)
INFO - root - 2017-12-07 23:50:02.115535: step 68160, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 53h:49m:55s remains)
INFO - root - 2017-12-07 23:50:08.803224: step 68170, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 49h:48m:31s remains)
INFO - root - 2017-12-07 23:50:15.657860: step 68180, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 46h:53m:05s remains)
INFO - root - 2017-12-07 23:50:22.431531: step 68190, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 52h:40m:18s remains)
INFO - root - 2017-12-07 23:50:29.051935: step 68200, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 50h:18m:21s remains)
2017-12-07 23:50:29.765062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3673263 -4.3699422 -4.3716 -4.3698111 -4.3683834 -4.364028 -4.3492475 -4.3202477 -4.2802677 -4.2422523 -4.213552 -4.1901913 -4.175971 -4.1829791 -4.2039752][-4.3713737 -4.3707676 -4.3700109 -4.365767 -4.3611474 -4.3530464 -4.3364983 -4.3098164 -4.275003 -4.2429271 -4.2227 -4.2121978 -4.2036643 -4.2078304 -4.2217617][-4.3692007 -4.3630848 -4.3574357 -4.3475008 -4.3365636 -4.3224673 -4.30338 -4.2821445 -4.2602124 -4.2415462 -4.237042 -4.2413006 -4.2404222 -4.2422624 -4.248385][-4.3610921 -4.346983 -4.3329091 -4.3147039 -4.2965069 -4.2780509 -4.2593961 -4.2453861 -4.2363863 -4.2324905 -4.2417803 -4.2570758 -4.2635813 -4.266654 -4.2677321][-4.3478503 -4.3230376 -4.2988324 -4.2724295 -4.2479167 -4.2274375 -4.2131305 -4.2081494 -4.2089128 -4.2159219 -4.230906 -4.2502165 -4.262918 -4.2682314 -4.2678943][-4.33566 -4.2996354 -4.2653303 -4.2312083 -4.2000523 -4.1789618 -4.172543 -4.1778383 -4.1847072 -4.1969662 -4.2137537 -4.2327938 -4.24871 -4.25921 -4.264349][-4.3320179 -4.2891464 -4.2485743 -4.209065 -4.1719909 -4.1477942 -4.1448584 -4.152669 -4.159565 -4.1727118 -4.1909013 -4.2103171 -4.2295694 -4.246089 -4.2585759][-4.3402076 -4.2993422 -4.2574725 -4.2156539 -4.1732163 -4.1417432 -4.1312304 -4.1321521 -4.1331716 -4.1430883 -4.1588755 -4.1774735 -4.1987195 -4.221272 -4.2429338][-4.355154 -4.3234653 -4.2849617 -4.2425828 -4.1951303 -4.1534195 -4.1300125 -4.1197314 -4.1134338 -4.1179781 -4.1269546 -4.1391554 -4.1603622 -4.1885819 -4.2208109][-4.363421 -4.3421812 -4.3104753 -4.2703962 -4.2222662 -4.1754804 -4.1414413 -4.1224451 -4.1116018 -4.1116219 -4.1121025 -4.1134772 -4.1308761 -4.162703 -4.200613][-4.3576503 -4.345459 -4.3221936 -4.2881107 -4.2440786 -4.2004447 -4.1639276 -4.1420603 -4.1313152 -4.1320443 -4.1293159 -4.1256347 -4.1394987 -4.1672544 -4.2017894][-4.3369823 -4.3324246 -4.3190737 -4.2958794 -4.2638559 -4.2304721 -4.2000213 -4.1803203 -4.1739697 -4.179 -4.1796942 -4.1797714 -4.1911273 -4.2093124 -4.2328563][-4.2872605 -4.2924395 -4.2933807 -4.2872858 -4.2748017 -4.2594724 -4.2429051 -4.2320452 -4.2327418 -4.2399282 -4.2406254 -4.2395926 -4.2444544 -4.2525182 -4.2635646][-4.2071648 -4.2253394 -4.2439208 -4.256484 -4.2625065 -4.2645617 -4.2618556 -4.2604413 -4.2667432 -4.2736759 -4.2695642 -4.2591476 -4.2537556 -4.2528272 -4.2555556][-4.1190839 -4.1462159 -4.179184 -4.2076764 -4.2279406 -4.24244 -4.2500205 -4.2548227 -4.2635016 -4.2685585 -4.256217 -4.2351832 -4.2200122 -4.2127371 -4.2099571]]...]
INFO - root - 2017-12-07 23:50:36.518839: step 68210, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 47h:50m:35s remains)
INFO - root - 2017-12-07 23:50:43.289909: step 68220, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 53h:51m:05s remains)
INFO - root - 2017-12-07 23:50:50.035624: step 68230, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 52h:49m:33s remains)
INFO - root - 2017-12-07 23:50:56.829163: step 68240, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 49h:01m:28s remains)
INFO - root - 2017-12-07 23:51:03.609305: step 68250, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 46h:53m:03s remains)
INFO - root - 2017-12-07 23:51:10.378734: step 68260, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 49h:40m:43s remains)
INFO - root - 2017-12-07 23:51:17.109591: step 68270, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 50h:22m:15s remains)
INFO - root - 2017-12-07 23:51:23.909027: step 68280, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 51h:49m:53s remains)
INFO - root - 2017-12-07 23:51:30.728114: step 68290, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 50h:27m:04s remains)
INFO - root - 2017-12-07 23:51:37.294812: step 68300, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 45h:25m:05s remains)
2017-12-07 23:51:38.025010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.228199 -4.1539664 -4.0853271 -4.058116 -4.0801258 -4.1132212 -4.1210465 -4.1442437 -4.1906061 -4.2388325 -4.2622061 -4.2542577 -4.2303033 -4.2146587 -4.2324405][-4.2528663 -4.1755996 -4.0960512 -4.0498862 -4.0634208 -4.1027155 -4.1256518 -4.1605453 -4.2088528 -4.2473755 -4.2620387 -4.2491755 -4.2337418 -4.2399011 -4.2756243][-4.2798963 -4.2032037 -4.1169758 -4.0534759 -4.0522513 -4.0853343 -4.1137123 -4.1516023 -4.1989894 -4.2316365 -4.2428212 -4.2326765 -4.2312179 -4.2610383 -4.3084526][-4.2965927 -4.2246313 -4.1412506 -4.0695949 -4.0484653 -4.0642395 -4.0861039 -4.1292453 -4.1831131 -4.2129865 -4.2217188 -4.2186937 -4.2355852 -4.2829041 -4.3314052][-4.2951379 -4.2310352 -4.1555777 -4.0821834 -4.0412807 -4.0346179 -4.0484438 -4.1025467 -4.1660147 -4.1924329 -4.19803 -4.2086763 -4.2481666 -4.3051248 -4.3467484][-4.287209 -4.2344217 -4.1666908 -4.0962296 -4.0448761 -4.0234213 -4.0307741 -4.0870562 -4.1493015 -4.1693487 -4.1762886 -4.2063179 -4.2679105 -4.3269753 -4.3560925][-4.284234 -4.2441821 -4.1849589 -4.12382 -4.0705175 -4.0418472 -4.0442891 -4.0941963 -4.1422081 -4.1531715 -4.1671686 -4.2166128 -4.2886252 -4.3415794 -4.3582559][-4.291007 -4.2654729 -4.2208638 -4.170001 -4.1171632 -4.082406 -4.0758443 -4.1106329 -4.1409793 -4.1471076 -4.171134 -4.2341223 -4.3049946 -4.347743 -4.3556061][-4.3061934 -4.2927222 -4.2621388 -4.2180247 -4.1632552 -4.1167359 -4.0929861 -4.1085138 -4.1303253 -4.1475835 -4.1873851 -4.2545576 -4.3172779 -4.3498058 -4.3506632][-4.3195658 -4.312027 -4.2887068 -4.2479424 -4.1924562 -4.1334667 -4.0907083 -4.0914245 -4.1158295 -4.1542239 -4.2098732 -4.2730503 -4.3232632 -4.3469095 -4.343646][-4.3220949 -4.3141122 -4.2927117 -4.2548747 -4.2009158 -4.1335588 -4.0763211 -4.06529 -4.0983138 -4.1586914 -4.22677 -4.2841368 -4.3236442 -4.3410797 -4.3357139][-4.3192739 -4.3099937 -4.2898045 -4.2557936 -4.2076464 -4.1422896 -4.0803375 -4.0634336 -4.1024928 -4.1744294 -4.2456355 -4.2949286 -4.3257275 -4.3409405 -4.3359838][-4.3178382 -4.3084326 -4.2918925 -4.26387 -4.2253246 -4.171443 -4.11667 -4.0971661 -4.1312494 -4.1988525 -4.2637525 -4.3056121 -4.3317919 -4.3466625 -4.3430238][-4.3207812 -4.311862 -4.2991982 -4.2784553 -4.2499528 -4.2114229 -4.1715951 -4.15542 -4.178916 -4.2313738 -4.2845979 -4.3190966 -4.3410912 -4.3539038 -4.3498044][-4.3260818 -4.3195591 -4.3116484 -4.2983375 -4.278923 -4.2542686 -4.2281923 -4.2146974 -4.2289205 -4.2652063 -4.3045259 -4.3317866 -4.3493657 -4.358799 -4.354352]]...]
INFO - root - 2017-12-07 23:51:44.828972: step 68310, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 48h:57m:08s remains)
INFO - root - 2017-12-07 23:51:51.571727: step 68320, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 47h:51m:38s remains)
INFO - root - 2017-12-07 23:51:58.338036: step 68330, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 48h:14m:40s remains)
INFO - root - 2017-12-07 23:52:05.276893: step 68340, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 53h:03m:34s remains)
INFO - root - 2017-12-07 23:52:12.191954: step 68350, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 54h:41m:41s remains)
INFO - root - 2017-12-07 23:52:19.017878: step 68360, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 49h:07m:04s remains)
INFO - root - 2017-12-07 23:52:25.829041: step 68370, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 47h:21m:35s remains)
INFO - root - 2017-12-07 23:52:32.646067: step 68380, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 46h:14m:00s remains)
INFO - root - 2017-12-07 23:52:39.491738: step 68390, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 53h:14m:27s remains)
INFO - root - 2017-12-07 23:52:46.108709: step 68400, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.740 sec/batch; 54h:16m:05s remains)
2017-12-07 23:52:46.791962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3517852 -4.346468 -4.3429828 -4.3409762 -4.3412533 -4.3399253 -4.3328242 -4.3197689 -4.2985611 -4.277462 -4.2639189 -4.2639403 -4.2678485 -4.2739596 -4.2791052][-4.3551216 -4.3475189 -4.3397427 -4.3323565 -4.32978 -4.3309541 -4.3303061 -4.3224583 -4.3078318 -4.2929568 -4.2867479 -4.290391 -4.2922616 -4.294064 -4.2937083][-4.3478618 -4.3389335 -4.325129 -4.3082871 -4.2983246 -4.299746 -4.3068123 -4.305356 -4.2974977 -4.292407 -4.2936325 -4.2996254 -4.2996993 -4.2985406 -4.2977142][-4.325325 -4.3126044 -4.2924061 -4.2661619 -4.2446651 -4.2389741 -4.2480364 -4.255794 -4.2640886 -4.2755919 -4.286366 -4.2967033 -4.2980642 -4.2969661 -4.2970738][-4.2954388 -4.2740278 -4.2443862 -4.2054439 -4.1670551 -4.1411061 -4.1370511 -4.1502438 -4.1821046 -4.2185235 -4.2460446 -4.2694292 -4.2797427 -4.2862096 -4.2918577][-4.263485 -4.2338991 -4.195642 -4.1450858 -4.0797553 -4.0142412 -3.9797316 -3.9898784 -4.0498815 -4.1232138 -4.1774969 -4.2208991 -4.2487755 -4.2670746 -4.2801447][-4.23171 -4.2075424 -4.1697516 -4.1127276 -4.0165505 -3.8947957 -3.8059075 -3.8030424 -3.8929596 -4.0052838 -4.0893741 -4.1576505 -4.2093134 -4.2433224 -4.2640829][-4.2049513 -4.20104 -4.1751466 -4.1275721 -4.0294366 -3.8807125 -3.746629 -3.7128122 -3.805156 -3.9285846 -4.0235724 -4.1048565 -4.1728449 -4.2198029 -4.2484446][-4.1880469 -4.2059751 -4.202693 -4.1813593 -4.1152225 -3.999171 -3.875037 -3.8172185 -3.8654406 -3.9535074 -4.0280075 -4.0942488 -4.1594877 -4.2073283 -4.23684][-4.1937075 -4.2181273 -4.2332835 -4.2321019 -4.2012777 -4.1392083 -4.06915 -4.0227175 -4.02796 -4.065392 -4.1042652 -4.1421514 -4.1875734 -4.2213063 -4.2402983][-4.216114 -4.2307405 -4.2500615 -4.2580676 -4.2490215 -4.225956 -4.1983976 -4.1769791 -4.1659236 -4.1753397 -4.1944809 -4.2094069 -4.2338681 -4.2502589 -4.2557168][-4.2453928 -4.2454491 -4.2578769 -4.2712212 -4.2775359 -4.2764606 -4.2692189 -4.2566147 -4.2393317 -4.2394056 -4.2529354 -4.2584705 -4.2683024 -4.2730956 -4.27021][-4.26874 -4.2591953 -4.2626314 -4.2732882 -4.284564 -4.2933655 -4.2936783 -4.2822528 -4.263514 -4.2598224 -4.2710319 -4.275723 -4.2831588 -4.2856164 -4.2808905][-4.2818322 -4.2744942 -4.2749066 -4.2772975 -4.2842736 -4.2921329 -4.2928333 -4.2796845 -4.2590775 -4.2517619 -4.2609234 -4.269311 -4.2814384 -4.2884336 -4.2865973][-4.286293 -4.2817311 -4.2809954 -4.2799239 -4.2824793 -4.2866755 -4.2851114 -4.2713985 -4.2506161 -4.240891 -4.2479687 -4.2611523 -4.2775855 -4.2883224 -4.2895584]]...]
INFO - root - 2017-12-07 23:52:53.602281: step 68410, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 47h:37m:25s remains)
INFO - root - 2017-12-07 23:53:00.493874: step 68420, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 52h:32m:25s remains)
INFO - root - 2017-12-07 23:53:07.280463: step 68430, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 53h:19m:35s remains)
INFO - root - 2017-12-07 23:53:14.034286: step 68440, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 47h:45m:29s remains)
INFO - root - 2017-12-07 23:53:20.778824: step 68450, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 46h:34m:03s remains)
INFO - root - 2017-12-07 23:53:27.463247: step 68460, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 48h:44m:01s remains)
INFO - root - 2017-12-07 23:53:34.235155: step 68470, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 53h:06m:03s remains)
INFO - root - 2017-12-07 23:53:40.858791: step 68480, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 48h:21m:54s remains)
INFO - root - 2017-12-07 23:53:47.598357: step 68490, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 46h:52m:57s remains)
INFO - root - 2017-12-07 23:53:54.256472: step 68500, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.660 sec/batch; 48h:22m:52s remains)
2017-12-07 23:53:55.109728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.228981 -4.2332044 -4.2395983 -4.2458978 -4.2521887 -4.2607741 -4.2676673 -4.2615366 -4.2362018 -4.194447 -4.1620593 -4.1531792 -4.1658306 -4.1819334 -4.1913762][-4.2388053 -4.2410812 -4.2379332 -4.2317591 -4.2310071 -4.2385592 -4.2463083 -4.24154 -4.2174773 -4.1783018 -4.1487274 -4.148066 -4.1729178 -4.2003374 -4.2130089][-4.251308 -4.253602 -4.2463965 -4.2345796 -4.2295995 -4.2351918 -4.2422438 -4.2389884 -4.21846 -4.1863413 -4.1594768 -4.1593618 -4.1862583 -4.2141104 -4.2233725][-4.2511234 -4.2578053 -4.2566648 -4.2455144 -4.2391119 -4.2405267 -4.242043 -4.2397075 -4.2287254 -4.211895 -4.194406 -4.1916523 -4.2095914 -4.2241273 -4.223175][-4.223403 -4.2416105 -4.2542968 -4.2467771 -4.2325211 -4.2189417 -4.2046494 -4.2013855 -4.2106957 -4.22098 -4.2207327 -4.219027 -4.2264085 -4.2262378 -4.2134948][-4.1517415 -4.1887379 -4.2236414 -4.2233658 -4.1978226 -4.159492 -4.1196785 -4.1100693 -4.1431122 -4.1879821 -4.2152719 -4.223702 -4.2212262 -4.2046638 -4.1835575][-4.0632563 -4.121798 -4.1765208 -4.1808739 -4.1409864 -4.0703177 -3.9964178 -3.9745979 -4.033041 -4.1201415 -4.1791244 -4.2034225 -4.19883 -4.1724558 -4.1473007][-4.0280733 -4.0883126 -4.142786 -4.1398783 -4.0819979 -3.9812827 -3.882709 -3.8491316 -3.9242487 -4.0439696 -4.134212 -4.1816134 -4.1855173 -4.1594019 -4.1366944][-4.0663118 -4.1061368 -4.1413321 -4.1291704 -4.0684719 -3.9740937 -3.8907518 -3.8679597 -3.9346817 -4.0450382 -4.1361446 -4.1900015 -4.199954 -4.1777573 -4.16016][-4.1414189 -4.1565485 -4.1670346 -4.1506028 -4.1079321 -4.0451832 -3.993531 -3.9853165 -4.0331445 -4.1083589 -4.174367 -4.2133632 -4.2219977 -4.2071486 -4.196075][-4.2166371 -4.2106805 -4.2029657 -4.1889195 -4.1664371 -4.1314883 -4.1031733 -4.0995674 -4.1273756 -4.1687627 -4.204248 -4.2267003 -4.234478 -4.2292271 -4.2264919][-4.2595654 -4.245698 -4.23276 -4.2252579 -4.2173028 -4.20111 -4.1855936 -4.1825242 -4.1951747 -4.2118711 -4.2234263 -4.2314482 -4.2388816 -4.2425613 -4.2472105][-4.2843218 -4.27242 -4.2622862 -4.2573447 -4.2526817 -4.2416878 -4.2317195 -4.2287097 -4.2324996 -4.2366886 -4.2377739 -4.2370152 -4.2387061 -4.2436228 -4.2522717][-4.3087869 -4.3001766 -4.2912087 -4.2847471 -4.2780824 -4.2673526 -4.2565918 -4.2498031 -4.2459192 -4.24562 -4.2445564 -4.2376008 -4.2334223 -4.2368145 -4.2455153][-4.3260965 -4.3185959 -4.3109636 -4.3057737 -4.3019261 -4.2943559 -4.2831569 -4.2708597 -4.2594914 -4.2562647 -4.2529798 -4.2415075 -4.2320104 -4.2311883 -4.2365875]]...]
INFO - root - 2017-12-07 23:54:01.700082: step 68510, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 45h:58m:18s remains)
INFO - root - 2017-12-07 23:54:08.498708: step 68520, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 47h:50m:23s remains)
INFO - root - 2017-12-07 23:54:15.354266: step 68530, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 54h:14m:43s remains)
INFO - root - 2017-12-07 23:54:22.133788: step 68540, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.757 sec/batch; 55h:29m:41s remains)
INFO - root - 2017-12-07 23:54:28.943436: step 68550, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.705 sec/batch; 51h:43m:18s remains)
INFO - root - 2017-12-07 23:54:35.672079: step 68560, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 45h:50m:37s remains)
INFO - root - 2017-12-07 23:54:42.454015: step 68570, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 49h:20m:06s remains)
INFO - root - 2017-12-07 23:54:49.256767: step 68580, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 50h:13m:04s remains)
INFO - root - 2017-12-07 23:54:56.131150: step 68590, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 51h:57m:26s remains)
INFO - root - 2017-12-07 23:55:02.671162: step 68600, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 49h:33m:22s remains)
2017-12-07 23:55:03.374813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2897024 -4.2938776 -4.3031592 -4.3093395 -4.3089337 -4.3016863 -4.2936335 -4.2895675 -4.2919555 -4.3034129 -4.3216615 -4.3340316 -4.3370013 -4.3323641 -4.3209028][-4.2976246 -4.3008437 -4.3081832 -4.3110085 -4.3057718 -4.2952929 -4.2845407 -4.2779584 -4.2768264 -4.28795 -4.3119435 -4.3300223 -4.3338914 -4.3270435 -4.311481][-4.2955647 -4.2937036 -4.2954774 -4.2926636 -4.2815886 -4.2659583 -4.2549238 -4.2494669 -4.2468839 -4.25754 -4.2841673 -4.3068342 -4.3132157 -4.3073092 -4.2914052][-4.2863221 -4.2760372 -4.26889 -4.2580662 -4.2430506 -4.228107 -4.2246156 -4.226285 -4.2241888 -4.22998 -4.2522464 -4.2740674 -4.2826271 -4.2803059 -4.26982][-4.2690096 -4.2512302 -4.2377977 -4.2243786 -4.2094235 -4.1979647 -4.1998191 -4.209075 -4.2102509 -4.2123656 -4.2267542 -4.2425237 -4.2481079 -4.248086 -4.2461038][-4.2453728 -4.2290425 -4.2122197 -4.1936345 -4.1765466 -4.1652822 -4.170433 -4.1892757 -4.2021842 -4.2070804 -4.2137008 -4.2205806 -4.2211814 -4.2228408 -4.2277641][-4.2231889 -4.2092738 -4.1891046 -4.1631417 -4.1363583 -4.1162858 -4.1216555 -4.1554437 -4.18858 -4.202126 -4.2029157 -4.2015467 -4.1989522 -4.2031837 -4.2146244][-4.2084141 -4.1969604 -4.1732435 -4.1367311 -4.0933938 -4.0619769 -4.0693874 -4.1169434 -4.1681809 -4.1929026 -4.1930041 -4.1881762 -4.1854677 -4.1921663 -4.2061758][-4.2035222 -4.1926169 -4.1682134 -4.1248331 -4.0689588 -4.02673 -4.0328689 -4.0867505 -4.1483183 -4.1830359 -4.188427 -4.1849833 -4.183197 -4.191545 -4.2073035][-4.2108269 -4.2012424 -4.176755 -4.1308165 -4.0698814 -4.0224686 -4.0221868 -4.0725102 -4.1332788 -4.1711187 -4.1844687 -4.1897173 -4.1921182 -4.2005305 -4.2127433][-4.2281442 -4.2197151 -4.1983695 -4.1566744 -4.0986342 -4.0494676 -4.0388293 -4.0754666 -4.1221552 -4.1561394 -4.1775026 -4.1955762 -4.2034011 -4.2104 -4.2169323][-4.23648 -4.2328773 -4.2189126 -4.1895914 -4.1440234 -4.0982494 -4.0770478 -4.0907025 -4.1160817 -4.13846 -4.1603193 -4.188684 -4.2043076 -4.2152476 -4.2240086][-4.2354093 -4.2343116 -4.2267356 -4.2074885 -4.17214 -4.1314764 -4.1040025 -4.1015863 -4.1071076 -4.1158357 -4.1359949 -4.1757331 -4.2077332 -4.226984 -4.2374039][-4.2134542 -4.2100658 -4.2053719 -4.1945734 -4.1710067 -4.1394095 -4.1112809 -4.0987453 -4.0869341 -4.0829725 -4.103302 -4.1525931 -4.1992912 -4.2256989 -4.2367635][-4.1760774 -4.1721687 -4.1687651 -4.16314 -4.1499572 -4.128108 -4.0994592 -4.0802412 -4.0583386 -4.0451541 -4.0634937 -4.11794 -4.1735315 -4.2067733 -4.2217188]]...]
INFO - root - 2017-12-07 23:55:10.137856: step 68610, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 52h:16m:43s remains)
INFO - root - 2017-12-07 23:55:16.947678: step 68620, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 50h:31m:50s remains)
INFO - root - 2017-12-07 23:55:23.699264: step 68630, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.693 sec/batch; 50h:46m:33s remains)
INFO - root - 2017-12-07 23:55:30.384428: step 68640, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 46h:14m:07s remains)
INFO - root - 2017-12-07 23:55:37.253422: step 68650, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 49h:34m:04s remains)
INFO - root - 2017-12-07 23:55:44.077304: step 68660, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 53h:28m:29s remains)
INFO - root - 2017-12-07 23:55:50.944097: step 68670, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 51h:50m:53s remains)
INFO - root - 2017-12-07 23:55:57.762417: step 68680, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.618 sec/batch; 45h:17m:32s remains)
INFO - root - 2017-12-07 23:56:04.528715: step 68690, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 47h:09m:08s remains)
INFO - root - 2017-12-07 23:56:11.119303: step 68700, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 49h:10m:06s remains)
2017-12-07 23:56:11.884284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23378 -4.2258863 -4.2350717 -4.2559304 -4.266665 -4.2609186 -4.2478542 -4.2354035 -4.231741 -4.2320457 -4.2387753 -4.2551293 -4.2733917 -4.2932043 -4.3183231][-4.2005816 -4.2048931 -4.2327452 -4.268023 -4.2814322 -4.2720942 -4.2519078 -4.2331095 -4.2259459 -4.2233772 -4.2262135 -4.2426147 -4.2619781 -4.2814188 -4.30856][-4.1836843 -4.2064667 -4.2517824 -4.2934303 -4.3035502 -4.2866664 -4.256393 -4.2269669 -4.2120752 -4.2033916 -4.2025518 -4.2202091 -4.2425957 -4.2649379 -4.2959962][-4.1857667 -4.2242727 -4.2779789 -4.317471 -4.3221254 -4.2952352 -4.250268 -4.2013488 -4.1678357 -4.1488233 -4.1475258 -4.1750216 -4.2086182 -4.2425294 -4.2831583][-4.2055235 -4.2516422 -4.302763 -4.3353672 -4.3320789 -4.2905312 -4.2203922 -4.1402855 -4.0826664 -4.0533071 -4.0624 -4.1148334 -4.171052 -4.2239065 -4.2760825][-4.2396407 -4.2824225 -4.3211951 -4.3391061 -4.3207498 -4.258707 -4.1617255 -4.0509744 -3.9776514 -3.9567931 -3.9956222 -4.080761 -4.1595378 -4.2248845 -4.2804775][-4.2683535 -4.2985239 -4.3198147 -4.32074 -4.2864513 -4.2076406 -4.0932631 -3.9677346 -3.9011803 -3.9136686 -3.989578 -4.0958924 -4.1811209 -4.2444468 -4.2927308][-4.2895818 -4.3076568 -4.3126779 -4.2981248 -4.2540269 -4.1708364 -4.0580549 -3.9476097 -3.9076798 -3.9497805 -4.0401134 -4.1394506 -4.2113023 -4.26141 -4.2979426][-4.293951 -4.305511 -4.3008876 -4.2793236 -4.2334151 -4.1521564 -4.0537491 -3.972574 -3.959892 -4.0144625 -4.0906491 -4.1647639 -4.2187796 -4.259479 -4.2911868][-4.2904234 -4.2962909 -4.2861867 -4.2609277 -4.2167315 -4.1447315 -4.0669441 -4.01388 -4.0160909 -4.063025 -4.1139688 -4.1639371 -4.2073307 -4.2414689 -4.2735672][-4.2879267 -4.2852154 -4.2685022 -4.2419391 -4.2041278 -4.1476049 -4.092401 -4.0559378 -4.0596433 -4.0894151 -4.11704 -4.1516285 -4.1916776 -4.2242346 -4.2600613][-4.2900281 -4.2787004 -4.2563467 -4.232491 -4.2049723 -4.1658545 -4.1287293 -4.1022172 -4.1035614 -4.1169386 -4.1282125 -4.1556344 -4.1977935 -4.2326336 -4.2696667][-4.2997546 -4.2864537 -4.2666168 -4.2515554 -4.2360168 -4.21395 -4.1896296 -4.1697092 -4.1693034 -4.1711583 -4.174222 -4.1983409 -4.2385325 -4.2710104 -4.3000312][-4.3084941 -4.2986112 -4.2850242 -4.27795 -4.2715082 -4.2611136 -4.2447872 -4.2316508 -4.2338758 -4.2356925 -4.2396851 -4.2608023 -4.2919054 -4.31504 -4.332438][-4.3141317 -4.3063273 -4.2979126 -4.2952561 -4.2947578 -4.2915988 -4.282423 -4.2741895 -4.2778277 -4.2811022 -4.2868977 -4.3055358 -4.3274851 -4.3425879 -4.3523626]]...]
INFO - root - 2017-12-07 23:56:18.768262: step 68710, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.620 sec/batch; 45h:26m:13s remains)
INFO - root - 2017-12-07 23:56:25.558058: step 68720, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 46h:58m:25s remains)
INFO - root - 2017-12-07 23:56:32.370000: step 68730, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 50h:31m:22s remains)
INFO - root - 2017-12-07 23:56:39.192473: step 68740, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 52h:02m:27s remains)
INFO - root - 2017-12-07 23:56:45.972764: step 68750, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 49h:36m:34s remains)
INFO - root - 2017-12-07 23:56:52.684552: step 68760, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 46h:00m:16s remains)
INFO - root - 2017-12-07 23:56:59.470130: step 68770, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 47h:05m:56s remains)
INFO - root - 2017-12-07 23:57:06.311789: step 68780, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 52h:48m:48s remains)
INFO - root - 2017-12-07 23:57:12.920281: step 68790, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.713 sec/batch; 52h:12m:08s remains)
INFO - root - 2017-12-07 23:57:19.533264: step 68800, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 47h:23m:59s remains)
2017-12-07 23:57:20.242344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2280626 -4.2053328 -4.1864839 -4.1833472 -4.1955271 -4.2050786 -4.1924152 -4.1615081 -4.1393552 -4.1470766 -4.1813107 -4.2092738 -4.2117267 -4.2043386 -4.2025614][-4.219553 -4.201571 -4.187644 -4.185122 -4.1971517 -4.210453 -4.2020249 -4.1721697 -4.1438761 -4.1425428 -4.17403 -4.1990523 -4.197556 -4.1862922 -4.1805086][-4.2049069 -4.1977425 -4.1880412 -4.1831965 -4.1934533 -4.2113757 -4.21296 -4.1945596 -4.1697431 -4.1645384 -4.1884146 -4.2049079 -4.19808 -4.1836133 -4.1720858][-4.1552439 -4.1612763 -4.1600823 -4.1584973 -4.1724629 -4.1971192 -4.2086091 -4.2033195 -4.1906772 -4.1910129 -4.211803 -4.224402 -4.21751 -4.2021651 -4.1846666][-4.08971 -4.1034884 -4.113173 -4.1201568 -4.1402349 -4.1686516 -4.1856623 -4.1906347 -4.1902075 -4.1968927 -4.2154918 -4.227891 -4.2252154 -4.2111282 -4.1901431][-4.06839 -4.0883751 -4.1083765 -4.1208367 -4.13713 -4.1527762 -4.1607118 -4.1586318 -4.1588659 -4.1733923 -4.1986542 -4.2131433 -4.2145119 -4.1973424 -4.1683383][-4.0886312 -4.1101394 -4.1342435 -4.1469555 -4.1523933 -4.1456957 -4.1272373 -4.0999255 -4.0851369 -4.1046844 -4.1459622 -4.1726089 -4.1773195 -4.155375 -4.1175075][-4.10248 -4.11453 -4.1354957 -4.1473937 -4.1444569 -4.1182928 -4.0676126 -4.00016 -3.948761 -3.9706106 -4.0465484 -4.1045747 -4.12656 -4.1146011 -4.08606][-4.112709 -4.1060057 -4.1148977 -4.122602 -4.1116233 -4.07079 -3.9945772 -3.8916769 -3.8032036 -3.8228033 -3.9385254 -4.0356655 -4.0871153 -4.0999188 -4.0957069][-4.1337829 -4.1145635 -4.1089454 -4.1105146 -4.09872 -4.0607285 -3.9917121 -3.8966265 -3.8115618 -3.8243728 -3.9333851 -4.0321374 -4.0949731 -4.1241016 -4.1359758][-4.169816 -4.1437979 -4.1288419 -4.1257553 -4.12091 -4.1007767 -4.0594592 -3.9991429 -3.9444723 -3.9535756 -4.0250826 -4.0975285 -4.1508536 -4.1809464 -4.1931434][-4.213069 -4.1861219 -4.1692615 -4.1647081 -4.1660609 -4.1622109 -4.1451321 -4.1165791 -4.0890975 -4.0917158 -4.1288633 -4.1749549 -4.2130804 -4.23437 -4.2395329][-4.25049 -4.2305851 -4.2194796 -4.2180443 -4.2216129 -4.2255783 -4.2230635 -4.2135291 -4.1991 -4.1954703 -4.2091956 -4.2322869 -4.253643 -4.2655067 -4.2683644][-4.2822289 -4.2709308 -4.2652421 -4.2640281 -4.2662358 -4.2711678 -4.27288 -4.2702036 -4.2617536 -4.2563362 -4.2603021 -4.2709146 -4.2812757 -4.2865767 -4.2880259][-4.3061771 -4.2975655 -4.29372 -4.2922859 -4.2930393 -4.2967844 -4.2991924 -4.2992663 -4.2963281 -4.2919803 -4.2909584 -4.2943397 -4.298368 -4.3005381 -4.3013282]]...]
INFO - root - 2017-12-07 23:57:26.916547: step 68810, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 51h:43m:19s remains)
INFO - root - 2017-12-07 23:57:33.663294: step 68820, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 50h:22m:13s remains)
INFO - root - 2017-12-07 23:57:40.348623: step 68830, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 45h:20m:39s remains)
INFO - root - 2017-12-07 23:57:47.128990: step 68840, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 47h:33m:52s remains)
INFO - root - 2017-12-07 23:57:53.884113: step 68850, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 52h:48m:00s remains)
INFO - root - 2017-12-07 23:58:00.715829: step 68860, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 50h:16m:24s remains)
INFO - root - 2017-12-07 23:58:07.433045: step 68870, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 46h:25m:45s remains)
INFO - root - 2017-12-07 23:58:14.198666: step 68880, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 45h:52m:02s remains)
INFO - root - 2017-12-07 23:58:20.952166: step 68890, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 48h:34m:58s remains)
INFO - root - 2017-12-07 23:58:27.535057: step 68900, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 51h:15m:42s remains)
2017-12-07 23:58:28.265418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2664657 -4.2677674 -4.2826748 -4.2901912 -4.2875323 -4.2614737 -4.2081528 -4.1436629 -4.090014 -4.08635 -4.1349125 -4.2040024 -4.2532454 -4.2634888 -4.255012][-4.2736139 -4.272716 -4.2826166 -4.2832174 -4.2704372 -4.2360311 -4.1789865 -4.1187973 -4.0728788 -4.0712776 -4.1215224 -4.1964378 -4.2536058 -4.2679753 -4.2594891][-4.2826633 -4.2827334 -4.2902737 -4.2874322 -4.2658191 -4.2233953 -4.1672049 -4.1162276 -4.0816894 -4.082685 -4.1312885 -4.2039671 -4.2618375 -4.2785821 -4.2740231][-4.2989869 -4.3005929 -4.3043451 -4.2985134 -4.2718415 -4.2228966 -4.1659551 -4.1171904 -4.0866885 -4.0853896 -4.1298356 -4.2008104 -4.2642555 -4.289875 -4.2909861][-4.3165784 -4.3181748 -4.3151445 -4.3044448 -4.2745385 -4.2200546 -4.1559105 -4.0973396 -4.0559387 -4.0481076 -4.0911226 -4.1666512 -4.2425008 -4.2846475 -4.2939343][-4.3228436 -4.3244762 -4.3175073 -4.3039775 -4.2738194 -4.2167315 -4.1414752 -4.0672903 -4.0100832 -3.9940464 -4.0398631 -4.122684 -4.2093253 -4.2671046 -4.2889118][-4.3209119 -4.324048 -4.3172293 -4.3053041 -4.280045 -4.2264156 -4.1466222 -4.0615635 -3.99394 -3.9693503 -4.0114613 -4.0924459 -4.1816525 -4.2507243 -4.2869649][-4.3196015 -4.3263741 -4.3224549 -4.3146815 -4.2960296 -4.2514811 -4.1781735 -4.0936246 -4.0174851 -3.9783289 -4.005465 -4.0785923 -4.1662054 -4.2425375 -4.2901797][-4.3176022 -4.3314157 -4.3312168 -4.3270369 -4.3158927 -4.2840562 -4.2208529 -4.1415639 -4.061799 -4.0101719 -4.0194755 -4.0824013 -4.1641955 -4.2401528 -4.2927213][-4.3001904 -4.3251572 -4.3325009 -4.3331881 -4.3285012 -4.3073926 -4.2555175 -4.1860909 -4.1136584 -4.0656037 -4.0674176 -4.1162257 -4.1798134 -4.2385635 -4.2831149][-4.2663302 -4.3010716 -4.3175077 -4.3242493 -4.3242025 -4.3099227 -4.2699971 -4.2126222 -4.152843 -4.1168647 -4.1200323 -4.1557026 -4.1962147 -4.2311754 -4.2614079][-4.2327409 -4.2729774 -4.2951179 -4.3044548 -4.304306 -4.2917862 -4.2578359 -4.2057967 -4.1538916 -4.1271472 -4.1349835 -4.1659603 -4.1957741 -4.2157378 -4.2352057][-4.2187233 -4.2604146 -4.2831855 -4.2900567 -4.2844334 -4.2678423 -4.2340121 -4.1821136 -4.1333976 -4.110548 -4.1217289 -4.1541471 -4.1831913 -4.1956124 -4.2058282][-4.2218 -4.2611685 -4.28088 -4.2828264 -4.2728062 -4.2544932 -4.2236 -4.1755776 -4.1319938 -4.1124163 -4.1264639 -4.158699 -4.1821108 -4.1817985 -4.1780882][-4.2281518 -4.2661638 -4.284586 -4.2832284 -4.2702818 -4.2525868 -4.2267303 -4.1851087 -4.1489725 -4.1332612 -4.1473436 -4.1734734 -4.1866097 -4.1735086 -4.1549473]]...]
INFO - root - 2017-12-07 23:58:35.017959: step 68910, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 47h:08m:01s remains)
INFO - root - 2017-12-07 23:58:41.799813: step 68920, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.620 sec/batch; 45h:24m:04s remains)
INFO - root - 2017-12-07 23:58:48.576215: step 68930, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 52h:22m:22s remains)
INFO - root - 2017-12-07 23:58:55.414919: step 68940, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 52h:17m:19s remains)
INFO - root - 2017-12-07 23:59:02.143295: step 68950, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 47h:28m:23s remains)
INFO - root - 2017-12-07 23:59:08.948797: step 68960, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 49h:34m:05s remains)
INFO - root - 2017-12-07 23:59:15.733103: step 68970, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 50h:43m:16s remains)
INFO - root - 2017-12-07 23:59:22.559342: step 68980, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 53h:11m:13s remains)
INFO - root - 2017-12-07 23:59:29.370575: step 68990, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 51h:46m:48s remains)
INFO - root - 2017-12-07 23:59:36.041448: step 69000, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 45h:50m:04s remains)
2017-12-07 23:59:36.801713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.36324 -4.3612986 -4.3567433 -4.3532333 -4.3501058 -4.3472562 -4.3430896 -4.33812 -4.3339911 -4.3321109 -4.3318157 -4.3325677 -4.3341184 -4.3368073 -4.3404236][-4.3689556 -4.3662691 -4.3604851 -4.3550959 -4.35 -4.3436341 -4.3343282 -4.3241572 -4.3181734 -4.3163967 -4.3177614 -4.3231258 -4.3284664 -4.3343949 -4.3402925][-4.363863 -4.3598943 -4.3517613 -4.3416314 -4.3297749 -4.3133574 -4.29334 -4.2753692 -4.2698417 -4.2734509 -4.2834182 -4.2977104 -4.3087745 -4.3186579 -4.32938][-4.3409085 -4.3371572 -4.3275614 -4.3125992 -4.2861791 -4.2497711 -4.2119255 -4.1834097 -4.1782541 -4.1940742 -4.2221732 -4.25234 -4.27273 -4.2881565 -4.3054957][-4.3033767 -4.2976437 -4.287539 -4.267839 -4.2257609 -4.1674571 -4.1043348 -4.0577941 -4.0533385 -4.0904241 -4.1403775 -4.1870322 -4.2198129 -4.2460856 -4.2712059][-4.2674003 -4.2580824 -4.2465315 -4.2202072 -4.1625614 -4.0803943 -3.986269 -3.9103558 -3.90437 -3.9757082 -4.0607271 -4.1275406 -4.1739278 -4.2122507 -4.2429657][-4.2539015 -4.2423615 -4.2245922 -4.182354 -4.1018176 -3.9935262 -3.8665111 -3.7571611 -3.756357 -3.8808651 -4.0119438 -4.0991807 -4.1574073 -4.20229 -4.2334423][-4.2616572 -4.2430115 -4.2138677 -4.1570897 -4.0604067 -3.9382432 -3.7993259 -3.6898744 -3.7032928 -3.8511703 -4.0012589 -4.1004972 -4.1658854 -4.2082157 -4.2349186][-4.2628365 -4.241271 -4.208241 -4.152647 -4.0688829 -3.9693236 -3.8680341 -3.8075466 -3.8254476 -3.9290452 -4.0439334 -4.129159 -4.1890025 -4.2205653 -4.2401628][-4.2607322 -4.2363739 -4.2061143 -4.1694489 -4.12163 -4.0610185 -4.0078878 -3.983532 -3.98602 -4.0369978 -4.1055317 -4.1614113 -4.2003574 -4.2191558 -4.2326922][-4.2492995 -4.218482 -4.1933417 -4.1849937 -4.1646285 -4.1297889 -4.1071863 -4.1027231 -4.0932178 -4.1074038 -4.1384439 -4.1643229 -4.1823378 -4.1902237 -4.2011313][-4.2297983 -4.1886473 -4.1680779 -4.1814475 -4.1780124 -4.159018 -4.158546 -4.1658521 -4.1527514 -4.1422143 -4.1450534 -4.150918 -4.1553636 -4.1574955 -4.1703658][-4.2071352 -4.159462 -4.14584 -4.1752639 -4.1824431 -4.1754184 -4.1853127 -4.1932187 -4.1777487 -4.1561394 -4.1482577 -4.14558 -4.14572 -4.149992 -4.1688175][-4.1885376 -4.1441579 -4.1427507 -4.18253 -4.196753 -4.1967421 -4.2074003 -4.21103 -4.1969852 -4.1795287 -4.1722751 -4.1668453 -4.1678829 -4.1779323 -4.2007327][-4.2025971 -4.1719875 -4.1799564 -4.2187963 -4.2340865 -4.2399173 -4.248661 -4.246664 -4.2344851 -4.2210064 -4.2141533 -4.2075806 -4.2089119 -4.2205982 -4.2409592]]...]
INFO - root - 2017-12-07 23:59:43.662327: step 69010, loss = 2.03, batch loss = 1.98 (10.6 examples/sec; 0.753 sec/batch; 55h:05m:35s remains)
INFO - root - 2017-12-07 23:59:50.499785: step 69020, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 48h:59m:12s remains)
INFO - root - 2017-12-07 23:59:57.294166: step 69030, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 46h:03m:54s remains)
INFO - root - 2017-12-08 00:00:04.173232: step 69040, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 50h:01m:24s remains)
INFO - root - 2017-12-08 00:00:10.956048: step 69050, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 52h:13m:57s remains)
INFO - root - 2017-12-08 00:00:17.853944: step 69060, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 54h:49m:29s remains)
INFO - root - 2017-12-08 00:00:24.541554: step 69070, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 49h:16m:51s remains)
INFO - root - 2017-12-08 00:00:31.350586: step 69080, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 47h:03m:36s remains)
INFO - root - 2017-12-08 00:00:38.225781: step 69090, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 48h:37m:56s remains)
INFO - root - 2017-12-08 00:00:44.652203: step 69100, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 50h:07m:31s remains)
2017-12-08 00:00:45.402295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2292128 -4.2340531 -4.23413 -4.2248974 -4.2098389 -4.2017369 -4.1973619 -4.1908484 -4.1990561 -4.2163038 -4.2246575 -4.2194562 -4.2171612 -4.2000718 -4.1787214][-4.2203059 -4.2254415 -4.2297292 -4.2224522 -4.2101865 -4.203208 -4.1983604 -4.191978 -4.1986918 -4.2158422 -4.2271872 -4.2267356 -4.22915 -4.2195349 -4.2010126][-4.2292809 -4.2384372 -4.2462025 -4.2441292 -4.23543 -4.22674 -4.217279 -4.212718 -4.2205286 -4.2376733 -4.2527184 -4.2574 -4.2615428 -4.2562323 -4.2403574][-4.2503929 -4.2625089 -4.2715287 -4.2682705 -4.2584705 -4.2445364 -4.2279077 -4.2235551 -4.2353191 -4.2536836 -4.2730217 -4.2812023 -4.285326 -4.2793574 -4.2648106][-4.2579832 -4.2782812 -4.2946873 -4.2885427 -4.2710795 -4.2437482 -4.2161202 -4.2068067 -4.219687 -4.24094 -4.2679434 -4.2834806 -4.2879295 -4.2774606 -4.2612338][-4.2203245 -4.251462 -4.2742963 -4.2669268 -4.237771 -4.1886415 -4.1392074 -4.1197777 -4.145298 -4.1881232 -4.2327738 -4.2625856 -4.2737465 -4.26169 -4.2442222][-4.1486855 -4.1950831 -4.2227144 -4.2141371 -4.167305 -4.0856957 -3.9978395 -3.9595189 -4.0091181 -4.0942631 -4.1689749 -4.2177992 -4.2442226 -4.2383261 -4.2248154][-4.0605764 -4.1252947 -4.1541753 -4.1412859 -4.078434 -3.9664435 -3.8368316 -3.775322 -3.8522651 -3.9864812 -4.0948968 -4.1632891 -4.2074995 -4.2118359 -4.2032623][-3.9887776 -4.0679822 -4.0923471 -4.0715551 -4.008009 -3.8921628 -3.7435157 -3.6584678 -3.7391243 -3.8996711 -4.0373449 -4.124063 -4.1810417 -4.1952114 -4.1915812][-4.0303249 -4.1018038 -4.1174026 -4.0978317 -4.054523 -3.9742839 -3.8626118 -3.7801404 -3.816277 -3.943054 -4.068769 -4.1477652 -4.1977539 -4.2079363 -4.2050381][-4.1394238 -4.1857724 -4.1896439 -4.1717882 -4.1512494 -4.115119 -4.0619621 -4.0048289 -4.0043974 -4.0764093 -4.1537762 -4.197042 -4.2210894 -4.2176723 -4.2087011][-4.2011876 -4.2272062 -4.2271805 -4.2134995 -4.2067661 -4.1954303 -4.1748433 -4.1425476 -4.1269226 -4.1632056 -4.202352 -4.214036 -4.2148452 -4.1961989 -4.1831121][-4.1937184 -4.210887 -4.2156482 -4.2111092 -4.2110209 -4.2087216 -4.1999216 -4.1822014 -4.1642733 -4.1816306 -4.2032523 -4.2000184 -4.1870131 -4.1533089 -4.1335039][-4.1632652 -4.1743774 -4.1815662 -4.18538 -4.1889448 -4.1894693 -4.1846504 -4.1762052 -4.1644692 -4.1732926 -4.1861997 -4.1765919 -4.1573377 -4.1146655 -4.0859966][-4.1495237 -4.1541038 -4.161633 -4.166122 -4.1670389 -4.1642127 -4.1604261 -4.1562619 -4.1514883 -4.1608243 -4.1730404 -4.1677656 -4.1502714 -4.1065054 -4.0696816]]...]
INFO - root - 2017-12-08 00:00:52.229359: step 69110, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 47h:18m:19s remains)
INFO - root - 2017-12-08 00:00:59.089010: step 69120, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.699 sec/batch; 51h:06m:42s remains)
INFO - root - 2017-12-08 00:01:05.983983: step 69130, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 52h:17m:14s remains)
INFO - root - 2017-12-08 00:01:12.771766: step 69140, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 47h:35m:42s remains)
INFO - root - 2017-12-08 00:01:19.572603: step 69150, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 47h:46m:53s remains)
INFO - root - 2017-12-08 00:01:26.471525: step 69160, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.689 sec/batch; 50h:23m:12s remains)
INFO - root - 2017-12-08 00:01:33.411974: step 69170, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.737 sec/batch; 53h:52m:45s remains)
INFO - root - 2017-12-08 00:01:40.171904: step 69180, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 48h:38m:40s remains)
INFO - root - 2017-12-08 00:01:46.978927: step 69190, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 48h:02m:42s remains)
INFO - root - 2017-12-08 00:01:53.731289: step 69200, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 46h:02m:10s remains)
2017-12-08 00:01:54.497812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2682033 -4.2738409 -4.2782803 -4.281312 -4.2813377 -4.2783046 -4.2753205 -4.2791786 -4.2868114 -4.2950406 -4.2997203 -4.2914596 -4.26511 -4.2432714 -4.2466183][-4.265233 -4.2695179 -4.2761149 -4.2828379 -4.2855411 -4.2844687 -4.2840438 -4.2899752 -4.2998352 -4.3114014 -4.3193278 -4.3127556 -4.2841578 -4.2595558 -4.2583385][-4.2419839 -4.2457757 -4.2539473 -4.2654171 -4.2717166 -4.271358 -4.2712235 -4.277462 -4.2868133 -4.2996798 -4.3108807 -4.3077326 -4.2793031 -4.2511649 -4.2451777][-4.2050261 -4.2110496 -4.2223635 -4.2379661 -4.2464933 -4.2443419 -4.2392464 -4.2423072 -4.24979 -4.2616982 -4.2763648 -4.2793212 -4.2545819 -4.2234254 -4.2110896][-4.1500726 -4.1601481 -4.1751528 -4.1921854 -4.1972079 -4.1867685 -4.1700273 -4.1649613 -4.171886 -4.1852145 -4.2050929 -4.2152386 -4.1957173 -4.1640844 -4.1507821][-4.0947428 -4.1072845 -4.1239467 -4.1380858 -4.1333742 -4.10581 -4.0689068 -4.0497952 -4.0573082 -4.0758028 -4.1041741 -4.1250606 -4.1164894 -4.0920486 -4.0838604][-4.0849652 -4.0838532 -4.0837688 -4.0785036 -4.0547481 -4.0078106 -3.9509978 -3.9193382 -3.9355202 -3.9695346 -4.01211 -4.0479479 -4.0605288 -4.0557637 -4.0598159][-4.1340952 -4.109561 -4.0813537 -4.0486512 -4.0031414 -3.9412584 -3.8714576 -3.8321919 -3.8580425 -3.9052076 -3.9570613 -4.003931 -4.0393949 -4.0600662 -4.0826507][-4.219738 -4.185369 -4.1455307 -4.1050668 -4.058219 -4.0035672 -3.9444902 -3.9057012 -3.9186051 -3.9521914 -3.9913628 -4.0312777 -4.072639 -4.1052513 -4.1372128][-4.2747169 -4.2481704 -4.2191505 -4.1958795 -4.1695466 -4.1369529 -4.0993791 -4.0653534 -4.0609527 -4.0750241 -4.0948758 -4.1172905 -4.1471457 -4.1735392 -4.2018237][-4.2724504 -4.2571306 -4.2460423 -4.2456665 -4.2450724 -4.2366877 -4.2191572 -4.1965256 -4.1887708 -4.1980195 -4.2105241 -4.2233586 -4.2391367 -4.2532668 -4.269105][-4.2093749 -4.2031879 -4.2046509 -4.2233367 -4.2470551 -4.263031 -4.267478 -4.2625022 -4.26331 -4.2783294 -4.29457 -4.3044453 -4.3088913 -4.310051 -4.3137336][-4.1073551 -4.1019783 -4.1075115 -4.1410432 -4.18689 -4.2266207 -4.252521 -4.2638192 -4.2762308 -4.300344 -4.3265696 -4.3418384 -4.3431907 -4.3369508 -4.3330007][-4.02732 -4.0155878 -4.0175452 -4.057734 -4.1180134 -4.1738272 -4.2135186 -4.2360096 -4.25885 -4.2919426 -4.3245773 -4.3428211 -4.3433938 -4.3361773 -4.3305721][-4.0319562 -4.0188284 -4.0168271 -4.0516348 -4.108036 -4.16217 -4.2016325 -4.2256985 -4.2500176 -4.283741 -4.3169403 -4.3361163 -4.3371925 -4.3302813 -4.3250747]]...]
INFO - root - 2017-12-08 00:02:01.272550: step 69210, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 50h:04m:50s remains)
INFO - root - 2017-12-08 00:02:08.183243: step 69220, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 51h:28m:55s remains)
INFO - root - 2017-12-08 00:02:14.982167: step 69230, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 49h:39m:28s remains)
INFO - root - 2017-12-08 00:02:21.849179: step 69240, loss = 2.03, batch loss = 1.97 (11.0 examples/sec; 0.728 sec/batch; 53h:13m:25s remains)
INFO - root - 2017-12-08 00:02:28.714843: step 69250, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.719 sec/batch; 52h:35m:00s remains)
INFO - root - 2017-12-08 00:02:35.538055: step 69260, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 48h:51m:58s remains)
INFO - root - 2017-12-08 00:02:42.348729: step 69270, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 48h:11m:20s remains)
INFO - root - 2017-12-08 00:02:49.237023: step 69280, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 49h:10m:43s remains)
INFO - root - 2017-12-08 00:02:56.128664: step 69290, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 53h:25m:13s remains)
INFO - root - 2017-12-08 00:03:02.713945: step 69300, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 49h:15m:44s remains)
2017-12-08 00:03:03.463050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1801367 -4.165257 -4.1428332 -4.1254716 -4.1156921 -4.1246967 -4.1552234 -4.193943 -4.2257247 -4.2463479 -4.2560039 -4.2593532 -4.2536922 -4.2365112 -4.2136593][-4.227119 -4.2014055 -4.1619897 -4.1304197 -4.1187172 -4.1322217 -4.1644368 -4.1992908 -4.2300777 -4.2520862 -4.2629461 -4.2691116 -4.2707028 -4.2661428 -4.2543507][-4.2483191 -4.2144423 -4.1646185 -4.1317816 -4.1275091 -4.1491995 -4.1848259 -4.2187209 -4.2451005 -4.2654986 -4.2751822 -4.2803183 -4.2831531 -4.2852492 -4.2844586][-4.2518315 -4.2080169 -4.1518288 -4.1206145 -4.1226363 -4.1492734 -4.1817455 -4.2149448 -4.2420092 -4.2622242 -4.2743568 -4.278852 -4.2813716 -4.28695 -4.2912626][-4.2328115 -4.1844726 -4.1355648 -4.1138592 -4.1162238 -4.1355329 -4.1518874 -4.1675568 -4.1854029 -4.20516 -4.2260933 -4.2382355 -4.2468562 -4.2600784 -4.2683234][-4.2103839 -4.1729407 -4.1435041 -4.141026 -4.1399627 -4.1357703 -4.1156788 -4.0930157 -4.0927505 -4.117774 -4.1511412 -4.1675019 -4.1737208 -4.1860232 -4.192359][-4.205884 -4.1890512 -4.1710391 -4.1651683 -4.1474719 -4.1122007 -4.0434008 -3.9746025 -3.9687166 -4.0188994 -4.0755048 -4.0994706 -4.1045151 -4.1108384 -4.1092591][-4.2168946 -4.213841 -4.1943674 -4.1688662 -4.1255188 -4.0546136 -3.9397216 -3.8371644 -3.8567405 -3.9556224 -4.0402565 -4.0826468 -4.1005206 -4.1064906 -4.0948472][-4.2294588 -4.2364583 -4.2148018 -4.1780858 -4.1284704 -4.0502667 -3.9368823 -3.8437757 -3.8787367 -3.9765351 -4.0518646 -4.090816 -4.1114712 -4.1177163 -4.1069012][-4.2427416 -4.259037 -4.2372971 -4.2009888 -4.1624022 -4.1075807 -4.0382161 -3.9835582 -4.0114894 -4.0658622 -4.0973916 -4.1125011 -4.1221037 -4.12307 -4.1149607][-4.2259245 -4.2511468 -4.2330265 -4.1959076 -4.1695042 -4.1400719 -4.1058192 -4.0770764 -4.0990348 -4.1262431 -4.1342063 -4.1354561 -4.1350932 -4.1362352 -4.1390839][-4.2061019 -4.2343745 -4.2208185 -4.1916709 -4.1817703 -4.1706643 -4.1545525 -4.136826 -4.1542559 -4.1673455 -4.1637454 -4.1571169 -4.1558986 -4.1648026 -4.1812][-4.2056537 -4.2314787 -4.2281947 -4.2150774 -4.2232523 -4.2255735 -4.2209678 -4.2075391 -4.2144856 -4.2149568 -4.1991425 -4.1825981 -4.1833091 -4.1993675 -4.2238503][-4.1998777 -4.2178788 -4.2235494 -4.2214088 -4.23315 -4.2427392 -4.243886 -4.2323194 -4.232697 -4.2295609 -4.2152004 -4.2003522 -4.2045236 -4.2245564 -4.2485752][-4.1935186 -4.2072129 -4.2192993 -4.2181845 -4.2248459 -4.2371325 -4.2388163 -4.2256713 -4.2187562 -4.2120581 -4.2069082 -4.2035069 -4.2134728 -4.2339926 -4.258152]]...]
INFO - root - 2017-12-08 00:03:10.389246: step 69310, loss = 2.09, batch loss = 2.04 (11.0 examples/sec; 0.726 sec/batch; 53h:02m:57s remains)
INFO - root - 2017-12-08 00:03:17.252610: step 69320, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.744 sec/batch; 54h:21m:18s remains)
INFO - root - 2017-12-08 00:03:24.071725: step 69330, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 51h:29m:40s remains)
INFO - root - 2017-12-08 00:03:30.829315: step 69340, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 46h:15m:44s remains)
INFO - root - 2017-12-08 00:03:37.717419: step 69350, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.667 sec/batch; 48h:43m:40s remains)
INFO - root - 2017-12-08 00:03:44.550816: step 69360, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.721 sec/batch; 52h:44m:14s remains)
INFO - root - 2017-12-08 00:03:51.310426: step 69370, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.758 sec/batch; 55h:22m:23s remains)
INFO - root - 2017-12-08 00:03:58.104131: step 69380, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 46h:56m:37s remains)
INFO - root - 2017-12-08 00:04:04.847621: step 69390, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 46h:31m:54s remains)
INFO - root - 2017-12-08 00:04:11.493946: step 69400, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 51h:41m:17s remains)
2017-12-08 00:04:12.246833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3538294 -4.3434138 -4.32974 -4.3165388 -4.3006716 -4.2927055 -4.2939291 -4.2941284 -4.2832975 -4.2594881 -4.2254596 -4.1950579 -4.1798038 -4.1901 -4.2261863][-4.362114 -4.3557534 -4.3410411 -4.3247342 -4.3102169 -4.30331 -4.2989988 -4.2896838 -4.2761865 -4.2567987 -4.2266994 -4.1993027 -4.1846309 -4.1964521 -4.2323146][-4.36339 -4.3629608 -4.3515935 -4.3341126 -4.3178129 -4.3049192 -4.2894864 -4.2703438 -4.2562094 -4.24676 -4.2301373 -4.2166948 -4.2084227 -4.2184381 -4.2464247][-4.3560739 -4.3612814 -4.35444 -4.3329492 -4.3051391 -4.2770777 -4.2437263 -4.2142205 -4.2010956 -4.2064795 -4.2136316 -4.2251339 -4.2321692 -4.2397318 -4.257081][-4.3386092 -4.3492155 -4.3453951 -4.3214965 -4.2793832 -4.2295814 -4.1707878 -4.1226192 -4.1101866 -4.1401343 -4.1802726 -4.2185521 -4.2417874 -4.2495294 -4.257277][-4.3155355 -4.3307209 -4.3297033 -4.3036184 -4.24925 -4.1713896 -4.0688276 -3.9860914 -3.9751842 -4.0403566 -4.1164794 -4.180397 -4.2184138 -4.2356052 -4.2400293][-4.284215 -4.3020988 -4.30167 -4.2705956 -4.2087054 -4.1040816 -3.9634855 -3.851773 -3.8480682 -3.9449093 -4.0439391 -4.1256771 -4.1796794 -4.2098413 -4.2154121][-4.2602453 -4.2739444 -4.274754 -4.2439938 -4.1840987 -4.0832191 -3.9470155 -3.8426881 -3.8369591 -3.9289117 -4.0237346 -4.09736 -4.1564021 -4.1949515 -4.1989603][-4.2418413 -4.2525325 -4.259346 -4.2397442 -4.1938138 -4.1144376 -4.0065265 -3.9230158 -3.9085181 -3.9680486 -4.0394926 -4.0935345 -4.1418209 -4.1820645 -4.1870451][-4.2336187 -4.2433095 -4.2550049 -4.25049 -4.2214532 -4.1637287 -4.0843449 -4.0233693 -4.0047398 -4.035603 -4.0809526 -4.1158838 -4.1474214 -4.17787 -4.1791286][-4.2440972 -4.2503462 -4.2615337 -4.2669325 -4.2556157 -4.2183967 -4.1646042 -4.1215563 -4.0959573 -4.0997024 -4.1215725 -4.1398587 -4.1563163 -4.1777773 -4.176939][-4.2605357 -4.2656622 -4.27411 -4.2814136 -4.2750106 -4.247303 -4.2092285 -4.1800251 -4.1548471 -4.1401234 -4.1427331 -4.1499219 -4.1616807 -4.1867285 -4.1942692][-4.2631545 -4.269371 -4.2754498 -4.2793388 -4.2732844 -4.2538729 -4.2324157 -4.2171712 -4.2004209 -4.1820126 -4.175045 -4.1725225 -4.177577 -4.2051167 -4.2267413][-4.2498541 -4.260056 -4.267005 -4.2680664 -4.2613988 -4.2511683 -4.2415147 -4.2384262 -4.2349825 -4.2215695 -4.2092524 -4.2024093 -4.2065549 -4.23355 -4.2602582][-4.2391539 -4.2519231 -4.2597814 -4.2598109 -4.2524166 -4.24693 -4.2456388 -4.2496176 -4.2536087 -4.2441716 -4.2308521 -4.2277055 -4.2362485 -4.2588172 -4.2845273]]...]
INFO - root - 2017-12-08 00:04:18.849645: step 69410, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 46h:52m:33s remains)
INFO - root - 2017-12-08 00:04:25.698329: step 69420, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 53h:27m:31s remains)
INFO - root - 2017-12-08 00:04:32.662779: step 69430, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 54h:16m:59s remains)
INFO - root - 2017-12-08 00:04:39.539272: step 69440, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 50h:58m:14s remains)
INFO - root - 2017-12-08 00:04:46.361272: step 69450, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 47h:44m:58s remains)
INFO - root - 2017-12-08 00:04:53.135053: step 69460, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 48h:34m:01s remains)
INFO - root - 2017-12-08 00:04:59.854475: step 69470, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 52h:50m:02s remains)
INFO - root - 2017-12-08 00:05:06.598848: step 69480, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 51h:16m:34s remains)
INFO - root - 2017-12-08 00:05:13.265757: step 69490, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 47h:36m:37s remains)
INFO - root - 2017-12-08 00:05:19.888744: step 69500, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 49h:00m:50s remains)
2017-12-08 00:05:20.682421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2767081 -4.2532506 -4.225347 -4.2058887 -4.1985784 -4.2015729 -4.2165346 -4.2377067 -4.2575212 -4.2722254 -4.2811337 -4.289268 -4.2977204 -4.3067989 -4.3141036][-4.2519684 -4.2126527 -4.1699381 -4.1429634 -4.134758 -4.1401877 -4.1617408 -4.193809 -4.2246304 -4.2477932 -4.2632794 -4.2758393 -4.28816 -4.3000693 -4.3077388][-4.2258825 -4.1721592 -4.1153541 -4.0810595 -4.0733423 -4.0782409 -4.1024628 -4.1409688 -4.1828914 -4.2147121 -4.2376533 -4.2567863 -4.2742858 -4.2889132 -4.2978449][-4.2084293 -4.1460619 -4.0793161 -4.039175 -4.0322924 -4.0346675 -4.053297 -4.0882916 -4.1371655 -4.1750369 -4.2054424 -4.2310424 -4.253149 -4.272748 -4.2851152][-4.2049136 -4.1393895 -4.065083 -4.01884 -4.0081873 -4.0077395 -4.0192747 -4.0444417 -4.0909352 -4.13037 -4.1645536 -4.1966724 -4.2235065 -4.2487192 -4.2674756][-4.2096152 -4.1428204 -4.065949 -4.0173063 -3.9984286 -3.9930592 -3.9956219 -4.0036421 -4.0385156 -4.0791855 -4.1180949 -4.1555257 -4.1891546 -4.2211027 -4.2468705][-4.2182631 -4.1502085 -4.0740547 -4.0259128 -4.00065 -3.988821 -3.9805026 -3.9721775 -3.9923236 -4.0309477 -4.0770965 -4.122633 -4.16233 -4.1963091 -4.226965][-4.2311783 -4.16568 -4.0941896 -4.0470037 -4.018887 -4.0032282 -3.9887238 -3.9702671 -3.9813445 -4.016922 -4.0630784 -4.1103358 -4.1488285 -4.182241 -4.2136111][-4.24804 -4.1881762 -4.124639 -4.0779586 -4.0513453 -4.0412111 -4.0298429 -4.0065708 -4.0106726 -4.0388474 -4.0748072 -4.1128283 -4.1470942 -4.1735353 -4.1986728][-4.2674274 -4.2178392 -4.1668291 -4.1248107 -4.1017981 -4.1014771 -4.0944581 -4.0680547 -4.063179 -4.0757418 -4.0898352 -4.1133151 -4.1428428 -4.1628819 -4.181426][-4.2858391 -4.2507529 -4.213541 -4.1807737 -4.1657887 -4.171792 -4.1692696 -4.1470375 -4.1323023 -4.1233883 -4.1141233 -4.1188054 -4.1405077 -4.1575027 -4.170795][-4.3013396 -4.2805171 -4.2556663 -4.2317019 -4.2220488 -4.2280993 -4.2273636 -4.2141018 -4.1979146 -4.1754384 -4.1523962 -4.14098 -4.15189 -4.1636519 -4.1704774][-4.3120961 -4.3007722 -4.2859344 -4.2706661 -4.2638588 -4.2674794 -4.2671475 -4.2580462 -4.2401347 -4.2124338 -4.1838694 -4.1650639 -4.1625896 -4.1641955 -4.1624813][-4.3184929 -4.3129654 -4.3057542 -4.298903 -4.2958565 -4.2977991 -4.2986803 -4.2916894 -4.2716818 -4.2403784 -4.2057867 -4.1791749 -4.1623316 -4.1535058 -4.1470871][-4.3196335 -4.31781 -4.3151298 -4.3128572 -4.31264 -4.3151951 -4.3184824 -4.3126507 -4.2931685 -4.2627878 -4.2257242 -4.1907072 -4.1630869 -4.1479173 -4.1395183]]...]
INFO - root - 2017-12-08 00:05:27.482269: step 69510, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 49h:25m:44s remains)
INFO - root - 2017-12-08 00:05:34.249857: step 69520, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 49h:35m:12s remains)
INFO - root - 2017-12-08 00:05:41.071167: step 69530, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 46h:09m:26s remains)
INFO - root - 2017-12-08 00:05:47.791339: step 69540, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 47h:57m:46s remains)
INFO - root - 2017-12-08 00:05:54.597590: step 69550, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 49h:57m:25s remains)
INFO - root - 2017-12-08 00:06:01.422505: step 69560, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 52h:40m:30s remains)
INFO - root - 2017-12-08 00:06:08.306996: step 69570, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 51h:02m:14s remains)
INFO - root - 2017-12-08 00:06:15.044794: step 69580, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.606 sec/batch; 44h:14m:12s remains)
INFO - root - 2017-12-08 00:06:21.782627: step 69590, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 47h:04m:58s remains)
INFO - root - 2017-12-08 00:06:28.319253: step 69600, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 49h:50m:20s remains)
2017-12-08 00:06:29.101551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3076658 -4.3087845 -4.307704 -4.302855 -4.2967706 -4.2948723 -4.2955732 -4.2980642 -4.3017321 -4.3058987 -4.3082952 -4.3080611 -4.3023906 -4.2907729 -4.2709107][-4.3247395 -4.3258581 -4.322506 -4.31386 -4.3016744 -4.2942758 -4.2925835 -4.2963347 -4.3012819 -4.3047452 -4.307838 -4.3078632 -4.3001041 -4.281981 -4.2513638][-4.3408933 -4.3387709 -4.3313951 -4.3190742 -4.3014107 -4.2865691 -4.2800241 -4.2824841 -4.2893839 -4.2923732 -4.293972 -4.2937131 -4.2842655 -4.2585945 -4.2179794][-4.3440037 -4.3352051 -4.3218842 -4.3026094 -4.2767487 -4.2526989 -4.2387762 -4.23949 -4.2502112 -4.2580805 -4.2646737 -4.2684431 -4.2576618 -4.2247567 -4.1800909][-4.3384795 -4.320488 -4.2963815 -4.2647524 -4.2254376 -4.1871648 -4.1640649 -4.1657853 -4.1842723 -4.2060909 -4.2275929 -4.2402587 -4.2315764 -4.1978106 -4.1565547][-4.3297639 -4.3004909 -4.2633677 -4.219254 -4.1649017 -4.1081204 -4.075264 -4.0806527 -4.1104808 -4.1487341 -4.1862645 -4.2112722 -4.2134595 -4.1914344 -4.1612287][-4.3132615 -4.2720208 -4.2230015 -4.1659007 -4.1017203 -4.0349636 -3.9915993 -3.9969468 -4.0340528 -4.0849042 -4.1366205 -4.1781769 -4.1984296 -4.1996512 -4.1917496][-4.2903647 -4.2380118 -4.1802483 -4.1197562 -4.0565786 -3.9944136 -3.954119 -3.9566147 -3.9899092 -4.0440784 -4.1072173 -4.163475 -4.2017951 -4.2222939 -4.2297606][-4.2782292 -4.2176175 -4.1548824 -4.0993748 -4.0515985 -4.0044079 -3.9759657 -3.9770119 -4.0005116 -4.0488238 -4.1125579 -4.1754451 -4.2237697 -4.2524962 -4.2656236][-4.2908397 -4.233767 -4.17516 -4.1258969 -4.0916786 -4.0636373 -4.0479007 -4.0477991 -4.0643497 -4.1015935 -4.1529922 -4.2078557 -4.2540979 -4.2851205 -4.3003273][-4.3136744 -4.2717085 -4.228261 -4.1935549 -4.1714826 -4.1531777 -4.1445532 -4.1453013 -4.1575794 -4.181006 -4.2163472 -4.257781 -4.2932739 -4.317235 -4.3284678][-4.3357663 -4.3107705 -4.2841663 -4.2624297 -4.2503142 -4.2394819 -4.23497 -4.2380128 -4.2489223 -4.2604361 -4.2792697 -4.3047481 -4.32744 -4.3413787 -4.3451424][-4.3516846 -4.33995 -4.3249249 -4.3110938 -4.3024344 -4.2952204 -4.2927923 -4.2966037 -4.3026328 -4.3057055 -4.3147368 -4.3329744 -4.3483343 -4.3560643 -4.3538971][-4.3627882 -4.3587079 -4.3522663 -4.3438435 -4.3353257 -4.3281627 -4.3266487 -4.3276715 -4.3271427 -4.3248973 -4.3295984 -4.3443007 -4.3570886 -4.361485 -4.3561034][-4.3633423 -4.3642511 -4.3628168 -4.3576689 -4.3507695 -4.346139 -4.345551 -4.3457227 -4.3434753 -4.3402677 -4.3423166 -4.3511772 -4.3579698 -4.359149 -4.3537879]]...]
INFO - root - 2017-12-08 00:06:35.882005: step 69610, loss = 2.08, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 45h:41m:23s remains)
INFO - root - 2017-12-08 00:06:42.692887: step 69620, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 46h:35m:17s remains)
INFO - root - 2017-12-08 00:06:49.550902: step 69630, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 52h:44m:46s remains)
INFO - root - 2017-12-08 00:06:56.459496: step 69640, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.758 sec/batch; 55h:21m:13s remains)
INFO - root - 2017-12-08 00:07:03.232095: step 69650, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.669 sec/batch; 48h:51m:45s remains)
INFO - root - 2017-12-08 00:07:09.996229: step 69660, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 45h:11m:51s remains)
INFO - root - 2017-12-08 00:07:16.940078: step 69670, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 50h:38m:38s remains)
INFO - root - 2017-12-08 00:07:23.801103: step 69680, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 51h:36m:09s remains)
INFO - root - 2017-12-08 00:07:30.661578: step 69690, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 50h:48m:07s remains)
INFO - root - 2017-12-08 00:07:36.990608: step 69700, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 48h:50m:15s remains)
2017-12-08 00:07:37.834437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1473818 -4.1364746 -4.1436062 -4.1502271 -4.1403174 -4.1242032 -4.118309 -4.130137 -4.15177 -4.1774163 -4.2009768 -4.2145987 -4.2307224 -4.2324572 -4.2103095][-4.139606 -4.1397147 -4.1536193 -4.1585526 -4.1463938 -4.123064 -4.1066084 -4.1138406 -4.1357856 -4.1614237 -4.1850052 -4.202383 -4.2190733 -4.2249432 -4.21033][-4.129684 -4.1442881 -4.1686282 -4.1777272 -4.1686864 -4.145967 -4.1227374 -4.1196246 -4.131258 -4.1466451 -4.1650195 -4.1815906 -4.1969724 -4.2079749 -4.2031832][-4.1246843 -4.1492105 -4.1795635 -4.1910505 -4.1829133 -4.1556358 -4.1254435 -4.1110024 -4.1101184 -4.11611 -4.134789 -4.1559477 -4.1723242 -4.1876235 -4.1938081][-4.1232409 -4.1456137 -4.1769733 -4.1845264 -4.1695461 -4.1381559 -4.1029172 -4.0786133 -4.0696912 -4.0772786 -4.1038151 -4.1333776 -4.1521769 -4.1673756 -4.1802955][-4.1193757 -4.1315041 -4.1566896 -4.1593995 -4.1346135 -4.09648 -4.0564733 -4.0290861 -4.0282817 -4.0500026 -4.092247 -4.1328511 -4.1514049 -4.1594534 -4.1697903][-4.1167507 -4.1187434 -4.1303887 -4.1220369 -4.0856228 -4.0406947 -3.9999316 -3.9800684 -3.9983535 -4.0413151 -4.0994477 -4.1471033 -4.1659136 -4.1666493 -4.1719203][-4.1275673 -4.1202803 -4.1172748 -4.0984716 -4.0552006 -4.0090013 -3.9729302 -3.9679847 -4.0043087 -4.0592895 -4.1177139 -4.1631904 -4.1804729 -4.1787148 -4.1794257][-4.1527081 -4.1401854 -4.1314888 -4.1077909 -4.0682769 -4.0291739 -4.0062251 -4.0138779 -4.0585632 -4.1074119 -4.1477203 -4.1780896 -4.1901274 -4.1868477 -4.1844745][-4.1792054 -4.1686335 -4.1643815 -4.1452727 -4.1151142 -4.0870066 -4.0722342 -4.0824122 -4.1255755 -4.1651111 -4.1833329 -4.1917357 -4.1900768 -4.1807966 -4.1742043][-4.1959233 -4.1961527 -4.2013392 -4.1872988 -4.1632953 -4.1424212 -4.1305108 -4.1389632 -4.1767664 -4.2084742 -4.2134953 -4.2039881 -4.1813827 -4.1547003 -4.1414518][-4.1958485 -4.2087789 -4.2206478 -4.210228 -4.1904812 -4.1764865 -4.1674733 -4.1741748 -4.2076478 -4.2342615 -4.2325091 -4.2151618 -4.1799679 -4.1374483 -4.11551][-4.1808777 -4.1973534 -4.2114339 -4.2047744 -4.1910958 -4.1867332 -4.1817389 -4.1903248 -4.222815 -4.2460966 -4.2410707 -4.2222943 -4.1850533 -4.1409121 -4.1153994][-4.1568327 -4.1696625 -4.1818032 -4.1790838 -4.1740732 -4.1774364 -4.1773028 -4.1911988 -4.2248249 -4.2468772 -4.243578 -4.2245855 -4.1855192 -4.1460443 -4.1246939][-4.1464958 -4.148344 -4.1503358 -4.1470909 -4.1473703 -4.1574469 -4.1665182 -4.1905727 -4.2268591 -4.2454586 -4.2443357 -4.2276449 -4.1900072 -4.1549048 -4.1386771]]...]
INFO - root - 2017-12-08 00:07:44.686730: step 69710, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 53h:31m:55s remains)
INFO - root - 2017-12-08 00:07:51.198461: step 69720, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.613 sec/batch; 44h:45m:03s remains)
INFO - root - 2017-12-08 00:07:58.096749: step 69730, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 47h:36m:17s remains)
INFO - root - 2017-12-08 00:08:04.948736: step 69740, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 52h:52m:02s remains)
INFO - root - 2017-12-08 00:08:11.818384: step 69750, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 53h:57m:58s remains)
INFO - root - 2017-12-08 00:08:18.575246: step 69760, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 50h:41m:53s remains)
INFO - root - 2017-12-08 00:08:25.351200: step 69770, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 47h:04m:41s remains)
INFO - root - 2017-12-08 00:08:32.136913: step 69780, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 47h:36m:13s remains)
INFO - root - 2017-12-08 00:08:38.967166: step 69790, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 53h:31m:55s remains)
INFO - root - 2017-12-08 00:08:45.586925: step 69800, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 52h:47m:34s remains)
2017-12-08 00:08:46.286888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3010011 -4.2880697 -4.2618189 -4.2277808 -4.20325 -4.1863141 -4.1859589 -4.2221522 -4.264432 -4.277586 -4.2667613 -4.2495666 -4.2312174 -4.2105894 -4.1967549][-4.3087969 -4.2966948 -4.2728448 -4.2371321 -4.2100463 -4.1933737 -4.1888676 -4.2230339 -4.2595463 -4.2637753 -4.2440839 -4.2249112 -4.2087317 -4.1842885 -4.1649919][-4.3121195 -4.3017569 -4.2811613 -4.244698 -4.2145057 -4.1961155 -4.1858258 -4.2119212 -4.2373619 -4.2328181 -4.21122 -4.1968441 -4.1891165 -4.1675429 -4.1427684][-4.3064528 -4.2951221 -4.276125 -4.241333 -4.2090149 -4.1823268 -4.1574521 -4.1675706 -4.1881247 -4.1904421 -4.1821094 -4.1821141 -4.1809297 -4.1638861 -4.1258364][-4.2927003 -4.2775879 -4.2599549 -4.2248869 -4.1850986 -4.1391315 -4.08295 -4.0712585 -4.0988116 -4.1254597 -4.1422224 -4.1628332 -4.1728535 -4.1623163 -4.1185508][-4.2810259 -4.2596359 -4.23233 -4.1838412 -4.1244731 -4.0495915 -3.9594109 -3.9285617 -3.9830871 -4.0514846 -4.10088 -4.1394782 -4.1532989 -4.1440763 -4.1067677][-4.2725821 -4.2419596 -4.2033448 -4.1449308 -4.07082 -3.9824989 -3.8864472 -3.8556359 -3.9385014 -4.0363531 -4.0974422 -4.1320581 -4.1355047 -4.1230497 -4.1007657][-4.2531557 -4.20772 -4.1611772 -4.1073742 -4.0419245 -3.9719048 -3.9128511 -3.9061868 -3.9903855 -4.0746789 -4.1160178 -4.1271496 -4.1131749 -4.094851 -4.0873346][-4.2381592 -4.1864343 -4.1391644 -4.0985832 -4.0575533 -4.0203228 -3.9994943 -4.0088725 -4.0677156 -4.11981 -4.1334395 -4.124198 -4.1028228 -4.0827489 -4.0824919][-4.2392159 -4.1945996 -4.1563048 -4.1316409 -4.114296 -4.10239 -4.0975189 -4.1037006 -4.1316657 -4.1577983 -4.1548018 -4.1362329 -4.1176167 -4.0977297 -4.0998936][-4.2567482 -4.2246337 -4.1994243 -4.1894083 -4.1888528 -4.1901526 -4.1858635 -4.182785 -4.1895933 -4.1995182 -4.1892929 -4.167974 -4.151875 -4.1366 -4.1436906][-4.28385 -4.262886 -4.2503929 -4.2501221 -4.256825 -4.2634482 -4.2576485 -4.2487178 -4.2441025 -4.2435122 -4.2319188 -4.2161589 -4.2058492 -4.197958 -4.2093744][-4.3063707 -4.2935429 -4.2902246 -4.2943344 -4.3009472 -4.3062086 -4.3003178 -4.2909455 -4.2820139 -4.2777357 -4.2686553 -4.263813 -4.2621913 -4.2620096 -4.2744756][-4.3210478 -4.3115196 -4.3095407 -4.3118091 -4.3138361 -4.3143635 -4.3100286 -4.3027945 -4.2953038 -4.2924361 -4.2902284 -4.2943592 -4.3013587 -4.3073163 -4.3159561][-4.3312612 -4.3220334 -4.3167157 -4.3144312 -4.3119116 -4.30875 -4.3045688 -4.30032 -4.2952361 -4.2934246 -4.2941594 -4.3002453 -4.308435 -4.3151412 -4.3207784]]...]
INFO - root - 2017-12-08 00:08:53.043337: step 69810, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 51h:33m:03s remains)
INFO - root - 2017-12-08 00:08:59.860855: step 69820, loss = 2.10, batch loss = 2.04 (10.6 examples/sec; 0.758 sec/batch; 55h:16m:56s remains)
INFO - root - 2017-12-08 00:09:06.620624: step 69830, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 47h:34m:46s remains)
INFO - root - 2017-12-08 00:09:13.369072: step 69840, loss = 2.08, batch loss = 2.02 (13.3 examples/sec; 0.603 sec/batch; 44h:00m:49s remains)
INFO - root - 2017-12-08 00:09:20.156607: step 69850, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 48h:21m:26s remains)
INFO - root - 2017-12-08 00:09:27.079927: step 69860, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 51h:59m:39s remains)
INFO - root - 2017-12-08 00:09:33.965743: step 69870, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 52h:37m:23s remains)
INFO - root - 2017-12-08 00:09:40.796656: step 69880, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 48h:36m:29s remains)
INFO - root - 2017-12-08 00:09:47.649019: step 69890, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.651 sec/batch; 47h:29m:38s remains)
INFO - root - 2017-12-08 00:09:54.264842: step 69900, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 52h:18m:59s remains)
2017-12-08 00:09:55.018934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3057647 -4.292624 -4.2726555 -4.2466373 -4.2301221 -4.2300491 -4.2465267 -4.2709575 -4.2789559 -4.2812548 -4.2924485 -4.303771 -4.2949352 -4.2634425 -4.214829][-4.3264656 -4.3333035 -4.3271861 -4.3063831 -4.2879725 -4.2793741 -4.2868342 -4.3036766 -4.3123083 -4.3198767 -4.3345137 -4.3438144 -4.3315129 -4.2943225 -4.2454004][-4.3145676 -4.333499 -4.3317895 -4.3083835 -4.2833538 -4.2695541 -4.2736139 -4.2862487 -4.3003168 -4.3180723 -4.3425031 -4.35742 -4.353919 -4.3270721 -4.2854486][-4.2776251 -4.3015709 -4.2995486 -4.2722549 -4.2397742 -4.2188225 -4.2192631 -4.2330556 -4.2551136 -4.2833986 -4.3170233 -4.3405232 -4.3491812 -4.3383603 -4.3123455][-4.2285295 -4.2546973 -4.2457705 -4.2038951 -4.1518316 -4.1164045 -4.10849 -4.1195469 -4.1529021 -4.1988993 -4.2487078 -4.2871408 -4.3129263 -4.3230176 -4.3148494][-4.1862488 -4.2173448 -4.2034669 -4.1424847 -4.06216 -3.9921992 -3.951261 -3.9366145 -3.9751086 -4.0543747 -4.1388745 -4.203557 -4.2481847 -4.2811441 -4.2914762][-4.1796045 -4.2080269 -4.1890011 -4.115468 -4.0124464 -3.9044929 -3.8055298 -3.7309861 -3.7593622 -3.876317 -4.0050964 -4.1042771 -4.1723433 -4.227006 -4.2553487][-4.2051988 -4.2288814 -4.2134409 -4.1479893 -4.050046 -3.9321079 -3.8037581 -3.6824346 -3.6775405 -3.7936811 -3.9325821 -4.044929 -4.1239438 -4.190691 -4.230298][-4.2462659 -4.2703114 -4.2679381 -4.2226343 -4.1468949 -4.0498948 -3.9460397 -3.8455286 -3.8231156 -3.8918664 -3.9913714 -4.0788822 -4.1406889 -4.1950388 -4.2312994][-4.2944965 -4.3182039 -4.3260169 -4.30446 -4.2593718 -4.1946945 -4.1271782 -4.0618653 -4.042603 -4.0753479 -4.1314292 -4.1843519 -4.2190909 -4.2487082 -4.2701139][-4.33233 -4.3520432 -4.3606191 -4.35448 -4.3365664 -4.3035254 -4.2679873 -4.2326212 -4.21901 -4.23114 -4.25853 -4.284461 -4.2986145 -4.3097057 -4.3191018][-4.3505616 -4.3637352 -4.3681226 -4.366188 -4.3622904 -4.3535552 -4.3431087 -4.3308277 -4.3250957 -4.3281841 -4.3363881 -4.3436909 -4.3451619 -4.3462305 -4.34815][-4.3575797 -4.3643088 -4.3662162 -4.3646407 -4.3636756 -4.3627281 -4.3628273 -4.3625336 -4.3626375 -4.3633027 -4.3638458 -4.3624897 -4.3587308 -4.3573904 -4.3577318][-4.3595724 -4.3621716 -4.3634429 -4.3630891 -4.3617392 -4.3607545 -4.361033 -4.3631315 -4.3650169 -4.3650918 -4.36331 -4.3605833 -4.3576059 -4.3566842 -4.356874][-4.3609867 -4.3626852 -4.3635063 -4.3637543 -4.3628736 -4.3613329 -4.3605061 -4.3609667 -4.3619494 -4.3620296 -4.3612838 -4.3608818 -4.3604426 -4.3603139 -4.360116]]...]
INFO - root - 2017-12-08 00:10:01.783599: step 69910, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 46h:18m:46s remains)
INFO - root - 2017-12-08 00:10:08.578466: step 69920, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 47h:03m:44s remains)
INFO - root - 2017-12-08 00:10:15.363616: step 69930, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 52h:09m:57s remains)
INFO - root - 2017-12-08 00:10:22.122644: step 69940, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.744 sec/batch; 54h:16m:06s remains)
INFO - root - 2017-12-08 00:10:28.933395: step 69950, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 49h:58m:32s remains)
INFO - root - 2017-12-08 00:10:35.684407: step 69960, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 45h:30m:39s remains)
INFO - root - 2017-12-08 00:10:42.533790: step 69970, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 47h:54m:49s remains)
INFO - root - 2017-12-08 00:10:49.288391: step 69980, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 52h:12m:48s remains)
INFO - root - 2017-12-08 00:10:56.094194: step 69990, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 0.738 sec/batch; 53h:46m:43s remains)
INFO - root - 2017-12-08 00:11:02.768351: step 70000, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:58m:49s remains)
2017-12-08 00:11:03.519971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2009273 -4.2059665 -4.2014484 -4.1928473 -4.1826825 -4.1733851 -4.1744194 -4.1785131 -4.1808805 -4.2003307 -4.2221122 -4.2225933 -4.2101121 -4.1893568 -4.1606269][-4.2244658 -4.2304173 -4.2274432 -4.2166328 -4.2018557 -4.1922531 -4.1929731 -4.1958127 -4.2048626 -4.2306705 -4.2488375 -4.2428274 -4.2261219 -4.2032576 -4.1691403][-4.2337093 -4.2368364 -4.2357521 -4.2252584 -4.2129622 -4.2064657 -4.2129474 -4.2159576 -4.2235341 -4.24697 -4.2584877 -4.2462225 -4.2273941 -4.2030182 -4.1675386][-4.2418618 -4.2376513 -4.2337732 -4.2195868 -4.201263 -4.19084 -4.1964164 -4.1987767 -4.2068849 -4.2281356 -4.2365713 -4.228507 -4.2113991 -4.1888247 -4.1543493][-4.2447128 -4.2380915 -4.2318521 -4.2139125 -4.1818118 -4.1549168 -4.1531577 -4.153244 -4.1648407 -4.1847897 -4.1956191 -4.1993213 -4.1915569 -4.1708341 -4.1376705][-4.236568 -4.2307091 -4.2208657 -4.1950216 -4.15255 -4.1024628 -4.0832815 -4.0793176 -4.0972447 -4.1266131 -4.1446333 -4.159677 -4.1640406 -4.1476431 -4.122901][-4.2058029 -4.2035766 -4.1922817 -4.1630359 -4.1138482 -4.044713 -4.0045052 -3.99786 -4.0231647 -4.0635042 -4.0886021 -4.1102219 -4.1257486 -4.1206217 -4.1113677][-4.17258 -4.1751423 -4.1655493 -4.1391768 -4.09363 -4.02434 -3.976799 -3.9712615 -3.9982305 -4.0378036 -4.0565605 -4.0702891 -4.0853124 -4.08968 -4.0937438][-4.1591554 -4.1640987 -4.1601076 -4.1460609 -4.1174035 -4.0699162 -4.0289412 -4.0214286 -4.039927 -4.0666265 -4.0731654 -4.0687065 -4.0693693 -4.0711923 -4.0803294][-4.1674228 -4.1797562 -4.1821284 -4.1737885 -4.1592627 -4.1365457 -4.1095352 -4.0976739 -4.1035256 -4.1185679 -4.1231952 -4.1134229 -4.1068449 -4.1001525 -4.1024222][-4.1904216 -4.2062583 -4.2106586 -4.1999936 -4.1875334 -4.1781416 -4.1657419 -4.1634564 -4.1705055 -4.1792722 -4.1826863 -4.1754007 -4.1692362 -4.1602039 -4.158174][-4.221633 -4.2344103 -4.2351818 -4.2212334 -4.2060585 -4.1981254 -4.1948476 -4.2030644 -4.2206011 -4.2333183 -4.23579 -4.2294641 -4.2249765 -4.2190423 -4.2144585][-4.2458186 -4.2543082 -4.2519569 -4.2403116 -4.2288823 -4.2233281 -4.2231865 -4.2327728 -4.2505126 -4.26342 -4.2671218 -4.2632465 -4.2614923 -4.2596827 -4.2536411][-4.2520127 -4.257463 -4.2589455 -4.2576494 -4.255991 -4.2553086 -4.2562065 -4.2637725 -4.2757049 -4.2845478 -4.2867746 -4.2824707 -4.2810564 -4.2798142 -4.2714343][-4.2510133 -4.2592216 -4.2675662 -4.2739658 -4.277616 -4.2801919 -4.2842031 -4.2908587 -4.2965403 -4.300312 -4.2993546 -4.2940497 -4.2918425 -4.2866845 -4.275269]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 00:11:10.953503: step 70010, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 47h:49m:51s remains)
INFO - root - 2017-12-08 00:11:17.696895: step 70020, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 47h:06m:36s remains)
INFO - root - 2017-12-08 00:11:24.349121: step 70030, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 52h:06m:54s remains)
INFO - root - 2017-12-08 00:11:31.184725: step 70040, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 50h:31m:17s remains)
INFO - root - 2017-12-08 00:11:37.965583: step 70050, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 47h:10m:06s remains)
INFO - root - 2017-12-08 00:11:44.768393: step 70060, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 47h:07m:30s remains)
INFO - root - 2017-12-08 00:11:51.477241: step 70070, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.627 sec/batch; 45h:44m:26s remains)
INFO - root - 2017-12-08 00:11:58.345761: step 70080, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 52h:04m:49s remains)
INFO - root - 2017-12-08 00:12:05.147898: step 70090, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 53h:16m:23s remains)
INFO - root - 2017-12-08 00:12:11.782754: step 70100, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 45h:55m:30s remains)
2017-12-08 00:12:12.442360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3230391 -4.3086991 -4.297101 -4.2751818 -4.2424145 -4.2076912 -4.178792 -4.1569147 -4.1590104 -4.1691785 -4.1662717 -4.1569991 -4.1435666 -4.1367702 -4.1400375][-4.3218913 -4.3093529 -4.3019338 -4.2867351 -4.2635226 -4.2387495 -4.2127104 -4.1851025 -4.1879034 -4.2032909 -4.2009611 -4.18662 -4.1571364 -4.1278305 -4.1152859][-4.3246655 -4.3130188 -4.3069229 -4.2923818 -4.2711415 -4.2511463 -4.2274919 -4.2012839 -4.2089396 -4.2287169 -4.2322307 -4.2214203 -4.1919527 -4.1522422 -4.1281614][-4.3254819 -4.3109989 -4.3012528 -4.283432 -4.257906 -4.2309051 -4.2025604 -4.1834607 -4.2014728 -4.2314844 -4.247539 -4.2525358 -4.2361584 -4.2030687 -4.1797075][-4.3231082 -4.30619 -4.2937932 -4.2754264 -4.2470646 -4.2102995 -4.1726933 -4.1504374 -4.1656561 -4.1977468 -4.2230177 -4.2433467 -4.2437482 -4.2268343 -4.2156878][-4.3179479 -4.2989912 -4.2847819 -4.2686405 -4.2392187 -4.1959572 -4.15082 -4.1213961 -4.1205139 -4.1407938 -4.1621428 -4.1850834 -4.1983142 -4.200572 -4.2024655][-4.3090944 -4.2882462 -4.2729769 -4.2564907 -4.2269297 -4.1806846 -4.1352477 -4.1052504 -4.0948062 -4.0974531 -4.0989523 -4.1055074 -4.1193085 -4.1390438 -4.1547966][-4.2966523 -4.2711678 -4.2542205 -4.2368894 -4.2099266 -4.1662111 -4.1222596 -4.0923433 -4.07878 -4.0713029 -4.0582161 -4.0487723 -4.0541677 -4.0787868 -4.0964637][-4.2841196 -4.2547154 -4.2343445 -4.2150631 -4.1914806 -4.1536074 -4.1127772 -4.0838695 -4.063673 -4.0442891 -4.0224929 -4.0060229 -4.009964 -4.0405312 -4.0561323][-4.2772632 -4.2469234 -4.2264934 -4.2097635 -4.1917558 -4.1647553 -4.1327558 -4.1074839 -4.0888233 -4.063386 -4.036242 -4.0163341 -4.0191879 -4.0525022 -4.0690093][-4.2784877 -4.2498789 -4.2333837 -4.224575 -4.2174468 -4.2026753 -4.177731 -4.1575603 -4.1471777 -4.1272221 -4.1021724 -4.0848055 -4.0872407 -4.1134777 -4.1292319][-4.2920942 -4.2703204 -4.2609534 -4.2592049 -4.2602472 -4.2551751 -4.23669 -4.2187986 -4.2123847 -4.1976748 -4.1747861 -4.16085 -4.1656737 -4.1858506 -4.201426][-4.3093467 -4.2955532 -4.292232 -4.29394 -4.2989731 -4.3015924 -4.2926307 -4.2798176 -4.2739248 -4.2613893 -4.2403626 -4.2250166 -4.2273445 -4.2423353 -4.2596273][-4.3234534 -4.3138824 -4.3115425 -4.3111806 -4.3133693 -4.3193979 -4.3199043 -4.314106 -4.3107853 -4.3014402 -4.2853689 -4.2730107 -4.2723031 -4.2804356 -4.29253][-4.3359041 -4.3286276 -4.3244395 -4.320117 -4.3180971 -4.3224411 -4.3269825 -4.3262663 -4.3255782 -4.3200345 -4.3117537 -4.3057017 -4.3050075 -4.3081594 -4.312439]]...]
INFO - root - 2017-12-08 00:12:19.312766: step 70110, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 51h:41m:07s remains)
INFO - root - 2017-12-08 00:12:26.171569: step 70120, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 51h:29m:02s remains)
INFO - root - 2017-12-08 00:12:32.893907: step 70130, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 47h:46m:04s remains)
INFO - root - 2017-12-08 00:12:39.634220: step 70140, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 46h:55m:40s remains)
INFO - root - 2017-12-08 00:12:46.367673: step 70150, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 48h:42m:23s remains)
INFO - root - 2017-12-08 00:12:53.134050: step 70160, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 53h:02m:10s remains)
INFO - root - 2017-12-08 00:13:00.018256: step 70170, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 52h:41m:26s remains)
INFO - root - 2017-12-08 00:13:06.965018: step 70180, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 48h:14m:55s remains)
INFO - root - 2017-12-08 00:13:13.664015: step 70190, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.620 sec/batch; 45h:12m:31s remains)
INFO - root - 2017-12-08 00:13:20.460074: step 70200, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 53h:45m:28s remains)
2017-12-08 00:13:21.302172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2213926 -4.2237654 -4.2169204 -4.2132912 -4.2183528 -4.2195225 -4.2187119 -4.1999507 -4.1629829 -4.1419697 -4.1512275 -4.1877785 -4.2230887 -4.2349725 -4.2260804][-4.2543921 -4.2531691 -4.2492957 -4.2504592 -4.25814 -4.2617664 -4.2653942 -4.2563 -4.227644 -4.2061362 -4.2066345 -4.229578 -4.2554231 -4.2652297 -4.256597][-4.255589 -4.2414656 -4.2341542 -4.2432404 -4.2613549 -4.27271 -4.2778463 -4.2717638 -4.24656 -4.2246923 -4.2216296 -4.2348509 -4.2527452 -4.2634969 -4.2548966][-4.2318177 -4.2143273 -4.2027154 -4.2099442 -4.2281814 -4.2418437 -4.2470059 -4.2402792 -4.2155027 -4.194838 -4.1929135 -4.2016759 -4.2191935 -4.2372913 -4.2354584][-4.2150674 -4.1977339 -4.1808586 -4.1779995 -4.1852612 -4.1909075 -4.1893678 -4.1780624 -4.151566 -4.1290908 -4.1229267 -4.1323051 -4.1638412 -4.197135 -4.2116189][-4.2180195 -4.2029319 -4.1839466 -4.1705141 -4.1617842 -4.1524143 -4.14011 -4.1268706 -4.1064787 -4.0776529 -4.0557842 -4.0609231 -4.10589 -4.1527328 -4.1860523][-4.2206254 -4.2089109 -4.1904931 -4.1696854 -4.1507134 -4.1318521 -4.1144466 -4.1121497 -4.1112905 -4.0855584 -4.0470071 -4.0311308 -4.0603504 -4.1068912 -4.1568441][-4.2256169 -4.2176704 -4.2068462 -4.1889143 -4.1667438 -4.144803 -4.1256447 -4.1287007 -4.1383476 -4.1190166 -4.07668 -4.0445 -4.0493727 -4.0840693 -4.144938][-4.2330112 -4.2348266 -4.2382913 -4.2302866 -4.2130036 -4.192512 -4.1762691 -4.1796393 -4.1868711 -4.1712685 -4.1366534 -4.1005907 -4.0889878 -4.108706 -4.1634154][-4.2500858 -4.2628913 -4.2771716 -4.2784715 -4.2664194 -4.2517185 -4.2435741 -4.242527 -4.244792 -4.2350531 -4.2088265 -4.1733479 -4.1518087 -4.1568556 -4.1952481][-4.2655625 -4.2808952 -4.2975578 -4.3035212 -4.2947259 -4.285975 -4.2834148 -4.2848735 -4.2903361 -4.2878423 -4.2699833 -4.2378836 -4.20645 -4.1913643 -4.2068348][-4.2629056 -4.27043 -4.2810726 -4.2862639 -4.28109 -4.2790537 -4.2888813 -4.3037128 -4.3189335 -4.3209491 -4.3037252 -4.2706823 -4.230093 -4.1984043 -4.1925597][-4.2490449 -4.2423139 -4.2361493 -4.2285471 -4.2221327 -4.2286482 -4.2544012 -4.2881494 -4.3160954 -4.3195906 -4.3014712 -4.2718043 -4.233521 -4.2030163 -4.1912036][-4.2390718 -4.2202816 -4.1925282 -4.1600661 -4.1427722 -4.1538105 -4.1942549 -4.2461128 -4.285675 -4.290998 -4.2753487 -4.2533617 -4.2269244 -4.2081251 -4.2010927][-4.2486162 -4.2335596 -4.1996336 -4.1527538 -4.1192551 -4.1182857 -4.1538815 -4.2071233 -4.2490234 -4.255517 -4.243463 -4.2306209 -4.2180409 -4.2080417 -4.2032142]]...]
INFO - root - 2017-12-08 00:13:27.946646: step 70210, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 47h:31m:33s remains)
INFO - root - 2017-12-08 00:13:34.779078: step 70220, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 48h:19m:24s remains)
INFO - root - 2017-12-08 00:13:41.519528: step 70230, loss = 2.09, batch loss = 2.04 (10.8 examples/sec; 0.741 sec/batch; 53h:59m:07s remains)
INFO - root - 2017-12-08 00:13:48.360584: step 70240, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 49h:46m:01s remains)
INFO - root - 2017-12-08 00:13:55.123412: step 70250, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 44h:58m:36s remains)
INFO - root - 2017-12-08 00:14:01.893669: step 70260, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 46h:07m:47s remains)
INFO - root - 2017-12-08 00:14:08.766814: step 70270, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 52h:48m:51s remains)
INFO - root - 2017-12-08 00:14:15.662425: step 70280, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.730 sec/batch; 53h:11m:27s remains)
INFO - root - 2017-12-08 00:14:22.526011: step 70290, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 49h:47m:49s remains)
INFO - root - 2017-12-08 00:14:29.165430: step 70300, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 47h:43m:18s remains)
2017-12-08 00:14:29.973727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1760674 -4.1811466 -4.201941 -4.2247858 -4.2368755 -4.2321277 -4.2204347 -4.2022982 -4.1860342 -4.1783495 -4.1855392 -4.2002769 -4.2241106 -4.2455759 -4.2666211][-4.1710777 -4.1803069 -4.2030973 -4.2253628 -4.2373347 -4.229598 -4.2128887 -4.1909995 -4.1763864 -4.1743388 -4.1855612 -4.2019691 -4.2282324 -4.2521043 -4.2724814][-4.1659164 -4.17521 -4.1956682 -4.2151828 -4.2226334 -4.2102876 -4.1913214 -4.1705952 -4.1600523 -4.165257 -4.1832438 -4.2020378 -4.22919 -4.2537475 -4.274188][-4.1645017 -4.1719503 -4.1854458 -4.1978149 -4.1977105 -4.1813722 -4.1644607 -4.148427 -4.1437116 -4.1581311 -4.1796074 -4.1992397 -4.2257471 -4.2502131 -4.2711625][-4.1650915 -4.167521 -4.1710033 -4.1733932 -4.1644206 -4.1427097 -4.1285081 -4.1214685 -4.1257529 -4.1464772 -4.1677585 -4.1871982 -4.213521 -4.2399783 -4.2649407][-4.1692514 -4.1610966 -4.1501918 -4.1370735 -4.1126204 -4.0809875 -4.0680118 -4.0759921 -4.0961294 -4.1246328 -4.1485829 -4.1709266 -4.2012577 -4.23283 -4.2635093][-4.1847415 -4.1662736 -4.1434264 -4.1145449 -4.0724525 -4.0225034 -4.005022 -4.034833 -4.0785728 -4.1176786 -4.1456389 -4.16968 -4.1999488 -4.2341919 -4.2679286][-4.1843352 -4.1626053 -4.1342692 -4.0984225 -4.0461435 -3.9782479 -3.9503694 -3.9982185 -4.0622725 -4.1143708 -4.1486053 -4.1741152 -4.2038913 -4.2378545 -4.2717161][-4.1728516 -4.153698 -4.1282997 -4.0971971 -4.0506587 -3.9836371 -3.9552052 -4.00363 -4.0648746 -4.1144915 -4.1489654 -4.1756358 -4.2060428 -4.2401304 -4.274312][-4.1564813 -4.1400948 -4.1210785 -4.1043096 -4.0780883 -4.0315347 -4.0117354 -4.0428534 -4.0813584 -4.1157956 -4.1444192 -4.1687431 -4.199564 -4.236599 -4.2736378][-4.1426973 -4.1273756 -4.1126523 -4.1073356 -4.1004353 -4.0762286 -4.0627575 -4.0753651 -4.0933557 -4.1138568 -4.1351423 -4.1559491 -4.1875048 -4.228229 -4.2692642][-4.1313629 -4.1162181 -4.1054335 -4.1066074 -4.1135759 -4.105917 -4.096653 -4.0971217 -4.1032295 -4.1154661 -4.1337118 -4.1547284 -4.1869745 -4.2280555 -4.2690763][-4.140357 -4.1257424 -4.1190186 -4.1240292 -4.1366515 -4.1375279 -4.1299334 -4.1242781 -4.124846 -4.1329484 -4.1507916 -4.1742854 -4.206871 -4.2440495 -4.2792177][-4.1715894 -4.1591926 -4.1568017 -4.1626525 -4.1734385 -4.1750135 -4.1654711 -4.1572847 -4.1577873 -4.166358 -4.1851983 -4.2099142 -4.2393708 -4.2691717 -4.2956095][-4.2155757 -4.2076168 -4.2073231 -4.2091393 -4.2125893 -4.2099957 -4.1987839 -4.1917148 -4.1955996 -4.2089877 -4.229424 -4.2516203 -4.2746844 -4.29487 -4.3121176]]...]
INFO - root - 2017-12-08 00:14:36.765268: step 70310, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 49h:33m:04s remains)
INFO - root - 2017-12-08 00:14:43.598506: step 70320, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 47h:37m:56s remains)
INFO - root - 2017-12-08 00:14:50.305877: step 70330, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.620 sec/batch; 45h:10m:22s remains)
INFO - root - 2017-12-08 00:14:57.008440: step 70340, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 51h:43m:43s remains)
INFO - root - 2017-12-08 00:15:03.829383: step 70350, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 50h:22m:48s remains)
INFO - root - 2017-12-08 00:15:10.605139: step 70360, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 48h:38m:43s remains)
INFO - root - 2017-12-08 00:15:17.298348: step 70370, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 45h:52m:48s remains)
INFO - root - 2017-12-08 00:15:24.143677: step 70380, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 47h:27m:24s remains)
INFO - root - 2017-12-08 00:15:30.976067: step 70390, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.773 sec/batch; 56h:18m:53s remains)
INFO - root - 2017-12-08 00:15:37.579323: step 70400, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 51h:01m:53s remains)
2017-12-08 00:15:38.306551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3105044 -4.3047476 -4.3065639 -4.3161068 -4.3261232 -4.3320684 -4.3312039 -4.3262649 -4.3200259 -4.3208842 -4.3289175 -4.3370447 -4.3426452 -4.3472314 -4.3486385][-4.2905707 -4.28312 -4.2846184 -4.29267 -4.2988997 -4.2986603 -4.291461 -4.2803545 -4.27082 -4.2761917 -4.2929487 -4.3062358 -4.3136258 -4.3174472 -4.3171363][-4.2720551 -4.2596793 -4.2573233 -4.2594433 -4.2581649 -4.24628 -4.2280364 -4.2079797 -4.1955357 -4.2076144 -4.2358036 -4.2569909 -4.2667537 -4.2663665 -4.2606072][-4.2552404 -4.2362013 -4.2268257 -4.2219434 -4.2136283 -4.1922803 -4.1660161 -4.1398773 -4.1310272 -4.152966 -4.1874704 -4.2092152 -4.2171454 -4.209383 -4.1942644][-4.2394357 -4.2161078 -4.2023597 -4.1923518 -4.1762433 -4.1457119 -4.1088591 -4.0759511 -4.07718 -4.1137552 -4.1518879 -4.17333 -4.1785727 -4.1650667 -4.1465826][-4.2292337 -4.2063661 -4.1905742 -4.1738925 -4.1422105 -4.0895553 -4.0248303 -3.9820609 -4.0090618 -4.0723238 -4.1191182 -4.1463714 -4.1545587 -4.1420026 -4.1342783][-4.2317853 -4.21303 -4.1966486 -4.171844 -4.1164923 -4.0307713 -3.9260104 -3.8668933 -3.9306848 -4.0248046 -4.0776267 -4.1065464 -4.1164079 -4.1086507 -4.1195526][-4.2394042 -4.2246847 -4.2095137 -4.179122 -4.1122971 -4.0103374 -3.8820786 -3.809675 -3.8956933 -3.9983425 -4.0427914 -4.060318 -4.0626044 -4.0621052 -4.08947][-4.2440295 -4.2345657 -4.2253275 -4.2012281 -4.14633 -4.0637116 -3.9649444 -3.9115508 -3.9705262 -4.03667 -4.0579481 -4.0547214 -4.0490818 -4.05894 -4.0916309][-4.243938 -4.2422371 -4.2432513 -4.2316818 -4.1945548 -4.13866 -4.0770311 -4.0441103 -4.0783672 -4.1121187 -4.1170483 -4.1001763 -4.0918274 -4.1090441 -4.1420302][-4.232276 -4.237761 -4.247179 -4.245481 -4.2231145 -4.1874638 -4.1503181 -4.1320329 -4.1501822 -4.168704 -4.1690712 -4.1515551 -4.146173 -4.1652565 -4.1961393][-4.208941 -4.2213292 -4.2374287 -4.2446809 -4.2345419 -4.212101 -4.1909337 -4.1853027 -4.19819 -4.2105985 -4.2069407 -4.1858454 -4.1772857 -4.191546 -4.2184172][-4.1935778 -4.2093225 -4.2266717 -4.2415161 -4.2434573 -4.2339916 -4.2249427 -4.2262645 -4.2369084 -4.2437983 -4.2387094 -4.2209282 -4.2125134 -4.2237234 -4.246768][-4.1964059 -4.210887 -4.2241745 -4.2404456 -4.2492304 -4.2479668 -4.2459545 -4.2493124 -4.259449 -4.2708683 -4.2746525 -4.2695847 -4.2662473 -4.2735596 -4.2902303][-4.2093539 -4.2204046 -4.2277713 -4.2409635 -4.2541733 -4.2588944 -4.2599168 -4.2587132 -4.2637019 -4.2804646 -4.2976151 -4.3060236 -4.30881 -4.3133564 -4.3227572]]...]
INFO - root - 2017-12-08 00:15:45.125874: step 70410, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 48h:59m:27s remains)
INFO - root - 2017-12-08 00:15:52.027697: step 70420, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.735 sec/batch; 53h:30m:37s remains)
INFO - root - 2017-12-08 00:15:58.884810: step 70430, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 53h:12m:30s remains)
INFO - root - 2017-12-08 00:16:05.718912: step 70440, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 48h:23m:39s remains)
INFO - root - 2017-12-08 00:16:12.460768: step 70450, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 45h:20m:15s remains)
INFO - root - 2017-12-08 00:16:19.373070: step 70460, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:57m:12s remains)
INFO - root - 2017-12-08 00:16:26.288808: step 70470, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 54h:05m:06s remains)
INFO - root - 2017-12-08 00:16:33.112896: step 70480, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 48h:16m:22s remains)
INFO - root - 2017-12-08 00:16:40.033220: step 70490, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 51h:13m:34s remains)
INFO - root - 2017-12-08 00:16:46.774931: step 70500, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 46h:50m:07s remains)
2017-12-08 00:16:47.616973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2119684 -4.2100925 -4.2080903 -4.192451 -4.1574059 -4.1246896 -4.128386 -4.1741824 -4.2080173 -4.2094035 -4.2100763 -4.2165031 -4.2052193 -4.1813936 -4.1834741][-4.2046318 -4.2015929 -4.1945014 -4.1782393 -4.1430192 -4.10908 -4.1049852 -4.1504831 -4.1876926 -4.1938515 -4.195168 -4.2023 -4.1963329 -4.1767321 -4.182117][-4.2114191 -4.2069988 -4.195549 -4.1769357 -4.1416283 -4.10642 -4.0938687 -4.1353006 -4.17674 -4.188468 -4.1910486 -4.1957588 -4.1900768 -4.1755147 -4.1838465][-4.2243433 -4.221312 -4.2065091 -4.1836977 -4.1473866 -4.1052814 -4.0811849 -4.1196795 -4.1703739 -4.1911821 -4.1961994 -4.1959443 -4.189579 -4.1801004 -4.1885538][-4.2393131 -4.2363505 -4.219038 -4.1912608 -4.1522236 -4.0996785 -4.0575562 -4.0932922 -4.16318 -4.2019529 -4.2133703 -4.2118864 -4.2070727 -4.2034912 -4.2111664][-4.2477636 -4.2445827 -4.2269144 -4.1987195 -4.1540413 -4.0856018 -4.0185041 -4.0533757 -4.1474009 -4.2098923 -4.2327442 -4.2345471 -4.232728 -4.2297058 -4.2330985][-4.2519889 -4.2509956 -4.2382059 -4.2121744 -4.1596341 -4.0734372 -3.982867 -4.0132895 -4.1279159 -4.2091851 -4.2412577 -4.2470746 -4.2504077 -4.2502122 -4.2539649][-4.2534752 -4.2538037 -4.2463613 -4.2240734 -4.1649365 -4.0646453 -3.9566045 -3.98402 -4.1146059 -4.2105184 -4.2510328 -4.2624683 -4.2720437 -4.2751479 -4.2775893][-4.2434564 -4.2414031 -4.2398014 -4.2254963 -4.1670327 -4.0628114 -3.95584 -3.9859867 -4.1190085 -4.2178164 -4.2617135 -4.2780371 -4.2914295 -4.2983255 -4.3002305][-4.2280493 -4.2204552 -4.2239757 -4.2245774 -4.1825657 -4.0991125 -4.0186472 -4.0495515 -4.1604776 -4.240736 -4.2784815 -4.2946939 -4.306159 -4.3137555 -4.3151555][-4.2070003 -4.1911812 -4.201972 -4.2205167 -4.2059469 -4.1538377 -4.1035652 -4.1316032 -4.2140174 -4.2649937 -4.2862873 -4.2961826 -4.3041563 -4.3095574 -4.3117223][-4.1887913 -4.1631513 -4.1773977 -4.2103 -4.2216282 -4.199224 -4.1703162 -4.1893477 -4.2444034 -4.2712231 -4.2774792 -4.2852507 -4.2962904 -4.306901 -4.3130016][-4.1776495 -4.1455603 -4.1596131 -4.2009449 -4.2306423 -4.2304993 -4.2110367 -4.2154717 -4.2468042 -4.2613287 -4.2628274 -4.2724738 -4.2891889 -4.306088 -4.3129673][-4.1695089 -4.1386094 -4.1544447 -4.2008338 -4.2422676 -4.2544589 -4.2368693 -4.2298408 -4.2463789 -4.2594986 -4.2618561 -4.2679815 -4.2809944 -4.297471 -4.3041458][-4.1586261 -4.1322942 -4.1532025 -4.2041016 -4.2524738 -4.2702241 -4.2520356 -4.2374697 -4.2473974 -4.2639313 -4.2669654 -4.2658372 -4.273313 -4.2883272 -4.29628]]...]
INFO - root - 2017-12-08 00:16:54.496218: step 70510, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 51h:28m:51s remains)
INFO - root - 2017-12-08 00:17:01.305240: step 70520, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 45h:30m:11s remains)
INFO - root - 2017-12-08 00:17:08.173064: step 70530, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 49h:01m:19s remains)
INFO - root - 2017-12-08 00:17:15.100551: step 70540, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 53h:50m:42s remains)
INFO - root - 2017-12-08 00:17:21.937956: step 70550, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.746 sec/batch; 54h:15m:04s remains)
INFO - root - 2017-12-08 00:17:28.647546: step 70560, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 47h:01m:26s remains)
INFO - root - 2017-12-08 00:17:35.446033: step 70570, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 46h:57m:58s remains)
INFO - root - 2017-12-08 00:17:42.321062: step 70580, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 48h:08m:11s remains)
INFO - root - 2017-12-08 00:17:49.214325: step 70590, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 51h:05m:32s remains)
INFO - root - 2017-12-08 00:17:55.888875: step 70600, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 51h:28m:39s remains)
2017-12-08 00:17:56.623323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2265964 -4.2280774 -4.2255993 -4.2203431 -4.2425027 -4.2737885 -4.2859149 -4.2824311 -4.2894721 -4.3059239 -4.3127275 -4.3129482 -4.3062482 -4.29779 -4.2877035][-4.2260652 -4.2258587 -4.224184 -4.2227559 -4.2461729 -4.27789 -4.288837 -4.2882142 -4.2990913 -4.3186817 -4.3285165 -4.3300118 -4.3236322 -4.3164415 -4.3074613][-4.2097087 -4.2041969 -4.2037387 -4.2031622 -4.2259684 -4.25523 -4.2680154 -4.2731166 -4.2845135 -4.3030872 -4.3142347 -4.3164582 -4.3100209 -4.3023243 -4.294394][-4.1836367 -4.1669264 -4.16655 -4.1666651 -4.1915364 -4.2213573 -4.2406039 -4.2517743 -4.2612815 -4.2726479 -4.2856121 -4.2908072 -4.2855148 -4.27719 -4.2693524][-4.1615653 -4.1370478 -4.1301408 -4.1225739 -4.1480713 -4.178246 -4.1995344 -4.2123251 -4.2191868 -4.2263556 -4.2443676 -4.2554665 -4.2492275 -4.2344031 -4.2229166][-4.1501932 -4.1176782 -4.0988364 -4.0815959 -4.1049442 -4.1315384 -4.1441336 -4.146059 -4.1422768 -4.1487603 -4.1762257 -4.1918612 -4.1847906 -4.1661305 -4.1546645][-4.1617508 -4.1207671 -4.084 -4.0490017 -4.06253 -4.0787287 -4.0759835 -4.0581727 -4.0398464 -4.0557103 -4.100389 -4.1270108 -4.1317115 -4.1197443 -4.1159425][-4.1771493 -4.1327591 -4.0849152 -4.0380516 -4.0470839 -4.0589161 -4.047421 -4.0133777 -3.9838526 -4.0092306 -4.0668144 -4.1008563 -4.1201177 -4.1223145 -4.1278791][-4.2027044 -4.1677132 -4.1245503 -4.0799375 -4.0854187 -4.095593 -4.0809107 -4.0437689 -4.0117984 -4.0341096 -4.0839143 -4.1123848 -4.13383 -4.1429763 -4.15152][-4.2131462 -4.1923079 -4.1599951 -4.1253119 -4.1328311 -4.1475496 -4.1419387 -4.1197333 -4.10183 -4.1215181 -4.1497355 -4.16186 -4.1730986 -4.1779795 -4.1822324][-4.2118416 -4.2058439 -4.1882315 -4.1708345 -4.1884317 -4.2061758 -4.2054386 -4.1922722 -4.1836071 -4.19723 -4.2060795 -4.2058983 -4.2099133 -4.2090855 -4.2086811][-4.2356 -4.2327409 -4.2165995 -4.2055569 -4.2294621 -4.2483063 -4.2483664 -4.2380996 -4.2326202 -4.2402596 -4.2345672 -4.2246561 -4.2213516 -4.2168 -4.214633][-4.2697325 -4.2629962 -4.2413921 -4.2290673 -4.2548156 -4.2753887 -4.2779279 -4.2725587 -4.2710571 -4.2723308 -4.2577219 -4.241745 -4.2326174 -4.2247272 -4.2213974][-4.2982535 -4.2910523 -4.2688994 -4.2548504 -4.277153 -4.2939286 -4.2938318 -4.2916322 -4.2924852 -4.2904716 -4.2763977 -4.2606244 -4.250001 -4.2409797 -4.237463][-4.3125172 -4.3044124 -4.2824225 -4.2662444 -4.2816172 -4.2922759 -4.2901931 -4.2887039 -4.28896 -4.2878256 -4.2806573 -4.2717142 -4.2637715 -4.2562785 -4.2510977]]...]
INFO - root - 2017-12-08 00:18:03.408579: step 70610, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 50h:43m:10s remains)
INFO - root - 2017-12-08 00:18:10.260976: step 70620, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 53h:40m:32s remains)
INFO - root - 2017-12-08 00:18:17.057272: step 70630, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 50h:55m:38s remains)
INFO - root - 2017-12-08 00:18:23.927040: step 70640, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 47h:34m:09s remains)
INFO - root - 2017-12-08 00:18:30.506227: step 70650, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.608 sec/batch; 44h:15m:32s remains)
INFO - root - 2017-12-08 00:18:37.356886: step 70660, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 51h:40m:57s remains)
INFO - root - 2017-12-08 00:18:44.214700: step 70670, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.744 sec/batch; 54h:08m:11s remains)
INFO - root - 2017-12-08 00:18:50.974132: step 70680, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 48h:51m:18s remains)
INFO - root - 2017-12-08 00:18:57.688202: step 70690, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 46h:12m:46s remains)
INFO - root - 2017-12-08 00:19:04.265240: step 70700, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 45h:41m:44s remains)
2017-12-08 00:19:05.048805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2407341 -4.2320304 -4.239625 -4.2489657 -4.2413096 -4.2286997 -4.2276912 -4.2285018 -4.2312651 -4.2341352 -4.2263188 -4.2063808 -4.1736293 -4.1369443 -4.1233888][-4.2259226 -4.2103405 -4.2121611 -4.2214608 -4.2098036 -4.1965742 -4.1947937 -4.1934314 -4.1969376 -4.205214 -4.2044806 -4.1867051 -4.1468329 -4.0973268 -4.0745053][-4.221693 -4.2019968 -4.1951585 -4.197402 -4.1775212 -4.1629381 -4.1632814 -4.1642742 -4.1704512 -4.1856589 -4.194344 -4.1830621 -4.1430435 -4.086288 -4.0518637][-4.2099123 -4.1859288 -4.1696725 -4.1620955 -4.132988 -4.1161461 -4.1205578 -4.1274052 -4.1387668 -4.1638021 -4.1868153 -4.1878271 -4.1591582 -4.1096711 -4.06905][-4.19094 -4.1592221 -4.1315794 -4.1092753 -4.0677223 -4.0423017 -4.0470104 -4.0625424 -4.080245 -4.1163549 -4.1538916 -4.1690187 -4.1607671 -4.1313128 -4.0984874][-4.1630144 -4.1285343 -4.090888 -4.0501738 -3.988553 -3.9485357 -3.9457536 -3.9618793 -3.9855871 -4.0348978 -4.0898976 -4.1212645 -4.1334119 -4.1249008 -4.1095781][-4.1356416 -4.1036406 -4.0633521 -4.0091019 -3.9331567 -3.8788328 -3.861495 -3.8648605 -3.8849671 -3.9448385 -4.0163302 -4.0595045 -4.0824547 -4.08713 -4.0866508][-4.1214867 -4.0950537 -4.0625644 -4.0104494 -3.934557 -3.8740497 -3.84233 -3.8258672 -3.8290129 -3.8816786 -3.9574463 -4.0060897 -4.0334806 -4.0447278 -4.0522766][-4.1205721 -4.1016445 -4.081111 -4.0422683 -3.9778957 -3.917666 -3.8770723 -3.848628 -3.8368688 -3.8692472 -3.9354329 -3.9819577 -4.0041232 -4.0127244 -4.0217118][-4.1295457 -4.1197333 -4.1143312 -4.0929022 -4.0421038 -3.9827785 -3.9379334 -3.9050591 -3.8863733 -3.9029617 -3.956511 -3.9977746 -4.0120292 -4.0144863 -4.0205188][-4.1563807 -4.1511011 -4.1546226 -4.1456523 -4.1080766 -4.0556736 -4.0123768 -3.9807134 -3.9627705 -3.9705005 -4.0136571 -4.0518866 -4.0629411 -4.0607214 -4.0610905][-4.1965733 -4.1909242 -4.1944332 -4.1904507 -4.1643391 -4.1232438 -4.087574 -4.0623097 -4.0500917 -4.05605 -4.0885835 -4.1213436 -4.1303353 -4.1254349 -4.1219926][-4.2385616 -4.2316375 -4.2316236 -4.2288089 -4.2122855 -4.1850276 -4.1594372 -4.1422892 -4.1368279 -4.1435924 -4.1646953 -4.1863618 -4.1920877 -4.1874995 -4.1836996][-4.2746811 -4.2677922 -4.2649641 -4.2624507 -4.25332 -4.23793 -4.2226624 -4.2127433 -4.2104979 -4.2155557 -4.2267895 -4.23747 -4.2404437 -4.2388463 -4.2376213][-4.3023448 -4.297636 -4.2944231 -4.2921739 -4.2877502 -4.2805519 -4.2730722 -4.2681289 -4.2668743 -4.2694654 -4.274682 -4.2788486 -4.2800789 -4.2804127 -4.2810283]]...]
INFO - root - 2017-12-08 00:19:11.831404: step 70710, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 48h:41m:26s remains)
INFO - root - 2017-12-08 00:19:18.601775: step 70720, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 49h:45m:58s remains)
INFO - root - 2017-12-08 00:19:25.364434: step 70730, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 47h:15m:47s remains)
INFO - root - 2017-12-08 00:19:32.177084: step 70740, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 52h:44m:16s remains)
INFO - root - 2017-12-08 00:19:38.947893: step 70750, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 52h:40m:21s remains)
INFO - root - 2017-12-08 00:19:45.630899: step 70760, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 48h:18m:41s remains)
INFO - root - 2017-12-08 00:19:52.412103: step 70770, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 50h:13m:03s remains)
INFO - root - 2017-12-08 00:19:59.296942: step 70780, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 47h:32m:08s remains)
INFO - root - 2017-12-08 00:20:06.110630: step 70790, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 48h:42m:39s remains)
INFO - root - 2017-12-08 00:20:12.804501: step 70800, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 53h:13m:20s remains)
2017-12-08 00:20:13.527192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2019992 -4.2362757 -4.2734756 -4.299902 -4.3033457 -4.281445 -4.251462 -4.2327232 -4.2287641 -4.2374949 -4.2479329 -4.252419 -4.247478 -4.238482 -4.233788][-4.2172751 -4.261405 -4.2989006 -4.3148079 -4.3045187 -4.2648454 -4.2150373 -4.1865749 -4.1866927 -4.2107449 -4.2372112 -4.2510109 -4.2510891 -4.243979 -4.2414994][-4.2469654 -4.2883725 -4.3160467 -4.3182883 -4.2890997 -4.2268443 -4.153007 -4.116724 -4.1337204 -4.1870584 -4.2346354 -4.2573991 -4.2623196 -4.25809 -4.2565455][-4.2815065 -4.3128366 -4.3264537 -4.311677 -4.2597809 -4.168962 -4.0666552 -4.027422 -4.0751653 -4.1654816 -4.2331057 -4.2646432 -4.2749953 -4.2777777 -4.2777171][-4.3112574 -4.3276415 -4.3243361 -4.2925782 -4.2168856 -4.0896583 -3.9586115 -3.9281681 -4.0176597 -4.1400509 -4.2198071 -4.256381 -4.2744312 -4.28608 -4.2888918][-4.3201666 -4.3170204 -4.2962275 -4.25081 -4.1526628 -3.9941709 -3.845768 -3.8433366 -3.9830692 -4.1254869 -4.2019434 -4.23603 -4.2589078 -4.2763863 -4.2834067][-4.2921863 -4.2661076 -4.2351913 -4.183919 -4.0753961 -3.9066174 -3.7710359 -3.8159027 -3.9882107 -4.1265335 -4.1862922 -4.2089491 -4.2290869 -4.2503457 -4.2643189][-4.2307172 -4.1914344 -4.1619096 -4.1162949 -4.017643 -3.8706849 -3.7817791 -3.8604388 -4.0228486 -4.1345358 -4.17168 -4.1832175 -4.2020369 -4.22563 -4.2425265][-4.1838412 -4.1482377 -4.1300883 -4.0998807 -4.0280814 -3.9234118 -3.8788204 -3.9509718 -4.0692725 -4.1406541 -4.1567826 -4.1617212 -4.1824951 -4.2063212 -4.2184963][-4.1651473 -4.1507697 -4.1534023 -4.1460133 -4.1034236 -4.0391426 -4.0160532 -4.0586205 -4.122015 -4.1528459 -4.1488252 -4.1472077 -4.1679316 -4.1875467 -4.1930914][-4.1636786 -4.17375 -4.191998 -4.2008634 -4.18323 -4.1515751 -4.140419 -4.1550374 -4.1751537 -4.1748 -4.1567917 -4.1520491 -4.1713133 -4.185123 -4.1869936][-4.1551409 -4.1788325 -4.2026215 -4.2201657 -4.2191324 -4.2109156 -4.2127538 -4.2199817 -4.2251797 -4.2109351 -4.1869197 -4.1816907 -4.1936469 -4.198246 -4.1935768][-4.1392436 -4.1621366 -4.1858335 -4.2072964 -4.2142916 -4.2174845 -4.22911 -4.2417235 -4.2504282 -4.2396054 -4.2226186 -4.2219648 -4.226696 -4.2199306 -4.2113233][-4.140852 -4.1538091 -4.1711392 -4.1879883 -4.1936822 -4.1981225 -4.2121387 -4.2345157 -4.2503738 -4.2489767 -4.2441082 -4.2506318 -4.2532144 -4.2452011 -4.238658][-4.1461334 -4.1485105 -4.1590691 -4.1687927 -4.1716881 -4.1768847 -4.189857 -4.2165565 -4.2373209 -4.2457843 -4.2529721 -4.261631 -4.2654948 -4.2645779 -4.2651148]]...]
INFO - root - 2017-12-08 00:20:20.308521: step 70810, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 47h:48m:59s remains)
INFO - root - 2017-12-08 00:20:27.118944: step 70820, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 50h:38m:16s remains)
INFO - root - 2017-12-08 00:20:33.882963: step 70830, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 51h:08m:33s remains)
INFO - root - 2017-12-08 00:20:40.670613: step 70840, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 48h:10m:48s remains)
INFO - root - 2017-12-08 00:20:47.344591: step 70850, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 47h:40m:44s remains)
INFO - root - 2017-12-08 00:20:54.061720: step 70860, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 47h:42m:19s remains)
INFO - root - 2017-12-08 00:21:00.882593: step 70870, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 51h:48m:09s remains)
INFO - root - 2017-12-08 00:21:07.640171: step 70880, loss = 2.09, batch loss = 2.04 (10.9 examples/sec; 0.732 sec/batch; 53h:10m:49s remains)
INFO - root - 2017-12-08 00:21:14.376170: step 70890, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 53h:49m:41s remains)
INFO - root - 2017-12-08 00:21:21.052767: step 70900, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 51h:03m:57s remains)
2017-12-08 00:21:21.839425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20948 -4.2287827 -4.242764 -4.2510014 -4.2537107 -4.2552261 -4.2564249 -4.2650919 -4.2771678 -4.2848997 -4.280345 -4.2667961 -4.2530737 -4.2378268 -4.2237959][-4.2107358 -4.2224412 -4.2237735 -4.2220044 -4.2199135 -4.22121 -4.2257471 -4.2405176 -4.2536092 -4.2544532 -4.2411022 -4.2276893 -4.2232027 -4.2198286 -4.2158761][-4.2160521 -4.2168527 -4.2017012 -4.1882105 -4.1801724 -4.1767011 -4.176856 -4.1914372 -4.2043138 -4.1995521 -4.1829653 -4.1790543 -4.1913271 -4.2032609 -4.2096081][-4.2352562 -4.2222571 -4.1902089 -4.1661553 -4.1488647 -4.1316075 -4.1148624 -4.1151347 -4.1216803 -4.1141119 -4.1030221 -4.1204643 -4.15984 -4.1920252 -4.2094026][-4.2756362 -4.2531323 -4.2086554 -4.1714482 -4.1382365 -4.1028409 -4.0623026 -4.036932 -4.0281773 -4.0128441 -4.00985 -4.0577378 -4.1319642 -4.188642 -4.2173433][-4.3218436 -4.3013287 -4.2577443 -4.213726 -4.1662159 -4.1184354 -4.0656209 -4.0224371 -3.9957738 -3.966675 -3.9632323 -4.0257463 -4.1198792 -4.1935539 -4.2284188][-4.3547144 -4.3412657 -4.307693 -4.2664285 -4.2166038 -4.1715646 -4.1239977 -4.0793753 -4.042901 -4.0060115 -3.9984143 -4.0520878 -4.1349683 -4.203393 -4.2378097][-4.3653936 -4.3523836 -4.3253064 -4.2924485 -4.2517114 -4.2216611 -4.1905026 -4.1548371 -4.1189508 -4.0876551 -4.0797648 -4.1128554 -4.1673203 -4.2152491 -4.2376742][-4.3587546 -4.3455892 -4.3209529 -4.2977915 -4.2728386 -4.2595267 -4.2421088 -4.2170215 -4.188539 -4.1663556 -4.15912 -4.1757164 -4.2037354 -4.2262139 -4.2304788][-4.3361154 -4.3232622 -4.3044276 -4.2913723 -4.2818856 -4.2777138 -4.2668214 -4.2525172 -4.2365713 -4.2235422 -4.2207456 -4.2282853 -4.2376213 -4.2416377 -4.22886][-4.3089395 -4.2947025 -4.2829437 -4.2816315 -4.2817812 -4.2792416 -4.26966 -4.2617245 -4.2576 -4.2542896 -4.2545657 -4.2572803 -4.2563806 -4.24895 -4.2277803][-4.3023262 -4.2878823 -4.2833815 -4.2908006 -4.2954049 -4.2892566 -4.2747483 -4.2634645 -4.2626514 -4.2651787 -4.2663136 -4.2648864 -4.25631 -4.2403131 -4.2127757][-4.29989 -4.2908072 -4.2968059 -4.3127513 -4.3196664 -4.3106561 -4.2899408 -4.2719169 -4.2689862 -4.2711959 -4.2687459 -4.26483 -4.2518072 -4.2283735 -4.1948161][-4.2854929 -4.2832832 -4.3003755 -4.3257346 -4.3377972 -4.3301921 -4.3101807 -4.2900553 -4.2853451 -4.2836308 -4.2751083 -4.2696433 -4.2555575 -4.2276134 -4.1889343][-4.2499948 -4.2521243 -4.2765045 -4.3105922 -4.3311749 -4.3303552 -4.3177285 -4.3040061 -4.3027072 -4.3020163 -4.2946138 -4.2916641 -4.2797933 -4.2505322 -4.2054148]]...]
INFO - root - 2017-12-08 00:21:28.666761: step 70910, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 54h:11m:18s remains)
INFO - root - 2017-12-08 00:21:35.453445: step 70920, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 48h:58m:26s remains)
INFO - root - 2017-12-08 00:21:42.318872: step 70930, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 46h:36m:13s remains)
INFO - root - 2017-12-08 00:21:49.169433: step 70940, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.650 sec/batch; 47h:15m:02s remains)
INFO - root - 2017-12-08 00:21:56.151311: step 70950, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 53h:46m:18s remains)
INFO - root - 2017-12-08 00:22:02.728928: step 70960, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 48h:36m:45s remains)
INFO - root - 2017-12-08 00:22:09.579319: step 70970, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 48h:52m:45s remains)
INFO - root - 2017-12-08 00:22:16.355307: step 70980, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 46h:12m:36s remains)
INFO - root - 2017-12-08 00:22:23.260693: step 70990, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 53h:18m:11s remains)
INFO - root - 2017-12-08 00:22:29.982745: step 71000, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 51h:06m:41s remains)
2017-12-08 00:22:30.652005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.106039 -4.1112709 -4.1215606 -4.1407151 -4.15687 -4.1701913 -4.1904211 -4.2122 -4.226769 -4.2379985 -4.2546506 -4.2730041 -4.2904291 -4.3091598 -4.3262658][-4.1252117 -4.134264 -4.1360941 -4.1446352 -4.1492176 -4.1502047 -4.1586723 -4.1731739 -4.1872568 -4.2004452 -4.2226033 -4.2495284 -4.27582 -4.2983942 -4.3193064][-4.1685362 -4.1859012 -4.1847668 -4.182127 -4.1696157 -4.1543908 -4.1446166 -4.1454768 -4.1539564 -4.1653991 -4.187243 -4.2194924 -4.25634 -4.2860165 -4.3096313][-4.2060061 -4.2306256 -4.2343225 -4.22736 -4.203938 -4.1760612 -4.1504064 -4.13719 -4.1366143 -4.1449137 -4.160244 -4.1897106 -4.2342119 -4.2730842 -4.3000774][-4.2311006 -4.2593584 -4.2729073 -4.2704129 -4.2451892 -4.2099814 -4.1697316 -4.136591 -4.1200428 -4.1268759 -4.1397924 -4.1609116 -4.2085981 -4.2579112 -4.2903929][-4.241365 -4.270515 -4.2943249 -4.2964778 -4.27281 -4.2304096 -4.1662674 -4.1025109 -4.0659842 -4.078001 -4.1011391 -4.124341 -4.1780367 -4.239924 -4.2804732][-4.2301493 -4.2588453 -4.2881265 -4.2918291 -4.26508 -4.2077932 -4.1159863 -4.0175834 -3.960413 -3.9813004 -4.0292168 -4.0720949 -4.139945 -4.220511 -4.272048][-4.2024264 -4.2232027 -4.2497482 -4.2466526 -4.206018 -4.1257687 -4.0170741 -3.9013929 -3.8294818 -3.8641815 -3.9438763 -4.0162959 -4.1031523 -4.202837 -4.2635069][-4.1802177 -4.1851692 -4.2012911 -4.1886621 -4.1326485 -4.0340281 -3.91982 -3.8043005 -3.7301073 -3.779753 -3.8843863 -3.9831536 -4.0856824 -4.19664 -4.2614074][-4.1772375 -4.1732664 -4.1836157 -4.1705031 -4.1148715 -4.0171094 -3.9089861 -3.8072996 -3.7477403 -3.8056808 -3.9131482 -4.009201 -4.1087704 -4.2146378 -4.2711635][-4.18853 -4.1835747 -4.19256 -4.1875029 -4.15161 -4.0823388 -4.0062447 -3.9316306 -3.8863144 -3.9296103 -4.0151753 -4.0880256 -4.1670613 -4.2515283 -4.2909074][-4.1978736 -4.1928964 -4.2021461 -4.2111692 -4.2022166 -4.1694322 -4.1317306 -4.0801821 -4.0385818 -4.0610456 -4.11593 -4.161478 -4.2169557 -4.27857 -4.3042397][-4.1992621 -4.1928406 -4.2051392 -4.227035 -4.2330852 -4.2227392 -4.204649 -4.1651235 -4.1269317 -4.136322 -4.1680584 -4.1952972 -4.2384458 -4.2876215 -4.3072343][-4.1920056 -4.1824737 -4.1944442 -4.218348 -4.2313862 -4.23205 -4.2176294 -4.1804953 -4.143456 -4.1465325 -4.1672878 -4.1911483 -4.2348318 -4.2809153 -4.3024917][-4.177464 -4.1677489 -4.1772423 -4.1962533 -4.2093849 -4.2113271 -4.1871576 -4.1407604 -4.0991483 -4.1054006 -4.1340866 -4.1670666 -4.2177505 -4.2689328 -4.2957892]]...]
INFO - root - 2017-12-08 00:22:37.554953: step 71010, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 47h:52m:24s remains)
INFO - root - 2017-12-08 00:22:44.371112: step 71020, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 51h:23m:58s remains)
INFO - root - 2017-12-08 00:22:51.057496: step 71030, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 51h:07m:30s remains)
INFO - root - 2017-12-08 00:22:57.860395: step 71040, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 47h:36m:38s remains)
INFO - root - 2017-12-08 00:23:04.653279: step 71050, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 46h:18m:36s remains)
INFO - root - 2017-12-08 00:23:11.508238: step 71060, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 47h:30m:28s remains)
INFO - root - 2017-12-08 00:23:18.409655: step 71070, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.762 sec/batch; 55h:19m:48s remains)
INFO - root - 2017-12-08 00:23:25.187877: step 71080, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 52h:49m:51s remains)
INFO - root - 2017-12-08 00:23:31.936557: step 71090, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 47h:58m:18s remains)
INFO - root - 2017-12-08 00:23:38.584575: step 71100, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 47h:49m:21s remains)
2017-12-08 00:23:39.380072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846479 -4.278893 -4.2776771 -4.2736907 -4.2620907 -4.2391515 -4.2182889 -4.2102146 -4.2312164 -4.2553072 -4.264091 -4.2687154 -4.2727542 -4.2733574 -4.2737617][-4.2844043 -4.2759757 -4.2726645 -4.2657413 -4.2497869 -4.2213025 -4.18947 -4.1699638 -4.1944532 -4.2289391 -4.2432423 -4.2482791 -4.24996 -4.2502542 -4.2506294][-4.2882266 -4.2746978 -4.2643528 -4.25029 -4.225256 -4.1885867 -4.1481786 -4.1179729 -4.1411281 -4.1841893 -4.2040238 -4.2120671 -4.2134976 -4.2130957 -4.2134452][-4.2920718 -4.2739863 -4.25804 -4.2362413 -4.2004743 -4.1538749 -4.1079869 -4.0664144 -4.0825887 -4.1352386 -4.1591749 -4.1696682 -4.173934 -4.1727648 -4.17017][-4.2932715 -4.2760315 -4.2562032 -4.2295718 -4.1889944 -4.13558 -4.0829434 -4.0301633 -4.0406837 -4.1024375 -4.1257734 -4.1304612 -4.1319432 -4.1324306 -4.1286111][-4.2902946 -4.2751541 -4.2513757 -4.221355 -4.176239 -4.1204123 -4.0656877 -4.0055814 -4.0085335 -4.0684958 -4.0858245 -4.08237 -4.0837803 -4.0912981 -4.0912185][-4.2849884 -4.2697849 -4.2442474 -4.2108545 -4.158597 -4.0954723 -4.0288095 -3.9463964 -3.9340639 -3.9921165 -4.0080137 -4.01181 -4.0230808 -4.0449896 -4.0535884][-4.2794442 -4.2642021 -4.23733 -4.1985116 -4.1342454 -4.0515428 -3.9528346 -3.8319457 -3.8074932 -3.8836432 -3.924469 -3.9601479 -3.9950047 -4.0282145 -4.0397987][-4.2680449 -4.2491312 -4.2170887 -4.1697259 -4.0902267 -3.9889929 -3.8691254 -3.7408011 -3.7396684 -3.8539116 -3.9249916 -3.9788625 -4.0200105 -4.0439606 -4.0441823][-4.2459044 -4.2216449 -4.1898217 -4.1435456 -4.073061 -3.9926822 -3.9087572 -3.8297966 -3.8501523 -3.9526265 -4.0053887 -4.0430541 -4.0692563 -4.0765972 -4.0680432][-4.2279634 -4.2035918 -4.1813445 -4.1523418 -4.1071754 -4.0594697 -4.01378 -3.9692397 -3.9821634 -4.047812 -4.07053 -4.0881267 -4.1044016 -4.1067967 -4.1017857][-4.2252464 -4.2045989 -4.1924114 -4.1804867 -4.1569796 -4.13015 -4.1034393 -4.0727944 -4.0721774 -4.1077948 -4.1182675 -4.1288385 -4.1392412 -4.1397004 -4.1400638][-4.2410035 -4.2264647 -4.2218294 -4.219656 -4.2087574 -4.1949458 -4.1804862 -4.1604142 -4.154038 -4.1778712 -4.1890011 -4.1959181 -4.199172 -4.1949916 -4.194149][-4.262917 -4.2537265 -4.2545576 -4.2596245 -4.2588053 -4.2539797 -4.2490568 -4.24106 -4.237164 -4.2530251 -4.2580824 -4.2580619 -4.254797 -4.2494626 -4.2491269][-4.2821245 -4.27677 -4.2787948 -4.2854843 -4.2906337 -4.2924347 -4.2924271 -4.2897277 -4.2865634 -4.2916708 -4.2896075 -4.2855062 -4.2830606 -4.2833238 -4.28755]]...]
INFO - root - 2017-12-08 00:23:46.293496: step 71110, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 53h:07m:47s remains)
INFO - root - 2017-12-08 00:23:53.068976: step 71120, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 48h:31m:40s remains)
INFO - root - 2017-12-08 00:23:59.799908: step 71130, loss = 2.06, batch loss = 2.00 (13.5 examples/sec; 0.594 sec/batch; 43h:07m:13s remains)
INFO - root - 2017-12-08 00:24:06.623258: step 71140, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 46h:00m:14s remains)
INFO - root - 2017-12-08 00:24:13.426676: step 71150, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 50h:58m:12s remains)
INFO - root - 2017-12-08 00:24:20.194380: step 71160, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 52h:57m:38s remains)
INFO - root - 2017-12-08 00:24:26.984109: step 71170, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 48h:08m:04s remains)
INFO - root - 2017-12-08 00:24:33.774489: step 71180, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 45h:47m:00s remains)
INFO - root - 2017-12-08 00:24:40.574109: step 71190, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.687 sec/batch; 49h:50m:35s remains)
INFO - root - 2017-12-08 00:24:47.109947: step 71200, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 52h:09m:35s remains)
2017-12-08 00:24:47.837979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.307127 -4.2968621 -4.2813635 -4.2727623 -4.2641158 -4.2492819 -4.2205229 -4.1846251 -4.167706 -4.1616817 -4.1738014 -4.1914129 -4.2024469 -4.229517 -4.250073][-4.29612 -4.2795229 -4.2584786 -4.2531943 -4.2517848 -4.2424488 -4.2169409 -4.1830292 -4.1664529 -4.1566987 -4.1704769 -4.1896825 -4.2040691 -4.2332654 -4.2554226][-4.2759132 -4.2534695 -4.2260413 -4.2224579 -4.227035 -4.2237864 -4.2034922 -4.1707888 -4.1509218 -4.1391454 -4.1563663 -4.1828752 -4.2065239 -4.2393947 -4.2636809][-4.2551355 -4.2304335 -4.2004323 -4.1954737 -4.1991119 -4.1991959 -4.1842184 -4.1539378 -4.1354318 -4.1298113 -4.1522279 -4.1847568 -4.2156205 -4.2494431 -4.2747679][-4.2421503 -4.2174244 -4.186481 -4.1806431 -4.1785269 -4.1758952 -4.1603847 -4.1296277 -4.1156425 -4.1225071 -4.1562667 -4.1966166 -4.233551 -4.2672529 -4.2922759][-4.2350841 -4.211175 -4.1819148 -4.1752391 -4.1669736 -4.1540828 -4.1268969 -4.0933676 -4.0891786 -4.1106834 -4.1576385 -4.2079945 -4.2519822 -4.2876115 -4.3109326][-4.2268162 -4.2042012 -4.1762033 -4.1689415 -4.1545348 -4.1280785 -4.0811052 -4.0447044 -4.0598874 -4.099658 -4.1582389 -4.217658 -4.2697 -4.3087783 -4.32914][-4.2143278 -4.192368 -4.1673675 -4.1609664 -4.1435885 -4.1064806 -4.0423727 -4.0053124 -4.0440969 -4.102519 -4.1692324 -4.2361121 -4.2929864 -4.3313971 -4.34404][-4.2074332 -4.1870241 -4.1652064 -4.159503 -4.1405578 -4.1006541 -4.0338063 -4.0013037 -4.0528326 -4.1207738 -4.1898255 -4.2586746 -4.3132191 -4.347014 -4.3536029][-4.212285 -4.1930556 -4.1696997 -4.1635528 -4.1410632 -4.1044931 -4.0517359 -4.0317955 -4.0838723 -4.1502733 -4.2160611 -4.2805376 -4.32768 -4.3538842 -4.35698][-4.2195859 -4.2003732 -4.1757402 -4.1665835 -4.1380181 -4.1080351 -4.0767417 -4.0758743 -4.1277609 -4.186738 -4.2445292 -4.3001747 -4.3374319 -4.3538103 -4.3540483][-4.218863 -4.1990952 -4.1761527 -4.168169 -4.138248 -4.1170053 -4.107657 -4.1243744 -4.1750927 -4.2245693 -4.2691526 -4.3112235 -4.3373961 -4.3452373 -4.3441005][-4.2088857 -4.1890788 -4.1722264 -4.172822 -4.152916 -4.1454382 -4.1541176 -4.181427 -4.2244296 -4.2589545 -4.2863016 -4.31269 -4.3289471 -4.3324184 -4.3324742][-4.1881785 -4.167542 -4.1596665 -4.1755133 -4.1753454 -4.1845021 -4.2047753 -4.2325349 -4.2628288 -4.2796626 -4.2894912 -4.3032537 -4.3149424 -4.3202715 -4.3249493][-4.1577616 -4.1380086 -4.140996 -4.1746922 -4.1943579 -4.2183194 -4.2447472 -4.2693634 -4.2854829 -4.2879596 -4.2883472 -4.2948318 -4.3044605 -4.312706 -4.3211966]]...]
INFO - root - 2017-12-08 00:24:54.596576: step 71210, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.613 sec/batch; 44h:31m:13s remains)
INFO - root - 2017-12-08 00:25:01.307708: step 71220, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.618 sec/batch; 44h:49m:25s remains)
INFO - root - 2017-12-08 00:25:08.153428: step 71230, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 51h:20m:16s remains)
INFO - root - 2017-12-08 00:25:14.934875: step 71240, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 52h:39m:47s remains)
INFO - root - 2017-12-08 00:25:21.787038: step 71250, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.661 sec/batch; 47h:57m:07s remains)
INFO - root - 2017-12-08 00:25:28.424964: step 71260, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.609 sec/batch; 44h:10m:01s remains)
INFO - root - 2017-12-08 00:25:35.112478: step 71270, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 46h:07m:14s remains)
INFO - root - 2017-12-08 00:25:41.970673: step 71280, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.737 sec/batch; 53h:27m:58s remains)
INFO - root - 2017-12-08 00:25:48.791569: step 71290, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 50h:13m:09s remains)
INFO - root - 2017-12-08 00:25:55.430057: step 71300, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.626 sec/batch; 45h:23m:24s remains)
2017-12-08 00:25:56.148156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2809443 -4.2621984 -4.245924 -4.2299881 -4.2157 -4.2111926 -4.211617 -4.2036853 -4.1949682 -4.2011294 -4.22147 -4.2453403 -4.2611494 -4.2794752 -4.2975111][-4.2664604 -4.2434058 -4.2274404 -4.2127762 -4.1994481 -4.1941895 -4.19179 -4.1805487 -4.171206 -4.18009 -4.2098775 -4.2425175 -4.2604394 -4.2791982 -4.2960792][-4.2526221 -4.2259016 -4.2116966 -4.1988187 -4.1859994 -4.178309 -4.17338 -4.16239 -4.1583028 -4.1741552 -4.2108245 -4.2505713 -4.2726974 -4.2911692 -4.3030992][-4.2359529 -4.2030454 -4.1874657 -4.175458 -4.1620531 -4.1490984 -4.1387672 -4.1264381 -4.1271029 -4.1528473 -4.1972508 -4.2412357 -4.2713547 -4.2930918 -4.3029709][-4.2070489 -4.1674724 -4.1462746 -4.1380773 -4.1307144 -4.1135364 -4.0866318 -4.0529361 -4.0532088 -4.1004591 -4.1616306 -4.2079391 -4.2457018 -4.2760458 -4.2915049][-4.1689305 -4.1215186 -4.0921168 -4.091454 -4.0919895 -4.0652652 -4.00484 -3.9284277 -3.9207687 -4.0051436 -4.0958729 -4.1565118 -4.2072806 -4.2513986 -4.2777257][-4.1316161 -4.0797472 -4.050652 -4.0542741 -4.0518708 -4.0090451 -3.8995638 -3.7551847 -3.7423816 -3.884315 -4.0213795 -4.1072946 -4.1705894 -4.2256351 -4.2639403][-4.1129756 -4.069983 -4.0490742 -4.0464544 -4.0310197 -3.9703419 -3.8178751 -3.6196189 -3.6225204 -3.819206 -3.9905214 -4.0894489 -4.1546674 -4.2108064 -4.2532854][-4.128778 -4.104147 -4.0957036 -4.0880804 -4.062499 -4.001122 -3.8679154 -3.7166049 -3.741719 -3.9069977 -4.0500336 -4.1309118 -4.1810188 -4.225708 -4.2605929][-4.1692495 -4.1625729 -4.1636453 -4.152895 -4.1256719 -4.0833979 -4.0072026 -3.9334927 -3.9597123 -4.0568466 -4.14726 -4.1951175 -4.2256055 -4.2540298 -4.2756939][-4.2276583 -4.2314162 -4.2341127 -4.2229929 -4.1987 -4.1717119 -4.1346655 -4.1024351 -4.1212335 -4.1789494 -4.2315936 -4.2544327 -4.26802 -4.2818432 -4.2922649][-4.2848949 -4.2944908 -4.2939773 -4.2836375 -4.2668972 -4.2494879 -4.2313285 -4.2139606 -4.2243743 -4.2633724 -4.29785 -4.3086386 -4.3134508 -4.3137584 -4.3170767][-4.3170886 -4.3268995 -4.324863 -4.3165517 -4.308867 -4.3041153 -4.3011346 -4.2918034 -4.2974772 -4.32373 -4.3438482 -4.3483133 -4.3461142 -4.3386116 -4.335608][-4.321763 -4.3265991 -4.3241062 -4.3179297 -4.3144164 -4.3150606 -4.3194184 -4.3215928 -4.3300982 -4.3472967 -4.3566408 -4.3560262 -4.35146 -4.3443022 -4.3408737][-4.3157964 -4.3139238 -4.310586 -4.3067074 -4.3053432 -4.303926 -4.3074441 -4.3130069 -4.3193431 -4.3277512 -4.3331075 -4.3355932 -4.3365369 -4.3351712 -4.3332891]]...]
INFO - root - 2017-12-08 00:26:02.895009: step 71310, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 51h:44m:53s remains)
INFO - root - 2017-12-08 00:26:09.664274: step 71320, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 46h:50m:44s remains)
INFO - root - 2017-12-08 00:26:16.476765: step 71330, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 45h:09m:59s remains)
INFO - root - 2017-12-08 00:26:23.273959: step 71340, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 47h:23m:58s remains)
INFO - root - 2017-12-08 00:26:30.092126: step 71350, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 51h:13m:57s remains)
INFO - root - 2017-12-08 00:26:36.966121: step 71360, loss = 2.03, batch loss = 1.98 (10.7 examples/sec; 0.745 sec/batch; 54h:00m:38s remains)
INFO - root - 2017-12-08 00:26:43.758373: step 71370, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 49h:59m:36s remains)
INFO - root - 2017-12-08 00:26:50.566801: step 71380, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.607 sec/batch; 44h:01m:02s remains)
INFO - root - 2017-12-08 00:26:57.400586: step 71390, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 47h:39m:42s remains)
INFO - root - 2017-12-08 00:27:04.019913: step 71400, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.749 sec/batch; 54h:19m:13s remains)
2017-12-08 00:27:04.722980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2852983 -4.255064 -4.2156725 -4.1720452 -4.1351271 -4.1205654 -4.1092043 -4.0823956 -4.0796065 -4.1106458 -4.1230617 -4.1159544 -4.0856462 -4.0442863 -4.0245247][-4.2706947 -4.23554 -4.1910667 -4.1436353 -4.1140752 -4.1138997 -4.1170144 -4.0973454 -4.091785 -4.1183929 -4.133779 -4.1297989 -4.1010771 -4.0585237 -4.0371666][-4.257627 -4.2097931 -4.149024 -4.0905809 -4.0625758 -4.0733285 -4.093255 -4.0895786 -4.0911269 -4.1180687 -4.1451287 -4.1540232 -4.125905 -4.0769997 -4.0468764][-4.2448759 -4.1872835 -4.1142273 -4.0424886 -4.0021329 -4.008152 -4.0346689 -4.0492554 -4.0656109 -4.09443 -4.1270065 -4.1499448 -4.1357946 -4.0861969 -4.0514541][-4.2308559 -4.1714873 -4.0982494 -4.0166 -3.9618566 -3.9538152 -3.9846883 -4.0148368 -4.0404186 -4.0640559 -4.0910521 -4.1225214 -4.1309457 -4.0932136 -4.0628643][-4.2179928 -4.162365 -4.0907383 -4.0068173 -3.9404988 -3.927484 -3.9626412 -3.9988847 -4.02128 -4.0294204 -4.053637 -4.09127 -4.117589 -4.1015854 -4.0816436][-4.2154555 -4.1643224 -4.099103 -4.027451 -3.9685097 -3.9571753 -3.9864278 -4.0144162 -4.0237041 -4.01667 -4.0355067 -4.0759091 -4.116816 -4.119504 -4.1064072][-4.2243128 -4.1763744 -4.1246781 -4.0757275 -4.0301118 -4.0126696 -4.0278163 -4.041276 -4.0364423 -4.0201058 -4.0325065 -4.0743046 -4.1224079 -4.1347404 -4.1282096][-4.2346945 -4.1939874 -4.1581092 -4.1301823 -4.0969133 -4.0740333 -4.0726748 -4.0744753 -4.0572729 -4.0343485 -4.0414338 -4.0830426 -4.1324615 -4.1485329 -4.1501403][-4.246809 -4.2156754 -4.1912737 -4.1765709 -4.1547713 -4.1242557 -4.1098824 -4.10906 -4.0954013 -4.0740843 -4.0778561 -4.114614 -4.1592264 -4.1796827 -4.1909809][-4.2599564 -4.2329621 -4.2116795 -4.2037611 -4.190177 -4.1568942 -4.1356626 -4.144474 -4.1521072 -4.1387444 -4.1376514 -4.1640491 -4.1997447 -4.2250752 -4.2399697][-4.27978 -4.2559395 -4.2354784 -4.2263622 -4.2153344 -4.1894426 -4.1746674 -4.1909852 -4.2133408 -4.2079935 -4.2020469 -4.2189922 -4.2450881 -4.2687216 -4.2811074][-4.2977643 -4.2812762 -4.264277 -4.2531123 -4.2435021 -4.2270675 -4.220758 -4.2363224 -4.2577109 -4.2597518 -4.2543812 -4.2591329 -4.2739539 -4.2898669 -4.2972889][-4.31285 -4.3009748 -4.2870831 -4.27729 -4.2706633 -4.2622352 -4.2581668 -4.2689471 -4.2858968 -4.2910547 -4.28655 -4.2857256 -4.293056 -4.3010459 -4.3026719][-4.3258314 -4.31712 -4.30599 -4.2992091 -4.2956395 -4.2895889 -4.2843924 -4.2912207 -4.3047981 -4.3111739 -4.3091292 -4.3077507 -4.3108854 -4.3140478 -4.3158846]]...]
INFO - root - 2017-12-08 00:27:11.480986: step 71410, loss = 2.06, batch loss = 2.01 (13.1 examples/sec; 0.612 sec/batch; 44h:23m:36s remains)
INFO - root - 2017-12-08 00:27:18.365671: step 71420, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 46h:43m:43s remains)
INFO - root - 2017-12-08 00:27:25.177430: step 71430, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 51h:31m:33s remains)
INFO - root - 2017-12-08 00:27:31.947534: step 71440, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 51h:31m:08s remains)
INFO - root - 2017-12-08 00:27:38.886853: step 71450, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 49h:35m:18s remains)
INFO - root - 2017-12-08 00:27:45.714963: step 71460, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 47h:54m:01s remains)
INFO - root - 2017-12-08 00:27:52.600269: step 71470, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 50h:59m:05s remains)
INFO - root - 2017-12-08 00:27:59.485501: step 71480, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.687 sec/batch; 49h:46m:59s remains)
INFO - root - 2017-12-08 00:28:06.246365: step 71490, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 47h:51m:06s remains)
INFO - root - 2017-12-08 00:28:12.753712: step 71500, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 45h:42m:15s remains)
2017-12-08 00:28:13.524662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3167305 -4.3169894 -4.3092403 -4.2939734 -4.2767229 -4.2702727 -4.280767 -4.2971935 -4.3099122 -4.3155923 -4.3130016 -4.3053713 -4.3010936 -4.3015556 -4.3018327][-4.3191686 -4.316359 -4.3011479 -4.2725611 -4.2416735 -4.2290025 -4.2438722 -4.2730312 -4.2994437 -4.3165073 -4.3207679 -4.313014 -4.3064919 -4.3050075 -4.3040214][-4.3198595 -4.3129849 -4.2901731 -4.2466812 -4.1978436 -4.174036 -4.1887522 -4.231194 -4.2731528 -4.3071408 -4.3247075 -4.320528 -4.3127041 -4.3093305 -4.306355][-4.3216553 -4.3107014 -4.281455 -4.2240944 -4.1572919 -4.1209583 -4.1334548 -4.1864047 -4.24038 -4.2880888 -4.3200822 -4.3243952 -4.3185673 -4.3127565 -4.3076506][-4.31909 -4.30614 -4.27641 -4.2104306 -4.1273918 -4.0773563 -4.0845566 -4.1444321 -4.2056818 -4.25985 -4.3038874 -4.3206329 -4.32013 -4.3136873 -4.3069081][-4.3078027 -4.2981892 -4.2735019 -4.2046165 -4.1079826 -4.03793 -4.0302954 -4.0936341 -4.1645617 -4.2252779 -4.2777057 -4.3077745 -4.3156734 -4.3123226 -4.3054147][-4.2860413 -4.2821879 -4.2662106 -4.2005715 -4.0917616 -3.994549 -3.9581597 -4.0177116 -4.1041684 -4.1804643 -4.2448549 -4.2895122 -4.307128 -4.3088984 -4.304008][-4.2579947 -4.2607045 -4.2528911 -4.193635 -4.0777931 -3.9547687 -3.882894 -3.9329329 -4.0394268 -4.141542 -4.2213683 -4.2777228 -4.3048239 -4.3112493 -4.3080754][-4.2324162 -4.242878 -4.2411551 -4.1929832 -4.0837731 -3.9518082 -3.86154 -3.9007716 -4.0171232 -4.1330476 -4.2178192 -4.27557 -4.3051953 -4.3149714 -4.3145003][-4.2303424 -4.2446342 -4.2446609 -4.2080121 -4.1183987 -3.9992509 -3.9160609 -3.9498835 -4.0519509 -4.154181 -4.2264485 -4.2749753 -4.2995286 -4.3102384 -4.3149786][-4.2557859 -4.2661247 -4.2636218 -4.2355609 -4.1676126 -4.070271 -4.0024958 -4.0274549 -4.106626 -4.1853104 -4.2367916 -4.26652 -4.2800913 -4.2902803 -4.3035855][-4.2899952 -4.2943726 -4.2910285 -4.2713628 -4.221 -4.1466165 -4.0949116 -4.107862 -4.1636643 -4.2170172 -4.243587 -4.24887 -4.245513 -4.2517128 -4.2749696][-4.315134 -4.3167162 -4.3151846 -4.3055096 -4.273634 -4.2247996 -4.1902928 -4.194118 -4.2274194 -4.2537332 -4.2492933 -4.2230306 -4.1996803 -4.1989541 -4.2331705][-4.3264914 -4.3267918 -4.3269668 -4.324801 -4.30933 -4.2827959 -4.263267 -4.262814 -4.2797966 -4.2829819 -4.2476106 -4.1891212 -4.1472993 -4.1443481 -4.19066][-4.3191891 -4.3181977 -4.3207178 -4.322576 -4.3179584 -4.30448 -4.2943316 -4.2942581 -4.30245 -4.2916188 -4.2341185 -4.1500406 -4.0905795 -4.0859642 -4.1467743]]...]
INFO - root - 2017-12-08 00:28:20.293824: step 71510, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 50h:28m:42s remains)
INFO - root - 2017-12-08 00:28:27.111815: step 71520, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 50h:50m:09s remains)
INFO - root - 2017-12-08 00:28:33.750673: step 71530, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 48h:34m:08s remains)
INFO - root - 2017-12-08 00:28:40.478374: step 71540, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 47h:11m:19s remains)
INFO - root - 2017-12-08 00:28:47.342932: step 71550, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 47h:52m:27s remains)
INFO - root - 2017-12-08 00:28:54.218649: step 71560, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 52h:06m:16s remains)
INFO - root - 2017-12-08 00:29:00.974112: step 71570, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 50h:50m:33s remains)
INFO - root - 2017-12-08 00:29:07.673809: step 71580, loss = 2.05, batch loss = 1.99 (13.6 examples/sec; 0.590 sec/batch; 42h:44m:41s remains)
INFO - root - 2017-12-08 00:29:14.454666: step 71590, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 49h:46m:14s remains)
INFO - root - 2017-12-08 00:29:21.165841: step 71600, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:39m:04s remains)
2017-12-08 00:29:22.108003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3272448 -4.3276086 -4.3245687 -4.3199315 -4.3170171 -4.3171806 -4.3165388 -4.3168254 -4.3185534 -4.3193088 -4.3202052 -4.3188519 -4.314826 -4.3108845 -4.3081865][-4.3237181 -4.32268 -4.3196278 -4.3162408 -4.3126822 -4.310936 -4.3058171 -4.3057613 -4.3096561 -4.313201 -4.3188925 -4.31876 -4.310638 -4.2997155 -4.2909193][-4.3134089 -4.3086066 -4.3047304 -4.3020139 -4.2971883 -4.2927642 -4.2830806 -4.2862039 -4.296464 -4.30588 -4.31972 -4.3248024 -4.3122077 -4.2876058 -4.2679987][-4.2993722 -4.2903924 -4.2842393 -4.2793503 -4.2673659 -4.2537174 -4.23556 -4.2417321 -4.2615991 -4.282825 -4.310472 -4.3251433 -4.3126955 -4.276875 -4.2436056][-4.2878466 -4.2733808 -4.2621665 -4.2509542 -4.2267075 -4.194222 -4.1578474 -4.1635776 -4.1994109 -4.2404041 -4.2829289 -4.3064842 -4.295959 -4.2578268 -4.2162275][-4.2752709 -4.2509341 -4.2329392 -4.2147808 -4.1720228 -4.1083994 -4.0380177 -4.0364432 -4.1001387 -4.174314 -4.2341022 -4.2648377 -4.2627788 -4.2350893 -4.1975484][-4.2694683 -4.2363081 -4.2098866 -4.1798077 -4.1138973 -4.0122843 -3.8958676 -3.8763356 -3.977356 -4.0934577 -4.1761513 -4.2203197 -4.2332468 -4.2219577 -4.1960487][-4.2749848 -4.2386346 -4.2074914 -4.1656375 -4.083324 -3.9590921 -3.8120584 -3.7754507 -3.9051261 -4.0480618 -4.1444907 -4.1972809 -4.223228 -4.2255907 -4.2066779][-4.2889967 -4.2612014 -4.2350426 -4.1969032 -4.1250238 -4.017972 -3.8965764 -3.8782363 -3.986203 -4.0928164 -4.1685896 -4.2125125 -4.2410941 -4.2493072 -4.230617][-4.2966924 -4.2782912 -4.2593236 -4.2293787 -4.1755304 -4.0968084 -4.0166211 -4.0231552 -4.0994987 -4.1619034 -4.2112942 -4.2455139 -4.2699924 -4.2750549 -4.25232][-4.2911725 -4.2788134 -4.2658019 -4.2435575 -4.2057309 -4.1542273 -4.112802 -4.1350975 -4.1856027 -4.2191882 -4.2503762 -4.272418 -4.2865038 -4.2836957 -4.2553492][-4.2854829 -4.2809281 -4.2765245 -4.2622514 -4.23954 -4.2077608 -4.1877122 -4.2128162 -4.2467904 -4.2653761 -4.2833767 -4.2932262 -4.2963648 -4.2852082 -4.2542968][-4.2887359 -4.2889991 -4.2888727 -4.281455 -4.2697897 -4.2501011 -4.2376609 -4.256537 -4.2769523 -4.286356 -4.2967577 -4.2998486 -4.2991705 -4.2838764 -4.2513843][-4.2983952 -4.2988482 -4.2962589 -4.29118 -4.2841182 -4.2689171 -4.2587538 -4.2713881 -4.2831225 -4.2858763 -4.2902184 -4.2952323 -4.2972918 -4.2832475 -4.2552624][-4.3068852 -4.3030725 -4.2954116 -4.2877889 -4.27991 -4.2679133 -4.2616081 -4.2722712 -4.2800794 -4.2798533 -4.282517 -4.2877331 -4.291502 -4.2829947 -4.2654181]]...]
INFO - root - 2017-12-08 00:29:28.955472: step 71610, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 52h:38m:37s remains)
INFO - root - 2017-12-08 00:29:35.712562: step 71620, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 52h:36m:36s remains)
INFO - root - 2017-12-08 00:29:42.488816: step 71630, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 48h:44m:43s remains)
INFO - root - 2017-12-08 00:29:49.206188: step 71640, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 46h:20m:10s remains)
INFO - root - 2017-12-08 00:29:55.944029: step 71650, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.636 sec/batch; 46h:06m:56s remains)
INFO - root - 2017-12-08 00:30:02.743948: step 71660, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 51h:53m:32s remains)
INFO - root - 2017-12-08 00:30:09.603298: step 71670, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.747 sec/batch; 54h:05m:48s remains)
INFO - root - 2017-12-08 00:30:16.318948: step 71680, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.636 sec/batch; 46h:04m:25s remains)
INFO - root - 2017-12-08 00:30:23.136152: step 71690, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 48h:03m:19s remains)
INFO - root - 2017-12-08 00:30:29.676033: step 71700, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 45h:28m:52s remains)
2017-12-08 00:30:30.449820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2383676 -4.2368245 -4.23633 -4.2419457 -4.247952 -4.2504086 -4.246171 -4.233253 -4.2163949 -4.200284 -4.18856 -4.1906495 -4.2107668 -4.2293572 -4.2446733][-4.21004 -4.2084579 -4.2131767 -4.2252665 -4.2362957 -4.2387609 -4.2280064 -4.2065 -4.1826553 -4.1620407 -4.1485209 -4.1502609 -4.1719151 -4.1929216 -4.21029][-4.1969624 -4.193006 -4.201901 -4.2195935 -4.2319646 -4.2306738 -4.2093592 -4.1798859 -4.1527624 -4.1341691 -4.1292119 -4.1370926 -4.1550856 -4.1756535 -4.1948028][-4.1786723 -4.1690645 -4.1778069 -4.1940947 -4.2051067 -4.202611 -4.1745329 -4.1375031 -4.1041594 -4.0906692 -4.0987072 -4.1133304 -4.1338449 -4.1604805 -4.185349][-4.1696424 -4.1495852 -4.1486006 -4.1573572 -4.1587729 -4.1463771 -4.1031356 -4.0472054 -4.002234 -4.0074558 -4.0392594 -4.0665922 -4.0982161 -4.1359448 -4.1713309][-4.1666131 -4.1356936 -4.1251025 -4.1249657 -4.1097536 -4.0780578 -4.0149837 -3.9330828 -3.8747683 -3.91034 -3.9743614 -4.0215406 -4.071332 -4.121428 -4.1650381][-4.1675873 -4.1266203 -4.1033835 -4.0897555 -4.0614204 -4.0169415 -3.9395947 -3.8383968 -3.7681737 -3.828588 -3.9249728 -3.996927 -4.0655179 -4.122499 -4.1653762][-4.1615033 -4.1189966 -4.0911837 -4.0677042 -4.0354261 -3.995816 -3.9298184 -3.8409095 -3.7810907 -3.8383145 -3.9300261 -4.0017571 -4.0700312 -4.1290836 -4.1733284][-4.1622238 -4.1236987 -4.1006541 -4.0745492 -4.0412903 -4.0120211 -3.9736562 -3.9200583 -3.878572 -3.9114826 -3.9715559 -4.0255194 -4.08376 -4.1435647 -4.1912079][-4.1819429 -4.1488442 -4.1289325 -4.1069717 -4.0835485 -4.0666704 -4.0497332 -4.027719 -4.0008125 -4.0084171 -4.0370235 -4.0701909 -4.1188707 -4.1760316 -4.22402][-4.2166634 -4.1908865 -4.1773496 -4.1631064 -4.1504288 -4.1422949 -4.1365347 -4.1302829 -4.1078167 -4.1023564 -4.1150913 -4.1394968 -4.1802216 -4.2253728 -4.2641][-4.25057 -4.2337832 -4.2262573 -4.2182956 -4.2091169 -4.2015085 -4.2001009 -4.201272 -4.1864462 -4.1799221 -4.1907573 -4.2114415 -4.2399135 -4.2709746 -4.2980838][-4.2796817 -4.2691393 -4.2665596 -4.2646928 -4.2594986 -4.2523208 -4.2539692 -4.2565894 -4.2443438 -4.2389927 -4.2487411 -4.2640867 -4.2830305 -4.30454 -4.3237314][-4.298902 -4.2931957 -4.2929 -4.2938004 -4.2912617 -4.2877083 -4.2908077 -4.2907667 -4.2824044 -4.2779217 -4.2838526 -4.2932673 -4.3052893 -4.32024 -4.3347206][-4.3124781 -4.3099451 -4.3097181 -4.3104358 -4.3094873 -4.3093491 -4.3121619 -4.3116126 -4.3072329 -4.3041987 -4.3057041 -4.3098478 -4.3162093 -4.3254843 -4.3354354]]...]
INFO - root - 2017-12-08 00:30:37.252485: step 71710, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 48h:55m:25s remains)
INFO - root - 2017-12-08 00:30:43.975489: step 71720, loss = 2.03, batch loss = 1.97 (12.4 examples/sec; 0.645 sec/batch; 46h:44m:08s remains)
INFO - root - 2017-12-08 00:30:50.697821: step 71730, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 46h:50m:39s remains)
INFO - root - 2017-12-08 00:30:57.474936: step 71740, loss = 2.03, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 50h:22m:11s remains)
INFO - root - 2017-12-08 00:31:04.370198: step 71750, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 53h:42m:07s remains)
INFO - root - 2017-12-08 00:31:11.187717: step 71760, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 46h:39m:17s remains)
INFO - root - 2017-12-08 00:31:17.928193: step 71770, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 46h:42m:18s remains)
INFO - root - 2017-12-08 00:31:24.665401: step 71780, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 46h:49m:45s remains)
INFO - root - 2017-12-08 00:31:31.524218: step 71790, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 50h:51m:38s remains)
INFO - root - 2017-12-08 00:31:38.198137: step 71800, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 53h:00m:19s remains)
2017-12-08 00:31:38.979572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.146337 -4.1343637 -4.1617222 -4.2048616 -4.2381306 -4.24855 -4.2108545 -4.1287746 -4.0424104 -3.9851332 -3.9924908 -4.048563 -4.1057591 -4.1499062 -4.1886911][-4.1667175 -4.1469679 -4.1808906 -4.2358003 -4.2766018 -4.289834 -4.2553039 -4.1764555 -4.0874672 -4.018539 -4.0077562 -4.0530596 -4.113503 -4.1643939 -4.2070446][-4.1770434 -4.1502705 -4.1865182 -4.2491608 -4.2973194 -4.3100357 -4.2773166 -4.2078619 -4.1310287 -4.0689578 -4.0509391 -4.0809584 -4.1355634 -4.1842213 -4.2270207][-4.1890049 -4.1574745 -4.1871533 -4.2470717 -4.2943211 -4.3033042 -4.2728882 -4.2162404 -4.1569147 -4.1041451 -4.080246 -4.095181 -4.1380124 -4.1834269 -4.2284827][-4.2007756 -4.1646605 -4.1786976 -4.222795 -4.257576 -4.2650189 -4.2443762 -4.2020745 -4.1531811 -4.1058283 -4.0797553 -4.0868421 -4.1226931 -4.1686683 -4.2169261][-4.20313 -4.1568217 -4.1530113 -4.1766076 -4.195415 -4.200141 -4.1909475 -4.158525 -4.1108327 -4.0675163 -4.0483718 -4.06013 -4.0939579 -4.1404672 -4.1910968][-4.2224107 -4.1713848 -4.1458168 -4.1356792 -4.1267171 -4.1214767 -4.1173377 -4.08991 -4.0458746 -4.0104189 -3.9980989 -4.0092769 -4.0384607 -4.0886412 -4.1485524][-4.2464437 -4.1978469 -4.1553259 -4.1166916 -4.0830193 -4.0702643 -4.066905 -4.0432343 -4.0085864 -3.9921553 -3.9882884 -3.9880109 -4.0101242 -4.0597091 -4.1165004][-4.2603564 -4.2181649 -4.1734705 -4.1288056 -4.089931 -4.0725865 -4.061162 -4.0364037 -4.0186481 -4.0289931 -4.0375066 -4.0366526 -4.0537529 -4.0889049 -4.1265621][-4.2623696 -4.2229447 -4.1857553 -4.1519737 -4.1233206 -4.10868 -4.0939069 -4.0723524 -4.0688396 -4.09124 -4.109973 -4.1171093 -4.1279387 -4.1439705 -4.16608][-4.2600355 -4.2283907 -4.2050357 -4.1910143 -4.1821103 -4.1766667 -4.1658773 -4.1499586 -4.1459656 -4.1567159 -4.1671314 -4.1724029 -4.1779118 -4.1868496 -4.2025075][-4.2491617 -4.2315183 -4.2273259 -4.2355046 -4.245213 -4.2481151 -4.2436852 -4.2317495 -4.22178 -4.2115521 -4.2068987 -4.202538 -4.1999922 -4.2074032 -4.2200646][-4.2400813 -4.2337041 -4.2396727 -4.253963 -4.2691741 -4.2784028 -4.2800722 -4.273706 -4.2635913 -4.245327 -4.236834 -4.2289815 -4.223146 -4.227746 -4.2312317][-4.2252722 -4.2224088 -4.2332239 -4.2496386 -4.2623591 -4.2722726 -4.2773824 -4.2795072 -4.2767825 -4.2678165 -4.2641983 -4.25387 -4.2463489 -4.2467713 -4.2403765][-4.1964388 -4.1982732 -4.213253 -4.2286463 -4.2363482 -4.2412291 -4.2485628 -4.26124 -4.2688918 -4.2658253 -4.2585778 -4.2430511 -4.2384586 -4.2410169 -4.2302427]]...]
INFO - root - 2017-12-08 00:31:45.673267: step 71810, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 47h:47m:12s remains)
INFO - root - 2017-12-08 00:31:52.507645: step 71820, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 51h:31m:28s remains)
INFO - root - 2017-12-08 00:31:59.181102: step 71830, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 51h:31m:32s remains)
INFO - root - 2017-12-08 00:32:05.982274: step 71840, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 52h:02m:46s remains)
INFO - root - 2017-12-08 00:32:12.755075: step 71850, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 45h:20m:59s remains)
INFO - root - 2017-12-08 00:32:19.614320: step 71860, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 48h:39m:27s remains)
INFO - root - 2017-12-08 00:32:26.434757: step 71870, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 53h:47m:55s remains)
INFO - root - 2017-12-08 00:32:33.285211: step 71880, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 49h:35m:41s remains)
INFO - root - 2017-12-08 00:32:40.052459: step 71890, loss = 2.05, batch loss = 1.99 (13.4 examples/sec; 0.595 sec/batch; 43h:04m:36s remains)
INFO - root - 2017-12-08 00:32:46.473863: step 71900, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 47h:09m:08s remains)
2017-12-08 00:32:47.259044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1483579 -4.2000823 -4.228713 -4.2444992 -4.2451711 -4.2349062 -4.2313347 -4.245501 -4.2663097 -4.2890677 -4.3046913 -4.3024764 -4.2840648 -4.263833 -4.2556076][-4.162611 -4.2095852 -4.2303767 -4.235641 -4.2264171 -4.2115426 -4.20966 -4.2320261 -4.2623906 -4.2929158 -4.3150907 -4.319561 -4.3031759 -4.278151 -4.263588][-4.1809559 -4.2176375 -4.224597 -4.2125416 -4.1903586 -4.1714373 -4.1723471 -4.2022943 -4.2397528 -4.27773 -4.3126016 -4.3296676 -4.32241 -4.2999897 -4.283998][-4.19981 -4.2181396 -4.2008133 -4.1585431 -4.1112719 -4.0874119 -4.1033258 -4.1529818 -4.206913 -4.2539272 -4.2987137 -4.32473 -4.328361 -4.3125625 -4.2977991][-4.2208514 -4.2169623 -4.1673522 -4.0837131 -4.0080237 -3.981884 -4.0186768 -4.0984492 -4.1707911 -4.2270041 -4.278419 -4.31072 -4.3214474 -4.3118043 -4.2999048][-4.2336555 -4.2099652 -4.1340218 -4.0155635 -3.9243288 -3.9030275 -3.9594612 -4.0563345 -4.1348038 -4.196373 -4.2529364 -4.2845559 -4.2961779 -4.28946 -4.281383][-4.2240944 -4.1917739 -4.110538 -3.9869146 -3.9024103 -3.893259 -3.9529815 -4.040019 -4.10952 -4.1675739 -4.2182121 -4.2415195 -4.2460194 -4.2392154 -4.23161][-4.1846013 -4.1573534 -4.0933828 -3.9918401 -3.9285202 -3.9307175 -3.9784238 -4.0403805 -4.093092 -4.1365666 -4.1694994 -4.176826 -4.1686764 -4.1596904 -4.1531706][-4.1396127 -4.1285315 -4.0902557 -4.0212541 -3.9799063 -3.9885573 -4.0241394 -4.0627074 -4.0936251 -4.1117544 -4.1170368 -4.1015978 -4.0817566 -4.0791564 -4.0843086][-4.1152062 -4.1244779 -4.110949 -4.0742908 -4.0565963 -4.0677075 -4.0903015 -4.1098804 -4.1198969 -4.1126313 -4.092104 -4.0586567 -4.03549 -4.0496039 -4.0753345][-4.1381063 -4.1588435 -4.1615992 -4.149065 -4.14664 -4.1551065 -4.1654477 -4.1698589 -4.1659527 -4.1482387 -4.120934 -4.0860491 -4.0669208 -4.09211 -4.1294985][-4.2003546 -4.2199225 -4.2272711 -4.2272811 -4.2331591 -4.2416577 -4.2452831 -4.2413568 -4.2309361 -4.2130733 -4.1908622 -4.1633234 -4.1511397 -4.1761589 -4.2100782][-4.268692 -4.2811284 -4.2859149 -4.2893348 -4.2977529 -4.3071752 -4.3097219 -4.3047619 -4.2937241 -4.2799788 -4.2658467 -4.2485385 -4.2418 -4.2595997 -4.2829795][-4.3211021 -4.3276095 -4.3293858 -4.3322563 -4.3378434 -4.3437729 -4.3456736 -4.3429041 -4.3355126 -4.3269162 -4.321094 -4.3128171 -4.3110595 -4.3218613 -4.3327084][-4.356349 -4.3609562 -4.3623633 -4.3640203 -4.36669 -4.3688307 -4.3688173 -4.3662071 -4.3616414 -4.3589163 -4.359005 -4.3574557 -4.3582439 -4.3634787 -4.3674564]]...]
INFO - root - 2017-12-08 00:32:54.039124: step 71910, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 50h:25m:12s remains)
INFO - root - 2017-12-08 00:33:00.808287: step 71920, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 48h:05m:34s remains)
INFO - root - 2017-12-08 00:33:07.679110: step 71930, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 48h:11m:47s remains)
INFO - root - 2017-12-08 00:33:14.506172: step 71940, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 51h:59m:47s remains)
INFO - root - 2017-12-08 00:33:21.391370: step 71950, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.754 sec/batch; 54h:35m:35s remains)
INFO - root - 2017-12-08 00:33:28.145216: step 71960, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 48h:42m:21s remains)
INFO - root - 2017-12-08 00:33:34.870094: step 71970, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 46h:29m:14s remains)
INFO - root - 2017-12-08 00:33:41.734599: step 71980, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 45h:26m:58s remains)
INFO - root - 2017-12-08 00:33:48.565873: step 71990, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 51h:36m:36s remains)
INFO - root - 2017-12-08 00:33:55.174557: step 72000, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 52h:20m:13s remains)
2017-12-08 00:33:55.916298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.346004 -4.348568 -4.3441525 -4.3340974 -4.3223758 -4.3101325 -4.2974052 -4.2908263 -4.2936363 -4.300117 -4.305994 -4.3084388 -4.309082 -4.3096628 -4.3122358][-4.3378606 -4.3424549 -4.3374553 -4.3221793 -4.3046484 -4.2859931 -4.264792 -4.2484303 -4.2451882 -4.2518559 -4.2594147 -4.2638512 -4.2677317 -4.2727261 -4.2781425][-4.3288164 -4.3353491 -4.3295064 -4.3089337 -4.2845764 -4.2588615 -4.2297158 -4.2040381 -4.193254 -4.20047 -4.2095418 -4.2105427 -4.2141523 -4.2236967 -4.2319026][-4.3218541 -4.327136 -4.3177457 -4.2935591 -4.2638211 -4.2337794 -4.2009645 -4.1693907 -4.1531434 -4.1597929 -4.1713576 -4.1696944 -4.169682 -4.1802187 -4.1904235][-4.3116474 -4.3104687 -4.2947068 -4.2679515 -4.235548 -4.2027593 -4.1706533 -4.1395698 -4.1211572 -4.1260056 -4.1397591 -4.1435637 -4.1432681 -4.15285 -4.1635838][-4.2984085 -4.2890406 -4.2648921 -4.2351193 -4.2013378 -4.1677361 -4.1369524 -4.1105862 -4.0954061 -4.0985203 -4.1131611 -4.1239557 -4.1300049 -4.1395721 -4.1516652][-4.2896013 -4.277657 -4.2513804 -4.2203989 -4.1855035 -4.1517191 -4.1211753 -4.1013408 -4.0925918 -4.0949411 -4.1082144 -4.1220732 -4.1331534 -4.1431532 -4.1558356][-4.2910671 -4.2831187 -4.2621984 -4.2326617 -4.1984448 -4.1618924 -4.1297693 -4.1144037 -4.1108141 -4.1147213 -4.1287832 -4.1428843 -4.1550913 -4.1637096 -4.1748447][-4.2956204 -4.2904148 -4.2766705 -4.2556524 -4.2284203 -4.1919479 -4.1621771 -4.1502004 -4.1491256 -4.1545973 -4.1697936 -4.1819224 -4.1905737 -4.1964445 -4.2063913][-4.3024306 -4.2970166 -4.2869649 -4.2725668 -4.250165 -4.21557 -4.19147 -4.1846642 -4.1844745 -4.1890917 -4.2052608 -4.2181358 -4.2266083 -4.23399 -4.242321][-4.3147707 -4.3091736 -4.3005791 -4.2893071 -4.2678795 -4.2358918 -4.2171793 -4.2177563 -4.2194896 -4.2227745 -4.237433 -4.2502961 -4.257484 -4.2639179 -4.2689304][-4.3298907 -4.3247886 -4.3198104 -4.3129334 -4.2951183 -4.2676654 -4.2527976 -4.2561183 -4.260529 -4.2646313 -4.2764997 -4.2857404 -4.289722 -4.2906623 -4.2924523][-4.3432231 -4.3384457 -4.3378773 -4.3381009 -4.3282256 -4.3085194 -4.2968144 -4.2982516 -4.30317 -4.3081117 -4.31696 -4.3233128 -4.3246427 -4.3219509 -4.3197145][-4.3513207 -4.3463774 -4.3458123 -4.3481326 -4.3451447 -4.3353939 -4.3276334 -4.3268642 -4.3298912 -4.3333654 -4.3380251 -4.3409905 -4.3407931 -4.3379445 -4.3354292][-4.3567905 -4.3518481 -4.3484674 -4.34733 -4.3452787 -4.3406625 -4.3358183 -4.3333755 -4.3342443 -4.3365717 -4.3392987 -4.3412986 -4.3420696 -4.3417954 -4.3416672]]...]
INFO - root - 2017-12-08 00:34:02.747211: step 72010, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 50h:57m:48s remains)
INFO - root - 2017-12-08 00:34:09.560966: step 72020, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 51h:01m:16s remains)
INFO - root - 2017-12-08 00:34:16.282093: step 72030, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 49h:09m:43s remains)
INFO - root - 2017-12-08 00:34:23.056912: step 72040, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 46h:34m:09s remains)
INFO - root - 2017-12-08 00:34:29.806511: step 72050, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 46h:31m:21s remains)
INFO - root - 2017-12-08 00:34:36.696890: step 72060, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 53h:10m:36s remains)
INFO - root - 2017-12-08 00:34:43.595152: step 72070, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 53h:40m:17s remains)
INFO - root - 2017-12-08 00:34:50.379389: step 72080, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 47h:33m:27s remains)
INFO - root - 2017-12-08 00:34:57.155403: step 72090, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 44h:29m:59s remains)
INFO - root - 2017-12-08 00:35:03.803753: step 72100, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 46h:16m:38s remains)
2017-12-08 00:35:04.669262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2955017 -4.2859054 -4.2632394 -4.2316995 -4.2077994 -4.2044239 -4.2016664 -4.1910233 -4.1753755 -4.1686363 -4.1816483 -4.2104888 -4.2272553 -4.2330103 -4.2424841][-4.2935581 -4.27909 -4.2397695 -4.1865554 -4.1513834 -4.1494417 -4.1497941 -4.1358781 -4.11454 -4.110662 -4.1306615 -4.1631193 -4.177063 -4.1814094 -4.1963048][-4.2963834 -4.2788849 -4.2284532 -4.1607 -4.1163 -4.1131206 -4.1152253 -4.103601 -4.084702 -4.0898619 -4.1169291 -4.1441417 -4.1486063 -4.1523027 -4.1718297][-4.2996407 -4.2800584 -4.2234807 -4.1474376 -4.09485 -4.08763 -4.0889416 -4.080215 -4.0719404 -4.0927091 -4.1311693 -4.1557956 -4.1532545 -4.1564908 -4.177][-4.2981958 -4.2715349 -4.2065258 -4.1265173 -4.0655522 -4.0505552 -4.0466943 -4.0415564 -4.0475955 -4.0902896 -4.141891 -4.16891 -4.1686611 -4.1746273 -4.1909971][-4.2957163 -4.2636013 -4.1935954 -4.108356 -4.02823 -3.9898887 -3.9640241 -3.9486413 -3.9703183 -4.048264 -4.1245718 -4.1654954 -4.1760979 -4.1870251 -4.19913][-4.2984238 -4.2662449 -4.1985288 -4.1059375 -3.9959559 -3.91629 -3.8485208 -3.8046365 -3.8467896 -3.9752538 -4.0902057 -4.1573219 -4.1902714 -4.2091866 -4.2182689][-4.3015509 -4.2698965 -4.2055187 -4.1113586 -3.9892278 -3.8838854 -3.7877383 -3.7214448 -3.7802286 -3.9370432 -4.0713897 -4.151114 -4.1974983 -4.2220936 -4.23533][-4.3050723 -4.2715068 -4.2090964 -4.1212659 -4.0129781 -3.9247108 -3.8508286 -3.8102002 -3.86636 -3.9890487 -4.0936451 -4.1560521 -4.1934443 -4.2179952 -4.2378531][-4.3083043 -4.2742424 -4.2125793 -4.13745 -4.0563135 -4.0034914 -3.9643109 -3.9504762 -3.9968393 -4.077949 -4.1473837 -4.1846113 -4.2008934 -4.2169547 -4.2370324][-4.308969 -4.2778182 -4.2229781 -4.1636682 -4.1115284 -4.0924139 -4.0800257 -4.0772939 -4.1039672 -4.1479654 -4.1892214 -4.2106686 -4.2143087 -4.2204471 -4.234499][-4.3102703 -4.2844892 -4.2414603 -4.196311 -4.1669168 -4.172338 -4.1782327 -4.1799388 -4.1878262 -4.1992579 -4.2124987 -4.2199149 -4.2174482 -4.219492 -4.23094][-4.3124552 -4.2931914 -4.2619095 -4.2303019 -4.2162914 -4.2342019 -4.2490716 -4.2533588 -4.252254 -4.2478404 -4.24623 -4.2459755 -4.238596 -4.2357688 -4.240624][-4.3152552 -4.3000135 -4.2783513 -4.257544 -4.2541037 -4.27547 -4.2939997 -4.3004618 -4.2957506 -4.2858162 -4.2785063 -4.2765546 -4.2683029 -4.263741 -4.2648935][-4.3182731 -4.3066964 -4.2917447 -4.2789135 -4.2798853 -4.2976732 -4.3144965 -4.3220444 -4.3194489 -4.3108721 -4.3045816 -4.3034444 -4.2972097 -4.2932744 -4.2929955]]...]
INFO - root - 2017-12-08 00:35:11.442942: step 72110, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 48h:10m:42s remains)
INFO - root - 2017-12-08 00:35:18.236824: step 72120, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 46h:34m:18s remains)
INFO - root - 2017-12-08 00:35:25.073178: step 72130, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.705 sec/batch; 50h:58m:36s remains)
INFO - root - 2017-12-08 00:35:31.977294: step 72140, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 50h:10m:42s remains)
INFO - root - 2017-12-08 00:35:38.746261: step 72150, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 47h:31m:14s remains)
INFO - root - 2017-12-08 00:35:45.533195: step 72160, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 45h:32m:22s remains)
INFO - root - 2017-12-08 00:35:52.253308: step 72170, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 46h:29m:48s remains)
INFO - root - 2017-12-08 00:35:59.053739: step 72180, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 51h:15m:54s remains)
INFO - root - 2017-12-08 00:36:05.871464: step 72190, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 50h:16m:35s remains)
INFO - root - 2017-12-08 00:36:12.561958: step 72200, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 48h:39m:28s remains)
2017-12-08 00:36:13.307145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.350606 -4.3512015 -4.347353 -4.3403649 -4.3333421 -4.3270478 -4.3223119 -4.3194132 -4.3163877 -4.3168235 -4.3193884 -4.3231406 -4.3295894 -4.3369431 -4.344769][-4.351603 -4.3528504 -4.3478384 -4.3390207 -4.3298597 -4.3201165 -4.3100858 -4.3017192 -4.2943897 -4.2928696 -4.2948208 -4.3008671 -4.3125272 -4.3243194 -4.3362579][-4.3506083 -4.35206 -4.3454871 -4.3356261 -4.324966 -4.3110394 -4.2948055 -4.2784562 -4.2652125 -4.260601 -4.26087 -4.2693152 -4.2878594 -4.3056574 -4.3236775][-4.3494391 -4.3507748 -4.3433285 -4.3325706 -4.3202143 -4.303709 -4.2821727 -4.2570705 -4.2388062 -4.2316365 -4.2302728 -4.2399416 -4.26439 -4.2882314 -4.3125067][-4.3486691 -4.3505239 -4.34207 -4.3285627 -4.3140326 -4.2958665 -4.2697186 -4.2375288 -4.2153945 -4.2065978 -4.2057104 -4.2167087 -4.2457728 -4.2749319 -4.3043127][-4.3458042 -4.3468652 -4.3360467 -4.3184991 -4.300324 -4.2798533 -4.249372 -4.2131858 -4.190536 -4.18162 -4.18219 -4.1940885 -4.2263322 -4.2611861 -4.2950072][-4.3404207 -4.3385072 -4.3247261 -4.3054581 -4.2847071 -4.2653241 -4.2339807 -4.1947131 -4.1707296 -4.1615705 -4.1649809 -4.17754 -4.2108841 -4.2499833 -4.2882109][-4.3325987 -4.3273458 -4.3120465 -4.2928195 -4.2723145 -4.2557411 -4.2263618 -4.1862631 -4.1608553 -4.1496134 -4.1547847 -4.1686239 -4.2014852 -4.2427077 -4.2829957][-4.3246837 -4.3169279 -4.3005276 -4.2830744 -4.263526 -4.2489281 -4.222971 -4.1857181 -4.1609817 -4.1485081 -4.1555057 -4.1715622 -4.2020946 -4.2418103 -4.2813406][-4.32034 -4.3120837 -4.2963977 -4.2828083 -4.2652974 -4.2514696 -4.2265773 -4.1914716 -4.1698675 -4.1596022 -4.1688123 -4.1854262 -4.2116551 -4.247283 -4.283731][-4.318706 -4.3115058 -4.2986236 -4.2896862 -4.2753325 -4.2623992 -4.240037 -4.2085905 -4.1909051 -4.1846995 -4.1948009 -4.2104735 -4.2313886 -4.2613816 -4.2928381][-4.3228049 -4.3178787 -4.3082418 -4.3040032 -4.2938375 -4.2822676 -4.2635784 -4.2374082 -4.222003 -4.217371 -4.2260375 -4.2389097 -4.2545094 -4.2789741 -4.3044372][-4.3280182 -4.3249679 -4.3178453 -4.3153048 -4.3084507 -4.2990685 -4.2851143 -4.2663012 -4.2547946 -4.2512751 -4.2574 -4.2668562 -4.2785435 -4.2974372 -4.3170724][-4.33124 -4.3294859 -4.3247547 -4.3225961 -4.3179193 -4.3112774 -4.3016167 -4.29062 -4.2843518 -4.2824621 -4.28646 -4.2933917 -4.3021116 -4.3162374 -4.3305063][-4.3353748 -4.3335752 -4.3297815 -4.3270097 -4.3232422 -4.3185391 -4.3125138 -4.3074059 -4.3055935 -4.3053746 -4.3084474 -4.3141561 -4.3211741 -4.3315873 -4.3414373]]...]
INFO - root - 2017-12-08 00:36:20.106877: step 72210, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.748 sec/batch; 54h:06m:10s remains)
INFO - root - 2017-12-08 00:36:26.850245: step 72220, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 47h:31m:42s remains)
INFO - root - 2017-12-08 00:36:33.566375: step 72230, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 45h:13m:18s remains)
INFO - root - 2017-12-08 00:36:40.360872: step 72240, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 47h:17m:05s remains)
INFO - root - 2017-12-08 00:36:47.006639: step 72250, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 52h:27m:11s remains)
INFO - root - 2017-12-08 00:36:53.856042: step 72260, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.744 sec/batch; 53h:46m:21s remains)
INFO - root - 2017-12-08 00:37:00.754845: step 72270, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 49h:21m:47s remains)
INFO - root - 2017-12-08 00:37:07.615900: step 72280, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 47h:30m:40s remains)
INFO - root - 2017-12-08 00:37:14.411252: step 72290, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 47h:49m:00s remains)
INFO - root - 2017-12-08 00:37:21.208214: step 72300, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.750 sec/batch; 54h:11m:24s remains)
2017-12-08 00:37:22.035391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2419057 -4.2305322 -4.2316446 -4.2460718 -4.2602868 -4.2693071 -4.2746882 -4.2776842 -4.275372 -4.2649069 -4.2407594 -4.2166853 -4.2060738 -4.2049112 -4.210176][-4.1970453 -4.1809797 -4.1861582 -4.2094207 -4.227828 -4.2372594 -4.2416105 -4.2438712 -4.2427354 -4.2349305 -4.2133622 -4.1946492 -4.190063 -4.1922693 -4.2006063][-4.1498828 -4.1397438 -4.1499052 -4.1717734 -4.1840792 -4.1893573 -4.1880622 -4.188993 -4.1955218 -4.2002778 -4.1921759 -4.1849127 -4.1829042 -4.179616 -4.1803803][-4.1145549 -4.1137896 -4.12775 -4.1429043 -4.1443086 -4.1398797 -4.1275973 -4.1205578 -4.1340513 -4.1639047 -4.1848426 -4.1894913 -4.1841373 -4.1698127 -4.1592011][-4.0933151 -4.0950246 -4.1081929 -4.1153374 -4.1052194 -4.0826635 -4.0575542 -4.0447273 -4.0677 -4.1225491 -4.1663551 -4.1822085 -4.175971 -4.1581945 -4.1472836][-4.0987968 -4.1042547 -4.115037 -4.1119 -4.0868587 -4.0440807 -4.0021052 -3.9862361 -4.0203557 -4.0876784 -4.1383615 -4.1597071 -4.1551518 -4.1412835 -4.1299443][-4.1378627 -4.1482549 -4.1542711 -4.1371212 -4.102356 -4.052938 -4.0037446 -3.9887819 -4.0243921 -4.0753274 -4.1084738 -4.1280274 -4.1311045 -4.1198649 -4.1075521][-4.1934848 -4.204308 -4.2072124 -4.186758 -4.1563931 -4.1192608 -4.0762415 -4.0588231 -4.0765467 -4.09711 -4.1065283 -4.1182747 -4.1248231 -4.1112919 -4.09842][-4.2361908 -4.2442179 -4.2466636 -4.2318811 -4.2103257 -4.1835761 -4.1489286 -4.1233573 -4.1217451 -4.1241956 -4.11632 -4.1231961 -4.1351237 -4.1309004 -4.1216903][-4.2614417 -4.2672863 -4.2684622 -4.2617474 -4.24494 -4.2221775 -4.1941204 -4.1617451 -4.1467924 -4.1443224 -4.13549 -4.139492 -4.1540222 -4.1628437 -4.1611032][-4.2831869 -4.2864928 -4.2865548 -4.283936 -4.2707157 -4.2479172 -4.2183666 -4.1870666 -4.1727638 -4.1745095 -4.1679482 -4.1666107 -4.1773429 -4.1914091 -4.1950769][-4.2940984 -4.2994986 -4.3009644 -4.2997108 -4.2889266 -4.2675104 -4.2432489 -4.2204504 -4.2110324 -4.2160997 -4.21024 -4.1984367 -4.2011571 -4.2165804 -4.2249708][-4.2908835 -4.2953596 -4.297915 -4.296741 -4.2888737 -4.2769122 -4.2676811 -4.2556839 -4.2505808 -4.2554832 -4.248148 -4.2360477 -4.2364926 -4.2525592 -4.2620692][-4.2756181 -4.2805376 -4.2825484 -4.2809491 -4.2740521 -4.2700653 -4.2735233 -4.2721424 -4.2710032 -4.2760086 -4.2703824 -4.2617559 -4.2646084 -4.2816 -4.2910161][-4.2604537 -4.2634134 -4.2594557 -4.255362 -4.2524104 -4.2569828 -4.2684183 -4.270267 -4.2714648 -4.2782235 -4.2770624 -4.2737079 -4.2774453 -4.2909737 -4.3005967]]...]
INFO - root - 2017-12-08 00:37:28.793698: step 72310, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.648 sec/batch; 46h:48m:13s remains)
INFO - root - 2017-12-08 00:37:35.604195: step 72320, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 50h:21m:43s remains)
INFO - root - 2017-12-08 00:37:42.528563: step 72330, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 53h:36m:20s remains)
INFO - root - 2017-12-08 00:37:49.285817: step 72340, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 49h:31m:05s remains)
INFO - root - 2017-12-08 00:37:56.075642: step 72350, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 45h:41m:48s remains)
INFO - root - 2017-12-08 00:38:02.955961: step 72360, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 49h:16m:50s remains)
INFO - root - 2017-12-08 00:38:09.814956: step 72370, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.770 sec/batch; 55h:36m:33s remains)
INFO - root - 2017-12-08 00:38:16.706644: step 72380, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 50h:43m:24s remains)
INFO - root - 2017-12-08 00:38:23.468180: step 72390, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 47h:22m:55s remains)
INFO - root - 2017-12-08 00:38:30.133169: step 72400, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 45h:22m:04s remains)
2017-12-08 00:38:30.945182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.285954 -4.2908149 -4.2841439 -4.2580366 -4.2177267 -4.1764421 -4.1449685 -4.12088 -4.1123924 -4.1142244 -4.1237779 -4.1412024 -4.1586461 -4.16746 -4.1611748][-4.2771811 -4.2818356 -4.2760744 -4.2524147 -4.2170763 -4.1845913 -4.1618457 -4.1415968 -4.1354909 -4.1438432 -4.1580229 -4.1727166 -4.1845155 -4.192574 -4.1842871][-4.2674727 -4.2693624 -4.2611461 -4.2375073 -4.20361 -4.1761284 -4.1592731 -4.1477289 -4.1544576 -4.1734362 -4.1883926 -4.1964936 -4.2025781 -4.2092538 -4.1995373][-4.25952 -4.2605009 -4.248837 -4.2222857 -4.1846337 -4.1555161 -4.1400247 -4.1365285 -4.1577272 -4.1880326 -4.2025886 -4.202208 -4.2010894 -4.2048383 -4.1928892][-4.2545705 -4.25578 -4.2396135 -4.2059865 -4.1591835 -4.11992 -4.09966 -4.1043997 -4.1436453 -4.186223 -4.2027287 -4.1967 -4.1892772 -4.1875987 -4.1727328][-4.2442193 -4.2479234 -4.2287035 -4.1867175 -4.1265464 -4.0686417 -4.0353165 -4.0484457 -4.111969 -4.1719241 -4.1971154 -4.19445 -4.1884532 -4.1847811 -4.1663089][-4.224545 -4.2331781 -4.2143545 -4.1683636 -4.0990677 -4.0217443 -3.9687562 -3.9838896 -4.0699072 -4.1507473 -4.1893582 -4.1953163 -4.1942568 -4.1896834 -4.1718307][-4.1921844 -4.2062559 -4.1951232 -4.1605444 -4.1008258 -4.0227094 -3.9602818 -3.9664683 -4.0522571 -4.1401672 -4.1833334 -4.1914353 -4.190917 -4.1885109 -4.1797328][-4.1671233 -4.183681 -4.1817169 -4.1661768 -4.12888 -4.0697255 -4.0192184 -4.018013 -4.0814514 -4.1543913 -4.1886292 -4.1882315 -4.181839 -4.1821327 -4.1831441][-4.1647964 -4.1738567 -4.1716533 -4.1682835 -4.1512752 -4.1180072 -4.0919051 -4.0969234 -4.1379271 -4.1837807 -4.2016306 -4.1924334 -4.1817622 -4.1821723 -4.189754][-4.1808138 -4.1803164 -4.1695876 -4.1632533 -4.1540694 -4.1384373 -4.1337619 -4.1486435 -4.1757984 -4.200079 -4.2032738 -4.191031 -4.181314 -4.1839857 -4.1977859][-4.20478 -4.1988373 -4.1815681 -4.1661854 -4.1518874 -4.1390944 -4.1418414 -4.1601963 -4.181129 -4.1949005 -4.192359 -4.1803551 -4.171555 -4.1765642 -4.1977987][-4.220674 -4.2164917 -4.2013674 -4.1838665 -4.1632924 -4.1479397 -4.1510577 -4.1672726 -4.18421 -4.1941185 -4.1900535 -4.1791811 -4.1673527 -4.1675425 -4.1897912][-4.2184668 -4.2155242 -4.20338 -4.19001 -4.1724753 -4.1601973 -4.1646419 -4.1787281 -4.1924133 -4.20207 -4.1996374 -4.190105 -4.1750617 -4.1672368 -4.1837049][-4.2029896 -4.1987343 -4.1841283 -4.1726947 -4.1631713 -4.1595082 -4.1694913 -4.1872869 -4.2032833 -4.2135167 -4.2114348 -4.203373 -4.1891685 -4.1769485 -4.1859531]]...]
INFO - root - 2017-12-08 00:38:37.756071: step 72410, loss = 2.11, batch loss = 2.05 (12.7 examples/sec; 0.631 sec/batch; 45h:36m:43s remains)
INFO - root - 2017-12-08 00:38:44.654273: step 72420, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 47h:44m:25s remains)
INFO - root - 2017-12-08 00:38:51.375576: step 72430, loss = 2.08, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 45h:14m:27s remains)
INFO - root - 2017-12-08 00:38:58.278393: step 72440, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 52h:55m:05s remains)
INFO - root - 2017-12-08 00:39:05.144052: step 72450, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 52h:48m:33s remains)
INFO - root - 2017-12-08 00:39:12.004822: step 72460, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 48h:37m:21s remains)
INFO - root - 2017-12-08 00:39:18.665153: step 72470, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 46h:57m:08s remains)
INFO - root - 2017-12-08 00:39:25.405086: step 72480, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 46h:29m:06s remains)
INFO - root - 2017-12-08 00:39:32.249112: step 72490, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 51h:28m:25s remains)
INFO - root - 2017-12-08 00:39:38.860390: step 72500, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 51h:47m:15s remains)
2017-12-08 00:39:39.637093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3229485 -4.3230858 -4.3233075 -4.3229351 -4.3219872 -4.317543 -4.3151388 -4.3159976 -4.3191285 -4.3203015 -4.3171272 -4.3111157 -4.3053966 -4.2990046 -4.2961588][-4.3203073 -4.3148818 -4.3083897 -4.3018246 -4.2917595 -4.2826138 -4.2789598 -4.2784529 -4.2828913 -4.2899675 -4.2923012 -4.2920408 -4.2958808 -4.2969747 -4.2984204][-4.3115349 -4.3024125 -4.2897582 -4.2735333 -4.2526383 -4.2367768 -4.2303295 -4.2273035 -4.2355242 -4.2544541 -4.2691565 -4.2772431 -4.288063 -4.294239 -4.2973661][-4.2886453 -4.278389 -4.2613592 -4.2357011 -4.2044411 -4.1791592 -4.1638675 -4.1562495 -4.1713219 -4.2103672 -4.24536 -4.2635016 -4.2796321 -4.2900934 -4.2944293][-4.2557688 -4.243176 -4.2210808 -4.18583 -4.1447253 -4.1082234 -4.0807705 -4.065433 -4.0891128 -4.1519151 -4.2087054 -4.2382956 -4.2606921 -4.2773323 -4.2858968][-4.2242789 -4.2058778 -4.1780806 -4.1347895 -4.0849743 -4.0400219 -4.0025611 -3.9795649 -4.014873 -4.0984974 -4.1702871 -4.20865 -4.236927 -4.2622795 -4.2774982][-4.2152925 -4.1931996 -4.1616507 -4.11629 -4.0662155 -4.0243435 -3.9910536 -3.9730804 -4.0169353 -4.098249 -4.165153 -4.1996765 -4.2277594 -4.2573304 -4.2752271][-4.2334013 -4.2160974 -4.1908903 -4.1578436 -4.1202316 -4.0892124 -4.0642753 -4.0554481 -4.0911202 -4.145534 -4.1899838 -4.2138619 -4.2381325 -4.2665119 -4.2821674][-4.259985 -4.249404 -4.2352505 -4.2170258 -4.1934156 -4.1725955 -4.1559029 -4.1521659 -4.1719403 -4.1977468 -4.2204013 -4.2362847 -4.2595892 -4.2848492 -4.294466][-4.26855 -4.2654138 -4.2599 -4.2494826 -4.2348504 -4.2221947 -4.2130728 -4.2121878 -4.2208695 -4.230618 -4.2415214 -4.2561088 -4.2801709 -4.3020377 -4.3065143][-4.2484174 -4.2574549 -4.2637186 -4.2632103 -4.25825 -4.2527637 -4.2463508 -4.2425756 -4.2394972 -4.2385306 -4.2454386 -4.26316 -4.2884316 -4.3086963 -4.3127317][-4.2226396 -4.2465415 -4.2628226 -4.2701955 -4.272006 -4.2683511 -4.2602415 -4.2488909 -4.2366419 -4.232615 -4.2427545 -4.2637343 -4.288774 -4.3070774 -4.3124704][-4.2213879 -4.2531962 -4.2710066 -4.2794766 -4.2823691 -4.2761521 -4.2627215 -4.244678 -4.2283421 -4.227664 -4.2450585 -4.2672005 -4.2885189 -4.3027549 -4.31004][-4.243351 -4.2713957 -4.2845483 -4.28908 -4.2889838 -4.2771373 -4.2580004 -4.2387652 -4.22827 -4.236618 -4.2585278 -4.2782736 -4.2926769 -4.3044248 -4.3121943][-4.2714915 -4.28712 -4.2928839 -4.2941074 -4.2922335 -4.2776275 -4.25636 -4.2397914 -4.2357869 -4.2505603 -4.27279 -4.2861176 -4.2947893 -4.3052611 -4.3131447]]...]
INFO - root - 2017-12-08 00:39:46.365179: step 72510, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 48h:20m:15s remains)
INFO - root - 2017-12-08 00:39:53.157054: step 72520, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 52h:33m:46s remains)
INFO - root - 2017-12-08 00:39:59.953166: step 72530, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 47h:57m:14s remains)
INFO - root - 2017-12-08 00:40:06.582809: step 72540, loss = 2.05, batch loss = 1.99 (13.1 examples/sec; 0.613 sec/batch; 44h:14m:40s remains)
INFO - root - 2017-12-08 00:40:13.397604: step 72550, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 46h:35m:45s remains)
INFO - root - 2017-12-08 00:40:20.307887: step 72560, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.755 sec/batch; 54h:32m:30s remains)
INFO - root - 2017-12-08 00:40:27.218921: step 72570, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 50h:50m:46s remains)
INFO - root - 2017-12-08 00:40:33.975947: step 72580, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 47h:49m:00s remains)
INFO - root - 2017-12-08 00:40:40.704156: step 72590, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.625 sec/batch; 45h:06m:40s remains)
INFO - root - 2017-12-08 00:40:47.307278: step 72600, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.649 sec/batch; 46h:52m:33s remains)
2017-12-08 00:40:48.125036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0569358 -4.0819478 -4.1222506 -4.1570611 -4.1733656 -4.1742973 -4.1549835 -4.1316037 -4.1215191 -4.1512685 -4.1772947 -4.1953325 -4.2108049 -4.2373667 -4.265429][-4.0270276 -4.04762 -4.0883327 -4.124939 -4.1438847 -4.1498985 -4.1352448 -4.1146317 -4.1030183 -4.1309814 -4.1634974 -4.1866479 -4.2066574 -4.2331495 -4.2611313][-4.0168343 -4.0279078 -4.0620327 -4.0912971 -4.109292 -4.12252 -4.1217618 -4.109724 -4.0986843 -4.1260071 -4.1599221 -4.18448 -4.2035379 -4.2284231 -4.2558208][-4.0271297 -4.0304561 -4.0564814 -4.0765944 -4.0897532 -4.1013174 -4.1081653 -4.1101718 -4.1102657 -4.1417108 -4.1758242 -4.2004108 -4.21573 -4.2346859 -4.2564788][-4.0504613 -4.0504112 -4.0660787 -4.072751 -4.070406 -4.0690207 -4.0752826 -4.0881567 -4.1067004 -4.15401 -4.1906338 -4.2157059 -4.2289028 -4.2404442 -4.2590933][-4.074543 -4.074944 -4.0802865 -4.0744343 -4.0527973 -4.0197248 -4.0046215 -4.0169044 -4.0618486 -4.1350789 -4.1797776 -4.2062616 -4.22299 -4.2365885 -4.2595005][-4.107542 -4.10744 -4.1031508 -4.0833111 -4.0399928 -3.963727 -3.8976159 -3.8991342 -3.9855492 -4.0941715 -4.151772 -4.1815834 -4.2042146 -4.2280612 -4.258491][-4.1370707 -4.1403351 -4.1398482 -4.1143651 -4.0581789 -3.948561 -3.8282311 -3.8038456 -3.9184749 -4.0545893 -4.1222677 -4.15476 -4.1822877 -4.2177439 -4.2536][-4.1470952 -4.1577215 -4.1704884 -4.1551485 -4.1090693 -4.013051 -3.8954031 -3.8414414 -3.9280033 -4.0488777 -4.1127224 -4.1421437 -4.1714754 -4.2141018 -4.2504272][-4.1331987 -4.1452651 -4.1710782 -4.1829677 -4.1657944 -4.1090975 -4.0329738 -3.9738517 -4.0114117 -4.0939245 -4.1386552 -4.1572413 -4.1793585 -4.2200079 -4.2519035][-4.103302 -4.1102238 -4.1447096 -4.181819 -4.1930118 -4.163393 -4.10937 -4.0527897 -4.0702066 -4.1382165 -4.1718626 -4.1814747 -4.1982608 -4.2335925 -4.2596664][-4.0752668 -4.0777812 -4.1189733 -4.1705432 -4.189652 -4.1657743 -4.1145067 -4.0589423 -4.07686 -4.1536088 -4.1902981 -4.1947994 -4.2117348 -4.2420611 -4.2620711][-4.077642 -4.0838089 -4.1249309 -4.169167 -4.1785932 -4.1424017 -4.0899839 -4.0375657 -4.0694027 -4.1561241 -4.1967778 -4.1992712 -4.2149572 -4.2411909 -4.2567916][-4.105176 -4.1142759 -4.1406465 -4.1618872 -4.1580968 -4.1122751 -4.061718 -4.0157166 -4.0545745 -4.1431761 -4.1853733 -4.1903229 -4.2083697 -4.2312818 -4.2426982][-4.1326733 -4.1457787 -4.1536083 -4.1558962 -4.1453428 -4.0979691 -4.0458465 -3.9995248 -4.0341992 -4.1205812 -4.1646237 -4.1769781 -4.2006168 -4.21974 -4.2295051]]...]
INFO - root - 2017-12-08 00:40:54.998934: step 72610, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 50h:38m:19s remains)
INFO - root - 2017-12-08 00:41:01.782396: step 72620, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 45h:37m:16s remains)
INFO - root - 2017-12-08 00:41:08.662944: step 72630, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 46h:32m:22s remains)
INFO - root - 2017-12-08 00:41:15.445448: step 72640, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 53h:27m:06s remains)
INFO - root - 2017-12-08 00:41:22.179790: step 72650, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 52h:15m:08s remains)
INFO - root - 2017-12-08 00:41:28.997767: step 72660, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.703 sec/batch; 50h:43m:47s remains)
INFO - root - 2017-12-08 00:41:35.777067: step 72670, loss = 2.11, batch loss = 2.05 (12.9 examples/sec; 0.621 sec/batch; 44h:50m:48s remains)
INFO - root - 2017-12-08 00:41:42.528126: step 72680, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 45h:26m:54s remains)
INFO - root - 2017-12-08 00:41:49.302912: step 72690, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 49h:53m:37s remains)
INFO - root - 2017-12-08 00:41:55.883156: step 72700, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 50h:44m:18s remains)
2017-12-08 00:41:56.659334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.169281 -4.162724 -4.1587853 -4.1559229 -4.1627073 -4.1755419 -4.1750121 -4.1690278 -4.1579051 -4.1489944 -4.1558323 -4.1727328 -4.1863308 -4.1859264 -4.1780205][-4.1769562 -4.1730332 -4.169024 -4.1631379 -4.1650143 -4.1735582 -4.1724572 -4.1651587 -4.1518621 -4.1397705 -4.1415296 -4.15347 -4.162405 -4.1600132 -4.1505289][-4.1876025 -4.1836572 -4.1803279 -4.1771255 -4.179441 -4.1819105 -4.1763554 -4.1661344 -4.151391 -4.1398053 -4.1410251 -4.1495147 -4.1552072 -4.1531415 -4.1448808][-4.1776443 -4.1723518 -4.16986 -4.1696959 -4.1750016 -4.174861 -4.1641688 -4.1481695 -4.1338086 -4.1263018 -4.131722 -4.1440706 -4.1526909 -4.1540294 -4.1467681][-4.1484571 -4.1446095 -4.1446209 -4.14581 -4.149147 -4.1424432 -4.1217809 -4.1008177 -4.0893731 -4.0904126 -4.106451 -4.1292925 -4.1456547 -4.1530037 -4.1502452][-4.1175423 -4.1147804 -4.1173248 -4.1179914 -4.1167583 -4.1012235 -4.0724707 -4.050981 -4.0437083 -4.05224 -4.0783052 -4.109386 -4.1309223 -4.142128 -4.1430669][-4.0901179 -4.08691 -4.0883636 -4.0874481 -4.082602 -4.0598707 -4.0265145 -4.007607 -4.0029697 -4.0153441 -4.0470047 -4.0809312 -4.1041927 -4.116869 -4.1200371][-4.0737157 -4.0664306 -4.0650954 -4.0655651 -4.0614805 -4.0387764 -4.00929 -3.993824 -3.9874649 -3.9979627 -4.0296783 -4.0624537 -4.0849528 -4.0970378 -4.101275][-4.0608673 -4.0536938 -4.051733 -4.0589938 -4.0651183 -4.052474 -4.0335402 -4.02152 -4.0119243 -4.0171142 -4.042654 -4.0676703 -4.0842628 -4.0925312 -4.0966568][-4.0429559 -4.0378914 -4.0398645 -4.0563283 -4.0767074 -4.0808511 -4.07634 -4.0711145 -4.0632811 -4.0663662 -4.0851908 -4.1013613 -4.1093864 -4.1110659 -4.1125946][-4.0296478 -4.0255585 -4.0295939 -4.0506268 -4.07994 -4.096777 -4.1038327 -4.1067472 -4.1045194 -4.1085362 -4.1215272 -4.1296883 -4.1292357 -4.1248021 -4.122602][-4.0380969 -4.0353713 -4.0413704 -4.0608816 -4.0893741 -4.1075716 -4.1139412 -4.1162338 -4.1155214 -4.1208463 -4.1306734 -4.1366553 -4.135417 -4.1310754 -4.1288619][-4.0714207 -4.07133 -4.078651 -4.0916309 -4.108851 -4.1185141 -4.1170721 -4.1135178 -4.1109982 -4.1154618 -4.1232872 -4.1293006 -4.13235 -4.133625 -4.1364632][-4.1054611 -4.1079636 -4.1141009 -4.11722 -4.1191535 -4.1169724 -4.1095634 -4.1045012 -4.1047764 -4.1113863 -4.1185379 -4.1236863 -4.12885 -4.1343718 -4.1411314][-4.1228652 -4.1266356 -4.1313715 -4.1287589 -4.1227789 -4.1149874 -4.1071181 -4.1055994 -4.1103163 -4.1191821 -4.1250687 -4.1279793 -4.1325631 -4.1393256 -4.1461334]]...]
INFO - root - 2017-12-08 00:42:03.382505: step 72710, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 44h:36m:11s remains)
INFO - root - 2017-12-08 00:42:10.173328: step 72720, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 48h:00m:36s remains)
INFO - root - 2017-12-08 00:42:17.062490: step 72730, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 50h:31m:33s remains)
INFO - root - 2017-12-08 00:42:23.872422: step 72740, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 46h:36m:49s remains)
INFO - root - 2017-12-08 00:42:30.532989: step 72750, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 46h:15m:59s remains)
INFO - root - 2017-12-08 00:42:37.366129: step 72760, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 46h:44m:42s remains)
INFO - root - 2017-12-08 00:42:44.228878: step 72770, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 48h:05m:23s remains)
INFO - root - 2017-12-08 00:42:51.120587: step 72780, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 52h:11m:14s remains)
INFO - root - 2017-12-08 00:42:57.983842: step 72790, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 51h:24m:25s remains)
INFO - root - 2017-12-08 00:43:04.509953: step 72800, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 46h:34m:43s remains)
2017-12-08 00:43:05.204291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2573457 -4.2562008 -4.2705441 -4.2839184 -4.2828708 -4.2735891 -4.2619019 -4.25559 -4.2551446 -4.2581115 -4.2496891 -4.2215023 -4.2110229 -4.2379556 -4.2788315][-4.2538862 -4.2571163 -4.270092 -4.2765541 -4.2694063 -4.2579064 -4.2525325 -4.2563882 -4.2630253 -4.2625947 -4.247777 -4.2183576 -4.2056503 -4.2308826 -4.2760978][-4.2503471 -4.2556133 -4.2631321 -4.2593355 -4.2471423 -4.2395115 -4.2406592 -4.2504406 -4.2655487 -4.2699628 -4.2527385 -4.2220588 -4.2055287 -4.22503 -4.27209][-4.2601042 -4.2615714 -4.2596035 -4.2422705 -4.2256422 -4.2226744 -4.2290053 -4.244206 -4.2661681 -4.2782969 -4.263978 -4.2332768 -4.2116771 -4.2212954 -4.2647786][-4.2743673 -4.2707348 -4.2546539 -4.2180948 -4.1900349 -4.188365 -4.2004676 -4.2246175 -4.2562065 -4.2778053 -4.2696428 -4.2422762 -4.2162375 -4.21311 -4.247395][-4.2758374 -4.2681274 -4.2375913 -4.1737857 -4.1189575 -4.1115 -4.1368289 -4.1815586 -4.2325068 -4.2661481 -4.264255 -4.2394648 -4.2112522 -4.1992 -4.2248235][-4.2668495 -4.255867 -4.2130671 -4.122654 -4.0302887 -4.0041585 -4.0414028 -4.1117973 -4.1889057 -4.2380409 -4.2433333 -4.2224236 -4.1983786 -4.1870513 -4.2105155][-4.2648678 -4.2534876 -4.2082925 -4.1089454 -3.9918733 -3.9367175 -3.9704394 -4.0556645 -4.1522822 -4.2109423 -4.2225776 -4.2093678 -4.1946707 -4.1898122 -4.2138295][-4.2863293 -4.2785435 -4.2420497 -4.1578078 -4.0445795 -3.9656589 -3.9719167 -4.0455208 -4.1363072 -4.1899896 -4.2038097 -4.2030988 -4.2030826 -4.2065263 -4.2306914][-4.3148513 -4.3139458 -4.28894 -4.2266736 -4.1360545 -4.051342 -4.02299 -4.0625572 -4.1304674 -4.1717887 -4.183794 -4.1923327 -4.2062407 -4.220963 -4.2471261][-4.3239045 -4.3282881 -4.3113847 -4.2679811 -4.2029438 -4.12694 -4.0763426 -4.0810132 -4.1196828 -4.1444736 -4.1543908 -4.1705751 -4.1970553 -4.2246604 -4.2589889][-4.317935 -4.3230824 -4.3091941 -4.2783146 -4.2362289 -4.17937 -4.1267581 -4.1075182 -4.1176023 -4.1206851 -4.1208153 -4.1411629 -4.180666 -4.2227182 -4.2671275][-4.31432 -4.3180618 -4.3061876 -4.2833266 -4.256196 -4.217597 -4.1742992 -4.1490307 -4.1414104 -4.1241875 -4.1067019 -4.122232 -4.1695657 -4.2198324 -4.2710543][-4.3111548 -4.313107 -4.3056164 -4.290143 -4.2727451 -4.2483473 -4.2170396 -4.194726 -4.1828232 -4.1561546 -4.124126 -4.1286917 -4.1724586 -4.22285 -4.2730474][-4.3096652 -4.3083873 -4.3043723 -4.2955179 -4.2859612 -4.2723255 -4.2526889 -4.2352462 -4.2230968 -4.197649 -4.16285 -4.1566477 -4.1921921 -4.2403035 -4.2868156]]...]
INFO - root - 2017-12-08 00:43:12.030786: step 72810, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.757 sec/batch; 54h:34m:47s remains)
INFO - root - 2017-12-08 00:43:18.688548: step 72820, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 51h:13m:57s remains)
INFO - root - 2017-12-08 00:43:25.262471: step 72830, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 47h:29m:31s remains)
INFO - root - 2017-12-08 00:43:32.073421: step 72840, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 45h:30m:24s remains)
INFO - root - 2017-12-08 00:43:38.955644: step 72850, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 50h:44m:46s remains)
INFO - root - 2017-12-08 00:43:45.815260: step 72860, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 51h:46m:59s remains)
INFO - root - 2017-12-08 00:43:52.702022: step 72870, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 47h:02m:00s remains)
INFO - root - 2017-12-08 00:43:59.514336: step 72880, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 48h:40m:46s remains)
INFO - root - 2017-12-08 00:44:06.294815: step 72890, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 45h:21m:28s remains)
INFO - root - 2017-12-08 00:44:12.983566: step 72900, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 52h:48m:43s remains)
2017-12-08 00:44:13.760268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3242297 -4.3260098 -4.3243036 -4.3220906 -4.3207707 -4.3137918 -4.3039541 -4.2987766 -4.2981291 -4.2980809 -4.297555 -4.2968535 -4.2942305 -4.291338 -4.2900038][-4.32812 -4.3349938 -4.3355412 -4.3340707 -4.3305144 -4.32144 -4.3095708 -4.3032026 -4.3028812 -4.3032918 -4.3020787 -4.2993312 -4.2913718 -4.2813153 -4.2748137][-4.3240032 -4.3335814 -4.3322325 -4.3253293 -4.3167267 -4.3046174 -4.2909946 -4.2830038 -4.2853451 -4.2882986 -4.284636 -4.280221 -4.2683063 -4.2567616 -4.2522826][-4.3119226 -4.322413 -4.3165669 -4.3032069 -4.2887254 -4.2705355 -4.251749 -4.2390938 -4.2437444 -4.2492805 -4.2411008 -4.2335448 -4.2230177 -4.2175112 -4.2205224][-4.2931876 -4.3023782 -4.289917 -4.2712493 -4.2517004 -4.226079 -4.1960244 -4.1738234 -4.181108 -4.1945882 -4.1864538 -4.1796083 -4.1777673 -4.1826611 -4.1937437][-4.2682285 -4.2729125 -4.2497344 -4.2211356 -4.1917291 -4.1560435 -4.1096425 -4.072701 -4.0871835 -4.1234593 -4.131784 -4.1352592 -4.1460819 -4.1622109 -4.1843133][-4.2474561 -4.2411733 -4.2046561 -4.1623249 -4.1192164 -4.0689731 -3.9987936 -3.9350829 -3.9584496 -4.0351644 -4.07619 -4.098012 -4.1207266 -4.1479969 -4.1818004][-4.2262826 -4.201139 -4.1481953 -4.0935359 -4.0391026 -3.9753592 -3.8880394 -3.7994375 -3.829401 -3.9487555 -4.0255823 -4.0671186 -4.1001725 -4.1355524 -4.1766105][-4.2194891 -4.1904268 -4.1375284 -4.0867972 -4.0416245 -3.9856532 -3.9081566 -3.8311312 -3.8563979 -3.9650276 -4.0406637 -4.0810266 -4.1092148 -4.1397085 -4.176116][-4.222424 -4.2001686 -4.1595221 -4.1237183 -4.0963492 -4.0604253 -4.0086517 -3.9597268 -3.9734993 -4.04359 -4.0983047 -4.1257238 -4.14206 -4.1615677 -4.1876121][-4.2246842 -4.2102933 -4.1835947 -4.1614218 -4.1460609 -4.1253033 -4.0906887 -4.0551009 -4.0587397 -4.1019273 -4.1419234 -4.1632752 -4.1726885 -4.18607 -4.207478][-4.2244749 -4.2190146 -4.2055631 -4.194736 -4.1862211 -4.1755128 -4.1530175 -4.1251063 -4.1201396 -4.1453686 -4.1761484 -4.1974983 -4.2067289 -4.2192249 -4.2378359][-4.2234182 -4.2249713 -4.2205563 -4.2180009 -4.2156982 -4.2119617 -4.1968055 -4.1722736 -4.162962 -4.1762261 -4.19927 -4.22195 -4.2367859 -4.2523103 -4.2687807][-4.2345691 -4.2370329 -4.2354326 -4.2342286 -4.2332597 -4.2315149 -4.2225423 -4.2068758 -4.1999264 -4.2069364 -4.22342 -4.243474 -4.2611704 -4.2782059 -4.2916889][-4.2612343 -4.2593908 -4.2554388 -4.2533484 -4.251431 -4.2485 -4.2434936 -4.2368422 -4.2351465 -4.2393212 -4.2505946 -4.2668486 -4.2832651 -4.2974744 -4.3072252]]...]
INFO - root - 2017-12-08 00:44:20.482712: step 72910, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.620 sec/batch; 44h:40m:26s remains)
INFO - root - 2017-12-08 00:44:27.354656: step 72920, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 46h:38m:23s remains)
INFO - root - 2017-12-08 00:44:34.182346: step 72930, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 52h:53m:11s remains)
INFO - root - 2017-12-08 00:44:41.038973: step 72940, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.760 sec/batch; 54h:48m:13s remains)
INFO - root - 2017-12-08 00:44:47.865256: step 72950, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 49h:38m:24s remains)
INFO - root - 2017-12-08 00:44:54.649869: step 72960, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.616 sec/batch; 44h:26m:21s remains)
INFO - root - 2017-12-08 00:45:01.380013: step 72970, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 47h:18m:30s remains)
INFO - root - 2017-12-08 00:45:08.242997: step 72980, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:24m:13s remains)
INFO - root - 2017-12-08 00:45:15.089040: step 72990, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 52h:56m:06s remains)
INFO - root - 2017-12-08 00:45:21.680701: step 73000, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 50h:01m:56s remains)
2017-12-08 00:45:22.464722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.349587 -4.3372674 -4.3191614 -4.2987514 -4.2921066 -4.303124 -4.3201232 -4.3322849 -4.3341484 -4.3258305 -4.3148079 -4.3089523 -4.3074784 -4.3073583 -4.3084669][-4.352046 -4.3354697 -4.3087087 -4.2793794 -4.2661977 -4.2764335 -4.2985048 -4.3192363 -4.3301306 -4.3287053 -4.3206306 -4.3143239 -4.3105249 -4.3075075 -4.3063111][-4.3377767 -4.320847 -4.2876163 -4.2493429 -4.2276397 -4.2326946 -4.25629 -4.2838292 -4.3070741 -4.3211894 -4.3247018 -4.3224549 -4.317143 -4.3105211 -4.3063116][-4.3114963 -4.2982659 -4.2616038 -4.2124715 -4.1777844 -4.1729894 -4.1930342 -4.2258496 -4.2632732 -4.2968531 -4.3187647 -4.3281903 -4.3272634 -4.3202338 -4.3120012][-4.2898335 -4.2831516 -4.2442722 -4.1818275 -4.1265264 -4.1040664 -4.1203356 -4.1622305 -4.2117376 -4.259716 -4.2973981 -4.3222671 -4.3339152 -4.3330221 -4.3234386][-4.2694578 -4.2681918 -4.225028 -4.1468215 -4.0645342 -4.0187354 -4.0304461 -4.0886183 -4.1580458 -4.2186513 -4.2640724 -4.2994046 -4.325098 -4.33667 -4.332128][-4.2503128 -4.2535448 -4.2103014 -4.1199279 -4.0128374 -3.9335253 -3.9296017 -4.0031023 -4.098206 -4.1734042 -4.2257133 -4.2676349 -4.3055096 -4.3288455 -4.3321438][-4.23402 -4.2464151 -4.2152152 -4.1317129 -4.02122 -3.924535 -3.8933237 -3.9567561 -4.0566559 -4.13565 -4.1876254 -4.2323771 -4.2777381 -4.309845 -4.3220816][-4.2296443 -4.2515721 -4.2376337 -4.1779852 -4.0985961 -4.0303631 -3.9966505 -4.0191884 -4.0780768 -4.1325278 -4.1698179 -4.2092795 -4.25607 -4.2939854 -4.3137488][-4.2377667 -4.2656145 -4.2622457 -4.22355 -4.1762819 -4.1445537 -4.1287704 -4.1347923 -4.1557178 -4.1785221 -4.19462 -4.2223773 -4.2635264 -4.2996168 -4.319499][-4.26331 -4.289938 -4.2893109 -4.2605987 -4.2267675 -4.2096171 -4.2019196 -4.205904 -4.2163153 -4.2277074 -4.238884 -4.2588019 -4.2907043 -4.3190966 -4.334085][-4.3032885 -4.3209729 -4.3123894 -4.27706 -4.2375259 -4.21538 -4.203733 -4.2044854 -4.2195368 -4.2422185 -4.2658639 -4.2885351 -4.3166556 -4.3389263 -4.3480439][-4.3347244 -4.3407097 -4.31953 -4.2683759 -4.2109675 -4.1713948 -4.1505833 -4.1517339 -4.1790171 -4.2204423 -4.2609835 -4.2899079 -4.3188262 -4.3432703 -4.3526807][-4.3298097 -4.3271642 -4.2984595 -4.2340741 -4.1544361 -4.0877943 -4.0492735 -4.0516992 -4.1028833 -4.1732326 -4.2332826 -4.2688837 -4.2988982 -4.3287945 -4.3452859][-4.2940707 -4.2910037 -4.2645993 -4.1983123 -4.1061263 -4.0153437 -3.947053 -3.9370418 -4.0046458 -4.1038952 -4.1829543 -4.2305264 -4.2682824 -4.3066573 -4.3315196]]...]
INFO - root - 2017-12-08 00:45:29.245928: step 73010, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 52h:57m:03s remains)
INFO - root - 2017-12-08 00:45:36.006142: step 73020, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 51h:40m:08s remains)
INFO - root - 2017-12-08 00:45:42.764259: step 73030, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 47h:31m:25s remains)
INFO - root - 2017-12-08 00:45:49.574717: step 73040, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.637 sec/batch; 45h:55m:41s remains)
INFO - root - 2017-12-08 00:45:56.394226: step 73050, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 45h:54m:52s remains)
INFO - root - 2017-12-08 00:46:03.227553: step 73060, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 50h:29m:43s remains)
INFO - root - 2017-12-08 00:46:10.065348: step 73070, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 51h:23m:48s remains)
INFO - root - 2017-12-08 00:46:16.867543: step 73080, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 50h:50m:17s remains)
INFO - root - 2017-12-08 00:46:23.747710: step 73090, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 46h:19m:37s remains)
INFO - root - 2017-12-08 00:46:30.394474: step 73100, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 47h:31m:48s remains)
2017-12-08 00:46:31.211972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2694612 -4.2490034 -4.2251053 -4.2106643 -4.2058082 -4.2148466 -4.2274127 -4.2293711 -4.2111106 -4.2011003 -4.2070551 -4.2213054 -4.22829 -4.217381 -4.2030725][-4.26634 -4.2434459 -4.2154813 -4.191092 -4.1760273 -4.1798677 -4.1892047 -4.1892304 -4.173707 -4.1719179 -4.18535 -4.2025084 -4.2097406 -4.1987324 -4.1816835][-4.237339 -4.2106047 -4.1824336 -4.1536808 -4.1312513 -4.1292872 -4.1275969 -4.1162868 -4.1048026 -4.1080856 -4.1254935 -4.1496954 -4.1628547 -4.1556768 -4.140358][-4.1983366 -4.1673589 -4.1374588 -4.1062169 -4.0799665 -4.072834 -4.061893 -4.040308 -4.0303197 -4.0339789 -4.0495448 -4.0761805 -4.0958786 -4.0993295 -4.0924993][-4.141252 -4.1039658 -4.070847 -4.0352917 -4.0094986 -3.9966533 -3.9788461 -3.9528062 -3.946692 -3.9550123 -3.9727457 -4.0030189 -4.0276709 -4.0341349 -4.0321031][-4.0792379 -4.0329132 -3.9907823 -3.9456835 -3.9122956 -3.8856149 -3.8529127 -3.8252192 -3.8338516 -3.8591104 -3.885386 -3.9188595 -3.9393418 -3.9423673 -3.9392715][-4.013608 -3.9547319 -3.901365 -3.8438938 -3.7957115 -3.7510421 -3.6999507 -3.6730545 -3.6987078 -3.7371712 -3.7692149 -3.8037028 -3.8241537 -3.8320553 -3.8349257][-4.0027385 -3.9407787 -3.88737 -3.8292508 -3.7741165 -3.7261186 -3.6766791 -3.6527863 -3.6737485 -3.6950498 -3.7104373 -3.7382941 -3.7676933 -3.7933419 -3.8035994][-4.0675864 -4.0205965 -3.9815679 -3.9379904 -3.8933609 -3.8597617 -3.8345888 -3.8258834 -3.8378754 -3.8377337 -3.8305805 -3.8352916 -3.8530602 -3.8770719 -3.8809648][-4.1573811 -4.1304245 -4.1069264 -4.07989 -4.0502019 -4.0299954 -4.0238838 -4.0256858 -4.03356 -4.0269074 -4.0139513 -4.0062819 -4.0105071 -4.0255613 -4.0285077][-4.2354937 -4.2230773 -4.2126732 -4.1972857 -4.1786809 -4.1673317 -4.1698179 -4.1742897 -4.1794252 -4.1734 -4.1624179 -4.1545949 -4.1549525 -4.1645179 -4.1699533][-4.2905817 -4.2876215 -4.2849073 -4.2770028 -4.2664771 -4.2615714 -4.2662711 -4.2702646 -4.272408 -4.2671971 -4.2597256 -4.254673 -4.25394 -4.25763 -4.2620997][-4.3325624 -4.3337932 -4.3332486 -4.3293452 -4.3238173 -4.3204007 -4.3219166 -4.3232126 -4.32342 -4.3203559 -4.3157659 -4.3118548 -4.3100424 -4.3105216 -4.3133283][-4.3585281 -4.3601227 -4.3598566 -4.3581691 -4.3559213 -4.3542757 -4.3544683 -4.354609 -4.3542457 -4.3526654 -4.3500342 -4.3465648 -4.3439684 -4.3431263 -4.34443][-4.3701611 -4.3714948 -4.371892 -4.37178 -4.3714447 -4.371295 -4.3714495 -4.37136 -4.3710747 -4.3705473 -4.3694553 -4.367909 -4.3663759 -4.3655648 -4.3656683]]...]
INFO - root - 2017-12-08 00:46:38.006503: step 73110, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 51h:48m:02s remains)
INFO - root - 2017-12-08 00:46:44.784308: step 73120, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.626 sec/batch; 45h:08m:02s remains)
INFO - root - 2017-12-08 00:46:51.710292: step 73130, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 49h:49m:20s remains)
INFO - root - 2017-12-08 00:46:58.322469: step 73140, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 51h:23m:49s remains)
INFO - root - 2017-12-08 00:47:05.208715: step 73150, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 50h:58m:39s remains)
INFO - root - 2017-12-08 00:47:11.962155: step 73160, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.620 sec/batch; 44h:40m:57s remains)
INFO - root - 2017-12-08 00:47:18.879966: step 73170, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 0.767 sec/batch; 55h:12m:57s remains)
INFO - root - 2017-12-08 00:47:25.807164: step 73180, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 52h:50m:20s remains)
INFO - root - 2017-12-08 00:47:32.707402: step 73190, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 49h:54m:16s remains)
INFO - root - 2017-12-08 00:47:39.299745: step 73200, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 44h:48m:15s remains)
2017-12-08 00:47:40.113265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3199749 -4.3047929 -4.2824669 -4.2691903 -4.2696548 -4.2731986 -4.2747369 -4.27457 -4.2725658 -4.2736855 -4.2774644 -4.2824059 -4.2836242 -4.2804918 -4.2756686][-4.3120704 -4.2948689 -4.2706766 -4.2569723 -4.2598324 -4.2650228 -4.2661633 -4.2656956 -4.2625732 -4.2593141 -4.2565842 -4.2554979 -4.2535491 -4.2484093 -4.2421951][-4.3112183 -4.2972817 -4.276484 -4.262156 -4.2616091 -4.261373 -4.2566147 -4.2517018 -4.2466016 -4.2427154 -4.2362952 -4.23157 -4.228797 -4.2234898 -4.2167673][-4.3112307 -4.2977734 -4.2763829 -4.2555857 -4.2427349 -4.2308531 -4.2174687 -4.2071381 -4.2050257 -4.2077661 -4.2068014 -4.20685 -4.2089195 -4.206512 -4.20012][-4.3126965 -4.2962275 -4.2696519 -4.2386284 -4.2089672 -4.1804194 -4.1538453 -4.137032 -4.1381216 -4.1509886 -4.1634321 -4.17546 -4.1871662 -4.18876 -4.1814928][-4.3205886 -4.3032207 -4.272788 -4.2362928 -4.195127 -4.1537385 -4.1173968 -4.0939016 -4.0949764 -4.1158929 -4.1413813 -4.1643534 -4.1818881 -4.1863451 -4.1807785][-4.3290167 -4.313807 -4.286099 -4.2540665 -4.214663 -4.1736121 -4.136785 -4.1093273 -4.103044 -4.1167712 -4.1387715 -4.1593204 -4.1759686 -4.1845965 -4.1852202][-4.3272672 -4.3132191 -4.2895126 -4.2649755 -4.2359991 -4.2049026 -4.1745238 -4.1462717 -4.1349044 -4.1354094 -4.1430397 -4.1533971 -4.1658754 -4.1794791 -4.1887445][-4.3218322 -4.3070416 -4.2850394 -4.266552 -4.2476053 -4.2240877 -4.1978073 -4.1715431 -4.1573052 -4.1461315 -4.1422319 -4.1450567 -4.1566148 -4.1746907 -4.1900792][-4.3194075 -4.3042107 -4.2844634 -4.2723346 -4.261766 -4.2427011 -4.2184949 -4.1916471 -4.1726494 -4.156354 -4.14942 -4.1521969 -4.1651726 -4.1835008 -4.1965389][-4.3177843 -4.303741 -4.2871404 -4.2802811 -4.2767115 -4.2639761 -4.2428188 -4.2179704 -4.1994367 -4.1857786 -4.1803627 -4.183423 -4.1937838 -4.2068653 -4.2148404][-4.3173494 -4.3056211 -4.29141 -4.2852936 -4.2832613 -4.2732048 -4.2546782 -4.2352347 -4.22215 -4.213892 -4.20958 -4.2123518 -4.2193556 -4.2301373 -4.2383661][-4.3157396 -4.3068237 -4.2917075 -4.2818909 -4.2770448 -4.2659159 -4.2476277 -4.2327266 -4.2251873 -4.223 -4.2221265 -4.2239676 -4.2295451 -4.2414331 -4.2537236][-4.3116665 -4.3021955 -4.284317 -4.2692118 -4.2594795 -4.2477946 -4.2321897 -4.2227764 -4.2233911 -4.2274261 -4.2287469 -4.2314157 -4.2367721 -4.2472363 -4.2600617][-4.3070235 -4.2923856 -4.2674561 -4.24631 -4.2347665 -4.2263875 -4.2170773 -4.2158217 -4.2249703 -4.2339458 -4.2379265 -4.2423377 -4.2454643 -4.2526822 -4.2621355]]...]
INFO - root - 2017-12-08 00:47:46.935062: step 73210, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 53h:22m:31s remains)
INFO - root - 2017-12-08 00:47:53.752526: step 73220, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.675 sec/batch; 48h:36m:21s remains)
INFO - root - 2017-12-08 00:48:00.496129: step 73230, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 45h:40m:42s remains)
INFO - root - 2017-12-08 00:48:07.335313: step 73240, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 48h:51m:54s remains)
INFO - root - 2017-12-08 00:48:14.294417: step 73250, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.741 sec/batch; 53h:22m:50s remains)
INFO - root - 2017-12-08 00:48:21.074357: step 73260, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 51h:56m:21s remains)
INFO - root - 2017-12-08 00:48:27.843714: step 73270, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 49h:17m:02s remains)
INFO - root - 2017-12-08 00:48:34.650004: step 73280, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.622 sec/batch; 44h:45m:47s remains)
INFO - root - 2017-12-08 00:48:41.590890: step 73290, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 49h:17m:58s remains)
INFO - root - 2017-12-08 00:48:48.280569: step 73300, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 52h:56m:10s remains)
2017-12-08 00:48:48.993502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3056688 -4.3087535 -4.3111405 -4.3123679 -4.3106008 -4.304883 -4.2979264 -4.2934227 -4.292902 -4.2948422 -4.2970357 -4.2994571 -4.3027749 -4.306128 -4.3092313][-4.3009653 -4.3039074 -4.3068652 -4.3063755 -4.3007307 -4.2882161 -4.2731876 -4.2631512 -4.2627063 -4.2702742 -4.279254 -4.286787 -4.2930517 -4.2979889 -4.3026395][-4.2956777 -4.297853 -4.29909 -4.29381 -4.2782793 -4.2512589 -4.2214761 -4.2024913 -4.2051349 -4.22589 -4.2489004 -4.2657161 -4.27782 -4.2856369 -4.2916131][-4.2833571 -4.2827711 -4.2778292 -4.261281 -4.2279367 -4.1763339 -4.1245461 -4.0956345 -4.1101394 -4.1574478 -4.2037354 -4.2373691 -4.2583442 -4.2697864 -4.2761226][-4.2666135 -4.2608414 -4.245121 -4.2122393 -4.1549892 -4.0763607 -4.0001698 -3.964093 -4.003037 -4.0862923 -4.160284 -4.2105155 -4.2379336 -4.24876 -4.2518039][-4.2439623 -4.23018 -4.2002106 -4.1459808 -4.0651212 -3.9624217 -3.8684087 -3.8353631 -3.9099069 -4.032588 -4.1309443 -4.1919351 -4.2213984 -4.2272167 -4.2239857][-4.2171917 -4.1953893 -4.1510353 -4.0776939 -3.9786348 -3.8599038 -3.7533889 -3.7306073 -3.8472588 -4.0037527 -4.1129427 -4.1758819 -4.2020717 -4.2019877 -4.1943026][-4.2092962 -4.1869431 -4.1348929 -4.0517054 -3.9523871 -3.8483038 -3.7661281 -3.7651045 -3.885983 -4.0347633 -4.1294312 -4.1787944 -4.1952758 -4.1886783 -4.1790237][-4.2100453 -4.1939325 -4.1467261 -4.0709796 -3.9945519 -3.9360981 -3.905036 -3.922698 -4.0092893 -4.110815 -4.1688685 -4.1967831 -4.20549 -4.197978 -4.1906948][-4.2044864 -4.1991968 -4.1616316 -4.1017933 -4.0535221 -4.0349522 -4.0369544 -4.0589089 -4.11737 -4.1777992 -4.2018013 -4.2122922 -4.2192087 -4.2173672 -4.217751][-4.2053437 -4.2117777 -4.1863866 -4.140089 -4.1118555 -4.1162238 -4.1321435 -4.1525807 -4.1881738 -4.2207866 -4.2234392 -4.2206888 -4.2251449 -4.2320466 -4.2442551][-4.2057676 -4.2261062 -4.2178745 -4.1864786 -4.1733418 -4.186976 -4.20568 -4.2226276 -4.2443771 -4.259222 -4.24659 -4.2294412 -4.2260232 -4.2373834 -4.255826][-4.22351 -4.2513795 -4.2553682 -4.23438 -4.2243052 -4.2343755 -4.2495623 -4.2641821 -4.2772455 -4.2817907 -4.2615128 -4.2331095 -4.2205114 -4.234056 -4.2570376][-4.2557883 -4.2847586 -4.2922149 -4.27525 -4.2582231 -4.2535181 -4.259654 -4.2732029 -4.2853994 -4.2915063 -4.2743077 -4.242065 -4.2251515 -4.2376528 -4.2565846][-4.278811 -4.30441 -4.312582 -4.2964978 -4.2705822 -4.2496328 -4.246418 -4.2592053 -4.2761803 -4.29306 -4.2849631 -4.2572007 -4.2416387 -4.2490516 -4.2564073]]...]
INFO - root - 2017-12-08 00:48:55.730500: step 73310, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 45h:52m:23s remains)
INFO - root - 2017-12-08 00:49:02.546702: step 73320, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 50h:01m:30s remains)
INFO - root - 2017-12-08 00:49:09.312590: step 73330, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 52h:28m:40s remains)
INFO - root - 2017-12-08 00:49:16.099255: step 73340, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 52h:12m:52s remains)
INFO - root - 2017-12-08 00:49:22.776208: step 73350, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 47h:46m:50s remains)
INFO - root - 2017-12-08 00:49:29.499647: step 73360, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.609 sec/batch; 43h:50m:16s remains)
INFO - root - 2017-12-08 00:49:36.362295: step 73370, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 47h:32m:33s remains)
INFO - root - 2017-12-08 00:49:43.226454: step 73380, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 52h:16m:50s remains)
INFO - root - 2017-12-08 00:49:49.976624: step 73390, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 52h:13m:14s remains)
INFO - root - 2017-12-08 00:49:56.643152: step 73400, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 48h:01m:42s remains)
2017-12-08 00:49:57.417523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1461048 -4.1549349 -4.15368 -4.1283522 -4.0918331 -4.064837 -4.0567141 -4.0770531 -4.111454 -4.155726 -4.1868629 -4.1994538 -4.2029772 -4.1868968 -4.1590433][-4.1717634 -4.17483 -4.17108 -4.1477933 -4.1182804 -4.1007247 -4.0961456 -4.1090417 -4.1280651 -4.1497722 -4.1560817 -4.1552424 -4.15859 -4.1484785 -4.1294241][-4.1941733 -4.1945772 -4.193758 -4.1805043 -4.1645122 -4.1560121 -4.1519556 -4.1532063 -4.1521673 -4.1486187 -4.133822 -4.125937 -4.1325955 -4.1297817 -4.1199079][-4.2066236 -4.2101045 -4.2160239 -4.2135086 -4.2067733 -4.1988754 -4.18832 -4.176404 -4.1631751 -4.1522045 -4.1343102 -4.127574 -4.137682 -4.1382113 -4.1316152][-4.2181854 -4.229919 -4.2418947 -4.24318 -4.2335744 -4.2122 -4.1835952 -4.1559424 -4.1427569 -4.143899 -4.1425877 -4.148283 -4.16265 -4.1627827 -4.1539936][-4.2203341 -4.2413855 -4.255178 -4.2509661 -4.2261844 -4.18007 -4.1245732 -4.0830617 -4.0797167 -4.1084614 -4.1395283 -4.1685891 -4.1884761 -4.18524 -4.1716876][-4.2061682 -4.2366095 -4.2469521 -4.2339187 -4.1927195 -4.1204805 -4.0351863 -3.9785862 -3.9924998 -4.0622067 -4.1328883 -4.1808453 -4.1992297 -4.1869473 -4.1706233][-4.1771951 -4.2073865 -4.2096167 -4.1867681 -4.1359606 -4.0496116 -3.9522374 -3.900867 -3.9408808 -4.0389109 -4.1251612 -4.1724787 -4.1782022 -4.1538138 -4.1340718][-4.1617212 -4.1793509 -4.1742105 -4.1472278 -4.1006222 -4.0297565 -3.9570343 -3.9258814 -3.9648736 -4.0466547 -4.1101913 -4.1405811 -4.1327448 -4.0987067 -4.0771437][-4.1674175 -4.1753 -4.16905 -4.1487684 -4.1174927 -4.0761161 -4.0371389 -4.0201397 -4.0410004 -4.0888696 -4.1221561 -4.1316948 -4.1133742 -4.0761166 -4.0580959][-4.1794109 -4.181375 -4.1779666 -4.1672144 -4.1520534 -4.1359591 -4.1212687 -4.1118979 -4.117754 -4.1395068 -4.1513557 -4.1486416 -4.1268868 -4.0937567 -4.0802727][-4.1802444 -4.1801147 -4.1818938 -4.18155 -4.18174 -4.1813712 -4.1802258 -4.1781993 -4.178021 -4.1831989 -4.1870794 -4.1808977 -4.1618729 -4.1317525 -4.1126][-4.1682496 -4.1664324 -4.1721115 -4.1815867 -4.1912293 -4.1974344 -4.2009826 -4.2051926 -4.2049227 -4.2059083 -4.2082229 -4.2057509 -4.1934695 -4.1642761 -4.1413765][-4.1509314 -4.149229 -4.1569614 -4.1696768 -4.1845374 -4.1946912 -4.1994352 -4.20697 -4.2110705 -4.211277 -4.2131486 -4.2140036 -4.2096696 -4.1887445 -4.1723185][-4.1358614 -4.1350579 -4.1457267 -4.161459 -4.1780529 -4.1896729 -4.1952028 -4.204299 -4.2098517 -4.2100067 -4.211092 -4.2140317 -4.216238 -4.2086558 -4.202333]]...]
INFO - root - 2017-12-08 00:50:04.149910: step 73410, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 51h:06m:40s remains)
INFO - root - 2017-12-08 00:50:10.950551: step 73420, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 48h:51m:13s remains)
INFO - root - 2017-12-08 00:50:17.777687: step 73430, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.669 sec/batch; 48h:10m:43s remains)
INFO - root - 2017-12-08 00:50:24.525475: step 73440, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.618 sec/batch; 44h:28m:05s remains)
INFO - root - 2017-12-08 00:50:31.221787: step 73450, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 53h:37m:39s remains)
INFO - root - 2017-12-08 00:50:38.006125: step 73460, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 49h:26m:59s remains)
INFO - root - 2017-12-08 00:50:44.749351: step 73470, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 49h:11m:44s remains)
INFO - root - 2017-12-08 00:50:51.459425: step 73480, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.614 sec/batch; 44h:11m:37s remains)
INFO - root - 2017-12-08 00:50:58.266441: step 73490, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.638 sec/batch; 45h:52m:56s remains)
INFO - root - 2017-12-08 00:51:04.988620: step 73500, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 51h:37m:35s remains)
2017-12-08 00:51:05.780996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393308 -4.2209048 -4.2072949 -4.2020721 -4.1923633 -4.1859469 -4.1857443 -4.179956 -4.173954 -4.1734743 -4.1784239 -4.1967888 -4.2232718 -4.2387505 -4.2350192][-4.2340021 -4.2108068 -4.1908517 -4.1804781 -4.1657357 -4.1560836 -4.1522951 -4.1388383 -4.1275296 -4.1290488 -4.1405306 -4.1647635 -4.1922083 -4.2021766 -4.1950359][-4.2199926 -4.1977777 -4.1777687 -4.17055 -4.1576748 -4.1462173 -4.1424179 -4.1270843 -4.114388 -4.11623 -4.131269 -4.1584368 -4.1875591 -4.1983294 -4.1931357][-4.2062092 -4.1876349 -4.1713195 -4.1684017 -4.1589789 -4.1475697 -4.1438751 -4.1291962 -4.1209822 -4.1288104 -4.149096 -4.1773939 -4.2057319 -4.2183781 -4.2159834][-4.2094269 -4.1930313 -4.18007 -4.1795411 -4.1732311 -4.1635156 -4.1592703 -4.1467571 -4.1464128 -4.1613154 -4.1811914 -4.2044163 -4.2241821 -4.2318144 -4.2262321][-4.2214761 -4.2072458 -4.1945157 -4.1881981 -4.1740122 -4.1568313 -4.1450915 -4.1333342 -4.1417146 -4.1661072 -4.1913052 -4.2160907 -4.2358732 -4.2437425 -4.2383566][-4.2039351 -4.1805954 -4.1575184 -4.1372108 -4.1091633 -4.0806026 -4.0621161 -4.0525455 -4.07041 -4.1075797 -4.1462746 -4.1819487 -4.2095833 -4.220768 -4.2170863][-4.1720228 -4.1368818 -4.1023521 -4.072197 -4.0378609 -4.0039835 -3.9823928 -3.9732485 -3.9948621 -4.0375128 -4.0866938 -4.1299334 -4.1591892 -4.1699657 -4.168622][-4.1700878 -4.1351986 -4.1045709 -4.0839224 -4.0614209 -4.0379181 -4.0239 -4.0155354 -4.0283351 -4.0587139 -4.1001005 -4.1378164 -4.1616068 -4.1701822 -4.1693592][-4.1829529 -4.1572232 -4.1392097 -4.1325026 -4.1206112 -4.1066575 -4.1007743 -4.0955882 -4.099648 -4.1185536 -4.1506543 -4.1813989 -4.1999526 -4.2034883 -4.1969104][-4.1827345 -4.1601362 -4.1444468 -4.1403327 -4.1322155 -4.1209621 -4.117588 -4.1119232 -4.1087408 -4.1207013 -4.1486559 -4.1770973 -4.1961112 -4.1993322 -4.18947][-4.17313 -4.1475015 -4.1246691 -4.1158 -4.1077161 -4.098217 -4.094584 -4.08466 -4.0743136 -4.0777712 -4.0963392 -4.1174583 -4.1384635 -4.1478796 -4.1390767][-4.1827197 -4.1622605 -4.1439118 -4.1377616 -4.1323266 -4.1228409 -4.1150317 -4.1008577 -4.09003 -4.0923309 -4.1043568 -4.1182594 -4.1338124 -4.1388831 -4.1253719][-4.20712 -4.1969633 -4.189939 -4.1891203 -4.1844578 -4.1752048 -4.1669512 -4.1561775 -4.1541862 -4.1631541 -4.1749129 -4.1859965 -4.1972766 -4.1999555 -4.1851206][-4.2164693 -4.2167993 -4.2157822 -4.2131734 -4.2047195 -4.1945972 -4.1894317 -4.1847496 -4.1899176 -4.1994514 -4.20411 -4.2064195 -4.2116966 -4.2134366 -4.2056456]]...]
INFO - root - 2017-12-08 00:51:12.543166: step 73510, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 46h:47m:36s remains)
INFO - root - 2017-12-08 00:51:19.436344: step 73520, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 51h:39m:04s remains)
INFO - root - 2017-12-08 00:51:26.220112: step 73530, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 51h:00m:16s remains)
INFO - root - 2017-12-08 00:51:33.066632: step 73540, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 46h:34m:06s remains)
INFO - root - 2017-12-08 00:51:39.878433: step 73550, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 48h:58m:31s remains)
INFO - root - 2017-12-08 00:51:46.653380: step 73560, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 45h:20m:47s remains)
INFO - root - 2017-12-08 00:51:53.532745: step 73570, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 50h:02m:16s remains)
INFO - root - 2017-12-08 00:52:00.355837: step 73580, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 52h:05m:26s remains)
INFO - root - 2017-12-08 00:52:07.151578: step 73590, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.675 sec/batch; 48h:33m:21s remains)
INFO - root - 2017-12-08 00:52:13.790888: step 73600, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 47h:34m:54s remains)
2017-12-08 00:52:14.500331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3364973 -4.3412948 -4.3445172 -4.3475671 -4.3507195 -4.3524413 -4.3515124 -4.3484135 -4.3440056 -4.3408394 -4.3359103 -4.3249917 -4.3105397 -4.2947941 -4.2802119][-4.3266263 -4.3308992 -4.3367252 -4.34437 -4.352263 -4.3581381 -4.3605185 -4.3601885 -4.357305 -4.3545876 -4.3503623 -4.3399878 -4.3255692 -4.3082418 -4.2912493][-4.2850595 -4.2853107 -4.2927094 -4.3065453 -4.3226089 -4.3374271 -4.3472342 -4.3528128 -4.3549395 -4.3559694 -4.3555503 -4.3490357 -4.3377714 -4.3227744 -4.3065104][-4.2149725 -4.2104611 -4.2187223 -4.2380037 -4.2630324 -4.2876086 -4.30608 -4.3179708 -4.3258543 -4.3321319 -4.3369679 -4.335741 -4.3295021 -4.3194675 -4.3083844][-4.1290159 -4.1148729 -4.1172543 -4.1372833 -4.1698685 -4.2052388 -4.2348576 -4.2562194 -4.2720418 -4.2860513 -4.2983856 -4.3047161 -4.3042088 -4.2993994 -4.295023][-4.051981 -4.0266285 -4.018815 -4.0331044 -4.0660405 -4.1055536 -4.1414495 -4.1690803 -4.1916065 -4.2137141 -4.2356544 -4.251667 -4.2595778 -4.2619252 -4.2660255][-4.0255795 -3.9977272 -3.9840655 -3.9904656 -4.0168729 -4.0492568 -4.0772796 -4.0978403 -4.1154695 -4.1359735 -4.1594696 -4.1797733 -4.1943769 -4.2065592 -4.2226443][-4.0637708 -4.0434346 -4.0324168 -4.0339956 -4.0494728 -4.0657735 -4.0761437 -4.0818334 -4.0879426 -4.1011567 -4.120409 -4.1396685 -4.1560636 -4.1726651 -4.1947908][-4.1442924 -4.1333222 -4.127141 -4.1264453 -4.1318669 -4.1338172 -4.1320257 -4.1299486 -4.130465 -4.1392193 -4.15467 -4.1700673 -4.1818109 -4.1920791 -4.2070336][-4.2316933 -4.2273812 -4.2240853 -4.2216578 -4.2213755 -4.2180624 -4.21409 -4.2116585 -4.2115517 -4.2187376 -4.2300582 -4.2391806 -4.2419519 -4.2415175 -4.2441449][-4.2940803 -4.2918978 -4.2892122 -4.2853346 -4.28323 -4.2814465 -4.281004 -4.2817369 -4.2839189 -4.2908049 -4.2983718 -4.3008962 -4.2953882 -4.28607 -4.2785096][-4.3296618 -4.3292675 -4.3263569 -4.3217716 -4.3197813 -4.3212471 -4.3249578 -4.328311 -4.3310766 -4.3352046 -4.3369465 -4.3330317 -4.3215251 -4.3074374 -4.2951293][-4.3317971 -4.3326483 -4.3306489 -4.326663 -4.3253446 -4.3286247 -4.3347621 -4.3397737 -4.3437805 -4.3468952 -4.3460627 -4.339541 -4.32631 -4.3115735 -4.29835][-4.315331 -4.3168964 -4.3166871 -4.3154635 -4.3156962 -4.3191791 -4.325006 -4.3296542 -4.3335962 -4.3360896 -4.3340974 -4.3261604 -4.313539 -4.30105 -4.2903838][-4.2922721 -4.2961736 -4.2998285 -4.3031311 -4.3072162 -4.312829 -4.3186541 -4.322196 -4.3249731 -4.3263059 -4.3230915 -4.3143764 -4.3024292 -4.2915797 -4.2830734]]...]
INFO - root - 2017-12-08 00:52:21.381891: step 73610, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 51h:49m:40s remains)
INFO - root - 2017-12-08 00:52:28.200862: step 73620, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 47h:26m:44s remains)
INFO - root - 2017-12-08 00:52:35.061785: step 73630, loss = 2.11, batch loss = 2.05 (12.3 examples/sec; 0.649 sec/batch; 46h:40m:56s remains)
INFO - root - 2017-12-08 00:52:41.937814: step 73640, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 45h:46m:03s remains)
INFO - root - 2017-12-08 00:52:48.753738: step 73650, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 48h:41m:13s remains)
INFO - root - 2017-12-08 00:52:55.666454: step 73660, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 51h:46m:58s remains)
INFO - root - 2017-12-08 00:53:02.472089: step 73670, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.675 sec/batch; 48h:33m:56s remains)
INFO - root - 2017-12-08 00:53:09.197903: step 73680, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 47h:56m:11s remains)
INFO - root - 2017-12-08 00:53:15.962795: step 73690, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 45h:19m:03s remains)
INFO - root - 2017-12-08 00:53:22.490294: step 73700, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 45h:01m:17s remains)
2017-12-08 00:53:23.311453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.298996 -4.2904506 -4.2854381 -4.2820411 -4.2783189 -4.2740812 -4.2655258 -4.2562418 -4.25168 -4.2572036 -4.2695379 -4.28368 -4.2915077 -4.2924047 -4.2889953][-4.3002996 -4.289649 -4.2827449 -4.2769365 -4.2741523 -4.2687187 -4.2579513 -4.2437482 -4.2307696 -4.2337971 -4.2471848 -4.2634149 -4.2721558 -4.2737885 -4.2696357][-4.3062854 -4.2909207 -4.28007 -4.27137 -4.2669473 -4.2590137 -4.2468414 -4.2273908 -4.2066841 -4.2042837 -4.2173076 -4.2370114 -4.2501297 -4.2577257 -4.2567558][-4.309701 -4.2893214 -4.274899 -4.2641315 -4.2555442 -4.245441 -4.2326822 -4.2092543 -4.1843619 -4.1744905 -4.184607 -4.2058215 -4.226809 -4.2451048 -4.2501197][-4.3057284 -4.2777367 -4.255569 -4.2391715 -4.2237253 -4.2137218 -4.2043114 -4.1850863 -4.1594653 -4.1440291 -4.1447878 -4.1623144 -4.1912932 -4.2236881 -4.2403722][-4.2971869 -4.259582 -4.2258563 -4.1977725 -4.170609 -4.1626635 -4.1653252 -4.1598616 -4.1397223 -4.120224 -4.1052904 -4.1112409 -4.1354146 -4.1788177 -4.2154374][-4.2954059 -4.2496204 -4.2060919 -4.1679697 -4.1320648 -4.1236286 -4.1296215 -4.1293426 -4.1152229 -4.0961003 -4.0681119 -4.0546384 -4.0665026 -4.112051 -4.1630344][-4.2972264 -4.2444692 -4.1910081 -4.14549 -4.1087956 -4.0983229 -4.1022062 -4.1021166 -4.0905538 -4.073626 -4.0411496 -4.0136218 -4.01512 -4.0567126 -4.1093588][-4.3000984 -4.2471137 -4.1870422 -4.134315 -4.0970211 -4.0871258 -4.0920844 -4.0950904 -4.0830674 -4.0641422 -4.0361705 -4.0088868 -4.0065994 -4.0407887 -4.0853081][-4.3077745 -4.2629652 -4.2076125 -4.1568074 -4.118505 -4.1080551 -4.1163235 -4.1219506 -4.1083145 -4.0901361 -4.0683441 -4.049356 -4.05141 -4.0819912 -4.1135545][-4.3159542 -4.2792158 -4.2320504 -4.1889787 -4.1544662 -4.142509 -4.1527624 -4.165031 -4.1578503 -4.145741 -4.1299305 -4.1150331 -4.1196232 -4.1481481 -4.1675696][-4.3235788 -4.2965679 -4.2584453 -4.2249103 -4.2001629 -4.1911106 -4.2041669 -4.2228031 -4.2232461 -4.2177768 -4.2069573 -4.1904263 -4.1886659 -4.2089596 -4.2195029][-4.3314772 -4.3162789 -4.2887754 -4.26252 -4.2451959 -4.2433157 -4.2590227 -4.2796946 -4.2868738 -4.285 -4.2757344 -4.2592211 -4.2511873 -4.2631321 -4.2666893][-4.3335128 -4.3268867 -4.3082905 -4.2867842 -4.274251 -4.2778039 -4.2948575 -4.314909 -4.3252993 -4.3275089 -4.3202739 -4.3061767 -4.2968488 -4.3014789 -4.2985983][-4.3335032 -4.3313584 -4.3199487 -4.3027844 -4.2917938 -4.2950177 -4.3086758 -4.3237696 -4.3330836 -4.336401 -4.3332882 -4.3264141 -4.3219786 -4.3233905 -4.3178296]]...]
INFO - root - 2017-12-08 00:53:30.070950: step 73710, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 48h:50m:28s remains)
INFO - root - 2017-12-08 00:53:36.824235: step 73720, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 45h:01m:33s remains)
INFO - root - 2017-12-08 00:53:43.730413: step 73730, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 49h:05m:03s remains)
INFO - root - 2017-12-08 00:53:50.553284: step 73740, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 52h:00m:32s remains)
INFO - root - 2017-12-08 00:53:57.401636: step 73750, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 51h:52m:15s remains)
INFO - root - 2017-12-08 00:54:04.040015: step 73760, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 46h:57m:48s remains)
INFO - root - 2017-12-08 00:54:10.853233: step 73770, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 47h:02m:27s remains)
INFO - root - 2017-12-08 00:54:17.673646: step 73780, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 50h:38m:50s remains)
INFO - root - 2017-12-08 00:54:24.431256: step 73790, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 52h:10m:31s remains)
INFO - root - 2017-12-08 00:54:31.157806: step 73800, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 51h:47m:44s remains)
2017-12-08 00:54:32.102549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2072492 -4.2378759 -4.2587953 -4.2592335 -4.2445822 -4.214932 -4.1774678 -4.1565256 -4.17022 -4.1999979 -4.2278686 -4.253469 -4.273077 -4.2873812 -4.2985716][-4.1915369 -4.2151847 -4.2344284 -4.2365694 -4.2239518 -4.1939845 -4.1540608 -4.1326079 -4.147171 -4.175477 -4.2002959 -4.2224579 -4.2435865 -4.2643695 -4.2848454][-4.1996918 -4.2073684 -4.2142358 -4.2088437 -4.1952324 -4.1696115 -4.1383371 -4.123137 -4.137795 -4.1610551 -4.1784143 -4.1949844 -4.2172093 -4.2444887 -4.2730002][-4.218915 -4.2138047 -4.2025738 -4.18069 -4.1581392 -4.1370053 -4.1188726 -4.1150923 -4.13209 -4.1527462 -4.1671228 -4.1832848 -4.2073569 -4.2375383 -4.2681665][-4.2294126 -4.2142653 -4.18327 -4.1395955 -4.1033397 -4.0847306 -4.0816784 -4.0935903 -4.1198545 -4.1435318 -4.1619163 -4.1828995 -4.2093673 -4.2415395 -4.2714376][-4.2297707 -4.2050204 -4.1544333 -4.089406 -4.0394793 -4.0214868 -4.0323367 -4.0620441 -4.1000628 -4.1293764 -4.1539207 -4.182529 -4.2134118 -4.2482185 -4.2772789][-4.2164125 -4.1871314 -4.1227121 -4.0453267 -3.9913466 -3.9734449 -3.9884963 -4.0281606 -4.0775518 -4.1155925 -4.1491613 -4.1840692 -4.21939 -4.2556391 -4.2828403][-4.20047 -4.1676 -4.0990906 -4.0221691 -3.9738314 -3.9611976 -3.9766774 -4.0185809 -4.0741224 -4.11626 -4.1514616 -4.1852732 -4.2195168 -4.2536974 -4.2801633][-4.20583 -4.1706586 -4.1104193 -4.0457006 -4.0084815 -4.0047169 -4.0239191 -4.0635734 -4.1126037 -4.1448784 -4.1679106 -4.191782 -4.2181892 -4.2457042 -4.2701449][-4.2202582 -4.1878843 -4.1441841 -4.0980916 -4.0713453 -4.0748634 -4.0970125 -4.1298742 -4.1662736 -4.1837726 -4.1932554 -4.2047477 -4.2214737 -4.2416062 -4.2623854][-4.2400742 -4.2134562 -4.187273 -4.161962 -4.1479325 -4.1562657 -4.1735377 -4.1938415 -4.2162352 -4.2242541 -4.2261152 -4.2293677 -4.2398453 -4.2542019 -4.2698097][-4.26069 -4.2409549 -4.227529 -4.2168508 -4.2126584 -4.2182188 -4.2206726 -4.2208548 -4.2252479 -4.2270474 -4.231678 -4.2388997 -4.2549462 -4.2733922 -4.2891703][-4.2766991 -4.261364 -4.2515 -4.2462869 -4.243319 -4.2390676 -4.2222919 -4.1987605 -4.1852026 -4.1867561 -4.204411 -4.2274494 -4.258121 -4.2872543 -4.3067203][-4.2849941 -4.2734208 -4.2631478 -4.2560592 -4.2487803 -4.2346997 -4.2033753 -4.1611919 -4.1313691 -4.1330314 -4.1637678 -4.2021852 -4.2465954 -4.28658 -4.3111835][-4.2946529 -4.2852488 -4.2743025 -4.2641039 -4.2507267 -4.2290735 -4.1893158 -4.1352782 -4.0930562 -4.09075 -4.12724 -4.1747489 -4.228179 -4.2750235 -4.3051019]]...]
INFO - root - 2017-12-08 00:54:38.919565: step 73810, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 49h:53m:34s remains)
INFO - root - 2017-12-08 00:54:45.623239: step 73820, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 47h:06m:01s remains)
INFO - root - 2017-12-08 00:54:52.363929: step 73830, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 49h:15m:01s remains)
INFO - root - 2017-12-08 00:54:59.171618: step 73840, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 45h:28m:15s remains)
INFO - root - 2017-12-08 00:55:05.913368: step 73850, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 51h:12m:43s remains)
INFO - root - 2017-12-08 00:55:12.780995: step 73860, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 51h:27m:42s remains)
INFO - root - 2017-12-08 00:55:19.529142: step 73870, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 49h:06m:36s remains)
INFO - root - 2017-12-08 00:55:26.354953: step 73880, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 45h:39m:32s remains)
INFO - root - 2017-12-08 00:55:33.259979: step 73890, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 48h:45m:00s remains)
INFO - root - 2017-12-08 00:55:40.004530: step 73900, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.751 sec/batch; 53h:57m:19s remains)
2017-12-08 00:55:40.731677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1119151 -4.1136723 -4.1183724 -4.1236377 -4.1348057 -4.1363344 -4.1153688 -4.08583 -4.0818195 -4.1210756 -4.1663876 -4.1967735 -4.2219548 -4.2536745 -4.2837863][-4.1169834 -4.1166596 -4.1270466 -4.1356974 -4.1373353 -4.1369939 -4.12274 -4.1032724 -4.1086593 -4.1531277 -4.1896982 -4.2070522 -4.2251296 -4.2539158 -4.2770581][-4.1227803 -4.1212139 -4.1300511 -4.1423759 -4.1402116 -4.1443315 -4.1362324 -4.1083403 -4.1057506 -4.1483059 -4.1786695 -4.1941242 -4.2174363 -4.2487268 -4.269742][-4.1256003 -4.1261759 -4.1284986 -4.1471949 -4.1552043 -4.1600347 -4.1453362 -4.1022115 -4.0913267 -4.1199765 -4.14442 -4.1642642 -4.1925464 -4.2288818 -4.2555871][-4.1234951 -4.1244359 -4.1298394 -4.1468296 -4.1568265 -4.1514664 -4.1195531 -4.059216 -4.0495934 -4.085187 -4.1243834 -4.1567178 -4.1854997 -4.2230115 -4.2524333][-4.12408 -4.1269178 -4.1293421 -4.1285253 -4.1216555 -4.091712 -4.0203705 -3.9380088 -3.9618547 -4.0473266 -4.1231084 -4.1684 -4.1949248 -4.2263341 -4.25448][-4.1215215 -4.1220732 -4.1091042 -4.0860152 -4.0507073 -3.9741421 -3.8448937 -3.7463377 -3.8316092 -3.9948421 -4.1107974 -4.1716456 -4.2057242 -4.2368612 -4.2632365][-4.1340456 -4.132185 -4.1037121 -4.0631342 -4.0123954 -3.9123645 -3.770752 -3.6849146 -3.8034887 -3.9899158 -4.1081471 -4.1674142 -4.2065363 -4.2421856 -4.2666163][-4.1576738 -4.1493092 -4.11596 -4.0786386 -4.0396338 -3.9774837 -3.9031291 -3.857975 -3.9388916 -4.066484 -4.1436024 -4.1833291 -4.2197456 -4.2527628 -4.2702045][-4.1754642 -4.16396 -4.1371603 -4.1114182 -4.0933304 -4.0707164 -4.0425892 -4.0210152 -4.0645142 -4.1365442 -4.1858492 -4.2114234 -4.2348089 -4.2592206 -4.2732096][-4.1897597 -4.1834922 -4.1669922 -4.1486177 -4.1419106 -4.1404867 -4.1264191 -4.1096907 -4.1335144 -4.1747851 -4.20322 -4.21719 -4.2365046 -4.2601352 -4.2720618][-4.2029757 -4.1990056 -4.1884604 -4.176785 -4.1739054 -4.1819773 -4.1801491 -4.1759458 -4.1912827 -4.2122068 -4.2215967 -4.227654 -4.2447925 -4.2651014 -4.2715187][-4.2285814 -4.22675 -4.2206397 -4.2103281 -4.2043419 -4.2099886 -4.2181044 -4.2263093 -4.2364607 -4.2416005 -4.2440839 -4.2467961 -4.2591872 -4.2754426 -4.2742772][-4.2547455 -4.2543716 -4.25146 -4.2427449 -4.2342062 -4.2331586 -4.2402411 -4.2487955 -4.2561865 -4.2600822 -4.2652359 -4.2664909 -4.2722869 -4.2797313 -4.2723784][-4.2772336 -4.2772336 -4.2762761 -4.2684913 -4.2589464 -4.256094 -4.2585392 -4.2643089 -4.2717972 -4.2799211 -4.2850313 -4.2840796 -4.2836146 -4.2834392 -4.2766981]]...]
INFO - root - 2017-12-08 00:55:47.534613: step 73910, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 45h:43m:37s remains)
INFO - root - 2017-12-08 00:55:54.360897: step 73920, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 51h:15m:11s remains)
INFO - root - 2017-12-08 00:56:01.160199: step 73930, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 49h:12m:32s remains)
INFO - root - 2017-12-08 00:56:07.983419: step 73940, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 49h:08m:41s remains)
INFO - root - 2017-12-08 00:56:14.817967: step 73950, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 47h:57m:54s remains)
INFO - root - 2017-12-08 00:56:21.641044: step 73960, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.675 sec/batch; 48h:29m:59s remains)
INFO - root - 2017-12-08 00:56:28.550260: step 73970, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 52h:04m:34s remains)
INFO - root - 2017-12-08 00:56:35.445390: step 73980, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 53h:02m:49s remains)
INFO - root - 2017-12-08 00:56:42.318904: step 73990, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 48h:22m:50s remains)
INFO - root - 2017-12-08 00:56:48.859542: step 74000, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.604 sec/batch; 43h:23m:57s remains)
2017-12-08 00:56:49.637933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3197842 -4.3227067 -4.32089 -4.3176694 -4.3078556 -4.2930107 -4.2798023 -4.2726278 -4.2684679 -4.263515 -4.2637177 -4.2624822 -4.258194 -4.2535629 -4.2455387][-4.2845817 -4.28593 -4.284761 -4.2842846 -4.2821016 -4.275743 -4.2655058 -4.2610974 -4.2626505 -4.2581658 -4.2553172 -4.25437 -4.2561274 -4.2609897 -4.2617249][-4.2434888 -4.2455869 -4.2432952 -4.2387285 -4.2391148 -4.2376251 -4.2336011 -4.2382736 -4.2491198 -4.250731 -4.2482095 -4.2464519 -4.2542233 -4.2685242 -4.2758703][-4.1953411 -4.1985865 -4.1937122 -4.1804266 -4.1746845 -4.173521 -4.1767206 -4.1933188 -4.2108135 -4.2182055 -4.2191138 -4.2188716 -4.232554 -4.256752 -4.2714615][-4.1520958 -4.1478744 -4.1286721 -4.0942259 -4.0714746 -4.0680151 -4.0784488 -4.1105986 -4.1395369 -4.1590519 -4.1715236 -4.1797352 -4.1989679 -4.2273169 -4.2452455][-4.118083 -4.0939651 -4.0489049 -3.9894476 -3.9503412 -3.9490967 -3.9717426 -4.0259266 -4.0753841 -4.1078367 -4.1265383 -4.1413379 -4.1645408 -4.1917033 -4.2099357][-4.1087136 -4.062923 -3.9989178 -3.937463 -3.9079392 -3.9248118 -3.9592054 -4.0175323 -4.07416 -4.109045 -4.1264114 -4.1447005 -4.1672006 -4.1894712 -4.203433][-4.1452208 -4.0915012 -4.0339875 -4.0016174 -3.9988544 -4.0235653 -4.0500736 -4.0874157 -4.1288452 -4.1565557 -4.1729388 -4.1927567 -4.2095881 -4.222012 -4.2286243][-4.2181993 -4.1799569 -4.1431947 -4.13167 -4.1384845 -4.1513748 -4.1591654 -4.1737933 -4.1972146 -4.2173657 -4.233676 -4.2503047 -4.2588668 -4.2596903 -4.2587533][-4.2786789 -4.2603631 -4.244585 -4.2433424 -4.2494988 -4.2518373 -4.2477794 -4.2489748 -4.2579064 -4.2694836 -4.2826681 -4.2953105 -4.2978082 -4.2900229 -4.2822371][-4.3150082 -4.3085203 -4.306076 -4.3107252 -4.3141165 -4.3091722 -4.3005986 -4.2964916 -4.2990332 -4.3064218 -4.3160796 -4.3240848 -4.322772 -4.3086658 -4.2941346][-4.3353949 -4.33386 -4.3358011 -4.3394637 -4.3397045 -4.3325596 -4.3245559 -4.3205976 -4.3224754 -4.3279686 -4.3340549 -4.33744 -4.3340936 -4.3199635 -4.3054895][-4.3449574 -4.3441429 -4.3447332 -4.3449311 -4.3414984 -4.3353043 -4.3310165 -4.330586 -4.3335838 -4.338243 -4.3420238 -4.3428249 -4.3394289 -4.3292365 -4.3199186][-4.346127 -4.346303 -4.3465147 -4.3454909 -4.3419113 -4.3383126 -4.3376079 -4.338655 -4.3420577 -4.3455319 -4.3470612 -4.34609 -4.34299 -4.3361444 -4.3307009][-4.3415318 -4.3443904 -4.3473625 -4.3485627 -4.3470731 -4.3453836 -4.3453245 -4.3456097 -4.3477964 -4.3492188 -4.348659 -4.3472786 -4.3445559 -4.33999 -4.3366942]]...]
INFO - root - 2017-12-08 00:56:56.454297: step 74010, loss = 2.10, batch loss = 2.04 (10.7 examples/sec; 0.748 sec/batch; 53h:43m:58s remains)
INFO - root - 2017-12-08 00:57:03.245170: step 74020, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 49h:29m:02s remains)
INFO - root - 2017-12-08 00:57:10.001273: step 74030, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 48h:24m:48s remains)
INFO - root - 2017-12-08 00:57:16.878319: step 74040, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 45h:49m:18s remains)
INFO - root - 2017-12-08 00:57:23.638142: step 74050, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 51h:01m:08s remains)
INFO - root - 2017-12-08 00:57:30.398792: step 74060, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 50h:30m:21s remains)
INFO - root - 2017-12-08 00:57:36.936860: step 74070, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 45h:52m:02s remains)
INFO - root - 2017-12-08 00:57:43.855640: step 74080, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 49h:06m:58s remains)
INFO - root - 2017-12-08 00:57:50.704163: step 74090, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 51h:44m:52s remains)
INFO - root - 2017-12-08 00:57:57.439354: step 74100, loss = 2.09, batch loss = 2.04 (11.7 examples/sec; 0.685 sec/batch; 49h:09m:45s remains)
2017-12-08 00:57:58.269471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25179 -4.2453179 -4.2454371 -4.2523031 -4.2486496 -4.2357025 -4.2223864 -4.208395 -4.2072225 -4.2221684 -4.23769 -4.2301211 -4.204802 -4.1779556 -4.1755948][-4.2754378 -4.2681427 -4.2667632 -4.2704363 -4.2688484 -4.2550983 -4.22984 -4.2015224 -4.192245 -4.2129731 -4.2374792 -4.2415915 -4.2321835 -4.2156558 -4.2179489][-4.2928572 -4.2892466 -4.2888637 -4.2894869 -4.2869158 -4.2681437 -4.2281909 -4.182869 -4.1686339 -4.2010064 -4.2384367 -4.2484684 -4.245647 -4.2351651 -4.2352338][-4.2955179 -4.2953892 -4.2948279 -4.2897878 -4.2809138 -4.2537217 -4.2010803 -4.14557 -4.1344938 -4.1839929 -4.2377162 -4.2552862 -4.25972 -4.2554188 -4.2513657][-4.2730327 -4.2781963 -4.2781105 -4.2665539 -4.2501249 -4.2133646 -4.1447926 -4.0841742 -4.090785 -4.165338 -4.2344584 -4.2605529 -4.2705212 -4.270462 -4.2649393][-4.2352643 -4.2440228 -4.2444415 -4.2282491 -4.2002258 -4.1461587 -4.0498734 -3.974632 -4.0097437 -4.1231771 -4.2109537 -4.250267 -4.2683225 -4.2710342 -4.2662477][-4.1899533 -4.1975865 -4.1953826 -4.1767187 -4.1372662 -4.0579724 -3.9203379 -3.8176165 -3.8832388 -4.0492973 -4.1659837 -4.2244725 -4.2545271 -4.2597561 -4.252389][-4.1458178 -4.1504517 -4.1464214 -4.1233635 -4.0681705 -3.958775 -3.7754316 -3.6410205 -3.7449341 -3.9632819 -4.1125441 -4.1876163 -4.2277894 -4.2348142 -4.2238779][-4.1188149 -4.1243615 -4.1218877 -4.0943141 -4.0276494 -3.9059081 -3.7102652 -3.5754693 -3.6967936 -3.9205947 -4.0741143 -4.1521626 -4.19088 -4.1939292 -4.1840386][-4.127965 -4.1386938 -4.1449852 -4.1199117 -4.0612268 -3.9687324 -3.8304522 -3.7472103 -3.8278809 -3.9808879 -4.0867605 -4.141664 -4.1648178 -4.1622725 -4.1515546][-4.1708941 -4.1831632 -4.1947904 -4.1780515 -4.1386685 -4.0829892 -4.0046744 -3.9637141 -4.0048814 -4.084311 -4.1344 -4.1517248 -4.1561213 -4.1489692 -4.1376524][-4.2071581 -4.2185674 -4.235805 -4.2272305 -4.1985703 -4.1618152 -4.1203871 -4.1017675 -4.1234865 -4.1605587 -4.1714668 -4.1597395 -4.1490455 -4.1355758 -4.1262][-4.2207136 -4.23748 -4.2640014 -4.2611012 -4.23454 -4.2031636 -4.1801324 -4.1739836 -4.1843925 -4.1988039 -4.1867046 -4.1548553 -4.1281519 -4.1106534 -4.1071892][-4.2136354 -4.2408829 -4.2742715 -4.2773094 -4.2536721 -4.22474 -4.20816 -4.204134 -4.2046127 -4.205874 -4.1808348 -4.1377869 -4.0948977 -4.0734081 -4.0797396][-4.2071838 -4.2420435 -4.2763019 -4.2800097 -4.2595353 -4.2334671 -4.2189884 -4.2103181 -4.2040238 -4.1995993 -4.1706314 -4.1172247 -4.0581651 -4.0242167 -4.0354471]]...]
INFO - root - 2017-12-08 00:58:05.020732: step 74110, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 49h:07m:58s remains)
INFO - root - 2017-12-08 00:58:11.764469: step 74120, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 47h:41m:17s remains)
INFO - root - 2017-12-08 00:58:18.578874: step 74130, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.639 sec/batch; 45h:50m:44s remains)
INFO - root - 2017-12-08 00:58:25.345574: step 74140, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 46h:21m:34s remains)
INFO - root - 2017-12-08 00:58:32.128338: step 74150, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 52h:17m:24s remains)
INFO - root - 2017-12-08 00:58:38.875652: step 74160, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 52h:00m:03s remains)
INFO - root - 2017-12-08 00:58:45.699327: step 74170, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 46h:46m:20s remains)
INFO - root - 2017-12-08 00:58:52.419789: step 74180, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 44h:06m:57s remains)
INFO - root - 2017-12-08 00:58:59.290191: step 74190, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 44h:37m:57s remains)
INFO - root - 2017-12-08 00:59:06.008160: step 74200, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 51h:50m:20s remains)
2017-12-08 00:59:06.764266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0479088 -4.0702772 -4.0891385 -4.1153803 -4.1473064 -4.1771965 -4.1879182 -4.171535 -4.1319485 -4.0958481 -4.07054 -4.0667853 -4.0874434 -4.1261072 -4.1550779][-4.0720477 -4.08782 -4.1005507 -4.1244521 -4.1600761 -4.1959515 -4.2182198 -4.2107768 -4.18245 -4.1569605 -4.134099 -4.1129189 -4.1033244 -4.1251554 -4.1526074][-4.1038332 -4.1241679 -4.135344 -4.1548347 -4.1838241 -4.2175608 -4.2377787 -4.228755 -4.2080464 -4.1961818 -4.180407 -4.1484814 -4.1125731 -4.1097207 -4.1269884][-4.1221094 -4.1454835 -4.1582613 -4.1753478 -4.1995339 -4.2203093 -4.2180262 -4.1935811 -4.1778231 -4.1825976 -4.1836071 -4.1632829 -4.1267643 -4.1110458 -4.1258287][-4.12322 -4.1513577 -4.1699185 -4.1886091 -4.2021389 -4.1953206 -4.16032 -4.1132364 -4.0995016 -4.1215253 -4.1522374 -4.1648788 -4.151649 -4.1433358 -4.1584077][-4.1025534 -4.1434979 -4.1754737 -4.1945772 -4.1913753 -4.1525159 -4.0833793 -4.0071058 -3.9729953 -3.9942226 -4.0624337 -4.1307454 -4.1582422 -4.1666489 -4.18582][-4.0584941 -4.1181951 -4.164115 -4.1799359 -4.1582756 -4.0936775 -3.9934928 -3.8805902 -3.8048851 -3.8132122 -3.9268687 -4.0606165 -4.1387005 -4.1722713 -4.1980729][-3.9937837 -4.0658116 -4.1195226 -4.1340942 -4.1073246 -4.0373254 -3.919601 -3.7766676 -3.6724186 -3.684922 -3.8346775 -4.005024 -4.1191039 -4.1747851 -4.1997461][-3.9597347 -4.0173392 -4.0614791 -4.0776215 -4.0636783 -4.0116987 -3.9114416 -3.7884057 -3.7071347 -3.7295554 -3.8600309 -4.0093412 -4.1192055 -4.17249 -4.1862907][-4.000649 -4.0288467 -4.051631 -4.0637984 -4.0663176 -4.0470543 -3.9904802 -3.9188573 -3.875273 -3.8963532 -3.9784808 -4.0749536 -4.1495876 -4.1825056 -4.1815872][-4.0903172 -4.0986614 -4.1055737 -4.1092515 -4.1157379 -4.114121 -4.0847387 -4.0499821 -4.036828 -4.0564795 -4.1005812 -4.1535616 -4.192956 -4.2026172 -4.1936722][-4.1804676 -4.1820154 -4.1778669 -4.1713719 -4.1711526 -4.1695652 -4.1525788 -4.1401653 -4.1440077 -4.1637588 -4.1882362 -4.2168541 -4.2356105 -4.234489 -4.22599][-4.253583 -4.2502055 -4.2402992 -4.2258472 -4.2156773 -4.2095342 -4.2003264 -4.2007928 -4.2169204 -4.2384844 -4.2544007 -4.2703228 -4.2791228 -4.2762904 -4.2666116][-4.308063 -4.3017049 -4.2873154 -4.268795 -4.2554464 -4.2497964 -4.2467675 -4.2550325 -4.2768369 -4.2996488 -4.3122973 -4.318047 -4.3181887 -4.3125172 -4.3013897][-4.3441176 -4.3391991 -4.3256855 -4.3092289 -4.2966342 -4.2913136 -4.2911997 -4.3018355 -4.322906 -4.3421688 -4.3516889 -4.3516855 -4.3458638 -4.3374724 -4.3269081]]...]
INFO - root - 2017-12-08 00:59:13.579980: step 74210, loss = 2.06, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 44h:55m:42s remains)
INFO - root - 2017-12-08 00:59:20.391101: step 74220, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 46h:04m:10s remains)
INFO - root - 2017-12-08 00:59:27.272035: step 74230, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 53h:09m:15s remains)
INFO - root - 2017-12-08 00:59:34.189186: step 74240, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 50h:04m:22s remains)
INFO - root - 2017-12-08 00:59:40.944187: step 74250, loss = 2.05, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 44h:26m:23s remains)
INFO - root - 2017-12-08 00:59:47.705684: step 74260, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 46h:42m:13s remains)
INFO - root - 2017-12-08 00:59:54.472721: step 74270, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 48h:20m:50s remains)
INFO - root - 2017-12-08 01:00:01.419485: step 74280, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.737 sec/batch; 52h:53m:41s remains)
INFO - root - 2017-12-08 01:00:08.165558: step 74290, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 50h:48m:02s remains)
INFO - root - 2017-12-08 01:00:14.834449: step 74300, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 45h:04m:05s remains)
2017-12-08 01:00:15.623050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23885 -4.237875 -4.2376165 -4.2253361 -4.2049513 -4.1838026 -4.1713738 -4.1925526 -4.1983585 -4.1793232 -4.151927 -4.1321077 -4.1437774 -4.1700354 -4.1945643][-4.2780728 -4.2766457 -4.2717538 -4.25333 -4.2246633 -4.1930413 -4.1666346 -4.1850433 -4.1981578 -4.1841712 -4.1633897 -4.14504 -4.1555996 -4.1815262 -4.2077742][-4.3020978 -4.3010464 -4.2881761 -4.2553077 -4.2116194 -4.1722026 -4.13945 -4.1625643 -4.190464 -4.1875205 -4.1729116 -4.1518545 -4.1562228 -4.182682 -4.2136459][-4.3073754 -4.310524 -4.2957087 -4.2476811 -4.1820545 -4.1259918 -4.0877495 -4.1228819 -4.1724563 -4.1898522 -4.1835904 -4.1579995 -4.1503925 -4.17175 -4.206893][-4.2903867 -4.2999682 -4.2889824 -4.2289186 -4.1390052 -4.0555644 -3.9971998 -4.0467858 -4.12939 -4.1769233 -4.1856556 -4.157794 -4.1365471 -4.1424522 -4.1759334][-4.2434273 -4.259553 -4.2586331 -4.194838 -4.0829859 -3.961638 -3.8608382 -3.9220061 -4.0465894 -4.1330466 -4.16995 -4.1498561 -4.1194806 -4.1058269 -4.1236887][-4.1876869 -4.2090464 -4.2236676 -4.1710596 -4.0538177 -3.9018705 -3.7465007 -3.8022339 -3.9614916 -4.08137 -4.14656 -4.1478157 -4.1201296 -4.0933957 -4.088881][-4.1594381 -4.1845684 -4.2116504 -4.1791964 -4.0786676 -3.9372323 -3.7750282 -3.8046203 -3.9473279 -4.0652156 -4.1349874 -4.1525364 -4.1392288 -4.11643 -4.0999589][-4.1578383 -4.1849771 -4.2199607 -4.2080803 -4.134449 -4.0300317 -3.9062674 -3.9162612 -4.0118418 -4.0981517 -4.1528521 -4.1739011 -4.1718812 -4.1589141 -4.1406231][-4.172998 -4.1993351 -4.2368565 -4.242197 -4.193151 -4.1233811 -4.0377736 -4.0357952 -4.0914378 -4.14607 -4.1842904 -4.2046137 -4.208066 -4.20288 -4.1914372][-4.1865091 -4.210454 -4.246839 -4.2626262 -4.23581 -4.1938791 -4.1377511 -4.1332455 -4.1649876 -4.1951685 -4.219964 -4.2378054 -4.2418637 -4.2436175 -4.244112][-4.1941786 -4.2155352 -4.2484918 -4.2704749 -4.2617149 -4.241035 -4.208169 -4.206027 -4.2256188 -4.24253 -4.2578025 -4.2700372 -4.2708654 -4.2757821 -4.2824554][-4.211906 -4.2291059 -4.2566772 -4.2800531 -4.2852464 -4.2810111 -4.264267 -4.2644529 -4.2789836 -4.2892747 -4.296608 -4.3011436 -4.2983632 -4.3021483 -4.3053617][-4.2491927 -4.2600179 -4.2790766 -4.298317 -4.3087139 -4.31031 -4.3003249 -4.2969623 -4.3048763 -4.3102331 -4.31125 -4.311451 -4.3092303 -4.3098989 -4.3081093][-4.2731214 -4.2808504 -4.2928867 -4.3060374 -4.315311 -4.3178749 -4.310946 -4.3054 -4.3078227 -4.3082595 -4.3051434 -4.3022609 -4.3001852 -4.3000336 -4.2970152]]...]
INFO - root - 2017-12-08 01:00:22.618750: step 74310, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 0.782 sec/batch; 56h:06m:28s remains)
INFO - root - 2017-12-08 01:00:29.494633: step 74320, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 51h:26m:53s remains)
INFO - root - 2017-12-08 01:00:36.214627: step 74330, loss = 2.07, batch loss = 2.02 (13.3 examples/sec; 0.601 sec/batch; 43h:05m:57s remains)
INFO - root - 2017-12-08 01:00:43.030601: step 74340, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 48h:43m:05s remains)
INFO - root - 2017-12-08 01:00:49.966785: step 74350, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.747 sec/batch; 53h:33m:57s remains)
INFO - root - 2017-12-08 01:00:56.920217: step 74360, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 47h:23m:13s remains)
INFO - root - 2017-12-08 01:01:03.615025: step 74370, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 45h:48m:20s remains)
INFO - root - 2017-12-08 01:01:10.275914: step 74380, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 53h:10m:30s remains)
INFO - root - 2017-12-08 01:01:16.994593: step 74390, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 48h:45m:14s remains)
INFO - root - 2017-12-08 01:01:23.701294: step 74400, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 50h:17m:29s remains)
2017-12-08 01:01:24.417706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3494267 -4.3444929 -4.3412638 -4.3334975 -4.3252721 -4.3202381 -4.3170834 -4.31794 -4.3185182 -4.3227243 -4.3251672 -4.3186631 -4.308609 -4.3029938 -4.3000569][-4.3592906 -4.3515406 -4.3439646 -4.3315339 -4.32176 -4.3166571 -4.3126841 -4.3142071 -4.3179669 -4.3267975 -4.3300519 -4.3202043 -4.3078895 -4.2990365 -4.293273][-4.3508668 -4.3403363 -4.3281527 -4.31044 -4.2992997 -4.2929015 -4.2877421 -4.28938 -4.2947712 -4.3046794 -4.3085446 -4.2992554 -4.2893524 -4.2805228 -4.273397][-4.3349376 -4.321187 -4.3062167 -4.2861967 -4.2733684 -4.266326 -4.2597089 -4.260149 -4.2660909 -4.2733321 -4.2790141 -4.2727175 -4.2651763 -4.2573085 -4.2488055][-4.3227968 -4.3039269 -4.2879772 -4.2670746 -4.2476563 -4.2309828 -4.2172737 -4.2182159 -4.232028 -4.2471685 -4.2521729 -4.244679 -4.2387772 -4.2318516 -4.2246366][-4.3149748 -4.2885542 -4.2645121 -4.2309895 -4.1954246 -4.1614747 -4.1329274 -4.1350336 -4.1781015 -4.2151952 -4.2219486 -4.2123156 -4.2068691 -4.2002234 -4.1959081][-4.3015723 -4.265501 -4.2264705 -4.1726551 -4.1175923 -4.05905 -3.9999187 -3.9938617 -4.07736 -4.15066 -4.165143 -4.1550403 -4.1522255 -4.1532135 -4.1591921][-4.28973 -4.2468405 -4.1963482 -4.1270146 -4.0525379 -3.9684343 -3.8713014 -3.8359804 -3.9467404 -4.0563135 -4.0863514 -4.0770855 -4.078764 -4.0924091 -4.1130443][-4.288691 -4.2453966 -4.195859 -4.1246858 -4.0447273 -3.9600391 -3.8595231 -3.8034132 -3.896682 -4.0055213 -4.0405641 -4.0317521 -4.0307779 -4.049087 -4.0782166][-4.2903595 -4.2543344 -4.2175393 -4.1632366 -4.1027088 -4.0473881 -3.9845872 -3.9406667 -3.9850662 -4.0481606 -4.0695624 -4.0611176 -4.0568271 -4.0688386 -4.0906472][-4.2952542 -4.2681866 -4.2412477 -4.2040148 -4.1627111 -4.1377153 -4.1096454 -4.0859032 -4.099822 -4.1253786 -4.1318135 -4.123765 -4.118866 -4.1271148 -4.1398468][-4.3030562 -4.2777381 -4.2551246 -4.2312713 -4.2109904 -4.2092829 -4.2005053 -4.1912289 -4.1927958 -4.1969066 -4.1915364 -4.1809044 -4.1761951 -4.1817083 -4.1894484][-4.3160563 -4.2881365 -4.2623134 -4.2485213 -4.2452831 -4.2543788 -4.2551517 -4.2564468 -4.259757 -4.259275 -4.2494564 -4.2359672 -4.2276616 -4.2289 -4.2325749][-4.3173413 -4.2909627 -4.2694426 -4.2687469 -4.2777967 -4.2877421 -4.2906289 -4.2948503 -4.2991714 -4.2982244 -4.2902441 -4.2777767 -4.2681241 -4.26578 -4.2663736][-4.3134241 -4.2981935 -4.288291 -4.2972951 -4.3109918 -4.3182788 -4.3199115 -4.3229837 -4.3234038 -4.3189263 -4.3099537 -4.2997756 -4.2894735 -4.28601 -4.2863393]]...]
INFO - root - 2017-12-08 01:01:31.352600: step 74410, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 52h:05m:07s remains)
INFO - root - 2017-12-08 01:01:38.291865: step 74420, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.744 sec/batch; 53h:21m:31s remains)
INFO - root - 2017-12-08 01:01:44.988243: step 74430, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 48h:46m:01s remains)
INFO - root - 2017-12-08 01:01:51.751059: step 74440, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.617 sec/batch; 44h:15m:49s remains)
INFO - root - 2017-12-08 01:01:58.650445: step 74450, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 51h:02m:22s remains)
INFO - root - 2017-12-08 01:02:05.441366: step 74460, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 51h:18m:18s remains)
INFO - root - 2017-12-08 01:02:12.130225: step 74470, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 51h:46m:03s remains)
INFO - root - 2017-12-08 01:02:18.822048: step 74480, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 48h:01m:38s remains)
INFO - root - 2017-12-08 01:02:25.679458: step 74490, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 47h:44m:42s remains)
INFO - root - 2017-12-08 01:02:32.306864: step 74500, loss = 2.11, batch loss = 2.05 (12.4 examples/sec; 0.644 sec/batch; 46h:09m:48s remains)
2017-12-08 01:02:33.118365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861652 -4.2826362 -4.2841897 -4.2857747 -4.2905364 -4.2937007 -4.2862067 -4.2793336 -4.2849646 -4.2938461 -4.3028164 -4.2982407 -4.2828126 -4.2688317 -4.2586403][-4.2677288 -4.2662954 -4.2710013 -4.2744837 -4.2851424 -4.2898893 -4.2772083 -4.2629194 -4.2673922 -4.2803211 -4.2906389 -4.2808104 -4.2583351 -4.2411036 -4.2303958][-4.242393 -4.2456779 -4.2531233 -4.2609577 -4.2813716 -4.2900977 -4.267427 -4.2402368 -4.2416844 -4.2624512 -4.2760253 -4.2622056 -4.229043 -4.2082167 -4.1974382][-4.2150636 -4.2208352 -4.2312965 -4.2469468 -4.2722688 -4.2743473 -4.233284 -4.1845021 -4.1851258 -4.2244749 -4.2572317 -4.2513294 -4.2120218 -4.1865664 -4.1740341][-4.1879196 -4.1951876 -4.2086535 -4.2281156 -4.24192 -4.2216573 -4.1508112 -4.0760856 -4.0837374 -4.1553435 -4.2207265 -4.2357116 -4.2028995 -4.1716428 -4.1546659][-4.1553583 -4.1606226 -4.1748333 -4.1915402 -4.1894588 -4.138485 -4.0233502 -3.9147952 -3.9411221 -4.05814 -4.1614528 -4.1991973 -4.1778836 -4.1453009 -4.126][-4.1274619 -4.13275 -4.1453829 -4.1517897 -4.1324415 -4.0520287 -3.8856223 -3.7392731 -3.7962692 -3.9740305 -4.1141973 -4.1599021 -4.1449466 -4.10967 -4.0881567][-4.1274672 -4.1267195 -4.1336074 -4.130209 -4.0992994 -4.0050864 -3.8148725 -3.6479964 -3.7329555 -3.95218 -4.0978041 -4.1352906 -4.1188297 -4.0857439 -4.0655642][-4.1594286 -4.1569343 -4.1576948 -4.1480618 -4.1141653 -4.0349865 -3.879375 -3.7514315 -3.8485937 -4.0396614 -4.1424828 -4.1565475 -4.12891 -4.1022663 -4.0898066][-4.1978827 -4.2007546 -4.2030334 -4.1951289 -4.1635575 -4.1046176 -4.0018206 -3.9335959 -4.0217547 -4.1538072 -4.2043118 -4.1930723 -4.1548824 -4.138289 -4.1433754][-4.2390957 -4.2464137 -4.2542605 -4.2503848 -4.2229648 -4.17885 -4.1174116 -4.0922656 -4.156023 -4.235384 -4.2552972 -4.2331929 -4.1939383 -4.1869221 -4.2053542][-4.2803864 -4.290122 -4.3004375 -4.2970619 -4.2714314 -4.2356091 -4.2017713 -4.1974497 -4.2392292 -4.2870412 -4.2888174 -4.2623739 -4.2280092 -4.2312369 -4.2592373][-4.3097396 -4.3191762 -4.3251595 -4.3174062 -4.292047 -4.263689 -4.2480717 -4.2510271 -4.2751832 -4.3039575 -4.3027363 -4.2835436 -4.2584119 -4.2670121 -4.2996454][-4.3113241 -4.3212366 -4.3252439 -4.3162737 -4.2938223 -4.2727785 -4.2653055 -4.2687645 -4.2800145 -4.2981868 -4.2985168 -4.2873755 -4.2703743 -4.2776651 -4.308506][-4.2983217 -4.3064284 -4.3091955 -4.3039346 -4.2907605 -4.2783656 -4.275094 -4.2759938 -4.2776084 -4.284977 -4.2859721 -4.2808223 -4.2708158 -4.2767382 -4.3023982]]...]
INFO - root - 2017-12-08 01:02:39.981039: step 74510, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 48h:11m:00s remains)
INFO - root - 2017-12-08 01:02:46.748141: step 74520, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 45h:43m:56s remains)
INFO - root - 2017-12-08 01:02:53.583371: step 74530, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.664 sec/batch; 47h:34m:43s remains)
INFO - root - 2017-12-08 01:03:00.436394: step 74540, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 52h:17m:49s remains)
INFO - root - 2017-12-08 01:03:07.261315: step 74550, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.772 sec/batch; 55h:20m:09s remains)
INFO - root - 2017-12-08 01:03:14.006114: step 74560, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.675 sec/batch; 48h:23m:47s remains)
INFO - root - 2017-12-08 01:03:20.733701: step 74570, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 45h:33m:08s remains)
INFO - root - 2017-12-08 01:03:27.588550: step 74580, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.658 sec/batch; 47h:06m:52s remains)
INFO - root - 2017-12-08 01:03:34.455968: step 74590, loss = 2.09, batch loss = 2.04 (11.1 examples/sec; 0.723 sec/batch; 51h:45m:51s remains)
INFO - root - 2017-12-08 01:03:41.125177: step 74600, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 50h:00m:14s remains)
2017-12-08 01:03:41.911590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1462226 -4.1686893 -4.195622 -4.2204695 -4.2397733 -4.2476897 -4.2492771 -4.2494855 -4.2413282 -4.2332311 -4.2409658 -4.2562618 -4.2600889 -4.240973 -4.2213392][-4.1407552 -4.16094 -4.1879287 -4.21475 -4.2376032 -4.24793 -4.2507768 -4.2520885 -4.2461448 -4.2434149 -4.2564187 -4.2729664 -4.2741904 -4.2488151 -4.2210755][-4.148149 -4.1610012 -4.1874213 -4.2191377 -4.2458849 -4.2552371 -4.2543511 -4.2513547 -4.2462878 -4.249527 -4.2662964 -4.2842836 -4.2871675 -4.2637658 -4.2353654][-4.1644449 -4.1731319 -4.201417 -4.2378607 -4.2646656 -4.2708812 -4.2593441 -4.2417793 -4.2306981 -4.2386031 -4.2614732 -4.2827115 -4.2896252 -4.2737589 -4.2511625][-4.1876683 -4.1971045 -4.2259011 -4.2603979 -4.2792125 -4.2733974 -4.2426782 -4.2002406 -4.1772671 -4.1920447 -4.22823 -4.2605524 -4.2762885 -4.2716775 -4.2571979][-4.2053113 -4.2189231 -4.2440276 -4.2678256 -4.2726383 -4.2494745 -4.1928086 -4.11981 -4.0832028 -4.1106076 -4.1692371 -4.220346 -4.2469969 -4.2544513 -4.2480693][-4.2132716 -4.2313428 -4.2501559 -4.2586684 -4.2423162 -4.1963344 -4.1137633 -4.0102396 -3.9582846 -4.0046096 -4.0963407 -4.1712246 -4.2093711 -4.2269034 -4.2268257][-4.2210693 -4.2362285 -4.2435164 -4.2341166 -4.1964126 -4.1294622 -4.0231991 -3.8917186 -3.8252068 -3.9049993 -4.0412126 -4.141428 -4.1890483 -4.2098575 -4.2101278][-4.2273216 -4.2365375 -4.2343459 -4.2137351 -4.1666141 -4.0919633 -3.9867132 -3.86528 -3.8108339 -3.9016659 -4.0438919 -4.1431928 -4.1861835 -4.2007456 -4.1994004][-4.2292562 -4.2349062 -4.2303295 -4.2109857 -4.1711173 -4.1148195 -4.0438333 -3.9695868 -3.9334555 -3.9899487 -4.0942144 -4.169014 -4.1940732 -4.1962671 -4.1929131][-4.2278376 -4.2359557 -4.2392597 -4.2293792 -4.2033448 -4.1708646 -4.1321425 -4.0875473 -4.0570211 -4.0808115 -4.1453266 -4.1965556 -4.2061334 -4.1983194 -4.1926627][-4.2226362 -4.2361321 -4.2485909 -4.2496343 -4.23584 -4.2188616 -4.1983619 -4.1688652 -4.1437707 -4.1490006 -4.1816344 -4.2119541 -4.2131467 -4.19938 -4.1916542][-4.2079282 -4.2246666 -4.2434826 -4.2565365 -4.2559881 -4.2473516 -4.2369976 -4.215837 -4.1924343 -4.1873245 -4.2009449 -4.2180805 -4.2150364 -4.1989932 -4.1899428][-4.2011251 -4.2167487 -4.2371225 -4.2562056 -4.26406 -4.2608743 -4.2553825 -4.2397194 -4.2182918 -4.2078972 -4.2152848 -4.2285194 -4.225986 -4.21025 -4.20013][-4.2199221 -4.226604 -4.2409825 -4.2601881 -4.2725034 -4.2734289 -4.2719707 -4.2613173 -4.2403789 -4.2273159 -4.2321315 -4.2448072 -4.2477889 -4.238543 -4.2310824]]...]
INFO - root - 2017-12-08 01:03:48.734496: step 74610, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 51h:41m:49s remains)
INFO - root - 2017-12-08 01:03:55.516968: step 74620, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 51h:51m:43s remains)
INFO - root - 2017-12-08 01:04:02.210206: step 74630, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 46h:34m:03s remains)
INFO - root - 2017-12-08 01:04:08.912157: step 74640, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 46h:43m:41s remains)
INFO - root - 2017-12-08 01:04:15.676796: step 74650, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 46h:16m:35s remains)
INFO - root - 2017-12-08 01:04:22.579531: step 74660, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 51h:37m:32s remains)
INFO - root - 2017-12-08 01:04:29.433183: step 74670, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.733 sec/batch; 52h:30m:21s remains)
INFO - root - 2017-12-08 01:04:36.365421: step 74680, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 46h:57m:54s remains)
INFO - root - 2017-12-08 01:04:42.991928: step 74690, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 46h:13m:19s remains)
INFO - root - 2017-12-08 01:04:49.626264: step 74700, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.685 sec/batch; 49h:03m:48s remains)
2017-12-08 01:04:50.461002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2374434 -4.2317305 -4.2341638 -4.2415385 -4.2434897 -4.2309418 -4.2198119 -4.1983061 -4.2030387 -4.227109 -4.2505627 -4.2444253 -4.2154617 -4.1904864 -4.1910253][-4.2579484 -4.2566719 -4.2583127 -4.258441 -4.2587361 -4.2490883 -4.2303491 -4.2024851 -4.2034912 -4.2237182 -4.2401872 -4.2209086 -4.1794477 -4.1478786 -4.1491][-4.2896962 -4.291688 -4.2864661 -4.2744727 -4.2666903 -4.2573824 -4.2386146 -4.2180052 -4.22139 -4.2412381 -4.2556028 -4.22961 -4.1777515 -4.1304884 -4.1174822][-4.3123546 -4.3179951 -4.3083053 -4.279386 -4.2531137 -4.2306657 -4.2088933 -4.2045732 -4.2198129 -4.245584 -4.2673097 -4.2538309 -4.2055197 -4.1463394 -4.1136975][-4.318315 -4.3243756 -4.3056536 -4.2555857 -4.20314 -4.1573691 -4.124404 -4.1346331 -4.170826 -4.2135887 -4.2512484 -4.2600808 -4.2245474 -4.1644998 -4.1221228][-4.3004227 -4.3053017 -4.2707038 -4.1931725 -4.1084323 -4.0298624 -3.9731119 -3.9953074 -4.0645089 -4.1416111 -4.2059956 -4.2439113 -4.2327538 -4.1871371 -4.1476512][-4.266748 -4.2730384 -4.2243948 -4.1196589 -3.9993057 -3.8779011 -3.7906852 -3.8162024 -3.9234281 -4.0379739 -4.127162 -4.1936483 -4.2138124 -4.1981664 -4.1759806][-4.2254658 -4.2363763 -4.1858206 -4.0741873 -3.9351656 -3.7880712 -3.6870663 -3.7123387 -3.8310707 -3.9510717 -4.046875 -4.1299582 -4.179522 -4.1987114 -4.2010465][-4.1959233 -4.2090559 -4.1638784 -4.06378 -3.9351926 -3.8046014 -3.7306106 -3.7611241 -3.8559389 -3.94562 -4.0272741 -4.1085329 -4.1725073 -4.2121282 -4.2294888][-4.1919312 -4.2019143 -4.1653008 -4.0872512 -3.9849412 -3.8866556 -3.8392606 -3.863431 -3.9300594 -3.9976654 -4.0662942 -4.1316566 -4.189815 -4.2330856 -4.2561646][-4.2093725 -4.2141185 -4.1816869 -4.1234183 -4.0490847 -3.9805133 -3.9485559 -3.9581099 -4.0007434 -4.0540562 -4.1136394 -4.1635013 -4.2080235 -4.2424397 -4.263876][-4.2378397 -4.2391105 -4.2073078 -4.1581359 -4.1010718 -4.0524292 -4.0334864 -4.0390725 -4.0656309 -4.1052532 -4.1503086 -4.1809783 -4.2110538 -4.2312293 -4.244813][-4.2642531 -4.2627225 -4.2349253 -4.1952848 -4.1503267 -4.1145816 -4.1017056 -4.1027503 -4.1179452 -4.1474462 -4.1809998 -4.2031379 -4.2213945 -4.2245255 -4.2286611][-4.2813253 -4.2815142 -4.2628341 -4.2358327 -4.2049966 -4.1799836 -4.1689725 -4.1633391 -4.1665177 -4.1875858 -4.2168584 -4.23673 -4.2459397 -4.2419038 -4.2420912][-4.2912621 -4.2960448 -4.2886934 -4.2752161 -4.2587929 -4.2438612 -4.2357235 -4.2301016 -4.2325964 -4.24586 -4.2668128 -4.2824864 -4.2865949 -4.2820611 -4.2806797]]...]
INFO - root - 2017-12-08 01:04:57.223372: step 74710, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 45h:21m:30s remains)
INFO - root - 2017-12-08 01:05:04.043860: step 74720, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 45h:44m:42s remains)
INFO - root - 2017-12-08 01:05:10.946026: step 74730, loss = 2.04, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 53h:01m:11s remains)
INFO - root - 2017-12-08 01:05:17.798644: step 74740, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 52h:10m:54s remains)
INFO - root - 2017-12-08 01:05:24.479653: step 74750, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 48h:06m:28s remains)
INFO - root - 2017-12-08 01:05:31.245794: step 74760, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 44h:25m:12s remains)
INFO - root - 2017-12-08 01:05:38.005653: step 74770, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 46h:38m:03s remains)
INFO - root - 2017-12-08 01:05:44.833181: step 74780, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 52h:07m:27s remains)
INFO - root - 2017-12-08 01:05:51.618818: step 74790, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 50h:58m:19s remains)
INFO - root - 2017-12-08 01:05:58.240307: step 74800, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 48h:50m:24s remains)
2017-12-08 01:05:58.945412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.279139 -4.2775631 -4.275609 -4.2694268 -4.2692542 -4.2706137 -4.2705936 -4.263474 -4.2326345 -4.1835995 -4.1532273 -4.1677957 -4.1997666 -4.2232523 -4.2390785][-4.2772222 -4.2756777 -4.2770448 -4.2754049 -4.278698 -4.2815118 -4.2778478 -4.26609 -4.2359715 -4.1850319 -4.1479974 -4.1574564 -4.1844211 -4.2055669 -4.2205853][-4.2628675 -4.2649837 -4.2689877 -4.2675858 -4.265121 -4.2622933 -4.2541533 -4.2410526 -4.21472 -4.1729813 -4.1448479 -4.1585217 -4.1839 -4.2031841 -4.2162881][-4.2525959 -4.2624149 -4.2647471 -4.25412 -4.2371469 -4.2216539 -4.207191 -4.1957326 -4.1796832 -4.1605296 -4.1570716 -4.1832819 -4.20427 -4.2142396 -4.2234688][-4.2394214 -4.2515049 -4.2540717 -4.2342386 -4.1986227 -4.1613564 -4.1351185 -4.1280584 -4.1236906 -4.1288848 -4.1536455 -4.1940994 -4.2183003 -4.2277036 -4.2308903][-4.2381678 -4.2424307 -4.2291207 -4.1920519 -4.1327128 -4.0629096 -4.0250006 -4.0250788 -4.0297627 -4.0600119 -4.1182027 -4.1851907 -4.2243176 -4.2390518 -4.24013][-4.2375073 -4.2247443 -4.183526 -4.1144638 -4.0162892 -3.9041176 -3.8650997 -3.886009 -3.9044375 -3.9594116 -4.0523739 -4.1504369 -4.2026939 -4.2238531 -4.23037][-4.2446413 -4.2138767 -4.147738 -4.0525556 -3.9322183 -3.8058097 -3.7869418 -3.8302393 -3.8537865 -3.9156725 -4.0209346 -4.1286287 -4.188992 -4.2159882 -4.2288742][-4.2603908 -4.2272363 -4.1628356 -4.0726709 -3.9656439 -3.8740382 -3.8866146 -3.922173 -3.9231081 -3.9584017 -4.044631 -4.1400194 -4.196548 -4.224494 -4.2399259][-4.2596183 -4.2362204 -4.1913929 -4.1270046 -4.0497012 -4.0037861 -4.0338354 -4.0512657 -4.0328965 -4.0388904 -4.0934534 -4.1626916 -4.2062211 -4.2293372 -4.2412992][-4.2481818 -4.2390966 -4.2164116 -4.1812229 -4.1403203 -4.1266232 -4.1601949 -4.1683173 -4.1425576 -4.1297121 -4.1536865 -4.1889381 -4.2133331 -4.2266908 -4.2339377][-4.2450848 -4.2392831 -4.2274323 -4.2097616 -4.1942596 -4.1981583 -4.227407 -4.2352152 -4.2215786 -4.2125649 -4.2184153 -4.225903 -4.2284636 -4.228436 -4.2296333][-4.2554464 -4.250752 -4.2458625 -4.2398005 -4.236928 -4.2430477 -4.26065 -4.2678275 -4.2645874 -4.2604971 -4.2574344 -4.2517352 -4.24533 -4.2413282 -4.2412052][-4.273787 -4.2748737 -4.2758818 -4.2756915 -4.2766037 -4.2820477 -4.2910137 -4.2945843 -4.2941866 -4.2927351 -4.2865005 -4.2747254 -4.26616 -4.2629786 -4.264596][-4.2813306 -4.2858663 -4.2900434 -4.2938342 -4.2981205 -4.3046689 -4.3113346 -4.3141203 -4.3125916 -4.3080812 -4.2979608 -4.2826743 -4.2720752 -4.2706213 -4.2749262]]...]
INFO - root - 2017-12-08 01:06:05.873171: step 74810, loss = 2.10, batch loss = 2.04 (10.7 examples/sec; 0.746 sec/batch; 53h:22m:00s remains)
INFO - root - 2017-12-08 01:06:12.673042: step 74820, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 48h:50m:43s remains)
INFO - root - 2017-12-08 01:06:19.395758: step 74830, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.627 sec/batch; 44h:54m:16s remains)
INFO - root - 2017-12-08 01:06:26.126243: step 74840, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 47h:19m:06s remains)
INFO - root - 2017-12-08 01:06:32.870627: step 74850, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 49h:49m:21s remains)
INFO - root - 2017-12-08 01:06:39.716437: step 74860, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.736 sec/batch; 52h:39m:25s remains)
INFO - root - 2017-12-08 01:06:46.441111: step 74870, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 47h:34m:31s remains)
INFO - root - 2017-12-08 01:06:53.148420: step 74880, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 45h:39m:29s remains)
INFO - root - 2017-12-08 01:06:59.860198: step 74890, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 46h:57m:25s remains)
INFO - root - 2017-12-08 01:07:06.570143: step 74900, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 52h:24m:22s remains)
2017-12-08 01:07:07.299277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2083349 -4.1952133 -4.1968107 -4.1950583 -4.1894016 -4.18473 -4.1780295 -4.1690445 -4.16703 -4.1754808 -4.1917191 -4.2127371 -4.2233186 -4.2150722 -4.1948423][-4.2254376 -4.212934 -4.2119112 -4.2074 -4.1967187 -4.1875768 -4.1797338 -4.1793509 -4.18621 -4.1976175 -4.2113285 -4.2268806 -4.2344208 -4.2249336 -4.2076335][-4.2248235 -4.2129979 -4.2071967 -4.1997194 -4.1885214 -4.1824183 -4.1822882 -4.1949043 -4.207407 -4.2163396 -4.226213 -4.240766 -4.2492576 -4.243381 -4.2325249][-4.1938004 -4.1831379 -4.1717877 -4.1648912 -4.1604385 -4.1616235 -4.1739464 -4.1949754 -4.2077341 -4.2147622 -4.2254963 -4.242487 -4.2547326 -4.2548828 -4.2519479][-4.1527538 -4.1456032 -4.13632 -4.1328735 -4.1320772 -4.1304355 -4.1361046 -4.1485162 -4.1561551 -4.1725292 -4.199234 -4.2281203 -4.2475309 -4.2552962 -4.2608128][-4.1389971 -4.133502 -4.123951 -4.1144176 -4.1056175 -4.0874453 -4.0733652 -4.06461 -4.0599141 -4.0859046 -4.1328187 -4.1813087 -4.220192 -4.2438226 -4.2601357][-4.1524258 -4.1406603 -4.1242204 -4.099381 -4.0774097 -4.0408096 -3.9989161 -3.9512985 -3.9279513 -3.974581 -4.0541081 -4.1325488 -4.1954231 -4.230967 -4.2541924][-4.1807804 -4.1606193 -4.1370821 -4.10303 -4.0723867 -4.0225778 -3.9669013 -3.9048765 -3.8837767 -3.9549372 -4.0514359 -4.136961 -4.1990438 -4.2318234 -4.2500892][-4.2120934 -4.1868649 -4.1634741 -4.13657 -4.1157794 -4.0858397 -4.05302 -4.012033 -4.0069704 -4.064847 -4.1306252 -4.1884127 -4.2287121 -4.2461457 -4.25436][-4.2259927 -4.1975679 -4.181006 -4.1715646 -4.1672349 -4.1612711 -4.1498551 -4.1272869 -4.13035 -4.1679754 -4.2030334 -4.2307892 -4.2493339 -4.2572641 -4.2628269][-4.2339683 -4.2074161 -4.1918459 -4.1871676 -4.1901627 -4.1936483 -4.1936369 -4.182291 -4.1874681 -4.2149725 -4.2371798 -4.2470942 -4.2500215 -4.2508426 -4.256228][-4.2361584 -4.2161317 -4.2038512 -4.2043114 -4.2106805 -4.2180386 -4.2242088 -4.2174096 -4.2149606 -4.2294188 -4.2373118 -4.2313247 -4.2233663 -4.2153897 -4.2185378][-4.2226062 -4.2195549 -4.2216983 -4.2298689 -4.2395391 -4.2442951 -4.2431564 -4.2299094 -4.2104526 -4.2064409 -4.2061667 -4.1953006 -4.1854725 -4.1740317 -4.1736093][-4.2148056 -4.2260227 -4.2346067 -4.2366786 -4.2414689 -4.2448878 -4.2419949 -4.2264371 -4.2011056 -4.189599 -4.1895962 -4.1852579 -4.1806331 -4.16603 -4.1530228][-4.2113047 -4.22236 -4.224009 -4.2130857 -4.2061639 -4.2106414 -4.2130394 -4.2043395 -4.1861734 -4.1781607 -4.1822243 -4.1866961 -4.186276 -4.1699734 -4.1462207]]...]
INFO - root - 2017-12-08 01:07:14.121057: step 74910, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 46h:17m:18s remains)
INFO - root - 2017-12-08 01:07:20.885115: step 74920, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 46h:06m:19s remains)
INFO - root - 2017-12-08 01:07:27.806026: step 74930, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.749 sec/batch; 53h:35m:57s remains)
INFO - root - 2017-12-08 01:07:34.694104: step 74940, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.748 sec/batch; 53h:32m:03s remains)
INFO - root - 2017-12-08 01:07:41.550905: step 74950, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 49h:06m:30s remains)
INFO - root - 2017-12-08 01:07:48.306283: step 74960, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 45h:41m:46s remains)
INFO - root - 2017-12-08 01:07:55.150731: step 74970, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 47h:16m:38s remains)
INFO - root - 2017-12-08 01:08:01.954120: step 74980, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 51h:57m:20s remains)
INFO - root - 2017-12-08 01:08:08.807005: step 74990, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 51h:47m:48s remains)
INFO - root - 2017-12-08 01:08:15.243892: step 75000, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.641 sec/batch; 45h:50m:43s remains)
2017-12-08 01:08:15.988238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3116932 -4.2995906 -4.2886639 -4.2756925 -4.267487 -4.26623 -4.2675347 -4.2725143 -4.2810755 -4.289371 -4.2925091 -4.293045 -4.293396 -4.2910943 -4.286067][-4.2939925 -4.2785544 -4.26749 -4.2557297 -4.2515192 -4.2540579 -4.2561779 -4.25703 -4.2607007 -4.2681608 -4.2745972 -4.278492 -4.2813077 -4.2836876 -4.2783175][-4.2753205 -4.2593141 -4.2489853 -4.2409945 -4.2434349 -4.2508154 -4.2535315 -4.2511134 -4.2535524 -4.25968 -4.2694125 -4.2793832 -4.2840519 -4.2876573 -4.2803917][-4.2467275 -4.2304592 -4.2231321 -4.2191677 -4.2273235 -4.2376757 -4.2400427 -4.2360544 -4.2436552 -4.2534828 -4.2675409 -4.28194 -4.2840366 -4.2860718 -4.2758265][-4.2211657 -4.2051983 -4.2011909 -4.19812 -4.2066617 -4.2152534 -4.2132692 -4.20227 -4.218606 -4.2376442 -4.2564287 -4.2742181 -4.2761426 -4.2755938 -4.2636418][-4.2026215 -4.1850591 -4.1817718 -4.1727471 -4.1737475 -4.1736779 -4.1568313 -4.1296577 -4.1511707 -4.1835265 -4.2083406 -4.2334876 -4.244668 -4.2499714 -4.2451148][-4.1872125 -4.1633172 -4.153285 -4.1340485 -4.119771 -4.1024027 -4.0591016 -4.0030541 -4.0304489 -4.0894995 -4.129941 -4.1661921 -4.1915336 -4.2088795 -4.2171683][-4.1769028 -4.1480336 -4.1300282 -4.1016541 -4.0728765 -4.0425715 -3.9785497 -3.9027829 -3.9435258 -4.0281806 -4.0784407 -4.1189914 -4.1496196 -4.1696553 -4.1803784][-4.1815939 -4.1531405 -4.1310048 -4.1026158 -4.0762234 -4.0491209 -3.9988058 -3.9463282 -3.9839602 -4.047533 -4.0811138 -4.1116567 -4.137444 -4.150054 -4.1533151][-4.1999836 -4.1795225 -4.1578836 -4.1347418 -4.1195989 -4.102282 -4.0671859 -4.0368123 -4.0626411 -4.09558 -4.1061959 -4.1241894 -4.1448941 -4.1506023 -4.1498895][-4.2289715 -4.21313 -4.191267 -4.1692624 -4.1580997 -4.1455193 -4.1202016 -4.1016603 -4.1186523 -4.1283994 -4.1228833 -4.133873 -4.1520987 -4.1580362 -4.1574435][-4.2605872 -4.2484269 -4.2259488 -4.2024226 -4.1914744 -4.1830578 -4.1649647 -4.152678 -4.1659923 -4.1649666 -4.1528716 -4.1603918 -4.1763382 -4.1833839 -4.1848788][-4.283927 -4.2770486 -4.2612867 -4.2417827 -4.2316084 -4.2268314 -4.2154789 -4.2084713 -4.2208085 -4.2198329 -4.209188 -4.213665 -4.2260437 -4.2323704 -4.2344422][-4.2978177 -4.2931018 -4.2860494 -4.2774673 -4.2722397 -4.2704163 -4.2654977 -4.2627835 -4.2727995 -4.2730279 -4.2644553 -4.2651024 -4.2708979 -4.2764535 -4.2803636][-4.3076692 -4.3029675 -4.3025413 -4.3018241 -4.299274 -4.2967086 -4.2947125 -4.2933717 -4.2982831 -4.2974839 -4.2922039 -4.2909627 -4.2930717 -4.2966127 -4.301548]]...]
INFO - root - 2017-12-08 01:08:22.688260: step 75010, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.661 sec/batch; 47h:15m:44s remains)
INFO - root - 2017-12-08 01:08:29.477484: step 75020, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:52m:35s remains)
INFO - root - 2017-12-08 01:08:36.204811: step 75030, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.611 sec/batch; 43h:40m:34s remains)
INFO - root - 2017-12-08 01:08:42.976181: step 75040, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 47h:10m:50s remains)
INFO - root - 2017-12-08 01:08:49.683359: step 75050, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 51h:15m:10s remains)
INFO - root - 2017-12-08 01:08:56.479005: step 75060, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 51h:58m:26s remains)
INFO - root - 2017-12-08 01:09:03.264778: step 75070, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 46h:50m:42s remains)
INFO - root - 2017-12-08 01:09:10.078696: step 75080, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 44h:09m:06s remains)
INFO - root - 2017-12-08 01:09:16.884238: step 75090, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 45h:44m:23s remains)
INFO - root - 2017-12-08 01:09:23.554337: step 75100, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 51h:28m:35s remains)
2017-12-08 01:09:24.278031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2816391 -4.3112054 -4.3386831 -4.3547707 -4.3576632 -4.3563595 -4.3533406 -4.3512616 -4.3414407 -4.3159928 -4.2894435 -4.2693748 -4.2684703 -4.2789712 -4.3009658][-4.3458424 -4.3615561 -4.3721771 -4.3751454 -4.369616 -4.3601336 -4.3511381 -4.3437757 -4.3297653 -4.3036995 -4.2827072 -4.2691951 -4.27234 -4.2825141 -4.3038239][-4.3729424 -4.3766909 -4.3774552 -4.3748045 -4.36439 -4.3485017 -4.3346148 -4.3227067 -4.3064985 -4.2823086 -4.2678561 -4.2629609 -4.2708139 -4.282937 -4.3048515][-4.3716822 -4.3681841 -4.3649683 -4.3593826 -4.3454571 -4.3240485 -4.30505 -4.2882214 -4.2714562 -4.2521706 -4.2462277 -4.2498431 -4.2635131 -4.2807827 -4.3044548][-4.3541775 -4.3455791 -4.3396192 -4.3292093 -4.309269 -4.2804809 -4.2537413 -4.2316179 -4.2153358 -4.2023416 -4.2058997 -4.2211404 -4.2457705 -4.2714524 -4.2985473][-4.32196 -4.3080106 -4.2979527 -4.2830915 -4.2581644 -4.2222061 -4.1875834 -4.1605363 -4.1424761 -4.1355476 -4.1489072 -4.180337 -4.2180567 -4.2546453 -4.287127][-4.2783151 -4.2591038 -4.2451696 -4.2247739 -4.1901083 -4.1458836 -4.105381 -4.0778608 -4.0652089 -4.0723581 -4.1035495 -4.1525655 -4.1989851 -4.2403679 -4.2715435][-4.2320738 -4.2098951 -4.1919312 -4.1626368 -4.1163239 -4.0648327 -4.0265265 -4.0088396 -4.0114589 -4.0384936 -4.0869431 -4.1404862 -4.1837792 -4.2181354 -4.2409868][-4.20871 -4.1846352 -4.1614323 -4.125134 -4.0759139 -4.0278664 -3.9991012 -3.9945676 -4.0111666 -4.0462341 -4.0895581 -4.12809 -4.1566114 -4.1789947 -4.191721][-4.2190695 -4.1984048 -4.174511 -4.139874 -4.0986633 -4.0602527 -4.0404119 -4.0388823 -4.0531812 -4.08006 -4.10994 -4.1334596 -4.1491694 -4.1616311 -4.167326][-4.2569242 -4.2442541 -4.2276378 -4.2047687 -4.1800132 -4.153728 -4.134604 -4.1249509 -4.1247058 -4.1344366 -4.1516023 -4.1678309 -4.1805935 -4.1909547 -4.1945205][-4.301764 -4.2945862 -4.2841177 -4.2706194 -4.2573652 -4.2430696 -4.2296004 -4.2167406 -4.2070541 -4.2058496 -4.2148409 -4.2273445 -4.24013 -4.2491736 -4.2507062][-4.3330169 -4.3275061 -4.3202724 -4.312089 -4.3065472 -4.3020635 -4.29742 -4.2907376 -4.2836809 -4.2821875 -4.2874241 -4.2945395 -4.3016644 -4.30546 -4.3033772][-4.35627 -4.3546233 -4.3518281 -4.348299 -4.3457918 -4.3434658 -4.3394356 -4.3331656 -4.3277793 -4.3272333 -4.3309312 -4.3331389 -4.3340406 -4.3337984 -4.3304539][-4.37085 -4.3716035 -4.3718824 -4.3721647 -4.3719311 -4.3696404 -4.3643661 -4.3569069 -4.3504224 -4.3481021 -4.3500347 -4.3520279 -4.3528986 -4.352385 -4.349874]]...]
INFO - root - 2017-12-08 01:09:31.039205: step 75110, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 46h:14m:46s remains)
INFO - root - 2017-12-08 01:09:37.884693: step 75120, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 46h:32m:12s remains)
INFO - root - 2017-12-08 01:09:44.709059: step 75130, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 51h:52m:44s remains)
INFO - root - 2017-12-08 01:09:51.551798: step 75140, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 50h:10m:00s remains)
INFO - root - 2017-12-08 01:09:58.371486: step 75150, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 49h:23m:29s remains)
INFO - root - 2017-12-08 01:10:05.180027: step 75160, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 46h:47m:48s remains)
INFO - root - 2017-12-08 01:10:12.091649: step 75170, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 50h:18m:43s remains)
INFO - root - 2017-12-08 01:10:18.951733: step 75180, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 52h:10m:22s remains)
INFO - root - 2017-12-08 01:10:25.895904: step 75190, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.735 sec/batch; 52h:29m:54s remains)
INFO - root - 2017-12-08 01:10:32.567150: step 75200, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 47h:56m:49s remains)
2017-12-08 01:10:33.324213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2427821 -4.2487841 -4.2676148 -4.2813773 -4.2889795 -4.2848492 -4.2763562 -4.2712831 -4.2671719 -4.2689929 -4.2753386 -4.2809763 -4.28343 -4.2760887 -4.2654428][-4.2365704 -4.2464747 -4.2722845 -4.2924972 -4.3046994 -4.3014507 -4.290102 -4.2818761 -4.277916 -4.28113 -4.2911119 -4.3010845 -4.3036919 -4.2935615 -4.2789493][-4.232131 -4.2426105 -4.2688389 -4.2920985 -4.3070273 -4.30716 -4.2986465 -4.2921667 -4.2879286 -4.2871385 -4.2918997 -4.2978468 -4.299119 -4.2927113 -4.2840557][-4.2318196 -4.240294 -4.2611918 -4.2836547 -4.2989578 -4.302177 -4.2975311 -4.293376 -4.2890563 -4.283752 -4.2819405 -4.283864 -4.2845879 -4.2790341 -4.2702546][-4.2357316 -4.2383723 -4.2516203 -4.2680578 -4.278512 -4.2789407 -4.2720423 -4.2684131 -4.2673798 -4.2634153 -4.2603426 -4.2594905 -4.2544036 -4.2407713 -4.2263932][-4.2443275 -4.2395606 -4.2445512 -4.2467685 -4.240006 -4.2222433 -4.197628 -4.1872725 -4.1942606 -4.2067528 -4.2170806 -4.2174792 -4.2042952 -4.183589 -4.1690292][-4.2496982 -4.2365294 -4.2285805 -4.2104206 -4.1742144 -4.1184087 -4.0602856 -4.0448089 -4.0793133 -4.128746 -4.1626577 -4.1697836 -4.1540356 -4.1363368 -4.1322589][-4.2446766 -4.2227769 -4.2022352 -4.1685853 -4.110321 -4.0222015 -3.9344721 -3.9251294 -4.0048423 -4.0936322 -4.1431623 -4.1523857 -4.1361418 -4.1218657 -4.1235285][-4.232255 -4.2114716 -4.1907558 -4.1581345 -4.1011949 -4.0211186 -3.9529345 -3.9659371 -4.0521951 -4.1359005 -4.1763225 -4.1764989 -4.1532664 -4.1349607 -4.1363416][-4.2195129 -4.2073469 -4.1972909 -4.1805272 -4.1500731 -4.1126776 -4.0871916 -4.1044445 -4.1572838 -4.2093577 -4.2317638 -4.2216973 -4.1939445 -4.1727977 -4.171186][-4.1993976 -4.1955318 -4.2009511 -4.2069793 -4.2067366 -4.2035022 -4.2006869 -4.2114172 -4.2331371 -4.2566085 -4.265099 -4.2530518 -4.2289219 -4.210146 -4.2060637][-4.1636691 -4.1647582 -4.1861587 -4.2156844 -4.2378631 -4.2521515 -4.255764 -4.2597156 -4.2633014 -4.2676888 -4.2677622 -4.2603431 -4.245122 -4.2294316 -4.2215891][-4.1358724 -4.1402826 -4.1728764 -4.2175426 -4.2493639 -4.2663231 -4.2690225 -4.26807 -4.2636967 -4.260613 -4.2585163 -4.2569733 -4.2488236 -4.2345066 -4.2278743][-4.1426654 -4.1518283 -4.1864772 -4.2289238 -4.2558303 -4.26561 -4.2637095 -4.2611542 -4.2551622 -4.2502956 -4.2513657 -4.2534881 -4.2479405 -4.2355318 -4.2350969][-4.1674504 -4.1791244 -4.2104721 -4.2393832 -4.2553735 -4.2605877 -4.2600927 -4.25981 -4.2527227 -4.2458115 -4.2459493 -4.2475061 -4.2443132 -4.237411 -4.2449074]]...]
INFO - root - 2017-12-08 01:10:40.166372: step 75210, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 52h:15m:56s remains)
INFO - root - 2017-12-08 01:10:47.006479: step 75220, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 50h:10m:34s remains)
INFO - root - 2017-12-08 01:10:53.817291: step 75230, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 46h:54m:15s remains)
INFO - root - 2017-12-08 01:11:00.663081: step 75240, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 47h:06m:41s remains)
INFO - root - 2017-12-08 01:11:07.509475: step 75250, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 51h:46m:27s remains)
INFO - root - 2017-12-08 01:11:14.320361: step 75260, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 53h:15m:41s remains)
INFO - root - 2017-12-08 01:11:21.140469: step 75270, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 48h:10m:30s remains)
INFO - root - 2017-12-08 01:11:27.933113: step 75280, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 46h:01m:44s remains)
INFO - root - 2017-12-08 01:11:34.755742: step 75290, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 49h:29m:58s remains)
INFO - root - 2017-12-08 01:11:41.489313: step 75300, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 50h:08m:39s remains)
2017-12-08 01:11:42.234763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2692194 -4.2651744 -4.2625494 -4.2584 -4.2553267 -4.2531343 -4.2498593 -4.246769 -4.2448168 -4.2416945 -4.2375989 -4.2330165 -4.2234511 -4.1992097 -4.1685758][-4.2703428 -4.26243 -4.2569666 -4.2505064 -4.2466521 -4.2449942 -4.24164 -4.2384377 -4.2361684 -4.2318888 -4.2263269 -4.2200189 -4.2082953 -4.1800108 -4.146143][-4.2678432 -4.2562175 -4.2485752 -4.2421947 -4.2392092 -4.2383776 -4.2355857 -4.2334275 -4.2341766 -4.2346683 -4.2338109 -4.2301979 -4.2197328 -4.19227 -4.1602569][-4.2378912 -4.2214766 -4.2106743 -4.2057743 -4.2048488 -4.2035918 -4.2004929 -4.19947 -4.2068443 -4.2163687 -4.2226472 -4.22427 -4.2193274 -4.2012539 -4.17955][-4.1991715 -4.1777086 -4.1611695 -4.1557355 -4.1545606 -4.1482677 -4.1406817 -4.1420088 -4.1588855 -4.1769896 -4.1890984 -4.1959357 -4.2004089 -4.1989751 -4.1940494][-4.1686177 -4.1449194 -4.123651 -4.1158605 -4.1083045 -4.0860524 -4.065042 -4.0715475 -4.1032887 -4.1302052 -4.1454449 -4.1553326 -4.1694689 -4.1838894 -4.1947641][-4.1455164 -4.1275277 -4.1050472 -4.0887117 -4.061357 -4.0053697 -3.9559085 -3.9711394 -4.029273 -4.0747418 -4.0991497 -4.1151934 -4.137701 -4.1622052 -4.1822405][-4.1094365 -4.1108747 -4.097724 -4.0765977 -4.0277157 -3.938062 -3.864819 -3.8939977 -3.9776754 -4.0379457 -4.0687013 -4.0903597 -4.1197319 -4.148828 -4.1710448][-4.0703063 -4.0954189 -4.1032381 -4.0976439 -4.0618591 -3.9889679 -3.9326386 -3.95628 -4.0216627 -4.0658627 -4.0861554 -4.1058407 -4.135498 -4.1624274 -4.1793041][-4.0676675 -4.1036248 -4.1302991 -4.1460485 -4.1354074 -4.096168 -4.0636635 -4.0720372 -4.1038733 -4.1242571 -4.1325436 -4.147788 -4.1727729 -4.1915193 -4.1977611][-4.1046672 -4.1362567 -4.1689119 -4.1941671 -4.1973863 -4.1819496 -4.168107 -4.1693969 -4.1791906 -4.1860814 -4.1898155 -4.2004046 -4.21555 -4.2228837 -4.2184014][-4.152864 -4.1743212 -4.2031436 -4.228085 -4.2388949 -4.2406287 -4.2416224 -4.2430968 -4.2425656 -4.2433529 -4.2432456 -4.24531 -4.2472982 -4.2417789 -4.2304931][-4.2043347 -4.2193594 -4.2396908 -4.25573 -4.2645059 -4.2697244 -4.27507 -4.2766886 -4.2750349 -4.2745795 -4.2725348 -4.2690783 -4.2610559 -4.2464271 -4.2319613][-4.2479658 -4.2568293 -4.2672405 -4.2745271 -4.2780995 -4.2801871 -4.2834573 -4.2838392 -4.281795 -4.2812624 -4.2799497 -4.2772937 -4.2671785 -4.2500319 -4.236002][-4.2774658 -4.2802415 -4.2834897 -4.2853484 -4.2856131 -4.28516 -4.2849059 -4.283103 -4.2803555 -4.2791529 -4.2779455 -4.277072 -4.2696242 -4.2562709 -4.2461343]]...]
INFO - root - 2017-12-08 01:11:48.934908: step 75310, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 51h:49m:40s remains)
INFO - root - 2017-12-08 01:11:55.840490: step 75320, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 53h:12m:40s remains)
INFO - root - 2017-12-08 01:12:02.743931: step 75330, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 48h:16m:54s remains)
INFO - root - 2017-12-08 01:12:09.415947: step 75340, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.617 sec/batch; 44h:04m:48s remains)
INFO - root - 2017-12-08 01:12:16.230390: step 75350, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 49h:15m:05s remains)
INFO - root - 2017-12-08 01:12:23.071745: step 75360, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 49h:43m:53s remains)
INFO - root - 2017-12-08 01:12:29.920700: step 75370, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 48h:46m:09s remains)
INFO - root - 2017-12-08 01:12:36.700415: step 75380, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.615 sec/batch; 43h:55m:29s remains)
INFO - root - 2017-12-08 01:12:43.537108: step 75390, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 45h:39m:36s remains)
INFO - root - 2017-12-08 01:12:50.310553: step 75400, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 52h:59m:00s remains)
2017-12-08 01:12:50.993143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3213954 -4.3228121 -4.3264103 -4.3304963 -4.334888 -4.3344154 -4.3290257 -4.3210087 -4.315104 -4.3147058 -4.322504 -4.3337445 -4.344624 -4.3548284 -4.3624444][-4.2879577 -4.2894216 -4.2921748 -4.2946286 -4.2996845 -4.3002639 -4.2910528 -4.2788606 -4.2725282 -4.2748613 -4.2869153 -4.3041072 -4.32112 -4.3377614 -4.3521814][-4.2379036 -4.2428508 -4.2472014 -4.2461796 -4.2473392 -4.2435584 -4.228014 -4.2096534 -4.2023406 -4.2113214 -4.2354727 -4.263967 -4.2894363 -4.3118019 -4.3329506][-4.1791725 -4.1934361 -4.2011051 -4.1942549 -4.1835585 -4.1653605 -4.1383634 -4.1117921 -4.1052117 -4.130023 -4.175077 -4.2179003 -4.251193 -4.2800589 -4.3072033][-4.1247497 -4.1522269 -4.1667652 -4.1505685 -4.1179209 -4.0753527 -4.0351191 -4.0053959 -4.0059643 -4.0527272 -4.1247139 -4.1802764 -4.2174511 -4.2488303 -4.2798243][-4.0773277 -4.1176457 -4.1364465 -4.11141 -4.0612607 -3.9976113 -3.9471333 -3.9201236 -3.9281235 -3.9907024 -4.08504 -4.1528082 -4.1910486 -4.2219653 -4.2541547][-4.0496593 -4.0907779 -4.1096354 -4.0863152 -4.0362458 -3.9732447 -3.9231436 -3.8988731 -3.9067006 -3.9701455 -4.0673504 -4.1404786 -4.1816216 -4.2113438 -4.2432747][-4.0490675 -4.075182 -4.0904469 -4.0844712 -4.0556631 -4.0120354 -3.9732199 -3.948271 -3.9469702 -3.9974103 -4.0813808 -4.1500969 -4.1898932 -4.2154927 -4.2475605][-4.0750728 -4.0820541 -4.0945473 -4.1080585 -4.1040659 -4.0844193 -4.0595751 -4.0366993 -4.0235395 -4.0587311 -4.1255894 -4.1841211 -4.2169232 -4.2349916 -4.2637963][-4.1249413 -4.123353 -4.1372867 -4.1626587 -4.1753407 -4.1704407 -4.1571932 -4.1374769 -4.1186242 -4.1418476 -4.1936016 -4.2382245 -4.2622056 -4.2737842 -4.2935081][-4.1863289 -4.1872959 -4.2020926 -4.2255874 -4.2415333 -4.2416797 -4.2340236 -4.2227087 -4.2124257 -4.2337871 -4.2717118 -4.2986121 -4.3129859 -4.3176446 -4.3256693][-4.2471151 -4.2523847 -4.26483 -4.2815146 -4.2943192 -4.2944355 -4.2872581 -4.2795305 -4.2792749 -4.3009887 -4.3293295 -4.34562 -4.3522887 -4.3514757 -4.3495669][-4.292933 -4.2996426 -4.3091812 -4.3200316 -4.3273807 -4.3237305 -4.314671 -4.3087 -4.3145514 -4.3335328 -4.3558617 -4.3679671 -4.3710461 -4.3687782 -4.3624897][-4.3238783 -4.3289 -4.3334303 -4.3371778 -4.3385572 -4.3328381 -4.3256049 -4.3243184 -4.3325815 -4.3471603 -4.3638835 -4.3745084 -4.377337 -4.3749142 -4.3678942][-4.3427944 -4.3454285 -4.3462706 -4.3459406 -4.3445067 -4.3392687 -4.3348536 -4.3363147 -4.3434811 -4.3535104 -4.3654575 -4.3738313 -4.3758178 -4.3737078 -4.3684869]]...]
INFO - root - 2017-12-08 01:12:57.762747: step 75410, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 46h:30m:52s remains)
INFO - root - 2017-12-08 01:13:04.673387: step 75420, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 48h:30m:32s remains)
INFO - root - 2017-12-08 01:13:11.523738: step 75430, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 51h:50m:14s remains)
INFO - root - 2017-12-08 01:13:18.373641: step 75440, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 52h:57m:52s remains)
INFO - root - 2017-12-08 01:13:25.260964: step 75450, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 48h:53m:55s remains)
INFO - root - 2017-12-08 01:13:32.009459: step 75460, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 44h:58m:13s remains)
INFO - root - 2017-12-08 01:13:38.820707: step 75470, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 47h:16m:38s remains)
INFO - root - 2017-12-08 01:13:45.636668: step 75480, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 50h:38m:57s remains)
INFO - root - 2017-12-08 01:13:52.397490: step 75490, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 49h:03m:34s remains)
INFO - root - 2017-12-08 01:13:58.979902: step 75500, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 46h:18m:32s remains)
2017-12-08 01:13:59.721730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3011231 -4.3014307 -4.2958374 -4.2853179 -4.2702303 -4.2587147 -4.2592392 -4.2662344 -4.2744694 -4.2819762 -4.28686 -4.2801633 -4.2682581 -4.2567573 -4.2453723][-4.2987547 -4.2970428 -4.2845778 -4.2631884 -4.2373691 -4.2192726 -4.21822 -4.2274122 -4.2418547 -4.2596941 -4.2802315 -4.2876186 -4.2857718 -4.2796192 -4.2721515][-4.2802343 -4.2727494 -4.2498064 -4.2191916 -4.1822972 -4.1541595 -4.14444 -4.147181 -4.1680441 -4.1993918 -4.2389827 -4.2673712 -4.2838211 -4.292491 -4.2955441][-4.2412724 -4.2310963 -4.2016888 -4.1654391 -4.122251 -4.0776029 -4.045197 -4.0297341 -4.0598874 -4.116127 -4.1755457 -4.2225266 -4.2573705 -4.2847114 -4.2971597][-4.1794605 -4.1672239 -4.1357036 -4.10055 -4.0558505 -3.9910738 -3.9187331 -3.8760006 -3.9193661 -4.0151491 -4.103086 -4.1677084 -4.2163863 -4.2610765 -4.28468][-4.0969009 -4.0770531 -4.0470581 -4.0254955 -3.9893818 -3.9117916 -3.7926919 -3.7007456 -3.755729 -3.8974829 -4.0175257 -4.097868 -4.1544251 -4.2122879 -4.2515955][-4.0110517 -3.9870534 -3.9693031 -3.9687436 -3.9453437 -3.8673959 -3.7178955 -3.5791771 -3.6322453 -3.808531 -3.9423504 -4.01791 -4.0689726 -4.1359243 -4.1920938][-3.9743557 -3.9552624 -3.9557512 -3.970463 -3.9574463 -3.9033091 -3.7830906 -3.6621096 -3.6910741 -3.8227053 -3.9172752 -3.9656618 -4.0020604 -4.0729308 -4.14109][-3.9789686 -3.9729502 -3.9824519 -3.9931047 -3.9887795 -3.968785 -3.9060202 -3.8339822 -3.8363056 -3.8888519 -3.922688 -3.9409561 -3.97685 -4.0555553 -4.1252685][-4.0332623 -4.0345912 -4.0475316 -4.0568585 -4.0577712 -4.0524745 -4.0249481 -3.9835739 -3.9688797 -3.9687657 -3.9568634 -3.9574916 -3.9971137 -4.0735903 -4.1354451][-4.130353 -4.1275721 -4.1364841 -4.1461391 -4.1445732 -4.1349006 -4.1174021 -4.0938649 -4.0836687 -4.071991 -4.049922 -4.0417571 -4.0700836 -4.1288514 -4.174469][-4.2319517 -4.2248454 -4.2308354 -4.2404885 -4.2342129 -4.2173142 -4.2004542 -4.186625 -4.1860495 -4.1814141 -4.1622806 -4.1477833 -4.160933 -4.1982317 -4.2256632][-4.2942495 -4.2865977 -4.288209 -4.2964745 -4.2953196 -4.2847948 -4.2729158 -4.2642555 -4.2651558 -4.2631626 -4.2482686 -4.2329988 -4.2364063 -4.2543144 -4.2666349][-4.3179216 -4.3099432 -4.3026786 -4.308176 -4.3146143 -4.3178849 -4.314714 -4.3082218 -4.3086467 -4.3092861 -4.2994809 -4.2869086 -4.28528 -4.2915096 -4.2942762][-4.3083277 -4.2974482 -4.2852006 -4.2871284 -4.29762 -4.3146834 -4.3231897 -4.3236408 -4.3310671 -4.3354988 -4.3277802 -4.3181143 -4.3150792 -4.3141832 -4.3100286]]...]
INFO - root - 2017-12-08 01:14:06.480682: step 75510, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 51h:25m:39s remains)
INFO - root - 2017-12-08 01:14:13.276469: step 75520, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 49h:17m:33s remains)
INFO - root - 2017-12-08 01:14:19.941437: step 75530, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 45h:57m:13s remains)
INFO - root - 2017-12-08 01:14:26.679215: step 75540, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 45h:07m:57s remains)
INFO - root - 2017-12-08 01:14:33.417314: step 75550, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 45h:50m:46s remains)
INFO - root - 2017-12-08 01:14:40.231392: step 75560, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 50h:02m:48s remains)
INFO - root - 2017-12-08 01:14:46.980574: step 75570, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 46h:40m:49s remains)
INFO - root - 2017-12-08 01:14:53.757465: step 75580, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 48h:35m:35s remains)
INFO - root - 2017-12-08 01:15:00.529817: step 75590, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 46h:45m:52s remains)
INFO - root - 2017-12-08 01:15:07.168637: step 75600, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 45h:51m:23s remains)
2017-12-08 01:15:07.959407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2771716 -4.2587156 -4.2432685 -4.224978 -4.1933975 -4.1490417 -4.1120381 -4.1076932 -4.13424 -4.1735792 -4.2143488 -4.2468319 -4.2686343 -4.2854247 -4.2990303][-4.3011842 -4.2823062 -4.2584968 -4.2280364 -4.184063 -4.1345148 -4.1021533 -4.1118088 -4.1515746 -4.1975985 -4.2420611 -4.2771711 -4.2970486 -4.308939 -4.3169618][-4.3127141 -4.2991209 -4.2735491 -4.2329607 -4.1781945 -4.1253524 -4.0988579 -4.1182947 -4.1687517 -4.2229009 -4.2724657 -4.3078032 -4.3239956 -4.3282804 -4.3258452][-4.3049812 -4.298811 -4.2778893 -4.23684 -4.1791339 -4.1246738 -4.0979424 -4.1153493 -4.1649404 -4.2228279 -4.2774339 -4.31578 -4.33119 -4.3255568 -4.3066111][-4.2913284 -4.2913613 -4.2772708 -4.2405095 -4.1848712 -4.1288118 -4.0956459 -4.0963559 -4.1306696 -4.1855245 -4.2452927 -4.2903094 -4.3091125 -4.2952747 -4.25797][-4.2758012 -4.2801895 -4.2718611 -4.2394223 -4.1832166 -4.1171131 -4.0612965 -4.0322447 -4.0492682 -4.1032553 -4.1717191 -4.2306314 -4.2577529 -4.2404137 -4.18842][-4.2624598 -4.272789 -4.2675195 -4.23193 -4.1635513 -4.0715332 -3.976104 -3.9129591 -3.92264 -3.9933033 -4.0842943 -4.16655 -4.2074976 -4.1901789 -4.1295576][-4.2558365 -4.2677736 -4.2595248 -4.2153406 -4.1317687 -4.0164175 -3.8905754 -3.8074255 -3.8220212 -3.9128227 -4.0271177 -4.1236978 -4.168406 -4.1505146 -4.0888433][-4.2524428 -4.2569509 -4.2410917 -4.1901422 -4.1046257 -3.9943318 -3.8819017 -3.8190331 -3.8418872 -3.9310141 -4.0385208 -4.1203361 -4.1521873 -4.1328092 -4.0772033][-4.2532783 -4.2487421 -4.2255387 -4.1754861 -4.1047397 -4.0294828 -3.9687684 -3.9477038 -3.9783139 -4.0443888 -4.1161766 -4.1618052 -4.1738877 -4.1552534 -4.1108031][-4.264483 -4.2557931 -4.2276649 -4.1784964 -4.1221871 -4.0796704 -4.0639539 -4.0786242 -4.118154 -4.1636639 -4.199801 -4.2130394 -4.2099261 -4.192986 -4.162467][-4.259697 -4.2612181 -4.2426729 -4.2009521 -4.1539774 -4.1283646 -4.1351848 -4.1663289 -4.2067347 -4.2373724 -4.2508669 -4.2452879 -4.234643 -4.2244644 -4.2120914][-4.2333488 -4.2537193 -4.2541022 -4.2273822 -4.1925416 -4.1737566 -4.1820521 -4.2097039 -4.2415552 -4.2611127 -4.2625504 -4.2529593 -4.2467251 -4.2489552 -4.2526526][-4.1996078 -4.231318 -4.2477059 -4.2382112 -4.2204385 -4.208086 -4.2100887 -4.2247953 -4.2412634 -4.2491646 -4.24592 -4.2425675 -4.250649 -4.2677541 -4.2842994][-4.1619015 -4.1982026 -4.2256231 -4.2332516 -4.2327089 -4.2289562 -4.2254033 -4.2236795 -4.2191195 -4.2119546 -4.206233 -4.2120895 -4.2334256 -4.2612195 -4.2854209]]...]
INFO - root - 2017-12-08 01:15:14.696502: step 75610, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 46h:51m:52s remains)
INFO - root - 2017-12-08 01:15:21.226152: step 75620, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 45h:24m:52s remains)
INFO - root - 2017-12-08 01:15:28.165220: step 75630, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 51h:06m:01s remains)
INFO - root - 2017-12-08 01:15:34.981109: step 75640, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 51h:48m:42s remains)
INFO - root - 2017-12-08 01:15:41.903683: step 75650, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 50h:35m:49s remains)
INFO - root - 2017-12-08 01:15:48.645806: step 75660, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 44h:42m:27s remains)
INFO - root - 2017-12-08 01:15:55.582051: step 75670, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 47h:12m:59s remains)
INFO - root - 2017-12-08 01:16:02.419790: step 75680, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.708 sec/batch; 50h:30m:04s remains)
INFO - root - 2017-12-08 01:16:09.257441: step 75690, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 53h:00m:32s remains)
INFO - root - 2017-12-08 01:16:15.927757: step 75700, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 46h:47m:34s remains)
2017-12-08 01:16:16.661713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981234 -4.2966366 -4.2925663 -4.2881413 -4.2867422 -4.2857141 -4.2835817 -4.2894292 -4.3023219 -4.3106294 -4.31006 -4.3005414 -4.2878795 -4.2790842 -4.2715693][-4.2774315 -4.2750983 -4.2699351 -4.2651992 -4.2617407 -4.2587318 -4.2546477 -4.2649093 -4.2838869 -4.2934113 -4.2918334 -4.2792077 -4.2595468 -4.2441864 -4.2349973][-4.2601662 -4.2550235 -4.2481327 -4.2427149 -4.2363181 -4.2277126 -4.2159166 -4.2287312 -4.2560763 -4.2705879 -4.2727675 -4.2592235 -4.2349038 -4.2169695 -4.2073245][-4.2391624 -4.2274261 -4.21709 -4.2121925 -4.201911 -4.1799979 -4.1535826 -4.1630936 -4.203856 -4.2307706 -4.2404761 -4.2328663 -4.2119012 -4.1971111 -4.1916337][-4.2152805 -4.1930709 -4.1753392 -4.1653948 -4.1443396 -4.1028414 -4.0564079 -4.0636096 -4.1221495 -4.1703086 -4.1972904 -4.2038422 -4.1986995 -4.1936073 -4.1934538][-4.1981916 -4.1651692 -4.1369858 -4.1137447 -4.0734334 -4.0072994 -3.9325001 -3.9314299 -4.0100179 -4.0912366 -4.1485996 -4.1743493 -4.1808038 -4.1830249 -4.1896448][-4.19689 -4.158987 -4.1213455 -4.0820045 -4.0182996 -3.9181168 -3.796891 -3.7742529 -3.8749342 -3.9920337 -4.0832295 -4.131959 -4.1428542 -4.1411715 -4.1479278][-4.1997209 -4.1611533 -4.1238527 -4.0805926 -4.0127516 -3.9011335 -3.7577262 -3.7228918 -3.8289416 -3.9572074 -4.0576534 -4.1139584 -4.1248302 -4.1156616 -4.1246505][-4.1970916 -4.1550303 -4.122685 -4.0959954 -4.052403 -3.9722848 -3.8595302 -3.8272371 -3.9005063 -3.9957252 -4.0800962 -4.1326995 -4.143055 -4.1398268 -4.1524224][-4.1880126 -4.1412768 -4.1137753 -4.1064081 -4.0901561 -4.0455456 -3.9723907 -3.9473789 -3.9870317 -4.0470533 -4.1112261 -4.1550665 -4.1715832 -4.1835451 -4.2013621][-4.187139 -4.1404 -4.1215744 -4.1274362 -4.1249609 -4.0970459 -4.0472078 -4.032485 -4.0565066 -4.0926361 -4.1351123 -4.1707535 -4.1959105 -4.2211823 -4.2434464][-4.2012463 -4.160141 -4.1484275 -4.1578135 -4.1568108 -4.1347013 -4.1014585 -4.0992002 -4.1157951 -4.1320815 -4.1541605 -4.1798792 -4.2031317 -4.2310429 -4.2562079][-4.2236834 -4.1886191 -4.1774144 -4.1818166 -4.1771307 -4.160481 -4.1443949 -4.1511784 -4.1656737 -4.1746612 -4.1835585 -4.1996727 -4.21468 -4.2330689 -4.2541966][-4.2503057 -4.2218552 -4.2093973 -4.207 -4.1976647 -4.1832924 -4.1761656 -4.1878924 -4.2045183 -4.21127 -4.2114272 -4.2175479 -4.2259173 -4.2358756 -4.2498703][-4.2786164 -4.25569 -4.2414412 -4.23474 -4.2254438 -4.2171264 -4.2155528 -4.2252989 -4.2382426 -4.2436805 -4.2418151 -4.242969 -4.2475271 -4.2545147 -4.2641897]]...]
INFO - root - 2017-12-08 01:16:23.457658: step 75710, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 51h:22m:18s remains)
INFO - root - 2017-12-08 01:16:30.277727: step 75720, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 48h:12m:41s remains)
INFO - root - 2017-12-08 01:16:37.077484: step 75730, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 47h:26m:39s remains)
INFO - root - 2017-12-08 01:16:43.909148: step 75740, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 45h:12m:01s remains)
INFO - root - 2017-12-08 01:16:50.721219: step 75750, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 52h:24m:16s remains)
INFO - root - 2017-12-08 01:16:57.646887: step 75760, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 51h:14m:29s remains)
INFO - root - 2017-12-08 01:17:04.384639: step 75770, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 51h:12m:28s remains)
INFO - root - 2017-12-08 01:17:11.156593: step 75780, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 45h:13m:12s remains)
INFO - root - 2017-12-08 01:17:18.022978: step 75790, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 47h:08m:17s remains)
INFO - root - 2017-12-08 01:17:24.597090: step 75800, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 49h:37m:39s remains)
2017-12-08 01:17:25.365305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2577958 -4.2542286 -4.2545671 -4.251533 -4.2478595 -4.244525 -4.2418089 -4.2419987 -4.2442393 -4.2452917 -4.2424512 -4.2331982 -4.2130442 -4.1952248 -4.1944513][-4.2634525 -4.258832 -4.2639356 -4.2643914 -4.2629504 -4.2590351 -4.2557049 -4.2570195 -4.2613964 -4.2629137 -4.2582364 -4.2449479 -4.21847 -4.1942139 -4.1921029][-4.2490506 -4.2398787 -4.24706 -4.248714 -4.248385 -4.2435951 -4.2399373 -4.2421765 -4.2477612 -4.2505875 -4.2468486 -4.233233 -4.2064123 -4.1813359 -4.1783137][-4.2355027 -4.2159572 -4.2165384 -4.2157054 -4.2135181 -4.2063088 -4.201211 -4.2025156 -4.2100563 -4.2160711 -4.2164159 -4.2068615 -4.1868625 -4.1650805 -4.1603394][-4.2170358 -4.1830988 -4.1715746 -4.1686258 -4.1648922 -4.1558709 -4.1476736 -4.1464791 -4.1567397 -4.1721573 -4.183743 -4.1857605 -4.1764708 -4.16095 -4.1564565][-4.2009006 -4.15707 -4.1344943 -4.1279135 -4.1202621 -4.1052761 -4.0850391 -4.0705948 -4.0803013 -4.1099844 -4.138113 -4.1555648 -4.1572771 -4.151433 -4.1526833][-4.1793303 -4.1364617 -4.1105552 -4.0947261 -4.0744386 -4.0462155 -4.0046692 -3.9635694 -3.9663072 -4.0121727 -4.062851 -4.1015835 -4.1176648 -4.1227932 -4.1310344][-4.1755533 -4.1397462 -4.1166615 -4.0942464 -4.0638671 -4.0257525 -3.9650838 -3.8978157 -3.8886111 -3.9428284 -4.00926 -4.0653749 -4.0911069 -4.1004667 -4.1094108][-4.197228 -4.1692033 -4.1513982 -4.1317654 -4.1079493 -4.082087 -4.0347195 -3.9797168 -3.9650187 -3.9939277 -4.0400548 -4.0865607 -4.1063194 -4.1091514 -4.1126947][-4.2089329 -4.1859136 -4.1739244 -4.1620889 -4.1496587 -4.1404114 -4.1168184 -4.08387 -4.0675435 -4.0707116 -4.0877042 -4.1132812 -4.121336 -4.1186032 -4.1218491][-4.19184 -4.1725378 -4.1651535 -4.1610246 -4.1600375 -4.1624646 -4.1544456 -4.1393285 -4.126977 -4.1209435 -4.122179 -4.13175 -4.1290951 -4.123323 -4.132441][-4.1793089 -4.161376 -4.1578717 -4.1598525 -4.1666541 -4.1753249 -4.1757541 -4.1723843 -4.168232 -4.1655879 -4.1641788 -4.1644783 -4.1540246 -4.144021 -4.1529012][-4.1750917 -4.1556249 -4.154501 -4.15812 -4.1668463 -4.177712 -4.1832871 -4.1871839 -4.1913128 -4.1947665 -4.1956782 -4.1918292 -4.178081 -4.1629705 -4.1647058][-4.1750879 -4.1581712 -4.157855 -4.1611996 -4.1703196 -4.1810985 -4.1885681 -4.1954494 -4.2037697 -4.2099385 -4.2112842 -4.2060251 -4.1926312 -4.1744037 -4.1682205][-4.1934872 -4.181994 -4.1820574 -4.1843081 -4.1912837 -4.19927 -4.2052407 -4.2104874 -4.2167869 -4.2212739 -4.2225957 -4.2195945 -4.2121892 -4.1991606 -4.1930056]]...]
INFO - root - 2017-12-08 01:17:32.103705: step 75810, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 43h:51m:04s remains)
INFO - root - 2017-12-08 01:17:38.993724: step 75820, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 50h:52m:02s remains)
INFO - root - 2017-12-08 01:17:45.909475: step 75830, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 50h:02m:08s remains)
INFO - root - 2017-12-08 01:17:52.578655: step 75840, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 47h:49m:49s remains)
INFO - root - 2017-12-08 01:17:59.389882: step 75850, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 45h:29m:52s remains)
INFO - root - 2017-12-08 01:18:06.249693: step 75860, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 46h:09m:23s remains)
INFO - root - 2017-12-08 01:18:13.152195: step 75870, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 52h:05m:48s remains)
INFO - root - 2017-12-08 01:18:20.110101: step 75880, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 50h:32m:52s remains)
INFO - root - 2017-12-08 01:18:26.949559: step 75890, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 46h:01m:46s remains)
INFO - root - 2017-12-08 01:18:33.481094: step 75900, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 45h:48m:13s remains)
2017-12-08 01:18:34.249004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2565923 -4.2613955 -4.2740355 -4.2923121 -4.319943 -4.3489132 -4.3650489 -4.3487463 -4.2897358 -4.2120633 -4.1675277 -4.1787639 -4.2226582 -4.2637053 -4.2828765][-4.2279491 -4.2342162 -4.2488756 -4.2700644 -4.2973871 -4.3247323 -4.3446851 -4.3432021 -4.2995243 -4.2282944 -4.1846104 -4.1942062 -4.2333922 -4.2678695 -4.2809234][-4.2067761 -4.2130623 -4.2279005 -4.2495656 -4.2751141 -4.2984738 -4.318233 -4.3278279 -4.3039222 -4.2504964 -4.2164965 -4.2251158 -4.2578344 -4.282208 -4.2858782][-4.2011638 -4.2043915 -4.2158556 -4.2327137 -4.2535791 -4.2724252 -4.28924 -4.3017282 -4.2902517 -4.2547708 -4.2345986 -4.2478933 -4.2801857 -4.3021221 -4.3028507][-4.2056217 -4.2044692 -4.2100811 -4.2204266 -4.2350893 -4.2478309 -4.256732 -4.2638621 -4.2535181 -4.2297564 -4.2249608 -4.2482004 -4.2881265 -4.3188066 -4.3277135][-4.2086763 -4.20306 -4.20256 -4.2068038 -4.2162113 -4.2201052 -4.2179303 -4.2161841 -4.2021165 -4.1809225 -4.185112 -4.2220392 -4.2760391 -4.3229942 -4.3465247][-4.2123518 -4.2029376 -4.1976109 -4.1971025 -4.2023997 -4.1995859 -4.1886511 -4.1748819 -4.1519642 -4.1259947 -4.1358986 -4.1858273 -4.2523127 -4.3111181 -4.3470426][-4.2168784 -4.2058754 -4.199995 -4.1960063 -4.19788 -4.1895185 -4.1732512 -4.14701 -4.1095276 -4.0806065 -4.0992842 -4.16116 -4.2362757 -4.2976251 -4.3351326][-4.2241855 -4.2148633 -4.2090316 -4.2022066 -4.200839 -4.1899505 -4.1721287 -4.1356177 -4.0824447 -4.0486374 -4.0729475 -4.1441827 -4.2254224 -4.2874489 -4.3226171][-4.2273755 -4.222662 -4.2180552 -4.2108965 -4.2065649 -4.1957068 -4.1783829 -4.1412759 -4.0867348 -4.0544896 -4.0791087 -4.1469884 -4.2236447 -4.2820067 -4.3130512][-4.231061 -4.228229 -4.2261577 -4.2222147 -4.2173557 -4.2088528 -4.1987958 -4.1753902 -4.1361017 -4.1133428 -4.1310225 -4.1803384 -4.237556 -4.2822027 -4.3040566][-4.2563605 -4.2543373 -4.2519794 -4.249588 -4.2482386 -4.2463889 -4.2454638 -4.2382746 -4.2209835 -4.2107811 -4.2194157 -4.2428474 -4.27256 -4.2965603 -4.3078065][-4.2923794 -4.2908635 -4.2905717 -4.2913008 -4.2921767 -4.2927265 -4.2949967 -4.2949286 -4.290215 -4.2854156 -4.2857728 -4.2931671 -4.305829 -4.3162279 -4.3203874][-4.3197532 -4.3191042 -4.3204823 -4.3215795 -4.3221254 -4.3224373 -4.3239651 -4.3245792 -4.3237724 -4.3218 -4.3200483 -4.32166 -4.326324 -4.3307142 -4.3323236][-4.3353567 -4.335259 -4.3362956 -4.336647 -4.336122 -4.3356228 -4.335475 -4.3358068 -4.3365631 -4.3368945 -4.3362722 -4.3363194 -4.3373623 -4.33901 -4.3400269]]...]
INFO - root - 2017-12-08 01:18:41.072254: step 75910, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.731 sec/batch; 52h:04m:10s remains)
INFO - root - 2017-12-08 01:18:47.826109: step 75920, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 48h:16m:14s remains)
INFO - root - 2017-12-08 01:18:54.437422: step 75930, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 46h:23m:38s remains)
INFO - root - 2017-12-08 01:19:01.318510: step 75940, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.756 sec/batch; 53h:51m:59s remains)
INFO - root - 2017-12-08 01:19:08.107502: step 75950, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 52h:03m:58s remains)
INFO - root - 2017-12-08 01:19:14.844438: step 75960, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 47h:58m:59s remains)
INFO - root - 2017-12-08 01:19:21.698141: step 75970, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 45h:20m:58s remains)
INFO - root - 2017-12-08 01:19:28.509299: step 75980, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 46h:14m:46s remains)
INFO - root - 2017-12-08 01:19:35.374869: step 75990, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 50h:40m:05s remains)
INFO - root - 2017-12-08 01:19:42.112546: step 76000, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 50h:08m:28s remains)
2017-12-08 01:19:42.837824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.213716 -4.17965 -4.1705647 -4.1982241 -4.2184691 -4.2240648 -4.208693 -4.1726 -4.1431513 -4.1559863 -4.1825762 -4.1743765 -4.1308956 -4.0877142 -4.0906544][-4.2267423 -4.1858163 -4.1618428 -4.1747766 -4.1925387 -4.201808 -4.2006931 -4.184916 -4.1765838 -4.1961503 -4.2168803 -4.2031927 -4.1559963 -4.1086259 -4.109868][-4.2432203 -4.2011538 -4.164217 -4.1582117 -4.172255 -4.1889796 -4.1953492 -4.1927266 -4.2029343 -4.23188 -4.2535348 -4.2362976 -4.18684 -4.1336222 -4.1234646][-4.2595091 -4.2182627 -4.1751795 -4.1556897 -4.1619306 -4.1692424 -4.1701603 -4.1703806 -4.1877184 -4.2251477 -4.2517881 -4.2426629 -4.1989589 -4.146615 -4.1274872][-4.2794075 -4.2426562 -4.1977448 -4.1682906 -4.163063 -4.148737 -4.126256 -4.1101565 -4.1300673 -4.1835532 -4.2187629 -4.2220469 -4.1908393 -4.1465335 -4.1277609][-4.2945809 -4.2652349 -4.2248945 -4.1848063 -4.1587214 -4.1179929 -4.0628786 -4.026217 -4.0615482 -4.1460075 -4.1952372 -4.2059379 -4.1827059 -4.1468725 -4.1317048][-4.3028145 -4.2799668 -4.2438526 -4.1970253 -4.1532826 -4.0923405 -4.0142074 -3.9684191 -4.0255189 -4.1346192 -4.1933589 -4.2058735 -4.1858435 -4.1540713 -4.1439962][-4.3044643 -4.2869492 -4.2568932 -4.2111373 -4.1620851 -4.0915174 -4.0076165 -3.9729526 -4.0440154 -4.1538229 -4.2113476 -4.2214422 -4.2024784 -4.1746483 -4.1683574][-4.278594 -4.2688174 -4.2516084 -4.2248549 -4.1925216 -4.1356606 -4.0671711 -4.0513854 -4.1143827 -4.1953254 -4.2347846 -4.23946 -4.2238426 -4.1994667 -4.1942353][-4.23944 -4.2325878 -4.2300282 -4.2310386 -4.2249141 -4.195621 -4.1535878 -4.151166 -4.1932764 -4.2403231 -4.2599559 -4.2591891 -4.2464166 -4.2264261 -4.2182155][-4.2080936 -4.1965084 -4.1981373 -4.2182169 -4.2324681 -4.2263026 -4.2091832 -4.2135863 -4.2396555 -4.2682781 -4.283299 -4.2891612 -4.2774396 -4.2539468 -4.23516][-4.2051439 -4.1809363 -4.1797276 -4.2059464 -4.2311115 -4.2427649 -4.2428718 -4.2491851 -4.2623372 -4.2818618 -4.2989373 -4.3103108 -4.2944164 -4.2539124 -4.2158303][-4.2429943 -4.2084861 -4.1943822 -4.2142706 -4.2426367 -4.2625513 -4.2715344 -4.2755623 -4.280632 -4.2915115 -4.3032503 -4.3079486 -4.2824144 -4.2260375 -4.171093][-4.2798376 -4.2448196 -4.2212648 -4.2319736 -4.2605157 -4.2862716 -4.3005662 -4.3070035 -4.3072834 -4.3076196 -4.3068619 -4.297792 -4.2610378 -4.1897488 -4.1217079][-4.307456 -4.2777629 -4.2507253 -4.2514753 -4.275075 -4.300252 -4.3175473 -4.3263564 -4.3244267 -4.3181796 -4.3096132 -4.2882638 -4.2406874 -4.1579909 -4.0821681]]...]
INFO - root - 2017-12-08 01:19:49.786194: step 76010, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.677 sec/batch; 48h:13m:08s remains)
INFO - root - 2017-12-08 01:19:56.575356: step 76020, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 50h:42m:46s remains)
INFO - root - 2017-12-08 01:20:03.322962: step 76030, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 48h:21m:42s remains)
INFO - root - 2017-12-08 01:20:10.105578: step 76040, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.670 sec/batch; 47h:43m:38s remains)
INFO - root - 2017-12-08 01:20:16.933462: step 76050, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 46h:43m:15s remains)
INFO - root - 2017-12-08 01:20:23.754899: step 76060, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 50h:24m:35s remains)
INFO - root - 2017-12-08 01:20:30.601624: step 76070, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 52h:23m:27s remains)
INFO - root - 2017-12-08 01:20:37.413780: step 76080, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 47h:23m:40s remains)
INFO - root - 2017-12-08 01:20:44.297706: step 76090, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 46h:55m:44s remains)
INFO - root - 2017-12-08 01:20:50.941597: step 76100, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 45h:45m:28s remains)
2017-12-08 01:20:51.695555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1662984 -4.179687 -4.1851478 -4.1697359 -4.1443763 -4.1247354 -4.1182761 -4.1219807 -4.1060424 -4.08907 -4.0966606 -4.1227288 -4.1458974 -4.1543455 -4.1629133][-4.1643457 -4.1721191 -4.1701231 -4.1503897 -4.1233997 -4.1068211 -4.1080556 -4.121232 -4.1138453 -4.1021438 -4.1119089 -4.1333666 -4.152328 -4.1576505 -4.15939][-4.1603546 -4.161303 -4.1534195 -4.13162 -4.1045671 -4.0885119 -4.0962152 -4.1202512 -4.1261754 -4.125061 -4.1333947 -4.1424789 -4.1439729 -4.1355133 -4.1257596][-4.1415882 -4.1392617 -4.1266117 -4.1012878 -4.0705805 -4.0528507 -4.0647173 -4.1036263 -4.1283951 -4.136795 -4.1421413 -4.1391149 -4.1278906 -4.1110668 -4.0972772][-4.114099 -4.1171741 -4.1026516 -4.0756245 -4.0419955 -4.0193706 -4.0311217 -4.0828404 -4.1221542 -4.1358056 -4.138 -4.1306319 -4.1183405 -4.1063194 -4.0988903][-4.0992403 -4.10551 -4.0922804 -4.0674376 -4.0374827 -4.0109615 -4.0141773 -4.0647984 -4.1040182 -4.1135235 -4.1124482 -4.1091666 -4.106729 -4.107564 -4.1123638][-4.0952978 -4.0999255 -4.0919089 -4.0749993 -4.0535288 -4.0270848 -4.0223527 -4.062418 -4.0932727 -4.0966783 -4.0883274 -4.0870657 -4.0965962 -4.1098771 -4.1226306][-4.0927067 -4.0979691 -4.0976844 -4.0898495 -4.0746565 -4.0485663 -4.0395455 -4.070775 -4.093523 -4.0934 -4.0819592 -4.0782242 -4.0872588 -4.1014495 -4.1141553][-4.0882239 -4.09114 -4.0944762 -4.0893064 -4.0761385 -4.0512753 -4.04233 -4.06721 -4.0865569 -4.0909872 -4.0822387 -4.077878 -4.0836267 -4.0927343 -4.0981917][-4.0839195 -4.0777 -4.0784683 -4.0743203 -4.0635352 -4.0432987 -4.0414534 -4.0648365 -4.0866928 -4.09619 -4.0884151 -4.0806441 -4.081511 -4.0878892 -4.0936046][-4.0927019 -4.0804234 -4.0775533 -4.0743742 -4.064837 -4.0483675 -4.0490017 -4.0676174 -4.09022 -4.1022491 -4.090385 -4.0731673 -4.0662088 -4.0697021 -4.0814314][-4.1085591 -4.0967755 -4.0922246 -4.08643 -4.0754004 -4.0601563 -4.057353 -4.0673285 -4.0858736 -4.0960112 -4.0821791 -4.0591607 -4.04342 -4.0457783 -4.0662241][-4.1133246 -4.104012 -4.1013103 -4.0957494 -4.0883417 -4.0774355 -4.0715866 -4.0765266 -4.0940242 -4.1026793 -4.0870481 -4.0590792 -4.0382357 -4.0411777 -4.0649605][-4.1061554 -4.1007352 -4.10227 -4.1019816 -4.1007309 -4.0966339 -4.0916357 -4.0932908 -4.1086183 -4.1195259 -4.1068053 -4.0782638 -4.0560722 -4.0573959 -4.07613][-4.0924826 -4.090147 -4.0950069 -4.1008935 -4.1061888 -4.1083164 -4.107717 -4.11106 -4.123951 -4.1344366 -4.1252108 -4.1016355 -4.0831037 -4.0831442 -4.0970907]]...]
INFO - root - 2017-12-08 01:20:58.491669: step 76110, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 47h:42m:07s remains)
INFO - root - 2017-12-08 01:21:05.277293: step 76120, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 45h:39m:43s remains)
INFO - root - 2017-12-08 01:21:12.229509: step 76130, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 47h:56m:01s remains)
INFO - root - 2017-12-08 01:21:19.078121: step 76140, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.742 sec/batch; 52h:49m:47s remains)
INFO - root - 2017-12-08 01:21:25.871441: step 76150, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.755 sec/batch; 53h:46m:00s remains)
INFO - root - 2017-12-08 01:21:32.743228: step 76160, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 48h:12m:53s remains)
INFO - root - 2017-12-08 01:21:39.431670: step 76170, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 44h:39m:09s remains)
INFO - root - 2017-12-08 01:21:46.227175: step 76180, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 47h:28m:11s remains)
INFO - root - 2017-12-08 01:21:52.992280: step 76190, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.724 sec/batch; 51h:34m:48s remains)
INFO - root - 2017-12-08 01:21:59.735422: step 76200, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 52h:00m:59s remains)
2017-12-08 01:22:00.506047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2666507 -4.2604032 -4.264967 -4.2742157 -4.2729921 -4.2629747 -4.2570176 -4.2582574 -4.262599 -4.2672176 -4.2692614 -4.2624474 -4.2553897 -4.2562728 -4.2547584][-4.2428346 -4.236814 -4.2404408 -4.2434545 -4.2322135 -4.2165103 -4.213593 -4.2267566 -4.2396126 -4.2494521 -4.2502871 -4.24283 -4.2392483 -4.2452412 -4.2475619][-4.2189741 -4.2095017 -4.2061772 -4.1989627 -4.1779351 -4.1566081 -4.1547518 -4.1798854 -4.2045636 -4.2190084 -4.2206283 -4.2136526 -4.215035 -4.2292595 -4.2371325][-4.2033939 -4.18679 -4.1734209 -4.1586518 -4.1316624 -4.0982018 -4.0893459 -4.1178222 -4.152647 -4.17525 -4.1834588 -4.1776886 -4.1770253 -4.19571 -4.2085762][-4.1969 -4.1731291 -4.1516132 -4.1330442 -4.1016755 -4.0627818 -4.0490847 -4.0734277 -4.1111417 -4.140944 -4.1584644 -4.1539707 -4.1419182 -4.1563945 -4.1743793][-4.1937203 -4.1664243 -4.1437936 -4.1241655 -4.0935431 -4.0587139 -4.0411625 -4.0499191 -4.0842543 -4.1204853 -4.1478868 -4.1463766 -4.1195264 -4.1206861 -4.141963][-4.1753874 -4.1448393 -4.1254234 -4.1068296 -4.0805326 -4.0416379 -4.0014586 -3.9863496 -4.0321765 -4.096076 -4.1349645 -4.1376123 -4.1057153 -4.0943561 -4.1122837][-4.1511917 -4.1211071 -4.1085157 -4.0934496 -4.069447 -4.0199223 -3.9555337 -3.9085288 -3.9737153 -4.0746403 -4.1299877 -4.1275845 -4.0869827 -4.0600257 -4.0714755][-4.1394448 -4.1181283 -4.1144433 -4.1034417 -4.0862179 -4.0443606 -3.9890811 -3.939882 -3.996103 -4.0909758 -4.1412287 -4.1297731 -4.074254 -4.0243692 -4.0253806][-4.1414466 -4.1295967 -4.1299963 -4.12255 -4.1161156 -4.0911059 -4.060359 -4.0338941 -4.0683141 -4.1287107 -4.1655803 -4.1520824 -4.0971694 -4.0455112 -4.0356517][-4.1462259 -4.1390915 -4.1378536 -4.1307597 -4.128067 -4.1149316 -4.0989437 -4.0841012 -4.0990839 -4.1366367 -4.1673765 -4.1600547 -4.1187954 -4.0850296 -4.078021][-4.1508551 -4.1518679 -4.1487131 -4.1404452 -4.1356249 -4.1267242 -4.1154623 -4.1069522 -4.111732 -4.132566 -4.1529241 -4.1488256 -4.1201525 -4.1000776 -4.0990467][-4.1510148 -4.1597209 -4.1603594 -4.1559467 -4.1523795 -4.1431203 -4.1315613 -4.1276565 -4.1278415 -4.13608 -4.1484962 -4.1456466 -4.1241632 -4.1083565 -4.1116776][-4.1537714 -4.1636939 -4.1686258 -4.1686606 -4.1661367 -4.1567354 -4.1448464 -4.1419711 -4.1374431 -4.1380353 -4.1429114 -4.1366549 -4.1185951 -4.106709 -4.1124187][-4.1629744 -4.1698155 -4.177402 -4.1861978 -4.1863194 -4.1739936 -4.1549044 -4.1476111 -4.1392393 -4.1394534 -4.1385412 -4.1224785 -4.1035275 -4.0937223 -4.097343]]...]
INFO - root - 2017-12-08 01:22:07.352495: step 76210, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 52h:04m:41s remains)
INFO - root - 2017-12-08 01:22:14.213477: step 76220, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.731 sec/batch; 52h:00m:13s remains)
INFO - root - 2017-12-08 01:22:20.996310: step 76230, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 47h:42m:05s remains)
INFO - root - 2017-12-08 01:22:27.600780: step 76240, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 46h:05m:48s remains)
INFO - root - 2017-12-08 01:22:34.479530: step 76250, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 49h:51m:18s remains)
INFO - root - 2017-12-08 01:22:41.427911: step 76260, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 51h:01m:26s remains)
INFO - root - 2017-12-08 01:22:48.193060: step 76270, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 48h:15m:54s remains)
INFO - root - 2017-12-08 01:22:54.937117: step 76280, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 46h:28m:03s remains)
INFO - root - 2017-12-08 01:23:01.701514: step 76290, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 46h:43m:24s remains)
INFO - root - 2017-12-08 01:23:08.344804: step 76300, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.712 sec/batch; 50h:39m:48s remains)
2017-12-08 01:23:09.142227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2664967 -4.2545834 -4.2520308 -4.2569742 -4.2640357 -4.2674947 -4.2644715 -4.2599044 -4.2602921 -4.2660642 -4.2719188 -4.2743077 -4.2671404 -4.2544241 -4.2492104][-4.2404122 -4.2282367 -4.2289777 -4.2364531 -4.242877 -4.2420588 -4.231885 -4.2230597 -4.2265682 -4.2357597 -4.2420092 -4.244751 -4.2384148 -4.2312069 -4.23257][-4.2090864 -4.202539 -4.2120981 -4.2245092 -4.2306895 -4.22336 -4.2014008 -4.1836076 -4.1897278 -4.2015042 -4.2047515 -4.2024417 -4.1937871 -4.1960568 -4.2086015][-4.1706123 -4.1736894 -4.1958385 -4.2157617 -4.2210817 -4.20394 -4.1646118 -4.13951 -4.1560469 -4.1747808 -4.1761146 -4.1662335 -4.150877 -4.1571608 -4.1793671][-4.1300006 -4.1397662 -4.1714506 -4.1965265 -4.1979508 -4.1630754 -4.0978942 -4.066102 -4.1065731 -4.1478338 -4.1553769 -4.1389937 -4.116209 -4.1207266 -4.1457434][-4.1088686 -4.1189342 -4.1520281 -4.1760225 -4.1705647 -4.1131372 -4.011189 -3.9620273 -4.0309758 -4.10977 -4.1358528 -4.11981 -4.0950632 -4.0987754 -4.1235237][-4.115478 -4.1165237 -4.1424208 -4.1651816 -4.1555085 -4.0820971 -3.9492583 -3.8666596 -3.9501023 -4.0633035 -4.1134591 -4.1086192 -4.0938015 -4.0998755 -4.122571][-4.1438866 -4.1302762 -4.1434731 -4.1643882 -4.161211 -4.102088 -3.982018 -3.8856683 -3.9458592 -4.0542569 -4.1104879 -4.1159587 -4.1137781 -4.1274042 -4.1487226][-4.1724 -4.1415873 -4.1399565 -4.1584606 -4.170012 -4.1496348 -4.079988 -4.008575 -4.0345411 -4.1009154 -4.1385622 -4.1416984 -4.1430688 -4.1602011 -4.1806445][-4.1856613 -4.1422343 -4.1250629 -4.1357861 -4.1568327 -4.1684041 -4.1484003 -4.1173687 -4.1340003 -4.1692023 -4.1848974 -4.1778641 -4.1734595 -4.1841044 -4.1985059][-4.193469 -4.1488667 -4.1277719 -4.1267552 -4.1420627 -4.165874 -4.1754193 -4.1734962 -4.192945 -4.217464 -4.224081 -4.2130442 -4.2031646 -4.2050824 -4.2109785][-4.2010479 -4.1644382 -4.1504683 -4.1409163 -4.1395478 -4.1570988 -4.1807914 -4.1961303 -4.2171383 -4.2378063 -4.2464705 -4.2407007 -4.23192 -4.2252808 -4.2213931][-4.20405 -4.1796794 -4.1769671 -4.1651349 -4.1466784 -4.1484494 -4.1682 -4.1905627 -4.2148008 -4.2347026 -4.2466631 -4.2512007 -4.2488017 -4.2388968 -4.2283955][-4.1933475 -4.1808848 -4.18738 -4.1834168 -4.1620541 -4.1525779 -4.16351 -4.1846309 -4.2078094 -4.2255321 -4.2399006 -4.2513566 -4.254487 -4.2463007 -4.236805][-4.1966228 -4.1934133 -4.2044988 -4.2079954 -4.1931205 -4.1809473 -4.1845775 -4.1980157 -4.2136283 -4.22651 -4.24139 -4.2548594 -4.2619791 -4.2609582 -4.2585578]]...]
INFO - root - 2017-12-08 01:23:15.974658: step 76310, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 46h:19m:02s remains)
INFO - root - 2017-12-08 01:23:22.926869: step 76320, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 48h:18m:08s remains)
INFO - root - 2017-12-08 01:23:29.822720: step 76330, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 52h:05m:57s remains)
INFO - root - 2017-12-08 01:23:36.514967: step 76340, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 47h:11m:02s remains)
INFO - root - 2017-12-08 01:23:43.332532: step 76350, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 46h:33m:19s remains)
INFO - root - 2017-12-08 01:23:50.050508: step 76360, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.659 sec/batch; 46h:52m:30s remains)
INFO - root - 2017-12-08 01:23:56.906187: step 76370, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 50h:04m:15s remains)
INFO - root - 2017-12-08 01:24:03.634434: step 76380, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 51h:33m:40s remains)
INFO - root - 2017-12-08 01:24:10.490724: step 76390, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 46h:58m:33s remains)
INFO - root - 2017-12-08 01:24:17.053081: step 76400, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 45h:31m:50s remains)
2017-12-08 01:24:17.813047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0972047 -4.1031303 -4.1607971 -4.2452359 -4.3101158 -4.3408732 -4.34171 -4.3185344 -4.2784305 -4.2230525 -4.1718812 -4.1598048 -4.1943603 -4.2468929 -4.2957458][-4.1818056 -4.1976132 -4.2445683 -4.3022342 -4.3363266 -4.3426404 -4.3299088 -4.300849 -4.2580338 -4.2093678 -4.1782942 -4.1891489 -4.2329645 -4.2802067 -4.317606][-4.2641854 -4.2775097 -4.3028412 -4.32779 -4.3329878 -4.3221984 -4.3020029 -4.2713704 -4.2340393 -4.2064013 -4.2029071 -4.230742 -4.2737956 -4.3100214 -4.3327317][-4.3206582 -4.3245821 -4.3279967 -4.3277655 -4.3123622 -4.2860284 -4.2559795 -4.2226481 -4.1994686 -4.2036352 -4.2259398 -4.2629437 -4.3025436 -4.3283558 -4.3383837][-4.3452754 -4.3412333 -4.3281279 -4.3064771 -4.2707052 -4.2251558 -4.1822453 -4.1520052 -4.1522961 -4.1875892 -4.229373 -4.2724795 -4.3107843 -4.3333969 -4.335412][-4.3537569 -4.3393731 -4.3085179 -4.2594619 -4.1938982 -4.12399 -4.0751305 -4.06306 -4.0957522 -4.1592875 -4.217485 -4.2674932 -4.308466 -4.3291636 -4.3261466][-4.350071 -4.3201137 -4.266624 -4.1903238 -4.092206 -3.9996531 -3.9523048 -3.9695663 -4.0420713 -4.1329861 -4.2081528 -4.2632709 -4.3041782 -4.3226089 -4.317821][-4.3357697 -4.2920518 -4.2226992 -4.1311855 -4.0183463 -3.9235411 -3.8853803 -3.9279823 -4.0319123 -4.1430559 -4.2258477 -4.27452 -4.3037672 -4.3160105 -4.3103318][-4.322916 -4.2770615 -4.2088857 -4.1201844 -4.0215144 -3.9518423 -3.9338217 -3.9866762 -4.0934725 -4.194922 -4.2600818 -4.2884412 -4.29957 -4.3065405 -4.3031635][-4.3225727 -4.2862563 -4.2319241 -4.1592216 -4.0882483 -4.0519681 -4.0507283 -4.0941954 -4.1741772 -4.2432523 -4.2787375 -4.2881269 -4.2878971 -4.2921824 -4.2941203][-4.3278184 -4.3064656 -4.2705708 -4.219326 -4.1753049 -4.1602674 -4.1634688 -4.1883922 -4.2357364 -4.2668772 -4.2726908 -4.2670293 -4.2630858 -4.2692108 -4.2803879][-4.331749 -4.3249788 -4.3055487 -4.2743387 -4.2521591 -4.2460279 -4.246264 -4.2546921 -4.2696357 -4.2671933 -4.2458887 -4.2272053 -4.2225728 -4.2349205 -4.25794][-4.3328681 -4.3345408 -4.32892 -4.3132892 -4.3039327 -4.3021564 -4.2972431 -4.2894526 -4.2720366 -4.2307024 -4.1775551 -4.1474137 -4.151711 -4.1814861 -4.22233][-4.336329 -4.3406935 -4.3426018 -4.3366342 -4.3318286 -4.3289356 -4.318851 -4.2932181 -4.2398691 -4.1507077 -4.0602493 -4.0215583 -4.0458856 -4.1102986 -4.177629][-4.3394847 -4.3457794 -4.3492308 -4.3473697 -4.3441286 -4.3385549 -4.318995 -4.2717614 -4.1825995 -4.0527105 -3.9363649 -3.9044342 -3.9669919 -4.0740948 -4.1684475]]...]
INFO - root - 2017-12-08 01:24:24.570441: step 76410, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 50h:14m:49s remains)
INFO - root - 2017-12-08 01:24:31.371447: step 76420, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 49h:46m:44s remains)
INFO - root - 2017-12-08 01:24:38.166862: step 76430, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 46h:14m:00s remains)
INFO - root - 2017-12-08 01:24:44.895579: step 76440, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 45h:13m:51s remains)
INFO - root - 2017-12-08 01:24:51.659536: step 76450, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 49h:09m:51s remains)
INFO - root - 2017-12-08 01:24:58.448498: step 76460, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 51h:18m:30s remains)
INFO - root - 2017-12-08 01:25:05.202126: step 76470, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 46h:24m:54s remains)
INFO - root - 2017-12-08 01:25:11.932620: step 76480, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.643 sec/batch; 45h:42m:07s remains)
INFO - root - 2017-12-08 01:25:18.725044: step 76490, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 45h:10m:35s remains)
INFO - root - 2017-12-08 01:25:25.402149: step 76500, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.661 sec/batch; 47h:01m:06s remains)
2017-12-08 01:25:26.335456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2072763 -4.2050772 -4.2121029 -4.2178373 -4.2167029 -4.2202106 -4.2185035 -4.2038856 -4.1878085 -4.1756144 -4.1585274 -4.1477585 -4.1383538 -4.1376829 -4.155808][-4.1881042 -4.1846905 -4.1925898 -4.202179 -4.201704 -4.1982045 -4.1897035 -4.1742549 -4.1614189 -4.1511316 -4.1426816 -4.1377182 -4.1335597 -4.137311 -4.1569967][-4.1714029 -4.1674309 -4.1754513 -4.1905265 -4.1982384 -4.1952062 -4.1852646 -4.1719036 -4.1618214 -4.15547 -4.1566596 -4.1598449 -4.1561084 -4.1558738 -4.1728554][-4.1611362 -4.1594663 -4.1712222 -4.1860771 -4.1965213 -4.1916018 -4.1769586 -4.1604037 -4.1494613 -4.147047 -4.1583686 -4.1678042 -4.1638203 -4.1613421 -4.1775069][-4.1405368 -4.1418366 -4.1603069 -4.1773977 -4.185606 -4.173871 -4.1540022 -4.130765 -4.120841 -4.124486 -4.1393127 -4.1493864 -4.1471596 -4.1458559 -4.1653996][-4.1289306 -4.1204963 -4.1321654 -4.1469021 -4.1476111 -4.1302514 -4.1027308 -4.0716381 -4.0631766 -4.0758138 -4.0960078 -4.1091571 -4.1107688 -4.1149 -4.1422367][-4.1088715 -4.0906873 -4.0920324 -4.0929103 -4.0822382 -4.0619583 -4.0324106 -3.9973941 -3.9966357 -4.0286827 -4.060565 -4.0778751 -4.0820808 -4.0893526 -4.1207037][-4.0872 -4.0688367 -4.0667739 -4.0613823 -4.0471988 -4.03109 -4.0110688 -3.9842286 -3.9927466 -4.0294771 -4.0577021 -4.0714192 -4.0742993 -4.0831685 -4.1182179][-4.0930796 -4.0826306 -4.0852351 -4.0833807 -4.0734396 -4.0652237 -4.0549593 -4.0408421 -4.0530767 -4.0743928 -4.0851159 -4.0894208 -4.0903931 -4.1002388 -4.1321812][-4.1483769 -4.1424708 -4.1446023 -4.1427021 -4.1372509 -4.1343021 -4.1283293 -4.1222548 -4.1329174 -4.1386185 -4.1314 -4.1206684 -4.1129203 -4.1174679 -4.1427565][-4.2150354 -4.211062 -4.2102122 -4.2052431 -4.1996584 -4.1948581 -4.1874108 -4.1824827 -4.1893516 -4.1884265 -4.1721954 -4.1530375 -4.1343126 -4.1314011 -4.1496072][-4.2580032 -4.2552509 -4.25643 -4.252996 -4.2471881 -4.2392392 -4.2282963 -4.2233782 -4.2287879 -4.225306 -4.2064075 -4.1820221 -4.1600957 -4.1529765 -4.163228][-4.259901 -4.2594867 -4.264555 -4.265871 -4.2623243 -4.2566624 -4.2494779 -4.2489662 -4.2550364 -4.2497649 -4.2309933 -4.2062693 -4.1861825 -4.17892 -4.1816649][-4.2419114 -4.2393379 -4.2458382 -4.2515759 -4.2523179 -4.2507839 -4.2492785 -4.2499704 -4.2491789 -4.2393208 -4.2218375 -4.201251 -4.1868739 -4.1847792 -4.1910195][-4.2458868 -4.2404003 -4.2428031 -4.2467914 -4.2497811 -4.2508168 -4.2513576 -4.2515149 -4.2464442 -4.2344484 -4.2214918 -4.2098379 -4.2025619 -4.2043753 -4.2152023]]...]
INFO - root - 2017-12-08 01:25:33.029827: step 76510, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 48h:25m:59s remains)
INFO - root - 2017-12-08 01:25:39.722618: step 76520, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 45h:19m:08s remains)
INFO - root - 2017-12-08 01:25:46.566494: step 76530, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 51h:03m:09s remains)
INFO - root - 2017-12-08 01:25:53.376265: step 76540, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 51h:00m:23s remains)
INFO - root - 2017-12-08 01:25:59.956505: step 76550, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 48h:41m:46s remains)
INFO - root - 2017-12-08 01:26:06.746464: step 76560, loss = 2.11, batch loss = 2.05 (12.3 examples/sec; 0.651 sec/batch; 46h:15m:31s remains)
INFO - root - 2017-12-08 01:26:13.604567: step 76570, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 46h:02m:23s remains)
INFO - root - 2017-12-08 01:26:20.470023: step 76580, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 51h:34m:51s remains)
INFO - root - 2017-12-08 01:26:27.271266: step 76590, loss = 2.11, batch loss = 2.06 (11.2 examples/sec; 0.713 sec/batch; 50h:39m:57s remains)
INFO - root - 2017-12-08 01:26:33.896901: step 76600, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 48h:01m:05s remains)
2017-12-08 01:26:34.589816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2431364 -4.2266808 -4.2150631 -4.2066417 -4.2078867 -4.2112765 -4.2054067 -4.21543 -4.2385082 -4.2518358 -4.256331 -4.2579026 -4.2720442 -4.2819982 -4.2559452][-4.2484026 -4.2313495 -4.2141619 -4.1994987 -4.1954236 -4.1974015 -4.1928988 -4.2071586 -4.2364163 -4.2575822 -4.2735577 -4.2844734 -4.29887 -4.3051 -4.2717848][-4.2626438 -4.2450681 -4.2232294 -4.1985917 -4.1815372 -4.175015 -4.166 -4.1794152 -4.21066 -4.2409205 -4.27103 -4.2943196 -4.3133221 -4.3178768 -4.2844663][-4.2693033 -4.2497025 -4.2223449 -4.1877079 -4.1590562 -4.1415253 -4.1215024 -4.1247463 -4.1552057 -4.1935468 -4.2380643 -4.2763495 -4.3040643 -4.3087649 -4.2798729][-4.2654457 -4.2417593 -4.2071452 -4.1648674 -4.1278081 -4.0984979 -4.0602512 -4.0478806 -4.0812697 -4.1291122 -4.1843891 -4.2380867 -4.2764516 -4.2855082 -4.2610703][-4.2521791 -4.2229123 -4.1813574 -4.1304584 -4.0801268 -4.0310669 -3.9597449 -3.922605 -3.9698715 -4.039072 -4.1092792 -4.1755648 -4.2229619 -4.2379107 -4.2203484][-4.2257471 -4.189187 -4.1415472 -4.0833006 -4.0247793 -3.9596829 -3.8562183 -3.7967482 -3.8676705 -3.9658368 -4.0499954 -4.1184964 -4.1685228 -4.1871672 -4.1798854][-4.1959953 -4.1557322 -4.1074157 -4.048727 -3.9981771 -3.9493222 -3.8708258 -3.8296328 -3.8966856 -3.9917662 -4.0676727 -4.1241155 -4.1655951 -4.1808238 -4.1795368][-4.18302 -4.1479387 -4.1091986 -4.0604496 -4.0272889 -4.0112309 -3.9797478 -3.9691563 -4.0148106 -4.0793347 -4.1300583 -4.1684213 -4.1978784 -4.2087007 -4.21014][-4.1929321 -4.1629276 -4.1352587 -4.1006193 -4.0800276 -4.0833883 -4.0823531 -4.091465 -4.124259 -4.164803 -4.1939588 -4.2187486 -4.2420187 -4.2512555 -4.2522278][-4.2150164 -4.1902609 -4.1755686 -4.159019 -4.1495862 -4.1623559 -4.1779165 -4.1968751 -4.2224283 -4.2453222 -4.2566738 -4.266583 -4.2798495 -4.284174 -4.28212][-4.2429891 -4.2278743 -4.2258453 -4.2244067 -4.225913 -4.2423325 -4.260025 -4.2758207 -4.2901936 -4.298131 -4.2959986 -4.2920885 -4.2936845 -4.2925158 -4.2873468][-4.2691703 -4.2644405 -4.2699604 -4.2744102 -4.2815013 -4.2966771 -4.30709 -4.3116193 -4.3140359 -4.3126912 -4.3054519 -4.295074 -4.2889438 -4.28471 -4.2786093][-4.2827435 -4.2805686 -4.2867885 -4.2915783 -4.297997 -4.30867 -4.3123732 -4.3090587 -4.3054991 -4.2997665 -4.2903504 -4.27806 -4.2702904 -4.2658277 -4.2626352][-4.2851372 -4.2803407 -4.2843862 -4.288753 -4.2929492 -4.2973337 -4.2945414 -4.2886539 -4.2841568 -4.2767148 -4.2657509 -4.2531481 -4.2462831 -4.2419028 -4.2415204]]...]
INFO - root - 2017-12-08 01:26:41.429366: step 76610, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 0.741 sec/batch; 52h:38m:54s remains)
INFO - root - 2017-12-08 01:26:48.255000: step 76620, loss = 2.03, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 50h:14m:08s remains)
INFO - root - 2017-12-08 01:26:55.006018: step 76630, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 45h:26m:01s remains)
INFO - root - 2017-12-08 01:27:01.836314: step 76640, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 47h:02m:36s remains)
INFO - root - 2017-12-08 01:27:08.657202: step 76650, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 49h:46m:56s remains)
INFO - root - 2017-12-08 01:27:15.569296: step 76660, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 49h:53m:02s remains)
INFO - root - 2017-12-08 01:27:22.420552: step 76670, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 47h:27m:28s remains)
INFO - root - 2017-12-08 01:27:29.108797: step 76680, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 44h:57m:27s remains)
INFO - root - 2017-12-08 01:27:35.879243: step 76690, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 44h:08m:47s remains)
INFO - root - 2017-12-08 01:27:42.680237: step 76700, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 51h:57m:06s remains)
2017-12-08 01:27:43.452245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.247447 -4.2652082 -4.2835336 -4.2963781 -4.2992458 -4.2906294 -4.2712646 -4.2527986 -4.2460389 -4.2477756 -4.2491555 -4.2390232 -4.211822 -4.166903 -4.1240239][-4.2268071 -4.245647 -4.2653222 -4.2820573 -4.2914824 -4.2918215 -4.2812996 -4.267909 -4.2627945 -4.2640095 -4.2604985 -4.2483072 -4.2249393 -4.1840639 -4.1414537][-4.1990695 -4.2102971 -4.225039 -4.2442532 -4.2616839 -4.2702155 -4.2661376 -4.2565889 -4.25644 -4.265924 -4.2685537 -4.2652416 -4.2576017 -4.2336817 -4.201601][-4.184895 -4.1850476 -4.1907473 -4.2093506 -4.2278461 -4.2349181 -4.2283864 -4.2170191 -4.2220321 -4.2390594 -4.2501688 -4.2575746 -4.2605691 -4.2511606 -4.2335615][-4.1700482 -4.1585975 -4.1568971 -4.1712732 -4.1838937 -4.1761632 -4.1515284 -4.1255913 -4.1318216 -4.1633525 -4.1888704 -4.2033772 -4.2132392 -4.216289 -4.2178464][-4.1503181 -4.1325383 -4.1290188 -4.1388221 -4.1393266 -4.10656 -4.048893 -3.995589 -4.0045147 -4.0632024 -4.1114531 -4.1322732 -4.1434183 -4.154892 -4.1748266][-4.1413026 -4.1224494 -4.1162739 -4.1197233 -4.1066308 -4.0447125 -3.9428663 -3.8490798 -3.8609581 -3.9513946 -4.0254593 -4.0494013 -4.05584 -4.0717397 -4.1030321][-4.1527457 -4.1284461 -4.1193142 -4.1221595 -4.1072521 -4.03831 -3.9195814 -3.8044558 -3.8108377 -3.9023931 -3.9730601 -3.9864435 -3.9826086 -4.0000539 -4.0388737][-4.1817145 -4.1610379 -4.1550078 -4.1577706 -4.1480079 -4.099473 -4.014627 -3.9280894 -3.9194365 -3.9719911 -4.0130939 -4.0110369 -3.9982698 -4.0095463 -4.0432034][-4.208015 -4.1967583 -4.1938677 -4.1947184 -4.1884723 -4.1585331 -4.105505 -4.0468631 -4.0329866 -4.0607247 -4.0794721 -4.0705872 -4.058641 -4.0650616 -4.0871177][-4.2278719 -4.2196808 -4.2167144 -4.2157545 -4.213809 -4.198925 -4.1690187 -4.1349783 -4.1246204 -4.1387925 -4.1472497 -4.1427226 -4.1386485 -4.14169 -4.1478448][-4.2509441 -4.2411046 -4.2375927 -4.2371173 -4.2418737 -4.2378278 -4.2242012 -4.2080784 -4.2046595 -4.2120757 -4.2147512 -4.2110152 -4.2102537 -4.2087846 -4.2018552][-4.2747431 -4.2598629 -4.2535057 -4.253809 -4.2641048 -4.2675514 -4.2638803 -4.2586689 -4.2602425 -4.2640553 -4.2598114 -4.2530017 -4.2513595 -4.2462435 -4.2319341][-4.3096189 -4.295608 -4.288559 -4.2902336 -4.3023 -4.3097577 -4.3104877 -4.3089385 -4.308229 -4.3040047 -4.2930479 -4.2796865 -4.2685089 -4.2532096 -4.2312202][-4.3326735 -4.3234196 -4.3190427 -4.3222752 -4.332922 -4.3405962 -4.3414044 -4.3377762 -4.3296776 -4.3153653 -4.2949357 -4.2715178 -4.2487755 -4.2240849 -4.1987276]]...]
INFO - root - 2017-12-08 01:27:50.164709: step 76710, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 46h:20m:16s remains)
INFO - root - 2017-12-08 01:27:57.047235: step 76720, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 47h:56m:35s remains)
INFO - root - 2017-12-08 01:28:03.840356: step 76730, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 51h:11m:59s remains)
INFO - root - 2017-12-08 01:28:10.723559: step 76740, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.760 sec/batch; 53h:57m:45s remains)
INFO - root - 2017-12-08 01:28:17.525101: step 76750, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 48h:07m:53s remains)
INFO - root - 2017-12-08 01:28:24.352712: step 76760, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 45h:57m:30s remains)
INFO - root - 2017-12-08 01:28:31.179341: step 76770, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 51h:19m:54s remains)
INFO - root - 2017-12-08 01:28:38.132797: step 76780, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 53h:01m:02s remains)
INFO - root - 2017-12-08 01:28:44.904932: step 76790, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 47h:40m:19s remains)
INFO - root - 2017-12-08 01:28:51.603485: step 76800, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 45h:15m:16s remains)
2017-12-08 01:28:52.303227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3551016 -4.3492885 -4.3386965 -4.3283367 -4.325613 -4.3296814 -4.3348289 -4.3392873 -4.3421121 -4.3451567 -4.347755 -4.3521662 -4.3546166 -4.3567958 -4.3572745][-4.3479118 -4.3359051 -4.3158493 -4.2939477 -4.281661 -4.2808256 -4.2858348 -4.2959557 -4.3063006 -4.3148274 -4.3220992 -4.3332539 -4.3417034 -4.3499222 -4.3539329][-4.3242135 -4.3061433 -4.2776031 -4.2416306 -4.2134409 -4.2031908 -4.20325 -4.2130585 -4.2321057 -4.2515297 -4.2676334 -4.2885327 -4.3064346 -4.3254528 -4.3373842][-4.2860322 -4.2586007 -4.218976 -4.1700926 -4.1301727 -4.1131659 -4.1015553 -4.1035581 -4.1346064 -4.16852 -4.1962671 -4.2292967 -4.2625833 -4.2950268 -4.3160934][-4.2417736 -4.1982136 -4.137259 -4.0708728 -4.0207667 -3.9903903 -3.9553275 -3.9428663 -4.0022154 -4.0636921 -4.10596 -4.1514082 -4.202559 -4.2524414 -4.2870693][-4.205811 -4.1414037 -4.0533223 -3.9603205 -3.891207 -3.8304338 -3.7427931 -3.6972094 -3.8001564 -3.9098268 -3.9829271 -4.0485229 -4.1185627 -4.1884975 -4.240098][-4.1919103 -4.11137 -3.9999068 -3.880641 -3.7854388 -3.6842237 -3.5259595 -3.4277623 -3.5711875 -3.733928 -3.8458738 -3.9373512 -4.0294986 -4.1175938 -4.1811967][-4.219996 -4.140974 -4.032835 -3.9142785 -3.8140955 -3.7094445 -3.5469232 -3.4389918 -3.5583818 -3.7005031 -3.8076677 -3.8991468 -3.99578 -4.08259 -4.1437297][-4.269331 -4.2123241 -4.1322179 -4.0447106 -3.9681966 -3.8904982 -3.778832 -3.7078133 -3.7683022 -3.8431234 -3.9062691 -3.9684436 -4.0444589 -4.1120243 -4.1592355][-4.3122969 -4.2798071 -4.2339439 -4.1825767 -4.1380076 -4.0882168 -4.0258293 -3.9889972 -4.01036 -4.0392227 -4.0650229 -4.0979381 -4.1441936 -4.1872282 -4.2157345][-4.3390403 -4.3218327 -4.2993331 -4.2753463 -4.2553983 -4.2308092 -4.2011652 -4.1866059 -4.1931267 -4.2059536 -4.2141366 -4.2293148 -4.2533932 -4.2744126 -4.2851257][-4.3553619 -4.3487854 -4.3399825 -4.3314147 -4.3243461 -4.314518 -4.3017035 -4.2961006 -4.2990713 -4.3057704 -4.3071065 -4.3128567 -4.32299 -4.3312206 -4.3341637][-4.3594213 -4.3594656 -4.3571091 -4.3563075 -4.3551064 -4.3513412 -4.3461905 -4.3446188 -4.3468337 -4.3487206 -4.34513 -4.3440313 -4.3473983 -4.3519506 -4.3545451][-4.3589196 -4.3616347 -4.3622203 -4.3645697 -4.3664322 -4.3665385 -4.3663464 -4.3664303 -4.3666415 -4.3653345 -4.360652 -4.35687 -4.35667 -4.3584743 -4.3608861][-4.358809 -4.362247 -4.3631935 -4.3648067 -4.3664756 -4.3679547 -4.3693762 -4.3701286 -4.3693748 -4.3676734 -4.365314 -4.3629723 -4.3618536 -4.3624058 -4.3641319]]...]
INFO - root - 2017-12-08 01:28:59.069082: step 76810, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 49h:14m:35s remains)
INFO - root - 2017-12-08 01:29:05.732091: step 76820, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 47h:04m:18s remains)
INFO - root - 2017-12-08 01:29:12.552060: step 76830, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 46h:23m:51s remains)
INFO - root - 2017-12-08 01:29:19.341429: step 76840, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 50h:01m:45s remains)
INFO - root - 2017-12-08 01:29:26.152922: step 76850, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 51h:18m:04s remains)
INFO - root - 2017-12-08 01:29:32.783874: step 76860, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 46h:41m:43s remains)
INFO - root - 2017-12-08 01:29:39.584826: step 76870, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 46h:47m:54s remains)
INFO - root - 2017-12-08 01:29:46.386982: step 76880, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 51h:05m:26s remains)
INFO - root - 2017-12-08 01:29:53.237980: step 76890, loss = 2.06, batch loss = 2.01 (10.6 examples/sec; 0.755 sec/batch; 53h:35m:44s remains)
INFO - root - 2017-12-08 01:29:59.883308: step 76900, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 47h:25m:41s remains)
2017-12-08 01:30:00.652329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3191757 -4.306818 -4.305656 -4.3089924 -4.3103871 -4.3081355 -4.3083143 -4.3076515 -4.304935 -4.3036404 -4.2999244 -4.2910051 -4.2744789 -4.2508297 -4.21864][-4.3035707 -4.2846246 -4.2795615 -4.2815857 -4.2841377 -4.2812448 -4.2820945 -4.2833514 -4.2808995 -4.2790418 -4.2767491 -4.2685018 -4.2460465 -4.2103434 -4.1598053][-4.2555962 -4.2353325 -4.228375 -4.2269979 -4.2301192 -4.2300568 -4.2350774 -4.24061 -4.2418666 -4.2438869 -4.24619 -4.2420239 -4.2201962 -4.182651 -4.1248221][-4.2010174 -4.181623 -4.1766744 -4.171433 -4.1732244 -4.1725273 -4.1765885 -4.1849656 -4.1973767 -4.2120161 -4.2218308 -4.2239642 -4.2106962 -4.1842771 -4.138247][-4.1861725 -4.1652064 -4.1605988 -4.1489315 -4.1397572 -4.1208758 -4.1056881 -4.1084485 -4.1337519 -4.1703782 -4.1929007 -4.2033243 -4.2030759 -4.19356 -4.1705179][-4.1939139 -4.1638427 -4.1523552 -4.1241875 -4.0894718 -4.0337553 -3.9773345 -3.9647796 -4.0071878 -4.0713024 -4.1123028 -4.1352468 -4.1560431 -4.1692123 -4.1702023][-4.2020559 -4.15913 -4.1292238 -4.0745444 -4.0102468 -3.9196513 -3.8249085 -3.7943413 -3.8612092 -3.9585955 -4.0223184 -4.06042 -4.0985193 -4.1258316 -4.1438737][-4.2102542 -4.1524644 -4.1137056 -4.0588889 -3.999207 -3.9189944 -3.8338282 -3.8019469 -3.8564777 -3.9451151 -4.0055056 -4.0388188 -4.0739479 -4.0953107 -4.1122503][-4.2263789 -4.1694064 -4.137331 -4.1049209 -4.0674086 -4.0189266 -3.9661698 -3.9377317 -3.9595873 -4.0108509 -4.0490403 -4.0644078 -4.0836906 -4.0890388 -4.0930758][-4.2153497 -4.1788006 -4.1695848 -4.1656837 -4.152277 -4.1258159 -4.0969653 -4.0758352 -4.0774403 -4.1019316 -4.1223125 -4.1242332 -4.1294689 -4.1218381 -4.1164503][-4.1724567 -4.16437 -4.1815562 -4.2042503 -4.2127829 -4.2050157 -4.1914411 -4.1799083 -4.1747737 -4.184433 -4.1955538 -4.1930637 -4.1937127 -4.180686 -4.17289][-4.0968356 -4.1141429 -4.156672 -4.2010722 -4.2312407 -4.2431774 -4.2431059 -4.2413845 -4.2409244 -4.2474623 -4.2542162 -4.2515721 -4.2511969 -4.2380257 -4.2279387][-4.0366488 -4.064702 -4.1221004 -4.1811404 -4.2277994 -4.2552781 -4.2678628 -4.2759504 -4.2812548 -4.2888455 -4.2932196 -4.2936888 -4.2936373 -4.2820244 -4.2663422][-4.0244856 -4.0498047 -4.105577 -4.16881 -4.2212448 -4.255444 -4.277071 -4.2933745 -4.3017888 -4.3077178 -4.3102522 -4.3120108 -4.3139648 -4.3048143 -4.287715][-4.0879931 -4.1035676 -4.1432853 -4.195128 -4.2402792 -4.2706456 -4.2942762 -4.3136849 -4.3227119 -4.3260226 -4.3265023 -4.3282089 -4.3300362 -4.3221684 -4.3064313]]...]
INFO - root - 2017-12-08 01:30:07.390169: step 76910, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 49h:48m:45s remains)
INFO - root - 2017-12-08 01:30:14.215733: step 76920, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 48h:38m:50s remains)
INFO - root - 2017-12-08 01:30:21.144884: step 76930, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 48h:37m:33s remains)
INFO - root - 2017-12-08 01:30:27.894598: step 76940, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 45h:49m:08s remains)
INFO - root - 2017-12-08 01:30:34.790162: step 76950, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 51h:28m:38s remains)
INFO - root - 2017-12-08 01:30:41.619086: step 76960, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 52h:01m:57s remains)
INFO - root - 2017-12-08 01:30:48.537634: step 76970, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 48h:00m:47s remains)
INFO - root - 2017-12-08 01:30:55.248078: step 76980, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.626 sec/batch; 44h:26m:51s remains)
INFO - root - 2017-12-08 01:31:02.091063: step 76990, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 46h:44m:45s remains)
INFO - root - 2017-12-08 01:31:08.670771: step 77000, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 51h:44m:48s remains)
2017-12-08 01:31:09.367566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2711825 -4.253346 -4.2339091 -4.2234168 -4.2187848 -4.216805 -4.2050762 -4.1925826 -4.1958103 -4.2054467 -4.1970625 -4.1622233 -4.1171918 -4.0768585 -4.0606322][-4.2840381 -4.2782135 -4.2764497 -4.2678065 -4.2599812 -4.2541986 -4.2365408 -4.2210679 -4.2194633 -4.2277102 -4.2242813 -4.2013865 -4.1572385 -4.11245 -4.0893583][-4.2639017 -4.2700648 -4.2773523 -4.271349 -4.261889 -4.2539988 -4.2309628 -4.20943 -4.2071624 -4.2182212 -4.2284756 -4.2197595 -4.1865711 -4.1486692 -4.12355][-4.2264395 -4.2409668 -4.2538614 -4.2536154 -4.2404757 -4.2198572 -4.1794095 -4.1577945 -4.1688223 -4.1940765 -4.2175746 -4.2228885 -4.2068906 -4.1803331 -4.1545858][-4.2012038 -4.2126503 -4.2206244 -4.2179365 -4.1873279 -4.1356759 -4.0638065 -4.0363059 -4.0772705 -4.1386986 -4.1842375 -4.2080588 -4.2133961 -4.2032022 -4.1835923][-4.1611 -4.1549497 -4.158042 -4.1497793 -4.0955496 -3.9939675 -3.8591561 -3.8034244 -3.9050272 -4.0366817 -4.122777 -4.1692424 -4.1911545 -4.2016625 -4.1979437][-4.0816374 -4.0557652 -4.0590544 -4.0636826 -4.0026979 -3.8506804 -3.6320641 -3.5129809 -3.678025 -3.8877914 -4.025434 -4.1031251 -4.1505184 -4.17545 -4.1862807][-4.0073881 -3.9668884 -3.9737277 -3.9928954 -3.9491305 -3.8116164 -3.6007335 -3.4859772 -3.6473069 -3.8506904 -3.9798853 -4.0554328 -4.1164837 -4.1583028 -4.1750245][-3.9887247 -3.9475014 -3.9609685 -3.9901581 -3.9756663 -3.9026341 -3.7918034 -3.7465506 -3.8460298 -3.966486 -4.0343037 -4.0801973 -4.1282387 -4.17437 -4.1988964][-4.0114813 -3.9825635 -4.001863 -4.0254083 -4.0175352 -3.9902034 -3.9500868 -3.9463489 -4.0045929 -4.0662456 -4.0964808 -4.1192112 -4.1566792 -4.1978259 -4.2146587][-4.0260434 -3.9993105 -4.0156946 -4.0320697 -4.0261755 -4.02463 -4.0228577 -4.0381002 -4.077579 -4.1077662 -4.1187177 -4.1305413 -4.1580839 -4.1862059 -4.1886][-4.0476651 -4.0212197 -4.0275626 -4.0356565 -4.0352678 -4.0501595 -4.0678658 -4.0908704 -4.12148 -4.1333308 -4.1278772 -4.1315026 -4.1470618 -4.1597743 -4.149262][-4.1057887 -4.0925317 -4.0914993 -4.093255 -4.0936337 -4.10988 -4.1291828 -4.1509042 -4.1723518 -4.172617 -4.15171 -4.1388035 -4.1309514 -4.1260438 -4.1066322][-4.1704721 -4.1620278 -4.1624136 -4.1656952 -4.1687717 -4.1813607 -4.1956577 -4.2119355 -4.2268476 -4.2191081 -4.1878963 -4.1621008 -4.1431932 -4.13013 -4.1101208][-4.2191463 -4.20694 -4.2027884 -4.2037826 -4.2105494 -4.2239952 -4.23784 -4.2508655 -4.2609696 -4.2555761 -4.2326546 -4.2104363 -4.1923347 -4.1755643 -4.15812]]...]
INFO - root - 2017-12-08 01:31:16.056937: step 77010, loss = 2.07, batch loss = 2.01 (13.3 examples/sec; 0.599 sec/batch; 42h:32m:19s remains)
INFO - root - 2017-12-08 01:31:22.924743: step 77020, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 48h:17m:25s remains)
INFO - root - 2017-12-08 01:31:29.728292: step 77030, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 51h:29m:22s remains)
INFO - root - 2017-12-08 01:31:36.545834: step 77040, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 48h:54m:59s remains)
INFO - root - 2017-12-08 01:31:43.394664: step 77050, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 47h:26m:01s remains)
INFO - root - 2017-12-08 01:31:50.098479: step 77060, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 45h:31m:52s remains)
INFO - root - 2017-12-08 01:31:56.975708: step 77070, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 50h:46m:42s remains)
INFO - root - 2017-12-08 01:32:03.808843: step 77080, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 51h:33m:08s remains)
INFO - root - 2017-12-08 01:32:10.574439: step 77090, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 46h:41m:24s remains)
INFO - root - 2017-12-08 01:32:17.221377: step 77100, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 44h:38m:43s remains)
2017-12-08 01:32:17.952118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2678986 -4.2485232 -4.2288694 -4.2356067 -4.2396955 -4.2250466 -4.2079258 -4.1977186 -4.2053418 -4.2143426 -4.2039266 -4.1711817 -4.1376548 -4.1462703 -4.1758142][-4.2749162 -4.2562847 -4.2368913 -4.2318597 -4.2254276 -4.2058392 -4.1822023 -4.1651025 -4.1704826 -4.1831355 -4.1751084 -4.1469846 -4.1233993 -4.1370153 -4.1682715][-4.2756305 -4.2570405 -4.2377605 -4.2270384 -4.2140646 -4.1912146 -4.164495 -4.1457372 -4.1461635 -4.1546063 -4.1433125 -4.11825 -4.1055675 -4.1286364 -4.1646757][-4.2632208 -4.2452273 -4.2330861 -4.2267475 -4.2127714 -4.1920309 -4.1629629 -4.1419892 -4.141088 -4.1501932 -4.1451159 -4.1286087 -4.1247373 -4.154954 -4.1964626][-4.248126 -4.2245383 -4.2189322 -4.2207284 -4.2119141 -4.1928892 -4.1602182 -4.1369734 -4.144464 -4.1656504 -4.1721449 -4.1592174 -4.1528997 -4.1781387 -4.2171321][-4.2278066 -4.1925416 -4.1880136 -4.2007761 -4.1967192 -4.172389 -4.1291032 -4.0990353 -4.1186843 -4.1620378 -4.1846828 -4.1773825 -4.1689267 -4.1846976 -4.2168808][-4.200376 -4.1532 -4.1462564 -4.1703815 -4.1712818 -4.1442976 -4.0935616 -4.0582194 -4.085144 -4.1464977 -4.1879873 -4.1932549 -4.1897631 -4.1976895 -4.2189603][-4.1848269 -4.1335282 -4.1254067 -4.1568537 -4.1627326 -4.141242 -4.0959811 -4.0615606 -4.0830555 -4.1437516 -4.192008 -4.2074709 -4.2085447 -4.210885 -4.2239647][-4.1952367 -4.1452756 -4.1327887 -4.1572251 -4.1606417 -4.14646 -4.1168194 -4.091033 -4.1022644 -4.1509318 -4.1971903 -4.2190347 -4.2239528 -4.224967 -4.2340307][-4.2113538 -4.1665959 -4.1503739 -4.1671777 -4.1730804 -4.1773114 -4.174634 -4.16516 -4.1688094 -4.1944947 -4.2230406 -4.2373652 -4.241209 -4.2440734 -4.2494354][-4.2216344 -4.1859903 -4.1726241 -4.1877503 -4.1981921 -4.2145309 -4.22955 -4.2296185 -4.2282844 -4.2364283 -4.2473445 -4.2538586 -4.2607288 -4.2678676 -4.2730203][-4.2257981 -4.1972237 -4.1880302 -4.2041936 -4.21776 -4.23869 -4.2602377 -4.2641773 -4.2607522 -4.26161 -4.2636666 -4.2649236 -4.2718658 -4.2816234 -4.2870541][-4.2289481 -4.2048044 -4.1972785 -4.2143283 -4.2289939 -4.249548 -4.2715759 -4.2802939 -4.2805786 -4.2822452 -4.2822385 -4.2754846 -4.2728338 -4.2768526 -4.281599][-4.2390866 -4.2170587 -4.2087822 -4.2241468 -4.2324438 -4.241992 -4.2564349 -4.2658935 -4.2697611 -4.2749591 -4.2773371 -4.2694259 -4.2639914 -4.2681994 -4.2753935][-4.2490721 -4.2292905 -4.2230639 -4.2409635 -4.2481756 -4.2494869 -4.2543373 -4.2579937 -4.259161 -4.2632203 -4.2668304 -4.2638359 -4.2613859 -4.2662473 -4.2732]]...]
INFO - root - 2017-12-08 01:32:24.766959: step 77110, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 52h:27m:01s remains)
INFO - root - 2017-12-08 01:32:31.599545: step 77120, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 46h:54m:09s remains)
INFO - root - 2017-12-08 01:32:38.549267: step 77130, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 48h:05m:51s remains)
INFO - root - 2017-12-08 01:32:45.334317: step 77140, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.637 sec/batch; 45h:13m:02s remains)
INFO - root - 2017-12-08 01:32:52.228230: step 77150, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.728 sec/batch; 51h:39m:41s remains)
INFO - root - 2017-12-08 01:32:59.029242: step 77160, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 50h:35m:59s remains)
INFO - root - 2017-12-08 01:33:05.701989: step 77170, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 48h:46m:28s remains)
INFO - root - 2017-12-08 01:33:12.391536: step 77180, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 44h:06m:17s remains)
INFO - root - 2017-12-08 01:33:19.199349: step 77190, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 47h:14m:31s remains)
INFO - root - 2017-12-08 01:33:25.986647: step 77200, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 51h:21m:24s remains)
2017-12-08 01:33:26.735397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2465644 -4.2401848 -4.2508025 -4.2726412 -4.2929053 -4.3030262 -4.2973104 -4.282578 -4.2728438 -4.269289 -4.2711396 -4.2855959 -4.2993731 -4.3099933 -4.3198633][-4.181191 -4.1815238 -4.2036462 -4.2367287 -4.263844 -4.2745132 -4.2615585 -4.2386332 -4.2277765 -4.2280951 -4.2361574 -4.2594819 -4.2806597 -4.2958493 -4.3089738][-4.12362 -4.1307383 -4.1598692 -4.1972718 -4.225091 -4.2306652 -4.2084632 -4.178762 -4.171814 -4.1817193 -4.201014 -4.2352748 -4.2644353 -4.2847204 -4.3006964][-4.0934124 -4.1049395 -4.1329179 -4.1652994 -4.1872554 -4.1812873 -4.1442356 -4.1066928 -4.1109529 -4.1366 -4.1694632 -4.2142224 -4.2513504 -4.2762613 -4.2946792][-4.0906572 -4.1030965 -4.1255455 -4.1461811 -4.1553874 -4.1318808 -4.0717883 -4.0241704 -4.0496016 -4.09976 -4.1467009 -4.200685 -4.242847 -4.2708974 -4.2905569][-4.0959725 -4.1107187 -4.1291718 -4.1379142 -4.1334448 -4.0891356 -4.0009928 -3.9384186 -3.9897616 -4.0712724 -4.1338224 -4.1949673 -4.23932 -4.2686629 -4.2894073][-4.1073761 -4.1242714 -4.1384683 -4.1383786 -4.1212478 -4.0578804 -3.9374871 -3.8510902 -3.9313409 -4.0485325 -4.1281796 -4.1951108 -4.2406359 -4.2702179 -4.2906275][-4.12829 -4.145884 -4.1518593 -4.14507 -4.1195388 -4.0478306 -3.9115274 -3.8130765 -3.9115455 -4.0464344 -4.1317997 -4.1987042 -4.2434607 -4.2720914 -4.2911229][-4.1555009 -4.1723113 -4.1716094 -4.1616426 -4.1340766 -4.0718136 -3.9650722 -3.8966517 -3.9741023 -4.0829363 -4.1531558 -4.2081666 -4.2465582 -4.2724605 -4.2905946][-4.186316 -4.1975489 -4.1892409 -4.1745186 -4.1492844 -4.1051021 -4.040657 -4.0096679 -4.0643492 -4.1357064 -4.1834941 -4.2199669 -4.2485471 -4.2727408 -4.2911072][-4.2173271 -4.2226367 -4.2083611 -4.1892304 -4.1676846 -4.1386714 -4.1076031 -4.106894 -4.1494479 -4.1913681 -4.21741 -4.2352896 -4.2529097 -4.2746229 -4.2939939][-4.2459035 -4.2473078 -4.2320013 -4.2111192 -4.1917415 -4.1736083 -4.16345 -4.1788239 -4.2120256 -4.2377224 -4.2507029 -4.2558913 -4.2637868 -4.280725 -4.2998376][-4.2653165 -4.2667136 -4.2562551 -4.2383523 -4.2231817 -4.2129192 -4.216732 -4.2367973 -4.2604847 -4.2757783 -4.2831511 -4.2835755 -4.2846079 -4.2941608 -4.3086371][-4.2805805 -4.28006 -4.2733688 -4.26129 -4.2498817 -4.2459121 -4.2573991 -4.2775369 -4.2943668 -4.3045249 -4.3109708 -4.3107009 -4.308126 -4.3105597 -4.31924][-4.2978053 -4.2961431 -4.2920737 -4.28459 -4.2765012 -4.2747717 -4.2849741 -4.3002849 -4.3128753 -4.3213563 -4.3276982 -4.3284855 -4.3263292 -4.3255696 -4.3295364]]...]
INFO - root - 2017-12-08 01:33:33.612695: step 77210, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 48h:13m:02s remains)
INFO - root - 2017-12-08 01:33:40.513674: step 77220, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 52h:16m:07s remains)
INFO - root - 2017-12-08 01:33:47.266450: step 77230, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 50h:47m:56s remains)
INFO - root - 2017-12-08 01:33:54.014685: step 77240, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 47h:43m:58s remains)
INFO - root - 2017-12-08 01:34:00.721524: step 77250, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 45h:32m:37s remains)
INFO - root - 2017-12-08 01:34:07.557886: step 77260, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 46h:48m:30s remains)
INFO - root - 2017-12-08 01:34:14.424301: step 77270, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.759 sec/batch; 53h:47m:02s remains)
INFO - root - 2017-12-08 01:34:21.146735: step 77280, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 49h:51m:56s remains)
INFO - root - 2017-12-08 01:34:27.947949: step 77290, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 45h:54m:04s remains)
INFO - root - 2017-12-08 01:34:34.627923: step 77300, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.635 sec/batch; 45h:02m:18s remains)
2017-12-08 01:34:35.363404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14012 -4.1323195 -4.1232433 -4.1286321 -4.1383796 -4.142992 -4.1657052 -4.1935177 -4.1843777 -4.1640635 -4.1657724 -4.1638141 -4.1542549 -4.1463957 -4.1407952][-4.1478329 -4.1360683 -4.1149158 -4.1154394 -4.1288428 -4.13565 -4.1601834 -4.193234 -4.1886239 -4.177969 -4.1835022 -4.1769228 -4.1673374 -4.1684341 -4.17249][-4.1859784 -4.1752973 -4.1473851 -4.14271 -4.1580114 -4.16193 -4.1787658 -4.2048278 -4.2019892 -4.19804 -4.2044711 -4.1986704 -4.1933193 -4.2043228 -4.2188277][-4.2309632 -4.228343 -4.2045636 -4.19633 -4.207057 -4.2091546 -4.2165318 -4.2311788 -4.2252188 -4.2177892 -4.2181315 -4.2123423 -4.207118 -4.2198153 -4.2360358][-4.2554307 -4.2561927 -4.2380309 -4.2286205 -4.2283831 -4.2224288 -4.2287316 -4.2444053 -4.242846 -4.2351928 -4.23294 -4.2276978 -4.219605 -4.2251625 -4.2330489][-4.245039 -4.2417407 -4.2262092 -4.2108912 -4.1960235 -4.1785717 -4.1862845 -4.2116642 -4.2233829 -4.2239752 -4.2275519 -4.2336287 -4.2331858 -4.2366705 -4.2376423][-4.2091269 -4.1925249 -4.1746387 -4.1563358 -4.1278672 -4.0934591 -4.09808 -4.1371546 -4.169951 -4.1840634 -4.1979961 -4.2167463 -4.226645 -4.2329426 -4.2293863][-4.1554804 -4.1245461 -4.0997558 -4.080946 -4.0406842 -3.9905756 -3.9946024 -4.0467196 -4.0988297 -4.1319523 -4.1619706 -4.191998 -4.2114124 -4.2189569 -4.2105479][-4.0961661 -4.06521 -4.0516238 -4.0449328 -4.0077572 -3.9614339 -3.9660783 -4.0067945 -4.0595493 -4.1089387 -4.1559668 -4.1922174 -4.2099476 -4.2129173 -4.1965][-4.0626087 -4.0461593 -4.0555143 -4.0642457 -4.0394344 -4.0166788 -4.0234294 -4.0350509 -4.0630512 -4.1115365 -4.1687274 -4.2065897 -4.2204518 -4.2179985 -4.1940584][-4.0727844 -4.0708594 -4.0937028 -4.1145725 -4.10881 -4.1098866 -4.1161113 -4.1020188 -4.0984073 -4.12939 -4.1797347 -4.2155223 -4.2308283 -4.2249546 -4.1968107][-4.1247029 -4.1295376 -4.1531692 -4.1751175 -4.1860089 -4.1999359 -4.2018085 -4.176815 -4.1583571 -4.1702886 -4.2032914 -4.2317777 -4.2463679 -4.2388806 -4.2097011][-4.1929822 -4.1990471 -4.2122378 -4.23017 -4.24861 -4.2641349 -4.2627444 -4.2401543 -4.2205939 -4.2187581 -4.2347455 -4.2542133 -4.2650185 -4.2597303 -4.2389235][-4.2375808 -4.245338 -4.2502 -4.2622733 -4.2776484 -4.287724 -4.2861266 -4.2729445 -4.2599669 -4.2539415 -4.2601137 -4.2718439 -4.2791886 -4.2775841 -4.2686481][-4.2608204 -4.2649174 -4.2636075 -4.2694073 -4.2797685 -4.2852736 -4.2850013 -4.281867 -4.2789631 -4.2748938 -4.274117 -4.278791 -4.2803845 -4.28012 -4.2796512]]...]
INFO - root - 2017-12-08 01:34:42.098010: step 77310, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 51h:15m:31s remains)
INFO - root - 2017-12-08 01:34:48.896483: step 77320, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 47h:15m:17s remains)
INFO - root - 2017-12-08 01:34:55.681860: step 77330, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 44h:37m:48s remains)
INFO - root - 2017-12-08 01:35:02.521625: step 77340, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 47h:07m:45s remains)
INFO - root - 2017-12-08 01:35:09.434486: step 77350, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 52h:21m:40s remains)
INFO - root - 2017-12-08 01:35:16.261711: step 77360, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 50h:05m:08s remains)
INFO - root - 2017-12-08 01:35:23.083924: step 77370, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:23m:40s remains)
INFO - root - 2017-12-08 01:35:29.926447: step 77380, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 46h:15m:48s remains)
INFO - root - 2017-12-08 01:35:36.797344: step 77390, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.736 sec/batch; 52h:07m:56s remains)
INFO - root - 2017-12-08 01:35:43.437371: step 77400, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 51h:58m:49s remains)
2017-12-08 01:35:44.197929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1623182 -4.1782627 -4.1956191 -4.2012124 -4.2016592 -4.2114182 -4.2311811 -4.2498879 -4.26068 -4.2639418 -4.2568827 -4.2381282 -4.2190366 -4.2192903 -4.2390347][-4.1346064 -4.153121 -4.1758175 -4.1884732 -4.1985469 -4.2159123 -4.2389054 -4.2588315 -4.2709908 -4.2717123 -4.2602649 -4.2374353 -4.2218103 -4.2281365 -4.2475543][-4.1291108 -4.1480165 -4.173481 -4.1904984 -4.2064886 -4.2281785 -4.2485318 -4.2659893 -4.2779493 -4.2781835 -4.2659607 -4.2462525 -4.2387929 -4.247735 -4.2604241][-4.1446633 -4.16415 -4.1903033 -4.2060843 -4.2201443 -4.2367005 -4.2494068 -4.2609644 -4.2700424 -4.2705727 -4.2610431 -4.2475567 -4.2480769 -4.2559371 -4.2639604][-4.1649189 -4.1824784 -4.2048488 -4.2159953 -4.224052 -4.2311492 -4.236444 -4.2429338 -4.2490754 -4.2492795 -4.2415009 -4.2341089 -4.2387266 -4.2451386 -4.2542915][-4.1791883 -4.1849222 -4.1936216 -4.1960211 -4.1978383 -4.2010126 -4.207118 -4.2144485 -4.2202272 -4.2195663 -4.21314 -4.20953 -4.2159462 -4.2238436 -4.2371416][-4.1766672 -4.1630588 -4.1510229 -4.1411247 -4.1402993 -4.1493082 -4.1638303 -4.1754694 -4.1834669 -4.1840086 -4.1808214 -4.1827173 -4.1934729 -4.2070589 -4.2256484][-4.1730347 -4.1383147 -4.1052089 -4.0822697 -4.0796642 -4.0956612 -4.1197681 -4.1366491 -4.148037 -4.1512389 -4.1538167 -4.1657863 -4.1858082 -4.2057962 -4.2251692][-4.1801915 -4.1326761 -4.0878892 -4.0565705 -4.0510583 -4.0668869 -4.0918331 -4.1105924 -4.1214809 -4.126081 -4.1355834 -4.1591797 -4.1873484 -4.20943 -4.2259221][-4.199275 -4.1569152 -4.1164904 -4.0872092 -4.0768366 -4.0809622 -4.0952516 -4.107388 -4.1143408 -4.1187477 -4.1301594 -4.1580672 -4.1851535 -4.2034817 -4.2159581][-4.22521 -4.1970787 -4.170826 -4.1503429 -4.1372833 -4.1255541 -4.1212873 -4.1192832 -4.1177955 -4.117589 -4.1275654 -4.1530166 -4.17453 -4.1874752 -4.1952853][-4.2479963 -4.2328916 -4.2203555 -4.21102 -4.2004585 -4.1810169 -4.15972 -4.1403251 -4.1245236 -4.1127071 -4.1154847 -4.1365566 -4.1557436 -4.1681638 -4.1726627][-4.2592711 -4.25285 -4.248745 -4.2464538 -4.2409468 -4.2253966 -4.1976652 -4.1631842 -4.1306429 -4.1024017 -4.0912118 -4.1020179 -4.1209478 -4.1369319 -4.1435938][-4.2648535 -4.2626247 -4.261651 -4.2615767 -4.2578745 -4.2468014 -4.2235208 -4.1833134 -4.140389 -4.1015735 -4.0762577 -4.0733161 -4.0859289 -4.1017594 -4.1098871][-4.2688441 -4.2685256 -4.2694345 -4.2708216 -4.2686515 -4.2598114 -4.2415829 -4.2072482 -4.1673675 -4.1288934 -4.0980749 -4.0828905 -4.0801907 -4.0851722 -4.08915]]...]
INFO - root - 2017-12-08 01:35:51.010941: step 77410, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 46h:05m:03s remains)
INFO - root - 2017-12-08 01:35:57.882578: step 77420, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.766 sec/batch; 54h:15m:43s remains)
INFO - root - 2017-12-08 01:36:04.783093: step 77430, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 51h:27m:40s remains)
INFO - root - 2017-12-08 01:36:11.691929: step 77440, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.675 sec/batch; 47h:50m:31s remains)
INFO - root - 2017-12-08 01:36:18.418588: step 77450, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 44h:53m:06s remains)
INFO - root - 2017-12-08 01:36:25.282196: step 77460, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 51h:04m:25s remains)
INFO - root - 2017-12-08 01:36:32.100806: step 77470, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 50h:39m:27s remains)
INFO - root - 2017-12-08 01:36:38.675120: step 77480, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 45h:14m:01s remains)
INFO - root - 2017-12-08 01:36:45.453379: step 77490, loss = 2.04, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 45h:06m:46s remains)
INFO - root - 2017-12-08 01:36:52.114841: step 77500, loss = 2.11, batch loss = 2.05 (10.6 examples/sec; 0.757 sec/batch; 53h:35m:14s remains)
2017-12-08 01:36:52.843274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3314247 -4.33483 -4.3325443 -4.3317394 -4.3310003 -4.32458 -4.3068795 -4.2862396 -4.2630816 -4.2376323 -4.222084 -4.2205014 -4.22219 -4.2357879 -4.2604308][-4.3420272 -4.3495684 -4.3459949 -4.3364573 -4.32228 -4.3004723 -4.26971 -4.242692 -4.2231259 -4.2035751 -4.1966968 -4.2066178 -4.217628 -4.2363114 -4.2605891][-4.3414178 -4.3519669 -4.347394 -4.3268147 -4.2974119 -4.2603445 -4.2158761 -4.1842136 -4.1746349 -4.1677718 -4.1710081 -4.1896749 -4.2070322 -4.22955 -4.25349][-4.3268638 -4.338829 -4.33658 -4.3109093 -4.2734313 -4.2295518 -4.1720915 -4.1259079 -4.1236687 -4.1347985 -4.1514697 -4.17926 -4.2041392 -4.2331991 -4.2579923][-4.3075418 -4.3184443 -4.3162012 -4.2864347 -4.2441363 -4.1975741 -4.1221218 -4.0527887 -4.0638647 -4.1070151 -4.1494503 -4.19107 -4.224288 -4.2575588 -4.2787776][-4.286809 -4.2911348 -4.2876964 -4.2537303 -4.2061739 -4.1497855 -4.0510917 -3.9586957 -3.988194 -4.0690885 -4.1383119 -4.1937766 -4.2359996 -4.273757 -4.2918892][-4.2758336 -4.271749 -4.264205 -4.2293696 -4.1728182 -4.0962405 -3.9733219 -3.8728511 -3.9249892 -4.0317922 -4.1159644 -4.17759 -4.2247958 -4.265481 -4.28353][-4.2703729 -4.260016 -4.25069 -4.22424 -4.1694779 -4.0889215 -3.9724548 -3.8874688 -3.9405982 -4.0356054 -4.1073637 -4.1600838 -4.2054629 -4.24857 -4.2681966][-4.2602243 -4.2538357 -4.2486067 -4.2335544 -4.1951308 -4.1382189 -4.0658293 -4.0080214 -4.0264134 -4.0788479 -4.1238227 -4.1591153 -4.1995406 -4.2417603 -4.2628274][-4.2569056 -4.2569885 -4.2568851 -4.24679 -4.2208996 -4.1900353 -4.1531205 -4.1130538 -4.1063223 -4.1299071 -4.1548405 -4.1759996 -4.2088189 -4.2467628 -4.2666636][-4.2740006 -4.2778111 -4.2764311 -4.2647548 -4.2421508 -4.2249889 -4.2025309 -4.1674232 -4.1537356 -4.1657066 -4.178997 -4.1925015 -4.2195172 -4.2510624 -4.2700195][-4.3019819 -4.3017321 -4.2976494 -4.2860432 -4.2691631 -4.2578311 -4.2390294 -4.2054968 -4.1853552 -4.1831884 -4.1837406 -4.1907048 -4.2142978 -4.2410054 -4.2623277][-4.3133154 -4.31146 -4.307786 -4.3008213 -4.2900558 -4.2828851 -4.2672892 -4.23896 -4.2108212 -4.1915593 -4.1807752 -4.1858063 -4.2032108 -4.2207203 -4.2419167][-4.299262 -4.3004546 -4.3030491 -4.3054895 -4.3023076 -4.2967405 -4.2832484 -4.2635126 -4.2320857 -4.1981525 -4.1814613 -4.1890407 -4.2014847 -4.2080579 -4.2242837][-4.2697253 -4.2743387 -4.28499 -4.2975941 -4.3045354 -4.3052692 -4.2949185 -4.2836194 -4.2520843 -4.2098441 -4.1907926 -4.2013869 -4.2127886 -4.2132559 -4.2227464]]...]
INFO - root - 2017-12-08 01:36:59.576286: step 77510, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 44h:27m:26s remains)
INFO - root - 2017-12-08 01:37:06.383561: step 77520, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 46h:35m:46s remains)
INFO - root - 2017-12-08 01:37:13.231856: step 77530, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 50h:24m:59s remains)
INFO - root - 2017-12-08 01:37:20.026585: step 77540, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 50h:29m:49s remains)
INFO - root - 2017-12-08 01:37:26.820298: step 77550, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 48h:42m:47s remains)
INFO - root - 2017-12-08 01:37:33.616178: step 77560, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 44h:54m:14s remains)
INFO - root - 2017-12-08 01:37:40.420748: step 77570, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.639 sec/batch; 45h:16m:35s remains)
INFO - root - 2017-12-08 01:37:47.320414: step 77580, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.756 sec/batch; 53h:30m:27s remains)
INFO - root - 2017-12-08 01:37:54.034065: step 77590, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 49h:57m:15s remains)
INFO - root - 2017-12-08 01:38:00.593807: step 77600, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 47h:43m:56s remains)
2017-12-08 01:38:01.387449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3530192 -4.339673 -4.32744 -4.3214602 -4.3206034 -4.3071094 -4.277132 -4.242847 -4.2116876 -4.1931453 -4.2032852 -4.2414265 -4.2845678 -4.3074059 -4.3247795][-4.3640504 -4.3516564 -4.3348022 -4.3196297 -4.3102908 -4.2949104 -4.2673731 -4.2318387 -4.1995864 -4.1770263 -4.1848454 -4.2249932 -4.2707505 -4.299572 -4.3261595][-4.3624206 -4.3526173 -4.3340979 -4.3137121 -4.2966185 -4.275794 -4.2487321 -4.2140017 -4.1874366 -4.1693134 -4.1746035 -4.2116842 -4.2567863 -4.2909107 -4.3236017][-4.35032 -4.3473778 -4.3315291 -4.3114944 -4.286109 -4.25615 -4.2249093 -4.1942592 -4.1760907 -4.1649351 -4.1722693 -4.206295 -4.2472138 -4.2817392 -4.3154144][-4.3171568 -4.3261781 -4.3183689 -4.3004212 -4.2634773 -4.2189336 -4.17247 -4.14466 -4.1409779 -4.1480389 -4.1668439 -4.2013249 -4.2328277 -4.2631726 -4.2931228][-4.2798409 -4.29581 -4.2944279 -4.2744007 -4.2236819 -4.1610174 -4.0923362 -4.0602756 -4.0822887 -4.1280913 -4.1681423 -4.1998682 -4.217423 -4.2390985 -4.2604203][-4.2498426 -4.2721705 -4.2753811 -4.2510662 -4.1867366 -4.0997543 -4.001369 -3.95273 -4.0093656 -4.1147337 -4.1804276 -4.2013431 -4.202961 -4.2119088 -4.2181959][-4.2268624 -4.253159 -4.2592745 -4.2289715 -4.1486597 -4.0294638 -3.8950448 -3.8207419 -3.9073031 -4.0704317 -4.1618333 -4.1757221 -4.1682987 -4.1685014 -4.1660066][-4.2158637 -4.2421017 -4.2508759 -4.2189531 -4.1300845 -3.9942646 -3.8467579 -3.7658315 -3.8583643 -4.0353479 -4.1325006 -4.1455436 -4.1367311 -4.1354017 -4.1299028][-4.2062678 -4.2309213 -4.2463732 -4.2290406 -4.1519418 -4.0304728 -3.9087148 -3.8557696 -3.9369678 -4.0730772 -4.1443157 -4.1478815 -4.139307 -4.1389265 -4.1313672][-4.2210755 -4.2382283 -4.2536292 -4.2500291 -4.1994004 -4.1077061 -4.0180635 -3.990706 -4.0484653 -4.1289744 -4.1667542 -4.1632891 -4.158802 -4.1625009 -4.159996][-4.2477241 -4.2567811 -4.2719193 -4.2752752 -4.2503772 -4.1914258 -4.129848 -4.1161 -4.1502538 -4.18603 -4.1917543 -4.1871204 -4.1884995 -4.1994729 -4.200294][-4.2679534 -4.2725992 -4.2878971 -4.2947917 -4.2871075 -4.2489529 -4.2017021 -4.1861291 -4.197155 -4.2020783 -4.1892371 -4.1863828 -4.19758 -4.21737 -4.2254238][-4.2688327 -4.2746887 -4.2919245 -4.3012896 -4.3021493 -4.2784328 -4.243135 -4.2209086 -4.2134852 -4.2041235 -4.1879749 -4.1841645 -4.2000132 -4.2262959 -4.2453322][-4.2572951 -4.2689281 -4.2857533 -4.2991519 -4.3044944 -4.2904515 -4.2659755 -4.241931 -4.2254705 -4.2145004 -4.2053437 -4.2029982 -4.2195225 -4.2481256 -4.2669764]]...]
INFO - root - 2017-12-08 01:38:08.258159: step 77610, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.741 sec/batch; 52h:28m:39s remains)
INFO - root - 2017-12-08 01:38:15.069516: step 77620, loss = 2.09, batch loss = 2.04 (11.4 examples/sec; 0.701 sec/batch; 49h:36m:30s remains)
INFO - root - 2017-12-08 01:38:21.667566: step 77630, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 46h:24m:56s remains)
INFO - root - 2017-12-08 01:38:28.323118: step 77640, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 44h:24m:20s remains)
INFO - root - 2017-12-08 01:38:35.113932: step 77650, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 47h:27m:24s remains)
INFO - root - 2017-12-08 01:38:41.948209: step 77660, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.762 sec/batch; 53h:57m:54s remains)
INFO - root - 2017-12-08 01:38:48.785785: step 77670, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 44h:43m:42s remains)
INFO - root - 2017-12-08 01:38:55.620573: step 77680, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 45h:07m:09s remains)
INFO - root - 2017-12-08 01:39:02.406523: step 77690, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 44h:42m:58s remains)
INFO - root - 2017-12-08 01:39:09.056003: step 77700, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 51h:48m:33s remains)
2017-12-08 01:39:09.937704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15114 -4.14624 -4.1365185 -4.129148 -4.127841 -4.1222262 -4.1072531 -4.1006913 -4.1103477 -4.137413 -4.1590953 -4.1694875 -4.1760764 -4.1862969 -4.1926522][-4.1553621 -4.1481175 -4.1361885 -4.1215463 -4.1023607 -4.0751643 -4.04452 -4.0410523 -4.0678349 -4.1082749 -4.1405153 -4.1547432 -4.1641345 -4.1721425 -4.1724997][-4.1583643 -4.1493278 -4.135808 -4.1142712 -4.0808272 -4.0404024 -4.0036926 -4.0041175 -4.0419569 -4.0891838 -4.1264338 -4.1400142 -4.1430831 -4.141717 -4.1358333][-4.1646218 -4.1544476 -4.1432338 -4.1236405 -4.0940719 -4.0562277 -4.0211511 -4.0204439 -4.0579195 -4.1044865 -4.1372643 -4.1431389 -4.1330347 -4.1196041 -4.1111622][-4.1739407 -4.1678476 -4.1599851 -4.1456723 -4.1272926 -4.09486 -4.0589924 -4.0493617 -4.0805731 -4.1218762 -4.1454129 -4.14607 -4.1287389 -4.1076083 -4.1010728][-4.179893 -4.1743727 -4.1664643 -4.1579194 -4.1415887 -4.0984993 -4.0494409 -4.0312657 -4.065063 -4.109303 -4.1358891 -4.1450043 -4.1318688 -4.1133676 -4.1022925][-4.1773095 -4.1702795 -4.1582031 -4.1500325 -4.1259995 -4.0682178 -4.0015197 -3.9776869 -4.0218749 -4.0856285 -4.1306076 -4.1530786 -4.1489935 -4.13782 -4.1267004][-4.1628933 -4.1622987 -4.152051 -4.1460066 -4.1207256 -4.0589304 -3.9789579 -3.9488585 -4.0013175 -4.0805182 -4.1387339 -4.170424 -4.173193 -4.1674871 -4.1584549][-4.1345973 -4.1446152 -4.1440492 -4.1437836 -4.1309814 -4.0918479 -4.0244684 -3.9913218 -4.0349026 -4.1109066 -4.1657419 -4.1958933 -4.2008352 -4.1946959 -4.18608][-4.1277471 -4.147532 -4.1566615 -4.1626582 -4.1620436 -4.1459017 -4.1025577 -4.07085 -4.1013761 -4.1606922 -4.2002039 -4.21853 -4.2183676 -4.2059064 -4.1943312][-4.1429806 -4.1656661 -4.1791749 -4.1894288 -4.1952872 -4.1909657 -4.1645703 -4.136992 -4.1551237 -4.2007923 -4.22825 -4.2378616 -4.2323275 -4.2163525 -4.1967211][-4.1585326 -4.1773224 -4.1921926 -4.2045507 -4.21768 -4.2264519 -4.2148743 -4.1933689 -4.2025237 -4.236299 -4.25518 -4.2567344 -4.2418437 -4.2223649 -4.2036633][-4.1612062 -4.1774025 -4.1979303 -4.2141261 -4.2312 -4.2510605 -4.252398 -4.2383542 -4.2431974 -4.2683153 -4.2790751 -4.2702274 -4.2468615 -4.2244644 -4.2113247][-4.1653786 -4.18241 -4.2055216 -4.2236662 -4.2408552 -4.2646413 -4.2737646 -4.2644291 -4.2661433 -4.2812357 -4.2835546 -4.2705431 -4.2472138 -4.2255607 -4.2134991][-4.17544 -4.1904712 -4.2135091 -4.2333174 -4.248486 -4.2716136 -4.2846937 -4.2770863 -4.2742224 -4.2786517 -4.27582 -4.266602 -4.2497244 -4.231451 -4.2179165]]...]
INFO - root - 2017-12-08 01:39:16.657984: step 77710, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 46h:34m:25s remains)
INFO - root - 2017-12-08 01:39:23.451960: step 77720, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 50h:28m:35s remains)
INFO - root - 2017-12-08 01:39:30.298288: step 77730, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 51h:51m:17s remains)
INFO - root - 2017-12-08 01:39:37.090837: step 77740, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 47h:38m:11s remains)
INFO - root - 2017-12-08 01:39:43.888971: step 77750, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 43h:55m:57s remains)
INFO - root - 2017-12-08 01:39:50.883511: step 77760, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 51h:43m:29s remains)
INFO - root - 2017-12-08 01:39:57.709053: step 77770, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 49h:54m:26s remains)
INFO - root - 2017-12-08 01:40:04.465002: step 77780, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 46h:47m:36s remains)
INFO - root - 2017-12-08 01:40:11.037459: step 77790, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 44h:35m:41s remains)
INFO - root - 2017-12-08 01:40:17.655436: step 77800, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 45h:58m:48s remains)
2017-12-08 01:40:18.491509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3209963 -4.3131809 -4.293251 -4.2626629 -4.2219715 -4.1722465 -4.1375833 -4.1290817 -4.1398993 -4.168364 -4.18965 -4.1924605 -4.1848125 -4.1774683 -4.1745477][-4.3230305 -4.3145781 -4.2906981 -4.2523456 -4.2051125 -4.1533217 -4.120729 -4.1217475 -4.1405158 -4.1709166 -4.1907282 -4.1863074 -4.1722064 -4.1619968 -4.160708][-4.3245878 -4.3147449 -4.287137 -4.2426009 -4.188952 -4.1313066 -4.0951409 -4.101532 -4.125504 -4.1575146 -4.1775074 -4.1706319 -4.1553264 -4.1434846 -4.1392779][-4.3260603 -4.3165884 -4.2878728 -4.2395844 -4.1800408 -4.1130018 -4.070178 -4.0812192 -4.1099315 -4.1412611 -4.1602583 -4.1551952 -4.1435609 -4.1322684 -4.1219521][-4.3264694 -4.3183732 -4.2899671 -4.2374663 -4.1696892 -4.0908537 -4.0404329 -4.0562463 -4.0922213 -4.1261425 -4.1487436 -4.1528764 -4.1493182 -4.1421261 -4.129118][-4.3247828 -4.3182707 -4.2914276 -4.2355795 -4.1575937 -4.0630078 -4.0000768 -4.01874 -4.0686226 -4.1148663 -4.1472631 -4.1638117 -4.1697664 -4.1693969 -4.156765][-4.3234162 -4.3180375 -4.295404 -4.2418113 -4.1607265 -4.0585837 -3.9877527 -4.002563 -4.0626888 -4.122716 -4.16273 -4.1871719 -4.198216 -4.1981964 -4.1827435][-4.3240595 -4.3192525 -4.2999382 -4.2520361 -4.1777439 -4.0854473 -4.0242505 -4.0339365 -4.0883465 -4.14709 -4.18533 -4.2115126 -4.2230582 -4.2185335 -4.2015553][-4.3268409 -4.3227758 -4.3051758 -4.2610555 -4.1954818 -4.1192861 -4.0714369 -4.0748305 -4.1127739 -4.1608133 -4.1947346 -4.2224159 -4.234014 -4.2286806 -4.2161183][-4.3280663 -4.3231235 -4.3050323 -4.2645569 -4.2068014 -4.1427088 -4.1020164 -4.0940919 -4.1118426 -4.1467729 -4.1793585 -4.2117443 -4.2262092 -4.2247734 -4.2193966][-4.3253193 -4.3194375 -4.3016071 -4.2648487 -4.2132626 -4.1563435 -4.1127906 -4.0893321 -4.0905032 -4.1159968 -4.1489782 -4.1858087 -4.2058229 -4.210454 -4.2121291][-4.3195481 -4.3127046 -4.2965384 -4.2626281 -4.2119265 -4.1545563 -4.1056232 -4.0738616 -4.0665007 -4.0864134 -4.1179438 -4.1541166 -4.1776886 -4.187624 -4.1963725][-4.3128114 -4.3048868 -4.2901425 -4.2581124 -4.2066088 -4.14945 -4.1008973 -4.0706177 -4.0607362 -4.0744472 -4.0976572 -4.1258731 -4.1490989 -4.16268 -4.1762519][-4.3089437 -4.3002157 -4.2861857 -4.2564282 -4.2093935 -4.159966 -4.116406 -4.0867019 -4.0694571 -4.0713749 -4.0835967 -4.106411 -4.1333294 -4.1541929 -4.1704288][-4.309175 -4.301044 -4.2871714 -4.2582741 -4.2170582 -4.1785326 -4.1418257 -4.1115737 -4.0889049 -4.0854464 -4.0952849 -4.1191587 -4.1480703 -4.1683569 -4.1787834]]...]
INFO - root - 2017-12-08 01:40:25.273872: step 77810, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 45h:54m:24s remains)
INFO - root - 2017-12-08 01:40:32.069772: step 77820, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 45h:41m:26s remains)
INFO - root - 2017-12-08 01:40:38.931901: step 77830, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.758 sec/batch; 53h:39m:02s remains)
INFO - root - 2017-12-08 01:40:45.758129: step 77840, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.744 sec/batch; 52h:37m:42s remains)
INFO - root - 2017-12-08 01:40:52.498533: step 77850, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 48h:23m:18s remains)
INFO - root - 2017-12-08 01:40:59.231089: step 77860, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 45h:02m:05s remains)
INFO - root - 2017-12-08 01:41:05.975924: step 77870, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 45h:31m:08s remains)
INFO - root - 2017-12-08 01:41:12.818954: step 77880, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 50h:16m:09s remains)
INFO - root - 2017-12-08 01:41:19.670804: step 77890, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 50h:26m:32s remains)
INFO - root - 2017-12-08 01:41:26.327060: step 77900, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.657 sec/batch; 46h:28m:22s remains)
2017-12-08 01:41:27.114484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1372871 -4.1558404 -4.1678681 -4.16978 -4.1556845 -4.1350117 -4.1243482 -4.1318965 -4.1476903 -4.1628327 -4.1580024 -4.1301684 -4.092896 -4.068326 -4.0673766][-4.1105132 -4.1338816 -4.151886 -4.1619325 -4.1602149 -4.1495409 -4.1376028 -4.1439757 -4.165308 -4.1823545 -4.17608 -4.1455584 -4.1045504 -4.0816984 -4.082819][-4.1022763 -4.1284761 -4.1495972 -4.1600451 -4.1641045 -4.1582613 -4.1451063 -4.1409407 -4.1589074 -4.1751933 -4.1744294 -4.1497974 -4.1138659 -4.0984669 -4.1043606][-4.109035 -4.1313581 -4.1479087 -4.153039 -4.1518116 -4.1411023 -4.1153879 -4.0927095 -4.1028166 -4.1299806 -4.1496806 -4.1491203 -4.1374292 -4.14094 -4.1549945][-4.1308188 -4.1390858 -4.1434045 -4.1345854 -4.1165791 -4.0859275 -4.0386515 -3.9961534 -4.005167 -4.0546045 -4.10874 -4.1442375 -4.1660371 -4.1889706 -4.2081981][-4.1610508 -4.144875 -4.1233282 -4.0865183 -4.0387216 -3.9924877 -3.9392278 -3.8964565 -3.9130795 -3.9849663 -4.0777264 -4.1486359 -4.1973729 -4.2292018 -4.2460308][-4.1789956 -4.1346631 -4.0794239 -4.0080442 -3.9351451 -3.8956296 -3.8666792 -3.8495431 -3.8821948 -3.9688168 -4.0811744 -4.167522 -4.2232571 -4.2514019 -4.2565002][-4.1819935 -4.1146083 -4.0408192 -3.9621084 -3.8953485 -3.8882978 -3.8990552 -3.9089947 -3.9472532 -4.0242119 -4.1167059 -4.1853166 -4.2274776 -4.2447443 -4.2406158][-4.1639161 -4.0971441 -4.0390873 -3.9972098 -3.9744561 -3.9931915 -4.0188274 -4.0359092 -4.0623713 -4.1077185 -4.1534176 -4.1848359 -4.2051692 -4.211441 -4.2075133][-4.1247592 -4.0787897 -4.0618191 -4.0661178 -4.082418 -4.1155386 -4.1415987 -4.1530809 -4.1565013 -4.1634994 -4.1659532 -4.15909 -4.14915 -4.1478205 -4.1549311][-4.0887733 -4.0738745 -4.0971208 -4.1343322 -4.1699734 -4.2055316 -4.2251425 -4.2237105 -4.2022152 -4.1783352 -4.1487117 -4.1135688 -4.0815907 -4.0795684 -4.1024518][-4.0722108 -4.0902991 -4.1437454 -4.1975207 -4.235671 -4.2630491 -4.271235 -4.257709 -4.2212157 -4.1752138 -4.1224732 -4.0733957 -4.040534 -4.0461879 -4.0758858][-4.0786052 -4.1207414 -4.18526 -4.2436218 -4.2770472 -4.2938471 -4.2925892 -4.2734771 -4.232481 -4.1774478 -4.1088605 -4.0583177 -4.0384836 -4.0521641 -4.0819769][-4.0942979 -4.141778 -4.2004247 -4.2505865 -4.2778168 -4.2900729 -4.2887917 -4.2727342 -4.2393603 -4.190268 -4.1226215 -4.077929 -4.0713 -4.0848007 -4.1064758][-4.1133437 -4.155272 -4.1965976 -4.2269897 -4.2456079 -4.2595 -4.2676134 -4.2621565 -4.2402544 -4.2069883 -4.1555481 -4.1211519 -4.1200085 -4.1283207 -4.1397338]]...]
INFO - root - 2017-12-08 01:41:33.874999: step 77910, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 51h:00m:22s remains)
INFO - root - 2017-12-08 01:41:40.622672: step 77920, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 47h:33m:05s remains)
INFO - root - 2017-12-08 01:41:47.434645: step 77930, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 47h:09m:42s remains)
INFO - root - 2017-12-08 01:41:54.371877: step 77940, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 47h:52m:30s remains)
INFO - root - 2017-12-08 01:42:01.178898: step 77950, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.724 sec/batch; 51h:11m:04s remains)
INFO - root - 2017-12-08 01:42:07.863815: step 77960, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 50h:01m:05s remains)
INFO - root - 2017-12-08 01:42:14.649287: step 77970, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 49h:11m:05s remains)
INFO - root - 2017-12-08 01:42:21.398122: step 77980, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 45h:16m:07s remains)
INFO - root - 2017-12-08 01:42:28.138856: step 77990, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 47h:00m:42s remains)
INFO - root - 2017-12-08 01:42:34.777696: step 78000, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 50h:22m:15s remains)
2017-12-08 01:42:35.578496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2334805 -4.2372766 -4.2419219 -4.247149 -4.2473016 -4.2345157 -4.2099047 -4.1797528 -4.1561074 -4.1518445 -4.1659584 -4.1882071 -4.2082715 -4.223701 -4.2316427][-4.2118025 -4.2193818 -4.2275324 -4.2351203 -4.2352481 -4.2203703 -4.1918497 -4.1567826 -4.1324835 -4.1323934 -4.1534319 -4.1813126 -4.2033448 -4.2173443 -4.2225161][-4.2152929 -4.2314854 -4.2457089 -4.2554164 -4.25409 -4.2358327 -4.2028108 -4.162919 -4.1369767 -4.1394181 -4.1647534 -4.1943855 -4.2138667 -4.2220097 -4.2211175][-4.2373738 -4.25807 -4.2738748 -4.2827024 -4.2780437 -4.2558312 -4.219542 -4.1760139 -4.1495423 -4.1553473 -4.1852775 -4.2161622 -4.2313857 -4.2338481 -4.2293391][-4.2393293 -4.25541 -4.268075 -4.2771831 -4.2717538 -4.2461376 -4.2055435 -4.1562204 -4.1323409 -4.1474872 -4.1846952 -4.2182693 -4.2309184 -4.2297139 -4.2238178][-4.2039709 -4.2151408 -4.2271981 -4.2383652 -4.2298532 -4.1945086 -4.1398478 -4.0759192 -4.0547757 -4.0871534 -4.1403065 -4.1825142 -4.1962385 -4.1934628 -4.1898465][-4.1498976 -4.160233 -4.1713047 -4.1787343 -4.1557703 -4.0999613 -4.0194464 -3.9327219 -3.9229548 -3.9871292 -4.0639229 -4.1193089 -4.1354876 -4.1302848 -4.1267772][-4.1068163 -4.1188345 -4.1313004 -4.1322865 -4.0961089 -4.0268292 -3.9323015 -3.838681 -3.84453 -3.9308972 -4.0184975 -4.0751987 -4.0885134 -4.077323 -4.0688176][-4.1195045 -4.136797 -4.1531034 -4.1538014 -4.1223 -4.0677218 -3.9940956 -3.9258156 -3.9365561 -4.0058236 -4.0711493 -4.1093259 -4.1115341 -4.0940833 -4.0791054][-4.1823173 -4.2033749 -4.2210212 -4.2227831 -4.2015181 -4.1644096 -4.1137419 -4.0708337 -4.0797114 -4.124125 -4.1652808 -4.1866746 -4.1821942 -4.1627779 -4.1440606][-4.25075 -4.27428 -4.29181 -4.2940826 -4.27782 -4.2505 -4.2161555 -4.1900082 -4.19746 -4.2263412 -4.2522621 -4.2646279 -4.2605329 -4.2427974 -4.2218046][-4.2939138 -4.3114209 -4.3219857 -4.3204184 -4.3057895 -4.2848349 -4.2626247 -4.2487888 -4.2554502 -4.2749004 -4.2930741 -4.3031464 -4.3027716 -4.2906632 -4.2729325][-4.3125811 -4.3225679 -4.3286629 -4.3273993 -4.3173862 -4.303247 -4.2902284 -4.2839494 -4.2886553 -4.3006325 -4.3124619 -4.3201361 -4.3219833 -4.3149133 -4.3023515][-4.3074856 -4.315793 -4.3220115 -4.3229775 -4.3176351 -4.3101892 -4.3049264 -4.303051 -4.3073726 -4.3146048 -4.3213925 -4.3263969 -4.3278732 -4.324255 -4.3158927][-4.2671289 -4.2774878 -4.2869353 -4.2947049 -4.29815 -4.2980909 -4.29691 -4.2958727 -4.30009 -4.3066111 -4.3123326 -4.3162174 -4.3167672 -4.3130136 -4.3071342]]...]
INFO - root - 2017-12-08 01:42:42.469733: step 78010, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 47h:10m:03s remains)
INFO - root - 2017-12-08 01:42:49.348598: step 78020, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 50h:11m:05s remains)
INFO - root - 2017-12-08 01:42:56.258636: step 78030, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.759 sec/batch; 53h:40m:23s remains)
INFO - root - 2017-12-08 01:43:03.134246: step 78040, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 47h:32m:27s remains)
INFO - root - 2017-12-08 01:43:10.036442: step 78050, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 46h:05m:48s remains)
INFO - root - 2017-12-08 01:43:16.801153: step 78060, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 46h:58m:43s remains)
INFO - root - 2017-12-08 01:43:23.702583: step 78070, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 48h:55m:24s remains)
INFO - root - 2017-12-08 01:43:30.429953: step 78080, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 47h:36m:54s remains)
INFO - root - 2017-12-08 01:43:37.231563: step 78090, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 43h:41m:29s remains)
INFO - root - 2017-12-08 01:43:43.722538: step 78100, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 50h:02m:51s remains)
2017-12-08 01:43:44.443925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2467947 -4.2380614 -4.2313089 -4.2259626 -4.2138619 -4.2030091 -4.1903658 -4.18708 -4.189115 -4.18873 -4.1913843 -4.183404 -4.162034 -4.146822 -4.1421952][-4.2328835 -4.2172165 -4.2067561 -4.2007608 -4.1928782 -4.1857481 -4.1714654 -4.160459 -4.1627326 -4.172184 -4.1848927 -4.1923523 -4.1813755 -4.1671567 -4.1584988][-4.21563 -4.1952353 -4.1853366 -4.18407 -4.1899753 -4.19102 -4.17101 -4.1484156 -4.1537123 -4.1724024 -4.1910543 -4.2064238 -4.2041378 -4.1910887 -4.1785169][-4.1944623 -4.1738758 -4.1680179 -4.1713467 -4.1881289 -4.1942811 -4.1647682 -4.1298709 -4.1413603 -4.1705933 -4.1954355 -4.212441 -4.213963 -4.2000542 -4.1887112][-4.1815615 -4.1647091 -4.1595631 -4.1567554 -4.1684074 -4.1666422 -4.1157384 -4.062007 -4.0938659 -4.1475244 -4.1854014 -4.2071633 -4.2095904 -4.1926246 -4.1833067][-4.1685514 -4.1598692 -4.153029 -4.1374288 -4.1324849 -4.1087685 -4.0231223 -3.9448457 -4.0094991 -4.1029825 -4.1605048 -4.1864877 -4.18742 -4.1691942 -4.1590323][-4.15233 -4.1520271 -4.1464729 -4.1195979 -4.0944524 -4.0431352 -3.9205594 -3.8186498 -3.9181466 -4.0512357 -4.1269107 -4.1561036 -4.1600351 -4.1434903 -4.1308413][-4.1207714 -4.13058 -4.1327662 -4.1048727 -4.0728645 -4.0238237 -3.9165807 -3.8364437 -3.9341478 -4.0566912 -4.1214962 -4.1473055 -4.1534796 -4.1411214 -4.1322637][-4.10518 -4.1197615 -4.1263013 -4.1083198 -4.0894127 -4.0704527 -4.0208349 -3.9864304 -4.0522919 -4.1290073 -4.164309 -4.1747675 -4.1722517 -4.1603708 -4.1560473][-4.135819 -4.1441841 -4.1498041 -4.1430807 -4.1381574 -4.144093 -4.1359668 -4.1262808 -4.1626649 -4.2028365 -4.2149687 -4.2139058 -4.2031565 -4.1919675 -4.1899729][-4.1812129 -4.1867447 -4.1939178 -4.1956229 -4.1975794 -4.2137446 -4.22251 -4.2188106 -4.235693 -4.255321 -4.2560616 -4.2521167 -4.2422667 -4.2319827 -4.227788][-4.22038 -4.2218571 -4.2319226 -4.2456961 -4.251842 -4.2653747 -4.2739334 -4.2702441 -4.2771358 -4.2861862 -4.2812233 -4.2779732 -4.2725234 -4.2634654 -4.2596278][-4.2552633 -4.2523346 -4.2616944 -4.2786851 -4.2841511 -4.2911248 -4.2952414 -4.2927313 -4.2961693 -4.3032293 -4.2993984 -4.2933741 -4.2861619 -4.2781296 -4.2742677][-4.2884316 -4.2781935 -4.2812438 -4.2917919 -4.293221 -4.2934475 -4.2956882 -4.2954478 -4.2974443 -4.301827 -4.29924 -4.2870717 -4.2726493 -4.2638588 -4.2615733][-4.2970648 -4.282104 -4.2791128 -4.2830796 -4.2828746 -4.2811561 -4.2859163 -4.2911992 -4.2927103 -4.2929907 -4.2897887 -4.2769814 -4.2605805 -4.2517772 -4.2511907]]...]
INFO - root - 2017-12-08 01:43:51.299783: step 78110, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 44h:03m:23s remains)
INFO - root - 2017-12-08 01:43:58.196593: step 78120, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 46h:19m:01s remains)
INFO - root - 2017-12-08 01:44:05.071977: step 78130, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.735 sec/batch; 51h:57m:56s remains)
INFO - root - 2017-12-08 01:44:12.083113: step 78140, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 50h:47m:15s remains)
INFO - root - 2017-12-08 01:44:18.844160: step 78150, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 46h:54m:03s remains)
INFO - root - 2017-12-08 01:44:25.642239: step 78160, loss = 2.07, batch loss = 2.02 (13.1 examples/sec; 0.610 sec/batch; 43h:05m:56s remains)
INFO - root - 2017-12-08 01:44:32.462515: step 78170, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 48h:25m:19s remains)
INFO - root - 2017-12-08 01:44:39.302170: step 78180, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.729 sec/batch; 51h:31m:24s remains)
INFO - root - 2017-12-08 01:44:46.195526: step 78190, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 47h:45m:41s remains)
INFO - root - 2017-12-08 01:44:52.800319: step 78200, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 44h:23m:36s remains)
2017-12-08 01:44:53.567114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3509951 -4.3452206 -4.3364763 -4.3279309 -4.3197513 -4.3116651 -4.3042531 -4.3010912 -4.303874 -4.3075314 -4.31321 -4.3224335 -4.32944 -4.3348851 -4.3401208][-4.3440437 -4.3348441 -4.3224006 -4.3107691 -4.3004017 -4.2899575 -4.2778683 -4.2746563 -4.2824306 -4.2905083 -4.2961483 -4.3058958 -4.3133349 -4.319972 -4.32584][-4.3303285 -4.31591 -4.2977462 -4.2820354 -4.2670417 -4.2526875 -4.2322111 -4.2271266 -4.2415423 -4.261126 -4.2719011 -4.2828579 -4.2919397 -4.3022032 -4.3110433][-4.3128314 -4.2910795 -4.2643223 -4.239562 -4.2186947 -4.200048 -4.1732712 -4.1685472 -4.1931381 -4.2302446 -4.257194 -4.2770262 -4.2924137 -4.30539 -4.3132234][-4.2900972 -4.2573528 -4.2216916 -4.1879392 -4.1600685 -4.1370764 -4.1088715 -4.1041236 -4.1418729 -4.1965022 -4.2379847 -4.2676377 -4.2924366 -4.3098488 -4.3190832][-4.2690811 -4.2232776 -4.1758027 -4.1276846 -4.0774317 -4.0328736 -3.9952078 -3.9892483 -4.0425582 -4.1144204 -4.1703014 -4.2141662 -4.2538824 -4.2824869 -4.3027768][-4.25106 -4.1937566 -4.1310182 -4.058249 -3.9675748 -3.8807907 -3.8256207 -3.8265505 -3.90624 -4.0091763 -4.0911632 -4.1548066 -4.2125731 -4.2560382 -4.2871227][-4.2396975 -4.1802053 -4.1104684 -4.0263638 -3.9211459 -3.8161991 -3.7500563 -3.7575116 -3.8550358 -3.9739783 -4.0692959 -4.1441293 -4.207828 -4.2547393 -4.2900381][-4.2396808 -4.188355 -4.12823 -4.0633063 -3.9949632 -3.9277587 -3.8771472 -3.8815296 -3.9569259 -4.0496974 -4.1254516 -4.1871791 -4.237967 -4.2739596 -4.3022361][-4.25172 -4.2080903 -4.1604142 -4.1166458 -4.0790381 -4.0409117 -4.0055256 -4.0062761 -4.0586014 -4.125093 -4.18041 -4.2294836 -4.2696872 -4.2943974 -4.3162465][-4.2705636 -4.2352333 -4.2000494 -4.1710448 -4.1511092 -4.1317434 -4.1104031 -4.1109834 -4.148046 -4.1937346 -4.2312317 -4.26779 -4.2984681 -4.3144364 -4.3289948][-4.2884693 -4.2612729 -4.23714 -4.2206388 -4.2105489 -4.2012649 -4.1917272 -4.1954279 -4.2219429 -4.252213 -4.27427 -4.2980204 -4.3181553 -4.3286409 -4.3395061][-4.3042154 -4.2849975 -4.2686825 -4.2599154 -4.2568641 -4.2544312 -4.250823 -4.2547078 -4.2704487 -4.2880979 -4.300437 -4.3149114 -4.3275633 -4.3371391 -4.3458238][-4.3212748 -4.3078876 -4.2961378 -4.2896571 -4.2885227 -4.28823 -4.2865767 -4.2896814 -4.2986341 -4.3088818 -4.3174529 -4.3277168 -4.3368344 -4.343688 -4.350019][-4.3436055 -4.3361 -4.3294082 -4.3254137 -4.3246565 -4.3249106 -4.32461 -4.327466 -4.3326144 -4.3378897 -4.3422751 -4.3470798 -4.3510771 -4.3542447 -4.3573933]]...]
INFO - root - 2017-12-08 01:45:00.402076: step 78210, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.734 sec/batch; 51h:49m:37s remains)
INFO - root - 2017-12-08 01:45:07.262689: step 78220, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.687 sec/batch; 48h:29m:52s remains)
INFO - root - 2017-12-08 01:45:14.081669: step 78230, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 44h:14m:51s remains)
INFO - root - 2017-12-08 01:45:21.008966: step 78240, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 47h:48m:21s remains)
INFO - root - 2017-12-08 01:45:27.801673: step 78250, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 51h:36m:51s remains)
INFO - root - 2017-12-08 01:45:34.712096: step 78260, loss = 2.03, batch loss = 1.98 (11.1 examples/sec; 0.721 sec/batch; 50h:55m:46s remains)
INFO - root - 2017-12-08 01:45:41.555323: step 78270, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 47h:26m:38s remains)
INFO - root - 2017-12-08 01:45:48.451298: step 78280, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 46h:59m:24s remains)
INFO - root - 2017-12-08 01:45:55.357878: step 78290, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 48h:55m:26s remains)
INFO - root - 2017-12-08 01:46:02.107285: step 78300, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 50h:27m:45s remains)
2017-12-08 01:46:02.818913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2601981 -4.2591748 -4.2523794 -4.2331095 -4.2122931 -4.1909685 -4.1795592 -4.1818047 -4.1845536 -4.1748695 -4.1547928 -4.1437712 -4.1638851 -4.2009807 -4.2256913][-4.237123 -4.2352705 -4.23114 -4.2123804 -4.1887927 -4.162981 -4.1414418 -4.1342363 -4.136076 -4.1284037 -4.1048126 -4.0936136 -4.1188841 -4.166049 -4.1991653][-4.2348757 -4.2311311 -4.2252555 -4.2061672 -4.1825619 -4.1566367 -4.1217685 -4.0953755 -4.0921464 -4.090724 -4.0710616 -4.0614138 -4.0860157 -4.1335773 -4.1727147][-4.256258 -4.2455721 -4.2309494 -4.20607 -4.18132 -4.1552534 -4.111495 -4.0682197 -4.0587177 -4.0639787 -4.0536914 -4.0472574 -4.069541 -4.1124725 -4.1510081][-4.2818437 -4.2622523 -4.2323713 -4.1943817 -4.163259 -4.1417193 -4.1029878 -4.0563097 -4.0423956 -4.05568 -4.0614429 -4.0642414 -4.0813513 -4.1102643 -4.1343379][-4.293407 -4.2619267 -4.215734 -4.1672354 -4.1309133 -4.1187487 -4.1002164 -4.0670958 -4.0540352 -4.075109 -4.0932617 -4.1076021 -4.122261 -4.1369352 -4.1413417][-4.2963057 -4.2588344 -4.203124 -4.147634 -4.1069183 -4.0967741 -4.0973186 -4.0825262 -4.0779018 -4.1009431 -4.12484 -4.1493835 -4.1662087 -4.1746049 -4.1685524][-4.2946486 -4.2563982 -4.200747 -4.1467562 -4.1087008 -4.1008143 -4.1098328 -4.1018376 -4.0962954 -4.1182394 -4.1484275 -4.1792145 -4.1955171 -4.1983857 -4.1916852][-4.2931957 -4.2581635 -4.2113667 -4.1705756 -4.1428461 -4.13961 -4.1407013 -4.1192589 -4.0986004 -4.1182981 -4.1572847 -4.1898847 -4.2026381 -4.2047176 -4.2047024][-4.2956734 -4.2701287 -4.2343826 -4.2079134 -4.193306 -4.1932154 -4.1830735 -4.1483927 -4.1164274 -4.1312203 -4.1670837 -4.1913662 -4.1984744 -4.2098083 -4.222599][-4.293848 -4.2814884 -4.257257 -4.2380323 -4.2274837 -4.2255082 -4.212039 -4.178596 -4.150846 -4.1536379 -4.1695118 -4.1726384 -4.1735363 -4.195271 -4.2214832][-4.2707286 -4.2729278 -4.2601714 -4.2421083 -4.229023 -4.2202311 -4.2072277 -4.1831317 -4.1636896 -4.1572266 -4.1524415 -4.1320643 -4.1242318 -4.1546197 -4.1923041][-4.2250314 -4.2362328 -4.2313571 -4.2125106 -4.2021637 -4.1903372 -4.1764359 -4.1573763 -4.1438994 -4.1361451 -4.117733 -4.0814834 -4.0736713 -4.1137552 -4.1600823][-4.1736388 -4.1839013 -4.1835732 -4.1740465 -4.1754389 -4.1667767 -4.1521811 -4.1293154 -4.112689 -4.106617 -4.0771532 -4.0335922 -4.0323844 -4.0789256 -4.1309452][-4.1441703 -4.148653 -4.1483378 -4.1469941 -4.1620684 -4.16225 -4.1480536 -4.1192789 -4.0947442 -4.0793839 -4.0440435 -4.0077333 -4.0215673 -4.0723834 -4.1254754]]...]
INFO - root - 2017-12-08 01:46:09.598015: step 78310, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 44h:29m:08s remains)
INFO - root - 2017-12-08 01:46:16.502551: step 78320, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 50h:18m:07s remains)
INFO - root - 2017-12-08 01:46:23.353944: step 78330, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 50h:11m:42s remains)
INFO - root - 2017-12-08 01:46:30.132017: step 78340, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 47h:53m:09s remains)
INFO - root - 2017-12-08 01:46:36.903717: step 78350, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 43h:39m:37s remains)
INFO - root - 2017-12-08 01:46:43.586340: step 78360, loss = 2.03, batch loss = 1.97 (12.6 examples/sec; 0.637 sec/batch; 44h:57m:00s remains)
INFO - root - 2017-12-08 01:46:50.421111: step 78370, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 51h:26m:48s remains)
INFO - root - 2017-12-08 01:46:57.265421: step 78380, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.764 sec/batch; 53h:54m:52s remains)
INFO - root - 2017-12-08 01:47:04.015991: step 78390, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 46h:22m:27s remains)
INFO - root - 2017-12-08 01:47:10.599435: step 78400, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 45h:46m:47s remains)
2017-12-08 01:47:11.376465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2752433 -4.1988578 -4.113255 -4.060678 -4.0566263 -4.0944281 -4.1520233 -4.1940727 -4.2169752 -4.2403378 -4.2664247 -4.2797942 -4.2744064 -4.25715 -4.2363052][-4.2417569 -4.1437607 -4.0433631 -3.9853756 -3.9893684 -4.0428967 -4.1214514 -4.1801271 -4.21369 -4.2443056 -4.2786708 -4.2971354 -4.2911305 -4.2692952 -4.2450871][-4.2022605 -4.09 -3.988368 -3.9378805 -3.9521987 -4.0171781 -4.1050887 -4.1751151 -4.2133865 -4.2429981 -4.2775908 -4.3005157 -4.2982841 -4.279377 -4.2569814][-4.1659012 -4.054821 -3.9718847 -3.9375269 -3.9537721 -4.0151668 -4.0998249 -4.1741843 -4.2143793 -4.2438278 -4.2797484 -4.3071113 -4.3101482 -4.2944427 -4.2753372][-4.14374 -4.0526648 -3.9942908 -3.9672465 -3.9739943 -4.0201011 -4.092474 -4.1665258 -4.2104783 -4.2425404 -4.2771277 -4.3081174 -4.3176541 -4.3075471 -4.2926431][-4.1346745 -4.06738 -4.0274949 -4.0039849 -4.0006876 -4.0291061 -4.0887089 -4.1589971 -4.2071109 -4.2421212 -4.2742853 -4.3027105 -4.3147783 -4.3107991 -4.3002505][-4.1384993 -4.0967531 -4.0717859 -4.0518937 -4.0394526 -4.0496707 -4.0886989 -4.1462049 -4.1958213 -4.2357659 -4.2691846 -4.2943859 -4.3031797 -4.2976727 -4.2870045][-4.1613684 -4.1400118 -4.1233759 -4.1016521 -4.0800433 -4.0754452 -4.0948548 -4.1354046 -4.1815186 -4.2257881 -4.2612076 -4.2838311 -4.2878809 -4.2781286 -4.2655706][-4.1917839 -4.1824055 -4.1701713 -4.1494122 -4.1249905 -4.1117673 -4.1168451 -4.1427789 -4.1817894 -4.2239985 -4.2559109 -4.2724419 -4.2716365 -4.2598987 -4.2468929][-4.2263927 -4.2243657 -4.2155128 -4.1989179 -4.1790147 -4.1649766 -4.1645112 -4.1824808 -4.2128034 -4.245872 -4.2694373 -4.2797513 -4.2766943 -4.2657132 -4.2536798][-4.2616358 -4.2630978 -4.257936 -4.2473779 -4.2333965 -4.2223186 -4.2217603 -4.2344146 -4.2565084 -4.27922 -4.2934742 -4.2976809 -4.29356 -4.2851634 -4.2753882][-4.2989807 -4.3008947 -4.2974215 -4.2902718 -4.2806015 -4.2731075 -4.2725644 -4.2807913 -4.2959356 -4.31087 -4.3194814 -4.3211951 -4.3174391 -4.3102317 -4.3020611][-4.3260336 -4.3264794 -4.3238907 -4.3191919 -4.3129826 -4.30834 -4.3070621 -4.31077 -4.3190751 -4.3285875 -4.3356204 -4.3380384 -4.33544 -4.329349 -4.3222075][-4.3376889 -4.3369179 -4.3347583 -4.3319221 -4.3286624 -4.3265023 -4.3257346 -4.3268118 -4.329896 -4.3348575 -4.3398356 -4.3421831 -4.3412566 -4.337203 -4.3316131][-4.3360419 -4.334094 -4.3331509 -4.3323522 -4.3315148 -4.3310428 -4.3310733 -4.3317585 -4.33334 -4.3354769 -4.3375154 -4.3386054 -4.3381619 -4.3361793 -4.3332381]]...]
INFO - root - 2017-12-08 01:47:17.984269: step 78410, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 46h:07m:41s remains)
INFO - root - 2017-12-08 01:47:24.665153: step 78420, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 44h:14m:16s remains)
INFO - root - 2017-12-08 01:47:31.533328: step 78430, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 48h:08m:43s remains)
INFO - root - 2017-12-08 01:47:38.401210: step 78440, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.684 sec/batch; 48h:15m:03s remains)
INFO - root - 2017-12-08 01:47:45.225185: step 78450, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 46h:59m:35s remains)
INFO - root - 2017-12-08 01:47:52.024899: step 78460, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 46h:19m:50s remains)
INFO - root - 2017-12-08 01:47:58.817206: step 78470, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 46h:23m:19s remains)
INFO - root - 2017-12-08 01:48:05.545227: step 78480, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 48h:55m:59s remains)
INFO - root - 2017-12-08 01:48:12.359386: step 78490, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 50h:56m:26s remains)
INFO - root - 2017-12-08 01:48:18.963292: step 78500, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 45h:06m:40s remains)
2017-12-08 01:48:19.635934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2945642 -4.2838912 -4.2714272 -4.2611632 -4.2529793 -4.2461767 -4.2448959 -4.249774 -4.2528472 -4.2519407 -4.2470422 -4.2333379 -4.2228136 -4.2148285 -4.2123442][-4.2962914 -4.2852421 -4.2724581 -4.2636642 -4.2565184 -4.2478051 -4.2402949 -4.244453 -4.2497168 -4.2502861 -4.2427316 -4.2216392 -4.2022533 -4.1879025 -4.1884475][-4.3027349 -4.2909479 -4.2756619 -4.2652512 -4.2551703 -4.2423239 -4.23009 -4.236732 -4.2445345 -4.2436 -4.2310176 -4.2059479 -4.1844635 -4.1674066 -4.1741934][-4.2987928 -4.2836571 -4.2662535 -4.2554922 -4.2397456 -4.220634 -4.2065291 -4.2196612 -4.2321124 -4.2282524 -4.2111907 -4.1893663 -4.1719356 -4.1569905 -4.1681542][-4.2779946 -4.2632151 -4.2454381 -4.2293348 -4.2040248 -4.1760421 -4.157208 -4.1756506 -4.1961017 -4.1915913 -4.1746478 -4.1601858 -4.1480675 -4.1390443 -4.152617][-4.2352839 -4.2148142 -4.1958556 -4.1770306 -4.1475954 -4.1117787 -4.0821433 -4.1048594 -4.1347 -4.1333423 -4.1248074 -4.1233039 -4.1194568 -4.1155596 -4.1328177][-4.189239 -4.1546049 -4.1277227 -4.1045589 -4.0674291 -4.0158081 -3.9680409 -4.0000772 -4.0542536 -4.0727549 -4.0832219 -4.0963759 -4.1045642 -4.1082921 -4.1250963][-4.1865554 -4.1436887 -4.1103668 -4.0832958 -4.0447721 -3.991559 -3.9333537 -3.9615834 -4.0277686 -4.0624371 -4.0856581 -4.1070576 -4.1229229 -4.1320949 -4.1475959][-4.2309914 -4.1975269 -4.1667714 -4.1372852 -4.1056657 -4.0696244 -4.018641 -4.0333509 -4.0823994 -4.110323 -4.1296425 -4.1520352 -4.1725416 -4.1830678 -4.1986165][-4.2882748 -4.2733617 -4.2511892 -4.2230582 -4.1950812 -4.1666393 -4.1260066 -4.1318169 -4.1669345 -4.1865845 -4.1960974 -4.2098074 -4.2263403 -4.2319293 -4.2452416][-4.3244762 -4.322268 -4.3090019 -4.2864451 -4.2594948 -4.2352028 -4.205646 -4.2110205 -4.2356138 -4.2524061 -4.2573085 -4.2651882 -4.2741175 -4.27139 -4.2809935][-4.329576 -4.333 -4.3289013 -4.3174934 -4.2988935 -4.2820635 -4.2615848 -4.2671747 -4.2828546 -4.2964716 -4.299253 -4.3008986 -4.3052864 -4.30149 -4.3072724][-4.322196 -4.3259673 -4.3268013 -4.3234234 -4.3143592 -4.30629 -4.2962003 -4.3028049 -4.3141036 -4.3234005 -4.3242803 -4.320303 -4.3191123 -4.3157778 -4.318326][-4.3133321 -4.316555 -4.320096 -4.3215537 -4.3194141 -4.3156352 -4.311182 -4.3147297 -4.3217764 -4.3285084 -4.32936 -4.3251343 -4.3216343 -4.3176618 -4.3155336][-4.3128257 -4.3160267 -4.3189483 -4.3210082 -4.3214865 -4.3207765 -4.3188891 -4.3199625 -4.3220072 -4.32434 -4.3249574 -4.3228788 -4.3214211 -4.3204632 -4.3197789]]...]
INFO - root - 2017-12-08 01:48:26.490992: step 78510, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 51h:07m:55s remains)
INFO - root - 2017-12-08 01:48:33.277491: step 78520, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 0.754 sec/batch; 53h:09m:55s remains)
INFO - root - 2017-12-08 01:48:40.090188: step 78530, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.658 sec/batch; 46h:27m:09s remains)
INFO - root - 2017-12-08 01:48:46.866661: step 78540, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 45h:25m:43s remains)
INFO - root - 2017-12-08 01:48:53.596864: step 78550, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 45h:38m:29s remains)
INFO - root - 2017-12-08 01:49:00.401753: step 78560, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.699 sec/batch; 49h:18m:47s remains)
INFO - root - 2017-12-08 01:49:07.221452: step 78570, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 50h:25m:16s remains)
INFO - root - 2017-12-08 01:49:14.074059: step 78580, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 48h:54m:12s remains)
INFO - root - 2017-12-08 01:49:20.952745: step 78590, loss = 2.06, batch loss = 2.00 (13.3 examples/sec; 0.603 sec/batch; 42h:30m:12s remains)
INFO - root - 2017-12-08 01:49:27.604625: step 78600, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 47h:27m:18s remains)
2017-12-08 01:49:28.313833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2225428 -4.2225051 -4.2216725 -4.2321148 -4.2380333 -4.2332606 -4.2267871 -4.2162251 -4.2055407 -4.1993523 -4.2019463 -4.2161403 -4.2260842 -4.2242737 -4.2178483][-4.2328148 -4.2366548 -4.2334995 -4.23382 -4.2287741 -4.2183924 -4.2051258 -4.1850071 -4.1676927 -4.1575246 -4.1581 -4.1770744 -4.1915412 -4.1939573 -4.1977119][-4.2446904 -4.248105 -4.2441974 -4.2391958 -4.2283092 -4.21986 -4.2070866 -4.1881657 -4.1728282 -4.1626625 -4.1601276 -4.1750131 -4.1842175 -4.1828761 -4.1902261][-4.2261343 -4.220439 -4.2119894 -4.2058706 -4.1982222 -4.1929092 -4.1809359 -4.1698375 -4.1692195 -4.166523 -4.1635857 -4.1759014 -4.1803455 -4.1729994 -4.1782527][-4.1857171 -4.1706085 -4.16008 -4.1488361 -4.1341367 -4.1239281 -4.1038547 -4.0936995 -4.1089935 -4.1269627 -4.1390743 -4.1592321 -4.162498 -4.1452665 -4.1462893][-4.1366377 -4.1136689 -4.0969076 -4.0730209 -4.04597 -4.0319905 -3.9939344 -3.971873 -4.0083642 -4.0572047 -4.0911727 -4.1173296 -4.1230297 -4.1048164 -4.1020565][-4.0719519 -4.0442514 -4.0136542 -3.9733317 -3.9302378 -3.8979378 -3.8146482 -3.7593756 -3.8403339 -3.9451215 -4.0076761 -4.0406733 -4.0514879 -4.0410881 -4.0422826][-4.0611176 -4.0392084 -4.0134258 -3.975415 -3.9351432 -3.8978777 -3.8003678 -3.7209203 -3.7989666 -3.9119711 -3.9812734 -4.0180826 -4.0409679 -4.0406914 -4.0461984][-4.1248255 -4.1144257 -4.1056452 -4.0902705 -4.0695539 -4.0521455 -3.9981742 -3.9545145 -3.9937475 -4.0579228 -4.10162 -4.1229367 -4.1382737 -4.1372457 -4.1361952][-4.2133355 -4.2096968 -4.2106228 -4.2050529 -4.2014651 -4.1971173 -4.16979 -4.1483784 -4.1690865 -4.2097216 -4.2345862 -4.238986 -4.2423773 -4.243413 -4.2389][-4.2776427 -4.279798 -4.2807336 -4.2763596 -4.2743282 -4.2723455 -4.2573466 -4.2442508 -4.2557812 -4.2816067 -4.2968764 -4.2950134 -4.2936716 -4.29574 -4.28927][-4.2791247 -4.2816534 -4.2826014 -4.2797861 -4.2768173 -4.2802138 -4.278739 -4.2769566 -4.2852254 -4.3024793 -4.3141828 -4.3087554 -4.3018327 -4.3041553 -4.2981648][-4.2445421 -4.2463565 -4.2456517 -4.2445593 -4.2437229 -4.25038 -4.2595577 -4.266418 -4.277565 -4.2880449 -4.2920213 -4.282455 -4.2739596 -4.2785611 -4.2744293][-4.2082272 -4.209723 -4.2063122 -4.2096477 -4.2132921 -4.2200308 -4.2319274 -4.240798 -4.2483754 -4.2510157 -4.2481723 -4.2352219 -4.2256293 -4.2308412 -4.2277431][-4.1890664 -4.1863751 -4.1834035 -4.1928992 -4.2024226 -4.2046747 -4.211514 -4.2181129 -4.2220354 -4.2220063 -4.2162604 -4.2036958 -4.1976533 -4.2053962 -4.2041535]]...]
INFO - root - 2017-12-08 01:49:35.159877: step 78610, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 47h:35m:52s remains)
INFO - root - 2017-12-08 01:49:41.879051: step 78620, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 44h:45m:35s remains)
INFO - root - 2017-12-08 01:49:48.667015: step 78630, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 44h:26m:31s remains)
INFO - root - 2017-12-08 01:49:55.628633: step 78640, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 49h:42m:48s remains)
INFO - root - 2017-12-08 01:50:02.498736: step 78650, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 49h:27m:21s remains)
INFO - root - 2017-12-08 01:50:09.352066: step 78660, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 49h:45m:36s remains)
INFO - root - 2017-12-08 01:50:16.120178: step 78670, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 45h:39m:26s remains)
INFO - root - 2017-12-08 01:50:22.905814: step 78680, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 46h:08m:26s remains)
INFO - root - 2017-12-08 01:50:29.763092: step 78690, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 49h:35m:38s remains)
INFO - root - 2017-12-08 01:50:36.396435: step 78700, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 50h:19m:17s remains)
2017-12-08 01:50:37.110405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2690477 -4.24394 -4.2111292 -4.1872292 -4.1779385 -4.1659164 -4.1665487 -4.1779351 -4.1812129 -4.1899385 -4.2163777 -4.2537522 -4.2767749 -4.2775764 -4.2674146][-4.2548728 -4.2320209 -4.2005939 -4.1776018 -4.1636896 -4.1423483 -4.1327939 -4.1391816 -4.1395812 -4.1499262 -4.18228 -4.2286754 -4.2613826 -4.2675009 -4.2599683][-4.2289777 -4.2054358 -4.1757841 -4.1555376 -4.1431689 -4.1220546 -4.1071534 -4.1035972 -4.0973086 -4.11185 -4.1539316 -4.2049913 -4.2429976 -4.2546959 -4.2537637][-4.20714 -4.1793523 -4.1485572 -4.1285024 -4.1181164 -4.1018219 -4.0878768 -4.0830441 -4.0734816 -4.08774 -4.1339149 -4.1843886 -4.2239647 -4.2408891 -4.2475767][-4.1987653 -4.1699429 -4.1423445 -4.121769 -4.1084108 -4.0930228 -4.0822387 -4.0768065 -4.0616474 -4.0696292 -4.1136794 -4.1615171 -4.2018585 -4.2254148 -4.237637][-4.1810732 -4.1591144 -4.13802 -4.1174841 -4.0967603 -4.0767441 -4.065351 -4.0553408 -4.0312843 -4.0336466 -4.0718312 -4.1144257 -4.1583791 -4.1918597 -4.2136464][-4.1464176 -4.1404657 -4.116385 -4.0844641 -4.0454555 -4.0103264 -3.9992943 -3.9903171 -3.9628122 -3.9638968 -3.9999804 -4.0450268 -4.0990653 -4.1479988 -4.1877508][-4.09522 -4.1048436 -4.0734758 -4.0244503 -3.9628088 -3.9121947 -3.9059434 -3.9076757 -3.8867869 -3.8911018 -3.9292619 -3.9837549 -4.0552187 -4.1248159 -4.1813335][-4.0719404 -4.0876627 -4.0601244 -4.0079894 -3.9425166 -3.8932695 -3.8908966 -3.9025223 -3.8904467 -3.8934693 -3.9268801 -3.9839864 -4.0634265 -4.1405172 -4.2014561][-4.0952663 -4.1074944 -4.0889325 -4.0517673 -4.0055718 -3.9706702 -3.9677463 -3.979646 -3.9747889 -3.9775348 -4.0017633 -4.0502672 -4.1183124 -4.1874208 -4.240963][-4.1670556 -4.1757236 -4.1673155 -4.1425695 -4.1114144 -4.0912414 -4.0892849 -4.0978332 -4.0959568 -4.1016908 -4.118885 -4.1533918 -4.1993885 -4.2486186 -4.2868361][-4.2484255 -4.2513366 -4.2490478 -4.2322574 -4.2112274 -4.2024193 -4.2072444 -4.2161345 -4.2165375 -4.2210441 -4.229012 -4.2470055 -4.2714744 -4.301733 -4.3262811][-4.3134141 -4.3114614 -4.3120847 -4.3044896 -4.2912469 -4.2868719 -4.292778 -4.299768 -4.2998357 -4.2995033 -4.3000889 -4.306591 -4.3177919 -4.3353572 -4.3487921][-4.3395486 -4.3370471 -4.3386035 -4.336134 -4.3307495 -4.3301396 -4.3330603 -4.3379016 -4.3389449 -4.3379107 -4.33641 -4.3388591 -4.3443842 -4.3523583 -4.3578625][-4.3450236 -4.3440814 -4.3442912 -4.3438931 -4.3424797 -4.3419485 -4.3429766 -4.3470807 -4.3503885 -4.3505321 -4.3496776 -4.3503647 -4.3529134 -4.3561773 -4.3589473]]...]
INFO - root - 2017-12-08 01:50:44.028607: step 78710, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 49h:33m:38s remains)
INFO - root - 2017-12-08 01:50:50.711945: step 78720, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 46h:13m:52s remains)
INFO - root - 2017-12-08 01:50:57.616771: step 78730, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 48h:25m:21s remains)
INFO - root - 2017-12-08 01:51:04.355869: step 78740, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.648 sec/batch; 45h:39m:23s remains)
INFO - root - 2017-12-08 01:51:11.164550: step 78750, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 49h:42m:08s remains)
INFO - root - 2017-12-08 01:51:17.951166: step 78760, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 51h:17m:29s remains)
INFO - root - 2017-12-08 01:51:24.818417: step 78770, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 47h:57m:15s remains)
INFO - root - 2017-12-08 01:51:31.517051: step 78780, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 46h:24m:10s remains)
INFO - root - 2017-12-08 01:51:38.356967: step 78790, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 46h:11m:45s remains)
INFO - root - 2017-12-08 01:51:45.062653: step 78800, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 52h:03m:15s remains)
2017-12-08 01:51:45.742055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3065014 -4.2965665 -4.2801094 -4.2602091 -4.2406034 -4.2281084 -4.2306371 -4.2456913 -4.26163 -4.2743454 -4.2863183 -4.2911677 -4.2900887 -4.2855935 -4.2788005][-4.3150268 -4.3019638 -4.2784967 -4.2476716 -4.2173333 -4.200356 -4.2069511 -4.2305098 -4.2549963 -4.2725158 -4.2838068 -4.28567 -4.282805 -4.2788839 -4.2732553][-4.316618 -4.2971177 -4.2642512 -4.2178454 -4.1717939 -4.1508079 -4.1694183 -4.2082195 -4.2459636 -4.2726936 -4.2825179 -4.2797451 -4.2739925 -4.27088 -4.2693067][-4.3131638 -4.2831516 -4.2391028 -4.1776452 -4.1181245 -4.0957241 -4.1308651 -4.1901426 -4.2428675 -4.27784 -4.2867427 -4.2806387 -4.2717242 -4.2684832 -4.2703667][-4.3012309 -4.2571754 -4.1967368 -4.1199994 -4.0504255 -4.0358272 -4.092216 -4.1728358 -4.2400804 -4.2819819 -4.2908645 -4.2850347 -4.2762661 -4.2738786 -4.2776017][-4.2856669 -4.2253957 -4.143899 -4.0491114 -3.9738631 -3.9752033 -4.0562592 -4.1554575 -4.2333841 -4.2821293 -4.2959542 -4.2939725 -4.2886248 -4.2873778 -4.29116][-4.2669597 -4.1898236 -4.0848985 -3.969012 -3.8855748 -3.9051166 -4.01244 -4.1275611 -4.2149444 -4.273109 -4.2975254 -4.3037381 -4.3026094 -4.3019123 -4.3057809][-4.2411971 -4.1526241 -4.0344095 -3.9026425 -3.8120351 -3.8479414 -3.9754019 -4.0971961 -4.1892424 -4.25465 -4.289237 -4.3033686 -4.3061571 -4.30617 -4.3107815][-4.2108979 -4.1234312 -4.0131969 -3.8925416 -3.8173323 -3.8642385 -3.9868956 -4.0970469 -4.1783404 -4.2394838 -4.2765093 -4.2921476 -4.294497 -4.2940035 -4.2983894][-4.1911049 -4.1136346 -4.0240574 -3.9330161 -3.8867774 -3.9354446 -4.0321097 -4.1159325 -4.180625 -4.2340164 -4.2678652 -4.278975 -4.2786522 -4.2768583 -4.2796741][-4.1869421 -4.1287622 -4.0633459 -4.0013919 -3.9765949 -4.0180178 -4.0870185 -4.1485496 -4.1968889 -4.2362876 -4.260726 -4.2652969 -4.2626224 -4.2596722 -4.2604527][-4.2042727 -4.1660118 -4.1224666 -4.08301 -4.0699062 -4.1009588 -4.1464214 -4.1866817 -4.2177095 -4.2409549 -4.2549548 -4.2560425 -4.2532744 -4.2502623 -4.2496281][-4.2350488 -4.2137384 -4.1899996 -4.16942 -4.1640005 -4.1856275 -4.213479 -4.237577 -4.2538795 -4.2623858 -4.2647567 -4.2622128 -4.2592525 -4.2570333 -4.2557755][-4.2705994 -4.2644687 -4.2582941 -4.251925 -4.2519145 -4.2640862 -4.2770329 -4.2863588 -4.2902489 -4.2883835 -4.2836113 -4.2794552 -4.2776079 -4.2759604 -4.2744913][-4.3013582 -4.3044415 -4.3065515 -4.3072972 -4.3098879 -4.3160157 -4.3196826 -4.3203564 -4.3179183 -4.3115416 -4.3050365 -4.3009844 -4.3001504 -4.299067 -4.2975535]]...]
INFO - root - 2017-12-08 01:51:52.511739: step 78810, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 43h:23m:08s remains)
INFO - root - 2017-12-08 01:51:59.316851: step 78820, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 47h:39m:27s remains)
INFO - root - 2017-12-08 01:52:06.100314: step 78830, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 50h:19m:06s remains)
INFO - root - 2017-12-08 01:52:12.920833: step 78840, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 51h:49m:21s remains)
INFO - root - 2017-12-08 01:52:19.646714: step 78850, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 47h:19m:39s remains)
INFO - root - 2017-12-08 01:52:26.455366: step 78860, loss = 2.03, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 47h:05m:47s remains)
INFO - root - 2017-12-08 01:52:33.393065: step 78870, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 46h:13m:01s remains)
INFO - root - 2017-12-08 01:52:40.246071: step 78880, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 49h:55m:55s remains)
INFO - root - 2017-12-08 01:52:47.069906: step 78890, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 49h:03m:40s remains)
INFO - root - 2017-12-08 01:52:53.712330: step 78900, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 45h:49m:31s remains)
2017-12-08 01:52:54.415398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.142869 -4.1286507 -4.1481695 -4.140718 -4.1397285 -4.1419978 -4.1223373 -4.1316218 -4.1255245 -4.118371 -4.1230278 -4.104775 -4.1074166 -4.0931082 -4.0871153][-4.2000012 -4.182528 -4.1949959 -4.1842275 -4.1802058 -4.1853466 -4.1673121 -4.1755061 -4.1764121 -4.1729488 -4.1797175 -4.1605768 -4.1618247 -4.1536345 -4.1520429][-4.2242956 -4.2043557 -4.2120633 -4.2004766 -4.1968369 -4.2048969 -4.1894984 -4.197206 -4.2008376 -4.2029443 -4.2071953 -4.1848626 -4.1860428 -4.1832867 -4.1886878][-4.2244229 -4.2004819 -4.1995549 -4.1866903 -4.1820526 -4.1939192 -4.1855311 -4.1937284 -4.2016039 -4.2072144 -4.2093482 -4.1873865 -4.1887388 -4.1886582 -4.1964827][-4.2040529 -4.1759133 -4.1637492 -4.1471977 -4.1422124 -4.1585789 -4.1593056 -4.1694388 -4.1814656 -4.1892309 -4.191412 -4.1755929 -4.1795039 -4.1814404 -4.1851139][-4.1683173 -4.1382222 -4.1140585 -4.090323 -4.086884 -4.1106019 -4.1266332 -4.14267 -4.1585135 -4.1662016 -4.1672482 -4.158771 -4.163064 -4.165565 -4.1665816][-4.1391759 -4.1052823 -4.0734353 -4.0493903 -4.0505185 -4.0833859 -4.1120768 -4.1328349 -4.1496196 -4.1547232 -4.1501169 -4.1446257 -4.1483426 -4.1478658 -4.1459208][-4.120079 -4.0864916 -4.0542903 -4.0406675 -4.0476956 -4.0842547 -4.11617 -4.13488 -4.1467409 -4.144794 -4.1342397 -4.1281509 -4.1275768 -4.1244097 -4.1229305][-4.0979486 -4.0702677 -4.0478177 -4.0548368 -4.0730662 -4.1118746 -4.1427636 -4.156981 -4.159699 -4.14614 -4.1244588 -4.1118417 -4.1045084 -4.09787 -4.0995955][-4.0895104 -4.0723648 -4.0658712 -4.0893106 -4.1172891 -4.1531305 -4.17869 -4.1903925 -4.1862144 -4.165977 -4.1341228 -4.1122484 -4.0997906 -4.0918775 -4.095149][-4.1240573 -4.1120219 -4.1147308 -4.1368828 -4.1598258 -4.1843238 -4.1998038 -4.2097626 -4.2029133 -4.1837711 -4.1543784 -4.1358404 -4.1277342 -4.119895 -4.123764][-4.1776624 -4.1701894 -4.1748672 -4.1860476 -4.1965404 -4.2042255 -4.2067852 -4.2131796 -4.2066941 -4.1950359 -4.1739335 -4.1637039 -4.1631284 -4.1586728 -4.1635423][-4.2279959 -4.222002 -4.2266951 -4.2268171 -4.2254648 -4.2203727 -4.2114491 -4.2136617 -4.2075391 -4.2013793 -4.1916981 -4.1910667 -4.1995783 -4.2011266 -4.2052574][-4.2653265 -4.2574673 -4.2609839 -4.2538214 -4.2465415 -4.2369428 -4.2219777 -4.2228031 -4.2205257 -4.2182665 -4.21809 -4.2249389 -4.2372375 -4.2415872 -4.2457733][-4.2812929 -4.270308 -4.2730789 -4.2656112 -4.26077 -4.2560449 -4.24298 -4.2438149 -4.2431989 -4.2425718 -4.247838 -4.252893 -4.2605224 -4.2607474 -4.26142]]...]
INFO - root - 2017-12-08 01:53:01.358210: step 78910, loss = 2.04, batch loss = 1.98 (10.5 examples/sec; 0.762 sec/batch; 53h:40m:31s remains)
INFO - root - 2017-12-08 01:53:08.277123: step 78920, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 46h:48m:13s remains)
INFO - root - 2017-12-08 01:53:15.153561: step 78930, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 47h:58m:02s remains)
INFO - root - 2017-12-08 01:53:21.937237: step 78940, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 46h:29m:30s remains)
INFO - root - 2017-12-08 01:53:28.860679: step 78950, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 51h:14m:52s remains)
INFO - root - 2017-12-08 01:53:35.656018: step 78960, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 50h:27m:53s remains)
INFO - root - 2017-12-08 01:53:42.450380: step 78970, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 47h:36m:38s remains)
INFO - root - 2017-12-08 01:53:49.210965: step 78980, loss = 2.11, batch loss = 2.05 (13.2 examples/sec; 0.607 sec/batch; 42h:43m:06s remains)
INFO - root - 2017-12-08 01:53:55.967414: step 78990, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 44h:21m:46s remains)
INFO - root - 2017-12-08 01:54:02.656844: step 79000, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 52h:06m:42s remains)
2017-12-08 01:54:03.394985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2287135 -4.2294331 -4.2331161 -4.2354679 -4.2373219 -4.2454877 -4.2476692 -4.2492366 -4.2645226 -4.2738876 -4.2675858 -4.2682929 -4.2775269 -4.2915268 -4.3121219][-4.2095146 -4.2107697 -4.2208467 -4.2261128 -4.2244744 -4.22641 -4.2232852 -4.226563 -4.252018 -4.269918 -4.2646942 -4.263134 -4.2717128 -4.2870226 -4.3100853][-4.1725407 -4.1777425 -4.1945348 -4.2081075 -4.2054567 -4.1922688 -4.1799412 -4.1883941 -4.22665 -4.2568641 -4.2585917 -4.25844 -4.2684946 -4.2844415 -4.3082356][-4.1416392 -4.1584468 -4.1806254 -4.1996803 -4.1964884 -4.1636724 -4.132246 -4.1429672 -4.1960011 -4.2405853 -4.2530894 -4.2567968 -4.2690024 -4.2848396 -4.3079081][-4.1152973 -4.144383 -4.1687093 -4.1852565 -4.1788855 -4.1290812 -4.0787563 -4.0915632 -4.1642184 -4.2252855 -4.24844 -4.2580767 -4.2720366 -4.2874084 -4.3088813][-4.0930314 -4.1335616 -4.1602135 -4.1665611 -4.1506419 -4.0895934 -4.0230117 -4.0384631 -4.1308508 -4.2081757 -4.2398095 -4.2584949 -4.2752633 -4.2911215 -4.3107257][-4.083096 -4.1251035 -4.1509323 -4.1492844 -4.1296225 -4.0649376 -3.9912219 -4.0045404 -4.1064854 -4.1908278 -4.2262444 -4.2532053 -4.2740121 -4.2919965 -4.3119526][-4.0595174 -4.0887127 -4.1154666 -4.1196208 -4.10696 -4.0460172 -3.9733317 -3.9784491 -4.0784397 -4.1670914 -4.2065916 -4.2411776 -4.2687559 -4.2912288 -4.3123426][-4.0511255 -4.0618739 -4.0867772 -4.1046648 -4.1045403 -4.05864 -3.9985232 -4.0017943 -4.0855465 -4.1622496 -4.1973214 -4.2309151 -4.2626581 -4.2892165 -4.3118525][-4.0813112 -4.0759583 -4.095314 -4.1210327 -4.1318169 -4.1083088 -4.0719504 -4.0745296 -4.1335087 -4.1877251 -4.2080927 -4.2312746 -4.26023 -4.2867055 -4.3104253][-4.1281013 -4.1199279 -4.1318097 -4.1543646 -4.1703687 -4.165906 -4.1527796 -4.1569209 -4.195662 -4.2299719 -4.2336836 -4.241251 -4.2618136 -4.2858472 -4.3099151][-4.176898 -4.1728821 -4.1823387 -4.1984191 -4.2132506 -4.2195463 -4.2229209 -4.2310772 -4.2583961 -4.2773628 -4.2647271 -4.25768 -4.2700548 -4.2902732 -4.3123217][-4.212018 -4.2131848 -4.223237 -4.2338428 -4.244709 -4.2535081 -4.2628531 -4.2730794 -4.2935514 -4.305553 -4.2870197 -4.2744007 -4.2812543 -4.2981648 -4.3165603][-4.2414522 -4.2443924 -4.2525606 -4.2577477 -4.2637539 -4.26988 -4.276639 -4.2877183 -4.3063622 -4.3162026 -4.2987175 -4.2868681 -4.291821 -4.30534 -4.3198595][-4.2646217 -4.2673931 -4.2707138 -4.27131 -4.271081 -4.27083 -4.2733955 -4.2822409 -4.2996788 -4.3116927 -4.3015976 -4.29413 -4.2989869 -4.31076 -4.3227472]]...]
INFO - root - 2017-12-08 01:54:10.100048: step 79010, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 45h:49m:18s remains)
INFO - root - 2017-12-08 01:54:16.960763: step 79020, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 46h:48m:42s remains)
INFO - root - 2017-12-08 01:54:23.615693: step 79030, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 52h:14m:59s remains)
INFO - root - 2017-12-08 01:54:30.326322: step 79040, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 46h:33m:49s remains)
INFO - root - 2017-12-08 01:54:37.143650: step 79050, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.613 sec/batch; 43h:10m:47s remains)
INFO - root - 2017-12-08 01:54:43.954292: step 79060, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 45h:07m:55s remains)
INFO - root - 2017-12-08 01:54:50.803642: step 79070, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 47h:47m:48s remains)
INFO - root - 2017-12-08 01:54:57.676558: step 79080, loss = 2.09, batch loss = 2.04 (10.9 examples/sec; 0.732 sec/batch; 51h:29m:42s remains)
INFO - root - 2017-12-08 01:55:04.427132: step 79090, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 48h:35m:00s remains)
INFO - root - 2017-12-08 01:55:11.088876: step 79100, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 45h:51m:42s remains)
2017-12-08 01:55:11.829254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3173423 -4.3207946 -4.3255243 -4.332583 -4.3375359 -4.33829 -4.3366046 -4.3336248 -4.32955 -4.3267903 -4.3265023 -4.32922 -4.3325143 -4.33585 -4.3393331][-4.3104019 -4.3152633 -4.3222637 -4.3299809 -4.3300347 -4.3212323 -4.3134613 -4.3106041 -4.310914 -4.3151383 -4.3223138 -4.3324318 -4.3410959 -4.3447113 -4.3451791][-4.3013744 -4.3060493 -4.3118052 -4.3144851 -4.3008175 -4.2735853 -4.2503128 -4.2447629 -4.2547274 -4.2758484 -4.2993965 -4.3220978 -4.33651 -4.3376355 -4.3303404][-4.2982059 -4.3002739 -4.3004322 -4.2916241 -4.2555728 -4.1997113 -4.1492872 -4.130981 -4.1518388 -4.2007475 -4.2512937 -4.2923574 -4.3111453 -4.30375 -4.2812009][-4.3058596 -4.3048954 -4.2973814 -4.2743196 -4.2146721 -4.1254797 -4.0324788 -3.9788766 -4.0061564 -4.096982 -4.1858091 -4.249701 -4.2723403 -4.2525887 -4.2037239][-4.3179369 -4.3146782 -4.3008385 -4.2645526 -4.1836338 -4.0639267 -3.9243526 -3.8254709 -3.8587708 -4.0008526 -4.1377563 -4.2225308 -4.2410588 -4.1953158 -4.1075411][-4.3219929 -4.3170519 -4.3012056 -4.2551322 -4.1631866 -4.0339856 -3.8744957 -3.7481499 -3.7817171 -3.9508889 -4.1159058 -4.2079005 -4.2134881 -4.13802 -4.0168858][-4.3094239 -4.3059406 -4.29122 -4.2434082 -4.158679 -4.0516706 -3.9229002 -3.8231521 -3.849571 -3.9811087 -4.1154871 -4.1868315 -4.1776571 -4.0933504 -3.9767711][-4.2852831 -4.2864504 -4.2768769 -4.2381172 -4.1722507 -4.0976892 -4.017396 -3.9656312 -3.983408 -4.04664 -4.1127858 -4.1474915 -4.13632 -4.0768776 -4.005887][-4.2643952 -4.2710195 -4.2684112 -4.2445192 -4.2026939 -4.1576924 -4.1135573 -4.0938487 -4.0993462 -4.1041894 -4.1072173 -4.1108727 -4.10574 -4.0825624 -4.0625591][-4.2529774 -4.2629528 -4.2675414 -4.2592835 -4.2383966 -4.2150054 -4.1943684 -4.1858883 -4.175859 -4.1455526 -4.1136565 -4.0980716 -4.0952878 -4.09753 -4.1083727][-4.2563505 -4.266798 -4.2757368 -4.2780237 -4.2720928 -4.2651486 -4.2589917 -4.2527456 -4.2317839 -4.1898708 -4.1482368 -4.1212196 -4.1156449 -4.1292868 -4.15009][-4.2714787 -4.2817135 -4.2930255 -4.3009667 -4.3031521 -4.3045907 -4.3054686 -4.2995973 -4.2790608 -4.2441068 -4.2096729 -4.1824946 -4.17443 -4.1885004 -4.2079883][-4.28796 -4.2964926 -4.3061686 -4.3145714 -4.3190961 -4.3229132 -4.3260903 -4.3215055 -4.3076539 -4.2869468 -4.2657924 -4.2479844 -4.2439237 -4.2551351 -4.26735][-4.2998648 -4.3062434 -4.3111811 -4.3163681 -4.3199668 -4.3238392 -4.3271971 -4.3247175 -4.317215 -4.3083634 -4.2992034 -4.2901478 -4.2896943 -4.2971783 -4.3029156]]...]
INFO - root - 2017-12-08 01:55:18.616886: step 79110, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 48h:02m:35s remains)
INFO - root - 2017-12-08 01:55:25.346688: step 79120, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 46h:22m:44s remains)
INFO - root - 2017-12-08 01:55:32.196332: step 79130, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 49h:15m:44s remains)
INFO - root - 2017-12-08 01:55:38.919024: step 79140, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.623 sec/batch; 43h:52m:24s remains)
INFO - root - 2017-12-08 01:55:45.726885: step 79150, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 48h:58m:41s remains)
INFO - root - 2017-12-08 01:55:52.604215: step 79160, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 50h:21m:12s remains)
INFO - root - 2017-12-08 01:55:59.456346: step 79170, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 49h:29m:38s remains)
INFO - root - 2017-12-08 01:56:06.221407: step 79180, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 46h:35m:40s remains)
INFO - root - 2017-12-08 01:56:13.017014: step 79190, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 45h:05m:11s remains)
INFO - root - 2017-12-08 01:56:19.685832: step 79200, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 51h:29m:27s remains)
2017-12-08 01:56:20.515057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2534084 -4.2478409 -4.2459307 -4.2567105 -4.2734084 -4.279861 -4.273088 -4.264576 -4.2582936 -4.2523389 -4.2451191 -4.2413249 -4.2425523 -4.2487288 -4.2577763][-4.22951 -4.22798 -4.2286577 -4.2404542 -4.2542453 -4.255466 -4.2461491 -4.2372246 -4.2302909 -4.2244158 -4.2198653 -4.2184567 -4.220109 -4.22596 -4.23274][-4.2436318 -4.2505703 -4.2512307 -4.254303 -4.2541609 -4.2431068 -4.2252126 -4.2159572 -4.2118397 -4.212049 -4.2112417 -4.209537 -4.2110267 -4.2172909 -4.2223387][-4.26645 -4.279933 -4.2773771 -4.2654824 -4.2453194 -4.2173381 -4.1936841 -4.1887884 -4.1905122 -4.1957459 -4.1953864 -4.1900816 -4.1926665 -4.205524 -4.2159867][-4.27094 -4.2830739 -4.2732587 -4.2485332 -4.2119608 -4.1706648 -4.1463866 -4.1453042 -4.1521215 -4.1596117 -4.156599 -4.1480236 -4.155364 -4.1819968 -4.2044854][-4.2445407 -4.2476258 -4.2301278 -4.1953225 -4.1478028 -4.0978589 -4.0682392 -4.0621705 -4.0757246 -4.0933194 -4.0972605 -4.0956159 -4.1109958 -4.1533976 -4.191195][-4.1943808 -4.1790228 -4.1485248 -4.09979 -4.0395904 -3.9745159 -3.929455 -3.9205325 -3.95383 -3.99775 -4.0240345 -4.0366192 -4.0611258 -4.1176763 -4.1747937][-4.1214986 -4.0805359 -4.0283012 -3.9665728 -3.8980293 -3.8168795 -3.7518408 -3.743819 -3.8083386 -3.8881822 -3.9450505 -3.9787745 -4.0179706 -4.0862036 -4.1578341][-4.0575366 -3.9995062 -3.9375567 -3.87926 -3.8236246 -3.7567892 -3.7108355 -3.7226615 -3.8047066 -3.8945975 -3.9608395 -4.0016356 -4.0409184 -4.1045227 -4.170083][-4.0504994 -3.9992189 -3.9556515 -3.9261179 -3.9049335 -3.8755488 -3.8669047 -3.8931627 -3.9576676 -4.0209813 -4.0649643 -4.0887852 -4.1105433 -4.1545229 -4.2001605][-4.0924749 -4.0643382 -4.0485182 -4.0503879 -4.0569539 -4.0500946 -4.0508971 -4.0707932 -4.1084995 -4.1398816 -4.1623034 -4.1746588 -4.1833143 -4.2026477 -4.2252493][-4.1479778 -4.1413279 -4.1459746 -4.1634088 -4.1797962 -4.1818552 -4.18362 -4.1921396 -4.2065034 -4.214798 -4.2228255 -4.224689 -4.22144 -4.2240229 -4.232306][-4.1928792 -4.201838 -4.2181125 -4.23925 -4.2542725 -4.2580657 -4.2571492 -4.2544875 -4.2496023 -4.2408614 -4.2374129 -4.2327323 -4.2248878 -4.2247477 -4.233881][-4.234098 -4.2480531 -4.2641063 -4.2801366 -4.2902312 -4.2920637 -4.2898211 -4.283896 -4.2724347 -4.2583256 -4.252883 -4.2497139 -4.2457008 -4.2483978 -4.2598653][-4.2718997 -4.2825408 -4.2938056 -4.3036003 -4.3088961 -4.30974 -4.30792 -4.3017082 -4.2917075 -4.2812667 -4.2776504 -4.2770858 -4.2777925 -4.2833123 -4.2924738]]...]
INFO - root - 2017-12-08 01:56:27.258085: step 79210, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 45h:28m:10s remains)
