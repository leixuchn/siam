INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "65"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 08:11:12.934512: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:11:12.934554: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:11:12.934560: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:11:12.934564: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:11:12.934568: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:11:13.492884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 08:11:13.492922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 08:11:13.492930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 08:11:13.492944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 08:11:16.495863: step 0, loss = 2.02, batch loss = 1.97 (3.5 examples/sec; 2.264 sec/batch; 209h:04m:35s remains)
2017-12-05 08:11:16.900644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3217239 -4.3196573 -4.3207307 -4.3202243 -4.3145928 -4.3024664 -4.2854161 -4.2673826 -4.2593389 -4.2600179 -4.2664847 -4.27994 -4.3010588 -4.3197703 -4.3319154][-4.319562 -4.3179569 -4.3193603 -4.3165574 -4.3028312 -4.2789316 -4.2487445 -4.2225332 -4.2149081 -4.2213855 -4.2349143 -4.2553949 -4.2839808 -4.3078928 -4.324564][-4.316185 -4.3157616 -4.3159752 -4.3052254 -4.2762532 -4.235198 -4.1883922 -4.15301 -4.1478992 -4.1654458 -4.1923504 -4.2250195 -4.2622871 -4.2931046 -4.3159781][-4.3109794 -4.3130274 -4.3100877 -4.2861176 -4.2403178 -4.1782236 -4.1099968 -4.0637021 -4.0643654 -4.09848 -4.1421189 -4.1918545 -4.2416892 -4.2803936 -4.3092747][-4.3016181 -4.3064809 -4.2989979 -4.2611904 -4.2002354 -4.1184711 -4.0256653 -3.9625542 -3.9735422 -4.0280132 -4.0915208 -4.1608181 -4.2245026 -4.2718678 -4.3060246][-4.2888794 -4.2951069 -4.282917 -4.2316809 -4.154048 -4.0500255 -3.9304063 -3.8527198 -3.8863828 -3.9689112 -4.0539265 -4.1412611 -4.2139306 -4.2674761 -4.3052621][-4.275074 -4.2791877 -4.2584848 -4.1948762 -4.0991917 -3.9711378 -3.8234689 -3.7429242 -3.8173606 -3.9311752 -4.0348144 -4.1339927 -4.2111173 -4.2669764 -4.3057084][-4.2609072 -4.2580905 -4.2276731 -4.1566234 -4.0474272 -3.8997872 -3.7292552 -3.6677079 -3.7962568 -3.9370561 -4.049521 -4.1458368 -4.2191377 -4.2721534 -4.3075218][-4.2418022 -4.2287827 -4.1956625 -4.1324863 -4.0341225 -3.900696 -3.7560902 -3.7450347 -3.888093 -4.0137367 -4.1035514 -4.1779923 -4.2372007 -4.2815557 -4.3113966][-4.2202764 -4.2025418 -4.1761446 -4.1358538 -4.0700283 -3.98009 -3.8925419 -3.9114013 -4.0190039 -4.1045895 -4.1599827 -4.2064977 -4.2512927 -4.2899055 -4.3160677][-4.2019548 -4.1835375 -4.1650252 -4.1448278 -4.1067524 -4.0549479 -4.0104661 -4.0331111 -4.1038294 -4.1564412 -4.1859508 -4.2155433 -4.2557683 -4.2950425 -4.3204222][-4.1865306 -4.1687655 -4.1561542 -4.1461987 -4.127533 -4.1038742 -4.084147 -4.1037416 -4.1489973 -4.1803441 -4.1960115 -4.2209864 -4.2609396 -4.300333 -4.3243828][-4.1840067 -4.1705885 -4.1646795 -4.1626158 -4.1595387 -4.1543684 -4.1481853 -4.160676 -4.181314 -4.1958523 -4.2035537 -4.2277732 -4.2677021 -4.305625 -4.3275156][-4.2072539 -4.2016964 -4.20126 -4.2031484 -4.2072897 -4.2067757 -4.2014289 -4.2031651 -4.2040825 -4.2065635 -4.2087488 -4.2316747 -4.272203 -4.3087487 -4.3291469][-4.2359939 -4.2390218 -4.2416129 -4.2418995 -4.2431245 -4.2357321 -4.2230082 -4.2149959 -4.206296 -4.2039566 -4.2057405 -4.2298684 -4.2722049 -4.308424 -4.328413]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 08:11:19.634806: step 10, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 21h:11m:00s remains)
INFO - root - 2017-12-05 08:11:21.760128: step 20, loss = 2.04, batch loss = 1.98 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:49s remains)
INFO - root - 2017-12-05 08:11:23.895027: step 30, loss = 2.03, batch loss = 1.97 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:23s remains)
INFO - root - 2017-12-05 08:11:26.023555: step 40, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:29s remains)
INFO - root - 2017-12-05 08:11:28.164820: step 50, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 20h:48m:32s remains)
INFO - root - 2017-12-05 08:11:30.295227: step 60, loss = 2.06, batch loss = 2.01 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:58s remains)
INFO - root - 2017-12-05 08:11:32.419042: step 70, loss = 2.02, batch loss = 1.96 (38.1 examples/sec; 0.210 sec/batch; 19h:23m:18s remains)
INFO - root - 2017-12-05 08:11:34.523910: step 80, loss = 2.01, batch loss = 1.95 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:53s remains)
INFO - root - 2017-12-05 08:11:36.660503: step 90, loss = 2.04, batch loss = 1.98 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:53s remains)
INFO - root - 2017-12-05 08:11:38.799066: step 100, loss = 2.01, batch loss = 1.95 (36.9 examples/sec; 0.217 sec/batch; 20h:02m:28s remains)
2017-12-05 08:11:39.093313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2587175 -4.2175331 -4.1691241 -4.1241117 -4.0956011 -4.0993667 -4.1161647 -4.132844 -4.1551924 -4.1665859 -4.16596 -4.1723404 -4.1752157 -4.1634545 -4.1491232][-4.26496 -4.2175155 -4.159596 -4.1031461 -4.0656157 -4.0649428 -4.0817933 -4.0970831 -4.1134453 -4.12491 -4.129972 -4.14009 -4.14509 -4.1337657 -4.1229815][-4.2708435 -4.2198009 -4.1582742 -4.0994611 -4.061554 -4.0579844 -4.0719371 -4.0837827 -4.0960741 -4.1092315 -4.1184912 -4.1262617 -4.1274834 -4.1137338 -4.102149][-4.2713375 -4.219645 -4.1598039 -4.1028385 -4.0654311 -4.0574903 -4.067821 -4.0788436 -4.0916505 -4.1084933 -4.1211381 -4.1261773 -4.1218119 -4.1079612 -4.0953803][-4.2690525 -4.2183456 -4.1620183 -4.1071186 -4.0681553 -4.0522137 -4.0556355 -4.0618157 -4.0713196 -4.0903349 -4.10862 -4.1164474 -4.11246 -4.1013656 -4.0906744][-4.26741 -4.2197227 -4.1677527 -4.1132135 -4.0693579 -4.0430417 -4.0354538 -4.0291152 -4.02662 -4.0446272 -4.0744052 -4.0926318 -4.0970187 -4.0913463 -4.0839362][-4.2636056 -4.2166266 -4.1646676 -4.1058674 -4.0563745 -4.0240731 -4.0108409 -3.9942412 -3.9781373 -3.9933436 -4.0331073 -4.0600243 -4.0728521 -4.0734854 -4.0713058][-4.2605715 -4.2122717 -4.1575937 -4.0950251 -4.0433617 -4.0129857 -4.0021977 -3.9850581 -3.9652443 -3.9786859 -4.0162606 -4.0393186 -4.0503516 -4.053762 -4.0557184][-4.2599444 -4.2110639 -4.1553373 -4.0922351 -4.0431128 -4.0191689 -4.0142093 -4.0038962 -3.990519 -4.0043106 -4.0311246 -4.04185 -4.0431046 -4.0434556 -4.0461731][-4.2631726 -4.2138958 -4.1578708 -4.0963883 -4.0505009 -4.0308518 -4.0271091 -4.0206671 -4.0180335 -4.0383153 -4.0601373 -4.061522 -4.0535374 -4.0477924 -4.047102][-4.2731452 -4.2253661 -4.1711359 -4.1158166 -4.0767708 -4.0620537 -4.0587826 -4.0545034 -4.0566816 -4.0775514 -4.0968022 -4.0957718 -4.0843172 -4.0755558 -4.0710316][-4.2860622 -4.2417874 -4.1923971 -4.1452413 -4.115396 -4.1080589 -4.11027 -4.1117435 -4.1170225 -4.1326051 -4.1447105 -4.1423769 -4.1317997 -4.1236806 -4.1182361][-4.3005505 -4.26197 -4.2203364 -4.18263 -4.15999 -4.1567211 -4.1647358 -4.1751742 -4.1860456 -4.1979628 -4.2032685 -4.1988487 -4.1883593 -4.179399 -4.1738873][-4.3195906 -4.2915845 -4.2619629 -4.2351637 -4.2179 -4.2151566 -4.2246757 -4.2390537 -4.2524958 -4.2608824 -4.2616324 -4.25602 -4.2469296 -4.2391367 -4.2364297][-4.33864 -4.3218694 -4.3052044 -4.2898121 -4.2786427 -4.2763042 -4.282867 -4.2935228 -4.3028436 -4.3081231 -4.3078604 -4.3033328 -4.2980242 -4.2940068 -4.293931]]...]
INFO - root - 2017-12-05 08:11:41.231758: step 110, loss = 2.02, batch loss = 1.96 (36.1 examples/sec; 0.222 sec/batch; 20h:29m:17s remains)
INFO - root - 2017-12-05 08:11:43.357721: step 120, loss = 1.97, batch loss = 1.91 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:57s remains)
INFO - root - 2017-12-05 08:11:45.516875: step 130, loss = 1.98, batch loss = 1.92 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:06s remains)
INFO - root - 2017-12-05 08:11:47.649030: step 140, loss = 1.97, batch loss = 1.91 (38.7 examples/sec; 0.206 sec/batch; 19h:03m:40s remains)
INFO - root - 2017-12-05 08:11:49.777373: step 150, loss = 2.02, batch loss = 1.96 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:55s remains)
INFO - root - 2017-12-05 08:11:51.940510: step 160, loss = 1.96, batch loss = 1.90 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:39s remains)
INFO - root - 2017-12-05 08:11:54.080125: step 170, loss = 2.01, batch loss = 1.95 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:41s remains)
INFO - root - 2017-12-05 08:11:56.216619: step 180, loss = 1.95, batch loss = 1.89 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:25s remains)
INFO - root - 2017-12-05 08:11:58.365343: step 190, loss = 1.96, batch loss = 1.90 (36.9 examples/sec; 0.217 sec/batch; 20h:02m:13s remains)
INFO - root - 2017-12-05 08:12:00.521991: step 200, loss = 1.91, batch loss = 1.85 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:19s remains)
2017-12-05 08:12:00.823029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1414514 -4.1231813 -4.1054873 -4.1025972 -4.1134229 -4.1184282 -4.1173325 -4.11626 -4.1166325 -4.1224766 -4.1283436 -4.1330729 -4.1321945 -4.1330175 -4.1432743][-4.0972271 -4.0684466 -4.044858 -4.0396142 -4.0504336 -4.055841 -4.05004 -4.0435677 -4.0404387 -4.0478382 -4.0553155 -4.0604753 -4.0590234 -4.0617518 -4.0757837][-4.03972 -4.0077929 -3.9929085 -3.9900041 -3.9957085 -3.9997065 -3.990932 -3.981638 -3.983449 -3.9993687 -4.0094004 -4.0098314 -4.0013728 -3.9993157 -4.0122094][-3.97693 -3.9498665 -3.9475079 -3.9498427 -3.9479365 -3.9458146 -3.9280021 -3.9151688 -3.9349263 -3.9689384 -3.9822168 -3.9740489 -3.9542744 -3.9390032 -3.9436536][-3.9305503 -3.9085193 -3.9126697 -3.9208724 -3.9155943 -3.8995264 -3.8663554 -3.847152 -3.8830383 -3.9353058 -3.9547966 -3.9436939 -3.9153697 -3.8862646 -3.8777113][-3.9002762 -3.873877 -3.8802061 -3.8929758 -3.8844008 -3.851501 -3.7901888 -3.7481925 -3.7992213 -3.8762257 -3.9106731 -3.9073563 -3.8789835 -3.8438604 -3.8263414][-3.8944688 -3.8506892 -3.8499696 -3.8610749 -3.8473845 -3.7912743 -3.6762328 -3.59175 -3.6663311 -3.7877767 -3.8521731 -3.8645236 -3.8395994 -3.8084755 -3.8019855][-3.883038 -3.8201489 -3.8106592 -3.8297193 -3.8182569 -3.738647 -3.5572164 -3.4161055 -3.5273831 -3.7068572 -3.8056524 -3.8300591 -3.8029404 -3.7714877 -3.7825222][-3.8699446 -3.8073435 -3.8002982 -3.8389051 -3.8399584 -3.7600212 -3.5719557 -3.4270182 -3.5288837 -3.7050316 -3.8111002 -3.8415651 -3.8178718 -3.7890909 -3.8028553][-3.8749692 -3.8285816 -3.8258753 -3.8678374 -3.8805368 -3.8257565 -3.6937847 -3.5993061 -3.6518774 -3.770596 -3.8542848 -3.8860319 -3.8700116 -3.8451176 -3.8496795][-3.9108255 -3.878346 -3.8745234 -3.9055917 -3.9156625 -3.8804457 -3.7956719 -3.732398 -3.7484615 -3.8234243 -3.8887603 -3.9176221 -3.9100785 -3.8955297 -3.8923559][-3.9487882 -3.9230111 -3.9153554 -3.9327893 -3.9403784 -3.9181483 -3.8590245 -3.8126111 -3.8096652 -3.8536286 -3.9037855 -3.9333665 -3.9336851 -3.9269693 -3.9194689][-3.9933057 -3.9703689 -3.9610524 -3.9701238 -3.9749699 -3.9665833 -3.929513 -3.8963299 -3.8874977 -3.9062059 -3.941843 -3.971101 -3.9767261 -3.9756513 -3.9681382][-4.0445161 -4.0250931 -4.0190396 -4.0251193 -4.0255046 -4.0260544 -4.0052853 -3.9833808 -3.97619 -3.9841774 -4.0109897 -4.0345917 -4.0384531 -4.03891 -4.0337543][-4.1002111 -4.087709 -4.0850468 -4.0882196 -4.0878925 -4.0918922 -4.0810914 -4.0646672 -4.0582471 -4.0609984 -4.0779738 -4.0953884 -4.0990863 -4.0960469 -4.0886173]]...]
INFO - root - 2017-12-05 08:12:02.978069: step 210, loss = 1.90, batch loss = 1.84 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:21s remains)
INFO - root - 2017-12-05 08:12:05.127302: step 220, loss = 1.92, batch loss = 1.86 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:08s remains)
INFO - root - 2017-12-05 08:12:07.274389: step 230, loss = 1.88, batch loss = 1.82 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:26s remains)
INFO - root - 2017-12-05 08:12:09.437265: step 240, loss = 1.85, batch loss = 1.79 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:06s remains)
INFO - root - 2017-12-05 08:12:11.576697: step 250, loss = 1.90, batch loss = 1.84 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:11s remains)
INFO - root - 2017-12-05 08:12:13.731390: step 260, loss = 1.86, batch loss = 1.80 (38.2 examples/sec; 0.210 sec/batch; 19h:21m:06s remains)
INFO - root - 2017-12-05 08:12:15.904646: step 270, loss = 1.90, batch loss = 1.84 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:59s remains)
INFO - root - 2017-12-05 08:12:18.041798: step 280, loss = 1.79, batch loss = 1.73 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:26s remains)
INFO - root - 2017-12-05 08:12:20.193943: step 290, loss = 1.92, batch loss = 1.87 (36.7 examples/sec; 0.218 sec/batch; 20h:08m:02s remains)
INFO - root - 2017-12-05 08:12:22.325550: step 300, loss = 1.89, batch loss = 1.83 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:07s remains)
2017-12-05 08:12:22.601047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1878681 -4.1107955 -4.0145574 -3.9058375 -3.7975683 -3.7083964 -3.6429007 -3.6051345 -3.6067908 -3.6535287 -3.7251709 -3.7949836 -3.8455367 -3.8699515 -3.85925][-4.1274018 -4.0522346 -3.9600413 -3.8560133 -3.75118 -3.6685705 -3.6120529 -3.5763919 -3.5700505 -3.606276 -3.6759541 -3.7476945 -3.8000424 -3.8326876 -3.8362994][-4.0495524 -3.9831212 -3.9044304 -3.8160539 -3.7312012 -3.6697698 -3.6311495 -3.6048923 -3.5990007 -3.6292136 -3.6889439 -3.7481465 -3.7878091 -3.815084 -3.8230159][-3.94817 -3.8965261 -3.8364906 -3.7667773 -3.6998003 -3.6572766 -3.6387329 -3.6313682 -3.6360066 -3.6596651 -3.7003479 -3.7368467 -3.7595911 -3.7787786 -3.7910857][-3.8183391 -3.7779748 -3.7351739 -3.6843319 -3.6320455 -3.6043048 -3.6068668 -3.6199143 -3.635916 -3.6497948 -3.6695604 -3.6849322 -3.6940668 -3.7043097 -3.7187812][-3.6605482 -3.6268964 -3.5996313 -3.5637693 -3.5164621 -3.4918013 -3.5032537 -3.5258057 -3.5457036 -3.5535984 -3.565279 -3.5778668 -3.5885692 -3.5990682 -3.6187234][-3.5368667 -3.4991713 -3.4734638 -3.4363637 -3.3812122 -3.351254 -3.3593631 -3.3808286 -3.4029593 -3.4112036 -3.4219732 -3.4368472 -3.4514465 -3.4649737 -3.4909148][-3.5440171 -3.50475 -3.4810097 -3.4466703 -3.3926377 -3.3535519 -3.3413224 -3.3411322 -3.3514352 -3.3541679 -3.3550086 -3.359796 -3.3637214 -3.3674548 -3.3868825][-3.6156919 -3.5916667 -3.5822358 -3.5707343 -3.5414171 -3.5118041 -3.4884636 -3.4693186 -3.4639924 -3.46001 -3.45468 -3.4477496 -3.4346492 -3.4227929 -3.4254587][-3.7065644 -3.7074175 -3.719692 -3.7305169 -3.7209487 -3.6997983 -3.6705763 -3.6412787 -3.6286371 -3.6254563 -3.6241863 -3.6185713 -3.6060355 -3.5902314 -3.579699][-3.8382094 -3.8494673 -3.8713012 -3.8900824 -3.886977 -3.8694031 -3.8399282 -3.8097205 -3.7970376 -3.7999556 -3.8087077 -3.8116751 -3.8062735 -3.7916555 -3.7695796][-4.0099816 -4.0114431 -4.024694 -4.03573 -4.030302 -4.01215 -3.9863508 -3.9617133 -3.9530272 -3.9609494 -3.9771936 -3.9863133 -3.9826384 -3.9646103 -3.9325743][-4.1493597 -4.1442633 -4.1463623 -4.1458755 -4.1354284 -4.1182413 -4.0982528 -4.0829563 -4.0807676 -4.0919228 -4.11088 -4.1229906 -4.1201429 -4.1006346 -4.0657005][-4.2429233 -4.2371526 -4.2347236 -4.2295904 -4.2176523 -4.2032247 -4.1890192 -4.1799703 -4.1811643 -4.1917777 -4.2077403 -4.2188277 -4.216754 -4.198525 -4.164885][-4.3009915 -4.2962332 -4.2922769 -4.2860312 -4.2768736 -4.2670732 -4.2583752 -4.2541347 -4.2563038 -4.2634726 -4.2727852 -4.27961 -4.2784371 -4.2641549 -4.2347794]]...]
INFO - root - 2017-12-05 08:12:24.772112: step 310, loss = 1.82, batch loss = 1.76 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:46s remains)
INFO - root - 2017-12-05 08:12:26.950336: step 320, loss = 1.77, batch loss = 1.71 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:03s remains)
INFO - root - 2017-12-05 08:12:29.098743: step 330, loss = 1.84, batch loss = 1.78 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:27s remains)
INFO - root - 2017-12-05 08:12:31.246456: step 340, loss = 1.81, batch loss = 1.76 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-05 08:12:33.394866: step 350, loss = 1.76, batch loss = 1.70 (36.1 examples/sec; 0.221 sec/batch; 20h:25m:54s remains)
INFO - root - 2017-12-05 08:12:35.562247: step 360, loss = 1.73, batch loss = 1.67 (37.2 examples/sec; 0.215 sec/batch; 19h:51m:11s remains)
INFO - root - 2017-12-05 08:12:37.730914: step 370, loss = 1.67, batch loss = 1.61 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:54s remains)
INFO - root - 2017-12-05 08:12:39.884757: step 380, loss = 1.65, batch loss = 1.59 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:04s remains)
INFO - root - 2017-12-05 08:12:42.041824: step 390, loss = 1.61, batch loss = 1.55 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:46s remains)
INFO - root - 2017-12-05 08:12:44.205337: step 400, loss = 1.48, batch loss = 1.42 (36.6 examples/sec; 0.218 sec/batch; 20h:09m:02s remains)
2017-12-05 08:12:44.497293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3366814 -4.3246284 -4.3140707 -4.30141 -4.2913179 -4.2901573 -4.2977767 -4.3106751 -4.3236709 -4.335011 -4.3428254 -4.3426957 -4.3337359 -4.317275 -4.2949295][-4.3481903 -4.3365841 -4.3206968 -4.2986326 -4.2780852 -4.2713757 -4.2818279 -4.3025146 -4.3239584 -4.3406825 -4.3499351 -4.3456378 -4.3237247 -4.2847519 -4.2345376][-4.3538532 -4.3366075 -4.3058858 -4.26348 -4.2245488 -4.2062931 -4.2160082 -4.2474389 -4.2858648 -4.319747 -4.3411832 -4.3395305 -4.3065987 -4.2426872 -4.158638][-4.3525543 -4.322165 -4.2676063 -4.1961432 -4.1309805 -4.0915141 -4.0901084 -4.1267142 -4.1867809 -4.2497187 -4.2977529 -4.3108215 -4.2776136 -4.1983256 -4.0837555][-4.3468657 -4.2991815 -4.2184677 -4.1154933 -4.018115 -3.947144 -3.9189496 -3.9478335 -4.0254941 -4.1221356 -4.2061119 -4.247777 -4.23055 -4.1532488 -4.0234323][-4.3380094 -4.2764969 -4.1748352 -4.0435886 -3.9122093 -3.801156 -3.7323642 -3.7358751 -3.8186679 -3.9471133 -4.0715804 -4.1523938 -4.1696148 -4.1164975 -3.9989483][-4.3327484 -4.2651339 -4.1553235 -4.00765 -3.8461864 -3.6905134 -3.5692635 -3.528996 -3.5988836 -3.7520795 -3.9166105 -4.0395288 -4.099668 -4.090836 -4.0159168][-4.3349619 -4.2725034 -4.1720448 -4.0291357 -3.8574317 -3.6712177 -3.4981415 -3.3977151 -3.431149 -3.5854771 -3.7735186 -3.9300833 -4.034234 -4.0771813 -4.0608888][-4.3448663 -4.2957039 -4.21884 -4.1040649 -3.9563611 -3.7850108 -3.6041756 -3.4638598 -3.4417758 -3.5481594 -3.7088904 -3.8655868 -3.9955683 -4.0811725 -4.1170411][-4.3592215 -4.3273525 -4.2771635 -4.200007 -4.0951257 -3.9704185 -3.8324163 -3.711185 -3.6603839 -3.6968422 -3.7854691 -3.8962727 -4.0118022 -4.1098185 -4.1752295][-4.3723288 -4.3563976 -4.3286505 -4.28405 -4.2208118 -4.1450438 -4.0601745 -3.9807894 -3.935025 -3.93106 -3.9601395 -4.0171576 -4.0947032 -4.1746092 -4.2394881][-4.3795671 -4.3740587 -4.361887 -4.3407173 -4.3098135 -4.271451 -4.2263732 -4.1816759 -4.150681 -4.1363173 -4.1380405 -4.1609168 -4.2022443 -4.2526627 -4.2988772][-4.3816891 -4.3809662 -4.3774605 -4.3704486 -4.3595486 -4.344552 -4.3248315 -4.3020864 -4.28239 -4.2683558 -4.2622766 -4.2683759 -4.2859092 -4.3121591 -4.3388734][-4.3809376 -4.3807688 -4.3803473 -4.379662 -4.378202 -4.3750916 -4.3692765 -4.3601894 -4.3498139 -4.3401923 -4.333169 -4.3321018 -4.3380485 -4.349318 -4.3616147][-4.3789 -4.377913 -4.3771758 -4.3771181 -4.377913 -4.3787003 -4.379005 -4.3778129 -4.3748016 -4.3707657 -4.3670616 -4.3652644 -4.3662024 -4.3695664 -4.3735671]]...]
INFO - root - 2017-12-05 08:12:46.648479: step 410, loss = 1.57, batch loss = 1.51 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-05 08:12:48.813102: step 420, loss = 1.50, batch loss = 1.44 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:11s remains)
INFO - root - 2017-12-05 08:12:51.003594: step 430, loss = 1.61, batch loss = 1.55 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:00s remains)
INFO - root - 2017-12-05 08:12:53.162441: step 440, loss = 1.43, batch loss = 1.38 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:37s remains)
INFO - root - 2017-12-05 08:12:55.330041: step 450, loss = 1.48, batch loss = 1.42 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:41s remains)
INFO - root - 2017-12-05 08:12:57.502198: step 460, loss = 1.48, batch loss = 1.42 (36.1 examples/sec; 0.221 sec/batch; 20h:25m:09s remains)
INFO - root - 2017-12-05 08:12:59.669545: step 470, loss = 1.39, batch loss = 1.33 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:53s remains)
INFO - root - 2017-12-05 08:13:01.841619: step 480, loss = 1.50, batch loss = 1.44 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:20s remains)
INFO - root - 2017-12-05 08:13:04.021767: step 490, loss = 1.44, batch loss = 1.38 (37.2 examples/sec; 0.215 sec/batch; 19h:51m:28s remains)
INFO - root - 2017-12-05 08:13:06.188218: step 500, loss = 1.44, batch loss = 1.38 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:41s remains)
2017-12-05 08:13:06.496504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3682435 -3.5400262 -3.773917 -3.9880857 -4.1254859 -4.1581192 -4.1035514 -3.9925003 -3.8638265 -3.7545249 -3.682724 -3.642282 -3.6176672 -3.5824895 -3.5071716][-3.1786816 -3.4027681 -3.6823084 -3.9184093 -4.0509844 -4.0545192 -3.9542961 -3.7914476 -3.6221633 -3.4978848 -3.4408541 -3.4329364 -3.4486349 -3.4552634 -3.4203699][-3.0819416 -3.3435214 -3.6325614 -3.8499081 -3.9467154 -3.9030881 -3.7478368 -3.5365772 -3.3493133 -3.2473869 -3.2445254 -3.3041124 -3.3844936 -3.4519572 -3.471951][-3.0746913 -3.3292184 -3.5831223 -3.7451282 -3.7811866 -3.6789095 -3.4716144 -3.2377934 -3.0786233 -3.0429957 -3.124826 -3.2677884 -3.4179859 -3.5452509 -3.6146157][-3.1069369 -3.302218 -3.4829841 -3.5650012 -3.5288935 -3.3668163 -3.1260352 -2.9164503 -2.8401003 -2.906297 -3.0816875 -3.2994719 -3.5065579 -3.6756794 -3.7753308][-3.1041589 -3.2021384 -3.284389 -3.2828293 -3.18224 -2.9781406 -2.739675 -2.6054511 -2.6555562 -2.8381181 -3.0986924 -3.3693223 -3.6070557 -3.7906773 -3.8995869][-3.0289378 -3.0164962 -3.0039115 -2.9363763 -2.8008089 -2.5926266 -2.3935776 -2.3707967 -2.5584829 -2.8420792 -3.1592007 -3.4500327 -3.6870685 -3.8610759 -3.961417][-2.8585639 -2.7581556 -2.6912699 -2.5985131 -2.47444 -2.3175561 -2.2038035 -2.2967932 -2.5849664 -2.9252234 -3.2576461 -3.5385509 -3.7549024 -3.9055924 -3.9895055][-2.6084485 -2.4697957 -2.4028673 -2.3390226 -2.2757058 -2.2204216 -2.2325425 -2.4179888 -2.7433362 -3.0891385 -3.4013093 -3.6527538 -3.837466 -3.9609621 -4.0269432][-2.3885899 -2.2668469 -2.2451539 -2.2396824 -2.2487636 -2.2954323 -2.415235 -2.6567192 -2.9806604 -3.3015118 -3.5775402 -3.7922974 -3.9424212 -4.0393834 -4.089962][-2.3295565 -2.2640426 -2.3016388 -2.3518231 -2.414639 -2.524425 -2.6979957 -2.9518363 -3.2497864 -3.5292358 -3.7607708 -3.934571 -4.0517964 -4.1258311 -4.1647921][-2.4962232 -2.491581 -2.5741868 -2.6579885 -2.7495317 -2.881233 -3.0556397 -3.2808118 -3.5297904 -3.7527835 -3.9315538 -4.0638857 -4.1523871 -4.2075973 -4.2373753][-2.843137 -2.8742962 -2.9703965 -3.066256 -3.1662042 -3.2897296 -3.433804 -3.6072814 -3.7928689 -3.9529326 -4.0790544 -4.174614 -4.2384834 -4.2775192 -4.299448][-3.2798393 -3.3171387 -3.4008546 -3.4880047 -3.5755281 -3.6720889 -3.7755985 -3.8947244 -4.0183587 -4.1213155 -4.2015257 -4.2632246 -4.3050327 -4.3298392 -4.3440509][-3.6931491 -3.7243898 -3.7840788 -3.8466225 -3.9080033 -3.9705856 -4.0345368 -4.1080241 -4.18136 -4.2410021 -4.2870736 -4.3224015 -4.3464317 -4.3609543 -4.3698931]]...]
INFO - root - 2017-12-05 08:13:08.643581: step 510, loss = 1.37, batch loss = 1.32 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:43s remains)
INFO - root - 2017-12-05 08:13:10.856629: step 520, loss = 1.36, batch loss = 1.30 (37.1 examples/sec; 0.215 sec/batch; 19h:52m:12s remains)
INFO - root - 2017-12-05 08:13:13.008970: step 530, loss = 1.54, batch loss = 1.49 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:08s remains)
INFO - root - 2017-12-05 08:13:15.181045: step 540, loss = 1.40, batch loss = 1.34 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:57s remains)
INFO - root - 2017-12-05 08:13:17.330798: step 550, loss = 1.46, batch loss = 1.40 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:16s remains)
INFO - root - 2017-12-05 08:13:19.495049: step 560, loss = 1.30, batch loss = 1.25 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:00s remains)
INFO - root - 2017-12-05 08:13:21.634872: step 570, loss = 1.37, batch loss = 1.31 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:53s remains)
INFO - root - 2017-12-05 08:13:23.804606: step 580, loss = 1.42, batch loss = 1.36 (34.3 examples/sec; 0.233 sec/batch; 21h:30m:24s remains)
INFO - root - 2017-12-05 08:13:25.965968: step 590, loss = 1.25, batch loss = 1.19 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:33s remains)
INFO - root - 2017-12-05 08:13:28.152386: step 600, loss = 1.15, batch loss = 1.09 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:57s remains)
2017-12-05 08:13:28.422078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1964331 -4.1976852 -4.1997452 -4.19659 -4.1934471 -4.1873536 -4.1863756 -4.1947174 -4.2049713 -4.217092 -4.236063 -4.2542591 -4.2636342 -4.2668791 -4.2676382][-4.1969886 -4.1956449 -4.1949515 -4.1866164 -4.1769123 -4.1656508 -4.1602936 -4.1665177 -4.1778221 -4.1914191 -4.2127461 -4.2368059 -4.2553172 -4.2683225 -4.2791839][-4.153163 -4.1476145 -4.1465564 -4.1392593 -4.1296029 -4.1180291 -4.1117482 -4.1160712 -4.1255865 -4.136116 -4.1555004 -4.1826344 -4.2096243 -4.2352061 -4.2606468][-4.0690145 -4.0588036 -4.0590949 -4.0577869 -4.0525513 -4.0441184 -4.0408072 -4.0460868 -4.0538983 -4.0614505 -4.0776911 -4.1055021 -4.1376171 -4.17214 -4.2110114][-3.9679236 -3.9555788 -3.9598486 -3.965302 -3.9636087 -3.9557624 -3.9543321 -3.9604526 -3.9676726 -3.9751737 -3.9897943 -4.0149612 -4.0459938 -4.0821719 -4.129303][-3.8720012 -3.864656 -3.8755894 -3.8845503 -3.8830192 -3.8716533 -3.8670149 -3.8709846 -3.8784041 -3.886436 -3.9001141 -3.9189653 -3.940388 -3.9698091 -4.017889][-3.8043246 -3.8011551 -3.8169074 -3.8244748 -3.8198304 -3.8030436 -3.7901249 -3.7901134 -3.7980316 -3.8057895 -3.8154187 -3.82568 -3.8328183 -3.8524752 -3.8964443][-3.778276 -3.7747061 -3.7933507 -3.8005443 -3.7958949 -3.7761803 -3.7551861 -3.7488616 -3.7535417 -3.7561636 -3.7577095 -3.759464 -3.7555537 -3.7670646 -3.8070774][-3.7977827 -3.7875583 -3.8097336 -3.8243322 -3.8292222 -3.8175144 -3.7966387 -3.7840636 -3.7803068 -3.7728591 -3.7631316 -3.75618 -3.7465916 -3.7536893 -3.7932584][-3.8583007 -3.8400323 -3.8647084 -3.8908014 -3.911756 -3.9165828 -3.9058647 -3.8928249 -3.8821621 -3.8665702 -3.8489234 -3.8365526 -3.8240631 -3.8303552 -3.8695421][-3.939898 -3.9201815 -3.9464483 -3.9841661 -4.0222836 -4.0468535 -4.0527139 -4.0464025 -4.0362754 -4.019979 -4.0018525 -3.987875 -3.9757364 -3.9825017 -4.0153174][-4.0175276 -4.0046558 -4.0359983 -4.08329 -4.1325564 -4.1723032 -4.1924086 -4.19556 -4.1911106 -4.1796174 -4.16462 -4.1523046 -4.1433716 -4.1492558 -4.1719604][-4.0786238 -4.07855 -4.1161127 -4.1680512 -4.2207427 -4.2653637 -4.291451 -4.3007169 -4.3014283 -4.2962718 -4.2872844 -4.279058 -4.2734518 -4.2772593 -4.290421][-4.1191735 -4.1323528 -4.1756296 -4.2300467 -4.2818685 -4.3234282 -4.3478351 -4.3588924 -4.3622828 -4.3611488 -4.3571615 -4.3528695 -4.3498797 -4.3514137 -4.3574753][-4.1400456 -4.1673803 -4.2178478 -4.2746964 -4.3238497 -4.3582854 -4.3770123 -4.3859544 -4.3891797 -4.3895116 -4.3878803 -4.3857055 -4.3839254 -4.3839498 -4.3858123]]...]
INFO - root - 2017-12-05 08:13:30.590592: step 610, loss = 1.64, batch loss = 1.58 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:40s remains)
INFO - root - 2017-12-05 08:13:32.736826: step 620, loss = 1.32, batch loss = 1.26 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:00s remains)
INFO - root - 2017-12-05 08:13:34.890649: step 630, loss = 1.18, batch loss = 1.12 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:51s remains)
INFO - root - 2017-12-05 08:13:37.033091: step 640, loss = 1.28, batch loss = 1.22 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:09s remains)
INFO - root - 2017-12-05 08:13:39.174351: step 650, loss = 1.24, batch loss = 1.18 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:43s remains)
INFO - root - 2017-12-05 08:13:41.322912: step 660, loss = 1.26, batch loss = 1.20 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:02s remains)
INFO - root - 2017-12-05 08:13:43.505296: step 670, loss = 1.40, batch loss = 1.34 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:23s remains)
INFO - root - 2017-12-05 08:13:45.700704: step 680, loss = 1.35, batch loss = 1.29 (34.5 examples/sec; 0.232 sec/batch; 21h:21m:43s remains)
INFO - root - 2017-12-05 08:13:47.893537: step 690, loss = 1.23, batch loss = 1.17 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:42s remains)
INFO - root - 2017-12-05 08:13:50.090556: step 700, loss = 1.28, batch loss = 1.22 (37.3 examples/sec; 0.215 sec/batch; 19h:46m:26s remains)
2017-12-05 08:13:50.358179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3973403 -4.3975711 -4.3977337 -4.3977962 -4.3978271 -4.3978205 -4.3977895 -4.3977342 -4.3975854 -4.39739 -4.3970933 -4.3967204 -4.3962197 -4.3956027 -4.3953338][-4.4000978 -4.400485 -4.4006882 -4.4006886 -4.4006143 -4.40048 -4.4002542 -4.3999925 -4.3996797 -4.3993082 -4.3988843 -4.3983622 -4.3977237 -4.3970323 -4.3966765][-4.4020796 -4.4026518 -4.4029384 -4.4029074 -4.4027505 -4.4025292 -4.4020991 -4.4016376 -4.4011264 -4.4006057 -4.4000969 -4.39953 -4.3988814 -4.398315 -4.3981342][-4.4024396 -4.4032025 -4.4036231 -4.4036655 -4.4034777 -4.4031916 -4.4026513 -4.4021254 -4.4015522 -4.4010363 -4.4005966 -4.400074 -4.3994842 -4.3990507 -4.3990693][-4.4014387 -4.4021115 -4.4025807 -4.4026747 -4.4024577 -4.4020929 -4.4015756 -4.4011526 -4.4007907 -4.4005036 -4.4003277 -4.4000058 -4.3995571 -4.3992906 -4.399385][-4.3998065 -4.3999977 -4.4001112 -4.3999386 -4.3996196 -4.3992348 -4.3988996 -4.3987646 -4.3988371 -4.399014 -4.3993254 -4.399333 -4.3991013 -4.3990941 -4.3993349][-4.3981209 -4.3978119 -4.3975878 -4.39726 -4.3969445 -4.396615 -4.3962379 -4.3960462 -4.3963008 -4.3968706 -4.3976974 -4.3981462 -4.3982534 -4.39861 -4.3991942][-4.3969884 -4.3966532 -4.3964686 -4.3962836 -4.3960614 -4.3956118 -4.3947864 -4.3941684 -4.3942804 -4.3949122 -4.3959918 -4.3967557 -4.3972268 -4.3980384 -4.3990617][-4.396822 -4.3968964 -4.3971162 -4.3972864 -4.3972263 -4.3967762 -4.3957562 -4.3948231 -4.39453 -4.3948293 -4.3956294 -4.3963227 -4.3968759 -4.3978176 -4.3991227][-4.3975616 -4.3980136 -4.3984914 -4.3988733 -4.3990226 -4.3988743 -4.3982267 -4.3973932 -4.3968768 -4.3967271 -4.39701 -4.3973637 -4.3976974 -4.398397 -4.3995743][-4.3984079 -4.3988338 -4.3991747 -4.3994508 -4.3996239 -4.3997717 -4.399539 -4.3990431 -4.3986745 -4.3984866 -4.3985434 -4.3986278 -4.3986459 -4.3989229 -4.3997192][-4.3995295 -4.3997993 -4.3998628 -4.3998032 -4.3997612 -4.3997579 -4.3994756 -4.3991022 -4.3989978 -4.3990345 -4.3991022 -4.3991189 -4.3990021 -4.398911 -4.39921][-4.399116 -4.399209 -4.3989615 -4.3986216 -4.3983383 -4.398057 -4.3975759 -4.3972149 -4.3973331 -4.3975868 -4.3977141 -4.3977365 -4.3976569 -4.3973646 -4.397244][-4.3976064 -4.3975439 -4.3971624 -4.3967018 -4.396297 -4.3959389 -4.3954554 -4.3951859 -4.3953452 -4.3956389 -4.3957539 -4.3957634 -4.3957386 -4.3953242 -4.3948588][-4.3964658 -4.3963389 -4.3959975 -4.3956146 -4.395257 -4.3949676 -4.3946366 -4.3944569 -4.3945355 -4.394671 -4.3947039 -4.3947048 -4.3946328 -4.3940983 -4.3934326]]...]
INFO - root - 2017-12-05 08:13:52.533516: step 710, loss = 1.46, batch loss = 1.40 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:21s remains)
INFO - root - 2017-12-05 08:13:54.701006: step 720, loss = 1.37, batch loss = 1.31 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:27s remains)
INFO - root - 2017-12-05 08:13:56.863982: step 730, loss = 1.64, batch loss = 1.58 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:32s remains)
INFO - root - 2017-12-05 08:13:59.070994: step 740, loss = 1.38, batch loss = 1.32 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:56s remains)
INFO - root - 2017-12-05 08:14:01.235070: step 750, loss = 1.33, batch loss = 1.27 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:46s remains)
INFO - root - 2017-12-05 08:14:03.413543: step 760, loss = 1.35, batch loss = 1.29 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:26s remains)
INFO - root - 2017-12-05 08:14:05.587602: step 770, loss = 1.28, batch loss = 1.22 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:25s remains)
INFO - root - 2017-12-05 08:14:07.764658: step 780, loss = 1.47, batch loss = 1.41 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:51s remains)
INFO - root - 2017-12-05 08:14:09.935249: step 790, loss = 1.28, batch loss = 1.22 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:04s remains)
INFO - root - 2017-12-05 08:14:12.127098: step 800, loss = 1.16, batch loss = 1.10 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:42s remains)
2017-12-05 08:14:12.409416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.724406 -1.8492377 -1.9053402 -1.8134301 -1.5757289 -1.2165916 -0.78819871 -0.39946413 -0.13584471 -0.01595068 -0.047768593 -0.17231178 -0.29630852 -0.34516525 -0.28337669][-1.8331692 -1.9534783 -2.0174556 -1.9275205 -1.6673195 -1.2520325 -0.72736216 -0.21402311 0.18481731 0.43219137 0.506588 0.45214748 0.36403942 0.32643557 0.37957][-1.6595662 -1.7765775 -1.8625791 -1.8089857 -1.5776367 -1.1711121 -0.63097453 -0.075460434 0.38547993 0.70264387 0.84428406 0.84573841 0.79254246 0.76191711 0.7969923][-1.0605903 -1.1653008 -1.2812667 -1.292206 -1.1369014 -0.80363131 -0.32842875 0.18247986 0.62354708 0.93858242 1.0904336 1.1069212 1.0662708 1.0391793 1.0629964][-0.21915388 -0.28021288 -0.41358089 -0.49038076 -0.43262291 -0.21089268 0.14868259 0.55674124 0.9152627 1.1677651 1.2805729 1.2788138 1.2347975 1.2149749 1.2469854][0.58819532 0.59758377 0.47849941 0.36485386 0.34184742 0.454494 0.68980885 0.97622967 1.2174835 1.3644657 1.3999705 1.3529825 1.2972889 1.2938581 1.3569541][1.1169224 1.2184305 1.1563106 1.0561113 0.99878836 1.0341821 1.1648264 1.3329201 1.4437099 1.4648056 1.4027491 1.29918 1.2332921 1.257226 1.3726668][1.2599549 1.4582882 1.4894853 1.4584622 1.4234424 1.4315276 1.4956398 1.568059 1.5537863 1.4449239 1.2776918 1.1068311 1.0230274 1.0681605 1.2364559][1.0473514 1.3274426 1.4592752 1.5225444 1.5397072 1.5488558 1.5743146 1.5749211 1.4575992 1.238307 0.97724676 0.73745918 0.61875868 0.66014957 0.85612345][0.60549021 0.94021749 1.1427717 1.2694874 1.3145723 1.3055363 1.2779975 1.2090106 1.0293088 0.75681639 0.45105696 0.16836596 0.0052804947 0.00900507 0.19618607][0.097504616 0.4576478 0.67863417 0.80219746 0.8180275 0.75482655 0.65905952 0.532568 0.33452129 0.066082954 -0.23772621 -0.53933406 -0.74942517 -0.80856419 -0.66948009][-0.29571438 0.045703411 0.2211175 0.2683959 0.18853855 0.029548645 -0.145576 -0.3126111 -0.49794459 -0.72929049 -1.0020502 -1.3015296 -1.5473008 -1.6722941 -1.6009247][-0.45220923 -0.17618704 -0.10732603 -0.19977999 -0.43283105 -0.72845626 -1.0016534 -1.2143617 -1.3885262 -1.578058 -1.8037527 -2.0715253 -2.3195572 -2.4777489 -2.4585457][-0.36414194 -0.17954206 -0.25021982 -0.51317716 -0.9250443 -1.3798261 -1.7695742 -2.0472393 -2.2323315 -2.3962905 -2.5764802 -2.7919574 -3.003283 -3.1499236 -3.1457686][-0.092867374 -0.0057096481 -0.20907116 -0.6266799 -1.1979396 -1.8012226 -2.3119497 -2.6700888 -2.8935814 -3.0560875 -3.2025027 -3.3608937 -3.5169616 -3.6254761 -3.613251]]...]
INFO - root - 2017-12-05 08:14:14.565681: step 810, loss = 1.34, batch loss = 1.28 (37.0 examples/sec; 0.217 sec/batch; 19h:56m:52s remains)
INFO - root - 2017-12-05 08:14:16.708546: step 820, loss = 1.52, batch loss = 1.46 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:04s remains)
INFO - root - 2017-12-05 08:14:18.867713: step 830, loss = 1.21, batch loss = 1.15 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:41s remains)
INFO - root - 2017-12-05 08:14:21.036375: step 840, loss = 1.11, batch loss = 1.05 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:23s remains)
INFO - root - 2017-12-05 08:14:23.221436: step 850, loss = 1.71, batch loss = 1.65 (39.1 examples/sec; 0.204 sec/batch; 18h:49m:36s remains)
INFO - root - 2017-12-05 08:14:25.393181: step 860, loss = 1.46, batch loss = 1.40 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:00s remains)
INFO - root - 2017-12-05 08:14:27.566400: step 870, loss = 1.20, batch loss = 1.14 (36.8 examples/sec; 0.218 sec/batch; 20h:02m:59s remains)
INFO - root - 2017-12-05 08:14:29.739889: step 880, loss = 1.49, batch loss = 1.43 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:25s remains)
INFO - root - 2017-12-05 08:14:31.927851: step 890, loss = 1.17, batch loss = 1.11 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:55s remains)
INFO - root - 2017-12-05 08:14:34.081673: step 900, loss = 1.65, batch loss = 1.59 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:14s remains)
2017-12-05 08:14:34.364771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3924279 -4.3897238 -4.3867054 -4.383214 -4.378983 -4.3748612 -4.3719392 -4.3709216 -4.3716235 -4.373982 -4.3787956 -4.3848882 -4.3909807 -4.3958774 -4.3990407][-4.3860068 -4.3828125 -4.3798237 -4.37664 -4.3726187 -4.3686738 -4.3658533 -4.3654256 -4.3669472 -4.3700643 -4.3752375 -4.3814268 -4.3875122 -4.39249 -4.3961754][-4.3813529 -4.3773537 -4.3743057 -4.3714972 -4.3677578 -4.364059 -4.3614931 -4.3616419 -4.3639007 -4.3676 -4.3726764 -4.3783317 -4.3836765 -4.3879623 -4.3913808][-4.37971 -4.3750229 -4.3716631 -4.368865 -4.3654504 -4.3619561 -4.35944 -4.3597445 -4.3625803 -4.3666949 -4.3716712 -4.3767815 -4.3811069 -4.3841972 -4.3865967][-4.3803344 -4.3751907 -4.3716435 -4.3689508 -4.3658628 -4.3623776 -4.3593574 -4.3590145 -4.3617091 -4.3661828 -4.3711367 -4.3757405 -4.37941 -4.3816481 -4.3832622][-4.38209 -4.3768625 -4.3732162 -4.3705869 -4.3675766 -4.3638492 -4.3600216 -4.358139 -4.3596392 -4.3637238 -4.3684545 -4.3729892 -4.3767447 -4.3791337 -4.3807869][-4.3848772 -4.3797688 -4.3761396 -4.3731837 -4.36973 -4.3655534 -4.3607168 -4.3570356 -4.3570457 -4.3600841 -4.3638206 -4.3677945 -4.3718948 -4.3752761 -4.3778234][-4.3891468 -4.3841553 -4.3801975 -4.3763494 -4.3724718 -4.3682232 -4.3630428 -4.3582091 -4.3568292 -4.3581533 -4.3600349 -4.3625417 -4.3661613 -4.3703027 -4.3741832][-4.393115 -4.3885827 -4.3844008 -4.38017 -4.3764205 -4.3727937 -4.3686023 -4.3640065 -4.3613849 -4.3605046 -4.3603015 -4.3610134 -4.3633742 -4.367106 -4.3715534][-4.3947258 -4.3908868 -4.3867726 -4.3827848 -4.3795619 -4.3770456 -4.3741632 -4.3705225 -4.367496 -4.3651891 -4.3634672 -4.3629942 -4.3643217 -4.3672986 -4.3714175][-4.39235 -4.38903 -4.3850036 -4.3809924 -4.3778858 -4.3757982 -4.3740182 -4.371563 -4.3689203 -4.3666224 -4.3648863 -4.3643627 -4.365459 -4.3680649 -4.3718715][-4.3861985 -4.3831482 -4.3793597 -4.3753719 -4.371841 -4.3690243 -4.3670387 -4.3652439 -4.3634777 -4.3623891 -4.36196 -4.3624644 -4.3641148 -4.36724 -4.3711691][-4.3788252 -4.3761387 -4.3727651 -4.3687816 -4.3645549 -4.360764 -4.3581238 -4.3563685 -4.355258 -4.3553433 -4.3563681 -4.3582463 -4.36105 -4.364984 -4.369205][-4.3750377 -4.3730359 -4.3703127 -4.3665133 -4.361887 -4.3573666 -4.3537827 -4.3512392 -4.3497777 -4.34991 -4.3515153 -4.3542905 -4.3581882 -4.3628869 -4.3675418][-4.3782039 -4.3770809 -4.3750558 -4.3717046 -4.3673849 -4.3629594 -4.3589563 -4.355648 -4.3535771 -4.3533096 -4.35477 -4.3576694 -4.3616257 -4.3661704 -4.3706512]]...]
INFO - root - 2017-12-05 08:14:36.534020: step 910, loss = 1.54, batch loss = 1.48 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:20s remains)
INFO - root - 2017-12-05 08:14:38.723302: step 920, loss = 1.54, batch loss = 1.48 (35.6 examples/sec; 0.225 sec/batch; 20h:40m:56s remains)
INFO - root - 2017-12-05 08:14:40.934360: step 930, loss = 1.53, batch loss = 1.47 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:38s remains)
INFO - root - 2017-12-05 08:14:43.081176: step 940, loss = 1.61, batch loss = 1.55 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:07s remains)
INFO - root - 2017-12-05 08:14:45.240992: step 950, loss = 1.80, batch loss = 1.74 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:12s remains)
INFO - root - 2017-12-05 08:14:47.405681: step 960, loss = 1.53, batch loss = 1.47 (36.6 examples/sec; 0.218 sec/batch; 20h:06m:47s remains)
INFO - root - 2017-12-05 08:14:49.590106: step 970, loss = 1.76, batch loss = 1.70 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:08s remains)
INFO - root - 2017-12-05 08:14:51.793535: step 980, loss = 1.53, batch loss = 1.47 (35.8 examples/sec; 0.223 sec/batch; 20h:33m:24s remains)
INFO - root - 2017-12-05 08:14:54.005528: step 990, loss = 1.87, batch loss = 1.81 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:56s remains)
INFO - root - 2017-12-05 08:14:56.196125: step 1000, loss = 1.67, batch loss = 1.61 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:58s remains)
2017-12-05 08:14:56.467045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.395853 -4.3960195 -4.3972468 -4.3982592 -4.3989949 -4.3988481 -4.3983703 -4.3983359 -4.3992057 -4.4006886 -4.4020863 -4.4026294 -4.4030142 -4.4033904 -4.4030461][-4.3985791 -4.3990922 -4.4001861 -4.4009604 -4.4015088 -4.4012308 -4.4007535 -4.4007912 -4.4015865 -4.4027724 -4.4036283 -4.4036517 -4.4034882 -4.4035435 -4.4030032][-4.401351 -4.4020061 -4.4029417 -4.4035392 -4.4040227 -4.4038043 -4.40327 -4.4032187 -4.403729 -4.4043741 -4.4045467 -4.4040308 -4.4035449 -4.4033875 -4.4027905][-4.4032784 -4.4036503 -4.404202 -4.4045172 -4.40482 -4.4046378 -4.4040556 -4.4039745 -4.404295 -4.4044442 -4.404139 -4.4034257 -4.4030452 -4.4029469 -4.4024611][-4.4050331 -4.4051428 -4.4053206 -4.4052644 -4.4052658 -4.4049344 -4.404283 -4.404181 -4.4043274 -4.4042244 -4.4037971 -4.4031758 -4.4028807 -4.4027991 -4.4023843][-4.4062409 -4.4061189 -4.4060717 -4.4058838 -4.4057126 -4.4052491 -4.4044642 -4.4042225 -4.4042993 -4.4041963 -4.4038548 -4.4033623 -4.4030147 -4.4028239 -4.4023757][-4.405632 -4.4052372 -4.4050488 -4.4049277 -4.4048905 -4.4046564 -4.4041443 -4.4041405 -4.4043536 -4.4043436 -4.4039907 -4.4034719 -4.4030437 -4.4027538 -4.4023561][-4.4050584 -4.4045844 -4.4043851 -4.404326 -4.4043965 -4.4044213 -4.4041991 -4.4043322 -4.4045343 -4.404377 -4.4039936 -4.4035072 -4.4030581 -4.4027653 -4.402379][-4.4053197 -4.4049411 -4.40481 -4.4048471 -4.4049664 -4.4051161 -4.4049273 -4.4048738 -4.404799 -4.4044051 -4.4039578 -4.4034977 -4.40307 -4.402792 -4.4024167][-4.4055152 -4.40525 -4.4051642 -4.4051995 -4.405302 -4.4053931 -4.4050813 -4.4047613 -4.4044409 -4.4039187 -4.4034247 -4.4030604 -4.4028068 -4.4026742 -4.4023843][-4.4055848 -4.4054146 -4.4053993 -4.4054437 -4.405477 -4.4053936 -4.4049072 -4.40437 -4.4039183 -4.4033551 -4.402854 -4.4026155 -4.402575 -4.4025688 -4.4023376][-4.4055028 -4.4053788 -4.4053764 -4.4054089 -4.4053712 -4.4051409 -4.4045553 -4.4039059 -4.4033909 -4.4028368 -4.4023862 -4.4022546 -4.4023552 -4.4024291 -4.4022465][-4.4051433 -4.4050436 -4.4050117 -4.4050145 -4.4049544 -4.4046655 -4.4041 -4.4035244 -4.4030519 -4.4025741 -4.4021726 -4.4020185 -4.4021354 -4.4022589 -4.4021487][-4.4045291 -4.4044371 -4.4043789 -4.4043078 -4.4041934 -4.403882 -4.4034295 -4.4030056 -4.4026623 -4.4023118 -4.4019437 -4.4017835 -4.4019156 -4.4020762 -4.4020424][-4.4037981 -4.4036717 -4.4035826 -4.4034834 -4.4033418 -4.40309 -4.4027734 -4.4024706 -4.4022303 -4.4019537 -4.4016919 -4.4015889 -4.4017277 -4.4018989 -4.4019113]]...]
INFO - root - 2017-12-05 08:14:58.632135: step 1010, loss = 1.78, batch loss = 1.72 (38.0 examples/sec; 0.210 sec/batch; 19h:22m:49s remains)
INFO - root - 2017-12-05 08:15:00.803944: step 1020, loss = 1.36, batch loss = 1.30 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:59s remains)
INFO - root - 2017-12-05 08:15:02.980902: step 1030, loss = 1.36, batch loss = 1.30 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:06s remains)
INFO - root - 2017-12-05 08:15:05.160881: step 1040, loss = 1.81, batch loss = 1.75 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:14s remains)
INFO - root - 2017-12-05 08:15:07.326212: step 1050, loss = 1.65, batch loss = 1.59 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:04s remains)
INFO - root - 2017-12-05 08:15:09.500181: step 1060, loss = 1.35, batch loss = 1.29 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:40s remains)
INFO - root - 2017-12-05 08:15:11.662532: step 1070, loss = 1.56, batch loss = 1.50 (39.0 examples/sec; 0.205 sec/batch; 18h:53m:22s remains)
INFO - root - 2017-12-05 08:15:13.855401: step 1080, loss = 1.42, batch loss = 1.36 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:53s remains)
INFO - root - 2017-12-05 08:15:16.027407: step 1090, loss = 1.53, batch loss = 1.47 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:55s remains)
INFO - root - 2017-12-05 08:15:18.185266: step 1100, loss = 0.99, batch loss = 0.93 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:42s remains)
2017-12-05 08:15:18.477083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4033833 -4.4033146 -4.4032822 -4.4033003 -4.4033055 -4.403266 -4.4032059 -4.4031649 -4.4031057 -4.4030776 -4.4030609 -4.4030237 -4.4030156 -4.4030695 -4.4031205][-4.4033318 -4.403213 -4.40312 -4.4030552 -4.4029441 -4.4027734 -4.4026184 -4.4024615 -4.4022603 -4.4021511 -4.4021349 -4.4021029 -4.4021196 -4.4022279 -4.4023161][-4.402987 -4.402864 -4.40275 -4.4026093 -4.4023423 -4.401999 -4.4017215 -4.4014215 -4.4010596 -4.4008846 -4.4008842 -4.4008703 -4.4009404 -4.4011188 -4.401238][-4.4021177 -4.4020295 -4.4019012 -4.4016871 -4.4012437 -4.4007072 -4.4002833 -4.3997993 -4.3992019 -4.3988829 -4.3988333 -4.3988881 -4.3991323 -4.3994431 -4.3996439][-4.400919 -4.4007044 -4.4003563 -4.3998857 -4.3992171 -4.3984227 -4.3976693 -4.396771 -4.3957887 -4.395174 -4.3949571 -4.395102 -4.3957105 -4.3964286 -4.3970075][-4.3995819 -4.3988495 -4.3977232 -4.396523 -4.3953261 -4.3941364 -4.3927617 -4.3910842 -4.3896489 -4.3888178 -4.3884773 -4.3887773 -4.3898392 -4.3911238 -4.3925514][-4.3981023 -4.3961716 -4.3934817 -4.3907738 -4.3886604 -4.3870149 -4.3849649 -4.3826084 -4.3811131 -4.3805013 -4.3801017 -4.3806214 -4.3823409 -4.384439 -4.3868704][-4.3964491 -4.39293 -4.3884339 -4.3840556 -4.3809218 -4.3789496 -4.376719 -4.3741355 -4.3729496 -4.372849 -4.3728123 -4.3737879 -4.3763871 -4.3793674 -4.38254][-4.3950028 -4.3901868 -4.3843431 -4.3791656 -4.3757472 -4.3737793 -4.3717432 -4.3693113 -4.368433 -4.3689 -4.3694763 -4.3710647 -4.3743396 -4.3780093 -4.3815508][-4.3938184 -4.3885484 -4.3825903 -4.3777342 -4.3748856 -4.3734488 -4.3719521 -4.3700776 -4.3695612 -4.3704224 -4.3715544 -4.3735666 -4.3770881 -4.3806491 -4.3837481][-4.3932467 -4.3885474 -4.3835397 -4.3797288 -4.3777947 -4.3773637 -4.37684 -4.3756595 -4.3754506 -4.3764167 -4.3776917 -4.3796363 -4.382586 -4.3853397 -4.3874617][-4.393755 -4.39016 -4.3864574 -4.3839641 -4.3831115 -4.3836093 -4.3837838 -4.3832097 -4.3831286 -4.3839788 -4.3851242 -4.3865027 -4.3883166 -4.3899765 -4.39108][-4.3952074 -4.3927345 -4.3903913 -4.388999 -4.3889151 -4.389761 -4.3903046 -4.3901081 -4.3900533 -4.3906827 -4.3914986 -4.392345 -4.3932109 -4.393827 -4.394177][-4.3969417 -4.3954515 -4.3941741 -4.3935766 -4.39384 -4.3946495 -4.39532 -4.3953881 -4.3953395 -4.3956194 -4.395987 -4.3963561 -4.3965883 -4.3966079 -4.3965669][-4.3983307 -4.3975549 -4.3968735 -4.3966069 -4.3969059 -4.3975568 -4.3981423 -4.3983521 -4.3984022 -4.3985066 -4.3985853 -4.3985758 -4.3984504 -4.3982363 -4.3980947]]...]
INFO - root - 2017-12-05 08:15:20.653600: step 1110, loss = 2.15, batch loss = 2.09 (36.6 examples/sec; 0.219 sec/batch; 20h:07m:07s remains)
INFO - root - 2017-12-05 08:15:22.832764: step 1120, loss = 1.92, batch loss = 1.86 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:22s remains)
INFO - root - 2017-12-05 08:15:25.037221: step 1130, loss = 1.78, batch loss = 1.72 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:55s remains)
INFO - root - 2017-12-05 08:15:27.196856: step 1140, loss = 1.73, batch loss = 1.67 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:03s remains)
INFO - root - 2017-12-05 08:15:29.400764: step 1150, loss = 1.62, batch loss = 1.56 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:59s remains)
INFO - root - 2017-12-05 08:15:31.574046: step 1160, loss = 1.37, batch loss = 1.31 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:23s remains)
INFO - root - 2017-12-05 08:15:33.737985: step 1170, loss = 1.53, batch loss = 1.48 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:47s remains)
INFO - root - 2017-12-05 08:15:35.907424: step 1180, loss = 1.51, batch loss = 1.45 (37.1 examples/sec; 0.215 sec/batch; 19h:49m:14s remains)
INFO - root - 2017-12-05 08:15:38.080501: step 1190, loss = 1.77, batch loss = 1.71 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:35s remains)
INFO - root - 2017-12-05 08:15:40.272526: step 1200, loss = 1.59, batch loss = 1.53 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:19s remains)
2017-12-05 08:15:40.571698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3753157 -3.428602 -3.511883 -3.6047587 -3.706049 -3.7991879 -3.8775852 -3.9419551 -4.0012655 -4.0581088 -4.112865 -4.1655645 -4.2168083 -4.2673612 -4.3148155][-2.6102009 -2.6596336 -2.7868531 -2.9442947 -3.1137619 -3.2635593 -3.3862123 -3.4874117 -3.5861273 -3.6929789 -3.8070393 -3.920994 -4.0311151 -4.1371608 -4.2330914][-1.7331479 -1.7618876 -1.9183543 -2.1323779 -2.3641837 -2.5658393 -2.7267613 -2.8576579 -2.9907246 -3.1511707 -3.3413687 -3.5431864 -3.7430096 -3.9333839 -4.1041546][-1.0665507 -1.0538824 -1.2017746 -1.4242804 -1.6694221 -1.8818021 -2.0491343 -2.1843297 -2.3284307 -2.523387 -2.7807064 -3.0755186 -3.3779702 -3.6673722 -3.92824][-0.85431218 -0.79628491 -0.89665055 -1.0651543 -1.2493572 -1.4085398 -1.5341473 -1.6371439 -1.7584016 -1.9507854 -2.2375848 -2.5970252 -2.9861226 -3.3674932 -3.7146182][-1.1393988 -1.0425801 -1.0660963 -1.1285572 -1.1946571 -1.2469268 -1.2838676 -1.3185132 -1.3874197 -1.5403898 -1.8062279 -2.1750369 -2.6062818 -3.0515881 -3.4681895][-1.7725883 -1.6408606 -1.5731795 -1.5111585 -1.4418523 -1.3710489 -1.3037117 -1.2560408 -1.2600524 -1.3494852 -1.5483296 -1.8644609 -2.2777174 -2.7415376 -3.1982977][-2.5254936 -2.3517845 -2.193538 -2.0211718 -1.8446474 -1.6856134 -1.5511346 -1.4464109 -1.3948138 -1.4108684 -1.5098991 -1.7199659 -2.0544837 -2.482928 -2.9426715][-3.1494451 -2.9346719 -2.7131009 -2.4843645 -2.2747321 -2.1063058 -1.9830515 -1.8829281 -1.8042214 -1.7474172 -1.7299261 -1.8022497 -2.013643 -2.3573127 -2.7760978][-3.5024605 -3.25955 -3.0231028 -2.8083811 -2.6475029 -2.5535235 -2.5202928 -2.4912963 -2.4269361 -2.3202643 -2.2026618 -2.1432805 -2.21945 -2.4498093 -2.7891214][-3.6495004 -3.3993654 -3.1819782 -3.0261722 -2.9554205 -2.9674253 -3.0476017 -3.1202869 -3.111671 -3.0068712 -2.8473339 -2.7113836 -2.683291 -2.7986283 -3.03274][-3.7394202 -3.509789 -3.3355432 -3.24356 -3.2469063 -3.33397 -3.48629 -3.6296308 -3.6836336 -3.6272638 -3.4971807 -3.3635645 -3.2966578 -3.3304565 -3.4578726][-3.8726745 -3.696604 -3.5740812 -3.5265017 -3.5610445 -3.6635785 -3.8192773 -3.9709864 -4.05533 -4.0531058 -3.9875386 -3.9072177 -3.8558722 -3.8549092 -3.9054263][-4.0548015 -3.9453518 -3.8704896 -3.8453016 -3.87565 -3.9505782 -4.0599504 -4.170898 -4.2455416 -4.2698421 -4.2537556 -4.2233138 -4.2006574 -4.1947894 -4.2084379][-4.2192721 -4.1664476 -4.12737 -4.1118269 -4.1260281 -4.1640663 -4.22077 -4.2814603 -4.3290415 -4.3538556 -4.3580885 -4.351593 -4.3445077 -4.3414526 -4.3437076]]...]
INFO - root - 2017-12-05 08:15:42.751589: step 1210, loss = 1.64, batch loss = 1.58 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:10s remains)
INFO - root - 2017-12-05 08:15:44.934621: step 1220, loss = 1.73, batch loss = 1.67 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:01s remains)
INFO - root - 2017-12-05 08:15:47.086233: step 1230, loss = 1.42, batch loss = 1.37 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:05s remains)
INFO - root - 2017-12-05 08:15:49.260935: step 1240, loss = 1.59, batch loss = 1.53 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:13s remains)
INFO - root - 2017-12-05 08:15:51.423726: step 1250, loss = 1.41, batch loss = 1.35 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:28s remains)
INFO - root - 2017-12-05 08:15:53.563455: step 1260, loss = 1.43, batch loss = 1.37 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:00s remains)
INFO - root - 2017-12-05 08:15:55.743585: step 1270, loss = 1.83, batch loss = 1.77 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:20s remains)
INFO - root - 2017-12-05 08:15:57.908277: step 1280, loss = 1.32, batch loss = 1.26 (38.2 examples/sec; 0.210 sec/batch; 19h:16m:44s remains)
INFO - root - 2017-12-05 08:16:00.056838: step 1290, loss = 1.49, batch loss = 1.43 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:45s remains)
INFO - root - 2017-12-05 08:16:02.246761: step 1300, loss = 1.86, batch loss = 1.80 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:27s remains)
2017-12-05 08:16:02.517050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3778219 -4.3380947 -4.2608848 -4.1555581 -4.0367851 -3.9241283 -3.8520494 -3.8434813 -3.8909459 -3.9679949 -4.0544143 -4.1420574 -4.2258444 -4.2941613 -4.303741][-4.3511939 -4.2873659 -4.1840763 -4.0566063 -3.9220519 -3.7912922 -3.69041 -3.6563435 -3.6945896 -3.7866459 -3.9045446 -4.0285139 -4.1455078 -4.2439623 -4.2850251][-4.3137059 -4.2273955 -4.1080432 -3.9732957 -3.8405528 -3.7110896 -3.6001782 -3.5522909 -3.5830097 -3.6791725 -3.813498 -3.9582729 -4.0919333 -4.2036738 -4.2691016][-4.274931 -4.1717215 -4.0471654 -3.9180508 -3.7989922 -3.6847253 -3.5820112 -3.5350914 -3.5614619 -3.64658 -3.7725627 -3.9134965 -4.0439925 -4.1523647 -4.2253389][-4.2446985 -4.1320491 -4.0103955 -3.8928204 -3.7907283 -3.6974857 -3.6130755 -3.5720754 -3.5882158 -3.6480765 -3.7416039 -3.8528996 -3.9612331 -4.0524206 -4.116878][-4.2287493 -4.1137319 -4.0000291 -3.8970575 -3.8117056 -3.7385709 -3.6711044 -3.6280303 -3.6222129 -3.6438699 -3.6859598 -3.747468 -3.8165846 -3.8776741 -3.9222531][-4.2280397 -4.1156025 -4.0102739 -3.9204431 -3.8476074 -3.7871652 -3.7266209 -3.6707022 -3.6342216 -3.6114244 -3.596365 -3.5985117 -3.6189117 -3.6440239 -3.6652694][-4.2408772 -4.138463 -4.04392 -3.9678626 -3.9076614 -3.855067 -3.796247 -3.7272954 -3.6603742 -3.5953805 -3.5285025 -3.4735913 -3.4428282 -3.4318476 -3.4330616][-4.262599 -4.1774464 -4.0987511 -4.0386448 -3.9925482 -3.9509997 -3.8997746 -3.8300128 -3.7461317 -3.6505573 -3.5444994 -3.4445684 -3.3715513 -3.3301523 -3.3165798][-4.2911863 -4.2274141 -4.1682014 -4.1255765 -4.0949183 -4.0681882 -4.0323448 -3.9770005 -3.895396 -3.7884395 -3.6628015 -3.5377786 -3.437839 -3.3756957 -3.3534591][-4.3247476 -4.2823677 -4.2432 -4.2170048 -4.2001128 -4.1884274 -4.1711116 -4.1387358 -4.0741277 -3.9752018 -3.8511159 -3.7226386 -3.6140337 -3.5424404 -3.5145929][-4.358119 -4.3350816 -4.3132877 -4.2999191 -4.2930803 -4.2904439 -4.2859564 -4.2709384 -4.227201 -4.1516089 -4.0501 -3.9400327 -3.8413556 -3.77343 -3.7437096][-4.383441 -4.3739781 -4.3643723 -4.3585892 -4.3564939 -4.3567853 -4.3564048 -4.3492284 -4.3243489 -4.2790313 -4.2124047 -4.1364365 -4.0636878 -4.0110946 -3.9844172][-4.3962011 -4.3934927 -4.3905439 -4.3888965 -4.3884764 -4.3889403 -4.3890963 -4.3852086 -4.3734169 -4.351697 -4.3185673 -4.2794909 -4.2375989 -4.2055106 -4.1859188][-4.4002724 -4.3999882 -4.3995962 -4.3992162 -4.3991075 -4.3992085 -4.39919 -4.3979383 -4.3937922 -4.3859916 -4.373847 -4.3585405 -4.340785 -4.3268495 -4.316587]]...]
INFO - root - 2017-12-05 08:16:04.686871: step 1310, loss = 1.25, batch loss = 1.19 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:12s remains)
INFO - root - 2017-12-05 08:16:06.846376: step 1320, loss = 1.54, batch loss = 1.48 (35.5 examples/sec; 0.226 sec/batch; 20h:45m:14s remains)
INFO - root - 2017-12-05 08:16:09.036824: step 1330, loss = 1.35, batch loss = 1.30 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:45s remains)
INFO - root - 2017-12-05 08:16:11.205492: step 1340, loss = 1.62, batch loss = 1.56 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:18s remains)
INFO - root - 2017-12-05 08:16:13.368066: step 1350, loss = 1.40, batch loss = 1.34 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:27s remains)
INFO - root - 2017-12-05 08:16:15.571141: step 1360, loss = 1.79, batch loss = 1.74 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:50s remains)
INFO - root - 2017-12-05 08:16:17.739214: step 1370, loss = 1.69, batch loss = 1.63 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:10s remains)
INFO - root - 2017-12-05 08:16:19.905339: step 1380, loss = 2.01, batch loss = 1.95 (36.1 examples/sec; 0.222 sec/batch; 20h:24m:05s remains)
INFO - root - 2017-12-05 08:16:22.079528: step 1390, loss = 1.60, batch loss = 1.54 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:44s remains)
INFO - root - 2017-12-05 08:16:24.276353: step 1400, loss = 1.57, batch loss = 1.51 (36.6 examples/sec; 0.219 sec/batch; 20h:07m:43s remains)
2017-12-05 08:16:24.573282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4009509 -4.4009748 -4.4009719 -4.4009633 -4.4009461 -4.4009047 -4.4008422 -4.4007683 -4.4006629 -4.40058 -4.4005327 -4.4005437 -4.4005961 -4.4006505 -4.4007349][-4.4008994 -4.4008822 -4.4008336 -4.4007554 -4.4006443 -4.4004817 -4.4002962 -4.4001064 -4.3999109 -4.3997517 -4.3997 -4.3998089 -4.3999972 -4.4002218 -4.4004736][-4.4007521 -4.4006982 -4.4005055 -4.4002023 -4.3998246 -4.3993506 -4.3989105 -4.3985233 -4.3982558 -4.3981056 -4.3981066 -4.3983812 -4.3988013 -4.3992672 -4.3997893][-4.4003572 -4.4002218 -4.3997669 -4.3990541 -4.3981757 -4.3972235 -4.3964257 -4.3958569 -4.3956375 -4.3956609 -4.3959079 -4.3965044 -4.3972535 -4.3979926 -4.3987856][-4.3997974 -4.3994808 -4.3986168 -4.39739 -4.3959765 -4.3945074 -4.3932824 -4.3924642 -4.3923464 -4.3927183 -4.3934021 -4.3944297 -4.3955717 -4.3966923 -4.3976879][-4.399157 -4.3985748 -4.3972731 -4.395515 -4.393559 -4.391603 -4.3900485 -4.389101 -4.3892055 -4.3900852 -4.3913145 -4.3927288 -4.3941817 -4.3955536 -4.3966594][-4.3986044 -4.3977408 -4.3962126 -4.3941078 -4.391736 -4.3894835 -4.3878284 -4.3869457 -4.3872938 -4.3886657 -4.3903494 -4.3921261 -4.3937869 -4.3952389 -4.3963223][-4.3980851 -4.397027 -4.3954496 -4.3933496 -4.3909688 -4.3887639 -4.3872623 -4.3865757 -4.3870788 -4.3887591 -4.3907876 -4.392808 -4.3946109 -4.3960443 -4.3969922][-4.3977804 -4.3966279 -4.3950768 -4.393147 -4.3910379 -4.3891788 -4.38806 -4.3877764 -4.3884645 -4.3902307 -4.392314 -4.3943443 -4.3960962 -4.3973374 -4.397985][-4.3975773 -4.3964958 -4.3951669 -4.3935895 -4.3919067 -4.3904643 -4.3897777 -4.389945 -4.390851 -4.3925056 -4.39429 -4.3959332 -4.3972683 -4.398159 -4.3984866][-4.3975387 -4.396596 -4.3955903 -4.3944707 -4.3933611 -4.3924031 -4.3919811 -4.39235 -4.3933468 -4.3947668 -4.3960958 -4.3972373 -4.3980894 -4.3985868 -4.3986225][-4.39781 -4.3970509 -4.3962955 -4.3956017 -4.3950734 -4.3947124 -4.3946514 -4.3951015 -4.3958354 -4.3967338 -4.3974762 -4.3980646 -4.3985138 -4.3986926 -4.3985782][-4.3979216 -4.3975205 -4.39716 -4.3968544 -4.3967175 -4.3967376 -4.3969254 -4.3974061 -4.3979478 -4.3984294 -4.398644 -4.3986835 -4.3987117 -4.3986669 -4.3985815][-4.3973918 -4.3973494 -4.3974552 -4.3975735 -4.3977532 -4.3980246 -4.3983607 -4.398808 -4.3992505 -4.3995466 -4.3995304 -4.3993263 -4.3991385 -4.3989763 -4.39892][-4.3959427 -4.3964219 -4.3970428 -4.3975215 -4.3979511 -4.3983707 -4.3987675 -4.3992052 -4.39961 -4.3999209 -4.4000306 -4.3999586 -4.399859 -4.3997397 -4.3997235]]...]
INFO - root - 2017-12-05 08:16:26.745065: step 1410, loss = 1.60, batch loss = 1.54 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:19s remains)
INFO - root - 2017-12-05 08:16:28.940056: step 1420, loss = 1.42, batch loss = 1.36 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:08s remains)
INFO - root - 2017-12-05 08:16:31.080785: step 1430, loss = 1.72, batch loss = 1.66 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:46s remains)
INFO - root - 2017-12-05 08:16:33.266098: step 1440, loss = 1.75, batch loss = 1.69 (36.5 examples/sec; 0.219 sec/batch; 20h:10m:41s remains)
INFO - root - 2017-12-05 08:16:35.473740: step 1450, loss = 1.81, batch loss = 1.75 (38.2 examples/sec; 0.209 sec/batch; 19h:15m:35s remains)
INFO - root - 2017-12-05 08:16:37.649220: step 1460, loss = 1.54, batch loss = 1.48 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:39s remains)
INFO - root - 2017-12-05 08:16:39.820676: step 1470, loss = 1.76, batch loss = 1.70 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:24s remains)
INFO - root - 2017-12-05 08:16:41.983800: step 1480, loss = 1.54, batch loss = 1.48 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:52s remains)
INFO - root - 2017-12-05 08:16:44.165943: step 1490, loss = 1.41, batch loss = 1.35 (35.8 examples/sec; 0.223 sec/batch; 20h:31m:35s remains)
INFO - root - 2017-12-05 08:16:46.352189: step 1500, loss = 1.75, batch loss = 1.69 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:30s remains)
2017-12-05 08:16:46.661772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4054809 -3.0817838 -2.7825885 -2.5619917 -2.4827125 -2.5760846 -2.815974 -3.1394062 -3.4761267 -3.7630539 -3.9704628 -4.1113105 -4.2098789 -4.2809196 -4.3307762][-2.7665 -2.2517958 -1.7842455 -1.445111 -1.3269377 -1.4753094 -1.8529336 -2.3636608 -2.8983622 -3.3599796 -3.7002277 -3.9334409 -4.0942421 -4.2092609 -4.2908449][-2.0963445 -1.3793819 -0.73607159 -0.27754784 -0.12237072 -0.32940483 -0.84830046 -1.5495012 -2.2853267 -2.9254227 -3.4047682 -3.7363176 -3.9649587 -4.1293058 -4.2468295][-1.5302198 -0.63295507 0.16423655 0.72371864 0.90869474 0.65323496 0.019822121 -0.83416748 -1.7308819 -2.5164733 -3.1143475 -3.535053 -3.8292534 -4.0438871 -4.1998186][-1.184057 -0.15892935 0.74692678 1.3756647 1.5822096 1.3022661 0.60869074 -0.32827902 -1.3146749 -2.1851487 -2.8592992 -3.3474324 -3.698149 -3.9595513 -4.1535249][-1.1076031 -0.019490719 0.94514418 1.6146693 1.8434739 1.5737329 0.882359 -0.059271336 -1.055522 -1.9434445 -2.6474748 -3.1790042 -3.5773447 -3.8821054 -4.1115971][-1.2585275 -0.15974665 0.83028889 1.5303597 1.7941117 1.5670004 0.92659712 0.040246487 -0.90528393 -1.7608099 -2.4604831 -3.0162082 -3.4553037 -3.8036883 -4.0692282][-1.5537293 -0.48513126 0.50211668 1.2236023 1.5287194 1.3623834 0.80060577 0.00089979172 -0.86484528 -1.6654792 -2.3428762 -2.9061704 -3.3709168 -3.7492497 -4.0391016][-1.9565079 -0.96443892 -0.018893242 0.69946432 1.0358996 0.9298563 0.45265865 -0.250319 -1.0198278 -1.7428722 -2.3714619 -2.90968 -3.3636055 -3.7399616 -4.0326252][-2.4450121 -1.5830884 -0.73440623 -0.065113068 0.27392244 0.21730471 -0.1742835 -0.76943684 -1.4248161 -2.0434341 -2.5873017 -3.0590386 -3.4601455 -3.795253 -4.0583854][-2.9724245 -2.2849939 -1.5860803 -1.0188491 -0.71664977 -0.74213576 -1.0495293 -1.528569 -2.0564635 -2.5508962 -2.9821944 -3.3521709 -3.6629651 -3.9221563 -4.1269813][-3.4709635 -2.9776978 -2.4597716 -2.0299377 -1.795258 -1.8093274 -2.0401616 -2.4027267 -2.7980478 -3.1580641 -3.4605582 -3.7095823 -3.9126387 -4.0805779 -4.214057][-3.8782151 -3.5692136 -3.2328191 -2.9489145 -2.7944932 -2.8107429 -2.974803 -3.2254043 -3.4906175 -3.7181475 -3.8933768 -4.0270481 -4.1316304 -4.2183228 -4.2891068][-4.15616 -3.9945934 -3.8118243 -3.65591 -3.5754051 -3.5952585 -3.6992078 -3.8475809 -3.9954603 -4.1104412 -4.1870112 -4.23814 -4.2755961 -4.3083272 -4.3382907][-4.3059516 -4.2391076 -4.1618314 -4.0968022 -4.0665307 -4.0827565 -4.1354575 -4.202332 -4.2638106 -4.3067188 -4.330647 -4.3438311 -4.35284 -4.3618689 -4.3714523]]...]
INFO - root - 2017-12-05 08:16:48.808324: step 1510, loss = 1.64, batch loss = 1.58 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:34s remains)
INFO - root - 2017-12-05 08:16:50.997085: step 1520, loss = 1.84, batch loss = 1.79 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:05s remains)
INFO - root - 2017-12-05 08:16:53.155726: step 1530, loss = 1.56, batch loss = 1.50 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:09s remains)
INFO - root - 2017-12-05 08:16:55.343893: step 1540, loss = 1.58, batch loss = 1.52 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:35s remains)
INFO - root - 2017-12-05 08:16:57.533202: step 1550, loss = 1.63, batch loss = 1.57 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:38s remains)
INFO - root - 2017-12-05 08:16:59.718861: step 1560, loss = 1.29, batch loss = 1.23 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:34s remains)
INFO - root - 2017-12-05 08:17:01.898292: step 1570, loss = 1.61, batch loss = 1.55 (35.7 examples/sec; 0.224 sec/batch; 20h:35m:08s remains)
INFO - root - 2017-12-05 08:17:04.074823: step 1580, loss = 1.99, batch loss = 1.93 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:23s remains)
INFO - root - 2017-12-05 08:17:06.225044: step 1590, loss = 1.90, batch loss = 1.84 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:24s remains)
INFO - root - 2017-12-05 08:17:08.359182: step 1600, loss = 1.50, batch loss = 1.45 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:50s remains)
2017-12-05 08:17:08.657235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5130918 -3.5496953 -3.5443292 -3.4819448 -3.3676608 -3.2316198 -3.10711 -3.0267191 -3.0131752 -3.0788975 -3.2042575 -3.3444304 -3.4597926 -3.5284004 -3.537951][-2.5852003 -2.5818024 -2.5159507 -2.3743088 -2.1768022 -1.9657803 -1.7858157 -1.6827729 -1.6822126 -1.8044393 -2.016978 -2.2549136 -2.45851 -2.5910349 -2.6299632][-1.4157319 -1.2902896 -1.0836179 -0.80010962 -0.48146868 -0.18888998 0.032671928 0.13798332 0.10609007 -0.080879688 -0.38045788 -0.71370435 -1.0133836 -1.2334096 -1.3398161][-0.20022011 0.13857365 0.56962442 1.0676732 1.5566473 1.9515862 2.2106819 2.303761 2.2341909 1.9946175 1.6359096 1.2367983 0.85487366 0.53579426 0.32023382][0.8596611 1.4637542 2.1782246 2.948595 3.6583948 4.1906796 4.5076771 4.6039391 4.5169325 4.2623062 3.8947592 3.476274 3.0473366 2.6447082 2.3010941][1.6059389 2.4554725 3.4540119 4.5180893 5.4845991 6.2036543 6.6318665 6.7847109 6.7307787 6.5101814 6.1806612 5.7910147 5.3635015 4.9137478 4.4611893][2.0291572 3.0163245 4.2126732 5.5323772 6.7655096 7.7155514 8.3184395 8.6004715 8.6376991 8.495719 8.23493 7.9028549 7.5130825 7.0591326 6.5488091][2.1733646 3.1525927 4.4102678 5.8971319 7.3501673 8.5295324 9.3377419 9.7972355 9.9683495 9.9285069 9.7470989 9.4745941 9.1340666 8.7066889 8.1897459][2.1016622 2.9540739 4.1379118 5.6605368 7.2217703 8.5575333 9.5282955 10.143562 10.438721 10.492264 10.371664 10.141855 9.8427534 9.4501171 8.95889][1.8023086 2.450593 3.4545083 4.8743763 6.4071379 7.77697 8.8163567 9.5100374 9.8641071 9.9581242 9.8524685 9.6309681 9.3454685 8.9768591 8.5249939][1.2317309 1.6564808 2.4144578 3.622263 4.9914217 6.2575369 7.2445059 7.9151936 8.252099 8.3247595 8.194046 7.9524336 7.6563625 7.3002691 6.888258][0.38218069 0.60735273 1.1037006 2.0301266 3.1403022 4.189683 5.0217152 5.5834522 5.8503027 5.8784018 5.7158165 5.4544878 5.1507282 4.8125906 4.4460373][-0.65471029 -0.57232261 -0.30631304 0.32670641 1.1327419 1.9079676 2.5335102 2.95051 3.1407256 3.1367917 2.9690785 2.72234 2.4395747 2.1395903 1.8300276][-1.6980512 -1.7075589 -1.6058798 -1.2279551 -0.70148516 -0.18294716 0.24965429 0.54118729 0.67833233 0.6829114 0.56014824 0.37187481 0.1393137 -0.10924292 -0.36683798][-2.6017561 -2.64647 -2.6361179 -2.4199476 -2.0835948 -1.7350423 -1.4229934 -1.1940548 -1.0558248 -0.99944234 -1.035496 -1.1436136 -1.3225155 -1.5425696 -1.7901924]]...]
INFO - root - 2017-12-05 08:17:10.854346: step 1610, loss = 1.49, batch loss = 1.43 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:17s remains)
INFO - root - 2017-12-05 08:17:13.022973: step 1620, loss = 2.12, batch loss = 2.06 (37.9 examples/sec; 0.211 sec/batch; 19h:24m:55s remains)
INFO - root - 2017-12-05 08:17:15.288515: step 1630, loss = 1.99, batch loss = 1.94 (37.3 examples/sec; 0.215 sec/batch; 19h:44m:02s remains)
INFO - root - 2017-12-05 08:17:17.468572: step 1640, loss = 1.36, batch loss = 1.30 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:39s remains)
INFO - root - 2017-12-05 08:17:19.623996: step 1650, loss = 1.80, batch loss = 1.74 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:20s remains)
INFO - root - 2017-12-05 08:17:21.793867: step 1660, loss = 1.71, batch loss = 1.66 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:02s remains)
INFO - root - 2017-12-05 08:17:23.955960: step 1670, loss = 2.19, batch loss = 2.13 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:36s remains)
INFO - root - 2017-12-05 08:17:26.118930: step 1680, loss = 1.78, batch loss = 1.72 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:27s remains)
INFO - root - 2017-12-05 08:17:28.292813: step 1690, loss = 1.60, batch loss = 1.54 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:13s remains)
INFO - root - 2017-12-05 08:17:30.484085: step 1700, loss = 1.30, batch loss = 1.25 (36.8 examples/sec; 0.218 sec/batch; 20h:00m:03s remains)
2017-12-05 08:17:30.754886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017717361 0.33737755 0.4214201 0.26170349 -0.11361647 -0.64634776 -1.2637234 -1.8668611 -2.3759012 -2.7491236 -2.9879189 -3.1268008 -3.2003264 -3.2633729 -3.3608744][0.79096508 1.1758633 1.234942 0.99986935 0.51478767 -0.1389823 -0.86633396 -1.5500433 -2.107486 -2.5000021 -2.7448087 -2.8885989 -2.9756575 -3.0590267 -3.1821346][1.2141724 1.6431694 1.7309508 1.4980969 0.98857355 0.29969358 -0.45522952 -1.1487122 -1.7012346 -2.0851521 -2.3327131 -2.4997792 -2.6333742 -2.7775507 -2.9610515][1.1756988 1.6680098 1.8534908 1.7286878 1.3218021 0.72152948 0.046877384 -0.57464814 -1.0725558 -1.4326987 -1.6956882 -1.9200382 -2.1465206 -2.4004455 -2.6868989][0.64402008 1.1828022 1.5036263 1.5707889 1.3793774 0.99374247 0.51783752 0.059410572 -0.32860088 -0.64539862 -0.93665004 -1.2533708 -1.6140833 -2.013907 -2.4306653][-0.29323292 0.24499369 0.68810844 0.97885895 1.0741763 0.9952035 0.80259323 0.57466936 0.33611012 0.068016529 -0.26855278 -0.70101166 -1.2121043 -1.7648244 -2.3077886][-1.4087787 -0.93787766 -0.43517637 0.044526577 0.42261744 0.66852045 0.79202175 0.81958008 0.74179125 0.52062035 0.13255215 -0.41563797 -1.0662425 -1.7450006 -2.3782997][-2.4145446 -2.0542061 -1.5611928 -0.97666025 -0.39237881 0.1162796 0.50653791 0.75123167 0.80029535 0.60701895 0.16977787 -0.464967 -1.2056448 -1.9507658 -2.6129034][-3.1111226 -2.8770695 -2.4566655 -1.87854 -1.2208014 -0.57488585 -0.024351597 0.35622263 0.48253727 0.30771494 -0.15333176 -0.82108068 -1.57727 -2.3106194 -2.9351969][-3.4835029 -3.3612049 -3.0508304 -2.5634878 -1.9648557 -1.3365872 -0.77153182 -0.3634305 -0.21503353 -0.36662626 -0.79630351 -1.413969 -2.0990262 -2.7462549 -3.2773485][-3.6628306 -3.6167965 -3.4177089 -3.0582755 -2.5902028 -2.0841029 -1.6190407 -1.2823396 -1.1648359 -1.2901018 -1.6367731 -2.1253488 -2.6615963 -3.1625876 -3.5667756][-3.7899003 -3.7893589 -3.687031 -3.4611065 -3.1479478 -2.7998137 -2.474966 -2.2446141 -2.17494 -2.2726429 -2.5161829 -2.8455868 -3.2019789 -3.5320022 -3.7994103][-3.9456952 -3.9610615 -3.9217796 -3.8026431 -3.6262202 -3.4246988 -3.2365665 -3.1047525 -3.0682187 -3.1259408 -3.2616024 -3.440114 -3.6329222 -3.8130195 -3.9659536][-4.1118712 -4.1223545 -4.1139779 -4.0643692 -3.9841168 -3.8875585 -3.7954431 -3.7285218 -3.7065494 -3.7265086 -3.7791286 -3.8479328 -3.9253995 -4.0030513 -4.0786834][-4.2403073 -4.2430787 -4.2447925 -4.2321186 -4.2065754 -4.1714878 -4.1335511 -4.1005311 -4.0829849 -4.0781274 -4.0825028 -4.0894709 -4.1020465 -4.1222253 -4.1528397]]...]
INFO - root - 2017-12-05 08:17:32.918905: step 1710, loss = 1.75, batch loss = 1.69 (37.1 examples/sec; 0.215 sec/batch; 19h:47m:25s remains)
INFO - root - 2017-12-05 08:17:35.093742: step 1720, loss = 1.35, batch loss = 1.30 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:46s remains)
INFO - root - 2017-12-05 08:17:37.287464: step 1730, loss = 1.45, batch loss = 1.39 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:26s remains)
INFO - root - 2017-12-05 08:17:39.501913: step 1740, loss = 1.64, batch loss = 1.58 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:07s remains)
INFO - root - 2017-12-05 08:17:41.668396: step 1750, loss = 1.63, batch loss = 1.57 (35.6 examples/sec; 0.225 sec/batch; 20h:38m:11s remains)
INFO - root - 2017-12-05 08:17:43.867467: step 1760, loss = 1.54, batch loss = 1.48 (35.6 examples/sec; 0.225 sec/batch; 20h:38m:30s remains)
INFO - root - 2017-12-05 08:17:46.027827: step 1770, loss = 1.69, batch loss = 1.63 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:38s remains)
INFO - root - 2017-12-05 08:17:48.206628: step 1780, loss = 1.99, batch loss = 1.93 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:28s remains)
INFO - root - 2017-12-05 08:17:50.384562: step 1790, loss = 1.58, batch loss = 1.53 (36.8 examples/sec; 0.217 sec/batch; 19h:57m:25s remains)
INFO - root - 2017-12-05 08:17:52.556180: step 1800, loss = 1.37, batch loss = 1.32 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:12s remains)
2017-12-05 08:17:52.817744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9208608 -3.8455455 -3.8192117 -3.8449216 -3.9142237 -4.0097308 -4.1104507 -4.2038522 -4.2808466 -4.3342028 -4.3619719 -4.3709168 -4.3707094 -4.3688989 -4.3689194][-3.8874631 -3.7849298 -3.7102032 -3.6689849 -3.6680455 -3.7130256 -3.800703 -3.9222136 -4.0553751 -4.1671019 -4.2313342 -4.2496195 -4.2408195 -4.2308073 -4.2358561][-3.8614595 -3.7288861 -3.5861177 -3.436933 -3.3087087 -3.2429278 -3.2691479 -3.3869963 -3.5636396 -3.7361569 -3.842494 -3.8708668 -3.8541896 -3.8467972 -3.8854072][-3.8027015 -3.6365514 -3.4159052 -3.1354594 -2.8421035 -2.6174021 -2.529675 -2.5938845 -2.7693365 -2.9689445 -3.1018105 -3.1447825 -3.1419039 -3.1731038 -3.2925618][-3.6650727 -3.4586396 -3.1633725 -2.7597909 -2.3008254 -1.8994596 -1.6612003 -1.6244972 -1.7446856 -1.9239242 -2.0599332 -2.1277521 -2.1750767 -2.2921193 -2.5423331][-3.4107232 -3.1502743 -2.7922668 -2.3023353 -1.7220213 -1.1726315 -0.78705621 -0.62520218 -0.65661335 -0.78662395 -0.91329408 -1.0139756 -1.1352506 -1.3637226 -1.7617052][-3.0805936 -2.7621531 -2.3608873 -1.8327813 -1.2008541 -0.568403 -0.070734978 0.21096802 0.28006411 0.20941305 0.090458393 -0.052608013 -0.25662565 -0.5913353 -1.1116891][-2.7645741 -2.3978257 -1.9760215 -1.4538994 -0.84048438 -0.2072134 0.33604956 0.69785595 0.85083961 0.83378077 0.71819496 0.53012419 0.24934244 -0.16617966 -0.75708032][-2.5683486 -2.1728473 -1.749284 -1.2631304 -0.71552324 -0.14709902 0.3654685 0.73694372 0.92140007 0.92710018 0.80190229 0.575377 0.24276209 -0.21250963 -0.8102777][-2.5440698 -2.1534445 -1.7501183 -1.3159707 -0.85651183 -0.39397 0.026116371 0.33675194 0.49112463 0.48128986 0.33821058 0.090450764 -0.25356102 -0.693326 -1.232584][-2.6697352 -2.3131506 -1.953651 -1.5853624 -1.2214861 -0.87997007 -0.58532953 -0.37634993 -0.2879467 -0.33283949 -0.48893404 -0.73004246 -1.0434914 -1.418642 -1.8505025][-2.8771982 -2.5760932 -2.2793329 -1.9852455 -1.7098248 -1.471045 -1.2871027 -1.1769235 -1.1579645 -1.2382383 -1.3976421 -1.6110117 -1.8653502 -2.1478691 -2.4529133][-3.0407319 -2.8042557 -2.5793588 -2.35942 -2.1580951 -1.9922633 -1.8807316 -1.8363354 -1.8669171 -1.9698014 -2.1218135 -2.2970529 -2.4838824 -2.6738052 -2.8644471][-3.1130941 -2.9401164 -2.7878222 -2.6385374 -2.5004554 -2.3855417 -2.3103497 -2.2896545 -2.3342907 -2.4388309 -2.5757518 -2.7163849 -2.8465297 -2.9611263 -3.0630226][-3.1456954 -3.0274019 -2.9423637 -2.8575475 -2.7755525 -2.69914 -2.6406524 -2.6174688 -2.6464143 -2.7269678 -2.8369942 -2.9470189 -3.0391333 -3.1065912 -3.1547403]]...]
INFO - root - 2017-12-05 08:17:54.997912: step 1810, loss = 1.62, batch loss = 1.56 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:55s remains)
INFO - root - 2017-12-05 08:17:57.195494: step 1820, loss = 1.71, batch loss = 1.65 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:27s remains)
INFO - root - 2017-12-05 08:17:59.378686: step 1830, loss = 1.62, batch loss = 1.56 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:50s remains)
INFO - root - 2017-12-05 08:18:01.554395: step 1840, loss = 1.40, batch loss = 1.34 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:29s remains)
INFO - root - 2017-12-05 08:18:03.751460: step 1850, loss = 1.79, batch loss = 1.73 (34.9 examples/sec; 0.229 sec/batch; 21h:01m:34s remains)
INFO - root - 2017-12-05 08:18:05.954017: step 1860, loss = 1.98, batch loss = 1.93 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:04s remains)
INFO - root - 2017-12-05 08:18:08.118265: step 1870, loss = 1.42, batch loss = 1.37 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:42s remains)
INFO - root - 2017-12-05 08:18:10.311165: step 1880, loss = 1.90, batch loss = 1.84 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:02s remains)
INFO - root - 2017-12-05 08:18:12.474489: step 1890, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:39s remains)
INFO - root - 2017-12-05 08:18:14.637343: step 1900, loss = 1.37, batch loss = 1.31 (37.3 examples/sec; 0.215 sec/batch; 19h:42m:04s remains)
2017-12-05 08:18:14.918347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3879025 -0.88042116 -0.69649744 -0.79475212 -1.0380266 -1.2910471 -1.4523435 -1.5017869 -1.467768 -1.4029291 -1.3528235 -1.3453753 -1.4058049 -1.5430989 -1.7473857][0.79167271 1.5580091 1.8286223 1.6931653 1.3606019 1.0281205 0.82755995 0.77965069 0.83704996 0.91808271 0.95998287 0.92195749 0.77617311 0.51007032 0.13260221][3.2644 4.2863417 4.6436834 4.489007 4.1077623 3.7479124 3.5536456 3.5386419 3.6286478 3.7188778 3.7338448 3.6259317 3.3719497 2.949954 2.3545938][5.2798576 6.499053 6.9407492 6.8094606 6.4487162 6.1470881 6.0329409 6.0949745 6.229949 6.328084 6.3160486 6.1513376 5.8159094 5.2664118 4.4701662][6.137207 7.4248104 7.9163256 7.8604174 7.6109133 7.4592943 7.5031385 7.6941452 7.9072247 8.0410833 8.0464468 7.8921347 7.5528612 6.9531879 6.0246458][5.5783377 6.7793541 7.26974 7.3218575 7.258482 7.3365602 7.6079512 7.979105 8.32458 8.55767 8.665554 8.629179 8.39281 7.8429375 6.8727484][3.8599281 4.8593531 5.3135195 5.4749994 5.6147108 5.9241743 6.4227924 6.9867182 7.5020466 7.8985252 8.1962223 8.3790388 8.3449717 7.9445448 7.0377398][1.5420384 2.2908688 2.699605 2.9638968 3.2691627 3.7358532 4.3708229 5.057519 5.7125435 6.2897339 6.8178806 7.2664804 7.4918365 7.3094292 6.5554237][-0.76549196 -0.26210403 0.0896821 0.42719126 0.84176588 1.3850312 2.0585208 2.7776265 3.4943185 4.1870518 4.8849821 5.545558 5.9977474 6.0320873 5.48489][-2.5990615 -2.3031836 -2.0241659 -1.7081563 -1.2984381 -0.77307653 -0.14568472 0.52957344 1.2380743 1.9758801 2.7381425 3.482882 4.0481243 4.2348509 3.8876944][-3.6899567 -3.549819 -3.3739622 -3.1336484 -2.8123796 -2.4086008 -1.917016 -1.3456316 -0.71987462 -0.040176868 0.68620682 1.405632 1.9881315 2.2585239 2.0864701][-4.1381831 -4.0718889 -3.9803481 -3.8402796 -3.6446295 -3.3871431 -3.0567327 -2.6557407 -2.1855743 -1.6496303 -1.0613897 -0.46104169 0.059631348 0.35775614 0.32546806][-4.2367506 -4.1919422 -4.137579 -4.0558176 -3.9437923 -3.7974839 -3.6072745 -3.3611739 -3.0585241 -2.7131429 -2.3257411 -1.9148903 -1.5405667 -1.3001282 -1.262495][-4.2261868 -4.17937 -4.1345234 -4.0756321 -4.0038667 -3.9141529 -3.7996054 -3.6611462 -3.5023849 -3.3225112 -3.1071157 -2.86976 -2.6431408 -2.486289 -2.4407809][-4.2077589 -4.1455641 -4.0921488 -4.045577 -4.0021358 -3.9559627 -3.8991585 -3.8266284 -3.7442145 -3.6532865 -3.5500462 -3.4358392 -3.3238935 -3.241833 -3.2039785]]...]
INFO - root - 2017-12-05 08:18:17.083711: step 1910, loss = 1.42, batch loss = 1.36 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:12s remains)
INFO - root - 2017-12-05 08:18:19.280100: step 1920, loss = 1.47, batch loss = 1.41 (35.8 examples/sec; 0.224 sec/batch; 20h:31m:54s remains)
INFO - root - 2017-12-05 08:18:21.447078: step 1930, loss = 1.87, batch loss = 1.81 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:42s remains)
INFO - root - 2017-12-05 08:18:23.613882: step 1940, loss = 1.78, batch loss = 1.72 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:41s remains)
INFO - root - 2017-12-05 08:18:25.798581: step 1950, loss = 1.47, batch loss = 1.41 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:13s remains)
INFO - root - 2017-12-05 08:18:28.016523: step 1960, loss = 1.52, batch loss = 1.46 (36.5 examples/sec; 0.219 sec/batch; 20h:07m:01s remains)
INFO - root - 2017-12-05 08:18:30.166969: step 1970, loss = 1.62, batch loss = 1.56 (36.7 examples/sec; 0.218 sec/batch; 19h:59m:24s remains)
INFO - root - 2017-12-05 08:18:32.368221: step 1980, loss = 1.91, batch loss = 1.86 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:57s remains)
INFO - root - 2017-12-05 08:18:34.516142: step 1990, loss = 1.93, batch loss = 1.88 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:56s remains)
INFO - root - 2017-12-05 08:18:36.680377: step 2000, loss = 1.75, batch loss = 1.69 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:35s remains)
2017-12-05 08:18:36.957415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3929963 -4.3929949 -4.3929939 -4.3929968 -4.3930054 -4.393003 -4.3928833 -4.3925934 -4.3922534 -4.3920054 -4.3921027 -4.3924842 -4.3928037 -4.3930235 -4.3930774][-4.39302 -4.3929958 -4.3929386 -4.3928742 -4.392796 -4.3925047 -4.3915439 -4.3898821 -4.3879957 -4.3866692 -4.3868113 -4.3883181 -4.3901343 -4.3915534 -4.3923612][-4.3929772 -4.3924923 -4.3918934 -4.3913836 -4.3908463 -4.3894305 -4.3857374 -4.3791189 -4.3702936 -4.3627777 -4.3608456 -4.366014 -4.3748684 -4.383184 -4.3883338][-4.3905058 -4.3871021 -4.3835545 -4.3808956 -4.3789454 -4.3758354 -4.3665757 -4.3463793 -4.3167844 -4.2899017 -4.2818456 -4.2980938 -4.328639 -4.3585973 -4.377244][-4.3768921 -4.3601627 -4.3416781 -4.3274894 -4.321156 -4.3192768 -4.3055191 -4.2646847 -4.2007484 -4.1430478 -4.129436 -4.1707492 -4.2430568 -4.3135266 -4.3580275][-4.3322992 -4.2715807 -4.2028389 -4.1518283 -4.1351867 -4.1468539 -4.1488781 -4.1065497 -4.0249372 -3.950798 -3.9462588 -4.0263886 -4.14996 -4.2651591 -4.3375378][-4.2306967 -4.0746303 -3.9003098 -3.7753751 -3.7443836 -3.7984467 -3.8684168 -3.8884721 -3.8546016 -3.8188195 -3.8508785 -3.9654262 -4.1162181 -4.2483964 -4.3295522][-4.0745592 -3.780591 -3.4582944 -3.2344341 -3.1960659 -3.3315635 -3.5324502 -3.691267 -3.7795119 -3.8352745 -3.9139431 -4.0331125 -4.1656423 -4.2740951 -4.3389344][-3.9229913 -3.5016921 -3.0479386 -2.7460017 -2.7229958 -2.9608188 -3.3115578 -3.6259604 -3.8431182 -3.9798849 -4.0798578 -4.1724229 -4.2576 -4.32166 -4.3587112][-3.8657944 -3.4023788 -2.912601 -2.6052077 -2.6184425 -2.9239984 -3.3534503 -3.7398748 -4.0062366 -4.1601605 -4.2444282 -4.2977467 -4.3370872 -4.3636303 -4.3783164][-3.9468811 -3.5586057 -3.1570077 -2.9211726 -2.9618645 -3.2453806 -3.6264086 -3.9602089 -4.1787028 -4.2928843 -4.3433356 -4.3657284 -4.37796 -4.3850441 -4.3889403][-4.1129193 -3.8669717 -3.6165454 -3.4776077 -3.5180311 -3.7113905 -3.9616711 -4.171792 -4.2992339 -4.3590975 -4.3814282 -4.3885369 -4.3911629 -4.3923473 -4.3928413][-4.2656856 -4.1495094 -4.0317492 -3.9692302 -3.9941516 -4.0912848 -4.2121553 -4.308465 -4.3614459 -4.3836708 -4.3911128 -4.3927283 -4.3929677 -4.3930154 -4.3930078][-4.3514857 -4.3114557 -4.2700672 -4.248127 -4.2579346 -4.2930794 -4.3357148 -4.3685937 -4.3849812 -4.3908811 -4.3927674 -4.3930187 -4.3930092 -4.3930006 -4.3929977][-4.3808656 -4.3693075 -4.3567853 -4.3494959 -4.3521352 -4.3626156 -4.3756971 -4.38661 -4.3916845 -4.3927255 -4.3929658 -4.3929987 -4.3929996 -4.3929977 -4.3929977]]...]
INFO - root - 2017-12-05 08:18:39.103278: step 2010, loss = 1.90, batch loss = 1.84 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:46s remains)
INFO - root - 2017-12-05 08:18:41.293489: step 2020, loss = 1.65, batch loss = 1.59 (35.8 examples/sec; 0.223 sec/batch; 20h:29m:41s remains)
INFO - root - 2017-12-05 08:18:43.475745: step 2030, loss = 1.70, batch loss = 1.64 (35.3 examples/sec; 0.226 sec/batch; 20h:47m:04s remains)
INFO - root - 2017-12-05 08:18:45.671024: step 2040, loss = 1.67, batch loss = 1.61 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:00s remains)
INFO - root - 2017-12-05 08:18:47.860605: step 2050, loss = 2.02, batch loss = 1.96 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:55s remains)
INFO - root - 2017-12-05 08:18:50.035755: step 2060, loss = 1.52, batch loss = 1.46 (36.6 examples/sec; 0.219 sec/batch; 20h:04m:15s remains)
INFO - root - 2017-12-05 08:18:52.210913: step 2070, loss = 1.83, batch loss = 1.78 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:18s remains)
INFO - root - 2017-12-05 08:18:54.391983: step 2080, loss = 1.90, batch loss = 1.84 (35.6 examples/sec; 0.225 sec/batch; 20h:38m:10s remains)
INFO - root - 2017-12-05 08:18:56.548840: step 2090, loss = 1.59, batch loss = 1.53 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:53s remains)
INFO - root - 2017-12-05 08:18:58.739682: step 2100, loss = 1.84, batch loss = 1.78 (36.3 examples/sec; 0.220 sec/batch; 20h:12m:04s remains)
2017-12-05 08:18:59.038682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1807518 -4.0201297 -3.8747325 -3.7848728 -3.752115 -3.7594266 -3.7781632 -3.8002052 -3.8356538 -3.9056365 -4.0123677 -4.1317616 -4.2327762 -4.2991476 -4.3360453][-4.1973348 -4.0360975 -3.89048 -3.7975087 -3.7592924 -3.7577057 -3.7696836 -3.7936401 -3.8424959 -3.9304504 -4.0471807 -4.1635141 -4.2543864 -4.3119411 -4.343564][-4.2005749 -4.0432262 -3.9047854 -3.8179798 -3.7801926 -3.7702789 -3.7719429 -3.7918916 -3.8493171 -3.9513485 -4.075489 -4.18865 -4.2713475 -4.3224196 -4.3502336][-4.1992178 -4.0511241 -3.9279661 -3.8547409 -3.82035 -3.8016994 -3.7896595 -3.7988944 -3.8557811 -3.9643447 -4.092751 -4.2037783 -4.281724 -4.3297782 -4.3558507][-4.1972404 -4.06544 -3.9646444 -3.9108503 -3.8832994 -3.8563886 -3.8281407 -3.8218787 -3.8694906 -3.9752645 -4.10303 -4.21227 -4.2880087 -4.334847 -4.3603072][-4.1954427 -4.085937 -4.0112443 -3.9772255 -3.9563646 -3.9208064 -3.8742919 -3.8469138 -3.8770971 -3.9738595 -4.10024 -4.2102933 -4.2876277 -4.3361988 -4.3627429][-4.19892 -4.1123619 -4.0598335 -4.0384207 -4.0198307 -3.9748573 -3.9099288 -3.8592122 -3.8667738 -3.9506135 -4.0766048 -4.1927996 -4.2766137 -4.3306727 -4.360981][-4.217176 -4.1518831 -4.1124468 -4.0914721 -4.0643888 -4.0047975 -3.9203541 -3.8458464 -3.8301063 -3.8993549 -4.0249472 -4.1519818 -4.24802 -4.3121476 -4.3494797][-4.2478275 -4.1987948 -4.1641035 -4.1341038 -4.0889273 -4.0098166 -3.9069574 -3.8132386 -3.7755153 -3.8263221 -3.9456565 -4.080276 -4.1904793 -4.2684312 -4.3171291][-4.2829003 -4.2457132 -4.2113819 -4.1701603 -4.1064363 -4.0092111 -3.8907 -3.7817774 -3.72474 -3.7534971 -3.8561788 -3.9863517 -4.1017404 -4.1893258 -4.2500367][-4.3152485 -4.2879972 -4.2548079 -4.2064495 -4.1304927 -4.023169 -3.8976367 -3.7813225 -3.7098951 -3.714062 -3.7880576 -3.8951895 -3.998256 -4.0827436 -4.1496124][-4.3380327 -4.3230562 -4.29612 -4.2496433 -4.1745672 -4.0711031 -3.9509747 -3.8367939 -3.7571397 -3.7375402 -3.7758934 -3.8444848 -3.9159639 -3.9798887 -4.0399022][-4.3440905 -4.344202 -4.329761 -4.2949295 -4.2337255 -4.1479406 -4.0462503 -3.9451425 -3.8660409 -3.8289165 -3.8330054 -3.8583744 -3.8889217 -3.9198287 -3.9570656][-4.3324475 -4.3488827 -4.3498034 -4.33206 -4.2921915 -4.2316818 -4.1563139 -4.0761609 -4.0059619 -3.9607568 -3.9398274 -3.9279115 -3.917743 -3.9118235 -3.9181297][-4.3101044 -4.3414326 -4.3562918 -4.3550735 -4.3366337 -4.3016915 -4.2537484 -4.1985807 -4.1449828 -4.1017947 -4.0683594 -4.0332003 -3.9933853 -3.9549873 -3.9277709]]...]
INFO - root - 2017-12-05 08:19:01.190852: step 2110, loss = 2.10, batch loss = 2.04 (35.2 examples/sec; 0.227 sec/batch; 20h:51m:39s remains)
INFO - root - 2017-12-05 08:19:03.378072: step 2120, loss = 1.71, batch loss = 1.65 (35.4 examples/sec; 0.226 sec/batch; 20h:44m:44s remains)
INFO - root - 2017-12-05 08:19:05.561367: step 2130, loss = 1.52, batch loss = 1.46 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:55s remains)
INFO - root - 2017-12-05 08:19:07.712553: step 2140, loss = 1.91, batch loss = 1.85 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:20s remains)
INFO - root - 2017-12-05 08:19:09.901147: step 2150, loss = 1.47, batch loss = 1.41 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:28s remains)
INFO - root - 2017-12-05 08:19:12.056055: step 2160, loss = 1.60, batch loss = 1.54 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:32s remains)
INFO - root - 2017-12-05 08:19:14.243995: step 2170, loss = 1.73, batch loss = 1.68 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:56s remains)
INFO - root - 2017-12-05 08:19:16.415553: step 2180, loss = 1.69, batch loss = 1.63 (37.3 examples/sec; 0.215 sec/batch; 19h:41m:28s remains)
INFO - root - 2017-12-05 08:19:18.560434: step 2190, loss = 1.61, batch loss = 1.55 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:35s remains)
INFO - root - 2017-12-05 08:19:20.746534: step 2200, loss = 1.78, batch loss = 1.72 (35.2 examples/sec; 0.228 sec/batch; 20h:52m:51s remains)
2017-12-05 08:19:21.038009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3889241 -4.3889241 -4.3889241 -4.3889241 -4.3889241 -4.3889241 -4.3889241 -4.3889246 -4.3889241 -4.3889241 -4.3889241 -4.3889246 -4.3889246 -4.3889241 -4.3889236][-4.3889236 -4.3889227 -4.3889232 -4.3889232 -4.3889232 -4.3889222 -4.3889227 -4.3889232 -4.3889227 -4.3889222 -4.3889232 -4.3889232 -4.3889227 -4.3889236 -4.3889241][-4.3889194 -4.388916 -4.388917 -4.3889184 -4.3889189 -4.3889174 -4.388917 -4.3889184 -4.3889213 -4.3889217 -4.3889213 -4.3889208 -4.3889208 -4.3889208 -4.3889227][-4.388916 -4.3889136 -4.3889117 -4.3889127 -4.3889117 -4.3889089 -4.3889103 -4.3889108 -4.3889141 -4.3889227 -4.3889213 -4.3889174 -4.3889184 -4.3889194 -4.3889208][-4.3889141 -4.3889132 -4.3889122 -4.3889122 -4.3889089 -4.3888974 -4.3888822 -4.3888731 -4.3888874 -4.3889117 -4.3889232 -4.38892 -4.3889151 -4.3889203 -4.3889174][-4.3888984 -4.3888993 -4.3889008 -4.3888941 -4.3888721 -4.388835 -4.3887711 -4.3887339 -4.3887768 -4.3888426 -4.3889046 -4.3889174 -4.3889194 -4.388926 -4.3889222][-4.3888593 -4.3888593 -4.3888669 -4.3888483 -4.3888111 -4.3887477 -4.3886209 -4.3885188 -4.3885684 -4.3886919 -4.3888178 -4.3888736 -4.3888965 -4.388907 -4.3889112][-4.3888717 -4.3888788 -4.3888888 -4.388875 -4.3888516 -4.3887982 -4.3886623 -4.38855 -4.388587 -4.3886943 -4.3888111 -4.3888593 -4.3888764 -4.3888836 -4.3888893][-4.3889003 -4.3889012 -4.3889055 -4.3889022 -4.3889079 -4.3888969 -4.3888316 -4.3887706 -4.388772 -4.3888021 -4.388864 -4.38888 -4.3888836 -4.3888845 -4.38889][-4.3889222 -4.3889194 -4.3889184 -4.388917 -4.388917 -4.3889136 -4.388905 -4.388886 -4.3888826 -4.3888855 -4.3889027 -4.3889112 -4.3889084 -4.3889132 -4.3889174][-4.38893 -4.3889313 -4.3889289 -4.3889232 -4.3889194 -4.388917 -4.38892 -4.3889227 -4.3889256 -4.3889265 -4.3889303 -4.3889284 -4.3889217 -4.3889227 -4.3889236][-4.3889275 -4.3889284 -4.388927 -4.3889241 -4.3889213 -4.3889222 -4.3889241 -4.3889275 -4.3889318 -4.388936 -4.388936 -4.38893 -4.3889275 -4.3889265 -4.3889275][-4.3889246 -4.3889256 -4.3889251 -4.3889241 -4.3889236 -4.3889246 -4.3889251 -4.3889251 -4.3889246 -4.3889256 -4.3889265 -4.388926 -4.3889275 -4.388926 -4.3889256][-4.3889251 -4.3889246 -4.3889241 -4.3889236 -4.3889236 -4.3889222 -4.3889222 -4.3889222 -4.3889222 -4.3889208 -4.3889217 -4.3889227 -4.3889236 -4.3889241 -4.3889236][-4.3889236 -4.3889236 -4.3889241 -4.3889236 -4.3889241 -4.3889256 -4.388926 -4.3889222 -4.3889213 -4.3889184 -4.3889179 -4.3889184 -4.3889217 -4.3889232 -4.3889232]]...]
INFO - root - 2017-12-05 08:19:23.228842: step 2210, loss = 2.17, batch loss = 2.11 (36.6 examples/sec; 0.218 sec/batch; 20h:02m:45s remains)
INFO - root - 2017-12-05 08:19:25.418877: step 2220, loss = 1.68, batch loss = 1.62 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:11s remains)
INFO - root - 2017-12-05 08:19:27.585537: step 2230, loss = 1.65, batch loss = 1.60 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:52s remains)
INFO - root - 2017-12-05 08:19:29.746581: step 2240, loss = 2.14, batch loss = 2.08 (38.4 examples/sec; 0.208 sec/batch; 19h:05m:36s remains)
INFO - root - 2017-12-05 08:19:31.915649: step 2250, loss = 1.98, batch loss = 1.92 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:49s remains)
INFO - root - 2017-12-05 08:19:34.072273: step 2260, loss = 2.12, batch loss = 2.07 (36.8 examples/sec; 0.217 sec/batch; 19h:55m:11s remains)
INFO - root - 2017-12-05 08:19:36.237655: step 2270, loss = 1.62, batch loss = 1.56 (37.6 examples/sec; 0.213 sec/batch; 19h:30m:10s remains)
INFO - root - 2017-12-05 08:19:38.408966: step 2280, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:35s remains)
INFO - root - 2017-12-05 08:19:40.573913: step 2290, loss = 1.78, batch loss = 1.72 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:36s remains)
INFO - root - 2017-12-05 08:19:42.740533: step 2300, loss = 1.45, batch loss = 1.39 (38.5 examples/sec; 0.208 sec/batch; 19h:03m:34s remains)
2017-12-05 08:19:43.036038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.756108 -2.0652022 -1.6522353 -1.566117 -1.7611828 -2.0627317 -2.3390677 -2.4800637 -2.4646573 -2.3083081 -2.0573208 -1.7331755 -1.4283402 -1.1742241 -1.0553198][-2.692708 -2.027564 -1.6624804 -1.6241219 -1.8422103 -2.1310575 -2.3762834 -2.4903007 -2.4639177 -2.3147116 -2.0828555 -1.7855043 -1.5171237 -1.3137705 -1.2463083][-2.7452703 -2.126188 -1.7993734 -1.774189 -1.9693022 -2.2116947 -2.413816 -2.5040414 -2.4893599 -2.3754961 -2.1950419 -1.9539487 -1.7483923 -1.6095176 -1.5932138][-2.9239154 -2.3669341 -2.0647831 -2.0204947 -2.1599748 -2.3406165 -2.4980876 -2.5740695 -2.5808191 -2.5030313 -2.3775148 -2.2094305 -2.0818739 -2.0222154 -2.0620131][-3.1929846 -2.7068496 -2.4207525 -2.3404503 -2.411994 -2.5389714 -2.6595247 -2.7353587 -2.7709234 -2.7300377 -2.657496 -2.5594459 -2.505198 -2.5222204 -2.6058826][-3.47586 -3.0552142 -2.7772365 -2.6598916 -2.6721501 -2.7608454 -2.8618622 -2.9374261 -2.9884391 -2.9877138 -2.9705172 -2.9334173 -2.9365144 -3.0078187 -3.110266][-3.6933386 -3.317811 -3.0430813 -2.8928912 -2.8583279 -2.9249916 -3.0222917 -3.1016397 -3.1590729 -3.1867485 -3.2126651 -3.2264051 -3.2665005 -3.3578682 -3.4513588][-3.7877095 -3.4306288 -3.160502 -3.0014114 -2.9508944 -3.0042067 -3.0970178 -3.1775711 -3.2388365 -3.2871232 -3.3409338 -3.3880391 -3.439744 -3.4990985 -3.5377142][-3.7652984 -3.3944447 -3.1270928 -2.9855824 -2.9523582 -3.012639 -3.0996637 -3.1683435 -3.2175393 -3.2684305 -3.3318677 -3.3892066 -3.4180672 -3.4146135 -3.3794737][-3.6741543 -3.2796583 -3.0180631 -2.9056 -2.9081767 -2.9894066 -3.0774345 -3.1217504 -3.1398349 -3.1718907 -3.2116797 -3.2352326 -3.2118938 -3.1327548 -3.0195451][-3.5832858 -3.1760759 -2.929255 -2.843905 -2.8813305 -2.9780347 -3.0579705 -3.0737143 -3.0553238 -3.0437937 -3.0327625 -3.005688 -2.9159055 -2.749383 -2.5584948][-3.5519257 -3.1505141 -2.9258921 -2.8650944 -2.9276361 -3.0242233 -3.0780871 -3.053648 -2.9950552 -2.9325089 -2.8643856 -2.779006 -2.6272011 -2.3943086 -2.1392984][-3.6031501 -3.2262366 -3.0248024 -2.9843931 -3.0638654 -3.1530094 -3.1724446 -3.1057663 -3.0051126 -2.8915911 -2.7664385 -2.617784 -2.4092641 -2.1406519 -1.8519301][-3.7169018 -3.3864541 -3.2167306 -3.1932712 -3.2801015 -3.3584623 -3.3463516 -3.2360415 -3.0894012 -2.9386034 -2.773366 -2.5818706 -2.3408108 -2.0519392 -1.7488408][-3.854763 -3.5845919 -3.4521737 -3.4487131 -3.5383024 -3.6006074 -3.5648637 -3.4307208 -3.258532 -3.0839877 -2.8946834 -2.6774774 -2.4251571 -2.1382322 -1.8517969]]...]
INFO - root - 2017-12-05 08:19:45.213278: step 2310, loss = 1.87, batch loss = 1.81 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:30s remains)
INFO - root - 2017-12-05 08:19:47.412551: step 2320, loss = 1.58, batch loss = 1.52 (35.1 examples/sec; 0.228 sec/batch; 20h:53m:42s remains)
INFO - root - 2017-12-05 08:19:49.576087: step 2330, loss = 1.43, batch loss = 1.37 (36.6 examples/sec; 0.219 sec/batch; 20h:02m:56s remains)
INFO - root - 2017-12-05 08:19:51.771575: step 2340, loss = 1.42, batch loss = 1.37 (36.6 examples/sec; 0.218 sec/batch; 20h:01m:50s remains)
INFO - root - 2017-12-05 08:19:53.938322: step 2350, loss = 1.63, batch loss = 1.57 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:21s remains)
INFO - root - 2017-12-05 08:19:56.117386: step 2360, loss = 1.65, batch loss = 1.59 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:41s remains)
INFO - root - 2017-12-05 08:19:58.297462: step 2370, loss = 1.48, batch loss = 1.42 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:48s remains)
INFO - root - 2017-12-05 08:20:00.459705: step 2380, loss = 1.62, batch loss = 1.57 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:38s remains)
INFO - root - 2017-12-05 08:20:02.657788: step 2390, loss = 1.52, batch loss = 1.46 (36.0 examples/sec; 0.222 sec/batch; 20h:21m:43s remains)
INFO - root - 2017-12-05 08:20:04.866418: step 2400, loss = 1.56, batch loss = 1.50 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:30s remains)
2017-12-05 08:20:05.163618: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6690464 1.1169415 0.37946987 -0.32870674 -0.88603783 -1.1795645 -1.2382367 -1.1311994 -1.0435112 -1.1558614 -1.5011373 -2.0187497 -2.5780718 -3.1190774 -3.5816054][2.0497775 1.467968 0.69194078 -0.038301468 -0.57742572 -0.82604623 -0.8361454 -0.70277405 -0.61684847 -0.76045775 -1.1746402 -1.772764 -2.4127007 -3.0103407 -3.5143943][2.223978 1.696672 0.98836946 0.32551575 -0.13400316 -0.30972242 -0.27603674 -0.15560532 -0.12080145 -0.33647633 -0.84610391 -1.5382359 -2.2616668 -2.9159679 -3.4531951][2.3143659 1.901731 1.3331776 0.79780626 0.438138 0.31905794 0.35902262 0.43556452 0.38185978 0.063563347 -0.54393053 -1.3247385 -2.1316957 -2.8413744 -3.4098303][2.3803539 2.1048527 1.6835308 1.278718 1.0035143 0.910131 0.92671442 0.93161106 0.78954458 0.3901062 -0.28343296 -1.1297204 -1.9977038 -2.7432156 -3.342746][2.3717113 2.2340522 1.9348874 1.6308098 1.4098296 1.305181 1.2685766 1.2030177 0.99403095 0.55477953 -0.14404249 -1.004777 -1.888546 -2.653712 -3.2709489][2.2106705 2.1763725 1.9516997 1.7094655 1.4996371 1.3613057 1.2736363 1.1558228 0.92190981 0.48984098 -0.17958355 -1.0040705 -1.8577523 -2.6024218 -3.2115464][1.7720222 1.7952523 1.5991273 1.3718481 1.1519828 0.976645 0.8566947 0.72120523 0.50726986 0.13775921 -0.45462179 -1.2018588 -1.9879959 -2.6781173 -3.2484307][0.94644356 0.99102259 0.80794621 0.57889652 0.36064291 0.17109776 0.039987564 -0.079181194 -0.23853827 -0.52091956 -1.0019281 -1.6217573 -2.2858191 -2.8778591 -3.3731217][-0.21632528 -0.17079163 -0.32975626 -0.53716731 -0.7367065 -0.91259933 -1.0375125 -1.1264935 -1.2302587 -1.4259369 -1.7782416 -2.2347839 -2.7260771 -3.1729007 -3.5530791][-1.4977381 -1.4579442 -1.5800805 -1.7384295 -1.8880947 -2.0335343 -2.1285868 -2.1911016 -2.2558992 -2.3816316 -2.6070261 -2.8921509 -3.2082448 -3.4904532 -3.7364972][-2.6653128 -2.6201859 -2.6955848 -2.792896 -2.8868876 -2.9910493 -3.0582352 -3.1039879 -3.1293678 -3.198319 -3.3210955 -3.4596248 -3.6140807 -3.748306 -3.8795764][-3.5250306 -3.4754467 -3.5064211 -3.5428634 -3.5817966 -3.6399612 -3.6750529 -3.7074871 -3.7167077 -3.7527804 -3.7958131 -3.8263526 -3.8698025 -3.8984609 -3.938482][-3.9898095 -3.9369717 -3.9288223 -3.9155431 -3.9231222 -3.9446959 -3.9506898 -3.9760046 -3.9785428 -4.0074158 -4.0200753 -4.0050545 -3.9943719 -3.9654474 -3.9651742][-4.1610818 -4.1146016 -4.0827708 -4.0456963 -4.0279956 -4.0280905 -4.024292 -4.0427675 -4.0529561 -4.0787506 -4.085578 -4.0750794 -4.059164 -4.0202374 -3.9982831]]...]
INFO - root - 2017-12-05 08:20:07.337023: step 2410, loss = 1.58, batch loss = 1.52 (38.2 examples/sec; 0.209 sec/batch; 19h:11m:38s remains)
INFO - root - 2017-12-05 08:20:09.505742: step 2420, loss = 1.64, batch loss = 1.58 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:13s remains)
INFO - root - 2017-12-05 08:20:11.679329: step 2430, loss = 1.58, batch loss = 1.52 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:08s remains)
INFO - root - 2017-12-05 08:20:13.833536: step 2440, loss = 1.55, batch loss = 1.49 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:59s remains)
INFO - root - 2017-12-05 08:20:15.998519: step 2450, loss = 1.82, batch loss = 1.76 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:35s remains)
INFO - root - 2017-12-05 08:20:18.162705: step 2460, loss = 1.23, batch loss = 1.17 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:18s remains)
INFO - root - 2017-12-05 08:20:20.333490: step 2470, loss = 1.40, batch loss = 1.35 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:01s remains)
INFO - root - 2017-12-05 08:20:22.515131: step 2480, loss = 1.97, batch loss = 1.92 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:26s remains)
INFO - root - 2017-12-05 08:20:24.687150: step 2490, loss = 1.69, batch loss = 1.63 (36.1 examples/sec; 0.221 sec/batch; 20h:17m:32s remains)
INFO - root - 2017-12-05 08:20:26.859375: step 2500, loss = 1.61, batch loss = 1.55 (37.5 examples/sec; 0.213 sec/batch; 19h:31m:50s remains)
2017-12-05 08:20:27.120232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3667512 -4.359159 -4.3512011 -4.3436513 -4.3375077 -4.3330145 -4.3304477 -4.3307719 -4.3351789 -4.3407068 -4.3475657 -4.3552136 -4.3621855 -4.3673763 -4.3718967][-4.336947 -4.3100653 -4.276722 -4.2415142 -4.2085295 -4.1792626 -4.1589522 -4.1540465 -4.1673861 -4.1919441 -4.224391 -4.2596374 -4.291245 -4.3149786 -4.3342862][-4.2617316 -4.1832647 -4.0824437 -3.9745884 -3.870872 -3.7761812 -3.7066035 -3.681004 -3.7068863 -3.7731838 -3.8697605 -3.9780324 -4.0785413 -4.1584249 -4.2216873][-4.1083708 -3.9263937 -3.694591 -3.4501472 -3.2203512 -3.0142989 -2.860661 -2.7965484 -2.8400726 -2.9757633 -3.1808872 -3.4179137 -3.6466749 -3.8392866 -3.9949138][-3.878603 -3.5444019 -3.1235604 -2.6850281 -2.2819886 -1.9323916 -1.6753271 -1.5681548 -1.6401279 -1.8691542 -2.2141855 -2.616837 -3.0134759 -3.3621821 -3.6557751][-3.6070724 -3.0961621 -2.4584959 -1.7987347 -1.208076 -0.71825767 -0.37125206 -0.23755932 -0.3528223 -0.68541741 -1.1734312 -1.7416849 -2.3106942 -2.8264856 -3.2717137][-3.344923 -2.6657639 -1.8280625 -0.97042513 -0.22578573 0.35634613 0.74030972 0.86055088 0.68699265 0.26413774 -0.32949734 -1.0157478 -1.713392 -2.3635986 -2.9362974][-3.1279373 -2.3148394 -1.3279123 -0.33608246 0.48695278 1.0824876 1.4290481 1.486865 1.2490501 0.77110577 0.13641024 -0.58875585 -1.3372412 -2.0534511 -2.6967542][-2.9612541 -2.057749 -0.98833227 0.052839279 0.86699438 1.3967495 1.6417794 1.6059985 1.3149614 0.82728052 0.21755981 -0.47008419 -1.1918051 -1.9036109 -2.55891][-2.8616574 -1.9264863 -0.85497546 0.14589262 0.87255573 1.2808714 1.4009967 1.2768431 0.9600091 0.51370382 -0.010254383 -0.60343719 -1.2432401 -1.8971403 -2.5200844][-2.8883309 -2.0022767 -1.0214934 -0.14616394 0.43600893 0.70138788 0.7074213 0.52696323 0.22641993 -0.13899565 -0.54231906 -1.0041647 -1.5251737 -2.0804121 -2.6270332][-3.0900698 -2.3452082 -1.5439794 -0.85848236 -0.44493484 -0.31041431 -0.38201571 -0.57673907 -0.82847285 -1.0919886 -1.3634608 -1.6847997 -2.0679779 -2.4918842 -2.9214325][-3.4412012 -2.9042718 -2.3370292 -1.866116 -1.6067777 -1.5626311 -1.6613598 -1.8278191 -2.0088377 -2.1717772 -2.3256893 -2.5187178 -2.7664671 -3.0517173 -3.3459938][-3.820715 -3.494535 -3.1513224 -2.868854 -2.7236915 -2.720233 -2.8053498 -2.9252772 -3.0390744 -3.1247125 -3.1957681 -3.2929335 -3.428838 -3.5940032 -3.7674923][-4.10924 -3.9419713 -3.7641454 -3.6136267 -3.5349817 -3.5355263 -3.5870328 -3.6572618 -3.7195385 -3.7618456 -3.7910247 -3.8306286 -3.8923664 -3.9749792 -4.0675654]]...]
INFO - root - 2017-12-05 08:20:29.263230: step 2510, loss = 1.77, batch loss = 1.71 (37.8 examples/sec; 0.212 sec/batch; 19h:25m:23s remains)
INFO - root - 2017-12-05 08:20:31.424959: step 2520, loss = 1.84, batch loss = 1.78 (37.8 examples/sec; 0.211 sec/batch; 19h:23m:05s remains)
INFO - root - 2017-12-05 08:20:33.595864: step 2530, loss = 1.92, batch loss = 1.86 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:59s remains)
INFO - root - 2017-12-05 08:20:35.778830: step 2540, loss = 1.74, batch loss = 1.69 (36.6 examples/sec; 0.218 sec/batch; 20h:00m:45s remains)
INFO - root - 2017-12-05 08:20:37.933697: step 2550, loss = 1.42, batch loss = 1.36 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:39s remains)
INFO - root - 2017-12-05 08:20:40.120916: step 2560, loss = 2.11, batch loss = 2.05 (37.8 examples/sec; 0.212 sec/batch; 19h:25m:18s remains)
INFO - root - 2017-12-05 08:20:42.273840: step 2570, loss = 1.68, batch loss = 1.62 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:26s remains)
INFO - root - 2017-12-05 08:20:44.436615: step 2580, loss = 1.58, batch loss = 1.52 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:06s remains)
INFO - root - 2017-12-05 08:20:46.635025: step 2590, loss = 1.65, batch loss = 1.59 (36.2 examples/sec; 0.221 sec/batch; 20h:15m:00s remains)
INFO - root - 2017-12-05 08:20:48.820097: step 2600, loss = 1.61, batch loss = 1.56 (35.8 examples/sec; 0.224 sec/batch; 20h:30m:07s remains)
2017-12-05 08:20:49.100581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3816457 -4.381361 -4.3806472 -4.3795743 -4.3784504 -4.3773155 -4.3760424 -4.3749776 -4.3741665 -4.3737121 -4.3734536 -4.3729086 -4.3705845 -4.3646336 -4.35454][-4.3816872 -4.3813581 -4.3805108 -4.3791089 -4.3771672 -4.3746548 -4.3715281 -4.3682361 -4.3653455 -4.3643603 -4.3656816 -4.3678927 -4.3685365 -4.3650432 -4.3571162][-4.3813519 -4.3807368 -4.3792238 -4.3766112 -4.3726563 -4.3667922 -4.3595672 -4.351757 -4.3456016 -4.3448195 -4.349164 -4.3561983 -4.3621578 -4.3636732 -4.3601408][-4.3802476 -4.3787351 -4.3758216 -4.3711009 -4.3636508 -4.3524351 -4.3384991 -4.3241768 -4.31393 -4.3137608 -4.3222909 -4.3359604 -4.3496084 -4.3584151 -4.360774][-4.3779211 -4.3747344 -4.3694153 -4.3610287 -4.3484774 -4.3306618 -4.309001 -4.2885542 -4.2756014 -4.2768712 -4.28982 -4.3100147 -4.3315449 -4.348464 -4.356864][-4.3735991 -4.3676634 -4.358602 -4.345572 -4.3271995 -4.3032761 -4.2761583 -4.2531056 -4.2405434 -4.2442589 -4.2604055 -4.2846775 -4.3119011 -4.335022 -4.3481784][-4.3678513 -4.3583694 -4.3445482 -4.3266788 -4.3039227 -4.277741 -4.2509351 -4.2313552 -4.2226939 -4.2279663 -4.2445617 -4.2689953 -4.2973485 -4.3221445 -4.336607][-4.3620758 -4.3489294 -4.3311005 -4.3104954 -4.2872958 -4.2642751 -4.2438664 -4.2324843 -4.2293305 -4.2346983 -4.2490191 -4.2694821 -4.2932043 -4.3140631 -4.3259244][-4.3582768 -4.3429 -4.323782 -4.3039827 -4.284121 -4.2674356 -4.2557459 -4.2528882 -4.254869 -4.2599559 -4.2708373 -4.2849655 -4.3004937 -4.3139806 -4.3206882][-4.358695 -4.3441181 -4.3271661 -4.3110719 -4.296814 -4.2869368 -4.2823849 -4.2851486 -4.2904406 -4.2949491 -4.3018293 -4.3091722 -4.3164039 -4.3223391 -4.3240175][-4.3628945 -4.3516674 -4.3395438 -4.3289242 -4.3208694 -4.3166437 -4.3165941 -4.3211689 -4.3267984 -4.3299556 -4.3328795 -4.3347178 -4.336338 -4.3374162 -4.336206][-4.3689265 -4.3618207 -4.3549819 -4.3498197 -4.3467512 -4.3461976 -4.3479047 -4.35154 -4.35518 -4.3565087 -4.3567538 -4.3558536 -4.3551979 -4.3543859 -4.352417][-4.3748755 -4.3711982 -4.3680544 -4.3663197 -4.3659396 -4.3667231 -4.3683825 -4.370347 -4.3718081 -4.3718934 -4.3714285 -4.3703437 -4.3695745 -4.3686566 -4.3671813][-4.3787355 -4.3771873 -4.3760891 -4.3757339 -4.3760018 -4.3767796 -4.3777752 -4.3785172 -4.3789058 -4.3787351 -4.3784757 -4.3781085 -4.3777728 -4.3772774 -4.3764696][-4.3806629 -4.3800778 -4.3797855 -4.3798203 -4.380074 -4.38047 -4.3808603 -4.3810654 -4.3811865 -4.381114 -4.3810868 -4.3810163 -4.3809452 -4.3807936 -4.3804936]]...]
INFO - root - 2017-12-05 08:20:51.260489: step 2610, loss = 1.61, batch loss = 1.55 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:26s remains)
INFO - root - 2017-12-05 08:20:53.425833: step 2620, loss = 1.54, batch loss = 1.48 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:22s remains)
INFO - root - 2017-12-05 08:20:55.604054: step 2630, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:22s remains)
INFO - root - 2017-12-05 08:20:57.766267: step 2640, loss = 1.72, batch loss = 1.67 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:27s remains)
INFO - root - 2017-12-05 08:20:59.959059: step 2650, loss = 1.81, batch loss = 1.75 (36.0 examples/sec; 0.222 sec/batch; 20h:22m:12s remains)
INFO - root - 2017-12-05 08:21:02.140078: step 2660, loss = 1.80, batch loss = 1.74 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:22s remains)
INFO - root - 2017-12-05 08:21:04.288577: step 2670, loss = 1.79, batch loss = 1.74 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:20s remains)
INFO - root - 2017-12-05 08:21:06.455890: step 2680, loss = 1.46, batch loss = 1.40 (37.3 examples/sec; 0.214 sec/batch; 19h:38m:28s remains)
INFO - root - 2017-12-05 08:21:08.612573: step 2690, loss = 1.86, batch loss = 1.80 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:05s remains)
INFO - root - 2017-12-05 08:21:10.794732: step 2700, loss = 1.68, batch loss = 1.62 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:58s remains)
2017-12-05 08:21:11.092938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3793941 -4.3793955 -4.3793917 -4.3793917 -4.3793945 -4.3793936 -4.3793917 -4.3793879 -4.3793855 -4.3793855 -4.3793859 -4.3793869 -4.3793869 -4.3793859 -4.3793845][-4.3794146 -4.3794107 -4.3794127 -4.3793964 -4.3793836 -4.3793845 -4.3793831 -4.3793812 -4.3793812 -4.3793812 -4.3793812 -4.3793826 -4.3793836 -4.3793831 -4.3793817][-4.3793988 -4.3794031 -4.3794193 -4.3794093 -4.3793836 -4.3793745 -4.3793783 -4.379384 -4.379384 -4.3793831 -4.3793845 -4.3793855 -4.3793864 -4.3793864 -4.379385][-4.3793936 -4.3794112 -4.3794303 -4.3794174 -4.3793888 -4.37938 -4.3793821 -4.3793836 -4.379384 -4.3793831 -4.3793821 -4.3793831 -4.379384 -4.3793826 -4.3793817][-4.3793783 -4.3794065 -4.3794155 -4.3794036 -4.3793764 -4.3793812 -4.3793869 -4.3793826 -4.3793774 -4.3793778 -4.3793812 -4.3793831 -4.3793793 -4.3793745 -4.379374][-4.3793631 -4.3793821 -4.379385 -4.3793755 -4.3793612 -4.3793678 -4.3793821 -4.3793783 -4.3793716 -4.3793721 -4.3793783 -4.3793764 -4.379365 -4.3793545 -4.379353][-4.3793268 -4.379353 -4.3793783 -4.379354 -4.3793497 -4.3793659 -4.3793797 -4.3793745 -4.3793693 -4.3793588 -4.3793521 -4.3793492 -4.3793435 -4.3793368 -4.3793468][-4.3792906 -4.3793464 -4.3793783 -4.3793554 -4.3793392 -4.3793569 -4.3793778 -4.3793654 -4.3793468 -4.3793087 -4.3793087 -4.3793297 -4.3793473 -4.3793454 -4.3793645][-4.3793015 -4.3793325 -4.3793468 -4.3793283 -4.3793178 -4.3793354 -4.3793449 -4.37933 -4.3793082 -4.3792777 -4.3792934 -4.3793306 -4.3793478 -4.3793459 -4.3793678][-4.3793159 -4.3793473 -4.3793616 -4.3793607 -4.3793631 -4.3793716 -4.3793659 -4.3793588 -4.379354 -4.3793516 -4.3793559 -4.3793516 -4.3793411 -4.3793411 -4.379355][-4.3793907 -4.3794103 -4.3794141 -4.379406 -4.3793964 -4.3793907 -4.3793836 -4.3793759 -4.3793678 -4.3793659 -4.3793707 -4.3793664 -4.3793621 -4.3793635 -4.3793578][-4.3794317 -4.3794332 -4.3794231 -4.3794103 -4.3793926 -4.3793907 -4.3793902 -4.3793864 -4.3793778 -4.3793726 -4.379374 -4.37938 -4.3793869 -4.3793855 -4.3793859][-4.3794217 -4.3794155 -4.3794055 -4.379395 -4.3793826 -4.3793836 -4.3793855 -4.3793859 -4.3793821 -4.3793745 -4.3793716 -4.3793778 -4.379384 -4.3793931 -4.3794003][-4.3793969 -4.3793936 -4.3793917 -4.37939 -4.3793855 -4.3793831 -4.3793826 -4.3793821 -4.3793817 -4.3793793 -4.3793774 -4.3793807 -4.3793855 -4.3793941 -4.3793983][-4.3793902 -4.3793874 -4.3793869 -4.3793864 -4.3793864 -4.379385 -4.379385 -4.3793845 -4.3793859 -4.3793855 -4.379384 -4.3793859 -4.3793869 -4.3793907 -4.3793945]]...]
INFO - root - 2017-12-05 08:21:13.249816: step 2710, loss = 1.79, batch loss = 1.74 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:57s remains)
INFO - root - 2017-12-05 08:21:15.406548: step 2720, loss = 1.81, batch loss = 1.75 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:33s remains)
INFO - root - 2017-12-05 08:21:17.581398: step 2730, loss = 1.68, batch loss = 1.62 (37.1 examples/sec; 0.215 sec/batch; 19h:43m:34s remains)
INFO - root - 2017-12-05 08:21:19.746878: step 2740, loss = 1.52, batch loss = 1.46 (37.5 examples/sec; 0.214 sec/batch; 19h:33m:28s remains)
INFO - root - 2017-12-05 08:21:21.940522: step 2750, loss = 1.64, batch loss = 1.58 (36.6 examples/sec; 0.218 sec/batch; 19h:59m:56s remains)
INFO - root - 2017-12-05 08:21:24.114728: step 2760, loss = 1.78, batch loss = 1.72 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:19s remains)
INFO - root - 2017-12-05 08:21:26.294613: step 2770, loss = 1.66, batch loss = 1.61 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:29s remains)
INFO - root - 2017-12-05 08:21:28.477411: step 2780, loss = 1.47, batch loss = 1.41 (35.3 examples/sec; 0.226 sec/batch; 20h:44m:02s remains)
INFO - root - 2017-12-05 08:21:30.671940: step 2790, loss = 1.56, batch loss = 1.50 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:27s remains)
INFO - root - 2017-12-05 08:21:32.884870: step 2800, loss = 1.46, batch loss = 1.40 (36.4 examples/sec; 0.219 sec/batch; 20h:06m:04s remains)
2017-12-05 08:21:33.174688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9163315 -3.9591641 -3.9768426 -3.9775958 -3.9587741 -3.9337695 -3.9094636 -3.9012544 -3.9291065 -3.9872124 -4.066256 -4.1466064 -4.2219181 -4.278101 -4.3080096][-3.6820493 -3.7413416 -3.7860808 -3.8090272 -3.814014 -3.8063941 -3.8091371 -3.8281155 -3.887167 -3.9803755 -4.0888319 -4.1796107 -4.2524033 -4.29744 -4.3176303][-3.443 -3.5073786 -3.58982 -3.6414917 -3.6781595 -3.6914439 -3.708806 -3.7666876 -3.8664227 -3.9930565 -4.1227326 -4.2162523 -4.2807169 -4.3145437 -4.328351][-3.2428207 -3.3076055 -3.4082689 -3.4902744 -3.5598519 -3.6023114 -3.645041 -3.7200639 -3.8536766 -4.011652 -4.1550441 -4.2476211 -4.2990918 -4.3197985 -4.3236713][-3.1055431 -3.1709933 -3.274955 -3.3604565 -3.4359207 -3.5007966 -3.57104 -3.6643379 -3.8291438 -4.0081573 -4.1614542 -4.2623272 -4.3136063 -4.3315387 -4.3334551][-3.0227268 -3.0805879 -3.1736894 -3.2477615 -3.3176532 -3.3897336 -3.4744403 -3.5938177 -3.7735524 -3.9715872 -4.1433511 -4.2541809 -4.3123322 -4.3339114 -4.3363857][-2.918015 -2.9590225 -3.0483239 -3.1163492 -3.1844625 -3.2729349 -3.3879538 -3.5289509 -3.7011635 -3.9012003 -4.0898104 -4.2139525 -4.2994637 -4.3336325 -4.3417444][-2.8042102 -2.8371768 -2.9146085 -2.9705963 -3.0413198 -3.1475716 -3.2903144 -3.4517276 -3.6259263 -3.8126652 -4.0049272 -4.1441369 -4.2586045 -4.312211 -4.3382249][-2.6952519 -2.7063096 -2.7824464 -2.8285093 -2.9026065 -3.0244603 -3.187109 -3.3662195 -3.5448463 -3.72772 -3.9214392 -4.0726662 -4.2074904 -4.284554 -4.3299813][-2.6285157 -2.5947099 -2.6578865 -2.7093153 -2.7908478 -2.925467 -3.1052489 -3.3041773 -3.49898 -3.6804159 -3.8674223 -4.023654 -4.1610832 -4.258522 -4.3162737][-2.613579 -2.554306 -2.5927005 -2.6331756 -2.7216635 -2.8729081 -3.0656071 -3.2815189 -3.4924951 -3.684495 -3.8707287 -4.0221639 -4.1512237 -4.2370405 -4.2917929][-2.6458478 -2.5819268 -2.6047449 -2.6489496 -2.7325869 -2.8825445 -3.0720854 -3.2931981 -3.5151079 -3.7244737 -3.906168 -4.0486155 -4.1556463 -4.225956 -4.2703023][-2.7785692 -2.7102149 -2.7169793 -2.7572613 -2.8323827 -2.9690669 -3.1432185 -3.3516788 -3.5731578 -3.7900212 -3.9709153 -4.0966115 -4.1692009 -4.2135572 -4.2440681][-2.991333 -2.9337358 -2.9263594 -2.9583731 -3.0201669 -3.1344566 -3.2845168 -3.463583 -3.6661835 -3.8679087 -4.0363207 -4.1493697 -4.2043066 -4.2311392 -4.2526107][-3.2421083 -3.2011104 -3.1932144 -3.2111583 -3.257895 -3.35363 -3.4869113 -3.6314754 -3.7933893 -3.9497094 -4.0911775 -4.1935754 -4.2494984 -4.2757335 -4.2877235]]...]
INFO - root - 2017-12-05 08:21:35.357336: step 2810, loss = 1.83, batch loss = 1.77 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:53s remains)
INFO - root - 2017-12-05 08:21:37.562157: step 2820, loss = 2.14, batch loss = 2.09 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:19s remains)
INFO - root - 2017-12-05 08:21:39.736676: step 2830, loss = 1.45, batch loss = 1.39 (38.2 examples/sec; 0.210 sec/batch; 19h:11m:24s remains)
INFO - root - 2017-12-05 08:21:41.900311: step 2840, loss = 1.69, batch loss = 1.63 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:16s remains)
INFO - root - 2017-12-05 08:21:44.057287: step 2850, loss = 1.75, batch loss = 1.69 (37.5 examples/sec; 0.214 sec/batch; 19h:33m:36s remains)
INFO - root - 2017-12-05 08:21:46.208006: step 2860, loss = 1.68, batch loss = 1.62 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:59s remains)
INFO - root - 2017-12-05 08:21:48.368400: step 2870, loss = 1.46, batch loss = 1.40 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:41s remains)
INFO - root - 2017-12-05 08:21:50.569868: step 2880, loss = 1.71, batch loss = 1.65 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:18s remains)
INFO - root - 2017-12-05 08:21:52.737871: step 2890, loss = 1.82, batch loss = 1.76 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:46s remains)
INFO - root - 2017-12-05 08:21:55.054104: step 2900, loss = 2.04, batch loss = 1.98 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:03s remains)
2017-12-05 08:21:55.332366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432][-4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432 -4.3745432]]...]
INFO - root - 2017-12-05 08:21:57.502046: step 2910, loss = 1.72, batch loss = 1.66 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:28s remains)
INFO - root - 2017-12-05 08:21:59.686930: step 2920, loss = 1.39, batch loss = 1.33 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:53s remains)
INFO - root - 2017-12-05 08:22:01.883135: step 2930, loss = 1.96, batch loss = 1.90 (35.8 examples/sec; 0.224 sec/batch; 20h:28m:51s remains)
INFO - root - 2017-12-05 08:22:04.064377: step 2940, loss = 1.69, batch loss = 1.63 (36.8 examples/sec; 0.218 sec/batch; 19h:55m:22s remains)
INFO - root - 2017-12-05 08:22:06.232248: step 2950, loss = 1.53, batch loss = 1.47 (36.6 examples/sec; 0.219 sec/batch; 20h:00m:09s remains)
INFO - root - 2017-12-05 08:22:08.447704: step 2960, loss = 1.78, batch loss = 1.72 (35.3 examples/sec; 0.227 sec/batch; 20h:45m:45s remains)
INFO - root - 2017-12-05 08:22:10.641721: step 2970, loss = 1.88, batch loss = 1.82 (35.9 examples/sec; 0.223 sec/batch; 20h:22m:48s remains)
INFO - root - 2017-12-05 08:22:12.819598: step 2980, loss = 2.00, batch loss = 1.94 (36.6 examples/sec; 0.219 sec/batch; 20h:01m:27s remains)
INFO - root - 2017-12-05 08:22:15.007374: step 2990, loss = 2.00, batch loss = 1.94 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:32s remains)
INFO - root - 2017-12-05 08:22:17.173618: step 3000, loss = 1.64, batch loss = 1.58 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:40s remains)
2017-12-05 08:22:17.435183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722048 -4.3722043 -4.3722053 -4.3722062 -4.3722014 -4.3721557 -4.3720889 -4.3720722 -4.3721032 -4.3721466 -4.372191][-4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722034 -4.3722034 -4.3722038 -4.3722029 -4.3721852 -4.3721685 -4.372179 -4.3721933 -4.3722053][-4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722034 -4.3722024 -4.3722005 -4.3722 -4.3722019 -4.3721976 -4.3721957 -4.3721986 -4.3722076][-4.3722043 -4.3722043 -4.3722043 -4.3722043 -4.3722038 -4.3722029 -4.3722043 -4.3722034 -4.3722005 -4.3721991 -4.3722024 -4.3722034 -4.3722043 -4.3722034 -4.3722076][-4.3722043 -4.3722043 -4.3722048 -4.3722048 -4.3722034 -4.3722024 -4.3722014 -4.3722019 -4.3722034 -4.3722043 -4.3722053 -4.3722034 -4.3722043 -4.3722038 -4.3722053][-4.3722043 -4.3722043 -4.3722053 -4.3722053 -4.3722019 -4.3721933 -4.3721819 -4.3721786 -4.3722043 -4.3722138 -4.3722138 -4.3722057 -4.3722048 -4.3722019 -4.3722029][-4.3722043 -4.3722043 -4.3722048 -4.3722053 -4.3722014 -4.3721833 -4.3721361 -4.3721113 -4.3721542 -4.3721957 -4.3722119 -4.3722091 -4.3722072 -4.3722029 -4.3722029][-4.3722043 -4.3722043 -4.3722048 -4.3722062 -4.3722038 -4.3721905 -4.3721356 -4.3720918 -4.3721366 -4.3721862 -4.3722105 -4.3722105 -4.3722076 -4.3722038 -4.3722048][-4.3722043 -4.3722043 -4.3722043 -4.3722048 -4.3722048 -4.3722034 -4.3721886 -4.37218 -4.3721919 -4.3722 -4.3722062 -4.3722081 -4.3722062 -4.3722029 -4.3722053][-4.3722043 -4.3722038 -4.3722038 -4.3722029 -4.3722024 -4.3722024 -4.3722014 -4.3722043 -4.3722038 -4.3722005 -4.3722014 -4.3722053 -4.3722062 -4.3722043 -4.3722048][-4.3722043 -4.3722038 -4.3722034 -4.3722029 -4.3722034 -4.3722029 -4.3722038 -4.3722076 -4.3722076 -4.3722057 -4.3722043 -4.3722048 -4.3722053 -4.3722043 -4.3722043][-4.3722048 -4.3722043 -4.3722038 -4.3722034 -4.3722029 -4.3722014 -4.372201 -4.3722034 -4.3722038 -4.3722038 -4.3722038 -4.3722034 -4.3722038 -4.3722034 -4.3722038][-4.3722043 -4.3722038 -4.3722038 -4.3722038 -4.3722043 -4.3722038 -4.3722038 -4.3722067 -4.3722057 -4.3722053 -4.3722048 -4.3722048 -4.3722043 -4.3722038 -4.3722038][-4.3722043 -4.3722038 -4.3722029 -4.3722034 -4.3722038 -4.3722043 -4.3722038 -4.3722053 -4.3722048 -4.3722057 -4.3722067 -4.3722067 -4.3722043 -4.3722043 -4.3722048][-4.3722034 -4.3722024 -4.3722029 -4.3722038 -4.3722034 -4.3722048 -4.3722043 -4.3722038 -4.3722 -4.3722024 -4.3722048 -4.3722057 -4.3722053 -4.3722057 -4.3722057]]...]
INFO - root - 2017-12-05 08:22:19.655123: step 3010, loss = 1.93, batch loss = 1.87 (36.1 examples/sec; 0.221 sec/batch; 20h:16m:10s remains)
INFO - root - 2017-12-05 08:22:21.835749: step 3020, loss = 1.94, batch loss = 1.88 (36.8 examples/sec; 0.217 sec/batch; 19h:52m:35s remains)
INFO - root - 2017-12-05 08:22:23.997529: step 3030, loss = 1.73, batch loss = 1.68 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:45s remains)
INFO - root - 2017-12-05 08:22:26.178247: step 3040, loss = 1.81, batch loss = 1.75 (36.8 examples/sec; 0.218 sec/batch; 19h:54m:58s remains)
INFO - root - 2017-12-05 08:22:28.351394: step 3050, loss = 1.69, batch loss = 1.63 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:39s remains)
INFO - root - 2017-12-05 08:22:30.541982: step 3060, loss = 1.57, batch loss = 1.51 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:44s remains)
INFO - root - 2017-12-05 08:22:32.725187: step 3070, loss = 1.69, batch loss = 1.63 (35.7 examples/sec; 0.224 sec/batch; 20h:29m:12s remains)
INFO - root - 2017-12-05 08:22:34.909062: step 3080, loss = 1.86, batch loss = 1.80 (36.0 examples/sec; 0.222 sec/batch; 20h:21m:15s remains)
INFO - root - 2017-12-05 08:22:37.107936: step 3090, loss = 2.01, batch loss = 1.95 (36.6 examples/sec; 0.218 sec/batch; 19h:58m:36s remains)
INFO - root - 2017-12-05 08:22:39.306986: step 3100, loss = 2.27, batch loss = 2.21 (36.6 examples/sec; 0.218 sec/batch; 19h:58m:50s remains)
2017-12-05 08:22:39.594353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3671083 -4.3670783 -4.36701 -4.366961 -4.3668985 -4.3667946 -4.3667412 -4.3667049 -4.3665266 -4.3662195 -4.3659048 -4.3656797 -4.3656416 -4.3658824 -4.36629][-4.3668456 -4.3667374 -4.3665156 -4.366271 -4.3659511 -4.36557 -4.3653908 -4.3653016 -4.3648653 -4.3640776 -4.3631916 -4.3623042 -4.3618131 -4.3619928 -4.362906][-4.36613 -4.3658285 -4.3652649 -4.3645263 -4.3634782 -4.3623929 -4.3618217 -4.3615222 -4.360456 -4.3586526 -4.3565645 -4.3545561 -4.353291 -4.3533626 -4.355196][-4.3651495 -4.3643608 -4.3629074 -4.36084 -4.3580813 -4.3555174 -4.3538976 -4.3527069 -4.3509078 -4.3483877 -4.3453388 -4.3423576 -4.3407226 -4.3411098 -4.3441844][-4.3641791 -4.3625808 -4.3596287 -4.3552694 -4.3497868 -4.344768 -4.341083 -4.3381405 -4.3358173 -4.3338966 -4.331636 -4.3292994 -4.3287687 -4.3305917 -4.3350658][-4.3636494 -4.3610578 -4.3562522 -4.3490324 -4.3401842 -4.3321996 -4.3255796 -4.3200593 -4.3178163 -4.318429 -4.3194842 -4.3204923 -4.3232374 -4.3277307 -4.3335896][-4.3638163 -4.3605294 -4.3541284 -4.3443208 -4.3327589 -4.3219295 -4.3118467 -4.3036313 -4.3021345 -4.3065825 -4.3124809 -4.3186817 -4.3257179 -4.333014 -4.33986][-4.364512 -4.3612485 -4.3543 -4.3434625 -4.3307185 -4.3182521 -4.30592 -4.2959967 -4.2950377 -4.302299 -4.3123803 -4.3231864 -4.333426 -4.3421292 -4.3489323][-4.3654189 -4.3628545 -4.3568945 -4.3471994 -4.3355432 -4.3234677 -4.3115139 -4.3020959 -4.3007836 -4.3083954 -4.3198633 -4.3321576 -4.3429093 -4.3510652 -4.3567348][-4.3661933 -4.3645358 -4.3605285 -4.35348 -4.344542 -4.3348775 -4.3255267 -4.3182592 -4.3169179 -4.322854 -4.3325281 -4.342937 -4.3517966 -4.3579879 -4.361938][-4.3666949 -4.3658171 -4.3636556 -4.3595257 -4.3539886 -4.3478074 -4.3419313 -4.3375778 -4.3366914 -4.3401456 -4.3461928 -4.3528733 -4.3586226 -4.3626127 -4.3649797][-4.36697 -4.3666248 -4.3656945 -4.3636932 -4.36091 -4.3578677 -4.355083 -4.3532472 -4.3528652 -4.3543372 -4.3570251 -4.3602591 -4.3631549 -4.3651767 -4.3661909][-4.3670979 -4.3670058 -4.366684 -4.36588 -4.364738 -4.3635969 -4.3627028 -4.362205 -4.3621821 -4.3626771 -4.3635464 -4.3646588 -4.3657055 -4.3664351 -4.366467][-4.3671284 -4.3671169 -4.3670268 -4.36676 -4.3663578 -4.3659897 -4.3658257 -4.3658013 -4.3659096 -4.3660908 -4.3662877 -4.3665218 -4.3667531 -4.3668947 -4.366456][-4.367126 -4.367125 -4.3671169 -4.3670268 -4.3668995 -4.3667917 -4.3667789 -4.3668513 -4.366981 -4.367084 -4.3671126 -4.3670821 -4.367074 -4.3670521 -4.3664484]]...]
INFO - root - 2017-12-05 08:22:41.787987: step 3110, loss = 1.84, batch loss = 1.78 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:03s remains)
INFO - root - 2017-12-05 08:22:43.975888: step 3120, loss = 1.86, batch loss = 1.80 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:01s remains)
INFO - root - 2017-12-05 08:22:46.154856: step 3130, loss = 1.59, batch loss = 1.53 (37.3 examples/sec; 0.214 sec/batch; 19h:35m:56s remains)
INFO - root - 2017-12-05 08:22:48.353567: step 3140, loss = 1.78, batch loss = 1.73 (36.3 examples/sec; 0.220 sec/batch; 20h:08m:13s remains)
INFO - root - 2017-12-05 08:22:50.520517: step 3150, loss = 1.99, batch loss = 1.93 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:12s remains)
INFO - root - 2017-12-05 08:22:52.690982: step 3160, loss = 2.13, batch loss = 2.07 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:40s remains)
INFO - root - 2017-12-05 08:22:54.861043: step 3170, loss = 1.62, batch loss = 1.56 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:49s remains)
INFO - root - 2017-12-05 08:22:57.053882: step 3180, loss = 1.67, batch loss = 1.62 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:43s remains)
INFO - root - 2017-12-05 08:22:59.198470: step 3190, loss = 2.25, batch loss = 2.20 (38.1 examples/sec; 0.210 sec/batch; 19h:12m:39s remains)
INFO - root - 2017-12-05 08:23:01.394281: step 3200, loss = 1.65, batch loss = 1.59 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:58s remains)
2017-12-05 08:23:01.672907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3647666 -4.3647661 -4.3647652 -4.3647661 -4.3647671 -4.3647628 -4.3647571 -4.3647585 -4.3647661 -4.3647604 -4.3647361 -4.3647232 -4.3647323 -4.3647461 -4.3647485][-4.3647666 -4.3647671 -4.3647656 -4.3647671 -4.3647676 -4.3647671 -4.36476 -4.3647594 -4.3647614 -4.3647585 -4.3647494 -4.3647432 -4.3647532 -4.3647532 -4.3647542][-4.3647661 -4.3647661 -4.3647661 -4.3647671 -4.3647656 -4.3647633 -4.3647661 -4.3647633 -4.3647614 -4.3647614 -4.3647571 -4.3647552 -4.3647571 -4.3647561 -4.3647594][-4.3647661 -4.3647666 -4.3647676 -4.36477 -4.3647666 -4.3647661 -4.3647656 -4.36476 -4.3647594 -4.3647633 -4.364759 -4.364759 -4.3647566 -4.3647552 -4.364758][-4.3647652 -4.364769 -4.3647718 -4.3647723 -4.364769 -4.3647656 -4.3647661 -4.3647566 -4.3647518 -4.3647571 -4.3647609 -4.3647628 -4.3647556 -4.3647547 -4.3647552][-4.3647637 -4.3647714 -4.3647747 -4.3647742 -4.3647728 -4.3647676 -4.3647509 -4.3647246 -4.364717 -4.364737 -4.3647523 -4.3647628 -4.36476 -4.3647618 -4.3647594][-4.3647633 -4.3647728 -4.36477 -4.364768 -4.3647723 -4.3647542 -4.3647189 -4.364677 -4.3646665 -4.3647151 -4.3647523 -4.3647652 -4.3647633 -4.3647637 -4.3647604][-4.3647642 -4.3647709 -4.364769 -4.3647633 -4.3647642 -4.3647447 -4.3647203 -4.364696 -4.3646874 -4.3647308 -4.36476 -4.3647676 -4.3647637 -4.36476 -4.364758][-4.3647652 -4.3647671 -4.3647647 -4.3647523 -4.3647537 -4.3647532 -4.3647466 -4.364737 -4.3647342 -4.3647556 -4.3647604 -4.3647542 -4.3647561 -4.3647623 -4.3647637][-4.3647656 -4.3647647 -4.3647585 -4.3647413 -4.3647404 -4.3647571 -4.36476 -4.3647585 -4.364758 -4.3647628 -4.3647518 -4.3647294 -4.3647323 -4.364758 -4.3647614][-4.3647656 -4.3647661 -4.3647656 -4.3647528 -4.364748 -4.364768 -4.3647742 -4.3647752 -4.3647723 -4.3647604 -4.3647256 -4.3646884 -4.3646851 -4.3647156 -4.3647203][-4.3647666 -4.3647661 -4.3647676 -4.3647637 -4.364758 -4.3647652 -4.3647733 -4.3647757 -4.3647685 -4.3647556 -4.3647275 -4.364697 -4.364686 -4.3646889 -4.3646984][-4.3647656 -4.3647661 -4.3647656 -4.3647656 -4.3647594 -4.364758 -4.3647637 -4.3647594 -4.3647513 -4.364747 -4.3647428 -4.3647342 -4.3647223 -4.3647151 -4.3647122][-4.3647656 -4.3647671 -4.364769 -4.364768 -4.3647642 -4.3647552 -4.3647494 -4.3647447 -4.3647447 -4.3647528 -4.3647594 -4.364759 -4.36475 -4.364738 -4.3647385][-4.3647661 -4.3647647 -4.3647685 -4.364769 -4.3647685 -4.36476 -4.3647518 -4.3647537 -4.3647561 -4.3647561 -4.3647623 -4.3647633 -4.3647542 -4.3647456 -4.3647532]]...]
INFO - root - 2017-12-05 08:23:03.822990: step 3210, loss = 1.43, batch loss = 1.37 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:15s remains)
INFO - root - 2017-12-05 08:23:05.975545: step 3220, loss = 1.58, batch loss = 1.53 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:22s remains)
INFO - root - 2017-12-05 08:23:08.149917: step 3230, loss = 2.29, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:52m:14s remains)
INFO - root - 2017-12-05 08:23:10.306205: step 3240, loss = 1.71, batch loss = 1.65 (38.8 examples/sec; 0.206 sec/batch; 18h:52m:01s remains)
INFO - root - 2017-12-05 08:23:12.496515: step 3250, loss = 2.19, batch loss = 2.14 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:25s remains)
INFO - root - 2017-12-05 08:23:14.673135: step 3260, loss = 1.94, batch loss = 1.88 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:59s remains)
INFO - root - 2017-12-05 08:23:16.887418: step 3270, loss = 2.26, batch loss = 2.20 (36.3 examples/sec; 0.221 sec/batch; 20h:10m:19s remains)
INFO - root - 2017-12-05 08:23:19.043507: step 3280, loss = 1.62, batch loss = 1.56 (37.3 examples/sec; 0.214 sec/batch; 19h:35m:20s remains)
INFO - root - 2017-12-05 08:23:21.215675: step 3290, loss = 1.77, batch loss = 1.71 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:02s remains)
INFO - root - 2017-12-05 08:23:23.389491: step 3300, loss = 1.53, batch loss = 1.47 (36.8 examples/sec; 0.217 sec/batch; 19h:51m:26s remains)
2017-12-05 08:23:23.720783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.359508 -4.3560338 -4.3500891 -4.34156 -4.3312211 -4.3235197 -4.322156 -4.3263645 -4.3331556 -4.3409324 -4.3479013 -4.3533125 -4.3566566 -4.3586092 -4.3594756][-4.3516331 -4.3372407 -4.3108506 -4.2706113 -4.222899 -4.1879468 -4.1812997 -4.2011104 -4.2332225 -4.26906 -4.3005867 -4.3234839 -4.33825 -4.3470922 -4.3517356][-4.3297381 -4.2817631 -4.1924458 -4.0624194 -3.9226558 -3.8294559 -3.8171868 -3.8743029 -3.9663725 -4.0688534 -4.1640625 -4.2361135 -4.2857151 -4.3163223 -4.3331861][-4.27915 -4.1534157 -3.9327536 -3.633729 -3.3380728 -3.1545346 -3.1382031 -3.2628975 -3.4634016 -3.6872954 -3.8970127 -4.0598245 -4.1758018 -4.2520428 -4.2968063][-4.1858916 -3.9337223 -3.5188689 -2.9892116 -2.4934053 -2.1944809 -2.1730058 -2.3908937 -2.7454457 -3.1457753 -3.5198543 -3.8111198 -4.0182881 -4.1551452 -4.237124][-4.0598145 -3.6589611 -3.033824 -2.2683113 -1.5767074 -1.166502 -1.1441913 -1.4650378 -1.9891424 -2.5818505 -3.13373 -3.5624967 -3.8622105 -4.0557504 -4.1704855][-3.9442632 -3.4242244 -2.64292 -1.7128375 -0.89198518 -0.41573548 -0.40376091 -0.81551027 -1.4824035 -2.227386 -2.9077611 -3.4265668 -3.7771726 -3.9953504 -4.1229687][-3.8941181 -3.3300834 -2.5007992 -1.5328031 -0.6925211 -0.22526789 -0.24781656 -0.72103596 -1.4603994 -2.2597976 -2.9656072 -3.4843419 -3.8129005 -4.0035486 -4.1106148][-3.9354272 -3.4187005 -2.6638267 -1.7918265 -1.0474505 -0.65903378 -0.72927475 -1.2132668 -1.9265385 -2.6608381 -3.2741306 -3.696753 -3.9391677 -4.0644293 -4.1305933][-4.0432792 -3.6428077 -3.0544534 -2.3769779 -1.8095229 -1.5422173 -1.6476347 -2.0745039 -2.6617436 -3.2315078 -3.677947 -3.9644954 -4.1102247 -4.1700339 -4.1899529][-4.1688304 -3.9116244 -3.5244532 -3.0759606 -2.7058954 -2.5511699 -2.660192 -2.9815812 -3.3948245 -3.766777 -4.02749 -4.1767097 -4.2385926 -4.2527347 -4.2509189][-4.2687945 -4.1362891 -3.9306102 -3.6845059 -3.4795008 -3.3990993 -3.4760208 -3.6740947 -3.9182148 -4.1245165 -4.2470827 -4.3015943 -4.3127623 -4.3036566 -4.2915411][-4.3266158 -4.2718196 -4.1839495 -4.0778918 -3.984235 -3.9406769 -3.974932 -4.0691462 -4.1880217 -4.2869 -4.3346634 -4.3453064 -4.3416476 -4.3313422 -4.31955][-4.3513513 -4.329329 -4.293097 -4.2476888 -4.2074456 -4.1825213 -4.1854234 -4.2220721 -4.2793274 -4.3324986 -4.3564334 -4.3580728 -4.3536587 -4.3464007 -4.3380303][-4.3562422 -4.3453412 -4.3228087 -4.2922549 -4.2634044 -4.2434196 -4.2410269 -4.2563968 -4.292294 -4.3355675 -4.3583946 -4.361001 -4.3595257 -4.35672 -4.3518462]]...]
INFO - root - 2017-12-05 08:23:25.886592: step 3310, loss = 1.59, batch loss = 1.53 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:25s remains)
INFO - root - 2017-12-05 08:23:28.069607: step 3320, loss = 2.12, batch loss = 2.07 (37.1 examples/sec; 0.215 sec/batch; 19h:42m:04s remains)
INFO - root - 2017-12-05 08:23:30.245694: step 3330, loss = 1.70, batch loss = 1.64 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:34s remains)
INFO - root - 2017-12-05 08:23:32.462591: step 3340, loss = 1.53, batch loss = 1.47 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:07s remains)
INFO - root - 2017-12-05 08:23:34.610227: step 3350, loss = 2.01, batch loss = 1.95 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:15s remains)
INFO - root - 2017-12-05 08:23:36.799854: step 3360, loss = 1.83, batch loss = 1.77 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:30s remains)
INFO - root - 2017-12-05 08:23:38.980820: step 3370, loss = 1.90, batch loss = 1.84 (36.6 examples/sec; 0.219 sec/batch; 19h:59m:24s remains)
INFO - root - 2017-12-05 08:23:41.174156: step 3380, loss = 1.98, batch loss = 1.92 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:32s remains)
INFO - root - 2017-12-05 08:23:43.367875: step 3390, loss = 1.60, batch loss = 1.54 (37.0 examples/sec; 0.216 sec/batch; 19h:45m:37s remains)
INFO - root - 2017-12-05 08:23:45.547727: step 3400, loss = 1.81, batch loss = 1.75 (36.8 examples/sec; 0.217 sec/batch; 19h:52m:51s remains)
2017-12-05 08:23:45.846295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.46593 -1.8141603 -2.1377041 -2.4645286 -2.8052721 -3.1576078 -3.4818754 -3.7502007 -3.9315288 -4.0565143 -4.1362486 -4.1739397 -4.1953583 -4.2000895 -4.2047448][-0.7422781 -1.1895828 -1.573885 -1.9206955 -2.2733655 -2.6562943 -3.040019 -3.3842857 -3.6486287 -3.8550551 -4.001646 -4.0833921 -4.1307039 -4.1469636 -4.148447][-0.37827897 -0.86762381 -1.2363684 -1.504693 -1.7636616 -2.0780208 -2.4455521 -2.8324046 -3.1913006 -3.5004666 -3.7467856 -3.9173837 -4.0252738 -4.073349 -4.0781446][-0.36244798 -0.80203676 -1.0505583 -1.1376853 -1.1938894 -1.342227 -1.6282425 -2.019635 -2.4580708 -2.8961773 -3.2987757 -3.6111341 -3.8215065 -3.9483662 -3.988018][-0.58298755 -0.87182355 -0.91636109 -0.74934983 -0.5481391 -0.4698875 -0.61534429 -0.96046543 -1.4416265 -1.9909456 -2.5586309 -3.0479984 -3.4147053 -3.6652651 -3.7779057][-0.86575294 -0.90558314 -0.67572331 -0.2299695 0.2307477 0.53736448 0.54828119 0.27700996 -0.23163891 -0.88171148 -1.6130006 -2.2909606 -2.8368874 -3.2365189 -3.4517958][-1.0791881 -0.83576226 -0.32467127 0.36661768 1.0373445 1.5263619 1.6708126 1.4754686 0.9695611 0.24976063 -0.61899185 -1.4503133 -2.1460836 -2.6830711 -3.0208385][-1.1869938 -0.69986463 0.024244785 0.87792873 1.6730185 2.2582521 2.4789824 2.3319497 1.8314161 1.0949955 0.18798542 -0.69383907 -1.4586942 -2.0687318 -2.5086808][-1.2579329 -0.61129022 0.22778654 1.1350722 1.9497304 2.5307336 2.7531571 2.6248956 2.1606817 1.4913335 0.664042 -0.13411522 -0.86072493 -1.4685397 -1.9629443][-1.4113083 -0.72232151 0.11603785 0.983459 1.736237 2.2468319 2.4404893 2.332047 1.9511528 1.436831 0.8031559 0.2085681 -0.36655641 -0.88478971 -1.3679159][-1.7243385 -1.0811763 -0.32819319 0.438406 1.0964479 1.5219216 1.6874142 1.6184087 1.3665733 1.0547566 0.68605566 0.35486364 -0.0072684288 -0.37158251 -0.79343772][-2.1409025 -1.6170969 -0.993726 -0.34532118 0.21048832 0.58011627 0.75192976 0.7418642 0.62504768 0.52354956 0.41607523 0.35525751 0.23404169 0.052532673 -0.26733017][-2.5375309 -2.1530125 -1.6696558 -1.1302748 -0.65137672 -0.30220747 -0.10802984 -0.044765949 -0.026019096 0.026294708 0.11262369 0.25810146 0.33354759 0.32361126 0.1327486][-2.8367691 -2.5960379 -2.2550852 -1.8123024 -1.3819714 -1.0248494 -0.79049754 -0.65715981 -0.54887414 -0.3883357 -0.1730752 0.081871033 0.28717852 0.39983797 0.32657528][-3.0622749 -2.9455237 -2.7315569 -2.39217 -2.0265641 -1.6742804 -1.4099329 -1.2191985 -1.0511882 -0.84243178 -0.58288145 -0.2820735 0.00731802 0.19204664 0.22251844]]...]
INFO - root - 2017-12-05 08:23:47.998817: step 3410, loss = 1.43, batch loss = 1.37 (36.3 examples/sec; 0.221 sec/batch; 20h:10m:26s remains)
INFO - root - 2017-12-05 08:23:50.144303: step 3420, loss = 1.87, batch loss = 1.81 (37.5 examples/sec; 0.213 sec/batch; 19h:29m:31s remains)
INFO - root - 2017-12-05 08:23:52.291427: step 3430, loss = 1.57, batch loss = 1.51 (37.6 examples/sec; 0.213 sec/batch; 19h:26m:26s remains)
INFO - root - 2017-12-05 08:23:54.462034: step 3440, loss = 1.67, batch loss = 1.61 (36.1 examples/sec; 0.221 sec/batch; 20h:14m:39s remains)
INFO - root - 2017-12-05 08:23:56.622732: step 3450, loss = 2.17, batch loss = 2.11 (37.3 examples/sec; 0.214 sec/batch; 19h:34m:39s remains)
INFO - root - 2017-12-05 08:23:58.812557: step 3460, loss = 1.74, batch loss = 1.68 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:45s remains)
INFO - root - 2017-12-05 08:24:00.997609: step 3470, loss = 2.08, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:47s remains)
INFO - root - 2017-12-05 08:24:03.179717: step 3480, loss = 1.70, batch loss = 1.64 (37.3 examples/sec; 0.214 sec/batch; 19h:34m:50s remains)
INFO - root - 2017-12-05 08:24:05.358103: step 3490, loss = 1.76, batch loss = 1.70 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:17s remains)
INFO - root - 2017-12-05 08:24:07.520201: step 3500, loss = 1.96, batch loss = 1.90 (37.1 examples/sec; 0.215 sec/batch; 19h:41m:33s remains)
2017-12-05 08:24:07.802418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4713383 -3.2780948 -3.1498823 -3.0793862 -3.0246174 -2.9450612 -2.8164821 -2.6304193 -2.3859739 -2.1072617 -1.8448327 -1.6763582 -1.6778979 -1.8716223 -2.2208219][-3.5449872 -3.3647013 -3.2381139 -3.1627073 -3.0994115 -3.0090127 -2.8634336 -2.647567 -2.3593049 -2.0226548 -1.695864 -1.4733946 -1.4502509 -1.6565802 -2.049449][-3.7108185 -3.564837 -3.4554441 -3.3858581 -3.3239093 -3.237062 -3.091841 -2.8658643 -2.552793 -2.17478 -1.7977169 -1.5247402 -1.4634776 -1.65294 -2.0503206][-3.8921564 -3.7854187 -3.7024107 -3.6485462 -3.5993094 -3.5231915 -3.3838177 -3.1543305 -2.8221161 -2.4091644 -1.9857781 -1.6598847 -1.5440612 -1.6836066 -2.0501435][-4.0192447 -3.9419098 -3.8810925 -3.8460402 -3.8189292 -3.7625763 -3.6334271 -3.398484 -3.0436888 -2.5890572 -2.1077373 -1.7123146 -1.5201359 -1.5848103 -1.89501][-4.0784397 -4.0169907 -3.9720926 -3.9554157 -3.9482012 -3.9106205 -3.7912321 -3.5478525 -3.1674776 -2.6678863 -2.1257272 -1.6596158 -1.388016 -1.3667552 -1.6000831][-4.1117067 -4.0552835 -4.017004 -4.011013 -4.0174947 -3.992265 -3.8788445 -3.6297626 -3.2309289 -2.6983414 -2.1131327 -1.595077 -1.259788 -1.1594019 -1.3089039][-4.144608 -4.0954809 -4.0638671 -4.0616808 -4.0719271 -4.0508084 -3.9425344 -3.6962984 -3.2983069 -2.7652123 -2.1759312 -1.6488791 -1.2894888 -1.1404433 -1.2210984][-4.1837015 -4.1431012 -4.1192474 -4.1176634 -4.1247311 -4.1049991 -4.0102835 -3.7920387 -3.4347014 -2.9531031 -2.4146545 -1.930419 -1.5907311 -1.4281764 -1.4644914][-4.2222366 -4.189899 -4.1707649 -4.1701374 -4.1765814 -4.16175 -4.0888853 -3.9164066 -3.6299562 -3.2442665 -2.8101115 -2.4170051 -2.1332424 -1.9894884 -2.003835][-4.2580051 -4.2337685 -4.2191539 -4.2161832 -4.2184682 -4.2085586 -4.15944 -4.0381556 -3.8358941 -3.5651093 -3.2567394 -2.9734113 -2.7595968 -2.6451035 -2.6444178][-4.283814 -4.2643814 -4.25351 -4.2503295 -4.2502351 -4.243042 -4.2139769 -4.1392431 -4.0139894 -3.8469784 -3.6533113 -3.4739347 -3.3315752 -3.2524912 -3.2465725][-4.2975168 -4.2818022 -4.2713265 -4.2671523 -4.2675705 -4.2648497 -4.2514062 -4.2130351 -4.1471095 -4.0578046 -3.9507134 -3.8485126 -3.7623818 -3.7148445 -3.7088218][-4.2931762 -4.2805176 -4.2729678 -4.270268 -4.2711535 -4.271688 -4.2676764 -4.250484 -4.2207708 -4.1768355 -4.1195874 -4.0626063 -4.0120821 -3.984149 -3.980428][-4.2605371 -4.2472596 -4.2436333 -4.2477512 -4.2539368 -4.2570128 -4.256249 -4.2485437 -4.2357206 -4.2124758 -4.1810627 -4.1474905 -4.1133604 -4.093152 -4.0895591]]...]
INFO - root - 2017-12-05 08:24:09.994379: step 3510, loss = 1.99, batch loss = 1.93 (36.6 examples/sec; 0.219 sec/batch; 19h:58m:45s remains)
INFO - root - 2017-12-05 08:24:12.184768: step 3520, loss = 1.60, batch loss = 1.54 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:49s remains)
INFO - root - 2017-12-05 08:24:14.357664: step 3530, loss = 1.93, batch loss = 1.88 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:53s remains)
INFO - root - 2017-12-05 08:24:16.560892: step 3540, loss = 1.79, batch loss = 1.73 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:44s remains)
INFO - root - 2017-12-05 08:24:18.746760: step 3550, loss = 1.74, batch loss = 1.68 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:39s remains)
INFO - root - 2017-12-05 08:24:20.891641: step 3560, loss = 1.77, batch loss = 1.71 (36.6 examples/sec; 0.218 sec/batch; 19h:56m:58s remains)
INFO - root - 2017-12-05 08:24:23.066907: step 3570, loss = 1.89, batch loss = 1.83 (37.3 examples/sec; 0.215 sec/batch; 19h:36m:47s remains)
INFO - root - 2017-12-05 08:24:25.232212: step 3580, loss = 1.97, batch loss = 1.91 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:20s remains)
INFO - root - 2017-12-05 08:24:27.418101: step 3590, loss = 1.91, batch loss = 1.85 (36.8 examples/sec; 0.218 sec/batch; 19h:52m:48s remains)
INFO - root - 2017-12-05 08:24:29.577510: step 3600, loss = 1.94, batch loss = 1.88 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:58s remains)
2017-12-05 08:24:29.876187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2988057 -4.2695761 -4.2416015 -4.2257724 -4.2269607 -4.2393951 -4.2565031 -4.2701831 -4.2741365 -4.2618265 -4.229085 -4.1789279 -4.1201797 -4.0508375 -3.9682014][-4.2252707 -4.1821904 -4.1364446 -4.1065707 -4.1029458 -4.1211076 -4.1557341 -4.1928039 -4.2198524 -4.2217054 -4.1879568 -4.1212263 -4.03565 -3.9342117 -3.8223693][-4.1063633 -4.0632386 -4.001411 -3.9510176 -3.9346037 -3.9545224 -4.0077362 -4.0753536 -4.1378241 -4.1701641 -4.1517591 -4.0832567 -3.9817007 -3.857193 -3.7245212][-3.9620731 -3.928885 -3.8572004 -3.7844076 -3.7458334 -3.7563934 -3.8180935 -3.9114916 -4.0121245 -4.0868549 -4.1039925 -4.0570226 -3.9598353 -3.8278565 -3.684674][-3.8243241 -3.8038659 -3.7290676 -3.6339619 -3.5637732 -3.5488529 -3.6015434 -3.7054203 -3.8343592 -3.9498205 -4.0123539 -4.0039058 -3.9303029 -3.8091459 -3.6704772][-3.7198312 -3.704448 -3.6251969 -3.5087817 -3.4044373 -3.3541007 -3.3821862 -3.4782896 -3.6174078 -3.7596781 -3.8621702 -3.8937132 -3.8550162 -3.7600756 -3.6420927][-3.6554112 -3.6309862 -3.5441966 -3.4100635 -3.2762547 -3.1894424 -3.1815991 -3.2543125 -3.3851891 -3.5336566 -3.65805 -3.7224343 -3.7232609 -3.6670988 -3.5855618][-3.6375046 -3.5920949 -3.4901142 -3.3379083 -3.1803851 -3.0590925 -3.0131633 -3.0542545 -3.162245 -3.300087 -3.4283726 -3.5105786 -3.5439446 -3.5322909 -3.4997561][-3.667201 -3.5969632 -3.476891 -3.308042 -3.1327219 -2.9852288 -2.90727 -2.9170353 -2.9961071 -3.1060324 -3.2171292 -3.3010426 -3.3571749 -3.3883862 -3.4071085][-3.7562833 -3.6663468 -3.533072 -3.3585126 -3.1794691 -3.0174801 -2.9197755 -2.9050984 -2.9538362 -3.0265045 -3.1055367 -3.1763577 -3.2415347 -3.3030944 -3.3645015][-3.9030409 -3.8088942 -3.6767721 -3.5129833 -3.3461173 -3.1915288 -3.0912287 -3.0606215 -3.0809484 -3.1196957 -3.1658273 -3.2140114 -3.2744117 -3.3469105 -3.430249][-4.0689449 -3.9900799 -3.8789911 -3.7447979 -3.6064711 -3.4769182 -3.3885531 -3.350343 -3.3494632 -3.36273 -3.3840737 -3.4169002 -3.4660873 -3.5283794 -3.6056342][-4.2009854 -4.1512322 -4.0780859 -3.9848704 -3.8857639 -3.7897203 -3.7164955 -3.6785474 -3.6666002 -3.6600072 -3.6618595 -3.6804962 -3.7138112 -3.7590282 -3.8168008][-4.2753663 -4.2506862 -4.2141719 -4.164856 -4.1048465 -4.0410638 -3.9838243 -3.9446864 -3.9204385 -3.9021297 -3.8934293 -3.8977721 -3.9128206 -3.9405236 -3.9776034][-4.3025503 -4.292099 -4.278379 -4.2565341 -4.225275 -4.1902122 -4.1497607 -4.1133213 -4.083415 -4.0571351 -4.0363626 -4.0281668 -4.0276003 -4.0374475 -4.0545182]]...]
INFO - root - 2017-12-05 08:24:32.037767: step 3610, loss = 1.68, batch loss = 1.62 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:23s remains)
INFO - root - 2017-12-05 08:24:34.237581: step 3620, loss = 2.13, batch loss = 2.07 (35.9 examples/sec; 0.223 sec/batch; 20h:20m:12s remains)
INFO - root - 2017-12-05 08:24:36.406788: step 3630, loss = 1.52, batch loss = 1.46 (36.3 examples/sec; 0.220 sec/batch; 20h:07m:15s remains)
INFO - root - 2017-12-05 08:24:38.596355: step 3640, loss = 2.12, batch loss = 2.06 (36.7 examples/sec; 0.218 sec/batch; 19h:53m:53s remains)
INFO - root - 2017-12-05 08:24:40.784967: step 3650, loss = 1.84, batch loss = 1.79 (36.2 examples/sec; 0.221 sec/batch; 20h:10m:19s remains)
INFO - root - 2017-12-05 08:24:42.944503: step 3660, loss = 1.89, batch loss = 1.83 (36.3 examples/sec; 0.221 sec/batch; 20h:08m:58s remains)
INFO - root - 2017-12-05 08:24:45.162334: step 3670, loss = 1.79, batch loss = 1.73 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:04s remains)
INFO - root - 2017-12-05 08:24:47.326982: step 3680, loss = 1.77, batch loss = 1.71 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:25s remains)
INFO - root - 2017-12-05 08:24:49.503099: step 3690, loss = 1.57, batch loss = 1.51 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:18s remains)
INFO - root - 2017-12-05 08:24:51.663920: step 3700, loss = 1.46, batch loss = 1.40 (36.8 examples/sec; 0.217 sec/batch; 19h:50m:55s remains)
2017-12-05 08:24:51.942615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1545014 -4.1517606 -4.1448383 -4.1349263 -4.1192961 -4.094574 -4.0642266 -4.0276761 -3.9934378 -3.9791956 -4.002039 -4.0613017 -4.1366835 -4.2099695 -4.2697716][-3.8130016 -3.8186228 -3.8166571 -3.8056316 -3.7862306 -3.7449532 -3.694211 -3.6289885 -3.5668447 -3.5405424 -3.5872071 -3.7105217 -3.8718629 -4.032773 -4.167448][-3.2927125 -3.3152957 -3.3261423 -3.3162358 -3.2890179 -3.2244847 -3.1449714 -3.0515504 -2.9667542 -2.9371138 -3.0244303 -3.2323489 -3.5052893 -3.7784634 -4.0072947][-2.6625967 -2.6883011 -2.7043672 -2.6943882 -2.6588168 -2.5708723 -2.4647059 -2.3519239 -2.2612958 -2.2589786 -2.4144053 -2.7263384 -3.1237707 -3.5107417 -3.8305616][-1.9756632 -1.9743872 -1.9678276 -1.9491491 -1.8999178 -1.7966075 -1.6777778 -1.5588887 -1.4976113 -1.5537827 -1.8164942 -2.2559819 -2.7882152 -3.284548 -3.6846685][-1.2438495 -1.1819789 -1.1317637 -1.0867412 -1.0212719 -0.91866994 -0.80505872 -0.708802 -0.71198916 -0.86907268 -1.2621231 -1.8410821 -2.5176563 -3.1204817 -3.597126][-0.50266075 -0.36770439 -0.26710987 -0.18615246 -0.0898819 0.014387608 0.11771202 0.17386723 0.0863018 -0.20053005 -0.75332546 -1.4800351 -2.2882369 -2.9943638 -3.5442355][0.16001272 0.35218906 0.48772383 0.60289097 0.73319387 0.86280012 0.98829889 1.0272551 0.87740421 0.46164656 -0.25384474 -1.1407938 -2.0894258 -2.889086 -3.4981072][0.65822935 0.86846304 1.0012345 1.1197853 1.2744436 1.4407139 1.603291 1.6552625 1.4798126 0.99065828 0.1677413 -0.84604478 -1.909224 -2.7869091 -3.4449723][0.86235 1.0468802 1.1266751 1.2148695 1.3665533 1.5537572 1.7494307 1.8261929 1.6589913 1.1632266 0.3195076 -0.73123193 -1.8279974 -2.7471528 -3.4235411][0.70427275 0.80995274 0.79344606 0.83149052 0.95378685 1.1393733 1.3526759 1.4549584 1.3232794 0.86723566 0.083824158 -0.90535235 -1.9426811 -2.816335 -3.4554737][0.18876886 0.18442011 0.06807518 0.041629314 0.12014866 0.28520679 0.47897005 0.57981634 0.46765852 0.08310461 -0.57487249 -1.4081771 -2.2861271 -3.0276291 -3.5614171][-0.61066127 -0.69730353 -0.88100386 -0.957041 -0.92787194 -0.80352068 -0.64995503 -0.55754542 -0.65960264 -0.966311 -1.4662001 -2.0975108 -2.7603033 -3.3221092 -3.7185326][-1.536211 -1.6416371 -1.8320827 -1.9240978 -1.9336469 -1.8617048 -1.7658823 -1.6980321 -1.7831161 -2.0024023 -2.3532834 -2.7820354 -3.2269454 -3.6131158 -3.8794816][-2.3931818 -2.4711852 -2.6235738 -2.7002316 -2.7282183 -2.6987097 -2.6523907 -2.6170068 -2.6848383 -2.8348446 -3.0575619 -3.3193889 -3.5920956 -3.8314302 -3.9997828]]...]
INFO - root - 2017-12-05 08:24:54.125363: step 3710, loss = 1.73, batch loss = 1.68 (35.2 examples/sec; 0.227 sec/batch; 20h:45m:07s remains)
INFO - root - 2017-12-05 08:24:56.306932: step 3720, loss = 1.94, batch loss = 1.88 (35.1 examples/sec; 0.228 sec/batch; 20h:48m:14s remains)
INFO - root - 2017-12-05 08:24:58.452253: step 3730, loss = 1.98, batch loss = 1.92 (37.8 examples/sec; 0.212 sec/batch; 19h:20m:34s remains)
INFO - root - 2017-12-05 08:25:00.628228: step 3740, loss = 1.70, batch loss = 1.64 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:48s remains)
INFO - root - 2017-12-05 08:25:02.821700: step 3750, loss = 1.83, batch loss = 1.77 (38.0 examples/sec; 0.211 sec/batch; 19h:13m:31s remains)
INFO - root - 2017-12-05 08:25:05.007577: step 3760, loss = 1.56, batch loss = 1.50 (37.3 examples/sec; 0.215 sec/batch; 19h:36m:14s remains)
INFO - root - 2017-12-05 08:25:07.189791: step 3770, loss = 1.79, batch loss = 1.73 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:10s remains)
INFO - root - 2017-12-05 08:25:09.344776: step 3780, loss = 1.71, batch loss = 1.65 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:16s remains)
INFO - root - 2017-12-05 08:25:11.523822: step 3790, loss = 2.03, batch loss = 1.97 (36.5 examples/sec; 0.219 sec/batch; 19h:59m:32s remains)
INFO - root - 2017-12-05 08:25:13.674820: step 3800, loss = 1.66, batch loss = 1.60 (37.8 examples/sec; 0.212 sec/batch; 19h:19m:06s remains)
2017-12-05 08:25:13.955491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0914807 -3.7647738 -3.2678661 -2.6652632 -2.0390112 -1.4402919 -0.89378715 -0.45163131 -0.20274639 -0.19775009 -0.41816497 -0.755774 -1.1114204 -1.4465058 -1.7622316][-4.1037049 -3.7856877 -3.2942123 -2.674027 -1.980958 -1.2627749 -0.57912207 -0.029619217 0.25940418 0.22270107 -0.11165142 -0.61902189 -1.1496239 -1.6078126 -1.9552658][-4.1307597 -3.8488741 -3.3968537 -2.7799282 -2.0209503 -1.1677871 -0.32681942 0.33809996 0.65326881 0.53634787 0.035731316 -0.68429303 -1.413646 -2.0045328 -2.3987181][-4.1776705 -3.9549553 -3.5670762 -2.9666271 -2.1391652 -1.1323481 -0.10215425 0.70293236 1.0395761 0.79968309 0.074364185 -0.89137053 -1.8126197 -2.5081105 -2.9278722][-4.241097 -4.0899253 -3.774452 -3.1988506 -2.3066366 -1.1418629 0.088567734 1.039577 1.3763547 0.97685194 0.0059375763 -1.1898239 -2.252362 -2.9927783 -3.3980014][-4.2902284 -4.1939549 -3.9349837 -3.3789668 -2.42801 -1.1199114 0.29241276 1.359046 1.6488671 1.0501318 -0.15447998 -1.5098028 -2.6214812 -3.3383489 -3.6987033][-4.3081141 -4.2348175 -4.0036683 -3.4541073 -2.4551978 -1.0342255 0.50751162 1.6274018 1.8193054 1.0124326 -0.38186789 -1.8181338 -2.9051008 -3.5494421 -3.8518338][-4.3155985 -4.247272 -4.0257306 -3.4767246 -2.4498041 -0.97723389 0.59654617 1.6666198 1.7087865 0.73046732 -0.747859 -2.1546845 -3.1418748 -3.6750379 -3.9127734][-4.3204041 -4.2571912 -4.0511537 -3.5282514 -2.5364237 -1.1240337 0.33736897 1.2433434 1.1358385 0.10630178 -1.2958789 -2.5400939 -3.3594806 -3.7644048 -3.9323785][-4.3215966 -4.2668085 -4.0906773 -3.6353388 -2.7652946 -1.5477481 -0.34251165 0.32049465 0.11252546 -0.81072211 -1.9619703 -2.9256368 -3.5338922 -3.8253813 -3.94835][-4.3234482 -4.2805552 -4.146028 -3.792686 -3.1129131 -2.1808624 -1.2991276 -0.87580419 -1.1058712 -1.822335 -2.658277 -3.3249328 -3.7167344 -3.8871996 -3.9564989][-4.3273087 -4.2972326 -4.2072949 -3.9690471 -3.5098605 -2.8924098 -2.332792 -2.0975146 -2.2820597 -2.7533174 -3.2753613 -3.6727991 -3.8859742 -3.9632771 -3.9910734][-4.3331594 -4.3158655 -4.266037 -4.1314311 -3.868479 -3.5246482 -3.2243018 -3.110765 -3.2182932 -3.4701657 -3.7403522 -3.935627 -4.0249624 -4.0400734 -4.0288582][-4.3372917 -4.3297281 -4.3083491 -4.2480845 -4.1248622 -3.9624953 -3.8177023 -3.7589998 -3.7985678 -3.9054313 -4.025557 -4.1104569 -4.1389723 -4.12622 -4.1011634][-4.3385687 -4.3345985 -4.3232603 -4.2919879 -4.2320895 -4.15093 -4.0689611 -4.0289812 -4.0404739 -4.0941191 -4.1630096 -4.2159572 -4.2308993 -4.2116137 -4.182776]]...]
INFO - root - 2017-12-05 08:25:16.117645: step 3810, loss = 1.52, batch loss = 1.46 (36.6 examples/sec; 0.219 sec/batch; 19h:57m:59s remains)
INFO - root - 2017-12-05 08:25:18.291529: step 3820, loss = 1.54, batch loss = 1.48 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:58s remains)
INFO - root - 2017-12-05 08:25:20.467947: step 3830, loss = 1.86, batch loss = 1.80 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:31s remains)
INFO - root - 2017-12-05 08:25:22.655929: step 3840, loss = 2.10, batch loss = 2.04 (35.7 examples/sec; 0.224 sec/batch; 20h:29m:04s remains)
INFO - root - 2017-12-05 08:25:24.840833: step 3850, loss = 1.87, batch loss = 1.81 (37.3 examples/sec; 0.214 sec/batch; 19h:34m:10s remains)
INFO - root - 2017-12-05 08:25:27.029879: step 3860, loss = 1.88, batch loss = 1.82 (36.6 examples/sec; 0.218 sec/batch; 19h:56m:15s remains)
INFO - root - 2017-12-05 08:25:29.190187: step 3870, loss = 1.87, batch loss = 1.81 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:14s remains)
INFO - root - 2017-12-05 08:25:31.373387: step 3880, loss = 1.53, batch loss = 1.47 (36.3 examples/sec; 0.221 sec/batch; 20h:07m:45s remains)
INFO - root - 2017-12-05 08:25:33.533807: step 3890, loss = 1.87, batch loss = 1.81 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:53s remains)
INFO - root - 2017-12-05 08:25:35.701247: step 3900, loss = 1.79, batch loss = 1.73 (36.9 examples/sec; 0.217 sec/batch; 19h:48m:14s remains)
2017-12-05 08:25:35.994849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3327107 -4.3296628 -4.3240004 -4.318378 -4.315093 -4.3142209 -4.3131657 -4.3075333 -4.3004336 -4.2976236 -4.3038154 -4.312119 -4.3143945 -4.3079481 -4.2941985][-4.3317246 -4.3247066 -4.3107471 -4.293766 -4.2806878 -4.2733707 -4.2678146 -4.2568231 -4.2450876 -4.2415476 -4.2564034 -4.28064 -4.2995896 -4.3069973 -4.3046093][-4.3291521 -4.3170385 -4.2923017 -4.2576456 -4.2239237 -4.1964507 -4.1754227 -4.1557312 -4.1411762 -4.1403747 -4.1663752 -4.2119908 -4.2571492 -4.2847648 -4.2928543][-4.324544 -4.3074565 -4.2735772 -4.2221518 -4.1625509 -4.1025753 -4.0527449 -4.0187545 -4.0023289 -4.0041547 -4.0362329 -4.0967522 -4.1683912 -4.2254963 -4.2534547][-4.3191476 -4.2961879 -4.2548509 -4.1942005 -4.1183791 -4.0323582 -3.9526181 -3.8962743 -3.8688602 -3.8610399 -3.8779461 -3.9312048 -4.0169163 -4.1069031 -4.1712379][-4.3156195 -4.288187 -4.2426319 -4.1831422 -4.1101508 -4.0209737 -3.9239392 -3.8364649 -3.7748523 -3.7332401 -3.7138987 -3.7421062 -3.8290229 -3.95105 -4.0622249][-4.317306 -4.2908888 -4.2512584 -4.2053046 -4.1514816 -4.0790486 -3.9853489 -3.8778374 -3.778156 -3.6918201 -3.628953 -3.6218047 -3.689255 -3.817409 -3.9552302][-4.3240924 -4.3083205 -4.28545 -4.262691 -4.23647 -4.1934276 -4.1239433 -4.0268583 -3.9143405 -3.7963111 -3.6951616 -3.6484139 -3.6784708 -3.780494 -3.9131398][-4.329567 -4.3238254 -4.3168063 -4.3118153 -4.3067689 -4.2910552 -4.2567029 -4.1973214 -4.1113758 -4.00343 -3.8949828 -3.8231864 -3.8148868 -3.8729527 -3.9719715][-4.3320637 -4.3295264 -4.3279409 -4.3280845 -4.3298182 -4.3292108 -4.3204346 -4.2976685 -4.2559772 -4.1930194 -4.11885 -4.0579166 -4.0320506 -4.0499263 -4.1015606][-4.33297 -4.3302951 -4.3281989 -4.3279543 -4.3291183 -4.3307052 -4.3312545 -4.3290219 -4.3200917 -4.2995906 -4.2687588 -4.2375331 -4.2175035 -4.2162666 -4.2307944][-4.3333883 -4.3317041 -4.3297539 -4.3288984 -4.3294291 -4.3303003 -4.3312249 -4.3318777 -4.3316545 -4.3291807 -4.3235636 -4.3155284 -4.3057637 -4.2992907 -4.29813][-4.333683 -4.3331957 -4.3325443 -4.3320131 -4.332026 -4.3319583 -4.3321824 -4.3324947 -4.3329077 -4.3330069 -4.3327351 -4.33193 -4.330328 -4.3284974 -4.327075][-4.3340783 -4.3339796 -4.3338056 -4.3336658 -4.333611 -4.3334451 -4.33345 -4.3335471 -4.3337297 -4.3339396 -4.3340707 -4.3340554 -4.3337379 -4.3334589 -4.3332009][-4.3342419 -4.3342552 -4.3342237 -4.33421 -4.3342519 -4.3342752 -4.3342743 -4.3342466 -4.33423 -4.3342271 -4.3342094 -4.33419 -4.3341694 -4.334156 -4.3341718]]...]
INFO - root - 2017-12-05 08:25:38.155707: step 3910, loss = 2.18, batch loss = 2.12 (37.2 examples/sec; 0.215 sec/batch; 19h:36m:31s remains)
INFO - root - 2017-12-05 08:25:40.364725: step 3920, loss = 1.76, batch loss = 1.70 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:41s remains)
INFO - root - 2017-12-05 08:25:42.533134: step 3930, loss = 1.79, batch loss = 1.73 (37.4 examples/sec; 0.214 sec/batch; 19h:32m:01s remains)
INFO - root - 2017-12-05 08:25:44.730181: step 3940, loss = 1.93, batch loss = 1.88 (35.9 examples/sec; 0.223 sec/batch; 20h:20m:03s remains)
INFO - root - 2017-12-05 08:25:46.898153: step 3950, loss = 1.77, batch loss = 1.71 (37.1 examples/sec; 0.215 sec/batch; 19h:40m:01s remains)
INFO - root - 2017-12-05 08:25:49.082129: step 3960, loss = 1.76, batch loss = 1.70 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:35s remains)
INFO - root - 2017-12-05 08:25:51.293830: step 3970, loss = 1.78, batch loss = 1.72 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:52s remains)
INFO - root - 2017-12-05 08:25:53.487128: step 3980, loss = 2.00, batch loss = 1.94 (37.0 examples/sec; 0.216 sec/batch; 19h:42m:22s remains)
INFO - root - 2017-12-05 08:25:55.668614: step 3990, loss = 1.71, batch loss = 1.65 (36.5 examples/sec; 0.219 sec/batch; 20h:00m:29s remains)
INFO - root - 2017-12-05 08:25:57.865285: step 4000, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:36m:44s remains)
2017-12-05 08:25:58.168073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3060637 -4.303442 -4.3028617 -4.3043809 -4.3079729 -4.3129559 -4.3182244 -4.3224292 -4.324801 -4.325592 -4.3258862 -4.3261738 -4.3265061 -4.3268404 -4.3270144][-4.3106375 -4.3079629 -4.3070741 -4.3077755 -4.3100505 -4.313817 -4.3184509 -4.3224659 -4.3248839 -4.3258548 -4.3263206 -4.3265915 -4.3268471 -4.3270583 -4.3272004][-4.3160586 -4.3133569 -4.311903 -4.3116894 -4.3127356 -4.3151464 -4.3187828 -4.3224387 -4.3249321 -4.3260188 -4.3264503 -4.3267436 -4.3270512 -4.3272638 -4.3273559][-4.3206482 -4.3182817 -4.3164458 -4.3152461 -4.3150034 -4.31622 -4.3188438 -4.322053 -4.3245921 -4.3259578 -4.3265338 -4.3267841 -4.3269963 -4.3272223 -4.3273959][-4.3236461 -4.3217158 -4.3196425 -4.317708 -4.3163862 -4.3164678 -4.318243 -4.3211184 -4.3237762 -4.3255405 -4.3263721 -4.3267326 -4.3269682 -4.3271484 -4.3272991][-4.3250542 -4.323534 -4.3213305 -4.3188925 -4.31686 -4.3160963 -4.316946 -4.3194408 -4.3223629 -4.3246489 -4.3258443 -4.3264236 -4.32676 -4.3270135 -4.327198][-4.3254042 -4.3239584 -4.321805 -4.3191924 -4.316752 -4.3152604 -4.3153744 -4.3173013 -4.3203068 -4.3231668 -4.3248272 -4.3257289 -4.3262563 -4.3266721 -4.32698][-4.3257532 -4.3241887 -4.321969 -4.319119 -4.3164563 -4.3145556 -4.3140168 -4.3152351 -4.3180895 -4.3212404 -4.3234119 -4.3246622 -4.32542 -4.3261156 -4.3266687][-4.326314 -4.3248811 -4.3226957 -4.3196397 -4.3167524 -4.314662 -4.3137646 -4.31429 -4.3165188 -4.3195109 -4.321928 -4.3233829 -4.3243756 -4.3253212 -4.3262157][-4.3268714 -4.325839 -4.3240089 -4.3212018 -4.3183045 -4.3161974 -4.3150048 -4.3149204 -4.3162484 -4.3185339 -4.3206973 -4.3222184 -4.3233824 -4.3245444 -4.3257089][-4.3272862 -4.3266339 -4.3253727 -4.3232355 -4.3207421 -4.3186388 -4.3173766 -4.316999 -4.31745 -4.318778 -4.3203845 -4.3217726 -4.322957 -4.3241825 -4.3254452][-4.3275161 -4.3272519 -4.3265219 -4.3250761 -4.3232856 -4.3215804 -4.3203888 -4.3197336 -4.3196096 -4.3201685 -4.321207 -4.3222613 -4.3231955 -4.32426 -4.325449][-4.3275981 -4.3275237 -4.3272233 -4.3265147 -4.3253965 -4.3241496 -4.32323 -4.3226013 -4.3222389 -4.3222218 -4.3227277 -4.3233657 -4.3240066 -4.3247552 -4.325633][-4.3276 -4.3275719 -4.3274574 -4.3271618 -4.3266406 -4.3259778 -4.3254046 -4.3249397 -4.324574 -4.3243589 -4.324491 -4.3247652 -4.3251615 -4.3256216 -4.3261166][-4.327599 -4.327579 -4.3275127 -4.3273458 -4.3270893 -4.3268008 -4.3265104 -4.3262587 -4.3260584 -4.3259077 -4.3258023 -4.3259115 -4.3262196 -4.3265147 -4.3267021]]...]
INFO - root - 2017-12-05 08:26:00.351367: step 4010, loss = 1.94, batch loss = 1.89 (35.3 examples/sec; 0.227 sec/batch; 20h:42m:00s remains)
INFO - root - 2017-12-05 08:26:02.533212: step 4020, loss = 1.77, batch loss = 1.72 (36.7 examples/sec; 0.218 sec/batch; 19h:53m:56s remains)
INFO - root - 2017-12-05 08:26:04.696631: step 4030, loss = 1.91, batch loss = 1.85 (37.5 examples/sec; 0.213 sec/batch; 19h:28m:19s remains)
INFO - root - 2017-12-05 08:26:06.885000: step 4040, loss = 1.81, batch loss = 1.75 (36.8 examples/sec; 0.217 sec/batch; 19h:48m:48s remains)
INFO - root - 2017-12-05 08:26:09.071888: step 4050, loss = 1.91, batch loss = 1.85 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:57s remains)
INFO - root - 2017-12-05 08:26:11.266249: step 4060, loss = 1.47, batch loss = 1.41 (37.0 examples/sec; 0.216 sec/batch; 19h:42m:25s remains)
INFO - root - 2017-12-05 08:26:13.451293: step 4070, loss = 1.88, batch loss = 1.82 (37.1 examples/sec; 0.215 sec/batch; 19h:39m:08s remains)
INFO - root - 2017-12-05 08:26:15.645101: step 4080, loss = 2.00, batch loss = 1.94 (36.3 examples/sec; 0.220 sec/batch; 20h:06m:07s remains)
INFO - root - 2017-12-05 08:26:17.793769: step 4090, loss = 1.91, batch loss = 1.85 (37.4 examples/sec; 0.214 sec/batch; 19h:31m:13s remains)
INFO - root - 2017-12-05 08:26:19.960836: step 4100, loss = 1.68, batch loss = 1.62 (37.3 examples/sec; 0.214 sec/batch; 19h:33m:26s remains)
2017-12-05 08:26:20.273318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2535105 -4.2437053 -4.236454 -4.2319236 -4.2295341 -4.226377 -4.2196426 -4.2104993 -4.2024369 -4.1985044 -4.1993575 -4.2049928 -4.2168727 -4.2358427 -4.2582593][-4.1520481 -4.1374011 -4.1287475 -4.1252913 -4.124599 -4.1204925 -4.1090779 -4.0936112 -4.080039 -4.0739236 -4.0770693 -4.0903554 -4.1152244 -4.1522994 -4.1938729][-3.997036 -3.982075 -3.9768715 -3.977428 -3.9799376 -3.9752598 -3.9588091 -3.9373705 -3.9190407 -3.9121535 -3.9195213 -3.9426858 -3.9830191 -4.0397754 -4.10143][-3.7898903 -3.7753344 -3.7751632 -3.7809105 -3.7868662 -3.7805581 -3.7566748 -3.7252645 -3.6991961 -3.6909165 -3.7043247 -3.742373 -3.8048575 -3.8873806 -3.9739106][-3.5168219 -3.5052834 -3.5108552 -3.5206223 -3.5275636 -3.5185931 -3.4862423 -3.4409707 -3.4062819 -3.3972309 -3.4206576 -3.4818895 -3.576798 -3.6944563 -3.815846][-3.1963432 -3.1890702 -3.204649 -3.224586 -3.2362502 -3.2236071 -3.1821251 -3.1292324 -3.0940437 -3.0909958 -3.1262238 -3.2086887 -3.3315349 -3.4817803 -3.6396844][-2.8891377 -2.890624 -2.9211197 -2.9501672 -2.9671161 -2.9551613 -2.9127803 -2.8649673 -2.8420408 -2.858345 -2.9072607 -2.994308 -3.1181428 -3.2716238 -3.4401479][-2.6483719 -2.6558313 -2.6963739 -2.733326 -2.7585273 -2.7568059 -2.7276177 -2.6999898 -2.7033722 -2.7380922 -2.7874327 -2.85665 -2.9513783 -3.078135 -3.2343009][-2.4911304 -2.4949355 -2.5378819 -2.5793185 -2.6167157 -2.6348727 -2.6309745 -2.632906 -2.659245 -2.7004728 -2.7350373 -2.7749944 -2.8328619 -2.9235051 -3.0564976][-2.4151022 -2.40906 -2.4487672 -2.493227 -2.5403173 -2.5788476 -2.6025903 -2.6281233 -2.6634793 -2.6981902 -2.7123709 -2.7243268 -2.7467198 -2.808295 -2.9245381][-2.3838463 -2.3645046 -2.3993926 -2.443481 -2.4956074 -2.5447192 -2.5842924 -2.620882 -2.6507049 -2.670213 -2.6608977 -2.6509213 -2.6465354 -2.6888175 -2.7953944][-2.362062 -2.3287292 -2.3586307 -2.3994184 -2.452425 -2.501812 -2.5402045 -2.5694451 -2.5874972 -2.5903435 -2.5621338 -2.5379159 -2.5226088 -2.5592349 -2.6674695][-2.3947554 -2.3434281 -2.3547423 -2.3869309 -2.4370174 -2.4820318 -2.512383 -2.5248675 -2.525152 -2.5103698 -2.4720895 -2.4385915 -2.4240394 -2.471251 -2.5991516][-2.5102491 -2.4428294 -2.4405737 -2.456804 -2.4975204 -2.5319314 -2.5486541 -2.5437355 -2.5268507 -2.497858 -2.4559076 -2.4265642 -2.4268503 -2.4894555 -2.6296551][-2.7238865 -2.644279 -2.6316938 -2.6380169 -2.6656389 -2.6820085 -2.6831975 -2.6604145 -2.6283159 -2.5888433 -2.5476913 -2.525918 -2.544045 -2.62476 -2.7720978]]...]
INFO - root - 2017-12-05 08:26:22.440547: step 4110, loss = 2.00, batch loss = 1.94 (36.7 examples/sec; 0.218 sec/batch; 19h:53m:00s remains)
INFO - root - 2017-12-05 08:26:24.628255: step 4120, loss = 1.85, batch loss = 1.80 (36.1 examples/sec; 0.222 sec/batch; 20h:12m:39s remains)
INFO - root - 2017-12-05 08:26:26.824456: step 4130, loss = 1.99, batch loss = 1.94 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:07s remains)
INFO - root - 2017-12-05 08:26:28.990604: step 4140, loss = 1.52, batch loss = 1.46 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:32s remains)
INFO - root - 2017-12-05 08:26:31.185668: step 4150, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.215 sec/batch; 19h:35m:00s remains)
INFO - root - 2017-12-05 08:26:33.382118: step 4160, loss = 1.92, batch loss = 1.86 (36.1 examples/sec; 0.221 sec/batch; 20h:11m:45s remains)
INFO - root - 2017-12-05 08:26:35.551025: step 4170, loss = 1.92, batch loss = 1.86 (37.2 examples/sec; 0.215 sec/batch; 19h:35m:22s remains)
INFO - root - 2017-12-05 08:26:37.721419: step 4180, loss = 1.81, batch loss = 1.75 (36.9 examples/sec; 0.217 sec/batch; 19h:45m:55s remains)
INFO - root - 2017-12-05 08:26:39.892369: step 4190, loss = 1.75, batch loss = 1.69 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:20s remains)
INFO - root - 2017-12-05 08:26:42.064330: step 4200, loss = 2.01, batch loss = 1.96 (35.7 examples/sec; 0.224 sec/batch; 20h:27m:27s remains)
2017-12-05 08:26:42.347471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421][-4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421 -4.316421]]...]
INFO - root - 2017-12-05 08:26:44.506359: step 4210, loss = 1.74, batch loss = 1.68 (35.9 examples/sec; 0.223 sec/batch; 20h:19m:57s remains)
INFO - root - 2017-12-05 08:26:46.659404: step 4220, loss = 1.77, batch loss = 1.72 (37.0 examples/sec; 0.216 sec/batch; 19h:43m:37s remains)
INFO - root - 2017-12-05 08:26:48.833309: step 4230, loss = 1.65, batch loss = 1.59 (37.3 examples/sec; 0.214 sec/batch; 19h:32m:05s remains)
INFO - root - 2017-12-05 08:26:50.993530: step 4240, loss = 2.04, batch loss = 1.98 (36.4 examples/sec; 0.220 sec/batch; 20h:01m:58s remains)
INFO - root - 2017-12-05 08:26:53.168198: step 4250, loss = 1.80, batch loss = 1.74 (36.8 examples/sec; 0.217 sec/batch; 19h:47m:49s remains)
INFO - root - 2017-12-05 08:26:55.348418: step 4260, loss = 1.99, batch loss = 1.93 (36.5 examples/sec; 0.219 sec/batch; 19h:59m:58s remains)
INFO - root - 2017-12-05 08:26:57.536158: step 4270, loss = 1.86, batch loss = 1.80 (37.9 examples/sec; 0.211 sec/batch; 19h:13m:18s remains)
INFO - root - 2017-12-05 08:26:59.678417: step 4280, loss = 1.70, batch loss = 1.64 (37.9 examples/sec; 0.211 sec/batch; 19h:15m:53s remains)
INFO - root - 2017-12-05 08:27:01.856161: step 4290, loss = 1.84, batch loss = 1.78 (36.8 examples/sec; 0.217 sec/batch; 19h:48m:28s remains)
INFO - root - 2017-12-05 08:27:04.024484: step 4300, loss = 1.93, batch loss = 1.88 (36.9 examples/sec; 0.217 sec/batch; 19h:46m:38s remains)
2017-12-05 08:27:04.333778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778][-4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778 -4.3080778]]...]
INFO - root - 2017-12-05 08:27:06.495918: step 4310, loss = 1.92, batch loss = 1.86 (36.8 examples/sec; 0.217 sec/batch; 19h:48m:44s remains)
INFO - root - 2017-12-05 08:27:08.658885: step 4320, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 19h:54m:58s remains)
INFO - root - 2017-12-05 08:27:10.836903: step 4330, loss = 1.64, batch loss = 1.58 (36.7 examples/sec; 0.218 sec/batch; 19h:52m:26s remains)
INFO - root - 2017-12-05 08:27:13.000458: step 4340, loss = 1.71, batch loss = 1.65 (38.0 examples/sec; 0.210 sec/batch; 19h:10m:08s remains)
INFO - root - 2017-12-05 08:27:15.170802: step 4350, loss = 1.74, batch loss = 1.69 (35.5 examples/sec; 0.226 sec/batch; 20h:33m:56s remains)
INFO - root - 2017-12-05 08:27:17.360406: step 4360, loss = 1.95, batch loss = 1.89 (37.3 examples/sec; 0.214 sec/batch; 19h:31m:26s remains)
INFO - root - 2017-12-05 08:27:19.519362: step 4370, loss = 1.62, batch loss = 1.56 (37.4 examples/sec; 0.214 sec/batch; 19h:30m:31s remains)
INFO - root - 2017-12-05 08:27:21.702349: step 4380, loss = 1.79, batch loss = 1.73 (36.6 examples/sec; 0.219 sec/batch; 19h:55m:32s remains)
INFO - root - 2017-12-05 08:27:23.877266: step 4390, loss = 2.03, batch loss = 1.98 (36.6 examples/sec; 0.219 sec/batch; 19h:55m:22s remains)
INFO - root - 2017-12-05 08:27:26.021264: step 4400, loss = 1.92, batch loss = 1.86 (38.4 examples/sec; 0.208 sec/batch; 18h:57m:53s remains)
2017-12-05 08:27:26.299995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116][-4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116][-4.2996116 -4.2996116 -4.299612 -4.299612 -4.299612 -4.2996125 -4.299612 -4.2996111 -4.2996111 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116 -4.2996116][-4.2996116 -4.2996111 -4.2996097 -4.2996092 -4.2996092 -4.2996097 -4.2996092 -4.2996087 -4.2996092 -4.29961 -4.2996106 -4.2996111 -4.2996111 -4.2996111 -4.2996111][-4.29961 -4.2996068 -4.299603 -4.2996025 -4.2996058 -4.2996078 -4.2996068 -4.2996068 -4.2996068 -4.2996078 -4.2996092 -4.29961 -4.2996106 -4.2996111 -4.2996116][-4.2996087 -4.2996039 -4.299602 -4.2996011 -4.299602 -4.2996016 -4.2996016 -4.2996035 -4.2996039 -4.2996068 -4.2996087 -4.29961 -4.2996106 -4.2996111 -4.2996111][-4.2996049 -4.2995992 -4.2995954 -4.2995944 -4.299593 -4.299592 -4.2995963 -4.2995996 -4.2995987 -4.2995973 -4.2995973 -4.2995963 -4.2995973 -4.2996006 -4.299603][-4.299572 -4.2995648 -4.2995591 -4.2995553 -4.2995548 -4.2995582 -4.2995653 -4.2995539 -4.2995186 -4.299489 -4.29949 -4.2995071 -4.2995319 -4.2995567 -4.2995729][-4.299469 -4.2994504 -4.2994361 -4.2994423 -4.2994614 -4.2994833 -4.2994843 -4.299439 -4.2993746 -4.29935 -4.2993684 -4.2993913 -4.2994437 -4.2994971 -4.2995372][-4.2994514 -4.2994504 -4.2994504 -4.2994595 -4.2994819 -4.299499 -4.2994895 -4.2994485 -4.2993956 -4.2993617 -4.2993536 -4.2993622 -4.2994223 -4.2994857 -4.299531][-4.2995114 -4.29952 -4.2995272 -4.2995358 -4.2995472 -4.2995539 -4.2995439 -4.2995081 -4.2994666 -4.29943 -4.2994127 -4.2994137 -4.299468 -4.2995396 -4.2995734][-4.2996149 -4.2996197 -4.2996149 -4.2996106 -4.2996087 -4.2996063 -4.2996025 -4.299582 -4.29955 -4.2995229 -4.2995052 -4.29951 -4.2995596 -4.2996068 -4.2996225][-4.2996273 -4.2996254 -4.29962 -4.2996144 -4.2996149 -4.2996173 -4.2996197 -4.2996178 -4.29961 -4.2995958 -4.2995811 -4.2995815 -4.2996049 -4.2996273 -4.2996244][-4.29961 -4.2996082 -4.2996035 -4.2996011 -4.2996025 -4.2996054 -4.2996082 -4.2996116 -4.2996159 -4.2996144 -4.2996087 -4.2996097 -4.2996221 -4.29963 -4.29962][-4.2996011 -4.2996 -4.2995996 -4.2996011 -4.2996016 -4.299602 -4.2996035 -4.2996039 -4.2996058 -4.2996078 -4.2996073 -4.2996058 -4.29961 -4.29961 -4.2996049]]...]
INFO - root - 2017-12-05 08:27:28.464025: step 4410, loss = 1.95, batch loss = 1.89 (37.8 examples/sec; 0.211 sec/batch; 19h:16m:22s remains)
INFO - root - 2017-12-05 08:27:30.651800: step 4420, loss = 1.85, batch loss = 1.80 (37.1 examples/sec; 0.216 sec/batch; 19h:38m:36s remains)
INFO - root - 2017-12-05 08:27:32.789977: step 4430, loss = 1.86, batch loss = 1.80 (36.8 examples/sec; 0.217 sec/batch; 19h:47m:03s remains)
INFO - root - 2017-12-05 08:27:34.943599: step 4440, loss = 1.56, batch loss = 1.50 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:09s remains)
INFO - root - 2017-12-05 08:27:37.119213: step 4450, loss = 1.71, batch loss = 1.65 (37.5 examples/sec; 0.213 sec/batch; 19h:26m:12s remains)
INFO - root - 2017-12-05 08:27:39.328110: step 4460, loss = 1.97, batch loss = 1.92 (35.6 examples/sec; 0.225 sec/batch; 20h:29m:42s remains)
INFO - root - 2017-12-05 08:27:41.507821: step 4470, loss = 1.85, batch loss = 1.80 (37.6 examples/sec; 0.213 sec/batch; 19h:21m:50s remains)
INFO - root - 2017-12-05 08:27:43.678586: step 4480, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 20h:24m:35s remains)
INFO - root - 2017-12-05 08:27:45.854293: step 4490, loss = 1.59, batch loss = 1.53 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:50s remains)
INFO - root - 2017-12-05 08:27:48.026912: step 4500, loss = 1.87, batch loss = 1.81 (37.2 examples/sec; 0.215 sec/batch; 19h:35m:00s remains)
2017-12-05 08:27:48.318592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2909527 -4.2909608 -4.2909436 -4.2909255 -4.2909169 -4.2908406 -4.2907882 -4.2908092 -4.2908425 -4.29089 -4.2909236 -4.2909427 -4.2909274 -4.2909269 -4.2909436][-4.2909513 -4.2909527 -4.2909045 -4.2908092 -4.29066 -4.290482 -4.2902832 -4.29025 -4.2903619 -4.2905889 -4.2907877 -4.2908769 -4.2908988 -4.2909236 -4.29095][-4.2909322 -4.2909122 -4.2908359 -4.2906294 -4.2902284 -4.2897944 -4.2893329 -4.289094 -4.2893119 -4.2899055 -4.2904058 -4.2907038 -4.2908506 -4.2909217 -4.2909522][-4.2908978 -4.290834 -4.2907186 -4.2903643 -4.289536 -4.2882352 -4.2869797 -4.2863564 -4.2871304 -4.288754 -4.2898426 -4.2904286 -4.2907224 -4.2908573 -4.2909069][-4.2908769 -4.2907639 -4.2905574 -4.289959 -4.2882686 -4.2850556 -4.2813921 -4.27971 -4.2824912 -4.2866821 -4.2890453 -4.290103 -4.2905755 -4.2907786 -4.2908564][-4.2908869 -4.2907424 -4.2904072 -4.2894559 -4.2864161 -4.2795844 -4.2706895 -4.2668819 -4.2741833 -4.2833786 -4.2880392 -4.2897778 -4.29051 -4.2907825 -4.2908692][-4.2909112 -4.2907672 -4.2903371 -4.289041 -4.2844219 -4.2726874 -4.2562151 -4.2505412 -4.2646418 -4.2801418 -4.2873945 -4.2896676 -4.2905478 -4.2908235 -4.290905][-4.2909241 -4.2908068 -4.2903943 -4.28898 -4.2836304 -4.2691827 -4.2481918 -4.2424049 -4.2605858 -4.2791481 -4.2873168 -4.2897367 -4.290627 -4.29087 -4.2909336][-4.2909241 -4.290832 -4.29051 -4.2893314 -4.2848105 -4.2724261 -4.2538843 -4.2489071 -4.2647333 -4.2805123 -4.28763 -4.2898445 -4.2906733 -4.2908978 -4.290946][-4.2909231 -4.2908607 -4.2906647 -4.2898669 -4.2868924 -4.278852 -4.2667308 -4.2635446 -4.2734284 -4.2833958 -4.2882767 -4.2900171 -4.2907076 -4.2909026 -4.2909489][-4.2909346 -4.2909026 -4.2907867 -4.2903795 -4.2887573 -4.284513 -4.2782245 -4.2767611 -4.2814522 -4.2863631 -4.289115 -4.2902369 -4.2907248 -4.2908945 -4.2909436][-4.2909489 -4.2909341 -4.29086 -4.2906876 -4.28994 -4.2879915 -4.2851524 -4.2845349 -4.2863312 -4.2884111 -4.2897949 -4.2904806 -4.2907672 -4.2908921 -4.29094][-4.2909503 -4.2909484 -4.2909155 -4.290853 -4.2905602 -4.2897563 -4.2885451 -4.2881804 -4.2887912 -4.2896314 -4.2902784 -4.29065 -4.2908144 -4.2909064 -4.2909436][-4.2909441 -4.2909484 -4.2909446 -4.2909288 -4.2908225 -4.2905455 -4.2901111 -4.2898831 -4.2900267 -4.2903142 -4.2905889 -4.2907648 -4.2908592 -4.2909307 -4.29095][-4.2909489 -4.2909527 -4.2909536 -4.2909508 -4.2909117 -4.290832 -4.2907109 -4.2906241 -4.2906451 -4.2907057 -4.2907948 -4.2908659 -4.290906 -4.2909417 -4.29095]]...]
INFO - root - 2017-12-05 08:27:50.485065: step 4510, loss = 1.60, batch loss = 1.54 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:07s remains)
INFO - root - 2017-12-05 08:27:52.651456: step 4520, loss = 1.99, batch loss = 1.93 (35.4 examples/sec; 0.226 sec/batch; 20h:35m:02s remains)
INFO - root - 2017-12-05 08:27:54.850889: step 4530, loss = 2.12, batch loss = 2.06 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:37s remains)
INFO - root - 2017-12-05 08:27:57.030010: step 4540, loss = 1.79, batch loss = 1.74 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:40s remains)
INFO - root - 2017-12-05 08:27:59.202199: step 4550, loss = 1.66, batch loss = 1.60 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:43s remains)
INFO - root - 2017-12-05 08:28:01.362187: step 4560, loss = 1.85, batch loss = 1.79 (37.0 examples/sec; 0.216 sec/batch; 19h:40m:53s remains)
INFO - root - 2017-12-05 08:28:03.529970: step 4570, loss = 1.60, batch loss = 1.54 (36.6 examples/sec; 0.219 sec/batch; 19h:54m:29s remains)
INFO - root - 2017-12-05 08:28:05.703408: step 4580, loss = 1.63, batch loss = 1.57 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:26s remains)
INFO - root - 2017-12-05 08:28:07.870343: step 4590, loss = 1.89, batch loss = 1.83 (36.9 examples/sec; 0.217 sec/batch; 19h:45m:07s remains)
INFO - root - 2017-12-05 08:28:10.057881: step 4600, loss = 1.74, batch loss = 1.68 (37.7 examples/sec; 0.212 sec/batch; 19h:19m:32s remains)
2017-12-05 08:28:10.328102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667][-4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667 -4.2865667]]...]
INFO - root - 2017-12-05 08:28:12.501458: step 4610, loss = 1.80, batch loss = 1.74 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:01s remains)
INFO - root - 2017-12-05 08:28:14.725957: step 4620, loss = 1.78, batch loss = 1.72 (36.3 examples/sec; 0.220 sec/batch; 20h:04m:10s remains)
INFO - root - 2017-12-05 08:28:16.901240: step 4630, loss = 1.83, batch loss = 1.77 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:09s remains)
INFO - root - 2017-12-05 08:28:19.078572: step 4640, loss = 1.70, batch loss = 1.64 (37.2 examples/sec; 0.215 sec/batch; 19h:35m:08s remains)
INFO - root - 2017-12-05 08:28:21.256020: step 4650, loss = 1.52, batch loss = 1.46 (37.7 examples/sec; 0.212 sec/batch; 19h:21m:00s remains)
INFO - root - 2017-12-05 08:28:23.450329: step 4660, loss = 1.92, batch loss = 1.86 (34.5 examples/sec; 0.232 sec/batch; 21h:05m:47s remains)
INFO - root - 2017-12-05 08:28:25.629166: step 4670, loss = 1.77, batch loss = 1.71 (35.8 examples/sec; 0.224 sec/batch; 20h:21m:10s remains)
INFO - root - 2017-12-05 08:28:27.790249: step 4680, loss = 1.47, batch loss = 1.41 (36.7 examples/sec; 0.218 sec/batch; 19h:51m:45s remains)
INFO - root - 2017-12-05 08:28:29.953568: step 4690, loss = 1.71, batch loss = 1.65 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:39s remains)
INFO - root - 2017-12-05 08:28:32.130461: step 4700, loss = 1.62, batch loss = 1.57 (39.3 examples/sec; 0.203 sec/batch; 18h:31m:08s remains)
2017-12-05 08:28:32.424681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3690763 -2.89895 -2.3359313 -1.7374809 -1.1646726 -0.68069577 -0.31546617 -0.050686836 0.13591909 0.31442308 0.38863468 0.33573437 0.086867332 -0.30100775 -0.769459][-3.0076747 -2.5087957 -1.9451201 -1.3713124 -0.83811474 -0.39527297 -0.059030056 0.1960516 0.39130449 0.60362291 0.71902037 0.70961714 0.49166107 0.13984823 -0.31673479][-2.6580894 -2.1719019 -1.6271489 -1.0707543 -0.54648852 -0.10272074 0.23852921 0.49577188 0.69034719 0.89381552 1.008678 1.0095725 0.80849648 0.47053432 0.021845818][-2.4259057 -1.9829347 -1.4643114 -0.90688109 -0.34965563 0.15207863 0.54916477 0.83839464 1.0361748 1.2206588 1.311141 1.272028 1.0315537 0.62748241 0.12029123][-2.3929415 -2.0059712 -1.515234 -0.94424844 -0.33358002 0.25420427 0.73604012 1.0789318 1.2915349 1.4436908 1.475956 1.3543291 1.025166 0.52084494 -0.081431389][-2.6063304 -2.2678368 -1.7854185 -1.1778169 -0.49111819 0.19391727 0.76224041 1.1365023 1.3375316 1.4222841 1.3542089 1.1084862 0.65870905 0.0487895 -0.63739991][-3.0771806 -2.7877107 -2.3206315 -1.6862633 -0.93244743 -0.15799522 0.48549938 0.89541578 1.0924039 1.1219912 0.96398973 0.61079454 0.067861557 -0.61778116 -1.3492343][-3.5817971 -3.3287928 -2.895931 -2.273206 -1.5014131 -0.68297887 0.0013895035 0.42985868 0.60879087 0.578382 0.34784555 -0.0696826 -0.64560485 -1.308948 -1.9868886][-3.930346 -3.7106271 -3.3349605 -2.7855344 -2.08115 -1.3022413 -0.63407421 -0.20791578 -0.048940182 -0.13141584 -0.41808581 -0.85774422 -1.3991582 -1.9685707 -2.5106087][-4.0790982 -3.9240112 -3.6433189 -3.2033296 -2.6175926 -1.9432974 -1.3315434 -0.91015863 -0.74445724 -0.8486979 -1.1700361 -1.6157594 -2.11886 -2.6065221 -3.025408][-4.1464181 -4.0508938 -3.8671441 -3.5583751 -3.1130471 -2.57273 -2.0556598 -1.6666956 -1.5002956 -1.5939538 -1.9024849 -2.3074136 -2.7439945 -3.1398401 -3.448189][-4.1703181 -4.1163473 -4.0074115 -3.8177831 -3.5251245 -3.1335604 -2.7275889 -2.3955152 -2.2403419 -2.3053668 -2.5533409 -2.8867273 -3.2415085 -3.54227 -3.7620878][-4.1950693 -4.1445885 -4.0746422 -3.958714 -3.7845976 -3.5373914 -3.2485156 -2.9975286 -2.8636343 -2.8940802 -3.0608718 -3.3105743 -3.5783372 -3.8006206 -3.9612968][-4.2069573 -4.1589088 -4.1050315 -4.0219693 -3.9038453 -3.7550812 -3.5619 -3.384762 -3.2699339 -3.25689 -3.3517594 -3.5094993 -3.6838119 -3.8504176 -3.9750075][-4.1915045 -4.1546812 -4.1144881 -4.0612473 -3.9740658 -3.8649616 -3.7237549 -3.5900559 -3.4829731 -3.44649 -3.4871712 -3.5871718 -3.7082629 -3.8326852 -3.9359579]]...]
INFO - root - 2017-12-05 08:28:34.592992: step 4710, loss = 1.54, batch loss = 1.48 (37.0 examples/sec; 0.216 sec/batch; 19h:42m:21s remains)
INFO - root - 2017-12-05 08:28:36.745275: step 4720, loss = 1.86, batch loss = 1.81 (37.3 examples/sec; 0.215 sec/batch; 19h:32m:08s remains)
INFO - root - 2017-12-05 08:28:38.954151: step 4730, loss = 1.77, batch loss = 1.71 (35.9 examples/sec; 0.223 sec/batch; 20h:15m:40s remains)
INFO - root - 2017-12-05 08:28:41.143772: step 4740, loss = 1.53, batch loss = 1.47 (36.8 examples/sec; 0.217 sec/batch; 19h:47m:40s remains)
INFO - root - 2017-12-05 08:28:43.318983: step 4750, loss = 1.67, batch loss = 1.61 (36.4 examples/sec; 0.220 sec/batch; 20h:01m:22s remains)
INFO - root - 2017-12-05 08:28:45.539015: step 4760, loss = 1.91, batch loss = 1.85 (34.8 examples/sec; 0.230 sec/batch; 20h:56m:37s remains)
INFO - root - 2017-12-05 08:28:47.711122: step 4770, loss = 1.96, batch loss = 1.90 (37.0 examples/sec; 0.216 sec/batch; 19h:40m:02s remains)
INFO - root - 2017-12-05 08:28:49.918917: step 4780, loss = 1.62, batch loss = 1.56 (36.4 examples/sec; 0.220 sec/batch; 19h:59m:20s remains)
INFO - root - 2017-12-05 08:28:52.090768: step 4790, loss = 1.67, batch loss = 1.61 (36.4 examples/sec; 0.220 sec/batch; 20h:01m:27s remains)
INFO - root - 2017-12-05 08:28:54.307208: step 4800, loss = 1.22, batch loss = 1.16 (37.0 examples/sec; 0.216 sec/batch; 19h:41m:55s remains)
2017-12-05 08:28:54.603728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656][-4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656][-4.271656 -4.271656 -4.271656 -4.271656 -4.2716556 -4.2716556 -4.271656 -4.271657 -4.2716565 -4.271657 -4.2716565 -4.271656 -4.271656 -4.271656 -4.271656][-4.271656 -4.271656 -4.2716565 -4.271656 -4.2716575 -4.2716622 -4.2716627 -4.2716632 -4.2716618 -4.2716589 -4.2716579 -4.2716565 -4.271656 -4.271656 -4.271656][-4.271656 -4.2716565 -4.2716556 -4.271656 -4.27166 -4.2716713 -4.2716737 -4.27167 -4.2716665 -4.2716608 -4.2716608 -4.2716589 -4.271657 -4.271656 -4.271656][-4.271656 -4.271656 -4.2716551 -4.2716522 -4.2716489 -4.2716556 -4.2716608 -4.2716537 -4.2716537 -4.2716589 -4.2716641 -4.2716613 -4.2716584 -4.271656 -4.271656][-4.2716565 -4.2716565 -4.2716579 -4.27165 -4.2716269 -4.2716222 -4.2716212 -4.2716103 -4.27162 -4.27164 -4.2716565 -4.2716589 -4.2716579 -4.2716556 -4.271656][-4.271657 -4.2716589 -4.2716632 -4.2716513 -4.2716126 -4.2715859 -4.2715774 -4.2715716 -4.2715859 -4.2716126 -4.2716389 -4.2716517 -4.2716541 -4.2716546 -4.271656][-4.2716556 -4.2716551 -4.2716594 -4.2716503 -4.2716217 -4.2715969 -4.2715883 -4.2715816 -4.27159 -4.2716026 -4.2716227 -4.2716451 -4.2716522 -4.2716546 -4.271656][-4.2716556 -4.2716546 -4.2716527 -4.2716508 -4.271637 -4.2716389 -4.2716455 -4.2716331 -4.2716241 -4.2716131 -4.2716269 -4.2716489 -4.2716541 -4.2716551 -4.2716556][-4.271656 -4.2716556 -4.2716527 -4.2716513 -4.2716494 -4.271657 -4.2716684 -4.2716656 -4.2716489 -4.2716327 -4.27164 -4.2716517 -4.2716541 -4.2716556 -4.2716556][-4.271656 -4.2716556 -4.2716541 -4.2716532 -4.2716546 -4.2716594 -4.2716665 -4.2716651 -4.2716522 -4.2716441 -4.271647 -4.2716522 -4.2716551 -4.271656 -4.271656][-4.271656 -4.2716556 -4.2716551 -4.2716551 -4.2716556 -4.271657 -4.2716594 -4.2716589 -4.2716537 -4.2716508 -4.2716522 -4.2716546 -4.271656 -4.271656 -4.271656][-4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.2716565 -4.2716565 -4.2716551 -4.2716546 -4.2716551 -4.2716556 -4.271656 -4.271656 -4.271656][-4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656 -4.271656]]...]
INFO - root - 2017-12-05 08:28:56.773068: step 4810, loss = 1.94, batch loss = 1.88 (38.2 examples/sec; 0.209 sec/batch; 19h:02m:21s remains)
INFO - root - 2017-12-05 08:28:58.927528: step 4820, loss = 1.91, batch loss = 1.85 (38.2 examples/sec; 0.209 sec/batch; 19h:03m:57s remains)
INFO - root - 2017-12-05 08:29:01.087908: step 4830, loss = 1.75, batch loss = 1.69 (36.8 examples/sec; 0.217 sec/batch; 19h:46m:24s remains)
INFO - root - 2017-12-05 08:29:03.265326: step 4840, loss = 1.68, batch loss = 1.63 (36.6 examples/sec; 0.219 sec/batch; 19h:54m:31s remains)
INFO - root - 2017-12-05 08:29:05.450918: step 4850, loss = 1.87, batch loss = 1.81 (36.8 examples/sec; 0.217 sec/batch; 19h:45m:50s remains)
INFO - root - 2017-12-05 08:29:07.641558: step 4860, loss = 1.96, batch loss = 1.91 (35.5 examples/sec; 0.225 sec/batch; 20h:29m:57s remains)
INFO - root - 2017-12-05 08:29:09.794112: step 4870, loss = 1.87, batch loss = 1.81 (37.3 examples/sec; 0.215 sec/batch; 19h:31m:36s remains)
INFO - root - 2017-12-05 08:29:11.959887: step 4880, loss = 1.57, batch loss = 1.52 (36.4 examples/sec; 0.220 sec/batch; 20h:01m:28s remains)
INFO - root - 2017-12-05 08:29:14.149026: step 4890, loss = 1.81, batch loss = 1.75 (37.0 examples/sec; 0.216 sec/batch; 19h:39m:40s remains)
INFO - root - 2017-12-05 08:29:16.316183: step 4900, loss = 1.86, batch loss = 1.80 (36.2 examples/sec; 0.221 sec/batch; 20h:07m:18s remains)
2017-12-05 08:29:16.636666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631717 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631717][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631717 -4.2631717][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631717 -4.2631717][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721][-4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631721 -4.2631726 -4.2631721 -4.2631721 -4.2631726 -4.2631731 -4.2631726]]...]
INFO - root - 2017-12-05 08:29:18.811327: step 4910, loss = 2.03, batch loss = 1.97 (36.4 examples/sec; 0.219 sec/batch; 19h:58m:21s remains)
INFO - root - 2017-12-05 08:29:20.986680: step 4920, loss = 1.70, batch loss = 1.64 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:51s remains)
INFO - root - 2017-12-05 08:29:23.176197: step 4930, loss = 1.60, batch loss = 1.54 (37.9 examples/sec; 0.211 sec/batch; 19h:12m:01s remains)
INFO - root - 2017-12-05 08:29:25.377528: step 4940, loss = 1.77, batch loss = 1.71 (36.4 examples/sec; 0.220 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-05 08:29:27.542555: step 4950, loss = 2.05, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:53s remains)
INFO - root - 2017-12-05 08:29:29.725335: step 4960, loss = 1.76, batch loss = 1.70 (36.4 examples/sec; 0.220 sec/batch; 19h:59m:07s remains)
INFO - root - 2017-12-05 08:29:31.888786: step 4970, loss = 1.62, batch loss = 1.56 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:18s remains)
INFO - root - 2017-12-05 08:29:34.066393: step 4980, loss = 1.77, batch loss = 1.71 (36.6 examples/sec; 0.218 sec/batch; 19h:52m:05s remains)
INFO - root - 2017-12-05 08:29:36.233332: step 4990, loss = 1.73, batch loss = 1.67 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:09s remains)
INFO - root - 2017-12-05 08:29:38.423685: step 5000, loss = 2.08, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 20h:41m:14s remains)
2017-12-05 08:29:38.709510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017616749 -0.0030755997 0.12493181 0.29631662 0.45749187 0.61082077 0.66301489 0.55282354 0.37664413 0.087118149 -0.22131157 -0.66732073 -1.0068717 -1.3091171 -1.6427639][-0.010091305 -0.031228065 0.083173275 0.24914789 0.42069292 0.59059048 0.62222815 0.43690062 0.11551237 -0.29887176 -0.7447238 -1.2301462 -1.5530291 -1.755209 -1.9282365][0.049167156 -0.0048880577 0.085077286 0.25923395 0.44510221 0.62053061 0.61789417 0.3829689 -0.050589085 -0.5580318 -1.0849643 -1.5765162 -1.8514752 -1.9713273 -2.0095422][0.16028738 0.10819149 0.17206144 0.33582544 0.51243782 0.6643343 0.61981773 0.33921671 -0.15093946 -0.73183846 -1.3178742 -1.7859192 -2.0023477 -2.0656579 -2.0345016][0.31939888 0.282609 0.353096 0.49271202 0.6411624 0.77026415 0.69136333 0.37904358 -0.16334724 -0.76366282 -1.3787785 -1.834079 -2.0236247 -2.0878353 -2.0327394][0.46549559 0.47755766 0.57535458 0.68510962 0.80836058 0.92350721 0.83004236 0.50483131 -0.065383434 -0.67574716 -1.3124938 -1.7628591 -1.9821291 -2.0185196 -1.9453101][0.57789135 0.65885592 0.80474281 0.90611935 1.0135765 1.1083655 1.0045934 0.68128061 0.10570002 -0.49296331 -1.1331315 -1.5968182 -1.8294935 -1.8305235 -1.6962636][0.60404062 0.78284025 1.0002751 1.1312671 1.2406564 1.3318706 1.2357917 0.9158287 0.34895086 -0.25271463 -0.87435508 -1.3409386 -1.6088116 -1.5936913 -1.4575288][0.43984842 0.73808384 1.0556822 1.2353191 1.363483 1.4776344 1.4030237 1.0999937 0.616642 0.073847771 -0.49105096 -0.92397141 -1.1563554 -1.0806432 -0.94341445][0.067128658 0.45603943 0.87059355 1.1126685 1.2844291 1.4153433 1.3526783 1.1017427 0.70211267 0.2890625 -0.12432051 -0.42904997 -0.56624317 -0.40875721 -0.22743273][-0.43077016 -0.014640331 0.4336791 0.73316383 0.96848822 1.1273966 1.1001911 0.91115189 0.63823795 0.39564419 0.16438818 0.061971188 0.10526133 0.38288832 0.62829542][-0.93387508 -0.533622 -0.093696594 0.24094057 0.53753662 0.74313784 0.77624655 0.71027136 0.59352446 0.52383947 0.47441244 0.56035089 0.7412591 1.0685053 1.3443499][-1.4510455 -1.0850277 -0.65912271 -0.29183745 0.06859827 0.37416744 0.50342035 0.56580591 0.59050512 0.67820358 0.778049 0.97369957 1.240304 1.5692348 1.8492289][-1.8838773 -1.5745349 -1.1806724 -0.77578139 -0.35094166 0.036126137 0.28582668 0.50630569 0.66996 0.85009718 1.0203776 1.2425394 1.5165057 1.8093591 2.0739446][-2.2258134 -1.9841697 -1.6185157 -1.1799352 -0.69026256 -0.21699715 0.14663649 0.50234365 0.76356888 1.0026321 1.2185655 1.4139109 1.6138253 1.79076 1.9611311]]...]
INFO - root - 2017-12-05 08:29:40.862225: step 5010, loss = 1.78, batch loss = 1.72 (38.0 examples/sec; 0.211 sec/batch; 19h:10m:18s remains)
INFO - root - 2017-12-05 08:29:43.037708: step 5020, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:30m:58s remains)
INFO - root - 2017-12-05 08:29:45.191166: step 5030, loss = 1.83, batch loss = 1.78 (38.4 examples/sec; 0.208 sec/batch; 18h:57m:47s remains)
INFO - root - 2017-12-05 08:29:47.363862: step 5040, loss = 1.80, batch loss = 1.75 (37.5 examples/sec; 0.213 sec/batch; 19h:24m:43s remains)
INFO - root - 2017-12-05 08:29:49.510184: step 5050, loss = 1.42, batch loss = 1.36 (37.7 examples/sec; 0.212 sec/batch; 19h:18m:42s remains)
INFO - root - 2017-12-05 08:29:51.681027: step 5060, loss = 2.02, batch loss = 1.96 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:08s remains)
INFO - root - 2017-12-05 08:29:53.849864: step 5070, loss = 1.62, batch loss = 1.57 (37.1 examples/sec; 0.215 sec/batch; 19h:35m:12s remains)
INFO - root - 2017-12-05 08:29:56.039215: step 5080, loss = 1.55, batch loss = 1.49 (34.7 examples/sec; 0.230 sec/batch; 20h:57m:10s remains)
INFO - root - 2017-12-05 08:29:58.204414: step 5090, loss = 1.63, batch loss = 1.57 (36.7 examples/sec; 0.218 sec/batch; 19h:48m:53s remains)
INFO - root - 2017-12-05 08:30:00.390015: step 5100, loss = 1.72, batch loss = 1.67 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:34s remains)
2017-12-05 08:30:00.675991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6655619 -3.2709885 -2.6845274 -1.9793222 -1.2669029 -0.64916039 -0.22846699 -0.037124634 -0.14319181 -0.51381183 -1.0735223 -1.6512034 -2.209734 -2.6341105 -2.9522898][-3.5804281 -3.2109532 -2.6117027 -1.8609421 -1.0705907 -0.34377718 0.18930292 0.47860718 0.3842802 -0.028961182 -0.6666379 -1.3250854 -1.9648676 -2.475934 -2.8796942][-3.476836 -3.1586051 -2.5783036 -1.8169553 -0.96804714 -0.13967609 0.49216366 0.85800362 0.81875849 0.41967726 -0.28355932 -1.0397635 -1.806304 -2.4178467 -2.8961194][-3.40976 -3.1900363 -2.6722074 -1.9331732 -1.0320137 -0.09169054 0.68491173 1.1749482 1.181613 0.77931786 0.045981407 -0.77836633 -1.6639614 -2.3701191 -2.9358039][-3.3466773 -3.2747226 -2.8614712 -2.1699705 -1.233686 -0.18455172 0.749207 1.3624802 1.4295211 1.016809 0.25531578 -0.6111331 -1.5491416 -2.2878718 -2.9179525][-3.2889881 -3.3197014 -2.9908485 -2.3585877 -1.4086392 -0.28956223 0.73346186 1.4133554 1.492682 1.0823598 0.3060298 -0.58998036 -1.5161669 -2.2395451 -2.8643985][-3.2068014 -3.3197551 -3.0565996 -2.461031 -1.5057557 -0.3587532 0.6990304 1.3803496 1.4374957 0.98833275 0.18659067 -0.69135284 -1.5694773 -2.2509725 -2.8159697][-3.1112814 -3.26337 -3.0396464 -2.4633429 -1.5338924 -0.39059615 0.67057705 1.3132243 1.2982693 0.76822805 -0.0646925 -0.918993 -1.7312591 -2.3611994 -2.8145227][-3.0083809 -3.180871 -2.9668469 -2.39427 -1.4879839 -0.41087723 0.58503008 1.1615181 1.0822458 0.49498129 -0.36287308 -1.198349 -1.936096 -2.4813292 -2.8261733][-2.8931129 -3.0261827 -2.7994635 -2.2458987 -1.3965385 -0.42256522 0.4365654 0.88647127 0.74673319 0.1587429 -0.65549588 -1.4233646 -2.0624225 -2.5305486 -2.7855916][-2.7413158 -2.788609 -2.5369544 -1.9925098 -1.2285836 -0.40999508 0.27772427 0.59264469 0.38315392 -0.20088434 -0.93877149 -1.581166 -2.0958247 -2.5033436 -2.6809433][-2.5212512 -2.4668541 -2.1725354 -1.6548553 -1.0133977 -0.36165166 0.15033531 0.35069036 0.12121534 -0.42977 -1.0732522 -1.6183236 -2.0376008 -2.3877003 -2.5326152][-2.2339988 -2.0814481 -1.761559 -1.2907443 -0.77826905 -0.29886937 0.041136265 0.13706446 -0.11171389 -0.57872796 -1.1194935 -1.5903273 -1.9492204 -2.2282307 -2.3667102][-1.9393587 -1.6893728 -1.3538749 -0.95806193 -0.58595824 -0.29209995 -0.12130308 -0.13658905 -0.3842082 -0.73079515 -1.1397636 -1.5034282 -1.8412313 -2.1017988 -2.26105][-1.5823309 -1.2538371 -0.9176712 -0.62384439 -0.43999195 -0.35187578 -0.38568091 -0.50200844 -0.72160625 -0.94799614 -1.1991537 -1.4473031 -1.7243514 -1.9722333 -2.1615934]]...]
INFO - root - 2017-12-05 08:30:02.825263: step 5110, loss = 1.76, batch loss = 1.70 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:30s remains)
INFO - root - 2017-12-05 08:30:04.979796: step 5120, loss = 1.70, batch loss = 1.65 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:49s remains)
INFO - root - 2017-12-05 08:30:07.188540: step 5130, loss = 1.96, batch loss = 1.90 (36.5 examples/sec; 0.219 sec/batch; 19h:55m:55s remains)
INFO - root - 2017-12-05 08:30:09.392111: step 5140, loss = 1.82, batch loss = 1.76 (37.3 examples/sec; 0.215 sec/batch; 19h:30m:58s remains)
INFO - root - 2017-12-05 08:30:11.583765: step 5150, loss = 1.69, batch loss = 1.63 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:06s remains)
INFO - root - 2017-12-05 08:30:13.764952: step 5160, loss = 1.77, batch loss = 1.71 (36.9 examples/sec; 0.217 sec/batch; 19h:41m:37s remains)
INFO - root - 2017-12-05 08:30:15.936188: step 5170, loss = 1.93, batch loss = 1.87 (36.5 examples/sec; 0.219 sec/batch; 19h:54m:16s remains)
INFO - root - 2017-12-05 08:30:18.115505: step 5180, loss = 1.72, batch loss = 1.66 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:02s remains)
INFO - root - 2017-12-05 08:30:20.290736: step 5190, loss = 1.75, batch loss = 1.70 (38.6 examples/sec; 0.207 sec/batch; 18h:49m:14s remains)
INFO - root - 2017-12-05 08:30:22.469975: step 5200, loss = 1.73, batch loss = 1.67 (36.0 examples/sec; 0.222 sec/batch; 20h:10m:35s remains)
2017-12-05 08:30:22.765057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2472878 -4.2472425 -4.2468705 -4.2451415 -4.240416 -4.2283721 -4.191494 -4.0903788 -3.9212842 -3.7701285 -3.7058673 -3.7526979 -3.8584609 -3.9521415 -4.0149875][-4.2472353 -4.2471619 -4.24672 -4.2443552 -4.2368655 -4.2176795 -4.1681252 -4.0552588 -3.8913422 -3.7499988 -3.6953039 -3.7512746 -3.8617024 -3.9696584 -4.0353756][-4.2471809 -4.2470837 -4.2465291 -4.2428412 -4.2290955 -4.1924672 -4.1112852 -3.9650881 -3.7865937 -3.6515894 -3.622509 -3.7032213 -3.8347206 -3.9564869 -4.0201921][-4.2471209 -4.2469964 -4.246213 -4.2405615 -4.2169271 -4.1541276 -4.0272818 -3.8320093 -3.6298051 -3.5025592 -3.5031943 -3.6165748 -3.7749274 -3.9180088 -3.9886112][-4.247118 -4.2469482 -4.2455893 -4.2330213 -4.1898637 -4.0888009 -3.9066231 -3.6625562 -3.448498 -3.3443577 -3.3875549 -3.5372088 -3.7165337 -3.8692222 -3.9420955][-4.2471595 -4.2468867 -4.2431931 -4.216013 -4.1415315 -3.98816 -3.7460561 -3.4642038 -3.2579923 -3.2003825 -3.3040919 -3.5030591 -3.7091441 -3.8654392 -3.9336331][-4.2471414 -4.2465253 -4.2388787 -4.1922865 -4.078763 -3.8683133 -3.5757229 -3.2808294 -3.101151 -3.1000223 -3.2650075 -3.5093751 -3.7353687 -3.8900335 -3.9512856][-4.2471266 -4.2458692 -4.2328992 -4.1649551 -4.0166879 -3.7636254 -3.4481781 -3.1692662 -3.0290594 -3.0887027 -3.3101954 -3.58459 -3.8114622 -3.945724 -3.9894552][-4.2470207 -4.2446218 -4.2249084 -4.1376829 -3.9632652 -3.6973293 -3.3975365 -3.16257 -3.0795732 -3.1902146 -3.4422112 -3.7153342 -3.9145904 -4.0184617 -4.046639][-4.2466025 -4.2420321 -4.2140503 -4.1118946 -3.925642 -3.672328 -3.4229388 -3.2534361 -3.2294242 -3.3747077 -3.6249425 -3.8676922 -4.0239835 -4.0931883 -4.1064005][-4.2456636 -4.2377429 -4.2007623 -4.0925393 -3.913527 -3.6918762 -3.5070992 -3.4131949 -3.4325206 -3.5794873 -3.7961788 -3.99019 -4.1063213 -4.1535568 -4.160531][-4.2445045 -4.2340264 -4.191196 -4.0841527 -3.926476 -3.7506895 -3.6240797 -3.5893576 -3.6299005 -3.751024 -3.9182708 -4.0652742 -4.152266 -4.1897693 -4.1972108][-4.2435336 -4.2318211 -4.1868486 -4.0855989 -3.9525373 -3.8252387 -3.7542746 -3.7522149 -3.7961805 -3.8854096 -4.000905 -4.1087618 -4.1770363 -4.2099934 -4.2197213][-4.242238 -4.2299871 -4.1872354 -4.0969887 -3.9883764 -3.9026642 -3.8749466 -3.8938239 -3.9227824 -3.9776804 -4.0525808 -4.1266551 -4.1830988 -4.2157288 -4.229847][-4.2407322 -4.2288194 -4.1916351 -4.1173663 -4.0303674 -3.970505 -3.965199 -3.9926667 -4.0097747 -4.0333781 -4.0770159 -4.1282754 -4.1756692 -4.2097926 -4.2293696]]...]
INFO - root - 2017-12-05 08:30:24.924600: step 5210, loss = 1.74, batch loss = 1.68 (36.5 examples/sec; 0.219 sec/batch; 19h:56m:15s remains)
INFO - root - 2017-12-05 08:30:27.092831: step 5220, loss = 1.77, batch loss = 1.71 (36.8 examples/sec; 0.217 sec/batch; 19h:45m:10s remains)
INFO - root - 2017-12-05 08:30:29.250169: step 5230, loss = 1.77, batch loss = 1.71 (38.0 examples/sec; 0.211 sec/batch; 19h:09m:39s remains)
INFO - root - 2017-12-05 08:30:31.428126: step 5240, loss = 1.89, batch loss = 1.83 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:21s remains)
INFO - root - 2017-12-05 08:30:33.579217: step 5250, loss = 1.71, batch loss = 1.65 (37.1 examples/sec; 0.216 sec/batch; 19h:36m:10s remains)
INFO - root - 2017-12-05 08:30:35.736173: step 5260, loss = 1.99, batch loss = 1.93 (36.6 examples/sec; 0.219 sec/batch; 19h:52m:38s remains)
INFO - root - 2017-12-05 08:30:37.926646: step 5270, loss = 1.79, batch loss = 1.73 (36.6 examples/sec; 0.219 sec/batch; 19h:51m:57s remains)
INFO - root - 2017-12-05 08:30:40.097469: step 5280, loss = 1.87, batch loss = 1.81 (37.5 examples/sec; 0.213 sec/batch; 19h:23m:25s remains)
INFO - root - 2017-12-05 08:30:42.295054: step 5290, loss = 1.86, batch loss = 1.80 (36.4 examples/sec; 0.220 sec/batch; 19h:59m:02s remains)
INFO - root - 2017-12-05 08:30:44.486676: step 5300, loss = 1.88, batch loss = 1.82 (35.9 examples/sec; 0.223 sec/batch; 20h:16m:38s remains)
2017-12-05 08:30:44.760120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411866 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411876 -4.2411871 -4.2411861 -4.2411861 -4.2411866 -4.2411876 -4.2411871 -4.2411866 -4.2411871 -4.2411871 -4.2411866][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411876 -4.2411871 -4.2411852 -4.2411833 -4.2411857 -4.2411876 -4.2411871 -4.2411866 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411866 -4.2411823 -4.241179 -4.2411823 -4.2411861 -4.2411866 -4.2411861 -4.2411866 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411866 -4.2411871 -4.2411871 -4.2411866 -4.2411823 -4.241179 -4.2411823 -4.2411857 -4.2411857 -4.2411857 -4.2411866 -4.2411876 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411852 -4.2411842 -4.2411857 -4.2411861 -4.2411861 -4.2411866 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411876 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871][-4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871 -4.2411871]]...]
INFO - root - 2017-12-05 08:30:46.904423: step 5310, loss = 1.85, batch loss = 1.79 (36.9 examples/sec; 0.217 sec/batch; 19h:41m:10s remains)
INFO - root - 2017-12-05 08:30:49.095356: step 5320, loss = 1.98, batch loss = 1.92 (34.8 examples/sec; 0.230 sec/batch; 20h:54m:16s remains)
INFO - root - 2017-12-05 08:30:51.261507: step 5330, loss = 1.71, batch loss = 1.65 (37.0 examples/sec; 0.216 sec/batch; 19h:40m:29s remains)
INFO - root - 2017-12-05 08:30:53.433653: step 5340, loss = 2.01, batch loss = 1.95 (37.9 examples/sec; 0.211 sec/batch; 19h:09m:43s remains)
INFO - root - 2017-12-05 08:30:55.606702: step 5350, loss = 2.01, batch loss = 1.96 (36.3 examples/sec; 0.220 sec/batch; 20h:01m:21s remains)
INFO - root - 2017-12-05 08:30:57.758933: step 5360, loss = 1.87, batch loss = 1.81 (39.9 examples/sec; 0.200 sec/batch; 18h:12m:34s remains)
INFO - root - 2017-12-05 08:30:59.968785: step 5370, loss = 1.73, batch loss = 1.67 (35.1 examples/sec; 0.228 sec/batch; 20h:44m:20s remains)
INFO - root - 2017-12-05 08:31:02.133983: step 5380, loss = 2.02, batch loss = 1.96 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:01s remains)
INFO - root - 2017-12-05 08:31:04.313040: step 5390, loss = 1.82, batch loss = 1.76 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:37s remains)
INFO - root - 2017-12-05 08:31:06.481593: step 5400, loss = 1.63, batch loss = 1.57 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:14s remains)
2017-12-05 08:31:06.777403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2369452 -4.2366295 -4.2358661 -4.2341957 -4.2316694 -4.229238 -4.2277374 -4.2284279 -4.2310629 -4.23369 -4.2349625 -4.2352228 -4.2348962 -4.2348046 -4.2350349][-4.2367754 -4.2354851 -4.2312317 -4.2214365 -4.2058 -4.1890564 -4.1790185 -4.1823506 -4.1966715 -4.2128091 -4.2235708 -4.2285037 -4.2300634 -4.2309437 -4.2319951][-4.2364368 -4.2325258 -4.218822 -4.1860628 -4.132102 -4.0724874 -4.0338268 -4.0382304 -4.0813079 -4.13582 -4.1778851 -4.2021875 -4.2145061 -4.2210121 -4.2253027][-4.23529 -4.2257586 -4.1939659 -4.1179886 -3.9935305 -3.8545456 -3.7579699 -3.7533035 -3.8377867 -3.9578781 -4.0630388 -4.1332369 -4.1739807 -4.1948233 -4.2055659][-4.2321362 -4.2129159 -4.154274 -4.01974 -3.804455 -3.56189 -3.3833323 -3.3535576 -3.4783006 -3.6796658 -3.8749349 -4.0191627 -4.1078734 -4.1514025 -4.1689572][-4.2265973 -4.1940246 -4.10453 -3.9113336 -3.6105993 -3.2704916 -3.0095258 -2.9447095 -3.0969491 -3.3744817 -3.664881 -3.8938229 -4.0394554 -4.1100841 -4.1330848][-4.2200255 -4.1738605 -4.0580368 -3.8246522 -3.4728751 -3.0780401 -2.769609 -2.6762791 -2.8315191 -3.14924 -3.5030723 -3.7965474 -3.9896486 -4.0849919 -4.1139073][-4.2166057 -4.1638942 -4.0377121 -3.7976873 -3.4481039 -3.0633333 -2.7639341 -2.6622505 -2.7933106 -3.0928059 -3.4475422 -3.7579048 -3.9707835 -4.0810666 -4.1169028][-4.2190223 -4.17235 -4.0598702 -3.8512206 -3.5570025 -3.2424827 -3.0019789 -2.915729 -3.0082421 -3.238915 -3.5282626 -3.7967775 -3.9917827 -4.1006894 -4.1412196][-4.2256303 -4.1939578 -4.113018 -3.96074 -3.7502413 -3.5325656 -3.3710055 -3.3126843 -3.3682606 -3.5135796 -3.7046778 -3.8939817 -4.0426283 -4.1340117 -4.173831][-4.2315006 -4.214746 -4.1674466 -4.0736909 -3.9434383 -3.8125117 -3.7195024 -3.6879919 -3.7184162 -3.7953281 -3.8988924 -4.0095129 -4.105392 -4.1703053 -4.2018213][-4.2347488 -4.227078 -4.2035527 -4.1530657 -4.0798349 -4.006556 -3.956948 -3.9434285 -3.9630146 -4.0039029 -4.05558 -4.1112456 -4.1623006 -4.1994128 -4.2193565][-4.2357745 -4.2314835 -4.2190189 -4.1912866 -4.14812 -4.1034665 -4.0746603 -4.0699196 -4.0874205 -4.1173658 -4.1494927 -4.17834 -4.2014432 -4.2179976 -4.22772][-4.236176 -4.2327018 -4.2228961 -4.2026348 -4.1718884 -4.1399612 -4.119431 -4.1182938 -4.1363726 -4.1643028 -4.1913385 -4.2106233 -4.2219844 -4.2286735 -4.2324648][-4.2366943 -4.2342076 -4.2257414 -4.2081671 -4.1823926 -4.1565995 -4.1411381 -4.1424222 -4.1603808 -4.1861095 -4.2094803 -4.2242107 -4.2311511 -4.2341576 -4.235405]]...]
INFO - root - 2017-12-05 08:31:08.966521: step 5410, loss = 1.86, batch loss = 1.81 (36.0 examples/sec; 0.222 sec/batch; 20h:11m:30s remains)
INFO - root - 2017-12-05 08:31:11.162965: step 5420, loss = 2.11, batch loss = 2.05 (35.3 examples/sec; 0.227 sec/batch; 20h:35m:31s remains)
INFO - root - 2017-12-05 08:31:13.330156: step 5430, loss = 1.90, batch loss = 1.84 (36.3 examples/sec; 0.220 sec/batch; 20h:00m:08s remains)
INFO - root - 2017-12-05 08:31:15.531043: step 5440, loss = 1.70, batch loss = 1.64 (37.3 examples/sec; 0.214 sec/batch; 19h:28m:47s remains)
INFO - root - 2017-12-05 08:31:17.709537: step 5450, loss = 2.13, batch loss = 2.07 (37.0 examples/sec; 0.216 sec/batch; 19h:38m:16s remains)
INFO - root - 2017-12-05 08:31:19.844602: step 5460, loss = 1.92, batch loss = 1.86 (37.7 examples/sec; 0.212 sec/batch; 19h:17m:14s remains)
INFO - root - 2017-12-05 08:31:22.034126: step 5470, loss = 1.56, batch loss = 1.51 (35.4 examples/sec; 0.226 sec/batch; 20h:31m:40s remains)
INFO - root - 2017-12-05 08:31:24.202657: step 5480, loss = 1.78, batch loss = 1.72 (37.1 examples/sec; 0.216 sec/batch; 19h:36m:32s remains)
INFO - root - 2017-12-05 08:31:26.373979: step 5490, loss = 1.92, batch loss = 1.87 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:50s remains)
INFO - root - 2017-12-05 08:31:28.540140: step 5500, loss = 1.60, batch loss = 1.54 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:39s remains)
2017-12-05 08:31:28.840151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6879196 -2.4189599 -2.1506963 -1.9781859 -1.9445102 -1.9979737 -2.1724272 -2.4115744 -2.7338536 -3.05949 -3.3693335 -3.6170888 -3.7444105 -3.7498169 -3.7186849][-2.2772923 -1.9106939 -1.5346851 -1.296247 -1.2234828 -1.2721641 -1.4991343 -1.8187475 -2.2496247 -2.655719 -3.0190945 -3.2832339 -3.4154248 -3.4058268 -3.3488159][-1.929873 -1.4603028 -0.99731135 -0.69570804 -0.58319092 -0.64869308 -0.92704988 -1.3244932 -1.8306279 -2.2658043 -2.6203179 -2.8335114 -2.9178932 -2.8819437 -2.8087716][-1.6137917 -1.0209959 -0.47579098 -0.14929914 -0.037257671 -0.12970495 -0.46768856 -0.94478059 -1.5015333 -1.9286973 -2.2169976 -2.3144989 -2.3066645 -2.229481 -2.1501119][-1.2149038 -0.52476168 0.058433056 0.36561537 0.4323287 0.28530359 -0.12290621 -0.65552831 -1.2557242 -1.6776304 -1.9129667 -1.8936729 -1.7703054 -1.631062 -1.5534847][-0.67736077 0.070391655 0.61978388 0.84642076 0.81610012 0.56652164 0.072952271 -0.50918889 -1.1228392 -1.5363646 -1.7582655 -1.6916733 -1.5044537 -1.3071573 -1.2087562][-0.1146903 0.65462017 1.12255 1.2461004 1.107214 0.74222755 0.18122196 -0.43212414 -1.0449867 -1.4585516 -1.7065005 -1.6874418 -1.5268431 -1.3157535 -1.2154782][0.26799011 1.0012589 1.3346105 1.3558478 1.1373544 0.70404196 0.10041809 -0.4996078 -1.0487192 -1.4377661 -1.7474859 -1.8258419 -1.7469707 -1.5762472 -1.4932191][0.3268075 0.95027161 1.1053009 1.0156789 0.77589273 0.35510969 -0.2614696 -0.83276129 -1.300374 -1.6413491 -1.9892251 -2.190093 -2.2206225 -2.1005597 -2.0271912][0.0831399 0.55365992 0.51061058 0.36126947 0.12995911 -0.24666882 -0.83181357 -1.3780472 -1.7806759 -2.0828941 -2.443944 -2.7303822 -2.862258 -2.8011823 -2.7288883][-0.456244 -0.10171986 -0.25889659 -0.39362407 -0.540009 -0.83205652 -1.3724947 -1.8830221 -2.2138324 -2.509341 -2.8900986 -3.2073271 -3.3986349 -3.397197 -3.3517408][-1.2280967 -0.95946407 -1.1181612 -1.1734669 -1.1870866 -1.3375659 -1.7977135 -2.276329 -2.5611768 -2.8297987 -3.1786373 -3.4762967 -3.6855378 -3.7261972 -3.7325423][-2.0968051 -1.9083648 -1.9943883 -1.9710205 -1.8869891 -1.9279146 -2.2646108 -2.6602921 -2.8985839 -3.1021824 -3.3209605 -3.5479193 -3.7360609 -3.7954559 -3.8674738][-2.8237398 -2.6874843 -2.7051611 -2.6225634 -2.4880824 -2.4424617 -2.6425447 -2.9135761 -3.0847964 -3.2489407 -3.3443947 -3.5124545 -3.6668186 -3.7358105 -3.8795905][-3.3142574 -3.2131724 -3.1742711 -3.0745831 -2.9542584 -2.8799205 -2.9518113 -3.0503931 -3.0949144 -3.1866488 -3.2429647 -3.4021275 -3.5400305 -3.6103687 -3.8088109]]...]
INFO - root - 2017-12-05 08:31:31.003255: step 5510, loss = 1.81, batch loss = 1.75 (36.1 examples/sec; 0.221 sec/batch; 20h:06m:31s remains)
INFO - root - 2017-12-05 08:31:33.176129: step 5520, loss = 1.78, batch loss = 1.73 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:07s remains)
INFO - root - 2017-12-05 08:31:35.359482: step 5530, loss = 1.83, batch loss = 1.77 (37.3 examples/sec; 0.215 sec/batch; 19h:30m:01s remains)
INFO - root - 2017-12-05 08:31:37.538939: step 5540, loss = 2.01, batch loss = 1.95 (36.6 examples/sec; 0.219 sec/batch; 19h:52m:03s remains)
INFO - root - 2017-12-05 08:31:39.725867: step 5550, loss = 1.63, batch loss = 1.57 (37.5 examples/sec; 0.213 sec/batch; 19h:21m:34s remains)
INFO - root - 2017-12-05 08:31:41.885296: step 5560, loss = 1.87, batch loss = 1.81 (37.0 examples/sec; 0.216 sec/batch; 19h:38m:36s remains)
INFO - root - 2017-12-05 08:31:44.055988: step 5570, loss = 1.89, batch loss = 1.83 (37.3 examples/sec; 0.214 sec/batch; 19h:27m:22s remains)
INFO - root - 2017-12-05 08:31:46.256609: step 5580, loss = 1.99, batch loss = 1.93 (37.4 examples/sec; 0.214 sec/batch; 19h:26m:03s remains)
INFO - root - 2017-12-05 08:31:48.450510: step 5590, loss = 1.51, batch loss = 1.46 (37.3 examples/sec; 0.215 sec/batch; 19h:29m:22s remains)
INFO - root - 2017-12-05 08:31:50.645671: step 5600, loss = 1.82, batch loss = 1.76 (37.4 examples/sec; 0.214 sec/batch; 19h:25m:41s remains)
2017-12-05 08:31:50.920627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304554 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304554][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304554 -4.2304554 -4.2304554][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304554][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304554 -4.2304559 -4.2304554 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304544 -4.230454 -4.2304554 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304554 -4.2304554 -4.2304535 -4.2304544 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.230454 -4.2304549 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304554 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559][-4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304564 -4.2304559 -4.2304559 -4.2304559 -4.2304559 -4.2304559]]...]
INFO - root - 2017-12-05 08:31:53.105821: step 5610, loss = 1.80, batch loss = 1.75 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:24s remains)
INFO - root - 2017-12-05 08:31:55.317373: step 5620, loss = 1.83, batch loss = 1.78 (36.3 examples/sec; 0.220 sec/batch; 19h:59m:30s remains)
INFO - root - 2017-12-05 08:31:57.480890: step 5630, loss = 1.94, batch loss = 1.88 (38.8 examples/sec; 0.206 sec/batch; 18h:43m:00s remains)
INFO - root - 2017-12-05 08:31:59.664945: step 5640, loss = 1.67, batch loss = 1.61 (38.0 examples/sec; 0.210 sec/batch; 19h:05m:59s remains)
INFO - root - 2017-12-05 08:32:01.823556: step 5650, loss = 1.71, batch loss = 1.65 (37.4 examples/sec; 0.214 sec/batch; 19h:25m:52s remains)
INFO - root - 2017-12-05 08:32:03.970200: step 5660, loss = 1.65, batch loss = 1.60 (38.3 examples/sec; 0.209 sec/batch; 18h:58m:02s remains)
INFO - root - 2017-12-05 08:32:06.154675: step 5670, loss = 2.05, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:54s remains)
INFO - root - 2017-12-05 08:32:08.372333: step 5680, loss = 1.92, batch loss = 1.86 (36.9 examples/sec; 0.217 sec/batch; 19h:40m:08s remains)
INFO - root - 2017-12-05 08:32:10.556304: step 5690, loss = 1.85, batch loss = 1.80 (36.6 examples/sec; 0.218 sec/batch; 19h:49m:22s remains)
INFO - root - 2017-12-05 08:32:12.742028: step 5700, loss = 1.83, batch loss = 1.77 (37.1 examples/sec; 0.215 sec/batch; 19h:33m:16s remains)
2017-12-05 08:32:13.012790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242][-4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242 -4.2264242]]...]
INFO - root - 2017-12-05 08:32:15.164342: step 5710, loss = 1.57, batch loss = 1.52 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:49s remains)
INFO - root - 2017-12-05 08:32:17.338551: step 5720, loss = 1.69, batch loss = 1.63 (37.7 examples/sec; 0.212 sec/batch; 19h:16m:48s remains)
INFO - root - 2017-12-05 08:32:19.506123: step 5730, loss = 1.73, batch loss = 1.68 (37.3 examples/sec; 0.214 sec/batch; 19h:27m:43s remains)
INFO - root - 2017-12-05 08:32:21.718893: step 5740, loss = 1.98, batch loss = 1.92 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:29s remains)
INFO - root - 2017-12-05 08:32:23.869137: step 5750, loss = 1.73, batch loss = 1.67 (36.7 examples/sec; 0.218 sec/batch; 19h:45m:34s remains)
INFO - root - 2017-12-05 08:32:26.027536: step 5760, loss = 1.84, batch loss = 1.78 (36.8 examples/sec; 0.217 sec/batch; 19h:43m:02s remains)
INFO - root - 2017-12-05 08:32:28.183888: step 5770, loss = 2.00, batch loss = 1.95 (39.3 examples/sec; 0.204 sec/batch; 18h:29m:43s remains)
INFO - root - 2017-12-05 08:32:30.330841: step 5780, loss = 1.66, batch loss = 1.60 (37.1 examples/sec; 0.216 sec/batch; 19h:34m:37s remains)
INFO - root - 2017-12-05 08:32:32.502530: step 5790, loss = 1.81, batch loss = 1.75 (36.7 examples/sec; 0.218 sec/batch; 19h:46m:05s remains)
INFO - root - 2017-12-05 08:32:34.667070: step 5800, loss = 1.55, batch loss = 1.49 (37.2 examples/sec; 0.215 sec/batch; 19h:30m:58s remains)
2017-12-05 08:32:34.965793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207][-4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238212 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238212 -4.2238212 -4.2238207][-4.22382 -4.2238212 -4.2238207 -4.2238212 -4.2238212 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.22382 -4.22382 -4.22382 -4.2238207 -4.2238207 -4.2238207][-4.2238188 -4.22382 -4.2238212 -4.2238212 -4.2238207 -4.2238197 -4.2238207 -4.2238207 -4.22382 -4.2238193 -4.2238193 -4.2238193 -4.2238197 -4.2238207 -4.2238212][-4.2238178 -4.2238188 -4.2238207 -4.2238207 -4.22382 -4.2238183 -4.2238188 -4.2238193 -4.2238188 -4.2238188 -4.2238193 -4.2238197 -4.2238193 -4.22382 -4.2238207][-4.2238183 -4.2238193 -4.2238197 -4.22382 -4.2238188 -4.2238183 -4.2238193 -4.2238178 -4.2238164 -4.2238173 -4.2238183 -4.2238188 -4.22382 -4.22382 -4.22382][-4.2238197 -4.22382 -4.22382 -4.22382 -4.2238183 -4.2238164 -4.2238169 -4.2238193 -4.2238173 -4.2238173 -4.2238159 -4.2238173 -4.2238188 -4.22382 -4.22382][-4.2238207 -4.2238221 -4.22382 -4.2238197 -4.2238197 -4.2238193 -4.2238207 -4.2238226 -4.2238207 -4.2238169 -4.2238188 -4.2238188 -4.2238193 -4.2238207 -4.2238207][-4.2238207 -4.2238216 -4.22382 -4.22382 -4.2238207 -4.22382 -4.2238212 -4.2238216 -4.2238197 -4.2238173 -4.2238193 -4.2238207 -4.22382 -4.2238207 -4.2238207][-4.2238207 -4.2238212 -4.2238212 -4.22382 -4.2238193 -4.2238197 -4.2238197 -4.2238212 -4.22382 -4.22382 -4.22382 -4.2238212 -4.2238197 -4.2238207 -4.2238207][-4.2238169 -4.2238188 -4.2238207 -4.22382 -4.2238193 -4.2238197 -4.2238207 -4.2238212 -4.2238207 -4.2238207 -4.2238212 -4.2238212 -4.22382 -4.2238212 -4.2238212][-4.2238183 -4.2238183 -4.2238193 -4.2238197 -4.2238207 -4.2238197 -4.22382 -4.22382 -4.2238207 -4.2238207 -4.2238212 -4.2238207 -4.2238212 -4.2238212 -4.2238212][-4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238212 -4.2238207 -4.22382 -4.22382 -4.22382 -4.22382 -4.2238207 -4.2238207 -4.2238207 -4.2238207][-4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.22382 -4.22382 -4.22382 -4.2238207 -4.2238207 -4.2238207][-4.2238207 -4.2238207 -4.22382 -4.22382 -4.2238207 -4.2238207 -4.22382 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207 -4.2238207]]...]
INFO - root - 2017-12-05 08:32:37.161144: step 5810, loss = 1.55, batch loss = 1.49 (35.4 examples/sec; 0.226 sec/batch; 20h:28m:48s remains)
INFO - root - 2017-12-05 08:32:39.323290: step 5820, loss = 1.57, batch loss = 1.51 (36.8 examples/sec; 0.217 sec/batch; 19h:43m:09s remains)
INFO - root - 2017-12-05 08:32:41.500922: step 5830, loss = 2.05, batch loss = 1.99 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:53s remains)
INFO - root - 2017-12-05 08:32:43.654540: step 5840, loss = 1.79, batch loss = 1.74 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:29s remains)
INFO - root - 2017-12-05 08:32:45.867716: step 5850, loss = 1.89, batch loss = 1.84 (36.2 examples/sec; 0.221 sec/batch; 20h:02m:34s remains)
INFO - root - 2017-12-05 08:32:48.038501: step 5860, loss = 1.87, batch loss = 1.81 (34.0 examples/sec; 0.235 sec/batch; 21h:19m:03s remains)
INFO - root - 2017-12-05 08:32:50.221508: step 5870, loss = 1.77, batch loss = 1.71 (36.8 examples/sec; 0.218 sec/batch; 19h:44m:05s remains)
INFO - root - 2017-12-05 08:32:52.384935: step 5880, loss = 1.71, batch loss = 1.65 (36.1 examples/sec; 0.221 sec/batch; 20h:05m:24s remains)
INFO - root - 2017-12-05 08:32:54.571629: step 5890, loss = 1.76, batch loss = 1.71 (37.5 examples/sec; 0.214 sec/batch; 19h:22m:13s remains)
INFO - root - 2017-12-05 08:32:56.731236: step 5900, loss = 1.81, batch loss = 1.75 (37.6 examples/sec; 0.213 sec/batch; 19h:18m:45s remains)
2017-12-05 08:32:57.000638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206211 -4.2206206 -4.2206206 -4.2206206 -4.22062 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.22062 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.22062 -4.2206206 -4.2206211 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206211 -4.2206206 -4.22062 -4.2206206 -4.2206211 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206211 -4.2206206 -4.22062 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206211 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206211 -4.2206211 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206][-4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206 -4.2206206]]...]
INFO - root - 2017-12-05 08:32:59.198159: step 5910, loss = 1.70, batch loss = 1.64 (36.6 examples/sec; 0.219 sec/batch; 19h:49m:34s remains)
INFO - root - 2017-12-05 08:33:01.346208: step 5920, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:51m:51s remains)
INFO - root - 2017-12-05 08:33:03.547654: step 5930, loss = 1.91, batch loss = 1.86 (37.6 examples/sec; 0.213 sec/batch; 19h:16m:44s remains)
INFO - root - 2017-12-05 08:33:05.709758: step 5940, loss = 1.96, batch loss = 1.91 (35.3 examples/sec; 0.227 sec/batch; 20h:34m:34s remains)
INFO - root - 2017-12-05 08:33:07.867072: step 5950, loss = 1.68, batch loss = 1.62 (36.3 examples/sec; 0.221 sec/batch; 20h:01m:00s remains)
INFO - root - 2017-12-05 08:33:10.066077: step 5960, loss = 1.88, batch loss = 1.82 (37.5 examples/sec; 0.213 sec/batch; 19h:19m:30s remains)
INFO - root - 2017-12-05 08:33:12.245332: step 5970, loss = 1.95, batch loss = 1.89 (37.7 examples/sec; 0.212 sec/batch; 19h:15m:43s remains)
INFO - root - 2017-12-05 08:33:14.418292: step 5980, loss = 1.98, batch loss = 1.92 (37.0 examples/sec; 0.216 sec/batch; 19h:37m:01s remains)
INFO - root - 2017-12-05 08:33:16.587413: step 5990, loss = 1.96, batch loss = 1.90 (35.0 examples/sec; 0.229 sec/batch; 20h:44m:43s remains)
INFO - root - 2017-12-05 08:33:18.766791: step 6000, loss = 1.92, batch loss = 1.86 (37.4 examples/sec; 0.214 sec/batch; 19h:25m:19s remains)
2017-12-05 08:33:19.069635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2173519 -4.2173529 -4.2174497 -4.2175851 -4.2176838 -4.2177567 -4.2177858 -4.2177725 -4.2177596 -4.2177291 -4.2176809 -4.2173572 -4.2161317 -4.2132759 -4.2092628][-4.216392 -4.2163329 -4.2165813 -4.2170148 -4.2173519 -4.2176032 -4.2176981 -4.2176995 -4.2177086 -4.2177238 -4.2177162 -4.2176127 -4.217164 -4.2159882 -4.2141752][-4.2139249 -4.2136021 -4.2141585 -4.215261 -4.2162051 -4.2168789 -4.2172151 -4.2173586 -4.2174716 -4.2175827 -4.2176476 -4.2176046 -4.2174063 -4.2169962 -4.2163658][-4.2086749 -4.2073612 -4.2082138 -4.2105432 -4.2129092 -4.2146716 -4.21572 -4.2163272 -4.2167544 -4.2170415 -4.2173085 -4.2174449 -4.2173867 -4.2172475 -4.2170763][-4.2019053 -4.1980877 -4.1981096 -4.2014461 -4.2057672 -4.20934 -4.2118955 -4.213654 -4.2148261 -4.2156053 -4.2164 -4.2170033 -4.2172213 -4.2172875 -4.2172966][-4.1983681 -4.1914124 -4.1892085 -4.1920176 -4.1967158 -4.2009931 -4.2051983 -4.2091675 -4.2118573 -4.2135468 -4.2150192 -4.2162104 -4.2168565 -4.2171922 -4.2173648][-4.1994705 -4.1911597 -4.1882 -4.1905251 -4.1936316 -4.1957846 -4.1996937 -4.20517 -4.2095566 -4.2121477 -4.2139931 -4.2154536 -4.2163692 -4.2169409 -4.2172871][-4.2023444 -4.195765 -4.1949496 -4.1981874 -4.1996784 -4.1990609 -4.2005906 -4.2050438 -4.2093534 -4.2119918 -4.2137876 -4.2152028 -4.2161889 -4.2168026 -4.2171993][-4.2048903 -4.2009821 -4.2023873 -4.2065082 -4.2079391 -4.2066469 -4.2064629 -4.2084455 -4.2107534 -4.2124214 -4.2139106 -4.2153387 -4.21639 -4.2169814 -4.2172909][-4.2049556 -4.2033682 -4.2058611 -4.2106519 -4.2131786 -4.2130375 -4.2125745 -4.2126265 -4.2128325 -4.2132115 -4.2141547 -4.2155604 -4.2167158 -4.2173138 -4.2175179][-4.198184 -4.19944 -4.2043681 -4.2108073 -4.2146163 -4.215713 -4.2157035 -4.2152767 -4.2146449 -4.2142997 -4.2147808 -4.2158952 -4.2169166 -4.2174635 -4.2176638][-4.1787081 -4.1864986 -4.1980867 -4.2085733 -4.2141709 -4.2162371 -4.2168188 -4.2166662 -4.2160592 -4.2155757 -4.215795 -4.2165303 -4.2172194 -4.2175918 -4.21772][-4.1432614 -4.163343 -4.186615 -4.2037015 -4.211957 -4.2152715 -4.2166982 -4.21717 -4.2170062 -4.2167063 -4.2167883 -4.217145 -4.2175069 -4.2176943 -4.2177534][-4.0996342 -4.1348853 -4.1709933 -4.1952577 -4.2069564 -4.2124772 -4.2155666 -4.2170234 -4.2173958 -4.2173481 -4.2173767 -4.2175269 -4.2176771 -4.2177405 -4.2177587][-4.0651875 -4.1114092 -4.1545 -4.1833892 -4.1986117 -4.207438 -4.21339 -4.2164693 -4.2174692 -4.217649 -4.2176867 -4.2177243 -4.21775 -4.2177582 -4.2177625]]...]
INFO - root - 2017-12-05 08:33:21.235157: step 6010, loss = 1.99, batch loss = 1.93 (35.6 examples/sec; 0.225 sec/batch; 20h:24m:04s remains)
INFO - root - 2017-12-05 08:33:23.412301: step 6020, loss = 2.02, batch loss = 1.96 (37.3 examples/sec; 0.214 sec/batch; 19h:26m:29s remains)
INFO - root - 2017-12-05 08:33:25.590433: step 6030, loss = 2.01, batch loss = 1.95 (35.2 examples/sec; 0.227 sec/batch; 20h:35m:27s remains)
INFO - root - 2017-12-05 08:33:27.765287: step 6040, loss = 1.92, batch loss = 1.86 (37.8 examples/sec; 0.211 sec/batch; 19h:10m:45s remains)
INFO - root - 2017-12-05 08:33:29.944290: step 6050, loss = 1.97, batch loss = 1.92 (35.5 examples/sec; 0.225 sec/batch; 20h:24m:52s remains)
INFO - root - 2017-12-05 08:33:32.098401: step 6060, loss = 1.88, batch loss = 1.82 (36.7 examples/sec; 0.218 sec/batch; 19h:44m:29s remains)
INFO - root - 2017-12-05 08:33:34.276541: step 6070, loss = 1.76, batch loss = 1.70 (37.3 examples/sec; 0.215 sec/batch; 19h:27m:02s remains)
INFO - root - 2017-12-05 08:33:36.459941: step 6080, loss = 1.98, batch loss = 1.92 (37.1 examples/sec; 0.215 sec/batch; 19h:32m:19s remains)
INFO - root - 2017-12-05 08:33:38.621081: step 6090, loss = 1.93, batch loss = 1.87 (35.1 examples/sec; 0.228 sec/batch; 20h:39m:21s remains)
INFO - root - 2017-12-05 08:33:40.773876: step 6100, loss = 1.49, batch loss = 1.43 (37.3 examples/sec; 0.214 sec/batch; 19h:26m:09s remains)
2017-12-05 08:33:41.056446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688][-4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688 -4.2143688]]...]
INFO - root - 2017-12-05 08:33:43.214200: step 6110, loss = 1.85, batch loss = 1.79 (36.9 examples/sec; 0.217 sec/batch; 19h:40m:27s remains)
INFO - root - 2017-12-05 08:33:45.387677: step 6120, loss = 1.94, batch loss = 1.88 (37.0 examples/sec; 0.216 sec/batch; 19h:36m:51s remains)
INFO - root - 2017-12-05 08:33:47.555273: step 6130, loss = 1.77, batch loss = 1.71 (37.3 examples/sec; 0.215 sec/batch; 19h:27m:49s remains)
INFO - root - 2017-12-05 08:33:49.724664: step 6140, loss = 1.92, batch loss = 1.86 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:13s remains)
INFO - root - 2017-12-05 08:33:51.899631: step 6150, loss = 1.91, batch loss = 1.85 (37.5 examples/sec; 0.213 sec/batch; 19h:20m:16s remains)
INFO - root - 2017-12-05 08:33:54.074700: step 6160, loss = 1.79, batch loss = 1.73 (36.5 examples/sec; 0.219 sec/batch; 19h:51m:00s remains)
INFO - root - 2017-12-05 08:33:56.265192: step 6170, loss = 1.65, batch loss = 1.60 (37.0 examples/sec; 0.216 sec/batch; 19h:36m:09s remains)
INFO - root - 2017-12-05 08:33:58.422356: step 6180, loss = 1.70, batch loss = 1.64 (38.2 examples/sec; 0.209 sec/batch; 18h:57m:39s remains)
INFO - root - 2017-12-05 08:34:00.593692: step 6190, loss = 1.85, batch loss = 1.80 (36.9 examples/sec; 0.217 sec/batch; 19h:38m:00s remains)
INFO - root - 2017-12-05 08:34:02.758561: step 6200, loss = 1.94, batch loss = 1.88 (34.6 examples/sec; 0.231 sec/batch; 20h:56m:16s remains)
2017-12-05 08:34:03.048117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1941648 -4.1859765 -4.1744232 -4.1547785 -4.1193314 -4.0585194 -3.9699447 -3.8677111 -3.7825584 -3.7384393 -3.7364182 -3.7804687 -3.8727355 -3.9867179 -4.0801835][-4.1570864 -4.1285667 -4.0827208 -4.004704 -3.8836582 -3.715081 -3.513778 -3.3164132 -3.1717415 -3.1088195 -3.1292124 -3.2455914 -3.4583097 -3.7113991 -3.9177954][-4.071907 -3.9963105 -3.8819067 -3.6972814 -3.4250052 -3.0758648 -2.7062283 -2.3919306 -2.1943047 -2.1318243 -2.2019148 -2.4356701 -2.8326986 -3.2994995 -3.6842763][-3.9443364 -3.7962527 -3.5829635 -3.2532439 -2.7753344 -2.17602 -1.573818 -1.1075249 -0.85567546 -0.81424332 -0.96531773 -1.3541892 -1.979485 -2.7157044 -3.3427286][-3.8227699 -3.5949185 -3.2635407 -2.762007 -2.0467746 -1.164573 -0.29591608 0.35138798 0.66815662 0.67056513 0.40465546 -0.16786861 -1.029448 -2.0327322 -2.9119737][-3.750299 -3.4600761 -3.0228796 -2.3600984 -1.430033 -0.30127716 0.809525 1.6482143 2.0619326 2.0440245 1.6536741 0.89534616 -0.16992092 -1.3818376 -2.4652624][-3.7584119 -3.4513214 -2.9633279 -2.2086639 -1.1510534 0.12761021 1.4159741 2.4333277 2.9746618 2.9676805 2.4759297 1.5653801 0.36258125 -0.96265268 -2.1611066][-3.8577137 -3.5815611 -3.1094766 -2.3532515 -1.2879279 0.014370918 1.3692212 2.5008721 3.1671429 3.1985674 2.6346622 1.6212244 0.36894274 -0.9395225 -2.1096878][-3.9880548 -3.7731762 -3.3750522 -2.712359 -1.7669759 -0.5957644 0.66994524 1.7789888 2.4877772 2.5743198 2.0398688 1.0426764 -0.13599777 -1.2870882 -2.2880683][-4.0857658 -3.9436991 -3.6616874 -3.1716709 -2.46135 -1.552789 -0.51774716 0.44753742 1.1180758 1.2626357 0.84473705 0.011726856 -0.97801971 -1.9069154 -2.670933][-4.1399522 -4.0621204 -3.9049785 -3.6207139 -3.1810377 -2.5819459 -1.8589876 -1.127219 -0.55438423 -0.3489027 -0.57726884 -1.1566701 -1.8798733 -2.5485435 -3.0781784][-4.176343 -4.1373811 -4.0634289 -3.9319642 -3.7159028 -3.395298 -2.9605472 -2.4764271 -2.0626957 -1.8439207 -1.8927619 -2.1902955 -2.6534164 -3.1098242 -3.4489326][-4.1913285 -4.1728935 -4.1352005 -4.0688224 -3.9641957 -3.8201156 -3.6112337 -3.3437243 -3.0759788 -2.896512 -2.8642082 -2.9712348 -3.2159181 -3.5098817 -3.7375028][-4.19554 -4.180335 -4.15723 -4.1173053 -4.0512843 -3.9561994 -3.8451486 -3.7049148 -3.5529454 -3.4161925 -3.366276 -3.4141204 -3.5777521 -3.795104 -3.9507453][-4.198565 -4.1879725 -4.16713 -4.13178 -4.0866947 -4.0135632 -3.906348 -3.7777314 -3.6643391 -3.5616879 -3.5131235 -3.5684597 -3.7185276 -3.9438925 -4.0986943]]...]
INFO - root - 2017-12-05 08:34:05.252651: step 6210, loss = 1.87, batch loss = 1.81 (38.1 examples/sec; 0.210 sec/batch; 19h:02m:13s remains)
INFO - root - 2017-12-05 08:34:07.445393: step 6220, loss = 1.96, batch loss = 1.90 (35.2 examples/sec; 0.228 sec/batch; 20h:37m:08s remains)
INFO - root - 2017-12-05 08:34:09.640544: step 6230, loss = 1.76, batch loss = 1.70 (37.3 examples/sec; 0.215 sec/batch; 19h:26m:27s remains)
INFO - root - 2017-12-05 08:34:11.824575: step 6240, loss = 1.99, batch loss = 1.93 (37.0 examples/sec; 0.216 sec/batch; 19h:35m:15s remains)
INFO - root - 2017-12-05 08:34:14.022359: step 6250, loss = 1.66, batch loss = 1.60 (36.0 examples/sec; 0.222 sec/batch; 20h:09m:08s remains)
INFO - root - 2017-12-05 08:34:16.203997: step 6260, loss = 1.64, batch loss = 1.58 (37.0 examples/sec; 0.216 sec/batch; 19h:34m:04s remains)
INFO - root - 2017-12-05 08:34:18.371263: step 6270, loss = 1.84, batch loss = 1.78 (36.5 examples/sec; 0.219 sec/batch; 19h:50m:15s remains)
INFO - root - 2017-12-05 08:34:20.530176: step 6280, loss = 1.72, batch loss = 1.66 (37.1 examples/sec; 0.216 sec/batch; 19h:32m:28s remains)
INFO - root - 2017-12-05 08:34:22.676173: step 6290, loss = 1.99, batch loss = 1.94 (37.1 examples/sec; 0.216 sec/batch; 19h:32m:34s remains)
