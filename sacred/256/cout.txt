INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "256"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-13 09:45:30.385061: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 09:45:30.385099: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 09:45:30.385257: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 09:45:30.385263: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 09:45:30.385267: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-13 09:45:36.859998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-13 09:45:36.860035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-13 09:45:36.860042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-13 09:45:36.860049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-13 09:45:48.385256: step 0, loss = 2.28, batch loss = 2.23 (1.0 examples/sec; 8.405 sec/batch; 776h:18m:15s remains)
2017-12-13 09:45:49.044905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3789015 -4.3788018 -4.3787303 -4.3786411 -4.3785439 -4.3784504 -4.3783574 -4.378274 -4.3782148 -4.378201 -4.3782458 -4.3783474 -4.3785181 -4.3787222 -4.3789272][-4.3787475 -4.3786182 -4.3785105 -4.3783803 -4.3782372 -4.3780928 -4.3779411 -4.3778086 -4.3777256 -4.377718 -4.3777962 -4.3779588 -4.3782082 -4.3784914 -4.3787608][-4.3786111 -4.3784518 -4.3783092 -4.3781376 -4.3779426 -4.3777313 -4.3775058 -4.37731 -4.3771958 -4.3772006 -4.3773293 -4.3775754 -4.3779221 -4.3782949 -4.3786345][-4.3784666 -4.378283 -4.3781133 -4.3779011 -4.3776536 -4.3773718 -4.3770676 -4.3768063 -4.3766661 -4.3766947 -4.3768926 -4.3772373 -4.3776793 -4.3781319 -4.37853][-4.3783355 -4.3781314 -4.3779383 -4.3776889 -4.3773942 -4.3770533 -4.3766851 -4.3763747 -4.3762188 -4.3762932 -4.3765788 -4.3770175 -4.377532 -4.3780351 -4.3784695][-4.3782182 -4.3779964 -4.3777852 -4.3775096 -4.3771849 -4.3768144 -4.3764172 -4.3760791 -4.3759236 -4.3760643 -4.3764424 -4.3769488 -4.3774924 -4.378006 -4.3784494][-4.378118 -4.3778834 -4.3776641 -4.3773823 -4.3770571 -4.3766975 -4.3763208 -4.3760056 -4.3758883 -4.3761134 -4.3765516 -4.3770661 -4.3775811 -4.3780565 -4.3784742][-4.3780532 -4.3778181 -4.3776069 -4.3773417 -4.3770504 -4.3767509 -4.3764586 -4.3762336 -4.3762054 -4.3764849 -4.3769069 -4.3773518 -4.3777766 -4.3781757 -4.3785386][-4.3780293 -4.3778043 -4.3776159 -4.3773832 -4.3771462 -4.3769341 -4.3767571 -4.3766518 -4.37671 -4.3769846 -4.3773327 -4.3776712 -4.3779902 -4.3783064 -4.378613][-4.3780422 -4.377831 -4.3776703 -4.3774824 -4.3773055 -4.3771796 -4.3771095 -4.3770962 -4.3771935 -4.3774219 -4.3776832 -4.3779149 -4.3781452 -4.3784 -4.3786688][-4.37809 -4.3779025 -4.3777781 -4.3776417 -4.3775249 -4.3774633 -4.3774562 -4.3774791 -4.3775716 -4.3777366 -4.3779159 -4.37807 -4.3782444 -4.3784676 -4.3787131][-4.3781581 -4.3780084 -4.3779283 -4.3778462 -4.3777814 -4.3777556 -4.3777671 -4.3777843 -4.3778419 -4.377943 -4.3780537 -4.3781581 -4.3783073 -4.37852 -4.3787513][-4.378222 -4.3781209 -4.3780909 -4.3780527 -4.37802 -4.3780041 -4.3780031 -4.3779883 -4.3779955 -4.3780384 -4.3781013 -4.3781805 -4.3783226 -4.3785367 -4.3787656][-4.3782492 -4.3781838 -4.3781915 -4.3781824 -4.3781633 -4.3781409 -4.3781128 -4.3780589 -4.3780174 -4.3780179 -4.378057 -4.3781338 -4.3782868 -4.3785129 -4.3787508][-4.3782659 -4.378221 -4.3782535 -4.3782659 -4.3782539 -4.3782187 -4.3781571 -4.3780632 -4.3779836 -4.377955 -4.3779874 -4.3780746 -4.3782439 -4.3784847 -4.3787322]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-13 09:45:54.330306: step 10, loss = 1.91, batch loss = 1.84 (17.4 examples/sec; 0.459 sec/batch; 42h:24m:12s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4
INFO - root - 2017-12-13 09:45:58.859577: step 20, loss = 1.43, batch loss = 1.35 (17.7 examples/sec; 0.451 sec/batch; 41h:39m:20s remains)
INFO - root - 2017-12-13 09:46:03.422124: step 30, loss = 1.00, batch loss = 0.93 (17.3 examples/sec; 0.463 sec/batch; 42h:43m:31s remains)
INFO - root - 2017-12-13 09:46:08.044888: step 40, loss = 1.05, batch loss = 0.98 (17.4 examples/sec; 0.461 sec/batch; 42h:33m:53s remains)
INFO - root - 2017-12-13 09:46:12.723693: step 50, loss = 1.03, batch loss = 0.95 (17.7 examples/sec; 0.452 sec/batch; 41h:47m:04s remains)
INFO - root - 2017-12-13 09:46:17.335193: step 60, loss = 0.87, batch loss = 0.79 (17.4 examples/sec; 0.460 sec/batch; 42h:28m:47s remains)
INFO - root - 2017-12-13 09:46:21.732297: step 70, loss = 0.82, batch loss = 0.75 (16.6 examples/sec; 0.481 sec/batch; 44h:26m:03s remains)
INFO - root - 2017-12-13 09:46:26.450093: step 80, loss = 0.81, batch loss = 0.73 (16.9 examples/sec; 0.474 sec/batch; 43h:45m:01s remains)
INFO - root - 2017-12-13 09:46:31.135161: step 90, loss = 0.81, batch loss = 0.73 (16.8 examples/sec; 0.475 sec/batch; 43h:52m:08s remains)
INFO - root - 2017-12-13 09:46:35.767632: step 100, loss = 1.02, batch loss = 0.94 (16.6 examples/sec; 0.481 sec/batch; 44h:22m:06s remains)
2017-12-13 09:46:36.207324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.33587551 0.021483898 0.28213215 0.40944481 0.47813296 0.47906113 0.46780729 0.47187757 0.47365093 0.47937155 0.50154018 0.55538821 0.61578012 0.64573979 0.62807751][0.00052046776 0.40840149 0.6980772 0.8402667 0.91890574 0.93505287 0.94041491 0.95525503 0.9668951 0.97859621 1.0011559 1.0465508 1.0945616 1.1086922 1.0771122][0.26909065 0.74026823 1.0603232 1.2155995 1.3023047 1.3306732 1.3526378 1.3820992 1.4043221 1.4201174 1.4412131 1.476953 1.5219436 1.5289989 1.5035701][0.38824511 0.89913416 1.249423 1.4168086 1.5175085 1.5533552 1.5813956 1.6120667 1.6374083 1.6474981 1.6649456 1.6870284 1.7075529 1.7050791 1.6801376][0.44176316 0.94471073 1.3093939 1.476191 1.5848994 1.6205859 1.6487813 1.6670375 1.6810107 1.6932898 1.7109199 1.7323356 1.7540708 1.7542424 1.7283597][0.43609023 0.93190718 1.2941298 1.4625335 1.5745373 1.6023622 1.6218863 1.632885 1.639585 1.646174 1.6589074 1.6775756 1.6975455 1.6990938 1.6790266][0.41486239 0.90423632 1.2705989 1.4430685 1.5532589 1.5706143 1.5770717 1.5760779 1.5722499 1.5726738 1.5813456 1.5974288 1.6166339 1.6242976 1.6134214][0.40540767 0.88600779 1.2481532 1.4189291 1.5262394 1.5338068 1.5280557 1.5125937 1.4972572 1.489006 1.494555 1.5132227 1.5356975 1.5496292 1.54705][0.38708282 0.85998726 1.2218013 1.3989525 1.5071287 1.5066152 1.4889994 1.4567032 1.4260798 1.4103308 1.4166312 1.4435439 1.4768171 1.5053205 1.5138874][0.37192941 0.84828711 1.2114453 1.3886695 1.4913163 1.4841237 1.4555941 1.4125671 1.3694839 1.3429565 1.3477807 1.3782911 1.4192038 1.458858 1.4772267][0.38394928 0.85494614 1.2014217 1.3871164 1.4802217 1.4669104 1.4299045 1.3772178 1.3259482 1.2952619 1.298007 1.3303008 1.378181 1.423295 1.445087][0.41645861 0.87491894 1.200346 1.3858166 1.4672508 1.4502463 1.4107361 1.3560219 1.3007245 1.2676396 1.2708907 1.3047915 1.3548617 1.4027333 1.4280963][0.41356182 0.87208891 1.2089329 1.3846908 1.4608173 1.4417076 1.4030938 1.3488293 1.2948313 1.2600074 1.2609396 1.2960796 1.347156 1.3927693 1.4174237][0.27903247 0.71885443 1.0708299 1.2377353 1.312459 1.2937093 1.260385 1.244278 1.2278652 1.2320547 1.259634 1.2641225 1.2925615 1.2976565 1.290319][0.13894558 0.57133651 0.91817 1.0827813 1.1599288 1.142179 1.114162 1.107625 1.1027846 1.1074061 1.1415935 1.1754251 1.2220435 1.2324557 1.2149229]]...]
