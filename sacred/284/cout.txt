INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "284"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc_2/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc_2/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_2/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_3/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_3/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_3/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-15 09:58:03.556146: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:58:03.556187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:58:03.556194: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:58:03.556198: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:58:03.556202: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-15 09:58:03.889350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-15 09:58:03.889390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-15 09:58:03.889397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-15 09:58:03.889405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-15 09:58:08.223149: step 0, loss = 2.28, batch loss = 2.23 (2.6 examples/sec; 3.107 sec/batch; 286h:56m:14s remains)
2017-12-15 09:58:08.837120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4195089 -4.4194894 -4.4194803 -4.4194765 -4.4194694 -4.4194551 -4.4194407 -4.4194393 -4.4194446 -4.4194407 -4.419426 -4.419405 -4.4193897 -4.419385 -4.4193873][-4.4194684 -4.4194522 -4.4194508 -4.4194541 -4.4194474 -4.419426 -4.4194021 -4.4193916 -4.419394 -4.4193954 -4.4193888 -4.4193783 -4.4193673 -4.4193664 -4.419373][-4.4194317 -4.4194226 -4.4194341 -4.4194484 -4.4194427 -4.4194117 -4.4193754 -4.4193554 -4.41936 -4.4193797 -4.419395 -4.4194021 -4.4194007 -4.4194021 -4.4194088][-4.4194036 -4.419405 -4.4194307 -4.4194527 -4.4194431 -4.4194055 -4.4193611 -4.4193354 -4.4193487 -4.4193912 -4.4194317 -4.419456 -4.4194641 -4.4194646 -4.4194632][-4.4193835 -4.4193907 -4.4194245 -4.4194508 -4.4194341 -4.4193854 -4.41933 -4.4193015 -4.4193277 -4.4193959 -4.4194632 -4.4195037 -4.4195189 -4.419517 -4.4195065][-4.4193726 -4.4193764 -4.4194112 -4.4194336 -4.4194069 -4.4193411 -4.4192634 -4.4192266 -4.41927 -4.4193635 -4.4194551 -4.4195108 -4.4195333 -4.41953 -4.4195151][-4.4193811 -4.4193759 -4.4194021 -4.4194126 -4.4193721 -4.4192863 -4.4191794 -4.4191279 -4.4191847 -4.4192996 -4.4194121 -4.4194837 -4.4195142 -4.4195118 -4.4194946][-4.41942 -4.4194055 -4.4194179 -4.4194179 -4.4193773 -4.4192948 -4.4191918 -4.4191356 -4.41918 -4.4192824 -4.4193859 -4.4194546 -4.4194851 -4.4194832 -4.4194674][-4.4194679 -4.419456 -4.4194651 -4.4194679 -4.4194441 -4.41939 -4.4193215 -4.4192739 -4.4192867 -4.4193459 -4.4194126 -4.4194584 -4.4194779 -4.4194756 -4.4194641][-4.4195108 -4.4195094 -4.4195247 -4.4195366 -4.4195294 -4.4195013 -4.4194622 -4.4194293 -4.4194207 -4.41944 -4.4194703 -4.4194937 -4.4195027 -4.4195 -4.4194942][-4.4195385 -4.4195476 -4.4195685 -4.4195876 -4.4195905 -4.4195762 -4.4195585 -4.41954 -4.4195256 -4.4195242 -4.4195294 -4.4195352 -4.4195342 -4.41953 -4.4195285][-4.4195576 -4.4195704 -4.4195943 -4.4196153 -4.4196219 -4.4196167 -4.4196119 -4.4196038 -4.4195929 -4.4195857 -4.4195771 -4.419569 -4.41956 -4.4195518 -4.4195523][-4.4195747 -4.4195852 -4.4196067 -4.4196262 -4.4196362 -4.4196377 -4.41964 -4.4196372 -4.4196286 -4.4196186 -4.4196043 -4.4195905 -4.4195766 -4.4195652 -4.4195628][-4.4195757 -4.4195814 -4.4195981 -4.4196181 -4.419632 -4.4196367 -4.4196386 -4.4196343 -4.4196258 -4.4196181 -4.4196053 -4.4195914 -4.4195771 -4.4195647 -4.419558][-4.4195523 -4.4195547 -4.419569 -4.4195895 -4.4196053 -4.4196095 -4.4196081 -4.4196048 -4.4196 -4.4195981 -4.419591 -4.4195805 -4.4195681 -4.4195576 -4.41955]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 09:58:15.713654: step 10, loss = 1.31, batch loss = 1.24 (13.4 examples/sec; 0.598 sec/batch; 55h:12m:01s remains)
INFO - root - 2017-12-15 09:58:21.580354: step 20, loss = 0.96, batch loss = 0.87 (13.6 examples/sec; 0.589 sec/batch; 54h:22m:10s remains)
INFO - root - 2017-12-15 09:58:27.541707: step 30, loss = 0.80, batch loss = 0.68 (13.5 examples/sec; 0.592 sec/batch; 54h:41m:35s remains)
INFO - root - 2017-12-15 09:58:33.476510: step 40, loss = 0.77, batch loss = 0.61 (13.5 examples/sec; 0.592 sec/batch; 54h:39m:57s remains)
INFO - root - 2017-12-15 09:58:39.406046: step 50, loss = 0.78, batch loss = 0.59 (13.6 examples/sec; 0.590 sec/batch; 54h:28m:44s remains)
INFO - root - 2017-12-15 09:58:45.360105: step 60, loss = 0.83, batch loss = 0.62 (13.4 examples/sec; 0.595 sec/batch; 54h:56m:30s remains)
INFO - root - 2017-12-15 09:58:51.305908: step 70, loss = 0.93, batch loss = 0.69 (13.7 examples/sec; 0.582 sec/batch; 53h:45m:53s remains)
INFO - root - 2017-12-15 09:58:57.238898: step 80, loss = 0.80, batch loss = 0.54 (13.3 examples/sec; 0.601 sec/batch; 55h:27m:28s remains)
INFO - root - 2017-12-15 09:59:03.194953: step 90, loss = 0.87, batch loss = 0.60 (13.6 examples/sec; 0.587 sec/batch; 54h:12m:42s remains)
INFO - root - 2017-12-15 09:59:09.167230: step 100, loss = 0.78, batch loss = 0.50 (13.4 examples/sec; 0.599 sec/batch; 55h:19m:18s remains)
2017-12-15 09:59:09.642587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5383029 -1.2310586 -0.94350529 -0.73450065 -0.55752993 -0.71024656 -0.94152164 -1.1894181 -1.4069591 -1.5850027 -1.8268967 -2.0094566 -2.2138255 -2.3110232 -2.3785772][-1.4145608 -1.0442927 -0.74175882 -0.49549222 -0.23240829 -0.32578373 -0.51501679 -0.78383565 -1.0755074 -1.3024254 -1.6486623 -1.9909369 -2.2622504 -2.4325733 -2.4491742][-1.2478547 -0.86633945 -0.53917456 -0.2454977 0.071773052 0.036139011 -0.11062169 -0.39029503 -0.69018722 -0.939528 -1.4393737 -1.9059491 -2.2871726 -2.6488724 -2.76][-1.219337 -0.891855 -0.5785625 -0.24367046 0.14428806 0.15351868 0.055478811 -0.1366539 -0.39780474 -0.65000653 -1.1670935 -1.6920562 -2.2506623 -2.773721 -2.9747531][-1.3407302 -1.0903704 -0.80041027 -0.44907451 -0.035569191 0.066060066 0.12816906 0.087201595 -0.080540657 -0.31624174 -0.8788116 -1.4994407 -2.172009 -2.8062444 -3.2298014][-1.4516811 -1.3243196 -1.1209185 -0.80912161 -0.42150998 -0.21762514 -0.0083794594 0.089407682 0.063130379 -0.029851913 -0.54320812 -1.2230535 -1.9720982 -2.7246757 -3.3447475][-1.5308068 -1.4832752 -1.3383825 -1.0871241 -0.75076437 -0.523273 -0.30848455 -0.10489845 0.0080516338 -0.018008947 -0.45460892 -1.0247955 -1.7379234 -2.5414314 -3.2279437][-1.3906734 -1.3739011 -1.4001541 -1.3003643 -1.0669479 -0.88506413 -0.66219473 -0.39661241 -0.22458625 -0.10890794 -0.33799362 -0.82709622 -1.4657757 -2.18113 -2.89316][-1.2115729 -1.1791637 -1.2336247 -1.1926999 -1.0587833 -0.98852849 -0.87309194 -0.69179249 -0.53935313 -0.41878724 -0.51875114 -0.77871394 -1.172163 -1.7555506 -2.3699594][-1.16376 -1.1758986 -1.2603073 -1.2410245 -1.1980402 -1.1874914 -1.1276479 -1.0019112 -0.87016821 -0.70236444 -0.67000103 -0.75550723 -1.0262055 -1.4478343 -1.9651399][-1.3158493 -1.3454823 -1.3798084 -1.3534439 -1.3347363 -1.3240783 -1.2580602 -1.1689913 -1.0439479 -0.90584636 -0.8484726 -0.81786346 -0.97616076 -1.2544808 -1.6219158][-1.7152214 -1.7157042 -1.7755437 -1.7912159 -1.7532692 -1.7181418 -1.6177938 -1.5155337 -1.3218646 -1.1605196 -1.0530906 -0.97967792 -1.0542793 -1.192688 -1.5730553][-2.0531015 -2.0713069 -2.1106734 -2.1218834 -2.1093059 -2.0349078 -1.8974808 -1.7297838 -1.4695549 -1.3186386 -1.1351578 -1.0008118 -1.0159979 -1.0597067 -1.3589778][-2.2318034 -2.3006721 -2.3222871 -2.2969952 -2.2421002 -2.1938548 -2.089972 -1.8684328 -1.6062691 -1.4411969 -1.222671 -1.0408409 -0.91319537 -0.838917 -1.0318694][-2.1290321 -2.1679337 -2.1303864 -2.1098533 -2.0354331 -1.9486206 -1.8248451 -1.6459303 -1.4396534 -1.2581632 -1.1038721 -1.0067058 -0.92137504 -0.77773857 -0.83350873]]...]
INFO - root - 2017-12-15 09:59:15.665264: step 110, loss = 0.90, batch loss = 0.60 (13.1 examples/sec; 0.611 sec/batch; 56h:24m:34s remains)
2017-12-15 09:59:18.662677: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 54345 get requests, put_count=54341 evicted_count=1000 eviction_rate=0.0184023 and unsatisfied allocation rate=0.0203147
2017-12-15 09:59:18.662705: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
INFO - root - 2017-12-15 09:59:21.681658: step 120, loss = 0.80, batch loss = 0.49 (13.4 examples/sec; 0.596 sec/batch; 55h:00m:00s remains)
INFO - root - 2017-12-15 09:59:27.649048: step 130, loss = 0.86, batch loss = 0.54 (13.5 examples/sec; 0.594 sec/batch; 54h:50m:21s remains)
INFO - root - 2017-12-15 09:59:33.684735: step 140, loss = 0.78, batch loss = 0.46 (13.3 examples/sec; 0.603 sec/batch; 55h:41m:32s remains)
INFO - root - 2017-12-15 09:59:39.748218: step 150, loss = 0.80, batch loss = 0.48 (13.2 examples/sec; 0.605 sec/batch; 55h:51m:28s remains)
INFO - root - 2017-12-15 09:59:45.752264: step 160, loss = 0.73, batch loss = 0.42 (13.3 examples/sec; 0.603 sec/batch; 55h:37m:22s remains)
INFO - root - 2017-12-15 09:59:51.801993: step 170, loss = 0.79, batch loss = 0.48 (13.4 examples/sec; 0.599 sec/batch; 55h:18m:36s remains)
INFO - root - 2017-12-15 09:59:57.842562: step 180, loss = 0.71, batch loss = 0.39 (13.3 examples/sec; 0.602 sec/batch; 55h:35m:40s remains)
INFO - root - 2017-12-15 10:00:03.868177: step 190, loss = 0.85, batch loss = 0.54 (12.9 examples/sec; 0.620 sec/batch; 57h:15m:56s remains)
INFO - root - 2017-12-15 10:00:09.970734: step 200, loss = 0.83, batch loss = 0.52 (13.0 examples/sec; 0.617 sec/batch; 56h:56m:03s remains)
2017-12-15 10:00:10.427389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6631243 -1.2444203 -0.65715194 -0.14952445 0.36708188 0.40092874 0.38096595 0.35836148 0.3282783 0.31362319 0.10056567 -0.269423 -0.93179059 -1.7392809 -2.5174923][-1.6262109 -1.1927466 -0.59888005 -0.10127926 0.36226392 0.36953807 0.3453095 0.32290721 0.29765058 0.27793813 0.071191311 -0.31423259 -0.98276472 -1.8015342 -2.5625792][-1.6567175 -1.1669025 -0.533479 -0.034489632 0.39040351 0.38832879 0.36502433 0.3392036 0.31426454 0.28929305 0.061083555 -0.33079648 -1.0044198 -1.8414145 -2.6139796][-1.7190895 -1.2109089 -0.55711389 -0.043210268 0.39386916 0.37952065 0.35215831 0.3190186 0.28741813 0.2585175 0.028974295 -0.35795403 -1.0215509 -1.8468745 -2.5933409][-1.782712 -1.3129063 -0.63751268 -0.10378385 0.35953498 0.35016227 0.32211709 0.27867675 0.23065782 0.19140172 -0.033269644 -0.40744495 -1.0478234 -1.8618469 -2.6107993][-1.8090379 -1.3888249 -0.70008206 -0.15193486 0.35316777 0.34659553 0.32988 0.3134501 0.29173636 0.26548314 0.055022 -0.31481433 -0.94360685 -1.7585733 -2.4791379][-1.8192267 -1.3615456 -0.67513323 -0.12521434 0.43272185 0.44826007 0.45727134 0.451658 0.44708943 0.44304585 0.24740624 -0.11103463 -0.73873091 -1.5687695 -2.3015206][-1.740694 -1.2855549 -0.60251665 -0.059639215 0.51255774 0.54357028 0.57457328 0.59892583 0.62371469 0.6330812 0.43961692 0.083265543 -0.55610585 -1.3867326 -2.1311665][-1.6815286 -1.1911077 -0.50995016 0.041762114 0.62204623 0.66315293 0.70255733 0.74300647 0.78630567 0.80317187 0.60804963 0.23704362 -0.4218688 -1.2519238 -2.0118623][-1.6590154 -1.1911881 -0.57332969 -2.6464462e-05 0.60856795 0.65296245 0.69611526 0.73367 0.76642108 0.77282357 0.57029414 0.17374682 -0.45507717 -1.2202098 -1.9474711][-2.2965515 -1.930728 -1.4237139 -0.98575044 -0.51508451 -0.47862744 -0.44866395 -0.4221704 -0.40523648 -0.40291333 -0.60946727 -1.0013888 -1.5299885 -2.1348619 -2.6709414][-3.1947002 -2.986393 -2.6073532 -2.2960072 -1.950153 -1.9132907 -1.8876054 -1.871273 -1.8671403 -1.8714468 -2.0439534 -2.3322887 -2.692261 -3.1054711 -3.4269032][-4.1253061 -3.9797537 -3.7080588 -3.5271773 -3.3171208 -3.289659 -3.2731838 -3.265604 -3.2667286 -3.2737329 -3.3942363 -3.5896487 -3.7970414 -4.0169563 -4.1476412][-5.1664977 -5.0768666 -4.8938179 -4.809237 -4.7260489 -4.7031703 -4.6906476 -4.6811781 -4.6746483 -4.6743155 -4.6832809 -4.7320995 -4.8106995 -4.8247962 -4.7800512][-5.726645 -5.5926328 -5.4117064 -5.4130969 -5.4253931 -5.3962831 -5.3735948 -5.3750958 -5.3787556 -5.3651423 -5.3429289 -5.3250914 -5.2746911 -5.1452546 -4.996325]]...]
INFO - root - 2017-12-15 10:00:16.491916: step 210, loss = 0.75, batch loss = 0.44 (13.3 examples/sec; 0.599 sec/batch; 55h:19m:22s remains)
INFO - root - 2017-12-15 10:00:22.549102: step 220, loss = 0.78, batch loss = 0.46 (13.2 examples/sec; 0.604 sec/batch; 55h:47m:24s remains)
INFO - root - 2017-12-15 10:00:28.636834: step 230, loss = 0.70, batch loss = 0.38 (13.3 examples/sec; 0.603 sec/batch; 55h:37m:12s remains)
INFO - root - 2017-12-15 10:00:34.688828: step 240, loss = 0.77, batch loss = 0.45 (13.4 examples/sec; 0.599 sec/batch; 55h:17m:56s remains)
INFO - root - 2017-12-15 10:00:40.820056: step 250, loss = 0.75, batch loss = 0.44 (12.8 examples/sec; 0.625 sec/batch; 57h:43m:00s remains)
INFO - root - 2017-12-15 10:00:46.896446: step 260, loss = 0.75, batch loss = 0.44 (12.9 examples/sec; 0.619 sec/batch; 57h:08m:19s remains)
INFO - root - 2017-12-15 10:00:52.950713: step 270, loss = 0.80, batch loss = 0.49 (13.3 examples/sec; 0.602 sec/batch; 55h:30m:44s remains)
INFO - root - 2017-12-15 10:00:59.030839: step 280, loss = 0.76, batch loss = 0.45 (13.3 examples/sec; 0.601 sec/batch; 55h:29m:57s remains)
INFO - root - 2017-12-15 10:01:05.117668: step 290, loss = 0.77, batch loss = 0.46 (13.3 examples/sec; 0.599 sec/batch; 55h:18m:27s remains)
INFO - root - 2017-12-15 10:01:11.295774: step 300, loss = 0.75, batch loss = 0.44 (13.1 examples/sec; 0.610 sec/batch; 56h:16m:24s remains)
2017-12-15 10:01:11.745323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8391538 -2.5743914 -2.0478308 -1.3275278 -0.65886045 -0.19519806 0.14714408 -0.011771202 -0.33757281 -0.7835536 -1.273083 -1.716809 -2.1236534 -2.5509291 -2.923141][-2.744823 -2.4246974 -1.9656193 -1.3599453 -0.54014874 0.016993284 0.32204938 0.23108935 -0.09422636 -0.66115808 -1.2294676 -1.7971573 -2.2431238 -2.6760371 -3.0036557][-2.7129154 -2.4252768 -1.9967978 -1.2586524 -0.49545288 -0.0062327385 0.294276 0.34212279 -0.022244453 -0.59910345 -1.2388403 -1.8860157 -2.346282 -2.7396991 -3.0832224][-2.6950085 -2.3510695 -1.8444729 -1.2469125 -0.52188683 -0.079660892 0.29769397 0.36267543 0.1294868 -0.27365851 -1.0416112 -1.7483745 -2.2477391 -2.6060333 -2.832726][-2.5511289 -2.1787143 -1.6748829 -0.96377039 -0.22823906 0.13617682 0.27670455 0.35272574 0.24606252 -0.097895384 -0.72890091 -1.3197455 -1.8982904 -2.3445373 -2.6587362][-2.08033 -1.5942719 -1.0856767 -0.52678752 -0.064035416 0.233289 0.45156217 0.51183391 0.31129909 0.033843279 -0.37532854 -0.969182 -1.5357339 -2.0064864 -2.4573064][-1.6619499 -1.0149169 -0.43401003 -0.01926446 0.360049 0.43993211 0.45931458 0.46989751 0.44872212 0.295336 -0.17078996 -0.697438 -1.2017703 -1.7575068 -2.3554025][-1.3682518 -0.8183 -0.26343107 0.25001979 0.60498977 0.62455964 0.61671281 0.60137868 0.50267673 0.34481692 -0.049203873 -0.56834626 -1.1220527 -1.676182 -2.2541294][-1.4743662 -0.86587453 -0.2004385 0.28000569 0.58486772 0.62978911 0.60847354 0.57755685 0.52681708 0.42843127 0.0387733 -0.45176053 -0.92586207 -1.4620736 -2.1008933][-1.8492324 -1.2180097 -0.62252879 -0.092377663 0.38379359 0.40932298 0.33510709 0.30763841 0.28754449 0.1843493 -0.13052726 -0.59284663 -1.1174316 -1.6188993 -2.1598577][-2.4705076 -2.0868506 -1.6816566 -1.2413838 -0.87588906 -0.76862478 -0.66619515 -0.68943429 -0.7566483 -0.85413527 -1.1316214 -1.5410833 -1.9002116 -2.2798533 -2.7412834][-3.3195159 -3.0162592 -2.7004061 -2.4164095 -2.1339259 -1.9653008 -1.9364498 -1.9642069 -1.9769464 -2.0648725 -2.238719 -2.4839611 -2.7186251 -3.0265875 -3.2849936][-4.064827 -3.9086154 -3.7881963 -3.5970008 -3.3743854 -3.2298176 -3.1605816 -3.1127524 -3.1845491 -3.2236054 -3.2992737 -3.4336226 -3.5921474 -3.6984787 -3.8815789][-4.1761308 -4.1481843 -4.0078621 -3.8658264 -3.827451 -3.6502576 -3.4815679 -3.4097085 -3.4687502 -3.5535283 -3.6919458 -3.8574355 -3.9821012 -4.0934229 -4.1670327][-4.3060007 -4.2553582 -4.1284165 -3.9643166 -3.9162395 -3.7496393 -3.6411579 -3.5506122 -3.6005583 -3.7119341 -3.8483911 -3.978534 -4.0761662 -4.0692992 -4.0262542]]...]
INFO - root - 2017-12-15 10:01:17.777138: step 310, loss = 0.71, batch loss = 0.40 (13.3 examples/sec; 0.600 sec/batch; 55h:19m:15s remains)
INFO - root - 2017-12-15 10:01:23.851863: step 320, loss = 0.84, batch loss = 0.53 (12.7 examples/sec; 0.629 sec/batch; 58h:03m:21s remains)
INFO - root - 2017-12-15 10:01:30.004132: step 330, loss = 0.69, batch loss = 0.38 (13.1 examples/sec; 0.610 sec/batch; 56h:16m:52s remains)
INFO - root - 2017-12-15 10:01:36.078561: step 340, loss = 0.73, batch loss = 0.42 (13.5 examples/sec; 0.593 sec/batch; 54h:40m:25s remains)
INFO - root - 2017-12-15 10:01:42.205587: step 350, loss = 0.68, batch loss = 0.37 (13.3 examples/sec; 0.602 sec/batch; 55h:30m:47s remains)
INFO - root - 2017-12-15 10:01:48.309824: step 360, loss = 0.71, batch loss = 0.40 (12.8 examples/sec; 0.626 sec/batch; 57h:44m:10s remains)
INFO - root - 2017-12-15 10:01:54.473604: step 370, loss = 0.69, batch loss = 0.38 (12.6 examples/sec; 0.633 sec/batch; 58h:24m:28s remains)
INFO - root - 2017-12-15 10:02:00.571951: step 380, loss = 0.70, batch loss = 0.39 (13.2 examples/sec; 0.607 sec/batch; 56h:02m:38s remains)
INFO - root - 2017-12-15 10:02:06.735669: step 390, loss = 0.70, batch loss = 0.39 (13.1 examples/sec; 0.610 sec/batch; 56h:17m:43s remains)
INFO - root - 2017-12-15 10:02:12.867026: step 400, loss = 0.72, batch loss = 0.41 (13.2 examples/sec; 0.608 sec/batch; 56h:05m:48s remains)
2017-12-15 10:02:13.334358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9582586 -3.153933 -3.2857122 -3.3053739 -3.3576596 -3.5250072 -3.5490208 -3.3567493 -3.0737424 -2.6930845 -2.7189951 -3.0441821 -3.564122 -4.1096883 -4.5531449][-2.9748197 -2.9820349 -2.9201698 -2.8886361 -2.9384758 -3.0538912 -3.149404 -3.02392 -2.8674331 -2.5858612 -2.5523996 -2.8152821 -3.3425834 -3.9169903 -4.4617577][-2.4552779 -2.5639925 -2.6082392 -2.5629325 -2.4896402 -2.421716 -2.3932362 -2.3215554 -2.2571011 -2.1750104 -2.42655 -2.732352 -3.1466074 -3.7352443 -4.351284][-1.9831364 -1.997108 -2.0517323 -2.0272899 -1.9513009 -1.9063354 -1.6772022 -1.4390786 -1.3619468 -1.3525972 -1.7125995 -2.3151174 -2.9551067 -3.5110323 -4.0397654][-1.3990264 -1.246304 -1.228025 -1.2180603 -1.1191289 -1.0237181 -0.68921185 -0.28670692 -0.015302658 -0.060914993 -0.58521581 -1.2702463 -2.0121644 -2.846025 -3.5930429][-0.71127868 -0.43801522 -0.23670721 -0.049255371 0.14245987 0.25290346 0.40986013 0.66574335 0.92499161 0.95260382 0.42107534 -0.34142089 -1.1782846 -1.9696977 -2.798789][0.0051531792 0.43066692 0.70009565 0.90303373 1.1125298 1.1986632 1.3231144 1.4501657 1.5644693 1.5907412 1.0386839 0.36929178 -0.45799756 -1.3307352 -2.1978073][0.51943731 0.94963217 1.2649689 1.52494 1.7352767 1.7499223 1.7659655 1.8252602 1.9467893 2.0697961 1.6253586 0.86008024 0.011044025 -0.8919661 -1.8854904][0.77738857 1.2661977 1.5927005 1.8100362 1.9488831 1.8619375 1.8143148 1.8550653 1.9532862 2.1266232 1.9354577 1.2938447 0.4088254 -0.54986072 -1.4994838][0.18255043 0.67950821 1.1225562 1.2669897 1.3093295 1.2154007 1.1149001 1.07617 1.2170954 1.403801 1.3325539 0.97714186 0.32140064 -0.59862733 -1.5338366][-1.0016077 -0.68763328 -0.48677492 -0.37095642 -0.23524594 -0.35140276 -0.49752045 -0.56782818 -0.49159861 -0.25844288 -0.12023568 -0.31247783 -0.7926743 -1.449105 -2.1672664][-2.498081 -2.0957718 -1.8492072 -1.8695049 -1.8649809 -1.9076142 -1.9927745 -2.1034956 -2.0764668 -1.9965079 -1.9025695 -1.8679247 -2.091083 -2.6219969 -3.1455188][-3.1645563 -2.9765348 -2.8022447 -2.7178319 -2.5892572 -2.6179287 -2.6945338 -2.8474874 -2.9653854 -2.9615717 -2.873929 -2.9439297 -3.1401441 -3.4528704 -3.7361326][-3.4812813 -3.2540951 -3.1942041 -3.2078135 -3.144531 -3.09861 -3.0667086 -3.1336505 -3.2324607 -3.3572536 -3.4360132 -3.4821754 -3.6076245 -3.8779647 -4.0710611][-3.9040017 -3.6980257 -3.5471482 -3.50818 -3.4758015 -3.4522719 -3.4389393 -3.4579549 -3.4759808 -3.4547105 -3.48655 -3.6686106 -3.8821173 -4.12838 -4.2573905]]...]
INFO - root - 2017-12-15 10:02:19.454945: step 410, loss = 0.72, batch loss = 0.41 (13.0 examples/sec; 0.618 sec/batch; 56h:58m:41s remains)
INFO - root - 2017-12-15 10:02:25.619498: step 420, loss = 0.67, batch loss = 0.36 (13.0 examples/sec; 0.615 sec/batch; 56h:45m:48s remains)
INFO - root - 2017-12-15 10:02:31.752794: step 430, loss = 0.83, batch loss = 0.52 (13.2 examples/sec; 0.604 sec/batch; 55h:44m:43s remains)
INFO - root - 2017-12-15 10:02:37.888932: step 440, loss = 0.85, batch loss = 0.55 (13.0 examples/sec; 0.617 sec/batch; 56h:54m:56s remains)
INFO - root - 2017-12-15 10:02:44.056210: step 450, loss = 0.72, batch loss = 0.42 (12.8 examples/sec; 0.623 sec/batch; 57h:27m:37s remains)
INFO - root - 2017-12-15 10:02:50.202960: step 460, loss = 0.75, batch loss = 0.44 (13.1 examples/sec; 0.611 sec/batch; 56h:20m:02s remains)
INFO - root - 2017-12-15 10:02:56.333538: step 470, loss = 0.83, batch loss = 0.51 (12.6 examples/sec; 0.635 sec/batch; 58h:32m:33s remains)
INFO - root - 2017-12-15 10:03:02.488187: step 480, loss = 0.83, batch loss = 0.49 (13.1 examples/sec; 0.611 sec/batch; 56h:20m:56s remains)
INFO - root - 2017-12-15 10:03:08.631452: step 490, loss = 0.77, batch loss = 0.41 (12.2 examples/sec; 0.654 sec/batch; 60h:20m:07s remains)
INFO - root - 2017-12-15 10:03:14.774772: step 500, loss = 0.91, batch loss = 0.54 (13.1 examples/sec; 0.612 sec/batch; 56h:23m:38s remains)
2017-12-15 10:03:15.234760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0614743 -1.823961 -1.7196229 -1.6979127 -1.6361787 -1.7526896 -1.8455045 -1.8886969 -1.8597503 -1.8084574 -2.0503304 -2.2803855 -2.6225908 -3.0321026 -3.2450519][-2.1222308 -1.8448465 -1.6769595 -1.6895607 -1.6654484 -1.735008 -1.8037589 -1.8422742 -1.8439972 -1.7240689 -1.9118245 -2.2179036 -2.6259398 -3.068378 -3.3003802][-1.8810065 -1.6384892 -1.4479611 -1.4006593 -1.3072693 -1.3808322 -1.4211979 -1.4506643 -1.4246705 -1.387152 -1.7316658 -1.9826975 -2.3455105 -2.8779271 -3.1127386][-1.6158669 -1.2504082 -0.988801 -0.91226149 -0.80807209 -0.86527157 -0.830843 -0.75796103 -0.67080975 -0.66480613 -1.0156379 -1.4942629 -2.0537975 -2.5032821 -2.6798573][-1.1670029 -0.76845384 -0.44437218 -0.31984138 -0.15179062 -0.13314724 -0.091875553 -0.10389757 -0.13222837 -0.094032288 -0.44378519 -0.95739317 -1.5640728 -2.1511898 -2.3859885][-0.62386131 -0.29301238 -0.034635067 0.11081171 0.38251877 0.44073486 0.4747057 0.54280138 0.5612998 0.53428507 0.0098209381 -0.65096855 -1.3556304 -2.0062292 -2.1769235][-0.27597904 0.15393162 0.52487516 0.57124758 0.80434036 0.93015289 1.0571499 1.0802398 1.0654635 0.99891758 0.45399809 -0.26819682 -1.0638235 -1.8273001 -2.1442981][-0.042275906 0.22501421 0.672565 0.89651632 1.1863694 1.2424445 1.3108926 1.4241428 1.5062642 1.3748355 0.72920275 -0.05150032 -0.88001657 -1.7583821 -2.2303107][0.26655626 0.5303421 0.80953741 0.91800785 1.1417317 1.3014398 1.4756985 1.5065846 1.4590831 1.3734221 0.76389408 -0.079648972 -1.0496829 -2.0064518 -2.5445414][-0.59154606 -0.28483558 0.063318729 0.1934514 0.33153868 0.41276026 0.51652 0.588109 0.57202673 0.44907856 -0.12964153 -0.93416166 -1.9001222 -2.8640859 -3.509841][-1.2465343 -1.2924719 -1.3346243 -1.179878 -1.037066 -0.92163658 -0.81670403 -0.77615881 -0.78215647 -0.82920909 -1.3441808 -2.0929253 -2.82823 -3.6014652 -4.2023377][-2.16671 -2.0665321 -2.0283091 -2.0815685 -2.0662472 -1.9412048 -1.8165565 -1.7836533 -1.7964172 -1.8900301 -2.3411298 -2.9543781 -3.612246 -4.364996 -4.8937268][-3.0067329 -3.073369 -3.1525173 -3.0512595 -2.9431152 -2.9062004 -2.8263314 -2.7563064 -2.7151012 -2.7850957 -3.1306415 -3.641124 -4.1856871 -4.8179288 -5.3322477][-3.9466574 -4.0185351 -4.0626845 -4.0338821 -3.9767118 -3.8782449 -3.7577207 -3.7332029 -3.7192955 -3.7439544 -3.9373837 -4.2593813 -4.7205844 -5.1305647 -5.5257225][-4.7353725 -4.8019176 -4.8509068 -4.8605871 -4.8169456 -4.77128 -4.7226071 -4.6488438 -4.5958037 -4.6192927 -4.6620774 -4.8199396 -5.1212687 -5.3435497 -5.4813714]]...]
INFO - root - 2017-12-15 10:03:21.346112: step 510, loss = 0.94, batch loss = 0.55 (13.1 examples/sec; 0.612 sec/batch; 56h:25m:39s remains)
INFO - root - 2017-12-15 10:03:27.478673: step 520, loss = 0.96, batch loss = 0.55 (13.1 examples/sec; 0.608 sec/batch; 56h:06m:28s remains)
INFO - root - 2017-12-15 10:03:33.560292: step 530, loss = 1.08, batch loss = 0.64 (13.0 examples/sec; 0.614 sec/batch; 56h:37m:33s remains)
INFO - root - 2017-12-15 10:03:39.721901: step 540, loss = 0.95, batch loss = 0.46 (12.8 examples/sec; 0.623 sec/batch; 57h:24m:35s remains)
INFO - root - 2017-12-15 10:03:45.759749: step 550, loss = 0.96, batch loss = 0.43 (13.4 examples/sec; 0.596 sec/batch; 54h:58m:43s remains)
INFO - root - 2017-12-15 10:03:51.854091: step 560, loss = 0.95, batch loss = 0.40 (13.3 examples/sec; 0.601 sec/batch; 55h:26m:55s remains)
INFO - root - 2017-12-15 10:03:57.990807: step 570, loss = 1.07, batch loss = 0.50 (13.0 examples/sec; 0.616 sec/batch; 56h:50m:16s remains)
INFO - root - 2017-12-15 10:04:04.057228: step 580, loss = 0.97, batch loss = 0.38 (13.1 examples/sec; 0.612 sec/batch; 56h:25m:08s remains)
INFO - root - 2017-12-15 10:04:10.154121: step 590, loss = 1.07, batch loss = 0.47 (13.1 examples/sec; 0.608 sec/batch; 56h:05m:34s remains)
INFO - root - 2017-12-15 10:04:16.248472: step 600, loss = 1.13, batch loss = 0.51 (12.9 examples/sec; 0.622 sec/batch; 57h:20m:18s remains)
2017-12-15 10:04:16.688990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.40538931 -0.21273971 -0.075435162 -0.010813236 0.067346573 -0.0095539093 -0.078709126 -0.14223099 -0.23361564 -0.22572017 -0.48810554 -0.78930616 -1.1492825 -1.4864266 -1.7380483][0.020645618 0.29425049 0.48576975 0.53464079 0.58343935 0.54836655 0.47704697 0.39261961 0.3027997 0.3274703 0.081555843 -0.22319126 -0.57211471 -0.97781277 -1.281487][0.54108381 0.83657503 0.99293232 1.0687523 1.1150565 1.0835161 1.0165868 0.94813967 0.87703276 0.83648872 0.53139067 0.20537376 -0.16824722 -0.54427648 -0.8169415][1.0656152 1.3413563 1.4962206 1.5387883 1.5499835 1.4954052 1.4337358 1.3643069 1.2944989 1.2836385 0.98545313 0.63585949 0.25328875 -0.13204575 -0.42436314][1.2020788 1.5181766 1.6859875 1.6645675 1.6486144 1.6425266 1.6317482 1.6211886 1.6095095 1.6007686 1.2854609 0.88559961 0.4509902 0.0030040741 -0.3496592][1.2124062 1.6119485 1.7876935 1.7535725 1.7312393 1.7203898 1.7105327 1.69559 1.6838284 1.6720343 1.3590627 0.9425745 0.51224136 0.06740427 -0.27562547][1.3141928 1.6539845 1.8397202 1.823257 1.8025684 1.7871017 1.7680755 1.7538548 1.7436357 1.7351918 1.4498243 1.0428758 0.59553528 0.14658403 -0.20235491][1.2241116 1.6007271 1.8667636 1.8523626 1.8368516 1.8302255 1.8163576 1.7931056 1.7708311 1.7629395 1.4875364 1.0942407 0.65649652 0.20983171 -0.15798664][1.0679131 1.5380254 1.7904601 1.7872772 1.8177686 1.8097358 1.8001847 1.7885919 1.771884 1.749095 1.4700618 1.0805917 0.65685987 0.22424603 -0.13379622][0.66260958 0.93250084 1.0383744 1.0895653 1.1482549 1.1448369 1.1370702 1.1232753 1.1059532 1.0880928 0.89465904 0.59124327 0.27728987 -0.032803535 -0.35120535][0.292593 0.54873657 0.70815134 0.66856527 0.64972973 0.64909554 0.64255905 0.62559223 0.605741 0.58875179 0.4138093 0.13932371 -0.12788963 -0.39957547 -0.6203351][-0.5151484 -0.20389223 -0.11074018 -0.1838851 -0.17167711 -0.17259741 -0.18016052 -0.19264746 -0.20847607 -0.22446728 -0.30494285 -0.47137237 -0.65137625 -0.82577395 -0.96548343][-0.99514842 -0.93770289 -0.9192698 -0.88669586 -0.84480596 -0.84755373 -0.85314703 -0.86477828 -0.87774539 -0.88845563 -0.93747568 -1.0162191 -1.0952377 -1.1627762 -1.2389553][-1.4862747 -1.3649507 -1.44642 -1.5103726 -1.5267506 -1.5265102 -1.5298076 -1.5365107 -1.5431991 -1.5517609 -1.5640595 -1.5349085 -1.4960065 -1.4290223 -1.3728571][-1.8493288 -1.7041252 -1.713268 -1.832921 -1.9004567 -1.8997824 -1.9000263 -1.9026721 -1.9026039 -1.9022989 -1.8653483 -1.7911296 -1.717273 -1.6322544 -1.5623593]]...]
INFO - root - 2017-12-15 10:04:22.741504: step 610, loss = 1.10, batch loss = 0.47 (13.3 examples/sec; 0.600 sec/batch; 55h:17m:30s remains)
INFO - root - 2017-12-15 10:04:28.846506: step 620, loss = 1.07, batch loss = 0.44 (13.2 examples/sec; 0.607 sec/batch; 55h:58m:38s remains)
INFO - root - 2017-12-15 10:04:34.982205: step 630, loss = 1.15, batch loss = 0.52 (13.3 examples/sec; 0.602 sec/batch; 55h:30m:27s remains)
INFO - root - 2017-12-15 10:04:41.146621: step 640, loss = 1.06, batch loss = 0.43 (13.0 examples/sec; 0.617 sec/batch; 56h:50m:36s remains)
INFO - root - 2017-12-15 10:04:47.267797: step 650, loss = 1.09, batch loss = 0.46 (13.2 examples/sec; 0.604 sec/batch; 55h:41m:58s remains)
INFO - root - 2017-12-15 10:04:53.323956: step 660, loss = 1.02, batch loss = 0.39 (13.0 examples/sec; 0.616 sec/batch; 56h:48m:54s remains)
INFO - root - 2017-12-15 10:04:59.393670: step 670, loss = 2.36, batch loss = 1.72 (13.4 examples/sec; 0.597 sec/batch; 55h:02m:44s remains)
INFO - root - 2017-12-15 10:05:05.467473: step 680, loss = 1.41, batch loss = 0.76 (13.1 examples/sec; 0.609 sec/batch; 56h:08m:35s remains)
INFO - root - 2017-12-15 10:05:11.568169: step 690, loss = 1.20, batch loss = 0.55 (13.3 examples/sec; 0.603 sec/batch; 55h:32m:43s remains)
INFO - root - 2017-12-15 10:05:17.612874: step 700, loss = 1.13, batch loss = 0.49 (13.4 examples/sec; 0.598 sec/batch; 55h:07m:16s remains)
2017-12-15 10:05:18.066469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.15412998 0.015371323 0.16209888 0.027349472 -0.11585402 -0.14214563 -0.0368433 -0.14427018 -0.36078954 -0.24944258 -1.1468761 -2.0390053 -2.858882 -3.5629056 -3.6428471][-0.025625229 -0.27580619 -0.47826076 -0.44779015 -0.35867524 -0.48243904 -0.65215039 -0.51568317 -0.38806748 -0.47934723 -1.5849504 -2.5799384 -3.4443429 -4.2083597 -4.3410225][0.22123098 -0.10556269 -0.35299563 -0.33921957 -0.27027535 -0.20714712 -0.13722849 -0.17249131 -0.27207923 -0.32086086 -1.3585103 -2.4671688 -3.4111166 -4.2146726 -4.3763738][0.54634714 0.22879362 -0.052099705 -0.040448666 0.010037899 0.061177731 0.09112978 0.091181278 0.078062057 -0.027977228 -1.1629839 -2.2658296 -3.1950622 -4.0013075 -4.1549497][0.65800381 0.34136868 0.080392361 0.10353708 0.13181877 0.19469786 0.247056 0.22752762 0.17049837 0.082036495 -0.99155068 -2.1393175 -3.090076 -3.8697016 -4.01335][0.67218781 0.3983674 0.15933752 0.21262074 0.28229237 0.33457804 0.38112831 0.38515806 0.34797573 0.21548319 -0.92843556 -2.063602 -2.9942558 -3.7553091 -3.8475177][0.52085066 0.2772541 0.12327194 0.19030142 0.25871754 0.31947422 0.35707474 0.34518909 0.27353859 0.16099739 -0.95396733 -2.061548 -2.9765012 -3.7399359 -3.8705766][0.2396965 0.14586973 0.00046396255 0.088520527 0.16429138 0.20437908 0.21832848 0.21664 0.17698002 0.10471725 -0.94046354 -2.0018229 -2.914053 -3.7177849 -3.9116647][-0.10100198 -0.14537024 -0.13588548 -0.038209915 0.037619114 0.053376675 0.044626713 0.047897339 0.059431553 0.10126591 -0.84460878 -1.8663461 -2.7889993 -3.6748075 -3.9708884][-2.012063 -2.1677842 -2.1760359 -2.0059257 -1.9552813 -1.983674 -2.0078468 -1.9903595 -1.908788 -1.7649026 -2.3358166 -2.9931068 -3.5841212 -4.195282 -4.5001812][-3.5013213 -4.0039115 -4.4178705 -4.2777948 -4.2214627 -4.2390866 -4.2448816 -4.2125306 -4.0929809 -3.9209323 -4.1511412 -4.6751809 -4.8594494 -4.9817276 -5.1111917][-4.357801 -4.8794956 -5.2993908 -5.1637049 -5.0755029 -5.0716591 -5.0624456 -4.9792094 -4.8559027 -4.7518244 -4.6544352 -4.8817463 -5.0664988 -5.2076492 -5.3030553][-4.1003327 -4.6267886 -5.0649614 -4.9619493 -4.8750677 -4.8207741 -4.7631092 -4.7019219 -4.6314611 -4.6031113 -4.6047459 -4.8320718 -4.9895897 -5.1249776 -5.2101426][-3.842397 -4.3794379 -4.8240213 -4.7165585 -4.64928 -4.6109242 -4.5681858 -4.517611 -4.4796796 -4.4720874 -4.4850264 -4.5833206 -4.6168208 -4.6372604 -4.6140537][-3.496423 -3.8467176 -4.084897 -3.9736314 -3.8951983 -3.8614118 -3.825011 -3.7957873 -3.7833271 -3.7859075 -3.8098836 -3.8845425 -4.0279889 -3.9606378 -3.8801942]]...]
INFO - root - 2017-12-15 10:05:24.095836: step 710, loss = 1.06, batch loss = 0.41 (13.5 examples/sec; 0.591 sec/batch; 54h:30m:43s remains)
INFO - root - 2017-12-15 10:05:30.076606: step 720, loss = 1.04, batch loss = 0.40 (13.3 examples/sec; 0.603 sec/batch; 55h:36m:53s remains)
INFO - root - 2017-12-15 10:05:36.134883: step 730, loss = 0.99, batch loss = 0.34 (13.1 examples/sec; 0.609 sec/batch; 56h:09m:09s remains)
INFO - root - 2017-12-15 10:05:42.231163: step 740, loss = 0.95, batch loss = 0.31 (13.4 examples/sec; 0.598 sec/batch; 55h:07m:05s remains)
INFO - root - 2017-12-15 10:05:48.287361: step 750, loss = 0.91, batch loss = 0.27 (13.3 examples/sec; 0.601 sec/batch; 55h:25m:01s remains)
INFO - root - 2017-12-15 10:05:54.388709: step 760, loss = 0.92, batch loss = 0.28 (12.9 examples/sec; 0.619 sec/batch; 57h:04m:53s remains)
INFO - root - 2017-12-15 10:06:00.487304: step 770, loss = 1.06, batch loss = 0.42 (13.0 examples/sec; 0.616 sec/batch; 56h:47m:01s remains)
INFO - root - 2017-12-15 10:06:06.499838: step 780, loss = 0.99, batch loss = 0.35 (13.4 examples/sec; 0.598 sec/batch; 55h:07m:35s remains)
INFO - root - 2017-12-15 10:06:12.548748: step 790, loss = 0.95, batch loss = 0.31 (13.0 examples/sec; 0.614 sec/batch; 56h:36m:01s remains)
INFO - root - 2017-12-15 10:06:18.624686: step 800, loss = 0.96, batch loss = 0.32 (13.4 examples/sec; 0.599 sec/batch; 55h:08m:49s remains)
2017-12-15 10:06:19.076855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5044122 -4.4124489 -4.3878927 -4.4139771 -4.0418768 -4.048315 -4.0596142 -3.9259081 -3.9301939 -3.846204 -4.1378632 -4.5889945 -4.9506993 -5.3393335 -5.6651564][-4.0739441 -4.3515611 -4.0089221 -3.8314977 -3.6517611 -3.6627448 -3.4788694 -3.4612956 -3.4376557 -3.41082 -3.8607879 -4.4963269 -5.0565186 -5.4398313 -5.8203206][-3.8324702 -4.0461087 -3.6447933 -3.2254143 -2.697382 -2.6402738 -2.2908881 -2.2458167 -2.2409668 -2.2960939 -2.7604728 -3.4566414 -4.0938854 -4.7066951 -5.3002768][-4.0661526 -3.8620348 -3.3348877 -2.7570784 -2.1775784 -1.9390588 -1.4860017 -1.4626431 -1.5150092 -1.5484128 -1.927717 -2.638134 -3.300581 -4.0228047 -4.8050885][-3.5852296 -3.4501104 -2.9462819 -2.4101977 -1.5762131 -1.2097883 -0.74561381 -0.83821964 -1.1314037 -1.1338177 -1.5538514 -2.3273866 -3.0364776 -3.5770876 -4.159328][-2.5369453 -2.8958027 -2.5090179 -1.8304391 -0.98952556 -0.33147955 0.55408621 0.54707241 0.36107397 0.29668045 -0.2756207 -1.211123 -2.1562223 -2.7462635 -3.2713165][-2.2288671 -2.5694382 -1.7983673 -1.2177491 -0.0852952 0.70086718 2.0130048 2.1583405 2.1155744 2.0033908 1.4538736 0.52925682 -0.34655976 -1.0066822 -1.6958594][-2.6626468 -2.2801557 -1.9780359 -1.217824 -0.099239349 0.75377464 1.9720049 2.1281633 2.152967 2.0525494 1.5055208 0.64347315 -0.2612536 -0.91192031 -1.5940635][-2.4280338 -2.4521217 -1.4845166 -0.8651247 -0.25113106 0.52159548 1.7792535 1.8808632 1.8395643 1.8853192 1.4103508 0.58122826 -0.18791556 -0.93169093 -1.7017558][-1.4032755 -1.8432479 -2.1185822 -1.7315688 -0.63116169 0.13572168 1.2240086 1.3134112 1.4021454 1.403832 0.9082613 0.23134041 -0.50437641 -1.1227276 -1.764627][-2.6971626 -2.6453481 -2.8720341 -2.7194757 -1.896589 -1.2242999 -0.017991066 0.087069035 0.00064468384 0.061510563 -0.045871258 -0.72063923 -1.451587 -1.6996248 -2.0678692][-2.9721861 -2.7341352 -2.5897033 -2.1462288 -1.3768952 -0.73883271 0.24006748 0.27999878 0.20642948 0.17786789 0.046954155 -0.2413044 -1.1159034 -1.4119847 -1.4390409][-3.593148 -3.2718873 -2.14597 -1.7150574 -1.4984982 -0.98050117 0.11880064 0.11748934 0.02640295 0.12952042 -0.037301064 -0.21705389 -0.32335806 -0.52690554 -1.0746272][-3.5550036 -4.2295384 -3.8513241 -3.2958965 -2.5389714 -2.1181736 -1.1898253 -1.1805525 -1.1086607 -1.0996916 -1.1487432 -1.1863844 -1.2831643 -1.5141041 -1.7863991][-4.8844666 -4.9429083 -5.2110643 -4.8569708 -4.7873306 -4.3397007 -3.3965132 -3.4682887 -3.5512252 -3.3678093 -3.0471249 -3.2571266 -3.3827574 -3.0654869 -2.9148479]]...]
INFO - root - 2017-12-15 10:06:25.182588: step 810, loss = 0.94, batch loss = 0.30 (13.0 examples/sec; 0.613 sec/batch; 56h:30m:55s remains)
INFO - root - 2017-12-15 10:06:31.225905: step 820, loss = 0.99, batch loss = 0.35 (13.2 examples/sec; 0.605 sec/batch; 55h:42m:53s remains)
INFO - root - 2017-12-15 10:06:37.221916: step 830, loss = 1.27, batch loss = 0.63 (13.3 examples/sec; 0.603 sec/batch; 55h:34m:01s remains)
INFO - root - 2017-12-15 10:06:43.257556: step 840, loss = 1.11, batch loss = 0.47 (13.3 examples/sec; 0.604 sec/batch; 55h:36m:23s remains)
INFO - root - 2017-12-15 10:06:49.315744: step 850, loss = 1.08, batch loss = 0.44 (13.2 examples/sec; 0.605 sec/batch; 55h:44m:57s remains)
INFO - root - 2017-12-15 10:06:55.391202: step 860, loss = 0.96, batch loss = 0.33 (13.4 examples/sec; 0.598 sec/batch; 55h:04m:19s remains)
INFO - root - 2017-12-15 10:07:01.437109: step 870, loss = 1.03, batch loss = 0.39 (13.5 examples/sec; 0.591 sec/batch; 54h:25m:31s remains)
INFO - root - 2017-12-15 10:07:07.521467: step 880, loss = 1.00, batch loss = 0.36 (13.2 examples/sec; 0.606 sec/batch; 55h:46m:58s remains)
INFO - root - 2017-12-15 10:07:13.602411: step 890, loss = 0.95, batch loss = 0.32 (13.3 examples/sec; 0.604 sec/batch; 55h:36m:50s remains)
INFO - root - 2017-12-15 10:07:19.683870: step 900, loss = 0.99, batch loss = 0.36 (13.1 examples/sec; 0.610 sec/batch; 56h:10m:26s remains)
2017-12-15 10:07:20.147103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.584908 -1.6210945 -1.8996432 -2.0190413 -2.0609982 -2.1756027 -2.3349619 -2.4373569 -2.5338507 -2.6803682 -3.6765516 -5.2369061 -6.4286351 -7.29652 -7.9200511][-2.0830567 -2.2602935 -1.5822515 -1.5884762 -2.0229802 -2.1478972 -2.2640467 -2.3356032 -2.4112902 -2.5400743 -3.5154729 -5.0602989 -6.2125187 -7.1073194 -7.7589617][-1.7431681 -2.5729485 -2.328526 -2.1422896 -1.7101912 -1.6748745 -1.774035 -1.8304048 -1.8398142 -1.8794949 -2.8206229 -4.3042679 -5.3401556 -6.3414826 -7.1317968][-1.446908 -2.168014 -1.8082719 -1.4022114 -0.95586586 -0.85241127 -0.73880291 -0.75936532 -0.8622241 -0.8228693 -1.6008518 -3.0666132 -3.93447 -4.8227448 -5.7384448][-1.3682213 -1.8966086 -1.7298629 -1.1678364 -0.48536706 -0.36705685 -0.25807357 -0.22819543 -0.14797354 -0.18323994 -1.0874352 -2.4656329 -3.3715911 -4.1853447 -4.7399645][-1.0897615 -0.8919816 -1.2140002 -0.59121943 0.512805 0.86486387 1.0191264 1.0343533 1.0782375 1.1182361 0.29748917 -1.1717308 -2.207015 -2.9491072 -3.501781][-1.175523 -1.0026028 -0.93225765 -0.20163631 0.95810986 1.6679544 2.0011196 2.0392289 2.065639 2.0875573 1.2753448 -0.15714407 -1.206301 -2.0421853 -2.642724][-0.89939022 -0.96556258 -1.2020617 -0.36284828 0.9081521 1.2867417 2.0338492 2.0582552 2.0944362 2.1002054 1.2579556 -0.20033312 -1.2907622 -2.1518202 -2.694283][-0.87156224 -0.70509887 -0.82806945 -0.20666528 0.8224225 1.2836456 2.0233922 2.0105562 2.0131569 1.9999623 1.1620111 -0.31587815 -1.4573185 -2.4183371 -2.9996958][-1.1338458 -1.1093507 -1.0270665 -0.38549876 0.62944651 1.0043945 1.7239604 1.711525 1.7038698 1.696692 1.0372505 -0.16427517 -1.2647474 -2.2089305 -2.8432753][-2.0536869 -2.5633161 -2.1925182 -1.5480354 -0.35298252 -0.034107208 0.7185874 0.54162073 0.39716816 0.59161186 0.24974632 -0.65146732 -1.4898331 -2.3350229 -3.0632942][-2.9678226 -2.6060543 -2.3969855 -2.1276746 -1.378108 -0.94582248 -0.26290631 -0.39413548 -0.50271058 -0.48138738 -0.8025651 -1.4734418 -1.9960425 -2.3845758 -2.7544503][-3.1612887 -2.7242074 -1.4913189 -1.0738919 -0.84771252 -0.59391522 -0.0069098473 -0.153368 -0.37835217 -0.40230465 -0.47788858 -0.7681334 -1.0029669 -1.2394321 -1.6355543][-4.6121559 -3.9714079 -3.3011248 -2.8301103 -1.969485 -1.7867584 -1.143018 -1.2670972 -1.4784262 -1.4020264 -1.2019284 -1.015729 -1.0374122 -1.2707109 -1.5521133][-5.4754539 -4.8701849 -4.1139321 -3.6439857 -2.9441733 -2.6466699 -2.0252762 -2.1902549 -2.4642582 -2.4028807 -2.1830986 -2.2977233 -2.6205561 -2.5278234 -2.5680552]]...]
INFO - root - 2017-12-15 10:07:26.244496: step 910, loss = 1.00, batch loss = 0.37 (13.0 examples/sec; 0.613 sec/batch; 56h:28m:30s remains)
INFO - root - 2017-12-15 10:07:32.327831: step 920, loss = 0.99, batch loss = 0.35 (13.2 examples/sec; 0.605 sec/batch; 55h:45m:16s remains)
INFO - root - 2017-12-15 10:07:38.402974: step 930, loss = 0.98, batch loss = 0.35 (13.2 examples/sec; 0.607 sec/batch; 55h:54m:18s remains)
INFO - root - 2017-12-15 10:07:44.452493: step 940, loss = 0.98, batch loss = 0.35 (13.2 examples/sec; 0.605 sec/batch; 55h:43m:40s remains)
INFO - root - 2017-12-15 10:07:50.491171: step 950, loss = 0.95, batch loss = 0.32 (12.9 examples/sec; 0.618 sec/batch; 56h:54m:35s remains)
INFO - root - 2017-12-15 10:07:56.579767: step 960, loss = 0.94, batch loss = 0.31 (13.4 examples/sec; 0.598 sec/batch; 55h:02m:34s remains)
INFO - root - 2017-12-15 10:08:02.591258: step 970, loss = 0.95, batch loss = 0.33 (13.4 examples/sec; 0.596 sec/batch; 54h:54m:15s remains)
INFO - root - 2017-12-15 10:08:08.625574: step 980, loss = 0.92, batch loss = 0.29 (13.1 examples/sec; 0.610 sec/batch; 56h:11m:00s remains)
INFO - root - 2017-12-15 10:08:14.689846: step 990, loss = 0.93, batch loss = 0.30 (13.5 examples/sec; 0.592 sec/batch; 54h:31m:49s remains)
INFO - root - 2017-12-15 10:08:20.757309: step 1000, loss = 0.93, batch loss = 0.30 (13.3 examples/sec; 0.599 sec/batch; 55h:11m:33s remains)
2017-12-15 10:08:21.222512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9890182 -1.1209538 -1.2626696 -1.2850702 -1.286665 -1.2742958 -1.2858331 -1.3794355 -1.4699411 -1.4678226 -1.3379331 -2.1461129 -2.7597175 -3.6826231 -4.4820085][-1.7507894 -1.8857987 -1.8562036 -1.997601 -2.0177746 -2.0662982 -2.046706 -2.0250578 -2.1159554 -2.1371593 -1.8290429 -2.4984186 -3.0784976 -3.5722768 -4.1403375][-2.7066784 -2.712193 -2.5388403 -2.5352547 -2.4185565 -2.4375994 -2.4668498 -2.4729853 -2.4730585 -2.4856515 -2.4767671 -3.0288458 -3.1323004 -3.57106 -3.9095027][-2.5666397 -2.5481396 -2.1660485 -2.1707029 -2.1143239 -2.0131528 -1.9498904 -1.9961634 -2.0533969 -2.1195526 -1.8628254 -2.5376163 -3.0968475 -3.6061368 -3.7912035][-2.2863138 -2.4492311 -2.0924776 -2.0387452 -1.9653172 -2.029717 -2.0686822 -2.0493786 -2.051445 -1.9577405 -1.9006839 -2.6613159 -2.7409353 -3.16057 -3.9044175][-1.6132 -1.4406023 -0.85039663 -0.64432979 -0.25029564 -0.17695045 -0.1406765 -0.21876097 -0.27559757 -0.029199123 0.12161398 -0.81868196 -1.401288 -2.0890489 -2.5090885][-1.3570814 -1.0470674 -0.33302689 0.13936758 0.67639494 0.90429449 1.2037177 1.1463752 1.0844889 1.0173101 0.9201889 0.22911835 -0.29897022 -1.1503792 -1.8801863][-1.2518568 -0.92947984 -0.21218395 0.29960966 0.90787983 1.1692662 1.4255376 1.3839898 1.2746329 1.2920036 1.3776274 0.23181009 -0.6140759 -1.1413274 -1.7836564][-0.98320436 -0.6586287 -0.016666889 0.43540287 1.0349498 1.3081927 1.5794787 1.5015235 1.4039941 1.3864269 1.3798943 0.50627375 -0.069405556 -1.0897911 -2.2403903][-2.6383929 -2.3976848 -1.795136 -1.4411118 -0.96734858 -0.70926332 -0.46102071 -0.47563052 -0.58056378 -0.58108711 -0.20632267 -0.5617249 -0.6134851 -0.8261559 -1.2604136][-4.6581874 -4.5140095 -3.9555097 -3.6728857 -3.0767283 -2.7494955 -2.4520352 -2.4452353 -2.5368686 -2.5741808 -2.4368529 -2.3078353 -2.1088886 -1.863781 -1.6804914][-4.1953173 -4.0203867 -3.5697489 -3.0760713 -2.4339554 -2.0467935 -1.774287 -1.6744494 -1.6846745 -1.8091774 -1.6935258 -1.7600219 -1.8098221 -1.7430184 -1.5916867][-4.030982 -3.8335624 -3.1367922 -2.6852732 -1.973778 -1.5781896 -1.2159002 -1.0558558 -0.9502492 -1.0969944 -1.0450959 -0.98593187 -0.96040058 -0.88824034 -0.79274249][-4.9926882 -5.004406 -4.5969205 -4.2357459 -3.4234238 -3.0138435 -2.7155809 -2.609199 -2.5755868 -2.3514855 -2.2197375 -1.7853587 -1.3713517 -0.93827677 -0.55020666][-5.2101045 -4.8783393 -4.4515386 -4.0728297 -3.4266627 -2.808897 -2.3414993 -2.3185427 -2.333359 -2.4546218 -2.4694471 -2.1409011 -1.7083879 -1.1552761 -0.63744307]]...]
INFO - root - 2017-12-15 10:08:27.330500: step 1010, loss = 0.92, batch loss = 0.29 (13.2 examples/sec; 0.606 sec/batch; 55h:47m:13s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-6-6-val-clipgradient100
INFO - root - 2017-12-15 10:08:33.342914: step 1020, loss = 0.90, batch loss = 0.28 (13.3 examples/sec; 0.600 sec/batch; 55h:16m:32s remains)
INFO - root - 2017-12-15 10:08:39.358310: step 1030, loss = 0.89, batch loss = 0.27 (13.3 examples/sec; 0.601 sec/batch; 55h:19m:53s remains)
INFO - root - 2017-12-15 10:08:45.380595: step 1040, loss = 0.91, batch loss = 0.28 (13.0 examples/sec; 0.614 sec/batch; 56h:29m:31s remains)
INFO - root - 2017-12-15 10:08:51.408912: step 1050, loss = 0.91, batch loss = 0.28 (13.1 examples/sec; 0.613 sec/batch; 56h:25m:18s remains)
INFO - root - 2017-12-15 10:08:57.512290: step 1060, loss = 0.89, batch loss = 0.27 (13.3 examples/sec; 0.602 sec/batch; 55h:27m:55s remains)
INFO - root - 2017-12-15 10:09:03.523690: step 1070, loss = 0.87, batch loss = 0.24 (13.1 examples/sec; 0.609 sec/batch; 56h:03m:23s remains)
INFO - root - 2017-12-15 10:09:09.564214: step 1080, loss = 0.86, batch loss = 0.24 (13.4 examples/sec; 0.598 sec/batch; 55h:05m:17s remains)
INFO - root - 2017-12-15 10:09:15.598047: step 1090, loss = 0.87, batch loss = 0.25 (13.3 examples/sec; 0.604 sec/batch; 55h:34m:31s remains)
INFO - root - 2017-12-15 10:09:21.633600: step 1100, loss = 0.88, batch loss = 0.26 (13.2 examples/sec; 0.606 sec/batch; 55h:46m:49s remains)
2017-12-15 10:09:22.091402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.43296 -2.3660409 -2.25066 -2.3798306 -2.4943445 -2.4788017 -2.4704711 -2.4612198 -2.4876444 -2.9851284 -3.1655998 -3.2675252 -3.0504189 -3.0384889 -3.3978968][-3.0362597 -3.3280942 -3.5647326 -3.7953267 -3.9649272 -3.8565934 -3.732358 -3.7420931 -3.7738044 -4.2620554 -4.4498682 -4.5449471 -4.3352613 -4.3283896 -4.3213878][-3.9997118 -4.3396597 -4.4386158 -4.5148854 -4.432147 -4.1943059 -3.9634695 -3.9522047 -3.9828515 -4.0290389 -4.2234249 -4.8281078 -4.5810809 -4.6345186 -4.5977569][-3.9590673 -4.2377596 -4.3534031 -4.463182 -4.389852 -4.2078133 -4.0054326 -3.9810438 -3.990222 -4.4802661 -4.6891546 -4.3665161 -4.73434 -4.7609158 -4.7082973][-3.6074886 -3.9196353 -3.8227532 -3.7479005 -3.4947124 -3.3522153 -3.199688 -3.1923313 -3.2282062 -3.7240443 -3.9238348 -4.0555196 -4.0809045 -4.0659289 -4.2849016][-2.6371984 -2.7205057 -2.1906002 -1.7653267 -1.0605958 -0.70031285 -0.40620565 -0.50381756 -0.59350824 -1.1043489 -1.2984011 -1.4713819 -1.495703 -1.7717714 -2.250823][-1.6838474 -1.6563439 -1.0088651 -0.3429141 0.59737206 1.1477985 1.6142573 1.4791493 1.3132262 0.75017691 0.47271729 0.28003359 0.19346046 -0.23811865 -0.86617875][-1.6097746 -1.586767 -0.9358182 -0.29364729 0.61717892 1.1479559 1.6375771 1.5417919 1.3917956 0.79859447 0.56752491 0.28854847 0.047668934 -0.26267052 -0.93433571][-1.6579134 -1.6948249 -1.0856476 -0.46754503 0.40442467 0.93448305 1.5094528 1.5231853 1.44448 0.84853077 0.5818882 0.35331154 0.26694393 -0.17349815 -0.77882385][-2.1822245 -2.2339437 -1.6432879 -1.2201195 -0.52605438 -0.0097961426 0.61229467 0.69552422 0.66202879 0.048727036 -0.016676903 -0.74976659 -0.87639785 -1.1923811 -1.7319667][-4.4381323 -4.4165549 -3.3925142 -3.1940222 -2.7210021 -2.2033973 -2.0954571 -2.0231757 -2.0430648 -2.1572881 -2.1503561 -2.422524 -2.5164471 -2.7724118 -2.7993927][-4.354167 -4.0915537 -3.6714537 -3.4142444 -2.3117959 -1.9244409 -1.4812696 -1.4780009 -1.5258348 -1.9845245 -1.8941627 -2.0768065 -2.2121456 -2.4997969 -2.5673428][-4.4553528 -4.4778562 -3.980515 -3.5320137 -2.8386919 -2.3298266 -1.3922172 -1.4415452 -1.4845424 -1.5092862 -1.9543078 -2.1648598 -2.2189651 -2.4810755 -2.4693558][-5.044631 -5.0698471 -4.5515971 -4.0796347 -3.2837238 -2.6674323 -2.1716127 -2.1839433 -2.173383 -1.7207181 -2.1554117 -2.2926497 -2.4914837 -2.6167185 -2.4870746][-4.7747245 -4.5915866 -4.0401454 -3.6542964 -2.9127169 -2.3524346 -1.9214017 -1.9512925 -1.9290423 -1.8402903 -1.7696903 -1.7408347 -1.9509399 -2.0621903 -1.9233806]]...]
INFO - root - 2017-12-15 10:09:28.117952: step 1110, loss = 0.88, batch loss = 0.26 (13.4 examples/sec; 0.596 sec/batch; 54h:52m:39s remains)
INFO - root - 2017-12-15 10:09:34.146236: step 1120, loss = 0.85, batch loss = 0.23 (13.3 examples/sec; 0.600 sec/batch; 55h:12m:02s remains)
INFO - root - 2017-12-15 10:09:40.164264: step 1130, loss = 0.86, batch loss = 0.24 (13.4 examples/sec; 0.597 sec/batch; 54h:57m:52s remains)
INFO - root - 2017-12-15 10:09:46.242845: step 1140, loss = 0.85, batch loss = 0.23 (13.4 examples/sec; 0.597 sec/batch; 54h:56m:26s remains)
INFO - root - 2017-12-15 10:09:52.269406: step 1150, loss = 0.86, batch loss = 0.25 (13.3 examples/sec; 0.601 sec/batch; 55h:16m:27s remains)
INFO - root - 2017-12-15 10:09:58.326581: step 1160, loss = 0.85, batch loss = 0.24 (13.2 examples/sec; 0.604 sec/batch; 55h:37m:30s remains)
INFO - root - 2017-12-15 10:10:04.371140: step 1170, loss = 0.88, batch loss = 0.26 (13.4 examples/sec; 0.595 sec/batch; 54h:47m:38s remains)
INFO - root - 2017-12-15 10:10:10.464433: step 1180, loss = 0.88, batch loss = 0.27 (13.4 examples/sec; 0.597 sec/batch; 54h:58m:43s remains)
INFO - root - 2017-12-15 10:10:16.536752: step 1190, loss = 0.88, batch loss = 0.26 (13.3 examples/sec; 0.604 sec/batch; 55h:33m:55s remains)
INFO - root - 2017-12-15 10:10:22.606971: step 1200, loss = 0.86, batch loss = 0.25 (13.0 examples/sec; 0.616 sec/batch; 56h:41m:18s remains)
2017-12-15 10:10:23.082189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.29701805 -0.17433167 0.075914383 0.055174351 0.11104631 0.11105633 0.11106014 -0.01639986 -0.14436579 -1.331686 -2.2098224 -2.4923766 -2.2963061 -1.1994479 -3.0221][-1.827168 -1.7044947 -1.9086306 -2.0571644 -2.1100497 -2.0808587 -2.0516307 -2.0521972 -2.0523181 -3.3588634 -4.3115129 -4.5437822 -4.6029916 -5.0631337 -4.8507876][-2.7848177 -3.1164436 -3.4483066 -3.7054462 -3.8572068 -3.8573749 -3.7155182 -3.657315 -3.5994551 -3.3875186 -4.2121024 -6.0695577 -6.2825274 -6.357461 -5.7114305][-3.9810927 -3.9670875 -3.95338 -3.8551092 -3.6513696 -3.6224153 -3.5934925 -3.4812675 -3.2274985 -4.3471756 -4.9507995 -3.9560766 -4.1528597 -5.6301808 -5.1127191][-3.4062033 -3.3633256 -3.14821 -2.8497062 -2.4445984 -2.4446492 -2.4450352 -2.4166551 -2.3882284 -3.4512935 -4.0220318 -4.3054776 -4.065805 -2.7510257 -3.7627823][-2.0771413 -1.8515379 -1.4842305 -1.0322521 -0.54115391 -0.54127026 -0.541687 -0.54271364 -0.5439229 -1.7263587 -2.5560818 -2.6853476 -2.6098254 -2.6254206 -1.9718077][-0.43302774 -0.23755217 0.099813461 0.52186155 1.2118373 1.2125788 1.212184 1.2110639 1.2092743 0.030184746 -0.81391716 -0.33523703 -0.4709959 -1.3920445 -1.0110862][-0.43494225 -0.23930216 0.097976208 0.51995039 1.2099242 1.2104225 1.2103062 1.2097406 1.2082295 -0.0066533089 -0.86399674 -1.1212134 -1.2068233 -0.59237766 -0.84178829][-0.525857 -0.22894287 0.095344067 0.51693773 1.2066836 1.2071776 1.2072234 1.2063866 1.2059259 0.028161526 -0.78755164 -1.0856602 -1.1902852 -1.4254861 -1.1701303][-0.62387657 -0.29624224 0.022850037 0.25845385 0.76174736 0.76176882 0.76178169 0.76161003 0.76196241 -0.41573811 -0.34729958 -2.2005949 -2.3067062 -2.4401393 -2.0778027][-4.651104 -4.3156815 -4.174623 -4.1251688 -3.8185856 -3.8317187 -3.8319159 -3.8319318 -3.8319998 -3.7620866 -3.6855555 -3.9894607 -4.1169629 -4.3125076 -3.5457635][-6.2883024 -6.1544971 -6.1104531 -6.2570782 -6.1337948 -6.1387062 -6.138689 -6.1386414 -6.1386905 -6.0688238 -5.9926906 -6.341258 -6.4554982 -6.1474471 -5.0465856][-6.1736832 -5.9784942 -5.9331245 -6.000936 -5.7916937 -5.7966156 -5.7966309 -5.7964649 -5.7963328 -5.7263527 -5.6426129 -5.9233871 -5.969882 -5.7366314 -4.615314][-5.1294308 -5.9021754 -5.670959 -5.7336869 -5.4806948 -5.3964958 -5.3202024 -5.320045 -5.3198433 -5.28478 -5.2360549 -5.6650295 -5.8820744 -5.5165071 -4.36055][-6.4620581 -6.3323035 -6.2324781 -5.97656 -5.49833 -5.3847766 -5.2625122 -5.2624 -5.2622094 -5.2270446 -5.1920986 -5.4198089 -5.5636835 -5.2670765 -4.0257826]]...]
INFO - root - 2017-12-15 10:10:29.143301: step 1210, loss = 0.88, batch loss = 0.27 (12.7 examples/sec; 0.629 sec/batch; 57h:52m:06s remains)
INFO - root - 2017-12-15 10:10:35.205171: step 1220, loss = 0.91, batch loss = 0.30 (13.2 examples/sec; 0.607 sec/batch; 55h:51m:49s remains)
INFO - root - 2017-12-15 10:10:41.292045: step 1230, loss = 0.92, batch loss = 0.31 (13.4 examples/sec; 0.595 sec/batch; 54h:46m:01s remains)
INFO - root - 2017-12-15 10:10:47.337429: step 1240, loss = 0.88, batch loss = 0.27 (13.0 examples/sec; 0.616 sec/batch; 56h:40m:45s remains)
INFO - root - 2017-12-15 10:10:53.394036: step 1250, loss = 0.88, batch loss = 0.27 (13.4 examples/sec; 0.597 sec/batch; 54h:56m:24s remains)
INFO - root - 2017-12-15 10:10:59.468116: step 1260, loss = 0.88, batch loss = 0.27 (13.3 examples/sec; 0.602 sec/batch; 55h:24m:51s remains)
INFO - root - 2017-12-15 10:11:05.483585: step 1270, loss = 0.88, batch loss = 0.27 (13.1 examples/sec; 0.609 sec/batch; 56h:01m:24s remains)
INFO - root - 2017-12-15 10:11:11.549973: step 1280, loss = 0.89, batch loss = 0.28 (13.3 examples/sec; 0.600 sec/batch; 55h:11m:02s remains)
INFO - root - 2017-12-15 10:11:17.625361: step 1290, loss = 0.88, batch loss = 0.27 (12.7 examples/sec; 0.631 sec/batch; 58h:01m:07s remains)
INFO - root - 2017-12-15 10:11:23.706647: step 1300, loss = 0.87, batch loss = 0.26 (13.0 examples/sec; 0.613 sec/batch; 56h:24m:02s remains)
2017-12-15 10:11:24.186745: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.88577509 -0.41835451 -0.51749039 -0.30938578 0.40327024 0.40370798 0.4427948 0.60243368 0.63918161 0.68769932 0.95547724 -1.5307322 -2.6271577 -3.016417 -3.7380698][-3.9598579 -4.1667352 -4.404882 -4.6211691 -4.0372796 -3.7340789 -3.5905464 -3.6042597 -3.5745053 -3.4818358 -3.8063104 -4.1343522 -4.993885 -5.809793 -5.4471989][-5.7061877 -5.8834972 -5.9232068 -5.5777073 -4.5938778 -4.3187418 -4.0662112 -3.8804302 -3.8273983 -3.8132162 -3.3928511 -5.3292084 -5.2825503 -5.396863 -5.35169][-5.1875877 -5.0859504 -5.2366652 -4.957767 -3.9363475 -3.6797256 -3.4903164 -3.4372389 -3.5377917 -4.2059531 -4.7623634 -5.0148106 -5.2767687 -6.22713 -6.0777936][-4.018312 -3.8443208 -3.5221486 -3.3070922 -2.310056 -2.0359998 -1.9214299 -1.9320827 -2.0733752 -2.9781246 -2.9967952 -4.8050632 -5.909668 -5.0681448 -5.9430523][-2.571687 -2.0149841 -1.5441329 -0.88079023 0.14298248 0.36054182 0.47389269 0.55138874 0.53945923 -0.86175466 -1.503437 -1.720026 -2.5275154 -4.1099157 -4.4159179][-1.4045162 -0.32749224 0.13345671 0.68046045 2.1026211 2.1677423 2.2264013 2.5292616 2.561172 1.0002713 0.49833393 -0.15361166 -1.2662604 -1.3813674 -2.8741529][-1.7833898 -1.0457509 -0.056804657 0.62975216 2.0695076 2.0248857 2.049192 2.1224523 2.157517 1.9021907 0.48062181 -0.92848325 -1.367908 -1.329277 -0.925365][-2.3338032 -2.1061738 -0.50586128 0.792686 2.1979451 2.1852813 2.1651697 2.4237666 2.3586884 1.8045239 1.0825281 0.23196507 -0.92269444 -0.840631 0.43081236][-5.1625848 -4.2594347 -3.5093274 -2.1934958 -0.19682741 -0.28978443 -0.44463587 -0.5940938 -0.61722183 -1.1418927 -1.9102111 -3.2144055 -2.7038121 -2.2283893 -1.2402303][-9.348114 -8.8299665 -8.3842545 -7.8063326 -5.5238557 -5.6568556 -5.9583464 -6.1051321 -6.3106365 -6.4348764 -7.4520364 -7.0928 -7.2883978 -5.9715204 -3.5614266][-9.1393929 -8.41861 -7.394043 -6.9388418 -5.1316433 -5.1694145 -5.3290286 -5.620532 -5.8772917 -6.4245996 -6.8664093 -6.8265519 -6.1252503 -5.3252587 -3.7917924][-9.4426823 -8.458868 -7.6369934 -7.3267822 -5.9593768 -5.7810688 -5.7915254 -6.0002689 -6.2291136 -6.3272986 -7.5186472 -6.7744684 -6.010962 -4.8626528 -4.0174208][-8.2170525 -7.3061194 -7.2395077 -6.9462624 -5.9267821 -5.8380837 -5.8057523 -5.8924818 -6.0532236 -5.9581642 -6.9916334 -7.2052794 -6.2970405 -4.9103117 -3.3311462][-7.1494722 -6.8972988 -6.5021729 -6.102706 -5.2693925 -5.1672149 -5.1767364 -5.2271404 -5.2496896 -5.8244944 -6.999486 -5.5621343 -4.6300898 -4.443224 -2.9451203]]...]
INFO - root - 2017-12-15 10:11:30.241603: step 1310, loss = 0.87, batch loss = 0.26 (13.2 examples/sec; 0.606 sec/batch; 55h:44m:21s remains)
INFO - root - 2017-12-15 10:11:36.283372: step 1320, loss = 0.86, batch loss = 0.26 (13.5 examples/sec; 0.594 sec/batch; 54h:40m:31s remains)
INFO - root - 2017-12-15 10:11:42.411252: step 1330, loss = 0.88, batch loss = 0.27 (13.3 examples/sec; 0.601 sec/batch; 55h:18m:51s remains)
INFO - root - 2017-12-15 10:11:48.463112: step 1340, loss = 0.93, batch loss = 0.33 (13.2 examples/sec; 0.605 sec/batch; 55h:38m:00s remains)
INFO - root - 2017-12-15 10:11:54.457833: step 1350, loss = 0.92, batch loss = 0.31 (13.4 examples/sec; 0.596 sec/batch; 54h:51m:28s remains)
