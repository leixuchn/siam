INFO - Obj-Siam-FC - Running command 'main'
INFO - Obj-Siam-FC - Started run with ID "4"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-11-30 07:15:04.735509: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-30 07:15:04.735579: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-30 07:15:04.735605: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-11-30 07:15:04.735627: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-30 07:15:04.735646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-11-30 07:15:05.347719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-11-30 07:15:05.347784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-11-30 07:15:05.347810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-11-30 07:15:05.347835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-11-30 07:15:08.212791: step 0, loss = 1.60, batch loss = 1.32 (3.3 examples/sec; 2.417 sec/batch; 223h:15m:46s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-11-30 07:15:10.610788: step 10, loss = 0.93, batch loss = 0.65 (50.4 examples/sec; 0.159 sec/batch; 14h:39m:53s remains)
INFO - root - 2017-11-30 07:15:13.597152: step 20, loss = 0.88, batch loss = 0.59 (36.1 examples/sec; 0.221 sec/batch; 20h:27m:23s remains)
INFO - root - 2017-11-30 07:15:16.844280: step 30, loss = 0.89, batch loss = 0.60 (20.8 examples/sec; 0.384 sec/batch; 35h:28m:01s remains)
INFO - root - 2017-11-30 07:15:20.459060: step 40, loss = 0.87, batch loss = 0.58 (17.2 examples/sec; 0.464 sec/batch; 42h:53m:29s remains)
INFO - root - 2017-11-30 07:15:23.961564: step 50, loss = 0.85, batch loss = 0.56 (52.8 examples/sec; 0.151 sec/batch; 13h:59m:01s remains)
INFO - root - 2017-11-30 07:15:27.546772: step 60, loss = 0.85, batch loss = 0.57 (34.1 examples/sec; 0.235 sec/batch; 21h:41m:09s remains)
INFO - root - 2017-11-30 07:15:31.498231: step 70, loss = 0.84, batch loss = 0.55 (26.0 examples/sec; 0.307 sec/batch; 28h:21m:48s remains)
INFO - root - 2017-11-30 07:15:33.936322: step 80, loss = 0.82, batch loss = 0.53 (51.3 examples/sec; 0.156 sec/batch; 14h:23m:23s remains)
INFO - root - 2017-11-30 07:15:35.843453: step 90, loss = 0.82, batch loss = 0.54 (49.9 examples/sec; 0.160 sec/batch; 14h:48m:50s remains)
INFO - root - 2017-11-30 07:15:38.364300: step 100, loss = 0.86, batch loss = 0.58 (53.7 examples/sec; 0.149 sec/batch; 13h:45m:41s remains)
INFO - root - 2017-11-30 07:15:41.151521: step 110, loss = 0.78, batch loss = 0.49 (23.3 examples/sec; 0.344 sec/batch; 31h:45m:10s remains)
INFO - root - 2017-11-30 07:15:43.788615: step 120, loss = 0.83, batch loss = 0.55 (53.9 examples/sec; 0.148 sec/batch; 13h:41m:58s remains)
INFO - root - 2017-11-30 07:15:46.285887: step 130, loss = 0.79, batch loss = 0.51 (54.3 examples/sec; 0.147 sec/batch; 13h:35m:36s remains)
INFO - root - 2017-11-30 07:15:50.547657: step 140, loss = 0.78, batch loss = 0.49 (17.1 examples/sec; 0.468 sec/batch; 43h:10m:15s remains)
INFO - root - 2017-11-30 07:15:53.417623: step 150, loss = 0.78, batch loss = 0.49 (39.6 examples/sec; 0.202 sec/batch; 18h:37m:44s remains)
INFO - root - 2017-11-30 07:15:55.655304: step 160, loss = 0.81, batch loss = 0.52 (42.8 examples/sec; 0.187 sec/batch; 17h:14m:15s remains)
INFO - root - 2017-11-30 07:15:58.280497: step 170, loss = 0.79, batch loss = 0.51 (51.6 examples/sec; 0.155 sec/batch; 14h:18m:34s remains)
INFO - root - 2017-11-30 07:16:00.585437: step 180, loss = 0.80, batch loss = 0.51 (44.2 examples/sec; 0.181 sec/batch; 16h:42m:55s remains)
INFO - root - 2017-11-30 07:16:02.511766: step 190, loss = 0.76, batch loss = 0.47 (50.5 examples/sec; 0.158 sec/batch; 14h:37m:35s remains)
INFO - root - 2017-11-30 07:16:04.098818: step 200, loss = 0.75, batch loss = 0.47 (51.7 examples/sec; 0.155 sec/batch; 14h:17m:16s remains)
INFO - root - 2017-11-30 07:16:06.935893: step 210, loss = 0.81, batch loss = 0.52 (16.7 examples/sec; 0.478 sec/batch; 44h:05m:23s remains)
INFO - root - 2017-11-30 07:16:09.096510: step 220, loss = 0.75, batch loss = 0.47 (24.1 examples/sec; 0.333 sec/batch; 30h:41m:39s remains)
INFO - root - 2017-11-30 07:16:10.906283: step 230, loss = 0.73, batch loss = 0.44 (51.0 examples/sec; 0.157 sec/batch; 14h:28m:50s remains)
INFO - root - 2017-11-30 07:16:12.916702: step 240, loss = 0.82, batch loss = 0.54 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:40s remains)
INFO - root - 2017-11-30 07:16:14.859990: step 250, loss = 0.80, batch loss = 0.52 (52.2 examples/sec; 0.153 sec/batch; 14h:08m:23s remains)
INFO - root - 2017-11-30 07:16:16.677838: step 260, loss = 0.82, batch loss = 0.54 (52.1 examples/sec; 0.153 sec/batch; 14h:09m:29s remains)
INFO - root - 2017-11-30 07:16:18.674356: step 270, loss = 0.78, batch loss = 0.50 (51.5 examples/sec; 0.155 sec/batch; 14h:19m:43s remains)
INFO - root - 2017-11-30 07:16:20.812940: step 280, loss = 0.79, batch loss = 0.51 (51.9 examples/sec; 0.154 sec/batch; 14h:14m:07s remains)
INFO - root - 2017-11-30 07:16:24.425215: step 290, loss = 0.74, batch loss = 0.45 (13.8 examples/sec; 0.580 sec/batch; 53h:29m:41s remains)
INFO - root - 2017-11-30 07:16:27.631242: step 300, loss = 0.79, batch loss = 0.51 (18.4 examples/sec; 0.435 sec/batch; 40h:08m:32s remains)
INFO - root - 2017-11-30 07:16:30.171780: step 310, loss = 0.83, batch loss = 0.54 (40.3 examples/sec; 0.198 sec/batch; 18h:17m:59s remains)
INFO - root - 2017-11-30 07:16:32.604184: step 320, loss = 0.80, batch loss = 0.51 (52.3 examples/sec; 0.153 sec/batch; 14h:06m:04s remains)
INFO - root - 2017-11-30 07:16:34.693704: step 330, loss = 0.79, batch loss = 0.50 (32.1 examples/sec; 0.249 sec/batch; 22h:59m:41s remains)
INFO - root - 2017-11-30 07:16:36.703191: step 340, loss = 0.79, batch loss = 0.50 (44.7 examples/sec; 0.179 sec/batch; 16h:31m:13s remains)
INFO - root - 2017-11-30 07:16:39.927544: step 350, loss = 0.82, batch loss = 0.54 (30.3 examples/sec; 0.264 sec/batch; 24h:22m:55s remains)
INFO - root - 2017-11-30 07:16:42.901404: step 360, loss = 0.73, batch loss = 0.45 (23.7 examples/sec; 0.338 sec/batch; 31h:10m:21s remains)
INFO - root - 2017-11-30 07:16:45.889235: step 370, loss = 0.72, batch loss = 0.43 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:28s remains)
INFO - root - 2017-11-30 07:16:48.242182: step 380, loss = 0.85, batch loss = 0.57 (30.0 examples/sec; 0.267 sec/batch; 24h:36m:16s remains)
INFO - root - 2017-11-30 07:16:50.282046: step 390, loss = 0.75, batch loss = 0.46 (52.2 examples/sec; 0.153 sec/batch; 14h:07m:48s remains)
INFO - root - 2017-11-30 07:16:52.358284: step 400, loss = 0.82, batch loss = 0.53 (39.0 examples/sec; 0.205 sec/batch; 18h:55m:05s remains)
INFO - root - 2017-11-30 07:16:54.165466: step 410, loss = 0.75, batch loss = 0.46 (49.7 examples/sec; 0.161 sec/batch; 14h:50m:03s remains)
INFO - root - 2017-11-30 07:16:56.085690: step 420, loss = 0.76, batch loss = 0.48 (35.1 examples/sec; 0.228 sec/batch; 21h:02m:29s remains)
INFO - root - 2017-11-30 07:17:00.132984: step 430, loss = 0.78, batch loss = 0.49 (11.9 examples/sec; 0.673 sec/batch; 62h:06m:37s remains)
INFO - root - 2017-11-30 07:17:03.259982: step 440, loss = 0.79, batch loss = 0.50 (42.2 examples/sec; 0.189 sec/batch; 17h:28m:45s remains)
INFO - root - 2017-11-30 07:17:05.416188: step 450, loss = 0.76, batch loss = 0.47 (52.2 examples/sec; 0.153 sec/batch; 14h:07m:42s remains)
INFO - root - 2017-11-30 07:17:07.484044: step 460, loss = 0.70, batch loss = 0.41 (51.8 examples/sec; 0.154 sec/batch; 14h:14m:51s remains)
INFO - root - 2017-11-30 07:17:09.477159: step 470, loss = 0.73, batch loss = 0.45 (52.5 examples/sec; 0.152 sec/batch; 14h:03m:00s remains)
INFO - root - 2017-11-30 07:17:12.383357: step 480, loss = 0.81, batch loss = 0.52 (45.3 examples/sec; 0.177 sec/batch; 16h:17m:58s remains)
INFO - root - 2017-11-30 07:17:15.162965: step 490, loss = 0.69, batch loss = 0.40 (45.8 examples/sec; 0.175 sec/batch; 16h:06m:01s remains)
INFO - root - 2017-11-30 07:17:17.457493: step 500, loss = 0.81, batch loss = 0.52 (40.7 examples/sec; 0.197 sec/batch; 18h:08m:19s remains)
INFO - root - 2017-11-30 07:17:20.367154: step 510, loss = 0.72, batch loss = 0.44 (20.1 examples/sec; 0.397 sec/batch; 36h:37m:17s remains)
INFO - root - 2017-11-30 07:17:22.744425: step 520, loss = 0.83, batch loss = 0.54 (29.8 examples/sec; 0.269 sec/batch; 24h:47m:00s remains)
INFO - root - 2017-11-30 07:17:24.292041: step 530, loss = 0.72, batch loss = 0.43 (53.2 examples/sec; 0.150 sec/batch; 13h:51m:44s remains)
INFO - root - 2017-11-30 07:17:26.562087: step 540, loss = 0.78, batch loss = 0.50 (51.2 examples/sec; 0.156 sec/batch; 14h:24m:34s remains)
INFO - root - 2017-11-30 07:17:28.128924: step 550, loss = 0.73, batch loss = 0.44 (53.6 examples/sec; 0.149 sec/batch; 13h:45m:17s remains)
INFO - root - 2017-11-30 07:17:30.289814: step 560, loss = 0.74, batch loss = 0.45 (22.7 examples/sec; 0.353 sec/batch; 32h:30m:48s remains)
INFO - root - 2017-11-30 07:17:32.436971: step 570, loss = 0.76, batch loss = 0.48 (15.1 examples/sec; 0.531 sec/batch; 48h:59m:15s remains)
INFO - root - 2017-11-30 07:17:38.493443: step 580, loss = 0.77, batch loss = 0.48 (34.4 examples/sec; 0.233 sec/batch; 21h:27m:40s remains)
INFO - root - 2017-11-30 07:17:40.238765: step 590, loss = 0.70, batch loss = 0.42 (52.3 examples/sec; 0.153 sec/batch; 14h:05m:47s remains)
INFO - root - 2017-11-30 07:17:42.085375: step 600, loss = 0.80, batch loss = 0.51 (54.0 examples/sec; 0.148 sec/batch; 13h:40m:11s remains)
INFO - root - 2017-11-30 07:17:44.189964: step 610, loss = 0.88, batch loss = 0.59 (52.8 examples/sec; 0.152 sec/batch; 13h:58m:25s remains)
INFO - root - 2017-11-30 07:17:46.256907: step 620, loss = 0.81, batch loss = 0.52 (50.9 examples/sec; 0.157 sec/batch; 14h:28m:45s remains)
INFO - root - 2017-11-30 07:17:48.094487: step 630, loss = 0.76, batch loss = 0.48 (52.4 examples/sec; 0.153 sec/batch; 14h:04m:46s remains)
INFO - root - 2017-11-30 07:17:49.637557: step 640, loss = 0.67, batch loss = 0.39 (50.7 examples/sec; 0.158 sec/batch; 14h:32m:05s remains)
INFO - root - 2017-11-30 07:17:51.964301: step 650, loss = 0.81, batch loss = 0.53 (52.3 examples/sec; 0.153 sec/batch; 14h:05m:14s remains)
INFO - root - 2017-11-30 07:17:54.150077: step 660, loss = 0.65, batch loss = 0.37 (52.3 examples/sec; 0.153 sec/batch; 14h:05m:31s remains)
INFO - root - 2017-11-30 07:17:56.299210: step 670, loss = 0.70, batch loss = 0.42 (49.8 examples/sec; 0.161 sec/batch; 14h:48m:41s remains)
INFO - root - 2017-11-30 07:18:00.834560: step 680, loss = 0.74, batch loss = 0.46 (29.5 examples/sec; 0.272 sec/batch; 25h:01m:58s remains)
INFO - root - 2017-11-30 07:18:02.804135: step 690, loss = 0.65, batch loss = 0.37 (51.4 examples/sec; 0.156 sec/batch; 14h:20m:24s remains)
INFO - root - 2017-11-30 07:18:05.105404: step 700, loss = 0.68, batch loss = 0.39 (31.9 examples/sec; 0.251 sec/batch; 23h:06m:05s remains)
INFO - root - 2017-11-30 07:18:07.351669: step 710, loss = 0.77, batch loss = 0.49 (51.9 examples/sec; 0.154 sec/batch; 14h:12m:41s remains)
INFO - root - 2017-11-30 07:18:09.356994: step 720, loss = 0.67, batch loss = 0.38 (47.6 examples/sec; 0.168 sec/batch; 15h:29m:23s remains)
INFO - root - 2017-11-30 07:18:11.578132: step 730, loss = 0.70, batch loss = 0.42 (50.1 examples/sec; 0.160 sec/batch; 14h:43m:10s remains)
INFO - root - 2017-11-30 07:18:14.227637: step 740, loss = 0.66, batch loss = 0.38 (40.4 examples/sec; 0.198 sec/batch; 18h:15m:27s remains)
INFO - root - 2017-11-30 07:18:17.570706: step 750, loss = 0.72, batch loss = 0.44 (42.7 examples/sec; 0.187 sec/batch; 17h:16m:35s remains)
INFO - root - 2017-11-30 07:18:20.370837: step 760, loss = 0.72, batch loss = 0.44 (43.3 examples/sec; 0.185 sec/batch; 17h:01m:26s remains)
INFO - root - 2017-11-30 07:18:22.335838: step 770, loss = 0.81, batch loss = 0.53 (52.7 examples/sec; 0.152 sec/batch; 13h:59m:46s remains)
INFO - root - 2017-11-30 07:18:24.054817: step 780, loss = 0.70, batch loss = 0.41 (53.3 examples/sec; 0.150 sec/batch; 13h:50m:00s remains)
INFO - root - 2017-11-30 07:18:25.878936: step 790, loss = 0.76, batch loss = 0.48 (51.2 examples/sec; 0.156 sec/batch; 14h:23m:23s remains)
INFO - root - 2017-11-30 07:18:28.833191: step 800, loss = 0.76, batch loss = 0.47 (8.1 examples/sec; 0.992 sec/batch; 91h:21m:46s remains)
INFO - root - 2017-11-30 07:18:31.246909: step 810, loss = 0.66, batch loss = 0.38 (30.8 examples/sec; 0.260 sec/batch; 23h:57m:01s remains)
INFO - root - 2017-11-30 07:18:33.071229: step 820, loss = 0.69, batch loss = 0.41 (23.3 examples/sec; 0.344 sec/batch; 31h:40m:17s remains)
INFO - root - 2017-11-30 07:18:34.993227: step 830, loss = 0.78, batch loss = 0.50 (24.8 examples/sec; 0.322 sec/batch; 29h:42m:26s remains)
INFO - root - 2017-11-30 07:18:37.758218: step 840, loss = 0.75, batch loss = 0.47 (22.7 examples/sec; 0.352 sec/batch; 32h:25m:03s remains)
INFO - root - 2017-11-30 07:18:40.196627: step 850, loss = 0.68, batch loss = 0.39 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:40s remains)
INFO - root - 2017-11-30 07:18:41.975668: step 860, loss = 0.72, batch loss = 0.44 (52.6 examples/sec; 0.152 sec/batch; 14h:00m:33s remains)
INFO - root - 2017-11-30 07:18:43.861115: step 870, loss = 0.72, batch loss = 0.44 (51.0 examples/sec; 0.157 sec/batch; 14h:27m:25s remains)
INFO - root - 2017-11-30 07:18:45.578311: step 880, loss = 0.71, batch loss = 0.43 (50.2 examples/sec; 0.159 sec/batch; 14h:41m:24s remains)
INFO - root - 2017-11-30 07:18:47.528841: step 890, loss = 0.68, batch loss = 0.40 (51.4 examples/sec; 0.156 sec/batch; 14h:19m:37s remains)
INFO - root - 2017-11-30 07:18:50.146980: step 900, loss = 0.73, batch loss = 0.44 (40.4 examples/sec; 0.198 sec/batch; 18h:14m:56s remains)
INFO - root - 2017-11-30 07:18:52.263190: step 910, loss = 0.78, batch loss = 0.50 (52.5 examples/sec; 0.152 sec/batch; 14h:02m:38s remains)
INFO - root - 2017-11-30 07:18:54.249014: step 920, loss = 0.72, batch loss = 0.44 (26.1 examples/sec; 0.307 sec/batch; 28h:14m:20s remains)
INFO - root - 2017-11-30 07:18:56.526697: step 930, loss = 0.83, batch loss = 0.55 (49.7 examples/sec; 0.161 sec/batch; 14h:48m:59s remains)
INFO - root - 2017-11-30 07:18:59.306773: step 940, loss = 0.70, batch loss = 0.42 (41.9 examples/sec; 0.191 sec/batch; 17h:35m:33s remains)
INFO - root - 2017-11-30 07:19:02.034784: step 950, loss = 0.71, batch loss = 0.43 (23.7 examples/sec; 0.337 sec/batch; 31h:03m:53s remains)
INFO - root - 2017-11-30 07:19:04.029500: step 960, loss = 0.73, batch loss = 0.44 (50.6 examples/sec; 0.158 sec/batch; 14h:33m:11s remains)
INFO - root - 2017-11-30 07:19:06.213857: step 970, loss = 0.63, batch loss = 0.35 (53.2 examples/sec; 0.150 sec/batch; 13h:50m:28s remains)
INFO - root - 2017-11-30 07:19:08.923012: step 980, loss = 0.73, batch loss = 0.45 (51.2 examples/sec; 0.156 sec/batch; 14h:24m:02s remains)
INFO - root - 2017-11-30 07:19:10.936797: step 990, loss = 0.65, batch loss = 0.36 (49.9 examples/sec; 0.160 sec/batch; 14h:46m:04s remains)
INFO - root - 2017-11-30 07:19:12.978262: step 1000, loss = 0.69, batch loss = 0.41 (31.6 examples/sec; 0.254 sec/batch; 23h:20m:51s remains)
INFO - root - 2017-11-30 07:19:15.779179: step 1010, loss = 0.72, batch loss = 0.44 (9.8 examples/sec; 0.815 sec/batch; 75h:01m:10s remains)
INFO - root - 2017-11-30 07:19:17.641372: step 1020, loss = 0.66, batch loss = 0.38 (25.6 examples/sec; 0.313 sec/batch; 28h:49m:01s remains)
INFO - root - 2017-11-30 07:19:20.656551: step 1030, loss = 0.65, batch loss = 0.37 (40.1 examples/sec; 0.199 sec/batch; 18h:20m:48s remains)
INFO - root - 2017-11-30 07:19:23.346876: step 1040, loss = 0.84, batch loss = 0.55 (51.2 examples/sec; 0.156 sec/batch; 14h:22m:36s remains)
INFO - root - 2017-11-30 07:19:25.116330: step 1050, loss = 0.73, batch loss = 0.45 (53.0 examples/sec; 0.151 sec/batch; 13h:54m:14s remains)
INFO - root - 2017-11-30 07:19:26.827964: step 1060, loss = 0.71, batch loss = 0.43 (51.9 examples/sec; 0.154 sec/batch; 14h:11m:30s remains)
INFO - root - 2017-11-30 07:19:28.794241: step 1070, loss = 0.70, batch loss = 0.41 (18.4 examples/sec; 0.434 sec/batch; 39h:58m:17s remains)
INFO - root - 2017-11-30 07:19:30.756508: step 1080, loss = 0.67, batch loss = 0.39 (43.9 examples/sec; 0.182 sec/batch; 16h:46m:51s remains)
INFO - root - 2017-11-30 07:19:33.232390: step 1090, loss = 0.60, batch loss = 0.32 (16.6 examples/sec; 0.481 sec/batch; 44h:15m:49s remains)
INFO - root - 2017-11-30 07:19:35.097325: step 1100, loss = 0.76, batch loss = 0.48 (38.2 examples/sec; 0.210 sec/batch; 19h:17m:31s remains)
INFO - root - 2017-11-30 07:19:38.079994: step 1110, loss = 0.64, batch loss = 0.36 (33.6 examples/sec; 0.238 sec/batch; 21h:55m:49s remains)
INFO - root - 2017-11-30 07:19:40.586439: step 1120, loss = 0.64, batch loss = 0.36 (52.1 examples/sec; 0.154 sec/batch; 14h:08m:00s remains)
INFO - root - 2017-11-30 07:19:42.483459: step 1130, loss = 0.76, batch loss = 0.48 (24.2 examples/sec; 0.331 sec/batch; 30h:29m:04s remains)
INFO - root - 2017-11-30 07:19:44.450890: step 1140, loss = 0.79, batch loss = 0.51 (49.9 examples/sec; 0.160 sec/batch; 14h:45m:49s remains)
INFO - root - 2017-11-30 07:19:46.326834: step 1150, loss = 0.64, batch loss = 0.36 (52.6 examples/sec; 0.152 sec/batch; 13h:59m:38s remains)
INFO - root - 2017-11-30 07:19:48.327025: step 1160, loss = 0.72, batch loss = 0.44 (30.0 examples/sec; 0.267 sec/batch; 24h:32m:14s remains)
INFO - root - 2017-11-30 07:19:50.848528: step 1170, loss = 0.65, batch loss = 0.36 (51.0 examples/sec; 0.157 sec/batch; 14h:26m:08s remains)
INFO - root - 2017-11-30 07:19:52.678497: step 1180, loss = 0.74, batch loss = 0.46 (53.2 examples/sec; 0.150 sec/batch; 13h:49m:43s remains)
INFO - root - 2017-11-30 07:19:54.740669: step 1190, loss = 0.73, batch loss = 0.45 (53.6 examples/sec; 0.149 sec/batch; 13h:44m:45s remains)
INFO - root - 2017-11-30 07:19:56.583064: step 1200, loss = 0.70, batch loss = 0.42 (40.8 examples/sec; 0.196 sec/batch; 18h:01m:24s remains)
INFO - root - 2017-11-30 07:19:58.574075: step 1210, loss = 0.80, batch loss = 0.52 (53.7 examples/sec; 0.149 sec/batch; 13h:42m:07s remains)
INFO - root - 2017-11-30 07:20:00.426691: step 1220, loss = 0.70, batch loss = 0.42 (49.5 examples/sec; 0.162 sec/batch; 14h:52m:48s remains)
INFO - root - 2017-11-30 07:20:02.311085: step 1230, loss = 0.68, batch loss = 0.40 (50.5 examples/sec; 0.158 sec/batch; 14h:33m:49s remains)
INFO - root - 2017-11-30 07:20:04.614075: step 1240, loss = 0.80, batch loss = 0.52 (50.5 examples/sec; 0.158 sec/batch; 14h:34m:07s remains)
INFO - root - 2017-11-30 07:20:06.379289: step 1250, loss = 0.70, batch loss = 0.42 (52.7 examples/sec; 0.152 sec/batch; 13h:58m:34s remains)
INFO - root - 2017-11-30 07:20:08.318687: step 1260, loss = 0.68, batch loss = 0.40 (52.6 examples/sec; 0.152 sec/batch; 13h:59m:44s remains)
INFO - root - 2017-11-30 07:20:10.452595: step 1270, loss = 0.66, batch loss = 0.38 (49.2 examples/sec; 0.163 sec/batch; 14h:58m:09s remains)
INFO - root - 2017-11-30 07:20:12.161729: step 1280, loss = 0.68, batch loss = 0.40 (53.3 examples/sec; 0.150 sec/batch; 13h:48m:08s remains)
INFO - root - 2017-11-30 07:20:15.632416: step 1290, loss = 0.74, batch loss = 0.46 (4.8 examples/sec; 1.680 sec/batch; 154h:36m:00s remains)
INFO - root - 2017-11-30 07:20:19.242236: step 1300, loss = 0.73, batch loss = 0.45 (53.7 examples/sec; 0.149 sec/batch; 13h:42m:01s remains)
INFO - root - 2017-11-30 07:20:21.739834: step 1310, loss = 0.68, batch loss = 0.40 (52.4 examples/sec; 0.153 sec/batch; 14h:02m:38s remains)
INFO - root - 2017-11-30 07:20:23.307672: step 1320, loss = 0.69, batch loss = 0.41 (50.6 examples/sec; 0.158 sec/batch; 14h:32m:32s remains)
INFO - root - 2017-11-30 07:20:25.083952: step 1330, loss = 0.82, batch loss = 0.54 (49.9 examples/sec; 0.160 sec/batch; 14h:44m:17s remains)
INFO - root - 2017-11-30 07:20:27.054641: step 1340, loss = 0.60, batch loss = 0.32 (51.4 examples/sec; 0.156 sec/batch; 14h:19m:35s remains)
INFO - root - 2017-11-30 07:20:28.952100: step 1350, loss = 0.65, batch loss = 0.37 (30.8 examples/sec; 0.260 sec/batch; 23h:52m:29s remains)
INFO - root - 2017-11-30 07:20:31.041540: step 1360, loss = 0.74, batch loss = 0.45 (53.4 examples/sec; 0.150 sec/batch; 13h:47m:15s remains)
INFO - root - 2017-11-30 07:20:32.919036: step 1370, loss = 0.74, batch loss = 0.46 (42.7 examples/sec; 0.188 sec/batch; 17h:15m:01s remains)
INFO - root - 2017-11-30 07:20:34.828553: step 1380, loss = 0.76, batch loss = 0.47 (52.1 examples/sec; 0.154 sec/batch; 14h:08m:07s remains)
INFO - root - 2017-11-30 07:20:36.373907: step 1390, loss = 0.66, batch loss = 0.38 (51.0 examples/sec; 0.157 sec/batch; 14h:25m:48s remains)
INFO - root - 2017-11-30 07:20:38.088535: step 1400, loss = 0.67, batch loss = 0.39 (50.8 examples/sec; 0.157 sec/batch; 14h:28m:56s remains)
INFO - root - 2017-11-30 07:20:39.846454: step 1410, loss = 0.62, batch loss = 0.34 (48.2 examples/sec; 0.166 sec/batch; 15h:15m:21s remains)
INFO - root - 2017-11-30 07:20:42.231798: step 1420, loss = 0.70, batch loss = 0.42 (26.1 examples/sec; 0.307 sec/batch; 28h:13m:15s remains)
INFO - root - 2017-11-30 07:20:44.156952: step 1430, loss = 0.73, batch loss = 0.44 (52.7 examples/sec; 0.152 sec/batch; 13h:57m:34s remains)
INFO - root - 2017-11-30 07:20:46.055700: step 1440, loss = 0.67, batch loss = 0.39 (51.7 examples/sec; 0.155 sec/batch; 14h:14m:14s remains)
INFO - root - 2017-11-30 07:20:47.679182: step 1450, loss = 0.59, batch loss = 0.31 (54.4 examples/sec; 0.147 sec/batch; 13h:31m:25s remains)
INFO - root - 2017-11-30 07:20:49.746920: step 1460, loss = 0.65, batch loss = 0.37 (12.1 examples/sec; 0.663 sec/batch; 60h:57m:24s remains)
INFO - root - 2017-11-30 07:20:54.135888: step 1470, loss = 0.66, batch loss = 0.38 (53.2 examples/sec; 0.150 sec/batch; 13h:49m:20s remains)
INFO - root - 2017-11-30 07:20:55.673517: step 1480, loss = 0.76, batch loss = 0.48 (51.1 examples/sec; 0.156 sec/batch; 14h:23m:08s remains)
INFO - root - 2017-11-30 07:20:57.338538: step 1490, loss = 0.66, batch loss = 0.38 (52.3 examples/sec; 0.153 sec/batch; 14h:04m:36s remains)
INFO - root - 2017-11-30 07:20:59.117909: step 1500, loss = 0.71, batch loss = 0.43 (24.4 examples/sec; 0.328 sec/batch; 30h:08m:56s remains)
INFO - root - 2017-11-30 07:21:01.579656: step 1510, loss = 0.68, batch loss = 0.40 (50.9 examples/sec; 0.157 sec/batch; 14h:26m:34s remains)
INFO - root - 2017-11-30 07:21:03.183391: step 1520, loss = 0.66, batch loss = 0.38 (52.2 examples/sec; 0.153 sec/batch; 14h:04m:45s remains)
INFO - root - 2017-11-30 07:21:05.001884: step 1530, loss = 0.69, batch loss = 0.41 (51.9 examples/sec; 0.154 sec/batch; 14h:10m:10s remains)
INFO - root - 2017-11-30 07:21:06.706172: step 1540, loss = 0.65, batch loss = 0.36 (51.5 examples/sec; 0.155 sec/batch; 14h:17m:24s remains)
INFO - root - 2017-11-30 07:21:08.563707: step 1550, loss = 0.62, batch loss = 0.34 (49.9 examples/sec; 0.160 sec/batch; 14h:43m:26s remains)
INFO - root - 2017-11-30 07:21:12.022577: step 1560, loss = 0.80, batch loss = 0.52 (39.6 examples/sec; 0.202 sec/batch; 18h:34m:40s remains)
INFO - root - 2017-11-30 07:21:13.921895: step 1570, loss = 0.70, batch loss = 0.42 (51.6 examples/sec; 0.155 sec/batch; 14h:15m:45s remains)
INFO - root - 2017-11-30 07:21:15.470148: step 1580, loss = 0.62, batch loss = 0.34 (51.4 examples/sec; 0.156 sec/batch; 14h:18m:34s remains)
INFO - root - 2017-11-30 07:21:17.068424: step 1590, loss = 0.69, batch loss = 0.40 (49.9 examples/sec; 0.160 sec/batch; 14h:44m:40s remains)
INFO - root - 2017-11-30 07:21:19.444749: step 1600, loss = 0.71, batch loss = 0.42 (30.4 examples/sec; 0.263 sec/batch; 24h:11m:15s remains)
INFO - root - 2017-11-30 07:21:21.667587: step 1610, loss = 0.69, batch loss = 0.41 (52.3 examples/sec; 0.153 sec/batch; 14h:03m:23s remains)
INFO - root - 2017-11-30 07:21:24.058712: step 1620, loss = 0.71, batch loss = 0.43 (53.1 examples/sec; 0.151 sec/batch; 13h:50m:03s remains)
INFO - root - 2017-11-30 07:21:26.415556: step 1630, loss = 0.81, batch loss = 0.52 (49.9 examples/sec; 0.160 sec/batch; 14h:44m:38s remains)
INFO - root - 2017-11-30 07:21:28.440150: step 1640, loss = 0.65, batch loss = 0.37 (23.6 examples/sec; 0.339 sec/batch; 31h:09m:07s remains)
INFO - root - 2017-11-30 07:21:30.627149: step 1650, loss = 0.68, batch loss = 0.40 (51.0 examples/sec; 0.157 sec/batch; 14h:24m:48s remains)
INFO - root - 2017-11-30 07:21:32.170997: step 1660, loss = 0.59, batch loss = 0.31 (52.4 examples/sec; 0.153 sec/batch; 14h:01m:35s remains)
INFO - root - 2017-11-30 07:21:33.966397: step 1670, loss = 0.73, batch loss = 0.45 (26.9 examples/sec; 0.298 sec/batch; 27h:22m:39s remains)
INFO - root - 2017-11-30 07:21:35.537222: step 1680, loss = 0.71, batch loss = 0.43 (50.5 examples/sec; 0.159 sec/batch; 14h:34m:11s remains)
INFO - root - 2017-11-30 07:21:37.085327: step 1690, loss = 0.72, batch loss = 0.44 (51.1 examples/sec; 0.156 sec/batch; 14h:22m:24s remains)
INFO - root - 2017-11-30 07:21:38.864224: step 1700, loss = 0.62, batch loss = 0.34 (52.3 examples/sec; 0.153 sec/batch; 14h:02m:36s remains)
INFO - root - 2017-11-30 07:21:41.859711: step 1710, loss = 0.64, batch loss = 0.36 (50.6 examples/sec; 0.158 sec/batch; 14h:31m:03s remains)
INFO - root - 2017-11-30 07:21:43.560993: step 1720, loss = 0.66, batch loss = 0.38 (42.6 examples/sec; 0.188 sec/batch; 17h:14m:33s remains)
INFO - root - 2017-11-30 07:21:45.343988: step 1730, loss = 0.67, batch loss = 0.39 (52.3 examples/sec; 0.153 sec/batch; 14h:02m:40s remains)
INFO - root - 2017-11-30 07:21:47.665605: step 1740, loss = 0.70, batch loss = 0.42 (33.8 examples/sec; 0.237 sec/batch; 21h:44m:48s remains)
INFO - root - 2017-11-30 07:21:49.677885: step 1750, loss = 0.76, batch loss = 0.48 (48.6 examples/sec; 0.165 sec/batch; 15h:06m:59s remains)
INFO - root - 2017-11-30 07:21:51.622345: step 1760, loss = 0.65, batch loss = 0.37 (51.4 examples/sec; 0.156 sec/batch; 14h:17m:17s remains)
INFO - root - 2017-11-30 07:21:53.558731: step 1770, loss = 0.76, batch loss = 0.48 (51.5 examples/sec; 0.155 sec/batch; 14h:15m:31s remains)
INFO - root - 2017-11-30 07:21:56.353079: step 1780, loss = 0.73, batch loss = 0.45 (52.5 examples/sec; 0.152 sec/batch; 14h:00m:34s remains)
INFO - root - 2017-11-30 07:21:58.123745: step 1790, loss = 0.66, batch loss = 0.38 (52.1 examples/sec; 0.153 sec/batch; 14h:05m:48s remains)
INFO - root - 2017-11-30 07:21:59.697385: step 1800, loss = 0.72, batch loss = 0.43 (50.8 examples/sec; 0.157 sec/batch; 14h:28m:00s remains)
INFO - root - 2017-11-30 07:22:01.387239: step 1810, loss = 0.69, batch loss = 0.41 (51.5 examples/sec; 0.155 sec/batch; 14h:16m:52s remains)
INFO - root - 2017-11-30 07:22:03.050154: step 1820, loss = 0.71, batch loss = 0.43 (32.5 examples/sec; 0.246 sec/batch; 22h:38m:14s remains)
INFO - root - 2017-11-30 07:22:04.718173: step 1830, loss = 0.65, batch loss = 0.37 (49.7 examples/sec; 0.161 sec/batch; 14h:47m:16s remains)
INFO - root - 2017-11-30 07:22:06.882274: step 1840, loss = 0.62, batch loss = 0.34 (51.8 examples/sec; 0.154 sec/batch; 14h:10m:21s remains)
INFO - root - 2017-11-30 07:22:08.544834: step 1850, loss = 0.68, batch loss = 0.40 (48.3 examples/sec; 0.166 sec/batch; 15h:13m:04s remains)
INFO - root - 2017-11-30 07:22:11.528437: step 1860, loss = 0.72, batch loss = 0.44 (31.2 examples/sec; 0.257 sec/batch; 23h:34m:10s remains)
INFO - root - 2017-11-30 07:22:13.469070: step 1870, loss = 0.64, batch loss = 0.36 (39.9 examples/sec; 0.200 sec/batch; 18h:24m:23s remains)
INFO - root - 2017-11-30 07:22:15.848625: step 1880, loss = 0.61, batch loss = 0.33 (50.1 examples/sec; 0.160 sec/batch; 14h:40m:19s remains)
INFO - root - 2017-11-30 07:22:17.624401: step 1890, loss = 0.84, batch loss = 0.56 (28.9 examples/sec; 0.277 sec/batch; 25h:27m:46s remains)
INFO - root - 2017-11-30 07:22:19.919092: step 1900, loss = 0.65, batch loss = 0.37 (21.3 examples/sec; 0.376 sec/batch; 34h:29m:47s remains)
INFO - root - 2017-11-30 07:22:21.946676: step 1910, loss = 0.69, batch loss = 0.41 (52.0 examples/sec; 0.154 sec/batch; 14h:08m:04s remains)
INFO - root - 2017-11-30 07:22:24.147839: step 1920, loss = 0.70, batch loss = 0.42 (51.8 examples/sec; 0.154 sec/batch; 14h:10m:27s remains)
INFO - root - 2017-11-30 07:22:27.528319: step 1930, loss = 0.59, batch loss = 0.31 (51.9 examples/sec; 0.154 sec/batch; 14h:08m:48s remains)
INFO - root - 2017-11-30 07:22:29.157159: step 1940, loss = 0.65, batch loss = 0.37 (52.9 examples/sec; 0.151 sec/batch; 13h:53m:10s remains)
INFO - root - 2017-11-30 07:22:30.708843: step 1950, loss = 0.84, batch loss = 0.56 (50.2 examples/sec; 0.159 sec/batch; 14h:37m:22s remains)
INFO - root - 2017-11-30 07:22:32.472480: step 1960, loss = 0.64, batch loss = 0.36 (51.5 examples/sec; 0.155 sec/batch; 14h:15m:11s remains)
INFO - root - 2017-11-30 07:22:34.613709: step 1970, loss = 0.62, batch loss = 0.34 (18.0 examples/sec; 0.444 sec/batch; 40h:46m:03s remains)
INFO - root - 2017-11-30 07:22:36.314988: step 1980, loss = 0.65, batch loss = 0.37 (51.2 examples/sec; 0.156 sec/batch; 14h:21m:11s remains)
INFO - root - 2017-11-30 07:22:38.039285: step 1990, loss = 0.73, batch loss = 0.45 (51.9 examples/sec; 0.154 sec/batch; 14h:08m:36s remains)
INFO - root - 2017-11-30 07:22:39.904859: step 2000, loss = 0.61, batch loss = 0.33 (52.4 examples/sec; 0.153 sec/batch; 14h:01m:17s remains)
INFO - root - 2017-11-30 07:22:41.497379: step 2010, loss = 0.76, batch loss = 0.48 (52.6 examples/sec; 0.152 sec/batch; 13h:58m:09s remains)
INFO - root - 2017-11-30 07:22:43.358492: step 2020, loss = 0.82, batch loss = 0.54 (50.6 examples/sec; 0.158 sec/batch; 14h:31m:05s remains)
INFO - root - 2017-11-30 07:22:45.422389: step 2030, loss = 0.63, batch loss = 0.35 (51.3 examples/sec; 0.156 sec/batch; 14h:19m:04s remains)
INFO - root - 2017-11-30 07:22:47.387556: step 2040, loss = 0.67, batch loss = 0.39 (30.7 examples/sec; 0.261 sec/batch; 23h:55m:53s remains)
INFO - root - 2017-11-30 07:22:49.722972: step 2050, loss = 0.64, batch loss = 0.36 (50.9 examples/sec; 0.157 sec/batch; 14h:25m:19s remains)
INFO - root - 2017-11-30 07:22:51.533183: step 2060, loss = 0.67, batch loss = 0.39 (52.7 examples/sec; 0.152 sec/batch; 13h:56m:27s remains)
INFO - root - 2017-11-30 07:22:53.220810: step 2070, loss = 0.59, batch loss = 0.31 (49.7 examples/sec; 0.161 sec/batch; 14h:45m:44s remains)
INFO - root - 2017-11-30 07:22:55.251002: step 2080, loss = 0.67, batch loss = 0.39 (27.2 examples/sec; 0.294 sec/batch; 26h:57m:30s remains)
INFO - root - 2017-11-30 07:22:57.252330: step 2090, loss = 0.68, batch loss = 0.40 (50.2 examples/sec; 0.159 sec/batch; 14h:37m:44s remains)
INFO - root - 2017-11-30 07:22:58.983484: step 2100, loss = 0.85, batch loss = 0.57 (48.0 examples/sec; 0.167 sec/batch; 15h:17m:28s remains)
INFO - root - 2017-11-30 07:23:00.805753: step 2110, loss = 0.61, batch loss = 0.33 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:15s remains)
INFO - root - 2017-11-30 07:23:02.571008: step 2120, loss = 0.69, batch loss = 0.41 (53.5 examples/sec; 0.150 sec/batch; 13h:43m:27s remains)
INFO - root - 2017-11-30 07:23:05.008027: step 2130, loss = 0.68, batch loss = 0.40 (11.7 examples/sec; 0.681 sec/batch; 62h:31m:45s remains)
INFO - root - 2017-11-30 07:23:07.180309: step 2140, loss = 0.74, batch loss = 0.46 (48.6 examples/sec; 0.165 sec/batch; 15h:06m:47s remains)
INFO - root - 2017-11-30 07:23:09.575211: step 2150, loss = 0.75, batch loss = 0.47 (53.5 examples/sec; 0.149 sec/batch; 13h:42m:33s remains)
INFO - root - 2017-11-30 07:23:11.565599: step 2160, loss = 0.58, batch loss = 0.30 (28.1 examples/sec; 0.285 sec/batch; 26h:09m:38s remains)
INFO - root - 2017-11-30 07:23:13.840613: step 2170, loss = 0.63, batch loss = 0.35 (53.0 examples/sec; 0.151 sec/batch; 13h:50m:34s remains)
INFO - root - 2017-11-30 07:23:15.666791: step 2180, loss = 0.65, batch loss = 0.37 (51.9 examples/sec; 0.154 sec/batch; 14h:08m:40s remains)
INFO - root - 2017-11-30 07:23:17.788827: step 2190, loss = 0.64, batch loss = 0.36 (34.1 examples/sec; 0.234 sec/batch; 21h:29m:52s remains)
INFO - root - 2017-11-30 07:23:19.601920: step 2200, loss = 0.74, batch loss = 0.46 (50.8 examples/sec; 0.157 sec/batch; 14h:26m:47s remains)
INFO - root - 2017-11-30 07:23:21.780348: step 2210, loss = 0.66, batch loss = 0.38 (52.0 examples/sec; 0.154 sec/batch; 14h:06m:08s remains)
INFO - root - 2017-11-30 07:23:23.339702: step 2220, loss = 0.68, batch loss = 0.40 (51.7 examples/sec; 0.155 sec/batch; 14h:12m:26s remains)
INFO - root - 2017-11-30 07:23:24.876955: step 2230, loss = 0.70, batch loss = 0.42 (52.6 examples/sec; 0.152 sec/batch; 13h:57m:14s remains)
INFO - root - 2017-11-30 07:23:27.030800: step 2240, loss = 0.60, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 15h:32m:52s remains)
INFO - root - 2017-11-30 07:23:29.182929: step 2250, loss = 0.66, batch loss = 0.38 (51.4 examples/sec; 0.156 sec/batch; 14h:16m:36s remains)
INFO - root - 2017-11-30 07:23:30.729540: step 2260, loss = 0.58, batch loss = 0.30 (51.5 examples/sec; 0.155 sec/batch; 14h:14m:59s remains)
INFO - root - 2017-11-30 07:23:32.504786: step 2270, loss = 0.74, batch loss = 0.46 (42.8 examples/sec; 0.187 sec/batch; 17h:08m:51s remains)
INFO - root - 2017-11-30 07:23:34.693959: step 2280, loss = 0.65, batch loss = 0.37 (20.9 examples/sec; 0.382 sec/batch; 35h:04m:13s remains)
INFO - root - 2017-11-30 07:23:36.864219: step 2290, loss = 0.79, batch loss = 0.51 (46.8 examples/sec; 0.171 sec/batch; 15h:41m:04s remains)
INFO - root - 2017-11-30 07:23:38.415751: step 2300, loss = 0.68, batch loss = 0.40 (50.1 examples/sec; 0.160 sec/batch; 14h:39m:19s remains)
INFO - root - 2017-11-30 07:23:40.516779: step 2310, loss = 0.80, batch loss = 0.52 (51.1 examples/sec; 0.157 sec/batch; 14h:22m:14s remains)
INFO - root - 2017-11-30 07:23:42.550806: step 2320, loss = 0.65, batch loss = 0.37 (42.1 examples/sec; 0.190 sec/batch; 17h:25m:09s remains)
INFO - root - 2017-11-30 07:23:44.295422: step 2330, loss = 0.73, batch loss = 0.45 (51.1 examples/sec; 0.156 sec/batch; 14h:21m:03s remains)
INFO - root - 2017-11-30 07:23:46.021114: step 2340, loss = 0.81, batch loss = 0.53 (52.8 examples/sec; 0.151 sec/batch; 13h:52m:59s remains)
INFO - root - 2017-11-30 07:23:47.581644: step 2350, loss = 0.69, batch loss = 0.41 (50.9 examples/sec; 0.157 sec/batch; 14h:25m:32s remains)
INFO - root - 2017-11-30 07:23:49.148683: step 2360, loss = 0.69, batch loss = 0.41 (49.3 examples/sec; 0.162 sec/batch; 14h:52m:58s remains)
INFO - root - 2017-11-30 07:23:50.708273: step 2370, loss = 0.70, batch loss = 0.42 (52.6 examples/sec; 0.152 sec/batch; 13h:56m:58s remains)
INFO - root - 2017-11-30 07:23:52.278332: step 2380, loss = 0.69, batch loss = 0.41 (51.3 examples/sec; 0.156 sec/batch; 14h:18m:03s remains)
INFO - root - 2017-11-30 07:23:53.999100: step 2390, loss = 0.69, batch loss = 0.41 (40.0 examples/sec; 0.200 sec/batch; 18h:21m:39s remains)
INFO - root - 2017-11-30 07:23:56.721229: step 2400, loss = 0.60, batch loss = 0.32 (51.4 examples/sec; 0.156 sec/batch; 14h:16m:41s remains)
INFO - root - 2017-11-30 07:23:59.058132: step 2410, loss = 0.69, batch loss = 0.41 (47.9 examples/sec; 0.167 sec/batch; 15h:18m:01s remains)
INFO - root - 2017-11-30 07:24:00.607233: step 2420, loss = 0.67, batch loss = 0.39 (52.6 examples/sec; 0.152 sec/batch; 13h:56m:09s remains)
INFO - root - 2017-11-30 07:24:02.256174: step 2430, loss = 0.68, batch loss = 0.40 (41.3 examples/sec; 0.194 sec/batch; 17h:45m:34s remains)
INFO - root - 2017-11-30 07:24:03.824214: step 2440, loss = 0.72, batch loss = 0.44 (51.3 examples/sec; 0.156 sec/batch; 14h:18m:19s remains)
INFO - root - 2017-11-30 07:24:05.971654: step 2450, loss = 0.60, batch loss = 0.32 (22.6 examples/sec; 0.354 sec/batch; 32h:29m:20s remains)
INFO - root - 2017-11-30 07:24:07.642855: step 2460, loss = 0.71, batch loss = 0.43 (49.6 examples/sec; 0.161 sec/batch; 14h:47m:56s remains)
INFO - root - 2017-11-30 07:24:09.213879: step 2470, loss = 0.76, batch loss = 0.48 (50.6 examples/sec; 0.158 sec/batch; 14h:28m:53s remains)
INFO - root - 2017-11-30 07:24:11.226494: step 2480, loss = 0.74, batch loss = 0.47 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:16s remains)
INFO - root - 2017-11-30 07:24:13.627543: step 2490, loss = 0.59, batch loss = 0.32 (47.5 examples/sec; 0.168 sec/batch; 15h:25m:26s remains)
INFO - root - 2017-11-30 07:24:16.062783: step 2500, loss = 0.58, batch loss = 0.30 (32.5 examples/sec; 0.246 sec/batch; 22h:33m:18s remains)
INFO - root - 2017-11-30 07:24:19.478982: step 2510, loss = 0.76, batch loss = 0.48 (29.7 examples/sec; 0.269 sec/batch; 24h:40m:09s remains)
INFO - root - 2017-11-30 07:24:21.294708: step 2520, loss = 0.61, batch loss = 0.33 (51.3 examples/sec; 0.156 sec/batch; 14h:17m:12s remains)
INFO - root - 2017-11-30 07:24:23.135375: step 2530, loss = 0.70, batch loss = 0.42 (35.4 examples/sec; 0.226 sec/batch; 20h:44m:04s remains)
INFO - root - 2017-11-30 07:24:24.915847: step 2540, loss = 0.60, batch loss = 0.32 (49.4 examples/sec; 0.162 sec/batch; 14h:50m:39s remains)
INFO - root - 2017-11-30 07:24:26.609814: step 2550, loss = 0.69, batch loss = 0.41 (40.6 examples/sec; 0.197 sec/batch; 18h:03m:10s remains)
INFO - root - 2017-11-30 07:24:28.445131: step 2560, loss = 0.56, batch loss = 0.28 (50.8 examples/sec; 0.158 sec/batch; 14h:26m:08s remains)
INFO - root - 2017-11-30 07:24:30.009809: step 2570, loss = 0.67, batch loss = 0.39 (52.6 examples/sec; 0.152 sec/batch; 13h:56m:11s remains)
INFO - root - 2017-11-30 07:24:31.562935: step 2580, loss = 0.63, batch loss = 0.35 (50.2 examples/sec; 0.159 sec/batch; 14h:35m:25s remains)
INFO - root - 2017-11-30 07:24:33.117073: step 2590, loss = 0.66, batch loss = 0.38 (49.3 examples/sec; 0.162 sec/batch; 14h:52m:43s remains)
INFO - root - 2017-11-30 07:24:35.196391: step 2600, loss = 0.75, batch loss = 0.47 (20.6 examples/sec; 0.389 sec/batch; 35h:38m:56s remains)
